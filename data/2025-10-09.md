<div id=toc></div>

# Table of Contents

- [cs.PF](#cs.PF) [Total: 1]
- [cs.AR](#cs.AR) [Total: 4]
- [cs.NI](#cs.NI) [Total: 3]
- [cs.MA](#cs.MA) [Total: 1]
- [cs.SE](#cs.SE) [Total: 11]
- [cs.DC](#cs.DC) [Total: 7]


<div id='cs.PF'></div>

# cs.PF [[Back]](#toc)

### [1] [Accelerating Sparse Ternary GEMM for Quantized LLM inference on Apple Silicon](https://arxiv.org/abs/2510.06957)
*Baraq Lipshitz,Alessio Melone,Charalampos Maraziaris,Muhammed Bilal*

Main category: cs.PF

TL;DR: 本文针对 Apple M 系列芯片优化了稀疏三元通用矩阵乘法（Sparse Ternary GEMM），通过架构感知优化（如新型稀疏数据格式、提升指令级并行性和 NEON SIMD 向量化），在不同稀疏度下显著提升性能，最高可达基线方法的 5.98 倍，并实现处理器理论峰值性能的 50.2%。


<details>
  <summary>Details</summary>
Motivation: 现有库在 Apple Silicon CPU 上对稀疏三元 GEMM 的优化不足，亟需针对其架构特性进行专门优化以提升性能。

Method: 提出面向 Apple M 系列处理器的架构感知优化策略，包括新颖的分块交错稀疏数据格式以改善内存局部性、提升指令级并行性（ILP）的策略，以及基于 NEON 的 SIMD 向量化以利用数据级并行性。

Result: 标量实现在 50% 稀疏度下比传统 TCSC 基线快 5.98 倍，达到处理器理论峰值性能的 50.2%；向量化实现在 25% 稀疏度下快 5.59 倍，且在不同稀疏度下性能稳定。

Conclusion: 针对 Apple Silicon 的稀疏三元 GEMM 优化可显著提升性能，所提方法在不同稀疏条件下均表现出高效与稳定性，为相关硬件平台上的稀疏计算提供了有效解决方案。

Abstract: Sparse Ternary General Matrix-Matrix Multiplication (GEMM) remains
under-optimized in existing libraries for Apple Silicon CPUs. We present a
Sparse Ternary GEMM kernel optimized specifically for Apple's M-series
processors. We propose a set of architecture-aware optimizations, including a
novel blocked and interleaved sparse data format to improve memory locality,
strategies to increase Instruction-Level Parallelism (ILP), and NEON-based
Single Instruction Multiple Data (SIMD) vectorization to exploit data-level
parallelism. Our scalar implementation achieves up to a 5.98x performance
increase over a traditional Ternary Compressed Sparse Column (TCSC) baseline
for large matrices with 50% ternary nonzero values (sparsity), reaching up to a
50.2% of the processor's theoretical peak performance, and remains stable
across varying sparsity levels. Our vectorized implementation delivers up to a
5.59x performance increase for large matrices with 25% sparsity, and remains
stable across varying sparsity levels.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [2] [On-Package Memory with Universal Chiplet Interconnect Express (UCIe): A Low Power, High Bandwidth, Low Latency and Low Cost Approach](https://arxiv.org/abs/2510.06513)
*Debendra Das Sharma,Swadesh Choudhary,Peter Onufryk,Rob Pelt*

Main category: cs.AR

TL;DR: 该论文提出通过增强UCIe协议以支持内存语义，从而在各类计算场景中提供高带宽密度、低延迟、低功耗且低成本的封装内内存解决方案。


<details>
  <summary>Details</summary>
Motivation: 现有封装内内存方案（如HBM4和LPDDR）无法满足AI等新兴计算应用对高能效带宽的需求，面临“内存墙”问题。

Method: 通过逻辑芯片复用现有的LPDDR6和HBM内存，并利用UCIe连接SoC；同时提出让DRAM芯片原生支持UCIe接口而非LPDDR6总线。

Result: 相比现有方案，新方法实现了高达10倍的带宽密度、3倍更低的延迟、3倍更低的功耗以及更低的成本。

Conclusion: 增强UCIe以支持内存语义是一种高效、经济且可扩展的封装内内存架构，能有效应对AI等应用的内存瓶颈。

Abstract: Emerging computing applications such as Artificial Intelligence (AI) are
facing a memory wall with existing on-package memory solutions that are unable
to meet the power-efficient bandwidth demands. We propose to enhance UCIe with
memory semantics to deliver power-efficient bandwidth and cost-effective
on-package memory solutions applicable across the entire computing continuum.
We propose approaches by reusing existing LPDDR6 and HBM memory through a logic
die that connects to the SoC using UCIe. We also propose an approach where the
DRAM die natively supports UCIe instead of the LPDDR6 bus interface. Our
approaches result in significantly higher bandwidth density (up to 10x), lower
latency (up to 3x), lower power (up to 3x), and lower cost compared to existing
HBM4 and LPDDR on-package memory solutions.

</details>


### [3] [RTGS: Real-Time 3D Gaussian Splatting SLAM via Multi-Level Redundancy Reduction](https://arxiv.org/abs/2510.06644)
*Leshu Li,Jiayin Qin,Jie Peng,Zishen Wan,Huaizhi Qu,Ye Han,Pingqing Zheng,Hongsen Zhang,Yu,Cao,Tianlong Chen,Yang,Zhao*

Main category: cs.AR

TL;DR: 本文提出RTGS，一种算法-硬件协同设计框架，通过消除3D高斯泼溅SLAM中的冗余，在边缘设备上实现实时性能，能效提升最高达82.5倍，且几乎无质量损失。


<details>
  <summary>Details</summary>
Motivation: 现有基于3D高斯泼溅（3DGS）的SLAM系统虽具备优异的渲染效率与精度，但由于计算速度不足，尚未能在资源受限的边缘设备上部署。

Method: RTGS从算法和硬件两方面协同优化：算法上引入自适应高斯剪枝和动态下采样；硬件上设计子图块流式策略、像素级调度、R&B缓冲区和梯度合并单元（GMU），以减少冗余和内存访问开销。

Result: 在四个数据集和三种算法上实现≥30 FPS的实时性能，相比基线能效最高提升82.5倍，图像质量损失可忽略。

Conclusion: RTGS有效解决了3DGS-SLAM在边缘设备上的实时性瓶颈，展示了算法-硬件协同设计在高效SLAM系统中的巨大潜力。

Abstract: 3D Gaussian Splatting (3DGS) based Simultaneous Localization and Mapping
(SLAM) systems can largely benefit from 3DGS's state-of-the-art rendering
efficiency and accuracy, but have not yet been adopted in resource-constrained
edge devices due to insufficient speed. Addressing this, we identify notable
redundancies across the SLAM pipeline for acceleration. While conceptually
straightforward, practical approaches are required to minimize the overhead
associated with identifying and eliminating these redundancies. In response, we
propose RTGS, an algorithm-hardware co-design framework that comprehensively
reduces the redundancies for real-time 3DGS-SLAM on edge. To minimize the
overhead, RTGS fully leverages the characteristics of the 3DGS-SLAM pipeline.
On the algorithm side, we introduce (1) an adaptive Gaussian pruning step to
remove the redundant Gaussians by reusing gradients computed during
backpropagation; and (2) a dynamic downsampling technique that directly reuses
the keyframe identification and alpha computing steps to eliminate redundant
pixels. On the hardware side, we propose (1) a subtile-level streaming strategy
and a pixel-level pairwise scheduling strategy that mitigates workload
imbalance via a Workload Scheduling Unit (WSU) guided by previous iteration
information; (2) a Rendering and Backpropagation (R&B) Buffer that accelerates
the rendering backpropagation by reusing intermediate data computed during
rendering; and (3) a Gradient Merging Unit (GMU) to reduce intensive memory
accesses caused by atomic operations while enabling pipelined aggregation.
Integrated into an edge GPU, RTGS achieves real-time performance (>= 30 FPS) on
four datasets and three algorithms, with up to 82.5x energy efficiency over the
baseline and negligible quality loss. Code is available at
https://github.com/UMN-ZhaoLab/RTGS.

</details>


### [4] [Hardware-Efficient CNNs: Interleaved Approximate FP32 Multipliers for Kernel Computation](https://arxiv.org/abs/2510.06767)
*Bindu G Gowda,Yogesh Goyal,Yash Gupta,Madhav Rao*

Main category: cs.AR

TL;DR: 本文提出在CNN推理中使用不同精度的近似FP32乘法器，通过优化其在卷积核中的排布，在保持准确率的同时显著降低硬件开销。


<details>
  <summary>Details</summary>
Motivation: FP32乘法计算开销大、硬件复杂，而神经网络推理对精度要求相对宽松，允许以精度换取面积、功耗和速度的提升。

Method: 采用误差可变的近似压缩器近似FP32乘法器的尾数乘法，并利用NSGA-II算法优化不同近似乘法器在卷积核中的排布。

Result: 通过在卷积层中交错使用不同近似程度的FP32乘法器，有效平衡了模型准确率与硬件效率。

Conclusion: 在CNN推理中合理部署近似FP32乘法器并优化其排布，可在几乎不影响精度的前提下显著降低硬件成本。

Abstract: Single-precision floating point (FP32) data format, defined by the IEEE 754
standard, is widely employed in scientific computing, signal processing, and
deep learning training, where precision is critical. However, FP32
multiplication is computationally expensive and requires complex hardware,
especially for precisely handling mantissa multiplication. In practical
applications like neural network inference, perfect accuracy is not always
necessary, minor multiplication errors often have little impact on final
accuracy. This enables trading precision for gains in area, power, and speed.
This work focuses on CNN inference using approximate FP32 multipliers, where
the mantissa multiplication is approximated by employing error-variant
approximate compressors, that significantly reduce hardware cost. Furthermore,
this work optimizes CNN performance by employing differently approximated FP32
multipliers and studying their impact when interleaved within the kernels
across the convolutional layers. The placement and ordering of these
approximate multipliers within each kernel are carefully optimized using the
Non-dominated Sorting Genetic Algorithm-II, balancing the trade-off between
accuracy and hardware efficiency.

</details>


### [5] [Cocoon: A System Architecture for Differentially Private Training with Correlated Noises](https://arxiv.org/abs/2510.07304)
*Donghwan Kim,Xin Gu,Jinho Baek,Timothy Lo,Younghoon Min,Kwangsik Shin,Jongryool Kim,Jongse Park,Kiwan Maeng*

Main category: cs.AR

TL;DR: 该论文研究了使用差分隐私（如DP-SGD）训练机器学习模型时因添加噪声而导致的准确率下降问题，提出了一种名为Cocoon的软硬件协同设计框架，通过预计算并以合并格式存储相关噪声（Cocoon-Emb）以及使用定制近内存处理设备（Cocoon-NMP）来高效支持带嵌入表的大模型训练，在FPGA原型系统上实现了显著的性能提升。


<details>
  <summary>Details</summary>
Motivation: 差分隐私训练方法（如DP-SGD）在每次迭代中添加噪声，损害模型准确率；虽然引入相关噪声可在迭代间相互抵消以提升准确率，但这类方法在大模型或含大型嵌入表时带来显著开销。

Method: 提出Cocoon框架，包含两种技术：Cocoon-Emb通过预计算并以合并格式存储相关噪声来加速含嵌入表的模型；Cocoon-NMP则利用定制的近内存处理设备支持大模型训练。

Result: 在基于FPGA的NMP设备原型系统上，Cocoon-Emb带来2.33–10.82倍性能提升，Cocoon-NMP带来1.55–3.06倍性能提升。

Conclusion: Cocoon通过软硬件协同设计有效缓解了相关噪声机制在大模型和嵌入表场景下的性能开销，显著提升了差分隐私训练的效率。

Abstract: Machine learning (ML) models memorize and leak training data, causing serious
privacy issues to data owners. Training algorithms with differential privacy
(DP), such as DP-SGD, have been gaining attention as a solution. However,
DP-SGD adds a noise at each training iteration, which degrades the accuracy of
the trained model. To improve accuracy, a new family of approaches adds
carefully designed correlated noises, so that noises cancel out each other
across iterations. We performed an extensive characterization study of these
new mechanisms, for the first time to the best of our knowledge, and show they
incur non-negligible overheads when the model is large or uses large embedding
tables. Motivated by the analysis, we propose Cocoon, a hardware-software
co-designed framework for efficient training with correlated noises. Cocoon
accelerates models with embedding tables through pre-computing and storing
correlated noises in a coalesced format (Cocoon-Emb), and supports large models
through a custom near-memory processing device (Cocoon-NMP). On a real system
with an FPGA-based NMP device prototype, Cocoon improves the performance by
2.33-10.82x(Cocoon-Emb) and 1.55-3.06x (Cocoon-NMP).

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [6] [Adaptive Semantic Communication for UAV/UGV Cooperative Path Planning](https://arxiv.org/abs/2510.06901)
*Fangzhou Zhao,Yao Sun,Jianglin Lan,Lan Zhang,Xuesong Liu,Muhammad Ali Imran*

Main category: cs.NI

TL;DR: 本文提出一种语义通信（SemCom）框架，用于在无线通信不可靠的复杂环境中提升无人机（UAV）与无人地面车（UGV）协同路径规划的效率。该框架通过仅传输路径规划所需的关键语义信息，显著减少数据传输量，同时保持规划准确性。


<details>
  <summary>Details</summary>
Motivation: 在复杂环境中，UAV与UGV协同执行任务（如搜救、监控）时，受限于严重干扰和非视距条件，传统无线通信难以支持及时准确的路径规划。

Method: 设计面向UAV-UGV协同路径规划的语义通信框架，定义路径规划的关键语义，并开发专用收发器，仅传输关键语义信息而非原始数据。

Result: 仿真结果表明，所提收发器相比传统SemCom收发器，在显著降低数据传输量的同时，保持了路径规划的准确性，提升了系统协同效率。

Conclusion: 语义通信能有效应对复杂环境下UAV-UGV协同路径规划中的通信挑战，在减少带宽需求的同时保障任务性能，具有实际应用潜力。

Abstract: Effective path planning is fundamental to the coordination of unmanned aerial
vehicles (UAVs) and unmanned ground vehicles (UGVs) systems, particularly in
applications such as surveillance, navigation, and emergency response.
Combining UAVs' broad field of view with UGVs' ground-level operational
capability greatly improve the likelihood of successfully achieving task
objectives such as locating victims, monitoring target areas, or navigating
hazardous terrain. In complex environments, UAVs need to provide precise
environmental perception information for UGVs to optimize their routing policy.
However, due to severe interference and non-line-of-sight conditions, wireless
communication is often unstable in such complex environments, making it
difficult to support timely and accurate path planning for UAV-UGV
coordination. To this end, this paper proposes a semantic communication
(SemCom) framework to enhance UAV/UGV cooperative path planning under
unreliable wireless conditions. Unlike traditional methods that transmit raw
data, SemCom transmits only the key information for path planning, reducing
transmission volume without sacrificing accuracy. The proposed framework is
developed by defining key semantics for path planning and designing a
transceiver for meeting the requirements of UAV-UGV cooperative path planning.
Simulation results show that, compared to conventional SemCom transceivers, the
proposed transceiver significantly reduces data transmission volume while
maintaining path planning accuracy, thereby enhancing system collaboration
efficiency.

</details>


### [7] [Dynamic Control Aware Semantic Communication Enabled Image Transmission for Lunar Landing](https://arxiv.org/abs/2510.06916)
*Fangzhou Zhao,Yao Sun,Jianglin Lan,Muhammad Ali Imran*

Main category: cs.NI

TL;DR: 本文提出一种基于语义通信（SemCom）的新框架，用于月球着陆器与轨道卫星之间的图像传输，通过动态调整传输策略提升自主着陆的精度与可靠性。


<details>
  <summary>Details</summary>
Motivation: 传统月球着陆任务中，本地控制系统在高动态环境下可靠性不足，而现有通信方式难以应对月面极端环境（如温度波动、太阳辐射和月尘干扰），限制了着陆精度与安全性。

Method: 设计一种语义通信框架，使着陆器与运行远程着陆控制算法的卫星之间传输图像；编码器-解码器根据着陆控制算法的实时反馈动态调整传输策略，确保关键图像特征准确传递。

Result: 仿真结果表明，相比传统通信方法，所提语义通信方法显著提升了自主着陆性能，同时理论分析验证了该框架在提升控制算法精度和降低端到端传输时延方面的有效性。

Conclusion: 语义通信能有效应对月面恶劣通信环境，在减轻着陆器计算负担的同时提升着陆控制的可靠性与精度，为未来自主月球着陆任务提供新思路。

Abstract: The primary challenge in autonomous lunar landing missions lies in the
unreliable local control system, which has limited capacity to handle
high-dynamic conditions, severely affecting landing precision and safety.
Recent advancements in lunar satellite communication make it possible to
establish a wireless link between lunar orbit satellites and the lunar lander.
This enables satellites to run high-performance autonomous landing algorithms,
improving landing accuracy while reducing the lander's computational and
storage load. Nevertheless, traditional communication paradigms are not
directly applicable due to significant temperature fluctuations on the lunar
surface, intense solar radiation, and severe interference caused by lunar dust
on hardware. The emerging technique of semantic communication (SemCom) offers
significant advantages in robustness and resource efficiency, particularly
under harsh channel conditions. In this paper, we introduce a novel SemCom
framework for transmitting images from the lander to satellites operating the
remote landing control system. The proposed encoder-decoder dynamically adjusts
the transmission strategy based on real-time feedback from the lander's control
algorithm, ensuring the accurate delivery of critical image features and
enhancing control reliability. We provide a rigorous theoretical analysis of
the conditions that improve the accuracy of the control algorithm and reduce
end-to-end transmission time under the proposed framework. Simulation results
demonstrate that our SemCom method significantly enhances autonomous landing
performance compared to traditional communication methods.

</details>


### [8] [A Genetic Algorithm Approach to Anti-Jamming UAV Swarm Behavior](https://arxiv.org/abs/2510.07292)
*Tiago Silva,António Grilo*

Main category: cs.NI

TL;DR: 本文提出利用遗传算法联合优化无人机群的编队、波束成形天线和流量路由，以减轻主协调信道中干扰的影响，并通过仿真验证了该方法的有效性。


<details>
  <summary>Details</summary>
Motivation: 无人机群依赖无线通信进行态势感知与协同，但通信信道易受干扰，成为其“阿喀琉斯之踵”；现有抗干扰技术虽成熟，但如何通过智能群行为增强抗干扰能力仍是开放问题。

Method: 采用遗传算法（GA）联合优化无人机群的编队结构、波束成形天线方向和通信流量路由，同时假设存在一个更稳健但低数据率的信道用于编队管理信号。

Result: 仿真实验表明所提方法能有效缓解干扰对主协调信道的影响，但计算开销较大。

Conclusion: 该方法在提升无人机群抗干扰能力方面具有潜力，但高昂的计算成本需进一步研究优化。

Abstract: In recent years, Unmanned Aerial Vehicles (UAVs) have brought a new true
revolution to military tactics. While UAVs already constitute an advantage when
operating alone, multi-UAV swarms expand the available possibilities, allowing
the UAVs to collaborate and support each other as a team to carry out a given
task. This entails the capability to exchange information related with
situation awareness and action coordination by means of a suitable wireless
communication technology. In such scenario, the adversary is expected to
disrupt communications by jamming the communication channel. The latter becomes
the Achilles heel of the swarm. While anti-jamming techniques constitute a well
covered topic in the literature, the use of intelligent swarm behaviors to
leverage those techniques is still an open research issue.
  This paper explores the use of Genetic Algorithms (GAs) to jointly optimize
UAV swarm formation, beam-steering antennas and traffic routing in order to
mitigate the effect of jamming in the main coordination channel, under the
assumption that a more robust and low data rate channel is used for formation
management signaling. Simulation results show the effectiveness of proposed
approach. However, the significant computational cost paves the way for further
research.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [9] [R3R: Decentralized Multi-Agent Collision Avoidance with Infinite-Horizon Safety](https://arxiv.org/abs/2510.06436)
*Thomas Marshall Vielmetti,Devansh R. Agrawal,Dimitra Panagou*

Main category: cs.MA

TL;DR: R3R 是首个在距离通信约束下，为非线性多智能体系统提供无限时域安全保证的去中心化异步运动规划框架。


<details>
  <summary>Details</summary>
Motivation: 现有去中心化多智能体运动规划方法缺乏形式化的无限时域安全保证，尤其在通信受限系统中。

Method: R3R 结合了门卫安全框架与一种称为 R-Boundedness 的几何约束，将智能体的通信半径与其安全规划能力形式化关联，并限制轨迹在由通信半径决定的固定规划半径内，仅使用局部信息即可保证轨迹始终安全。该算法完全异步，适用于动态变化的网络。

Result: 在多达 128 个 Dubins 车辆的仿真中，R3R 在高密度、障碍物丰富的场景中实现了 100% 的安全性，且性能随智能体密度而非问题规模扩展。

Conclusion: R3R 提供了一种可扩展且形式化安全的多智能体运动规划解决方案，适用于通信受限、动态变化的去中心化系统。

Abstract: Existing decentralized methods for multi-agent motion planning lack formal,
infinite-horizon safety guarantees, especially for communication-constrained
systems. We present R3R, to our knowledge the first decentralized and
asynchronous framework for multi-agent motion planning under distance-based
communication constraints with infinite-horizon safety guarantees for systems
of nonlinear agents. R3R's novelty lies in combining our gatekeeper safety
framework with a geometric constraint called R-Boundedness, which together
establish a formal link between an agent's communication radius and its ability
to plan safely. We constrain trajectories to within a fixed planning radius
that is a function of the agent's communication radius, which enables
trajectories to be shown provably safe for all time, using only local
information. Our algorithm is fully asynchronous, and ensures the forward
invariance of these guarantees even in time-varying networks where agents
asynchronously join, leave, and replan. We validate our approach in simulations
of up to 128 Dubins vehicles, demonstrating 100% safety in dense, obstacle rich
scenarios. Our results demonstrate that R3R's performance scales with agent
density rather than problem size, providing a practical solution for scalable
and provably safe multi-agent systems.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [10] [Leveraging Large Language Models for Cybersecurity Risk Assessment -- A Case from Forestry Cyber-Physical Systems](https://arxiv.org/abs/2510.06343)
*Fikret Mert Gültekin,Oscar Lilja,Ranim Khojah,Rebekka Wohlrab,Marvin Damschen,Mazen Mohamad*

Main category: cs.SE

TL;DR: 本文探讨了在林业等安全关键领域中，利用本地部署的大语言模型（LLM）结合检索增强生成技术辅助网络安全风险评估的可行性，强调在满足数据隐私要求的同时，LLM可支持专家进行初步评估、威胁识别和冗余检查，但仍需人工监督。


<details>
  <summary>Details</summary>
Motivation: 在许多软件团队中缺乏足够的网络安全专家，导致工程师需自行开展风险评估活动，亟需工具支持以减轻专家负担并提升评估效率，同时满足数据保护与隐私限制。

Method: 采用设计科学研究方法，在一个大型项目中通过访谈、互动会议和问卷调查，与12位专家合作，评估本地部署的LLM结合检索增强生成在网络安全风险评估中的应用效果。

Result: LLM能够有效辅助专家生成初步风险评估、识别威胁并进行冗余检查；专家虽对LLM的信任有限，但愿意在特定辅助角色中使用LLM，前提是保留人工监督以确保准确性和合规性。

Conclusion: 本地部署的LLM结合检索增强生成可作为安全关键领域中网络安全风险评估的有效辅助工具，但必须辅以人类专家监督，未来可在更多网络物理系统中推广此类智能代理支持机制。

Abstract: In safety-critical software systems, cybersecurity activities become
essential, with risk assessment being one of the most critical. In many
software teams, cybersecurity experts are either entirely absent or represented
by only a small number of specialists. As a result, the workload for these
experts becomes high, and software engineers would need to conduct
cybersecurity activities themselves. This creates a need for a tool to support
cybersecurity experts and engineers in evaluating vulnerabilities and threats
during the risk assessment process. This paper explores the potential of
leveraging locally hosted large language models (LLMs) with retrieval-augmented
generation to support cybersecurity risk assessment in the forestry domain
while complying with data protection and privacy requirements that limit
external data sharing. We performed a design science study involving 12 experts
in interviews, interactive sessions, and a survey within a large-scale project.
The results demonstrate that LLMs can assist cybersecurity experts by
generating initial risk assessments, identifying threats, and providing
redundancy checks. The results also highlight the necessity for human oversight
to ensure accuracy and compliance. Despite trust concerns, experts were willing
to utilize LLMs in specific evaluation and assistance roles, rather than solely
relying on their generative capabilities. This study provides insights that
encourage the use of LLM-based agents to support the risk assessment process of
cyber-physical systems in safety-critical domains.

</details>


### [11] [Improving Assignment Submission in Higher Education through a Git-Enabled System: An Iterative Case Study](https://arxiv.org/abs/2510.06363)
*Ololade Babatunde,Tomisin Ayodabo,Raqibul Raqibul*

Main category: cs.SE

TL;DR: 本文提出并评估了一种定制的基于 Git 的作业提交系统，显著提升了作业追踪、协作效率和学生学习体验，并减少了教师管理负担和系统存储需求。


<details>
  <summary>Details</summary>
Motivation: 传统高校作业提交方式存在效率低、协作困难和管理负担重等问题，亟需更现代化、高效的解决方案。

Method: 采用迭代式软件开发与以用户为中心的设计方法，在真实大学环境中部署并评估该 Git 提交系统，结合可用性测试与学生反馈进行持续优化。

Result: 85% 的教师认为该系统更易用，84% 的学生更偏好此系统；作业提交与评审时间减少 38%，存储需求降低 48%；学生反馈学习效果提升，行政负担减轻。

Conclusion: 基于 Git 的作业提交系统在教育环境中具有显著优势，能有效提升教学效率与学生参与度，为软件工程及相关学科的教学实践提供实用参考。

Abstract: This study addresses challenges in traditional assignment submission methods
used in higher education by introducing and evaluating a customized Git-based
submission system. Employing iterative software development and user-centered
design methodologies, the system was integrated within a real-world university
environment. Empirical evaluation, including usability testing and student
feedback, indicated significant improvements in assignment tracking,
collaboration, and submission efficiency. Students reported positive
experiences using distributed version control workflows, highlighting improved
learning outcomes and reduced administrative burden. Challenges related to
initial adoption and student learning curves were identified and mitigated
through iterative improvements. The proposed system contributes practical
insights for integrating distributed version control into educational settings,
enhancing both instructor oversight and student engagement in software
engineering and related disciplines. Based on our results, the research showed
that 85% of instructors found the git based system easier to use, with 84% of
students preferring it over traditional methods, as it provides a 38% reduction
in time taken for submission and review, while also leading to a 48% reduction
in storage requirements.

</details>


### [12] [Addressing Visual Impairments with Model-Driven Engineering: A Systematic Literature Review](https://arxiv.org/abs/2510.06483)
*Judith Michael,Lukas Netz,Bernhard Rumpe,Ingo Müller,John Grundy,Shavindra Wickramathilaka,Hourieh Khalajzadeh*

Main category: cs.SE

TL;DR: 本文系统综述了模型驱动工程（MDE）在支持视障用户无障碍方面的研究现状，发现现有工作虽常引用WCAG指南，但缺乏具体建模方法、可复用的技术细节和充分的用户验证，整体支持不足，并提出了未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 软件常对有无障碍需求的用户（如视障人士）构成障碍，而MDE具备系统化代码生成能力，有望在减少人工干预的同时将无障碍需求融入开发流程，但其实际支持效果尚不明确。

Method: 开展系统性文献综述，从447篇初始论文中筛选出30篇符合标准的主研究，分析其对视障无障碍的支持方式、建模内容、技术细节及验证方法。

Result: 约三分之二的研究引用了WCAG，但多为项目特定适配且缺乏用户验证；研究普遍建模了界面结构、交互、用户能力等，但极少说明具体建模技术或展示完整系统；MDE方法细节（如转换规则、代码模板）不足，限制了复用与复现；用户参与和开发者无障碍专业知识有限，导致实证验证薄弱。

Conclusion: 当前MDE研究对视障无障碍的支持仍不充分，需在建模方法、技术细节公开、用户参与和开发者能力建设等方面加强，并提出了嵌入无障碍支持的MDE研究议程。

Abstract: Software applications often pose barriers for users with accessibility needs,
e.g., visual impairments. Model-driven engineering (MDE), with its systematic
nature of code derivation, offers systematic methods to integrate accessibility
concerns into software development while reducing manual effort. This paper
presents a systematic literature review on how MDE addresses accessibility for
vision impairments. From 447 initially identified papers, 30 primary studies
met the inclusion criteria. About two-thirds reference the Web Content
Accessibility Guidelines (WCAG), yet their project-specific adaptions and
end-user validations hinder wider adoption in MDE. The analyzed studies model
user interface structures, interaction and navigation, user capabilities,
requirements, and context information. However, only few specify concrete
modeling techniques on how to incorporate accessibility needs or demonstrate
fully functional systems. Insufficient details on MDE methods, i.e.,
transformation rules or code templates, hinder the reuse, generalizability, and
reproducibility. Furthermore, limited involvement of affected users and limited
developer expertise in accessibility contribute to weak empirical validation.
Overall, the findings indicate that current MDE research insufficiently
supports vision-related accessibility. Our paper concludes with a research
agenda outlining how support for vision impairments can be more effectively
embedded in MDE processes.

</details>


### [13] [Beyond More Context: How Granularity and Order Drive Code Completion Quality](https://arxiv.org/abs/2510.06606)
*Uswat Yusuf,Genevieve Caumartin,Diego Elias Costa*

Main category: cs.SE

TL;DR: 本文针对代码补全任务中上下文选择的挑战，提出基于静态分析的块级检索策略，在Python任务中相较无上下文基线提升16%，强调检索粒度、上下文排序与混合策略的重要性。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在代码补全任务中依赖相关且充分的上下文，但在大型代码库中，受限于上下文长度及噪声干扰，如何有效构建上下文成为关键挑战。

Method: 作者设计并评估了多种文件级与块级（chunk-level）的检索策略，结合静态分析实现块级上下文检索，并研究上下文大小与文件排序对模型性能的影响。

Result: 块级检索策略在Python任务中相较最佳文件级策略提升6%，相较无上下文基线提升16%，验证了检索粒度和上下文排序对性能的显著影响。

Conclusion: 有效的上下文收集需综合考虑检索粒度、上下文排序及混合策略，块级静态分析方法能显著提升代码补全质量。

Abstract: Context plays an important role in the quality of code completion, as Large
Language Models (LLMs) require sufficient and relevant information to assist
developers in code generation tasks. However, composing a relevant context for
code completion poses challenges in large repositories: First, the limited
context length of LLMs makes it impractical to include all repository files.
Second, the quality of generated code is highly sensitive to noisy or
irrelevant context. In this paper, we present our approach for the ASE 2025
Context Collection Challenge. The challenge entails outperforming JetBrains
baselines by designing effective retrieval and context collection strategies.
We develop and evaluate a series of experiments that involve retrieval
strategies at both the file and chunk levels. We focus our initial experiments
on examining the impact of context size and file ordering on LLM performance.
Our results show that the amount and order of context can significantly
influence the performance of the models. We introduce chunk-based retrieval
using static analysis, achieving a 6% improvement over our best file-retrieval
strategy and a 16% improvement over the no-context baseline for Python in the
initial phase of the competition. Our results highlight the importance of
retrieval granularity, ordering and hybrid strategies in developing effective
context collection pipelines for real-world development scenarios.

</details>


### [14] [AISysRev -- LLM-based Tool for Title-abstract Screening](https://arxiv.org/abs/2510.06708)
*Aleksi Huotala,Miikka Kuutila,Olli-Pekka Turtio,Mika Mäntylä*

Main category: cs.SE

TL;DR: 本文提出了一种基于大语言模型（LLM）的系统性文献筛选工具AiSysRev，用于辅助软件工程领域的系统性综述，尤其在标题和摘要筛选阶段。该工具支持多种LLM、零样本与少样本提示，并通过用户界面将LLM结果作为人工审核的参考。试验研究表明，LLM可有效分类“易纳入”和“易排除”论文，但在边界案例中仍需人工干预，从而显著减轻人工筛选负担。


<details>
  <summary>Details</summary>
Motivation: 系统性综述在软件工程中是总结证据的标准做法，但其筛选阶段工作量巨大，尤其当论文数量庞大时。现有研究表明大语言模型（LLM）在标题-摘要筛选方面表现接近硕士生水平，虽不可完全信赖，但可用于加速综述流程（如快速综述）。因此，作者旨在开发一个实用工具，结合LLM与人工判断，提高筛选效率。

Method: 作者开发了AiSysRev——一个基于LLM的筛选工具，以Docker容器形式部署的Web应用。用户上传包含论文标题和摘要的CSV文件，并设定纳入/排除标准。工具通过OpenRouter支持多种LLM，提供零样本和少样本筛选模式，并在界面上展示LLM结果以辅助人工审核。

Result: 在包含137篇论文的试验中，作者发现论文可分为四类：易纳入、易排除、边界纳入和边界排除。LLM在边界案例中容易出错，凸显了人工干预的必要性。结果表明，LLM虽不能取代人类判断，但能显著减少大规模文献筛选的工作量。

Conclusion: 大语言模型在系统性综述的筛选阶段具有实用价值，尤其在处理大量文献时可有效减轻人工负担。然而，在边界案例中仍需人类专家介入。AiSysRev工具为结合LLM与人工审核提供了一个可行框架，有助于提升系统性综述的效率与可扩展性。

Abstract: Systematic reviews are a standard practice for summarizing the state of
evidence in software engineering. Conducting systematic reviews is laborious,
especially during the screening or study selection phase, where the number of
papers can be overwhelming. During this phase, papers are assessed against
inclusion and exclusion criteria based on their titles and abstracts. Recent
research has demonstrated that large language models (LLMs) can perform
title-abstract screening at a level comparable to that of a master's student.
While LLMs cannot be fully trusted, they can help, for example, in Rapid
Reviews, which try to expedite the review process. Building on recent research,
we developed AiSysRev, an LLM-based screening tool implemented as a web
application running in a Docker container. The tool accepts a CSV file
containing paper titles and abstracts. Users specify inclusion and exclusion
criteria. One can use multiple LLMs for screening via OpenRouter. AiSysRev
supports both zero-shot and few-shot screening, and also allows for manual
screening through interfaces that display LLM results as guidance for human
reviewers.We conducted a trial study with 137 papers using the tool. Our
findings indicate that papers can be classified into four categories: Easy
Includes, Easy Excludes, Boundary Includes, and Boundary Excludes. The Boundary
cases, where LLMs are prone to errors, highlight the need for human
intervention. While LLMs do not replace human judgment in systematic reviews,
they can significantly reduce the burden of assessing large volumes of
scientific literature. Video: https://www.youtube.com/watch?v=jVbEj4Y4tQI Tool:
https://github.com/EvoTestOps/AISysRev

</details>


### [15] [LLM Company Policies and Policy Implications in Software Organizations](https://arxiv.org/abs/2510.06718)
*Ranim Khojah,Mazen Mohamad,Linda Erlenhov,Francisco Gomes de Oliveira Neto,Philipp Leitner*

Main category: cs.SE

TL;DR: 本文研究了11家公司如何制定大型语言模型（LLM）聊天机器人在软件组织中的使用政策，以帮助管理者安全地将其整合到开发流程中。


<details>
  <summary>Details</summary>
Motivation: 由于在软件组织中采用LLM聊天机器人存在风险，因此需要明确的政策来指导其安全使用。

Method: 通过对11家公司的政策制定过程及其影响因素进行案例研究。

Result: 揭示了影响政策制定的关键因素，并为管理者提供了实践建议。

Conclusion: 制定清晰的LLM聊天机器人使用政策对于安全集成到软件开发流程至关重要，公司应根据自身情况考虑多种影响因素。

Abstract: The risks associated with adopting large language model (LLM) chatbots in
software organizations highlight the need for clear policies. We examine how 11
companies create these policies and the factors that influence them, aiming to
help managers safely integrate chatbots into development workflows.

</details>


### [16] [Oops!... I did it again. Conclusion (In-)Stability in Quantitative Empirical Software Engineering: A Large-Scale Analysis](https://arxiv.org/abs/2510.06844)
*Nicole Hoess,Carlos Paradis,Rick Kazman,Wolfgang Mauerer*

Main category: cs.SE

TL;DR: 该研究通过复现三篇高影响力论文，使用四种不同的软件仓库挖掘工具，发现工具设计与实现中的技术细节差异会显著影响数据提取、分析结果甚至研究结论，强调了工具选择、复用及提供复现包的重要性。


<details>
  <summary>Details</summary>
Motivation: 软件仓库挖掘工具被广泛用于研究与实践中，但其局限性及不同工具间的一致性缺乏系统理解，可能威胁研究有效性。

Method: 通过轻量级文献综述选取三篇关于协作协调、软件维护和软件质量的高水平研究，使用四种独立且系统选定的挖掘工具进行正式复现，定量与定性比较所提取的数据、分析结果及结论。

Result: 工具在复杂挖掘流程中的技术细节差异会累积，导致基线数据、衍生数据、统计分析结果乃至特定情境下的研究结论出现显著差异。

Conclusion: 用户应谨慎选择工具并评估其局限性；推荐重用工具，研究者和工具开发者应通过提供复现包和开展类似比较研究来提升可复现性与减少不确定性。

Abstract: Context: Mining software repositories is a popular means to gain insights
into a software project's evolution, monitor project health, support decisions
and derive best practices. Tools supporting the mining process are commonly
applied by researchers and practitioners, but their limitations and agreement
are often not well understood.
  Objective: This study investigates some threats to validity in complex tool
pipelines for evolutionary software analyses and evaluates the tools' agreement
in terms of data, study outcomes and conclusions for the same research
questions.
  Method: We conduct a lightweight literature review to select three studies on
collaboration and coordination, software maintenance and software quality from
high-ranked venues, which we formally replicate with four independent,
systematically selected mining tools to quantitatively and qualitatively
compare the extracted data, analysis results and conclusions.
  Results: We find that numerous technical details in tool design and
implementation accumulate along the complex mining pipelines and can cause
substantial differences in the extracted baseline data, its derivatives,
subsequent results of statistical analyses and, under specific circumstances,
conclusions.
  Conclusions: Users must carefully choose tools and evaluate their limitations
to assess the scope of validity in an adequate way. Reusing tools is
recommended. Researchers and tool authors can promote reusability and help
reducing uncertainties by reproduction packages and comparative studies
following our approach.

</details>


### [17] [An empirical study on declined proposals: why are these proposals declined?](https://arxiv.org/abs/2510.06984)
*Masanari Kondo,Mahmoud Alfadel,Shane McIntosh,Yasutaka Kamei,Naoyasu Ubayashi*

Main category: cs.SE

TL;DR: 该研究分析了Go语言项目中1091个设计提案，发现提案被拒绝的比例高于接受，平均处理时间超过一个月，且仅14.7%被拒提案会重新提交。通过定性分析归纳出九类拒绝原因，并验证了基于GPT的大模型可在讨论早期预测提案结果（F1=0.71），有助于优化审查流程和提升贡献者体验。


<details>
  <summary>Details</summary>
Motivation: 开源软件项目中的设计提案流程资源消耗大、反馈不明确，导致贡献者挫败感，而提案被拒的原因缺乏系统理解，限制了流程优化和贡献者指导。

Method: 采用混合方法对Go项目中的1091个提案进行实证研究：量化提案结果、通过定性编码构建拒绝原因分类体系，并评估大语言模型（LLM）在早期预测提案结果的能力。

Result: 提案多被拒绝，平均处理时间超一个月；仅14.7%被拒提案被重提；识别出九类拒绝原因（如重复、用例有限、违反项目原则等）；GPT模型可在仅有部分评论时以F1=0.71预测结果。

Conclusion: 提案流程存在效率低下问题，但通过结构化理解历史拒绝原因和引入早期预测工具，可改善贡献者体验并减轻审查负担。

Abstract: Design-level decisions in open-source software (OSS) projects are often made
through structured mechanisms such as proposals, which require substantial
community discussion and review. Despite their importance, the proposal process
is resource-intensive and often leads to contributor frustration, especially
when proposals are declined without clear feedback. Yet, the reasons behind
proposal rejection remain poorly understood, limiting opportunities to
streamline the process or guide contributors effectively. This study
investigates the characteristics and outcomes of proposals in the Go
programming language to understand why proposals are declined and how such
outcomes might be anticipated. We conduct a mixed-method empirical study on
1,091 proposals submitted to the Go project. We quantify proposal outcomes,
build a taxonomy of decline reasons, and evaluate large language models (LLMs)
for predicting these outcomes. We find that proposals are more often declined
than accepted, and resolution typically takes over a month. Only 14.7% of
declined proposals are ever resubmitted. Through qualitative coding, we
identify nine key reasons for proposal decline, such as duplication, limited
use cases, or violations of project principles. This taxonomy can help
contributors address issues in advance, e.g., checking for existing
alternatives can reduce redundancy. We also demonstrate that GPT-based models
can predict decline decisions early in the discussion (F1 score = 0.71 with
partial comments), offering a practical tool for prioritizing review effort.
Our findings reveal inefficiencies in the proposal process and highlight
actionable opportunities for improving both contributor experience and reviewer
workload by enabling early triage and guiding contributors to strengthen their
proposals using a structured understanding of past decline reasons.

</details>


### [18] [Human-aligned AI Model Cards with Weighted Hierarchy Architecture](https://arxiv.org/abs/2510.06989)
*Pengyue Yang,Haolin Jin,Qingwen Zeng,Jiawen Wen,Harry Rao,Huaming Chen*

Main category: cs.SE

TL;DR: 本文提出了一种新的大语言模型文档框架CRAI-MCF，通过引入量化标准和价值敏感设计，实现对模型更全面、可比较和负责任的评估与选择。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型生态系统中，模型文档不一致、不完整且不平衡，现有文档框架（如Model Cards）多为静态且缺乏量化机制，难以支持跨模型的严谨比较，导致模型利用率低和负责任采用受阻。

Method: 基于对240个开源项目的实证分析，提炼出217个参数，构建了一个包含八个模块、与人类价值观对齐的CRAI-MCF框架，并引入量化充分性标准以支持统一的模型评估与比较。

Result: CRAI-MCF框架能够有效整合技术、伦理与操作维度，支持用户高效、可信地评估和选用大语言模型。

Conclusion: CRAI-MCF通过将静态文档转变为可操作、与人类价值观对齐的动态框架，显著提升了大语言模型的可发现性、可比性和负责任采用水平。

Abstract: The proliferation of Large Language Models (LLMs) has led to a burgeoning
ecosystem of specialized, domain-specific models. While this rapid growth
accelerates innovation, it has simultaneously created significant challenges in
model discovery and adoption. Users struggle to navigate this landscape due to
inconsistent, incomplete, and imbalanced documentation across platforms.
Existing documentation frameworks, such as Model Cards and FactSheets, attempt
to standardize reporting but are often static, predominantly qualitative, and
lack the quantitative mechanisms needed for rigorous cross-model comparison.
This gap exacerbates model underutilization and hinders responsible adoption.
To address these shortcomings, we introduce the Comprehensive Responsible AI
Model Card Framework (CRAI-MCF), a novel approach that transitions from static
disclosures to actionable, human-aligned documentation. Grounded in Value
Sensitive Design (VSD), CRAI-MCF is built upon an empirical analysis of 240
open-source projects, distilling 217 parameters into an eight-module,
value-aligned architecture. Our framework introduces a quantitative sufficiency
criterion to operationalize evaluation and enables rigorous cross-model
comparison under a unified scheme. By balancing technical, ethical, and
operational dimensions, CRAI-MCF empowers practitioners to efficiently assess,
select, and adopt LLMs with greater confidence and operational integrity.

</details>


### [19] [Building an Open AIBOM Standard in the Wild](https://arxiv.org/abs/2510.07070)
*Gopi Krishnan Rajbahadur,Keheliya Gallaba,Elyas Rashno,Arthit Suriyawongkul,Karen Bennet,Kate Stewart,Ahmed E. Hassan*

Main category: cs.SE

TL;DR: 本文通过行动研究方法，记录并验证了AI物料清单（AIBOM）规范的制定过程，该规范扩展了SPDX标准以涵盖AI组件，并通过法规对齐、用例映射、从业者访谈和工业案例研究进行验证。


<details>
  <summary>Details</summary>
Motivation: 在AI等快速演进的领域，社区驱动的开放标准如何制定尚不明确，亟需探索有效的方法和实践经验。

Method: 采用行动研究（Action Research）框架，组织全球90多位利益相关者参与多轮AR循环，开发AIBOM规范，并通过四种互补方式验证：与法规和伦理标准对齐、映射六个行业用例、半结构化从业者访谈及工业案例研究。

Result: 成功制定并验证了AIBOM规范，该规范扩展了SPDX标准以支持AI组件（如数据集和训练产物），并提炼出适用于未来软件工程标准化工作的实践启示。

Conclusion: AIBOM的开发不仅产出一个经验证的标准工件，还展示了在真实环境中通过行动研究推动社区驱动标准制定的可行路径，为软件工程领域的未来标准化工作提供了宝贵经验。

Abstract: Modern software engineering increasingly relies on open, community-driven
standards, yet how such standards are created in fast-evolving domains like
AI-powered systems remains underexplored. This paper presents a detailed
experience report on the development of the AI Bill of Materials AIBOM
specification, an extension of the ISO/IEC 5962:2021 Software Package Data
Exchange (SPDX) software bill of materials (SBOM) standard, which captures AI
components such as datasets and iterative training artifacts. Framed through
the lens of Action Research (AR), we document a global, multi-stakeholder
effort involving over 90 contributors and structured AR cycles. The resulting
specification was validated through four complementary approaches: alignment
with major regulations and ethical standards (e.g., EU AI Act and IEEE 7000
standards), systematic mapping to six industry use cases, semi-structured
practitioner interviews, and an industrial case study. Beyond delivering a
validated artefact, our paper documents the process of building the AIBOM
specification in the wild, and reflects on how it aligns with the AR cycle, and
distills lessons that can inform future standardization efforts in the software
engineering community.

</details>


### [20] [Prompt, Synthesize, Fine-Tune: A Secure Code Generation Recipe](https://arxiv.org/abs/2510.07189)
*Junjie Li,Fazle Rabbi,Bo Yang,Song Wang,Jinqiu Yang*

Main category: cs.SE

TL;DR: 本文提出了Secure-Instruct框架，通过自动生成高质量的漏洞与安全代码示例及对应指令，对大语言模型进行指令微调，显著提升了生成代码的安全性与功能正确性，在CWEBench和CWEval两个基准上均优于现有方法如SafeCoder。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在自动生成代码时常常生成不安全的代码，而现有提升代码安全性的方法受限于数据集规模小且不平衡，导致效果和泛化能力有限。

Method: 提出Secure-Instruct框架，自动合成高质量的漏洞与安全代码对，生成微调指令，并对大语言模型进行指令微调，以对齐任务描述与安全代码生成能力。

Result: 在CWEBench和CWEval两个基准上，Secure-Instruct显著提升了多个LLM的安全代码生成能力，安全率平均提升14.3%，并优于SafeCoder 7.6%至15.8%；同时提高了代码的功能正确性。

Conclusion: Secure-Instruct有效提升了大语言模型生成代码的安全性与功能性，是一种优于现有方法的可行方案。

Abstract: Although Large Language Models (LLMs) show promising solutions to automated
code generation, they often produce insecure code that threatens software
security. Current approaches (e.g., SafeCoder) to improve secure code
generation suffer from limited and imbalanced datasets, reducing their
effectiveness and generalizability. In this work, we present Secure-Instruct, a
novel framework that automatically synthesizes high-quality vulnerable and
secure code examples, generates fine-tuning instructions, and instruction-tunes
LLMs to align task description and secure code generation abilities. We
evaluate Secure-Instruct on four representative LLMs using two benchmarks: our
own CWEBench and the existing CWEval. CWEBench comprises 93 scenarios on 44
CWEs, all without overlap with Secure-Instruct's synthetic instruction-tuning
dataset, while CWEval covers 31 CWEs with 119 manually verified
security-critical tasks. We find that Secure-Instruct improves not only the
security but also the functional correctness of the generated code. On
CWEBench, Secure-Instruct substantially improves secure code generation, giving
a 14.3% average increase in secure ratio over the pretrained models and
outperforms SafeCoder by 7.6%. On CWEval, Secure-Instruct achieves a 14%
increase for CodeLlama-7B and 5.8% for Mistral-7B in Func-Sec@1 over pretrained
models, and surpasses SafeCoder by 15.8% and 6.8% respectively.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [21] [DiLi: A Lock-Free Asynchronously Distributable Linked List](https://arxiv.org/abs/2510.06387)
*Raaghav Ravishankar,Sandeep Kulkarni,Sathya Peri,Gokarna Sharma*

Main category: cs.DC

TL;DR: 本文提出了DiLi，一种条件无锁、线性化且可分布的链表结构，支持动态分区与负载均衡，能在单机上媲美现有无锁并发结构，并在多机环境下实现线性吞吐扩展。


<details>
  <summary>Details</summary>
Motivation: 现代数据库需处理高吞吐需求，但受限于单机硬件容量，静态分区会导致负载不均且难以调整。因此需要一种支持动态、无锁、可分布的数据结构。

Method: 引入“条件无锁”概念，设计名为DiLi的可分布链表，支持异步动态分区与跨机器负载均衡，并通过新型遍历策略（结合二分查找与有限线性遍历）实现高效查找。

Result: 实验表明，DiLi在单机性能上与最先进的无锁并发链表结构相当，且在多机部署下吞吐量随机器数量线性扩展。

Conclusion: DiLi提供了一种实用且高效的分布式无锁数据结构解决方案，兼顾单机性能与多机可扩展性，适用于高吞吐数据库系统。

Abstract: Modern databases use dynamic search structures that store a huge amount of
data, and often serve them using multi-threaded algorithms to support the
ever-increasing throughput needs. When this throughput need exceeds the
capacity of the machine hosting the structure, one either needs to replace the
underlying hardware (an option that is typically not viable and introduces a
long down time) or make the data structure distributed. Static partitioning of
the data structure for distribution is not desirable, as it is prone to uneven
load distribution over time, and having to change the partitioning scheme later
will require downtime.
  Since a distributed data structure, inherently, relies on communication
support from the network stack and operating systems, we introduce the notion
of conditional lock-freedom that extends the notion of lock-free computation
with reasonable assumptions about communication between processes. We present
DiLi, a conditional lock-free, linearizable, and distributable linked list that
can be asynchronously and dynamically (1) partitioned into multiple sublists
and (2) load balanced by distributing sublists across multiple machines. DiLi
contains primitives for these that also maintain the lock-free property of the
underlying search structure that supports find, remove, and insert of a key as
the client operations.
  Searching for an item in DiLi is by a novel traversal that involves a binary
search on the partitioning scheme, and then a linear traversal on a limitable
number of linked nodes. As a result, we are able to empirically show that DiLi
performs as well as the state-of-the-art lock-free concurrent search structures
that are based off of a linked list when executed on a single-machine. We also
show that the throughput of DiLi scales linearly with the number of machines
that host it.

</details>


### [22] [Adaptive Protein Design Protocols and Middleware](https://arxiv.org/abs/2510.06396)
*Aymen Alsaadi,Jonathan Ash,Mikhail Titov,Matteo Turilli,Andre Merzky,Shantenu Jha,Sagar Khare*

Main category: cs.DC

TL;DR: 本文介绍了IMPRESS框架，通过将AI与高性能计算结合，实现自适应蛋白质设计协议和动态资源分配，从而提升蛋白质设计的一致性与吞吐量。


<details>
  <summary>Details</summary>
Motivation: 蛋白质序列和结构空间极其庞大，传统方法在生成与预测结构一致性方面面临巨大计算挑战，亟需高效整合AI与高性能计算资源。

Method: 提出IMPRESS系统，结合AI/ML与高性能计算，开发自适应蛋白质设计协议，并通过动态资源分配与异步任务执行优化计算流程。

Result: 实现了更一致的蛋白质设计质量与更高的设计吞吐量，验证了该框架在评估设计、模型和模拟方面的有效性。

Conclusion: IMPRESS为大规模蛋白质设计提供了一种高效、可扩展的计算范式，显著提升了AI驱动蛋白质设计的实用性与效率。

Abstract: Computational protein design is experiencing a transformation driven by
AI/ML. However, the range of potential protein sequences and structures is
astronomically vast, even for moderately sized proteins. Hence, achieving
convergence between generated and predicted structures demands substantial
computational resources for sampling. The Integrated Machine-learning for
Protein Structures at Scale (IMPRESS) offers methods and advanced computing
systems for coupling AI to high-performance computing tasks, enabling the
ability to evaluate the effectiveness of protein designs as they are developed,
as well as the models and simulations used to generate data and train models.
This paper introduces IMPRESS and demonstrates the development and
implementation of an adaptive protein design protocol and its supporting
computing infrastructure. This leads to increased consistency in the quality of
protein design and enhanced throughput of protein design due to dynamic
resource allocation and asynchronous workload execution.

</details>


### [23] [Multi-Dimensional Autoscaling of Stream Processing Services on Edge Devices](https://arxiv.org/abs/2510.06882)
*Boris Sedlak,Philipp Raith,Andrea Morichetta,Víctor Casamayor Pujol,Schahram Dustdar*

Main category: cs.DC

TL;DR: 本文提出了一个多维自动扩缩平台MUDAP，结合基于结构知识回归分析（RASK）的扩缩代理，在资源受限的边缘设备上通过服务级和资源级的细粒度垂直扩缩，有效减少服务等级目标（SLO）违规。


<details>
  <summary>Details</summary>
Motivation: 边缘设备资源有限，传统仅关注资源扩缩的自动扩缩机制难以满足多个流处理服务的SLO需求，亟需支持多维度、细粒度的扩缩策略。

Method: 提出MUDAP平台，支持服务特定的多维垂直扩缩（如数据质量、模型大小等），并设计基于RASK的扩缩代理，通过回归模型学习处理环境，推断最优扩缩动作。

Result: 在单个边缘设备上对多达9个服务进行实验，RASK仅需20次迭代（200秒）即可构建准确回归模型；相比Kubernetes VPA和强化学习基线，RASK在增加弹性维度后减少28%的SLO违规。

Conclusion: MUDAP与RASK代理能有效在资源受限的边缘环境中实现多维自动扩缩，显著提升SLO满足率，优于现有自动扩缩方法。

Abstract: Edge devices have limited resources, which inevitably leads to situations
where stream processing services cannot satisfy their needs. While existing
autoscaling mechanisms focus entirely on resource scaling, Edge devices require
alternative ways to sustain the Service Level Objectives (SLOs) of competing
services. To address these issues, we introduce a Multi-dimensional Autoscaling
Platform (MUDAP) that supports fine-grained vertical scaling across both
service- and resource-level dimensions. MUDAP supports service-specific scaling
tailored to available parameters, e.g., scale data quality or model size for a
particular service. To optimize the execution across services, we present a
scaling agent based on Regression Analysis of Structural Knowledge (RASK). The
RASK agent efficiently explores the solution space and learns a continuous
regression model of the processing environment for inferring optimal scaling
actions. We compared our approach with two autoscalers, the Kubernetes VPA and
a reinforcement learning agent, for scaling up to 9 services on a single Edge
device. Our results showed that RASK can infer an accurate regression model in
merely 20 iterations (i.e., observe 200s of processing). By increasingly adding
elasticity dimensions, RASK sustained the highest request load with 28% less
SLO violations, compared to baselines.

</details>


### [24] [MuFASA -- Asynchronous Checkpoint for Weakly Consistent Fully Replicated Databases](https://arxiv.org/abs/2510.06404)
*Raaghav Ravishankar,Sandeep Kulkarni,Nitin H Vaidya*

Main category: cs.DC

TL;DR: 本文研究了在完全复制的弱一致性分布式数据库中进行检查点的问题（称为DTCS），提出了一种开销最小（仅需O(n)条新消息和在现有消息中增加一个计数器）且能生成强一致性快照的检查点算法，以应对弱一致性系统中的异常问题。


<details>
  <summary>Details</summary>
Motivation: 弱一致性系统（如主内存数据库）常产生用户难以预料的异常，频繁检查点有助于验证所需不变性；但传统检查点方法要么开销大，要么导致不一致。

Method: 提出了面向完全复制数据库的“规模最小检查点”概念，并设计了一种仅需O(n)新消息并在现有消息中添加单个计数器的低开销检查点算法（DTCS）。

Result: 该算法显著优于现有分布式系统和主内存数据库中的检查点方法，能以极低开销生成强一致快照序列。

Conclusion: DTCS通过生成强一致快照序列，使用户能在弱一致性系统出现异常时，仅关注异常时间点附近的快照，从而有效定位和分析问题。

Abstract: We focus on the problem of checkpointing in fully replicated weakly
consistent distributed databases, which we refer to as Distributed Transaction
Consistent Snapshot (DTCS). A typical example of such a system is a main-memory
database that provides strong eventual consistency. This problem is important
and challenging for several reasons: (1) eventual consistency often creates
anomalies that the users do not anticipate. Hence, frequent checkpoints to
ascertain desired invariants is highly beneficial in their use, and (2)
traditional checkpoints lead to significant overhead and/or inconsistencies. By
showing that the traditional checkpoint leads to inconsistencies or excessive
overhead, we define the notion of size-minimal checkpointing for fully
replicated databases. We present an algorithm for checkpointing with minimal
checkpointing overhead (only O(n) new messages and addition of a single counter
for existing messages). It also provides a significant benefit over existing
checkpointing algorithms for distributed systems and main-memory databases.
  A key benefit of DTCS is that it summarizes the computation by a sequence of
snapshots that are strongly consistent even though the underlying computation
is weakly consistent. In essence, when anomalies arise in an eventually
consistent system, DTCS enables one to concentrate solely on the snapshots
surrounding the time point of the anomaly.

</details>


### [25] [GROMACS Unplugged: How Power Capping and Frequency Shapes Performance on GPUs](https://arxiv.org/abs/2510.06902)
*Ayesha Afzal,Anna Kahler,Georg Hager,Gerhard Wellein*

Main category: cs.DC

TL;DR: 本文对四款NVIDIA GPU（A40、A100、L4、L40）在GROMACS分子动力学模拟中的性能进行了全面分析，结合计算密集型和内存密集型基准测试，揭示了不同规模系统在频率缩放和功耗限制下的性能表现，为硬件选型和性能优化提供指导。


<details>
  <summary>Details</summary>
Motivation: 分子动力学模拟的性能高度依赖于硬件选择与配置，但缺乏针对最新GPU在GROMACS中系统性的性能评估，尤其是在频率调节和功耗限制下的行为。

Method: 使用六种代表性GROMACS生物分子工作负载及两个合成基准（Pi Solver和STREAM Triad），在四款NVIDIA GPU上测试性能，分析图形时钟频率缩放效应及功耗限制对性能的影响。

Result: 小规模GROMACS系统对频率敏感，而大规模系统迅速饱和并受内存带宽限制；在功耗限制下，高端GPU（如A100）在达到特定阈值前能维持接近峰值性能。

Conclusion: 研究为在功耗约束下选择合适GPU硬件和优化大规模分子动力学工作流的GROMACS性能提供了实用依据。

Abstract: Molecular dynamics simulations are essential tools in computational
biophysics, but their performance depend heavily on hardware choices and
configuration. In this work, we presents a comprehensive performance analysis
of four NVIDIA GPU accelerators -- A40, A100, L4, and L40 -- using six
representative GROMACS biomolecular workloads alongside two synthetic
benchmarks: Pi Solver (compute bound) and STREAM Triad (memory bound). We
investigate how performance scales with GPU graphics clock frequency and how
workloads respond to power capping. The two synthetic benchmarks define the
extremes of frequency scaling: Pi Solver shows ideal compute scalability, while
STREAM Triad reveals memory bandwidth limits -- framing GROMACS's performance
in context. Our results reveal distinct frequency scaling behaviors: Smaller
GROMACS systems exhibit strong frequency sensitivity, while larger systems
saturate quickly, becoming increasingly memory bound. Under power capping,
performance remains stable until architecture- and workload-specific thresholds
are reached, with high-end GPUs like the A100 maintaining near-maximum
performance even under reduced power budgets. Our findings provide practical
guidance for selecting GPU hardware and optimizing GROMACS performance for
large-scale MD workflows under power constraints.

</details>


### [26] [REACH: Reinforcement Learning for Adaptive Microservice Rescheduling in the Cloud-Edge Continuum](https://arxiv.org/abs/2510.06675)
*Xu Bai,Muhammed Tawfiqul Islam,Rajkumar Buyya,Adel N. Toosi*

Main category: cs.DC

TL;DR: 本文提出了一种基于强化学习的微服务动态重调度算法REACH，用于在云边连续体中优化微服务部署，以应对资源异构性和动态性带来的挑战，从而降低端到端延迟并减少延迟波动。


<details>
  <summary>Details</summary>
Motivation: 云边连续体虽结合了边缘计算的响应性和云计算的可扩展性，但其异构且动态的资源环境给微服务的最优部署带来了挑战，尤其是在满足低延迟需求方面。

Method: 提出REACH算法，利用强化学习在真实环境中动态调整微服务部署，以实时响应资源可用性和性能变化。

Result: 在真实测试平台上，REACH在三个基准微服务应用中分别将平均端到端延迟降低了7.9%、10%和8%，并有效缓解了延迟波动和突发延迟。

Conclusion: REACH算法能有效提升云边连续体中微服务部署的实时适应性与性能稳定性，满足延迟敏感型应用的需求。

Abstract: Cloud computing, despite its advantages in scalability, may not always fully
satisfy the low-latency demands of emerging latency-sensitive pervasive
applications. The cloud-edge continuum addresses this by integrating the
responsiveness of edge resources with cloud scalability. Microservice
Architecture (MSA) characterized by modular, loosely coupled services, aligns
effectively with this continuum. However, the heterogeneous and dynamic
computing resource poses significant challenges to the optimal placement of
microservices. We propose REACH, a novel rescheduling algorithm that
dynamically adapts microservice placement in real time using reinforcement
learning to react to fluctuating resource availability, and performance
variations across distributed infrastructures. Extensive experiments on a
real-world testbed demonstrate that REACH reduces average end-to-end latency by
7.9%, 10%, and 8% across three benchmark MSA applications, while effectively
mitigating latency fluctuations and spikes.

</details>


### [27] [Evaluating Rapid Makespan Predictions for Heterogeneous Systems with Programmable Logic](https://arxiv.org/abs/2510.06998)
*Martin Wilhelm,Franz Freitag,Max Tzschoppe,Thilo Pionteck*

Main category: cs.DC

TL;DR: 本文提出了一个灵活的评估框架，用于在包含CPU、GPU和FPGA的异构系统中，基于抽象任务图快速收集真实执行时间（makespan），并评估现有解析方法对makespan的预测能力，同时分析数据传输开销和设备拥塞等高层挑战。


<details>
  <summary>Details</summary>
Motivation: 在异构计算系统中，任务映射对整体执行时间影响显著，但准确预测映射变更对makespan的影响十分困难。现有模拟器需完整实现任务（尤其对可编程逻辑耗时），而纯解析方法虽快但过于理想化。因此，亟需一个能弥合理论与实践差距的评估工具以支持快速makespan预测算法的开发。

Method: 构建一个高度灵活的评估框架，支持基于抽象任务图在CPU、GPU和FPGA组成的异构系统中收集真实makespan数据，并利用该框架评估现有解析预测方法的准确性。

Result: 通过该框架分析了现有解析方法在预测实际makespan方面的有效性，并揭示了异构系统中由数据传输开销和设备拥塞等高层特性带来的常见挑战。

Conclusion: 所提出的评估框架有助于开发更准确的快速makespan预测算法，并强调了在异构系统任务映射中必须考虑数据传输和设备拥塞等现实因素。

Abstract: Heterogeneous computing systems, which combine general-purpose processors
with specialized accelerators, are increasingly important for optimizing the
performance of modern applications. A central challenge is to decide which
parts of an application should be executed on which accelerator or, more
generally, how to map the tasks of an application to available devices.
Predicting the impact of a change in a task mapping on the overall makespan is
non-trivial. While there are very capable simulators, these generally require a
full implementation of the tasks in question, which is particularly
time-intensive for programmable logic. A promising alternative is to use a
purely analytical function, which allows for very fast predictions, but
abstracts significantly from reality. Bridging the gap between theory and
practice poses a significant challenge to algorithm developers. This paper aims
to aid in the development of rapid makespan prediction algorithms by providing
a highly flexible evaluation framework for heterogeneous systems consisting of
CPUs, GPUs and FPGAs, which is capable of collecting real-world makespan
results based on abstract task graph descriptions. We analyze to what extent
actual makespans can be predicted by existing analytical approaches.
Furthermore, we present common challenges that arise from high-level
characteristics such as data transfer overhead and device congestion in
heterogeneous systems.

</details>
