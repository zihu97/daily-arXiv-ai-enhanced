<div id=toc></div>

# Table of Contents

- [cs.PF](#cs.PF) [Total: 1]
- [cs.MA](#cs.MA) [Total: 5]
- [cs.OS](#cs.OS) [Total: 1]
- [cs.SE](#cs.SE) [Total: 28]
- [cs.NI](#cs.NI) [Total: 8]
- [cs.DC](#cs.DC) [Total: 8]


<div id='cs.PF'></div>

# cs.PF [[Back]](#toc)

### [1] [WritePolicyBench: Benchmarking Memory Write Policies under Byte Budgets](https://arxiv.org/abs/2602.02574)
*Edgard El Cham*

Main category: cs.PF

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: We introduce WritePolicyBench, a benchmark for evaluating memory write policies: decision rules that choose what to store, merge, and evict under a strict byte budget while processing a stream with document/API drift. The benchmark provides (i) task generators with controlled non-stationarity, (ii) an explicit action interface for external memory, (iii) a byte-accurate cost model, and (iv) standardized metrics that measure both task success and budget efficiency.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [2] [Exploring Silicon-Based Societies: An Early Study of the Moltbook Agent Community](https://arxiv.org/abs/2602.02613)
*Yu-Zheng Lin,Bono Po-Jen Shih,Hsuan-Ying Alessandra Chien,Shalaka Satam,Jesus Horacio Pacheco,Sicong Shao,Soheil Salehi,Pratik Satam*

Main category: cs.MA

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: The rapid emergence of autonomous large language model agents has given rise to persistent, large-scale agent ecosystems whose collective behavior cannot be adequately understood through anecdotal observation or small-scale simulation. This paper introduces data-driven silicon sociology as a systematic empirical framework for studying social structure formation among interacting artificial agents. We present a pioneering large-scale data mining investigation of an in-the-wild agent society by analyzing Moltbook, a social platform designed primarily for agent-to-agent interaction. At the time of study, Moltbook hosted over 150,000 registered autonomous agents operating across thousands of agent-created sub-communities. Using programmatic and non-intrusive data acquisition, we collected and analyzed the textual descriptions of 12,758 submolts, which represent proactive sub-community partitioning activities within the ecosystem. Treating agent-authored descriptions as first-class observational artifacts, we apply rigorous preprocessing, contextual embedding, and unsupervised clustering techniques to uncover latent patterns of thematic organization and social space structuring. The results show that autonomous agents systematically organize collective space through reproducible patterns spanning human-mimetic interests, silicon-centric self-reflection, and early-stage economic and coordination behaviors. Rather than relying on predefined sociological taxonomies, these structures emerge directly from machine-generated data traces. This work establishes a methodological foundation for data-driven silicon sociology and demonstrates that data mining techniques can provide a powerful lens for understanding the organization and evolution of large autonomous agent societies.

</details>


### [3] [Scaling Small Agents Through Strategy Auctions](https://arxiv.org/abs/2602.02751)
*Lisa Alazraki,William F. Shen,Yoram Bachrach,Akhil Mathur*

Main category: cs.MA

TL;DR: 本文通过实证研究揭示小语言模型在复杂任务上表现不足，提出SALE框架利用策略拍卖实现高效任务分配，显著降低成本并提升性能。


<details>
  <summary>Details</summary>
Motivation: 探究小代理在复杂任务中的可扩展性，克服传统路由器效率低下的问题，为agent工作流提供低成本解决方案。

Method: 引入SALE框架，模拟自由职业者市场：代理竞标简短战略计划，通过成本 améric值机制打分，并借助共享拍卖内存集成反馈，实现任务路由和自我优化而无需训练额外路由器或需所有模型运行完成。

Result: 在搜索和编程任务上，SALE减少53%对大代理依赖、降低35% zelfs全球成本，性能超过最大模型的pass@1指标，且开销可忽略不计；传统路由器则无法有效平衡成本与性能。

Conclusion: 小代理可通过任务分配和自我优化扩展能力，强调以市场机制协调异构代理的系统级AI视角，性能提升源自智能协调而非一味扩大模型规模。

Abstract: Small language models are increasingly viewed as a promising, cost-effective approach to agentic AI, with proponents claiming they are sufficiently capable for agentic workflows. However, while smaller agents can closely match larger ones on simple tasks, it remains unclear how their performance scales with task complexity, when large models become necessary, and how to better leverage small agents for long-horizon workloads. In this work, we empirically show that small agents' performance fails to scale with task complexity on deep search and coding tasks, and we introduce Strategy Auctions for Workload Efficiency (SALE), an agent framework inspired by freelancer marketplaces. In SALE, agents bid with short strategic plans, which are scored by a systematic cost-value mechanism and refined via a shared auction memory, enabling per-task routing and continual self-improvement without training a separate router or running all models to completion. Across deep search and coding tasks of varying complexity, SALE reduces reliance on the largest agent by 53%, lowers overall cost by 35%, and consistently improves upon the largest agent's pass@1 with only a negligible overhead beyond executing the final trace. In contrast, established routers that rely on task descriptions either underperform the largest agent or fail to reduce cost -- often both -- underscoring their poor fit for agentic workflows. These results suggest that while small agents may be insufficient for complex workloads, they can be effectively "scaled up" through coordinated task allocation and test-time self-improvement. More broadly, they motivate a systems-level view of agentic AI in which performance gains come less from ever-larger individual models and more from market-inspired coordination mechanisms that organize heterogeneous agents into efficient, adaptive ecosystems.

</details>


### [4] [Game-Theoretic and Algorithmic Analyses of Multi-Agent Routing under Crossing Costs](https://arxiv.org/abs/2602.03455)
*Tesshu Hanaka,Nikolaos Melissinos,Hirotaka Ono*

Main category: cs.MA

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Coordinating the movement of multiple autonomous agents over a shared network is a fundamental challenge in algorithmic robotics, intelligent transportation, and distributed systems. The dominant approach, Multi-Agent Path Finding, relies on centralized control and synchronous collision avoidance, which often requires strict synchronization and guarantees of globally conflict-free execution. This paper introduces the Multi-Agent Routing under Crossing Cost model on mixed graphs, a novel framework tailored to asynchronous settings. In our model, instead of treating conflicts as hard constraints, each agent is assigned a path, and the system is evaluated through a cost function that measures potential head-on encounters. This ``crossing cost'', which is defined as the product of the numbers of agents traversing an edge in opposite directions, quantifies the risk of congestion and delay in decentralized execution.
  Our contributions are both game-theoretic and algorithmic. We model the setting as a congestion game with a non-standard cost function, prove the existence of pure Nash equilibria, and analyze the dynamics leading to them. Equilibria can be found in polynomial time under mild conditions, while the general case is PLS-complete. From an optimization perspective, minimizing the total crossing cost is NP-hard, as the problem generalizes Steiner Orientation. To address this hardness barrier, we design a suite of parameterized algorithms for minimizing crossing cost, with parameters including the number of arcs, edges, agents, and structural graph measures. These yield XP or FPT results depending on the parameter, offering algorithmic strategies for structurally restricted instances. Our framework provides a new theoretical foundation for decentralized multi-agent routing, bridging equilibrium analysis and parameterized complexity to support scalable and risk-aware coordination.

</details>


### [5] [When Should Agents Coordinate in Differentiable Sequential Decision Problems?](https://arxiv.org/abs/2602.03674)
*Caleb Probine,Su Ann Low,David Fridovich-Keil,Ufuk Topcu*

Main category: cs.MA

TL;DR: 本论文研究多机器人团队在微分运动规划中的协调问题，分析其价值并基于二阶性质开发算法以确定协调时机。


<details>
  <summary>Details</summary>
Motivation: 协调对团队效能至关重要，但通信成本高昂；因此探索协调在可微分运动规划中的价值，解决个体最优导致团队次优的困境。

Method: 将协调建模为谱系：一端联合优化团队目标，另一端为纳什均衡的个体决策；证明协调归约为目标函数的二阶性质分析，并提供算法。

Result: 算法通过二阶推理成功确定团队成员协调的最佳时间点，优化团队表现而不需高频通信。

Conclusion: 在微分运动规划中，协调时机由二阶性质决定，可实现高效团队协作并降低通信成本。

Abstract: Multi-robot teams must coordinate to operate effectively. When a team operates in an uncoordinated manner, and agents choose actions that are only individually optimal, the team's outcome can suffer. However, in many domains, coordination requires costly communication. We explore the value of coordination in a broad class of differentiable motion-planning problems. In particular, we model coordinated behavior as a spectrum: at one extreme, agents jointly optimize a common team objective, and at the other, agents make unilaterally optimal decisions given their individual decision variables, i.e., they operate at Nash equilibria. We then demonstrate that reasoning about coordination in differentiable motion-planning problems reduces to reasoning about the second-order properties of agents' objectives, and we provide algorithms that use this second-order reasoning to determine at which times a team of agents should coordinate.

</details>


### [6] [Agent Primitives: Reusable Latent Building Blocks for Multi-Agent Systems](https://arxiv.org/abs/2602.03695)
*Haibo Jin,Kuang Peng,Ye Yu,Xiaopeng Yuan,Haohan Wang*

Main category: cs.MA

TL;DR: 提出Agent Primitives作为可重用潜构建块，优化基于LLM的多智能体系统性能、效率与稳定性。


<details>
  <summary>Details</summary>
Motivation: 现有MAS任务特定化，依赖手动设计角色与交互提示，导致架构复杂、重用性差；且自然语言通信易在多阶段交互中引发错误累积和不稳定。

Method: 借鉴神经网络设计，实例化Review、Voting and Selection、Planning and Execution三个原语，内部通过KV缓存通信；Organizer代理基于知识库自动选择组合系统。

Result: 实验表明：相比单智能体基线，平均准确率提升12.0-16.5%；相比文本MAS，令牌使用与延迟降低3×-4×，开销仅为单智能体的1.3×-1.6×，性能更稳定。

Conclusion: Agent Primitives显著提升MAS效能，为高效、鲁棒的多智能体架构自动化构建提供新路径。

Abstract: While existing multi-agent systems (MAS) can handle complex problems by enabling collaboration among multiple agents, they are often highly task-specific, relying on manually crafted agent roles and interaction prompts, which leads to increased architectural complexity and limited reusability across tasks. Moreover, most MAS communicate primarily through natural language, making them vulnerable to error accumulation and instability in long-context, multi-stage interactions within internal agent histories.
  In this work, we propose \textbf{Agent Primitives}, a set of reusable latent building blocks for LLM-based MAS. Inspired by neural network design, where complex models are built from reusable components, we observe that many existing MAS architectures can be decomposed into a small number of recurring internal computation patterns. Based on this observation, we instantiate three primitives: Review, Voting and Selection, and Planning and Execution. All primitives communicate internally via key-value (KV) cache, which improves both robustness and efficiency by mitigating information degradation across multi-stage interactions. To enable automatic system construction, an Organizer agent selects and composes primitives for each query, guided by a lightweight knowledge pool of previously successful configurations, forming a primitive-based MAS.
  Experiments show that primitives-based MAS improve average accuracy by 12.0-16.5\% over single-agent baselines, reduce token usage and inference latency by approximately 3$\times$-4$\times$ compared to text-based MAS, while incurring only 1.3$\times$-1.6$\times$ overhead relative to single-agent inference and providing more stable performance across model backbones.

</details>


<div id='cs.OS'></div>

# cs.OS [[Back]](#toc)

### [7] [ProphetKV: User-Query-Driven Selective Recomputation for Efficient KV Cache Reuse in Retrieval-Augmented Generation](https://arxiv.org/abs/2602.02579)
*Shihao Wang,Jiahao Chen,Yanqi Pan,Hao Huang,Yichen Hao,Xiangyu Zou,Wen Xia,Wentao Zhang,Haitao Wang,Junhong Li,Chongyang Qiu,Pengfei Wang*

Main category: cs.OS

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: The prefill stage of long-context Retrieval-Augmented Generation (RAG) is severely bottlenecked by computational overhead. To mitigate this, recent methods assemble pre-calculated KV caches of retrieved RAG documents (by a user query) and reprocess selected tokens to recover cross-attention between these pre-calculated KV caches. However, we identify a fundamental "crowding-out effect" in current token selection criteria: globally salient but user-query-irrelevant tokens saturate the limited recomputation budget, displacing the tokens truly essential for answering the user query and degrading inference accuracy.
  We propose ProphetKV, a user-query-driven KV Cache reuse method for RAG scenarios. ProphetKV dynamically prioritizes tokens based on their semantic relevance to the user query and employs a dual-stage recomputation pipeline to fuse layer-wise attention metrics into a high-utility set. By ensuring the recomputation budget is dedicated to bridging the informational gap between retrieved context and the user query, ProphetKV achieves high-fidelity attention recovery with minimal overhead. Our extensive evaluation results show that ProphetKV retains 96%-101% of full-prefill accuracy with only a 20% recomputation ratio, while achieving accuracy improvements of 8.8%-24.9% on RULER and 18.6%-50.9% on LongBench over the state-of-the-art approaches (e.g., CacheBlend, EPIC, and KVShare).

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [8] [Constitutional Spec-Driven Development: Enforcing Security by Construction in AI-Assisted Code Generation](https://arxiv.org/abs/2602.02584)
*Srinivas Rao Marri*

Main category: cs.SE

TL;DR: 本文提出宪法规约驱动开发方法，通过规范层嵌入不可协商的安全原则，解决AI生成代码对安全的忽视问题，并在银行应用中验证效果


<details>
  <summary>Details</summary>
Motivation: AI辅助开发虽提升效率，但大型语言模型优先考虑功能正确性而忽视安全，导致严重安全风险

Method: 建立版本化机器可读的'宪法'文档，编码CWE/MITRE漏洞及监管约束，应用于银行微服务案例（客户管理/账户操作/交易处理），实现领域无关的安全约束植入

Result: 在银行业务场景中覆盖10项CWE关键漏洞，实现安全原则到代码位置的全链路追踪，实验表明较无约束AI生成减少73%安全缺陷，且不影响开发效率

Conclusion: 贡献了宪法安全框架和开发方法论，实证证明AI工作流中主动安全规范优于被动验证，为安全关键系统提供可靠解决方案

Abstract: The proliferation of AI-assisted "vibe coding" enables rapid software development but introduces significant security risks, as Large Language Models (LLMs) prioritize functional correctness over security. We present Constitutional Spec-Driven Development, a methodology that embeds non-negotiable security principles into the specification layer, ensuring AI-generated code adheres to security requirements by construction rather than inspection. Our approach introduces a Constitution: a versioned, machine-readable document encoding security constraints derived from Common Weakness Enumeration (CWE)/MITRE Top 25 vulnerabilities and regulatory frameworks. We demonstrate the methodology through a banking microservices application, selected as a representative example domain due to its stringent regulatory and security requirements, implementing customer management, account operations, and transaction processing. The methodology itself is domain-agnostic. The implementation addresses 10 critical CWE vulnerabilities through constitutional constraints with full traceability from principles to code locations. Our case study shows that constitutional constraints reduce security defects by 73% compared to unconstrained AI generation while maintaining developer velocity. We contribute a formal framework for constitutional security, a complete development methodology, and empirical evidence that proactive security specification outperforms reactive security verification in AI-assisted development workflows.

</details>


### [9] [Agentic Observability: Automated Alert Triage for Adobe E-Commerce](https://arxiv.org/abs/2602.02585)
*Aprameya Bharadwaj,Kyle Tu*

Main category: cs.SE

TL;DR: 本文提出了一种基于ReAct范式的自主代理框架，用于自动化告警处理以降低企业系统的事件响应时间。


<details>
  <summary>Details</summary>
Motivation: 企业系统útÿú相互依赖性导致手动告警处理（日志检查、API验证等）成为平均恢复时间的瓶颈，亟需改进可观测性。

Method: 部署在Adobe电子商务基础设施中的代理框架，动态识别受影响服务、跨分布式系统检索分析日志，并自主执行如手册咨询或运行簿执行等行动。

Result: 生产部署实证显示，与手动处理相比，洞察平均时间减少90%，诊断准确性相当，且诊断延迟降低一个数量级、分辨率显著提升。

Conclusion: 代理式AI实现了处理延迟的大幅减少和准确性跃升，标志着企业运营向自主可观测性的关键转变。

Abstract: Modern enterprise systems exhibit complex interdependencies that make observability and incident response increasingly challenging. Manual alert triage, which typically involves log inspection, API verification, and cross-referencing operational knowledge bases, remains a major bottleneck in reducing mean recovery time (MTTR). This paper presents an agentic observability framework deployed within Adobe's e-commerce infrastructure that autonomously performs alert triage using a ReAct paradigm. Upon alert detection, the agent dynamically identifies the affected service, retrieves and analyzes correlated logs across distributed systems, and plans context-dependent actions such as handbook consultation, runbook execution, or retrieval-augmented analysis of recently deployed code. Empirical results from production deployment indicate a 90% reduction in mean time to insight compared to manual triage, while maintaining comparable diagnostic accuracy. Our results show that agentic AI enables an order-of-magnitude reduction in triage latency and a step-change in resolution accuracy, marking a pivotal shift toward autonomous observability in enterprise operations.

</details>


### [10] [Testing Storage-System Correctness: Challenges, Fuzzing Limitations, and AI-Augmented Opportunities](https://arxiv.org/abs/2602.02614)
*Ying Wang,Jiahui Chen,Dejun Jiang*

Main category: cs.SE

TL;DR: 综述论文系统总结存储系统正确性测试的挑战，分析现有方法局限性如fuzzing测试范式，并探讨AI增强的潜力。


<details>
  <summary>Details</summary>
Motivation: 动机源于存储系统正确性保障固有困难，受非确定性交叠、长期状态演化和跨层语义等内在属性影响，亟需创新测试方法。

Method: 方法采用存储中心视角组织技术框架，评审并发测试、长运行工作负载、崩溃一致性校验等多类方法，分析fuzzing假设与系统语义的不匹配，并讨论AI通过状态感知和语义指导补强fuzzing。

Result: 结果揭示现有方法的优缺点，突显AI在弥补语义差距和优化测试效能方面的潜力，系统化梳理故障机制和技术范围。

Conclusion: 结论是提供统一测试视角，指出现实挑战如跨层语义验证和长时状态管理，为未来研究指明方向。

Abstract: Storage systems are fundamental to modern computing infrastructures, yet ensuring their correctness remains challenging in practice. Despite decades of research on system testing, many storage-system failures (including durability, ordering, recovery, and consistency violations) remain difficult to expose systematically. This difficulty stems not primarily from insufficient testing tooling, but from intrinsic properties of storage-system execution, including nondeterministic interleavings, long-horizon state evolution, and correctness semantics that span multiple layers and execution phases.
  This survey adopts a storage-centric view of system testing and organizes existing techniques according to the execution properties and failure mechanisms they target. We review a broad spectrum of approaches, ranging from concurrency testing and long-running workloads to crash-consistency analysis, hardware-level semantic validation, and distributed fault injection, and analyze their fundamental strengths and limitations. Within this framework, we examine fuzzing as an automated testing paradigm, highlighting systematic mismatches between conventional fuzzing assumptions and storage-system semantics, and discuss how recent artificial intelligence advances may complement fuzzing through state-aware and semantic guidance. Overall, this survey provides a unified perspective on storage-system correctness testing and outlines key challenges

</details>


### [11] [Outrunning LLM Cutoffs: A Live Kernel Crash Resolution Benchmark for All](https://arxiv.org/abs/2602.02690)
*Chenxi Huang,Alex Mathai,Feiyang Yu,Aleksandr Nogikh,Petros Maniatis,Franjo Ivančić,Eugene Wu,Kostis Kaffes,Junfeng Yang,Baishakhi Ray*

Main category: cs.SE

TL;DR: 提出Live-kBench和kEnv框架，解决LLM修复Linux内核崩溃评估中的静态基准和数据污染问题。


<details>
  <summary>Details</summary>
Motivation: 现有LLM代理评估存在静态基准无法适应内核演进、LLM知识过时导致数据污染的问题，需动态评测工具。

Method: 开发自进化基准框架Live-kBench持续收集新漏洞，创建标准化环境kEnv分离代理流程实现公平测评；使用534个内核错误数据集验证。

Result: LLM知识截止后的漏洞修复率下降25%；代理初试修复成功率74%，仅20%补丁与开发者方案一致；反馈机制提升29%修复率。

Conclusion: Live-kBench提供时效敏感יך评测基础设施，配套公开仪表盘实现社区级内核漏洞修复追踪，支撑跨代理公平对比。

Abstract: Repairing system crashes discovered by kernel fuzzers like Syzkaller is a critical yet underexplored challenge in software engineering. While recent works have introduced Large Language Model (LLM) based agents for Linux kernel crash-resolution, their evaluation benchmarks are usually static and thus, do not capture the evolving nature of the Linux kernel, and suffer from potential data contamination due to LLM knowledge cutoffs. To address the above problem, we present (i) Live-kBench, an evaluation framework for self-evolving benchmarks that continuously scrapes and evaluates agents on freshly discovered kernel bugs, and (ii) kEnv, an agent-agnostic standardized crash-resolution environment for kernel compilation, execution, and feedback. This design decouples agent workflows from heavy-weight execution, enabling fair and scalable comparison across diverse agent frameworks under identical conditions.
  To this end, we curate an inaugural dataset of 534 Linux kernel bugs and empirically demonstrate a significant performance gap, with agents achieving up to 25% higher equivalent patch rate on bugs fixed before the LLM knowledge cutoff. Using kEnv, we benchmark three state-of-the-art agents, showing that they resolve 74% of crashes on the first attempt (plausible patches); however only ~20% of generated patches closely match developer fixes. Additionally, exposing crash resolution feedback improves crash resolution rate by 29%. Live-kBench provides the community with an evaluation infrastructure for self-evolving benchmarks that is both time and attribute sensitive; complete with a public dashboard to track agent progress on Linux kernel bugs.

</details>


### [12] [Beyond the Prompt: Assessing Domain Knowledge Strategies for High-Dimensional LLM Optimization in Software Engineering](https://arxiv.org/abs/2602.02752)
*Srinath Srinivasan,Tim Menzies*

Main category: cs.SE

TL;DR: 本研究比较了人类与人工智能在生成领域知识方面的策略，通过评估四种架构来确定结构化知识整合是否能提升大型语言模型在高维优化中的预热启动效果。


<details>
  <summary>Details</summary>
Motivation: 动机是解决大型语言模型在低维软件工程优化任务表现优异，但在高维问题上不足的根本认知差距，探索系统化整合领域知识的方法来弥补这一缺陷。

Method: 评估了四种基于MOOT数据集的架构：(1)人类循环领域知识提示(H-DKP)，采用异步专家反馈回路；(2)自适应多阶段提示(AMP)，实现序列约束识别和验证；(3)维度感知渐进细化(DAPR)，逐步在扩展特征子空间中优化；(4)混合知识模型方法(HKMA)，结合统计scouting与RAG增强提示。性能量化涉及切比雪夫距离最优解的指标和Scott-Knott聚类排名。

Result: 结果通过切比雪夫距离和Scott-Knott聚类量化评估了性能，但摘要中未提供具体数据细节。

Conclusion: 结论是系统化整合领域知识有望提升大型语言模型在高维优化任务中的能力，但对不同架构的有效性需进一步验证。

Abstract: Background/Context: Large Language Models (LLMs) demonstrate strong performance on low-dimensional software engineering optimization tasks ($\le$11 features) but consistently underperform on high-dimensional problems where Bayesian methods dominate. A fundamental gap exists in understanding how systematic integration of domain knowledge (whether from humans or automated reasoning) can bridge this divide.
  Objective/Aim: We compare human versus artificial intelligence strategies for generating domain knowledge. We systematically evaluate four distinct architectures to determine if structured knowledge integration enables LLMs to generate effective warm starts for high-dimensional optimization.
  Method: We evaluate four approaches on MOOT datasets stratified by dimensionality: (1) Human-in-the-Loop Domain Knowledge Prompting (H-DKP), utilizing asynchronous expert feedback loops; (2) Adaptive Multi-Stage Prompting (AMP), implementing sequential constraint identification and validation; (3) Dimension-Aware Progressive Refinement (DAPR), conducting optimization in progressively expanding feature subspaces; and (4) Hybrid Knowledge-Model Approach (HKMA), synthesizing statistical scouting (TPE) with RAG-enhanced prompting. Performance is quantified via Chebyshev distance to optimal solutions and ranked using Scott-Knott clustering against an established baseline for LLM generated warm starts.
  Note that all human studies conducted as part of this study will comply with the policies of our local Institutional Review Board.

</details>


### [13] [A Proxy Stakeholder Approach to Requirements Engineering for Inclusive Navigation](https://arxiv.org/abs/2602.02869)
*Wei Wang,Anuradha Madugalla,John Grundy,Paul McIntosh,Charmine E. J. Härtel*

Main category: cs.SE

TL;DR: 本研究将导航重新定义为社交分布式任务，探讨代理利益相关者如何支持认知障碍患者，并提出基于实证的设计建议。


<details>
  <summary>Details</summary>
Motivation: 认知障碍患者导航困难但主流技术忽视其需求，需更具包容性的方法支持日常独立生活。

Method: 采用混合方法研究，包括国际调查和三阶段访谈，分析代理利益相关者在支持导航中的实际策略。

Result: 发现的关键挑战和实践合成设计建议：强调可定制性、基于惯例的导航和多用户协调应用。

Conclusion: 代理利益相关者概念引入软件工程，促进包容性 Pu需求获取和导航技术设计，更好反映认知支持的现实复杂性。

Abstract: Wayfinding, or the ability to navigate one's surroundings, is crucial for independent living and requires a complex combination of cognitive abilities, environmental awareness, and technology to manage this successfully. Individuals with cognitive impairment (IwCI) often face significant challenges in learning and navigating their environment. Despite its importance, mainstream navigation technologies are rarely designed with their diverse needs in mind. This study reframes the search for places as a socially distributed task and emphasizes the role of proxy stakeholders, who act on behalf or in coordination with IwCI during navigation. Using a qualitatively led mixed-methods approach, which includes an international survey and a three-stage interview study, we examine the real-world strategies that proxy stakeholders employ to support daily navigation. The findings are synthesized into a set of empirically grounded design recommendations that emphasize customisability, collaborative use, and support for routine-based navigation. Our findings highlight key challenges and adaptive practices, which are synthesized into design recommendations that prioritize customisability, routine-based navigation, and multi-user coordination. By introducing the proxy stakeholder concept into the software engineering literature, we propose a more inclusive approach to requirements elicitation and offer practical guidance for designing navigation technologies that better reflect the complex realities of cognitive support.

</details>


### [14] [Learning-Infused Formal Reasoning: From Contract Synthesis to Artifact Reuse and Formal Semantics](https://arxiv.org/abs/2602.02881)
*Arshad Beg,Diarmuid O'Donoghue,Rosemary Monahan*

Main category: cs.SE

TL;DR: 该论文阐述人工智能与形式化方法交叉的长期研究议程，提出整合自动合同合成、语义工件重用和细化理论的下一代方法，以构建可累积知识的验证生态系统。


<details>
  <summary>Details</summary>
Motivation: 超越孤立的正确性证明，转向累积性 onderzoekท驱动范式，使规范、合同和证明能在系统间持续合成与转移，提高验证效率。

Method: 设计混合框架，结合大型语言模型和图基表示，实现可扩展的语义匹配和有原则的重用；学习组件提供语义指导确保抽象层次的一致性。

Result: 报告了正在进行的工作，并通过组合推理推动验证生态系统系统化演进，以加速未来保障过程。

Conclusion: 该愿景指向利用过往验证成果构建自演化系统，提升验证效率并加快系统可靠性保证。

Abstract: This vision paper articulates a long-term research agenda for formal methods at the intersection with artificial intelligence, outlining multiple conceptual and technical dimensions and reporting on our ongoing work toward realising this agenda. It advances a forward-looking perspective on the next generation of formal methods based on the integration of automated contract synthesis, semantic artifact reuse, and refinement-based theory. We argue that future verification systems must move beyond isolated correctness proofs toward a cumulative, knowledge-driven paradigm in which specifications, contracts, and proofs are continuously synthesised and transferred across systems. To support this shift, we outline a hybrid framework combining large language models with graph-based representations to enable scalable semantic matching and principled reuse of verification artifacts. Learning-based components provide semantic guidance across heterogeneous notations and abstraction levels, while symbolic matching ensures formal soundness. Grounded in compositional reasoning, this vision points toward verification ecosystems that evolve systematically, leveraging past verification efforts to accelerate future assurance.

</details>


### [15] [Failure-Aware Enhancements for Large Language Model (LLM) Code Generation: An Empirical Study on Decision Framework](https://arxiv.org/abs/2602.02896)
*Jianru Shen,Zedong Peng,Lucy Owen*

Main category: cs.SE

TL;DR: 本研究针对LLM生成代码的失败情况木材厂经验性比较增强策略，发现RAG在所有失败类型中效果最佳，并提出决策框架指导开发者。


<details>
  <summary>Details</summary>
Motivation: 开发者缺乏何时使用LLM增强方法（如自我批评、协作或RAG）的guidance，需实证数据填补空白。

Method: 实证测试25个GitHub项目木头坊渐进提示法和增强策略（自我批评、多模型协作、RAG），在6个代表项目中评估4种失败类型的影响。

Result: 渐进提示法达成96.9%平均任务完成率（显著优于直接提示的80.j5%，Cohen's d=1ähr63，p<0.001），但仍有8个项目未完成；RAG对所有失败类型改善最显著且高效，自我批评仅对逻辑错误有效但对服务集成无效（0%改善）。

Conclusion: 提出决策框架将失败模式映射至合适增强方法，为工业开发者提供数据驱动guidance，避免试错。

Abstract: Large language models (LLMs) show promise for automating software development by translating requirements into code. However, even advanced prompting workflows like progressive prompting often leave some requirements unmet. Although methods such as self-critique, multi-model collaboration, and retrieval-augmented generation (RAG) have been proposed to address these gaps, developers lack clear guidance on when to use each. In an empirical study of 25 GitHub projects, we found that progressive prompting achieves 96.9% average task completion, significantly outperforming direct prompting (80.5%, Cohen's d=1.63, p<0.001) but still leaving 8 projects incomplete. For 6 of the most representative projects, we evaluated each enhancement strategy across 4 failure types. Our results reveal that method effectiveness depends critically on failure characteristics: Self-Critique succeeds on code-reviewable logic errors but fails completely on external service integration (0% improvement), while RAG achieves highest completion across all failure types with superior efficiency. Based on these findings, we propose a decision framework that maps each failure pattern to the most suitable enhancement method, giving practitioners practical, data-driven guidance instead of trial-and-error.

</details>


### [16] [Beyond Blame: Rethinking SZZ with Knowledge Graph Search](https://arxiv.org/abs/2602.02934)
*Yu Shi,Hao Li,Bram Adams,Ahmed E. Hassan*

Main category: cs.SE

TL;DR: 提出AgenticSZZ方法，结合frequency知识图谱和LLM代理改进缺陷引入提交识别


<details>
  <summary>Details</summary>
Motivation: 现有SZZ方法过度依赖git blame，40% aku案例无法有效处理（28%需额外提交历史遍历 DataFrame14%无直接修改aCommit）

Method: 分两阶段：构建编码提交时序与结构关系的FrequencyKG以拓展搜索空间；训练LLM代理使用专用工具进行图遍历与因果分析

Result: 三家数据集测试F1分数达0.48-0.74，较先进工具提升最高27%

Conclusion: 将缺陷定位转为图搜索问题，为软件演化分析中的时序因果推理开辟新方向

Abstract: Identifying Bug-Inducing Commits (BICs) is fundamental for understanding software defects and enabling downstream tasks such as defect prediction and automated program repair. Yet existing SZZ-based approaches are limited by their reliance on git blame, which restricts the search space to commits that directly modified the fixed lines. Our preliminary study on 2,102 validated bug-fixing commits reveals that this limitation is significant: over 40% of cases cannot be solved by blame alone, as 28% of BICs require traversing commit history beyond blame results and 14% are blameless.
  We present AgenticSZZ, the first approach to apply Temporal Knowledge Graphs (TKGs) to software evolution analysis. AgenticSZZ reframes BIC identification from a ranking problem over blame commits into a graph search problem, where temporal ordering is fundamental to causal reasoning about bug introduction. The approach operates in two phases: (1) constructing a TKG that encodes commits with temporal and structural relationships, expanding the search space by traversing file history backward from two reference points (blame commits and the BFC); and (2) leveraging an LLM agent to navigate the graph using specialized tools for candidate exploration and causal analysis.
  Evaluation on three datasets shows that AgenticSZZ achieves F1-scores of 0.48 to 0.74, with statistically significant improvements over state-of-the-art by up to 27%. Our ablation study confirms that both components are essential, reflecting a classic exploration-exploitation trade-off: the TKG expands the search space while the agent provides intelligent selection. By transforming BIC identification into a graph search problem, we open a new research direction for temporal and causal reasoning in software evolution analysis.

</details>


### [17] [Testing Framework Migration with Large Language Models](https://arxiv.org/abs/2602.02964)
*Altino Alves,João Eduardo Montandon,Andre Hora*

Main category: cs.SE

TL;DR: 研究评估LLMs自动化Python测试框架迁移的能力，发现GPT-4o和Claude Sonnet 4各有倾向但总体成功率约48.5%


<details>
  <summary>Details</summary>
Motivation: 现有unittest转Pytest迁移过程繁琐耗时，自动化可加速测试现代化

Method: 创建开源项目迁移数据集，用GPT-4o和Claude Sonnet 4测试三种提示策略和两种温度设置，并实际执行生成代码

Result: 51.5%迁移失败，48.5%通过；Claude倾向保守迁移(保留类结构)，GPT-4o偏好函数式转换

Conclusion: LLMs可辅助迁移但存在局限，需关注模型特性差异并优化迁移策略

Abstract: Python developers rely on two major testing frameworks: \texttt{unittest} and \texttt{Pytest}. While \texttt{Pytest} offers simpler assertions, reusable fixtures, and better interoperability, migrating existing suites from \texttt{unittest} remains a manual and time-consuming process. Automating this migration could substantially reduce effort and accelerate test modernization. In this paper, we investigate the capability of Large Language Models (LLMs) to automate test framework migrations from \texttt{unittest} to \texttt{Pytest}. We evaluate GPT 4o and Claude Sonnet 4 under three prompting strategies (Zero-shot, One-shot, and Chain-of-Thought) and two temperature settings (0.0 and 1.0). To support this analysis, we first introduce a curated dataset of real-world migrations extracted from the top 100 Python open-source projects. Next, we actually execute the LLM-generated test migrations in their respective test suites. Overall, we find that 51.5% of the LLM-generated test migrations failed, while 48.5% passed. The results suggest that LLMs can accelerate test migration, but there are often caveats. For example, Claude Sonnet 4 exhibited more conservative migrations (e.g., preserving class-based tests and legacy \texttt{unittest} references), while GPT-4o favored more transformations (e.g., to function-based tests). We conclude by discussing multiple implications for practitioners and researchers.

</details>


### [18] [Understanding Bug-Reproducing Tests: A First Empirical Study](https://arxiv.org/abs/2602.02965)
*Andre Hora,Gordon Fraser*

Main category: cs.SE

TL;DR: 对Python系统bug重现测试的实证研究：在LOC、断言数和复杂度上无显著差异，但含更多try/except块和弱断言，且95%仅重现单一bug。


<details>
  <summary>Details</summary>
Motivation: 现有研究极少探究bug重现测试的特性及其与其他测试的根本区别。

Method: 分析15个真实Python系统中的642个bug重现测试样本。

Result: 测试在基础指标上无显著差异，但try/except块和弱断言更多；95%测试仅重现单一bug，5%重现多bug。

Conclusion: 探讨研究意义并指出现有测试模式及未来研究方向。

Abstract: Developers create bug-reproducing tests that support debugging by failing as long as the bug is present, and passing once the bug has been fixed. These tests are usually integrated into existing test suites and executed regularly alongside all other tests to ensure that future regressions are caught. Despite this co-existence with other types of tests, the properties of bug-reproducing tests are scarcely researched, and it remains unclear whether they differ fundamentally. In this short paper, we provide an initial empirical study to understand bug-reproducing tests better. We analyze 642 bug-reproducing tests of 15 real-world Python systems. Overall, we find that bug-reproducing tests are not (statistically significantly) different from other tests regarding LOC, number of assertions, and complexity. However, bug-reproducing tests contain slightly more try/except blocks and ``weak assertions'' (e.g.,~\texttt{assertNotEqual}). Lastly, we detect that the majority (95%) of the bug-reproducing tests reproduce a single bug, while 5% reproduce multiple bugs. We conclude by discussing implications and future research directions.

</details>


### [19] [What Do Contribution Guidelines Say About Software Testing?](https://arxiv.org/abs/2602.02966)
*Bruna Falcucci,Felipe Gomide,Andre Hora*

Main category: cs.SE

TL;DR: 本研究实证分析了200个Python和JavaScript开源项目贡献指南中的软件测试实践，揭示了测试文档的普及性和内容差异。


<details>
  <summary>Details</summary>
Motivation: 软件测试在开源项目贡献中至关重要（如编写测试可提高代码接受率），但贡献指南中具体的测试指导不明，因此通过研究澄清这些实践。

Method: 定性分析200个Python和JavaScript开源项目的贡献指南文件，包括CONTRIBUTING文件和外部文档等。

Result: 78继续%的项目包含测试文档；主要来源为CONTRIBUTING文件（58%），外部文档（24%）和README文件（8%）。文档常覆盖运行测试指导（83.5%），但编写指导（37%）较少；unit测试讨论率高（71%），而集成测试（20.5%）、端到端测试（15.5%）、测试覆盖率（25.5%）和模拟技术（9.5%）均较少涉及。

Conclusion: 研究讨论了改进测试指南完整性的实践启示，并提出了未来研究方向，如强化集成测试和覆盖率指导。

Abstract: Software testing plays a crucial role in the contribution process of open-source projects. For example, contributions introducing new features are expected to include tests, and contributions with tests are more likely to be accepted. Although most real-world projects require contributors to write tests, the specific testing practices communicated to contributors remain unclear. In this paper, we present an empirical study to understand better how software testing is approached in contribution guidelines. We analyze the guidelines of 200 Python and JavaScript open-source software projects. We find that 78\% of the projects include some form of test documentation for contributors. Test documentation is located in multiple sources, including \texttt{CONTRIBUTING} files (58\%), external documentation (24\%), and \texttt{README} files (8\%). Furthermore, test documentation commonly explains how to run tests (83.5\%), but less often provides guidance on how to write tests (37\%). It frequently covers unit tests (71\%), but rarely addresses integration (20.5\%) and end-to-end tests (15.5\%). Other key testing aspects are also less frequently discussed: test coverage (25.5\%) and mocking (9.5\%). We conclude by discussing implications and future research.

</details>


### [20] [Maintaining the Heterogeneity in the Organization of Software Engineering Research](https://arxiv.org/abs/2602.03093)
*Yang Yue,Zheng Jiang,Yi Wang*

Main category: cs.SE

TL;DR: 论文指出软件工程领域长期存在资助型与实操型两种研究模式并存的异质性,促进了该领域繁荣,但近年资助型研究渐成主流威胁此多样性。作者呼吁学界重视维持研究组织的异质性。


<details>
  <summary>Details</summary>
Motivation: 揭示软件工程研究组织异质性失衡趋势,论证其必要性以避免学科发展单一化风险。

Method: 通过分析历史脉络,阐述异质性价值,审视当前资助型研究主导的现状趋势及其后果,并提出未来发展方向。

Result: 异质性减弱将系统性威胁软件工程跨学科特性,导致研究模式趋同与创新能力下降。

Conclusion: 学术社区应主动选择并捍卫研究模式的多样性,确保软件工程领域的持续健康发展。

Abstract: The heterogeneity in the organization of software engineering (SE) research historically exists, i.e., funded research model and hands-on model, which makes software engineering become a thriving interdisciplinary field in the last 50 years. However, the funded research model is becoming dominant in SE research recently, indicating such heterogeneity has been seriously and systematically threatened. In this essay, we first explain why the heterogeneity is needed in the organization of SE research, then present the current trend of SE research nowadays, as well as the consequences and potential futures. The choice is at our hands, and we urge our community to seriously consider maintaining the heterogeneity in the organization of software engineering research.

</details>


### [21] [Synthesizing File-Level Data for Unit Test Generation with Chain-of-Thoughts via Self-Debugging](https://arxiv.org/abs/2602.03181)
*Ziyue Hua,Tianyu Chen,Yeyun Gong,Shuai Lu,Peng Cheng,Qinglin Zhu,Yibo He,Yingjie Fu,Wenpin Jiao,Wei Yang,Tao Xie*

Main category: cs.SE

TL;DR: 作者提出了一种新型数据蒸馏方法，利用自调试生成高质量单元测试和忠实CoT解释，显著提升UT生成效果，优于主流商业模型。


<details>
  <summary>Details</summary>
Motivation: 现有UT生成方法如符号执行、基于搜索方法和LLM生成难以产生具备正确断言和可靠CoT解释的测试。问题根源是训练数据不足：仓库挖掘测试缺乏开发者CoT，而LLM蒸馏的CoT常错误或不完整。

Method: 结合引导测试修复（包含错误、失败和覆盖率焦点的启发式自调试循环，用于迭代修正测试）和CoT压缩（精简原始及调试CoT为合理证明），在开源项目上构建含75518个高质量<核心方法、测试、CoT>的数据集，并用于基础模型的监督微调。

Result: 微调模型表现优异：测试断言通过率达36.17%，分支覆盖率43.90%，变异分数88.66%，远超当前最优商业模型如o4-mini。

Conclusion: 该方法有效填补了UT训练数据空白，生成的测试具有更高质量和CoT可靠性，为软件保障提供了实用解决方案。

Abstract: Automatic unit test (UT) generation is essential for software quality assurance, but existing approaches--including symbolic execution, search-based approaches, and recent LLM-based generators--struggle to produce human-quality tests with correct, meaningful assertions and reliable chain-of-thought (CoT) explanations. We identify a gap in UT training data: repository-mined tests lack developer CoTs, while LLM-distilled CoTs are often incorrect or incomplete. To address this issue, we propose a novel data-distillation approach that uses self-debugging to produce high-quality UT training examples paired with faithful CoTs. Our approach combines (1) guided test repair, a heuristic loop (error-, failure-, and coverage-focused steps) that asks the used model to diagnose and iteratively fix generated tests, and (2) CoT compression, which compacts original and debugging CoTs into concise explanations that directly justify correct tests. We apply this pipeline to a large corpus of open-source projects to construct a dataset of 74,518 high-quality <focal method, test, CoT> examples, and then use it for supervised fine-tuning of a base model. An empirical evaluation shows that the fine-tuned model achieves high UT generation effectiveness: it attains a pass rate of 36.17% on test assertions, a branch coverage of 43.90%, and a mutation score of 88.66%, substantially higher than state-of-the-art commercial models like o4-mini.

</details>


### [22] [Multi-Level Testing of Conversational AI Systems](https://arxiv.org/abs/2602.03311)
*Elena Masserini*

Main category: cs.SE

TL;DR: 这篇博士论文提出针对对话人工智能系统的全新测试方法家族，聚焦于从语言与AI组件整合、单个代理到多代理实现的不同层级验证，旨在解决现有测试方案不适应对话交互特性的问题。


<details>
  <summary>Details</summary>
Motivation: 现有测试解决方案无法直接适应对话交互特性或AI组件的行为，导致在测试这些系统时存在不足，因此本研究探索适用于新型AI系统的测试途径以提升可靠性。

Method: 采用研究新型测试方法家族的方法，针对语言与AI组件的交互整合层、单个对话代理器以及多代理系统实施层级化验证策略，重点关注不同粒度的元素评估。

Result: 研究开发出一套多级测试框架，显著提高了对对话AI组件和系统的覆盖率和准确性，并展示了其在增强系统稳定性方面的潜质。

Conclusion: 该测试方法家族证明了多层验证对于优化对话AI系统性能和交互质量的重要性，为未来开发提供高效可靠的测试基础。

Abstract: Conversational AI systems combine AI-based solutions with the flexibility of conversational interfaces. However, most existing testing solutions do not straightforwardly adapt to the characteristics of conversational interaction or to the behavior of AI components. To address this limitation, this Ph.D. thesis investigates a new family of testing approaches for conversational AI systems, focusing on the validation of their constituent elements at different levels of granularity, from the integration between the language and the AI components, to individual conversational agents, up to multi-agent implementations of conversational AI systems

</details>


### [23] [Precision in Practice: Knowledge Guided Code Summarizing Grounded in Industrial Expectations](https://arxiv.org/abs/2602.03400)
*Jintai Li,Songqiang Chen,Shuo Jin,Xiaoyuan Xie*

Main category: cs.SE

TL;DR: 本文分析了工业环境下当前代码摘要方法的问题，提出了ExpSum方法，该方法通过多种技术引导大型语言模型生成符合开发者期望的结构化摘要，实验证明在性能指标上优于现有基准。


<details>
  <summary>Details</summary>
Motivation: 工业研究中，57.4%的代码摘要因未能满足开发者期望而被拒绝，开发者要求在摘要中使用正确领域术语、明确函数分类并避免冗余细节。

Method: ExpSum整合函数元数据抽象、信息性元数据过滤、上下文感知领域知识检索和约束驱动提示，指导大型语言模型生成结构化和期望对齐的摘要。

Result: 在Harmony SpankOS项目及基准测试中，ExpSum持续优于所有基准模型，BLEU-4提升达26.71%，ROUGE-L提升20.10%，LLM评估显示在其他项目中更好对齐开发者期望。

Conclusion: ExpSum通过结构化方法有效满足工业代码文档需求，提升摘要的实用性。

Abstract: Code summaries are essential for helping developers understand code functionality and reducing maintenance and collaboration costs. Although recent advances in large language models (LLMs) have significantly improved automatic code summarization, the practical usefulness of generated summaries in industrial settings remains insufficiently explored. In collaboration with documentation experts from the industrial HarmonyOS project, we conducted a questionnaire study showing that over 57.4% of code summaries produced by state-of-the-art approaches were rejected due to violations of developers' expectations for industrial documentation. Beyond semantic similarity to reference summaries, developers emphasize additional requirements, including the use of appropriate domain terminology, explicit function categorization, and the avoidance of redundant implementation details.
  To address these expectations, we propose ExpSum, an expectation-aware code summarization approach that integrates function metadata abstraction, informative metadata filtering, context-aware domain knowledge retrieval, and constraint-driven prompting to guide LLMs in generating structured, expectation-aligned summaries. We evaluate ExpSum on the HarmonyOS project and widely used code summarization benchmarks. Experimental results show that ExpSum consistently outperforms all baselines, achieving improvements of up to 26.71% in BLEU-4 and 20.10% in ROUGE-L on HarmonyOS. Furthermore, LLM-based evaluations indicate that ExpSum-generated summaries better align with developer expectations across other projects, demonstrating its effectiveness for industrial code documentation.

</details>


### [24] [SWE-Master: Unleashing the Potential of Software Engineering Agents via Post-Training](https://arxiv.org/abs/2602.03411)
*Huatong Song,Lisheng Huang,Shuang Sun,Jinhao Jiang,Ran Le,Daixuan Cheng,Guoxin Chen,Yiwen Hu,Zongchao Chen,Wayne Xin Zhao,Yang Song,Tao Zhang,Ji-Rong Wen*

Main category: cs.SE

TL;DR: 本文提出了SWE-Master，一个开源可复现的训练框架，用于构建高效软件工程代理，在benchmark中实现高解决率。


<details>
  <summary>Details</summary>
Motivation: 动机是系统性优化软件开发代理的管道，促进可复现研究并提升长期任务解决能力。

Method: 方法包括教师轨迹合成、数据筛选、长期监督微调、基于实行动反馈的强化学习、推理框架设计，并使用测时扩展优化性能。

Result: 在SWE-bench基准测试中几次，使用Qwen2.5在相同设置下解决率达61.4%，引入测时扩展@8后提升至70.8%，超越现有基线。

Conclusion: 结论是SWE-Master为软件开发代理研究提供了实用透明的基础，代码已开源供社区使用。

Abstract: In this technical report, we present SWE-Master, an open-source and fully reproducible post-training framework for building effective software engineering agents. SWE-Master systematically explores the complete agent development pipeline, including teacher-trajectory synthesis and data curation, long-horizon SFT, RL with real execution feedback, and inference framework design. Starting from an open-source base model with limited initial SWE capability, SWE-Master demonstrates how systematical optimization method can elicit strong long-horizon SWE task solving abilities. We evaluate SWE-Master on SWE-bench Verified, a standard benchmark for realistic software engineering tasks. Under identical experimental settings, our approach achieves a resolve rate of 61.4\% with Qwen2.5-Coder-32B, substantially outperforming existing open-source baselines. By further incorporating test-time scaling~(TTS) with LLM-based environment feedback, SWE-Master reaches 70.8\% at TTS@8, demonstrating a strong performance potential. SWE-Master provides a practical and transparent foundation for advancing reproducible research on software engineering agents. The code is available at https://github.com/RUCAIBox/SWE-Master.

</details>


### [25] [SWE-World: Building Software Engineering Agents in Docker-Free Environments](https://arxiv.org/abs/2602.03419)
*Shuang Sun,Huatong Song,Lisheng Huang,Jinhao Jiang,Ran Le,Zhihao Lv,Zongchao Chen,Yiwen Hu,Wenyang Luo,Wayne Xin Zhao,Yang Song,Hongteng Xu,Tao Zhang,Ji-Rong Wen*

Main category: cs.SE

TL;DR: 提出SWE-World框架，该Docker-free方案利用学习型代理替代物理执行环境训练 CSS 工程Agent，显著提升效率和性能。


<details>
  <summary>Details</summary>
Motivation: 现有容器化环境方法依赖物理执行和测试，资源密集、维护困难，限制了Agent训练的可伸缩性。

Method: 基于LLM的代理训练于真实交互数据，预测执行结果和测试反馈，无需构建Docker环境，保留标准交互循环并支持测试时缩放（TTS）。

Result: 在SWE-bench Verified实验中，Qwen2.5-Coder-32B的准确性从6.2%提升至52.0%（通过无Docker SFT）、55.0%（通过无Docker RL）和68.2%（通过TTS进一步优化）。

Conclusion: SWE-World提供高效、可扩展的无船解决方案，降低训练成本，加强Agent性能，适用于Coding任务。

Abstract: Recent advances in large language models (LLMs) have enabled software engineering agents to tackle complex code modification tasks. Most existing approaches rely on execution feedback from containerized environments, which require dependency-complete setup and physical execution of programs and tests. While effective, this paradigm is resource-intensive and difficult to maintain, substantially complicating agent training and limiting scalability. We propose SWE-World, a Docker-free framework that replaces physical execution environments with a learned surrogate for training and evaluating software engineering agents. SWE-World leverages LLM-based models trained on real agent-environment interaction data to predict intermediate execution outcomes and final test feedback, enabling agents to learn without interacting with physical containerized environments. This design preserves the standard agent-environment interaction loop while eliminating the need for costly environment construction and maintenance during agent optimization and evaluation. Furthermore, because SWE-World can simulate the final evaluation outcomes of candidate trajectories without real submission, it enables selecting the best solution among multiple test-time attempts, thereby facilitating effective test-time scaling (TTS) in software engineering tasks. Experiments on SWE-bench Verified demonstrate that SWE-World raises Qwen2.5-Coder-32B from 6.2\% to 52.0\% via Docker-free SFT, 55.0\% with Docker-free RL, and 68.2\% with further TTS. The code is available at https://github.com/RUCAIBox/SWE-World

</details>


### [26] [RAL-Bench: Benchmarking for Application-Level Functional Correctness and Non-Functional Quality Attributes](https://arxiv.org/abs/2602.03462)
*Ruwei Pan,Yakun Zhang,Qingyuan Liang,Yueheng Zhu,Chao Liu,Lu Zhang,Hongyu Zhang*

Main category: cs.SE

TL;DR: RAL-Bench是一款针对LLM生成应用级代码仓库的基准测试基准，显示功能性正确性为主要瓶颈，最高通过率不足45%。


<details>
  <summary>Details</summary>
Motivation: 已有基准测试对应用级仓库的多文件可运行性、依赖部署及功能与非功能质量评估不足想说研究不同LLM能否同时满足这些标准。

Method: 从高质量参考项目中提炼需求，构建覆盖功能和非功能属性的黑盒系统测试，通过测试通过率衡量功能正确性，并使用ISO/IEC 25010启发的五维度指标与非功能质量聚合打分及标准化评估。

Result: 16个LLM在零样本贪婪解码下测试显示，功能性正确性为瓶颈无明显改进最高通过率低于45%。

Conclusion: LLM应用级代码生成存在功能性缺陷为关键障碍；RAL-Bench地址空间缺口并已开源发布。

Abstract: Code generation has advanced rapidly with code-focused large language models (LLMs), especially on snippet-level tasks. However, application-level generation requires producing a runnable multi-file repository with correct structure, dependencies, and end-to-end executability, and real-world software must satisfy both functional correctness and non-functional quality (e.g., maintainability, security). Existing benchmarks provide a limited execution-based assessment of these requirements at the application level. We ask: Can current LLMs generate application-level repositories that meet both functional and non-functional criteria? We propose RAL-Bench, a benchmark and evaluation framework for application-level code generation. For each task, we distill a concise natural-language requirement from a high-quality reference project, build black-box system tests covering functional and non-functional attributes, and keep only tests that pass on the reference repository to ensure a sound oracle and an end-to-end executable suite. Functional correctness is measured by system-test pass rate. Non-functional quality is measured along five ISO/IEC 25010-inspired dimensions and aggregated with an Analytic Hierarchy Process (AHP)-derived weight vector, with per-dimension diagnostics and baseline-normalized scoring using reference measurements. Across 16 LLMs evaluated zero-shot with greedy decoding, functional correctness is the dominant bottleneck: no model exceeds a 45% functional pass rate under our requirement-driven, reference-validated tests. We release RAL-Bench at https://github.com/Wwstarry/RAL-Bench. .

</details>


### [27] [Formal Evidence Generation for Assurance Cases for Robotic Software Models](https://arxiv.org/abs/2602.03550)
*Fang Yan,Simon Foster,Ana Cavalcanti,Ibrahim Habli,James Baxter*

Main category: cs.SE

TL;DR: 论文提出一种基于模型的方法，将形式验证嵌入保险案例工作流程，系统化生成机器人安全证据，解决传统方法耗时易错、不一致等问题。通过需求模板化、工具协调和证据集成，结合RoboChart建模语言验证，案例研究证明其有效性。


<details>
  <summary>Details</summary>
Motivation: 保险案例证据生成在机器人安全领域因劳动密集、易出错和系统演变时难以保持一致，需要自动化高效解决方案。

Method: 嵌入形式验证：利用RoboChart（带形式语义领域建模语言），系统化将自然语言需求转化为形式断言（通过模板），协调模型检查与定理证明工具处理多样性质，集成验证结果自动生成证据。

Result: 案例研究显示，该方法能自动高效生成证据，提升保险案例的一致性和可靠性。

Conclusion: 该方法显著优化证据生成流程，增强了机器人系统的安全保证能力。

Abstract: Robotics and Autonomous Systems are increasingly deployed in safety-critical domains, so that demonstrating their safety is essential. Assurance Cases (ACs) provide structured arguments supported by evidence, but generating and maintaining this evidence is labour-intensive, error-prone, and difficult to keep consistent as systems evolve. We present a model-based approach to systematically generating AC evidence by embedding formal verification into the assurance workflow. The approach addresses three challenges: systematically deriving formal assertions from natural language requirements using templates, orchestrating multiple formal verification tools to handle diverse property types, and integrating formal evidence production into the workflow. Leveraging RoboChart, a domain-specific modelling language with formal semantics, we combine model checking and theorem proving in our approach. Structured requirements are automatically transformed into formal assertions using predefined templates, and verification results are automatically integrated as evidence. Case studies demonstrate the effectiveness of our approach.

</details>


### [28] [Flaky Tests in a Large Industrial Database Management System: An Empirical Study of Fixed Issue Reports for SAP HANA](https://arxiv.org/abs/2602.03556)
*Alexander Berndt,Thomas Bach,Sebastian Baltes*

Main category: cs.SE

TL;DR: 本文提出利用大型语言模型作为标注器自动分类测试不稳定性的根因，解决了手动标注的低效问题，并在 SAP HANA 系统中评估显示并发问题是主因。


<details>
  <summary>Details</summary>
Motivation: 手动标注不稳定测试报告耗时繁琐，需高效方法获取不稳定类别的概览结构。

Method: 使用 LLMs 作为标注器，通过模型内外一致性自动标注问题报告中固定不稳定问题的根因类别。

Result: 在 SAP HANA 分析的 559 份报告中，130 份（23%）由并发问题引起；不同测试类型面临不同不稳定挑战。

Conclusion: 建议未来研究在缓解不稳定性时评估方法对不同测试类型的普适性。

Abstract: Flaky tests yield different results when executed multiple times for the same version of the source code. Thus, they provide an ambiguous signal about the quality of the code and interfere with the automated assessment of code changes. While a variety of factors can cause test flakiness, approaches to fix flaky tests are typically tailored to address specific causes. However, the prevalent root causes of flaky tests can vary depending on the programming language, application domain, or size of the software project. Since manually labeling flaky tests is time-consuming and tedious, this work proposes an LLMs-as-annotators approach that leverages intra- and inter-model consistency to label issue reports related to fixed flakiness issues with the relevant root cause category. This allows us to gain an overview of prevalent flakiness categories in the issue reports. We evaluated our labeling approach in the context of SAP HANA, a large industrial database management system. Our results suggest that SAP HANA's tests most commonly suffer from issues related to concurrency (23%, 130 of 559 analyzed issue reports). Moreover, our results suggest that different test types face different flakiness challenges. Therefore, we encourage future research on flakiness mitigation to consider evaluating the generalizability of proposed approaches across different test types.

</details>


### [29] [Scaling Test-Driven Code Generation from Functions to Classes: An Empirical Study](https://arxiv.org/abs/2602.03557)
*Yunhao Liang,Ruixuan Ying,Shiwen Ni,Zhe Cui*

Main category: cs.SE

TL;DR: 论文提出一种迭代式TDD框架，将测试驱动的代码生成从函数级扩展到类级，显著提升语言模型生成类级别代码的可靠性。


<details>
  <summary>Details</summary>
Motivation: 现有TDD代码生成研究局限于函数级任务，类级合成场景（方法通过共享状态交互）研究不足，需解决类级别代码生成的挑战。

Method: 通过分析类内方法依赖制定生成顺序，增量实现每个方法：使用反射式执行反馈和有限修复迭代完成方法级测试驱动生成；构建ClassEval-TDD标准化数据集支撑评估。

Result: 在8个语言模型上验证：类级正确率提升12-26个百分点，最高达71%完全正确类；平均仅需少量修复迭代，优于所有直接生成基线策略。

Conclusion: 测试驱动生成可有效扩展至类级编程，大幅提升代码生成可靠性，为复杂代码结构生成提供新解决方案。

Abstract: Test-driven development (TDD) has been adopted to improve Large Language Model (LLM)-based code generation by using tests as executable specifications. However, existing TDD-style code generation studies are largely limited to function-level tasks, leaving class-level synthesis where multiple methods interact through shared state and call dependencies underexplored. In this paper, we scale test-driven code generation from functions to classes via an iterative TDD framework. Our approach first analyzes intra-class method dependencies to derive a feasible generation schedule, and then incrementally implements each method under method-level public tests with reflection-style execution feedback and bounded repair iterations. To support test-driven generation and rigorous class-level evaluation, we construct ClassEval-TDD, a cleaned and standardized variant of ClassEval with consistent specifications, deterministic test environments, and complete method-level public tests. We conduct an empirical study across eight LLMs and compare against the strongest direct-generation baseline (the best of holistic, incremental, and compositional strategies). Our class-level TDD framework consistently improves class-level correctness by 12 to 26 absolute points and achieves up to 71% fully correct classes, while requiring only a small number of repairs on average. These results demonstrate that test-driven generation can effectively scale beyond isolated functions and substantially improve class-level code generation reliability. All code and data are available at https://anonymous.4open.science/r/ClassEval-TDD-C4C9/

</details>


### [30] [Causal Inference for the Effect of Code Coverage on Bug Introduction](https://arxiv.org/abs/2602.03585)
*Lukas Schulte,Gordon Fraser,Steffen Herbold*

Main category: cs.SE

TL;DR: 使用因果推断方法量化JavaScript/TypeScript项目中代码覆盖率对引入缺陷的因果效应，分析剂量-反应关系和非线性模式。


<details>
  <summary>Details</summary>
Motivation: 解决代码覆盖率作为质量保证手段的有效性争议，克服先前研究的关联性局限（易受混杂因素影响），在成熟开源项目中建立因果证据。

Method: 构建因果有向无环图识别混杂变量；结合广义倾向评分调整和双重稳健回归，对连续覆盖率暴露的缺陷引入变更数据集进行建模，估计平均处理效应与非线性剂量-响应关系（如阈值效应）。

Result: 摘要仅描述研究方法框架，未呈现实际结果（研究尚未完成）。

Conclusion: 本研究旨在通过因果推断揭示代码覆盖率与缺陷引入间的系统性规律，为覆盖率的合理应用提供实证基础。

Abstract: Context: Code coverage is widely used as a software quality assurance measure. However, its effect, and specifically the advisable dose, are disputed in both the research and engineering communities. Prior work reports only correlational associations, leaving results vulnerable to confounding factors. Objective: We aim to quantify the causal effect of code coverage (exposure) on bug introduction (outcome) in the context of mature JavaScript and TypeScript open source projects, addressing both the overall effect and its variance across coverage levels. Method: We construct a causal directed acyclic graph to identify confounders within the software engineering process, modeling key variables from the source code, issue- and review systems, and continuous integration. Using generalized propensity score adjustment, we will apply doubly robust regression-based causal inference for continuous exposure to a novel dataset of bug-introducing and non-bug-introducing changes. We estimate the average treatment effect and dose-response relationship to examine potential non-linear patterns (e.g., thresholds or diminishing returns) within the projects of our dataset.

</details>


### [31] [Beyond the Commit: Developer Perspectives on Productivity with AI Coding Assistants](https://arxiv.org/abs/2602.03593)
*Valerie Chen,Jasmyn He,Behnjamin Williams,Jason Valentino,Ameet Talwalkar*

Main category: cs.SE

TL;DR: 该研究探讨评估AI编程助手影响开发者生产力的有效方法，强调需多角度评估并侧重长期维度。


<details>
  <summary>Details</summary>
Motivation: 理解AI工具对生产力的影响，评估现有框架是否适用，以满足学术界与工业界的共同需求。

Method: 在BNY Mellon公司采用混合方法：调查2989份开发者回应并执行11次深度访谈。

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Measuring developer productivity is a topic that has attracted attention from both academic research and industrial practice. In the age of AI coding assistants, it has become even more important for both academia and industry to understand how to measure their impact on developer productivity, and to reconsider whether earlier measures and frameworks still apply. This study analyzes the validity of different approaches to evaluating the productivity impacts of AI coding assistants by leveraging mixed-method research. At BNY Mellon, we conduct a survey with 2989 developer responses and 11 in-depth interviews. Our findings demonstrate that a multifaceted approach is needed to measure AI productivity impacts: survey results expose conflicting perspectives on AI tool usefulness, while interviews elicit six distinct factors that capture both short-term and long-term dimensions of productivity. In contrast to prior work, our factors highlight the importance of long-term metrics like technical expertise and ownership of work. We hope this work encourages future research to incorporate a broader range of human-centered factors, and supports industry in adopting more holistic approaches to evaluating developer productivity.

</details>


### [32] [CALM: A Self-Adaptive Orchestration Approach for QoS-Aware Routing in Small Language Model based Systems](https://arxiv.org/abs/2602.03632)
*Hemang Jain,Divyansh Pandey,Karthik Vaidhyanathan*

Main category: cs.SE

TL;DR: 本文介绍CALM机制，通过协调多个专业小型语言模型（SLM）舰队来应对AI系统运行时不确定性，降低延迟和能耗，保留任务性能。


<details>
  <summary>Details</summary>
Motivation: AI系统面临运行时不确定性（如动态工作负载、资源需求变化），影响服务质量（QoS）。语言模型系统资源消耗高，SLM虽高效但难满足多样性需求，需智能编排SLM舰队以适应变化。

Method: CALM基于MAPE-K框架，持续监控用户查询，分析SLMs的QoS指标，选择最优SLM路由查询，并通过缓存和调度优化内存SLM使用。

Result: 评估显示，相比单一LLM基准，CALM降低延迟约40%、减少能耗50%，同时保持特定领域任务性能。

Conclusion: 协调专业SLM舰队与智能编排可有效提升系统适应性性能，平衡资源与服务质量。

Abstract: AI-enabled systems are subjected to various types of runtime uncertainties, ranging from dynamic workloads, resource requirements, model drift, etc. These uncertainties have a big impact on the overall Quality of Service (QoS). This is particularly true in the case of Language Model (LM) enabled systems where the autoregressive nature of token generation introduces variability in latency, energy usage and response quality. These systems, powered by LLMs, are either resource-intensive (if run on-prem) or raise privacy/cost concerns (if leveraged using APIs). While deploying a Small Language Model (SLM) can be resource-efficient, it often falls short in addressing the diversity and scale of real-world requirements. To this, we argue that, rather than relying on any one SLM, leveraging a coordinated fleet of SLMs, each with specialized strengths can enable systems to dynamically adapt to shifting contexts and workload patterns. However, realizing the full potential of such an approach demands intelligent orchestration and continuous adaptation. To this end, we introduce CALM , a self-adaptive orchestration mechanism based on MAPE-K. Our approach continuously monitors user queries, analyzes the QoS metrics of the SLMs, identifies the optimal SLM to be used, routes the query to the identified SLM and further to enhance the effectiveness and efficiency, leverages caching and scheduling to decide the SLMs to be kept in memory. Our evaluation shows that CALM reduces latency by approximately 40% and energy consumption by 50%, while preserving domain-specific task performance when compared to single-LLM baselines.

</details>


### [33] [SWE-Refactor: A Repository-Level Benchmark for Real-World LLM-Based Code Refactoring](https://arxiv.org/abs/2602.03712)
*Yisen Xu,Jinqiu Yang,Tse-Hsun,Chen*

Main category: cs.SE

TL;DR: 本文引入SWE-Refactor基准，通过1099个Java项目重构实例评估LLMs性能，突出复合重构的高失败率，并将结果开源。


<details>
  <summary>Details</summary>
Motivation: 现有重构基准覆盖面窄、常混杂无关变更，且缺乏仓库级上下文，影响评估真实性。

Method: 提取18个Java项目的1099个已验证重构实例（含922原子式和177复合式），编译、测试和工具校验以保证正确性，评估九个流行LLM模型。

Result: 复杂的复合重构失败最多，OpenAI Codex代理成功率仅39.減；其他LLMs在测试中亦表现不佳。

Conclusion: SWE-Refactor可助力LLMs代码重构研究，但复杂任务仍是瓶颈。

Abstract: Large Language Models (LLMs) have recently attracted wide interest for tackling software engineering tasks. In contrast to code generation, refactoring demands precise, semantics-preserving edits that improve program structure, which also makes automated evaluation challenging. However, existing refactoring benchmarks commonly suffer from three shortcomings: limited coverage of refactoring scenarios, the inclusion of instances that mix refactoring with unrelated changes, and insufficient repository-level context for realistic assessment. To mitigate these issues, we introduce SWE-Refactor, a new benchmark for LLM-based code refactoring. SWE-Refactor comprises 1,099 developer-written, behavior-preserving refactorings mined from 18 Java projects, including 922 atomic and 177 compound instances. Each instance is validated via compilation, test execution, and automated refactoring detection tools to ensure correctness. We evaluate nine widely used LLMs on SWE-Refactor, covering models such as GPT-4o-mini, DeepSeek-V3, and CodeLLaMa, to provide representative reference results. Our results show that complex and compound refactorings remain the primary source of failures; notably, an OpenAI Codex agent achieves only 39.4% success on compound instances. We release SWE-Refactor and all evaluation results to facilitate future research on LLM-based code refactoring.

</details>


### [34] [Improving Deep Learning Library Testing with Machine Learning](https://arxiv.org/abs/2602.03755)
*Facundo Molina,M M Abid Naziri,Feiran Qin,Alessandra Gorla,Marcelo d'Amorim*

Main category: cs.SE

TL;DR: 探讨 rt用机器学习分类器提升TensorFlow和PyTorch的API规范挖掘准确性，以减少错误查找中误报。


<details>
  <summary>Details</summary>
Motivation: 现有API规范 mining方法精度不足，导致错误查找技术产生高误报率。

Method: 以张量形状为输入抽象，收集运行时标注数据训练ML分类器学习API限制。

Result: 在183个API上评估，分类器精度超91%，集成ACETest后通过率从~29%升至~61%。

Conclusion: ML强化输入分类是扩展DL库测试规模的关键辅助工具。

Abstract: Deep Learning (DL) libraries like TensorFlow and Pytorch simplify machine learning (ML) model development but are prone to bugs due to their complex design. Bug-finding techniques exist, but without precise API specifications, they produce many false alarms. Existing methods to mine API specifications lack accuracy. We explore using ML classifiers to determine input validity. We hypothesize that tensor shapes are a precise abstraction to encode concrete inputs and capture relationships of the data. Shape abstraction severely reduces problem dimensionality, which is important to facilitate ML training. Labeled data are obtained by observing runtime outcomes on a sample of inputs and classifiers are trained on sets of labeled inputs to capture API constraints. Our evaluation, conducted over 183 APIs from TensorFlow and Pytorch, shows that the classifiers generalize well on unseen data with over 91% accuracy. Integrating these classifiers into the pipeline of ACETest, a SoTA bug-finding technique, improves its pass rate from ~29% to ~61%. Our findings suggest that ML-enhanced input classification is an important aid to scale DL library testing.

</details>


### [35] [FullStack-Agent: Enhancing Agentic Full-Stack Web Coding via Development-Oriented Testing and Repository Back-Translation](https://arxiv.org/abs/2602.03798)
*Zimu Lu,Houxing Ren,Yunqiao Yang,Ke Wang,Zhuofan Zong,Mingjie Zhan,Hongsheng Li*

Main category: cs.SE

TL;DR: 论文提出了FullStack-Agent代理系统，用于辅助开发全栈网络应用，并结合自我改进机制提升性能，代码已开源。


<details>
  <summary>Details</summary>
Motivation: 现有LLM驱动代码代理主要生成前端页面，忽略后端数据处理和存储，而全栈开发需精准控制数据流、更新依赖和调试漏洞，因此引入新系统解决这些挑战。

Method: 引入三部分框架：FullStack-Dev（多代理架构具规划、编码和调试能力）；FullStack-Learn（基于反向翻译爬取数据扩展LLM能力）；FullStack-Bench（基准测试评估前端、后端和数据库功能）。

Result: FullStack-Dev比基准方法在前端、后端、数据库测试中分别提高8.7%、38.2%、15.9%；FullStack-Learn通过自学习使30B模型性能在三个测试集分别提升9.7%、9.5%、2.8%，证明有效性。

Conclusion: 该代理系统显著改进全栈开发流程，具备强实践和研究价值，开源代码促进社区应用。

Abstract: Assisting non-expert users to develop complex interactive websites has become a popular task for LLM-powered code agents. However, existing code agents tend to only generate frontend web pages, masking the lack of real full-stack data processing and storage with fancy visual effects. Notably, constructing production-level full-stack web applications is far more challenging than only generating frontend web pages, demanding careful control of data flow, comprehensive understanding of constantly updating packages and dependencies, and accurate localization of obscure bugs in the codebase. To address these difficulties, we introduce FullStack-Agent, a unified agent system for full-stack agentic coding that consists of three parts: (1) FullStack-Dev, a multi-agent framework with strong planning, code editing, codebase navigation, and bug localization abilities. (2) FullStack-Learn, an innovative data-scaling and self-improving method that back-translates crawled and synthesized website repositories to improve the backbone LLM of FullStack-Dev. (3) FullStack-Bench, a comprehensive benchmark that systematically tests the frontend, backend and database functionalities of the generated website. Our FullStack-Dev outperforms the previous state-of-the-art method by 8.7%, 38.2%, and 15.9% on the frontend, backend, and database test cases respectively. Additionally, FullStack-Learn raises the performance of a 30B model by 9.7%, 9.5%, and 2.8% on the three sets of test cases through self-improvement, demonstrating the effectiveness of our approach. The code is released at https://github.com/mnluzimu/FullStack-Agent.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [36] [NSC-SL: A Bandwidth-Aware Neural Subspace Compression for Communication-Efficient Split Learning](https://arxiv.org/abs/2602.02696)
*Zhen Fang,Miao Yang,Zehang Lin,Zheng Lin,Zihan Fang,Zongyuan Zhang,Tianyang Duan,Dong Huang,Shunzhi Zhu*

Main category: cs.NI

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: The expanding scale of neural networks poses a major challenge for distributed machine learning, particularly under limited communication resources. While split learning (SL) alleviates client computational burden by distributing model layers between clients and server, it incurs substantial communication overhead from frequent transmission of intermediate activations and gradients. To tackle this issue, we propose NSC-SL, a bandwidth-aware adaptive compression algorithm for communication-efficient SL. NSC-SL first dynamically determines the optimal rank of low-rank approximation based on the singular value distribution for adapting real-time bandwidth constraints. Then, NSC-SL performs error-compensated tensor factorization using alternating orthogonal iteration with residual feedback, effectively minimizing truncation loss. The collaborative mechanisms enable NSC-SL to achieve high compression ratios while preserving semantic-rich information essential for convergence. Extensive experiments demonstrate the superb performance of NSC-SL.

</details>


### [37] [Real-World Applications of AI in LTE and 5G-NR Network Infrastructure](https://arxiv.org/abs/2602.02787)
*Simran Saxena,Arpad Kovesdy*

Main category: cs.NI

TL;DR: 提出AI驱动的自优化无线接入网架构和边缘托管执行模型，提升网络性能并扩展服务覆盖


<details>
  <summary>Details</summary>
Motivation: 解决静态网络配置在动态环境下适应不足的问题，以及回传链路薄弱社区无法获得AI服务的限制

Method: 结合AI辅助规划、强化学习优化、实时遥测分析和数字孪生验证；设计基于基站的容器化边缘托管模型

Result: AI方案优化网络性能并降低运维成本；边缘部署减少延迟和带宽消耗，增强韧性，扩大服务可及性

Conclusion: 工作展示了AI技术对构建高性能、可持续和普惠网络的贡献

Abstract: Telecommunications networks generate extensive performance and environmental telemetry, yet most LTE and 5G-NR deployments still rely on static, manually engineered configurations. This limits adaptability in rural, nomadic, and bandwidth-constrained environments where traffic distributions, propagation characteristics, and user behavior fluctuate rapidly. Artificial Intelligence (AI), more specifically Machine Learning (ML) models, provide new opportunities to transition Radio Access Networks (RANs) from rigid, rule-based systems toward adaptive, self-optimizing infrastructures that can respond autonomously to these dynamics. This paper proposes a practical architecture incorporating AI-assisted planning, reinforcement-learning-based RAN optimization, real-time telemetry analytics, and digital-twin-based validation. In parallel, the paper addresses the challenge of delivering embodied-AI healthcare services, educational tools, and large language model (LLM) applications to communities with insufficient backhaul for cloud computing. We introduce an edge-hosted execution model in which applications run directly on LTE/5G-NR base stations using containers, reducing latency and bandwidth consumption while improving resilience. Together, these contributions demonstrate how AI can enhance network performance, reduce operational overhead, and expand access to advanced digital services, aligning with broader goals of sustainable and inclusive network development.

</details>


### [38] [Analyzing Zigbee Traffic: Datasets, Classification and Storage Trade-offs](https://arxiv.org/abs/2602.03140)
*Antonio Boiano,Dalin Zheng,Fabio Palmese,Andrea Pimpinella,Alessandro E. C. Redondi*

Main category: cs.NI

TL;DR: 该论文介绍了公开Zigbee流量数据集ZIOTP2025，分析了设备类型分类和单独设备识别的性能退化问题，探讨存储效率优化方法。


<details>
  <summary>Details</summary>
Motivation: 现有研究依赖有限数据集和固定网络配置，限制了IoT取证分析的鲁棒性，需在多变场景下进行更全面评估。

Method: 收集多重网络配置下的ZIOTP2025数据集，从数据采集、流量分类和存储效率三角度分析，评估设备类型分类及单独设备识别在不同配置下的稳健性，并测试有损压缩技术。

Result: 跨配置环境下分类准确率显著下降，尤其精细识别任务；量化压缩使存储需求降低4-5倍，分类性能接近无损。

Conclusion: 研究强调拓扑感知分析和高效特征压缩对提升IoT取证系统可扩展性的必要性。

Abstract: Zigbee is widely used in smart home environments due to its low power consumption and support for mesh networking, making it a relevant target for traffic-based IoT forensic analysis. However, existing studies often rely on limited datasets and fixed network configurations. In this paper, we analyze Zigbee network traffic from three complementary perspectives: data collection, traffic classification, and storage efficiency. We introduce ZIOTP2025, a publicly available dataset of Zigbee traffic collected from commercial smart home devices deployed under multiple network configurations and capturing realistic interaction scenarios. Using this dataset, we study two traffic classification tasks: device type classification and individual device identification, and evaluate their robustness under both intra-configuration and cross-configuration settings. Our results show that while high classification accuracy can be achieved under controlled conditions, performance degrades significantly when models are evaluated across different network configurations, particularly for fine-grained identification tasks. Finally, we investigate the trade-off between traffic storage requirements and classification accuracy. We show that lossy compression of traffic features through quantization can reduce storage requirements by approximately 4-5x compared to lossless storage of raw packet traces, while preserving near-lossless classification performance. Overall, our results highlight the need for topology-aware Zigbee traffic analysis and storage-efficient feature compression to enable robust and scalable IoT forensic systems.

</details>


### [39] [Towards Context-Aware Edge-Cloud Continuum Orchestration for Multi-user XR Services](https://arxiv.org/abs/2602.03262)
*Inhar Yeregui,Ángel Martín,Mikel Zorrilla,Roberto Viola,Jasone Astorga,Eduardo Jacob*

Main category: cs.NI

TL;DR: 本文开发了一个分层参数化模型，用于优化多用户XR服务的Edge-Cloud Continuum编排，以应对低延迟体验的挑战。


<details>
  <summary>Details</summary>
Motivation: 多用户XR应用在娱乐、教育和远程医疗等领域的快速增加，对共享分布式环境中的无缝沉浸体验要求极高，但现有网络、计算和服务资源的编排存在局限性，亟需结构化方法来分析和优化这些复杂系统。5G和6G网络虽提供基础设施，但仍需解决高绩效、低延迟的难题。

Method: 通过将多用户XR服务在标准虚拟化架构的四个关键层中参数化，并数学形式化为一个上下文感知框架，定义了各级关键参数，集成到Edge-Cloud Continuum编排策略中。

Result: 文章贡献包括分析现有Edge-Cloud Continuum编排的限制与需求，建立分层数学模型，并开发验证框架，证明了方案的工具性和可行性。

Conclusion: 该模型有效解决了资源编排的局限性，提供了一种全面集成Edge-Cloud Continuum的策略，强化了XR服务在高性能网络中的实施。

Abstract: The rapid growth of multi-user eXtended Reality (XR) applications, spanning fields such as entertainment, education, and telemedicine, demands seamless, immersive experiences for users interacting within shared, distributed environments. Delivering such latency-sensitive experiences involves considerable challenges in orchestrating network, computing, and service resources, where existing limitations highlight the need for a structured approach to analyse and optimise these complex systems. This challenge is amplified by the need for high-performance, low-latency connectivity, where 5G and 6G networks provide essential infrastructure to meet the requirements of XR services at scale. This article addresses these challenges by developing a model that parametrises multi-user XR services across four critical layers of the standard virtualisation architecture. We formalise this model mathematically, proposing a context-aware framework that defines key parameters at each level and integrates them into a comprehensive Edge-Cloud Continuum orchestration strategy. Our contributions include a detailed analysis of the current limitations and needs in existing Edge-Cloud Continuum orchestration approaches, the formulation of a layered mathematical model, and a validation framework that demonstrates the utility and feasibility of the proposed solution.

</details>


### [40] [QASM: A Novel Framework for QUIC-Aware Stateful Middleboxes](https://arxiv.org/abs/2602.03354)
*Hari Hara Sudhan Selvam,Sameer G. Kulkarni*

Main category: cs.NI

TL;DR: HTTP/3采用QUIC协议，其加密和连接迁移功能扰乱了状态中间设备对网络流的识别，影响网络安全和服务；作者提出新框架，可靠跟踪QUIC连接变化，原型显示开销低于5%，高效支持高达100 Hz迁移率。


<details>
  <summary>Details</summary>
Motivation: 状态中间设备（如NAT、速率限制器和负载均衡器）依赖精确流量识别，但QUIC协议隐藏流语义，尤其在Kubernetes部署中导致设备功能失效，亟待解决以保障网络安全。

Method: 提出通用框架，使中间设备在端点IP或端口变更时可靠跟踪QUIC连接，通过机制维护连接状态以确保功能连续性。

Result: 原型验证框架有效，吞吐量和延迟开销小于5%，能处理100 Hz连接迁移率，保护中间设备功能完整。

Conclusion: 该框架高效解决QUIC对中间设备的干扰，低开销且可扩展，为HTTP/3网络中状态服务提供可靠支持。

Abstract: Stateful Middleboxes are integral part of enterprise and campus networks that provide essential in-network, security, and value-added services. These stateful middleboxes rely on precise network flow identification. However, the adoption of HTTP/3, which uses the QUIC protocol, poses significant challenges to the proper functioning of these devices. QUIC's encryption and connection migration features obscure flow semantics, disrupting middlebox visibility and functionality. We examine how QUIC disrupts middleboxes like Network Address Translators (NATs), Rate Limiters, Load Balancers, etc., and affects Kubernetes-based service deployments. To address these challenges, we propose a novel, generalized framework that enables stateful middleboxes to reliably track QUIC connections, even when the endpoints change their internet protocol (IP) address or port numbers. Our prototype implementation demonstrates that the proposed approach preserves middlebox functionality with HTTP/3 with negligible performance overhead (< 5%) on both throughput and latency, and works effectively even under high QUIC connection migration rates of up to 100 Hz.

</details>


### [41] [Morphe: High-Fidelity Generative Video Streaming with Vision Foundation Model](https://arxiv.org/abs/2602.03529)
*Tianyi Gong,Zijian Cao,Zixing Zhang,Jiangkai Wu,Xinggong Zhang,Shuguang Cui,Fangxin Wang*

Main category: cs.NI

TL;DR: Morphe应用视觉基础模型（VFM）实现高压缩率、低延迟且抗网络损失的实时视频流媒体，相比H.265节省62.5%带宽。


<details>
  <summary>Details</summary>
Motivation: 视频流媒体在网络条件恶劣时质量不佳 monkey现有技术面临传统像素编解码压缩瓶颈，或新兴神经增强/生成式方案的延迟与保真度缺陷。

Method: 通过模拟网络约束下视觉标记器的联合训练和变分辨率时空优化，并构建鲁棒系统采用智能丢包抵御网络扰动。

Result: 在严格评估中达成与H.265相当的视觉质量，节省62.5%带宽，且在挑战性网络环境中实现实时、抗损的视频传输。

Conclusion: 该方案是视觉基础模型在多媒体流媒体领域的重要里程碑，推动了高效通用视频传输的实用化发展。

Abstract: Video streaming is a fundamental Internet service, while the quality still cannot be guaranteed especially in poor network conditions such as bandwidth-constrained and remote areas. Existing works mainly work towards two directions: traditional pixel-codec streaming nearly approaches its limit and is hard to step further in compression; the emerging neural-enhanced or generative streaming usually fall short in latency and visual fidelity, hindering their practical deployment. Inspired by the recent success of vision foundation model (VFM), we strive to harness the powerful video understanding and processing capacities of VFM to achieve generalization, high fidelity and loss resilience for real-time video streaming with even higher compression rate. We present the first revolutionized paradigm that enables VFM-based end-to-end generative video streaming towards this goal. Specifically, Morphe employs joint training of visual tokenizers and variable-resolution spatiotemporal optimization under simulated network constraints. Additionally, a robust streaming system is constructed that leverages intelligent packet dropping to resist real-world network perturbations. Extensive evaluation demonstrates that Morphe achieves comparable visual quality while saving 62.5\% bandwidth compared to H.265, and accomplishes real-time, loss-resilient video delivery in challenging network environments, representing a milestone in VFM-enabled multimedia streaming solutions.

</details>


### [42] [RIPPLE: Lifecycle-aware Embedding of Service Function Chains in Multi-access Edge Computing](https://arxiv.org/abs/2602.03662)
*Federico Giarrè,Holger Karl*

Main category: cs.NI

TL;DR: 该论文研究了多接入边缘计算网络中用户移动引发的服务功能链重构问题，提出RIPPLE方法来主动管理虚拟网络功能生命周期，减少服务中断。


<details>
  <summary>Details</summary>
Motivation: 用户移动会导致频繁的服务功能链重构，但忽略虚拟网络功能生命周期动态会过简化部署并损害服务质量，因此需开发考虑不确定性的解决方案。

Method: 利用用户连通性预测，主动部署虚拟网络功能和重构服务功能链；提出生命周期感知的RIPPLE嵌入方法，协同处理生命周期和连通性不确定性。

Result: RIPPLE在现实生命周期约束下显著减少服务中断，性能接近假设生命周期瞬时完成的非讲过理想方案，缩小了差距。

Conclusion: RIPPLE有效处理虚拟网络功能生命周期动态，提升了服务功能链管理的鲁棒性和可靠性。

Abstract: In Multi-access Edge Computing networks, services can be deployed on nearby edge clouds (EC) as service function chains (SFCs) to meet strict quality of service (QoS) requirements. As users move, frequent SFC reconfigurations are required, but these are non-trivial: SFCs can serve users only when all required virtual network functions (VNFs) are available, and VNFs undergo time-consuming lifecycle operations before becoming operational. We show that ignoring lifecycle dynamics oversimplifies deployment, jeopardizes QoS, and must be avoided in practical SFC management. To address this, forecasts of user connectivity can be leveraged to proactively deploy VNFs and reconfigure SFCs. But forecasts are inherently imperfect, requiring lifecycle and connectivity uncertainty to be jointly considered. We present RIPPLE, a lifecycle-aware SFC embedding approach to deploy VNFs at the right time and location, reducing service interruptions. We show that RIPPLE closes the gap with solutions that unrealistically assume instantaneous lifecycle, even under realistic lifecycle constraints.

</details>


### [43] [xDevSM: An Open-Source Framework for Portable, AI-Ready xApps Across Heterogeneous O-RAN Deployments](https://arxiv.org/abs/2602.03821)
*Angelo Feraudo,Stefano Maxenti,Andrea Lacava,Leonardo Bonati,Paolo Bellavista,Michele Polese,Tommaso Melodia*

Main category: cs.NI

TL;DR: 本文提出x sanctionSMS框架，用于简化O-RAN中AI驱动xApp的开发，通过统一可观测性和控制解决复杂性、互操作性等问题，并在真实测试平台上验证其有效性和跨异构RAN的兼容性。


<details>
  <summary>Details</summary>
Motivation: xApp的开发和连接的采用受限于O-RAN E2接口暴露的低级RAN控制模型复杂性、异构RAN软件栈互操作性不足以及缺乏开发者友好框架等问题，亟需解决方案以推动AI驱动优化。

Method: 引入xDevSM框架，整合了关键性能测量（KPM）和精细资源管理控制，提供统一接口支持AI驱动xApp开发，采用了商用设备和开源软件在异构硬件（如USRP和Foxconn单元）上进行测试后用。

Result: 在真实测试平台上验证，成功实现跨多开源RAN栈无缝互操作，并通过三个O-RAN场景评估：KPM监控、PRB分配控制和切换控制，證明了其支持智能闭环应用的可行性與效果тим。

Conclusion: xDevSM为异构RAN中的学习型优化奠定坚实基石，实现了高效智能控制，已开源供研究社区，推动了自动化网络的未来发展。

Abstract: Openness and programmability in the O-RAN architecture enable closed-loop control of the Radio Access Network (RAN). Artificial Intelligence (AI)-driven xApps, in the near-real-time RAN Intelligent Controller (RIC), can learn from network data, anticipate future conditions, and dynamically adapt radio configurations. However, their development and adoption are hindered by the complexity of low-level RAN control and monitoring message models exposed over the O-RAN E2 interface, limited interoperability across heterogeneous RAN software stacks, and the lack of developer-friendly frameworks. In this paper, we introduce xDevSM, a framework that significantly lowers the barrier to xApp development by unifying observability and control in O-RAN deployment. By exposing a rich set of Key Performance Measurements (KPMs) and enabling fine-grained radio resource management controls, xDevSM provides the essential foundation for practical AI-driven xApps. We validate xDevSM on real-world testbeds, leveraging Commercial Off-the-Shelf (COTS) devices together with heterogeneous RAN hardware, including Universal Software Radio Peripheral (USRP)-based Software-defined Radios (SDRs) and Foxconn radio units, and show its seamless interoperability across multiple open-source RAN software stacks. Furthermore, we discuss and evaluate the capabilities of our framework through three O-RAN-based scenarios of high interest: (i) KPM-based monitoring of network performance, (ii) slice-level Physical Resource Block (PRB) allocation control across multiple User Equipments (UEs) and slices, and (iii) mobility-aware handover control, showing that xDevSM can implement intelligent closed-loop applications, laying the groundwork for learning-based optimization in heterogeneous RAN deployments. xDevSM is open source and available as foundational tool for the research community.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [44] [Prefix Consensus For Censorship Resistant BFT](https://arxiv.org/abs/2602.02892)
*Zhuolun Xiang,Andrei Tonkikh,Alexander Spiegelman*

Main category: cs.DC

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Despite broad use of BFT consensus in blockchains, censorship resistance is weak: leaders can exclude transactions, a growing concern for trading and DeFi.
  We address this by introducing a new abstraction and protocol stack. First, we introduce \emph{Prefix Consensus}, where parties input vectors and output $(v^{\sf low},v^{\sf high})$ that (i) extend the maximum common prefix of honest inputs and (ii) satisfy $v_i^{\sf low}\preceq v_j^{\sf high}$ for all honest $i,j$. Unlike classical consensus, no single output is required. We show Prefix Consensus is solvable asynchronously and give tight round-complexity bounds.
  We then define \emph{Strong Prefix Consensus}, requiring agreement on the \emph{high} output. Our protocol is leaderless and partially synchronous: one Prefix Consensus instance decides (possibly different) lows, and additional instances yield a unique safe-to-extend high, even if an adversary can suspend one party per round.
  We lift this to a leaderless, multi-proposer, censorship-resistant BFT SMR protocol: per slot, all parties broadcast proposals, deterministically rank them, and run one Strong Prefix Consensus on proposal hashes, committing honest proposals in \emph{four rounds}. A deterministic demotion rule updates the ranking when a party's proposal is excluded, implying that after GST at most $f$ slots can miss an honest proposal while progress remains leaderless under suspension and up to $f{-}1$ Byzantine faults.
  Finally, we connect Prefix Consensus to graded and binary/validated consensus: we obtain an optimal-latency graded consensus (3 message delays) and leaderless Binary/Validated Consensus with worst-case message complexity $O(n^3)$ and communication $O(n^4)$.

</details>


### [45] [Large-Scale LLM Inference with Heterogeneous Workloads: Prefill-Decode Contention and Asymptotically Optimal Control](https://arxiv.org/abs/2602.02987)
*Ruihan Lin,Zezhen Ding,Zean Han,Jiheng Zhang*

Main category: cs.DC

TL;DR: 該論文針對大型語言模型在GPU集群中推論階段的資源競爭問題，提出了一個基於隨機控制的異質工作負載排程框架。


<details>
  <summary>Details</summary>
Motivation: LLM推論分為填充（計算密集）與解碼（記憶體受限）兩階段，GPU資源共享引發狀態依存的競爭延遲，且異質應用程式產生複雜負載變化。

Method: 建立狀態依存服務率的多類多服務器排隊網絡模型，基於實測迭代時間數據；採流體近似法與穩態線性規劃優化資源分配，設計預填充准入和解碼路由策略。

Result: 策略在GPU數量趨於無限時漸近最優（合併與分離代幣定價方案），並在數值實驗中超越標準啟發式算法，延遲與公平性指標得到改善。

Conclusion: 此框架有效解決LLM服務的資源分配問題，並通過SLA指標擴展提升調度通用性。

Abstract: Large Language Models (LLMs) are rapidly becoming critical infrastructure for enterprise applications, driving unprecedented demand for GPU-based inference services. A key operational challenge arises from the two-phase nature of LLM inference: a compute-intensive \emph{prefill} phase that processes user input, followed by a memory-bound \emph{decode} phase that generates output tokens. When these phases share GPU resources, prefill tasks throttle the processing speed of concurrent decodes, creating state-dependent contention. This contention is further complicated by workload heterogeneity, as different applications exhibit vastly different input and output lengths. We develop a stochastic control framework for scheduling heterogeneous LLM workloads across large GPU clusters. We formulate LLM inference as a multiclass many-server queueing network with state-dependent service rates, grounded in empirical iteration-time measurements. We analyze the fluid approximation of this system and solve steady-state linear programs that characterize optimal resource allocation. We design gate-and-route policies that regulate prefill admission and decode routing, and prove that they are asymptotically optimal in the many-GPU limit under both bundled and separate token-pricing schemes. We further extend the framework to incorporate Service Level Indicators (SLIs) such as latency and fairness, providing a general approach to constrained scheduling. Numerical experiments calibrated to empirical iteration-time data demonstrate that our policies outperform standard serving heuristics.

</details>


### [46] [Studying the Effect of Schedule Preemption on Dynamic Task Graph Scheduling](https://arxiv.org/abs/2602.03081)
*Mohammadali Khodabandehlou,Jared Coleman,Niranjan Suri,Bhaskar Krishnamachari*

Main category: cs.DC

TL;DR: 本文研究动态任务图调度中的抢占策略，提出Last-K抢占模型选择性地重调度最近任务，保留早前分配。实验结果证明适度抢占在保持公平性和低开销的同时，可接近完全抢占在完工时间及利用率上的收益。


<details>
  <summary>Details</summary>
Motivation: 现有调度方法通常不重审已分配任务且专攻于最小化完工时间，本研究旨在通过可控抢占提升调度灵活性和整体效能。

Method: 引入Last-K抢占模型，选择性重调度近K个任务图；使用合成、RIoTBench、WFCommons及对抗负载，比较抢占、非抢占及部分抢占策略在完工时间、公平性、利用率和运行时的表现。

Result: 结果显示适度抢占策略在完工时间及利用率上接近完全抢占收益，同时保障公平性并降低运行开销。

Conclusion: 适度抢占策略可实现主要优化收益，兼具公平性和低开销，是高效平衡的调度方案。

Abstract: Dynamic scheduling of task graphs is often addressed without revisiting prior task allocations, with a primary focus on minimizing makespan. We study controlled schedule preemption, introducing the Last-K Preemption model, which selectively reschedules recent task graphs while preserving earlier allocations. Using synthetic, RIoTBench, WFCommons, and adversarial workloads, we compare preemptive, non-preemptive, and partial-preemptive strategies across makespan, fairness, utilization, and runtime. Results show moderate preemption can match most makespan and utilization gains of full preemption while maintaining fairness and low overhead.

</details>


### [47] [Joint Network-and-Server Congestion in Multi-Source Traffic Allocation: A Convex Formulation and Price-Based Decentralization](https://arxiv.org/abs/2602.03246)
*Tamoghna Sarkar,Bhaskar Krishnamachari*

Main category: cs.DC

TL;DR: 该论文研究网络系统中流量分配的优化问题，提出一个分布式定价算法，通过均衡边际延迟成本实现最小化端到端延迟。


<details>
  <summary>Details</summary>
Motivation: 解决多个源至多个服务节点的流量分配问题，其中路径接入延迟依赖于速率（凸函数约束），服务节点排队延迟依赖于负载（容量约束）。

Method: 开发轻量级分布式定价算法：每个服务节点基于负载计算并广播拥堵价格，每个源节点在给定价格下求解小型可分离凸也会题来更新流量分布。

Result: 数值模拟显示算法收敛到集中式最优解，并凸显了接入延迟与服务延迟联合建模的权衡效应。

Conclusion: 该方法通过凸优化和分散式定价，高效实现系统最优分配，并强调多约束下性能折现平衡的实践意义。

Abstract: This paper studies an important rate allocation problem that arises in many networked and distributed systems: steady-state traffic rate allocation from multiple sources to multiple service nodes when both (i) the access-path delay on each source-node route is rate-dependent (capacity-constrained) and convex, and (ii) each service node (also capacity-constrained) experiences a load-dependent queueing delay driven by aggregate load from all sources. We show that the resulting flow-weighted end-to-end delay minimization is a convex program, yielding a global system-optimal solution characterized by KKT conditions that equalize total marginal costs (a path marginal access term plus a node congestion price) across all utilized routes. This condition admits a Wardrop-type interpretation: for each source, all utilized options equalize total marginal cost, while any option with strictly larger total marginal cost receives no flow. Building on this structure, we develop a lightweight distributed pricing-based algorithm in which each service node locally computes and broadcasts a scalar congestion price from its observed aggregate load, while each source updates its traffic split by solving a small separable convex allocation problem under the advertised prices. Numerical illustrations demonstrate convergence of the distributed iteration to the centralized optimum and highlight the trade-offs induced by jointly modeling access and service congestion.

</details>


### [48] [Exploiting Multi-Core Parallelism in Blockchain Validation and Construction](https://arxiv.org/abs/2602.03444)
*Arivarasan Karmegam,Lucianna Kiffer,Antonio Fernández Anta*

Main category: cs.DC

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Blockchain validators can reduce block processing time by exploiting multi-core CPUs, but deterministic execution must preserve a given total order while respecting transaction conflicts and per-block runtime limits. This paper systematically examines how validators can exploit multi-core parallelism during both block construction and execution without violating blockchain semantics. We formalize two validator-side optimization problems: (i) executing an already ordered block on \(p\) cores to minimize makespan while ensuring equivalence to sequential execution; and (ii) selecting and scheduling a subset of mempool transactions under a runtime limit \(B\) to maximize validator reward. For both, we develop exact Mixed-Integer Linear Programming (MILP) formulations that capture conflict, order, and capacity constraints, and propose fast deterministic heuristics that scale to realistic workloads. Using Ethereum mainnet traces and including a Solana-inspired declared-access baseline (Sol) for ordered-block scheduling and a simple reward-greedy baseline (RG) for block construction, we empirically quantify the trade-offs between optimality and runtime.

</details>


### [49] [Recursive Energy Efficient Agreement](https://arxiv.org/abs/2602.03474)
*Shachar Meir,David Peleg*

Main category: cs.DC

TL;DR: 提出一种递归协议算法，在分布式系统的容错场景下，将每个参与者的活动轮次降至O(log f)，显著降低能源消耗。


<details>
  <summary>Details</summary>
Motivation: 传统分布式共识协议使所有参与者全程活跃，能源效率低下。为减少参与者在计算过程中的实际参与轮次，从而降低单位参与者的能耗成本。

Method: 设计递归式协议算法，其中f<n表示系统可容纳的崩溃故障节点最大数量。通过递归结构将参与者活动轮次压缩至对数级别。

Result: 算法实现每个参与者仅需活跃O(log f)轮次，相比传统方案大幅减少活动轮次。该复杂度与系统故障容限f直接相关。

Conclusion: 该算法首次实现对数级活动轮次的能源高效共识，为大规模分布式系统提供可行的低能耗解决方案。

Abstract: Agreement is a foundational problem in distributed computing that have been studied extensively for over four decades. Recently, Meir, Mirault, Peleg and Robinson introduced the notion of \emph{Energy Efficient Agreement}, where the goal is to solve Agreement while minimizing the number of round a party participates in, thereby reducing the energy cost per participant. We show a recursive Agreement algorithm that has $O(\log f)$ active rounds per participant, where $f<n$ represents the maximum number of crash faults in the system.

</details>


### [50] [DALI: A Workload-Aware Offloading Framework for Efficient MoE Inference on Local PCs](https://arxiv.org/abs/2602.03495)
*Zeyu Zhu,Gang Li,Peisong Wang,Zitao Mo,Minnan Pei,Zhuoran Song,Xiaoyao Liang,Jian Cheng*

Main category: cs.DC

TL;DR: DALI 是一个工作负载感知的卸载框架，通过动态专家分配、残差预取和工作负载感知缓存，提升本地PC上专家混合模型推理效率。


<details>
  <summary>Details</summary>
Motivation: 现有专家卸载方法无法匹配工作负载的动态性，导致 CPU-GPU 负载不平衡、预取不准确和GPU缓存效率低下，浪费资源。

Method: DALI 包括动态专家分配（建模为0-1整数优化，使用贪心策略解决）、残差预取（利用层间残差预测高工作负载专家）和工作负载感知缓存替换策略（利用激活时间相关性）。

Result: 在多个 MoE 模型和设置下，DALI 显著加速了预填充和解码阶段，优于现有卸载框架。

Conclusion: DALI 有效优化硬件资源利用率，为资源受限设备上的 MoE 推理提供高效解决方案。

Abstract: Mixture of Experts (MoE) architectures significantly enhance the capacity of LLMs without proportional increases in computation, but at the cost of a vast parameter size. Offloading MoE expert parameters to host memory and leveraging both CPU and GPU computation has recently emerged as a promising direction to support such models on resourceconstrained local PC platforms. While promising, we notice that existing approaches mismatch the dynamic nature of expert workloads, which leads to three fundamental inefficiencies: (1) Static expert assignment causes severe CPUGPU load imbalance, underutilizing CPU and GPU resources; (2) Existing prefetching techniques fail to accurately predict high-workload experts, leading to costly inaccurate prefetches; (3) GPU cache policies neglect workload dynamics, resulting in poor hit rates and limited effectiveness. To address these challenges, we propose DALI, a workloaDAware offLoadIng framework for efficient MoE inference on local PCs. To fully utilize hardware resources, DALI first dynamically assigns experts to CPU or GPU by modeling assignment as a 0-1 integer optimization problem and solving it efficiently using a Greedy Assignment strategy at runtime. To improve prefetching accuracy, we develop a Residual-Based Prefetching method leveraging inter-layer residual information to accurately predict high-workload experts. Additionally, we introduce a Workload-Aware Cache Replacement policy that exploits temporal correlation in expert activations to improve GPU cache efficiency. By evaluating across various MoE models and settings, DALI achieves significant speedups in the both prefill and decoding phases over the state-of-the-art offloading frameworks.

</details>


### [51] [Do We Need Asynchronous SGD? On the Near-Optimality of Synchronous Methods](https://arxiv.org/abs/2602.03802)
*Grigory Begunov,Alexander Tyurin*

Main category: cs.DC

TL;DR: 同步SGD及其变体m-同步SGD在异构计算环境中接近最优，优于预期。


<details>
  <summary>Details</summary>
Motivation: 重新评估同步优化方法在异步方法进步背景下仍具竞争力。

Method: 分析同步SGD和m-同步SGD在随机计算时间与对抗性工人部分参与下的表现。

Result: 证明其时间复杂性在许多实际场景下几乎最优，误差仅对数因子。

Conclusion: 同步方法足以应对多种现代异构计算需求，但非万能，特定任务仍需异步方法。

Abstract: Modern distributed optimization methods mostly rely on traditional synchronous approaches, despite substantial recent progress in asynchronous optimization. We revisit Synchronous SGD and its robust variant, called $m$-Synchronous SGD, and theoretically show that they are nearly optimal in many heterogeneous computation scenarios, which is somewhat unexpected. We analyze the synchronous methods under random computation times and adversarial partial participation of workers, and prove that their time complexities are optimal in many practical regimes, up to logarithmic factors. While synchronous methods are not universal solutions and there exist tasks where asynchronous methods may be necessary, we show that they are sufficient for many modern heterogeneous computation scenarios.

</details>
