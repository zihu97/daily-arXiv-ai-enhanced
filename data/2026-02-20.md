<div id=toc></div>

# Table of Contents

- [cs.SE](#cs.SE) [Total: 12]
- [cs.AR](#cs.AR) [Total: 2]
- [cs.DC](#cs.DC) [Total: 6]
- [cs.PF](#cs.PF) [Total: 1]
- [cs.NI](#cs.NI) [Total: 4]
- [cs.MA](#cs.MA) [Total: 5]


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [1] [A Construction-Phase Digital Twin Framework for Quality Assurance and Decision Support in Civil Infrastructure Projects](https://arxiv.org/abs/2602.16748)
*Md Asiful Islam,Shanto Jouerder,Md Sabit As Sami,Afia Jahin Prema*

Main category: cs.SE

TL;DR: 该研究提出了一种用于施工阶段质量保证的数字孪生框架，通过集成检查记录、材料数据和早期传感技术，支持元件级质量评估和早期决策，旨在减少延迟并优化工程管理。


<details>
  <summary>Details</summary>
Motivation: 工程建设质量保证常依赖完工后延迟数天或数周的检测记录和实验结果，易导致返工、进度延误和文档碎片化风险，需开发早期干预方法以提升效率。

Method: 采用数字孪生框架，链接检查记录、材料生产和放置数据、早期传感技术及强度预测模型至单个施工元件，集成数据流以实时追踪元件质量状态，支持在标准测试前做出释放或暂停决策。

Result: 框架提升了质量评估的追溯性和时效性，支持基于数据的早期决策；同时讨论了数据整合、合同限制和实施挑战等实际问题，但未取代既有检测流程，而是补充现有工作流。

Conclusion: 研究为施工质量保证提供了一个由延迟文档评审向主动元件级决策支持转型的结构化路径，促进工程管理的优化和创新。

Abstract: Quality assurance (QA) during construction often relies on inspection records and laboratory test results that become available days or weeks after work is completed. On large highway and bridge projects, this delay limits early intervention and increases the risk of rework, schedule impacts, and fragmented documentation. This study presents a construction-phase digital twin framework designed to support element-level QA and readiness-based decision making during active construction. The framework links inspection records, material production and placement data, early-age sensing, and predictive strength models to individual construction elements. By integrating these data streams, the system represents the evolving quality state of each element and supports structured release or hold decisions before standard-age test results are available. The approach does not replace established inspection and testing procedures. Instead, it supplements existing workflows by improving traceability and enabling earlier, data-informed quality assessments. Practical considerations related to data integration, contractual constraints, and implementation challenges are also discussed. The proposed framework provides a structured pathway for transitioning construction QA from delayed, document-driven review toward proactive, element-level decision support during construction.

</details>


### [2] [Exploring LLMs for User Story Extraction from Mockups](https://arxiv.org/abs/2602.16997)
*Diego Firmenich,Leandro Antonelli,Bruno Pazos,Fabricio Lozada,Leonardo Morales*

Main category: cs.SE

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: User stories are one of the most widely used artifacts in the software industry to define functional requirements. In parallel, the use of high-fidelity mockups facilitates end-user participation in defining their needs. In this work, we explore how combining these techniques with large language models (LLMs) enables agile and automated generation of user stories from mockups. To this end, we present a case study that analyzes the ability of LLMs to extract user stories from high-fidelity mockups, both with and without the inclusion of a glossary of the Language Extended Lexicon (LEL) in the prompts. Our results demonstrate that incorporating the LEL significantly enhances the accuracy and suitability of the generated user stories. This approach represents a step forward in the integration of AI into requirements engineering, with the potential to improve communication between users and developers.

</details>


### [3] [Not Only for Developers: Exploring Plugin Maintenance for Knowledge-Centric Communities](https://arxiv.org/abs/2602.17018)
*Giovanni Rosa,David Moreno-Lumbreras,Raula Gaikovina Kula*

Main category: cs.SE

TL;DR: 研究非开发者混合社区Obsidian的插件生态系统,通过分析396个插件识别出六大主题,并展示软件进化证据,为生态系统可持续性提供见解。


<details>
  <summary>Details</summary>
Motivation: 针对非开发者社区维护插件生态的挑战,探究Obsidian混合社区如何建立庞大的生态系统。

Method: 使用仓库挖掘和LLM主题建模分析396个代表性插件样本,并进行PR行动分析。

Result: 识别六大主题:动态编辑组织、界面布局、创意写作生产力、知识同步方案、链接脚本工具和工作流增强工具;PR分析证实显著软件进化。

Conclusion: 证明混合社区插件生态可形成工程结构,提出三个未来研究方向及六大问题,聚焦健康与可持续性。

Abstract: The adoption of third-party libraries has become integral to modern software development, leading to large ecosystems such as PyPI, NPM, and Maven, where contributors typically share the technical expertise to sustain extensions. In communities that are not exclusively composed of developers, however, maintaining plugin ecosystems can present different challenges. In this early results paper, we study Obsidian, a knowledge--centric platform whose community is focused on writing, organization, and creativity--has built a substantial plugin ecosystem despite not being developer--centric. We investigate what kinds of plugins exist within this hybrid ecosystem and establish a foundation for understanding how they are maintained. Using repository mining and LLM-based topic modeling on a representative sample of 396 plugins, we identify six topics related to knowledge management and tooling, which is (i) dynamic editing and organization, (ii) interface and layouts, (iii) creative writing and productivity, (iv) knowledge sync solutions, (v) linking and script tools, and (vi) workflow enhancements tools. Furthermore, analysis of the Pull Requests from these plugins show that much software evolution has been performed on these ecosystem. These findings suggest that even in mixed communities, plugin ecosystems can develop recognizable engineering structures, motivating future work that highlight three different research directions with six research questions related to the health and sustainability of these non-developer ecosystems.

</details>


### [4] [Wink: Recovering from Misbehaviors in Coding Agents](https://arxiv.org/abs/2602.17037)
*Rahul Nanda,Chandra Maddila,Smriti Jha,Euna Mehnaz Khan,Matteo Paltenghi,Satish Chandra*

Main category: cs.SE

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Autonomous coding agents, powered by large language models (LLMs), are increasingly being adopted in the software industry to automate complex engineering tasks. However, these agents are prone to a wide range of misbehaviors, such as deviating from the user's instructions, getting stuck in repetitive loops, or failing to use tools correctly. These failures disrupt the development workflow and often require resource-intensive manual intervention. In this paper, we present a system for automatically recovering from agentic misbehaviors at scale. We first introduce a taxonomy of misbehaviors grounded in an analysis of production traffic, identifying three primary categories: Specification Drift, Reasoning Problems, and Tool Call Failures, which we find occur in about 30% of all agent trajectories.
  To address these issues, we developed a lightweight, asynchronous self-intervention system named Wink. Wink observes agent trajectories and provides targeted course-correction guidance to nudge the agent back to a productive path. We evaluated our system on over 10,000 real world agent trajectories and found that it successfully resolves 90% of the misbehaviors that require a single intervention. Furthermore, a live A/B test in our production environment demonstrated that our system leads to a statistically significant reduction in Tool Call Failures, Tokens per Session and Engineer Interventions per Session. We present our experience designing and deploying this system, offering insights into the challenges of building resilient agentic systems at scale.

</details>


### [5] [Multi-Ecosystem Modeling of OSS Project Sustainability](https://arxiv.org/abs/2602.17112)
*Arjun Ashok,Nafiz Imtiaz Khan,Swati Singhvi,Stefan Stanciulescu,Zhouhao Wang,Vladimir Filkov*

Main category: cs.SE

TL;DR: 论文实证分析了开源项目加入Apache、Eclipse便被基金会支持的可持续性，比较不同基金会政策差异，并基于社会技术追踪开发模型和项目分类预测可持续性结果，应用于GitHub非基金会项目。


<details>
  <summary>Details</summary>
Motivation: 基金会政策、资金模式和支撑策略各异，项目和生命周期阶段多样，导致项目匹配基金会和制定可持续发展计划困难，需通过研究定量评估和改进匹配策略。

Method: 对Apache、Eclipse便被基金会孵化项目和GitHub非基金会项目进行实证研究与定量分析，开发基金会特定可持续发展模型及项目分类，基于社会技术追踪配置。

Result: 模型结合分类可有效预测基金会内外可持续性结果，并推广至GitHub项目；辅以恢复策略用于失败项目案例研究。

Conclusion: 研究突显社会技术框架在表征和解决软件项目可持续性问题中的核心价值。

Abstract: Many OSS projects join foundations such as Apache, Eclipse, and OSGeo, to aid their immediate plans and improve long-term prospects by getting governance advice, incubation support, and community-building mechanisms. But foundations differ in their policies, funding models, and support strategies. Moreover, since projects joining these foundations are diverse, coming at different lifecycle stages and having different needs, it can be challenging to decide on the appropriate project-foundation match and on the project-specific plan for sustainability.
  Here, we present an empirical study and quantitative analysis of the sustainability of incubator projects in the Apache, Eclipse, and OSGeo foundations, and, additionally, of OSS projects from GitHub outside of foundations. We develop foundation-specific sustainability models and a project triage, based on projects' sociotechnical trace profiles, and demonstrate their effectiveness across the foundations. Our results show that our models with triage can effectively forecast sustainability outcomes not only within but across foundations. In addition, the generalizability of the framework allows us to apply the approach to GitHub projects outside the foundations. We complement our findings with actionable recovery strategies from previous work and apply them to case studies of failed incubator projects. Our study highlights the value of sociotechnical frameworks in characterizing and addressing software project sustainability issues.

</details>


### [6] [Quantifying Competitive Relationships Among Open-Source Software Projects](https://arxiv.org/abs/2602.17131)
*Yuki Takei,Toshiaki Aoki,Chaiyong Ragkhitwetsagul*

Main category: cs.SE

TL;DR: 本研究提出MIAO方法，量化开源软件(OSS)项目间的竞争关系。该方法以高达81%精度识别因竞争而停止开发的项目，并以77%精度预测一年内停止事件。MIAO有助于理解OSS生态系统动态。


<details>
  <summary>Details</summary>
Motivation: 开源软件生存受竞争关系影响但不清，尤其在快速发展的领域，存在丧失竞争优势的风险。研究旨在量化这些影响并提供预测工具。

Method: 使用结构向量自回归模型和脉冲响应函数分析OSS项目交互，该方法借鉴宏观经济分析技术。在187个OSS项目组的数据挖掘和分析中实施。

Result: MIAO以81%精度识别项目停止开发，生成特征支持一年前置预测实验达77%精度。

Conclusion: MIAO可作为OSS维护者理解生态动态和预测项目兴衰的有价值工具。

Abstract: Throughout the history of software, evolution has occurred in cycles of rise and fall driven by competition, and open-source software (OSS) is no exception. This cycle is accelerating, particularly in rapidly evolving domains such as web development and deep learning. However, the impact of competitive relationships among OSS projects on their survival remains unclear, and there are risks of losing a competitive edge to rivals. To address this, this study proposes a new automated method called ``Mutual Impact Analysis of OSS (MIAO)'' to quantify these competitive relationships. The proposed method employs a structural vector autoregressive model and impulse response functions, normally used in macroeconomic analysis, to analyze the interactions among OSS projects. In an empirical analysis involving mining and analyzing 187 OSS project groups, MIAO identified projects that were forced to cease development owing to competitive influences with up to 81\% accuracy, and the resulting features supported predictive experiments that anticipate cessation one year ahead with up to 77\% accuracy. This suggests that MIAO could be a valuable tool for OSS project maintainers to understand the dynamics of OSS ecosystems and predict the rise and fall of OSS projects.

</details>


### [7] [Robustness and Reasoning Fidelity of Large Language Models in Long-Context Code Question Answering](https://arxiv.org/abs/2602.17183)
*Kishan Maharaj,Nandakishore Menon,Ashita Saxena,Srikanth Tamilselvam*

Main category: cs.SE

TL;DR: 本研究系统评估大语言模型在长代码上下文中的稳健性，发现在多选选项打乱、开卷问题及线索干扰场景下性能显著下滑，并提出了更全面的代码推理基准。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型广泛协助软件工程中长代码上下文任务，但其在不同输入条件下的稳健性尚未明确被确定。

Method: 通过扩展LongCodeBench Python数据集（新增COBOL和Java问答集），进行控制性测试：包括打乱多项选择选项、开放式问题以及“针在草堆”上下文（含相关和无关信息）。

Result: 模型在多选选项打乱和开放性问题中表现大幅下降，在无关线索下行为脆弱。

Conclusion: 研究揭示了当前长上下文评估的局限性，并为新旧系统的代码推理提供了更广泛基准。

Abstract: Large language models (LLMs) increasingly assist software engineering tasks that require reasoning over long code contexts, yet their robustness under varying input conditions remains unclear. We conduct a systematic study of long-context code question answering using controlled ablations that test sensitivity to answer format, distractors, and context scale. Extending LongCodeBench Python dataset with new COBOL and Java question-answer sets, we evaluate state-of-the-art models under three settings: (i) shuffled multiple-choice options, (ii) open-ended questions and (iii) needle-in-a-haystack contexts containing relevant and adversarially irrelevant information. Results show substantial performance drops in both shuffled multiple-choice options and open-ended questions, and brittle behavior in the presence of irrelevant cues. Our findings highlight limitations of current long-context evaluations and provide a broader benchmark for assessing code reasoning in both legacy and modern systems.

</details>


### [8] [The Case for HTML First Web Development](https://arxiv.org/abs/2602.17193)
*Juho Vepsäläinen*

Main category: cs.SE

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Since its introduction in the early 90s, the web has become the largest application platform available globally. HyperText Markup Language (HTML) has been an essential part of the web since the beginning, as it allows defining webpages in a tree-like manner, including semantics and content. Although the web was never meant to be an application platform, it evolved as such, especially since the early 2000s, as web application frameworks became available. While the emergence of frameworks made it easier than ever to develop complex applications, it also put HTML on the back burner. As web standards caught up, especially with milestones such as HTML5, the gap between the web platform and frameworks was reduced. HTML First development emphasizes this shift and puts focus on literally using HTML first when possible, while encouraging minimalism familiar from the early days of the web. It seems HTML-oriented web development can provide clear benefits to developers, especially when it is combined with comple- mentary approaches, such as embracing hypermedia and moving a large part of application logic to the server side. In the context of the htmx project, it was observed that moving towards HTML can reduce the size of a codebase greatly while leading to maintenance and development benefits due to the increased conceptual simplicity. Holotype-based comparisons for content-oriented websites show performance benefits, and the same observation was confirmed by a small case study where the Yle website was converted to follow HTML First principles. In short, the HTML First approach seems to have clear advantages for web developers, while there are open questions related to the magnitude of the benefits and the alignment with the recent trend of AI-driven web development.

</details>


### [9] [Disjunction Composition of BDD Transition Systems for Model-Based Testing](https://arxiv.org/abs/2602.17237)
*Tannaz Zameni,Petra van den Bos,Arend Rensink*

Main category: cs.SE

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: 解决在行为驱动开发中，因文本化需求场景描述碎片化导致难以集成不同行为模型且无法保证测试覆盖率的问题。

Method: 形式化定义析取组合机制，将BDD场景转译为转移系统；通过符号语义证明等价性保证测试一致性，辅以工业案例验证。

Result: 证明析取组合能无损继承原场景测试能力（符号等价确保相同用例失败），工业应用显示具备实践潜力。

Conclusion: 该组合法可高效建模系统集成行为，保持测试完整性，为敏捷测试提供可靠理论基础。

Abstract: We introduce a compositional approach to model-based test generation in Behavior-Driven Development (BDD). BDD is an agile methodology in which system behavior is specified through textual scenarios that, in our approach, are translated into transition systems used for model-based testing. This paper formally defines disjunction composition, to combine BDD transition systems that represent alternative system behaviors. Disjunction composition allows for modeling and testing the integrated behavior while ensuring that the testing power of the original set of scenarios is preserved. This is proved using a symbolic semantics for BDD transition systems, with the property that the symbolic equivalence of two BDD transition systems guarantees that they fail the same test cases. Also, we demonstrate the potential of disjunction composition by applying the composition in an industrial case study.

</details>


### [10] [Socio-Technical Well-Being of Quantum Software Communities: An Overview on Community Smells](https://arxiv.org/abs/2602.17320)
*Stefano Lambiase,Manuel De Stefano,Fabio Palomba,Filomena Ferrucci,Andrea De Lucia*

Main category: cs.SE

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Quantum computing has gained significant attention due to its potential to solve computational problems beyond the capabilities of classical computers. With major corporations and academic institutions investing in quantum hardware and software, there has been a rise in the development of quantum-enabled systems, particularly within open-source communities. However, despite the promising nature of quantum technologies, these communities face critical socio-technical challenges, including the emergence of socio-technical anti-patterns known as community smells. These anti-patterns, prevalent in open-source environments, have the potential to negatively impact both product quality and community health by introducing technical debt and amplifying architectural and code smells. Despite the importance of these socio-technical factors, there remains a scarcity of research investigating their influence within quantum open-source communities. This work aims to address this gap by providing a first step in analyzing the socio-technical well-being of quantum communities through a cross-sectional study. By understanding the socio-technical dynamics at play, it is expected that foundational knowledge can be established to mitigate the risks associated with community smells and ensure the long-term sustainability of open-source quantum initiatives.

</details>


### [11] [The Runtime Dimension of Ethics in Self-Adaptive Systems](https://arxiv.org/abs/2602.17426)
*Marco Autili,Gianluca Filippone,Mashal Afzal Memon,Patrizio Pelliccione*

Main category: cs.SE

TL;DR: 本文主张自适应性系统应从静态伦理规则转向运行时伦理推理，以处理人类伦理偏好的多样性、变迁性及冲突矛盾，确保合法合规框架内的动态道德决策。


<details>
  <summary>Details</summary>
Motivation: 现有方法将伦理编码为固定规则或单一理论，忽视人类互动情境中伦理偏好的个体差异、动态演变和潜在冲突，同时仍需遵守法律硬性约束（如安全和合规）。

Method: 提出将伦理偏好视为运行时要求，在系统运行时基于多维度伦理协商机制实时洞察、表达和修订需求，管理多方道德取舍（如用户、社会和环境驱动的冲突）。

Result: 系统识别出道德不确定性、伦理价值观冲突（涵盖人类、社会和环境影响）及多维多党多驱动谈判挑战，并勾勒了伦理自适应性正好系统研究方向的关键问题。

Conclusion: 结论是实现伦理健全的自适应性系统必须整合运行时伦理推理与协商机制，推动道德贸易取舍研究以适应不断变化的利益相关者场景。

Abstract: Self-adaptive systems increasingly operate in close interaction with humans, often sharing the same physical or virtual environments and making decisions with ethical implications at runtime. Current approaches typically encode ethics as fixed, rule-based constraints or as a single chosen ethical theory embedded at design time. This overlooks a fundamental property of human-system interaction settings: ethical preferences vary across individuals and groups, evolve with context, and may conflict, while still needing to remain within a legally and regulatorily defined hard-ethics envelope (e.g., safety and compliance constraints). This paper advocates a shift from static ethical rules to runtime ethical reasoning for self-adaptive systems, where ethical preferences are treated as runtime requirements that must be elicited, represented, and continuously revised as stakeholders and situations change. We argue that satisfying such requirements demands explicit ethics-based negotiation to manage ethical trade-offs among multiple humans who interact with, are represented by, or are affected by a system. We identify key challenges, ethical uncertainty, conflicts among ethical values (including human, societal, and environmental drivers), and multi-dimensional/multi-party/multi-driver negotiation, and outline research directions and questions toward ethically self-adaptive systems.

</details>


### [12] [Towards a Software Reference Architecture for Natural Language Processing Tools in Requirements Engineering](https://arxiv.org/abs/2602.17498)
*Julian Frattini,Quim Motger*

Main category: cs.SE

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Natural Language Processing (NLP) tools support requirements engineering (RE) tasks like requirements elicitation, classification, and validation. However, they are often developed from scratch despite functional overlaps, and abandoned after publication. This lack of interoperability and maintenance incurs unnecessary development effort, impedes tool comparison and benchmarking, complicates documentation, and diminishes the long-term sustainability of NLP4RE tools. To address these issues, we postulate a vision to transition from monolithic NLP4RE tools to an ecosystem of reusable, interoperable modules. We outline a research roadmap towards a software reference architecture (SRA) to realize this vision, elaborated following a standard methodological framework for SRA development. As an initial step, we conducted a stakeholder-driven focus group session to elicit generic system requirements for NLP4RE tools. This activity resulted in 36 key system requirements, further motivating the need for a dedicated SRA. Overall, the proposed vision, roadmap, and initial contribution pave the way towards improved development, reuse, and long-term maintenance of NLP4RE tools.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [13] [Low-Cost IoT-Enabled Tele-ECG Monitoring for Resource-Constrained Settings: System Design and Prototype](https://arxiv.org/abs/2602.17114)
*Seemron Neupane,Aashish Ghimire*

Main category: cs.AR

TL;DR: 论文探讨自动化机械导致人类懒惰和疾病问题，并提出基于IoT的远程心电图监测系统来改善心血管疾病管理，聚焦操作员、医生和服务器三要素。


<details>
  <summary>Details</summary>
Motivation: 动机在于全球医疗资源匮乏区域需要低成本解决方案，通过早期干预提高心血管疾病治愈率并减少患者出行负担。

Method: 方法涉及开发和实施一个物联网远程ECG监控系统，侧重于操作员协作、医生诊断支持及服务器数据传输机制。

Result: 结果显示该系统可大幅降低就诊成本和旅程开支，赋能患者自我管理，但面临财务和物流挑战。

Conclusion: 结论指出IoT技术可优化慢性病远程监控和即时干预潜力，但需解决现存障碍以普及化。

Abstract: With the availability of automation machinery and its superiority, are being slothful and inviting many diseases to invade them. The world still has so many places where people lack basic health facilities. Due to early detection and intervention, CDV can be cured to an extreme extent. It heavily reduces travel and associated costs. A remote ECG monitoring system enables community health workers to support and empower patients through telemedicine. However, there remains some financial and logistical burden. Heart disease cannot be taken lightly. These patients require regular health check-ups and the attention of health personnel in a short period if their health deteriorates suddenly and rapidly. Chronic diseases are extremely variable in their symptoms and evolution of treatment. Some, if not treated early, will end the patient's life. The trend of the INTERNET OF THINGS, IoT, is spreading massively. This paper focuses on the three main: the operator, the doctor, and the server over which the data is being sent.

</details>


### [14] [When Models Ignore Definitions: Measuring Semantic Override Hallucinations in LLM Reasoning](https://arxiv.org/abs/2602.17520)
*Yogeswar Reddy Thota,Setareh Rafatirad,Homayoun Houman,Tooraj Nikoubin*

Main category: cs.AR

TL;DR: 大型语言模型在标准逻辑任务表现优秀，但在局部语义重定义时出现系统失效，如语义覆盖和假设注入。本研究通过陷阱式测试揭示LLMs无法可靠遵循局部规范，呼吁改进评估协议。


<details>
  <summary>Details</summary>
Motivation: 探究LLMs在形式设置（如电路规范和硬件文档）中处理局部语义重定义时的可靠性不足问题。

Method: 设计包含30项逻辑与数字电路推理任务的微型基准测试，涵盖布尔代数、操作符重载、重新定义门等陷阱场景。

Result: 评估三个前沿LLM时，观察到不符规范、自信但矛盾的假设及忽略约束的错误持续存在。

Conclusion: 表层正确性与基于规范的推理存在差距，推动开发针对局部遗忘和语义遵从的形式领域评估新方法。

Abstract: Large language models (LLMs) demonstrate strong performance on standard digital logic and Boolean reasoning tasks, yet their reliability under locally redefined semantics remains poorly understood. In many formal settings, such as circuit specifications, examinations, and hardware documentation, operators and components are explicitly redefined within narrow scope. Correct reasoning in these contexts requires models to temporarily suppress globally learned conventions in favor of prompt-local definitions. In this work, we study a systematic failure mode we term semantic override, in which an LLM reverts to its pretrained default interpretation of operators or gate behavior despite explicit redefinition in the prompt. We also identify a related class of errors, assumption injection, where models commit to unstated hardware semantics when critical details are underspecified, rather than requesting clarification. We introduce a compact micro-benchmark of 30 logic and digital-circuit reasoning tasks designed as verifier-style traps, spanning Boolean algebra, operator overloading, redefined gates, and circuit-level semantics. Evaluating three frontier LLMs, we observe persistent noncompliance with local specifications, confident but incompatible assumptions, and dropped constraints even in elementary settings. Our findings highlight a gap between surface-level correctness and specification-faithful reasoning, motivating evaluation protocols that explicitly test local unlearning and semantic compliance in formal domains.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [15] [Read-Modify-Writable Snapshots from Read/Write operations](https://arxiv.org/abs/2602.16903)
*Armando Castañeda,Braulio Ramses Hernández Martínez*

Main category: cs.DC

TL;DR: 本文探讨了在异步并发共享内存系统中，仅使用读写操作实现RMWable快照算法的可能性，提出了两种算法分别适用于进程数量有限已知的标准模型和并发无界但活动进程有限的 élèves模型。


<details>
  <summary>Details</summary>
Motivation: 已有RMWable快照算法依赖强大底层操作如compare&swap，而我们旨在探索仅用较弱读写操作能否实现同类功能，以推进分布式系统中基础操作能力的理论研究。

Method: 开发了两种算法：第一种针对n进程数量有限且已知的标准并发共享内存模型；第二种适用于无界并发模型，其中进程数无穷但每个执行中活跃进程有限，两种算法均仅依赖读写操作。

Result: 成功设计并验证了两种仅使用读写操作的RMWable快照算法，证明其在指定模型下可行，无需compare&swap等高级操作。

Conclusion: 研究突破了读写操作的限制，拓展了RMWable快照在弱一致性分布式系统中的适用范围，为系统优化提供了新思路。

Abstract: In the context of asynchronous concurrent shared-memory systems, a snapshot algorithm allows failure-prone processes to concurrently and atomically write on the entries of a shared array MEM , and also atomically read the whole array. Recently, Read-Modify-Writable (RMWable) snapshot was proposed, a variant of snapshot that allows processes to perform operations more complex than just read and write, specifically, each entry MEM[k] is an arbitrary readable object. The known RMWable snapshot algorithms heavily rely on powerful low-level operations such as compare&swap or load-link/store-conditional to correctly produce snapshots of MEM. Following the large body of research devoted to understand the limits of what can be solved using the simple read/write low-level operations, which are known to be strictly weaker than compare&swap and load-link/store-conditional, we explore if RMWable snapshots are possible using only read/write operations. We present two read/write RMWable snapshot algorithms, the first one in the standard concurrent shared-memory model where the number of processes n is finite and known in advance, and the second one in a variant of the standard model with unbounded concurrency, where there are infinitely many processes, but at any moment only finitely many processes participate in an execution.

</details>


### [16] [Visual Insights into Agentic Optimization of Pervasive Stream Processing Services](https://arxiv.org/abs/2602.17282)
*Boris Sedlak,Víctor Casamayor Pujol,Schahram Dustdar*

Main category: cs.DC

TL;DR: 提出用于边缘流处理服务的情境感知自动扩展平台及智能体优化方法，解决资源波动和协调问题。


<details>
  <summary>Details</summary>
Motivation: 解决智能城市等应用在边缘设备上资源波动、服务动作各异且缺乏协调导致资源争抢的三个关键问题。

Method: 开发平台让开发者监控并调整多参数服务执行，引入智能体通过探索各服务动作空间学习环境并优化执行。

Result: 智能体构建环境理解后优化服务运行，参与者可通过视频总结和仓库扩展定制测试。

Conclusion: 系统支持动态扩展服务以适应波动需求，提升效率并提供可扩展实践框架。

Abstract: Processing sensory data close to the data source, often involving Edge devices, promises low latency for pervasive applications, like smart cities. This commonly involves a multitude of processing services, executed with limited resources; this setup faces three problems: first, the application demand and the resource availability fluctuate, so the service execution must scale dynamically to sustain processing requirements (e.g., latency); second, each service permits different actions to adjust its operation, so they require individual scaling policies; third, without a higher-level mediator, services would cannibalize any resources of services co-located on the same device. This demo first presents a platform for context-aware autoscaling of stream processing services that allows developers to monitor and adjust the service execution across multiple service-specific parameters. We then connect a scaling agent to these interfaces that gradually builds an understanding of the processing environment by exploring each service's action space; the agent then optimizes the service execution according to this knowledge. Participants can revisit the demo contents as video summary and introductory poster, or build a custom agent by extending the artifact repository.

</details>


### [17] [Heterogeneous Federated Fine-Tuning with Parallel One-Rank Adaptation](https://arxiv.org/abs/2602.16936)
*Zikai Zhang,Rui Hu,Jiahao Xu*

Main category: cs.DC

TL;DR: 提出Fed-PLoRA框架解决联邦学习中因客户端异构资源导致LoRA适配效果差的问题，通过并行单秩模块和选择折叠策略提升精度与效率。


<details>
  <summary>Details</summary>
Motivation: 联邦学习中客户端异构资源引发LoRA秩差异，产生初始化与聚合噪声降低性能，需适应性优化方案。

Method: 提出PLo trastqjiangshu7替换传统多秩模块为多个并行单秩模块，配合Select-N-Fold将未训练模块折叠至预训练权重以适应资源差异。

Result: 多任务实验验证Fed-PLoRA在精度与效率上均超越现有方法，兼具协同训练效能。

Conclusion: Fed告PLoRA有效克服异构联邦微调挑战，代码已开源@ https://github.com/TNI-playground/Fed-PLoRA equiv

Abstract: Large Language Models (LLMs) have demonstrated remarkable effectiveness in adapting to downstream tasks through fine-tuning. Federated Learning (FL) extends this capability by enabling collaborative fine-tuning across distributed clients using Low-Rank Adaptation (LoRA), while preserving data privacy by avoiding raw data sharing. However, practical deployments face challenges when clients have heterogeneous resources and thus adopt different LoRA ranks, leading to substantial initialization and aggregation noise that undermines performance. To address these challenges, we propose Fed-PLoRA, a novel lightweight heterogeneous federated fine-tuning (FFT) framework. Fed-PLoRA introduces Parallel One-Rank Adaptation (PLoRA), a new LoRA variant that replaces the classic multi-rank LoRA module with multiple parallel one-rank modules, and a novel Select-N-Fold strategy that folds untrained PLoRA modules into the pre-trained weights before local training, thereby accommodating heterogeneous client resources. We provide a unified analysis of initialization and aggregation noise of Fed-PLoRA and demonstrate how it addresses the limitations of state-of-the-art methods. Extensive experiments on diverse LLM fine-tuning tasks demonstrate that Fed-PLoRA consistently outperforms existing methods in both accuracy and efficiency. The code is available at https://github.com/TNI-playground/Fed-PLoRA.

</details>


### [18] [Evaluating Malleable Job Scheduling in HPC Clusters using Real-World Workloads](https://arxiv.org/abs/2602.17318)
*Patrick Zojer,Jonas Posner,Taylan Özden*

Main category: cs.DC

TL;DR: 弹性资源分配在高性能计算中显著提升系统效能：通过动态调整可扩展作业资源，关键指标优化最高达99%，节点利用率提升达52%。


<details>
  <summary>Details</summary>
Motivation: 传统HPC作业调度存在资源利用率低、作业等待时间长的问题，研究需探索弹性资源分配对系统性能的提升潜力。

Method: 使用Cori/Eagle/Theta超算的真实工作负载，通过ElastiSim软件模拟0-100%可扩展作业比例，评估含新型策略在内的5种调度算法。

Result: 最佳策略下：作业周转时间降37-67%，作业完成周期降16-65%，等待时间降73-99%，节点利用率升5-52%；20%可扩展作业比例即产生显著增益。

Conclusion: 作业特性与-playing性比例存在强关联性；引入弹性资源管理可有效优化HPC系统，有限采用即能带来实质性收益，建议集成进资源管理实践。

Abstract: Optimizing resource utilization in high-performance computing (HPC) clusters is essential for maximizing both system efficiency and user satisfaction. However, traditional rigid job scheduling often results in underutilized resources and increased job waiting times.
  This work evaluates the benefits of resource elasticity, where the job scheduler dynamically adjusts the resource allocation of malleable jobs at runtime. Using real workload traces from the Cori, Eagle, and Theta supercomputers, we simulate varying proportions (0-100%) of malleable jobs with the ElastiSim software.
  We evaluate five job scheduling strategies, including a novel one that maintains malleable jobs at their preferred resource allocation when possible. Results show that, compared to fully rigid workloads, malleable jobs yield significant improvements across all key metrics. Considering the best-performing scheduling strategy for each supercomputer, job turnaround times decrease by 37-67%, job makespan by 16-65%, job wait times by 73-99%, and node utilization improves by 5-52%. Although improvements vary, gains remain substantial even at 20% malleable jobs.
  This work highlights important correlations between workload characteristics (e.g., job runtimes and node requirements), malleability proportions, and scheduling strategies. These findings confirm the potential of malleability to address inefficiencies in current HPC practices and demonstrate that even limited adoption can provide substantial advantages, encouraging its integration into HPC resource management.

</details>


### [19] [TopoSZp: Lightweight Topology-Aware Error-controlled Compression for Scientific Data](https://arxiv.org/abs/2602.17552)
*Tripti Agarwal,Sheng Di,Xin Liang,Zhaoyuan Su,Yuxiao Li,Ganesh Gopalakrishnan,Hanqi Guo,Franck Cappello*

Main category: cs.DC

TL;DR: 本文提出了TopoSZp，一种轻量级拓扑感知的误差控制有损压缩器，有效保留关键点（如最小值、最大值和鞍点）并显著提高压缩和解压性能。


<details>
  <summary>Details</summary>
Motivation: 现有压缩器（如SZ和ZFP）无法保留关键拓扑结构，而现有拓扑感知压缩器计算开销过大，难以满足大规模HPC模拟数据处理需求。

Method: 基于高效SZp压缩器，整合临界点检测、局部顺序保留和定向鞍点细化技术，在放松但严格错误边界下运行。

Result: 实验显示，相比其他拓扑感知压缩器，TopoSZp非保留临界点减少3至100倍，无假阳性或错误类型，压缩速度快100至10000倍，解压速度快10至500倍，压缩率保持竞争力。

Conclusion: TopoSZp高效保护拓扑结构并提升性能，为科学数据分析提供了实用解决方案。 فاقد

Abstract: Error-bounded lossy compression is essential for managing the massive data volumes produced by large-scale HPC simulations. While state-of-the-art compressors such as SZ and ZFP provide strong numerical error guarantees, they often fail to preserve topological structures (example, minima, maxima, and saddle points) that are critical for scientific analysis. Existing topology-aware compressors address this limitation but incur substantial computational overhead. We present TopoSZp, a lightweight, topology-aware, error-controlled lossy compressor that preserves critical points and their relationships while maintaining high compression and decompression performance. Built on the high-throughput SZp compressor, TopoSZp integrates efficient critical point detection, local ordering preservation, and targeted saddle point refinement, all within a relaxed but strictly enforced error bound. Experimental results on real-world scientific datasets show that TopoSZp achieves 3 to 100 times fewer non-preserved critical points, introduces no false positives or incorrect critical point types, and delivers 100 to 10000 times faster compression and 10 to 500 times faster decompression compared to existing topology-aware compressors, while maintaining competitive compression ratios.

</details>


### [20] [Exploring Novel Data Storage Approaches for Large-Scale Numerical Weather Prediction](https://arxiv.org/abs/2602.17610)
*Nicolau Manubens Gil*

Main category: cs.DC

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Driven by scientific and industry ambition, HPC and AI applications such as operational Numerical Weather Prediction (NWP) require processing and storing ever-increasing data volumes as fast as possible. Whilst POSIX distributed file systems and NVMe SSDs are currently a common HPC storage configuration providing I/O to applications, new storage solutions have proliferated or gained traction over the last decade with potential to address performance limitations POSIX file systems manifest at scale for certain I/O workloads.
  This work has primarily aimed to assess the suitability and performance of two object storage systems -namely DAOS and Ceph- for the ECMWF's operational NWP as well as for HPC and AI applications in general. New software-level adapters have been developed which enable the ECMWF's NWP to leverage these systems, and extensive I/O benchmarking has been conducted on a few computer systems, comparing the performance delivered by the evaluated object stores to that of equivalent Lustre file system deployments on the same hardware. Challenges of porting to object storage and its benefits with respect to the traditional POSIX I/O approach have been discussed and, where possible, domain-agnostic performance analysis has been conducted, leading to insight also of relevance to I/O practitioners and the broader HPC community.
  DAOS and Ceph have both demonstrated excellent performance, but DAOS stood out relative to Ceph and Lustre, providing superior scalability and flexibility for applications to perform I/O at scale as desired. This sets a promising outlook for DAOS and object storage, which might see greater adoption at HPC centres in the years to come, although not necessarily implying a shift away from POSIX-like I/O.

</details>


<div id='cs.PF'></div>

# cs.PF [[Back]](#toc)

### [21] [GDEV-AI: A Generalized Evaluation of Deep Learning Inference Scaling and Architectural Saturation](https://arxiv.org/abs/2602.16858)
*Kathiravan Palaniappan*

Main category: cs.PF

TL;DR: 研究对比新旧CPU在CNN推理中的可扩展性，揭示遗产CPU吞吐饱和快且扩展受限，现代CPU利用AMX提升性能但面临争用问题，提出GDEV-AI基准框架优化异构数据中心规划。


<details>
  <summary>Details</summary>
Motivation: 由于CPU推理在遗产数据中心和成本敏感环境中仍盛行，阐明其可扩展性瓶颈对提升吞吐、延迟和硬件效率至关重要。

Method: 通过对ResNet模型在不同批次大小下进行基准测试，比较旧Intel Xeon E5-2403 v2和新Intel Xeon 6 'Granite Rapids'平台的性能。

Result: 遗产CPU因指令级和内存限制快速达到吞吐饱和；现代CPU借助AMX提升吞吐，但超订阅物理核心引发执行争用和尾延迟放大，暴露性能退化问题。

Conclusion: 引入GDEV-AI作为可重现基准框架，提供架构饱和分析的厂商中立基线，可识别瓶颈并指导异构数据中心容量规划。

Abstract: The deployment of deep learning inference in production environments continues to grow, where throughput, latency, and hardware efficiency are critical. Although specialized accelerators are increasingly adopted, many inference workloads still run on CPU-only systems, particularly in legacy data centers and cost-sensitive environments. This study investigates the scalability limits of CPU-based inference for convolutional neural networks by benchmarking ResNet models across varying batch sizes on two hardware tiers: a legacy Intel Xeon E5-2403 v2 processor and a modern Intel Xeon 6 "Granite Rapids" platform.
  Results show that legacy CPUs quickly reach throughput saturation, with limited scaling beyond small batch sizes due to instruction-level and memory constraints. In contrast, the Granite Rapids system leverages Intel Advanced Matrix Extensions (AMX) to achieve substantially higher throughput. However, oversubscription beyond physical core limits introduces execution contention and tail-latency amplification, revealing a performance degradation regime in modern architectures.
  We introduce GDEV-AI, a reproducible benchmarking framework for analyzing scalability behavior and architectural saturation in CPU-based inference. By establishing a vendor-neutral baseline, this work provides empirical insight into performance bottlenecks and informs capacity planning in heterogeneous data center environments.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [22] [HyRA: A Hybrid Resource Allocation Framework for RAN Slicing](https://arxiv.org/abs/2602.16952)
*Mohammad Zangooei,Bo Sun,Noura Limam,Raouf Boutaba*

Main category: cs.NI

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: The advent of 5G and the emergence of 6G networks demand unprecedented flexibility and efficiency in Radio Access Network (RAN) resource management to satisfy diverse service-level agreements (SLAs). Existing RAN slicing frameworks predominantly rely on per-slice resource reservation, which ensures performance isolation but leads to inefficient utilization, particularly under bursty traffic. We introduce HyRA, a hybrid resource allocation framework for RAN slicing that combines dedicated per-slice allocations with shared resource pooling across slices. HyRA preserves performance isolation while improving resource efficiency by leveraging multiplexing gains in bursty traffic conditions. We formulate this design as a bi-level stochastic optimization problem, where the outer loop determines the dedicated and shared resource budgets and the inner loop performs per-UE scheduling under a novel water-filling approach. By using the sample-average approximation, the Karush-Kuhn-Tucker (KKT) conditions of the inner loop, and Big-M encoding, we transform the problem into a tractable mixed-integer program that standard optimization solvers can solve. Extensive simulations under diverse demand patterns, SLA configurations, and traffic burstiness show that HyRA achieves up to 50-75% spectrum savings compared to dedicated-only and shared-only baselines. These results highlight HyRA as a viable approach for resource-efficient, SLA-compliant RAN slicing in future mobile networks.

</details>


### [23] [Robust and Extensible Measurement of Broadband Plans with BQT+](https://arxiv.org/abs/2602.16969)
*Laasya Koduru,Sylee Beltiukov,Alexander Nguyen,Eugene Vuong,Jaber Daneshamooz,Tejas Narechania,Elizabeth Belding,Arpit Gupta*

Main category: cs.NI

TL;DR: BQT+框架通过声明式规范和NFA建模改进宽带数据收集，支持政策评估如BEAD计划。


<details>
  <summary>Details</summary>
Motivation: 宽带基础设施投资评估需要地址级的纵向数据，包括可用性、质量和可负担性，但现有系统难以满足稳健性、可扩展性和低技术负担的要求。

Method: 采用声明式状态/操作规范代替整体工作流，将查询意图建模为抽象非确定有限自动机状态空间，并通过运行时路径选择适应接口变化和交互流程。

Result: BQT+实现对64家ISP的长期监控，支持100多家ISP查询，并应用于构建BEAD拨款基准和四个州124,000多个地址的宽带负担能力基准测试。

Conclusion: BQT+有效填补了现有系统的不足，为宽带政策研究提供了高效可靠的工具。

Abstract: Independent, street address-level broadband data is essential for evaluating Internet infrastructure investments, such as the $42B Broadband Equity, Access, and Deployment (BEAD) program. Evaluating these investments requires longitudinal visibility into broadband availability, quality, and affordability, including data on pre-disbursement baselines and changes in providers' advertised plans. While such data can be obtained through Internet Service Provider (ISP) web interfaces, these workloads impose three fundamental system requirements: robustness to frequent interface evolution, extensibility across hundreds of providers, and low technical overhead for non-expert users. Existing systems fail to meet these three essential requirements.
  We present BQT+, a broadband plan measurement framework that replaces monolithic workflows with declarative state/action specifications. BQT+ models querying intent as an interaction state space, formalized as an abstract nondeterministic finite automaton (NFA), and selects execution paths at runtime to accommodate alternative interaction flows and localized interface changes. We show that BQT+ sustains longitudinal monitoring of 64 ISPs, supporting querying for over 100 ISPs. We apply it to two policy studies: constructing a BEAD pre-disbursement baseline and benchmarking broadband affordability across over 124,000 addresses in four states.

</details>


### [24] [ACOS: Arrays of Cheap Optical Switches](https://arxiv.org/abs/2602.17449)
*Daniel Amir,Ori Cohen,Jakob Krebs,Mark Silberstein*

Main category: cs.NI

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Machine learning training places immense demands on cluster networks, motivating specialized architectures and co-design with parallelization strategies. Recent designs incorporating optical circuit switches (OCSes) are promising, offering improved cost, power efficiency, and long-term bandwidth scaling than packet switches. However, most existing approaches rely on costly high-radix OCSes and/or combine them with packet switches to achieve competitive performance at scale. Unfortunately, high-radix OCSes are both expensive and slow to reconfigure, limiting both scalability and performance.
  We propose Arrays of Cheap Optical Switches (ACOS), which bring application co-design directly to the structure of the reconfigurable fabric. Using low-radix OCSes as building blocks, ACOS supports the forms of reconfiguration needed in training clusters including topology selection, workload adaptation, and failure resilience. The cost of ACOS scales with supported topologies and adaptations rather than with port count, breaking past the scalability barriers of current specialized ML networks. We show through simulation that ACOS-based deployments match the performance of fully provisioned packet-switched networks when training state-of-the-art LLMs at scale, while delivering significant cost savings using existing off-the-shelf OCSes, with strong bandwidth scaling and higher cost savings in the future.

</details>


### [25] [HAP Networks for the Future: Applications in Sensing, Computing, and Communication](https://arxiv.org/abs/2602.17534)
*Sultan Çoğay,T. Tolga Sari,Muhammad Nadeem Ali,Byung-Seo Kim,Gökhan Seçinti*

Main category: cs.NI

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: High Altitude Platforms (HAPs) are a major advancement in non-terrestrial networks, offering broad coverage and unique capabilities. They form a vital link between satellite systems and terrestrial networks and play a key role in next-generation communication technologies. This study reviews HAP network applications, focusing on advanced airborne communications, integrated sensing, and airborne informatics. Our survey assesses the current state of HAP-centric applications by examining data processing, network performance, computational and storage requirements, economic feasibility, and regulatory challenges. The analysis highlights the evolving role of HAPs in global communication and identifies future research directions to support their deployment.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [26] [Guiding LLM-Based Human Mobility Simulation with Mobility Measures from Shared Data](https://arxiv.org/abs/2602.16726)
*Hua Yan,Heng Tan,Yu Yang*

Main category: cs.MA

TL;DR: 提出了M2LSimu框架，使用移动指标引导的多prompt调整，通过协调群体行为改进大规模移动模拟。


<details>
  <summary>Details</summary>
Motivation: 现有基于大语言模型►▼的方法独立生成个体移动轨迹，缺乏群体级协调机制，无法捕捉集体行为涌现。

Method: 利用共享数据中的移动指标作为指导，设计渐进式调整策略，通过粗粒度调整和细粒度个体适应，在有限预算下满足群体级目标。

Result: 在两个公开数据集上实验，性能显著超过现有的LLM-based方法。

Conclusion: M2LSimu框架有效整合群体指导和个体适配，实现了更真实且协调的移动轨迹生成。

Abstract: Large-scale human mobility simulation is critical for many science domains such as urban science, epidemiology, and transportation analysis. Recent works treat large language models (LLMs) as human agents to simulate realistic mobility trajectories by modeling individual-level cognitive processes. However, these approaches generate individual mobility trajectories independently, without any population-level coordination mechanism, and thus fail to capture the emergence of collective behaviors. To address this issue, we design M2LSimu, a mobility measures-guided multi-prompt adjustment framework that leverages mobility measures derived from shared data as guidance to refine individual-level prompts for realistic mobility generation. Our framework applies coarse-grained adjustment strategies guided by mobility measures, progressively enabling fine-grained individual-level adaptation while satisfying multiple population-level mobility objectives under a limited budget. Experiments show that M2LSimu significantly outperforms state-of-the-art LLM-based methods on two public datasets.

</details>


### [27] [Self-Evolving Multi-Agent Network for Industrial IoT Predictive Maintenance](https://arxiv.org/abs/2602.16738)
*Rebin Saleh,Khanh Pham Dinh,Balázs Villányi,Truong-Son Hy*

Main category: cs.MA

TL;DR: 提出分层多智能体系统SEMAS，在工业物联网预测性维护中实现低延迟异常检测与高可解释性。


<details>
  <summary>Details</summary>
Motivation: 传统静态模型无法适应动态工业环境，LLM模型资源消耗过高，难以在边缘端实时部署。

Method: 构建边-雾-云三层架构：边缘节点轻量特征提取，雾节点集成检测与动态共识投票，云端通过PPO持续优化策略，结合LLM解释与联邦知识聚合。

Result: 在两个工业基准测试中展现优异的异常检测稳定性与适应性，延迟显著降低，消融实验验证PPO进化、共识投票和联邦聚合的关键作用。

Conclusion: 资源感知的自进化多智能体协同是满足工业物联网严格延迟与可解释性要求的核心解决方案。

Abstract: Industrial IoT predictive maintenance requires systems capable of real-time anomaly detection without sacrificing interpretability or demanding excessive computational resources. Traditional approaches rely on static, offline-trained models that cannot adapt to evolving operational conditions, while LLM-based monolithic systems demand prohibitive memory and latency, rendering them impractical for on-site edge deployment. We introduce SEMAS, a self-evolving hierarchical multi-agent system that distributes specialized agents across Edge, Fog, and Cloud computational tiers. Edge agents perform lightweight feature extraction and pre-filtering; Fog agents execute diversified ensemble detection with dynamic consensus voting; and Cloud agents continuously optimize system policies via Proximal Policy Optimization (PPO) while maintaining asynchronous, non-blocking inference. The framework incorporates LLM-based response generation for explainability and federated knowledge aggregation for adaptive policy distribution. This architecture enables resource-aware specialization without sacrificing real-time performance or model interpretability. Empirical evaluation on two industrial benchmarks (Boiler Emulator and Wind Turbine) demonstrates that SEMAS achieves superior anomaly detection performance with exceptional stability under adaptation, sustains prediction accuracy across evolving operational contexts, and delivers substantial latency improvements enabling genuine real-time deployment. Ablation studies confirm that PPO-driven policy evolution, consensus voting, and federated aggregation each contribute materially to system effectiveness. These findings indicate that resource-aware, self-evolving 1multi-agent coordination is essential for production-ready industrial IoT predictive maintenance under strict latency and explainability constraints.

</details>


### [28] [AdaptOrch: Task-Adaptive Multi-Agent Orchestration in the Era of LLM Performance Convergence](https://arxiv.org/abs/2602.16873)
*Geunbin Yu*

Main category: cs.MA

TL;DR: 论文提出AdaptOrch框架，论证在多智能体系统中，协作拓扑结构比单个模型能力对性能影响更大。


<details>
  <summary>Details</summary>
Motivation: 当不同大模型在基准测试中趋近相似性能时，单一模型选择方案回报递减，系统级性能取决于如何协调多智能体的拓扑结构。

Method: 开发动态拓扑选择框架：1) 形式化性能收敛定律；2) 基于任务依赖图的时间复杂度O(|V|+|E|)的路由算法；3) 带终止保证的自适应َل合成协议。

Result: 在编程、推理和检索增强任务中，拓扑感知协作较静态基线提升12-23%性能（相同模型前提下）。

Conclusion: 协作设计应成为独立于模型缩放的核心优化目标，推动人工智能系统设计范式的转变。

Abstract: As large language models from diverse providers converge toward comparable benchmark performance, the traditional paradigm of selecting a single best model per task yields diminishing returns. We argue that orchestration topology -- the structural composition of how multiple agents are coordinated, parallelized, and synthesized -- now dominates system-level performance over individual model capability. We present AdaptOrch, a formal framework for task-adaptive multi-agent orchestration that dynamically selects among four canonical topologies (parallel, sequential, hierarchical, and hybrid) based on task dependency graphs and empirically derived domain characteristics. Our framework introduces three key contributions: (1) a Performance Convergence Scaling Law, formalizing conditions under which orchestration selection outweighs model selection; (2) a Topology Routing Algorithm that maps task decomposition DAGs to optimal orchestration patterns in O(|V| + |E|) time; and (3) an Adaptive Synthesis Protocol with provable termination guarantees and heuristic consistency scoring for parallel agent outputs. We validate AdaptOrch across coding (SWE-bench), reasoning (GPQA), and retrieval-augmented generation tasks, demonstrating that topology-aware orchestration achieves 12-23% improvement over static single-topology baselines, even when using identical underlying models. Our results establish orchestration design as a first-class optimization target independent of model scaling.

</details>


### [29] [AgentConductor: Topology Evolution for Multi-Agent Competition-Level Code Generation](https://arxiv.org/abs/2602.17100)
*Siyu Wang,Ruotian Lu,Zhihao Yang,Yuchao Wang,Yanzhou Zhang,Lei Xu,Qimin Xu,Guojun Yin,Cailian Chen,Xinping Guan*

Main category: cs.MA

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Large language model(LLM)-driven multi-agent systems(MAS) coordinate specialized agents through predefined interaction topologies and have shown promise for complex tasks such as competition-level code generation. Recent studies demonstrate that carefully designed multi-agent workflows and communication graphs can significantly improve code generation performance by leveraging collaborative reasoning. However, existing methods neither adapt topology density to task difficulty nor iteratively refine the topology within an instance using execution feedback, which leads to redundant communication and performance bottlenecks. To address these issues, we propose AgentConductor: a reinforcement learning-optimized MAS with an LLM-based orchestrator agent as its core, which enables end-to-end feedback-driven dynamic generation of interaction topologies. For each query, AgentConductor infers agent roles and task difficulty, then constructs a task-adapted, density-aware layered directed acyclic graph (DAG) topology, underpinned by two key innovations. First, we design a novel topological density function that captures communication-aware mathematical characterizations of multi-agent interactions. Second, we adopt difficulty interval partitioning to avoid excessive pruning for precise topological density upper bound measurement per difficulty level and finer-grained control. Empirically, across three competition-level and two foundational code datasets, AgentConductor achieves state-of-the-art accuracy, outperforming the strongest baseline by up to 14.6% in pass@1 accuracy, 13% in density reduction, and 68% in token cost reduction.

</details>


### [30] [Algorithmic Collusion at Test Time: A Meta-game Design and Evaluation](https://arxiv.org/abs/2602.17203)
*Yuhong Luo,Daniel Schoepflin,Xintong Wang*

Main category: cs.MA

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: The threat of algorithmic collusion, and whether it merits regulatory intervention, remains debated, as existing evaluations of its emergence often rely on long learning horizons, assumptions about counterparty rationality in adopting collusive strategies, and symmetry in hyperparameters and economic settings among players. To study collusion risk, we introduce a meta-game design for analyzing algorithmic behavior under test-time constraints. We model agents as possessing pretrained policies with distinct strategic characteristics (e.g., competitive, naively cooperative, robustly collusive), and formulate the problem as selecting a meta-strategy that combines a pretrained, initial policy with an in-game adaptation rule. We seek to examine whether collusion can emerge under rational choices and how agents co-adapt toward cooperation or competition. To this end, we sample normal-form empirical games over meta-strategy profiles, % across random initial game states, compute relevant game statistics (e.g., payoffs against individuals and regret against an equilibrium mixture of opponents), and construct empirical best-response graphs to uncover strategic relationships. We evaluate both reinforcement-learning and LLM-based strategies in repeated pricing games under symmetric and asymmetric cost settings, and present findings on the feasibility of algorithmic collusion and the effectiveness of pricing strategies in practical ``test-time'' environments.
  The source code and the full paper with appendix are available at: https://github.com/chailab-rutgers/CollusionMetagame.

</details>
