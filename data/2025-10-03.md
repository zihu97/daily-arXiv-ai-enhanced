<div id=toc></div>

# Table of Contents

- [cs.MA](#cs.MA) [Total: 2]
- [cs.NI](#cs.NI) [Total: 1]
- [cs.SE](#cs.SE) [Total: 14]
- [cs.AR](#cs.AR) [Total: 2]
- [cs.DC](#cs.DC) [Total: 6]


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [1] [LLM-based Multi-Agent Blackboard System for Information Discovery in Data Science](https://arxiv.org/abs/2510.01285)
*Alireza Salemi,Mihir Parmar,Palash Goyal,Yiwen Song,Jinsung Yoon,Hamed Zamani,Hamid Palangi,Tomas Pfister*

Main category: cs.MA

TL;DR: 提出基于黑板架构的多智能体通信范式，用于解决大型异构数据湖中的数据发现问题，相比传统方法显著提升任务成功率和F1分数


<details>
  <summary>Details</summary>
Motivation: 现有单智能体系统难以处理大型异构数据湖，而主从式多智能体系统需要中央控制器精确了解每个子智能体的能力，缺乏灵活性和可扩展性

Method: 采用黑板架构，中央智能体将请求发布到共享黑板，自主子智能体根据自身能力自愿响应，无需中央协调器预先了解所有子智能体的专业知识

Result: 在三个基准测试上显著优于基线方法（包括RAG和主从式多智能体范式），端到端任务成功率相对提升13%-57%，数据发现F1分数相对提升最高9%

Conclusion: 黑板范式为多智能体系统提供了一个可扩展和可泛化的通信框架

Abstract: The rapid advancement of Large Language Models (LLMs) has opened new
opportunities in data science, yet their practical deployment is often
constrained by the challenge of discovering relevant data within large
heterogeneous data lakes. Existing methods struggle with this: single-agent
systems are quickly overwhelmed by large, heterogeneous files in the large data
lakes, while multi-agent systems designed based on a master-slave paradigm
depend on a rigid central controller for task allocation that requires precise
knowledge of each sub-agent's capabilities. To address these limitations, we
propose a novel multi-agent communication paradigm inspired by the blackboard
architecture for traditional AI models. In this framework, a central agent
posts requests to a shared blackboard, and autonomous subordinate agents --
either responsible for a partition of the data lake or general information
retrieval -- volunteer to respond based on their capabilities. This design
improves scalability and flexibility by eliminating the need for a central
coordinator to have prior knowledge of all sub-agents' expertise. We evaluate
our method on three benchmarks that require explicit data discovery: KramaBench
and modified versions of DS-Bench and DA-Code to incorporate data discovery.
Experimental results demonstrate that the blackboard architecture substantially
outperforms baselines, including RAG and the master-slave multi-agent paradigm,
achieving between 13% to 57% relative improvement in end-to-end task success
and up to a 9% relative gain in F1 score for data discovery over the
best-performing baselines across both proprietary and open-source LLMs. Our
findings establish the blackboard paradigm as a scalable and generalizable
communication framework for multi-agent systems.

</details>


### [2] [SimCity: Multi-Agent Urban Development Simulation with Rich Interactions](https://arxiv.org/abs/2510.01297)
*Yeqi Feng,Yucheng Lu,Hongyu Su,Tianxing He*

Main category: cs.MA

TL;DR: SimCity是一个基于大语言模型的多智能体宏观经济模拟框架，通过自然语言推理实现异构代理和丰富交互，能够重现经典宏观经济现象。


<details>
  <summary>Details</summary>
Motivation: 传统均衡模型因可处理性限制而限制异质性，传统基于代理的模型依赖手工制定的决策规则，需要更灵活、自适应且具有透明推理的宏观经济模拟方法。

Method: 使用LLMs构建四种核心代理类型（家庭、企业、中央银行和政府），在摩擦劳动力市场、异质商品市场和金融市场中进行交互，并利用视觉语言模型确定企业地理位置和渲染虚拟城市。

Result: 框架能够自然重现价格需求弹性、恩格尔定律、奥肯定律、菲利普斯曲线和贝弗里奇曲线等经典宏观经济现象，并在多次模拟运行中保持稳健性。

Conclusion: SimCity展示了LLMs在构建现实且可解释的宏观经济模拟方面的潜力，为研究宏观经济规律和城市扩张动态提供了统一环境。

Abstract: Large Language Models (LLMs) open new possibilities for constructing
realistic and interpretable macroeconomic simulations. We present SimCity, a
multi-agent framework that leverages LLMs to model an interpretable
macroeconomic system with heterogeneous agents and rich interactions. Unlike
classical equilibrium models that limit heterogeneity for tractability, or
traditional agent-based models (ABMs) that rely on hand-crafted decision rules,
SimCity enables flexible, adaptive behavior with transparent natural-language
reasoning. Within SimCity, four core agent types (households, firms, a central
bank, and a government) deliberate and participate in a frictional labor
market, a heterogeneous goods market, and a financial market. Furthermore, a
Vision-Language Model (VLM) determines the geographic placement of new firms
and renders a mapped virtual city, allowing us to study both macroeconomic
regularities and urban expansion dynamics within a unified environment. To
evaluate the framework, we compile a checklist of canonical macroeconomic
phenomena, including price elasticity of demand, Engel's Law, Okun's Law, the
Phillips Curve, and the Beveridge Curve, and show that SimCity naturally
reproduces these empirical patterns while remaining robust across simulation
runs.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [3] [MMGaP: Multi-User MIMO Detection and Precoding using GPU-assisted Physics-inspired Computation](https://arxiv.org/abs/2510.01579)
*Abhishek Kumar Singh,Kyle Jamieson*

Main category: cs.NI

TL;DR: MMGaP是一个基于CUDA内核的大规模MIMO检测器和预编码器，首次在GPU平台上实现了物理层处理算法，显著提升了5G网络的上下行吞吐量。


<details>
  <summary>Details</summary>
Motivation: 填补物理层量子计算和物理启发方法在商用处理器上实际实现的空白，解决理论先进性与实际系统吞吐量之间的差距。

Method: 开发基于裸金属CUDA内核的大规模MIMO处理算法，可打包为TensorFlow模块，并与NVIDIA软件定义的GPU加速5G平台集成。

Result: 在100MHz带宽、8天线8用户的5G网络中，MMGaP使上行吞吐量每用户提升约50Mbps，下行吞吐量每用户提升100Mbps；在16天线16用户场景下，上行吞吐量每用户提升超过50Mbps。

Conclusion: MMGaP能够满足最先进5G系统的时序要求，在线速率下运行，为下一代蜂窝网络提供了实用的大规模MIMO处理解决方案。

Abstract: Physics-inspired and quantum compute based methods for processing in the
physical layer of next-generation cellular radio access networks have
demonstrated theoretical advances in spectral efficiency in recent years, but
have stopped short of practical realization on commodity processors, leaving a
gap between the throughput practical systems can achieve and the projected
throughput the state-of-the-art should achieve. To fill this gap, this paper
proposes MMGaP, an uplink multi-user MIMO detector and downlink Vector
perturbation precoder for next-generation cellular networks. MMGaP realizes
these large MIMO processing algorithms for the first time on bare-metal CUDA
kernels that scale to run on large GPU processing platforms, and can be
packaged as TensorFlow modules, allowing easy integration with a variety of
systems. We integrate MMGaP with NVIDIA's software-defined, GPU-accelerated 5G
platform and evaluate its performance against the state-of-the-art. In a 5G
cellular network using 100 MHz of radio bandwidth, eight antennas at the base
station and eight concurrent users, we show that MMGaP improves uplink
throughput by approximately 50 Mbps per user and downlink throughput by 100
Mbps per user over a wide range of SNR. We further show that MMGaP can also
support larger MIMO sizes: for 16 antennas at the base station and 16
concurrent users, MMGaP provides more than 50 Mbps higher uplink throughput per
user. We measure the execution time of MMGaP on different NVIDIA GPUs and show
that it can operate at line-rate and meet the timing requirements of
state-of-the-art 5G systems.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [4] [Beyond Single LLMs: Enhanced Code Generation via Multi-Stage Performance-Guided LLM Orchestration](https://arxiv.org/abs/2510.01379)
*Huashan Chen,Zhenyu Qi,Haotang Li,Hong Chen,Jinfu Chen,Kebin Peng,In Kee Kim,Kyu Hyung Lee,Sen He*

Main category: cs.SE

TL;DR: 提出PerfOrch框架，通过多阶段性能引导的编排机制动态选择最适合的LLM来处理不同编程语言的代码生成任务，显著提升了代码正确性和运行时性能。


<details>
  <summary>Details</summary>
Motivation: 当前单一模型方法忽视了不同LLM在不同编程语言、算法领域和开发阶段的异构计算优势，需要一种能够动态利用各模型优势的解决方案。

Method: 基于对17个先进LLM在5种编程语言上的实证研究，开发了PerfOrch框架，采用生成-修复-优化的多阶段工作流，通过阶段验证和回滚机制动态路由任务到最适合的LLM。

Result: 在HumanEval-X和EffiBench-X基准测试中分别达到96.22%和91.37%的平均正确率，显著超越GPT-4o的78.66%和49.11%，同时在58.76%的问题上改善了执行时间，中位数加速达到17.67%-27.66%。

Conclusion: PerfOrch框架提供了一种无需微调即可实现生产级自动化软件工程的范式，其即插即用架构能够适应快速发展的生成式AI格局。

Abstract: While Large Language Models (LLMs) have become the predominant paradigm for
automated code generation, current single-model approaches fundamentally ignore
the heterogeneous computational strengths that different models exhibit across
programming languages, algorithmic domains, and development stages. This paper
challenges the single-model convention by introducing a multi-stage,
performance-guided orchestration framework that dynamically routes coding tasks
to the most suitable LLMs within a structured generate-fix-refine workflow. Our
approach is grounded in a comprehensive empirical study of 17 state-of-the-art
LLMs across five programming languages (Python, Java, C++, Go, and Rust) using
HumanEval-X benchmark. The study, which evaluates both functional correctness
and runtime performance metrics (execution time, mean/max memory utilization,
and CPU efficiency), reveals pronounced performance heterogeneity by language,
development stage, and problem category. Guided by these empirical insights, we
present PerfOrch, an LLM agent that orchestrates top-performing LLMs for each
task context through stage-wise validation and rollback mechanisms. Without
requiring model fine-tuning, PerfOrch achieves substantial improvements over
strong single-model baselines: average correctness rates of 96.22% and 91.37%
on HumanEval-X and EffiBench-X respectively, surpassing GPT-4o's 78.66% and
49.11%. Beyond correctness gains, the framework delivers consistent performance
optimizations, improving execution time for 58.76% of problems with median
speedups ranging from 17.67% to 27.66% across languages on two benchmarks. The
framework's plug-and-play architecture ensures practical scalability, allowing
new LLMs to be profiled and integrated seamlessly, thereby offering a paradigm
for production-grade automated software engineering that adapts to the rapidly
evolving generative AI landscape.

</details>


### [5] [Deciphering WONTFIX: A Mixed-Method Study on Why GitHub Issues Get Rejected](https://arxiv.org/abs/2510.01514)
*J. Alexander Curtis,Sharadha Kasiviswanathan,Nasir Eisty*

Main category: cs.SE

TL;DR: GitHub中wontfix标签的定量和定性分析：30%项目使用该标签，主要用于bug报告和功能请求，识别出8个常见原因，既有资源管理价值也可能影响社区参与


<details>
  <summary>Details</summary>
Motivation: 研究GitHub中广泛使用但理解有限的wontfix标签，探索其对开源项目管理的影响和社区动态

Method: 混合方法：从3132个最受欢迎GitHub仓库收集数据，定量分析wontfix标签普及率，定性分析使用原因，采用开放编码和主题分析

Result: 约30%项目使用wontfix标签，主要用于用户提交的bug报告和功能请求，识别出8个主题原因，包括用户特定因素和维护者决策

Conclusion: wontfix标签是管理资源和指导贡献者的重要工具，但可能抑制社区参与和降低管理透明度，理解这些原因有助于项目管理者做出明智决策

Abstract: Context: The ``wontfix'' label is a widely used yet narrowly understood tool
in GitHub repositories, indicating that an issue will not be pursued further.
Despite its prevalence, the impact of this label on project management and
community dynamics within open-source software development is not clearly
defined. Objective: This study examines the prevalence and reasons behind
issues being labeled as wontfix across various open-source repositories on
GitHub. Method: Employing a mixed-method approach, we analyze both quantitative
data to assess the prevalence of the wontfix label and qualitative data to
explore the reasoning that it was used. Data were collected from 3,132 of
GitHub's most-popular repositories. Later, we employ open coding and thematic
analysis to categorize the reasons behind wontfix labels, providing a
structured understanding of the issue management landscape. Results: Our
findings show that about 30% of projects on GitHub apply the wontfix label to
some issues. These issues most often occur on user-submitted issues for bug
reports and feature requests. The study identified eight common themes behind
labeling issues as wontfix, ranging from user-specific control factors to
maintainer-specific decisions. Conclusions: The wontfix label is a critical
tool for managing resources and guiding contributor efforts in GitHub projects.
However, it can also discourage community involvement and obscure the
transparency of project management. Understanding these reasons aids project
managers in making informed decisions and fostering efficient collaboration
within open-source communities.

</details>


### [6] [MIMIC: Integrating Diverse Personality Traits for Better Game Testing Using Large Language Model](https://arxiv.org/abs/2510.01635)
*Yifei Chen,Sarra Habchi,Lili Wei*

Main category: cs.SE

TL;DR: MIMIC是一个将多样化人格特质整合到游戏代理中的框架，通过模仿不同玩家的游戏策略来提高测试覆盖率和游戏交互多样性。


<details>
  <summary>Details</summary>
Motivation: 传统游戏测试算法难以应对现代游戏的复杂性，现有基于强化学习、模仿学习和大语言模型的游戏代理往往忽视人类玩家的多样化策略，导致重复解决方案和无法发现边缘情况。

Method: 提出MIMIC框架，通过整合多样化人格特质，使游戏代理能够在相似情境下采用不同的游戏策略，模仿不同的游戏风格。

Result: 在Minecraft中超越了最先进的代理，实现了更高的任务完成率和更多样化的解决方案，获得了更高的测试覆盖率和更丰富的游戏内交互。

Conclusion: MIMIC在游戏测试方面具有显著潜力，能够有效提高游戏质量和发现边缘情况。

Abstract: Modern video games pose significant challenges for traditional automated
testing algorithms, yet intensive testing is crucial to ensure game quality. To
address these challenges, researchers designed gaming agents using
Reinforcement Learning, Imitation Learning, or Large Language Models. However,
these agents often neglect the diverse strategies employed by human players due
to their different personalities, resulting in repetitive solutions in similar
situations. Without mimicking varied gaming strategies, these agents struggle
to trigger diverse in-game interactions or uncover edge cases.
  In this paper, we present MIMIC, a novel framework that integrates diverse
personality traits into gaming agents, enabling them to adopt different gaming
strategies for similar situations. By mimicking different playstyles, MIMIC can
achieve higher test coverage and richer in-game interactions across different
games. It also outperforms state-of-the-art agents in Minecraft by achieving a
higher task completion rate and providing more diverse solutions. These results
highlight MIMIC's significant potential for effective game testing.

</details>


### [7] [FOSS-chain: using blockchain for Open Source Software license compliance](https://arxiv.org/abs/2510.01740)
*Kypros Iacovou,Georgia M. Kapitsaki,Evangelia Vanezi*

Main category: cs.SE

TL;DR: FOSS-chain是一个基于区块链的开源软件许可证管理平台，旨在解决衍生作品创建时的许可证兼容性问题，通过自动化合规流程覆盖14种OSS许可证


<details>
  <summary>Details</summary>
Motivation: 开源软件许可证合规性复杂，许可证不兼容可能导致法律纠纷，而区块链技术能提供透明性和不可篡改的记录机制

Method: 设计并实现了FOSS-chain网络平台，集成区块链技术自动化许可证合规流程，并通过小规模用户研究进行初步评估

Result: 初步结果令人鼓舞，展示了该平台在实际软件系统中应用的潜力

Conclusion: 区块链与许可证管理的整合为解决OSS许可证兼容性问题提供了有前景的解决方案

Abstract: Open Source Software (OSS) is widely used and carries licenses that indicate
the terms under which the software is provided for use, also specifying
modification and distribution rules. Ensuring that users are respecting OSS
license terms when creating derivative works is a complex process. Compliance
issues arising from incompatibilities among licenses may lead to legal
disputes. At the same time, the blockchain technology with immutable entries
offers a mechanism to provide transparency when it comes to licensing and
ensure software changes are recorded. In this work, we are introducing an
integration of blockchain and license management when creating derivative
works, in order to tackle the issue of OSS license compatibility. We have
designed, implemented and performed a preliminary evaluation of FOSS-chain, a
web platform that uses blockchain and automates the license compliance process,
covering 14 OSS licenses. We have evaluated the initial prototype version of
the FOSS-chain platform via a small scale user study. Our preliminary results
are promising, demonstrating the potential of the platform for adaptation on
realistic software systems.

</details>


### [8] [FalseCrashReducer: Mitigating False Positive Crashes in OSS-Fuzz-Gen Using Agentic AI](https://arxiv.org/abs/2510.02185)
*Paschal C. Amusuo,Dongge Liu,Ricardo Andres Calvo Mendez,Jonathan Metzman,Oliver Chang,James C. Davis*

Main category: cs.SE

TL;DR: 本文提出两种AI驱动的策略来减少模糊测试驱动生成中的误报崩溃：基于约束的驱动生成和基于上下文的崩溃验证，在OSS-Fuzz基准测试中显著降低了误报率。


<details>
  <summary>Details</summary>
Motivation: 模糊测试驱动自动生成经常产生误报崩溃，特别是在处理结构化输入和复杂状态需求的函数时，这影响了工业级模糊测试系统（如OSS-Fuzz-Gen）的可信度。

Method: 采用两种AI策略：1）基于约束的模糊驱动生成，主动对函数输入和状态施加约束；2）基于上下文的崩溃验证，通过分析函数调用者来验证崩溃是否可行。

Result: 在1,500个OSS-Fuzz基准函数上，这些策略将虚假崩溃减少高达8%，报告的崩溃数量减少一半以上，证明前沿大语言模型可以作为可靠的程序分析代理。

Conclusion: 研究结果展示了将AI集成到大规模模糊测试管道中的前景和挑战，为工业级模糊测试提供了有效的误报减少方案。

Abstract: Fuzz testing has become a cornerstone technique for identifying software bugs
and security vulnerabilities, with broad adoption in both industry and
open-source communities. Directly fuzzing a function requires fuzz drivers,
which translate random fuzzer inputs into valid arguments for the target
function. Given the cost and expertise required to manually develop fuzz
drivers, methods exist that leverage program analysis and Large Language Models
to automatically generate these drivers. However, the generated fuzz drivers
frequently lead to false positive crashes, especially in functions highly
structured input and complex state requirements. This problem is especially
crucial in industry-scale fuzz driver generation efforts like OSS-Fuzz-en, as
reporting false positive crashes to maintainers impede trust in both the system
and the team.
  This paper presents two AI-driven strategies to reduce false positives in
OSS-Fuzz-Gen, a multi-agent system for automated fuzz driver generation. First,
constraint-based fuzz driver generation proactively enforces constraints on a
function's inputs and state to guide driver creation. Second, context-based
crash validation reactively analyzes function callers to determine whether
reported crashes are feasible from program entry points. Using 1,500 benchmark
functions from OSS-Fuzz, we show that these strategies reduce spurious crashes
by up to 8%, cut reported crashes by more than half, and demonstrate that
frontier LLMs can serve as reliable program analysis agents. Our results
highlight the promise and challenges of integrating AI into large-scale fuzzing
pipelines.

</details>


### [9] [ARENA: A tool for measuring and analysing the energy efficiency of Android apps](https://arxiv.org/abs/2510.01754)
*Hina Anwar*

Main category: cs.SE

TL;DR: ARENA是一个IDE插件工具，通过硬件方式准确测量Android应用能耗，简化了传统硬件能耗测量的复杂流程


<details>
  <summary>Details</summary>
Motivation: 现有的硬件能耗测量方法设置复杂、耗时且不易复现，缺乏开源工具支持开发者和研究人员进行可靠的硬件能耗测量

Method: 开发ARENA作为IntelliJ和Android Studio插件，连接物理测量设备，执行测试场景，自动进行数据聚合、统计分析、报告和可视化

Result: 实现了在IDE内直接进行硬件能耗测量的工具，支持开发者比较不同应用或版本的能耗差异

Conclusion: ARENA为开发者和研究人员提供了便捷可靠的硬件能耗测量解决方案，简化了传统方法的复杂性

Abstract: To build energy-efficient apps, there is a need to estimate and analyze their
energy consumption in typical usage scenarios. The energy consumption of
Android apps could be estimated via software-based and hardware-based
approaches. Software-based approaches, while easier to implement, are not as
accurate as hardware-based approaches. The process of measuring the energy
consumption of an Android app via a hardware-based approach typically involves
1) setting up a measurement environment, 2) executing the app under test on a
mobile device, 3) recording current/voltage data via a hardware device to
measure energy consumption, and 4) cleaning and aggregating data for analyses,
reports, and visualizations. Specialized scripts are written for selected
hardware and software components to ensure reliable energy measurements. The
energy measurement process is repeated many times and aggregated to remove
noise. These steps make the hardware-based energy measurement process
time-consuming and not easy to adapt or reproduce. There is a lack of
open-source tools available for developers and researchers to take reliable
energy measurements via hardware devices. In this paper, we present and
demonstrate ARENA, a support tool that enables developers and researchers to
connect to a physical measurement device without leaving the comfort of their
IDE. Developers could use ARENA during development to compare energy
consumption between different apps or versions of the same app. ARENA
calculates energy consumption on an Android smartphone by executing a test
scenario on the app under development. Further, ARENA helps aggregate,
statistically analyze, report, and visualize the data, allowing developers and
researchers to dig into the data directly or visually. We implemented ARENA as
an IntelliJ and Android Studio plugin.

</details>


### [10] [Towards Speeding up Program Repair with Non-Autoregressive Model](https://arxiv.org/abs/2510.01825)
*Zhenyu Yang,Yue Pan,Zhen Yang,Zhongxing Yu*

Main category: cs.SE

TL;DR: NARRepair是首个针对自动程序修复任务定制的非自回归代码生成模型，通过修复动作预测器、令牌间依赖提取器和两阶段解码器三大创新，在保证修复质量的同时显著提升修复速度1.4-6.4倍。


<details>
  <summary>Details</summary>
Motivation: 传统基于机器学习的自动程序修复技术采用自回归方式，存在巨大的时间延迟问题，特别是参数量大的模型延迟更严重。非自回归方法虽然可以并行输出代码避免延迟，但直接应用于APR任务会导致补丁质量下降。

Method: 提出NARRepair模型，包含三个核心组件：1)修复动作预测器缓解过度修正问题；2)令牌间依赖提取器解决缺乏令牌间依赖信息的问题；3)两阶段解码器解决缺乏上下文信息的问题。

Result: 在三个广泛使用的APR数据集上评估显示，NARRepair在有限修复时间内性能最佳，相比AR-based APR技术在GPU环境中修复速度提升1.4-6.4倍。

Conclusion: NARRepair在修复速度和准确性方面达到了最先进的综合性能，成功解决了传统AR方法的延迟问题，同时保持了高质量的修复效果。

Abstract: Enlightened by the success of machine learning techniques in various
application areas, recent years have witnessed a surge of research efforts on
automatic program repair (APR) using machine learning techniques. Previous
machine learning-based APR techniques essentially modified bugs in the
autoregressive (AR) manner, which predicts future values based on past values.
Due to the manner of token-by-token generation, the AR-based APR technique has
a huge time delay. In particular, the delay of the APR model with a large
number of parameters is more serious. To address the issue, we aim to apply the
non-autoregressive (NAR) method to the APR task, which can output target code
in a parallel manner to avoid huge repair delays. However, the naive use of the
NAR manner for the APR task suffers from the issue of compromised patch
quality. To effectively adapt the NAR manner for the APR task, we in this paper
propose NARRepair, the first customized NAR code generation model for the APR
task. The NARRepair model features three major novelties, including 1) the
repair action predictor for alleviating the over-correction issue, 2) the
inter-token dependency extractor for alleviating the issue of lacking
inter-token dependency information, and 3) the two-stage decoder for
alleviating the issue of lacking contextual information. We evaluated NARRepair
on three widely used datasets in the APR community, and the results show that
1) compared to other APR techniques, the NARRepair model has the best
performance within the limited repair time, and 2) compared to AR-based APR
techniques, the repair speed of NARRepair has been increased by 1.4-6.4 times
in the GPU environment. Overall, the results show that NARRepair has achieved
state-of-the-art comprehensive performance in terms of repair speed and
accuracy.

</details>


### [11] [RefFilter: Improving Semantic Conflict Detection via Refactoring-Aware Static Analysis](https://arxiv.org/abs/2510.01960)
*Victor Lira,Paulo Borba,Rodrigo Bonifácio,Galileu Santos e Matheus barbosa*

Main category: cs.SE

TL;DR: RefFilter是一个重构感知的语义干扰检测工具，通过自动检测行为保持的重构来减少现有静态分析技术中的假阳性，在保持检测覆盖率的同时将假阳性降低了近32%。


<details>
  <summary>Details</summary>
Motivation: 协作软件开发中语义干扰检测面临假阳性率高的问题，主要原因是现有技术无法有效区分行为保持的代码重构和影响行为的更改。

Method: 在现有静态分析技术基础上集成自动化重构检测，从报告中过滤掉行为保持的重构操作。

Result: 在标记数据集上假阳性减少近32%，虽然假阴性略有增加但精度提升显著超过召回率的微小损失。

Conclusion: 重构感知的干扰检测是改进现代开发工作流中合并支持的实用有效策略。

Abstract: Detecting semantic interference remains a challenge in collaborative software
development. Recent lightweight static analysis techniques improve efficiency
over SDG-based methods, but they still suffer from a high rate of false
positives. A key cause of these false positives is the presence of
behavior-preserving code refactorings, which current techniques cannot
effectively distinguish from changes that impact behavior and can interfere
with others. To handle this problem we present RefFilter, a refactoring-aware
tool for semantic interference detection. It builds on existing static
techniques by incorporating automated refactoring detection to improve
precision. RefFilter discards behavior-preserving refactorings from reports,
reducing false positives while preserving detection coverage. To evaluate
effectiveness and scalability, use two datasets: a labeled dataset with 99
scenarios and ground truth, and a novel dataset of 1,087 diverse merge
scenarios that we have built. Experimental results show that RefFilter reduces
false positives by nearly 32% on the labeled dataset. While this reduction
comes with a non significant increase in false negatives, the overall gain in
precision significantly outweighs the minor trade-off in recall. These findings
demonstrate that refactoring-aware interference detection is a practical and
effective strategy for improving merge support in modern development workflows.

</details>


### [12] [Clarifying Semantics of In-Context Examples for Unit Test Generation](https://arxiv.org/abs/2510.01994)
*Chen Yang,Lin Yang,Ziqi Wang,Dong Wang,Jianyi Zhou,Junjie Chen*

Main category: cs.SE

TL;DR: CLAST是一种通过程序分析和LLM重写来系统化改进单元测试语义清晰度的技术，能够有效提升基于ICL的单元测试生成质量，在多个指标上显著优于现有最佳技术UTgen。


<details>
  <summary>Details</summary>
Motivation: 现有基于ICL的单元测试生成方法严重依赖上下文示例的质量，但现有测试往往结构混乱、语义不清晰，导致生成的测试质量不佳。

Method: CLAST通过将复杂测试分解为逻辑更清晰的测试，并结合程序分析和基于LLM的重写技术来提高测试的语义清晰度。

Result: CLAST完全保留了原始测试的有效性，在编译成功率、通过率、测试覆盖率和变异分数等指标上均优于UTgen。用户研究中85.33%的参与者更偏好CLAST改进的测试清晰度。

Conclusion: CLAST技术能显著提升单元测试的语义清晰度，作为上下文示例使用时能有效改进基于ICL的测试生成方法，具有重要的实践价值和研究前景。

Abstract: Recent advances in large language models (LLMs) have enabled promising
performance in unit test generation through in-context learning (ICL). However,
the quality of in-context examples significantly influences the effectiveness
of generated tests-poorly structured or semantically unclear test examples
often lead to suboptimal outputs. In this paper, we propose CLAST, a novel
technique that systematically refines unit tests to improve their semantic
clarity, thereby enhancing their utility as in-context examples. The approach
decomposes complex tests into logically clearer ones and improves semantic
clarity through a combination of program analysis and LLM-based rewriting. We
evaluated CLAST on four open-source and three industrial projects. The results
demonstrate that CLAST largely outperforms UTgen, the state-of-the-art
refinement technique, in both preserving test effectiveness and enhancing
semantic clarity. Specifically, CLAST fully retains the original effectiveness
of unit tests, while UTgen reduces compilation success rate (CSR), pass rate
(PR), test coverage (Cov), and mutation score (MS) by an average of 12.90%,
35.82%, 4.65%, and 5.07%, respectively. Over 85.33% of participants in our user
study preferred the semantic clarity of CLAST-refined tests. Notably,
incorporating CLAST-refined tests as examples effectively improves ICL-based
unit test generation approaches such as RAGGen and TELPA, resulting in an
average increase of 25.97% in CSR, 28.22% in PR, and 45.99% in Cov for
generated tests, compared to incorporating UTgen-refined tests. The insights
from the follow-up user study not only reinforce CLAST's potential impact in
software testing practice but also illuminate avenues for future research.

</details>


### [13] [Automatic Generation of Combinatorial Reoptimisation Problem Specifications: A Vision](https://arxiv.org/abs/2510.02002)
*Maximilian Kratz,Steffen Zschaler,Jens Kosiol,Gabriele Taentzer*

Main category: cs.SE

TL;DR: 论文提出使用模型驱动工程（MDE）方法，通过声明式建模语言和模型转换技术，从原始优化问题规范中系统推导出再优化问题规范，以解决上下文变化时的解决方案适应问题。


<details>
  <summary>Details</summary>
Motivation: 当优化问题的上下文因素发生变化时，需要重新优化解决方案，但传统方法面临三个挑战：需要最小化对原始解决方案的更改、某些部分可能无法更改、需要生成变更脚本。现有方法缺乏系统化的再优化问题推导机制。

Method: 采用模型驱动工程方法，使用声明式建模语言和模型转换技术，基于GIPS工具实现概念验证，对组合再优化问题进行分类并推导相应的再优化规范。

Result: 开发了基于GIPS工具的初始概念验证实现，并将其应用于教学助理分配的资源分配问题示例中。

Conclusion: 模型驱动工程为系统化推导再优化问题规范提供了新的机会，特别是在组合优化问题领域，该方法能够有效处理上下文变化时的解决方案适应需求。

Abstract: Once an optimisation problem has been solved, the solution may need
adaptation when contextual factors change. This challenge, also known as
reoptimisation, has been addressed in various problem domains, such as railway
crew rescheduling, nurse rerostering, or aircraft recovery. This requires a
modified problem to be solved again to ensure that the adapted solution is
optimal in the new context. However, the new optimisation problem differs
notably from the original problem: (i) we want to make only minimal changes to
the original solution to minimise the impact; (ii) we may be unable to change
some parts of the original solution (e.g., because they refer to past
allocations); and (iii) we need to derive a change script from the original
solution to the new solution. In this paper, we argue that Model-Driven
Engineering (MDE) - in particular, the use of declarative modelling languages
and model transformations for the high-level specification of optimisation
problems - offers new opportunities for the systematic derivation of
reoptimisation problems from the original optimisation problem specification.
We focus on combinatorial reoptimisation problems and provide an initial
categorisation of changing problems and strategies for deriving the
corresponding reoptimisation specifications. We introduce an initial
proof-of-concept implementation based on the GIPS (Graph-Based (Mixed) Integer
Linear Programming Problem Specification) tool and apply it to an example
resource-allocation problem: the allocation of teaching assistants to teaching
sessions.

</details>


### [14] [ACM SIGSOFT SEN Empirical Software Engineering: Introducing Our New Regular Column](https://arxiv.org/abs/2510.02007)
*Justus Bogner,Roberto Verdecchia*

Main category: cs.SE

TL;DR: 介绍ACM SIGSOFT SEN新专栏SEN-ESE，旨在讨论实证软件工程研究的元方面问题，包括研究可重复性、外部有效性、评审主观性等挑战，通过专家访谈、焦点小组等方式促进ESE研究的改进。


<details>
  <summary>Details</summary>
Motivation: 实证软件工程研究虽然成熟但仍面临诸多挑战，如研究可重复性差、外部有效性有限、评审过程主观性强、研究成果难以转化到工业实践等问题，且许多研究细节缺乏明确文档化，不利于新人学习。

Method: 通过新设立的ACM SIGSOFT SEN专栏(SEN-ESE)，采用专家访谈、焦点小组、调查和立场文章等多种形式，讨论ESE研究的元方面问题。

Result: 建立了一个专门讨论ESE研究元问题的平台，涵盖研究可重复性包的最佳实践、统计方法、访谈转录工具、跨学科研究发表等主题。

Conclusion: 该专栏旨在促进ESE研究社区的反思和改进，鼓励就具有挑战性、争议性或未充分探索的主题进行讨论，并根据社区兴趣塑造专栏内容。

Abstract: From its early foundations in the 1970s, empirical software engineering (ESE)
has evolved into a mature research discipline that embraces a plethora of
different topics, methodologies, and industrial practices. Despite its
remarkable progress, the ESE research field still needs to keep evolving, as
new impediments, shortcoming, and technologies emerge. Research
reproducibility, limited external validity, subjectivity of reviews, and
porting research results to industrial practices are just some examples of the
drivers for improvements to ESE research. Additionally, several facets of ESE
research are not documented very explicitly, which makes it difficult for
newcomers to pick them up. With this new regular ACM SIGSOFT SEN column
(SEN-ESE), we introduce a venue for discussing meta-aspects of ESE research,
ranging from general topics such as the nature and best practices for
replication packages, to more nuanced themes such as statistical methods,
interview transcription tools, and publishing interdisciplinary research. Our
aim for the column is to be a place where we can regularly spark conversations
on ESE topics that might not often be touched upon or are left implicit.
Contributions to this column will be grounded in expert interviews, focus
groups, surveys, and position pieces, with the goal of encouraging reflection
and improvement in how we conduct, communicate, teach, and ultimately improve
ESE research. Finally, we invite feedback from the ESE community on
challenging, controversial, or underexplored topics, as well as suggestions for
voices you would like to hear from. While we cannot promise to act on every
idea, we aim to shape this column around the community interests and are
grateful for all contributions.

</details>


### [15] [Towards fairer public transit: Real-time tensor-based multimodal fare evasion and fraud detection](https://arxiv.org/abs/2510.02165)
*Peter Wauyo,Dalia Bwiza,Alain Murara,Edwin Mugume,Eric Umuhoza*

Main category: cs.SE

TL;DR: 提出基于Vision Transformer和Audio Spectrogram Transformer的多模态系统，通过张量融合网络检测公共交通欺诈和逃票行为，准确率达89.5%，显著优于现有方法


<details>
  <summary>Details</summary>
Motivation: 解决公共交通中欺诈和逃票行为导致的收入损失问题，传统系统检测效果有限，需要更先进的多模态分析方法

Method: 使用ViViT模型提取视频特征，AST模型分析音频，采用Tensor Fusion Network架构进行2折笛卡尔积融合，捕捉视觉行为与音频线索的跨模态交互

Result: 在自定义数据集上达到89.5%准确率、87.2%精确率和84.0%召回率，比传统方法F1分数提升7.0%，召回率提升8.8%

Conclusion: 该系统支持实时检测，能有效帮助公共交通运营商减少收入损失、提高乘客安全性和运营合规性，多模态融合方法显著优于单模态和早期融合基准

Abstract: This research introduces a multimodal system designed to detect fraud and
fare evasion in public transportation by analyzing closed circuit television
(CCTV) and audio data. The proposed solution uses the Vision Transformer for
Video (ViViT) model for video feature extraction and the Audio Spectrogram
Transformer (AST) for audio analysis. The system implements a Tensor Fusion
Network (TFN) architecture that explicitly models unimodal and bimodal
interactions through a 2-fold Cartesian product. This advanced fusion technique
captures complex cross-modal dynamics between visual behaviors (e.g.,
tailgating,unauthorized access) and audio cues (e.g., fare transaction sounds).
The system was trained and tested on a custom dataset, achieving an accuracy of
89.5%, precision of 87.2%, and recall of 84.0% in detecting fraudulent
activities, significantly outperforming early fusion baselines and exceeding
the 75% recall rates typically reported in state-of-the-art transportation
fraud detection systems. Our ablation studies demonstrate that the tensor
fusion approach provides a 7.0% improvement in the F1 score and an 8.8% boost
in recall compared to traditional concatenation methods. The solution supports
real-time detection, enabling public transport operators to reduce revenue
loss, improve passenger safety, and ensure operational compliance.

</details>


### [16] [SIEVE: Towards Verifiable Certification for Code-datasets](https://arxiv.org/abs/2510.02166)
*Fatou Ndiaye Mbodji,El-hacen Diallo,Jordan Samhi,Kui Liu,Jacques Klein,Tegawendé F. Bissyande*

Main category: cs.SE

TL;DR: SIEVE框架通过可验证的统计证书替代传统数据集卡片，为代码数据集提供可审计的质量保证，降低质量保证成本并增强信任度


<details>
  <summary>Details</summary>
Motivation: 当前公共代码数据集缺乏可验证的质量保证，静态数据集卡片不可审计且无法提供统计保证，团队需要构建孤立的临时清理流程，导致效率低下和成本增加

Method: 提出SIEVE社区驱动框架，将属性检查转化为置信卡片——机器可读、可验证的证书，包含随时有效的统计边界

Result: SIEVE框架能够提供可验证的数据集质量认证，取代叙述性卡片

Conclusion: SIEVE框架预计将降低质量保证成本，提高对代码数据集的信任度，为代码代理和实证软件工程提供更可靠的数据基础

Abstract: Code agents and empirical software engineering rely on public code datasets,
yet these datasets lack verifiable quality guarantees. Static 'dataset cards'
inform, but they are neither auditable nor do they offer statistical
guarantees, making it difficult to attest to dataset quality. Teams build
isolated, ad-hoc cleaning pipelines. This fragments effort and raises cost. We
present SIEVE, a community-driven framework. It turns per-property checks into
Confidence Cards-machine-readable, verifiable certificates with anytime-valid
statistical bounds. We outline a research plan to bring SIEVE to maturity,
replacing narrative cards with anytime-verifiable certification. This shift is
expected to lower quality-assurance costs and increase trust in code-datasets.

</details>


### [17] [TAIBOM: Bringing Trustworthiness to AI-Enabled Systems](https://arxiv.org/abs/2510.02169)
*Vadim Safronov,Anthony McCaigue,Nicholas Allott,Andrew Martin*

Main category: cs.SE

TL;DR: 提出了Trusted AI Bill of Materials (TAIBOM)框架，将SBOM原则扩展到AI领域，解决AI系统特有的动态数据驱动特性和松散耦合依赖关系问题


<details>
  <summary>Details</summary>
Motivation: 开源软件和AI技术的融合增加了软件供应链的复杂性，现有SBOM框架无法有效捕捉AI系统的独特特性，包括动态数据驱动性质和跨数据集、模型、软件组件的松散耦合依赖关系

Method: 开发了TAIBOM框架，包含：(i)针对AI组件的结构化依赖模型，(ii)跨异构AI管道的完整性声明传播机制，(iii)验证组件来源的信任证明过程

Result: TAIBOM在AI工作流中支持保证、安全和合规性，相比SPDX和CycloneDX等现有标准具有优势

Conclusion: 这项工作通过结构化软件透明度为可信和可验证的AI系统奠定了基础

Abstract: The growing integration of open-source software and AI-driven technologies
has introduced new layers of complexity into the software supply chain,
challenging existing methods for dependency management and system assurance.
While Software Bills of Materials (SBOMs) have become critical for enhancing
transparency and traceability, current frameworks fall short in capturing the
unique characteristics of AI systems -- namely, their dynamic, data-driven
nature and the loosely coupled dependencies across datasets, models, and
software components. These challenges are compounded by fragmented governance
structures and the lack of robust tools for ensuring integrity, trust, and
compliance in AI-enabled environments.
  In this paper, we introduce Trusted AI Bill of Materials (TAIBOM) -- a novel
framework extending SBOM principles to the AI domain. TAIBOM provides (i) a
structured dependency model tailored for AI components, (ii) mechanisms for
propagating integrity statements across heterogeneous AI pipelines, and (iii) a
trust attestation process for verifying component provenance. We demonstrate
how TAIBOM supports assurance, security, and compliance across AI workflows,
highlighting its advantages over existing standards such as SPDX and CycloneDX.
This work lays the foundation for trustworthy and verifiable AI systems through
structured software transparency.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [18] [Edge GPU Aware Multiple AI Model Pipeline for Accelerated MRI Reconstruction and Analysis](https://arxiv.org/abs/2510.01730)
*Ashiyana Abdul Majeed,Mahmoud Meribout,Safa Mohammed Sali*

Main category: cs.AR

TL;DR: 提出了一种硬件加速的多模型系统，用于从CT图像同时进行MRI重建和诊断，通过硬件引擎调度和模型微调实现了实时性能（150帧/秒）和5%的准确率提升。


<details>
  <summary>Details</summary>
Motivation: AI在医学影像处理中取得了进展，但很少有研究关注硬件加速的多模型系统优化。随着专用边缘设备的出现，高效利用其加速器变得至关重要。

Method: 利用现代NVIDIA边缘GPU的硬件引擎（GPU和DLA）和调度技术，对多个AI模型的不同层进行硬件分配以减少硬件引擎间的理想时间。对GAN模型进行微调以避免回退到GPU引擎执行。

Result: 实现了近150帧/秒的吞吐量，微调后的边缘GPU感知AI模型准确率提升了5%。两个微调后的GPU感知GAN模型的硬件分配证明性能可以翻倍。

Conclusion: 结果证明了在医学图像分析和诊断中采用硬件感知并行模型的有效性。

Abstract: Advancements in AI have greatly enhanced the medical imaging process, making
it quicker to diagnose patients. However, very few have investigated the
optimization of a multi-model system with hardware acceleration. As specialized
edge devices emerge, the efficient use of their accelerators is becoming
increasingly crucial. This paper proposes a hardware-accelerated method for
simultaneous reconstruction and diagnosis of \ac{MRI} from \ac{CT} images.
Real-time performance of achieving a throughput of nearly 150 frames per second
was achieved by leveraging hardware engines available in modern NVIDIA edge
GPU, along with scheduling techniques. This includes the GPU and the \ac{DLA}
available in both Jetson AGX Xavier and Jetson AGX Orin, which were considered
in this paper. The hardware allocation of different layers of the multiple AI
models was done in such a way that the ideal time between the hardware engines
is reduced. In addition, the AI models corresponding to the \ac{GAN} model were
fine-tuned in such a way that no fallback execution into the GPU engine is
required without compromising accuracy. Indeed, the accuracy corresponding to
the fine-tuned edge GPU-aware AI models exhibited an accuracy enhancement of
5\%. A further hardware allocation of two fine-tuned GPU-aware GAN models
proves they can double the performance over the original model, leveraging
adequate partitioning on the NVIDIA Jetson AGX Xavier and Orin devices. The
results prove the effectiveness of employing hardware-aware models in parallel
for medical image analysis and diagnosis.

</details>


### [19] [Multiplier-free In-Memory Vector-Matrix Multiplication Using Distributed Arithmetic](https://arxiv.org/abs/2510.02099)
*Felix Zeller,John Reuben,Dietmar Fey*

Main category: cs.AR

TL;DR: 提出一种基于分布式算法(DA)的ReRAM内存计算方案，通过存储权重和并使用移位加法电路实现向量矩阵乘法，相比传统位切片方法延迟降低4.5倍，能耗降低12倍，且完全消除了功耗大的ADC需求。


<details>
  <summary>Details</summary>
Motivation: 传统内存计算中的向量矩阵乘法需要大量ADC/DAC，消耗大量功耗和面积，需要更高效的实现方法。

Method: 扩展分布式算法技术，在ReRAM内存外围使用移位加法电路实现向量矩阵乘法，存储权重和来避免硬件乘法器。

Result: 通过晶体管级仿真验证，相比传统位切片方法延迟降低4.5倍，能耗降低12倍，完全消除了ADC需求。

Conclusion: 分布式算法为内存计算中的向量矩阵乘法提供了高效解决方案，显著降低了延迟和能耗，同时消除了ADC带来的面积和功耗开销。

Abstract: Vector-Matrix Multiplication (VMM) is the fundamental and frequently required
computation in inference of Neural Networks (NN). Due to the large data
movement required during inference, VMM can benefit greatly from in-memory
computing. However, ADC/DACs required for in-memory VMM consume significant
power and area. `Distributed Arithmetic (DA)', a technique in computer
architecture prevalent in 1980s was used to achieve inner product or dot
product of two vectors without using a hard-wired multiplier when one of the
vectors is a constant. In this work, we extend the DA technique to multiply an
input vector with a constant matrix. By storing the sum of the weights in
memory, DA achieves VMM using shift-and-add circuits in the periphery of ReRAM
memory. We verify functional and also estimate non-functional properties
(latency, energy, area) by performing transistor-level simulations. Using
energy-efficient sensing and fine grained pipelining, our approach achieves 4.5
x less latency and 12 x less energy than VMM performed in memory conventionally
by bit slicing. Furthermore, DA completely eliminated the need for power-hungry
ADCs which are the main source of area and energy consumption in the current
VMM implementations in memory.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [20] [Odontoceti: Ultra-Fast DAG Consensus with Two Round Commitment](https://arxiv.org/abs/2510.01216)
*Preston Vander Vos*

Main category: cs.DC

TL;DR: Odontoceti是一种基于DAG的新型共识协议，通过将容错率从33%降低到20%，实现了仅需两轮通信的快速确认，在现实网络条件下达到300毫秒中位延迟和10,000 TPS的高吞吐量性能。


<details>
  <summary>Details</summary>
Motivation: 解决区块链用户对可扩展性的需求，追求低延迟和即时交易处理，同时探索较低容错率共识协议的实际可行性。

Method: 采用n=5f+1验证者架构，构建无认证DAG结构，引入新颖的区块提交决策规则，并包含针对崩溃故障的进度优化机制。

Result: 相比现有生产协议实现20-25%的延迟改进，在两轮通信内完成区块确认，在现实网络条件下达到300ms中位延迟和10,000 TPS吞吐量。

Conclusion: 该研究证明了较低容错率共识协议在区块链中的实际可行性，通过战略性安全权衡获得了显著的性能提升。

Abstract: Users of blockchains value scalability, expecting fast confirmations and
immediate transaction processing. Odontoceti, the latest in DAG-based
consensus, addresses these concerns by prioritizing low latency and high
throughput, making a strategic trade-off in security by operating with a 20%
fault tolerance instead of the established 33% level. It is the first DAG-based
protocol to achieve commitment in just two communication rounds, delivering
median latency of 300 milliseconds while processing 10,000 transactions per
second under realistic network conditions. Odontoceti operates with n = 5f + 1
validators and creates an uncertified DAG with a novel decision rule for
committing blocks. The protocol includes an optimization that advances progress
when participants are slow, benefiting crash fault scenarios which are more
common in practice than Byzantine faults. Evaluation results demonstrate 20-25%
latency improvements compared to an existing production protocol, validating
that reducing wave length from three rounds to two rounds yields meaningful
performance benefits. This paper establishes the practical viability of lower
fault tolerance consensus protocols for blockchains.

</details>


### [21] [Kant: An Efficient Unified Scheduling System for Large-Scale AI Clusters](https://arxiv.org/abs/2510.01256)
*Lingling Zeng,Gen Zhang,Jialin Peng,Xiang Xu,Yuan Xu,Lijun Ma*

Main category: cs.DC

TL;DR: Kant是一个高效统一的大规模AI容器集群调度平台，支持训练和推理任务的协同调度，通过Backfill和增强Binpack等策略显著提升资源利用率和调度效率。


<details>
  <summary>Details</summary>
Motivation: 随着AI集群规模扩大和大语言模型训练推理需求快速增长，传统调度系统在资源利用率、调度效率和服务质量平衡方面面临挑战。

Method: 基于Kant系统实践实现，采用Backfill和增强Binpack（E-Binpack）等调度策略，定义了一套AI集群关键评估指标。

Result: 实验结果显示Kant在数百到数万GPU规模的集群中表现优异，显著提升资源利用率、降低资源碎片化和分布式训练通信开销。

Conclusion: Kant为构建高性能、高可用的AI原生调度基础设施提供了实用的工程解决方案，已在多个AI数据中心集群中稳定部署。

Abstract: As AI cluster sizes continue to expand and the demand for
large-language-model (LLM) training and inference workloads grows rapidly,
traditional scheduling systems face significant challenges in balancing
resource utilization, scheduling efficiency, and service quality. This paper
presents and evaluates Kant: an efficient unified scheduling platform designed
for large-scale AI container clusters, supporting the co-scheduling of both
training and inference jobs. Based on the practical implementation of the Kant
system, we systematically define a set of key evaluation metrics for AI
clusters, including GPU Allocation Ratio (GAR), Scheduling Occupancy Rate
(SOR), GPU Node Fragmentation Ratio (GFR), Job Waiting Time Distribution
(JWTD), and Job Training Time Estimation Distribution (JTTED), providing a
foundation for quantitative performance analysis. Experimental results
demonstrate that Kant achieves exceptional performance in clusters ranging from
hundreds to tens of thousands of GPUs. By leveraging scheduling strategies such
as Backfill and Enhanced Binpack (E-Binpack), the system significantly improves
resource utilization and scheduling efficiency, while effectively reducing
resource fragmentation and communication overhead in distributed training. The
system has been deployed in multiple AI data center clusters, where it stably
supports large-scale intelligent computing workloads. This work provides a
practical engineering approach for building high-performance, highly available,
AI-native scheduling infrastructure.

</details>


### [22] [Accuracy vs Performance: An abstraction model for deadline constrained offloading at the mobile-edge](https://arxiv.org/abs/2510.01885)
*Jamie Cotter,Ignacio Castineiras,Victor Cionca*

Main category: cs.DC

TL;DR: 提出了一种面向移动边缘设备的低延迟DNN卸载调度算法，通过轻量级网络状态表示、动态带宽估计和优先级感知抢占机制，在资源稀缺时提高任务吞吐量。


<details>
  <summary>Details</summary>
Motivation: 解决移动边缘设备上DNN任务卸载的低延迟和截止时间约束问题，特别是在高工作负载和资源稀缺情况下的性能优化。

Method: 设计调度算法，包含资源可用性表示、网络离散化和动态带宽估计机制，并在四个树莓派2设备上实现系统，模拟废物分类传送带场景。

Result: 新型低延迟抽象模型在高工作负载下表现更好，动态带宽估计有助于任务放置并在资源稀缺时提高任务吞吐量。

Conclusion: 该调度算法通过创新的网络状态表示和动态带宽估计机制，有效降低了DNN任务卸载的延迟，在资源受限环境下实现了更好的性能表现。

Abstract: In this paper, we present a solution for low-latency deadline-constrained DNN
offloading on mobile edge devices. We design a scheduling algorithm with
lightweight network state representation, considering device availability,
communication on the network link, priority-aware pre-emption, and task
deadlines. The scheduling algorithm aims to reduce latency by designing a
resource availability representation, as well as a network discretisation and a
dynamic bandwidth estimation mechanism. We implement the scheduling algorithm
into a system composed of four Raspberry Pi 2 (model Bs) mobile edge devices,
sampling a waste classification conveyor belt at a set frame rate. The system
is evaluated and compared to a previous approach of ours, which was proven to
outcompete work-stealers and a non-pre-emption based scheduling heuristic under
the aforementioned waste classification scenario. Our findings show the novel
lower latency abstraction models yield better performance under high-volume
workloads, with the dynamic bandwidth estimation assisting the task placement
while, ultimately, increasing task throughput in times of resource scarcity.

</details>


### [23] [IoT-MCP: Bridging LLMs and IoT Systems Through Model Context Protocol](https://arxiv.org/abs/2510.01260)
*Ningyuan Yang,Guanliang Lyu,Mingchen Ma,Yiyi Lu,Yiming Li,Zhihui Gao,Hancheng Ye,Jianyi Zhang,Tingjun Chen,Yiran Chen*

Main category: cs.DC

TL;DR: IoT-MCP框架通过边缘服务器实现MCP协议，成功连接LLM与IoT设备，在1140个复杂任务中实现100%成功率、205ms响应时间和74KB内存占用


<details>
  <summary>Details</summary>
Motivation: 解决LLM与IoT系统集成中的硬件异构性和控制复杂性挑战，需要标准化通信协议来连接语言模型与物理设备

Method: 提出IoT-MCP框架，通过边缘部署的服务器实现Model Context Protocol(MCP)，并建立IoT-MCP Bench基准测试集，包含114个基础任务和1140个复杂任务

Result: 在22种传感器类型和6种微控制器单元上验证，实现100%任务成功率、205ms平均响应时间和74KB峰值内存占用

Conclusion: 该工作提供了开源集成框架和标准化评估方法，为LLM-IoT系统的发展奠定了基础

Abstract: The integration of Large Language Models (LLMs) with Internet-of-Things (IoT)
systems faces significant challenges in hardware heterogeneity and control
complexity. The Model Context Protocol (MCP) emerges as a critical enabler,
providing standardized communication between LLMs and physical devices. We
propose IoT-MCP, a novel framework that implements MCP through edge-deployed
servers to bridge LLMs and IoT ecosystems. To support rigorous evaluation, we
introduce IoT-MCP Bench, the first benchmark containing 114 Basic Tasks (e.g.,
``What is the current temperature?'') and 1,140 Complex Tasks (e.g., ``I feel
so hot, do you have any ideas?'') for IoT-enabled LLMs. Experimental validation
across 22 sensor types and 6 microcontroller units demonstrates IoT-MCP's 100%
task success rate to generate tool calls that fully meet expectations and
obtain completely accurate results, 205ms average response time, and 74KB peak
memory footprint. This work delivers both an open-source integration framework
(https://github.com/Duke-CEI-Center/IoT-MCP-Servers) and a standardized
evaluation methodology for LLM-IoT systems.

</details>


### [24] [QScale: Probabilistic Chained Consensus for Moderate-Scale Systems](https://arxiv.org/abs/2510.01536)
*Hasan Heydari,Alysson Bessani,Kartik Nayak*

Main category: cs.DC

TL;DR: QScale协议为中等规模分布式账本系统设计，实现了亚线性通信复杂度和低延迟，填补了PBFT和Algorand之间的空白


<details>
  <summary>Details</summary>
Motivation: 现有分布式账本协议要么通信复杂度高（如PBFT），要么需要大量进程（如Algorand），都不适合几百到一千个进程的中等规模系统

Method: 提出QScale协议，采用亚线性通信复杂度设计，每个进程每块预期通信复杂度为Õ(κ√n)，总通信复杂度为Õ(nκ)

Result: 协议在保持安全性和活跃性的同时，实现了最佳情况O(κ)轮延迟，其中κ是小的安全参数

Conclusion: QScale为中等规模分布式账本系统提供了实用的解决方案，平衡了通信复杂度和延迟要求

Abstract: Existing distributed ledger protocols either incur a high communication
complexity and are thus suited to systems with a small number of processes
(e.g., PBFT), or rely on committee-sampling-based approaches that only work for
a very large number of processes (e.g., Algorand). Neither of these lines of
work is well-suited for moderate-scale distributed ledgers ranging from a few
hundred to a thousand processes, which are common in production (e.g, Redbelly,
Sui). The goal of this work is to design a distributed ledger with sub-linear
communication complexity per process, sub-quadratic total communication
complexity, and low latency for finalizing a block into the ledger, such that
it can be used for moderate-scale systems. We propose QScale, a protocol in
which every process incurs only $\widetilde{O}(\kappa \sqrt{n})$ communication
complexity per-block in expectation, $\widetilde{O}(n\kappa)$ total
communication complexity per-block in expectation, and a best-case latency of
$O(\kappa)$ rounds while ensuring safety and liveness with overwhelming
probability, with $\kappa$ being a small security parameter.

</details>


### [25] [Programming RISC-V accelerators via Fortran](https://arxiv.org/abs/2510.02170)
*Nick Brown,Jake Davies,Felix LeClair*

Main category: cs.DC

TL;DR: 提出一种通过Fortran驱动RISC-V加速器的方法，避免科学计算代码重写


<details>
  <summary>Details</summary>
Motivation: RISC-V加速器具有高性能计算潜力，但需要专门的编程模型和API，而科学计算中大量复杂的Fortran代码难以重写

Method: 开发一种方法使Fortran代码能够直接驱动RISC-V加速器架构

Result: 实现了无需代码重开发即可利用RISC-V加速器的能力

Conclusion: 该方法为科学计算领域使用RISC-V加速器提供了可行的解决方案，保护了现有的Fortran代码投资

Abstract: A range of RISC-V based accelerators are available and coming to market, and
there is strong potential for these to be used for High Performance Computing
(HPC) workloads. However, such accelerators tend to provide bespoke programming
models and APIs that require codes to be rewritten. In scientific computing,
where many of the simulation code are highly complex, extensive, and written in
Fortran, this is not realistic. In this extended abstract we present an
approach that enables driving such architectures via Fortran, avoiding code
redevelopment.

</details>
