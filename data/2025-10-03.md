<div id=toc></div>

# Table of Contents

- [cs.AR](#cs.AR) [Total: 2]
- [cs.NI](#cs.NI) [Total: 1]
- [cs.MA](#cs.MA) [Total: 2]
- [cs.SE](#cs.SE) [Total: 14]
- [cs.DC](#cs.DC) [Total: 6]


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [1] [Edge GPU Aware Multiple AI Model Pipeline for Accelerated MRI Reconstruction and Analysis](https://arxiv.org/abs/2510.01730)
*Ashiyana Abdul Majeed,Mahmoud Meribout,Safa Mohammed Sali*

Main category: cs.AR

TL;DR: 本文提出一种硬件加速方法，用于在NVIDIA边缘GPU上同时实现MRI重建与CT图像诊断，通过优化多模型在GPU和DLA上的硬件分配与调度，达到近150 FPS的实时性能，并在保持精度的同时提升5%准确率，双模型并行部署更使性能翻倍。


<details>
  <summary>Details</summary>
Motivation: 现有研究较少关注多模型系统在专用边缘设备上的硬件加速优化，而随着边缘AI设备的发展，高效利用其加速器对医学影像实时处理至关重要。

Method: 利用NVIDIA Jetson AGX Xavier和Orin上的GPU与DLA硬件引擎，对多个AI模型（特别是GAN）进行硬件感知的层分配与调度，并对模型进行微调以避免回退至GPU执行。

Result: 实现了近150帧/秒的吞吐量；微调后的模型准确率提升5%；两个优化模型并行部署时性能相比原始模型翻倍。

Conclusion: 硬件感知的多模型并行部署能显著提升医学图像重建与诊断的效率与精度，验证了在边缘设备上协同利用多种硬件加速器的有效性。

Abstract: Advancements in AI have greatly enhanced the medical imaging process, making
it quicker to diagnose patients. However, very few have investigated the
optimization of a multi-model system with hardware acceleration. As specialized
edge devices emerge, the efficient use of their accelerators is becoming
increasingly crucial. This paper proposes a hardware-accelerated method for
simultaneous reconstruction and diagnosis of \ac{MRI} from \ac{CT} images.
Real-time performance of achieving a throughput of nearly 150 frames per second
was achieved by leveraging hardware engines available in modern NVIDIA edge
GPU, along with scheduling techniques. This includes the GPU and the \ac{DLA}
available in both Jetson AGX Xavier and Jetson AGX Orin, which were considered
in this paper. The hardware allocation of different layers of the multiple AI
models was done in such a way that the ideal time between the hardware engines
is reduced. In addition, the AI models corresponding to the \ac{GAN} model were
fine-tuned in such a way that no fallback execution into the GPU engine is
required without compromising accuracy. Indeed, the accuracy corresponding to
the fine-tuned edge GPU-aware AI models exhibited an accuracy enhancement of
5\%. A further hardware allocation of two fine-tuned GPU-aware GAN models
proves they can double the performance over the original model, leveraging
adequate partitioning on the NVIDIA Jetson AGX Xavier and Orin devices. The
results prove the effectiveness of employing hardware-aware models in parallel
for medical image analysis and diagnosis.

</details>


### [2] [Multiplier-free In-Memory Vector-Matrix Multiplication Using Distributed Arithmetic](https://arxiv.org/abs/2510.02099)
*Felix Zeller,John Reuben,Dietmar Fey*

Main category: cs.AR

TL;DR: 本文提出一种基于分布式算术（DA）的新型存内向量-矩阵乘法（VMM）方法，通过在ReRAM存储器外围使用移位-加法电路实现，无需ADC/DAC，显著降低延迟和能耗。


<details>
  <summary>Details</summary>
Motivation: 传统存内VMM计算依赖ADC/DAC，带来高功耗和面积开销，限制了能效；而分布式算术（DA）技术可在无需硬连线乘法器的情况下完成向量点积，适用于常数权重场景。

Method: 将DA技术扩展至输入向量与常数矩阵相乘，通过在ReRAM中存储权重之和，并利用外围的移位-加法电路实现VMM，结合高效传感与细粒度流水线。

Result: 晶体管级仿真表明，相比传统的位切片存内VMM方法，该方法延迟降低4.5倍，能耗降低12倍，并完全消除了对高功耗ADC的需求。

Conclusion: 所提出的DA-based存内VMM方案在能效和面积方面显著优于现有方法，为神经网络推理提供了一种高效硬件实现路径。

Abstract: Vector-Matrix Multiplication (VMM) is the fundamental and frequently required
computation in inference of Neural Networks (NN). Due to the large data
movement required during inference, VMM can benefit greatly from in-memory
computing. However, ADC/DACs required for in-memory VMM consume significant
power and area. `Distributed Arithmetic (DA)', a technique in computer
architecture prevalent in 1980s was used to achieve inner product or dot
product of two vectors without using a hard-wired multiplier when one of the
vectors is a constant. In this work, we extend the DA technique to multiply an
input vector with a constant matrix. By storing the sum of the weights in
memory, DA achieves VMM using shift-and-add circuits in the periphery of ReRAM
memory. We verify functional and also estimate non-functional properties
(latency, energy, area) by performing transistor-level simulations. Using
energy-efficient sensing and fine grained pipelining, our approach achieves 4.5
x less latency and 12 x less energy than VMM performed in memory conventionally
by bit slicing. Furthermore, DA completely eliminated the need for power-hungry
ADCs which are the main source of area and energy consumption in the current
VMM implementations in memory.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [3] [MMGaP: Multi-User MIMO Detection and Precoding using GPU-assisted Physics-inspired Computation](https://arxiv.org/abs/2510.01579)
*Abhishek Kumar Singh,Kyle Jamieson*

Main category: cs.NI

TL;DR: 本文提出了MMGaP，一种可在GPU上高效运行的多用户MIMO检测与预编码方案，显著提升5G上下行吞吐量，并首次在裸金属CUDA内核上实现大规模MIMO算法，满足实际5G系统时序要求。


<details>
  <summary>Details</summary>
Motivation: 现有基于物理启发和量子计算的物理层方法虽在理论上提升了频谱效率，但尚未在通用处理器上实现实用化，导致实际系统吞吐量远低于理论预期。

Method: 提出MMGaP方案，通过裸金属CUDA内核实现上行多用户MIMO检测器和下行向量扰动预编码器，并将其封装为TensorFlow模块，集成到NVIDIA的GPU加速5G平台中。

Result: 在100 MHz带宽、8天线8用户场景下，MMGaP使每用户上行吞吐量提升约50 Mbps，下行提升100 Mbps；在16天线16用户场景下，上行每用户吞吐量仍提升超50 Mbps；且在多种NVIDIA GPU上均可满足5G线速处理时序要求。

Conclusion: MMGaP成功弥合理论与实践之间的差距，首次在GPU平台上高效实现大规模MIMO物理层算法，显著提升5G系统吞吐量并具备良好的可扩展性与实用性。

Abstract: Physics-inspired and quantum compute based methods for processing in the
physical layer of next-generation cellular radio access networks have
demonstrated theoretical advances in spectral efficiency in recent years, but
have stopped short of practical realization on commodity processors, leaving a
gap between the throughput practical systems can achieve and the projected
throughput the state-of-the-art should achieve. To fill this gap, this paper
proposes MMGaP, an uplink multi-user MIMO detector and downlink Vector
perturbation precoder for next-generation cellular networks. MMGaP realizes
these large MIMO processing algorithms for the first time on bare-metal CUDA
kernels that scale to run on large GPU processing platforms, and can be
packaged as TensorFlow modules, allowing easy integration with a variety of
systems. We integrate MMGaP with NVIDIA's software-defined, GPU-accelerated 5G
platform and evaluate its performance against the state-of-the-art. In a 5G
cellular network using 100 MHz of radio bandwidth, eight antennas at the base
station and eight concurrent users, we show that MMGaP improves uplink
throughput by approximately 50 Mbps per user and downlink throughput by 100
Mbps per user over a wide range of SNR. We further show that MMGaP can also
support larger MIMO sizes: for 16 antennas at the base station and 16
concurrent users, MMGaP provides more than 50 Mbps higher uplink throughput per
user. We measure the execution time of MMGaP on different NVIDIA GPUs and show
that it can operate at line-rate and meet the timing requirements of
state-of-the-art 5G systems.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [4] [LLM-based Multi-Agent Blackboard System for Information Discovery in Data Science](https://arxiv.org/abs/2510.01285)
*Alireza Salemi,Mihir Parmar,Palash Goyal,Yiwen Song,Jinsung Yoon,Hamed Zamani,Hamid Palangi,Tomas Pfister*

Main category: cs.MA

TL;DR: 本文提出一种受黑板架构启发的新型多智能体通信范式，用于在大型异构数据湖中高效发现相关数据，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有单智能体系统难以应对大型异构数据湖中的数据发现任务，而基于主从范式的多智能体系统依赖中心控制器对子智能体能力的精确了解，缺乏灵活性和可扩展性。

Method: 提出一种基于黑板架构的多智能体通信框架：中心智能体将请求发布到共享黑板，自主的子智能体根据自身能力自愿响应，无需中心协调器预先了解所有子智能体的专业知识。

Result: 在KramaBench及修改后的DS-Bench和DA-Code三个基准上评估，该方法在端到端任务成功率上比基线提升13%至57%，在数据发现F1分数上最高提升9%。

Conclusion: 黑板范式是一种可扩展且通用的多智能体通信框架，能有效解决大型数据湖中的数据发现挑战。

Abstract: The rapid advancement of Large Language Models (LLMs) has opened new
opportunities in data science, yet their practical deployment is often
constrained by the challenge of discovering relevant data within large
heterogeneous data lakes. Existing methods struggle with this: single-agent
systems are quickly overwhelmed by large, heterogeneous files in the large data
lakes, while multi-agent systems designed based on a master-slave paradigm
depend on a rigid central controller for task allocation that requires precise
knowledge of each sub-agent's capabilities. To address these limitations, we
propose a novel multi-agent communication paradigm inspired by the blackboard
architecture for traditional AI models. In this framework, a central agent
posts requests to a shared blackboard, and autonomous subordinate agents --
either responsible for a partition of the data lake or general information
retrieval -- volunteer to respond based on their capabilities. This design
improves scalability and flexibility by eliminating the need for a central
coordinator to have prior knowledge of all sub-agents' expertise. We evaluate
our method on three benchmarks that require explicit data discovery: KramaBench
and modified versions of DS-Bench and DA-Code to incorporate data discovery.
Experimental results demonstrate that the blackboard architecture substantially
outperforms baselines, including RAG and the master-slave multi-agent paradigm,
achieving between 13% to 57% relative improvement in end-to-end task success
and up to a 9% relative gain in F1 score for data discovery over the
best-performing baselines across both proprietary and open-source LLMs. Our
findings establish the blackboard paradigm as a scalable and generalizable
communication framework for multi-agent systems.

</details>


### [5] [SimCity: Multi-Agent Urban Development Simulation with Rich Interactions](https://arxiv.org/abs/2510.01297)
*Yeqi Feng,Yucheng Lu,Hongyu Su,Tianxing He*

Main category: cs.MA

TL;DR: 本文提出了SimCity，一个基于大语言模型（LLM）的多智能体宏观经济模拟框架，能够自然再现多种经典宏观经济现象，并支持城市扩张动态研究。


<details>
  <summary>Details</summary>
Motivation: 传统宏观经济模型在处理异质性方面受限，而传统基于智能体的模型依赖人工制定的决策规则，缺乏灵活性和可解释性。因此，作者希望利用LLM构建一个更具适应性、可解释且能体现丰富交互的宏观经济模拟系统。

Method: SimCity框架利用大语言模型模拟四类核心智能体（家庭、企业、中央银行和政府），使其在劳动力市场、商品市场和金融市场中进行互动；同时引入视觉-语言模型（VLM）决定新企业的地理布局并渲染虚拟城市地图。

Result: SimCity成功复现了包括需求价格弹性、恩格尔定律、奥肯定律、菲利普斯曲线和贝弗里奇曲线在内的多个经典宏观经济规律，且在多次模拟中表现稳健。

Conclusion: SimCity展示了大语言模型在构建可解释、异质性强且具现实感的宏观经济模拟系统中的潜力，为宏观经济与城市动态联合研究提供了新范式。

Abstract: Large Language Models (LLMs) open new possibilities for constructing
realistic and interpretable macroeconomic simulations. We present SimCity, a
multi-agent framework that leverages LLMs to model an interpretable
macroeconomic system with heterogeneous agents and rich interactions. Unlike
classical equilibrium models that limit heterogeneity for tractability, or
traditional agent-based models (ABMs) that rely on hand-crafted decision rules,
SimCity enables flexible, adaptive behavior with transparent natural-language
reasoning. Within SimCity, four core agent types (households, firms, a central
bank, and a government) deliberate and participate in a frictional labor
market, a heterogeneous goods market, and a financial market. Furthermore, a
Vision-Language Model (VLM) determines the geographic placement of new firms
and renders a mapped virtual city, allowing us to study both macroeconomic
regularities and urban expansion dynamics within a unified environment. To
evaluate the framework, we compile a checklist of canonical macroeconomic
phenomena, including price elasticity of demand, Engel's Law, Okun's Law, the
Phillips Curve, and the Beveridge Curve, and show that SimCity naturally
reproduces these empirical patterns while remaining robust across simulation
runs.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [6] [FalseCrashReducer: Mitigating False Positive Crashes in OSS-Fuzz-Gen Using Agentic AI](https://arxiv.org/abs/2510.02185)
*Paschal C. Amusuo,Dongge Liu,Ricardo Andres Calvo Mendez,Jonathan Metzman,Oliver Chang,James C. Davis*

Main category: cs.SE

TL;DR: 本文提出两种AI驱动策略以减少OSS-Fuzz-Gen中自动生成模糊测试驱动程序所产生的误报崩溃：基于约束的驱动生成和基于上下文的崩溃验证，显著降低虚假崩溃率。


<details>
  <summary>Details</summary>
Motivation: 自动生成的模糊测试驱动程序常因无法满足函数对输入结构和状态的复杂要求而产生大量误报崩溃，影响工业级模糊测试系统（如OSS-Fuzz-Gen）的可信度和实用性。

Method: 1）基于约束的模糊驱动生成：在驱动创建过程中主动施加函数输入和状态约束；2）基于上下文的崩溃验证：通过分析函数调用者判断崩溃是否可从程序入口点触发。

Result: 在1500个OSS-Fuzz基准函数上实验表明，所提方法最多减少8%的虚假崩溃，报告崩溃数量减少一半以上，并验证前沿大语言模型可作为可靠的程序分析代理。

Conclusion: 将AI集成到大规模模糊测试流程中具有显著潜力，但也面临挑战；所提策略有效提升了自动生成模糊驱动的准确性和实用性。

Abstract: Fuzz testing has become a cornerstone technique for identifying software bugs
and security vulnerabilities, with broad adoption in both industry and
open-source communities. Directly fuzzing a function requires fuzz drivers,
which translate random fuzzer inputs into valid arguments for the target
function. Given the cost and expertise required to manually develop fuzz
drivers, methods exist that leverage program analysis and Large Language Models
to automatically generate these drivers. However, the generated fuzz drivers
frequently lead to false positive crashes, especially in functions highly
structured input and complex state requirements. This problem is especially
crucial in industry-scale fuzz driver generation efforts like OSS-Fuzz-en, as
reporting false positive crashes to maintainers impede trust in both the system
and the team.
  This paper presents two AI-driven strategies to reduce false positives in
OSS-Fuzz-Gen, a multi-agent system for automated fuzz driver generation. First,
constraint-based fuzz driver generation proactively enforces constraints on a
function's inputs and state to guide driver creation. Second, context-based
crash validation reactively analyzes function callers to determine whether
reported crashes are feasible from program entry points. Using 1,500 benchmark
functions from OSS-Fuzz, we show that these strategies reduce spurious crashes
by up to 8%, cut reported crashes by more than half, and demonstrate that
frontier LLMs can serve as reliable program analysis agents. Our results
highlight the promise and challenges of integrating AI into large-scale fuzzing
pipelines.

</details>


### [7] [Beyond Single LLMs: Enhanced Code Generation via Multi-Stage Performance-Guided LLM Orchestration](https://arxiv.org/abs/2510.01379)
*Huashan Chen,Zhenyu Qi,Haotang Li,Hong Chen,Jinfu Chen,Kebin Peng,In Kee Kim,Kyu Hyung Lee,Sen He*

Main category: cs.SE

TL;DR: 本文提出PerfOrch，一个无需微调的多阶段性能引导LLM编排框架，通过动态选择最适合的模型执行生成-修复-优化流程，在代码正确性和运行性能上显著优于单一模型（如GPT-4o）。


<details>
  <summary>Details</summary>
Motivation: 当前单一大语言模型（LLM）在自动代码生成中忽略了不同模型在编程语言、算法领域和开发阶段上的异构计算优势。

Method: 基于对17个先进LLM在五种语言和HumanEval-X基准上的全面实证研究，构建了一个多阶段、性能引导的编排框架PerfOrch，通过阶段验证与回滚机制动态路由任务至最优模型。

Result: PerfOrch在HumanEval-X和EffiBench-X上分别达到96.22%和91.37%的平均正确率，远超GPT-4o；并在58.76%的问题中提升执行速度，中位加速率达17.67%–27.66%。

Conclusion: PerfOrch提供了一种可扩展、即插即用的自动化软件工程新范式，能适应生成式AI的快速发展，在正确性和性能上均取得显著提升。

Abstract: While Large Language Models (LLMs) have become the predominant paradigm for
automated code generation, current single-model approaches fundamentally ignore
the heterogeneous computational strengths that different models exhibit across
programming languages, algorithmic domains, and development stages. This paper
challenges the single-model convention by introducing a multi-stage,
performance-guided orchestration framework that dynamically routes coding tasks
to the most suitable LLMs within a structured generate-fix-refine workflow. Our
approach is grounded in a comprehensive empirical study of 17 state-of-the-art
LLMs across five programming languages (Python, Java, C++, Go, and Rust) using
HumanEval-X benchmark. The study, which evaluates both functional correctness
and runtime performance metrics (execution time, mean/max memory utilization,
and CPU efficiency), reveals pronounced performance heterogeneity by language,
development stage, and problem category. Guided by these empirical insights, we
present PerfOrch, an LLM agent that orchestrates top-performing LLMs for each
task context through stage-wise validation and rollback mechanisms. Without
requiring model fine-tuning, PerfOrch achieves substantial improvements over
strong single-model baselines: average correctness rates of 96.22% and 91.37%
on HumanEval-X and EffiBench-X respectively, surpassing GPT-4o's 78.66% and
49.11%. Beyond correctness gains, the framework delivers consistent performance
optimizations, improving execution time for 58.76% of problems with median
speedups ranging from 17.67% to 27.66% across languages on two benchmarks. The
framework's plug-and-play architecture ensures practical scalability, allowing
new LLMs to be profiled and integrated seamlessly, thereby offering a paradigm
for production-grade automated software engineering that adapts to the rapidly
evolving generative AI landscape.

</details>


### [8] [Deciphering WONTFIX: A Mixed-Method Study on Why GitHub Issues Get Rejected](https://arxiv.org/abs/2510.01514)
*J. Alexander Curtis,Sharadha Kasiviswanathan,Nasir Eisty*

Main category: cs.SE

TL;DR: 本文研究了 GitHub 上“wontfix”标签的使用情况及其对开源项目管理和社区互动的影响，发现约30%的项目使用该标签，并归纳出八大常见原因。


<details>
  <summary>Details</summary>
Motivation: 尽管“wontfix”标签在 GitHub 上广泛使用，但其对项目管理和社区动态的具体影响尚不明确，因此有必要系统研究其使用原因与后果。

Method: 采用混合方法，从3,132个热门 GitHub 仓库中收集数据，结合定量分析（评估标签使用频率）和定性分析（通过开放式编码和主题分析归类使用原因）。

Result: 约30%的项目使用“wontfix”标签，主要用于用户提交的 bug 报告和功能请求；研究识别出八大使用该标签的主题原因，涵盖用户和维护者两方面因素。

Conclusion: “wontfix”标签是资源管理和引导贡献者的重要工具，但也可能抑制社区参与并降低项目管理透明度；理解其使用原因有助于提升开源协作效率。

Abstract: Context: The ``wontfix'' label is a widely used yet narrowly understood tool
in GitHub repositories, indicating that an issue will not be pursued further.
Despite its prevalence, the impact of this label on project management and
community dynamics within open-source software development is not clearly
defined. Objective: This study examines the prevalence and reasons behind
issues being labeled as wontfix across various open-source repositories on
GitHub. Method: Employing a mixed-method approach, we analyze both quantitative
data to assess the prevalence of the wontfix label and qualitative data to
explore the reasoning that it was used. Data were collected from 3,132 of
GitHub's most-popular repositories. Later, we employ open coding and thematic
analysis to categorize the reasons behind wontfix labels, providing a
structured understanding of the issue management landscape. Results: Our
findings show that about 30% of projects on GitHub apply the wontfix label to
some issues. These issues most often occur on user-submitted issues for bug
reports and feature requests. The study identified eight common themes behind
labeling issues as wontfix, ranging from user-specific control factors to
maintainer-specific decisions. Conclusions: The wontfix label is a critical
tool for managing resources and guiding contributor efforts in GitHub projects.
However, it can also discourage community involvement and obscure the
transparency of project management. Understanding these reasons aids project
managers in making informed decisions and fostering efficient collaboration
within open-source communities.

</details>


### [9] [MIMIC: Integrating Diverse Personality Traits for Better Game Testing Using Large Language Model](https://arxiv.org/abs/2510.01635)
*Yifei Chen,Sarra Habchi,Lili Wei*

Main category: cs.SE

TL;DR: 本文提出MIMIC框架，通过将多样化的个性特征融入游戏智能体，使其在相似情境下采用不同策略，从而提升游戏测试覆盖率与交互多样性，并在Minecraft中优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于强化学习、模仿学习或大语言模型的游戏智能体忽视了人类玩家因个性差异而产生的多样化策略，导致行为重复，难以触发丰富的游戏交互或发现边缘情况。

Method: 提出MIMIC框架，将不同的人格特质整合到游戏智能体中，使其能模仿多种游戏风格，在相似情境下采取不同策略。

Result: MIMIC在多个游戏中实现了更高的测试覆盖率和更丰富的交互，在Minecraft中任务完成率更高，且提供更丰富的解决方案。

Conclusion: MIMIC通过引入人格多样性显著提升了游戏智能体的测试效能，展现出在游戏测试中的巨大潜力。

Abstract: Modern video games pose significant challenges for traditional automated
testing algorithms, yet intensive testing is crucial to ensure game quality. To
address these challenges, researchers designed gaming agents using
Reinforcement Learning, Imitation Learning, or Large Language Models. However,
these agents often neglect the diverse strategies employed by human players due
to their different personalities, resulting in repetitive solutions in similar
situations. Without mimicking varied gaming strategies, these agents struggle
to trigger diverse in-game interactions or uncover edge cases.
  In this paper, we present MIMIC, a novel framework that integrates diverse
personality traits into gaming agents, enabling them to adopt different gaming
strategies for similar situations. By mimicking different playstyles, MIMIC can
achieve higher test coverage and richer in-game interactions across different
games. It also outperforms state-of-the-art agents in Minecraft by achieving a
higher task completion rate and providing more diverse solutions. These results
highlight MIMIC's significant potential for effective game testing.

</details>


### [10] [FOSS-chain: using blockchain for Open Source Software license compliance](https://arxiv.org/abs/2510.01740)
*Kypros Iacovou,Georgia M. Kapitsaki,Evangelia Vanezi*

Main category: cs.SE

TL;DR: 本文提出FOSS-chain，一个结合区块链技术的开源软件许可证管理平台，用于自动化处理衍生作品中的许可证兼容性问题，并通过初步用户研究验证其可行性。


<details>
  <summary>Details</summary>
Motivation: 开源软件许可证的合规性管理复杂，许可证之间的不兼容可能导致法律纠纷；区块链技术因其不可篡改性和透明性，可有效记录软件变更并提升许可证管理的可信度。

Method: 设计并实现了一个名为FOSS-chain的Web平台，利用区块链技术自动化管理14种开源许可证的兼容性检查与合规流程。

Result: 通过小规模用户研究对FOSS-chain原型进行了初步评估，结果表明该平台在真实软件系统中具有应用潜力。

Conclusion: 将区块链技术整合到开源许可证管理中是可行且有前景的，有助于提升许可证合规的自动化与透明度。

Abstract: Open Source Software (OSS) is widely used and carries licenses that indicate
the terms under which the software is provided for use, also specifying
modification and distribution rules. Ensuring that users are respecting OSS
license terms when creating derivative works is a complex process. Compliance
issues arising from incompatibilities among licenses may lead to legal
disputes. At the same time, the blockchain technology with immutable entries
offers a mechanism to provide transparency when it comes to licensing and
ensure software changes are recorded. In this work, we are introducing an
integration of blockchain and license management when creating derivative
works, in order to tackle the issue of OSS license compatibility. We have
designed, implemented and performed a preliminary evaluation of FOSS-chain, a
web platform that uses blockchain and automates the license compliance process,
covering 14 OSS licenses. We have evaluated the initial prototype version of
the FOSS-chain platform via a small scale user study. Our preliminary results
are promising, demonstrating the potential of the platform for adaptation on
realistic software systems.

</details>


### [11] [ARENA: A tool for measuring and analysing the energy efficiency of Android apps](https://arxiv.org/abs/2510.01754)
*Hina Anwar*

Main category: cs.SE

TL;DR: 本文提出了ARENA，一个集成于IDE（如IntelliJ和Android Studio）的插件工具，用于简化基于硬件的Android应用能耗测量流程，支持自动执行测试、数据采集、分析与可视化。


<details>
  <summary>Details</summary>
Motivation: 当前基于硬件的Android应用能耗测量过程繁琐、耗时且缺乏可复现性，同时缺少开源工具支持开发者和研究人员进行可靠测量。

Method: 开发ARENA工具，作为IDE插件，连接物理测量设备，在开发过程中自动执行测试场景、采集电流/电压数据，并对数据进行清洗、聚合、统计分析与可视化。

Result: ARENA使开发者能够在IDE内便捷地比较不同应用或版本的能耗，实现自动化的能耗测量与数据洞察。

Conclusion: ARENA有效降低了硬件级能耗测量的门槛，提升了测量的可重复性与可用性，为开发者和研究人员提供了实用的支持工具。

Abstract: To build energy-efficient apps, there is a need to estimate and analyze their
energy consumption in typical usage scenarios. The energy consumption of
Android apps could be estimated via software-based and hardware-based
approaches. Software-based approaches, while easier to implement, are not as
accurate as hardware-based approaches. The process of measuring the energy
consumption of an Android app via a hardware-based approach typically involves
1) setting up a measurement environment, 2) executing the app under test on a
mobile device, 3) recording current/voltage data via a hardware device to
measure energy consumption, and 4) cleaning and aggregating data for analyses,
reports, and visualizations. Specialized scripts are written for selected
hardware and software components to ensure reliable energy measurements. The
energy measurement process is repeated many times and aggregated to remove
noise. These steps make the hardware-based energy measurement process
time-consuming and not easy to adapt or reproduce. There is a lack of
open-source tools available for developers and researchers to take reliable
energy measurements via hardware devices. In this paper, we present and
demonstrate ARENA, a support tool that enables developers and researchers to
connect to a physical measurement device without leaving the comfort of their
IDE. Developers could use ARENA during development to compare energy
consumption between different apps or versions of the same app. ARENA
calculates energy consumption on an Android smartphone by executing a test
scenario on the app under development. Further, ARENA helps aggregate,
statistically analyze, report, and visualize the data, allowing developers and
researchers to dig into the data directly or visually. We implemented ARENA as
an IntelliJ and Android Studio plugin.

</details>


### [12] [Towards Speeding up Program Repair with Non-Autoregressive Model](https://arxiv.org/abs/2510.01825)
*Zhenyu Yang,Yue Pan,Zhen Yang,Zhongxing Yu*

Main category: cs.SE

TL;DR: 本文提出NARRepair，首个面向自动程序修复（APR）任务的非自回归（NAR）代码生成模型，在保证修复质量的同时显著提升修复速度。


<details>
  <summary>Details</summary>
Motivation: 现有基于自回归（AR）的机器学习APR方法逐token生成修复代码，导致修复延迟严重，尤其在大模型中更为明显；而直接采用非自回归（NAR）方法又会降低补丁质量。

Method: 提出NARRepair模型，包含三项创新：1）修复动作预测器以缓解过度修复问题；2）token间依赖提取器以补充token依赖信息；3）两阶段解码器以增强上下文信息。

Result: 在三个常用APR数据集上的实验表明：1）在限定时间内，NARRepair修复效果优于其他APR方法；2）相比AR方法，其修复速度在GPU环境下提升1.4–6.4倍。

Conclusion: NARRepair在修复速度与准确率方面均达到当前最优的综合性能，验证了NAR方法在APR任务中的有效性与潜力。

Abstract: Enlightened by the success of machine learning techniques in various
application areas, recent years have witnessed a surge of research efforts on
automatic program repair (APR) using machine learning techniques. Previous
machine learning-based APR techniques essentially modified bugs in the
autoregressive (AR) manner, which predicts future values based on past values.
Due to the manner of token-by-token generation, the AR-based APR technique has
a huge time delay. In particular, the delay of the APR model with a large
number of parameters is more serious. To address the issue, we aim to apply the
non-autoregressive (NAR) method to the APR task, which can output target code
in a parallel manner to avoid huge repair delays. However, the naive use of the
NAR manner for the APR task suffers from the issue of compromised patch
quality. To effectively adapt the NAR manner for the APR task, we in this paper
propose NARRepair, the first customized NAR code generation model for the APR
task. The NARRepair model features three major novelties, including 1) the
repair action predictor for alleviating the over-correction issue, 2) the
inter-token dependency extractor for alleviating the issue of lacking
inter-token dependency information, and 3) the two-stage decoder for
alleviating the issue of lacking contextual information. We evaluated NARRepair
on three widely used datasets in the APR community, and the results show that
1) compared to other APR techniques, the NARRepair model has the best
performance within the limited repair time, and 2) compared to AR-based APR
techniques, the repair speed of NARRepair has been increased by 1.4-6.4 times
in the GPU environment. Overall, the results show that NARRepair has achieved
state-of-the-art comprehensive performance in terms of repair speed and
accuracy.

</details>


### [13] [RefFilter: Improving Semantic Conflict Detection via Refactoring-Aware Static Analysis](https://arxiv.org/abs/2510.01960)
*Victor Lira,Paulo Borba,Rodrigo Bonifácio,Galileu Santos e Matheus barbosa*

Main category: cs.SE

TL;DR: 本文提出 RefFilter，一种结合自动化重构检测的语义干扰检测工具，通过过滤行为保持型重构显著降低误报率，提升合并支持的精度。


<details>
  <summary>Details</summary>
Motivation: 当前轻量级静态分析方法在检测协作开发中的语义干扰时存在大量误报，主要原因是无法区分行为保持型重构与真正影响行为的变更。

Method: RefFilter 在现有静态分析技术基础上引入自动化重构识别机制，过滤掉行为保持型重构，从而减少误报，同时保留对真实干扰的检测能力。

Result: 在包含99个标注场景的数据集上，RefFilter 将误报率降低了近32%，尽管召回率略有下降，但整体精度显著提升；在包含1,087个合并场景的新数据集上也验证了其可扩展性。

Conclusion: 引入重构感知的干扰检测是一种实用且有效的方法，能显著提升现代软件开发流程中代码合并的支持能力。

Abstract: Detecting semantic interference remains a challenge in collaborative software
development. Recent lightweight static analysis techniques improve efficiency
over SDG-based methods, but they still suffer from a high rate of false
positives. A key cause of these false positives is the presence of
behavior-preserving code refactorings, which current techniques cannot
effectively distinguish from changes that impact behavior and can interfere
with others. To handle this problem we present RefFilter, a refactoring-aware
tool for semantic interference detection. It builds on existing static
techniques by incorporating automated refactoring detection to improve
precision. RefFilter discards behavior-preserving refactorings from reports,
reducing false positives while preserving detection coverage. To evaluate
effectiveness and scalability, use two datasets: a labeled dataset with 99
scenarios and ground truth, and a novel dataset of 1,087 diverse merge
scenarios that we have built. Experimental results show that RefFilter reduces
false positives by nearly 32% on the labeled dataset. While this reduction
comes with a non significant increase in false negatives, the overall gain in
precision significantly outweighs the minor trade-off in recall. These findings
demonstrate that refactoring-aware interference detection is a practical and
effective strategy for improving merge support in modern development workflows.

</details>


### [14] [Clarifying Semantics of In-Context Examples for Unit Test Generation](https://arxiv.org/abs/2510.01994)
*Chen Yang,Lin Yang,Ziqi Wang,Dong Wang,Jianyi Zhou,Junjie Chen*

Main category: cs.SE

TL;DR: 本文提出CLAST方法，通过程序分析与大语言模型重写，系统性地提升单元测试的语义清晰度，从而显著增强其作为上下文示例在大模型单元测试生成中的效果。


<details>
  <summary>Details</summary>
Motivation: 现有基于上下文学习（ICL）的单元测试生成方法严重依赖示例质量，而语义不清或结构不佳的测试示例会导致生成结果不佳。

Method: CLAST将复杂测试分解为逻辑更清晰的测试，并结合程序分析与大语言模型进行重写，以提升语义清晰度。

Result: 在四个开源项目和三个工业项目上的实验表明，CLAST在保持测试有效性的同时显著优于现有方法UTgen；用户研究中超过85%的参与者偏好CLAST生成的测试；将其作为示例可使RAGGen和TELPA等方法的编译成功率、通过率和覆盖率平均提升25.97%、28.22%和45.99%。

Conclusion: CLAST能有效提升单元测试的语义清晰度，不仅保留原始测试效果，还能显著增强基于ICL的测试生成性能，在软件测试实践中具有重要应用潜力。

Abstract: Recent advances in large language models (LLMs) have enabled promising
performance in unit test generation through in-context learning (ICL). However,
the quality of in-context examples significantly influences the effectiveness
of generated tests-poorly structured or semantically unclear test examples
often lead to suboptimal outputs. In this paper, we propose CLAST, a novel
technique that systematically refines unit tests to improve their semantic
clarity, thereby enhancing their utility as in-context examples. The approach
decomposes complex tests into logically clearer ones and improves semantic
clarity through a combination of program analysis and LLM-based rewriting. We
evaluated CLAST on four open-source and three industrial projects. The results
demonstrate that CLAST largely outperforms UTgen, the state-of-the-art
refinement technique, in both preserving test effectiveness and enhancing
semantic clarity. Specifically, CLAST fully retains the original effectiveness
of unit tests, while UTgen reduces compilation success rate (CSR), pass rate
(PR), test coverage (Cov), and mutation score (MS) by an average of 12.90%,
35.82%, 4.65%, and 5.07%, respectively. Over 85.33% of participants in our user
study preferred the semantic clarity of CLAST-refined tests. Notably,
incorporating CLAST-refined tests as examples effectively improves ICL-based
unit test generation approaches such as RAGGen and TELPA, resulting in an
average increase of 25.97% in CSR, 28.22% in PR, and 45.99% in Cov for
generated tests, compared to incorporating UTgen-refined tests. The insights
from the follow-up user study not only reinforce CLAST's potential impact in
software testing practice but also illuminate avenues for future research.

</details>


### [15] [Automatic Generation of Combinatorial Reoptimisation Problem Specifications: A Vision](https://arxiv.org/abs/2510.02002)
*Maximilian Kratz,Steffen Zschaler,Jens Kosiol,Gabriele Taentzer*

Main category: cs.SE

TL;DR: 本文提出利用模型驱动工程（MDE）方法，特别是声明式建模语言和模型转换，系统地从原始优化问题规范中推导出再优化问题，以应对上下文变化带来的解决方案调整需求。


<details>
  <summary>Details</summary>
Motivation: 优化问题求解后，当上下文因素变化时，需对解进行适应性调整（即再优化）。传统方法需重新求解修改后的问题，但新问题具有特殊要求：仅做最小改动、部分原解不可更改、需生成变更脚本。现有方法缺乏系统化支持。

Method: 采用模型驱动工程（MDE）方法，使用声明式建模语言和模型转换来高层描述优化问题，并据此系统推导再优化问题。作者对组合再优化问题进行了初步分类，并基于GIPS工具实现了一个概念验证系统。

Result: 提出了一种基于MDE的再优化问题推导框架，对变化类型和推导策略进行了初步分类，并通过GIPS工具在助教分配案例中验证了方法的可行性。

Conclusion: 模型驱动工程为再优化问题提供了系统化的新途径，能有效支持从原始问题规范自动生成再优化问题，尤其适用于需最小化变更和保留部分原解的场景。

Abstract: Once an optimisation problem has been solved, the solution may need
adaptation when contextual factors change. This challenge, also known as
reoptimisation, has been addressed in various problem domains, such as railway
crew rescheduling, nurse rerostering, or aircraft recovery. This requires a
modified problem to be solved again to ensure that the adapted solution is
optimal in the new context. However, the new optimisation problem differs
notably from the original problem: (i) we want to make only minimal changes to
the original solution to minimise the impact; (ii) we may be unable to change
some parts of the original solution (e.g., because they refer to past
allocations); and (iii) we need to derive a change script from the original
solution to the new solution. In this paper, we argue that Model-Driven
Engineering (MDE) - in particular, the use of declarative modelling languages
and model transformations for the high-level specification of optimisation
problems - offers new opportunities for the systematic derivation of
reoptimisation problems from the original optimisation problem specification.
We focus on combinatorial reoptimisation problems and provide an initial
categorisation of changing problems and strategies for deriving the
corresponding reoptimisation specifications. We introduce an initial
proof-of-concept implementation based on the GIPS (Graph-Based (Mixed) Integer
Linear Programming Problem Specification) tool and apply it to an example
resource-allocation problem: the allocation of teaching assistants to teaching
sessions.

</details>


### [16] [ACM SIGSOFT SEN Empirical Software Engineering: Introducing Our New Regular Column](https://arxiv.org/abs/2510.02007)
*Justus Bogner,Roberto Verdecchia*

Main category: cs.SE

TL;DR: 本文介绍了ACM SIGSOFT SEN新专栏SEN-ESE，旨在探讨实证软件工程（ESE）研究中的元层面问题，如可复现性、外部有效性、评审主观性及工业落地等挑战，并通过专家访谈、焦点小组等方式促进社区对ESE研究方法与实践的反思与改进。


<details>
  <summary>Details</summary>
Motivation: 实证软件工程（ESE）虽已成熟，但仍面临可复现性差、外部有效性有限、评审主观性强、研究成果难以工业落地等问题，且许多研究细节缺乏明确文档，不利于新人入门。因此需要一个专门平台讨论这些元层面议题。

Method: 通过设立定期专栏（SEN-ESE），采用专家访谈、焦点小组、调查和立场文章等形式，系统探讨ESE研究中的隐性或较少被讨论的方法论与实践问题。

Result: 提出并启动一个面向ESE社区的持续对话平台，聚焦研究方法、工具、跨学科发表等具体议题，以提升ESE研究的质量、透明度与影响力。

Conclusion: SEN-ESE专栏旨在激发社区对ESE研究中隐性或争议性话题的讨论，推动研究实践的持续改进，并欢迎社区成员贡献议题与建议，共同塑造该专栏的发展方向。

Abstract: From its early foundations in the 1970s, empirical software engineering (ESE)
has evolved into a mature research discipline that embraces a plethora of
different topics, methodologies, and industrial practices. Despite its
remarkable progress, the ESE research field still needs to keep evolving, as
new impediments, shortcoming, and technologies emerge. Research
reproducibility, limited external validity, subjectivity of reviews, and
porting research results to industrial practices are just some examples of the
drivers for improvements to ESE research. Additionally, several facets of ESE
research are not documented very explicitly, which makes it difficult for
newcomers to pick them up. With this new regular ACM SIGSOFT SEN column
(SEN-ESE), we introduce a venue for discussing meta-aspects of ESE research,
ranging from general topics such as the nature and best practices for
replication packages, to more nuanced themes such as statistical methods,
interview transcription tools, and publishing interdisciplinary research. Our
aim for the column is to be a place where we can regularly spark conversations
on ESE topics that might not often be touched upon or are left implicit.
Contributions to this column will be grounded in expert interviews, focus
groups, surveys, and position pieces, with the goal of encouraging reflection
and improvement in how we conduct, communicate, teach, and ultimately improve
ESE research. Finally, we invite feedback from the ESE community on
challenging, controversial, or underexplored topics, as well as suggestions for
voices you would like to hear from. While we cannot promise to act on every
idea, we aim to shape this column around the community interests and are
grateful for all contributions.

</details>


### [17] [Towards fairer public transit: Real-time tensor-based multimodal fare evasion and fraud detection](https://arxiv.org/abs/2510.02165)
*Peter Wauyo,Dalia Bwiza,Alain Murara,Edwin Mugume,Eric Umuhoza*

Main category: cs.SE

TL;DR: 本文提出了一种基于多模态融合的公共交通欺诈检测系统，结合视频（ViViT）与音频（AST）特征，并采用张量融合网络（TFN）进行高效融合，在自建数据集上实现了89.5%的准确率和84.0%的召回率，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 公共交通中的逃票和欺诈行为造成严重收入损失，现有检测方法在多模态信息融合方面存在不足，难以有效捕捉视觉与音频线索之间的复杂交互。

Method: 系统采用ViViT模型提取视频特征，AST模型处理音频数据，并通过张量融合网络（TFN）显式建模单模态与双模态交互，以捕捉视觉行为（如尾随、非法进入）与音频线索（如刷卡声）之间的跨模态动态。

Result: 在自建数据集上，系统达到89.5%准确率、87.2%精确率和84.0%召回率，F1分数和召回率分别比传统拼接方法提升7.0%和8.8%，显著优于早期融合基线和现有交通欺诈检测系统。

Conclusion: 所提出的多模态张量融合方法在公共交通欺诈检测中表现出色，支持实时应用，有助于减少收入损失、提升乘客安全并保障运营合规性。

Abstract: This research introduces a multimodal system designed to detect fraud and
fare evasion in public transportation by analyzing closed circuit television
(CCTV) and audio data. The proposed solution uses the Vision Transformer for
Video (ViViT) model for video feature extraction and the Audio Spectrogram
Transformer (AST) for audio analysis. The system implements a Tensor Fusion
Network (TFN) architecture that explicitly models unimodal and bimodal
interactions through a 2-fold Cartesian product. This advanced fusion technique
captures complex cross-modal dynamics between visual behaviors (e.g.,
tailgating,unauthorized access) and audio cues (e.g., fare transaction sounds).
The system was trained and tested on a custom dataset, achieving an accuracy of
89.5%, precision of 87.2%, and recall of 84.0% in detecting fraudulent
activities, significantly outperforming early fusion baselines and exceeding
the 75% recall rates typically reported in state-of-the-art transportation
fraud detection systems. Our ablation studies demonstrate that the tensor
fusion approach provides a 7.0% improvement in the F1 score and an 8.8% boost
in recall compared to traditional concatenation methods. The solution supports
real-time detection, enabling public transport operators to reduce revenue
loss, improve passenger safety, and ensure operational compliance.

</details>


### [18] [SIEVE: Towards Verifiable Certification for Code-datasets](https://arxiv.org/abs/2510.02166)
*Fatou Ndiaye Mbodji,El-hacen Diallo,Jordan Samhi,Kui Liu,Jacques Klein,Tegawendé F. Bissyande*

Main category: cs.SE

TL;DR: SIEVE 是一个社区驱动的框架，通过将数据集属性检查转化为可验证的“置信卡”，提供具有统计保证的代码数据集质量认证，以替代传统的静态数据集卡片。


<details>
  <summary>Details</summary>
Motivation: 当前公共代码数据集缺乏可验证的质量保证，静态数据集卡片不可审计且无统计保障，导致团队需各自构建孤立的数据清洗流程，造成资源浪费和成本上升。

Method: 提出 SIEVE 框架，将针对数据集属性的检查转化为机器可读、可验证的“置信卡”，并提供任意时间有效的统计边界。

Result: 该框架有望降低代码数据集质量保证的成本，并提升对其质量的信任。

Conclusion: 通过用可随时验证的认证机制取代叙述性数据集卡片，SIEVE 有望成为提升代码数据集可信度和效率的新范式。

Abstract: Code agents and empirical software engineering rely on public code datasets,
yet these datasets lack verifiable quality guarantees. Static 'dataset cards'
inform, but they are neither auditable nor do they offer statistical
guarantees, making it difficult to attest to dataset quality. Teams build
isolated, ad-hoc cleaning pipelines. This fragments effort and raises cost. We
present SIEVE, a community-driven framework. It turns per-property checks into
Confidence Cards-machine-readable, verifiable certificates with anytime-valid
statistical bounds. We outline a research plan to bring SIEVE to maturity,
replacing narrative cards with anytime-verifiable certification. This shift is
expected to lower quality-assurance costs and increase trust in code-datasets.

</details>


### [19] [TAIBOM: Bringing Trustworthiness to AI-Enabled Systems](https://arxiv.org/abs/2510.02169)
*Vadim Safronov,Anthony McCaigue,Nicholas Allott,Andrew Martin*

Main category: cs.SE

TL;DR: 本文提出了可信AI物料清单（TAIBOM），将传统软件物料清单（SBOM）扩展至AI领域，以应对AI系统在依赖管理、完整性验证和可信溯源方面的独特挑战。


<details>
  <summary>Details</summary>
Motivation: 现有SBOM框架无法有效描述AI系统的动态性、数据驱动特性以及数据集、模型和软件组件之间松耦合的依赖关系，且缺乏支持AI环境中完整性、信任和合规性的工具。

Method: 作者设计了TAIBOM框架，包含：(i) 面向AI组件的结构化依赖模型，(ii) 跨异构AI流水线传播完整性声明的机制，以及(iii) 验证组件来源的信任证明流程。

Result: TAIBOM在AI工作流中有效支持了系统保障、安全性和合规性，并在功能上优于SPDX和CycloneDX等现有标准。

Conclusion: TAIBOM为构建可验证、可信赖的AI系统提供了基础，通过结构化的软件透明性提升了AI供应链的可信度。

Abstract: The growing integration of open-source software and AI-driven technologies
has introduced new layers of complexity into the software supply chain,
challenging existing methods for dependency management and system assurance.
While Software Bills of Materials (SBOMs) have become critical for enhancing
transparency and traceability, current frameworks fall short in capturing the
unique characteristics of AI systems -- namely, their dynamic, data-driven
nature and the loosely coupled dependencies across datasets, models, and
software components. These challenges are compounded by fragmented governance
structures and the lack of robust tools for ensuring integrity, trust, and
compliance in AI-enabled environments.
  In this paper, we introduce Trusted AI Bill of Materials (TAIBOM) -- a novel
framework extending SBOM principles to the AI domain. TAIBOM provides (i) a
structured dependency model tailored for AI components, (ii) mechanisms for
propagating integrity statements across heterogeneous AI pipelines, and (iii) a
trust attestation process for verifying component provenance. We demonstrate
how TAIBOM supports assurance, security, and compliance across AI workflows,
highlighting its advantages over existing standards such as SPDX and CycloneDX.
This work lays the foundation for trustworthy and verifiable AI systems through
structured software transparency.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [20] [Odontoceti: Ultra-Fast DAG Consensus with Two Round Commitment](https://arxiv.org/abs/2510.01216)
*Preston Vander Vos*

Main category: cs.DC

TL;DR: Odontoceti 是一种新型 DAG 共识协议，在容忍 20% 拜占庭节点（而非传统的 33%）的前提下，通过两轮通信实现区块确认，达到 300 毫秒中位延迟和每秒 1 万笔交易的吞吐量，显著优于现有协议。


<details>
  <summary>Details</summary>
Motivation: 区块链用户对可扩展性有高要求，期望快速确认和即时交易处理，而现有协议在延迟和吞吐量方面仍有不足，因此需要一种在实际网络条件下兼顾性能与可行安全性的新共识机制。

Method: Odontoceti 采用基于 DAG 的结构，使用 n = 5f + 1 个验证者，设计了一种无需证书的 DAG 和新颖的区块提交决策规则，并通过优化机制在参与者响应慢时推进共识，特别适用于更常见的崩溃故障场景。

Result: 在真实网络条件下，Odontoceti 实现了 300 毫秒的中位延迟和每秒 1 万笔交易的处理能力，相比现有生产协议降低延迟 20–25%，验证了将通信轮次从三轮减至两轮的有效性。

Conclusion: 该论文证明了在适度降低容错率（20%）的前提下，低延迟、高吞吐的共识协议在区块链中具有实际可行性，为未来可扩展区块链系统提供了新方向。

Abstract: Users of blockchains value scalability, expecting fast confirmations and
immediate transaction processing. Odontoceti, the latest in DAG-based
consensus, addresses these concerns by prioritizing low latency and high
throughput, making a strategic trade-off in security by operating with a 20%
fault tolerance instead of the established 33% level. It is the first DAG-based
protocol to achieve commitment in just two communication rounds, delivering
median latency of 300 milliseconds while processing 10,000 transactions per
second under realistic network conditions. Odontoceti operates with n = 5f + 1
validators and creates an uncertified DAG with a novel decision rule for
committing blocks. The protocol includes an optimization that advances progress
when participants are slow, benefiting crash fault scenarios which are more
common in practice than Byzantine faults. Evaluation results demonstrate 20-25%
latency improvements compared to an existing production protocol, validating
that reducing wave length from three rounds to two rounds yields meaningful
performance benefits. This paper establishes the practical viability of lower
fault tolerance consensus protocols for blockchains.

</details>


### [21] [Kant: An Efficient Unified Scheduling System for Large-Scale AI Clusters](https://arxiv.org/abs/2510.01256)
*Lingling Zeng,Gen Zhang,Jialin Peng,Xiang Xu,Yuan Xu,Lijun Ma*

Main category: cs.DC

TL;DR: 本文提出了Kant——一个面向大规模AI容器集群的高效统一调度平台，支持训练与推理任务的协同调度，并通过Backfill和增强Binpack等策略显著提升资源利用率和调度效率。


<details>
  <summary>Details</summary>
Motivation: 随着AI集群规模扩大及大语言模型训练与推理需求激增，传统调度系统难以在资源利用率、调度效率和服务质量之间取得平衡。

Method: 设计并实现Kant调度平台，引入Backfill和增强Binpack（E-Binpack）等调度策略，并定义GPU分配率（GAR）、调度占用率（SOR）、GPU节点碎片率（GFR）、作业等待时间分布（JWTD）和作业训练时间估计分布（JTTED）等关键评估指标。

Result: Kant在数百至数万GPU规模的集群中表现出色，显著提升资源利用率和调度效率，有效降低资源碎片和分布式训练通信开销，并已在多个AI数据中心稳定部署。

Conclusion: Kant为构建高性能、高可用、AI原生的调度基础设施提供了一种切实可行的工程方案。

Abstract: As AI cluster sizes continue to expand and the demand for
large-language-model (LLM) training and inference workloads grows rapidly,
traditional scheduling systems face significant challenges in balancing
resource utilization, scheduling efficiency, and service quality. This paper
presents and evaluates Kant: an efficient unified scheduling platform designed
for large-scale AI container clusters, supporting the co-scheduling of both
training and inference jobs. Based on the practical implementation of the Kant
system, we systematically define a set of key evaluation metrics for AI
clusters, including GPU Allocation Ratio (GAR), Scheduling Occupancy Rate
(SOR), GPU Node Fragmentation Ratio (GFR), Job Waiting Time Distribution
(JWTD), and Job Training Time Estimation Distribution (JTTED), providing a
foundation for quantitative performance analysis. Experimental results
demonstrate that Kant achieves exceptional performance in clusters ranging from
hundreds to tens of thousands of GPUs. By leveraging scheduling strategies such
as Backfill and Enhanced Binpack (E-Binpack), the system significantly improves
resource utilization and scheduling efficiency, while effectively reducing
resource fragmentation and communication overhead in distributed training. The
system has been deployed in multiple AI data center clusters, where it stably
supports large-scale intelligent computing workloads. This work provides a
practical engineering approach for building high-performance, highly available,
AI-native scheduling infrastructure.

</details>


### [22] [Accuracy vs Performance: An abstraction model for deadline constrained offloading at the mobile-edge](https://arxiv.org/abs/2510.01885)
*Jamie Cotter,Ignacio Castineiras,Victor Cionca*

Main category: cs.DC

TL;DR: 本文提出了一种面向移动边缘设备的低延迟、截止时间约束的DNN卸载调度算法，通过轻量级网络状态表示、动态带宽估计和优先级感知抢占机制，在高负载下显著提升任务吞吐量。


<details>
  <summary>Details</summary>
Motivation: 在移动边缘计算场景中，DNN任务常受延迟和截止时间约束，现有调度方法在资源紧张时难以有效保证性能，亟需一种能兼顾设备可用性、网络通信和任务优先级的低开销调度方案。

Method: 设计了一种轻量级调度算法，包含资源可用性表示、网络离散化和动态带宽估计机制，并在由四个树莓派组成的边缘系统上实现，用于垃圾分拣传送带的实时视频帧处理。

Result: 在高负载条件下，所提方法相比作者先前已优于work-stealing和非抢占式启发式算法的方案，进一步降低了延迟并提高了任务吞吐量，尤其在资源稀缺时表现更佳。

Conclusion: 该调度算法通过新颖的低延迟抽象模型和动态带宽估计，有效提升了移动边缘设备上DNN任务的调度效率与系统吞吐能力，适用于实时性要求高的边缘智能场景。

Abstract: In this paper, we present a solution for low-latency deadline-constrained DNN
offloading on mobile edge devices. We design a scheduling algorithm with
lightweight network state representation, considering device availability,
communication on the network link, priority-aware pre-emption, and task
deadlines. The scheduling algorithm aims to reduce latency by designing a
resource availability representation, as well as a network discretisation and a
dynamic bandwidth estimation mechanism. We implement the scheduling algorithm
into a system composed of four Raspberry Pi 2 (model Bs) mobile edge devices,
sampling a waste classification conveyor belt at a set frame rate. The system
is evaluated and compared to a previous approach of ours, which was proven to
outcompete work-stealers and a non-pre-emption based scheduling heuristic under
the aforementioned waste classification scenario. Our findings show the novel
lower latency abstraction models yield better performance under high-volume
workloads, with the dynamic bandwidth estimation assisting the task placement
while, ultimately, increasing task throughput in times of resource scarcity.

</details>


### [23] [IoT-MCP: Bridging LLMs and IoT Systems Through Model Context Protocol](https://arxiv.org/abs/2510.01260)
*Ningyuan Yang,Guanliang Lyu,Mingchen Ma,Yiyi Lu,Yiming Li,Zhihui Gao,Hancheng Ye,Jianyi Zhang,Tingjun Chen,Yiran Chen*

Main category: cs.DC

TL;DR: 本文提出了IoT-MCP框架，通过边缘部署的服务器实现模型上下文协议（MCP），以标准化大语言模型（LLM）与物联网（IoT）设备之间的通信，并配套发布了首个LLM-IoT评测基准IoT-MCP Bench，实验表明该框架在任务成功率、响应时间和内存占用方面表现优异。


<details>
  <summary>Details</summary>
Motivation: 大语言模型与物联网系统集成面临硬件异构性和控制复杂性的挑战，亟需标准化通信机制以实现高效、可靠的交互。

Method: 提出IoT-MCP框架，利用边缘服务器部署MCP协议，连接LLM与IoT设备；同时构建包含114个基础任务和1140个复杂任务的评测基准IoT-MCP Bench。

Result: 在22种传感器和6种微控制器上验证，IoT-MCP实现100%任务成功率、205ms平均响应时间及74KB峰值内存占用。

Conclusion: IoT-MCP为LLM与IoT集成提供了开源框架和标准化评测方法，有效解决了异构硬件环境下的通信与控制难题。

Abstract: The integration of Large Language Models (LLMs) with Internet-of-Things (IoT)
systems faces significant challenges in hardware heterogeneity and control
complexity. The Model Context Protocol (MCP) emerges as a critical enabler,
providing standardized communication between LLMs and physical devices. We
propose IoT-MCP, a novel framework that implements MCP through edge-deployed
servers to bridge LLMs and IoT ecosystems. To support rigorous evaluation, we
introduce IoT-MCP Bench, the first benchmark containing 114 Basic Tasks (e.g.,
``What is the current temperature?'') and 1,140 Complex Tasks (e.g., ``I feel
so hot, do you have any ideas?'') for IoT-enabled LLMs. Experimental validation
across 22 sensor types and 6 microcontroller units demonstrates IoT-MCP's 100%
task success rate to generate tool calls that fully meet expectations and
obtain completely accurate results, 205ms average response time, and 74KB peak
memory footprint. This work delivers both an open-source integration framework
(https://github.com/Duke-CEI-Center/IoT-MCP-Servers) and a standardized
evaluation methodology for LLM-IoT systems.

</details>


### [24] [QScale: Probabilistic Chained Consensus for Moderate-Scale Systems](https://arxiv.org/abs/2510.01536)
*Hasan Heydari,Alysson Bessani,Kartik Nayak*

Main category: cs.DC

TL;DR: 本文提出了QScale协议，旨在为中等规模（数百到上千节点）的分布式账本系统提供次线性单节点通信复杂度、次平方总通信复杂度和低延迟的解决方案。


<details>
  <summary>Details</summary>
Motivation: 现有分布式账本协议要么通信复杂度高，仅适用于小规模系统（如PBFT），要么依赖委员会抽样机制，仅适用于超大规模系统（如Algorand），缺乏适用于中等规模生产系统的高效协议。

Method: 提出QScale协议，通过设计新颖的通信结构，在保证安全性和活性的前提下，实现每个节点每区块期望通信复杂度为$\widetilde{O}(\kappa \sqrt{n})$，总通信复杂度为$\widetilde{O}(n\kappa)$，最佳情况下区块确认延迟为$O(\kappa)$轮。

Result: QScale在中等规模系统中实现了次线性单节点通信开销、次平方总通信开销和低延迟，同时以压倒性概率保证安全性和活性。

Conclusion: QScale填补了现有协议在中等规模分布式账本场景下的空白，为实际生产系统（如Redbelly、Sui）提供了一种高效可行的新方案。

Abstract: Existing distributed ledger protocols either incur a high communication
complexity and are thus suited to systems with a small number of processes
(e.g., PBFT), or rely on committee-sampling-based approaches that only work for
a very large number of processes (e.g., Algorand). Neither of these lines of
work is well-suited for moderate-scale distributed ledgers ranging from a few
hundred to a thousand processes, which are common in production (e.g, Redbelly,
Sui). The goal of this work is to design a distributed ledger with sub-linear
communication complexity per process, sub-quadratic total communication
complexity, and low latency for finalizing a block into the ledger, such that
it can be used for moderate-scale systems. We propose QScale, a protocol in
which every process incurs only $\widetilde{O}(\kappa \sqrt{n})$ communication
complexity per-block in expectation, $\widetilde{O}(n\kappa)$ total
communication complexity per-block in expectation, and a best-case latency of
$O(\kappa)$ rounds while ensuring safety and liveness with overwhelming
probability, with $\kappa$ being a small security parameter.

</details>


### [25] [Programming RISC-V accelerators via Fortran](https://arxiv.org/abs/2510.02170)
*Nick Brown,Jake Davies,Felix LeClair*

Main category: cs.DC

TL;DR: 本文提出了一种方法，使复杂的Fortran科学计算代码无需重写即可在提供定制编程模型的RISC-V加速器上运行。


<details>
  <summary>Details</summary>
Motivation: RISC-V加速器虽有潜力用于高性能计算，但其定制化编程模型要求重写现有复杂Fortran代码，这在科学计算中不现实。

Method: 提出一种无需重写代码即可通过Fortran驱动RISC-V加速器架构的方法。

Result: 实现了对RISC-V加速器的Fortran语言支持，避免了代码重构。

Conclusion: 该方法有效解决了在科学计算中利用RISC-V加速器时面临的编程模型不兼容问题，提升了现有Fortran代码的可移植性。

Abstract: A range of RISC-V based accelerators are available and coming to market, and
there is strong potential for these to be used for High Performance Computing
(HPC) workloads. However, such accelerators tend to provide bespoke programming
models and APIs that require codes to be rewritten. In scientific computing,
where many of the simulation code are highly complex, extensive, and written in
Fortran, this is not realistic. In this extended abstract we present an
approach that enables driving such architectures via Fortran, avoiding code
redevelopment.

</details>
