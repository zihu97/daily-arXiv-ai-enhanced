<div id=toc></div>

# Table of Contents

- [cs.DC](#cs.DC) [Total: 9]
- [cs.SE](#cs.SE) [Total: 10]
- [cs.NI](#cs.NI) [Total: 2]
- [cs.MA](#cs.MA) [Total: 4]
- [cs.PF](#cs.PF) [Total: 1]


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [1] [An MLIR pipeline for offloading Fortran to FPGAs via OpenMP](https://arxiv.org/abs/2511.08713)
*Gabriel Rodriguez-Canal,David Katz,Nick Brown*

Main category: cs.DC

TL;DR: 本文首次在MLIR中通过OpenMP target指令实现了面向FPGA的选择性代码卸载，结合MLIR OpenMP方言与HLS方言，构建了可移植的FPGA编译流程，并支持任意MLIR兼容前端（如Flang），展示了MLIR生态系统的可组合性优势。


<details>
  <summary>Details</summary>
Motivation: 随着摩尔定律放缓，FPGA等异构计算平台在加速高性能计算（HPC）工作负载方面受到越来越多关注。现有基于OpenMP的FPGA方案依赖定制编译器，缺乏通用性和可扩展性，因此需要一种更灵活、可集成于主流编译基础设施的解决方案。

Method: 作者在MLIR框架中结合OpenMP方言与高级综合（HLS）方言，利用MLIR已有的构建模块，实现了一个支持OpenMP target指令的FPGA代码卸载编译流程，并通过Flang前端进行验证。该方法支持通过标准OpenMP指令对卸载内核进行手动优化。

Result: 成功实现了首个基于MLIR的OpenMP target指令FPGA卸载方案，显著减少了开发工作量，验证了MLIR生态系统在异构编译中的可组合性和扩展性，并为基于指令的FPGA加速提供了灵活路径。

Conclusion: 该工作展示了将OpenMP FPGA加速无缝集成到MLIR生态中的可行性，不仅提升了可移植性和前端兼容性，还为未来扩展和优化奠定了基础。

Abstract: With the slowing of Moore's Law, heterogeneous computing platforms such as Field Programmable Gate Arrays (FPGAs) have gained increasing interest for accelerating HPC workloads. In this work we present, to the best of our knowledge, the first implementation of selective code offloading to FPGAs via the OpenMP target directive within MLIR. Our approach combines the MLIR OpenMP dialect with a High-Level Synthesis (HLS) dialect to provide a portable compilation flow targeting FPGAs. Unlike prior OpenMP FPGA efforts that rely on custom compilers, by contrast we integrate with MLIR and so support any MLIR-compatible front end, demonstrated here with Flang. Building upon a range of existing MLIR building blocks significantly reduces the effort required and demonstrates the composability benefits of the MLIR ecosystem. Our approach supports manual optimisation of offloaded kernels through standard OpenMP directives, and this work establishes a flexible and extensible path for directive-based FPGA acceleration integrated within the MLIR ecosystem.

</details>


### [2] [Distribution and Management of Datacenter Load Decoupling](https://arxiv.org/abs/2511.08936)
*Liuzixuan Lin,Andrew A. Chien*

Main category: cs.DC

TL;DR: 本文研究通过数据中心与电网协同实现负载解耦，以降低碳排放并提升可再生能源利用率，在减少70%解耦资源需求下仍可实现98%的潜在碳减排效益。


<details>
  <summary>Details</summary>
Motivation: 人工智能和云计算数据中心能耗激增，加剧了其碳足迹问题，尤其因其持续用电需求与可再生能源发电波动性之间存在矛盾，亟需通过负载灵活性提升电网对可再生能源的消纳能力。

Method: 提出通过能源资源整合实现数据中心功率容量与电网负载的解耦，并优化解耦资源的分布与管理策略，考虑站点差异及数据中心-电网协作机制。

Result: 优化后的解耦资源配置可在仅使用70%资源的情况下实现超过98%的潜在碳减排；双向协作机制相较单向信息共享可提升1.4倍碳减排效果；多数数据中心在经济上具备可行性，但站点间收益不均可能需电网干预。

Conclusion: 数据中心通过合理设计的负载解耦策略，可在显著降低碳排放的同时兼顾经济效益，但需电网协调以应对站点间的不平衡问题。

Abstract: The exploding power consumption of AI and cloud datacenters (DCs) intensifies the long-standing concerns about their carbon footprint, especially because DCs' need for constant power clashes with volatile renewable generation needed for grid decarbonization. DC flexibility (a.k.a. load adaptation) is a key to reducing DC carbon emissions by improving grid renewable absorption.
  DC flexibility can be created, without disturbing datacenter capacity by decoupling a datacenter's power capacity and grid load with a collection of energy resources. Because decoupling can be costly, we study how to best distribute and manage decoupling to maximize benefits for all. Key considerations include site variation and datacenter-grid cooperation.
  We first define and compute the power and energy needs of datacenter load decoupling, and then we evaluate designed distribution and management approaches. Evaluation shows that optimized distribution can deliver >98% of the potential grid carbon reduction with 70% of the total decoupling need. For management, DC-grid cooperation (2-way sharing and control vs. 1-way info sharing) enables 1.4x grid carbon reduction. Finally, we show that decoupling may be economically viable, as on average datacenters can get power cost and carbon emissions benefits greater than their local costs of decoupling. However, skew across sites suggests grid intervention may be required.

</details>


### [3] [Evaluating HPC-Style CPU Performance and Cost in Virtualized Cloud Infrastructures](https://arxiv.org/abs/2511.08948)
*Jay Tharwani,Shobhit Aggarwal,Arnab A Purkayastha*

Main category: cs.DC

TL;DR: 该论文评估了四大云服务商（AWS、Azure、GCP、OCI）在不同CPU架构（Intel、AMD、ARM）下的OpenMP工作负载性能与成本，发现AWS性能最优但价格高，OCI最经济但较慢，GCP在AMD上表现提升显著，而其ARM实例性能差且昂贵。


<details>
  <summary>Details</summary>
Motivation: 为帮助用户在虚拟化云环境中根据性能或成本需求选择合适的CPU实例类型和云服务提供商。

Method: 使用SPEC ACCEL套件中的OpenMP工作负载子集，在AWS、Azure、GCP和OCI的通用型实例上测试Intel、AMD和ARM架构的运行时间和价格，涵盖按需和一年折扣定价模式。

Result: AWS在所有实例类型中运行时间最短但收费最高；OCI成本最低但性能较慢；Azure表现居中；GCP从Intel切换到AMD有明显性能提升，但其ARM实例比AMD慢两倍以上且更贵；AWS内部比较显示其ARM实例比Intel和AMD快最多49%。

Conclusion: 实例类型和云服务商的选择对运行时间和成本影响显著，用户应根据工作负载优先级（速度或成本）做出决策。

Abstract: This paper evaluates HPC-style CPU performance and cost in virtualized cloud infrastructures using a subset of OpenMP workloads in the SPEC ACCEL suite. Four major cloud providers by market share AWS, Azure, Google Cloud Platform (GCP), and Oracle Cloud Infrastructure (OCI) are compared across Intel, AMD, and ARM general purpose instance types under both on-demand and one-year discounted pricing. AWS consistently delivers the shortest runtime in all three instance types, yet charges a premium, especially for on-demand usage. OCI emerges as the most economical option across all CPU families, although it generally runs workloads more slowly than AWS. Azure often exhibits mid-range performance and cost, while GCP presents a mixed profile: it sees a notable boost when moving from Intel to AMD. On the other hand, its ARM instance is more than twice as slow as its own AMD offering and remains significantly more expensive. AWS's internal comparisons reveal that its ARM instance can outperform its Intel and AMD siblings by up to 49 percent in runtime. These findings highlight how instance choices and provider selection can yield substantial variations in both runtime and price, indicating that workload priorities, whether raw speed or cost minimization, should guide decisions on instance types.

</details>


### [4] [Experiences Building Enterprise-Level Privacy-Preserving Federated Learning to Power AI for Science](https://arxiv.org/abs/2511.08998)
*Zilinghan Li,Aditya Sinha,Yijiang Li,Kyle Chard,Kibaek Kim,Ravi Madduri*

Main category: cs.DC

TL;DR: 本文提出了一个面向企业的隐私保护联邦学习框架愿景，旨在无缝扩展跨异构计算环境的部署，并弥合研究原型与实际应用之间的差距。


<details>
  <summary>Details</summary>
Motivation: 在科学领域中，数据隐私、所有权和合规性限制使得集中式数据共享不可行，因此需要一种既用户友好又可扩展且隐私保护的企业级联邦学习框架。

Method: 基于开发Advanced Privacy-Preserving Federated Learning（APPFL）框架的经验，提出企业级联邦学习框架应具备的关键能力，并讨论实现这些目标的架构设计。

Result: 识别出五大关键能力：可扩展的本地仿真与原型设计、从仿真到部署的无缝过渡、跨异构基础设施的分布式部署、兼顾易用性与研究灵活性的多层抽象、以及通过差分隐私、安全聚合、强认证和机密计算等技术实现全面隐私与安全保障。

Conclusion: 该框架旨在连接研究原型与企业级部署，推动科学领域中可扩展、可靠且隐私保护的人工智能应用。

Abstract: Federated learning (FL) is a promising approach to enabling collaborative model training without centralized data sharing, a crucial requirement in scientific domains where data privacy, ownership, and compliance constraints are critical. However, building user-friendly enterprise-level FL frameworks that are both scalable and privacy-preserving remains challenging, especially when bridging the gap between local prototyping and distributed deployment across heterogeneous client computing infrastructures. In this paper, based on our experiences building the Advanced Privacy-Preserving Federated Learning (APPFL) framework, we present our vision for an enterprise-grade, privacy-preserving FL framework designed to scale seamlessly across computing environments. We identify several key capabilities that such a framework must provide: (1) Scalable local simulation and prototyping to accelerate experimentation and algorithm design; (2) seamless transition from simulation to deployment; (3) distributed deployment across diverse, real-world infrastructures, from personal devices to cloud clusters and HPC systems; (4) multi-level abstractions that balance ease of use and research flexibility; and (5) comprehensive privacy and security through techniques such as differential privacy, secure aggregation, robust authentication, and confidential computing. We further discuss architectural designs to realize these goals. This framework aims to bridge the gap between research prototypes and enterprise-scale deployment, enabling scalable, reliable, and privacy-preserving AI for science.

</details>


### [5] [Flex-MIG: Enabling Distributed Execution on MIG](https://arxiv.org/abs/2511.09143)
*Myungsu Kim,Ikjun Yeom,Younghoon Kim*

Main category: cs.DC

TL;DR: Flex-MIG 是一种纯软件框架，通过将 NVIDIA MIG 的一对一分配模型改为一对多模型，并支持跨 MIG 实例的主机共享内存集合通信，在无需硬件修改的情况下减少碎片、消除重配置时的排空需求，并最多提升 17% 的作业完成效率。


<details>
  <summary>Details</summary>
Motivation: 在多租户 GPU 集群中，NVIDIA MIG 虽提供硬件级隔离，但其硬件刚性和传统的一对一分配模型导致严重的资源碎片和集群利用率低下。

Method: 提出 Flex-MIG 框架，采用软件协调方式实现一对多的 MIG 分配模型，并支持跨 MIG 实例的主机共享内存集体通信，无需硬件改动。

Result: Flex-MIG 消除了重配置所需的排空操作，显著减少资源碎片，在多种负载轨迹下最多将作业完成时间（makespan）缩短 17%。

Conclusion: 通过将 MIG 的运行模型重新设计为由软件协调的资源层，可大幅提升 GPU 集群的整体资源利用效率。

Abstract: GPU clusters in multi-tenant settings often suffer from underutilization, making GPU-sharing technologies essential for efficient resource use. Among them, NVIDIA Multi-Instance GPU (MIG) has gained traction for providing hardware-level isolation that enables concurrent workloads without interference. However, MIG's hardware rigidity and the conventional one-to-one allocation model jointly lead to severe fragmentation and cluster-wide underutilization. We present Flex-MIG, a software-only framework that replaces one-to-one with a one-to-many allocation model and enables host-shared-memory collectives across MIG instances without hardware modification. Flex-MIG eliminates drain-required reconfiguration, reduces fragmentation, and improves makespan by up to 17% across diverse traces, showing that rethinking MIG's operational model as a software-coordinated layer substantially improves cluster efficiency.

</details>


### [6] [Minimize Your Critical Path with Combine-and-Exchange Locks](https://arxiv.org/abs/2511.09194)
*Simon König,Lukas Epple,Christian Becker*

Main category: cs.DC

TL;DR: 本文提出了一种名为Combine-and-Exchange Scheduling（CES）的新型用户态调度方法，用于优化协程等完全在用户空间调度的任务的同步机制，在多个语言和库中实现了最高8倍的微基准性能提升和3倍的应用基准性能提升。


<details>
  <summary>Details</summary>
Motivation: 当前用户态同步原语沿用内核级调度的思路，导致关键路径上出现不必要的延迟，限制了吞吐量；因此需要重新思考完全在用户空间调度任务（如协程、纤程）的同步机制。

Method: 提出Combine-and-Exchange Scheduling（CES）方法，使竞争的关键区保持在同一执行线程上，同时将可并行的工作均匀分配到其余线程。

Result: 该方法可应用于多种现有语言和库，在应用基准测试中实现3倍性能提升，在微基准测试中实现高达8倍的性能提升。

Conclusion: 针对完全在用户空间调度的任务，CES提供了一种更高效的同步与调度策略，显著提升了系统吞吐性能。

Abstract: Coroutines are experiencing a renaissance as many modern programming languages support the use of cooperative multitasking for highly parallel or asynchronous applications. One of the greatest advantages of this is that concurrency and synchronization is manged entirely in the userspace, omitting heavy-weight system calls. However, we find that state-of-the-art userspace synchronization primitives approach synchronization in the userspace from the perspective of kernel-level scheduling. This introduces unnecessary delays on the critical path of the application, limiting throughput. In this paper, we re-think synchronization for tasks that are scheduled entirely in the userspace (e.g., coroutines, fibers, etc.). We develop Combine-and-Exchange Scheduling (CES), a novel scheduling approach that ensures contended critical sections stay on the same thread of execution while parallelizable work is evenly spread across the remaining threads. We show that our approach can be applied to many existing languages and libraries, resulting in 3-fold performance improvements in application benchmarks as well as 8-fold performance improvements in microbenchmarks.

</details>


### [7] [No Cords Attached: Coordination-Free Concurrent Lock-Free Queues](https://arxiv.org/abs/2511.09410)
*Yusuf Motiwala*

Main category: cs.DC

TL;DR: 本文提出了一种名为循环内存保护（CMP）的无锁队列，通过有界保护窗口在保持严格FIFO语义、无界容量和无锁进展的同时，显著简化了并发队列的设计，并在高并发场景下性能优于现有先进方案。


<details>
  <summary>Details</summary>
Motivation: 现有无锁队列为解决ABA、释放后使用和不安全回收等问题引入了复杂的协调机制，牺牲了FIFO顺序、无界容量或无锁进展，且过度保护反而降低系统韧性；尤其在AI时代高并发场景下，协调开销远超队列基本操作成本。

Method: 提出循环内存保护（CMP）机制，采用有界保护窗口实现实际可行的内存回收保障，在无需协调的前提下维持严格FIFO语义、无界容量与无锁进展；并通过线性化和有界回收分析证明其正确性。

Result: 实验表明，CMP在高竞争环境下相比当前最先进的无锁队列性能提升1.72至4倍，并能良好扩展至数百个线程。

Conclusion: 高度并发的队列可以在不削弱语义的前提下回归其本质的简洁性，CMP验证了这一可能性。

Abstract: The queue is conceptually one of the simplest data structures-a basic FIFO container. However, ensuring correctness in the presence of concurrency makes existing lock-free implementations significantly more complex than their original form. Coordination mechanisms introduced to prevent hazards such as ABA, use-after-free, and unsafe reclamation often dominate the design, overshadowing the queue itself. Many schemes compromise strict FIFO ordering, unbounded capacity, or lock-free progress to mask coordination overheads. Yet the true source of complexity lies in the pursuit of infinite protection against reclamation hazards--theoretically sound but impractical and costly. This pursuit not only drives unnecessary complexity but also creates a protection paradox where excessive protection reduces system resilience rather than improving it. While such costs may be tolerable in conventional workloads, the AI era has shifted the paradigm: training and inference pipelines involve hundreds to thousands of concurrent threads per node, and at this scale, protection and coordination overheads dominate, often far heavier than the basic queue operations themselves.
  This paper introduces Cyclic Memory Protection (CMP), a coordination-free queue that preserves strict FIFO semantics, unbounded capacity, and lock-free progress while restoring simplicity. CMP reclaims the strict FIFO that other approaches sacrificed through bounded protection windows that provide practical reclamation guarantees. We prove strict FIFO and safety via linearizability and bounded reclamation analysis, and show experimentally that CMP outperforms state-of-the-art lock-free queues by up to 1.72-4x under high contention while maintaining scalability to hundreds of threads. Our work demonstrates that highly concurrent queues can return to their fundamental simplicity without weakening queue semantics.

</details>


### [8] [Formal Verification of a Generic Algorithm for TDM Communication Over Inter Satellite Links](https://arxiv.org/abs/2511.09485)
*Miroslav Popovic,Marko Popovic,Pavle Vasiljevic,Miodrag Djukic*

Main category: cs.DC

TL;DR: 本文使用CSP过程代数对Python联邦学习测试平台中的通用TDM通信算法进行形式化验证，通过建模和模型检测证明其无死锁性和成功终止性。


<details>
  <summary>Details</summary>
Motivation: 已有工作已对集中式和去中心化联邦学习算法进行了形式化验证，本文旨在对同一框架中的第三种通用算法——时隙内的通用TDM通信机制——进行类似的形式化验证，以确保其正确性。

Method: 采用CSP（通信顺序进程）过程代数方法：首先构建忠实于实际Python代码的CSP模型，然后利用模型检测工具PAT自动验证该算法的无死锁性（安全性）和成功终止性（活性）。

Result: 成功构建了TDM通信算法的CSP模型，并通过PAT工具自动证明了该算法满足无死锁和成功终止的性质。

Conclusion: 通用TDM通信算法在所提出的联邦学习测试平台中是形式化正确的，增强了该框架在边缘系统中部署的可靠性。

Abstract: The Python Testbed for Federated Learning Algorithms is a simple FL framework targeting edge systems, which provides the three generic algorithms: the centralized federated learning, the decentralized federated learning, and the universal TDM communication in the current time slot. The first two were formally verified in a previous paper using the CSP process algebra, and in this paper, we use the same approach to formally verify the third one, in two phases. In the first phase, we construct the CSP model as a faithful representation of the real Python code. In the second phase, the model checker PAT automatically proves correctness of the third generic algorithm by proving its deadlock freeness (safety property) and successful termination (liveness property).

</details>


### [9] [LLM Inference Beyond a Single Node: From Bottlenecks to Mitigations with Fast All-Reduce Communication](https://arxiv.org/abs/2511.09557)
*Prajwal Singhania,Siddharth Singh,Lannie Dalton Hough,Akarsh Srivastava,Harshitha Menon,Charles Fredrick Jekel,Abhinav Bhatele*

Main category: cs.DC

TL;DR: 本文研究了在GPU超级计算机上进行大语言模型（LLM）多节点分布式推理的性能，提出了一种基于NVSHMEM的分层全归约算法NVRAR，显著降低了通信延迟和端到端批处理延迟。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型规模不断增大，跨多节点的高效分布式推理变得至关重要，而现有模型并行策略在多节点扩展时面临通信瓶颈，尤其是all-reduce操作成为性能限制因素。

Method: 作者通过多种先进推理引擎及自研原型引擎YALIS进行实验，分析不同模型并行方案的强扩展性，并设计了基于递归加倍和NVSHMEM的分层all-reduce算法NVRAR以优化通信效率。

Result: NVRAR在HPE Slingshot和InfiniBand互连上对128 KB至2 MB消息大小的all-reduce操作实现了比NCCL低1.9x–3.6x的延迟；集成到YALIS后，在Llama 3.1 405B模型的多节点解码密集型任务中，端到端批处理延迟最多降低1.72倍。

Conclusion: 通过优化多节点通信中的关键瓶颈，特别是all-reduce操作，NVRAR显著提升了大语言模型在多节点环境下的推理性能，为未来超大规模模型的高效部署提供了有效方案。

Abstract: As large language models (LLMs) continue to grow in size, distributed inference has become increasingly important. Model-parallel strategies must now efficiently scale not only across multiple GPUs but also across multiple nodes. In this work, we present a detailed performance study of multi-node distributed inference using LLMs on GPU-based supercomputers. We conduct experiments with several state-of-the-art inference engines alongside YALIS, a research-oriented prototype engine designed for controlled experimentation. We analyze the strong-scaling behavior of different model-parallel schemes and identify key bottlenecks. Since all-reduce operations are a common performance bottleneck, we develop NVRAR, a hierarchical all-reduce algorithm based on recursive doubling with NVSHMEM. NVRAR achieves up to 1.9x-3.6x lower latency than NCCL for message sizes between 128 KB and 2 MB on HPE Slingshot and InfiniBand interconnects. Integrated into YALIS, NVRAR achieves up to a 1.72x reduction in end-to-end batch latency for the Llama 3.1 405B model in multi-node decode-heavy workloads using tensor parallelism.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [10] [Triage in Software Engineering: A Systematic Review of Research and Practice](https://arxiv.org/abs/2511.08607)
*Yongxin Zhao,Shenglin Zhang,Yujia Wu,Yuxin Sun,Yongqian Sun,Dan Pei,Chetan Bansal,Minghua Ma*

Main category: cs.SE

TL;DR: 本文综述了2004年至今的234篇关于软件系统中问题分诊（triage）的研究论文，系统梳理了其基本概念、架构与挑战，对比学术与工业界目标差异，总结常用数据集与评估指标，并指出未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 随着现代软件系统日益复杂，高效的问题分诊对保障系统可靠性、可维护性及快速响应至关重要；尽管过去二十年在自动化分诊方面取得显著进展，但实际部署仍面临诸多障碍。

Method: 对2004年至今的234篇相关论文进行系统性综述，分析学术与工业研究目标差异，考察工业实践中的实证研究，归纳常用开源数据集和评估指标。

Result: 识别出限制分诊系统实际应用的主要障碍，提供了统一的评估视角，并整理了相关论文与项目资源。

Conclusion: 为促进学术创新与工业应用的深度融合，需进一步关注实际部署挑战，并探索新兴研究方向。

Abstract: As modern software systems continue to grow in complexity, triage has become a fundamental process in system operations and maintenance. Triage aims to efficiently prioritize, assign, and assess issues to ensure the reliability of complex environments. The vast amount of heterogeneous data generated by software systems has made effective triage indispensable for maintaining reliability, facilitating maintainability, and enabling rapid issue response. Motivated by these challenges, researchers have devoted extensive effort to advancing triage automation and have achieved significant progress over the past two decades. This survey provides a comprehensive review of 234 papers from 2004 to the present, offering an in-depth examination of the fundamental concepts, system architecture, and problem statement. By comparing the distinct goals of academic and industrial research and by analyzing empirical studies of industrial practices, we identify the major obstacles that limit the practical deployment of triage systems. To assist practitioners in method selection and performance evaluation, we summarize widely adopted open-source datasets and evaluation metrics, providing a unified perspective on the measurement of triage effectiveness. Finally, we outline potential future directions and emerging opportunities to foster a closer integration between academic innovation and industrial application. All reviewed papers and projects are available at https://github.com/AIOps-Lab-NKU/TriageSurvey.

</details>


### [11] [Energy Consumption of Dataframe Libraries for End-to-End Deep Learning Pipelines:A Comparative Analysis](https://arxiv.org/abs/2511.08644)
*Punit Kumar,Asif Imran,Tevfik Kosar*

Main category: cs.SE

TL;DR: 本文对比分析了Pandas、Polars和Dask三大Python数据处理库在完整深度学习训练与推理流程中的性能表现，特别关注其与GPU密集型任务（如数据加载、预处理和批处理）的交互。


<details>
  <summary>Details</summary>
Motivation: 现有文献缺乏对数据处理库在深度学习全流程中与GPU负载交互影响的系统研究，本文旨在填补这一空白。

Method: 作者在多种机器学习模型和数据集上，测量了Pandas、Polars和Dask在运行时间、内存占用、磁盘使用以及CPU/GPU能耗等关键指标上的表现。

Result: 实验量化了三种库在深度学习管道不同阶段的性能差异，揭示了它们在资源消耗和效率方面的优劣。

Conclusion: 选择合适的数据处理库对深度学习整体性能有显著影响，Polars和Dask在某些场景下优于传统Pandas，尤其在处理大规模数据与GPU协同工作时。

Abstract: This paper presents a detailed comparative analysis of the performance of three major Python data manipulation libraries - Pandas, Polars, and Dask - specifically when embedded within complete deep learning (DL) training and inference pipelines. The research bridges a gap in existing literature by studying how these libraries interact with substantial GPU workloads during critical phases like data loading, preprocessing, and batch feeding. The authors measured key performance indicators including runtime, memory usage, disk usage, and energy consumption (both CPU and GPU) across various machine learning models and datasets.

</details>


### [12] [An insight into the technical debt-fix trade off in software backporting](https://arxiv.org/abs/2511.09000)
*Jarin Tasnim,Debasish Chakroborti,Chanchal K. Roy,Kevin A. Schneider*

Main category: cs.SE

TL;DR: 该研究分析了在Apache、Eclipse和Python三大软件生态系统中，向旧版本回移植（backporting）代码时引入技术债务的情况。研究发现约4.3%的回移植会引入新的技术债务，其中Apache绝对数量最多，而Python和Eclipse的技术债务密度更高；不同生态系统的债务高发阶段及开发者因素也存在显著差异。


<details>
  <summary>Details</summary>
Motivation: 回移植是维护稳定软件版本的重要手段，但在此过程中可能引入新的技术债务。目前缺乏对回移植引发技术债务的系统性理解，因此本文旨在揭示其发生时机、原因及影响因素。

Method: 作者分析了来自87个仓库、涵盖Apache、Eclipse和Python三大生态系统的31,076个回移植源中的105,396次提交，通过识别这些提交是否引入新的技术债务，结合时间阶段、开发者角色与工作负载等因素进行统计分析。

Result: 约4.3%的回移植引入了新的技术债务；Apache的绝对数量最多，但Python和Eclipse的债务/提交比约为Apache的三倍；Apache早期因功能迁移易产生债务，而Python和Eclipse则在发布周期中期更易积累债务；经验不足、高负荷或非项目所有者的开发者更可能引入技术债务。

Conclusion: 回移植虽有助于维护旧版本，但存在一定风险，可能引入技术债务。不同生态系统在债务模式上表现不同，且开发者特征显著影响债务引入概率，建议在回移植实践中加强代码审查与开发者支持。

Abstract: Maintaining software is an ongoing process that stretches beyond the initial release. Stable software versions continuously evolve to fix bugs, add improvements, address security issues, and ensure compatibility. This ongoing support involves Backporting, which means taking a fix or update from a newer version and applying it to an older version of the same software. As software versions evolve, new technical debt can arise during backport maintenance activities. This study examines the technical debt involved in fixing 105,396 commits from 31,076 backport sources across 87 repositories in three software ecosystems (Apache, Eclipse, and Python). The goal is to identify when and why new technical debt arises during backporting in stable source code. Our results indicate that approximately 4.3% of backports introduce new technical debt. Apache contributes the most absolute instances, while Python and Eclipse exhibit nearly three times higher debt-to-commit ratios than Apache. Feature migrations make older Apache releases debt-prone in the early phase, whereas Python and Eclipse releases tend to accumulate technical debt mostly during the middle phase of their release cycles. Additionally, developers who are inexperienced, under high workloads, or non-owners are more likely to introduce technical debt during backporting.

</details>


### [13] [Test Plan Generation for Live Testing of Cloud Services](https://arxiv.org/abs/2511.09038)
*Oussama Jebbar,Ferhat Khendek,Maria Toeroe*

Main category: cs.SE

TL;DR: 本文提出了一种自动生成测试计划的方法，以减少在生产环境中进行实时测试时对正常服务流量的干扰。


<details>
  <summary>Details</summary>
Motivation: 在生产环境中进行实时测试容易干扰正常业务流量，而手动设计测试计划繁琐且易出错，尤其在大型复杂系统中更为困难，因此需要自动化方法来生成测试计划。

Method: 提出一种自动化生成测试计划的方法，涵盖测试配置选择/生成、部署规划、测试运行调度以及干扰风险缓解策略等内容。

Result: 通过一个案例研究展示了该方法的可行性和有效性，并讨论了其多个方面。

Conclusion: 所提出的自动化测试计划生成方法有助于降低生产环境中测试活动引发的服务中断风险，提升测试效率与可靠性。

Abstract: Live testing is performed in the production environment ideally without causing unacceptable disturbance to the production traffic. Thus, test activities have to be orchestrated properly to avoid interferences with the production traffic. A test plan is the road map that specifies how the test activities need to be orchestrated. Developing a test plan includes tasks such as test configuration selection/generation, test configuration deployment planning, creating the test runs schedule, choosing strategies to mitigate the risk of interferences, etc. The manual design of a test plan is tedious and error prone. This task becomes harder especially when the systems are large and complex. In this paper we propose an approach for automating test plans generation. With this approach we aim at reducing service disruption that may be induced by the testing activities in production. We illustrate our approach with a case study and discuss its different aspects.

</details>


### [14] [Vendor-Aware Industrial Agents: RAG-Enhanced LLMs for Secure On-Premise PLC Code Generation](https://arxiv.org/abs/2511.09122)
*Joschka Kersting,Michael Rummel,Gesa Benndorf*

Main category: cs.SE

TL;DR: 本文提出了一种适用于工业场景的低数据领域编码助手解决方案，通过无需微调大模型、仅微调小型本地模型，并结合检索增强生成（RAG）、提示工程与自动代码验证，在边缘设备上实现了高质量PLC代码生成。


<details>
  <summary>Details</summary>
Motivation: 由于可编程逻辑控制器（PLC）使用专有代码方言，且企业不信任云服务提供商，现有大型语言模型虽能生成IEC 61131-3兼容代码，但缺乏对特定功能块和项目代码的理解，因此需要一种本地化、低数据依赖的编码助手。

Method: 采用小型本地模型进行微调，结合检索增强生成（RAG）、精心设计的提示工程、多模型竞争机制、自动错误修正以及在聊天界面中直接编译验证代码的方法。

Result: 实验评估表明，该方法在低数据条件下仍能生成高质量代码，用户评分和代码编译统计结果均支持其有效性。

Conclusion: 通过RAG、定向检索和高级提示工程，可以在不依赖大规模微调的情况下构建适用于工业边缘设备的高效PLC编码助手。

Abstract: Programmable Logic Controllers are operated by proprietary code dialects; this makes it challenging to train coding assistants. Current LLMs are trained on large code datasets and are capable of writing IEC 61131-3 compatible code out of the box, but they neither know specific function blocks, nor related project code. Moreover, companies like Mitsubishi Electric and their customers do not trust cloud providers. Hence, an own coding agent is the desired solution to cope with this. In this study, we present our work on a low-data domain coding assistant solution for industrial use. We show how we achieved high quality code generation without fine-tuning large models and by fine-tuning small local models for edge device usage. Our tool lets several AI models compete with each other, uses reasoning, corrects bugs automatically and checks code validity by compiling it directly in the chat interface. We support our approach with an extensive evaluation that comes with code compilation statistics and user ratings. We found that a Retrieval-Augmented Generation (RAG) supported coding assistant can work in low-data domains by using extensive prompt engineering and directed retrieval.

</details>


### [15] [Leveraging Self-Paced Learning for Software Vulnerability Detection](https://arxiv.org/abs/2511.09212)
*Zeru Cheng,Yanjing Yang,He Zhang,Lanxin Yang,Jinghao Hu,Jinwei Xu,Bohan Liu,Haifeng Shen*

Main category: cs.SE

TL;DR: 本文提出了一种名为SPLVD（Self-Paced Learning for Software Vulnerability Detection）的新方法，通过模拟人类由易到难的学习过程，动态选择训练用的源代码，从而提升漏洞检测模型的准确率。在多个基准数据集和OpenHarmony项目上的实验表明，SPLVD在F1分数和精确率上均优于现有最先进方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于深度学习的软件漏洞检测方法在实践中准确率有限，主要原因是训练数据（源代码）质量较低。为解决这一问题，作者受人类渐进式学习启发，提出一种能动态筛选高质量、难度适配训练样本的方法。

Method: SPLVD采用自定步调学习策略，在每个训练轮次前利用专门设计的数据选择器重新评估源代码的难度，并优先选择“较易”的样本进行训练，同时不断更新数据选择器，实现从易到难的渐进式学习。

Result: 在包含超过239K源代码（其中25K个含漏洞）的三个基准数据集上，SPLVD分别取得了89.2%、68.7%和43.5%的最高F1分数；在未被通用大模型学习过的OpenHarmony项目中，其精确率达到90.9%，显著优于现有方法。

Conclusion: SPLVD通过引入自定步调学习机制有效提升了软件漏洞检测的性能，尤其在真实且新颖的代码生态中展现出良好的实用性和泛化能力。

Abstract: Software vulnerabilities are major risks to software systems. Recently, researchers have proposed many deep learning approaches to detect software vulnerabilities. However, their accuracy is limited in practice. One of the main causes is low-quality training data (i.e., source code). To this end, we propose a new approach: SPLVD (Self-Paced Learning for Software Vulnerability Detection). SPLVD dynamically selects source code for model training based on the stage of training, which simulates the human learning process progressing from easy to hard. SPLVD has a data selector that is specifically designed for the vulnerability detection task, which enables it to prioritize the learning of easy source code. Before each training epoch, SPLVD uses the data selector to recalculate the difficulty of the source code, select new training source code, and update the data selector. When evaluating SPLVD, we first use three benchmark datasets with over 239K source code in which 25K are vulnerable for standard evaluations. Experimental results demonstrate that SPLVD achieves the highest F1 of 89.2%, 68.7%, and 43.5%, respectively, outperforming the state-of-the-art approaches. Then we collect projects from OpenHarmony, a new ecosystem that has not been learned by general LLMs, to evaluate SPLVD further. SPLVD achieves the highest precision of 90.9%, demonstrating its practical effectiveness.

</details>


### [16] [AILINKPREVIEWER: Enhancing Code Reviews with LLM-Powered Link Previews](https://arxiv.org/abs/2511.09223)
*Panya Trakoolgerntong,Tao Xiao,Masanari Kondo,Chaiyong Ragkhitwetsagul,Morakot Choetkiertikul,Pattaraporn Sangaroonsilp,Yasutaka Kamei*

Main category: cs.SE

TL;DR: 本文提出 AILINKPREVIEWER 工具，利用大语言模型（LLM）为 Pull Request 中的外部链接生成上下文感知的预览内容，以丰富代码审查信息。实验表明上下文摘要在自动指标上表现更优，但用户更偏好非上下文摘要，揭示了指标与可用性之间的权衡。


<details>
  <summary>Details</summary>
Motivation: 在软件工程中，Pull Request 常包含指向问题或外部资源的链接，但这些链接在自动化任务（如 PR 摘要和评论生成）中通常被忽略，导致审查者需频繁切换上下文，增加认知负担。

Method: 作者开发了 AILINKPREVIEWER 工具，结合 PR 元数据（标题、描述、评论）和链接内容，使用大语言模型生成三种类型的链接预览：上下文 LLM 摘要、非上下文 LLM 摘要和仅基于元数据的预览，并在 50 个 GitHub 仓库上进行评估。

Result: 自动评估指标（BLEU、BERTScore、压缩率）显示上下文摘要优于其他方法；但在包含 7 名参与者的用户研究中，多数人更偏好非上下文摘要。

Conclusion: LLM 驱动的链接预览有潜力提升代码审查效率并为开发者和自动化工具提供更丰富的上下文，但需在自动指标与用户体验之间取得平衡。

Abstract: Code review is a key practice in software engineering, where developers evaluate code changes to ensure quality and maintainability. Links to issues and external resources are often included in Pull Requests (PRs) to provide additional context, yet they are typically discarded in automated tasks such as PR summarization and code review comment generation. This limits the richness of information available to reviewers and increases cognitive load by forcing context-switching. To address this gap, we present AILINKPREVIEWER, a tool that leverages Large Language Models (LLMs) to generate previews of links in PRs using PR metadata, including titles, descriptions, comments, and link body content. We analyzed 50 engineered GitHub repositories and compared three approaches: Contextual LLM summaries, Non-Contextual LLM summaries, and Metadata-based previews. The results in metrics such as BLEU, BERTScore, and compression ratio show that contextual summaries consistently outperform other methods. However, in a user study with seven participants, most preferred non-contextual summaries, suggesting a trade-off between metric performance and perceived usability. These findings demonstrate the potential of LLM-powered link previews to enhance code review efficiency and to provide richer context for developers and automation in software engineering.
  The video demo is available at https://www.youtube.com/watch?v=h2qH4RtrB3E, and the tool and its source code can be found at https://github.com/c4rtune/AILinkPreviewer.

</details>


### [17] [Leveraging Large Language Models for Use Case Model Generation from Software Requirements](https://arxiv.org/abs/2511.09231)
*Tobias Eisenreich,Nicholas Friedlaender,Stefan Wagner*

Main category: cs.SE

TL;DR: 本文提出一种基于大语言模型（LLM）的方法，通过高级提示工程从软件需求中自动提取参与者和用例，显著提升用例建模效率，建模时间减少60%，同时保持模型质量，并为工程师提供过程指导。


<details>
  <summary>Details</summary>
Motivation: 手动创建用例模型耗时费力，实践中常被跳过，因此需要一种高效辅助方法来支持用例建模过程。

Method: 集成开源权重的大语言模型，结合高级提示工程技术，从软件需求文档中系统性地提取参与者和用例。

Result: 在五名专业软件工程师参与的探索性研究中，与传统手动建模相比，该方法将建模时间减少了60%，模型质量相当，并获得参与者对过程指导价值的积极反馈。

Conclusion: 大语言模型能有效辅助用例建模，在显著提升效率的同时保持模型质量，并为建模人员提供有益引导。

Abstract: Use case modeling employs user-centered scenarios to outline system requirements. These help to achieve consensus among relevant stakeholders. Because the manual creation of use case models is demanding and time-consuming, it is often skipped in practice. This study explores the potential of Large Language Models (LLMs) to assist in this tedious process. The proposed method integrates an open-weight LLM to systematically extract actors and use cases from software requirements with advanced prompt engineering techniques. The method is evaluated using an exploratory study conducted with five professional software engineers, which compares traditional manual modeling to the proposed LLM-based approach. The results show a substantial acceleration, reducing the modeling time by 60\%. At the same time, the model quality remains on par. Besides improving the modeling efficiency, the participants indicated that the method provided valuable guidance in the process.

</details>


### [18] [Decoding the Configuration of AI Coding Agents: Insights from Claude Code Projects](https://arxiv.org/abs/2511.09268)
*Helio Victor F. Santos,Vitor Costa,Joao Eduardo Montandon,Marco Tulio Valente*

Main category: cs.SE

TL;DR: 本文对Claude Code的328个公开配置文件进行了实证研究，揭示了智能体代码助手配置中涵盖的软件工程关注点及其共现模式，强调了在配置中明确定义架构规范的重要性。


<details>
  <summary>Details</summary>
Motivation: 尽管智能体代码助手能显著提升软件开发效率，但其行为和效果高度依赖于定义架构约束、编码实践和工具使用策略的配置文件。然而，目前对这些配置文件的结构和内容缺乏系统了解。

Method: 收集并分析了328个公开Claude Code项目的配置文件，识别其中指定的软件工程关注点与实践，并研究这些关注点在单个配置文件中的共现情况。

Result: 结果表明，智能体配置文件中需涵盖广泛的软件工程关注点与实践，尤其强调明确指定智能体应遵循的系统架构。

Conclusion: 智能体代码助手的有效性与其配置文件中对架构和其他工程实践的详细定义密切相关，未来应重视配置生态系统的标准化与优化。

Abstract: Agentic code assistants are a new generation of AI systems capable of performing end-to-end software engineering tasks. While these systems promise unprecedented productivity gains, their behavior and effectiveness depend heavily on configuration files that define architectural constraints, coding practices, and tool usage policies. However, little is known about the structure and content of these configuration artifacts. This paper presents an empirical study of the configuration ecosystem of Claude Code, one of the most widely used agentic coding systems. We collected and analyzed 328 configuration files from public Claude Code projects to identify (i) the software engineering concerns and practices they specify and (ii) how these concerns co-occur within individual files. The results highlight the importance of defining a wide range of concerns and practices in agent configuration files, with particular emphasis on specifying the architecture the agent should follow.

</details>


### [19] [Routesplain: Towards Faithful and Intervenable Routing for Software-related Tasks](https://arxiv.org/abs/2511.09373)
*Adam Štorek,Vikas Upadhyay,Marianne Menglin Liu,Daniel W. Peterson,Anshul Mittal,Sujeeth Bharadwaj,Fahad Shah,Dan Roth*

Main category: cs.SE

TL;DR: 本文提出了Routesplain，这是首个面向软件相关任务的LLM路由器，通过提取人类可解释的概念（如任务类型、领域、推理复杂度）进行路由决策，在准确性和成本上优于单个模型，并超越现有黑盒路由方法。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型（LLMs）在各类软件任务中表现不一，合理将用户查询路由至合适的LLM有助于提升响应质量并降低成本。然而现有研究多聚焦于通用黑盒路由方法，缺乏对软件任务特性的针对性与可解释性。

Method: Routesplain首先从用户查询中提取人类可解释的概念（如任务类别、领域知识、推理复杂度等），然后基于这些概念进行路由决策，从而提供透明且可信的路由依据。

Result: 在8类软件相关任务和16个前沿LLM上的实验表明，Routesplain在准确率和成本方面均优于单个模型，并达到或超过所有黑盒基线方法；同时，基于概念的干预分析为路由器的进一步优化提供了方向。

Conclusion: Routesplain通过引入可解释的概念驱动路由机制，在软件相关任务中实现了高效、透明且性能优越的LLM选择策略，为未来智能路由系统的设计提供了新思路。

Abstract: LLMs now tackle a wide range of software-related tasks, yet we show that their performance varies markedly both across and within these tasks. Routing user queries to the appropriate LLMs can therefore help improve response quality while reducing cost. Prior work, however, has focused mainly on general-purpose LLM routing via black-box models. We introduce Routesplain, the first LLM router for software-related tasks, including multilingual code generation and repair, input/output prediction, and computer science QA. Unlike existing routing approaches, Routesplain first extracts human-interpretable concepts from each query (e.g., task, domain, reasoning complexity) and only routes based on these concepts, thereby providing intelligible, faithful rationales. We evaluate Routesplain on 16 state-of-the-art LLMs across eight software-related tasks; Routesplain outperforms individual models both in terms of accuracy and cost, and equals or surpasses all black-box baselines, with concept-level intervention highlighting avenues for further router improvements.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [20] [Learning-based Radio Link Failure Prediction Based on Measurement Dataset in Railway Environments](https://arxiv.org/abs/2511.08851)
*Po-Heng Chou,Da-Chih Lin,Hung-Yu Wei,Walid Saad,Yu Tsao*

Main category: cs.NI

TL;DR: 本文提出了一种基于测量的框架，用于在5G非独立组网（NSA）铁路环境中提前预测无线链路失败（RLF）。通过使用10Hz的地铁列车轨迹数据及服务小区与邻区指标，对六种模型（CNN、LSTM、XGBoost、Anomaly Transformer、PatchTST和TimesNet）在不同观测窗口和预测时长下的性能进行了评估。结果表明，TimesNet在3秒观测窗口和3秒预测时长下F1得分最高，而CNN在2秒预测时长下具有良好的准确率与延迟权衡，适用于主动切换等控制策略。


<details>
  <summary>Details</summary>
Motivation: 5G NSA铁路通信系统中，无线链路失败（RLF）可能导致服务中断，影响运行安全与效率。现有方法难以在实际部署中实现低开销、高时效的早期预警。因此，亟需一种基于商用设备可获取的轻量级特征、能提前数秒预测RLF的实用框架。

Method: 利用10Hz采样的地铁列车实测轨迹数据，结合服务小区与邻区的信号指标，构建RLF预测任务；在多种观测窗口（如3秒）和预测时长（如2–3秒）设置下，系统评估了六种时序模型（CNN、LSTM、XGBoost、Anomaly Transformer、PatchTST、TimesNet）的性能，重点关注F1分数与推理延迟的平衡。

Result: 实验显示，在3秒观测窗口下，TimesNet在3秒预测时长下取得最高F1分数；CNN在2秒预测时长下展现出更优的准确率-延迟折衷，适合实时应用。所有模型均仅依赖商用终端可获取的轻量特征，验证了深度时序模型在铁路5G系统中实现RLF早期预警的可行性。

Conclusion: 深度时序模型能够利用轻量级、易获取的无线测量数据，在5G NSA铁路场景中提前数秒预测链路失败，为冗余机制和自适应切换等主动控制策略提供支持，具有良好的工程实用价值。

Abstract: In this paper, a measurement-driven framework is proposed for early radio link failure (RLF) prediction in 5G non-standalone (NSA) railway environments. Using 10 Hz metro-train traces with serving and neighbor-cell indicators, we benchmark six models, namely CNN, LSTM, XGBoost, Anomaly Transformer, PatchTST, and TimesNet, under varied observation windows and prediction horizons. When the observation window is three seconds, TimesNet attains the highest F1 score with a three-second prediction horizon, while CNN provides a favorable accuracy-latency tradeoff with a two-second horizon, enabling proactive actions such as redundancy and adaptive handovers. The results indicate that deep temporal models can anticipate reliability degradations several seconds in advance using lightweight features available on commercial devices, offering a practical path to early-warning control in 5G-based railway systems.

</details>


### [21] [Hierarchical Reinforcement Learning for Integrated Cloud-Fog-Edge Computing in IoT Systems](https://arxiv.org/abs/2511.09006)
*Ameneh Zarei,Mahmood Ahmadi,Farhad Mardukhi*

Main category: cs.NI

TL;DR: 本文提出了一种名为HIPA的分层物联网处理架构，通过机器学习在云、雾和边缘计算之间动态分配任务，以降低延迟、提升可扩展性并保障数据隐私。


<details>
  <summary>Details</summary>
Motivation: 传统云计算架构难以应对物联网应用产生的海量数据和实时性需求，因此需要更高效的计算范式来优化性能。

Method: 提出一种名为HIPA（Hierarchical IoT Processing Architecture）的新框架，利用机器学习在云、雾和边缘三层之间动态分配计算任务。

Result: 该框架能够有效降低延迟、提高系统可扩展性，并增强数据隐私保护，从而构建高效、安全且可扩展的物联网生态系统。

Conclusion: 云、雾和边缘计算的协同作用对于提升物联网系统的整体性能至关重要，而所提出的HIPA框架为实现这一目标提供了可行路径。

Abstract: The Internet of Things (IoT) is transforming industries by connecting billions of devices to collect, process, and share data. However, the massive data volumes and real-time demands of IoT applications strain traditional cloud computing architectures. This paper explores the complementary roles of cloud, fog, and edge computing in enhancing IoT performance, focusing on their ability to reduce latency, improve scalability, and ensure data privacy. We propose a novel framework, the Hierarchical IoT Processing Architecture (HIPA), which dynamically allocates computational tasks across cloud, fog, and edge layers using machine learning. By synthesizing current research and introducing HIPA, this paper highlights how these paradigms can create efficient, secure, and scalable IoT ecosystems.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [22] [Introduction to Automated Negotiation](https://arxiv.org/abs/2511.08659)
*Dave de Jonge*

Main category: cs.MA

TL;DR: 本书是一本面向计算机专业初学者的自动谈判入门教材，附带一个简单的Python谈判框架，便于读者实现和实验自己的谈判算法。


<details>
  <summary>Details</summary>
Motivation: 为完全没有自动谈判背景知识的计算机科学学生提供一本易于上手的入门教材。

Method: 提供基础数学与编程知识即可理解的内容，并配套一个轻量级、可移植的Python谈判框架供实践使用。

Result: 读者能够通过该书及其框架快速理解自动谈判的基本概念并实现自己的算法。

Conclusion: 该书通过简洁的内容和实用的工具有效降低了自动谈判领域的学习门槛。

Abstract: This book is an introductory textbook targeted towards computer science students who are completely new to the topic of automated negotiation. It does not require any prerequisite knowledge, except for elementary mathematics and basic programming skills.
  This book comes with an simple toy-world negotiation framework implemented in Python that can be used by the readers to implement their own negotiation algorithms and perform experiments with them. This framework is small and simple enough that any reader who does not like to work in Python should be able to re-implement it very quickly in any other programming language of their choice.

</details>


### [23] [Convergence dynamics of Agent-to-Agent Interactions with Misaligned objectives](https://arxiv.org/abs/2511.08710)
*Romain Cosentino,Sarath Shekkizhar,Adam Earle*

Main category: cs.MA

TL;DR: 本文提出一个理论框架，用于分析目标不一致的语言模型智能体在多智能体交互中的生成动态，揭示了由此产生的有偏均衡现象，并提供了可证明实现单边成功的对抗算法，实验验证了该理论。


<details>
  <summary>Details</summary>
Motivation: 研究语言模型智能体在目标不一致的多智能体交互中如何相互影响，以及这种交互如何导致偏离各自目标的有偏均衡，从而理解提示设计与交互设置对系统稳定性、偏差和鲁棒性的影响。

Method: 构建理论框架，分析两个基于语言模型的智能体在上下文中进行迭代梯度更新时的动态行为；推导目标错位下的收敛条件，并提出一种可证明实现单边成功的对抗算法。

Result: 理论分析表明，目标不一致会导致有偏均衡，残差误差可由目标差距和提示几何结构预测；实验使用训练好的Transformer模型和GPT-5在上下文线性回归任务中验证了该理论。

Conclusion: 该框架为理解和预测多智能体系统的交互行为提供了基础，并强调了提示设计对系统稳定性与鲁棒性的关键作用，同时为防御此类系统中的不良行为提供方向。

Abstract: We develop a theoretical framework for agent-to-agent interactions in multi-agent scenarios. We consider the setup in which two language model based agents perform iterative gradient updates toward their respective objectives in-context, using the output of the other agent as input. We characterize the generation dynamics associated with the interaction when the agents have misaligned objectives, and show that this results in a biased equilibrium where neither agent reaches its target - with the residual errors predictable from the objective gap and the geometry induced by the prompt of each agent. We establish the conditions for asymmetric convergence and provide an algorithm that provably achieves an adversarial result, producing one-sided success. Experiments with trained transformer models as well as GPT$5$ for the task of in-context linear regression validate the theory. Our framework presents a setup to study, predict, and defend multi-agent systems; explicitly linking prompt design and interaction setup to stability, bias, and robustness.

</details>


### [24] [Achieving Equilibrium under Utility Heterogeneity: An Agent-Attention Framework for Multi-Agent Multi-Objective Reinforcement Learning](https://arxiv.org/abs/2511.08926)
*Zhuhui Li,Chunbo Luo,Liming Huang,Luyu Qi,Geyong Min*

Main category: cs.MA

TL;DR: 该论文提出了一种名为AA-MAMORL的新框架，通过在集中训练阶段隐式学习其他智能体的效用函数和策略，使每个智能体在去中心化执行时能基于局部观测和自身私有效用函数近似贝叶斯纳什均衡，从而在多智能体多目标系统中显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有MAMOS优化方法难以处理异构目标和效用函数设置下的非平稳性问题，尤其在去中心化执行约束下缺乏对全局效用函数的有效利用。

Method: 提出AA-MAMORL框架，在集中训练中通过智能体注意力机制隐式建模其他智能体的效用函数与策略，构建联合信念；执行阶段各智能体仅依赖局部观测和私有效用函数独立决策，无需通信。

Result: 在自定义MAMO Particle环境和标准MOMALand基准上的实验表明，该方法显著优于现有最先进方法，验证了利用全局偏好信息的有效性。

Conclusion: 访问全局效用函数对于实现去中心化执行下的贝叶斯纳什均衡是必要的，所提出的AA-MAMORL框架通过隐式建模全局信息有效提升了多智能体多目标系统的性能。

Abstract: Multi-agent multi-objective systems (MAMOS) have emerged as powerful frameworks for modelling complex decision-making problems across various real-world domains, such as robotic exploration, autonomous traffic management, and sensor network optimisation. MAMOS offers enhanced scalability and robustness through decentralised control and more accurately reflects inherent trade-offs between conflicting objectives. In MAMOS, each agent uses utility functions that map return vectors to scalar values. Existing MAMOS optimisation methods face challenges in handling heterogeneous objective and utility function settings, where training non-stationarity is intensified due to private utility functions and the associated policies. In this paper, we first theoretically prove that direct access to, or structured modeling of, global utility functions is necessary for the Bayesian Nash Equilibrium under decentralised execution constraints. To access the global utility functions while preserving the decentralised execution, we propose an Agent-Attention Multi-Agent Multi-Objective Reinforcement Learning (AA-MAMORL) framework. Our approach implicitly learns a joint belief over other agents' utility functions and their associated policies during centralised training, effectively mapping global states and utilities to each agent's policy. In execution, each agent independently selects actions based on local observations and its private utility function to approximate a BNE, without relying on inter-agent communication. We conduct comprehensive experiments in both a custom-designed MAMO Particle environment and the standard MOMALand benchmark. The results demonstrate that access to global preferences and our proposed AA-MAMORL significantly improve performance and consistently outperform state-of-the-art methods.

</details>


### [25] [Learning Efficient Communication Protocols for Multi-Agent Reinforcement Learning](https://arxiv.org/abs/2511.09171)
*Xinren Zhang,Jiadong Yu,Zixin Zhong*

Main category: cs.MA

TL;DR: 本文提出了一种用于学习高效多轮通信协议的通用框架，并引入三种新的通信效率度量指标（IEI、SEI 和 TEI），以优化多智能体强化学习中的信息交换效率与协作性能。


<details>
  <summary>Details</summary>
Motivation: 现有研究在多智能体强化学习中缺乏对通信协议（包括通信拓扑和消息内容）的明确定义与优化，导致信息交换冗余或低效。

Method: 作者构建了一个通用学习框架，通过将信息熵效率指数（IEI）和专业化效率指数（SEI）作为调整后的损失函数来优化通信，并利用拓扑效率指数（TEI）评估通信量与任务性能之间的权衡。

Result: 实验表明，所学得的通信协议显著提升了通信效率，并在合作任务中取得了更高的成功率和更好的整体性能。

Conclusion: 该研究为多智能体系统中高效通信协议的学习提供了新思路和有效工具，兼顾了通信效率与任务表现。

Abstract: Multi-Agent Systems (MAS) have emerged as a powerful paradigm for modeling complex interactions among autonomous entities in distributed environments. In Multi-Agent Reinforcement Learning (MARL), communication enables coordination but can lead to inefficient information exchange, since agents may generate redundant or non-essential messages. While prior work has focused on boosting task performance with information exchange, the existing research lacks a thorough investigation of both the appropriate definition and the optimization of communication protocols (communication topology and message). To fill this gap, we introduce a generalized framework for learning multi-round communication protocols that are both effective and efficient. Within this framework, we propose three novel Communication Efficiency Metrics (CEMs) to guide and evaluate the learning process: the Information Entropy Efficiency Index (IEI) and Specialization Efficiency Index (SEI) for efficiency-augmented optimization, and the Topology Efficiency Index (TEI) for explicit evaluation. We integrate IEI and SEI as the adjusted loss functions to promote informative messaging and role specialization, while using TEI to quantify the trade-off between communication volume and task performance. Through comprehensive experiments, we demonstrate that our learned communication protocol can significantly enhance communication efficiency and achieves better cooperation performance with improved success rates.

</details>


<div id='cs.PF'></div>

# cs.PF [[Back]](#toc)

### [26] [PANDA: Noise-Resilient Antagonist Identification in Production Datacenters](https://arxiv.org/abs/2511.08803)
*Sixiang Zhou,Nan Deng,Krzysiek Rzadca,Charlie Y. Hu,Xiaojun Lin*

Main category: cs.PF

TL;DR: PANDA是一种用于数据中心生产环境的抗噪干扰任务识别框架，通过利用全局历史信息和引入机器级CPI指标，显著提升了对抗性任务识别的准确性和多受害者场景下的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有对抗性任务检测方法存在离线分析成本高、不可扩展，或在线采样噪声大、在多受害者场景下失效的问题，亟需一种适用于大规模生产环境且对噪声鲁棒的解决方案。

Method: PANDA沿用基于CPI的相关性方法，但创新地结合所有机器的全局历史知识以抑制采样噪声，并提出一种新的机器级CPI指标来刻画多个共置任务间的共享资源竞争。

Result: 在Google近期生产追踪数据上的评估表明，PANDA将真实对抗任务的平均可疑度百分位从50–55%提升至82.6%，并在多受害者场景下保持稳定识别能力，且运行开销可忽略。

Conclusion: PANDA有效解决了现有方法在噪声和多受害者场景下的局限性，为大规模数据中心提供了一种高效、准确且实用的对抗性任务识别方案。

Abstract: Modern warehouse-scale datacenters commonly collocate multiple jobs on shared machines to improve resource utilization. However, such collocation often leads to performance interference caused by antagonistic jobs that overconsume shared resources. Existing antagonist-detection approaches either rely on offline profiling, which is costly and unscalable, or use a sample-from-production approach, which suffers from noisy measurements and fails under multi-victim scenarios. We present PANDA, a noise-resilient antagonist identification framework for production-scale datacenters. Like prior correlation-based methods, PANDA uses cycles per instruction (CPI) as its performance metric, but it differs by (i) leveraging global historical knowledge across all machines to suppress sampling noise and (ii) introducing a machine-level CPI metric that captures shared-resource contention among multiple co-located tasks. Evaluation on a recent Google production trace shows that PANDA ranks true antagonists far more accurately than prior methods -- improving average suspicion percentile from 50-55% to 82.6% -- and achieves consistent antagonist identification under multi-victim scenarios, all with negligible runtime overhead.

</details>
