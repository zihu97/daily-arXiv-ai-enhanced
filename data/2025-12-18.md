<div id=toc></div>

# Table of Contents

- [cs.DC](#cs.DC) [Total: 3]
- [cs.MA](#cs.MA) [Total: 1]
- [cs.SE](#cs.SE) [Total: 20]
- [cs.AR](#cs.AR) [Total: 1]
- [cs.NI](#cs.NI) [Total: 2]


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [1] [LLMQ: Efficient Lower-Precision Pretraining for Consumer GPUs](https://arxiv.org/abs/2512.15306)
*Erik Schultheis,Dan Alistarh*

Main category: cs.DC

TL;DR: LLMQ enables efficient training of medium-sized LLMs on affordable GPUs via optimized CUDA/C++ implementation.


<details>
  <summary>Details</summary>
Motivation: To make large language model training accessible on low-memory, consumer-grade GPUs.

Method: Uses activation checkpointing, offloading, and copy-engine collectives in an end-to-end CUDA/C++ framework.

Result: Successfully trains 7B models on a single 16GB GPU and 32B models on 4 RTX 4090s with ~50% FLOP utilization.

Conclusion: LLMQ achieves efficiency comparable to cloud-grade systems while using commodity hardware.

Abstract: We present LLMQ, an end-to-end CUDA/C++ implementation for medium-sized language-model training, e.g. 3B to 32B parameters, on affordable, commodity GPUs. These devices are characterized by low memory availability and slow communication compared to datacentre-grade GPUs. Consequently, we showcase a range of optimizations that target these bottlenecks, including activation checkpointing, offloading, and copy-engine based collectives. LLMQ is able to train or fine-tune a 7B model on a single 16GB mid-range gaming card, or a 32B model on a workstation equipped with 4 RTX 4090s. This is achieved while executing a standard 8-bit training pipeline, without additional algorithmic approximations, and maintaining FLOP utilization of around 50%. The efficiency of LLMQ rivals that of production-scale systems on much more expensive cloud-grade GPUs.

</details>


### [2] [Optimizing Bloom Filters for Modern GPU Architectures](https://arxiv.org/abs/2512.15595)
*Daniel Jünger,Kevin Kristensen,Yunsong Wang,Xiangyao Yu,Bertil Schmidt*

Main category: cs.DC

TL;DR: 本文提出了一种针对GPU优化的布隆过滤器设计，通过向量化、线程协作和计算延迟三个维度优化，在保持高精度的同时显著提升吞吐量。


<details>
  <summary>Details</summary>
Motivation: 现有GPU上的布隆过滤器设计研究不足，无法充分发挥其并行计算潜力。

Method: 从向量化、线程协作与计算延迟三方面探索GPU上布隆过滤器的设计空间，并结合硬件特性调优参数配置。

Result: 在B200 GPU上实现超过92%的实际极限速度，批量查询和构建分别比现有技术快11.35倍和15.4倍，同时突破速度与精度的传统权衡。

Conclusion: 所提方法高效利用GPU硬件特性，显著提升布隆过滤器性能，代码将开源以促进后续研究与应用。

Abstract: Bloom filters are a fundamental data structure for approximate membership queries, with applications ranging from data analytics to databases and genomics. Several variants have been proposed to accommodate parallel architectures. GPUs, with massive thread-level parallelism and high-bandwidth memory, are a natural fit for accelerating these Bloom filter variants potentially to billions of operations per second. Although CPU-optimized implementations have been well studied, GPU designs remain underexplored. We close this gap by exploring the design space on GPUs along three dimensions: vectorization, thread cooperation, and compute latency.
  Our evaluation shows that the combination of these optimization points strongly affects throughput, with the largest gains achieved when the filter fits within the GPU's cache domain. We examine how the hardware responds to different parameter configurations and relate these observations to measured performance trends. Crucially, our optimized design overcomes the conventional trade-off between speed and precision, delivering the throughput typically restricted to high-error variants while maintaining the superior accuracy of high-precision configurations. At iso error rate, the proposed method outperforms the state-of-the-art by $11.35\times$ ($15.4\times$) for bulk filter lookup (construction), respectively, achieving above $92\%$ of the practical speed-of-light across a wide range of configurations on a B200 GPU. We propose a modular CUDA/C++ implementation, which will be openly available soon.

</details>


### [3] [Dynamic Rebatching for Efficient Early-Exit Inference with DREX](https://arxiv.org/abs/2512.15705)
*Xuting Liu,Daniel Alexander,Siva Kesava Reddy Kakarla,Behnaz Arzani,Vincent Liu*

Main category: cs.DC

TL;DR: DREX通过动态重组批次和优化调度，提升Early-Exit LLM推理吞吐量并确保输出质量。


<details>
  <summary>Details</summary>
Motivation: 解决传统批处理框架在Early-Exit架构中无法灵活处理不同退出时机的问题，避免强制统一决策或过早退出导致的质量下降。

Method: 提出Dynamic Rebatching机制，结合无拷贝缓冲区与EE/SLA感知调度器，并高效处理跳过层的KV缓存缺失问题。

Result: 实验表明DREX相比基线方法提升2%-12%吞吐量，同时完全消除非自愿退出，保障模型预期输出质量。

Conclusion: DREX为Early-Exit LLM提供高效且保质的推理方案，具备实际部署价值。

Abstract: Early-Exit (EE) is a Large Language Model (LLM) architecture that accelerates inference by allowing easier tokens to be generated using only a subset of the model's layers. However, traditional batching frameworks are ill-suited for EE LLMs, as not all requests in a batch may be ready to exit at the same time. Existing solutions either force a uniform decision on the batch, which overlooks EE opportunities, or degrade output quality by forcing premature exits. We propose Dynamic Rebatching, a solution where we dynamically reorganize the batch at each early-exit point. Requests that meet the exit criteria are immediately processed, while those that continue are held in a buffer, re-grouped into a new batch, and forwarded to deeper layers. We introduce DREX, an early-exit inference system that implements Dynamic Rebatching with two key optimizations: 1) a copy-free rebatching buffer that avoids physical data movement, and 2) an EE and SLA-aware scheduler that analytically predicts whether a given rebatching operation will be profitable. DREX also efficiently handles the missing KV cache from skipped layers using memory-efficient state-copying. Our evaluation shows that DREX improves throughput by 2-12% compared to baseline approaches while maintaining output quality. Crucially, DREX completely eliminates involuntary exits, providing a key guarantee for preserving the output quality intended by the EE model.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [4] [Mapis: A Knowledge-Graph Grounded Multi-Agent Framework for Evidence-Based PCOS Diagnosis](https://arxiv.org/abs/2512.15398)
*Zanxiang He,Meng Li,Liyun Shi,Weiye Daia,Liming Nie*

Main category: cs.MA

TL;DR: 提出Mapis框架，首个基于指南的多智能体系统，用于可解释、精准的PCOS诊断。


<details>
  <summary>Details</summary>
Motivation: 现有PCOS诊断工具依赖大量标注数据且缺乏可解释性，通用医疗多智能体系统领域知识不足。

Method: 构建基于2023国际指南的协作式多智能体框架，结合妇科内分泌、影像学和排除病因三类智能体，并辅以PCOS知识图谱支持决策。

Result: 在临床数据集上，Mapis准确率较传统机器学习高13.56%，较单智能体高6.55%，较既有医疗多智能体系统高7.05%。

Conclusion: Mapis显著提升PCOS诊断性能，兼具高准确性与临床可解释性，为专科医学多智能体系统提供新范式。

Abstract: Polycystic Ovary Syndrome (PCOS) constitutes a significant public health issue affecting 10% of reproductive-aged women, highlighting the critical importance of developing effective diagnostic tools. Previous machine learning and deep learning detection tools are constrained by their reliance on large-scale labeled data and an lack of interpretability. Although multi-agent systems have demonstrated robust capabilities, the potential of such systems for PCOS detection remains largely unexplored. Existing medical multi-agent frameworks are predominantly designed for general medical tasks, suffering from insufficient domain integration and a lack of specific domain knowledge. To address these challenges, we propose Mapis, the first knowledge-grounded multi-agent framework explicitly designed for guideline-based PCOS diagnosis. Specifically, it built upon the 2023 International Guideline into a structured collaborative workflow that simulates the clinical diagnostic process. It decouples complex diagnostic tasks across specialized agents: a gynecological endocrine agent and a radiology agent collaborative to verify inclusion criteria, while an exclusion agent strictly rules out other causes. Furthermore, we construct a comprehensive PCOS knowledge graph to ensure verifiable, evidence-based decision-making. Extensive experiments on public benchmarks and specialized clinical datasets, benchmarking against nine diverse baselines, demonstrate that Mapis significantly outperforms competitive methods. On the clinical dataset, it surpasses traditional machine learning models by 13.56%, single-agent by 6.55%, and previous medical multi-agent systems by 7.05% in Accuracy.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [5] [How Deep Does Your Dependency Tree Go? An Empirical Study of Dependency Amplification Across 10 Package Ecosystems](https://arxiv.org/abs/2512.14739)
*Jahidul Arafat*

Main category: cs.SE

TL;DR: 该研究分析了十大主流包管理生态系统的依赖放大现象，发现Maven的平均放大倍数最高（24.70倍），而CocoaPods最低（0.32倍），表明不同生态系统因设计差异导致安全风险各异，需采取针对性策略。


<details>
  <summary>Details</summary>
Motivation: 现代软件开发依赖包生态系统，但跨生态系统的依赖放大模式尚未系统比较，这关系到软件供应链安全。

Method: 对来自十个主要生态系统的500个项目进行实证分析，比较其直接与传递依赖的比率，并评估生态系统设计因素的影响。

Result: Maven放大倍数显著高于其他生态；28%的Maven项目放大超10倍；npm并非放大最严重者；生态系统设计如依赖解析机制、标准库完整性等是关键影响因素。

Conclusion: 应根据生态系统特性制定安全策略：Maven需系统审计，npm和RubyGems需重点检测异常，低放大生态可维持现有实践。

Abstract: Modern software development relies on package ecosystems where a single declared dependency can pull in many additional transitive packages. This dependency amplification, defined as the ratio of transitive to direct dependencies, has major implications for software supply chain security, yet amplification patterns across ecosystems have not been compared at scale. We present an empirical study of 500 projects across ten major ecosystems, including Maven Central for Java, npm Registry for JavaScript, crates io for Rust, PyPI for Python, NuGet Gallery for dot NET, RubyGems for Ruby, Go Modules for Go, Packagist for PHP, CocoaPods for Swift and Objective C, and Pub for Dart. Our analysis shows that Maven exhibits mean amplification of 24.70 times, compared to 4.48 times for Go Modules, 4.32 times for npm, and 0.32 times for CocoaPods. We find significant differences with large effect sizes in 22 of 45 pairwise comparisons, challenging the assumption that npm has the highest amplification due to its many small purpose packages. We observe that 28 percent of Maven projects exceed 10 times amplification, indicating a systematic pattern rather than isolated outliers, compared to 14 percent for RubyGems, 12 percent for npm, and zero percent for Cargo, PyPI, Packagist, CocoaPods, and Pub. We attribute these differences to ecosystem design choices such as dependency resolution behavior, standard library completeness, and platform constraints. Our findings suggest adopting ecosystem specific security strategies, including systematic auditing for Maven environments, targeted outlier detection for npm and RubyGems, and continuation of current practices for ecosystems with controlled amplification. We provide a full replication package with data and analysis scripts.

</details>


### [6] [VDMN: A Graphical Notation for Modelling Value Driver Trees](https://arxiv.org/abs/2512.14740)
*Benjamin Matthies*

Main category: cs.SE

TL;DR: 本文提出了价值驱动建模符号（VDMN），以系统化和标准化价值驱动树（VDT）的建模过程。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏系统性的VDT建模指南，阻碍了其在管理决策中的有效应用。

Method: 开发VDMN图形符号，并通过两个案例研究与专家访谈评估其实际效用。

Result: VDMN能支持一致且易懂的VDT建模，获得专家认可。

Conclusion: VDMN是推动VDT建模系统化与标准化的重要进展。

Abstract: Value Driver Trees (VDTs) are conceptual models used to illustrate and analyse the causal relationships between key performance indicators and business outcomes, thereby supporting managerial decision-making and value-based management. Despite their increasing application, there are still no systematic guidelines for the modelling of such conceptual models. To fill this gap, this study introduces the Value Driver Modelling Notation (VDMN), a graphical notation developed to systematically guide VDT modelling. This notation includes a comprehensive set of semantic constructs and an intuitive graphical syntax. To evaluate its practical utility, the VDMN was applied in two case studies and assessed through expert interviews. The results show that the notation supports a consistent and comprehensible modelling of VDTs. The VDMN thus represents a significant step towards the systematisation and standardisation of VDT modelling.

</details>


### [7] [Revisiting the Reliability of Language Models in Instruction-Following](https://arxiv.org/abs/2512.14754)
*Jianshuo Dong,Yutong Zhang,Yan Liu,Zhenyu Zhong,Tao Wei,Chao Zhang,Han Qiu*

Main category: cs.SE

TL;DR: 本文提出nuance-oriented reliability概念，通过新指标reliable@k和自动化数据增强构建IFEval++评估集，发现当前LLM在细微语义变化下性能显著下降，并探索三种改进方案。


<details>
  <summary>Details</summary>
Motivation: 现有LLM在基准测试中表现优异，但在真实场景中面对用户表达的细微变化时可靠性不足，亟需系统性评估与改进。

Method: 设计新评估指标reliable@k，构建自动化生成cousin prompts的数据增强流程，建立IFEval++评测集，在46个模型上进行系统评估并尝试三种改进策略。

Result: 实验显示当前模型在细微提示修改下性能最多下降61.8%，揭示nuance-oriented reliability是当前LLM的重要短板。

Conclusion: nuance-oriented reliability是提升LLM实际可用性与可信度的关键方向，值得后续深入研究。

Abstract: Advanced LLMs have achieved near-ceiling instruction-following accuracy on benchmarks such as IFEval. However, these impressive scores do not necessarily translate to reliable services in real-world use, where users often vary their phrasing, contextual framing, and task formulations. In this paper, we study nuance-oriented reliability: whether models exhibit consistent competence across cousin prompts that convey analogous user intents but with subtle nuances. To quantify this, we introduce a new metric, reliable@k, and develop an automated pipeline that generates high-quality cousin prompts via data augmentation. Building upon this, we construct IFEval++ for systematic evaluation. Across 20 proprietary and 26 open-source LLMs, we find that current models exhibit substantial insufficiency in nuance-oriented reliability -- their performance can drop by up to 61.8% with nuanced prompt modifications. What's more, we characterize it and explore three potential improvement recipes. Our findings highlight nuance-oriented reliability as a crucial yet underexplored next step toward more dependable and trustworthy LLM behavior. Our code and benchmark are accessible: https://github.com/jianshuod/IFEval-pp.

</details>


### [8] [Examining Software Developers' Needs for Privacy Enforcing Techniques: A survey](https://arxiv.org/abs/2512.14756)
*Ioanna Theophilou,Georgia M. Kapitsaki*

Main category: cs.SE

TL;DR: 本文通过调查68名开发者，探讨了在软件开发中实现隐私合规的自动化工具需求，发现经验丰富的开发者更关注隐私工具，强调亟需隐私辅助工具。


<details>
  <summary>Details</summary>
Motivation: 数据隐私法规（如GDPR和CCPA/CPRA）要求所有软件系统必须合规，但开发者因缺乏法律知识难以有效集成相关功能，因此需要研究其实际需求以推动自动化工具发展。

Method: 通过问卷调查68名开发者，分析影响开发者对隐私合规工具需求的因素，并评估现有开发实践中的痛点。

Result: 大多数开发者希望获得更多自动化工具支持，且具备隐私经验的开发者对隐私工具的需求更高。

Conclusion: 研究结果有助于开发者更好地定位隐私合规开发活动，并凸显了对隐私辅助工具的迫切需求。

Abstract: Data privacy legislation, such as GDPR and CCPA/CPRA, has rendered data privacy law compliance a requirement of all software systems. Developers need to implement various kinds of functionalities to cover law needs, including user rights and law principles. As data compliance is tightly coupled with legal knowledge, it is not always easy to perform such integrations in software systems. Prior studies have focused on developers' understanding of privacy principles, such as Privacy by Design, and have examined privacy techniques used in the software industry. Nevertheless, emerging developer needs that can assist in privacy law compliance have not been examined but are useful in understanding what development automation tools, such as Generative AI, need to cover to make the compliance process more straightforward and seamless within the development process. In this work, we present a survey that examines the above needs with the participation of 68 developers, while we have examined which factors affect practitioners' needs. Most developers express a need for more automated tools, while privacy experience increases practitioners' concerns for privacy tools. Our results can assist practitioners in better positioning their development activities within privacy law compliance and point to an urgent need for privacy facilitators.

</details>


### [9] [CAPE: Capability Achievement via Policy Execution](https://arxiv.org/abs/2512.14761)
*David Ball*

Main category: cs.SE

TL;DR: CAPE协议通过将需求转化为可执行规范，显著降低模型违规率并降低成本。


<details>
  <summary>Details</summary>
Motivation: 现有AI系统缺乏表达和强制约束的能力，导致部署失败。

Method: 提出Capability Engineering方法，使用CAPE协议实现Specify->Verify->Correct->Train循环。

Result: 在六领域109,500例中，CAPE相较DPO降低81%违规率，成本减少5-20倍。

Conclusion: CAPE推动评估从智能基准转向能力测量，提升模型可靠性与开发效率。

Abstract: Modern AI systems lack a way to express and enforce requirements. Pre-training produces intelligence, and post-training optimizes preferences, but neither guarantees that models reliably satisfy explicit, context-dependent constraints. This missing abstraction explains why highly intelligent models routinely fail in deployment despite strong benchmark performance.
  We introduce Capability Engineering, the systematic practice of converting requirements into executable specifications and training models to satisfy them by default. We operationalize this practice through CAPE (Capability Achievement via Policy Execution), a protocol implementing a Specify -> Verify -> Correct -> Train loop.
  CAPE is grounded in two empirical findings: (1) contextual objectivity, where properties appearing subjective become objective once context is fixed (inter-annotator agreement rises from kappa = 0.42 to kappa = 0.98), and (2) verification-fidelity scaling, where verification accuracy improves with model scale (r = 0.94), unlike preference agreement which plateaus at 30 to 50 percent disagreement regardless of compute. Across 109,500 examples in six domains, CAPE reduces violation rates by 81 percent relative to DPO (standard deviation less than 0.3 percent). By replacing per-example annotation with reusable specifications, CAPE reduces costs by 5 to 20 times and shortens timelines from months to weeks.
  We release the CAPE protocol, PredicateGraph schema, CPL specification language, and policy packs under Apache 2.0. We also launch CapabilityBench, a public registry of model evaluations against community-contributed policies, shifting evaluation from intelligence benchmarks toward capability measurement.

</details>


### [10] [Workflows vs Agents for Code Translation](https://arxiv.org/abs/2512.14762)
*Henry Gray,Tom Yotam,Octavian Udrea*

Main category: cs.SE

TL;DR: 本文比较了两种基于大语言模型的MATLAB到HDL语法修复方法，发现智能体方法在中小型模型上显著提升仿真通过率。


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型因缺乏HDL训练数据导致端到端转译易出错的问题，提升FPGA/ASIC部署效率。

Method: 对比专家设计的结构化流程与基于MCP协议的自主智能体方法，在42个MATLAB信号处理函数上评估语法修复效果。

Result: 智能体方法在三个模型规模上均更有效修复语法错误，尤其使中型模型仿真通过率提升超20个百分点；条件检索对8B/30B模型有效，235B模型收益较小。

Conclusion: 合理设计的智能体框架能有效弥补中小型模型能力不足，其优势源于短提示、激进上下文管理和条件工具调用。

Abstract: Translating algorithms from high-level languages like MATLAB to hardware description languages (HDLs) is a resource-intensive but necessary step for deployment on FPGAs and ASICs. While large language models (LLMs) offer a path to automation, their limited training on HDL code makes end-to-end transpilation brittle and prone to syntax errors. We compare two LLM-driven methods for syntax repair in a MATLAB-to-HDL pipeline: a structured, expert-designed flow that follows a fixed sequence of operations, and a more autonomous agentic approach that uses the Model Context Protocol (MCP) \cite{anthropic2024mcp} to dynamically select its own tools. We study 42 MATLAB signal-processing functions and isolate the syntax-repair stage. Across three model scales, the agentic approach is more effective at resolving initial syntax errors, unblocking a greater number of candidates to proceed through the pipeline. This upstream improvement yields measurable downstream improvements, most notably on mid-sized models, where it increases the simulation reach rate by over 20 percentage points. We hypothesize the gains come from short prompts, aggressive context management, and conditional tool use. Conditional retrieval helps at 8B and 30B; at 235B final-success gains are small and a naive RAG variant attains the highest final success. Our findings suggest that these agentic frameworks, when properly designed, are most effective at compensating for the capacity limits of small and mid-sized models.

</details>


### [11] [A High-level Synthesis Toolchain for the Julia Language](https://arxiv.org/abs/2512.15679)
*Benedict Short,Ian McInerney,John Wickerson*

Main category: cs.SE

TL;DR: 提出基于MLIR的编译工具链，将Julia语言内核自动转为SystemVerilog，支持FPGA部署，无需额外指令或语言定制。


<details>
  <summary>Details</summary>
Motivation: 解决特定问题加速器开发中的‘双语言问题’，降低硬件加速器开发门槛。

Method: 构建MLIR编译工具链，支持动态/静态调度，集成AXI4-Stream协议，生成厂商无关RTL。

Result: 成功合成多个信号处理/数学基准，在真实FPGA上达100MHz频率，吞吐量达先进工具链的59.71%-82.6%。

Conclusion: 该工具链使领域专家可直接用Julia编写内核并无缝移植至FPGA，无需修改或添加编译指示。

Abstract: With the push towards Exascale computing and data-driven methods, problem sizes have increased dramatically, increasing the computational requirements of the underlying algorithms. This has led to a push to offload computations to general purpose hardware accelerators such as GPUs and TPUs, and a renewed interest in designing problem-specific accelerators using FPGAs. However, the development process of these problem-specific accelerators currently suffers from the "two-language problem": algorithms are developed in one (usually higher-level) language, but the kernels are implemented in another language at a completely different level of abstraction and requiring fundamentally different expertise. To address this problem, we propose a new MLIR-based compiler toolchain that unifies the development process by automatically compiling kernels written in the Julia programming language into SystemVerilog without the need for any additional directives or language customisations. Our toolchain supports both dynamic and static scheduling, directly integrates with the AXI4-Stream protocol to interface with subsystems like on- and off-chip memory, and generates vendor-agnostic RTL. This prototype toolchain is able to synthesize a set of signal processing/mathematical benchmarks that can operate at 100MHz on real FPGA devices, achieving between 59.71% and 82.6% of the throughput of designs generated by state-of-the-art toolchains that only compile from low-level languages like C or C++. Overall, this toolchain allows domain experts to write compute kernels in Julia as they normally would, and then retarget them to an FPGA without additional pragmas or modifications.

</details>


### [12] [Let the Barbarians In: How AI Can Accelerate Systems Performance Research](https://arxiv.org/abs/2512.14806)
*Audrey Cheng,Shu Liu,Melissa Pan,Zhifei Li,Shubham Agarwal,Mert Cemri,Bowen Wang,Alexander Krentsel,Tian Xia,Jongseok Park,Shuo Yang,Jeff Chen,Lakshya Agrawal,Ashwin Naren,Shulu Li,Ruiying Ma,Aditya Desai,Jiarong Xing,Koushik Sen,Matei Zaharia,Ion Stoica*

Main category: cs.SE

TL;DR: 本文探讨了AI驱动的系统研究（ADRS）范式，通过生成、评估与优化循环自动化发现高性能系统方案，在多个案例中超越人工设计，并总结最佳实践与未来方向。


<details>
  <summary>Details</summary>
Motivation: 推动系统研究从手工设计转向AI自动化探索，借助可验证性优势加速创新并提升性能。

Method: 采用开源ADRS框架（如OpenEvolve、GEPA、ShinkaEvolve），在十个系统优化案例中迭代生成与评估候选方案。

Result: ADRS生成方案在云调度、负载均衡、SQL优化等任务中匹配或超越人类顶尖设计。

Conclusion: ADRS展现巨大潜力，虽尚无通用方法，但已为未来研究提供可行路径与实践指导。

Abstract: Artificial Intelligence (AI) is beginning to transform the research process by automating the discovery of new solutions. This shift depends on the availability of reliable verifiers, which AI-driven approaches require to validate candidate solutions. Research focused on improving systems performance is especially well-suited to this paradigm because system performance problems naturally admit such verifiers: candidates can be implemented in real systems or simulators and evaluated against predefined workloads. We term this iterative cycle of generation, evaluation, and refinement AI-Driven Research for Systems (ADRS). Using several open-source ADRS instances (i.e., OpenEvolve, GEPA, and ShinkaEvolve), we demonstrate across ten case studies (e.g., multi-region cloud scheduling, mixture-of-experts load balancing, LLM-based SQL, transaction scheduling) that ADRS-generated solutions can match or even outperform human state-of-the-art designs. Based on these findings, we outline best practices (e.g., level of prompt specification, amount of feedback, robust evaluation) for effectively using ADRS, and we discuss future research directions and their implications. Although we do not yet have a universal recipe for applying ADRS across all of systems research, we hope our preliminary findings, together with the challenges we identify, offer meaningful guidance for future work as researcher effort shifts increasingly toward problem formulation and strategic oversight. Note: This paper is an extension of our prior work [14]. It adds extensive evaluation across multiple ADRS frameworks and provides deeper analysis and insights into best practices.

</details>


### [13] [Evaluating Code Reasoning Abilities of Large Language Models Under Real-World Settings](https://arxiv.org/abs/2512.14917)
*Changshu Liu,Alireza Ghazanfari,Yang Chen,Reyhaneh Jabbarvand*

Main category: cs.SE

TL;DR: 本文提出RE2-Bench，一个包含1101个代码推理问题的基准测试，旨在更真实地评估大语言模型在复杂代码场景下的推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试过于简单，无法反映真实代码复杂性，导致对LLM泛化能力的误判。

Method: 利用静态与动态程序分析自动处理复杂类型，并通过九项可解释复杂度指标的多数投票机制划分难度等级。

Result: 六个主流LLM在Easy到Hard问题上表现显著下降（输入预测降51.5%，输出预测降42.15%），揭示先前评估高估了模型能力。

Conclusion: RE2-Bench提供更贴近现实的评估框架，有助于精准校准LLM代码推理能力。

Abstract: Code reasoning tasks are becoming prevalent in large language model (LLM) assessments. Existing benchmarks involve simple programs, failing to represent real-world complexities such as inter- or intra-procedural dependencies, core or third-party API calls, highly nested constructs, and non-primitive complex types. Evaluating LLMs under such a simplistic setting poses a significant threat to assumptions about their generalizability in practice. To enable a more realistic evaluation of code reasoning, this paper proposes RE2-Bench, a benchmark of 1,101 reasoning problems, including 195 drawn from mature real-world projects. RE2-Bench leverages static and dynamic program analysis to automatically serialize and deserialize compound, complex, and custom types in real-world code, going far beyond the primitive-only settings used in prior work.
  A key feature of RE2-Bench is categorizing each reasoning problem as Easy or Hard via a principled majority-vote mechanism over nine interpretable code complexity metrics, resulting in two well-separated and semantically meaningful difficulty categories suitable for precise calibration of LLM reasoning ability. A comprehensive evaluation of six general-purpose and reasoning-oriented LLMs on two widely used code reasoning tasks -- input prediction and output prediction -- using RE2-Bench reveals a significant performance drop from Easy to Hard problems (51.50\% for input prediction and 42.15\% for output prediction), confirming that prior evaluations substantially overestimate the reasoning capabilities of LLMs.

</details>


### [14] [Imitation Game: Reproducing Deep Learning Bugs Leveraging an Intelligent Agent](https://arxiv.org/abs/2512.14990)
*Mehil B Shah,Mohammad Masudur Rahman,Foutse Khomh*

Main category: cs.SE

TL;DR: RepGen 是一种基于 LLM 的自动化智能方法，用于高效复现深度学习中的 bug，显著提升复现率并降低开发者认知负担。


<details>
  <summary>Details</summary>
Motivation: 深度学习应用广泛但 bug 复现困难，现有手动方法仅能复现约 3% 的 bug，亟需自动化解决方案。

Method: RepGen 通过构建学习增强上下文、制定复现计划、采用生成-验证-优化迭代机制，利用大语言模型自动生成复现代码。

Result: 在 106 个真实 bug 上实现 80.19% 复现率，较 SOTA 提升 19.81%；用户研究表明成功率提升 23.35%，耗时减少 56.8%，认知负荷降低。

Conclusion: RepGen 显著提升 DL bug 复现效率与效果，为开发者提供实用工具，推动 DL 系统可靠性提升。

Abstract: Despite their wide adoption in various domains (e.g., healthcare, finance, software engineering), Deep Learning (DL)-based applications suffer from many bugs, failures, and vulnerabilities. Reproducing these bugs is essential for their resolution, but it is extremely challenging due to the inherent nondeterminism of DL models and their tight coupling with hardware and software environments. According to recent studies, only about 3% of DL bugs can be reliably reproduced using manual approaches. To address these challenges, we present RepGen, a novel, automated, and intelligent approach for reproducing deep learning bugs. RepGen constructs a learning-enhanced context from a project, develops a comprehensive plan for bug reproduction, employs an iterative generate-validate-refine mechanism, and thus generates such code using an LLM that reproduces the bug at hand. We evaluate RepGen on 106 real-world deep learning bugs and achieve a reproduction rate of 80.19%, a 19.81% improvement over the state-of-the-art measure. A developer study involving 27 participants shows that RepGen improves the success rate of DL bug reproduction by 23.35%, reduces the time to reproduce by 56.8%, and lowers participants' cognitive load.

</details>


### [15] [Toxicity Ahead: Forecasting Conversational Derailment on GitHub](https://arxiv.org/abs/2512.15031)
*Mia Mohammad Imran,Robert Zita,Rahat Rizvi Rahman,Preetha Chatterjee,Kostadin Damevski*

Main category: cs.SE

TL;DR: 本文提出一种基于大语言模型的两步提示框架，用于预测GitHub讨论中的对话偏离，以实现开源社区中有害对话的早期检测和主动管理。


<details>
  <summary>Details</summary>
Motivation: 开源软件社区中的有害互动降低贡献者参与度，威胁项目可持续性，现有手动管理方式效率低下，亟需可扩展的自动化解决方案。

Method: 构建包含有毒与无毒对话的数据集，采用Least-to-Most提示策略生成对话动态摘要，并据此预测对话偏离可能性，在Qwen和Llama模型上进行评估。

Result: 在内部数据集上F1-score分别达0.901和0.852，在外部验证集上最高达0.797，优于传统NLP基线方法。

Conclusion: 结构化的大语言模型提示策略能有效实现开源社区对话偏离的早期检测，支持可解释、主动式的社区管理。

Abstract: Toxic interactions in Open Source Software (OSS) communities reduce contributor engagement and threaten project sustainability. Preventing such toxicity before it emerges requires a clear understanding of how harmful conversations unfold. However, most proactive moderation strategies are manual, requiring significant time and effort from community maintainers. To support more scalable approaches, we curate a dataset of 159 derailed toxic threads and 207 non-toxic threads from GitHub discussions. Our analysis reveals that toxicity can be forecast by tension triggers, sentiment shifts, and specific conversational patterns.
  We present a novel Large Language Model (LLM)-based framework for predicting conversational derailment on GitHub using a two-step prompting pipeline. First, we generate \textit{Summaries of Conversation Dynamics} (SCDs) via Least-to-Most (LtM) prompting; then we use these summaries to estimate the \textit{likelihood of derailment}. Evaluated on Qwen and Llama models, our LtM strategy achieves F1-scores of 0.901 and 0.852, respectively, at a decision threshold of 0.3, outperforming established NLP baselines on conversation derailment. External validation on a dataset of 308 GitHub issue threads (65 toxic, 243 non-toxic) yields an F1-score up to 0.797. Our findings demonstrate the effectiveness of structured LLM prompting for early detection of conversational derailment in OSS, enabling proactive and explainable moderation.

</details>


### [16] [Aligning Academia with Industry: An Empirical Study of Industrial Needs and Academic Capabilities in AI-Driven Software Engineering](https://arxiv.org/abs/2512.15148)
*Hang Yu,Yuzhou Lai,Li Zhang,Xiaoli Lian,Fang Liu,Yanrui Dong,Ting Zhang,Zhi Jin,David Lo*

Main category: cs.SE

TL;DR: 该研究通过分析1367篇顶会论文和17家企业的282份问卷，揭示了学术界与工业界在软件工程智能化研究中的脱节，并提出七大关键启示以引导未来研究更贴近产业需求。


<details>
  <summary>Details</summary>
Motivation: 弥合软件工程领域学术研究与工业实践之间的差距，确保AI驱动的研究成果真正解决实际问题。

Method: 系统文献分析（2022–2025年FSE/ASE/ICSE会议论文）+ 企业问卷调研（17组织、282份反馈），聚焦六大主题对比学术能力与工业需求。

Result: 识别出学术界过度关注性能指标而忽视需求架构、可靠性、可解释性、输入假设、评估实用性及伦理等工业核心挑战。

Conclusion: 呼吁学术界重新聚焦被忽视的工业痛点，推动软件工程研究向更具实际影响力的方向发展。

Abstract: The rapid advancement of large language models (LLMs) is fundamentally reshaping software engineering (SE), driving a paradigm shift in both academic research and industrial practice. While top-tier SE venues continue to show sustained or emerging focus on areas like automated testing and program repair, with researchers worldwide reporting continuous performance gains, the alignment of these academic advances with real industrial needs remains unclear. To bridge this gap, we first conduct a systematic analysis of 1,367 papers published in FSE, ASE, and ICSE between 2022 and 2025, identifying key research topics, commonly used benchmarks, industrial relevance, and open-source availability. We then carry out an empirical survey across 17 organizations, collecting 282 responses on six prominent topics, i.e., program analysis, automated testing, code generation/completion, issue resolution, pre-trained code models, and dependency management, through structured questionnaires. By contrasting academic capabilities with industrial feedback, we derive seven critical implications, highlighting under-addressed challenges in software requirements and architecture, the reliability and explainability of intelligent SE approaches, input assumptions in academic research, practical evaluation tensions, and ethical considerations. This study aims to refocus academic attention on these important yet under-explored problems and to guide future SE research toward greater industrial impact.

</details>


### [17] [Automating Execution and Verification of BPMN+DMN Business Processes](https://arxiv.org/abs/2512.15214)
*Giuseppe Della Penna,Igor Melatti*

Main category: cs.SE

TL;DR: 本文提出BDTransTest工具，用于将BPMN+DMN流程转换为Java程序并自动生成测试计划，以检测语义错误并分析覆盖率。


<details>
  <summary>Details</summary>
Motivation: 现有工具仅能检测BPMN+DMN模型的语法错误，无法发现语义缺陷，且缺乏透明的仿真转换机制。

Method: 设计BDTransTest工具，实现BPMN+DMN到Java程序的转换、测试计划生成与执行、覆盖分析，并通过文献案例进行实验评估。

Result: 实验表明该方法能有效检测语义错误并提供结构化覆盖分析，优于依赖手动仿真的传统方式。

Conclusion: BDTransTest填补了BPMN+DMN语义验证工具的空白，提升了业务流程设计的可靠性和自动化水平。

Abstract: The increasing and widespread use of BPMN business processes, also embodying DMN tables, requires tools and methodologies to verify their correctness. However, most commonly used frameworks to build BPMN+DMN models only allow designers to detect syntactical errors, thus ignoring semantic (behavioural) faults. This forces business processes designers to manually run single executions of their BPMN+DMN processes using proprietary tools in order to detect failures. Furthermore, how proprietary tools translate a BPMN+DMN process to a computer simulation is left unspecified. In this paper, we advance this state of the art by designing a tool, named BDTransTest providing: i) a translation from a BPMN + DMN process B to a Java program P ; ii) the synthesis and execution of a testing plan for B, that may require the business designer to disambiguate some input domain; iii) the analysis of the coverage achieved by the testing plan in terms of nodes and edges of B. Finally, we provide an experimental evaluation of our methodology on BPMN+DMN processes from the literature.

</details>


### [18] [Heterogeneous Model Alignment in Digital Twin](https://arxiv.org/abs/2512.15281)
*Faima Abbasi,Jean-Sébastien Sottet,Cedric Pruski*

Main category: cs.SE

TL;DR: 提出一种基于大语言模型的异构模型对齐框架，用于多层模型驱动的数字孪生系统，提升语义一致性与自动化程度。


<details>
  <summary>Details</summary>
Motivation: 解决多层数字孪生中因模型异构导致的语义不匹配、同步困难及手动映射易错问题。

Method: 结合自适应一致性机制与大语言模型验证流程，实现元模型动态对齐与领域知识引导的结构保真。

Result: 在空气质量用例与OAEI测试集中验证，有效减少人工映射、提升跨模型扩展性与语义一致性。

Conclusion: 该方法显著增强多层数字孪生系统的自动化对齐能力，保障全生命周期内模型语义连贯与数据完整性。

Abstract: Digital twin (DT) technology integrates heterogeneous data and models, along with semantic technologies to create multi-layered digital representation of physical systems. DTs enable monitoring, simulation, prediction, and optimization to enhance decision making and operational efficiency. A key challenge in multi-layered, model-driven DTs is aligning heterogeneous models across abstraction layers, which can lead to semantic mismatches, inconsistencies, and synchronization issues. Existing methods, relying on static mappings and manual updates, are often inflexible, error-prone, and risk compromising data integrity. To address these limitations, we present a heterogeneous model alignment approach for multi-layered, model-driven DTs. The framework incorporates a flexibility mechanism that allows metamodels to adapt and interconnect seamlessly while maintaining semantic coherence across abstraction layers. It integrates: (i) adaptive conformance mechanisms that link metamodels with evolving models and (ii) a large language model (LLM) validated alignment process that grounds metamodels in domain knowledge, ensuring structural fidelity and conceptual consistency throughout the DT lifecycle. This approach automates semantic correspondences discovery, minimizes manual mapping, and enhances scalability across diverse model types. We illustrate the approach using air quality use case and validate its performance using different test cases from Ontology Alignment Evaluation Initiative (OAEI) tracks.

</details>


### [19] [Can AI Generate more Comprehensive Test Scenarios? Review on Automated Driving Systems Test Scenario Generation Methods](https://arxiv.org/abs/2512.15422)
*Ji Zhou,Yongqi Zhao,Yixian Hu,Hexuan Li,Zhengguo Gu,Nan Xu,Arno Eichberger*

Main category: cs.SE

TL;DR: 本文综述了2015至2025年间自动驾驶系统场景测试方法，重点分析2023年后AI与多模态技术驱动的最新框架，提出标准化评估、伦理整合与ODD覆盖三大改进方向。


<details>
  <summary>Details</summary>
Motivation: 传统道路测试成本高昂，现有综述未能全面覆盖近年AI驱动的场景测试方法进展，亟需系统性梳理与评估框架。

Method: 系统分析31项主研究与10篇综述，聚焦2023~2025年生成模型（如LLM、GAN、扩散模型、强化学习）驱动的场景合成方法，并构建分类体系与评估工具。

Result: 识别出三大研究缺口：缺乏标准评估指标、伦理与人因整合不足、多模态与ODD特定场景覆盖不全；并提出包含多模态扩展分类法、伦理安全清单及ODD覆盖图的新框架。

Conclusion: 本综述为研究人员提供方法论清晰度，为产业界提供可复现评估指南，加速高等级自动驾驶系统的安全部署。

Abstract: Ensuring the safety and reliability of Automated Driving Systems (ADS) remains a critical challenge, as traditional verification methods such as large-scale on-road testing are prohibitively costly and time-consuming.To address this,scenario-based testing has emerged as a scalable and efficient alternative,yet existing surveys provide only partial coverage of recent methodological and technological advances.This review systematically analyzes 31 primary studies,and 10 surveys identified through a comprehensive search spanning 2015~2025;however,the in-depth methodological synthesis and comparative evaluation focus primarily on recent frameworks(2023~2025),reflecting the surge of Artificial Intelligent(AI)-assisted and multimodal approaches in this period.Traditional approaches rely on expert knowledge,ontologies,and naturalistic driving or accident data,while recent developments leverage generative models,including large language models,generative adversarial networks,diffusion models,and reinforcement learning frameworks,to synthesize diverse and safety-critical scenarios.Our synthesis identifies three persistent gaps:the absence of standardized evaluation metrics,limited integration of ethical and human factors,and insufficient coverage of multimodal and Operational Design Domain (ODD)-specific scenarios.To address these challenges,this review contributes a refined taxonomy that incorporates multimodal extensions,an ethical and safety checklist for responsible scenario design,and an ODD coverage map with a scenario-difficulty schema to enable transparent benchmarking.Collectively,these contributions provide methodological clarity for researchers and practical guidance for industry,supporting reproducible evaluation and accelerating the safe deployment of higher-level ADS.

</details>


### [20] [Insecure Ingredients? Exploring Dependency Update Patterns of Bundled JavaScript Packages on the Web](https://arxiv.org/abs/2512.15447)
*Ben Swierzy,Marc Ohm,Michael Meier*

Main category: cs.SE

TL;DR: Aletheia是一种新的包无关方法，通过抄袭检测算法识别JavaScript包版本，显著优于现有方法，并发现捆绑包更新速度远快于CDN引入的包。


<details>
  <summary>Details</summary>
Motivation: 现有方法无法大规模分析现代Web应用的依赖更新行为，且npm上流行但存在漏洞的包版本广泛使用，需更准确的检测机制。

Method: 提出Aletheia方法，利用抄袭检测算法从JavaScript bundle中提取并识别包版本，适用于任意包。

Result: 在Tranco前10万域名中，5%-20%的网站在16周内更新依赖；捆绑包更新更快，漏洞版本少10倍，但少数供应商主导了及时更新。

Conclusion: Aletheia能高效识别包版本，揭示现代Web应用依赖更新模式，但更新行为受少数供应商影响，仅靠量化数据不足以全面评估。

Abstract: Reusable software components, typically distributed as packages, are a central paradigm of modern software development. The JavaScript ecosystem serves as a prime example, offering millions of packages with their use being promoted as idiomatic. However, download statistics on npm raise security concerns as they indicate a high popularity of vulnerable package versions while their real prevalence on production websites remains unknown. Package version detection mechanisms fill this gap by extracting utilized packages and versions from observed artifacts on the web. Prior research focuses on mechanisms for either hand-selected popular packages in bundles or for single-file resources utilizing the global namespace. This does not allow for a thorough analysis of modern web applications' dependency update behavior at scale. In this work, we improve upon this by presenting Aletheia, a package-agnostic method which dissects JavaScript bundles to identify package versions through algorithms originating from the field of plagiarism detection. We show that this method clearly outperforms the existing approaches in practical settings. Furthermore, we crawl the Tranco top 100,000 domains to reveal that 5% - 20% of domains update their dependencies within 16 weeks. Surprisingly, from a longitudinal perspective, bundled packages are updated significantly faster than their CDN-included counterparts, with consequently up to 10 times fewer known vulnerable package versions included. Still, we observe indicators that few widespread vendors seem to be a major driving force behind timely updates, implying that quantitative measures are not painting a complete picture.

</details>


### [21] [A Container-based Approach For Proactive Asset Administration Shell Digital Twins](https://arxiv.org/abs/2512.15452)
*Carsten Ellwein,Jingxi Zhang,Andreas Wortmann,Antony Ayman Alfy Meckhael*

Main category: cs.SE

TL;DR: 本文提出一种基于子模型的架构，使资产 administration shell (AAS) 能动态集成容器化服务，从静态数据模型升级为主动执行增值服务的智能接口。


<details>
  <summary>Details</summary>
Motivation: 现有 AAS 模型缺乏对动态服务集成与系统自适应的支持，限制了其在智能制造中发挥更大价值。

Method: 通过扩展子模型加入行为定义，构建模块化事件驱动架构，依据触发条件部署容器化服务。

Result: 以三轴铣床为案例验证，实现了 AAS 实例运行时的服务动态交互与自适应能力。

Conclusion: 该方法使 AAS 不仅作为数字镜像，更成为执行增值服务的主动接口，为未来 AI 驱动的系统智能奠定基础。

Abstract: In manufacturing, digital twins, realized as Asset Administration Shells (AAS), have emerged as a prevalent practice. These digital replicas, often utilized as structured repositories of asset-related data, facilitate interoperability across diverse systems. However, extant approaches treat the AAS as a static information model, lacking support for dynamic service integration and system adaptation. The existing body of literature has not yet thoroughly explored the potential for integrating executable behavior, particularly in the form of containerized services, into or from the AAS. This integration could serve to enable proactive functionality. In this paper, we propose a submodel-based architecture that introduces a structured service notion to the AAS, enabling services to dynamically interact with and adapt AAS instances at runtime. This concept is implemented through the extension of a submodel with behavioral definitions, resulting in a modular event-driven architecture capable of deploying containerized services based on embedded trigger conditions. The approach is illustrated through a case study on a 3-axis milling machine. Our contribution enables the AAS to serve not only as a passive digital representation but also as an active interface for executing added-value services.%, thereby laying the foundation for future AI-driven adaptation and system-level intelligence in digital twin environments.

</details>


### [22] [On Assessing the Relevance of Code Reviews Authored by Generative Models](https://arxiv.org/abs/2512.15466)
*Robert Heumüller,Frank Ortmeier*

Main category: cs.SE

TL;DR: 提出基于多主观排名的评估方法，用于更有效地评估ChatGPT在代码审查中的表现。


<details>
  <summary>Details</summary>
Motivation: 现有评估方法无法准确衡量生成式AI在代码审查中的质量，需改进评估方式。

Method: 通过280个CodeReview StackExchange数据集，由多名人类评委对ChatGPT与人类评论进行质量排名比较。

Result: ChatGPT生成的评论质量显著优于人类评论，甚至超过平台采纳的答案。

Conclusion: 新方法能更合理评估AI在代码审查中的表现，同时警示盲目集成AI的风险。

Abstract: The use of large language models like ChatGPT in code review offers promising efficiency gains but also raises concerns about correctness and safety. Existing evaluation methods for code review generation either rely on automatic comparisons to a single ground truth, which fails to capture the variability of human perspectives, or on subjective assessments of "usefulness", a highly ambiguous concept. We propose a novel evaluation approach based on what we call multi-subjective ranking. Using a dataset of 280 self-contained code review requests and corresponding comments from CodeReview StackExchange, multiple human judges ranked the quality of ChatGPT-generated comments alongside the top human responses from the platform. Results show that ChatGPT's comments were ranked significantly better than human ones, even surpassing StackExchange's accepted answers. Going further, our proposed method motivates and enables more meaningful assessments of generative AI's performance in code review, while also raising awareness of potential risks of unchecked integration into review processes.

</details>


### [23] [How Do Semantically Equivalent Code Transformations Impact Membership Inference on LLMs for Code?](https://arxiv.org/abs/2512.15468)
*Hua Yang,Alejandro Velasco,Thanh Le-Cong,Md Nazmul Haque,Bowen Xu,Denys Poshyvanyk*

Main category: cs.SE

TL;DR: 本文研究语义等价代码变换对成员推断攻击检测的影响，发现变量重命名可显著降低检测成功率，暴露了代码大模型训练中许可合规的漏洞。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型依赖大量代码数据，可能包含受许可证限制的代码，需评估现有成员推断技术能否有效检测此类违规使用。

Method: 系统性测试多种语义等价代码变换规则对成员推断检测效果的影响，并通过因果分析验证变量重命名的主导作用。

Result: 单一变换（如变量重命名）可使MI成功率下降10.19%，但组合变换未进一步削弱检测效果；模型在变换后数据上微调性能仅下降1.5%。

Conclusion: 语义等价变换（尤其是变量重命名）能有效规避成员推断检测，揭示当前代码大模型训练许可合规机制存在严重漏洞。

Abstract: The success of large language models for code relies on vast amounts of code data, including public open-source repositories, such as GitHub, and private, confidential code from companies. This raises concerns about intellectual property compliance and the potential unauthorized use of license-restricted code. While membership inference (MI) techniques have been proposed to detect such unauthorized usage, their effectiveness can be undermined by semantically equivalent code transformation techniques, which modify code syntax while preserving semantic.
  In this work, we systematically investigate whether semantically equivalent code transformation rules might be leveraged to evade MI detection. The results reveal that model accuracy drops by only 1.5% in the worst case for each rule, demonstrating that transformed datasets can effectively serve as substitutes for fine-tuning. Additionally, we find that one of the rules (RenameVariable) reduces MI success by 10.19%, highlighting its potential to obscure the presence of restricted code. To validate these findings, we conduct a causal analysis confirming that variable renaming has the strongest causal effect in disrupting MI detection. Notably, we find that combining multiple transformations does not further reduce MI effectiveness. Our results expose a critical loophole in license compliance enforcement for training large language models for code, showing that MI detection can be substantially weakened by transformation-based obfuscation techniques.

</details>


### [24] [WuppieFuzz: Coverage-Guided, Stateful REST API Fuzzing](https://arxiv.org/abs/2512.15554)
*Thomas Rooijakkers,Anne Nijsten,Cristian Daniele,Erieke Weitenberg,Ringo Groenewegen,Arthur Melissen*

Main category: cs.SE

TL;DR: WuppieFuzz是一个基于LibAFL的开源REST API模糊测试工具，支持白盒、灰盒和黑盒测试，能自动生成请求序列并利用覆盖率引导优化测试路径。


<details>
  <summary>Details</summary>
Motivation: 为降低REST API因暴露端点带来的安全风险，需高效自动化测试技术，如模糊测试。

Method: 基于OpenAPI规范生成初始请求序列，结合REST专用与LibAFL变异器进行突变，并通过覆盖率反馈选择后续测试路径，自动构建测试环境。

Result: 在Petstore API上的评估表明，其白盒方法稳健，不同功率调度策略有效，且随时间推移能提升端点与代码覆盖率。

Conclusion: WuppieFuzz显著减少人工干预，提高REST API模糊测试效率与深度，是保障Web服务安全的有效工具。

Abstract: Many business processes currently depend on web services, often using REST APIs for communication. REST APIs expose web service functionality through endpoints, allowing easy client interaction over the Internet. To reduce the security risk resulting from exposed endpoints, thorough testing is desired. Due to the generally vast number of endpoints, automated testing techniques, like fuzzing, are of interest.
  This paper introduces WuppieFuzz, an open-source REST API fuzzer built on LibAFL, supporting white-box, grey-box and black-box fuzzing. Using an OpenAPI specification, it can generate an initial input corpus consisting of sequences of requests. These are mutated with REST-specific and LibAFL-provided mutators to explore different code paths in the software under test. Guided by the measured coverage, WuppieFuzz then selects which request sequences to send next to reach complex states in the software under test. In this process, it automates harness creation to reduce manual efforts often required in fuzzing. Different kinds of reporting are provided by the fuzzer to help fixing bugs.
  We evaluated our tool on the Petstore API to assess the robustness of the white-box approach and the effectiveness of different power schedules. We further monitored endpoint and code coverage over time to measure the efficacy of the approach.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [25] [FAME: FPGA Acceleration of Secure Matrix Multiplication with Homomorphic Encryption](https://arxiv.org/abs/2512.15515)
*Zhihan Xu,Rajgopal Kannan,Viktor K. Prasanna*

Main category: cs.AR

TL;DR: 本文提出了一种带宽高效的FPGA实现方案FAME，用于加速同态加密矩阵乘法，显著降低内存开销并提升221倍性能。


<details>
  <summary>Details</summary>
Motivation: 同态加密计算开销大，尤其矩阵乘法是瓶颈，阻碍其在隐私保护场景的实际应用。

Method: 构建成本模型分析片上内存需求，设计新型数据通路减少片外访问，实现支持任意矩阵形状的FPGA加速器FAME。

Result: 在Alveo U280上实现，相比CPU方案平均提速221倍，具备良好扩展性和实用性。

Conclusion: FAME有效解决了同态加密矩阵乘法的效率问题，为隐私保护机器学习等应用提供了实用化硬件加速方案。

Abstract: Homomorphic Encryption (HE) enables secure computation on encrypted data, addressing privacy concerns in cloud computing. However, the high computational cost of HE operations, particularly matrix multiplication (MM), remains a major barrier to its practical deployment. Accelerating homomorphic encrypted MM (HE MM) is therefore crucial for applications such as privacy-preserving machine learning.
  In this paper, we present a bandwidth-efficient FPGA implementation of HE MM. We first develop a cost model to evaluate the on-chip memory requirements for a given set of HE parameters and input matrix sizes. Our analysis shows that optimizing on-chip memory usage is critical for scalable and efficient HE MM. To this end, we design a novel datapath for Homomorphic Linear Transformation (HLT), the primary bottleneck in HE MM. The proposed datapath significantly reduces off-chip memory traffic and on-chip memory demand by enabling fine-grained data reuse. Leveraging this datapath, we introduce FAME, the first FPGA-based accelerator specifically tailored for HE MM. FAME supports arbitrary matrix shapes and is configurable across a wide range of HE parameter sets. We implement FAME on an Alveo U280 FPGA and evaluate its performance across diverse matrix sizes and shapes. Experimental results show that FAME achieves an average speedup of 221x over state-of-the-art CPU-based implementations, demonstrating its scalability and practicality for large-scale consecutive HE MM and real-world workloads.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [26] [DNS-based dynamic context resolution for SCHC](https://arxiv.org/abs/2512.15217)
*Antoine Bernard,Sandoche Balakrichenan,Michel Marot,Benoit Ampeau*

Main category: cs.NI

TL;DR: 本文提出了一种基于DNS的机制，用于动态获取与终端设备关联的SCHC上下文规则，并通过实验评估了该机制对通信延迟的影响。


<details>
  <summary>Details</summary>
Motivation: 由于LPWAN中终端设备种类繁多，静态存储上下文规则不具扩展性，因此需要一种动态获取规则的机制。

Method: 利用DNS查询定位HTTP服务器上的SCHC上下文规则文件，并下载至终端设备。

Result: 通过真实测试平台测量，验证了该机制在实际部署中的通信延迟表现。

Conclusion: 所提机制能有效支持异构终端设备的动态规则获取，同时保持可接受的通信延迟。

Abstract: LPWANs are networks characterised by the scarcity of their radio resources and their limited payload size. LoRaWAN offers an open, easy-to-deploy and efficient solution to operate a long-range network. To efficiently communicate using IPv6, the LPWAN working group from the IETF developed a solution called Static Context Header Compression (SCHC). It uses context rules, which are linked to a given End Device, to compress the IPv6 and UDP header. Since there may be a huge variety of End Devices profile, it makes sense to store the rules remotely and use a system to retrieve the profiles dynamically. In this paper we propose a mechanism based on DNS to find the context rules associated with an End Device, allowing it to be downloaded from an HTTP Server. We evaluate the corresponding delay added to the communications using experimental measurements from a real testbed.

</details>


### [27] [GenAI-enabled Residual Motion Estimation for Energy-Efficient Semantic Video Communication](https://arxiv.org/abs/2512.15481)
*Shavbo Salehi,Pedro Enrique Iturria-Rivera,Medhat Elsayed,Majid Bavand,Yigit Ozcan,Melike Erol-Kantarci*

Main category: cs.NI

TL;DR: 提出PENME方法，通过自适应选择运动估计模型和选择性扩散优化视频语义通信，显著降低延迟与带宽消耗并提升质量。


<details>
  <summary>Details</summary>
Motivation: 传统香农范式在视频传输中资源消耗高，语义通信可聚焦意义传输以节省资源。

Method: 采用五步策略动态选择运动提取模型，并结合LCM-4选择性扩散优化帧重建，同时感知信道状态分配无线资源。

Result: 在Vimeo90K数据集上，相比基线方法，延迟降低40%，传输数据减少90%，吞吐量提高35%，PSNR、MS-SSIM和LPIPS分别提升40%、19%和降低35%。

Conclusion: PENME有效提升视频语义通信效率与质量，兼顾低功耗与高保真。

Abstract: Semantic communication addresses the limitations of the Shannon paradigm by focusing on transmitting meaning rather than exact representations, thereby reducing unnecessary resource consumption. This is particularly beneficial for video, which dominates network traffic and demands high bandwidth and power, making semantic approaches ideal for conserving resources while maintaining quality. In this paper, we propose a Predictability-aware and Entropy-adaptive Neural Motion Estimation (PENME) method to address challenges related to high latency, high bitrate, and power consumption in video transmission. PENME makes per-frame decisions to select a residual motion extraction model, convolutional neural network, vision transformer, or optical flow, using a five-step policy based on motion strength, global motion consistency, peak sharpness, heterogeneity, and residual error. The residual motions are then transmitted to the receiver, where the frames are reconstructed via motion-compensated updates. Next, a selective diffusion-based refinement, the Latent Consistency Model (LCM-4), is applied on frames that trigger refinement due to low predictability or large residuals, while predictable frames skip refinement. PENME also allocates radio resource blocks with awareness of residual motion and channel state, reducing power consumption and bandwidth usage while maintaining high semantic similarity. Our simulation results on the Vimeo90K dataset demonstrate that the proposed PENME method handles various types of video, outperforming traditional communication, hybrid, and adaptive bitrate semantic communication techniques, achieving 40% lower latency, 90% less transmitted data, and 35% higher throughput. For semantic communication metrics, PENME improves PSNR by about 40%, increases MS-SSIM by roughly 19%, and reduces LPIPS by nearly 35%, compared with the baseline methods.

</details>
