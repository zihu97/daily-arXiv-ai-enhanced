{"id": "2509.21550", "categories": ["cs.NI", "cs.OS", "cs.PL", "C.2.2; C.2.3; D.3.3"], "pdf": "https://arxiv.org/pdf/2509.21550", "abs": "https://arxiv.org/abs/2509.21550", "authors": ["Pedro Mizuno", "Kimiya Mohammadtaheri", "Linfan Qian", "Joshua Johnson", "Danny Akbarzadeh", "Chris Neely", "Mario Baldi", "Nacihket Kapre", "Mina Tahmasbi Arashloo"], "title": "A Target-Agnostic Protocol-Independent Interface for the Transport Layer", "comment": null, "summary": "Transport protocols are fundamental to network communications, continuously\nevolving to meet the demands of new applications, workloads, and network\narchitectures while running in a wide range of execution environments (a.k.a\ntargets). We argue that this diversity across protocols and targets calls for a\nhigh-level, target-agnostic programming abstraction for the transport layer.\nSpecifically, we propose to specify transport protocols as high-level programs\nthat take an event and flow state as input, and using constrained C-like\nconstructs, produce the updated state along with target-agnostic instructions\nfor key transport operations such as data reassembly, packet generation and\nscheduling, and timer manipulations.\n  We show the benefits of our high-level transport programs by developing\nmultiple transport protocols in our programming framework called TINF,\ndeveloping two TINF- compliant backends, one in DPDK and one in Linux eXpress\nDataPath, and deploying TINF programs for multiple protocols across both\nbackends. Inspired by the benefits unlocked by L2/L3 packet-processing\nlanguages like P4, we believe target-agnostic transport programs can reduce the\ndevelopment effort for transport protocols, enable automated analysis and\nformal verification of the transport layer, and further research in\nprogrammable targets for transport protocols."}
{"id": "2509.21762", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2509.21762", "abs": "https://arxiv.org/abs/2509.21762", "authors": ["Ian McDougall", "Michael Davies", "Rahul Chatterjee", "Somesh Jha", "Karthikeyan Sankaralingam"], "title": "Privacy-Preserving Performance Profiling of In-The-Wild GPUs", "comment": "26 pages, 10 figures", "summary": "GPUs are the dominant platform for many important applications today\nincluding deep learning, accelerated computing, and scientific simulation.\nHowever, as the complexity of both applications and hardware increases, GPU\nchip manufacturers face a significant challenge: how to gather comprehensive\nperformance characteristics and value profiles from GPUs deployed in real-world\nscenarios. Such data, encompassing the types of kernels executed and the time\nspent in each, is crucial for optimizing chip design and enhancing application\nperformance. Unfortunately, despite the availability of low-level tools like\nNSYS and NCU, current methodologies fall short, offering data collection\ncapabilities only on an individual user basis rather than a broader, more\ninformative fleet-wide scale. This paper takes on the problem of realizing a\nsystem that allows planet-scale real-time GPU performance profiling of\nlow-level hardware characteristics. The three fundamental problems we solve\nare: i) user experience of achieving this with no slowdown; ii) preserving user\nprivacy, so that no 3rd party is aware of what applications any user runs; iii)\nefficacy in showing we are able to collect data and assign it applications even\nwhen run on 1000s of GPUs. Our results simulate a 100,000 size GPU deployment,\nrunning applications from the Torchbench suite, showing our system addresses\nall 3 problems."}
{"id": "2509.21490", "categories": ["cs.NI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.21490", "abs": "https://arxiv.org/abs/2509.21490", "authors": ["Md Sajid Islam", "Tanvir Hasan"], "title": "Context-Aware Hybrid Routing in Bluetooth Mesh Networks Using Multi-Model Machine Learning and AODV Fallback", "comment": "15 pages, 2 figures", "summary": "Bluetooth-based mesh networks offer a promising infrastructure for offline\ncommunication in emergency and resource constrained scenarios. However,\ntraditional routing strategies such as Ad hoc On-Demand Distance Vector (AODV)\noften degrade under congestion and dynamic topological changes. This study\nproposes a hybrid intelligent routing framework that augments AODV with\nsupervised machine learning to improve next-hop selection under varied network\nconstraints. The framework integrates four predictive models: a delivery\nsuccess classifier, a TTL regressor, a delay regressor, and a forwarder\nsuitability classifier, into a unified scoring mechanism that dynamically ranks\nneighbors during multi-hop message transmission. A simulation environment with\nstationary node deployments was developed, incorporating buffer constraints and\ndevice heterogeneity to evaluate three strategies: baseline AODV, a partial\nhybrid ML model (ABC), and the full hybrid ML model (ABCD). Across ten\nscenarios, the Hybrid ABCD model achieves approximately 99.97 percent packet\ndelivery under these controlled conditions, significantly outperforming both\nthe baseline and intermediate approaches. The results demonstrate that\nlightweight, explainable machine learning models can enhance routing\nreliability and adaptability in Bluetooth mesh networks, particularly in\ninfrastructure-less environments where delivery success is prioritized over\nlatency constraints."}
{"id": "2509.21789", "categories": ["cs.MA", "cs.CV"], "pdf": "https://arxiv.org/pdf/2509.21789", "abs": "https://arxiv.org/abs/2509.21789", "authors": ["Xinlei Yu", "Chengming Xu", "Guibin Zhang", "Yongbo He", "Zhangquan Chen", "Zhucun Xue", "Jiangning Zhang", "Yue Liao", "Xiaobin Hu", "Yu-Gang Jiang", "Shuicheng Yan"], "title": "Visual Multi-Agent System: Mitigating Hallucination Snowballing via Visual Flow", "comment": null, "summary": "Multi-Agent System (MAS) powered by Visual Language Models (VLMs) enables\nchallenging tasks but suffers from a novel failure term, multi-agent visual\nhallucination snowballing, where hallucinations are seeded in a single agent\nand amplified by following ones due to the over-reliance on textual flow to\nrelay visual information. Through turn-, layer-, and token-wise attention\nanalyses, we provide detailed insights into the essence of hallucination\nsnowballing regarding the reduction of visual attention allocation. It leads us\nto identify a subset of vision tokens with a unimodal attention peak in middle\nlayers that best preserve visual evidence but gradually diminish in deeper\nagent turns, resulting in the visual hallucination snowballing in MAS. Thus, we\npropose ViF, a lightweight, plug-and-play mitigation paradigm that relays\ninter-agent messages with Visual Flow powered by the selected visual relay\ntokens and applies attention reallocation to amplify this pattern. The\nexperiment results demonstrate that our method markedly reduces hallucination\nsnowballing, consistently improving the performance across eight benchmarks\nbased on four common MAS structures and ten base models. The source code will\nbe available at: https://github.com/YU-deep/ViF.git."}
{"id": "2509.21693", "categories": ["cs.PF"], "pdf": "https://arxiv.org/pdf/2509.21693", "abs": "https://arxiv.org/abs/2509.21693", "authors": ["Runhan Xie", "Esa Hyyti√§", "Rhonda Righter"], "title": "Size-Aware Dispatching to Fluid Queues", "comment": null, "summary": "We develop a fluid-flow model for routing problems, where fluid consists of\ndifferent size particles and the task is to route the incoming fluid to $n$\nparallel servers using the size information in order to minimize the mean\nlatency. The problem corresponds to the dispatching problem of (discrete) jobs\narriving according to a stochastic process. In the fluid model the problem\nreduces to finding an optimal path to empty the system in $n$-dimensional\nspace. We use the calculus of variation to characterize the structure of\noptimal policies. Numerical examples shed further light on the fluid routing\nproblem and the optimal control of large distributed service systems."}
{"id": "2509.21427", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.21427", "abs": "https://arxiv.org/abs/2509.21427", "authors": ["Ying Wang", "Wenjun Mao", "Chong Wang", "Zhenhao Zhou", "Yicheng Zhou", "Wenyun Zhao", "Yiling Lou", "Xin Peng"], "title": "Extracting Conceptual Knowledge to Locate Software Issues", "comment": null, "summary": "Issue localization, which identifies faulty code elements such as files or\nfunctions, is critical for effective bug fixing. While recent LLM-based and\nLLM-agent-based approaches improve accuracy, they struggle in large-scale\nrepositories due to concern mixing, where relevant logic is buried in large\nfunctions, and concern scattering, where related logic is dispersed across\nfiles.\n  To address these challenges, we propose RepoLens, a novel approach that\nabstracts and leverages conceptual knowledge from code repositories. RepoLens\ndecomposes fine-grained functionalities and recomposes them into high-level\nconcerns, semantically coherent clusters of functionalities that guide LLMs. It\noperates in two stages: an offline stage that extracts and enriches conceptual\nknowledge into a repository-wide knowledge base, and an online stage that\nretrieves issue-specific terms, clusters and ranks concerns by relevance, and\nintegrates them into localization workflows via minimally intrusive prompt\nenhancements. We evaluate RepoLens on SWE-Lancer-Loc, a benchmark of 216 tasks\nderived from SWE-Lancer. RepoLens consistently improves three state-of-the-art\ntools, namely AgentLess, OpenHands, and mini-SWE-agent, achieving average gains\nof over 22% in Hit@k and 46% in Recall@k for file- and function-level\nlocalization. It generalizes across models (GPT-4o, GPT-4o-mini, GPT-4.1) with\nHit@1 and Recall@10 gains up to 504% and 376%, respectively. Ablation studies\nand manual evaluation confirm the effectiveness and reliability of the\nconstructed concerns."}
{"id": "2509.21527", "categories": ["cs.DC", "cs.PF", "physics.comp-ph"], "pdf": "https://arxiv.org/pdf/2509.21527", "abs": "https://arxiv.org/abs/2509.21527", "authors": ["Mahesh Doijade", "Andrey Alekseenko", "Ania Brown", "Alan Gray", "Szil√°rd P√°ll"], "title": "Redesigning GROMACS Halo Exchange: Improving Strong Scaling with GPU-initiated NVSHMEM", "comment": "17 pages, 8 figures, submitted to PAW-ATM Workshop, SC 2025", "summary": "Improving time-to-solution in molecular dynamics simulations often requires\nstrong scaling due to fixed-sized problems. GROMACS is highly\nlatency-sensitive, with peak iteration rates in the sub-millisecond, making\nscalability on heterogeneous supercomputers challenging. MPI's CPU-centric\nnature introduces additional latencies on GPU-resident applications' critical\npath, hindering GPU utilization and scalability. To address these limitations,\nwe present an NVSHMEM-based GPU kernel-initiated redesign of the GROMACS domain\ndecomposition halo-exchange algorithm. Highly tuned GPU kernels fuse data\npacking and communication, leveraging hardware latency-hiding for fine-grained\noverlap. We employ kernel fusion across overlapped data forwarding\ncommunication phases and utilize the asynchronous copy engine over NVLink to\noptimize latency and bandwidth. Our GPU-resident formulation greatly increases\ncommunication-computation overlap, improving GROMACS strong scaling performance\nacross NVLink by up to 1.5x (intra-node) and 2x (multi-node), and up to 1.3x\nmulti-node over NVLink+InfiniBand. This demonstrates the profound benefits of\nGPU-initiated communication for strong-scaling a broad range of\nlatency-sensitive applications."}
{"id": "2509.22410", "categories": ["cs.AR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.22410", "abs": "https://arxiv.org/abs/2509.22410", "authors": ["Shayne Wadle", "Yanxin Zhang", "Vikas Singh", "Karthikeyan Sankaralingam"], "title": "NeuroScalar: A Deep Learning Framework for Fast, Accurate, and In-the-Wild Cycle-Level Performance Prediction", "comment": null, "summary": "The evaluation of new microprocessor designs is constrained by slow,\ncycle-accurate simulators that rely on unrepresentative benchmark traces. This\npaper introduces a novel deep learning framework for high-fidelity,\n``in-the-wild'' simulation on production hardware. Our core contribution is a\nDL model trained on microarchitecture-independent features to predict\ncycle-level performance for hypothetical processor designs. This unique\napproach allows the model to be deployed on existing silicon to evaluate future\nhardware. We propose a complete system featuring a lightweight hardware trace\ncollector and a principled sampling strategy to minimize user impact. This\nsystem achieves a simulation speed of 5 MIPS on a commodity GPU, imposing a\nmere 0.1% performance overhead. Furthermore, our co-designed Neutrino on-chip\naccelerator improves performance by 85x over the GPU. We demonstrate that this\nframework enables accurate performance analysis and large-scale hardware A/B\ntesting on a massive scale using real-world applications."}
{"id": "2509.21550", "categories": ["cs.NI", "cs.OS", "cs.PL", "C.2.2; C.2.3; D.3.3"], "pdf": "https://arxiv.org/pdf/2509.21550", "abs": "https://arxiv.org/abs/2509.21550", "authors": ["Pedro Mizuno", "Kimiya Mohammadtaheri", "Linfan Qian", "Joshua Johnson", "Danny Akbarzadeh", "Chris Neely", "Mario Baldi", "Nacihket Kapre", "Mina Tahmasbi Arashloo"], "title": "A Target-Agnostic Protocol-Independent Interface for the Transport Layer", "comment": null, "summary": "Transport protocols are fundamental to network communications, continuously\nevolving to meet the demands of new applications, workloads, and network\narchitectures while running in a wide range of execution environments (a.k.a\ntargets). We argue that this diversity across protocols and targets calls for a\nhigh-level, target-agnostic programming abstraction for the transport layer.\nSpecifically, we propose to specify transport protocols as high-level programs\nthat take an event and flow state as input, and using constrained C-like\nconstructs, produce the updated state along with target-agnostic instructions\nfor key transport operations such as data reassembly, packet generation and\nscheduling, and timer manipulations.\n  We show the benefits of our high-level transport programs by developing\nmultiple transport protocols in our programming framework called TINF,\ndeveloping two TINF- compliant backends, one in DPDK and one in Linux eXpress\nDataPath, and deploying TINF programs for multiple protocols across both\nbackends. Inspired by the benefits unlocked by L2/L3 packet-processing\nlanguages like P4, we believe target-agnostic transport programs can reduce the\ndevelopment effort for transport protocols, enable automated analysis and\nformal verification of the transport layer, and further research in\nprogrammable targets for transport protocols."}
{"id": "2509.21834", "categories": ["cs.MA"], "pdf": "https://arxiv.org/pdf/2509.21834", "abs": "https://arxiv.org/abs/2509.21834", "authors": ["Shengxiang Xu", "Jiayi Zhang", "Shimin Di", "Yuyu Luo", "Liang Yao", "Hanmo Liu", "Jia Zhu", "Fan Liu", "Min-Ling Zhang"], "title": "RobustFlow: Towards Robust Agentic Workflow Generation", "comment": null, "summary": "The automated generation of agentic workflows is a promising frontier for\nenabling large language models (LLMs) to solve complex tasks. However, our\ninvestigation reveals that the robustness of agentic workflow remains a\ncritical, unaddressed challenge. Current methods often generate wildly\ninconsistent workflows when provided with instructions that are semantically\nidentical but differently phrased. This brittleness severely undermines their\nreliability and trustworthiness for real-world applications. To quantitatively\ndiagnose this instability, we propose metrics based on nodal and topological\nsimilarity to evaluate workflow consistency against common semantic variations\nsuch as paraphrasing and noise injection. Subsequently, we further propose a\nnovel training framework, RobustFlow, that leverages preference optimization to\nteach models invariance to instruction variations. By training on sets of\nsynonymous task descriptions, RobustFlow boosts workflow robustness scores to\n70\\% - 90\\%, which is a substantial improvement over existing approaches. The\ncode is publicly available at https://github.com/DEFENSE-SEU/RobustFlow."}
{"id": "2509.22405", "categories": ["cs.PF", "cs.AR"], "pdf": "https://arxiv.org/pdf/2509.22405", "abs": "https://arxiv.org/abs/2509.22405", "authors": ["Shayne Wadle", "Karthikeyan Sankaralingam"], "title": "SAHM: State-Aware Heterogeneous Multicore for Single-Thread Performance", "comment": null, "summary": "Improving single-thread performance remains a critical challenge in modern\nprocessor design, as conventional approaches such as deeper speculation, wider\npipelines, and complex out-of-order execution face diminishing returns. This\nwork introduces SAHM-State-Aware Heterogeneous Multicore-a novel architecture\nthat targets performance gains by exploiting fine-grained, time-varying\nbehavioral diversity in single-threaded workloads. Through empirical\ncharacterization of performance counter data, we define 16 distinct behavioral\nstates representing different microarchitectural demands. Rather than\nover-provisioning a monolithic core with all optimizations, SAHM uses a set of\nspecialized cores tailored to specific states and migrates threads at runtime\nbased on detected behavior. This design enables composable microarchitectural\nenhancements without incurring prohibitive area, power, or complexity costs.\n  We evaluate SAHM in both single-threaded and multiprogrammed scenarios,\ndemonstrating its ability to maintain core utilization while improving overall\nperformance through intelligent state-driven scheduling. Experimental results\nshow opportunity for 17% speed up in realistic scenarios. These speed ups are\nrobust against high-cost migration, decreasing by less than 1%. Overall,\nstate-aware core specialization is a new path forward for enhancing\nsingle-thread performance."}
{"id": "2509.21533", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.21533", "abs": "https://arxiv.org/abs/2509.21533", "authors": ["Shalini Chakraborty", "Sebastian Baltes"], "title": "Lost in Transition: The Struggle of Women Returning to Software Engineering Research after Career Breaks", "comment": "3 pages, published in the Proceedings of the 18th International\n  Conference on Cooperative and Human Aspects of Software Engineering (CHASE\n  2025)", "summary": "The IT industry provides supportive pathways such as returnship programs,\ncoding boot camps, and buddy systems for women re-entering their job after a\ncareer break. Academia, however, offers limited opportunities to motivate women\nto return. We propose a diverse multicultural research project investigating\nthe challenges faced by women with software engineering (SE) backgrounds\nre-entering academia or related research roles after a career break. Career\ndisruptions due to pregnancy, immigration status, or lack of flexible work\noptions can significantly impact women's career progress, creating barriers for\nreturning as lecturers, professors, or senior researchers. Although many\ncompanies promote gender diversity policies, such measures are less prominent\nand often under-recognized within academic institutions. Our goal is to explore\nthe specific challenges women encounter when re-entering academic roles\ncompared to industry roles; to understand the institutional perspective,\nincluding a comparative analysis of existing policies and opportunities in\ndifferent countries for women to return to the field; and finally, to provide\nrecommendations that support transparent hiring practices. The research project\nwill be carried out in multiple universities and in multiple countries to\ncapture the diverse challenges and policies that vary by location."}
{"id": "2509.21841", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2509.21841", "abs": "https://arxiv.org/abs/2509.21841", "authors": ["Chang Chen", "Tiancheng Chen", "Jiangfei Duan", "Qianchao Zhu", "Zerui Wang", "Qinghao Hu", "Peng Sun", "Xiuhong Li", "Chao Yang", "Torsten Hoefler"], "title": "Zeppelin: Balancing Variable-length Workloads in Data Parallel Large Model Training", "comment": null, "summary": "Training large language models (LLMs) with increasingly long and varying\nsequence lengths introduces severe load imbalance challenges in large-scale\ndata-parallel training. Recent frameworks attempt to mitigate these issues\nthrough data reorganization or hybrid parallel strategies. However, they often\noverlook how computational and communication costs scale with sequence length,\nresulting in suboptimal performance. We identify three critical challenges: (1)\nvarying computation-to-communication ratios across sequences of different\nlengths in distributed attention, (2) mismatch between static NIC-GPU affinity\nand dynamic parallel workloads, and (3) distinct optimal partitioning\nstrategies required for quadratic attention versus linear components. To\naddress these challenges, we present Zeppelin, a novel training system that\nintegrates three key techniques: (1) a hierarchical sequence partitioning\nmethod for the attention module that reduces communication overhead and\nbalances computation, supported by an efficient attention engine that applies\ndivergent parallel strategies; (2) a routing layer that orchestrates inter-node\ntransfers to fully utilize NIC bandwidth; and (3) a remapping layer that\ntransforms sequence layouts between attention and linear modules, ensuring high\ncomputational efficiency across both. Comprehensive evaluations across diverse\nconfigurations show that Zeppelin delivers an average 2.80x speedup over\nstate-of-the-art methods."}
{"id": "2509.22512", "categories": ["cs.AR", "n/a"], "pdf": "https://arxiv.org/pdf/2509.22512", "abs": "https://arxiv.org/abs/2509.22512", "authors": ["Soroush Ahadi", "Mehdi Modarressi", "Masoud Daneshtalab"], "title": "AxLLM: accelerator architecture for large language models with computation reuse capability", "comment": "7 pages, 9 figures", "summary": "Large language models demand massive computational power and memory\nresources, posing significant challenges for efficient deployment. While\nquantization has been widely explored to reduce model size and computation,\nthis paper demonstrates an additional benefit: quantization increases parameter\nlocality, creating opportunities for computation reuse. Building on this\ninsight, we propose AxLLM, a hardware accelerator architecture designed for\nquantized models. Axllm introduces a novel redundancy elimination technique\nthat caches and reuses multiplication results for repeated weight values,\nsubstantially reducing redundant operations. The architecture features dual\nmultiply and reuse pipelines, efficiently supporting both base models and LoRA\nfine-tuned models without altering parameters, retraining, or requiring offline\npreprocessing. Experimental results show that AxLLM achieves up to 90%\nreduction in computations, delivering 28% lower energy consumption and a 1.7x\nspeedup over baseline execution. These results highlight Axllm as a scalable\nand efficient solution for accelerating LLMs on specialized hardware."}
{"id": "2509.21649", "categories": ["cs.NI"], "pdf": "https://arxiv.org/pdf/2509.21649", "abs": "https://arxiv.org/abs/2509.21649", "authors": ["Yeison Stiven Murcia", "Oscar Mauricio Caicedo", "Daniela Maria Casas", "Nelson Luis Saldanha da Fonseca"], "title": "eXplainable Artificial Intelligence for RL-based Networking Solutions", "comment": null, "summary": "Reinforcement Learning (RL) agents have been widely used to improve\nnetworking tasks. However, understanding the decisions made by these agents is\nessential for their broader adoption in networking and network management. To\naddress this, we introduce eXplaNet - a pipeline grounded in explainable\nartificial intelligence - designed to help networking researchers and\npractitioners gain deeper insights into the decision-making processes of\nRL-based solutions. We demonstrate how eXplaNet can be applied to refine a\nrouting solution powered by a Q-learning agent, specifically by improving its\nreward function. In addition, we discuss the opportunities and challenges of\nincorporating explainability into RL to better optimize network performance."}
{"id": "2509.22130", "categories": ["cs.MA", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.22130", "abs": "https://arxiv.org/abs/2509.22130", "authors": ["Merve Atasever", "Matthew Hong", "Mihir Nitin Kulkarni", "Qingpei Li", "Jyotirmoy V. Deshmukh"], "title": "Multi-Agent Path Finding via Offline RL and LLM Collaboration", "comment": null, "summary": "Multi-Agent Path Finding (MAPF) poses a significant and challenging problem\ncritical for applications in robotics and logistics, particularly due to its\ncombinatorial complexity and the partial observability inherent in realistic\nenvironments. Decentralized reinforcement learning methods commonly encounter\ntwo substantial difficulties: first, they often yield self-centered behaviors\namong agents, resulting in frequent collisions, and second, their reliance on\ncomplex communication modules leads to prolonged training times, sometimes\nspanning weeks. To address these challenges, we propose an efficient\ndecentralized planning framework based on the Decision Transformer (DT),\nuniquely leveraging offline reinforcement learning to substantially reduce\ntraining durations from weeks to mere hours. Crucially, our approach\neffectively handles long-horizon credit assignment and significantly improves\nperformance in scenarios with sparse and delayed rewards. Furthermore, to\novercome adaptability limitations inherent in standard RL methods under dynamic\nenvironmental changes, we integrate a large language model (GPT-4o) to\ndynamically guide agent policies. Extensive experiments in both static and\ndynamically changing environments demonstrate that our DT-based approach,\naugmented briefly by GPT-4o, significantly enhances adaptability and\nperformance."}
{"id": "2509.21527", "categories": ["cs.DC", "cs.PF", "physics.comp-ph"], "pdf": "https://arxiv.org/pdf/2509.21527", "abs": "https://arxiv.org/abs/2509.21527", "authors": ["Mahesh Doijade", "Andrey Alekseenko", "Ania Brown", "Alan Gray", "Szil√°rd P√°ll"], "title": "Redesigning GROMACS Halo Exchange: Improving Strong Scaling with GPU-initiated NVSHMEM", "comment": "17 pages, 8 figures, submitted to PAW-ATM Workshop, SC 2025", "summary": "Improving time-to-solution in molecular dynamics simulations often requires\nstrong scaling due to fixed-sized problems. GROMACS is highly\nlatency-sensitive, with peak iteration rates in the sub-millisecond, making\nscalability on heterogeneous supercomputers challenging. MPI's CPU-centric\nnature introduces additional latencies on GPU-resident applications' critical\npath, hindering GPU utilization and scalability. To address these limitations,\nwe present an NVSHMEM-based GPU kernel-initiated redesign of the GROMACS domain\ndecomposition halo-exchange algorithm. Highly tuned GPU kernels fuse data\npacking and communication, leveraging hardware latency-hiding for fine-grained\noverlap. We employ kernel fusion across overlapped data forwarding\ncommunication phases and utilize the asynchronous copy engine over NVLink to\noptimize latency and bandwidth. Our GPU-resident formulation greatly increases\ncommunication-computation overlap, improving GROMACS strong scaling performance\nacross NVLink by up to 1.5x (intra-node) and 2x (multi-node), and up to 1.3x\nmulti-node over NVLink+InfiniBand. This demonstrates the profound benefits of\nGPU-initiated communication for strong-scaling a broad range of\nlatency-sensitive applications."}
{"id": "2509.21816", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.21816", "abs": "https://arxiv.org/abs/2509.21816", "authors": ["Yuhang Xie", "Jian Mu", "Xiaojun Ma", "Chaoyun Zhang", "Lu Wang", "Mengyu Zhou", "Mugeng Liu", "Si Qin", "Qingwei Lin", "Saravan Rajmohan", "Shi Han", "Dongmei Zhang"], "title": "No More Manual Guides: Automatic and Scalable Generation of High-Quality Excel Tutorials", "comment": null, "summary": "Excel is one of the most widely used productivity tools across domains,\noffering rich functionality but also overwhelming users with its complexity.\nThis creates a persistent demand for tutorials to support effective usage.\nHowever, existing tutorials are manually authored by experts, require frequent\nupdates after each software release, and incur substantial labor costs. Prior\nwork has not achieved fully automated tutorial generation, since existing\nmethods still depend on handcrafted operation sequences or example materials.\nIn this paper, we present the first framework for automatically generating\nExcel tutorials directly from natural language task descriptions. Our framework\nfirst instantiates the task. Then a central component of this framework,\nExecution Agent, plans and executes the solution in Excel, and collects the\nintermediate artifacts required for tutorial construction. These artifacts are\nthen transformed into both structured Excel documents and video demonstrations.\nTo build a comprehensive tutorial corpus, we collected 1,559 task descriptions\nfrom real-world scenarios. In addition, we designed a systematic evaluation\nframework that integrates assessments from both large language models (LLMs)\nand human reviewers. Experimental results show that our framework improves task\nexecution success rates by 8.5% over state-of-the-art baselines. Moreover, the\ngenerated tutorials demonstrate superior readability and instructional\neffectiveness, often approaching or surpassing expert-authored materials.\nImportantly, the automated pipeline eliminates manual labor and reduces time\ncosts to 1/20 of expert authoring, making scalable and high-quality tutorial\ngeneration practical for the first time."}
{"id": "2509.22068", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2509.22068", "abs": "https://arxiv.org/abs/2509.22068", "authors": ["Sebastian Werner", "Mathis K√§hler", "Alireza Hakamian"], "title": "Code once, Run Green: Automated Green Code Translation in Serverless Computing", "comment": "Accepted at IC2E 2025", "summary": "The rapid digitization and the increasing use of emerging technologies such\nas AI models have significantly contributed to the emissions of computing\ninfrastructure. Efforts to mitigate this impact typically focus on the\ninfrastructure level such as powering data centers with renewable energy, or\nthrough the specific design of energy-efficient software. However, both\nstrategies rely on stakeholder intervention, making their adoption in legacy\nand already-deployed systems unlikely. As a result, past architectural and\nimplementation decisions continue to incur additional energy usage - a\nphenomenon we refer to as energy debt.\n  Hence, in this paper, we investigate the potential of serverless computing\nplatforms to automatically reduce energy debt by leveraging the unique access\nto function source code. Specifically, we explore whether large language models\n(LLMs) can translate serverless functions into more energy-efficient\nprogramming languages while preserving functional correctness. To this end, we\ndesign and implement ReFaaS and integrate it into the Fission serverless\nframework. We evaluate multiple LLMs on their ability to perform such code\ntranslations and analyze their impact on energy consumption.\n  Our preliminary results indicate that translated functions can reduce\ninvocation energy by up to 70%, achieving net energy savings after\napproximately 3,000 to 5,000 invocations, depending on the LLM used.\nNonetheless, the approach faces several challenges: not all functions are\nsuitable for translation, and for some, the amortization threshold is\nsignificantly higher or unreachable. Despite these limitations, we identify\nfour key research challenges whose resolution could unlock long-term, automated\nmitigation of energy debt in serverless computing."}
{"id": "2509.22405", "categories": ["cs.PF", "cs.AR"], "pdf": "https://arxiv.org/pdf/2509.22405", "abs": "https://arxiv.org/abs/2509.22405", "authors": ["Shayne Wadle", "Karthikeyan Sankaralingam"], "title": "SAHM: State-Aware Heterogeneous Multicore for Single-Thread Performance", "comment": null, "summary": "Improving single-thread performance remains a critical challenge in modern\nprocessor design, as conventional approaches such as deeper speculation, wider\npipelines, and complex out-of-order execution face diminishing returns. This\nwork introduces SAHM-State-Aware Heterogeneous Multicore-a novel architecture\nthat targets performance gains by exploiting fine-grained, time-varying\nbehavioral diversity in single-threaded workloads. Through empirical\ncharacterization of performance counter data, we define 16 distinct behavioral\nstates representing different microarchitectural demands. Rather than\nover-provisioning a monolithic core with all optimizations, SAHM uses a set of\nspecialized cores tailored to specific states and migrates threads at runtime\nbased on detected behavior. This design enables composable microarchitectural\nenhancements without incurring prohibitive area, power, or complexity costs.\n  We evaluate SAHM in both single-threaded and multiprogrammed scenarios,\ndemonstrating its ability to maintain core utilization while improving overall\nperformance through intelligent state-driven scheduling. Experimental results\nshow opportunity for 17% speed up in realistic scenarios. These speed ups are\nrobust against high-cost migration, decreasing by less than 1%. Overall,\nstate-aware core specialization is a new path forward for enhancing\nsingle-thread performance."}
{"id": "2509.21656", "categories": ["cs.NI"], "pdf": "https://arxiv.org/pdf/2509.21656", "abs": "https://arxiv.org/abs/2509.21656", "authors": ["Max Schr√∂tter", "Sten Heimbrodt", "Bettina Schnor"], "title": "XenoFlow: How Fast Can a SmartNIC-Based DNS Load Balancer Run?", "comment": null, "summary": "With the advent of programmable network hardware, more and more\n  functionality can be moved from software running on general purpose CPUs to\n  the NIC. Early NICs only allowed offloading fixed functions like checksum\n  computation. Recent NICs like the Nvidia Bluefield-3 allow a fully\n  programmable dataplane. In this paper, we present our first steps towards a\n  load balancer named XenoFlow running on the Bluefield-3. Furthermore, we\n  show the capabilities and limitations of the Bluefield-3 eSwitch. Our\n  results show that the Bluefield-3 will not achieve line rate with only 2\n  entries in a Flow Pipe. However, we also show the adventages of hardware\n  offloading on the NIC and being closer to the network. With XenoFlow, we\n  achieve an 44% lower latency compared to a comparable eBPF-based load\n  balancer running on the host. Furthermore, XenoFlow achieves this low\n  latency even under high load."}
{"id": "2509.22216", "categories": ["cs.MA", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.22216", "abs": "https://arxiv.org/abs/2509.22216", "authors": ["Ahmet Onur Akman", "Anastasia Psarou", "Zolt√°n Gy√∂rgy Varga", "Grzegorz Jamr√≥z", "Rafa≈Ç Kucharski"], "title": "Impact of Collective Behaviors of Autonomous Vehicles on Urban Traffic Dynamics: A Multi-Agent Reinforcement Learning Approach", "comment": "Work presented at the European Workshop on Reinforcement Learning\n  (EWRL 2024)", "summary": "This study examines the potential impact of reinforcement learning\n(RL)-enabled autonomous vehicles (AV) on urban traffic flow in a mixed traffic\nenvironment. We focus on a simplified day-to-day route choice problem in a\nmulti-agent setting. We consider a city network where human drivers travel\nthrough their chosen routes to reach their destinations in minimum travel time.\nThen, we convert one-third of the population into AVs, which are RL agents\nemploying Deep Q-learning algorithm. We define a set of optimization targets,\nor as we call them behaviors, namely selfish, collaborative, competitive,\nsocial, altruistic, and malicious. We impose a selected behavior on AVs through\ntheir rewards. We run our simulations using our in-house developed RL framework\nPARCOUR. Our simulations reveal that AVs optimize their travel times by up to\n5\\%, with varying impacts on human drivers' travel times depending on the AV\nbehavior. In all cases where AVs adopt a self-serving behavior, they achieve\nshorter travel times than human drivers. Our findings highlight the complexity\ndifferences in learning tasks of each target behavior. We demonstrate that the\nmulti-agent RL setting is applicable for collective routing on traffic\nnetworks, though their impact on coexisting parties greatly varies with the\nbehaviors adopted."}
{"id": "2509.21881", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.21881", "abs": "https://arxiv.org/abs/2509.21881", "authors": ["Chaman Wijesiriwardana", "Prasad Wimalaratne"], "title": "Software Engineering Data Analytics: A Framework Based on a Multi-Layered Abstraction Mechanism", "comment": null, "summary": "This paper presents a concept of a domain-specific framework for software\nanalytics by enabling querying, modeling, and integration of heterogeneous\nsoftware repositories. The framework adheres to a multi-layered abstraction\nmechanism that consists of domain-specific operators. We showcased the\npotential of this approach by employing a case study."}
{"id": "2509.22117", "categories": ["cs.DC", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.22117", "abs": "https://arxiv.org/abs/2509.22117", "authors": ["Lucio Anderlini", "Giulio Bianchini", "Diego Ciangottini", "Stefano Dal Pra", "Diego Michelotto", "Rosa Petrini", "Daniele Spiga"], "title": "The AI_INFN Platform: Artificial Intelligence Development in the Cloud", "comment": "To be published in SciPost Physics Proceedings for European AI for\n  Fundamental Physics Conference (EuCAIFCon 2025)", "summary": "Machine Learning (ML) is driving a revolution in the way scientists design,\ndevelop, and deploy data-intensive software. However, the adoption of ML\npresents new challenges for the computing infrastructure, particularly in terms\nof provisioning and orchestrating access to hardware accelerators for\ndevelopment, testing, and production. The INFN-funded project AI_INFN\n(Artificial Intelligence at INFN) aims at fostering the adoption of ML\ntechniques within INFN use cases by providing support on multiple aspects,\nincluding the provisioning of AI-tailored computing resources. It leverages\ncloud-native solutions in the context of INFN Cloud, to share hardware\naccelerators as effectively as possible, ensuring the diversity of the\nInstitute's research activities is not compromised. In this contribution, we\nprovide an update on the commissioning of a Kubernetes platform designed to\nease the development of GPU-powered data analysis workflows and their\nscalability on heterogeneous distributed computing resources, also using the\noffloading mechanism with Virtual Kubelet and InterLink API. This setup can\nmanage workflows across different resource providers, including sites of the\nWorldwide LHC Computing Grid and supercomputers such as CINECA Leonardo,\nproviding a model for use cases requiring dedicated infrastructures for\ndifferent parts of the workload. Initial test results, emerging case studies,\nand integration scenarios will be presented with functional tests and\nbenchmarks."}
{"id": "2509.21949", "categories": ["cs.NI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2509.21949", "abs": "https://arxiv.org/abs/2509.21949", "authors": ["Arina Caraus", "Alessio Buscemi", "Sumit Kumar", "Ion Turcanu"], "title": "Evaluating Open-Source Large Language Models for Technical Telecom Question Answering", "comment": "Accepted at the IEEE GLOBECOM Workshops 2025: \"Large AI Model over\n  Future Wireless Networks\"", "summary": "Large Language Models (LLMs) have shown remarkable capabilities across\nvarious fields. However, their performance in technical domains such as\ntelecommunications remains underexplored. This paper evaluates two open-source\nLLMs, Gemma 3 27B and DeepSeek R1 32B, on factual and reasoning-based questions\nderived from advanced wireless communications material. We construct a\nbenchmark of 105 question-answer pairs and assess performance using lexical\nmetrics, semantic similarity, and LLM-as-a-judge scoring. We also analyze\nconsistency, judgment reliability, and hallucination through source attribution\nand score variance. Results show that Gemma excels in semantic fidelity and\nLLM-rated correctness, while DeepSeek demonstrates slightly higher lexical\nconsistency. Additional findings highlight current limitations in telecom\napplications and the need for domain-adapted models to support trustworthy\nArtificial Intelligence (AI) assistants in engineering."}
{"id": "2509.22218", "categories": ["cs.MA", "cs.AI", "cs.DB"], "pdf": "https://arxiv.org/pdf/2509.22218", "abs": "https://arxiv.org/abs/2509.22218", "authors": ["Sandaru Fernando", "Imasha Jayarathne", "Sithumini Abeysekara", "Shanuja Sithamparanthan", "Thushari Silva", "Deshan Jayawardana"], "title": "VizGen: Data Exploration and Visualization from Natural Language via a Multi-Agent AI Architecture", "comment": null, "summary": "Data visualization is essential for interpreting complex datasets, yet\ntraditional tools often require technical expertise, limiting accessibility.\nVizGen is an AI-assisted graph generation system that empowers users to create\nmeaningful visualizations using natural language. Leveraging advanced NLP and\nLLMs like Claude 3.7 Sonnet and Gemini 2.0 Flash, it translates user queries\ninto SQL and recommends suitable graph types. Built on a multi-agent\narchitecture, VizGen handles SQL generation, graph creation, customization, and\ninsight extraction. Beyond visualization, it analyzes data for patterns,\nanomalies, and correlations, and enhances user understanding by providing\nexplanations enriched with contextual information gathered from the internet.\nThe system supports real-time interaction with SQL databases and allows\nconversational graph refinement, making data analysis intuitive and accessible.\nVizGen democratizes data visualization by bridging the gap between technical\ncomplexity and user-friendly design."}
{"id": "2509.21891", "categories": ["cs.SE", "cs.CL"], "pdf": "https://arxiv.org/pdf/2509.21891", "abs": "https://arxiv.org/abs/2509.21891", "authors": ["Yangtian Zi", "Zixuan Wu", "Aleksander Boruch-Gruszecki", "Jonathan Bell", "Arjun Guha"], "title": "AgentPack: A Dataset of Code Changes, Co-Authored by Agents and Humans", "comment": null, "summary": "Fine-tuning large language models for code editing has typically relied on\nmining commits and pull requests. The working hypothesis has been that commit\nmessages describe human intent in natural language, and patches to code\ndescribe the changes that implement that intent. However, much of the\npreviously collected data is noisy: commit messages are terse, human-written\ncommits commingle several unrelated edits, and many commits come from simple,\nrule-based bots.\n  The recent adoption of software engineering agents changes this landscape.\nCode changes co-authored by humans and agents tend to be more narrowly scoped\nand focused on clearer goals. Their commit messages, generated by LLMs,\narticulate intent and rationale in much greater detail. Moreover, when these\nchanges land in public repositories, they are implicitly filtered by humans:\nmaintainers discard low-quality commits to their projects.\n  We present AgentPack, a corpus of 1.3M code edits co-authored by Claude Code,\nOpenAI Codex, and Cursor Agent across public GitHub projects up to mid-August\n2025. We describe the identification and curation pipeline, quantify adoption\ntrends of these agents, and analyze the structural properties of the edits.\nFinally, we show that models fine-tuned on AgentPack can outperform models\ntrained on prior human-only commit corpora, highlighting the potential of using\npublic data from software engineering agents to train future code-editing\nmodels."}
{"id": "2509.22233", "categories": ["cs.DC", "cs.DS"], "pdf": "https://arxiv.org/pdf/2509.22233", "abs": "https://arxiv.org/abs/2509.22233", "authors": ["Thomas Boudier", "Filippo Casagrande", "Avinandan Das", "Massimo Equi", "Henrik Lievonen", "Augusto Modanese", "Ronja Stimpert"], "title": "Orientation does not help with 3-coloring a grid in online-LOCAL", "comment": "16 pages, 3 figures", "summary": "The online-LOCAL and SLOCAL models are extensions of the LOCAL model where\nnodes are processed in a sequential but potentially adversarial order. So far,\nthe only problem we know of where the global memory of the online-LOCAL model\nhas an advantage over SLOCAL is 3-coloring bipartite graphs. Recently, Chang et\nal. [PODC 2024] showed that even in grids, 3-coloring requires $\\Omega(\\log n)$\nlocality in deterministic online-LOCAL. This result was subsequently extended\nby Akbari et al. [STOC 2025] to also hold in randomized online-LOCAL. However,\nboth proofs heavily rely on the assumption that the algorithm does not have\naccess to the orientation of the underlying grid. In this paper, we show how to\nlift this requirement and obtain the same lower bound (against either model)\neven when the algorithm is explicitly given a globally consistent orientation\nof the grid."}
{"id": "2509.22547", "categories": ["cs.NI"], "pdf": "https://arxiv.org/pdf/2509.22547", "abs": "https://arxiv.org/abs/2509.22547", "authors": ["Dian Echevarr√≠a P√©rez", "Onel L. Alcaraz L√≥pez", "Hirley Alves"], "title": "Extreme Value Theory-enhanced Radio Maps for Handovers in Ultra-reliable Communications", "comment": "11 pages, 11 figures, accepted for publication in IEEE Transactions\n  on Vehicular Technology", "summary": "Efficient handover (HO) strategies are essential for maintaining the\nstringent performance requirements of ultra-reliable communication (URC)\nsystems. This work introduces a novel HO framework designed from a\nphysical-layer perspective, where the decision-making process focuses on\ndetermining the optimal time and location for performing HOs. Leveraging\nextreme value theory (EVT) and statistical radio maps, the proposed method\npredicts signal behaviour and enables efficient resource allocation. The\nframework ensures seamless HOs and improved system performance by facilitating\neffective resource transitions and coordination across spatial locations while\nincorporating mechanisms to mitigate the ping-pong effect. Comparative\nevaluations demonstrate that this strategy provides superior service\navailability and energy efficiency than traditional HO mechanisms, highlighting\nits effectiveness in URC environments."}
{"id": "2509.22596", "categories": ["cs.MA", "cs.LG", "math.OC"], "pdf": "https://arxiv.org/pdf/2509.22596", "abs": "https://arxiv.org/abs/2509.22596", "authors": ["Qixin Zhang", "Yan Sun", "Can Jin", "Xikun Zhang", "Yao Shu", "Puning Zhao", "Li Shen", "Dacheng Tao"], "title": "Effective Policy Learning for Multi-Agent Online Coordination Beyond Submodular Objectives", "comment": "Accepted to NeurIPS 2025", "summary": "In this paper, we present two effective policy learning algorithms for\nmulti-agent online coordination(MA-OC) problem. The first one, \\texttt{MA-SPL},\nnot only can achieve the optimal $(1-\\frac{c}{e})$-approximation guarantee for\nthe MA-OC problem with submodular objectives but also can handle the unexplored\n$\\alpha$-weakly DR-submodular and $(\\gamma,\\beta)$-weakly submodular scenarios,\nwhere $c$ is the curvature of the investigated submodular functions, $\\alpha$\ndenotes the diminishing-return(DR) ratio and the tuple $(\\gamma,\\beta)$\nrepresents the submodularity ratios. Subsequently, in order to reduce the\nreliance on the unknown parameters $\\alpha,\\gamma,\\beta$ inherent in the\n\\texttt{MA-SPL} algorithm, we further introduce the second online algorithm\nnamed \\texttt{MA-MPL}. This \\texttt{MA-MPL} algorithm is entirely\n\\emph{parameter-free} and simultaneously can maintain the same approximation\nratio as the first \\texttt{MA-SPL} algorithm. The core of our \\texttt{MA-SPL}\nand \\texttt{MA-MPL} algorithms is a novel continuous-relaxation technique\ntermed as \\emph{policy-based continuous extension}. Compared with the\nwell-established \\emph{multi-linear extension}, a notable advantage of this new\n\\emph{policy-based continuous extension} is its ability to provide a lossless\nrounding scheme for any set function, thereby enabling us to tackle the\nchallenging weakly submodular objectives. Finally, extensive simulations are\nconducted to validate the effectiveness of our proposed algorithms."}
{"id": "2509.21945", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.21945", "abs": "https://arxiv.org/abs/2509.21945", "authors": ["Pengzhou Chen", "Hongyuan Liang", "Tao Chen"], "title": "Unveiling Many Faces of Surrogate Models for Configuration Tuning: A Fitness Landscape Analysis Perspective", "comment": "This paper is under review", "summary": "To efficiently tune configuration for better system performance (e.g.,\nlatency), many tuners have leveraged a surrogate model to expedite the process\ninstead of solely relying on the profoundly expensive system measurement. As\nsuch, it is naturally believed that we need more accurate models. However, the\nfact of accuracy can lie-a somewhat surprising finding from prior work-has left\nus many unanswered questions regarding what role the surrogate model plays in\nconfiguration tuning. This paper provides the very first systematic exploration\nand discussion, together with a resolution proposal, to disclose the many faces\nof surrogate models for configuration tuning, through the novel perspective of\nfitness landscape analysis. We present a theory as an alternative to accuracy\nfor assessing the model usefulness in tuning, based on which we conduct an\nextensive empirical study involving up to 27,000 cases. Drawing on the above,\nwe propose Model4Tune, an automated predictive tool that estimates which\nmodel-tuner pairs are the best for an unforeseen system without expensive tuner\nprofiling. Our results suggest that Moldel4Tune, as one of the first of its\nkind, performs significantly better than random guessing in 79%-82% of the\ncases. Our results not only shed light on the possible future research\ndirections but also offer a practical resolution that can assist practitioners\nin evaluating the most useful model for configuration tuning."}
{"id": "2509.22568", "categories": ["cs.NI", "cs.CR", "cs.CY", "cs.ET"], "pdf": "https://arxiv.org/pdf/2509.22568", "abs": "https://arxiv.org/abs/2509.22568", "authors": ["Karim Khamaisi", "Oliver Kamer", "Bruno Rodrigues", "Jan von der Assen", "Burkhard Stiller"], "title": "Bridging Technical Capability and User Accessibility: Off-grid Civilian Emergency Communication", "comment": null, "summary": "During large-scale crises disrupting cellular and Internet infrastructure,\ncivilians lack reliable methods for communication, aid coordination, and access\nto trustworthy information. This paper presents a unified emergency\ncommunication system integrating a low-power, long-range network with a\ncrisis-oriented smartphone application, enabling decentralized and off-grid\ncivilian communication. Unlike previous solutions separating physical layer\nresilience from user layer usability, our design merges these aspects into a\ncohesive crisis-tailored framework.\n  The system is evaluated in two dimensions: communication performance and\napplication functionality. Field experiments in urban Z\\\"urich demonstrate that\nthe 868 MHz band, using the LongFast configuration, achieves a communication\nrange of up to 1.2 km with 92% Packet Delivery Ratio, validating network\nrobustness under real-world infrastructure degraded conditions. In parallel, a\npurpose-built mobile application featuring peer-to-peer messaging, identity\nverification, and community moderation was evaluated through a\nrequirements-based analysis."}
{"id": "2509.22620", "categories": ["cs.MA", "cs.CR"], "pdf": "https://arxiv.org/pdf/2509.22620", "abs": "https://arxiv.org/abs/2509.22620", "authors": ["Andr√©s F√°brega", "Amy Zhao", "Jay Yu", "James Austgen", "Sarah Allen", "Kushal Babel", "Mahimna Kelkar", "Ari Juels"], "title": "Voting-Bloc Entropy: A New Metric for DAO Decentralization", "comment": "Full version of the paper published in USENIX Security 2025", "summary": "Decentralized Autonomous Organizations (DAOs) use smart contracts to foster\ncommunities working toward common goals. Existing definitions of\ndecentralization, however -- the 'D' in DAO -- fall short of capturing the key\nproperties characteristic of diverse and equitable participation. This work\nproposes a new framework for measuring DAO decentralization called Voting-Bloc\nEntropy (VBE, pronounced ''vibe''). VBE is based on the idea that voters with\nclosely aligned interests act as a centralizing force and should be modeled as\nsuch. VBE formalizes this notion by measuring the similarity of participants'\nutility functions across a set of voting rounds. Unlike prior, ad hoc\ndefinitions of decentralization, VBE derives from first principles: We\nintroduce a simple (yet powerful) reinforcement learning-based conceptual model\nfor voting, that in turn implies VBE. We first show VBE's utility as a\ntheoretical tool. We prove a number of results about the (de)centralizing\neffects of vote delegation, proposal bundling, bribery, etc. that are\noverlooked in previous notions of DAO decentralization. Our results lead to\npractical suggestions for enhancing DAO decentralization. We also show how VBE\ncan be used empirically by presenting measurement studies and VBE-based\ngovernance experiments. We make the tools we developed for these results\navailable to the community in the form of open-source artifacts in order to\nfacilitate future study of DAO decentralization."}
{"id": "2509.22097", "categories": ["cs.SE", "cs.AI", "cs.CL", "cs.CR"], "pdf": "https://arxiv.org/pdf/2509.22097", "abs": "https://arxiv.org/abs/2509.22097", "authors": ["Junkai Chen", "Huihui Huang", "Yunbo Lyu", "Junwen An", "Jieke Shi", "Chengran Yang", "Ting Zhang", "Haoye Tian", "Yikun Li", "Zhenhao Li", "Xin Zhou", "Xing Hu", "David Lo"], "title": "SecureAgentBench: Benchmarking Secure Code Generation under Realistic Vulnerability Scenarios", "comment": null, "summary": "Large language model (LLM) powered code agents are rapidly transforming\nsoftware engineering by automating tasks such as testing, debugging, and\nrepairing, yet the security risks of their generated code have become a\ncritical concern. Existing benchmarks have offered valuable insights but remain\ninsufficient: they often overlook the genuine context in which vulnerabilities\nwere introduced or adopt narrow evaluation protocols that fail to capture\neither functional correctness or newly introduced vulnerabilities. We therefore\nintroduce SecureAgentBench, a benchmark of 105 coding tasks designed to\nrigorously evaluate code agents' capabilities in secure code generation. Each\ntask includes (i) realistic task settings that require multi-file edits in\nlarge repositories, (ii) aligned contexts based on real-world open-source\nvulnerabilities with precisely identified introduction points, and (iii)\ncomprehensive evaluation that combines functionality testing, vulnerability\nchecking through proof-of-concept exploits, and detection of newly introduced\nvulnerabilities using static analysis. We evaluate three representative agents\n(SWE-agent, OpenHands, and Aider) with three state-of-the-art LLMs (Claude 3.7\nSonnet, GPT-4.1, and DeepSeek-V3.1). Results show that (i) current agents\nstruggle to produce secure code, as even the best-performing one, SWE-agent\nsupported by DeepSeek-V3.1, achieves merely 15.2% correct-and-secure solutions,\n(ii) some agents produce functionally correct code but still introduce\nvulnerabilities, including new ones not previously recorded, and (iii) adding\nexplicit security instructions for agents does not significantly improve secure\ncoding, underscoring the need for further research. These findings establish\nSecureAgentBench as a rigorous benchmark for secure code generation and a step\ntoward more reliable software development with LLMs."}
{"id": "2509.22114", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.22114", "abs": "https://arxiv.org/abs/2509.22114", "authors": ["Hanzhuo Tan", "Weihao Li", "Xiaolong Tian", "Siyi Wang", "Jiaming Liu", "Jing Li", "Yuqun Zhang"], "title": "SK2Decompile: LLM-based Two-Phase Binary Decompilation from Skeleton to Skin", "comment": null, "summary": "Large Language Models (LLMs) have emerged as a promising approach for binary\ndecompilation. However, the existing LLM-based decompilers still are somewhat\nlimited in effectively presenting a program's source-level structure with its\noriginal identifiers. To mitigate this, we introduce SK2Decompile, a novel\ntwo-phase approach to decompile from the skeleton (semantic structure) to the\nskin (identifier) of programs. Specifically, we first apply a Structure\nRecovery model to translate a program's binary code to an Intermediate\nRepresentation (IR) as deriving the program's \"skeleton\", i.e., preserving\ncontrol flow and data structures while obfuscating all identifiers with generic\nplaceholders. We also apply reinforcement learning to reward the model for\nproducing program structures that adhere to the syntactic and semantic rules\nexpected by compilers. Second, we apply an Identifier Naming model to produce\nmeaningful identifiers which reflect actual program semantics as deriving the\nprogram's \"skin\". We train the Identifier Naming model with a separate\nreinforcement learning objective that rewards the semantic similarity between\nits predictions and the reference code. Such a two-phase decompilation process\nfacilitates advancing the correctness and readability of decompilation\nindependently. Our evaluations indicate that SK2Decompile, significantly\noutperforms the SOTA baselines, achieving 21.6% average re-executability rate\ngain over GPT-5-mini on the HumanEval dataset and 29.4% average R2I improvement\nover Idioms on the GitHub2025 benchmark."}
{"id": "2509.22170", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.22170", "abs": "https://arxiv.org/abs/2509.22170", "authors": ["Chengjia Wang", "Lanling Tang", "Ming Yuan", "Jiongchi Yu", "Xiaofei Xie", "Jiajun Bu"], "title": "Leveraging LLM Agents for Automated Video Game Testing", "comment": "17 pages", "summary": "Testing MMORPGs (Massively Multiplayer Online Role-Playing Games) is a\ncritical yet labor-intensive task in game development due to their complexity\nand frequent updating nature. Traditional automated game testing approaches\nstruggle to achieve high state coverage and efficiency in these rich,\nopen-ended environments, while existing LLM-based game-playing approaches are\nlimited to shallow reasoning ability in understanding complex game state-action\nspaces and long-complex tasks. To address these challenges, we propose TITAN,\nan effective LLM-driven agent framework for intelligent MMORPG testing. TITAN\nincorporates four key components to: (1) perceive and abstract high-dimensional\ngame states, (2) proactively optimize and prioritize available actions, (3)\nenable long-horizon reasoning with action trace memory and reflective\nself-correction, and (4) employ LLM-based oracles to detect potential\nfunctional and logic bugs with diagnostic reports.\n  We implement the prototype of TITAN and evaluate it on two large-scale\ncommercial MMORPGs spanning both PC and mobile platforms. In our experiments,\nTITAN achieves significantly higher task completion rates (95%) and bug\ndetection performance compared to existing automated game testing approaches.\nAn ablation study further demonstrates that each core component of TITAN\ncontributes substantially to its overall performance. Notably, TITAN detects\nfour previously unknown bugs that prior testing approaches fail to identify. We\nprovide an in-depth discussion of these results, which offer guidance for new\navenues of advancing intelligent, general-purpose testing systems. Moreover,\nTITAN has been deployed in eight real-world game QA pipelines, underscoring its\npractical impact as an LLM-driven game testing framework."}
{"id": "2509.22202", "categories": ["cs.SE", "cs.CL"], "pdf": "https://arxiv.org/pdf/2509.22202", "abs": "https://arxiv.org/abs/2509.22202", "authors": ["Lukas Twist", "Jie M. Zhang", "Mark Harman", "Helen Yannakoudakis"], "title": "Library Hallucinations in LLMs: Risk Analysis Grounded in Developer Queries", "comment": "23 pages, 5 tables", "summary": "Large language models (LLMs) are increasingly used to generate code, yet they\ncontinue to hallucinate, often inventing non-existent libraries. Such library\nhallucinations are not just benign errors: they can mislead developers, break\nbuilds, and expose systems to supply chain threats such as slopsquatting.\nDespite increasing awareness of these risks, little is known about how\nreal-world prompt variations affect hallucination rates. Therefore, we present\nthe first systematic study of how user-level prompt variations impact library\nhallucinations in LLM-generated code. We evaluate six diverse LLMs across two\nhallucination types: library name hallucinations (invalid imports) and library\nmember hallucinations (invalid calls from valid libraries). We investigate how\nrealistic user language extracted from developer forums and how user errors of\nvarying degrees (one- or multi-character misspellings and completely fake\nnames/members) affect LLM hallucination rates. Our findings reveal systemic\nvulnerabilities: one-character misspellings in library names trigger\nhallucinations in up to 26% of tasks, fake library names are accepted in up to\n99% of tasks, and time-related prompts lead to hallucinations in up to 84% of\ntasks. Prompt engineering shows promise for mitigating hallucinations, but\nremains inconsistent and LLM-dependent. Our results underscore the fragility of\nLLMs to natural prompt variation and highlight the urgent need for safeguards\nagainst library-related hallucinations and their potential exploitation."}
{"id": "2509.22320", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.22320", "abs": "https://arxiv.org/abs/2509.22320", "authors": ["Vincenzo De Martino", "Mohammad Amin Zadenoori", "Xavier Franch", "Alessio Ferrari"], "title": "Green Prompt Engineering: Investigating the Energy Impact of Prompt Design in Software Engineering", "comment": null, "summary": "Language Models are increasingly applied in software engineering, yet their\ninference raises growing environmental concerns. Prior work has examined\nhardware choices and prompt length, but little attention has been paid to\nlinguistic complexity as a sustainability factor. This paper introduces Green\nPrompt Engineering, framing linguistic complexity as a design dimension that\ncan influence energy consumption and performance. We conduct an empirical study\non requirement classification using open-source Small Language Models, varying\nthe readability of prompts. Our results reveal that readability affects\nenvironmental sustainability and performance, exposing trade-offs between them.\nFor practitioners, simpler prompts can reduce energy costs without a\nsignificant F1-score loss; for researchers, it opens a path toward guidelines\nand studies on sustainable prompt design within the Green AI agenda."}
{"id": "2509.22337", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.22337", "abs": "https://arxiv.org/abs/2509.22337", "authors": ["Haoyu Feng", "Xin Zhang"], "title": "GPU-Accelerated Loopy Belief Propagation for Program Analysis", "comment": null, "summary": "Loopy Belief Propagation (LBP) is a widely used approximate inference\nalgorithm in probabilistic graphical models, with applications in computer\nvision, error correction codes, protein folding, program analysis, etc.\nHowever, LBP faces significant computational challenges when applied to\nlarge-scale program analysis. While GPU (Graphics Processing Unit) parallel\ncomputing provides a promising solution, existing approaches lack support for\nflexible update strategies and have yet to integrate logical constraints with\nGPU acceleration, leading to suboptimal practical performance.\n  This paper presents a GPU-accelerated LBP algorithm for program analysis. To\nsupport the diverse update strategies required by users, we propose a unified\nrepresentation for specifying arbitrary user-defined update strategies, along\nwith a dependency analysis algorithm. Furthermore, building on previous work\nthat leverages the local structure of Horn clauses to simplify message passing,\nwe group messages to minimize warp divergence and better utilize GPU resources.\nExperimental results on datarace analysis over eight real-world Java programs\nshow that our approach achieves an average speedup of $2.14\\times$ over the\nstate-of-the-art sequential approach and $5.56\\times$ over the state-of-the-art\nGPU-based approach, while maintaining high accuracy."}
{"id": "2509.22379", "categories": ["cs.SE", "cs.RO"], "pdf": "https://arxiv.org/pdf/2509.22379", "abs": "https://arxiv.org/abs/2509.22379", "authors": ["Stefano Carlo Lambertenghi", "Mirena Flores Valdez", "Andrea Stocco"], "title": "A Multi-Modality Evaluation of the Reality Gap in Autonomous Driving Systems", "comment": "In proceedings of the 40th IEEE/ACM International Conference on\n  Automated Software Engineering (ASE '25)", "summary": "Simulation-based testing is a cornerstone of Autonomous Driving System (ADS)\ndevelopment, offering safe and scalable evaluation across diverse driving\nscenarios. However, discrepancies between simulated and real-world behavior,\nknown as the reality gap, challenge the transferability of test results to\ndeployed systems. In this paper, we present a comprehensive empirical study\ncomparing four representative testing modalities: Software-in-the-Loop (SiL),\nVehicle-in-the-Loop (ViL), Mixed-Reality (MR), and full real-world testing.\nUsing a small-scale physical vehicle equipped with real sensors (camera and\nLiDAR) and its digital twin, we implement each setup and evaluate two ADS\narchitectures (modular and end-to-end) across diverse indoor driving scenarios\ninvolving real obstacles, road topologies, and indoor environments. We\nsystematically assess the impact of each testing modality along three\ndimensions of the reality gap: actuation, perception, and behavioral fidelity.\nOur results show that while SiL and ViL setups simplify critical aspects of\nreal-world dynamics and sensing, MR testing improves perceptual realism without\ncompromising safety or control. Importantly, we identify the conditions under\nwhich failures do not transfer across testing modalities and isolate the\nunderlying dimensions of the gap responsible for these discrepancies. Our\nfindings offer actionable insights into the respective strengths and\nlimitations of each modality and outline a path toward more robust and\ntransferable validation of autonomous driving systems."}
{"id": "2509.22420", "categories": ["cs.SE", "cs.CY"], "pdf": "https://arxiv.org/pdf/2509.22420", "abs": "https://arxiv.org/abs/2509.22420", "authors": ["Ziyi Zhang", "Devjeet Roy", "Venera Arnaoudova"], "title": "Context-Specific Instruction: A Longitudinal Study on Debugging Skill Acquisition and Retention for Novice Programmers", "comment": "31 pages (25 pages for the paper, rest pages are references and\n  appendix). 4 tables, 7 figures", "summary": "Bug localization is a critical skill, yet novices often lack systematic\napproaches. Prior work tested abstract guidelines and general concrete steps;\nthe impact of context-specific instruction is unclear. We ran an eight-week\nlongitudinal study with four conditions: no instruction (G1), abstract\nguidelines (G2), concrete steps (G3), and our context-specific instruction that\npairs concrete bug-localization steps with problem-specific details (G4).\nForty-four undergraduates participated; 41 completed all five sessions (S1-S5).\nEach session included 2-3 debugging tasks to identify the minimal code element\ncontaining a seeded logical fault. We measured correctness (binary), time to\ncompletion, self-perceived scores (stress, difficulty, satisfaction, and\nstrategy adherence). G4 achieved higher correctness and shorter time to\ncompletion: it reached 80% correctness after one session (vs. 20-44% for other\ngroups) and maintained 80% after three weeks, outperforming all groups (p <\n0.05); its time to completion stabilized at 13-15 minutes in S1, whereas other\ngroups took 2-3 sessions to stabilize at 22-27 minutes. Qualitative responses\nshowed lower stress and higher satisfaction in G4, with participants\ninternalizing strategies via contextual examples. We conclude that\ncontext-specific instruction yields faster skill acquisition and stronger\nretention than abstract guidelines or context-agnostic steps. Even 1-2 sessions\nproduced significant gains, while extended practice optimized and stabilized\nperformance. Integrating contextual examples with abstract principles may\nbridge theory-practice gaps in bug-localization education and provide a more\nequitable path for novices."}
{"id": "2509.22431", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.22431", "abs": "https://arxiv.org/abs/2509.22431", "authors": ["Zhengyu Chen", "Zhaoyi Meng", "Wenxiang Zhao", "Wansen Wang", "Haoyang Zhao", "Jiahao Zhan", "Jie Cui", "Hong Zhong"], "title": "TreeMind: Automatically Reproducing Android Bug Reports via LLM-empowered Monte Carlo Tree Search", "comment": null, "summary": "Automatically reproducing Android app crashes from textual bug reports is\nchallenging, particularly when the reports are incomplete and the modern UI\nexhibits high combinatorial complexity. Existing approaches based on\nreinforcement learning or large language models (LLMs) exhibit limitations in\nsuch scenarios. They struggle to infer unobserved steps and reconstruct the\nunderlying user action sequences to navigate the vast UI interaction space,\nprimarily due to limited goal-directed reasoning and planning. We present\nTreeMind, a novel technique that integrates LLMs with a customized Monte Carlo\nTree Search (MCTS) algorithm to achieve strategic UI exploration in bug\nreproduction. To the best of our knowledge, this is the first work to combine\nexternal decision-making with LLM semantic reasoning for reliable bug\nreproduction. We formulate the reproduction task as a target-driven search\nproblem, leveraging MCTS as the core planning mechanism to iteratively refine\naction sequences. To enhance MCTS with semantic reasoning, we introduce two\nLLM-guided agents with distinct roles: Expander generates top-k promising\nactions based on the current UI state and exploration history, while Simulator\nestimates the likelihood that each action leads toward successful reproduction.\nBy incorporating multi-modal UI inputs and advanced prompting techniques,\nTreeMind conducts feedback-aware navigation that identifies missing but\nessential user actions and incrementally reconstructs the reproduction paths.\nWe evaluate TreeMind on a dataset of 93 real-world Android bug reports from\nthree widely-used benchmarks. Experimental results show that it significantly\noutperforms four state-of-the-art baselines in reproduction success rate. A\nreal-world case study indicates that integrating LLM reasoning with MCTS-based\nplanning is a compelling direction for automated bug reproduction."}
{"id": "2509.22530", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.22530", "abs": "https://arxiv.org/abs/2509.22530", "authors": ["Baijun Cheng", "Kailong Wang", "Ling Shi", "Haoyu Wang", "Peng Di", "Yao Guo", "Ding Li", "Xiangqun Chen"], "title": "Boosting Pointer Analysis With Large Language Model-Enhanced Allocation Function Detection", "comment": null, "summary": "Pointer analysis is foundational for many static analysis tasks, yet its\neffectiveness is often hindered by imprecise modeling of heap allocations,\nparticularly in C/C++ programs where user-defined allocation functions (AFs)\nare pervasive. Existing approaches largely overlook these custom allocators,\nleading to coarse aliasing and reduced analysis precision. In this paper, we\npresent AFD, a novel technique that enhances pointer analysis by automatically\nidentifying and modeling custom allocation functions. AFD employs a hybrid\napproach: it uses value-flow analysis to detect straightforward wrappers and\nleverages Large Language Models (LLMs) to reason about more complex allocation\npatterns with side effects. This targeted enhancement enables precise modeling\nof heap objects at each call site, achieving context-sensitivity-like benefits\nwithout the associated overhead. We evaluate AFD on 15 real-world C projects,\nidentifying over 600 custom AFs. Integrating AFD into a baseline pointer\nanalysis yields a 26x increase in modeled heap objects and a 39% reduction in\nalias set sizes, with only 1.4x runtime overhead. Furthermore, our enhanced\nanalysis improves indirect call resolution and uncovers 17 previously\nundetected memory bugs. These results demonstrate that precise modeling of\ncustom allocation functions offers a scalable and practical path to improving\npointer analysis in large software systems."}
