<div id=toc></div>

# Table of Contents

- [cs.MA](#cs.MA) [Total: 2]
- [cs.AR](#cs.AR) [Total: 1]
- [cs.SE](#cs.SE) [Total: 2]
- [cs.NI](#cs.NI) [Total: 4]
- [cs.DC](#cs.DC) [Total: 6]


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [1] [Towards Optimal Performance and Action Consistency Guarantees in Dec-POMDPs with Inconsistent Beliefs and Limited Communication](https://arxiv.org/abs/2512.20778)
*Moshe Rafaeli Shimron,Vadim Indelman*

Main category: cs.MA

TL;DR: 提出一种考虑信念不一致的去中心化多智能体决策框架，提升协作性能与安全性。


<details>
  <summary>Details</summary>
Motivation: 现有方法假设智能体信念一致，但现实中因通信受限导致信念不一致，影响协作效果与安全。

Method: 构建新框架以显式处理信念不一致，提供动作一致性与性能的概率保证，并按需触发通信。

Result: 仿真表明该方法优于现有最先进算法。

Conclusion: 该框架有效应对信念不一致问题，兼顾性能与通信效率，适用于实际多智能体系统。

Abstract: Multi-agent decision-making under uncertainty is fundamental for effective and safe autonomous operation. In many real-world scenarios, each agent maintains its own belief over the environment and must plan actions accordingly. However, most existing approaches assume that all agents have identical beliefs at planning time, implying these beliefs are conditioned on the same data. Such an assumption is often impractical due to limited communication. In reality, agents frequently operate with inconsistent beliefs, which can lead to poor coordination and suboptimal, potentially unsafe, performance. In this paper, we address this critical challenge by introducing a novel decentralized framework for optimal joint action selection that explicitly accounts for belief inconsistencies. Our approach provides probabilistic guarantees for both action consistency and performance with respect to open-loop multi-agent POMDP (which assumes all data is always communicated), and selectively triggers communication only when needed. Furthermore, we address another key aspect of whether, given a chosen joint action, the agents should share data to improve expected performance in inference. Simulation results show our approach outperforms state-of-the-art algorithms.

</details>


### [2] [DAO-Agent: Zero Knowledge-Verified Incentives for Decentralized Multi-Agent Coordination](https://arxiv.org/abs/2512.20973)
*Yihan Xia,Taotao Wang,Wenxin Xu,Shengli Zhang*

Main category: cs.MA

TL;DR: DAO-Agent框架通过结合链上DAO治理、零知识证明和混合架构，实现LLM多智能体在去中心化环境中的可审计执行与公平激励，同时保护隐私并大幅降低链上成本。


<details>
  <summary>Details</summary>
Motivation: 解决在无信任环境中自主LLM智能体协作时面临的透明贡献度量、公平激励分配及隐私保护问题。

Method: 提出DAO-Agent框架，包含链上DAO治理机制、基于ZKP的链下Shapley贡献度计算、以及链上验证ZKP结果的混合架构。

Result: 实验表明DAO-Agent相比纯链上方案减少99.9%验证Gas成本，且验证复杂度不随联盟规模增长，具备高可扩展性。

Conclusion: DAO-Agent为去中心化环境中大规模LLM智能体协作提供了兼顾效率、公平与隐私的可行解决方案。

Abstract: Autonomous Large Language Model (LLM)-based multi-agent systems have emerged as a promising paradigm for facilitating cross-application and cross-organization collaborations. These autonomous agents often operate in trustless environments, where centralized coordination faces significant challenges, such as the inability to ensure transparent contribution measurement and equitable incentive distribution. While blockchain is frequently proposed as a decentralized coordination platform, it inherently introduces high on-chain computation costs and risks exposing sensitive execution information of the agents. Consequently, the core challenge lies in enabling auditable task execution and fair incentive distribution for autonomous LLM agents in trustless environments, while simultaneously preserving their strategic privacy and minimizing on-chain costs. To address this challenge, we propose DAO-Agent, a novel framework that integrates three key technical innovations: (1) an on-chain decentralized autonomous organization (DAO) governance mechanism for transparent coordination and immutable logging; (2) a ZKP mechanism approach that enables Shapley-based contribution measurement off-chain, and (3) a hybrid on-chain/off-chain architecture that verifies ZKP-validated contribution measurements on-chain with minimal computational overhead. We implement DAO-Agent and conduct end-to-end experiments using a crypto trading task as a case study. Experimental results demonstrate that DAO-Agent achieves up to 99.9% reduction in verification gas costs compared to naive on-chain alternatives, with constant-time verification complexity that remains stable as coalition size increases, thereby establishing a scalable foundation for agent coordination in decentralized environments.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [3] [NotSoTiny: A Large, Living Benchmark for RTL Code Generation](https://arxiv.org/abs/2512.20823)
*Razine Moundir Ghorab,Emanuele Parisi,Cristian Gutierrez,Miquel Alberti-Binimelis,Miquel Moreto,Dario Garcia-Gasulla,Gokcen Kestor*

Main category: cs.AR

TL;DR: 本文提出NotSoTiny基准，用于评估LLM在生成结构丰富、上下文感知的RTL代码方面的能力。


<details>
  <summary>Details</summary>
Motivation: 现有RTL基准规模有限、设计简单、验证不足且易受数据污染影响，需更有效的评估方法推动LLM在硬件设计中的发展。

Method: 基于Tiny Tapeout社区数百个真实硬件设计，构建自动化流程去重、验证正确性并定期更新设计以避免数据污染。

Result: 评估表明NotSoTiny任务比以往基准更具挑战性，能有效揭示LLM在硬件设计中的局限性。

Conclusion: NotSoTiny为改进LLM在硬件设计领域的应用提供了可靠且具指导意义的评估基准。

Abstract: LLMs have shown early promise in generating RTL code, yet evaluating their capabilities in realistic setups remains a challenge. So far, RTL benchmarks have been limited in scale, skewed toward trivial designs, offering minimal verification rigor, and remaining vulnerable to data contamination. To overcome these limitations and to push the field forward, this paper introduces NotSoTiny, a benchmark that assesses LLM on the generation of structurally rich and context-aware RTL. Built from hundreds of actual hardware designs produced by the Tiny Tapeout community, our automated pipeline removes duplicates, verifies correctness and periodically incorporates new designs to mitigate contamination, matching Tiny Tapeout release schedule. Evaluation results show that NotSoTiny tasks are more challenging than prior benchmarks, emphasizing its effectiveness in overcoming current limitations of LLMs applied to hardware design, and in guiding the improvement of such promising technology.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [4] [Process Analytics -- Data-driven Business Process Management](https://arxiv.org/abs/2512.20703)
*Matthias Stierle,Karsten Kraume,Martin Matzner*

Main category: cs.SE

TL;DR: 本文提出了一种结合组织与利益相关者的新视角，以重新定义数据驱动的流程分析，并通过案例研究验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 当前流程挖掘研究过于关注技术层面，忽视了人文与组织因素，因此需要一种更全面的分析框架。

Method: 采用归纳与演绎相结合的方法，构建“流程分析学”概念及其多维框架，并结合大型企业案例进行对比讨论。

Result: 提出的流程分析学框架能够更好地整合技术、组织与人员因素，提升数据驱动流程分析的实际应用效果。

Conclusion: 将社会技术视角引入流程分析，有助于弥补现有研究的不足，推动更全面和实用的业务流程改进方法。

Abstract: Data-driven analysis of business processes has a long tradition in research. However, recently the term of process mining is mostly used when referring to data-driven process analysis. As a consequence, awareness for the many facets of process analysis is decreasing. In particular, while an increasing focus is put onto technical aspects of the analysis, human and organisational concerns remain under the radar. Following the socio-technical perspective of information systems research, we propose a new perspective onto data-driven process analysis that combines the process of analysis with the organisation and its stakeholders. This paper conceptualises the term process analytics and its various dimensions by following both an inductive and deductive approach. The results are discussed by contrasting them to a real-life case study from a large company implementing data-driven process analysis and automation.

</details>


### [5] [Assessing the Software Security Comprehension of Large Language Models](https://arxiv.org/abs/2512.21238)
*Mohammed Latif Siddiq,Natalie Sekerak,Antonio Karam,Maria Leal,Arvin Islam-Gomes,Joanna C. S. Santos*

Main category: cs.SE

TL;DR: 本文系统评估了五种主流大语言模型在软件安全领域的认知能力，发现其在低阶任务表现良好，但在高阶推理与系统构建任务中显著退化，并识别出51种常见误解模式。


<details>
  <summary>Details</summary>
Motivation: 厘清当前大语言模型在软件安全专业领域的实际能力边界，以指导其合理应用。

Method: 基于布鲁姆分类法六大认知维度，结合多类数据集（选择题、漏洞代码、课程考题、案例研究、项目任务）对五种LLM进行系统评测。

Result: 模型在记忆与理解层面准确率高，但在分析、评估与创造等高阶任务上性能明显下降；提出‘安全知识边界’概念并归纳51种典型误解模式。

Conclusion: 当前LLM尚未具备全面胜任软件安全高阶任务的能力，需谨慎用于架构设计与系统构建场景。

Abstract: Large language models (LLMs) are increasingly used in software development, but their level of software security expertise remains unclear. This work systematically evaluates the security comprehension of five leading LLMs: GPT-4o-Mini, GPT-5-Mini, Gemini-2.5-Flash, Llama-3.1, and Qwen-2.5, using Blooms Taxonomy as a framework. We assess six cognitive dimensions: remembering, understanding, applying, analyzing, evaluating, and creating. Our methodology integrates diverse datasets, including curated multiple-choice questions, vulnerable code snippets (SALLM), course assessments from an Introduction to Software Security course, real-world case studies (XBOW), and project-based creation tasks from a Secure Software Engineering course. Results show that while LLMs perform well on lower-level cognitive tasks such as recalling facts and identifying known vulnerabilities, their performance degrades significantly on higher-order tasks that require reasoning, architectural evaluation, and secure system creation. Beyond reporting aggregate accuracy, we introduce a software security knowledge boundary that identifies the highest cognitive level at which a model consistently maintains reliable performance. In addition, we identify 51 recurring misconception patterns exhibited by LLMs across Blooms levels.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [6] [Embodied AI-Enhanced IoMT Edge Computing: UAV Trajectory Optimization and Task Offloading with Mobility Prediction](https://arxiv.org/abs/2512.20902)
*Siqi Mu,Shuo Wen,Yang Lu,Ruihong Jiang,Bo Ai*

Main category: cs.NI

TL;DR: 本文提出一种基于AI增强的医疗物联网边缘计算框架，结合多尺度Transformer轨迹预测与深度强化学习算法，优化无人机飞行路径与任务卸载，以降低加权平均任务完成时间。


<details>
  <summary>Details</summary>
Motivation: 针对医疗物联网中无线体域网用户任务关键性时变及用户与无人机双重移动性问题，需优化任务卸载与无人机轨迹以提升服务效率。

Method: 构建基于历史轨迹的多尺度Transformer用户轨迹预测模型，并设计融合预测信息的深度强化学习算法，联合优化无人机飞行轨迹与任务卸载决策。

Result: 真实轨迹与仿真结果表明，所提方法在降低任务完成时间方面优于现有基准方案。

Conclusion: 该AI增强框架能有效应对动态环境下的任务调度与资源分配挑战，显著提升无人机辅助医疗边缘计算的服务性能。

Abstract: Due to their inherent flexibility and autonomous operation, unmanned aerial vehicles (UAVs) have been widely used in Internet of Medical Things (IoMT) to provide real-time biomedical edge computing service for wireless body area network (WBAN) users. In this paper, considering the time-varying task criticality characteristics of diverse WBAN users and the dual mobility between WBAN users and UAV, we investigate the dynamic task offloading and UAV flight trajectory optimization problem to minimize the weighted average task completion time of all the WBAN users, under the constraint of UAV energy consumption. To tackle the problem, an embodied AI-enhanced IoMT edge computing framework is established. Specifically, we propose a novel hierarchical multi-scale Transformer-based user trajectory prediction model based on the users' historical trajectory traces captured by the embodied AI agent (i.e., UAV). Afterwards, a prediction-enhanced deep reinforcement learning (DRL) algorithm that integrates predicted users' mobility information is designed for intelligently optimizing UAV flight trajectory and task offloading decisions. Real-word movement traces and simulation results demonstrate the superiority of the proposed methods in comparison with the existing benchmarks.

</details>


### [7] [SLIDE: Simultaneous Model Downloading and Inference at the Wireless Network Edge](https://arxiv.org/abs/2512.20946)
*Guanqiao Qu,Tao Li,Qian Chen,Xianhao Chen,Sheng Zhou*

Main category: cs.NI

TL;DR: 提出SLIDE框架，通过边下载边推理优化模型部署，显著提升任务吞吐量。


<details>
  <summary>Details</summary>
Motivation: 解决大模型下载与推理延迟过高问题，满足移动端实时AI服务需求。

Method: 联合优化模型分发、带宽与计算资源分配，设计多项式时间最优算法处理层间递归依赖。

Result: 仿真表明SLIDE在延迟和通信约束下显著优于传统方案。

Conclusion: SLIDE框架有效降低端到端延迟，提升多用户系统任务吞吐性能。

Abstract: To support on-device inference, the next-generation mobile networks are expected to support real-time model downloading services to mobile users. However, powerful AI models typically have large model sizes, resulting in excessive end-to-end (E2E) downloading-and-inference (DAI) latency. To address this issue, we propose a simultaneous model downloading and inference (SLIDE) framework, which allows users to perform inference with downloaded layers while simultaneously receiving the remaining layers of the model. To this end, we formulate a task throughput maximization problem by jointly optimizing model provisioning, spectrum bandwidth allocation, and computing resource allocation for multi-user downlink systems. Unlike traditional DAI frameworks, SLIDE introduces recursive dependencies across layers, where inference latency depends recursively on the downloading bandwidth and computing resource allocation for each of the preceding layers. To solve this challenging problem, we design an efficient algorithm that acquires the optimal solution with polynomial-time complexity. Simulation results demonstrate that the proposed SLIDE framework significantly improves task throughput under latency and communication resource constraints compared with the conventional model downloading schemes.

</details>


### [8] [Synecdoche: Efficient and Accurate In-Network Traffic Classification via Direct Packet Sequential Pattern Matching](https://arxiv.org/abs/2512.21116)
*Minyuan Xiao,Yunchun Li,Yuchen Zhao,Tong Guan,Mingyuan Xia,Wei Li*

Main category: cs.NI

TL;DR: Synecdoche 是首个在可编程数据平面通过模式匹配部署包序列特征的流量分类框架，兼顾高准确率与高效率。


<details>
  <summary>Details</summary>
Motivation: 现有方法在准确率与效率间难以兼顾，统计方法受限于精度，深度学习方法计算开销大。

Method: 采用“离线发现、在线匹配”范式，利用深度学习离线提取关键片段（Key Segments），编译为表项供数据平面高效匹配。

Result: 实验表明，F1分数最高提升26.4%，延迟降低13.0%，SRAM使用减少79.2%。

Conclusion: Synecdoche 在保证高精度的同时显著提升效率，是流量分类领域的重要突破。

Abstract: Traffic classification on programmable data plane holds great promise for line-rate processing, with methods evolving from per-packet to flow-level analysis for higher accuracy. However, a trade-off between accuracy and efficiency persists. Statistical feature-based methods align with hardware constraints but often exhibit limited accuracy, while online deep learning methods using packet sequential features achieve superior accuracy but require substantial computational resources. This paper presents Synecdoche, the first traffic classification framework that successfully deploys packet sequential features on a programmable data plane via pattern matching, achieving both high accuracy and efficiency. Our key insight is that discriminative information concentrates in short sub-sequences--termed Key Segments--that serve as compact traffic features for efficient data plane matching. Synecdoche employs an "offline discovery, online matching" paradigm: deep learning models automatically discover Key Segment patterns offline, which are then compiled into optimized table entries for direct data plane matching. Extensive experiments demonstrate Synecdoche's superior accuracy, improving F1-scores by up to 26.4% against statistical methods and 18.3% against online deep learning methods, while reducing latency by 13.0% and achieving 79.2% reduction in SRAM usage. The source code of Synecdoche is publicly available to facilitate reproducibility and further research.

</details>


### [9] [Encrypted Traffic Detection in Resource Constrained IoT Networks: A Diffusion Model and LLM Integrated Framework](https://arxiv.org/abs/2512.21144)
*Hongjuan Li,Hui Kang,Chenbang Liu,Ruolin Wang,Jiahui Li,Geng Sun,Jiacheng Wang,Shuang Liang,Shiwen Mao*

Main category: cs.NI

TL;DR: DMLITE是一个结合扩散模型与大语言模型的流量嵌入框架，用于资源受限的物联网环境中的加密流量检测，显著提升准确率并降低训练时间。


<details>
  <summary>Details</summary>
Motivation: 应对物联网环境中动态流量、计算资源受限和低延迟需求带来的加密流量检测挑战。

Method: 采用三阶段架构：流量可视化预处理、基于扩散模型的多层次特征提取、大语言模型引导的特征优化，并结合对比学习与粒子群参数自适应调整。

Result: 在USTC-TFC、ISCX-VPN和Edge-IIoTset数据集上分别达到98.87%、92.61%和99.83%的分类准确率，平均提升3.7%，训练时间减少41.9%。

Conclusion: DMLITE有效解决了资源受限环境下加密流量检测的难题，兼具高精度与高效性，具备实际部署潜力。

Abstract: The proliferation of Internet-of-things (IoT) infrastructures and the widespread adoption of traffic encryption present significant challenges, particularly in environments characterized by dynamic traffic patterns, constrained computational capabilities, and strict latency constraints. In this paper, we propose DMLITE, a diffusion model and large language model (LLM) integrated traffic embedding framework for network traffic detection within resource-limited IoT environments. The DMLITE overcomes these challenges through a tri-phase architecture including traffic visual preprocessing, diffusion-based multi-level feature extraction, and LLM-guided feature optimization. Specifically, the framework utilizes self-supervised diffusion models to capture both fine-grained and abstract patterns in encrypted traffic through multi-level feature fusion and contrastive learning with representative sample selection, thus enabling rapid adaptation to new traffic patterns with minimal labeled data. Furthermore, DMLITE incorporates LLMs to dynamically adjust particle swarm optimization parameters for intelligent feature selection by implementing a dual objective function that minimizes both classification error and variance across data distributions. Comprehensive experimental validation on benchmark datasets confirms the effectiveness of DMLITE, achieving classification accuracies of 98.87\%, 92.61\%, and 99.83\% on USTC-TFC, ISCX-VPN, and Edge-IIoTset datasets, respectively. This improves classification accuracy by an average of 3.7\% and reduces training time by an average of 41.9\% compared to the representative deep learning model.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [10] [RHAPSODY: Execution of Hybrid AI-HPC Workflows at Scale](https://arxiv.org/abs/2512.20795)
*Aymen Alsaadi,Mason Hooten,Mariya Goliyad,Andre Merzky,Andrew Shao,Mikhail Titov,Tianle Wang,Yian Chen,Maria Kalantzi,Kent Lee,Andrew Park,Indira Pimpalkhare,Nick Radcliffe,Colin Wahl,Pete Mendygral,Matteo Turilli,Shantenu Jha*

Main category: cs.DC

TL;DR: RHAPSODY是一个多运行时中间件，支持在超算平台上并发执行异构AI-HPC工作流，通过统一抽象协调现有运行时，实现低开销与高效扩展。


<details>
  <summary>Details</summary>
Motivation: 现有系统难以全面支持AI-HPC混合工作流的异构需求，限制了大规模应用的发展。

Method: 设计RHAPSODY中间件，整合MPI、持久AI服务、细粒度任务等，通过统一抽象协调多种运行时。

Result: 实验证明RHAPSODY开销极小，支持高异构性扩展，推理负载近线性扩展，AI与HPC任务耦合高效。

Conclusion: RHAPSODY有效解决了AI-HPC混合工作流的运行时挑战，为领导级超算平台提供可扩展支持。

Abstract: Hybrid AI-HPC workflows combine large-scale simulation, training, high-throughput inference, and tightly coupled, agent-driven control within a single execution campaign. These workflows impose heterogeneous and often conflicting requirements on runtime systems, spanning MPI executables, persistent AI services, fine-grained tasks, and low-latency AI-HPC coupling. Existing systems typically address only subsets of these requirements, limiting their ability to support emerging AI-HPC applications at scale. We present RHAPSODY, a multi-runtime middleware that enables concurrent execution of heterogeneous AI-HPC workloads through uniform abstractions for tasks, services, resources, and execution policies. Rather than replacing existing runtimes, RHAPSODY composes and coordinates them, allowing simulation codes, inference services, and agentic workflows to coexist within a single job allocation on leadership-class HPC platforms. We evaluate RHAPSODY with Dragon and vLLM on multiple HPC systems using representative heterogeneous, inference-at-scale, and tightly coupled AI-HPC workflows. Our results show that RHAPSODY introduces minimal runtime overhead, sustains increasing heterogeneity at scale, achieves near-linear scaling for high-throughput inference workloads, and data- and control-efficient coupling between AI and HPC tasks in agentic workflows.

</details>


### [11] [Stochastic well-structured transition systems](https://arxiv.org/abs/2512.20939)
*James Aspnes*

Main category: cs.DC

TL;DR: 该论文定义了一类新的随机良构迁移系统，涵盖多种分布式计算模型，并证明其计算能力与BPP类语言等价。


<details>
  <summary>Details</summary>
Motivation: 研究随机调度规则下良构迁移系统的计算能力及其扩展特性。

Method: 通过引入相位时钟和终止计算的期望时间分析，刻画系统计算能力。

Result: 证明此类系统在多项式期望时间内完成或失败，且增强系统可计算BPP类语言。

Conclusion: 增强后的随机良构迁移系统精确对应BPP类语言，未增强系统仅对应对称BPL类语言。

Abstract: Extending well-structured transition systems to incorporate a probabilistic scheduling rule, we define a new class of stochastic well-structured transition systems that includes population protocols, chemical reaction networks, and many common gossip models; as well as augmentations of these systems by an oracle that exposes a total order on agents as in population protocols in the comparison model or an equivalence relation as in population protocols with unordered data.
  We show that any implementation of a phase clock in these systems either stops or ticks too fast after polynomially many expected steps, and that any terminating computation in these systems finishes or fails in expected polynomial time. This latter property allows an exact characterization of the computational power of many stochastic well-structured transition systems augmented with a total order or equivalence relation on agents, showing that these compute exactly the languages in BPP, while the corresponding unaugmented systems compute just the symmetric languages in BPL.

</details>


### [12] [Diving into 3D Parallelism with Heterogeneous Spot Instance GPUs: Design and Implications](https://arxiv.org/abs/2512.20953)
*Yuxiao Wang,Yuedong Xu,Qingyang Duan,Yuxuan Liu,Lei Jiao,Yinghao Yu,Jun Wu*

Main category: cs.DC

TL;DR: AutoHet 是一种针对异构 GPU 环境优化的分布式训练系统，支持非对称三维并行与弹性恢复，显著提升训练吞吐量与容错效率。


<details>
  <summary>Details</summary>
Motivation: 应对大语言模型增长与 GPU 异构化带来的分布式训练挑战，解决并行策略适配、负载均衡与容错恢复难题。

Method: 提出 AutoHet 系统，建模设备分组与负载平衡为优化问题，支持非对称 3D 并行结构和细粒度任务分配，并设计基于本地优先的弹性恢复机制。

Result: 在三种大规模模型与多类 GPU 组合测试中，相比 Megatron-LM 和 Whale 最高提升 1.79 倍训练吞吐量，恢复速度提升 4.38 倍。

Conclusion: AutoHet 能高效适配异构硬件环境，在计算效率、内存利用与弹性恢复方面优于现有系统，推动大规模模型训练实用性。

Abstract: The rapid growth of large language models (LLMs) and the continuous release of new GPU products have significantly increased the demand for distributed training across heterogeneous GPU environments. In this paper, we present a comprehensive analysis of the challenges involved in implementing 3D parallelism in such environments, addressing critical issues such as the need for symmetric tensor parallelism, efficient gradient synchronization in asymmetric pipeline parallelism, and the trade-offs between memory utilization and computational efficiency. Building upon these insights, we introduce AutoHet, a novel system that automatically identifies the optimal parallelism plan for distributed training on heterogeneous GPUs. AutoHet supports asymmetric 3D parallelism structures and facilitates fine-grained workload distribution. We propose a theoretical model that frames the device grouping and load balancing as an optimization problem to minimize per-iteration training time, thus effectively balancing computing power and memory usage across GPUs with diverse capabilities. To enable elastic training upon spot instance preemption, AutoHet presents an efficient recovery strategy that prioritizes to retrieve training states from local nodes, and only downloads the missing checkpoints from the cloud storage. Our extensive evaluation, conducted on three large-scale models and utilizing combinations of three different GPU types, demonstrates that AutoHet outperforms existing DNN training systems, achieving up to a 1.79$\times$ speedup in training throughput compared with Megatron-LM and Whale, and a 4.38$\times$ speedup of recovery speed compared to a spot instance baseline.

</details>


### [13] [Deadline-Aware Online Scheduling for LLM Fine-Tuning with Spot Market Predictions](https://arxiv.org/abs/2512.20967)
*Linggao Kong,Yuedong Xu,Lei Jiao,Chuan Xu*

Main category: cs.DC

TL;DR: 本文提出一种结合预测的在线调度算法，用于在混合使用spot和on-demand GPU实例时优化大模型微调的成本与截止时间约束。


<details>
  <summary>Details</summary>
Motivation: 大模型微调成本高昂，spot实例虽便宜但波动大，需兼顾成本与截止时间进行智能调度。

Method: 构建整数规划模型，设计基于承诺水平的预测型在线算法，并辅以无预测备用算法，通过在线策略选择动态适配市场变化。

Result: 实验表明该框架能自适应选择最优策略，在不同市场动态下优于基线方法，效用最高提升54.8%。

Conclusion: 结合预测与在线学习的混合实例调度框架可显著降低大模型训练成本，同时保障时效性。

Abstract: As foundation models grow in size, fine-tuning them becomes increasingly expensive. While GPU spot instances offer a low-cost alternative to on-demand resources, their volatile prices and availability make deadline-aware scheduling particularly challenging. We tackle this difficulty by using a mix of spot and on-demand instances. Distinctively, we show the predictability of prices and availability in a spot instance market, the power of prediction in enabling cost-efficient scheduling and its sensitivity to estimation errors. An integer programming problem is formulated to capture the use of mixed instances under both the price and availability dynamics. We propose an online allocation algorithm with prediction based on the committed horizon control approach that leverages a \emph{commitment level} to enforce the partial sequence of decisions. When this prediction becomes inaccurate, we further present a complementary online algorithm without predictions. An online policy selection algorithm is developed that learns the best policy from a pool constructed by varying the parameters of both algorithms. We prove that the prediction-based algorithm achieves tighter performance bounds as prediction error decreases, while the policy selection algorithm possesses a regret bound of $\mathcal{O}(\sqrt{T})$. Experimental results demonstrate that our online framework can adaptively select the best policy under varying spot market dynamics and prediction quality, consistently outperforming baselines and improving utility by up to 54.8\%.

</details>


### [14] [Mesh-Attention: A New Communication-Efficient Distributed Attention with Improved Data Locality](https://arxiv.org/abs/2512.20968)
*Sirui Chen,Jingji Chen,Siqi Zhu,Ziheng Jiang,Yanghua Peng,Xuehai Qian*

Main category: cs.DC

TL;DR: 提出Mesh-Attention算法，通过二维分块降低通信开销，在256 GPU上平均提速2.9倍、通信量减少79%。


<details>
  <summary>Details</summary>
Motivation: 解决Ring-Attention因通信流量过大导致的可扩展性瓶颈，提升大语言模型分布式注意力效率。

Method: 基于矩阵模型设计二维计算分块分配策略，结合贪心调度算法优化GPU间通信，支持灵活调节通信计算比。

Result: 实验显示平均提速2.9倍、通信量减少79%，在大规模部署中保持优异扩展性。

Conclusion: Mesh-Attention显著优于现有方法，理论与实证均验证其低通信复杂度与高可扩展性优势。

Abstract: Distributed attention is a fundamental problem for scaling context window for Large Language Models (LLMs). The state-of-the-art method, Ring-Attention, suffers from scalability limitations due to its excessive communication traffic. This paper proposes a new distributed attention algorithm, Mesh-Attention, by rethinking the design space of distributed attention with a new matrix-based model. Our method assigns a two-dimensional tile -- rather than one-dimensional row or column -- of computation blocks to each GPU to achieve higher efficiency through lower communication-computation (CommCom) ratio. The general approach covers Ring-Attention as a special case, and allows the tuning of CommCom ratio with different tile shapes. Importantly, we propose a greedy algorithm that can efficiently search the scheduling space within the tile with restrictions that ensure efficient communication among GPUs. The theoretical analysis shows that Mesh-Attention leads to a much lower communication complexity and exhibits good scalability comparing to other current algorithms.
  Our extensive experiment results show that Mesh-Attention can achieve up to 3.4x speedup (2.9x on average) and reduce the communication volume by up to 85.4% (79.0% on average) on 256 GPUs. Our scalability results further demonstrate that Mesh-Attention sustains superior performance as the system scales, substantially reducing overhead in large-scale deployments. The results convincingly confirm the advantage of Mesh-Attention.

</details>


### [15] [ESCHER: Efficient and Scalable Hypergraph Evolution Representation with Application to Triad Counting](https://arxiv.org/abs/2512.21009)
*S. M. Shovan,Arindam Khanda,Sanjukta Bhowmick,Sajal K. Das*

Main category: cs.DC

TL;DR: ESCHER是一种面向GPU的高效可扩展动态超图表示结构，结合优化的三元组计数框架，在多种三元组类型上实现显著加速。


<details>
  <summary>Details</summary>
Motivation: 现有方法缺乏专用数据结构与软件支持，难以高效处理大规模动态超图分析。

Method: 提出ESCHER并行数据结构及配套三元组更新框架，减少冗余计算并充分利用GPU能力。

Result: 在真实与合成数据集上，相较现有最优方法分别实现最高104.5x、473.7x和112.5x加速。

Conclusion: ESCHER有效解决了大规模动态超图高效分析难题，为高阶交互研究提供实用工具。

Abstract: Higher-order interactions beyond pairwise relationships in large complex networks are often modeled as hypergraphs. Analyzing hypergraph properties such as triad counts is essential, as hypergraphs can reveal intricate group interaction patterns that conventional graphs fail to capture. In real-world scenarios, these networks are often large and dynamic, introducing significant computational challenges. Due to the absence of specialized software packages and data structures, the analysis of large dynamic hypergraphs remains largely unexplored. Motivated by this gap, we propose ESCHER, a GPU-centric parallel data structure for Efficient and Scalable Hypergraph Evolution Representation, designed to manage large scale hypergraph dynamics efficiently. We also design a hypergraph triad-count update framework that minimizes redundant computation while fully leveraging the capabilities of ESCHER for dynamic operations. We validate the efficacy of our approach across multiple categories of hypergraph triad counting, including hyperedge-based, incident-vertex-based, and temporal triads. Empirical results on both large real-world and synthetic datasets demonstrate that our proposed method outperforms existing state-of-the-art methods, achieving speedups of up to 104.5x, 473.7x, and 112.5x for hyperedge-based, incident-vertex-based, and temporal triad types, respectively.

</details>
