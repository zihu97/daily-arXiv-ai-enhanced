<div id=toc></div>

# Table of Contents

- [cs.SE](#cs.SE) [Total: 18]
- [cs.DC](#cs.DC) [Total: 9]
- [cs.MA](#cs.MA) [Total: 2]


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [1] [ACCeLLiuM: Supervised Fine-Tuning for Automated OpenACC Pragma Generation](https://arxiv.org/abs/2509.20380)
*Samyak Jhaveri,Vanessa Klotzmann,Crista Lopes*

Main category: cs.SE

TL;DR: ACCeLLiuM是两个专门针对生成OpenACC指令的大型语言模型，通过监督微调在4033个OpenACC pragma-loop对数据集上训练，显著提升了生成正确OpenACC指令的能力。


<details>
  <summary>Details</summary>
Motivation: GPU硬件和并行编程框架日益复杂，虽然基于指令的并行编程标准如OpenACC简化了GPU编程，但仍需要专业知识才能有效使用这些指令。

Method: 开发了两个专门的大型语言模型，使用从GitHub C/C++仓库挖掘的4033个OpenACC pragma-loop对数据集进行监督微调，其中3223对用于训练，810对用于测试。

Result: 微调后的模型在测试集上能生成正确指令类型的pragma达87%，完全准确的pragma（包括指令、子句、子句顺序和变量）达50%。即使不完全准确，生成的pragma也经常包含正确的子句或额外的控制子句。

Conclusion: ACCeLLiuM通过公开代码、模型和数据集，为LLM驱动的OpenACC pragma生成建立了可复现的基准，降低了自动化GPU卸载串行程序的障碍。

Abstract: The increasing ubiquity of GPUs is accompanied by the increasing complexity
of their hardware and parallel programming frameworks. Directive-based parallel
programming standards like OpenACC simplify GPU programming to some extent by
abstracting away low-level complexities, but a fair amount of expertise is
still required in order to use those directives effectively.
  We introduce ACCeLLiuM, two open weights Large Language Models specifically
fine-tuned for generating expert OpenACC directives for data-parallel loops,
along with the supervised fine-tuning dataset that was used to train them. The
ACCeLLiuM SFT dataset contains 4,033 OpenACC pragma-loop pairs mined from
public GitHub C/C++ repositories, with 3,223 pairs for training and 810 for
testing. Experimental evaluations show a pronounced performance gap in
generating correct OpenACC pragmas between base LLMs and our fine-tuned
versions. On the held-out test set, base LLMs fail to consistently generate
valid pragmas, whereas LLMs fine-tuned on the ACCeLLiuM dataset generate valid
pragmas with the correct directive type for $87\%$ of the data-parallel loops,
and exact pragmas--including directives, clauses, clause order, and clause
variables--for $50\%$ of the cases. Even when not exact, generated pragmas
frequently incorporate the correct clauses in a different order than the
ground-truth label, or include additional clauses that enable finer control
over parallel execution, data movement, and concurrency, offering practical
value beyond strict string-matching. By publicly releasing the code, models,
and dataset as ACCeLLiuM we hope to establish a reproducible benchmark for
LLM-powered OpenACC pragma generation, and lower the barrier to automated GPU
offloading of serially written programs.

</details>


### [2] [State-of-the-Art in Software Security Visualization: A Systematic Review](https://arxiv.org/abs/2509.20385)
*Ishara Devendra,Chaman Wijesiriwardana,Prasad Wimalaratne*

Main category: cs.SE

TL;DR: 本文通过系统文献综述构建了软件安全可视化技术的综合分类法，将其分为四类：基于图、基于符号、基于矩阵和基于隐喻的可视化方法，并指出了该领域的关键问题和未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 随着软件系统日益复杂和威胁环境不断演变，传统的文本和数值分析方法在处理安全问题时效率低下，需要将复杂的安全数据转化为易于理解的视觉格式。

Method: 通过对60多篇关键研究论文的系统文献综述，创建了软件安全可视化技术的分类体系，并将其分为四种主要类型进行分析。

Result: 识别出软件安全可视化的两个主要领域：软件开发可视化和操作安全/网络安全可视化，强调了创新可视化技术对适应不断变化的安全环境的必要性。

Conclusion: 研究强调了开发适应性强的新型可视化技术的迫切需求，这些技术对提升威胁检测能力、改进安全响应策略以及指导未来研究具有重要实践意义。

Abstract: Software security visualization is an interdisciplinary field that combines
the technical complexity of cybersecurity, including threat intelligence and
compliance monitoring, with visual analytics, transforming complex security
data into easily digestible visual formats. As software systems get more
complex and the threat landscape evolves, traditional text-based and numerical
methods for analyzing and interpreting security concerns become increasingly
ineffective. The purpose of this paper is to systematically review existing
research and create a comprehensive taxonomy of software security visualization
techniques through literature, categorizing these techniques into four types:
graph-based, notation-based, matrix-based, and metaphor-based visualization.
This systematic review explores over 60 recent key research papers in software
security visualization, highlighting its key issues, recent advancements, and
prospective future research directions. From the comprehensive analysis, the
two main areas were distinctly highlighted as extensive software development
visualization, focusing on advanced methods for depicting software
architecture: operational security visualization and cybersecurity
visualization. The findings highlight the necessity for innovative
visualization techniques that adapt to the evolving security landscape, with
practical implications for enhancing threat detection, improving security
response strategies, and guiding future research.

</details>


### [3] [Dynamic ReAct: Scalable Tool Selection for Large-Scale MCP Environments](https://arxiv.org/abs/2509.20386)
*Nishant Gaurav,Adit Akarsh,Ankit Ranjan,Manoj Bajaj*

Main category: cs.SE

TL;DR: Dynamic ReAct是一种新颖方法，使ReAct代理能够高效处理超出LLM上下文限制的大型MCP工具集，通过搜索加载机制减少50%工具加载量同时保持任务准确性。


<details>
  <summary>Details</summary>
Motivation: 解决在包含数百或数千个工具的环境中，同时加载所有工具计算不可行的问题，实现智能工具选择。

Method: 提出并评估五种逐步优化工具选择过程的架构，最终采用搜索加载机制实现最小计算开销的智能工具选择。

Result: 实验结果显示该方法将工具加载量减少高达50%，同时保持任务完成准确性。

Conclusion: 该方法推动了真正通用AI代理的发展，使其能够动态适应多样化任务环境。

Abstract: We present Dynamic ReAct, a novel approach for enabling ReAct agents to ef-
ficiently operate with extensive Model Control Protocol (MCP) tool sets that
exceed the contextual memory limitations of large language models. Our approach
addresses the fundamental challenge of tool selection in environments
containing hundreds or thousands of available tools, where loading all tools
simultaneously is computationally infeasible. We propose and evaluate five
distinct architectures that progressively refine the tool selection process,
culminating in a search-and-load mechanism that achieves intelligent tool
selection with minimal computational overhead. Our experimental results
demonstrate that the proposed approach reduces tool loading by up to 50% while
maintaining task completion accuracy, advancing the path towards truly
general-purpose AI agents capable of dynamically adapting to diverse task
environments.

</details>


### [4] [Towards Systematic Specification and Verification of Fairness Requirements: A Position Paper](https://arxiv.org/abs/2509.20387)
*Qusai Ramadan,Jukka Ruohonen,Abhishek Tiwari,Adam Alami,Zeyd Boukhers*

Main category: cs.SE

TL;DR: 提出了基于知识图谱的公平性框架，用于形式化地指定和验证软件系统的公平性需求，以解决算法歧视问题


<details>
  <summary>Details</summary>
Motivation: 现有研究忽视了歧视往往源于缺乏明确定义的公平性需求及其验证机制，专家知识通常隐含难以形式化

Method: 开发基于知识图谱的框架，借鉴安全工程领域的成功经验，通过知识图谱形式化公平性知识，辅助需求规范和验证

Result: 提出了该领域的研究挑战、研究问题和实施路线图

Conclusion: 知识图谱方法有望为软件系统公平性需求的规范和验证提供有效的形式化机制，填补当前研究空白

Abstract: Decisions suggested by improperly designed software systems might be prone to
discriminate against people based on protected characteristics, such as gender
and ethnicity. Previous studies attribute such undesired behavior to flaws in
algorithmic design or biased data. However, these studies ignore that
discrimination is often the result of a lack of well-specified fairness
requirements and their verification. The fact that experts' knowledge about
fairness is often implicit makes the task of specifying precise and verifiable
fairness requirements difficult. In related domains, such as security
engineering, knowledge graphs have been proven to be effective in formalizing
knowledge to assist requirements specification and verification. To address the
lack of formal mechanisms for specifying and verifying fairness requirements,
we propose the development of a knowledge graph-based framework for fairness.
In this paper, we discuss the challenges, research questions, and a road map
towards addressing the research questions.

</details>


### [5] [Online-Optimized RAG for Tool Use and Function Calling](https://arxiv.org/abs/2509.20415)
*Yu Pan,Xiaocheng Li,Hanzhao Wang*

Main category: cs.SE

TL;DR: Online-Optimized RAG是一个部署时框架，通过在线梯度更新持续优化检索嵌入，解决嵌入不对齐问题，提高工具选择准确性和任务成功率。


<details>
  <summary>Details</summary>
Motivation: 解决RAG系统中由于不完美嵌入模型或噪声描述导致的嵌入不对齐问题，这种不对齐可能导致错误检索和任务失败。

Method: 使用轻量级在线梯度更新，从实时交互中持续适应检索嵌入，仅需最小反馈（如任务成功），无需改变底层LLM。

Result: 在各种工具使用和文档检索场景中，该方法持续提高了工具选择准确性和最终任务成功率。

Conclusion: 提供了一个简单实用的路径，实现稳健、自我改进的RAG系统，支持单跳和多跳工具使用、动态工具库和K检索重排序。

Abstract: In many applications, retrieval-augmented generation (RAG) drives tool use
and function calling by embedding the (user) queries and matching them to
pre-specified tool/function descriptions. In this paper, we address an
embedding misalignment issue that often arises in practical applications due to
imperfect embedding models or noisy descriptions; such misalignment may lead to
incorrect retrieval and task failure. We introduce Online-Optimized RAG, a
deployment-time framework that continually adapts retrieval embeddings from
live interactions using minimal feedback (e.g., task success). Online-Optimized
RAG applies lightweight online gradient updates with negligible per-query
latency and requires no changes to the underlying LLM. The method is
plug-and-play: it supports both single- and multi-hop tool use, dynamic tool
inventories, and $K$-retrieval with re-ranking. We provide a problem-dependent
theoretical analysis that quantifies how the method's performance depends on
the initialization quality of the embeddings and other related quantities.
Across diverse tool-use and document-retrieval scenarios, our Online-Optimized
RAG consistently improves tool selection accuracy and end-task success, thus
providing a simple, practical path to robust, self-improving RAG systems.

</details>


### [6] [Formal Verification of Legal Contracts: A Translation-based Approach](https://arxiv.org/abs/2509.20421)
*Reiner Hähnle,Cosimo Laneve,Adele Veschetti*

Main category: cs.SE

TL;DR: Stipula语言通过转换为带JML注解的Java代码，使用KeY工具自动验证资产转移法律合同的正确性


<details>
  <summary>Details</summary>
Motivation: 需要为法律合同（特别是涉及资产转移和义务的合同）提供形式化验证方法，确保合同条款的正确性和可执行性

Method: 将Stipula合同翻译成带有Java Modeling Language规范的Java代码，然后使用KeY演绎验证工具进行自动验证

Result: 成功实现了对具有不相交循环的大型Stipula合同子集的部分和完全正确性的全自动验证

Conclusion: 通用演绎验证工具可以通过翻译方法成功应用于领域特定语言的形式化验证

Abstract: Stipula is a domain-specific programming language designed to model legal
contracts with enforceable properties, especially those involving asset
transfers and obligations. This paper presents a methodology to formally verify
the correctness of Stipula contracts through translation into Java code
annotated with Java Modeling Language specifications. As a verification
backend, the deductive verification tool KeY is used. Both, the translation and
the verification of partial and total correctness for a large subset of Stipula
contracts, those with disjoint cycles, is fully automatic. Our work
demonstrates that a general-purpose deductive verification tool can be used
successfully in a translation approach.

</details>


### [7] [AI-Specific Code Smells: From Specification to Detection](https://arxiv.org/abs/2509.20491)
*Brahim Mahmoudi,Naouel Moha,Quentin Stievenert,Florent Avellaneda*

Main category: cs.SE

TL;DR: SpecDetect4AI是一个基于DSL的静态分析工具，专门用于检测AI系统中的代码异味，在826个AI系统上达到88.66%的精确度和88.89%的召回率。


<details>
  <summary>Details</summary>
Motivation: AI系统的兴起带来了新的软件问题，现有检测工具往往无法识别AI特定的代码异味，这些问题可能导致不可复现性、静默失败或模型泛化能力差等深层问题。

Method: 开发了SpecDetect4AI工具，结合高级声明式领域特定语言(DSL)进行规则规范，以及可扩展的静态分析工具来解析和检测AI系统中的代码异味规则。

Result: 在826个AI系统(2000万行代码)上评估，实现了88.66%的精确度和88.89%的召回率，优于现有检测工具，SUS评分为81.7/100。

Conclusion: SpecDetect4AI通过专用规则有效支持AI特定代码异味的规范和检测，能够高效分析大型AI系统，展示了良好的效率和可扩展性。

Abstract: The rise of Artificial Intelligence (AI) is reshaping how software systems
are developed and maintained. However, AI-based systems give rise to new
software issues that existing detection tools often miss. Among these, we focus
on AI-specific code smells, recurring patterns in the code that may indicate
deeper problems such as unreproducibility, silent failures, or poor model
generalization. We introduce SpecDetect4AI, a tool-based approach for the
specification and detection of these code smells at scale. This approach
combines a high-level declarative Domain-Specific Language (DSL) for rule
specification with an extensible static analysis tool that interprets and
detects these rules for AI-based systems. We specified 22 AI-specific code
smells and evaluated SpecDetect4AI on 826 AI-based systems (20M lines of code),
achieving a precision of 88.66% and a recall of 88.89%, outperforming other
existing detection tools. Our results show that SpecDetect4AI supports the
specification and detection of AI-specific code smells through dedicated rules
and can effectively analyze large AI-based systems, demonstrating both
efficiency and extensibility (SUS 81.7/100).

</details>


### [8] [PromptDebt: A Comprehensive Study of Technical Debt Across LLM Projects](https://arxiv.org/abs/2509.20497)
*Ahmed Aljohani,Hyunsook Do*

Main category: cs.SE

TL;DR: LLM集成带来技术债务：54.49%来自OpenAI，12.35%来自LangChain，提示设计是主要债务来源(6.61%)


<details>
  <summary>Details</summary>
Motivation: 随着LLM通过API集成到软件中，产生了特有的技术债务问题，需要系统研究其来源、普遍性和缓解策略

Method: 分析93,142个Python文件，涵盖主要LLM API，识别LLM特有的技术债务实例及其分布

Result: 发现提示设计是主要债务来源(6.61%)，指令提示(38.60%)和少样本提示(18.13%)最易产生债务，OpenAI集成占54.49%债务

Conclusion: LLM集成确实带来显著技术债务，需要关注提示设计和参数调优，提供了数据集和实践指导来管理LLM系统的技术债务

Abstract: Large Language Models (LLMs) are increasingly embedded in software via APIs
like OpenAI, offering powerful AI features without heavy infrastructure. Yet
these integrations bring their own form of self-admitted technical debt (SATD).
In this paper, we present the first large-scale empirical study of LLM-specific
SATD: its origins, prevalence, and mitigation strategies. By analyzing 93,142
Python files across major LLM APIs, we found that 54.49% of SATD instances stem
from OpenAI integrations and 12.35% from LangChain use. Prompt design emerged
as the primary source of LLM-specific SATD, with 6.61% of debt related to
prompt configuration and optimization issues, followed by hyperparameter tuning
and LLM-framework integration. We further explored which prompt techniques
attract the most debt, revealing that instruction-based prompts (38.60%) and
few-shot prompts (18.13%) are particularly vulnerable due to their dependence
on instruction clarity and example quality. Finally, we release a comprehensive
SATD dataset to support reproducibility and offer practical guidance for
managing technical debt in LLM-powered systems.

</details>


### [9] [Enhancing Python Programming Education with an AI-Powered Code Helper: Design, Implementation, and Impact](https://arxiv.org/abs/2509.20518)
*Sayed Mahbub Hasan Amiri,Md Mainul Islam*

Main category: cs.SE

TL;DR: 基于AI-Python的编程学习聊天机器人，结合静态代码分析、动态执行追踪和LLM，帮助学生解决编程问题，在错误解决率、调试时间和编程能力提升方面显著优于传统工具。


<details>
  <summary>Details</summary>
Motivation: 传统IDE和静态分析工具缺乏机器人辅助，而AI代码助手如GitHub Copilot主要关注代码完成而非学习过程。需要填补这一空白，提供既能解决问题又能促进学习的工具。

Method: 采用混合架构：CodeLlama用于代码嵌入，GPT-4处理自然语言交互，Docker沙箱确保安全执行。结合静态代码分析和动态执行追踪技术。

Result: 在1500份学生提交中达到85%的错误解决成功率，优于pylint(62%)和GPT-4(73%)。调试时间减少59.3%，编程能力提升34%，特别是在递归和异常处理方面。

Conclusion: 该研究为优先考虑教育公平和长期技能保留而非单纯代码完成的AI工具提供了蓝图，展示了AI如何增强人类教学，促进编程教育中更深层次的概念理解。

Abstract: This is the study that presents an AI-Python-based chatbot that helps
students to learn programming by demonstrating solutions to such problems as
debugging errors, solving syntax problems or converting abstract theoretical
concepts to practical implementations. Traditional coding tools like Integrated
Development Environments (IDEs) and static analyzers do not give robotic help
while AI-driven code assistants such as GitHub Copilot focus on getting things
done. To close this gap, our chatbot combines static code analysis, dynamic
execution tracing, and large language models (LLMs) to provide the students
with relevant and practical advice, hence promoting the learning process. The
chatbots hybrid architecture employs CodeLlama for code embedding, GPT-4 for
natural language interactions, and Docker-based sandboxing for secure
execution. Evaluated through a mixed-methods approach involving 1,500 student
submissions, the system demonstrated an 85% error resolution success rate,
outperforming standalone tools like pylint (62%) and GPT-4 (73%). Quantitative
results revealed a 59.3% reduction in debugging time among users, with pre- and
post-test assessments showing a 34% improvement in coding proficiency,
particularly in recursion and exception handling. Qualitative feedback from 120
students highlighted the chatbots clarity, accessibility, and
confidence-building impact, though critiques included occasional latency and
restrictive code sanitization. By balancing technical innovation with
pedagogical empathy, this research provides a blueprint for AI tools that
prioritize educational equity and long-term skill retention over mere code
completion. The chatbot exemplifies how AI can augment human instruction,
fostering deeper conceptual understanding in programming education.

</details>


### [10] [Enhancing LLM-based Fault Localization with a Functionality-Aware Retrieval-Augmented Generation Framework](https://arxiv.org/abs/2509.20552)
*Xinyu Shi,Zhenhao Li,An Ran Chen*

Main category: cs.SE

TL;DR: FaR-Loc是一个基于检索增强生成(RAG)的故障定位框架，通过LLM功能提取、语义密集检索和LLM重排序三个组件，显著提升了方法级故障定位的准确性。


<details>
  <summary>Details</summary>
Motivation: 传统LLM在复杂系统故障定位中面临项目特定知识缺乏和大项目导航困难的问题，需要一种能够有效整合代码语义理解的方法。

Method: FaR-Loc采用三阶段方法：1) LLM生成失败行为的自然语言描述；2) 预训练编码器在共享语义空间中检索功能相似的方法；3) LLM基于上下文相关性对检索结果重排序。

Result: 在Defects4J基准测试中，FaR-Loc在Top-1准确率上比SoapFL和AutoFL分别提升14.6%和9.1%，在Top-5准确率上分别提升19.2%和22.1%，且无需重新训练。

Conclusion: FaR-Loc通过RAG技术有效结合LLM和代码语义理解，显著提升了故障定位性能，特别是采用包含代码结构的预训练嵌入模型可带来49.0%的性能提升。

Abstract: Fault localization (FL) is a critical but time-consuming task in software
debugging, aiming to identify faulty code elements. While recent advances in
large language models (LLMs) have shown promise for FL, they often struggle
with complex systems due to the lack of project-specific knowledge and the
difficulty of navigating large projects. To address these limitations, we
propose FaR-Loc, a novel framework that enhances method-level FL by integrating
LLMs with retrieval-augmented generation (RAG). FaR-Loc consists of three key
components: LLM Functionality Extraction, Semantic Dense Retrieval, and LLM
Re-ranking. First, given a failed test and its associated stack trace, the LLM
Functionality Extraction module generates a concise natural language
description that captures the failing behavior. Next, the Semantic Dense
Retrieval component leverages a pre-trained code-understanding encoder to embed
both the functionality description (natural language) and the covered methods
(code) into a shared semantic space, enabling the retrieval of methods with
similar functional behavior. Finally, the LLM Re-ranking module reorders the
retrieved methods based on their contextual relevance. Our experiments on the
widely used Defects4J benchmark show that FaR-Loc outperforms state-of-the-art
LLM-based baselines SoapFL and AutoFL, by 14.6% and 9.1% in Top-1 accuracy, by
19.2% and 22.1% in Top-5 accuracy, respectively. It also surpasses all
learning-based and spectrum-based baselines across all Top-N metrics without
requiring re-training. Furthermore, we find that pre-trained code embedding
models that incorporate code structure, such as UniXcoder, can significantly
improve fault localization performance by up to 49.0% in Top-1 accuracy.
Finally, we conduct a case study to illustrate the effectiveness of FaR-Loc and
to provide insights for its practical application.

</details>


### [11] [Design, Implementation and Evaluation of a Novel Programming Language Topic Classification Workflow](https://arxiv.org/abs/2509.20631)
*Michael Zhang,Yuan Tian,Mariam Guizani*

Main category: cs.SE

TL;DR: 提出了一种新的编程语言主题分类工作流，结合多标签SVM和滑动窗口投票策略，在IBM CodeNet数据集上实现平均F1分数0.90，可用于代码分析和数据驱动的软件工程


<details>
  <summary>Details</summary>
Motivation: 随着软件系统规模和复杂度的增长，理解源代码中编程语言主题的分布对于技术决策、改进入职流程以及指导工具和教育变得越来越重要

Method: 设计了一种结合多标签支持向量机(SVM)与滑动窗口和投票策略的工作流，能够精确定位核心语言概念如运算符重载、虚函数、继承和模板

Result: 在IBM Project CodeNet数据集上训练，模型在主题分类上达到平均F1分数0.90，在代码-主题高亮上达到0.75

Conclusion: 研究成果为代码分析和数据驱动软件工程提供了实证见解和可复用的分析管道

Abstract: As software systems grow in scale and complexity, understanding the
distribution of programming language topics within source code becomes
increasingly important for guiding technical decisions, improving onboarding,
and informing tooling and education. This paper presents the design,
implementation, and evaluation of a novel programming language topic
classification workflow. Our approach combines a multi-label Support Vector
Machine (SVM) with a sliding window and voting strategy to enable fine-grained
localization of core language concepts such as operator overloading, virtual
functions, inheritance, and templates. Trained on the IBM Project CodeNet
dataset, our model achieves an average F1 score of 0.90 across topics and 0.75
in code-topic highlight. Our findings contribute empirical insights and a
reusable pipeline for researchers and practitioners interested in code analysis
and data-driven software engineering.

</details>


### [12] [Exploring Engagement in Hybrid Meetings](https://arxiv.org/abs/2509.20780)
*Daniela Grassi,Fabio Calefato,Darja Smite,Nicole Novielli,Filippo Lanubile*

Main category: cs.SE

TL;DR: 混合工作模式下，远程和现场参与者在会议参与度上表现相当，但远程参与者在长会议中参与度较低。主动角色、会议规模和时段是影响参与度的关键因素。


<details>
  <summary>Details</summary>
Motivation: COVID-19疫情后混合工作模式普及，软件团队面临沟通协作新挑战，需要了解混合会议中远程参与者的参与度差异和影响因素。

Method: 采用多模态方法，通过自填问卷和生物识别设备收集三家软件公司专业人员在混合会议中的生理数据和自我报告数据，进行回归分析。

Result: 现场和远程参与者参与度相当，但远程参与者在长会议中参与度较低；主动角色与高参与度正相关，大型会议和下午时段与低参与度相关。

Conclusion: 研究结果为混合会议中的参与度影响因素提供了见解，提出了会议改进建议，这些发现对软件团队和其他知识密集型组织具有参考价值。

Abstract: Background. The widespread adoption of hybrid work following the COVID-19
pandemic has fundamentally transformed software development practices,
introducing new challenges in communication and collaboration as organizations
transition from traditional office-based structures to flexible working
arrangements. This shift has established a new organizational norm where even
traditionally office-first companies now embrace hybrid team structures. While
remote participation in meetings has become commonplace in this new
environment, it may lead to isolation, alienation, and decreased engagement
among remote team members. Aims. This study aims to identify and characterize
engagement patterns in hybrid meetings through objective measurements, focusing
on the differences between co-located and remote participants. Method. We
studied professionals from three software companies over several weeks,
employing a multimodal approach to measure engagement. Data were collected
through self-reported questionnaires and physiological measurements using
biometric devices during hybrid meetings to understand engagement dynamics.
Results. The regression analyses revealed comparable engagement levels between
onsite and remote participants, though remote participants show lower
engagement in long meetings regardless of participation mode. Active roles
positively correlate with higher engagement, while larger meetings and
afternoon sessions are associated with lower engagement. Conclusions. Our
results offer insights into factors associated with engagement and
disengagement in hybrid meetings, as well as potential meeting improvement
recommendations. These insights are potentially relevant not only for software
teams but also for knowledge-intensive organizations across various sectors
facing similar hybrid collaboration challenges.

</details>


### [13] [Verification Limits Code LLM Training](https://arxiv.org/abs/2509.20837)
*Srishti Gureja,Elena Tommasone,Jingyi He,Sara Hooker,Matthias Gallé,Marzieh Fadaee*

Main category: cs.SE

TL;DR: 本文研究发现代码生成中的验证瓶颈问题，指出当前验证方法过于严格，过滤了有价值的多样性数据。通过调整验证策略（如放宽通过标准、使用LLM软验证）可以提升模型性能2-4个百分点。


<details>
  <summary>Details</summary>
Motivation: 随着代码生成模型越来越依赖合成数据，验证能力成为训练数据质量和多样性的瓶颈，即验证天花板问题。需要系统研究验证策略如何影响模型性能。

Method: 系统研究验证设计和策略：(1)分析测试复杂度和数量的影响；(2)探索放宽通过阈值的方法；(3)通过控制对比和人工评估验证必要性

Result: 更丰富的测试套件平均提升3个pass@1点；放宽验证标准可回收有价值训练数据，带来2-4点性能提升；保留多样化正确解能带来一致泛化收益

Conclusion: 当前验证实践过于严格，但不能完全丢弃。通过校准验证与多样化问题-解决方案对的结合，可以突破验证天花板，开发更强的代码生成模型。

Abstract: Large language models for code generation increasingly rely on synthetic
data, where both problem solutions and verification tests are generated by
models. While this enables scalable data creation, it introduces a previously
unexplored bottleneck: the verification ceiling, in which the quality and
diversity of training data are fundamentally constrained by the capabilities of
synthetic verifiers. In this work, we systematically study how verification
design and strategies influence model performance. We investigate (i) what we
verify by analyzing the impact of test complexity and quantity: richer test
suites improve code generation capabilities (on average +3 pass@1), while
quantity alone yields diminishing returns, (ii) how we verify by exploring
relaxed pass thresholds: rigid 100% pass criteria can be overly restrictive. By
allowing for relaxed thresholds or incorporating LLM-based soft verification,
we can recover valuable training data, leading to a 2-4 point improvement in
pass@1 performance. However, this benefit is contingent upon the strength and
diversity of the test cases used, and (iii) why verification remains necessary
through controlled comparisons of formally correct versus incorrect solutions
and human evaluation: retaining diverse correct solutions per problem yields
consistent generalization gains. Our results show that Verification as
currently practiced is too rigid, filtering out valuable diversity. But it
cannot be discarded, only recalibrated. By combining calibrated verification
with diverse, challenging problem-solution pairs, we outline a path to break
the verification ceiling and unlock stronger code generation models.

</details>


### [14] [PseudoBridge: Pseudo Code as the Bridge for Better Semantic and Logic Alignment in Code Retrieval](https://arxiv.org/abs/2509.20881)
*Yixuan Li,Xinyi Liu,Weidong Yang,Ben Fei,Shuhao Li,Mingjie Zhou,Lipeng Ma*

Main category: cs.SE

TL;DR: PseudoBridge是一个新颖的代码检索框架，通过引入伪代码作为中间模态来更好地对齐自然语言查询和编程语言逻辑，显著提升了代码搜索的准确性和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有基于预训练语言模型的代码搜索方法面临两个关键挑战：人类意图与机器执行逻辑之间的语义鸿沟，以及对不同代码风格的有限鲁棒性。

Method: PseudoBridge采用两阶段方法：1) 使用大语言模型合成伪代码，实现自然语言查询与伪代码的显式对齐；2) 引入逻辑不变代码风格增强策略，生成风格多样但逻辑等价的代码实现，增强模型对代码风格变化的鲁棒性。

Result: 在10个不同预训练语言模型和6种主流编程语言上的广泛实验表明，PseudoBridge始终优于基线方法，在检索准确性和泛化能力方面取得显著提升，特别是在零样本领域迁移场景下表现突出。

Conclusion: 通过伪代码实现显式逻辑对齐是有效的，PseudoBridge展示了作为鲁棒、可泛化代码检索解决方案的潜力。

Abstract: Code search aims to precisely find relevant code snippets that match natural
language queries within massive codebases, playing a vital role in software
development. Recent advances leverage pre-trained language models (PLMs) to
bridge the semantic gap between unstructured natural language (NL) and
structured programming languages (PL), yielding significant improvements over
traditional information retrieval and early deep learning approaches. However,
existing PLM-based methods still encounter key challenges, including a
fundamental semantic gap between human intent and machine execution logic, as
well as limited robustness to diverse code styles. To address these issues, we
propose PseudoBridge, a novel code retrieval framework that introduces
pseudo-code as an intermediate, semi-structured modality to better align NL
semantics with PL logic. Specifically, PseudoBridge consists of two stages.
First, we employ an advanced large language model (LLM) to synthesize
pseudo-code, enabling explicit alignment between NL queries and pseudo-code.
Second, we introduce a logic-invariant code style augmentation strategy and
employ the LLM to generate stylistically diverse yet logically equivalent code
implementations with pseudo-code, then align the code snippets of different
styles with pseudo-code, enhancing model robustness to code style variation. We
build PseudoBridge across 10 different PLMs and evaluate it on 6 mainstream
programming languages. Extensive experiments demonstrate that PseudoBridge
consistently outperforms baselines, achieving significant gains in retrieval
accuracy and generalization, particularly under zero-shot domain transfer
scenarios such as Solidity and XLCoST datasets. These results demonstrate the
effectiveness of explicit logical alignment via pseudo-code and highlight
PseudoBridge's potential as a robust, generalizable solution for code
retrieval.

</details>


### [15] [Designing for Novice Debuggers: A Pilot Study on an AI-Assisted Debugging Tool](https://arxiv.org/abs/2509.21067)
*Oka Kurniawan,Erick Chandra,Christopher M. Poskitt,Yannic Noller,Kenny Tsu Wei Choo,Cyrille Jegourel*

Main category: cs.SE

TL;DR: CodeHinter是一个结合传统调试工具和LLM技术的调试助手，旨在帮助编程新手修复语义错误，同时促进他们在调试过程中的主动参与。


<details>
  <summary>Details</summary>
Motivation: 现有的AI辅助调试工具容易导致学生对AI过度依赖，缺乏主动参与调试过程，需要设计既能有效修复错误又能促进学生参与的工具。

Method: 结合传统调试工具和基于大语言模型的技术，开发CodeHinter调试助手，并通过本科生群体进行测试和迭代设计。

Result: 学生认为该工具在解决语义错误方面非常有效，且比第一版显著更容易使用；错误定位是最有价值的功能。

Conclusion: AI辅助调试工具应该基于用户画像进行个性化定制，以优化与学生的交互效果。

Abstract: Debugging is a fundamental skill that novice programmers must develop.
Numerous tools have been created to assist novice programmers in this process.
Recently, large language models (LLMs) have been integrated with automated
program repair techniques to generate fixes for students' buggy code. However,
many of these tools foster an over-reliance on AI and do not actively engage
students in the debugging process. In this work, we aim to design an intuitive
debugging assistant, CodeHinter, that combines traditional debugging tools with
LLM-based techniques to help novice debuggers fix semantic errors while
promoting active engagement in the debugging process. We present findings from
our second design iteration, which we tested with a group of undergraduate
students. Our results indicate that the students found the tool highly
effective in resolving semantic errors and significantly easier to use than the
first version. Consistent with our previous study, error localization was the
most valuable feature. Finally, we conclude that any AI-assisted debugging tool
should be personalized based on user profiles to optimize their interactions
with students.

</details>


### [16] [An Improved Quantum Software Challenges Classification Approach using Transfer Learning and Explainable AI](https://arxiv.org/abs/2509.21068)
*Nek Dil Khan,Javed Ali Khan,Mobashir Husain,Muhammad Sohail Khan,Arif Ali Khan,Muhammad Azeem Akbar,Shahid Hussain*

Main category: cs.SE

TL;DR: 该研究通过分析Stack Overflow上的量子计算相关讨论，识别量子软件工程中的常见挑战，并使用Transformer模型实现95%的分类准确率，为量子开发者社区提供更好的讨论组织方式。


<details>
  <summary>Details</summary>
Motivation: 量子开发者面临优化量子计算和量子软件工程概念的挑战，他们在Stack Overflow等平台上讨论问题时使用的量子标签往往侧重于技术方面而非开发者实际问题。需要系统分类这些问题来识别频繁出现的QSE挑战。

Method: 从问答平台提取2829个量子相关标签的问题，通过内容分析和扎根理论识别常见挑战类别（工具、理论、学习、概念、错误、API使用），使用ChatGPT验证人工标注并解决分歧，最后使用BERT、DistilBERT和RoBERTa等Transformer模型进行分类。

Result: Transformer模型平均准确率达到95%，优于传统深度学习模型（FNN 89%、CNN 86%、LSTM 84%），准确率提升6%。使用SHAP进行模型可解释性分析，揭示了语言特征如何驱动预测。

Conclusion: 该方法能有效帮助量子供应商和论坛更好地组织讨论内容，提高可访问性和可读性。但还需要与实际开发者和供应商进行实证评估研究。

Abstract: Quantum Software Engineering (QSE) is a research area practiced by tech
firms. Quantum developers face challenges in optimizing quantum computing and
QSE concepts. They use Stack Overflow (SO) to discuss challenges and label
posts with specialized quantum tags, which often refer to technical aspects
rather than developer posts. Categorizing questions based on quantum concepts
can help identify frequent QSE challenges. We conducted studies to classify
questions into various challenges. We extracted 2829 questions from Q&A
platforms using quantum-related tags. Posts were analyzed to identify frequent
challenges and develop a novel grounded theory. Challenges include Tooling,
Theoretical, Learning, Conceptual, Errors, and API Usage. Through content
analysis and grounded theory, discussions were annotated with common challenges
to develop a ground truth dataset. ChatGPT validated human annotations and
resolved disagreements. Fine-tuned transformer algorithms, including BERT,
DistilBERT, and RoBERTa, classified discussions into common challenges. We
achieved an average accuracy of 95% with BERT DistilBERT, compared to
fine-tuned Deep and Machine Learning (D&ML) classifiers, including Feedforward
Neural Networks (FNN), Convolutional Neural Networks (CNN), and Long Short-Term
Memory networks (LSTM), which achieved accuracies of 89%, 86%, and 84%,
respectively. The Transformer-based approach outperforms the D&ML-based
approach with a 6\% increase in accuracy by processing actual discussions,
i.e., without data augmentation. We applied SHAP (SHapley Additive
exPlanations) for model interpretability, revealing how linguistic features
drive predictions and enhancing transparency in classification. These findings
can help quantum vendors and forums better organize discussions for improved
access and readability. However,empirical evaluation studies with actual
developers and vendors are needed.

</details>


### [17] [Fine-Tuning LLMs to Analyze Multiple Dimensions of Code Review: A Maximum Entropy Regulated Long Chain-of-Thought Approach](https://arxiv.org/abs/2509.21170)
*Yongda Yu,Guohao Shi,Xianwei Wu,Haochuan He,XueMing Gu,Qianqian Zhao,Kui Liu,Qiushi Wang,Zhao Tian,Haifeng Shen,Guoping Rong*

Main category: cs.SE

TL;DR: MelcotCR是一种基于思维链的微调方法，通过长链思维技术为代码审查提供丰富的结构化信息，使小参数模型在代码问题检测和描述准确性上达到甚至超越大模型性能。


<details>
  <summary>Details</summary>
Motivation: 现有LLM在代码审查中受限于训练数据，无法像人类评审员那样同时分析多个维度。虽然通过代码审查数据微调能提升性能，但由于信息有限或模糊，这些方法的潜力未能充分发挥。

Method: 提出MelcotCR方法，结合最大熵建模原则和预定义推理路径，利用长思维链技术提供结构化信息，解决LLM处理长提示时的上下文丢失和推理逻辑丢失问题。

Result: 在MelcotCR数据集和CodeReviewer数据集上的实验表明，14B参数的Qwen2.5模型经过MelcotCR微调后，在代码问题检测和描述准确性上超越了现有最优方法，性能与671B的DeepSeek-R1模型相当。

Conclusion: MelcotCR通过思维链微调和结构化推理路径，有效提升了LLM在代码审查任务中的多维度分析能力，使小模型能够达到大模型的性能水平。

Abstract: Large Language Models (LLMs) have shown great potential in supporting
automated code review due to their impressive capabilities in context
understanding and reasoning. However, these capabilities are still limited
compared to human-level cognition because they are heavily influenced by the
training data. Recent research has demonstrated significantly improved
performance through fine-tuning LLMs with code review data. However, compared
to human reviewers who often simultaneously analyze multiple dimensions of code
review to better identify issues, the full potential of these methods is
hampered by the limited or vague information used to fine-tune the models. This
paper contributes MelcotCR, a chain-of-thought (COT) fine-tuning approach that
trains LLMs with an impressive reasoning ability to analyze multiple dimensions
of code review by harnessing long COT techniques to provide rich structured
information. To address context loss and reasoning logic loss issues that
frequently occur when LLMs process long COT prompts, we propose a solution that
combines the Maximum Entropy (ME) modeling principle with pre-defined reasoning
pathways in MelcotCR to enable more effective utilization of in-context
knowledge within long COT prompts while strengthening the logical tightness of
the reasoning process. Empirical evaluations on our curated MelcotCR dataset
and the public CodeReviewer dataset reveal that a low-parameter base model,
such as 14B Qwen2.5, fine-tuned with MelcotCR can surpass state-of-the-art
methods in terms of the accuracy of detecting and describing code issues, with
its performance remarkably on par with that of the 671B DeepSeek-R1 model.

</details>


### [18] [Semantic Clustering of Civic Proposals: A Case Study on Brazil's National Participation Platform](https://arxiv.org/abs/2509.21292)
*Ronivaldo Ferreira,Guilherme da Silva,Carla Rocha,Gustavo Pinto*

Main category: cs.SE

TL;DR: 提出了一种结合BERTopic、种子词和LLM自动验证的方法，用于大规模处理公民参与平台上的贡献内容，实现自动分类并与官方分类法对齐。


<details>
  <summary>Details</summary>
Motivation: 数字平台上公民参与量大但难以有效利用，手动分类不可行，需要专家参与且必须与官方分类法对齐。

Method: 使用BERTopic主题建模结合种子词引导，并通过大型语言模型进行自动验证。

Result: 生成的主题具有连贯性且与机构分类法对齐，所需人工干预极少。

Conclusion: 该方法能将大量公民输入转化为可用于公共政策的可操作数据。

Abstract: Promoting participation on digital platforms such as Brasil Participativo has
emerged as a top priority for governments worldwide. However, due to the sheer
volume of contributions, much of this engagement goes underutilized, as
organizing it presents significant challenges: (1) manual classification is
unfeasible at scale; (2) expert involvement is required; and (3) alignment with
official taxonomies is necessary. In this paper, we introduce an approach that
combines BERTopic with seed words and automatic validation by large language
models. Initial results indicate that the generated topics are coherent and
institutionally aligned, with minimal human effort. This methodology enables
governments to transform large volumes of citizen input into actionable data
for public policy.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [19] [FZModules: A Heterogeneous Computing Framework for Customizable Scientific Data Compression Pipelines](https://arxiv.org/abs/2509.20563)
*Skyler Ruiter,Jiannan Tian,Fengguang Song*

Main category: cs.DC

TL;DR: FZModules是一个异构框架，用于通过高性能模块组装误差有界的自定义压缩流水线，提供异步执行和数据依赖管理，在保持压缩质量的同时实现高速压缩。


<details>
  <summary>Details</summary>
Motivation: 现代科学模拟和仪器生成的数据量超过了内存和存储容量，限制了可扩展性。现有GPU压缩器虽然提供高吞吐量，但通常采用硬编码的内核，阻碍快速实验，且在率失真性能上表现不佳。

Method: 提出了FZModules框架，通过简洁可扩展的接口从高性能模块组装误差有界的自定义压缩流水线，并使用异步任务支持执行库来推断数据依赖、管理内存移动，并暴露分支和阶段级并发。

Result: 在四个代表性科学数据集上评估三个基于FZModules构建的流水线，结果显示它们能够达到与融合内核GPU压缩器相当的端到端加速比，同时实现与更高保真度的CPU或混合压缩器相似的率失真性能。

Conclusion: FZModules框架支持快速、领域定制的压缩流水线设计，在保持压缩质量的同时提供高性能压缩能力，解决了科学数据压缩中的可扩展性和定制化需求。

Abstract: Modern scientific simulations and instruments generate data volumes that
overwhelm memory and storage, throttling scalability. Lossy compression
mitigates this by trading controlled error for reduced footprint and throughput
gains, yet optimal pipelines are highly data and objective specific, demanding
compression expertise. GPU compressors supply raw throughput but often
hard-code fused kernels that hinder rapid experimentation, and underperform in
rate-distortion. We present FZModules, a heterogeneous framework for assembling
error-bounded custom compression pipelines from high-performance modules
through a concise extensible interface. We further utilize an asynchronous
task-backed execution library that infers data dependencies, manages memory
movement, and exposes branch and stage level concurrency for powerful
asynchronous compression pipelines. Evaluating three pipelines built with
FZModules on four representative scientific datasets, we show they can compare
end-to-end speedup of fused-kernel GPU compressors while achieving similar
rate-distortion to higher fidelity CPU or hybrid compressors, enabling rapid,
domain-tailored design.

</details>


### [20] [Experience Deploying Containerized GenAI Services at an HPC Center](https://arxiv.org/abs/2509.20603)
*Angel M. Beltre,Jeff Ogden,Kevin Pedretti*

Main category: cs.DC

TL;DR: 本文分享了在高性能计算（HPC）中心部署生成式AI工作负载的经验，讨论了HPC与云计算环境的集成方法，提出了融合计算架构来支持容器化的GenAI应用部署。


<details>
  <summary>Details</summary>
Motivation: 生成式AI应用通常基于容器化组件构建，但在HPC中心的部署能力仍在发展中，需要探索HPC与云计算环境的有效集成方案。

Method: 提出融合计算架构，整合HPC和Kubernetes平台来运行容器化的GenAI工作负载；通过案例研究展示了在多容器运行时环境下跨平台部署Llama大语言模型的方法。

Result: 成功实现了在HPC环境中部署容器化GenAI工作负载，验证了融合架构的可行性，并为HPC容器社区提供了实用的部署经验和指导。

Conclusion: 该研究为HPC中心部署GenAI应用提供了实践指导，指出了未来研究和工具开发的方向，促进了HPC与云计算环境的有效融合。

Abstract: Generative Artificial Intelligence (GenAI) applications are built from
specialized components -- inference servers, object storage, vector and graph
databases, and user interfaces -- interconnected via web-based APIs. While
these components are often containerized and deployed in cloud environments,
such capabilities are still emerging at High-Performance Computing (HPC)
centers. In this paper, we share our experience deploying GenAI workloads
within an established HPC center, discussing the integration of HPC and cloud
computing environments. We describe our converged computing architecture that
integrates HPC and Kubernetes platforms running containerized GenAI workloads,
helping with reproducibility. A case study illustrates the deployment of the
Llama Large Language Model (LLM) using a containerized inference server (vLLM)
across both Kubernetes and HPC platforms using multiple container runtimes. Our
experience highlights practical considerations and opportunities for the HPC
container community, guiding future research and tool development.

</details>


### [21] [Distributed-memory Algorithms for Sparse Matrix Permutation, Extraction, and Assignment](https://arxiv.org/abs/2509.20776)
*Elaheh Hassani,Md Taufique Hussain,Ariful Azad*

Main category: cs.DC

TL;DR: 提出了分布式内存中稀疏矩阵置换、提取和赋值的高效算法，采用Identify-Exchange-Build策略减少通信开销，并通过无同步多线程加速本地计算，性能优于现有库。


<details>
  <summary>Details</summary>
Motivation: 现有分布式稀疏矩阵操作（如CombBLAS和PETSc）在通信和计算效率方面存在不足，需要更高效的算法来支持大规模稀疏矩阵处理应用。

Method: 采用Identify-Exchange-Build三阶段策略：识别要发送的非零元素、交换数据、构建本地子矩阵。使用无同步多线程算法加速本地计算。

Result: 在多个集群和超级计算机上的实验表明，该算法在通信和计算性能上显著优于CombBLAS和PETSc，适用于负载均衡、矩阵重排序、子图提取等应用场景。

Conclusion: 该研究提供了稀疏矩阵操作的完整算法、软件实现和评估，为分布式内存环境中的高效稀疏矩阵处理提供了有效解决方案。

Abstract: We present scalable distributed-memory algorithms for sparse matrix
permutation, extraction, and assignment. Our methods follow an
Identify-Exchange-Build (IEB) strategy where each process identifies the local
nonzeros to be sent, exchanges the required data, and then builds its local
submatrix from the received elements. This approach reduces communication
compared to SpGEMM-based methods in distributed memory. By employing
synchronization-free multithreaded algorithms, we further accelerate local
computations, achieving substantially better performance than existing
libraries such as CombBLAS and PETSc. We design efficient software for these
operations and evaluate their performance on two university clusters and the
Perlmutter supercomputer. Our experiments span a variety of application
scenarios, including matrix permutation for load balancing, matrix reordering,
subgraph extraction, and streaming graph applications. In all cases, we compare
our algorithms against CombBLAS, the most comprehensive distributed library for
these operations, and, in some scenarios, against PETSc. Overall, this work
provides a comprehensive study of algorithms, software implementations,
experimental evaluations, and applications for sparse matrix permutation,
extraction, and assignment.

</details>


### [22] [Integrating and Characterizing HPC Task Runtime Systems for hybrid AI-HPC workloads](https://arxiv.org/abs/2509.20819)
*Andre Merzky,Mikhail Titov,Matteo Turilli,Shantenu Jha*

Main category: cs.DC

TL;DR: RADICAL-Pilot集成Flux和Dragon运行时系统，相比传统Slurm的srun，在HPC和AI混合工作负载中实现了更高的任务执行速率和资源利用率，任务吞吐量提升超过10倍，在药物发现应用中减少30-60%的执行时间。


<details>
  <summary>Details</summary>
Motivation: 科学工作流越来越多地同时包含HPC和机器学习任务，但传统的Slurm srun启动器在并发性和吞吐量方面存在限制，不适合动态异构工作负载。

Method: 将RADICAL-Pilot（RP）与Flux和Dragon两个互补的运行时系统集成，实现分层资源管理和高吞吐量函数执行，使用合成和生产规模工作负载在Frontier超级计算机上进行性能研究。

Result: RP+Flux维持930任务/秒，RP+Flux+Dragon超过1,500任务/秒，利用率超过99.6%；而srun峰值仅152任务/秒，利用率低于50%。在IMPECCABLE.v2药物发现活动中，RP+Flux相比srun/Slurm减少30-60%的执行时间，吞吐量提高4倍以上。

Conclusion: RP与运行时系统的混合集成是处理混合AI-HPC工作负载的可扩展方法，显著提升了任务执行性能和资源利用率。

Abstract: Scientific workflows increasingly involve both HPC and machine-learning
tasks, combining MPI-based simulations, training, and inference in a single
execution. Launchers such as Slurm's srun constrain concurrency and throughput,
making them unsuitable for dynamic and heterogeneous workloads. We present a
performance study of RADICAL-Pilot (RP) integrated with Flux and Dragon, two
complementary runtime systems that enable hierarchical resource management and
high-throughput function execution. Using synthetic and production-scale
workloads on Frontier, we characterize the task execution properties of RP
across runtime configurations. RP+Flux sustains up to 930 tasks/s, and
RP+Flux+Dragon exceeds 1,500 tasks/s with over 99.6% utilization. In contrast,
srun peaks at 152 tasks/s and degrades with scale, with utilization below 50%.
For IMPECCABLE.v2 drug discovery campaign, RP+Flux reduces makespan by 30-60%
relative to srun/Slurm and increases throughput more than four times on up to
1,024. These results demonstrate hybrid runtime integration in RP as a scalable
approach for hybrid AI-HPC workloads.

</details>


### [23] [RollPacker: Mitigating Long-Tail Rollouts for Fast, Synchronous RL Post-Training](https://arxiv.org/abs/2509.21009)
*Wei Gao,Yuheng Zhao,Dakai An,Tianyuan Wu,Lunxi Cao,Shaopan Xiong,Ju Huang,Weixun Wang,Siran Yang,Wenbo Su,Jiamang Wang,Lin Qu,Bo Zheng,Wei Wang*

Main category: cs.DC

TL;DR: 提出tail batching方法解决RL训练中GPU利用率低的问题，通过将长响应请求集中调度到特定轮次，显著加速训练而不损失精度


<details>
  <summary>Details</summary>
Motivation: 同步RL后训练存在GPU利用率低的问题，由响应长度不平衡导致的计算气泡造成。现有方法放松同步会牺牲训练精度

Method: 提出tail batching调度策略，将长尾响应请求集中到少量长轮次，大多数短轮次只处理平衡的短响应。开发RollPacker系统实现全流程优化

Result: 在128个H800 GPU上，相比veRL实现2.03-2.56倍端到端训练时间减少，相比RLHFuse最高2.24倍加速

Conclusion: tail batching能有效减少GPU空闲时间，显著加速RL训练同时保持精度，为LLM推理能力提升提供高效解决方案

Abstract: Reinforcement Learning (RL) is a pivotal post-training technique for
enhancing the reasoning capabilities of Large Language Models (LLMs). However,
synchronous RL post-training often suffers from significant GPU
underutilization, referred to as bubbles, caused by imbalanced response lengths
within rollout steps. Many RL systems attempt to alleviate this problem by
relaxing synchronization, but this can compromise training accuracy. In this
paper, we introduce tail batching, a novel rollout scheduling strategy for
synchronous RL that systematically consolidates prompts leading to long-tail
responses into a small subset of rollout steps (long rounds), while ensuring
that the majority of steps (short rounds) involve only balanced, short
rollouts. By excluding long responses from short rounds and rescheduling them
into a few designated long rounds, tail batching effectively reduces GPU idle
time during rollouts and significantly accelerates RL training without
sacrificing accuracy. We present RollPacker, a system that fully harnesses the
benefits of tail batching through holistic optimizations across all three RL
stages: elastic parallelism adaptation for rollout, dynamic resource allocation
and scheduling for reward, and stream-based training. Empirical results show
that RollPacker achieves a 2.03x-2.56x end-to-end training time reduction
compared to veRL and up to 2.24x speedup compared to RLHFuse for the Qwen2.5
family of LLMs on up to 128 H800 GPUs.

</details>


### [24] [Utilizing Sparsity in the GPU-accelerated Assembly of Schur Complement Matrices in Domain Decomposition Methods](https://arxiv.org/abs/2509.21037)
*Jakub Homola,Ondřej Meca,Lubomír Říha,Tomáš Brzobohatý*

Main category: cs.DC

TL;DR: 本文提出利用输入矩阵稀疏性优化GPU上Schur补矩阵组装的方法，在FETI方法中实现5.1倍GPU代码加速和3.3倍整体组装加速


<details>
  <summary>Details</summary>
Motivation: 随着高性能计算集群转向GPU架构，需要加速域分解方法中的Schur补矩阵组装，现有GPU组装方法仍有优化空间

Method: 通过智能利用输入矩阵的稀疏性来优化GPU上的Schur补矩阵组装过程

Result: 在FETI方法中实现了GPU代码部分5.1倍加速和整体组装3.3倍加速，从10次迭代开始就能体现加速效益

Conclusion: 利用稀疏性优化GPU组装可显著提升Schur补矩阵组装效率，使GPU加速在较少迭代次数下就能产生效益

Abstract: Schur complement matrices emerge in many domain decomposition methods that
can solve complex engineering problems using supercomputers. Today, as most of
the high-performance clusters' performance lies in GPUs, these methods should
also be accelerated.
  Typically, the offloaded components are the explicitly assembled dense Schur
complement matrices used later in the iterative solver for multiplication with
a vector. As the explicit assembly is expensive, it represents a significant
overhead associated with this approach to acceleration. It has already been
shown that the overhead can be minimized by assembling the Schur complements
directly on the GPU.
  This paper shows that the GPU assembly can be further improved by wisely
utilizing the sparsity of the input matrices. In the context of FETI methods,
we achieved a speedup of 5.1 in the GPU section of the code and 3.3 for the
whole assembly, making the acceleration beneficial from as few as 10
iterations.

</details>


### [25] [Mojo: MLIR-Based Performance-Portable HPC Science Kernels on GPUs for the Python Ecosystem](https://arxiv.org/abs/2509.21039)
*William F. Godoy,Tatiana Melnichenko,Pedro Valero-Lara,Wael Elwasif,Philip Fackler,Rafael Ferreira Da Silva,Keita Teranishi,Jeffrey S. Vetter*

Main category: cs.DC

TL;DR: Mojo语言在GPU科学计算中的性能与可移植性评估，显示其在内存密集型任务上与CUDA/HIP竞争，但在原子操作和快速数学计算密集型任务上存在差距。


<details>
  <summary>Details</summary>
Motivation: 探索基于MLIR的新型Mojo语言在科学计算工作负载中的性能和可移植性，旨在弥合Python生态系统在科学计算与AI融合中的性能和生产效率差距。

Method: 针对四种科学计算工作负载（七点模板、BabelStream、miniBUDE、Hartree-Fock），在NVIDIA H100和AMD MI300A GPU上与供应商基线（CUDA和HIP）进行性能比较。

Result: Mojo在内存密集型内核上的性能与CUDA和HIP竞争，但在AMD GPU上的原子操作以及AMD和NVIDIA GPU上的快速数学计算密集型内核存在性能差距。

Conclusion: 尽管学习曲线和编程要求仍较低级，Mojo能够在科学计算与AI融合的碎片化Python生态系统中弥合重要差距。

Abstract: We explore the performance and portability of the novel Mojo language for
scientific computing workloads on GPUs. As the first language based on the
LLVM's Multi-Level Intermediate Representation (MLIR) compiler infrastructure,
Mojo aims to close performance and productivity gaps by combining Python's
interoperability and CUDA-like syntax for compile-time portable GPU
programming. We target four scientific workloads: a seven-point stencil
(memory-bound), BabelStream (memory-bound), miniBUDE (compute-bound), and
Hartree-Fock (compute-bound with atomic operations); and compare their
performance against vendor baselines on NVIDIA H100 and AMD MI300A GPUs. We
show that Mojo's performance is competitive with CUDA and HIP for memory-bound
kernels, whereas gaps exist on AMD GPUs for atomic operations and for fast-math
compute-bound kernels on both AMD and NVIDIA GPUs. Although the learning curve
and programming requirements are still fairly low-level, Mojo can close
significant gaps in the fragmented Python ecosystem in the convergence of
scientific computing and AI.

</details>


### [26] [From GPUs to RRAMs: Distributed In-Memory Primal-Dual Hybrid Gradient Method for Solving Large-Scale Linear Optimization Problem](https://arxiv.org/abs/2509.21137)
*Huynh Q. N. Vo,Md Tawsif Rahman Chowdhury,Paritosh Ramanan,Gozde Tutuncuoglu,Junchi Yang,Feng Qiu,Murat Yildirim*

Main category: cs.DC

TL;DR: 提出了一种针对RRAM阵列的分布式内存原始-对偶混合梯度方法，通过算法-硬件协同设计解决大规模线性规划问题，相比GPU加速求解器实现了三个数量级的能耗和延迟降低。


<details>
  <summary>Details</summary>
Motivation: 传统架构受限于基本物理极限，无法满足计算工作负载的指数增长需求。基于RRAM的内存计算虽然提供了有前景的替代方案，但现有算法不适用于内存计算，特别是对于需要频繁矩阵重编程的约束优化问题。

Method: 开发了分布式内存原始-对偶混合梯度方法，最小化昂贵的写入周期，包含对器件非理想性的鲁棒性，并利用对称块矩阵公式统一分布式交叉阵列的操作。集成了MELISO+物理仿真框架评估实际器件条件下的性能。

Result: 在大规模线性程序上与GPU加速求解器对比，RRAM求解器实现了相当的精度，同时能耗和延迟降低了三个数量级。

Conclusion: 这是首个在RRAM上实现的PDHG线性规划求解器，展示了算法-硬件协同设计通过分布式内存计算解决大规模优化问题的变革潜力。

Abstract: The exponential growth of computational workloads is surpassing the
capabilities of conventional architectures, which are constrained by
fundamental limits. In-memory computing (IMC) with RRAM provides a promising
alternative by providing analog computations with significant gains in latency
and energy use. However, existing algorithms developed for conventional
architectures do not translate to IMC, particularly for constrained
optimization problems where frequent matrix reprogramming remains
cost-prohibitive for IMC applications. Here we present a distributed in-memory
primal-dual hybrid gradient (PDHG) method, specifically co-designed for arrays
of RRAM devices. Our approach minimizes costly write cycles, incorporates
robustness against device non-idealities, and leverages a symmetric
block-matrix formulation to unify operations across distributed crossbars. We
integrate a physics-based simulation framework called MELISO+ to evaluate
performance under realistic device conditions. Benchmarking against
GPU-accelerated solvers on large-scale linear programs demonstrates that our
RRAM-based solver achieves comparable accuracy with up to three orders of
magnitude reductions in energy consumption and latency. These results
demonstrate the first PDHG-based LP solver implemented on RRAMs, showcasing the
transformative potential of algorithm-hardware co-design for solving
large-scale optimization through distributed in-memory computing.

</details>


### [27] [Data-Centric Elastic Pipeline Parallelism for Efficient Long-Context LLM Training](https://arxiv.org/abs/2509.21275)
*Shiju Wang,Yujie Wang,Ao Sun,Fangcheng Fu,Zijian Zhu,Bin Cui,Xu Han,Kaisheng Ma*

Main category: cs.DC

TL;DR: InfiniPipe系统通过弹性流水线并行(EPP)技术，自适应选择批级和令牌级流水线并行粒度，结合序列分割打包和协同优化调度，在长上下文训练中实现1.69倍加速。


<details>
  <summary>Details</summary>
Motivation: 现有流水线并行方案在长上下文训练中存在内存消耗高或硬件利用率低的权衡问题，且静态调度方法无法处理真实数据集中序列长度的偏斜分布，导致性能不佳。

Method: 提出弹性流水线并行(EPP)技术，包含：(1)资源感知和负载均衡的序列处理器，分割长序列并打包短序列；(2)协同优化方法，通过阶段感知的块级自适应检查点机制联合优化流水线调度和梯度检查点。

Result: 综合实验表明，InfiniPipe相比最先进系统实现了1.69倍的加速比。

Conclusion: EPP方法能够有效适应资源和负载异质性，通过动态粒度选择和协同优化解决了长上下文训练中的内存效率和硬件利用率问题。

Abstract: Long context training is crucial for LLM's context extension. Existing
schemes, such as sequence parallelism, incur substantial communication
overhead. Pipeline parallelism (PP) reduces this cost, but its effectiveness
hinges on partitioning granularity. Batch-level PP dividing input samples
exhibits high memory consumption in long-context scenario, whereas token-level
PP splitting sequences into slices alleviates memory overhead but may incur
hardware under-utilization. This trade-off motivates adaptively selecting PP
granularity to match resource and workload characteristics. Moreover, sequence
length distribution of the real-world dataset exhibits skewness, posing a
challenge on PP's workload balance and efficient scheduling. Current static PP
scheduling methods overlook the variance of sequence length, leading to
suboptimal performance. In this paper, we propose Elastic Pipeline Parallelism
(EPP) that orchestrates token-level PP and batch-level PP to adapt to resource
and workload heterogeneity. We build InfiniPipe, a distributed training system
that unleashes the potential of EPP via (1) a resource-aware and
workload-balanced sequence processor that splits long sequences and packs short
ones; and (2) a co-optimization methodology that jointly optimizes pipeline
schedule and gradient checkpointing via a mechanism named stage-aware
chunk-level adaptive checkpointing. Comprehensive experiments demonstrate that
InfiniPipe achieves a 1.69x speedup over state-of-the-art systems.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [28] [Structuring Collective Action with LLM-Guided Evolution: From Ill-Structured Problems to Executable Heuristics](https://arxiv.org/abs/2509.20412)
*Kevin Bradley Dsouza,Graham Alexander Watt,Yuri Leonenko,Juan Moreno-Cruz*

Main category: cs.MA

TL;DR: ECHO-MIMIC是一个计算框架，通过发现紧凑可执行的启发式规则和有说服力的理由，将集体行动问题转化为每个代理可处理的良好结构化问题。


<details>
  <summary>Details</summary>
Motivation: 集体行动问题需要将个体激励与集体目标对齐，是典型的非结构化问题。个体代理难以理解本地行动与全局结果之间的因果关系，利益相关者目标经常冲突，没有单一明确的算法能够连接微观选择和宏观福利。

Method: 框架分为两个阶段：ECHO通过进化搜索生成Python代码片段编码候选行为策略，MIMIC进化伴随的自然语言消息来说服代理采纳这些策略。两个阶段都使用大型语言模型驱动的进化搜索，LLM提出多样化的代码或文本变体，群体级选择保留那些在模拟环境中最大化集体性能的策略。

Result: 在农业景观管理的经典非结构化问题上验证，ECHO-MIMIC发现了比基线方法性能更高的启发式规则，并成功制作了定制消息，使模拟农民行为与景观级生态目标对齐。

Conclusion: 通过将算法规则发现与定制通信相结合，ECHO-MIMIC将集体行动的认知负担转化为简单的代理级指令，使以前非结构化的问题在实践中可解，为可扩展、自适应政策设计开辟了新路径。

Abstract: Collective action problems, which require aligning individual incentives with
collective goals, are classic examples of Ill-Structured Problems (ISPs). For
an individual agent, the causal links between local actions and global outcomes
are unclear, stakeholder objectives often conflict, and no single, clear
algorithm can bridge micro-level choices with macro-level welfare. We present
ECHO-MIMIC, a computational framework that converts this global complexity into
a tractable, Well-Structured Problem (WSP) for each agent by discovering
compact, executable heuristics and persuasive rationales. The framework
operates in two stages: ECHO (Evolutionary Crafting of Heuristics from
Outcomes) evolves snippets of Python code that encode candidate behavioral
policies, while MIMIC (Mechanism Inference & Messaging for
Individual-to-Collective Alignment) evolves companion natural language messages
that motivate agents to adopt those policies. Both phases employ a
large-language-model-driven evolutionary search: the LLM proposes diverse and
context-aware code or text variants, while population-level selection retains
those that maximize collective performance in a simulated environment. We
demonstrate this framework on a canonical ISP in agricultural landscape
management, where local farming decisions impact global ecological
connectivity. Results show that ECHO-MIMIC discovers high-performing heuristics
compared to baselines and crafts tailored messages that successfully align
simulated farmer behavior with landscape-level ecological goals. By coupling
algorithmic rule discovery with tailored communication, ECHO-MIMIC transforms
the cognitive burden of collective action into a simple set of agent-level
instructions, making previously ill-structured problems solvable in practice
and opening a new path toward scalable, adaptive policy design.

</details>


### [29] [RadAgents: Multimodal Agentic Reasoning for Chest X-ray Interpretation with Radiologist-like Workflows](https://arxiv.org/abs/2509.20490)
*Kai Zhang,Corey D Barrett,Jangwon Kim,Lichao Sun,Tara Taghavi,Krishnaram Kenthapadi*

Main category: cs.MA

TL;DR: RadAgents是一个用于胸部X光片解读的多智能体框架，通过结合临床先验知识和任务感知的多模态推理来解决现有方法的局限性，提供更可靠、透明且符合临床实践的输出。


<details>
  <summary>Details</summary>
Motivation: 现有胸部X光片解读方法存在三个主要问题：推理过程缺乏临床可解释性且不符合指南；多模态证据融合不足，仅生成文本解释而缺乏视觉基础；系统无法检测或解决工具间的不一致性，缺乏验证机制。

Method: 提出RadAgents多智能体框架，结合临床先验知识和任务感知的多模态推理，集成基础验证和多模态检索增强来验证和解决上下文冲突。

Result: 该系统能够产生更可靠、透明且与临床实践一致的输出结果。

Conclusion: RadAgents框架通过多智能体协作和验证机制，有效解决了胸部X光片解读中的关键挑战，为临床任务提供了更加可靠的解决方案。

Abstract: Agentic systems offer a potential path to solve complex clinical tasks
through collaboration among specialized agents, augmented by tool use and
external knowledge bases. Nevertheless, for chest X-ray (CXR) interpretation,
prevailing methods remain limited: (i) reasoning is frequently neither
clinically interpretable nor aligned with guidelines, reflecting mere
aggregation of tool outputs; (ii) multimodal evidence is insufficiently fused,
yielding text-only rationales that are not visually grounded; and (iii) systems
rarely detect or resolve cross-tool inconsistencies and provide no principled
verification mechanisms. To bridge the above gaps, we present RadAgents, a
multi-agent framework for CXR interpretation that couples clinical priors with
task-aware multimodal reasoning. In addition, we integrate grounding and
multimodal retrieval-augmentation to verify and resolve context conflicts,
resulting in outputs that are more reliable, transparent, and consistent with
clinical practice.

</details>
