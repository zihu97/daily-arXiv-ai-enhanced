<div id=toc></div>

# Table of Contents

- [cs.DC](#cs.DC) [Total: 5]
- [cs.SE](#cs.SE) [Total: 8]
- [cs.NI](#cs.NI) [Total: 1]
- [cs.MA](#cs.MA) [Total: 1]
- [cs.AR](#cs.AR) [Total: 5]


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [1] [Adaptive Execution Scheduler for DataDios SmartDiff](https://arxiv.org/abs/2510.07811)
*Aryan Poduri*

Main category: cs.DC

TL;DR: 提出了一种自适应调度器SmartDiff，通过动态调整批处理大小和工作线程数，在固定CPU和内存预算下最小化p95延迟，相比基线方法降低23-40%延迟和16-32%内存使用。


<details>
  <summary>Details</summary>
Motivation: 需要为差分引擎设计一个智能调度器，在固定资源约束下最小化p95延迟，同时避免内存溢出，通过动态选择执行模式（内存线程或Dask并行）来优化性能。

Method: 采用轻量级预分析器估算字节/行和I/O率；在线成本/内存模型过滤不安全操作；受保护的爬山策略优先考虑低延迟并处理背压和慢节点；通过保守工作集估计选择后端，优先使用内存执行。

Result: 在合成和公开表格基准测试中，调度器相比调优的预热启发式方法降低23-28% p95延迟，相比固定网格基线降低35-40%；峰值内存降低16-22%（相比固定基线25-32%），零OOM错误且吞吐量相当。

Conclusion: SmartDiff自适应调度器通过动态资源分配和智能后端选择，在保证系统稳定性的同时显著提升了性能，证明了自适应调度在差分计算中的有效性。

Abstract: We present an adaptive scheduler for a single differencing engine (SmartDiff)
with two execution modes: (i) in-memory threads and (ii) Dask based
parallelism. The scheduler continuously tunes batch size and worker/thread
count within fixed CPU and memory budgets to minimize p95 latency. A
lightweight preflight profiler estimates bytes/row and I/O rate; an online
cost/memory model prunes unsafe actions; and a guarded hill-climb policy favors
lower latency with backpressure and straggler mitigation. Backend selection is
gated by a conservative working-set estimate so that in-memory execution is
chosen when safe, otherwise Dask is used. Across synthetic and public tabular
benchmarks, the scheduler reduces p95 latency by 23 to 28 percent versus a
tuned warm-up heuristic (and by 35 to 40 percent versus fixed grid baselines),
while lowering peak memory by 16 to 22 percent (25 to 32 percent vs. fixed)
with zero OOMs and comparable throughput.

</details>


### [2] [A Multi-Simulation Bridge for IoT Digital Twins](https://arxiv.org/abs/2510.08164)
*Marco Picone,Samuele Burattini,Marco Melloni,Prasad Talasila,Davide Ziglioli,Matteo Martinelli,Nicola Bicocchi,Alessandro Ricci,Peter Gorm Larsen*

Main category: cs.DC

TL;DR: DT Simulation Bridge是一个软件框架，实现了数字孪生与仿真环境之间的多样化交互模式，支持数字孪生开发生命周期和主动运行中的仿真集成，通过双向数据交换动态更新模型并实时调整仿真参数。


<details>
  <summary>Details</summary>
Motivation: 随着数字孪生在物联网和工业物联网领域的能力不断增强，需要与仿真平台无缝集成以支持系统设计、验证和实时运行。

Method: 开发了DT Simulation Bridge软件框架，描述了其架构设计和核心软件组件，确保灵活的互操作性和可扩展部署，并进行了实验验证。

Result: 实验结果表明，DT Simulation Bridge提高了设计敏捷性，便于虚拟调试，并能支持现实条件下的实时行为分析，在多种工业场景中证明了其有效性。

Conclusion: DT Simulation Bridge成功实现了数字孪生与仿真环境的有效集成，为系统设计、验证和实时操作提供了强大的支持工具。

Abstract: The increasing capabilities of Digital Twins (DTs) in the context of the
Internet of Things (IoT) and Industrial IoT (IIoT) call for seamless
integration with simulation platforms to support system design, validation, and
real-time operation. This paper introduces the concept, design, and
experimental evaluation of the DT Simulation Bridge - a software framework that
enables diverse interaction patterns between active DTs and simulation
environments. The framework supports both the DT development lifecycle and the
incorporation of simulations during active operation. Through bidirectional
data exchange, simulations can update DT models dynamically, while DTs provide
real-time feedback to adapt simulation parameters. We describe the
architectural design and core software components that ensure flexible
interoperability and scalable deployment. Experimental results show that the DT
Simulation Bridge enhances design agility, facilitates virtual commissioning,
and supports live behavioral analysis under realistic conditions, demonstrating
its effectiveness across a range of industrial scenarios.

</details>


### [3] [Towards Energy-Efficient Serverless Computing with Hardware Isolation](https://arxiv.org/abs/2510.08180)
*Natalie Carl,Tobias Pfandzelter,David Bermbach*

Main category: cs.DC

TL;DR: 提出了一种针对无服务器计算的新型硬件架构，使用硬件隔离而非软件隔离，通过为每个函数分配独立处理器，实现按需资源分配，显著降低能源消耗。


<details>
  <summary>Details</summary>
Motivation: 当前无服务器平台在传统服务器硬件上运行大量函数，需要昂贵的软件隔离机制和高程度的资源过度配置，导致能源效率低下。现有服务器硬件优化针对大型单一工作负载，而非运行数千个隔离函数。

Method: 重新设计无服务器硬件架构，使用硬件隔离方式为每个函数分配独立处理器，替代软件隔离方法，构建仅在应用程序实际工作时才消耗能源的无服务器硬件堆栈。

Result: 通过真实硬件和典型无服务器工作负载的初步评估显示，该方法可将能源消耗开销降低90.63%，相当于平均节省70.8MW能源。

Conclusion: 无服务器计算通过重新思考硬件架构，采用硬件隔离方案，能够显著提高能源效率，减少资源浪费，为可持续计算提供新的技术路径。

Abstract: Serverless computing provides just-in-time infrastructure provisioning with
rapid elasticity and a finely-grained pricing model. As full control of
resource allocation is in the hands of the cloud provider and applications only
consume resources when they actually perform work, we believe that serverless
computing is uniquely positioned to maximize energy efficiency.
  However, the focus of current serverless platforms is to run hundreds or
thousands of serverless functions from different tenants on traditional server
hardware, requiring expensive software isolation mechanisms and a high degree
of overprovisioning, i.e., idle servers, to anticipate load spikes. With shared
caches, high clock frequencies, and many-core architectures, servers today are
optimized for large, singular workloads but not to run thousands of isolated
functions.
  We propose rethinking the serverless hardware architecture to align it with
the requirements of serverless software. Specifically, we propose using
hardware isolation with individual processors per function instead of software
isolation resulting in a serverless hardware stack that consumes energy only
when an application actually performs work. In preliminary evaluation with real
hardware and a typical serverless workload we find that this could reduce
energy consumption overheads by 90.63% or an average 70.8MW.

</details>


### [4] [Distributed Resource Selection for Self-Organising Cloud-Edge Systems](https://arxiv.org/abs/2510.08228)
*Quentin Renau,Amjad Ullah,Emma Hart*

Main category: cs.DC

TL;DR: 本文提出了一种分布式资源选择机制，通过共识机制和本地知识协作，在云边缘环境中实现高效、可扩展和弹性的资源分配，避免了中心化协调的瓶颈问题。


<details>
  <summary>Details</summary>
Motivation: 在高度动态的云边缘环境中，中心化协调成为瓶颈，需要分布式决策机制来确保效率、可扩展性和弹性。

Method: 采用基于共识的机制，利用本地知识和智能体间协作，无需依赖中央控制器，实现分布式资源分配决策。

Result: 计算时间是影响分配决策的关键因素，该方法能够在不牺牲最优性的情况下快速完成分配，比中心化启发式算法快30倍，实现了大规模情况下的及时结果。

Conclusion: 该分布式资源选择机制为分布式自组织编排系统的核心组件奠定了基础，能够智能地实时部署和调整应用，为云边缘环境中的分布式编排提供了新途径。

Abstract: This paper presents a distributed resource selection mechanism for diverse
cloud-edge environments, enabling dynamic and context-aware allocation of
resources to meet the demands of complex distributed applications. By
distributing the decision-making process, our approach ensures efficiency,
scalability, and resilience in highly dynamic cloud-edge environments where
centralised coordination becomes a bottleneck. The proposed mechanism aims to
function as a core component of a broader, distributed, and self-organising
orchestration system that facilitates the intelligent placement and adaptation
of applications in real-time. This work leverages a consensus-based mechanism
utilising local knowledge and inter-agent collaboration to achieve efficient
results without relying on a central controller, thus paving the way for
distributed orchestration. Our results indicate that computation time is the
key factor influencing allocation decisions. Our approach consistently delivers
rapid allocations without compromising optimality or incurring additional cost,
achieving timely results at scale where exhaustive search is infeasible and
centralised heuristics run up to 30 times slower.

</details>


### [5] [Investigating Matrix Repartitioning to Address the Over- and Undersubscription Challenge for a GPU-based CFD Solver](https://arxiv.org/abs/2510.08536)
*Gregor Olenik,Marcel Koch,Hartwig Anzt*

Main category: cs.DC

TL;DR: Processing failed


<details>
  <summary>Details</summary>
Motivation: Processing failed

Method: Processing failed

Result: Processing failed

Conclusion: Processing failed

Abstract: Modern high-performance computing (HPC) increasingly relies on GPUs, but
integrating GPU acceleration into complex scientific frameworks like OpenFOAM
remains a challenge. Existing approaches either fully refactor the codebase or
use plugin-based GPU solvers, each facing trade-offs between performance and
development effort. In this work, we address the limitations of plugin-based
GPU acceleration in OpenFOAM by proposing a repartitioning strategy that better
balances CPU matrix assembly and GPU-based linear solves. We present a detailed
computational model, describe a novel matrix repartitioning and update
procedure, and evaluate its performance on large-scale CFD simulations. Our
results show that the proposed method significantly mitigates oversubscription
issues, improving solver performance and resource utilization in heterogeneous
CPU-GPU environments.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [6] [Modeling Developer Burnout with GenAI Adoption](https://arxiv.org/abs/2510.07435)
*Zixuan Feng,Sadia Afroz,Anita Sarma*

Main category: cs.SE

TL;DR: 研究利用JD-R模型实证分析发现，生成式AI采用虽然增加工作需求导致倦怠，但工作资源和积极认知可缓解这种影响，重新定位采用为机遇。


<details>
  <summary>Details</summary>
Motivation: 现有研究关注生成式AI在软件开发中的生产力提升，忽视了其采用对开发者福祉的潜在负面影响。本研究旨在探究生成式AI采用与开发者倦怠之间的关系。

Method: 采用并发式嵌入混合研究设计，通过442名开发者的问卷调查，结合PLS-SEM回归建模分析工作需求、资源与倦怠的关系，并辅以开放式回答的质性分析进行情境化解释。

Result: 生成式AI采用通过增加工作需求导致倦怠加剧，而工作资源和积极的AI认知能够缓解这种影响，将采用重新定位为发展机遇。

Conclusion: 生成式AI采用对开发者倦怠存在双重影响：既可能加剧倦怠，也可能成为积极资源。组织需关注工作需求管理，培养开发者积极认知，以实现AI技术的健康采用。

Abstract: Generative AI (GenAI) is rapidly reshaping software development workflows.
While prior studies emphasize productivity gains, the adoption of GenAI also
introduces new pressures that may harm developers' well-being. In this paper,
we investigate the relationship between the adoption of GenAI and developers'
burnout. We utilized the Job Demands--Resources (JD--R) model as the analytic
lens in our empirical study. We employed a concurrent embedded mixed-methods
research design, integrating quantitative and qualitative evidence. We first
surveyed 442 developers across diverse organizations, roles, and levels of
experience. We then employed Partial Least Squares--Structural Equation
Modeling (PLS-SEM) and regression to model the relationships among job demands,
job resources, and burnout, complemented by a qualitative analysis of
open-ended responses to contextualize the quantitative findings. Our results
show that GenAI adoption heightens burnout by increasing job demands, while job
resources and positive perceptions of GenAI mitigate these effects, reframing
adoption as an opportunity.

</details>


### [7] [HotBugs.jar: A Benchmark of Hot Fixes for Time-Critical Bugs](https://arxiv.org/abs/2510.07529)
*Carol Hanna,Federica Sarro,Mark Harman,Justyna Petke*

Main category: cs.SE

TL;DR: HotBugs.jar是首个专注于真实世界热修复的数据集，包含679个经人工验证的热修复案例和110个可复现的测试用例，已被SBSE会议采用为官方挑战数据集。


<details>
  <summary>Details</summary>
Motivation: 尽管热修复对生产系统至关重要，但目前没有专门针对热修复的评估基准，缺乏系统化的研究工具和数据支持。

Method: 从10个活跃Apache项目中挖掘19万次提交和15万份问题报告，筛选出746个候选热修复，通过人工验证确认679个真实案例，构建集成Bug.jar框架的HotBugs.jar数据集。

Result: 创建了包含679个手动验证热修复和110个可复现测试用例的数据集，每个案例都配备完整元数据和可复现格式，已被SBSE会议采用为官方挑战数据集。

Conclusion: HotBugs.jar填补了热修复领域评估基准的空白，为快速调试、自动修复和生产级弹性研究提供了重要工具，推动该领域研究发展。

Abstract: Hot fixes are urgent, unplanned changes deployed to production systems to
address time-critical issues. Despite their importance, no existing evaluation
benchmark focuses specifically on hot fixes. We present HotBugs$.$jar, the
first dataset dedicated to real-world hot fixes. From an initial mining of 10
active Apache projects totaling over 190K commits and 150K issue reports, we
identified 746 software patches that met our hot-fix criteria. After manual
evaluation, 679 were confirmed as genuine hot fixes, of which 110 are
reproducible using a test suite. Building upon the Bugs$.$jar framework,
HotBugs$.$jar integrates these 110 reproducible cases and makes available all
679 manually validated hot fixes, each enriched with comprehensive metadata to
support future research. Each hot fix was systematically identified using Jira
issue data, validated by independent reviewers, and packaged in a reproducible
format with buggy and fixed versions, test suites, and metadata. HotBugs$.$jar
has already been adopted as the official challenge dataset for the Search-Based
Software Engineering (SBSE) Conference Challenge Track, demonstrating its
immediate impact. This benchmark enables the study and evaluation of tools for
rapid debugging, automated repair, and production-grade resilience in modern
software systems to drive research in this essential area forward.

</details>


### [8] [RustAssure: Differential Symbolic Testing for LLM-Transpiled C-to-Rust Code](https://arxiv.org/abs/2510.07604)
*Yubo Bai,Tapti Palit*

Main category: cs.SE

TL;DR: RustAssure使用大型语言模型自动将C代码库安全地转换为Rust，并通过差异符号测试确保语义等价性，实现了89.8%的函数可编译率和69.9%的语义等价率。


<details>
  <summary>Details</summary>
Motivation: 现有不安全的内存语言代码库需要转换为Rust以获得更好的安全性保证，但手动转换成本高昂且容易出错。

Method: 使用提示工程技术引导大型语言模型生成符合Rust惯用和安全性的代码，并通过差异符号测试验证原始C代码与生成的Rust代码的语义相似性。

Result: 在五个真实世界应用程序和库的评估中，RustAssure能够为89.8%的C函数生成可编译的Rust函数，其中69.9%的函数在符号返回值方面具有等价性。

Conclusion: RustAssure系统有效解决了C代码到Rust的自动转换问题，在保证安全性的同时实现了较高的转换质量和语义等价性。

Abstract: Rust is a memory-safe programming language that significantly improves
software security. Existing codebases written in unsafe memory languages, such
as C, must first be transpiled to Rust to take advantage of Rust's improved
safety guarantees. RustAssure presents a system that uses Large Language Models
(LLMs) to automatically transpile existing C codebases to Rust. RustAssure uses
prompt engineering techniques to maximize the chances of the LLM generating
idiomatic and safe Rust code. Moreover, because LLMs often generate code with
subtle bugs that can be missed under traditional unit or fuzz testing,
RustAssure performs differential symbolic testing to establish the semantic
similarity between the original C and LLM-transpiled Rust code. We evaluated
RustAssure with five real-world applications and libraries, and showed that our
system is able to generate compilable Rust functions for 89.8% of all C
functions, of which 69.9% produced equivalent symbolic return values for both
the C and Rust functions.

</details>


### [9] [AppForge: From Assistant to Independent Developer -- Are GPTs Ready for Software Development?](https://arxiv.org/abs/2510.07740)
*Dezhi Ran,Yuan Cao,Mengzhou Wu,Simin Chen,Yuzhe Guo,Jun Ren,Zihe Song,Hao Yu,Jialei Wei,Linyi Li,Wei Yang,Baishakhi Ray,Tao Xie*

Main category: cs.SE

TL;DR: 提出APPFORGE基准，评估大语言模型构建完整软件系统的能力，结果显示当前模型表现不佳


<details>
  <summary>Details</summary>
Motivation: 现有基准仅能评估LLM在函数级代码生成任务上的能力，缺乏对LLM构建完整软件系统能力的评估，尤其是需要协调组件交互、维护状态一致性和处理框架约束的系统级任务

Method: 构建APPFORGE基准，包含101个来自真实Android应用的开发问题，利用多智能体系统自动总结功能并生成测试用例，通过Android专家人工验证后建立自动化评估框架

Result: 在12个旗舰LLM的评估中，表现最好的GPT-5也只能构建18.8%功能正确的应用，表明当前模型在处理复杂多组件软件工程挑战方面存在根本性局限

Conclusion: LLM在构建完整软件系统方面存在严重局限性，APPFORGE基准为未来研究提供了可靠的评估工具，推动模型改进

Abstract: Large language models (LLMs) have demonstrated remarkable capability in
function-level code generation tasks. Unlike isolated functions, real-world
applications demand reasoning over the entire software system: developers must
orchestrate how different components interact, maintain consistency across
states over time, and ensure the application behaves correctly within the
lifecycle and framework constraints. Yet, no existing benchmark adequately
evaluates whether LLMs can bridge this gap and construct entire software
systems from scratch. To address this gap, we propose APPFORGE, a benchmark
consisting of 101 software development problems drawn from real-world Android
apps. Given a natural language specification detailing the app functionality, a
language model is tasked with implementing the functionality into an Android
app from scratch. Developing an Android app from scratch requires understanding
and coordinating app states, lifecycle management, and asynchronous operations,
calling for LLMs to generate context-aware, robust, and maintainable code. To
construct APPFORGE, we design a multi-agent system to automatically summarize
the main functionalities from app documents and navigate the app to synthesize
test cases validating the functional correctness of app implementation.
Following rigorous manual verification by Android development experts, APPFORGE
incorporates the test cases within an automated evaluation framework that
enables reproducible assessment without human intervention, making it easily
adoptable for future research. Our evaluation on 12 flagship LLMs show that all
evaluated models achieve low effectiveness, with the best-performing model
(GPT-5) developing only 18.8% functionally correct applications, highlighting
fundamental limitations in current models' ability to handle complex,
multi-component software engineering challenges.

</details>


### [10] [Interleaved Learning and Exploration: A Self-Adaptive Fuzz Testing Framework for MLIR](https://arxiv.org/abs/2510.07815)
*Zeyu Sun,Jingjing Liang,Weiyi Wang,Chenyao Suo,Junjie Chen,Fanjiang Xu*

Main category: cs.SE

TL;DR: Processing failed


<details>
  <summary>Details</summary>
Motivation: Processing failed

Method: Processing failed

Result: Processing failed

Conclusion: Processing failed

Abstract: MLIR (Multi-Level Intermediate Representation) has rapidly become a
foundational technology for modern compiler frameworks, enabling extensibility
across diverse domains. However, ensuring the correctness and robustness of
MLIR itself remains challenging. Existing fuzzing approaches-based on manually
crafted templates or rule-based mutations-struggle to generate sufficiently
diverse and semantically valid test cases, making it difficult to expose subtle
or deep-seated bugs within MLIR's complex and evolving code space. In this
paper, we present FLEX, a novel self-adaptive fuzzing framework for MLIR. FLEX
leverages neural networks for program generation, a perturbed sampling strategy
to encourage diversity, and a feedback-driven augmentation loop that
iteratively improves its model using both crashing and non-crashing test cases.
Starting from a limited seed corpus, FLEX progressively learns valid syntax and
semantics and autonomously produces high-quality test inputs. We evaluate FLEX
on the upstream MLIR compiler against four state-of-the-art fuzzers. In a
30-day campaign, FLEX discovers 80 previously unknown bugs-including multiple
new root causes and parser bugs-while in 24-hour fixed-revision comparisons, it
detects 53 bugs (over 3.5x as many as the best baseline) and achieves 28.2%
code coverage, outperforming the next-best tool by 42%. Ablation studies
further confirm the critical role of both perturbed generation and diversity
augmentation in FLEX's effectiveness.

</details>


### [11] [Bug Histories as Sources of Compiler Fuzzing Mutators](https://arxiv.org/abs/2510.07834)
*Lingjun Liu,Feiran Qin,Owolabi Legunsen,Marcelo d'Amorim*

Main category: cs.SE

TL;DR: IssueMut首次从编译器bug历史中提取变异器，显著提升了编译器模糊测试的bug检测能力，发现并验证了65个新bug。


<details>
  <summary>Details</summary>
Motivation: 编译器作为关键基础设施，其中的bug影响巨大。变异模糊测试器通过系统变异编译器输入来检测bug，但其效果依赖于变异器的质量。然而，以往研究未利用编译器bug历史作为变异器来源。

Method: 提出IssueMut，首个从bug报告中提取编译器模糊测试变异器的方法。通过自动化方法从bug报告中挖掘变异器，并将这些变异器集成到现有的变异编译器模糊测试器中。从1760个GCC和LLVM bug报告中提取了587个变异器。

Result: 使用IssueMut挖掘的"bug历史"变异器效果显著：在GCC中发现28个新bug，在LLVM中发现37个新bug，这些bug都被最先进的变异编译器模糊测试器遗漏。其中60个bug已被确认或修复，验证了bug历史包含丰富的编译器模糊测试器应利用的信息。

Conclusion: IssueMut证明了编译器bug历史是生成高质量变异器的宝贵来源，能够有效提升模糊测试的bug检测能力，为编译器测试提供了新的有效方法。

Abstract: Bugs in compilers, which are critical infrastructure today, can have outsized
negative impacts. Mutational fuzzers aid compiler bug detection by
systematically mutating compiler inputs, i.e., programs. Their effectiveness
depends on the quality of the mutators used. Yet, no prior work used compiler
bug histories as a source of mutators. We propose IssueMut, the first approach
for extracting compiler fuzzing mutators from bug histories. Our insight is
that bug reports contain hints about program elements that induced compiler
bugs; they can guide fuzzers towards similar bugs. IssueMut uses an automated
method to mine mutators from bug reports and retrofit such mutators into
existing mutational compiler fuzzers. Using IssueMut, we mine 587 mutators from
1760 GCC and LLVM bug reports. Then, we run IssueMut on these compilers, with
all their test inputs as seed corpora. We find that "bug history" mutators are
effective: they find new bugs that a state-of-the-art mutational compiler
fuzzer misses-28 in GCC and 37 in LLVM. Of these, 60 were confirmed or fixed,
validating our idea that bug histories have rich information that compiler
fuzzers should leverage.

</details>


### [12] [An AUTOSAR-Aligned Architectural Study of Vulnerabilities in Automotive SoC Software](https://arxiv.org/abs/2510.07941)
*Srijita Basu,Haraldsson Bengt,Miroslaw Staron,Christian Berger,Jennifer Horkoff,Magnus Almgren*

Main category: cs.SE

TL;DR: Processing failed


<details>
  <summary>Details</summary>
Motivation: Processing failed

Method: Processing failed

Result: Processing failed

Conclusion: Processing failed

Abstract: Cooperative, Connected and Automated Mobility (CCAM) are complex
cyber-physical systems (CPS) that integrate computation, communication, and
control in safety-critical environments. At their core, System-on-Chip (SoC)
platforms consolidate processing units, communication interfaces, AI
accelerators, and security modules into a single chip. AUTOSAR (AUTomotive Open
System ARchitecture) standard was developed in the automotive domain to better
manage this complexity, defining layered software structures and interfaces to
facilitate reuse of HW/SW components. However, in practice, this integrated SoC
software architecture still poses security challenges, particularly in
real-time, safety-critical environments. Recent reports highlight a surge in
SoC-related vulnerabilities, yet systematic analysis of their root causes and
impact within AUTOSAR-aligned architectures is lacking. This study fills that
gap by analyzing 180 publicly reported automotive SoC vulnerabilities, mapped
to a representative SoC software architecture model that is aligned with
AUTOSAR principles for layered abstraction and service orientation. We identify
16 root causes and 56 affected software modules, and examine mitigation delays
across Common Weakness Enumeration (CWE) categories and architectural layers.
We uncover dominant vulnerability patterns and critical modules with prolonged
patch delays, and provide actionable insights for securing automotive CPS
platforms, including guides for improved detection, prioritization, and
localization strategies for SoC software architectures in SoC-based vehicle
platforms.

</details>


### [13] [Building Whitespace-Sensitive Languages Using Whitespace-Insensitive Components](https://arxiv.org/abs/2510.08200)
*Alexander Hellwig,Nico Jansen,Bernhard Rumpe*

Main category: cs.SE

TL;DR: 提出一种通过预处理技术，使用模块化的空白不敏感语言组件构建空白敏感语言的方法，提升语言组件的可重用性。


<details>
  <summary>Details</summary>
Motivation: 软件语言工程中存在空白敏感和空白不敏感语言之间的集成差距，导致语言组件难以重用，空白敏感语言需要从头开发，限制了软件语言的复用性和开发效率。

Method: 通过预处理语言工件，在解析前使用模块化的空白不敏感语言模块构建空白敏感语言的技术，解决两种语言类型之间的集成问题。

Result: 通过重构简化的Python语言对该方法进行了评估，证明其能够有效利用空白不敏感语言组件构建空白敏感语言。

Conclusion: 该解决方案旨在提高现有语言组件的可重用性，减少开发时间，提升软件语言的总体质量。

Abstract: In Software Language Engineering, there is a trend towards reusability by
composing modular language components. However, this reusability is severely
inhibited by a gap in integrating whitespace-sensitive and
whitespace-insensitive languages. There is currently no consistent procedure
for seamlessly reusing such language components in both cases, such that
libraries often cannot be reused, and whitespacesensitive languages are
developed from scratch. This paper presents a technique for using modular,
whitespaceinsensitive language modules to construct whitespace sensitive
languages by pre-processing language artifacts before parsing. The approach is
evaluated by reconstructing a simplified version of the programming language
Python. Our solution aims to increase the reusability of existing language
components to reduce development time and increase the overall quality of
software languages.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [14] [TDoA-Based Self-Supervised Channel Charting with NLoS Mitigation](https://arxiv.org/abs/2510.08001)
*Mohsen Ahadi,Omid Esrafilian,Florian Kaltenberger,Adeel Malik*

Main category: cs.NI

TL;DR: 本研究提出了一种新颖的信道图绘制方法，通过结合信道脉冲响应数据和实用特征（如TDoA和TRP位置），实现了全球范围内的自监督定位功能，并有效处理了非视距条件下的信号失真问题。


<details>
  <summary>Details</summary>
Motivation: 现有信道图绘制方法在面对全球规模扩展和非视距条件下的失真问题时存在局限性，需要一种能够处理这些挑战的新方法。

Method: 提出基于信道脉冲响应的自监督定位框架，结合TDoA和TRP位置特征，利用短间隔用户设备位移测量提高定位连续性和鲁棒性，并设计非视距噪声识别和掩蔽机制。

Result: 在5G测试床中验证，性能优于现有半监督和自监督方法，在90%的情况下达到2-4米定位精度，并公开了相关数据集。

Conclusion: 该研究成功解决了信道图绘制在全球扩展和非视距条件下的挑战，为实际应用提供了高精度的定位解决方案。

Abstract: Channel Charting (CC) has emerged as a promising framework for data-driven
radio localization, yet existing approaches often struggle to scale globally
and to handle the distortions introduced by non-line-of-sight (NLoS)
conditions. In this work, we propose a novel CC method that leverages Channel
Impulse Response (CIR) data enriched with practical features such as Time
Difference of Arrival (TDoA) and Transmission Reception Point (TRP) locations,
enabling a self-supervised localization function on a global scale. The
proposed framework is further enhanced with short-interval User Equipment (UE)
displacement measurements, which improve the continuity and robustness of the
learned positioning function. Our algorithm incorporates a mechanism to
identify and mask NLoS-induced noisy measurements, leading to significant
performance gains. We present the evaluations of our proposed models in a real
5G testbed and benchmarked against centimeter-accurate Real-Time Kinematic
(RTK) positioning, in an O-RAN--based 5G network by OpenAirInterface (OAI)
software at EURECOM. It demonstrated outperforming results against the
state-of-the-art semi-supervised and self-supervised CC approaches in a
real-world scenario. The results show localization accuracies of 2-4 meters in
90% of cases, across a range of NLoS ratios. Furthermore, we provide public
datasets of CIR recordings, along with the true position labels used in this
paper's evaluation.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [15] [Network Topology and Information Efficiency of Multi-Agent Systems: Study based on MARL](https://arxiv.org/abs/2510.07888)
*Xinren Zhang,Sixi Cheng,Zixin Zhong,Jiadong Yu*

Main category: cs.MA

TL;DR: 本文研究多智能体系统中的通信拓扑和信息效率，提出定向和顺序拓扑可提升性能并降低通信开销，同时引入信息熵效率指数(IEI)和专业效率指数(SEI)来评估信息紧凑性和角色分化，将这些指标纳入训练目标可提高成功率与收敛速度。


<details>
  <summary>Details</summary>
Motivation: 多智能体强化学习面临非平稳性和部分可观察性挑战，虽然通信是解决方案，但关于其最优结构和评估的问题仍需探索。

Method: 研究两个被忽视的方面：通信拓扑和信息效率；使用定向和顺序拓扑；提出IEI和SEI指标；将这些指标纳入训练目标。

Result: 定向和顺序拓扑在异构和同构任务中都能提升性能并减少通信开销；IEI和SEI指标能评估信息紧凑性和角色分化；将这些指标纳入训练目标可提高成功率和收敛速度。

Conclusion: 设计具有信息高效消息的自适应通信拓扑对复杂多智能体系统中有效协调至关重要。

Abstract: Multi-agent systems (MAS) solve complex problems through coordinated
autonomous entities with individual decision-making capabilities. While
Multi-Agent Reinforcement Learning (MARL) enables these agents to learn
intelligent strategies, it faces challenges of non-stationarity and partial
observability. Communications among agents offer a solution, but questions
remain about its optimal structure and evaluation. This paper explores two
underexamined aspects: communication topology and information efficiency. We
demonstrate that directed and sequential topologies improve performance while
reducing communication overhead across both homogeneous and heterogeneous
tasks. Additionally, we introduce two metrics -- Information Entropy Efficiency
Index (IEI) and Specialization Efficiency Index (SEI) -- to evaluate message
compactness and role differentiation. Incorporating these metrics into training
objectives improves success rates and convergence speed. Our findings highlight
that designing adaptive communication topologies with information-efficient
messaging is essential for effective coordination in complex MAS.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [16] [How long can you sleep? Idle Time System Inefficiencies and Opportunities](https://arxiv.org/abs/2510.07449)
*Georgia Antoniou,Haris Volos,Jawad Haj Yahya,Yiannakis Sazeides*

Main category: cs.AR

TL;DR: 本文提出基于队列模型的框架，揭示运行延迟关键应用现代服务器的空闲机会，发现由于空闲调度器不准确和传统空闲状态转换延迟高导致错过深度空闲状态的问题。


<details>
  <summary>Details</summary>
Motivation: 现代服务器运行延迟关键应用时存在未被充分利用的空闲机会，需要准确评估和利用这些空闲时间来提高能效。

Method: 使用三种队列模型(M/M/1、cxM/M/1、M/M/c)估算CPU核心和系统级别的理论空闲时间分布，对比实际服务器空闲时间与理论模型的差异。

Result: 发现服务器由于空闲调度器不准确和传统深度空闲状态转换高延迟，错过了大量进入深度空闲状态的机会。

Conclusion: 所提出的方法为早期设计探索提供了工具，能够分析不同服务器系统配置和负载下的空闲时间行为和优化机会。

Abstract: This work introduces a model-based framework that reveals the idle
opportunity of modern servers running latency-critical applications.
Specifically, three queuing models, M/M/1, cxM/M/1, and M/M/c, are used to
estimate the theoretical idle time distribution at the CPU core and system
(package) level. A comparison of the actual idleness of a real server and that
from the theoretical models reveals significant missed opportunities to enter
deep idle states. This inefficiency is attributed to the idle-governor
inaccuracy and the high latency to transition to/from legacy deep-idle states.
The proposed methodology offers the means for an early-stage design exploration
and insights into idle time behavior and opportunities for varying server
system configurations and load.

</details>


### [17] [DL-PIM: Improving Data Locality in Processing-in-Memory Systems](https://arxiv.org/abs/2510.07719)
*Parker Hao Tian,Zahra Yousefijamarani,Alaa Alameldeen*

Main category: cs.AR

TL;DR: DL-PIM架构通过动态检测数据移动开销并主动移动数据到本地内存，实现了在HMC和HMC上平均内存延迟减少54%和50%，所有工作负载分别获得6%和3%的性能提升。


<details>
  <summary>Details</summary>
Motivation: 现有PIM架构虽然能提高能效和性能，但数据移动开销会严重影响其优势，因为数据离处理单元较远时需要传输数据，导致性能和能效下降。

Method: 提出DL-PIM架构，使用分布式地址间接硬件查找表重定向流量，在HMC和HBM两种3D堆叠内存上实现，并设计了自适应机制评估间接访问成本收益。

Result: 在HMC上延迟减少54%，HBM减少50%，数据重用密集的工作负载分别提升15%和5%，所有代表工作负载在HMC和HMB上分别获得6%和3%的加速。

Conclusion: DL-PIM通过增强数据局部性显著改善了系统整体性能，证明了其在不同内存架构中的有效性和适应性。

Abstract: PIM architectures aim to reduce data transfer costs between processors and
memory by integrating processing units within memory layers. Prior PIM
architectures have shown potential to improve energy efficiency and
performance. However, such advantages rely on data proximity to the processing
units performing computations. Data movement overheads can degrade PIM's
performance and energy efficiency due to the need to move data between a
processing unit and a distant memory location. %they face challenges due to the
overhead of transferring data from remote memory locations to processing units
inside memory for computation. In this paper, we demonstrate that a large
fraction of PIM's latency per memory request is attributed to data transfers
and queuing delays from remote memory accesses. To improve PIM's data locality,
we propose DL-PIM, a novel architecture that dynamically detects the overhead
of data movement, and proactively moves data to a reserved area in the local
memory of the requesting processing unit. DL-PIM uses a distributed
address-indirection hardware lookup table to redirect traffic to the current
data location. We propose DL-PIM implementations on two 3D stacked memories:
HMC and HBM. While some workloads benefit from DL-PIM, others are negatively
impacted by the additional latency due to indirection accesses. Therefore, we
propose an adaptive mechanism that assesses the cost and benefit of indirection
and dynamically enables or disables it to prevent degrading workloads that
suffer from indirection. Overall, DL-PIM reduces the average memory latency per
request by 54% in HMC and 50% in HBM which resulted in performance improvement
of 15% for workloads with substantial data reuse in HMC and 5% in HBM. For all
representative workloads, DL-PIM achieved a 6% speedup in HMC and a 3% speedup
in HBM, showing that DL-PIM enhances data locality and overall system
performance.

</details>


### [18] [A Scalable FPGA Architecture With Adaptive Memory Utilization for GEMM-Based Operations](https://arxiv.org/abs/2510.08137)
*Anastasios Petropoulos,Theodore Antonakopoulos*

Main category: cs.AR

TL;DR: Processing failed


<details>
  <summary>Details</summary>
Motivation: Processing failed

Method: Processing failed

Result: Processing failed

Conclusion: Processing failed

Abstract: Deep neural network (DNN) inference relies increasingly on specialized
hardware for high computational efficiency. This work introduces a
field-programmable gate array (FPGA)-based dynamically configurable accelerator
featuring systolic arrays, high-bandwidth memory, and UltraRAMs. We present two
processing unit (PU) configurations with different computing capabilities using
the same interfaces and peripheral blocks. By instantiating multiple PUs and
employing a heuristic weight transfer schedule, the architecture achieves
notable throughput efficiency over prior works. Moreover, we outline how the
architecture can be extended to emulate analog in-memory computing (AIMC)
devices to aid next-generation heterogeneous AIMC chip designs and investigate
device-level noise behavior. Overall, this brief presents a versatile DNN
inference acceleration architecture adaptable to various models and future FPGA
designs.

</details>


### [19] [FMCache: File-System Metadata Caching in Programmable Switches](https://arxiv.org/abs/2510.08351)
*Qingxiu Liu,Jiazhen Cai,Siyuan Sheng,Yuhui Chen,Lu Tang,Zhirong Shen,Patrick P. C. Lee*

Main category: cs.AR

TL;DR: FMCache是一种可编程交换机内的文件系统元数据缓存框架，通过在数据平面直接处理客户端请求，显著提升分布式文件系统性能，同时保持低延迟和有限的交换机资源使用。


<details>
  <summary>Details</summary>
Motivation: 分布式文件系统需要快速可扩展的元数据管理来处理大量文件和目录，客户端缓存虽能减轻服务器负载，但在客户端数量增加时会带来显著的一致性维护开销和复杂度。

Method: 提出FMCache框架，利用可编程交换机在数据平面直接服务多客户端的文件系统元数据请求，解决了传统键值缓存方法在严格交换机资源约束下无法处理的文件系统路径依赖问题。

Result: 在基于Hadoop HDFS的实现和Tofino交换机测试床上，FMCache相比原生HDFS吞吐量提升高达181.6%，与客户端缓存结合可获得额外139.6%的吞吐量提升，同时保持低延迟和有限的交换机资源使用。

Conclusion: FMCache通过在交换机数据平面实现文件系统元数据缓存，有效解决了分布式文件系统元数据管理中的可扩展性和一致性挑战，为高性能文件系统架构提供了新的解决方案。

Abstract: Fast and scalable metadata management across multiple metadata servers is
crucial for distributed file systems to handle numerous files and directories.
Client-side caching of frequently accessed metadata can mitigate server loads,
but incurs significant overhead and complexity in maintaining cache consistency
when the number of clients increases. We propose FMCache, an in-switch
file-system metadata caching framework that leverages programmable switches to
serve file-system metadata requests from multiple clients directly in the
switch data plane. Unlike prior in-switch key-value caching approaches, FMCache
addresses file-system-specific path dependencies under stringent switch
resource constraints. We implement FMCache atop Hadoop HDFS and evaluate it on
a Tofino-switch testbed using real-world file-system metadata workloads.
FMCache achieves up to 181.6% higher throughput than vanilla HDFS and
complements client-side caching with additional throughput gains of up to
139.6%. It also incurs low latencies and limited switch resource usage.

</details>


### [20] [SPAD: Specialized Prefill and Decode Hardware for Disaggregated LLM Inference](https://arxiv.org/abs/2510.08544)
*Hengrui Zhang,Pratyush Patel,August Ning,David Wentzlaff*

Main category: cs.AR

TL;DR: SPAD是一种专门为大型语言模型的prefill和decode阶段设计的专用硬件架构，通过分离两个阶段并分别使用优化的芯片，显著降低了硬件成本和功耗。


<details>
  <summary>Details</summary>
Motivation: 现有数据中心GPU和TPU采用"越多越好"的设计哲学，导致在prefill阶段内存带宽利用率低，在decode阶段计算能力利用率低，这种不匹配造成了资源浪费和成本增加。

Method: 提出SPAD架构，采用"少即是多"的设计理念，分别设计专门的prefill芯片（更大的脉动阵列，使用成本效益高的GDDR内存）和decode芯片（保持高内存带宽但减少计算能力），支持动态重新分配芯片。

Result: 与模拟的H100相比，prefill芯片平均性能提高8%，硬件成本降低52%；decode芯片达到97%的decode性能，TDP降低28%。端到端仿真显示SPAD相比基线集群硬件成本降低19%-41%，TDP降低2%-17%，即使在模型和负载变化时仍能实现11%-43%的成本降低。

Conclusion: SPAD通过prefill-decode解耦和专用硬件设计，有效解决了LLM推理中两个阶段的资源不匹配问题，在保持相同性能的同时显著降低了硬件成本和功耗，具有良好的可扩展性和长期适用性。

Abstract: Large Language Models (LLMs) have gained popularity in recent years, driving
up the demand for inference. LLM inference is composed of two phases with
distinct characteristics: a compute-bound prefill phase followed by a
memory-bound decode phase. To efficiently serve LLMs, prior work proposes
prefill-decode disaggregation to run each phase on separate hardware. However,
existing hardware poorly matches the different requirements of each phase.
Current datacenter GPUs and TPUs follow a more-is-better design philosophy that
maximizes compute and memory resources, causing memory bandwidth
underutilization in the prefill phase and compute underutilization in the
decode phase. Such underutilization directly translates into increased serving
costs.
  This paper proposes SPAD (Specialized Prefill and Decode hardware), adopting
a less-is-more methodology to design specialized chips tailored to the distinct
characteristics of prefill and decode phases. The proposed Prefill Chips have
larger systolic arrays and use cost-effective GDDR memory, whereas the proposed
Decode Chips retain high memory bandwidth but reduce compute capacity. Compared
to modeled H100s, simulations show that the proposed Prefill Chips deliver 8%
higher prefill performance on average at 52% lower hardware cost, while the
proposed Decode Chips achieve 97% of the decode performance with 28% lower TDP.
  End-to-end simulations on production traces show that SPAD reduces hardware
cost by 19%-41% and TDP by 2%-17% compared to modeled baseline clusters while
offering the same performance. Even when models and workloads change, SPAD can
reallocate either type of chip to run either phase and still achieve 11%-43%
lower hardware costs, demonstrating the longevity of the SPAD design.

</details>
