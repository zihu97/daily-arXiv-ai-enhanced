<div id=toc></div>

# Table of Contents

- [cs.SE](#cs.SE) [Total: 7]
- [cs.NI](#cs.NI) [Total: 2]
- [cs.MA](#cs.MA) [Total: 3]
- [cs.DC](#cs.DC) [Total: 2]
- [cs.AR](#cs.AR) [Total: 2]


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [1] [Peer Code Review in Research Software Development: The Research Software Engineer Perspective](https://arxiv.org/abs/2511.10781)
*Md Ariful Islam Malik,Jeffrey C. Carver,Nasir U. Eisty*

Main category: cs.SE

TL;DR: 本研究通过调查61名科研软件工程师（RSEs），探讨他们对同行代码审查的看法、实践与挑战，发现尽管存在独特障碍，但通过结构化流程、改进工具和针对性培训可提升审查的采纳与效果。


<details>
  <summary>Details</summary>
Motivation: 科研软件对科研发现至关重要，但其质量与可维护性常受需求变化、复杂输入和遗留依赖影响；同行代码审查虽能提升质量，但在RSE群体中的应用情况尚不明确。

Method: 通过问卷调查收集RSE对同行代码审查的观点，问卷设计与先前研究对齐以支持比较分析，并针对RSE增设特定问题。

Result: 获得61份有效问卷，结果既验证了已有研究结论，也揭示了RSE相较于其他科研软件开发者在审查实践与挑战方面的独特见解。

Conclusion: 同行代码审查对提升科研软件的质量、可维护性与可靠性至关重要；通过结构化流程、改进工具和针对性培训可有效应对RSE面临的挑战，促进审查的广泛应用。

Abstract: Background: Research software is crucial for enabling research discoveries and supporting data analysis, simulation, and interpretation across domains. However, evolving requirements, complex inputs, and legacy dependencies hinder the software quality and maintainability. While peer code review can improve software quality, its adoption by research software engineers (RSEs) remains unexplored. Aims: This study explores RSE perspectives on peer code review, focusing on their practices, challenges, and potential improvements. Building on prior work, it aims to uncover how RSEs insights differ from those of other research software developers and identify factors that can enhance code review adoption in this domain. Method: We surveyed RSEs to gather insights into their perspectives on peer code review. The survey design aligned with previous research to enable comparative analysis while including additional questions tailored to RSEs. Results: We received 61 valid responses from the survey. The findings align with prior research while uncovering unique insights about the challenges and practices of RSEs compared to broader developer groups. Conclusions: Peer code review is vital in improving research software's quality, maintainability, and reliability. Despite the unique challenges RSEs face, addressing these through structured processes, improved tools, and targeted training can enhance peer review adoption and effectiveness in research software development.

</details>


### [2] [Towards a Human-in-the-Loop Framework for Reliable Patch Evaluation Using an LLM-as-a-Judge](https://arxiv.org/abs/2511.10865)
*Sherry Shi,Renyao Wei,Michele Tufano,José Cambronero,Runxiang Cheng,Franjo Ivančić,Pat Rondon*

Main category: cs.SE

TL;DR: 本文提出一种结合人类反馈与大语言模型（LLM）的“人在回路”方法，用于自动评估程序修复补丁的有效性，通过为每个漏洞生成并人工审核评分标准（rubric），显著提升了与人类判断的一致性。


<details>
  <summary>Details</summary>
Motivation: 当前自动程序修复（APR）的评估主要依赖基于单元测试的执行方法（如 pass@k），无法准确反映补丁的真实有效性；而人工标注成本高昂，因此需要一种更高效、可靠且低成本的补丁有效性判断方法。

Method: 提出一种人在回路的方法：首先用LLM为每个bug生成评分标准（rubric），经人工一次性审核和可选优化后，再用LLM依据该标准对补丁进行二元有效性判断。

Result: 在人类标注者完全一致的数据子集上，该方法与人类共识高度一致（Cohen's kappa 0.75，召回率0.94，精确率0.80）；在包含人类分歧的完整数据集上表现稍弱（kappa 0.57，召回率0.93，精确率0.65），但仍具改进潜力。

Conclusion: 该方法有效降低了人工标注成本，同时在补丁有效性判断上取得了与人类高度一致的结果，为APR评估提供了一种可行的新范式，并指出了未来改进方向。

Abstract: Reliable evaluation is crucial for advancing Automated Program Repair (APR), but prevailing benchmarks rely on execution-based evaluation methods (unit test pass@k), which fail to capture true patch validity. Determining validity can require costly manual annotation. To reduce this cost, we introduce a human-in-the-loop approach to LLM-based patch validity judgment. Inspired by the observation that human judgment is better aligned when using a shared rubric, we first employ an LLM to generate a per-bug rubric, followed by a one-time human review and optional refinement to this rubric, and then employ an LLM to judge patches using the refined rubric. We apply this approach to assign binary validity labels to patches for issues found by Google sanitizer tools. Our results show that this approach yields substantial agreement with human consensus (Cohen's kappa 0.75), high recall (0.94) and high precision (0.80), when considering patches that have unanimous agreement from 3 human raters on the validity labels. On the full dataset including patches where human raters disagree, we find this approach can still be further improved (Cohen's kappa 0.57, recall 0.93, precision 0.65) and identify possible future directions.

</details>


### [3] [Architecting software monitors for control-flow anomaly detection through large language models and conformance checking](https://arxiv.org/abs/2511.10876)
*Francesco Vitale,Francesco Flammini,Mauro Caporuscio,Nicola Mazzocca*

Main category: cs.SE

TL;DR: 本文提出一种结合大语言模型（LLMs）与一致性检查的软件监控方法，用于检测运行时控制流异常。该方法通过LLMs自动将设计模型映射到源代码并进行插桩，生成事件日志，再利用一致性检查技术实现高精度异常检测，在ERTMS/ETCS铁路系统案例中取得了高达96.61%的F1分数和93.52%的AUC。


<details>
  <summary>Details</summary>
Motivation: 现代基于计算机的系统日益复杂，尽管在设计阶段经过验证，但运行时行为可能因“未知的未知”而偏离预期，出现控制流异常，因此需要有效的运行时监控手段来保障系统的可靠性。

Method: 提出一种结合大语言模型（LLMs）与一致性检查的监控方法：利用LLMs将设计时模型与实现代码对齐，自动完成源码插桩以生成事件日志，并通过一致性检查分析日志以检测控制流异常。

Result: 在ERTMS/ETCS铁路系统案例中，LLM驱动的插桩实现了84.775%的设计模型控制流覆盖率，一致性检查异常检测达到96.610%的F1分数和93.515%的AUC。

Conclusion: 引入领域知识引导LLM进行源码插桩，能有效生成高质量日志，从而支持通过一致性检查实现高效、可解释的控制流异常检测。

Abstract: Context: Ensuring high levels of dependability in modern computer-based systems has become increasingly challenging due to their complexity. Although systems are validated at design time, their behavior can be different at run-time, possibly showing control-flow anomalies due to "unknown unknowns".
  Objective: We aim to detect control-flow anomalies through software monitoring, which verifies run-time behavior by logging software execution and detecting deviations from expected control flow.
  Methods: We propose a methodology to develop software monitors for control-flow anomaly detection through Large Language Models (LLMs) and conformance checking. The methodology builds on existing software development practices to maintain traditional V&V while providing an additional level of robustness and trustworthiness. It leverages LLMs to link design-time models and implementation code, automating source-code instrumentation. The resulting event logs are analyzed via conformance checking, an explainable and effective technique for control-flow anomaly detection.
  Results: We test the methodology on a case-study scenario from the European Railway Traffic Management System / European Train Control System (ERTMS/ETCS), which is a railway standard for modern interoperable railways. The results obtained from the ERTMS/ETCS case study demonstrate that LLM-based source-code instrumentation can achieve up to 84.775% control-flow coverage of the reference design-time process model, while the subsequent conformance checking-based anomaly detection reaches a peak performance of 96.610% F1-score and 93.515% AUC.
  Conclusion: Incorporating domain-specific knowledge to guide LLMs in source-code instrumentation significantly allowed obtaining reliable and quality software logs and enabled effective control-flow anomaly detection through conformance checking.

</details>


### [4] [Utilizing LLMs for Industrial Process Automation: A Case Study on Modifying RAPID Programs](https://arxiv.org/abs/2511.11125)
*Salim Fares,Steffen Herbold*

Main category: cs.SE

TL;DR: 本文探讨了在工业过程自动化领域中，企业如何在不投入大量资源训练专用模型的情况下，利用少量示例提示（few-shot prompting）使大语言模型（LLM）有效处理专有领域语言中的简单任务，并支持本地部署以保护敏感数据。


<details>
  <summary>Details</summary>
Motivation: 现有研究多聚焦于通用编程语言，而工业自动化领域使用的高度专业化、专有语言尚未被充分探索；企业希望在不大量投入训练成本的前提下，安全地利用LLM提升软件工程效率。

Method: 采用少量示例提示（few-shot prompting）方法，在不进行领域特定模型训练的情况下，测试LLM对非主流专有语言的处理能力，并在本地环境中部署以保障数据安全。

Result: 研究表明，few-shot prompting足以让LLM解决专有语言中的简单问题，且可在本地运行，无需依赖外部模型训练或云端服务。

Conclusion: 对于使用专有领域语言的工业企业而言，无需大规模定制模型，仅通过few-shot prompting即可在保障数据隐私的前提下有效利用现成LLM完成基础软件工程任务。

Abstract: How to best use Large Language Models (LLMs) for software engineering is covered in many publications in recent years. However, most of this work focuses on widely-used general purpose programming languages. The utility of LLMs for software within the industrial process automation domain, with highly-specialized languages that are typically only used in proprietary contexts, is still underexplored. Within this paper, we study enterprises can achieve on their own without investing large amounts of effort into the training of models specific to the domain-specific languages that are used. We show that few-shot prompting approaches are sufficient to solve simple problems in a language that is otherwise not well-supported by an LLM and that is possible on-premise, thereby ensuring the protection of sensitive company data.

</details>


### [5] [SQuaD: The Software Quality Dataset](https://arxiv.org/abs/2511.11265)
*Mikel Robredo,Matteo Esposito,Davide Taibi,Rafael Peñaloza,Valentina Lenarduzzi*

Main category: cs.SE

TL;DR: 本文提出了SQuaD（Software Quality Dataset），一个涵盖450个成熟开源项目的多维度、时间感知的软件质量数据集，整合了9种静态分析工具提取的700多个指标，支持对软件可维护性、技术债、演化和质量评估的大规模实证研究。


<details>
  <summary>Details</summary>
Motivation: 现有软件质量数据集通常仅关注代码异味、技术债或重构活动等有限维度，难以支持跨时间和多质量维度的综合分析，因此需要构建一个更全面、时间感知且覆盖多生态系统的数据集。

Method: 作者从Apache、Mozilla、FFmpeg和Linux内核等多样化的开源生态系统中选取450个项目，利用9种先进的静态分析工具（如SonarQube、PMD、RefactoringMiner等）提取方法、类、文件和项目级别的700多个质量指标，并整合版本控制、问题追踪、漏洞（CVE/CWE）及过程指标数据。

Result: 构建了包含63,586个发布版本的SQuaD数据集，提供多层次、多维度的软件质量与过程数据，支持JIT缺陷预测、技术债分析、软件演化等研究，并已公开发布于ZENODO。

Conclusion: SQuaD为软件质量相关的大规模实证研究提供了前所未有的资源，并为自动化更新和跨项目质量建模等未来研究方向奠定基础。

Abstract: Software quality research increasingly relies on large-scale datasets that measure both the product and process aspects of software systems. However, existing resources often focus on limited dimensions, such as code smells, technical debt, or refactoring activity, thereby restricting comprehensive analyses across time and quality dimensions. To address this gap, we present the Software Quality Dataset (SQuaD), a multi-dimensional, time-aware collection of software quality metrics extracted from 450 mature open-source projects across diverse ecosystems, including Apache, Mozilla, FFmpeg, and the Linux kernel. By integrating nine state-of-the-art static analysis tools, i.e., SonarQube, CodeScene, PMD, Understand, CK, JaSoMe, RefactoringMiner, RefactoringMiner++, and PyRef, our dataset unifies over 700 unique metrics at method, class, file, and project levels. Covering a total of 63,586 analyzed project releases, SQuaD also provides version control and issue-tracking histories, software vulnerability data (CVE/CWE), and process metrics proven to enhance Just-In-Time (JIT) defect prediction. The SQuaD enables empirical research on maintainability, technical debt, software evolution, and quality assessment at unprecedented scale. We also outline emerging research directions, including automated dataset updates and cross-project quality modeling to support the continuous evolution of software analytics. The dataset is publicly available on ZENODO (DOI: 10.5281/zenodo.17566690).

</details>


### [6] [SCRUTINEER: Detecting Logic-Level Usage Violations of Reusable Components in Smart Contracts](https://arxiv.org/abs/2511.11411)
*Xingshuang Lin,Binbin Zhao,Jinwen Wang,Qinge Xie,Xibin Zhao,Shouling Ji*

Main category: cs.SE

TL;DR: 本文提出了SCRUTINEER，首个用于自动检测智能合约可重用组件（SCR）逻辑级使用违规的实用系统，通过多阶段方法实现高精度识别。


<details>
  <summary>Details</summary>
Motivation: 智能合约可重用组件（SCR）虽提升开发效率，但其逻辑级使用违规（即符合使用规范却违背具体业务逻辑）会引发严重漏洞，现有方法难以有效检测此类问题。

Method: 提出SCRUTINEER系统：1）设计复合特征提取方法生成三种互补特征表示；2）构建基于大语言模型的知识库框架，提取逻辑级使用规则；3）开发检索增强生成驱动的检查器进行快速检索与分析；4）集成相似性检查器和快照推理冲突检查器进行精准检测。

Result: 在三个真实数据集上的评估表明，SCRUTINEER在检测SCR逻辑级使用违规方面达到80.77%的精确率、82.35%的召回率和81.55%的F1分数。

Conclusion: SCRUTINEER是首个能有效、自动检测SCR逻辑级使用违规的系统，实验证明其具备高准确性和实用性，有助于提升智能合约安全性。

Abstract: Smart Contract Reusable Components(SCRs) play a vital role in accelerating the development of business-specific contracts by promoting modularity and code reuse. However, the risks associated with SCR usage violations have become a growing concern. One particular type of SCR usage violation, known as a logic-level usage violation, is becoming especially harmful. This violation occurs when the SCR adheres to its specified usage rules but fails to align with the specific business logic of the current context, leading to significant vulnerabilities. Detecting such violations necessitates a deep semantic understanding of the contract's business logic, including the ability to extract implicit usage patterns and analyze fine-grained logical behaviors. To address these challenges, we propose SCRUTINEER, the first automated and practical system for detecting logic-level usage violations of SCRs. First, we design a composite feature extraction approach that produces three complementary feature representations, supporting subsequent analysis. We then introduce a Large Language Model-powered knowledge construction framework, which leverages comprehension-oriented prompts and domain-specific tools to extract logic-level usage and build the SCR knowledge base. Next, we develop a Retrieval-Augmented Generation-driven inspector, which combines a rapid retrieval strategy with both comprehensive and targeted analysis to identify potentially insecure logic-level usages. Finally, we implement a logic-level usage violation analysis engine that integrates a similarity-based checker and a snapshot-based inference conflict checker to enable accurate and robust detection. We evaluate SCRUTINEER from multiple perspectives on 3 ground-truth datasets. The results show that SCRUTINEER achieves a precision of 80.77%, a recall of 82.35%, and an F1-score of 81.55% in detecting logic-level usage violations of SCRs.

</details>


### [7] [CertiA360: Enhance Compliance Agility in Aerospace Software Development](https://arxiv.org/abs/2511.11550)
*J. Antonio Dantas Macedo,Hugo Fernandes,J. Eduardo Ferreira Ribeiro*

Main category: cs.SE

TL;DR: 本文提出了一种名为CertiA360的工具，旨在将敏捷方法的优势与航空航天领域安全关键系统开发中的DO-178C合规要求相结合，通过自动化变更管理和需求追溯，提升效率并支持认证。


<details>
  <summary>Details</summary>
Motivation: 在航空航天等高度监管的行业中，敏捷方法的灵活性与DO-178C等标准对文档、可追溯性和验证验证的严格要求之间存在冲突，亟需一种既能保持敏捷优势又能满足合规要求的解决方案。

Method: 设计并开发了CertiA360工具，用于在整个软件开发生命周期中自动化管理变更请求、提升需求成熟度并确保可追溯性；该工具在航空航天行业专家的密切合作下进行设计和验证。

Result: 专家反馈表明，CertiA360能减少人工工作量，在应对需求变更的同时确保符合DO-178C标准；尽管尚未通过DO-330工具资质认证，但结果显示出敏捷方法在适当调整后可有效应用于安全关键系统开发。

Conclusion: 适当定制的敏捷方法不仅能与航空航天等高监管领域的安全系统开发和认证要求共存，还能通过工具支持（如CertiA360）提升开发效率和合规性。

Abstract: Agile methods are characterised by iterative and incremental processes with a strong focus on flexibility and accommodating changing requirements based on either technical, regulatory, or stakeholder feedback. However, integrating Agile methods into safety-critical system development in the aerospace industry presents substantial challenges due to its strict compliance requirements, such as those outlined in the DO-178C standard. To achieve this vision, the flexibility of Agile must align with the rigorous certification guidelines, which emphasize documentation, traceability of requirements across different levels and disciplines, and comprehensive verification and validation (V&V) activities. The research work described in this paper proposes a way of using the strengths of the flexible nature of Agile methods to automate and manage change requests throughout the whole software development lifecycle, ensuring robust traceability, regulatory compliance and ultimately facilitating successful certification. This study proposes CertiA360, a tool designed to help teams improve requirement maturity, automate the changes in traceability, and align with the regulatory objectives. The tool was designed and validated in close collaboration with aerospace industry experts, using their feedback to ensure practical application and real-life effectiveness. The feedback collected demonstrated that the automation given by CertiA360 may reduce manual effort and allow response to changing requirements while ensuring compliance with DO-178C. While the tool is not yet qualified under DO-330 (Tool Qualification), findings suggest that when tailored appropriately, Agile methods can not only coexist with the requirements of safety-system development and certification in highly regulated domains like aerospace, but also add efficiency.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [8] [Millimeter-Wave UAV Channel Model with Height-Dependent Path Loss and Shadowing in Urban Scenarios](https://arxiv.org/abs/2511.10763)
*Abdul Saboor,Evgenii Vinogradov*

Main category: cs.NI

TL;DR: 本文提出了一种依赖于无人机基站（ABS）高度的毫米波信道模型，通过大量射线追踪仿真发现城市几何布局即使在建筑参数相同的情况下也会对路径损耗指数（PLE）产生小幅但一致的影响，并验证了所提模型在复杂城市场景中适用于ABS部署规划。


<details>
  <summary>Details</summary>
Motivation: 现有毫米波空地信道模型多依赖标准建筑参数，忽略了城市空间布局对视距概率（PLoS）和大尺度衰落（LSF）的潜在影响，而ABS高度变化显著影响信道特性，因此需要建立更精确的高度依赖性信道模型。

Method: 在26 GHz频段下，利用MATLAB射线追踪技术对四种具有相同建筑参数但空间布局不同的城市场景进行约1万次仿真；采用Sigmoid模型拟合仰角相关的PLoS，并通过指数拟合提取高度依赖的路径损耗指数（PLE）和阴影衰落趋势。

Result: 非视距（NLoS）路径损耗指数随高度增加趋近于2.5–3，视距（LoS）路径损耗指数稳定在2左右，阴影衰落随高度上升而减弱；此外，即使建筑参数一致，不同几何布局仍导致PLE出现±0.2的变化。

Conclusion: 所提出的统一高度依赖大尺度衰落模型与射线追踪结果吻合良好，能有效支持复杂城市场景下无人机基站的部署与链路规划。

Abstract: Uncrewed Aerial Vehicles (UAVs) serving as Aerial Base Stations (ABSs) are expected to extend 6G millimeter-Wave (mmWave) coverage and improve link reliability in urban areas. However, UAV-based Air-to-Ground (A2G) channels are highly dependent on height and urban geometry. This paper proposes an ABS height-dependent mmWave channel model and investigates whether urban geometry, beyond the standard built-up parameters, significantly affects LoS probability (PLoS) and Large-Scale Fading (LSF). Using MATLAB ray tracing at 26 GHz, we simulate approximately 10K city realizations for four urban layouts that share identical built-up parameters but differ in their spatial organization. We extract elevation-based PLoS using a sigmoid model and derive height-dependent Path-Loss Exponents (PLEs) and shadow-fading trends using exponential fits. Results show that PLE for Non-Line-of-Sight (NLoS) decreases toward 2.5-3 at high altitudes, Line-of-Sight (LoS) PLE remains near 2, and shadow fading reduces with height. We also find that geometric layout introduces a modest but consistent change in PLE (+/- 0.2), even when built-up parameters are fixed. The proposed unified model aligns well with ray-tracing statistics and offers a practical, height-dependent LSF model suitable for ABS planning in complex urban scenarios.

</details>


### [9] [Advancing IoT System Dependability: A Deep Dive into Management and Operation Plane Separation](https://arxiv.org/abs/2511.11204)
*Luoyao Hao,Shuo Zhang,Henning Schulzrinne*

Main category: cs.NI

TL;DR: 该论文提出通过分离管理平面与操作平面来提升大规模物联网系统的可靠性，设计了一种不依赖身份的策略框架，以灵活描述符实现对安全规范、操作标准和能耗限制等全局策略的主动部署与适应性执行。


<details>
  <summary>Details</summary>
Motivation: 当前大规模物联网系统缺乏统一且灵活的管理机制，难以有效实施全局策略（如安全规范、能效限制等），同时需兼容现有操作流程并整合多方管理实体（如监管机构与制造商）。

Method: 提出一种分离的管理平面架构，其中包含一个不依赖具体身份的策略框架，使用灵活描述符代替固定标识符，支持多类管理实体协同，并保持原有物联网操作流程不变。

Result: 在三个数据集上的评估表明，所提出的框架在策略表达能力和策略执行可靠性方面接近最优。

Conclusion: 该方法有效提升了大规模物联网系统的可管理性与策略执行的可靠性，同时具备良好的适应性和兼容性。

Abstract: We propose to enhance the dependability of large-scale IoT systems by separating the management and operation plane. We innovate the management plane to enforce overarching policies, such as safety norms, operation standards, and energy restrictions, and integrate multi-faceted management entities, including regulatory agencies and manufacturers, while the current IoT operational workflow remains unchanged. Central to the management plane is a meticulously designed, identity-independent policy framework that employs flexible descriptors rather than fixed identifiers, allowing for proactive deployment of overarching policies with adaptability to system changes. Our evaluation across three datasets indicates that the proposed framework can achieve near-optimal expressiveness and dependable policy enforcement.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [10] [Towards Assume-Guarantee Verification of Abilities in Stochastic Multi-Agent Systems](https://arxiv.org/abs/2511.10649)
*Wojciech Jamroga,Damian Kurpiewski,Łukasz Mikulski*

Main category: cs.MA

TL;DR: 本文提出了几种用于不完全信息下概率交替时态逻辑的假设-保证验证方案，并证明了其正确性，同时引入了一种新的非概率交替时态逻辑变体。


<details>
  <summary>Details</summary>
Motivation: 由于在具有不完全信息和随机环境下的智能体策略能力模型检测问题极其困难，作者希望通过假设-保证推理将复杂问题分解为更易处理的子问题。

Method: 提出多种适用于不完全信息下概率交替时态逻辑的假设-保证验证方案，并引入一种新的“至多达成φ”语义的交替时态逻辑变体。

Result: 证明了所提假设-保证方案的正确性，并对其完备性进行了讨论。

Conclusion: 假设-保证推理可有效辅助不完全信息下策略能力的模型检测，所提逻辑变体为形式化“仅达成某性质”的能力提供了新途径。

Abstract: Model checking of strategic abilities is a notoriously hard problem, even more so in the realistic case of agents with imperfect information, acting in a stochastic environment. Assume-guarantee reasoning can be of great help here, providing a way to decompose the complex problem into a small set of easier subproblems.
  In this paper, we propose several schemes for assume-guarantee verification of probabilistic alternating-time temporal logic with imperfect information. We prove the soundness of the schemes, and discuss their completeness. On the way, we also propose a new variant of (non-probabilistic) alternating-time logic, where the strategic modalities capture "achieving at most $\varphi$," analogous to Levesque's logic of "only knowing."

</details>


### [11] [Who Gets the Reward, Who Gets the Blame? Evaluation-Aligned Training Signals for Multi-LLM Agents](https://arxiv.org/abs/2511.10687)
*Chih-Hsuan Yang,Tanwi Mallick,Le Chen,Krishnan Raghavan,Azton Wells,Amal Gueroudji,Ian T. Foster,Rajeev Thakur*

Main category: cs.MA

TL;DR: 本文提出一个理论框架，将系统级评估通过合作博弈论归因与过程奖励建模转化为智能体级信用和消息级奖励信号，为大语言模型多智能体系统的训练提供统一、可审计的监督路径。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型在多智能体系统中的训练方法缺乏将系统级评估有效映射到智能体级和消息级学习信号的原则性方法。

Method: 结合合作博弈论（如Shapley值）进行智能体信用分配，并将其细化为每条消息的奖励信号；在失败情形下，通过首次错误定位生成修复感知的偏好信号。

Result: 该方法生成局部、带符号且信用守恒的学习信号，在成功时促进合作并抑制冗余或破坏行为，在失败时惩罚有害步骤并奖励修正尝试。

Conclusion: 本工作为多智能体大语言模型训练提供了概念性和理论性的基础，其训练信号可直接用于强化学习或偏好优化，但实证验证留待未来研究。

Abstract: Large Language Models (LLMs) in multi-agent systems (MAS) have shown promise for complex tasks, yet current training methods lack principled ways to connect system-level evaluation with agent-level and message-level learning. We propose a theoretical framework that unifies cooperative game-theoretic attribution with process reward modeling to transform system evaluation into agent credit and then into response-level signals. Unlike prior approaches that rely only on attribution (e.g., Shapley) or step-level labels (e.g., PRM), our method produces local, signed, and credit-conserving signals. In success cases, Shapley-based credit assignment fairly allocates outcomes across agents and is refined into per-message rewards that promote cooperation while discouraging redundancy or sabotage. In failure cases, first-error localization yields repair-aware preferences that penalize harmful steps while rewarding corrective attempts. The resulting signals are bounded, cooperative, and directly compatible with reinforcement-based or preference-based post-training, providing a unified and auditable pathway from global evaluation to local supervision in LLM multi-agent training. Our contribution is conceptual: we present a theoretical foundation and training signals, leaving empirical validation for future work.

</details>


### [12] [Exposing Weak Links in Multi-Agent Systems under Adversarial Prompting](https://arxiv.org/abs/2511.10949)
*Nirmit Arora,Sathvik Joel,Ishan Kavathekar,Palak,Rohan Gandhi,Yash Pandya,Tanuja Ganu,Aditya Kanade,Akshay Nambi*

Main category: cs.MA

TL;DR: SafeAgents 是一个用于细粒度评估多智能体系统（MAS）安全性的统一可扩展框架，揭示了常见架构设计中的安全漏洞，并提出了诊断指标 Dharma 以识别多智能体流程中的薄弱环节。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注单智能体安全性，缺乏针对多智能体系统中由协作机制引入的独特安全风险的统一评估框架和指标。

Method: 提出 SafeAgents 框架，系统分析计划构建策略、智能体间上下文共享和回退行为等设计选择对对抗性提示的敏感性；引入诊断指标 Dharma；在五种主流 MAS 架构和四个任务数据集上进行综合实验。

Result: 发现集中式架构将有害目标隐藏在原子指令中会降低鲁棒性；不同 MAS 设计模式普遍存在显著安全漏洞；Dharma 能有效识别多智能体管道中的弱点。

Conclusion: 多智能体系统的设计必须纳入安全考量，SafeAgents 为 MAS 安全评估提供了有效工具和实证依据。

Abstract: LLM-based agents are increasingly deployed in multi-agent systems (MAS). As these systems move toward real-world applications, their security becomes paramount. Existing research largely evaluates single-agent security, leaving a critical gap in understanding the vulnerabilities introduced by multi-agent design. However, existing systems fall short due to lack of unified frameworks and metrics focusing on unique rejection modes in MAS. We present SafeAgents, a unified and extensible framework for fine-grained security assessment of MAS. SafeAgents systematically exposes how design choices such as plan construction strategies, inter-agent context sharing, and fallback behaviors affect susceptibility to adversarial prompting. We introduce Dharma, a diagnostic measure that helps identify weak links within multi-agent pipelines. Using SafeAgents, we conduct a comprehensive study across five widely adopted multi-agent architectures (centralized, decentralized, and hybrid variants) on four datasets spanning web tasks, tool use, and code generation. Our findings reveal that common design patterns carry significant vulnerabilities. For example, centralized systems that delegate only atomic instructions to sub-agents obscure harmful objectives, reducing robustness. Our results highlight the need for security-aware design in MAS. Link to code is https://github.com/microsoft/SafeAgents

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [13] [FengHuang: Next-Generation Memory Orchestration for AI Inferencing](https://arxiv.org/abs/2511.10753)
*Jiamin Li,Lei Qu,Tao Zhang,Grigory Chirkov,Shuotao Xu,Peng Cheng,Lidong Zhou*

Main category: cs.DC

TL;DR: 本文提出了一种名为FengHuang的新型解耦式AI推理基础设施平台，通过多级共享内存架构、主动张量分页和近内存计算技术，显著降低了GPU内存需求与计算开销，并大幅提升GPU间通信效率，在多个大语言模型上验证了其可扩展性、灵活性与成本效益。


<details>
  <summary>Details</summary>
Motivation: 传统以GPU为中心的AI基础设施在推理任务中面临内存容量、带宽及互连扩展性的瓶颈，难以满足大规模语言模型日益增长的资源需求，亟需一种更具可扩展性和成本效益的新架构。

Method: 提出FengHuang平台，采用解耦式多级共享内存架构，结合高速本地内存与集中式远程内存，并引入主动张量分页和近内存计算技术优化张量操作。

Result: 仿真结果显示，FengHuang最多可减少93%的本地内存占用、节省50%的GPU计算资源，并将GPU间通信速度提升16至70倍；在GPT-3、Grok-1和QWEN3-235B等模型上，可在保持端到端性能的同时减少最多50%的GPU使用量。

Conclusion: FengHuang作为一种机架级AI推理基础设施扩展方案，在性能、成本和灵活性之间实现了良好平衡，其开放异构设计有助于打破厂商锁定，提升供应链弹性，并显著降低基础设施与能耗成本。

Abstract: This document presents a vision for a novel AI infrastructure design that has been initially validated through inference simulations on state-of-the-art large language models. Advancements in deep learning and specialized hardware have driven the rapid growth of large language models (LLMs) and generative AI systems. However, traditional GPU-centric architectures face scalability challenges for inference workloads due to limitations in memory capacity, bandwidth, and interconnect scaling. To address these issues, the FengHuang Platform, a disaggregated AI infrastructure platform, is proposed to overcome memory and communication scaling limits for AI inference. FengHuang features a multi-tier shared-memory architecture combining high-speed local memory with centralized disaggregated remote memory, enhanced by active tensor paging and near-memory compute for tensor operations. Simulations demonstrate that FengHuang achieves up to 93% local memory capacity reduction, 50% GPU compute savings, and 16x to 70x faster inter-GPU communication compared to conventional GPU scaling. Across workloads such as GPT-3, Grok-1, and QWEN3-235B, FengHuang enables up to 50% GPU reductions while maintaining end-user performance, offering a scalable, flexible, and cost-effective solution for AI inference infrastructure. FengHuang provides an optimal balance as a rack-level AI infrastructure scale-up solution. Its open, heterogeneous design eliminates vendor lock-in and enhances supply chain flexibility, enabling significant infrastructure and power cost reductions.

</details>


### [14] [HPCAgentTester: A Multi-Agent LLM Approach for Enhanced HPC Unit Test Generation](https://arxiv.org/abs/2511.10860)
*Rabimba Karanjai,Lei Xu,Weidong Shi*

Main category: cs.DC

TL;DR: 本文提出HPCAgentTester，一种基于多智能体大语言模型的自动化单元测试生成框架，专门用于提升高性能计算（HPC）中OpenMP和MPI程序的测试质量。


<details>
  <summary>Details</summary>
Motivation: 高性能计算中的单元测试面临并行性、复杂算法和硬件多样性等挑战，传统方法难以应对非确定性行为和同步问题。

Method: 设计了一个多智能体LLM框架HPCAgentTester，包含Recipe Agent和Test Agent，通过协作与批判循环迭代生成并优化针对并行结构和通信模式的上下文感知单元测试。

Result: HPCAgentTester能生成可编译且功能正确的测试用例，有效发现传统方法遗漏的细微错误，在编译成功率和正确性方面显著优于单一大语言模型。

Conclusion: HPCAgentTester为并行软件系统提供了一种更稳健、可扩展的自动化测试解决方案，显著提升了HPC软件的可靠性保障能力。

Abstract: Unit testing in High-Performance Computing (HPC) is critical but challenged by parallelism, complex algorithms, and diverse hardware. Traditional methods often fail to address non-deterministic behavior and synchronization issues in HPC applications. This paper introduces HPCAgentTester, a novel multi-agent Large Language Model (LLM) framework designed to automate and enhance unit test generation for HPC software utilizing OpenMP and MPI. HPCAgentTester employs a unique collaborative workflow where specialized LLM agents (Recipe Agent and Test Agent) iteratively generate and refine test cases through a critique loop. This architecture enables the generation of context-aware unit tests that specifically target parallel execution constructs, complex communication patterns, and hierarchical parallelism. We demonstrate HPCAgentTester's ability to produce compilable and functionally correct tests for OpenMP and MPI primitives, effectively identifying subtle bugs that are often missed by conventional techniques. Our evaluation shows that HPCAgentTester significantly improves test compilation rates and correctness compared to standalone LLMs, offering a more robust and scalable solution for ensuring the reliability of parallel software systems.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [15] [Tiny Chiplets Enabled by Packaging Scaling: Opportunities in ESD Protection and Signal Integrity](https://arxiv.org/abs/2511.10760)
*Emad Haque,Pragnya Sudershan Nalla,Jeff Zhang,Sachin S. Sapatnekar,Chaitali Chakrabarti,Yu Cao*

Main category: cs.AR

TL;DR: 本文指出在2.5D/3D先进封装中，传统I/O电路（如ESD保护和信号传输）带来显著面积开销，限制了chiplet尺寸缩小；通过重新审视可靠性需求并结合寄生提取与SPICE仿真，作者证明未来封装技术可大幅简化这些电路，从而推动chiplet微型化及提升其组合性与复用性。


<details>
  <summary>Details</summary>
Motivation: 传统I/O电路（包括ESD保护和信号机制）在2.5D/3D异构集成中引入了显著的面积开销，成为chiplet尺寸难以缩小至100 mm²以下的主要瓶颈。

Method: 从chiplet接口设计角度重新评估可靠性要求，并通过寄生参数提取与SPICE仿真验证ESD保护与芯片间信号电路在未来先进封装中可被大幅简化。

Result: 研究表明，在未来2.5D/3D封装技术下，ESD保护和互连信号电路可以显著简化，从而支持更小尺寸的chiplet设计。

Conclusion: 简化I/O电路为chiplet进一步微型化铺平道路，并提升微小chiplet的组合性与复用性。

Abstract: The scaling of advanced packaging technologies provides abundant interconnection resources for 2.5D/3D heterogeneous integration (HI), thereby enabling the construction of larger-scale VLSI systems with higher energy efficiency in data movement. However, conventional I/O circuitry, including electrostatic discharge (ESD) protection and signaling, introduces significant area overhead. Prior studies have identified this overhead as a major constraint in reducing chiplet size below 100 mm2. In this study, we revisit reliability requirements from the perspective of chiplet interface design. Through parasitic extraction and SPICE simulations, we demonstrate that ESD protection and inter-chiplet signaling can be substantially simplified in future 2.5D/3D packaging technologies. Such simplification, in turn, paves the road for further chiplet miniaturization and improves the composability and reusability of tiny chiplets.

</details>


### [16] [T-MAN: Enabling End-to-End Low-Bit LLM Inference on NPUs via Unified Table Lookup](https://arxiv.org/abs/2511.11248)
*Jianyu Wei,Qingtao Li,Shijie Cao,Lingxiao Ma,Zixu Hao,Yanyong Zhang,Xiaoyan Hu,Ting Cao*

Main category: cs.AR

TL;DR: 该论文提出了一种基于查表法的新型方法，以提升在设备端NPU上运行大语言模型（LLM）推理的效率。通过统一的表布局与分块策略，结合两阶段融合查表反量化和并发层次引导的分块技术，在预填充和解码阶段分别实现了1.4倍和3.1倍的加速，并节省了84%的能耗。


<details>
  <summary>Details</summary>
Motivation: 当前设备上的NPU在执行LLM推理时性能不如CPU，主要因为NPU对GEMM以外的操作（如反量化）支持较差。现有方法要么将预填充和解码阶段分别放在NPU和CPU上执行，要么全部放在NPU上但牺牲精度。因此，亟需一种能在NPU上高效、准确地执行完整LLM推理的方法。

Method: 作者基于低比特可将目标计算编码进可接受大小查找表的洞察，提出用查表操作替代NPU不擅长的硬件操作。具体包括：(1) 融合两级查表反量化；(2) 并发-层次引导的分块策略；并据此设计统一的表布局，将预填充阶段实现为三阶段流水线，将基于查表的解码映射到NPU的向量单元上。

Result: 实验结果表明，相比基线NPU方法，所提方法在预填充和解码阶段分别实现了1.4倍和3.1倍的速度提升，并节省了84%的能耗。

Conclusion: 该工作通过查表机制有效解决了NPU在非GEMM操作上的性能瓶颈，显著提升了LLM在终端设备NPU上的推理效率与能效，同时保持精度，为端侧LLM部署提供了实用方案。

Abstract: Large language models (LLMs) are increasingly deployed on customer devices. To support them, current devices are adopting SoCs (System on Chip) with NPUs (Neural Processing Unit) installed. Although high performance is expected, LLM inference on NPUs is slower than its CPU counterpart. The reason is that NPUs have poor performance on computations other than GEMM, like dequantization. Current works either disaggregate prefill on the NPUs and decoding on the CPUs, or put both on the NPUs but with an accuracy loss. To solve this issue, based on the insight that low-bit can enable target computation encoded within an acceptably sized table, we propose table lookup to subsume hardware operations otherwise unsupported. To realize this, we overcome the conflicting hardware behavior of prefill and decoding to design a unified table layout and tiling through (1) fused two-level table-based dequantization and (2) concurrency-hierarchy-guided tiling. Based on that, we implement the prefill phase by three-stage pipeline and map the table-lookup-based decoding to NPU's vector units. Results show 1.4x and 3.1x speedup for prefill and decoding respectively, and 84% energy savings compared to the baseline NPU methods. The code is available at https://github.com/microsoft/T-MAC/tree/main/t-man.

</details>
