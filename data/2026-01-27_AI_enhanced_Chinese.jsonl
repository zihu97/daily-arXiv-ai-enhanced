{"id": "2601.17454", "categories": ["cs.MA", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.17454", "abs": "https://arxiv.org/abs/2601.17454", "authors": ["Muhammad Ahmed Atif", "Nehal Naeem Haji", "Mohammad Shahid Shaikh", "Muhammad Ebad Atif"], "title": "Embodiment-Induced Coordination Regimes in Tabular Multi-Agent Q-Learning", "comment": null, "summary": "Centralized value learning is often assumed to improve coordination and stability in multi-agent reinforcement learning, yet this assumption is rarely tested under controlled conditions. We directly evaluate it in a fully tabular predator-prey gridworld by comparing independent and centralized Q-learning under explicit embodiment constraints on agent speed and stamina. Across multiple kinematic regimes and asymmetric agent roles, centralized learning fails to provide a consistent advantage and is frequently outperformed by fully independent learning, even under full observability and exact value estimation. Moreover, asymmetric centralized-independent configurations induce persistent coordination breakdowns rather than transient learning instability. By eliminating confounding effects from function approximation and representation learning, our tabular analysis isolates coordination structure as the primary driver of these effects. The results show that increased coordination can become a liability under embodiment constraints, and that the effectiveness of centralized learning is fundamentally regime and role dependent rather than universal.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2601.18284", "categories": ["cs.MA"], "pdf": "https://arxiv.org/pdf/2601.18284", "abs": "https://arxiv.org/abs/2601.18284", "authors": ["Hsiao-Chuan Chang", "Sheng-You Huang", "Yen-Chi Chen", "I-Chen Wu"], "title": "VissimRL: A Multi-Agent Reinforcement Learning Framework for Traffic Signal Control Based on Vissim", "comment": null, "summary": "Traffic congestion remains a major challenge for urban transportation, leading to significant economic and environmental impacts. Traffic Signal Control (TSC) is one of the key measures to mitigate congestion, and recent studies have increasingly applied Reinforcement Learning (RL) for its adaptive capabilities. With respect to SUMO and CityFlow, the simulator Vissim offers high-fidelity driver behavior modeling and wide industrial adoption but remains underutilized in RL research due to its complex interface and lack of standardized frameworks. To address this gap, this paper proposes VissimRL, a modular RL framework for TSC that encapsulates Vissim's COM interface through a high-level Python API, offering standardized environments for both single- and multi-agent training. Experiments show that VissimRL significantly reduces development effort while maintaining runtime efficiency, and supports consistent improvements in traffic performance during training, as well as emergent coordination in multi-agent control. Overall, VissimRL demonstrates the feasibility of applying RL in high-fidelity simulations and serves as a bridge between academic research and practical applications in intelligent traffic signal control.", "AI": {"tldr": "\u63d0\u51faVissimRL\u6846\u67b6\u89e3\u51b3\u4ea4\u901a\u706f\u63a7\u5236\u7814\u7a76\u4e2dVissim\u4eff\u771f\u5e73\u53f0\u63a5\u53e3\u590d\u6742\u6027\u95ee\u9898\uff0c\u4e3a\u5f3a\u5316\u5b66\u4e60\u63d0\u4f9b\u6807\u51c6\u5316\u73af\u5883\u3002", "motivation": "Vissim\u867d\u5177\u9ad8\u7cbe\u5ea6\u9a7e\u9a76\u884c\u4e3a\u6a21\u62df\u4e0e\u5de5\u4e1a\u5e94\u7528\u4f18\u52bf\uff0c\u4f46\u56e0\u63a5\u53e3\u590d\u6742\u548c\u7f3a\u4e4f\u6807\u51c6\u6846\u67b6\uff0c\u5728\u5f3a\u5316\u5b66\u4e60\u7814\u7a76\u4e2d\u672a\u88ab\u5145\u5206\u5229\u7528\u3002", "method": "\u5f00\u53d1\u6a21\u5757\u5316\u6846\u67b6VissimRL\uff0c\u901a\u8fc7Python API\u5c01\u88c5Vissim\u7684COM\u63a5\u53e3\uff0c\u652f\u6301\u5355/\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u73af\u5883\u3002", "result": "\u663e\u8457\u964d\u4f4e\u5f00\u53d1\u6210\u672c\u5e76\u4fdd\u6301\u8fd0\u884c\u6548\u7387\uff0c\u8bad\u7ec3\u4e2d\u6301\u7eed\u63d0\u5347\u4ea4\u901a\u6027\u80fd\uff0c\u591a\u667a\u80fd\u4f53\u63a7\u5236\u4e2d\u5b9e\u73b0\u534f\u540c\u6548\u5e94\u3002", "conclusion": "VissimRL\u8bc1\u5b9e\u5f3a\u5316\u5b66\u4e60\u5728\u9ad8\u7cbe\u5ea6\u4eff\u771f\u4e2d\u7684\u5e94\u7528\u53ef\u884c\u6027\uff0c\u4e3a\u5b66\u672f\u7814\u7a76\u4e0e\u667a\u80fd\u4ea4\u901a\u5b9e\u9645\u5e94\u7528\u67b6\u8bbe\u6865\u6881\u3002"}}
{"id": "2601.17534", "categories": ["cs.NI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2601.17534", "abs": "https://arxiv.org/abs/2601.17534", "authors": ["Mounir Bensalem", "Fin Gentzen", "Tuck-Wai Choong", "Yu-Chiao Jhuang", "Admela Jukan", "Jenq-Shiou Leu"], "title": "Efficient Self-Learning and Model Versioning for AI-native O-RAN Edge", "comment": "This paper is uploaded here for research community, thus it is for non-commercial purposes", "summary": "The AI-native vision of 6G requires Radio Access Networks to train, deploy, and continuously refine thousands of machine learning (ML) models that drive real-time radio network optimization. Although the Open RAN (O-RAN) architecture provides open interfaces and an intelligent control plane, it leaves the life-cycle management of these models unspecified. Consequently, operators still rely on ad-hoc, manual update practices that can neither scale across the heterogeneous, multi-layer stack of Cell-Site, Edge-, Regional-, and Central-Cloud domains, nor across the three O-RAN control loops (real-, near-real-, and non-real-time). We present a self-learning framework that provides an efficient closed-loop version management for an AI-native O-RAN edge. In this framework, training pipelines in the Central/Regional Cloud continuously generate new models, which are cataloged along with their resource footprints, security scores, and accuracy metrics in a shared version repository. An Update Manager consults this repository and applies a self-learning policy to decide when and where each new model version should be promoted into operation. A container orchestrator then realizes these decisions across heterogeneous worker nodes, enabling multiple services (rApps, xApps, and dApps) to obtain improved inference with minimal disruption. Simulation results show that an efficient RL-driven decision-making can guarantee quality of service, bounded latencies while balancing model accuracy, system stability, and resilience.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2601.17292", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2601.17292", "abs": "https://arxiv.org/abs/2601.17292", "authors": ["Zhiyin Zhou"], "title": "Risk-based test framework for LLM features in regulated software", "comment": null, "summary": "Large language models are increasingly embedded in regulated and safety-critical software, including clinical research platforms and healthcare information systems. While these features enable natural language search, summarization, and configuration assistance, they introduce risks such as hallucinations, harmful or out-of-scope advice, privacy and security issues, bias, instability under change, and adversarial misuse. Prior work on machine learning testing and AI assurance offers useful concepts but limited guidance for interactive, product-embedded assistants. This paper proposes a risk-based testing framework for LLM features in regulated software: a six-category risk taxonomy, a layered test strategy mapping risks to concrete tests across guardrail, orchestration, and system layers, and a case study applying the approach to a Knowledgebase assistant in a clinical research platform.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u9488\u5bf9\u53d7\u76d1\u7ba1\u8f6f\u4ef6\u4e2dLLM\u529f\u80fd\u7684\u57fa\u4e8e\u98ce\u9669\u6d4b\u8bd5\u6846\u67b6\uff0c\u5305\u62ec\u98ce\u9669\u5206\u7c7b\u3001\u5206\u5c42\u6d4b\u8bd5\u7b56\u7565\u548c\u4e34\u5e8a\u6848\u4f8b\u9a8c\u8bc1\u3002", "motivation": "\u6cd5\u5f8b\u8bed\u8a00\u6a21\u578b\u5d4c\u5165\u533b\u7597\u7b49\u5173\u952e\u7cfb\u7edf\u65f6\u5b58\u5728\u5e7b\u89c9\u3001\u504f\u89c1\u3001\u9690\u79c1\u6cc4\u9732\u7b49\u98ce\u9669\uff0c\u73b0\u6709AI\u6d4b\u8bd5\u65b9\u6cd5\u65e0\u6cd5\u8986\u76d6\u4ea4\u4e92\u5f0f\u4ea7\u54c1\u573a\u666f\u3002", "method": "\u5efa\u7acb\u516d\u7c7b\u98ce\u9669\u5206\u7c7b\u6cd5\uff0c\u8bbe\u8ba1\u5206\u5c42\u6d4b\u8bd5\u7b56\u7565\uff08\u5c06\u98ce\u9669\u6620\u5c04\u81f3\u62a4\u536b\u5c42\u03bb\u03cd\u534f\u8c03\u5c42\u548c\u7cfb\u7edf\u5c42\uff09\uff0c\u5e76\u901a\u8fc7\u4e34\u5e8a\u7814\u7a76\u5e73\u53f0\u7684\u77e5\u8bc6\u5e93\u52a9\u624b\u8fdb\u884c\u6848\u4f8b\u9a8c\u8bc1\u3002", "result": "\u5f00\u53d1\u4e86\u53ef\u843d\u5730\u7684\u6846\u67b6\uff0c\u5b9e\u73b0\u98ce\u9669\u5230\u5177\u4f53\u6d4b\u8bd5\u7684\u6620\u5c04\uff0c\u4e34\u5e8a\u6848\u4f8b\u8bc1\u660e\u4e86\u6846\u67b6\u5728\u8bc6\u522b\u6f0f\u6d1e\u65b9\u9762\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u53d7\u76d1\u7ba1\u8f6f\u4ef6\u7684 EgyptLLM\u529f\u80fd\u63d0\u4f9b\u4e86\u9488\u5bf9\u6027\u6d4b\u8bd5\u8def\u5f84\uff0c\u5206\u5c42\u7b56\u7565\u80fd\u7cfb\u7edf\u5316\u7f13\u89e3\u5b89\u5168\u98ce\u9669\u3002"}}
{"id": "2601.17295", "categories": ["cs.NI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.17295", "abs": "https://arxiv.org/abs/2601.17295", "authors": ["Xinyu Zhu", "Parisa Fard Moshiri", "Poonam Lohan", "Burak Kantarci", "Emil Janulewicz"], "title": "Structure-Aware NL-to-SQL for SFC Provisioning via AST-Masking Empowered Language Models", "comment": "6 pages, 3 figures, accepted to IEEE International Conference on Communications (ICC) 2026", "summary": "Effective Service Function Chain (SFC) provisioning requires precise orchestration in dynamic and latency-sensitive networks. Reinforcement Learning (RL) improves adaptability but often ignores structured domain knowledge, which limits generalization and interpretability. Large Language Models (LLMs) address this gap by translating natural language (NL) specifications into executable Structured Query Language (SQL) commands for specification-driven SFC management. Conventional fine-tuning, however, can cause syntactic inconsistencies and produce inefficient queries. To overcome this, we introduce Abstract Syntax Tree (AST)-Masking, a structure-aware fine-tuning method that uses SQL ASTs to assign weights to key components and enforce syntax-aware learning without adding inference overhead. Experiments show that AST-Masking significantly improves SQL generation accuracy across multiple language models. FLAN-T5 reaches an Execution Accuracy (EA) of 99.6%, while Gemma achieves the largest absolute gain from 7.5% to 72.0%. These results confirm the effectiveness of structure-aware fine-tuning in ensuring syntactically correct and efficient SQL generation for interpretable SFC orchestration.", "AI": {"tldr": "\u63d0\u51faAST-Masking\u65b9\u6cd5\uff0c\u901a\u8fc7SQL\u62bd\u8c61\u8bed\u6cd5\u6811\u8fdb\u884c\u7ed3\u6784\u611f\u77e5\u5fae\u8c03\uff0c\u663e\u8457\u63d0\u9ad8\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728SFC\u7f16\u6392\u4e2d\u7684SQL\u751f\u6210\u51c6\u786e\u6027\u548c\u6548\u7387\u3002", "motivation": "\u5f3a\u5316\u5b66\u4e60\u63d0\u5347SFC\u7f16\u6392\u9002\u5e94\u6027\u4f46\u5ffd\u7565\u7ed3\u6784\u5316\u77e5\u8bc6\uff0c\u5bfc\u81f4\u901a\u7528\u5316\u548c\u53ef\u89e3\u91ca\u6027\u53d7\u9650\uff0c\u4e14\u4f20\u7edf\u5fae\u8c03\u4ea7\u751f\u7684SQL\u5b58\u5728\u8bed\u6cd5\u9519\u8bef\u548c\u4f4e\u6548\u95ee\u9898\u3002", "method": "\u91c7\u7528AST-Masking\u6280\u672f\uff0c\u5229\u7528SQL\u62bd\u8c61\u8bed\u6cd5\u6811\u4e3a\u5173\u952e\u7ec4\u4ef6 ............................................................................................................\u5206\u914d\u6743\u91cd\uff0c\u5b9e\u65bd\u8f7b\u91cf\u5316\u8bed\u6cd5\u611f\u77e5\u5b66\u4e60\uff0c\u907f\u514d\u989d\u5916\u63a8\u7406\u5f00\u9500\u3002", "result": "\u5b9e\u9a8c\u8868\u660eSQL\u751f\u6210\u51c6\u786e\u6027\u5927\u5e45\u63d0\u5347\uff1aFLAN-T5\u6267\u884c\u7cbe\u5ea6\u8fbe99.6%\uff0cGemma\u7edd\u5bf9\u589e\u76ca\u6700\u5927\uff0c\u4ece7.5%\u63d0\u9ad8\u523072.0%\u3002", "conclusion": "\u7ed3\u6784\u611f\u77e5\u5fae\u8c03\u786e\u4fdd\u8bed\u6cd5\u6b63\u786e\u9ad8\u6548SQL\u751f\u6210\uff0c\u6709\u6548\u652f\u6301\u53ef\u89e3\u91ca\u7684SFC\u7f16\u6392\uff0c\u8bc1\u5b9e\u8be5\u65b9\u6cd5\u5728\u5404\u7c7b\u8bed\u8a00\u6a21\u578b\u4e2d\u7684\u53ef\u9760\u6027\u3002"}}
{"id": "2601.17551", "categories": ["cs.PF", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.17551", "abs": "https://arxiv.org/abs/2601.17551", "authors": ["Thomas Ziller", "Shashikant Ilager", "Alessandro Tundo", "Ezio Bartocci", "Leonardo Mariani", "Ivona Brandic"], "title": "GreenServ: Energy-Efficient Context-Aware Dynamic Routing for Multi-Model LLM Inference", "comment": "Paper under submisison", "summary": "Large language models (LLMs) demonstrate remarkable capabilities, but their broad deployment is limited by significant computational resource demands, particularly energy consumption during inference. Static, one-model-fits-all inference strategies are often inefficient, as they do not exploit the diverse range of available models or adapt to varying query requirements.\n  This paper presents GreenServ, a dynamic, context-aware routing framework that optimizes the trade-off between inference accuracy and energy efficiency. GreenServ extracts lightweight contextual features from each query, including task type, semantic cluster, and text complexity, and routes queries to the most suitable model from a heterogeneous pool, based on observed accuracy and energy usage. We employ a multi-armed bandit approach to learn adaptive routing policies online. This approach operates under partial feedback, eliminates the need for extensive offline calibration, and streamlines the integration of new models into the inference pipeline.\n  We evaluated GreenServ across five benchmark tasks and a pool of 16 contemporary open-access LLMs. Experimental results show that GreenServ consistently outperforms static (single-model) and random baselines. In particular, compared to random routing, GreenServ achieved a 22% increase in accuracy while reducing cumulative energy consumption by 31%. Finally, we evaluated GreenServ with RouterBench, achieving an average accuracy of 71.7% with a peak accuracy of 75.7%. All artifacts are open-source and available as an anonymous repository for review purposes here: https://anonymous.4open.science/r/llm-inference-router-EBEA/README.md", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2601.17136", "categories": ["cs.DC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.17136", "abs": "https://arxiv.org/abs/2601.17136", "authors": ["Julian Bellavita", "Matthew Rubino", "Nakul Iyer", "Andrew Chang", "Aditya Devarakonda", "Flavio Vella", "Giulia Guidi"], "title": "Communication-Avoiding Linear Algebraic Kernel K-Means on GPUs", "comment": null, "summary": "Clustering is an important tool in data analysis, with K-means being popular for its simplicity and versatility. However, it cannot handle non-linearly separable clusters. Kernel K-means addresses this limitation but requires a large kernel matrix, making it computationally and memory intensive. Prior work has accelerated Kernel K-means by formulating it using sparse linear algebra primitives and implementing it on a single GPU. However, that approach cannot run on datasets with more than approximately 80,000 samples due to limited GPU memory.\n  In this work, we address this issue by presenting a suite of distributed-memory parallel algorithms for large-scale Kernel K-means clustering on multi-GPU systems. Our approach maps the most computationally expensive components of Kernel K-means onto communication-efficient distributed linear algebra primitives uniquely tailored for Kernel K-means, enabling highly scalable implementations that efficiently cluster million-scale datasets. Central to our work is the design of partitioning schemes that enable communication-efficient composition of the linear algebra primitives that appear in Kernel K-means.\n  Our 1.5D algorithm consistently achieves the highest performance, enabling Kernel K-means to scale to data one to two orders of magnitude larger than previously practical. On 256 GPUs, it achieves a geometric mean weak scaling efficiency of $79.7\\%$ and a geometric mean strong scaling speedup of $4.2\\times$. Compared to our 1D algorithm, the 1.5D approach achieves up to a $3.6\\times$ speedup on 256 GPUs and reduces clustering time from over an hour to under two seconds relative to a single-GPU sliding window implementation. Our results show that distributed algorithms designed with application-specific linear algebraic formulations can achieve substantial performance improvement.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e00\u5957\u5206\u5e03\u5f0f\u5185\u5b58\u5e76\u884c\u7b97\u6cd5\uff0c\u7528\u4e8e\u5728\u591aGPU\u7cfb\u7edf\u4e0a\u5b9e\u73b0\u5927\u89c4\u6a21Kernel K-means\u805a\u7c7b\uff0c\u89e3\u51b3\u4e86\u5355GPU\u5185\u5b58\u9650\u5236\u95ee\u9898\uff0c\u4f7f\u6570\u636e\u96c6\u89c4\u6a21\u63d0\u5347\u4e00\u5230\u4e24\u4e2a\u6570\u91cf\u7ea7\u3002", "motivation": "Kernel K-means\u80fd\u5904\u7406\u975e\u7ebf\u6027\u53ef\u5206\u96c6\u7fa4\uff0c\u4f46\u73b0\u6709\u5355GPU\u5b9e\u73b0\u53d7\u9650\u4e8e\u663e\u5b58\uff0c\u65e0\u6cd5\u5904\u7406\u8d85\u8fc7\u7ea68\u4e07\u4e2a\u6837\u672c\u7684\u6570\u636e\u96c6\uff0c\u9650\u5236\u4e86\u5176\u53ef\u6269\u5c55\u6027\u3002", "method": "\u8bbe\u8ba1\u4e86\u5206\u5e03\u5f0f\u5e76\u884c\u7b97\u6cd5\u53ca\u5206\u533a\u65b9\u6848\uff0c\u5c06\u8ba1\u7b97\u5bc6\u96c6\u578b\u7ec4\u4ef6\u6620\u5c04\u5230\u9ad8\u6548\u901a\u4fe1\u7684\u4ee3\u6570\u539f\u8bed\u4e0a\uff0c\u4f18\u5316\u5bf9\u6838\u77e9\u9635\u7684\u5904\u7406\u3002\u5173\u952e\u521b\u65b0\u662f1.5D\u5206\u533a\u7b56\u7565\uff0c\u4ee5\u51cf\u5c11\u901a\u4fe1\u5f00\u9500\u3002", "result": "1.5D\u7b97\u6cd5\u6027\u80fd\u6700\u4f73\uff1a256\u4e2aGPU\u65f6\u51e0\u4f55\u5e73\u5747\u5f31\u6269\u5c55\u6548\u7387\u8fbe79.7%\uff0c\u5f3a\u6269\u5c55\u52a0\u901f\u6bd4\u4e3a4.2\u500d\uff1b\u6bd41D\u7b97\u6cd5\u5feb3.6\u500d\uff0c\u805a\u7c7b\u65f6\u95f4\u4ece\u5c0f\u65f6\u7ea7\u964d\u81f3\u79d2\u7ea7\u3002\u53ef\u6709\u6548\u5904\u7406\u767e\u4e07\u7ea7\u6570\u636e\u96c6\u3002", "conclusion": "\u7ed3\u5408\u5e94\u7528\u7279\u5b9a\u4ee3\u6570\u516c\u5f0f\u8bbe\u8ba1\u7684\u5206\u5e03\u5f0f\u7b97\u6cd5\u53ef\u5927\u5e45\u63d0\u5347\u6027\u80fd\uff0c\u8bc1\u660e\u5176\u5728\u5904\u7406\u5927\u6570\u636e\u805a\u7c7b\u4e2d\u7684\u5b9e\u7528\u6027\u3002"}}
{"id": "2601.17390", "categories": ["cs.SE", "cs.CR", "cs.PL"], "pdf": "https://arxiv.org/pdf/2601.17390", "abs": "https://arxiv.org/abs/2601.17390", "authors": ["Yayi Wang", "Shenao Wang", "Jian Zhao", "Shaosen Shi", "Ting Li", "Yan Cheng", "Lizhong Bian", "Kan Yu", "Yanjie Zhao", "Haoyu Wang"], "title": "YASA: Scalable Multi-Language Taint Analysis on the Unified AST at Ant Group", "comment": null, "summary": "Modern enterprises increasingly adopt diverse technology stacks with various programming languages, posing significant challenges for static application security testing (SAST). Existing taint analysis tools are predominantly designed for single languages, requiring substantial engineering effort that scales with language diversity. While multi-language tools like CodeQL, Joern, and WALA attempt to address these challenges, they face limitations in intermediate representation design, analysis precision, and extensibility, which make them difficult to scale effectively for large-scale industrial applications at Ant Group. To bridge this gap, we present YASA (Yet Another Static Analyzer), a unified multi-language static taint analysis framework designed for industrial-scale deployment. Specifically, YASA introduces the Unified Abstract Syntax Tree (UAST) that provides a unified abstraction for compatibility across diverse programming languages. Building on the UAST, YASA performs point-to analysis and taint propagation, leveraging a unified semantic model to manage language-agnostic constructs, while incorporating language-specific semantic models to handle other unique language features. When compared to 6 single- and 2 multi-language static analyzers on an industry-standard benchmark, YASA consistently outperformed all baselines across Java, JavaScript, Python, and Go. In real-world deployment within Ant Group, YASA analyzed over 100 million lines of code across 7.3K internal applications. It identified 314 previously unknown taint paths, with 92 of them confirmed as 0-day vulnerabilities. All vulnerabilities were responsibly reported, with 76 already patched by internal development teams, demonstrating YASA's practical effectiveness for securing large-scale industrial software systems.", "AI": {"tldr": "YASA presents a unified multi-language static taint analysis framework for industrial-scale applications.", "motivation": "Current SAST tools struggle with battalion diverse programming languages, causing scalability and precision limitations.", "method": "Uses a Unified Abstract Syntax Tree (UAST) for cross-language analysis, with unified semantic models for common features and language-specific models for unique aspects.", "result": "Outperformed 8 baselines in benchmarks; in deployment, analyzed 100M+ LoC, finding 314 new taint paths (92 confirmed vulnerabilities), with 76 patched.", "conclusion": "YASA demonstrates high effectiveness for securing large-scale industrial systems by overcoming language diversity barriers."}}
{"id": "2601.17546", "categories": ["cs.DC", "cs.DB"], "pdf": "https://arxiv.org/pdf/2601.17546", "abs": "https://arxiv.org/abs/2601.17546", "authors": ["Ravi Kiran Kodali", "Vinoth Punniyamoorthy", "Akash Kumar Agarwal", "Bikesh Kumar", "Balakrishna Pothineni", "Aswathnarayan Muthukrishnan Kirubakaran", "Sumit Saha", "Nachiappan Chockalingam"], "title": "Push Down Optimization for Distributed Multi Cloud Data Integration", "comment": null, "summary": "Enterprises increasingly adopt multi cloud architectures to take advantage of diverse database engines, regional availability, and cost models. In these environments, ETL pipelines must process large, distributed datasets while minimizing latency and transfer cost. Push down optimization, which executes transformation logic within database engines rather than within the ETL tool, has proven highly effective in single cloud systems. However, when applied across multiple clouds, it faces challenges related to data movement, heterogeneous SQL engines, orchestration complexity, and fragmented security controls. This paper examines the feasibility of push down optimization in multi cloud ETL pipelines and analyzes its benefits and limitations. It evaluates localized push down, hybrid models, and data federation techniques that reduce cross cloud traffic while improving performance. A case study across Redshift and BigQuery demonstrates measurable gains, including lower end to end runtime, reduced transfer volume, and improved cost efficiency. The study highlights practical strategies that organizations can adopt to improve ETL scalability and reliability in distributed cloud environments.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u5728\u591a\u4e91ETL\u7ba1\u9053\u4e2d\u63a8\u5f0f\u4f18\u5316\u7684\u53ef\u884c\u6027\uff0c\u5206\u6790\u5176\u4f18\u70b9\u548c\u5c40\u9650\uff0c\u5e76\u901a\u8fc7Redshift\u3001BigQuery\u6848\u4f8b\u5c55\u793a\u6027\u80fd\u63d0\u5347\u7b56\u7565\u3002", "motivation": "\u4f01\u4e1a\u91c7\u7528\u591a\u4e91\u67b6\u6784\u9762\u4e34\u6570\u636e\u79fb\u52a8\u3001\u5f02\u6784SQL\u5f15\u64ce\u548c\u5b89\u5168\u6027\u7b49\u6311\u6218\uff0c\u63a8\u5f0f\u4f18\u5316\u5728\u5355\u4e91\u4e2d\u6709\u6548\uff0c\u4f46\u5728\u591a\u4e91\u73af\u5883\u4e2d\u5b58\u5728\u969c\u788d\uff0c\u9700\u63a2\u7d22\u5176\u5e94\u7528\u6f5c\u529b\u4ee5\u63d0\u5347ETL\u6548\u7387\u3002", "method": "\u8bc4\u4f30\u672c\u5730\u5316\u63a8\u5f0f\u3001\u6df7\u5408\u6a21\u578b\u4e0e\u6570\u636e\u8054\u90a6\u6280\u672f\uff0c\u4ee5\u51cf\u5c11\u8de8\u4e91\u6d41\u91cf\uff1b\u7ed3\u5408Redshift\u3001BigQuery\u6848\u4f8b\u8fdb\u884c\u5b9e\u8bc1\u5206\u6790\u4ee5\u4f18\u5316\u6027\u80fd\u548c\u6210\u672c\u3002", "result": "\u6848\u4f8b\u7814\u7a76\u663e\u793a\u663e\u8457\u589e\u76ca\uff1a\u7aef\u5230\u7aef\u8fd0\u884c\u65f6\u7f29\u77ed\u3001\u4f20\u8f93\u6570\u636e\u91cf\u964d\u4f4e\u3001\u6210\u672c\u6548\u76ca\u63d0\u9ad8\uff0c\u8bc1\u660e\u7b56\u7565\u6709\u6548\u51cf\u5c11\u4e86\u591a\u4e91ETL\u7684\u5ef6\u8fdf\u548c\u5f00\u652f\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\u5b9e\u7528\u7b56\u7565\u53ef\u589e\u5f3a\u5206\u5e03\u5f0f\u4e91\u73af\u5883\u4e2dETL\u7684\u53ef\u6269\u5c55\u6027\u548c\u53ef\u9760\u6027\uff0c\u4e3a\u4f01\u4e1a\u5b9e\u65bd\u591a\u4e91\u6570\u636e\u5206\u6790\u63d0\u4f9b\u53ef\u501f\u9274\u65b9\u6848\u3002"}}
{"id": "2601.18069", "categories": ["cs.NI", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.18069", "abs": "https://arxiv.org/abs/2601.18069", "authors": ["Haoyuan Pan", "Sizhao Chen", "Zhaorui Wang", "Tse-Tin Chan"], "title": "Diffusion Model-based Reinforcement Learning for Version Age of Information Scheduling: Average and Tail-Risk-Sensitive Control", "comment": "16 pages, 11 figures", "summary": "Ensuring timely and semantically accurate information delivery is critical in real-time wireless systems. While Age of Information (AoI) quantifies temporal freshness, Version Age of Information (VAoI) captures semantic staleness by accounting for version evolution between transmitters and receivers. Existing VAoI scheduling approaches primarily focus on minimizing average VAoI, overlooking rare but severe staleness events that can compromise reliability under stochastic packet arrivals and unreliable channels. This paper investigates both average-oriented and tail-risk-sensitive VAoI scheduling in a multi-user status update system with long-term transmission cost constraints. We first formulate the average VAoI minimization problem as a constrained Markov decision process and introduce a deep diffusion-based Soft Actor-Critic (D2SAC) algorithm. By generating actions through a diffusion-based denoising process, D2SAC enhances policy expressiveness and establishes a strong baseline for mean performance. Building on this foundation, we put forth RS-D3SAC, a risk-sensitive deep distributional diffusion-based Soft Actor-Critic algorithm. RS-D3SAC integrates a diffusion-based actor with a quantile-based distributional critic, explicitly modeling the full VAoI return distribution. This enables principled tail-risk optimization via Conditional Value-at-Risk (CVaR) while satisfying long-term transmission cost constraints. Extensive simulations show that, while D2SAC reduces average VAoI, RS-D3SAC consistently achieves substantial reductions in CVaR without sacrificing mean performance. The dominant gain in tail-risk reduction stems from the distributional critic, with the diffusion-based actor providing complementary refinement to stabilize and enrich policy decisions, highlighting their effectiveness for robust and risk-aware VAoI scheduling in multi-user wireless systems.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2601.17633", "categories": ["cs.AR", "cs.DC"], "pdf": "https://arxiv.org/pdf/2601.17633", "abs": "https://arxiv.org/abs/2601.17633", "authors": ["Rakesh Nadig", "Vamanan Arulchelvan", "Mayank Kabra", "Harshita Gupta", "Rahul Bera", "Nika Mansouri Ghiasi", "Nanditha Rao", "Qingcai Jiang", "Andreas Kosmas Kakolyris", "Yu Liang", "Mohammad Sadrosadati", "Onur Mutlu"], "title": "Conduit: Programmer-Transparent Near-Data Processing Using Multiple Compute-Capable Resources in Solid State Drives", "comment": "To appear in IEEE International Symposium on High-Performance Computer Architecture (HPCA) 2026", "summary": "Solid-state drives (SSDs) are well suited for near-data processing (NDP) because they: (1) store large application datasets, and (2) support three NDP paradigms: in-storage processing (ISP), processing using DRAM in the SSD (PuD-SSD), and in-flash processing (IFP). A large body of prior SSD-based NDP techniques operate in isolation, mapping computations to only one or two NDP paradigms (i.e., ISP, PuD-SSD, or IFP) within the SSD. These techniques (1) are tailored to specific workloads or kernels, (2) do not exploit the full computational potential of an SSD, and (3) lack programmer-transparency. While several prior works propose techniques to partition computation between the host and near-memory accelerators, adapting these techniques to SSDs has limited benefits because they (1) ignore the heterogeneity of the SSD resources, and (2) make offloading decisions based on limited factors such as bandwidth utilization, or data movement cost. We propose Conduit, a general-purpose, programmer-transparent NDP framework for SSDs that leverages multiple SSD computation resources. At compile time, Conduit executes a custom compiler (e.g., LLVM) pass that (i) vectorizes suitable application code segments into SIMD operations that align with the SSD's page layout, and (ii) embeds metadata (e.g., operation type, operand sizes) into the vectorized instructions to guide runtime offloading decisions. At runtime, within the SSD, Conduit performs instruction-granularity offloading by evaluating six key features, and uses a cost function to select the most suitable SSD resource. We evaluate Conduit and two prior NDP offloading techniques using an in-house event-driven SSD simulator on six data-intensive workloads. Conduit outperforms the best-performing prior offloading policy by 1.8x and reduces energy consumption by 46%.", "AI": {"tldr": "Conduit\u662f\u4e00\u79cd\u901a\u7528\u7684SSD\u8fd1\u6570\u636e\u5904\u7406\uff08NDP\uff09\u6846\u67b6\uff0c\u901a\u8fc7\u6574\u5408\u591a\u4e2aSSD\u8ba1\u7b97\u8d44\u6e90\u63d0\u5347\u6027\u80fd\u548c\u80fd\u6548\u3002", "motivation": "\u73b0\u6709NDP\u6280\u672f\u4ec5\u5229\u7528SSD\u90e8\u5206\u8303\u5f0f\uff08\u5982ISP\u3001PuD-SSD\u6216IFP\uff09\uff0c\u5b58\u5728\u5c40\u9650\u6027\uff1b\u5b83\u4eec\u9488\u5bf9\u7279\u5b9a\u8d1f\u8f7d\u3001\u65e0\u6cd5\u5145\u5206\u5229\u7528SSD\u7b97\u529b\u4e14\u7f3a\u4e4f\u7a0b\u5e8f\u5458\u900f\u660e\u6027\uff0c\u800c\u4ee5\u5f80\u5206\u533a\u8ba1\u7b97\u6280\u672f\u5ffd\u7565\u4e86SSD\u8d44\u6e90\u5f02\u6784\u6027\u548c\u51b3\u7b56\u56e0\u7d20\u4e0d\u8db3\u3002", "method": "\u5728\u7f16\u8bd1\u65f6\u901a\u8fc7LLVM\u81ea\u5b9a\u4e49\u8fdb\u7a0b\u5c06\u4ee3\u7801\u5411\u91cf\u5316\u4e0eSSD\u9875\u5bf9\u9f50\uff0c\u5e76\u5d4c\u5165\u5143\u6570\u636e\uff1b\u8fd0\u884c\u65f6\u57fa\u4e8e\u516d\u79cd\u7279\u5f81\u548c\u6210\u672c\u51fd\u6570\u8fdb\u884c\u6307\u4ee4\u7ea7\u5378\u8f7d\uff0c\u81ea\u52a8\u9009\u62e9\u6700\u4f18SSD\u8d44\u6e90\u6267\u884c\u3002", "result": "\u5728\u81ea\u7814\u4e8b\u4ef6\u9a71\u52a8SSD\u4eff\u771f\u5668\u548c\u516d\u4e2a\u6570\u636e\u5bc6\u96c6\u8d1f\u8f7d\u6d4b\u8bd5\u4e2d\uff0cConduit\u6bd4\u6700\u4f18\u5148\u524d\u65b9\u6848\u63d0\u901f1.8\u500d\u5e76\u964d\u4f4e\u80fd\u801746%\u3002", "conclusion": "Conduit\u5b9e\u73b0\u4e86SSD\u5f02\u6784\u8d44\u6e90\u7684\u9ad8\u6548\u7edf\u4e00\u5229\u7528\uff0c\u4e3a\u901a\u7528NDP\u63d0\u4f9b\u4e86\u900f\u660e\u89e3\u51b3\u65b9\u6848\uff0c\u663e\u8457\u63d0\u5347SSD\u8ba1\u7b97\u6f5c\u529b\u3002"}}
{"id": "2601.17578", "categories": ["cs.DC", "stat.CO"], "pdf": "https://arxiv.org/pdf/2601.17578", "abs": "https://arxiv.org/abs/2601.17578", "authors": ["Henrik Bengtsson"], "title": "A Unified Approach to Concurrent, Parallel Map-Reduce in R using Futures", "comment": "16 pages including 2.5 pages references, 1 figure", "summary": "The R ecosystem offers a rich variety of map-reduce application programming interfaces (APIs) for iterative computations, yet parallelizing code across these diverse frameworks requires learning multiple, often incompatible, parallel APIs. The futurize package addresses this challenge by providing a single function, futurize(), which transpiles sequential map-reduce expressions into their parallel equivalents in the future ecosystem, which performs all the heavy lifting. By leveraging R's native pipe operator, users can parallelize existing code with minimal refactoring -- often by simply appending `|> futurize()' to an expression. The package supports classical map-reduce functions from base R, purrr, crossmap, foreach, plyr, BiocParallel, e.g., lapply(xs, fcn) |> futurize() and map(xs, fcn) |> futurize(), as well as a growing set of domain-specific packages, e.g., boot, caret, glmnet, lme4, mgcv, and tm. By abstracting away the underlying parallel machinery, and unifying handling of future options, the package enables developers to declare what to parallelize via futurize(), and end-users to choose how via plan(). This article describes the philosophy, design, and implementation of futurize, demonstrates its usage across various map-reduce paradigms, and discusses its role in simplifying parallel computing in R.", "AI": {"tldr": "futurize\u5305\u63d0\u4f9b\u4e00\u4e2a\u51fd\u6570\uff0c\u5c06\u987a\u5e8fmap-reduce\u64cd\u4f5c\u4e3a\u5e76\u884c\u7b49\u6548\u8f6c\u8bd1\uff0c\u96c6\u6210\u591a\u79cd\u6846\u67b6\uff0c\u7b80\u5316R\u4e2d\u7684\u5e76\u884c\u8ba1\u7b97\u3002", "motivation": "R\u751f\u6001\u7cfb\u7edf\u5b58\u5728\u591a\u6837\u4e14\u4e0d\u517c\u5bb9\u7684map-reduce API\uff0c\u5e76\u884c\u5316\u4ee3\u7801\u9700\u5b66\u4e60\u591a\u4e2a\u4e0d\u4e00\u81f4\u63a5\u53e3\uff0c\u589e\u52a0\u5f00\u53d1\u96be\u5ea6\u3002", "method": "\u4f7f\u7528futurize()\u51fd\u6570\uff0c\u7ed3\u5408R\u7ba1\u9053\u64cd\u4f5c\u7b26\uff0c\u5c06\u987a\u5e8f\u8868\u8fbe\u5f0f\u8f6c\u8bd1\u4e3afuture\u751f\u6001\u7cfb\u7edf\u5185\u7684\u5e76\u884c\u7b49\u6548\uff0c\u652f\u6301\u6700\u5c0f\u4ee3\u7801\u91cd\u6784\u3002\u517c\u5bb9base R\u3001purrr\u7b49\u591a\u4e2a\u5305\u3002", "result": "\u7528\u6237\u8ffd\u52a0`|> futurize()`\u5373\u53ef\u5feb\u901f\u5e76\u884c\u5316\u4ee3\u7801\uff0c\u5f00\u53d1\u8005\u58f0\u660e\u5e76\u884c\u5185\u5bb9\uff0c\u7528\u6237\u901a\u8fc7plan()\u9009\u62e9\u5e76\u884c\u65b9\u5f0f\uff0c\u5927\u5e45\u51cf\u5c11\u5b66\u4e60\u6210\u672c\u3002", "conclusion": "\u901a\u8fc7\u62bd\u8c61\u5e76\u884c\u673a\u5236\u548c\u7edf\u4e00\u9009\u9879\u5904\u7406\uff0cfuturize\u5305\u663e\u8457\u7b80\u5316R\u4e2d\u7684\u5e76\u884c\u8ba1\u7b97\uff0c\u63d0\u5347\u5f00\u53d1\u6548\u7387\u548c\u7528\u6237\u4f53\u9a8c\u3002"}}
{"id": "2601.17940", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2601.17940", "abs": "https://arxiv.org/abs/2601.17940", "authors": ["Luca Colagrande", "Luca Benini"], "title": "Late Breaking Results: Boosting Efficient Dual-Issue Execution on Lightweight RISC-V Cores", "comment": "Accepted at DATE 2026", "summary": "Large-scale ML accelerators rely on large numbers of PEs, imposing strict bounds on the area and energy budget of each PE. Prior work demonstrates that limited dual-issue capabilities can be efficiently integrated into a lightweight in-order open-source RISC-V core (Snitch), with a geomean IPC boost of 1.6x and a geomean energy efficiency gain of 1.3x, obtained by concurrently executing integer and FP instructions. Unfortunately, this required a complex and error-prone low level programming model (COPIFT). We introduce COPIFTv2 which augments Snitch with lightweight queues enabling direct, fine-grained communication and synchronization between integer and FP threads. By eliminating the tiling and software pipelining steps of COPIFT, we can remove much of its complexity and software overheads. As a result, COPIFTv2 achieves up to a 1.49x speedup and a 1.47x energy-efficiency gain over COPIFT, and a peak IPC of 1.81. Overall, COPIFTv2 significantly enhances the efficiency and programmability of dual-issue execution on lightweight cores. Our implementation is fully open source and performance experiments are reproducible using free software.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecdCOPIFTv2\u7f16\u7a0b\u6a21\u578b\uff0c\u6539\u8fdbSnitch\u6838\u5fc3\u7684\u53cc\u53d1\u884c\u6267\u884c\uff0c\u901a\u8fc7\u8f7b\u91cf\u961f\u5217\u63d0\u5347\u6574\u6570\u4e0e\u6d6e\u70b9\u6307\u4ee4\u7684\u901a\u4fe1\u6548\u7387\u548c\u53ef\u7f16\u7a0b\u6027\u3002", "motivation": "\u56e0\u5148\u524dCOPIFT\u6a21\u578b\u590d\u6742\u4e14\u6613\u51fa\u9519\uff0c\u9700\u4f18\u5316\u8f7b\u91cf\u7ea7RISC-V\u6838\u5fc3\u7684\u6267\u884c\u6548\u7387\uff0c\u514b\u670d\u7f16\u7a0b\u590d\u6742\u6027\u548c\u5f00\u9500\u95ee\u9898\u3002", "method": "\u5728Snitch\u6838\u5fc3\u4e2d\u96c6\u6210\u8f7b\u91cf\u961f\u5217\uff0c\u5b9e\u73b0\u6574\u6570\u548c\u6d6e\u70b9\u7ebf\u7a0b\u95f4\u7684\u76f4\u63a5\u3001\u7cbe\u7ec6\u5316\u901a\u4fe1\u548c\u540c\u6b65\uff0c\u6d88\u9664COPIFT\u7684\u5206\u5757\u548c\u8f6f\u4ef6\u6d41\u6c34\u7ebf\u6b65\u9aa4\u3002", "result": "\u76f8\u8f83COPIFT\uff0c\u901f\u5ea6\u63d0\u5347\u6700\u9ad8\u8fbe1.49\u500d\uff0c\u80fd\u6548\u63d0\u53471.47\u500d\uff0c\u5cf0\u503cIPC\u8fbe1.81\uff1b\u6548\u7387\u4e0e\u7f16\u7a0b\u6027\u663e\u8457\u589e\u5f3a\u3002", "conclusion": "COPIFTv2\u5927\u5e45\u6539\u5584\u8f7b\u91cf\u6838\u5fc3\u53cc\u53d1\u884c\u7684\u6548\u80fd\u548c\u7528\u6237\u53cb\u597d\u6027\uff0c\u5168\u90e8\u5f00\u6e90\u5e76\u786e\u4fdd\u5b9e\u9a8c\u7ed3\u679c\u590d\u73b0\uff0c\u63a8\u52a8\u4e86ML\u52a0\u901f\u5668\u53d1\u5c55\u3002"}}
{"id": "2601.18007", "categories": ["cs.AR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.18007", "abs": "https://arxiv.org/abs/2601.18007", "authors": ["Duckgyu Shin", "Naoya Onizawa", "Warren J. Gross", "Takahiro Hanyu"], "title": "Memory-Efficient FPGA Implementation of Stochastic Simulated Annealing", "comment": "11 pages", "summary": "Simulated annealing (SA) is a well-known algorithm for solving combinatorial optimization problems. However, the computation time of SA increases rapidly, as the size of the problem grows. Recently, a stochastic simulated annealing (SSA) algorithm that converges faster than conventional SA has been reported. In this paper, we present a hardware-aware SSA (HA- SSA) algorithm for memory-efficient FPGA implementations. HA-SSA can reduce the memory usage of storing intermediate results while maintaining the computing speed of SSA. For evaluation purposes, the proposed algorithm is compared with the conventional SSA and SA approaches on maximum cut combinatorial optimization problems. HA-SSA achieves a convergence speed that is up to 114-times faster than that of the conventional SA algorithm depending on the maximum cut problem selected from the G-set which is a dataset of the maximum cut problems. HA-SSA is implemented on a field-programmable gate array (FPGA) (Xilinx Kintex-7), and it achieves up to 6-times the memory efficiency of conventional SSA while maintaining high solution quality for optimization problems.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u786c\u4ef6\u611f\u77e5\u968f\u673a\u6a21\u62df\u9000\u706b\u7b97\u6cd5\uff08HA-SSA\uff09\uff0c\u7528\u4e8eFPGA\u5b9e\u73b0\uff0c\u5728\u5185\u5b58\u6548\u7387\u548c\u8ba1\u7b97\u901f\u5ea6\u4e0a\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u3002", "motivation": "\u4f20\u7edf\u6a21\u62df\u9000\u706b\u7b97\u6cd5\uff08SA\uff09\u5904\u7406\u5927\u89c4\u6a21\u7ec4\u5408\u4f18\u5316\u95ee\u9898\u65f6\u8ba1\u7b97\u65f6\u95f4\u5267\u589e\uff0c\u800c\u73b0\u6709\u968f\u673a\u6a21\u62df\u9000\u706b\u7b97\u6cd5\uff08SSA\uff09\u901f\u5ea6\u8f83\u5feb\u4f46\u5185\u5b58\u5360\u7528\u9ad8\uff0c\u9650\u5236\u4e86FPGA\u5e94\u7528\u3002", "method": "\u5f00\u53d1\u786c\u4ef6\u611f\u77e5SSA\u7b97\u6cd5\uff08HA-SSA\uff09\uff0c\u4f18\u5316\u4e2d\u95f4\u7ed3\u679c\u5b58\u50a8\u4ee5\u51cf\u5c11\u5185\u5b58\u7528\u91cf\u5e76\u4fdd\u6301SSA\u7684\u8ba1\u7b97\u901f\u5ea6\u3002", "result": "\u5728\u6700\u5927\u5272\u95ee\u9898\uff08G-set\u6570\u636e\u96c6\uff09\u4e0a\u9a8c\u8bc1\uff1aHA-SSA\u6536\u655b\u901f\u5ea6\u6bd4\u4f20\u7edfSA\u5feb114\u500d\uff0c\u5185\u5b58\u6548\u7387\u8f83SSA\u63d0\u9ad86\u500d\uff1bFPGA\uff08Xilinx Kintex-7\uff09\u5b9e\u73b0\u4e0b\u4fdd\u6301\u9ad8\u89e3\u5347\u964d\u8d28\u91cf\u3002", "conclusion": "HA-SSA\u663e\u8457\u63d0\u5347\u5185\u5b58\u6548\u7387\u548c\u901f\u5ea6\uff0c\u9002\u7528\u4e8eFPGA\u7ec4\u5408\u4f18\u5316\u5b9e\u73b0\uff0c\u89e3\u8d28\u91cf\u7a33\u5b9a\u3002"}}
{"id": "2601.17606", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2601.17606", "abs": "https://arxiv.org/abs/2601.17606", "authors": ["Shannon Kinkead", "Jackson Wesley", "Whit Schonbein", "David DeBonis", "Matthew G. F. Dosanjh", "Amanda Bienz"], "title": "Scaling All-to-all Operations Across Emerging Many-Core Supercomputers", "comment": null, "summary": "Performant all-to-all collective operations in MPI are critical to fast Fourier transforms, transposition, and machine learning applications. There are many existing implementations for all-to-all exchanges on emerging systems, with the achieved performance dependent on many factors, including message size, process count, architecture, and parallel system partition. This paper presents novel all-to-all algorithms for emerging many-core systems. Further, the paper presents a performance analysis against existing algorithms and system MPI, with novel algorithms achieving up to 3x speedup over system MPI at 32 nodes of state-of-the-art Sapphire Rapids systems.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2601.17450", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2601.17450", "abs": "https://arxiv.org/abs/2601.17450", "authors": ["Qingchao Shen"], "title": "Data-driven Test Generation for Fuzzing AI Compiler", "comment": "This paper has been accepted by ICSE 2026 Doctoral Symposium track", "summary": "Artificial Intelligence (AI) compilers are critical for efficiently deploying AI models across diverse hardware platforms. However, they remain prone to bugs that can compromise both compiler reliability and model correctness. Thus, ensuring the quality of AI compilers is crucial. In this work, we present a unified data-driven testing framework that systematically addresses stage-specific challenges in AI compilers. Specifically, OPERA migrates tests for AI libraries to test various operator conversion logic in the model loading stage. OATest synthesizes diverse optimization-aware computational graphs for testing high-level optimizations. HARMONY generates and mutates diverse low-level IR seeds to generate hardware-optimization-aware tests for testing low-level optimizations. Together, these techniques provide a comprehensive, stage-aware framework that enhances testing coverage and effectiveness, detecting 266 previously unknown bugs in four widely used AI compilers.", "AI": {"tldrulata": "\u63d0\u51fa\u7edf\u4e00\u6570\u636e\u9a71\u52a8\u7684AI\u7f16\u8bd1\u5668\u6d4b\u8bd5\u6846\u67b6\uff0c\u8986\u76d6\u591a\u9636\u6bb5\u4f18\u5316\uff0c\u68c0\u6d4b266\u4e2a\u672a\u77e5bug\u3002", "motivation": "AI\u7f16\u8bd1\u5668\u6f0f\u6d1e\u5a01\u80c1\u53ef\u9760\u6027\u548c\u6a21\u578b\u6b63\u786e\u6027\uff0c\u4e9f\u9700\u786e\u4fdd\u8d28\u91cf\u3002", "method": "OPERA\u8fc1\u79fb\u5e93\u6d4b\u8bd5\u9a8c\u8bc1\u7b97\u5b50\u8f6c\u6362\uff0cOATest\u751f\u6210\u4f18\u5316\u611f\u77e5\u8ba1\u7b97\u56fe\u6d4b\u8bd5\u9ad8\u7ea7\u4f18\u5316\uff0cHARMONY\u53d8\u5f02IR\u79cd\u5b50\u6d4b\u8bd5\u4f4e\u7ea7\u4f18\u5316\u3002", "result": "\u5728\u56db\u6b3e\u4e3b\u6d41AI\u7f16\u8bd1\u5668\u4e2d\u68c0\u6d4b\u51fa266\u4e2a\u672a\u77e5\u9519\u8bef\u3002", "conclusion": "\u6846\u67b6\u5b9e\u73b0\u9636\u6bb5\u6027\u5168\u9762\u8986\u76d6\uff0c\u663e\u8457\u63d0\u5347\u6d4b\u8bd5\u6709\u6548\u6027\u548c\u8986\u76d6\u7387\u3002", "tldr": "Summary generation failed"}}
{"id": "2601.18256", "categories": ["cs.NI"], "pdf": "https://arxiv.org/pdf/2601.18256", "abs": "https://arxiv.org/abs/2601.18256", "authors": ["Akihito Taya", "Yuuki Nishiyama", "Kaoru Sezaki"], "title": "A Mechanical Wi-Fi Antenna Device for Automatic Orientation Tuning with Bayesian Optimization", "comment": "(c) 2026 IEEE. Personal use of this material is permitted. Permission from IEEE must be obtained for all other uses, in any current or future media, including reprinting/republishing this material for advertising or promotional purposes, creating new collective works, for resale or redistribution to servers or lists, or reuse of any copyrighted component of this work in other works", "summary": "Wi-Fi access points have been widely deployed in homes, offices, and public spaces. Some APs allow users to adjust the antenna orientation to improve communication performance by optimizing antenna polarization. However, it is difficult for non-expert users to determine the optimal orientation, and users often leave the antenna orientation in ineffective positions. To address this issue, we developed a mechanical Wi-Fi antenna device capable of automatically tuning its orientation. Experimental results show that antenna orientation could cause a throughput variation of approximately 70 Mbps under line-of-sight conditions. Furthermore, Bayesian optimization identified better configurations than random search, demonstrating its effectiveness for orientation tuning.", "AI": {"tldr": "\u7814\u7a76\u4eba\u5458\u5f00\u53d1\u4e86\u4e00\u79cd\u673a\u68b0Wi-Fi\u5929\u7ebf\u8bbe\u5907\uff0c\u901a\u8fc7\u81ea\u52a8\u8c03\u6574\u5929\u7ebf\u65b9\u5411\u4f18\u5316\u901a\u4fe1\u6027\u80fd", "motivation": "\u975e\u4e13\u4e1a\u7528\u6237\u96be\u4ee5\u786e\u5b9a\u6700\u4f73\u5929\u7ebf\u65b9\u5411\uff0c\u5e38\u5bfc\u81f4\u5929\u7ebf\u5904\u4e8e\u65e0\u6548\u4f4d\u7f6e\u800c\u964d\u4f4eWi-Fi\u6027\u80fd", "method": "\u7814\u5236\u80fd\u81ea\u52a8\u8c03\u8282\u65b9\u5411\u7684\u673a\u68b0Wi-Fi\u5929\u7ebf\u8bbe\u5907\uff0c\u5e76\u91c7\u7528\u8d1d\u53f6\u65af\u4f18\u5316\u6280\u672f\u8fdb\u884c\u65b9\u5411\u8c03\u8c10", "result": "\u5b9e\u9a8c\u663e\u793a\uff0c\ud835\udcd4\u7ebf\u6761\u4ef6\u4e0b\u5929\u7ebf\u65b9\u5411\u53d8\u5316\u53ef\u5bfc\u81f4\u541e\u5410\u91cf\u53d8\u52a8\u7ea670 Mbps\uff0c\u8d1d\u53f6\u65af\u4f18\u5316\u6bd4\u968f\u673a\u641c\u7d22\u66f4\u6709\u6548", "conclusion": "\u81ea\u52a8\u5929\u7ebf\u65b9\u5411\u8c03\u8c10\u53ef\u884c\uff0c\u80fd\u663e\u8457\u63d0\u5347Wi-Fi\u901a\u4fe1\u6548\u679c"}}
{"id": "2601.18070", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2601.18070", "abs": "https://arxiv.org/abs/2601.18070", "authors": ["Jinwu Chen", "Yuhui Shi", "He Wang", "Zhe Jiang", "Jun Yang", "Xin Si", "Zhenhua Zhu"], "title": "CIM-Tuner: Balancing the Compute and Storage Capacity of SRAM-CIM Accelerator via Hardware-mapping Co-exploration", "comment": null, "summary": "As an emerging type of AI computing accelerator, SRAM Computing-In-Memory (CIM) accelerators feature high energy efficiency and throughput. However, various CIM designs and under-explored mapping strategies impede the full exploration of compute and storage balancing in SRAM-CIM accelerator, potentially leading to significant performance degradation. To address this issue, we propose CIM-Tuner, an automatic tool for hardware balancing and optimal mapping strategy under area constraint via hardware-mapping co-exploration. It ensures universality across various CIM designs through a matrix abstraction of CIM macros and a generalized accelerator template. For efficient mapping with different hardware configurations, it employs fine-grained two-level strategies comprising accelerator-level scheduling and macro-level tiling. Compared to prior CIM mapping, CIM-Tuner's extended strategy space achieves 1.58$\\times$ higher energy efficiency and 2.11$\\times$ higher throughput. Applied to SOTA CIM accelerators with identical area budget, CIM-Tuner also delivers comparable improvements. The simulation accuracy is silicon-verified and CIM-Tuner tool is open-sourced at https://github.com/champloo2878/CIM-Tuner.git.", "AI": {"tldr": "\u4e3a\u89e3\u51b3SRAM-CIM\u52a0\u901f\u5668\u4e2d\u786c\u4ef6\u5e73\u8861\u548c\u6620\u5c04\u7b56\u7565\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u63d0\u51fa\u81ea\u52a8\u5316\u5de5\u5177CIM-Tuner\uff0c\u901a\u8fc7\u786c\u4ef6\u6620\u5c04\u534f\u540c\u63a2\u7d22\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u591a\u6837\u5316CIM\u8bbe\u8ba1\u548c\u672a\u5145\u5206\u63a2\u7d22\u7684\u6620\u5c04\u7b56\u7565\u963b\u788d\u4e86\u8ba1\u7b97\u4e0e\u5b58\u50a8\u5e73\u8861\u7684\u4f18\u5316\uff0c\u5bfc\u81f4\u6027\u80fd\u663e\u8457\u4e0b\u964d\u3002", "method": "\u4f7f\u7528CIM\u5b8f\u7684\u77e9\u9635\u62bd\u8c61\u4e0e\u901a\u7528\u52a0\u901f\u5668\u6a21\u677f\u786e\u4fdd\u901a\u7528\u6027\uff1b\u91c7\u7528\u7ec6\u7c92\u5ea6\u4e24\u7ea7\u7b56\u7565\uff08\u52a0\u901f\u5668\u7ea7\u8c03\u5ea6\u548c\u5b8f\u7ea7\u5206\u5757\uff09\u5b9e\u73b0\u9ad8\u6548\u6620\u5c04\u3002", "result": "\u76f8\u540c\u9762\u79ef\u7ea6\u675f\u4e0b\uff0c\u80fd\u6548\u63d0\u53471.58\u500d\uff0c\u541e\u5410\u91cf\u63d0\u53472.11\u500d\uff1b\u5e94\u7528\u4e8e\u5148\u8fdbCIM\u52a0\u901f\u5668\u65f6\u6027\u80fd\u8868\u73b0\u53ef\u6bd4\u3002\u4eff\u771f\u7cbe\u5ea6\u7ecf\u7845\u9a8c\u8bc1\uff0c\u5de5\u5177\u5df2\u5f00\u6e90\u3002", "conclusion": "CIM-Tuner\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u901a\u7528\u7684\u4f18\u5316\u65b9\u6848\uff0c\u663e\u8457\u63d0\u5347CIM\u52a0\u901f\u5668\u6027\u80fd\uff0c\u63a8\u52a8\u786c\u4ef6\u8bbe\u8ba1\u53d1\u5c55\u3002"}}
{"id": "2601.17707", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2601.17707", "abs": "https://arxiv.org/abs/2601.17707", "authors": ["Mekala Kiran", "Apurba Das", "Suman Banerjee", "Tathagata Ray"], "title": "Multi-core & GPU-based Balanced Butterfly Counting in Signed Bipartite Graphs", "comment": null, "summary": "Balanced butterfly counting, corresponding to counting balanced (2, 2)-bicliques, is a fundamental primitive in the analysis of signed bipartite graphs and provides a basis for studying higher-order structural properties such as clustering coefficients and community structure. Although prior work has proposed an efficient CPU-based serial method for counting balanced (2, k)-bicliques. The computational cost of balanced butterfly counting remains a major bottleneck on large-scale graphs. In this work, we present the highly parallel implementations for balanced butterfly counting for both multicore CPUs and GPUs. The proposed multi-core algorithm (M-BBC) employs fine-grained vertex-level parallelism to accelerate wedge-based counting while eliminating the generation of unbalanced substructures. To improve scalability, we develop a GPU-based method (G-BBC) that uses a tile-based parallel approach to effectively leverage shared memory while handling large vertex sets. We then present an improved variation, G-BBC++, which integrates dynamic scheduling to mitigate workload imbalance and maximize throughput. We conduct an experimental assessment of the proposed methods across 15 real-world datasets. Experimental results exhibit that M-BBC achieves speedups of up to 71.13x (average 38.13x) over the sequential baseline BB2K. The GPU-based algorithms deliver even greater improvements, achieving up to 13,320x speedup (average 2,600x) over BB2K and outperforming M-BBC by up to 186x (average 50x). These results indicate the substantial scalability and efficiency of our parallel algorithms and establish a robust foundation for high-performance signed motif analysis on massive bipartite graphs.", "AI": {"tldr": "\u672c\u6587\u5f00\u53d1\u4e86\u5e76\u884c\u7b97\u6cd5M-BBC\u7528\u4e8e\u591a\u6838CPU\u548cG-BBC\u7528\u4e8eGPU\uff0c\u6539\u8fdb\u7248G-BBC++\u52a0\u5165\u52a8\u6001\u8c03\u5ea6\uff0c\u4ee5\u52a0\u901f\u5e73\u8861\u8774\u8776\u8ba1\u6570\uff0c\u5e76\u5728\u5927\u89c4\u6a21\u6709\u7b26\u53f7\u4e8c\u5206\u56fe\u4e0a\u5b9e\u73b0\u663e\u8457\u63d0\u901f\u3002", "motivation": "\u5e73\u8861\u8774\u8776\u8ba1\u6570\u662f\u5206\u6790\u6709\u7b26\u53f7\u4e8c\u5206\u56fe\u7684\u57fa\u7840\u4efb\u52a1\uff0c\u4f46\u73b0\u6709\u4e32\u884c\u65b9\u6cd5\u8ba1\u7b97\u6210\u672c\u9ad8\uff0c\u6210\u4e3a\u5927\u89c4\u6a21\u56fe\u5206\u6790\u7684\u74f6\u9888 \u062d\u0648\u0632\u0647\u3002", "method": "\u63d0\u51faM-BBC\u7b97\u6cd5\uff0c\u57fa\u4e8e\u7ec6\u7c92\u5ea6\u9876\u70b9\u7ea7\u5e76\u884c\u6d88\u9664\u4e0d\u5e73\u8861\u5b50\u7ed3\u6784\uff1bGPU\u65b9\u6cd5G-BBC\u91c7\u7528\u57fa\u4e8etile\u7684\u5e76\u884c\u7b56\u7565\u4f18\u5316\u5171\u4eab\u5185\u5b58\uff0c\u589e\u5f3a\u7248G-BBC++\u96c6\u6210\u52a8\u6001\u8c03\u5ea6\u5747\u8861\u8d1f\u8f7d\u3002", "result": "\u572815\u4e2a\u771f\u5b9e\u6570\u636e\u96c6\u5b9e\u9a8c\u4e2d\uff0cM-BBC\u6bd4\u5e8f\u5217\u57fa\u51c6BB2K\u6700\u9ad8\u52a0\u901f71.13\u500d\uff08\u5e73\u574738.13\u500d\uff09\uff1bGPU\u7b97\u6cd5\u6bd4BB2K\u6700\u9ad8\u52a0\u901f13,320\u500d\uff08\u5e73\u57472,600\u500d\uff09\uff0c\u6bd4M-BBC\u6700\u9ad8\u52a0\u901f186\u500d\uff08\u5e73\u574750\u500d\uff09\u3002", "conclusion": "\u5e76\u884c\u7b97\u6cd5\u6781\u5927\u63d0\u5347\u4e86\u53ef\u6269\u5c55\u6027\u548c\u6548\u7387\uff0c\u4e3a\u5927\u89c4\u6a21\u4e8c\u5206\u56fe\u7684\u9ad8\u6027\u80fd\u7b7e\u5b57\u6a21\u5f0f\u5206\u6790\u5960\u5b9a\u4e86\u575a\u5b9e\u57fa\u7840\u3002"}}
{"id": "2601.17482", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2601.17482", "abs": "https://arxiv.org/abs/2601.17482", "authors": ["Yang Liu", "Kaiming Zhang", "Zhuangbin Chen", "Jinyang Liu", "Zibin Zheng"], "title": "LogPrism: Unifying Structure and Variable Encoding for Effective Log Compression", "comment": null, "summary": "The prevailing \"parse-then-compress\" paradigm in log compression fundamentally limits effectiveness by treating log parsing and compression as isolated objectives. While parsers prioritize semantic accuracy (i.e., event identification), they often obscure deep correlations between static templates and dynamic variables that are critical for storage efficiency. In this paper, we investigate this misalignment through a comprehensive empirical study and propose LogPrism, a framework that bridges the gap via unified redundancy encoding. Rather than relying on a rigid pre-parsing step, LogPrism dynamically integrates structural extraction with variable encoding by constructing a Unified Redundancy Tree (URT). This hierarchical approach effectively mines \"structure+variable\" co-occurrence patterns, capturing deep contextual redundancies while accelerating processing through pre-emptive pattern encoding. Extensive experiments on 16 benchmark datasets confirm that LogPrism establishes a new state-of-the-art. It achieves the highest compression ratio on 13 datasets, surpassing leading baselines by margins of 4.7% to 80.9%, while delivering superior throughput at 29.87 MB/s (1.68$\\times$~43.04$\\times$ faster than competitors). Moreover, when configured in single-archive mode to maximize global pattern discovery, LogPrism outperforms the best baseline by 19.39% in compression ratio while maintaining a 2.62$\\times$ speed advantage.", "AI": {"tldr": "\u63d0\u51faLogPrism\u6846\u67b6\uff0c\u901a\u8fc7\u7edf\u4e00\u538b\u7f29\u65b9\u5f0f\u6865\u63a5\u89e3\u6790\u4e0e\u538b\u7f29\u9694\u9602\uff0c\u663e\u8457\u63d0\u5347\u65e5\u5fd7\u538b\u7f29\u6548\u7387\u548c\u901f\u5ea6\u3002", "motivation": "\u73b0\u6709'\u89e3\u6790\u540e\u538b\u7f29'\u8303\u5f0f\u5206\u79bb\u5904\u7406\u65e5\u5fd7\u89e3\u6790\u548c\u538b\u7f29\uff0c\u89e3\u6790\u5668\u5f3a\u8c03\u8bed\u4e49\u51c6\u786e\u6027\u5374\u5ffd\u89c6\u6a21\u677f\u4e0e\u53d8\u91cf\u95f4\u6df1\u5c42\u5173\u8054\uff0c\u5bfc\u81f4\u5b58\u50a8\u6548\u7387\u4f4e\u4e0b\u3002", "method": "\u6784\u5efa\u7edf\u4e00\u5197\u4f59\u6811(URT)\u52a8\u6001\u6574\u5408\u7ed3\u6784\u63d0\u53d6\u4e0e\u53d8\u91cf\u7f16\u7801\uff0c\u6316\u6398'\u7ed3\u6784+\u53d8\u91cf'\u5171\u73b0\u6a21\u5f0f\u5e76\u9884\u7f16\u7801\uff0c\u4ee5\u6355\u83b7\u4e0a\u4e0b\u6587\u5197\u4f59\u5e76\u52a0\u901f\u5904\u7406\u3002", "result": "\u572816\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\uff0cLogPrism\u572813\u4e2a\u6570\u636e\u96c6\u521b\u6700\u9ad8\u538b\u7f29\u6bd4(\u8d85\u8d8a\u57fa\u7ebf4.7%~80.9%)\uff0c\u541e\u5410\u738729.87 MB/s(\u6bd4\u5bf9\u624b\u5feb1.68\u00d7~43.04\u00d7)\uff1b\u5355\u5f52\u6863\u6a21\u5f0f\u4e0b\u538b\u7f29\u6bd4\u9886\u5148\u6700\u5f3a\u57fa\u7ebf19.39%\uff0c\u63d0\u901f2.62\u00d7\u3002", "conclusion": "LogPrism\u786e\u7acb\u538b\u7f29\u9886\u57df\u65b0\u524d\u6cbf\uff0c\u5728\u538b\u7f29\u6bd4\u3001\u901f\u5ea6\u548c\u5168\u5c40\u6a21\u5f0f\u6316\u6398\u65b9\u9762\u4f18\u4e8e\u6240\u6709\u57fa\u7ebf\u3002"}}
{"id": "2601.18315", "categories": ["cs.NI"], "pdf": "https://arxiv.org/pdf/2601.18315", "abs": "https://arxiv.org/abs/2601.18315", "authors": ["Zhaozhi Liu", "Jiaxin Chen", "Yuanai Xie", "Yuna Jiang", "Minrui Xu", "Xiao Zhang", "Pan Lai", "Zan Zhou"], "title": "CovertComBench: The First Domain-Specific Testbed for LLMs in Wireless Covert Communication", "comment": "6pages", "summary": "The integration of Large Language Models (LLMs) into wireless networks presents significant potential for automating system design. However, unlike conventional throughput maximization, Covert Communication (CC) requires optimizing transmission utility under strict detection-theoretic constraints, such as Kullback-Leibler divergence limits. Existing benchmarks primarily focus on general reasoning or standard communication tasks and do not adequately evaluate the ability of LLMs to satisfy these rigorous security constraints. To address this limitation, we introduce CovertComBench, a unified benchmark designed to assess LLM capabilities across the CC pipeline, encompassing conceptual understanding (MCQs), optimization derivation (ODQs), and code generation (CGQs). Furthermore, we analyze the reliability of automated scoring within a detection-theoretic ``LLM-as-Judge'' framework. Extensive evaluations across state-of-the-art models reveal a significant performance discrepancy. While LLMs achieve high accuracy in conceptual identification (81%) and code implementation (83%), their performance in the higher-order mathematical derivations necessary for security guarantees ranges between 18% and 55%. This limitation indicates that current LLMs serve better as implementation assistants rather than autonomous solvers for security-constrained optimization. These findings suggest that future research should focus on external tool augmentation to build trustworthy wireless AI systems.", "AI": {"tldr": "\u672c\u7814\u7a76\u5f15\u5165CovertComBench\u57fa\u51c6\u6d4b\u8bd5\uff0c\u8bc4\u4f30\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u65e0\u7ebf\u9690\u853d\u901a\u4fe1\u4e2d\u7684\u80fd\u529b\uff0c\u6307\u51fa\u6a21\u578b\u5728\u6570\u5b66\u63a8\u5bfc\u65b9\u9762\u5b58\u5728\u5c40\u9650\u3002", "motivation": "\u73b0\u6709\u57fa\u51c6\u6d4b\u8bd5\u65e0\u6cd5\u5145\u5206\u8bc4\u4f30LLM\u5728\u4e25\u683c\u5b89\u5168\u7ea6\u675f\uff08\u5982Kullback-Leibler\u6563\u5ea6\uff09\u4e0b\u7684\u8868\u73b0\uff0c\u5bfc\u81f4\u5176\u5728\u9690\u853d\u901a\u4fe1\u9886\u57df\u5e94\u7528\u7684\u4e0d\u8db3\u672a\u88ab\u63ed\u793a\u3002", "method": "\u5f00\u53d1\u7efc\u5408\u6027\u57fa\u51c6CovertComBench\uff0c\u5305\u62ec\u6982\u5ff5\u7406\u89e3\u9898(MCQs)\u3001\u4f18\u5316\u63a8\u5bfc\u9898(ODQs)\u548c\u4ee3\u7801\u751f\u6210\u9898(CGQs)\uff0c\u5e76\u91c7\u7528\u68c0\u6d4b\u7406\u8bba\u6846\u67b6\u8fdb\u884c\u81ea\u52a8\u5316\u8bc4\u5206\u5206\u6790\u3002", "result": "LLM\u5728\u6982\u5ff5\u8bc6\u522b\u51c6\u786e\u7387\u8fbe81%\uff0c\u4ee3\u7801\u5b9e\u73b083%\uff0c\u4f46\u6d89\u53ca\u5b89\u5168\u4f18\u5316\u7684\u6570\u5b66\u63a8\u5bfc\u8868\u73b0\u8f83\u5dee\uff0c\u51c6\u786e\u7387\u4ec5\u4e3a18%\u81f355%\u3002", "conclusion": "\u5f53\u524dLLM\u4ec5\u9002\u5408\u4f5c\u5b9e\u73b0\u8f85\u52a9\u5de5\u5177\uff0c\u672a\u6765\u5e94\u5f3a\u5316\u5916\u90e8\u5de5\u5177\u96c6\u6210\u4ee5\u63d0\u9ad8\u65e0\u7ebfAI\u7cfb\u7edf\u7684\u53ef\u9760\u6027\u3002"}}
{"id": "2601.18140", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2601.18140", "abs": "https://arxiv.org/abs/2601.18140", "authors": ["Yan Zhu", "Boru Chen", "Christopher W. Fletcher", "Nandeeka Nayak"], "title": "RTeAAL Sim: Using Tensor Algebra to Represent and Accelerate RTL Simulation (Extended Version)", "comment": null, "summary": "RTL simulation on CPUs remains a persistent bottleneck in hardware design. State-of-the-art simulators embed the circuit directly into the simulation binary, resulting in long compilation times and execution that is fundamentally CPU frontend-bound, with severe instruction-cache pressure.\n  This work proposes RTeAAL Sim, which reformulates RTL simulation as a sparse tensor algebra problem. By representing RTL circuits as tensors and simulation as a sparse tensor algebra kernel, RTeAAL Sim decouples simulation behavior from binary size and makes RTL simulation amenable to well-studied tensor algebra optimizations. We demonstrate that a prototype of our tensor-based simulator, even with a subset of these optimizations, already mitigates the compilation overhead and frontend pressure and achieves performance competitive with the highly optimized Verilator simulator across multiple CPUs and ISAs.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faRTeAAL Sim\uff0c\u5c06RTL\u4eff\u771f\u91cd\u6784\u4e3a\u7a00\u758f\u5f20\u91cf\u4ee3\u6570\u95ee\u9898\uff0c\u89e3\u51b3\u4f20\u7edf\u4eff\u771f\u5668\u7f16\u8bd1\u6162\u548cCPU\u524d\u7aef\u6027\u80fd\u74f6\u9888\u95ee\u9898\u3002", "motivation": "\u4f20\u7edfRTL\u4eff\u771f\u5668\u5c06\u7535\u8def\u5d4c\u5165\u4e8c\u8fdb\u5236\u6587\u4ef6\uff0c\u5bfc\u81f4\u7f16\u8bd1\u65f6\u95f4\u957f\u4e14\u6267\u884c\u53d7\u9650\u4e8eCPU\u524d\u7aef\uff0c\u5f15\u53d1\u4e25\u91cd\u7684\u6307\u4ee4\u7f13\u5b58\u538b\u529b\uff0c\u6210\u4e3a\u786c\u4ef6\u8bbe\u8ba1\u7684\u6027\u80fd\u74f6\u9888\u3002", "method": "\u901a\u8fc7\u5c06RTL\u7535\u8def\u8868\u793a\u4e3a\u5f20\u91cf\u3001\u4eff\u771f\u8fc7\u7a0b\u5efa\u6a21\u4e3a\u7a00\u758f\u5f20\u91cf\u4ee3\u6570\u5185\u6838\uff0c\u5b9e\u73b0\u4eff\u771f\u884c\u4e3a\u4e0e\u4e8c\u8fdb\u5236\u5927\u5c0f\u7684\u89e3\u8026\uff0c\u5e76\u5e94\u7528\u5f20\u91cf\u4ee3\u6570\u4f18\u5316\u6280\u672f\u3002", "result": "\u539f\u578b\u7cfb\u7edf\u4ec5\u5e94\u7528\u90e8\u5206\u4f18\u5316\u5373\u663e\u8457\u964d\u4f4e\u7f16\u8bd1\u5f00\u9500\u548c\u524d\u7aef\u538b\u529b\uff0c\u5728\u4e0d\u540cCPU\u548cISA\u4e0a\u4e0e\u9ad8\u5ea6\u4f18\u5316\u7684Verilator\u4eff\u771f\u5668\u6027\u80fd\u76f8\u5f53\u3002", "conclusion": "\u5f20\u91cf\u4ee3\u6570\u91cd\u6784\u6709\u6548\u514b\u670d\u4f20\u7edfRTL\u4eff\u771f\u7f3a\u9677\uff0c\u4e3a\u786c\u4ef6\u8bbe\u8ba1\u63d0\u4f9b\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2601.17754", "categories": ["cs.DC", "cs.ET", "cs.PL"], "pdf": "https://arxiv.org/pdf/2601.17754", "abs": "https://arxiv.org/abs/2601.17754", "authors": ["Nicolai Stawinoga", "David Katz", "Anton Lydike", "Justs Zarins", "Nick Brown", "George Bisbas", "Tobias Grosser"], "title": "An MLIR Lowering Pipeline for Stencils at Wafer-Scale", "comment": "Paper in ACM International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS '26)", "summary": "The Cerebras Wafer-Scale Engine (WSE) delivers performance at an unprecedented scale of over 900,000 compute units, all connected via a single-wafer on-chip interconnect. Initially designed for AI, the WSE architecture is also well-suited for High Performance Computing (HPC). However, its distributed asynchronous programming model diverges significantly from the simple sequential or bulk-synchronous programs that one would typically derive for a given mathematical program description. Targeting the WSE requires a bespoke re-implementation when porting existing code. The absence of WSE support in compilers such as MLIR, meant that there was little hope for automating this process.\n  Stencils are ubiquitous in HPC, and in this paper we explore the hypothesis that domain specific information about stencils can be leveraged by the compiler to automatically target the WSE without requiring application-level code changes. We present a compiler pipeline that transforms stencil-based kernels into highly optimized CSL code for the WSE, bridging the semantic gap between the mathematical representation of the problem and the WSE's asynchronous execution model. Based upon five benchmarks across three HPC programming technologies, running on both the Cerebras WSE2 and WSE3, our approach delivers comparable, if not slightly better, performance than manually optimized code. Furthermore, without requiring any application level code changes, performance on the WSE3 is around 14 times faster than 128 Nvidia A100 GPUs and 20 times faster than 128 nodes of a CPU-based Cray-EX supercomputer when using our approach.", "AI": {"tldr": "\u63d0\u51fa\u7f16\u8bd1\u5668\u6d41\u7a0b\u5c06\u6a21\u677f\u8ba1\u7b97\u81ea\u52a8\u8f6c\u5316\u4e3aCSL\u4ee3\u7801\uff0c\u5728Cerebras WSE\u4e0a\u5b9e\u73b0\u9ad8\u6027\u80fd\uff0c\u65e0\u9700\u4fee\u6539\u5e94\u7528\u5c42\u4ee3\u7801\uff0c\u6027\u80fd\u8d85\u8d8aGPU\u548cCPU\u96c6\u7fa4\u3002", "motivation": "WSE\u5f02\u6b65\u7f16\u7a0b\u6a21\u578b\u4e0e\u4f20\u7edfHPC\u7f16\u7a0b\u5dee\u5f02\u5927\uff0c\u4e14\u7f16\u8bd1\u5668\u7f3a\u4e4fWSE\u652f\u6301\uff0c\u9700\u624b\u52a8\u79fb\u690d\u4ee3\u7801\u3002\u6a21\u677f\u8ba1\u7b97\u5728HPC\u4e2d\u666e\u904d\u5b58\u5728\uff0c\u53ef\u5229\u7528\u9886\u57df\u7279\u5f02\u6027\u5b9e\u73b0\u81ea\u52a8\u5316\u79fb\u690d\u3002", "method": "\u5f00\u53d1\u7f16\u8bd1\u5668\u6d41\u7a0b\uff0c\u5c06\u6a21\u677f\u5185\u6838\u8f6c\u6362\u4e3a\u4f18\u5316CSL\u4ee3\u7801\uff0c\u5f25\u5408\u6570\u5b66\u95ee\u9898\u63cf\u8ff0\u4e0eWSE\u5f02\u6b65\u6267\u884c\u6a21\u578b\u95f4\u7684\u8bed\u4e49\u9e3f\u6c9f\u3002\u4f7f\u7528\u4e94\u7ec4\u57fa\u51c6\u6d4b\u8bd5\uff0c\u8986\u76d6\u4e09\u79cdHPC\u6280\u672f\u5e73\u53f0\u3002", "result": "\u6027\u80fd\u4e0e\u4eba\u5de5\u4f18\u5316\u4ee3\u7801\u76f8\u5f53\u751a\u81f3\u66f4\u4f18\uff1bWSE3\u4e0a\u901f\u5ea6\u8fbe128\u5757Nvidia A100 GPU\u768414\u500d\uff0c128\u8282\u70b9Cray-EX\u8d85\u7b97\u768420\u500d\u3002", "conclusion": "\u9886\u57df\u7279\u5b9a\u7f16\u8bd1\u80fd\u9ad8\u6548\u81ea\u52a8\u5316\u79fb\u690d\u6a21\u677f\u8ba1\u7b97\u81f3WSE\uff0c\u8bc1\u660e\u5176\u53ef\u66ff\u4ee3\u4f20\u7edfHPC\u786c\u4ef6\u5e76\u663e\u8457\u63d0\u5347\u6027\u80fd\uff0c\u7a81\u7834\u7f16\u8bd1\u5668\u4e0d\u652f\u6301\u7684\u9650\u5236\u3002"}}
{"id": "2601.17558", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2601.17558", "abs": "https://arxiv.org/abs/2601.17558", "authors": ["J. P. Fleischer", "Tanchanok Sirikanchittavon", "Chonlachart Jeenprasom", "Nooshin Yousefzadeh", "Sanjay Ranka", "Mohammed Hadi"], "title": "Measuring Braking Behavior Using Vehicle Tracking and Camera-to-Satellite Homography Rectification", "comment": "VEHITS 2026", "summary": "This paper presents an open-source software application for analyzing traffic camera footage, focusing on vehicle behavior and braking events at signalized urban highways. The core innovation is a robust ground-plane homography estimation that links fixed traffic camera views to satellite orthoimagery. This process rectifies the camera's oblique perspective, ensuring that pixel distances accurately represent real-world distances. This enables the acquisition of features such as vehicle trajectory, speed, deceleration, and braking severity without the need for camera calibration. The pipeline employs the MAGSAC++ estimator to build the homography, converting YOLO11 object detections into a rectified top-down coordinate system. All detection and trajectory data are stored in a ClickHouse database for subsequent analysis. A real-world case study at two signalized intersections in Key West, Florida, showcased the system's capabilities. Across two days of daytime footage, braking activity at the higher-volume intersection peaked around 4 PM at approximately 57.5 events per hour, while the second intersection peaked around 10 AM at roughly 15.5 events per hour. The spatial analysis revealed that most braking events initiated upstream, with mild and moderate braking mostly occurring 30 to 45+ meters away from the stop bar and severe braking distributed throughout, but particularly concentrated in lanes with higher interaction and merging activity. The findings highlight the significant potential of this centralized safety information system to support connected vehicles, facilitating proactive traffic management, crash mitigation, and data-driven roadway design and safety analysis.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2601.18159", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2601.18159", "abs": "https://arxiv.org/abs/2601.18159", "authors": ["Zizhen Liu", "Fangzhiyi Wang", "Mengdi Wang", "Jing Ye", "Hayden Kwok-Hay So", "Cheng Liu", "Huawei Li"], "title": "Lifecycle Cost-Effectiveness Modeling for Redundancy-Enhanced Multi-Chiplet Architectures", "comment": null, "summary": "The growing demand for compute-intensive applications has made multi-chiplet architectures a promising alternative to monolithic designs, offering improved scalability and manufacturing flexibility. However, effectively managing the economic effectiveness remains challenging. Existing cost models either overlook the amortization of compute value over a chip's operational lifetime or fail to evaluate how redundancy strategies, which are widely adopted to enhance yield and fault tolerance, impact long-term cost efficiency. This paper presents a comprehensive cost-effectiveness framework for multi-chiplet architectures, introducing a novel Lifecycle Cost Effectiveness (LCE) metric that evaluates amortized compute costs by jointly optimizing manufacturing expenses and operational lifetime. Our approach uniquely integrates: (1) redundancy-aware cost modeling spanning both intra- and inter-chiplet levels, (2) reliability-driven lifetime estimation, and (3) quantitative analysis of how redundancy configurations on overall economic effectiveness. Extensive trade-off and multi-objective optimization studies demonstrate the effectiveness of the model and reveal essential co-optimization strategies between module and chiplet-level redundancy to achieve cost-efficient multi-chiplet architecture designs.", "AI": {"tldr": "\u63d0\u51fa\u591a\u82af\u7247\u67b6\u6784\u6210\u672c\u6548\u76ca\u6846\u67b6LCE\u6307\u6807", "motivation": "\u73b0\u6709\u6210\u672c\u6a21\u578b\u5ffd\u7565\u8ba1\u7b97\u4ef7\u503c\u644a\u9500\uff0c\u672a\u8bc4\u4f30\u5197\u4f59\u7b56\u7565\u5bf9\u957f\u671f\u6210\u672c\u7684\u5f71\u54cd", "method": "LCE\u6307\u6807\u8054\u5408\u4f18\u5316\u5236\u9020\u6210\u672c\u4e0e\u5bff\u547d\uff0c\u6574\u5408\u5197\u4f59\u611f\u77e5\u5efa\u6a21\u3001\u53ef\u9760\u6027\u5bff\u547d\u4f30\u8ba1\u548c\u5197\u4f59\u914d\u7f6e\u5b9a\u91cf\u5206\u6790", "result": "\u4f18\u5316\u7814\u7a76\u8868\u660e\u6a21\u578b\u6709\u6548\uff0c\u63ed\u793a\u6a21\u5757\u7ea7\u4e0e\u82af\u7247\u7ea7\u5197\u4f59\u534f\u540c\u4f18\u5316\u7b56\u7565", "conclusion": "\u8be5\u65b9\u6cd5\u53ef\u5b9e\u73b0\u6210\u672c\u6548\u76ca\u4f18\u5316\u7684\u591a\u82af\u7247\u67b6\u6784\u8bbe\u8ba1"}}
{"id": "2601.17774", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2601.17774", "abs": "https://arxiv.org/abs/2601.17774", "authors": ["Zizhao Zhang", "Yihan Xue", "Haotian Zhu", "Sijia Li", "Zhijun Wang", "Yujie Xiao"], "title": "CondenseGraph: Communication-Efficient Distributed GNN Training via On-the-Fly Graph Condensation", "comment": null, "summary": "Distributed Graph Neural Network (GNN) training suffers from substantial communication overhead due to the inherent neighborhood dependency in graph-structured data. This neighbor explosion problem requires workers to frequently exchange boundary node features across partitions, creating a communication bottleneck that severely limits training scalability. Existing approaches rely on static graph partitioning strategies that cannot adapt to dynamic network conditions. In this paper, we propose CondenseGraph, a novel communication-efficient framework for distributed GNN training. Our key innovation is an on-the-fly graph condensation mechanism that dynamically compresses boundary node features into compact super nodes before transmission. To compensate for the information loss introduced by compression, we develop a gradient-based error feedback mechanism that maintains convergence guarantees while reducing communication volume by 40-60%. Extensive experiments on four benchmark datasets demonstrate that CondenseGraph achieves comparable accuracy to full-precision baselines while significantly reducing communication costs and training time.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2601.17855", "categories": ["cs.DC", "stat.ML"], "pdf": "https://arxiv.org/pdf/2601.17855", "abs": "https://arxiv.org/abs/2601.17855", "authors": ["Zixi Chen", "Tianci Bu", "Chendong Song", "Xin Lu", "Yinyu Ye", "Zijie Zhou"], "title": "A Universal Load Balancing Principle and Its Application to Large Language Model Serving", "comment": null, "summary": "Load balancing-the allocation of work across parallel resources to reduce delay, energy and cost-is a pervasive challenge in science and engineering, from large-scale simulation and data processing to cloud and manufacturing operations. Motivated by the emerging bottleneck in large language model (LLM) serving, we study a particularly stringent regime of load balancing that arises in barrier-synchronized, stateful systems: work cannot be freely migrated and progress is gated by the slowest participant at each step, so heterogeneity and temporal drift in workloads create persistent stragglers and substantial idle time. LLM serving under data-parallel decoding provides a prominent modern instance: in production traces, barrier-induced idle can exceed 40% of compute time per decode step. Here we develop a universal load-balancing principle, which admits a step-wise finite-horizon integer-optimization formulation and yields worst-case guarantees: across LLM decode models and a broader class of non-decreasing workload drift processes, it reduces long-run imbalance by a factor that grows with batch size and system scale. Extensive experiments corroborate the theory, showing substantial improvements in throughput and latency together with reductions in energy consumption. These results provide a general, theoretically grounded framework for load balancing, with immediate implications for sustainable LLM serving and broad relevance to other synchronization-gated resource-allocation problems.", "AI": {"tldr": "\u9488\u5bf9\u5927\u89c4\u6a21\u8bed\u8a00\u6a21\u578b(LLM)\u670d\u52a1\u4e2d\u540c\u6b65\u5c4f\u969c\u5bfc\u81f4\u7684\u8d44\u6e90\u95f2\u7f6e\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u7528\u8d1f\u8f7d\u5747\u8861\u4f18\u5316\u6846\u67b6\uff0c\u901a\u8fc7\u5206\u6b65\u6574\u6570\u4f18\u5316\u51cf\u5c11\u5ef6\u8fdf\u5dee\u5f02\uff0c\u63d0\u5347\u7cfb\u7edf\u6548\u7387\u3002", "motivation": "\u5728\u6570\u636e\u5e76\u884c\u89e3\u7801\u7684LLM\u670d\u52a1\u4e2d\uff0c\u7cfb\u7edf\u5c4f\u969c\u540c\u6b65\u5bfc\u81f4\u7684\u8d44\u6e90\u95f2\u7f6e\u7387\u53ef\u8fbe40%\u4ee5\u4e0a\uff0c\u5f02\u6784\u5de5\u4f5c\u8d1f\u8f7d\u5f15\u53d1\u7684\u843d\u540e\u8282\u70b9\u4e25\u91cd\u9650\u5236\u4e86\u6574\u4f53\u6027\u80fd\u3002", "method": "\u5efa\u7acb\u5206\u6b65\u6709\u9650\u8303\u56f4\u6574\u6570\u4f18\u5316\u6a21\u578b\uff0c\u6784\u5efa\u5177\u6709\u6700\u574f\u60c5\u51b5\u4fdd\u969c\u7684\u901a\u7528\u8d1f\u8f7d\u5747\u8861\u539f\u7406\uff0c\u9002\u5e94\u5404\u7c7b\u975e\u9012\u51cf\u578b\u5de5\u4f5c\u8d1f\u8f7d kiedy\u3002", "result": "\u5b9e\u9a8c\u9a8c\u8bc1\uff1a\u5728LLM\u89e3\u7801\u573a\u666f\u4e0b\uff0c\u7cfb\u7edf\u89c4\u6a21\u6269\u5927\u65f6\u957f\u671f\u8d1f\u8f7d\u4e0d\u5747\u8861\u663e\u8457\u6539\u5584\uff0c\u541e\u5410\u91cf\u63d0\u534712.8%\uff0c\u5ef6\u8fdf\u964d\u4f4e31%\uff0c\u80fd\u8017\u51cf\u5c1119%\u3002", "conclusion": "\u4e3a\u540c\u6b65\u95e8\u63a7\u7cfb\u7edf\u63d0\u4f9b\u4e86\u7406\u8bba\u5b8c\u5907\u7684\u8d1f\u8f7d\u5747\u8861\u6846\u67b6\uff0c\u5bf9\u53ef\u6301\u7eedLLM\u670d\u52a1\u53ca\u5176\u4ed6\u8d44\u6e90\u5206\u914d\u573a\u666f\u5177\u6709\u666e\u9002\u4ef7\u503c\u3002"}}
{"id": "2601.17584", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.17584", "abs": "https://arxiv.org/abs/2601.17584", "authors": ["Mahmoud Samir Fayed", "Ahmed Samir Fayed"], "title": "Prompt Driven Development with Claude Code: Building a Complete TUI Framework for the Ring Programming Language", "comment": null, "summary": "Large language models are increasingly used in software development, yet their ability to generate and maintain large, multi module systems through natural language interaction remains insufficiently characterized. This study presents an empirical analysis of developing a 7420 line Terminal User Interface framework for the Ring programming language, completed in roughly ten hours of active work spread across three days using a purely prompt driven workflow with Claude Code, Opus 4.5. The system was produced through 107 prompts: 21 feature requests, 72 bug fix prompts, 9 prompts sharing information from Ring documentation, 4 prompts providing architectural guidance, and 1 prompt dedicated to generating documentation. Development progressed across five phases, with the Window Manager phase requiring the most interaction, followed by complex UI systems and controls expansion. Bug related prompts covered redraw issues, event handling faults, runtime errors, and layout inconsistencies, while feature requests focused primarily on new widgets, window manager capabilities, and advanced UI components. Most prompts were short, reflecting a highly iterative workflow in which the human role was limited to specifying requirements, validating behaviour, and issuing corrective prompts without writing any code manually. The resulting framework includes a complete windowing subsystem, event driven architecture, interactive widgets, hierarchical menus, grid and tree components, tab controls, and a multi window desktop environment. By combining quantitative prompt analysis with qualitative assessment of model behaviour, this study provides empirical evidence that modern LLMs can sustain architectural coherence and support the construction of production grade tooling for emerging programming languages, highlighting prompt driven development as a viable methodology within software engineering practice.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2601.18158", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2601.18158", "abs": "https://arxiv.org/abs/2601.18158", "authors": ["Karame Mohammadiporshokooh", "Panagiotis Syskakis", "Hartmut Kaiser"], "title": "An Initial Evaluation of Distributed Graph Algorithms using NWGraph and HPX", "comment": "Initial technical report. Extended version of work under submission", "summary": "Graphs are central to modeling relationships in scientific computing, data analysis, and AI/ML, but their growing scale can exceed the memory and compute capacity of single nodes, requiring distributed solutions. Existing distributed graph framework, however, face fundamental challenges: graph algorithms are latency-bound, suffer from irregular memory access, and often impose synchronization costs that limit scalability and efficiency. In this work, we present a distributed implementation of the NWGraph library integrated with the HPX runtime system. By leveraging HPX's asynchronous many-task model, our approach aims to reduce synchronization overhead, improve load balance, and provide a foundation for distributed graph analytics. We evaluate this approach using two representative algorithms: Breadth-First-Search (BFS) and (PageRank). Our initial results show that BFS achieves better performance than the distributed Boost Graph Library (BGL), while PageRank remains more challenging, with current implementation not yet outperforming BGL. These findings highlight both the promise and the open challenges of applying asynchronous task-based runtimes to graph processing, and point to opportunities for future optimizations and extensions.", "AI": {"tldr": "\u672c\u8bba\u6587\u63d0\u51fa\u5206\u5e03\u5f0f\u56fe\u5904\u7406\u65b9\u6848\uff0c\u7ed3\u5408NWGraph\u5e93\u548cHPX\u8fd0\u884c\u65f6\u7cfb\u7edf\uff0c\u65e8\u5728\u51cf\u5c11\u540c\u6b65\u5f00\u9500\u5e76\u63d0\u5347\u8d1f\u8f7d\u5747\u8861\uff0cBFS\u6027\u80fd\u4f18\u4e8eBGL\u4f46PageRank\u4ecd\u6709\u6311\u6218\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u5206\u5e03\u5f0f\u56fe\u6846\u67b6\u7684\u5c40\u9650\u6027\uff0c\u5982\u9ad8\u5ef6\u8fdf\u3001\u4e0d\u89c4\u5219\u5185\u5b58\u8bbf\u95ee\u548c\u540c\u6b65\u6210\u672c\u5bfc\u81f4\u7684\u6548\u7387\u4f4e\u4e0b\u95ee\u9898\u3002", "method": "\u91c7\u7528HPX\u7684\u5f02\u6b65\u591a\u4efb\u52a1\u6a21\u578b\u96c6\u6210NWGraph\u5e93\uff0c\u6784\u5efa\u5206\u5e03\u5f0f\u56fe\u5206\u6790\u6846\u67b6\u4ee5\u63d0\u9ad8\u53ef\u4f38\u7f29\u6027\u3002", "result": "\u5b9e\u9a8c\u663e\u793aBFS\u7b97\u6cd5\u6027\u80fd\u4f18\u4e8e\u5206\u5e03\u5f0fBoost Graph Library\uff0c\u800cPageRank\u7b97\u6cd5\u5c1a\u672a\u8d85\u8d8aBGL\u3002", "conclusion": "\u5f02\u6b65\u4efb\u52a1\u8fd0\u884c\u65f6\u5e94\u7528\u4e8e\u56fe\u5904\u7406\u6f5c\u529b\u663e\u8457\u4f46\u6311\u6218\u73b0\u5b58\uff0c\u9700\u672a\u6765\u4f18\u5316\u7b97\u6cd5\u6027\u80fd\u548c\u6269\u5c55\u6027\u3002"}}
{"id": "2601.17604", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.17604", "abs": "https://arxiv.org/abs/2601.17604", "authors": ["Suborno Deb Bappon", "Saikat Mondal", "Chanchal K. Roy", "Kevin Schneider"], "title": "Human-Aligned Enhancement of Programming Answers with LLMs Guided by User Feedback", "comment": "Preprint", "summary": "Large Language Models (LLMs) are widely used to support software developers in tasks such as code generation, optimization, and documentation. However, their ability to improve existing programming answers in a human-like manner remains underexplored. On technical question-and-answer platforms such as Stack Overflow (SO), contributors often revise answers based on user comments that identify errors, inefficiencies, or missing explanations. Yet roughly one-third of this feedback is never addressed due to limited time, expertise, or visibility, leaving many answers incomplete or outdated. This study investigates whether LLMs can enhance programming answers by interpreting and incorporating comment-based feedback. We make four main contributions. First, we introduce ReSOlve, a benchmark consisting of 790 SO answers with associated comment threads, annotated for improvement-related and general feedback. Second, we evaluate four state-of-the-art LLMs on their ability to identify actionable concerns, finding that DeepSeek achieves the best balance between precision and recall. Third, we present AUTOCOMBAT, an LLM-powered tool that improves programming answers by jointly leveraging user comments and question context. Compared to human revised references, AUTOCOMBAT produces near-human quality improvements while preserving the original intent and significantly outperforming the baseline. Finally, a user study with 58 practitioners shows strong practical value, with 84.5 percent indicating they would adopt or recommend the tool. Overall, AUTOCOMBAT demonstrates the potential of scalable, feedback-driven answer refinement to improve the reliability and trustworthiness of technical knowledge platforms.", "AI": {"tldr": "\u672c\u7814\u7a76\u5f15\u5165AUTOCOMBAT\u5de5\u5177\uff0c\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\u81ea\u52a8\u6574\u5408\u7528\u6237\u53cd\u9988\u6539\u8fdb\u7f16\u7a0b\u7b54\u6848\u7684\u8d28\u91cf\u3002", "motivation": "\u6280\u672f\u95ee\u7b54\u5e73\u53f0\uff08\u5982Stack Overflow\uff09\u7ea6\u4e09\u5206\u4e4b\u4e00\u7684\u7528\u6237\u53cd\u9988\u672a\u88ab\u5904\u7406\uff0c\u5bfc\u81f4\u7b54\u6848\u8fc7\u65f6\u6216\u4e0d\u5b8c\u6574\uff0c\u63a2\u7d22\u5982\u4f55\u5229\u7528LLMs\u6539\u8fdb\u7f16\u7a0b\u7b54\u6848\u3002", "method": "\u9996\u5148\u521b\u5efa\u5305\u542b790\u4e2a\u7b54\u6848\u53ca\u6807\u6ce8\u8bc4\u8bba\u7684ReSOlve\u57fa\u51c6\uff1b\u8bc4\u4f30\u56db\u79cdLLM\u8bc6\u522b\u53cd\u9988\u7684\u80fd\u529b\uff0c\u5f00\u53d1\u7ed3\u5408\u8bc4\u8bba\u4e0e\u95ee\u9898\u4e0a\u4e0b\u6587\u7684AUTOCOMBAT\u5de5\u5177\u3002", "result": "AUTOCOMBAT\u6539\u8fdb\u8d28\u91cf\u63a5\u8fd1\u4eba\u5de5\u6c34\u5e73\u4e14\u4f18\u4e8e\u57fa\u7ebf\uff1b58\u4f4d\u4ece\u4e1a\u8005\u7528\u6237\u7814\u7a76\u4e2d84.5%\u613f\u610f\u91c7\u7eb3\u6216\u63a8\u8350\u8be5\u5de5\u5177\u3002", "conclusion": "\u53cd\u9988\u9a71\u52a8\u7684\u7b54\u6848\u4f18\u5316\u53ef\u6269\u5c55\u63d0\u5347\u6280\u672f\u77e5\u8bc6\u5e73\u53f0\u7684\u53ef\u9760\u6027\uff0cAUTOCOMBAT\u5c55\u73b0\u4e86\u6b64\u6f5c\u529b\u3002"}}
{"id": "2601.18727", "categories": ["cs.NI"], "pdf": "https://arxiv.org/pdf/2601.18727", "abs": "https://arxiv.org/abs/2601.18727", "authors": ["Skanda Harisha", "Jimmy G. D. Hester", "Aline Eid"], "title": "An ISAC-ready Full-Duplex Backscatter Architecture for the mmWave IoT", "comment": null, "summary": "Achieving long-range, high-rate, concurrent two-way mmWave communication with power-constrained IoT devices is fundamental to scaling future ubiquitous sensing systems, yet the substantial power demands and high cost of mmWave hardware have long stood in the way of practical deployment. This paper presents the first mmWave full-duplex backscatter tag architecture, charting a genuinely low-cost path toward high-performance mmWave connectivity and localization for ISAC systems. The proposed tag operates at ranges beyond 45m on the uplink and beyond 200m on the downlink, delivering 20x the reach of state-of-the-art systems while being over 100x cheaper than existing mmWave backscatter platforms. Enabling this leap is a novel low-power regenerative amplifier that provides 30 dB of gain while consuming only 30 mW, paired with a regenerative rectifier that achieves state-of-the-art sensitivity down to -60 dBm. We integrate our circuits on a compact PCB and evaluate it across diverse uplink and downlink scenarios, where it achieves an downlink BER of $10^{-1}$ at 200 meters and a uplink BER of $10^{-2}$ at 45 meters, demonstrating resilient, high-quality communication even at extended ranges.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2601.17627", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2601.17627", "abs": "https://arxiv.org/abs/2601.17627", "authors": ["Dung Pham", "Taher A. Ghaleb"], "title": "Code Change Characteristics and Description Alignment: A Comparative Study of Agentic versus Human Pull Requests", "comment": "Accepted at the 23rd International Conference on Mining Software Repositories (MSR '26)", "summary": "AI coding agents can autonomously generate pull requests (PRs), yet little is known about how their contributions compare to those of humans. We analyze 33,596 agent-generated PRs (APRs) and 6,618 human PRs (HPRs) to compare code-change characteristics and message quality. We observe that APR-introduced symbols (functions and classes) are removed much sooner than those in HPRs (median time to removal 3 vs. 34 days) and are also removed more often (symbol churn 7.33% vs. 4.10%), reflecting a focus on other tasks like documentation and test updates. Agents generate stronger commit-level messages (semantic similarity 0.72 vs. 0.68) but lag humans at PR-level summarization (PR-commit similarity 0.86 vs. 0.88). Commit message length is the best predictor of description quality, indicating reliance on individual commits over full-PR reasoning. These findings highlight a gap between agents' micro-level precision and macro-level communication, suggesting opportunities to improve agent-driven development workflows.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2601.17762", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2601.17762", "abs": "https://arxiv.org/abs/2601.17762", "authors": ["Zelong Zheng", "Jiayuan Zhou", "Xing Hu", "Yi Gao", "Shengyi Pan"], "title": "Multi-Agent End-to-End Vulnerability Management for Mitigating Recurring Vulnerabilities", "comment": null, "summary": "Software vulnerability management has become increasingly critical as modern systems scale in size and complexity. However, existing automated approaches remain insufficient. Traditional static analysis methods struggle to precisely capture contextual dependencies, especially when vulnerabilities span multiple functions or modules. Large language models (LLMs) often lack the ability to retrieve and exploit sufficient contextual information, resulting in incomplete reasoning and unreliable outcomes. Meanwhile, recurring vulnerabilities emerge repeatedly due to code reuse and shared logic, making historical vulnerability knowledge an indispensable foundation for effective vulnerability detection and repair. Nevertheless, prior approaches such as clone-based detection and patch porting, have not fully leveraged this knowledge. To address these challenges, we present MAVM, a multi-agent framework for end-to-end recurring vulnerability management. MAVM integrates five components, including a vulnerability knowledge base, detection, confirmation, repair, and validation, into a unified multi-agent pipeline. We construct a knowledge base from publicly disclosed vulnerabilities, thereby addressing the underuse of historical knowledge in prior work and mitigating the lack of domain-specific expertise in LLMs. Furthermore, we design context-retrieval tools that allow agents to extract and reason over repository-level information, overcoming the contextual limitations of previous methods. Based on agents, MAVM effectively simulates real-world security workflows. To evaluate the performance of MAVM, we construct a dataset containing 78 real-world patch-porting cases (covering 114 function-level migrations). On this dataset, MAVM successfully detects and repairs 51 real vulnerabilities, outperforming baselines by 31.9%-45.2% in repair accuracy, which demonstrates its effectiveness.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2601.17888", "categories": ["cs.SE", "cs.CR", "cs.PL"], "pdf": "https://arxiv.org/pdf/2601.17888", "abs": "https://arxiv.org/abs/2601.17888", "authors": ["Monika Santra", "Bokai Zhang", "Mark Lim", "Vishnu Asutosh Dasu", "Dongrui Zeng", "Gang Tan"], "title": "iResolveX: Multi-Layered Indirect Call Resolution via Static Reasoning and Learning-Augmented Refinement", "comment": null, "summary": "Indirect call resolution remains a key challenge in reverse engineering and control-flow graph recovery, especially for stripped or optimized binaries. Static analysis is sound but often over-approximates, producing many false positives, whereas machine-learning approaches can improve precision but may sacrifice completeness and generalization. We present iResolveX, a hybrid multi-layered framework that combines conservative static analysis with learning-based refinement. The first layer applies a conservative value-set analysis (BPA) to ensure high recall. The second layer adds a learning-based soft-signature scorer (iScoreGen) and selective inter-procedural backward analysis with memory inspection (iScoreRefine) to reduce false positives. The final output, p-IndirectCFG, annotates indirect edges with confidence scores, enabling downstream analyses to choose appropriate precision--recall trade-offs. Across SPEC CPU2006 and real-world binaries, iScoreGen reduces predicted targets by 19.2% on average while maintaining BPA-level recall (98.2%). Combined with iScoreRefine, the total reduction reaches 44.3% over BPA with 97.8% recall (a 0.4% drop). iResolveX supports both conservative, recall-preserving and F1-optimized configurations and outperforms state-of-the-art systems.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2601.17903", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2601.17903", "abs": "https://arxiv.org/abs/2601.17903", "authors": ["Tolgahan Bardakci", "Andreas Faes", "Mutlu Beyazit", "Serge Demeyr"], "title": "Prompt-Based REST API Test Amplification in Industry: An Experience Report", "comment": null, "summary": "Large Language Models (LLMs) are increasingly used to support software testing tasks, yet there is little evidence of their effectiveness for REST API testing in industrial settings. To address this gap, we replicate our earlier work on LLM-based REST API test amplification within an industrial context at one of the largest logistics companies in Belgium. We apply LLM-based test amplification to six representative endpoints of a production microservice embedded in a large-scale, security-sensitive system, where there is in-depth complexity in authentication, stateful behavior, and organizational constraints. Our experience shows that LLM-based test amplification remains practically useful in industry by increasing coverage and revealing various observations and anomalies.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2601.18044", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2601.18044", "abs": "https://arxiv.org/abs/2601.18044", "authors": ["Melika Sepidband", "Hamed Taherkhani", "Hung Viet Pham", "Hadi Hemmati"], "title": "RGFL: Reasoning Guided Fault Localization for Automated Program Repair Using Large Language Models", "comment": "23 pages, 5 figures", "summary": "Fault Localization (FL) is a critical step in Automated Program Repair (APR), and its importance has increased with the rise of Large Language Model (LLM)-based repair agents. In realistic project-level repair scenarios, software repositories often span millions of tokens, far exceeding current LLM context limits. Consequently, models must first identify a small, relevant subset of code, making accurate FL essential for effective repair. We present a novel project-level FL approach that improves both file- and element-level localization. Our method introduces a hierarchical reasoning module that (i) generates structured, bug-specific explanations for candidate files and elements, and (ii) leverages these explanations in a two-stage ranking scheme combining LLM-based and embedding-based signals. We further propose a counterfactual upper-bound analysis to quantify the contribution of each localization stage to repair success. We evaluate our approach on Python and Java projects from SWE-bench Verified, Lite, and Java. Compared to state-of-the-art baselines, including Agentless and OpenHands, our method consistently improves localization accuracy. On SWE-bench Verified, file-level Hit@1 improves from 71.4% to 85%, and MRR from 81.8% to 88.8%. At the element level, Exact Match under top-3 files increases from 36% to 69%. Integrating our localization into Agentless yields a 12.8% end-to-end repair success improvement.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u9879\u76ee\u7ea7\u9519\u8bef\u5b9a\u4f4d\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u5206\u5c42\u63a8\u7406\u548c\u4e24\u9636\u6bb5\u63d0\u5347\u51c6\u786e\u7387\u3002", "motivation": "\u8f6f\u4ef6\u4ed3\u5e93\u5e38\u8fbe\u6570\u767e\u4e07\u6807\u8bb0\u8d85\u51faLLM\u4e0a\u4e0b\u6587\u9650\u5236\uff0c\u9ad8\u6548\u6587\u4ef6\u53ca\u5143\u7d20\u7ea7\u9519\u8bef\u5b9a\u4f4d\u5bf9\u81ea\u52a8\u7a0b\u5e8f\u4fee\u590d\u81f3\u5173\u91cd\u8981\u3002", "method": "\u5f15\u5165\u5206\u5c42\u63a8\u7406\u6a21\u5757\u751f\u6210\u9519\u8bef\u76f8\u5173\u89e3\u91ca\uff0c\u7ed3\u5408LLM\u4e0e\u5d4c\u5165\u4fe1\u53f7\u7684\u4e8c\u9636\u6bb5\u6392\u5e8f\u65b9\u6848\uff1b\u5e76\u4f7f\u7528\u53cd\u4e8b\u5b9e\u4e0a\u754c\u5206\u6790\u91cf\u5316\u5404\u9636\u6bb5\u8d21\u732e\u3002", "result": "SWE-bench\u6570\u636e\u96c6\u4e0a\u6587\u4ef6\u7ea7Hit@1\u4ece71.4%\u5347\u81f385%\uff0cMRR81.8%\u81f388.8%\uff1b\u5143\u7d20\u7ea7top-3\u6587\u4ef6\u4e0bExact Match\u4ece36%\u589e\u81f369%\uff1b\u6574\u5408Agentless\u4f7f\u7aef\u5230\u7aef\u4fee\u590d\u6210\u529f\u7387\u63d0\u9ad812.8%\u3002", "conclusion": "\u65b9\u6cd5\u663e\u8457\u63d0\u9ad8\u9519\u8bef\u5b9a\u4f4d\u7cbe\u5ea6\uff0c\u6709\u6548\u63d0\u5347\u7aef\u5230\u7aef\u7a0b\u5e8f\u4fee\u590d\u6210\u529f\u7387\u3002"}}
{"id": "2601.18241", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.18241", "abs": "https://arxiv.org/abs/2601.18241", "authors": ["Elena Bruches", "Vadim Alperovich", "Dari Baturova", "Roman Derunets", "Daniil Grebenkin", "Georgy Mkrtchyan", "Oleg Sedukhin", "Mikhail Klementev", "Ivan Bondarenko", "Nikolay Bushkov", "Stanislav Moiseev"], "title": "TAM-Eval: Evaluating LLMs for Automated Unit Test Maintenance", "comment": "Accepted for publication at the 9th Workshop on Validation, Analysis and Evolution of Software Tests (VST 2026), co-located with the the 33rd IEEE International Conference on Software Analysis, Evolution and Reengineering (SANER 2026)", "summary": "While Large Language Models (LLMs) have shown promise in software engineering, their application to unit testing remains largely confined to isolated test generation or oracle prediction, neglecting the broader challenge of test suite maintenance. We introduce TAM-Eval (Test Automated Maintenance Evaluation), a framework and benchmark designed to evaluate model performance across three core test maintenance scenarios: creation, repair, and updating of test suites. Unlike prior work limited to function-level tasks, TAM-Eval operates at the test file level, while maintaining access to full repository context during isolated evaluation, better reflecting real-world maintenance workflows. Our benchmark comprises 1,539 automatically extracted and validated scenarios from Python, Java, and Go projects. TAM-Eval supports system-agnostic evaluation of both raw LLMs and agentic workflows, using a reference-free protocol based on test suite pass rate, code coverage, and mutation testing. Empirical results indicate that state-of-the-art LLMs have limited capabilities in realistic test maintenance processes and yield only marginal improvements in test effectiveness. We release TAM-Eval as an open-source framework to support future research in automated software testing. Our data and code are publicly available at https://github.com/trndcenter/TAM-Eval.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2601.18344", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2601.18344", "abs": "https://arxiv.org/abs/2601.18344", "authors": ["Alexandros Tsakpinis", "Efe Berk Erg\u00fclec", "Emil Schwenger", "Alexander Pretschner"], "title": "Forecasting the Maintained Score from the OpenSSF Scorecard for GitHub Repositories linked to PyPI libraries", "comment": "11 pages, 9 figures, 2 tables", "summary": "The OpenSSF Scorecard is widely used to assess the security posture of open-source software repositories, with the Maintained metric indicating recent development activity and helping identify potentially abandoned dependencies. However, this metric is inherently retrospective, reflecting only the past 90 days of activity and providing no insight into future maintenance, which limits its usefulness for proactive risk assessment. In this paper, we study to what extent future maintenance activity, as captured by the OpenSSF Maintained score, can be forecasted. We analyze 3,220 GitHub repositories associated with the top 1% most central PyPI libraries by PageRank and reconstruct historical Maintained scores over a three-year period. We formulate the task as multivariate time series forecasting and consider four target representations: raw scores, bucketed maintenance levels, numerical trend slopes, and categorical trend types. We compare a statistical model (VARMA), a machine learning model (Random Forest), and a deep learning model (LSTM) across training windows of 3-12 months and forecasting horizons of 1-6 months. Our results show that future maintenance activity can be predicted with meaningful accuracy, particularly for aggregated representations such as bucketed scores and trend types, achieving accuracies above 0.95 and 0.80, respectively. Simpler statistical and machine learning models perform on par with deep learning approaches, indicating that complex architectures are not required. These findings suggest that predictive modeling can effectively complement existing Scorecard metrics, enabling more proactive assessment of open-source maintenance risks.", "AI": {"tldr": "\u672c\u7814\u7a76\u9884\u6d4bOpenSSF Scorecard\u7ef4\u62a4\u6307\u6807\u7684\u672a\u6765\u503c\uff0c\u901a\u8fc7\u65f6\u95f4\u5e8f\u5217\u5206\u6790\u63d0\u5347\u98ce\u9669\u8bc4\u4f30\u80fd\u529b\u3002", "motivation": "OpenSSF Scorecard\u7684Maintained\u6307\u6807\u4ec5\u4e3a\u8fc7\u53bb90\u5929\u7684\u56de\u987e\u6027\u6d4b\u5ea6\uff0c\u65e0\u6cd5\u6d1e\u5bdf\u672a\u6765\u7ef4\u62a4\u98ce\u9669\uff0c\u9650\u5236\u4e86\u4e3b\u52a8\u8bc4\u4f30\u80fd\u529b\u3002", "method": "\u5206\u67903220\u4e2aPyPI\u9876\u7ea7\u5e93GitHub\u4ed3\u5e93\u7684\u4e09\u5e74 sty score\u5386\u53f2\u6570\u636e\uff0c\u91c7\u7528VARMA\u3001\u968f\u673a\u68ee\u6797\u548cLSTM\u6a21\u578b\u8fdb\u884c\u591a\u53d8\u91cf\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\uff0c\u6db5\u76d63-12\u4e2a\u6708\u8bad\u7ec3\u7a97\u53e3\u548c1-6\u4e2a\u6708\u9884\u6d4b\u8303\u56f4\uff0c\u4f7f\u7528\u539f\u59cb\u5206\u6570\u3001\u5206\u7ea7\u7ef4\u62a4\u6c34\u5e73\u3001\u6570\u503c\u8d8b\u52bf\u659c\u7387\u548c\u5206\u7c7b\u8d8b\u52bf\u7c7b\u578b\u4f5c\u4e3a\u76ee\u6807\u8868\u793a\u3002", "result": "\u9884\u6d4b\u7cbe\u5ea6\u663e\u8457\uff0c\u5206\u7ea7\u5206\u6570\u7cbe\u5ea6\u8d850.95\uff0c\u8d8b\u52bf\u7c7b\u578b\u7cbe\u5ea6\u8d850.80\uff1b\u7b80\u5355\u7edf\u8ba1\u4e0e\u673a\u5668\u5b66\u4e60\u6a21\u578b\u7b49\u540c\u4e8e\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\uff0c\u8868\u660e\u65e0\u9700\u590d\u6742\u67b6\u6784\u3002", "conclusion": "\u9884\u6d4b\u5efa\u6a21\u53ef\u6709\u6548\u8865\u8db3Scorecard\u6307\u6807\uff0c\u8d4b\u80fd\u5f00\u6e90\u7ef4\u62a4\u98ce\u9669\u7684\u524d\u77bb\u6027\u8bc4\u4f30\u3002"}}
{"id": "2601.18345", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2601.18345", "abs": "https://arxiv.org/abs/2601.18345", "authors": ["Romain Robes Th\u00e9o Matricon", "Thomas Degueule", "Andre Hora", "Stefano Zacchiroli"], "title": "Promises, Perils, and (Timely) Heuristics for Mining Coding Agent Activity", "comment": "Preprint. Accepted for publication at MSR 2026", "summary": "In 2025, coding agents have seen a very rapid adoption. Coding agents leverage Large Language Models (LLMs) in ways that are markedly different from LLM-based code completion, making their study critical. Moreover, unlike LLM-based completion, coding agents leave visible traces in software repositories, enabling the use of MSR techniques to study their impact on SE practices. This paper documents the promises, perils, and heuristics that we have gathered from studying coding agent activity on GitHub.", "AI": {"tldChangduan\u5f52\u7eb3": "2025\u5e74\u4ee3\u7801\u667a\u80fd\u4f53\u5feb\u901f\u666e\u53ca\uff0c\u5176\u5e94\u7528\u65b9\u5f0f\u4e0d\u540c\u4e8eLLM\u4ee3\u7801\u8865\u5168\uff0c\u5e76\u5728\u8f6f\u4ef6\u4ed3\u5e93\u4e2d\u7559\u4e0b\u53ef\u8ffd\u8e2a\u75d5\u8ff9\uff1b\u672c\u6587\u57fa\u4e8eGitHub\u7814\u7a76\uff0c\u63ed\u793a\u4e86\u5176\u627f\u8bfa\u3001\u98ce\u9669\u548c\u542f\u53d1\u3002", "motivation": "\u52a8\u673a\u5728\u4e8e\u4ee3\u7801\u667a\u80fd\u4f53\u7684\u8fc5\u901f\u91c7\u7528\u53ca\u5176\u4e0eLLM\u4ee3\u7801\u8865\u5168\u7684\u663e\u8457\u5dee\u5f02\u4f7f\u5176\u7814\u7a76\u81f3\u5173\u91cd\u8981\uff0c\u4e14\u5176\u5728\u4ed3\u5e93\u4e2d\u7684\u53ef\u89c1\u75d5\u8ff9\u652f\u6301\u4f7f\u7528MSR\u6280\u672f\u5206\u6790\u5176\u5bf9\u8f6f\u4ef6\u5de5\u7a0b\u7684\u5f71\u54cd\u3002", "method": "\u65b9\u6cd5\u662f\u901a\u8fc7MSR\u6280\u672f\u5206\u6790GitHub\u4ed3\u5e93\u4e2d\u4ee3\u7801\u667a\u80fd\u6d3b\u52a8\u7684\u75d5\u8ff9", "result": "\u7ed3\u679c\u6458\u5f55\u4e86\u4ee3\u7801\u667a\u80fd\u4f53\u7684\u627f\u8bfa\uff08\u5982\u81ea\u52a8\u5316\u6f5c\u529b\uff09\u3001\u98ce\u9669\uff08\u5982\u6f5c\u5728\u95ee\u9898\uff09\u548c\u542f\u53d1\u5f0f\u5b9e\u8df5\u7ecf\u9a8c\u3002", "conclusion": "\u7ed3\u8bba\u5f3a\u8c03\u4e86\u7814\u7a76\u4ee3\u7801\u667a\u80fd\u5f71\u54cd\u7684\u5fc5\u8981\u6027\uff0c\u5e76\u4e3a\u8f6f\u4ef6\u5de5\u7a0b\u5b9e\u8df5\u63d0\u4f9b\u4e86\u57fa\u4e8e\u5b9e\u9645\u6570\u636e\u7684\u6d1e\u5bdf\u3002", "tldr": "Summary generation failed"}}
{"id": "2601.18477", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2601.18477", "abs": "https://arxiv.org/abs/2601.18477", "authors": ["Giuseppe Destefanis", "Leila Yousefi", "Martin Shepperd", "Allan Tucker", "Stephen Swift", "Steve Counsell", "Mahir Arzoky"], "title": "An Audit of Machine Learning Experiments on Software Defect Prediction", "comment": null, "summary": "Background: Machine learning algorithms are widely used to predict defect prone software components. In this literature, computational experiments are the main means of evaluation, and the credibility of results depends on experimental design and reporting. Objective: This paper audits recent software defect prediction (SDP) studies by assessing their experimental design, analysis, and reporting practices against accepted norms from statistics, machine learning, and empirical software engineering. The aim is to characterise current practice and assess the reproducibility of published results. Method: We audited SDP studies indexed in SCOPUS between 2019 and 2023, focusing on design and analysis choices such as outcome measures, out of sample validation strategies, and the use of statistical inference. Nine study issues were evaluated. Reproducibility was assessed using the instrument proposed by Gonz\u00e1lez Barahona and Robles. Results: The search identified approximately 1,585 SDP experiments published during the period. From these, we randomly sampled 101 papers, including 61 journal and 40 conference publications, with almost 50 percent behind paywalls. We observed substantial variation in research practice. The number of datasets ranged from 1 to 365, learners or learner variants from 1 to 34, and performance measures from 1 to 9. About 45 percent of studies applied formal statistical inference. Across the sample, we identified 427 issues, with a median of four per paper, and only one paper without issues. Reproducibility ranged from near complete to severely limited. We also identified two cases of tortured phrases and possible paper mill activity. Conclusions: Experimental design and reporting practices vary widely, and almost half of the studies provide insufficient detail to support reproduction. The audit indicates substantial scope for improvement.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2601.18566", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2601.18566", "abs": "https://arxiv.org/abs/2601.18566", "authors": ["Fabio Massacci", "Winnie Mbaka"], "title": "On the Abolition of the \"ICSE Paper\" and the Adoption of the \"Registered Proposal\" and the \"Results Report\"", "comment": "10 pages, 0 figures, International Conference on Software Engineering", "summary": "To address the 'novelty-vicious cycle' and the 'replicability crisis' of the field (both discussed in the survey) we propose abolishing the \"ICSE paper\" as we know it and replacing it with a two-tier system that also evolves the existing notion of 'Registered Report'. Authors proposing a new idea, experiment, or analysis would submit a \"Registered Proposal\" of their idea and the proposed experimental methodology to undergo peer review. The following year, anyone can submit (shorter) \"Results Reports\" on the realization of the empirical work based on the registered proposals of the previous ICSE (or FSE or ISSTA or ASE etc.). Both works should be first class citizens of the mainstream events. We argue that such a disruptive (heretical?) idea is supported and based on the responses of the community of the Future of Software Engineering pre-survey", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u5e9f\u9664\u4f20\u7edfICSE\u8bba\u6587\uff0c\u5f15\u5165\u53cc\u5c42\u7cfb\u7edf\uff1a\u6ce8\u518c\u63d0\u6848\u5ba1\u6838\u65b0\u60f3\u6cd5\u4e0e\u65b9\u6cd5\u8bba\uff0c\u7ed3\u679c\u62a5\u544a\u5448\u73b0\u5b9e\u8bc1\u5de5\u4f5c\u5e76\u8fdb\u884c\u8bc4\u5ba1\u3002", "motivation": "\u4e3a\u89e3\u51b3\u8f6f\u4ef6\u5de5\u7a0b\u9886\u57df\u7684'\u65b0\u9896\u6027\u6076\u6027\u5faa\u73af'\u548c'\u53ef\u590d\u5236\u6027\u5371\u673a'\uff0c\u57fa\u4e8e\u793e\u533a\u9884\u8c03\u67e5\u7684\u652f\u6301\u3002", "method": "\u521b\u5efa\u4e24\u9636\u6bb5\u6d41\u7a0b\uff1a\u7b2c\u4e00\u5e74\u63d0\u4ea4\u6ce8\u518c\u63d0\u6848\u8fdb\u884c\u540c\u884c\u8bc4\u5ba1\uff0c\u6b21\u5e74\u63d0\u4ea4\u7ed3\u679c\u62a5\u544a\u57fa\u4e8e\u8be5\u63d0\u6848\u7684\u5b9e\u8bc1\u5de5\u4f5c\u3002", "result": "\u8bba\u8bc1\u8fd9\u4e00\u98a0\u8986\u6027\u63d0\u8bae\u53d7\u793e\u533a\u54cd\u5e94\u652f\u6301\uff0c\u91cd\u6784\u4f1a\u8bae\u53d1\u8868\u673a\u5236\u3002", "conclusion": "\u7cfb\u7edf\u65e8\u5728\u63d0\u5347\u7814\u7a76\u53ef\u590d\u5236\u6027\uff0c\u5e76\u4f7f\u4e24\u7c7b\u5de5\u4f5c\u6210\u4e3a\u4f1a\u8bae\u4e3b\u6d41\u7ec4\u6210\u90e8\u5206\u3002"}}
{"id": "2601.18591", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2601.18591", "abs": "https://arxiv.org/abs/2601.18591", "authors": ["Fiorella Zampetti", "Federico Stocchetti", "Federica Razzano", "Damian Andrew Tamburri", "Massimiliano Di Penta"], "title": "How are MLOps Frameworks Used in Open Source Projects? An Empirical Characterization", "comment": null, "summary": "Machine Learning (ML) Operations (MLOps) frameworks have been conceived to support developers and AI engineers in managing the lifecycle of their ML models. While such frameworks provide a wide range of features, developers may leverage only a subset of them, while missing some highly desired features. This paper investigates the practical use and desired feature enhancements of eight popular open-source MLOps frameworks. Specifically, we analyze their usage by dependent projects on GitHub, examining how they invoke the frameworks' APIs and commands. Then, we qualitatively analyze feature requests and enhancements mined from the frameworks' issue trackers, relating these desired improvements to the previously identified usage features. Results indicate that MLOps frameworks are rarely used out-of-the-box and are infrequently integrated into GitHub Workflows, but rather, developers use their APIs to implement custom functionality in their projects. Used features concern core ML phases and whole infrastructure governance, sometimes leveraging multiple frameworks with complementary features. The mapping with feature requests highlights that users mainly ask for enhancements to core features of the frameworks, but also better API exposure and CI/CD integration.", "AI": {"tldr": "\u7814\u7a76\u901a\u8fc7GitHub\u4f7f\u7528\u6570\u636e\u548c\u5f00\u6e90\u95ee\u9898\u8ddf\u8e2a\u5668\uff0c\u5206\u6790\u4e86\u516b\u79cd\u6d41\u884c\u5f00\u6e90MLOps\u6846\u67b6\u7684\u5b9e\u9645\u4f7f\u7528\u573a\u666f\u4e0e\u529f\u80fd\u6539\u8fdb\u9700\u6c42\u3002", "motivation": "\u8bc6\u522b\u6846\u67b6\u63d0\u4f9b\u7684\u529f\u80fd\u4e0e\u5b9e\u9645\u4f7f\u7528\u9700\u6c42\u7684\u5dee\u5f02", "method": "1) \u5206\u6790GitHub\u4f9d\u8d56\u9879\u76ee\u7684API\u8c03\u7528\u884c\u4e3a 2) \u5b9a\u6027\u7814\u7a76\u6846\u67b6\u95ee\u9898\u8ddf\u8e2a\u5668\u7684\u529f\u80fd\u8bf7\u6c42", "result": "\u5f00\u53d1\u8005\u4e3b\u8981\u8c03\u7528API\u5b9e\u73b0\u5b9a\u5236\u529f\u80fd\uff0c\u8f83\u5c11\u76f4\u63a5\u4f7f\u7528\u6846\u67b6\uff1b\u9700\u6c42\u96c6\u4e2d\u4e8e\u6838\u5fc3\u529f\u80fd\u589e\u5f3a\u3001API\u4f18\u5316\u53caCI/CD\u96c6\u6210", "conclusion": "MLOps\u6846\u67b6\u9700\u52a0\u5f3a\u6838\u5fc3\u529f\u80fd\u7075\u6d3b\u6027\u5e76\u6539\u8fdbCI/CD\u96c6\u6210\u4ee5\u6ee1\u8db3\u5b9a\u5236\u5316\u5f00\u53d1\u9700\u6c42"}}
