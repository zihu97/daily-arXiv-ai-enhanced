<div id=toc></div>

# Table of Contents

- [cs.NI](#cs.NI) [Total: 2]
- [cs.MA](#cs.MA) [Total: 8]
- [cs.DC](#cs.DC) [Total: 10]
- [cs.SE](#cs.SE) [Total: 21]
- [cs.AR](#cs.AR) [Total: 22]


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [1] [Enhancing 5G V2X Mode 2 for Sporadic Traffic](https://arxiv.org/abs/2510.17395)
*Dmitry Bankov,Artem Krasilov,Artem Otmakhov,Aleksei Shashin,Evgeny Khorov*

Main category: cs.NI

TL;DR: 本文针对5G车联网（V2X）中突发性安全类业务，分析了Mode 2（自主资源选择）的性能，并提出了若干改进方法，在几乎不增加复杂度的前提下将系统容量提升高达40%。


<details>
  <summary>Details</summary>
Motivation: 新兴的道路安全与自动驾驶应用要求车辆与车辆、车辆与基础设施之间实现低时延、高可靠的数据传输。对于突发性安全事件（如危险情况检测），传统调度方式难以满足严苛的时延要求，因此需优化基于Mode 2的自主资源分配机制。

Method: 分析5G V2X中Mode 2在突发流量场景下的性能表现，并提出若干改进策略以提升系统容量和可靠性。

Result: 仿真结果表明，所提出的改进方法可在复杂度增加极少的情况下，将系统容量最多提升40%。

Conclusion: 针对突发性高可靠低时延V2X通信，优化Mode 2机制能显著提升系统性能，为未来车联网安全应用提供有效支持。

Abstract: The emerging road safety and autonomous vehicle applications require timely
and reliable data delivery between vehicles and between vehicles and
infrastructure. To satisfy this demand, 3GPP develops a 5G
Vehicle-to-Everything (V2X) technology. Depending on the served traffic type,
5G V2X specifications propose two channel access methods: (i) Mode 1, according
to which a base station allocates resources to users, and (ii) Mode 2,
according to which users autonomously select resources for their transmissions.
In the paper, we consider a scenario with sporadic traffic, e.g., a vehicle
generates a packet at a random time moment when it detects a dangerous
situation, which imposes strict requirements on delay and reliability. To
satisfy strict delay requirements, vehicles use Mode 2. We analyze the
performance of Mode 2 for sporadic traffic and propose several approaches to
improve it. Simulation results show that the proposed approaches can increase
the system capacity by up to 40% with a low impact on complexity.

</details>


### [2] [Is It Worth to Use Feedback Channel in 5G V2X Platoon Scenarios?](https://arxiv.org/abs/2510.17410)
*Dmitry Bankov,Artem Krasilov,Artem Otmakhov,Pavel Savlukovich,Evgeny Khorov*

Main category: cs.NI

TL;DR: 本文研究了5G V2X中反馈信道对系统容量的影响，发现其在某些场景下可使系统容量提升至2倍，而在其他情况下则可能降低近一半，并提出了自适应选择反馈信道参数的建议。


<details>
  <summary>Details</summary>
Motivation: 5G V2X引入反馈信道以提升数据传输可靠性并辅助传输参数选择，但反馈信道占用资源会减少可用于数据传输的资源，因此需评估其对系统整体容量的影响。

Method: 通过NS-3仿真平台，在包含编队车辆（产生组播流量）和周围车辆（产生传统广播流量）的场景中，分析反馈信道使用对系统容量的影响。

Result: 仿真结果表明，反馈信道对系统容量的影响高度依赖于编队规模、组播与广播流量强度及服务质量需求：在某些条件下系统容量可提升至2倍，而在其他条件下则可能下降近50%。

Conclusion: 反馈信道的使用对5G V2X系统容量具有双重影响，应根据具体场景动态调整其参数以优化系统性能。

Abstract: 5G Vehicle-to-Everything (V2X) is a new technology developed by 3GPP to
support inter-vehicle communication. In contrast to 4G V2X which allows only
broadcast communication, 5G V2X enables groupcast and unicast communication.
Such types of communication are needed for new V2X scenarios: platooning,
extended sensors, remote driving, etc. To improve the data transmission
reliability and assist in the selection of the transmission parameters in these
scenarios, 5G V2X introduces a feedback channel that allows receivers to send
acknowledgments in response to data packets. However, some part of the overall
resource shall be allocated for the feedback channel, which reduces the amount
of channel resources available for data transmission. In this paper, we
consider a scenario with a platoon, which generates groupcast traffic, and
surrounding vehicles, which generate legacy broadcast traffic. Using extensive
simulations in NS-3, we analyze how the usage of the feedback channel
influences the overall system capacity. Our results show that depending on the
platoon size, groupcast, and broadcast traffic intensities, and their quality
of service requirements, the usage of the feedback channel can in some cases
significantly increase the system capacity (up to 2x), while in other cases it
almost halves the system capacity. We explain the reasons for such effects and
discuss how to adaptively select the feedback channel parameters.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [3] [Disaster Management in the Era of Agentic AI Systems: A Vision for Collective Human-Machine Intelligence for Augmented Resilience](https://arxiv.org/abs/2510.16034)
*Bo Li,Junwei Ma,Kai Yin,Yiming Xiao,Chia-Wei Hsu,Ali Mostafavi*

Main category: cs.MA

TL;DR: 本文提出“灾难副驾”（Disaster Copilot）——一种多智能体AI系统，通过整合预测分析、态势感知与影响评估等专用AI模块，在资源受限环境中实现协同决策，推动灾难数字孪生从静态模型向主动智能环境演进。


<details>
  <summary>Details</summary>
Motivation: 传统灾害应对能力常因数据碎片化、技术孤岛、资源限制及机构记忆流失而难以有效响应日益频发和严重的灾害事件，亟需一种系统性解决方案提升决策效率与韧性。

Method: 设计一个多智能体AI架构，由中央协调器统筹多个专业子智能体（如风险预测、态势感知、影响评估），融合多模态数据，支持设备端部署，并嵌入机构知识保存机制。

Result: 该系统可提供实时整体作战视图，支撑灾难数字孪生的智能化升级，并在资源受限条件下保持功能，同时缓解人员流动带来的知识断层问题。

Conclusion: Disaster Copilot通过人机协同智能，为构建更具适应性、数据驱动和韧性的社区提供了变革性愿景，并提出技术、组织能力与人机协作三阶段并行发展路线图。

Abstract: The escalating frequency and severity of disasters routinely overwhelm
traditional response capabilities, exposing critical vulnerability in disaster
management. Current practices are hindered by fragmented data streams, siloed
technologies, resource constraints, and the erosion of institutional memory,
which collectively impede timely and effective decision making. This study
introduces Disaster Copilot, a vision for a multi-agent artificial intelligence
system designed to overcome these systemic challenges by unifying specialized
AI tools within a collaborative framework. The proposed architecture utilizes a
central orchestrator to coordinate diverse sub-agents, each specializing in
critical domains such as predictive risk analytics, situational awareness, and
impact assessment. By integrating multi-modal data, the system delivers a
holistic, real-time operational picture and serve as the essential AI backbone
required to advance Disaster Digital Twins from passive models to active,
intelligent environments. Furthermore, it ensures functionality in
resource-limited environments through on-device orchestration and incorporates
mechanisms to capture institutional knowledge, mitigating the impact of staff
turnover. We detail the system architecture and propose a three-phased roadmap
emphasizing the parallel growth of technology, organizational capacity, and
human-AI teaming. Disaster Copilot offers a transformative vision, fostering
collective human-machine intelligence to build more adaptive, data-driven and
resilient communities.

</details>


### [4] [Zero-Shot Coordination in Ad Hoc Teams with Generalized Policy Improvement and Difference Rewards](https://arxiv.org/abs/2510.16187)
*Rupal Nigam,Niket Parikh,Hamid Osooli,Mikihisa Yuasa,Jacob Heglund,Huy T. Tran*

Main category: cs.MA

TL;DR: 本文提出了一种名为GPAT的新算法，通过广义策略改进和差异奖励，在无需额外训练的情况下实现对新团队的零样本迁移，有效提升多智能体系统在未知队友环境下的协作能力。


<details>
  <summary>Details</summary>
Motivation: 现实世界中的多智能体系统常需与未知队友即时协作完成任务（即即兴组队），现有方法要么基于对新队友的推断选择预训练策略，要么训练对各类队友都鲁棒的单一策略，但这些方法未能充分利用所有预训练策略的知识。

Method: 将即兴组队问题形式化为即兴多智能体马尔可夫决策过程，并结合广义策略改进（Generalized Policy Improvement）与差异奖励（Difference Rewards）机制，实现不同团队间高效有效的知识迁移。

Result: 在三个模拟环境（合作觅食、捕食者-猎物、Overcooked）及一个真实多机器人系统中，GPAT算法成功实现了对新团队的零样本迁移。

Conclusion: 所提出的GPAT算法能够有效利用多个预训练策略，在无需微调的情况下快速适应未知队友，显著提升即兴组队场景下的协作性能。

Abstract: Real-world multi-agent systems may require ad hoc teaming, where an agent
must coordinate with other previously unseen teammates to solve a task in a
zero-shot manner. Prior work often either selects a pretrained policy based on
an inferred model of the new teammates or pretrains a single policy that is
robust to potential teammates. Instead, we propose to leverage all pretrained
policies in a zero-shot transfer setting. We formalize this problem as an ad
hoc multi-agent Markov decision process and present a solution that uses two
key ideas, generalized policy improvement and difference rewards, for efficient
and effective knowledge transfer between different teams. We empirically
demonstrate that our algorithm, Generalized Policy improvement for Ad hoc
Teaming (GPAT), successfully enables zero-shot transfer to new teams in three
simulated environments: cooperative foraging, predator-prey, and Overcooked. We
also demonstrate our algorithm in a real-world multi-robot setting.

</details>


### [5] [Heterogeneous Multi-Agent Task-Assignment with Uncertain Execution Times and Preferences](https://arxiv.org/abs/2510.16221)
*Qinshuang Wei,Vaibhav Srivastava,Vijay Gupta*

Main category: cs.MA

TL;DR: 本文研究多智能体任务分配问题，其中智能体在完成任务时具有异构的能力与偏好，且任务相关的奖励、执行时间和资源消耗均为未知分布的随机变量。作者提出一种bandit算法，在有限时间范围内最大化团队总期望奖励，同时满足资源约束，并分析了在精确与近似求解最优任务分配下的累积遗憾。


<details>
  <summary>Details</summary>
Motivation: 现有研究多集中于单智能体的序列任务分配，而对具有异构能力与偏好的多智能体任务分配问题缺乏深入探讨，尤其在任务奖励、执行时间和资源消耗均为未知随机变量的情况下。

Method: 提出并分析一种bandit算法，该算法在每个时间步重复求解带资源约束的最优任务分配问题，并分别考虑精确求解与近似求解两种情形。

Result: 分析了所提bandit算法在精确和近似求解最优任务分配时的累积遗憾（regret）性能。

Conclusion: 该工作为具有异构能力和资源约束的多智能体随机任务分配问题提供了理论框架和算法，并对不同求解精度下的性能进行了遗憾分析。

Abstract: While sequential task assignment for a single agent has been widely studied,
such problems in a multi-agent setting, where the agents have heterogeneous
task preferences or capabilities, remain less well-characterized. We study a
multi-agent task assignment problem where a central planner assigns recurring
tasks to multiple members of a team over a finite time horizon. For any given
task, the members have heterogeneous capabilities in terms of task completion
times, task resource consumption (which can model variables such as energy or
attention), and preferences in terms of the rewards they collect upon task
completion. We assume that the reward, execution time, and resource consumption
for each member to complete any task are stochastic with unknown distributions.
The goal of the planner is to maximize the total expected reward that the team
receives over the problem horizon while ensuring that the resource consumption
required for any assigned task is within the capability of the agent. We
propose and analyze a bandit algorithm for this problem. Since the bandit
algorithm relies on solving an optimal task assignment problem repeatedly, we
analyze the achievable regret in two cases: when we can solve the optimal task
assignment exactly and when we can solve it only approximately.

</details>


### [6] [Prompt Optimization via Retrieved Reasoning Assets and Multi-Agent Analysis](https://arxiv.org/abs/2510.16635)
*Wonduk Seo,Juhyeon Lee,Junseo Koh,Hyunjin An,Jian Park,Seunghyun Lee,Haihua Chen,Yi Bu*

Main category: cs.MA

TL;DR: 本文提出MA-SAPO，一种多智能体框架，通过将评估分数与结构化推理结合，实现可解释、可控的提示优化。


<details>
  <summary>Details</summary>
Motivation: 现有提示优化方法将评估视为黑箱，仅依赖数值分数，缺乏对提示成功或失败原因的深入理解，且依赖难以解释和控制的试错式改进。

Method: MA-SAPO包含两个阶段：推理阶段中，多个智能体协作解释评估分数、诊断弱点并生成可复用的结构化推理资产；测试阶段中，智能体检索这些资产，仅应用有证据支持的编辑。

Result: 在HelpSteer1/2基准测试中，MA-SAPO在性能上持续优于单次提示、检索增强基线和现有多种多智能体策略。

Conclusion: 通过将评估信号转化为可解释的推理链，MA-SAPO实现了更透明、可审计和可控的提示优化，验证了该方法的有效性。

Abstract: Prompt optimization has emerged as an effective alternative to retraining for
improving the performance of Large Language Models (LLMs). However, most
existing approaches treat evaluation as a black box, relying solely on
numerical scores while offering limited insight into why a prompt succeeds or
fails. They also depend heavily on trial-and-error refinements, which are
difficult to interpret and control. In this paper, we introduce MA-SAPO, a
Multi-Agent framework for Score-Aware Prompt Optimization. Compared to prior
methods, MA-SAPO explicitly couples evaluation outcomes with structured
reasoning to guide systematic edits. The framework specifically consists of two
stages: during the Reasoning Phase, agents collaboratively explain metric
scores, diagnose weaknesses, and synthesize targeted refinements that are
stored as reusable reasoning assets; during the Test Phase, agents retrieve
these assets to analyze optimized prompts and apply only evidence-grounded
edits. By turning evaluation signals into interpretable reasoning chains,
MA-SAPO produces prompt refinements that are more transparent, auditable, and
controllable. Experiments on the HelpSteer1/2 benchmarks demonstrate consistent
improvements over single-pass prompting, retrieval-augmented baselines, and
prior multi-agent strategies, validating the effectiveness of our approach.

</details>


### [7] [DiRAC - Distributed Robot Awareness and Consensus](https://arxiv.org/abs/2510.16850)
*Uday Gopan,Manjari Kulkarni,Lakshasri S,Kashish Mittal,Sriram Radhakrishna,Aditya Naskar,Rameshwar DL*

Main category: cs.MA

TL;DR: DiRAC is a scalable, distributed framework for task assignment and path planning in large robotic swarms, featuring zone partitioning, dynamic leader election, tick-synchronized consensus, and a force-based decentralized planner, validated via ROS 2 simulations in warehouse scenarios.


<details>
  <summary>Details</summary>
Motivation: To address the challenges of coordination, scalability, and real-time collision avoidance in very large robotic swarms for industrial and logistics applications.

Method: DiRAC employs a zone-partitioned architecture with dynamically elected leaders and a tick-synchronized consensus protocol for strong consistency, combined with a novel force-based decentralized path planning algorithm for real-time collision resolution.

Result: Preliminary ROS 2 simulations in warehouse environments show that DiRAC achieves architectural scalability and modular efficiency.

Conclusion: DiRAC provides a promising foundation for deploying large-scale robotic swarms in real-world industrial and logistics settings.

Abstract: DiRAC is a scalable, distributed framework designed to enable efficient task
assignment and path planning in very large robotic swarms. It introduces a
novel zone-partitioned architecture with dynamically elected leaders and a
tick-synchronized consensus protocol that yields strong consistency and
deterministic outcomes. For path planning, DiRAC uses a novel algorithm, a
force-based decentralized planner for real-time collision resolution. Validated
within ROS 2 middleware through preliminary simulation, DiRAC demonstrates
architectural scalability and modular efficiency in simulated warehouse
environments, laying the groundwork for real-world deployment in large-scale
industrial and logistics domains.

</details>


### [8] [Lark: Biologically Inspired Neuroevolution for Multi-Stakeholder LLM Agents](https://arxiv.org/abs/2510.16978)
*Dheeraj Chintapalli,Rikhil Tanugula,Sunkalp Chandra*

Main category: cs.MA

TL;DR: Lark 是一个受生物启发的决策框架，结合大语言模型（LLM）推理与进化式多智能体系统，通过四种机制（可塑性调整、复制成熟、加权排序投票、计算成本感知）实现简洁、高效、兼顾多方利益的策略生成，在实验中表现优异且成本可控。


<details>
  <summary>Details</summary>
Motivation: 现有决策系统在处理多方利益权衡时往往冗长低效，缺乏对计算成本和利益相关者偏好的透明整合。Lark 旨在构建一个兼顾简洁性、多方偏好和计算效率的实用型决策框架。

Method: Lark 框架融合 LLM 推理与进化式多智能体系统，引入四种核心机制：(i) 可塑性（对候选方案进行简洁调整）；(ii) 复制与成熟（复制高性能方案并特化为新模块）；(iii) 基于影响力加权的排序选择投票（Borda 计分）；(iv) 基于 token 的计算成本惩罚机制。系统通过迭代生成策略、模拟评估、聚合偏好、选择与进化，并将计算成本纳入最终评分。

Result: 在30轮对照实验中，Lark Full 平均排名2.55（95% CI [2.17, 2.93]），平均综合得分29.4/50（95% CI [26.34, 32.46]），80%的轮次进入前三，单任务成本仅$0.016。消融实验证明四个机制均显著有效，其中复制/成熟机制影响最大（ΔScore = 3.5, p < 0.001）。

Conclusion: Lark 提供了一种实用、透明且计算高效的神经进化决策循环，能有效生成兼顾多方利益的策略。该研究为概念验证，未来将拓展至真实场景验证。

Abstract: We present Lark, a biologically inspired decision-making framework that
couples LLM-driven reasoning with an evolutionary, stakeholder-aware
Multi-Agent System (MAS). To address verbosity and stakeholder trade-offs, we
integrate four mechanisms: (i) plasticity, which applies concise adjustments to
candidate solutions; (ii) duplication and maturation, which copy
high-performing candidates and specialize them into new modules; (iii)
ranked-choice stakeholder aggregation using influence-weighted Borda scoring;
and (iv) compute awareness via token-based penalties that reward brevity. The
system iteratively proposes diverse strategies, applies plasticity tweaks,
simulates stakeholder evaluations, aggregates preferences, selects top
candidates, and performs duplication/maturation while factoring compute cost
into final scores. In a controlled evaluation over 30 rounds comparing 14
systems, Lark Full achieves a mean rank of 2.55 (95% CI [2.17, 2.93]) and a
mean composite score of 29.4/50 (95% CI [26.34, 32.46]), finishing Top-3 in 80%
of rounds while remaining cost competitive with leading commercial models
($0.016 per task). Paired Wilcoxon tests confirm that all four mechanisms
contribute significantly as ablating duplication/maturation yields the largest
deficit ({\Delta}Score = 3.5, Cohen's d_z = 2.53, p < 0.001), followed by
plasticity ({\Delta}Score = 3.4, d_z = 1.86), ranked-choice voting
({\Delta}Score = 2.4, d_z = 1.20), and token penalties ({\Delta}Score = 2.2,
d_z = 1.63). Rather than a formal Markov Decision Process with constrained
optimization, Lark is a practical, compute-aware neuroevolutionary loop that
scales stakeholder-aligned strategy generation and makes trade-offs transparent
through per-step metrics. Our work presents proof-of-concept findings and
invites community feedback as we expand toward real-world validation studies.

</details>


### [9] [ReclAIm: A multi-agent framework for degradation-aware performance tuning of medical imaging AI](https://arxiv.org/abs/2510.17004)
*Eleftherios Tzanis,Michail E. Klontzas*

Main category: cs.MA

TL;DR: ReclAIm 是一个基于大语言模型的多智能体框架，能通过自然语言交互自动监控、评估并微调医学影像分类模型，在无需编程技能的情况下实现模型性能的持续维护。


<details>
  <summary>Details</summary>
Motivation: 在临床实践中，AI模型的长期可靠性依赖于持续的性能监控和性能下降时的及时修正，而现有方法通常需要专业编程技能，限制了其广泛应用。

Method: ReclAIm 利用多智能体架构和大语言模型核心，通过自然语言交互实现对医学图像分类模型的自动监控、评估和微调，支持 MRI、CT 和 X 光等多种影像数据。

Result: 在面对高达 -41.1% 的性能下降（如 MRI InceptionV3 模型）时，ReclAIm 能自动执行先进微调策略，将性能恢复至初始水平的 1.5% 以内。

Conclusion: ReclAIm 提供了一种用户友好、无需编程的自动化方案，可有效维持医学影像 AI 模型的长期性能，有助于其在科研和临床环境中的广泛部署。

Abstract: Ensuring the long-term reliability of AI models in clinical practice requires
continuous performance monitoring and corrective actions when degradation
occurs. Addressing this need, this manuscript presents ReclAIm, a multi-agent
framework capable of autonomously monitoring, evaluating, and fine-tuning
medical image classification models. The system, built on a large language
model core, operates entirely through natural language interaction, eliminating
the need for programming expertise. ReclAIm successfully trains, evaluates, and
maintains consistent performance of models across MRI, CT, and X-ray datasets.
Once ReclAIm detects significant performance degradation, it autonomously
executes state-of-the-art fine-tuning procedures that substantially reduce the
performance gap. In cases with performance drops of up to -41.1% (MRI
InceptionV3), ReclAIm managed to readjust performance metrics within 1.5% of
the initial model results. ReclAIm enables automated, continuous maintenance of
medical imaging AI models in a user-friendly and adaptable manner that
facilitates broader adoption in both research and clinical environments.

</details>


### [10] [MiCRO for Multilateral Negotiations](https://arxiv.org/abs/2510.17401)
*David Aguilera-Luzon,Dave de Jonge,Javier Larrosa*

Main category: cs.MA

TL;DR: 本文提出了MiCRO策略的多边谈判变体，并在与ANAC竞赛优胜策略的对比中表现更优，且构成经验纳什均衡。


<details>
  <summary>Details</summary>
Motivation: MiCRO策略在双边谈判中表现出色，但其如何推广到多边谈判尚不明确；作者旨在填补这一空白。

Method: 提出MiCRO的多边变体，并与ANAC 2015、2017和2018年的优胜策略进行比较，同时进行经验博弈论分析。

Result: 新提出的多边MiCRO策略优于以往ANAC竞赛中的优胜策略，并构成经验纳什均衡。

Conclusion: MiCRO策略可成功推广至多边谈判场景，且在性能和博弈稳定性方面表现优异。

Abstract: Recently, a very simple new bilateral negotiation strategy called MiCRO was
introduced that does not make use of any kind of opponent modeling or machine
learning techniques and that does not require fine-tuning of any parameters.
Despite its simplicity, it was shown that MiCRO performs similar to -- or even
better than -- most state-of-the-art negotiation strategies. This lead its
authors to argue that the benchmark domains on which negotiation algorithms are
typically tested may be too simplistic. However, one question that was left
open, was how MiCRO could be generalized to multilateral negotiations. In this
paper we fill this gap by introducing a multilateral variant of MiCRO. We
compare it with the winners of the Automated Negotiating Agents Competitions
(ANAC) of 2015, 2017 and 2018 and show that it outperforms them. Furthermore,
we perform an empirical game-theoretical analysis to show that our new version
of MiCRO forms an empirical Nash equilibrium.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [11] [Integrating Performance Tools in Model Reasoning for GPU Kernel Optimization](https://arxiv.org/abs/2510.17158)
*Daniel Nichols,Konstantinos Parasyris,Charles Jekel,Abhinav Bhatele,Harshitha Menon*

Main category: cs.DC

TL;DR: 本文提出一种训练语言模型的方法，使其在推理过程中能与性能工具交互，并成功应用于GPU内核优化任务。


<details>
  <summary>Details</summary>
Motivation: 当前语言模型在代码性能相关任务（如优化）上表现不佳，因为这些任务依赖于源代码中未直接体现的环境和硬件等复杂数据。

Method: 提出一种训练语言模型的方法，使其在推理过程中能够与性能工具进行交互。

Result: 该方法被用于训练一个先进的GPU内核优化模型，展示了其有效性。

Conclusion: 通过让语言模型在推理中结合性能工具，可以显著提升其在代码性能优化等复杂任务上的表现。

Abstract: Language models are now prevalent in software engineering with many
developers using them to automate tasks and accelerate their development. While
language models have been tremendous at accomplishing complex software
engineering tasks, there are still many areas where they fail to deliver
desirable results, for instance code performance related tasks. Tasks like
optimization depend on many complex data from the environment, hardware, etc.
that are not directly represented in source code. Recent efforts have seen
large improvements in general code modeling tasks using chain-of-thought style
reasoning, but these models still fail to comprehend how the environment
interacts with code performance. In this paper we propose a methodology to
train language models that can interact with performance tools during their
reasoning process. We then demonstrate how this methodology can be used to
train a state-of-the-art GPU kernel optimization model.

</details>


### [12] [Communication-Efficient and Memory-Aware Parallel Bootstrapping using MPI](https://arxiv.org/abs/2510.16284)
*Di Zhang*

Main category: cs.DC

TL;DR: 本文提出两种基于MPI的并行自助法（bootstrapping）策略，通过本地统计量聚合和同步伪随机数生成，显著降低通信开销与内存占用，实现大规模数据下的高效并行自助法。


<details>
  <summary>Details</summary>
Motivation: 传统自助法在大数据集或高重采样次数下计算成本过高，难以在分布式环境中高效扩展，主要受限于通信开销大和内存限制。

Method: 提出两种新策略：1）本地统计量聚合，通过传输充分统计量而非完整重采样数据以减少通信；2）同步伪随机数生成，支持在单进程无法存储全量数据时进行分布式重采样。同时建立通信与计算复杂度的分析模型。

Result: 所提方法相比朴素基线显著减少了通信量和内存使用，提升了并行自助法在大规模系统中的可扩展性。

Conclusion: 通过优化通信与内存使用，本文提出的并行自助法策略有效解决了大规模数据下自助法的计算瓶颈，为高效统计推断提供了可行方案。

Abstract: Bootstrapping is a powerful statistical resampling technique for estimating
the sampling distribution of an estimator. However, its computational cost
becomes prohibitive for large datasets or a high number of resamples. This
paper presents a theoretical analysis and design of parallel bootstrapping
algorithms using the Message Passing Interface (MPI). We address two key
challenges: high communication overhead and memory constraints in distributed
environments. We propose two novel strategies: 1) Local Statistic Aggregation,
which drastically reduces communication by transmitting sufficient statistics
instead of full resampled datasets, and 2) Synchronized Pseudo-Random Number
Generation, which enables distributed resampling when the entire dataset cannot
be stored on a single process. We develop analytical models for communication
and computation complexity, comparing our methods against naive baseline
approaches. Our analysis demonstrates that the proposed methods offer
significant reductions in communication volume and memory usage, facilitating
scalable parallel bootstrapping on large-scale systems.

</details>


### [13] [FourierCompress: Layer-Aware Spectral Activation Compression for Efficient and Accurate Collaborative LLM Inference](https://arxiv.org/abs/2510.16418)
*Jian Ma,Xinchen Lyu,Jun Jiang,Longhao Zou,Chenshan Ren,Qimei Cui,Xiaofeng Tao*

Main category: cs.DC

TL;DR: 本文提出了一种名为FourierCompress的新型激活压缩框架，利用LLM激活在频域中的稀疏性，通过快速傅里叶变换（FFT）对低频系数进行压缩，在边缘设备上实现高效、近无损的大模型协同推理。


<details>
  <summary>Details</summary>
Motivation: 协同大语言模型推理在边缘设备上面临通信瓶颈问题，主要由于自回归解码结构导致中间激活数据传输量随输出长度线性增长，现有压缩方法难以兼顾高压缩率、低重建误差和计算效率。

Method: FourierCompress将激活转换到频域，保留紧凑的低频系数块，并利用共轭对称性在服务器端重建信号；该方法特别适用于第一层Transformer输出的平滑且能量集中的激活，支持DSP和FPGA硬件加速。

Result: 在Llama 3和Qwen2.5模型及10个常识推理数据集上的实验表明，FourierCompress相比Top-k、QR和SVD等方法，在保持性能接近无压缩基线的同时，平均减少7.6倍激活数据量、平均精度损失低于0.3%，并通过硬件加速实现压缩时间减少超32倍。

Conclusion: FourierCompress有效平衡了通信效率、近无损推理精度与压缩速度，为资源受限边缘设备上的大语言模型协同推理提供了实用解决方案。

Abstract: Collaborative large language model (LLM) inference enables real-time,
privacy-preserving AI services on resource-constrained edge devices by
partitioning computational workloads between client devices and edge servers.
However, this paradigm is severely hindered by communication bottlenecks caused
by the transmission of high-dimensional intermediate activations, exacerbated
by the autoregressive decoding structure of LLMs, where bandwidth consumption
scales linearly with output length. Existing activation compression methods
struggle to simultaneously achieve high compression ratios, low reconstruction
error, and computational efficiency. This paper proposes FourierCompress, a
novel, layer-aware activation compression framework that exploits the
frequency-domain sparsity of LLM activations. We rigorously demonstrate that
activations from the first Transformer layer exhibit strong smoothness and
energy concentration in the low-frequency domain, making them highly amenable
to near-lossless compression via the Fast Fourier Transform (FFT).
FourierCompress transforms activations into the frequency domain, retains only
a compact block of low-frequency coefficients, and reconstructs the signal at
the server using conjugate symmetry, enabling seamless hardware acceleration on
DSPs and FPGAs. Extensive experiments on Llama 3 and Qwen2.5 models across 10
commonsense reasoning datasets demonstrate that FourierCompress preserves
performance remarkably close to the uncompressed baseline, outperforming Top-k,
QR, and SVD. FourierCompress bridges the gap between communication efficiency
(an average 7.6x reduction in activation size), near-lossless inference (less
than 0.3% average accuracy loss), and significantly faster compression
(achieving over 32x reduction in compression time compared to Top-k via
hardware acceleration) for edge-device LLM inference.

</details>


### [14] [Edge-Based Speech Transcription and Synthesis for Kinyarwanda and Swahili Languages](https://arxiv.org/abs/2510.16497)
*Pacome Simon Mbonimpa,Diane Tuyizere,Azizuddin Ahmed Biyabani,Ozan K. Tonguz*

Main category: cs.DC

TL;DR: 本文提出了一种面向东非语言（卢旺达语和斯瓦希里语）的边缘-云协同语音转写与合成框架，通过在边缘设备和云端之间分配Whisper和SpeechT5模型的推理任务，在有限资源下实现低延迟、低内存占用的高效语音处理。


<details>
  <summary>Details</summary>
Motivation: 东非国家广泛使用卢旺达语和斯瓦希里语，但受限于技术基础设施，缺乏高效的语音处理工具。本文旨在解决这些语言在语音转写（STT）和语音合成（TTS）方面的资源匮乏问题，提升可访问性和处理效率。

Method: 采用预训练模型Whisper（用于STT）和SpeechT5（用于TTS），构建一种级联式边缘-云并行架构，将模型推理任务在边缘设备与云端之间动态分配，以降低延迟和资源消耗。在边缘端对模型进行压缩优化，减少内存占用。

Result: 在1.7 GHz CPU、1 MB/s带宽的边缘设备上，系统可在一分钟内完成270字符文本的STT和TTS处理；SpeechT5和Whisper模型内存占用分别压缩9.5%和14%，最大内存使用为149 MB。基于肯尼亚真实调查数据验证了该架构的准确性与响应速度。

Conclusion: 所提出的级联边缘-云架构能有效支持资源受限环境下卢旺达语和斯瓦希里语的高质量语音转写与合成，具备良好的实用性与可扩展性。

Abstract: This paper presents a novel framework for speech transcription and synthesis,
leveraging edge-cloud parallelism to enhance processing speed and accessibility
for Kinyarwanda and Swahili speakers. It addresses the scarcity of powerful
language processing tools for these widely spoken languages in East African
countries with limited technological infrastructure. The framework utilizes the
Whisper and SpeechT5 pre-trained models to enable speech-to-text (STT) and
text-to-speech (TTS) translation. The architecture uses a cascading mechanism
that distributes the model inference workload between the edge device and the
cloud, thereby reducing latency and resource usage, benefiting both ends. On
the edge device, our approach achieves a memory usage compression of 9.5% for
the SpeechT5 model and 14% for the Whisper model, with a maximum memory usage
of 149 MB. Experimental results indicate that on a 1.7 GHz CPU edge device with
a 1 MB/s network bandwidth, the system can process a 270-character text in less
than a minute for both speech-to-text and text-to-speech transcription. Using
real-world survey data from Kenya, it is shown that the cascaded edge-cloud
architecture proposed could easily serve as an excellent platform for STT and
TTS transcription with good accuracy and response time.

</details>


### [15] [Reimagining RDMA Through the Lens of ML](https://arxiv.org/abs/2510.16606)
*Ertza Warraich,Ali Imran,Annus Zulfiqar,Shay Vargaftik,Sonia Fahmy,Muhammad Shahbaz*

Main category: cs.DC

TL;DR: Celeris 是一种面向分布式机器学习的新型 RDMA 传输方案，通过放弃传统 RDMA 中的重传和有序交付机制，利用 ML 对数据丢失的容忍性，显著降低尾部延迟并提升可扩展性。


<details>
  <summary>Details</summary>
Motivation: 在大规模分布式机器学习中，传统 RDMA（如 RoCE、IRN、SRNIC）依赖重传和包排序来保证可靠性和有序性，但这些机制在面对罕见丢包或延迟时会导致尾部延迟显著上升，成为性能瓶颈。

Method: Celeris 移除了 RDMA 网卡中的重传和有序交付机制，采用尽力而为的传输方式，并保留拥塞控制（如 DCQCN）；同时通过软件层机制（如自适应超时、数据优先级）和 ML 管道中的容错方法（如 Hadamard 变换）处理丢包问题。

Result: 实验表明，Celeris 将第 99 百分位延迟最多降低 2.3 倍，BRAM 使用减少 67%，网卡对故障的容忍度接近翻倍。

Conclusion: Celeris 通过重新设计 RDMA 可靠性保障机制，为大规模机器学习提供了低延迟、高可扩展且具有容错能力的通信传输方案。

Abstract: As distributed machine learning (ML) workloads scale to thousands of GPUs
connected by ultra-high-speed inter-connects, tail latency in collective
communication has emerged as a primary bottleneck. Prior RDMA designs, like
RoCE, IRN, and SRNIC, enforce strict reliability and in-order delivery, relying
on retransmissions and packet sequencing to ensure correctness. While effective
for general-purpose workloads, these mechanisms introduce complexity and
latency that scale poorly, where even rare packet losses or delays can
consistently degrade system performance. We introduce Celeris, a
domain-specific RDMA transport that revisits traditional reliability guarantees
based on ML's tolerance for lost or partial data. Celeris removes
retransmissions and in-order delivery from the RDMA NIC, enabling best-effort
transport that exploits the robustness of ML workloads. It retains congestion
control (e.g., DCQCN) and manages communication with software-level mechanisms
such as adaptive timeouts and data prioritization, while shifting loss recovery
to the ML pipeline (e.g., using the Hadamard Transform). Early results show
that Celeris reduces 99th-percentile latency by up to 2.3x, cuts BRAM usage by
67%, and nearly doubles NIC resilience to faults -- delivering a resilient,
scalable transport tailored for ML at cluster scale.

</details>


### [16] [Layout-Agnostic MPI Abstraction for Distributed Computing in Modern C++](https://arxiv.org/abs/2510.16890)
*Jiří Klepl,Martin Kruliš,Matyáš Brabec*

Main category: cs.DC

TL;DR: 本文提出了一种基于C++ Noarr库的新型MPI抽象，通过布局无关的设计提升MPI应用的灵活性，并在保持与现有MPI C++绑定相当性能的同时，改善了类型安全和泛型支持。


<details>
  <summary>Details</summary>
Motivation: MPI长期使用古老的纯C接口，缺乏现代语言（如C++）的特性，例如类型检查和泛型代码设计支持，限制了开发效率与代码安全性。

Method: 作者将MPI抽象作为C++ Noarr库的扩展实现，遵循Noarr的范式（如一等布局和遍历抽象），提供布局无关的MPI应用设计方式，并通过一个布局无关的分布式GEMM内核作为案例研究。

Result: 所提出的抽象在性能上与当前最先进的MPI C++绑定相当，同时提供了更灵活的分布式应用设计能力。

Conclusion: 该工作成功地将现代C++特性引入MPI编程，通过Noarr库实现了一种兼具性能与灵活性的MPI抽象，有助于提升高性能计算中分布式应用的开发效率与可维护性。

Abstract: Message Passing Interface (MPI) has been a well-established technology in the
domain of distributed high-performance computing for several decades. However,
one of its greatest drawbacks is a rather ancient pure-C interface. It lacks
many useful features of modern languages (namely C++), like basic type-checking
or support for generic code design. In this paper, we propose a novel
abstraction for MPI, which we implemented as an extension of the C++ Noarr
library. It follows Noarr paradigms (first-class layout and traversal
abstraction) and offers layout-agnostic design of MPI applications. We also
implemented a layout-agnostic distributed GEMM kernel as a case study to
demonstrate the usability and syntax of the proposed abstraction. We show that
the abstraction achieves performance comparable to the state-of-the-art MPI C++
bindings while allowing for a more flexible design of distributed applications.

</details>


### [17] [FTI-TMR: A Fault Tolerance and Isolation Algorithm for Interconnected Multicore Systems](https://arxiv.org/abs/2510.16896)
*Yiming Hu*

Main category: cs.DC

TL;DR: 本文提出了一种用于互连多核系统的集成容错架构，通过构建稳定性度量和周期性诊断，在无需额外硬件的情况下实现永久性故障隔离与自适应任务调度，相比传统TMR方法降低约30%任务负载，并显著提升可靠性与能效。


<details>
  <summary>Details</summary>
Motivation: 传统两阶段三模冗余（TMR）在无故障时节能但无法应对永久性故障；而Reactive-TMR虽能处理永久性故障，却依赖额外硬件，增加系统复杂性且在多个核心或辅助模块失效时容错能力下降。因此，亟需一种无需额外硬件、兼具高可靠性和能效的容错架构。

Method: 提出一种集成容错架构，通过构建稳定性度量识别可靠计算单元，并结合周期性诊断机制，实现永久性故障隔离与自适应任务调度，无需引入额外硬件。

Result: 实验结果表明，该方法相比基线TMR降低约30%的任务负载，在故障覆盖率和隔离准确率方面表现更优。

Conclusion: 所提架构在不增加硬件开销的前提下，有效提升了互连多核系统的可靠性与能效，克服了现有TMR方案在永久性故障处理方面的局限性。

Abstract: Two-Phase Triple Modular Redundancy TMR divides redundancy operations into
two stages, omitting part of the computation during fault-free operation to
reduce energy consumption. However, it becomes ineffective under permanent
faults, limiting its reliability in critical systems. To address this,
Reactive-TMR (R-TMR) introduces permanent fault isolation mechanisms for faulty
cores, tolerating both transient and permanent faults. Yet, its reliance on
additional hardware increases system complexity and reduces fault tolerance
when multiple cores or auxiliary modules fail. This paper proposes an
integrated fault-tolerant architecture for interconnected multicore systems. By
constructing a stability metric to identify reliable machines and performing
periodic diagnostics, the method enables permanent fault isolation and adaptive
task scheduling without extra hardware. Experimental results show that it
reduces task workload by approximately 30% compared to baseline TMR and
achieves superior fault coverage and isolation accuracy, significantly
improving both reliability and energy efficiency.

</details>


### [18] [Tutoring LLM into a Better CUDA Optimizer](https://arxiv.org/abs/2510.16933)
*Matyáš Brabec,Jiří Klepl,Michal Töpfer,Martin Kruliš*

Main category: cs.DC

TL;DR: 本文评估了最新大语言模型（LLMs）在生成优化CUDA代码方面的能力，发现其虽具备较强编码能力，但需通过提示中的详细指导（“辅导”）才能达到专家级优化水平。


<details>
  <summary>Details</summary>
Motivation: 探索当前大语言模型在生成针对已知任务的优化CUDA代码方面的潜力，特别是其自主实现代码优化与并行模式的能力，以及通过提示中的辅导是否能进一步提升性能。

Method: 通过自动评估（正确性与加速比）和人工代码审查，测试LLM在有无辅导提示下生成的CUDA代码质量，并尝试交互式方法让模型在会话中修正错误。

Result: LLM展现出较强的编码能力，但在无辅导情况下难以达到专家级优化效果；通过提供更详细的提示可显著提升其生成代码的优化水平。

Conclusion: 大语言模型在CUDA代码生成方面具有潜力，但要实现专家级性能优化，仍需依赖提示中的明确指导或交互式修正机制。

Abstract: Recent leaps in large language models (LLMs) caused a revolution in
programming tools (like GitHub Copilot) that can help with code generation,
debugging, and even performance optimization. In this paper, we focus on the
capabilities of the most recent reasoning models to generate optimized CUDA
code for predefined, well-known tasks. Our objective is to determine which
types of code optimizations and parallel patterns the LLMs can perform by
themselves and whether they can be improved by tutoring (providing more
detailed hints and guidelines in the prompt). The generated solutions were
evaluated both automatically (for correctness and speedup) and manually (code
reviews) to provide a more detailed perspective. We also tried an interactive
approach where the LLM can fix its previous mistakes within a session. The
results indicate that LLMs are quite skilled coders; however, they require
tutoring to reach optimized solutions provided by parallel computing experts.

</details>


### [19] [Host-Side Telemetry for Performance Diagnosis in Cloud and HPC GPU Infrastructure](https://arxiv.org/abs/2510.16946)
*Erfan Darzi,Aldo Pareja,Shreeanant Bharadwaj*

Main category: cs.DC

TL;DR: 本文提出了一种基于eBPF的遥测系统，用于诊断云和高性能计算环境中GPU尾部延迟尖峰问题，通过关联主机指标与GPU内部事件实现高精度、低开销的根因分析。


<details>
  <summary>Details</summary>
Motivation: 现有监控工具在共享计算环境中缺乏细粒度的根因分析能力，难以有效诊断GPU尾部延迟尖峰，影响性能可预测性和资源利用率。

Method: 设计并实现了一个基于eBPF的遥测系统，统一监控GPU工作负载，将eBPF获取的主机指标与GPU内部事件进行关联，以实现系统级的整体可观测性。

Result: 该系统在分布式学习工作负载中实现了81–88%的诊断准确率，可在5秒内检测到延迟尖峰，并在6–8秒内完成根因分析，CPU开销仅为1.21%（100Hz采样），成功识别出NIC争用、PCIe压力和CPU干扰等根因。

Conclusion: 该eBPF遥测系统能够在无需集群范围插桩的情况下，有效支持多租户GPU基础设施的运行时调试，显著提升GPU尾部延迟问题的诊断效率与精度。

Abstract: Diagnosing GPU tail latency spikes in cloud and HPC infrastructure is
critical for maintaining performance predictability and resource utilization,
yet existing monitoring tools lack the granularity for root cause analysis in
shared computing environments. We introduce an eBPF-based telemetry system that
provides unified host-side monitoring of GPU workloads, correlating
eBPF-derived host metrics with GPU-internal events for holistic system
observability. The system achieves 81--88\% diagnostic accuracy, detects spikes
within 5 seconds, and completes root cause analysis in 6--8 seconds, operating
with 1.21\% CPU overhead at 100Hz sampling. Evaluated on distributed learning
workloads, the system identifies root causes including NIC contention, PCIe
pressure, and CPU interference, enabling operational debugging for multi-tenant
GPU infrastructure without requiring cluster-wide instrumentation.

</details>


### [20] [On the Universality of Round Elimination Fixed Points](https://arxiv.org/abs/2510.17639)
*Alkida Balliu,Sebastian Brandt,Ole Gabsdil,Dennis Olivetti,Jukka Suomela*

Main category: cs.DC

TL;DR: 本文研究了轮次消除（round elimination）不动点是否能作为证明分布式图算法中局部可检验问题下界的通用技术。作者通过新方法证明了此前唯一障碍（同态问题）确实可通过轮次消除处理，但同时揭示了新障碍：某些带输入的问题虽需Ω(log n)轮，却无法通过松弛到非平凡轮次消除不动点来证明下界，因此轮次消除对带输入问题并非通用，但可能对无输入问题通用。此外，作者还给出了首个适用于任意轮次消除不动点问题（无论是否带输入）的通用下界定理。


<details>
  <summary>Details</summary>
Motivation: 探究轮次消除不动点是否是证明分布式LOCAL模型中局部可检验问题Ω(log n)轮下界的通用方法，这对判断分布式计算复杂性中关键部分是否可判定具有重要意义。

Method: 提出一种基于“三幂输入”（tripotent inputs）的新技术，系统性地构建轮次消除下界；利用该方法分析同态问题，并构造反例以揭示轮次消除的局限性；同时证明一个适用于任意轮次消除不动点问题的通用下界定理。

Result: 1）证明了此前唯一障碍（ITCS 2022中的同态问题）确实存在基于轮次消除不动点的下界证明；2）发现新障碍：存在某些带输入的问题虽需Ω(log n)轮，但无法通过松弛到非平凡轮次消除不动点来证明下界；3）建立了首个适用于任意轮次消除不动点问题（含或不含输入）的通用下界定理。

Conclusion: 轮次消除不动点不是处理所有带输入问题的通用下界证明技术，但可能对无输入问题具有普适性；本文既消除了旧障碍，又揭示了新限制，并推进了轮次消除理论的通用性边界。

Abstract: Recent work on distributed graph algorithms [e.g. STOC 2022, ITCS 2022, PODC
2020] has drawn attention to the following open question: are round elimination
fixed points a universal technique for proving lower bounds? That is, given a
locally checkable problem $\Pi$ that requires at least $\Omega(\log n)$ rounds
in the deterministic LOCAL model, can we always find a relaxation $\Pi'$ of
$\Pi$ that is a nontrivial fixed point for the round elimination technique [see
STOC 2016, PODC 2019]? If yes, then a key part of distributed computational
complexity would be also decidable.
  The key obstacle so far has been a certain family of homomorphism problems
[ITCS 2022], which require $\Omega(\log n)$ rounds, but the only known proof is
based on Marks' technique [J.AMS 2016].
  We develop a new technique for constructing round elimination lower bounds
systematically. Using so-called tripotent inputs we show that the
aforementioned homomorphism problems indeed admit a lower bound proof that is
based on round elimination fixed points. Hence we eliminate the only known
obstacle for the universality of round elimination.
  Yet we also present a new obstacle: we show that there are some problems with
inputs that require $\Omega(\log n)$ rounds, yet there is no proof that is
based on relaxations to nontrivial round elimination fixed points. Hence round
elimination cannot be a universal technique for problems with inputs (but it
might be universal for problems without inputs).
  We also prove the first fully general lower bound theorem that is applicable
to any problem, with or without inputs, that is a fixed point in round
elimination. Prior results of this form were only able to handle certain very
restricted inputs.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [21] [Code Contribution and Credit in Science](https://arxiv.org/abs/2510.16242)
*Eva Maxfield Brown,Isaac Slaughter,Nicholas Weber*

Main category: cs.SE

TL;DR: 该研究通过匹配约14万篇科研论文与代码仓库，发现近30%的论文包含未被列为作者的代码贡献者，且频繁参与编码的作者其学术影响力指标（如h指数）反而更低，表明软件贡献与传统学术认可之间存在脱节。


<details>
  <summary>Details</summary>
Motivation: 探索软件开发在科研中的作用及其与传统学术认可机制（如作者署名和引用）之间的关系，以理解当前学术奖励体系是否充分认可软件贡献。

Method: 构建包含约14万对科研论文与代码仓库的数据集，开发预测模型将论文作者与代码仓库开发者账户匹配，并通过统计分析探究软件贡献与学术影响力指标之间的关系。

Result: 近30%的论文包含未署名的代码贡献者；代码贡献作者的引用量仅小幅增加（约4.2%），但在控制领域、文章类型和开放获取状态后不显著；第一作者更可能是代码贡献者；频繁编码者的h指数显著低于非编码者，即使控制了发表数量、作者位置、领域和文章类型。

Conclusion: 软件贡献在当前学术评价体系中未获得充分认可，甚至与传统学术影响力指标呈负相关，这提示需要改革机构奖励机制和科研政策，以更公平地承认软件开发在科研中的价值。

Abstract: Software development has become essential to scientific research, but its
relationship to traditional metrics of scholarly credit remains poorly
understood. We develop a dataset of approximately 140,000 paired research
articles and code repositories, as well as a predictive model that matches
research article authors with software repository developer accounts. We use
this data to investigate how software development activities influence credit
allocation in collaborative scientific settings. Our findings reveal
significant patterns distinguishing software contributions from traditional
authorship credit. We find that nearly 30% of articles include non-author code
contributors- individuals who participated in software development but received
no formal authorship recognition. While code-contributing authors show a modest
$\sim$4.2% increase in article citations, this effect becomes non-significant
when controlling for domain, article type, and open access status. First
authors are significantly more likely to be code contributors than other author
positions. Notably, we identify a negative relationship between coding
frequency and scholarly impact metrics. Authors who contribute code more
frequently exhibit progressively lower h-indices than non-coding colleagues,
even when controlling for publication count, author position, domain, and
article type. These results suggest a disconnect between software contributions
and credit, highlighting important implications for institutional reward
structures and science policy.

</details>


### [22] [MLCPD: A Unified Multi-Language Code Parsing Dataset with Universal AST Schema](https://arxiv.org/abs/2510.16357)
*Jugal Gajjar,Kamalasankari Subramaniakuppusamy*

Main category: cs.SE

TL;DR: 本文提出了多语言代码解析器数据集（MLCPD），这是一个涵盖十种主流编程语言、包含超过七百万个源文件的大规模语言无关数据集，采用统一的抽象语法树（AST）模式，支持跨语言的结构化代码理解和分析。


<details>
  <summary>Details</summary>
Motivation: 现有代码语料库通常仅关注词元级别的代码或孤立的解析器，缺乏统一的结构表示和跨语言一致性，限制了多语言程序分析和表示学习的发展。

Method: 构建并发布MLCPD数据集，提出一种通用的抽象语法树（AST）模式，对十种编程语言的源代码进行解析和标准化，保留层次化树结构和丰富的元数据，并以Parquet格式存储；同时提供完整的数据生成、语法编译和可视化工具链。

Result: 实证分析表明，Python、Java、Go等差异显著的语言在该统一AST模式下展现出强结构规律性，验证了跨语言对齐的可行性。

Conclusion: MLCPD为跨语言代码表示学习和程序分析提供了开放、可复现的基础资源，推动了多语言软件理解的研究。

Abstract: We introduce the MultiLang Code Parser Dataset (MLCPD), a large-scale,
language-agnostic dataset unifying syntactic and structural representations of
code across ten major programming languages. MLCPD contains over seven million
parsed source files normalized under our proposed universal Abstract Syntax
Tree (AST) schema, enabling consistent cross-language reasoning, structural
learning, and multilingual software analysis. Unlike existing corpora that
focus purely on token-level code or isolated parsers, MLCPD provides both
hierarchical tree representations and rich metadata for every file, ensuring
lossless syntactic coverage and structural uniformity. Each entry includes a
normalized schema, language-level metadata, and abstracted node semantics
stored in Parquet format for scalable retrieval. Empirical analyses reveal
strong cross-language structural regularities-demonstrating that syntactic
graphs from languages as diverse as Python, Java, and Go can be aligned under a
shared schema. We release the dataset publicly on Hugging Face and the
accompanying codebase on GitHub, which includes complete pipelines for dataset
reproduction, grammar compilation, and a visualization tool for exploring the
unified AST across languages. Together, these resources establish MLCPD as an
open, reproducible foundation for future research in cross-language
representation learning and program analysis.

</details>


### [23] [SemOpt: LLM-Driven Code Optimization via Rule-Based Analysis](https://arxiv.org/abs/2510.16384)
*Yuwei Zhao,Yuan-An Xiao,Qianyu Xiao,Zhao Zhang,Yingfei Xiong*

Main category: cs.SE

TL;DR: 本文提出SemOpt，一种结合静态程序分析与大语言模型（LLM）的自动化代码优化框架，通过构建优化策略库、生成Semgrep规则并指导LLM进行优化，在多个基准和真实C/C++项目中显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于LLM的代码优化方法依赖BM25等信息检索技术从开源提交中获取优化示例，但由于语义等价的优化可能在语法上差异较大，导致检索失败，影响优化效果。

Method: SemOpt包含三个LLM驱动的核心组件：(1) 从真实代码变更中提取并聚类优化策略的策略库构建器；(2) 生成Semgrep静态分析规则以识别适用优化条件的规则生成器；(3) 利用策略库生成优化代码的优化器。

Result: 在包含151个优化任务的基准测试中，SemOpt相比基线方法将成功优化数量提升1.38至28倍；在主流C/C++项目中，性能指标提升5.04%至218.07%。

Conclusion: SemOpt通过结合静态分析与LLM，有效克服了现有检索方法在语义-语法不一致问题上的局限性，显著提升了自动化代码优化的效果和实用性。

Abstract: Automated code optimization aims to improve performance in programs by
refactoring code, and recent studies focus on utilizing LLMs for the
optimization. Typical existing approaches mine optimization commits from
open-source codebases to construct a large-scale knowledge base, then employ
information retrieval techniques such as BM25 to retrieve relevant optimization
examples for hotspot code locations, thereby guiding LLMs to optimize these
hotspots. However, since semantically equivalent optimizations can manifest in
syntactically dissimilar code snippets, current retrieval methods often fail to
identify pertinent examples, leading to suboptimal optimization performance.
This limitation significantly reduces the effectiveness of existing
optimization approaches.
  To address these limitations, we propose SemOpt, a novel framework that
leverages static program analysis to precisely identify optimizable code
segments, retrieve the corresponding optimization strategies, and generate the
optimized results. SemOpt consists of three key components: (1) A strategy
library builder that extracts and clusters optimization strategies from
real-world code modifications. (2) A rule generator that generates Semgrep
static analysis rules to capture the condition of applying the optimization
strategy. (3) An optimizer that utilizes the strategy library to generate
optimized code results. All the three components are powered by LLMs.
  On our benchmark containing 151 optimization tasks, SemOpt demonstrates its
effectiveness under different LLMs by increasing the number of successful
optimizations by 1.38 to 28 times compared to the baseline. Moreover, on
popular large-scale C/C++ projects, it can improve individual performance
metrics by 5.04% to 218.07%, demonstrating its practical utility.

</details>


### [24] [Code Digital Twin: Empowering LLMs with Tacit Knowledge for Complex Software Development](https://arxiv.org/abs/2510.16395)
*Xin Peng,Chong Wang*

Main category: cs.SE

TL;DR: 本文提出“代码数字孪生”（Code Digital Twin）框架，通过整合混合知识表示、多阶段知识抽取、增量更新、大语言模型应用与人机协同反馈，将企业软件开发中的隐性知识显性化，以弥合大语言模型能力与企业级软件开发实际需求之间的鸿沟。


<details>
  <summary>Details</summary>
Motivation: 企业软件开发主要依赖渐进式演进，涉及大量隐性知识（如设计决策和历史权衡），而当前大语言模型虽在编码任务中表现出色，却难以应对这类复杂现实挑战，因此需要一种能将AI能力与企业开发实践对齐的新框架。

Method: 提出“代码数字孪生”框架，该框架建模软件的物理层与概念层，通过混合知识表示、多阶段抽取管道、增量更新机制、大语言模型驱动的应用以及人在环路反馈，实现隐性知识的持续捕获与显性化。

Result: 该框架能将碎片化的开发知识转化为明确且可操作的表示形式，支持问题定位、影响分析等关键任务，提升AI在复杂企业软件开发中的实用性。

Conclusion: 代码数字孪生为连接AI技术进步与企业软件开发现实提供了可行路径，是实现超复杂系统可持续、智能和韧性演进的关键基础设施。

Abstract: Recent advances in large language models (LLMs) have demonstrated strong
capabilities in software engineering tasks, raising expectations of
revolutionary productivity gains. However, enterprise software development is
largely driven by incremental evolution, where challenges extend far beyond
routine coding and depend critically on tacit knowledge, including design
decisions at different levels and historical trade-offs. To achieve effective
AI-powered support for complex software development, we should align emerging
AI capabilities with the practical realities of enterprise development. To this
end, we systematically identify challenges from both software and LLM
perspectives. Alongside these challenges, we outline opportunities where AI and
structured knowledge frameworks can enhance decision-making in tasks such as
issue localization and impact analysis. To address these needs, we propose the
Code Digital Twin, a living framework that models both the physical and
conceptual layers of software, preserves tacit knowledge, and co-evolves with
the codebase. By integrating hybrid knowledge representations, multi-stage
extraction pipelines, incremental updates, LLM-empowered applications, and
human-in-the-loop feedback, the Code Digital Twin transforms fragmented
knowledge into explicit and actionable representations. Our vision positions it
as a bridge between AI advancements and enterprise software realities,
providing a concrete roadmap toward sustainable, intelligent, and resilient
development and evolution of ultra-complex systems.

</details>


### [25] [Large-Scale Empirical Analysis of Continuous Fuzzing: Insights from 1 Million Fuzzing Sessions](https://arxiv.org/abs/2510.16433)
*Tatsuya Shirai,Olivier Nourry,Yutaro Kashiwa,Kenji Fujiwara,Yasutaka Kamei,Hajimu Iida*

Main category: cs.SE

TL;DR: 本文通过分析OSS-Fuzz平台上878个项目的约112万次模糊测试会话，揭示了持续模糊测试在漏洞检测中的作用：早期检测率高、代码覆盖率持续增长，且覆盖率变化有助于发现漏洞。


<details>
  <summary>Details</summary>
Motivation: 尽管持续模糊测试已被广泛采用，但其在漏洞检测中的实际贡献尚不明确。本文旨在通过实证研究阐明持续模糊测试如何影响漏洞的发现。

Method: 作者从Google提供的OSS-Fuzz平台收集了问题报告、覆盖率报告和模糊测试日志，对878个开源项目共计约112万次模糊测试会话进行了实证分析。

Result: 研究发现：(i) 大量模糊测试漏洞在集成持续模糊测试前已存在，导致早期检测率高；(ii) 随着持续模糊测试进行，代码覆盖率持续提升；(iii) 覆盖率的变化有助于发现新的模糊测试漏洞。

Conclusion: 本研究为持续模糊测试在漏洞检测中的作用提供了实证依据，并对未来持续模糊测试策略和工具开发具有实际指导意义。

Abstract: Software vulnerabilities are constantly being reported and exploited in
software products, causing significant impacts on society. In recent years, the
main approach to vulnerability detection, fuzzing, has been integrated into the
continuous integration process to run in short and frequent cycles. This
continuous fuzzing allows for fast identification and remediation of
vulnerabilities during the development process. Despite adoption by thousands
of projects, however, it is unclear how continuous fuzzing contributes to
vulnerability detection. This study aims to elucidate the role of continuous
fuzzing in vulnerability detection. Specifically, we investigate the coverage
and the total number of fuzzing sessions when fuzzing bugs are discovered. We
collect issue reports, coverage reports, and fuzzing logs from OSS-Fuzz, an
online service provided by Google that performs fuzzing during continuous
integration. Through an empirical study of a total of approximately 1.12
million fuzzing sessions from 878 projects participating in OSS-Fuzz, we reveal
that (i) a substantial number of fuzzing bugs exist prior to the integration of
continuous fuzzing, leading to a high detection rate in the early stages; (ii)
code coverage continues to increase as continuous fuzzing progresses; and (iii)
changes in coverage contribute to the detection of fuzzing bugs. This study
provides empirical insights into how continuous fuzzing contributes to fuzzing
bug detection, offering practical implications for future strategies and tool
development in continuous fuzzing.

</details>


### [26] [On the Use of Large Language Models for Qualitative Synthesis](https://arxiv.org/abs/2510.16502)
*Sebastián Pizard,Ramiro Moreira,Federico Galiano,Ignacio Sastre,Lorena Etcheverry*

Main category: cs.SE

TL;DR: 本文探讨了将大语言模型（LLMs）用于系统综述中的定性综合（QS）所面临的挑战，通过两次试验的协作自民族志研究，评估其方法严谨性与实用价值，并结合LLMs的技术特性与局限性进行解读。


<details>
  <summary>Details</summary>
Motivation: 由于定性综合在系统综述中报告不一致、执行方式多样，直接应用大语言模型存在误用风险，可能放大现有缺陷并削弱综述结果的可信度，因此有必要深入探讨其应用挑战。

Method: 采用协作自民族志方法，开展两项试验，评估其方法学严谨性和实际效用，并结合大语言模型的技术构建原理与当前局限进行技术性解读。

Result: 研究揭示了在定性综合中使用大语言模型所面临的具体挑战，包括方法适配性、结果可靠性及技术局限带来的潜在风险。

Conclusion: 在将大语言模型应用于系统综述的定性综合阶段时，需谨慎对待其技术局限，避免因误用而损害综述质量；未来应加强方法规范与模型适配性研究。

Abstract: Large language models (LLMs) show promise for supporting systematic reviews
(SR), even complex tasks such as qualitative synthesis (QS). However, applying
them to a stage that is unevenly reported and variably conducted carries
important risks: misuse can amplify existing weaknesses and erode confidence in
the SR findings. To examine the challenges of using LLMs for QS, we conducted a
collaborative autoethnography involving two trials. We evaluated each trial for
methodological rigor and practical usefulness, and interpreted the results
through a technical lens informed by how LLMs are built and their current
limitations.

</details>


### [27] [Human-Aligned Code Readability Assessment with Large Language Models](https://arxiv.org/abs/2510.16579)
*Wendkûuni C. Ouédraogo,Yinghua Li,Xueqi Dang,Pawel Borsukiewicz,Xin Zhou,Anil Koyuncu,Jacques Klein,David Lo,Tegawendé F. Bissyandé*

Main category: cs.SE

TL;DR: 本文提出了 CoReEval，首个用于评估大语言模型（LLM）代码可读性判断能力的大规模基准，涵盖10个LLM、3种语言、多种提示策略和开发者角色设定，发现基于人类定义维度的引导式提示能提升LLM与人类判断的一致性和解释质量。


<details>
  <summary>Details</summary>
Motivation: 传统静态指标难以准确反映代码可读性的人类主观判断，而大语言模型虽具潜力，但其在可读性评估中的表现尚缺乏系统研究。

Method: 构建包含140万次评估的 CoReEval 基准，覆盖10个LLM、3种编程语言、2类代码、4种提示策略、9种解码设置及面向初级/高级开发者的人设提示；通过与人工标注和静态模型对比，从数值一致性（MAE、Pearson、Spearman）和解释质量（情感、维度覆盖、语义聚类）两方面进行分析。

Result: 引导式提示（尤其是结合人类定义的可读性维度和开发者人设）在结构化上下文中显著提升LLM与人类判断的一致性与解释质量，但评分变异性增加，揭示了对齐性、稳定性与可解释性之间的权衡。

Conclusion: CoReEval 为LLM在代码可读性评估中的提示工程、模型对齐及人机协同评估提供了坚实基础，适用于教育、新人引导和CI/CD等场景，使LLM成为可解释、可定制的代码评审工具。

Abstract: Code readability is crucial for software comprehension and maintenance, yet
difficult to assess at scale. Traditional static metrics often fail to capture
the subjective, context-sensitive nature of human judgments. Large Language
Models (LLMs) offer a scalable alternative, but their behavior as readability
evaluators remains underexplored. We introduce CoReEval, the first large-scale
benchmark for evaluating LLM-based code readability assessment, comprising over
1.4 million model-snippet-prompt evaluations across 10 state of the art LLMs.
The benchmark spans 3 programming languages (Java, Python, CUDA), 2 code types
(functional code and unit tests), 4 prompting strategies (ZSL, FSL, CoT, ToT),
9 decoding settings, and developer-guided prompts tailored to junior and senior
personas. We compare LLM outputs against human annotations and a validated
static model, analyzing numerical alignment (MAE, Pearson's, Spearman's) and
justification quality (sentiment, aspect coverage, semantic clustering). Our
findings show that developer-guided prompting grounded in human-defined
readability dimensions improves alignment in structured contexts, enhances
explanation quality, and enables lightweight personalization through persona
framing. However, increased score variability highlights trade-offs between
alignment, stability, and interpretability. CoReEval provides a robust
foundation for prompt engineering, model alignment studies, and human in the
loop evaluation, with applications in education, onboarding, and CI/CD
pipelines where LLMs can serve as explainable, adaptable reviewers.

</details>


### [28] [Contrasting the Hyperparameter Tuning Impact Across Software Defect Prediction Scenarios](https://arxiv.org/abs/2510.16665)
*Mohamed Sami Rakha,Andriy Miranskyy,Daniel Alencar da Costa*

Main category: cs.SE

TL;DR: 本研究比较了超参数调优在两种软件缺陷预测（SDP）场景——版本内缺陷预测（IVDP）和跨版本缺陷预测（CVDP）中的效果差异，发现IVDP中调优带来的性能提升显著高于CVDP，且小规模数据集对调优效果更敏感。


<details>
  <summary>Details</summary>
Motivation: 现有研究表明超参数调优可提升SDP性能，但其效果可能因SDP场景不同而异。为增强SDP模型的鲁棒性、泛化能力和实用性，有必要系统比较调优在不同SDP场景下的影响。

Method: 研究采用两种调优算法、28种机器学习算法、53个发布后软件数据集和五种优化指标，在IVDP和CVDP两种场景下进行实验，并通过统计分析比较调优对整体性能、单个算法及不同数据集规模下的影响差异。

Result: IVDP场景中超参数调优带来的性能提升显著大于CVDP；28种算法中有24种的性能增益无法在不同SDP场景间一致保持；小规模软件数据集对调优效果的差异更敏感。

Conclusion: 软件工程研究人员和从业者在期望通过超参数调优获得性能提升时，应充分考虑所选SDP场景的影响。

Abstract: Software defect prediction (SDP) is crucial for delivering high-quality
software products. Recent research has indicated that prediction performance
improvements in SDP are achievable by applying hyperparameter tuning to a
particular SDP scenario. However, the positive impact resulting from the
hyperparameter tuning step may differ based on the targeted SDP scenario.
Comparing the impact of hyperparameter tuning across SDP scenarios is necessary
to provide comprehensive insights and enhance the robustness, generalizability,
and, eventually, the practicality of SDP modeling for quality assurance.
  Therefore, in this study, we contrast the impact of hyperparameter tuning
across two pivotal and consecutive SDP scenarios: (1) Inner Version Defect
Prediction (IVDP) and (2) Cross Version Defect Prediction (CVDP). The main
distinctions between the two scenarios lie in the scope of defect prediction
and the selected evaluation setups. This study's experiments use common
evaluation setups, 28 machine learning (ML) algorithms, 53 post-release
software datasets, two tuning algorithms, and five optimization metrics. We
apply statistical analytics to compare the SDP performance impact differences
by investigating the overall impact, the single ML algorithm impact, and
variations across different software dataset sizes.
  The results indicate that the SDP gains within the IVDP scenario are
significantly larger than those within the CVDP scenario. The results reveal
that asserting performance gains for up to 24 out of 28 ML algorithms may not
hold across multiple SDP scenarios. Furthermore, we found that small software
datasets are more susceptible to larger differences in performance impacts.
Overall, the study findings recommend software engineering researchers and
practitioners to consider the effect of the selected SDP scenario when
expecting performance gains from hyperparameter tuning.

</details>


### [29] [QuanBench: Benchmarking Quantum Code Generation with Large Language Models](https://arxiv.org/abs/2510.16779)
*Xiaoyu Guo,Minggu Wang,Jianjun Zhao*

Main category: cs.SE

TL;DR: 本文提出了QuanBench，一个用于评估大语言模型（LLMs）在量子代码生成能力上的基准，包含44个涵盖量子算法、态制备、门分解和量子机器学习的编程任务，并通过功能正确性和量子语义等价性进行评估。实验表明当前LLMs在该任务上表现有限，准确率低于40%，并存在多种常见错误。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型在通用代码生成方面表现良好，但其在量子代码生成方面的能力尚未得到充分研究，因此需要一个专门的基准来评估和推动该领域的发展。

Method: 构建QuanBench基准，包含44个具有可执行标准解的量子编程任务，采用功能正确性（Pass@K）和量子语义等价性（过程保真度）作为评估指标，并对多个通用和代码专用的大语言模型进行测试。

Result: 当前大语言模型在量子代码生成任务上的整体准确率低于40%，常出现API过时、电路构建错误和算法逻辑错误等语义问题。

Conclusion: QuanBench为未来提升大语言模型在量子代码生成方面的能力提供了评估基础和研究方向。

Abstract: Large language models (LLMs) have demonstrated good performance in general
code generation; however, their capabilities in quantum code generation remain
insufficiently studied. This paper presents QuanBench, a benchmark for
evaluating LLMs on quantum code generation. QuanBench includes 44 programming
tasks that cover quantum algorithms, state preparation, gate decomposition, and
quantum machine learning. Each task has an executable canonical solution and is
evaluated by functional correctness (Pass@K) and quantum semantic equivalence
(Process Fidelity). We evaluate several recent LLMs, including general-purpose
and code-specialized models. The results show that current LLMs have limited
capability in generating the correct quantum code, with overall accuracy below
40% and frequent semantic errors. We also analyze common failure cases, such as
outdated API usage, circuit construction errors, and incorrect algorithm logic.
QuanBench provides a basis for future work on improving quantum code generation
with LLMs.

</details>


### [30] [More with Less: An Empirical Study of Turn-Control Strategies for Efficient Coding Agents](https://arxiv.org/abs/2510.16786)
*Pengfei Gao,Chao Peng*

Main category: cs.SE

TL;DR: 本文系统研究了大语言模型驱动的编码智能体在执行软件工程任务时的轮次控制策略，发现动态轮次分配策略在保持甚至提升任务解决率的同时显著降低成本，优于固定轮次限制和无限制基线。


<details>
  <summary>Details</summary>
Motivation: 大语言模型驱动的编码智能体在实际部署中面临成本高且不可预测的问题，主要源于每轮迭代中token数量的二次增长、模型价格高昂、完成真实任务所需轮次多以及智能体常执行低效或不必要操作。现有研究多聚焦于单轮优化，而对整体轮次的战略控制缺乏探索。

Method: 在SWE-bench基准上，使用三个最先进的模型，评估三种轮次控制策略：无限制基线、带提醒的固定轮次限制（设为基线第75百分位）、以及一种新颖的按需动态扩展轮次的策略。

Result: 研究发现无限制设置下模型在性能、成本和轮次效率之间存在根本权衡；固定轮次限制（75百分位）可大幅降低成本（24%-68%）且对解决率影响很小；动态轮次策略表现最优，在保持或提升解决率的同时，比固定限制进一步节省12%-24%的成本。

Conclusion: 动态资源分配是一种优越且易于实现的方法，能有效平衡编码智能体的性能与成本，为开发者提供了简单而有效的部署指导。

Abstract: LLM-powered coding agents, which operate in iterative loops (turns) to solve
software engineering tasks, are becoming increasingly powerful. However, their
practical deployment is hindered by significant and unpredictable costs. This
challenge arises from a combination of factors: quadratically growing token
counts with each turn, the high price of models, the large number of turns
required for real-world tasks, and the tendency of agents to take inefficient
or unnecessary actions. While existing research focuses on optimizing
individual turns, the strategic control of the total number of turns remains an
underexplored area for managing agent performance and cost. To address this
gap, we conduct a comprehensive empirical study on SWE-bench using three
state-of-the-art models and evaluate the impact of three distinct turn-control
strategies: an unrestricted baseline, a fixed-turn limit with reminders, and a
novel dynamic-turn strategy that grants extensions on-demand. Our findings
first reveal a fundamental trade-off in the unrestricted setting, where no
single model excels across performance, cost, and turn efficiency. We then show
that a fixed-turn limit, specifically at the 75th percentile of the baseline,
serves as a "sweet spot", substantially reducing costs (by 24%-68%) with
minimal impact on solve rates. Most significantly, the dynamic-turn strategy
consistently outperforms fixed-limit approaches, achieving comparable or better
solve rates while further reducing costs by an additional 12%-24% by
intelligently allocating resources only to tasks that need them. This work
provides the first systematic analysis of turn-control strategies, offering
simple yet effective guidelines for developers to balance cost and efficacy. We
demonstrate that dynamic resource allocation is a superior, easy-to-implement
approach for deploying powerful yet economically viable coding agents.

</details>


### [31] [When Many-Shot Prompting Fails: An Empirical Study of LLM Code Translation](https://arxiv.org/abs/2510.16809)
*Amirkia Rafiei Oskooei,Kaan Baturalp Cosdan,Husamettin Isiktas,Mehmet S. Aktas*

Main category: cs.SE

TL;DR: 本文通过大规模实验发现，在代码翻译任务中，使用少量精心挑选的示例（5-25个）效果最佳，而增加示例数量反而会降低功能正确性，揭示了“多示例悖论”。


<details>
  <summary>Details</summary>
Motivation: 探究在上下文学习（ICL）中，增加示例数量是否总能提升大语言模型在代码翻译任务中的性能。

Method: 对超过90,000次代码翻译进行大规模实证研究，系统评估从零样本到多达625个示例（提示长度达80万token）的性能变化。

Result: 功能正确性在5-25个示例时达到峰值，更多示例反而导致性能下降；静态相似性指标虽略有提升，但无法反映实际功能表现。

Conclusion: 在代码翻译任务中，示例质量远胜于数量，“越多越好”的假设并不成立，最优提示策略具有任务依赖性。

Abstract: Large Language Models (LLMs) with vast context windows offer new avenues for
in-context learning (ICL), where providing many examples ("many-shot"
prompting) is often assumed to enhance performance. We investigate this
assumption for the complex task of code translation. Through a large-scale
empirical study of over 90,000 translations, we systematically evaluate the
impact of scaling in-context examples from zero-shot to many-shot
configurations of up to 625 examples, with prompts spanning from approximately
100,000 to 800,000 tokens. Our findings reveal a "many-shot paradox": while
static similarity metrics may modestly improve with more examples, functional
correctness consistently peaks with few-shot prompting (5-25 examples).
Providing substantially more examples often degrades this crucial functional
performance. This study highlights that for code translation, the quality of a
few well-chosen examples outweighs sheer quantity, challenging the universal
efficacy of "more is better" for ICL and underscoring the task-dependent nature
of optimal prompting strategies. Our results have significant implications for
effectively leveraging LLMs in software engineering.

</details>


### [32] [When AI Takes the Wheel: Security Analysis of Framework-Constrained Program Generation](https://arxiv.org/abs/2510.16823)
*Yue Liu,Zhenchang Xing,Shidong Pan,Chakkrit Tantithamthavorn*

Main category: cs.SE

TL;DR: 该研究发现，当前主流大语言模型（LLMs）在生成受框架约束的Chrome扩展程序时，存在严重的安全漏洞问题，漏洞率高达18%-50%，尤其在身份认证和Cookie管理场景中更为突出，且高级推理模型反而表现更差。


<details>
  <summary>Details</summary>
Motivation: 随着LLMs在软件开发中的广泛应用，开发者越来越依赖其生成复杂程序，但往往忽视了其中潜藏的安全问题，尤其是在具有复杂安全模型的框架（如Chrome扩展）中。因此，有必要系统评估LLMs生成代码的安全性。

Method: 构建包含140个提示的ChromeSecBench数据集，基于已知漏洞扩展；使用9个主流LLMs生成完整Chrome扩展；从场景类型、模型差异和漏洞类别三个维度分析生成代码中的安全漏洞。

Result: LLMs生成的程序漏洞率高达18%-50%，在Authentication & Identity和Cookie Management场景中分别高达83%和78%；多数漏洞导致敏感浏览器数据泄露；高级推理模型反而比简单模型产生更多漏洞。

Conclusion: 尽管LLMs具备强大的编码能力，但在编写安全的框架约束程序方面存在显著不足，亟需提升其对安全模型和权限边界的理解能力。

Abstract: In recent years, the AI wave has grown rapidly in software development. Even
novice developers can now design and generate complex framework-constrained
software systems based on their high-level requirements with the help of Large
Language Models (LLMs). However, when LLMs gradually "take the wheel" of
software development, developers may only check whether the program works. They
often miss security problems hidden in how the generated programs are
implemented.
  In this work, we investigate the security properties of framework-constrained
programs generated by state-of-the-art LLMs. We focus specifically on Chrome
extensions due to their complex security model involving multiple privilege
boundaries and isolated components. To achieve this, we built ChromeSecBench, a
dataset with 140 prompts based on known vulnerable extensions. We used these
prompts to instruct nine state-of-the-art LLMs to generate complete Chrome
extensions, and then analyzed them for vulnerabilities across three dimensions:
scenario types, model differences, and vulnerability categories. Our results
show that LLMs produced vulnerable programs at alarmingly high rates (18%-50%),
particularly in Authentication & Identity and Cookie Management scenarios (up
to 83% and 78% respectively). Most vulnerabilities exposed sensitive browser
data like cookies, history, or bookmarks to untrusted code. Interestingly, we
found that advanced reasoning models performed worse, generating more
vulnerabilities than simpler models. These findings highlight a critical gap
between LLMs' coding skills and their ability to write secure
framework-constrained programs.

</details>


### [33] [Will AI also replace inspectors? Investigating the potential of generative AIs in usability inspection](https://arxiv.org/abs/2510.17056)
*Luis F. G. Campos,Leonardo C. Marques,Walter T. Nakamura*

Main category: cs.SE

TL;DR: 本文研究了生成式AI（GPT-4o和Gemini 2.5 Flash）在可用性检查中的表现，发现其虽不能替代人类专家，但可作为有效辅助工具提升缺陷发现效率和覆盖范围。


<details>
  <summary>Details</summary>
Motivation: 可用性检查成本高且依赖专家知识，而人工智能的发展为提升该过程效率提供了新机会。

Method: 通过对比四位人类专家与两个AI模型（GPT-4o和Gemini 2.5 Flash）对同一软件原型的可用性问题检测结果，采用精确率、召回率和F1分数等指标进行评估。

Result: 人类专家在精确率和整体覆盖方面表现最佳；AI模型虽有较高个体表现并发现许多新缺陷，但存在较多误报和重复报告；人机结合效果最优。

Conclusion: 当前AI尚不能取代人类检查员，但可作为有价值的辅助工具，提升可用性检查的效率与缺陷覆盖范围。

Abstract: Usability inspection is a well-established technique for identifying
interaction issues in software interfaces, thereby contributing to improved
product quality. However, it is a costly process that requires time and
specialized knowledge from inspectors. With advances in Artificial Intelligence
(AI), new opportunities have emerged to support this task, particularly through
generative models capable of interpreting interfaces and performing inspections
more efficiently. This study examines the performance of generative AIs in
identifying usability problems, comparing them to those of experienced human
inspectors. A software prototype was evaluated by four specialists and two AI
models (GPT-4o and Gemini 2.5 Flash), using metrics such as precision, recall,
and F1-score. While inspectors achieved the highest levels of precision and
overall coverage, the AIs demonstrated high individual performance and
discovered many novel defects, but with a higher rate of false positives and
redundant reports. The combination of AIs and human inspectors produced the
best results, revealing their complementarity. These findings suggest that AI,
in its current stage, cannot replace human inspectors but can serve as a
valuable augmentation tool to improve efficiency and expand defect coverage.
The results provide evidence based on quantitative analysis to inform the
discussion on the role of AI in usability inspections, pointing to viable paths
for its complementary use in software quality assessment contexts.

</details>


### [34] [M2QCode: A Model-Driven Framework for Generating Multi-Platform Quantum Programs](https://arxiv.org/abs/2510.17110)
*Xiaoyu Guo,Shinobu Saito,Jianjun Zhao*

Main category: cs.SE

TL;DR: 本文提出了一种基于模型驱动开发（MDD）的方法，用于结构化设计和实现量子系统，并能自动生成多种量子编程语言的代码，提升开发效率与跨平台一致性。


<details>
  <summary>Details</summary>
Motivation: 尽管量子计算日益受到关注且已实现量子优越性，但模型驱动开发（MDD）在量子系统工程中的应用仍鲜有研究。

Method: 提出一个基于MDD的框架，支持量子系统的结构化设计，并能自动生成多种量子编程语言（QPLs）的代码。

Result: 通过多个案例研究验证了该方法的有效性和实用性。

Conclusion: 该MDD方法能有效提升量子系统开发的效率和跨平台一致性，为量子软件工程提供了新的可行路径。

Abstract: With the growing interest in quantum computing, the emergence of quantum
supremacy has marked a pivotal milestone in the field. As a result, numerous
quantum programming languages (QPLs) have been introduced to support the
development of quantum algorithms. However, the application of Model-Driven
Development (MDD) in quantum system engineering remains largely underexplored.
This paper presents an MDD-based approach to support the structured design and
implementation of quantum systems. Our framework enables the automatic
generation of quantum code for multiple QPLs, thereby enhancing development
efficiency and consistency across heterogeneous quantum platforms. The
effectiveness and practicality of our approach have been demonstrated through
multiple case studies.

</details>


### [35] [SEER: Enhancing Chain-of-Thought Code Generation through Self-Exploring Deep Reasoning](https://arxiv.org/abs/2510.17130)
*Shuzheng Gao,Chaozheng Wang,Cuiyun Gao,Michael R. Lyu*

Main category: cs.SE

TL;DR: 本文提出SEER框架，通过多样化推理路径探索、推理质量感知训练和自适应CoT推理，提升大语言模型在代码生成任务中的准确性和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有基于思维链（CoT）的代码生成方法存在三大局限：推理路径多样性不足、中间推理步骤缺乏质量评估、以及“过度思考”可能导致复杂且错误的解决方案。

Method: 提出SEER框架，包含三个核心组件：(1) 多样化推理路径探索，自动标注中间步骤；(2) 推理质量感知的模型训练，包括生成候选推理步骤的策略模型和评估其质量的价值模型；(3) 自适应CoT推理，根据问题动态选择直接生成或逐步推理。

Result: SEER有效提升了代码生成的准确性与适应性，缓解了现有方法在泛化性、可靠性及复杂性方面的不足。

Conclusion: 将CoT代码生成视为决策问题，并通过SEER框架实现更准确、可靠和高效的推理过程，为代码生成任务提供了新思路。

Abstract: Code generation, the task of creating executable programs from natural
language requirements, has recently seen tremendous advances through
Chain-of-Thought (CoT) reasoning, which enables Large Language Models (LLMs) to
develop high-level reasoning plans before writing code. Recent research has
proposed various methods to enhance models' CoT reasoning for code generation
such as prompt engineering and supervised fine-tuning. However, existing
approaches still face three critical limitations: (1) limited exploration of
diverse reasoning paths, which constrains generalization across various
programming scenarios, (2) lack of quality assessment for intermediate
reasoning steps, which hampers the reliability of the generated plans and code,
and (3) the potential negative impact of "overthinking", potentially leading to
unnecessarily complex and incorrect solutions. To address these limitations, we
frame CoT code generation as a decision making problem and present SEER, a
SElf-Exploring deep Reasoning framework that enables accurate and adaptive
reasoning for code generation. SEER introduces three key components: (1)
Diverse reasoning path exploration, which aims at exploring diverse reasoning
paths and annotating intermediate steps without relying on manual experts or
closed-source proprietary models; (2) Reasoning quality-aware model training,
which trains a policy model for generating candidate reasoning steps and a
value model for assessing their quality; and (3) Adaptive CoT reasoning, which
dynamically switches between direct generation and step-by-step reasoning for
different problems.

</details>


### [36] [PEACE: Towards Efficient Project-Level Efficiency Optimization via Hybrid Code Editing](https://arxiv.org/abs/2510.17142)
*Xiaoxue Ren,Jun Wan,Yun Peng,Zhongxin Liu,Ming Liang,Dajun Chen,Wei Jiang,Yong Li*

Main category: cs.SE

TL;DR: 本文提出了Peace，一个用于项目级代码效率优化的自动代码编辑混合框架，通过依赖感知的函数序列构建、有效关联编辑识别和效率优化迭代，在新构建的基准PeacExec上显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于大语言模型的代码效率优化方法仅关注函数级优化，忽略了函数间的交互，难以适用于真实开发场景；而代码编辑技术虽有项目级优化潜力，却面临无效编辑和次优内部函数等问题。

Method: Peace框架包含三个关键阶段：依赖感知的优化函数序列构建、有效关联编辑识别、效率优化编辑迭代，并在新构建的项目级基准PeacExec上进行评估。

Result: Peace在PeacExec基准上达到69.2%的正确率（pass@1），优化成功率提升46.9%，执行效率加速比达0.840，尤其在多函数复杂优化任务中显著优于现有基线。

Conclusion: Peace通过混合框架设计有效实现了项目级代码效率优化，在保证项目正确性和完整性的同时显著优于现有方法，验证了其各组件的有效性和整体设计的合理性。

Abstract: Large Language Models (LLMs) have demonstrated significant capability in code
generation, but their potential in code efficiency optimization remains
underexplored. Previous LLM-based code efficiency optimization approaches
exclusively focus on function-level optimization and overlook interaction
between functions, failing to generalize to real-world development scenarios.
Code editing techniques show great potential for conducting project-level
optimization, yet they face challenges associated with invalid edits and
suboptimal internal functions. To address these gaps, we propose Peace, a novel
hybrid framework for Project-level code Efficiency optimization through
Automatic Code Editing, which also ensures the overall correctness and
integrity of the project. Peace integrates three key phases: dependency-aware
optimizing function sequence construction, valid associated edits
identification, and efficiency optimization editing iteration. To rigorously
evaluate the effectiveness of Peace, we construct PeacExec, the first benchmark
comprising 146 real-world optimization tasks from 47 high-impact GitHub Python
projects, along with highly qualified test cases and executable environments.
Extensive experiments demonstrate Peace's superiority over the state-of-the-art
baselines, achieving a 69.2% correctness rate (pass@1), +46.9% opt rate, and
0.840 speedup in execution efficiency. Notably, our Peace outperforms all
baselines by significant margins, particularly in complex optimization tasks
with multiple functions. Moreover, extensive experiments are also conducted to
validate the contributions of each component in Peace, as well as the rationale
and effectiveness of our hybrid framework design.

</details>


### [37] [TREAT: A Code LLMs Trustworthiness / Reliability Evaluation and Testing Framework](https://arxiv.org/abs/2510.17163)
*Shuzheng Gao,Eric John Li,Man Ho Lam,Jingyu Xiao,Yuxuan Wan,Chaozheng Wang,Ng Man Tik,Michael R. Lyu*

Main category: cs.SE

TL;DR: 本文提出了TREAT评估框架，用于全面评估代码大模型在软件工程任务中的可信性与可靠性，涵盖多任务、多语言、多模态及鲁棒性等方面，并对26个前沿模型进行了系统评估。


<details>
  <summary>Details</summary>
Motivation: 现有代码大模型评估基准任务范围有限，缺乏对模型鲁棒性和可靠性的综合考量，难以反映其在真实软件工程场景中的可信表现。

Method: 提出TREAT评估框架，包含四大改进：(1) 多任务整体评估；(2) 多语言与多模态评估；(3) 鲁棒性评估（通过语义保持的代码变换）；(4) 严谨的评估方法（多样提示与自适应解提取）。

Result: 对26个先进模型的评估显示：(1) 模型在不同编程任务中性能差异显著；(2) 多模态语言模型在UI代码生成与编辑方面存在明显局限。

Conclusion: TREAT框架为代码大模型的可信性评估提供了更全面、可靠的方法，揭示了当前模型的优势与不足，为未来模型开发与评估提供了重要参考。

Abstract: Large foundation models are fundamentally transforming the software
engineering landscape, demonstrating exceptional capabilities across diverse
tasks such as code generation, debugging, and testing. Despite this rapid
progress, a significant gap remains in how to comprehensively evaluate these
models' trustworthiness in real-world software engineering scenarios. Existing
benchmarks suffer from limited task scope and fail to incorporate critical
evaluation aspects such as the robustness and reliability of models. To bridge
this gap, we present an evaluation framework called TREAT (Code LLMs
Trustworthiness / Reliability Evaluation And Testing) that provides a holistic
assessment of model performance in code intelligence tasks. Our evaluation
framework addresses key limitations in existing approaches with four main
improvements: (1) Multi-Task Holistic Evaluation that spans diverse software
engineering activities rather than limited coding tasks; (2) Multi-Language and
Multi-Modality Assessment that extends beyond traditional single-language,
text-only benchmarks to include multi-modality coding tasks; (3) Robustness
Assessment that evaluates model reliability under semantically-preserving code
transformations; and (4) Rigorous Evaluation Methodology that enhances the
trustworthiness of evaluation results through diverse evaluation prompts and
adaptive solution extraction. Based on this evaluation framework, we assess 26
state-of-the-art models and uncover both their strengths and limitations,
yielding several key insights:(1) Current models show substantial performance
variation across programming tasks; (2) Multi-modal language models demonstrate
specific performance limitations in UI code generation and edit;

</details>


### [38] [Software Testing with Large Language Models: An Interview Study with Practitioners](https://arxiv.org/abs/2510.17164)
*Maria Deolinda Santana,Cleyton Magalhaes,Ronnie de Souza Santos*

Main category: cs.SE

TL;DR: 本研究通过访谈15名软件测试人员，揭示了大语言模型（LLM）在软件测试中的实际应用模式，并提出了一套初步的、以实践者经验为基础的使用指南。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型在软件测试中的应用缺乏系统性指导，多依赖非正式实验，亟需基于从业者经验的结构化指南以支持其有效整合。

Method: 采用定性研究方法，对来自不同角色和领域的15名软件测试人员进行半结构化访谈，并基于扎根理论进行主题分析。

Result: 测试人员采用包含目标定义、提示工程、输出评估与持续学习的迭代流程，并强调人类监督和验证的重要性，以应对LLM的幻觉和推理不一致等问题。

Conclusion: LLM在软件测试中的应用正在增长，但实践仍处于演进阶段且需谨慎应对风险；本研究为结构化使用LLM提供了初步框架，并呼吁未来研究进一步完善相关实践。

Abstract: \textit{Background:} The use of large language models in software testing is
growing fast as they support numerous tasks, from test case generation to
automation, and documentation. However, their adoption often relies on informal
experimentation rather than structured guidance. \textit{Aims:} This study
investigates how software testing professionals use LLMs in practice to propose
a preliminary, practitioner-informed guideline to support their integration
into testing workflows. \textit{Method:} We conducted a qualitative study with
15 software testers from diverse roles and domains. Data were collected through
semi-structured interviews and analyzed using grounded theory-based processes
focused on thematic analysis. \textit{Results:} Testers described an iterative
and reflective process that included defining testing objectives, applying
prompt engineering strategies, refining prompts, evaluating outputs, and
learning over time. They emphasized the need for human oversight and careful
validation, especially due to known limitations of LLMs such as hallucinations
and inconsistent reasoning. \textit{Conclusions:} LLM adoption in software
testing is growing, but remains shaped by evolving practices and caution around
risks. This study offers a starting point for structuring LLM use in testing
contexts and invites future research to refine these practices across teams,
tools, and tasks.

</details>


### [39] [OLIVAW: ACIMOV's GitHub robot assisting agile collaborative ontology development](https://arxiv.org/abs/2510.17184)
*Nicolas Robert,Fabien Gandon,Maxime Lefrançois*

Main category: cs.SE

TL;DR: 本文提出了OLIVAW工具，基于ACIMOV方法论，利用GitHub和W3C标准支持模块化本体的敏捷开发与持续验证。


<details>
  <summary>Details</summary>
Motivation: 为支持用户驱动、持续更新并能随系统演化的本体设计，需要合适的持续验证工具来确保本体始终符合开发者需求。

Method: 开发了OLIVAW工具，结合ACIMOV方法论，在GitHub上通过Composite Actions、pre-commit hooks或命令行接口，利用W3C标准辅助模块化本体开发。

Result: OLIVAW在多个本体项目中进行了测试，验证了其有效性、通用性和可重用性，并提供了模板仓库以供快速启动。

Conclusion: OLIVAW为本体的敏捷协作开发提供了实用、通用且可复用的持续验证支持，有助于提升本体与系统需求的一致性。

Abstract: Agile and collaborative approaches to ontologies design are crucial because
they contribute to making them userdriven, up-to-date, and able to evolve
alongside the systems they support, hence proper continuous validation tooling
is required to ensure ontologies match developers' requirements all along their
development. We propose OLIVAW (Ontology Long-lived Integration Via ACIMOV
Workflow), a tool supporting the ACIMOV methodology on GitHub. It relies on W3C
Standards to assist the development of modular ontologies through GitHub
Composite Actions, pre-commit hooks, or a command line interface. OLIVAW was
tested on several ontology projects to ensure its usefulness, genericity and
reusability. A template repository is available for a quick start. OLIVAW is

</details>


### [40] [AdapTrack: Constrained Decoding without Distorting LLM's Output Intent](https://arxiv.org/abs/2510.17376)
*Yongmin Li,Jia Li,Ge Li,Zhi Jin*

Main category: cs.SE

TL;DR: AdapTrack 是一种结合回溯机制的代码生成方法，通过避免约束解码对模型输出意图的扭曲，在满足语法和 API 等约束的同时，显著提升生成代码的语义正确性和与模型原始意图的一致性。


<details>
  <summary>Details</summary>
Motivation: 现有约束解码方法在强制满足约束（如语法正确性、API 存在性）时会扭曲语言模型的原始输出意图，导致生成的代码虽符合约束但语义上不符合开发需求。

Method: 提出 AdapTrack 方法，在生成过程中引入回溯机制，使得模型在满足约束的同时保留其原始输出意图；并通过理论证明该方法生成的分布与模型原始分布一致。

Result: 在合成和真实 API 补全数据集上，AdapTrack 相比约束解码分别提升 360.87% 和 38.93%；在 HumanEval 和 MBPP 基准上分别提升 7.84% 和 6.42%；在 DSL 任务中也表现出更强的一致性。

Conclusion: AdapTrack 通过回溯机制有效缓解了约束解码对模型意图的扭曲问题，在多种代码生成任务中显著优于传统约束解码方法，同时理论和实验均验证了其对模型原始分布的保真性。

Abstract: Language model-based code generation and completion tools have been widely
adopted, but they may sometimes produce code that does not meet necessary
constraints, such as syntactic correctness or API existence. Constrained
decoding techniques are developed to help the model generate code adhering to
the constraints by greedily eliminating generation options that violate
constraints at each step of the generation process. However, there is a severe
limitation of constrained decoding, that it distorts the model's output intent,
forcing it to produce code that may satisfy the constraint but does not match
the development intent and is therefore incorrect. In response to this
challenge, we propose AdapTrack. By incorporating backtracking into the
generation process, AdapTrack avoids distorting the output intent of the model,
thereby producing results that are not only constraint-compliant but also more
semantically aligned with model's output intent. On our synthetic API
completion dataset, AdapTrack can achieve up to 360.87% improvement compared to
constrained decoding; on the real-world API completion dataset we collect that
exhibits similar issues, AdapTrack can achieve up to 38.93% improvement over
constrained decoding; in general code genration benchmarks, compared to
constrained decoding, AdapTrack can achieve up to 7.84% improvement on
HumanEval, and up to 6.42% improvement on MBPP. This indicates that, simply by
better adhering to the model's output intent, AdapTrack can achieve significant
improvements. We provide a theoretical proof that the distribution produced by
AdapTrack aligns with the model's distribution given the generated tokens,
thereby ensuring that the model's output intent is not distorted. Experiments
on DSL problems show that, compared to existing methods, our approach can
provide generation results that are more consistent with the language model's
distribution.

</details>


### [41] [Scalable CI/CD for Legacy Modernization: An Industrial Experience Addressing Internal Challenges Related to the 2025 Japan Cliff](https://arxiv.org/abs/2510.17430)
*Kuniaki Kudo,Sherine Devi*

Main category: cs.SE

TL;DR: 本文针对日本2025年“IT悬崖”问题，提出并实现了一种可扩展的CI/CD流水线，通过动态创建隔离开发环境，结合GitHub、Jenkins、AWS和Docker等技术，有效降低老旧系统维护成本并推动数字化转型。


<details>
  <summary>Details</summary>
Motivation: 日本大量核心IT系统即将在2025年达到服务寿命终点，导致维护成本激增、系统难以更新或替换，阻碍数字化转型；Asahi公司内部也面临类似问题，包括手动维护流程、缺乏QA环境及中间件/操作系统长期未更新。

Method: 构建并实施一种可扩展的CI/CD流水线，利用GitHub进行代码管理、Jenkins实现自动化、AWS提供可扩展基础设施、Docker实现环境容器化，支持动态创建和销毁隔离的开发测试环境。

Result: 开发者可在独立环境中安全地测试维护流程和尝试新技术，显著降低维护成本，并有效推动企业数字化转型（DX）。

Conclusion: 所提出的可扩展CI/CD流水线为应对2025年日本IT悬崖问题提供了一种可行且高效的解决方案，有助于老旧IT系统的现代化和持续演进。

Abstract: We have developed a Scalable CI/CD Pipeline to address internal challenges
related to Japan 2025 cliff problem, a critical issue where the mass end of
service life of legacy core IT systems threatens to significantly increase the
maintenance cost and black box nature of these system also leads to difficult
update moreover replace, which leads to lack of progress in Digital
Transformation (DX). If not addressed, Japan could potentially lose up to 12
trillion yen per year after 2025, which is 3 times more than the cost in
previous years. Asahi also faced the same internal challenges regarding legacy
system, where manual maintenance workflows and limited QA environment have left
critical systems outdated and difficult to update. Middleware and OS version
have remained unchanged for years, leading to now its nearing end of service
life which require huge maintenance cost and effort to continue its operation.
To address this problem, we have developed and implemented a Scalable CI/CD
Pipeline where isolated development environments can be created and deleted
dynamically and is scalable as needed. This Scalable CI/CD Pipeline incorporate
GitHub for source code control and branching, Jenkins for pipeline automation,
Amazon Web Services for scalable environment, and Docker for environment
containerization. This paper presents the design and architecture of the
Scalable CI/CD Pipeline, with the implementation along with some use cases.
Through Scalable CI/CD, developers can freely and safely test maintenance
procedures and do experiments with new technology in their own environment,
reducing maintenance cost and drive Digital Transformation (DX).
  key words: 2025 Japan Cliff, Scalable CI/CD, DevOps, Legacy IT Modernization.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [42] [Multimodal Chip Physical Design Engineer Assistant](https://arxiv.org/abs/2510.15872)
*Yun-Da Tsai,Chang-Yu Chao,Liang-Yeh Shen,Tsung-Han Lin,Haoyu Yang,Mark Ho,Yi-Chen Lu,Wen-Hao Liu,Shou-De Lin,Haoxing Ren*

Main category: cs.AR

TL;DR: 本文提出了一种多模态大语言模型助手（MLLMA），用于预测芯片物理设计中的布线拥塞并提供可解释的设计建议，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有电子设计自动化（EDA）工具在布线拥塞问题上缺乏可解释的反馈和可操作的优化指导，限制了工程师的决策效率。

Method: 结合多模态大语言模型引导的遗传提示进行自动特征生成，并构建一个可解释的偏好学习框架，整合视觉、表格和文本输入，生成“设计建议卡”以突出关键布局特征并提出针对性优化。

Result: 在CircuitNet基准测试中，该方法在准确性和可解释性方面均优于现有模型；案例研究和定性分析表明其建议符合实际设计原则且对工程师具有可操作性。

Conclusion: 多模态大语言模型有潜力作为交互式助手，实现可解释、上下文感知的物理设计优化。

Abstract: Modern chip physical design relies heavily on Electronic Design Automation
(EDA) tools, which often struggle to provide interpretable feedback or
actionable guidance for improving routing congestion. In this work, we
introduce a Multimodal Large Language Model Assistant (MLLMA) that bridges this
gap by not only predicting congestion but also delivering human-interpretable
design suggestions. Our method combines automated feature generation through
MLLM-guided genetic prompting with an interpretable preference learning
framework that models congestion-relevant tradeoffs across visual, tabular, and
textual inputs. We compile these insights into a "Design Suggestion Deck" that
surfaces the most influential layout features and proposes targeted
optimizations. Experiments on the CircuitNet benchmark demonstrate that our
approach outperforms existing models on both accuracy and explainability.
Additionally, our design suggestion guidance case study and qualitative
analyses confirm that the learned preferences align with real-world design
principles and are actionable for engineers. This work highlights the potential
of MLLMs as interactive assistants for interpretable and context-aware physical
design optimization.

</details>


### [43] [Putting the Context back into Memory](https://arxiv.org/abs/2510.15878)
*David A. Roberts*

Main category: cs.AR

TL;DR: 本文提出一种在内存读地址流中无损编码用户可见程序状态的方法，使内存设备能够获取程序上下文信息，从而支持更精细的内存优化。


<details>
  <summary>Details</summary>
Motivation: 由于硬件预取、调度和交织等因素，主存请求与程序员观察到的行为存在差异，导致程序上下文信息在内存总线上丢失，限制了数据移动和分层优化的潜力。

Method: 通过在内存读地址流中以非破坏性方式嵌入用户可见状态作为可检测的数据包，无需额外驱动或特权，实现程序上下文在内存设备端的可见性，并构建端到端原型系统进行验证。

Result: 原型系统能够从内存地址轨迹中可靠地检测和解码元数据，展示了精确的代码执行标记和对象地址范围跟踪等应用场景。

Conclusion: 该方法为近内存计算提供了获取定制遥测数据和响应应用提示的新途径，未来可用于请求优先级排序、数据重映射和设备重配置等优化。

Abstract: Requests arriving at main memory are often different from what programmers
can observe or estimate by using CPU-based monitoring. Hardware cache
prefetching, memory request scheduling and interleaving cause a loss of
observability that limits potential data movement and tiering optimizations. In
response, memory-side telemetry hardware like page access heat map units (HMU)
and page prefetchers were proposed to inform Operating Systems with accurate
usage data. However, it is still hard to map memory activity to software
program functions and objects because of the decoupled nature of host
processors and memory devices. Valuable program context is stripped out from
the memory bus, leaving only commands, addresses and data. Programmers have
expert knowledge of future data accesses, priorities, and access to processor
state, which could be useful hints for runtime memory device optimization. This
paper makes context visible at memory devices by encoding any user-visible
state as detectable packets in the memory read address stream, in a
nondestructive manner without significant capacity overhead, drivers or special
access privileges. We prototyped an end-to-end system with metadata injection
that can be reliably detected and decoded from a memory address trace, either
by a host processor, or a memory module. We illustrate a use case with precise
code execution markers and object address range tracking. In the future, real
time metadata decoding with near-memory computing (NMC) could provide
customized telemetry and statistics to users, or act on application hints to
perform functions like prioritizing requests, remapping data and reconfiguring
devices.

</details>


### [44] [Opportunities and Challenges for 3D Systems and Their Design](https://arxiv.org/abs/2510.15880)
*Philip Emma,Eren Kurshan*

Main category: cs.AR

TL;DR: 本文探讨了3D集成技术在延续摩尔定律、提升集成密度方面的潜力，同时系统分析了其在功耗密度、跨层协同设计、可测性及制造良率等方面带来的新挑战。


<details>
  <summary>Details</summary>
Motivation: 随着光刻工艺缩放难度加大和微型通孔制造能力提升，3D集成技术重新受到广泛关注；然而，其高集成密度也带来了功耗、设计协同、测试和制造等方面的新问题，亟需系统性研究。

Method: 本文通过综述和分析现有研究，梳理3D集成系统的优势，并系统阐述其在设计、组装和测试阶段所面临的关键挑战。

Result: 文章明确了3D集成在提升密度方面的优势，并识别出包括跨层协同设计、各层独立可测性、组装良率控制以及成品测试等核心挑战。

Conclusion: 要充分发挥3D集成的潜力，必须全面理解其优势，并系统应对设计、制造与测试中的新挑战，推动相关标准与方法的发展。

Abstract: Although it is not a new concept, 3D integration increasingly receives
widespread interest and focus as lithographic scaling becomes more challenging,
and as the ability to make miniature vias greatly improves. Like Moores law, 3D
integration improves density. With improvements in packaging density, however,
come the challenges associated with its inherently higher power density. And
though it acts somewhat as a scaling accelerator, the vertical integration also
poses new challenges to design and manufacturing technologies. The placement of
circuits, vias, and macros in the planes of a 3D stack must be co-designed
across layers (or must conform to new standards) so that, when assembled, they
have correct spatial correspondence. Each layer, although perhaps being a mere
functional slice through a system (and we can slice the system in many
different ways), must be independently testable so that we can systematically
test and diagnose subsystems before and after final assembly. When those layers
are assembled, they must come together in a way that enables a sensible yield
and facilitates testing the finished product. To make the most of 3D
integration, we should articulate the leverages of 3D systems (other
researchers offer a more complete treatment elsewhere). Then we can enumerate
and elucidate many of the new challenges posed by the design, assembly, and
test of 3D systems.

</details>


### [45] [FlexLink: Boosting your NVLink Bandwidth by 27% without accuracy concern](https://arxiv.org/abs/2510.15882)
*Ao Shen,Rui Zhang,Junping Zhao*

Main category: cs.AR

TL;DR: FlexLink 是一种新型集体通信框架，通过聚合 NVLink、PCIe 和 RDMA NIC 等异构链路，动态分配通信流量，在 H800 GPU 上将 AllReduce 和 AllGather 的带宽分别提升最多 26% 和 27%，且兼容 NCCL API，可无损替换。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型规模扩大，多节点部署成为必需，通信成为性能瓶颈。现有通信库（如 NCCL）仅使用单一互连（如 NVLink），在 H800 等硬件上造成带宽瓶颈，同时 PCIe 和 RDMA NIC 等资源未被充分利用。

Method: 提出 FlexLink 框架，通过两阶段自适应负载均衡策略，动态将通信流量分配到所有可用链路（NVLink、PCIe、RDMA NIC），避免高速链路被低速链路拖累，并作为 NCCL API 的无损替代方案实现。

Result: 在 8-GPU H800 服务器上，FlexLink 相比 NCCL 基线将 AllReduce 和 AllGather 的带宽分别提升最多 26% 和 27%，并将 2–22% 的通信流量卸载至原先未充分利用的 PCIe 和 RDMA NIC。

Conclusion: FlexLink 有效利用异构互连资源，显著提升多 GPU 系统中的集体通信性能，且具备良好的兼容性和易部署性，为大规模 LLM 训练提供了实用的通信优化方案。

Abstract: As large language models (LLMs) continue to scale, multi-node deployment has
become a necessity. Consequently, communication has become a critical
performance bottleneck. Current intra-node communication libraries, like NCCL,
typically make use of a single interconnect such as NVLink. This approach
creates performance ceilings, especially on hardware like the H800 GPU where
the primary interconnect's bandwidth can become a bottleneck, and leaves other
hardware resources like PCIe and Remote Direct Memory Access (RDMA)-capable
Network Interface Cards (NICs) largely idle during intensive workloads. We
propose FlexLink, the first collective communication framework to the best of
our knowledge designed to systematically address this by aggregating these
heterogeneous links-NVLink, PCIe, and RDMA NICs-into a single, high-performance
communication fabric. FlexLink employs an effective two-stage adaptive load
balancing strategy that dynamically partitions communication traffic across all
available links, ensuring that faster interconnects are not throttled by slower
ones. On an 8-GPU H800 server, our design improves the bandwidth of collective
operators such as AllReduce and AllGather by up to 26% and 27% over the NCCL
baseline, respectively. This gain is achieved by offloading 2-22% of the total
communication traffic to the previously underutilized PCIe and RDMA NICs.
FlexLink provides these improvements as a lossless, drop-in replacement
compatible with the NCCL API, ensuring easy adoption.

</details>


### [46] [Generalized Methodology for Determining Numerical Features of Hardware Floating-Point Matrix Multipliers: Part I](https://arxiv.org/abs/2510.15884)
*Faizan A Khattak,Mantas Mikaitis*

Main category: cs.AR

TL;DR: 本文提出了一种架构无关的测试方案，用于分析消费级NVIDIA GPU中矩阵乘法器的数值特性（如舍入、归一化和累加器内部精度），适用于多种混合精度格式，并验证了RTX-3060与数据中心GPU A100在数值特性上的一致性。


<details>
  <summary>Details</summary>
Motivation: 当前对GPU矩阵乘法器数值特性的研究主要集中在数据中心级硬件，缺乏适用于消费级GPU且不依赖设备特定常量或穷举搜索的通用分析方法。

Method: 提出一种架构无关的测试向量生成方法，不进行穷举搜索，也不依赖硬编码的设备特定常量，可适用于多种输入/输出精度格式，包括binary16、TensorFloat32、bfloat16等。

Result: 成功应用于RTX-3060（Ampere）和Ada RTX-1000（Ada Lovelace）显卡，发现RTX-3060的矩阵乘法器数值特性与A100数据中心GPU完全一致。

Conclusion: 所提方法具有良好的通用性和前瞻性，无需修改即可用于分析未来NVIDIA GPU（如Hopper、Blackwell）及新型8位浮点格式的矩阵乘法器数值特性。

Abstract: Numerical features of matrix multiplier hardware units in NVIDIA and AMD data
centre GPUs have recently been studied. Features such as rounding,
normalisation, and internal precision of the accumulators are of interest. In
this paper, we extend the methodology for analysing those features, to
consumer-grade NVIDIA GPUs by implementing an architecture-independent test
scheme for various input and output precision formats. Unlike current
approaches, the proposed test vector generation method neither performs an
exhaustive search nor relies on hard-coded {constants that are device-specific,
yet remains applicable to a wide range of mixed-precision formats. We have
applied the scheme to the RTX-3060 (Ampere architecture), and Ada RTX-1000 (Ada
Lovelace architecture) graphics cards and determined numerical features of
matrix multipliers for binary16, TensorFloat32, and bfloat16 input floating
point formats and binary16 and binary32 IEEE 754 output formats. Our
methodology allowed us to determine that} the numerical features of RTX-3060, a
consumer-grade GPU, are identical to those of the A100, a data centre GPU. We
do not expect our code to require any changes for performing analysis of matrix
multipliers on newer NVIDIA GPUs, Hopper or Blackwell, and their future
successors, and any input/output format combination, including the latest 8-bit
floating-point formats.

</details>


### [47] [ConZone+: Practical Zoned Flash Storage Emulation for Consumer Devices](https://arxiv.org/abs/2510.15885)
*Dingcui Yu,Zonghuan Yan,Jialin Liu,Yumiao Zhao,Yanyun Wang,Xinghui Duan,Yina Lv,Liang Shi*

Main category: cs.AR

TL;DR: ConZone+ 是一个面向消费级分区域闪存存储的增强型模拟器，通过引入块接口支持文件系统挂载，提升了原始 ConZone 的可用性，并可用于研究存储架构与文件系统适配问题。


<details>
  <summary>Details</summary>
Motivation: 现有 ConZone 模拟器因缺乏原地更新能力，无法支持依赖该特性的文件系统（如 F2FS）挂载，限制了其在系统软件协同优化中的应用。

Method: 在 ConZone 基础上扩展块接口支持，提供部署脚本并引入多项改进，使模拟器能与文件系统集成，同时保留对消费级分区域闪存关键特性的建模能力。

Result: 通过与真实硬件及现有先进方案对比验证了 ConZone+ 的准确性，并利用其开展多个案例研究，揭示了当前文件系统在分区域存储设计中的不足。

Conclusion: ConZone+ 有效提升了消费级分区域闪存模拟器的实用性，为软硬件协同优化和文件系统适配研究提供了有力工具。

Abstract: To facilitate the understanding and efficient enhancement of software and
hardware design for consumer-grade zoned flash storage, ConZone is proposed as
the first emulator designed to model the resource constraints and architectural
features typical of such systems. It incorporates essential components commonly
deployed in consumer-grade devices, including limited logical to physical
mapping caches, constrained write buffers, and hybrid flash media management.
However, ConZone cannot be mounted with the file system due to the lack of
in-place update capability, which is required by the metadata area of F2FS. To
improve the usability of the emulator, ConZone+ extends ConZone with support
for a block interface. We also provide a script to help the deployment and
introduces several enhancements over the original version. Users can explore
the internal architecture of consumer-grade zoned flash storage and integrate
their optimizations with system software using ConZone+. We validate the
accuracy of ConZone+ by comparing a hardware architecture representative of
consumer-grade zoned flash storage and comparing it with the state-of-the-art.
In addition, we conduct several case studies using ConZone+ to investigate the
design of zoned storage and explore the inadequacies of the current file
system.

</details>


### [48] [UPMEM Unleashed: Software Secrets for Speed](https://arxiv.org/abs/2510.15927)
*Krystian Chmielewski,Jarosław Ławnicki,Uladzislau Lukyanau,Tadeusz Kobus,Maciej Maciejewski*

Main category: cs.AR

TL;DR: 本文通过非标准编程技术优化UPMEM PIM平台的软件栈，在整数运算、低精度位串行处理及内存分配等方面显著提升性能，使INT8和INT4的GEMV分别比CPU快3倍和10倍。


<details>
  <summary>Details</summary>
Motivation: PIM平台（如UPMEM）虽有SDK支持，但在数据管理和并行编程方面仍存在性能瓶颈，亟需优化。

Method: 对UPMEM编译器生成的汇编代码进行简单修改；采用位串行处理低精度数据；扩展API以适配服务器的NUMA架构。

Result: 整数加法提速1.6–2倍，乘法提速1.4–5.9倍；INT4位串行点积提速2.7倍以上；主机-PIM数据传输吞吐提升达2.9倍；预加载矩阵下，INT8和INT4 GEMV分别比双路CPU快3倍和10倍。

Conclusion: 通过软硬件协同的非传统优化手段，可显著释放UPMEM PIM平台的计算潜力，尤其在低精度计算场景中优势明显。

Abstract: Developing kernels for Processing-In-Memory (PIM) platforms poses unique
challenges in data management and parallel programming on limited processing
units. Although software development kits (SDKs) for PIM, such as the UPMEM
SDK, provide essential tools, these emerging platforms still leave significant
room for performance optimization. In this paper, we reveal surprising
inefficiencies in UPMEM software stack and play with non-standard programming
techniques. By making simple modifications to the assembly generated by the
UPMEM compiler, we achieve speedups of 1.6-2x in integer addition and 1.4-5.9x
in integer multiplication, depending on the data type. We also demonstrate that
bit-serial processing of low precision data is a viable option for UPMEM: in
INT4 bit-serial dot-product calculation, UPMEM can achieve over 2.7x speedup
over the baseline. Minor API extensions for PIM allocation that account for the
non-uniform memory access (NUMA) architecture of the server further improve the
consistency and throughput of host-PIM data transfers by up to 2.9x. Finally,
we show that, when the matrix is preloaded into PIM, our optimized kernels
outperform a dual-socket CPU server by over 3x for INT8 generalized
matrix-vector multiplication (GEMV) and by 10x for INT4 GEMV. Our optimized
INT8 GEMV kernel outperforms the baseline 3.5x.

</details>


### [49] [basic_RV32s: An Open-Source Microarchitectural Roadmap for RISC-V RV32I](https://arxiv.org/abs/2510.15887)
*Hyun Woo Kang,Ji Woong Choi*

Main category: cs.AR

TL;DR: 本文提出了BASIC_RV32s，一个开源的RISC-V RV32I微架构实现框架，从单周期核心逐步演进为具备完整流水线、前递、动态分支预测和异常处理的五级流水线核心，并在FPGA上验证，同时开源全部RTL代码与文档。


<details>
  <summary>Details</summary>
Motivation: 弥合理论知识与RISC-V硬件实现之间的差距，为开源硬件社区提供可复现的教学路径。

Method: 基于Patterson和Hennessy的经典方法，从单周期核心逐步构建五级流水线核心，包含前递、动态分支预测和异常处理；最终集成到SoC并在Xilinx Artix-7 FPGA上通过UART进行验证。

Result: 在50 MHz频率下实现1.09 DMIPS/MHz性能，并在GitHub上以MIT许可证开源全部RTL代码、逻辑框图和开发日志。

Conclusion: BASIC_RV32s为RISC-V教学和开源硬件开发提供了一个完整、可复现且实用的微架构实现范例。

Abstract: This paper introduces BASIC_RV32s, an open-source framework providing a
practical microarchitectural roadmap for the RISC-V RV32I architecture,
addressing the gap between theoretical knowledge and hardware implementation.
Following the classic Patterson and Hennessy methodology, the design evolves
from a basic single-cycle core to a 5-stage pipelined core design with full
hazard forwarding, dynamic branch prediction, and exception handling. For
verification, the final core design is integrated into a System-on-Chip (SoC)
with Universal Asynchronous Receiver-Transmitter (UART) communication
implemented on a Xilinx Artix-7 Field-Programmable Gate Array (FPGA), achieving
1.09 Dhrystone million instructions per second per megahertz (DMIPS/MHz) at 50
MHz. By releasing all Register-Transfer Level (RTL) source code, signal-level
logic block diagrams, and development logs under MIT license on GitHub,
BASIC_RV32s offers a reproducible instructional pathway for the open-source
hardware ecosystem.

</details>


### [50] [Limited Read-Write/Set Hardware Transactional Memory without modifying the ISA or the Coherence Protocol](https://arxiv.org/abs/2510.15888)
*Konstantinos Kafousis*

Main category: cs.AR

TL;DR: 该论文提出了一种无需新增指令、不修改缓存一致性协议的硬件事务内存（HTM）实现方法，仅通过扩展现有Load-Linked/Store-Conditional指令语义，并将实现限制在L1数据缓存中，适用于读写集不超过少量缓存行的场景。实验表明该方法在低争用和多节点分散争用情况下性能良好。


<details>
  <summary>Details</summary>
Motivation: 现有HTM实现因硬件复杂度高、需扩展指令集架构（ISA）及修改缓存一致性协议而难以广泛应用，作者旨在设计一种更简单、兼容性更强的HTM方案。

Method: 通过扩展现有Load-Linked和Store-Conditional指令的语义实现HTM，不引入新指令；不修改标准缓存一致性协议；将HTM实现限制在L1数据缓存中，并限定事务的读写集不超过少量缓存行；提出两种基于重试检测的前向进展保障机制。

Result: 在Gem5中模拟实现，用于多个并发数据结构，结果表明读写集最多只需8个缓存行；在多节点分散争用下事务中止率低；在低拥塞的原子fetch-and-increment基准测试中，性能优于TTS锁。

Conclusion: 所提出的HTM设计在保持硬件简洁性和兼容性的同时，在典型并发数据结构中展现出良好的性能和可扩展性，尤其适用于读写集较小且争用分散的场景。

Abstract: Hardware Transactional Memory (HTM) allows lock-free programming as easy as
with traditional coarse-grain locks or similar, while benefiting from the
performance advantages of fine-grained locking. Many HTM implementations have
been proposed, but they have not received widespread adoption because of their
high hardware complexity, their need for additions to the Instruction Set
Architecture (ISA), and often for modifications to the cache coherence
protocol.
  We show that HTM can be implemented without adding new instructions -- merely
by extending the semantics of two existing, Load-Linked and Store-Conditional.
Also, our proposed design does not modify or extend standard coherence
protocols. We further propose to drastically simplify the implementation of HTM
-- confined to modifications in the L1 Data Cache only -- by restricting it to
applications where the write set plus the read set of each transaction do not
exceed a small number of cache lines. We also propose two alternative
mechanisms to guarantee forward progress, both based on detecting retrial
attempts.
  We simulated our proposed design in Gem5, and we used it to implement several
popular concurrent data structures, showing that a maximum of eight (8) words
(cache lines) suffice for the write plus read sets. We provide a detailed
explanation of selected implementations, clarifying the intended usage of our
HTM from a programmer's perspective. We evaluated our HTM under varying
contention levels to explore its scalability limits. The results indicate that
our HTM provides good performance in concurrent data structures when contention
is spread across multiple nodes: in such cases, the percentage of aborts
relative to successful commits is very low. In the atomic fetch-and-increment
benchmark for multiple shared counters, the results show that, under
low-congestion, our HTM improves performance relative to the TTS lock.

</details>


### [51] [Accelerating Frontier MoE Training with 3D Integrated Optics](https://arxiv.org/abs/2510.15893)
*Mikhail Bernadskiy,Peter Carson,Thomas Graham,Taylor Groves,Ho John Lee,Eric Yeh*

Main category: cs.AR

TL;DR: 本文探讨了在AI工作负载持续增长的背景下，利用3D共封装光学（CPO）技术突破传统电互连限制，实现跨机架GPU大规模互联，从而显著提升大模型训练效率。


<details>
  <summary>Details</summary>
Motivation: 传统半导体工艺缩放放缓，而AI工作负载对算力、内存和互连性能的需求不断增长，亟需新的互连技术来扩展GPU逻辑规模，尤其是在训练超大规模模型（如万亿参数MoE模型）时。

Method: 分析铜互连与3D堆叠光互连（3D CPO）在扩展性、功耗和带宽等方面的权衡，并对基于3D CPO的GPU和交换机在训练前沿MoE模型中的性能进行建模与评估。

Result: 3D CPO技术可将扩展能力提升8倍，支持多维并行，使训练时间减少2.7倍，显著提升模型扩展能力。

Conclusion: 3D共封装光学互连是满足前沿大模型严苛性能与功耗目标的关键技术，能有效突破单机架限制，实现跨机架大规模GPU互联，大幅提升训练效率。

Abstract: The unabated growth in AI workload demands is driving the need for concerted
advances in compute, memory, and interconnect performance. As traditional
semiconductor scaling slows, high-speed interconnects have emerged as the new
scaling engine, enabling the creation of larger logical GPUs by linking many
GPUs into a single, low-latency, high-bandwidth compute domain. While initial
scale-up fabrics leveraged copper interconnects for their power and cost
advantages, the maximum reach of passive electrical interconnects
(approximately 1 meter) effectively limits the scale-up domain to within a
single rack. The advent of 3D-stacked optics and logic offers a transformative,
power-efficient scale-up solution for connecting hundreds of GPU packages
(thousands of GPUs) across multiple data center racks. This work explores the
design tradeoffs of scale-up technologies and demonstrates how frontier LLMs
necessitate novel photonic solutions to achieve aggressive power and
performance targets. We model the benefits of 3D CPO (Passage) enabled GPUs and
switches within the scale-up domain when training Frontier Mixture of Experts
(MoE) models exceeding one trillion parameters. Our results show that the
substantial increases in bandwidth and radix enabled by 3D CPO allow for an 8X
increase in scale-up capability. This affords new opportunities for
multi-dimensional parallelism within the scale-up domain and results in a 2.7X
reduction in time-to-train, unlocking unprecedented model scaling.

</details>


### [52] [DiffPlace: A Conditional Diffusion Framework for Simultaneous VLSI Placement Beyond Sequential Paradigms](https://arxiv.org/abs/2510.15897)
*Kien Le Trung,Truong-Son Hy*

Main category: cs.AR

TL;DR: DiffPlace 是一种基于条件去噪扩散过程的芯片布局新方法，无需针对每个新电路重新训练，即可在未见过的电路网表上实现高质量、合法的布局。


<details>
  <summary>Details</summary>
Motivation: 传统芯片布局方法在处理硬性布局约束时存在困难，或需为每个新电路设计进行昂贵的在线训练，难以高效泛化。

Method: 将芯片布局建模为条件去噪扩散过程，利用扩散模型的生成能力，在考虑电路连接性和相对质量指标的条件下全局探索布局空间，并结合能量引导采样与约束流形扩散以确保布局合法性。

Result: 在所有实验场景中均实现了极低的模块重叠，表明该方法能有效生成高质量且合法的芯片布局。

Conclusion: DiffPlace 成功融合了基于优化与基于学习的方法优势，为现代 VLSI 设计提供了一条实用的自动化高质量芯片布局路径。

Abstract: Chip placement, the task of determining optimal positions of circuit modules
on a chip canvas, is a critical step in the VLSI design flow that directly
impacts performance, power consumption, and routability. Traditional methods
rely on analytical optimization or reinforcement learning, which struggle with
hard placement constraints or require expensive online training for each new
circuit design. To address these limitations, we introduce DiffPlace, a
framework that formulates chip placement as a conditional denoising diffusion
process, enabling transferable placement policies that generalize to unseen
circuit netlists without retraining. DiffPlace leverages the generative
capabilities of diffusion models to efficiently explore the vast space of
placement while conditioning on circuit connectivity and relative quality
metrics to identify optimal solutions globally. Our approach combines
energy-guided sampling with constrained manifold diffusion to ensure placement
legality, achieving extremely low overlap across all experimental scenarios.
Our method bridges the gap between optimization-based and learning-based
approaches, offering a practical path toward automated, high-quality chip
placement for modern VLSI design. Our source code is publicly available at:
https://github.com/HySonLab/DiffPlace/

</details>


### [53] [LLM-VeriPPA: Power, Performance, and Area Optimization aware Verilog Code Generation with Large Language Models](https://arxiv.org/abs/2510.15899)
*Kiran Thorat,Jiahui Zhao,Yaotian Liu,Amit Hasan,Hongwu Peng,Xi Xie,Bin Lei,Caiwen Ding*

Main category: cs.AR

TL;DR: 本文提出了一种名为VeriPPA的新框架，利用大语言模型（LLM）在芯片设计中优化功耗-性能-面积（PPA）并生成准确的Verilog代码。该框架采用两阶段方法，分别提升Verilog代码的功能与语法正确性，并优化PPA。实验表明，VeriPPA在多个数据集上均优于当前最先进的方法。


<details>
  <summary>Details</summary>
Motivation: 芯片设计中的功耗-性能-面积（PPA）优化和高质量Verilog代码生成是复杂且耗时的任务。现有方法在功能与语法正确性方面仍有不足。本文旨在探索大语言模型（LLM）在这一高技术门槛领域的应用潜力，提升自动化芯片设计的效率与准确性。

Method: 提出VeriPPA框架，采用两阶段流程：第一阶段提升生成Verilog代码的语法与功能正确性；第二阶段对代码进行PPA优化以满足电路设计约束。该方法基于大语言模型实现，并在RTLLM和VerilogEval数据集上进行评估。

Result: 在RTLLM数据集上，VeriPPA实现了81.37%的语法正确率和62.06%的功能正确率；在VerilogEval数据集上，语法正确率达99.56%，功能正确率为43.79%，均优于当前SOTA方法（语法92.11%，功能33.57%）。此外，框架还能有效优化设计的PPA。

Conclusion: 研究表明，大语言模型在复杂技术领域如芯片设计中具有显著潜力，VeriPPA框架在Verilog代码生成与PPA优化方面取得了优于现有方法的成果，为芯片设计自动化提供了新的可行路径。

Abstract: Large Language Models (LLMs) are gaining prominence in various fields, thanks
to their ability to generate high- quality content from human instructions.
This paper delves into the field of chip design using LLMs, specifically in
Power- Performance-Area (PPA) optimization and the generation of accurate
Verilog codes for circuit designs. We introduce a novel framework VeriPPA
designed to optimize PPA and generate Verilog code using LLMs. Our method
includes a two-stage process where the first stage focuses on improving the
functional and syntactic correctness of the generated Verilog codes, while the
second stage focuses on optimizing the Verilog codes to meet PPA constraints of
circuit designs, a crucial element of chip design. Our framework achieves an
81.37% success rate in syntactic correctness and 62.06% in functional
correctness for code genera- tion, outperforming current state-of-the-art
(SOTA) methods. On the RTLLM dataset. On the VerilogEval dataset, our framework
achieves 99.56% syntactic correctness and 43.79% functional correctness, also
surpassing SOTA, which stands at 92.11% for syntactic correctness and 33.57%
for functional correctness. Furthermore, Our framework able to optimize the PPA
of the designs. These results highlight the potential of LLMs in handling
complex technical areas and indicate an encouraging development in the
automation of chip design processes.

</details>


### [54] [Fully Automated Verification Framework for Configurable IPs: From Requirements to Results](https://arxiv.org/abs/2510.15902)
*Shuhang Zhang,Jelena Radulovic,Thorsten Dworzak*

Main category: cs.AR

TL;DR: 本文提出了一种全自动的需求驱动功能验证框架，用于降低可配置IP验证的成本与复杂性。


<details>
  <summary>Details</summary>
Motivation: 半导体行业竞争加剧，需在保证质量与可靠性的同时降低芯片价格，而可配置IP的功能验证因复杂且资源密集，成为开发成本的主要来源。

Method: 构建一个全自动框架，集成vPlan生成、测试平台创建、回归执行和需求管理工具中的报告等关键流程。

Result: 显著减少验证工作量，加快开发周期，降低人为错误，并提升覆盖率。

Conclusion: 该框架为可配置IP验证提供了一种可扩展且高效的解决方案。

Abstract: The increasing competition in the semiconductor industry has created
significant pressure to reduce chip prices while maintaining quality and
reliability. Functional verification, particularly for configurable IPs, is a
major contributor to development costs due to its complexity and
resource-intensive nature. To address this, we propose a fully automated
framework for requirements driven functional verification. The framework
automates key processes, including vPlan generation, testbench creation,
regression execution, and reporting in a requirements management tool,
drastically reducing verification effort. This approach accelerates development
cycles, minimizes human error, and enhances coverage, offering a scalable and
efficient solution to the challenges of verifying configurable IPs.

</details>


### [55] [Symbolic Timing Analysis of Digital Circuits Using Analytic Delay Functions](https://arxiv.org/abs/2510.15907)
*Era Thaqi,Dennis Eigner,Arman Ferdowsi,Ulrich Schmid*

Main category: cs.AR

TL;DR: 本文提出了一种基于解析延迟公式的符号时序分析新方法，适用于数字集成电路，无需仿真即可进行逐跳变时序分析，并支持对输入信号和门参数的解析敏感性研究。


<details>
  <summary>Details</summary>
Motivation: 传统时序分析依赖仿真，难以高效研究时序特性对输入信号和门参数的依赖关系；作者旨在通过解析表达式实现无需仿真的符号时序分析。

Method: 利用Ferdowsi等人提出的2输入NOR、NAND和Muller-C门的解析延迟公式，在给定所有输入和内部信号跳变顺序的前提下，推导出内部信号跳变时间的闭式解析表达式，该表达式依赖于输入信号的符号跳变时间和门模型参数，并通过SageMath实现。

Result: 在c17 slack基准电路的NOR门版本上成功应用该方法，验证了其可行性；所得公式支持直接实例化进行时序分析，并可通过求导实现敏感性分析。

Conclusion: 所提方法为数字电路提供了一种高效的符号时序分析手段，不仅避免了仿真开销，还支持对时序特性的解析研究，具有良好的理论和应用潜力。

Abstract: We propose a novel approach to symbolic timing analysis for digital
integrated circuits based on recently developed analytic delay formulas for
2-input NOR, NAND, and Muller-C gates by Ferdowsi et al. (NAHS 2025). Given a
fixed order of the transitions of all input and internal signals of a circuit,
our framework computes closed-form analytic delay expressions for all the
internal signal transition times that depend on (i) the symbolic transition
times of the relevant input signals and (ii) the model parameters of the
relevant gates. The resulting formulas facilitate per-transition timing
analysis without any simulation, by instantiating the symbolic input transition
times and the gate parameters. More importantly, however, they also enable an
\emph{analytic} study of the dependencies of certain timing properties on input
signals and gate parameters. For instance, differentiating a symbolic delay
expression with respect to a gate parameter or input transition time enables
sensitivity analysis. As a proof of concept, we implement our approach using
the computer algebra system SageMath and apply it to the NOR-gate version of
the c17 slack benchmark circuit.

</details>


### [56] [Belenos: Bottleneck Evaluation to Link Biomechanics to Novel Computing Optimizations](https://arxiv.org/abs/2510.15908)
*Hana Chitsaz,Johnson Umeike,Amirmahdi Namjoo,Babak N. Safa,Bahar Asgari*

Main category: cs.AR

TL;DR: 本文通过FEBio、gem5和VTune对有限元生物力学工作负载进行综合表征，揭示了当前软硬件架构在处理此类任务时的性能瓶颈，并指出面向领域的架构协同设计对提升效率至关重要。


<details>
  <summary>Details</summary>
Motivation: 当前生物力学有限元模拟受限于软硬件架构效率低下，难以兼顾计算精度与可扩展性，尤其在材料参数识别等迭代任务中表现明显，亟需探索更高效的加速方案。

Method: 结合广泛使用的FEBio模拟器、gem5架构敏感性研究和VTune性能分析工具，对有限元生物力学工作负载进行系统表征，评估不同规模任务的性能瓶颈及硬件配置影响。

Result: VTune分析显示小规模任务存在约13.1%的前端停顿，而大规模任务以后端瓶颈为主（59.9%–82.2%）；gem5研究表明次优的流水线、内存或分支预测配置可导致高达37.1%的性能下降。

Conclusion: 为高效支持生物力学模拟，需采用架构感知的软硬件协同设计，充分发挥可重构硬件（如FPGA）在领域专用加速中的潜力。

Abstract: Finite element simulations are essential in biomechanics, enabling detailed
modeling of tissues and organs. However, architectural inefficiencies in
current hardware and software stacks limit performance and scalability,
especially for iterative tasks like material parameter identification. As a
result, workflows often sacrifice fidelity for tractability. Reconfigurable
hardware, such as FPGAs, offers a promising path to domain-specific
acceleration without the cost of ASICs, but its potential in biomechanics
remains underexplored. This paper presents Belenos, a comprehensive workload
characterization of finite element biomechanics using FEBio, a widely adopted
simulator, gem5 sensitivity studies, and VTune analysis. VTune results reveal
that smaller workloads experience moderate front-end stalls, typically around
13.1%, whereas larger workloads are dominated by significant back-end
bottlenecks, with backend-bound cycles ranging from 59.9% to over 82.2%.
Complementary gem5 sensitivity studies identify optimal hardware configurations
for Domain-Specific Accelerators (DSA), showing that suboptimal pipeline,
memory, or branch predictor settings can degrade performance by up to 37.1%.
These findings underscore the need for architecture-aware co-design to
efficiently support biomechanical simulation workloads.

</details>


### [57] [SoCks - Simplifying Firmware and Software Integration for Heterogeneous SoCs](https://arxiv.org/abs/2510.15910)
*Marvin Fuchs,Lukas Scheller,Timo Muscheid,Oliver Sander,Luis E. Ardila-Perez*

Main category: cs.AR

TL;DR: 本文提出了一种名为SoCks的模块化构建框架，通过将SoC镜像划分为独立封装的“块”，显著降低开发复杂性，提升构建速度并支持灵活复用与CI/CD集成。


<details>
  <summary>Details</summary>
Motivation: 现代异构SoC设备日益复杂，开发工具缺乏足够支持，导致学习曲线陡峭、调试困难，亟需一种能简化开发流程的解决方案。

Method: 引入SoCks构建框架，将SoC镜像划分为高阶“块”，每个块独立封装构建，通过标准化接口进行必要交互，最小化依赖。

Result: SoCks可将完整SoC镜像的构建速度提升至传统工具的三倍，并支持块的复用、版本替换及CI/CD流程。

Conclusion: SoCks通过模块化和接口标准化有效降低了SoC开发复杂性，提高了构建效率和开发流程的自动化水平。

Abstract: Modern heterogeneous System-on-Chip (SoC) devices integrate advanced
components into a single package, offering powerful capabilities while also
introducing significant complexity. To manage these sophisticated devices,
firmware and software developers need powerful development tools. However, as
these tools become increasingly complex, they often lack adequate support,
resulting in a steep learning curve and challenging troubleshooting. To address
this, this work introduces System-on-Chip blocks (SoCks), a flexible and
expandable build framework that reduces complexity by partitioning the SoC
image into high-level units called blocks. SoCks builds each firmware and
software block in an encapsulated way, independently from other components of
the image, thereby reducing dependencies to a minimum. While some information
exchange between the blocks is unavoidable to ensure seamless runtime
integration, this interaction is standardized via interfaces. A small number of
dependencies and well-defined interfaces simplify the reuse of existing block
implementations and facilitate seamless substitution between versions-for
instance, when choosing root file systems for the embedded Linux operating
system. Additionally, this approach facilitates the establishment of a
decentralized and partially automated development flow through Continuous
Integration and Continuous Delivery (CI/CD). Measurement results demonstrate
that SoCks can build a complete SoC image up to three times faster than
established tools.

</details>


### [58] [Intent-Driven Storage Systems: From Low-Level Tuning to High-Level Understanding](https://arxiv.org/abs/2510.15917)
*Shai Bergman,Won Wook Song,Lukas Cavigelli,Konstantin Berestizshevsky,Ke Zhou,Ji Zhang*

Main category: cs.AR

TL;DR: 本文提出意图驱动存储系统（IDSS），利用大语言模型（LLM）从非结构化信号中推断工作负载意图，实现跨层自适应配置，在FileBench测试中IOPS最高提升2.45倍。


<details>
  <summary>Details</summary>
Motivation: 现有存储系统缺乏对工作负载意图的感知能力，导致依赖脆弱的启发式方法和孤立的优化策略，难以适应现代大规模数据密集型应用的语义需求。

Method: 引入大语言模型（LLM）到存储系统的控制回路中，通过推断工作负载与系统意图，结合策略约束，实现对缓存、预取等组件的自适应配置；提出四项设计原则和相应系统架构。

Result: 在FileBench工作负载上的初步实验表明，IDSS可将IOPS提升最多2.45倍，验证了LLM在策略约束下可作为高层语义优化器的有效性。

Conclusion: 当LLM被嵌入结构化工作流并受策略约束时，可有效弥合应用目标与底层系统控制之间的语义鸿沟，推动存储系统向更自适应、自主和意图对齐的方向发展。

Abstract: Existing storage systems lack visibility into workload intent, limiting their
ability to adapt to the semantics of modern, large-scale data-intensive
applications. This disconnect leads to brittle heuristics and fragmented,
siloed optimizations. To address these limitations, we propose Intent-Driven
Storage Systems (IDSS), a vision for a new paradigm where large language models
(LLMs) infer workload and system intent from unstructured signals to guide
adaptive and cross-layer parameter reconfiguration. IDSS provides holistic
reasoning for competing demands, synthesizing safe and efficient decisions
within policy guardrails. We present four design principles for integrating
LLMs into storage control loops and propose a corresponding system
architecture. Initial results on FileBench workloads show that IDSS can improve
IOPS by up to 2.45X by interpreting intent and generating actionable
configurations for storage components such as caching and prefetching. These
findings suggest that, when constrained by guardrails and embedded within
structured workflows, LLMs can function as high-level semantic optimizers,
bridging the gap between application goals and low-level system control. IDSS
points toward a future in which storage systems are increasingly adaptive,
autonomous, and aligned with dynamic workload demands.

</details>


### [59] [TeLLMe v2: An Efficient End-to-End Ternary LLM Prefill and Decode Accelerator with Table-Lookup Matmul on Edge FPGAs](https://arxiv.org/abs/2510.15926)
*Ye Qiao,Zhiheng Chen,Yifan Zhang,Yian Wang,Sitao Huang*

Main category: cs.AR

TL;DR: TeLLMe 是一种专为低功耗边缘 FPGA 设计的基于查表的三元大语言模型（LLM）加速器，支持 1.58 位权重和 8 位激活，显著提升能效并降低预填充和自回归解码阶段的延迟。


<details>
  <summary>Details</summary>
Motivation: 在可穿戴设备和嵌入式系统兴起的背景下，将大语言模型部署到边缘平台成为迫切需求，但受限于高计算与内存开销、片上资源有限、功耗预算紧张以及预填充阶段的高延迟。

Method: TeLLMe 引入多项创新技术：(1) 基于查表的三元矩阵乘法引擎（TLMM），结合分组激活与在线预计算；(2) 基于分析的细粒度 URAM 权重缓冲管理；(3) 融合浮点逐元素操作与线性计算的流式数据流架构；(4) 重排反向预填充注意力机制；(5) 资源高效的专用解码注意力模块。

Result: 在 5W 功耗限制下，TeLLMe 实现最高 25 tokens/s 的解码吞吐量，对 64–128 token 的提示词实现 0.45–0.96 秒的首 token 延迟（TTFT）。

Conclusion: TeLLMe 在边缘 FPGA 上实现了高效能的大语言模型推理，显著推进了低功耗边缘设备上 LLM 部署的可行性。

Abstract: With the emergence of wearable devices and other embedded systems, deploying
large language models (LLMs) on edge platforms has become an urgent need.
However, this is challenging because of their high computational and memory
demands. Although recent low-bit quantization methods (e.g., BitNet, DeepSeek)
compress weights to as low as 1.58~bits with minimal accuracy loss, edge
deployment is still constrained by limited on-chip resources, power budgets,
and the often-neglected long latency of the prefill stage. We present
\textbf{TeLLMe}, the first table-lookup-based ternary LLM accelerator for
low-power edge FPGAs that fully supports both prefill and autoregressive
decoding using 1.58-bit weights and 8-bit activations. TeLLMe incorporates
several novel techniques, including (1) a table-lookup-based ternary matrix
multiplication (TLMM) engine utilizing grouped activations and online
precomputation for low resource utilization and high throughput; (2) a
fine-grained analytic URAM-based weight buffer management scheme for efficient
loading and compute engine access; (3) a streaming dataflow architecture that
fuses floating-point element-wise operations with linear computations to hide
latency; (4) a reversed-reordered prefill stage attention with fused attention
operations for high memory efficiency; and (5) a resource-efficient specialized
decoding stage attention. Under a 5~W power budget, TeLLMe delivers up to
25~tokens/s decoding throughput and 0.45--0.96~s time-to-first-token (TTFT) for
64--128 token prompts, marking a significant energy-efficiency advancement in
LLM inference on edge FPGAs.

</details>


### [60] [Kelle: Co-design KV Caching and eDRAM for Efficient LLM Serving in Edge Computing](https://arxiv.org/abs/2510.16040)
*Tianhua Xia,Sai Qian Zhang*

Main category: cs.AR

TL;DR: 该论文提出了一种名为Kelle的软硬件协同设计方法，通过将嵌入式DRAM（eDRAM）作为边缘设备上大语言模型（LLM）推理中键值（KV）缓存的主要存储介质，并结合细粒度的内存驱逐、重计算和刷新控制算法，在显著降低内存开销的同时，实现了3.9倍的速度提升和4.5倍的能耗节省。


<details>
  <summary>Details</summary>
Motivation: 在边缘设备上运行大语言模型（LLM）对降低延迟、提升实时性和隐私保护至关重要，但KV缓存随输入序列长度线性增长，带来巨大的内存占用和访问开销，而边缘设备资源受限，难以高效支持此类缓存需求。

Method: 提出使用嵌入式DRAM（eDRAM）替代传统SRAM作为KV缓存的主存储，并设计名为Kelle的软硬件协同加速方案，集成细粒度的内存驱逐、重计算和刷新控制算法，以优化eDRAM的使用效率和数据完整性。

Result: Kelle方案相比现有基线方法实现了3.9倍的推理速度提升和4.5倍的能效提升。

Conclusion: 通过软硬件协同设计，Kelle有效缓解了边缘设备上LLM推理中KV缓存带来的内存与能耗瓶颈，为在资源受限设备上高效部署大语言模型提供了可行路径。

Abstract: Running Large Language Models (LLMs) on edge devices is crucial for reducing
latency, improving real-time processing, and enhancing privacy. By performing
inference directly on the device, data does not need to be sent to the cloud,
ensuring faster responses and reducing reliance on network connectivity.
However, implementing LLMs on edge devices presents challenges, particularly
with managing key-value (KV) caches, which plays a pivotal role in LLM serving.
As the input text lengthens, the size of the KV cache increases linearly with
the sequence length, leading to a significant memory footprint and data access
costs. On the other hand, edge devices have limited memory and computational
power, making it hard to store and efficiently access the large caches needed
for LLM inference.
  To mitigate the substantial overhead caused by KV cache, we propose using
embedded DRAM (eDRAM) as the primary storage for LLM serving in edge device,
which offers higher storage density compared to SRAM. However, to ensure data
integrity, eDRAM needs periodic refresh operations, which are power-intensive.
To reduce eDRAM costs and improve overall system performance, we
propose~\textit{Kelle}, a software-hardware co-design solution optimized for
deploying LLMs on eDRAM-based edge systems. Combined with our fine-grained
memory eviction, recomputation, and refresh control algorithms, the
\textit{Kelle} accelerator delivers a $3.9\times$ speedup and $4.5\times$
energy savings compared to existing baseline solutions.

</details>


### [61] [Architecture, Simulation and Software Stack to Support Post-CMOS Accelerators: The ARCHYTAS Project](https://arxiv.org/abs/2510.16487)
*Giovanni Agosta,Stefano Cherubin,Derek Christ,Francesco Conti,Asbjørn Djupdal,Matthias Jung,Georgios Keramidas,Roberto Passerone,Paolo Rech,Elisa Ricci,Philippe Velha,Flavio Vella,Kasim Sinan Yildirim,Nils Wilbert*

Main category: cs.AR

TL;DR: ARCHYTAS开发面向国防AI应用的非传统硬件加速器（如光电、存内计算、神经形态），并构建配套的系统架构、软件栈与仿真工具。


<details>
  <summary>Details</summary>
Motivation: 传统AI硬件在功耗、效率和可扩展性方面面临瓶颈，尤其在国防场景（如无人车、侦察无人机、海空平台）中亟需更高效的计算方案。

Method: 设计并集成多种非传统硬件加速器，开发统一的系统架构、软件栈及早期仿真工具以支持全系统原型验证。

Result: 提出了一套完整的软硬件协同框架，用于支持面向国防AI应用的新型加速器开发与评估。

Conclusion: ARCHYTAS通过软硬件协同设计，为国防领域AI系统提供高效、低功耗、可扩展的非传统计算解决方案。

Abstract: ARCHYTAS aims to design and evaluate non-conventional hardware accelerators,
in particular, optoelectronic, volatile and non-volatile processing-in-memory,
and neuromorphic, to tackle the power, efficiency, and scalability bottlenecks
of AI with an emphasis on defense use cases (e.g., autonomous vehicles,
surveillance drones, maritime and space platforms). In this paper, we present
the system architecture and software stack that ARCHYTAS will develop to
integrate and support those accelerators, as well as the simulation software
needed for early prototyping of the full system and its components.

</details>


### [62] [Towards Intelligent Traffic Signaling in Dhaka City Based on Vehicle Detection and Congestion Optimization](https://arxiv.org/abs/2510.16622)
*Kazi Ababil Azam,Hasan Masum,Masfiqur Rahaman,A. B. M. Alim Al Islam*

Main category: cs.AR

TL;DR: 本文提出了一种适用于发展中国家非车道化异构交通环境的智能交通信号系统，结合RTSP视频流、树莓派4B和基于YOLO的检测模型，并通过NSGA-II多目标优化算法动态调整信号灯时长，在达卡市实际路口测试中显著提升了交通效率。


<details>
  <summary>Details</summary>
Motivation: 发展中国家如孟加拉国达卡市的交通具有非车道化、高度异构的特点，传统智能交通信号系统主要面向发达国家的结构化交通，难以适用，因此亟需一种契合本地交通特征的智能信号控制方案。

Method: 系统采用RTSP视频流输入，利用资源受限的树莓派4B平台运行基于YOLO的目标检测模型（在NHT-1071数据集上训练），识别并分类异构交通参与者；随后使用NSGA-II多目标优化算法，以最小化等待时间和最大化车辆通行量为目标，生成优化的信号配时方案。

Result: 在达卡Palashi五路交叉口的实际测试表明，所提系统能显著改善此类复杂交通环境下的管理效率。

Conclusion: 该研究为发展中国家复杂交通场景提供了可行、低成本且高效的智能交通信号解决方案，具有良好的推广潜力。

Abstract: The vehicular density in urbanizing cities of developing countries such as
Dhaka, Bangladesh result in a lot of traffic congestion, causing poor on-road
experiences. Traffic signaling is a key component in effective traffic
management for such situations, but the advancements in intelligent traffic
signaling have been exclusive to developed countries with structured traffic.
The non-lane-based, heterogeneous traffic of Dhaka City requires a contextual
approach. This study focuses on the development of an intelligent traffic
signaling system feasible in the context of developing countries such as
Bangladesh. We propose a pipeline leveraging Real Time Streaming Protocol
(RTSP) feeds, a low resources system Raspberry Pi 4B processing, and a state of
the art YOLO-based object detection model trained on the Non-lane-based and
Heterogeneous Traffic (NHT-1071) dataset to detect and classify heterogeneous
traffic. A multi-objective optimization algorithm, NSGA-II, then generates
optimized signal timings, minimizing waiting time while maximizing vehicle
throughput. We test our implementation in a five-road intersection at Palashi,
Dhaka, demonstrating the potential to significantly improve traffic management
in similar situations. The developed testbed paves the way for more contextual
and effective Intelligent Traffic Signaling (ITS) solutions for developing
areas with complicated traffic dynamics such as Dhaka City.

</details>


### [63] [SmaRTLy: RTL Optimization with Logic Inferencing and Structural Rebuilding](https://arxiv.org/abs/2510.17251)
*Chengxi Li,Yang Sun,Lei Chen,Yiwen Wang,Mingxuan Yuan,Evangeline F. Y. Young*

Main category: cs.AR

TL;DR: 本文提出了一种名为smaRTLy的新型寄存器传输级（RTL）多路复用器优化技术，通过逻辑推理和结构重构显著减少门数，在多个基准测试中优于传统工具Yosys。


<details>
  <summary>Details</summary>
Motivation: 传统RTL综合工具（如Yosys）在优化多路复用器树时仅通过遍历树并监控控制端口值，未能充分利用信号间的内在逻辑关系和结构优化潜力。

Method: 开发了创新策略，用于消除冗余的多路复用器树并重构剩余结构，从而减少整体门数。

Result: 在IWLS-2005和RISC-V基准测试中，相比Yosys额外减少了8.95%的AIG面积；在百万门级工业基准测试中，比Yosys多减少了47.2%的AIG面积。

Conclusion: 所提出的逻辑推理与结构重建技术能有效提升RTL优化效果，实现更高效的硬件设计。

Abstract: This paper proposes smaRTLy: a new optimization technique for multiplexers in
Register-Transfer Level (RTL) logic synthesis. Multiplexer trees are very
common in RTL designs, and traditional tools like Yosys optimize them by
traversing the tree and monitoring control port values. However, this method
does not fully exploit the intrinsic logical relationships among signals or the
potential for structural optimization. To address these limitations, we develop
innovative strategies to remove redundant multiplexer trees and restructure the
remaining ones, significantly reducing the overall gate count. We evaluate
smaRTLy on the IWLS-2005 and RISC-V benchmarks, achieving an additional 8.95%
reduction in AIG area compared to Yosys. We also evaluate smaRTLy on an
industrial benchmark in the scale of millions of gates, results show that
smaRTLy can remove 47.2% more AIG area than Yosys. These results demonstrate
the effectiveness of our logic inferencing and structural rebuilding techniques
in enhancing the RTL optimization process, leading to more efficient hardware
designs.

</details>
