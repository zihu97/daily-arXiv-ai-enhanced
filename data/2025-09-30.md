<div id=toc></div>

# Table of Contents

- [cs.SE](#cs.SE) [Total: 35]
- [cs.AR](#cs.AR) [Total: 7]
- [cs.NI](#cs.NI) [Total: 19]
- [cs.MA](#cs.MA) [Total: 6]
- [cs.DC](#cs.DC) [Total: 25]
- [cs.OS](#cs.OS) [Total: 1]
- [cs.PF](#cs.PF) [Total: 2]


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [1] [A benchmark for vericoding: formally verified program synthesis](https://arxiv.org/abs/2509.22908)
*Sergiu Bursuc,Theodore Ehrenborg,Shaowei Lin,Lacramioara Astefanoaei,Ionel Emilian Chiosa,Jure Kukovec,Alok Singh,Oliver Butterley,Adem Bizid,Quinn Dougherty,Miranda Zhao,Max Tan,Max Tegmark*

Main category: cs.SE

TL;DR: 提出了最大的验证编码基准测试，包含12,504个形式化规范，测试LLM从形式化规范生成验证代码的能力，发现在Dafny中成功率最高达82%，自然语言描述对性能提升不明显。


<details>
  <summary>Details</summary>
Motivation: 建立大规模基准来评估LLM从形式化规范生成验证代码的能力，与基于自然语言描述的代码生成进行对比。

Method: 创建包含12,504个形式化规范的基准集（Dafny、Verus/Rust、Lean），使用现成的LLM进行验证编码测试，分析不同语言的成功率。

Result: 验证编码成功率：Lean 27%、Verus/Rust 44%、Dafny 82%；自然语言描述未显著提升性能；Dafny纯验证性能从68%提升至96%。

Conclusion: 验证编码在Dafny中表现最佳，LLM在形式化验证方面取得显著进展，但不同语言间存在性能差异，自然语言辅助作用有限。

Abstract: We present and test the largest benchmark for vericoding, LLM-generation of
formally verified code from formal specifications - in contrast to vibe coding,
which generates potentially buggy code from a natural language description. Our
benchmark contains 12,504 formal specifications, with 3,029 in Dafny, 2,334 in
Verus/Rust and 7,141 in Lean. Of these, 6,174 are new unseen problems. We find
vericoding success rates of 27% in Lean, 44% in Verus/Rust and 82% in Dafny
using off-the-shelf LLMs. Adding natural-language descriptions does not
significantly improve performance. We also find that LLM progress has improved
progress on pure Dafny verification from 68% to 96% over the past year. The
benchmark and vericoding results are shared at
https://github.com/Beneficial-AI-Foundation/vericoding-benchmark

</details>


### [2] [Towards Human-interpretable Explanation in Code Clone Detection using LLM-based Post Hoc Explainer](https://arxiv.org/abs/2509.22978)
*Teeradaj Racharak,Chaiyong Ragkhitwetsagul,Chayanee Junplong,Akara Supratak*

Main category: cs.SE

TL;DR: 利用ChatGPT-4作为后置解释器来解释基于GraphCodeBERT的代码克隆检测结果，准确率高达98%，但实际解释效果有限


<details>
  <summary>Details</summary>
Motivation: 机器学习代码克隆检测器虽然准确但缺乏可解释性，现有后置解释技术需要白盒访问或计算成本高，需要更先进的解释方法

Method: 利用大语言模型的上下文学习能力，使用ChatGPT-4来解释GraphCodeBERT的代码克隆检测预测结果

Result: 方法作为后置解释器很有前景，正确解释率达到98%，95%的情况下提供良好解释，但解释和代码示例的有用性有限，降低温度参数可提高准确性

Conclusion: 该研究为使用大语言模型作为软件工程任务的后置解释器开辟了新途径，并指出了未来改进的方向

Abstract: Recent studies highlight various machine learning (ML)-based techniques for
code clone detection, which can be integrated into developer tools such as
static code analysis. With the advancements brought by ML in code
understanding, ML-based code clone detectors could accurately identify and
classify cloned pairs, especially semantic clones, but often operate as black
boxes, providing little insight into the decision-making process. Post hoc
explainers, on the other hand, aim to interpret and explain the predictions of
these ML models after they are made, offering a way to understand the
underlying mechanisms driving the model's decisions. However, current post hoc
techniques require white-box access to the ML model or are computationally
expensive, indicating a need for advanced post hoc explainers. In this paper,
we propose a novel approach that leverages the in-context learning capabilities
of large language models to elucidate the predictions made by the ML-based code
clone detectors. We perform a study using ChatGPT-4 to explain the code clone
results inferred by GraphCodeBERT. We found that our approach is promising as a
post hoc explainer by giving the correct explanations up to 98% and offering
good explanations 95% of the time. However, the explanations and the code line
examples given by the LLM are useful in some cases. We also found that lowering
the temperature to zero helps increase the accuracy of the explanation. Lastly,
we list the insights that can lead to further improvements in future work. This
study paves the way for future studies in using LLMs as a post hoc explainer
for various software engineering tasks.

</details>


### [3] [The Matthew Effect of AI Programming Assistants: A Hidden Bias in Software Evolution](https://arxiv.org/abs/2509.23261)
*Fei Gu,Zi Liang,Hongzong LI,Jiahao MA*

Main category: cs.SE

TL;DR: 研究发现AI辅助编程存在马太效应：编程语言或框架越流行，LLM生成代码的成功率越高，这可能强化现有技术生态的集中趋势


<details>
  <summary>Details</summary>
Motivation: 探索AI辅助编程如何影响软件工程的迭代动态和生态系统，现有研究主要关注提示设计和代码生成质量，但对更广泛影响的研究不足

Method: 在数千个算法编程任务和数百个框架选择任务上进行大规模实验，系统研究AI辅助编程与软件生态系统的交互

Result: 揭示了明显的马太效应：编程语言或框架越流行，LLM生成代码的成功率越高

Conclusion: AI系统可能强化现有的流行度层级，加速向主导工具收敛，同时阻碍多样性和创新，这对编程生态系统的未来演进具有重要意义

Abstract: AI-assisted programming is rapidly reshaping software development, with large
language models (LLMs) enabling new paradigms such as vibe coding and agentic
coding. While prior works have focused on prompt design and code generation
quality, the broader impact of LLM-driven development on the iterative dynamics
of software engineering remains underexplored. In this paper, we conduct
large-scale experiments on thousands of algorithmic programming tasks and
hundreds of framework selection tasks to systematically investigate how
AI-assisted programming interacts with the software ecosystem. Our analysis
reveals \textbf{a striking Matthew effect: the more popular a programming
language or framework, the higher the success rate of LLM-generated code}. The
phenomenon suggests that AI systems may reinforce existing popularity
hierarchies, accelerating convergence around dominant tools while hindering
diversity and innovation. We provide a quantitative characterization of this
effect and discuss its implications for the future evolution of programming
ecosystems.

</details>


### [4] [Code Arcades: 3d Visualization of Classes, Dependencies and Software Metrics](https://arxiv.org/abs/2509.23297)
*Anthony Savidis,Christos Vasilopoulos*

Main category: cs.SE

TL;DR: 提出了一个可配置的软件可视化系统，通过灵活的代码元素分组机制、多粒度软件度量指标和交互式可视化引擎，提升源代码理解和分析能力


<details>
  <summary>Details</summary>
Motivation: 软件可视化旨在通过图形化表示软件制品来增强对源代码的理解、分析和维护，但现有工具在灵活性和洞察力方面存在不足

Method: 引入可配置的分组机制支持基于任意关系的代码元素组织；结合细粒度和粗粒度软件度量提供多层级系统属性视角；开发交互式可视化引擎允许动态调整渲染属性

Result: 开发了一个更适应性和洞察力更强的源代码理解方法，能够提供宏观概览同时支持对特定程序元素的聚焦检查

Conclusion: 通过三个创新贡献（可配置分组、多级度量、交互引擎）提供了更灵活和深入的软件可视化解决方案，有助于理解大规模系统

Abstract: Software visualization seeks to represent software artifacts graphical-ly in
two or three dimensions, with the goal of enhancing comprehension, anal-ysis,
maintenance, and evolution of the source code. In this context, visualiza-tions
employ graphical forms such as dependency structures, treemaps, or time-lines
that incorporate repository histories. These visualizations allow software
engineers to identify structural patterns, detect complexity hotspots, and
infer system behaviors that are difficult to perceive directly from source
text. By adopting metaphor-based approaches, visualization tools provide
macroscopic overviews while enabling focused inspection of specific program
elements, thus offering an accessible means of understanding large-scale
systems. The contri-bution of our work lies in three areas. First, we introduce
a configurable group-ing mechanism that supports flexible organization of code
elements based on arbitrary relationships. Second, we combine fine-grained and
coarse-grained software metrics to provide a multi-level perspective on system
properties. Third, we present an interactive visualization engine that allows
developers to dynamically adjust rendering attributes. Collectively, these
advances provide a more adaptable and insightful approach to source code
comprehension.

</details>


### [5] [Methods for evaluating software accessibility](https://arxiv.org/abs/2509.23469)
*Mykola Kuz,Ivan Yaremiy,Hanna Yaremii,Mykola Pikuliak,Ihor Lazarovych,Mykola Kozlenko,Denys Vekeryk*

Main category: cs.SE

TL;DR: 开发了基于分类和数学模型的软件可访问性评估方法，特别关注视觉障碍用户的需求，相比标准化方法更详细实用


<details>
  <summary>Details</summary>
Motivation: 现有可访问性评估方法过于通用，无法满足不同用户群体的特定需求，需要开发更详细的评估指标来提升软件交互质量

Method: 构建分类和数学模型，开发基于ISO/IEC 25023和WCAG标准的可访问性评估方法，重点关注可用性质量特性中的可访问性子特性

Result: 对Vasyl Stefanyk Precarpathian国立大学网站主页进行了可访问性分析，提出了具体的改进建议以增强包容性

Conclusion: 提出的方法比标准化方法更详细和实用，是实现包容性数字环境的重要一步，能够有效分析网站对视觉障碍用户的包容性

Abstract: The development and enhancement of methods for evaluating software
accessibility is a relevant challenge in modern software engineering, as
ensuring equal access to digital services is a key factor in improving their
efficiency and inclusivity. The increasing digitalization of society
necessitates the creation of software that complies with international
accessibility standards such as ISO/IEC 25023 and WCAG. Adhering to these
standards helps eliminate barriers to software use for individuals with diverse
physical, sensory, and cognitive needs. Despite advancements in regulatory
frameworks, existing accessibility evaluation methodologies are often
generalized and fail to account for the specific needs of different user
categories or the unique ways they interact with digital systems. This
highlights the need for the development of new, more detailed methods for
defining metrics that influence the quality of user interaction with software
products. Building a classification and mathematical model and developing
accessibility assessment methods for software based on it. A method for
assessing the quality subcharacteristic "Accessibility", which is part of the
"Usability" quality characteristic, has been developed. This enabled the
analysis of a website's inclusivity for individuals with visual impairments,
and the formulation of specific recommendations for further improvements, which
is a crucial step toward creating an inclusive digital environment. Comparing
to standardized approaches, a more detailed and practically oriented
accessibility assessment methodology has been proposed. Using this methodology,
an analysis of the accessibility of the main pages of Vasyl Stefanyk
Precarpathian National University's website was conducted, and improvements
were suggested to enhance its inclusivity.

</details>


### [6] [Improving the Efficiency of LLM Agent Systems through Trajectory Reduction](https://arxiv.org/abs/2509.23586)
*Yuan-An Xiao,Pengfei Gao,Chao Peng,Yingfei Xiong*

Main category: cs.SE

TL;DR: AgentDiet是一种推理时轨迹缩减方法，可减少多轮LLM代理系统中的无用、冗余和过期信息，在保持性能的同时显著降低计算成本


<details>
  <summary>Details</summary>
Motivation: 现有LLM代理系统在处理软件工程任务时，由于轨迹不断增长导致输入令牌计算成本过高，效率问题被忽视

Method: 通过分析代理轨迹发现普遍存在无用、冗余和过期信息，设计了AgentDiet方法自动识别和移除这些浪费信息

Result: 在两个LLM和两个基准测试上，AgentDiet可减少39.9%~59.7%的输入令牌，或降低21.1%~35.9%的最终计算成本，同时保持相同代理性能

Conclusion: 轨迹缩减是代理系统一个有前景的研究方向，AgentDiet证明了在不影响性能的情况下显著提升效率的可行性

Abstract: Multi-turn agent systems based on Large Language Models (LLMs) have been
increasingly popular for software engineering tasks. While LLM agents show
decent effectiveness, the high computational cost of input tokens due to the
ever-growing trajectory remains an efficiency concern for their applications.
Efficiency is largely neglected in existing studies and agent products, and
this paper fills the gap by introducing an inference-time trajectory reduction
approach to reduce the cost of agents.
  Through analyzing existing agent trajectories, we demonstrate that useless,
redundant, and expired information is widespread in all trajectories, which can
be identified and reduced without harming the agent's performance. We then
design a simple yet effective trajectory reduction approach, AgentDiet, which
automatically removes such waste information. We implement AgentDiet on a
top-performing coding agent, and the evaluation on two LLMs and two benchmarks
shows that AgentDiet can reduce input tokens by 39.9% ~ 59.7%, or the final
computational cost by 21.1% ~ 35.9%, while maintaining the same agent
performance. This indicates that trajectory reduction is a promising direction
for agent systems.

</details>


### [7] [Similarity-Based Assessment of Computational Reproducibility in Jupyter Notebooks](https://arxiv.org/abs/2509.23645)
*A S M Shahadat Hossain,Colin Brown,David Koop,Tanu Malik*

Main category: cs.SE

TL;DR: 提出基于相似性的可重现性指数(SRI)，用于量化评估Jupyter Notebook的计算可重现性，通过特定相似度指标比较重新运行与原始输出的差异


<details>
  <summary>Details</summary>
Motivation: Jupyter Notebook虽然便于运行和共享计算实验，但由于随机性、库版本变化等因素，重新运行时可能无法获得一致结果，需要量化评估可重现性的方法

Method: 开发基于不同类型Python对象相似度指标的新方法，为每个生成输出的单元格计算[0,1]范围内的定量分数并提供定性分析

Result: 通过案例研究展示了如何利用各种相似度指标来量化计算可重现性

Conclusion: SRI提供了一种系统化的方法来评估Jupyter Notebook的可重现性，有助于提高计算实验的可靠性和可重复性

Abstract: Computational reproducibility refers to obtaining consistent results when
rerunning an experiment. Jupyter Notebook, a web-based computational notebook
application, facilitates running, publishing, and sharing computational
experiments along with their results. However, rerunning a Jupyter Notebook may
not always generate identical results due to various factors, such as
randomness, changes in library versions, or variations in the computational
environment. This paper introduces the Similarity-based Reproducibility Index
(SRI) -- a metric for assessing the reproducibility of results in Jupyter
Notebooks. SRI employs novel methods developed based on similarity metrics
specific to different types of Python objects to compare rerun outputs against
original outputs. For every cell generating an output in a rerun notebook, SRI
reports a quantitative score in the range [0, 1] as well as some qualitative
insights to assess reproducibility. The paper also includes a case study in
which the proposed metric is applied to a set of Jupyter Notebooks,
demonstrating how various similarity metrics can be leveraged to quantify
computational reproducibility.

</details>


### [8] [PerfBench: Can Agents Resolve Real-World Performance Bugs?](https://arxiv.org/abs/2509.24091)
*Spandan Garg,Roshanak Zilouchian Moghaddam*

Main category: cs.SE

TL;DR: PerfBench是一个专注于性能bug修复的基准测试，包含81个真实.NET项目中的性能问题修复任务，通过创新的评估框架验证修复效果，显示当前AI代理在性能优化方面表现不佳（仅3%成功率），改进后的代理达到20%成功率。


<details>
  <summary>Details</summary>
Motivation: 现有软件工程代理基准主要关注功能正确性，缺乏对性能bug等非功能性问题的评估能力，而性能bug难以检测和修复且浪费计算资源。

Method: 从GitHub热门.NET仓库收集81个真实性能bug修复任务，构建包含性能基准生成和指标对比的新型评估框架，开发性能感知工具和指令的OpenHands-Perf-Agent。

Result: 基线OpenHands代理成功率仅约3%，改进后的OpenHands-Perf-Agent达到约20%成功率，表明适当的性能基准测试工具和指令能显著提升代理性能。

Conclusion: PerfBench为提升AI代理修复性能问题的能力提供了具有挑战性的测试集，当前代理在性能优化方面仍有很大改进空间。

Abstract: Performance bugs are inefficiencies in software that waste computational
resources without causing functional failures, making them particularly
challenging to detect and fix. While recent advances in Software Engineering
agents have shown promise in automated bug fixing, existing benchmarks
primarily focus on functional correctness and fail to evaluate agents'
abilities to identify and resolve non-functional issues like performance bugs.
We introduce PerfBench, a benchmark comprising 81 real-world performance
bug-fixing tasks from popular .NET repositories on GitHub. Unlike existing
benchmarks that rely on pre-existing test suites, PerfBench features a novel
evaluation harness that allows agents to generate their own performance
benchmarks and validates fixes by comparing execution metrics collected for
developer fix and agent fix. Each task in PerfBench is derived from actual
developer fixes linked to performance-related issues, which are then verified
by human experts, ensuring real-world relevance. Our evaluation reveals that
current state-of-the-art coding agents struggle with performance optimization
tasks, with baseline OpenHands agent achieving only a ~3% success rate on our
benchmark. We develop OpenHands-Perf-Agent, which incorporates
performance-aware tooling and instructions and achieves a ~20% success rate on
the benchmark. We show that by ensuring the agent has proper instructions to
benchmark its changes and tooling for benchmark output processing, we can
improve the agent performance significantly, but room for improvement still
remains. PerfBench provides a challenging test set for furthering the
capabilities of agents in fixing performance issues.

</details>


### [9] [PAT-Agent: Autoformalization for Model Checking](https://arxiv.org/abs/2509.23675)
*Xinyue Zuo,Yifan Zhang,Hongshu Wang,Yufan Cai,Zhe Hou,Jing Sun,Jin Song Dong*

Main category: cs.SE

TL;DR: PAT-Agent是一个结合LLM生成能力和形式验证的端到端框架，用于自然语言自动形式化和形式模型修复，显著提升了形式验证的成功率和效率。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在形式化方法自动化方面具有潜力，但由于规范语言的复杂性、幻觉输出风险以及自然语言与形式逻辑之间的语义鸿沟，其在形式验证中的应用仍面临挑战。

Method: 使用规划LLM提取关键建模元素并生成详细计划，然后通过代码生成LLM合成语法正确且语义忠实的正式模型，利用PAT模型检查器进行验证，并通过修复循环迭代修正模型。

Result: 在40个系统上的实验结果显示，PAT-Agent始终优于基线方法，实现了高验证成功率和卓越效率。消融研究确认了规划和修复组件的重要性。

Conclusion: PAT-Agent框架有效解决了LLM在形式验证中的应用挑战，为非形式方法专家提供了可访问且有效的形式建模工具，展示了自动化形式方法的可行性。

Abstract: Recent advances in large language models (LLMs) offer promising potential for
automating formal methods. However, applying them to formal verification
remains challenging due to the complexity of specification languages, the risk
of hallucinated output, and the semantic gap between natural language and
formal logic. We introduce PAT-Agent, an end-to-end framework for natural
language autoformalization and formal model repair that combines the generative
capabilities of LLMs with the rigor of formal verification to automate the
construction of verifiable formal models. In PAT-Agent, a Planning LLM first
extracts key modeling elements and generates a detailed plan using semantic
prompts, which then guides a Code Generation LLM to synthesize syntactically
correct and semantically faithful formal models. The resulting code is verified
using the Process Analysis Toolkit (PAT) model checker against user-specified
properties, and when discrepancies occur, a Repair Loop is triggered to
iteratively correct the model using counterexamples. To improve flexibility, we
built a web-based interface that enables users, particularly non-FM-experts, to
describe, customize, and verify system behaviors through user-LLM interactions.
Experimental results on 40 systems show that PAT-Agent consistently outperforms
baselines, achieving high verification success with superior efficiency. The
ablation studies confirm the importance of both planning and repair components,
and the user study demonstrates that our interface is accessible and supports
effective formal modeling, even for users with limited formal methods
experience.

</details>


### [10] [Satellite: Detecting and Analyzing Smart Contract Vulnerabilities caused by Subcontract Misuse](https://arxiv.org/abs/2509.23679)
*Zeqin Liao,Yuhong Nan,Zixu Gao,Henglong Liang,Sicheng Hao,Jiajing Wu,Zibin Zheng*

Main category: cs.SE

TL;DR: Satellite是一个针对智能合约中子合约误用漏洞(SMV)检测的字节码级静态分析框架，通过迁移学习恢复继承方法、细粒度方法级特征提取和SMV指标识别，在真实数据集上达到84.68%准确率和92.11%召回率


<details>
  <summary>Details</summary>
Motivation: 智能合约开发中广泛复用子合约以提高效率，但这种复用可能意外引入漏洞。由于智能合约编译为字节码后类级信息和语义被完全隐藏，自动检测此类问题面临独特挑战

Method: Satellite采用迁移学习方法恢复继承方法，提取细粒度方法级特征并进行方法级比较来识别子合约复用部分，根据漏洞类型总结SMV指标进行有效检测

Result: 在包含58个真实攻击SMV和56个SOTA研究SMV模式的数据集上，Satellite达到84.68%精确率和92.11%召回率。在10,011个真实智能合约中发现14个新的未知SMV，影响价值201,358美元的加密资产

Conclusion: Satellite框架能有效检测智能合约中的子合约误用漏洞，在准确性和召回率方面表现优异，并能发现新的未知漏洞，对保护智能合约安全具有重要意义

Abstract: Developers of smart contracts pervasively reuse subcontracts to improve
development efficiency. Like any program language, such subcontract reuse may
unexpectedly include, or introduce vulnerabilities to the end-point smart
contract. Unfortunately, automatically detecting such issues poses several
unique challenges. Particularly, in most cases, smart contracts are compiled as
bytecode, whose class-level information (e.g., inheritance, virtual function
table), and even semantics (e.g., control flow and data flow) are fully
obscured as a single smart contract after compilation.
  In this paper, we propose Satellite, a new bytecode-level static analysis
framework for subcontract misuse vulnerability (SMV) detection in smart
contracts. Satellite incorporates a series of novel designs to enhance its
overall effectiveness.. Particularly, Satellite utilizes a transfer learning
method to recover the inherited methods, which are critical for identifying
subcontract reuse in smart contracts. Further, Satellite extracts a set of
fine-grained method-level features and performs a method-level comparison, for
identifying the reuse part of subcontract in smart contracts. Finally,
Satellite summarizes a set of SMV indicators according to their types, and
hence effectively identifies SMVs. To evaluate Satellite, we construct a
dataset consisting of 58 SMVs derived from real-world attacks and collect
additional 56 SMV patterns from SOTA studies. Experiment results indicate that
Satellite exhibits good performance in identifying SMV, with a precision rate
of 84.68% and a recall rate of 92.11%. In addition, Satellite successfully
identifies 14 new/unknown SMV over 10,011 real-world smart contracts, affecting
a total amount of digital assets worth 201,358 USD.

</details>


### [11] [Influence-Guided Concolic Testing of Transformer Robustness](https://arxiv.org/abs/2509.23806)
*Chih-Duo Hong,Yu Wang,Yao-Chen Chang,Fang Yu*

Main category: cs.SE

TL;DR: 提出了基于影响力引导的concolic测试方法，通过SHAP评估路径谓词对模型输出的影响，为Transformer分类器寻找标签翻转输入。


<details>
  <summary>Details</summary>
Motivation: 为现代Transformer架构开发可行的concolic测试方法，解决传统方法在深层网络上的约束增长问题。

Method: 使用SHAP影响力评估指导搜索，设计求解器兼容的纯Python多头自注意力语义，引入轻量级调度启发式方法控制约束增长。

Result: 在紧凑Transformer上的白盒研究中，影响力引导方法比FIFO基线更高效地找到标签翻转输入，在深层网络上保持稳定进展。

Conclusion: 影响力信号为符号探索提供有用搜索偏置，求解器友好的注意力语义和轻量级调度使concolic测试适用于当代Transformer模型，对调试和模型审计具有潜在价值。

Abstract: Concolic testing for deep neural networks alternates concrete execution with
constraint solving to search for inputs that flip decisions. We present an
{influence-guided} concolic tester for Transformer classifiers that ranks path
predicates by SHAP-based estimates of their impact on the model output. To
enable SMT solving on modern architectures, we prototype a solver-compatible,
pure-Python semantics for multi-head self-attention and introduce practical
scheduling heuristics that temper constraint growth on deeper models. In a
white-box study on compact Transformers under small $L_0$ budgets, influence
guidance finds label-flip inputs more efficiently than a FIFO baseline and
maintains steady progress on deeper networks. Aggregating successful attack
instances with a SHAP-based critical decision path analysis reveals recurring,
compact decision logic shared across attacks. These observations suggest that
(i) influence signals provide a useful search bias for symbolic exploration,
and (ii) solver-friendly attention semantics paired with lightweight scheduling
make concolic testing feasible for contemporary Transformer models, offering
potential utility for debugging and model auditing.

</details>


### [12] [Navigating the Labyrinth: Path-Sensitive Unit Test Generation with Large Language Models](https://arxiv.org/abs/2509.23812)
*Dianshu Liao,Xin Yin,Shidong Pan,Chao Ni,Zhenchang Xing,Xiaoyu Sun*

Main category: cs.SE

TL;DR: JUnitGenie是一个路径敏感的单元测试生成框架，结合代码知识和LLM语义能力，相比传统启发式方法和LLM基准方法平均提升29.60%分支覆盖率和31.00%行覆盖率。


<details>
  <summary>Details</summary>
Motivation: 现有单元测试生成方法存在路径不敏感问题，依赖固定启发式规则或有限上下文信息，难以处理深层控制流结构，导致覆盖率不足。

Method: 从Java项目中提取代码知识，将其提炼为结构化提示来指导LLM生成高覆盖率的单元测试，实现路径敏感的上下文感知测试生成。

Result: 在10个真实Java项目的2,258个复杂方法上评估，生成有效测试用例，平均分支覆盖率提升29.60%，行覆盖率提升31.00%，并能发现真实bug。

Conclusion: JUnitGenie通过结合代码知识和LLM语义能力，有效解决了路径敏感测试生成问题，显著提升了测试覆盖率和bug检测能力。

Abstract: Unit testing is essential for software quality assurance, yet writing and
maintaining tests remains time-consuming and error-prone. To address this
challenge, researchers have proposed various techniques for automating unit
test generation, including traditional heuristic-based methods and more recent
approaches that leverage large language models (LLMs). However, these existing
approaches are inherently path-insensitive because they rely on fixed
heuristics or limited contextual information and fail to reason about deep
control-flow structures. As a result, they often struggle to achieve adequate
coverage, particularly for deep or complex execution paths. In this work, we
present a path-sensitive framework, JUnitGenie, to fill this gap by combining
code knowledge with the semantic capabilities of LLMs in guiding context-aware
unit test generation. After extracting code knowledge from Java projects,
JUnitGenie distills this knowledge into structured prompts to guide the
generation of high-coverage unit tests. We evaluate JUnitGenie on 2,258 complex
focal methods from ten real-world Java projects. The results show that
JUnitGenie generates valid tests and improves branch and line coverage by
29.60% and 31.00% on average over both heuristic and LLM-based baselines. We
further demonstrate that the generated test cases can uncover real-world bugs,
which were later confirmed and fixed by developers.

</details>


### [13] [SolContractEval: A Benchmark for Evaluating Contract-Level Solidity Code Generation](https://arxiv.org/abs/2509.23824)
*Zhifan Ye,Jiachi Chen,Zhenzhe Shao,Lingfeng Bao,Xiaohu Yang,Zhongxin Liu*

Main category: cs.SE

TL;DR: 本文介绍了SolContractEval，首个Solidity智能合约级别的代码生成基准测试，评估发现现有LLMs在Solidity代码生成方面表现不如通用编程语言，特别是在复杂逻辑和跨合约依赖方面存在困难。


<details>
  <summary>Details</summary>
Motivation: 区块链发展推动智能合约需求增长，但现有LLMs针对Solidity语言的代码生成能力研究不足，且现有评估方法局限于孤立函数和合成输入，无法真实反映合约开发能力。

Method: 构建包含124个真实链上合约任务的SolContractEval基准测试，采用动态评估框架基于历史交易重放进行自动化功能正确性评估，系统评估了6个主流LLMs。

Result: Claude-3.7-Sonnet表现最佳，但所有模型在Solidity代码生成方面都逊于通用编程语言的类级别生成任务；模型在处理标准模式任务时表现较好，但在复杂逻辑和跨合约依赖方面存在困难。

Conclusion: 现有LLMs对Solidity特定功能和上下文依赖的理解有限，需要针对Solidity语言特性进行专门优化，SolContractEval为智能合约代码生成提供了更真实的评估基准。

Abstract: The rise of blockchain has brought smart contracts into mainstream use,
creating a demand for smart contract generation tools. While large language
models (LLMs) excel at generating code in general-purpose languages, their
effectiveness on Solidity, the primary language for smart contracts, remains
underexplored. Solidity constitutes only a small portion of typical LLM
training data and differs from general-purpose languages in its
version-sensitive syntax and limited flexibility. These factors raise concerns
about the reliability of existing LLMs for Solidity code generation.
Critically, existing evaluations, focused on isolated functions and synthetic
inputs, fall short of assessing models' capabilities in real-world contract
development.
  To bridge this gap, we introduce SolContractEval, the first contract-level
benchmark for Solidity code generation. It comprises 124 tasks drawn from real
on-chain contracts across nine major domains. Each task input, consisting of
complete context dependencies, a structured contract framework, and a concise
task prompt, is independently annotated and cross-validated by experienced
developers. To enable precise and automated evaluation of functional
correctness, we also develop a dynamic evaluation framework based on historical
transaction replay. Building on SolContractEval, we perform a systematic
evaluation of six mainstream LLMs. We find that Claude-3.7-Sonnet achieves the
highest overall performance, though evaluated models underperform relative to
their capabilities on class-level generation tasks in general-purpose
programming languages. Second, current models perform better on tasks that
follow standard patterns but struggle with complex logic and inter-contract
dependencies. Finally, they exhibit limited understanding of Solidity-specific
features and contextual dependencies.

</details>


### [14] [HFuzzer: Testing Large Language Models for Package Hallucinations via Phrase-based Fuzzing](https://arxiv.org/abs/2509.23835)
*Yukai Zhao,Menghan Wu,Xing Hu,Xin Xia*

Main category: cs.SE

TL;DR: HFUZZER是一个基于短语的模糊测试框架，用于检测大型语言模型在代码生成中的包幻觉问题，能有效发现模型推荐不存在软件包的安全风险。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在代码生成中存在包幻觉问题，会推荐不存在的软件包，这可能被恶意攻击者利用进行软件供应链攻击，但目前缺乏针对此类问题的测试方法。

Method: 采用基于短语的模糊测试技术，从包信息或编码任务中提取短语，引导模型推断更广泛的合理信息，生成足够多样化的编码任务来测试包幻觉。

Result: HFUZZER在所有测试模型中均触发了包幻觉，相比变异模糊测试框架发现了2.6倍更多的独特幻觉包，在GPT-4o上发现了46个独特幻觉包。

Conclusion: 包幻觉不仅发生在代码生成过程中，还出现在环境配置辅助场景中，HFUZZER能有效检测和缓解这一安全风险。

Abstract: Large Language Models (LLMs) are widely used for code generation, but they
face critical security risks when applied to practical production due to
package hallucinations, in which LLMs recommend non-existent packages. These
hallucinations can be exploited in software supply chain attacks, where
malicious attackers exploit them to register harmful packages. It is critical
to test LLMs for package hallucinations to mitigate package hallucinations and
defend against potential attacks. Although researchers have proposed testing
frameworks for fact-conflicting hallucinations in natural language generation,
there is a lack of research on package hallucinations. To fill this gap, we
propose HFUZZER, a novel phrase-based fuzzing framework to test LLMs for
package hallucinations. HFUZZER adopts fuzzing technology and guides the model
to infer a wider range of reasonable information based on phrases, thereby
generating enough and diverse coding tasks. Furthermore, HFUZZER extracts
phrases from package information or coding tasks to ensure the relevance of
phrases and code, thereby improving the relevance of generated tasks and code.
We evaluate HFUZZER on multiple LLMs and find that it triggers package
hallucinations across all selected models. Compared to the mutational fuzzing
framework, HFUZZER identifies 2.60x more unique hallucinated packages and
generates more diverse tasks. Additionally, when testing the model GPT-4o,
HFUZZER finds 46 unique hallucinated packages. Further analysis reveals that
for GPT-4o, LLMs exhibit package hallucinations not only during code generation
but also when assisting with environment configuration.

</details>


### [15] [Learning-Based Testing for Deep Learning: Enhancing Model Robustness with Adversarial Input Prioritization](https://arxiv.org/abs/2509.23961)
*Sheikh Md Mushfiqur Rahman,Nasir Eisty*

Main category: cs.SE

TL;DR: 提出了一种基于学习的测试方法，通过结合假设测试和变异测试来优先处理对抗性测试用例，有效提升深度神经网络的故障检测效率和模型鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 深度神经网络在关键应用中部署时，对抗性输入的鲁棒性至关重要。现有的基于覆盖率或置信度的测试优先级方法往往无法有效识别最具故障揭示性的输入，限制了实际效果。

Method: 采用学习型测试方法，选择具有高概率暴露模型故障的对抗性输入子集，不依赖架构特定特征或形式验证，使其能够适应各种深度神经网络。

Result: 该方法在优先处理故障揭示性输入和加速故障检测方面持续超越基线方法，通过有效组织测试排列，在各种数据集、模型架构和对抗攻击技术中显著更快地发现所有潜在故障。

Conclusion: 该方法不仅提高了故障检测效率，还保持了输入多样性并为模型重新训练提供有效指导，进一步增强了鲁棒性，成为现实世界深度神经网络应用中对抗性测试优先级的有力实用解决方案。

Abstract: Context: Deep Neural Networks (DNNs) are increasingly deployed in critical
applications, where resilience against adversarial inputs is paramount.
However, whether coverage-based or confidence-based, existing test
prioritization methods often fail to efficiently identify the most
fault-revealing inputs, limiting their practical effectiveness. Aims: This
project aims to enhance fault detection and model robustness in DNNs by
integrating Learning-Based Testing (LBT) with hypothesis and mutation testing
to efficiently prioritize adversarial test cases. Methods: Our method selects a
subset of adversarial inputs with a high likelihood of exposing model faults,
without relying on architecture-specific characteristics or formal
verification, making it adaptable across diverse DNNs. Results: Our results
demonstrate that the proposed LBT method consistently surpasses baseline
approaches in prioritizing fault-revealing inputs and accelerating fault
detection. By efficiently organizing test permutations, it uncovers all
potential faults significantly faster across various datasets, model
architectures, and adversarial attack techniques. Conclusion: Beyond improving
fault detection, our method preserves input diversity and provides effective
guidance for model retraining, further enhancing robustness. These advantages
establish our approach as a powerful and practical solution for adversarial
test prioritization in real-world DNN applications.

</details>


### [16] [SandCell: Sandboxing Rust Beyond Unsafe Code](https://arxiv.org/abs/2509.24032)
*Jialun Zhang,Merve Gülmez,Thomas Nyman,Gang Tan*

Main category: cs.SE

TL;DR: SandCell是一个轻量级的Rust隔离系统，通过现有语法边界实现灵活隔离，允许程序员以最小标注代价指定需要沙盒化的组件，并提供新颖的数据传输技术来降低开销。


<details>
  <summary>Details</summary>
Motivation: Rust通过所有权和借用规则确保内存安全，但unsafe关键字允许绕过这些限制，带来安全风险。现有隔离方法只能提供固定隔离边界，无法支持需要同时沙盒化安全和unsafe代码的表达性策略。

Method: 利用Rust现有的语法边界，通过最小化标注指定需要隔离的组件，引入新颖的数据传输技术来降低沙盒间数据传输的开销。

Result: 评估显示SandCell能有效防止各种Rust应用中的漏洞，同时保持合理的性能开销。

Conclusion: SandCell提供了灵活且轻量级的隔离解决方案，能够在不显著影响性能的情况下增强Rust程序的安全性。

Abstract: Rust is a modern systems programming language that ensures memory safety by
enforcing ownership and borrowing rules at compile time. While the unsafe
keyword allows programmers to bypass these restrictions, it introduces
significant risks. Various approaches for isolating unsafe code to protect safe
Rust from vulnerabilities have been proposed, yet these methods provide only
fixed isolation boundaries and do not accommodate expressive policies that
require sandboxing both safe and unsafe code. This paper presents SandCell for
flexible and lightweight isolation in Rust by leveraging existing syntactic
boundaries. SandCell allows programmers to specify which components to sandbox
with minimal annotation effort, enabling fine-grained control over isolation.
The system also introduces novel techniques to minimize overhead when
transferring data between sandboxes. Our evaluation demonstrates SandCell's
effectiveness in preventing vulnerabilities across various Rust applications
while maintaining reasonable performance overheads.

</details>


### [17] [TENET: Leveraging Tests Beyond Validation for Code Generation](https://arxiv.org/abs/2509.24148)
*Yiran Hu,Nan Jiang,Shanchao Liang,Yi Wu,Lin Tan*

Main category: cs.SE

TL;DR: TENET是一个基于TDD的LLM代理，通过测试套件选择、代码检索和反馈驱动的迭代优化，在复杂代码库中生成函数，在两个基准测试中显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 在vibe coding时代，开发者越来越多地将代码编写委托给LLM，但需要TDD来确保代码质量。现有方法面临测试套件选择、上下文检索和测试反馈利用三大挑战。

Method: TENET包含三个核心组件：(1)测试套件选择机制最大化使用场景多样性；(2)定制化工具集实现高效代码检索和交互调试；(3)基于反射的迭代优化工作流分析失败、补充上下文和应用代码改进。

Result: 在RepoCod和RepoEval基准测试上分别达到69.08%和81.77%的Pass@1，比最佳基线方法分别高出9.49和2.17个百分点。

Conclusion: TENET成功解决了TDD环境下LLM代码生成的三大挑战，是首个研究仓库级上下文的测试驱动代码生成工作，证明了测试套件不同方面对LLM代理性能的重要影响。

Abstract: Test-Driven Development (TDD) is a widely adopted software engineering
practice that requires developers to create and execute tests alongside code
implementation, ensuring that software behavior is continuously validated and
refined. In the era of vibe coding, where developers increasingly delegate code
writing to large language models (LLMs) by specifying high-level intentions,
TDD becomes even more crucial, as test cases serve as executable specifications
that explicitly define and verify intended functionality beyond what
natural-language descriptions and code context can convey. While vibe coding
under TDD is promising, there are three main challenges: (1) selecting a small
yet effective test suite to improve the generation accuracy and control the
execution workload, (2) retrieving context such as relevant code effectively,
and (3) systematically using test feedback for effective code refinement. To
address these challenges, we introduce TENET, an LLM agent for generating
functions in complex real-world repositories under the TDD setting. TENET
features three components: (1) a novel test harness mechanism that selects a
concise test suite to maximize diversity of target usage scenarios; (2) a
tailored agent toolset that performs efficient retrieval of relevant code with
interactive debugging; and (3) a reflection-based refinement workflow that
iteratively analyzes failures, replenishes context, and applies code
refinement. TENET achieves 69.08% and 81.77% Pass@1 on RepoCod and RepoEval
benchmarks, outperforming the best agentic baselines by 9.49 and 2.17
percentage points, respectively. In addition, this is the first study of
test-driven code generation with repository-level context, examining how
different aspects of test suites affect the performance of LLM agents under the
TDD setting.

</details>


### [18] [Metamorphic Testing for Audio Content Moderation Software](https://arxiv.org/abs/2509.24215)
*Wenxuan Wang,Yongjiang Wu,Junyuan Zhang,Shuqing Li,Yun Peng,Wenting Chen,Shuai Wang,Michael R. Lyu*

Main category: cs.SE

TL;DR: MTAM是一种用于音频内容审核软件的蜕变测试框架，通过定义14种蜕变关系生成能够绕过检测的有害音频测试用例，在测试5个商业软件和1个学术模型时发现了高达51.1%的错误检测率。


<details>
  <summary>Details</summary>
Motivation: 音频平台被滥用于传播有害内容，现有审核工具容易被恶意攻击者通过音频修改绕过，且缺乏对这些对抗性输入的充分研究。

Method: 提出MTAM蜕变测试框架，定义14种蜕变关系（基于音频特征和启发式扰动），对2000个音频片段进行试点研究，生成难以检测的有害音频测试用例。

Result: 测试5个商业软件和1个学术模型，错误发现率最高达到：Gladia 38.6%、Assembly AI 18.3%、Baidu 35.1%、Nextdata 16.7%、Tencent 51.1%，学术模型45.7%。

Conclusion: MTAM框架能有效发现音频内容审核软件的漏洞，现有审核系统在面对对抗性音频修改时存在严重缺陷，需要更鲁棒的审核机制。

Abstract: The rapid growth of audio-centric platforms and applications such as WhatsApp
and Twitter has transformed the way people communicate and share audio content
in modern society. However, these platforms are increasingly misused to
disseminate harmful audio content, such as hate speech, deceptive
advertisements, and explicit material, which can have significant negative
consequences (e.g., detrimental effects on mental health). In response,
researchers and practitioners have been actively developing and deploying audio
content moderation tools to tackle this issue. Despite these efforts, malicious
actors can bypass moderation systems by making subtle alterations to audio
content, such as modifying pitch or inserting noise. Moreover, the
effectiveness of modern audio moderation tools against such adversarial inputs
remains insufficiently studied. To address these challenges, we propose MTAM, a
Metamorphic Testing framework for Audio content Moderation software.
Specifically, we conduct a pilot study on 2000 audio clips and define 14
metamorphic relations across two perturbation categories: Audio Features-Based
and Heuristic perturbations. MTAM applies these metamorphic relations to toxic
audio content to generate test cases that remain harmful while being more
likely to evade detection. In our evaluation, we employ MTAM to test five
commercial textual content moderation software and an academic model against
three kinds of toxic content. The results show that MTAM achieves up to 38.6%,
18.3%, 35.1%, 16.7%, and 51.1% error finding rates (EFR) when testing
commercial moderation software provided by Gladia, Assembly AI, Baidu,
Nextdata, and Tencent, respectively, and it obtains up to 45.7% EFR when
testing the state-of-the-art algorithms from the academy.

</details>


### [19] [Comparing Open-Source and Commercial LLMs for Domain-Specific Analysis and Reporting: Software Engineering Challenges and Design Trade-offs](https://arxiv.org/abs/2509.24344)
*Theo Koraag,Niklas Wagner,Felix Dobslaw,Lucas Gren*

Main category: cs.SE

TL;DR: LLMs在金融报告分析自动化方面潜力巨大，但面临集成挑战。云端模型流畅但数据隐私风险高，本地开源模型控制性好但工程成本大。


<details>
  <summary>Details</summary>
Motivation: 探索开源和商业LLMs在金融报告分析和评论生成中的应用，研究软件工程实施中的挑战。

Method: 采用设计科学研究方法，通过迭代设计评估两个LLM系统：一个使用本地开源模型的多代理工作流，另一个使用商业GPT-4o模型，并通过专家评估进行验证。

Result: LLMs在自动化金融报告任务方面表现良好，但集成存在显著挑战，包括提示设计、上下文依赖性和实现权衡等问题。云端模型流畅度更高但存在数据隐私问题，本地模型控制性更好但需要更多工程投入。

Conclusion: LLMs在金融报告自动化方面具有强大潜力，但成功集成需要关注架构设计、提示工程和系统可靠性，通过定制验证机制和工程策略来平衡准确性、控制性和合规性。

Abstract: Context: Large Language Models (LLMs) enable automation of complex natural
language processing across domains, but research on domain-specific
applications like Finance remains limited. Objectives: This study explored
open-source and commercial LLMs for financial report analysis and commentary
generation, focusing on software engineering challenges in implementation.
Methods: Using Design Science Research methodology, an exploratory case study
iteratively designed and evaluated two LLM-based systems: one with local
open-source models in a multi-agent workflow, another using commercial GPT-4o.
Both were assessed through expert evaluation of real-world financial reporting
use cases. Results: LLMs demonstrated strong potential for automating financial
reporting tasks, but integration presented significant challenges. Iterative
development revealed issues including prompt design, contextual dependency, and
implementation trade-offs. Cloud-based models offered superior fluency and
usability but raised data privacy and external dependency concerns. Local
open-source models provided better data control and compliance but required
substantially more engineering effort for reliability and usability.
Conclusion: LLMs show strong potential for financial reporting automation, but
successful integration requires careful attention to architecture, prompt
design, and system reliability. Implementation success depends on addressing
domain-specific challenges through tailored validation mechanisms and
engineering strategies that balance accuracy, control, and compliance.

</details>


### [20] [Efficient Decomposition Identification of Deterministic Finite Automata from Examples](https://arxiv.org/abs/2509.24347)
*Junjie Meng,Jie An,Yong Li,Andrea Turrini,Fanjiang Xu,Naijun Zhan,Miaomiao Zhang*

Main category: cs.SE

TL;DR: 提出基于3值DFA的新框架，替代传统APTA方法，显著提升DFA分解识别问题的可扩展性和效率


<details>
  <summary>Details</summary>
Motivation: 传统DFA学习方法产生的大型单一DFA缺乏简洁性和互操作性，现有DFA分解方法依赖APTA的SAT编码存在可扩展性限制

Method: 使用从标记示例直接导出的3值DFA替代APTA，减少冗余，改进SAT编码，研究Pareto最优和状态最优两种DFA分解变体

Result: 实验表明3DFA方法在Pareto最优DIP上获得显著效率提升，并为状态最优DIP提供可扩展解决方案

Conclusion: 3DFA框架有效解决了DFA分解识别问题的可扩展性挑战，为复杂系统行为的模块化建模提供了更高效的方法

Abstract: The identification of deterministic finite automata (DFAs) from labeled
examples is a cornerstone of automata learning, yet traditional methods focus
on learning monolithic DFAs, which often yield a large DFA lacking simplicity
and interoperability. Recent work addresses these limitations by exploring DFA
decomposition identification problems (DFA-DIPs), which model system behavior
as intersections of multiple DFAs, offering modularity for complex tasks.
However, existing DFA-DIP approaches depend on SAT encodings derived from
Augmented Prefix Tree Acceptors (APTAs), incurring scalability limitations due
to their inherent redundancy.
  In this work, we advance DFA-DIP research through studying two variants: the
traditional Pareto-optimal DIP and the novel states-optimal DIP, which
prioritizes a minimal number of states. We propose a novel framework that
bridges DFA decomposition with recent advancements in automata representation.
One of our key innovations replaces APTA with 3-valued DFA (3DFA) derived
directly from labeled examples. This compact representation eliminates
redundancies of APTA, thus drastically reducing variables in the improved SAT
encoding. Experimental results demonstrate that our 3DFA-based approach
achieves significant efficiency gains for the Pareto-optimal DIP while enabling
a scalable solution for the states-optimal DIP.

</details>


### [21] [Walk the Talk: Is Your Log-based Software Reliability Maintenance System Really Reliable?](https://arxiv.org/abs/2509.24352)
*Minghua He,Tong Jia,Chiming Duan,Pei Xiao,Lingzhe Zhang,Kangjin Wang,Yifan Wu,Ying Li,Gang Huang*

Main category: cs.SE

TL;DR: 本文提出了FaithLog系统，通过因果引导注意力机制和对抗一致性学习来提高日志异常检测的诊断可信度，解决现有深度学习方法缺乏可解释性的问题。


<details>
  <summary>Details</summary>
Motivation: 现有的深度学习日志异常检测方法对服务提供商来说是黑盒，无法理解检测过程，阻碍了在生产环境中的信任和部署。

Method: 定义了诊断可信度指标，设计了基于注意力的根因定位和事件扰动评估任务，提出了FaithLog系统，采用因果引导注意力机制和对抗一致性学习。

Result: 在两个公开数据集和一个工业数据集上的评估表明，该方法在诊断可信度方面达到了最先进的性能。

Conclusion: FaithLog系统通过提高诊断可信度，解决了日志异常检测方法的可解释性问题，有助于在实际生产环境中获得服务提供商的信任和部署。

Abstract: Log-based software reliability maintenance systems are crucial for sustaining
stable customer experience. However, existing deep learning-based methods
represent a black box for service providers, making it impossible for providers
to understand how these methods detect anomalies, thereby hindering trust and
deployment in real production environments. To address this issue, this paper
defines a trustworthiness metric, diagnostic faithfulness, for models to gain
service providers' trust, based on surveys of SREs at a major cloud provider.
We design two evaluation tasks: attention-based root cause localization and
event perturbation. Empirical studies demonstrate that existing methods perform
poorly in diagnostic faithfulness. Consequently, we propose FaithLog, a
faithful log-based anomaly detection system, which achieves faithfulness
through a carefully designed causality-guided attention mechanism and
adversarial consistency learning. Evaluation results on two public datasets and
one industrial dataset demonstrate that the proposed method achieves
state-of-the-art performance in diagnostic faithfulness.

</details>


### [22] [United We Stand: Towards End-to-End Log-based Fault Diagnosis via Interactive Multi-Task Learning](https://arxiv.org/abs/2509.24364)
*Minghua He,Chiming Duan,Pei Xiao,Tong Jia,Siyu Yu,Lingzhe Zhang,Weijie Hong,Jin Han,Yifan Wu,Ying Li,Gang Huang*

Main category: cs.SE

TL;DR: Chimera是一种端到端的日志故障诊断方法，通过异常检测和根因定位的双向交互与知识转移，解决了传统任务独立方法导致的诊断偏差、昂贵监控数据依赖和任务协作关系忽视等问题。


<details>
  <summary>Details</summary>
Motivation: 现有故障诊断方法采用任务独立方式，无法在数据形式和诊断目标上弥合异常检测与根因定位之间的差距，导致诊断偏差累积、系统部署依赖昂贵监控数据、忽视诊断任务间协作关系三大问题。

Method: 基于交互式多任务学习，在数据、特征和诊断结果三个层面精心设计异常检测与根因定位之间的交互策略，在统一的端到端框架内实现两个子任务的交互式协同诊断。

Result: 在两个公共数据集和一个工业数据集上的评估显示，Chimera在异常检测和根因定位方面均优于现有方法，分别实现了2.92%-5.00%和19.01%-37.09%的提升，并已成功部署到工业云平台。

Conclusion: Chimera通过双向交互和知识转移实现了端到端故障诊断，有效解决了传统方法的局限性，在实际工业环境中表现出优越性能。

Abstract: Log-based fault diagnosis is essential for maintaining software system
availability. However, existing fault diagnosis methods are built using a
task-independent manner, which fails to bridge the gap between anomaly
detection and root cause localization in terms of data form and diagnostic
objectives, resulting in three major issues: 1) Diagnostic bias accumulates in
the system; 2) System deployment relies on expensive monitoring data; 3) The
collaborative relationship between diagnostic tasks is overlooked. Facing this
problems, we propose a novel end-to-end log-based fault diagnosis method,
Chimera, whose key idea is to achieve end-to-end fault diagnosis through
bidirectional interaction and knowledge transfer between anomaly detection and
root cause localization. Chimera is based on interactive multi-task learning,
carefully designing interaction strategies between anomaly detection and root
cause localization at the data, feature, and diagnostic result levels, thereby
achieving both sub-tasks interactively within a unified end-to-end framework.
Evaluation on two public datasets and one industrial dataset shows that Chimera
outperforms existing methods in both anomaly detection and root cause
localization, achieving improvements of over 2.92% - 5.00% and 19.01% - 37.09%,
respectively. It has been successfully deployed in production, serving an
industrial cloud platform.

</details>


### [23] [Agentic Services Computing](https://arxiv.org/abs/2509.24380)
*Shuiguang Deng,Hailiang Zhao,Ziqi Wang,Guanjie Cheng,Peng Chen,Wenzhuo Qian,Zhiwei Ling,Jianwei Yin,Albert Y. Zomaya,Schahram Dustdar*

Main category: cs.SE

TL;DR: Agentic Service Computing (ASC) 是一个新范式，将服务重新构想为智能、自适应和社会嵌入的实体，通过设计、部署、操作和演进四个核心阶段的生命周期框架来组织。


<details>
  <summary>Details</summary>
Motivation: LLM驱动的智能代理正在推动服务计算从静态的请求-响应功能向动态、目标导向和自主的多代理生态系统转变，需要新的计算范式来适应这一变革。

Method: 提出了一个生命周期驱动的ASC框架，围绕四个核心阶段构建，并通过四个基础研究维度（感知与环境建模、自主决策与任务执行、多代理协作与组织、评估与信任）进行系统分析。

Result: 研究发现代理服务不是简单组装而是编排的：上下文感知支持稳健部署，自主推理支持实时操作，协作结构通过交互演进，可信度是贯穿整个生命周期的关键要求。

Conclusion: 通过将传统服务计算原则与基于LLM的多代理系统进展相结合，ASC为开发自适应、可问责和以人为中心的智能服务建立了整体性和前瞻性的基础。

Abstract: The rise of LLM-powered agents is driving a fundamental transformation in
services computing: from static, request-response functions to dynamic,
goal-oriented, and autonomous multi-agent ecosystems. In response to this
shift, we introduce Agentic Service Computing (ASC), a new paradigm that
reimagines services as intelligent, self-adaptive, and socially embedded
entities. This comprehensive survey presents a lifecycle-driven framework for
ASC, structured around four core phases: Design, Deployment, Operation, and
Evolution. We systematically analyze ASC through four foundational research
dimensions: (1) Perception, Context, and Environment Modeling, (2) Autonomous
Decision-Making and Task Execution, (3) Multi-Agent Collaboration and
Organization, and (4) Evaluation, Value Alignment, and Trustworthiness. We
examine how these dimensions are instantiated, integrated, and continuously
adapted across the service lifecycle. Our synthesis reveals that agentic
services are not merely assembled but orchestrated: contextual awareness
enables robust deployment; autonomous reasoning supports real-time operation;
collaborative structures emerge and evolve through interaction; and
trustworthiness must be upheld as a cross-cutting, lifelong imperative. We
further identify and discuss emerging trends shaping the future of ASC. By
integrating classical principles of services computing with advances in
LLM-based multi-agent systems, this work establishes a holistic and
forward-looking foundation for ASC. It provides a unified reference for
researchers and practitioners aiming to develop adaptive, accountable, and
human-centered intelligent services.

</details>


### [24] [Unit Test Update through LLM-Driven Context Collection and Error-Type-Aware Refinement](https://arxiv.org/abs/2509.24419)
*Yuanhe Zhang,Zhiquan Yang,Shengyi Pan,Zhongxin Liu*

Main category: cs.SE

TL;DR: TESTUPDATER是一个基于LLM的自动化单元测试更新方法，能够同时修复和增强测试用例，显著提高了测试更新的正确性和覆盖率。


<details>
  <summary>Details</summary>
Motivation: 传统手动维护单元测试效率低且容易遗漏修复，现有自动化方法主要关注测试修复而忽略测试增强，且缺乏验证机制导致正确率低。

Method: 利用LLM分析代码变更并提取相关上下文，通过精心设计的提示词逐步指导LLM处理各种代码变更类型，引入错误类型感知的迭代优化机制来执行和修复测试失败。

Result: 在新建基准UPDATES4J上达到94.4%的编译通过率和86.7%的测试通过率，分别比现有最佳方法SYNTER高出15.9%和20.0%，分支覆盖率和行覆盖率也分别高出12.9%和15.2%。

Conclusion: TESTUPDATER有效解决了自动化测试维护中的关键挑战，为即时测试更新提供了可行的解决方案，显著提升了测试质量和代码覆盖率。

Abstract: Unit testing is critical for ensuring software quality and software system
stability. The current practice of manually maintaining unit tests suffers from
low efficiency and the risk of delayed or overlooked fixes. Therefore, an
automated approach is required to instantly update unit tests, with the
capability to both repair and enhance unit tests. However, existing automated
test maintenance methods primarily focus on repairing broken tests, neglecting
the scenario of enhancing existing tests to verify new functionality.
Meanwhile, due to their reliance on rule-based context collection and the lack
of verification mechanisms, existing approaches struggle to handle complex code
changes and often produce test cases with low correctness. To address these
challenges, we propose TESTUPDATER, a novel LLM based approach that enables
automated just-in-time test updates in response to production code changes.
TESTUPDATER first leverages the LLM to analyze code changes and identify
relevant context, which it then extracts and filters. Then, through carefully
designed prompts, TESTUPDATER guides the LLM step by step to handle various
types of code changes and introduce new dependencies, enabling both test repair
and enhancement. Finally, we introduce an error-type-aware iterative refinement
mechanism that executes the LLM-updated tests and repairs failures, which
significantly improves the overall correctness of test updates. Since existing
test repair datasets lack scenarios of test enhancement, we further construct a
new benchmark, UPDATES4J, with 195 real-world samples from 7 projects.
Experimental results show that TESTUPDATER achieves a compilation pass rate of
94.4% and a test pass rate of 86.7%, outperforming the state-of-the-art method
SYNTER by 15.9% and 20.0%, respectively. Furthermore, TESTUPDATER exhibits
12.9% higher branch coverage and 15.2% greater line coverage than SYNTER.

</details>


### [25] [Towards Shift-Up: A Framework and a Prestudy on High-Value Activities in GenAI Native Software Development](https://arxiv.org/abs/2509.24485)
*Vlad Stirbu,Mateen Ahmed Abbasi,Teerath Das,Jesse Haimi,Niko Iljin,Pyry Kotilainen,Petrus Lipsanen,Niko Mäkitalo,Maiju Sipilä,Venla Veijalainen,Tommi Mikkonen*

Main category: cs.SE

TL;DR: 提出了名为shift-up的GenAI原生开发框架，帮助软件团队专注于高价值工作，同时获得GenAI支持


<details>
  <summary>Details</summary>
Motivation: 生成式AI工具正在改变软件工程，基于提示的专门代理正在取代人类开发者，需要新的开发框架来适应这种转变

Method: 提出了shift-up框架，并通过初步研究使用现有GenAI工具测试这些想法

Result: 提出了一个初步的GenAI原生开发框架，并进行了概念验证

Conclusion: 需要进一步研究来详细探索shift-up框架的应用和效果

Abstract: Generative AI (GenAI) has significantly influenced software engineering.
Associated tools have created a shift in software engineering, where
specialized agents, based on user-provided prompts, are replacing human
developers. In this paper, we propose a framework for GenAI native development
that we call \textit{shift-up}, which helps software teams focus on high-value
work while being supported by GenAI. Furthermore, we also present a preliminary
study testing these ideas with current GenAI tools. Towards the end of the
paper, we propose future research goals to study shift-up in more detail.

</details>


### [26] [JSProtect: A Scalable Obfuscation Framework for Mini-Games in WeChat](https://arxiv.org/abs/2509.24498)
*Zhihao Li,Chaozheng Wang,Zongjie Li,Xinyong Peng,Zelin Su,Qun Xia,Haochuan Lu,Ting Xiong,Man Ho Lam,Shuzheng Gao,Yuchong Xie,Cuiyun Gao,Shuai Wang,Yuetang Deng,Huafeng Ma*

Main category: cs.SE

TL;DR: JSProtect是一个针对微信小游戏生态的高吞吐量并行化JavaScript混淆框架，解决了现有工具处理大型应用时的性能瓶颈和代码膨胀问题


<details>
  <summary>Details</summary>
Motivation: 微信小游戏生态系统面临知识产权被盗用的问题，现有JavaScript混淆工具在处理大规模应用时存在处理时间长、运行时性能下降严重和代码膨胀不可持续等根本性限制

Method: 提出了并行感知作用域分析(PASA)算法，实现两个关键优化：独立代码分区支持多核处理，以及独立命名空间管理通过重用短标识符来对抗代码膨胀

Result: JSProtect能在几分钟内处理20MB代码库，保持100%语义等价，将代码膨胀控制在仅20%（基准工具超过1000%），保持接近原生运行时性能，并提供对抗静态分析工具和大语言模型的优越安全有效性

Conclusion: 这项工作提出了工业级JavaScript保护的新范式，有效平衡了强大安全性与高性能和可扩展性

Abstract: The WeChat mini-game ecosystem faces rampant intellectual property theft to
other platforms via secondary development, yet existing JavaScript obfuscation
tools are ill-equipped for large-scale applications, suffering from prohibitive
processing times, severe runtime performance degradation, and unsustainable
code size inflation. This paper introduces JSProtect, a high-throughput
parallelized obfuscation framework designed to overcome these fundamental
limitations. At the core of our framework is the Parallel-Aware Scope Analysis
(PASA) algorithm, which enables two key optimizations: independent code
partitioning for multi-core processing and independent namespace management
that aggressively reuses short identifiers to combat code bloat. Our evaluation
demonstrates that JSProtectprocesses 20MB codebases in minutes, maintaining
100\% semantic equivalence while controlling code size inflation to as low as
20\% compared to over 1,000\% with baseline tools. Furthermore, it preserves
near-native runtime performance and provides superior security effectiveness
against both static analysis tools and large language models. This work
presents a new paradigm for industrial-scale JavaScript protection that
effectively balances robust security with high performance and scalability.

</details>


### [27] [SemGuard: Real-Time Semantic Evaluator for Correcting LLM-Generated Code](https://arxiv.org/abs/2509.24507)
*Qinglin Wang,Zhihong Sun,Ruyun Wang,Tao Huang,Zhi Jin,Ge Li,Chen Lyu*

Main category: cs.SE

TL;DR: SemGuard是一个语义评估驱动的框架，通过实时行级语义监督在代码生成过程中检测和修复语义错误，无需执行程序或测试用例，显著降低语义错误率。


<details>
  <summary>Details</summary>
Motivation: 现有LLM生成的代码中语义错误（编译通过但行为错误）占多数，传统后修复方法存在延迟、依赖不完整测试套件和错误定位不准的问题，需要在解码过程中注入早期精确的语义信号。

Method: 构建SemDiff数据集进行细粒度标注，训练语义评估器嵌入LLM解码器，实时监控部分代码并标记偏差，回滚到错误行并指导重新生成。

Result: 在四个基准测试中均优于最先进基线，在SemDiff上相对ROCODE降低19.86%语义错误率，在LiveCodeBench上提升48.92% Pass@1，在不同模型和语言上均有效。

Conclusion: SemGuard通过实时语义监督有效解决了LLM代码生成中的语义错误问题，具有模型和语言无关的通用有效性。

Abstract: Large Language Models (LLMs) can translate natural language requirements into
code, yet empirical analyses of representative models reveal that semantic
errors-programs that compile but behave incorrectly-constitute the majority of
observed faults (e.g., >60% on DeepSeek-Coder-6.7B and QwenCoder-7B). Post-hoc
repair pipelines detect such faults only after execution, incurring latency,
relying on incomplete test suites, and often mis-localizing the defect. Since
semantic drift originates in the autoregressive decoding process, intervening
while the code is being generated is a direct way to stop error propagation.
Constrained-decoding approaches such as ROCODE attempt this, but still wait
until the entire program runs to obtain feedback and use entropy heuristics
that do not truly capture semantics. A more effective solution must inject
semantic signals-early and precisely-into the decoding process.We present
SemGuard, a semantic-evaluator-driven framework that performs real-time,
line-level semantic supervision. To train the evaluator, we build SemDiff, the
first dataset with fine-grained annotations that mark the exact line where a
correct and an incorrect implementation diverge. The evaluator, once embedded
in the LLM's decoder, flags deviations on partial code, rolls back to the
faulty line, and guides regeneration-without executing the program or requiring
test cases. Across four benchmarks, SemGuard consistently outperforms
state-of-the-art baselines. It lowers the semantic error rate by 19.86% on
SemDiff relative to ROCODE, and lifts Pass@1 by 48.92% on the real-world
LiveCodeBench with CodeLlama-7B. Similar gains hold for StarCoder2-7B on MBPP
and for DeepSeekCoder-6.7B on the Java benchmark SemDiff-Java, demonstrating
model- and language-agnostic effectiveness.

</details>


### [28] [Agentic Specification Generator for Move Programs](https://arxiv.org/abs/2509.24515)
*Yu-Fu Fu,Meng Xu,Taesoo Kim*

Main category: cs.SE

TL;DR: MSG是一个为Move智能合约设计的自动化规范生成工具，展示了LLM对新兴非主流语言的强大代码理解和规范生成能力，成功为84%的测试函数生成可验证规范。


<details>
  <summary>Details</summary>
Motivation: 现有LLM规范生成工具主要关注主流编程语言，而忽略了新兴且面向验证的Move语言，需要探索LLM在新生态系统中的应用潜力。

Method: 采用基于代理的模块化设计，显式利用规范语言特性，并整合验证工具链的反馈机制。

Result: 成功为84%的Move函数生成可验证规范，识别出专家遗漏的条款，比传统设计多生成57%的可验证条款，整合反馈后生成可验证规范的能力提升30%。

Conclusion: LLM即使对非主流语言也表现出强大的代码理解和规范生成能力，模块化设计和工具链反馈能显著提升规范生成质量，为新兴语言生态系统提供了有效的规范生成解决方案。

Abstract: While LLM-based specification generation is gaining traction, existing tools
primarily focus on mainstream programming languages like C, Java, and even
Solidity, leaving emerging and yet verification-oriented languages like Move
underexplored. In this paper, we introduce MSG, an automated specification
generation tool designed for Move smart contracts. MSG aims to highlight key
insights that uniquely present when applying LLM-based specification generation
to a new ecosystem. Specifically, MSG demonstrates that LLMs exhibit robust
code comprehension and generation capabilities even for non-mainstream
languages. MSG successfully generates verifiable specifications for 84% of
tested Move functions and even identifies clauses previously overlooked by
experts. Additionally, MSG shows that explicitly leveraging specification
language features through an agentic, modular design improves specification
quality substantially (generating 57% more verifiable clauses than conventional
designs). Incorporating feedback from the verification toolchain further
enhances the effectiveness of MSG, leading to a 30% increase in generated
verifiable specifications.

</details>


### [29] [Bridging Developer Instructions and Code Completion Through Instruction-Aware Fill-in-the-Middle Paradigm](https://arxiv.org/abs/2509.24637)
*Zhensu Sun,Chengran Yang,Chao Peng,Pengfei Gao,Xiaoning Du,Li Li,David Lo*

Main category: cs.SE

TL;DR: IFIM是一种专门为代码补全模型设计的指令微调方法，通过在输入中显式加入指令部分，使模型能够有效利用开发者提供的自然语言指令，同时保持原有的代码补全能力。


<details>
  <summary>Details</summary>
Motivation: 现有代码LLM在代码补全中难以有效利用自然语言指令，而通用指令微调技术会降低填充中间(FIM)性能，需要在指令跟随和填充能力之间做出权衡。

Method: 提出IFIM方法，扩展传统FIM训练目标，引入(前缀、指令、后缀)三元组训练数据，使用GPT-4o构建大规模数据集生成简洁的意图导向指令。

Result: 在Deepseek-Coder和Qwen2.5-Coder上应用IFIM后，HumanEval-infilling的Pass@1分数从84.6%提升至93.6%，且不影响无指令时的原始FIM性能。

Conclusion: IFIM成功解决了代码补全中指令利用的难题，实现了指令跟随能力和填充能力的双重提升，无需在两者间做出取舍。

Abstract: Large Language Models (LLMs) have significantly advanced code completion, yet
they often fail when the developer's intent is underspecified in the code
context. To address this, developers usually add natural language instructions
(e.g., comments) into the code context to clarify their intent. However,
existing code LLMs applied for code completion systems merely undergo a
fill-in-the-middle (FIM) pre-training, which struggles to leverage this
information effectively due to the lack of instruction-like training data.
Existing instruction-tuning techniques, which improve instruction-following in
general code generation, paradoxically degrade FIM performance, forcing a
trade-off between instruction-following and infilling capabilities. To address
this gap, we introduce Instruction-aware Fill-in-the-Middle (IFIM), an
instruction-tuning method specifically designed to enhance FIM code completion
models. IFIM extends the conventional FIM training objective by incorporating
an explicit instruction section into the input, enabling the model to learn
from (prefix, instruction, suffix) triplets. This approach allows the model to
effectively leverage developer-provided directives while preserving its core
completion abilities when no instructions are present. To facilitate this, we
constructed a large-scale dataset by using GPT-4o to generate concise,
intent-focused instructions for code infilling examples. We evaluated IFIM by
applying it to two popular base models, Deepseek-Coder and Qwen2.5-Coder, on
the benchmarks derived from HumanEval-infilling and RepoMasterEval. The results
demonstrate that IFIM significantly improves instruction-following
capabilities, boosting the Pass@1 score from 84.6% to 93.6% on
HumanEval-infilling. Moreover, this enhancement does not compromise the models'
original performance on FIM code completion tasks with no instructions
provided.

</details>


### [30] [CoTune: Co-evolutionary Configuration Tuning](https://arxiv.org/abs/2509.24694)
*Gangda Xiong,Tao Chen*

Main category: cs.SE

TL;DR: CoTune是一个通过协同进化方法自动调整系统配置的工具，能够有效处理复杂的性能需求，相比现有调优器在90%的情况下表现最佳，效率提升显著。


<details>
  <summary>Details</summary>
Motivation: 现有调优器设计大多忽略了复杂性能要求的存在，简单假设更好的性能总是更受青睐，这不仅浪费了需求中的宝贵信息，还可能消耗大量资源来调优一个收益很小的目标。

Method: CoTune通过协同进化方法，创建一个辅助性能需求与配置共同进化，当目标性能需求变得无效或误导时提供协助，使调优能够在需求指导下进行同时对可能的危害保持鲁棒性。

Result: 在162个测试案例（9个系统和18个需求）上的实验结果显示，CoTune显著优于现有调优器，在90%的情况下排名最佳（其他调优器仅为0%-35%），整体改进高达2.9倍，同时效率更高。

Conclusion: CoTune通过协同进化方法成功解决了性能需求调优中的关键问题，能够在考虑性能需求的同时保持调优过程的鲁棒性和效率，为系统配置自动调优提供了有效的解决方案。

Abstract: To automatically tune configurations for the best possible system performance
(e.g., runtime or throughput), much work has been focused on designing
intelligent heuristics in a tuner. However, existing tuner designs have mostly
ignored the presence of complex performance requirements (e.g., the latency
shall ideally be 2 seconds), but simply assume that better performance is
always more preferred. This would not only waste valuable information in a
requirement but might also consume extensive resources to tune for a goal with
little gain. Yet, prior studies have shown that simply incorporating the
requirement as a tuning objective is problematic since the requirement might be
too strict, harming convergence; or its highly diverse satisfactions might lead
to premature convergence. In this paper, we propose CoTune, a tool that takes
the information of a given target performance requirement into account through
co-evolution. CoTune is unique in the sense that it creates an auxiliary
performance requirement to be co-evolved with the configurations, which assists
the target performance requirement when it becomes ineffective or even
misleading, hence allowing the tuning to be guided by the requirement while
being robust to its harm. Experiment results on 162 cases (nine systems and 18
requirements) reveal that CoTune considerably outperforms existing tuners,
ranking as the best for 90% cases (against the 0%--35% for other tuners) with
up to 2.9x overall improvements, while doing so under a much better efficiency.

</details>


### [31] [Large language models for behavioral modeling: A literature survey](https://arxiv.org/abs/2509.24782)
*Muhammad Laiq*

Main category: cs.SE

TL;DR: 本文对使用大语言模型(LLMs)进行行为建模(特别是用例图和序列图生成)的研究进行了系统综述，发现LLMs在该领域展现出良好前景，但当前研究主要基于GPT模型且缺乏专家评估。


<details>
  <summary>Details</summary>
Motivation: 虽然LLMs已被广泛用于行为建模，但缺乏对该领域研究的系统综述，需要为未来研究方向提供指导，并帮助从业者和教育者了解LLMs在行为建模辅助方面的有效性。

Method: 通过术语搜索筛选出14篇相关主要研究，对这些研究进行分析和综述。

Result: LLMs在自动生成用例图和序列图方面显示出有希望的结果，但当前文献大多缺乏基于专家的评估，且主要使用基于GPT的模型。

Conclusion: 未来工作应评估更广泛的LLMs用于行为建模，并让领域专家参与评估LLMs的输出。

Abstract: In recent years, large language models (LLMs) have been extensively utilized
for behavioral modeling, for example, to automatically generate sequence
diagrams. However, no overview of this work has been published yet. Such an
overview will help identify future research directions and inform practitioners
and educators about the effectiveness of LLMs in assisting behavioral modeling.
This study aims to provide an overview of the existing research on the use of
LLMs for behavioral modeling, particularly focusing on use case and sequence
diagrams. Through a term-based search, we filtered and identified 14 relevant
primary studies. Our analysis of the selected primary studies reveals that LLMs
have demonstrated promising results in automatically generating use case and
sequence diagrams. In addition, we found that most of the current literature
lacks expert-based evaluations and has mainly used GPT-based models. Therefore,
future work should evaluate a broader range of LLMs for behavioral modeling and
involve domain experts to evaluate the output of LLMs.

</details>


### [32] [Evaluating SAP Joule for Code Generation](https://arxiv.org/abs/2509.24828)
*Joshua Heisler,Johannes Reisinger,Andreas Fischer*

Main category: cs.SE

TL;DR: SAP Joule在JavaScript代码生成评估中排名第五，准确率达到80.49%，是首个对其代码生成能力进行比较性评估的研究。


<details>
  <summary>Details</summary>
Motivation: 评估SAP公司专有的生成模型Joule在JavaScript代码生成方面的能力，尽管该模型主要不是为SAP特定的ABAP代码生成而设计。

Method: 使用HumanEval-X JavaScript基准测试，将SAP Joule与29个其他模型进行比较评估。

Result: SAP Joule在评估中排名第五，严格准确率达到80.49%。

Conclusion: SAP Joule在JavaScript代码生成方面表现出色，这是对其代码生成能力的首次比较性评估。

Abstract: SAP has released its own proprietary generative model SAP Joule, intended for
various generative tasks, including serving as a code assistant for software
engineers. While Joule is yet not focused on SAP-specific ABAP code generation,
it can be used for other common languages, including Javascript. This paper
compares SAP Joules Javascript coding capabilities against a total of 29 other
models using the HumanEval-X Javascript benchmark. SAP Joule achieves a strict
accuracy of 80.49% as the fifth best model in our evaluation. To the best of
our knowledge, this is the first comparative evaluation of SAP Joule code
generation capabilities.

</details>


### [33] [DiffTester: Accelerating Unit Test Generation for Diffusion LLMs via Repetitive Pattern](https://arxiv.org/abs/2509.24975)
*Lekang Yang,Yuetong Liu,Yitong Zhang,Jia Li*

Main category: cs.SE

TL;DR: DiffTester是一个针对扩散语言模型的加速框架，通过动态识别单元测试中的重复结构模式，在不降低质量的前提下提升测试生成效率


<details>
  <summary>Details</summary>
Motivation: 现有LLM逐个token生成测试用例效率低下，扩散LLM虽有并行生成能力但在UTG应用中面临效率与质量的权衡问题

Method: 通过抽象语法树分析动态识别单元测试中的共同结构模式，自适应增加每步生成的token数量

Result: 在三个基准测试和两种代表性模型上显著加速测试生成，同时保持测试覆盖率，且在不同dLLM和编程语言上泛化良好

Conclusion: DiffTester为软件开发中的高效单元测试生成提供了实用且可扩展的解决方案

Abstract: Software development relies heavily on extensive unit testing, which makes
the efficiency of automated Unit Test Generation (UTG) particularly important.
However, most existing LLMs generate test cases one token at a time in each
forward pass, which leads to inefficient UTG. Recently, diffusion LLMs (dLLMs)
have emerged, offering promising parallel generation capabilities and showing
strong potential for efficient UTG. Despite this advantage, their application
to UTG is still constrained by a clear trade-off between efficiency and test
quality, since increasing the number of tokens generated in each step often
causes a sharp decline in the quality of test cases. To overcome this
limitation, we present DiffTester, an acceleration framework specifically
tailored for dLLMs in UTG. The key idea of DiffTester is that unit tests
targeting the same focal method often share repetitive structural patterns. By
dynamically identifying these common patterns through abstract syntax tree
analysis during generation, DiffTester adaptively increases the number of
tokens produced at each step without compromising the quality of the output. To
enable comprehensive evaluation, we extend the original TestEval benchmark,
which was limited to Python, by introducing additional programming languages
including Java and C++. Extensive experiments on three benchmarks with two
representative models show that DiffTester delivers significant acceleration
while preserving test coverage. Moreover, DiffTester generalizes well across
different dLLMs and programming languages, providing a practical and scalable
solution for efficient UTG in software development. Code and data are publicly
available at https://github.com/wellbeingyang/DLM4UTG-open .

</details>


### [34] [Large Language Models for Software Testing: A Research Roadmap](https://arxiv.org/abs/2509.25043)
*Cristian Augusto,Antonia Bertolino,Guglielmo De Angelis,Francesca Lonetti,Jesús Morán*

Main category: cs.SE

TL;DR: 本文对基于大语言模型的软件测试研究现状进行了系统性综述，通过半系统文献回顾方法梳理当前进展、分类研究成果，并展望未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在软件测试领域展现出巨大潜力，但缺乏对相关研究进展和趋势的结构化梳理，研究者难以把握前沿动态。

Method: 采用半系统文献回顾方法，收集相关论文并将其映射到主要类别，分析当前状态和开放挑战。

Result: 提供了基于LLM的软件测试研究路线图，将贡献分类并识别出最有前景的研究方向。

Conclusion: LLM将对整个软件测试领域产生长期影响，本文为研究者提供了结构化的研究现状概览和未来发展方向。

Abstract: Large Language Models (LLMs) are starting to be profiled as one of the most
significant disruptions in the Software Testing field.
  Specifically, they have been successfully applied in software testing tasks
such as generating test code, or summarizing documentation.
  This potential has attracted hundreds of researchers, resulting in dozens of
new contributions every month, hardening researchers to
  stay at the forefront of the wave. Still, to the best of our knowledge, no
prior work has provided a structured vision of the progress
  and most relevant research trends in LLM-based testing. In this article, we
aim to provide a roadmap that illustrates its current state,
  grouping the contributions into different categories, and also sketching the
most promising and active research directions for the field.
  To achieve this objective, we have conducted a semi-systematic literature
review, collecting articles and mapping them into the most
  prominent categories, reviewing the current and ongoing status, and analyzing
the open challenges of LLM-based software testing.
  Lastly, we have outlined several expected long-term impacts of LLMs over the
whole software testing field.

</details>


### [35] [Towards Reliable Generation of Executable Workflows by Foundation Models](https://arxiv.org/abs/2509.25117)
*Sogol Masoumzadeh,Keheliya Gallaba,Dayi Lin,Ahmed E. Hassan*

Main category: cs.SE

TL;DR: 提出了一个利用静态分析反馈来检测和修复基础模型生成的DSL工作流缺陷的框架，包括缺陷分类、静态分析器和修复工具


<details>
  <summary>Details</summary>
Motivation: 基础模型生成的领域特定语言工作流存在高缺陷率（87.27%），需要自动化方法来提高可靠性和准确性

Method: 开发了Timon静态分析器检测9类缺陷，并通过Pumbaa工具结合静态分析反馈来修复缺陷

Result: 成功识别了18种缺陷类型，静态分析能有效检测其中9类，并通过反馈机制实现缺陷修复

Conclusion: 该框架为实现从自然语言需求可靠自动生成可执行工作流提供了关键步骤

Abstract: Recent advancements in Foundation Models (FMs) have demonstrated significant
progress in comprehending complex natural language to perform intricate tasks.
Successfully executing these tasks often requires orchestrating calls to FMs
alongside other software components. However, manually decomposing a task into
a coherent sequence of smaller, logically aggregated steps, commonly referred
to as workflows, demands considerable effort and specialized domain knowledge.
While FMs can assist in generating such workflows specified in domain-specific
languages (DSLs), achieving accuracy and reliability in this process remains a
challenge.
  This work introduces a framework that leverages static analysis feedback to
enable FMs to detect and repair defects in the DSL-based workflows they
generate. We begin by presenting the first-ever taxonomy of incidences of
defects in FM-generated DSL workflows, categorizing them into 18 distinct
types. Furthermore, we observe a high prevalence of defects across FM-generated
DSL workflows, with 87.27% of the studied instances containing at least one
defect. This, in turn, emphasizes the magnitude of the problem in practice and
underscores the necessity for implementing mitigation strategies. Following
this, we demonstrate that nine types of these defects can be effectively
identified through static analysis of the workflows. For this purpose, we
develop Timon, the first-of-its-kind static analyzer specifically designed for
FM-generated DSL workflows. Finally, we show that by incorporating feedback
from Timon, we can guide Pumbaa, an FM-based tool, to repair the detected
defect incidences. By systematically detecting and repairing defects, our work
provides a crucial step towards the reliable and automated generation of
executable workflows from natural language requirements.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [36] [\textit{No One-Size-Fits-All}: A Workload-Driven Characterization of Bit-Parallel vs. Bit-Serial Data Layouts for Processing-using-Memory](https://arxiv.org/abs/2509.22980)
*Jingyao Zhang,Elaheh Sadredini*

Main category: cs.AR

TL;DR: 本文首次系统性地分析了PIM架构中两种基本数据布局(BP和BS)的性能差异，发现没有单一最优布局，选择取决于工作负载特性，并提出了基于工作负载的设计指南。


<details>
  <summary>Details</summary>
Motivation: PIM社区长期以来将位并行(BP)和位串行(BS)两种数据布局视为可互换的，缺乏系统性的工作负载驱动指导来帮助架构师选择最优布局。

Method: 开发了等面积、周期精确的BP和BS PIM架构模型，使用MIMDRAM的细粒度微工作负载和PIMBench套件的大规模应用进行全面评估。

Result: BP在控制流密集型、内存访问模式不规则的任务上表现优异，而BS在AI中常见的大规模并行、低精度计算(如INT4/INT8)方面具有显著优势。

Conclusion: 挑战了PIM数据布局的一刀切观点，为设计下一代工作负载感知和潜在混合PIM系统提供了原则性基础。

Abstract: Processing-in-Memory (PIM) is a promising approach to overcoming the
memory-wall bottleneck. However, the PIM community has largely treated its two
fundamental data layouts, Bit-Parallel (BP) and Bit-Serial (BS), as if they
were interchangeable. This implicit "one-layout-fits-all" assumption, often
hard-coded into existing evaluation frameworks, creates a critical gap:
architects lack systematic, workload-driven guidelines for choosing the optimal
data layout for their target applications.
  To address this gap, this paper presents the first systematic,
workload-driven characterization of BP and BS PIM architectures. We develop
iso-area, cycle-accurate BP and BS PIM architectural models and conduct a
comprehensive evaluation using a diverse set of benchmarks. Our suite includes
both fine-grained microworkloads from MIMDRAM to isolate specific operational
characteristics, and large-scale applications from the PIMBench suite, such as
the VGG network, to represent realistic end-to-end workloads.
  Our results quantitatively demonstrate that no single layout is universally
superior; the optimal choice is strongly dependent on workload characteristics.
BP excels on control-flow-intensive tasks with irregular memory access
patterns, whereas BS shows substantial advantages in massively parallel,
low-precision (e.g., INT4/INT8) computations common in AI. Based on this
characterization, we distill a set of actionable design guidelines for
architects. This work challenges the prevailing one-size-fits-all view on PIM
data layouts and provides a principled foundation for designing
next-generation, workload-aware, and potentially hybrid PIM systems.

</details>


### [37] [ASIC-based Compression Accelerators for Storage Systems: Design, Placement, and Profiling Insights](https://arxiv.org/abs/2509.23693)
*Tao Lu,Jiapin Wang,Yelin Shan,Xiangping Zhang,Xiang Chen*

Main category: cs.AR

TL;DR: 本文研究了硬件压缩解压处理单元(CDPU)在数据中心中的不同部署位置(外围、片上、存储内)对性能的影响，发现部署位置对吞吐量、延迟、功耗效率和实际应用加速都有显著影响。


<details>
  <summary>Details</summary>
Motivation: CPU执行无损压缩带来巨大计算开销，硬件CDPU可以缓解此问题，但最优算法选择、微架构设计和系统级部署策略尚未被充分理解。

Method: 设计了ASIC-based存储内CDPU，并与Intel QAT 8970和QAT 4xxx进行端到端对比评估，涵盖三种主要部署模式。

Result: 发现CDPU部署位置对吞吐量和延迟极其敏感；压缩效率与数据模式/布局强相关；微基准测试增益与实际应用加速存在差异；模块级与系统级功耗效率不一致；各种CDPU存在可扩展性和多租户干扰问题。

Conclusion: 这些发现促使需要重新思考超大规模存储基础设施中硬件压缩解压的跨层设计，采用部署位置感知的方法。

Abstract: Lossless compression imposes significant computational over head on
datacenters when performed on CPUs. Hardware compression and decompression
processing units (CDPUs) can alleviate this overhead, but optimal algorithm
selection, microarchitectural design, and system-level placement of CDPUs are
still not well understood. We present the design of an ASIC-based in-storage
CDPU and provide a comprehensive end-to-end evaluation against two leading ASIC
accelerators, Intel QAT 8970 and QAT 4xxx. The evaluation spans three dominant
CDPU placement regimes: peripheral, on-chip, and in-storage. Our results
reveal: (i) acute sensitivity of throughput and latency to CDPU placement and
interconnection, (ii) strong correlation between compression efficiency and
data patterns/layouts, (iii) placement-driven divergences between
microbenchmark gains and real-application speedups, (iv) discrepancies between
module and system-level power efficiency, and (v) scalability and multi-tenant
interference is sues of various CDPUs. These findings motivate a
placement-aware, cross-layer rethinking of hardware (de)compression for
hyperscale storage infrastructures.

</details>


### [38] [Enhanced Hybrid Temporal Computing Using Deterministic Summations for Ultra-Low-Power Accelerators](https://arxiv.org/abs/2509.22999)
*Sachin Sachdeva,Jincong Lu,Wantong Li,Sheldon X. -D. Tan*

Main category: cs.AR

TL;DR: 本文提出了一种精度增强的混合时序计算(E-HTC)框架，通过两种新型加法器方案(EMBA和DTSA)解决了传统HTC架构因多路复用器加法导致的精度损失问题，在保持超低功耗的同时显著提升了计算精度。


<details>
  <summary>Details</summary>
Motivation: 传统HTC架构虽然通过脉冲率和时序数据编码降低了开关活动和能耗，但由于采用MUX-based缩放加法导致精度损失，需要开发既能保持低功耗又能提高精度的解决方案。

Method: 提出了两种位流加法方案：精确多输入二进制累加器(EMBA)进行精确二进制累加，确定性阈值缩放加法器(DTSA)采用阈值逻辑进行缩放加法，并集成到支持单极和双极编码的MAC单元中。

Result: 在4x4 MAC测试中，单极模式下E-HTC与最先进的CBSC MAC的RMSE相当，比MUX-based HTC精度提高94%，功耗和面积分别降低23%和7%；双极模式下RMSE为2.09%，比MUX-based HTC提升83%。在FIR和DCT/iDCT实验中均显示出显著的PSNR提升和功耗面积节省。

Conclusion: E-HTC框架成功解决了传统HTC架构的精度问题，在保持超低功耗优势的同时显著提升了计算精度，为超低功耗硬件加速器提供了有效的解决方案。

Abstract: This paper presents an accuracy-enhanced Hybrid Temporal Computing (E-HTC)
framework for ultra-low-power hardware accelerators with deterministic
additions. Inspired by the recently proposed HTC architecture, which leverages
pulse-rate and temporal data encoding to reduce switching activity and energy
consumption but loses accuracy due to its multiplexer (MUX)-based scaled
addition, we propose two bitstream addition schemes: (1) an Exact
Multiple-input Binary Accumulator (EMBA), which performs precise binary
accumulation, and (2) a Deterministic Threshold-based Scaled Adder (DTSA),
which employs threshold logic for scaled addition. These adders are integrated
into a multiplier accumulator (MAC) unit supporting both unipolar and bipolar
encodings. To validate the framework, we implement two accelerators: a Finite
Impulse Response (FIR) filter and an 8-point Discrete Cosine Transform
(DCT)/iDCT engine. Results on a 4x4 MAC show that, in unipolar mode, E-HTC
matches the RMSE of state-of-the-art Counter-Based Stochastic Computing (CBSC)
MAC, improves accuracy by 94% over MUX-based HTC, and reduces power and area by
23% and 7% compared to MUX-based HTC and 64% and 74% compared to CBSC. In
bipolar mode, E-HTC MAC achieves 2.09% RMSE -- an 83% improvement over
MUX-based HTC -- and approaches CBSC's 1.40% RMSE with area and power savings
of 28% and 43% vs. MUX-based HTC and about 76% vs. CBSC. In FIR experiments,
both E-HTC variants yield PSNR gains of 3--5 dB (30--45% RMSE reduction) while
saving 13% power and 3% area. For DCT/iDCT, E-HTC boosts PSNR by 10--13 dB
(70--75% RMSE reduction) while saving area and power over both MUX- and
CBSC-based designs.

</details>


### [39] [A Near-Cache Architectural Framework for Cryptographic Computing](https://arxiv.org/abs/2509.23179)
*Jingyao Zhang,Elaheh Sadredini*

Main category: cs.AR

TL;DR: 提出名为Crypto-Near-Cache (CNC)的近缓存切片计算范式，通过SRAM阵列和位线计算能力加速后量子密码算法，解决缓存带宽瓶颈问题。


<details>
  <summary>Details</summary>
Motivation: 后量子密码算法的公钥和签名长度是前量子密码的3-9倍，导致显著的性能和能效开销，缓存带宽成为关键瓶颈。

Method: 在缓存切片附近放置具有位线计算能力的SRAM阵列，实现高内部带宽和短数据移动，支持虚拟寻址，并提出ISA扩展。

Result: CNC范式能够提供高性能、卓越的能效和灵活性来加速后量子密码算法和其他应用。

Conclusion: CNC是一种创新的近缓存计算解决方案，能够无缝集成到现有系统中，解决后量子密码算法的性能和能效问题。

Abstract: Recent advancements in post-quantum cryptographic algorithms have led to
their standardization by the National Institute of Standards and Technology
(NIST) to safeguard information security in the post-quantum era. These
algorithms, however, employ public keys and signatures that are 3 to 9$\times$
longer than those used in pre-quantum cryptography, resulting in significant
performance and energy efficiency overheads. A critical bottleneck identified
in our analysis is the cache bandwidth. This limitation motivates the adoption
of on-chip in-/near-cache computing, a computing paradigm that offers
high-performance, exceptional energy efficiency, and flexibility to accelerate
post-quantum cryptographic algorithms. Our analysis of existing works reveals
challenges in integrating in-/near-cache computing into modern computer systems
and performance limitations due to external bandwidth limitation, highlighting
the need for innovative solutions that can seamlessly integrate into existing
systems without performance and energy efficiency issues. In this paper, we
introduce a near-cache-slice computing paradigm with support of customization
and virtual address, named Crypto-Near-Cache (CNC), designed to accelerate
post-quantum cryptographic algorithms and other applications. By placing SRAM
arrays with bitline computing capability near cache slices, high internal
bandwidth and short data movement are achieved with native support of virtual
addressing. An ISA extension to facilitate CNC is also proposed, with detailed
discussion on the implementation aspects of the core/cache datapath.

</details>


### [40] [AssertGen: Enhancement of LLM-aided Assertion Generation through Cross-Layer Signal Bridging](https://arxiv.org/abs/2509.23674)
*Hongqin Lyu,Yonghao Wang,Yunlin Du,Mingyu Shi,Zhiteng Chao,Wenxing Li,Tiancheng Wang,Huawei Li*

Main category: cs.AR

TL;DR: AssertGen是一个基于LLM的断言生成框架，通过链式思维提取验证目标并构建跨层信号链，显著提升SystemVerilog断言生成质量


<details>
  <summary>Details</summary>
Motivation: 现有LLM辅助断言生成方法无法有效识别设计规范与RTL设计之间的关系，导致生成的断言不充分

Method: 使用链式思维策略从规范中提取验证目标，构建跨层信号链连接目标与RTL信号，最后基于LLM生成SystemVerilog断言

Result: 在形式属性验证通过率、影响锥、证明核心和变异测试覆盖率等关键指标上优于现有最先进方法

Conclusion: AssertGen通过建立规范与RTL之间的明确联系，有效解决了断言生成中的关系识别问题，显著提升了断言质量

Abstract: Assertion-based verification (ABV) serves as a crucial technique for ensuring
that register-transfer level (RTL) designs adhere to their specifications.
While Large Language Model (LLM) aided assertion generation approaches have
recently achieved remarkable progress, existing methods are still unable to
effectively identify the relationship between design specifications and RTL
designs, which leads to the insufficiency of the generated assertions. To
address this issue, we propose AssertGen, an assertion generation framework
that automatically generates SystemVerilog assertions (SVA). AssertGen first
extracts verification objectives from specifications using a chain-of-thought
(CoT) reasoning strategy, then bridges corresponding signals between these
objectives and the RTL code to construct a cross-layer signal chain, and
finally generates SVAs based on the LLM. Experimental results demonstrate that
AssertGen outperforms the existing state-of-the-art methods across several key
metrics, such as pass rate of formal property verification (FPV), cone of
influence (COI), proof core and mutation testing coverage.

</details>


### [41] [AssertFix: Empowering Automated Assertion Fix via Large Language Models](https://arxiv.org/abs/2509.23972)
*Hongqin Lyu,Yunlin Du,Yonghao Wang,Zhiteng Chao,Tiancheng Wang,Huawei Li*

Main category: cs.AR

TL;DR: AssertFix是一个基于大语言模型的自动断言修复框架，能够自动定位RTL代码中的错误断言，识别根本原因并应用专门的修复策略，显著提高断言质量和验证覆盖率。


<details>
  <summary>Details</summary>
Motivation: 传统手动编写和维护SystemVerilog断言(SVA)既困难又容易出错，现有的基于LLM的断言生成方法将修复错误断言的负担留给人工处理，限制了这些方法的应用。

Method: 提出AssertFix框架，通过LLM自动定位与错误断言相关的RTL代码，系统识别断言错误的根本原因，分类错误类型，并应用专门的修复策略自动纠正错误。

Result: 在Opencore基准测试中，AssertFix在修复率和验证覆盖率方面都取得了显著改进。

Conclusion: AssertFix框架能够有效自动化断言修复过程，提高生成的断言质量，为基于LLM的断言生成方法提供了实用的自动化解决方案。

Abstract: Assertion-based verification (ABV) is critical in ensuring that
register-transfer level (RTL) designs conform to their functional
specifications. SystemVerilog Assertions (SVA) effectively specify design
properties, but writing and maintaining them manually is challenging and
error-prone. Although recent progress of assertion generation methods
leveraging large language models (LLMs) have shown great potential in improving
assertion quality, they typically treat assertion generation as a final step,
leaving the burden of fixing of the incorrect assertions to human effects,
which may significantly limits the application of these methods. To address the
above limitation, we propose an automatic assertion fix framework based on
LLMs, named AssertFix. AsserFix accurately locates the RTL code related to the
incorrect assertion, systematically identifies the root causes of the assertion
errors, classifies the error type and finally applies dedicated fix strategies
to automatically correct these errors, improving the overall quality of the
generated assertions. Experimental results show that AssertFix achieves
noticeable improvements in both fix rate and verification coverage across the
Opencore benchmarks.

</details>


### [42] [Fault Injection in On-Chip Interconnects: A Comparative Study of Wishbone, AXI-Lite, and AXI](https://arxiv.org/abs/2509.24929)
*Hongwei Zhao,Vianney Lapotre,Guy Gogniat*

Main category: cs.AR

TL;DR: 通过仿真驱动的故障注入分析三种主流总线协议（Wishbone、AXI Lite和AXI）的漏洞特征，揭示总线在故障攻击下的行为模式


<details>
  <summary>Details</summary>
Motivation: 随着SoC复杂度增加，片上通信总线作为IP核间互联的关键组件，成为故障注入攻击的潜在目标，需要系统性地评估其安全性

Method: 采用仿真驱动的故障注入方法，对三种总线协议进行系统性测试，分析故障成功率、空间漏洞分布和时间依赖性

Result: 发现了跨协议的一致性行为模式，为攻击建模和弹性SoC设计提供了实用见解

Conclusion: 总线协议在面对故障注入攻击时存在系统性漏洞，研究结果为SoC安全设计和攻击防护提供了重要参考

Abstract: Fault injection attacks exploit physical disturbances to compromise the
functionality and security of integrated circuits. As System on Chip (SoC)
architectures grow in complexity, the vulnerability of on chip communication
fabrics has become increasingly prominent. Buses, serving as interconnects
among various IP cores, represent potential vectors for fault-based
exploitation. In this study, we perform simulation-driven fault injection
across three mainstream bus protocols Wishbone, AXI Lite, and AXI. We
systematically examine fault success rates, spatial vulnerability
distributions, and timing dependencies to characterize how faults interact with
bus-level transactions. The results uncover consistent behavioral patterns
across protocols, offering practical insights for both attack modeling and the
development of resilient SoC designs.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [43] [Bridging Language Models and Formal Methods for Intent-Driven Optical Network Design](https://arxiv.org/abs/2509.22834)
*Anis Bekri,Amar Abane,Abdella Battou,Saddek Bensalem*

Main category: cs.NI

TL;DR: 提出结合LLM意图解析、形式化方法和光学RAG的混合管道，将自然语言意图转化为可验证的光网络拓扑设计


<details>
  <summary>Details</summary>
Motivation: 解决LLM在翻译自然语言意图到光网络拓扑时存在的模糊性和缺乏严谨性问题，确保关键任务网络的可靠性

Method: 集成LLM意图解析、形式化方法和光学检索增强生成(RAG)，通过领域特定的光学标准丰富设计决策，并系统性地结合符号推理和验证技术

Result: 生成可解释、可验证且可信的光网络设计，显著提升意图驱动网络的可靠性和正确性

Conclusion: 该方法通过形式化验证和领域知识整合，为关键任务网络提供了可靠的意图到拓扑转换解决方案

Abstract: Intent-Based Networking (IBN) aims to simplify network management by enabling
users to specify high-level goals that drive automated network design and
configuration. However, translating informal natural-language intents into
formally correct optical network topologies remains challenging due to inherent
ambiguity and lack of rigor in Large Language Models (LLMs). To address this,
we propose a novel hybrid pipeline that integrates LLM-based intent parsing,
formal methods, and Optical Retrieval-Augmented Generation (RAG). By enriching
design decisions with domain-specific optical standards and systematically
incorporating symbolic reasoning and verification techniques, our pipeline
generates explainable, verifiable, and trustworthy optical network designs.
This approach significantly advances IBN by ensuring reliability and
correctness, essential for mission-critical networking tasks.

</details>


### [44] [Impact of Environmental Factors on LoRa 2.4 GHz Time of Flight Ranging Outdoors](https://arxiv.org/abs/2509.23125)
*Yiqing Zhou,Xule Zhou,Zecan Cheng,Chenao Lu,Junhan Chen,Jiahong Pan,Yizhuo Liu,Sihao Li,Kyeong Soo Kim*

Main category: cs.NI

TL;DR: 论文提出了一个LoRa 2.4 GHz射频飞行时间测距数据集，用于研究环境因素（温湿度）对室外定位精度的影响，并通过DNN模型验证了环境因素对测距准确性的显著影响。


<details>
  <summary>Details</summary>
Motivation: 在WSN/IoT中，节点定位对长期环境监测至关重要，但现有LoRa 2.4 GHz数据集覆盖范围有限且未考虑温湿度等环境因素的变化影响。

Method: 在西交利物浦大学南校区体育场布置3x3网格覆盖400平方米区域，使用三个LoRa节点与基站进行三周的射频飞行时间测距数据采集，同时记录温湿度数据，并基于ESP32平台构建传输系统。

Result: 基于简单深度神经网络模型的初步研究表明，温度和湿度等环境因素显著影响测距精度，需要开发先进的环境因素补偿方法。

Conclusion: 室外LoRa射频飞行时间测距受环境因素影响较大，需要开发环境补偿方法来提高定位精度，该数据集为相关研究提供了重要基础。

Abstract: In WSN/IoT, node localization is essential to long-running applications for
accurate environment monitoring and event detection, often covering a large
area in the field. Due to the lower time resolution of typical WSN/IoT
platforms (e.g., 1 microsecond on ESP32 platforms) and the jitters in
timestamping, packet-level localization techniques cannot provide meter-level
resolution. For high-precision localization as well as world-wide
interoperability via 2.4-GHz ISM band, a new variant of LoRa, called LoRa 2.4
GHz, was proposed by semtech, which provides a radio frequency (RF) time of
flight (ToF) ranging method for meter-level localization. However, the existing
datasets reported in the literature are limited in their coverages and do not
take into account varying environmental factors such as temperature and
humidity. To address these issues, LoRa 2.4 GHz RF ToF ranging data was
collected on a sports field at the XJTLU south campus, where three LoRa nodes
logged samples of ranging with a LoRa base station, together with temperature
and humidity, at reference points arranged as a 3x3 grid covering 400 square
meter over three weeks and uploaded all measurement records to the base station
equipped with an ESP32-based transceiver for machine and user communications.
The results of a preliminary investigation based on a simple deep neural
network (DNN) model demonstrate that the environmental factors, including the
temperature and humidity, significantly affect the accuracy of ranging, which
calls for advanced methods of compensating for the effects of environmental
factors on LoRa RF ToF ranging outdoors.

</details>


### [45] [Unlicensed Band Allocation for Heterogeneous Networks](https://arxiv.org/abs/2509.23216)
*Po-Heng Chou*

Main category: cs.NI

TL;DR: 本文分析了LAA与Wi-Fi在非授权频段共存时的干扰问题，提出了四种频段分配方案并评估其性能，为LAA小基站设计提供指导。


<details>
  <summary>Details</summary>
Motivation: LAA和Wi-Fi在异构网络中共享非授权频段时会产生相互干扰，频段分配方案会显著影响两个系统的服务质量，需要研究最优的分配策略。

Method: 提出分析模型并进行仿真实验，研究四种非授权频段分配方案：UFA（完全分配）、UTA（时分分配）以及带缓冲机制的UFAB和UTAB方案。

Result: 评估了这些分配方案在LAA缓冲队列中LAA和Wi-Fi数据包接受率方面的性能表现。

Conclusion: 研究结果为LAA小基站的信道占用阶段和缓冲区大小设计提供了重要指导原则。

Abstract: Based on the License-Assisted Access (LAA) small cell architecture, the LAA
coexisting with Wi-Fi heterogeneous networks provides LTE mobile users with
high bandwidth efficiency as the unlicensed channels are shared among LAA and
Wi-Fi. However, LAA and Wi-Fi interfere with each other when both systems use
the same unlicensed channel in heterogeneous networks. In such a network,
unlicensed band allocation for LAA and Wi-Fi is an important issue that may
affect the quality of service (QoS) of both systems significantly. In this
paper, we propose an analytical model and conduct simulation experiments to
study four allocations for the unlicensed band: unlicensed full allocation
(UFA), unlicensed time-division allocation (UTA), and UFA/UTA with buffering
mechanism (UFAB and UTAB) for the LAA data packets. We evaluate the performance
of these unlicensed band allocation schemes in terms of the acceptance rate of
both LAA and Wi-Fi packet data in the LAA buffer queue. Our study provides
guidelines for designing the channel occupation phase and the buffer size of
the LAA small cell.

</details>


### [46] [Modeling the Unlicensed Band Allocation for LAA With Buffering Mechanism](https://arxiv.org/abs/2509.23217)
*Po-Heng Chou*

Main category: cs.NI

TL;DR: 研究LAA系统中基于先听后说机制和缓冲机制的未授权频段分配性能，通过分析模型和仿真实验评估LAA和Wi-Fi数据包的接受率


<details>
  <summary>Details</summary>
Motivation: 在异构网络中，LAA和Wi-Fi系统的未授权频段分配是一个重要问题，会显著影响两个系统的服务质量

Method: 提出分析模型并进行仿真实验，研究基于先听后说机制和缓冲机制的未授权频段分配方案

Result: 评估了LAA和Wi-Fi数据包的接受率性能

Conclusion: 为LAA系统的信道占用阶段和缓冲阈值设计提供了指导方针

Abstract: In this letter, we propose an analytical model and conduct simulation
experiments to study listen-before-talk-based unlicensed band allocation with
the buffering mechanism for the License-Assisted Access (LAA) packets in the
heterogeneous networks. In such a network, unlicensed band allocation for LAA
and Wi-Fi is an important issue, which may affect the quality of service for
both systems significantly. We evaluate the performance of these unlicensed
band allocations in terms of the acceptance rate of both LAA and Wi-Fi packets.
This letter provides the guidelines for designing the channel occupation phase
and buffer threshold of the LAA systems.

</details>


### [47] [Markov Modeling for Licensed and Unlicensed Band Allocation in Underlay and Overlay D2D](https://arxiv.org/abs/2509.23218)
*Po-Heng Chou,Yen-Ting Liu,Wei-Chang Chen,Walid Saad*

Main category: cs.NI

TL;DR: 提出了一种新颖的D2D辅助蜂窝网络资源分配分析模型，支持授权频段共享和蜂窝流量卸载，同时考虑与Wi-Fi系统的非授权频段共享问题。


<details>
  <summary>Details</summary>
Motivation: 解决D2D通信中资源分配问题，特别是在与Wi-Fi系统共享非授权频段时保证服务质量的需求。

Method: 建立全局系统状态模型反映D2D、蜂窝和Wi-Fi数据包的交互，采用基于阈值的流量控制保证Wi-Fi的QoS，并推导包阻塞概率。

Result: 仿真结果显示该方案在略微牺牲蜂窝性能的同时显著提升D2D性能，并保持Wi-Fi用户性能，相比underlay方案在D2D与Wi-Fi间具有更灵活的调节能力。

Conclusion: 提出的分析模型和流量控制方案能够有效平衡D2D、蜂窝和Wi-Fi系统的性能需求，提供了一种灵活的资源配置方法。

Abstract: In this paper, a novel analytical model for resource allocation is proposed
for a device-to-device (D2D) assisted cellular network. The proposed model can
be applied to underlay and overlay D2D systems for sharing licensed bands and
offloading cellular traffic. The developed model also takes into account the
problem of unlicensed band sharing with Wi-Fi systems. In the proposed model, a
global system state reflects the interaction among D2D, conventional cellular,
and Wi-Fi packets. Under the standard traffic model assumptions, a
threshold-based flow control is proposed for guaranteeing the
quality-of-service (QoS) of Wi-Fi. The packet blockage probability is then
derived. Simulation results show the proposed scheme sacrifices conventional
cellular performance slightly to improve overlay D2D performance significantly
while maintaining the performance for Wi-Fi users. Meanwhile, the proposed
scheme has more flexible adjustments between D2D and Wi-Fi than the underlay
scheme.

</details>


### [48] [A Modular KDN-Based Framework for IT/OT Autonomy in Industrial Systems](https://arxiv.org/abs/2509.23389)
*Tuğçe Bilen,Mehmet Ozdem*

Main category: cs.NI

TL;DR: 提出基于知识定义网络(KDN)的模块化框架，实现IT-OT基础设施的自适应自主控制，通过四模块闭环控制提升工业网络弹性、响应性和运行效率


<details>
  <summary>Details</summary>
Motivation: 传统基于规则或静态管理方法无法满足工业环境日益增长的复杂性、异构性和实时性需求，需要新的自适应控制方案

Method: 采用四模块架构：遥测收集器、知识构建器、决策引擎和控制执行器，形成闭环控制回路；使用基于图的系统状态表示和效用优化机制

Result: 评估了决策延迟、控制有效性和系统稳定性三个关键指标，证明能增强智能工业网络的弹性、响应性和运行效率

Conclusion: 该KDN框架为IT-OT融合环境提供了有效的自适应自主控制解决方案，能够应对动态工业环境的挑战

Abstract: The convergence of Information Technology (IT) and Operational Technology
(OT) is a critical enabler for achieving autonomous and intelligent industrial
systems. However, the increasing complexity, heterogeneity, and real-time
demands of industrial environments render traditional rule-based or static
management approaches insufficient. In this paper, we present a modular
framework based on the Knowledge-Defined Networking (KDN) paradigm, enabling
adaptive and autonomous control across IT-OT infrastructures. The proposed
architecture is composed of four core modules: Telemetry Collector, Knowledge
Builder, Decision Engine, and Control Enforcer. These modules operate in a
closed control loop to continuously observe system behavior, extract contextual
knowledge, evaluate control actions, and apply policy decisions across
programmable industrial endpoints. A graph-based abstraction is used to
represent system state, and a utility-optimization mechanism guides control
decisions under dynamic conditions. The framework's performance is evaluated
using three key metrics: decision latency, control effectiveness, and system
stability, demonstrating its capability to enhance resilience, responsiveness,
and operational efficiency in smart industrial networks.

</details>


### [49] [Knowledge-Defined and Twin-Assisted Network Management for 6G](https://arxiv.org/abs/2509.23398)
*Tuğçe Bilen,Mehmet Özdem*

Main category: cs.NI

TL;DR: 提出了一种集成数字孪生、语义推理和零样本学习的模块化知识定义架构，用于6G网络的自主决策管理，能够处理未见过的网络场景。


<details>
  <summary>Details</summary>
Motivation: 6G网络日益复杂、动态和异构的特性需要能够主动推理并超越预定义案例的管理系统，以应对未知网络场景的挑战。

Method: 采用数字孪生模型保持物理网络同步虚拟副本并预测状态转换，通过知识平面构建图网络抽象实现上下文感知意图生成，利用零样本策略匹配选择预学习动作。

Result: 仿真结果显示该框架在策略响应时间、SLA合规率和意图匹配准确性方面均有显著提升。

Conclusion: 该框架通过数字孪生、语义推理和零样本学习的结合，为6G网络提供了自适应、可扩展的自主管理解决方案。

Abstract: The increasing complexity, dynamism, and heterogeneity of 6G networks demand
management systems that can reason proactively and generalize beyond
pre-defined cases. In this paper, we propose a modular, knowledge-defined
architecture that integrates Digital Twin models with semantic reasoning and
zero-shot learning to enable autonomous decision-making for previously unseen
network scenarios. Real-time data streams are used to maintain synchronized
virtual replicas of the physical network, which also forecast short-term state
transitions. These predictions feed into a knowledge plane that constructs and
updates a graph-based abstraction of the network, enabling context-aware intent
generation via graph neural reasoning. To ensure adaptability without
retraining, the management plane performs zero-shot policy matching by
semantically embedding candidate intents and selecting suitable pre-learned
actions. The selected decisions are translated and enforced through the control
plane, while a closed-loop feedback mechanism continuously refines predictions,
knowledge, and policies over time. Simulation results confirm that the proposed
framework observes notable improvements in policy response time, SLA compliance
rate, and intent matching accuracy.

</details>


### [50] [AUV-Assisted Underwater 6G: Environmental Modeling and Multi-Stage Optimization](https://arxiv.org/abs/2509.23401)
*Mustafa Yavuz Engin,Mehmet Ozdem,Tuğçe Bilen*

Main category: cs.NI

TL;DR: 提出水下6G网络仿真模型，通过K-Means聚类结合遗传算法和粒子群优化算法优化传感器、AUV和集线器的部署位置，考虑温度、盐度等环境参数对电磁信号传输的影响。


<details>
  <summary>Details</summary>
Motivation: 水下6G网络部署面临环境参数复杂、信号衰减严重等挑战，需要开发能够综合考虑环境因素和网络性能的优化模型。

Method: 采用K-Means聚类进行初始分组，然后依次应用遗传算法(GA)和粒子群优化(PSO)优化集群配置，建立包含多跳传输、集群领导选择、队列管理和负载均衡的仿真模型。

Result: 仿真结果表明，通过考虑环境条件的水下6G网络架构可以有效建模和优化，有集群领导的场景性能更优。

Conclusion: 该研究证明了环境感知的水下6G网络优化方法的有效性，为实际部署提供了理论依据和仿真工具。

Abstract: This study presents a simulation model for underwater 6G networks, focusing
on the optimized placement of sensors, AUVs, and hubs. The network architecture
consists of fixed hub stations, mobile autonomous underwater vehicles (AUVs),
and numerous sensor nodes. Environmental parameters such as temperature,
salinity, and conductivity are considered in the transmission of
electromagnetic signals; signal attenuation and transmission delays are
calculated based on physical models. The optimization process begins with
K-Means clustering, followed by sequential application of Genetic Algorithm
(GA) and Particle Swarm Optimization (PSO) to refine the cluster
configurations. The simulation includes key network dynamics such as multi-hop
data transmission, cluster leader selection, queue management, and traffic load
balancing. To compare performance, two distinct scenarios -- one with cluster
leaders and one without -- are modeled and visualized through a PyQt5-based
real-time graphical interface. The results demonstrate that 6G network
architectures in underwater environments can be effectively modeled and
optimized by incorporating environmental conditions.

</details>


### [51] [Network Traffic Classification Using Self-Supervised Learning and Confident Learning](https://arxiv.org/abs/2509.23522)
*Ehsan Eslami,Walaa Hamouda*

Main category: cs.NI

TL;DR: 提出了一种结合自监督学习和置信学习的新框架，用于网络流量分类，减少对标注数据的依赖并提高准确性


<details>
  <summary>Details</summary>
Motivation: 传统网络流量分类方法在加密流量和动态端口分配场景下效果不佳，监督学习需要大量标注数据而难以获取，无监督学习方法准确率较低

Method: 使用自监督学习（如自动编码器或表格对比学习）从大量未标注数据生成伪标签，然后应用流量适应的置信学习来优化伪标签，减少噪声影响

Result: 在三个数据集上的实验表明，该方法相比最先进技术实现了更高的分类准确率

Conclusion: 该框架提供了一个通用解决方案，在最小化标注数据需求的同时提供高精度网络流量分类

Abstract: Network traffic classification (NTC) is vital for efficient network
management, security, and performance optimization, particularly with 5G/6G
technologies. Traditional methods, such as deep packet inspection (DPI) and
port-based identification, struggle with the rise of encrypted traffic and
dynamic port allocations. Supervised learning methods provide viable
alternatives but rely on large labeled datasets, which are difficult to acquire
given the diversity and volume of network traffic. Meanwhile, unsupervised
learning methods, while less reliant on labeled data, often exhibit lower
accuracy. To address these limitations, we propose a novel framework that first
leverages Self-Supervised Learning (SSL) with techniques such as autoencoders
or Tabular Contrastive Learning (TabCL) to generate pseudo-labels from
extensive unlabeled datasets, addressing the challenge of limited labeled data.
We then apply traffic-adopted Confident Learning (CL) to refine these
pseudo-labels, enhancing classification precision by mitigating the impact of
noise. Our proposed framework offers a generalizable solution that minimizes
the need for extensive labeled data while delivering high accuracy. Extensive
simulations and evaluations, conducted using three datasets (ISCX VPN-nonVPN,
self-generated dataset, and UCDavis--QUIC), and demonstrate that our method
achieves superior accuracy compared to state-of-the-art techniques in
classifying network traffic.

</details>


### [52] [Sim2Field: End-to-End Development of AI RANs for 6G](https://arxiv.org/abs/2509.23528)
*Russell Ford,Hao Chen,Pranav Madadi,Mandar Kulkarni,Xiaochuan Ma,Daoud Burghal,Guanbo Chen,Yeqing Hu,Chance Tarver,Panagiotis Skrimponis,Vitali Loseu,Yu Zhang,Yan Xin,Yang Li,Jianzhong Zhang,Shubham Khunteta,Yeswanth Guddeti Reddy,Ashok Kumar Reddy Chavva,Mahantesh Kothiwale,Davide Villa*

Main category: cs.NI

TL;DR: 本文提出了一种将AI引入真实无线网络的方法论，包括使用数字孪生模拟训练站点特定的AI物理层功能，以及构建强大的AI-RAN研究测试平台，最终通过商用UE实现了40%的吞吐量提升。


<details>
  <summary>Details</summary>
Motivation: 当前AI无线系统在真实环境中测试不足，模拟训练存在"现实差距"问题，且高性能原型实现成本高、复杂度大，阻碍了AI在5G/6G网络中的实际应用评估。

Method: 采用数字孪生模拟训练站点特定的AI物理层功能，构建AI-RAN研究测试平台支持快速原型开发、现场测试和数据收集，并通过商用用户设备评估AI信道估计算法。

Result: 通过空中测试验证，AI信道估计算法在真实环境中可实现高达40%的吞吐量提升，证明了AI在物理层应用的实质性性能增益。

Conclusion: 该方法成功解决了AI无线系统从模拟到实际部署的挑战，证明了数字孪生训练和测试平台的有效性，为AI在5G/6G网络中的实际应用提供了可行路径。

Abstract: Following state-of-the-art research results, which showed the potential for
significant performance gains by applying AI/ML techniques in the cellular
Radio Access Network (RAN), the wireless industry is now broadly pushing for
the adoption of AI in 5G and future 6G technology. Despite this enthusiasm,
AI-based wireless systems still remain largely untested in the field. Common
simulation methods for generating datasets for AI model training suffer from
"reality gap" and, as a result, the performance of these simulation-trained
models may not carry over to practical cellular systems. Additionally, the cost
and complexity of developing high-performance proof-of-concept implementations
present major hurdles for evaluating AI wireless systems in the field. In this
work, we introduce a methodology which aims to address the challenges of
bringing AI to real networks. We discuss how detailed Digital Twin simulations
may be employed for training site-specific AI Physical (PHY) layer functions.
We further present a powerful testbed for AI-RAN research and demonstrate how
it enables rapid prototyping, field testing and data collection. Finally, we
evaluate an AI channel estimation algorithm over-the-air with a commercial UE,
demonstrating that real-world throughput gains of up to 40% are achievable by
incorporating AI in the physical layer.

</details>


### [53] [Short-Term Guidance Algorithm on a Drone Road System](https://arxiv.org/abs/2509.23794)
*Zhouyu Qu,Andreas Willig,Xiaobing Wu*

Main category: cs.NI

TL;DR: 提出了一种无人机道路系统标记语言和去中心化贪婪引导算法，用于解决城市环境中无人机交通管理的碰撞避免和效率问题


<details>
  <summary>Details</summary>
Motivation: 随着无人机在城市环境中的使用增加，高密度无人机带来的碰撞风险和交通管理挑战需要有效的道路系统和引导算法来解决

Method: 开发了无人机道路系统描述标记语言，并提出短期去中心化贪婪(STDG)引导算法，仅使用附近无人机的位置和速度信息进行实时决策

Result: 通过仿真展示了关键无线和算法参数对无人机碰撞率、平均速度和系统吞吐量等性能指标的影响

Conclusion: 该去中心化方法使无人机能够独立运行，同时确保安全性和效率，相比依赖集中协调的现有方法具有优势

Abstract: Unmanned Aerial Vehicles (UAVs), commonly known as drones, have experienced
expanding use in urban environments in recent years. However, the growing
density of drones raises significant challenges, such as avoiding collisions
and managing air traffic efficiently, especially in congested areas. To address
these issues, a structured road system and an effective guidance algorithm are
essential. In this paper, we introduce a markup language allowing to describe
drone road systems (DRS), in which a road system is given by a set of
individual roads, each of which can have a varying number of lanes. Roads can
be linked through connecting lanes. Furthermore, we propose a novel short-term
decentralized greedy (STDG) guidance algorithm that uses only the position and
speed information of nearby drones -- communicated via periodically transmitted
beacons -- to make real-time decisions such as stopping, changing lanes, or
adjusting speed for the next few seconds. Unlike existing methods that rely on
centralized coordination, our algorithm enables drones to operate independently
while ensuring safety and efficiency. We present simulation results showing the
impact of key wireless and algorithm parameters on performance metrics like the
drone collision rate, average speed and throughput of the drone road system.

</details>


### [54] [A Synergy of Computing Power Networks and Low-Altitude Economy Intelligent Communications: Challenges, Design Principles, and Research Directions](https://arxiv.org/abs/2509.23810)
*Yan Sun,Yinqiu Liu,Shaoyong Guo,Ruichen Zhang,Jiacheng Wang,Xuesong Qiu,Geng Sun,Weifeng Gong,Dusit Niyato,Qihui Wu*

Main category: cs.NI

TL;DR: 本文探讨低空经济智能通信与算力网络的协同作用，分析两者如何相互增强以构建灵活、自适应的低空服务体系


<details>
  <summary>Details</summary>
Motivation: 低空经济智能通信面临计算能力有限、跨场景泛化不足和异构需求复杂等挑战，而算力网络也存在静态部署和适应性受限的问题，需要探索两者的协同解决方案

Method: 通过调研分析，研究算力网络如何支持低空智能通信（空-地协同控制、AI训练、通信计算协同优化等），以及低空通信如何增强算力网络（移动辅助控制、分布式智能训练等）

Result: 提出了CPN-LAE集成系统的设计原则和未来研究方向

Conclusion: 这项工作为构建灵活、自适应和弹性的架构提供了全面基础，通过算力网络与低空经济的协同作用，能够提供高质量和可持续的低空服务

Abstract: The rapid development of the Low-Altitude Economy (LAE) has created
opportunities for emerging services such as autonomous aerial transportation,
aerial sensing, and emergency response, all of which rely on efficient and
intelligent communications. However, LAE intelligent communications face
several challenges, including the limited computational capacity of aerial
nodes, the lack of cross-scenario generalization, and the complexity of
heterogeneous demands. Meanwhile, Computing Power Networks (CPNs) have emerged
as a new paradigm for integrating distributed computing, networking, and
storage resources, but they are also constrained by static deployment and
limited adaptability. In this survey, we explore the synergy between LAE
intelligent communications and CPNs. We first analyze how CPNs can support LAE
intelligent communications in areas such as air-ground collaborative control,
AI training, communication-computation co-ptimization, and ubiquitous
low-altitude information processing. Conversely, we discuss how LAE intelligent
communications can enhance CPNs through mobility-assisted control, distributed
intelligent training, dynamic routing, and in-network aerial computing.
Finally, based on these insights, we outline design principles and future
research directions for integrated CPN-LAE systems. This work provides a
comprehensive foundation for building flexible, adaptive, and resilient
architectures that leverage the synergy between CPNs and LAE to deliver
high-quality and sustainable low-altitude services.

</details>


### [55] [Continual Learning to Generalize Forwarding Strategies for Diverse Mobile Wireless Networks](https://arxiv.org/abs/2509.23913)
*Cheonjin Park,Victoria Manfredi,Xiaolan Zhang,Chengyi Liu,Alicia P Wolfe,Dongjin Song,Sarah Tasneem,Bing Wang*

Main category: cs.NI

TL;DR: 提出了一个深度强化学习框架，通过可泛化的基础模型和持续学习方法，解决移动无线网络中路由策略的泛化性问题


<details>
  <summary>Details</summary>
Motivation: 现有的深度强化学习方法在多跳移动无线网络中设计转发策略时，在训练环境显著不同的场景下缺乏泛化能力

Method: 开发了可泛化的基础模型，设计新特征来表征网络变化和特征质量，并采用持续学习方法避免灾难性遗忘

Result: 在未见过的移动场景中，相比最先进的启发式转发策略，延迟减少高达78%，交付率提高24%，转发次数相当或略高

Conclusion: 该方法能够有效泛化到不同的移动网络场景，显著提升网络性能

Abstract: Deep reinforcement learning (DRL) has been successfully used to design
forwarding strategies for multi-hop mobile wireless networks. While such
strategies can be used directly for networks with varied connectivity and
dynamic conditions, developing generalizable approaches that are effective on
scenarios significantly different from the training environment remains largely
unexplored. In this paper, we propose a framework to address the challenge of
generalizability by (i) developing a generalizable base model considering
diverse mobile network scenarios, and (ii) using the generalizable base model
for new scenarios, and when needed, fine-tuning the base model using a small
amount of data from the new scenarios. To support this framework, we first
design new features to characterize network variation and feature quality,
thereby improving the information used in DRL-based forwarding decisions. We
then develop a continual learning (CL) approach able to train DRL models across
diverse network scenarios without ``catastrophic forgetting.'' Using extensive
evaluation, including real-world scenarios in two cities, we show that our
approach is generalizable to unseen mobility scenarios. Compared to a
state-of-the-art heuristic forwarding strategy, it leads to up to 78% reduction
in delay, 24% improvement in delivery rate, and comparable or slightly higher
number of forwards.

</details>


### [56] [Performance Analysis of Zero-Forcing Beamforming Strategies for the Uplink of an MU-MIMO System with Multi-Antenna Users](https://arxiv.org/abs/2509.23921)
*João Paulo P. G. Marques,Catherine Rosenberg*

Main category: cs.NI

TL;DR: 评估OFDMA MU-MIMO上行链路中三种ZF波束成形策略的性能对比，提出基于贪婪搜索的启发式算法进行资源管理，发现在不同场景下BD和CTR1可替代更复杂的CTRF方案。


<details>
  <summary>Details</summary>
Motivation: 研究多天线用户OFDMA MU-MIMO系统的上行链路性能，比较三种ZF波束成形策略的有效性，解决由于功率管理导致的全时隙资源管理挑战。

Method: 提出基于贪婪上行搜索的启发式算法，在时隙级别操作，考虑公平性、实用调制编码方案和所有RRM过程，为流集提供可行解决方案。

Result: 在农村宏蜂窝场景中，用户数少时BD可替代CTRF，用户数多时CTR1可替代；在城市宏蜂窝场景中，CTR1因性能相近可作为CTRF的替代方案。系统参数对ZF策略性能影响显著，BD在简单功率管理方案下性能受损更严重。

Conclusion: 不同ZF波束成形策略在不同场景下各有优势，BD和CTR1在某些条件下可以替代更复杂的CTRF方案，系统参数和功率管理方案对性能有重要影响。

Abstract: We conduct a comprehensive evaluation of the performance of the uplink of
OFDMA-based MU-MIMO systems with multi-antenna users, for three Zero-Forcing
(ZF) Beamforming (BF) strategies: Coordinated-Transmit-Receive-1 (CTR1), where
only the strongest data stream is enabled per scheduled user; Block
Diagonalization (BD), where all possible streams are enabled per scheduled
user; Coordinated-Transmit-Receive-Flexible (CTRF), which allows a flexible
stream allocation per user. The Radio Resource Management (RRM) of the uplink
of all OFDMA-based systems must be done over an entire Time-Slot (TS) due to
power management, making it challenging. To enable this study, we propose an
efficient heuristic based on greedy-up searches for stream-sets that provides
feasible solutions. It operates over the TS and considers fairness, practical
Modulation and Coding Schemes and all RRM processes. The results show that, for
Rural Macro scenarios, BD (resp. CTR1) could replace the more complex CTRF if
the number of users is small (resp. large), while for Urban Macro scenarios,
CTR1 emerges as an alternative to CTRF due to its similar performance. We also
show that the system parameters can substantially impact the performance of the
ZF strategies and that BD performance is more impaired with a simpler power
management scheme than CTR1 and CTRF.

</details>


### [57] [Beyond Redundancy: Toward Agile Resilience in Optical Networks to Overcome Unpredictable Disasters](https://arxiv.org/abs/2509.24038)
*Toru Mano,Hideki Nishizawa,Takeo Sasai,Soichiroh Usui,Dmitrii Briantcev,Devika Dass,Brandt Bashaw,Eoin Kenny,Marco Ruffini,Yoshiaki Sone,Koichi Takasugi,Daniel Kilper*

Main category: cs.NI

TL;DR: 本文提出敏捷弹性概念，通过多运营商和多层动态适应性应对灾难不可预测性，验证了6小时内完成系统特性分析、光路配置和数据库迁移的可行性。


<details>
  <summary>Details</summary>
Motivation: 传统光网络弹性方法基于冗余和预规划恢复策略，假设灾难可预测。但气候变化、通信服务演进和地缘政治风险增加了灾难不可预测性，降低了传统方法的有效性。

Method: 引入敏捷弹性概念，强调跨多运营商和多层的动态适应性。提出关键需求、挑战和使能技术，并通过现场部署的传输系统进行验证。

Result: 在6小时内实现了快速系统特性分析、光路配置和数据库迁移，验证了所提使能技术的有效性。

Conclusion: 实验结果证实了敏捷弹性的可行性，为应对不可预测灾难提供了有效解决方案。

Abstract: Resilience in optical networks has traditionally relied on redundancy and
pre-planned recovery strategies, both of which assume a certain level of
disaster predictability. However, recent environmental changes such as climate
shifts, the evolution of communication services, and rising geopolitical risks
have increased the unpredictability of disasters, reducing the effectiveness of
conventional resilience approaches. To address this unpredictability, this
paper introduces the concept of agile resilience, which emphasizes dynamic
adaptability across multiple operators and layers. We identify key requirements
and challenges, and present enabling technologies for the realization of agile
resilience. Using a field-deployed transmission system, we demonstrate rapid
system characterization, optical path provisioning, and database migration
within six hours. These results validate the effectiveness of the proposed
enabling technologies and confirm the feasibility of agile resilience.

</details>


### [58] [Flexible and High-Performance Radio Access Networks for upcoming Sixth-Generation (6G) Systems](https://arxiv.org/abs/2509.24417)
*Peter Schefczik,Umar Toseef,Paolo Baracca,Ralf Klotsche,Torsten Dudda,Mai-Anh Phan,Lorenzo Miretti,David Ginthoer,Bin Han*

Main category: cs.NI

TL;DR: 6G-ANNA项目提出简化和自动化的6G无线接入网架构，通过分布式MIMO、载波聚合等技术提升效率和性能，采用云RAN实现可扩展性，并引入零信任安全框架。


<details>
  <summary>Details</summary>
Motivation: 现有RAN架构复杂且效率低下，无法满足未来大规模移动XR等应用的需求，需要重新设计6G RAN以实现运营和能源效率的提升。

Method: 提出灵活的6G RAN功能架构和协议栈，采用分布式MIMO和分布式载波聚合优化性能，通过云RAN和服务编排实现自适应和可扩展性。

Result: 开发了简化的6G RAN架构方案，能够更高效地处理多样化流量需求，同时通过零信任框架增强安全性。

Conclusion: 该6G RAN架构通过简化和自动化设计，在提升运营效率、能源效率和性能的同时，有效解决了安全风险问题，为未来6G网络发展提供了可行方案。

Abstract: The collaborative research project 6G-ANNA develops concepts for the 6G radio
access network (RAN) architecture and technology components. Previous RAN
generations have become inherently more complex and reach their limits in
handling foreseen future traffic demands with their diverse characteristics in
an efficient manner, e.g., for the use-case of mobile eXtended Reality (XR) on
a massive scale. One main objective of 6G is to regain both operational and
energy efficiency, i.e., by simplification and automation. To achieve this, in
this paper a flexible 6G RAN functional architecture and protocol stack as well
as implementation and deployment options are described. We outline how
performance is optimized by distributed Multiple Input Multiple Output (MIMO)
and distributed Carrier Aggregation (CA), and furthermore, how adaptiveness and
scalability is enabled by Cloud RAN and service orchestration. Finally, the
proposed zero-trust framework mitigates security risks in the described 6G RAN
architecture.

</details>


### [59] [Contrastive Learning for Correlating Network Incidents](https://arxiv.org/abs/2509.24446)
*Jeremias Dötterl*

Main category: cs.NI

TL;DR: 提出基于对比学习的自监督方法，用于网络事件相似性关联分析


<details>
  <summary>Details</summary>
Motivation: 网络服务提供商需要自动关联相似网络事件，但由于网络规模庞大，人工关联不可行

Method: 使用对比学习在大型未标记网络情境数据集上训练深度神经网络

Result: 在真实网络监控数据实验中实现了高精度

Conclusion: 对比学习是网络事件关联分析的有前景方法

Abstract: Internet service providers monitor their networks to detect, triage, and
remediate service impairments. When an incident is detected, it is important to
determine whether similar incidents have occurred in the past or are happening
concurrently elsewhere in the network. Manual correlation of such incidents is
infeasible due to the scale of the networks under observation, making automated
correlation a necessity. This paper presents a self-supervised learning method
for similarity-based correlation of network situations. Using this method, a
deep neural network is trained on a large unlabeled dataset of network
situations using contrastive learning. High precision achieved in experiments
on real-world network monitoring data suggests that contrastive learning is a
promising approach to network incident correlation.

</details>


### [60] [Blockchain-Driven Federation for Distributed Edge Systems: Design and Experimental Validation](https://arxiv.org/abs/2509.24846)
*Adam Zahir,Milan Groshev,Carlos J. Bernardos,Antonio de la Oliva*

Main category: cs.NI

TL;DR: 本文提出了一种基于区块链和智能合约的边缘计算联邦解决方案，能够在约18秒内自动化完成联邦生命周期管理，解决传统联邦方案依赖预建立协议和可信第三方的问题。


<details>
  <summary>Details</summary>
Motivation: 现有边缘计算联邦解决方案依赖预建立协议，存在操作复杂、手动操作延迟、高开销成本和依赖可信第三方等显著限制，需要一种动态、安全的联邦机制。

Method: 使用区块链和智能合约技术，基于ETSI多接入边缘计算框架，构建分布式MEC系统动态协商和执行联邦的安全、自动化、可扩展解决方案，并在私有以太坊区块链上进行性能评估。

Result: 实验结果表明，该解决方案能够在基线场景下约18秒内完成整个联邦生命周期（从协商到部署），在并发请求场景下能够高效扩展，多个MEC系统可以同时发起联邦请求。

Conclusion: 基于区块链的方法为处理边缘到云连续体中动态多域联邦的复杂性提供了一个有前景的方向，实现了安全、自动化的联邦协议动态创建和执行。

Abstract: Edge computing brings computation near end users, enabling the provisioning
of novel use cases. To satisfy end-user requirements, the concept of edge
federation has recently emerged as a key mechanism for dynamic resources and
services sharing across edge systems managed by different administrative
domains. However, existing federation solutions often rely on pre-established
agreements and face significant limitations, including operational complexity,
delays caused by manual operations, high overhead costs, and dependence on
trusted third parties. In this context, blockchain can create dynamic
federation agreements that enable service providers to securely interact and
share services without prior trust.
  This article first describes the problem of edge federation, using the
standardized ETSI multi-access edge computing framework as a reference
architecture, and how it is being addressed. Then, it proposes a novel solution
using blockchain and smart contracts to enable distributed MEC systems to
dynamically negotiate and execute federation in a secure, automated, and
scalable manner. We validate our framework's feasibility through a performance
evaluation using a private Ethereum blockchain, built on the open-source
Hyperledger Besu platform. The testbed includes a large number of MEC systems
and compares two blockchain consensus algorithms. Experimental results
demonstrate that our solution automates the entire federation lifecycle-from
negotiation to deployment-with a quantifiable overhead, achieving federation in
approximately 18 seconds in a baseline scenario. The framework scales
efficiently in concurrent request scenarios, where multiple MEC systems
initiate federation requests simultaneously. This approach provides a promising
direction for addressing the complexities of dynamic, multi-domain federations
across the edge-to-cloud continuum.

</details>


### [61] [Experimental Study of Magnetic Near-Field Microstrip Electronic Probe for PCB EMC Emission Measurement](https://arxiv.org/abs/2509.24944)
*Hongchuan Jia,Fayu Wan,Vladimir Mordachev,Jérôme Rossignol,Glauco Fontagalland,Nour Murad,Blaise Ravelo*

Main category: cs.NI

TL;DR: 本文开发了一种用于PCB磁近场扫描的实验研究方法，介绍了电磁近场扫描仪的设计安装、微波频段测试平台搭建、微带磁近场探头方法学，并按照IEC 61967-1标准进行探头校准。


<details>
  <summary>Details</summary>
Motivation: 针对6G无线通信PCB的EMC辐射发射问题，需要开发有效的近场扫描技术来分析和控制电路板辐射。

Method: 设计电磁近场扫描仪，使用微带磁近场探头，按照IEC 61967-1标准进行校准，在0.1-3GHz宽频范围内通过HFSS仿真和实验验证辐射磁场校准过程。

Result: 通过标准I型50Ω传输线测试验证了校准过程的有效性，并对非标准传输线DUT进行了实验，获得了2GHz和3GHz频率下的磁近场扫描图像。

Conclusion: 开发的近场扫描仪能够有效分析PCB磁近场辐射特性，为6G无线通信PCB的EMC辐射发射控制提供了有效的测试工具。

Abstract: An experimental study on magnetic near-field (NF) scanning of printed circuit
board (PCB) emission radiation is developed in this paper. The design and
installation of the electromagnetic (EM) NF scanner is introduced. The test bed
of magnetic NF emission in the microwave frequency range is described. The
methodology of the microstrip magnetic NF probe is discussed. The probe
calibration process was performed following the IEC 61967-1 NF scanning
standard. The NF scanner functioning is tested with passive microstrip circuit
square loop probe and device under test (DUT) PCB radiation in the test plan
positioned at 1-mm above the ground plane. Based on the standard test with
I-shape 50-$\Omega$ transmission line (TL), the calibration process of radiated
magnetic field was validated by comparison between HFSS__ simulation and
experimentation in very wideband frequency from 0.1-GHz to 3-GHz. Then, a
nonstandard TL based DUT was experimented. Accordingly, the cartographies of
scanned magnetic NF at two different test frequencies, 2 GHz and 3 GHz, are
discussed. The NF scanner is under development for targeting the EMC radiated
emission of PCB dedicated to operate in 6G wireless communication.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [62] [Game-Theoretic Understandings of Multi-Agent Systems with Multiple Objectives](https://arxiv.org/abs/2509.23026)
*Yue Wang*

Main category: cs.MA

TL;DR: 提出了多目标马尔可夫博弈(MOMG)框架来处理多智能体强化学习中的多目标问题，引入了帕累托-纳什均衡(PNE)作为核心解概念，并开发了相应的学习算法


<details>
  <summary>Details</summary>
Motivation: 实际多智能体系统中智能体通常具有多样化目标，每个智能体在多个标准上的表现依赖于所有智能体的联合行动，产生了复杂的战略权衡问题

Method: 提出MOMG框架和PNE解概念，证明PNE存在性及其与线性标量化博弈纳什均衡的等价关系，开发两阶段无偏好算法将探索与规划分离

Result: 建立了理论框架，证明了PNE的存在性和等价关系，提出了可计算较弱解概念和在线学习算法，能够高效计算任意偏好下的PNE

Conclusion: MOMG框架为多目标多智能体系统提供了理论基础和实用算法，通过解耦探索与规划实现了对帕累托-纳什前沿的高效表征

Abstract: In practical multi-agent systems, agents often have diverse objectives, which
makes the system more complex, as each agent's performance across multiple
criteria depends on the joint actions of all agents, creating intricate
strategic trade-offs. To address this, we introduce the Multi-Objective Markov
Game (MOMG), a framework for multi-agent reinforcement learning with multiple
objectives. We propose the Pareto-Nash Equilibrium (PNE) as the primary
solution concept, where no agent can unilaterally improve one objective without
sacrificing performance on another. We prove existence of PNE, and establish an
equivalence between the PNE and the set of Nash Equilibria of MOMG's
corresponding linearly scalarized games, enabling solutions of MOMG by
transferring to a standard single-objective Markov game. However, we note that
computing a PNE is theoretically and computationally challenging, thus we
propose and study weaker but more tractable solution concepts. Building on
these foundations, we develop online learning algorithm that identify a single
solution to MOMGs. Furthermore, we propose a two-phase, preference-free
algorithm that decouples exploration from planning. Our algorithm enables
computation of a PNE for any given preference profile without collecting new
samples, providing an efficient methodological characterization of the entire
Pareto-Nash front.

</details>


### [63] [Situational Awareness for Safe and Robust Multi-Agent Interactions Under Uncertainty](https://arxiv.org/abs/2509.23425)
*Benjamin Alcorn,Eman Hammad*

Main category: cs.MA

TL;DR: 提出一个多智能体系统中的意图预测和资源约束下最优行动框架，结合观测和估计算法来处理不确定性，并通过风险分析进行管理，使用强化学习和博弈论算法进行验证。


<details>
  <summary>Details</summary>
Motivation: 解决多智能体系统中非协调智能体的意图预测问题，以及在资源约束下实现目标而不显著牺牲性能的挑战。

Method: 开发了一个自主智能体模型，通过安全观测半径观察环境，确定周围智能体状态，估计未来行动并采取最优行动。在没有观测时使用估计算法基于历史轨迹预测其他智能体行动，并通过风险分析管理不确定性。

Result: 提出的方法使用强化学习和博弈论算法进行了验证，表明能够有效预测智能体意图并在资源约束下实现目标。

Conclusion: 该研究为多智能体系统提供了一种有效的意图预测和最优决策框架，能够处理观测不确定性和资源约束问题，具有广泛的应用潜力。

Abstract: Multi-agent systems are prevalent in a wide range of domains including power
systems, vehicular networks, and robotics. Two important problems to solve in
these types of systems are how the intentions of non-coordinating agents can be
determined to predict future behavior and how the agents can achieve their
objectives under resource constraints without significantly sacrificing
performance. To study this, we develop a model where an autonomous agent
observes the environment within a safety radius of observation, determines the
state of a surrounding agent of interest (within the observation radius),
estimates future actions to be taken, and acts in an optimal way. In the
absence of observations, agents are able to utilize an estimation algorithm to
predict the future actions of other agents based on historical trajectory. The
use of the proposed estimation algorithm introduces uncertainty, which is
managed via risk analysis. The proposed approach in this study is validated
using two different learning-based decision making frameworks: reinforcement
learning and game theoretic algorithms.

</details>


### [64] [PartnerMAS: An LLM Hierarchical Multi-Agent Framework for Business Partner Selection on High-Dimensional Features](https://arxiv.org/abs/2509.24046)
*Lingyao Li,Haolun Wu,Zhenkun Li,Jiabei Hu,Yu Wang,Xiaoshan Huang,Wenyue Hua,Wenqian Wang*

Main category: cs.MA

TL;DR: PartnerMAS是一个分层多智能体框架，通过规划者、专家和监督者三层协作，在高维决策任务中显著优于单智能体和辩论式多智能体基线，匹配率提升10-15%。


<details>
  <summary>Details</summary>
Motivation: 高维决策任务（如商业伙伴选择）需要评估具有异质数值、分类和文本特征的大型候选池。单智能体或辩论式系统在此类设置中往往难以扩展和保持一致性。

Method: 提出PartnerMAS分层多智能体框架：规划者智能体设计策略，专家智能体执行角色特定评估，监督者智能体整合输出。使用风险投资联合投资基准数据集进行系统评估。

Result: 在140个案例中，PartnerMAS始终优于单智能体和基于辩论的多智能体基线，匹配率提高10-15%。分析显示规划者对领域知识提示最敏感，专家提供互补特征覆盖，监督者在聚合中发挥重要作用。

Conclusion: LLM智能体之间的结构化协作比扩展单个模型能产生更稳健的结果，PartnerMAS是数据丰富领域中高维决策的有前景框架。

Abstract: High-dimensional decision-making tasks, such as business partner selection,
involve evaluating large candidate pools with heterogeneous numerical,
categorical, and textual features. While large language models (LLMs) offer
strong in-context reasoning capabilities, single-agent or debate-style systems
often struggle with scalability and consistency in such settings. We propose
PartnerMAS, a hierarchical multi-agent framework that decomposes evaluation
into three layers: a Planner Agent that designs strategies, Specialized Agents
that perform role-specific assessments, and a Supervisor Agent that integrates
their outputs. To support systematic evaluation, we also introduce a curated
benchmark dataset of venture capital co-investments, featuring diverse firm
attributes and ground-truth syndicates. Across 140 cases, PartnerMAS
consistently outperforms single-agent and debate-based multi-agent baselines,
achieving up to 10--15\% higher match rates. Analysis of agent reasoning shows
that planners are most responsive to domain-informed prompts, specialists
produce complementary feature coverage, and supervisors play an important role
in aggregation. Our findings demonstrate that structured collaboration among
LLM agents can generate more robust outcomes than scaling individual models,
highlighting PartnerMAS as a promising framework for high-dimensional
decision-making in data-rich domains.

</details>


### [65] [CORRECT: COndensed eRror RECognition via knowledge Transfer in multi-agent systems](https://arxiv.org/abs/2509.24088)
*Yifan Yu,Moyan Li,Shaoyuan Xu,Jinmiao Fu,Xinhai Hou,Fan Lai,Bryan Wang*

Main category: cs.MA

TL;DR: CORRECT是一个轻量级、无需训练的框架，通过在线缓存错误模式来识别和转移多智能体系统中的失败结构知识，显著提升错误定位能力


<details>
  <summary>Details</summary>
Motivation: 多智能体系统在复杂任务中容易产生错误传播，传统调试方法成本高昂且效率低下，需要一种能够识别重复错误模式的自动化解决方案

Method: 使用在线缓存存储蒸馏的错误模式知识，在推理时进行针对性错误定位，无需重新训练，支持动态多智能体系统部署

Result: 在7个不同MAS应用中，CORRECT将步骤级错误定位性能提升19.8%，接近零开销，显著缩小了自动化与人工错误识别之间的差距

Conclusion: CORRECT框架通过重用错误模式知识，为多智能体系统提供了一种高效、轻量的错误识别解决方案，具有实际部署价值

Abstract: Multi-agent systems (MAS) are increasingly capable of tackling complex
real-world tasks, yet their reliance on inter-agent coordination, tool use, and
long-horizon reasoning makes error recognition particularly challenging. Minor
errors can propagate across agents, escalating into task failures while
producing long, intertwined execution trajectories that impose significant
costs for both human developers and automated systems to debug and analyze. Our
key insight is that, despite surface differences in failure trajectories (e.g.,
logs), MAS errors often recur with similar structural patterns. This paper
presents CORRECT, the first lightweight, training-free framework that leverages
an online cache of distilled error schemata to recognize and transfer knowledge
of failure structures across new requests. This cache-based reuse allows LLMs
to perform targeted error localization at inference time, avoiding the need for
expensive retraining while adapting to dynamic MAS deployments in subseconds.
To support rigorous study in this domain, we also introduce CORRECT-Error, a
large-scale dataset of over 2,000 annotated trajectories collected through a
novel error-injection pipeline guided by real-world distributions, and further
validated through human evaluation to ensure alignment with natural failure
patterns. Experiments across seven diverse MAS applications show that CORRECT
improves step-level error localization up to 19.8% over existing advances while
at near-zero overhead, substantially narrowing the gap between automated and
human-level error recognition.

</details>


### [66] [MAS$^2$: Self-Generative, Self-Configuring, Self-Rectifying Multi-Agent Systems](https://arxiv.org/abs/2509.24323)
*Kun Wang,Guibin Zhang,ManKit Ye,Xinyu Deng,Dongxia Wang,Xiaobin Hu,Jinyang Guo,Yang Liu,Yufei Guo*

Main category: cs.MA

TL;DR: MAS²是一个基于递归自生成原则的多智能体系统范式，能够自主构建针对不同问题的定制化多智能体系统，在复杂场景中性能提升达19.6%，且具有优异的跨骨干网络泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有自动多智能体系统大多采用"一次生成部署"的刚性范式，导致系统脆弱且难以应对现实环境的动态性和不确定性。

Method: 设计了"生成器-执行器-修正器"三智能体团队，能够动态组合并自适应修正目标智能体系统；提出协作树优化方法来训练和专业化这些元智能体。

Result: 在7个基准测试中，MAS²在深度研究和代码生成等复杂场景中性能比最先进的多智能体系统提升高达19.6%；在未见过的LLM上也能实现15.1%的改进，且不会产生过高的token成本。

Conclusion: MAS²通过递归自生成范式成功克服了现有系统的局限性，实现了动态自适应和多智能体系统的自主架构，为复杂任务提供了有效的解决方案。

Abstract: The past two years have witnessed the meteoric rise of Large Language Model
(LLM)-powered multi-agent systems (MAS), which harness collective intelligence
and exhibit a remarkable trajectory toward self-evolution. This paradigm has
rapidly progressed from manually engineered systems that require bespoke
configuration of prompts, tools, roles, and communication protocols toward
frameworks capable of automated orchestration. Yet, dominant automatic
multi-agent systems, whether generated by external modules or a single LLM
agent, largely adhere to a rigid ``\textit{generate-once-and-deploy}''
paradigm, rendering the resulting systems brittle and ill-prepared for the
dynamism and uncertainty of real-world environments. To transcend this
limitation, we introduce MAS$^2$, a paradigm predicated on the principle of
recursive self-generation: a multi-agent system that autonomously architects
bespoke multi-agent systems for diverse problems. Technically, we devise a
``\textit{generator-implementer-rectifier}'' tri-agent team capable of
dynamically composing and adaptively rectifying a target agent system in
response to real-time task demands. Collaborative Tree Optimization is proposed
to train and specialize these meta-agents. Extensive evaluation across seven
benchmarks reveals that MAS$^2$ achieves performance gains of up to $19.6\%$
over state-of-the-art MAS in complex scenarios such as deep research and code
generation. Moreover, MAS$^2$ exhibits superior cross-backbone generalization,
effectively leveraging previously unseen LLMs to yield improvements of up to
$15.1\%$. Crucially, these gains are attained without incurring excessive token
costs, as MAS$^2$ consistently resides on the Pareto frontier of
cost-performance trade-offs. The source codes are available at
https://github.com/yeyeyeah2/MAS2.

</details>


### [67] [MARLIN: Multi-Agent Reinforcement Learning with Murmuration Intelligence and LLM Guidance for Reservoir Management](https://arxiv.org/abs/2509.25034)
*Heming Fu,Guojun Xiong,Jian Li,Shan Lin*

Main category: cs.MA

TL;DR: MARLIN是一个基于椋鸟群智能的去中心化水库管理框架，结合多智能体强化学习和LLM实时奖励塑造，有效处理水库网络中的级联不确定性，提升洪水响应速度68%，计算效率提高35%。


<details>
  <summary>Details</summary>
Motivation: 气候变化加剧极端天气事件，水库管理面临级联不确定性的挑战。传统集中式优化方法计算复杂度高，现有MARL方法在不确定性下无法实现有效协调，需要新的解决方案。

Method: MARLIN框架整合生物启发的对齐、分离和凝聚规则与多智能体强化学习，采用去中心化架构让各水库自主决策，同时通过LLM提供实时奖励塑造信号来适应环境变化和人类偏好。

Result: 在美国地质调查局真实数据上的实验显示：不确定性处理提升23%，计算量减少35%，洪水响应速度加快68%，在400到10,000节点规模下复杂度仅增长5.4倍，表现出超线性协调能力。

Conclusion: MARLIN通过智能可扩展的水资源管理方法，在灾害预防和社区保护方面展现出巨大潜力，为应对气候变化下的水库管理挑战提供了有效解决方案。

Abstract: As climate change intensifies extreme weather events, water disasters pose
growing threats to global communities, making adaptive reservoir management
critical for protecting vulnerable populations and ensuring water security.
Modern water resource management faces unprecedented challenges from cascading
uncertainties propagating through interconnected reservoir networks. These
uncertainties, rooted in physical water transfer losses and environmental
variability, make precise control difficult. For example, sending 10 tons
downstream may yield only 8-12 tons due to evaporation and seepage. Traditional
centralized optimization approaches suffer from exponential computational
complexity and cannot effectively handle such real-world uncertainties, while
existing multi-agent reinforcement learning (MARL) methods fail to achieve
effective coordination under uncertainty. To address these challenges, we
present MARLIN, a decentralized reservoir management framework inspired by
starling murmurations intelligence. Integrating bio-inspired alignment,
separation, and cohesion rules with MARL, MARLIN enables individual reservoirs
to make local decisions while achieving emergent global coordination. In
addition, a LLM provides real-time reward shaping signals, guiding agents to
adapt to environmental changes and human-defined preferences. Experiments on
real-world USGS data show that MARLIN improves uncertainty handling by 23\%,
cuts computation by 35\%, and accelerates flood response by 68\%, exhibiting
super-linear coordination, with complexity scaling 5.4x from 400 to 10,000
nodes. These results demonstrate MARLIN's potential for disaster prevention and
protecting communities through intelligent, scalable water resource management.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [68] [Analysis of the carbon footprint of HPC](https://arxiv.org/abs/2509.22679)
*Abdessalam Benhari,Yves Denneulin,Frédéric Desprez,Fanny Dufossé,Denis Trystram*

Main category: cs.DC

TL;DR: 本文分析了高性能计算系统碳排放的演变趋势，通过考虑系统全生命周期和能源结构，建立了预测未来5年HPC碳排放的模型


<details>
  <summary>Details</summary>
Motivation: 高性能计算系统的计算能力不断增长，但能耗和碳足迹也随之增加，需要研究其碳排放演变趋势

Method: 分析Top500系统能源结构，考虑多个大规模系统的全生命周期，建立预测模型

Result: 提出了预测未来5年高性能计算碳排放的模型

Conclusion: 高性能计算系统的碳排放需要系统性的全生命周期分析，预测模型有助于评估未来环境影响

Abstract: The demand in computing power has never stopped growing over the years.
Today, the performance of the most powerful systems exceeds the exascale.
Unfortunately, this growth also comes with ever-increasing energy costs,
leading to a high carbon footprint. This paper investigates the evolution of
high performance systems in terms of carbon emissions. A lot of studies focus
on Top500 (and Green500) as the tip of an iceberg to identify trends in the
domain in terms of computing performance. We propose here to go further in
considering the whole span life of several large scale systems and to link the
evolution with trajectory toward 2030. More precisely, we introduce the energy
mix in the analysis of Top500 systems and we derive a predictive model for
estimating the weight of HPC for the next 5 years.

</details>


### [69] [FLAME: A Serving System Optimized for Large-Scale Generative Recommendation with Efficiency](https://arxiv.org/abs/2509.22681)
*Xianwen Guo,Bin Huang,Xiaomeng Wu,Guanlin Wu,Fangjian Li,Shijia Wang,Qiang Xiao,Chuanjiang Luo,Yong Li*

Main category: cs.DC

TL;DR: FLAME系统通过CPU-GPU异构硬件、内存优化、融合内核引擎和动态流协调器，解决了生成式推荐模型在线部署的高计算负担问题，显著提升了系统性能。


<details>
  <summary>Details</summary>
Motivation: 生成式推荐模型相比传统深度学习推荐模型具有更强的扩展能力，但计算负担增加了4个数量级，在线部署面临巨大挑战，需要高效的在线服务系统解决方案。

Method: 采用CPU-GPU异构硬件解耦特征预处理和模型计算；设计Proximal Data Accelerator (PDA)模块优化内存使用；实现Fused Kernel Engine (FKE)模块加速模型计算；开发Dynamic Stream Orchestrator (DSO)模块协调并发请求。

Result: PDA模块实现1.9倍吞吐量提升和1.7倍延迟降低；FKE模块实现4.6-6.1倍加速比和4.7-6.3倍吞吐量提升；DSO模块在非均匀分布下实现1.3倍吞吐量提升和2.3倍加速。

Conclusion: FLAME系统有效支持大规模生成式推荐模型的在线部署，在系统性能方面取得了显著改进，为工业实践提供了可行的解决方案。

Abstract: Generative recommendation (GR) models possess greater scaling power compared
to traditional deep learning recommendation models (DLRMs), yet they also
impose a tremendous increase in computational burden. Measured in FLOPs, a
typical GR model's workload sits in $10^9 \sim 10^{11}$ range, roughly four
orders of magnitude higher than traditional DLRMs. Delivering accurate results
in a few tens of milliseconds while processing billions of such requests per
day puts extreme demands on the performance of the online serving system.
Therefore, for industry practitioners, the alluring gains of GR models are
tempered by the formidable challenge of online deployment at scale in
production services. In this work, we introduce a comprehensive solution of
online serving system tailored For Large-scale GenerAtive RecoMmendation with
Efficiency (FLAME). Specifically, we leveraging CPU-GPU heterogeneous hardware
to decouple feature pre-processing and model computation. We encapsulated
several memory optimization features as the Proximal Data Accelerator (PDA)
module to make full use of limited bandwidth and storage resources, which
achieves a 1.9x throughput gain and a 1.7x latency reduction. We implement the
Fused Kernel Engine (FKE) module based on the functionality and interface of
NVIDIA TensorRT to boost model computation, delivering a speedup ratio of
4.6x-6.1x, throughput gain ratio of 4.7x-6.3x one step further. In addition, we
design the Dynamic Stream Orchestrator (DSO) module to coordinate concurrent
requests, enhancing the system throughput performance with 1.3x improvement in
throughput and 2.3x speed-up under non-uniform distribution of upstream
candidates. Comprehensive evaluations demonstrate that our FLAME effectively
supports large-scale online deployment of GR models and achieves remarkable
improvements in system performance.

</details>


### [70] [ZKProphet: Understanding Performance of Zero-Knowledge Proofs on GPUs](https://arxiv.org/abs/2509.22684)
*Tarunesh Verma,Yichao Yuan,Nishil Talati,Todd Austin*

Main category: cs.DC

TL;DR: ZKProphet是对GPU上零知识证明性能的全面研究，发现NTT内核成为主要瓶颈，占GPU证明生成延迟的90%，并提出通过参数调优和优化数据表示来提升性能


<details>
  <summary>Details</summary>
Motivation: 现有文献缺乏对ZKPs执行瓶颈的系统性特征描述以及在现代GPU架构上的可扩展性分析，需要为ZKP社区提供GPU性能扩展路线图

Method: 通过系统性性能分析，识别NTT等内核的性能瓶颈，研究算术运算在GPU 32位整数流水线上的执行特性，探讨运行时参数调优和优化数据表示的方法

Result: 发现优化后的MSM实现使NTT成为主要瓶颈（占90%延迟），ZKP运算受限于GPU整数计算单元，数据依赖限制了指令级并行性

Conclusion: 为ZKP社区提供了GPU性能扩展的路线图，通过预计算输入和替代数据表示等优化可以提取额外加速，帮助构建适合应用需求和硬件资源的GPU加速ZKP

Abstract: Zero-Knowledge Proofs (ZKP) are protocols which construct cryptographic
proofs to demonstrate knowledge of a secret input in a computation without
revealing any information about the secret. ZKPs enable novel applications in
private and verifiable computing such as anonymized cryptocurrencies and
blockchain scaling and have seen adoption in several real-world systems. Prior
work has accelerated ZKPs on GPUs by leveraging the inherent parallelism in
core computation kernels like Multi-Scalar Multiplication (MSM). However, we
find that a systematic characterization of execution bottlenecks in ZKPs, as
well as their scalability on modern GPU architectures, is missing in the
literature. This paper presents ZKProphet, a comprehensive performance study of
Zero-Knowledge Proofs on GPUs. Following massive speedups of MSM, we find that
ZKPs are bottlenecked by kernels like Number-Theoretic Transform (NTT), as they
account for up to 90% of the proof generation latency on GPUs when paired with
optimized MSM implementations. Available NTT implementations under-utilize GPU
compute resources and often do not employ architectural features like
asynchronous compute and memory operations. We observe that the arithmetic
operations underlying ZKPs execute exclusively on the GPU's 32-bit integer
pipeline and exhibit limited instruction-level parallelism due to data
dependencies. Their performance is thus limited by the available integer
compute units. While one way to scale the performance of ZKPs is adding more
compute units, we discuss how runtime parameter tuning for optimizations like
precomputed inputs and alternative data representations can extract additional
speedup. With this work, we provide the ZKP community a roadmap to scale
performance on GPUs and construct definitive GPU-accelerated ZKPs for their
application requirements and available hardware resources.

</details>


### [71] [Enhancing Cluster Scheduling in HPC: A Continuous Transfer Learning for Real-Time Optimization](https://arxiv.org/abs/2509.22701)
*Leszek Sliwko,Jolanta Mizera-Pietraszko*

Main category: cs.DC

TL;DR: 机器学习辅助的集群任务调度优化方法，通过持续迁移学习实现动态适应性，在Google集群数据上达到99%以上准确率


<details>
  <summary>Details</summary>
Motivation: 传统调度器如Kubernetes缺乏实时适应性，无法有效处理节点亲和性约束条件下的任务调度问题

Method: 采用持续迁移学习模型，在运行过程中动态演化，最小化重新训练需求

Result: 在Google集群数据评估中实现超过99%的准确率，降低计算开销并改善约束任务的调度延迟

Conclusion: 该可扩展解决方案实现了实时优化，推动了机器学习在集群管理中的集成，为未来自适应调度策略奠定了基础

Abstract: This study presents a machine learning-assisted approach to optimize task
scheduling in cluster systems, focusing on node-affinity constraints.
Traditional schedulers like Kubernetes struggle with real-time adaptability,
whereas the proposed continuous transfer learning model evolves dynamically
during operations, minimizing retraining needs. Evaluated on Google Cluster
Data, the model achieves over 99% accuracy, reducing computational overhead and
improving scheduling latency for constrained tasks. This scalable solution
enables real-time optimization, advancing machine learning integration in
cluster management and paving the way for future adaptive scheduling
strategies.

</details>


### [72] [Intelligent Load Balancing in Cloud Computer Systems](https://arxiv.org/abs/2509.22704)
*Leszek Sliwko*

Main category: cs.DC

TL;DR: 提出了一种动态任务分配策略，避免云节点过载，保持系统稳定性并最小化成本。


<details>
  <summary>Details</summary>
Motivation: 云计算系统规模庞大，需要有效的任务调度策略来避免节点过载，维持系统稳定性并降低成本。

Method: 提出了三种调度器分类（OS级、集群和大数据），建立了云资源利用抽象模型，实验了虚拟机实时迁移，创建了基于Google工作负载的高保真模拟器，并比较了集中式和分布式资源管理方法。

Result: 在威斯敏斯特大学HPC集群上进行了广泛实验，获得了有希望的结果。

Conclusion: 该研究为云资源管理提供了新的分类框架、资源利用模型和实用的负载均衡方法，有助于提高云计算系统的效率和稳定性。

Abstract: Cloud computing is an established technology allowing users to share
resources on a large scale, never before seen in IT history. A cloud system
connects multiple individual servers in order to process related tasks in
several environments at the same time. Clouds are typically more cost-effective
than single computers of comparable computing performance. The sheer physical
size of the system itself means that thousands of machines may be involved. The
focus of this research was to design a strategy to dynamically allocate tasks
without overloading Cloud nodes which would result in system stability being
maintained at minimum cost. This research has added the following new
contributions to the state of knowledge: (i) a novel taxonomy and
categorisation of three classes of schedulers, namely OS-level, Cluster and Big
Data, which highlight their unique evolution and underline their different
objectives; (ii) an abstract model of cloud resources utilisation is specified,
including multiple types of resources and consideration of task migration
costs; (iii) a virtual machine live migration was experimented with in order to
create a formula which estimates the network traffic generated by this process;
(iv) a high-fidelity Cloud workload simulator, based on a month-long workload
traces from Google's computing cells, was created; (v) two possible approaches
to resource management were proposed and examined in the practical part of the
manuscript: the centralised metaheuristic load balancer and the decentralised
agent-based system. The project involved extensive experiments run on the
University of Westminster HPC cluster, and the promising results are presented
together with detailed discussions and a conclusion.

</details>


### [73] [Metadata-Guided Adaptable Frequency Scaling across Heterogeneous Applications and Devices](https://arxiv.org/abs/2509.22707)
*Jinqi Yan,Fang He,Qianlong Sang,Bifeng Tong,Peng Sun,Yili Gong,Chuang Hu,Dazhao Cheng*

Main category: cs.DC

TL;DR: MetaDVFS是一个基于元数据的强化学习框架，用于解决异构移动设备DVFS调度问题，通过跨任务知识共享实现更好的泛化能力和部署效率。


<details>
  <summary>Details</summary>
Motivation: 传统启发式DVFS调度器难以处理异构SoC设计和多样化应用负载的复杂性，而现有强化学习方法虽然性能更好但泛化能力差且需要为每个硬件应用组合重新训练，部署成本高。

Method: 将DVFS问题建模为多任务强化学习，提出MetaDVFS框架，利用设备和应用元数据来发现和迁移跨DVFS任务的共享知识，输出具有强泛化能力的DVFS模型集。

Result: 在5款Google Pixel设备和6个应用上的评估显示，MetaDVFS在性能功耗比上提升达17%，用户体验质量提升达26%，比现有方法适应速度快70.8%，性能提升5.8-27.6%。

Conclusion: MetaDVFS为异构移动环境中的DVFS部署提供了一个有效且可扩展的解决方案，避免了负迁移效应，显著提升了泛化能力和部署效率。

Abstract: Dynamic Voltage and Frequency Scaling is essential for enhancing energy
efficiency in mobile platforms. However, traditional heuristic-based governors
are increasingly inadequate for managing the complexity of heterogeneous
System-on-Chip designs and diverse application workloads. Although
reinforcement learning approaches offer improved performance, their poor
generalization capability and reliance on extensive retraining for each
hardware and application combination leads to significant deployment costs. In
this work, we observe that device and application metadata inherently
encapsulate valuable knowledge for DVFS, presenting an opportunity to overcome
these limitations. We formulate DVFS for heterogeneous devices and applications
as a multi-task reinforcement learning problem. We introduce MetaDVFS, which is
a metadata-guided framework that systematically leverages metadata to discover
and transfer shared knowledge across DVFS tasks. MetaDVFS can output a set of
DVFS models with significant generalization capability for various applications
of heterogeneous devices. Evaluations on five Google Pixel devices running six
applications show that MetaDVFS achieves up to 17% improvement in
Performance-Power Ratio and up to 26% improvement in Quality of Experience.
Compared to state-of-the-art methods, MetaDVFS delivers 70.8% faster adaptation
and 5.8-27.6% higher performance over standalone device-application specific
training, while avoiding negative transfer effects. These results establish
MetaDVFS as an effective and scalable solution for DVFS deployment in
heterogeneous mobile environments.

</details>


### [74] [TeraAgent: A Distributed Agent-Based Simulation Engine for Simulating Half a Trillion Agents](https://arxiv.org/abs/2509.24063)
*Lukas Breitwieser,Ahmad Hesam,Abdullah Giray Yağlıkçı,Mohammad Sadrosadati,Fons Rademakers,Onur Mutlu*

Main category: cs.DC

TL;DR: TeraAgent是一个分布式代理模拟引擎，解决了现有BioDynaMo平台无法跨服务器扩展的问题，通过优化的序列化和增量编码技术实现了万亿级代理的模拟。


<details>
  <summary>Details</summary>
Motivation: 现有的BioDynaMo平台由于基于共享内存实现，无法跨多个服务器进行扩展，限制了大规模复杂系统的模拟能力。

Method: 提出两种解决方案：1）定制的序列化机制，允许直接从接收缓冲区访问和修改代理；2）利用代理模拟的迭代特性，通过增量编码减少数据传输。

Result: 实现了0.5万亿代理的极端规模模拟（84倍改进），通过增加计算节点减少结果时间，提高与第三方工具的互操作性，并提供更多硬件灵活性。

Conclusion: TeraAgent成功解决了分布式代理模拟的性能瓶颈问题，为大规模复杂系统研究提供了有效的计算平台。

Abstract: Agent-based simulation is an indispensable paradigm for studying complex
systems. These systems can comprise billions of agents, requiring the computing
resources of multiple servers to simulate. Unfortunately, the state-of-the-art
platform, BioDynaMo, does not scale out across servers due to its
shared-memory-based implementation.
  To overcome this key limitation, we introduce TeraAgent, a distributed
agent-based simulation engine. A critical challenge in distributed execution is
the exchange of agent information across servers, which we identify as a major
performance bottleneck. We propose two solutions: 1) a tailored serialization
mechanism that allows agents to be accessed and mutated directly from the
receive buffer, and 2) leveraging the iterative nature of agent-based
simulations to reduce data transfer with delta encoding.
  Built on our solutions, TeraAgent enables extreme-scale simulations with half
a trillion agents (an 84x improvement), reduces time-to-result with additional
compute nodes, improves interoperability with third-party tools, and provides
users with more hardware flexibility.

</details>


### [75] [Efficient Fine-Grained GPU Performance Modeling for Distributed Deep Learning of LLM](https://arxiv.org/abs/2509.22832)
*Biyao Zhang,Mingkai Zheng,Debargha Ganguly,Xuecen Zhang,Vikash Singh,Vipin Chaudhary,Zhao Zhang*

Main category: cs.DC

TL;DR: 提出了一种基于计算原语分解的LLM训练时间预测框架，通过轻量级采样和硬件感知建模，在CPU上实现准确预测，避免了昂贵的集群实验


<details>
  <summary>Details</summary>
Motivation: 大规模语言模型训练是计算密集型任务，预测分布式训练时间面临挑战，现有学习方法采样成本高，分析方法难以处理实际网络和硬件复杂性

Method: 将LLM分解为核心计算原语，采用算子级分解进行细粒度分析，基于轻量级采样的硬件感知预测模型，构建端到端预测系统整合复杂并行策略

Result: 在两个大规模HPC系统上验证，Perlmutter(A100)平均预测误差4.98%，Vista(GH200)平均误差9.38%，支持最多20B参数模型和128个GPU

Conclusion: 该框架完全在CPU上运行，能够快速迭代硬件配置和训练策略，无需昂贵的集群实验，为LLM训练时间预测提供了有效解决方案

Abstract: Training Large Language Models(LLMs) is one of the most compute-intensive
tasks in high-performance computing. Predicting end-to-end training time for
multi-billion parameter models distributed across hundreds of GPUs remains
challenging due to complex interactions between transformer components,
parallelism strategies(data, model, pipeline, tensor), and multi-tier
communication. Learned models require costly sampling, while analytical models
often struggle with real-world network and hardware complexities. We address
this by decomposing LLMs into core computational primitives and modeling them
with: (1) operator-level decomposition for fine-grained analysis; (2)
lightweight sampling based hardware-aware prediction models for key operations;
(3) an end-to-end prediction system integrating these components across complex
parallelization strategies. Crucially, our methodology has been validated on
two large-scale HPC systems. Our framework achieves low average prediction
errors-4.98\% on Perlmutter(A100) and 9.38\% on Vista(GH200)-for models up to
20B parameters across 128 GPUs. Importantly, it runs entirely on CPUs, enabling
rapid iteration over hardware configurations and training strategies without
costly on-cluster experimentation.

</details>


### [76] [OptimES: Optimizing Federated Learning Using Remote Embeddings for Graph Neural Networks](https://arxiv.org/abs/2509.22922)
*Pranjal Naman,Yogesh Simmhan*

Main category: cs.DC

TL;DR: 提出了OptimES框架，通过远程邻域剪枝、嵌入推送与本地训练重叠、动态嵌入拉取等优化策略，显著降低了联邦图神经网络训练的通信成本和训练时间，在大型密集图上实现3.5倍加速和16%精度提升。


<details>
  <summary>Details</summary>
Motivation: 现实世界中的图数据通常分散在不同数据所有者手中，由于隐私问题无法集中训练。现有联邦GNN训练方法虽然通过共享边界顶点嵌入解决了隐私问题，但面临通信成本高导致性能下降的挑战。

Method: OptimES框架采用三种优化策略：1）远程邻域剪枝减少通信量；2）将嵌入推送到服务器与本地训练重叠执行；3）动态拉取嵌入机制降低网络成本。

Result: 在4个常见图数据集（最大1.11亿顶点、18亿边）上的评估显示：在Reddit和Products等大型密集图上，训练速度比EmbC快约3.5倍，精度比默认联邦GNN高16%；在稀疏图上虽然精度提升有限，但达到目标精度的速度比EmbC快约11倍。

Conclusion: OptimES通过优化通信策略有效解决了联邦GNN训练中的通信瓶颈问题，在保持隐私保护的同时显著提升了训练效率和模型性能，特别适用于大规模密集图数据的联邦学习场景。

Abstract: Graph Neural Networks (GNNs) have experienced rapid advancements in recent
years due to their ability to learn meaningful representations from graph data
structures. However, in most real-world settings, such as financial transaction
networks and healthcare networks, this data is localized to different data
owners and cannot be aggregated due to privacy concerns. Federated Learning
(FL) has emerged as a viable machine learning approach for training a shared
model that iteratively aggregates local models trained on decentralized data.
This addresses privacy concerns while leveraging parallelism. State-of-the-art
methods enhance the privacy-respecting convergence accuracy of federated GNN
training by sharing remote embeddings of boundary vertices through a server
(EmbC). However, they are limited by diminished performance due to large
communication costs. In this article, we propose OptimES, an optimized
federated GNN training framework that employs remote neighbourhood pruning,
overlapping the push of embeddings to the server with local training, and
dynamic pulling of embeddings to reduce network costs and training time. We
perform a rigorous evaluation of these strategies for four common graph
datasets with up to $111M$ vertices and $1.8B$ edges. We see that a modest drop
in per-round accuracy due to the preemptive push of embeddings is out-stripped
by the reduction in per-round training time for large and dense graphs like
Reddit and Products, converging up to $\approx 3.5\times$ faster than EmbC and
giving up to $\approx16\%$ better accuracy than the default federated GNN
learning. While accuracy improvements over default federated GNNs are modest
for sparser graphs like Arxiv and Papers, they achieve the target accuracy
about $\approx11\times$ faster than EmbC.

</details>


### [77] [Characterizing FaaS Workflows on Public Clouds: The Good, the Bad and the Ugly](https://arxiv.org/abs/2509.23013)
*Varad Kulkarni,Nikhil Reddy,Tuhin Khare,Abhinandan S. Prasad,Chitra Babu,Yogesh Simmhan*

Main category: cs.DC

TL;DR: 对AWS和Azure的三大FaaS工作流平台进行大规模评估，通过13.2万次调用运行25个微基准和应用工作流，揭示了函数执行、工作流编排、冷启动扩展和成本等方面的独特见解


<details>
  <summary>Details</summary>
Motivation: FaaS工作流平台中函数间复杂交互和专有平台内部可见性有限，阻碍了对平台的深入理解，缺乏对具有独特扩展性、性能和成本行为的FaaS工作流平台的系统研究

Method: 对AWS和Azure的三大流行FaaS工作流平台进行广泛评估，运行25个微基准测试和应用工作流，共计132,000次调用

Result: 分析确认了一些传统认知，但也揭示了函数执行、工作流编排、函数间交互、冷启动扩展和货币成本方面的独特见解

Conclusion: 研究结果帮助开发者更好地配置和编程这些平台，设定性能和可扩展性预期，并识别平台增强的研究空白

Abstract: Function-as-a-service (FaaS) is a popular serverless computing paradigm for
developing event-driven functions that elastically scale on public clouds. FaaS
workflows, such as AWS Step Functions and Azure Durable Functions, are composed
from FaaS functions, like AWS Lambda and Azure Functions, to build practical
applications. But, the complex interactions between functions in the workflow
and the limited visibility into the internals of proprietary FaaS platforms are
major impediments to gaining a deeper understanding of FaaS workflow platforms.
While several works characterize FaaS platforms to derive such insights, there
is a lack of a principled and rigorous study for FaaS workflow platforms, which
have unique scaling, performance and costing behavior influenced by the
platform design, dataflow and workloads. In this article, we perform extensive
evaluations of three popular FaaS workflow platforms from AWS and Azure,
running 25 micro-benchmark and application workflows over 132k invocations. Our
detailed analysis confirms some conventional wisdom but also uncovers unique
insights on the function execution, workflow orchestration, inter-function
interactions, cold-start scaling and monetary costs. Our observations help
developers better configure and program these platforms, set performance and
scalability expectations, and identify research gaps on enhancing the
platforms.

</details>


### [78] [Memory Efficient and Staleness Free Pipeline Parallel DNN Training Framework with Improved Convergence Speed](https://arxiv.org/abs/2509.23241)
*Ankita Dutta,Nabendu Chaki,Rajat K. De*

Main category: cs.DC

TL;DR: 提出了两种基于流水线并行的DNN训练框架：V-TiMePReSt（完全无陈旧系统）和I-TiMePReSt（陈旧感知系统），分别解决权重陈旧性和GPU内存效率的平衡问题。


<details>
  <summary>Details</summary>
Motivation: 多GPU环境下DNN训练的高资源需求促使开发各种并行技术，需要解决权重陈旧性和内存消耗之间的权衡问题。

Method: V-TiMePReSt采用完全无陈旧设计确保每次前向和反向传播都使用最新权重；I-TiMePReSt通过数学公式量化陈旧权重的重要性，计算中间权重进行反向传播，不依赖完全陈旧或完全新鲜的权重。

Result: 实验表明V-TiMePReSt在权重陈旧程度和GPU内存效率方面优于现有模型；I-TiMePReSt能在不移除权重存储的情况下消除权重陈旧性，并保持GPU内存消耗和收敛速度的良好平衡。

Conclusion: 两种框架为DNN训练提供了不同的优化方案：V-TiMePReSt专注于内存效率和完全新鲜权重，I-TiMePReSt则在权重新鲜度和训练效率之间取得更好平衡。

Abstract: High resource requirement for Deep Neural Network (DNN) training across
multiple GPUs necessitates development of various parallelism techniques. In
this paper, we introduce two interconnected DNN training frameworks, namely,
V-TiMePReSt and I-TiMePReSt, based on pipeline parallelism, a variant of model
parallelism. V-TiMePReSt is a completely staleness-free system which enables
the DNNs to be trained on the latest updated weights in each stage of all
forward and backward passes. Developing staleness-aware systems at the expense
of weight stashing reduces GPU-memory consumption, however, increases the
number of epochs to converge. Thus, we introduce I-TiMePReSt, which is also a
staleness-aware system, but not at the expense of weight stashing. It does not
rely solely on the stale weights or the latest updated weights. I-TiMePReSt
computes an intermediate weight towards the latter and performs backward pass
on it. Additionally, we formulate the significance of the stale weights
mathematically depending on the degree of staleness. In contrast to
V-TiMePReSt, I-TiMePReSt works based on the assumption that stale weights have
a significant contribution in training, which can be quantified mathematically
based on the degree of staleness, although there are other contributory factors
which should not be ignored. Experimental results show that V-TiMePReSt is
advantageous over existing models in terms of $1)$ the extent of staleness of
the weight parameter values and $2)$ GPU memory efficiency, while I-TiMePReSt
is superior in terms of $1)$ removing staleness of the weight parameters
without removing weight stashing and $2)$ maintaining the trade-off between GPU
memory consumption and convergence speed (number of epochs).

</details>


### [79] [Scaling LLM Test-Time Compute with Mobile NPU on Smartphones](https://arxiv.org/abs/2509.23324)
*Zixu Hao,Jianyu Wei,Tuowei Wang,Minxing Huang,Huiqiang Jiang,Shiqi Jiang,Ting Cao,Ju Ren*

Main category: cs.DC

TL;DR: 利用移动NPU的未充分利用计算资源，通过并行测试时缩放技术提升小型LLM性能，实现与大型模型相当的精度但更低成本


<details>
  <summary>Details</summary>
Motivation: 移动设备部署LLM面临小模型性能不足和大模型资源消耗过大的矛盾，而移动NPU的矩阵乘法单元在LLM推理中存在计算资源浪费

Method: 提出硬件感知的分块量化方案与基于LUT的复杂操作替换，设计端到端推理系统在Qualcomm Snapdragon平台上支持测试时缩放

Result: 实现混合精度GEMM最高19.0倍加速和Softmax 2.2倍加速，小模型通过测试时缩放能达到或超过大模型精度

Conclusion: 该方法为移动设备LLM部署建立了新的性能-成本帕累托前沿，有效解决了移动端部署的挑战

Abstract: Deploying Large Language Models (LLMs) on mobile devices faces the challenge
of insufficient performance in smaller models and excessive resource
consumption in larger ones. This paper highlights that mobile Neural Processing
Units (NPUs) have underutilized computational resources, particularly their
matrix multiplication units, during typical LLM inference. To leverage this
wasted compute capacity, we propose applying parallel test-time scaling
techniques on mobile NPUs to enhance the performance of smaller LLMs. However,
this approach confronts inherent NPU challenges, including inadequate hardware
support for fine-grained quantization and low efficiency in general-purpose
computations. To overcome these, we introduce two key techniques: a
hardware-aware tile quantization scheme that aligns group quantization with NPU
memory access patterns, and efficient LUT-based replacements for complex
operations such as Softmax and dequantization. We design and implement an
end-to-end inference system that leverages the NPU's compute capability to
support test-time scaling on Qualcomm Snapdragon platforms. Experiments show
our approach brings significant speedups: up to 19.0 for mixed-precision GEMM
and 2.2 for Softmax. More importantly, we demonstrate that smaller models using
test-time scaling can match or exceed the accuracy of larger models, achieving
a new performance-cost Pareto frontier.

</details>


### [80] [A Predictive and Synergistic Two-Layer Scheduling Framework for LLM Serving](https://arxiv.org/abs/2509.23384)
*Yue Zhang,Yuansheng Chen,Xuan Mo,Alex Xi,Jialun Li,WeiGang Wu*

Main category: cs.DC

TL;DR: SynergySched是一个跨层LLM推理服务调度框架，通过预测性编排解决传统两层架构中的信息鸿沟问题，显著提升SLO达成率和吞吐量。


<details>
  <summary>Details</summary>
Motivation: 传统LLM推理服务的两层架构存在信息鸿沟：集群层路由器依赖滞后粗粒度指标导致决策延迟，引擎层静态调度策略无法有效处理动态负载，造成SLO违规和资源浪费。

Method: 提出SynergySched框架，包含结构感知的在线性能模型，提供准确的逐步骤延迟和容量预测。引擎层LENS组件进行SLO感知的自适应调度，集群层PRISM组件使用预测信号进行状态驱动路由。

Result: 性能评估显示SynergySched平均提升43%的SLO达成率，在长上下文和异构场景中实现高达3倍的吞吐量加速，并在FlowGPT生产环境中验证了其优势。

Conclusion: SynergySched通过跨层预测性编排有效解决了LLM推理服务中的系统低效问题，显著提升了服务质量和资源利用率。

Abstract: LLM inference serving typically scales out with a two-tier architecture: a
cluster router distributes requests to multiple inference engines, each of
which then in turn performs its own internal scheduling. However, this commonly
used paradigm suffers from critical, systemic inefficiency caused by the
information gaps across two layers. At the cluster-layer, the router mainly
relies on lagging, coarse-grained metrics, such as average latency and queue
length to make decisions, resulting in "decision lag" that leads to suboptimal
request routing. At the engine-layer, static heuristic scheduling policies
cannot effectively handle the dynamic workloads, leading a poor balance between
latency and throughput. Besides, these gaps may cause SLO violations and
resource waste, especially in heterogeneous cloud environments.
  To bridge such gaps, we propose SynergySched, a cross-layer framework that
shifts LLM serving system from reactive load balancing to predictive
orchestration. The core of SynergySched lies in a structurally-informed online
performance model that provides accurate, forward-looking per-step latency and
capacity estimations. This model empowers two key components. At the
engine-layer, LENS performs SLO-aware, adaptive scheduling, dynamically
optimizing batching to meet SLOs under real-time loads. At the cluster-layer,
PRISM uses predictive signals to perform state-driven routing, maximizing
cluster-wide performance and SLO attainment. Performance evaluations show that
SynergySched improves SLO attainment by 43% on average and achieves up to 3x
throughput speedup in long-context and heterogeneous scenarios. Besides, we
also deploy SynergySched on FlowGPT's clusters to demonstrate its advantages in
production environment.

</details>


### [81] [Enhancing Communication Efficiency in FL with Adaptive Gradient Quantization and Communication Frequency Optimization](https://arxiv.org/abs/2509.23419)
*Asadullah Tariq,Tariq Qayyum,Mohamed Adel Serhani,Farag Sallabi,Ikbal Taleb,Ezedin S. Barka*

Main category: cs.DC

TL;DR: 提出一种三阶段联邦学习通信优化策略，通过特征消除、梯度量化和通信频率优化来减少通信开销，同时保持模型精度


<details>
  <summary>Details</summary>
Motivation: 联邦学习在无线网络中面临高通信开销的瓶颈，限制了在资源受限环境中的部署

Method: 自适应特征消除策略、自适应梯度创新和误差敏感量化、通信频率优化

Result: 在保持精度的同时实现了高通信效率

Conclusion: 所提出的三阶段策略有效解决了联邦学习的通信瓶颈问题

Abstract: Federated Learning (FL) enables participant devices to collaboratively train
deep learning models without sharing their data with the server or other
devices, effectively addressing data privacy and computational concerns.
However, FL faces a major bottleneck due to high communication overhead from
frequent model updates between devices and the server, limiting deployment in
resource-constrained wireless networks. In this paper, we propose a three-fold
strategy. Firstly, an Adaptive Feature-Elimination Strategy to drop less
important features while retaining high-value ones; secondly, Adaptive Gradient
Innovation and Error Sensitivity-Based Quantization, which dynamically adjusts
the quantization level for innovative gradient compression; and thirdly,
Communication Frequency Optimization to enhance communication efficiency. We
evaluated our proposed model's performance through extensive experiments,
assessing accuracy, loss, and convergence compared to baseline techniques. The
results show that our model achieves high communication efficiency in the
framework while maintaining accuracy.

</details>


### [82] [Lyte Quorum: Off-Chain Ready Smart Contract Hosted with Choice](https://arxiv.org/abs/2509.23448)
*Hao Hao,Dahlia Malkhi,Maofan Yin,Lizan Zhou*

Main category: cs.DC

TL;DR: Lyquor是一个去中心化平台，通过服务中心模型重新构想区块链基础设施，节点可选择性地托管智能合约（Lyquids）同时保持全局可组合性。


<details>
  <summary>Details</summary>
Motivation: 解决现有区块链系统的关键限制，包括状态访问瓶颈、缺乏可扩展性以及链上和链下计算之间的协调问题，同时保持与以太坊API的兼容性。

Method: 提出三个核心创新：Fate-Constrained Ordering（FCO）将共识与执行解耦；Direct Memory Architecture（DMA）消除状态访问瓶颈；Universal Procedure Call（UPC）实现分布式链下计算的容错可编程协调。

Result: 构建了一个统一的编程模型，支持传统智能合约模式和新型分布式应用，实现了真正可扩展的去中心化计算。

Conclusion: Lyquor通过创新的架构设计解决了区块链基础设施的关键挑战，为去中心化计算提供了可扩展的解决方案，同时保持向后兼容性。

Abstract: This paper introduces Lyquor, a decentralized platform that reimagines
blockchain infrastructure through a service-centric model where nodes
selectively host smart contracts (called Lyquids) while preserving global
composability. We present three key innovations: (1) Fate-Constrained Ordering
(FCO), which decouples consensus from execution to enable selective hosting
without sacrificing Layer-1 grade composability; (2) Direct Memory Architecture
(DMA), which eliminates state access bottlenecks by providing each contract
with persistent, byte-addressable virtual memory; and (3) Universal Procedure
Call (UPC), which enables fault-tolerant, programmable coordination across
distributed off-chain computation. Together, these components are powered by a
Rust-macroed unified programming model where on-chain and off-chain logic
coexist seamlessly, supporting both traditional smart contract patterns and
novel distributed applications. Lyquor addresses critical limitations in
existing systems while maintaining compatibility with Ethereum APIs, offering a
path toward truly scalable decentralized computation.

</details>


### [83] [Parallel Algorithms for the One Sided Crossing Minimization Problem](https://arxiv.org/abs/2509.23706)
*Bogdan-Ioan Popa,Adrian-Marius Dumitran,Livia Magureanu*

Main category: cs.DC

TL;DR: 本文研究二分图布局中的单边交叉最小化(OSCM)问题，通过并行化现有精确算法和固定参数可处理算法，在16核32线程机器上实现了接近19倍的加速比。


<details>
  <summary>Details</summary>
Motivation: OSCM问题在网络可视化和VLSI设计中具有重要应用价值，但现有精确算法和FPT算法的并行化研究不足，而多核系统的发展为并行化提供了机会。

Method: 实现和分析多种已有的精确算法和固定参数可处理算法，分别在顺序和并行形式下进行实验评估。

Result: 实证证明这些算法在并行化下能够实现接近线性的加速比，最佳结果在16核32线程机器上达到近19倍的加速。

Conclusion: 并行化OSCM算法能够显著提升性能，但线性加速比并不总能实现，需要仔细处理同步和内存管理问题。

Abstract: The One Sided Crossing Minimization (OSCM) problem is an optimization problem
in graph drawing that aims to minimize the number of edge crossings in
bipartite graph layouts. It has practical applications in areas such as network
visualization and VLSI (Very Large Scale Integration) design, where reducing
edge crossings improves the arrangement of circuit components and their
interconnections. Despite the rise of multi-core systems, the parallelization
of exact and fixed-parameter tractable (FPT) algorithms for OSCM remains
largely unexplored. Parallel variants offer significant potential for scaling
to larger graphs but require careful handling of synchronization and memory
management. In this paper, we explore various previously studied exact and FPT
algorithms for OSCM, implementing and analyzing them in both sequential and
parallel forms. Our main contribution lies in empirically proving that these
algorithms can achieve close to linear speedup under parallelization. In
particular, our best result achieves a speedup of nearly 19 on a 16-core,
32-thread machine. We further investigate and discuss the reasons why linear
speedup is not always attained.

</details>


### [84] [AdaPtis: Reducing Pipeline Bubbles with Adaptive Pipeline Parallelism on Heterogeneous Models](https://arxiv.org/abs/2509.23722)
*Jihu Guo,Tenghui Ma,Wei Gao,Peng Sun,Jiaxing Li,Xun Chen,Yuyang Jin,Dahua Lin*

Main category: cs.DC

TL;DR: AdaPtis是一个自适应流水线并行训练系统，通过联合优化模型划分、设备放置和工作负载调度，显著提升大型语言模型训练效率


<details>
  <summary>Details</summary>
Motivation: 现有流水线并行方法忽视了模型划分、设备放置和工作负载调度的协同优化，导致训练效率提升有限甚至性能下降

Method: 开发流水线性能模型准确估计训练吞吐量；联合优化模型划分、设备放置和工作负载调度策略；设计统一流水线执行器支持多样化流水线策略

Result: 在各种LLM架构和规模上相比Megatron-LM I-1F1B平均加速1.42倍，最高可达2.14倍

Conclusion: AdaPtis通过自适应流水线并行方法有效解决了异构架构导致的流水线气泡问题，显著提升了大型语言模型训练效率

Abstract: Pipeline parallelism is widely used to train large language models (LLMs).
However, increasing heterogeneity in model architectures exacerbates pipeline
bubbles, thereby reducing training efficiency. Existing approaches overlook the
co-optimization of model partition, model placement, and workload scheduling,
resulting in limited efficiency improvement or even performance degradation. To
respond, we propose AdaPtis, an LLM training system that supports adaptive
pipeline parallelism. First, we develop a pipeline performance model to
accurately estimate training throughput. Second, AdaPtis jointly optimizes
model partition, model placement, and workload scheduling policies guided by
this performance model. Third, we design a unified pipeline executor that
efficiently supports the execution of diverse pipeline strategies. Extensive
experiments show that AdaPtis achieves an average speedup of 1.42x (up to
2.14x) over Megatron-LM I-1F1B across various LLM architectures and scales.

</details>


### [85] [Graph Theory Meets Federated Learning over Satellite Constellations: Spanning Aggregations, Network Formation, and Performance Optimization](https://arxiv.org/abs/2509.24932)
*Fardis Nadimi,Payam Abdisarabshali,Jacob Chakareski,Nicholas Mastronarde,Seyyedali Hosseinalipour*

Main category: cs.DC

TL;DR: Fed-Span是一个基于最小生成树拓扑的联邦学习框架，专为低地球轨道卫星星座设计，通过图论方法解决卫星网络中的连接不稳定、计算能力异构和数据集时变等挑战。


<details>
  <summary>Details</summary>
Motivation: 解决卫星星座分布式学习中的关键挑战，包括间歇性连接、卫星计算能力异构以及数据集随时间变化的问题，需要一种高效的联邦学习框架。

Method: 基于最小生成树和最小生成森林拓扑，通过连续约束表示构建可优化框架，分析能耗和延迟，推导非凸损失函数的收敛边界，并将NP难问题转化为几何规划形式求解。

Result: 在真实数据集上的评估显示，Fed-Span相比现有方法具有更快的模型收敛速度、更高的能源效率和更低的延迟。

Conclusion: Fed-Span是卫星网络中高效分布式学习的新颖解决方案，通过图论优化方法有效解决了卫星网络特有的挑战。

Abstract: We introduce Fed-Span, a novel federated/distributed learning framework
designed for low Earth orbit satellite constellations. By leveraging
graph-theoretic principles, Fed-Span addresses critical challenges inherent to
distributed learning in dynamic satellite networks, including intermittent
satellite connectivity, heterogeneous computational capabilities of satellites,
and time-varying satellites' datasets. At its core, Fed-Span builds upon
minimum spanning tree (MST) and minimum spanning forest (MSF) topologies,
enabling spanning model aggregation and dispatching processes for distributed
learning. To formalize Fed-Span, we offer a fresh perspective on MST/MSF
topologies by formulating them through a set of continuous constraint
representations (CCRs), thereby devising graph-theoretical abstractions into an
optimizable framework for satellite networks. Using these CCRs, we obtain the
energy consumption and latency of operations in Fed-Span. Moreover, we derive
novel convergence bounds for non-convex machine learning loss functions,
accommodating the key system characteristics and degrees of freedom of
Fed-Span. Finally, we propose a comprehensive optimization problem that jointly
minimizes model prediction loss, energy consumption, and latency of Fed-Span.
We unveil that this problem is NP-hard and develop a systematic approach to
transform it into a geometric programming formulation, solved via successive
convex optimization with performance guarantees. Through evaluations on
real-world datasets, we demonstrate that Fed-Span outperforms existing methods,
with faster model convergence, greater energy efficiency, and reduced latency.
These results highlight Fed-Span as a novel solution for efficient distributed
learning in satellite networks.

</details>


### [86] [From Edge to HPC: Investigating Cross-Facility Data Streaming Architectures](https://arxiv.org/abs/2509.24030)
*Anjus George,Michael Brim,Christopher Zimmer,David Rogers,Sarp Oral,Zach Mayes*

Main category: cs.DC

TL;DR: 本文比较了三种跨设施数据流架构(DTS、PRS、MSS)，在HPC环境中评估其性能表现，发现DTS具有最佳吞吐量和延迟，MSS部署可行性最高但开销较大，PRS在性能和可扩展性之间取得平衡。


<details>
  <summary>Details</summary>
Motivation: 研究跨设施数据流架构在科学工作流中的性能表现，为AI-HPC通信模式提供架构选择指导。

Method: 使用DS2HPC架构框架和SciStream工具包，在OLCF的ACE基础设施上实现三种架构，通过模拟实验使用三种合成工作负载测试吞吐量、往返时间和开销。

Result: DTS提供最小跳数路径，具有更高吞吐量和更低延迟；MSS提供更好的部署可行性和多用户扩展性但开销显著；PRS性能在大多数情况下与DTS相当且具有可扩展性。

Conclusion: 不同架构各有优劣：DTS适合性能敏感场景，MSS适合多用户部署，PRS在性能和可扩展性之间提供良好平衡。

Abstract: In this paper, we investigate three cross-facility data streaming
architectures, Direct Streaming (DTS), Proxied Streaming (PRS), and Managed
Service Streaming (MSS). We examine their architectural variations in data flow
paths and deployment feasibility, and detail their implementation using the
Data Streaming to HPC (DS2HPC) architectural framework and the SciStream
memory-to-memory streaming toolkit on the production-grade Advanced Computing
Ecosystem (ACE) infrastructure at Oak Ridge Leadership Computing Facility
(OLCF). We present a workflow-specific evaluation of these architectures using
three synthetic workloads derived from the streaming characteristics of
scientific workflows. Through simulated experiments, we measure streaming
throughput, round-trip time, and overhead under work sharing, work sharing with
feedback, and broadcast and gather messaging patterns commonly found in AI-HPC
communication motifs. Our study shows that DTS offers a minimal-hop path,
resulting in higher throughput and lower latency, whereas MSS provides greater
deployment feasibility and scalability across multiple users but incurs
significant overhead. PRS lies in between, offering a scalable architecture
whose performance matches DTS in most cases.

</details>


### [87] [RServe: Overlapping Encoding and Prefill for Efficient LMM Inference](https://arxiv.org/abs/2509.24381)
*Tianyu Guo,Tianming Xu,Xianjie Chen,Junru Chen,Nong Xiao,Xianwei Zhang*

Main category: cs.DC

TL;DR: REDServe是一个高效的多模态模型推理系统，通过解耦编码模块和语言模型，采用细粒度调度策略，在单个请求内重叠多模态编码和语言模型计算，同时在多个请求间平衡计算负载，显著降低了延迟并提高了吞吐量。


<details>
  <summary>Details</summary>
Motivation: 现有的大型多模态模型推理系统存在资源干扰和数据依赖问题，虽然已有研究通过解耦编码模块和语言模型来缓解这一问题，但仍未能充分利用请求内和请求间的并行性。

Method: 基于解耦架构，REDServe采用细粒度调度方法：在请求内重叠多模态编码和语言模型前向计算；在请求间利用可调度token和token预算来平衡微批次间的计算负载，结合分块预填充实现新颖的调度策略。

Result: 实验评估显示，REDServe在代表性多模态模型上实现了高达66%的延迟降低和109%的吞吐量提升，显著优于现有服务方法。

Conclusion: REDServe通过有效协调请求内和请求间流水线，解决了多模态模型推理中的并行性利用问题，为高效服务大型多模态模型提供了有效的解决方案。

Abstract: Large multimodal models (LMMs) typically employ an encoding module to
transform multimodal data inputs into embeddings, which are then fed to
language models for further processing. However, efficiently serving LMMs
remains highly challenging due to the inherent complexity of their inference
pipelines. Traditional serving engines co-locate the encoding module and the
language model, leading to significant resource interference and tight data
dependency. Recent studies have alleviated this issue by disaggregating the
encoding module from the model, following a design style of prefill-decode
disaggregation. Nevertheless, these approaches fail to fully exploit
parallelism both within individual requests (intra-request) and across multiple
requests (inter-request).
  To overcome the limitation, we propose REDServe, an LMM inference system that
efficiently orchestrates intra- and inter-request pipelines. REDServe is
designed to reduce low latency and maximize parallelism at both intra- and
inter-request granularities. Built on the disaggregated architecture of the
encoding module and language model, REDServe adopts a fine-grained scheduling
method that overlaps multimodal encoding with the forward computation of the
language model within a single request. For inter-request pipeline, REDServe
leverages schedulable tokens and token budgets to balance computational loads
across micro-batches. Combined with chunked prefill, this enables a novel
scheduling strategy that coordinates the execution of intra- and inter-request
pipelines. Experimental evaluations on representative LMMs show that REDServe
achieves substantial latency reduction of up to 66% while improving throughput
by up to 109%, significantly outperforming existing serving approaches.

</details>


### [88] [SparseServe: Unlocking Parallelism for Dynamic Sparse Attention in Long-Context LLM Serving](https://arxiv.org/abs/2509.24626)
*Qihui Zhou,Peiqi Yin,Pengfei Zuo,James Cheng*

Main category: cs.DC

TL;DR: SparseServe通过分层HBM-DRAM管理解决动态稀疏注意力的HBM容量瓶颈问题，实现了更低的首次令牌延迟和更高的令牌生成吞吐量


<details>
  <summary>Details</summary>
Motivation: 长上下文LLM服务成本高，动态稀疏注意力算法虽然减轻了计算负担，但将性能瓶颈从HBM带宽转移到了HBM容量，未选中的KV缓存仍需保留在HBM中限制了并行批处理大小

Method: 提出SparseServe系统，包含三个关键技术：(1)碎片感知KV缓存传输，(2)工作集感知批处理大小控制，(3)分层预填充

Result: 相比最先进的LLM服务系统，实现了9.26倍更低的平均首次令牌延迟和3.14倍更高的令牌生成吞吐量

Conclusion: SparseServe通过有效的分层存储管理成功解锁了动态稀疏注意力的并行潜力，显著提升了长上下文LLM服务的性能

Abstract: Serving long-context LLMs is costly because attention computation grows
linearly with context length. Dynamic sparse attention algorithms (DSAs)
mitigate this by attending only to the key-value (KV) cache of critical tokens.
However, with DSAs, the main performance bottleneck shifts from HBM bandwidth
to HBM capacity: KV caches for unselected tokens must remain in HBM for
low-latency decoding, constraining parallel batch size and stalling further
throughput gains. Offloading these underutilized KV caches to DRAM could free
HBM capacity, allowing larger parallel batch sizes. Yet, achieving such
hierarchical HBM-DRAM storage raises new challenges, including fragmented KV
cache access, HBM cache contention, and high HBM demands of hybrid batching,
that remain unresolved in prior work.
  This paper proposes SparseServe, an LLM serving system that unlocks the
parallel potential of DSAs through efficient hierarchical HBM-DRAM management.
SparseServe introduces three key innovations to address the challenges
mentioned above: (1) fragmentation-aware KV cache transfer, which accelerates
HBM-DRAM data movement through GPU-direct loading (FlashH2D) and CPU-assisted
saving (FlashD2H); (2) working-set-aware batch size control that adjusts batch
sizes based on real-time working set estimation to minimize HBM cache
thrashing; (3) layer-segmented prefill that bounds HBM use during prefill to a
single layer, enabling efficient execution even for long prompts. Extensive
experimental results demonstrate that SparseServe achieves up to 9.26x lower
mean time-to-first-token (TTFT) latency and up to 3.14x higher token generation
throughput compared to state-of-the-art LLM serving systems.

</details>


### [89] [HAPT: Heterogeneity-Aware Automated Parallel Training on Heterogeneous Clusters](https://arxiv.org/abs/2509.24859)
*Antian Liang,Zhigang Zhao,Kai Zhang,Xuri Shi,Chuantao Li,Chunxiao Wang,Zhenying He,Yinan Jing,X. Sean Wang*

Main category: cs.DC

TL;DR: Hapt是一个专为异构集群设计的自动并行训练框架，通过细粒度规划器和异构感知调度器，在异构GPU环境中实现1.3-1.6倍的性能提升


<details>
  <summary>Details</summary>
Motivation: 随着GPU架构快速演进，训练基础设施的异构性不断增加。现有主要针对同构集群设计的框架在异构加速器和网络上表现出显著的资源利用不足问题

Method: Hapt引入细粒度规划器搜索算子间并行策略，减轻通信开销并保持异构加速器间的负载均衡；同时实现异构感知的1F1B调度器，根据网络特性自适应调整微批次的执行时序和顺序

Result: 评估结果显示Hapt在异构集群上比最先进的训练框架性能高出1.3-1.6倍

Conclusion: Hapt框架通过创新的规划器和调度器设计，有效解决了异构集群中的资源利用问题，显著提升了分布式模型训练的性能

Abstract: With the rapid evolution of GPU architectures, the heterogeneity of model
training infrastructures is steadily increasing. In such environments,
effectively utilizing all available heterogeneous accelerators becomes critical
for distributed model training. However, existing frameworks, which are
primarily designed for homogeneous clusters, often exhibit significant resource
underutilization when deployed on heterogeneous accelerators and networks. In
this paper, we present Hapt, an automated parallel training framework designed
specifically for heterogeneous clusters. Hapt introduces a fine-grained planner
that efficiently searches a wide space for the inter-operator parallel
strategy, enabling Hapt to alleviate communication overheads while maintaining
balanced loads across heterogeneous accelerators. In addition, Hapt implements
a heterogeneity-aware 1F1B scheduler that adaptively adjusts the execution
timing and ordering of microbatches based on network characteristics,
maximizing computation-communication overlap under cross-cluster interconnects
while incurring only minimal memory overhead. Our evaluation results show that
Hapt can deliver 1.3x-1.6x higher performance on heterogeneous clusters than
state-of-the-art training frameworks.

</details>


### [90] [GRACE-MoE: Grouping and Replication with Locality-Aware Routing for Efficient Distributed MoE Inference](https://arxiv.org/abs/2509.25041)
*Yu Han,Lehan Pan,Jie Peng,Ziyang Tao,Wuyang Zhang,Yanyong Zhang*

Main category: cs.DC

TL;DR: GRACE-MoE是一个针对稀疏专家混合模型推理的协同优化框架，通过分组复制和局部感知路由策略，同时减少通信开销和计算负载不平衡，实现最高3.79倍的推理加速。


<details>
  <summary>Details</summary>
Motivation: 稀疏专家混合模型(SMoE)在分布式推理时面临两个关键挑战：1)跨设备传输特征导致的通信开销；2)专家激活不均衡造成的计算负载不平衡。其中通信开销被识别为主要瓶颈，但减少通信可能会加剧负载不平衡问题。

Method: GRACE-MoE框架包含两个关键阶段：1)分组与复制：基于专家亲和性进行分组以减少跨设备通信，同时应用动态复制解决负载倾斜；2)路由：采用带有负载预测的局部感知路由策略，优先选择本地副本最小化通信，必要时平衡远程副本请求。

Result: 在多样化模型和多节点多GPU环境下的实验表明，GRACE-MoE有效减少了端到端推理延迟，相比最先进系统实现了最高3.79倍的加速比。

Conclusion: GRACE-MoE成功解决了SMoE分布式推理中的通信和计算负载平衡问题，通过协同优化策略显著提升了推理效率，为大规模语言模型的部署提供了有效解决方案。

Abstract: Sparse Mixture of Experts (SMoE) performs conditional computation by
selectively activating a subset of experts, thereby enabling scalable parameter
growth in large language models (LLMs). However, the expanded parameter scale
exceeds the memory capacity of a single device, necessitating distributed
deployment for inference. This setup introduces two critical challenges: (1)
Communication Issue: Transferring features to devices with activated experts
leads to significant communication overhead. (2) Computational Load Issue:
Skewed expert activation overloads certain GPUs, resulting in load imbalance
across devices. Among these, communication overhead is identified as the main
bottleneck in SMoE inference. Nevertheless, reducing communication between
devices may exacerbate computational load imbalance, leading to device idleness
and resource waste. Therefore, we present GRACE-MoE, short for Grouping and
Replication with Locality-Aware Routing for SMoE inference. GRACE-MoE is a
co-optimization framework that jointly reduces communication overhead and
alleviates computational load imbalance. Specifically, the framework comprises
two key phases: (1) Grouping & Replication: This phase groups experts based on
their affinity to reduce cross-device communication. Additionally, dynamic
replication is applied to address load skew, improving computational load
balance across GPUs. (2) Routing: This phase employs a locality-aware routing
strategy with load prediction. It prioritizes local replicas to minimize
communication overhead and balances requests across remote replicas when
necessary. Experiments on diverse models and multi-node, multi-GPU environments
demonstrate that GRACE-MoE efficiently reduces end-to-end inference latency,
achieving up to 3.79x speedup over state-of-the-art systems. Code for GRACE-MoE
will be released upon acceptance.

</details>


### [91] [Accelerating Dynamic Image Graph Construction on FPGA for Vision GNNs](https://arxiv.org/abs/2509.25121)
*Anvitha Ramachandran,Dhruv Parikh,Viktor Prasanna*

Main category: cs.DC

TL;DR: 提出了一种基于FPGA的流式深度流水线加速器，用于解决Vision GNN中动态图像图构建(DIGC)的计算瓶颈问题，相比CPU和GPU实现了显著加速。


<details>
  <summary>Details</summary>
Motivation: Vision GNNs中的动态图像图构建(DIGC)占据了超过50%的推理延迟，在高分辨率下甚至达到95%，成为主要计算瓶颈，需要硬件加速解决方案。

Method: 设计流式深度流水线FPGA加速器，采用片上缓冲处理小块输入特征，通过局部计算减少外部内存访问，使用局部归并排序和全局k路合并进行高效并行排序。

Result: 设计实现了高时钟频率，相比优化的CPU和GPU基准测试分别获得了16.6倍和6.8倍的加速比。

Conclusion: 该模块化架构能够无缝扩展到不同图像分辨率、ViG层类型和模型变体，为基于ViG的视觉骨干网络提供高效的DIGC加速解决方案。

Abstract: Vision Graph Neural Networks (Vision GNNs, or ViGs) represent images as
unstructured graphs, achieving state of the art performance in computer vision
tasks such as image classification, object detection, and instance
segmentation. Dynamic Image Graph Construction (DIGC) builds image graphs by
connecting patches (nodes) based on feature similarity, and is dynamically
repeated in each ViG layer following GNN based patch (node) feature updates.
However, DIGC constitutes over 50% of end to end ViG inference latency, rising
to 95% at high image resolutions, making it the dominant computational
bottleneck. While hardware acceleration holds promise, prior works primarily
optimize graph construction algorithmically, often compromising DIGC
flexibility, accuracy, or generality. To address these limitations, we propose
a streaming, deeply pipelined FPGA accelerator for DIGC, featuring on chip
buffers that process input features in small, uniform blocks. Our design
minimizes external memory traffic via localized computation and performs
efficient parallel sorting with local merge sort and global k way merging
directly on streaming input blocks via heap insertion. This modular
architecture scales seamlessly across image resolutions, ViG layer types, and
model sizes and variants, and supports DIGC across diverse ViG based vision
backbones. The design achieves high clock frequencies post place and route due
to the statically configured parallelism minimizing critical path delay and
delivers up to 16.6x and 6.8x speedups over optimized CPU and GPU DIGC
baselines.

</details>


### [92] [Context-Driven Performance Modeling for Causal Inference Operators on Neural Processing Units](https://arxiv.org/abs/2509.25155)
*Neelesh Gupta,Rakshith Jayanth,Dhruv Parikh,Viktor Prasanna*

Main category: cs.DC

TL;DR: 对现代NPU上各种因果推理算子的性能分析，发现二次注意力机制在长上下文时严重受内存限制，而次二次方法虽然扩展性更好但在NPU上会产生新的计算瓶颈


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在资源受限的边缘设备上部署面临挑战，标准注意力机制的二次复杂度与边缘加速器的内存和计算模式不匹配

Method: 在现代NPU上对标准二次注意力和多种次二次替代方案（包括结构化状态空间和线性注意力模型）进行基准测试

Result: 二次注意力在长上下文时缓存效率低下，流水线停顿超过95%；次二次模型在可编程向量核心上可能变得计算受限

Conclusion: 研究结果为硬件感知模型和优化策略的协同设计提供了关键见解，以支持设备端长上下文AI推理

Abstract: The proliferation of large language models (LLMs) has driven demand for long
context inference on resource constrained edge devices. However, deploying
these models on Neural Processing Units (NPUs) presents significant challenges
due to the architectural mismatch: quadratic complexity of standard attention
mechanisms conflicts with memory and compute patterns of edge accelerators.
This paper presents a comprehensive performance analysis of various causal
inference operators on a modern NPU. We benchmark standard quadratic attention
against several sub-quadratic alternatives, including structured state-space
and linear attention models. Our analysis reveals that while sub-quadratic
methods offer superior scalability, they introduce distinct computational
bottlenecks on the NPU's specialized execution units. We identify that
quadratic attention becomes severely memory-bound, suffering from cache
inefficiency and pipeline stalls exceeding 95% at long contexts. In contrast,
sub-quadratic models can become compute-bound on programmable vector cores.
These findings provide critical insights for the co-design of hardware-aware
models and optimization strategies to enable on-device AI inference with
long-contexts.

</details>


<div id='cs.OS'></div>

# cs.OS [[Back]](#toc)

### [93] [Joyride: Rethinking Linux's network stack design for better performance, security, and reliability](https://arxiv.org/abs/2509.25015)
*Yanlin Du,Ruslan Nikolaev*

Main category: cs.OS

TL;DR: Joyride是一个高性能网络框架，旨在替代Linux传统网络栈，通过集成DPDK和用户空间TCP/IP栈来提供高性能网络处理，同时保持与现有应用的兼容性。


<details>
  <summary>Details</summary>
Motivation: 现代分布式计算工作负载需要低延迟、高吞吐量、安全可靠的网络通信，但Linux传统TCP/IP栈在高性能网卡（100Gbps及以上）上存在性能瓶颈，主要来自内核空间处理、模式切换和数据拷贝的开销。

Method: 提出Joyride框架，采用微内核架构设计，集成内核旁路技术（DPDK）和用户空间TCP/IP栈，在保持应用兼容性的同时实现高性能网络处理。

Result: 该框架旨在解决传统网络栈的性能限制，同时避免DPDK和RDMA等替代方案需要大量应用重构和硬件依赖的问题。

Conclusion: Joyride提供了一个有前景的解决方案，能够在高性能网络硬件上提供卓越性能，同时维持与现有Linux应用的向后兼容性。

Abstract: Contemporary distributed computing workloads, including scientific
computation, data mining, and machine learning, increasingly demand OS
networking with minimal latency as well as high throughput, security, and
reliability. However, Linux's conventional TCP/IP stack becomes increasingly
problematic for high-end NICs, particularly those operating at 100 Gbps and
beyond.
  These limitations come mainly from overheads associated with kernel space
processing, mode switching, and data copying in the legacy architecture.
Although kernel bypass techniques such as DPDK and RDMA offer alternatives,
they introduce significant adoption barriers: both often require extensive
application redesign, and RDMA is not universally available on commodity
hardware.
  This paper proposes Joyride, a high performance framework with a grand vision
of replacing Linux's legacy network stack while providing compatibility with
existing applications. Joyride aims to integrate kernel bypass ideas,
specifically DPDK and a user-space TCP/IP stack, while designing a
microkernel-style architecture for Linux networking.

</details>


<div id='cs.PF'></div>

# cs.PF [[Back]](#toc)

### [94] [Tiny-QMoE](https://arxiv.org/abs/2509.22951)
*Jack Cashman,Jiaqi Nie*

Main category: cs.PF

TL;DR: QMoE模型专注于压缩大规模混合专家模型，但主要针对高端服务器硬件，未充分考虑移动设备等资源受限环境的特殊需求。


<details>
  <summary>Details</summary>
Motivation: 现有QMoE方法虽然能压缩TB级MoE模型，但主要针对NVIDIA高端服务器硬件（如H100/V100的80GB HBM），忽略了移动设备等资源严重受限的环境（如iPhone仅有4-8GB统一内存）以及网络延迟问题。

Method: 论文指出QMoE方法的局限性，强调需要专门针对移动设备和边缘计算环境的压缩解决方案，考虑内存共享、延迟敏感性和离线使用需求。

Result: 分析表明当前QMoE方法在移动设备部署方面存在不足，需要开发更适合资源受限环境的压缩技术。

Conclusion: 需要在QMoE基础上开发面向移动设备和边缘计算的优化压缩方案，解决内存限制、延迟问题和离线使用需求，以实现大规模MoE模型在资源受限环境中的高效部署。

Abstract: The QMoE model provides a practical approach for compression of massive
Mixture-of-Experts (MoE) models. QMoE offers a solution geared towards memory
limitations that often reach terabyte scales, and it has the advantage of
working with high sparsity models which implicitly lend themselves to
compression techniques. QMoE also has the advantage of only taking MoE models
into account and does not evaluate its use with non mixture of expert systems.
Although this prior attempt focuses on the limitations of large servers with
the latest NVIDIA hardware which in the case of the H100 and V100 which have 80
GB of HBM (High Bandwidth Memory), what is not being considered is a
significantly more constrained environment, such as in the case of mobile
devices which may have in the case of the iPhone anywhere from 4 to 8 GB of
unified memory which also needs to be shared with the operating system and
additional processes. Although edge devices such as phones and laptops are
becoming increasingly more computationally powerful, they are still not close
to the level of advanced server machines such as NVIDIA. An additional
constraint that we must consider is that of latency. The communication time of
sending a request to an LLM server and then getting it back is an additional
waiting time that can be removed. We may also want to use LLM technology in
environments where there is no reliable network connection.

</details>


### [95] [DarwinGame: Playing Tournaments for Tuning Applications in Noisy Cloud Environments](https://arxiv.org/abs/2509.25090)
*Rohan Basu Roy,Vijay Gadepally,Devesh Tiwari*

Main category: cs.PF

TL;DR: DarwinGame是一个针对共享干扰环境设计的性能调优器，通过锦标赛式方法在噪声环境中比较不同参数配置的相对性能，相比现有方案减少27%执行时间且性能变异小于0.5%。


<details>
  <summary>Details</summary>
Motivation: 现有性能调优器在设计上无法考虑共享计算环境中的干扰因素，导致在干扰易发的共享环境中表现显著次优。

Method: 采用基于锦标赛的设计方法，系统性地比较不同可调参数配置的应用程序执行情况，能够在噪声环境中识别不同配置的相对性能。

Result: 与现有解决方案相比，DarwinGame实现了超过27%的执行时间减少，性能变异小于0.5%。

Conclusion: DarwinGame是首个能够帮助开发者在共享、干扰易发的云环境中调优应用程序性能的调优器。

Abstract: This work introduces a new subarea of performance tuning -- performance
tuning in a shared interference-prone computing environment. We demonstrate
that existing tuners are significantly suboptimal by design because of their
inability to account for interference during tuning. Our solution, DarwinGame,
employs a tournament-based design to systematically compare application
executions with different tunable parameter configurations, enabling it to
identify the relative performance of different tunable parameter configurations
in a noisy environment. Compared to existing solutions, DarwinGame achieves
more than 27% reduction in execution time, with less than 0.5% performance
variability. DarwinGame is the first performance tuner that will help
developers tune their applications in shared, interference-prone, cloud
environments.

</details>
