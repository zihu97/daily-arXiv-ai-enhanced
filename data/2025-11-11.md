<div id=toc></div>

# Table of Contents

- [cs.SE](#cs.SE) [Total: 29]
- [cs.NI](#cs.NI) [Total: 2]
- [cs.MA](#cs.MA) [Total: 3]
- [cs.DC](#cs.DC) [Total: 8]
- [cs.AR](#cs.AR) [Total: 7]


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [1] [LLMs as Packagers of HPC Software](https://arxiv.org/abs/2511.05626)
*Caetano Melone,Daniel Nichols,Konstantinos Parasyris,Todd Gamblin,Harshitha Menon*

Main category: cs.SE

TL;DR: 本文提出了SpackIt框架，通过结合代码仓库分析、示例检索和诊断反馈迭代优化，显著提升了大语言模型自动生成Spack构建配方的成功率，从零样本的20%提高到最佳配置下的80%以上。


<details>
  <summary>Details</summary>
Motivation: 高性能计算（HPC）软件生态高度异构，依赖大量外部包，其构建配方需手动编写且维护成本高；尽管大语言模型在代码生成方面有潜力，但自动生成正确且可维护的Spack配方仍具挑战。

Method: 提出SpackIt端到端框架，整合仓库分析、相关示例检索和基于诊断反馈的迭代优化，用于辅助大语言模型生成Spack构建配方。

Result: 在308个开源HPC包的测试集上，SpackIt将安装成功率从零样本设置的20%提升至最佳配置下的80%以上。

Conclusion: 检索机制与结构化反馈能显著提升大语言模型在复杂构建系统中生成可靠软件包配方的能力，为自动化HPC软件生态维护提供了有效路径。

Abstract: High performance computing (HPC) software ecosystems are inherently heterogeneous, comprising scientific applications that depend on hundreds of external packages, each with distinct build systems, options, and dependency constraints. Tools such as Spack automate dependency resolution and environment management, but their effectiveness relies on manually written build recipes. As these ecosystems grow, maintaining existing specifications and creating new ones becomes increasingly labor-intensive. While large language models (LLMs) have shown promise in code generation, automatically producing correct and maintainable Spack recipes remains a significant challenge. We present a systematic analysis of how LLMs and context-augmentation methods can assist in the generation of Spack recipes. To this end, we introduce SpackIt, an end-to-end framework that combines repository analysis, retrieval of relevant examples, and iterative refinement through diagnostic feedback. We apply SpackIt to a representative subset of 308 open-source HPC packages to assess its effectiveness and limitations. Our results show that SpackIt increases installation success from 20% in a zero-shot setting to over 80% in its best configuration, demonstrating the value of retrieval and structured feedback for reliable package synthesis.

</details>


### [2] [Accelerating Control Systems with GitOps: A Path to Automation and Reliability](https://arxiv.org/abs/2511.05663)
*M. Gonzalez,M. Acosta*

Main category: cs.SE

TL;DR: 该论文介绍了GitOps如何通过将Git作为声明式配置的唯一真实来源，推动控制系统的基础设施现代化，并以Fermilab的ACORN项目为例，展示了GitOps、容器化、基础设施即代码及现代数据管道等技术在加速器控制体系中的应用。


<details>
  <summary>Details</summary>
Motivation: 传统控制系统基础设施难以满足现代科研设施对自动化、可审计性和版本控制的需求，因此需要借助GitOps等云原生方法实现现代化转型。

Method: 采用GitOps、容器化、基础设施即代码（IaC）以及现代数据管道等业界最佳实践与前沿技术标准，用于控制系统的基础设施与软件现代化。

Result: ACORN项目成功将GitOps等现代软件工程实践引入Fermilab加速器控制系统，为控制数据采集和AI/ML集成奠定基础。

Conclusion: GitOps及其配套技术为科研机构的控制系统现代化提供了有效路径，提升了基础设施的自动化水平、可追溯性与可维护性。

Abstract: GitOps is a foundational approach for modernizing infrastructure by leveraging Git as the single source of truth for declarative configurations. The poster explores how GitOps transforms traditional control system infrastructure, services and applications by enabling fully automated, auditable, and version-controlled infrastructure management. Cloud-native and containerized environments are shifting the ecosystem not only in the IT industry but also within the computational science field, as is the case of CERN [1] and Diamond Light Source [2] among other Accelerator/Science facilities which are slowly shifting towards modern software and infrastructure paradigms. The ACORN project, which aims to modernize Fermilab's control system infrastructure and software is implementing proven best-practices and cutting-edge technology standards including GitOps, containerization, infrastructure as code and modern data pipelines for control system data acquisition and the inclusion of AI/ML in our accelerator complex.

</details>


### [3] [SWE-fficiency: Can Language Models Optimize Real-World Repositories on Real Workloads?](https://arxiv.org/abs/2511.06090)
*Jeffrey Jian Ma,Milad Hashemi,Amir Yazdanbakhsh,Kevin Swersky,Ofir Press,Enhui Li,Vijay Janapa Reddi,Parthasarathy Ranganathan*

Main category: cs.SE

TL;DR: 本文提出了 SWE-fficiency 基准，用于评估智能体在真实代码库中进行性能优化的能力，强调“如何修复”而非仅识别问题。该基准包含来自9个流行仓库的498项任务，要求智能体分析语义、定位瓶颈并生成有效补丁。实验表明当前先进智能体在此任务上表现不佳，平均仅达到专家提速效果的15%。


<details>
  <summary>Details</summary>
Motivation: 现有基准多关注“修复什么”，而缺乏对“如何修复”性能问题的评估，尤其在大规模软件仓库中缺少衡量自动化性能优化能力的标准。

Method: 构建 SWE-fficiency 基准，通过自动化流程从 GitHub PR 中提取性能优化提交，结合关键词过滤、静态分析、覆盖率工具和执行验证，生成包含完整代码库、慢速工作负载、专家提速基线和相关单元测试的任务集。

Result: 对当前先进智能体的评估显示其平均仅实现专家提速效果的不到0.15倍，在定位优化点、跨函数执行推理和保持补丁正确性方面存在显著困难。

Conclusion: SWE-fficiency 为自动化性能工程和长程软件推理研究提供了新基准和数据管道，揭示了当前智能体在实际性能优化任务中的不足。

Abstract: Optimizing the performance of large-scale software repositories demands expertise in code reasoning and software engineering (SWE) to reduce runtime while preserving program correctness. However, most benchmarks emphasize what to fix rather than how to fix code. We introduce \textsc{SWE-fficiency}, a benchmark for evaluating repository-level performance optimization on real workloads. Our suite contains 498 tasks across nine widely used data-science, machine-learning, and HPC repositories (e.g., numpy, pandas, scipy): given a complete codebase and a slow workload, an agent must investigate code semantics, localize bottlenecks and relevant tests, and produce a patch that matches or exceeds expert speedup while passing the same unit tests. To enable this how-to-fix evaluation, our automated pipeline scrapes GitHub pull requests for performance-improving edits, combining keyword filtering, static analysis, coverage tooling, and execution validation to both confirm expert speedup baselines and identify relevant repository unit tests. Empirical evaluation of state-of-the-art agents reveals significant underperformance. On average, agents achieve less than 0.15x the expert speedup: agents struggle in localizing optimization opportunities, reasoning about execution across functions, and maintaining correctness in proposed edits. We release the benchmark and accompanying data pipeline to facilitate research on automated performance engineering and long-horizon software reasoning.

</details>


### [4] [An Empirical Study of Java Code Improvements Based on Stack Overflow Answer Edits](https://arxiv.org/abs/2511.05813)
*In-on Wiratsin,Chaiyong Ragkhitwetsagul,Matheus Paixao,Denis De Sousa,Pongpop Lapvikai,Peter Haddawy*

Main category: cs.SE

TL;DR: 本文通过实证研究分析了Stack Overflow上Java答案的编辑历史，并利用这些编辑改进开源项目中的代码。研究发现，近一半的编辑代码片段适用于开源项目，且部分基于这些编辑提出的修复建议已被项目维护者采纳。


<details>
  <summary>Details</summary>
Motivation: 软件系统中普遍存在次优代码，导致高昂的维护成本和技术债务。开发者常借助Stack Overflow等外部知识库辅助编程，而其答案内容会不断更新优化。作者希望探索如何利用这些更新后的优质代码片段来改进现有开源项目中的低质量代码。

Method: 作者使用改进的代码克隆搜索工具，结合SOTorrent中140,840个Java被采纳答案及其版本历史，对10,668个GitHub Java项目进行分析。通过人工分类答案编辑类型，并向开源项目提交拉取请求（Pull Requests）以建议代码改进。

Result: 研究发现6.91%的Java被采纳答案有多次修订（平均2.82次），其中49.24%的编辑代码片段可应用于开源项目。在提交的36个基于编辑的bug修复建议中，有11个被项目维护者接受。

Conclusion: Stack Overflow上的答案编辑蕴含大量可复用的代码改进机会，能有效帮助识别和修复开源项目中的次优代码，证明社区协作知识对提升软件质量具有实际价值。

Abstract: Suboptimal code is prevalent in software systems. Developers often write low-quality code due to factors like technical knowledge gaps, insufficient experience, time pressure, management decisions, or personal factors. Once integrated, the accumulation of this suboptimal code leads to significant maintenance costs and technical debt.
  Developers frequently consult external knowledge bases, such as API documentation and Q&A websites like Stack Overflow (SO), to aid their programming tasks. SO's crowdsourced, collaborative nature has created a vast repository of programming knowledge. Its community-curated content is constantly evolving, with new answers posted or existing ones edited.
  In this paper, we present an empirical study of SO Java answer edits and their application to improving code in open-source projects. We use a modified code clone search tool to analyze SO code snippets with version history and apply it to open-source Java projects. This identifies outdated or unoptimized code and suggests improved alternatives. Analyzing 140,840 Java accepted answers from SOTorrent and 10,668 GitHub Java projects, we manually categorized SO answer edits and created pull requests to open-source projects with the suggested code improvements. Our results show that 6.91% of SO Java accepted answers have more than one revision (average of 2.82). Moreover, 49.24% of the code snippets in the answer edits are applicable to open-source projects, and 11 out of 36 proposed bug fixes based on these edits were accepted by the GitHub project maintainers.

</details>


### [5] [WAR-Re: Web API Recommendation with Semantic Reasoning](https://arxiv.org/abs/2511.05820)
*Zishuo Xu,Dezhong Yao,Yao Wan*

Main category: cs.SE

TL;DR: 本文提出WAR-Re模型，基于大语言模型实现可变数量的Web API推荐，并通过语义推理生成推荐理由，在准确率和解释性上均优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有Web API推荐方法存在两个关键问题：一是固定数量的Top-N推荐无法适应不同mashup对API数量的差异化需求；二是仅提供排序列表而缺乏推荐理由，影响用户理解与信任。

Method: 提出WAR-Re模型，采用特殊起止标记处理可变推荐数量，并结合两阶段训练策略——监督微调与基于Group Relative Policy Optimization（GRPO）的强化学习，以同时提升推荐准确性与语义解释能力。

Result: 在ProgrammableWeb数据集上的实验表明，WAR-Re相比当前最优基线模型在推荐准确率上最高提升21.59%，并能持续生成高质量的语义推荐理由。

Conclusion: WAR-Re有效解决了Web API推荐中数量灵活性与可解释性不足的问题，为实际应用提供了更可靠、透明的推荐方案。

Abstract: With the development of cloud computing, the number of Web APIs has increased dramatically, further intensifying the demand for efficient Web API recommendation. Despite the demonstrated success of previous Web API recommendation solutions, two critical challenges persist: 1) a fixed top-N recommendation that cannot accommodate the varying API cardinality requirements of different mashups, and 2) these methods output only ranked API lists without accompanying reasons, depriving users of understanding the recommendation. To address these challenges, we propose WAR-Re, an LLM-based model for Web API recommendation with semantic reasoning for justification. WAR-Re leverages special start and stop tokens to handle the first challenge and uses two-stage training: supervised fine-tuning and reinforcement learning via Group Relative Policy Optimization (GRPO) to enhance the model's ability in both tasks. Comprehensive experimental evaluations on the ProgrammableWeb dataset demonstrate that WAR-Re achieves a gain of up to 21.59\% over the state-of-the-art baseline model in recommendation accuracy, while consistently producing high-quality semantic reasons for recommendations.

</details>


### [6] [PyGress: Tool for Analyzing the Progression of Code Proficiency in Python OSS Projects](https://arxiv.org/abs/2511.05821)
*Rujiphart Charatvaraphan,Bunradar Chatchaiyadech,Thitirat Sukijprasert,Chaiyong Ragkhitwetsagul,Morakot Choetkiertikul,Raula Gaikovina Kula,Thanwadee Sunetnanta,Kenichi Matsumoto*

Main category: cs.SE

TL;DR: PyGress 是一个基于网页的工具，用于自动评估和可视化 Python 开源项目中开发者的代码熟练度，依据 CEFR 等级（A1–C2）分析提交历史并生成个人与项目整体的熟练度可视化。


<details>
  <summary>Details</summary>
Motivation: 评估开源软件项目中开发者的编程熟练度对于理解项目动态和专家分布至关重要。

Method: 通过用户提交的 GitHub 仓库链接，PyGress 提取提交历史，利用 pycefr 工具对源代码进行 CEFR 等级分析，并生成可视化结果。

Result: 该工具能够展示每位贡献者的熟练度分布，并追踪项目整体代码熟练度随时间的变化。

Conclusion: PyGress 为探索 Python 开源项目中贡献者的编码水平提供了一种交互式手段，有助于更好地理解项目的人才结构与发展轨迹。

Abstract: Assessing developer proficiency in open-source software (OSS) projects is essential for understanding project dynamics, especially for expertise. This paper presents PyGress, a web-based tool designed to automatically evaluate and visualize Python code proficiency using pycefr, a Python code proficiency analyzer. By submitting a GitHub repository link, the system extracts commit histories, analyzes source code proficiency across CEFR-aligned levels (A1 to C2), and generates visual summaries of individual and project-wide proficiency. The PyGress tool visualizes per-contributor proficiency distribution and tracks project code proficiency progression over time. PyGress offers an interactive way to explore contributor coding levels in Python OSS repositories. The video demonstration of the PyGress tool can be found at https://youtu.be/hxoeK-ggcWk, and the source code of the tool is publicly available at https://github.com/MUICT-SERU/PyGress.

</details>


### [7] [The Impact of COVID-19 and Remote Work on Software Development in Thailand](https://arxiv.org/abs/2511.05824)
*Chaiyong Ragkhitwetsagul,Morakot Choetkiertikul,Srisupa Palakvangsa-Na-Ayudhya,Thanwadee Sunetnanta,Nattanee Satchanawakul*

Main category: cs.SE

TL;DR: 本文通过对194名泰国软件开发者的调查，研究了新冠疫情对泰国远程软件开发的影响，发现远程工作对开发者的生产力和幸福感无显著影响，同时揭示了其带来的挑战与益处，并为其他亚洲及中低收入国家提供参考。


<details>
  <summary>Details</summary>
Motivation: 现有研究缺乏针对泰国这一亚洲新兴软件市场在新冠疫情期间远程工作的专门探讨，因此本文旨在填补这一空白。

Method: 通过问卷调查194名泰国软件开发者，收集他们在疫情期间远程工作所面临的挑战与获得的益处，并分析其生产力与幸福感的变化。

Result: 研究发现，泰国软件开发者在远程工作前后，其生产力和幸福感没有统计学上的显著变化；远程工作既带来益处也带来挑战，与其他研究结果相似但存在一些独特差异。

Conclusion: 该研究为理解新冠疫情下泰国软件开发的远程工作影响提供了实证依据，并可为其他类似亚洲国家或中低收入国家提供借鉴。

Abstract: The COVID-19 pandemic impacted the way of working, including software development. During the pandemic, software companies were forced to work remotely, and many companies have been using such work arrangements. There are prior studies showing the benefits and drawbacks of remote work in software development during COVID-19. However, there is no study that targets Thailand, one of the growing software markets in Asia, specifically. This paper performs an empirical study of the effects of COVID-19 on software development in Thailand. We surveyed 194 Thai software developers regarding the challenges and benefits they faced while working remotely during the COVID-19 period. The results show no statistically significant changes in the productivity and well-being of Thai software developers before and after working remotely due to the pandemic. The results show that software developers in Thailand both received benefits and faced challenges from remote work during COVID-19, similar to results reported by other studies, but with some unique differences. This study can be beneficial to similar Asian countries or other low- and middle-income countries around the world.

</details>


### [8] [Design and Implementation of Data Acquisition and Analysis System for Programming Debugging Process Based On VS Code Plug-In](https://arxiv.org/abs/2511.05825)
*Boyang Liu*

Main category: cs.SE

TL;DR: 本文设计并实现了一个基于VS Code插件的编程调试过程数据采集与分析系统，通过抽象语法树等技术对学生的调试行为进行智能识别与分析，支持多语言、多文件、多任务的复杂调试场景，验证了其在编程教学中对学生调试能力评估的有效性。


<details>
  <summary>Details</summary>
Motivation: 传统评估方法难以全面评价学生的编程调试能力，因此需要一种能够实时采集和分析学生调试行为的系统，以提供更精准的教学反馈。

Method: 开发基于VS Code插件的数据采集系统，结合抽象语法树（AST）、节点标注、序列识别和聚类分析等技术构建调试行为分析模型，实现对学生调试过程的上下文追踪与关键特征识别。

Result: 系统在实际教学测试中表现出良好的可行性与稳定性，能有效支持编程调试教学中的过程性评价，并提升了调试数据分析的精细化水平。

Conclusion: 该系统为编程调试能力的评估提供了新方法，推动了调试行为分析研究的发展，具有良好的教学应用前景。

Abstract: In order to meet the needs of students' programming debugging ability training, this paper designs and implements a data acquisition and analysis system for programming debugging process based on VS Code plug-in, which aims to solve the limitation of traditional assessment methods that are difficult to fully evaluate students' debugging ability. The system supports a variety of programming languages, integrates debugging tasks and data acquisition functions, captures students' debugging behavior in the local editor in real time, and uploads the data to the platform database to realize the whole process monitoring and feedback, provides accurate debugging guidance for teachers, and improves the teaching effect. In terms of data analysis, the system proposed a debugging behavior analysis model based on abstract syntax tree, combined with node annotation, sequence recognition and cluster analysis and other technologies, to automatically track the context of students' debugging process and accurately identify key features in the debugging path. Through this tool, the system realizes the intelligent identification and labeling of the debugging direction and behavior pattern, and improves the refinement level of debugging data analysis. In this research system, a complex debugging scenario of multi-file and multi-task is introduced into the debugging problem design, which optimizes the multi-dimensional capturing ability of debugging data and lays a foundation for accurate debugging behavior analysis. Through several practical teaching tests, the feasibility and stability of the system are verified, which proves that it can effectively support procedural evaluation in programming debugging teaching, and provides a new direction for debugging behavior analysis research.

</details>


### [9] [The Lifecycle Workbench -- A Configurable Framework for Digitized Product Maintenance Services](https://arxiv.org/abs/2511.06149)
*Dominique Briechle,Mohammed Fahad Ali,Marit Briechle-Mathiszig,Tobias Geger,Robert Werner,Andreas Rausch*

Main category: cs.SE

TL;DR: 本文针对循环经济中服务定价不可靠和产品状态难以评估的问题，提出了一个名为“生命周期工作台（LCW）”的生态系统，通过数字化手段提升服务定价的可靠性与产品状况评估的准确性。


<details>
  <summary>Details</summary>
Motivation: 当前电子产品生产带来的环境与健康问题日益严重，亟需向循环经济转型。然而，用户因服务成本不确定而不愿使用可持续服务，服务提供方也因无法提前判断产品状态而面临经济风险。

Method: 作者提出“生命周期工作台（LCW）”生态系统，利用数字表征技术来增强循环经济中服务定价的可靠性，并改进对物品、组件和零件状态的评估。

Result: 该系统有望解决当前循环经济服务执行中的两大弱点：客户对服务价格的不确定性以及服务提供方对产品状态判断的困难。

Conclusion: LCW生态系统通过数字化手段提高了循环经济中服务的可靠性和可行性，有助于推动可持续服务的广泛应用和工业流程的绿色转型。

Abstract: The global production of electric goods is at an all-time high, causing negative environmental and health impacts as well as a continuing depletion of natural resources. Considering the worsening global climate change, a transition of current industrial processes is necessary to tackle the above-mentioned factors. To address this urgent issue, socio-economic systems like the Circular Economy (CE) provide options to reallocate the use of resources and products on a global scale. Especially in terms of product lifecycle-prolonging, this system provides suitable approaches to alter the current modes of product handling by society and industry alike, based on the condition of the products. Although the importance and benefits of sustainable services enabling these options are widely known, users tend to shy away from using them. One of the reasons is the missing reliability in terms of the knowledge of the costs associated with a particular service. This uncertainty in expected pricing can, therefore, lower the willingness of potential clients. However, not only clients struggle with the boundary conditions of such services. On the part of the potential providers of services, the monetary risk is often caused by the incapability to detect the condition of a product in advance. This can result on the provider side in a severe economic loss if this possibility is not covered by the service price or through the mass of items, which could allow equalization of serval service operations. To address these weak points in current service execution, the authors propose the \textit{Lifecycle Workbench (LCW)}-ecosystem, which features digital representations to enhance the reliability of service pricing as well as the assessment of the condition of items, assemblies, and parts in the Circular Economy domain.

</details>


### [10] [ZeroLog: Zero-Label Generalizable Cross-System Log-based Anomaly Detection](https://arxiv.org/abs/2511.05862)
*Xinlong Zhao,Tong Jia,Minghua He,Ying Li,Gang Huang*

Main category: cs.SE

TL;DR: 本文提出ZeroLog，一种无需目标系统标签的跨系统日志异常检测方法，通过无监督域自适应与元学习实现系统无关的特征表示，在零标签条件下达到与使用标签的先进方法相当的性能。


<details>
  <summary>Details</summary>
Motivation: 现有跨系统日志异常检测方法仍需少量目标系统标注日志，而实际中目标系统往往缺乏任何标签。因此，作者探索零标签条件下的跨系统日志异常检测这一更具挑战性且未被充分研究的设定。

Method: 提出ZeroLog方法，结合无监督域自适应（通过源域与目标域之间的对抗训练）和元学习，以学习系统无关的通用特征表示，并在无目标标签的情况下泛化到目标系统。

Result: 在三个不同系统的公开日志数据集上实验表明，ZeroLog在无标签条件下F1分数超过80%，性能媲美使用标签训练的最先进跨系统方法，并显著优于现有零标签方法。

Conclusion: ZeroLog有效解决了零标签跨系统日志异常检测问题，证明了系统无关表示与元学习结合在该任务中的潜力，为实际部署提供了可行方案。

Abstract: Log-based anomaly detection is an important task in ensuring the stability and reliability of software systems. One of the key problems in this task is the lack of labeled logs. Existing works usually leverage large-scale labeled logs from mature systems to train an anomaly detection model of a target system based on the idea of transfer learning. However, these works still require a certain number of labeled logs from the target system. In this paper, we take a step forward and study a valuable yet underexplored setting: zero-label cross-system log-based anomaly detection, that is, no labeled logs are available in the target system. Specifically, we propose ZeroLog, a system-agnostic representation meta-learning method that enables cross-system log-based anomaly detection under zero-label conditions. To achieve this, we leverage unsupervised domain adaptation to perform adversarial training between the source and target domains, aiming to learn system-agnostic general feature representations. By employing meta-learning, the learned representations are further generalized to the target system without any target labels. Experimental results on three public log datasets from different systems show that ZeroLog reaches over 80% F1-score without labels, comparable to state-of-the-art cross-system methods trained with labeled logs, and outperforms existing methods under zero-label conditions.

</details>


### [11] [Generality Is Not Enough: Zero-Label Cross-System Log-Based Anomaly Detection via Knowledge-Level Collaboration](https://arxiv.org/abs/2511.05882)
*Xinlong Zhao,Tong Jia,Minghua He,Ying Li*

Main category: cs.SE

TL;DR: 本文提出GeneralLog，一种在无目标标签场景下结合大语言模型（LLM）与小模型的协作方法，通过动态路由将“通用日志”交由小模型处理、“专有日志”交由LLM处理，实现跨系统的零样本日志异常检测，在三个公开数据集上F1分数超过90%。


<details>
  <summary>Details</summary>
Motivation: 现有跨系统日志异常检测方法在零标签设置下面临挑战：小模型忽略目标系统专有知识，而大语言模型依赖少量正样本且推理成本高；当前协作策略未考虑知识分离，无法有效应用于无监督迁移场景。

Method: 提出GeneralLog框架，在无目标标签条件下动态分配日志：利用小模型处理通用模式日志，大语言模型处理包含系统专有知识的日志，实现知识解耦与高效协作。

Result: 在三个公开日志数据集上的实验表明，GeneralLog在完全无标签设置下F1分数超过90%，显著优于现有方法。

Conclusion: GeneralLog通过合理的知识分工与动态路由机制，有效解决了零标签跨系统日志异常检测中的知识迁移与效率问题，为实际部署提供了可行方案。

Abstract: Log-based anomaly detection is crucial for ensuring software system stability. However, the scarcity of labeled logs limits rapid deployment to new systems. Cross-system transfer has become an important research direction. State-of-the-art approaches perform well with a few labeled target logs, but limitations remain: small-model methods transfer general knowledge but overlook mismatches with the target system's proprietary knowledge; LLM-based methods can capture proprietary patterns but rely on a few positive examples and incur high inference cost. Existing LLM-small model collaborations route 'simple logs' to the small model and 'complex logs' to the LLM based on output uncertainty. In zero-label cross-system settings, supervised sample complexity is unavailable, and such routing does not consider knowledge separation. To address this, we propose GeneralLog, a novel LLM-small model collaborative method for zero-label cross-system log anomaly detection. GeneralLog dynamically routes unlabeled logs, letting the LLM handle 'proprietary logs' and the small model 'general logs,' enabling cross-system generalization without labeled target logs. Experiments on three public log datasets show that GeneralLog achieves over 90% F1-score under a fully zero-label setting, significantly outperforming existing methods.

</details>


### [12] [Bridging the Prototype-Production Gap: A Multi-Agent System for Notebooks Transformation](https://arxiv.org/abs/2511.07257)
*Hanya Elhashemy,Youssef Lotfy,Yongjian Tang*

Main category: cs.SE

TL;DR: 本文提出Codelevate，一个基于多智能体的系统，能自动将Jupyter Notebook转换为结构良好、可维护的Python代码库，有效弥合从原型到生产的差距。


<details>
  <summary>Details</summary>
Motivation: Jupyter Notebook在数据科学和机器学习中广泛使用，但其缺乏软件工程规范，难以直接用于生产环境，因此需要一种自动化方法将其转化为高质量、可维护的代码。

Method: 设计了一个包含Architect、Developer和Structure三个专用智能体的多智能体系统，通过共享依赖树协同工作，实现Notebook到结构化Python项目的自动转换。

Result: 实验表明，Codelevate能在保持计算语义不变的前提下，显著提升代码质量指标，成功实现从原型到生产代码的自动化转化。

Conclusion: Codelevate有效解决了Jupyter Notebook向生产级代码迁移的难题，为数据科学工作流的工程化提供了可行路径。

Abstract: The increasing adoption of Jupyter notebooks in data science and machine learning workflows has created a gap between exploratory code development and production-ready software systems. While notebooks excel at iterative development and visualization, they often lack proper software engineering principles, making their transition to production environments challenging. This paper presents Codelevate, a novel multi-agent system that automatically transforms Jupyter notebooks into well-structured, maintainable Python code repositories. Our system employs three specialized agents - Architect, Developer, and Structure - working in concert through a shared dependency tree to ensure architectural coherence and code quality. Our experimental results validate Codelevate's capability to bridge the prototype-to-production gap through autonomous code transformation, yielding quantifiable improvements in code quality metrics while preserving computational semantics.

</details>


### [13] [Quality in model-driven engineering: a tertiary study](https://arxiv.org/abs/2511.06103)
*Miguel Goulão,Vasco Amaral,Marjan Mernik*

Main category: cs.SE

TL;DR: 本文通过三级研究综述了模型驱动工程（MDE）对软件质量影响的现有系统性文献，发现可维护性是最常被研究的质量属性，但多数研究侧重于映射现有成果而非回答具体问题，且缺乏对“使用质量”的关注和更多实证支持。


<details>
  <summary>Details</summary>
Motivation: 研究人员和从业者难以获取关于MDE对软件质量影响的集中证据，因为相关研究分散在大量文献中。作者旨在整合已有研究成果，帮助识别研究覆盖范围、主要发现及尚待探索的研究空白。

Method: 开展了一项关于MDE中质量影响的三级研究，筛选并分析了22篇系统性文献综述和映射研究，提取其中涉及的主要质量属性及其研究问题结构，并总结各综述的核心贡献。

Result: 可维护性是MDE中最常被报告的质量属性；83个研究问题中有80个偏向于研究映射而非具体比较；现有研究虽广泛覆盖产品质量，但普遍呼吁更多实证验证；对“使用质量”的关注较少。

Conclusion: 尽管MDE在软件产品质量方面已有较广研究覆盖，尤其是可维护性，但仍需更多实证工作来验证现有主张，并加强对使用质量等较少关注领域的研究。

Abstract: Model-driven engineering (MDE) is believed to have a significant impact in software quality. However, researchers and practitioners may have a hard time locating consolidated evidence on this impact, as the available information is scattered in several different publications. Our goal is to aggregate consolidated findings on quality in MDE, facilitating the work of researchers and practitioners in learning about the coverage and main findings of existing work as well as identifying relatively unexplored niches of research that need further attention. We performed a tertiary study on quality in MDE, in order to gain a better understanding of its most prominent findings and existing challenges, as reported in the literature. We identified 22 systematic literature reviews and mapping studies and the most relevant quality attributes addressed by each of those studies, in the context of MDE. Maintainability is clearly the most often studied and reported quality attribute impacted by MDE. Eighty out of 83 research questions in the selected secondary studies have a structure that is more often associated with mapping existing research than with answering more concrete research questions (e.g., comparing two alternative MDE approaches with respect to their impact on a specific quality attribute). We briefly outline the main contributions of each of the selected literature reviews. In the collected studies, we observed a broad coverage of software product quality, although frequently accompanied by notes on how much more empirical research is needed to further validate existing claims. Relatively, little attention seems to be devoted to the impact of MDE on the quality in use of products developed using MDE.

</details>


### [14] [On the impact of semantic transparency on understanding and reviewing social goal models](https://arxiv.org/abs/2511.06110)
*Mafalda Santos,Catarina Gralha,Miguel Goulão,João Araújo,Ana Moreira*

Main category: cs.SE

TL;DR: 本研究评估了语义透明度对理解与评审i*模型的影响，发现尽管替代性具体语法未显著提升准确率或速度，但显著降低了视觉负担。


<details>
  <summary>Details</summary>
Motivation: i*语言因其复杂性和工业界低采用率，成为改进其具体语法和提升利益相关者正确解读模型能力的研究对象。

Method: 通过准实验比较标准i*语法与语义透明度更高的替代语法，57名新手参与者完成理解与评审任务，使用眼动追踪和反馈收集任务成功率、时间和努力程度等指标。

Result: 替代语法未显著提高准确率或速度，感知易用性相似，但使用该语法时参与者在模型和语言键上的视觉努力显著减少。

Conclusion: 模型和语言键提供的上下文可能缓解先前研究中报告的i*符号识别缺陷，且替代语法显著降低了视觉努力。

Abstract: Context: i* is one of the most influential languages in the Requirements Engineering research community. Perhaps due to its complexity and low adoption in industry, it became a natural candidate for studies aiming at improving its concrete syntax and the stakeholders' ability to correctly interpret i* models.
  Objectives: We evaluate the impact of semantic transparency on understanding and reviewing i* models, in the presence of a language key. Methods: We performed a quasi-experiment comparing the standard i* concrete syntax with an alternative that has an increased semantic transparency. We asked 57 novice participants to perform understanding and reviewing tasks on i* models, and measured their accuracy, speed and ease, using metrics of task success, time and effort, collected with eye-tracking and participants' feedback.
  Results: We found no evidence of improved accuracy or speed attributable to the alternative concrete syntax. Although participants' perceived ease was similar, they devoted significantly less visual effort to the model and the provided language key, when using the alternative concrete syntax.
  Conclusions: The context provided by the model and language key may mitigate the i* symbol recognition deficit reported in previous works. However, the alternative concrete syntax required a significantly lower visual effort.

</details>


### [15] [Diagnosing and Resolving Android Applications Building Issues: An Empirical Study](https://arxiv.org/abs/2511.06186)
*Lakshmi Priya Bodepudi,Yutong Zhao,Ming Quan Fu,Yuanyuan Wu,Sen He,Yu Zhao*

Main category: cs.SE

TL;DR: 该研究对200个开源Android项目进行实证分析，识别出四类主要构建错误，并通过诊断与修复策略成功解决75.56%的失败案例；同时评估了大语言模型（如GPT-5）在错误诊断中的潜力，成功率达53.3%。


<details>
  <summary>Details</summary>
Motivation: 由于复杂的依赖关系、多样的配置以及Android生态系统的快速演进，可靠地构建Android应用仍是一项持续挑战，亟需系统性方法诊断和修复构建失败问题。

Method: 采用五阶段流程：数据收集、构建执行、失败分类、修复策略设计和大语言模型辅助评估，对200个Java/Kotlin编写的开源Android项目进行实证分析。

Result: 在135个初始构建失败的项目中，修复策略成功解决了102个（75.56%）；LLM辅助诊断的成功率为53.3%；项目属性分析表明构建成功率受编程语言、项目年龄和应用规模影响。

Conclusion: 研究为提升Android构建可靠性提供了实用见解，并展示了AI（特别是大语言模型）在软件维护中的辅助潜力。

Abstract: Building Android applications reliably remains a persistent challenge due to complex dependencies, diverse configurations, and the rapid evolution of the Android ecosystem. This study conducts an empirical analysis of 200 open-source Android projects written in Java and Kotlin to diagnose and resolve build failures. Through a five-phase process encompassing data collection, build execution, failure classification, repair strategy design, and LLM-assisted evaluation, we identified four primary types of build errors: environment issues, dependency and Gradle task errors, configuration problems, and syntax/API incompatibilities. Among the 135 projects that initially failed to build, our diagnostic and repair strategy enabled developers to resolve 102 cases (75.56%), significantly reducing troubleshooting effort. We further examined the potential of Large Language Models, such as GPT-5, to assist in error diagnosis, achieving a 53.3% success rate in suggesting viable fixes. An analysis of project attributes revealed that build success is influenced by programming language, project age, and app size. These findings provide practical insights into improving Android build reliability and advancing AI-assisted software maintenance.

</details>


### [16] [Assertion-Aware Test Code Summarization with Large Language Models](https://arxiv.org/abs/2511.06227)
*Anamul Haque Mollah,Ahmed Aljohani,Hyunsook Do*

Main category: cs.SE

TL;DR: 本文提出一个包含91个真实Java测试用例及其开发者撰写摘要的新基准，并通过消融实验研究了不同提示策略（如被测方法、断言语义等）对大语言模型生成测试摘要效果的影响。结果表明，仅使用断言语义即可在减少输入长度的同时提升摘要质量，其中Codex和Qwen-Coder表现最佳。


<details>
  <summary>Details</summary>
Motivation: 单元测试通常缺乏简洁明了的摘要来表达测试意图，尤其在自动生成或文档不足的代码库中更为明显。尽管大语言模型（LLMs）在代码摘要方面展现出潜力，但其在测试代码摘要任务中的有效性高度依赖于提示方式，而该任务因测试方法通过断言验证行为而非实现功能，具有独特挑战。

Method: 构建包含91个真实Java测试用例及对应人工摘要的基准数据集；设计七种提示配置，系统性地引入被测方法（MUT）、断言消息和断言语义等组件；在四个代码大模型（Codex、Codestral、DeepSeek、Qwen-Coder）上进行受控消融实验；采用n-gram指标（BLEU、ROUGE-L、METEOR）、语义相似度（BERTScore）及基于LLM的评估方法综合衡量摘要质量。

Result: 实验结果显示，仅提供断言语义的提示方式比完整MUT上下文平均提升0.10分（2.3%），且所需输入token更少；Codex和Qwen-Coder生成的摘要与人工摘要一致性最高，而DeepSeek尽管词汇重叠度高，整体表现却较差。

Conclusion: 断言语义是提升测试代码摘要质量的关键因素，合理设计提示策略可在降低计算开销的同时提高摘要准确性；不同模型在测试摘要任务中表现差异显著，需结合多种评估维度全面衡量性能。

Abstract: Unit tests often lack concise summaries that convey test intent, especially in auto-generated or poorly documented codebases. Large Language Models (LLMs) offer a promising solution, but their effectiveness depends heavily on how they are prompted. Unlike generic code summarization, test-code summarization poses distinct challenges because test methods validate expected behavior through assertions rather than implementing functionality. This paper presents a new benchmark of 91 real-world Java test cases paired with developer-written summaries and conducts a controlled ablation study to investigate how test code-related components-such as the method under test (MUT), assertion messages, and assertion semantics-affect the performance of LLM-generated test summaries. We evaluate four code LLMs (Codex, Codestral, DeepSeek, and Qwen-Coder) across seven prompt configurations using n-gram metrics (BLEU, ROUGE-L, METEOR), semantic similarity (BERTScore), and LLM-based evaluation. Results show that prompting with assertion semantics improves summary quality by an average of 0.10 points (2.3%) over full MUT context (4.45 vs. 4.35) while requiring fewer input tokens. Codex and Qwen-Coder achieve the highest alignment with human-written summaries, while DeepSeek underperforms despite high lexical overlap. The replication package is publicly available at https://doi.org/10. 5281/zenodo.17067550

</details>


### [17] [WebVIA: A Web-based Vision-Language Agentic Framework for Interactive and Verifiable UI-to-Code Generation](https://arxiv.org/abs/2511.06251)
*Mingde Xu,Zhen Yang,Wenyi Hong,Lihang Pan,Xinyue Fan,Yan Wang,Xiaotao Gu,Bin Xu,Jie Tang*

Main category: cs.SE

TL;DR: WebVIA 是首个面向交互式 UI 生成的智能体框架，通过探索、代码生成与验证三阶段流程，显著提升 UI 到可执行交互代码的生成质量。


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言模型（VLMs）在 UI-to-Code 任务中仅能生成静态布局，缺乏交互性，而人工实现交互功能过程重复且耗时。

Method: 提出 WebVIA 框架，包含：1）用于捕获多状态 UI 截图的探索智能体；2）生成可执行交互代码的 UI2Code 模型；3）验证交互性的模块。

Result: WebVIA-Agent 在 UI 探索上比通用智能体（如 Gemini-2.5-Pro）更稳定准确；微调后的 WebVIA-UI2Code 模型在交互性和静态 UI 生成任务上均优于基线模型。

Conclusion: WebVIA 首次实现了端到端的交互式 UI 自动生成与验证，显著推进了 UI 开发自动化水平。

Abstract: User interface (UI) development requires translating design mockups into functional code, a process that remains repetitive and labor-intensive. While recent Vision-Language Models (VLMs) automate UI-to-Code generation, they generate only static HTML/CSS/JavaScript layouts lacking interactivity. To address this, we propose WebVIA, the first agentic framework for interactive UI-to-Code generation and validation. The framework comprises three components: 1) an exploration agent to capture multi-state UI screenshots; 2) a UI2Code model that generates executable interactive code; 3) a validation module that verifies the interactivity. Experiments demonstrate that WebVIA-Agent achieves more stable and accurate UI exploration than general-purpose agents (e.g., Gemini-2.5-Pro). In addition, our fine-tuned WebVIA-UI2Code models exhibit substantial improvements in generating executable and interactive HTML/CSS/JavaScript code, outperforming their base counterparts across both interactive and static UI2Code benchmarks. Our code and models are available at \href{https://zheny2751-dotcom.github.io/webvia.github.io/}{\texttt{https://webvia.github.io}}.

</details>


### [18] [State of the Art on Self-adaptive Systems: An Essay](https://arxiv.org/abs/2511.06352)
*Sara Mahdavi Hezavehi,Danny Weyns,Paris Avgeriou*

Main category: cs.SE

TL;DR: 本文介绍了博士研究中关于不确定性与风险感知适应的基础概念及相关研究。


<details>
  <summary>Details</summary>
Motivation: 为开展关于不确定性与风险感知适应的博士研究奠定理论基础。

Method: 综述并讨论相关领域的基础概念和已有研究。

Result: 梳理了不确定性与风险感知适应领域中的关键概念和相关工作。

Conclusion: 该文为后续深入研究不确定性与风险感知适应提供了理论铺垫。

Abstract: In this essay, we introduce the basic concepts necessary to lay out the foundation for our PhD research on uncertainty and risk-aware adaptation, and discuss relevant related research.

</details>


### [19] [Understanding Student Interaction with AI-Powered Next-Step Hints: Strategies and Challenges](https://arxiv.org/abs/2511.06362)
*Anastasiia Birillo,Aleksei Rostovskii,Yaroslav Golubev,Hieke Keuning*

Main category: cs.SE

TL;DR: 本研究通过分析34名学生在IDE中使用AI驱动的下一步提示系统的交互日志，结合6名学生的访谈，识别出16种常见交互场景，并揭示了学生应对无效提示的策略，为改进编程教育中的提示设计提供了依据。


<details>
  <summary>Details</summary>
Motivation: 在计算机科学教育中，自动化反馈（尤其是下一步提示）对提升个性化学习体验至关重要，但尚缺乏对学生如何与这类系统互动的深入理解。

Method: 收集34名学生解决Kotlin任务时的提示交互日志，应用过程挖掘技术识别常见交互模式，并对6名学生进行半结构化访谈以了解其应对策略。

Result: 识别出16种常见的提示交互场景；学生会采用如调整部分提示或修改代码以生成相同提示变体等策略来应对无帮助的提示。

Conclusion: 研究结果和公开数据集为未来研究提供了基础，并有助于优化AI提示系统的设计，从而更好地支持学生学习。

Abstract: Automated feedback generation plays a crucial role in enhancing personalized learning experiences in computer science education. Among different types of feedback, next-step hint feedback is particularly important, as it provides students with actionable steps to progress towards solving programming tasks. This study investigates how students interact with an AI-driven next-step hint system in an in-IDE learning environment. We gathered and analyzed a dataset from 34 students solving Kotlin tasks, containing detailed hint interaction logs. We applied process mining techniques and identified 16 common interaction scenarios. Semi-structured interviews with 6 students revealed strategies for managing unhelpful hints, such as adapting partial hints or modifying code to generate variations of the same hint. These findings, combined with our publicly available dataset, offer valuable opportunities for future research and provide key insights into student behavior, helping improve hint design for enhanced learning support.

</details>


### [20] [Methodological Considerations for Self-adaptive Systems: An Essay](https://arxiv.org/abs/2511.06367)
*Sara Mahdavi Hezavehi,Danny Weyns,Paris Avgeriou*

Main category: cs.SE

TL;DR: 本文概述了为博士研究奠定基础所需的方法论考量，聚焦于不确定性与风险感知适应。


<details>
  <summary>Details</summary>
Motivation: 为开展关于不确定性与风险感知适应的博士研究，需明确相关方法论基础。

Method: 综述并分析支撑该研究主题的关键方法论要素。

Result: 提出了适用于不确定性与风险感知适应研究的方法论框架。

Conclusion: 确立清晰的方法论基础对推进不确定性与风险感知适应研究至关重要。

Abstract: In this essay, we provide an overview of methodological considerations necessary to lay out the foundation for our PhD research on uncertainty and risk-aware adaptation.

</details>


### [21] [Walking the Tightrope of LLMs for Software Development: A Practitioners' Perspective](https://arxiv.org/abs/2511.06428)
*Samuel Ferino,Rashina Hoda,John Grundy,Christoph Treude*

Main category: cs.SE

TL;DR: 该研究通过22次访谈，采用社会技术扎根理论分析了大语言模型（LLM）对软件开发在个体、团队、组织和社会层面的利弊，并提出了采纳LLM的最佳实践与权衡建议。


<details>
  <summary>Details</summary>
Motivation: 尽管已有研究开始探讨大语言模型（LLM）对软件开发的感知影响，但仍缺乏实证研究来理解如何平衡其正向与负向效应。

Method: 在2024年10月至2025年9月期间，研究者进行了三轮数据收集与分析，共对22名软件从业者进行访谈，并采用社会技术扎根理论（STGT）对访谈数据进行严谨分析。

Result: 研究识别出使用LLM在个体、团队、组织和社会层面的益处（如维持开发流程、提升开发者心智模型、促进创业）和弊端（如对开发者个性的负面影响及声誉损害），并总结了采纳LLM的最佳实践。

Conclusion: 研究揭示了软件从业者、团队和组织在使用LLM时面临的权衡，其发现对软件团队负责人和IT管理者评估LLM在其特定情境下的可行性具有重要参考价值。

Abstract: Background: Large Language Models emerged with the potential of provoking a revolution in software development (e.g., automating processes, workforce transformation). Although studies have started to investigate the perceived impact of LLMs for software development, there is a need for empirical studies to comprehend how to balance forward and backward effects of using LLMs. Objective: We investigated how LLMs impact software development and how to manage the impact from a software developer's perspective. Method: We conducted 22 interviews with software practitioners across 3 rounds of data collection and analysis, between October (2024) and September (2025). We employed socio-technical grounded theory (STGT) for data analysis to rigorously analyse interview participants' responses. Results: We identified the benefits (e.g., maintain software development flow, improve developers' mental model, and foster entrepreneurship) and disadvantages (e.g., negative impact on developers' personality and damage to developers' reputation) of using LLMs at individual, team, organisation, and society levels; as well as best practices on how to adopt LLMs. Conclusion: Critically, we present the trade-offs that software practitioners, teams, and organisations face in working with LLMs. Our findings are particularly useful for software team leaders and IT managers to assess the viability of LLMs within their specific context.

</details>


### [22] [Automatically Identifying Solution-Related Content in Issue Report Discussions with Language Models](https://arxiv.org/abs/2511.06501)
*Antu Saha,Mehedi Sun,Oscar Chaparro*

Main category: cs.SE

TL;DR: 本文提出使用语言模型自动识别软件问题报告中的解决方案内容，比较了嵌入、提示和微调三种方法在传统机器学习模型（MLMs）、预训练语言模型（PLMs）和大语言模型（LLMs）上的表现。实验基于356个Mozilla Firefox问题构建数据集，结果表明微调后的LLM（如LLAMAft）效果最佳（F1达0.716），集成模型进一步提升至0.737；模型具备跨项目迁移能力，少量目标项目数据即可提升性能。


<details>
  <summary>Details</summary>
Motivation: 在软件开发中，开发者依赖问题报告讨论缺陷修复、功能请求等变更方案，这些讨论包含大量解决方案及其评估信息。手动从冗长讨论中定位解决方案内容费时费力，因此亟需自动化方法以支持问题重开调查、回归分析、方案复用及代码变更理解。

Method: 本文将解决方案识别任务建模为监督分类问题，系统评估三种应用方式（嵌入、提示、微调）在三类模型（MLMs、PLMs、LLMs）上的效果。基于356个Mozilla Firefox问题构建标注数据集，训练并评估6种MLMs、4种PLMs和2种LLMs共68种配置，并测试模型跨项目迁移能力。

Result: 实验结果显示：MLMs结合LLM嵌入优于TF-IDF特征；提示方法表现不佳；微调LLM效果最优，其中LLAMAft达到0.716 F1分数；最佳模型集成后F1提升至0.737；错误分类主要源于误导性线索或上下文缺失；模型在少量目标项目数据下可有效迁移到其他项目。

Conclusion: 微调大语言模型是自动识别问题报告中解决方案内容的有效方法，具备良好性能与跨项目适应性。该工作有助于提升软件维护效率、问题理解深度和解决方案复用能力，未来需进一步增强模型对上下文的感知能力。

Abstract: During issue resolution, software developers rely on issue reports to discuss solutions for defects, feature requests, and other changes. These discussions contain proposed solutions-from design changes to code implementations-as well as their evaluations. Locating solution-related content is essential for investigating reopened issues, addressing regressions, reusing solutions, and understanding code change rationale. Manually understanding long discussions to identify such content can be difficult and time-consuming.
  This paper automates solution identification using language models as supervised classifiers. We investigate three applications-embeddings, prompting, and fine-tuning-across three classifier types: traditional ML models (MLMs), pre-trained language models (PLMs), and large language models (LLMs). Using 356 Mozilla Firefox issues, we created a dataset to train and evaluate six MLMs, four PLMs, and two LLMs across 68 configurations.
  Results show that MLMs with LLM embeddings outperform TF-IDF features, prompting underperforms, and fine-tuned LLMs achieve the highest performance, with LLAMAft reaching 0.716 F1 score. Ensembles of the best models further improve results (0.737 F1). Misclassifications often arise from misleading clues or missing context, highlighting the need for context-aware classifiers. Models trained on Mozilla transfer to other projects, with a small amount of project-specific data, further enhancing results. This work supports software maintenance, issue understanding, and solution reuse.

</details>


### [23] [LLM For Loop Invariant Generation and Fixing: How Far Are We?](https://arxiv.org/abs/2511.06552)
*Mostafijur Rahman Akhond,Saikat Chakraborty,Gias Uddin*

Main category: cs.SE

TL;DR: 本文研究了大语言模型（LLMs）在推断和修复程序循环不变式方面的表现，发现其在生成任务中最高成功率达78%，但在修复任务中仅达16%；辅助信息如领域知识和示例能显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 循环不变式对自动化程序安全性验证至关重要，而目前尚不清楚大语言模型在推断循环不变式方面的实际能力。

Method: 对多种开源与闭源、不同规模的大语言模型进行实证研究，评估其在推断归纳性循环不变式及修复错误不变式方面的表现。

Result: 大语言模型在生成循环不变式方面最高成功率为78%，但在修复任务中仅为16%；引入领域知识和示例等辅助信息可显著提升其性能。

Conclusion: 大语言模型在循环不变式推断与修复中具有一定潜力，但需依赖额外辅助信息才能有效提升准确率，尤其在修复任务中表现仍有限。

Abstract: A loop invariant is a property of a loop that remains true before and after each execution of the loop. The identification of loop invariants is a critical step to support automated program safety assessment. Recent advancements in Large Language Models (LLMs) have demonstrated potential in diverse software engineering (SE) and formal verification tasks. However, we are not aware of the performance of LLMs to infer loop invariants. We report an empirical study of both open-source and closed-source LLMs of varying sizes to assess their proficiency in inferring inductive loop invariants for programs and in fixing incorrect invariants. Our findings reveal that while LLMs exhibit some utility in inferring and repairing loop invariants, their performance is substantially enhanced when supplemented with auxiliary information such as domain knowledge and illustrative examples. LLMs achieve a maximum success rate of 78\% in generating, but are limited to 16\% in repairing the invariant.

</details>


### [24] [PhaseSeed: Precise Call Graph Construction for Split-Phase Applications using Dynamic Seeding](https://arxiv.org/abs/2511.06661)
*Tapti Palit,Seyedhamed Ghavamnia,Michalis Polychronakis*

Main category: cs.SE

TL;DR: PhaseSeed is a novel technique that enhances pointer analysis precision for split-phase applications by dynamically analyzing the initialization phase and seeding runtime points-to information into static analysis, significantly improving security mechanisms like CFI, debloating, and system call filtering.


<details>
  <summary>Details</summary>
Motivation: Traditional static pointer analysis techniques are imprecise because they ignore application architecture and aim for broad applicability, which undermines the effectiveness of security mechanisms relying on accurate call graphs.

Method: PhaseSeed dynamically analyzes the initialization phase of split-phase applications to collect runtime points-to relationships, then seeds this information into a subsequent static pointer analysis focused on code active during the processing phase.

Result: PhaseSeed improves CFI precision by up to 92.6% over traditional static methods and enables Seccomp profiles to filter nine additional security-critical system calls; it is sound under a fixed runtime configuration.

Conclusion: By leveraging phase-specific dynamic information to guide static analysis, PhaseSeed significantly boosts the precision and effectiveness of software security mechanisms dependent on accurate call graphs.

Abstract: Precise and sound call graph construction is crucial for many software security mechanisms. Unfortunately, traditional static pointer analysis techniques used to generate application call graphs suffer from imprecision. These techniques are agnostic to the application's architecture and are designed for broad applicability. To mitigate this precision problem, we propose PhaseSeed, a novel technique that improves the accuracy of pointer analysis for split-phase applications, which have distinct initialization and processing phases. PhaseSeed analyzes the initialization phase dynamically, collecting the points-to relationships established at runtime. At the end of the initialization phase, it then seeds this information to a static analysis stage that performs pointer analysis for all code that stays in scope during the processing phase, improving precision. Our observations show that, given the same runtime configuration options, the points-to relationships established during the initialization phase remain constant across multiple runs. Therefore, PhaseSeed is sound with respect to a given initial configuration. We apply PhaseSeed to three security mechanisms: control flow integrity (CFI), software debloating, and system call filtering. PhaseSeed provides up to 92.6% precision improvement for CFI compared to static call graph construction techniques, and filters nine additional security-critical system calls when used to generate Seccomp profiles.

</details>


### [25] [Structural Enforcement of Statistical Rigor in AI-Driven Discovery: A Functional Architecture](https://arxiv.org/abs/2511.06701)
*Karen Sargsyan*

Main category: cs.SE

TL;DR: 本文提出一种基于函数式编程的架构，利用Haskell中的Research monad和声明式脚手架，以结构化方式保障AI驱动科研系统中的统计严谨性，防止动态假设检验导致的虚假发现。


<details>
  <summary>Details</summary>
Motivation: AI驱动的自动化科研系统（AI-Scientists）在动态假设检验中容易产生虚假发现，缺乏对顺序统计协议（如在线FDR控制）的结构性保障，亟需一种能强制执行统计严谨性的方法。

Method: 设计了一个名为Research monad的Haskell嵌入式领域特定语言（eDSL），通过monad变换器栈强制实施顺序统计协议；同时采用声明式脚手架（Declarative Scaffolding）生成严格约束执行流程的代码框架，防止数据泄露等方法论错误。

Result: 通过大规模模拟（N=2000个假设）和端到端案例研究验证了该方法的有效性，展示了其在保障自动化科研完整性方面的纵深防御能力。

Conclusion: 函数式编程结合声明式脚手架能有效结构化地保障AI科研系统的统计严谨性，是实现可靠自动化科学的重要路径。

Abstract: Sequential statistical protocols require meticulous state management and robust error handling -- challenges naturally suited to functional programming. We present a functional architecture for structural enforcement of statistical rigor in automated research systems (AI-Scientists). These LLM-driven systems risk generating spurious discoveries through dynamic hypothesis testing. We introduce the Research monad, a Haskell eDSL that enforces sequential statistical protocols (e.g., Online FDR (false discovery rate) control) using a monad transformer stack. To address risks in hybrid architectures where LLMs generate imperative code, we employ Declarative Scaffolding -- generating rigid harnesses that structurally constrain execution and prevent methodological errors like data leakage. We validate this approach through large-scale simulation (N=2000 hypotheses) and an end-to-end case study, demonstrating essential defense-in-depth for automated science integrity.

</details>


### [26] [Minimizing Breaking Changes and Redundancy in Mitigating Technical Lag for Java Projects](https://arxiv.org/abs/2511.06762)
*Rui Lu,Lyuye Zhang,Kaixuan Li,Min Zhang,Yixiang Chen*

Main category: cs.SE

TL;DR: 本文提出了DepUpdater，一种自动化依赖更新工具，在最小化技术滞后的同时避免不兼容问题和冗余依赖。


<details>
  <summary>Details</summary>
Motivation: 开发者在升级开源软件库时面临兼容性问题和冗余依赖的挑战，导致不愿及时更新，从而产生技术滞后；因此需要一个能自动平衡升级、兼容性和依赖精简的解决方案。

Method: 提出DepUpdater工具，综合考虑版本升级、兼容性保障与冗余依赖剪枝，通过对比实验和消融研究验证其有效性。

Result: DepUpdater相比现有依赖管理工具能更有效地减少技术滞后、确保兼容性并剪枝冗余依赖；消融研究显示在升级过程中考虑剪枝有助于缓解不兼容问题；此外还探讨了传递依赖升级对客户端兼容性的影响。

Conclusion: DepUpdater为开源依赖自动更新提供了一种有效方法，兼顾技术滞后减少、兼容性维护和依赖精简，为未来相关研究提供了新思路。

Abstract: Re-using open-source software (OSS) can avoid reinventing the wheel, but failing to keep it up-to-date can lead to missing new features and persistent bugs or vulnerabilities that have already been resolved. The use of outdated OSS libraries introduces technical lag, necessitating timely upgrades. However, maintaining up-to-date libraries is challenging, as it may introduce incompatibility issues that break the project or redundant dependencies that unnecessarily increase the size of the project. These issues discourage developers from upgrading libraries, highlighting the need for a fully automated solution that balances version upgrades, reduces technical lag, ensures compatibility, and avoids redundant dependencies.
  To this end, we propose DepUpdater, which ensures that upgrades minimize technical lag as much as possible while avoiding incompatibility issues and redundant dependencies. The comparison with existing dependency management tools demonstrates that DepUpdater more effectively reduces technical lag while ensuring compatibility and pruning redundant dependencies. Additionally, an ablation study highlights the potential benefits of considering pruning requirements during upgrades to mitigate incompatibility issues. Finally, leveraging DepUpdater, we investigate the impact of transitive dependency upgrades on client compatibility, providing insights for future research.

</details>


### [27] [MetricSynth: Framework for Aggregating DORA and KPI Metrics Across Multi-Platform Engineering](https://arxiv.org/abs/2511.06864)
*Pallav Jain,Yuvraj Agrawal,Ashutosh Nigam,Pushpak Patil*

Main category: cs.SE

TL;DR: 本文提出并实现了一个集中式框架，用于整合多源开发数据，实时可视化开发者体验（DevEx）和关键绩效指标（KPI），显著减少人工报告工作量并提升决策效率。


<details>
  <summary>Details</summary>
Motivation: 现代大型软件开发中，数据分散在多个工具中，导致团队绩效与系统健康状况难以获得统一、实时、数据驱动的视图，人工生成报告耗时且易出错。

Method: 构建一个包含定时数据摄取层、双模式数据存储、指标预计算引擎、主动告警机制和基于Metabase的可视化界面的集中式架构，并通过基于角色的访问控制保障安全。

Result: 系统部署后每周节省约20人时的手动报告工作，并支持更快地识别开发瓶颈；同时评估了系统的可扩展性及其设计权衡。

Conclusion: 该框架有效提升了工程智能平台的数据整合与洞察能力，为开发者体验和效能度量提供了实用、可扩展的解决方案。

Abstract: In modern, large-scale software development, engineering leaders face the significant challenge of gaining a holistic and data-driven view of team performance and system health. Data is often siloed across numerous disparate tools, making manual report generation time-consuming and prone to inconsistencies. This paper presents the architecture and implementation of a centralized framework designed to provide near-real-time visibility into developer experience (DevEx) and Key Performance Indicator (KPI) metrics for a software ecosystem. By aggregating data from various internal tools and platforms, the system computes and visualizes metrics across key areas such as Developer Productivity, Quality, and Operational Efficiency. The architecture features a cron-based data ingestion layer, a dual-schema data storage approach, a processing engine for metric pre-computation, a proactive alerting system, and utilizes the open-source BI tool Metabase for visualization, all secured with role-based access control (RBAC). The implementation resulted in a significant reduction in manual reporting efforts, saving an estimated 20 person-hours per week, and enabled faster, data-driven bottleneck identification. Finally, we evaluate the system's scalability and discuss its trade-offs, positioning it as a valuable contribution to engineering intelligence platforms.

</details>


### [28] [A Collaborative Model for Improving Information Sharing among Cancer Care Groups using Software Engineering Principles](https://arxiv.org/abs/2511.06885)
*Davis Byamugisha,Francis Kamuganga,Adones Rukundo,John Businge*

Main category: cs.SE

TL;DR: 本文借鉴软件工程中的版本控制理念（特别是GitHub的bug修复机制），提出一种用于癌症诊疗团队间信息共享与协作的新模型，以减少诊疗延迟、提升早期诊断率和患者生存率。


<details>
  <summary>Details</summary>
Motivation: 当前癌症诊疗过程中存在因医疗人力不足及信息系统不整合而导致的信息断层，现有方法未能充分纳入所有关键利益相关者（如患者照护者和管理人员），造成诊疗延误与沟通障碍。

Method: 作者分析癌症治疗与软件工程在信息管理上的相似性，基于GitHub版本控制系统中的bug修复原则设计信息共享模型，并利用AnyLogic仿真软件在虚拟环境中验证模型效果。

Result: 仿真结果表明，采用软件工程中的bug修复与版本控制机制可有效促进癌症诊疗团队之间的协作与信息共享，涵盖所有利益相关者，从而改善治疗结果、实现早期诊断并提高患者生存几率。

Conclusion: 将软件工程中的信息管理原则应用于癌症诊疗流程，有助于构建高效、协同、全员参与的信息共享机制，显著提升癌症诊疗效率与患者预后。

Abstract: Effective treatment of cancer requires early diagnosis which involves the patient's awareness of the early signs and symptoms, leading to a consultation with a health provider, who would then promptly refer the patient for confirmation of the diagnosis and thereafter treatment. However, this is not always the case because of delays arising from limited skilled manpower and health information management systems that are neither integrated nor organized in their design hence leading to information gap among care groups. Existing methods focus on using accumulated data to support decision making, enhancing the sharing of secondary data while others exclude some critical stakeholders like patient caretakers and administrators thus, leaving an information gap that creates delays and miscommunication during case management. We however notice some similarities between cancer treatment and software engineering information management especially when progress history needs to be maintained (versioning).
  We analyze the similarities and propose a model for information sharing among cancer care groups using the software engineering principles approach. We model for reducing delays and improving coordination among care groups in cancer case management. Model design was guided by software engineering principles adopted in GitHub version control system for bug fixing in open-source code projects. Any-Logic simulation software was used to mimic the model realism in a virtual environment. Results show that bug resolution principles from software engineering and GitHub version control system can be adopted to coordinate collaboration and information sharing among care groups in a cancer case management environment while involving all stakeholders to improve care treatment outcomes, ensure early diagnosis and increase patient's survival chances.

</details>


### [29] [Benchmarking LLMs for Fine-Grained Code Review with Enriched Context in Practice](https://arxiv.org/abs/2511.07017)
*Ruida Hu,Xinchen Wang,Xin-Cheng Wen,Zhao Zhang,Bo Jiang,Pengfei Gao,Chao Peng,Cuiyun Gao*

Main category: cs.SE

TL;DR: 本文提出了ContextCRBench，一个高质量、富含上下文的细粒度代码审查基准，通过引入问题描述等文本上下文和完整的代码上下文，并结合多阶段数据过滤，解决了现有基准缺乏语义上下文、数据质量差和粒度粗糙的问题。该基准支持三个评估场景，并在实际工业部署中显著提升了代码审查系统的性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于大语言模型（LLM）的代码审查基准存在三大局限：缺乏语义上下文（如问题描述）、数据质量不高（包含过时或无关样本）以及粒度过粗（仅在文件或提交级别），无法支持细粒度的精准审查。

Method: 构建ContextCRBench基准，包括三个步骤：(1) 从顶级代码仓库爬取153.7K个问题和拉取请求；(2) 提取综合上下文，将问题与PR配对以获取文本上下文，并提取完整函数或类作为代码上下文；(3) 采用基于规则和LLM的多阶段过滤，保留67,910个高质量、富含上下文的样本。该基准支持hunk级质量评估、行级缺陷定位和行级评论生成三种评估场景。

Result: 在八个主流LLM上的评估表明，文本上下文比仅使用代码上下文更能提升模型性能，但当前LLM距离人类水平仍有较大差距。该基准已在字节跳动部署，驱动自进化代码审查系统，性能提升61.98%。

Conclusion: ContextCRBench是一个高质量、细粒度且富含上下文的代码审查基准，有效弥补了现有基准的不足，在学术评估和工业应用中均展现出显著价值和实用性。

Abstract: Code review is a cornerstone of software quality assurance, and recent advances in Large Language Models (LLMs) have shown promise in automating this process. However, existing benchmarks for LLM-based code review face three major limitations. (1) Lack of semantic context: most benchmarks provide only code diffs without textual information such as issue descriptions, which are crucial for understanding developer intent. (2) Data quality issues: without rigorous validation, many samples are noisy-e.g., reviews on outdated or irrelevant code-reducing evaluation reliability. (3) Coarse granularity: most benchmarks operate at the file or commit level, overlooking the fine-grained, line-level reasoning essential for precise review.
  We introduce ContextCRBench, a high-quality, context-rich benchmark for fine-grained LLM evaluation in code review. Our construction pipeline comprises: (1) Raw Data Crawling, collecting 153.7K issues and pull requests from top-tier repositories; (2) Comprehensive Context Extraction, linking issue-PR pairs for textual context and extracting the full surrounding function or class for code context; and (3) Multi-stage Data Filtering, combining rule-based and LLM-based validation to remove outdated, malformed, or low-value samples, resulting in 67,910 context-enriched entries.
  ContextCRBench supports three evaluation scenarios aligned with the review workflow: (1) hunk-level quality assessment, (2) line-level defect localization, and (3) line-level comment generation. Evaluating eight leading LLMs (four closed-source and four open-source) reveals that textual context yields greater performance gains than code context alone, while current LLMs remain far from human-level review ability. Deployed at ByteDance, ContextCRBench drives a self-evolving code review system, improving performance by 61.98% and demonstrating its robustness and industrial utility.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [30] [Digital Twin-Assisted Task Offloading and Resource Allocation in ISAC-Enabled Internet of Vehicles](https://arxiv.org/abs/2511.05789)
*Shanhao Zhan,Zhang Liu,Lianfen Huang,Shaowei Shen,Ziyang Bai,Zhibin Gao,Dusit Niyato*

Main category: cs.NI

TL;DR: 本文提出了一种基于数字孪生（DT）辅助的集成感知与通信（ISAC）车联网任务卸载与资源分配方法，通过引入指令级传输模式和Lyapunov优化框架，结合多智能体强化学习算法Ly-DTMPPO，在保证队列长期稳定的前提下有效降低了系统时延与能耗。


<details>
  <summary>Details</summary>
Motivation: 车联网（IoV）在6G时代面临频谱效率低、动态资源优化困难以及高动态环境下长期稳定性不足等挑战，亟需一种兼顾性能与稳定性的智能资源管理机制。

Method: 作者构建了一个ISAC赋能的IoV系统模型，引入原始数据传输（DataT）和指令传输（InstrT）两种模式，并采用Lyapunov优化将长期随机问题分解为每时隙确定性子问题；在此基础上，设计了Lyapunov驱动的DT增强型多智能体近端策略优化算法（Ly-DTMPPO），利用数字孪生实现全局状态感知，并在集中训练-分散执行（CTDE）架构下进行智能决策。

Result: 仿真实验表明，所提出的Ly-DTMPPO算法在系统成本（时延与能耗加权和）方面显著优于现有基准方法，同时保障了队列的长期稳定性。

Conclusion: 融合数字孪生与Lyapunov优化的多智能体强化学习方法能有效提升ISAC赋能车联网中的资源利用效率与系统稳定性，为未来智能交通系统提供了可行的技术路径。

Abstract: The convergence of the Internet of vehicles (IoV) and 6G networks is driving the evolution of next-generation intelligent transportation systems. However, IoV networks face persistent challenges, including low spectral efficiency in vehicular communications, difficulty in achieving dynamic and adaptive resource optimization, and the need for long-term stability under highly dynamic environments. In this paper, we study the problem of digital twin (DT)-assisted task offloading and resource allocation in integrated sensing and communication (ISAC)-enabled IoV networks. The objective is to minimize the long-term average system cost, defined as a weighted combination of delay and energy consumption, while ensuring queue stability over time. To address this, we employ an ISAC-enabled design and introduce two transmission modes (i.e., raw data transmission (DataT) and instruction transmission (InstrT)). The InstrT mode enables instruction-level transmission, thereby reducing data volume and improving spectral efficiency. We then employ Lyapunov optimization to decompose the long-term stochastic problem into per-slot deterministic problems, ensuring long-term queue stability. Building upon this, we propose a Lyapunov-driven DT-enhanced multi-agent proximal policy optimization (Ly-DTMPPO) algorithm, which leverages DT for global state awareness and intelligent decision-making within a centralized training and decentralized execution (CTDE) architecture. Extensive simulations verify that Ly-DTMPPO achieves superior performance compared with existing benchmarks.

</details>


### [31] [Graph Representation-based Model Poisoning on the Heterogeneous Internet of Agents](https://arxiv.org/abs/2511.07176)
*Hanlin Cai,Houtianfu Wang,Haofan Dong,Kai Li,Ozgur B. Akan*

Main category: cs.NI

TL;DR: 本文提出了一种基于图表示的模型投毒攻击（GRMP），通过构建参数相关图并利用变分图自编码器重塑高阶依赖关系，在联邦学习驱动的互联网智能体（IoA）系统中生成难以被检测的恶意本地模型，从而对系统准确性造成持续损害，并揭示了现有防御机制在大规模异构环境下的脆弱性。


<details>
  <summary>Details</summary>
Motivation: 联邦学习虽能支持分布式大语言模型智能体协同训练，但在互联网智能体（IoA）范式下面临模型投毒攻击的风险，而现有的基于距离和相似性的防御方法在十亿参数规模和数据异构环境下效果有限，亟需研究更隐蔽、更具破坏力的攻击方式以评估系统安全性。

Method: 提出GRMP攻击方法：首先被动观察良性本地模型，构建参数相关图；然后引入对抗性变分图自编码器，捕获并重塑参数间的高阶依赖关系；最终合成在统计特征上类似良性模型但嵌入对抗目标的恶意本地模型，以规避服务器端检测。

Result: 实验表明，GRMP攻击可导致系统准确率逐步下降，且现有主流防御机制难以有效识别该攻击，证明其对IoA系统的安全构成严重威胁。

Conclusion: GRMP攻击揭示了当前联邦学习在IoA场景下对高级模型投毒攻击的防御不足，强调了在大规模异构环境中重新设计鲁棒安全机制的必要性。

Abstract: Internet of Agents (IoA) envisions a unified, agent-centric paradigm where heterogeneous large language model (LLM) agents can interconnect and collaborate at scale. Within this paradigm, federated learning (FL) serves as a key enabler that allows distributed LLM agents to co-train global models without centralizing data. However, the FL-enabled IoA system remains vulnerable to model poisoning attacks, and the prevailing distance and similarity-based defenses become fragile at billion-parameter scale and under heterogeneous data distributions. This paper proposes a graph representation-based model poisoning (GRMP) attack, which passively exploits observed benign local models to construct a parameter correlation graph and extends an adversarial variational graph autoencoder to capture and reshape higher-order dependencies. The GRMP attack synthesizes malicious local models that preserve benign-like statistics while embedding adversarial objectives, remaining elusive to detection at the server. Experiments demonstrate a gradual drop in system accuracy under the proposed attack and the ineffectiveness of the prevailing defense mechanism in detecting the attack, underscoring a severe threat to the ambitious IoA paradigm.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [32] [A Graph-Theoretical Perspective on Law Design for Multiagent Systems](https://arxiv.org/abs/2511.06361)
*Qi Shi,Pavel Naumov*

Main category: cs.MA

TL;DR: 本文研究多智能体系统中两类法律（有用法律和无漏洞法律）的最小化问题，证明其为NP难问题，并指出可借助超图顶点覆盖的近似算法高效求解近似最小法律。


<details>
  <summary>Details</summary>
Motivation: 在多智能体系统中，需要通过施加行为约束（即“法律”）来避免不良结果。作者旨在寻找既能达成目标又限制最少的法律，以提高系统的效率与可行性。

Method: 分析两类法律（有用法律和无漏洞法律）的形式化定义，并将最小化问题归约为计算复杂性问题；利用超图顶点覆盖问题的近似算法来近似求解最小法律。

Result: 证明了在单次并发交互情形下，两类法律的最小化问题均为NP-hard；同时表明可使用超图顶点覆盖的近似算法对最小法律进行有效近似。

Conclusion: 最小法律的设计在理论上是困难的（NP-hard），但可通过已有近似算法获得实用解，为多智能体系统中的规则设计提供了理论基础与可行路径。

Abstract: A law in a multiagent system is a set of constraints imposed on agents' behaviours to avoid undesirable outcomes. The paper considers two types of laws: useful laws that, if followed, completely eliminate the undesirable outcomes and gap-free laws that guarantee that at least one agent can be held responsible each time an undesirable outcome occurs. In both cases, we study the problem of finding a law that achieves the desired result by imposing the minimum restrictions.
  We prove that, for both types of laws, the minimisation problem is NP-hard even in the simple case of one-shot concurrent interactions. We also show that the approximation algorithm for the vertex cover problem in hypergraphs could be used to efficiently approximate the minimum laws in both cases.

</details>


### [33] [When AI Agents Collude Online: Financial Fraud Risks by Collaborative LLM Agents on Social Platforms](https://arxiv.org/abs/2511.06448)
*Qibing Ren,Zhijie Zheng,Jiaxuan Guo,Junchi Yan,Lizhuang Ma,Jing Shao*

Main category: cs.MA

TL;DR: 本文研究了由大语言模型驱动的大规模多智能体系统中集体金融欺诈的风险，提出了包含28种典型在线欺诈场景的基准MultiAgentFraudBench，并分析了影响欺诈成功的关键因素及相应的缓解策略。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型驱动的多智能体系统在现实世界中的广泛应用，其潜在的集体金融欺诈风险日益凸显。现有研究缺乏对多智能体协同欺诈行为及其放大效应的系统性分析，也缺少用于评估和缓解此类风险的大规模基准。

Method: 作者构建了MultiAgentFraudBench基准，模拟覆盖完整欺诈生命周期的28种真实在线金融欺诈场景；通过该基准分析交互深度、活跃度和协作失败模式等关键因素对欺诈成功率的影响；并提出内容级警告、LLM监控器和群体信息共享等缓解策略。

Result: 实验发现恶意智能体能够协同实施欺诈，且其成功率受多种因素影响；所提出的缓解策略（如内容警告、LLM监控）在一定程度上有效，但恶意智能体可适应环境干预；群体层面的信息共享有助于提升整体抗欺诈能力。

Conclusion: 大规模多智能体系统存在显著的集体金融欺诈风险，需结合技术手段与社会机制进行综合治理。本研究为理解和防范此类风险提供了实证基础和实用对策。

Abstract: In this work, we study the risks of collective financial fraud in large-scale multi-agent systems powered by large language model (LLM) agents. We investigate whether agents can collaborate in fraudulent behaviors, how such collaboration amplifies risks, and what factors influence fraud success. To support this research, we present MultiAgentFraudBench, a large-scale benchmark for simulating financial fraud scenarios based on realistic online interactions. The benchmark covers 28 typical online fraud scenarios, spanning the full fraud lifecycle across both public and private domains. We further analyze key factors affecting fraud success, including interaction depth, activity level, and fine-grained collaboration failure modes. Finally, we propose a series of mitigation strategies, including adding content-level warnings to fraudulent posts and dialogues, using LLMs as monitors to block potentially malicious agents, and fostering group resilience through information sharing at the societal level. Notably, we observe that malicious agents can adapt to environmental interventions. Our findings highlight the real-world risks of multi-agent financial fraud and suggest practical measures for mitigating them. Code is available at https://github.com/zheng977/MutiAgent4Fraud.

</details>


### [34] [Multi-Agent Reinforcement Learning for Deadlock Handling among Autonomous Mobile Robots](https://arxiv.org/abs/2511.07071)
*Marcel Müller*

Main category: cs.MA

TL;DR: 本文研究了多智能体强化学习（MARL）在自主移动机器人（AMR）内物流系统中处理死锁问题的应用，提出结合集中训练与分散执行（CTDE）的MARL策略，在复杂拥堵环境中优于传统规则方法。


<details>
  <summary>Details</summary>
Motivation: 现有内物流系统在规划阶段常忽略死锁问题，依赖无法适应动态环境的刚性控制规则，难以保障系统吞吐量与可靠性。

Method: 构建考虑死锁的多智能体路径规划（MAPF）参考模型，利用基于网格的环境和外部仿真软件，对比传统策略与基于PPO和IMPALA算法的MARL方法，重点评估不同训练与执行模式（如CTDE）下的性能。

Result: 在复杂拥堵环境中，采用CTDE的MARL策略显著优于规则方法；而在空间宽松的简单环境中，规则方法因计算开销低仍具竞争力。

Conclusion: MARL为动态内物流系统的死锁处理提供了灵活可扩展的解决方案，但需根据具体运行环境进行针对性设计。

Abstract: This dissertation explores the application of multi-agent reinforcement learning (MARL) for handling deadlocks in intralogistics systems that rely on autonomous mobile robots (AMRs). AMRs enhance operational flexibility but also increase the risk of deadlocks, which degrade system throughput and reliability. Existing approaches often neglect deadlock handling in the planning phase and rely on rigid control rules that cannot adapt to dynamic operational conditions.
  To address these shortcomings, this work develops a structured methodology for integrating MARL into logistics planning and operational control. It introduces reference models that explicitly consider deadlock-capable multi-agent pathfinding (MAPF) problems, enabling systematic evaluation of MARL strategies. Using grid-based environments and an external simulation software, the study compares traditional deadlock handling strategies with MARL-based solutions, focusing on PPO and IMPALA algorithms under different training and execution modes.
  Findings reveal that MARL-based strategies, particularly when combined with centralized training and decentralized execution (CTDE), outperform rule-based methods in complex, congested environments. In simpler environments or those with ample spatial freedom, rule-based methods remain competitive due to their lower computational demands. These results highlight that MARL provides a flexible and scalable solution for deadlock handling in dynamic intralogistics scenarios, but requires careful tailoring to the operational context.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [35] [Inductive Loop Analysis for Practical HPC Application Optimization](https://arxiv.org/abs/2511.06052)
*Philipp Schaad,Tal Ben-Nun,Patrick Iff,Torsten Hoefler*

Main category: cs.DC

TL;DR: 本文提出了一种名为SILO的新技术，通过将数据访问和依赖关系建模为循环步长的函数，实现了自动并行化和数据移动优化，在科学计算核心应用中取得了最高12倍的加速。


<details>
  <summary>Details</summary>
Motivation: 高性能计算框架通常将细粒度的数据移动优化交给编译器处理，但其底层表示难以分析常见的模式（如跨步数据访问和循环间依赖），限制了优化效果。

Method: 提出符号归纳循环优化（SILO）方法，将数据访问与依赖关系抽象为循环嵌套步长的函数，从而支持自动并行化、软件预取和指针递增等优化。

Result: 在大气模型和数值求解器等科学计算核心程序上验证了SILO的有效性，相比现有技术最高获得12倍性能提升。

Conclusion: SILO通过高层次抽象有效解决了传统编译器在多层循环嵌套中难以识别和优化数据访问模式的问题，显著提升了科学计算应用的性能。

Abstract: Scientific computing applications heavily rely on multi-level loop nests operating on multidimensional arrays. This presents multiple optimization opportunities from exploiting parallelism to reducing data movement through prefetching and improved register usage. HPC frameworks often delegate fine-grained data movement optimization to compilers, but their low-level representations hamper analysis of common patterns, such as strided data accesses and loop-carried dependencies. In this paper, we introduce symbolic, inductive loop optimization (SILO), a novel technique that models data accesses and dependencies as functions of loop nest strides. This abstraction enables the automatic parallelization of sequentially-dependent loops, as well as data movement optimizations including software prefetching and pointer incrementation to reduce register spills. We demonstrate SILO on fundamental kernels from scientific applications with a focus on atmospheric models and numerical solvers, achieving up to 12$\times$ speedup over the state of the art.

</details>


### [36] [HYDRA: Breaking the Global Ordering Barrier in Multi-BFT Consensus](https://arxiv.org/abs/2511.05843)
*Hanzheng Lyu,Shaokang Xie,Jianyu Niu,Mohammad Sadoghi,Yinqian Zhang,Cong Wang,Ivan Beschastnikh,Chen Feng*

Main category: cs.DC

TL;DR: HYDRA is a novel Multi-BFT consensus framework that removes the need for global ordering by using an object-centric execution model with lightweight coordination, achieving better scalability and performance than existing approaches.


<details>
  <summary>Details</summary>
Motivation: Existing Multi-BFT protocols rely on a global ordering layer to serialize blocks across parallel BFT instances, which limits scalability, increases failure propagation, and complicates deployment.

Method: HYDRA partitions transactions by accessed objects and enables concurrent deterministic execution across BFT instances, using lightweight lock-based coordination and deadlock resolution to ensure consistency without global ordering.

Result: Experiments with up to 128 replicas in LAN and WAN settings show HYDRA outperforms state-of-the-art Multi-BFT protocols, especially in the presence of stragglers, while maintaining strong consistency.

Conclusion: Eliminating global ordering in Multi-BFT consensus is feasible and beneficial; HYDRA demonstrates that scalable, high-performance, and strongly consistent Multi-BFT can be achieved through object-centric execution and lightweight coordination.

Abstract: Multi-Byzantine Fault Tolerant (Multi-BFT) consensus, which runs multiple BFT instances in parallel, has recently emerged as a promising approach to overcome the leader bottleneck in classical BFT protocols. However, existing designs rely on a global ordering layer to serialize blocks across instances, an intuitive yet costly mechanism that constrains scalability, amplifies failure propagation, and complicates deployment. In this paper, we challenge this conventional wisdom. We present HYDRA, the first Multi-BFT consensus framework that eliminates global ordering altogether. HYDRA introduces an object-centric execution model that partitions transactions by their accessed objects, enabling concurrent yet deterministic execution across instances. To ensure consistency, HYDRA combines lightweight lock-based coordination with a deadlock resolution mechanism, achieving both scalability and correctness. We implement HYDRA and evaluate it on up to 128 replicas in both LAN and WAN environments. Experimental results show HYDRA outperforms several state-of-the-art Multi-BFT protocols in the presence of a straggler. These results demonstrate strong consistency and high performance by removing global ordering, opening a new direction toward scalable Multi-BFT consensus design.

</details>


### [37] [LiteCast: A Lightweight Forecaster for Carbon Optimizations](https://arxiv.org/abs/2511.06187)
*Mathew Joseph,Tanush Savadi,Abel Souza*

Main category: cs.DC

TL;DR: LiteCast 是一种轻量级碳强度预测方法，仅需少量历史数据即可快速适应电网变化，在全球50个地区实测中比现有方法节省20%碳排放，达到近似最优效果。


<details>
  <summary>Details</summary>
Motivation: 现有高精度碳强度预测模型计算开销大、扩展性差，且精度提升未必带来显著节能增益；作者认为保持预测值与真实值的相对排序更能驱动实际节能，因此提出更高效轻量的预测策略。

Method: 提出 LiteCast，一种轻量级时间序列预测方法，仅使用几天的历史能源与天气数据，快速建模区域能源结构以估算碳强度，并能迅速适应电网突变。

Result: 在50个全球地区的多种真实负载下评估表明，LiteCast 比当前最先进的预测器多实现20%的碳节省，达到最大可实现平均节省的97%，同时保持轻量、高效和对新数据的适应性。

Conclusion: LiteCast 证明了无需复杂高精度模型也能实现接近最优的碳感知调度效果，为碳强度预测提供了一种高效、实用的新范式。

Abstract: Over recent decades, electricity demand has experienced sustained growth through widespread electrification of transportation and the accelerated expansion of Artificial Intelligence (AI). Grids have managed the resulting surges by scaling generation capacity, incorporating additional resources such as solar and wind, and implementing demand-response mechanisms. Altogether, these policies influence a region's carbon intensity by affecting its energy mix. To mitigate the environmental impacts of consumption, carbon-aware optimizations often rely on long-horizon, high-accuracy forecasts of the grid's carbon intensity that typically use compute intensive models with extensive historical energy mix data. In addition to limiting scalability, accuracy improvements do not necessarily translate into proportional increases in savings. Highlighting the need for more efficient forecasting strategies, we argue that carbon forecasting solutions can achieve the majority of savings without requiring highly precise and complex predictions. Instead, it is the preservation of the ranking of forecasts relative to the ground-truth that drives realized savings. In this paper, we present LiteCast, a lightweight time series forecasting method capable of quickly modeling a region's energy mix to estimate its carbon intensity. LiteCast requires only a few days of historical energy and weather data, delivering fast forecasts that can quickly adapt to sudden changes in the electrical grid. Our evaluation in 50 worldwide regions under various real-world workloads shows that LiteCast outperforms state-of-the-art forecasters, delivering 20% higher savings with near-optimal performance, achieving 97% of the maximum attainable average savings, while remaining lightweight, efficient to run, and adaptive to new data.

</details>


### [38] [PRAGMA: A Profiling-Reasoned Multi-Agent Framework for Automatic Kernel Optimization](https://arxiv.org/abs/2511.06345)
*Kelun Lei,Hailong Yang,Huaitao Zhang,Xin You,Kaige Zhang,Zhongzhi Luan,Yi Liu,Depei Qian*

Main category: cs.DC

TL;DR: 本文提出PRAGMA，一种结合执行反馈与细粒度硬件性能分析的AI核函数生成框架，显著提升生成核函数的性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于大语言模型的自动核函数生成方法大多仅依赖正确性或执行时间反馈，缺乏对底层性能瓶颈的推理能力。

Method: PRAGMA将执行反馈和细粒度硬件性能分析整合到大语言模型的推理循环中，使其能够识别性能瓶颈、保留历史最优版本并迭代优化代码。

Result: 在KernelBench基准上，PRAGMA在CPU和GPU平台上分别比Torch平均提速2.81倍和2.30倍，并优于未启用性能分析的基线AIKG方法。

Conclusion: PRAGMA通过引入硬件性能分析显著提升了AI自动生成高性能核函数的能力，验证了其在多硬件平台上的有效性。

Abstract: Designing high-performance kernels requires expert-level tuning and a deep understanding of hardware characteristics. Recent advances in large language models (LLMs) have enabled automated kernel generation, yet most existing systems rely solely on correctness or execution time feedback, lacking the ability to reason about low-level performance bottlenecks. In this paper, we introduce PRAGMA, a profile-guided AI kernel generation framework that integrates execution feedback and fine-grained hardware profiling into the reasoning loop. PRAGMA enables LLMs to identify performance bottlenecks, preserve historical best versions, and iteratively refine code quality. We evaluate PRAGMA on KernelBench, covering GPU and CPU backends. Results show that PRAGMA consistently outperforms baseline AIKG without profiling enabled and achieves 2.81$\times$ and 2.30$\times$ averaged speedups against Torch on CPU and GPU platforms, respectively.

</details>


### [39] [Saarthi: An End-to-End Intelligent Platform for Optimising Distributed Serverless Workloads](https://arxiv.org/abs/2511.06599)
*Siddharth Agarwal,Maria A. Rodriguez,Rajkumar Buyya*

Main category: cs.DC

TL;DR: 本文提出了Saarthi，一种新型端到端无服务器框架，通过输入感知的资源预测、智能请求编排和多目标优化，在提升吞吐量的同时显著降低成本，并保障服务水平。


<details>
  <summary>Details</summary>
Motivation: 当前FaaS平台存在启动延迟高、资源配置静态、调度策略与工作负载无关等问题，导致函数性能不稳定和运营成本不可控，亟需一种能动态适配负载需求的智能无服务器框架。

Method: Saarthi采用输入感知机制预测函数资源需求，实现精准的函数资源配置；结合主动容错冗余机制，并利用多目标整数线性规划（ILP）模型优化函数实例数量，以最大化吞吐量并最小化成本。

Result: 在OpenFaaS上实现的实验表明，Saarthi相比基线可提升1.45倍吞吐量，降低1.84倍成本，同时维持高达98.3%的服务水平目标，仅引入最多0.2秒的开销。

Conclusion: Saarthi通过输入驱动的资源管理与智能调度，有效解决了现有FaaS平台在性能与成本之间的权衡问题，为迈向自驱动无服务器平台提供了可行路径。

Abstract: FaaS offers significant advantages with its infrastructure abstraction, on-demand execution, and attractive no idle resource pricing for modern cloud applications. Despite these benefits, challenges such as startup latencies, static configurations, sub-optimal resource allocation and scheduling still exist due to coupled resource offering and workload-agnostic generic scheduling behaviour. These issues often lead to inconsistent function performance and unexpected operational costs for users and service providers. This paper introduces Saarthi, a novel, end-to-end serverless framework that intelligently manages the dynamic resource needs of function workloads, representing a significant step toward self-driving serverless platforms. Unlike platforms that rely on static resource configurations, Saarthi is input-aware, allowing it to intelligently anticipate resource requirements based on the characteristics of an incoming request payload. This input-driven approach reinforces function right-sizing and enables smart request orchestration across available function configurations. Saarthi further integrates a proactive fault-tolerant redundancy mechanism and employs a multi-objective Integer Linear Programming (ILP) model to maintain an optimal function quantity. This optimisation aims to maximise system throughput while simultaneously reducing overall operational costs. We validate the effectiveness of Saarthi by implementing it as a framework atop OpenFaaS. Our results demonstrate Saarthi's ability to achieve up to 1.45x better throughput, 1.84x reduced costs, while maintaining up to 98.3% service level targets with an overhead of up to 0.2 seconds as compared to the baseline OpenFaaS.

</details>


### [40] [DMA Collectives for Efficient ML Communication Offloads](https://arxiv.org/abs/2511.06605)
*Suchita Pati,Mahzabeen Islam,Shaizeen Aga,Mohamed Assem Ibrahim*

Main category: cs.DC

TL;DR: 本文对在AMD MI300X GPU上将机器学习通信集合操作（如all-gather、all-to-all）卸载到DMA引擎的性能、功耗/能耗及同步开销进行了全面分析，发现DMA在大数据量时优于现有RCCL库，但在小数据量时延迟较高；通过利用现有DMA架构创新优化后，显著缩小了小数据量下的性能差距，并进一步提升了大尺寸下的性能与能效。


<details>
  <summary>Details</summary>
Motivation: 现有研究仅在有限场景下（如仅关注带宽受限的大数据传输和性能）评估DMA通信集合操作，缺乏对其在性能、功耗和同步开销等方面的全面分析，阻碍了其在主流集合通信库中的应用。

Method: 在AMD Instinct MI300X GPU上对DMA引擎卸载ML通信集合操作进行全面评估，包括性能、功耗/能耗和同步成本分析；随后利用未被充分利用的DMA架构特性设计优化方案，并在真实硬件上验证其效果。

Result: 相比RCCL库，原始DMA实现在大尺寸数据（几十MB至GB级）上性能提升16%、功耗降低32%，但在小尺寸数据上延迟显著更高（all-gather慢4.5倍，all-to-all慢2.5倍）；优化后的DMA实现在小尺寸上大幅缩小差距（all-gather慢30%，all-to-all快20%），并在大尺寸上进一步提升性能7%、节能3–10%。

Conclusion: 通过系统性分析与针对性优化，DMA通信集合操作在各类数据规模下均展现出实用潜力，为将其集成到主流集合通信库迈出了关键一步。

Abstract: Offloading machine learning (ML) communication collectives to direct memory access (DMA) engines has emerged as an interesting and low-cost solution to efficiently overlap computation and communication in inference and training. Doing so delivers superior concurrent performance by freeing up all GPU cores for computation and also lowers interference in the memory sub-system (caches). While DMA collectives show strong promise, prior works have only studied them in limited context (bandwidth-bound transfer sizes only, performance-only).
  To address this, we provide a comprehensive performance, power/energy and synchronization costs analysis of offloading ML communication collectives (all-gather, all-to-all) to DMA engines on state-of-the-art AMD Instinct MI300X GPUs. Our analysis reveals that, compared to the state-of-the-art RCCL communication collectives library, DMA collectives are at-par or better for large sizes (10s of MB to GB) in terms of both performance (16% better) and power (32% better). However, they significantly lag for latency-bound small sizes; 4.5X and 2.5X slower for all-gather and all-to-all, respectively. We provide a detailed latency breakdown of a DMA transfer and identify that DMA command scheduling and synchronization costs can limit DMA collective performance. To tackle this, we harness existing DMA architecture innovations, hitherto untapped, to build optimized DMA collectives and demonstrate their efficacy on real hardware. Our optimized implementations considerably close the performance gap for DMA collectives at smaller sizes (30% slower and 20% faster all-gather and all-to-all, respectively) and further improves performance (by 7%) and power savings at larger sizes (3-10%). Overall, this work represents a significant step toward making DMA collectives suitable for adoption in mainstream collective libraries.

</details>


### [41] [A GPU-boosted high-performance multi-working condition joint analysis framework for predicting dynamics of textured axial piston pump](https://arxiv.org/abs/2511.06824)
*Xin Yao,Yang Liu,Jin Jiang,Yesen Chen,Zhilong Chen,Hongkang Dong,Xiaofeng Wei,Teng Zhang,Dongyun Wang*

Main category: cs.DC

TL;DR: 本文提出了一种基于GPU加速的多工况联合分析框架（GMAF），用于高效模拟轴向柱塞泵（APP）在光滑和织构表面下的多周期动力学行为，显著提升了计算效率与精度。


<details>
  <summary>Details</summary>
Motivation: 传统CPU计算能力和迭代方法在处理具有织构表面、需要精细网格的复杂轴向柱塞泵动力学仿真时效率低下，难以支持多周期模拟。

Method: 设计了GPU加速的高性能多工况联合分析框架（GMAF），采用带近似对称逐次超松弛（ASSOR）预条件子的预条件共轭梯度法（PCG），并采用面向全局收敛的同步收敛策略，充分利用GPU进行大规模并行计算。

Result: GMAF显著加速了压力场代数系统的构建与求解以及油流引起的力和力矩的数值积分；仿真结果显示轴向油力和周向力矩直接响应输入压力，其他分量呈正弦变化；织构表面可提升压力承载能力和抗扭性能，并在压力场中形成对应“台阶”。

Conclusion: 所提出的GMAF框架能高效、准确地模拟轴向柱塞泵在多周期下的动力学特性，尤其适用于带织构表面的复杂工况，为泵的设计与优化提供了有力工具。

Abstract: Accurate simulation to dynamics of axial piston pump (APP) is essential for its design, manufacture and maintenance. However, limited by computation capacity of CPU device and traditional solvers, conventional iteration methods are inefficient in complicated case with textured surface requiring refined mesh, and could not handle simulation during multiple periods. To accelerate Picard iteration for predicting dynamics of APP, a GPU-boosted high-performance Multi-working condition joint Analysis Framework (GMAF) is designed, which adopts Preconditioned Conjugate Gradient method (PCG) using Approximate Symmetric Successive Over-Relaxation preconditioner (ASSOR). GMAF abundantly utilizes GPU device via elevating computational intensity and expanding scale of massive parallel computation. Therefore, it possesses novel performance in analyzing dynamics of both smooth and textured APPs during multiple periods, as the establishment and solution to joint algebraic system for pressure field are accelerated magnificently, as well as numerical integral for force and moment due to oil flow. Compared with asynchronized convergence strategy pursuing local convergence, synchronized convergence strategy targeting global convergence is adopted in PCG solver for the joint system. Revealed by corresponding results, oil force in axial direction and moment in circumferential directly respond to input pressure, while other components evolve in sinusoidal patterns. Specifically, force and moment due to normal pressure instantly reach their steady state initially, while ones due to viscous shear stress evolve during periods. After simulating dynamics of APP and pressure distribution via GMAF, the promotion of pressure capacity and torsion resistance due to textured surface is revealed numerically, as several 'steps' exist in the pressure field corresponding to textures.

</details>


### [42] [LLMServingSim2.0: A Unified Simulator for Heterogeneous Hardware and Serving Techniques in LLM Infrastructure](https://arxiv.org/abs/2511.07229)
*Jaehong Cho,Hyunmin Choi,Jongse Park*

Main category: cs.DC

TL;DR: LLMServingSim2.0 是一个用于大规模大语言模型（LLM）服务系统的异构硬件仿真平台，通过引入基于轨迹的性能建模和算子级延迟分析器，显著简化了新硬件加速器的集成，并支持多种现代 LLM 服务技术，具备高精度与低开销。


<details>
  <summary>Details</summary>
Motivation: 现有 LLM 服务系统仿真器存在两大局限：一是缺乏清晰的抽象使得将硬件模型集成到系统级仿真器中困难；二是仅支持有限的服务技术，无法涵盖现代 LLM 服务中的多样化方法。

Method: LLMServingSim2.0 采用基于轨迹的性能建模方法，并结合算子级延迟分析器，实现一键式新加速器集成；同时嵌入最新的服务技术，并提供灵活的请求路由、缓存管理和调度策略接口。

Result: 在 TPU 案例研究中，其分析器代码量减少 18.5 倍，且优于前代硬件-仿真器集成方式；在 GPU 场景下，仿真误差仅为 1.9%，同时保持合理的仿真耗时。

Conclusion: LLMServingSim2.0 提供了一个高效、准确且易于扩展的仿真平台，适用于硬件开发者和 LLM 服务提供商，能够全面支持异构硬件探索与服务策略评估。

Abstract: This paper introduces LLMServingSim2.0, a system simulator designed for exploring heterogeneous hardware in large-scale LLM serving systems. LLMServingSim2.0 addresses two key limitations of its predecessor: (1) integrating hardware models into system-level simulators is non-trivial due to the lack of a clear abstraction, and (2) existing simulators support only a narrow subset of serving techniques, leaving no infrastructure that captures the breadth of approaches in modern LLM serving. To overcome these issues, LLMServingSim2.0 adopts trace-driven performance modeling, accompanied by an operator-level latency profiler, enabling the integration of new accelerators with a single command. It further embeds up-to-date serving techniques while exposing flexible interfaces for request routing, cache management, and scheduling policies. In a TPU case study, our profiler requires 18.5x fewer LoC and outperforms the predecessor's hardware-simulator integration, demonstrating LLMServingSim2.0's low-effort hardware extensibility. Our experiments further show that LLMServingSim2.0 reproduces GPU-based LLM serving with 1.9% error, while maintaining practical simulation time, making it a comprehensive platform for both hardware developers and LLM service providers.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [43] [LUT-LLM: Efficient Large Language Model Inference with Memory-based Computations on FPGAs](https://arxiv.org/abs/2511.06174)
*Zifan He,Shengyu Ye,Rui Ma,Yang Wang,Jason Cong*

Main category: cs.AR

TL;DR: 本文提出LUT-LLM，首个基于FPGA的加速器，通过向量量化内存操作实现10亿级以上大语言模型（LLM）推理，利用查找表将计算从算术密集型转为内存密集型，在AMD V80 FPGA上部署定制Qwen 3 1.7B模型，相比AMD MI210和NVIDIA A100分别实现了更低延迟与更高能效，并可扩展至32B模型。


<details>
  <summary>Details</summary>
Motivation: 尽管GPU在算术计算方面优化显著，削弱了FPGA在能效方面的传统优势，但FPGA拥有丰富的片上内存资源。作者旨在利用这一特性，将大语言模型推理从依赖算术运算转向基于内存查找的计算范式，以提升在设备端单批次推理的效率。

Method: 提出LUT-LLM架构，采用激活值与权重协同量化策略，并结合三项关键技术：(1) 带宽感知的并行质心搜索；(2) 高效的二维查表操作；(3) 空间-时间混合设计以最小化数据缓存开销。

Result: 在AMD V80 FPGA上实现定制Qwen 3 1.7B模型推理，相比AMD MI210降低1.66倍延迟，相比NVIDIA A100提升1.72倍能效；该方法可扩展至32B模型，在A100上获得2.16倍的能效增益。

Conclusion: LUT-LLM成功展示了通过内存为中心的计算范式可在FPGA上高效运行大规模语言模型，显著优于当前主流GPU，在延迟和能效方面均取得突破，为端侧大模型部署提供了新路径。

Abstract: The rapid progress of large language models (LLMs) has advanced numerous applications, yet efficient single-batch inference remains vital for on-device intelligence. While FPGAs offer fine-grained data control and high energy efficiency, recent GPU optimizations have narrowed their advantage, especially under arithmetic-based computation. To overcome this, we leverage FPGAs' abundant on-chip memory to shift LLM inference from arithmetic- to memory-based computation through table lookups. We present LUT-LLM, the first FPGA accelerator enabling 1B+ LLM inference via vector-quantized memory operations. Our analysis identifies activation-weight co-quantization as the most effective scheme, supported by (1) bandwidth-aware parallel centroid search, (2) efficient 2D table lookups, and (3) a spatial-temporal hybrid design minimizing data caching. Implemented on an AMD V80 FPGA for a customized Qwen 3 1.7B model, LUT-LLM achieves 1.66x lower latency than AMD MI210 and 1.72x higher energy efficiency than NVIDIA A100, scaling to 32B models with 2.16x efficiency gain over A100.

</details>


### [44] [Precision-Scalable Microscaling Datapaths with Optimized Reduction Tree for Efficient NPU Integration](https://arxiv.org/abs/2511.06313)
*Stef Cuyckens,Xiaoling Yi,Robin Geens,Joren Dumoulin,Martin Wiesner,Chao Fang,Marian Verhelst*

Main category: cs.AR

TL;DR: 本文提出了一种混合精度可扩展的Microscaling（MX）乘积累加（MAC）架构，解决了现有MX MAC在整数累加与FP32累加之间的精度与效率权衡问题，并将其集成到SNAX神经处理单元平台中，在多种MX格式下实现了高能效与高吞吐。


<details>
  <summary>Details</summary>
Motivation: 新兴的持续学习应用要求下一代神经处理单元（NPU）同时支持训练和推理。MX标准虽能兼顾窄位宽推理与大动态范围训练，但现有MX MAC设计在累加方式上存在转换开销大或量化损失严重的问题。

Method: 提出一种混合精度可扩展的缩减树结构用于MX MAC，结合整数与浮点累加的优势，实现高效混合精度累加；并将8x8 MAC阵列集成到SNAX NPU平台，优化数据通路与控制。

Result: 在MXINT8、MXFP8/6和MXFP4格式下，系统分别达到657、1438–1675和4065 GOPS/W的能效，以及64、256和512 GOPS的吞吐量，优于当前最先进水平。

Conclusion: 所提出的混合精度可扩展MX MAC架构有效平衡了精度与效率，显著提升了NPU在持续学习场景下的能效与性能，为支持训练与推理一体化的下一代NPU提供了可行方案。

Abstract: Emerging continual learning applications necessitate next-generation neural processing unit (NPU) platforms to support both training and inference operations. The promising Microscaling (MX) standard enables narrow bit-widths for inference and large dynamic ranges for training. However, existing MX multiply-accumulate (MAC) designs face a critical trade-off: integer accumulation requires expensive conversions from narrow floating-point products, while FP32 accumulation suffers from quantization losses and costly normalization. To address these limitations, we propose a hybrid precision-scalable reduction tree for MX MACs that combines the benefits of both approaches, enabling efficient mixed-precision accumulation with controlled accuracy relaxation. Moreover, we integrate an 8x8 array of these MACs into the state-of-the-art (SotA) NPU integration platform, SNAX, to provide efficient control and data transfer to our optimized precision-scalable MX datapath. We evaluate our design both on MAC and system level and compare it to the SotA. Our integrated system achieves an energy efficiency of 657, 1438-1675, and 4065 GOPS/W, respectively, for MXINT8, MXFP8/6, and MXFP4, with a throughput of 64, 256, and 512 GOPS.

</details>


### [45] [Offloading Data Center Tax](https://arxiv.org/abs/2511.06558)
*Akshay Revankar,Charan Renganathan,Sartaj Wariah*

Main category: cs.AR

TL;DR: 本文研究如何通过联合卸载多个“税组件”（tax components）来优化数据中心中MongoDB微服务的性能，基于DeathStarBench基准测试分析其微架构特征并提出卸载建议。


<details>
  <summary>Details</summary>
Motivation: 现代数据中心运行多种工作负载，这些负载共享许多底层通用功能（称为税组件），优化这些组件可带来整体性能提升；然而，并非所有组件都适合卸载到加速器，因此需识别可联合卸载的机会。

Method: 作者以MongoDB为研究对象，在DeathStarBench基准套件中对其进行剖析，识别其税组件及其微架构影响，并据此推断出可联合卸载的组件。

Result: 论文识别了MongoDB中的若干税组件，并基于微架构分析提出了可行的联合卸载建议。

Conclusion: 通过联合卸载多个税组件，可在实际系统中实现更高效的性能优化，尤其适用于广泛部署的微服务如MongoDB。

Abstract: The data centers of today are running diverse workloads sharing many common lower level functions called tax components. Any optimization to any tax component will lead to performance improvements across the data center fleet. Typically, performance enhancements in tax components are achieved by offloading them to accelerators, however, it is not practical to offload every tax component. The goal of this paper is to identify opportunities to offload more than one tax component together. We focus on MongoDB which is a common microservice used in a large number of applications in the datacenter. We profile MongoDB running as part of the DeathStarBench benchmark suite, identifying its tax components and their microarchitectural implications. We make observations and suggestions based on the inferences made to offload a few of the tax components in this application.

</details>


### [46] [FPGA or GPU? Analyzing comparative research for application-specific guidance](https://arxiv.org/abs/2511.06565)
*Arnab A Purkayastha,Jay Tharwani,Shobhit Aggarwal*

Main category: cs.AR

TL;DR: 本文综述并比较了FPGA与GPU在不同应用领域中的适用性，超越单纯的性能指标，为用户在特定场景下选择合适的硬件加速器提供指导。


<details>
  <summary>Details</summary>
Motivation: 现有研究多聚焦于FPGA与GPU的性能对比，缺乏对各自最适合应用场景的深入分析，本文旨在填补这一空白。

Method: 综合分析多篇研究文献，对应用进行分类，并评估关键性能指标，以识别FPGA和GPU的优势、局限及理想使用场景。

Result: 明确了FPGA和GPU在性能、能效和可编程性方面的权衡，提出了针对不同领域应用的加速器选择建议。

Conclusion: FPGA和GPU各有优势，应根据具体应用需求选择；本文提供的指导有助于研究人员和工程师做出更优决策。

Abstract: The growing complexity of computational workloads has amplified the need for efficient and specialized hardware accelerators. Field Programmable Gate Arrays (FPGAs) and Graphics Processing Units (GPUs) have emerged as prominent solutions, each excelling in specific domains. Although there is substantial research comparing FPGAs and GPUs, most of the work focuses primarily on performance metrics, offering limited insight into the specific types of applications that each accelerator benefits the most. This paper aims to bridge this gap by synthesizing insights from various research articles to guide users in selecting the appropriate accelerator for domain-specific applications. By categorizing the reviewed studies and analyzing key performance metrics, this work highlights the strengths, limitations, and ideal use cases for FPGAs and GPUs. The findings offer actionable recommendations, helping researchers and practitioners navigate trade-offs in performance, energy efficiency, and programmability.

</details>


### [47] [Preemption-Enhanced Benchmark Suite for FPGAs](https://arxiv.org/abs/2511.06736)
*Arsalan Ali Malik,John Buchanan,Aydin Aysu*

Main category: cs.AR

TL;DR: 本文提出了首个开源的支持抢占的 FPGA 基准测试套件，包含 27 个涵盖多个领域的应用，内置上下文保存与恢复机制，旨在标准化 FPGA 抢占策略与调度算法的评估。


<details>
  <summary>Details</summary>
Motivation: 当前 FPGA 调度与抢占研究缺乏统一、可复现的基准测试框架，多数研究依赖私有或合成基准，限制了结果的通用性和可比性。

Method: 开发了一个开源的预集成抢占功能的基准测试套件，包含 27 个多样化应用，并提供上下文保存/恢复机制及新增基准的指南。

Result: 该套件支持对 FPGA 抢占策略和调度算法进行一致、可复现的评估，有助于操作系统研究中的公平性、资源分配效率和上下文切换性能分析。

Conclusion: 该基准套件填补了 FPGA 调度与抢占研究中的标准化空白，为未来 FPGA 操作系统和调度策略的发展提供了有力支撑。

Abstract: Field-Programmable Gate Arrays (FPGAs) have become essential in cloud computing due to their reconfigurability, energy efficiency, and ability to accelerate domain-specific workloads. As FPGA adoption grows, research into task scheduling and preemption techniques has intensified. However, the field lacks a standardized benchmarking framework for consistent and reproducible evaluation. Many existing studies propose innovative scheduling or preemption mechanisms but often rely on proprietary or synthetic benchmarks, limiting generalizability and making comparison difficult. This methodical fragmentation hinders effective evaluation of scheduling strategies and preemption in multi-tenant FPGA environments.
  This paper presents the first open-source preemption-enabled benchmark suite for evaluating FPGA preemption strategies and testing new scheduling algorithms, without requiring users to create preemption workloads from scratch. The suite includes 27 diverse applications spanning cryptography, AI/ML, computation-intensive workloads, communication systems, and multimedia processing. Each benchmark integrates comprehensive context-saving and restoration mechanisms, facilitating reproducible research and consistent comparisons. Our suite not only simplifies testing FPGA scheduling policies but also benefits OS research by enabling the evaluation of scheduling fairness, resource allocation efficiency, and context-switching performance in multi-tenant FPGA systems, ultimately supporting the development of better operating systems and scheduling policies for FPGA-based environments. We also provide guidelines for adding new benchmarks, enabling future research to expand and refine FPGA preemption and scheduling evaluation.

</details>


### [48] [P3-LLM: An Integrated NPU-PIM Accelerator for LLM Inference Using Hybrid Numerical Formats](https://arxiv.org/abs/2511.06838)
*Yuzong Chen,Chao Fang,Xilai Dai,Yuheng Wu,Thierry Tambe,Marian Verhelst,Mohamed S. Abdelfattah*

Main category: cs.AR

TL;DR: 本文提出P3-LLM，一种结合NPU与DRAM型存内计算（PIM）的混合精度大语言模型推理加速器，通过灵活的混合数值格式量化、轻量级PIM计算单元设计和低精度数据流优化，在保持高精度的同时显著提升吞吐量，相比现有方案平均提速2.0–4.9倍。


<details>
  <summary>Details</summary>
Motivation: 大语言模型推理面临高内存带宽和计算需求的挑战；现有基于高精度（如FP16）的PIM计算单元在DRAM工艺下面积和功耗开销大，限制了有效计算吞吐量。

Method: 提出三方面方法：1）灵活的混合精度量化方案，使用混合数值格式对不同操作数进行高效压缩并保持精度；2）为支持该格式设计轻量级PIM加速器架构；3）通过算子融合优化低精度数据流，减少运行时反量化开销。

Result: 在多种代表性大语言模型和任务上评估表明，P3-LLM在KV缓存量化和权重-激活量化方面均达到当前最优精度，并相较HBM-PIM、Ecco和Pimba等先进加速器分别实现平均4.9×、2.0×和3.4×的加速比。

Conclusion: P3-LLM通过混合数值格式量化与PIM架构协同设计，在保证精度的同时显著提升了大语言模型推理效率，为高效能LLM加速提供了新思路。

Abstract: The substantial memory bandwidth and computational demand of large language models (LLMs) present critical challenges for efficient inference. To tackle this, the literature has explored heterogeneous systems that combine neural processing units (NPUs) with DRAM-based processing-in-memory (PIM) for LLM acceleration. However, existing high-precision (e.g., FP16) PIM compute units incur significant area and power overhead in DRAM technology, limiting the effective computation throughput. In this paper, we introduce P3-LLM, a novel NPU-PIM integrated accelerator for LLM inference using hybrid numerical formats. Our approach is threefold: First, we propose a flexible mixed-precision quantization scheme, which leverages hybrid numerical formats to quantize different LLM operands with high compression efficiency and minimal accuracy loss. Second, we architect an efficient PIM accelerator co-design for P3-LLM, featuring lightweight compute units to support our hybrid numerical formats. The enhanced PIM compute units significantly boost the computation throughput under iso-area constraints. Third, we optimize the low-precision dataflow of different LLM modules by applying operator fusion to minimize the overhead of runtime dequantization. Our evaluation on a diverse set of representative LLMs and tasks demonstrates that P3-LLM achieves state-of-the-art quantization accuracy in terms of both KV-cache-only quantization and weight-activation quantization. Combining the proposed quantization scheme with PIM architecture co-design, P3-LLM yields an average of $4.9\times$, $2.0\times$, and $3.4\times$ speedups over the state-of-the-art LLM accelerators HBM-PIM, Ecco, and Pimba, respectively. Our quantization code is available at https://github.com/yc2367/P3-LLM.git

</details>


### [49] [Optimizing GEMM for Energy and Performance on Versal ACAP Architectures](https://arxiv.org/abs/2511.06907)
*Ilias Papalamprou,Dimosthenis Masouros,Ioannis Loudaros,Francky Catthoor,Dimitrios Soudris*

Main category: cs.AR

TL;DR: 本文提出了一种基于机器学习的自动化框架，用于在AMD Versal ACAP上优化通用矩阵乘法（GEMM）的性能或能效映射，相比现有方法在吞吐量和能效方面分别平均提升1.23倍和1.25倍。


<details>
  <summary>Details</summary>
Motivation: GEMM是众多科学计算和深度学习任务中的核心操作，常成为性能与能效瓶颈，尤其在资源受限的边缘设备上。尽管AMD Versal ACAP具备异构计算单元可应对该挑战，但如何在其上高效映射GEMM并权衡性能与能耗仍缺乏有效方法。

Method: 作者构建了一个基于机器学习的自动化设计空间探索框架，利用约6000次板级实验数据训练模型，以指导GEMM在Versal ACAP异构架构（AIE、PL、PS）上的映射优化。

Result: 在Versal VCK190平台上的评估表明，该方法相比当前最先进的框架，在吞吐量上平均提升1.23倍（最高2.5倍），在能效上平均提升1.25倍（最高2.7倍。

Conclusion: 结合实测数据驱动的机器学习模型能更有效地指导GEMM在异构硬件上的映射优化，在性能与能效方面均显著优于传统分析方法。

Abstract: General Matrix Multiplication (GEMM) is a fundamental operation in many scientific workloads, signal processing, and particularly deep learning. It is often a bottleneck for performance and energy efficiency, especially in edge environments with tight resource and power constraints. AMD's Versal ACAP offers heterogeneous components (AIEs, PL, PS) that can address these challenges, but mapping GEMM across them is complex, with prior works largely overlooking energy-performance trade-offs. In this paper, we propose an automated framework for Versal ACAP that generates GEMM mappings optimized for either performance or energy efficiency. Unlike prior analytical approaches, our method leverages a Machine Learning (ML) model, trained on approximately 6000 on-board experiments of different GEMM mappings, to guide Design Space Exploration, yielding more efficient designs. Evaluation on the Versal VCK190 shows geomean improvements of 1.23x (up to 2.5x) in throughput and 1.25x (up to 2.7x) in energy efficiency over state-of-the-art frameworks.

</details>
