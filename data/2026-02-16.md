<div id=toc></div>

# Table of Contents

- [cs.MA](#cs.MA) [Total: 3]
- [cs.DC](#cs.DC) [Total: 2]
- [cs.NI](#cs.NI) [Total: 8]
- [cs.AR](#cs.AR) [Total: 4]
- [cs.SE](#cs.SE) [Total: 7]


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [1] [Provably Convergent Actor-Critic in Risk-averse MARL](https://arxiv.org/abs/2602.12386)
*Yizhou Zhang,Eric Mazumdar*

Main category: cs.MA

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Learning stationary policies in infinite-horizon general-sum Markov games (MGs) remains a fundamental open problem in Multi-Agent Reinforcement Learning (MARL). While stationary strategies are preferred for their practicality, computing stationary forms of classic game-theoretic equilibria is computationally intractable -- a stark contrast to the comparative ease of solving single-agent RL or zero-sum games. To bridge this gap, we study Risk-averse Quantal response Equilibria (RQE), a solution concept rooted in behavioral game theory that incorporates risk aversion and bounded rationality. We demonstrate that RQE possesses strong regularity conditions that make it uniquely amenable to learning in MGs. We propose a novel two-timescale Actor-Critic algorithm characterized by a fast-timescale actor and a slow-timescale critic. Leveraging the regularity of RQE, we prove that this approach achieves global convergence with finite-sample guarantees. We empirically validate our algorithm in several environments to demonstrate superior convergence properties compared to risk-neutral baselines.

</details>


### [2] [Agent Skills for Large Language Models: Architecture, Acquisition, Security, and the Path Forward](https://arxiv.org/abs/2602.12430)
*Renjun Xu,Yang Yan*

Main category: cs.MA

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: The transition from monolithic language models to modular, skill-equipped agents marks a defining shift in how large language models (LLMs) are deployed in practice. Rather than encoding all procedural knowledge within model weights, agent skills -- composable packages of instructions, code, and resources that agents load on demand -- enable dynamic capability extension without retraining. It is formalized in a paradigm of progressive disclosure, portable skill definitions, and integration with the Model Context Protocol (MCP). This survey provides a comprehensive treatment of the agent skills landscape, as it has rapidly evolved during the last few months. We organize the field along four axes: (i) architectural foundations, examining the SKILL.md specification, progressive context loading, and the complementary roles of skills and MCP; (ii) skill acquisition, covering reinforcement learning with skill libraries (SAGE), autonomous skill discovery (SEAgent), and compositional skill synthesis; (iii) deployment at scale, including the computer-use agent (CUA) stack, GUI grounding advances, and benchmark progress on OSWorld and SWE-bench; and (iv) security, where recent empirical analyses reveal that 26.1\% of community-contributed skills contain vulnerabilities, motivating our proposed Skill Trust and Lifecycle Governance Framework -- a four-tier, gate-based permission model that maps skill provenance to graduated deployment capabilities. We identify seven open challenges -- from cross-platform skill portability to capability-based permission models -- and propose a research agenda for realizing trustworthy, self-improving skill ecosystems. Unlike prior surveys that broadly cover LLM agents or tool use, this work focuses specifically on the emerging skill abstraction layer and its implications for the next generation of agentic systems. Project repo: https://github.com/scienceaix/agentskills.

</details>


### [3] [Theory of Mind Guided Strategy Adaptation for Zero-Shot Coordination](https://arxiv.org/abs/2602.12458)
*Andrew Ni,Simon Stepputtis,Stefanos Nikolaidis,Michael Lewis,Katia P. Sycara,Woojun Kim*

Main category: cs.MA

TL;DR: 本文针对多智能体强化学习中智能体零样本适应未知队友的挑战，提出基于心智理论的自适应集成智能体方法，优于单策略基线。


<details>
  <summary>Details</summary>
Motivation: 现有两阶段方法导致智能体策略过于静态化而无法充分适应队友，限制了协同性能的提升。

Method: 利用心智理论推断队友意图，然后从策略集成中选择最佳政策，并在Overcooked环境中测试全可见及部分可见场景。

Result: 实证结果表明，该方法在性能上超越单策略基准，实现更高的协同 Eduardo效果。

Conclusion: 该方法有效解决了智能体自适应不足的问题，通过针对性策略选择提升了协调效率。

Abstract: A central challenge in multi-agent reinforcement learning is enabling agents to adapt to previously unseen teammates in a zero-shot fashion. Prior work in zero-shot coordination often follows a two-stage process, first generating a diverse training pool of partner agents, and then training a best-response agent to collaborate effectively with the entire training pool. While many previous works have achieved strong performance by devising better ways to diversify the partner agent pool, there has been less emphasis on how to leverage this pool to build an adaptive agent. One limitation is that the best-response agent may converge to a static, generalist policy that performs reasonably well across diverse teammates, rather than learning a more adaptive, specialist policy that can better adapt to teammates and achieve higher synergy. To address this, we propose an adaptive ensemble agent that uses Theory-of-Mind-based best-response selection to first infer its teammate's intentions and then select the most suitable policy from a policy ensemble. We conduct experiments in the Overcooked environment to evaluate zero-shot coordination performance under both fully and partially observable settings. The empirical results demonstrate the superiority of our method over a single best-response baseline.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [4] [Distance-based certification for leader election in meshed graphs and local recognition of their subclasses](https://arxiv.org/abs/2602.12894)
*Jérémie Chalopin,Victor Chepoi,Maria Kokkou*

Main category: cs.DC

TL;DR: 本文提出了一个2-本地证明标签方案用于匿名网格图的领导者选举，标签集为{0,1,2}，并提供了3-本地方案用于识别网格图的子类，标签大小为O(log D)。


<details>
  <summary>Details</summary>
Motivation: 网格图包含多种重要图类，如中位图和弦图，在图论和离散数学中广泛应用，旨在开发高效的局部验证方法以支持分布式算法。

Method: 通过局部验证顶点到根节点的距离(如d(s,v))并利用三角方复形的连通性实现整体鉴别；为领导者选举，额外检查距离模3以保证领导者指定。

Result: 领导者选举方案使用大小为3的标签(2-本地操作)，子类识别方案使用3-本地操作和O(log D)标签大小(其中D为图直径)。

Conclusion: 该工作证实了在网格图中可实现标签大小小且局部高效的证明标签方案，为分布式图算法提供了新工具。

Abstract: In this paper, we present a 2-local proof labeling scheme with labels in $\{ 0,1,2\}$ for leader election in anonymous meshed graphs. Meshed graphs form a general class of graphs defined by a distance condition. They comprise several important classes of graphs, which have long been the subject of intensive studies in metric graph theory, geometric group theory, and discrete mathematics: median graphs, bridged graphs, chordal graphs, Helly graphs, dual polar graphs, modular, weakly modular graphs, and basis graphs of matroids. We also provide 3-local proof labeling schemes to recognize these subclasses of meshed graphs using labels of size $O(\log D)$ (where $D$ is the diameter of the graph).
  To establish these results, we show that in meshed graphs, we can verify locally that every vertex $v$ is labeled by its distance $d(s,v)$ to an arbitrary root $s$. To design proof labeling schemes to recognize the subclasses of meshed graphs mentioned above, we use this distance verification to ensure that the triangle-square complex of the graph is simply connected and we then rely on existing local-to-global characterizations for the different classes we consider.
  To get a proof-labeling scheme for leader election with labels of constant size, we then show that we can check locally if every $v$ is labeled by $d(s,v) \pmod{3}$ for some root $s$ that we designate as the leader.

</details>


### [5] [Bloom Filter Look-Up Tables for Private and Secure Distributed Databases in Web3 (Revised Version)](https://arxiv.org/abs/2602.13167)
*Shlomi Dolev,Ehud Gudes,Daniel Shlomo*

Main category: cs.DC

TL;DR: 本文提出一种基于 BFLUT 算法的去中心化数据库方案，用于安全私有密钥管理，结合 OrbitDB、IPFS 和 IPNS 技术。


<details>
  <summary>Details</summary>
Motivation: 解决 Web3 去中心化系统中密钥管理风险，防止密钥在节点被攻击时泄露。

Method: 使用 BFLUT 算法编码并分布式存储密钥，避免直接暴露；集成 OrbitDB、IPFS 和 IPNS 实现数据一致性、可扩展性和更新支持。

Result: Result analysis unavailable

Conclusion: 方案为需要去中心化安全的 Web3 应用提供了可靠基础。

Abstract: The rapid growth of decentralized systems in theWeb3 ecosystem has introduced numerous challenges, particularly in ensuring data security, privacy, and scalability [3, 8]. These systems rely heavily on distributed architectures, requiring robust mechanisms to manage data and interactions among participants securely. One critical aspect of decentralized systems is key management, which is essential for encrypting files, securing database segments, and enabling private transactions. However, securely managing cryptographic keys in a distributed environment poses significant risks, especially when nodes in the network can be compromised [9]. This research proposes a decentralized database scheme specifically designed for secure and private key management. Our approach ensures that cryptographic keys are not stored explicitly at any location, preventing their discovery even if an attacker gains control of multiple nodes. Instead of traditional storage, keys are encoded and distributed using the BFLUT (Bloom Filter for Private Look-Up Tables) algorithm [7], which enables secure retrieval without direct exposure. The system leverages OrbitDB [4], IPFS [1], and IPNS [10] for decentralized data management, providing robust support for consistency, scalability, and simultaneous updates. By combining these technologies, our scheme enhances both security and privacy while maintaining high performance and reliability. Our findings demonstrate the system's capability to securely manage keys, prevent unauthorized access, and ensure privacy, making it a foundational solution for Web3 applications requiring decentralized security.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [6] [On Borrowed Time: Measurement-Informed Understanding of the NTP Pool's Robustness to Monopoly Attacks](https://arxiv.org/abs/2602.12321)
*Robert Beverly,Erik Rye*

Main category: cs.NI

TL;DR: 研究揭示NTP池协议的漏洞，显示大多服务器不独立且易受垄断攻击，提出改进方案。


<details>
  <summary>Details</summary>
Motivation: 评估NTP池的安全风险，因其分布式志愿者架构易成攻击目标，尤其为全球物联网设施提供时间同步。

Method:  anem九个月纵向测量，收集服务器配置、时效质量、流量负载等直接数据，涵盖活跃与非活跃服务器。

Result: 仅19.7%活动服务器完全独立 sous；仅需10个恶意服务器即可在90%国家垄断NPO池流量，暴露安全风险。

Conclusion: 结果突显漏洞，提议通过提升服务器独立性等方式增强NTP池健壮性。

Abstract: Internet services and applications depend critically on the availability and acc uracy of network time. The Network Time Protocol (NTP) is one of the oldest core network protocols and remains the de facto mechanism for clock synchronization across the Internet today. While multiple NTP infrastructures exist, one, the "NTP Pool," presents an attractive attack target for two basic reasons, it is: 1) administratively distributed and based on volunteer servers; and 2) heavily utilized, including by IoT and infrastructure devices worldwide. We %develop measurements to gather the first direct, non-inferential, and comprehensive data on the NTP pool, including: longitudinal server and account membership, server configurations, time quality, aliases, and global query traffic load.
  We gather complete and granular data over a nine month period to discover over 15k servers (both active and inactive) and shed new light into the NTP Pool's use, dynamics, and robustness. By analyzing address aliases, accounts, and network connectivity, we find that only 19.7% of the pool's active servers are fully independent. Finally, we show that an adversary informed with our data can better and more precisely mount "monopoly attacks" to capture the preponderance of NTP pool traffic in 90% of all countries with only 10 or fewer malicious NTP servers. Our results suggest multiple avenues by which the robustness of the pool can be improved.

</details>


### [7] [Tracking The Trackers: Commercial Surveillance Occurring on U.S. Army Networks](https://arxiv.org/abs/2602.12388)
*Alexander Master,Jaclyn Fox,Nicolas Starck,Maxwell Love,Benjamin Allison*

Main category: cs.NI

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Despite current security implementations, Internet activity on DoD networks is susceptible to web trackers and commercial data collection, which have the potential to expose information about service members and unit operations. This report documents the outcomes of a study to characterize web tracking occurring on Army CONUS unclassified networks. We derived a dataset from the Cloud-Based Internet Isolation (CBII) platform, encompassing data measured over a two-month period in 2024. This dataset comprised the 1,000 most frequently accessed Internet resources, determined by the number of connection requests on CONUS DoDIN-A during the study period. We then compared all domains and subdomains in the dataset against Ghostery's WhoTracks.me, an open-source database of commercial tracking entities. We found that over 21% of the domains accessed during the study period were Internet trackers. The ACI recommends that the Army implement changes to its enterprise networks to limit commercial Internet-based tracking, as well as policy changes towards the same end. With relatively minor configuration changes, CBII can serve as a more effective mitigation against risks posed by commercially available information.

</details>


### [8] [Adaptive Meta-Aggregation Federated Learning for Intrusion Detection in Heterogeneous Internet of Things](https://arxiv.org/abs/2602.12541)
*Saadat Izadi,Mahmood Ahmadi*

Main category: cs.NI

TL;DR: 本文提出AMAFed方法，通过联邦学习与自适应加权提升物联网入侵检测，在多个数据集上实现高准确率。


<details>
  <summary>Details</summary>
Motivation: 物联网快速扩张增加了安全漏洞，设备异构性使传统入侵检测系统面临挑战。

Method: 采用基于元学习的动态加权机制AMAFed，根据数据质量和贡献自适应分配本地模型权重，实现设备间的个性化协作学习。

Result: 在ToN-IoT准确率达99.8%，N-BaIoT达99.88%，BoT-IoT达98.12%，F1-score超过98%，优于现有方法。

Conclusion: AMAFed有效解决了物联网入侵检测中的异构问题，实现了卓越性能。

Abstract: The rapid proliferation of the Internet of Things (IoT) has brought remarkable advancements to industries by enabling interconnected systems and intelligent automation. However, this exponential growth has also introduced significant security vulnerabilities, making IoT networks increasingly targets for sophisticated cyberattacks. The heterogeneity of IoT devices poses critical challenges for traditional intrusion detection systems. To address these challenges, this paper proposes an innovative method called Adaptive Meta-Aggregation Federated Learning (AMAFed), designed to enhance intrusion detection in heterogeneous IoT networks. By employing a dynamic weighting mechanism using meta-learning, AMAFed assigns adaptive importance to local models based on their data quality and contributions, enabling personalized yet collaborative learning across devices. The proposed method was evaluated on three benchmark IoT datasets: ToN-IoT, N-BaIoT, and BoT-IoT, representing diverse real-world scenarios. Experimental results demonstrate that AMAFed achieves detection accuracy up to 99.8% on ToN-IoT, with F1-scores exceeding 98% across all datasets. On the N-BaIoT dataset, it reaches 99.88% accuracy, and on BoT-IoT, it achieves 98.12% accuracy, consistently outperforming state-of-the-art approaches.

</details>


### [9] [CF-HFC:Calibrated Federated based Hardware-aware Fuzzy Clustering for Intrusion Detection in Heterogeneous IoTs](https://arxiv.org/abs/2602.12557)
*Saadat Izadi,Mahmood Ahmadi*

Main category: cs.NI

TL;DR: 该论文提出CF-HFC方法，结合硬件感知聚类、模糊聚合和自适应校准技术，显著提升异构物联网中入侵检测的准确性与效率。


<details>
  <summary>Details</summary>
Motivation: 异构物联网设备易受攻击且资源受限，联邦学习面临设备/数据异构性导致的延迟、收敛不稳定及误报失衡问题。

Method: 采用三层边缘-雾-云架构：1)硬件感知模糊聚类按算力分组客户端；2)Fuzzy-FedProx聚合处理非IID数据；3)自适应校准动态平衡误报率。

Result: 在四个数据集上验证：准确率超99%，收敛速度提升30%，通信延迟降低25%，全面优于FedAvg等基线方法。

Conclusion: CF-HFC高效解决设备/数据异构性问题，为物联网提供高精度低延迟的入侵检测方案。

Abstract: The rapid expansion of heterogeneous Internet of Things (IoT) environments has heightened security risks, as resource-constrained devices remain vulnerable to diverse cyberattacks. Federated Learning (FL) has emerged as a privacy-preserving paradigm for collaborative intrusion detection; however, device and data heterogeneity introduce major challenges, including straggler delays, unstable convergence, and unbalanced error rates. This paper presents a Calibrated Federated Learning method with Hardware-aware Fuzzy Clustering (CF-HFC) to enhance intrusion detection performance in heterogeneous IoT networks. The proposed three-tier Edge-Fog-Cloud architecture integrates three complementary components: (1) hardware-aware fuzzy clustering, which organizes clients by computational capacity to mitigate straggler effects; (2) Fuzzy-FedProx aggregation, which stabilizes optimization under non-IID data distributions; and (3) Adaptive Conformal Calibration (ACC), which dynamically adjusts decision thresholds to balance false negative and false positive rates. Extensive experiments on ToN-IoT, BoT-IoT, Edge-IIoTset, and CICDDoS2019 datasets demonstrate that CF-HFC outperforms baseline methods such as FedAvg and FedProx, achieving over 99% detection accuracy, faster convergence, and lower communication latency. Overall, the results verify that CF-HFC effectively mitigates both device- and data-level heterogeneity, compared to existing federated learning approaches, providing accurate and efficient intrusion detection across Heterogeneous IoTs environment.

</details>


### [10] [Artic: AI-oriented Real-time Communication for MLLM Video Assistant](https://arxiv.org/abs/2602.12641)
*Jiangkai Wu,Zhiyuan Ren,Junquan Zhong,Liming Liu,Xinggong Zhang*

Main category: cs.NI

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: AI Video Assistant emerges as a new paradigm for Real-time Communication (RTC), where one peer is a Multimodal Large Language Model (MLLM) deployed in the cloud. This makes interaction between humans and AI more intuitive, akin to chatting with a real person. However, a fundamental mismatch exists between current RTC frameworks and AI Video Assistants, stemming from the drastic shift in Quality of Experience (QoE) and more challenging networks. Measurements on our production prototype also confirm that current RTC fails, causing latency spikes and accuracy drops.
  To address these challenges, we propose Artic, an AI-oriented RTC framework for MLLM Video Assistants, exploring the shift from "humans watching video" to "AI understanding video." Specifically, Artic proposes: (1) Response Capability-aware Adaptive Bitrate, which utilizes MLLM accuracy saturation to proactively cap bitrate, reserving bandwidth headroom to absorb future fluctuations for latency reduction; (2) Zero-overhead Context-aware Streaming, which allocates limited bitrate to regions most important for the response, maintaining accuracy even under ultra-low bitrates; and (3) Degraded Video Understanding Benchmark, the first benchmark evaluating how RTC-induced video degradation affects MLLM accuracy. Prototype experiments using real-world uplink traces show that compared with existing methods, Artic significantly improves accuracy by 15.12% and reduces latency by 135.31 ms. We will release the benchmark and codes at https://github.com/pku-netvideo/DeViBench.

</details>


### [11] [PEMI: Transparent Performance Enhancements for QUIC](https://arxiv.org/abs/2602.12732)
*Jie Zhang,Lei Zhang,Ziyi Wang,Chenxiang Sun,Yuming Hu,Xiaohui Xie,Zeqi Lai,Yong Cui*

Main category: cs.NI

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: QUIC, as the transport layer of the next-generation Web stack (HTTP/3), natively provides security and performance improvements over TCP-based stacks. However, since QUIC provides end-to-end encryption for both data and packet headers, in-network assistance like Performance-Enhancing Proxy (PEP) is unavailable for QUIC. To achieve the similar optimization as TCP, some works seek to collaborate endpoints and middleboxes to provide in-network assistance for QUIC. But involving both host and in-network devices increases the difficulty of deployment in the Internet.
  In this paper, by analyzing the QUIC standard, implementations, and the locality of application traffic, we identify opportunities for transparent middleboxes to measure RTT and infer packet loss for QUIC connections, despite the absence of plaintext ACK information. We then propose PEMI as a concrete system that continuously measures RTT and infers lost packets, enabling fast retransmissions for QUIC. PEMI enables performance enhancement for QUIC in a completely transparent manner, without requiring any explicit cooperation from the endpoints. To keep fairness, PEMI employs a delay-based congestion control and utilizes feedback-based methods to enforce CWND. Extensive evaluation results, including Mininet and trace-driven dynamic experiments, show that PEMI can significantly improve the performance of QUIC. For example, in the Mininet experiments, PEMI increases the goodput of file transfers by up to 2.5$\times$, and reduces the 90th percentile jitter of RTC frames by 20-75%.

</details>


### [12] [Chimera: Neuro-Symbolic Attention Primitives for Trustworthy Dataplane Intelligence](https://arxiv.org/abs/2602.12851)
*Rong Fu,Wenxin Zhang,Xiaowen Ma,Kun Liu,Wangyu Wu,Ziyu Kong,Jia Yee Tan,Tailong Luo,Xianda Li,Zeli Su,Youjin Wang,Yongtai Liu,Simon Fong*

Main category: cs.NI

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Deploying expressive learning models directly on programmable dataplanes promises line-rate, low-latency traffic analysis but remains hindered by strict hardware constraints and the need for predictable, auditable behavior. Chimera introduces a principled framework that maps attention-oriented neural computations and symbolic constraints onto dataplane primitives, enabling trustworthy inference within the match-action pipeline. Chimera combines a kernelized, linearized attention approximation with a two-layer key-selection hierarchy and a cascade fusion mechanism that enforces hard symbolic guarantees while preserving neural expressivity. The design includes a hardware-aware mapping protocol and a two-timescale update scheme that together permit stable, line-rate operation under realistic dataplane budgets. The paper presents the Chimera architecture, a hardware mapping strategy, and empirical evidence showing that neuro-symbolic attention primitives can achieve high-fidelity inference within the resource envelope of commodity programmable switches.

</details>


### [13] [TENORAN: Automating Fine-grained Energy Efficiency Profiling in Open RAN Systems](https://arxiv.org/abs/2602.13085)
*Ravis Shirkhani,Stefano Maxenti,Leonardo Bonati,Niloofar Mohamadi,Maxime Elkael,Umair Hashmi,Jeebak Mitra,Michele Polese,Tommaso Melodia,Salvatore D'Oro*

Main category: cs.NI

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: The transition to disaggregated and interoperable Open Radio Access Network (RAN) architectures and the introduction of RAN Intelligent Controllers (RICs) in O-RAN creates new resource optimization opportunities and fine-grained tuning and configuration of network components to save energy while fulfilling service demand. However, unlocking this potential requires fine-grained and accurate energy measurements across heterogeneous deployments. Three factors make this particularly challenging [...]. To address these challenges, we design the TENORAN framework, an automated measurement scaffold for fine-grained energy efficiency profiling of O-RAN deployments, and prototype it on a heterogeneous OpenShift cluster. TENORAN instruments an end-to-end deployment based on high-level specifications (e.g., gNB software stack and split options, traffic profiles), and collects synchronized performance metrics and power measurements for individual RAN components while the network is under controlled workloads including over-the-air traffic. Our experimental results demonstrate energy profiling of end-to-end experiments with xApps in the loop, energy efficiency differences between two RAN stacks, OpenAirInterface and srsRAN, in uplink and downlink, and core network power consumption trends.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [14] [Design Environment of Quantization-Aware Edge AI Hardware for Few-Shot Learning](https://arxiv.org/abs/2602.12295)
*R. Kanda,N. Onizawa,M. Leonardon,V. Gripon,T. Hanyu*

Main category: cs.AR

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: This study aims to ensure consistency in accuracy throughout the entire design flow in the implementation of edge AI hardware for few-shot learning, by implementing fixed-point data processing in the pre-training and evaluation phases. Specifically, the quantization module, called Brevitas, is applied to implement fixed-point data processing, which allows for arbitrary specification of the bit widths for the integer and fractional parts. Two methods of fixed-point data quantization, quantization-aware training (QAT) and post-training quantization (PTQ), are utilized in Brevitas. With Tensil, which is used in the current design flow, the bit widths of the integer and fractional parts need to be 8 bits each or 16 bits each when implemented in hardware, but performance validation has shown that accuracy comparable to floating-point operations can be maintained even with 6 bits or 5 bits each, indicating potential for further reduction in computational resources. These results clearly contribute to the creation of a versatile design and evaluation environment for edge AI hardware for few-shot learning.

</details>


### [15] [CacheMind: From Miss Rates to Why -- Natural-Language, Trace-Grounded Reasoning for Cache Replacement](https://arxiv.org/abs/2602.12422)
*Kaushal Mhapsekar,Azam Ghanbari,Bita Aslrousta,Samira Mirbagher-Ajorpaz*

Main category: cs.AR

TL;DR: 论文介绍CacheMind，一种基于检索增强生成（RAG）和大型语言模型（LLM）的工具，支持roud自然语言查询CPU缓存追踪数据semantic分析，从而改进缓存性能，并通过CacheMindBench评估其有效性。


<details>
  <summary>Details</summary>
Motivation: 缓存替换问题常依赖手动启发规则，分析过程缓慢且非交互式，限制了缓存效率；因此，需开发工具实现语义推理以提高交互性。

Method: 引入CacheMind工具，结合 전문 RAG和LL executor模型，允许用户通过自然语言提问进行缓存跟踪推理；测评使用CacheMindBench基准，对比SIEVE和RANGER等检索器性能。

Result: 评测中，对75个未见跟踪问题，CacheMind准确率达66.67%（SIEVE）或89.33%（RANGER）；对25个策略推理任务，达84.80%（SIEVE）或64.80%（RANGER）。实际应用中，如bypassing用例提升命中率7.66%，速度提升高达76%，显示明显优势。

Conclusion: Conclusion extraction failed

Abstract: Cache replacement remains a challenging problem in CPU microarchitecture, often addressed using hand-crafted heuristics, limiting cache performance. Cache data analysis requires parsing millions of trace entries with manual filtering, making the process slow and non-interactive. To address this, we introduce CacheMind, a conversational tool that uses Retrieval-Augmented Generation (RAG) and Large Language Models (LLMs) to enable semantic reasoning over cache traces. Architects can now ask natural language questions like, "Why is the memory access associated with PC X causing more evictions?", and receive trace-grounded, human-readable answers linked to program semantics for the first time. To evaluate CacheMind, we present CacheMindBench, the first verified benchmark suite for LLM-based reasoning for the cache replacement problem. Using the SIEVE retriever, CacheMind achieves 66.67% on 75 unseen trace-grounded questions and 84.80% on 25 unseen policy-specific reasoning tasks; with RANGER, it achieves 89.33% and 64.80% on the same evaluations. Additionally, with RANGER, CacheMind achieves 100% accuracy on 4 out of 6 categories in the trace-grounded tier of CacheMindBench. Compared to LlamaIndex (10% retrieval success), SIEVE achieves 60% and RANGER achieves 90%, demonstrating that existing Retrieval-Augmented Generation (RAGs) are insufficient for precise, trace-grounded microarchitectural reasoning. We provided four concrete actionable insights derived using CacheMind, wherein bypassing use case improved cache hit rate by 7.66% and speedup by 2.04%, software fix use case gives speedup of 76%, and Mockingjay replacement policy use case gives speedup of 0.7%; showing the utility of CacheMind on non-trivial queries that require a natural-language interface.

</details>


### [16] [MXFormer: A Microscaling Floating-Point Charge-Trap Transistor Compute-in-Memory Transformer Accelerator](https://arxiv.org/abs/2602.12480)
*George Karfakis,Samyak Chakrabarty,Vinod Kurian Jacob,Siyun Qiao,Subramanian S. Iyer,Sudhakar Pamarti,Puneet Gupta*

Main category: cs.AR

TL;DR: MXFormer是一种新型混合计算内存储存加速器，通过微观缩放MXFP4电荷陷阱晶体管阵列实现全权静态存储，针对大型Transformer模型提供高吞吐量和高效率推理。


<details>
  <summary>Details</summary>
Motivation: Transformer模型部署因过高计算和内存带宽需求受限，需提升推理效率和性能。

Method: 采用静态分区设计和深度流水线数据流，利用模拟CTT阵列处理静态权重层（含MXFP4精度对流和SAR ADC），数字块执行动态计算（如点积注意力）。

Result: ViT-L/32模型处理速率达58275 FPS（双芯片），ViT-B/16为41269 FPS（单芯片），计算密度超越现有非全权静态加速器3.3-60.5倍，能源效率高1.7-2.5倍，全权静态下密度提升20.9倍且存储密度翻倍，精度损失<1%。

Conclusion: MXFormer在不重新训练模型的前提下实现高性能和高效率的Transformer推理，维持接近数字精度。

Abstract: The proliferation of Transformer models is often constrained by the significant computational and memory bandwidth demands of deployment. To address this, we present MXFormer, a novel, hybrid, weight-stationary Compute-in-Memory (CIM) accelerator that provides high throughput and efficiency for fixed-model inference on large short-sequence Transformers. Our architecture's foundation is the use of ultra-dense Charge-Trap Transistors (CTTs) in Microscaling MXFP4 CIM arrays, uniquely enabling the on-chip storage of up to hundreds of millions of parameters in Fully Weight Stationary (FWS) fashion.
  We introduce a statically partitioned design with 12 Transformer blocks connected by a deeply pipelined dataflow. Static-weight layers (MLPs and linear projections) execute on highly parallel analog CTT arrays using an MXFP4-native flow with per-block exponent alignment and a 10-bit SAR ADC. Dynamic computations are handled in fully accurate digital blocks that utilize MXFP-enabled systolic arrays for scaled dot-product attention and vector units for LayerNorm and FlashAttention-style Softmax.
  By eliminating all weight movement, the deeply pipelined MXFormer architecture yields very high single-stream throughput and efficiency, processing 58275 FPS on ViT-L/32 (dual-chip) or 41269 FPS on ViT-B/16 (single chip). MXFormer outperforms comparable state-of-the-art non-FWS digital, hybrid and photonic Transformer accelerators ~3.3x-60.5x in compute density and ~1.7x-2.5x in energy efficiency. Against FWS accelerators, MXFormer improves compute density by ~20.9x and resident weight storage density by ~2x, while preserving near-digital accuracy (drop of <1%) without any model retraining.

</details>


### [17] [TriGen: NPU Architecture for End-to-End Acceleration of Large Language Models based on SW-HW Co-Design](https://arxiv.org/abs/2602.12962)
*Jonghun Lee,Junghoon Lee,Hyeonjin Kim,Seoho Jeon,Jisup Yoon,Hyunbin Park,Meejeong Park,Heonjae Ha*

Main category: cs.AR

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Recent studies have extensively explored NPU architectures for accelerating AI inference in on-device environments, which are inherently resource-constrained. Meanwhile, transformer-based large language models (LLMs) have become dominant, with rapidly increasing model sizes but low degree of parameter reuse compared to conventional CNNs, making end-to-end execution on resource-limited devices extremely challenging. To address these challenges, we propose TriGen, a novel NPU architecture tailored for resource-constrained environments through software-hardware co-design. Firstly, TriGen adopts low-precision computation using microscaling (MX) to enable additional optimization opportunities while preserving accuracy, and resolves the issues that arise by employing such precision. Secondly, to jointly optimize both nonlinear and linear operations, TriGen eliminates the need for specialized hardware for essential nonlinear operations by using fast and accurate LUT, thereby maximizing performance gains and reducing hardware-cost in on-device environments, and finally, by taking practical hardware constraints into account, further employs scheduling techniques to maximize computational utilization even under limited on-chip memory capacity. We evaluate the performance of TriGen on various LLMs and show that TriGen achieves an average 2.73x performance speedup and 52% less memory transfer over the baseline NPU design with negligible accuracy loss.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [18] [SHAPR: A Solo Human-Centred and AI-Assisted Practice Framework for Research Software Development](https://arxiv.org/abs/2602.12443)
*Ka Ching Chan*

Main category: cs.SE

TL;DR: 本文提出SHAPR框架作为实践级操作工具，为单人研究员在AI辅助研究软件开发中提供可行动指南，补充动作设计研究方法，并支持人类问责和学习机制。


<details>
  <summary>Details</summary>
Motivation: 动机是动作设计研究在单人AI辅助软件开bull发中缺乏日常操作指导，生成AI技术带来问责、反思和方法论严谨性挑战。

Method: 方法为提出SHAPR框架作为概念设计工件，通过反思ève其内部一致性、对齐ADR原则和实际适用性，进行形成性评估分析。

Result: 结果SHAPR阐明角色、工件、反射实践和轻量治理机制，支持动作设计研究的构建-干预-评估循环实施，确保AI辅助开发中人类问责和持续学习。

Conclusion: 结论SHAPR框架联系研究软件开发、人-AI协作和反思学习，推动知识生产和高级学位研究员培训的 broader讨论贡献。

Abstract: Research software has become a central vehicle for inquiry and learning in many Higher Degree Research (HDR) contexts, where solo researchers increasingly develop software-based artefacts as part of their research methodology. At the same time, generative artificial intelligence is reshaping development practice, offering powerful forms of assistance while introducing new challenges for accountability, reflection, and methodological rigour. Although Action Design Research (ADR) provides a well-established foundation for studying and constructing socio-technical artefacts, it offers limited guidance on how its principles can be operationalised in the day-to-day practice of solo, AI-assisted research software development. This paper proposes the SHAPR framework (Solo, Human-centred, AI-assisted PRactice) as a practice-level operational framework that complements ADR by translating its high-level principles into actionable guidance for contemporary research contexts. SHAPR supports the enactment of ADR Building-Intervention-Evaluation cycles by making explicit the roles, artefacts, reflective practices, and lightweight governance mechanisms required to sustain human accountability and learning in AI-assisted development. The contribution of the paper is conceptual: SHAPR itself is treated as the primary design artefact and unit of analysis and is evaluated formatively through reflective analysis of its internal coherence, alignment with ADR principles, and applicability to solo research practice. By explicitly linking research software development, Human-AI collaboration, and reflective learning, this study contributes to broader discussions on how SHAPR can support both knowledge production and HDR researcher training.

</details>


### [19] [FuncDroid: Towards Inter-Functional Flows for Comprehensive Mobile App GUI Testing](https://arxiv.org/abs/2602.12834)
*Jinlong He,Changwei Xia,Binru Huang,Jiwei Yan,Jun Yan,Jian Zhang*

Main category: cs.SE

TL;DR: 本文提出一种基于功能流程图（FFG）的新型GUI测试方法FuncDroid，用于检测移动应用中功能间交互导致的深层错误。该方法通过长短视图结合的自适应测试过程，显著提升了覆盖率和错误检测能力。


<details>
  <summary>Details</summary>
Motivation: 现有功能导向GUI测试方法忽视功能间交互，无法发现深层错误，导致应用可靠性不足。移动应用日渐复杂，迭代加速，亟需有效解决方案。

Method: 设计功能流程图（FFG）捕获功能单元及其交互；引入长短视图结合的测试过程，自适应优化功能边界并探索触发条件下的功能间流程；最终实现为工具FuncDroid。

Result: 在50个可复现崩溃错误的基准和52个商业应用中测试：FuncDroid覆盖率和错误检测数分别提升28%和107%，优于现有方法；并发现18个商业应用中新的非崩溃功能性错误。

Conclusion: FuncDroid证实了其在检测深层功能错误方面的实际效能，为复杂移动应用测试提供了高效手段。

Abstract: As mobile application (app) functionalities grow increasingly complex and their iterations accelerate, ensuring high reliability presents significant challenges. While functionality-oriented GUI testing has attracted growing research attention, existing approaches largely overlook interactions across functionalities, making them ineffective at uncovering deep bugs hidden in inter-functional behaviors. To fill this gap, we first design a Functional Flow Graph (FFG), a behavioral model that explicitly captures an app's functional units and their inter-functional interactions. Based on the FFG, we further introduce an inter-functional-flow-oriented GUI testing approach with the dual goals of precise model construction and deep bug detection. This approach is realized through a long-short-term-view-guided testing process. By combining two complementary test-generation views, it can adaptively refine functional boundaries and systematically explore inter-functional flows under diverse triggering conditions. We implement our approach in a tool called FuncDroid, and evaluate it on two benchmarks: (1) a widely-used open-source benchmark with 50 reproducible crash bugs and (2) a diverse set of 52 popular commercial apps. Experimental results demonstrate that FuncDroid significantly outperforms state-of-the-art baselines in both coverage (+28%) and bug detection number (+107%). Moreover, FuncDroid successfully uncovers 18 previously unknown non-crash functional bugs in commercial apps, confirming its practical effectiveness.

</details>


### [20] [A Microservice-Based Platform for Sustainable and Intelligent SLO Fulfilment and Service Management](https://arxiv.org/abs/2602.12875)
*Juan Luis Herrera,Daniel Wang,Schahram Dustdar*

Main category: cs.SE

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: The Microservices Architecture (MSA) design pattern has become a staple for modern applications, allowing functionalities to be divided across fine-grained microservices, fostering reusability, distribution, and interoperability. As MSA-based applications are deployed to the Computing Continuum (CC), meeting their Service Level Objectives (SLOs) becomes a challenge. Trading off performance and sustainability SLOs is especially challenging. This challenge can be addressed with intelligent decision systems, able to reconfigure the services during runtime to meet the SLOs. However, developing these agents while adhering to the MSA pattern is complex, especially because CC providers, who have key know-how and information to fulfill these SLOs, must comply with the privacy requirements of application developers. This work presents the Carbon-Aware SLO and Control plAtform (CASCA), an open-source MSA-based platform that allows CC providers to reconfigure services and fulfill their SLOs while maintaining the privacy of developers. CASCA is architected to be highly reusable, distributable, and easy to use, extend, and modify. CASCA has been evaluated in a real CC testbed for a media streaming service, where decision systems implemented in Bash, Rust, and Python successfully reconfigured the service, unaffected by upholding privacy.

</details>


### [21] [The Influence of Code Smells in Efferent Neighbors on Class Stability](https://arxiv.org/abs/2602.12950)
*Zushuai Zhang,Elliott Wen,Ewan Tempero*

Main category: cs.SE

TL;DR: 本研究通过分析100个GitHub高星项目数据，调查efferent邻居（依赖类）中的代码异味及其相互关系如何影响软件类的稳定性。


<details>
  <summary>Details</summary>
Motivation: 代码不稳定性会增加软件维护成本，但先前研究仅关注类内部异味，忽视因efferent邻居修改引发的连锁效应及异味互关与互动的作用，这可能导致质量下降风险加剧。

Method: 挖掘一年提交历史，检测代码异味和静态依赖关系，确定异味互关（smell interrelation）和互动（smell interaction），并将cial这些因素建模为类稳定性的预测指标。

Result: 摘要中未提供具体结果，但研究旨在量化异味互关与互动对稳定性的影响。

Conclusion: 本研究填补了代码异味互关和互动对稳定性影响的研究空白，为提升软件维护效率提供新视角。

Abstract: Understanding what drives code instability is essential for effective software maintenance, as unstable classes require larger or more frequent edits and increase the risk of unintended side effects. Although code smells are widely believed to harm maintainability, most prior stability studies examine only the smells within the class being modified. In practice, however, classes can change because their efferent neighbors (i.e., the classes they depend on) are modified due to ripple effects that propagate along static dependencies, even if the class itself is clean. Such ripple effects may be more severe when the efferent neighbor exhibits code smells. In addition, code smells rarely occur alone. They often appear together within a class or across classes connected by static dependencies, a phenomenon known as code smell interrelation. Such interrelation can lead to code smell interaction, where smells are directly connected through static dependencies and may further compound maintainability issues. However, the effect of code smell interrelation and interaction on code quality remains largely underexplored. Therefore, this study investigates whether the presence of code smells in a class's efferent neighbors affects its stability, considering the factor of code smell interrelation and interaction. To achieve this, we mine one year of commit history from 100 top-starred GitHub projects, detect code smells and static dependencies, determine code smell interrelation and interaction, and model these factors as predictors of class stability.

</details>


### [22] [Analysis of Asset Administration Shell-based Negotiation Processes for Scaling Applications](https://arxiv.org/abs/2602.13029)
*David Dietrich,Armin Lechler,Alexander Verl*

Main category: cs.SE

TL;DR: 研究主动资产壳在高规模工业应用中的协商效率，揭示扩展时的性能限制与通信开销。


<details>
  <summary>Details</summary>
Motivation: 现有主动资产壳研究局限于小规模资产，工业场景下的可扩展性未知，而标准聚焦子模型安全而非主动行为。

Method: 设计评估标准，构建可扩展主动资产壳架构，通过变资产数量实验分析消息负载等性能指标。

Result: 资产规模扩大导致协商性能下降，通信开销显著增加，但机制展现部分适应性。

Conclusion: 结果为优化资产壳的开发和标准化提供关键性能参考。

Abstract: The proactive Asset Administration Shell (AAS) enables bidirectional communication between assets. It uses the Language for I4.0 Components in VDI/VDE 2193 to facilitate negotiations, such as allocating products to available production resources. This paper investigates the efficiency of the negotiation, based on criteria, such as message load, for applications with a scaling number of assets. Currently, the focus of AAS standardization is on submodels and their security to enable interoperable data access. Their proactive behavior remains conceptual and is still a subject of scientific research. Existing studies examine proactive AAS architecture examples with a limited number of assets, raising questions about their scalability in industrial environments. To analyze proactive AAS for scaling applications, a scenario and evaluation criteria are introduced. A scalable implementation is developed using current architectures for proactive AAS, upon which experiments are conducted with a varying number of assets. The results reveal the performance limitations, communication overhead, and adaptability of the AAS-based negotiation mechanism scaling. This information can improve the further development and standardization of the AAS.

</details>


### [23] [Automated Testing of Task-based Chatbots: How Far Are We?](https://arxiv.org/abs/2602.13072)
*Diego Clerissi,Elena Masserini,Daniela Micucci,Leonardo Mariani*

Main category: cs.SE

TL;DR: 论文评估了业界领先的任务型聊天机器人测试技术，通过GitHub精选案例研究其现有局限性。


<details>
  <summary>Details</summary>
Motivation: 随着聊天机器人普及，质量评估至关重要。传统测试方法无法系统检验对话空间，新兴技术虽进步但仍存在生成场景简单化和验证机制薄弱等局限性。

Method: 通过精选GitHub上主流商业与开源平台开发的任务型聊天机器人，进行验证性研究评估最先进测试技术的有效性。

Result: 未在摘要中具体说明

Conclusion: 该研究旨在揭示当前测试技术的不足，为改进任务型聊天机器人的质量评估框架提供实证基础。

Abstract: Task-based chatbots are software, typically embedded in real-world applications, that assist users in completing tasks through a conversational interface. As chatbots are gaining popularity, effectively assessing their quality has become crucial. Whereas traditional testing techniques fail to systematically exercise the conversational space of chatbots, several approaches specifically targeting chatbots have emerged from both industry and research. Although these techniques have shown advancements over the years, they still exhibit limitations, such as simplicity of the generated test scenarios and weakness in implemented oracles. In this paper, we conduct a confirmatory study to investigate such limitations by evaluating the effectiveness of state-of-the-art chatbot testing techniques on a curated selection of task-based chatbots from GitHub, developed using the most popular commercial and open-source platforms.

</details>


### [24] [Source Code Hotspots: A Diagnostic Method for Quality Issues](https://arxiv.org/abs/2602.13170)
*Saleha Muzammil,Mughees Ur Rehman,Zoe Kotti,Diomidis Spinellis*

Main category: cs.SE

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Software source code often harbours "hotspots": small portions of the code that change far more often than the rest of the project and thus concentrate maintenance activity. We mine the complete version histories of 91 evolving, actively developed GitHub repositories and identify 15 recurring line-level hotspot patterns that explain why these hotspots emerge. The three most prevalent patterns are Pinned Version Bump (26%), revealing brittle release practices; Long Line Change (17%), signalling deficient layout; and Formatting Ping-Pong (9%), indicating missing or inconsistent style automation. Surprisingly, automated accounts generate 74% of all hotspot edits, suggesting that bot activity is a dominant but largely avoidable source of noise in change histories. By mapping each pattern to concrete refactoring guidelines and continuous integration checks, our taxonomy equips practitioners with actionable steps to curb hotspots and systematically improve software quality in terms of configurability, stability, and changeability.

</details>
