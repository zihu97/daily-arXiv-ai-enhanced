{"id": "2602.20826", "categories": ["cs.OS"], "pdf": "https://arxiv.org/pdf/2602.20826", "abs": "https://arxiv.org/abs/2602.20826", "authors": ["Yuanhai Zhang", "Songyang He", "Ruizhe Gou", "Mingyue Cui", "Boyang Li", "Shuai Zhao", "Kai Huang"], "title": "Exploiting Dependency and Parallelism: Real-Time Scheduling and Analysis for GPU Tasks", "comment": null, "summary": "With the rapid advancement of Artificial Intelligence, the Graphics Processing Unit (GPU) has become increasingly essential across a growing number of safety-critical application domains. Applying a GPU is indispensable for parallel computing; however, the complex data dependencies and resource contention across kernels within a GPU task may unpredictably delay its execution time. To address these problems, this paper presents a scheduling and analysis method for Directed Acyclic Graph (DAG)-structured GPU tasks. Given a DAG representation, the proposed scheduling scales the kernel-level parallelism and establishes inter-kernel dependencies to provide a reduced and predictable DAG response time. The corresponding timing analysis yields a safe yet nonpessimistic makespan bound without any assumption on kernel priorities. The proposed method is implemented using the standard CUDA API, requiring no additional software or hardware support. Experimental results under synthetic and real-world benchmarks demonstrate that the proposed approach effectively reduces the worst-case makespan and measured task execution time compared to the existing methods up to 32.8% and 21.3%, respectively.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9488\u5bf9DAG\u7ed3\u6784GPU\u4efb\u52a1\u7684\u53ef\u9884\u6d4b\u8c03\u5ea6\u4e0e\u5206\u6790\u65b9\u6cd5\uff0c\u901a\u8fc7\u6269\u5c55\u5185\u6838\u7ea7\u5e76\u884c\u6027\u548c\u5efa\u7acb\u5185\u6838\u95f4\u4f9d\u8d56\u5173\u7cfb\uff0c\u5728\u4e0d\u5047\u8bbe\u5185\u6838\u4f18\u5148\u7ea7\u7684\u60c5\u51b5\u4e0b\uff0c\u5b9e\u73b0\u4e86\u66f4\u5b89\u5168\u4e14\u975e\u60b2\u89c2\u7684makespan\u8fb9\u754c\u3002\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u6bd4\u73b0\u6709\u65b9\u6cd5\u6700\u591a\u53ef\u51cf\u5c1132.8%\u7684\u6700\u574f\u60c5\u51b5makespan\u548c21.3%\u7684\u5b9e\u9645\u4efb\u52a1\u6267\u884c\u65f6\u95f4\u3002", "motivation": "\u968f\u7740\u4eba\u5de5\u667a\u80fd\u7684\u5feb\u901f\u53d1\u5c55\uff0cGPU\u5728\u5b89\u5168\u5173\u952e\u5e94\u7528\u9886\u57df\u53d8\u5f97\u8d8a\u6765\u8d8a\u91cd\u8981\uff0c\u4f46\u5176\u590d\u6742\u7684\u6570\u636e\u4f9d\u8d56\u6027\u548c\u8d44\u6e90\u7ade\u4e89\u53ef\u80fd\u5bfc\u81f4\u6267\u884c\u65f6\u95f4\u4e0d\u53ef\u9884\u6d4b\uff0c\u8fd9\u9650\u5236\u4e86\u5176\u5728\u9700\u8981\u53ef\u9884\u6d4b\u6267\u884c\u65f6\u95f4\u7684\u5b89\u5168\u5173\u952e\u7cfb\u7edf\u4e2d\u7684\u5e94\u7528\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u9488\u5bf9\u6709\u5411\u65e0\u73af\u56fe(DAG)\u7ed3\u6784GPU\u4efb\u52a1\u7684\u8c03\u5ea6\u4e0e\u5206\u6790\u65b9\u6cd5\uff0c\u901a\u8fc7\u6269\u5c55\u5185\u6838\u7ea7\u5e76\u884c\u6027\u548c\u5efa\u7acb\u5185\u6838\u95f4\u4f9d\u8d56\u5173\u7cfb\u6765\u63d0\u4f9b\u7b80\u5316\u548c\u53ef\u9884\u6d4b\u7684DAG\u54cd\u5e94\u65f6\u95f4\uff0c\u5e76\u91c7\u7528\u6807\u51c6CUDA API\u5b9e\u73b0\uff0c\u65e0\u9700\u989d\u5916\u786c\u4ef6\u6216\u8f6f\u4ef6\u652f\u6301\u3002", "result": "\u5728\u5408\u6210\u548c\u771f\u5b9e\u4e16\u754c\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u8be5\u65b9\u6cd5\u6bd4\u73b0\u6709\u65b9\u6cd5\u6700\u591a\u53ef\u51cf\u5c1132.8%\u7684\u6700\u574f\u60c5\u51b5makespan\u548c21.3%\u7684\u5b9e\u6d4b\u4efb\u52a1\u6267\u884c\u65f6\u95f4\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86GPU\u4efb\u52a1\u6267\u884c\u65f6\u95f4\u4e0d\u53ef\u9884\u6d4b\u7684\u95ee\u9898\uff0c\u4e3a\u5b89\u5168\u5173\u952e\u5e94\u7528\u63d0\u4f9b\u4e86\u66f4\u53ef\u9760\u3001\u66f4\u9ad8\u6548\u7684GPU\u8ba1\u7b97\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.20471", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2602.20471", "abs": "https://arxiv.org/abs/2602.20471", "authors": ["Da Chen", "Guangyu Hu", "Kaihong Xu", "Kaichao Liang", "Songjiang Li", "Wei Yang", "XiangYu Wen", "Mingxuan Yuan"], "title": "SegSEM: Enabling and Enhancing SAM2 for SEM Contour Extraction", "comment": "4 pages, 6 figures, accpeted by ISCAS 2026", "summary": "Extracting high-fidelity 2D contours from Scanning Electron Microscope (SEM) images is critical for calibrating Optical Proximity Correction (OPC) models. While foundation models like Segment Anything 2 (SAM2) are promising, adapting them to specialized domains with scarce annotated data is a major challenge. This paper presents a case study on adapting SAM2 for SEM contour extraction in a few-shot setting. We propose SegSEM, a framework built on two principles: a data-efficient fine-tuning strategy that adapts by selectively training only the model's encoders, and a robust hybrid architecture integrating a traditional algorithm as a confidence-aware fallback. Using a small dataset of 60 production images, our experiments demonstrate this methodology's viability. The primary contribution is a methodology for leveraging foundation models in data-constrained industrial applications.", "AI": {"tldr": "\u7814\u7a76\u4eceSEM\u56fe\u50cf\u63d0\u53d62D\u8f6e\u5ed3\u7684\u6848\u4f8b\uff0c\u63d0\u51faSegSEM\u6846\u67b6\u5728\u5c11\u91cf\u6837\u672c\u8bbe\u7f6e\u4e0b\u6709\u6548\u9002\u5e94SAM2\u6a21\u578b", "motivation": "\u5728\u6570\u636e\u7a00\u7f3a\u7684\u5de5\u4e1a\u73af\u5883\u4e2d\uff0c\u9700\u8981\u6709\u6548\u7684\u57fa\u7840\u6a21\u578b\u9002\u5e94\u65b9\u6cd5\u6765\u4eceSEM\u56fe\u50cf\u63d0\u53d6\u9ad8\u4fdd\u771f\u8f6e\u5ed3\u7528\u4e8eOPC\u6a21\u578b\u6821\u51c6", "method": "SegSEM\u6846\u67b6\uff0c\u7ed3\u5408\u6570\u636e\u9ad8\u6548\u5fae\u8c03(\u4ec5\u8bad\u7ec3\u7f16\u7801\u5668)\u548c\u4f20\u7edf\u7b97\u6cd5\u4f5c\u4e3a\u7f6e\u4fe1\u611f\u77e5\u7684\u6df7\u5408\u67b6\u6784", "result": "\u4f7f\u752860\u5f20\u751f\u4ea7\u56fe\u50cf\u7684\u5c0f\u6570\u636e\u96c6\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\u7684\u53ef\u884c\u6027", "conclusion": "\u4e3a\u5728\u6570\u636e\u53d7\u9650\u7684\u5de5\u4e1a\u5e94\u7528\u4e2d\u5229\u7528\u57fa\u7840\u6a21\u578b\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u65b9\u6cd5\u8bba"}}
{"id": "2602.20341", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2602.20341", "abs": "https://arxiv.org/abs/2602.20341", "authors": ["Ignacio Amores-Sesar", "Mirza Ahad Baig", "Seth Gilbert", "Ray Neiheiser", "Michelle X. Yeo"], "title": "The Tragedy of Chain Commons", "comment": null, "summary": "Byzantine Fault Tolerant (BFT) consensus forms the foundation of many modern blockchains striving for both high throughput and low latency. A growing bottleneck is transaction execution and validation on the critical path of consensus, which has led to modular decoupled designs that separate ordering from execution: Consensus orders only metadata, while transactions are executed and validated concurrently. While this approach improves performance, it can leave invalid transactions in the ledger, increasing storage costs and enabling new forms of strategic behavior. We present the first systematic study of this setting, providing a formal framework to reason about the interaction between consensus and execution. Using this framework, we show that the decoupled design enables a previously unidentified attack, which we term gaslighting. We prove a fundamental trade-off between resilience to this attack and resource capacity utilization, where both are impossible to achieve deterministically in the decoupled model. To address this trade-off, we discuss an intermediate model for leader-based protocols that is robust to gaslighting attacks while achieving high throughput and low latency.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u6a21\u5757\u5316\u533a\u5757\u94fe\u67b6\u6784\u4e2d\u5206\u79bb\u5171\u8bc6\u4e0e\u4ea4\u6613\u6267\u884c\u8bbe\u8ba1\u7684\u5b89\u5168\u95ee\u9898\uff0c\u53d1\u73b0\u4e86\u4e00\u79cd\u79f0\u4e3a'gaslighting'\u7684\u65b0\u578b\u653b\u51fb\uff0c\u5e76\u8bc1\u660e\u4e86\u5728\u89e3\u8026\u6a21\u578b\u4e2d\u65e0\u6cd5\u540c\u65f6\u5b9e\u73b0\u5bf9\u6b64\u653b\u51fb\u7684\u62b5\u6297\u80fd\u529b\u548c\u8d44\u6e90\u5229\u7528\u7387\u7684\u6700\u4f18\u5316\u3002\u4f5c\u8005\u63d0\u51fa\u4e86\u4e00\u4e2a\u9488\u5bf9leader\u534f\u8bae\u7684\u4e2d\u95f4\u6a21\u578b\u6765\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\u3002", "motivation": "\u62dc\u5360\u5ead\u5bb9\u9519(BFT)\u5171\u8bc6\u662f\u8bb8\u591a\u73b0\u4ee3\u533a\u5757\u94fe\u7684\u57fa\u7840\uff0c\u4f46\u4ea4\u6613\u6267\u884c\u548c\u9a8c\u8bc1\u6210\u4e3a\u6027\u80fd\u74f6\u9888\u3002\u6a21\u5757\u5316\u8bbe\u8ba1\u5c06\u6392\u5e8f\u4e0e\u6267\u884c\u5206\u79bb\u53ef\u63d0\u9ad8\u6027\u80fd\uff0c\u4f46\u53ef\u80fd\u5bfc\u81f4\u65e0\u6548\u4ea4\u6613\u7559\u5728\u8d26\u672c\u4e2d\uff0c\u589e\u52a0\u5b58\u50a8\u6210\u672c\u5e76\u5f15\u53d1\u65b0\u7684\u6218\u7565\u884c\u4e3a\u3002", "method": "\u4f5c\u8005\u63d0\u4f9b\u4e86\u7b2c\u4e00\u4e2a\u7cfb\u7edf\u6027\u7814\u7a76\uff0c\u5efa\u7acb\u4e86\u5f62\u5f0f\u5316\u6846\u67b6\u6765\u5206\u6790\u5171\u8bc6\u4e0e\u6267\u884c\u7684\u4ea4\u4e92\u4f5c\u7528\uff0c\u5e76\u4f7f\u7528\u6b64\u6846\u67b6\u8bc6\u522b\u5e76\u5206\u6790\u4e86\u4e00\u79cd\u5148\u524d\u672a\u77e5\u7684'gaslighting'\u653b\u51fb\u3002\u4ed6\u4eec\u8bc1\u660e\u4e86\u5728\u89e3\u8026\u6a21\u578b\u4e2d\uff0c\u62b5\u6297\u6b64\u653b\u51fb\u4e0e\u8d44\u6e90\u5229\u7528\u7387\u4e4b\u95f4\u5b58\u5728\u6839\u672c\u6027\u6743\u8861\u3002", "result": "\u7814\u7a76\u8868\u660e\uff0c\u5728\u89e3\u8026\u6a21\u578b\u4e2d\uff0c\u65e0\u6cd5\u540c\u65f6\u786e\u5b9a\u6027\u5730\u62b5\u6297gaslighting\u653b\u51fb\u5e76\u5b9e\u73b0\u8d44\u6e90\u5bb9\u91cf\u5229\u7528\u7387\u7684\u6700\u4f18\u5316\u3002\u8fd9\u88ab\u89c6\u4e3a\u7cfb\u7edf\u8bbe\u8ba1\u4e2d\u7684\u4e00\u4e2a\u57fa\u672c\u6743\u8861\u3002", "conclusion": "\u4e3a\u89e3\u51b3\u8fd9\u4e00\u6743\u8861\u95ee\u9898\uff0c\u4f5c\u8005\u8ba8\u8bba\u4e86\u4e00\u4e2a\u9488\u5bf9leader\u534f\u8bae\u7684\u4e2d\u95f4\u6a21\u578b\uff0c\u8be5\u6a21\u578b\u5bf9gaslighting\u653b\u51fb\u5177\u6709\u9c81\u68d2\u6027\uff0c\u540c\u65f6\u4ecd\u80fd\u5b9e\u73b0\u9ad8\u541e\u5410\u91cf\u548c\u4f4e\u5ef6\u8fdf\u3002"}}
{"id": "2602.20229", "categories": ["cs.MA"], "pdf": "https://arxiv.org/pdf/2602.20229", "abs": "https://arxiv.org/abs/2602.20229", "authors": ["Tianjun Yao", "Zhaoyi Li", "Zhiqiang Shen"], "title": "HieraMAS: Optimizing Intra-Node LLM Mixtures and Inter-Node Topology for Multi-Agent Systems", "comment": "22 pages, 13 tables", "summary": "Multi-agent systems (MAS) built on large language models (LLMs) have shown strong performance across many tasks. Most existing approaches improve only one aspect at a time, such as the communication topology, role assignment, or LLM routing, while treating each agent as a single, indivisible unit. This misses the opportunity to use mixtures of LLMs within an agent to strengthen role-specific abilities. We propose HieraMAS, a hierarchical collaboration framework that combines intra-node LLM mixtures with an inter-node communication topology. HieraMAS introduces supernodes, where each functional role is implemented by multiple heterogeneous LLMs using a propose-synthesis structure. Optimizing HieraMAS creates unique credit-assignment challenges: final task performance depends heavily on the underlying LLMs' capabilities, which can lead reinforcement methods to incorrectly reward suboptimal configurations. To address this, we use a two-stage algorithm: (1) multi-level reward attribution, which provides fine-grained feedback at both the node level and the overall system level; (2) graph classification for topology selection, which treats choosing the communication structure as a holistic decision rather than optimizing edges one by one. Experiments on reasoning and coding benchmarks show that HieraMAS substantially outperforms existing methods while also delivering better cost-performance trade-offs.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aHieraMAS\u7684\u5206\u5c42\u591a\u667a\u80fd\u4f53\u534f\u4f5c\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u5408\u8282\u70b9\u5185\u5927\u8bed\u8a00\u6a21\u578b\u6df7\u5408\u548c\u8282\u70b9\u95f4\u901a\u4fe1\u62d3\u6251\u7ed3\u6784\uff0c\u663e\u8457\u63d0\u5347\u4e86\u63a8\u7406\u548c\u7f16\u7a0b\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u7684\u6027\u80fd\uff0c\u540c\u65f6\u63d0\u4f9b\u4e86\u66f4\u597d\u7684\u6210\u672c\u6548\u76ca\u6743\u8861\u3002", "motivation": "\u73b0\u6709\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u901a\u5e38\u4ec5\u6539\u5584\u5355\u4e00\u65b9\u9762\uff08\u5982\u901a\u4fe1\u62d3\u6251\u3001\u89d2\u8272\u5206\u914d\u6216LLM\u8def\u7531\uff09\uff0c\u5e76\u5c06\u6bcf\u4e2a\u667a\u80fd\u4f53\u89c6\u4e3a\u5355\u4e00\u4e0d\u53ef\u5206\u5272\u5355\u5143\uff0c\u672a\u80fd\u5145\u5206\u5229\u7528\u667a\u80fd\u4f53\u5185\u591a\u79cd\u5927\u8bed\u8a00\u6a21\u578b\u6df7\u5408\u6765\u589e\u5f3a\u89d2\u8272\u7279\u5b9a\u80fd\u529b\u3002", "method": "HieraMAS\u5f15\u5165\u8d85\u7ea7\u8282\u70b9\u6982\u5ff5\uff0c\u6bcf\u4e2a\u529f\u80fd\u89d2\u8272\u7531\u591a\u4e2a\u5f02\u6784\u5927\u8bed\u8a00\u6a21\u578b\u901a\u8fc7\u63d0\u8bae-\u5408\u6210\u7ed3\u6784\u5b9e\u73b0\uff0c\u5e76\u91c7\u7528\u4e24\u9636\u6bb5\u7b97\u6cd5\uff1a1) \u591a\u7ea7\u5956\u52b1\u5f52\u56e0\uff0c\u63d0\u4f9b\u8282\u70b9\u7ea7\u522b\u548c\u7cfb\u7edf\u7ea7\u522b\u7684\u7ec6\u7c92\u5ea6\u53cd\u9988\uff1b2) \u56fe\u5206\u7c7b\u8fdb\u884c\u62d3\u6251\u9009\u62e9\uff0c\u5c06\u901a\u4fe1\u7ed3\u6784\u9009\u62e9\u89c6\u4e3a\u6574\u4f53\u51b3\u7b56\u800c\u975e\u9010\u8fb9\u4f18\u5316\u3002", "result": "\u5728\u63a8\u7406\u548c\u7f16\u7a0b\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cHieraMAS\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u540c\u65f6\u63d0\u4f9b\u66f4\u597d\u7684\u6210\u672c\u6548\u76ca\u6743\u8861\u3002", "conclusion": "\u901a\u8fc7\u7ed3\u5408\u8282\u70b9\u5185LLM\u6df7\u5408\u548c\u8282\u70b9\u95f4\u901a\u4fe1\u4f18\u5316\uff0cHieraMAS\u5b9e\u73b0\u4e86\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u7684\u6027\u80fd\u7a81\u7834\uff0c\u4e3a\u590d\u6742\u4efb\u52a1\u89e3\u51b3\u63d0\u4f9b\u4e86\u65b0\u7684\u67b6\u6784\u8303\u5f0f\u3002"}}
{"id": "2602.20206", "categories": ["cs.SE", "cs.AI", "cs.CY", "cs.ET", "cs.MA"], "pdf": "https://arxiv.org/pdf/2602.20206", "abs": "https://arxiv.org/abs/2602.20206", "authors": ["Sreecharan Sankaranarayanan"], "title": "Mitigating \"Epistemic Debt\" in Generative AI-Scaffolded Novice Programming using Metacognitive Scripts", "comment": null, "summary": "The democratization of Large Language Models (LLMs) has given rise to ``Vibe Coding,\" a workflow where novice programmers prioritize semantic intent over syntactic implementation. While this lowers barriers to entry, we hypothesize that without pedagogical guardrails, it is fundamentally misaligned with cognitive skill acquisition. Drawing on the distinction between Cognitive Offloading and Cognitive Outsourcing, we argue that unrestricted AI encourages novices to outsource the Intrinsic Cognitive Load required for schema formation, rather than merely offloading Extraneous Load. This accumulation of ``Epistemic Debt\" creates ``Fragile Experts\" whose high functional utility masks critically low corrective competence.\n  To quantify and mitigate this debt, we conducted a between-subjects experiment (N=78) using a custom Cursor IDE plugin backed by Claude 3.5 Sonnet. Participants represented \"AI-Native\" learners across three conditions: Manual (Control), Unrestricted AI (Outsourcing), and Scaffolded AI (Offloading). The Scaffolded condition utilized a novel ``Explanation Gate,\" leveraging a real-time LLM-as-a-Judge framework to enforce a ``Teach-Back\" protocol before generated code could be integrated.\n  Results reveal a ``Collapse of Competence\": while Unrestricted AI users matched the productivity of the Scaffolded group (p < .001 vs. Manual), they suffered a 77% failure rate in a subsequent AI-Blackout maintenance task, compared to only 39% in the Scaffolded group. Qualitative analysis suggests that successful vibe coders naturally engage in self-scaffolding, treating the AI as a consultant rather than a contractor. We discuss the implications for the maintainability of AI-generated software and propose that future learning systems must enforce Metacognitive Friction to prevent the mass production of unmaintainable code.", "AI": {"tldr": "\u7814\u7a76\u663e\u793a unrestricted AI \u52a9\u7f16\u7a0b\u5bfc\u81f4\u8ba4\u77e5\u6280\u80fd\u83b7\u53d6\u4e0d\u8db3\uff0c\u5f62\u6210'\u8106\u5f31\u4e13\u5bb6'\uff0c\u800c\u7ed3\u6784\u5316 AI \u8f85\u52a9\u53ef\u51cf\u8f7b\u95ee\u9898\u540c\u65f6\u4fdd\u6301\u751f\u4ea7\u529b\u3002", "motivation": "\u63a2\u7a76 LLM \u6c11\u4e3b\u5316\u5e26\u6765\u7684'\u6c1b\u56f4\u7f16\u7a0b'\u5bf9\u65b0\u624b\u7a0b\u5e8f\u5458\u8ba4\u77e5\u6280\u80fd\u83b7\u53d6\u7684\u5f71\u54cd\uff0c\u4ee5\u53ca\u5982\u4f55\u9632\u6b62\u4ea7\u751f'\u8ba4\u77e5\u503a\u52a1'\u548c'\u8106\u5f31\u4e13\u5bb6'\u3002", "method": "\u8fdb\u884c 78 \u540d\u53c2\u4e0e\u8005\u7684\u53d7\u8bd5\u8005\u95f4\u5b9e\u9a8c\uff0c\u6bd4\u8f83\u4e09\u79cd\u6761\u4ef6\uff1a\u624b\u52a8\u7f16\u7a0b\uff08\u5bf9\u7167\u7ec4\uff09\u3001\u65e0\u9650\u5236 AI\uff08\u5916\u5305\uff09\u548c\u7ed3\u6784\u5316 AI\uff08\u5378\u8f7d\uff09\uff0c\u4f7f\u7528 Claude 3.5 Sonnet \u548c\u81ea\u5b9a\u4e49 Cursor IDE \u63d2\u4ef6\u3002", "result": "\u65e0\u9650\u5236 AI \u7528\u6237\u751f\u4ea7\u529b\u4e0e\u7ed3\u6784\u5316\u7ec4\u76f8\u5f53\uff0c\u4f46\u5728\u540e\u7eed AI \u9ed1\u76d2\u7ef4\u62a4\u4efb\u52a1\u4e2d\u5931\u8d25\u7387\u9ad8\u8fbe 77%\uff0c\u800c\u7ed3\u6784\u5316\u7ec4\u4ec5\u4e3a 39%\u3002\u6210\u529f\u7684\u6c1b\u56f4\u7f16\u7a0b\u8005\u81ea\u7136\u5730\u8fdb\u884c\u81ea\u6211\u652f\u67b6\uff0c\u5c06 AI \u89c6\u4e3a\u987e\u95ee\u800c\u975e\u627f\u5305\u5546\u3002", "conclusion": "\u672a\u6765\u5b66\u4e60\u7cfb\u7edf\u5fc5\u987b\u5f3a\u5236\u6267\u884c'\u5143\u8ba4\u77e5\u6469\u64e6'\uff0c\u9632\u6b62\u4ea7\u751f\u4e0d\u53ef\u7ef4\u62a4\u7684\u4ee3\u7801\uff0c\u5e73\u8861 AI \u8f85\u52a9\u4e0e\u8ba4\u77e5\u6280\u80fd\u53d1\u5c55\u3002"}}
{"id": "2602.20515", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2602.20515", "abs": "https://arxiv.org/abs/2602.20515", "authors": ["Rakshith Jayanth", "Viktor Prasanna"], "title": "FAST-Prefill: FPGA Accelerated Sparse Attention for Long Context LLM Prefill", "comment": null, "summary": "In long-context large language model (LLM) inference, the prefill stage dominates computation due to self-attention over the complete input context. Sparse attention significantly reduces self-attention computation by limiting each token's interactions to a subset of tokens. The attention sparsity pattern varies across input prompts, and within a prompt, each attention head can follow a distinct pattern. This makes attention sparsity dynamic. The requirement of generating the sparsity pattern, combined with limited data reuse in attention, shifts the prefill compute to being memory-bound. This, in addition to the huge energy requirements for long-context inference on GPU, motivates FPGAs as good candidates for accelerating dynamic long-context inference.\n  To tackle these challenges, we propose FAST-Prefill, the first FPGA accelerator for long-context prefill-stage inference with dynamic sparse attention. To efficiently generate sparse indices, we propose a \\textit{fused pipeline unit with a memory-aware execution order} to reduce large tensors and irregular memory accesses. To reduce off-chip memory traffic for accessing the KV cache, we utilize the memory hierarchy to design a \\textit{liveness-driven, dual-tier cache}. For high-throughput matrix multiplication, we design a \\textit{hybrid Matrix Processing Unit (MPU)} with DSPs and bit-plane decomposition using LUTs. We implement FAST-Prefill on Alveo U280 and evaluate it on the Llama and Qwen models (batch size = 1) for context lengths ranging from 4K to 128K tokens. We demonstrate an average speedup of up to 2.5$\\times$ in TTFT and 4.5$\\times$ improvement in energy efficiency over GPU implementation on Nvidia A5000 GPU.", "AI": {"tldr": "FAST-Prefill\u662f\u4e00\u4e2aFPGA\u52a0\u901f\u5668\uff0c\u901a\u8fc7\u52a8\u6001\u7a00\u758f\u6ce8\u610f\u529b\u6280\u672f\u89e3\u51b3\u4e86\u957f\u4e0a\u4e0b\u6587LLM\u9884\u586b\u5145\u63a8\u7406\u7684\u5185\u5b58\u9650\u5236\u95ee\u9898\uff0c\u5728TTFT\u548c\u80fd\u6548\u65b9\u9762\u663e\u8457\u4f18\u4e8eGPU\u5b9e\u73b0\u3002", "motivation": "\u957f\u4e0a\u4e0b\u6587LLM\u9884\u586b\u5145\u63a8\u7406\u56e0\u52a8\u6001\u6ce8\u610f\u529b\u7a00\u758f\u6a21\u5f0f\u548c\u6709\u9650\u7684\u6570\u636e\u91cd\u7528\u800c\u6210\u4e3a\u5185\u5b58\u9650\u5236\u578b\u4efb\u52a1\uff0c\u4e14\u5728GPU\u4e0a\u8fdb\u884c\u957f\u4e0a\u4e0b\u6587\u63a8\u7406\u80fd\u8017\u9ad8\u3002FPGA\u88ab\u63d0\u51fa\u4f5c\u4e3a\u9ad8\u6548\u66ff\u4ee3\u65b9\u6848\u3002", "method": "\u89e3\u51b3\u65b9\u6848\u5305\u62ec\uff1a\u5177\u6709\u5185\u5b58\u611f\u77e5\u6267\u884c\u987a\u5e8f\u7684\u878d\u5408\u6d41\u6c34\u7ebf\u5355\u5143\u7528\u4e8e\u7a00\u758f\u7d22\u5f15\u751f\u6210\uff1b\u57fa\u4e8e\u6d3b\u8dc3\u6027\u7684\u53cc\u5c42\u7f13\u5b58\u7528\u4e8eKV\u8bbf\u95ee\uff1b\u7ed3\u5408DSP\u548c\u4f4d\u5e73\u9762\u5206\u89e3\u7684\u6df7\u5408MPU\u7528\u4e8e\u77e9\u9635\u4e58\u6cd5\u3002", "result": "\u5728Alveo U280 FPGA\u4e0a\u8bc4\u4f30\u4e86Llama\u548cQwen\u6a21\u578b\uff084K-128K\u4e0a\u4e0b\u6587\uff09\uff0cFAST-Prefill\u5b9e\u73b0\u4e862.5\u500d\u7684TTFT\u52a0\u901f\u548c\u76f8\u6bd4Nvidia A5000 GPU 4.5\u500d\u7684\u80fd\u6548\u63d0\u5347\u3002", "conclusion": "FAST-Prefill\u901a\u8fc7\u4e13\u95e8\u8bbe\u8ba1\u7684\u786c\u4ef6\u67b6\u6784\u89e3\u51b3\u4e86\u5185\u5b58\u9650\u5236\u6311\u6218\uff0c\u5c55\u793a\u4e86FPGA\u5728\u52a0\u901f\u52a8\u6001\u957f\u4e0a\u4e0b\u6587LLM\u63a8\u7406\u65b9\u9762\u7684\u6f5c\u529b\u3002"}}
{"id": "2602.20444", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2602.20444", "abs": "https://arxiv.org/abs/2602.20444", "authors": ["Paul Borrill"], "title": "Circumventing the FLP Impossibility Result with Open Atomic Ethernet", "comment": "12 pages, 3 figures, 1 table", "summary": "The Fischer--Lynch--Paterson (FLP) impossibility result is widely regarded as one of the most fundamental negative results in distributed computing: no deterministic protocol can guarantee consensus in an asynchronous system with even one faulty process. For forty years, the field has treated this as an immovable constraint, designing around it with randomized protocols, failure detectors, and weakened consistency models. This essay argues that FLP is not a law of physics but a theorem about a particular system model -- and that Open Atomic Ethernet (OAE) circumvents it by rejecting the asynchronous model at its foundation. We introduce the term bisynchronous to describe OAE's key property: bounded-time bilateral resolution in which both parties reach common knowledge of outcome at every round boundary -- a strictly stronger guarantee than synchrony alone. By constructing a bisynchronous, swap-based protocol at Layer 2, OAE sidesteps the load-bearing assumptions of FLP's asynchronous model, achieving deterministic atomic coordination without violating any impossibility result.", "AI": {"tldr": "\u672c\u6587\u6311\u6218\u4e86FLP\u4e0d\u53ef\u80fd\u6027\u7ed3\u679c\u7684\u7edd\u5bf9\u6027\uff0c\u63d0\u51faOpen Atomic Ethernet (OAE)\u901a\u8fc7'\u53cc\u540c\u6b65'\u7279\u6027\u7ed5\u8fc7\u4e86\u8fd9\u4e00\u9650\u5236\uff0c\u5b9e\u73b0\u4e86\u786e\u5b9a\u6027\u539f\u5b50\u534f\u8c03\u3002", "motivation": "\u56db\u5341\u5e74\u6765\uff0c\u5206\u5e03\u5f0f\u8ba1\u7b97\u9886\u57df\u4e00\u76f4\u5c06FLP\u4e0d\u53ef\u80fd\u6027\u7ed3\u679c\u89c6\u4e3a\u4e0d\u53ef\u903e\u8d8a\u7684\u7ea6\u675f\uff0c\u8be5\u7ed3\u679c\u8868\u660e\u5728\u5b58\u5728\u5355\u4e2a\u6545\u969c\u8fdb\u7a0b\u7684\u5f02\u6b65\u7cfb\u7edf\u4e2d\uff0c\u6ca1\u6709\u786e\u5b9a\u6027\u534f\u8bae\u80fd\u4fdd\u8bc1\u5171\u8bc6\u3002\u672c\u6587\u65e8\u5728\u8bc1\u660e\u8fd9\u4e00\u7ed3\u679c\u5e76\u975e\u7269\u7406\u5b9a\u5f8b\uff0c\u800c\u662f\u7279\u5b9a\u7cfb\u7edf\u6a21\u578b\u7684\u5b9a\u7406\u3002", "method": "\u4f5c\u8005\u63d0\u51fa\u4e86'\u53cc\u540c\u6b65'\u6982\u5ff5\u6765\u63cf\u8ff0OAE\u7684\u5173\u952e\u7279\u6027\uff0c\u5373\u6bcf\u8f6e\u8fb9\u754c\u4e0a\u53cc\u65b9\u90fd\u80fd\u5728\u6709\u9650\u65f6\u95f4\u5185\u8fbe\u6210\u7ed3\u679c\u7684\u5171\u540c\u77e5\u8bc6\uff0c\u8fd9\u6bd4\u5355\u7eaf\u7684\u540c\u6b65\u6027\u66f4\u5f3a\u3002\u901a\u8fc7\u5728\u7b2c\u4e8c\u5c42\u6784\u5efa\u53cc\u540c\u6b65\u3001\u57fa\u4e8e\u4ea4\u6362\u7684\u534f\u8bae\uff0cOAE\u7ed5\u8fc7\u4e86FLP\u5f02\u6b65\u6a21\u578b\u7684\u57fa\u7840\u5047\u8bbe\u3002", "result": "OAE\u6210\u529f\u5b9e\u73b0\u4e86\u786e\u5b9a\u6027\u539f\u5b50\u534f\u8c03\uff0c\u65e0\u9700\u8fdd\u53cd\u4efb\u4f55\u4e0d\u53ef\u80fd\u6027\u7ed3\u679c\u3002\u901a\u8fc7\u62d2\u7edd\u5f02\u6b65\u6a21\u578b\u7684\u57fa\u7840\u5047\u8bbe\uff0c\u8be5\u7cfb\u7edf\u907f\u514d\u4e86FLP\u4e0d\u53ef\u80fd\u6027\u7ed3\u679c\u6240\u4f9d\u8d56\u7684\u5173\u952e\u6761\u4ef6\u3002", "conclusion": "FLP\u4e0d\u662f\u7269\u7406\u5b9a\u5f8b\uff0c\u800c\u662f\u5173\u4e8e\u7279\u5b9a\u7cfb\u7edf\u6a21\u578b\u7684\u5b9a\u7406\u3002\u901a\u8fc7\u91c7\u7528\u53cc\u540c\u6b65\u65b9\u6cd5\u800c\u975e\u5f02\u6b65\u6a21\u578b\uff0c\u7cfb\u7edf\u53ef\u4ee5\u7ed5\u8fc7FLP\u4e0d\u53ef\u80fd\u6027\uff0c\u5b9e\u73b0\u786e\u5b9a\u6027\u5206\u5e03\u5f0f\u534f\u8c03\u3002"}}
{"id": "2602.20662", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2602.20662", "abs": "https://arxiv.org/abs/2602.20662", "authors": ["Hongyi Guan", "Yijia Zhang", "Wenqiang Wang", "Yizhao Gao", "Shijie Cao", "Chen Zhang", "Ningyi Xu"], "title": "TOM: A Ternary Read-only Memory Accelerator for LLM-powered Edge Intelligence", "comment": "13 pages", "summary": "The deployment of Large Language Models (LLMs) for real-time intelligence on edge devices is rapidly growing. However, conventional hardware architectures face a fundamental memory wall challenge, where limited on-device memory capacity and bandwidth severely constrain the size of deployable models and their inference speed, while also limiting on-device adaptation. To address this challenge, we propose TOM, a hybrid ROM-SRAM accelerator co-designed with ternary quantization, which balances extreme density with on-device tunability. TOM exploits the synergy between ternary quantization and ROM to achieve extreme memory density and bandwidth, while preserving flexibility through a hybrid ROM-SRAM architecture designed for QLoRA-based tunability. Specifically, we introduce: (1) a sparsity-aware ROM architecture that synthesizes ternary weights as standard-cell logic, eliminating area overhead from zero-valued bits; (2) a distributed processing architecture that co-locates high-density ROM banks with flexible SRAM-based QLoRA adapters and compute units; and (3) a workload-aware dynamic power gating scheme that exploits the logic-based nature of ROM to power down inactive banks, minimizing dynamic energy consumption. TOM achieves an inference throughput of 3,306 TPS using BitNet-2B model, demonstrating its effectiveness in delivering real-time, energy-efficient edge intelligence.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faTOM\uff0c\u4e00\u79cd\u7ed3\u5408\u4e09\u5143\u91cf\u5316\u7684\u6df7\u5408ROM-SRAM\u52a0\u901f\u5668\uff0c\u7528\u4e8e\u89e3\u51b3\u8fb9\u7f18\u8bbe\u5907\u90e8\u7f72\u5927\u8bed\u8a00\u6a21\u578b\u7684\u5185\u5b58\u5899\u95ee\u9898\uff0c\u5b9e\u73b0\u4e863306 TPS\u7684\u63a8\u7406\u541e\u5410\u91cf\uff0c\u540c\u65f6\u4fdd\u6301\u9ad8\u80fd\u6548\u3002", "motivation": "\u89e3\u51b3\u8fb9\u7f18\u8bbe\u5907\u4e0a\u5185\u5b58\u5bb9\u91cf\u548c\u5e26\u5bbd\u6709\u9650\u5bf9\u53ef\u90e8\u7f72\u6a21\u578b\u5927\u5c0f\u3001\u63a8\u7406\u901f\u5ea6\u548c\u8bbe\u5907\u7aef\u9002\u5e94\u6027\u7684\u5236\u7ea6\uff0c\u5373\u5185\u5b58\u5899\u6311\u6218\u3002", "method": "TOM\u662f\u4e00\u79cd\u6df7\u5408ROM-SRAM\u52a0\u901f\u5668\uff0c\u4e0e\u4e09\u5143\u91cf\u5316\u534f\u540c\u8bbe\u8ba1\uff0c\u5305\u62ec\uff1a(1)\u7a00\u758f\u611f\u77e5ROM\u67b6\u6784\uff0c\u5c06\u4e09\u5143\u6743\u91cd\u5408\u6210\u4e3a\u6807\u51c6\u5355\u5143\u903b\u8f91\uff0c\u6d88\u9664\u96f6\u503c\u6bd4\u7279\u7684\u9762\u79ef\u5f00\u9500\uff1b(2)\u5206\u5e03\u5f0f\u5904\u7406\u67b6\u6784\uff0c\u5c06\u9ad8\u5bc6\u5ea6ROM\u5e93\u4e0e\u7075\u6d3b\u7684\u57fa\u4e8eSRAM\u7684QLoRA\u9002\u914d\u5668\u548c\u8ba1\u7b97\u5355\u5143\u534f\u540c\u5b9a\u4f4d\uff1b(3)\u5de5\u4f5c\u8d1f\u8f7d\u611f\u77e5\u7684\u52a8\u6001\u7535\u6e90\u95e8\u63a7\u65b9\u6848\uff0c\u5229\u7528ROM\u7684\u903b\u8f91\u7279\u6027\u5173\u95ed\u975e\u6d3b\u52a8\u5b58\u50a8\u5e93\uff0c\u6700\u5c0f\u5316\u52a8\u6001\u80fd\u8017\u3002", "result": "\u4f7f\u7528BitNet-2B\u6a21\u578b\u5b9e\u73b0\u4e863306 TPS\u7684\u63a8\u7406\u541e\u5410\u91cf\uff0c\u8bc1\u660e\u4e86\u5176\u5728\u63d0\u4f9b\u5b9e\u65f6\u3001\u80fd\u6548\u8fb9\u7f18\u667a\u80fd\u65b9\u9762\u7684\u6709\u6548\u6027\u3002", "conclusion": "TOM\u901a\u8fc7\u7ed3\u5408\u4e09\u5143\u91cf\u5316\u548c\u6df7\u5408ROM-SRAM\u67b6\u6784\uff0c\u6210\u529f\u89e3\u51b3\u4e86\u8fb9\u7f18\u8bbe\u5907LLM\u90e8\u7f72\u7684\u5185\u5b58\u5899\u6311\u6218\uff0c\u5b9e\u73b0\u4e86\u9ad8\u63a8\u7406\u541e\u5410\u91cf\u548c\u80fd\u6548\u3002"}}
{"id": "2602.20450", "categories": ["cs.DC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.20450", "abs": "https://arxiv.org/abs/2602.20450", "authors": ["Nihal Balivada", "Shrey Gupta", "Shashank Shreedhar Bhatt", "Suyash Gupta"], "title": "Heterogeneity-Aware Client Selection Methodology For Efficient Federated Learning", "comment": null, "summary": "Federated Learning (FL) enables a distributed client-server architecture where multiple clients collaboratively train a global Machine Learning (ML) model without sharing sensitive local data. However, FL often results in lower accuracy than traditional ML algorithms due to statistical heterogeneity across clients. Prior works attempt to address this by using model updates, such as loss and bias, from client models to select participants that can improve the global model's accuracy. However, these updates neither accurately represent a client's heterogeneity nor are their selection methods deterministic. We mitigate these limitations by introducing Terraform, a novel client selection methodology that uses gradient updates and a deterministic selection algorithm to select heterogeneous clients for retraining. This bi-pronged approach allows Terraform to achieve up to 47 percent higher accuracy over prior works. We further demonstrate its efficiency through comprehensive ablation studies and training time analyses, providing strong justification for the robustness of Terraform.", "AI": {"tldr": "Terraform\u662f\u4e00\u79cd\u65b0\u9896\u7684\u8054\u90a6\u5b66\u4e60\u5ba2\u6237\u9009\u62e9\u65b9\u6cd5\uff0c\u901a\u8fc7\u7ed3\u5408\u68af\u5ea6\u66f4\u65b0\u548c\u786e\u5b9a\u6027\u9009\u62e9\u7b97\u6cd5\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u56e0\u5ba2\u6237\u7aef\u5f02\u8d28\u6027\u5bfc\u81f4\u7684\u6a21\u578b\u51c6\u786e\u6027\u964d\u4f4e\u95ee\u9898\uff0c\u76f8\u6bd4\u5148\u524d\u65b9\u6cd5\u6700\u9ad8\u53ef\u63d0\u9ad847%\u7684\u51c6\u786e\u6027\u3002", "motivation": "\u8054\u90a6\u5b66\u4e60\u5728\u4fdd\u62a4\u6570\u636e\u9690\u79c1\u65b9\u9762\u6709\u4f18\u52bf\uff0c\u4f46\u7531\u4e8e\u5ba2\u6237\u7aef\u95f4\u7684\u7edf\u8ba1\u5f02\u8d28\u6027\uff0c\u901a\u5e38\u5bfc\u81f4\u6a21\u578b\u51c6\u786e\u6027\u4f4e\u4e8e\u4f20\u7edf\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u3002\u73b0\u6709\u7684\u5ba2\u6237\u9009\u62e9\u65b9\u6cd5\u4f7f\u7528\u6a21\u578b\u66f4\u65b0\u5982\u635f\u5931\u548c\u504f\u5dee\u6765\u9009\u62e9\u53c2\u4e0e\u8005\uff0c\u4f46\u8fd9\u4e9b\u66f4\u65b0\u4e0d\u80fd\u51c6\u786e\u8868\u793a\u5ba2\u6237\u7aef\u5f02\u8d28\u6027\uff0c\u4e14\u9009\u62e9\u65b9\u6cd5\u975e\u786e\u5b9a\u3002", "method": "Terraform\u91c7\u7528\u53cc\u7ba1\u9f50\u4e0b\u7684\u65b9\u6cd5\uff1a1)\u4f7f\u7528\u68af\u5ea6\u66f4\u65b0\u800c\u975e\u7b80\u5355\u7684\u635f\u5931\u6216\u504f\u5dee\u6765\u66f4\u51c6\u786e\u5730\u8868\u793a\u5ba2\u6237\u7aef\u5f02\u8d28\u6027\uff1b2)\u5b9e\u65bd\u786e\u5b9a\u6027\u9009\u62e9\u7b97\u6cd5\u800c\u975e\u968f\u673a\u9009\u62e9\uff0c\u786e\u4fdd\u9009\u62e9\u8fc7\u7a0b\u7684\u53ef\u9760\u6027\u3002", "result": "Terraform\u5728\u591a\u4e2a\u573a\u666f\u4e0b\u5b9e\u73b0\u4e86\u6bd4\u5148\u524d\u65b9\u6cd5\u9ad8\u8fbe47%\u7684\u51c6\u786e\u6027\u63d0\u5347\u3002\u901a\u8fc7\u5168\u9762\u7684\u6d88\u878d\u7814\u7a76\u548c\u8bad\u7ec3\u65f6\u95f4\u5206\u6790\uff0c\u8bc1\u660e\u4e86\u8be5\u65b9\u6cd5\u5728\u63d0\u9ad8\u6a21\u578b\u51c6\u786e\u6027\u548c\u8ba1\u7b97\u6548\u7387\u65b9\u9762\u7684\u6709\u6548\u6027\u3002", "conclusion": "Terraform\u901a\u8fc7\u7ed3\u5408\u68af\u5ea6\u66f4\u65b0\u548c\u786e\u5b9a\u6027\u9009\u62e9\u7b97\u6cd5\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u8054\u90a6\u5b66\u4e60\u4e2d\u56e0\u5ba2\u6237\u7aef\u5f02\u8d28\u6027\u5bfc\u81f4\u7684\u6a21\u578b\u51c6\u786e\u6027\u95ee\u9898\uff0c\u4e3a\u4fdd\u62a4\u6570\u636e\u9690\u79c1\u7684\u540c\u65f6\u4fdd\u6301\u9ad8\u6a21\u578b\u51c6\u786e\u6027\u63d0\u4f9b\u4e86\u53ef\u884c\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.20284", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2602.20284", "abs": "https://arxiv.org/abs/2602.20284", "authors": ["Han Fu", "Andreas Ermedahl", "Sigrid Eldh", "Kristian Wiklund", "Philipp Haller", "Cyrille Artho"], "title": "PhantomRun: Auto Repair of Compilation Errors in Embedded Open Source Software", "comment": "13 pages, 5 figures, Mining Software Repositories 2026 (MSR 2026) , Rio de Janeiro, Brazil, 13-14 April 2026", "summary": "Continuous Integration (CI) pipelines for embedded software sometimes fail during compilation, consuming significant developer time for debugging. We study four major open-source embedded system projects, spanning over 4000 build failures from the project's CI runs. We find that hardware dependencies account for the majority of compilation failures, followed by syntax errors and build-script issues. Most repairs need relatively small changes, making automated repair potentially suitable as long as the diverse setups and lack of test data can be handled.\n  In this paper, we present PhantomRun, an automated framework that leverages large language models (LLMs) to generate and validate fixes for CI compilation failures. The framework addresses the challenge of diverse build infrastructures and tool chains across embedded system projects by providing an adaptation layer for GitHub Actions and GitLab CI and four different build systems. PhantomRun utilizes build logs, source code, historical fixes, and compiler error messages to synthesize fixes using LLMs. Our evaluations show that PhantomRun successfully repairs up to 45% of CI compilation failures across the targeted projects, demonstrating the viability of LLM-based repairs for embedded-system CI pipelines.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u63d0\u51fa\u4e86PhantomRun\u6846\u67b6\uff0c\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\u81ea\u52a8\u4fee\u590d\u5d4c\u5165\u5f0f\u7cfb\u7edf\u7684CI\u7f16\u8bd1\u9519\u8bef\uff0c\u6210\u529f\u4fee\u590d\u4e86\u9ad8\u8fbe45%\u7684\u7f16\u8bd1\u5931\u8d25\u3002", "motivation": "\u5d4c\u5165\u5f0f\u7cfb\u7edf\u7684CI\u7ba1\u9053\u7ecf\u5e38\u5728\u7f16\u8bd1\u9636\u6bb5\u5931\u8d25\uff0c\u5360\u7528\u5f00\u53d1\u8005\u5927\u91cf\u8c03\u8bd5\u65f6\u95f4\u3002\u7814\u7a76\u53d1\u73b0\u786c\u4ef6\u4f9d\u8d56\u662f\u5bfc\u81f4\u7f16\u8bd1\u5931\u8d25\u7684\u4e3b\u8981\u539f\u56e0\uff0c\u4e14\u5927\u591a\u6570\u4fee\u590d\u53ea\u9700\u8981\u8f83\u5c0f\u7684\u4ee3\u7801\u66f4\u6539\u3002", "method": "\u5f00\u53d1\u4e86PhantomRun\u6846\u67b6\uff0c\u901a\u8fc7GitHub Actions\u548cGitLab CI\u7684\u9002\u914d\u5c42\u4ee5\u53ca\u56db\u79cd\u4e0d\u540c\u7684\u6784\u5efa\u7cfb\u7edf\u5904\u7406\u591a\u6837\u5316\u7684\u6784\u5efa\u73af\u5883\u3002\u8be5\u6846\u67b6\u5229\u7528\u6784\u5efa\u65e5\u5fd7\u3001\u6e90\u4ee3\u7801\u3001\u5386\u53f2\u4fee\u590d\u8bb0\u5f55\u548c\u7f16\u8bd1\u5668\u9519\u8bef\u6d88\u606f\uff0c\u4f7f\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\u751f\u6210\u548c\u9a8c\u8bc1\u4fee\u590d\u65b9\u6848\u3002", "result": "\u5728\u76ee\u6807\u9879\u76ee\u4e0a\u8bc4\u4f30\u663e\u793a\uff0cPhantomRun\u6210\u529f\u4fee\u590d\u4e86\u9ad8\u8fbe45%\u7684CI\u7f16\u8bd1\u5931\u8d25\uff0c\u8bc1\u660e\u4e86\u57fa\u4e8eLLM\u7684\u4fee\u590d\u65b9\u6cd5\u5bf9\u5d4c\u5165\u5f0f\u7cfb\u7edfCI\u7ba1\u9053\u7684\u53ef\u884c\u6027\u3002", "conclusion": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u53ef\u4ee5\u6709\u6548\u5730\u5e94\u7528\u4e8e\u5d4c\u5165\u5f0f\u7cfb\u7edfCI\u7ba1\u9053\u7684\u7f16\u8bd1\u9519\u8bef\u4fee\u590d\uff0c\u7279\u522b\u662f\u5f53\u8003\u8651\u5230\u591a\u6837\u5316\u7684\u8bbe\u7f6e\u548c\u7f3a\u4e4f\u6d4b\u8bd5\u6570\u636e\u7b49\u6311\u6218\u65f6\uff0cPhantomRun\u6846\u67b6\u4e3a\u6b64\u63d0\u4f9b\u4e86\u53ef\u884c\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.20802", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2602.20802", "abs": "https://arxiv.org/abs/2602.20802", "authors": ["Philippos Papaphilippou"], "title": "LUTstructions: Self-loading FPGA-based Reconfigurable Instructions", "comment": null, "summary": "General-purpose processors feature a limited number of instructions based on an instruction set. They can be numerous, such as with vector extensions that include hundreds or thousands of instructions, but this comes at a cost; they are often unable to express arbitrary tasks efficiently. This paper explores the concept of having reconfigurable instructions by incorporating reconfigurable areas in a softcore. It follows a relatively-recently proposed computer architecture concept for seamlessly loading instruction implementation-carrying bitstreams from main memory. The resulting softcore is entirely evaluated on an FPGA, essentially having an FPGA-on-an-FPGA for the instruction implementations, with no notable operating frequency overhead. This is achieved with a custom FPGA architecture called LUTstruction, which is tailored towards low-latency for custom instructions and wide reconfiguration, as well as a soft implementation for the purposes of architectural exploration.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5177\u6709\u53ef\u91cd\u6784\u6307\u4ee4\u7684\u8f6f\u6838\u5904\u7406\u5668\u67b6\u6784\uff0c\u901a\u8fc7\u5728FPGA\u4e0a\u5b9e\u73b0LUTstruction\u67b6\u6784\uff0c\u5b9e\u73b0\u4e86\u4ece\u4e3b\u5185\u5b58\u52a8\u6001\u52a0\u8f7d\u6307\u4ee4\u4f4d\u6d41\u7684\u529f\u80fd\uff0c\u65e0\u660e\u663e\u9891\u7387\u5f00\u9500\u3002", "motivation": "\u901a\u7528\u5904\u7406\u5668\u6307\u4ee4\u96c6\u6709\u9650\uff0c\u5373\u4f7f\u662f\u5305\u542b\u6570\u767e\u6216\u6570\u5343\u6761\u6307\u4ee4\u7684\u5411\u91cf\u6269\u5c55\uff0c\u901a\u5e38\u4e5f\u65e0\u6cd5\u6709\u6548\u8868\u8fbe\u4efb\u610f\u4efb\u52a1\uff0c\u9700\u8981\u66f4\u7075\u6d3b\u7684\u53ef\u91cd\u6784\u6307\u4ee4\u65b9\u6848\u3002", "method": "\u91c7\u7528\u8f6f\u6838\u5904\u7406\u5668\u5e76\u7ed3\u5408\u53ef\u91cd\u6784\u533a\u57df\uff0c\u5b9e\u73b0\u4ece\u4e3b\u5185\u5b58\u52a8\u6001\u52a0\u8f7d\u6307\u4ee4\u5b9e\u73b0\u4f4d\u6d41\u7684\u6280\u672f\uff0c\u4f7f\u7528\u540d\u4e3aLUTstruction\u7684\u81ea\u5b9a\u4e49FPGA\u67b6\u6784\uff0c\u9488\u5bf9\u4f4e\u5ef6\u8fdf\u81ea\u5b9a\u4e49\u6307\u4ee4\u548c\u5bbd\u8303\u56f4\u91cd\u6784\u8fdb\u884c\u4f18\u5316\u3002", "result": "\u5728FPGA\u4e0a\u6210\u529f\u5b9e\u73b0\u4e86\u5b8c\u5168\u8bc4\u4f30\u7684\u8f6f\u6838\uff0c\u5b9e\u8d28\u4e0a\u521b\u5efa\u4e86'\u5728FPGA\u4e0a\u7684FPGA'\u7528\u4e8e\u6307\u4ee4\u5b9e\u73b0\uff0c\u4e14\u65e0\u660e\u663e\u64cd\u4f5c\u9891\u7387\u5f00\u9500\u3002", "conclusion": "LUTstruction\u67b6\u6784\u4e3a\u53ef\u91cd\u6784\u6307\u4ee4\u5904\u7406\u5668\u63d0\u4f9b\u4e86\u6709\u6548\u5b9e\u73b0\u65b9\u5f0f\uff0c\u901a\u8fc7\u8f6f\u5b9e\u73b0\u652f\u6301\u4e86\u67b6\u6784\u63a2\u7d22\uff0c\u4e3a\u672a\u6765\u5904\u7406\u5668\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2602.20561", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2602.20561", "abs": "https://arxiv.org/abs/2602.20561", "authors": ["Sana Taghipour Anvar", "David Kaeli"], "title": "A Granularity Characterization of Task Scheduling Effectiveness", "comment": null, "summary": "Task-based runtime systems provide flexible load balancing and portability for parallel scientific applications, but their strong scaling is highly sensitive to task granularity. As parallelism increases, scheduling overhead may transition from negligible to dominant, leading to rapid drops in performance for some algorithms, while remaining negligible for others. Although such effects are widely observed empirically, there is a general lack of understanding how algorithmic structure impacts whether dynamic scheduling is always beneficial. In this work, we introduce a granularity characterization framework that directly links scheduling overhead growth to task-graph dependency topology. We show that dependency structure, rather than problem size alone, governs how overhead scales with parallelism. Based on this observation, we characterize execution behavior using a simple granularity measure that indicates when scheduling overhead can be amortized by parallel computation and when scheduling overhead dominates performance. Through experimental evaluation on representative parallel workloads with diverse dependency patterns, we demonstrate that the proposed characterization explains both gradual and abrupt strong-scaling breakdowns observed in practice. We further show that overhead models derived from dependency topology accurately predict strong-scaling limits and enable a practical runtime decision rule for selecting dynamic or static execution without requiring exhaustive strong-scaling studies or extensive offline tuning.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u4efb\u52a1\u7c92\u5ea6\u8868\u5f81\u6846\u67b6\uff0c\u5c06\u8c03\u5ea6\u5f00\u9500\u589e\u957f\u4e0e\u4efb\u52a1\u56fe\u4f9d\u8d56\u62d3\u6251\u76f4\u63a5\u5173\u8054\uff0c\u8868\u660e\u4f9d\u8d56\u7ed3\u6784\u800c\u975e\u95ee\u9898\u89c4\u6a21\u51b3\u5b9a\u5f00\u9500\u5982\u4f55\u968f\u5e76\u884c\u6027\u6269\u5c55\uff0c\u4f7f\u7cfb\u7edf\u80fd\u51c6\u786e\u9884\u6d4b\u5f3a\u6269\u5c55\u6781\u9650\u5e76\u52a8\u6001\u9009\u62e9\u52a8\u6001\u6216\u9759\u6001\u6267\u884c\u7b56\u7565\u3002", "motivation": "\u4efb\u52a1\u8fd0\u884c\u65f6\u7cfb\u7edf\u867d\u4e3a\u5e76\u884c\u79d1\u5b66\u5e94\u7528\u63d0\u4f9b\u4e86\u7075\u6d3b\u7684\u8d1f\u8f7d\u5747\u8861\u548c\u53ef\u79fb\u690d\u6027\uff0c\u4f46\u5176\u5f3a\u6269\u5c55\u6027\u5bf9\u4efb\u52a1\u7c92\u5ea6\u9ad8\u5ea6\u654f\u611f\u3002\u968f\u7740\u5e76\u884c\u5ea6\u589e\u52a0\uff0c\u8c03\u5ea6\u5f00\u9500\u53ef\u80fd\u4ece\u53ef\u5ffd\u7565\u8f6c\u53d8\u4e3a\u4e3b\u5bfc\uff0c\u5bfc\u81f4\u67d0\u4e9b\u7b97\u6cd5\u6027\u80fd\u6025\u5267\u4e0b\u964d\uff0c\u800c\u5bf9\u5176\u4ed6\u7b97\u6cd5\u5f71\u54cd\u8f83\u5c0f\u3002\u76ee\u524d\u7f3a\u4e4f\u5bf9\u7b97\u6cd5\u7ed3\u6784\u5982\u4f55\u5f71\u54cd\u52a8\u6001\u8c03\u5ea6\u76ca\u5904\u7684\u7cfb\u7edf\u7406\u89e3\u3002", "method": "\u5f15\u5165\u4e86\u7c92\u5ea6\u8868\u5f81\u6846\u67b6\uff0c\u5c06\u8c03\u5ea6\u5f00\u9500\u589e\u957f\u76f4\u63a5\u94fe\u63a5\u5230\u4efb\u52a1\u56fe\u4f9d\u8d56\u62d3\u6251\u3002\u5c55\u793a\u51fa\u4f9d\u8d56\u7ed3\u6784\u800c\u975e\u95ee\u9898\u89c4\u6a21\u5355\u72ec\u51b3\u5b9a\u4e86\u5f00\u9500\u5982\u4f55\u968f\u5e76\u884c\u6027\u6269\u5c55\u3002\u57fa\u4e8e\u6b64\uff0c\u4f7f\u7528\u7b80\u5355\u7684\u7c92\u5ea6\u5ea6\u91cf\u6765\u8868\u5f81\u6267\u884c\u884c\u4e3a\uff0c\u6307\u793a\u4f55\u65f6\u8c03\u5ea6\u5f00\u9500\u53ef\u88ab\u5e76\u884c\u8ba1\u7b97\u5206\u644a\uff0c\u4f55\u65f6\u8c03\u5ea6\u5f00\u9500\u4e3b\u5bfc\u6027\u80fd\u3002", "result": "\u901a\u8fc7\u5728\u5177\u6709\u4e0d\u540c\u4f9d\u8d56\u6a21\u5f0f\u7684\u4ee3\u8868\u6027\u5e76\u884c\u5de5\u4f5c\u8d1f\u8f7d\u4e0a\u7684\u5b9e\u9a8c\u8bc4\u4f30\uff0c\u8bc1\u660e\u6240\u63d0\u8868\u5f81\u80fd\u591f\u89e3\u91ca\u5b9e\u8df5\u4e2d\u89c2\u5bdf\u5230\u7684\u6e10\u53d8\u6027\u548c\u7a81\u53d1\u6027\u5f3a\u6269\u5c55\u5931\u6548\u3002\u8fdb\u4e00\u6b65\u8868\u660e\uff0c\u4ece\u4f9d\u8d56\u62d3\u6251\u5bfc\u51fa\u7684\u5f00\u9500\u6a21\u578b\u80fd\u51c6\u786e\u9884\u6d4b\u5f3a\u6269\u5c55\u6781\u9650\uff0c\u5e76\u5b9e\u73b0\u5b9e\u7528\u7684\u8fd0\u884c\u65f6\u51b3\u7b56\u89c4\u5219\u6765\u9009\u62e9\u52a8\u6001\u6216\u9759\u6001\u6267\u884c\uff0c\u65e0\u9700\u8fdb\u884c\u8be6\u5c3d\u7684\u5f3a\u6269\u5c55\u7814\u7a76\u6216\u5927\u91cf\u79bb\u7ebf\u8c03\u4f18\u3002", "conclusion": "\u8be5\u6846\u67b6\u901a\u8fc7\u7406\u89e3\u7b97\u6cd5\u7ed3\u6784\u5982\u4f55\u5f71\u54cd\u8c03\u5ea6\u5f00\u9500\u6269\u5c55\uff0c\u4e3a\u5e76\u884c\u7cfb\u7edf\u63d0\u4f9b\u4e86\u9884\u6d4b\u6269\u5c55\u6781\u9650\u548c\u4f18\u5316\u6267\u884c\u7b56\u7565\u7684\u7406\u8bba\u57fa\u7840\u548c\u5b9e\u8df5\u65b9\u6cd5\u3002"}}
