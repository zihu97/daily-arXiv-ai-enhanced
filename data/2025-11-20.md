<div id=toc></div>

# Table of Contents

- [cs.SE](#cs.SE) [Total: 14]
- [cs.PF](#cs.PF) [Total: 1]
- [cs.NI](#cs.NI) [Total: 1]
- [cs.DC](#cs.DC) [Total: 2]
- [cs.AR](#cs.AR) [Total: 3]


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [1] [Hybrid Quantum-Classical Machine Learning with PennyLane: A Comprehensive Guide for Computational Research](https://arxiv.org/abs/2511.14786)
*Sidney Shapiro*

Main category: cs.SE

TL;DR: 本文介绍了PennyLane这一Python框架，用于构建和优化混合量子-经典机器学习模型，并展示了其在量子核方法、变分量子本征求解器、投资组合优化及与主流机器学习库集成等方面的应用。


<details>
  <summary>Details</summary>
Motivation: 推动混合量子-经典机器学习的发展，为研究人员提供一个统一、高效的工具，以结合量子计算潜力与经典优化技术。

Method: 利用PennyLane框架实现量子电路与经典机器学习的无缝集成，支持自动微分、量子电路构建及与PyTorch、TensorFlow、JAX等库的协同使用。

Result: 通过具体示例展示了PennyLane在多个领域的应用能力，包括量子化学、金融优化和机器学习任务，验证了其在混合工作流中的实用性与灵活性。

Conclusion: PennyLane作为连接量子计算与经典机器学习的桥梁，是推动量子增强数据科学的重要工具，适合成为相关研究的标准引用。

Abstract: Hybrid quantum-classical machine learning represents a frontier in computational research, combining the potential advantages of quantum computing with established classical optimization techniques. PennyLane provides a Python framework that seamlessly bridges quantum circuits and classical machine learning, enabling researchers to build, optimize, and deploy variational quantum algorithms. This paper introduces PennyLane as a versatile tool for quantum machine learning, optimization, and quantum chemistry applications. We demonstrate use cases including quantum kernel methods, variational quantum eigensolvers, portfolio optimization, and integration with classical ML frameworks such as PyTorch, TensorFlow, and JAX. Through concrete Python examples with widely used libraries such as scikit-learn, pandas, and matplotlib, we show how PennyLane facilitates efficient quantum circuit construction, automatic differentiation, and hybrid optimization workflows. By situating PennyLane within the broader context of quantum computing and machine learning, we highlight its role as a methodological building block for quantum-enhanced data science. Our goal is to provide researchers and practitioners with a concise reference that bridges foundational quantum computing concepts and applied machine learning practice, making PennyLane a default citation for hybrid quantum-classical workflows in Python-based research.

</details>


### [2] [Enabling Predictive Maintenance in District Heating Substations: A Labelled Dataset and Fault Detection Evaluation Framework based on Service Data](https://arxiv.org/abs/2511.14791)
*Cyriana M. A. Roelofs,Edison Guevara Bastidas,Thomas Hugo,Stefan Faulstich,Anna Cadenbach*

Main category: cs.SE

TL;DR: 本文提出了一个开源框架，用于早期检测区域供热换热站故障，包含公开数据集、评估指标（准确性、可靠性、早期性）、基于EnergyFaultDetector的基线结果，并支持使用ARCANA进行根因分析。


<details>
  <summary>Details</summary>
Motivation: 区域供热换热站故障的早期检测对降低回水温度和提升系统效率至关重要，但该领域进展受限于缺乏公开且带标签的数据集。

Method: 构建了一个结合公开服务报告验证数据集、基于Accuracy/Reliability/Earliness三指标的评估方法，以及基于开源Python框架EnergyFaultDetector的基线模型；同时集成ARCANA支持根因分析。

Result: 模型在正常行为识别上准确率达0.98，事件级F-score（β=0.5）达0.83，能在用户报修前检测到60%的故障，平均提前3.9天。

Conclusion: 该框架通过整合开源数据集、评估指标、代码与基线，建立了可复现、以故障为中心且具有运行意义的基准，有助于推动区域供热换热站早期故障检测与诊断方法的发展。

Abstract: Early detection of faults in district heating substations is imperative to reduce return temperatures and enhance efficiency. However, progress in this domain has been hindered by the limited availability of public, labelled datasets. We present an open source framework combining a service report validated public dataset, an evaluation method based on Accuracy, Reliability, and Earliness, and baseline results implemented with EnergyFaultDetector, an open source Python framework.
  The dataset contains time series of operational data from 93 substations across two manufacturers, annotated with a list of disturbances due to faults and maintenance actions, a set of normal-event examples and detailed fault metadata. We evaluate the EnergyFaultDetector using three metrics: Accuracy for recognising normal behaviour, an eventwise F Score for reliable fault detection with few false alarms, and Earliness for early detection. The framework also supports root cause analysis using ARCANA. We demonstrate three use cases to assist operators in interpreting anomalies and identifying underlying faults. The models achieve high normal-behaviour accuracy (0.98) and eventwise F-score (beta=0.5) of 0.83, detecting 60% of the faults in the dataset before the customer reports a problem, with an average lead time of 3.9 days.
  Integrating an open dataset, metrics, open source code, and baselines establishes a reproducible, fault centric benchmark with operationally meaningful evaluation, enabling consistent comparison and development of early fault detection and diagnosis methods for district heating substations.

</details>


### [3] [irace-evo: Automatic Algorithm Configuration Extended With LLM-Based Code Evolution](https://arxiv.org/abs/2511.14794)
*Camilo Chacón Sartori,Christian Blum*

Main category: cs.SE

TL;DR: 本文提出irace-evo，一种结合自动参数调优与大语言模型驱动代码演化的框架，在低计算和经济成本下显著提升了启发式算法性能。


<details>
  <summary>Details</summary>
Motivation: 传统自动算法配置工具（如irace）仅优化参数而无法修改算法代码，限制了性能提升空间；作者希望同时探索参数和代码空间以获得更优的算法变体。

Method: 在irace基础上引入大语言模型进行代码演化，支持多语言、通过渐进式上下文管理降低token消耗，并采用Always-From-Original原则保障代码演化的稳健性。

Result: 在VSBPP问题的CMSA元启发式算法上验证，irace-evo发现的新变体优于当前最优实现，且使用轻量级模型（如Claude Haiku 3.5）总成本低于2欧元。

Conclusion: 将自动配置与LLM驱动的代码演化相结合，是一种高效、低成本推进启发式设计和元启发式优化的有效途径。

Abstract: Automatic algorithm configuration tools such as irace efficiently tune parameter values but leave algorithmic code unchanged. This paper introduces a first version of irace-evo, an extension of irace that integrates code evolution through large language models (LLMs) to jointly explore parameter and code spaces. The proposed framework enables multi-language support (e.g., C++, Python), reduces token consumption via progressive context management, and employs the Always-From-Original principle to ensure robust and controlled code evolution. We evaluate irace-evo on the Construct, Merge, Solve & Adapt (CMSA) metaheuristic for the Variable-Sized Bin Packing Problem (VSBPP). Experimental results show that irace-evo can discover new algorithm variants that outperform the state-of-the-art CMSA implementation while maintaining low computational and monetary costs. Notably, irace-evo generates competitive algorithmic improvements using lightweight models (e.g., Claude Haiku 3.5) with a total usage cost under 2 euros. These results demonstrate that coupling automatic configuration with LLM-driven code evolution provides a powerful, cost-efficient avenue for advancing heuristic design and metaheuristic optimization.

</details>


### [4] [Evaluating Generative AI for CS1 Code Grading: Direct vs Reverse Methods](https://arxiv.org/abs/2511.14798)
*Ahmad Memon,Abdallah Mohamed*

Main category: cs.SE

TL;DR: 本文比较了两种基于大语言模型（LLM）的编程作业自动评分方法：直接应用评分标准的“Direct”方法和先修复代码再根据修复量推断分数的新型“Reverse”方法。实验表明，Reverse 方法在细粒度评估方面更具优势，而 Direct 方法更快速直接；两者均需精心设计提示以处理部分得分和逻辑错误，并在合成与真实学生代码上进行了验证。


<details>
  <summary>Details</summary>
Motivation: 传统编程作业人工评分耗时且不一致，而现有基于单元测试的自动评分通常仅支持二元通过/失败判断，无法给予部分分数。大语言模型的发展为实现更客观、可扩展的自动评分提供了新可能。

Method: 提出并对比两种AI评分方法：Direct（AI直接依据评分标准打分）和Reverse（AI先修复学生代码，再根据修复内容和数量反推分数）。在原始评分尺度及其十倍扩展尺度下，将AI评分结果与人类助教评分进行对比，并使用Gemini Flash 2.0生成的合成代码扩展错误类型和难度范围以测试一致性。

Result: 初步结果表明，Direct方法更快更直接，而Reverse方法通过关注修复工作量能提供更细致的评分。两种方法均对提示工程敏感，尤其在分配部分分数和处理逻辑错误方面表现明显差异。

Conclusion: 两种AI评分方法各有优劣，合理设计提示对提升评分准确性至关重要。未来应探索人机混合评分系统，以兼顾效率、一致性和公平性，推动计算机科学课程评估的改进。

Abstract: Manual grading of programming assignments in introductory computer science courses can be time-consuming and prone to inconsistencies. While unit testing is commonly used for automatic evaluation, it typically follows a binary pass/fail model and does not give partial marks. Recent advances in large language models (LLMs) offer the potential for automated, scalable, and more objective grading.
  This paper compares two AI-based grading techniques: \textit{Direct}, where the AI model applies a rubric directly to student code, and \textit{Reverse} (a newly proposed approach), where the AI first fixes errors, then deduces a grade based on the nature and number of fixes. Each method was evaluated on both the instructor's original grading scale and a tenfold expanded scale to assess the impact of range on AI grading accuracy. To assess their effectiveness, AI-assigned scores were evaluated against human tutor evaluations on a range of coding problems and error types.
  Initial findings suggest that while the Direct approach is faster and straightforward, the Reverse technique often provides a more fine-grained assessment by focusing on correction effort. Both methods require careful prompt engineering, particularly for allocating partial credit and handling logic errors. To further test consistency, we also used synthetic student code generated using Gemini Flash 2.0, which allowed us to evaluate AI graders on a wider range of controlled error types and difficulty levels. We discuss the strengths and limitations of each approach, practical considerations for prompt design, and future directions for hybrid human-AI grading systems that aim to improve consistency, efficiency, and fairness in CS courses.

</details>


### [5] [Scalable and Efficient Large-Scale Log Analysis with LLMs: An IT Software Support Case Study](https://arxiv.org/abs/2511.14803)
*Pranjal Gupta,Karan Bhukar,Harshit Kumar,Seema Nagar,Prateeti Mohapatra,Debanjana Kar*

Main category: cs.SE

TL;DR: 本文提出了一种基于大语言模型（LLM）的日志分析工具，用于自动化处理和诊断IT系统日志，并通过在CPU上高效运行LLM实现大规模日志的快速处理。该工具自2024年3月上线以来已覆盖70个软件产品，处理超2000个工单，每月节省300多人工时及约15,444美元人力成本。


<details>
  <summary>Details</summary>
Motivation: IT环境中日志量巨大，人工检查不现实，亟需自动化日志分析以提升软件支持效率。

Method: 利用大语言模型（LLM）进行日志数据处理与问题诊断，并提出一种在CPU上高效运行LLM的新方法，以在保证输出质量的同时快速处理海量日志。

Result: 工具已在生产环境中部署，覆盖70个软件产品，处理2000多个工单，每月节省300+人工时和约15,444美元的人力成本。

Conclusion: 基于LLM的日志分析工具能显著提升IT支持效率，在保证诊断质量的同时大幅降低人力成本，且可在CPU上高效运行，适用于大规模实际部署。

Abstract: IT environments typically have logging mechanisms to monitor system health and detect issues. However, the huge volume of generated logs makes manual inspection impractical, highlighting the importance of automated log analysis in IT Software Support. In this paper, we propose a log analytics tool that leverages Large Language Models (LLMs) for log data processing and issue diagnosis, enabling the generation of automated insights and summaries. We further present a novel approach for efficiently running LLMs on CPUs to process massive log volumes in minimal time without compromising output quality. We share the insights and lessons learned from deployment of the tool - in production since March 2024 - scaled across 70 software products, processing over 2000 tickets for issue diagnosis, achieving a time savings of 300+ man hours and an estimated $15,444 per month in manpower costs compared to the traditional log analysis practices.

</details>


### [6] [Automatic Pipeline Provisioning](https://arxiv.org/abs/2511.14825)
*Alexandre-Xavier Labonté-Lamoureux,Simon Boyer*

Main category: cs.SE

TL;DR: 本文探讨了自动流水线配置的优势及其在CI（持续集成）中的应用，指出其在CD（持续交付）中也可能具有类似效果。


<details>
  <summary>Details</summary>
Motivation: 探索自动流水线配置的好处，并明确其在软件工程项目中的适用方式。

Method: 对自动流水线配置过程进行分析，重点研究其在CI流水线中的应用。

Result: 尚未提供具体结果，但预期该方法在CD流水线中也会产生类似效果。

Conclusion: 自动流水线配置能快速部署软件工程项目的流水线，在CI中具有明显优势，并可能同样适用于CD。

Abstract: The goal of this paper is to explore the benefits of automatic pipeline provisioning and identify how it can be applied. Automatic pipeline provisioning can be defined as a process of quickly deploying a pipeline for a software engineering project. This research will focus on CI pipelines, although the outcomes of this approach on CD pipelines will likely be similar.

</details>


### [7] [MermaidSeqBench: An Evaluation Benchmark for LLM-to-Mermaid Sequence Diagram Generation](https://arxiv.org/abs/2511.14967)
*Basel Shbita,Farhan Ahmed,Chad DeLuca*

Main category: cs.SE

TL;DR: 本文提出了MermaidSeqBench，一个用于评估大语言模型（LLM）从自然语言生成Mermaid序列图能力的新基准，包含132个人工验证并由LLM扩展的样本，并采用细粒度指标和LLM-as-a-judge方法进行评估。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏系统性评估大语言模型在从自然语言生成结构化图表（如Mermaid序列图）任务中正确性的基准，限制了该领域的研究进展。

Method: 构建了一个包含132个样本的基准数据集MermaidSeqBench，通过人工标注、上下文提示LLM生成和基于规则的变体扩增相结合的方式扩展数据；采用LLM-as-a-judge模型，从语法正确性、激活处理、错误处理和实用性等细粒度维度进行评估。

Result: 对多个前沿LLM的初步评估揭示了不同模型和评估模式之间存在显著的能力差距，验证了该基准的有效性和灵活性。

Conclusion: MermaidSeqBench为结构化图表生成研究提供了坚实基础，并推动了更严格、细粒度评估方法的发展。

Abstract: Large language models (LLMs) have demonstrated excellent capabilities in generating structured diagrams from natural language descriptions. In particular, they have shown great promise in generating sequence diagrams for software engineering, typically represented in a text-based syntax such as Mermaid. However, systematic evaluations in this space remain underdeveloped as there is a lack of existing benchmarks to assess the LLM's correctness in this task. To address this shortcoming, we introduce MermaidSeqBench, a human-verified and LLM-synthetically-extended benchmark for assessing an LLM's capabilities in generating Mermaid sequence diagrams from textual prompts. The benchmark consists of a core set of 132 samples, starting from a small set of manually crafted and verified flows. These were expanded via a hybrid methodology combining human annotation, in-context LLM prompting, and rule-based variation generation. Our benchmark uses an LLM-as-a-judge model to assess Mermaid sequence diagram generation across fine-grained metrics, including syntax correctness, activation handling, error handling, and practical usability. We perform initial evaluations on numerous state-of-the-art LLMs and utilize multiple LLM judge models to demonstrate the effectiveness and flexibility of our benchmark. Our results reveal significant capability gaps across models and evaluation modes. Our proposed benchmark provides a foundation for advancing research in structured diagram generation and for developing more rigorous, fine-grained evaluation methodologies.

</details>


### [8] [Finetuning LLMs for Automatic Form Interaction on Web-Browser in Selenium Testing Framework](https://arxiv.org/abs/2511.15168)
*Nguyen-Khang Le,Nguyen Hiep,Minh Nguyen,Son Luu,Trung Vo,Quan Bui,Nomura Shoshin,Le-Minh Nguyen*

Main category: cs.SE

TL;DR: 本文提出了一种训练大语言模型（LLM）生成高质量Selenium表单交互测试用例的新方法，构建了合成与人工标注数据集，并在语法正确性、脚本可执行性和字段覆盖率等指标上显著优于现有基线模型。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏针对大语言模型在Web表单交互测试任务上的系统性研究和公开基准数据集，限制了LLM在自动化Web测试中的应用。

Method: 构建包含合成与人工标注的多样化表单测试数据集，定义语法正确性、脚本可执行性和输入字段覆盖率等评估指标，用于训练和评估LLM生成Selenium测试脚本的能力。

Result: 所提方法在所有评估指标上均显著优于GPT-4o等强基线模型，验证了其在生成高质量表单交互测试脚本方面的有效性。

Conclusion: 该研究为基于LLM的Web自动化测试奠定了基础，并提供了可用于后续研究的数据集与评估框架。

Abstract: Automated web application testing is a critical component of modern software development, with frameworks like Selenium widely adopted for validating functionality through browser automation. Among the essential aspects of such testing is the ability to interact with and validate web forms, a task that requires syntactically correct, executable scripts with high coverage of input fields. Despite its importance, this task remains underexplored in the context of large language models (LLMs), and no public benchmark or dataset exists to evaluate LLMs on form interaction generation systematically. This paper introduces a novel method for training LLMs to generate high-quality test cases in Selenium, specifically targeting form interaction testing. We curate both synthetic and human-annotated datasets for training and evaluation, covering diverse real-world forms and testing scenarios. We define clear metrics for syntax correctness, script executability, and input field coverage. Our empirical study demonstrates that our approach significantly outperforms strong baselines, including GPT-4o and other popular LLMs, across all evaluation metrics. Our work lays the groundwork for future research on LLM-based web testing and provides resources to support ongoing progress in this area.

</details>


### [9] [From Code Smells to Best Practices: Tackling Resource Leaks in PyTorch, TensorFlow, and Keras](https://arxiv.org/abs/2511.15229)
*Bashar Abdallah,Martyna E. Wojciechowska,Gustavo Santos,Edmand Yu,Maxime Lamothe,Alain Abran,Mohammad Hamdaqa*

Main category: cs.SE

TL;DR: 本研究首次系统识别了PyTorch、TensorFlow和Keras中导致资源泄漏的代码坏味道，归纳出46种坏味道并提出50项最佳实践，以提升机器学习应用的资源效率与可持续性。


<details>
  <summary>Details</summary>
Motivation: 现有机器学习研究多关注模型性能指标，忽视了长期可持续性与资源效率；高效资源管理对稳健部署同样关键，因此需系统识别并解决ML应用中的资源泄漏问题。

Method: 通过实证分析开发者讨论与真实代码片段，识别PyTorch、TensorFlow和Keras中的资源泄漏相关代码坏味道，并按根本原因及通用/框架特性进行分类；为每种坏味道提炼至少一项最佳实践，并通过三阶段验证流程（三位作者独立分析+共识讨论）确保结果有效性。

Result: 识别出30种PyTorch相关坏味道和16种TensorFlow/Keras坏味道，共提出50项推荐编码模式，有效减少资源泄漏并提升效率。

Conclusion: 本研究首次全面考察主流ML框架中引发资源泄漏的代码坏味道，并提供可操作的最佳实践，有助于开发者构建更高效、可持续的机器学习应用，同时揭示资源泄漏的根本成因。

Abstract: Much of the existing ML research focuses on model performance metrics, leaving limited attention to the long-term sustainability and resource efficiency of ML applications. While high performance is essential, ensuring efficient resource management is equally critical for robust deployment. This study addresses this gap by systematically identifying code smells that lead to resource leaks in ML applications. We conducted an empirical investigation of developer discussions and real-world code snippets from PyTorch, TensorFlow, and Keras. The analysis identified 30 PyTorch-related smells and 16 TensorFlow/Keras smells linked to resource leaks. These smells were categorized in two ways: (1) based on their root causes, and (2) as general ML smells with framework-specific characteristics. For each smell, we derived at least one best practice, resulting in 50 recommended coding patterns aimed at reducing resource leakage and improving efficiency. To ensure the validity of our findings, we employed a three-phase validation process involving independent analysis by three authors followed by consensus discussions. This is the first comprehensive study to examine resource-leak-inducing code smells across major ML frameworks and to present actionable best practices for mitigating them. The contributions support developers in building more efficient and sustainable ML applications and offer a structured view of the underlying causes of resource leaks.

</details>


### [10] [M, Toolchain and Language for Reusable Model Compilation](https://arxiv.org/abs/2511.15257)
*Hiep Hong Trinh,Federico Ciccozzi,Abu Naser Masud,Marjan Sirjani,Mikael Sjödin*

Main category: cs.SE

TL;DR: 本文提出了一种名为 M 的建模语言和工具链，用于支持复杂并发、时间感知系统的多目标编译，能够从统一的高层模型生成仿真、部署和形式化验证等多种目标产物。


<details>
  <summary>Details</summary>
Motivation: 现有建模语言通常只针对单一用途（如仿真或实现），缺乏对多目标编译的支持；若在语言设计初期未考虑多目标编译，后续难以有效实现。因此需要一种从设计之初就支持多目标编译的建模语言。

Method: M 是一种基于参与者模型（actor model）并扩展了离散事件调度语义的文本型、语法驱动建模语言，提供系统实体、基于消息的交互以及时间/状态触发反应的建模构造，并通过其编译框架生成多种目标产物。

Result: M 能够从统一的源模型系统性地生成多种异构目标产物，同时保持与原始模型的语义一致性，并可作为中间语言供其他建模语言接入其编译框架。

Conclusion: M 为复杂并发、时间感知系统的模型驱动工程提供了一种支持多目标编译的统一建模语言和工具链，提升了开发效率与安全性，并具有良好的扩展性和互操作性。

Abstract: Complex software-driven systems often interleave distributed, concurrent computation processes with physical interactions with the environment. Developing these systems more efficiently and safely can be achieved by employing actionable, software-based models. From a high-level system model, engineers often need to derive multiple specialized models for different purposes, including simulation, deployment, and formal verification. Each of these target models usually rely on its own formalism, specification language, and execution platform. Traditionally, a compiler analyzes a program written in a programming language and generates executable code. In contrast, a model compiler processes a source model written in a modeling language and should ideally support the generation of multiple heterogeneous targets. However, most existing modeling languages are designed with a narrow focus, typically targeting only simulation or implementation. Multi-target compilation, when not considered during the language's early design, becomes significantly harder to achieve. In this paper, we introduce our initiative: a toolchain and modeling language called M, designed to support system modeling and multi-target compilation for model-driven engineering of complex, concurrent, and time-aware systems. M is a textual, grammar-driven language based on the actor model and extended with discrete-event scheduling semantics. It provides constructs for modeling system entities, message-based interactions, and time- or state-triggered reactions. From such models, M enables the systematic generation of diverse target artifacts while preserving semantic conformance to the original model. Moreover, M can serve as a middle language to which other modeling languages may anchor, thereby allowing them to benefit from its compilation framework.

</details>


### [11] [A Viable Paradigm of Software Automation: Iterative End-to-End Automated Software Development](https://arxiv.org/abs/2511.15293)
*Jia Li,Zhi Jin,Kechi Zhang,Huangzhao Zhang,Jiaru Qian,Tiankuo Zhao*

Main category: cs.SE

TL;DR: 本文提出了一种端到端自动化的软件开发范式AutoSW，通过分析-规划-实现-交付的循环，使AI系统作为主要参与者将自然语言描述的人类意图转化为可执行软件，并通过原型验证了其可行性。


<details>
  <summary>Details</summary>
Motivation: 当前软件开发自动化仍需大量人工参与，尽管AI被用作辅助工具，但尚未实现全流程自动化。作者旨在探索一种AI深度参与、覆盖全栈开发的端到端自动化范式。

Method: 提出名为AutoSW的迭代式端到端自动化软件开发范式，采用“分析-规划-实现-交付”循环机制，构建轻量级原型并在多种代表性案例中进行初步实验。

Result: 实验结果表明，AutoSW能够成功生成可执行软件，验证了该范式在实现真正端到端自动化软件开发方面的可行性。

Conclusion: AI有望成为软件开发生命周期中的核心参与者，AutoSW为实现全流程自动化提供了一个有前景的方向。

Abstract: Software development automation is a long-term goal in software engineering. With the development of artificial intelligence (AI), more and more researchers are exploring approaches to software automation. They view AI systems as tools or assistants in software development, still requiring significant human involvement. Another initiative is ``vibe coding'', where AI systems write and repeatedly revise most (or even all) of the code. We foresee these two development paths will converge towards the same destination: AI systems participate in throughout the software development lifecycle, expanding boundaries of full-stack software development. In this paper, we present a vision of an iterative end-to-end automated software development paradigm AutoSW. It operates in an analyze-plan-implement-deliver loop, where AI systems as human partners become first-class actors, translating human intentions expressed in natural language into executable software. We explore a lightweight prototype across the paradigm and initially execute various representative cases. The results indicate that AutoSW can successfully deliver executable software, providing a feasible direction for truly end-to-end automated software development.

</details>


### [12] [From Machine Learning Documentation to Requirements: Bridging Processes with Requirements Languages](https://arxiv.org/abs/2511.15340)
*Yi Peng,Hans-Martin Heyn,Jennifer Horkoff*

Main category: cs.SE

TL;DR: 该研究探讨了如何从ModelCards和DataSheets等机器学习文档中提取与需求工程（RE）相关的信息，并评估了三种传统RE表示方法（EARS、Rupp模板和Volere）在将其结构化为需求方面的有效性，结果表明这些文档包含大量RE相关信息，且可有效转化为结构化需求。


<details>
  <summary>Details</summary>
Motivation: 在面向机器学习系统的软件工程过程中，传统需求工程方法难以有效规范ML组件（包括模型和数据）的需求，而ML文档（如ModelCards和DataSheets）作为潜在的信息来源尚未被充分探索。

Method: 首先分析20份公开的ModelCards和DataSheets中RE相关信息的数量与性质；然后评估EARS、Rupp模板和Volere三种RE表示方法在将这些信息结构化为需求方面的效果。

Result: ML文档中包含大量与需求工程相关的信息，且可通过现有RE表示方法有效转化为结构化需求。

Conclusion: 将ML文档纳入软件工程流程是可行的，有助于将ML特定知识转化为结构化需求，从而支持ML系统的集成与验证。

Abstract: In software engineering processes for machine learning (ML)-enabled systems, integrating and verifying ML components is a major challenge. A prerequisite is the specification of ML component requirements, including models and data, an area where traditional requirements engineering (RE) processes face new obstacles. An underexplored source of RE-relevant information in this context is ML documentation such as ModelCards and DataSheets. However, it is uncertain to what extent RE-relevant information can be extracted from these documents. This study first investigates the amount and nature of RE-relevant information in 20 publicly available ModelCards and DataSheets. We show that these documents contain a significant amount of potentially RE-relevant information. Next, we evaluate how effectively three established RE representations (EARS, Rupp's template, and Volere) can structure this knowledge into requirements. Our results demonstrate that there is a pathway to transform ML-specific knowledge into structured requirements, incorporating ML documentation in software engineering processes for ML systems.

</details>


### [13] [MutDafny: A Mutation-Based Approach to Assess Dafny Specifications](https://arxiv.org/abs/2511.15403)
*Isabel Amaral,Alexandra Mendes,José Campos*

Main category: cs.SE

TL;DR: 本文提出 MutDafny，一种基于变异测试的工具，用于检测 Dafny 形式化规约中的弱点；通过引入32种变异算子，在794个真实程序中发现平均每241行代码就存在一个需加强的弱规约。


<details>
  <summary>Details</summary>
Motivation: Dafny 等验证感知编程语言中的形式化规约容易出错，可能导致“已验证”程序行为偏离预期，因此需要有效方法检测规约中的缺陷。

Method: 开发 MutDafny 工具，结合现有变异算子与从 GitHub Dafny 项目 bug 修复提交中提炼的新算子，共实现32种适用于 Dafny 的变异操作；通过变异测试，若带变异体的程序仍能通过验证，则表明规约可能存在弱点。

Result: 在794个真实 Dafny 程序上评估 MutDafny，手动分析未被检测到的变异体，发现5个真实弱规约，平均约每241行代码出现一个。

Conclusion: MutDafny 能有效识别 Dafny 规约中的潜在弱点，提升形式化规约的可靠性，为开发者提供实用的规约强化支持。

Abstract: This paper explores the use of mutation testing to reveal weaknesses in formal specifications written in Dafny. In verification-aware programming languages, such as Dafny, despite their critical role, specifications are as prone to errors as implementations. Flaws in specs can result in formally verified programs that deviate from the intended behavior.
  We present MutDafny, a tool that increases the reliability of Dafny specifications by automatically signaling potential weaknesses. Using a mutation testing approach, we introduce faults (mutations) into the code and rely on formal specifications for detecting them. If a program with a mutant verifies, this may indicate a weakness in the specification. We extensively analyze mutation operators from popular tools, identifying the ones applicable to Dafny. In addition, we synthesize new operators tailored for Dafny from bugfix commits in publicly available Dafny projects on GitHub. Drawing from both, we equipped our tool with a total of 32 mutation operators. We evaluate MutDafny's effectiveness and efficiency in a dataset of 794 real-world Dafny programs and we manually analyze a subset of the resulting undetected mutants, identifying five weak real-world specifications (on average, one at every 241 lines of code) that would benefit from strengthening.

</details>


### [14] [Quantum-Guided Test Case Minimization for LLM-Based Code Generation](https://arxiv.org/abs/2511.15665)
*Huixiang Zhang,Mahzabeen Emu*

Main category: cs.SE

TL;DR: 本文提出一种基于测试驱动开发（TDD）的框架，将代码生成问题转化为组合优化任务，利用量子退火求解测试用例最小化问题，显著提升代码生成效率与质量。


<details>
  <summary>Details</summary>
Motivation: 精确控制大语言模型（LLM）生成高效简洁的代码是软件工程中的核心挑战。

Method: 该框架首先引导LLM生成测试套件，然后将测试用例最小化（TCM）问题建模为二次无约束二值优化（QUBO）模型，并使用经典求解器或量子退火硬件进行求解。

Result: 实验表明，量子退火在核心TCM任务上比模拟退火快16倍，整体框架减少36.5%的token消耗并显著提升代码质量。

Conclusion: 本研究展示了生成式AI与组合优化在软件工程中的强大协同效应，强调了精确模型构建的重要性。

Abstract: Precisely controlling Large Language Models (LLMs) to generate efficient and concise code is a central challenge in software engineering. We introduce a framework based on Test-Driven Development (TDD) that transforms code specification into a combinatorial optimization task. The framework first prompts an LLM to generate a test suite, then formulates the Test Case Minimization (TCM) problem as a Quadratic Unconstrained Binary Optimization (QUBO) model. This QUBO paradigm is compatible with both classical solvers and emerging hardware such as quantum annealers. Experimentally, quantum annealing solves the core TCM task 16 times faster than simulated annealing. This performance underpins our end-to-end framework, which reduces total token consumption by 36.5\% and significantly improves code quality. This work demonstrates a powerful synergy between generative AI and combinatorial optimization in software engineering, highlighting the critical importance of precise model formulation.

</details>


<div id='cs.PF'></div>

# cs.PF [[Back]](#toc)

### [15] [Dynamic Expert Quantization for Scalable Mixture-of-Experts Inference](https://arxiv.org/abs/2511.15015)
*Kexin Chu,Dawei Xiang,Zixu Shen,Yiwei Yang,Zecheng Liu,Wei Zhang*

Main category: cs.PF

TL;DR: DynaExq 是一种针对 Mixture-of-Experts（MoE）模型的动态量化运行时系统，通过动态调整专家精度、异步切换精度和无碎片内存池，在有限 GPU 显存下显著提升推理准确率。


<details>
  <summary>Details</summary>
Motivation: 静态后训练量化无法适应 MoE 模型中专家激活模式的变化，导致在高压缩比下精度下降；同时，MoE 模型庞大的内存占用限制了其在消费级 GPU 上的部署。

Method: DynaExq 引入三项关键技术：(1) 基于热度感知的精度控制器，根据长期激活统计动态调整专家位宽；(2) 完全异步的精度切换流水线，将精度升降操作与 MoE 计算重叠；(3) 无碎片内存池机制，支持混合精度专家的确定性内存分配。

Result: 在 Qwen3-30B 和 Qwen3-80B MoE 模型及六个基准测试上，DynaExq 能在单张 RTX 5090 或 A6000 GPU 上部署大模型，并相比静态低精度基线最多提升 4.03 点准确率。

Conclusion: 面向工作负载的自适应量化是解决显存受限环境下 MoE 模型高效部署的有效策略。

Abstract: Mixture-of-Experts (MoE) models scale LLM capacity efficiently, but deployment on consumer GPUs is limited by the large memory footprint of inactive experts. Static post-training quantization reduces storage costs but cannot adapt to shifting activation patterns, causing accuracy loss under aggressive compression. So we present DynaExq, a runtime system that treats expert precision as a first-class, dynamically managed resource. DynaExq combines (1) a hotness-aware precision controller that continuously aligns expert bit-widths with long-term activation statistics, (2) a fully asynchronous precision-switching pipeline that overlaps promotion and demotion with MoE computation, and (3) a fragmentation-free memory pooling mechanism that supports hybrid-precision experts with deterministic allocation. Together, these components enable stable, non-blocking precision transitions under strict HBM budgets.
  Across Qwen3-30B and Qwen3-80B MoE models and six representative benchmarks, DynaExq deploys large LLMs on single RTX 5090 and A6000 GPUs and improves accuracy by up to 4.03 points over static low-precision baselines. The results show that adaptive, workload-aware quantization is an effective strategy for memory-constrained MoE serving.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [16] [RAID: In-Network RA Signaling Storm Detection for 5G Open RAN](https://arxiv.org/abs/2511.14921)
*Mohamed Rouili,Yang Xiao,Sihang Liu,Raouf Boutaba*

Main category: cs.NI

TL;DR: 本文提出RAID系统，利用P4可编程交换机ASIC在数据平面内实现基于机器学习的随机接入信令风暴实时检测与缓解，显著提升5G O-RAN控制面的安全性与响应速度。


<details>
  <summary>Details</summary>
Motivation: 5G开放无线接入网（O-RAN）中，随机接入信令风暴会迅速耗尽中央单元（CU）处理能力，导致大规模连接失败；现有基于n-RT RIC的检测方法因通用处理器架构延迟高，无法及时应对。

Method: RAID将轻量级随机森林分类器嵌入Tofino可编程交换机，在数据平面以线速对流量进行分类，实现微秒级确定性推理延迟，并在恶意请求到达RRC前进行过滤。

Result: RAID在多种流量负载下均能保持94%以上的检测准确率，单流推理延迟约为3.4微秒，满足O-RAN控制面严格时延要求。

Conclusion: RAID是一种快速、可扩展的解决方案，能够有效检测和缓解5G O-RAN中的信令风暴攻击，保障低时延关键业务的服务质量。

Abstract: The disaggregation and virtualization of 5G Open RAN (O-RAN) introduces new vulnerabilities in the control plane that can greatly impact the quality of service (QoS) of latency-sensitive 5G applications and services. One critical issue is Random Access (RA) signaling storms where, a burst of illegitimate or misbehaving user equipments (UEs) send Radio Resource Control (RRC) connection requests that rapidly saturate a Central Unit's (CU) processing pipeline. Such storms trigger widespread connection failures within the short contention resolution window defined by 3GPP. Existing detection and mitigation approaches based on near-real-time RAN Intelligent Controller (n-RT RIC) applications cannot guarantee a timely reaction to such attacks as RIC control loops incur tens to hundreds of milliseconds of latency due to the non-deterministic nature of their general purpose processor (GPP) based architectures. This paper presents RAID, an in-network RA signaling storm detection and mitigation system that leverages P4-programmable switch ASICs to enable real-time protection from malicious attacks. RAID embeds a lightweight Random Forest (RF) classifier into a programmable Tofino switch, enabling line-rate flow classification with deterministic microsecond-scale inference delay. By performing ML-based detection directly in the data plane, RAID catches and filters malicious RA requests before they reach and overwhelm the RRC. RAID achieves above 94% detection accuracy with a fixed per-flow inference delay on the order of 3.4 microseconds, effectively meeting strict O-RAN control-plane deadlines. These improvements are sustained across multiple traffic loads, making RAID a fast and scalable solution for the detection and mitigation of signaling storms in 5G O-RAN.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [17] [A Graph-Based, Distributed Memory, Modeling Abstraction for Optimization](https://arxiv.org/abs/2511.14966)
*David L. Cole,Jordan Jalving,Jonah Langlieb,Jesse D. Jenkins*

Main category: cs.DC

TL;DR: 本文提出了 RemoteOptiGraph 抽象模型，用于在分布式内存环境中构建和求解优化问题，并通过 Benders 分解在大规模实例上实现了 7.5 倍加速。


<details>
  <summary>Details</summary>
Motivation: 现有优化建模方法在分布式内存系统中缺乏统一、灵活的抽象，导致需为不同问题定制建模方式，限制了通用算法的开发与应用。

Method: 基于 Plasmo.jl 中的 OptiGraph 模型，引入 RemoteOptiGraph 抽象，利用 InterWorkerEdges 管理跨工作节点的链接约束，支持如 Benders 或拉格朗日分解等通用元算法。

Result: 在包含超过 1200 万个变量和约束的美国西部混合整数容量扩展模型上，RemoteOptiGraph 结合 Benders 分解比无分解方法快 7.5 倍。

Conclusion: RemoteOptiGraph 提供了一种统一且高效的分布式优化建模框架，显著提升了大规模问题的求解效率，并为通用分布式优化算法奠定了基础。

Abstract: We present a general, flexible modeling abstraction for building and working with distributed optimization problems called a RemoteOptiGraph. This abstraction extends the OptiGraph model in Plasmo$.$jl, where optimization problems are represented as hypergraphs with nodes that define modular subproblems (variables, constraints, and objectives) and edges that encode algebraic linking constraints between nodes. The RemoteOptiGraph allows OptiGraphs to be utilized in distributed memory environments through InterWorkerEdges, which manage linking constraints that span workers. This abstraction offers a unified approach for modeling optimization problems on distributed memory systems (avoiding bespoke modeling approaches), and provides a basis for developing general-purpose meta-algorithms that can exploit distributed memory structure such as Benders or Lagrangian decompositions. We implement this abstraction in the open-source package, Plasmo$.$jl and we illustrate how it can be used by solving a mixed integer capacity expansion model for the western United States containing over 12 million variables and constraints. The RemoteOptiGraph abstraction together with Benders decomposition performs 7.5 times faster than solving the same problem without decomposition.

</details>


### [18] [BlueBottle: Fast and Robust Blockchains through Subsystem Specialization](https://arxiv.org/abs/2511.15361)
*Preston Vander Vos,Alberto Sonnino,Giorgos Tsimos,Philipp Jovanovic,Lefteris Kokoris-Kogias*

Main category: cs.DC

TL;DR: BlueBottle 是一种两层区块链共识架构，在保持高吞吐和强安全性的前提下，实现亚秒级确认延迟。


<details>
  <summary>Details</summary>
Motivation: 区块链共识存在安全性、延迟与去中心化之间的三难困境。现有高吞吐系统常牺牲去中心化或对强敌手的鲁棒性，而高度去中心化且安全的系统则性能较低。

Method: 提出 BlueBottle 两层共识架构：核心层 BB-Core 使用 n=5f+1 的协议，在中等规模验证者集合下以降低容错能力换取更低的最终性延迟；守护层 BB-Guard 提供去中心化时间戳、主动检测 BB-Core 中的违规行为，并在发生安全或活性故障时提供同步恢复路径。

Result: 实验表明 BB-Core 相比 Mysticeti 降低 20–25% 的延迟；整体架构在温和同步假设下实现高吞吐、亚秒级乐观最终性，并保持强安全性和活性。

Conclusion: BlueBottle 通过分层设计有效缓解了区块链共识三难问题，在性能、安全与去中心化之间取得良好平衡。

Abstract: Blockchain consensus faces a trilemma of security, latency, and decentralization. High-throughput systems often require a reduction in decentralization or robustness against strong adversaries, while highly decentralized and secure systems tend to have lower performance. We present BlueBottle, a two-layer consensus architecture. The core layer, BB-Core, is an n=5f+1 protocol that trades some fault tolerance for a much lower finality latency with a medium-sized core validator set. Our experiments show that BB-Core reduces latency by 20-25% in comparison to Mysticeti. The guard layer, BB-Guard, provides decentralized timestamping, proactive misbehavior detection in BB-Core, and a synchronous recovery path. When it observes equivocations or liveness failures in the core -- while tolerating up to f<3n/5 faulty nodes in the primary layer -- guard validators disseminate evidence, agree on misbehaving parties for exclusion or slashing, and either restart the core protocol (for liveness violations) or select a canonical fork (for safety violations). Together, these layers enable optimistic sub-second finality at high throughput while maintaining strong safety and liveness under a mild synchrony assumption.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [19] [CoroAMU: Unleashing Memory-Driven Coroutines through Latency-Aware Decoupled Operations](https://arxiv.org/abs/2511.14990)
*Zhuolun Jiang,Songyue Wang,Xiaokun Pei,Tianyue Lu,Mingyu Chen*

Main category: cs.AR

TL;DR: 本文提出了CoroAMU，一种软硬件协同设计的内存中心化协程系统，通过编译器优化与增强型异步内存单元，在FPGA平台上显著提升了协程在高延迟解耦内存系统中的性能。


<details>
  <summary>Details</summary>
Motivation: 现代数据密集型应用在解耦内存系统中面临严重的内存延迟问题，现有协程方法难以在隐藏延迟效率与运行时开销之间取得良好平衡。

Method: CoroAMU结合了编译器优化（如协程代码生成、上下文最小化和请求合并）与硬件支持（包括解耦内存操作、协程专用内存指令及新型内存引导分支预测机制），并在LLVM和开源XiangShan RISC-V处理器上实现。

Result: 实验表明，仅使用CoroAMU编译器在Intel服务器处理器上比现有先进协程方法快1.51倍；结合优化硬件后，在FPGA模拟的解耦内存系统中，分别在200ns和800ns延迟下获得3.39倍和4.87倍的平均性能提升。

Conclusion: CoroAMU通过软硬件协同设计有效解决了协程在高延迟内存环境下的性能瓶颈，为未来内存为中心的计算架构提供了可行方案。

Abstract: Modern data-intensive applications face memory latency challenges exacerbated by disaggregated memory systems. Recent work shows that coroutines are promising in effectively interleaving tasks and hiding memory latency, but they struggle to balance latency-hiding efficiency with runtime overhead. We present CoroAMU, a hardware-software co-designed system for memory-centric coroutines. It introduces compiler procedures that optimize coroutine code generation, minimize context, and coalesce requests, paired with a simple interface. With hardware support of decoupled memory operations, we enhance the Asynchronous Memory Unit to further exploit dynamic coroutine schedulers by coroutine-specific memory operations and a novel memory-guided branch prediction mechanism. It is implemented with LLVM and open-source XiangShan RISC-V processor over the FPGA platform. Experiments demonstrate that the CoroAMU compiler achieves a 1.51x speedup over state-of-the-art coroutine methods on Intel server processors. When combined with optimized hardware of decoupled memory access, it delivers 3.39x and 4.87x average performance improvements over the baseline processor on FPGA-emulated disaggregated systems under 200ns and 800ns latency respectively.

</details>


### [20] [A Tensor Compiler for Processing-In-Memory Architectures](https://arxiv.org/abs/2511.15503)
*Peiming Yang,Sankeerth Durvasula,Ivan Fernandez,Mohammad Sadrosadati,Onur Mutlu,Gennady Pekhimenko,Christina Giannoula*

Main category: cs.AR

TL;DR: 本文提出DCC，首个面向PIM系统的以数据为中心的机器学习编译器，通过联合优化数据重排与计算代码，在多种PIM后端上显著加速ML内核和LLM推理。


<details>
  <summary>Details</summary>
Motivation: 现有编译方法在面向多样化PIM后端时缺乏对ML内核的系统性优化，尤其忽视了数据重排与计算代码之间的相互依赖关系，导致性能和可编程性受限。

Method: 设计DCC编译器，引入多层PIM抽象，将数据划分策略映射到计算循环划分，结合PIM特定代码优化，并利用快速准确的性能预测模型进行统一调优。

Result: 在HBM-PIM和AttAcc PIM后端上，DCC相比纯GPU执行在单个ML内核上分别最高提速7.68倍和13.17倍；在端到端LLM推理中，对GPT-3和LLaMA-2最高提速7.71倍。

Conclusion: DCC通过联合优化数据重排与计算，有效解决了PIM系统中因数据布局差异带来的性能瓶颈，显著提升了ML模型尤其是LLM在异构PIM架构上的执行效率。

Abstract: Processing-In-Memory (PIM) devices integrated with high-performance Host processors (e.g., GPUs) can accelerate memory-intensive kernels in Machine Learning (ML) models, including Large Language Models (LLMs), by leveraging high memory bandwidth at PIM cores. However, Host processors and PIM cores require different data layouts: Hosts need consecutive elements distributed across DRAM banks, while PIM cores need them within local banks. This necessitates data rearrangements in ML kernel execution that pose significant performance and programmability challenges, further exacerbated by the need to support diverse PIM backends. Current compilation approaches lack systematic optimization for diverse ML kernels across multiple PIM backends and may largely ignore data rearrangements during compute code optimization. We demonstrate that data rearrangements and compute code optimization are interdependent, and need to be jointly optimized during the tuning process. To address this, we design DCC, the first data-centric ML compiler for PIM systems that jointly co-optimizes data rearrangements and compute code in a unified tuning process. DCC integrates a multi-layer PIM abstraction that enables various data distribution and processing strategies on different PIM backends. DCC enables effective co-optimization by mapping data partitioning strategies to compute loop partitions, applying PIM-specific code optimizations and leveraging a fast and accurate performance prediction model to select optimal configurations. Our evaluations in various individual ML kernels demonstrate that DCC achieves up to 7.68x speedup (2.7x average) on HBM-PIM and up to 13.17x speedup (5.75x average) on AttAcc PIM backend over GPU-only execution. In end-to-end LLM inference, DCC on AttAcc accelerates GPT-3 and LLaMA-2 by up to 7.71x (4.88x average) over GPU.

</details>


### [21] [Toward Open-Source Chiplets for HPC and AI: Occamy and Beyond](https://arxiv.org/abs/2511.15564)
*Paul Scheffler,Thomas Benz,Tim Fischer,Lorenzo Leone,Sina Arjmandpour,Luca Benini*

Main category: cs.AR

TL;DR: 本文提出了一个面向高性能计算和人工智能的开源Chiplet架构RISC-V系统发展路线图，从12nm双芯粒Occamy起步，逐步演进至7nm四芯粒Ogopogo，并探讨了将开放性扩展到仿真、EDA、PDK和片外PHY等领域的可能性。


<details>
  <summary>Details</summary>
Motivation: 缩小开源RISC-V系统与专有设计在高性能计算和人工智能应用中的性能差距。

Method: 基于开源Chiplet架构，从已流片验证的12nm双芯粒RISC-V多核系统Occamy出发，逐步演进至采用Mesh-NoC的Ramora以及7nm四芯粒概念架构Ogopogo，并探索将开放性延伸至RTL以外的工具链和物理层接口。

Result: 实现了从12nm到7nm工艺节点的多代开源Chiplet RISC-V系统设计，其中Ogopogo达到业界领先的计算密度。

Conclusion: 开源Chiplet-based RISC-V架构具备实现高性能计算和AI应用的潜力，未来需进一步推动整个设计生态（包括EDA、PDK、PHY等）的开放化。

Abstract: We present a roadmap for open-source chiplet-based RISC-V systems targeting high-performance computing and artificial intelligence, aiming to close the performance gap to proprietary designs. Starting with Occamy, the first open, silicon-proven dual-chiplet RISC-V manycore in 12nm FinFET, we scale to Ramora, a mesh-NoC-based dual-chiplet system, and to Ogopogo, a 7nm quad-chiplet concept architecture achieving state-of-the-art compute density. Finally, we explore possible avenues to extend openness beyond logic-core RTL into simulation, EDA, PDKs, and off-die PHYs.

</details>
