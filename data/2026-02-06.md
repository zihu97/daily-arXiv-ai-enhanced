<div id=toc></div>

# Table of Contents

- [cs.SE](#cs.SE) [Total: 18]
- [cs.PF](#cs.PF) [Total: 1]
- [cs.AR](#cs.AR) [Total: 3]
- [cs.NI](#cs.NI) [Total: 1]
- [cs.DC](#cs.DC) [Total: 6]


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [1] [Reducing the Costs of Proof Synthesis on Rust Systems by Scaling Up a Seed Training Set](https://arxiv.org/abs/2602.04910)
*Nongyu Di,Tianyu Chen,Shan Lu,Shuai Lu,Yeyun Gong,Peng Cheng,Jacob R. Lorch,Yuan Yao,Xiaoxing Ma*

Main category: cs.SE

TL;DR: VeruSyn 提出了一种数据合成管道，用于生成大规模带形式化规范和证明的 Rust 代码数据集，并基于此微调出的 Qwen2.5-Coder-32B-Instruct 模型在成本和证明权衡上优于现有商业模型。


<details>
  <summary>Details</summary>
Motivation: This content has not passed the compliance test and has been hidden.

Method: This content has not passed the compliance test and has been hidden.

Result: This content has not passed the compliance test and has been hidden.

Conclusion: This content has not passed the compliance test and has been hidden.

Abstract: Large Language Models (LLMs) are widely used for code generation. However, the correctness of code generated by LLMs remains a concern. A potential remedy to this concern is to have LLMs generate formal correctness proofs along with such code. However, compared with code generation, code-proof generation requires much higher reasoning capability and has much less existing data to learn from. In this paper, we present VeruSyn, a data synthesis pipeline for Verus, a state-of-the-art verification tool for system software written in Rust. Through self-synthesis and tutorial-based synthesis, VeruSyn achieves much larger scale and Verus-feature coverage than previous data-synthesis techniques designed for Verus; VeruSyn also supplements its dataset with long-chain-of-thought (CoT) data through agent trajectory synthesis. With VeruSyn, we synthesize the largest set of Verus verified programs: 6.9 million Rust programs, each with a formal specification and a proof that it meets that specification. This dataset lets us create a fine-tuned Qwen2.5-Coder-32B-Instruct model with appealing cost-proof tradeoff compared with state-of-the-art commercial models like Claude Sonnet 4.5. It also significantly outperforms models like o4-mini and previously proposed research models.

</details>


### [2] [Emergence-as-Code for Self-Governing Reliable Systems](https://arxiv.org/abs/2602.05458)
*Anatoly A. Krasnovsky*

Main category: cs.SE

TL;DR: 提出 Emergence-as-Code (EmaC) 框架，通过代码化方式建模用户旅程可靠性，解决微服务架构中原子 SLO 与整体用户体验的脱节问题。


<details>
  <summary>Details</summary>
Motivation: 现有 SLO-as-code 方法仅能声明单服务可靠性，而用户体验依赖多服务的交互组合（拓扑、路由、后备机制等），导致全局可靠性目标无法代码化管理，易随时间漂移。

Method: 定义 EmaC 规范声明旅程意图（目标/控制流/操作约束）并绑定原子 SLO；通过实时推理组件综合追踪数据生成旅程模型；编译器基于概率假设推导有界 SLO，输出可审查的控制平面策略（告警/发布门限）

Result: 实现旅程可靠性的可计算与可治理，提供包含运行示例的匿名化仓库，支持 Git 工作流实现策略评审与部署控制。

Conclusion: EmaC 为复杂微服务系统的全局可靠性管理提供系统化解决方案，通过代码化约束降低人工启发式维护成本与资源浪费。

Abstract: SLO-as-code has made per-service} reliability declarative, but user experience is defined by journeys whose reliability is an emergent property of microservice topology, routing, redundancy, timeouts/fallbacks, shared failure domains, and tail amplification. As a result, journey objectives (e.g., "checkout p99 < 400 ms") are often maintained outside code and drift as the system evolves, forcing teams to either miss user expectations or over-provision and gate releases with ad-hoc heuristics. We propose Emergence-as-Code (EmaC), a vision for making journey reliability computable and governable via intent plus evidence. An EmaC spec declares journey intent (objective, control-flow operators, allowed actions) and binds it to atomic SLOs and telemetry. A runtime inference component consumes operational artifacts (e.g., tracing and traffic configuration) to synthesize a candidate journey model with provenance and confidence. From the last accepted model, the EmaC compiler/controller derives bounded journey SLOs and budgets under explicit correlation assumptions (optimistic independence vs. pessimistic shared fate), and emits control-plane artifacts (burn-rate alerts, rollout gates, action guards) that are reviewable in a Git workflow. An anonymized artifact repository provides a runnable example specification and generated outputs.

</details>


### [3] [Large Language Models in Software Documentation and Modeling: A Literature Review and Findings](https://arxiv.org/abs/2602.04938)
*Lukas Radosky,Ivan Polasek*

Main category: cs.SE

TL;DR: 本文综述大型语言模型在软件工程文档与建模任务中的应用研究情况。


<details>
  <summary>Details</summary>
Motivation: 生成式人工智能在软件工程中兴起，大型语言模型有能力理解自然语言与结构化语言，适用于处理文档、程序与模型，因此需系统评估其在相关任务中的应用潜力。

Method: 通过文献综述，分析该领域四个主要场地的文章，按任务分类，并探讨提示技术、度量指标、人工评估方法和关键数据集。

Result: 综述提供了LLM在文档生成、代码建模等任务中的综合概述，梳理了常用技术、评估策略和数据集使用情况。

Conclusion: 大型语言模型在软件工程领域展现出巨大潜力和多面性，文献总结有助于指导未来研究方向和实践应用。

Abstract: Generative artificial intelligence attracts significant attention, especially with the introduction of large language models. Its capabilities are being exploited to solve various software engineering tasks. Thanks to their ability to understand natural language and generate natural language responses, large language models are great for processing various software documentation artifacts. At the same time, large language models excel at understanding structured languages, having the potential for working with software programs and models. We conduct a literature review on the usage of large language models for software engineering tasks related to documentation and modeling. We analyze articles from four major venues in the area, organize them per tasks they solve, and provide an overview of used prompt techniques, metrics, approaches to human-based evaluation, and major datasets.

</details>


### [4] [Applying a Requirements-Focused Agile Management Approach for Machine Learning-Enabled Systems](https://arxiv.org/abs/2602.05042)
*Lucas Romao,Luiz Xavier,Júlia Condé Araújo,Marina Condé Araújo,Ariane Rodrigues,Marcos Kalinowski*

Main category: cs.SE

TL;DR: 本文提出RefineML方法，集成机器学习定制化需求规范与敏捷管理，用于机器学习系统的持续迭代。在产学合作项目中评估显示该方法吃过沟通和可行性评估，但存在ML/provider requirement操作化挑战和人力估算困难。


<details>
  <summary>Details</summary>
Motivation: 机器学习系统依赖数据、实验性且行为不确定的特征挑战传统需求工程和敏捷管理，现有方法集成不足且未充分适配这些特性。

Method: 通过产学合作项目（巴西安防公司EXA与PUC-Rio大学），采用问卷调查评估适用性与接受度，并进行半结构化访谈。利用主题分析法处理定性数据。

Result: 问卷显示高感知有用性和使用意愿；访谈证实方法改进了沟通、支持早期可行性评估，并实现ML与软件开发的双轨治理。但存在ML问题转为敏捷需求困难及工作量估算不足的局限。

Conclusion: RefineML有效整合ML特性与敏捷管理，提升系统迭代效率，但需进一步解决需求操作化和工作量估算的局限问题。

Abstract: Machine Learning (ML)-enabled systems challenge traditional Requirements Engineering (RE) and agile management due to data dependence, experimentation, and uncertain model behavior. Existing RE and agile practices remain poorly integrated and insufficiently tailored to these characteristics. This paper reports on the practical experience of applying RefineML, a requirements-focused approach for the continuous and agile refinement of ML-enabled systems, which integrates ML-tailored specification and agile management approaches with best practices derived from a systematic mapping study. The application context concerns an industry-academia collaboration project between PUC-Rio and EXA, a Brazilian cybersecurity company. For evaluation purposes, we applied questionnaires assessing RefineML's suitability and overall acceptance and semi-structured interviews. We applied thematic analysis to the collected qualitative data. Regarding suitability and acceptance, the results of the questionnaires indicated high perceived usefulness and intention to use. Based on the interviews, stakeholders perceived RefineML as improving communication and facilitating early feasibility assessments, as well as enabling dual-track governance of ML and software work, allowing continuous refinement of the model while evolving the overall software project. However, some limitations remain, particularly related to difficulties in operationalizing ML concerns into agile requirements and in estimating ML effort.

</details>


### [5] [Quality Model for Machine Learning Components](https://arxiv.org/abs/2602.05043)
*Grace A. Lewis,Rachel Brower-Sinning,Robert Edman,Ipek Ozkaya,Sebastián Echeverría,Alex Derr,Collin Beaudoin,Katherine R. Maffey*

Main category: cs.SE

TL;DR: 研究指出ML原型常因测试忽略系统级需求（如吞吐量）而难以量产；作者提出新ML组件质量模型，基于ISO标准改进测试，并通过工具实现实用。


<details>
  <summary>Details</summary>
Motivation: 当前ML测试注重模型性能，忽视系统级需求导致部署失败；标准ISO 25059结合系统与组件属性不便使用，需针对性解决方案。

Method: 设计ML组件质量模型促进需求协商；通过调查验证模型价值，并整合至开源测试工具以示范应用。

Result: 调查参与者认可模型相关性和价值；模型成功融入开源ML测试工具，展示了实际效能。

Conclusion: 新质量模型为ML组件提供结构化框架，强化系统需求对测试的指引，提升部署成功率和协作效率。

Abstract: Despite increased adoption and advances in machine learning (ML), there are studies showing that many ML prototypes do not reach the production stage and that testing is still largely limited to testing model properties, such as model performance, without considering requirements derived from the system it will be a part of, such as throughput, resource consumption, or robustness. This limited view of testing leads to failures in model integration, deployment, and operations. In traditional software development, quality models such as ISO 25010 provide a widely used structured framework to assess software quality, define quality requirements, and provide a common language for communication with stakeholders. A newer standard, ISO 25059, defines a more specific quality model for AI systems. However, a problem with this standard is that it combines system attributes with ML component attributes, which is not helpful for a model developer, as many system attributes cannot be assessed at the component level. In this paper, we present a quality model for ML components that serves as a guide for requirements elicitation and negotiation and provides a common vocabulary for ML component developers and system stakeholders to agree on and define system-derived requirements and focus their testing efforts accordingly. The quality model was validated through a survey in which the participants agreed with its relevance and value. The quality model has been successfully integrated into an open-source tool for ML component testing and evaluation demonstrating its practical application.

</details>


### [6] [TestMigrationsInPy: A Dataset of Test Migrations from Unittest to Pytest](https://arxiv.org/abs/2602.05122)
*Altino Alves,Andre Hora*

Main category: cs.SE

TL;DR: 提出TestMigrationsInPy数据集，包含923个unittest转pytest的真实迁移案例，支持Python测试框架自动化迁移研究。


<details>
  <summary>Details</summary>
Motivation: unittest迁移至pytest虽有益但过程耗时，需自动化解决方案支持迁移过程

Method: 收集开发者实际迁移案例构建数据集，标注迁移类型（如断言或固件变更），提供公开访问

Result: 创建含923个迁移实例的数据集，可作为新迁移方法的基准，支持不同复杂度的验证场景

Conclusion: 数据集推动Python测试框架迁移研究，赋能从简单断言到复杂固件迁移的验证能力，公开地址：https://github.com/altinoalvesjunior/TestMigrationsInPy

Abstract: Unittest and pytest are the most popular testing frameworks in Python. Overall, pytest provides some advantages, including simpler assertion, reuse of fixtures, and interoperability. Due to such benefits, multiple projects in the Python ecosystem have migrated from unittest to pytest. To facilitate the migration, pytest can also run unittest tests, thus, the migration can happen gradually over time. However, the migration can be time-consuming and take a long time to conclude. In this context, projects would benefit from automated solutions to support the migration process. In this paper, we propose TestMigrationsInPy, a dataset of test migrations from unittest to pytest. TestMigrationsInPy contains 923 real-world migrations performed by developers. Future research proposing novel solutions to migrate frameworks in Python can rely on TestMigrationsInPy as a ground truth. Moreover, as TestMigrationsInPy includes information about the migration type (e.g., changes in assertions or fixtures), our dataset enables novel solutions to be verified effectively, for instance, from simpler assertion migrations to more complex fixture migrations. TestMigrationsInPy is publicly available at: https://github.com/altinoalvesjunior/TestMigrationsInPy.

</details>


### [7] [Exceptional Behaviors: How Frequently Are They Tested?](https://arxiv.org/abs/2602.05123)
*Andre Hora,Gordon Fraser*

Main category: cs.SE

TL;DR: 本研究实证分析Python系统中异常行为的测试频率，发现 dict盖子21.4%测试方法引发异常，其中中值约10%调用涉及异常行为，80%方法异常稀少但20%频发。


<details>
  <summary>Details</summary>
Motivation: 现有研究局限在传播到测试的异常，忽略其他异常，需全面评估真实系统中的异常测试覆盖率。

Method: 运行工具化测试套件，监控运行时异常，收集25个Python系统的5372个方法、1790万次调用、140万个异常数据。

Result: 21.4%执行方法引发异常；异常方法中中值10%调用涉异常；80%方法异常发生率较低，20%较频繁。

Conclusion: 异常行为非必然稀少，建议开发新工具加强异常测试和重构try/except块，提醒研究者与实践者关注此点。

Abstract: Exceptions allow developers to handle error cases expected to occur infrequently. Ideally, good test suites should test both normal and exceptional behaviors to catch more bugs and avoid regressions. While current research analyzes exceptions that propagate to tests, it does not explore other exceptions that do not reach the tests. In this paper, we provide an empirical study to explore how frequently exceptional behaviors are tested in real-world systems. We consider both exceptions that propagate to tests and the ones that do not reach the tests. For this purpose, we run an instrumented version of test suites, monitor their execution, and collect information about the exceptions raised at runtime. We analyze the test suites of 25 Python systems, covering 5,372 executed methods, 17.9M calls, and 1.4M raised exceptions. We find that 21.4% of the executed methods do raise exceptions at runtime. In methods that raise exceptions, on the median, 1 in 10 calls exercise exceptional behaviors. Close to 80% of the methods that raise exceptions do so infrequently, but about 20% raise exceptions more frequently. Finally, we provide implications for researchers and practitioners. We suggest developing novel tools to support exercising exceptional behaviors and refactoring expensive try/except blocks. We also call attention to the fact that exception-raising behaviors are not necessarily "abnormal" or rare.

</details>


### [8] [The Necessity of a Holistic Safety Evaluation Framework for AI-Based Automation Features](https://arxiv.org/abs/2602.05157)
*Alireza Abbaspour,Shabin Mahadevan,Kilian Zwirglmaier,Jeff Stafford*

Main category: cs.SE

TL;DR: 本文论述人工智能组件在驾驶自动化安全分析中的重要性，强调必须评估质量管理组件对SOTIF和功能安全风险的影响。


<details>
  <summary>Details</summary>
Motivation: 传统安全分析忽略了质量管理组件的风险，但AI整合揭示了其可导致SOTIF相关危险事件，需遵循新兴AI安全标准如ISO/PAS 8800。

Method: 通过案例研究展示QM组件（如感知算法）的缺陷，采用理论分析和实际例子相结合的方法论证风险缓解策略。

Result: 研究表明QM组件缺陷可违反风险接受标准，引发关键安全问题，证明了修订现有框架的必要性。

Conclusion: 需整合FuSa、SOTIF和AI标准方法进行全面风险识别和缓解，以确保所有组件分类的安全保障适应AI挑战。

Abstract: The intersection of Safety of Intended Functionality (SOTIF) and Functional Safety (FuSa) analysis of driving automation features has traditionally excluded Quality Management (QM) components from rigorous safety impact evaluations. While QM components are not typically classified as safety-relevant, recent developments in artificial intelligence (AI) integration reveal that such components can contribute to SOTIF-related hazardous risks. Compliance with emerging AI safety standards, such as ISO/PAS 8800, necessitates re-evaluating safety considerations for these components. This paper examines the necessity of conducting holistic safety analysis and risk assessment on AI components, emphasizing their potential to introduce hazards with the capacity to violate risk acceptance criteria when deployed in safety-critical driving systems, particularly in perception algorithms. Using case studies, we demonstrate how deficiencies in AI-driven perception systems can emerge even in QM-classified components, leading to unintended functional behaviors with critical safety implications. By bridging theoretical analysis with practical examples, this paper argues for the adoption of comprehensive FuSa, SOTIF, and AI standards-driven methodologies to identify and mitigate risks in AI components. The findings demonstrate the importance of revising existing safety frameworks to address the evolving challenges posed by AI, ensuring comprehensive safety assurance across all component classifications spanning multiple safety standards.

</details>


### [9] [Sovereign-by-Design A Reference Architecture for AI and Blockchain Enabled Systems](https://arxiv.org/abs/2602.05486)
*Matteo Esposito,Lodovica Marchesi,Roberto Tonelli,Valentina Lenarduzzi*

Main category: cs.SE

TL;DR: 论文提出将数字主权视为首要架构属性，构建集成自管理身份、区块链审计、主权数据治理和受控生成式AI的参考架构，应对法规需求与技术风险。


<details>
  <summary>Details</summary>
Motivation: 非主权云基础设施统治、生成式AI广泛应用及严格法规要求凸显主权需求，现有方案因孤立处理治理/合规/安全而缺乏架构层可操作性指导。

Method: 提出主权参考架构：融合自管理身份负责任管理系统、基于区块链的可审计信任机制、主权数据治理框架，并对生成式AI实施显式架构控制，使其成为合规助推器而非风险源。

Result: 以架构质量属性重构主权概念，衔接监管目标与系统设计，为可审计、可演进、司法感知的AI系统建立基础框架，提供软件架构/生成式AI/数字主权跨域研究新起点。

Conclusion: 主权架构化使生成式AI在受控条件下转化为合规赋能工具，通过系统性整合技术组件弥合监管断层，为未来AI主权系统开发奠定理论基础。

Abstract: Digital sovereignty has emerged as a central concern for modern software-intensive systems, driven by the dominance of non-sovereign cloud infrastructures, the rapid adoption of Generative AI, and increasingly stringent regulatory requirements. While existing initiatives address governance, compliance, and security in isolation, they provide limited guidance on how sovereignty can be operationalized at the architectural level. In this paper, we argue that sovereignty must be treated as a first-class architectural property rather than a purely regulatory objective. We introduce a Sovereign Reference Architecture that integrates self-sovereign identity, blockchain-based trust and auditability, sovereign data governance, and Generative AI deployed under explicit architectural control. The architecture explicitly captures the dual role of Generative AI as both a source of governance risk and an enabler of compliance, accountability, and continuous assurance when properly constrained. By framing sovereignty as an architectural quality attribute, our work bridges regulatory intent and concrete system design, offering a coherent foundation for building auditable, evolvable, and jurisdiction-aware AI-enabled systems. The proposed reference architecture provides a principled starting point for future research and practice at the intersection of software architecture, Generative AI, and digital sovereignty.

</details>


### [10] [PatchGuru: Patch Oracle Inference from Natural Language Artifacts with Large Language Models](https://arxiv.org/abs/2602.05270)
*Thanh Le-Cong,Bach Le,Toby Murray,Michael Pradel,Cristian Cadar*

Main category: cs.SE

TL;DR: PatchGuru是一种净化技术，自动从拉取请求中大语言模型推断可执行补丁规范，以改进补丁验证和错误检测。


<details>
  <summary>Details</summary>
Motivation: 由于回归测试不完整且补丁意图的非正式自然语言描述难执行，导致软件补丁可能无意改变行为，需要自动化手段验证补丁语义。

Method: PatchGuru利用大语言模型从自然语言素材提取开发者意图，合成补丁预言（运行时纸上声明），通过比较前/后版本行为迭代精炼预言、识别违规、过滤不一致性并生成错误报告。

Result: 在4个Python项目的400个拉取请求5828582上，PatchGuru报告39个警告，精度0.62，确认24个真阳性含12个新bug；相较Testora多检出17个bug且精度更高（0.62比0.32），每请求平均耗时8.9分钟成本0.07美元。

Conclusion: PatchGuru通过提供可执行文档和自动验证，补充了代码审查和回归测试，提升了软件修补可靠性。

Abstract: As software systems evolve, patches may unintentionally alter program behavior. Validating patches against their intended semantics is difficult due to incomplete regression tests and informal, non-executable natural language (NL) descriptions of patch intent. We present PatchGuru, the first automated technique that infers executable patch specifications from real-world pull requests (PRs). Given a PR, PatchGuru uses large language models (LLMs) to extract developer intent from NL artifacts and synthesizes patch oracles: under-approximate yet practical specifications expressed as runtime assertions in comparison programs that integrate pre- and post-patch versions. Patch oracles focus on patch-relevant behaviors, enable automated validation, and support cross-version properties. PatchGuru iteratively refines inferred oracles by comparing pre- and post-patch behaviors, identifies violations, filters inconsistencies via self-review, and generates bug reports. We evaluate PatchGuru on 400 recent PRs from four widely used open-source Python projects. PatchGuru reports 39 warnings with a precision of 0.62, yielding 24 confirmed true positives, including 12 previously unknown bugs, 11 of which were subsequently fixed by developers. Compared to the state-of-the-art technique Testora, PatchGuru detects 17 more bugs (24 vs. 7) while improving precision from 0.32 to 0.62. PatchGuru incurs an average cost of 8.9 minutes and USD 0.07 per PR. These results suggest that PatchGuru complements code review and regression testing by providing executable documentation and automated validation of patch intent.

</details>


### [11] [Does Programming Language Matter? An Empirical Study of Fuzzing Bug Detection](https://arxiv.org/abs/2602.05312)
*Tatsuya Shirai,Olivier Nourry,Yutaro Kashiwa,Kenji Fujiwara,Hajimu Iida*

Main category: cs.SE

TL;DR: 跨语言大规模分析表明模糊测试效果受编程语言特性显著影响，C++/Rust检出率高，Python漏洞修复快但检测慢，Go不可复现问题多。


<details>
  <summary>Details</summary>
Motivation: 现有研究未检验编程语言差异对持续模糊测试效果的影响，需填补语言特性与模糊测试效率关系的知识空白。

Method: 分析55个项目中的61,444个模糊测试bug和999,248次构建，按主编程语言分类对比漏洞特征和检测效率。

Result: 1. C++/Rust的漏洞检出频率更高 2. Rust/Python的漏洞比例低但严重性高 3. Go的不可复现bug较多 4. Python修复率高但检测耗时长

Conclusion: 编程语言设计深刻影响模糊测试效能，为开发语言敏感的模糊测试策略提供实证依据。

Abstract: Fuzzing has become a popular technique for automatically detecting vulnerabilities and bugs by generating unexpected inputs. In recent years, the fuzzing process has been integrated into continuous integration workflows (i.e., continuous fuzzing), enabling short and frequent testing cycles. Despite its widespread adoption, prior research has not examined whether the effectiveness of continuous fuzzing varies across programming languages. This study conducts a large-scale cross-language analysis to examine how fuzzing bug characteristics and detection efficiency differ among languages. We analyze 61,444 fuzzing bugs and 999,248 builds from 559 OSS-Fuzz projects categorized by primary language. Our findings reveal that (i) C++ and Rust exhibit higher fuzzing bug detection frequencies, (ii) Rust and Python show low vulnerability ratios but tend to expose more critical vulnerabilities, (iii) crash types vary across languages and unreproducible bugs are more frequent in Go but rare in Rust, and (iv) Python attains higher patch coverage but suffers from longer time-to-detection. These results demonstrate that fuzzing behavior and effectiveness are strongly shaped by language design, providing insights for language-aware fuzzing strategies and tool development.

</details>


### [12] [Can We Classify Flaky Tests Using Only Test Code? An LLM-Based Empirical Study](https://arxiv.org/abs/2602.05465)
*Alexander Berndt,Vekil Bekmyradov,Rainer Gemulla,Marcus Kessel,Thomas Bach,Sebastian Baltes*

Main category: cs.SE

TL;DR: 论文评估LLMs在仅凭测试代码分类不稳定测试的效果，发现性能不佳且信息不足。


<details>
  <summary>Details</summary>
Motivation: 解决传统机器学习方法泛化性差的问题，探索预训练大语言模型在测试稳定性分类中的应用潜力。

Method: 使用两个通用LLM和一个代码专用LLM，通过三种提示技术对两个基准数据集评估；人工分析50个样本的信息充分性。

Result: LLMs分类效果略优于随机猜测；人工分析证实测试代码单独提供的信息不足以判断测试稳定性。

Conclusion: 当前方法无法有效分类信封测试，未来需结合检索增强生成等技术补充上下文信息。

Abstract: Flaky tests yield inconsistent results when they are repeatedly executed on the same code revision. They interfere with automated quality assurance of code changes and hinder efficient software testing. Previous work evaluated approaches to train machine learning models to classify flaky tests based on identifiers in the test code. However, the resulting classifiers have been shown to lack generalizability, hindering their applicability in practical environments. Recently, pre-trained Large Language Models (LLMs) have shown the capability to generalize across various tasks. Thus, they represent a promising approach to address the generalizability problem of previous approaches. In this study, we evaluated three LLMs (two general-purpose models, one code-specific model) using three prompting techniques on two benchmark datasets from prior studies on flaky test classification. Furthermore, we manually investigated 50 samples from the given datasets to determine whether classifying flaky tests based only on test code is feasible for humans. Our findings indicate that LLMs struggle to classify flaky tests given only the test code. The results of our best prompt-model combination were only marginally better than random guessing. In our manual analysis, we found that the test code does not necessarily contain sufficient information for a flakiness classification. Our findings motivate future work to evaluate LLMs for flakiness classification with additional context, for example, using retrieval-augmented generation or agentic AI.

</details>


### [13] [ArkTS-CodeSearch: A Open-Source ArkTS Dataset for Code Retrieval](https://arxiv.org/abs/2602.05550)
*Yulong He,Artem Ermakov,Sergey Kovalchuk,Artem Aliev,Dmitry Shalymov*

Main category: cs.SE

TL;DR: 针对OpenHarmony生态中ArkTS编程语言缺乏公开数据集和基准的问题，本文构建了首个大规模ArkTS数据集并建立了系统性代码检索基准。


<details>
  <summary>Details</summary>
Motivation: 现有ArkTS代码智能研究因缺乏公共数据集和评估基准而受限，阻碍了相关技术发展。

Method: 从GitHub与Gitee爬取ArkTS代码库，使用tree-sitter-arkts提取注释-函数对；进行跨平台去重和函数类型统计，评估开源代码嵌入模型，并结合ArkTS与TypeScript数据集微调模型。

Result: 创建了首个ArkTS大规模数据集，微调后获得高性能代码理解模型；数据集及模型已在Hugging Face平台开源，建立首个系统性代码检索基准。

Conclusion: 该研究填补了ArkTS代码智能研究空白，公开资源将促进OpenHarmony生态发展。

Abstract: ArkTS is a core programming language in the OpenHarmony ecosystem, yet research on ArkTS code intelligence is hindered by the lack of public datasets and evaluation benchmarks. This paper presents a large-scale ArkTS dataset constructed from open-source repositories, targeting code retrieval and code evaluation tasks. We design a single-search task, where natural language comments are used to retrieve corresponding ArkTS functions. ArkTS repositories are crawled from GitHub and Gitee, and comment-function pairs are extracted using tree-sitter-arkts, followed by cross-platform deduplication and statistical analysis of ArkTS function types. We further evaluate all existing open-source code embedding models on the single-search task and perform fine-tuning using both ArkTS and TypeScript training datasets, resulting in a high-performing model for ArkTS code understanding. This work establishes the first systematic benchmark for ArkTS code retrieval. Both the dataset and our fine-tuned model will be released publicly and are available at https://huggingface.co/hreyulog/embedinggemma_arkts and https://huggingface.co/datasets/hreyulog/arkts-code-docstring,establishing the first systematic benchmark for ArkTS code retrieval.

</details>


### [14] [Towards Green AI: Decoding the Energy of LLM Inference in Software Development](https://arxiv.org/abs/2602.05712)
*Lola Solovyeva,Fernando Castor*

Main category: cs.SE

TL;DR: 本研究分析大型语言模型推理能耗，发现预充值和解码阶段的能源消耗模式差异，预充值成本放大解码能耗；抑制模型啰嗦行为可节省最多89%能源且不影响准确性


<details>
  <summary>Details</summary>
Motivation: AI辅助工具集成软件开发带来高计算和能源成本，需降低LLM推理能耗以促进可持续开发

Method: 评估六款6B-7B和四款3B-4B转换器模型在HumanEval代码生成和LongBench代码理解基准上的表现，分析预出了问题和解码两种阶段

Result: 各模型显示独特能耗模式，预充值成本增加使解码期每个令牌能耗放大1.3%-51.8%；十个模型中有三个输出啰嗦内容增加能耗，抑制后节约44%-89%不影响准确

Conclusion: 预充值成本显著影响主导能耗的解码阶段，抑制啰嗦行为可节省高达89%能源；有效降低推理能耗需同时干预啰嗦行为和限制预充值影响

Abstract: Context: AI-assisted tools are increasingly integrated into software development workflows, but their reliance on large language models (LLMs) introduces substantial computational and energy costs. Understanding and reducing the energy footprint of LLM inference is therefore essential for sustainable software development. Objective: In this study, we conduct a phase-level analysis of LLM inference energy consumption, distinguishing between the (1) prefill, where the model processes the input and builds internal representations, and (2) decoding, where output tokens are generated using the stored state. Method: We investigate six 6B-7B and four 3B-4B transformer-based models, evaluating them on code-centric benchmarks HumanEval for code generation and LongBench for code understanding. Results: Our findings show that, within both parameter groups, models exhibit distinct energy patterns across phases. Furthermore, we observed that increases in prefill cost amplify the energy cost per token during decoding, with amplifications ranging from 1.3% to 51.8% depending on the model. Lastly, three out of ten models demonstrate babbling behavior, adding excessive content to the output that unnecessarily inflates energy consumption. We implemented babbling suppression for code generation, achieving energy savings ranging from 44% to 89% without affecting generation accuracy. Conclusion: These findings show that prefill costs influence decoding, which dominates energy consumption, and that babbling suppression can yield up to 89% energy savings. Reducing inference energy therefore requires both mitigating babbling behavior and limiting impact of prefill on decoding.

</details>


### [15] [A Bayesian Optimization-Based AutoML Framework for Non-Intrusive Load Monitoring](https://arxiv.org/abs/2602.05739)
*Nazanin Siavash,Armin Moin*

Main category: cs.SE

TL;DR: 本文提出一种新型自动化机器学习框架AutoML4NILM，用于非侵入式负载监测（NILM），通过贝叶斯优化实现自动模型选择和超参数调优，降低领域专家应用门槛。


<details>
  <summary>Details</summary>
Motivation: 为了解决传统NILM依赖智能电表或专业知识的成本高问题，该研究旨在提供一种经济高效、零编程门槛的方法，使电力监控更易普及。

Method: 集成AutoML到NILM领域，使用贝叶斯优化自动化处理模型选择和超参数调整；开发开源工具包AutoML4NILM，支持11种算法并允许灵活扩展算法及超opathology参数。

Result: 推出AutoML4NILM工具包，当前支持多种算法和超参数；框架设计灵活促进扩展，简化了能源负载分解的实现过程。

Conclusion: 该框架提升了NILM的可应用性，为从业者节省成本和时间，鼓励研究和工业界采用AutoML技术进行能源管理。

Abstract: Non-Intrusive Load Monitoring (NILM), commonly known as energy disaggregation, aims to estimate the power consumption of individual appliances by analyzing a home's total electricity usage. This method provides a cost-effective alternative to installing dedicated smart meters for each appliance. In this paper, we introduce a novel framework that incorporates Automated Machine Learning (AutoML) into the NILM domain, utilizing Bayesian Optimization for automated model selection and hyperparameter tuning. This framework empowers domain practitioners to effectively apply machine learning techniques without requiring advanced expertise in data science or machine learning. To support further research and industry adoption, we present AutoML4NILM, a flexible and extensible open-source toolkit designed to streamline the deployment of AutoML solutions for energy disaggregation. Currently, this framework supports 11 algorithms, each with different hyperparameters; however, its flexible design allows for the extension of both the algorithms and their hyperparameters.

</details>


### [16] [Toward Quantum-Safe Software Engineering: A Vision for Post-Quantum Cryptography Migration](https://arxiv.org/abs/2602.05759)
*Lei Zhang*

Main category: cs.SE

TL;DR: 文章分析后量子密码学迁移的挑战，提出自动化量子安全适应框架（AQuA），推动量子安全软件工程的新研究方向


<details>
  <summary>Details</summary>
Motivation: 量子计算威胁加速后量子密码学标准化，但迁移旧软件面临安全漏洞检测、重构工具不兼容概率行为、侧信道敏感性和性能权衡等新工程难题

Method: 引入AQuA框架，包含三大支柱：的啊PQC感知检测、语义重构和混合验证，旨在系统解决量子安全适配问题

Result: 构建量子安全软件工程（QSSE）工具新范式，为后量子算法迁移提供结构化方法论

Conclusion: 确立量子安全软件工程作为独立研究领域，通过创新工具推动密码学系统平稳过渡至后量子时代

Abstract: The quantum threat to cybersecurity has accelerated the standardization of Post-Quantum Cryptography (PQC). Migrating legacy software to these quantum-safe algorithms is not a simple library swap, but a new software engineering challenge: existing vulnerability detection, refactoring, and testing tools are not designed for PQC's probabilistic behavior, side-channel sensitivity, and complex performance trade-offs. To address these challenges, this paper outlines a vision for a new class of tools and introduces the Automated Quantum-safe Adaptation (AQuA) framework, with a three-pillar agenda for PQC-aware detection, semantic refactoring, and hybrid verification, thereby motivating Quantum-Safe Software Engineering (QSSE) as a distinct research direction.

</details>


### [17] [Automated Customization of LLMs for Enterprise Code Repositories Using Semantic Scopes](https://arxiv.org/abs/2602.05780)
*Ulrich Finkler,Irene Manotas,Wei Zhang,Geert Janssen,Octavian Popescu,Shyam Ramji*

Main category: cs.SE

TL;DR: 本文提出一种基于代码语义范围的LLM自动化定制方法，以提高对私有代码库的代码完成任务精度，并评估了RAG和微调策略。


<details>
  <summary>Details</summary>
Motivation: 开源LLMs在公开基准表现良好，但在未见的私有代码仓库上生成准确代码困难，需定制化以提升模型性能和开发效率。

Method: 使用语义范围机制摄取仓库数据并构建训练对，评估检索增强生成（RAG）和监督微调（FT）策略，应用于两个企业级私有仓库。

Result: 定制化模型在代码完成精度上显著优于更大规模未定制模型，提升开发生产力；公共基准测试显示类似优势。

Conclusion: 定制化可有效处理私有仓库特定模式，增强模型输出精确度；未来工作需进一步探索和扩展定制 колеба化方法的应用范围。

Abstract: Code completion (CC) is a task frequently used by developers when working in collaboration with LLM-based programming assistants. Despite the increased performance of LLMs on public benchmarks, out of the box LLMs still have a hard time generating code that aligns with a private code repository not previously seen by the model's training data. Customizing code LLMs to a private repository provides a way to improve the model performance. In this paper we present our approach for automated LLM customization based on semantic scopes in the code. We evaluate LLMs on real industry cases with two private enterprise code repositories with two customization strategies: Retrieval-Augmented Generation (RAG) and supervised Fine-Tuning (FT). Our mechanism for ingesting the repository's data and formulating the training data pairs with semantic scopes helps models to learn the underlying patterns specific to the repository, providing more precise code to developers and helping to boost their productivity. The code completions of moderately sized customized models can be significantly better than those of uncustomized models of much larger capacity. We also include an analysis of customization on two public benchmarks and present opportunities for future work.

</details>


### [18] [When Elo Lies: Hidden Biases in Codeforces-Based Evaluation of Large Language Models](https://arxiv.org/abs/2602.05891)
*Shenyu Zheng,Ximing Dong,Xiaoshuang Liu,Gustavo Oliva,Chong Chun Yong,Dayi Lin,Boyuan Chen,Shaowei Wang,Ahmed E. Hassan*

Main category: cs.SE

TL;DR: 本文揭示Codeforces的Elo评分在评估大语言模型竞争编程能力时存在严重偏差，因参数不透明导致分数不可靠。


<details>
  <summary>Details</summary>
Motivation: Elo评分常缺乏实验细节报告，引起同一模型分数波动达500点，需系统性探究其隐藏影响因素。

Method: 基于37场Codeforces比赛和13,691个测试案例，控制性实证评估提交顺序、比赛难度选择及随机变异性这三种偏差来源。

Result: 分数对参数高度敏感：提交顺序偏移394分，比赛选择差异达1122分，运行变异性最大分数差349分。

Conclusion: Elo直接比较不可靠且易误导，要求标准化流程并透明报告实验设置以确保可信评估。

Abstract: As Large Language Models (LLMs) achieve breakthroughs in complex reasoning, Codeforces-based Elo ratings have emerged as a prominent metric for evaluating competitive programming capabilities. However, these ratings are often reported without critical experimental details, leading to significant discrepancies illustrated by recent reports where the score of the same model version fluctuated by nearly 500 points. This paper presents a systematic empirical study on the hidden factors biasing Elo evaluations: (1) the temporal ordering of submissions, (2) contest difficulty selection, and (3) run to run stochastic variability of LLMs. Utilizing a controlled benchmark of 37 recent Codeforces contests and 13,691 generated test cases, we demonstrate that Elo scores are highly sensitive to these parameters. Our findings reveal that varying submission orders can shift scores by 394 points, while contest selection can cause differences of up to 1,122 points for the same model. Run to run performance exhibits substantial instability, with a maximum difference of 349 points in mean scores observed when evaluating identical contests. We conclude that direct Elo comparisons are unreliable and potentially misleading without strict standardization and transparent reporting of experimental settings.

</details>


<div id='cs.PF'></div>

# cs.PF [[Back]](#toc)

### [19] [Wasure: A Modular Toolkit for Comprehensive WebAssembly Benchmarking](https://arxiv.org/abs/2602.05488)
*Riccardo Carissimi,Ben L. Titzer*

Main category: cs.PF

TL;DR: 本文介绍Wasure工具，用于简化和比较WebAssembly基准测试。


<details>
  <summary>Details</summary>
Motivation: WebAssembly性能评估面临运行时引擎选择、硬件架构等多维挑战，需系统工具支持。

Method: 开发Wasure，一个模块化和可扩展的命令行工具包，并进行基准套件的动态分析。

Result: 分析显示代码覆盖率、控制流和执行模式存在显著差异，强调基准套件的多样性需求。

Conclusion: Wasure旨在帮助开发者和研究人员实现更系统、透明和有洞察力的WebAssembly引擎评估。

Abstract: WebAssembly (Wasm) has become a key compilation target for portable and efficient execution across diverse platforms. Benchmarking its performance, however, is a multi-dimensional challenge: it depends not only on the choice of runtime engines, but also on hardware architectures, application domains, source languages, benchmark suites, and runtime configurations. This paper introduces Wasure, a modular and extensible command-line toolkit that simplifies the execution and comparison of WebAssembly benchmarks. To complement performance evaluation, we also conducted a dynamic analysis of the benchmark suites included with Wasure. Our analysis reveals substantial differences in code coverage, control flow, and execution patterns, emphasizing the need for benchmark diversity. Wasure aims to support researchers and developers in conducting more systematic, transparent, and insightful evaluations of WebAssembly engines.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [20] [CVA6-CFI: A First Glance at RISC-V Control-Flow Integrity Extensions](https://arxiv.org/abs/2602.04991)
*Simone Manoni,Emanuele Parisi,Riccardo Tedeschi,Davide Rossi,Andrea Acquaviva,Andrea Bartolini*

Main category: cs.AR

TL;DR: 首次设计和评估外貌RISC-V的控制流完整性扩展Zicfiss和Zicfilp，通过硬件机制抵御攻击，表现出低开销：面积仅增1.0%，性能最高增15.6%。


<details>
  <summary>Details</summary>
Motivation: 解决控制流劫持攻击风险，提出基于硬件安全机制（如影子栈和着陆垫）保护程序执行，以增强RISC-V架构的完整性。

Method: 集成前端控制流保护（Zicfilp）和后端控制流保护（Zicfiss）硬件单元到开源CVA6核，利用22纳米FDX技术合成，并通过MiBench汽车基准子集进行性能评估。

Result: 硬件设计面积开销较低仅1.0%，性能开销范围最高达15.6%，同时实现完整开源发布。

Conclusion: 成功开发并公开硬件CFI机制，证明了在RISC-V平台上实施高效安全防护的可行性，平衡了安全性与资源开销。

Abstract: This work presents the first design, integration, and evaluation of the standard RISC-V extensions for Control-Flow Integrity (CFI). The Zicfiss and Zicfilp extensions aim at protecting the execution of a vulnerable program from control-flow hijacking attacks through the implementation of security mechanisms based on shadow stack and landing pad primitives. We introduce two independent and configurable hardware units implementing forward-edge and backward-edge control-flow protection, fully integrated into the open-source CVA6 core. Our design incurs in only 1.0% area overhead when synthesized in 22 nm FDX technology, and up to 15.6% performance overhead based on evaluation with the MiBench automotive benchmark subset. We release the complete implementation as open source.

</details>


### [21] [COFFEE: A Carbon-Modeling and Optimization Framework for HZO-based FeFET eNVMs](https://arxiv.org/abs/2602.05018)
*Hongbang Wu,Xuesi Chen,Shubham Jadhav,Amit Lal,Lillian Pentecost,Udit Gupta*

Main category: cs.AR

TL;DR: 提出碳建模框架COFFEE评估HZO铁电晶体管(eNVM)全生命周期碳排放，对比SRAM显示单位容量隐含碳降低76%，应用于边缘ML加速器可减排42.3%隐含碳+70%运行碳。


<details>
  <summary>Details</summary>
Motivation: 新兴存储技术环境影响不明，需量化硬件全生命周期碳足迹以实现可持续计算。

Method: 基于晶圆厂实测数据建立COFFEE框架，整合制造工艺与架构设计工具评估隐含碳（制造）和运行碳（使用）。

Result: 2MB容量下：1）FeFET单位面积隐含碳比CMOS高11%，但单位MB隐含碳比SRAM低76%。2）ML加速器案例中替换SRAM缓存实现隐含碳-42.3%/运行碳-70%。

Conclusion: HZO-FeFET显著降低存储器碳强度，为构建可持续计算系统提供量化支撑。

Abstract: Information and communication technologies account for a growing portion of global environmental impacts. While emerging technologies, such as emerging non-volatile memories (eNVM), offer a promising solution to energy efficient computing, their end-to-end footprint is not well understood. Understanding the environmental impact of hardware systems over their life cycle is the first step to realizing sustainable computing. This work conducts a detailed study of one example eNVM device: hafnium-zirconium-oxide (HZO)-based ferroelectric field-effect transistors (FeFETs). We present COFFEE, the first carbon modeling framework for HZO-based FeFET eNVMs across life cycle, from hardware manufacturing (embodied carbon) to use (operational carbon). COFFEE builds on data gathered from a real semiconductor fab and device fabrication recipes to estimate embodied carbon, and architecture level eNVM design space exploration tools to quantify use-phase performance and energy. Our evaluation shows that, at 2 MB capacity, the embodied carbon per unit area overhead of HZO-FeFETs can be up to 11% higher than the CMOS baseline, while the embodied carbon per MB remains consistently about 4.3x lower than SRAM across different memory capacity. A further case study applies COFFEE to an edge ML accelerator, showing that replacing the SRAM-based weight buffer with HZO-based FeFET eNVMs reduces embodied carbon by 42.3% and operational carbon by up to 70%.

</details>


### [22] [Balancing FP8 Computation Accuracy and Efficiency on Digital CIM via Shift-Aware On-the-fly Aligned-Mantissa Bitwidth Prediction](https://arxiv.org/abs/2602.05743)
*Liang Zhao,Kunming Shao,Zhipeng Liao,Xijie Huang,Tim Kwang-Ting Cheng,Chi-Ying Tsui,Yi Zou*

Main category: cs.AR

TL;DR: 本文提出一种灵活FP8数字计算内存(DCIM)加速器，通过动态调整位宽、简化输入对齐和使用可扩展MAC阵列，显著提高FP8处理效率，在28nm CMOS实现中达20.4 TFLOPS/W能耗比。


<details>
  <summary>Details</summary>
Motivation: 现有DCIM架构难以支持可变FP8对齐尾数位宽，因为统一的对齐策略和固定精度MAC单元无法处理输入数据的多样化分布，导致效率低下。

Method: 引入三种创新：动态移位感知位宽预测(DSBP)实时预测输入以调整权重(2/4/6/8b)和输入(2~12b)精度；FIFO基输入对齐单元(FIAU)用指针控制取代复杂桶形移位器；精度可扩展INT MAC阵列在低开销下实现权重精度灵活性。

Result: 28nm CMOS原型(64×96 CIM阵列)达到20.4 TFLOPS/W能耗比，FP8效率比先前工作高2.8倍，支持所有FP8格式；在Llama-7b模型上，DSBP在BoolQ和Winogrande数据集上优于固定位宽模式，提供可配置精度-效率权衡。

Conclusion: 该加速器通过自适应和可扩展设计，有效解决了FP8格式处理的挑战，为Transformer推理与训练提供高效且灵活的解决方案。

Abstract: FP8 low-precision formats have gained significant adoption in Transformer inference and training. However, existing digital compute-in-memory (DCIM) architectures face challenges in supporting variable FP8 aligned-mantissa bitwidths, as unified alignment strategies and fixed-precision multiply-accumulate (MAC) units struggle to handle input data with diverse distributions. This work presents a flexible FP8 DCIM accelerator with three innovations: (1) a dynamic shift-aware bitwidth prediction (DSBP) with on-the-fly input prediction that adaptively adjusts weight (2/4/6/8b) and input (2$\sim$12b) aligned-mantissa precision; (2) a FIFO-based input alignment unit (FIAU) replacing complex barrel shifters with pointer-based control; and (3) a precision-scalable INT MAC array achieving flexible weight precision with minimal overhead. Implemented in 28nm CMOS with a 64$\times$96 CIM array, the design achieves 20.4 TFLOPS/W for fixed E5M7, demonstrating 2.8$\times$ higher FP8 efficiency than previous work while supporting all FP8 formats. Results on Llama-7b show that the DSBP achieves higher efficiency than fixed bitwidth mode at the same accuracy level on both BoolQ and Winogrande datasets, with configurable parameters enabling flexible accuracy-efficiency trade-offs.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [23] [Wi-Fi Radar via Over-the-Air Referencing: Bridging Wi-Fi Sensing and Bistatic Radar](https://arxiv.org/abs/2602.05344)
*Koji Yamamoto*

Main category: cs.NI

TL;DR: 提出LoSRef方案，利用无线LoSpath作为参考信号，使非同步Wi-Fi系统实现相位相干雷达式感知


<details>
  <summary>Details</summary>
Motivation: 非同步收发端无法进行相位相干时延-多普勒分析，限制了Wi-Fi传感在人体探测中的应用

Method: 利用最早到达的视距路径(LoS)作为空中参考点，开发OTA LoSRef框架，实现延时校准和相位对齐

Result: Result analysis unavailable

Conclusion: 该方案架起了传统Wi-Fi传感与雷达系统的桥梁，灵敏度提升20dB，实现物理可解释的人体运动感知

Abstract: Wi-Fi sensing has attracted significant attention for human sensing and related applications. However, unsynchronized transmitters and receivers fundamentally preclude phase-coherent radar-like delay--Doppler analysis. By exploiting the line-of-sight (LoS) path, i.e., the earliest-arriving direct path, as an over-the-air (OTA) reference for delay and phase, we propose an OTA LoS-path referencing scheme, termed LoSRef, that enables delay calibration and phase alignment in unsynchronized Wi-Fi systems. Unlike conventional Wi-Fi bistatic radar systems that rely on wired reference signals or dedicated reference antennas, the proposed LoSRef-based framework bridges the long-standing gap between conventional Wi-Fi sensing and Wi-Fi radar, enabling phase-coherent bistatic radar-like operation in a drop-in Wi-Fi sensing configuration. Through human gait and respiration experiments in indoor environments, we demonstrate that phase-coherent channel impulse responses and corresponding delay--Doppler responses are obtained using only commodity Wi-Fi devices. This enables physically interpretable human motion sensing, including gait-induced range variation and respiration-induced sub-wavelength displacement, as well as the extraction of target-induced dynamics up to 20 dB weaker than dominant static multipath components.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [24] [A novel scalable high performance diffusion solver for multiscale cell simulations](https://arxiv.org/abs/2602.05017)
*Jose-Luis Estragues-Muñoz,Carlos Alvarez,Arnau Montagud,Daniel Jimenez-Gonzalez,Alfonso Valencia*

Main category: cs.DC

TL;DR: 提出可扩展的BioFVM库，用于肿瘤分子扩散模拟，实现200倍加速和36%内存优化


<details>
  <summary>Details</summary>
Motivation: 解决细胞分辨率模型难以扩展至acija-scale肿瘤仿真的瓶颈，为构建疾病数字孪生模型提供HPC基础

Method: 开发基于先进有限体积钜阵(BioFVM)的可扩展高性能计算方案，系统评估其分子扩散模型性能

Result: 较现有方案提升近200倍计算速度，内存占用降低36%

Conclusion: 为高效求解新一代生物计算问题奠定技术基础

Abstract: Agent-based cellular models simulate tissue evolution by capturing the behavior of individual cells, their interactions with neighboring cells, and their responses to the surrounding microenvironment. An important challenge in the field is scaling cellular resolution models to real-scale tumor simulations, which is critical for the development of digital twin models of diseases and requires the use of High-Performance Computing (HPC) since every time step involves trillions of operations. We hereby present a scalable HPC solution for the molecular diffusion modeling using an efficient implementation of state-of-the-art Finite Volume Method (FVM) frameworks. The paper systematically evaluates a novel scalable Biological Finite Volume Method (BioFVM) library and presents an extensive performance analysis of the available solutions. Results shows that our HPC proposal reach almost 200x speedup and up to 36% reduction in memory usage over the current state-of-the-art solutions, paving the way to efficiently compute the next generation of biological problems.

</details>


### [25] [Towards Advancing Research with Workflows: A perspective from the Workflows Community Summit -- Amsterdam, 2025](https://arxiv.org/abs/2602.05131)
*Irene Bonati,Silvina Caino-Lores,Tainã Coleman,Sagar Dolas,Sandro Fiore,Venkatesh Kannan,Marco Verdicchio,Sean R. Wilkinson,Rafael Ferreira da Silva*

Main category: cs.DC

TL;DR: 2025年科学工作流峰会分析工作流采用障碍，包括通用性与实用性矛盾、可持续性风险及资源缺口，并提出技术、政策与社群层面的行动方针以提升可重复性。


<details>
  <summary>Details</summary>
Motivation: 科学工作流对协调分布式资源、管理大数据及确保研究可复现性至关重要，峰会旨在审视新兴挑战与机遇。

Method: 召集国际专家识别障碍，讨论并行动提案：转向科学影响评估、规范工作流模式、培养国际社群协作、投资人力资源与教育整合。

Result: 识别四大障碍，提出行动线如变革评估指标、增强透明度与产学研结合，以解决可持续问题和标准缺口。

Conclusion: 峰会成果为通过工作流推动科学发现提供行动指南，强调生态系统挑战的系统应对以促进研究进步。

Abstract: Scientific workflows have become essential for orchestrating complex computational processes across distributed resources, managing large datasets, and ensuring reproducibility in modern research. The Workflows Community Summit 2025, held in Amsterdam on June 6th, 2025, convened international experts to examine emerging challenges and opportunities in this domain. Participants identified key barriers to workflow adoption, including tensions between system generality and domain-specific utility, concerns over long-term sustainability of workflow systems and services, insufficient recognition for those who develop and maintain reproducible workflows, and gaps in standardization, funding, training, and cross-disciplinary collaboration. To address these challenges, the summit proposed action lines spanning technology, policy, and community dimensions: shifting evaluation metrics from raw computational performance toward measuring genuine scientific impact; formalizing workflow patterns and community-driven benchmarks to improve transparency, reproducibility, and usability; cultivating a cohesive international workflows community that engages funding bodies and research stakeholders; and investing in human capital through dedicated workflow engineering roles, career pathways, and integration of workflow concepts into educational curricula and long-term training initiatives. This document presents the summit's findings, beginning with an overview of the current computing ecosystem and the rationale for workflow-centric approaches, followed by a discussion of identified challenges and recommended action lines for advancing scientific discovery through workflows.

</details>


### [26] [ORACL: Optimized Reasoning for Autoscaling via Chain of Thought with LLMs for Microservices](https://arxiv.org/abs/2602.05292)
*Haoyu Bai,Muhammed Tawfiqul Islam,Minxian Xu,Rajkumar Buyya*

Main category: cs.DC

TL;DR: 本文提出ORACL框架，利用大型语言模型进行微服务的自适应资源分配，提高诊断准确性和服务质量。


<details>
  <summary>Details</summary>
Motivation: 现有自动扩展策略存在训练成本高或泛化能力差的问题，需探索新型通用资源分配方法。

Method: ORACL将运行时遥测数据转换为语义自然语言sidebar，通过链式思维推理生成可解释决策并约束行动空间。

Result: 实验显示根因识别准确率提升15%，训练加速24倍，短期服务质量改善6%，无需额外训练。tom

Conclusion: ORACL证明LLM可作为普适且高效的微服务资源分配器，适用于快速迭代环境 תחברה

Abstract: Applications are moving away from monolithic designs to microservice and serverless architectures, where fleets of lightweight and independently deployable components run on public clouds. Autoscaling serves as the primary control mechanism for balancing resource utilization and quality of service, yet existing policies are either opaque learned models that require substantial per-deployment training or brittle hand-tuned rules that fail to generalize. We investigate whether large language models can act as universal few-shot resource allocators that adapt across rapidly evolving microservice deployments.
  We propose ORACL, Optimized Reasoning for Autoscaling via Chain of Thought with LLMs for Microservices, a framework that leverages prior knowledge and chain-of-thought reasoning to diagnose performance regressions and recommend resource allocations. ORACL transforms runtime telemetry, including pods, replicas, CPU and memory usage, latency, service-level objectives, and fault signals, into semantic natural-language state descriptions and invokes an LLM to produce an interpretable intermediate reasoning trace. This reasoning identifies likely root causes, prunes the action space, and issues safe allocation decisions under policy constraints. Experiments on representative open-source microservice workloads show that ORACL improves root-cause identification accuracy by 15 percent, accelerates training by up to 24x, and improves quality of service by 6 percent in short-term scenarios, without deployment-specific retraining.

</details>


### [27] [Proteus: Append-Only Ledgers for (Mostly) Trusted Execution Environments](https://arxiv.org/abs/2602.05346)
*Shubham Mishra,João Gonçalves,Chawinphat Tankuranand,Neil Giridharan,Natacha Crooks,Heidi Howard,Chris Jensen*

Main category: cs.DC

TL;DR:  Proteus 是一个新的分布式共识协议，通过在 CFT 协议中嵌入 BFT 协议来谨慎信任 TEEs，在保持高性能的同时增强分布式账本的韧性以保证完整性。


<details>
  <summary>Details</summary>
Motivation: 硬件 TEEs 可能遭受攻击，导致分布式账本的信任保证被破坏，因此需要设计一个协议来应对这些风险。

Method: 通过重构 CFT 和 BFT 协议的结构使其对齐，从而在 CFT 协议内部无缝嵌入 BFT 协议，无需增加额外消息。

Result: Proteus 的性能与传统 TEE-enabled 共识协议相当，并且在 TEE 平台受到威胁时仍能确保数据完整性。

Conclusion:  Proteus 成功提升了分布式账本的抗攻击能力，在不牺牲效率的前提下提供了更高的安全保障。

Abstract: Distributed ledgers are increasingly relied upon by industry to provide trustworthy accountability, strong integrity protection, and high availability for critical data without centralizing trust. Recently, distributed append-only logs are opting for a layered approach, combining crash-fault-tolerant (CFT) consensus with hardware-based Trusted Execution Environments (TEEs) for greater resiliency. Unfortunately, hardware TEEs can be subject to (rare) attacks, undermining the very guarantees that distributed ledgers are carefully designed to achieve. In response, we present Proteus, a new distributed consensus protocol that cautiously trusts the guarantees of TEEs. Proteus carefully embeds a Byzantine fault-tolerant (BFT) protocol inside of a CFT protocol with no additional messages. This is made possible through careful refactoring of both the CFT and BFT protocols such that their structure aligns. Proteus achieves performance in line with regular TEE-enabled consensus protocols, while guaranteeing integrity in the face of TEE platform compromises.

</details>


### [28] [Reaching Univalency with Subquadratic Communication](https://arxiv.org/abs/2602.05356)
*Andrew Lewis-Pye*

Main category: cs.DC

TL;DR: 该论文反驳Dolev-Reischuk下界结论，证明其二次通信成本仅源于传播阶段而非达成共识决定阶段（单值性达成）。


<details>
  <summary>Details</summary>
Motivation: 澄清Dolev-Reischuk下界的Ω电场² + n消息复杂度到底是用于达成协议结果的一致性（单值性），还是仅用于之后将结果传播给所有处理器。

Method: 引入ε-BA协议，允许少数正确处理器出错（错误率ε），在故障数f < n(1/3 - ε)时以O(n log n)复杂度实现；定义Extractable BA协议，在认证环境下实现O(f log f)通信。

Result: ε-BA协议可作为完整BA协议的第一阶段达成单值性（结果已决定），二次成本仅存在于后续全网广播阶段；证明Extractable BA的较低复杂度可行性。

Conclusion: Dolev-Reischuk下界的二次通信成本完全取决于}}{信息的传播需求，而非共识决定过程本身的难度。

Abstract: The Dolev-Reischuk lower bound establishes that any deterministic Byzantine Agreement (BA) protocol for $n$ processors tolerating $f$ faults requires $Ω(f^2+n)$ messages. But what exactly does this quadratic cost pay for? Even the minimal requirement that every correct processor \emph{receive at least one message} already necessitates $Ω(f^2 + n)$ messages. This raises a fundamental question: is the Dolev-Reischuk bound about the difficulty of \emph{reaching univalency} -- the point at which the protocol's outcome is determined -- or merely about \emph{disseminating} the outcome to all processors afterward?
  We resolve this question by showing that reaching univalency does \emph{not} require quadratic communication. Specifically, we introduce $ε$-BA, a relaxation allowing an $ε$-fraction of correct processors to output incorrectly, and prove it can be solved deterministically with $O(n \log n)$ communication complexity when $f < n(1/3 - ε)$. Crucially, any $ε$-BA protocol can serve as the first phase of a full BA protocol: after $ε$-BA, a single all-to-all exchange and majority vote completes BA. Since the outcome is already determined after $ε$-BA, this demonstrates that the quadratic cost in Dolev-Reischuk stems entirely from dissemination, rather than from reaching univalency. We also define Extractable BA for authenticated settings, capturing when processors collectively hold enough signed messages to determine the agreed value, and show it can be solved with communication complexity $O(f \log f)$.

</details>


### [29] [TimelyFreeze: Adaptive Parameter Freezing Mechanism for Pipeline Parallelism](https://arxiv.org/abs/2602.05754)
*Seonghye Cho,Jaemin Han,Hyunjin Kim,Euisoo Jung,Jae-Gil Lee*

Main category: cs.DC

TL;DR: TimelyFreeze提出优化参数冻结策略，通过线性规划最小化流水线气泡，提升训练吞吐量同时保证精度。


<details>
  <summary>Details</summary>
Motivation: 现有流水线并行训练因过度冻结参数导致精度损失，需平衡计算效率与模型精度。

Method: 将流水线调度建模为有向无环图，通过线性规划求解最优冻结比例以满足精度约束下最小批处理时间。

Result: LLaMA-8B训练吞吐量提升40%且精度相当

Conclusion: 该方法在多种流水线并行场景中有效提升大规模模型训练速度而不影响收敛。

Abstract: Pipeline parallelism enables training models that exceed single-device memory, but practical throughput remains limited by pipeline bubbles. Although parameter freezing can improve training throughput by adaptively skipping backward computation, existing methods often over-freeze parameters, resulting in unnecessary accuracy degradation. To address this issue, we propose TimelyFreeze, which models the pipeline schedule as a directed acyclic graph and solves a linear program to compute optimal freeze ratios that minimize batch execution time under accuracy constraints. Experiments show that TimelyFreeze achieves up to 40% training throughput improvement on LLaMA-8B with comparable accuracy. Overall, it enables faster large-scale model training without compromising convergence and generalizes across diverse pipeline-parallel settings.

</details>
