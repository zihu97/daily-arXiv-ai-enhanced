{"id": "2511.20780", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2511.20780", "abs": "https://arxiv.org/abs/2511.20780", "authors": ["Alison Silva", "Gustavo Callou"], "title": "Assessing Redundancy Strategies to Improve Availability in Virtualized System Architectures", "comment": null, "summary": "Cloud-based storage platforms are becoming more common in both academic and business settings due to their flexible access to data and support for collaborative functionalities. As reliability becomes a vital requirement, particularly for organizations looking for alternatives to public cloud services, assessing the dependability of these systems is crucial. This paper presents a methodology for analyzing the availability of a file server (Nextcloud) hosted in a private cloud environment using Apache CloudStack. The analysis is based on a modeling approach through Stochastic Petri Nets (SPNs) that allows the evaluation of different redundancy strategies to enhance the availability of such systems. Four architectural configurations were modeled, including the baseline, host-level redundancy, virtual machine (VM) redundancy, and a combination of both. The results show that redundancy at both the host and VM levels significantly improves availability and reduces expected downtime. The proposed approach provides a method to evaluate the availability of a private cloud and support infrastructure design decisions.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u968f\u673aPetri\u7f51\uff08SPN\uff09\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u8bc4\u4f30\u5728Apache CloudStack\u79c1\u6709\u4e91\u4e2d\u90e8\u7f72\u7684Nextcloud\u6587\u4ef6\u670d\u52a1\u5668\u7684\u53ef\u7528\u6027\uff0c\u5e76\u6bd4\u8f83\u56db\u79cd\u67b6\u6784\u914d\u7f6e\uff08\u5305\u62ec\u4e3b\u673a\u7ea7\u5197\u4f59\u3001\u865a\u62df\u673a\u5197\u4f59\u53ca\u5176\u7ec4\u5408\uff09\uff0c\u7ed3\u679c\u8868\u660e\u540c\u65f6\u91c7\u7528\u4e3b\u673a\u548c\u865a\u62df\u673a\u5197\u4f59\u80fd\u663e\u8457\u63d0\u5347\u7cfb\u7edf\u53ef\u7528\u6027\u3002", "motivation": "\u968f\u7740\u4e91\u5b58\u50a8\u5e73\u53f0\u5728\u5b66\u672f\u548c\u5546\u4e1a\u73af\u5883\u4e2d\u65e5\u76ca\u666e\u53ca\uff0c\u7ec4\u7ec7\u5bf9\u53ef\u9760\u6027\u7684\u9700\u6c42\u4e0d\u65ad\u63d0\u5347\uff0c\u5c24\u5176\u5728\u5bfb\u6c42\u516c\u6709\u4e91\u66ff\u4ee3\u65b9\u6848\u65f6\u3002\u56e0\u6b64\uff0c\u8bc4\u4f30\u79c1\u6709\u4e91\u73af\u5883\u4e2d\u6587\u4ef6\u670d\u52a1\u5668\u7cfb\u7edf\u7684\u53ef\u7528\u6027\u53d8\u5f97\u81f3\u5173\u91cd\u8981\u3002", "method": "\u91c7\u7528\u968f\u673aPetri\u7f51\uff08SPNs\uff09\u5bf9Nextcloud\u6587\u4ef6\u670d\u52a1\u5668\u5728Apache CloudStack\u79c1\u6709\u4e91\u4e2d\u7684\u56db\u79cd\u67b6\u6784\u914d\u7f6e\u8fdb\u884c\u5efa\u6a21\uff0c\u5305\u62ec\u57fa\u7ebf\u914d\u7f6e\u3001\u4e3b\u673a\u7ea7\u5197\u4f59\u3001\u865a\u62df\u673a\u5197\u4f59\u4ee5\u53ca\u4e24\u8005\u7ed3\u5408\u7684\u65b9\u5f0f\uff0c\u4ee5\u8bc4\u4f30\u4e0d\u540c\u5197\u4f59\u7b56\u7565\u5bf9\u7cfb\u7edf\u53ef\u7528\u6027\u7684\u5f71\u54cd\u3002", "result": "\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u5728\u4e3b\u673a\u548c\u865a\u62df\u673a\u4e24\u4e2a\u5c42\u9762\u540c\u65f6\u5b9e\u65bd\u5197\u4f59\u7b56\u7565\u80fd\u591f\u663e\u8457\u63d0\u9ad8\u7cfb\u7edf\u53ef\u7528\u6027\uff0c\u5e76\u6709\u6548\u51cf\u5c11\u9884\u671f\u505c\u673a\u65f6\u95f4\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u57fa\u4e8eSPN\u7684\u5efa\u6a21\u65b9\u6cd5\u4e3a\u79c1\u6709\u4e91\u73af\u5883\u4e2d\u6587\u4ef6\u670d\u52a1\u5668\u7684\u53ef\u7528\u6027\u8bc4\u4f30\u63d0\u4f9b\u4e86\u6709\u6548\u5de5\u5177\uff0c\u53ef\u8f85\u52a9\u57fa\u7840\u8bbe\u65bd\u67b6\u6784\u8bbe\u8ba1\u51b3\u7b56\u3002"}}
{"id": "2511.20975", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2511.20975", "abs": "https://arxiv.org/abs/2511.20975", "authors": ["Yinwei Dai", "Zhuofu Chen", "Anand Iyer", "Ravi Netravali"], "title": "Aragog: Just-in-Time Model Routing for Scalable Serving of Agentic Workflows", "comment": null, "summary": "Agentic workflows have emerged as a powerful paradigm for solving complex, multi-stage tasks, but serving them at scale is computationally expensive given the many LLM inferences that each request must pass through. Configuration selection, or the cost-aware assignment of workflow agents to specific LLMs, can reduce these costs, but existing approaches bind configuration decisions before request execution, making them ill-suited for the heterogeneous and lengthy execution of workflows. Specifically, system loads can fluctuate rapidly and substantially during a request's lifetime, causing fixed configurations to quickly become suboptimal. We present Aragog, a system that progressively adapts a request's configuration throughout its execution to match runtime dynamics. To make this practical despite the massive space of workflow configurations, Aragog decouples the problem into two core elements -- a one-time routing step that identifies all accuracy-preserving configurations, and a cheap per-stage scheduler that selects among them using up-to-date system observations -- and introduces novel strategies to accelerate each. Across diverse workflows and model families, Aragog increases maximum serving throughput by 50.0--217.0\\% and reduces median latency by 32.5--78.9\\% at peak request rates, while maintaining accuracy comparable to the most expensive configurations.", "AI": {"tldr": "Aragog \u662f\u4e00\u4e2a\u52a8\u6001\u8c03\u6574\u5de5\u4f5c\u6d41\u4e2d\u5404\u9636\u6bb5\u5927\u8bed\u8a00\u6a21\u578b\u914d\u7f6e\u7684\u7cfb\u7edf\uff0c\u901a\u8fc7\u8fd0\u884c\u65f6\u81ea\u9002\u5e94\u8c03\u5ea6\uff0c\u5728\u4fdd\u6301\u51c6\u786e\u7387\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u541e\u5410\u91cf\u5e76\u964d\u4f4e\u5ef6\u8fdf\u3002", "motivation": "\u73b0\u6709\u5de5\u4f5c\u6d41\u914d\u7f6e\u65b9\u6cd5\u5728\u8bf7\u6c42\u6267\u884c\u524d\u9759\u6001\u7ed1\u5b9a\u6a21\u578b\u9009\u62e9\uff0c\u65e0\u6cd5\u5e94\u5bf9\u6267\u884c\u8fc7\u7a0b\u4e2d\u7cfb\u7edf\u8d1f\u8f7d\u7684\u5feb\u901f\u53d8\u5316\uff0c\u5bfc\u81f4\u6210\u672c\u9ad8\u6216\u6027\u80fd\u4e0b\u964d\u3002", "method": "Aragog \u5c06\u914d\u7f6e\u95ee\u9898\u89e3\u8026\u4e3a\u4e24\u4e2a\u90e8\u5206\uff1a\u4e00\u6b21\u6027\u8def\u7531\u6b65\u9aa4\u8bc6\u522b\u6240\u6709\u4fdd\u51c6\u786e\u7387\u7684\u914d\u7f6e\uff0c\u4ee5\u53ca\u6bcf\u9636\u6bb5\u57fa\u4e8e\u5b9e\u65f6\u7cfb\u7edf\u72b6\u6001\u7684\u8f7b\u91cf\u7ea7\u8c03\u5ea6\u5668\u4ece\u4e2d\u9009\u62e9\u6700\u4f18\u914d\u7f6e\uff0c\u5e76\u5f15\u5165\u65b0\u7b56\u7565\u52a0\u901f\u8fd9\u4e24\u4e2a\u8fc7\u7a0b\u3002", "result": "\u5728\u591a\u79cd\u5de5\u4f5c\u6d41\u548c\u6a21\u578b\u65cf\u4e0a\uff0cAragog \u5728\u5cf0\u503c\u8bf7\u6c42\u901f\u7387\u4e0b\u5c06\u6700\u5927\u541e\u5410\u91cf\u63d0\u5347 50.0\u2013217.0%\uff0c\u4e2d\u4f4d\u5ef6\u8fdf\u964d\u4f4e 32.5\u201378.9%\uff0c\u540c\u65f6\u4fdd\u6301\u4e0e\u6700\u6602\u8d35\u914d\u7f6e\u76f8\u5f53\u7684\u51c6\u786e\u7387\u3002", "conclusion": "\u901a\u8fc7\u8fd0\u884c\u65f6\u52a8\u6001\u8c03\u6574\u914d\u7f6e\uff0cAragog \u80fd\u6709\u6548\u5e94\u5bf9\u7cfb\u7edf\u8d1f\u8f7d\u6ce2\u52a8\uff0c\u5728\u4fdd\u8bc1\u51c6\u786e\u6027\u7684\u524d\u63d0\u4e0b\u663e\u8457\u4f18\u5316\u670d\u52a1\u6548\u7387\u548c\u54cd\u5e94\u901f\u5ea6\u3002"}}
{"id": "2511.20982", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2511.20982", "abs": "https://arxiv.org/abs/2511.20982", "authors": ["Junhan Liao", "Minxian Xu", "Wanyi Zheng", "Yan Wang", "Kejiang Ye", "Rajkumar Buyya", "Chengzhong Xu"], "title": "A Dynamic PD-Disaggregation Architecture for Maximizing Goodput in LLM Inference Serving", "comment": "14 pages", "summary": "To meet strict Service-Level Objectives (SLOs),contemporary Large Language Models (LLMs) decouple the prefill and decoding stages and place them on separate GPUs to mitigate the distinct bottlenecks inherent to each phase. However, the heterogeneity of LLM workloads causes producerconsumer imbalance between the two instance types in such disaggregated architecture. To address this problem, we propose DOPD (Dynamic Optimal Prefill/Decoding), a dynamic LLM inference system that adjusts instance allocations to achieve an optimal prefill-to-decoding (P/D) ratio based on real-time load monitoring. Combined with an appropriate request-scheduling policy, DOPD effectively resolves imbalances between prefill and decoding instances and mitigates resource allocation mismatches due to mixed-length requests under high concurrency. Experimental evaluations show that, compared with vLLM and DistServe (representative aggregation-based and disaggregationbased approaches), DOPD improves overall system goodput by up to 1.5X, decreases P90 time-to-first-token (TTFT) by up to 67.5%, and decreases P90 time-per-output-token (TPOT) by up to 22.8%. Furthermore, our dynamic P/D adjustment technique performs proactive reconfiguration based on historical load, achieving over 99% SLOs attainment while using less additional resources.", "AI": {"tldr": "DOPD \u662f\u4e00\u79cd\u52a8\u6001 LLM \u63a8\u7406\u7cfb\u7edf\uff0c\u901a\u8fc7\u5b9e\u65f6\u76d1\u63a7\u8d1f\u8f7d\u52a8\u6001\u8c03\u6574\u9884\u586b\u5145\uff08prefill\uff09\u4e0e\u89e3\u7801\uff08decoding\uff09\u9636\u6bb5\u7684 GPU \u8d44\u6e90\u5206\u914d\u6bd4\u4f8b\uff0c\u4ee5\u89e3\u51b3\u5f02\u6784\u5de5\u4f5c\u8d1f\u8f7d\u5bfc\u81f4\u7684\u751f\u4ea7\u8005-\u6d88\u8d39\u8005\u5931\u8861\u95ee\u9898\uff0c\u4ece\u800c\u663e\u8457\u63d0\u5347\u7cfb\u7edf\u541e\u5410\u91cf\u5e76\u964d\u4f4e\u5ef6\u8fdf\u3002", "motivation": "\u5f53\u524d LLM \u63a8\u7406\u7cfb\u7edf\u5c06 prefill \u548c decoding \u9636\u6bb5\u5206\u79bb\u90e8\u7f72\u5728\u4e0d\u540c GPU \u4e0a\u4ee5\u5e94\u5bf9\u5404\u81ea\u74f6\u9888\uff0c\u4f46\u56e0\u5de5\u4f5c\u8d1f\u8f7d\u5f02\u6784\u6027\uff0c\u5bb9\u6613\u9020\u6210\u4e24\u4e2a\u9636\u6bb5\u8d44\u6e90\u5229\u7528\u4e0d\u5747\u8861\uff0c\u5f71\u54cd\u6574\u4f53\u6027\u80fd\u548c SLO \u8fbe\u6210\u3002", "method": "\u63d0\u51fa DOPD \u7cfb\u7edf\uff0c\u57fa\u4e8e\u5b9e\u65f6\u8d1f\u8f7d\u76d1\u6d4b\u52a8\u6001\u8c03\u6574 prefill/decoding \u5b9e\u4f8b\u6570\u91cf\u4ee5\u7ef4\u6301\u6700\u4f18 P/D \u6bd4\u4f8b\uff0c\u5e76\u7ed3\u5408\u5408\u9002\u7684\u8bf7\u6c42\u8c03\u5ea6\u7b56\u7565\uff1b\u540c\u65f6\u5229\u7528\u5386\u53f2\u8d1f\u8f7d\u4fe1\u606f\u8fdb\u884c\u4e3b\u52a8\u91cd\u914d\u7f6e\u3002", "result": "\u76f8\u6bd4 vLLM \u548c DistServe\uff0cDOPD \u6700\u591a\u63d0\u5347 1.5 \u500d\u7cfb\u7edf goodput\uff0cP90 TTFT \u964d\u4f4e\u6700\u591a 67.5%\uff0cP90 TPOT \u964d\u4f4e\u6700\u591a 22.8%\uff0c\u5e76\u5728\u4f7f\u7528\u8f83\u5c11\u989d\u5916\u8d44\u6e90\u7684\u60c5\u51b5\u4e0b\u5b9e\u73b0\u8d85\u8fc7 99% \u7684 SLO \u8fbe\u6210\u7387\u3002", "conclusion": "DOPD \u6709\u6548\u89e3\u51b3\u4e86 LLM \u63a8\u7406\u4e2d disaggregated \u67b6\u6784\u4e0b\u7684\u8d44\u6e90\u5931\u8861\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\u4e0e\u8d44\u6e90\u6548\u7387\uff0c\u540c\u65f6\u4fdd\u969c\u9ad8 SLO \u8fbe\u6210\u7387\u3002"}}
{"id": "2511.21232", "categories": ["cs.AR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.21232", "abs": "https://arxiv.org/abs/2511.21232", "authors": ["Muhammed Yildirim", "Ozcan Ozturk"], "title": "RISC-V Based TinyML Accelerator for Depthwise Separable Convolutions in Edge AI", "comment": "13 pages, 7 tables, 14 figures", "summary": "The increasing demand for on-device intelligence in Edge AI and TinyML applications requires the efficient execution of modern Convolutional Neural Networks (CNNs). While lightweight architectures like MobileNetV2 employ Depthwise Separable Convolutions (DSC) to reduce computational complexity, their multi-stage design introduces a critical performance bottleneck inherent to layer-by-layer execution: the high energy and latency cost of transferring intermediate feature maps to either large on-chip buffers or off-chip DRAM. To address this memory wall, this paper introduces a novel hardware accelerator architecture that utilizes a fused pixel-wise dataflow. Implemented as a Custom Function Unit (CFU) for a RISC-V processor, our architecture eliminates the need for intermediate buffers entirely, reducing the data movement up to 87\\% compared to conventional layer-by-layer execution. It computes a single output pixel to completion across all DSC stages-expansion, depthwise convolution, and projection-by streaming data through a tightly-coupled pipeline without writing to memory. Evaluated on a Xilinx Artix-7 FPGA, our design achieves a speedup of up to 59.3x over the baseline software execution on the RISC-V core. Furthermore, ASIC synthesis projects a compact 0.284 mm$^2$ footprint with 910 mW power at 2 GHz in 28 nm, and a 1.20 mm$^2$ footprint with 233 mW power at 300 MHz in 40 nm. This work confirms the feasibility of a zero-buffer dataflow within a TinyML resource envelope, offering a novel and effective strategy for overcoming the memory wall in edge AI accelerators.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u65b0\u578b\u786c\u4ef6\u52a0\u901f\u5668\u67b6\u6784\uff0c\u91c7\u7528\u878d\u5408\u50cf\u7d20\u7ea7\u6570\u636e\u6d41\uff0c\u5728RISC-V\u5904\u7406\u5668\u4e0a\u5b9e\u73b0\u96f6\u4e2d\u95f4\u7f13\u5b58\u7684\u6df1\u5ea6\u53ef\u5206\u79bb\u5377\u79ef\uff08DSC\uff09\u8ba1\u7b97\uff0c\u663e\u8457\u964d\u4f4e\u6570\u636e\u79fb\u52a8\u5f00\u9500\u5e76\u63d0\u5347\u80fd\u6548\u3002", "motivation": "\u8fb9\u7f18AI\u548cTinyML\u5e94\u7528\u5bf9\u8bbe\u5907\u7aef\u667a\u80fd\u7684\u9700\u6c42\u65e5\u76ca\u589e\u957f\uff0c\u4f46\u73b0\u6709\u8f7b\u91cf\u7ea7CNN\uff08\u5982MobileNetV2\uff09\u5728\u9010\u5c42\u6267\u884c\u65f6\u56e0\u9891\u7e41\u8bbf\u95ee\u7247\u4e0a\u6216\u7247\u5916\u5b58\u50a8\u800c\u4ea7\u751f\u9ad8\u80fd\u8017\u4e0e\u5ef6\u8fdf\uff0c\u5f62\u6210\u201c\u5185\u5b58\u5899\u201d\u74f6\u9888\u3002", "method": "\u8bbe\u8ba1\u4e00\u79cd\u57fa\u4e8e\u878d\u5408\u50cf\u7d20\u7ea7\u6570\u636e\u6d41\u7684\u5b9a\u5236\u529f\u80fd\u5355\u5143\uff08CFU\uff09\uff0c\u96c6\u6210\u4e8eRISC-V\u5904\u7406\u5668\uff0c\u901a\u8fc7\u7d27\u5bc6\u8026\u5408\u6d41\u6c34\u7ebf\u4e00\u6b21\u6027\u5b8c\u6210\u5355\u4e2a\u8f93\u51fa\u50cf\u7d20\u5728DSC\u5404\u9636\u6bb5\uff08\u6269\u5c55\u3001\u6df1\u5ea6\u5377\u79ef\u3001\u6295\u5f71\uff09\u7684\u8ba1\u7b97\uff0c\u65e0\u9700\u5199\u5165\u4e2d\u95f4\u7279\u5f81\u56fe\u3002", "result": "\u5728Xilinx Artix-7 FPGA\u4e0a\u5b9e\u73b0\u6700\u9ad859.3\u500d\u4e8eRISC-V\u8f6f\u4ef6\u57fa\u7ebf\u7684\u901f\u5ea6\u63d0\u5347\uff1bASIC\u7efc\u5408\u663e\u793a\u572828 nm\u5de5\u827a\u4e0b\u9762\u79ef\u4ec50.284 mm\u00b2\u3001\u529f\u8017910 mW\uff082 GHz\uff09\uff0c\u572840 nm\u4e0b\u4e3a1.20 mm\u00b2\u3001233 mW\uff08300 MHz\uff09\uff0c\u6570\u636e\u79fb\u52a8\u51cf\u5c11\u9ad8\u8fbe87%\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u9a8c\u8bc1\u4e86\u5728TinyML\u8d44\u6e90\u7ea6\u675f\u4e0b\u5b9e\u73b0\u96f6\u7f13\u5b58\u6570\u636e\u6d41\u7684\u53ef\u884c\u6027\uff0c\u4e3a\u8fb9\u7f18AI\u52a0\u901f\u5668\u7a81\u7834\u5185\u5b58\u5899\u63d0\u4f9b\u4e86\u9ad8\u6548\u65b0\u7b56\u7565\u3002"}}
{"id": "2511.21235", "categories": ["cs.OS", "eess.SY"], "pdf": "https://arxiv.org/pdf/2511.21235", "abs": "https://arxiv.org/abs/2511.21235", "authors": ["Daniel Berend", "Shlomi Dolev", "Sweta Kumari", "Dhruv Mishra", "Marina Kogan-Sadetsky", "Archit Somani"], "title": "DynamicAdaptiveClimb: Adaptive Cache Replacement with Dynamic Resizing", "comment": "19 pages, 11 figures, 3 tables, Patented", "summary": "Efficient cache management is critical for optimizing the system performance, and numerous caching mechanisms have been proposed, each exploring various insertion and eviction strategies. In this paper, we present AdaptiveClimb and its extension, DynamicAdaptiveClimb, two novel cache replacement policies that leverage lightweight, cache adaptation to outperform traditional approaches. Unlike classic Least Recently Used (LRU) and Incremental Rank Progress (CLIMB) policies, AdaptiveClimb dynamically adjusts the promotion distance (jump) of the cached objects based on recent hit and miss patterns, requiring only a single tunable parameter and no per-item statistics. This enables rapid adaptation to changing access distributions while maintaining low overhead. Building on this foundation, DynamicAdaptiveClimb further enhances adaptability by automatically tuning the cache size in response to workload demands. Our comprehensive evaluation across a diverse set of real-world traces, including 1067 traces from 6 different datasets, demonstrates that DynamicAdaptiveClimb consistently achieves substantial speedups and higher hit ratios compared to other state-of-the-art algorithms. In particular, our approach achieves up to a 29% improvement in hit ratio and a substantial reduction in miss penalties compared to the FIFO baseline. Furthermore, it outperforms the next-best contenders, AdaptiveClimb and SIEVE [43], by approximately 10% to 15%, especially in environments characterized by fluctuating working set sizes. These results highlight the effectiveness of our approach in delivering efficient performance, making it well-suited for modern, dynamic caching environments.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e24\u79cd\u65b0\u578b\u7f13\u5b58\u66ff\u6362\u7b56\u7565 AdaptiveClimb \u548c DynamicAdaptiveClimb\uff0c\u901a\u8fc7\u8f7b\u91cf\u7ea7\u81ea\u9002\u5e94\u673a\u5236\u52a8\u6001\u8c03\u6574\u7f13\u5b58\u9879\u7684\u63d0\u5347\u8ddd\u79bb\u548c\u7f13\u5b58\u5927\u5c0f\uff0c\u5728\u591a\u79cd\u771f\u5b9e\u5de5\u4f5c\u8d1f\u8f7d\u4e0b\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u4f20\u7edf\u7f13\u5b58\u66ff\u6362\u7b56\u7565\uff08\u5982 LRU \u548c CLIMB\uff09\u96be\u4ee5\u5728\u52a8\u6001\u53d8\u5316\u7684\u5de5\u4f5c\u8d1f\u8f7d\u4e0b\u9ad8\u6548\u9002\u5e94\uff0c\u4e14\u5f80\u5f80\u9700\u8981\u590d\u6742\u7684\u7edf\u8ba1\u4fe1\u606f\u6216\u9ad8\u5f00\u9500\u3002\u56e0\u6b64\uff0c\u4e9f\u9700\u4e00\u79cd\u4f4e\u5f00\u9500\u3001\u9ad8\u9002\u5e94\u6027\u7684\u7f13\u5b58\u7ba1\u7406\u673a\u5236\u3002", "method": "AdaptiveClimb \u6839\u636e\u6700\u8fd1\u7684\u547d\u4e2d/\u672a\u547d\u4e2d\u6a21\u5f0f\u52a8\u6001\u8c03\u6574\u7f13\u5b58\u5bf9\u8c61\u7684\u63d0\u5347\u8ddd\u79bb\uff0c\u4ec5\u9700\u4e00\u4e2a\u53ef\u8c03\u53c2\u6570\u4e14\u65e0\u9700\u9010\u9879\u7edf\u8ba1\uff1bDynamicAdaptiveClimb \u5728\u6b64\u57fa\u7840\u4e0a\u8fdb\u4e00\u6b65\u6839\u636e\u5de5\u4f5c\u8d1f\u8f7d\u81ea\u52a8\u8c03\u6574\u7f13\u5b58\u5bb9\u91cf\u3002", "result": "\u5728\u6db5\u76d66\u4e2a\u6570\u636e\u96c6\u30011067\u6761\u771f\u5b9e\u8f68\u8ff9\u7684\u8bc4\u4f30\u4e2d\uff0cDynamicAdaptiveClimb \u76f8\u6bd4 FIFO \u57fa\u7ebf\u6700\u9ad8\u63d0\u534729%\u547d\u4e2d\u7387\uff0c\u5e76\u6bd4\u6b21\u4f18\u65b9\u6cd5\uff08AdaptiveClimb \u548c SIEVE\uff09\u9ad8\u51fa10%\u201315%\u7684\u6027\u80fd\uff0c\u5c24\u5176\u5728\u5de5\u4f5c\u96c6\u5927\u5c0f\u6ce2\u52a8\u7684\u573a\u666f\u4e0b\u8868\u73b0\u7a81\u51fa\u3002", "conclusion": "\u6240\u63d0\u65b9\u6cd5\u5728\u4fdd\u6301\u4f4e\u5f00\u9500\u7684\u540c\u65f6\u5b9e\u73b0\u4e86\u9ad8\u6548\u7684\u7f13\u5b58\u6027\u80fd\uff0c\u7279\u522b\u9002\u7528\u4e8e\u73b0\u4ee3\u52a8\u6001\u7f13\u5b58\u73af\u5883\u3002"}}
{"id": "2511.20663", "categories": ["cs.MA", "cs.AI", "eess.SY"], "pdf": "https://arxiv.org/pdf/2511.20663", "abs": "https://arxiv.org/abs/2511.20663", "authors": ["Barak Or"], "title": "MTTR-A: Measuring Cognitive Recovery Latency in Multi-Agent Systems", "comment": "preprint", "summary": "Ensuring cognitive stability in autonomous multi-agent systems (MAS) is a central challenge for large-scale, distributed AI. While existing observability tools monitor system outputs, they cannot quantify how rapidly agentic workflows recover once reasoning coherence has been lost. We adapt classical reliability metrics-Mean Time-to-Recovery (MTTR), Mean Time Between Failures (MTBF), and related ratios-into the cognitive domain, defining MTTR-A (Mean Time-to-Recovery for Agentic Systems) as a runtime measure of cognitive recovery latency. MTTR-A quantifies the time required for a MAS to detect reasoning drift and restore consistent operation, capturing the recovery of reasoning coherence rather than infrastructural repair.\n  A benchmark simulation using the AG~News corpus and the LangGraph orchestration framework was conducted, modeling recovery latencies across multiple reflex modes. Automated reflexes restored stability within approximately 6s on average, while human-approval interventions required about 12s. Across 200 runs, the median simulated MTTR-A was 6.21+-2.14s, MTBF=6.7+-2.14s, and NRR=0.08, demonstrating measurable runtime resilience across reflex strategies.\n  By formalizing recovery latency as a quantifiable property of distributed reasoning-and deriving reliability bounds linking recovery time and cognitive uptime-this work establishes a foundation for runtime dependability in agentic cognition, transforming cognitive recovery from an ad-hoc process into a standardized, interpretable performance", "AI": {"tldr": "\u672c\u6587\u5c06\u4f20\u7edf\u53ef\u9760\u6027\u6307\u6807\uff08\u5982MTTR\u3001MTBF\uff09\u5f15\u5165\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u7684\u8ba4\u77e5\u9886\u57df\uff0c\u63d0\u51faMTTR-A\u4f5c\u4e3a\u8861\u91cf\u8ba4\u77e5\u6062\u590d\u5ef6\u8fdf\u7684\u8fd0\u884c\u65f6\u6307\u6807\uff0c\u5e76\u901a\u8fc7\u4eff\u771f\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u4e0d\u540c\u53cd\u5c04\u6a21\u5f0f\u4e0b\u7684\u6062\u590d\u6027\u80fd\uff0c\u4e3a\u667a\u80fd\u4f53\u8ba4\u77e5\u7684\u8fd0\u884c\u65f6\u53ef\u9760\u6027\u63d0\u4f9b\u4e86\u91cf\u5316\u57fa\u7840\u3002", "motivation": "\u73b0\u6709\u53ef\u89c2\u6d4b\u6027\u5de5\u5177\u65e0\u6cd5\u91cf\u5316\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u5728\u63a8\u7406\u4e00\u81f4\u6027\u4e27\u5931\u540e\u7684\u6062\u590d\u901f\u5ea6\uff0c\u7f3a\u4e4f\u5bf9\u8ba4\u77e5\u7a33\u5b9a\u6027\u6062\u590d\u8fc7\u7a0b\u7684\u5ea6\u91cf\u624b\u6bb5\u3002", "method": "\u5c06\u7ecf\u5178\u53ef\u9760\u6027\u6307\u6807\uff08MTTR\u3001MTBF\u7b49\uff09\u9002\u914d\u5230\u8ba4\u77e5\u57df\uff0c\u5b9a\u4e49MTTR-A\u4f5c\u4e3a\u8ba4\u77e5\u6062\u590d\u5ef6\u8fdf\u7684\u8fd0\u884c\u65f6\u5ea6\u91cf\uff1b\u5229\u7528AG~News\u8bed\u6599\u5e93\u548cLangGraph\u6846\u67b6\u6784\u5efa\u4eff\u771f\u73af\u5883\uff0c\u6a21\u62df\u591a\u79cd\u53cd\u5c04\u6a21\u5f0f\u4e0b\u7684\u6062\u590d\u5ef6\u8fdf\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff1a\u81ea\u52a8\u53cd\u5c04\u5e73\u5747\u7ea66\u79d2\u6062\u590d\u7a33\u5b9a\uff0c\u4eba\u5de5\u5e72\u9884\u7ea612\u79d2\uff1b200\u6b21\u8fd0\u884c\u4e2d\uff0c\u4e2d\u4f4dMTTR-A\u4e3a6.21\u00b12.14\u79d2\uff0cMTBF\u4e3a6.7\u00b12.14\u79d2\uff0cNRR\u4e3a0.08\uff0c\u8868\u660e\u4e0d\u540c\u53cd\u5c04\u7b56\u7565\u5177\u6709\u53ef\u6d4b\u91cf\u7684\u8fd0\u884c\u65f6\u97e7\u6027\u3002", "conclusion": "\u901a\u8fc7\u5c06\u6062\u590d\u5ef6\u8fdf\u5f62\u5f0f\u5316\u4e3a\u5206\u5e03\u5f0f\u63a8\u7406\u7684\u53ef\u91cf\u5316\u5c5e\u6027\uff0c\u5e76\u5efa\u7acb\u6062\u590d\u65f6\u95f4\u4e0e\u8ba4\u77e5\u53ef\u7528\u6027\u7684\u53ef\u9760\u6027\u8fb9\u754c\uff0c\u672c\u7814\u7a76\u4e3a\u667a\u80fd\u4f53\u8ba4\u77e5\u7684\u8fd0\u884c\u65f6\u53ef\u9760\u6027\u5960\u5b9a\u4e86\u57fa\u7840\uff0c\u4f7f\u8ba4\u77e5\u6062\u590d\u4ece\u4e34\u65f6\u6027\u8fc7\u7a0b\u8f6c\u53d8\u4e3a\u6807\u51c6\u5316\u3001\u53ef\u89e3\u91ca\u7684\u6027\u80fd\u6307\u6807\u3002"}}
{"id": "2511.21413", "categories": ["cs.DC", "cs.AI", "cs.DB", "cs.PF"], "pdf": "https://arxiv.org/pdf/2511.21413", "abs": "https://arxiv.org/abs/2511.21413", "authors": ["Tim Trappen", "Robert Ke\u00dfler", "Roland Pabel", "Viktor Achter", "Stefan Wesner"], "title": "Automated Dynamic AI Inference Scaling on HPC-Infrastructure: Integrating Kubernetes, Slurm and vLLM", "comment": "6 pages, 3 figures", "summary": "Due to rising demands for Artificial Inteligence (AI) inference, especially in higher education, novel solutions utilising existing infrastructure are emerging. The utilisation of High-Performance Computing (HPC) has become a prevalent approach for the implementation of such solutions. However, the classical operating model of HPC does not adapt well to the requirements of synchronous, user-facing dynamic AI application workloads. In this paper, we propose our solution that serves LLMs by integrating vLLM, Slurm and Kubernetes on the supercomputer \\textit{RAMSES}. The initial benchmark indicates that the proposed architecture scales efficiently for 100, 500 and 1000 concurrent requests, incurring only an overhead of approximately 500 ms in terms of end-to-end latency.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5728\u8d85\u7b97RAMSES\u4e0a\u7ed3\u5408vLLM\u3001Slurm\u548cKubernetes\u7684\u67b6\u6784\uff0c\u4ee5\u9ad8\u6548\u652f\u6301\u9762\u5411\u7528\u6237\u7684\u52a8\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u63a8\u7406\u670d\u52a1\uff0c\u5b9e\u9a8c\u8bc1\u660e\u5176\u5728\u9ad8\u5e76\u53d1\u8bf7\u6c42\u4e0b\u4ec5\u5f15\u5165\u7ea6500\u6beb\u79d2\u7684\u7aef\u5230\u7aef\u5ef6\u8fdf\u3002", "motivation": "\u4f20\u7edf\u9ad8\u6027\u80fd\u8ba1\u7b97\uff08HPC\uff09\u7684\u8fd0\u884c\u6a21\u5f0f\u96be\u4ee5\u6ee1\u8db3\u540c\u6b65\u3001\u9762\u5411\u7528\u6237\u7684\u52a8\u6001AI\u5e94\u7528\u8d1f\u8f7d\u9700\u6c42\uff0c\u5c24\u5176\u662f\u5728\u9ad8\u7b49\u6559\u80b2\u9886\u57df\u5bf9AI\u63a8\u7406\u9700\u6c42\u4e0d\u65ad\u589e\u957f\u7684\u80cc\u666f\u4e0b\uff0c\u4e9f\u9700\u5229\u7528\u73b0\u6709\u57fa\u7840\u8bbe\u65bd\u6784\u5efa\u65b0\u578b\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u5728\u8d85\u7b97RAMSES\u4e0a\u96c6\u6210vLLM\u3001Slurm\u548cKubernetes\uff0c\u6784\u5efa\u652f\u6301\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u63a8\u7406\u7684\u670d\u52a1\u67b6\u6784\u3002", "result": "\u521d\u6b65\u57fa\u51c6\u6d4b\u8bd5\u8868\u660e\uff0c\u8be5\u67b6\u6784\u5728100\u3001500\u548c1000\u4e2a\u5e76\u53d1\u8bf7\u6c42\u4e0b\u5747\u80fd\u9ad8\u6548\u6269\u5c55\uff0c\u7aef\u5230\u7aef\u5ef6\u8fdf\u4ec5\u589e\u52a0\u7ea6500\u6beb\u79d2\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u96c6\u6210\u67b6\u6784\u6709\u6548\u89e3\u51b3\u4e86\u4f20\u7edfHPC\u5728\u652f\u6301\u52a8\u6001AI\u63a8\u7406\u4efb\u52a1\u65f6\u7684\u9002\u914d\u95ee\u9898\uff0c\u4e3a\u5728\u73b0\u6709\u8d85\u7b97\u57fa\u7840\u8bbe\u65bd\u4e0a\u90e8\u7f72\u7528\u6237\u5bfc\u5411\u7684LLM\u670d\u52a1\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848\u3002"}}
{"id": "2511.21346", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2511.21346", "abs": "https://arxiv.org/abs/2511.21346", "authors": ["Mohamed Shahawy", "Julien de Castelnau", "Paolo Ienne"], "title": "Bombyx: OpenCilk Compilation for FPGA Hardware Acceleration", "comment": null, "summary": "Task-level parallelism (TLP) is a widely used approach in software where independent tasks are dynamically created and scheduled at runtime. Recent systems have explored architectural support for TLP on field-programmable gate arrays (FPGAs), often leveraging high-level synthesis (HLS) to create processing elements (PEs). In this paper, we present Bombyx, a compiler toolchain that lowers OpenCilk programs into a Cilk-1-inspired intermediate representation, enabling efficient mapping of CPU-oriented TLP applications to spatial architectures on FPGAs. Unlike OpenCilk's implicit task model, which requires costly context switching in hardware, Cilk-1 adopts explicit continuation-passing - a model that better aligns with the streaming nature of FPGAs. Bombyx supports multiple compilation targets: one is an OpenCilk-compatible runtime for executing Cilk-1-style code using the OpenCilk backend, and another is a synthesizable PE generator designed for HLS tools like Vitis HLS. Additionally, we introduce a decoupled access-execute optimization that enables automatic generation of high-performance PEs, improving memory-compute overlap and overall throughput.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86Bombyx\u7f16\u8bd1\u5668\u5de5\u5177\u94fe\uff0c\u5c06OpenCilk\u7a0b\u5e8f\u8f6c\u6362\u4e3a\u57fa\u4e8eCilk-1\u98ce\u683c\u7684\u4e2d\u95f4\u8868\u793a\uff0c\u4ee5\u66f4\u9ad8\u6548\u5730\u5728FPGA\u4e0a\u5b9e\u73b0\u4efb\u52a1\u7ea7\u5e76\u884c\uff08TLP\uff09\uff0c\u5e76\u901a\u8fc7\u89e3\u8026\u8bbf\u5b58-\u6267\u884c\u4f18\u5316\u81ea\u52a8\u751f\u6210\u9ad8\u6027\u80fd\u5904\u7406\u5355\u5143\u3002", "motivation": "\u73b0\u6709OpenCilk\u7684\u9690\u5f0f\u4efb\u52a1\u6a21\u578b\u5728\u786c\u4ef6\u4e2d\u9700\u8981\u6602\u8d35\u7684\u4e0a\u4e0b\u6587\u5207\u6362\uff0c\u96be\u4ee5\u9ad8\u6548\u6620\u5c04\u5230FPGA\u7b49\u7a7a\u95f4\u67b6\u6784\uff1b\u800cCilk-1\u7684\u663e\u5f0f\u5ef6\u7eed\u4f20\u9012\u6a21\u578b\u66f4\u9002\u5408FPGA\u7684\u6d41\u5f0f\u7279\u6027\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u65b0\u7684\u7f16\u8bd1\u65b9\u6cd5\u6765\u652f\u6301\u8fd9\u79cd\u6620\u5c04\u3002", "method": "Bombyx\u5de5\u5177\u94fe\u5c06OpenCilk\u7a0b\u5e8f\u964d\u7ea7\u4e3aCilk-1\u98ce\u683c\u7684\u4e2d\u95f4\u8868\u793a\uff0c\u5e76\u63d0\u4f9b\u4e24\u4e2a\u7f16\u8bd1\u76ee\u6807\uff1a\u4e00\u662f\u517c\u5bb9OpenCilk\u8fd0\u884c\u65f6\u7684\u540e\u7aef\uff0c\u4e8c\u662f\u9762\u5411Vitis HLS\u7b49\u9ad8\u5c42\u6b21\u7efc\u5408\u5de5\u5177\u7684\u53ef\u7efc\u5408\u5904\u7406\u5355\u5143\uff08PE\uff09\u751f\u6210\u5668\uff1b\u540c\u65f6\u5f15\u5165\u89e3\u8026\u8bbf\u5b58-\u6267\u884c\u4f18\u5316\u4ee5\u63d0\u5347\u5185\u5b58\u4e0e\u8ba1\u7b97\u91cd\u53e0\u3002", "result": "Bombyx\u80fd\u591f\u81ea\u52a8\u751f\u6210\u9ad8\u6027\u80fdPE\uff0c\u5728FPGA\u4e0a\u6709\u6548\u652f\u6301CPU\u5bfc\u5411\u7684TLP\u5e94\u7528\uff0c\u63d0\u5347\u541e\u5410\u91cf\u548c\u5185\u5b58-\u8ba1\u7b97\u91cd\u53e0\u6548\u7387\u3002", "conclusion": "\u901a\u8fc7\u91c7\u7528Cilk-1\u7684\u663e\u5f0f\u4efb\u52a1\u6a21\u578b\u548c\u7f16\u8bd1\u4f18\u5316\uff0cBombyx\u663e\u8457\u63d0\u5347\u4e86TLP\u5e94\u7528\u5728FPGA\u4e0a\u7684\u6620\u5c04\u6548\u7387\u548c\u6027\u80fd\uff0c\u4e3a\u8f6f\u786c\u4ef6\u534f\u540c\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2511.20943", "categories": ["cs.MA", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.20943", "abs": "https://arxiv.org/abs/2511.20943", "authors": ["Chuhao Qin", "Alexandru Sorici", "Andrei Olaru", "Evangelos Pournaras", "Adina Magda Florea"], "title": "Resilient Charging Infrastructure via Decentralized Coordination of Electric Vehicles at Scale", "comment": "14 pages, 12 figures. This work has been submitted to the IEEE for possible publication", "summary": "The rapid adoption of electric vehicles (EVs) introduces major challenges for decentralized charging control. Existing decentralized approaches efficiently coordinate a large number of EVs to select charging stations while reducing energy costs, preventing power peak and preserving driver privacy. However, they often struggle under severe contingencies, such as station outages or unexpected surges in charging requests. These situations create competition for limited charging slots, resulting in long queues and reduced driver comfort. To address these limitations, we propose a novel collective learning-based coordination framework that allows EVs to balance individual comfort on their selections against system-wide efficiency, i.e., the overall queues across all stations. In the framework, EVs are recommended for adaptive charging behaviors that shift priority between comfort and efficiency, achieving Pareto-optimal trade-offs under varying station capacities and dynamic spatio-temporal EV distribution. Experiments using real-world data from EVs and charging stations show that the proposed approach outperforms baseline methods, significantly reducing travel and queuing time. The results reveal that, under uncertain charging conditions, EV drivers that behave selfishly or altruistically at the right moments achieve shorter waiting time than those maintaining moderate behavior throughout. Our findings under high fractions of station outages and adversarial EVs further demonstrate improved resilience and trustworthiness of decentralized EV charging infrastructure.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u96c6\u4f53\u5b66\u4e60\u7684\u53bb\u4e2d\u5fc3\u5316\u7535\u52a8\u6c7d\u8f66\u5145\u7535\u534f\u8c03\u6846\u67b6\uff0c\u5728\u4fdd\u969c\u7cfb\u7edf\u6548\u7387\u7684\u540c\u65f6\u63d0\u5347\u4e2a\u4f53\u9a7e\u9a76\u8212\u9002\u5ea6\uff0c\u5e76\u5728\u7ad9\u6869\u6545\u969c\u6216\u9700\u6c42\u6fc0\u589e\u7b49\u6781\u7aef\u60c5\u51b5\u4e0b\u5c55\u73b0\u51fa\u66f4\u5f3a\u7684\u9c81\u68d2\u6027\u3002", "motivation": "\u73b0\u6709\u53bb\u4e2d\u5fc3\u5316\u5145\u7535\u63a7\u5236\u65b9\u6cd5\u5728\u9762\u5bf9\u5145\u7535\u6869\u6545\u969c\u6216\u7a81\u53d1\u9ad8\u9700\u6c42\u7b49\u4e25\u91cd\u5f02\u5e38\u60c5\u51b5\u65f6\u8868\u73b0\u4e0d\u4f73\uff0c\u6613\u5bfc\u81f4\u6392\u961f\u8fc7\u957f\u548c\u7528\u6237\u4f53\u9a8c\u4e0b\u964d\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u96c6\u4f53\u5b66\u4e60\u9a71\u52a8\u7684\u534f\u8c03\u6846\u67b6\uff0c\u4f7f\u7535\u52a8\u6c7d\u8f66\u5728\u4e2a\u4f53\u8212\u9002\u5ea6\u4e0e\u7cfb\u7edf\u6574\u4f53\u6548\u7387\uff08\u5982\u5404\u7ad9\u70b9\u6392\u961f\u60c5\u51b5\uff09\u4e4b\u95f4\u52a8\u6001\u6743\u8861\uff0c\u5b9e\u73b0\u5e15\u7d2f\u6258\u6700\u4f18\uff1b\u901a\u8fc7\u771f\u5b9e\u6570\u636e\u5b9e\u9a8c\u9a8c\u8bc1\u5176\u6709\u6548\u6027\u3002", "result": "\u6240\u63d0\u65b9\u6cd5\u663e\u8457\u51cf\u5c11\u4e86\u884c\u9a76\u4e0e\u6392\u961f\u7b49\u5f85\u65f6\u95f4\uff1b\u7814\u7a76\u53d1\u73b0\uff0c\u5728\u4e0d\u786e\u5b9a\u6761\u4ef6\u4e0b\uff0c\u9002\u65f6\u91c7\u53d6\u81ea\u79c1\u6216\u5229\u4ed6\u884c\u4e3a\u7684\u9a7e\u9a76\u5458\u6bd4\u59cb\u7ec8\u91c7\u53d6\u4e2d\u5eb8\u7b56\u7565\u8005\u7b49\u5f85\u65f6\u95f4\u66f4\u77ed\uff1b\u5728\u9ad8\u6bd4\u4f8b\u6869\u635f\u548c\u5bf9\u6297\u6027\u7528\u6237\u573a\u666f\u4e0b\u7cfb\u7edf\u4ecd\u5177\u826f\u597d\u97e7\u6027\u3002", "conclusion": "\u8be5\u6846\u67b6\u6709\u6548\u63d0\u5347\u4e86\u53bb\u4e2d\u5fc3\u5316\u7535\u52a8\u6c7d\u8f66\u5145\u7535\u7cfb\u7edf\u7684\u9002\u5e94\u6027\u3001\u6548\u7387\u4e0e\u53ef\u9760\u6027\uff0c\u4e3a\u5e94\u5bf9\u73b0\u5b9e\u590d\u6742\u573a\u666f\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2511.21535", "categories": ["cs.DC", "cs.PF"], "pdf": "https://arxiv.org/pdf/2511.21535", "abs": "https://arxiv.org/abs/2511.21535", "authors": ["Morteza Sadeghi"], "title": "Modeling the Effect of Data Redundancy on Speedup in MLFMA Near-Field Computation", "comment": null, "summary": "The near-field (P2P) operator in the Multilevel Fast Multipole Algorithm (MLFMA) is a performance bottleneck on GPUs due to poor memory locality. This work introduces data redundancy to improve spatial locality by reducing memory access dispersion. For validation of results, we propose an analytical model based on a Locality metric that combines data volume and access dispersion to predict speedup trends without hardware-specific profiling. The approach is validated on two MLFMA-based applications: an electromagnetic solver (DBIM-MLFMA) with regular structure, and a stellar dynamics code (PhotoNs-2.0) with irregular particle distribution. Results show up to 7X kernel speedup due to improved cache behavior. However, increased data volume raises overheads in data restructuring, limiting end-to-end application speedup to 1.04X. While the model cannot precisely predict absolute speedups, it reliably captures performance trends across different problem sizes and densities. The technique is injectable into existing implementations with minimal code changes. This work demonstrates that data redundancy can enhance GPU performance for P2P operator, provided locality gains outweigh data movement costs.", "AI": {"tldr": "\u672c\u6587\u901a\u8fc7\u5f15\u5165\u6570\u636e\u5197\u4f59\u6539\u5584MLFMA\u4e2d\u8fd1\u573a\uff08P2P\uff09\u7b97\u5b50\u5728GPU\u4e0a\u7684\u5185\u5b58\u5c40\u90e8\u6027\uff0c\u4ece\u800c\u63d0\u5347\u6027\u80fd\uff1b\u63d0\u51fa\u57fa\u4e8e\u5c40\u90e8\u6027\u6307\u6807\u7684\u5206\u6790\u6a21\u578b\u9884\u6d4b\u52a0\u901f\u8d8b\u52bf\uff0c\u5e76\u5728\u4e24\u7c7b\u5e94\u7528\u4e2d\u9a8c\u8bc1\uff0c\u83b7\u5f97\u6700\u9ad87\u500d\u7684\u6838\u51fd\u6570\u52a0\u901f\uff0c\u4f46\u7aef\u5230\u7aef\u52a0\u901f\u53d7\u9650\u4e8e\u6570\u636e\u91cd\u6784\u5f00\u9500\u3002", "motivation": "MLFMA\u4e2d\u7684\u8fd1\u573a\uff08P2P\uff09\u7b97\u5b50\u5728GPU\u4e0a\u56e0\u5185\u5b58\u5c40\u90e8\u6027\u5dee\u800c\u6210\u4e3a\u6027\u80fd\u74f6\u9888\u3002", "method": "\u5f15\u5165\u6570\u636e\u5197\u4f59\u4ee5\u51cf\u5c11\u5185\u5b58\u8bbf\u95ee\u5206\u6563\u6027\uff0c\u63d0\u5347\u7a7a\u95f4\u5c40\u90e8\u6027\uff1b\u6784\u5efa\u7ed3\u5408\u6570\u636e\u91cf\u4e0e\u8bbf\u95ee\u5206\u6563\u6027\u7684\u5c40\u90e8\u6027\u6307\u6807\u6a21\u578b\uff0c\u7528\u4e8e\u9884\u6d4b\u52a0\u901f\u8d8b\u52bf\uff1b\u5728DBIM-MLFMA\u548cPhotoNs-2.0\u4e24\u4e2a\u5e94\u7528\u4e2d\u8fdb\u884c\u9a8c\u8bc1\u3002", "result": "\u6838\u51fd\u6570\u6700\u9ad8\u52a0\u901f\u8fbe7\u500d\uff0c\u4f46\u56e0\u6570\u636e\u91cd\u6784\u5f00\u9500\uff0c\u7aef\u5230\u7aef\u5e94\u7528\u52a0\u901f\u4ec5\u8fbe1.04\u500d\uff1b\u6240\u63d0\u6a21\u578b\u867d\u4e0d\u80fd\u7cbe\u786e\u9884\u6d4b\u7edd\u5bf9\u52a0\u901f\u6bd4\uff0c\u4f46\u80fd\u53ef\u9760\u53cd\u6620\u4e0d\u540c\u95ee\u9898\u89c4\u6a21\u548c\u5bc6\u5ea6\u4e0b\u7684\u6027\u80fd\u8d8b\u52bf\u3002", "conclusion": "\u6570\u636e\u5197\u4f59\u53ef\u6709\u6548\u63d0\u5347P2P\u7b97\u5b50\u5728GPU\u4e0a\u7684\u6027\u80fd\uff0c\u524d\u63d0\u662f\u5c40\u90e8\u6027\u6536\u76ca\u8d85\u8fc7\u6570\u636e\u79fb\u52a8\u6210\u672c\uff1b\u8be5\u65b9\u6cd5\u6613\u4e8e\u96c6\u6210\u5230\u73b0\u6709\u5b9e\u73b0\u4e2d\uff0c\u4e14\u5206\u6790\u6a21\u578b\u5bf9\u6027\u80fd\u8d8b\u52bf\u5177\u6709\u826f\u597d\u7684\u9884\u6d4b\u80fd\u529b\u3002"}}
{"id": "2511.21451", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2511.21451", "abs": "https://arxiv.org/abs/2511.21451", "authors": ["Flurin Arquint", "Oscar Casta\u00f1eda", "Gian Marti", "Christoph Studer"], "title": "A Jammer-Resilient 2.87 mm$^2$ 1.28 MS/s 310 mW Multi-Antenna Synchronization ASIC in 65 nm", "comment": "Presented at the 2025 IEEE European Solid-State Electronics Research Conference (ESSERC)", "summary": "We present the first ASIC implementation of jammer-resilient multi-antenna time synchronization. The ASIC implements a recent algorithm that mitigates jamming attacks on synchronization signals using multi-antenna processing. Our design supports synchronization between a single-antenna transmitter and a 16-antenna receiver while mitigating smart jammers with up to two transmit antennas. The fabricated 65 nm ASIC has a core area of 2.87 mm$^2$, consumes a power of 310 mW, and supports a sampling rate of 1.28 mega-samples per second (MS/s).", "AI": {"tldr": "\u672c\u6587\u9996\u6b21\u5b9e\u73b0\u4e86\u6297\u5e72\u6270\u591a\u5929\u7ebf\u65f6\u95f4\u540c\u6b65\u7684ASIC\u82af\u7247\uff0c\u652f\u6301\u5355\u53d116\u6536\u5929\u7ebf\u914d\u7f6e\uff0c\u53ef\u62b5\u5fa1\u6700\u591a\u4e24\u6839\u5929\u7ebf\u7684\u667a\u80fd\u5e72\u6270\uff0c\u91c7\u752865 nm\u5de5\u827a\uff0c\u6838\u5fc3\u9762\u79ef2.87 mm\u00b2\uff0c\u529f\u8017310 mW\uff0c\u91c7\u6837\u7387\u8fbe1.28 MS/s\u3002", "motivation": "\u73b0\u6709\u65f6\u95f4\u540c\u6b65\u673a\u5236\u6613\u53d7\u5e72\u6270\u653b\u51fb\u5f71\u54cd\uff0c\u5c24\u5176\u5728\u5173\u952e\u901a\u4fe1\u573a\u666f\u4e2d\uff0c\u4e9f\u9700\u5177\u5907\u6297\u5e72\u6270\u80fd\u529b\u7684\u9ad8\u53ef\u9760\u6027\u540c\u6b65\u65b9\u6848\u3002", "method": "\u91c7\u7528\u591a\u5929\u7ebf\u4fe1\u53f7\u5904\u7406\u7b97\u6cd5\uff0c\u5728ASIC\u4e2d\u5b9e\u73b0\u5bf9\u540c\u6b65\u4fe1\u53f7\u7684\u6297\u5e72\u6270\u5904\u7406\uff0c\u652f\u6301\u5355\u5929\u7ebf\u53d1\u5c04\u7aef\u4e0e16\u5929\u7ebf\u63a5\u6536\u7aef\u4e4b\u95f4\u7684\u540c\u6b65\uff0c\u5e76\u80fd\u5e94\u5bf9\u6700\u591a\u4e24\u6839\u5929\u7ebf\u7684\u667a\u80fd\u5e72\u6270\u3002", "result": "\u6210\u529f\u6d41\u724765 nm ASIC\u82af\u7247\uff0c\u6838\u5fc3\u9762\u79ef2.87 mm\u00b2\uff0c\u529f\u8017310 mW\uff0c\u91c7\u6837\u73871.28 MS/s\uff0c\u9a8c\u8bc1\u4e86\u6240\u63d0\u7b97\u6cd5\u7684\u786c\u4ef6\u53ef\u884c\u6027\u4e0e\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u5c55\u793a\u4e86\u6297\u5e72\u6270\u591a\u5929\u7ebf\u65f6\u95f4\u540c\u6b65\u5728\u4e13\u7528\u786c\u4ef6\u4e0a\u7684\u53ef\u884c\u6027\u548c\u9ad8\u6548\u6027\uff0c\u4e3a\u9ad8\u5b89\u5168\u901a\u4fe1\u7cfb\u7edf\u63d0\u4f9b\u4e86\u5173\u952e\u652f\u6491\u3002"}}
{"id": "2511.21510", "categories": ["cs.MA", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.21510", "abs": "https://arxiv.org/abs/2511.21510", "authors": ["Ke Zhang", "Xiaoning Zhao", "Ce Zheng", "Jiahong Ning", "Dandan Zhu", "Wenqi Zhang", "Chen Sun", "Toshiharu Sugawara"], "title": "Tool-RoCo: An Agent-as-Tool Self-organization Large Language Model Benchmark in Multi-robot Cooperation", "comment": "9 pages, 3 figures", "summary": "This study proposes Tool-RoCo, a novel benchmark for evaluating large language models (LLMs) in long-term multi-agent cooperation based on RoCo, a multi-robot cooperative benchmark. Recent research on LLM-based multi-agent systems has relied on predefined orchestration, while ignoring agent autonomy. Tool-RoCo treats other agents as tools and introduces cooperative tools, leveraging tool usage to evaluate multi-agent cooperation and self-organization. Tool usage means that each agent (LLM) selects a tool from a candidate set based on the current state, receives feedback, and adjusts its selection in subsequent rounds. To evaluate different autonomy levels, we propose four LLM paradigms: (1) centralized cooperation, where a single LLM allocates tools to all agents; (2) centralized self-organization, where a central LLM autonomously activates agents while keeping others inactive; (3) decentralized cooperation, where each agent has its own LLM and calls tools based on local information; and (4) self-organization, where a randomly chosen initial agent can request collaboration, activating additional agents via tool calls. Tool-RoCo includes three multi-robot tasks, SORT, PACK, and CABINET, to measure format and parameter accuracy and agent coordination through tool usage. The results using several LLMs showed that cooperative tools accounted for only 7.09% of all tools, indicating that LLM-based agents rarely invoked others as assistants. Moreover, activation tools accounted for 96.42%, suggesting that current LLMs tend to maintain active agents while seldom deactivating them for adaptive coordination. Tool-RoCo provides a systematic benchmark to evaluate LLM autonomy and cooperation in multi-agent tasks. Code and Demo: https://github.com/ColaZhang22/Tool-Roco", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86Tool-RoCo\uff0c\u4e00\u4e2a\u57fa\u4e8e\u591a\u673a\u5668\u4eba\u534f\u4f5c\u57fa\u51c6RoCo\u7684\u65b0\u8bc4\u6d4b\u6846\u67b6\uff0c\u7528\u4e8e\u8bc4\u4f30\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u957f\u671f\u591a\u667a\u80fd\u4f53\u5408\u4f5c\u4e2d\u7684\u81ea\u4e3b\u6027\u4e0e\u534f\u4f5c\u80fd\u529b\u3002\u8be5\u6846\u67b6\u5c06\u5176\u4ed6\u667a\u80fd\u4f53\u89c6\u4e3a\u5de5\u5177\uff0c\u901a\u8fc7\u5de5\u5177\u8c03\u7528\u673a\u5236\u8861\u91cf\u4e0d\u540c\u5408\u4f5c\u8303\u5f0f\u4e0b\u7684\u534f\u8c03\u8868\u73b0\uff0c\u5e76\u5728\u4e09\u4e2a\u4efb\u52a1\u4e0a\u8fdb\u884c\u4e86\u5b9e\u9a8c\uff0c\u63ed\u793a\u5f53\u524dLLM\u5728\u4e3b\u52a8\u534f\u4f5c\u548c\u52a8\u6001\u6fc0\u6d3b/\u505c\u7528\u667a\u80fd\u4f53\u65b9\u9762\u4ecd\u5b58\u5728\u660e\u663e\u4e0d\u8db3\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8eLLM\u7684\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u7814\u7a76\u591a\u4f9d\u8d56\u9884\u8bbe\u7f16\u6392\uff0c\u5ffd\u89c6\u4e86\u667a\u80fd\u4f53\u7684\u81ea\u4e3b\u6027\u3002\u4e3a\u7cfb\u7edf\u8bc4\u4f30LLM\u5728\u591a\u667a\u80fd\u4f53\u73af\u5883\u4e2d\u7684\u81ea\u7ec4\u7ec7\u4e0e\u534f\u4f5c\u80fd\u529b\uff0c\u4f5c\u8005\u63d0\u51faTool-RoCo\u57fa\u51c6\u3002", "method": "Tool-RoCo\u5c06\u5176\u4ed6\u667a\u80fd\u4f53\u89c6\u4e3a\u201c\u534f\u4f5c\u5de5\u5177\u201d\uff0c\u8bbe\u8ba1\u4e86\u56db\u79cdLLM\u5408\u4f5c\u8303\u5f0f\uff08\u96c6\u4e2d\u5f0f\u5408\u4f5c\u3001\u96c6\u4e2d\u5f0f\u81ea\u7ec4\u7ec7\u3001\u53bb\u4e2d\u5fc3\u5316\u5408\u4f5c\u3001\u81ea\u7ec4\u7ec7\uff09\uff0c\u5e76\u5728SORT\u3001PACK\u548cCABINET\u4e09\u4e2a\u591a\u673a\u5668\u4eba\u4efb\u52a1\u4e2d\uff0c\u901a\u8fc7\u5de5\u5177\u8c03\u7528\u7684\u683c\u5f0f\u3001\u53c2\u6570\u51c6\u786e\u6027\u548c\u534f\u8c03\u6027\u6765\u8bc4\u4f30\u6027\u80fd\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u5728\u6240\u6709\u5de5\u5177\u8c03\u7528\u4e2d\uff0c\u534f\u4f5c\u5de5\u5177\u4ec5\u53607.09%\uff0c\u8bf4\u660eLLM\u5f88\u5c11\u4e3b\u52a8\u8bf7\u6c42\u5176\u4ed6\u667a\u80fd\u4f53\u534f\u52a9\uff1b\u800c\u6fc0\u6d3b\u7c7b\u5de5\u5177\u5360\u6bd4\u9ad8\u8fbe96.42%\uff0c\u663e\u793a\u5f53\u524dLLM\u503e\u5411\u4e8e\u4fdd\u6301\u667a\u80fd\u4f53\u6d3b\u8dc3\uff0c\u7f3a\u4e4f\u52a8\u6001\u505c\u7528\u4ee5\u5b9e\u73b0\u81ea\u9002\u5e94\u534f\u8c03\u7684\u80fd\u529b\u3002", "conclusion": "Tool-RoCo\u4e3a\u8bc4\u4f30LLM\u5728\u591a\u667a\u80fd\u4f53\u4efb\u52a1\u4e2d\u7684\u81ea\u4e3b\u6027\u4e0e\u534f\u4f5c\u80fd\u529b\u63d0\u4f9b\u4e86\u4e00\u4e2a\u7cfb\u7edf\u6027\u57fa\u51c6\uff0c\u63ed\u793a\u4e86\u5f53\u524d\u6a21\u578b\u5728\u771f\u6b63\u81ea\u7ec4\u7ec7\u534f\u4f5c\u65b9\u9762\u7684\u5c40\u9650\u6027\u3002"}}
{"id": "2511.20813", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2511.20813", "abs": "https://arxiv.org/abs/2511.20813", "authors": ["Simon Hacks"], "title": "Train While You Fight -- Technical Requirements for Advanced Distributed Learning Platforms", "comment": "17 pages, submitted to CAiSE - International Conference on Advanced information Systems Engineering 2026", "summary": "\"Train While You Fight\" (TWYF) advocates for continuous learning that occurs during operations, not just before or after. This paper examines the technical requirements that advanced distributed learning (ADL) platforms must meet to support TWYF, and how existing software engineering patterns can fulfill these requirements. Using a Design Science Research approach, we (i) derive challenges from PfPC/NATO documentation and recent practice, (ii) define solution objectives, and (iii) conduct a systematic mapping from challenges to proven patterns. We identify seven technical challenges: interoperability, resilience, multilingual support, data security and privacy, scalability, platform independence, and modularity. We illustrate the patterns with a national use case from the German armed forces.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u4e86\u652f\u6301\u201c\u8fb9\u6218\u8fb9\u8bad\u201d\uff08TWYF\uff09\u7406\u5ff5\u7684\u5148\u8fdb\u5206\u5e03\u5f0f\u5b66\u4e60\uff08ADL\uff09\u5e73\u53f0\u6240\u9700\u6ee1\u8db3\u7684\u6280\u672f\u8981\u6c42\uff0c\u5e76\u901a\u8fc7\u8bbe\u8ba1\u79d1\u5b66\u7814\u7a76\u65b9\u6cd5\uff0c\u5c06\u6765\u81ea\u5317\u7ea6\u6587\u6863\u548c\u5b9e\u8df5\u4e2d\u7684\u6311\u6218\u6620\u5c04\u5230\u5df2\u6709\u7684\u8f6f\u4ef6\u5de5\u7a0b\u6a21\u5f0f\uff0c\u63d0\u51fa\u4e86\u4e03\u9879\u5173\u952e\u6280\u672f\u6311\u6218\u53ca\u5176\u89e3\u51b3\u65b9\u6848\uff0c\u5e76\u4ee5\u5fb7\u56fd\u6b66\u88c5\u90e8\u961f\u7684\u56fd\u5bb6\u7528\u4f8b\u52a0\u4ee5\u8bf4\u660e\u3002", "motivation": "\u4e3a\u5b9e\u73b0\u201c\u8fb9\u6218\u8fb9\u8bad\u201d\uff08TWYF\uff09\u8fd9\u4e00\u5728\u4f5c\u6218\u8fc7\u7a0b\u4e2d\u6301\u7eed\u5b66\u4e60\u7684\u7406\u5ff5\uff0c\u9700\u8981ADL\u5e73\u53f0\u5177\u5907\u7279\u5b9a\u6280\u672f\u80fd\u529b\uff1b\u7136\u800c\u5f53\u524d\u5e73\u53f0\u5c1a\u7f3a\u4e4f\u5bf9\u8fd9\u4e9b\u9700\u6c42\u7684\u7cfb\u7edf\u6027\u5206\u6790\u4e0e\u5e94\u5bf9\u3002", "method": "\u91c7\u7528\u8bbe\u8ba1\u79d1\u5b66\u7814\u7a76\u65b9\u6cd5\uff0c\u4ece\u5317\u7ea6\u53ca\u5b9e\u8df5\u6587\u732e\u4e2d\u63d0\u70bc\u6280\u672f\u6311\u6218\uff0c\u660e\u786e\u89e3\u51b3\u76ee\u6807\uff0c\u5e76\u7cfb\u7edf\u5730\u5c06\u6311\u6218\u6620\u5c04\u5230\u6210\u719f\u7684\u8f6f\u4ef6\u5de5\u7a0b\u6a21\u5f0f\u3002", "result": "\u8bc6\u522b\u51fa\u4e03\u5927\u6280\u672f\u6311\u6218\uff1a\u4e92\u64cd\u4f5c\u6027\u3001\u97e7\u6027\u3001\u591a\u8bed\u8a00\u652f\u6301\u3001\u6570\u636e\u5b89\u5168\u4e0e\u9690\u79c1\u3001\u53ef\u6269\u5c55\u6027\u3001\u5e73\u53f0\u65e0\u5173\u6027\u548c\u6a21\u5757\u5316\uff0c\u5e76\u901a\u8fc7\u5fb7\u56fd\u519b\u961f\u7684\u5b9e\u9645\u7528\u4f8b\u5c55\u793a\u4e86\u76f8\u5173\u8f6f\u4ef6\u6a21\u5f0f\u7684\u5e94\u7528\u3002", "conclusion": "\u73b0\u6709\u8f6f\u4ef6\u5de5\u7a0b\u6a21\u5f0f\u80fd\u591f\u6709\u6548\u5e94\u5bf9TWYF\u5bf9ADL\u5e73\u53f0\u63d0\u51fa\u7684\u6280\u672f\u6311\u6218\uff0c\u4e3a\u6784\u5efa\u652f\u6301\u5b9e\u6218\u4e2d\u6301\u7eed\u5b66\u4e60\u7684\u7cfb\u7edf\u63d0\u4f9b\u4e86\u53ef\u884c\u8def\u5f84\u3002"}}
{"id": "2511.21431", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2511.21431", "abs": "https://arxiv.org/abs/2511.21431", "authors": ["Lu Zhao", "Rong Shi", "Shaoqing Zhang", "Yueqiang Chen", "Baoguo He", "Hongfeng Sun", "Ziqing Yin", "Shangchao Su", "Zhiyan Cui", "Liang Dong", "Xiyuan Li", "Lingbin Wang", "Jianwei He", "Jiesong Ma", "Weikang Huang", "Jianglei Tong", "Dongdong Gao", "Jian Zhang", "Hong Tian"], "title": "MemFine: Memory-Aware Fine-Grained Scheduling for MoE Training", "comment": null, "summary": "The training of large-scale Mixture of Experts (MoE) models faces a critical memory bottleneck due to severe load imbalance caused by dynamic token routing. This imbalance leads to memory overflow on GPUs with limited capacity, constraining model scalability. Existing load balancing methods, which cap expert capacity, compromise model accuracy and fail on memory-constrained hardware. To address this, we propose MemFine, a memory-aware fine-grained scheduling framework for MoE training. MemFine decomposes the token distribution and expert computation into manageable chunks and employs a chunked recomputation strategy, dynamically optimized through a theoretical memory model to balance memory efficiency and throughput. Experiments demonstrate that MemFine reduces activation memory by 48.03% and improves throughput by 4.42% compared to full recomputation-based baselines, enabling stable large-scale MoE training on memory-limited GPUs.", "AI": {"tldr": "MemFine is a memory-aware scheduling framework that enables efficient large-scale Mixture of Experts (MoE) model training on memory-limited GPUs by reducing activation memory and improving throughput through fine-grained chunking and dynamic recomputation.", "motivation": "Training large-scale MoE models suffers from severe memory bottlenecks due to load imbalance from dynamic token routing, causing GPU memory overflow and limiting scalability\u2014existing capacity-capping methods hurt accuracy and fail under tight memory constraints.", "method": "MemFine decomposes token distribution and expert computation into chunks and applies a dynamically optimized chunked recomputation strategy guided by a theoretical memory model to balance memory usage and computational throughput.", "result": "MemFine reduces activation memory by 48.03% and increases throughput by 4.42% compared to full recomputation baselines, enabling stable MoE training on GPUs with limited memory.", "conclusion": "MemFine effectively addresses the memory bottleneck in large-scale MoE training, making it feasible to train such models on memory-constrained hardware without sacrificing accuracy or stability."}}
{"id": "2511.21461", "categories": ["cs.AR", "eess.SP"], "pdf": "https://arxiv.org/pdf/2511.21461", "abs": "https://arxiv.org/abs/2511.21461", "authors": ["Jonas Elmiger", "Fabian Stuber", "Oscar Casta\u00f1eda", "Gian Marti", "Christoph Studer"], "title": "A 0.32 mm$^2$ 100 Mb/s 223 mW ASIC in 22FDX for Joint Jammer Mitigation, Channel Estimation, and SIMO Data Detection", "comment": "Presented at the 2025 IEEE European Solid-State Electronics Research Conference (ESSERC)", "summary": "We present the first single-input multiple-output (SIMO) receiver ASIC that jointly performs jammer mitigation, channel estimation, and data detection. The ASIC implements a recent algorithm called siMultaneous mitigAtion, Estimation, and Detection (MAED). MAED mitigates smart jammers via spatial filtering using a nonlinear optimization problem that unifies jammer estimation and nulling, channel estimation, and data detection to achieve state-of-the-art error-rate performance under jamming. The design supports eight receive antennas and enables mitigation of smart jammers as well as of barrage jammers. The ASIC is fabricated in 22 nm FD-SOI, has a core area of 0.32 mm$^2$, and achieves a throughput of 100 Mb/s at 223 mW, thus delivering 3$\\times$ higher per-user throughput and 4.5$\\times$ higher area efficiency than the state-of-the-art jammer-resilient detector.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u6b3e\u65b0\u578b\u5355\u8f93\u5165\u591a\u8f93\u51fa\uff08SIMO\uff09\u63a5\u6536\u673aASIC\uff0c\u96c6\u6210\u4e86\u5bf9\u6297\u667a\u80fd\u5e72\u6270\u548c\u963b\u585e\u5e72\u6270\u7684\u6291\u5236\u3001\u4fe1\u9053\u4f30\u8ba1\u4e0e\u6570\u636e\u68c0\u6d4b\u529f\u80fd\uff0c\u57fa\u4e8e\u540d\u4e3aMAED\u7684\u8054\u5408\u4f18\u5316\u7b97\u6cd5\uff0c\u572822nm FD-SOI\u5de5\u827a\u4e0b\u5b9e\u73b0\u4e86\u9ad8\u541e\u5410\u91cf\u4e0e\u9ad8\u9762\u79ef\u6548\u7387\u3002", "motivation": "\u73b0\u6709\u6297\u5e72\u6270\u63a5\u6536\u673a\u5728\u9762\u5bf9\u667a\u80fd\u5e72\u6270\u65f6\u6027\u80fd\u53d7\u9650\uff0c\u4e14\u901a\u5e38\u5c06\u5e72\u6270\u6291\u5236\u3001\u4fe1\u9053\u4f30\u8ba1\u548c\u6570\u636e\u68c0\u6d4b\u5206\u6b65\u5904\u7406\uff0c\u96be\u4ee5\u5b9e\u73b0\u6700\u4f18\u6027\u80fd\u3002\u56e0\u6b64\uff0c\u4e9f\u9700\u4e00\u79cd\u80fd\u8054\u5408\u5904\u7406\u8fd9\u4e9b\u4efb\u52a1\u7684\u9ad8\u6548\u786c\u4ef6\u65b9\u6848\u3002", "method": "\u91c7\u7528siMultaneous mitigAtion, Estimation, and Detection\uff08MAED\uff09\u7b97\u6cd5\uff0c\u901a\u8fc7\u975e\u7ebf\u6027\u4f18\u5316\u7edf\u4e00\u5efa\u6a21\u5e72\u6270\u4f30\u8ba1\u4e0e\u7f6e\u96f6\u3001\u4fe1\u9053\u4f30\u8ba1\u548c\u6570\u636e\u68c0\u6d4b\uff0c\u5e76\u5728ASIC\u4e2d\u5b9e\u73b0\u8be5\u7b97\u6cd5\uff0c\u652f\u63018\u6839\u63a5\u6536\u5929\u7ebf\u3002", "result": "\u6240\u8bbe\u8ba1\u7684ASIC\u572822 nm FD-SOI\u5de5\u827a\u4e0b\u6838\u5fc3\u9762\u79ef\u4e3a0.32 mm\u00b2\uff0c\u529f\u8017223 mW\uff0c\u541e\u5410\u7387\u8fbe100 Mb/s\uff0c\u76f8\u6bd4\u73b0\u6709\u6297\u5e72\u6270\u68c0\u6d4b\u5668\uff0c\u6bcf\u7528\u6237\u541e\u5410\u91cf\u63d0\u53473\u500d\uff0c\u9762\u79ef\u6548\u7387\u63d0\u53474.5\u500d\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u9996\u6b21\u5b9e\u73b0\u4e86\u96c6\u6210\u5e72\u6270\u6291\u5236\u3001\u4fe1\u9053\u4f30\u8ba1\u4e0e\u6570\u636e\u68c0\u6d4b\u4e8e\u4e00\u4f53\u7684SIMO\u63a5\u6536\u673aASIC\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6297\u5e72\u6270\u6027\u80fd\u4e0e\u786c\u4ef6\u6548\u7387\uff0c\u4e3a\u672a\u6765\u9ad8\u9c81\u68d2\u6027\u65e0\u7ebf\u901a\u4fe1\u7cfb\u7edf\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848\u3002"}}
{"id": "2511.21572", "categories": ["cs.MA", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.21572", "abs": "https://arxiv.org/abs/2511.21572", "authors": ["Liming Yang", "Junyu Luo", "Xuanzhe Liu", "Yiling Lou", "Zhenpeng Chen"], "title": "BAMAS: Structuring Budget-Aware Multi-Agent Systems", "comment": "Accepted by AAAI 2026 (oral paper)", "summary": "Large language model (LLM)-based multi-agent systems have emerged as a powerful paradigm for enabling autonomous agents to solve complex tasks. As these systems scale in complexity, cost becomes an important consideration for practical deployment. However, existing work rarely addresses how to structure multi-agent systems under explicit budget constraints. In this paper, we propose BAMAS, a novel approach for building multi-agent systems with budget awareness. BAMAS first selects an optimal set of LLMs by formulating and solving an Integer Linear Programming problem that balances performance and cost. It then determines how these LLMs should collaborate by leveraging a reinforcement learning-based method to select the interaction topology. Finally, the system is instantiated and executed based on the selected agents and their collaboration topology. We evaluate BAMAS on three representative tasks and compare it with state-of-the-art agent construction methods. Results show that BAMAS achieves comparable performance while reducing cost by up to 86%.", "AI": {"tldr": "BAMAS is a budget-aware method for constructing multi-agent LLM systems that jointly optimizes model selection and collaboration topology, achieving up to 86% cost reduction with comparable performance.", "motivation": "Existing multi-agent LLM systems rarely consider explicit budget constraints despite rising deployment costs as system complexity increases.", "method": "BAMAS formulates an Integer Linear Programming problem to select a cost-effective set of LLMs and uses reinforcement learning to determine their optimal interaction topology.", "result": "Evaluated on three tasks, BAMAS matches the performance of state-of-the-art methods while cutting costs by up to 86%.", "conclusion": "BAMAS effectively enables the construction of cost-efficient multi-agent LLM systems without sacrificing performance, addressing a critical gap in practical deployment under budget constraints."}}
{"id": "2511.20933", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2511.20933", "abs": "https://arxiv.org/abs/2511.20933", "authors": ["Mootez Saad", "Boqi Chen", "Jos\u00e9 Antonio Hern\u00e1ndez L\u00f3pez", "D\u00e1niel Varr\u00f3", "Tushar Sharma"], "title": "Hierarchical Evaluation of Software Design Capabilities of Large Language Models of Code", "comment": "18 figures", "summary": "Large language models (LLMs) are being increasingly adopted in the software engineering domain, yet the robustness of their grasp on core software design concepts remains unclear. We conduct an empirical study to systematically evaluate their understanding of cohesion (intra-module) and coupling (inter-module). We programmatically generate poorly designed code fragments and test the DeepSeek-R1 model family ($14$B, $32$B, $70$B) under varying levels of guidance, from simple \\textit{Verification} to \\textit{Guided} and \\textit{Open-ended Generation}, while varying contextual noise by injecting distractor elements. While models exhibit a solid baseline understanding of both concepts in ideal conditions, their practical knowledge is fragile and highly asymmetrical. Reasoning about coupling proves brittle; performance collapses in noisy, open-ended scenarios, with F1 scores dropping by over $50\\%$. In contrast, the models' analysis of cohesion is remarkably robust to internal noise in guided tasks, showing little performance degradation. However, this resilience also fails when all guidance is removed. Reasoning-trace analysis confirms these failure modes, revealing \\textit{cognitive shortcutting} for coupling versus a more exhaustive (yet still failing) analysis for cohesion. To summarize, while LLMs can provide reliable assistance for recognizing design flaws, their ability to reason autonomously in noisy, realistic contexts is limited, highlighting the critical need for more scalable and robust program understanding capabilities.", "AI": {"tldr": "\u672c\u6587\u8bc4\u4f30\u4e86DeepSeek-R1\u7cfb\u5217\u5927\u8bed\u8a00\u6a21\u578b\u5bf9\u8f6f\u4ef6\u5de5\u7a0b\u4e2d\u5185\u805a\u6027\u548c\u8026\u5408\u6027\u6982\u5ff5\u7684\u7406\u89e3\u80fd\u529b\uff0c\u53d1\u73b0\u5728\u7406\u60f3\u6761\u4ef6\u4e0b\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728\u566a\u58f0\u5e72\u6270\u6216\u7f3a\u4e4f\u5f15\u5bfc\u7684\u5f00\u653e\u573a\u666f\u4e0b\u63a8\u7406\u80fd\u529b\u663e\u8457\u4e0b\u964d\uff0c\u5c24\u5176\u5728\u8026\u5408\u6027\u5206\u6790\u4e0a\u66f4\u4e3a\u8106\u5f31\u3002", "motivation": "\u5f53\u524d\u5927\u8bed\u8a00\u6a21\u578b\u5728\u8f6f\u4ef6\u5de5\u7a0b\u4e2d\u88ab\u5e7f\u6cdb\u4f7f\u7528\uff0c\u4f46\u5176\u5bf9\u6838\u5fc3\u8f6f\u4ef6\u8bbe\u8ba1\u6982\u5ff5\uff08\u5982\u5185\u805a\u4e0e\u8026\u5408\uff09\u7684\u7406\u89e3\u662f\u5426\u7a33\u5065\u5c1a\u4e0d\u660e\u786e\uff0c\u56e0\u6b64\u6709\u5fc5\u8981\u7cfb\u7edf\u8bc4\u4f30\u5176\u5728\u4e0d\u540c\u6761\u4ef6\u4e0b\u7684\u8868\u73b0\u3002", "method": "\u901a\u8fc7\u7a0b\u5e8f\u5316\u751f\u6210\u8bbe\u8ba1\u4e0d\u826f\u7684\u4ee3\u7801\u7247\u6bb5\uff0c\u5728\u4e0d\u540c\u5f15\u5bfc\u7a0b\u5ea6\uff08\u9a8c\u8bc1\u3001\u5f15\u5bfc\u3001\u5f00\u653e\u5f0f\u751f\u6210\uff09\u548c\u4e0d\u540c\u566a\u58f0\u6c34\u5e73\uff08\u6ce8\u5165\u5e72\u6270\u5143\u7d20\uff09\u4e0b\uff0c\u6d4b\u8bd5DeepSeek-R1\u7cfb\u5217\u6a21\u578b\uff0814B\u300132B\u300170B\uff09\u5bf9\u5185\u805a\u6027\u548c\u8026\u5408\u6027\u7684\u7406\u89e3\u80fd\u529b\uff0c\u5e76\u5206\u6790\u5176\u63a8\u7406\u8f68\u8ff9\u3002", "result": "\u6a21\u578b\u5728\u7406\u60f3\u6761\u4ef6\u4e0b\u5bf9\u4e24\u4e2a\u6982\u5ff5\u6709\u8f83\u597d\u7406\u89e3\uff0c\u4f46\u5728\u566a\u58f0\u548c\u5f00\u653e\u573a\u666f\u4e2d\u8868\u73b0\u8106\u5f31\uff1a\u8026\u5408\u6027\u5206\u6790F1\u5206\u6570\u4e0b\u964d\u8d8550%\uff0c\u800c\u5185\u805a\u6027\u5728\u5f15\u5bfc\u4efb\u52a1\u4e2d\u8f83\u7a33\u5065\uff0c\u4f46\u5728\u65e0\u5f15\u5bfc\u65f6\u540c\u6837\u5931\u6548\uff1b\u63a8\u7406\u8f68\u8ff9\u663e\u793a\u6a21\u578b\u5bf9\u8026\u5408\u91c7\u53d6\u8ba4\u77e5\u6377\u5f84\uff0c\u5bf9\u5185\u805a\u5219\u8fdb\u884c\u66f4\u8be6\u5c3d\u4f46\u4ecd\u5931\u8d25\u7684\u5206\u6790\u3002", "conclusion": "\u5c3d\u7ba1\u5927\u8bed\u8a00\u6a21\u578b\u5728\u8bc6\u522b\u8bbe\u8ba1\u7f3a\u9677\u65b9\u9762\u53ef\u63d0\u4f9b\u53ef\u9760\u8f85\u52a9\uff0c\u4f46\u5728\u566a\u58f0\u5e72\u6270\u548c\u7f3a\u4e4f\u5f15\u5bfc\u7684\u771f\u5b9e\u573a\u666f\u4e2d\u81ea\u4e3b\u63a8\u7406\u80fd\u529b\u6709\u9650\uff0c\u4e9f\u9700\u63d0\u5347\u5176\u7a0b\u5e8f\u7406\u89e3\u7684\u53ef\u6269\u5c55\u6027\u4e0e\u9c81\u68d2\u6027\u3002"}}
{"id": "2511.21612", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2511.21612", "abs": "https://arxiv.org/abs/2511.21612", "authors": ["Shahir Abdullah", "Syed Rohit Zaman"], "title": "Diagonal Scaling: A Multi-Dimensional Resource Model and Optimization Framework for Distributed Databases", "comment": null, "summary": "Modern cloud databases present scaling as a binary decision: scale-out by adding nodes or scale-up by increasing per-node resources. This one-dimensional view is limiting because database performance, cost, and coordination overhead emerge from the joint interaction of horizontal elasticity and per-node CPU, memory, network bandwidth, and storage IOPS. As a result, systems often overreact to load spikes, underreact to memory pressure, or oscillate between suboptimal states. We introduce the Scaling Plane, a two-dimensional model in which each distributed database configuration is represented as a point (H, V), with H denoting node count and V a vector of resources. Over this plane, we define smooth approximations of latency, throughput, coordination overhead, and monetary cost, providing a unified view of performance trade-offs. We show analytically and empirically that optimal scaling trajectories frequently lie along diagonal paths: sequences of joint horizontal and vertical adjustments that simultaneously exploit cluster parallelism and per-node improvements. To compute such actions, we propose DIAGONALSCALE, a discrete local-search algorithm that evaluates horizontal, vertical, and diagonal moves in the Scaling Plane and selects the configuration minimizing a multi-objective function subject to SLA constraints. Using synthetic surfaces, microbenchmarks, and experiments on distributed SQL and KV systems, we demonstrate that diagonal scaling reduces p95 latency by up to 40 percent, lowers cost-per-query by up to 37 percent, and reduces rebalancing by 2 to 5 times compared to horizontal-only and vertical-only autoscaling. Our results highlight the need for multi-dimensional scaling models and provide a foundation for next-generation autoscaling in cloud database systems.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u201c\u7f29\u653e\u5e73\u9762\u201d\uff08Scaling Plane\uff09\u6a21\u578b\uff0c\u5c06\u6570\u636e\u5e93\u6269\u7f29\u5bb9\u89c6\u4e3a\u4e8c\u7ef4\u95ee\u9898\uff08\u8282\u70b9\u6570\u91cf\u4e0e\u5355\u8282\u70b9\u8d44\u6e90\uff09\uff0c\u5e76\u8bbe\u8ba1DIAGONALSCALE\u7b97\u6cd5\u8fdb\u884c\u8054\u5408\u4f18\u5316\uff0c\u5728\u5ef6\u8fdf\u3001\u6210\u672c\u548c\u91cd\u5e73\u8861\u5f00\u9500\u65b9\u9762\u663e\u8457\u4f18\u4e8e\u4f20\u7edf\u4ec5\u6c34\u5e73\u6216\u4ec5\u5782\u76f4\u6269\u7f29\u5bb9\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u4e91\u6570\u636e\u5e93\u5c06\u6269\u7f29\u5bb9\u7b80\u5316\u4e3a\u6c34\u5e73\u6269\u5c55\uff08\u52a0\u8282\u70b9\uff09\u6216\u5782\u76f4\u6269\u5c55\uff08\u63d0\u5347\u5355\u8282\u70b9\u8d44\u6e90\uff09\u7684\u4e8c\u5143\u9009\u62e9\uff0c\u5ffd\u7565\u4e86\u4e8c\u8005\u534f\u540c\u5bf9\u6027\u80fd\u3001\u6210\u672c\u548c\u534f\u8c03\u5f00\u9500\u7684\u7efc\u5408\u5f71\u54cd\uff0c\u5bfc\u81f4\u7cfb\u7edf\u54cd\u5e94\u8d1f\u8f7d\u53d8\u5316\u65f6\u6548\u7387\u4f4e\u4e0b\u3002", "method": "\u63d0\u51fa\u4e8c\u7ef4\u201c\u7f29\u653e\u5e73\u9762\u201d\u6a21\u578b\uff0c\u5176\u4e2d\u6bcf\u4e2a\u914d\u7f6e\u8868\u793a\u4e3a(H, V)\u70b9\uff0c\u5e76\u5728\u6b64\u5e73\u9762\u4e0a\u5b9a\u4e49\u5ef6\u8fdf\u3001\u541e\u5410\u91cf\u3001\u534f\u8c03\u5f00\u9500\u548c\u6210\u672c\u7684\u5e73\u6ed1\u8fd1\u4f3c\uff1b\u8bbe\u8ba1DIAGONALSCALE\u79bb\u6563\u5c40\u90e8\u641c\u7d22\u7b97\u6cd5\uff0c\u5728\u6ee1\u8db3SLA\u7ea6\u675f\u4e0b\uff0c\u901a\u8fc7\u8bc4\u4f30\u6c34\u5e73\u3001\u5782\u76f4\u53ca\u5bf9\u89d2\u7ebf\u79fb\u52a8\uff0c\u9009\u62e9\u591a\u76ee\u6807\u6700\u4f18\u914d\u7f6e\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u76f8\u6bd4\u4ec5\u6c34\u5e73\u6216\u4ec5\u5782\u76f4\u6269\u7f29\u5bb9\uff0c\u5bf9\u89d2\u7ebf\u6269\u7f29\u5bb9\u53ef\u5c06p95\u5ef6\u8fdf\u6700\u591a\u964d\u4f4e40%\uff0c\u6bcf\u67e5\u8be2\u6210\u672c\u6700\u591a\u964d\u4f4e37%\uff0c\u5e76\u5c06\u91cd\u5e73\u8861\u64cd\u4f5c\u51cf\u5c112\u81f35\u500d\u3002", "conclusion": "\u591a\u7ef4\u6269\u7f29\u5bb9\u6a21\u578b\u80fd\u66f4\u5168\u9762\u5730\u6355\u6349\u4e91\u6570\u636e\u5e93\u7684\u6027\u80fd\u6743\u8861\uff0c\u5bf9\u89d2\u7ebf\u6269\u7f29\u5bb9\u7b56\u7565\u663e\u8457\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\uff0c\u4e3a\u4e0b\u4e00\u4ee3\u4e91\u6570\u636e\u5e93\u81ea\u52a8\u6269\u7f29\u5bb9\u7cfb\u7edf\u63d0\u4f9b\u4e86\u65b0\u8303\u5f0f\u3002"}}
{"id": "2511.21661", "categories": ["cs.DC", "cs.DB"], "pdf": "https://arxiv.org/pdf/2511.21661", "abs": "https://arxiv.org/abs/2511.21661", "authors": ["Beth Plale", "Neelesh Karthikeyan", "Isuru Gamage", "Joe Stubbs", "Sachith Withana"], "title": "AI/ML Model Cards in Edge AI Cyberinfrastructure: towards Agentic AI", "comment": null, "summary": "AI/ML model cards can contain a benchmarked evaluation of an AI/ML model against intended use but a one time assessment during model training does not get at how and where a model is actually used over its lifetime. Through Patra Model Cards embedded in the ICICLE AI Institute software ecosystem we study model cards as dynamic objects. The study reported here assesses the benefits and tradeoffs of adopting the Model Context Protocol (MCP) as an interface to the Patra Model Card server. Quantitative assessment shows the overhead of MCP as compared to a REST interface. The core question however is of active sessions enabled by MCP; this is a qualitative question of fit and use in the context of dynamic model cards that we address as well.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u5c06\u6a21\u578b\u4e0a\u4e0b\u6587\u534f\u8bae\uff08MCP\uff09\u4f5c\u4e3aPatra\u6a21\u578b\u5361\u670d\u52a1\u5668\u63a5\u53e3\u7684\u4f18\u52a3\uff0c\u5bf9\u6bd4\u5176\u4e0eREST\u63a5\u53e3\u7684\u5f00\u9500\uff0c\u5e76\u63a2\u8ba8MCP\u5728\u652f\u6301\u52a8\u6001\u6a21\u578b\u5361\u4e2d\u7684\u9002\u7528\u6027\u3002", "motivation": "\u9759\u6001\u7684\u4e00\u6b21\u6027\u6a21\u578b\u8bc4\u4f30\u65e0\u6cd5\u53cd\u6620AI/ML\u6a21\u578b\u5728\u5176\u751f\u547d\u5468\u671f\u4e2d\u7684\u5b9e\u9645\u4f7f\u7528\u60c5\u51b5\uff0c\u56e0\u6b64\u9700\u8981\u5c06\u6a21\u578b\u5361\u89c6\u4e3a\u52a8\u6001\u5bf9\u8c61\u8fdb\u884c\u6301\u7eed\u8ffd\u8e2a\u548c\u66f4\u65b0\u3002", "method": "\u901a\u8fc7\u5728ICICLE AI\u7814\u7a76\u6240\u8f6f\u4ef6\u751f\u6001\u7cfb\u7edf\u4e2d\u5d4c\u5165Patra\u6a21\u578b\u5361\uff0c\u91c7\u7528\u5b9a\u91cf\u65b9\u6cd5\u6bd4\u8f83MCP\u4e0eREST\u63a5\u53e3\u7684\u6027\u80fd\u5f00\u9500\uff0c\u5e76\u901a\u8fc7\u5b9a\u6027\u5206\u6790\u8bc4\u4f30MCP\u5728\u652f\u6301\u52a8\u6001\u6a21\u578b\u5361\u573a\u666f\u4e0b\u7684\u9002\u7528\u6027\u548c\u7528\u6237\u4f53\u9a8c\u3002", "result": "\u5b9a\u91cf\u7ed3\u679c\u663e\u793aMCP\u76f8\u6bd4REST\u63a5\u53e3\u5b58\u5728\u4e00\u5b9a\u5f00\u9500\uff1b\u5b9a\u6027\u5206\u6790\u5219\u8868\u660eMCP\u80fd\u66f4\u597d\u5730\u652f\u6301\u52a8\u6001\u6a21\u578b\u5361\u6240\u9700\u7684\u6d3b\u8dc3\u4f1a\u8bdd\u548c\u4e0a\u4e0b\u6587\u4ea4\u4e92\u3002", "conclusion": "MCP\u867d\u6709\u6027\u80fd\u5f00\u9500\uff0c\u4f46\u5728\u652f\u6301\u52a8\u6001\u6a21\u578b\u5361\u7684\u4e0a\u4e0b\u6587\u611f\u77e5\u548c\u6301\u7eed\u4ea4\u4e92\u65b9\u9762\u5177\u6709\u4f18\u52bf\uff0c\u9002\u5408\u7528\u4e8e\u9700\u8981\u957f\u671f\u8ffd\u8e2a\u6a21\u578b\u4f7f\u7528\u60c5\u51b5\u7684\u573a\u666f\u3002"}}
{"id": "2511.21022", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2511.21022", "abs": "https://arxiv.org/abs/2511.21022", "authors": ["Guancheng Lin", "Xiao Yu", "Jacky Keung", "Xing Hu", "Xin Xia", "Alex X. Liu"], "title": "Lightweight Model Editing for LLMs to Correct Deprecated API Recommendations", "comment": null, "summary": "Pre-trained or fine-tuned on large code corpora, Large Language Models (LLMs) have demonstrated strong performance in code completion tasks. However, their embedded knowledge is constrained by the timeliness of training data, which often includes code using deprecated APIs. Consequently, LLMs frequently generate deprecated APIs that will no longer be supported in future versions of third-party libraries. While retraining LLMs on updated codebases could refresh their API knowledge, this approach is computationally expensive. Recently, lightweight model editing methods have emerged to efficiently correct specific knowledge in LLMs. However, it remains unclear whether these methods can effectively update deprecated API knowledge and enable edited models to generate up-to-date APIs. To address this gap, we conduct the first systematic study applying 10 state-of-the-art model editing techniques to update deprecated API knowledge in three LLMs: Qwen2.5-Coder, StarCoder2, and DeepSeek-Coder. We introduce EDAPIBench, a dedicated benchmark featuring over 70 deprecated APIs from 8 popular Python libraries, with more than 3,000 editing instances. Our results show that the parameter-efficient fine-tuning method AdaLoRA achieves the best performance in enabling edited models to generate correct, up-to-date APIs, but falls short in Specificity (i.e., the editing influences untargeted knowledge). To resolve this, we propose AdaLoRA-L, which defines \"Common API Layers\" (layers within the LLMs with high importance across all APIs, storing general knowledge and excluded from editing) and restricts edits exclusively to \"Specific API Layers\" (layers with high importance only for the target API, storing the API-specific knowledge). Experimental results demonstrate that AdaLoRA-L significantly improves Specificity while maintaining comparable performance across other evaluation metrics.", "AI": {"tldr": "\u672c\u6587\u9996\u6b21\u7cfb\u7edf\u8bc4\u4f30\u4e8610\u79cd\u524d\u6cbf\u6a21\u578b\u7f16\u8f91\u65b9\u6cd5\u5728\u66f4\u65b0\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u4e2d\u5df2\u5f03\u7528API\u77e5\u8bc6\u65b9\u9762\u7684\u6548\u679c\uff0c\u5e76\u63d0\u51fa\u4e86\u6539\u8fdb\u65b9\u6cd5AdaLoRA-L\uff0c\u901a\u8fc7\u533a\u5206\u201c\u901a\u7528API\u5c42\u201d\u4e0e\u201c\u7279\u5b9aAPI\u5c42\u201d\u663e\u8457\u63d0\u5347\u4e86\u7f16\u8f91\u7684\u7279\u5f02\u6027\uff0c\u540c\u65f6\u4fdd\u6301\u5176\u4ed6\u6307\u6807\u6027\u80fd\u3002", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b\u56e0\u8bad\u7ec3\u6570\u636e\u65f6\u6548\u6027\u9650\u5236\uff0c\u5e38\u751f\u6210\u5df2\u5f03\u7528\u7684API\uff1b\u91cd\u65b0\u8bad\u7ec3\u6210\u672c\u9ad8\uff0c\u800c\u73b0\u6709\u8f7b\u91cf\u7ea7\u6a21\u578b\u7f16\u8f91\u65b9\u6cd5\u662f\u5426\u80fd\u6709\u6548\u66f4\u65b0\u6b64\u7c7b\u77e5\u8bc6\u5c1a\u4e0d\u660e\u786e\u3002", "method": "\u6784\u5efa\u5305\u542b70\u591a\u4e2a\u5df2\u5f03\u7528API\u548c3000\u591a\u4e2a\u7f16\u8f91\u5b9e\u4f8b\u7684\u57fa\u51c6EDA PI Bench\uff0c\u5728Qwen2.5-Coder\u3001StarCoder2\u548cDeepSeek-Coder\u4e09\u79cd\u6a21\u578b\u4e0a\u8bc4\u4f3010\u79cd\u6a21\u578b\u7f16\u8f91\u6280\u672f\uff1b\u63d0\u51faAdaLoRA-L\u65b9\u6cd5\uff0c\u4ec5\u5728\u7279\u5b9aAPI\u5c42\u8fdb\u884c\u7f16\u8f91\u4ee5\u63d0\u5347\u7279\u5f02\u6027\u3002", "result": "AdaLoRA\u5728\u751f\u6210\u6b63\u786e\u65b0API\u65b9\u9762\u8868\u73b0\u6700\u4f73\u4f46\u7279\u5f02\u6027\u4e0d\u8db3\uff1bAdaLoRA-L\u663e\u8457\u63d0\u5347\u7279\u5f02\u6027\uff0c\u540c\u65f6\u4fdd\u6301\u5176\u4ed6\u6307\u6807\u76f8\u5f53\u3002", "conclusion": "\u901a\u8fc7\u533a\u5206\u6a21\u578b\u4e2d\u901a\u7528\u4e0e\u7279\u5b9a\u77e5\u8bc6\u5c42\uff0cAdaLoRA-L\u6709\u6548\u89e3\u51b3\u4e86\u6a21\u578b\u7f16\u8f91\u4e2d\u7684\u7279\u5f02\u6027\u95ee\u9898\uff0c\u4e3a\u9ad8\u6548\u66f4\u65b0LLM\u4e2d\u7684\u8fc7\u65f6API\u77e5\u8bc6\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848\u3002"}}
{"id": "2511.21151", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2511.21151", "abs": "https://arxiv.org/abs/2511.21151", "authors": ["M. Alecci", "P. Jim\u00e9nez", "J. Samhi", "T. Bissyand\u00e9", "J. Klein"], "title": "Exploring Hidden Geographic Disparities in Android Apps", "comment": null, "summary": "While mobile app evolution has been widely studied, geographical variation in app behavior remains largely unexplored. This paper presents a large-scale study of location-based Android app differentiation, uncovering two important and underexamined phenomena with security and fairness implications. First, we introduce GeoTwins: apps that are functionally similar and share branding but are released under different package names across countries. Despite their similarity, GeoTwins often diverge in requested permissions, third-party libraries, and privacy disclosures. Second, we examine the Android App Bundle ecosystem and reveal unexpected regional differences in supposedly consistent base.apk files. Contrary to common assumptions, even base.apk files vary by region, exposing hidden customizations that may affect app behavior or security.\n  These discrepancies have concrete consequences. Geographically distinct variants can lead the same app to be labeled benign in one malware study but suspicious in another, depending on the region of download. Such hidden variation undermines reproducibility and introduces geographic bias into assessments of security, privacy, and functionality. It also raises ethical concerns about transparency and consent: visually identical Google Play listings may mask subtle but important differences.\n  To study these issues, we built a distributed app collection pipeline spanning multiple regions and analyzed thousands of apps. We also release a dataset of 81,963 GeoTwins to support future work. Our findings reveal systemic regional disparities in mobile software, with implications for researchers, developers, platform architects, and policymakers.", "AI": {"tldr": "\u672c\u6587\u63ed\u793a\u4e86Android\u5e94\u7528\u5728\u4e0d\u540c\u5730\u7406\u533a\u57df\u5b58\u5728\u663e\u8457\u5dee\u5f02\uff0c\u63d0\u51fa\u4e86\u201cGeoTwins\u201d\u73b0\u8c61\u548cApp Bundle\u4e2dbase.apk\u7684\u533a\u57df\u5b9a\u5236\u95ee\u9898\uff0c\u6307\u51fa\u8fd9\u4e9b\u9690\u85cf\u5dee\u5f02\u5bf9\u5b89\u5168\u3001\u9690\u79c1\u3001\u516c\u5e73\u6027\u548c\u7814\u7a76\u53ef\u590d\u73b0\u6027\u6784\u6210\u6311\u6218\uff0c\u5e76\u53d1\u5e03\u4e86\u5305\u542b81,963\u4e2aGeoTwins\u7684\u6570\u636e\u96c6\u3002", "motivation": "\u5c3d\u7ba1\u79fb\u52a8\u5e94\u7528\u6f14\u5316\u5df2\u88ab\u5e7f\u6cdb\u7814\u7a76\uff0c\u4f46\u5176\u5730\u7406\u5dee\u5f02\u5c1a\u672a\u5f97\u5230\u5145\u5206\u5173\u6ce8\u3002\u4f5c\u8005\u65e8\u5728\u63ed\u793a\u5730\u7406\u4f4d\u7f6e\u5982\u4f55\u5bfc\u81f4\u540c\u4e00\u5e94\u7528\u5728\u6743\u9650\u3001\u7b2c\u4e09\u65b9\u5e93\u548c\u9690\u79c1\u653f\u7b56\u7b49\u65b9\u9762\u4ea7\u751f\u4e0d\u4e00\u81f4\uff0c\u8fdb\u800c\u5f71\u54cd\u5b89\u5168\u8bc4\u4f30\u3001\u7814\u7a76\u53ef\u590d\u73b0\u6027\u53ca\u7528\u6237\u77e5\u60c5\u6743\u3002", "method": "\u6784\u5efa\u4e86\u4e00\u4e2a\u8de8\u533a\u57df\u7684\u5206\u5e03\u5f0f\u5e94\u7528\u91c7\u96c6\u7cfb\u7edf\uff0c\u6536\u96c6\u5e76\u5206\u6790\u6570\u5343\u6b3eAndroid\u5e94\u7528\uff1b\u8bc6\u522b\u51fa\u529f\u80fd\u76f8\u4f3c\u4f46\u5305\u540d\u4e0d\u540c\u7684\u201cGeoTwins\u201d\uff1b\u5bf9\u6bd4\u4e0d\u540c\u5730\u533a\u4e0b\u8f7d\u7684base.apk\u6587\u4ef6\u4ee5\u68c0\u6d4b\u9690\u85cf\u7684\u533a\u57df\u5b9a\u5236\u884c\u4e3a\u3002", "result": "\u53d1\u73b0\u4e86\u4e24\u7c7b\u91cd\u8981\u73b0\u8c61\uff1a\uff081\uff09GeoTwins\u5728\u6743\u9650\u3001\u5e93\u548c\u9690\u79c1\u62ab\u9732\u65b9\u9762\u5b58\u5728\u663e\u8457\u5dee\u5f02\uff1b\uff082\uff09\u5373\u4f7f\u57fa\u7840APK\uff08base.apk\uff09\u4e5f\u56e0\u5730\u533a\u800c\u5f02\u3002\u8fd9\u4e9b\u5dee\u5f02\u53ef\u80fd\u5bfc\u81f4\u540c\u4e00\u5e94\u7528\u5728\u4e0d\u540c\u5730\u533a\u7684\u5b89\u5168\u8bc4\u4f30\u7ed3\u679c\u4e0d\u4e00\u81f4\uff0c\u5e76\u9020\u6210\u5730\u7406\u504f\u89c1\u3002", "conclusion": "\u79fb\u52a8\u5e94\u7528\u5b58\u5728\u7cfb\u7edf\u6027\u7684\u533a\u57df\u5dee\u5f02\uff0c\u8fd9\u5bf9\u5b89\u5168\u7814\u7a76\u3001\u5e73\u53f0\u8bbe\u8ba1\u3001\u5f00\u53d1\u8005\u5b9e\u8df5\u548c\u653f\u7b56\u5236\u5b9a\u5747\u6709\u6df1\u8fdc\u5f71\u54cd\u3002\u7814\u7a76\u547c\u5401\u63d0\u5347\u900f\u660e\u5ea6\uff0c\u5e76\u5f3a\u8c03\u9700\u5c06\u5730\u7406\u56e0\u7d20\u7eb3\u5165\u79fb\u52a8\u8f6f\u4ef6\u5206\u6790\u6846\u67b6\u3002"}}
{"id": "2511.21197", "categories": ["cs.SE", "cs.HC"], "pdf": "https://arxiv.org/pdf/2511.21197", "abs": "https://arxiv.org/abs/2511.21197", "authors": ["Paolo Buono", "Mary Cerullo", "Stefano Cirillo", "Giuseppe Desolda", "Francesco Greco", "Emanuela Guglielmi", "Grazia Margarella", "Giuseppe Polese", "Simone Scalabrino", "Cesare Tucci"], "title": "Bug Detective and Quality Coach: Developers' Mental Models of AI-Assisted IDE Tools", "comment": null, "summary": "AI-assisted tools support developers in performing cognitively demanding tasks such as bug detection and code readability assessment. Despite the advancements in the technical characteristics of these tools, little is known about how developers mentally model them and how mismatches affect trust, control, and adoption. We conducted six co-design workshops with 58 developers to elicit their mental models about AI-assisted bug detection and readability features. It emerged that developers conceive bug detection tools as \\textit{bug detectives}, which warn users only in case of critical issues, guaranteeing transparency, actionable feedback, and confidence cues. Readability assessment tools, on the other hand, are envisioned as \\textit{quality coaches}, which provide contextual, personalized, and progressive guidance. Trust, in both tasks, depends on the clarity of explanations, timing, and user control. A set of design principles for Human-Centered AI in IDEs has been distilled, aiming to balance disruption with support, conciseness with depth, and automation with human agency.", "AI": {"tldr": "\u8be5\u7814\u7a76\u901a\u8fc7\u516d\u573a\u8054\u5408\u8bbe\u8ba1\u5de5\u4f5c\u574a\uff0c\u63a2\u7d22\u4e8658\u540d\u5f00\u53d1\u8005\u5bf9AI\u8f85\u52a9\u7f3a\u9677\u68c0\u6d4b\u4e0e\u4ee3\u7801\u53ef\u8bfb\u6027\u8bc4\u4f30\u5de5\u5177\u7684\u5fc3\u7406\u6a21\u578b\uff0c\u5e76\u63d0\u70bc\u51fa\u4ee5\u4eba\u4e3a\u6838\u5fc3\u7684IDE\u4e2dAI\u5de5\u5177\u8bbe\u8ba1\u539f\u5219\u3002", "motivation": "\u5c3d\u7ba1AI\u8f85\u52a9\u5f00\u53d1\u5de5\u5177\u5728\u6280\u672f\u4e0a\u4e0d\u65ad\u8fdb\u6b65\uff0c\u4f46\u5f00\u53d1\u8005\u5982\u4f55\u7406\u89e3\u8fd9\u4e9b\u5de5\u5177\uff08\u5373\u5176\u5fc3\u7406\u6a21\u578b\uff09\u4ee5\u53ca\u6a21\u578b\u4e0d\u5339\u914d\u5982\u4f55\u5f71\u54cd\u4fe1\u4efb\u3001\u63a7\u5236\u548c\u91c7\u7eb3\u4ecd\u4e0d\u6e05\u695a\u3002", "method": "\u5f00\u5c55\u516d\u573a\u8054\u5408\u8bbe\u8ba1\u5de5\u4f5c\u574a\uff0c\u5171\u9080\u8bf758\u540d\u5f00\u53d1\u8005\u53c2\u4e0e\uff0c\u901a\u8fc7\u8ba8\u8bba\u5f15\u51fa\u4ed6\u4eec\u5bf9AI\u8f85\u52a9\u7f3a\u9677\u68c0\u6d4b\u548c\u53ef\u8bfb\u6027\u8bc4\u4f30\u529f\u80fd\u7684\u5fc3\u7406\u6a21\u578b\u3002", "result": "\u5f00\u53d1\u8005\u5c06\u7f3a\u9677\u68c0\u6d4b\u5de5\u5177\u89c6\u4e3a\u201c\u7f3a\u9677\u4fa6\u63a2\u201d\uff0c\u5f3a\u8c03\u5173\u952e\u95ee\u9898\u9884\u8b66\u3001\u900f\u660e\u6027\u3001\u53ef\u64cd\u4f5c\u53cd\u9988\u548c\u4fe1\u5fc3\u63d0\u793a\uff1b\u5c06\u53ef\u8bfb\u6027\u8bc4\u4f30\u5de5\u5177\u89c6\u4e3a\u201c\u8d28\u91cf\u6559\u7ec3\u201d\uff0c\u671f\u671b\u83b7\u5f97\u60c5\u5883\u5316\u3001\u4e2a\u6027\u5316\u548c\u6e10\u8fdb\u5f0f\u6307\u5bfc\u3002\u4e24\u7c7b\u5de5\u5177\u7684\u4fe1\u4efb\u5747\u4f9d\u8d56\u4e8e\u89e3\u91ca\u6e05\u6670\u5ea6\u3001\u65f6\u673a\u548c\u7528\u6237\u63a7\u5236\u6743\u3002", "conclusion": "\u7814\u7a76\u63d0\u70bc\u51fa\u4e00\u5957\u9762\u5411IDE\u4e2d\u4eba\u7c7b\u4e2d\u5fc3AI\u7684\u8bbe\u8ba1\u539f\u5219\uff0c\u65e8\u5728\u5e73\u8861\u5e72\u6270\u4e0e\u652f\u6301\u3001\u7b80\u6d01\u4e0e\u6df1\u5ea6\u3001\u81ea\u52a8\u5316\u4e0e\u4eba\u7c7b\u80fd\u52a8\u6027\u3002"}}
{"id": "2511.21380", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2511.21380", "abs": "https://arxiv.org/abs/2511.21380", "authors": ["Jingyi Chen", "Xiaoyan Guo", "Songqiang Chen", "Shing-Chi Cheung", "Jiasi Shen"], "title": "Multi-Agent Systems for Dataset Adaptation in Software Engineering: Capabilities, Limitations, and Future Directions", "comment": null, "summary": "Automating the adaptation of software engineering (SE) research artifacts across datasets is essential for scalability and reproducibility, yet it remains largely unstudied. Recent advances in large language model (LLM)-based multi-agent systems, such as GitHub Copilot's agent mode, promise to automate complex development workflows through coordinated reasoning, code generation, and tool interaction. This paper presents the first empirical study on how state-of-the-art multi-agent systems perform in dataset adaptation tasks. We evaluate Copilot, backed by GPT-4.1 and Claude Sonnet 4, on adapting SE research artifacts from benchmark repositories including ROCODE and LogHub2.0. Through a five-stage evaluation pipeline (file comprehension, code editing, command generation, validation, and final execution), we measure success rates, analyze failure patterns, and assess prompt-based interventions designed to enhance agent performance. Results show that current systems can identify key files and generate partial adaptations but rarely produce functionally correct implementations. Prompt-level interventions, especially providing execution error messages and reference code, substantially improve structural similarity to ground truth (from 7.25% to 67.14%), highlighting the importance of contextual and feedback-driven guidance. Our findings reveal both the promise and limitations of today's multi-agent LLM systems for dataset adaptation, and suggest concrete directions for building more reliable, self-correcting agents in future SE research.", "AI": {"tldr": "\u672c\u6587\u9996\u6b21\u5b9e\u8bc1\u7814\u7a76\u4e86\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\uff08\u5982GitHub Copilot\uff09\u5728\u8f6f\u4ef6\u5de5\u7a0b\u7814\u7a76\u4e2d\u8de8\u6570\u636e\u96c6\u9002\u914d\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\uff0c\u53d1\u73b0\u5176\u867d\u80fd\u8bc6\u522b\u5173\u952e\u6587\u4ef6\u5e76\u751f\u6210\u90e8\u5206\u9002\u914d\u4ee3\u7801\uff0c\u4f46\u5f88\u5c11\u4ea7\u51fa\u529f\u80fd\u6b63\u786e\u7684\u5b9e\u73b0\uff1b\u901a\u8fc7\u63d0\u4f9b\u6267\u884c\u9519\u8bef\u4fe1\u606f\u548c\u53c2\u8003\u4ee3\u7801\u7b49\u63d0\u793a\u5e72\u9884\uff0c\u53ef\u663e\u8457\u63d0\u5347\u8f93\u51fa\u4e0e\u771f\u5b9e\u7ed3\u679c\u7684\u7ed3\u6784\u76f8\u4f3c\u5ea6\u3002", "motivation": "\u81ea\u52a8\u5316\u9002\u914d\u8f6f\u4ef6\u5de5\u7a0b\u7814\u7a76\u5236\u54c1\u4ee5\u9002\u5e94\u4e0d\u540c\u6570\u636e\u96c6\u5bf9\u53ef\u6269\u5c55\u6027\u548c\u53ef\u590d\u73b0\u6027\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u76ee\u524d\u76f8\u5173\u7814\u7a76\u4ecd\u5341\u5206\u7f3a\u4e4f\u3002\u968f\u7740\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u7684\u53d1\u5c55\uff0c\u63a2\u7d22\u5176\u5728\u6b64\u7c7b\u4efb\u52a1\u4e2d\u7684\u80fd\u529b\u5177\u6709\u91cd\u8981\u610f\u4e49\u3002", "method": "\u4f5c\u8005\u6784\u5efa\u4e86\u4e00\u4e2a\u4e94\u9636\u6bb5\u8bc4\u4f30\u6d41\u7a0b\uff08\u6587\u4ef6\u7406\u89e3\u3001\u4ee3\u7801\u7f16\u8f91\u3001\u547d\u4ee4\u751f\u6210\u3001\u9a8c\u8bc1\u548c\u6700\u7ec8\u6267\u884c\uff09\uff0c\u5728ROCODE\u548cLogHub2.0\u7b49\u57fa\u51c6\u4ed3\u5e93\u4e0a\u8bc4\u4f30Copilot\uff08\u57fa\u4e8eGPT-4.1\u548cClaude Sonnet 4\uff09\u7684\u8868\u73b0\uff0c\u5e76\u6d4b\u8bd5\u4e86\u591a\u79cd\u63d0\u793a\u5e72\u9884\u7b56\u7565\u7684\u6548\u679c\u3002", "result": "\u5f53\u524d\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u80fd\u8bc6\u522b\u5173\u952e\u6587\u4ef6\u5e76\u751f\u6210\u90e8\u5206\u9002\u914d\uff0c\u4f46\u6781\u5c11\u4ea7\u51fa\u529f\u80fd\u6b63\u786e\u7684\u5b9e\u73b0\uff1b\u5f15\u5165\u6267\u884c\u9519\u8bef\u4fe1\u606f\u548c\u53c2\u8003\u4ee3\u7801\u7b49\u63d0\u793a\u5e72\u9884\u540e\uff0c\u8f93\u51fa\u4e0e\u771f\u5b9e\u7ed3\u679c\u7684\u7ed3\u6784\u76f8\u4f3c\u5ea6\u4ece7.25%\u663e\u8457\u63d0\u5347\u81f367.14%\u3002", "conclusion": "\u5f53\u524d\u591a\u667a\u80fd\u4f53LLM\u7cfb\u7edf\u5728\u6570\u636e\u96c6\u9002\u914d\u4efb\u52a1\u4e2d\u5c55\u73b0\u51fa\u6f5c\u529b\u4f46\u4e5f\u5b58\u5728\u660e\u663e\u5c40\u9650\uff0c\u672a\u6765\u9700\u8bbe\u8ba1\u66f4\u5177\u4e0a\u4e0b\u6587\u611f\u77e5\u80fd\u529b\u548c\u53cd\u9988\u9a71\u52a8\u673a\u5236\u7684\u81ea\u4fee\u6b63\u667a\u80fd\u4f53\u4ee5\u63d0\u5347\u53ef\u9760\u6027\u3002"}}
{"id": "2511.21382", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2511.21382", "abs": "https://arxiv.org/abs/2511.21382", "authors": ["Bei Chu", "Yang Feng", "Kui Liu", "Zifan Nan", "Zhaoqiang Guo", "Baowen Xu"], "title": "Large Language Models for Unit Test Generation: Achievements, Challenges, and the Road Ahead", "comment": "33 pages, 8 figures", "summary": "Unit testing is an essential yet laborious technique for verifying software and mitigating regression risks. Although classic automated methods effectively explore program structures, they often lack the semantic information required to produce realistic inputs and assertions. Large Language Models (LLMs) address this limitation by utilizing by leveraging their data-driven knowledge of code semantics and programming patterns. To analyze the state of the art in this domain, we conducted a systematic literature review of 115 publications published between May 2021 and August 2025. We propose a unified taxonomy based on the unit test generation lifecycle that treats LLMs as stochastic generators requiring systematic engineering constraints. This framework analyzes the literature regarding core generative strategies and a set of enhancement techniques ranging from pre-generation context enrichment to post-generation quality assurance. Our analysis reveals that prompt engineering has emerged as the dominant utilization strategy and accounts for 89% of the studies due to its flexibility. We find that iterative validation and repair loops have become the standard mechanism to ensure robust usability and lead to significant improvements in compilation and execution pass rates. However, critical challenges remain regarding the weak fault detection capabilities of generated tests and the lack of standardized evaluation benchmarks. We conclude with a roadmap for future research that emphasizes the progression towards autonomous testing agents and hybrid systems combining LLMs with traditional software engineering tools. This survey provides researchers and practitioners with a comprehensive perspective on converting the potential of LLMs into industrial-grade testing solutions.", "AI": {"tldr": "\u672c\u6587\u5bf92021\u5e745\u6708\u81f32025\u5e748\u6708\u95f4\u53d1\u8868\u7684115\u7bc7\u5173\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u5355\u5143\u6d4b\u8bd5\u751f\u6210\u4e2d\u5e94\u7528\u7684\u6587\u732e\u8fdb\u884c\u4e86\u7cfb\u7edf\u7efc\u8ff0\uff0c\u63d0\u51fa\u57fa\u4e8e\u5355\u5143\u6d4b\u8bd5\u751f\u6210\u751f\u547d\u5468\u671f\u7684\u7edf\u4e00\u5206\u7c7b\u6cd5\uff0c\u6307\u51fa\u63d0\u793a\u5de5\u7a0b\u662f\u4e3b\u6d41\u7b56\u7565\uff08\u536089%\uff09\uff0c\u8fed\u4ee3\u9a8c\u8bc1\u4e0e\u4fee\u590d\u663e\u8457\u63d0\u5347\u6d4b\u8bd5\u901a\u8fc7\u7387\uff0c\u4f46\u4ecd\u9762\u4e34\u6545\u969c\u68c0\u6d4b\u80fd\u529b\u5f31\u548c\u7f3a\u4e4f\u6807\u51c6\u8bc4\u4f30\u57fa\u51c6\u7b49\u6311\u6218\uff0c\u5e76\u5c55\u671b\u4e86\u81ea\u4e3b\u6d4b\u8bd5\u667a\u80fd\u4f53\u4e0e\u6df7\u5408\u7cfb\u7edf\u7684\u672a\u6765\u65b9\u5411\u3002", "motivation": "\u4f20\u7edf\u81ea\u52a8\u5316\u5355\u5143\u6d4b\u8bd5\u65b9\u6cd5\u867d\u80fd\u6709\u6548\u63a2\u7d22\u7a0b\u5e8f\u7ed3\u6784\uff0c\u4f46\u7f3a\u4e4f\u8bed\u4e49\u4fe1\u606f\uff0c\u96be\u4ee5\u751f\u6210\u771f\u5b9e\u8f93\u5165\u548c\u65ad\u8a00\uff1b\u800c\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u51ed\u501f\u5176\u5bf9\u4ee3\u7801\u8bed\u4e49\u548c\u7f16\u7a0b\u6a21\u5f0f\u7684\u6570\u636e\u9a71\u52a8\u7406\u89e3\uff0c\u6709\u671b\u5f25\u8865\u8fd9\u4e00\u4e0d\u8db3\uff0c\u56e0\u6b64\u6709\u5fc5\u8981\u7cfb\u7edf\u68b3\u7406\u8be5\u9886\u57df\u7684\u7814\u7a76\u73b0\u72b6\u4e0e\u53d1\u5c55\u8def\u5f84\u3002", "method": "\u4f5c\u8005\u5bf92021\u5e745\u6708\u81f32025\u5e748\u6708\u95f4\u53d1\u8868\u7684115\u7bc7\u76f8\u5173\u6587\u732e\u8fdb\u884c\u4e86\u7cfb\u7edf\u6027\u7efc\u8ff0\uff0c\u63d0\u51fa\u4e00\u4e2a\u57fa\u4e8e\u5355\u5143\u6d4b\u8bd5\u751f\u6210\u751f\u547d\u5468\u671f\u7684\u7edf\u4e00\u5206\u7c7b\u6846\u67b6\uff0c\u5c06LLMs\u89c6\u4e3a\u9700\u7cfb\u7edf\u5de5\u7a0b\u7ea6\u675f\u7684\u968f\u673a\u751f\u6210\u5668\uff0c\u5e76\u4ece\u751f\u6210\u7b56\u7565\u3001\u4e0a\u4e0b\u6587\u589e\u5f3a\u5230\u540e\u5904\u7406\u8d28\u91cf\u4fdd\u969c\u7b49\u65b9\u9762\u8fdb\u884c\u5206\u6790\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u63d0\u793a\u5de5\u7a0b\u662f\u4e3b\u5bfc\u6027\u4f7f\u7528\u7b56\u7565\uff08\u5360\u6bd489%\uff09\uff0c\u8fed\u4ee3\u9a8c\u8bc1\u4e0e\u4fee\u590d\u673a\u5236\u5df2\u6210\u4e3a\u63d0\u5347\u7f16\u8bd1\u548c\u6267\u884c\u901a\u8fc7\u7387\u7684\u6807\u51c6\u505a\u6cd5\uff1b\u4f46\u751f\u6210\u6d4b\u8bd5\u7684\u7f3a\u9677\u68c0\u6d4b\u80fd\u529b\u4ecd\u8f83\u5f31\uff0c\u4e14\u7f3a\u4e4f\u7edf\u4e00\u7684\u8bc4\u4f30\u57fa\u51c6\u3002", "conclusion": "\u672a\u6765\u7814\u7a76\u5e94\u805a\u7126\u4e8e\u53d1\u5c55\u81ea\u4e3b\u6d4b\u8bd5\u667a\u80fd\u4f53\u53ca\u878d\u5408LLMs\u4e0e\u4f20\u7edf\u8f6f\u4ef6\u5de5\u7a0b\u5de5\u5177\u7684\u6df7\u5408\u7cfb\u7edf\uff0c\u4ee5\u63a8\u52a8LLMs\u5728\u5de5\u4e1a\u7ea7\u6d4b\u8bd5\u4e2d\u7684\u5b9e\u9645\u5e94\u7528\u3002"}}
