<div id=toc></div>

# Table of Contents

- [cs.DC](#cs.DC) [Total: 8]
- [cs.NI](#cs.NI) [Total: 8]
- [cs.SE](#cs.SE) [Total: 13]
- [cs.AR](#cs.AR) [Total: 3]
- [cs.MA](#cs.MA) [Total: 5]


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [1] [Dodoor: Efficient Randomized Decentralized Scheduling with Load Caching for Heterogeneous Tasks and Clusters](https://arxiv.org/abs/2510.12889)
*Wei Da,Evangelia Kalyvianaki*

Main category: cs.DC

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: This paper introduces Dodoor, an efficient randomized decentralized scheduler
designed for task scheduling in modern data centers. Dodoor leverages advanced
research on the weighted balls-into-bins model with b-batched setting. Unlike
other decentralized schedulers that rely on real-time probing of remote
servers, Dodoor makes scheduling decisions based on cached server information,
which is updated in batches, to reduce communication overheads. To schedule
tasks with dynamic, multidimensional resource requirements in heterogeneous
cluster, Dodoor uses a novel load score to measure servers' loads for each
scheduled task. This score captures the anti-affinity between servers and tasks
in contrast to the commonly used heuristic of counting pending tasks to balance
load. On a 101-node heterogeneous cluster, Dodoor is evaluated using two
workloads: (i) simulated Azure virtual machines placements and (ii) real
serverless Python functions executions in Docker. The evaluation shows that
Dodoor reduces scheduling messages by 55--66% on both workloads. Dodoor can
also increase throughput by up to 33.2% and 21.5%, reduce mean makespan latency
by 12.1% and 7.2%, and improve tail latency by 21.9% and 24.6% across the two
workloads.

</details>


### [2] [Scrutiny new framework in integrated distributed reliable systems](https://arxiv.org/abs/2510.13203)
*Mehdi Zekriyapanah Gashti*

Main category: cs.DC

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: In this paper we represent a new framework for integrated distributed
systems. In the proposed framework we have used three parts to increase
Satisfaction and Performance of this framework. At first we analyse integrated
systems and their evolution process and also ERPSD and ERPDRT framework briefly
then we explain the new FDIRS framework. Finally we compare the results of
simulation of the new framework with presented frameworks. Result showed In
FIDRS framework, the technique of heterogeneous distributed data base is used
to improve Performance and speed in responding to users. Finally by using FDIRS
framework we succeeded to increase Efficiency, Performance and reliability of
integrated systems and remove some of previous frameworks problems.

</details>


### [3] [BanaServe: Unified KV Cache and Dynamic Module Migration for Balancing Disaggregated LLM Serving in AI Infrastructure](https://arxiv.org/abs/2510.13223)
*Yiyuan He,Minxian Xu,Jingfeng Wu,Jianmin Hu,Chong Ma,Min Shen,Le Chen,Chengzhong Xu,Lin Qu,Kejiang Ye*

Main category: cs.DC

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Large language models (LLMs) are increasingly deployed in AI infrastructure,
driving the need for high throughput, resource efficient serving systems.
Disaggregated LLM serving, which separates prompt prefill from auto-regressive
decode, has emerged as a promising architecture by isolating their
heterogeneous compute and memory demands. However, current disaggregated
systems face three key limitations: (i) static resource allocation cannot adapt
to highly dynamic workloads, causing over-provisioning that wastes resources or
under-provisioning that violates service level objectives (SLOs); (ii) inherent
load imbalance between prefill and decode stages, where prefill is
compute-bound and decode is memory-bound, causes under-utilization in one tier
while the other becomes a bottleneck; and (iii) prefix cache aware routing
skews load distribution, as high cache hit rate prefill nodes attract
disproportionately more requests, further degrading balance and efficiency. To
address these issues, we present BanaServe, a dynamic orchestration framework
that continuously rebalances computational and memory resources across prefill
and decode instances while eliminating hotspots induced by cache. BanaServe
introduces layer level weight migration, attention level Key Value Cache (KV
Cache) migration, and Global KV Cache Store sharing with layer wise overlapped
transmission, enabling both coarse grained (layer level) and fine grained
(attention level) load redistribution with minimal latency overhead. These
mechanisms allow routers to perform purely load aware scheduling, unconstrained
by cache placement. Compared to vLLM, BanaServe achieves 1.2x-3.9x higher
throughput with 3.9%-78.4% lower total processing time, and outperforms
DistServe by 1.1x-2.8x in throughput with 1.4%-70.1% latency reduction.

</details>


### [4] [Distributed Reductions for the Maximum Weight Independent Set Problem](https://arxiv.org/abs/2510.13306)
*Jannick Borowitz,Ernestine Gro√ümann,Mattthias Schimek*

Main category: cs.DC

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Finding maximum-weight independent sets in graphs is an important NP-hard
optimization problem. Given a vertex-weighted graph $G$, the task is to find a
subset of pairwise non-adjacent vertices of $G$ with maximum weight. Most
recently published practical exact algorithms and heuristics for this problem
use a variety of data-reduction rules to compute (near-)optimal solutions.
Applying these rules results in an equivalent instance of reduced size. An
optimal solution to the reduced instance can be easily used to construct an
optimal solution for the original input.
  In this work, we present the first distributed-memory parallel reduction
algorithms for this problem, targeting graphs beyond the scale of previous
sequential approaches. Furthermore, we propose the first distributed
reduce-and-greedy and reduce-and-peel algorithms for finding a maximum weight
independent set heuristically.
  In our practical evaluation, our experiments on up to $1024$ processors
demonstrate good scalability of our distributed reduce algorithms while
maintaining good reduction impact. Our asynchronous reduce-and-peel approach
achieves an average speedup of $33\times$ over a sequential state-of-the-art
reduce-and-peel approach on 36 real-world graphs with a solution quality close
to the sequential algorithm. Our reduce-and-greedy algorithms even achieve
average speedups of up to $50\times$ at the cost of a lower solution quality.
Moreover, our distributed approach allows us to consider graphs with more than
one billion vertices and 17 billion edges.

</details>


### [5] [Service-Level Energy Modeling and Experimentation for Cloud-Native Microservices](https://arxiv.org/abs/2510.13447)
*Julian Legler,Sebastian Werner,Maria C. Borges,Stefan Tai*

Main category: cs.DC

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Microservice architectures have become the dominant paradigm for cloud-native
systems, offering flexibility and scalability. However, this shift has also led
to increased demand for cloud resources, contributing to higher energy
consumption and carbon emissions. While existing research has focused on
measuring fine-grained energy usage of CPU and memory at the container level,
or on system-wide assessments, these approaches often overlook the energy
impact of cross-container service interactions, especially those involving
network and storage for auxiliary services such as observability and system
monitoring. To address this gap, we introduce a service-level energy model that
captures the distributed nature of microservice execution across containers.
Our model is supported by an experimentation tool that accounts for energy
consumption not just in CPU and memory, but also in network and storage
components. We validate our approach through extensive experimentation with
diverse experiment configurations of auxiliary services for a popular
open-source cloud-native microservice application. Results show that omitting
network and storage can lead to an underestimation of auxiliary service energy
use by up to 63%, highlighting the need for more comprehensive energy
assessments in the design of energy-efficient microservice architectures.

</details>


### [6] [Adaptive Rescheduling in Prefill-Decode Disaggregated LLM Inference](https://arxiv.org/abs/2510.13668)
*Zhibin Wang,Zetao Hong,Xue Li,Zibo Wang,Shipeng Li,Qingkai Meng,Qing Wang,Chengying Huan,Rong Gu,Sheng Zhong,Chen Tian*

Main category: cs.DC

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Large Language Model (LLM) inference has emerged as a fundamental paradigm.
In real-world scenarios, variations in output length cause severe workload
imbalance in the decode phase, particularly for long-output reasoning tasks.
Existing systems, such as PD disaggregation architectures, rely on static
prefill-to-decode scheduling, which often results in SLO violations and OOM
failures under evolving decode workloads.
  In this paper, we propose ARES, an adaptive decoding rescheduling system
powered by length prediction to anticipate future workloads. Our core
contributions include: (1) A lightweight and continuous LLM-native prediction
method that leverages LLM hidden state to model remaining generation length
with high precision (reducing MAE by 49.42%) and low overhead (cutting
predictor parameters by 93.28%); (2) A rescheduling solution in decode phase
with : A dynamic balancing mechanism that integrates current and predicted
workloads, reducing P99 TPOT by 74.77% and achieving up to 2.24 times higher
goodput.

</details>


### [7] [FIRST: Federated Inference Resource Scheduling Toolkit for Scientific AI Model Access](https://arxiv.org/abs/2510.13724)
*Aditya Tanikanti,Benoit C√¥t√©,Yanfei Guo,Le Chen,Nickolaus Saint,Ryan Chard,Ken Raffenetti,Rajeev Thakur,Thomas Uram,Ian Foster,Michael E. Papka,Venkatram Vishwanath*

Main category: cs.DC

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: We present the Federated Inference Resource Scheduling Toolkit (FIRST), a
framework enabling Inference-as-a-Service across distributed High-Performance
Computing (HPC) clusters. FIRST provides cloud-like access to diverse AI
models, like Large Language Models (LLMs), on existing HPC infrastructure.
Leveraging Globus Auth and Globus Compute, the system allows researchers to run
parallel inference workloads via an OpenAI-compliant API on private, secure
environments. This cluster-agnostic API allows requests to be distributed
across federated clusters, targeting numerous hosted models. FIRST supports
multiple inference backends (e.g., vLLM), auto-scales resources, maintains
"hot" nodes for low-latency execution, and offers both high-throughput batch
and interactive modes. The framework addresses the growing demand for private,
secure, and scalable AI inference in scientific workflows, allowing researchers
to generate billions of tokens daily on-premises without relying on commercial
cloud infrastructure.

</details>


### [8] [Tight Conditions for Binary-Output Tasks under Crashes](https://arxiv.org/abs/2510.13755)
*Timoth√© Albouy,Antonio Fern√°ndez Anta,Chryssis Georgiou,Nicolas Nicolaou,Junlang Wang*

Main category: cs.DC

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: This paper explores necessary and sufficient system conditions to solve
distributed tasks with binary outputs (\textit{i.e.}, tasks with output values
in $\{0,1\}$). We focus on the distinct output sets of values a task can
produce (intentionally disregarding validity and value multiplicity),
considering that some processes may output no value. In a distributed system
with $n$ processes, of which up to $t \leq n$ can crash, we provide a complete
characterization of the tight conditions on $n$ and $t$ under which every class
of tasks with binary outputs is solvable, for both synchronous and asynchronous
systems. This output-set approach yields highly general results: it unifies
multiple distributed computing problems, such as binary consensus and symmetry
breaking, and it produces impossibility proofs that hold for stronger task
formulations, including those that consider validity, account for value
multiplicity, or move beyond binary outputs.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [9] [Toward Hyper-Dimensional Connectivity in Beyond 6G: A Conceptual Framework](https://arxiv.org/abs/2510.12896)
*Ekram Hossain,Angelo Vera-Rivera*

Main category: cs.NI

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Cellular wireless networks enable mobile broadband connectivity for
Internet-based applications through their radio access and core network
infrastructure. While Fifth-Generation (5G) cellular systems are currently
being deployed, ongoing research on cellular technologies primarily focuses on
Sixth-Generation (6G) networks to set the stage for developing standards for
these systems. Therefore, the time has come to articulate the visions for
beyond 6G (B6G) systems. In this article, we present a visionary framework
toward hyper-dimensional connectivity in B6G that enables wireless access to
hyper-immersive Internet technologies. Our contributions include a conceptual
framework for B6G cellular systems with jointly integrated communication,
cognition, computing, and cyber-physical capabilities as core connectivity
dimensions, a set of technical definitions outlining potential use cases and
system-level requirements, a mapping of prospective technology enablers, and a
forward-looking research agenda for B6G systems. The conceptual discussions in
this article would be helpful for identifying innovation drivers, shaping
long-term technical goals, and defining research agendas for the future of
mobile broadband technologies.

</details>


### [10] [Towards xApp Conflict Evaluation with Explainable Machine Learning and Causal Inference in O-RAN](https://arxiv.org/abs/2510.13031)
*Pragya Sharma,Shihua Sun,Shachi Deshpande,Angelos Stavrou,Haining Wang*

Main category: cs.NI

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: The Open Radio Access Network (O-RAN) architecture enables a flexible,
vendor-neutral deployment of 5G networks by disaggregating base station
components and supporting third-party xApps for near real-time RAN control.
However, the concurrent operation of multiple xApps can lead to conflicting
control actions, which may cause network performance degradation. In this work,
we propose a framework for xApp conflict management that combines explainable
machine learning and causal inference to evaluate the causal relationships
between RAN Control Parameters (RCPs) and Key Performance Indicators (KPIs). We
use model explainability tools such as SHAP to identify RCPs that jointly
affect the same KPI, signaling potential conflicts, and represent these
interactions as a causal Directed Acyclic Graph (DAG). We then estimate the
causal impact of each of these RCPs on their associated KPIs using metrics such
as Average Treatment Effect (ATE) and Conditional Average Treatment Effect
(CATE). This approach offers network operators guided insights into identifying
conflicts and quantifying their impacts, enabling more informed and effective
conflict resolution strategies across diverse xApp deployments.

</details>


### [11] [Automated Network Protocol Testing with LLM Agents](https://arxiv.org/abs/2510.13248)
*Yunze Wei,Kaiwen Wei,Shibo Du,Jianyu Wang,Zhangzhong Liu,Yawen Wang,Zhanyou Li,Congcong Miao,Xiaohui Xie,Yong Cui*

Main category: cs.NI

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Network protocol testing is fundamental for modern network infrastructure.
However, traditional network protocol testing methods are labor-intensive and
error-prone, requiring manual interpretation of specifications, test case
design, and translation into executable artifacts, typically demanding one
person-day of effort per test case. Existing model-based approaches provide
partial automation but still involve substantial manual modeling and expert
intervention, leading to high costs and limited adaptability to diverse and
evolving protocols. In this paper, we propose a first-of-its-kind system called
NeTestLLM that takes advantage of multi-agent Large Language Models (LLMs) for
end-to-end automated network protocol testing. NeTestLLM employs hierarchical
protocol understanding to capture complex specifications, iterative test case
generation to improve coverage, a task-specific workflow for executable
artifact generation, and runtime feedback analysis for debugging and
refinement. NeTestLLM has been deployed in a production environment for several
months, receiving positive feedback from domain experts. In experiments,
NeTestLLM generated 4,632 test cases for OSPF, RIP, and BGP, covering 41
historical FRRouting bugs compared to 11 by current national standards. The
process of generating executable artifacts also improves testing efficiency by
a factor of 8.65x compared to manual methods. NeTestLLM provides the first
practical LLM-powered solution for automated end-to-end testing of
heterogeneous network protocols.

</details>


### [12] [NetMCP: Network-Aware Model Context Protocol Platform for LLM Capability Extension](https://arxiv.org/abs/2510.13467)
*Enhan Li,Hongyang Du,Kaibin Huang*

Main category: cs.NI

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Large Language Models (LLMs) remain static in functionality after training,
and extending their capabilities requires integration with external data,
computation, and services. The Model Context Protocol (MCP) has emerged as a
standard interface for such extensions, but current implementations rely solely
on semantic matching between users' requests and server function descriptions,
which makes current deployments and simulation testbeds fragile under latency
fluctuations or server failures. We address this gap by enhancing MCP tool
routing algorithms with real-time awareness of network and server status. To
provide a controlled test environment for development and evaluation, we
construct a heterogeneous experimental platform, namely Network-aware MCP
(NetMCP), which offers five representative network states and build a benchmark
for latency sequence generation and MCP server datasets. On top of NetMCP
platform, we analyze latency sequences and propose a Semantic-Oriented and
Network-Aware Routing (SONAR) algorithm, which jointly optimizes semantic
similarity and network Quality of Service (QoS) metrics for adaptive tool
routing. Results show that SONAR consistently improves task success rate and
reduces completion time and failure number compared with semantic-only,
LLM-based baselines, demonstrating the value of network-aware design for
production-scale LLM systems. The code for NetMCP is available at
https://github.com/NICE-HKU/NetMCP.

</details>


### [13] [Fair Ordering](https://arxiv.org/abs/2510.13664)
*Muhammad Haseeb,Jinkun Geng,Radhika Mittal,Aurojit Panda,Srinivas Narayana,Anirudh Sivaraman*

Main category: cs.NI

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: A growing class of applications demands \emph{fair ordering/sequencing} of
events which ensures that events generated earlier by one client are processed
before later events from other clients. However, achieving such sequencing is
fundamentally challenging due to the inherent limitations of clock
synchronization. We advocate for an approach that embraces, rather than
eliminates, clock variability. Instead of attempting to remove error from a
timestamp, Tommy, our proposed system, leverages a statistical model to compare
two noisy timestamps probabilistically by learning per-clock offset
distributions. Our preliminary statistical model computes the probability that
one event precedes another w.r.t. the wall-clock time without access to the
wall-clock. This serves as a foundation for a new relation:
\emph{likely-happened-before} denoted by $\xrightarrow{p}$ where $p$ represents
the probability of an event to have happened before another. The
$\xrightarrow{p}$ relation provides a basis for ordering multiple events which
are otherwise considered \emph{concurrent} by the typical
\emph{happened-before} ($\rightarrow$) relation. We highlight various related
challenges including intransitivity of $\xrightarrow{p}$ relation as opposed to
the transitive $\rightarrow$ relation. We also outline several research
directions: online fair sequencing, stochastically fair total ordering,
host-level support for fairness and more.

</details>


### [14] [Optimize Replica Server Placement in a Satellite Network](https://arxiv.org/abs/2510.13689)
*Zhiyuan He,Yi Xu,Cheng Luo,Lili Qiu,Yuqing Yang*

Main category: cs.NI

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Satellite communication offers Internet connectivity to remote locations,
such as villages, deserts, mountains, and at sea. However, transmitting content
over satellite networks is significantly more expensive than traditional
Internet. To address this issue, we propose placing content replica servers
within satellite networks and optimizing replica placement for important
performance metrics, such as latency, transmission, and storage cost. Our
approach can support different types of satellite networks, including Low Earth
Orbit (LEO), Medium Earth Orbit (MEO), Geostationary Orbit (GEO), and their
combinations. An important challenge for supporting content replicas in such
networks is that LEO and MEO satellites are constantly moving. We address this
challenge by explicitly considering their moving trajectories and strategically
optimizing not only client performance, but also the cost of transferring
content from one satellite to another as needed. We demonstrate the
effectiveness of our approach using both simulated traffic traces and a
prototype system.

</details>


### [15] [Investigating Web Content Delivery Performance over Starlink](https://arxiv.org/abs/2510.13710)
*Rohan Bose,Jinwei Zhao,Tanya Shreedhar,Jianping Pan,Nitinder Mohan*

Main category: cs.NI

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Low Earth Orbit (LEO) satellite ISPs promise universal Internet connectivity,
yet their interaction with content delivery remains poorly understood. We
present the first comprehensive measurement study decomposing Starlink's web
content delivery performance decomposed across Point of Presence (PoP), DNS,
and CDN layers. Through two years of measurements combining 225K Cloudflare AIM
tests, M-Lab data, and active probing from 99 RIPE Atlas and controlled
Starlink probes, we collect 6.1M traceroutes and 10.8M DNS queries to quantify
how satellite architecture disrupts terrestrial CDN assumptions. We identify
three distinct performance regimes based on infrastructure density. Regions
with local content-rich PoPs achieve near-terrestrial latencies with the
satellite segment dominating 80-90% of RTT. Infrastructure-sparse regions
suffer cascading penalties: remote PoPs force distant resolver selection, which
triggers CDN mis-localization, pushing latencies beyond 200 ms.
Dense-infrastructure regions show minimal sensitivity to PoP changes.
Leveraging Starlink's infrastructure expansion in early 2025 as a natural
experiment, we demonstrate that relocating PoPs closer to user location reduces
median page-fetch times by 60%. Our findings reveal that infrastructure
proximity, not satellite coverage, influences web performance, requiring
fundamental changes to CDN mapping and DNS resolution for satellite ISPs.

</details>


### [16] [Scalable Pilot Assignment for Distributed Massive MIMO using Channel Estimation Error](https://arxiv.org/abs/2510.13732)
*Mohd Saif Ali Khan,Karthik RM,Samar Agnihotri*

Main category: cs.NI

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Pilot contamination remains a major bottleneck in realizing the full
potential of distributed massive MIMO systems. We propose two dynamic and
scalable pilot assignment strategies designed for practical deployment in such
networks. First, we present a low complexity centralized algorithm that
sequentially assigns pilots to user equipments (UEs) to minimize the global
channel estimation errors across serving access points (APs). This improves the
channel estimation quality and reduces interference among UEs, enhancing the
spectral efficiency. Second, we develop a fully distributed algorithm that uses
a priority-based pilot selection approach. In this algorithm, each selected AP
minimizes estimation error using only local information and offers candidate
pilots to the UEs. Every UE then selects a suitable pilot based on AP priority.
This approach ensures consistency and minimizes interference while
significantly reducing pilot contamination. The method requires no global
coordination, maintains low signaling overhead, and adapts dynamically to the
UE deployment. Numerical simulations demonstrate the superiority of our
proposed schemes in terms of network throughput when compared to other
state-of-the-art benchmark schemes.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [17] [AutoCode: LLMs as Problem Setters for Competitive Programming](https://arxiv.org/abs/2510.12803)
*Shang Zhou,Zihan Zheng,Kaiyuan Liu,Zeyu Shen,Zerui Cheng,Zexing Chen,Hansen He,Jianzhu Yao,Huanzhi Mao,Qiuyang Mang,Tianfu Fu,Beichen Li,Dongruixuan Li,Wenhao Chai,Zhuang Liu,Aleksandra Korolova,Peter Henderson,Natasha Jaques,Pramod Viswanath,Saining Xie,Jingbo Shang*

Main category: cs.SE

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Writing competitive programming problems is exacting. Authors must: set
constraints, input distributions, and edge cases that rule out shortcuts;
target specific algorithms (e.g., max-flow, dynamic programming, data
structures); and calibrate complexity beyond the reach of most competitors. We
argue that this makes for an ideal test of general large language model
capabilities and study whether they can do this reliably. We introduce
AutoCode, which uses multiple rounds of validation to yield competition-grade
problem statements and test cases. On held-out problems, AutoCode test suites
approach 99% consistency with official judgments, a significant improvement
over current state-of-the-art methods like HardTests, which achieve less than
81%. Furthermore, starting with a random seed problem, AutoCode can create
novel variants with reference and brute-force solutions. By cross-verifying
these generated solutions against test cases, we can further filter out
malformed problems. Our system ensures high correctness, as verified by human
experts. AutoCode successfully produces novel problems judged by
Grandmaster-level (top 0.3%) competitive programmers to be of contest quality.

</details>


### [18] [SpareCodeSearch: Searching for Code Context When You Have No Spare GPU](https://arxiv.org/abs/2510.12948)
*Minh Nguyen*

Main category: cs.SE

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Retrieval-Augmented Generation (RAG) frameworks aim to enhance Code Language
Models (CLMs) by including another module for retrieving relevant context to
construct the input prompt. However, these retrieval modules commonly use
semantic search, requiring substantial computational resources for training and
hosting these embedded models, making them infeasible to integrate into
lightweight applications such as in-IDE AI-based code completion. In this
solution paper, we prove that using keyword-search is sufficient to retrieve
relevant and useful code context inside large codebases, without the need for
extensive GPU resources. The usefulness of code contexts found by our solution
is demonstrated through their completion results on the Code Context
Competition's benchmark, reaching 0.748 and 0.725 chRF scores on Kotlin and
Python tracks, respectively.

</details>


### [19] [ADPerf: Investigating and Testing Performance in Autonomous Driving Systems](https://arxiv.org/abs/2510.13078)
*Tri Minh-Triet Pham,Diego Elias Costa,Weiyi Shang,Jinqiu Yang*

Main category: cs.SE

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Obstacle detection is crucial to the operation of autonomous driving systems,
which rely on multiple sensors, such as cameras and LiDARs, combined with code
logic and deep learning models to detect obstacles for time-sensitive
decisions. Consequently, obstacle detection latency is critical to the safety
and effectiveness of autonomous driving systems. However, the latency of the
obstacle detection module and its resilience to various changes in the LiDAR
point cloud data are not yet fully understood. In this work, we present the
first comprehensive investigation on measuring and modeling the performance of
the obstacle detection modules in two industry-grade autonomous driving
systems, i.e., Apollo and Autoware. Learning from this investigation, we
introduce ADPerf, a tool that aims to generate realistic point cloud data test
cases that can expose increased detection latency. Increasing latency decreases
the availability of the detected obstacles and stresses the capabilities of
subsequent modules in autonomous driving systems, i.e., the modules may be
negatively impacted by the increased latency in obstacle detection.
  We applied ADPerf to stress-test the performance of widely used 3D obstacle
detection modules in autonomous driving systems, as well as the propagation of
such tests on trajectory prediction modules. Our evaluation highlights the need
to conduct performance testing of obstacle detection components, especially 3D
obstacle detection, as they can be a major bottleneck to increased latency of
the autonomous driving system. Such an adverse outcome will also further
propagate to other modules, reducing the overall reliability of autonomous
driving systems.

</details>


### [20] [TRUSTVIS: A Multi-Dimensional Trustworthiness Evaluation Framework for Large Language Models](https://arxiv.org/abs/2510.13106)
*Ruoyu Sun,Da Song,Jiayang Song,Yuheng Huang,Lei Ma*

Main category: cs.SE

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: As Large Language Models (LLMs) continue to revolutionize Natural Language
Processing (NLP) applications, critical concerns about their trustworthiness
persist, particularly in safety and robustness. To address these challenges, we
introduce TRUSTVIS, an automated evaluation framework that provides a
comprehensive assessment of LLM trustworthiness. A key feature of our framework
is its interactive user interface, designed to offer intuitive visualizations
of trustworthiness metrics. By integrating well-known perturbation methods like
AutoDAN and employing majority voting across various evaluation methods,
TRUSTVIS not only provides reliable results but also makes complex evaluation
processes accessible to users. Preliminary case studies on models like
Vicuna-7b, Llama2-7b, and GPT-3.5 demonstrate the effectiveness of our
framework in identifying safety and robustness vulnerabilities, while the
interactive interface allows users to explore results in detail, empowering
targeted model improvements. Video Link: https://youtu.be/k1TrBqNVg8g

</details>


### [21] [Isolating Compiler Bugs through Compilation Steps Analysis](https://arxiv.org/abs/2510.13128)
*Yujie Liu,Mingxuan Zhu,Shengyu Cheng,Dan Hao*

Main category: cs.SE

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Compilers are essential to software systems, and their bugs can propagate to
dependent software. Ensuring compiler correctness is critical. However,
isolating compiler bugs remains challenging due to the internal complexity of
compiler execution. Existing techniques primarily mutate compilation inputs to
generate passing and failing tests, but often lack causal analysis of internal
steps, limiting their effectiveness.
  To address this limitation, we propose CompSCAN, a novel compiler bug
isolation technique that applies analysis over the sequence of compilation
steps. CompSCAN follows a three-stage process: (1) extracting the array of
compilation steps that leads to the original failure, (2) identifying
bug-causing steps and collecting corresponding compiler code elements, and (3)
calculating suspicious scores for each code element and outputting a suspicious
ranking list as the bug isolation result.
  We evaluate CompSCAN on 185 real-world LLVM and GCC bugs. Results show that
CompSCAN outperforms state-of-the-art techniques in both effectiveness and
efficiency. CompSCAN successfully isolates 50, 85, 100, and 123 bugs within the
Top-1/3/5/10 ranks, respectively. Compared with ETEM and ODFL, two
state-of-the-art compiler bug isolation techniques, CompSCAN achieves relative
improvements of 44.51% / 50.18% / 36.24% / 24.49% over ETEM, and 31.58% /
49.12% / 44.93% / 21.78% over ODFL on those metrics. Moreover, CompSCAN runs
faster on average per bug than both baselines.

</details>


### [22] [GRACE: Globally-Seeded Representation-Aware Cluster-Specific Evolution for Compiler Auto-Tuning](https://arxiv.org/abs/2510.13176)
*Haolin Pan,Chao Zha,Jinyuan Dong,Mingjie Xing,Yanjun Wu*

Main category: cs.SE

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Compiler pass selection and phase ordering present a significant challenge in
achieving optimal program performance, particularly for objectives like code
size reduction. Standard compiler heuristics offer general applicability but
often yield suboptimal, program-specific results due to their one-size-fits-all
nature. While iterative compilation can find tailored solutions, its
prohibitive search cost limits practical use. Machine learning approaches
promise faster inference but frequently struggle with generalization to unseen
programs. This paper introduces GRACE, a novel framework for compiler
auto-tuning, demonstrated for LLVM IR instruction count optimization. GRACE
effectively curtails the search space by leveraging pass synergies and a
weighted scoring method to generate initial high-quality candidate sequences
and a pass pool. It then employs contrastive learning, using pass
sequence-based data augmentation, to create program embeddings that facilitate
similarity-aware clustering. Evolutionary search within these clusters yields a
coreset of $k$ specialized pass sequences designed for robust generalization to
unseen programs. At test time, GRACE efficiently selects the best coreset
sequence and refines it using lightweight techniques. Experimental results on
seven diverse datasets show that GRACE reduces LLVM IR instruction count by an
average of 10.09% on LLVM 10.0.0 and 10.19% on LLVM 18.1.6 compared to opt -Oz,
while incurring an average tuning time of less than 1s per program,
demonstrating its state-of-the-art performance and practical effectiveness.

</details>


### [23] [Synergy-Guided Compiler Auto-Tuning of Nested LLVM Pass Pipelines](https://arxiv.org/abs/2510.13184)
*Haolin Pan,Jinyuan Dong,Mingjie Xing,Yanjun Wu*

Main category: cs.SE

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Compiler optimization relies on sequences of passes to improve program
performance. Selecting and ordering these passes automatically, known as
compiler auto-tuning, is challenging due to the large and complex search space.
Existing approaches generally assume a linear sequence of passes, a model
compatible with legacy compilers but fundamentally misaligned with the
hierarchical design of the LLVM New Pass Manager. This misalignment prevents
them from guaranteeing the generation of syntactically valid optimization
pipelines. In this work, we present a new auto-tuning framework built from the
ground up for the New Pass Manager. We introduce a formal grammar to define the
space of valid nested pipelines and a forest-based data structure for their
native representation. Upon this foundation, we develop a structure-aware
Genetic Algorithm whose operators manipulate these forests directly, ensuring
that all candidate solutions are valid by construction. The framework first
mines synergistic pass relationships to guide the search. An optional
refinement stage further explores subtle performance variations arising from
different valid structural arrangements.
  We evaluate our approach on seven benchmark datasets using LLVM 18.1.6. The
discovered pipelines achieve an average of 13.62% additional instruction count
reduction compared to the standard opt -Oz optimization level, showing that our
framework is capable of navigating this complex, constrained search space to
identify valid and effective pass pipelines.

</details>


### [24] [Towards Richer Challenge Problems for Scientific Computing Correctness](https://arxiv.org/abs/2510.13423)
*Matthew Sottile,Mohit Tekriwal,John Sarracino*

Main category: cs.SE

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Correctness in scientific computing (SC) is gaining increasing attention in
the formal methods (FM) and programming languages (PL) community. Existing
PL/FM verification techniques struggle with the complexities of realistic SC
applications. Part of the problem is a lack of a common understanding between
the SC and PL/FM communities of machine-verifiable correctness challenges and
dimensions of correctness in SC applications.
  To address this gap, we call for specialized challenge problems to inform the
development and evaluation of FM/PL verification techniques for correctness in
SC. These specialized challenges are intended to augment existing problems
studied by FM/PL researchers for general programs to ensure the needs of SC
applications can be met. We propose several dimensions of correctness relevant
to scientific computing, and discuss some guidelines and criteria for designing
challenge problems to evaluate correctness in scientific computing.

</details>


### [25] [Verifying a Sparse Matrix Algorithm Using Symbolic Execution](https://arxiv.org/abs/2510.13424)
*Alexander C. Wilton*

Main category: cs.SE

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Scientific software is, by its very nature, complex. It is mathematical and
highly optimized which makes it prone to subtle bugs not as easily detected by
traditional testing. We outline how symbolic execution can be used to write
tests similar to traditional unit tests while providing stronger verification
guarantees and apply this methodology to a sparse matrix algorithm.

</details>


### [26] [OpenDerisk: An Industrial Framework for AI-Driven SRE, with Design, Implementation, and Case Studies](https://arxiv.org/abs/2510.13561)
*Peng Di,Faqiang Chen,Xiao Bai,Hongjun Yang,Qingfeng Li,Ganglin Wei,Jian Mou,Feng Shi,Keting Chen,Peng Tang,Zhitao Shen,Zheng Li,Wenhui Shi,Junwei Guo,Hang Yu*

Main category: cs.SE

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: The escalating complexity of modern software imposes an unsustainable
operational burden on Site Reliability Engineering (SRE) teams, demanding
AI-driven automation that can emulate expert diagnostic reasoning. Existing
solutions, from traditional AI methods to general-purpose multi-agent systems,
fall short: they either lack deep causal reasoning or are not tailored for the
specialized, investigative workflows unique to SRE. To address this gap, we
present OpenDerisk, a specialized, open-source multi-agent framework
architected for SRE. OpenDerisk integrates a diagnostic-native collaboration
model, a pluggable reasoning engine, a knowledge engine, and a standardized
protocol (MCP) to enable specialist agents to collectively solve complex,
multi-domain problems. Our comprehensive evaluation demonstrates that
OpenDerisk significantly outperforms state-of-the-art baselines in both
accuracy and efficiency. This effectiveness is validated by its large-scale
production deployment at Ant Group, where it serves over 3,000 daily users
across diverse scenarios, confirming its industrial-grade scalability and
practical impact. OpenDerisk is open source and available at
https://github.com/derisk-ai/OpenDerisk/

</details>


### [27] [Auto-repair without test cases: How LLMs fix compilation errors in large industrial embedded code](https://arxiv.org/abs/2510.13575)
*Han Fu,Sigrid Eldh,Kristian Wiklund,Andreas Ermedahl,Philipp Haller,Cyrille Artho*

Main category: cs.SE

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: The co-development of hardware and software in industrial embedded systems
frequently leads to compilation errors during continuous integration (CI).
Automated repair of such failures is promising, but existing techniques rely on
test cases, which are not available for non-compilable code.
  We employ an automated repair approach for compilation errors driven by large
language models (LLMs). Our study encompasses the collection of more than 40000
commits from the product's source code. We assess the performance of an
industrial CI system enhanced by four state-of-the-art LLMs, comparing their
outcomes with manual corrections provided by human programmers. LLM-equipped CI
systems can resolve up to 63 % of the compilation errors in our baseline
dataset. Among the fixes associated with successful CI builds, 83 % are deemed
reasonable. Moreover, LLMs significantly reduce debugging time, with the
majority of successful cases completed within 8 minutes, compared to hours
typically required for manual debugging.

</details>


### [28] [Property Testing for Ocean Models. Can We Specify It? (Invited Talk)](https://arxiv.org/abs/2510.13692)
*Deepak A. Cherian*

Main category: cs.SE

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: I take inspiration from the property-testing literature, particularly the
work of Prof. John Hughes, and explore how such ideas might be applied to
numerical models of the ocean. Specifically, I ask whether geophysical fluid
dynamics (GFD) theory, expressed as property tests, might be used to address
the oracle problem of testing the correctness of ocean models. I propose that a
number of simple idealized GFD problems can be framed as property tests. These
examples clearly illustrate how physics naturally lends itself to specifying
property tests. Which of these proposed tests might be most feasible and
useful, remains to be seen.

</details>


### [29] [On Pretraining for Project-Level Code Completion](https://arxiv.org/abs/2510.13697)
*Maksim Sapronov,Evgeniy Glukhov*

Main category: cs.SE

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Repository-level pretraining is commonly used to enable large language models
for code to leverage codebase-wide context. This enhances their ability to
generate accurate and context-aware code completions. In this work, we
investigate how different repository-processing strategies affect in-context
learning in OpenCoder, a 1.5B-parameter model. We extend its context window
from 4,096 to 16,384 tokens by training on additional 1B tokens of curated
repository-level data. Despite relying on a smaller dataset than competing
models (which often use hundreds of billions of tokens), our model achieves
comparable performance on the Long Code Arena benchmark. We find that various
repository-processing techniques yield similarly strong results, with the
primary gain coming from adapting to a new rotary positional embedding (RoPE)
scaling parameter. Finally, we show that a simpler file-level training approach
at the original sequence length remains highly effective, opening up
repository-level code completion research to settings with more constrained
data and compute resources.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [30] [D-com: Accelerating Iterative Processing to Enable Low-rank Decomposition of Activations](https://arxiv.org/abs/2510.13147)
*Faraz Tahmasebi,Michael Pelluer,Hyoukjun Kwon*

Main category: cs.AR

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: The computation and memory costs of large language models kept increasing
over last decade, which reached over the scale of 1T parameters. To address the
challenges from the large scale models, model compression techniques such as
low-rank decomposition have been explored. Previous model decomposition works
have focused on weight decomposition to avoid costly runtime decomposition,
whose latency often significantly exceeds the benefits from decomposition
(e.g., 38% more end-to-end latency when running Llama2-7b on A100 with 4K
sequence length with activation decomposition compared to no decomposition). In
this work, we debunk such observations and report that the input decomposition
can be significantly beneficial with a proper choice of decomposition algorithm
and hardware support. We adopt progressive decomposition algorithm, Lanczos
algorithm, and design a co-accelerator architecture for the decomposition
algorithm. To address the memory- boundness of the decomposition operation, we
introduce a novel compute replication methodology that moves the op- eration
toward compute-bound region, which enables 6.2x speedup in our evaluation. We
also develop an output shape- preserving computation scheme that eliminates
decomposi- tion costs in consecutive layers. To compensate model quality loss
from compression, we introduce a multi-track decom- position approach that
separately handles outlier channels for high accuracy and low perplexity with
minimal compu- tational costs. Combined together, our accelerator, D-com,
provides 22% end-to-end latency improvements compared to A100 GPU at the cost
of small model quality degradation (e.g., 3% on AI2 Reasoning Challenge task).

</details>


### [31] [Energy-Efficient FPGA Framework for Non-Quantized Convolutional Neural Networks](https://arxiv.org/abs/2510.13362)
*Angelos Athanasiadis,Nikolaos Tampouratzis,Ioannis Papaefstathiou*

Main category: cs.AR

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: The growing demand for real-time processing in artificial intelligence
applications, particularly those involving Convolutional Neural Networks
(CNNs), has highlighted the need for efficient computational solutions.
Conventional processors, very often, fall short in balancing performance, power
consumption, and latency, especially in embedded systems and edge computing
platforms. Field-Programmable Gate Arrays (FPGAs) offer a promising
alternative, combining high performance with energy efficiency and
reconfigurability. The presented framework addresses the complex and demanding
computations of CNNs on FPGAs maintaining full precision in all neural network
parameters. Specifically, our framework is based on Darknet which is very
widely used for the design of CNNs and allows the designer, by using a similar
input to that given to Darknet, to efficiently implement a CNN in a
heterogeneous system comprising of CPUs and FPGAs. When compared with the FPGA
frameworks that support quantization, our solution aims to offer similar
performance and/or energy efficiency without any degradation on the NN
accuracy.

</details>


### [32] [F-BFQ: Flexible Block Floating-Point Quantization Accelerator for LLMs](https://arxiv.org/abs/2510.13401)
*Jude Haris,Jos√© Cano*

Main category: cs.AR

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Large Language Models (LLMs) have become increasingly prominent for daily
tasks, from improving sound-totext translation to generating additional frames
for the latest video games. With the help of LLM inference frameworks, such as
llama.cpp, which support optimizations such as KV-caching and quantization, it
is now easier than ever to deploy LLMs on edge devices. Quantization is
fundamental to enable LLMs on resource-constrained edge devices, and llama.cpp
utilizes block floating point (BFP) quantization to drastically reduce the bit
width of weights and input tensors, the memory footprint, and the computational
power required to run LLMs. LLMs are typically quantized with mixed BFP
quantization across the model layers to reduce the loss of model accuracy due
to quantization. Therefore, to efficiently accelerate across the layers of
BFP-quantized LLMs, specialized accelerators need to support different BFP
variants without reconfiguration. To address this issue, we propose a Flexible
Block FloatingPoint Quantization (F-BFQ) accelerator, which can dynamically
switch between two BFP quantization variants and perform matrix multiplication
(MatMul) operations. Our initial F-BFQ accelerator design, deployed on the AMD
Kria board, reduces inference time by 1.4x on average over the Arm NEON-based
CPU execution across three BFP quantized LLMs while achieving 5.2 tokens per
second (~3.9 words per second).

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [33] [Semantic knowledge guides innovation and drives cultural evolution](https://arxiv.org/abs/2510.12837)
*Anil Yaman,Shen Tian,Bj√∂rn Lindstr√∂m*

Main category: cs.MA

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Cumulative cultural evolution enables human societies to generate
increasingly complex knowledge and technology over generations. While social
learning transmits innovations between individuals and generations, the
cognitive processes that generate these innovations remain poorly understood.
Here, we demonstrate that semantic knowledge-structured associations between
concepts and their functions-provides cognitive scaffolding for cumulative
innovation by guiding exploration toward plausible and meaningful actions. We
tested this hypothesis using a cultural evolutionary agent-based model and a
large-scale behavioural experiment (N = 1,243), in which individuals performed
a task requiring the combination of items into novel innovations. Across both
approaches, semantic knowledge and social learning interact synergistically to
enhance innovation. Behaviorally, participants without access to semantic
knowledge performed no better than chance, even when social learning was
available, and relied on shallow exploration strategies. These findings suggest
that semantic knowledge is a key cognitive process enabling human cumulative
culture.

</details>


### [34] [KVCOMM: Online Cross-context KV-cache Communication for Efficient LLM-based Multi-agent Systems](https://arxiv.org/abs/2510.12872)
*Hancheng Ye,Zhengqi Gao,Mingyuan Ma,Qinsi Wang,Yuzhe Fu,Ming-Yu Chung,Yueqian Lin,Zhijian Liu,Jianyi Zhang,Danyang Zhuo,Yiran Chen*

Main category: cs.MA

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Multi-agent large language model (LLM) systems are increasingly adopted for
complex language processing tasks that require communication and coordination
among agents. However, these systems often suffer substantial overhead from
repeated reprocessing of overlapping contexts across agents. In typical
pipelines, once an agent receives a message from its predecessor, the full
context-including prior turns-must be reprocessed from scratch, leading to
inefficient processing. While key-value (KV) caching is an effective solution
for avoiding redundant computation in single-agent settings where prefixes
remain unchanged, it cannot be directly reused in multi-agent scenarios due to
diverging prefixes introduced by agent-specific context extensions. We identify
that the core challenge lies in the offset variance of KV-caches across agents.
To address this, we propose KVCOMM, a training-free framework that enables
efficient prefilling in multi-agent inference by reusing KV-caches and aligning
cache offsets of overlapping contexts under diverse prefix contexts. KVCOMM
estimates and adjusts KV-caches for shared content by referencing a pool of
cached examples-termed anchors-that store observed cache deviations under
varying prefixes. The anchor pool is maintained and updated online, allowing
dynamic adaptation to distinct user requests and context structures. KVCOMM
achieves over 70% reuse rate across diverse multi-agent workloads, including
retrieval-augmented generation, math reasoning, and collaborative coding tasks,
all without quality degradation. Particularly, when each fully-connected agent
receives 1K input tokens with 512 prefix tokens and 512 output tokens under a
five-agent setting, KVCOMM achieves up to 7.8x speedup compared to the standard
prefill pipeline, reducing TTFT from ~430 ms to ~55 ms.

</details>


### [35] [Agentic Discovery: Closing the Loop with Cooperative Agents](https://arxiv.org/abs/2510.13081)
*J. Gregory Pauloski,Kyle Chard,Ian T. Foster*

Main category: cs.MA

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: As data-driven methods, artificial intelligence (AI), and automated workflows
accelerate scientific tasks, we see the rate of discovery increasingly limited
by human decision-making tasks such as setting objectives, generating
hypotheses, and designing experiments. We postulate that cooperative agents are
needed to augment the role of humans and enable autonomous discovery. Realizing
such agents will require progress in both AI and infrastructure.

</details>


### [36] [Altruistic Ride Sharing: A Community-Driven Approach to Short-Distance Mobility](https://arxiv.org/abs/2510.13227)
*Divyanshu Singh,Ashman Mehra,Snehanshu Saha,Santonu Sarkar*

Main category: cs.MA

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Urban mobility faces persistent challenges of congestion and fuel
consumption, specifically when people choose a private, point-to-point commute
option. Profit-driven ride-sharing platforms prioritize revenue over fairness
and sustainability. This paper introduces Altruistic Ride-Sharing (ARS), a
decentralized, peer-to-peer mobility framework where participants alternate
between driver and rider roles based on altruism points rather than monetary
incentives. The system integrates multi-agent reinforcement learning (MADDPG)
for dynamic ride-matching, game-theoretic equilibrium guarantees for fairness,
and a population model to sustain long-term balance. Using real-world New York
City taxi data, we demonstrate that ARS reduces travel distance and emissions,
increases vehicle utilization, and promotes equitable participation compared to
both no-sharing and optimization-based baselines. These results establish ARS
as a scalable, community-driven alternative to conventional ride-sharing,
aligning individual behavior with collective urban sustainability goals.

</details>


### [37] [AOAD-MAT: Transformer-based multi-agent deep reinforcement learning model considering agents' order of action decisions](https://arxiv.org/abs/2510.13343)
*Shota Takayama,Katsuhide Fujita*

Main category: cs.MA

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Multi-agent reinforcement learning focuses on training the behaviors of
multiple learning agents that coexist in a shared environment. Recently, MARL
models, such as the Multi-Agent Transformer (MAT) and ACtion dEpendent deep
Q-learning (ACE), have significantly improved performance by leveraging
sequential decision-making processes. Although these models can enhance
performance, they do not explicitly consider the importance of the order in
which agents make decisions. In this paper, we propose an Agent Order of Action
Decisions-MAT (AOAD-MAT), a novel MAT model that considers the order in which
agents make decisions. The proposed model explicitly incorporates the sequence
of action decisions into the learning process, allowing the model to learn and
predict the optimal order of agent actions. The AOAD-MAT model leverages a
Transformer-based actor-critic architecture that dynamically adjusts the
sequence of agent actions. To achieve this, we introduce a novel MARL
architecture that cooperates with a subtask focused on predicting the next
agent to act, integrated into a Proximal Policy Optimization based loss
function to synergistically maximize the advantage of the sequential
decision-making. The proposed method was validated through extensive
experiments on the StarCraft Multi-Agent Challenge and Multi-Agent MuJoCo
benchmarks. The experimental results show that the proposed AOAD-MAT model
outperforms existing MAT and other baseline models, demonstrating the
effectiveness of adjusting the AOAD order in MARL.

</details>
