<div id=toc></div>

# Table of Contents

- [cs.NI](#cs.NI) [Total: 1]
- [cs.MA](#cs.MA) [Total: 1]
- [cs.DC](#cs.DC) [Total: 5]
- [cs.SE](#cs.SE) [Total: 9]
- [cs.AR](#cs.AR) [Total: 2]


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [1] [Fair Ordering](https://arxiv.org/abs/2510.13664)
*Muhammad Haseeb,Jinkun Geng,Radhika Mittal,Aurojit Panda,Srinivas Narayana,Anirudh Sivaraman*

Main category: cs.NI

TL;DR: Tommy提出了一种基于统计模型的事件排序方法，通过学习各时钟的偏移分布，以概率方式比较带噪声的时间戳，从而定义“很可能先发生”（likely-happened-before）关系，用于实现更公平的事件排序。


<details>
  <summary>Details</summary>
Motivation: 传统事件排序依赖精确时钟同步，但时钟同步存在固有局限；现有“happened-before”关系无法区分并发事件，难以满足公平排序需求。

Method: Tommy利用统计模型学习每个客户端时钟的偏移分布，在不依赖真实墙钟时间的情况下，计算两个带噪声时间戳之间事件先后的概率，构建概率化的“很可能先发生”（$\xrightarrow{p}$）关系。

Result: 初步模型能有效估计事件在真实时间中的先后概率，为并发事件提供更细粒度的排序依据，并揭示了$\xrightarrow{p}$关系的非传递性等新挑战。

Conclusion: 该方法为公平事件排序提供了新思路，未来可探索在线公平排序、随机公平全序、主机级公平支持等方向。

Abstract: A growing class of applications demands \emph{fair ordering/sequencing} of
events which ensures that events generated earlier by one client are processed
before later events from other clients. However, achieving such sequencing is
fundamentally challenging due to the inherent limitations of clock
synchronization. We advocate for an approach that embraces, rather than
eliminates, clock variability. Instead of attempting to remove error from a
timestamp, Tommy, our proposed system, leverages a statistical model to compare
two noisy timestamps probabilistically by learning per-clock offset
distributions. Our preliminary statistical model computes the probability that
one event precedes another w.r.t. the wall-clock time without access to the
wall-clock. This serves as a foundation for a new relation:
\emph{likely-happened-before} denoted by $\xrightarrow{p}$ where $p$ represents
the probability of an event to have happened before another. The
$\xrightarrow{p}$ relation provides a basis for ordering multiple events which
are otherwise considered \emph{concurrent} by the typical
\emph{happened-before} ($\rightarrow$) relation. We highlight various related
challenges including intransitivity of $\xrightarrow{p}$ relation as opposed to
the transitive $\rightarrow$ relation. We also outline several research
directions: online fair sequencing, stochastically fair total ordering,
host-level support for fairness and more.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [2] [AOAD-MAT: Transformer-based multi-agent deep reinforcement learning model considering agents' order of action decisions](https://arxiv.org/abs/2510.13343)
*Shota Takayama,Katsuhide Fujita*

Main category: cs.MA

TL;DR: 本文提出了一种名为AOAD-MAT的新模型，通过在多智能体强化学习中显式建模智能体动作决策的顺序，利用基于Transformer的Actor-Critic架构和集成的子任务预测机制，在StarCraft和MuJoCo基准上取得了优于现有方法的性能。


<details>
  <summary>Details</summary>
Motivation: 现有MARL模型（如MAT和ACE）虽利用了序列决策过程，但未显式考虑智能体动作顺序对性能的影响。作者旨在通过建模最优动作顺序来进一步提升多智能体系统的协作效率与性能。

Method: 提出AOAD-MAT模型，结合Transformer架构的Actor-Critic框架，引入一个子任务用于预测下一个应执行动作的智能体，并将其整合到基于PPO的损失函数中，以动态调整智能体动作顺序。

Result: 在StarCraft Multi-Agent Challenge和Multi-Agent MuJoCo基准上的实验表明，AOAD-MAT优于原始MAT及其他基线模型。

Conclusion: 显式建模智能体动作顺序能有效提升多智能体强化学习性能，AOAD-MAT通过动态调整动作序列实现了更优的协作策略。

Abstract: Multi-agent reinforcement learning focuses on training the behaviors of
multiple learning agents that coexist in a shared environment. Recently, MARL
models, such as the Multi-Agent Transformer (MAT) and ACtion dEpendent deep
Q-learning (ACE), have significantly improved performance by leveraging
sequential decision-making processes. Although these models can enhance
performance, they do not explicitly consider the importance of the order in
which agents make decisions. In this paper, we propose an Agent Order of Action
Decisions-MAT (AOAD-MAT), a novel MAT model that considers the order in which
agents make decisions. The proposed model explicitly incorporates the sequence
of action decisions into the learning process, allowing the model to learn and
predict the optimal order of agent actions. The AOAD-MAT model leverages a
Transformer-based actor-critic architecture that dynamically adjusts the
sequence of agent actions. To achieve this, we introduce a novel MARL
architecture that cooperates with a subtask focused on predicting the next
agent to act, integrated into a Proximal Policy Optimization based loss
function to synergistically maximize the advantage of the sequential
decision-making. The proposed method was validated through extensive
experiments on the StarCraft Multi-Agent Challenge and Multi-Agent MuJoCo
benchmarks. The experimental results show that the proposed AOAD-MAT model
outperforms existing MAT and other baseline models, demonstrating the
effectiveness of adjusting the AOAD order in MARL.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [3] [Scrutiny new framework in integrated distributed reliable systems](https://arxiv.org/abs/2510.13203)
*Mehdi Zekriyapanah Gashti*

Main category: cs.DC

TL;DR: 本文提出了一种名为FDIRS的新框架，用于提升集成分布式系统的性能、效率和可靠性，通过异构分布式数据库技术优化用户响应速度，并在仿真中优于现有框架。


<details>
  <summary>Details</summary>
Motivation: 现有集成系统框架（如ERPSD和ERPDRT）存在性能、效率和可靠性方面的不足，亟需一种更优的解决方案来提升用户满意度和系统表现。

Method: 作者分析了现有集成系统及其演进过程，提出了包含三个组成部分的新FDIRS框架，并采用异构分布式数据库技术以增强系统性能。

Result: 仿真实验结果表明，FDIRS框架在性能、效率和可靠性方面优于现有框架，并有效解决了先前框架中的一些问题。

Conclusion: FDIRS框架通过引入异构分布式数据库技术，显著提升了集成分布式系统的整体表现，为未来系统设计提供了有效参考。

Abstract: In this paper we represent a new framework for integrated distributed
systems. In the proposed framework we have used three parts to increase
Satisfaction and Performance of this framework. At first we analyse integrated
systems and their evolution process and also ERPSD and ERPDRT framework briefly
then we explain the new FDIRS framework. Finally we compare the results of
simulation of the new framework with presented frameworks. Result showed In
FIDRS framework, the technique of heterogeneous distributed data base is used
to improve Performance and speed in responding to users. Finally by using FDIRS
framework we succeeded to increase Efficiency, Performance and reliability of
integrated systems and remove some of previous frameworks problems.

</details>


### [4] [BanaServe: Unified KV Cache and Dynamic Module Migration for Balancing Disaggregated LLM Serving in AI Infrastructure](https://arxiv.org/abs/2510.13223)
*Yiyuan He,Minxian Xu,Jingfeng Wu,Jianmin Hu,Chong Ma,Min Shen,Le Chen,Chengzhong Xu,Lin Qu,Kejiang Ye*

Main category: cs.DC

TL;DR: BanaServe 是一种面向解耦式大语言模型（LLM）服务的动态编排框架，通过层级别权重迁移、注意力级别 KV 缓存迁移和全局 KV 缓存共享，实现计算与内存资源的动态再平衡，显著提升吞吐量并降低延迟。


<details>
  <summary>Details</summary>
Motivation: 现有解耦式 LLM 服务系统存在三大问题：静态资源分配无法适应动态负载、prefill 与 decode 阶段负载不均衡、以及缓存感知路由导致热点和负载失衡。

Method: BanaServe 引入层级别权重迁移、注意力级别 KV 缓存迁移和全局 KV 缓存存储，并结合层间重叠传输机制，实现粗粒度与细粒度的负载再分配，使路由调度仅依赖负载状态而不受缓存位置限制。

Result: 相比 vLLM，BanaServe 吞吐量提升 1.2–3.9 倍，总处理时间减少 3.9%–78.4%；相比 DistServe，吞吐量提升 1.1–2.8 倍，延迟降低 1.4%–70.1%。

Conclusion: BanaServe 有效解决了现有解耦式 LLM 服务系统中的资源分配僵化、负载不均与缓存热点问题，显著提升了系统效率与性能。

Abstract: Large language models (LLMs) are increasingly deployed in AI infrastructure,
driving the need for high throughput, resource efficient serving systems.
Disaggregated LLM serving, which separates prompt prefill from auto-regressive
decode, has emerged as a promising architecture by isolating their
heterogeneous compute and memory demands. However, current disaggregated
systems face three key limitations: (i) static resource allocation cannot adapt
to highly dynamic workloads, causing over-provisioning that wastes resources or
under-provisioning that violates service level objectives (SLOs); (ii) inherent
load imbalance between prefill and decode stages, where prefill is
compute-bound and decode is memory-bound, causes under-utilization in one tier
while the other becomes a bottleneck; and (iii) prefix cache aware routing
skews load distribution, as high cache hit rate prefill nodes attract
disproportionately more requests, further degrading balance and efficiency. To
address these issues, we present BanaServe, a dynamic orchestration framework
that continuously rebalances computational and memory resources across prefill
and decode instances while eliminating hotspots induced by cache. BanaServe
introduces layer level weight migration, attention level Key Value Cache (KV
Cache) migration, and Global KV Cache Store sharing with layer wise overlapped
transmission, enabling both coarse grained (layer level) and fine grained
(attention level) load redistribution with minimal latency overhead. These
mechanisms allow routers to perform purely load aware scheduling, unconstrained
by cache placement. Compared to vLLM, BanaServe achieves 1.2x-3.9x higher
throughput with 3.9%-78.4% lower total processing time, and outperforms
DistServe by 1.1x-2.8x in throughput with 1.4%-70.1% latency reduction.

</details>


### [5] [Distributed Reductions for the Maximum Weight Independent Set Problem](https://arxiv.org/abs/2510.13306)
*Jannick Borowitz,Ernestine Großmann,Mattthias Schimek*

Main category: cs.DC

TL;DR: 本文提出了首个面向最大权独立集问题的分布式内存并行约简算法，并结合贪心和剥离策略设计了两种启发式方法，在保持良好约简效果的同时显著提升了大规模图的求解速度和可扩展性。


<details>
  <summary>Details</summary>
Motivation: 最大权独立集问题是NP难优化问题，现有实用算法依赖数据约简规则，但受限于串行方法的规模处理能力，难以应对超大规模图。

Method: 提出首个分布式内存并行约简算法，并结合贪心（reduce-and-greedy）与剥离（reduce-and-peel）策略，构建分布式启发式求解框架。

Result: 实验表明，所提方法在多达1024个处理器上具有良好可扩展性；异步reduce-and-peel方法在36个真实图上平均加速33倍，解质量接近串行算法；reduce-and-greedy方法平均加速达50倍，但解质量略低；可处理超十亿顶点和170亿边的图。

Conclusion: 该工作成功将最大权独立集问题的约简技术扩展至分布式内存环境，显著提升了大规模图的处理能力与求解效率，为后续研究提供了新方向。

Abstract: Finding maximum-weight independent sets in graphs is an important NP-hard
optimization problem. Given a vertex-weighted graph $G$, the task is to find a
subset of pairwise non-adjacent vertices of $G$ with maximum weight. Most
recently published practical exact algorithms and heuristics for this problem
use a variety of data-reduction rules to compute (near-)optimal solutions.
Applying these rules results in an equivalent instance of reduced size. An
optimal solution to the reduced instance can be easily used to construct an
optimal solution for the original input.
  In this work, we present the first distributed-memory parallel reduction
algorithms for this problem, targeting graphs beyond the scale of previous
sequential approaches. Furthermore, we propose the first distributed
reduce-and-greedy and reduce-and-peel algorithms for finding a maximum weight
independent set heuristically.
  In our practical evaluation, our experiments on up to $1024$ processors
demonstrate good scalability of our distributed reduce algorithms while
maintaining good reduction impact. Our asynchronous reduce-and-peel approach
achieves an average speedup of $33\times$ over a sequential state-of-the-art
reduce-and-peel approach on 36 real-world graphs with a solution quality close
to the sequential algorithm. Our reduce-and-greedy algorithms even achieve
average speedups of up to $50\times$ at the cost of a lower solution quality.
Moreover, our distributed approach allows us to consider graphs with more than
one billion vertices and 17 billion edges.

</details>


### [6] [FIRST: Federated Inference Resource Scheduling Toolkit for Scientific AI Model Access](https://arxiv.org/abs/2510.13724)
*Aditya Tanikanti,Benoit Côté,Yanfei Guo,Le Chen,Nickolaus Saint,Ryan Chard,Ken Raffenetti,Rajeev Thakur,Thomas Uram,Ian Foster,Michael E. Papka,Venkatram Vishwanath*

Main category: cs.DC

TL;DR: FIRST 是一个支持跨分布式高性能计算（HPC）集群提供推理即服务（Inference-as-a-Service）的工具包，通过兼容 OpenAI 的 API 实现私有、安全、可扩展的 AI 模型推理。


<details>
  <summary>Details</summary>
Motivation: 科学工作流对私有、安全且可扩展的 AI 推理需求日益增长，研究人员希望在本地 HPC 基础设施上高效运行大型语言模型等 AI 模型，而不依赖商业云服务。

Method: FIRST 利用 Globus Auth 和 Globus Compute 构建了一个集群无关的 API 框架，支持多种推理后端（如 vLLM），具备自动扩缩容、“热”节点维持、高吞吐批处理和交互式推理模式。

Result: 该框架可在联邦 HPC 集群上每日生成数十亿 tokens，实现低延迟、高吞吐的私有 AI 推理服务。

Conclusion: FIRST 有效满足了科研场景下对安全、可扩展、高性能本地 AI 推理的需求，为在现有 HPC 基础设施上部署和使用大型 AI 模型提供了实用解决方案。

Abstract: We present the Federated Inference Resource Scheduling Toolkit (FIRST), a
framework enabling Inference-as-a-Service across distributed High-Performance
Computing (HPC) clusters. FIRST provides cloud-like access to diverse AI
models, like Large Language Models (LLMs), on existing HPC infrastructure.
Leveraging Globus Auth and Globus Compute, the system allows researchers to run
parallel inference workloads via an OpenAI-compliant API on private, secure
environments. This cluster-agnostic API allows requests to be distributed
across federated clusters, targeting numerous hosted models. FIRST supports
multiple inference backends (e.g., vLLM), auto-scales resources, maintains
"hot" nodes for low-latency execution, and offers both high-throughput batch
and interactive modes. The framework addresses the growing demand for private,
secure, and scalable AI inference in scientific workflows, allowing researchers
to generate billions of tokens daily on-premises without relying on commercial
cloud infrastructure.

</details>


### [7] [Tight Conditions for Binary-Output Tasks under Crashes](https://arxiv.org/abs/2510.13755)
*Timothé Albouy,Antonio Fernández Anta,Chryssis Georgiou,Nicolas Nicolaou,Junlang Wang*

Main category: cs.DC

TL;DR: 本文研究了在最多 t 个进程可能崩溃的 n 进程分布式系统中，求解二元输出任务（输出值为 {0,1}）所需的 n 与 t 的精确条件，适用于同步与异步系统，并统一了如二元共识和对称性破缺等问题。


<details>
  <summary>Details</summary>
Motivation: 现有研究通常关注任务的有效性或输出值的多重性，而本文旨在通过仅关注任务可能产生的不同输出集合（忽略有效性与多重性），建立一个更通用的框架，以统一分析多种分布式计算问题并推导更强的不可能性结论。

Method: 作者采用“输出集”方法，分析在 n 个进程中最多 t 个可能崩溃的情况下，二元输出任务可解的充要系统条件，分别针对同步和异步系统进行完整刻画。

Result: 论文给出了在同步和异步系统中，所有二元输出任务类可解的 n 与 t 的紧致条件；该方法不仅统一了多个经典问题，还导出了适用于更广泛任务定义（如考虑有效性、值多重性或非二元输出）的不可能性证明。

Conclusion: 通过聚焦输出集而非传统约束，该工作为分布式任务可解性提供了高度通用的理论基础，增强了对共识、对称破缺等问题的理解，并扩展了不可能性结果的适用范围。

Abstract: This paper explores necessary and sufficient system conditions to solve
distributed tasks with binary outputs (\textit{i.e.}, tasks with output values
in $\{0,1\}$). We focus on the distinct output sets of values a task can
produce (intentionally disregarding validity and value multiplicity),
considering that some processes may output no value. In a distributed system
with $n$ processes, of which up to $t \leq n$ can crash, we provide a complete
characterization of the tight conditions on $n$ and $t$ under which every class
of tasks with binary outputs is solvable, for both synchronous and asynchronous
systems. This output-set approach yields highly general results: it unifies
multiple distributed computing problems, such as binary consensus and symmetry
breaking, and it produces impossibility proofs that hold for stronger task
formulations, including those that consider validity, account for value
multiplicity, or move beyond binary outputs.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [8] [AutoCode: LLMs as Problem Setters for Competitive Programming](https://arxiv.org/abs/2510.12803)
*Shang Zhou,Zihan Zheng,Kaiyuan Liu,Zeyu Shen,Zerui Cheng,Zexing Chen,Hansen He,Jianzhu Yao,Huanzhi Mao,Qiuyang Mang,Tianfu Fu,Beichen Li,Dongruixuan Li,Wenhao Chai,Zhuang Liu,Aleksandra Korolova,Peter Henderson,Natasha Jaques,Pramod Viswanath,Saining Xie,Jingbo Shang*

Main category: cs.SE

TL;DR: 本文提出AutoCode系统，通过多轮验证自动生成高质量的竞赛编程题目和测试用例，其生成结果接近官方判断标准（99%一致性），显著优于现有方法，并能创造新颖且符合竞赛标准的问题。


<details>
  <summary>Details</summary>
Motivation: 编写高质量的竞赛编程题目非常困难，需要设定精确的约束、输入分布和边界情况，以排除捷径解法并针对特定算法，这对大语言模型的能力提出了理想测试场景。

Method: AutoCode采用多轮验证机制生成竞赛级题目与测试用例，并通过参考解与暴力解交叉验证，过滤无效题目，确保正确性。

Result: 在保留测试集上，AutoCode生成的测试套件与官方判断一致性达99%，远超现有方法（如HardTests的81%以下）；生成的新题目被顶级（Grandmaster级）选手评为具备竞赛质量。

Conclusion: AutoCode能可靠地生成高质量、新颖且符合竞赛标准的编程题目，展示了大语言模型在复杂任务生成方面的潜力。

Abstract: Writing competitive programming problems is exacting. Authors must: set
constraints, input distributions, and edge cases that rule out shortcuts;
target specific algorithms (e.g., max-flow, dynamic programming, data
structures); and calibrate complexity beyond the reach of most competitors. We
argue that this makes for an ideal test of general large language model
capabilities and study whether they can do this reliably. We introduce
AutoCode, which uses multiple rounds of validation to yield competition-grade
problem statements and test cases. On held-out problems, AutoCode test suites
approach 99% consistency with official judgments, a significant improvement
over current state-of-the-art methods like HardTests, which achieve less than
81%. Furthermore, starting with a random seed problem, AutoCode can create
novel variants with reference and brute-force solutions. By cross-verifying
these generated solutions against test cases, we can further filter out
malformed problems. Our system ensures high correctness, as verified by human
experts. AutoCode successfully produces novel problems judged by
Grandmaster-level (top 0.3%) competitive programmers to be of contest quality.

</details>


### [9] [SpareCodeSearch: Searching for Code Context When You Have No Spare GPU](https://arxiv.org/abs/2510.12948)
*Minh Nguyen*

Main category: cs.SE

TL;DR: 该论文提出在检索增强生成（RAG）框架中使用关键词搜索替代语义搜索，以降低资源消耗，同时在大型代码库中仍能有效检索有用代码上下文，并在代码上下文竞赛基准上取得良好成绩。


<details>
  <summary>Details</summary>
Motivation: 现有的RAG框架依赖语义搜索进行上下文检索，需要大量计算资源，难以应用于轻量级场景（如IDE中的代码补全）。

Method: 采用关键词搜索方法从大型代码库中检索相关代码上下文，避免使用资源密集型的嵌入模型。

Result: 在Code Context Competition基准测试中，该方法在Kotlin和Python赛道分别取得了0.748和0.725的chRF分数。

Conclusion: 关键词搜索足以在不依赖GPU资源的情况下有效检索代码上下文，适用于轻量级代码补全应用。

Abstract: Retrieval-Augmented Generation (RAG) frameworks aim to enhance Code Language
Models (CLMs) by including another module for retrieving relevant context to
construct the input prompt. However, these retrieval modules commonly use
semantic search, requiring substantial computational resources for training and
hosting these embedded models, making them infeasible to integrate into
lightweight applications such as in-IDE AI-based code completion. In this
solution paper, we prove that using keyword-search is sufficient to retrieve
relevant and useful code context inside large codebases, without the need for
extensive GPU resources. The usefulness of code contexts found by our solution
is demonstrated through their completion results on the Code Context
Competition's benchmark, reaching 0.748 and 0.725 chRF scores on Kotlin and
Python tracks, respectively.

</details>


### [10] [ADPerf: Investigating and Testing Performance in Autonomous Driving Systems](https://arxiv.org/abs/2510.13078)
*Tri Minh-Triet Pham,Diego Elias Costa,Weiyi Shang,Jinqiu Yang*

Main category: cs.SE

TL;DR: 本文首次对工业级自动驾驶系统（Apollo和Autoware）中的障碍物检测模块进行性能测量与建模，提出工具ADPerf生成可加剧检测延迟的点云测试用例，揭示3D障碍物检测可能成为系统延迟瓶颈，并影响后续模块（如轨迹预测）的可靠性。


<details>
  <summary>Details</summary>
Motivation: 障碍物检测的延迟对自动驾驶系统的安全性和有效性至关重要，但目前对其延迟特性及对LiDAR点云变化的鲁棒性缺乏深入理解。

Method: 对Apollo和Autoware两个工业级自动驾驶系统中的障碍物检测模块进行性能测量与建模，并开发ADPerf工具生成能暴露高延迟的现实点云测试用例，进而对3D障碍物检测模块及其对轨迹预测模块的影响进行压力测试。

Result: 实验表明，3D障碍物检测模块在特定点云输入下会出现显著延迟，该延迟会传播至后续模块，降低整个自动驾驶系统的可靠性。

Conclusion: 必须对障碍物检测组件（尤其是3D检测）进行性能测试，因其可能成为系统延迟的主要瓶颈，并对整体系统可靠性产生连锁负面影响。

Abstract: Obstacle detection is crucial to the operation of autonomous driving systems,
which rely on multiple sensors, such as cameras and LiDARs, combined with code
logic and deep learning models to detect obstacles for time-sensitive
decisions. Consequently, obstacle detection latency is critical to the safety
and effectiveness of autonomous driving systems. However, the latency of the
obstacle detection module and its resilience to various changes in the LiDAR
point cloud data are not yet fully understood. In this work, we present the
first comprehensive investigation on measuring and modeling the performance of
the obstacle detection modules in two industry-grade autonomous driving
systems, i.e., Apollo and Autoware. Learning from this investigation, we
introduce ADPerf, a tool that aims to generate realistic point cloud data test
cases that can expose increased detection latency. Increasing latency decreases
the availability of the detected obstacles and stresses the capabilities of
subsequent modules in autonomous driving systems, i.e., the modules may be
negatively impacted by the increased latency in obstacle detection.
  We applied ADPerf to stress-test the performance of widely used 3D obstacle
detection modules in autonomous driving systems, as well as the propagation of
such tests on trajectory prediction modules. Our evaluation highlights the need
to conduct performance testing of obstacle detection components, especially 3D
obstacle detection, as they can be a major bottleneck to increased latency of
the autonomous driving system. Such an adverse outcome will also further
propagate to other modules, reducing the overall reliability of autonomous
driving systems.

</details>


### [11] [TRUSTVIS: A Multi-Dimensional Trustworthiness Evaluation Framework for Large Language Models](https://arxiv.org/abs/2510.13106)
*Ruoyu Sun,Da Song,Jiayang Song,Yuheng Huang,Lei Ma*

Main category: cs.SE

TL;DR: 本文提出了TRUSTVIS，一个用于自动评估大语言模型（LLM）可信度的框架，集成了扰动方法（如AutoDAN）和多种评估策略的多数投票机制，并通过交互式可视化界面帮助用户直观理解模型在安全性和鲁棒性方面的表现。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在自然语言处理应用中日益普及，但其在安全性和鲁棒性方面的可信度问题仍令人担忧，亟需系统化、可解释的评估工具。

Method: TRUSTVIS框架结合了已有的扰动方法（如AutoDAN），采用多种评估方法的多数投票机制，并通过交互式用户界面提供直观的可信度指标可视化。

Result: 在Vicuna-7b、Llama2-7b和GPT-3.5等模型上的初步案例研究表明，TRUSTVIS能有效识别模型在安全性和鲁棒性方面的漏洞，并支持用户深入探索评估结果。

Conclusion: TRUSTVIS为大语言模型的可信度评估提供了一个可靠、直观且用户友好的自动化框架，有助于推动针对性的模型改进。

Abstract: As Large Language Models (LLMs) continue to revolutionize Natural Language
Processing (NLP) applications, critical concerns about their trustworthiness
persist, particularly in safety and robustness. To address these challenges, we
introduce TRUSTVIS, an automated evaluation framework that provides a
comprehensive assessment of LLM trustworthiness. A key feature of our framework
is its interactive user interface, designed to offer intuitive visualizations
of trustworthiness metrics. By integrating well-known perturbation methods like
AutoDAN and employing majority voting across various evaluation methods,
TRUSTVIS not only provides reliable results but also makes complex evaluation
processes accessible to users. Preliminary case studies on models like
Vicuna-7b, Llama2-7b, and GPT-3.5 demonstrate the effectiveness of our
framework in identifying safety and robustness vulnerabilities, while the
interactive interface allows users to explore results in detail, empowering
targeted model improvements. Video Link: https://youtu.be/k1TrBqNVg8g

</details>


### [12] [GRACE: Globally-Seeded Representation-Aware Cluster-Specific Evolution for Compiler Auto-Tuning](https://arxiv.org/abs/2510.13176)
*Haolin Pan,Chao Zha,Jinyuan Dong,Mingjie Xing,Yanjun Wu*

Main category: cs.SE

TL;DR: 本文提出了GRACE框架，通过结合编译器pass协同效应、对比学习和进化搜索，在显著降低LLVM IR指令数的同时保持极低的调优开销，实现了对未见程序的良好泛化能力。


<details>
  <summary>Details</summary>
Motivation: 传统编译器启发式方法在代码大小优化等目标上表现次优，迭代编译搜索成本过高，而现有机器学习方法泛化能力有限，因此需要一种兼顾高效性与泛化能力的编译器自动调优方案。

Method: GRACE利用pass协同效应和加权评分生成高质量候选序列与pass池，通过基于pass序列的数据增强进行对比学习，构建程序嵌入以实现相似性感知聚类，并在聚类内进行进化搜索，得到k个专用pass序列；测试时从中选择最优序列并轻量微调。

Result: 在七个数据集上，GRACE相比LLVM的-Oz选项平均减少10.09%（LLVM 10.0.0）和10.19%（LLVM 18.1.6）的IR指令数，且每个程序平均调优时间不到1秒。

Conclusion: GRACE在编译器pass选择与排序问题上实现了先进性能，在显著提升优化效果的同时具备实用性和良好的泛化能力。

Abstract: Compiler pass selection and phase ordering present a significant challenge in
achieving optimal program performance, particularly for objectives like code
size reduction. Standard compiler heuristics offer general applicability but
often yield suboptimal, program-specific results due to their one-size-fits-all
nature. While iterative compilation can find tailored solutions, its
prohibitive search cost limits practical use. Machine learning approaches
promise faster inference but frequently struggle with generalization to unseen
programs. This paper introduces GRACE, a novel framework for compiler
auto-tuning, demonstrated for LLVM IR instruction count optimization. GRACE
effectively curtails the search space by leveraging pass synergies and a
weighted scoring method to generate initial high-quality candidate sequences
and a pass pool. It then employs contrastive learning, using pass
sequence-based data augmentation, to create program embeddings that facilitate
similarity-aware clustering. Evolutionary search within these clusters yields a
coreset of $k$ specialized pass sequences designed for robust generalization to
unseen programs. At test time, GRACE efficiently selects the best coreset
sequence and refines it using lightweight techniques. Experimental results on
seven diverse datasets show that GRACE reduces LLVM IR instruction count by an
average of 10.09% on LLVM 10.0.0 and 10.19% on LLVM 18.1.6 compared to opt -Oz,
while incurring an average tuning time of less than 1s per program,
demonstrating its state-of-the-art performance and practical effectiveness.

</details>


### [13] [Verifying a Sparse Matrix Algorithm Using Symbolic Execution](https://arxiv.org/abs/2510.13424)
*Alexander C. Wilton*

Main category: cs.SE

TL;DR: 本文提出使用符号执行来为科学软件编写类似传统单元测试的测试用例，以提供更强的验证保证，并将其应用于稀疏矩阵算法。


<details>
  <summary>Details</summary>
Motivation: 科学软件因其数学性和高度优化特性，容易产生传统测试难以发现的细微错误，因此需要更强大的验证方法。

Method: 采用符号执行技术编写测试用例，类似于传统单元测试，但能提供更强的验证保障。

Result: 该方法成功应用于稀疏矩阵算法，展示了其在检测科学软件中隐蔽错误方面的有效性。

Conclusion: 符号执行可有效增强科学软件的测试能力，提供比传统测试更强的正确性保证。

Abstract: Scientific software is, by its very nature, complex. It is mathematical and
highly optimized which makes it prone to subtle bugs not as easily detected by
traditional testing. We outline how symbolic execution can be used to write
tests similar to traditional unit tests while providing stronger verification
guarantees and apply this methodology to a sparse matrix algorithm.

</details>


### [14] [OpenDerisk: An Industrial Framework for AI-Driven SRE, with Design, Implementation, and Case Studies](https://arxiv.org/abs/2510.13561)
*Peng Di,Faqiang Chen,Xiao Bai,Hongjun Yang,Qingfeng Li,Ganglin Wei,Jian Mou,Feng Shi,Keting Chen,Peng Tang,Zhitao Shen,Zheng Li,Wenhui Shi,Junwei Guo,Hang Yu*

Main category: cs.SE

TL;DR: 本文提出了 OpenDerisk，一个专为站点可靠性工程（SRE）设计的开源多智能体框架，通过集成诊断原生协作模型、可插拔推理引擎、知识引擎和标准化协议（MCP），显著优于现有方法，在准确性和效率上表现突出，并已在蚂蚁集团大规模部署。


<details>
  <summary>Details</summary>
Motivation: 现代软件系统日益复杂，给SRE团队带来难以承受的运维负担，亟需能模拟专家诊断推理的AI自动化方案。现有方法（包括传统AI和通用多智能体系统）要么缺乏深度因果推理能力，要么无法适配SRE特有的调查型工作流。

Method: 提出 OpenDerisk 框架，其核心包括诊断原生的协作模型、可插拔推理引擎、知识引擎以及基于MCP标准协议的多智能体通信机制，使专业智能体能协同解决跨领域的复杂SRE问题。

Result: 实验评估表明，OpenDerisk 在准确性和效率上显著优于当前最先进的基线方法；已在蚂蚁集团大规模生产环境中部署，日均服务超3000名用户，验证了其工业级可扩展性与实际应用价值。

Conclusion: OpenDerisk 是一个高效、可扩展且专为SRE场景优化的多智能体框架，具备实际工业部署能力，现已开源，有望推动SRE智能化的发展。

Abstract: The escalating complexity of modern software imposes an unsustainable
operational burden on Site Reliability Engineering (SRE) teams, demanding
AI-driven automation that can emulate expert diagnostic reasoning. Existing
solutions, from traditional AI methods to general-purpose multi-agent systems,
fall short: they either lack deep causal reasoning or are not tailored for the
specialized, investigative workflows unique to SRE. To address this gap, we
present OpenDerisk, a specialized, open-source multi-agent framework
architected for SRE. OpenDerisk integrates a diagnostic-native collaboration
model, a pluggable reasoning engine, a knowledge engine, and a standardized
protocol (MCP) to enable specialist agents to collectively solve complex,
multi-domain problems. Our comprehensive evaluation demonstrates that
OpenDerisk significantly outperforms state-of-the-art baselines in both
accuracy and efficiency. This effectiveness is validated by its large-scale
production deployment at Ant Group, where it serves over 3,000 daily users
across diverse scenarios, confirming its industrial-grade scalability and
practical impact. OpenDerisk is open source and available at
https://github.com/derisk-ai/OpenDerisk/

</details>


### [15] [Property Testing for Ocean Models. Can We Specify It? (Invited Talk)](https://arxiv.org/abs/2510.13692)
*Deepak A. Cherian*

Main category: cs.SE

TL;DR: 本文受属性测试文献启发，探讨如何将地球物理流体动力学（GFD）理论转化为属性测试，以解决海洋模型验证中的“预言机问题”。


<details>
  <summary>Details</summary>
Motivation: 解决海洋数值模型验证中的“预言机问题”，即如何判断模型输出是否正确，作者受属性测试方法和John Hughes教授工作的启发，尝试将GFD理论用于构建可验证的属性测试。

Method: 将若干理想化的GFD问题重新表述为属性测试，利用物理规律自然地定义模型应满足的属性。

Result: 提出了若干可作为属性测试的GFD问题示例，展示了物理理论如何自然地支持属性测试的构建，但尚未评估哪些测试最可行或最有用。

Conclusion: GFD理论有潜力用于构建海洋模型的属性测试，但需进一步研究以确定最有效和实用的测试方案。

Abstract: I take inspiration from the property-testing literature, particularly the
work of Prof. John Hughes, and explore how such ideas might be applied to
numerical models of the ocean. Specifically, I ask whether geophysical fluid
dynamics (GFD) theory, expressed as property tests, might be used to address
the oracle problem of testing the correctness of ocean models. I propose that a
number of simple idealized GFD problems can be framed as property tests. These
examples clearly illustrate how physics naturally lends itself to specifying
property tests. Which of these proposed tests might be most feasible and
useful, remains to be seen.

</details>


### [16] [On Pretraining for Project-Level Code Completion](https://arxiv.org/abs/2510.13697)
*Maksim Sapronov,Evgeniy Glukhov*

Main category: cs.SE

TL;DR: 该研究通过扩展OpenCoder模型的上下文窗口至16,384个token，并在仅使用10亿token的精选仓库级数据上进行训练，发现不同的仓库处理策略对上下文学习影响有限，主要性能提升来自调整RoPE缩放参数；同时表明即使采用更简单的文件级训练方法，在原始上下文长度下也能取得优异效果。


<details>
  <summary>Details</summary>
Motivation: 探索不同仓库级数据处理策略如何影响大语言模型在代码任务中的上下文学习能力，并验证在有限数据和计算资源下是否仍能实现有效的仓库级代码补全。

Method: 将OpenCoder模型的上下文窗口从4,096扩展到16,384个token，使用额外10亿token的精选仓库级数据进行训练，并对比不同仓库处理策略及RoPE缩放参数的影响；同时评估简化后的文件级训练方法的效果。

Result: 尽管使用的数据量远小于同类模型（通常使用数千亿token），该模型在Long Code Arena基准上仍取得相当的性能；不同仓库处理策略效果相近，主要提升来自RoPE缩放参数的调整；文件级训练方法在原始序列长度下依然高效。

Conclusion: 仓库级预训练中，数据处理策略的影响有限，关键在于位置编码的适配；同时，资源受限场景下采用更简单的文件级训练也能实现高效的代码补全，为相关研究提供了低门槛路径。

Abstract: Repository-level pretraining is commonly used to enable large language models
for code to leverage codebase-wide context. This enhances their ability to
generate accurate and context-aware code completions. In this work, we
investigate how different repository-processing strategies affect in-context
learning in OpenCoder, a 1.5B-parameter model. We extend its context window
from 4,096 to 16,384 tokens by training on additional 1B tokens of curated
repository-level data. Despite relying on a smaller dataset than competing
models (which often use hundreds of billions of tokens), our model achieves
comparable performance on the Long Code Arena benchmark. We find that various
repository-processing techniques yield similarly strong results, with the
primary gain coming from adapting to a new rotary positional embedding (RoPE)
scaling parameter. Finally, we show that a simpler file-level training approach
at the original sequence length remains highly effective, opening up
repository-level code completion research to settings with more constrained
data and compute resources.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [17] [D-com: Accelerating Iterative Processing to Enable Low-rank Decomposition of Activations](https://arxiv.org/abs/2510.13147)
*Faraz Tahmasebi,Michael Pelluer,Hyoukjun Kwon*

Main category: cs.AR

TL;DR: 本文提出了一种名为D-com的加速器架构，通过改进输入分解算法（采用Lanczos算法）、设计协同加速器、引入计算复制方法和输出形状保持计算方案，并结合多轨分解策略，在仅轻微降低模型质量的前提下，相比A100 GPU实现了22%的端到端延迟改进。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型的计算和内存成本持续增长，已达万亿参数规模。现有模型压缩技术（如低秩分解）多聚焦于权重分解，以避免运行时分解带来的高延迟。然而，作者认为在合适的分解算法和硬件支持下，输入分解仍可带来显著收益。

Method: 采用渐进式分解算法（Lanczos算法），设计专用协加速器架构；引入计算复制方法缓解分解操作的内存瓶颈；开发输出形状保持计算方案以消除连续层中的分解开销；提出多轨分解方法，单独处理异常通道以在低计算开销下保持高精度和低困惑度。

Result: 在评估中，所提方法实现了6.2倍的分解操作加速，并在端到端推理中相比A100 GPU获得22%的延迟改善，同时模型质量仅轻微下降（如在AI2推理挑战任务上下降3%）。

Conclusion: 通过算法与硬件协同设计，输入分解在大型语言模型中具有显著潜力，D-com加速器在几乎不影响模型质量的前提下有效提升了推理效率。

Abstract: The computation and memory costs of large language models kept increasing
over last decade, which reached over the scale of 1T parameters. To address the
challenges from the large scale models, model compression techniques such as
low-rank decomposition have been explored. Previous model decomposition works
have focused on weight decomposition to avoid costly runtime decomposition,
whose latency often significantly exceeds the benefits from decomposition
(e.g., 38% more end-to-end latency when running Llama2-7b on A100 with 4K
sequence length with activation decomposition compared to no decomposition). In
this work, we debunk such observations and report that the input decomposition
can be significantly beneficial with a proper choice of decomposition algorithm
and hardware support. We adopt progressive decomposition algorithm, Lanczos
algorithm, and design a co-accelerator architecture for the decomposition
algorithm. To address the memory- boundness of the decomposition operation, we
introduce a novel compute replication methodology that moves the op- eration
toward compute-bound region, which enables 6.2x speedup in our evaluation. We
also develop an output shape- preserving computation scheme that eliminates
decomposi- tion costs in consecutive layers. To compensate model quality loss
from compression, we introduce a multi-track decom- position approach that
separately handles outlier channels for high accuracy and low perplexity with
minimal compu- tational costs. Combined together, our accelerator, D-com,
provides 22% end-to-end latency improvements compared to A100 GPU at the cost
of small model quality degradation (e.g., 3% on AI2 Reasoning Challenge task).

</details>


### [18] [F-BFQ: Flexible Block Floating-Point Quantization Accelerator for LLMs](https://arxiv.org/abs/2510.13401)
*Jude Haris,José Cano*

Main category: cs.AR

TL;DR: 本文提出了一种灵活的块浮点量化（F-BFQ）加速器，可在不同BFP变体间动态切换以高效执行量化大语言模型（LLM）的矩阵乘法，相比Arm NEON CPU平均推理速度提升1.4倍。


<details>
  <summary>Details</summary>
Motivation: 在边缘设备上部署大语言模型（LLM）需要低内存占用和低计算开销，块浮点（BFP）量化是关键手段。然而，现有LLM通常采用混合BFP量化策略，要求加速器能支持多种BFP格式而无需重新配置，因此需要一种灵活的硬件加速方案。

Method: 作者设计并实现了Flexible Block Floating-Point Quantization（F-BFQ）加速器，能够在运行时动态切换两种BFP量化格式，并高效执行矩阵乘法（MatMul）操作。该加速器部署在AMD Kria开发板上。

Result: 在三种BFP量化LLM上，F-BFQ加速器相比基于Arm NEON的CPU实现平均推理时间减少1.4倍，达到每秒5.2个token（约3.9个词）的吞吐量。

Conclusion: F-BFQ加速器有效支持混合BFP量化LLM的高效推理，在边缘设备上展现出显著性能优势，为资源受限场景下的LLM部署提供了可行的硬件加速方案。

Abstract: Large Language Models (LLMs) have become increasingly prominent for daily
tasks, from improving sound-totext translation to generating additional frames
for the latest video games. With the help of LLM inference frameworks, such as
llama.cpp, which support optimizations such as KV-caching and quantization, it
is now easier than ever to deploy LLMs on edge devices. Quantization is
fundamental to enable LLMs on resource-constrained edge devices, and llama.cpp
utilizes block floating point (BFP) quantization to drastically reduce the bit
width of weights and input tensors, the memory footprint, and the computational
power required to run LLMs. LLMs are typically quantized with mixed BFP
quantization across the model layers to reduce the loss of model accuracy due
to quantization. Therefore, to efficiently accelerate across the layers of
BFP-quantized LLMs, specialized accelerators need to support different BFP
variants without reconfiguration. To address this issue, we propose a Flexible
Block FloatingPoint Quantization (F-BFQ) accelerator, which can dynamically
switch between two BFP quantization variants and perform matrix multiplication
(MatMul) operations. Our initial F-BFQ accelerator design, deployed on the AMD
Kria board, reduces inference time by 1.4x on average over the Arm NEON-based
CPU execution across three BFP quantized LLMs while achieving 5.2 tokens per
second (~3.9 words per second).

</details>
