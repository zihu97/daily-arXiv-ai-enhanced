<div id=toc></div>

# Table of Contents

- [cs.SE](#cs.SE) [Total: 15]
- [cs.PF](#cs.PF) [Total: 1]
- [cs.AR](#cs.AR) [Total: 4]
- [cs.NI](#cs.NI) [Total: 6]
- [cs.DC](#cs.DC) [Total: 17]
- [cs.MA](#cs.MA) [Total: 3]


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [1] [Adaptive Reinforcement Learning for Dynamic Configuration Allocation in Pre-Production Testing](https://arxiv.org/abs/2510.05147)
*Yu Zhu*

Main category: cs.SE

TL;DR: 本文提出一种基于强化学习的动态配置分配方法，通过结合Q学习与混合奖励机制，在非平稳环境中高效、稳健地分配测试资源。


<details>
  <summary>Details</summary>
Motivation: 现代软件系统需在高度异构且不断演化的环境中进行可靠测试，但穷举测试不可行，且故障概率会随时间漂移，现有静态组合优化方法难以应对这种非平稳场景。

Method: 提出一种强化学习框架，将配置分配建模为序列决策问题，结合Q学习与融合模拟结果和实时反馈的混合奖励设计，并采用自适应的在线-离线训练策略。

Result: 大量仿真实验表明，该方法在性能上持续优于静态和基于优化的基线方法，接近理想Oracle表现。

Conclusion: 强化学习为动态测试资源配置提供了强大新范式，超越传统方法，具有在动态测试与资源调度领域广泛应用的潜力。

Abstract: Ensuring reliability in modern software systems requires rigorous
pre-production testing across highly heterogeneous and evolving environments.
Because exhaustive evaluation is infeasible, practitioners must decide how to
allocate limited testing resources across configurations where failure
probabilities may drift over time. Existing combinatorial optimization
approaches are static, ad hoc, and poorly suited to such non-stationary
settings. We introduce a novel reinforcement learning (RL) framework that
recasts configuration allocation as a sequential decision-making problem. Our
method is the first to integrate Q-learning with a hybrid reward design that
fuses simulated outcomes and real-time feedback, enabling both sample
efficiency and robustness. In addition, we develop an adaptive online-offline
training scheme that allows the agent to quickly track abrupt probability
shifts while maintaining long-run stability. Extensive simulation studies
demonstrate that our approach consistently outperforms static and
optimization-based baselines, approaching oracle performance. This work
establishes RL as a powerful new paradigm for adaptive configuration
allocation, advancing beyond traditional methods and offering broad
applicability to dynamic testing and resource scheduling domains.

</details>


### [2] [VeriGuard: Enhancing LLM Agent Safety via Verified Code Generation](https://arxiv.org/abs/2510.05156)
*Lesly Miculicich,Mihir Parmar,Hamid Palangi,Krishnamurthy Dj Dvijotham,Mirko Montanari,Tomas Pfister,Long T. Le*

Main category: cs.SE

TL;DR: VeriGuard 是一个为基于大语言模型（LLM）的智能体提供形式化安全保证的新框架，通过离线验证与在线监控两阶段架构，确保其行为始终符合预定义的安全约束。


<details>
  <summary>Details</summary>
Motivation: 在医疗等敏感领域部署自主AI智能体存在安全、隐私和对抗攻击等风险，现有系统缺乏对智能体行为提供形式化安全保证的有效机制。

Method: VeriGuard 采用双阶段架构：离线阶段明确用户意图、生成行为策略并进行测试与形式化验证；在线阶段作为运行时监控器，对每个动作进行实时验证。

Result: 该框架实现了对LLM智能体行为的形式化安全保证，在保障正确性的同时兼顾运行效率，显著提升了智能体的可信度。

Conclusion: 通过分离繁重的离线验证与轻量的在线监控，VeriGuard 为LLM智能体在敏感场景中的安全可靠部署提供了实用且可验证的解决方案。

Abstract: The deployment of autonomous AI agents in sensitive domains, such as
healthcare, introduces critical risks to safety, security, and privacy. These
agents may deviate from user objectives, violate data handling policies, or be
compromised by adversarial attacks. Mitigating these dangers necessitates a
mechanism to formally guarantee that an agent's actions adhere to predefined
safety constraints, a challenge that existing systems do not fully address. We
introduce VeriGuard, a novel framework that provides formal safety guarantees
for LLM-based agents through a dual-stage architecture designed for robust and
verifiable correctness. The initial offline stage involves a comprehensive
validation process. It begins by clarifying user intent to establish precise
safety specifications. VeriGuard then synthesizes a behavioral policy and
subjects it to both testing and formal verification to prove its compliance
with these specifications. This iterative process refines the policy until it
is deemed correct. Subsequently, the second stage provides online action
monitoring, where VeriGuard operates as a runtime monitor to validate each
proposed agent action against the pre-verified policy before execution. This
separation of the exhaustive offline validation from the lightweight online
monitoring allows formal guarantees to be practically applied, providing a
robust safeguard that substantially improves the trustworthiness of LLM agents.

</details>


### [3] [Test Case Generation from Bug Reports via Large Language Models: A Cognitive Layered Evaluation Framework](https://arxiv.org/abs/2510.05365)
*Irtaza Sajid Qureshi,Zhen Ming,Jiang*

Main category: cs.SE

TL;DR: 本文系统评估了大语言模型（LLM）在基于自然语言缺陷报告生成测试用例任务中的推理能力，依据布鲁姆分类法的认知层次进行分析，发现模型在记忆和理解层面表现尚可，但在应用层面（如标识符变异）性能大幅下降，而结构化技术元素对测试生成效果影响显著。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型在自动化软件测试中的应用日益广泛，但其是否能超越记忆模式、真正理解并推理自然语言缺陷报告尚不明确，亟需系统性评估其认知与推理能力。

Method: 基于布鲁姆分类法的六个认知层次（记忆、理解、应用、分析、评价、创造），在LIBRO框架下对StarCoder和GPT-4o在Defects4J、GHRB及其语言与语义变异数据集上进行测试用例生成评估，并分析不同提示策略和输入元素的影响。

Result: 模型主要复现已有结果（记忆）；对语言改写和翻译具有一定鲁棒性并能发现新bug（理解）；但在标识符变异下性能下降超60%（应用）；提供近似示例可提升成功率最多3倍；结构化技术元素（如测试代码、方法名）比叙述性描述更关键（分析）。

Conclusion: LLM在测试生成中的推理能力有限，尤其在需要泛化和适应代码结构变化时表现不佳；结构化信息对性能至关重要；研究为提升LLM测试生成能力提供了具体方向，并建立了更现实的评估范式。

Abstract: Large Language Models (LLMs) are increasingly applied to automated software
testing, yet their ability to generalize beyond memorized patterns and reason
about natural language bug reports remains unclear. We present a systematic
evaluation of LLM reasoning in test case generation, structured around the
cognitive layers of Bloom's taxonomy: \textit{Remember}, \textit{Understand},
\textit{Apply}, \textit{Analyze}, \textit{Evaluate}, and \textit{Create}, which
progressively assess higher levels of cognitive and reasoning capabilities.
Building on the LIBRO framework, we evaluate StarCoder and GPT-4o on Defects4J,
GHRB, and mutated variants that introduce linguistic and semantic challenges.
Our findings show that both models largely reproduce prior results with minor
deviations (\textit{Remember}), exhibit partial robustness to linguistic
rephrasings and translations while uncovering unique reproducible bugs
(\textit{Understand}), but suffer severe performance drops exceeding 60\% under
identifier mutations (\textit{Apply}). Conversely, providing near-identical
few-shot examples in an open-book setting improves success rates by up to three
times, and component-level analysis reveals that structured technical elements,
such as test code and method names, are far more impactful than narrative
descriptions for successful test generation (\textit{Analyze}). These insights
illuminate the cognitive processes underlying LLM-generated tests, suggest
concrete directions for improving performance, and establish a robust and
realistic evaluation paradigm for this task.

</details>


### [4] [Who Do You Think You Are? Creating RSE Personas from GitHub Interactions](https://arxiv.org/abs/2510.05390)
*Felicity Anderson,Julien Sindt,Neil Chue Hong*

Main category: cs.SE

TL;DR: 本文提出了一种结合软件仓库挖掘与数据驱动用户画像的方法，用于刻画科研软件工程（RSE）中的贡献者行为模式，并在GitHub上对1,284个科研软件仓库的115,174名贡献者进行了分析，识别出七种典型RSE角色。


<details>
  <summary>Details</summary>
Motivation: 科研软件工程（RSE）缺乏对贡献者行为模式的系统理解，阻碍了团队协作与项目改进。作者希望通过数据驱动的方式揭示RSE中的常见与罕见开发模式，帮助个体和团队更好地理解其贡献、影响及仓库动态。

Method: 结合软件仓库挖掘与数据驱动用户画像技术，分析GitHub上中等规模（10–300名提交者）科研软件仓库的协作行为数据，从Zenodo筛选出42,284个候选仓库，最终对1,284个仓库中的115,174名贡献者进行建模。

Result: 识别并命名了七种RSE贡献者画像，按互动程度从低到高依次为：短暂贡献者、偶尔贡献者、项目组织者、中等贡献者、低流程关闭者、低编码关闭者和活跃贡献者。

Conclusion: 该方法能有效应对科研软件项目在管理方式、研究领域和贡献者背景等方面的异质性，证明大规模数据可用于刻画RSE行为模式，为改进科研软件工程实践提供基础。

Abstract: We describe data-driven RSE personas: an approach combining software
repository mining and data-driven personas applied to research software (RS),
an attempt to describe and identify common and rare patterns of Research
Software Engineering (RSE) development. This allows individuals and RS project
teams to understand their contributions, impact and repository dynamics - an
important foundation for improving RSE. We evaluate the method on different
patterns of collaborative interaction behaviours by contributors to mid-sized
public RS repositories (those with 10-300 committers) on GitHub. We demonstrate
how the RSE personas method successfully characterises a sample of 115,174
repository contributors across 1,284 RS repositories on GitHub, sampled from
42,284 candidate software repository records queried from Zenodo. We identify,
name and summarise seven distinct personas from low to high interactivity:
Ephemeral Contributor; Occasional Contributor; Project Organiser; Moderate
Contributor; Low-Process Closer; Low-Coding Closer; and Active Contributor.
This demonstrates that large datasets can be analysed despite difficulties of
comparing software projects with different project management factors, research
domains and contributor backgrounds.

</details>


### [5] [UnitTenX: Generating Tests for Legacy Packages with AI Agents Powered by Formal Verification](https://arxiv.org/abs/2510.05441)
*Yiannis Charalambous,Claudionor N. Coelho Jr,Luis Lamb,Lucas C. Cordeiro*

Main category: cs.SE

TL;DR: UnitTenX 是一个结合 AI 多智能体、形式化方法与大语言模型的开源系统，用于自动生成遗留代码的单元测试，提升测试覆盖率、代码可读性与软件可靠性。


<details>
  <summary>Details</summary>
Motivation: 遗留代码通常缺乏测试覆盖，且结构复杂，传统方法难以高效生成高质量单元测试；同时大语言模型在缺陷检测方面存在局限，需要更鲁棒的框架来提升软件可维护性。

Method: 结合多智能体系统、形式化方法和大语言模型，构建名为 UnitTenX 的自动化单元测试生成框架。

Result: UnitTenX 能有效生成高质量单元测试，发现潜在问题，并提升遗留代码的可读性与文档化水平。

Conclusion: UnitTenX 提供了一种有效且鲁棒的方法，显著改善了遗留代码的测试覆盖与软件可靠性，弥补了纯大语言模型在测试生成中的不足。

Abstract: This paper introduces UnitTenX, a state-of-the-art open-source AI multi-agent
system designed to generate unit tests for legacy code, enhancing test coverage
and critical value testing. UnitTenX leverages a combination of AI agents,
formal methods, and Large Language Models (LLMs) to automate test generation,
addressing the challenges posed by complex and legacy codebases. Despite the
limitations of LLMs in bug detection, UnitTenX offers a robust framework for
improving software reliability and maintainability. Our results demonstrate the
effectiveness of this approach in generating high-quality tests and identifying
potential issues. Additionally, our approach enhances the readability and
documentation of legacy code.

</details>


### [6] [What Types of Code Review Comments Do Developers Most Frequently Resolve?](https://arxiv.org/abs/2510.05450)
*Saul Goldman,Hong Yi Lin,Jirat Pasuksmit,Patanamon Thongtanunam,Kla Tantithamthavorn,Zhe Wang,Ray Zhang,Ali Behnaz,Fan Jiang,Michael Siers,Ryan Jiang,Mike Buller,Minwoo Jeong,Ming Wu*

Main category: cs.SE

TL;DR: 本文研究了大语言模型（LLM）与人类在代码审查评论中的类型差异，并发现可读性、缺陷和可维护性相关的LLM评论更易被开发者采纳，表明LLM生成的评论具有较高可操作性。


<details>
  <summary>Details</summary>
Motivation: 理解哪些类型的LLM生成的代码审查评论更可能促使开发者修改代码，有助于识别可操作的评论，提升LLM驱动代码审查工具的实际效果。

Method: 作者构建了一个“LLM-as-a-Judge”系统，基于自定义的五类分类法自动对人类和LLM生成的代码审查评论进行分类，并通过实证研究比较其类型分布与解决率。

Result: 研究发现：(1) LLM与人类审查者在不同项目背景下各有优劣；(2) 可读性、缺陷和可维护性相关的评论比代码设计类评论有更高的解决率。

Conclusion: LLM生成的评论中有相当一部分是可操作的，LLM与人类审查者具有互补性，研究结果为提升LLM代码审查工具的有效性提供了建议。

Abstract: Large language model (LLM)-powered code review automation tools have been
introduced to generate code review comments. However, not all generated
comments will drive code changes. Understanding what types of generated review
comments are likely to trigger code changes is crucial for identifying those
that are actionable. In this paper, we set out to investigate (1) the types of
review comments written by humans and LLMs, and (2) the types of generated
comments that are most frequently resolved by developers. To do so, we
developed an LLM-as-a-Judge to automatically classify review comments based on
our own taxonomy of five categories. Our empirical study confirms that (1) the
LLM reviewer and human reviewers exhibit distinct strengths and weaknesses
depending on the project context, and (2) readability, bugs, and
maintainability-related comments had higher resolution rates than those focused
on code design. These results suggest that a substantial proportion of
LLM-generated comments are actionable and can be resolved by developers. Our
work highlights the complementarity between LLM and human reviewers and offers
suggestions to improve the practical effectiveness of LLM-powered code review
tools.

</details>


### [7] [An Empirical Study of Security-Policy Related Issues in Open Source Projects](https://arxiv.org/abs/2510.05604)
*Rintaro Kanaji,Brittany Reid,Yutaro Kashiwa,Raula Gaikovina Kula,Hajimu Iida*

Main category: cs.SE

TL;DR: 该研究分析了开源项目中SECURITY.md文件在漏洞报告流程中的作用与挑战，发现大多数相关议题是请求添加该文件，且包含链接的议题关闭更快。


<details>
  <summary>Details</summary>
Motivation: GitHub建议项目采用SECURITY.md文件以规范漏洞报告流程，但其实际效果和操作挑战尚不清楚，因此需要深入研究。

Method: 研究对711个随机抽样的SECURITY.md相关议题进行分类和内容分析，并对包括SECURITY.md在内的六种社区健康文件相关议题的关闭时间和回复数量进行了定量比较分析。

Result: 79.5%的SECURITY.md相关议题是请求添加该文件；包含链接的议题中位关闭时间缩短了2天。

Conclusion: 研究为改进安全报告政策和社区管理提供了实用见解，有助于构建更安全的开源生态系统。

Abstract: GitHub recommends that projects adopt a SECURITY.md file that outlines
vulnerability reporting procedures. However, the effectiveness and operational
challenges of such files are not yet fully understood. This study aims to
clarify the challenges that SECURITY.md files face in the vulnerability
reporting process within open-source communities. Specifically, we classified
and analyzed the content of 711 randomly sampled issues related to SECURITY.md.
We also conducted a quantitative comparative analysis of the close time and
number of responses for issues concerning six community health files, including
SECURITY.md. Our analysis revealed that 79.5% of SECURITY.md-related issues
were requests to add the file, and reports that included links were closed,
with a median time that was 2 days shorter. These findings offer practical
insights for improving security reporting policies and community management,
ultimately contributing to a more secure open-source ecosystem.

</details>


### [8] [The Software Observatory: aggregating and analysing software metadata for trend computation and FAIR assessment](https://arxiv.org/abs/2510.05705)
*Eva Martín del Pico,Josep Lluís Gelpí,Salvador Capella-Gutiérrez*

Main category: cs.SE

TL;DR: 本文介绍了OpenEBench的Software Observatory平台，该平台整合多源研究软件元数据，通过FAIRsoft Evaluator评估并可视化软件在FAIR（可发现、可访问、互操作、可重用）原则下的表现，以促进生命科学领域研究软件的发展与改进。


<details>
  <summary>Details</summary>
Motivation: 科研软件开发领域变化迅速，科学界需掌握当前趋势以识别可能阻碍科研进展的差距；FAIR原则可作为理解趋势和提出改进措施的依据。

Method: 构建Software Observatory网络门户，整合多源软件元数据，并通过FAIRsoft Evaluator组件对研究软件进行FAIR性评估，提供不同粒度的可视化分析。

Result: 平台支持用户分析生命科学研究软件生态的趋势与演进，评估软件FAIR性并获得改进建议，提升软件开发实践。

Conclusion: Software Observatory为研究人员、开发者和利益相关者提供了宝贵的资源，有助于推动研究软件遵循FAIR原则并提升整体质量。

Abstract: In the ever-changing realm of research software development, it is crucial
for the scientific community to grasp current trends to identify gaps that can
potentially hinder scientific progress. The adherence to the FAIR (Findable,
Accessible, Interoperable, Reusable) principles can serve as a proxy to
understand those trends and provide a mechanism to propose specific actions.
  The Software Observatory at OpenEBench
(https://openebench.bsc.es/observatory) is a novel web portal that consolidates
software metadata from various sources, offering comprehensive insights into
critical research software aspects. Our platform enables users to analyse
trends, identify patterns and advancements within the Life Sciences research
software ecosystem, and understand its evolution over time. It also evaluates
research software according to FAIR principles for research software, providing
scores for different indicators.
  Users have the ability to visualise this metadata at different levels of
granularity, ranging from the entire software landscape to specific communities
to individual software entries through the FAIRsoft Evaluator. Indeed, the
FAIRsoft Evaluator component streamlines the assessment process, helping
developers efficiently evaluate and obtain guidance to improve their software's
FAIRness.
  The Software Observatory represents a valuable resource for researchers and
software developers, as well as stakeholders, promoting better software
development practices and adherence to FAIR principles for research software.

</details>


### [9] [Digital Twins for Software Engineering Processes](https://arxiv.org/abs/2510.05768)
*Robin Kimmel,Judith Michael,Andreas Wortmann,Jingxi Zhang*

Main category: cs.SE

TL;DR: 本文提出将数字孪生（DT）应用于软件工程，以应对软件工程师短缺问题，提升专家效率并支持领域专家开发高质量软件。


<details>
  <summary>Details</summary>
Motivation: 面对熟练软件工程师短缺的挑战，作者希望通过数字孪生更好地表示、理解和优化软件工程过程，使软件专家高效利用时间，并帮助领域专家产出高质量软件。

Method: 论文并未提出具体技术方法，而是构想数字孪生在软件工程中的应用形式，探讨其实现与部署所需的关键要素。

Result: 论文勾勒了软件工程数字孪生的潜在优势、可能形态以及当前缺失的关键组成部分。

Conclusion: 软件工程数字孪生具有巨大潜力，但其实现仍需填补若干技术和协作方面的空白。

Abstract: Digital twins promise a better understanding and use of complex systems. To
this end, they represent these systems at their runtime and may interact with
them to control their processes. Software engineering is a wicked challenge in
which stakeholders from many domains collaborate to produce software artifacts
together. In the presence of skilled software engineer shortage, our vision is
to leverage DTs as means for better rep- resenting, understanding, and
optimizing software engineering processes to (i) enable software experts making
the best use of their time and (ii) support domain experts in producing
high-quality software. This paper outlines why this would be beneficial, what
such a digital twin could look like, and what is missing for realizing and
deploying software engineering digital twins.

</details>


### [10] [Mellum: Production-Grade in-IDE Contextual Code Completion with Multi-File Project Understanding](https://arxiv.org/abs/2510.05788)
*Nikita Pavlichenko,Iurii Nazarov,Ivan Dolgov,Ekaterina Garanina,Dmitry Ustalov,Ivan Bondyrev,Kseniia Lysaniuk,Evgeniia Vu,Kirill Chekmenev,Joseph Shtok,Yaroslav Golubev,Anton Semenkin,Uladzislau Sazanovich*

Main category: cs.SE

TL;DR: Mellum 是一个开源的 4B 参数代码补全模型家族，专为 JetBrains IDE 中的交互式使用而设计，采用 Llama 架构并在约 4T 个宽松许可的多语言代码 token 上预训练。通过精心的数据治理、多阶段训练（包括中间填充和项目上下文）以及基于真实用户反馈的直接偏好优化，Mellum 在离线和在线评估中均表现出高质量，并在 Apache-2.0 许可下公开发布。


<details>
  <summary>Details</summary>
Motivation: 为满足 JetBrains IDE 中交互式代码补全对低延迟、低成本和高质量的需求，开发一个专注于任务、可扩展且开源的代码补全模型。

Method: 构建端到端工业级流程，包括：严格的数据治理、多阶段训练（含 fill-in-the-middle 和项目上下文的监督微调）、以及利用真实场景反馈进行直接偏好优化（DPO）对齐。

Result: Mellum 模型在大规模离线基准和 JetBrains IDE 中的在线遥测数据上均表现优异，验证了数据筛选、分阶段训练和上下文打包对模型质量的关键作用，并成功部署服务数十万用户。

Conclusion: 一个紧凑、任务聚焦且经过良好工程化的开源模型，结合严谨的数据与训练流程，能够有效从研究原型转化为大规模生产应用，为工业界提供可复现的实践蓝图。

Abstract: We present the Mellum models family, open-weight code completion models
designed for interactive use in JetBrains IDEs. Mellums have 4B parameters,
adopt a Llama-style architecture, and are pre-trained on ~4T tokens of
permissively licensed, multi-language code. Our studies show that (i) careful
data curation and staged training significantly improve the model's quality,
(ii) editor-critical capabilities such as context packing are necessary for
high-quality suggestions, and (iii) a compact, task-focused model can meet the
cost and latency constraints of interactive completion.
  In the paper, we describe an end-to-end industrial pipeline for producing
contextualized in-editor completion: disciplined data governance, multi-stage
training that includes fill-in-the-middle and project context via supervised
fine-tuning, and alignment via direct preference optimization using feedback
from real-world scenarios. Our quality evaluations include both large-scale
offline benchmarks and online telemetry from production deployments in
JetBrains IDEs. Mellums are released under the Apache-2.0 license on
HuggingFace, with a public model card providing a reproducible reference for
practitioners. Our experience offers a pragmatic blueprint for taking a
focused, open model from a research prototype to at scale production for
hundreds of thousands of users.

</details>


### [11] [A Wave of Resignations in the Aftermath of Remote Onboarding](https://arxiv.org/abs/2510.05878)
*Darja Smite,Franz Zieris,Lars-Ola Damm*

Main category: cs.SE

TL;DR: 该研究分析了爱立信瑞典公司2016-2025年员工离职数据，发现疫情期间远程入职的员工在三年内离职率显著更高，主要因缺乏组织归属感；研究强调在混合办公模式下，新员工与团队及资深员工的线下互动对提升留任率至关重要。


<details>
  <summary>Details</summary>
Motivation: 新冠疫情使远程办公常态化，但软件团队在完全远程模式下面临挑战，尤其在员工留任方面。作者旨在探究不同办公模式（现场、远程、混合）对员工离职行为的影响，以指导后疫情时代的人力资源政策。

Method: 利用爱立信瑞典公司2016-2025年的人力资源数据，结合离职调查，分析疫情前后不同办公模式下员工（尤其是新入职员工）的离职趋势与原因。

Result: 2021年夏至2023年夏离职率显著上升，特别是入职不足五年的员工；疫情期间远程入职者即使返岗，三年内离职概率仍更高；离职调查指出远程入职难以建立组织归属感；公司通过差异化办公政策成功恢复至疫情前留任水平。

Conclusion: 在知识密集型企业中，混合办公模式需以组织归属感和导师制为核心，新员工返岗时应确保团队成员和资深员工同步在岗，以促进有效融入并提升留任率。

Abstract: The COVID-19 pandemic has permanently altered workplace structures,
normalizing remote work. However, critical evidence highlights challenges with
fully remote arrangements, particularly for software teams. This study
investigates employee resignation patterns at Ericsson, a global developer of
software-intensive systems, before, during, and after the pandemic. Using HR
data from 2016-2025 in Ericsson Sweden, we analyze how different work
modalities (onsite, remote, and hybrid) influence employee retention. Our
findings show a marked increase in resignations from summer 2021 to summer
2023, especially among employees with less than five years of tenure. Employees
onboarded remotely during the pandemic were significantly more likely to resign
within their first three years, even after returning to the office. Exit
surveys suggest that remote onboarding may fail to establish the necessary
organizational attachment, the feeling of belonging and long-term retention. By
contrast, the company's eventual successful return to pre-pandemic retention
rates illustrates the value of differentiated work policies and supports
reconsidering selective return-to-office (RTO) mandates. Our study demonstrates
the importance of employee integration practices in hybrid environments where
the requirement for in-office presence for recent hires shall be accompanied by
in-office presence from their team members and more senior staff whose
mentoring and social interactions contribute to integration into the corporate
work environment. We hope these actionable insights will inform HR leaders and
policymakers in shaping post-pandemic work practices, demonstrating that
carefully crafted hybrid models anchored in organizational attachment and
mentorship can sustain retention in knowledge-intensive companies.

</details>


### [12] [Extending ResourceLink: Patterns for Large Dataset Processing in MCP Applications](https://arxiv.org/abs/2510.05968)
*Scott Frees*

Main category: cs.SE

TL;DR: 本文提出了一种适用于大语言模型（LLM）驱动的报表系统的架构模式，通过将查询生成与数据检索解耦，解决上下文窗口限制问题，并引入支持迭代查询优化和带外数据访问的双响应模式，同时涵盖多租户安全与资源生命周期管理。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在将自然语言转换为数据库查询时受限于上下文窗口，无法直接用于完整数据集的报表系统；同时，尽管Model Context Protocol定义了ResourceLink机制，但缺乏可扩展报表架构的实用实现模式。

Method: 提出一种双响应模式，扩展ResourceLink以支持迭代查询优化和带外数据访问，并设计多租户安全与资源生命周期管理的配套模式。

Result: 所提出的模式有效解决了LLM驱动报表系统中的核心挑战，为开发者提供了可落地的架构指导。

Conclusion: 通过解耦查询生成与数据检索，并结合双响应、安全与资源管理模式，本文为构建可扩展、安全的LLM驱动报表系统提供了实用解决方案。

Abstract: Large language models translate natural language into database queries, yet
context window limitations prevent direct deployment in reporting systems where
complete datasets exhaust available tokens. The Model Context Protocol
specification defines ResourceLink for referencing external resources, but
practical patterns for implementing scalable reporting architectures remain
undocumented. This paper presents patterns for building LLM-powered reporting
systems that decouple query generation from data retrieval. We introduce a
dual-response pattern extending ResourceLink to support both iterative query
refinement and out-of-band data access, accompanied by patterns for
multi-tenant security and resource lifecycle management. These patterns address
fundamental challenges in LLM-driven reporting applications and provide
practical guidance for developers building them.

</details>


### [13] [Prompting in Practice: Investigating Software Developers' Use of Generative AI Tools](https://arxiv.org/abs/2510.06000)
*Daniel Otten,Trevor Stalnaker,Nathan Wintersgill,Oscar Chaparro,Denys Poshyvanyk*

Main category: cs.SE

TL;DR: 本研究通过一项针对91名软件工程师（其中72名为GenAI活跃用户）的大规模调查，系统分析了生成式人工智能（GenAI）在软件开发中的实际应用方式，揭示了开发者在提示策略、对话模式和任务可靠性评估等方面的实践模式。


<details>
  <summary>Details</summary>
Motivation: 尽管提示工程已成为关键技能，但现有研究多聚焦于个别技术，缺乏对软件开发者如何在整体工作流中整合GenAI工具的系统理解。

Method: 通过大规模问卷调查，收集91名软件工程师（含72名GenAI活跃用户）在各类软件工程任务中使用GenAI的提示策略、对话模式和可靠性评估数据，并进行系统分析。

Result: 研究得出14项关键发现：代码生成几乎普遍存在；熟练开发者更倾向于将AI用于调试和代码审查等复杂任务；开发者偏好多轮迭代对话而非单次提示；文档任务被认为最可靠，而复杂代码生成与调试仍具挑战性。

Conclusion: 该研究为当前开发者使用GenAI的实践提供了实证基线，涵盖从简单代码生成到深度工作流整合的多个层面，并为未来工具改进提供了可操作的见解。

Abstract: The integration of generative artificial intelligence (GenAI) tools has
fundamentally transformed software development. Although prompt engineering has
emerged as a critical skill, existing research focuses primarily on individual
techniques rather than software developers' broader workflows. This study
presents a systematic investigation of how software engineers integrate GenAI
tools into their professional practice through a large-scale survey examining
prompting strategies, conversation patterns, and reliability assessments across
various software engineering tasks.
  We surveyed 91 software engineers, including 72 active GenAI users, to
understand AI usage patterns throughout the development process. Our 14 key
findings show that while code generation is nearly universal, proficiency
strongly correlates with using AI for more nuanced tasks such as debugging and
code review, and that developers prefer iterative multi-turn conversations to
single-shot prompting. Documentation tasks are perceived as most reliable,
while complex code generation and debugging present sizable challenges. Our
insights provide an empirical baseline of current developer practices, from
simple code generation to deeper workflow integration, with actionable insights
for future improvements.

</details>


### [14] [Explaining Code Risk in OSS: Towards LLM-Generated Fault Prediction Interpretations](https://arxiv.org/abs/2510.06104)
*Elijah Kayode Adejumo,Brittany Johnson*

Main category: cs.SE

TL;DR: 本文探讨利用大语言模型（LLMs）将静态分析和缺陷预测工具生成的复杂指标转化为清晰、可操作的风险解释，以帮助开源软件（OSS）贡献者更安全地进行代码修改。


<details>
  <summary>Details</summary>
Motivation: 开源软件依赖多样背景的开发者，但安全地修改代码（如修复bug或添加功能）在高度耦合的面向对象系统中具有挑战性；现有缺陷预测工具提供的指标难以被不熟悉代码库的贡献者理解。

Method: 提出利用大语言模型将故障预测指标转化为三类人类可读的解释：描述性、上下文性和可操作性解释，并计划通过任务型用户研究评估其有效性。

Result: 尚未报告具体实验结果，但提出了LLM辅助解释的框架和未来通过对比实验（仅指标 vs. LLM解释）评估其在决策质量、完成时间和错误率方面的有效性。

Conclusion: LLMs有潜力通过提供清晰的风险解释和可操作建议，降低开源贡献者理解和使用缺陷预测指标的门槛，从而提升代码修改的安全性和效率。

Abstract: Open Source Software (OSS) has become a very important and crucial
infrastructure worldwide because of the value it provides. OSS typically
depends on contributions from developers across diverse backgrounds and levels
of experience. Making safe changes, such as fixing a bug or implementing a new
feature, can be challenging, especially in object-oriented systems where
components are interdependent. Static analysis and defect-prediction tools
produce metrics (e.g., complexity,coupling) that flag potentially fault-prone
components, but these signals are often hard for contributors new or unfamiliar
with the codebase to interpret. Large Language Models (LLMs) have shown strong
performance on software engineering tasks such as code summarization and
documentation generation. Building on this progress, we investigate whether
LLMs can translate fault-prediction metrics into clear, human-readable risk
explanations and actionable guidance to help OSS contributors plan and review
code modifications. We outline explanation types that an LLM-generated
assistant could provide (descriptive, contextual, and actionable explanations).
We also outline our next steps to assess usefulness through a task-based study
with OSS contributors, comparing metric-only baselines to LLM-generated
explanations on decision quality, time-to-completion, and error rates

</details>


### [15] [Automated Program Repair of Uncompilable Student Code](https://arxiv.org/abs/2510.06187)
*Griffin Pitts,Aum Pandya,Darsh Rank,Tirth Bhatt,Muntasir Hoq,Bita Akram*

Main category: cs.SE

TL;DR: 本文研究利用大语言模型（LLMs）自动修复CS1课程中学生提交的不可编译代码，在保证修复后代码可编译的同时尽可能保留学生原始代码的结构与逻辑，以支持更全面的学生建模和知识追踪。


<details>
  <summary>Details</summary>
Motivation: 大量学生在CS1学习环境中提交的代码无法编译，传统学生建模方法通常直接丢弃这些数据，导致丢失重要的学习过程信息。因此，亟需一种方法在保留学生编程意图的前提下修复这些代码。

Method: 评估GPT-5、Claude 3.5 Haiku和Gemini 2.5 Flash三种大语言模型在高/低上下文提示条件下对不可编译代码的自动修复能力，从可编译性、编辑距离及原始结构逻辑保留程度三个方面进行衡量。

Result: 三种LLM均能生成可编译的修复代码，但在保留学生控制流和代码结构方面表现不同，影响其在教学中的实用性。

Conclusion: 通过自动修复不可编译代码，本研究为更丰富、全面地分析学生编程过程和能力发展提供了可能。

Abstract: A significant portion of student programming submissions in CS1 learning
environments are uncompilable, limiting their use in student modeling and
downstream knowledge tracing. Traditional modeling pipelines often exclude
these cases, discarding observations of student learning. This study
investigates automated program repair as a strategy to recover uncompilable
code while preserving students' structural intent for use in student modeling.
Within this framework, we assess large language models (LLMs) as repair agents,
including GPT-5 (OpenAI), Claude 3.5 Haiku (Anthropic), and Gemini 2.5 Flash
(Google), under high- and low-context prompting conditions. Repairs were
evaluated for compilability, edit distance, and preservation of students'
original structure and logic. We find that while all three LLMs are capable of
producing compilable repairs, their behavior diverges in how well they preserve
students' control flow and code structure, which affects their pedagogical
utility. By recovering uncompilable submissions, this work enables richer and
more comprehensive analyses of learners' coding processes and development over
time.

</details>


<div id='cs.PF'></div>

# cs.PF [[Back]](#toc)

### [16] [Exploring and Evaluating Real-world CXL: Use Cases and System Adoption](https://arxiv.org/abs/2405.14209)
*Xi Wang,Jie Liu,Jianbo Wu,Shuangyan Yang,Jie Ren,Bhanu Shankar,Dong Li*

Main category: cs.PF

TL;DR: 本文通过实测三种不同厂商的CXL内存扩展卡，系统评估了CXL内存的基本性能、对HPC应用和大语言模型的性能影响，并提出了匹配内存访问模式的对象级交错策略。


<details>
  <summary>Details</summary>
Motivation: 由于CXL硬件尚未广泛可用，其性能特征和适用场景尚不明确，亟需实证研究以指导实际部署和优化。

Method: 对三个厂商的CXL内存扩展卡进行实测，分析其基础性能，评估其在高性能计算（HPC）和大语言模型（LLM）中的应用效果，并研究内存分层与页面交错策略；提出一种新的数据对象级交错策略。

Result: 揭示了CXL内存使用中的挑战与机遇，验证了其在特定负载下的性能潜力，并展示了所提对象级交错策略的有效性。

Conclusion: CXL内存具有应用潜力，但需结合访问模式进行精细化管理；合理的内存层级与交错策略可显著提升性能。

Abstract: Compute eXpress Link (CXL) is emerging as a promising memory interface
technology. However, its performance characteristics remain largely unclear due
to the limited availability of production hardware. Key questions include: What
are the use cases for the CXL memory? What are the impacts of the CXL memory on
application performance? How to use the CXL memory in combination with existing
memory components? In this work, we study the performance of three genuine CXL
memory-expansion cards from different vendors. We characterize the basic
performance of the CXL memory, study how HPC applications and large language
models (LLM) can benefit from the CXL memory, and study the interplay between
memory tiering and page interleaving. We also propose a novel data object-level
interleaving policy to match the interleaving policy with memory access
patterns. Our findings reveal the challenges and opportunities of using the CXL
memory.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [17] [Stratum: System-Hardware Co-Design with Tiered Monolithic 3D-Stackable DRAM for Efficient MoE Serving](https://arxiv.org/abs/2510.05245)
*Yue Pan,Zihan Xia,Po-Kai Hsu,Lanxiang Hu,Hyungyo Kim,Janak Sharda,Minxuan Zhou,Nam Sung Kim,Shimeng Yu,Tajana Rosing,Mingu Kang*

Main category: cs.AR

TL;DR: 本文提出Stratum，一种结合新型单片3D堆叠DRAM（Mono3D DRAM）、近存计算（NMP）和GPU加速的系统-硬件协同设计，用于高效部署稀疏激活的Mixture of Experts（MoE）大语言模型，显著提升推理吞吐量和能效。


<details>
  <summary>Details</summary>
Motivation: MoE架构虽能实现大规模参数量与较低推理成本，但其引入的大量数据对硬件部署构成挑战，尤其在内存带宽和能效方面。

Method: 采用Mono3D DRAM与近存计算结合GPU的异构架构，通过混合键合连接逻辑与存储芯片，并利用硅中介层连接DRAM堆栈与GPU；同时基于主题预测专家使用情况，对Mono3D DRAM内部进行分层数据放置以优化访问延迟和NMP吞吐。

Result: Stratum系统在多个基准测试中相比纯GPU基线实现了最高8.29倍的解码吞吐量提升和7.66倍的能效提升。

Conclusion: 通过系统-硬件协同设计，Stratum有效解决了MoE模型部署中的内存瓶颈问题，为高效推理提供了可行路径。

Abstract: As Large Language Models (LLMs) continue to evolve, Mixture of Experts (MoE)
architecture has emerged as a prevailing design for achieving state-of-the-art
performance across a wide range of tasks. MoE models use sparse gating to
activate only a handful of expert sub-networks per input, achieving
billion-parameter capacity with inference costs akin to much smaller models.
However, such models often pose challenges for hardware deployment due to the
massive data volume introduced by the MoE layers. To address the challenges of
serving MoE models, we propose Stratum, a system-hardware co-design approach
that combines the novel memory technology Monolithic 3D-Stackable DRAM (Mono3D
DRAM), near-memory processing (NMP), and GPU acceleration. The logic and Mono3D
DRAM dies are connected through hybrid bonding, whereas the Mono3D DRAM stack
and GPU are interconnected via silicon interposer. Mono3D DRAM offers higher
internal bandwidth than HBM thanks to the dense vertical interconnect pitch
enabled by its monolithic structure, which supports implementations of
higher-performance near-memory processing. Furthermore, we tackle the latency
differences introduced by aggressive vertical scaling of Mono3D DRAM along the
z-dimension by constructing internal memory tiers and assigning data across
layers based on access likelihood, guided by topic-based expert usage
prediction to boost NMP throughput. The Stratum system achieves up to 8.29x
improvement in decoding throughput and 7.66x better energy efficiency across
various benchmarks compared to GPU baselines.

</details>


### [18] [DeepV: A Model-Agnostic Retrieval-Augmented Framework for Verilog Code Generation with a High-Quality Knowledge Base](https://arxiv.org/abs/2510.05327)
*Zahin Ibnat,Paul E. Calzada,Rasin Mohammed Ihtemam,Sujan Kumar Saha,Jingbo Zhou,Farimah Farahmandi,Mark Tehranipoor*

Main category: cs.AR

TL;DR: 本文提出了DeepV，一种与模型无关的检索增强生成（RAG）框架，用于在无需RTL特定训练的情况下，利用高质量数据集提升寄存器传输级（RTL）代码生成性能，并在VerilogEval基准上使GPT-5性能提升近17%。


<details>
  <summary>Details</summary>
Motivation: 现有基于大语言模型（LLM）的RTL代码生成方法难以整合新型IP到模型知识库中，导致生成代码质量差；同时，针对旧模型的微调方法难以跟上通用LLM的进展。现有RAG方法存在使用低质量代码库、计算开销大或未直接用于RTL生成等问题。

Method: 提出DeepV框架，一种模型无关的RAG方法，通过大规模高质量数据集增强上下文，在不进行RTL特定训练的前提下辅助LLM生成RTL代码。

Result: 在VerilogEval基准测试中，DeepV使OpenAI的GPT-5性能提升近17%，并已在Hugging Face平台开源。

Conclusion: DeepV有效解决了现有RTL生成方法在知识更新和模型适配方面的局限性，为硬件设计自动化提供了一种高效、可扩展的解决方案。

Abstract: As large language models (LLMs) continue to be integrated into modern
technology, there has been an increased push towards code generation
applications, which also naturally extends to hardware design automation.
LLM-based solutions for register transfer level (RTL) code generation for
intellectual property (IP) designs have grown, especially with fine-tuned LLMs,
prompt engineering, and agentic approaches becoming popular in literature.
However, a gap has been exposed in these techniques, as they fail to integrate
novel IPs into the model's knowledge base, subsequently resulting in poorly
generated code. Additionally, as general-purpose LLMs continue to improve,
fine-tuned methods on older models will not be able to compete to produce more
accurate and efficient designs. Although some retrieval augmented generation
(RAG) techniques exist to mitigate challenges presented in fine-tuning
approaches, works tend to leverage low-quality codebases, incorporate
computationally expensive fine-tuning in the frameworks, or do not use RAG
directly in the RTL generation step. In this work, we introduce DeepV: a
model-agnostic RAG framework to generate RTL designs by enhancing context
through a large, high-quality dataset without any RTL-specific training. Our
framework benefits the latest commercial LLM, OpenAI's GPT-5, with a near 17%
increase in performance on the VerilogEval benchmark. We host DeepV for use by
the community in a Hugging Face (HF) Space:
https://huggingface.co/spaces/FICS-LLM/DeepV.

</details>


### [19] [From Principles to Practice: A Systematic Study of LLM Serving on Multi-core NPUs](https://arxiv.org/abs/2510.05632)
*Tianhao Zhu,Dahu Feng,Erhu Feng,Yubin Xia*

Main category: cs.AR

TL;DR: 本文针对多核NPU上大语言模型（LLM）推理性能优化问题，提出了一种多层次仿真框架，并系统分析了张量并行策略、核放置策略、内存管理方法及PD拆分/融合选择，实验表明其方案相比现有最优设计可实现1.32x至6.03x的加速。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型的广泛应用，对高性能推理服务的需求不断增长。当前多数AI加速器采用多核架构但缺乏SIMT架构的灵活性，若硬件配置与并行策略设计不当，易导致计算资源利用率低、推理性能不佳。

Method: 提出一个结合事务级与性能模型的多级仿真框架，用于多核NPU；基于该框架系统评估并优化张量并行策略、核放置策略、内存管理方法以及PD-disaggregation与PD-fusion的选择。

Result: 在多种LLM和NPU配置下实验表明，所提方案相比当前最先进的多核NPU设计可实现1.32倍至6.03倍的推理加速。

Conclusion: 该工作为在多核NPU上针对不同LLM负载设计最优硬件架构和推理服务策略提供了有效指导。

Abstract: With the widespread adoption of Large Language Models (LLMs), the demand for
high-performance LLM inference services continues to grow. To meet this demand,
a growing number of AI accelerators have been proposed, such as Google TPU,
Huawei NPU, Graphcore IPU, and Cerebras WSE, etc. Most of these accelerators
adopt multi-core architectures to achieve enhanced scalability, but lack the
flexibility of SIMT architectures. Therefore, without careful configuration of
the hardware architecture, as well as deliberate design of tensor parallelism
and core placement strategies, computational resources may be underutilized,
resulting in suboptimal inference performance.
  To address these challenges, we first present a multi-level simulation
framework with both transaction-level and performance-model-based simulation
for multi-core NPUs. Using this simulator, we conduct a systematic analysis and
further propose the optimal solutions for tensor parallelism strategies, core
placement policies, memory management methods, as well as the selection between
PD-disaggregation and PD-fusion on multi-core NPUs. We conduct comprehensive
experiments on representative LLMs and various NPU configurations. The
evaluation results demonstrate that, our solution can achieve 1.32x-6.03x
speedup compared to SOTA designs for multi-core NPUs across different hardware
configurations. As for LLM serving, our work offers guidance on designing
optimal hardware architectures and serving strategies for multi-core NPUs
across various LLM workloads.

</details>


### [20] [An opportunity to improve Data Center Efficiency: Optimizing the Server's Upgrade Cycle](https://arxiv.org/abs/2510.05787)
*Panagiota Nikolaou,Freddy Gabbay,Jawad Haj-Yahya,Yiannakis Sazeides*

Main category: cs.AR

TL;DR: 本文通过优化数据中心服务器的全局升级计划，在设计阶段制定覆盖整个生命周期的策略，以提升QPS/(TCO×CO2)指标，优于传统的固定周期局部升级方案。


<details>
  <summary>Details</summary>
Motivation: 现有服务器升级策略多采用固定周期的局部决策，忽视未来服务器型号信息，无法实现整体能效与成本的最优平衡；本文旨在通过全局规划挖掘提升数据中心效率的潜力。

Method: 基于历史服务器数据，构建涵盖服务器上线年份、性能和功耗等信息的全局升级模型，在数据中心设计阶段制定覆盖全生命周期的非固定周期升级计划。

Result: 研究发现，最优全局升级计划通常采用非固定时间间隔，相比仅基于当前可用机型、按固定周期升级的局部策略，能显著提升QPS/(TCO×CO2)指标。

Conclusion: 在数据中心设计初期制定考虑未来服务器演进的全局升级计划，可有效提升能效与经济性综合指标，优于传统局部、固定周期的升级方法。

Abstract: This work aims to improve a data center's efficiency by optimizing the server
upgrade plan: determine the optimal timing for replacing old servers with new
ones. The opportunity presented by this approach is demonstrated through a
study based on historical server data. The study establishes a significant
opportunity to increase the QPS/(TCOxCO2) metric by formulating a global
upgrade plan at the data center's design time covering its entire life cycle.
This plan leverages information, such as server entry year, performance, and
active power consumption for both existing and future servers. Our findings
reveal that an optimal global upgrade plan, may involve upgrades at non fixed
time periods and outperforms local upgrade plans. Local upgrade plans follow a
fixed, equal-length cycle and make decisions based only on currently available
server models. These local plans select the best available server at each
upgrade cycle without accounting for future server releases.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [21] [Rivaling Transformers: Multi-Scale Structured State-Space Mixtures for Agentic 6G O-RAN](https://arxiv.org/abs/2510.05255)
*Farhad Rezazadeh,Hatim Chergui,Merouane Debbah,Houbing Song,Dusit Niyato,Lingjia Liu*

Main category: cs.NI

TL;DR: 本文提出了一种轻量级多尺度结构化状态空间混合模型（MS3M），用于在6G O-RAN近实时约束下高效预测用户设备的关键性能指标（KPI），在保持准确率的同时显著降低推理延迟和模型复杂度。


<details>
  <summary>Details</summary>
Motivation: 在6G O-RAN中，实现近实时、低开销的控制级预测面临多时间尺度动态、延迟和计算资源限制等挑战，而传统Transformer模型因内存消耗大难以满足这些要求。

Method: 提出MS3M模型，融合HiPPO-LegS核以捕捉多尺度无线动态；采用双线性离散化构建稳定的状态空间模型，并将其因果脉冲响应作为深度可分离卷积；结合Squeeze-and-Excitation门控机制动态调整KPI通道权重，并通过紧凑的门控通道混合层建模跨特征非线性。

Result: 在自建O-RAN测试平台数据集（13个KPI，59,441个滑动窗口）上评估，MS3M推理延迟仅为0.057秒，参数量0.70M，在相同硬件上比Transformer基线快3–10倍，同时保持有竞争力的预测精度。

Conclusion: MS3M是一种适用于O-RAN近实时智能控制器的高效、轻量且KPI无关的预测模型，能有效支持前瞻性无线资源控制。

Abstract: In sixth-generation (6G) Open Radio Access Networks (O-RAN), proactive
control is preferable. A key open challenge is delivering control-grade
predictions within Near-Real-Time (Near-RT) latency and computational
constraints under multi-timescale dynamics. We therefore cast RAN Intelligent
Controller (RIC) analytics as an agentic perceive-predict xApp that turns
noisy, multivariate RAN telemetry into short-horizon per-User Equipment (UE)
key performance indicator (KPI) forecasts to drive anticipatory control. In
this regard, Transformers are powerful for sequence learning and time-series
forecasting, but they are memory-intensive, which limits Near-RT RIC use.
Therefore, we need models that maintain accuracy while reducing latency and
data movement. To this end, we propose a lightweight Multi-Scale Structured
State-Space Mixtures (MS3M) forecaster that mixes HiPPO-LegS kernels to capture
multi-timescale radio dynamics. We develop stable discrete state-space models
(SSMs) via bilinear (Tustin) discretization and apply their causal impulse
responses as per-feature depthwise convolutions. Squeeze-and-Excitation gating
dynamically reweights KPI channels as conditions change, and a compact gated
channel-mixing layer models cross-feature nonlinearities without
Transformer-level cost. The model is KPI-agnostic -- Reference Signal Received
Power (RSRP) serves as a canonical use case -- and is trained on sliding
windows to predict the immediate next step. Empirical evaluations conducted
using our bespoke O-RAN testbed KPI time-series dataset (59,441 windows across
13 KPIs). Crucially for O-RAN constraints, MS3M achieves a 0.057 s
per-inference latency with 0.70M parameters, yielding 3-10x lower latency than
the Transformer baselines evaluated on the same hardware, while maintaining
competitive accuracy.

</details>


### [22] [Impact of Packet Loss and Timing Errors on Scheduled Periodic Traffic with Time-Aware Shaping (TAS) in Time-Sensitive Networking (TSN)](https://arxiv.org/abs/2510.05290)
*Manuel Eppler,Steffen Lindner,Lukas Osswald,Thomas Stüber,Michael Menth*

Main category: cs.NI

TL;DR: 本文指出在时间敏感网络（TSN）中，若对高优先级流量的传输时隙进行严格限制，故障帧（如过早、过晚、缺失或多余帧）可能导致严重排队延迟甚至丢包，并通过示例和仿真验证该问题，提出延长或不限制高优先级时隙可缓解此问题。


<details>
  <summary>Details</summary>
Motivation: 现有TSN调度算法通常假设高优先级流量的传输时隙也受TAS限制，但这种假设在出现故障帧时可能导致严重性能问题，如延迟和丢包，因此需要重新审视该假设并提出改进方法。

Method: 通过构建最小化示例说明故障帧在单链路上的影响，并通过仿真展示其在网络中的传播效应；同时评估延长或不限制高优先级TAS时隙对缓解问题的效果。

Result: 仿真结果表明，即使单个轻微延迟的帧也可能在多个链路上引发丢包；而延长或不限制高优先级流量的TAS时隙可有效缓解或避免此类问题。

Conclusion: TSN中对高优先级流量严格限制TAS时隙的做法存在隐患，应考虑配置更宽松或不限制的时隙以提升系统鲁棒性。

Abstract: Time-Sensitive Networking (TSN) is a collection of mechanisms to enhance the
realtime transmission capability of Ethernet networks. TSN combines priority
queuing, traffic scheduling, and the Time-Aware Shaper (TAS) to carry periodic
traffic with ultra-low latency and jitter. That is, so-called Talkers send
periodic traffic with highest priority according to a schedule. The schedule is
designed such that the scheduled traffic is forwarded by the TSN bridges with
no or only little queuing delay. To protect that traffic against other frames,
the TAS is configured on all interfaces such that lower-priority queues can
send only when high-priority traffic is not supposed to be forwarded. In the
literature on scheduling algorithms for the TAS there is mostly the explicit or
implicit assumption that the TAS also limits transmission slots of
high-priority traffic.
  In this paper we show that this assumption can lead to tremendous problems
like very long queuing delay or even packet loss in case of faulty frames. A
faulty frame arrives too early or too late according to the schedule, it is
missing or additional. We construct minimal examples to illustrate basic
effects of faulty frames on a single link and demonstrate how this effect can
propagate through the networks and cause remote problems. We further show using
simulations that a single slightly delayed frame may lead to frame loss on
multiple links. We show that these problems can be alleviated or avoided when
TAS-based transmission slots for high-priority traffic are configured longer
than needed or if they are not limited at all.

</details>


### [23] [Generative AI-Driven Hierarchical Multi-Agent Framework for Zero-Touch Optical Networks](https://arxiv.org/abs/2510.05625)
*Yao Zhang,Yuchen Song,Shengnan Li,Yan Shi,Shikui Shen,Xiongyan Tang,Min Zhang,Danshi Wang*

Main category: cs.NI

TL;DR: 本文提出了一种基于生成式人工智能（GenAI）的分层多智能体框架，用于实现零接触光网络中的多任务自主执行，涵盖规划、运行和升级阶段的典型应用场景。


<details>
  <summary>Details</summary>
Motivation: 现有单智能体GenAI系统难以应对光网络全生命周期中多任务、跨层协作的复杂需求，亟需更高效、协同的智能管理方案以实现零接触光网络。

Method: 设计并实现了一个GenAI驱动的分层多智能体框架，通过多智能体协作完成光网络生命周期中的多任务自主执行，并在实际部署的网状网络中验证其在传输质量估计、动态信道增删和系统扩容等场景中的应用。

Result: 案例研究表明，该多智能体框架在多任务分配、协调、执行、评估与总结方面展现出良好能力，有效支持光网络的智能化与自适应管理。

Conclusion: 该框架为未来智能、高效、协同的零接触光网络管理提供了可行路径，推动光网络向更专业化和自适应方向发展。

Abstract: The rapid development of Generative Artificial Intelligence (GenAI) has
catalyzed a transformative technological revolution across all walks of life.
As the backbone of wideband communication, optical networks are expecting
high-level autonomous operation and zero-touch management to accommodate their
expanding network scales and escalating transmission bandwidth. The integration
of GenAI is deemed as the pivotal solution for realizing zero-touch optical
networks. However, the lifecycle management of optical networks involves a
multitude of tasks and necessitates seamless collaboration across multiple
layers, which poses significant challenges to the existing single-agent GenAI
systems. In this paper, we propose a GenAI-driven hierarchical multi-agent
framework designed to streamline multi-task autonomous execution for zero-touch
optical networks. We present the architecture, implementation, and applications
of this framework. A field-deployed mesh network is utilized to demonstrate
three typical scenarios throughout the lifecycle of optical network: quality of
transmission estimation in the planning stage, dynamic channel adding/dropping
in the operation stage, and system capacity increase in the upgrade stage. The
case studies, illustrate the capabilities of multi-agent framework in
multi-task allocation, coordination, execution, evaluation, and summarization.
This work provides a promising approach for the future development of
intelligent, efficient, and collaborative network management solutions, paving
the way for more specialized and adaptive zero-touch optical networks.

</details>


### [24] [On Enhancing Delay SLAs in TCP Networks through Joint Routing and Transport Assistant Deployment](https://arxiv.org/abs/2510.05686)
*José Gómez-delaHiz,Mohamed Faten Zhani,Jaime Galán-Jiménez,John Kaippallimalil*

Main category: cs.NI

TL;DR: 本文提出联合优化流路由与传输辅助器（TA）部署，以降低尽力而为网络中的数据包传输延迟或满足QoS网络中的延迟SLA，并通过整数线性规划建模及启发式算法求解，在降低延迟（最多16.4%）的同时显著减少部署成本（最多60.98%）。


<details>
  <summary>Details</summary>
Motivation: TCP因重传机制在丢包时导致高传输延迟，已有研究引入网络内缓存与重传的传输辅助器（TA）来缓解此问题；然而，如何有效部署TA并结合路由优化以最小化延迟或满足SLA仍需研究。

Method: 将联合路由与TA部署问题建模为两种场景下的整数线性规划（ILP），并为大规模实例设计启发式算法。

Result: 仿真实验表明，该方法在尽力而为网络中最多降低16.4%的数据包交付延迟，在QoS网络中有效满足延迟SLA，同时部署成本最多减少60.98%。

Conclusion: 联合优化流路由与TA部署能显著降低TCP传输延迟并节省部署成本，适用于不同网络场景，具有实际应用价值。

Abstract: The Transport Control Protocol has long been the primary transport protocol
for applications requiring performance and reliability over the Internet.
Unfortunately, due its retransmission mechanism, TCP incurs high packet
delivery delays when segments are lost. To address this issue, previous
research proposed to use a novel network function, namely Transport Assistant,
deployed within the network to cache and retransmit lost packets, thus reducing
retransmission delays. In this paper, we propose to jointly route the flows and
deploy TAs in order to minimize packet delivery delays in best-effort networks
(scenario 1) or to satisfy delay-based Service Level Agreements in QoS-based
networks (scenario 2). We hence formulate the joint routing and TA deployment
problem as Integer Linear Program for the two scenarios and propose a heuristic
solution for large-scale instances of the problem. Through extensive
simulations, we demonstrate the benefits of performing joint routing flows and
TA deployment in reducing packet delivery delays (up to 16.4%) while minimizing
deployment costs (up to 60.98%).

</details>


### [25] [A Deep Q-Network based power control mechanism to Minimize RLF driven Handover Failure in 5G Network](https://arxiv.org/abs/2510.05762)
*Kotha Kartheek,Shankar K. Ghosh,Megha Iyengar,Vinod Sharma,Souvik Deb*

Main category: cs.NI

TL;DR: 本文提出一种基于深度Q网络（DQN）的功率控制机制，通过结合切换参数和无线链路监测参数，动态调整发射功率以减少无线链路失败（RLF）及其引发的切换失败（HF），仿真表明该方法优于现有技术。


<details>
  <summary>Details</summary>
Motivation: 现有切换算法设计中普遍忽视了无线链路失败（RLF）的影响，而RLF是导致切换失败（HF）的主要原因之一，特别是在切换过程中发生RLF时。

Method: 提出一种基于深度强化学习（DRL）的功率控制算法，利用深度Q网络（DQN），以切换参数（如准备时间、执行时间、偏移量）和无线链路监控参数（T310、N310）作为输入，动态决策是否提升发射功率以避免RLF引发的HF。

Result: 仿真实验表明，将所提DRL功率控制算法与传统条件切换机制结合，能显著降低RLF和由此导致的HF，性能优于当前最先进的方法。

Conclusion: 将DRL驱动的功率控制机制融入切换流程，可有效缓解RLF问题并提升切换可靠性，为未来无线网络切换算法设计提供了新思路。

Abstract: The impact of Radio link failure (RLF) has been largely ignored in designing
handover algorithms, although RLF is a major contributor towards causing
handover failure (HF). RLF can cause HF if it is detected during an ongoing
handover. The objective of this work is to propose an efficient power control
mechanism based on Deep Q-Network (DQN), considering handover parameters (i.e.,
time-to-preparation, time-to-execute, preparation offset, execution offset) and
radio link monitoring parameters (T310 and N310) as input. The proposed DRL
based power control algorithm decides on a possible increase of transmitting
power to avoid RLF driven HF. Simulation results show that the traditional
conditional handover, when equipped with the proposed DRL based power control
algorithm can significantly reduce both RLFs and subsequent HFs, as compared to
the existing state of the art approaches.

</details>


### [26] [Leveraging Generative AI for large-scale prediction-based networking](https://arxiv.org/abs/2510.05797)
*Mathias Thorsager,Israel Leyva-Mayorga,Petar Popovski*

Main category: cs.NI

TL;DR: 本文提出利用生成式人工智能（GenAI）重构网络层，从传统的端到端路由转向基于预测的网络层，以突破吞吐量限制并控制延迟，在图像内容实时传输中已实现超过100%的性能提升，并探讨了在大规模多模态场景下部署GenAI网络节点的三个关键方向。


<details>
  <summary>Details</summary>
Motivation: 传统网络层在吞吐量和延迟控制方面存在局限，尤其在实时图像传输等场景下难以满足需求。作者希望通过引入生成式人工智能（GenAI）重构网络层功能，以解决这些问题并提升网络性能。

Method: 文章提出将GenAI集成到网络节点中，构建基于预测的网络层；并从三个方面支持其大规模部署：设计高效的提示（prompt）初始化协议、利用GenAI保障数据的及时交付、以及将其作为传统TCP拥塞控制算法的替代方案。

Result: 在图像内容实时传输场景中，采用GenAI辅助的网络节点可使到达目的地的数据流性能提升超过100%。

Conclusion: GenAI有潜力彻底改变网络层的设计范式，但需解决大规模部署中的提示初始化、多模态支持和传输控制等关键问题，才能实现其作为网络层工具的广泛应用。

Abstract: The traditional role of the network layer is to create an end-to-end route,
through which the intermediate nodes replicate and forward the packets towards
the destination. This role can be radically redefined by exploiting the power
of Generative AI (GenAI) to pivot towards a prediction-based network layer,
which addresses the problems of throughput limits and uncontrollable latency.
In the context of real-time delivery of image content, the use of GenAI-aided
network nodes has been shown to improve the flow arriving at the destination by
more than 100%. However, to successfully exploit GenAI nodes and achieve such
transition, we must provide solutions for the problems which arise as we scale
the networks to include large amounts of users and multiple data modalities
other than images. We present three directions that play a significant role in
enabling the use of GenAI as a network layer tool at a large scale. In terms of
design, we emphasize the need for initialization protocols to select the prompt
size efficiently. Next, we consider the use case of GenAI as a tool to ensure
timely delivery of data, as well as an alternative to traditional TCP
congestion control algorithms.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [27] [Toward Systems Foundations for Agentic Exploration](https://arxiv.org/abs/2510.05556)
*Jiakai Xu,Tianle Zhou,Eugene Wu,Kostis Kaffes*

Main category: cs.DC

TL;DR: 本文指出当前通用快照/恢复机制（如CRIU）无法满足大语言模型智能体在真实环境中高效分支与回溯的需求，并提出了三大核心挑战：分支语义、外部副作用处理和微秒级原生分支能力。


<details>
  <summary>Details</summary>
Motivation: 现有系统（如CRIU或容器提交）在支持大语言模型智能体进行多路径探索时性能不足，尤其在真实部署中因共享资源（文件、套接字、云API）而失效，亟需新的系统支持。

Method: 通过评测六种快照/恢复机制，分析其在隔离环境和真实部署中的表现，识别出智能体探索系统所需解决的关键问题。

Result: 发现通用工具在速度和共享环境适应性方面存在严重不足，并明确指出三大开放性挑战：分支语义、外部副作用处理、原生微秒级分支。

Conclusion: 为支持大语言模型智能体的高效探索，必须重新设计系统机制，以应对分支语义、外部副作用和高性能原生分支等根本性挑战。

Abstract: Agentic exploration, letting LLM-powered agents branch, backtrack, and search
across many execution paths, demands systems support well beyond today's
pass-at-k resets. Our benchmark of six snapshot/restore mechanisms shows that
generic tools such as CRIU or container commits are not fast enough even in
isolated testbeds, and they crumble entirely in real deployments where agents
share files, sockets, and cloud APIs with other agents and human users. In this
talk, we pinpoint three open fundamental challenges: fork semantics, which
concerns how branches reveal or hide tentative updates; external side-effects,
where fork awareness must be added to services or their calls intercepted; and
native forking, which requires cloning databases and runtimes in microseconds
without bulk copying.

</details>


### [28] [Tiny but Mighty: A Software-Hardware Co-Design Approach for Efficient Multimodal Inference on Battery-Powered Small Devices](https://arxiv.org/abs/2510.05109)
*Yilong Li,Shuai Zhang,Yijing Zeng,Hao Zhang,Xinmiao Xiong,Jingyu Liu,Pan Hu,Suman Banerjee*

Main category: cs.DC

TL;DR: 本文提出NANOMIND，一种面向大模型的软硬件协同推理框架，通过将多模态大模型拆分为模块化“砖块”并动态调度至最适合的异构加速器上执行，在统一内存SoC上显著提升能效与吞吐量，实现完全端侧运行。


<details>
  <summary>Details</summary>
Motivation: 当前大模型通常以整体方式执行，未能充分利用现代SoC中的异构加速器（如NPU、GPU、DSP），导致高延迟和资源浪费。

Method: 将大模型拆分为视觉、语言、音频等模块化组件，结合定制硬件设计、系统级调度和低比特计算内核，在统一内存SoC上实现模块级动态卸载和加速器映射，并通过token感知的缓冲管理减少CPU瓶颈和内存冗余。

Result: 在电池供电设备上实现完全端侧运行LLaVA-OneVision和LLaMA-3-8B，分别支持近半天的摄像头推理和20.8小时的语音交互；相比现有方案，能耗降低42.3%，GPU内存使用减少11.2%。

Conclusion: NANOMIND通过模块化调度与软硬件协同设计，显著提升端侧多模态大模型的能效与资源利用率，实现无网络依赖的高性能智能助理。

Abstract: Large Multimodal Models (LMMs) are inherently modular, consisting of vision
and audio encoders, projectors, and large language models. Yet, they are almost
always executed monolithically, which underutilizes the heterogeneous
accelerators (NPUs, GPUs, DSPs) in modern SoCs and leads to high end-to-end
latency. In this paper, we present NANOMIND, a hardware--software co-design
inference framework for Large Multimodal Models (LMMs) that breaks large models
into modular ``bricks'' (vision, language, audio, etc.) and maps each to its
ideal accelerator. The key insight is that large models can be broken into
modular components and scheduled to run on the most appropriate compute units.
It performs module-level dynamic offloading across accelerators on
unified-memory SoCs. By combining customized hardware design, system-level
scheduling, and optimized low-bit computation kernels, we demonstrate our
framework with a compact, battery-powered device capable of running LMMs
entirely on device. This prototype functions as a self-contained intelligent
assistant that requires no network connectivity, while achieving higher
throughput and superior power efficiency under strict resource constraints. The
design further bypasses CPU bottlenecks and reduces redundant memory usage
through token-aware buffer management and module-level coordination. Our system
outperforms existing implementations in resource efficiency, cutting energy
consumption by 42.3\% and GPU memory usage by 11.2\%. This enables a
battery-powered device to run LLaVA-OneVision with a camera for nearly half a
day and LLaMA-3-8B for voice interactions up to almost 20.8 hours.

</details>


### [29] [Agora: Bridging the GPU Cloud Resource-Price Disconnect](https://arxiv.org/abs/2510.05111)
*Ian McDougall,Noah Scott,Joon Huh,Kirthevasan Kandasamy,Karthikeyan Sankaralingam*

Main category: cs.DC

TL;DR: 本文指出当前云GPU按时间计费的模式对内存带宽受限任务存在经济低效问题，提出一种基于资源特征（如内存带宽）的新型定价框架，并设计了名为Agora的可实现系统架构，实验证明该方法能显著提升云GPU资源市场的透明度与效率。


<details>
  <summary>Details</summary>
Motivation: 摩尔定律在现代GPU上已出现分化：虽然FLOPs性能持续经济地提升，但内存带宽未同步增长，导致现有基于时间的云GPU计费模式无法反映内存带宽日益增长的边际成本，造成市场扭曲和硬件分配低效。

Method: 提出一种基于资源特征（包括内存带宽等）的定价框架，结合经济学与算法定义，并实现名为Agora的系统架构，通过高频采样（如10–50微秒）精确计量资源使用以支持该定价模型。

Result: 实验表明，50微秒采样仅损失5%收入，10微秒采样损失仅2.4%；现代遥测系统已支持该采样频率，且在多种GPU应用和硬件代际上验证了该方法的有效性。

Conclusion: 基于资源特征的定价框架能更准确反映云GPU资源的真实成本，提升市场效率与透明度，且具备实际部署可行性。

Abstract: The historic trend of Moore's Law, which predicted exponential growth in
computational performance per dollar, has diverged for modern Graphics
Processing Units (GPUs). While Floating Point Operations per Second (FLOPs)
capabilities have continued to scale economically, memory bandwidth has not,
creating a significant price-performance disconnect. This paper argues that the
prevailing time-based pricing models for cloud GPUs are economically
inefficient for bandwidth-bound workloads. These models fail to account for the
rising marginal cost of memory bandwidth, leading to market distortions and
suboptimal hardware allocation. To address this, we propose a novel
feature-based pricing framework that directly links cost to resource
consumption, including but not limited to memory bandwidth. We provide a robust
economic and algorithmic definition of this framework and introduce Agora, a
practical and secure system architecture for its implementation. Our
implementation of Agora shows that a 50us sampling provides nearly perfect
pricing as what ideal sampling would provide - losing only 5\% of revenue. 10us
sampling is even better result in 2.4\% loss. Modern telemetry systems can
already provide this rate of measurement, and our prototype implementation
shows the system design for feature-based pricing is buildable. Our evaluation
across diverse GPU applications and hardware generations empirically validates
the effectiveness of our approach in creating a more transparent and efficient
market for cloud GPU resources.

</details>


### [30] [cMPI: Using CXL Memory Sharing for MPI One-Sided and Two-Sided Inter-Node Communications](https://arxiv.org/abs/2510.05476)
*Xi Wang,Bin Ma,Jongryool Kim,Byungil Koh,Hoshik Kim,Dong Li*

Main category: cs.DC

TL;DR: cMPI 是首个利用 CXL 内存共享优化 MPI 点对点通信的工作，通过将跨节点通信转化为 CXL 内存中的内存事务和数据拷贝，显著降低延迟并提升带宽。


<details>
  <summary>Details</summary>
Motivation: 传统 MPI 库依赖复杂的网络协议栈（如 TCP、RoCE）和互连网络（如以太网、InfiniBand）进行跨节点通信，存在高延迟和低效问题。作者旨在利用 CXL 内存共享机制简化通信路径，提升性能。

Method: cMPI 利用真实 CXL 平台上的 CXL 内存共享机制，将 MPI 的单边和双边点对点通信转化为 CXL 内存中的内存事务和数据拷贝，绕过传统网络协议，并解决 dax 表示下的数据对象管理、缓存一致性和原子操作等挑战。

Result: 在小规模和中等规模集群中，CXL 内存共享相比基于 TCP 的互连可实现 7.2x–8.1x 的延迟降低；对于小消息，cMPI 相比标准以太网 NIC 和高端 SmartNIC，在延迟和带宽上分别最高提升 49 倍和 72 倍。

Conclusion: cMPI 证明了利用 CXL 内存共享优化 MPI 通信的可行性与显著性能优势，为高性能计算中的低延迟通信提供了新方向。

Abstract: Message Passing Interface (MPI) is a foundational programming model for
high-performance computing. MPI libraries traditionally employ network
interconnects (e.g., Ethernet and InfiniBand) and network protocols (e.g., TCP
and RoCE) with complex software stacks for cross-node communication. We present
cMPI, the first work to optimize MPI point-to-point communication (both
one-sided and two-sided) using CXL memory sharing on a real CXL platform,
transforming cross-node communication into memory transactions and data copies
within CXL memory, bypassing traditional network protocols. We analyze
performance across various interconnects and find that CXL memory sharing
achieves 7.2x-8.1x lower latency than TCP-based interconnects deployed in
small- and medium-scale clusters. We address challenges of CXL memory sharing
for MPI communication, including data object management over the dax
representation [50], cache coherence, and atomic operations. Overall, cMPI
outperforms TCP over standard Ethernet NIC and high-end SmartNIC by up to 49x
and 72x in latency and bandwidth, respectively, for small messages.

</details>


### [31] [A Flexible Programmable Pipeline Parallelism Framework for Efficient DNN Training](https://arxiv.org/abs/2510.05112)
*Lijuan Jiang,Xingjian Qian,Zhenxiang Ma,Zan Zong,Hengjie Li,Chao Yang,Jidong Zhai*

Main category: cs.DC

TL;DR: 本文提出了FlexPipe，一个可编程的流水线并行框架，通过领域特定语言（DSL）和自动调度器，实现高效、灵活且可定制的流水线调度，在性能上显著优于现有框架。


<details>
  <summary>Details</summary>
Motivation: 现有流水线并行方法依赖预定义调度策略，难以自动适配不断演化的DNN模型架构；同时，手动设计高效调度方案成本高、灵活性差，现有框架在自动化探索和可控性方面存在不足。

Method: FlexPipe包含两个核心组件：一种简洁的领域特定语言（DSL）用于灵活定义和定制流水线调度，以及一个自动调度器，可在广泛的调度空间中以较低搜索开销自动探索高效调度策略。

Result: 实验表明，FlexPipe相比主流大规模并行框架Megatron-LM最高提速2.28倍，相比当前最先进的自动流水线并行框架最高提速1.49倍。

Conclusion: FlexPipe通过可编程性和自动化调度，在提升流水线并行效率的同时增强了灵活性、可调优性和开发效率，为复杂多变的DNN模型提供了更优的并行解决方案。

Abstract: Pipeline parallelism is an essential distributed parallelism method.
Increasingly complex and diverse DNN models necessitate meticulously customized
pipeline schedules for performance. However, existing practices typically rely
on predefined schedules, each with strengths, but fail to adapt automatically
to the emerging model architectures. Exploring novel high-efficiency schedules
is daunting due to the enormous and varying schedule space. Besides, manually
implementing schedules can be challenging due to the onerous coding burdens and
constantly changing needs. Unfortunately, existing frameworks have limitations
in automated schedule exploration and lack flexibility and controllability.
  This paper presents FlexPipe, a programmable pipeline parallelism framework
with enhanced productivity, programmability, debuggability, and ease of tuning.
FlexPipe has two main components: a succinct domain-specific language (DSL) and
an automated scheduler. FlexPipe enables automated schedule exploration for
various parallel scenarios within a broad spectrum of schedule types at a small
search cost. Besides, users can swiftly develop and customize schedules using
the FlexPipe DSL, which embodies flexible controllability in the pipeline order
of micro-batch computations over stages. It also provides convenient mechanisms
to include new operations in schedules to meet changing demands. Our evaluation
results demonstrate that FlexPipe achieves up to 2.28X performance speedup
compared to the popular large-scale parallel framework Megtron-LM, and gains up
to 1.49X performance speedup compared to the state-of-the-art automated
pipeline parallelism framework.

</details>


### [32] [Orders in Chaos: Enhancing Large-Scale MoE LLM Serving with Data Movement Forecasting](https://arxiv.org/abs/2510.05497)
*Zhongkai Yu,Yue Guan,Zihao Yu,Chenyang Zhou,Shuyi Pei,Yangwook Kang,Yufei Ding,Po-An Tsai*

Main category: cs.DC

TL;DR: 本文首次对大规模Mixture of Experts（MoE）模型的数据移动行为进行了全面分析，通过分析150GB以上的专家选择轨迹，提炼出六项关键洞察，并以晶圆级GPU为例，展示了基于这些洞察的微小架构改进可带来显著性能提升（DeepSeek V3和Qwen3分别提速6.3倍和4.0倍）。


<details>
  <summary>Details</summary>
Motivation: MoE架构的大语言模型虽性能卓越，但其随机专家选择机制在多单元部署系统中引发严重的数据移动开销，成为主要性能瓶颈，亟需系统性理解与优化。

Method: 对三个200B–671B规模的先进MoE模型，在超过24,000个多样化请求下进行以数据移动为中心的全面剖析，从时间和空间维度系统分析150GB+的轨迹数据，并提炼设计指导原则。

Result: 提炼出六项关键洞察；在晶圆级GPU上应用这些洞察进行微架构调整，使DeepSeek V3和Qwen3平均性能分别提升6.3倍和4.0倍；公开发布轨迹数据集与即将开源仿真框架。

Conclusion: 该研究首次提供了大规模MoE模型数据移动行为的全面分析，其洞察可有效指导未来高效MoE服务系统的设计，并通过开源资源促进社区研究。

Abstract: Large Language Models (LLMs) with Mixture of Experts (MoE) architectures
achieve remarkable performance improvements, but their random expert selection
mechanism introduces significant data movement overhead that becomes the
dominant bottleneck in multi-unit serving systems. To forecast the patterns
underlying this data movement, we conduct comprehensive data-movement-centric
profiling across three state-of-the-art large-scale MoE models (200B- 671B)
using over 24,000 requests spanning diverse workloads. With the resulting
150GB+ trace files, we perform systematic analysis from both temporal and
spatial perspectives and distill six key insights to guide the design of
diverse future serving systems. Taking wafer-scale GPUs as a case study, we
demonstrate that minor architectural modifications leveraging our insights
achieve substantial performance gains, delivering 6.3X and 4.0X average
speedups on DeepSeek V3 and Qwen3, respectively. Our work provides the first
comprehensive data-centric analysis of MoE models at scale. Our profiling
traces and analysis results are publicly available at
{https://huggingface.co/datasets/core12345/MoE_expert_selection_trace. We will
also release our simulation framework shortly to facilitate future research in
this area.

</details>


### [33] [Lumos: Performance Characterization of WebAssembly as a Serverless Runtime in the Edge-Cloud Continuum](https://arxiv.org/abs/2510.05118)
*Cynthia Marcelino,Noah Krennmair,Thomas Pusztai,Stefan Nastic*

Main category: cs.DC

TL;DR: 本文提出了Lumos，一个用于评估无服务器运行时在边缘-云连续体中性能的模型与基准测试工具，并对比了WebAssembly（Wasm）与容器的性能表现。


<details>
  <summary>Details</summary>
Motivation: 当前对WebAssembly在无服务器场景下，特别是在边缘-云连续体中的性能优势与权衡缺乏充分理解。

Method: 开发Lumos性能模型和基准测试工具，识别工作负载、系统和环境层面的性能驱动因素，并对主流容器与Wasm运行时（解释执行与AOT编译模式）进行基准测试。

Result: AOT编译的Wasm镜像体积最多缩小30倍，冷启动延迟最多降低16%；而解释执行的Wasm在热延迟上最多高55倍，I/O序列化开销最多高10倍。

Conclusion: Wasm在AOT编译模式下在体积和冷启动方面显著优于容器，但解释执行模式性能较差，需谨慎选择执行模式以平衡性能与资源。

Abstract: WebAssembly has emerged as a lightweight and portable runtime to execute
serverless functions, particularly in heterogeneous and resource-constrained
environments such as the Edge Cloud Continuum. However, the performance
benefits versus trade-offs remain insufficiently understood. This paper
presents Lumos, a performance model and benchmarking tool for characterizing
serverless runtimes. Lumos identifies workload, system, and environment-level
performance drivers in the Edge-Cloud Continuum. We benchmark state-of-the-art
containers and the Wasm runtime in interpreted mode and with ahead-of-time
compilation. Our performance characterization shows that AoT-compiled Wasm
images are up to 30x smaller and decrease cold-start latency by up to 16%
compared to containers, while interpreted Wasm suffers up to 55x higher warm
latency and up to 10x I/O-serialization overhead.

</details>


### [34] [Artificial Intelligence for Cost-Aware Resource Prediction in Big Data Pipelines](https://arxiv.org/abs/2510.05127)
*Harshit Goyal*

Main category: cs.DC

TL;DR: 本文提出一种基于随机森林回归的人工智能方法，用于预测大数据流水线中的资源利用率，在Google Borg集群数据上实现了高精度预测（R²=0.99），展示了其在云环境中实现成本感知自动扩缩容的潜力。


<details>
  <summary>Details</summary>
Motivation: 现代云计算中，资源分配效率至关重要：过度配置会带来不必要成本，而配置不足则可能导致性能下降和服务等级协议（SLA）违规。

Method: 使用随机森林回归模型预测资源利用率；对Google Borg集群追踪数据进行清洗、转换并提取关键特征（如CPU、内存及使用分布）。

Result: 模型取得了高预测精度（R² = 0.99，MAE = 0.0048，RMSE = 0.137），能有效捕捉工作负载特征与资源使用之间的非线性关系；在中小型任务上表现优异，但在罕见的大规模任务上误差方差较高。

Conclusion: AI驱动的资源利用率预测在云环境中具有显著应用潜力，可在保障服务质量的同时减少不必要的资源分配，支持成本感知的自动扩缩容策略。

Abstract: Efficient resource allocation is a key challenge in modern cloud computing.
Over-provisioning leads to unnecessary costs, while under-provisioning risks
performance degradation and SLA violations. This work presents an artificial
intelligence approach to predict resource utilization in big data pipelines
using Random Forest regression. We preprocess the Google Borg cluster traces to
clean, transform, and extract relevant features (CPU, memory, usage
distributions). The model achieves high predictive accuracy (R Square = 0.99,
MAE = 0.0048, RMSE = 0.137), capturing non-linear relationships between
workload characteristics and resource utilization. Error analysis reveals
impressive performance on small-to-medium jobs, with higher variance in rare
large-scale jobs. These results demonstrate the potential of AI-driven
prediction for cost-aware autoscaling in cloud environments, reducing
unnecessary provisioning while safeguarding service quality.

</details>


### [35] [FlashResearch: Real-time Agent Orchestration for Efficient Deep Research](https://arxiv.org/abs/2510.05145)
*Lunyiu Nie,Nedim Lipka,Ryan A. Rossi,Swarat Chaudhuri*

Main category: cs.DC

TL;DR: FlashResearch 是一个新型高效深度研究框架，通过将串行处理转为动态并行的树状子任务结构，显著提升研究代理的效率与响应速度。


<details>
  <summary>Details</summary>
Motivation: 现有深度研究代理受限于串行推理过程，导致高延迟、运行时适应性差和资源利用低效，难以用于交互式应用。

Method: 提出 FlashResearch 框架，包含：(1) 自适应规划器，根据查询复杂度动态分配计算资源；(2) 实时编排层，监控进度并剪枝冗余路径；(3) 多维并行化机制，支持广度与深度的并发处理。

Result: 实验表明，FlashResearch 在固定时间预算下持续提升最终报告质量，并在保持相当质量的前提下实现最高 5 倍加速。

Conclusion: FlashResearch 有效解决了深度研究代理的效率瓶颈，为交互式应用场景提供了可行的高性能解决方案。

Abstract: Deep research agents, which synthesize information across diverse sources,
are significantly constrained by their sequential reasoning processes. This
architectural bottleneck results in high latency, poor runtime adaptability,
and inefficient resource allocation, making them impractical for interactive
applications. To overcome this, we introduce FlashResearch, a novel framework
for efficient deep research that transforms sequential processing into
parallel, runtime orchestration by dynamically decomposing complex queries into
tree-structured sub-tasks. Our core contributions are threefold: (1) an
adaptive planner that dynamically allocates computational resources by
determining research breadth and depth based on query complexity; (2) a
real-time orchestration layer that monitors research progress and prunes
redundant paths to reallocate resources and optimize efficiency; and (3) a
multi-dimensional parallelization framework that enables concurrency across
both research breadth and depth. Experiments show that FlashResearch
consistently improves final report quality within fixed time budgets, and can
deliver up to a 5x speedup while maintaining comparable quality.

</details>


### [36] [Decoupling Correctness from Policy: A Deterministic Causal Structure for Multi-Agent Systems](https://arxiv.org/abs/2510.05621)
*Zhiyuan Ren,Tao Zhang,Wenchi Chen*

Main category: cs.DC

TL;DR: 本文提出确定性因果结构（DCS），通过公理化方法将分布式多智能体系统中的正确性与调度、批处理等操作策略解耦，从而在异步计算中提供一种与策略无关的正确性基础。


<details>
  <summary>Details</summary>
Motivation: 在分布式多智能体系统中，正确性常与调度、批处理或路由等操作策略交织，导致系统脆弱——策略为提升性能而演进时可能破坏完整性保证。因此，亟需一种将正确性与策略解耦的形式化基础。

Method: 作者构建了一个最小公理化理论——确定性因果结构（DCS），并基于该理论证明了四个关键性质：存在性与唯一性、策略无关不变性、观测等价性以及公理最小性。

Result: DCS 能解决 CRDT 等以值为中心的收敛模型无法处理的因果歧义问题；且任何公理的缺失都会导致确定性崩溃为歧义。所有保证均由公理和形式化证明确立。

Conclusion: DCS 构成了异步计算中正确性的边界原则，类似于 CAP 和 FLP 定理，确立了“正确性即底盘”（Correctness-as-a-Chassis）范式，为构建模块化、安全且可演化的分布式智能系统提供基础。

Abstract: In distributed multi-agent systems, correctness is often entangled with
operational policies such as scheduling, batching, or routing, which makes
systems brittle since performance-driven policy evolution may break integrity
guarantees. This paper introduces the Deterministic Causal Structure (DCS), a
formal foundation that decouples correctness from policy. We develop a minimal
axiomatic theory and prove four results: existence and uniqueness,
policy-agnostic invariance, observational equivalence, and axiom minimality.
These results show that DCS resolves causal ambiguities that value-centric
convergence models such as CRDTs cannot address, and that removing any axiom
collapses determinism into ambiguity. DCS thus emerges as a boundary principle
of asynchronous computation, analogous to CAP and FLP: correctness is preserved
only within the expressive power of a join-semilattice. All guarantees are
established by axioms and proofs, with only minimal illustrative constructions
included to aid intuition. This work establishes correctness as a fixed,
policy-agnostic substrate, a Correctness-as-a-Chassis paradigm, on which
distributed intelligent systems can be built modularly, safely, and evolvably.

</details>


### [37] [Percepta: High Performance Stream Processing at the Edge](https://arxiv.org/abs/2510.05149)
*Clarisse Sousa,Tiago Fonseca,Luis Lino Ferreira,Ricardo Venâncio,Ricardo Severino*

Main category: cs.DC

TL;DR: 本文提出了Percepta，一种轻量级边缘数据流处理系统，专为支持边缘AI（尤其是强化学习）而设计，具备奖励函数计算、数据存储、实时数据准备、协议与采样率协调及缺失数据处理等功能。


<details>
  <summary>Details</summary>
Motivation: 云中心架构在实时数据和物联网（IoT）场景下面临延迟、带宽和隐私等问题，且IoT还带来数据速率协调、协议转换、数据丢失处理以及与AI模型集成等挑战。

Method: 设计并实现名为Percepta的轻量级数据流处理系统，集成奖励函数计算、模型再训练数据存储、实时数据准备、数据归一化、异构协议与采样率协调、缺失数据鲁棒处理等特性。

Result: Percepta能有效支持边缘端AI工作负载，特别是强化学习任务，应对边缘环境中数据异构性与不完整性等实际挑战。

Conclusion: Percepta为边缘AI部署提供了一个高效、轻量且功能全面的数据流处理解决方案，特别适用于需要持续决策的强化学习应用场景。

Abstract: The rise of real-time data and the proliferation of Internet of Things (IoT)
devices have highlighted the limitations of cloud-centric solutions,
particularly regarding latency, bandwidth, and privacy. These challenges have
driven the growth of Edge Computing. Associated with IoT appears a set of other
problems, like: data rate harmonization between multiple sources, protocol
conversion, handling the loss of data and the integration with Artificial
Intelligence (AI) models. This paper presents Percepta, a lightweight Data
Stream Processing (DSP) system tailored to support AI workloads at the edge,
with a particular focus on such as Reinforcement Learning (RL). It introduces
specialized features such as reward function computation, data storage for
model retraining, and real-time data preparation to support continuous
decision-making. Additional functionalities include data normalization,
harmonization across heterogeneous protocols and sampling rates, and robust
handling of missing or incomplete data, making it well suited for the
challenges of edge-based AI deployment.

</details>


### [38] [SATER: A Self-Aware and Token-Efficient Approach to Routing and Cascading](https://arxiv.org/abs/2510.05164)
*Yuanzhe Shen,Yide Liu,Zisu Huang,Ruicheng Yin,Xiaoqing Zheng,Xuanjing Huang*

Main category: cs.DC

TL;DR: 本文提出SATER，一种兼容预生成路由和级联路由的双模式方法，通过最短响应偏好优化和置信度感知拒绝机制，在保持性能的同时显著降低计算成本和延迟。


<details>
  <summary>Details</summary>
Motivation: 大语言模型（LLM）虽性能强大，但依赖昂贵的商业API或云服务，而小型语言模型（SLM）虽成本低但能力有限。现有路由策略（预生成路由和级联路由）各有局限，亟需兼顾性能、成本与效率的解决方案。

Method: 提出SATER方法，结合最短响应偏好优化和置信度感知拒绝机制，对模型进行微调，以减少冗余输出和响应时间，同时提升预生成路由性能和级联路由效率。

Result: 在三种SLM和六个不同类型的复杂数据集上的实验表明，SATER在性能相当的前提下，计算成本降低超过50%，级联延迟减少超过80%。

Conclusion: SATER有效平衡了语言模型的性能与成本，显著提升了现有路由策略的效率，为实际部署提供了更具性价比的解决方案。

Abstract: Large language models (LLMs) demonstrate remarkable performance across
diverse tasks, yet their effectiveness frequently depends on costly commercial
APIs or cloud services. Model selection thus entails a critical trade-off
between performance and cost: high-performing LLMs typically incur substantial
expenses, whereas budget-friendly small language models (SLMs) are constrained
by limited capabilities. Current research primarily proposes two routing
strategies: pre-generation routing and cascade routing. Both approaches have
distinct characteristics, with cascade routing typically offering superior
cost-effectiveness and accuracy despite its higher latency. To further address
the limitations of both approaches, we introduce SATER, a dual-mode compatible
approach that fine-tunes models through shortest-response preference
optimization and a confidence-aware rejection mechanism. SATER significantly
reduces redundant outputs and response times, while improving both the
performance of pre-generation routing and the efficiency of cascade routing.
Experiments across three SLMs and six datasets, varying in type and complexity,
demonstrate that SATER achieves comparable performance while consistently
reducing computational costs by over 50\% and cascade latency by over 80\%.

</details>


### [39] [OptPipe: Memory- and Scheduling-Optimized Pipeline Parallelism for LLM Training](https://arxiv.org/abs/2510.05186)
*Hongpei Li,Han Zhang,Huikang Liu,Dongdong Ge,Yinyu Ye*

Main category: cs.DC

TL;DR: 本文提出一种基于优化的细粒度流水线调度方法，通过联合优化内存限制、激活重用和流水线气泡最小化，在相同内存限制下最多减少50%的空闲时间，并提升吞吐量与内存利用率。


<details>
  <summary>Details</summary>
Motivation: 现有流水线并行中的激活卸载策略多为启发式且粒度较粗，未能充分权衡内存、计算与调度延迟之间的细粒度关系，也未有效结合内存约束与调度效率。

Method: 将流水线调度建模为一个带约束的优化问题，综合考虑设备内存容量、激活重用机会和流水线气泡最小化，动态生成细粒度调度策略。

Result: 在相同每设备内存限制下，空闲流水线时间最多减少50%，同时提升吞吐量和内存利用率，部分场景下还能支持更大模型的训练。

Conclusion: 通过将调度问题形式化为优化模型，本文方法能动态适应模型结构与硬件配置，在严格内存约束下显著提升流水线并行效率。

Abstract: Pipeline parallelism (PP) has become a standard technique for scaling large
language model (LLM) training across multiple devices. However, despite recent
progress in reducing memory consumption through activation offloading, existing
approaches remain largely heuristic and coarse-grained, often overlooking the
fine-grained trade-offs between memory, computation, and scheduling latency. In
this work, we revisit the pipeline scheduling problem from a principled
optimization perspective. We observe that prevailing strategies either rely on
static rules or aggressively offload activations without fully leveraging the
interaction between memory constraints and scheduling efficiency. To address
this, we formulate scheduling as a constrained optimization problem that
jointly accounts for memory capacity, activation reuse, and pipeline bubble
minimization. Solving this model yields fine-grained schedules that reduce
pipeline bubbles while adhering to strict memory budgets. Our approach
complements existing offloading techniques: whereas prior approaches trade
memory for time in a fixed pattern, we dynamically optimize the tradeoff with
respect to model structure and hardware configuration. Experimental results
demonstrate that our method consistently improves both throughput and memory
utilization. In particular, we reduce idle pipeline time by up to 50% under the
same per-device memory limit, and in some cases, enable the training of larger
models within limited memory budgets.

</details>


### [40] [Performance of a high-order MPI-Kokkos accelerated fluid solver](https://arxiv.org/abs/2510.05254)
*Filipp Sporykhin,Holger Homann*

Main category: cs.DC

TL;DR: 本文研究了一种基于不连续Galerkin方法和Runge-Kutta时间积分的高阶数值格式在现代高性能计算架构（包括多核CPU和GPU）上的性能表现，发现高阶方法（如八阶）在达到相同全局误差时计算效率更高，且GPU在大规模问题上优于CPU，但小规模问题则相反；同时指出新一代GPU虽能效更高，但需更大规模问题才能发挥优势，可能引发能耗反弹效应。


<details>
  <summary>Details</summary>
Motivation: 评估高阶数值方法在现代异构高性能计算平台（CPU/GPU）上的计算效率、可扩展性与能耗表现，为大规模流体动力学模拟提供高效、节能的实现策略。

Method: 采用空间上最高八阶的节点型不连续Galerkin方法，时间上耦合最高六阶的Runge-Kutta方法，求解线性对流方程和等温Euler方程（1D/2D/3D）；使用Kokkos库结合MPI实现单一源码在多种GPU和多核CPU系统上的可移植并行计算。

Result: 高阶空间离散（如八阶）显著减少达到指定误差所需的计算时间；四阶Runge-Kutta通常性能良好；GPU在自由度超过10⁷时显著优于CPU，而小规模问题CPU更快；GPU在大规模模拟中能耗更低，但新一代GPU需更大问题规模才能高效利用，可能因模拟规模扩大导致总能耗增加（反弹效应）；MPI实现展现出完美的弱可扩展性。

Conclusion: 高阶不连续Galerkin方法结合现代异构架构可显著提升流体模拟效率，但需根据问题规模合理选择硬件；尽管GPU在大规模问题中更高效节能，但硬件更新带来的效率增益可能被更大规模模拟的能耗反弹所抵消。

Abstract: This work discusses the performance of a modern numerical scheme for fluid
dynamical problems on modern high-performance computing architectures. Our code
implements a spatial nodal discontinuous Galerkin scheme that we test up to an
order of convergence of eight. It is temporally coupled to a set of Runge-Kutta
methods of orders up to six. The code integrates the linear advection equations
as well as the isothermal Euler equations in one, two, and three dimensions. In
order to target modern hardware involving many-core Central Processing Units
and accelerators such as Graphic Processing Units we use the Kokkos library in
conjunction with the Message Passing Interface to run our single source code on
various GPU systems. We find that the higher the order the faster is the code.
Eighth-order simulations attain a given global error with much less computing
time than third- or fourth-order simulations. The RK scheme has a smaller
impact on the code performance and a classical fourth-order scheme seems to
generally be a good choice. The code performs very well on all considered GPUs.
The many-CPU performance is also very good and perfect weak scaling is observed
up to many hundreds of CPU cores using MPI. We note that small grid-size
simulations are faster on CPUs than on GPUs while GPUs win significantly over
CPUs for simulations involving more than $10^7$ degrees of freedom ($\approx
3100^2$ grid points). When it comes to the environmental impact of numerical
simulations we estimate that GPUs consume less energy than CPUs for large
grid-size simulations but more energy on small grids. We observe a tendency
that the more modern is the GPU the larger needs to be the grid in order to use
it efficiently. This yields a rebound effect because larger simulations need
longer computing times and in turn more energy that is not compensated by the
energy efficiency gain of the newer GPUs.

</details>


### [41] [Intertemporal Pricing of Time-Bound Stablecoins: Measuring and Controlling the Liquidity-of-Time Premium](https://arxiv.org/abs/2510.05711)
*Ailiya Borjigin,Cong He*

Main category: cs.DC

TL;DR: 本文提出“时间流动性溢价”（TLP）概念，用于量化在主市场休市期间提供流动性的额外成本或收益，并构建无套利定价模型与动态LTV调控机制，以管理时间绑定稳定币的供应和风险。


<details>
  <summary>Details</summary>
Motivation: 传统金融市场存在时区割裂和休市期间流动性缺失的问题，时间绑定稳定币可在主市场关闭时临时代币化传统证券，提升跨市场连续流动性，但需解决其定价与风险管理问题。

Method: 结合金融工程（无套利条件、类期权定价）与实证金融（交叉上市股票与期货的事件研究），建立TLP的理论模型与封闭解，模拟不同波动率和抵押率下的情景，并设计动态调整LTV的政策机制。

Result: TLP随市场关闭时长和波动率上升而增大，但可通过自适应LTV机制有效控制；研究还提供了TLP的实证代理指标、回测结果及协议设计建议。

Conclusion: 时间绑定稳定币可作为缓解时间维度市场低效的工具，其定价与风控框架为未来DeFi协议设计和学术研究提供了基础。

Abstract: Time-bound stablecoins are DeFi assets that temporarily tokenize traditional
securities during market off-hours, enabling continuous cross-market liquidity.
We introduce the Liquidity-of-Time Premium (TLP): the extra return or cost of
providing liquidity when the primary market is closed. We build a no-arbitrage
pricing model that yields a band for fair values over different expiries, and a
dynamic risk-control mechanism that adjusts loan-to-value (LTV) ratios in real
time to keep TLP within a target range. Our analysis blends financial
engineering (no-arbitrage conditions, option-style pricing) with empirical
finance (event studies on cross-listed stocks and futures) to measure TLP under
time-zone frictions. We define TLP formally, derive closed-form expressions for
its term structure under idealized assumptions, and simulate scenarios that
vary volatility and collateralization. We then propose an LTV policy that
raises or lowers collateral to expand or curtail time-bound stablecoin supply,
analogous to a central bank adjusting rates to defend a peg. We outline
empirical proxies for TLP, including ADR premiums, overseas index futures
versus cash index divergence, and pre-market versus official close gaps.
Results show that TLP grows with closure length and volatility, yet can be
contained by adaptive LTV. We provide backtests and figures (term-structure
curves, capital-efficiency versus tail-risk trade-offs, time-liquidity
heatmaps) and discuss protocol design (vault structure, closing-price oracles,
on-chain auction liquidations). The findings position time-bound stablecoins as
a tool to reduce temporal market inefficiencies and inform future research and
deployment.

</details>


### [42] [A Review of Ontology-Driven Big Data Analytics in Healthcare: Challenges, Tools, and Applications](https://arxiv.org/abs/2510.05738)
*Ritesh Chandra,Sonali Agarwal,Navjot Singh,Sadhana Tiwari*

Main category: cs.DC

TL;DR: 本文综述了本体驱动的语义数据管理在医疗大数据分析中的应用，系统分类了六类研究方向，并探讨了其与主流大数据框架的集成及未来趋势。


<details>
  <summary>Details</summary>
Motivation: 医疗数据呈爆炸式增长且来源异构，传统数据湖若缺乏有效治理易沦为“数据沼泽”，亟需提升语义互操作性、数据可发现性与智能访问能力。

Method: 采用系统性文献综述方法，构建研究问题，在主要学术数据库中结构化检索，并将入选研究分为六类本体驱动的医疗分析方法进行归纳分析。

Result: 识别出六类本体驱动的医疗分析范式，总结了各类的技术方案、典型案例、挑战与趋势，并探讨了本体技术与Hadoop、Spark等大数据平台的融合潜力。

Conclusion: 本体驱动的语义数据管理能有效支撑可扩展、互操作且高性能的医疗数据生态系统，结合AI、IoT和实时分析等新兴技术，有望推动智能医疗分析的发展。

Abstract: Exponential growth in heterogeneous healthcare data arising from electronic
health records (EHRs), medical imaging, wearable sensors, and biomedical
research has accelerated the adoption of data lakes and centralized
architectures capable of handling the Volume, Variety, and Velocity of Big Data
for advanced analytics. However, without effective governance, these
repositories risk devolving into disorganized data swamps. Ontology-driven
semantic data management offers a robust solution by linking metadata to
healthcare knowledge graphs, thereby enhancing semantic interoperability,
improving data discoverability, and enabling expressive, domain-aware access.
This review adopts a systematic research strategy, formulating key research
questions and conducting a structured literature search across major academic
databases, with selected studies analyzed and classified into six categories of
ontology-driven healthcare analytics: (i) ontology-driven integration
frameworks, (ii) semantic modeling for metadata enrichment, (iii)
ontology-based data access (OBDA), (iv) basic semantic data management, (v)
ontology-based reasoning for decision support, and (vi) semantic annotation for
unstructured data. We further examine the integration of ontology technologies
with Big Data frameworks such as Hadoop, Spark, Kafka, and so on, highlighting
their combined potential to deliver scalable and intelligent healthcare
analytics. For each category, recent techniques, representative case studies,
technical and organizational challenges, and emerging trends such as artificial
intelligence, machine learning, the Internet of Things (IoT), and real-time
analytics are reviewed to guide the development of sustainable, interoperable,
and high-performance healthcare data ecosystems.

</details>


### [43] [EARL: Efficient Agentic Reinforcement Learning Systems for Large Language Models](https://arxiv.org/abs/2510.05943)
*Zheyue Tan,Mustapha Abdullahi,Tuo Shi,Huining Yuan,Zelai Xu,Chao Yu,Boxun Li,Bo Zhao*

Main category: cs.DC

TL;DR: 本文提出了EARL，一种用于高效代理式强化学习（agentic RL）的可扩展系统，通过动态调整并行策略和优化中间数据交换，解决了长上下文训练中的内存和通信瓶颈问题。


<details>
  <summary>Details</summary>
Motivation: 在大规模代理式强化学习中，随着上下文长度迅速增长，系统面临内存占用高、延迟大、OOM错误频发以及跨设备数据传输开销大的问题，亟需高效可扩展的训练系统。

Method: EARL引入了两个核心组件：一是并行选择器，根据序列长度和系统负载动态调整模型与训练并行策略；二是数据分发器，执行布局感知的去中心化中间数据批交换。

Result: 该方法显著提升了训练吞吐量，减少了长上下文导致的失败，并支持在不限制上下文长度的前提下稳定进行大规模代理式大语言模型训练。

Conclusion: EARL有效解决了代理式强化学习中的系统瓶颈，为大规模、长上下文的LLM代理训练提供了高效且稳定的解决方案。

Abstract: Reinforcement learning (RL) has become a pivotal component of large language
model (LLM) post-training, and agentic RL extends this paradigm to operate as
agents through multi-turn interaction and tool use. Scaling such systems
exposes two practical bottlenecks: (1) context length grows rapidly during
training, inflating memory usage and latency, and triggering out-of-memory
(OOM) failures; and (2) intermediate tensors accumulate with context length,
making cross-device data movement a major system bottleneck.
  We present EARL, a scalable system for efficient agentic RL. EARL designs a
parallelism selector that dynamically adapts model and training parallelism
across RL stages based on sequence length and system load, and a data
dispatcher that performs layout-aware, decentralized exchange of intermediate
data batches. Together, these components increase throughput, reduce
long-context failures, and enable stable large-scale training of agentic LLMs
without relying on hard limits or penalties of context length.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [44] [Emergent Coordination in Multi-Agent Language Models](https://arxiv.org/abs/2510.05174)
*Christoph Riedl*

Main category: cs.MA

TL;DR: 本文提出了一种信息论框架，用于判断多智能体大语言模型系统是否展现出高阶集体结构，而非仅是独立智能体的集合。通过部分信息分解方法分析时延互信息，作者在无直接通信的猜谜游戏中验证了提示设计（如赋予角色和引导换位思考）可促使系统从简单聚合体转变为具有协同互补性的高阶集体。


<details>
  <summary>Details</summary>
Motivation: 区分多智能体大语言模型系统是仅由独立智能体组成，还是形成了具有高阶结构的整合集体，对于理解其协同机制和提升集体智能至关重要。现有方法缺乏数据驱动的手段来检测和量化这种高阶结构。

Method: 引入一种基于信息分解的框架，通过时延互信息（TDMI）的部分信息分解，实现对多智能体系统中动态涌现的检测、定位，并区分虚假的时间耦合与真正影响性能的跨智能体协同。在无直接通信的猜谜游戏中，通过三种随机干预（控制组、赋予角色、引导换位思考）测试该框架。

Result: 控制组表现出强时间协同但跨智能体协调弱；赋予角色引入了稳定的身份差异化；结合角色与换位思考指令则同时展现出身份差异化和目标导向的互补性。结果在不同涌现度量和熵估计器下均稳健，且不能由无协调基线或纯时间动态解释。

Conclusion: 通过提示设计可引导多智能体LLM系统从简单聚合体转变为高阶集体；其互动模式虽不依赖人类认知假设，却与人类群体集体智能的核心原则一致：高效表现需共享目标的一致性与成员间贡献的互补性。

Abstract: When are multi-agent LLM systems merely a collection of individual agents
versus an integrated collective with higher-order structure? We introduce an
information-theoretic framework to test -- in a purely data-driven way --
whether multi-agent systems show signs of higher-order structure. This
information decomposition lets us measure whether dynamical emergence is
present in multi-agent LLM systems, localize it, and distinguish spurious
temporal coupling from performance-relevant cross-agent synergy. We implement
both a practical criterion and an emergence capacity criterion operationalized
as partial information decomposition of time-delayed mutual information (TDMI).
We apply our framework to experiments using a simple guessing game without
direct agent communication and only minimal group-level feedback with three
randomized interventions. Groups in the control condition exhibit strong
temporal synergy but only little coordinated alignment across agents. Assigning
a persona to each agent introduces stable identity-linked differentiation.
Combining personas with an instruction to ``think about what other agents might
do'' shows identity-linked differentiation and goal-directed complementarity
across agents. Taken together, our framework establishes that multi-agent LLM
systems can be steered with prompt design from mere aggregates to higher-order
collectives. Our results are robust across emergence measures and entropy
estimators, and not explained by coordination-free baselines or temporal
dynamics alone. Without attributing human-like cognition to the agents, the
patterns of interaction we observe mirror well-established principles of
collective intelligence in human groups: effective performance requires both
alignment on shared objectives and complementary contributions across members.

</details>


### [45] [AgentZero++: Modeling Fear-Based Behavior](https://arxiv.org/abs/2510.05185)
*Vrinda Malhotra,Jiaman Li,Nandini Pisupati*

Main category: cs.MA

TL;DR: AgentZero++ 是一个基于 Agent 的模型，通过整合认知、情感和社会机制，模拟空间分布系统中的去中心化集体暴力，扩展了原始 Agent_Zero 框架并引入八项行为增强机制。


<details>
  <summary>Details</summary>
Motivation: 现有模型在模拟集体暴力时缺乏对个体认知、情感及社会互动的综合建模，难以捕捉微观心理机制如何驱动宏观冲突模式。

Method: 在 Epstein 的 Agent_Zero 框架基础上，引入八项行为增强机制（如基于年龄的冲动控制、记忆风险评估、情感-认知耦合等），使用 Python 和 Mesa ABM 框架实现模块化建模与可视化。

Result: 模型成功再现了抗议不对称性、升级循环和局部报复等涌现动态，表明记忆、反应性和情感一致性等微小差异可通过反馈回路显著放大或抑制社会动荡。

Conclusion: AgentZero++ 提供了一个灵活可扩展的平台，有助于深入理解情感传染和基于心理机制的集体行动，强调微观认知异质性对宏观冲突模式的关键影响。

Abstract: We present AgentZero++, an agent-based model that integrates cognitive,
emotional, and social mechanisms to simulate decentralized collective violence
in spatially distributed systems. Building on Epstein's Agent\_Zero framework,
we extend the original model with eight behavioral enhancements: age-based
impulse control; memory-based risk estimation; affect-cognition coupling;
endogenous destructive radius; fight-or-flight dynamics; affective homophily;
retaliatory damage; and multi-agent coordination. These additions allow agents
to adapt based on internal states, previous experiences, and social feedback,
producing emergent dynamics such as protest asymmetries, escalation cycles, and
localized retaliation. Implemented in Python using the Mesa ABM framework,
AgentZero++ enables modular experimentation and visualization of how
micro-level cognitive heterogeneity shapes macro-level conflict patterns. Our
results highlight how small variations in memory, reactivity, and affective
alignment can amplify or dampen unrest through feedback loops. By explicitly
modeling emotional thresholds, identity-driven behavior, and adaptive networks,
this work contributes a flexible and extensible platform for analyzing
affective contagion and psychologically grounded collective action.

</details>


### [46] [Agent+P: Guiding UI Agents via Symbolic Planning](https://arxiv.org/abs/2510.06042)
*Shang Ma,Xusheng Xiao,Yanfang Ye*

Main category: cs.MA

TL;DR: AGENT+P 是一个结合符号规划与大语言模型（LLM）的 UI 自动化框架，通过构建 UI 转换图（UTG）将任务转化为路径规划问题，从而提升成功率并减少操作步骤。


<details>
  <summary>Details</summary>
Motivation: 现有基于 LLM 的 UI 智能体在长程任务中容易产生幻觉，因其缺乏对全局 UI 转换结构的理解。

Method: 提出 AGENT+P 框架，将应用的 UI 转换结构建模为 UI 转换图（UTG），利用现成的符号规划器生成正确且最优的高层计划，引导 LLM 智能体执行任务。

Result: 在 AndroidWorld 基准测试中，AGENT+P 将当前最优 UI 智能体的成功率最多提升 14%，动作步骤减少 37.7%。

Conclusion: AGENT+P 作为一种即插即用框架，有效提升了 LLM 智能体在 UI 自动化任务中的性能和效率。

Abstract: Large Language Model (LLM)-based UI agents show great promise for UI
automation but often hallucinate in long-horizon tasks due to their lack of
understanding of the global UI transition structure. To address this, we
introduce AGENT+P, a novel framework that leverages symbolic planning to guide
LLM-based UI agents. Specifically, we model an app's UI transition structure as
a UI Transition Graph (UTG), which allows us to reformulate the UI automation
task as a pathfinding problem on the UTG. This further enables an off-the-shelf
symbolic planner to generate a provably correct and optimal high-level plan,
preventing the agent from redundant exploration and guiding the agent to
achieve the automation goals. AGENT+P is designed as a plug-and-play framework
to enhance existing UI agents. Evaluation on the AndroidWorld benchmark
demonstrates that AGENT+P improves the success rates of state-of-the-art UI
agents by up to 14% and reduces the action steps by 37.7%.

</details>
