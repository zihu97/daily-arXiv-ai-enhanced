<div id=toc></div>

# Table of Contents

- [cs.DC](#cs.DC) [Total: 13]
- [cs.MA](#cs.MA) [Total: 3]
- [cs.NI](#cs.NI) [Total: 3]
- [cs.SE](#cs.SE) [Total: 12]
- [cs.PF](#cs.PF) [Total: 1]
- [cs.AR](#cs.AR) [Total: 3]
- [cs.OS](#cs.OS) [Total: 1]


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [1] [Towards Affordable, Adaptive and Automatic GNN Training on CPU-GPU Heterogeneous Platforms](https://arxiv.org/abs/2511.07421)
*Tong Qiao,Ao Zhou,Yingjie Qi,Yiou Wang,Han Wan,Jianlei Yang,Chunming Hu*

Main category: cs.DC

TL;DR: A3GNN 是一个面向异构 CPU-GPU 平台的高效 GNN 训练框架，通过局部性感知采样、细粒度并行调度和强化学习优化，在资源受限设备上实现接近高端硬件的性能。


<details>
  <summary>Details</summary>
Motivation: GNN 训练通常依赖昂贵的高性能计算平台，限制了其在资源受限场景下的应用；现有工作负载分析表明，通过充分利用可用资源可在低端设备上显著提升效率。

Method: 提出 A3GNN 框架，结合局部性感知采样、细粒度并行调度，并利用强化学习在吞吐量、内存占用和精度之间探索帕累托最优权衡。

Result: 实验表明，A3GNN 能使 7 块 Nvidia 2080Ti GPU 在吞吐量上比 2 块 A100 GPU 提升最多 1.8 倍，同时保持精度损失极小。

Conclusion: A3GNN 有效缩小了资源受限设备与高端硬件之间的 GNN 训练性能差距，为低成本、自适应、自动化的 GNN 训练提供了可行方案。

Abstract: Graph Neural Networks (GNNs) have been widely adopted due to their strong performance. However, GNN training often relies on expensive, high-performance computing platforms, limiting accessibility for many tasks. Profiling of representative GNN workloads indicates that substantial efficiency gains are possible on resource-constrained devices by fully exploiting available resources. This paper introduces A3GNN, a framework for affordable, adaptive, and automatic GNN training on heterogeneous CPU-GPU platforms. It improves resource usage through locality-aware sampling and fine-grained parallelism scheduling. Moreover, it leverages reinforcement learning to explore the design space and achieve pareto-optimal trade-offs among throughput, memory footprint, and accuracy. Experiments show that A3GNN can bridge the performance gap, allowing seven Nvidia 2080Ti GPUs to outperform two A100 GPUs by up to 1.8X in throughput with minimal accuracy loss.

</details>


### [2] [From Attention to Disaggregation: Tracing the Evolution of LLM Inference](https://arxiv.org/abs/2511.07422)
*Madabattula Rajesh Kumar,Srinivasa Rao Aravilli,Mustafa Saify,Shashank Srivastava*

Main category: cs.DC

TL;DR: 本文探讨了大语言模型推理中的性能瓶颈，并提出通过解耦推理架构（将预填充阶段与解码阶段分离）来优化延迟、吞吐量和成本。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型参数规模激增，实时推理成为主要瓶颈，传统单体GPU集群在内存带宽、计算吞吐和延迟方面面临挑战，亟需新的系统架构应对多目标优化需求。

Method: 引入解耦推理架构，借鉴分布式系统理念（如服务分解、资源解耦和工作负载分区），将计算密集的预填充阶段与内存密集的解码阶段拆分为可独立扩展的组件。

Result: 该架构有效缓解了资源争用问题，并支持对首Token时间（Time to First Token）和Token间延迟（Inter Token Latency）等关键指标进行独立优化。

Conclusion: 解耦推理是一种有前景的架构方向，能够显著提升大语言模型推理效率，在延迟、吞吐和成本之间实现更优平衡。

Abstract: The evolution of Large Language Models from the Transformer architecture to models with trillions of parameters has shifted the primary bottleneck from model training to real time inference. Deploying these massive models is a complex distributed systems challenge constrained by memory bandwidth, computational throughput, and latency requirements. LLM inference fundamentally requires solving a multi objective optimization problem to minimize latency, maximize throughput, and reduce cost. This paper explores the necessary architectural shift towards disaggregated inference, which applies distributed systems principles such as service decomposition, resource disaggregation, and workload partitioning to overcome the limitations of traditional monolithic GPU clusters. By decoupling the compute intensive prefill phase from the memory intensive decode phase into independently scalable components, this paradigm mitigates resource contention and enables independent optimization of key metrics like Time to First Token and Inter Token Latency.

</details>


### [3] [Synera: Synergistic LLM Serving across Device and Cloud at Scale](https://arxiv.org/abs/2511.07423)
*Genglin Wang,Liekang Zeng,Bufang Yang,Kaiwei Liu,Guoliang Xing,Chumin Sun,Li Zhou,Jie Sun,Zhenyu Yan*

Main category: cs.DC

TL;DR: 本文提出Synera，一种设备-云协同的大语言模型（LLM）服务系统，通过高效的小语言模型（SLM）与LLM协同机制，在保证延迟性能的同时显著提升生成质量并降低云服务成本。


<details>
  <summary>Details</summary>
Motivation: 当前在移动设备上部署大语言模型面临生成质量下降和延迟增加的问题；现有方案如云端卸载受限于通信瓶颈，而本地小模型则因资源限制牺牲生成质量。

Method: Synera通过实证研究识别出设备-云协同推理中的优化机会，并引入三项关键技术：通信高效的有选择性卸载、无停滞并行推理和可扩展的云端批处理。

Result: 实验表明，Synera相比现有基线方法在延迟相当的情况下，生成质量提升1.20–5.47倍；相比纯云服务，云服务成本降低8.2%–16.5%。

Conclusion: Synera有效解决了移动端LLM部署中的性能挑战，实现了高质量、低延迟和低成本的协同推理。

Abstract: Large Language Models (LLMs) are becoming key components in various mobile operating systems, driving smart applications like interactive chatbots and personal assistants. While bringing enhanced intelligence to mobile ends, their deployment suffers from a set of performance challenges, especially the generation quality degradation and prolonged latency. Prior works have mainly relied on solutions of cloud offloading or on-device Small Language Models (SLMs). However, the former is usually limited by the communication bottleneck, and the latter sacrifices generation quality due to resource constraints. To mitigate these limitations, this paper proposes Synera, a device-cloud synergistic LLM serving system that applies an efficient SLM-LLM synergistic mechanism. Through empirical studies on LLM's unique computing characteristics, Synera identifies a set of underexplored optimization opportunities in device-cloud synergistic LLM inference, including offloading decisions, pipeline stalls, and batching bottlenecks. To translate them into enhanced performance, Synera introduces tailored designs of communication-efficient selective offloading, stall-free parallel inference, and scalable cloud batching. Extensive evaluations with real-world testbeds show that Synera enables 1.20-5.47x better generation quality against competitive baselines with on-par latency performance. Compared with existing cloud serving, Synera achieves 8.2-16.5% lower cloud serving cost on various benchmarks.

</details>


### [4] [Enhancing reliability in AI inference services: An empirical study on real production incidents](https://arxiv.org/abs/2511.07424)
*Bhala Ranganathan,Mickey Zhang,Kai Wu*

Main category: cs.DC

TL;DR: 本文基于一年的运营经验，对156起高严重性大语言模型（LLM）推理事故进行了系统分析，提出了一个具有高标注一致性的分类法，识别出主要故障模式（如推理引擎故障和超时），并总结了多种缓解策略（如自动检测、流量调度、节点再平衡等），最终提供了一个供从业者采用的检查清单，以提升大规模LLM推理服务的可靠性与成本效益。


<details>
  <summary>Details</summary>
Motivation: 超大规模大语言模型推理对云系统提出极高要求，短暂停机也可能带来重大用户和业务影响。为理解并缓解这些风险，亟需基于实际运维经验的系统性分析。

Method: 作者基于一年的实际运维经验构建了一套事故分类法和分析方法，在156起高严重性事故上验证其有效性，并对2025年4月至6月的数据进行聚焦定量研究；使用Cohen's Kappa系数评估标注一致性，并结合案例提炼缓解策略和自动化机会。

Result: 研究发现约60%的事故源于推理引擎故障，其中约40%为超时问题；约74%的事故可被自动检测，28%需热修复；多数事故可通过流量路由、节点再平衡或扩容策略缓解；所提分类法指导了连接活性检测、GPU容量感知路由和端点隔离等针对性策略，有效降低事故影响并加速恢复。

Conclusion: 系统化、基于实证的LLM推理运维分析能够显著提升大规模服务的可靠性与成本效率，所提出的分类法和从业者检查清单有助于在其他系统中复现该成果。

Abstract: Hyperscale large language model (LLM) inference places extraordinary demands on cloud systems, where even brief failures can translate into significant user and business impact. To better understand and mitigate these risks, we present one of the first provider-internal, practice-based analysis of LLM inference incidents. We developed a taxonomy and methodology grounded in a year of operational experience, validating it on 156 high-severity incidents, and conducted a focused quantitative study of Apr-Jun 2025 to ensure recency and relevance. Our approach achieves high labeling consistency (Cohen's K ~0.89), identifies dominant failure modes (in our dataset ~60% inference engine failures, within that category ~40% timeouts), and surfaces mitigation levers (~74% auto-detected; ~28% required hotfix). Beyond hotfixes, many incidents were mitigated via traffic routing, node rebalancing, or capacity increase policies, indicating further automation opportunities. We also show how the taxonomy guided targeted strategies such as connection liveness, GPU capacity-aware routing, and per-endpoint isolation and reduced incident impact and accelerated recovery. Finally, we contribute a practitioner-oriented adoption checklist that enables others to replicate our taxonomy, analysis, and automation opportunities in their own systems. This study demonstrates how systematic, empirically grounded analysis of inference operations can drive more reliable and cost-efficient LLM serving at scale.

</details>


### [5] [An Evaluation of LLMs Inference on Popular Single-board Computers](https://arxiv.org/abs/2511.07425)
*Tung,Nguyen,Tuyen Nguyen*

Main category: cs.DC

TL;DR: 本文首次系统评估了25个量化开源大语言模型在树莓派和Orange Pi等单板计算机上的推理性能，比较了Ollama与Llamafile两种运行时在吞吐量、内存占用和功耗方面的表现，并提供了实用的部署建议。


<details>
  <summary>Details</summary>
Motivation: 随着对设备端大语言模型推理需求的增长，亟需在边缘硬件上部署轻量、低成本的AI解决方案。单板计算机（如树莓派）虽具潜力，但在大语言模型负载下的表现尚不明确。

Method: 在树莓派4、树莓派5和Orange Pi 5 Pro三种单板计算机上，使用Ollama和Llamafile两种推理运行时，对25个量化开源大语言模型进行基准测试，评估不同CPU配置下多种提示类型下的生成吞吐量、内存使用和功耗。

Result: 单板计算机可稳定支持最多1.5B参数的模型；Llamafile相比Ollama最高实现4倍吞吐量提升和30–40%的功耗降低；研究还识别出架构相关瓶颈并揭示了运行时层面的权衡。

Conclusion: 该研究为在低成本边缘设备上部署大语言模型提供了首个全面性能评估和实用指南，弥合了高性能语言模型与可负担边缘计算之间的差距。

Abstract: The growing demand for on-device large language model (LLM) inference is driving interest in deploying lightweight, cost-effective AI solutions on edge hardware. Single-board computers (SBCs) such as the Raspberry Pi and Orange Pi offer a promising platform for localized, privacy-preserving inference-but remain underexplored in the context of LLM workloads. In this work, we benchmark the performance of 25 quantized open-source LLMs across three SBCs-Raspberry Pi 4, Raspberry Pi 5, and Orange Pi 5 Pro-using two inference runtimes: Ollama and Llamafile. We evaluate generation throughput, memory usage, and power consumption under varying CPU configurations, using multiple prompt types to simulate realistic workloads. Our results show that SBCs can reliably support models up to 1.5B parameters, with Llamafile achieving up to 4x higher throughput and 30-40% lower power usage than Ollama. We identify architecture-specific bottlenecks, highlight runtime-level trade-offs, and provide practical deployment recommendations. This study offers the first broad evaluation of LLM inference on SBCs, bridging the gap between high-performance language models and affordable edge computing.

</details>


### [6] [Network and Systems Performance Characterization of MCP-Enabled LLM Agents](https://arxiv.org/abs/2511.07426)
*Zihao Ding,Mufeng Zhu,Yao Liu*

Main category: cs.DC

TL;DR: 本文对启用MCP（Model Context Protocol）的大语言模型交互进行了系统性测量分析，揭示了其在能力、性能与成本之间的权衡，并提出了提升效率和降低成本的优化策略。


<details>
  <summary>Details</summary>
Motivation: MCP虽能增强大语言模型与外部工具交互的能力，但引入大量上下文信息显著增加token使用量，进而推高费用和计算负担，亟需对其影响进行量化分析。

Method: 通过测量不同大语言模型及MCP配置下的交互表现，评估其在token效率、金钱成本、任务完成时间和成功率等关键指标上的差异。

Result: 研究发现MCP配置显著影响成本与性能，并验证了并行工具调用和任务中止机制等优化手段的有效性。

Conclusion: 该研究为构建高效、稳健且经济的MCP驱动工作流提供了实证依据和实用建议。

Abstract: Model Context Protocol (MCP) has recently gained increased attention within the AI community for providing a standardized way for large language models (LLMs) to interact with external tools and services, significantly enhancing their capabilities. However, the inclusion of extensive contextual information, including system prompts, MCP tool definitions, and context histories, in MCP-enabled LLM interactions, dramatically inflates token usage. Given that LLM providers charge based on tokens, these expanded contexts can quickly escalate monetary costs and increase the computational load on LLM services. This paper presents a comprehensive measurement-based analysis of MCP-enabled interactions with LLMs, revealing trade-offs between capability, performance, and cost. We explore how different LLM models and MCP configurations impact key performance metrics such as token efficiency, monetary cost, task completion times, and task success rates, and suggest potential optimizations, including enabling parallel tool calls and implementing robust task abort mechanisms. These findings provide useful insights for developing more efficient, robust, and cost-effective MCP-enabled workflows.

</details>


### [7] [DynaKV: Enabling Accurate and Efficient Long-Sequence LLM Decoding on Smartphones](https://arxiv.org/abs/2511.07427)
*Tuowei Wang,Minxing Huang,Fengzu Li,Ligeng Chen,Jinrui Zhang,Ju Ren*

Main category: cs.DC

TL;DR: DynaKV 是一种面向智能手机的自适应 KVCache 管理方法，通过无迁移聚类适配、以连续性为中心的闪存管理和内存高效缓存设计，在长序列解码中同时提升准确率与效率。


<details>
  <summary>Details</summary>
Motivation: 在智能手机上运行长序列大语言模型（LLM）推理时，受限于 DRAM 容量和闪存带宽，传统基于检索的 KVCache 管理方法因静态或局部聚类更新无法适应解码过程中 KVCache 分布的变化，导致关键条目遗漏或冗余数据读取，影响准确性和延迟。

Method: DynaKV 提出三项核心技术：(1) 无迁移聚类适配，在检索过程中动态拆分聚类而不引入额外数据传输；(2) 以连续性为中心的闪存管理，将相关条目和聚类共置并采用双头布局以支持高效更新；(3) 内存高效缓存设计，跨 DRAM 与闪存虚拟化缓存空间，并扩展替换策略以匹配聚类级访问模式。

Result: 实验表明，DynaKV 相比当前最优方案平均提升 1.38 倍检索准确率和 1.47 倍端到端速度，并可自然拓展至其他长上下文任务和多级内存架构。

Conclusion: DynaKV 首次在智能手机上实现了兼顾准确性和效率的自适应 KVCache 管理，有效解决了长序列 LLM 推理中的内存瓶颈问题，具有良好的通用性和实用价值。

Abstract: As the demand for human-like reasoning, multi-turn dialogues, and long-form responses grows, large language models (LLMs) are increasingly expected to support efficient and effective long-sequence decoding. However, due to limited DRAM capacity, long-seuqence LLM decoding on smartphones is constrained by the key-value cache (KVCache), whose memory footprint increases linearly with sequence length. Retrieval-based methods mitigate DRAM pressure by offloading KVCache to flash and retrieving query-relevant entries through cluster-based indexing. Unfortunately, as decoding progresses, KVCache distribution shifts render static or local cluster updates progressively misaligned, excluding essential entries or fetching redundant ones. These issues are further exacerbated by smartphone-specific limitations in bandwidth, IOPS, and memory capacity.
  We propose DynaKV, the first adaptive KVCache management approach that jointly addresses accuracy and efficiency for long-sequence decoding on smartphones. DynaKV integrates three key techniques: (1) Migration-Free Cluster Adaptation, which adaptively splits clusters during retrieval without incurring additional transfers; (2) Continuity-Centric Flash Management, which co-locates correlated entries and clusters and employs a dual-head layout for efficient updates; and (3) Memory-Efficient Cache Design, which virtualizes cache space across DRAM and flash and extends replacement policies to align with cluster-level access patterns. Evaluations demonstrate that DynaKV improves retrieval accuracy and reduces end-to-end latency compared to state-of-the-art solutions, achieving average gains of $1.38\times$ in accuracy and $1.47\times$ speedups. Furthermore, the insights of DynaKV naturally extend to other long-context workloads and multi-tier memory hierarchies, underscoring its broader applicability.

</details>


### [8] [HyProv: Hybrid Provenance Management for Scientific Workflows](https://arxiv.org/abs/2511.07574)
*Vasilis Bountris,Lauritz Thamsen,Ulf Leser*

Main category: cs.DC

TL;DR: 本文提出了HyProv，一种混合式溯源管理系统，结合集中式与联邦式架构，以实现对科学工作流执行过程中产生的溯源数据进行高效、实时且工作流感知的查询。


<details>
  <summary>Details</summary>
Motivation: 现有溯源系统难以在可扩展性、实时处理、在线溯源分析以及跨组件和计算资源的集成之间取得平衡，且大多数系统缺乏对工作流结构的认知，无法利用工作流规范进行优化。

Method: HyProv采用混合架构：使用集中式组件管理小规模且稳定的工作流规范相关溯源信息，并通过联邦查询机制访问分布式的、可扩展的监控与执行日志数据库，从而支持低延迟的复杂溯源查询。

Result: 实验表明，HyProv能够扩展至大规模工作流，以亚秒级延迟响应溯源查询，并仅引入较低的CPU和内存开销。

Conclusion: HyProv通过结合集中式与联邦式溯源管理，有效解决了现有系统在实时性、可扩展性和工作流感知方面的不足，为复杂分布式工作流提供了高效的溯源支持。

Abstract: Provenance plays a crucial role in scientific workflow execution, for instance by providing data for failure analysis, real-time monitoring, or statistics on resource utilization for right-sizing allocations. The workflows themselves, however, become increasingly complex in terms of involved components. Furthermore, they are executed on distributed cluster infrastructures, which makes the real-time collection, integration, and analysis of provenance data challenging. Existing provenance systems struggle to balance scalability, real-time processing, online provenance analytics, and integration across different components and compute resources. Moreover, most provenance solutions are not workflow-aware; by focusing on arbitrary workloads, they miss opportunities for workflow systems where optimization and analysis can exploit the availability of a workflow specification that dictates, to some degree, task execution orders and provides abstractions for physical tasks at a logical level.
  In this paper, we present HyProv, a hybrid provenance management system that combines centralized and federated paradigms to offer scalable, online, and workflow-aware queries over workflow provenance traces. HyProv uses a centralized component for efficient management of the small and stable workflow-specification-specific provenance, and complements this with federated querying over different scalable monitoring and provenance databases for the large-scale execution logs. This enables low-latency access to current execution data. Furthermore, the design supports complex provenance queries, which we exemplify for the workflow system Airflow in combination with the resource manager Kubernetes. Our experiments indicate that HyProv scales to large workflows, answers provenance queries with sub-second latencies, and adds only modest CPU and memory overhead to the cluster.

</details>


### [9] [Intelligence per Watt: Measuring Intelligence Efficiency of Local AI](https://arxiv.org/abs/2511.07885)
*Jon Saad-Falcon,Avanika Narayan,Hakki Orhun Akengin,J. Wes Griffin,Herumb Shandilya,Adrian Gamarra Lafuente,Medhya Goel,Rebecca Joseph,Shlok Natarajan,Etash Kumar Guha,Shang Zhu,Ben Athiwaratkun,John Hennessy,Azalia Mirhoseini,Christopher Ré*

Main category: cs.DC

TL;DR: 本文提出“每瓦智能”（IPW）作为衡量本地大语言模型推理能力与能效的指标，通过大规模实证研究表明：当前本地小模型已能准确回答88.7%的真实单轮对话与推理查询；2023至2025年间IPW提升5.3倍，本地查询覆盖率从23.2%升至71.3%；且本地加速器在相同模型下比云端加速器至少节能1.4倍。结果表明本地推理可有效分流集中式云基础设施负载。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型查询需求激增，集中式云基础设施面临扩展压力。同时，参数量≤20B的小模型性能显著提升，且本地设备（如Apple M4 Max）已能以交互级延迟运行这些模型。这促使研究者探索本地推理是否可有效分担云端负载，关键在于评估其在真实查询中的准确性与在功耗受限设备上的实用性。

Method: 提出“每瓦智能”（IPW）指标（任务准确率除以单位功耗），并在20多个前沿本地语言模型、8种加速器及100万条真实单轮聊天与推理查询上进行大规模实证研究，测量每条查询的准确率、能耗、延迟和功耗。

Result: 1）本地LM可准确回答88.7%的单轮查询，准确率因领域而异；2）2023–2025年IPW提升5.3倍，本地查询覆盖率从23.2%增至71.3%；3）本地加速器运行相同模型时IPW至少优于云端加速器1.4倍。

Conclusion: 本地推理已具备显著分流集中式云基础设施负载的潜力，IPW是衡量和推动这一转变的关键指标。作者开源了IPW评测工具以支持系统性基准测试。

Abstract: Large language model (LLM) queries are predominantly processed by frontier models in centralized cloud infrastructure. Rapidly growing demand strains this paradigm, and cloud providers struggle to scale infrastructure at pace. Two advances enable us to rethink this paradigm: small LMs (<=20B active parameters) now achieve competitive performance to frontier models on many tasks, and local accelerators (e.g., Apple M4 Max) run these models at interactive latencies. This raises the question: can local inference viably redistribute demand from centralized infrastructure? Answering this requires measuring whether local LMs can accurately answer real-world queries and whether they can do so efficiently enough to be practical on power-constrained devices (i.e., laptops). We propose intelligence per watt (IPW), task accuracy divided by unit of power, as a metric for assessing capability and efficiency of local inference across model-accelerator pairs. We conduct a large-scale empirical study across 20+ state-of-the-art local LMs, 8 accelerators, and a representative subset of LLM traffic: 1M real-world single-turn chat and reasoning queries. For each query, we measure accuracy, energy, latency, and power. Our analysis reveals $3$ findings. First, local LMs can accurately answer 88.7% of single-turn chat and reasoning queries with accuracy varying by domain. Second, from 2023-2025, IPW improved 5.3x and local query coverage rose from 23.2% to 71.3%. Third, local accelerators achieve at least 1.4x lower IPW than cloud accelerators running identical models, revealing significant headroom for optimization. These findings demonstrate that local inference can meaningfully redistribute demand from centralized infrastructure, with IPW serving as the critical metric for tracking this transition. We release our IPW profiling harness for systematic intelligence-per-watt benchmarking.

</details>


### [10] [UniFormer: Unified and Efficient Transformer for Reasoning Across General and Custom Computing](https://arxiv.org/abs/2511.08135)
*Zhuoheng Ran,Chong Wu,Renjie Xu,Maolin Che,Hong Yan*

Main category: cs.DC

TL;DR: 本文提出了UniFormer，一种面向通用与定制计算平台的统一高效Transformer架构，在GPU上实现SOTA精度与延迟，并在FPGA上展现良好适应性。


<details>
  <summary>Details</summary>
Motivation: 现有Transformer模型在通用与定制计算平台（如GPU与FPGA/ASIC）之间部署时，因并行计算范式差异导致模型迁移存在复杂性、效率或精度的折衷，且跨平台优化原则研究不足。

Method: 提出UniFormer架构，通过提升并行性与计算-存储融合，实现对通用和定制计算平台的统一支持。

Result: UniFormer在GPU上达到SOTA的准确率与延迟表现，同时在FPGA上展现出强适应性。

Conclusion: UniFormer是首个联合考虑通用与定制计算架构的高效Transformer工作，有效弥合了两类平台间的部署鸿沟。

Abstract: The success of neural networks such as convolutional neural networks (CNNs) has been largely attributed to their effective and widespread deployment on customised computing platforms, including field-programmable gate arrays (FPGAs) and application-specific integrated circuits (ASICs). In the current era, Transformer-based architectures underpin the majority of state-of-the-art (SOTA) larger models that are also increasingly deployed on customised computing hardware for low-power and real-time applications. However, the fundamentally different parallel computation paradigms between general-purpose and customised computing often lead to compromises in model transfer and deployability, which typically come at the cost of complexity, efficiency or accuracy. Moreover, many cross-platform optimisation principles have also remained underexplored in existing studies. This paper introduces UniFormer, a unified and efficient Transformer architecture for both general-purpose and customised computing platforms. By enabling higher parallelism and compute-storage fusion, UniFormer achieves state-of-the-art (SOTA) accuracy and latency on GPUs while exhibiting strong adaptability on FPGAs. To the best of our knowledge, this paper is the first efficient Transformer work that jointly considers both general-purpose and customised computing architectures.

</details>


### [11] [ProbSelect: Stochastic Client Selection for GPU-Accelerated Compute Devices in the 3D Continuum](https://arxiv.org/abs/2511.08147)
*Andrija Stanisic,Stefan Nastic*

Main category: cs.DC

TL;DR: 本文提出ProbSelect，一种无需历史数据或持续监控的客户端选择方法，通过分析建模与概率预测在GPU加速设备上实现高效联邦学习，在满足用户定义SLO的同时显著提升合规性并减少计算浪费。


<details>
  <summary>Details</summary>
Motivation: 在边缘、云和空间设备构成的3D连续体中，传统联邦学习客户端选择方法依赖持续监控和历史数据，在动态环境中（如卫星和移动设备频繁变化）难以适用；同时现有方法主要基于CPU计算，无法有效处理广泛存在的GPU加速训练场景。

Method: 提出ProbSelect方法，结合解析建模与概率预测，在无需历史数据或持续监控的前提下，针对GPU加速设备进行客户端选择，并将其建模为满足用户定义服务等级目标（SLO）的问题。

Result: 在多种GPU架构和工作负载下的实验表明，ProbSelect相比基线方法平均提升13.77%的SLO合规率，并减少72.5%的计算浪费。

Conclusion: ProbSelect有效解决了3D连续体中动态环境下GPU加速联邦学习的客户端选择难题，在不依赖历史数据的情况下显著提升了系统效率与服务质量。

Abstract: Integration of edge, cloud and space devices into a unified 3D continuum imposes significant challenges for client selection in federated learning systems. Traditional approaches rely on continuous monitoring and historical data collection, which becomes impractical in dynamic environments where satellites and mobile devices frequently change operational conditions. Furthermore, existing solutions primarily consider CPU-based computation, failing to capture complex characteristics of GPU-accelerated training that is prevalent across the 3D continuum. This paper introduces ProbSelect, a novel approach utilizing analytical modeling and probabilistic forecasting for client selection on GPU-accelerated devices, without requiring historical data or continuous monitoring. We model client selection within user-defined SLOs. Extensive evaluation across diverse GPU architectures and workloads demonstrates that ProbSelect improves SLO compliance by 13.77% on average while achieving 72.5% computational waste reduction compared to baseline approaches.

</details>


### [12] [Gathering in Vertex- and Edge-Transitive Graphs without Multiplicity Detection under Round Robin](https://arxiv.org/abs/2511.08222)
*Serafino Cicerone,Alessia Di Fonso,Gabriele Di Stefano,Alfredo Navarra*

Main category: cs.DC

TL;DR: 本文研究了在OBLOT模型下、受限于顶点和边传递图结构中的机器人聚集问题，假设初始配置可能包含多重占据且机器人无法感知多重性，在轮询调度机制下提出了针对无限网格和超立方体两种拓扑的时间最优聚集算法，并指出可能不存在适用于所有可解情形的通用算法。


<details>
  <summary>Details</summary>
Motivation: 经典聚集问题通常假设机器人能感知多重性或初始无多重占据，而本文考虑更“敌对”的设定：初始存在多重占据、机器人无法检测多重性，且运动受限于顶点与边传递图。为探索在此限制下聚集是否可行，作者开展此项研究。

Method: 在轮询（round-robin）调度机制下，利用无限网格和超立方体这两种特定顶点-边传递图的对称性和结构性质，分别设计两个时间最优的分布式聚集算法，并分析基本不可能性结果。

Result: 针对无限网格和超立方体，提出了两种时间最优的聚集算法；同时证明了一些基本的不可能性结果，并推测不存在适用于所有可解顶点-边传递图的通用聚集算法。

Conclusion: 在所设定的严苛条件下，虽然对某些特定拓扑（如无限网格和超立方体）可以实现时间最优的聚集，但因算法高度依赖底层图结构特性，很可能不存在适用于所有可解情形的通用解法。

Abstract: In the field of swarm robotics, one of the most studied problem is Gathering. It asks for a distributed algorithm that brings the robots to a common location, not known in advance. We consider the case of robots constrained to move along the edges of a graph under the well-known OBLOT model. Gathering is then accomplished once all the robots occupy a same vertex. Differently from classical settings, we assume: i) the initial configuration may contain multiplicities, i.e. more than one robot may occupy the same vertex; ii) robots cannot detect multiplicities; iii) robots move along the edges of vertex- and edge-transitive graphs, i.e. graphs where all the vertices (and the edges, resp.) belong to a same class of equivalence. To balance somehow such a `hostile' setting, as a scheduler for the activation of the robots, we consider the round-robin, where robots are cyclically activated one at a time.
  We provide some basic impossibility results and we design two different algorithms approaching the Gathering for robots moving on two specific topologies belonging to edge- and vertex-transitive graphs: infinite grids and hypercubes. The two algorithms are both time-optimal and heavily exploit the properties of the underlying topologies. Because of this, we conjecture that no general algorithm can exist for all the solvable cases.

</details>


### [13] [Priority Matters: Optimising Kubernetes Clusters Usage with Constraint-Based Pod Packing](https://arxiv.org/abs/2511.08373)
*Henrik Daniel Christensen,Saverio Giallorenzo,Jacopo Mauro*

Main category: cs.DC

TL;DR: 本文提出一种基于约束规划的Kubernetes调度插件，在默认调度器失败时作为后备机制，能在1秒内优化44%以上场景中的高优先级Pod分配，并在10秒内提升至73%，同时验证默认调度器在约19%场景中已是最优。


<details>
  <summary>Details</summary>
Motivation: Kubernetes默认调度器使用轻量级启发式算法，可能导致次优的Pod放置和资源碎片化，从而无法部署本可运行的Pod。

Method: 采用约束规划方法，利用OR-Tools约束求解器实现一个调度插件，作为默认调度器的后备机制，在调度失败时寻找满足所有优先级和资源需求的最优Pod分配方案。

Result: 在小到中型集群实验中，1秒调度窗口下，该方法在超过44%的可实现场景中优于默认调度器；10秒窗口下提升至73%；同时在约19%的场景中验证默认调度器结果已是最优。

Conclusion: 将约束规划引入Kubernetes调度可显著提升资源利用率与高优先级任务的调度成功率，且能有效评估默认调度器的最优性。

Abstract: Distributed applications employ Kubernetes for scalable, fault-tolerant deployments over computer clusters, where application components run in groups of containers called pods. The scheduler, at the heart of Kubernetes' architecture, determines the placement of pods given their priority and resource requirements on cluster nodes. To quickly allocate pods, the scheduler uses lightweight heuristics that can lead to suboptimal placements and resource fragmentation, preventing allocations of otherwise deployable pods on the available nodes.
  We propose the usage of constraint programming to find the optimal allocation of pods satisfying all their priorities and resource requests. Implementation-wise, our solution comes as a plug-in to the default scheduler that operates as a fallback mechanism when some pods cannot be allocated. Using the OR-Tools constraint solver, our experiments on small-to-mid-sized clusters indicate that, within a 1-second scheduling window, our approach places more higher-priority pods than the default scheduler (possibly demonstrating allocation optimality) in over 44\% of realisable allocation scenarios where the default scheduler fails, while certifying that the default scheduler's placement is already optimal in over 19\% of scenarios. With a 10-second window, our approach improves placements in over 73\% and still certifies that the default scheduler's placement is already optimal in over 19\% of scenarios.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [14] [A Historical Interaction-Enhanced Shapley Policy Gradient Algorithm for Multi-Agent Credit Assignment](https://arxiv.org/abs/2511.07778)
*Ao Ding,Licheng Sun,Yongjie Hou,Huaqing Zhang,Hongbin Ma*

Main category: cs.MA

TL;DR: 本文提出了一种名为HIS的多智能体信用分配算法，通过结合历史交互数据与Shapley值，在保持训练稳定性的同时更准确地评估个体贡献，在多个连续动作基准环境中表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 传统多智能体强化学习中的信用分配方法难以在强耦合任务中可靠捕捉个体贡献，同时保持训练稳定性，限制了算法的泛化能力和性能。

Method: 提出一种历史交互增强的Shapley策略梯度算法（HIS），采用混合信用分配机制，结合基础奖励与个体贡献激励，并利用历史交互数据高效计算Shapley值，同时保留全局奖励以维持训练稳定性。

Result: 在Multi-Agent Particle Environment、Multi-Agent MuJoCo和Bi-DexHands三个连续动作基准环境中的实验表明，HIS在强耦合复杂协作任务中显著优于当前最先进的方法。

Conclusion: HIS通过理论保证的混合信用分配机制，有效提升了多智能体系统在复杂协作场景下的性能与稳定性，为多智能体信用分配问题提供了新的解决思路。

Abstract: Multi-agent reinforcement learning (MARL) has demonstrated remarkable performance in multi-agent collaboration problems and has become a prominent topic in artificial intelligence research in recent years. However, traditional credit assignment schemes in MARL cannot reliably capture individual contributions in strongly coupled tasks while maintaining training stability, which leads to limited generalization capabilities and hinders algorithm performance. To address these challenges, we propose a Historical Interaction-Enhanced Shapley Policy Gradient Algorithm (HIS) for Multi-Agent Credit Assignment, which employs a hybrid credit assignment mechanism to balance base rewards with individual contribution incentives. By utilizing historical interaction data to calculate the Shapley value in a sample-efficient manner, HIS enhances the agent's ability to perceive its own contribution, while retaining the global reward to maintain training stability. Additionally, we provide theoretical guarantees for the hybrid credit assignment mechanism, ensuring that the assignment results it generates are both efficient and stable. We evaluate the proposed algorithm in three widely used continuous-action benchmark environments: Multi-Agent Particle Environment, Multi-Agent MuJoCo, and Bi-DexHands. Experimental results demonstrate that HIS outperforms state-of-the-art methods, particularly excelling in strongly coupled, complex collaborative tasks.

</details>


### [15] [Can LLM Agents Really Debate? A Controlled Study of Multi-Agent Debate in Logical Reasoning](https://arxiv.org/abs/2511.07784)
*Haolun Wu,Zhenkun Li,Lingyao Li*

Main category: cs.MA

TL;DR: 该研究通过骑士-骗子-间谍逻辑谜题，系统评估多智能体辩论（MAD）中结构与认知因素对大语言模型集体推理效果的影响，发现内在推理能力和群体多样性是成功关键，而多数压力会抑制独立纠错。


<details>
  <summary>Details</summary>
Motivation: 澄清大语言模型在多智能体辩论中是否真正具备审议式推理能力，而非仅依赖简单集成或多数投票，并深入理解影响其集体推理成败的关键因素。

Method: 在具有可验证真值的Knight–Knave–Spy逻辑谜题上进行受控实验，系统操控六个结构性和认知性因素（团队规模、组成、置信度可见性、辩论顺序、辩论深度、任务难度），并结合结果与过程层面的分析。

Result: 内在推理能力和群体多样性是辩论成功的主导因素；结构性参数（如顺序、置信度可见性）增益有限；过程分析揭示多数压力抑制独立纠错、有效团队能推翻错误共识、基于有效性的理性推理最能预测改进。

Conclusion: LLM多智能体辩论的成功依赖于个体能力与多样性，而非表面结构设计；研究为构建可解释、求真的多智能体推理系统提供了实证依据和设计指导。

Abstract: Multi-agent debate (MAD) has recently emerged as a promising framework for improving the reasoning performance of large language models (LLMs). Yet, whether LLM agents can genuinely engage in deliberative reasoning, beyond simple ensembling or majority voting, remains unclear. We address this question through a controlled study using the Knight--Knave--Spy logic puzzle, which enables precise, step-wise evaluation of debate outcomes and processes under verifiable ground truth. We systematically set up six structural and cognitive factors, including agent team size, composition, confidence visibility, debate order, debate depth, and task difficulty, to disentangle their respective effects on collective reasoning. Our results show that intrinsic reasoning strength and group diversity are the dominant drivers of debate success, while structural parameters such as order or confidence visibility offer limited gains. Beyond outcomes, process-level analyses identify key behavioral patterns: majority pressure suppresses independent correction, effective teams overturn incorrect consensus, and rational, validity-aligned reasoning most strongly predicts improvement. These findings provide valuable insights into how and why LLM debates succeed or fail, offering guidance for designing interpretable and truth-seeking multi-agent reasoning systems.

</details>


### [16] [Climate Driven Interactions Between Malaria Transmission and Diabetes Prevalence](https://arxiv.org/abs/2511.08562)
*Shivank,Anurag Singha,Fakhteh Ghanbarnejad,Ajay K Sharma*

Main category: cs.MA

TL;DR: 本文构建了一个新的区室流行病学模型，结合印度2019–2021年数据，揭示在气候变化背景下糖尿病患者感染疟疾的风险显著高于非糖尿病人群，并强调需制定兼顾疟疾与糖尿病的气候适应型公共卫生策略。


<details>
  <summary>Details</summary>
Motivation: 气候变化加剧了疟疾和糖尿病等传染性与慢性疾病的负担，尤其对脆弱人群影响显著。现有模型很少同时考虑这两种疾病在气候变化下的相互作用，因此亟需一个整合二者动态关系的新建模框架。

Method: 作者基于合成数据构建了一个三区室流行病学模型，纳入温度依赖的传播参数、季节变异性和糖尿病与非糖尿病人群的疾病动态差异；使用多起点优化结合序列二次规划进行模型校准。

Result: 模型显示糖尿病患者的疟疾感染几率是非糖尿病人群的1.8–4.0倍，峰值感染率分别为35–36%与20–21%；基本再生数在不同季节介于0.31至2.75之间，平均约为2.3。

Conclusion: 鉴于印度糖尿病人口预计到2050年将达1.57亿，研究结果凸显了发展整合疟疾与糖尿病防控的气候智能型健康监测与干预体系的紧迫性。

Abstract: Climate change is intensifying infectious and chronic diseases like malaria and diabetes, respectively, especially among the vulnerable populations. Global temperatures have risen by approximately $0.6^\circ$C since 1950, extending the window of transmission for mosquito-borne infections and worsening outcomes in diabetes due to metabolic stress caused by heat. People living with diabetes have already weakened immune defenses and, therefore, are at an alarmingly increased risk of contraction of malaria. However, most models rarely include both ways of interaction in changing climate conditions. In the paper, we introduce a new compartmental epidemiological model based on synthetic data fitted to disease patterns of India from 2019 to 2021. The framework captures temperature-dependent transmission parameters, seasonal variability, and different disease dynamics between diabetic and non-diabetic groups within the three-compartment system. Model calibration using Multi-Start optimization combined with Sequential Quadratic Programming allows us to find outstanding differences between populations. The odds of malaria infection in diabetic individuals were found to be 1.8--4.0 times higher, with peak infection levels in 35--36\%, as compared to 20--21\% in the non-diabetic ones. The fitted model was able to capture well the epidemiological patterns observed, while the basic reproduction number averaged around 2.3, ranging from 0.31 to 2.75 in different seasons. Given that India's diabetic population is set to rise to about 157 million people by 2050, these findings point to a pressing need for concerted efforts toward climate-informed health strategies and monitoring systems that address both malaria and diabetes jointly.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [17] [A Large-Scale Dataset and Reproducible Framework for RF Fingerprinting on IEEE 802.11g Same-Model Devices](https://arxiv.org/abs/2511.07770)
*Zewei Guo,Zhen Jia,JinXiao Zhu,Wenhao Huang,Yin Chen*

Main category: cs.NI

TL;DR: 本文提出了一个大规模同型号设备的射频指纹数据集及开源可复现实验框架，包含123个相同商用IEEE 802.11g设备的3542万原始I/Q样本和185万射频特征，并基于随机森林算法实现了89.06%的识别准确率。


<details>
  <summary>Details</summary>
Motivation: 现有射频指纹数据集受限于设备数量少且型号异构，难以支持对同型号设备的有效区分和机器学习模型的鲁棒训练与公平评估。

Method: 构建包含123个相同型号IEEE 802.11g设备的大规模数据集，采集3542万原始I/Q样本并提取185万射频特征；开发完全开源、可复现的实验框架，并采用基于随机森林的算法进行设备识别。

Result: 在所提数据集上实现了89.06%的设备识别准确率，并通过大量实验验证了所提取特征之间的关联性。

Conclusion: 该研究通过提供大规模同型号设备数据集与完整可复现框架，显著推动了射频指纹识别领域的发展，尤其在区分高度相似设备方面取得实质性进展。

Abstract: Radio frequency (RF) fingerprinting exploits hardware imperfections for device identification, but distinguishing between same-model devices remains challenging due to their minimal hardware variations. Existing datasets for RF fingerprinting are constrained by small device scales and heterogeneous models, which hinders robust training and fair evaluation for machine learning models. To address this gap, we introduce a large-scale dataset of same-model devices along with a fully reproducible, open-source experimental framework. The dataset is built using 123 identical commercial IEEE 802.11g devices and contains 35.42 million raw I/Q samples from the preambles and corresponding 1.85 million RF features. The open-source framework further ensures full reproducibility from data collection to final evaluation. Within this framework, a Random Forest-based algorithm is proposed to achieve 89.06% identification accuracy on this dataset. Extensive experimental evaluations further confirm the relationships between the extracted features.

</details>


### [18] [SRE-Llama -- Fine-Tuned Meta's Llama LLM, Federated Learning, Blockchain and NFT Enabled Site Reliability Engineering(SRE) Platform for Communication and Networking Software Services](https://arxiv.org/abs/2511.08282)
*Eranga Bandara,Safdar H. Bouk,Sachin Shetty,Ravi Mukkamala,Abdul Rahman,Peter Foytik,Ross Gore,Xueping Liang,Ng Wee Keong,Kasun De Zoysa*

Main category: cs.NI

TL;DR: 本文提出了一种名为SRE-Llama的新平台，结合生成式AI、联邦学习、区块链和NFT技术，旨在自动化并简化云原生环境中SLI/SLO的生成、监控与告警管理，提升开发者在站点可靠性工程（SRE）中的效率与准确性。


<details>
  <summary>Details</summary>
Motivation: 当前许多开发者对Prometheus、Grafana等SRE工具以及SLI/SLO定义缺乏深入理解，导致难以有效保障云原生服务的可靠性，因此需要一个更智能、易用且隐私安全的自动化SRE解决方案。

Method: 平台通过Prometheus和Mimir收集并存储云原生服务指标，利用联邦学习识别关键SLI指标以保护数据隐私；再基于微调后的Llama-3大语言模型自动生成SLI、SLO、错误预算及告警规则；最后将生成的SLI/SLO编码为NFT并存入区块链，由智能合约驱动整个自动化流程。

Result: 实现了SRE-Llama原型系统，并在定制化的Open5GS 5G核心网用例中验证了其在SLI/SLO自动化生成、监控与审计方面的可行性与有效性。

Conclusion: SRE-Llama平台通过融合多种前沿技术，显著降低了SRE实践门槛，提升了SLI/SLO管理的智能化、可审计性与隐私安全性，为云原生系统的可靠性保障提供了新范式。

Abstract: Software services are crucial for reliable communication and networking; therefore, Site Reliability Engineering (SRE) is important to ensure these systems stay reliable and perform well in cloud-native environments. SRE leverages tools like Prometheus and Grafana to monitor system metrics, defining critical Service Level Indicators (SLIs) and Service Level Objectives (SLOs) for maintaining high service standards. However, a significant challenge arises as many developers often lack in-depth understanding of these tools and the intricacies involved in defining appropriate SLIs and SLOs. To bridge this gap, we propose a novel SRE platform, called SRE-Llama, enhanced by Generative-AI, Federated Learning, Blockchain, and Non-Fungible Tokens (NFTs). This platform aims to automate and simplify the process of monitoring, SLI/SLO generation, and alert management, offering ease in accessibility and efficy for developers. The system operates by capturing metrics from cloud-native services and storing them in a time-series database, like Prometheus and Mimir. Utilizing this stored data, our platform employs Federated Learning models to identify the most relevant and impactful SLI metrics for different services and SLOs, addressing concerns around data privacy. Subsequently, fine-tuned Meta's Llama-3 LLM is adopted to intelligently generate SLIs, SLOs, error budgets, and associated alerting mechanisms based on these identified SLI metrics. A unique aspect of our platform is the encoding of generated SLIs and SLOs as NFT objects, which are then stored on a Blockchain. This feature provides immutable record-keeping and facilitates easy verification and auditing of the SRE metrics and objectives. The automation of the proposed platform is governed by the blockchain smart contracts. The proposed SRE-Llama platform prototype has been implemented with a use case featuring a customized Open5GS 5G Core.

</details>


### [19] [Demystifying QUIC from the Specifications](https://arxiv.org/abs/2511.08375)
*Darius Saif,Ashraf Matrawy*

Main category: cs.NI

TL;DR: 本文旨在以完整且易于理解的方式介绍QUIC协议，帮助读者克服其快速演进、RFC文档复杂以及跨层隐私设计带来的学习障碍。


<details>
  <summary>Details</summary>
Motivation: QUIC协议因其快速演进、相关RFC文档组织与语言复杂，以及跨层和注重隐私的实现方式，使初学者难以理解和调试，因此需要一篇通俗易懂的综述性文章来厘清其核心内容。

Method: 通过系统梳理QUIC协议的标准演进、关键特性及其与HTTP/3的关系，并对相关RFC进行整合解读，以清晰、连贯的方式呈现协议全貌。

Result: 为读者提供了一个全面而易于理解的QUIC协议介绍，有效降低了学习门槛，有助于理解其工作机制与设计思想。

Conclusion: QUIC作为下一代Web浏览的核心传输协议，尽管复杂，但通过结构化和简明的阐述可以被有效掌握；本文成功实现了对该协议的“去神秘化”。

Abstract: QUIC is an advanced transport layer protocol whose ubiquity on the Internet is now very apparent. Importantly, QUIC fuels the next generation of web browsing: HTTP/3. QUIC is a stateful and connection oriented protocol which offers similar features (and more) to the combination of TCP and TLS. There are several difficulties which readers may encounter when learning about QUIC: i.) its rapid evolution (particularly, differentiation between the QUIC standard and the now deprecated Google QUIC), ii.) numerous RFCs whose organization, language, and detail may be challenging to the casual reader, and iii.) the nature of QUIC's cross-layer and privacy-centric implementation, making it impossible to understand or debug by looking at packets alone. For these reasons, the aim of this paper is to present QUIC in a complete yet approachable fashion, thereby demystifying the protocol from its specifications.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [20] [A Service Suite for Specifying Digital Twins for Industry 5.0](https://arxiv.org/abs/2511.07506)
*Izaque Esteves,Regina Braga,José Maria David,Victor Stroele*

Main category: cs.SE

TL;DR: 本文提出了一套名为DT-Create的服务套件，用于在预测性维护中构建数字孪生（DT），以支持基于传感器数据的智能决策。


<details>
  <summary>Details</summary>
Motivation: 预测性维护面临如何基于数据敏捷且准确地做出决策的挑战，而数字孪生可提供实时物理设备的表示并生成可用于决策的数据。

Method: 采用设计科学研究（DSR）方法，通过两个开发周期构建DT-Create套件，并结合智能技术、语义数据处理和自适应机制，通过案例研究进行评估。

Result: 结果表明DT-Create在以下方面具有可行性：(i) 传感器数据的采集、存储与智能处理；(ii) 利用机器学习和本体论丰富信息；(iii) 智能选择适配数据集的预测模型；(iv) 支持决策与自适应能力。

Conclusion: DT-Create套件有效支持了面向预测性维护的数字孪生构建，提升了数据驱动决策的准确性与敏捷性。

Abstract: One of the challenges of predictive maintenance is making decisions based on data in an agile and assertive way. Connected sensors and operational data favor intelligent processing techniques to enrich information and enable decision-making. Digital Twins (DTs) can be used to process information and support decision-making. DTs are a real-time representation of physical machines and generate data that predictive maintenance can use to make assertive and quick decisions. The main contribution of this work is the specification of a suite of services for specifying DTs, called DT-Create, focused on decision support in predictive maintenance. DT-Create suite is based on intelligent techniques, semantic data processing, and self-adaptation. This suite was developed using the Design Science Research (DSR) methodology through two development cycles and evaluated through case studies. The results demonstrate the feasibility of using DT-Create in specifying DTs considering the following aspects: (i) collection, storage, and intelligent processing of data generated by sensors, (ii) enrichment of information through machine learning and ontologies, (iii) use of intelligent techniques to select predictive models that adhere to the available data set, and (iv) decision support and self-adaptation.

</details>


### [21] [An Exploratory Eye Tracking Study on How Developers Classify and Debug Python Code in Different Paradigms](https://arxiv.org/abs/2511.07612)
*Samuel W. Flint,Jigyasa Chauhan,Niloofar Mansoor,Bonita Sharif,Robert Dyer*

Main category: cs.SE

TL;DR: 该研究通过眼动追踪实验探索Python中不同编程范式（面向对象、过程式、函数式）对开发者代码分类与调试能力的影响，发现函数式代码更难理解且耗时更长，但范式转换并未显著影响调试正确率。


<details>
  <summary>Details</summary>
Motivation: 现有研究表明某些编程范式编写的代码更难理解，但尚无研究探讨具体是哪些范式相关的语言特性影响了代码的理解与调试。本文旨在填补这一空白。

Method: 开展了一项探索性的眼动追踪实证研究，招募29名开发者（主要为学生），完成4个代码范式分类任务和4个调试任务，记录其眼动数据以分析阅读模式。

Result: 参与者在区分函数式与过程式范式时存在混淆，但对面向对象范式识别较清晰；函数式代码任务耗时最长；范式变化未显著影响调试正确率，但开发者对函数式代码的自评信心较低；眼动数据显示调试函数式代码时阅读模式有显著差异，且分类时开发者未必关注范式相关的关键语言元素。

Conclusion: 尽管编程范式的改变未直接影响调试准确性，但函数式范式显著增加了认知负担和理解难度，表明特定语言特性可能影响开发者对多范式代码的理解效率。

Abstract: Modern programming languages, such as Python, support language features from several paradigms, such as object-oriented, procedural, and functional. Research has shown that code written in some paradigms can be harder to comprehend, but to date, no research has looked at which paradigm-specific language features impact comprehension. To this end, this study seeks to uncover which paradigm-specific features impactcomprehension and debugging of code or how multi-paradigm code might affect a developer's ability to do so. We present an exploratory empirical eye-tracking study to investigate 1) how developers classify the predominant paradigm in Python code and 2) how the paradigm affects their ability to debug Python code. The goal is to uncover if specific language features are looked at more often while classifying and debugging code with a predominant paradigm. Twenty-nine developers (primarily students) were recruited for the study and were each given four classification and four debugging tasks in Python. Eye movements were recorded during all the tasks. The results indicate confusion in labeling Functional and Procedural paradigms, but not Object-Oriented. The code with predominantly functional paradigms also took the longest to complete. Changing the predominant paradigm did not affect the ability to debug the code, though developers did rate themselves with lower confidence for Functional code. We report significant differences in reading patterns during debugging, especially in the Functional code. During classification, results show that developers do not necessarily read paradigm-relevant token types.

</details>


### [22] [A Self-Improving Architecture for Dynamic Safety in Large Language Models](https://arxiv.org/abs/2511.07645)
*Tyler Slater*

Main category: cs.SE

TL;DR: 本文提出了一种名为自改进安全框架（SISF）的新型运行时架构，使AI系统能在运行过程中自主检测安全漏洞并动态生成新的安全策略，从而显著降低攻击成功率且不产生误报。


<details>
  <summary>Details</summary>
Motivation: 现有软件架构模式静态且安全机制不可扩展，难以应对大语言模型（LLM）集成带来的新型对抗威胁，亟需一种能动态适应的安全架构。

Method: SISF架构结合一个未受保护的基础LLM（Mistral-7B-v0.1）、一个用于检测违规的AI仲裁器（GPT-4o）和一个策略合成模块（GPT-4 Turbo），后者能根据失败案例自动生成新的启发式与语义安全策略。

Result: 在AdvBench数据集上的实验表明，SISF从零策略开始，成功检测237次攻击、生成234条新策略，将攻击成功率从100%降至45.58%，并在良性提示测试中实现0.00%的误报率。

Conclusion: 基于自适应原则的架构化AI安全方法是可行且有效的，能够将安全保证从静态预部署转变为自动化运行时过程，提升AI系统的鲁棒性与可扩展性。

Abstract: Context: The integration of Large Language Models (LLMs) into core software systems is accelerating. However, existing software architecture patterns are static, while current safety assurance methods are not scalable, leaving systems vulnerable to novel adversarial threats.
  Objective: To design, implement, and evaluate a novel software architecture that enables an AI-driven system to autonomously and continuously adapt its own safety protocols at runtime.
  Method: We propose the Self-Improving Safety Framework (SISF), a runtime architecture that couples an unprotected, unaligned base LLM (mistralai/Mistral-7B-v0.1) with a dynamic feedback loop. This loop consists of an AI Adjudicator (GPT-4o) for breach detection and a Policy Synthesis Module (GPT-4 Turbo) that autonomously generates new, generalized safety policies (both heuristic and semantic) in response to failures.
  Results: We conducted a dynamic learning evaluation using the 520-prompt AdvBench dataset. The unprotected model was 100% vulnerable. Our SISF, starting from zero policies, demonstrated a clear learning curve: it detected 237 breaches, autonomously synthesized 234 new policies, and reduced the overall Attack Success Rate (ASR) to 45.58%. In a subsequent test on 520 benign prompts, the SISF achieved a 0.00% False Positive Rate (FPR), proving its ability to adapt without compromising user utility.
  Conclusion: An architectural approach to AI safety, based on the principles of self-adaptation, is a viable and effective strategy. Our framework demonstrates a practical path towards building more robust, resilient, and scalable AI-driven systems, shifting safety assurance from a static, pre-deployment activity to an automated, runtime process.

</details>


### [23] [Smart but Costly? Benchmarking LLMs on Functional Accuracy and Energy Efficiency](https://arxiv.org/abs/2511.07698)
*Mohammadjavad Mehditabar,Saurabhsingh Rajput,Antonio Mastropaolo,Tushar Sharma*

Main category: cs.SE

TL;DR: 本文提出了BRACE框架，用于在统一尺度上评估代码语言模型（CLM）在能耗与功能正确性（准确性）之间的权衡，并引入两种评级方法CIRC和OTER对22个主流模型进行基准测试。


<details>
  <summary>Details</summary>
Motivation: 现有研究缺乏系统性框架来评估代码语言模型在准确性和能耗之间的权衡，而AI技术在软件开发中的快速应用亟需对其环境影响进行系统评估。

Method: 提出BRACE框架，包含两种评级方法：基于欧氏距离、对异常值鲁棒的静态权衡方法CIRC，以及能捕捉能耗与准确性复杂关系的动态趋势感知方法OTER；在代码生成与摘要任务上对22个模型进行基准测试，并按1-5分制评级。

Result: 实验发现模型在代码摘要任务中表现更优（因无需生成语法正确的代码），且模型规模对其评级无显著影响，表明参数利用效率比模型大小更重要。

Conclusion: BRACE框架支持开发者根据部署需求（确定性比较或趋势感知评估）选择合适模型，在可持续性与任务性能之间取得平衡。

Abstract: The rapid advancement of AI technologies and their accelerated adoption in software development necessitates a systematic evaluation of their environmental impact alongside functional correctness. While prior studies have examined sustainability in large language models, existing approaches lack systematic frameworks for evaluating accuracy-energy trade-offs in Code Language Models (CLMs). In this paper, we present a framework, BRACE, to benchmark CLMs on a unified scale of energy efficiency and functional correctness (referred to as accuracy). We benchmark 22 state-of-the-art models on code generation and summarization tasks, proposing two rating methods: Concentric Incremental Rating Circles (CIRC) and Observation to Expectation Rating (OTER). CIRC provides deterministic Euclidean-based rankings with static trade-offs that are robust to outliers, and OTER offers trend-aware evaluation with dynamic trade-offs that capture the complex correlation between energy and accuracy, each offering a distinct perspective and addressing the problem in a unique way. These rating methods enable us to rate LLMs on a 1-5 scale reflecting their combined capabilities in terms of energy efficiency and functional correctness. Our analysis reveals models generally perform better in the code summarization tasks as they are not enforced to generate a grammar-based and syntactically correct output. Also, we find that models' size does not have a significant impact on their ratings, indicating that if models utilize their parameters efficiently, they can be ranked higher on these scales. The proposed BRACE framework empowers practitioners to make evidence-based model selections that balance sustainability with task requirements, guiding rating choice -- CIRC for deterministic comparisons or OTER for trend-aware evaluation -- based on deployment priorities.

</details>


### [24] [Post Processing Graphical User Interface for Heat Flow Visualization](https://arxiv.org/abs/2511.07709)
*Lars Olt,Luis Diego Fonseca Flores,Ian Mckinley*

Main category: cs.SE

TL;DR: 本文介绍了一个基于MATLAB和C++开发的图形用户界面（GUI），利用Thermal Desktop的API（OpenTD）和自定义解析器，高效提取并可视化热模型中的温度、导热系数和子模型指标，显著提升热工程师分析效率。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏有效工具从Thermal Desktop中提取和可视化热流相关指标，限制了热工程师快速分析结果的能力。

Method: 开发了一个结合MATLAB与C++的GUI，通过调用Thermal Desktop的OpenTD API，并利用其压缩求解结果（CSR）文件的副作用，高效加载温度、导热系数及子模型数据。

Result: 该方法将模型节点、导体与子模型ID关联的运行时间减少了多个数量级，显著提升了数据处理效率。

Conclusion: 尽管该方法在数据读取方面存在局限性，但为未来GUI的发展提供了方向，并对后续OpenTD版本提出了改进建议。

Abstract: Thermal Desktop (TD) is an industry-standard thermal analysis tool used to create and analyze thermal models for landers, rovers, spacecraft, and instrument payloads. Currently, limited software exists to extract and visualize metrics relevant to heat flow within TD, impeding thermal engineers from analyzing their results quickly. This paper discusses a graphical user interface (GUI) built in MATLAB and C++ which uses TDs application programming interface (API), OpenTD, and a custom parser to address this void. Specifically, we present a method for efficiently loading temperature, conductance, and submodel metrics using a side effect of TDs Compressed Solution Results (CSR) files. This approach can reduce the runtime for correlating model nodes and conductors with submodel IDs by orders of magnitude. Lastly, we reflect on the shortcomings of this method for reading data, consider the future of the GUI, and provide recommendations for subsequent OpenTD releases.

</details>


### [25] [Event-Driven Inconsistency Detection Between UML Class and Sequence Diagrams](https://arxiv.org/abs/2511.07742)
*Luan Lazzari,Kleinner Farias*

Main category: cs.SE

TL;DR: 本文介绍了Harmony Validator，一个集成于Papyrus建模环境的插件工具，用于实时检测UML类图和序列图中的不一致性，以支持软件工程教学中的建模学习。


<details>
  <summary>Details</summary>
Motivation: 软件建模需要抽象、保持一致性和精确沟通等技能，这些技能难以掌握且教学困难；师生常难以理解和管理建模过程中的不一致性问题。

Method: 开发并集成了名为Harmony Validator的插件工具，采用事件驱动架构，在Papyrus环境中实时监控建模操作并即时报告不一致性；并通过软件工程课程中的学生案例研究评估其教学效果。

Result: 案例研究表明，Harmony Validator有助于学生更好地理解模型一致性，并促进反思性学习实践。

Conclusion: Harmony Validator通过实时检测和反馈机制有效支持了软件建模教育，提升了学生对模型完整性的认知与建模能力。

Abstract: Modeling is a central and demanding activity in software engineering that requires skills such as abstraction, consistency maintenance, and precise communication. These skills are difficult to master and even harder to teach effectively. Educators and students often struggle to understand and manage inconsistencies that arise during the modeling process. To address this challenge, we present \texttt{Harmony Validator}, a tool integrated as a plugin for the Papyrus modeling environment, designed to automatically detect and report inconsistencies in UML models, including class and sequence diagrams. The tool adopts an event-driven architecture that continuously monitors modeling actions and notifies users of emerging inconsistencies in real time. This approach enhances awareness of model integrity and supports the iterative refinement of design artifacts. The paper describes the architecture, detection mechanisms, and usage scenarios of Harmony Validator. It also includes a case study conducted with students in a software engineering course to evaluate the perceived usefulness and benefits of UML modeling in teaching and learning. Our results indicate that Harmony Validator fosters a better understanding of model consistency and promotes reflective learning practices in software modeling education.

</details>


### [26] [Uncovering Scientific Software Sustainability through Community Engagement and Software Quality Metrics](https://arxiv.org/abs/2511.07851)
*Sharif Ahmed,Addi Malviya Thakur,Gregory R. Watson,Nasir U. Eisty*

Main category: cs.SE

TL;DR: 该论文研究了GitHub上科学开源软件（Sci-OSS）项目的可持续性，聚焦社区参与和软件质量两个维度，提出了一种新的可视化方法以综合展示软件指标的动态变化，并通过统计与自然语言分析揭示不同项目在可持续性策略上的差异。


<details>
  <summary>Details</summary>
Motivation: 科学开源软件对科研至关重要，但其长期可持续性面临挑战。现有研究缺乏对可持续性因素的系统性度量与可视化手段，难以支持项目维护者、资助方和开发者做出有效决策。

Method: 作者从文献中提取与可持续性相关的仓库指标，对十个知名Sci-OSS项目进行数据挖掘，并结合多模态分析（包括统计分析和自然语言分析）来评估社区参与和软件质量；同时开发了一种新型可视化技术，用于统一展示随时间演变的软件指标。

Result: 研究发现，即使在同一领域，不同项目的可持续性路径也存在显著差异；自然语言分析验证了项目特定反馈对软件质量的关键作用；所提出的可视化方法能有效替代多个传统图表，直观呈现软件可持续性状态。

Conclusion: 该研究为理解和提升科学开源软件的可持续性提供了可操作的分析框架与可视化工具，有助于利益相关者更好地支持和维护长期科研软件项目。

Abstract: Scientific open-source software (Sci-OSS) projects are critical for advancing research, yet sustaining these projects long-term remains a major challenge. This paper explores the sustainability of Sci-OSS hosted on GitHub, focusing on two factors drawn from stewardship organizations: community engagement and software quality. We map sustainability to repository metrics from the literature and mined data from ten prominent Sci-OSS projects. A multimodal analysis of these projects led us to a novel visualization technique, providing a robust way to display both current and evolving software metrics over time, replacing multiple traditional visualizations with one. Additionally, our statistical analysis shows that even similar-domain projects sustain themselves differently. Natural language analysis supports claims from the literature, highlighting that project-specific feedback plays a key role in maintaining software quality. Our visualization and analysis methods offer researchers, funders, and developers key insights into long-term software sustainability.

</details>


### [27] [Testing Question Answering Software with Context-Driven Question Generation](https://arxiv.org/abs/2511.07924)
*Shuang Liu,Zhirun Zhang,Jinhao Dong,Zan Wang,Qingchao Shen,Junjie Chen,Wei Lu,Xiaoyong Du*

Main category: cs.SE

TL;DR: 本文提出了一种名为CQ²A的上下文驱动问答系统测试方法，通过从上下文中提取实体和关系生成更自然、多样且贴近真实场景的问题，并结合大语言模型与一致性验证机制提升问题质量。实验表明该方法在缺陷检测能力、问题自然度和上下文覆盖率方面优于现有技术，且生成的测试用例还能有效降低被测问答系统的错误率。


<details>
  <summary>Details</summary>
Motivation: 现有基于元关系的问答系统测试方法存在两个主要问题：一是生成的问题不够自然，难以反映真实用户提问；二是依赖已有测试数据集，缺乏对更广泛上下文的利用，导致问题多样性与相关性受限。

Method: CQ²A方法从上下文中提取实体与关系以构建标准答案，并利用大语言模型结合上下文和标准答案生成问题。同时引入一致性验证和约束检查机制，提高大语言模型输出的可靠性。

Result: 在三个数据集上的实验表明，CQ²A在缺陷检测能力、生成问题的自然度以及上下文覆盖率方面均优于当前最先进的方法。此外，其生成的测试用例用于微调问答系统时可有效降低错误率。

Conclusion: CQ²A是一种有效的上下文驱动问答系统测试方法，不仅能生成更贴近真实使用场景的高质量测试问题，还能提升问答系统的整体性能和鲁棒性。

Abstract: Question-answering software is becoming increasingly integrated into our daily lives, with prominent examples including Apple Siri and Amazon Alexa. Ensuring the quality of such systems is critical, as incorrect answers could lead to significant harm. Current state-of-the-art testing approaches apply metamorphic relations to existing test datasets, generating test questions based on these relations. However, these methods have two key limitations. First, they often produce unnatural questions that humans are unlikely to ask, reducing the effectiveness of the generated questions in identifying bugs that might occur in real-world scenarios. Second, these questions are generated from pre-existing test datasets, ignoring the broader context and thus limiting the diversity and relevance of the generated questions.
  In this work, we introduce CQ^2A, a context-driven question generation approach for testing question-answering systems. Specifically, CQ^2A extracts entities and relationships from the context to form ground truth answers, and utilizes large language models to generate questions based on these ground truth answers and the surrounding context. We also propose the consistency verification and constraint checking to increase the reliability of LLM's outputs. Experiments conducted on three datasets demonstrate that CQ^2A outperforms state-of-the-art approaches on the bug detection capability, the naturalness of the generated questions as well as the coverage of the context. Moreover, the test cases generated by CQ^2A reduce error rate when utilized for fine-tuning the QA software under test

</details>


### [28] [A Small Leak Sinks All: Exploring the Transferable Vulnerability of Source Code Models](https://arxiv.org/abs/2511.08127)
*Weiye Li,Wenyi Tang*

Main category: cs.SE

TL;DR: 该论文系统研究了传统源代码模型（SCMs）与大语言模型用于代码（LLM4Code）之间的可迁移脆弱性，提出了一种无需访问下游分类器的通用对抗样本生成方法HABITAT，并揭示了影响迁移攻击成功率的关键因素。


<details>
  <summary>Details</summary>
Motivation: 现有研究缺乏对源代码模型（包括传统SCMs和LLM4Code）中可迁移脆弱性的深入探索，且多数方法依赖于对下游分类器的访问，难以在实际场景中应用。因此，亟需一种不依赖目标模型信息、适用于现代开发环境的通用对抗攻击方法，以评估和提升AI驱动软件生态的安全性。

Method: 作者提出了HABITAT框架，包含定制化的扰动插入机制和分层强化学习结构，能够在无需访问下游分类器的情况下自适应地选择最优扰动，生成针对源代码模型的对抗样本。

Result: 实验表明，基于传统SCMs生成的对抗样本对LLM4Code的攻击成功率最高可达64%，比现有最先进方法高出15%以上；同时揭示了传统SCMs与LLM4Code之间存在内在脆弱性关联及影响迁移攻击效果的关键因素。

Conclusion: 该研究揭示了源代码模型（包括传统模型与LLM4Code）中存在的可迁移脆弱性，强调了构建不依赖目标模型信息的通用防御机制的重要性，为未来提升AI赋能软件生态系统的安全性提供了关键方向。

Abstract: Source Code Model learn the proper embeddings from source codes, demonstrating significant success in various software engineering or security tasks. The recent explosive development of LLM extends the family of SCMs,bringing LLMs for code that revolutionize development workflows. Investigating different kinds of SCM vulnerability is the cornerstone for the security and trustworthiness of AI-powered software ecosystems, however, the fundamental one, transferable vulnerability, remains critically underexplored. Existing studies neither offer practical ways, i.e. require access to the downstream classifier of SCMs, to produce effective adversarial samples for adversarial defense, nor give heed to the widely used LLM4Code in modern software development platforms and cloud-based integrated development environments. Therefore, this work systematically studies the intrinsic vulnerability transferability of both traditional SCMs and LLM4Code, and proposes a victim-agnostic approach to generate practical adversarial samples. We design HABITAT, consisting of a tailored perturbation-inserting mechanism and a hierarchical Reinforcement Learning framework that adaptively selects optimal perturbations without requiring any access to the downstream classifier of SCMs. Furthermore, an intrinsic transferability analysis of SCM vulnerabilities is conducted, revealing the potential vulnerability correlation between traditional SCMs and LLM4Code, together with fundamental factors that govern the success rate of victim-agnostic transfer attacks. These findings of SCM vulnerabilities underscore the critical focal points for developing robust defenses in the future. Experimental evaluation demonstrates that our constructed adversarial examples crafted based on traditional SCMs achieve up to 64% success rates against LLM4Code, surpassing the state-of-the-art by over 15%.

</details>


### [29] [OWLAPY: A Pythonic Framework for OWL Ontology Engineering](https://arxiv.org/abs/2511.08232)
*Alkid Baci,Luke Friedrichs,Caglar Demir,Axel-Cyrille Ngonga Ngomo*

Main category: cs.SE

TL;DR: OWLAPY 是一个功能全面的 Python 框架，用于 OWL 本体工程，支持本体的创建、修改、序列化，并集成本地与外部推理机，同时提供多种语法格式转换及结合大语言模型进行自然语言到本体生成的能力。


<details>
  <summary>Details</summary>
Motivation: 为满足用户对灵活、高效且易于使用的 Python 工具在 OWL 本体工程中的需求，特别是帮助从 Java 环境迁移的用户，作者开发了 OWLAPY。

Method: OWLAPY 提供了对 OWL 2 本体的原生 Python 支持，整合了本地 Python 推理器和外部 Java 推理器，实现了多种本体组件实现方式，并支持 OWL 类表达式与描述逻辑、Manchester 语法和 SPARQL 等格式之间的转换。此外，还支持用户自定义工作流以结合大语言模型从自然语言生成本体。

Result: OWLAPY 已作为开源项目发布于 GitHub 和 PyPI，获得了超过 5 万次下载，成为一个经过充分测试、适用于高级本体工程的 Python 软件框架。

Conclusion: OWLAPY 为本体工程师提供了一个强大而灵活的 Python 工具，显著降低了 OWL 本体开发和推理的门槛，并促进了自然语言处理与本体工程的融合。

Abstract: In this paper, we introduce OWLAPY, a comprehensive Python framework for OWL ontology engineering. OWLAPY streamlines the creation, modification, and serialization of OWL 2 ontologies. It uniquely integrates native Python-based reasoners with support for external Java reasoners, offering flexibility for users. OWLAPY facilitates multiple implementations of core ontology components and provides robust conversion capabilities between OWL class expressions and formats such as Description Logics, Manchester Syntax, and SPARQL. It also allows users to define custom workflows to leverage large language models (LLMs) in ontology generation from natural language text. OWLAPY serves as a well-tested software framework for users seeking a flexible Python library for advanced ontology engineering, including those transitioning from Java-based environments. The project is publicly available on GitHub at https://github.com/dice-group/owlapy and on the Python Package Index (PyPI) at https://pypi.org/project/owlapy/ , with over 50,000 downloads at the time of writing.

</details>


### [30] [Designing LLM-based Multi-Agent Systems for Software Engineering Tasks: Quality Attributes, Design Patterns and Rationale](https://arxiv.org/abs/2511.08475)
*Yangxiao Cai,Ruiyin Li,Peng Liang,Mojtaba Shahin,Zengyang Li*

Main category: cs.SE

TL;DR: 本文系统研究了面向软件工程任务的基于大语言模型的多智能体系统（LLM-based MASs）的设计，分析了其关注的质量属性、常用设计模式及设计动机，并基于94篇相关论文提出了设计启示。


<details>
  <summary>Details</summary>
Motivation: 随着软件工程任务日益复杂，基于大语言模型的多智能体系统因其自主性和可扩展性受到关注，但尚缺乏对其系统性设计的研究，包括关注的质量属性、采用的设计模式及设计依据。

Method: 作者收集了94篇关于面向软件工程任务的LLM-based MASs的论文，通过内容分析识别其中关注的质量属性、使用的设计模式以及设计背后的动机。

Result: 研究发现：(1) 代码生成是最常见的应用任务；(2) 功能适用性是设计师最关注的质量属性；(3) 基于角色的协作是最常用的设计模式；(4) 提升生成代码质量是最主要的设计动机。

Conclusion: 该研究为面向软件工程任务的LLM-based MASs的设计提供了实证基础和实践启示，有助于指导未来系统的设计与优化。

Abstract: As the complexity of Software Engineering (SE) tasks continues to escalate, Multi-Agent Systems (MASs) have emerged as a focal point of research and practice due to their autonomy and scalability. Furthermore, through leveraging the reasoning and planning capabilities of Large Language Models (LLMs), the application of LLM-based MASs in the field of SE is garnering increasing attention. However, there is no dedicated study that systematically explores the design of LLM-based MASs, including the Quality Attributes (QAs) on which the designers mainly focus, the design patterns used by the designers, and the rationale guiding the design of LLM-based MASs for SE tasks. To this end, we conducted a study to identify the QAs that LLM-based MASs for SE tasks focus on, the design patterns used in the MASs, and the design rationale for the MASs. We collected 94 papers on LLM-based MASs for SE tasks as the source. Our study shows that: (1) Code Generation is the most common SE task solved by LLM-based MASs among ten identified SE tasks, (2) Functional Suitability is the QA on which designers of LLM-based MASs pay the most attention, (3) Role-Based Cooperation is the design pattern most frequently employed among 16 patterns used to construct LLM-based MASs, and (4) Improving the Quality of Generated Code is the most common rationale behind the design of LLM-based MASs. Based on the study results, we presented the implications for the design of LLM-based MASs to support SE tasks.

</details>


### [31] [Can Large Language Models Simulate Symbolic Execution Output Like KLEE?](https://arxiv.org/abs/2511.08530)
*Rong Feng,Vanisha Gupta,Vivek Patel,Viroopaksh Reddy Ernampati,Suman Saha*

Main category: cs.SE

TL;DR: 本文探索了使用大语言模型（如GPT-4o）模拟符号执行工具KLEE输出的可行性，旨在通过LLM识别程序中最受约束的路径以节省资源，实验在100个C程序上进行，准确率约为20%。


<details>
  <summary>Details</summary>
Motivation: KLEE等符号执行工具在处理具有大量分支路径的程序时效率低下、资源消耗大，因此研究者希望借助大语言模型来部分替代符号执行，从而提升效率并降低开销。

Method: 使用GPT-4o对100个C程序进行分析，尝试预测KLEE的输出并识别其中包含最多符号条件的最复杂（最受约束）执行路径。

Result: GPT-4o在生成类似KLEE输出和识别最复杂路径方面的准确率约为20%，表明当前LLM在此任务上能力有限但具备初步潜力。

Conclusion: 尽管准确率不高，该研究为理解大语言模型在模拟符号执行方面的能力边界提供了初步证据，指出了未来改进的方向。

Abstract: Symbolic execution helps check programs by exploring different paths based on symbolic inputs. Tools like KLEE are commonly used because they can automatically detect bugs and create test cases. But one of KLEE's biggest issues is how slow it can get when programs have lots of branching paths-it often becomes too resource-heavy to run on large or complex code. In this project, we wanted to see if a large language model like GPT-4o could simulate the kinds of outputs that KLEE generates. The idea was to explore whether LLMs could one day replace parts of symbolic execution to save time and resources.
  One specific goal was to have GPT-4o identify the most constrained path in a program, this is the execution path with the most symbolic conditions. These paths are especially important because they often represent edge cases that are harder to test and more likely to contain deep bugs. However, figuring this out usually requires fully running KLEE, which can be expensive. So, we tested whether GPT-4o could predict the KLEE outputs and the most complex path using a dataset of 100 C programs. Our results showed about 20% accuracy in generating KLEE-like outputs and identifying the most constrained path. While not highly accurate, this early work helps show what current LLMs can and can't do when it comes to simulating symbolic execution.

</details>


<div id='cs.PF'></div>

# cs.PF [[Back]](#toc)

### [32] [Machine Learning-Guided Memory Optimization for DLRM Inference on Tiered Memory](https://arxiv.org/abs/2511.08568)
*Jie Ren,Bin Ma,Shuangyan Yang,Benjamin Francis,Ehsan K. Ardestani,Min Si,Dong Li*

Main category: cs.PF

TL;DR: 本文提出RecMG，一种基于机器学习的系统，用于在分层内存架构中优化深度学习推荐模型（DLRM）的嵌入向量缓存与预取，显著减少按需获取次数并提升推理效率。


<details>
  <summary>Details</summary>
Motivation: DLRM模型对内存容量需求巨大，而分层内存虽成本低，却因复杂的嵌入访问模式带来嵌入向量放置难题，现有方法难以高效应对长重用距离或低频重用的访问场景。

Method: RecMG采用两个独立的机器学习模型分别处理缓存和预取任务，并设计了一种新颖的可微损失函数，以缩小预取搜索空间、减少按需获取；同时解决DLRM推理中数据标注和嵌入放置搜索空间的独特挑战。

Result: 相比当前最先进的时序、空间及基于ML的预取器，RecMG分别将按需获取次数减少了2.2倍、2.8倍和1.5倍，在工业级DLRM推理中端到端推理时间最多减少43%。

Conclusion: RecMG通过机器学习引导的缓存与预取策略，有效解决了DLRM在分层内存上的嵌入访问效率问题，显著提升了推理性能。

Abstract: Deep learning recommendation models (DLRMs) are widely used in industry, and their memory capacity requirements reach the terabyte scale. Tiered memory architectures provide a cost-effective solution but introduce challenges in embedding-vector placement due to complex embedding-access patterns. We propose RecMG, a machine learning (ML)-guided system for vector caching and prefetching on tiered memory. RecMG accurately predicts accesses to embedding vectors with long reuse distances or few reuses. The design of RecMG focuses on making ML feasible in the context of DLRM inference by addressing unique challenges in data labeling and navigating the search space for embedding-vector placement. By employing separate ML models for caching and prefetching, plus a novel differentiable loss function, RecMG narrows the prefetching search space and minimizes on-demand fetches. Compared to state-of-the-art temporal, spatial, and ML-based prefetchers, RecMG reduces on-demand fetches by 2.2x, 2.8x, and 1.5x, respectively. In industrial-scale DLRM inference scenarios, RecMG effectively reduces end-to-end DLRM inference time by up to 43%.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [33] [PIMfused: Near-Bank DRAM-PIM with Fused-layer Dataflow for CNN Data Transfer Optimization](https://arxiv.org/abs/2511.07985)
*Simei Yang,Xinyu Shi,Lu Zhao,Yunyu Ling,Quanjun Wang,Francky Catthoor*

Main category: cs.AR

TL;DR: PIMfused 是一种软硬件协同设计，通过融合层数据流在近存 DRAM-PIM 架构上高效执行 CNN，显著减少跨 Bank 数据传输，提升数据复用并打破 Bank 间依赖，在 ResNet18 上实现显著的性能、功耗与面积优势。


<details>
  <summary>Details</summary>
Motivation: 在近存 PIM 架构上运行 CNN 时，传统逐层数据流导致跨 Bank（或跨 PIMcore）的数据传输开销大，限制了性能。

Method: 提出 PIMfused，采用融合层数据流（fused-layer dataflow）进行端到端 CNN 执行，以提高数据复用并消除 Bank 间的数据依赖。

Result: 在 4-Bank PIMcore 配置下，相比 GDDR6-AiM 类基线，PIMfused 将内存周期降至 30.6%、能耗降至 83.4%、面积降至 76.5%。

Conclusion: PIMfused 通过融合层数据流有效优化了近存 PIM 架构上的 CNN 执行效率，在保持 Bank 并行性的同时大幅降低跨 Bank 通信开销，带来显著的 PPA 改进。

Abstract: Near-bank Processing-in-Memory (PIM) architectures integrate processing cores (PIMcores) close to DRAM banks to mitigate the high cost of off-chip memory accesses. When accelerating convolutional neural network (CNN) on DRAM-PIM, performance is often constrained by cross-bank (or cross-PIMcore) data transfers, which are induced by the conventional layer-by-layer dataflow that enforces inter-bank (or inter-PIMcore) dependencies across successive CNN layers. To address this challenge, we propose PIMfused, a hardware-software co-design that enables fused-layer dataflow for end-to-end CNN execution in near-bank DRAM-PIM. By adopting fused-layer dataflow, PIMfused improves data reuse and, more importantly, breaks inter-bank data dependencies, thereby optimizing cross-bank data transfers without sacrificing bank-level parallelism. We study the impact of buffer sizes and PIMcore parallelism (1-bank vs. 4-bank) on PIMfused using end-to-end ResNet18. We present three key takeaways and show that with 4-bank PIMcores, PIMfused achieves overall PPA gains over a GDDR6-AiM-like baseline, cutting memory cycles to 30.6%, energy to 83.4%, and area to 76.5%.

</details>


### [34] [Re$^{\text{2}}$MaP: Macro Placement by Recursively Prototyping and Packing Tree-based Relocating](https://arxiv.org/abs/2511.08054)
*Yunqi Shi,Xi Lin,Zhiang Wang,Siyuan Xu,Shixiong Kai,Yao Lai,Chengrui Gao,Ke Xue,Mingxuan Yuan,Chao Qian,Zhi-Hua Zhou*

Main category: cs.AR

TL;DR: Re²MaP是一种通过递归原型构建与基于打包树的重定位方法，实现高质量宏单元布局的新型布局算法，在时序、功耗和设计规则等方面优于现有学术布局器。


<details>
  <summary>Details</summary>
Motivation: 现有宏单元布局方法在处理复杂设计约束（如线长、数据流、时序等）时存在局限，难以兼顾布局质量与效率，因此需要一种能融合专家知识并迭代优化的新型布局策略。

Method: 该方法首先进行多层级宏分组与PPA感知的单元聚类，构建统一连接矩阵；然后利用DREAMPlace生成混合尺寸布局原型，并提出基于角度的ABPlace方法在椭圆上优化宏位置；接着通过打包树结构对宏组及其内部宏联合重定位，采用进化搜索优化融合多种设计约束的成本函数；整个流程递归执行，每次仅定位部分宏组以提升原型精度。

Result: 在标准后端流程下，Re²MaP相比当前最先进的学术布局器Hier-RTLMP，在最差负裕量（WNS）上最多提升22.22%（平均10.26%），总负裕量（TNS）最多提升97.91%（平均33.97%）；在七个测试案例中，其在WNS、TNS、功耗、DRC违规数和运行时间方面均优于其会议版本ReMaP。

Conclusion: Re²MaP通过递归原型构建与打包树重定位机制，有效融合了专家经验与自动化优化，在多项关键指标上显著超越现有方法，为高质量宏单元布局提供了新思路。

Abstract: This work introduces the Re$^{\text{2}}$MaP method, which generates expert-quality macro placements through recursively prototyping and packing tree-based relocating. We first perform multi-level macro grouping and PPA-aware cell clustering to produce a unified connection matrix that captures both wirelength and dataflow among macros and clusters. Next, we use DREAMPlace to build a mixed-size placement prototype and obtain reference positions for each macro and cluster. Based on this prototype, we introduce ABPlace, an angle-based analytical method that optimizes macro positions on an ellipse to distribute macros uniformly near chip periphery, while optimizing wirelength and dataflow. A packing tree-based relocating procedure is then designed to jointly adjust the locations of macro groups and the macros within each group, by optimizing an expertise-inspired cost function that captures various design constraints through evolutionary search. Re$^{\text{2}}$MaP repeats the above process: Only a subset of macro groups are positioned in each iteration, and the remaining macros are deferred to the next iteration to improve the prototype's accuracy. Using a well-established backend flow with sufficient timing optimizations, Re$^{\text{2}}$MaP achieves up to 22.22% (average 10.26%) improvement in worst negative slack (WNS) and up to 97.91% (average 33.97%) improvement in total negative slack (TNS) compared to the state-of-the-art academic placer Hier-RTLMP. It also ranks higher on WNS, TNS, power, design rule check (DRC) violations, and runtime than the conference version ReMaP, across seven tested cases. Our code is available at https://github.com/lamda-bbo/Re2MaP.

</details>


### [35] [DRACO: Co-design for DSP-Efficient Rigid Body Dynamics Accelerator](https://arxiv.org/abs/2511.08395)
*Xingyu Liu,Jiawei Liang,Yipu Zhang,Linfeng Du,Chaofang Ma,Hui Yu,Jiang Xu,Wei Zhang*

Main category: cs.AR

TL;DR: 本文提出了一种基于FPGA的硬件高效RBD加速器，通过精度感知量化、除法延迟优化和模块间DSP复用三项创新，在多种机器人系统上实现了最高8倍吞吐量提升和7.4倍延迟降低。


<details>
  <summary>Details</summary>
Motivation: 现有RBD（刚体动力学）加速器在高自由度机器人系统中存在硬件资源利用率低、延迟高和吞吐量不足的问题，亟需一种更高效的硬件加速方案。

Method: 提出三项关键技术：1）精度感知量化框架，在降低DSP需求的同时保持运动精度；2）在质量矩阵求逆算法中采用除法延迟优化，将倒数运算从关键路径中解耦；3）模块间DSP复用方法，提高DSP利用率并减少使用量。

Result: 实验表明，该加速器相比当前最先进的RBD加速器，在多种机器人类型上实现了最高8倍的吞吐量提升和7.4倍的延迟降低。

Conclusion: 所提出的FPGA加速器在保证机器人控制精度的同时显著提升了性能与资源效率，具备良好的可扩展性，适用于高自由度机器人系统。

Abstract: We propose a hardware-efficient RBD accelerator based on FPGA, introducing three key innovations. First, we propose a precision-aware quantization framework that reduces DSP demand while preserving motion accuracy. This is also the first study to systematically evaluate quantization impact on robot control and motion for hardware acceleration. Second, we leverage a division deferring optimization in mass matrix inversion algorithm, which decouples reciprocal operations from the longest latency path to improve the performance. Finally, we present an inter-module DSP reuse methodology to improve DSP utilization and save DSP usage. Experiment results show that our work achieves up to 8x throughput improvement and 7.4x latency reduction over state-of-the-art RBD accelerators across various robot types, demonstrating its effectiveness and scalability for high-DOF robotic systems.

</details>


<div id='cs.OS'></div>

# cs.OS [[Back]](#toc)

### [36] [Work-in-Progress: Function-as-Subtask API Replacing Publish/Subscribe for OS-Native DAG Scheduling](https://arxiv.org/abs/2511.08297)
*Takahiro Ishikawa-Aso,Atsushi Yano,Yutaro Kobayashi,Takumi Jin,Yuuki Takano,Shinpei Kato*

Main category: cs.OS

TL;DR: 本文提出了一种名为Function-as-Subtask（FasS）的新API，通过将子任务表示为函数并明确其输入输出依赖关系，从接口层面保障有向无环图（DAG）语义的正确性，避免了ROS 2中因发布/订阅机制导致的DAG约束失效问题，并在Rust实验内核上实现了原生DAG调度器进行验证。


<details>
  <summary>Details</summary>
Motivation: ROS 2的发布/订阅API无法强制执行DAG任务模型中的优先约束，依赖开发者约定维持语义，一旦违反则模型失效，因此需要一种能从API层面保证DAG语义的方法。

Method: 提出Function-as-Subtask（FasS）API，将每个子任务建模为函数，其参数和返回值分别对应输入和输出边，从而限制描述自由度；并在基于Rust的实验内核上实现支持FasS的DAG原生调度器。

Result: 实现了语义保真度更高的DAG调度系统，并提出了将FasS应用于Linux sched_ext的设计指南。

Conclusion: FasS API能够有效保障DAG任务模型的语义正确性，减少对程序员纪律的依赖，为实时系统中DAG调度的可靠实现提供了新思路。

Abstract: The Directed Acyclic Graph (DAG) task model for real-time scheduling finds its primary practical target in Robot Operating System 2 (ROS 2). However, ROS 2's publish/subscribe API leaves DAG precedence constraints unenforced: a callback may publish mid-execution, and multi-input callbacks let developers choose topic-matching policies. Thus preserving DAG semantics relies on conventions; once violated, the model collapses. We propose the Function-as-Subtask (FasS) API, which expresses each subtask as a function whose arguments/return values are the subtask's incoming/outgoing edges. By minimizing description freedom, DAG semantics is guaranteed at the API rather than by programmer discipline. We implement a DAG-native scheduler using FasS on a Rust-based experimental kernel and evaluate its semantic fidelity, and we outline design guidelines for applying FasS to Linux Linux sched_ext.

</details>
