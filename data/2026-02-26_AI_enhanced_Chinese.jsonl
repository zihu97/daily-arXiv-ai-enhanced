{"id": "2602.21278", "categories": ["cs.AR", "eess.SY"], "pdf": "https://arxiv.org/pdf/2602.21278", "abs": "https://arxiv.org/abs/2602.21278", "authors": ["Xinxin Wang", "Lixian Yan", "Shuhan Liu", "Luke Upton", "Zhuoqi Cai", "Yiming Tan", "Shengman Li", "Koustav Jana", "Peijing Li", "Jesse Cirimelli-Low", "Thierry Tambe", "Matthew Guthaus", "H. -S. Philip Wong"], "title": "Heterogeneous Memory Design Exploration for AI Accelerators with a Gain Cell Memory Compiler", "comment": null, "summary": "As memory increasingly dominates system cost and energy, heterogeneous on-chip memory systems that combine technologies with complementary characteristics are becoming essential. Gain Cell RAM (GCRAM) offers higher density, lower power, and tunable retention, expanding the design space beyond conventional SRAM. To this end, we create an OpenGCRAM compiler supporting both SRAM and GCRAM. It generates macro-level designs and layouts for commercial CMOS processes and characterizes area, delay, and power across user-defined configurations. The tool enables systematic identification of optimal heterogeneous memory configurations for AI tasks under specified performance metrics.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u4e00\u79cd\u540d\u4e3aOpenGCRAM\u7684\u7f16\u8bd1\u5668\u5de5\u5177\uff0c\u652f\u6301SRAM\u548cGCRAM\u4e24\u79cd\u5b58\u50a8\u6280\u672f\uff0c\u80fd\u591f\u4e3a\u5546\u7528CMOS\u5de5\u827a\u751f\u6210\u5b8f\u7ea7\u8bbe\u8ba1\u548c\u5e03\u5c40\uff0c\u5e2e\u52a9\u8bc6\u522b\u9488\u5bf9AI\u4efb\u52a1\u7684\u6700\u4f18\u5f02\u6784\u5b58\u50a8\u914d\u7f6e\u3002", "motivation": "\u968f\u7740\u5b58\u50a8\u5728\u7cfb\u7edf\u6210\u672c\u548c\u80fd\u8017\u4e2d\u5360\u6bd4\u65e5\u76ca\u589e\u52a0\uff0c\u7ed3\u5408\u4e92\u8865\u7279\u6027\u6280\u672f\u7684\u5f02\u6784\u7247\u4e0a\u5b58\u50a8\u7cfb\u7edf\u53d8\u5f97\u81f3\u5173\u91cd\u8981\u3002GCRAM\uff08\u589e\u76ca\u5355\u5143RAM\uff09\u76f8\u6bd4\u4f20\u7edfSRAM\u5177\u6709\u66f4\u9ad8\u5bc6\u5ea6\u3001\u66f4\u4f4e\u529f\u8017\u548c\u53ef\u8c03\u8282\u4fdd\u6301\u65f6\u95f4\u7684\u4f18\u52bf\uff0c\u80fd\u591f\u6269\u5c55\u8bbe\u8ba1\u7a7a\u95f4\u3002", "method": "\u5f00\u53d1OpenGCRAM\u7f16\u8bd1\u5668\uff0c\u652f\u6301SRAM\u548cGCRAM\u4e24\u79cd\u6280\u672f\uff0c\u80fd\u591f\u751f\u6210\u5546\u7528CMOS\u5de5\u827a\u7684\u5b8f\u7ea7\u8bbe\u8ba1\u548c\u5e03\u5c40\uff0c\u5e76\u9488\u5bf9\u7528\u6237\u5b9a\u4e49\u7684\u914d\u7f6e\u8fdb\u884c\u9762\u79ef\u3001\u5ef6\u8fdf\u548c\u529f\u8017\u7684\u8868\u5f81\u3002", "result": "\u8be5\u5de5\u5177\u80fd\u591f\u6839\u636e\u6307\u5b9a\u7684\u6027\u80fd\u6307\u6807\uff0c\u7cfb\u7edf\u6027\u5730\u8bc6\u522b\u9488\u5bf9AI\u4efb\u52a1\u7684\u6700\u4f18\u5f02\u6784\u5b58\u50a8\u914d\u7f6e\uff0c\u4e3a\u5b58\u50a8\u5b50\u7cfb\u7edf\u8bbe\u8ba1\u63d0\u4f9b\u7075\u6d3b\u9009\u62e9\u3002", "conclusion": "OpenGCRAM\u7f16\u8bd1\u5668\u4e3a\u5229\u7528GCRAM\u6280\u672f\u4f18\u5316\u5b58\u50a8\u5b50\u7cfb\u7edf\u63d0\u4f9b\u4e86\u5b8c\u6574\u5de5\u5177\u94fe\uff0c\u6709\u52a9\u4e8e\u964d\u4f4e\u7cfb\u7edf\u6210\u672c\u548c\u80fd\u8017\uff0c\u7279\u522b\u9002\u7528\u4e8e\u5bf9\u5b58\u50a8\u6709\u7279\u6b8a\u8981\u6c42\u7684AI\u5e94\u7528\u3002"}}
{"id": "2602.21251", "categories": ["cs.SE", "cs.AI", "cs.MA", "cs.PL"], "pdf": "https://arxiv.org/pdf/2602.21251", "abs": "https://arxiv.org/abs/2602.21251", "authors": ["Clemens Pohle"], "title": "AgenticTyper: Automated Typing of Legacy Software Projects Using Agentic AI", "comment": "Accepted at ICSE 2026 Student Research Competition (SRC)", "summary": "Legacy JavaScript systems lack type safety, making maintenance risky. While TypeScript can help, manually adding types is expensive. Previous automated typing research focuses on type inference but rarely addresses type checking setup, definition generation, bug identification, or behavioral correctness at repository scale. We present AgenticTyper, a Large Language Model (LLM)-based agentic system that addresses these gaps through iterative error correction and behavior preservation via transpilation comparison. Evaluation on two proprietary repositories (81K LOC) shows that AgenticTyper resolves all 633 initial type errors in 20 minutes, reducing manual effort from one working day.", "AI": {"tldr": "AgenticTyper\u662f\u4e00\u4e2a\u57fa\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u81ea\u52a8\u5316\u7cfb\u7edf\uff0c\u80fd\u591f\u9ad8\u6548\u89e3\u51b3JavaScript\u4ee3\u7801\u5e93\u4e2d\u7684TypeScript\u7c7b\u578b\u9519\u8bef\uff0c\u663e\u8457\u51cf\u5c11\u624b\u52a8\u5de5\u4f5c\u91cf\u3002", "method": "AgenticTyper\u662f\u4e00\u4e2a\u57fa\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u667a\u80fd\u7cfb\u7edf\uff0c\u901a\u8fc7\u8fed\u4ee3\u9519\u8bef\u7ea0\u6b63\u548c\u901a\u8fc7\u8f6c\u8bd1\u6bd4\u8f83\u4fdd\u6301\u884c\u4e3a\u6b63\u786e\u6027\uff0c\u81ea\u52a8\u89e3\u51b3TypeScript\u7c7b\u578b\u95ee\u9898\u3002", "result": "\u5728\u4e24\u4e2a\u79c1\u6709\u4ee3\u7801\u5e93(81K\u4ee3\u7801\u884c)\u4e0a\u8bc4\u4f30\u663e\u793a\uff0cAgenticTyper\u572820\u5206\u949f\u5185\u89e3\u51b3\u4e86\u6240\u6709633\u4e2a\u521d\u59cb\u7c7b\u578b\u9519\u8bef\uff0c\u5c06\u624b\u52a8\u5de5\u4f5c\u91cf\u4ece\u4e00\u4e2a\u4eba\u5de5\u4f5c\u65e5\u51cf\u5c11\u523020\u5206\u949f\u3002", "conclusion": "AgenticTyper\u901a\u8fc7\u63d0\u4f9b\u53ef\u6269\u5c55\u7684\u7c7b\u578b\u5b89\u5168\u89e3\u51b3\u65b9\u6848\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u81ea\u52a8\u5316\u7c7b\u578b\u8f6c\u6362\u4e2d\u7684\u7a7a\u767d\uff0c\u663e\u8457\u51cf\u5c11\u4e86\u624b\u52a8\u5de5\u4f5c\u91cf\uff0c\u540c\u65f6\u4fdd\u6301\u884c\u4e3a\u6b63\u786e\u6027\u3002", "motivation": "Motivation analysis unavailable"}}
{"id": "2602.21343", "categories": ["cs.NI"], "pdf": "https://arxiv.org/pdf/2602.21343", "abs": "https://arxiv.org/abs/2602.21343", "authors": ["Chao Feng", "Thomas Grubl", "Jan von der Assen", "Sandrin Raphael Hunkeler", "Linn Anna Spitz", "Gerome Bovet", "Burkhard Stiller"], "title": "UnlinkableDFL: a Practical Mixnet Protocol for Churn-Tolerant Decentralized FL Model Sharing", "comment": null, "summary": "Decentralized Federated Learning (DFL) eliminates the need for a central aggregator, but it can expose communication patterns that reveal participant identities. This work presents UnlinkableDFL, a DFL framework that combines a peer-based mixnet with fragment-based model aggregation to ensure unlinkability in fully decentralized settings. Model updates are divided into encrypted fragments, sent over separate multi-hop paths, and aggregated without using any identity information. A theoretical analysis indicates that relay and end-to-end unlinkability improve with larger mixing sets and longer paths, while convergence remains similar to standard FedAvg. A prototype implementation evaluates learning performance, latency, unlinkability, and resource usage. The results show that UnlinkableDFL converges reliably and adapts to node churn. Communication latency emerges as the main overhead, while memory and CPU usage stay moderate. These findings illustrate the balance between anonymity and system efficiency, demonstrating that strong unlinkability can be maintained in decentralized learning workflows.", "AI": {"tldr": "UnlinkableDFL\u662f\u4e00\u4e2a\u7ed3\u5408\u4e86\u57fa\u4e8e\u5bf9\u7b49\u65b9\u7684\u6df7\u5408\u7f51\u548c\u57fa\u4e8e\u7247\u6bb5\u7684\u6a21\u578b\u805a\u5408\u7684\u53bb\u4e2d\u5fc3\u5316\u8054\u90a6\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u5c06\u6a21\u578b\u66f4\u65b0\u5206\u5272\u4e3a\u52a0\u5bc6\u7247\u6bb5\u5e76\u901a\u8fc7\u591a\u8df3\u8def\u5f84\u53d1\u9001\uff0c\u786e\u4fdd\u4e86\u53bb\u4e2d\u5fc3\u5316\u8bbe\u7f6e\u4e2d\u7684\u4e0d\u53ef\u94fe\u63a5\u6027\uff0c\u540c\u65f6\u4fdd\u6301\u4e0e\u6807\u51c6FedAvg\u76f8\u4f3c\u7684\u6536\u655b\u6027\u80fd\u3002", "motivation": "\u53bb\u4e2d\u5fc3\u5316\u8054\u90a6\u5b66\u4e60(DFL)\u867d\u7136\u6d88\u9664\u4e86\u5bf9\u4e2d\u5fc3\u805a\u5408\u5668\u7684\u9700\u6c42\uff0c\u4f46\u4f1a\u66b4\u9732\u901a\u4fe1\u6a21\u5f0f\uff0c\u4ece\u800c\u53ef\u80fd\u63ed\u793a\u53c2\u4e0e\u8005\u7684\u8eab\u4efd\u3002", "method": "UnlinkableDFL\u6846\u67b6\u5c06\u6a21\u578b\u66f4\u65b0\u5206\u5272\u4e3a\u52a0\u5bc6\u7247\u6bb5\uff0c\u901a\u8fc7\u72ec\u7acb\u7684\u591a\u8df3\u8def\u5f84\u53d1\u9001\uff0c\u5e76\u5728\u4e0d\u4f7f\u7528\u4efb\u4f55\u8eab\u4efd\u4fe1\u606f\u7684\u60c5\u51b5\u4e0b\u8fdb\u884c\u805a\u5408\uff0c\u7ed3\u5408\u4e86\u57fa\u4e8e\u5bf9\u7b49\u65b9\u7684\u6df7\u5408\u7f51\u548c\u57fa\u4e8e\u7247\u6bb5\u7684\u6a21\u578b\u805a\u5408\u6280\u672f\u3002", "result": "\u7406\u8bba\u5206\u6790\u8868\u660e\uff0c\u66f4\u5927\u7684\u6df7\u5408\u96c6\u548c\u66f4\u957f\u7684\u8def\u5f84\u63d0\u9ad8\u4e86\u4e2d\u7ee7\u548c\u7aef\u5230\u7aef\u7684\u4e0d\u53ef\u94fe\u63a5\u6027\uff0c\u540c\u65f6\u6536\u655b\u6027\u7c7b\u4f3c\u4e8e\u6807\u51c6FedAvg\u3002\u539f\u578b\u8bc4\u4f30\u663e\u793aUnlinkableDFL\u53ef\u9760\u6536\u655b\u5e76\u80fd\u9002\u5e94\u8282\u70b9\u53d8\u5316\uff0c\u901a\u4fe1\u5ef6\u8fdf\u662f\u4e3b\u8981\u5f00\u9500\uff0c\u800c\u5185\u5b58\u548cCPU\u4f7f\u7528\u4fdd\u6301\u9002\u4e2d\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u8bf4\u660e\u4e86\u533f\u540d\u6027\u548c\u7cfb\u7edf\u6548\u7387\u4e4b\u95f4\u7684\u5e73\u8861\uff0c\u8bc1\u660e\u4e86\u5728\u53bb\u4e2d\u5fc3\u5316\u5b66\u4e60\u5de5\u4f5c\u6d41\u7a0b\u4e2d\u53ef\u4ee5\u7ef4\u6301\u5f3a\u5927\u7684\u4e0d\u53ef\u94fe\u63a5\u6027\u3002"}}
{"id": "2602.21411", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2602.21411", "abs": "https://arxiv.org/abs/2602.21411", "authors": ["Marc Dufay", "Diana Ghinea", "Anton Paramonov"], "title": "General Convex Agreement with Near-Optimal Communication", "comment": "Working paper", "summary": "Convex Agreement (CA) strengthens Byzantine Agreement (BA) by requiring the output agreed upon to lie in the convex hull of the honest parties' inputs. This validity condition is motivated by practical aggregation tasks (e.g., robust learning or sensor fusion) where honest inputs need not coincide but should still constrain the decision. CA inherits BA lower bounds, and optimal synchronous round complexity is easy to obtain (e.g., via Byzantine Broadcast). The main challenge is \\emph{communication}: standard approaches for CA have a communication complexity of $\u0398(Ln^2)$ for large $L$-bit inputs, leaving a gap in contrast to BA's lower bound of $\u03a9(Ln)$ bits. While recent work achieves optimal communication complexity of $O(Ln)$ for sufficiently large $L$ [GLW,PODC'25], translating this result to general convexity spaces remained an open problem.\n  We investigate this gap for abstract convexity spaces, and we present deterministic synchronous CA protocols with near-optimal communication complexity: when $L = \u03a9(n \\cdot \u03ba)$, where $\u03ba$ is a security parameter, we achieve $O(L\\cdot n\\log n)$ communication for finite convexity spaces and $O(L\\cdot n^{1+o(1)})$ communication for Euclidean spaces $\\mathbb{R}^d$. Our protocols have asymptotically optimal round complexity $O(n)$ and, when a bound on the inputs' lengths $L$ is fixed a priori, we achieve near-optimal resilience $t < n/(\u03c9+\\varepsilon)$ for any constant $\\varepsilon>0$, where $\u03c9$ is the Helly number of the convexity space. If $L$ is unknown, we still achieve resilience $t<n/(\u03c9+\\varepsilon+1)$ for any constant $\\varepsilon > 0$. We further note that our protocols can be leveraged to efficiently solve parallel BA.\n  Our main technical contribution is the use of extractor graphs to obtain a deterministic assignment of parties to committees, which is resilient against adaptive adversaries.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u51f8\u5171\u8bc6(Convex Agreement, CA)\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u786e\u5b9a\u6027\u540c\u6b65\u534f\u8bae\uff0c\u5728\u4e00\u822c\u51f8\u6027\u7a7a\u95f4\u4e2d\u5b9e\u73b0\u4e86\u63a5\u8fd1\u6700\u4f18\u7684\u901a\u4fe1\u590d\u6742\u5ea6\u3002\u5f53\u8f93\u5165\u957f\u5ea6L\u8db3\u591f\u5927\u65f6\uff0c\u6709\u9650\u51f8\u6027\u7a7a\u95f4\u7684\u901a\u4fe1\u590d\u6742\u5ea6\u4e3aO(L\u00b7n log n)\uff0c\u6b27\u51e0\u91cc\u5f97\u7a7a\u95f4\u7684\u590d\u6742\u5ea6\u4e3aO(L\u00b7n^{1+o(1)})\u3002\u8be5\u534f\u8bae\u5177\u6709\u6e10\u8fdb\u6700\u4f18\u7684\u8f6e\u590d\u6742\u5ea6O(n)\uff0c\u5e76\u4e14\u5177\u6709\u826f\u597d\u7684\u5bb9\u9519\u80fd\u529b\u3002", "motivation": "\u51f8\u5171\u8bc6\u662f\u5bf9\u62dc\u5360\u5ead\u5171\u8bc6(Byzantine Agreement, BA)\u7684\u52a0\u5f3a\uff0c\u8981\u6c42\u534f\u5546\u7684\u8f93\u51fa\u503c\u4f4d\u4e8e\u8bda\u5b9e\u65b9\u8f93\u5165\u7684\u51f8\u5305\u5185\u3002\u8fd9\u4e00\u6709\u6548\u6027\u6761\u4ef6\u5bf9\u4e8e\u5b9e\u9645\u805a\u5408\u4efb\u52a1(\u5982\u9c81\u68d2\u5b66\u4e60\u6216\u4f20\u611f\u5668\u878d\u5408)\u5f88\u91cd\u8981\uff0c\u5176\u4e2d\u8bda\u5b9e\u8f93\u5165\u4e0d\u9700\u8981\u4e00\u81f4\u4f46\u5e94\u7ea6\u675f\u51b3\u7b56\u3002\u6807\u51c6CA\u65b9\u6cd5\u7684\u901a\u4fe1\u590d\u6742\u5ea6\u4e3a\u0398(Ln\u00b2)\uff0c\u4e0eBA\u7684\u03a9(Ln)\u4e0b\u754c\u5b58\u5728\u5dee\u8ddd\uff0c\u4e14\u6700\u65b0\u6210\u679c\u5c1a\u672a\u6269\u5c55\u5230\u4e00\u822c\u51f8\u6027\u7a7a\u95f4\u3002", "method": "\u4f5c\u8005\u5229\u7528\u63d0\u53d6\u5668\u56fe(extractor graphs)\u6765\u5b9e\u73b0\u5bf9\u53c2\u4e0e\u65b9\u5230\u59d4\u5458\u4f1a\u7684\u786e\u5b9a\u6027\u5206\u914d\uff0c\u8fd9\u79cd\u65b9\u6cd5\u5bf9\u81ea\u9002\u5e94\u5bf9\u624b\u5177\u6709\u9c81\u68d2\u6027\u3002\u901a\u8fc7\u8fd9\u79cd\u65b9\u6cd5\uff0c\u4ed6\u4eec\u8bbe\u8ba1\u4e86\u9002\u7528\u4e8e\u4e00\u822c\u51f8\u6027\u7a7a\u95f4\u7684\u786e\u5b9a\u6027\u540c\u6b65CA\u534f\u8bae\uff0c\u514b\u670d\u4e86\u901a\u4fe1\u590d\u6742\u5ea6\u7684\u6311\u6218\u3002", "result": "\u8bba\u6587\u5b9e\u73b0\u4e86\u4ee5\u4e0b\u6210\u679c\uff1a1) \u5f53L = \u03a9(n\u00b7\u03ba)\u65f6\uff0c\u6709\u9650\u51f8\u6027\u7a7a\u95f4\u7684\u901a\u4fe1\u590d\u6742\u5ea6\u4e3aO(L\u00b7n log n)\uff0c\u6b27\u51e0\u91cc\u5f97\u7a7a\u95f4\u7684\u590d\u6742\u5ea6\u4e3aO(L\u00b7n^{1+o(1)})\uff1b2) \u534f\u8bae\u5177\u6709\u6e10\u8fdb\u6700\u4f18\u7684\u8f6e\u590d\u6742\u5ea6O(n)\uff1b3) \u56fa\u5b9aL\u65f6\u8fbe\u5230\u63a5\u8fd1\u6700\u4f18\u7684\u5bb9\u9519\u6027t < n/(\u03c9+\u03b5)\uff1b4) L\u672a\u77e5\u65f6\u4ecd\u80fd\u8fbe\u5230t < n/(\u03c9+\u03b5+1)\u7684\u5bb9\u9519\u6027\uff1b5) \u53ef\u6709\u6548\u7528\u4e8e\u5e76\u884c\u62dc\u5360\u5ead\u5e7f\u64ad\u95ee\u9898\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u586b\u8865\u4e86\u51f8\u5171\u8bc6\u5728\u4e00\u822c\u51f8\u6027\u7a7a\u95f4\u4e2d\u7684\u901a\u4fe1\u590d\u6742\u5ea6\u7a7a\u767d\uff0c\u63d0\u4f9b\u4e86\u5177\u6709\u826f\u597d\u901a\u4fe1\u6548\u7387\u3001\u8f6e\u590d\u6742\u5ea6\u548c\u5bb9\u9519\u6027\u7684\u786e\u5b9a\u6027\u534f\u8bae\u3002\u4e3b\u8981\u6280\u672f\u8d21\u732e\u662f\u4f7f\u7528\u63d0\u53d6\u5668\u56fe\u5b9e\u73b0\u786e\u5b9a\u6027\u59d4\u5458\u4f1a\u5206\u914d\uff0c\u5bf9\u81ea\u9002\u5e94\u5bf9\u624b\u5177\u6709\u9c81\u68d2\u6027\u3002\u8fd9\u4e9b\u6210\u679c\u5bf9\u4e8e\u9c81\u68d2\u673a\u5668\u5b66\u4e60\u3001\u4f20\u611f\u5668\u878d\u5408\u7b49\u5b9e\u9645\u5e94\u7528\u5177\u6709\u91cd\u8981\u610f\u4e49\u3002"}}
{"id": "2602.21568", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2602.21568", "abs": "https://arxiv.org/abs/2602.21568", "authors": ["Yuvraj Agrawal", "Pallav Jain"], "title": "From Ad-Hoc Scripts to Orchestrated Pipelines: Architecting a Resilient ELT Framework for Developer Productivity Metrics", "comment": null, "summary": "Developer Productivity Dashboards are essential for visualizing DevOps performance metrics such as Deployment Frequency and Change Failure Rate (DORA). However, the utility of these dashboards is frequently undermined by data reliability issues. In early iterations of our platform, ad-hoc ingestion scripts (Cron jobs) led to \"silent failures,\" where data gaps went undetected for days, eroding organizational trust. This paper reports on our experience migrating from legacy scheduling to a robust Extract-Load-Transform (ELT) pipeline using Directed Acyclic Graph (DAG) orchestration and Medallion Architecture. We detail the operational benefits of decoupling data extraction from transformation, the necessity of immutable raw history for metric redefinition, and the implementation of state-based dependency management. Our experience suggests that treating the metrics pipeline as a production-grade distributed system is a prerequisite for sustainable engineering analytics.", "AI": {"tldr": "\u8be5\u8bba\u6587\u62a5\u544a\u4e86\u4ece\u4f20\u7edf\u8c03\u5ea6\u7cfb\u7edf\u8fc1\u79fb\u5230\u57fa\u4e8eDAG\u7f16\u6392\u548cMedallion\u67b6\u6784\u7684ELT\u7ba1\u9053\u7684\u7ecf\u9a8c\uff0c\u89e3\u51b3\u4e86\u5f00\u53d1\u8005\u751f\u4ea7\u529b\u4eea\u8868\u677f\u4e2d\u7684\u6570\u636e\u53ef\u9760\u6027\u95ee\u9898\u3002", "motivation": "\u5f00\u53d1\u8005\u751f\u4ea7\u529b\u4eea\u8868\u677f\u7528\u4e8e\u53ef\u89c6\u5316DevOps\u6027\u80fd\u6307\u6807\uff0c\u4f46\u7ecf\u5e38\u56e0\u6570\u636e\u53ef\u9760\u6027\u95ee\u9898\u800c\u6548\u7528\u964d\u4f4e\u3002\u65e9\u671f\u4f7f\u7528\u4e34\u65f6\u811a\u672c(Cron jobs)\u5bfc\u81f4", "method": "\u4ece\u4f20\u7edf\u8c03\u5ea6\u8fc1\u79fb\u5230\u5065\u58ee\u7684\u63d0\u53d6-\u52a0\u8f7d-\u8f6c\u6362(ELT)\u7ba1\u9053\uff0c\u4f7f\u7528\u6709\u5411\u65e0\u73af\u56fe(DAG)\u7f16\u6392\u548cMedallion\u67b6\u6784\u3002\u89e3\u8026\u6570\u636e\u63d0\u53d6\u4e0e\u8f6c\u6362\uff0c\u4fdd\u6301\u4e0d\u53ef\u53d8\u7684\u539f\u59cb\u5386\u53f2\u8bb0\u5f55\u7528\u4e8e\u6307\u6807\u91cd\u65b0\u5b9a\u4e49\uff0c\u5e76\u5b9e\u65bd\u57fa\u4e8e\u72b6\u6001\u7684\u4f9d\u8d56\u7ba1\u7406\u3002", "result": "\u8be5\u8fc1\u79fb\u89e3\u51b3\u4e86\u6570\u636e\u53ef\u9760\u6027\u95ee\u9898\uff0c\u6d88\u9664\u4e86", "conclusion": "\u5c06\u6307\u6807\u7ba1\u9053\u89c6\u4e3a\u751f\u4ea7\u7ea7\u5206\u5e03\u5f0f\u7cfb\u7edf\u662f\u53ef\u6301\u7eed\u5de5\u7a0b\u5206\u6790\u7684\u524d\u63d0\u6761\u4ef6\uff0c\u9700\u8981\u67b6\u6784\u4e0a\u7684\u91cd\u89c6\u548c\u7cfb\u7edf\u6027\u65b9\u6cd5\u3002"}}
{"id": "2602.21477", "categories": ["cs.MA"], "pdf": "https://arxiv.org/pdf/2602.21477", "abs": "https://arxiv.org/abs/2602.21477", "authors": ["Zhengding Hu", "Zaifeng Pan", "Prabhleen Kaur", "Vibha Murthy", "Zhongkai Yu", "Yue Guan", "Zhen Wang", "Steven Swanson", "Yufei Ding"], "title": "Pancake: Hierarchical Memory System for Multi-Agent LLM Serving", "comment": null, "summary": "In this work, we identify and address the core challenges of agentic memory management in LLM serving, where large-scale storage, frequent updates, and multiple coexisting agents jointly introduce complex and high-cost approximate nearest neighbor (ANN) searching problems. We present Pancake, a multi-tier agentic memory system that unifies three key techniques: (i) multi-level index caching for single agents, (ii) coordinated index management across multiple agents, and (iii) collaborative GPU-CPU acceleration. Pancake exposes easy-to-use interface that can be integrated into memory-based agents like Mem-GPT, and is compatible with agentic frameworks such as LangChain and LlamaIndex. Experiments on realistic agent workloads show that Pancake substantially outperforms existing frameworks, achieving more than 4.29x end-to-end throughput improvement.", "AI": {"tldr": "\u8be5\u8bba\u6587\u4ecb\u7ecd\u4e86Pancake\uff0c\u4e00\u79cd\u9488\u5bf9LLM\u670d\u52a1\u4e2d\u667a\u80fd\u4f53\u5185\u5b58\u7ba1\u7406\u7684\u65b0\u578b\u591a\u5c42\u7ea7\u5185\u5b58\u7cfb\u7edf\uff0c\u901a\u8fc7\u591a\u7ea7\u7d22\u5f15\u7f13\u5b58\u3001\u8de8\u667a\u80fd\u4f53\u534f\u8c03\u7d22\u5f15\u7ba1\u7406\u548cGPU-CPU\u534f\u540c\u52a0\u901f\u6280\u672f\uff0c\u89e3\u51b3\u4e86\u5927\u89c4\u6a21\u5b58\u50a8\u3001\u9891\u7e41\u66f4\u65b0\u548c\u591a\u667a\u80fd\u4f53\u5171\u5b58\u5e26\u6765\u7684\u590d\u6742\u8fd1\u4f3c\u6700\u8fd1\u90bb\u641c\u7d22\u95ee\u9898\u3002", "motivation": "LLM\u670d\u52a1\u4e2d\u7684\u667a\u80fd\u4f53\u5185\u5b58\u7ba1\u7406\u9762\u4e34\u5927\u89c4\u6a21\u5b58\u50a8\u3001\u9891\u7e41\u66f4\u65b0\u548c\u591a\u667a\u80fd\u4f53\u5171\u5b58\u5e26\u6765\u7684\u590d\u6742\u4e14\u9ad8\u6210\u672c\u7684\u8fd1\u4f3c\u6700\u8fd1\u90bb(ANN)\u641c\u7d22\u6311\u6218\u3002", "method": "\u63d0\u51faPancake\u591a\u5c42\u7ea7\u667a\u80fd\u4f53\u5185\u5b58\u7cfb\u7edf\uff0c\u6574\u5408\u4e09\u9879\u5173\u952e\u6280\u672f\uff1a\u5355\u667a\u80fd\u4f53\u7684\u591a\u7ea7\u7d22\u5f15\u7f13\u5b58\u3001\u8de8\u667a\u80fd\u4f53\u7684\u534f\u8c03\u7d22\u5f15\u7ba1\u7406\u4ee5\u53ca\u534f\u540cGPU-CPU\u52a0\u901f\u3002", "result": "\u5728\u771f\u5b9e\u667a\u80fd\u4f53\u5de5\u4f5c\u8d1f\u8f7d\u5b9e\u9a8c\u4e2d\uff0cPancake\u663e\u8457\u8d85\u8d8a\u73b0\u6709\u6846\u67b6\uff0c\u5b9e\u73b0\u4e86\u8d85\u8fc74.29\u500d\u7684\u7aef\u5230\u7aef\u541e\u5410\u91cf\u63d0\u5347\u3002", "conclusion": "Pancake\u4e3a\u591a\u667a\u80fd\u4f53LLM\u670d\u52a1\u63d0\u4f9b\u4e86\u9ad8\u6548\u5185\u5b58\u7ba1\u7406\u89e3\u51b3\u65b9\u6848\uff0c\u6613\u4e8e\u96c6\u6210\u5230\u73b0\u6709\u667a\u80fd\u4f53\u6846\u67b6\u4e2d\uff0c\u4e3a\u6784\u5efa\u5927\u89c4\u6a21\u667a\u80fd\u4f53\u7cfb\u7edf\u63d0\u4f9b\u4e86\u6280\u672f\u57fa\u7840\u3002"}}
{"id": "2602.21626", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2602.21626", "abs": "https://arxiv.org/abs/2602.21626", "authors": ["Yifan Sun", "Gholamreza Haffar", "Minxian Xu", "Rajkumar Buyya", "Adel N. Toosi"], "title": "Multi-Layer Scheduling for MoE-Based LLM Reasoning", "comment": "12 pages, 10 figures", "summary": "Large Language Models (LLMs) have achieved remarkable success across a wide range of tasks, but serving them efficiently at scale remains a critical challenge due to their substantial computational and latency demands. While most existing inference frameworks rely on simple scheduling strategies such as First-Come-First-Serve (FCFS) at the engine level and Round-Robin (RR) at the scheduler or coordinator level, they often fail to fully utilize system resources and may suffer from issues such as head-of-line blocking and load imbalance. Recent advances in Mixture-of-Experts (MoE) models have also introduced new challenges in scheduling arising from expert parallelism and routing complexity. This research proposes a multi-layer scheduling framework tailored for MoE-based LLM serving. It targets scheduling at three levels: request-level, enginelevel, and expert-level. At the request level, we explore algorithms such as Shortest-Job-First (SJF) and priority-aware aging to improve throughput and reduce latency. At the engine level, we design load-aware dispatching strategies that account for the current prefix token load, KV cache utilization, and user stickiness to achieve better resource matching. At the expert level, we focus on alleviating expert hotspots and strategically placing inter-layer expert dependencies to balance load and improve routing efficiency. Extensive experimental results from more than 100 experiments conducted under diverse workload distributions show that our approach consistently outperforms the state-of-theart inference framework vLLM, achieving up to 17.8% reduction in Time To First Token (TTFT) latency and 13.3% reduction in Time-Per-Output-Token (TPOT) latency.", "AI": {"tldr": "\u63d0\u51fa\u9488\u5bf9MoE-based LLM\u670d\u52a1\u7684\u591a\u5c42\u6b21\u8c03\u5ea6\u6846\u67b6\uff0c\u5728\u8bf7\u6c42\u3001\u5f15\u64ce\u548c\u4e13\u5bb6\u4e09\u4e2a\u5c42\u9762\u4f18\u5316\u8c03\u5ea6\u7b56\u7565\uff0c\u663e\u8457\u964d\u4f4e\u5ef6\u8fdf\u5e76\u63d0\u9ad8\u541e\u5410\u91cf\u3002", "motivation": "\u73b0\u6709LLM\u63a8\u7406\u6846\u67b6\u4f7f\u7528\u7b80\u5355\u8c03\u5ea6\u7b56\u7565\u65e0\u6cd5\u5145\u5206\u5229\u7528\u7cfb\u7edf\u8d44\u6e90\uff0c\u5b58\u5728head-of-line\u963b\u585e\u548c\u8d1f\u8f7d\u4e0d\u5747\u8861\u95ee\u9898\uff1bMoE\u6a21\u578b\u5f15\u5165\u4e86\u4e13\u5bb6\u5e76\u884c\u548c\u8def\u7531\u590d\u6742\u6027\u7b49\u65b0\u6311\u6218\u3002", "method": "\u6784\u5efa\u4e09\u5c42\u8c03\u5ea6\u6846\u67b6\uff1a\u8bf7\u6c42\u5c42\u91c7\u7528SJF\u548c\u4f18\u5148\u7ea7\u611f\u77e5\u8001\u5316\u7b97\u6cd5\uff1b\u5f15\u64ce\u5c42\u8bbe\u8ba1\u8d1f\u8f7d\u611f\u77e5\u8c03\u5ea6\u7b56\u7565\uff0c\u8003\u8651\u524d\u7f00token\u8d1f\u8f7d\u3001KV\u7f13\u5b58\u5229\u7528\u7387\u548c\u7528\u6237\u7c98\u6027\uff1b\u4e13\u5bb6\u5c42\u7f13\u89e3\u4e13\u5bb6\u70ed\u70b9\u5e76 strategically \u653e\u7f6e\u5c42\u95f4\u4e13\u5bb6\u4f9d\u8d56\u5173\u7cfb\u3002", "result": "\u5728100\u591a\u6b21\u4e0d\u540c\u5de5\u4f5c\u8d1f\u8f7d\u5206\u5e03\u7684\u5b9e\u9a8c\u4e2d\uff0c\u663e\u8457\u4f18\u4e8evLLM\uff0cTTFT\u5ef6\u8fdf\u964d\u4f4e\u6700\u9ad8\u8fbe17.8%\uff0cTPOT\u5ef6\u8fdf\u964d\u4f4e\u6700\u9ad8\u8fbe13.3%\u3002", "conclusion": "\u591a\u5c42\u6b21\u8c03\u5ea6\u6846\u67b6\u80fd\u6709\u6548\u89e3\u51b3MoE-based LLM\u670d\u52a1\u4e2d\u7684\u8d44\u6e90\u5229\u7528\u548c\u5ef6\u8fdf\u95ee\u9898\uff0c\u4e3a\u5927\u89c4\u6a21LLM\u90e8\u7f72\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.21641", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2602.21641", "abs": "https://arxiv.org/abs/2602.21641", "authors": ["Man Zhang", "Yunyang Li", "Tao Yue"], "title": "Uncertainty Modeling for SysML v2", "comment": null, "summary": "Uncertainty is inherent in modern engineered systems, including cyber-physical systems, autonomous systems, and large-scale software-intensive infrastructures (such as microservice-based systems) operating in dynamic and partially observable environments. The recent publication of Precise Semantics for Uncertainty Modeling (PSUM) by the Object Management Group represents the first standardized specification for uncertainty modeling within the Model-Based Systems Engineering (MBSE) community, providing formally defined semantics for representing and reasoning about uncertainty in models. In parallel, the second version of Systems Modeling Language (SysML v2) was released as the next-generation systems modeling language, offering improved semantic rigor and reusability, yet lacking native constructs aligned with PSUM for first-class uncertainty representation. This paper proposes a systematic extension of SysML v2 that incorporates the PSUM metamodel into its modeling framework. The extension enables explicit specification of indeterminacy sources, structured characterization of uncertainties, and consistent propagation of uncertainty within system models, while preserving conformance with SysML v2 syntax and semantics. We validate the approach through seven case studies. Results demonstrate that the proposed extension (PSUM-SysMLv2) is expressive and applicable for uncertainty-aware MBSE, and potentially enables uncertainty and uncertainty propagation analyses.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5c06PSUM\u5143\u6a21\u578b\u6574\u5408\u5230SysML v2\u4e2d\u7684\u7cfb\u7edf\u5316\u6269\u5c55\u65b9\u6cd5\uff0c\u901a\u8fc7\u4e03\u4e2a\u6848\u4f8b\u7814\u7a76\u9a8c\u8bc1\u4e86\u8be5\u6269\u5c55\u5728\u4e0d\u786e\u5b9a\u611f\u77e5\u7684MBSE\u4e2d\u7684\u9002\u7528\u6027\uff0c\u5b9e\u73b0\u4e86\u4e0d\u786e\u5b9a\u6027\u6e90\u660e\u786e\u6307\u5b9a\u3001\u4e0d\u786e\u5b9a\u6027\u7ed3\u6784\u5316\u8868\u5f81\u548c\u7cfb\u7edf\u6a21\u578b\u4e2d\u4e0d\u786e\u5b9a\u6027\u7684\u4e00\u81f4\u4f20\u64ad\u3002", "motivation": "\u73b0\u4ee3\u5de5\u7a0b\u7cfb\u7edf\u5728\u52a8\u6001\u548c\u90e8\u5206\u53ef\u89c2\u5bdf\u73af\u5883\u4e2d\u8fd0\u884c\u5b58\u5728\u56fa\u6709\u4e0d\u786e\u5b9a\u6027\uff0c\u800c\u65b0\u4e00\u4ee3\u7cfb\u7edf\u5efa\u6a21\u8bed\u8a00SysML v2\u7f3a\u4e4f\u4e0ePSUM\u6807\u51c6\u5bf9\u9f50\u7684\u4e00\u6d41\u4e0d\u786e\u5b9a\u6027\u8868\u793a\u6784\u9020\u3002", "method": "\u63d0\u51fa\u5bf9SysML v2\u7684\u7cfb\u7edf\u5316\u6269\u5c55\uff0c\u5c06PSUM\u5143\u6a21\u578b\u6574\u5408\u5230\u5176\u5efa\u6a21\u6846\u67b6\u4e2d\uff0c\u540c\u65f6\u4fdd\u6301\u4e0eSysML v2\u8bed\u6cd5\u548c\u8bed\u4e49\u7684\u4e00\u81f4\u6027\u3002", "result": "\u901a\u8fc7\u4e03\u4e2a\u6848\u4f8b\u7814\u7a76\u9a8c\u8bc1\uff0c\u7ed3\u679c\u8868\u660ePSUM-SysMLv2\u6269\u5c55\u5728\u4e0d\u786e\u5b9a\u611f\u77e5\u7684MBSE\u4e2d\u5177\u6709\u8868\u8fbe\u6027\u548c\u9002\u7528\u6027\uff0c\u5e76\u53ef\u80fd\u652f\u6301\u4e0d\u786e\u5b9a\u6027\u53ca\u4f20\u64ad\u5206\u6790\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u6269\u5c55\u6210\u529f\u5730\u5c06\u4e0d\u786e\u5b9a\u6027\u5efa\u6a21\u80fd\u529b\u6574\u5408\u5230SysML v2\u4e2d\uff0c\u589e\u5f3a\u4e86\u8be5\u8bed\u8a00\u5728\u4e0d\u786e\u5b9a\u73af\u5883\u4e2d\u7cfb\u7edf\u5de5\u7a0b\u7684\u5b9e\u7528\u6027\u3002"}}
{"id": "2602.21788", "categories": ["cs.DC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.21788", "abs": "https://arxiv.org/abs/2602.21788", "authors": ["Yifan Niu", "Han Xiao", "Dongyi Liu", "Wei Zhou", "Jia Li"], "title": "DHP: Efficient Scaling of MLLM Training with Dynamic Hybrid Parallelism", "comment": null, "summary": "Scaling long-context capabilities is crucial for Multimodal Large Language Models (MLLMs). However, real-world multimodal datasets are extremely heterogeneous. Existing training frameworks predominantly rely on static parallelism strategies, which suffer from severe load imbalance, redundant communication, and suboptimal hardware utilization under data heterogeneity. In this work, we propose Dynamic Hybrid Parallelism (DHP), an efficient parallelism strategy that adaptively reconfigures communication groups and parallelism degrees during MLLM training. We generalize the non-power-of-two parallelism degrees and develop a polynomial-time algorithm to generate near-optimal parallelism strategies with only millisecond-level overhead per training batch. DHP is able to maintain high hardware efficiency even under extreme data variability. Experimental results demonstrate that DHP significantly outperforms Megatron-LM and DeepSpeed, achieving up to 1.36 $\\times$ speedup in training throughput while maintaining near-linear scaling efficiency across large-scale NPU clusters.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u52a8\u6001\u6df7\u5408\u5e76\u884c\u7b56\u7565(DHP)\uff0c\u80fd\u591f\u81ea\u9002\u5e94\u5730\u91cd\u65b0\u914d\u7f6e\u901a\u4fe1\u7ec4\u548c\u5e76\u884c\u5ea6\uff0c\u89e3\u51b3\u591a\u6a21\u6001\u5927\u6a21\u578b\u5728\u5f02\u6784\u6570\u636e\u96c6\u8bad\u7ec3\u4e2d\u7684\u8d1f\u8f7d\u5747\u8861\u548c\u786c\u4ef6\u5229\u7528\u7387\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u9759\u6001\u5e76\u884c\u7b56\u7565\u5728\u5904\u7406\u771f\u5b9e\u4e16\u754c\u591a\u6a21\u6001\u6570\u636e\u7684\u6781\u7aef\u5f02\u6784\u6027\u65f6\uff0c\u5b58\u5728\u4e25\u91cd\u7684\u8d1f\u8f7d\u4e0d\u5e73\u8861\u3001\u5197\u4f59\u901a\u4fe1\u548c\u6b21\u4f18\u786c\u4ef6\u5229\u7528\u95ee\u9898\u3002", "method": "DHP\u63a8\u5e7f\u4e86\u975e2\u7684\u5e42\u6b21\u5e76\u884c\u5ea6\uff0c\u5f00\u53d1\u4e86\u591a\u9879\u5f0f\u65f6\u95f4\u7b97\u6cd5\u751f\u6210\u8fd1\u4f18\u5e76\u884c\u7b56\u7565\uff0c\u6bcf\u4e2a\u8bad\u7ec3\u6279\u6b21\u4ec5\u6beb\u79d2\u7ea7\u5f00\u9500\uff0c\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u81ea\u9002\u5e94\u5730\u91cd\u65b0\u914d\u7f6e\u901a\u4fe1\u7ec4\u3002", "result": "DHP\u663e\u8457\u4f18\u4e8eMegatron-LM\u548cDeepSpeed\uff0c\u5728NPU\u96c6\u7fa4\u4e0a\u5b9e\u73b0\u9ad8\u8fbe1.36\u500d\u8bad\u7ec3\u541e\u5410\u91cf\u63d0\u5347\uff0c\u540c\u65f6\u4fdd\u6301\u8fd1\u7ebf\u6027\u6269\u5c55\u6548\u7387\u3002", "conclusion": "DHP\u901a\u8fc7\u52a8\u6001\u9002\u5e94\u5e76\u884c\u7b56\u7565\u6709\u6548\u89e3\u51b3\u4e86\u591a\u6a21\u6001\u5927\u6a21\u578b\u5728\u5f02\u6784\u6570\u636e\u4e0a\u7684\u8bad\u7ec3\u6311\u6218\uff0c\u63d0\u9ad8\u4e86\u786c\u4ef6\u5229\u7528\u7387\u548c\u8bad\u7ec3\u6548\u7387\u3002"}}
{"id": "2602.22158", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2602.22158", "abs": "https://arxiv.org/abs/2602.22158", "authors": ["Minqiu Sun", "Xin Huang", "Luanzheng Guo", "Nathan R. Tallent", "Kento Sato", "Dong Dai"], "title": "LLMTailor: A Layer-wise Tailoring Tool for Efficient Checkpointing of Large Language Models", "comment": "9 pages, 3 figures, accepted at PDSW'25", "summary": "Checkpointing is essential for fault tolerance in training large language models (LLMs). However, existing methods, regardless of their I/O strategies, periodically store the entire model and optimizer states, incurring substantial storage overhead and resource contention. Recent studies reveal that updates across LLM layers are highly non-uniform. Across training steps, some layers may undergo more significant changes, while others remain relatively stable or even unchanged. This suggests that selectively checkpointing only layers with significant updates could reduce overhead without harming training. Implementing such selective strategies requires fine-grained control over both weights and optimizer states, which no current tool provides. To address this gap, we propose \\texttt{LLMTailor}, a checkpoint-merging framework that filters and assembles layers from different checkpoints to form a composite checkpoint. Our evaluation indicates that LLMTailor can work with different selective checkpointing strategies and effectively reduce checkpoint size (e.g., 4.3 times smaller for Llama3.1-8B) and checkpoint time (e.g., 2.8 times faster for Qwen2.5-7B) while maintaining model quality.", "AI": {"tldr": "LLMTailor\u662f\u4e00\u4e2a\u68c0\u67e5\u70b9\u5408\u5e76\u6846\u67b6\uff0c\u901a\u8fc7\u9009\u62e9\u6027\u5730\u4ec5\u68c0\u67e5\u70b9\u6709\u663e\u8457\u66f4\u65b0\u7684\u5c42\uff0c\u5927\u5e45\u51cf\u5c11\u5927\u578b\u8bed\u8a00\u6a21\u578b\u8bad\u7ec3\u4e2d\u7684\u68c0\u67e5\u70b9\u5927\u5c0f\u548c\u65f6\u95f4\uff0c\u540c\u65f6\u4fdd\u6301\u6a21\u578b\u8d28\u91cf\u3002", "motivation": "\u73b0\u6709\u68c0\u67e5\u70b9\u65b9\u6cd5\u65e0\u8bbaI/O\u7b56\u7565\u5982\u4f55\uff0c\u90fd\u9700\u8981\u5468\u671f\u6027\u5b58\u50a8\u6574\u4e2a\u6a21\u578b\u548c\u4f18\u5316\u5668\u72b6\u6001\uff0c\u5bfc\u81f4\u5de8\u5927\u7684\u5b58\u50a8\u5f00\u9500\u548c\u8d44\u6e90\u7ade\u4e89\u3002\u7814\u7a76\u8868\u660eLLM\u5404\u5c42\u66f4\u65b0\u9ad8\u5ea6\u4e0d\u5747\u5300\uff0c\u8fd9\u4e3a\u9009\u62e9\u6027\u68c0\u67e5\u70b9\u63d0\u4f9b\u4e86\u53ef\u80fd\u6027\u3002", "method": "LLMTailor\u662f\u4e00\u4e2a\u68c0\u67e5\u70b9\u5408\u5e76\u6846\u67b6\uff0c\u80fd\u591f\u8fc7\u6ee4\u548c\u7ec4\u88c5\u6765\u81ea\u4e0d\u540c\u68c0\u67e5\u70b9\u7684\u5c42\uff0c\u5f62\u6210\u590d\u5408\u68c0\u67e5\u70b9\u3002\u5b83\u63d0\u4f9b\u4e86\u5bf9\u6743\u91cd\u548c\u4f18\u5316\u5668\u72b6\u6001\u7684\u7ec6\u7c92\u5ea6\u63a7\u5236\uff0c\u652f\u6301\u9009\u62e9\u6027\u68c0\u67e5\u70b9\u7b56\u7565\u3002", "result": "\u8bc4\u4f30\u663e\u793aLLMTailor\u53ef\u4e0e\u4e0d\u540c\u9009\u62e9\u6027\u68c0\u67e5\u70b9\u7b56\u7565\u914d\u5408\u4f7f\u7528\uff0c\u6709\u6548\u51cf\u5c11\u68c0\u67e5\u70b9\u5927\u5c0f\uff08\u4f8b\u5982Llama3.1-8B\u51cf\u5c114.3\u500d\uff09\u548c\u68c0\u67e5\u70b9\u65f6\u95f4\uff08\u4f8b\u5982Qwen2.5-7B\u52a0\u901f2.8\u500d\uff09\uff0c\u540c\u65f6\u4fdd\u6301\u6a21\u578b\u8d28\u91cf\u3002", "conclusion": "LLMTailor\u89e3\u51b3\u4e86\u73b0\u6709\u68c0\u67e5\u70b9\u65b9\u6cd5\u5728\u5927\u578b\u8bed\u8a00\u6a21\u578b\u8bad\u7ec3\u4e2d\u7684\u5b58\u50a8\u548c\u6548\u7387\u95ee\u9898\uff0c\u901a\u8fc7\u9009\u62e9\u6027\u68c0\u67e5\u70b9\u663e\u8457\u964d\u4f4e\u4e86\u5f00\u9500\uff0c\u4e3a\u66f4\u9ad8\u6548\u3001\u66f4\u5177\u6210\u672c\u6548\u76ca\u7684LLM\u8bad\u7ec3\u63d0\u4f9b\u4e86\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\u3002"}}
