{"id": "2512.20703", "categories": ["cs.SE", "cs.ET"], "pdf": "https://arxiv.org/pdf/2512.20703", "abs": "https://arxiv.org/abs/2512.20703", "authors": ["Matthias Stierle", "Karsten Kraume", "Martin Matzner"], "title": "Process Analytics -- Data-driven Business Process Management", "comment": null, "summary": "Data-driven analysis of business processes has a long tradition in research. However, recently the term of process mining is mostly used when referring to data-driven process analysis. As a consequence, awareness for the many facets of process analysis is decreasing. In particular, while an increasing focus is put onto technical aspects of the analysis, human and organisational concerns remain under the radar. Following the socio-technical perspective of information systems research, we propose a new perspective onto data-driven process analysis that combines the process of analysis with the organisation and its stakeholders. This paper conceptualises the term process analytics and its various dimensions by following both an inductive and deductive approach. The results are discussed by contrasting them to a real-life case study from a large company implementing data-driven process analysis and automation.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u7ec4\u7ec7\u4e0e\u5229\u76ca\u76f8\u5173\u8005\u7684\u65b0\u89c6\u89d2\uff0c\u4ee5\u91cd\u65b0\u5b9a\u4e49\u6570\u636e\u9a71\u52a8\u7684\u6d41\u7a0b\u5206\u6790\uff0c\u5e76\u901a\u8fc7\u6848\u4f8b\u7814\u7a76\u9a8c\u8bc1\u5176\u6709\u6548\u6027\u3002", "motivation": "\u5f53\u524d\u6d41\u7a0b\u6316\u6398\u7814\u7a76\u8fc7\u4e8e\u5173\u6ce8\u6280\u672f\u5c42\u9762\uff0c\u5ffd\u89c6\u4e86\u4eba\u6587\u4e0e\u7ec4\u7ec7\u56e0\u7d20\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u66f4\u5168\u9762\u7684\u5206\u6790\u6846\u67b6\u3002", "method": "\u91c7\u7528\u5f52\u7eb3\u4e0e\u6f14\u7ece\u76f8\u7ed3\u5408\u7684\u65b9\u6cd5\uff0c\u6784\u5efa\u201c\u6d41\u7a0b\u5206\u6790\u5b66\u201d\u6982\u5ff5\u53ca\u5176\u591a\u7ef4\u6846\u67b6\uff0c\u5e76\u7ed3\u5408\u5927\u578b\u4f01\u4e1a\u6848\u4f8b\u8fdb\u884c\u5bf9\u6bd4\u8ba8\u8bba\u3002", "result": "\u63d0\u51fa\u7684\u6d41\u7a0b\u5206\u6790\u5b66\u6846\u67b6\u80fd\u591f\u66f4\u597d\u5730\u6574\u5408\u6280\u672f\u3001\u7ec4\u7ec7\u4e0e\u4eba\u5458\u56e0\u7d20\uff0c\u63d0\u5347\u6570\u636e\u9a71\u52a8\u6d41\u7a0b\u5206\u6790\u7684\u5b9e\u9645\u5e94\u7528\u6548\u679c\u3002", "conclusion": "\u5c06\u793e\u4f1a\u6280\u672f\u89c6\u89d2\u5f15\u5165\u6d41\u7a0b\u5206\u6790\uff0c\u6709\u52a9\u4e8e\u5f25\u8865\u73b0\u6709\u7814\u7a76\u7684\u4e0d\u8db3\uff0c\u63a8\u52a8\u66f4\u5168\u9762\u548c\u5b9e\u7528\u7684\u4e1a\u52a1\u6d41\u7a0b\u6539\u8fdb\u65b9\u6cd5\u3002"}}
{"id": "2512.21238", "categories": ["cs.SE", "cs.CR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.21238", "abs": "https://arxiv.org/abs/2512.21238", "authors": ["Mohammed Latif Siddiq", "Natalie Sekerak", "Antonio Karam", "Maria Leal", "Arvin Islam-Gomes", "Joanna C. S. Santos"], "title": "Assessing the Software Security Comprehension of Large Language Models", "comment": "Submitted to Empirical Software Engineering (EMSE) journal", "summary": "Large language models (LLMs) are increasingly used in software development, but their level of software security expertise remains unclear. This work systematically evaluates the security comprehension of five leading LLMs: GPT-4o-Mini, GPT-5-Mini, Gemini-2.5-Flash, Llama-3.1, and Qwen-2.5, using Blooms Taxonomy as a framework. We assess six cognitive dimensions: remembering, understanding, applying, analyzing, evaluating, and creating. Our methodology integrates diverse datasets, including curated multiple-choice questions, vulnerable code snippets (SALLM), course assessments from an Introduction to Software Security course, real-world case studies (XBOW), and project-based creation tasks from a Secure Software Engineering course. Results show that while LLMs perform well on lower-level cognitive tasks such as recalling facts and identifying known vulnerabilities, their performance degrades significantly on higher-order tasks that require reasoning, architectural evaluation, and secure system creation. Beyond reporting aggregate accuracy, we introduce a software security knowledge boundary that identifies the highest cognitive level at which a model consistently maintains reliable performance. In addition, we identify 51 recurring misconception patterns exhibited by LLMs across Blooms levels.", "AI": {"tldr": "\u672c\u6587\u7cfb\u7edf\u8bc4\u4f30\u4e86\u4e94\u79cd\u4e3b\u6d41\u5927\u8bed\u8a00\u6a21\u578b\u5728\u8f6f\u4ef6\u5b89\u5168\u9886\u57df\u7684\u8ba4\u77e5\u80fd\u529b\uff0c\u53d1\u73b0\u5176\u5728\u4f4e\u9636\u4efb\u52a1\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728\u9ad8\u9636\u63a8\u7406\u4e0e\u7cfb\u7edf\u6784\u5efa\u4efb\u52a1\u4e2d\u663e\u8457\u9000\u5316\uff0c\u5e76\u8bc6\u522b\u51fa51\u79cd\u5e38\u89c1\u8bef\u89e3\u6a21\u5f0f\u3002", "motivation": "\u5398\u6e05\u5f53\u524d\u5927\u8bed\u8a00\u6a21\u578b\u5728\u8f6f\u4ef6\u5b89\u5168\u4e13\u4e1a\u9886\u57df\u7684\u5b9e\u9645\u80fd\u529b\u8fb9\u754c\uff0c\u4ee5\u6307\u5bfc\u5176\u5408\u7406\u5e94\u7528\u3002", "method": "\u57fa\u4e8e\u5e03\u9c81\u59c6\u5206\u7c7b\u6cd5\u516d\u5927\u8ba4\u77e5\u7ef4\u5ea6\uff0c\u7ed3\u5408\u591a\u7c7b\u6570\u636e\u96c6\uff08\u9009\u62e9\u9898\u3001\u6f0f\u6d1e\u4ee3\u7801\u3001\u8bfe\u7a0b\u8003\u9898\u3001\u6848\u4f8b\u7814\u7a76\u3001\u9879\u76ee\u4efb\u52a1\uff09\u5bf9\u4e94\u79cdLLM\u8fdb\u884c\u7cfb\u7edf\u8bc4\u6d4b\u3002", "result": "\u6a21\u578b\u5728\u8bb0\u5fc6\u4e0e\u7406\u89e3\u5c42\u9762\u51c6\u786e\u7387\u9ad8\uff0c\u4f46\u5728\u5206\u6790\u3001\u8bc4\u4f30\u4e0e\u521b\u9020\u7b49\u9ad8\u9636\u4efb\u52a1\u4e0a\u6027\u80fd\u660e\u663e\u4e0b\u964d\uff1b\u63d0\u51fa\u2018\u5b89\u5168\u77e5\u8bc6\u8fb9\u754c\u2019\u6982\u5ff5\u5e76\u5f52\u7eb351\u79cd\u5178\u578b\u8bef\u89e3\u6a21\u5f0f\u3002", "conclusion": "\u5f53\u524dLLM\u5c1a\u672a\u5177\u5907\u5168\u9762\u80dc\u4efb\u8f6f\u4ef6\u5b89\u5168\u9ad8\u9636\u4efb\u52a1\u7684\u80fd\u529b\uff0c\u9700\u8c28\u614e\u7528\u4e8e\u67b6\u6784\u8bbe\u8ba1\u4e0e\u7cfb\u7edf\u6784\u5efa\u573a\u666f\u3002"}}
{"id": "2512.20823", "categories": ["cs.AR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.20823", "abs": "https://arxiv.org/abs/2512.20823", "authors": ["Razine Moundir Ghorab", "Emanuele Parisi", "Cristian Gutierrez", "Miquel Alberti-Binimelis", "Miquel Moreto", "Dario Garcia-Gasulla", "Gokcen Kestor"], "title": "NotSoTiny: A Large, Living Benchmark for RTL Code Generation", "comment": "9 pages, 5 figures", "summary": "LLMs have shown early promise in generating RTL code, yet evaluating their capabilities in realistic setups remains a challenge. So far, RTL benchmarks have been limited in scale, skewed toward trivial designs, offering minimal verification rigor, and remaining vulnerable to data contamination. To overcome these limitations and to push the field forward, this paper introduces NotSoTiny, a benchmark that assesses LLM on the generation of structurally rich and context-aware RTL. Built from hundreds of actual hardware designs produced by the Tiny Tapeout community, our automated pipeline removes duplicates, verifies correctness and periodically incorporates new designs to mitigate contamination, matching Tiny Tapeout release schedule. Evaluation results show that NotSoTiny tasks are more challenging than prior benchmarks, emphasizing its effectiveness in overcoming current limitations of LLMs applied to hardware design, and in guiding the improvement of such promising technology.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faNotSoTiny\u57fa\u51c6\uff0c\u7528\u4e8e\u8bc4\u4f30LLM\u5728\u751f\u6210\u7ed3\u6784\u4e30\u5bcc\u3001\u4e0a\u4e0b\u6587\u611f\u77e5\u7684RTL\u4ee3\u7801\u65b9\u9762\u7684\u80fd\u529b\u3002", "motivation": "\u73b0\u6709RTL\u57fa\u51c6\u89c4\u6a21\u6709\u9650\u3001\u8bbe\u8ba1\u7b80\u5355\u3001\u9a8c\u8bc1\u4e0d\u8db3\u4e14\u6613\u53d7\u6570\u636e\u6c61\u67d3\u5f71\u54cd\uff0c\u9700\u66f4\u6709\u6548\u7684\u8bc4\u4f30\u65b9\u6cd5\u63a8\u52a8LLM\u5728\u786c\u4ef6\u8bbe\u8ba1\u4e2d\u7684\u53d1\u5c55\u3002", "method": "\u57fa\u4e8eTiny Tapeout\u793e\u533a\u6570\u767e\u4e2a\u771f\u5b9e\u786c\u4ef6\u8bbe\u8ba1\uff0c\u6784\u5efa\u81ea\u52a8\u5316\u6d41\u7a0b\u53bb\u91cd\u3001\u9a8c\u8bc1\u6b63\u786e\u6027\u5e76\u5b9a\u671f\u66f4\u65b0\u8bbe\u8ba1\u4ee5\u907f\u514d\u6570\u636e\u6c61\u67d3\u3002", "result": "\u8bc4\u4f30\u8868\u660eNotSoTiny\u4efb\u52a1\u6bd4\u4ee5\u5f80\u57fa\u51c6\u66f4\u5177\u6311\u6218\u6027\uff0c\u80fd\u6709\u6548\u63ed\u793aLLM\u5728\u786c\u4ef6\u8bbe\u8ba1\u4e2d\u7684\u5c40\u9650\u6027\u3002", "conclusion": "NotSoTiny\u4e3a\u6539\u8fdbLLM\u5728\u786c\u4ef6\u8bbe\u8ba1\u9886\u57df\u7684\u5e94\u7528\u63d0\u4f9b\u4e86\u53ef\u9760\u4e14\u5177\u6307\u5bfc\u610f\u4e49\u7684\u8bc4\u4f30\u57fa\u51c6\u3002"}}
{"id": "2512.20795", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2512.20795", "abs": "https://arxiv.org/abs/2512.20795", "authors": ["Aymen Alsaadi", "Mason Hooten", "Mariya Goliyad", "Andre Merzky", "Andrew Shao", "Mikhail Titov", "Tianle Wang", "Yian Chen", "Maria Kalantzi", "Kent Lee", "Andrew Park", "Indira Pimpalkhare", "Nick Radcliffe", "Colin Wahl", "Pete Mendygral", "Matteo Turilli", "Shantenu Jha"], "title": "RHAPSODY: Execution of Hybrid AI-HPC Workflows at Scale", "comment": null, "summary": "Hybrid AI-HPC workflows combine large-scale simulation, training, high-throughput inference, and tightly coupled, agent-driven control within a single execution campaign. These workflows impose heterogeneous and often conflicting requirements on runtime systems, spanning MPI executables, persistent AI services, fine-grained tasks, and low-latency AI-HPC coupling. Existing systems typically address only subsets of these requirements, limiting their ability to support emerging AI-HPC applications at scale. We present RHAPSODY, a multi-runtime middleware that enables concurrent execution of heterogeneous AI-HPC workloads through uniform abstractions for tasks, services, resources, and execution policies. Rather than replacing existing runtimes, RHAPSODY composes and coordinates them, allowing simulation codes, inference services, and agentic workflows to coexist within a single job allocation on leadership-class HPC platforms. We evaluate RHAPSODY with Dragon and vLLM on multiple HPC systems using representative heterogeneous, inference-at-scale, and tightly coupled AI-HPC workflows. Our results show that RHAPSODY introduces minimal runtime overhead, sustains increasing heterogeneity at scale, achieves near-linear scaling for high-throughput inference workloads, and data- and control-efficient coupling between AI and HPC tasks in agentic workflows.", "AI": {"tldr": "RHAPSODY\u662f\u4e00\u4e2a\u591a\u8fd0\u884c\u65f6\u4e2d\u95f4\u4ef6\uff0c\u652f\u6301\u5728\u8d85\u7b97\u5e73\u53f0\u4e0a\u5e76\u53d1\u6267\u884c\u5f02\u6784AI-HPC\u5de5\u4f5c\u6d41\uff0c\u901a\u8fc7\u7edf\u4e00\u62bd\u8c61\u534f\u8c03\u73b0\u6709\u8fd0\u884c\u65f6\uff0c\u5b9e\u73b0\u4f4e\u5f00\u9500\u4e0e\u9ad8\u6548\u6269\u5c55\u3002", "motivation": "\u73b0\u6709\u7cfb\u7edf\u96be\u4ee5\u5168\u9762\u652f\u6301AI-HPC\u6df7\u5408\u5de5\u4f5c\u6d41\u7684\u5f02\u6784\u9700\u6c42\uff0c\u9650\u5236\u4e86\u5927\u89c4\u6a21\u5e94\u7528\u7684\u53d1\u5c55\u3002", "method": "\u8bbe\u8ba1RHAPSODY\u4e2d\u95f4\u4ef6\uff0c\u6574\u5408MPI\u3001\u6301\u4e45AI\u670d\u52a1\u3001\u7ec6\u7c92\u5ea6\u4efb\u52a1\u7b49\uff0c\u901a\u8fc7\u7edf\u4e00\u62bd\u8c61\u534f\u8c03\u591a\u79cd\u8fd0\u884c\u65f6\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660eRHAPSODY\u5f00\u9500\u6781\u5c0f\uff0c\u652f\u6301\u9ad8\u5f02\u6784\u6027\u6269\u5c55\uff0c\u63a8\u7406\u8d1f\u8f7d\u8fd1\u7ebf\u6027\u6269\u5c55\uff0cAI\u4e0eHPC\u4efb\u52a1\u8026\u5408\u9ad8\u6548\u3002", "conclusion": "RHAPSODY\u6709\u6548\u89e3\u51b3\u4e86AI-HPC\u6df7\u5408\u5de5\u4f5c\u6d41\u7684\u8fd0\u884c\u65f6\u6311\u6218\uff0c\u4e3a\u9886\u5bfc\u7ea7\u8d85\u7b97\u5e73\u53f0\u63d0\u4f9b\u53ef\u6269\u5c55\u652f\u6301\u3002"}}
{"id": "2512.20778", "categories": ["cs.MA", "cs.AI", "cs.RO"], "pdf": "https://arxiv.org/pdf/2512.20778", "abs": "https://arxiv.org/abs/2512.20778", "authors": ["Moshe Rafaeli Shimron", "Vadim Indelman"], "title": "Towards Optimal Performance and Action Consistency Guarantees in Dec-POMDPs with Inconsistent Beliefs and Limited Communication", "comment": "9 pages, 3 figures, 2 tables", "summary": "Multi-agent decision-making under uncertainty is fundamental for effective and safe autonomous operation. In many real-world scenarios, each agent maintains its own belief over the environment and must plan actions accordingly. However, most existing approaches assume that all agents have identical beliefs at planning time, implying these beliefs are conditioned on the same data. Such an assumption is often impractical due to limited communication. In reality, agents frequently operate with inconsistent beliefs, which can lead to poor coordination and suboptimal, potentially unsafe, performance. In this paper, we address this critical challenge by introducing a novel decentralized framework for optimal joint action selection that explicitly accounts for belief inconsistencies. Our approach provides probabilistic guarantees for both action consistency and performance with respect to open-loop multi-agent POMDP (which assumes all data is always communicated), and selectively triggers communication only when needed. Furthermore, we address another key aspect of whether, given a chosen joint action, the agents should share data to improve expected performance in inference. Simulation results show our approach outperforms state-of-the-art algorithms.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u8003\u8651\u4fe1\u5ff5\u4e0d\u4e00\u81f4\u7684\u53bb\u4e2d\u5fc3\u5316\u591a\u667a\u80fd\u4f53\u51b3\u7b56\u6846\u67b6\uff0c\u63d0\u5347\u534f\u4f5c\u6027\u80fd\u4e0e\u5b89\u5168\u6027\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5047\u8bbe\u667a\u80fd\u4f53\u4fe1\u5ff5\u4e00\u81f4\uff0c\u4f46\u73b0\u5b9e\u4e2d\u56e0\u901a\u4fe1\u53d7\u9650\u5bfc\u81f4\u4fe1\u5ff5\u4e0d\u4e00\u81f4\uff0c\u5f71\u54cd\u534f\u4f5c\u6548\u679c\u4e0e\u5b89\u5168\u3002", "method": "\u6784\u5efa\u65b0\u6846\u67b6\u4ee5\u663e\u5f0f\u5904\u7406\u4fe1\u5ff5\u4e0d\u4e00\u81f4\uff0c\u63d0\u4f9b\u52a8\u4f5c\u4e00\u81f4\u6027\u4e0e\u6027\u80fd\u7684\u6982\u7387\u4fdd\u8bc1\uff0c\u5e76\u6309\u9700\u89e6\u53d1\u901a\u4fe1\u3002", "result": "\u4eff\u771f\u8868\u660e\u8be5\u65b9\u6cd5\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u7b97\u6cd5\u3002", "conclusion": "\u8be5\u6846\u67b6\u6709\u6548\u5e94\u5bf9\u4fe1\u5ff5\u4e0d\u4e00\u81f4\u95ee\u9898\uff0c\u517c\u987e\u6027\u80fd\u4e0e\u901a\u4fe1\u6548\u7387\uff0c\u9002\u7528\u4e8e\u5b9e\u9645\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u3002"}}
{"id": "2512.20939", "categories": ["cs.DC", "cs.CC"], "pdf": "https://arxiv.org/pdf/2512.20939", "abs": "https://arxiv.org/abs/2512.20939", "authors": ["James Aspnes"], "title": "Stochastic well-structured transition systems", "comment": "54 pages, 4 figures", "summary": "Extending well-structured transition systems to incorporate a probabilistic scheduling rule, we define a new class of stochastic well-structured transition systems that includes population protocols, chemical reaction networks, and many common gossip models; as well as augmentations of these systems by an oracle that exposes a total order on agents as in population protocols in the comparison model or an equivalence relation as in population protocols with unordered data.\n  We show that any implementation of a phase clock in these systems either stops or ticks too fast after polynomially many expected steps, and that any terminating computation in these systems finishes or fails in expected polynomial time. This latter property allows an exact characterization of the computational power of many stochastic well-structured transition systems augmented with a total order or equivalence relation on agents, showing that these compute exactly the languages in BPP, while the corresponding unaugmented systems compute just the symmetric languages in BPL.", "AI": {"tldr": "\u8be5\u8bba\u6587\u5b9a\u4e49\u4e86\u4e00\u7c7b\u65b0\u7684\u968f\u673a\u826f\u6784\u8fc1\u79fb\u7cfb\u7edf\uff0c\u6db5\u76d6\u591a\u79cd\u5206\u5e03\u5f0f\u8ba1\u7b97\u6a21\u578b\uff0c\u5e76\u8bc1\u660e\u5176\u8ba1\u7b97\u80fd\u529b\u4e0eBPP\u7c7b\u8bed\u8a00\u7b49\u4ef7\u3002", "motivation": "\u7814\u7a76\u968f\u673a\u8c03\u5ea6\u89c4\u5219\u4e0b\u826f\u6784\u8fc1\u79fb\u7cfb\u7edf\u7684\u8ba1\u7b97\u80fd\u529b\u53ca\u5176\u6269\u5c55\u7279\u6027\u3002", "method": "\u901a\u8fc7\u5f15\u5165\u76f8\u4f4d\u65f6\u949f\u548c\u7ec8\u6b62\u8ba1\u7b97\u7684\u671f\u671b\u65f6\u95f4\u5206\u6790\uff0c\u523b\u753b\u7cfb\u7edf\u8ba1\u7b97\u80fd\u529b\u3002", "result": "\u8bc1\u660e\u6b64\u7c7b\u7cfb\u7edf\u5728\u591a\u9879\u5f0f\u671f\u671b\u65f6\u95f4\u5185\u5b8c\u6210\u6216\u5931\u8d25\uff0c\u4e14\u589e\u5f3a\u7cfb\u7edf\u53ef\u8ba1\u7b97BPP\u7c7b\u8bed\u8a00\u3002", "conclusion": "\u589e\u5f3a\u540e\u7684\u968f\u673a\u826f\u6784\u8fc1\u79fb\u7cfb\u7edf\u7cbe\u786e\u5bf9\u5e94BPP\u7c7b\u8bed\u8a00\uff0c\u672a\u589e\u5f3a\u7cfb\u7edf\u4ec5\u5bf9\u5e94\u5bf9\u79f0BPL\u7c7b\u8bed\u8a00\u3002"}}
{"id": "2512.20973", "categories": ["cs.MA"], "pdf": "https://arxiv.org/pdf/2512.20973", "abs": "https://arxiv.org/abs/2512.20973", "authors": ["Yihan Xia", "Taotao Wang", "Wenxin Xu", "Shengli Zhang"], "title": "DAO-Agent: Zero Knowledge-Verified Incentives for Decentralized Multi-Agent Coordination", "comment": "10 pages, 1 figure", "summary": "Autonomous Large Language Model (LLM)-based multi-agent systems have emerged as a promising paradigm for facilitating cross-application and cross-organization collaborations. These autonomous agents often operate in trustless environments, where centralized coordination faces significant challenges, such as the inability to ensure transparent contribution measurement and equitable incentive distribution. While blockchain is frequently proposed as a decentralized coordination platform, it inherently introduces high on-chain computation costs and risks exposing sensitive execution information of the agents. Consequently, the core challenge lies in enabling auditable task execution and fair incentive distribution for autonomous LLM agents in trustless environments, while simultaneously preserving their strategic privacy and minimizing on-chain costs. To address this challenge, we propose DAO-Agent, a novel framework that integrates three key technical innovations: (1) an on-chain decentralized autonomous organization (DAO) governance mechanism for transparent coordination and immutable logging; (2) a ZKP mechanism approach that enables Shapley-based contribution measurement off-chain, and (3) a hybrid on-chain/off-chain architecture that verifies ZKP-validated contribution measurements on-chain with minimal computational overhead. We implement DAO-Agent and conduct end-to-end experiments using a crypto trading task as a case study. Experimental results demonstrate that DAO-Agent achieves up to 99.9% reduction in verification gas costs compared to naive on-chain alternatives, with constant-time verification complexity that remains stable as coalition size increases, thereby establishing a scalable foundation for agent coordination in decentralized environments.", "AI": {"tldr": "DAO-Agent\u6846\u67b6\u901a\u8fc7\u7ed3\u5408\u94fe\u4e0aDAO\u6cbb\u7406\u3001\u96f6\u77e5\u8bc6\u8bc1\u660e\u548c\u6df7\u5408\u67b6\u6784\uff0c\u5b9e\u73b0LLM\u591a\u667a\u80fd\u4f53\u5728\u53bb\u4e2d\u5fc3\u5316\u73af\u5883\u4e2d\u7684\u53ef\u5ba1\u8ba1\u6267\u884c\u4e0e\u516c\u5e73\u6fc0\u52b1\uff0c\u540c\u65f6\u4fdd\u62a4\u9690\u79c1\u5e76\u5927\u5e45\u964d\u4f4e\u94fe\u4e0a\u6210\u672c\u3002", "motivation": "\u89e3\u51b3\u5728\u65e0\u4fe1\u4efb\u73af\u5883\u4e2d\u81ea\u4e3bLLM\u667a\u80fd\u4f53\u534f\u4f5c\u65f6\u9762\u4e34\u7684\u900f\u660e\u8d21\u732e\u5ea6\u91cf\u3001\u516c\u5e73\u6fc0\u52b1\u5206\u914d\u53ca\u9690\u79c1\u4fdd\u62a4\u95ee\u9898\u3002", "method": "\u63d0\u51faDAO-Agent\u6846\u67b6\uff0c\u5305\u542b\u94fe\u4e0aDAO\u6cbb\u7406\u673a\u5236\u3001\u57fa\u4e8eZKP\u7684\u94fe\u4e0bShapley\u8d21\u732e\u5ea6\u8ba1\u7b97\u3001\u4ee5\u53ca\u94fe\u4e0a\u9a8c\u8bc1ZKP\u7ed3\u679c\u7684\u6df7\u5408\u67b6\u6784\u3002", "result": "\u5b9e\u9a8c\u8868\u660eDAO-Agent\u76f8\u6bd4\u7eaf\u94fe\u4e0a\u65b9\u6848\u51cf\u5c1199.9%\u9a8c\u8bc1Gas\u6210\u672c\uff0c\u4e14\u9a8c\u8bc1\u590d\u6742\u5ea6\u4e0d\u968f\u8054\u76df\u89c4\u6a21\u589e\u957f\uff0c\u5177\u5907\u9ad8\u53ef\u6269\u5c55\u6027\u3002", "conclusion": "DAO-Agent\u4e3a\u53bb\u4e2d\u5fc3\u5316\u73af\u5883\u4e2d\u5927\u89c4\u6a21LLM\u667a\u80fd\u4f53\u534f\u4f5c\u63d0\u4f9b\u4e86\u517c\u987e\u6548\u7387\u3001\u516c\u5e73\u4e0e\u9690\u79c1\u7684\u53ef\u884c\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2512.20953", "categories": ["cs.DC", "cs.NI"], "pdf": "https://arxiv.org/pdf/2512.20953", "abs": "https://arxiv.org/abs/2512.20953", "authors": ["Yuxiao Wang", "Yuedong Xu", "Qingyang Duan", "Yuxuan Liu", "Lei Jiao", "Yinghao Yu", "Jun Wu"], "title": "Diving into 3D Parallelism with Heterogeneous Spot Instance GPUs: Design and Implications", "comment": null, "summary": "The rapid growth of large language models (LLMs) and the continuous release of new GPU products have significantly increased the demand for distributed training across heterogeneous GPU environments. In this paper, we present a comprehensive analysis of the challenges involved in implementing 3D parallelism in such environments, addressing critical issues such as the need for symmetric tensor parallelism, efficient gradient synchronization in asymmetric pipeline parallelism, and the trade-offs between memory utilization and computational efficiency. Building upon these insights, we introduce AutoHet, a novel system that automatically identifies the optimal parallelism plan for distributed training on heterogeneous GPUs. AutoHet supports asymmetric 3D parallelism structures and facilitates fine-grained workload distribution. We propose a theoretical model that frames the device grouping and load balancing as an optimization problem to minimize per-iteration training time, thus effectively balancing computing power and memory usage across GPUs with diverse capabilities. To enable elastic training upon spot instance preemption, AutoHet presents an efficient recovery strategy that prioritizes to retrieve training states from local nodes, and only downloads the missing checkpoints from the cloud storage. Our extensive evaluation, conducted on three large-scale models and utilizing combinations of three different GPU types, demonstrates that AutoHet outperforms existing DNN training systems, achieving up to a 1.79$\\times$ speedup in training throughput compared with Megatron-LM and Whale, and a 4.38$\\times$ speedup of recovery speed compared to a spot instance baseline.", "AI": {"tldr": "AutoHet \u662f\u4e00\u79cd\u9488\u5bf9\u5f02\u6784 GPU \u73af\u5883\u4f18\u5316\u7684\u5206\u5e03\u5f0f\u8bad\u7ec3\u7cfb\u7edf\uff0c\u652f\u6301\u975e\u5bf9\u79f0\u4e09\u7ef4\u5e76\u884c\u4e0e\u5f39\u6027\u6062\u590d\uff0c\u663e\u8457\u63d0\u5347\u8bad\u7ec3\u541e\u5410\u91cf\u4e0e\u5bb9\u9519\u6548\u7387\u3002", "motivation": "\u5e94\u5bf9\u5927\u8bed\u8a00\u6a21\u578b\u589e\u957f\u4e0e GPU \u5f02\u6784\u5316\u5e26\u6765\u7684\u5206\u5e03\u5f0f\u8bad\u7ec3\u6311\u6218\uff0c\u89e3\u51b3\u5e76\u884c\u7b56\u7565\u9002\u914d\u3001\u8d1f\u8f7d\u5747\u8861\u4e0e\u5bb9\u9519\u6062\u590d\u96be\u9898\u3002", "method": "\u63d0\u51fa AutoHet \u7cfb\u7edf\uff0c\u5efa\u6a21\u8bbe\u5907\u5206\u7ec4\u4e0e\u8d1f\u8f7d\u5e73\u8861\u4e3a\u4f18\u5316\u95ee\u9898\uff0c\u652f\u6301\u975e\u5bf9\u79f0 3D \u5e76\u884c\u7ed3\u6784\u548c\u7ec6\u7c92\u5ea6\u4efb\u52a1\u5206\u914d\uff0c\u5e76\u8bbe\u8ba1\u57fa\u4e8e\u672c\u5730\u4f18\u5148\u7684\u5f39\u6027\u6062\u590d\u673a\u5236\u3002", "result": "\u5728\u4e09\u79cd\u5927\u89c4\u6a21\u6a21\u578b\u4e0e\u591a\u7c7b GPU \u7ec4\u5408\u6d4b\u8bd5\u4e2d\uff0c\u76f8\u6bd4 Megatron-LM \u548c Whale \u6700\u9ad8\u63d0\u5347 1.79 \u500d\u8bad\u7ec3\u541e\u5410\u91cf\uff0c\u6062\u590d\u901f\u5ea6\u63d0\u5347 4.38 \u500d\u3002", "conclusion": "AutoHet \u80fd\u9ad8\u6548\u9002\u914d\u5f02\u6784\u786c\u4ef6\u73af\u5883\uff0c\u5728\u8ba1\u7b97\u6548\u7387\u3001\u5185\u5b58\u5229\u7528\u4e0e\u5f39\u6027\u6062\u590d\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u7cfb\u7edf\uff0c\u63a8\u52a8\u5927\u89c4\u6a21\u6a21\u578b\u8bad\u7ec3\u5b9e\u7528\u6027\u3002"}}
{"id": "2512.20967", "categories": ["cs.DC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.20967", "abs": "https://arxiv.org/abs/2512.20967", "authors": ["Linggao Kong", "Yuedong Xu", "Lei Jiao", "Chuan Xu"], "title": "Deadline-Aware Online Scheduling for LLM Fine-Tuning with Spot Market Predictions", "comment": null, "summary": "As foundation models grow in size, fine-tuning them becomes increasingly expensive. While GPU spot instances offer a low-cost alternative to on-demand resources, their volatile prices and availability make deadline-aware scheduling particularly challenging. We tackle this difficulty by using a mix of spot and on-demand instances. Distinctively, we show the predictability of prices and availability in a spot instance market, the power of prediction in enabling cost-efficient scheduling and its sensitivity to estimation errors. An integer programming problem is formulated to capture the use of mixed instances under both the price and availability dynamics. We propose an online allocation algorithm with prediction based on the committed horizon control approach that leverages a \\emph{commitment level} to enforce the partial sequence of decisions. When this prediction becomes inaccurate, we further present a complementary online algorithm without predictions. An online policy selection algorithm is developed that learns the best policy from a pool constructed by varying the parameters of both algorithms. We prove that the prediction-based algorithm achieves tighter performance bounds as prediction error decreases, while the policy selection algorithm possesses a regret bound of $\\mathcal{O}(\\sqrt{T})$. Experimental results demonstrate that our online framework can adaptively select the best policy under varying spot market dynamics and prediction quality, consistently outperforming baselines and improving utility by up to 54.8\\%.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u7ed3\u5408\u9884\u6d4b\u7684\u5728\u7ebf\u8c03\u5ea6\u7b97\u6cd5\uff0c\u7528\u4e8e\u5728\u6df7\u5408\u4f7f\u7528spot\u548con-demand GPU\u5b9e\u4f8b\u65f6\u4f18\u5316\u5927\u6a21\u578b\u5fae\u8c03\u7684\u6210\u672c\u4e0e\u622a\u6b62\u65f6\u95f4\u7ea6\u675f\u3002", "motivation": "\u5927\u6a21\u578b\u5fae\u8c03\u6210\u672c\u9ad8\u6602\uff0cspot\u5b9e\u4f8b\u867d\u4fbf\u5b9c\u4f46\u6ce2\u52a8\u5927\uff0c\u9700\u517c\u987e\u6210\u672c\u4e0e\u622a\u6b62\u65f6\u95f4\u8fdb\u884c\u667a\u80fd\u8c03\u5ea6\u3002", "method": "\u6784\u5efa\u6574\u6570\u89c4\u5212\u6a21\u578b\uff0c\u8bbe\u8ba1\u57fa\u4e8e\u627f\u8bfa\u6c34\u5e73\u7684\u9884\u6d4b\u578b\u5728\u7ebf\u7b97\u6cd5\uff0c\u5e76\u8f85\u4ee5\u65e0\u9884\u6d4b\u5907\u7528\u7b97\u6cd5\uff0c\u901a\u8fc7\u5728\u7ebf\u7b56\u7565\u9009\u62e9\u52a8\u6001\u9002\u914d\u5e02\u573a\u53d8\u5316\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\u8be5\u6846\u67b6\u80fd\u81ea\u9002\u5e94\u9009\u62e9\u6700\u4f18\u7b56\u7565\uff0c\u5728\u4e0d\u540c\u5e02\u573a\u52a8\u6001\u4e0b\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\uff0c\u6548\u7528\u6700\u9ad8\u63d0\u534754.8%\u3002", "conclusion": "\u7ed3\u5408\u9884\u6d4b\u4e0e\u5728\u7ebf\u5b66\u4e60\u7684\u6df7\u5408\u5b9e\u4f8b\u8c03\u5ea6\u6846\u67b6\u53ef\u663e\u8457\u964d\u4f4e\u5927\u6a21\u578b\u8bad\u7ec3\u6210\u672c\uff0c\u540c\u65f6\u4fdd\u969c\u65f6\u6548\u6027\u3002"}}
{"id": "2512.20968", "categories": ["cs.DC", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.20968", "abs": "https://arxiv.org/abs/2512.20968", "authors": ["Sirui Chen", "Jingji Chen", "Siqi Zhu", "Ziheng Jiang", "Yanghua Peng", "Xuehai Qian"], "title": "Mesh-Attention: A New Communication-Efficient Distributed Attention with Improved Data Locality", "comment": null, "summary": "Distributed attention is a fundamental problem for scaling context window for Large Language Models (LLMs). The state-of-the-art method, Ring-Attention, suffers from scalability limitations due to its excessive communication traffic. This paper proposes a new distributed attention algorithm, Mesh-Attention, by rethinking the design space of distributed attention with a new matrix-based model. Our method assigns a two-dimensional tile -- rather than one-dimensional row or column -- of computation blocks to each GPU to achieve higher efficiency through lower communication-computation (CommCom) ratio. The general approach covers Ring-Attention as a special case, and allows the tuning of CommCom ratio with different tile shapes. Importantly, we propose a greedy algorithm that can efficiently search the scheduling space within the tile with restrictions that ensure efficient communication among GPUs. The theoretical analysis shows that Mesh-Attention leads to a much lower communication complexity and exhibits good scalability comparing to other current algorithms.\n  Our extensive experiment results show that Mesh-Attention can achieve up to 3.4x speedup (2.9x on average) and reduce the communication volume by up to 85.4% (79.0% on average) on 256 GPUs. Our scalability results further demonstrate that Mesh-Attention sustains superior performance as the system scales, substantially reducing overhead in large-scale deployments. The results convincingly confirm the advantage of Mesh-Attention.", "AI": {"tldr": "\u63d0\u51faMesh-Attention\u7b97\u6cd5\uff0c\u901a\u8fc7\u4e8c\u7ef4\u5206\u5757\u964d\u4f4e\u901a\u4fe1\u5f00\u9500\uff0c\u5728256 GPU\u4e0a\u5e73\u5747\u63d0\u901f2.9\u500d\u3001\u901a\u4fe1\u91cf\u51cf\u5c1179%\u3002", "motivation": "\u89e3\u51b3Ring-Attention\u56e0\u901a\u4fe1\u6d41\u91cf\u8fc7\u5927\u5bfc\u81f4\u7684\u53ef\u6269\u5c55\u6027\u74f6\u9888\uff0c\u63d0\u5347\u5927\u8bed\u8a00\u6a21\u578b\u5206\u5e03\u5f0f\u6ce8\u610f\u529b\u6548\u7387\u3002", "method": "\u57fa\u4e8e\u77e9\u9635\u6a21\u578b\u8bbe\u8ba1\u4e8c\u7ef4\u8ba1\u7b97\u5206\u5757\u5206\u914d\u7b56\u7565\uff0c\u7ed3\u5408\u8d2a\u5fc3\u8c03\u5ea6\u7b97\u6cd5\u4f18\u5316GPU\u95f4\u901a\u4fe1\uff0c\u652f\u6301\u7075\u6d3b\u8c03\u8282\u901a\u4fe1\u8ba1\u7b97\u6bd4\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\u5e73\u5747\u63d0\u901f2.9\u500d\u3001\u901a\u4fe1\u91cf\u51cf\u5c1179%\uff0c\u5728\u5927\u89c4\u6a21\u90e8\u7f72\u4e2d\u4fdd\u6301\u4f18\u5f02\u6269\u5c55\u6027\u3002", "conclusion": "Mesh-Attention\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u7406\u8bba\u4e0e\u5b9e\u8bc1\u5747\u9a8c\u8bc1\u5176\u4f4e\u901a\u4fe1\u590d\u6742\u5ea6\u4e0e\u9ad8\u53ef\u6269\u5c55\u6027\u4f18\u52bf\u3002"}}
{"id": "2512.21009", "categories": ["cs.DC", "cs.DS"], "pdf": "https://arxiv.org/pdf/2512.21009", "abs": "https://arxiv.org/abs/2512.21009", "authors": ["S. M. Shovan", "Arindam Khanda", "Sanjukta Bhowmick", "Sajal K. Das"], "title": "ESCHER: Efficient and Scalable Hypergraph Evolution Representation with Application to Triad Counting", "comment": null, "summary": "Higher-order interactions beyond pairwise relationships in large complex networks are often modeled as hypergraphs. Analyzing hypergraph properties such as triad counts is essential, as hypergraphs can reveal intricate group interaction patterns that conventional graphs fail to capture. In real-world scenarios, these networks are often large and dynamic, introducing significant computational challenges. Due to the absence of specialized software packages and data structures, the analysis of large dynamic hypergraphs remains largely unexplored. Motivated by this gap, we propose ESCHER, a GPU-centric parallel data structure for Efficient and Scalable Hypergraph Evolution Representation, designed to manage large scale hypergraph dynamics efficiently. We also design a hypergraph triad-count update framework that minimizes redundant computation while fully leveraging the capabilities of ESCHER for dynamic operations. We validate the efficacy of our approach across multiple categories of hypergraph triad counting, including hyperedge-based, incident-vertex-based, and temporal triads. Empirical results on both large real-world and synthetic datasets demonstrate that our proposed method outperforms existing state-of-the-art methods, achieving speedups of up to 104.5x, 473.7x, and 112.5x for hyperedge-based, incident-vertex-based, and temporal triad types, respectively.", "AI": {"tldr": "ESCHER\u662f\u4e00\u79cd\u9762\u5411GPU\u7684\u9ad8\u6548\u53ef\u6269\u5c55\u52a8\u6001\u8d85\u56fe\u8868\u793a\u7ed3\u6784\uff0c\u7ed3\u5408\u4f18\u5316\u7684\u4e09\u5143\u7ec4\u8ba1\u6570\u6846\u67b6\uff0c\u5728\u591a\u79cd\u4e09\u5143\u7ec4\u7c7b\u578b\u4e0a\u5b9e\u73b0\u663e\u8457\u52a0\u901f\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u7f3a\u4e4f\u4e13\u7528\u6570\u636e\u7ed3\u6784\u4e0e\u8f6f\u4ef6\u652f\u6301\uff0c\u96be\u4ee5\u9ad8\u6548\u5904\u7406\u5927\u89c4\u6a21\u52a8\u6001\u8d85\u56fe\u5206\u6790\u3002", "method": "\u63d0\u51faESCHER\u5e76\u884c\u6570\u636e\u7ed3\u6784\u53ca\u914d\u5957\u4e09\u5143\u7ec4\u66f4\u65b0\u6846\u67b6\uff0c\u51cf\u5c11\u5197\u4f59\u8ba1\u7b97\u5e76\u5145\u5206\u5229\u7528GPU\u80fd\u529b\u3002", "result": "\u5728\u771f\u5b9e\u4e0e\u5408\u6210\u6570\u636e\u96c6\u4e0a\uff0c\u76f8\u8f83\u73b0\u6709\u6700\u4f18\u65b9\u6cd5\u5206\u522b\u5b9e\u73b0\u6700\u9ad8104.5x\u3001473.7x\u548c112.5x\u52a0\u901f\u3002", "conclusion": "ESCHER\u6709\u6548\u89e3\u51b3\u4e86\u5927\u89c4\u6a21\u52a8\u6001\u8d85\u56fe\u9ad8\u6548\u5206\u6790\u96be\u9898\uff0c\u4e3a\u9ad8\u9636\u4ea4\u4e92\u7814\u7a76\u63d0\u4f9b\u5b9e\u7528\u5de5\u5177\u3002"}}
{"id": "2512.20902", "categories": ["cs.NI", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.20902", "abs": "https://arxiv.org/abs/2512.20902", "authors": ["Siqi Mu", "Shuo Wen", "Yang Lu", "Ruihong Jiang", "Bo Ai"], "title": "Embodied AI-Enhanced IoMT Edge Computing: UAV Trajectory Optimization and Task Offloading with Mobility Prediction", "comment": null, "summary": "Due to their inherent flexibility and autonomous operation, unmanned aerial vehicles (UAVs) have been widely used in Internet of Medical Things (IoMT) to provide real-time biomedical edge computing service for wireless body area network (WBAN) users. In this paper, considering the time-varying task criticality characteristics of diverse WBAN users and the dual mobility between WBAN users and UAV, we investigate the dynamic task offloading and UAV flight trajectory optimization problem to minimize the weighted average task completion time of all the WBAN users, under the constraint of UAV energy consumption. To tackle the problem, an embodied AI-enhanced IoMT edge computing framework is established. Specifically, we propose a novel hierarchical multi-scale Transformer-based user trajectory prediction model based on the users' historical trajectory traces captured by the embodied AI agent (i.e., UAV). Afterwards, a prediction-enhanced deep reinforcement learning (DRL) algorithm that integrates predicted users' mobility information is designed for intelligently optimizing UAV flight trajectory and task offloading decisions. Real-word movement traces and simulation results demonstrate the superiority of the proposed methods in comparison with the existing benchmarks.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8eAI\u589e\u5f3a\u7684\u533b\u7597\u7269\u8054\u7f51\u8fb9\u7f18\u8ba1\u7b97\u6846\u67b6\uff0c\u7ed3\u5408\u591a\u5c3a\u5ea6Transformer\u8f68\u8ff9\u9884\u6d4b\u4e0e\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\uff0c\u4f18\u5316\u65e0\u4eba\u673a\u98de\u884c\u8def\u5f84\u4e0e\u4efb\u52a1\u5378\u8f7d\uff0c\u4ee5\u964d\u4f4e\u52a0\u6743\u5e73\u5747\u4efb\u52a1\u5b8c\u6210\u65f6\u95f4\u3002", "motivation": "\u9488\u5bf9\u533b\u7597\u7269\u8054\u7f51\u4e2d\u65e0\u7ebf\u4f53\u57df\u7f51\u7528\u6237\u4efb\u52a1\u5173\u952e\u6027\u65f6\u53d8\u53ca\u7528\u6237\u4e0e\u65e0\u4eba\u673a\u53cc\u91cd\u79fb\u52a8\u6027\u95ee\u9898\uff0c\u9700\u4f18\u5316\u4efb\u52a1\u5378\u8f7d\u4e0e\u65e0\u4eba\u673a\u8f68\u8ff9\u4ee5\u63d0\u5347\u670d\u52a1\u6548\u7387\u3002", "method": "\u6784\u5efa\u57fa\u4e8e\u5386\u53f2\u8f68\u8ff9\u7684\u591a\u5c3a\u5ea6Transformer\u7528\u6237\u8f68\u8ff9\u9884\u6d4b\u6a21\u578b\uff0c\u5e76\u8bbe\u8ba1\u878d\u5408\u9884\u6d4b\u4fe1\u606f\u7684\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\uff0c\u8054\u5408\u4f18\u5316\u65e0\u4eba\u673a\u98de\u884c\u8f68\u8ff9\u4e0e\u4efb\u52a1\u5378\u8f7d\u51b3\u7b56\u3002", "result": "\u771f\u5b9e\u8f68\u8ff9\u4e0e\u4eff\u771f\u7ed3\u679c\u8868\u660e\uff0c\u6240\u63d0\u65b9\u6cd5\u5728\u964d\u4f4e\u4efb\u52a1\u5b8c\u6210\u65f6\u95f4\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u57fa\u51c6\u65b9\u6848\u3002", "conclusion": "\u8be5AI\u589e\u5f3a\u6846\u67b6\u80fd\u6709\u6548\u5e94\u5bf9\u52a8\u6001\u73af\u5883\u4e0b\u7684\u4efb\u52a1\u8c03\u5ea6\u4e0e\u8d44\u6e90\u5206\u914d\u6311\u6218\uff0c\u663e\u8457\u63d0\u5347\u65e0\u4eba\u673a\u8f85\u52a9\u533b\u7597\u8fb9\u7f18\u8ba1\u7b97\u7684\u670d\u52a1\u6027\u80fd\u3002"}}
{"id": "2512.20946", "categories": ["cs.NI"], "pdf": "https://arxiv.org/pdf/2512.20946", "abs": "https://arxiv.org/abs/2512.20946", "authors": ["Guanqiao Qu", "Tao Li", "Qian Chen", "Xianhao Chen", "Sheng Zhou"], "title": "SLIDE: Simultaneous Model Downloading and Inference at the Wireless Network Edge", "comment": "15 pages, 10 figures", "summary": "To support on-device inference, the next-generation mobile networks are expected to support real-time model downloading services to mobile users. However, powerful AI models typically have large model sizes, resulting in excessive end-to-end (E2E) downloading-and-inference (DAI) latency. To address this issue, we propose a simultaneous model downloading and inference (SLIDE) framework, which allows users to perform inference with downloaded layers while simultaneously receiving the remaining layers of the model. To this end, we formulate a task throughput maximization problem by jointly optimizing model provisioning, spectrum bandwidth allocation, and computing resource allocation for multi-user downlink systems. Unlike traditional DAI frameworks, SLIDE introduces recursive dependencies across layers, where inference latency depends recursively on the downloading bandwidth and computing resource allocation for each of the preceding layers. To solve this challenging problem, we design an efficient algorithm that acquires the optimal solution with polynomial-time complexity. Simulation results demonstrate that the proposed SLIDE framework significantly improves task throughput under latency and communication resource constraints compared with the conventional model downloading schemes.", "AI": {"tldr": "\u63d0\u51faSLIDE\u6846\u67b6\uff0c\u901a\u8fc7\u8fb9\u4e0b\u8f7d\u8fb9\u63a8\u7406\u4f18\u5316\u6a21\u578b\u90e8\u7f72\uff0c\u663e\u8457\u63d0\u5347\u4efb\u52a1\u541e\u5410\u91cf\u3002", "motivation": "\u89e3\u51b3\u5927\u6a21\u578b\u4e0b\u8f7d\u4e0e\u63a8\u7406\u5ef6\u8fdf\u8fc7\u9ad8\u95ee\u9898\uff0c\u6ee1\u8db3\u79fb\u52a8\u7aef\u5b9e\u65f6AI\u670d\u52a1\u9700\u6c42\u3002", "method": "\u8054\u5408\u4f18\u5316\u6a21\u578b\u5206\u53d1\u3001\u5e26\u5bbd\u4e0e\u8ba1\u7b97\u8d44\u6e90\u5206\u914d\uff0c\u8bbe\u8ba1\u591a\u9879\u5f0f\u65f6\u95f4\u6700\u4f18\u7b97\u6cd5\u5904\u7406\u5c42\u95f4\u9012\u5f52\u4f9d\u8d56\u3002", "result": "\u4eff\u771f\u8868\u660eSLIDE\u5728\u5ef6\u8fdf\u548c\u901a\u4fe1\u7ea6\u675f\u4e0b\u663e\u8457\u4f18\u4e8e\u4f20\u7edf\u65b9\u6848\u3002", "conclusion": "SLIDE\u6846\u67b6\u6709\u6548\u964d\u4f4e\u7aef\u5230\u7aef\u5ef6\u8fdf\uff0c\u63d0\u5347\u591a\u7528\u6237\u7cfb\u7edf\u4efb\u52a1\u541e\u5410\u6027\u80fd\u3002"}}
{"id": "2512.21116", "categories": ["cs.NI"], "pdf": "https://arxiv.org/pdf/2512.21116", "abs": "https://arxiv.org/abs/2512.21116", "authors": ["Minyuan Xiao", "Yunchun Li", "Yuchen Zhao", "Tong Guan", "Mingyuan Xia", "Wei Li"], "title": "Synecdoche: Efficient and Accurate In-Network Traffic Classification via Direct Packet Sequential Pattern Matching", "comment": "Accepted by IEEE INFOCOM 2026", "summary": "Traffic classification on programmable data plane holds great promise for line-rate processing, with methods evolving from per-packet to flow-level analysis for higher accuracy. However, a trade-off between accuracy and efficiency persists. Statistical feature-based methods align with hardware constraints but often exhibit limited accuracy, while online deep learning methods using packet sequential features achieve superior accuracy but require substantial computational resources. This paper presents Synecdoche, the first traffic classification framework that successfully deploys packet sequential features on a programmable data plane via pattern matching, achieving both high accuracy and efficiency. Our key insight is that discriminative information concentrates in short sub-sequences--termed Key Segments--that serve as compact traffic features for efficient data plane matching. Synecdoche employs an \"offline discovery, online matching\" paradigm: deep learning models automatically discover Key Segment patterns offline, which are then compiled into optimized table entries for direct data plane matching. Extensive experiments demonstrate Synecdoche's superior accuracy, improving F1-scores by up to 26.4% against statistical methods and 18.3% against online deep learning methods, while reducing latency by 13.0% and achieving 79.2% reduction in SRAM usage. The source code of Synecdoche is publicly available to facilitate reproducibility and further research.", "AI": {"tldr": "Synecdoche \u662f\u9996\u4e2a\u5728\u53ef\u7f16\u7a0b\u6570\u636e\u5e73\u9762\u901a\u8fc7\u6a21\u5f0f\u5339\u914d\u90e8\u7f72\u5305\u5e8f\u5217\u7279\u5f81\u7684\u6d41\u91cf\u5206\u7c7b\u6846\u67b6\uff0c\u517c\u987e\u9ad8\u51c6\u786e\u7387\u4e0e\u9ad8\u6548\u7387\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u51c6\u786e\u7387\u4e0e\u6548\u7387\u95f4\u96be\u4ee5\u517c\u987e\uff0c\u7edf\u8ba1\u65b9\u6cd5\u53d7\u9650\u4e8e\u7cbe\u5ea6\uff0c\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u8ba1\u7b97\u5f00\u9500\u5927\u3002", "method": "\u91c7\u7528\u201c\u79bb\u7ebf\u53d1\u73b0\u3001\u5728\u7ebf\u5339\u914d\u201d\u8303\u5f0f\uff0c\u5229\u7528\u6df1\u5ea6\u5b66\u4e60\u79bb\u7ebf\u63d0\u53d6\u5173\u952e\u7247\u6bb5\uff08Key Segments\uff09\uff0c\u7f16\u8bd1\u4e3a\u8868\u9879\u4f9b\u6570\u636e\u5e73\u9762\u9ad8\u6548\u5339\u914d\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cF1\u5206\u6570\u6700\u9ad8\u63d0\u534726.4%\uff0c\u5ef6\u8fdf\u964d\u4f4e13.0%\uff0cSRAM\u4f7f\u7528\u51cf\u5c1179.2%\u3002", "conclusion": "Synecdoche \u5728\u4fdd\u8bc1\u9ad8\u7cbe\u5ea6\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u6548\u7387\uff0c\u662f\u6d41\u91cf\u5206\u7c7b\u9886\u57df\u7684\u91cd\u8981\u7a81\u7834\u3002"}}
{"id": "2512.21144", "categories": ["cs.NI"], "pdf": "https://arxiv.org/pdf/2512.21144", "abs": "https://arxiv.org/abs/2512.21144", "authors": ["Hongjuan Li", "Hui Kang", "Chenbang Liu", "Ruolin Wang", "Jiahui Li", "Geng Sun", "Jiacheng Wang", "Shuang Liang", "Shiwen Mao"], "title": "Encrypted Traffic Detection in Resource Constrained IoT Networks: A Diffusion Model and LLM Integrated Framework", "comment": "This paper is accepted by IEEE Transactions on Network Science and Engineering", "summary": "The proliferation of Internet-of-things (IoT) infrastructures and the widespread adoption of traffic encryption present significant challenges, particularly in environments characterized by dynamic traffic patterns, constrained computational capabilities, and strict latency constraints. In this paper, we propose DMLITE, a diffusion model and large language model (LLM) integrated traffic embedding framework for network traffic detection within resource-limited IoT environments. The DMLITE overcomes these challenges through a tri-phase architecture including traffic visual preprocessing, diffusion-based multi-level feature extraction, and LLM-guided feature optimization. Specifically, the framework utilizes self-supervised diffusion models to capture both fine-grained and abstract patterns in encrypted traffic through multi-level feature fusion and contrastive learning with representative sample selection, thus enabling rapid adaptation to new traffic patterns with minimal labeled data. Furthermore, DMLITE incorporates LLMs to dynamically adjust particle swarm optimization parameters for intelligent feature selection by implementing a dual objective function that minimizes both classification error and variance across data distributions. Comprehensive experimental validation on benchmark datasets confirms the effectiveness of DMLITE, achieving classification accuracies of 98.87\\%, 92.61\\%, and 99.83\\% on USTC-TFC, ISCX-VPN, and Edge-IIoTset datasets, respectively. This improves classification accuracy by an average of 3.7\\% and reduces training time by an average of 41.9\\% compared to the representative deep learning model.", "AI": {"tldr": "DMLITE\u662f\u4e00\u4e2a\u7ed3\u5408\u6269\u6563\u6a21\u578b\u4e0e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u6d41\u91cf\u5d4c\u5165\u6846\u67b6\uff0c\u7528\u4e8e\u8d44\u6e90\u53d7\u9650\u7684\u7269\u8054\u7f51\u73af\u5883\u4e2d\u7684\u52a0\u5bc6\u6d41\u91cf\u68c0\u6d4b\uff0c\u663e\u8457\u63d0\u5347\u51c6\u786e\u7387\u5e76\u964d\u4f4e\u8bad\u7ec3\u65f6\u95f4\u3002", "motivation": "\u5e94\u5bf9\u7269\u8054\u7f51\u73af\u5883\u4e2d\u52a8\u6001\u6d41\u91cf\u3001\u8ba1\u7b97\u8d44\u6e90\u53d7\u9650\u548c\u4f4e\u5ef6\u8fdf\u9700\u6c42\u5e26\u6765\u7684\u52a0\u5bc6\u6d41\u91cf\u68c0\u6d4b\u6311\u6218\u3002", "method": "\u91c7\u7528\u4e09\u9636\u6bb5\u67b6\u6784\uff1a\u6d41\u91cf\u53ef\u89c6\u5316\u9884\u5904\u7406\u3001\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u591a\u5c42\u6b21\u7279\u5f81\u63d0\u53d6\u3001\u5927\u8bed\u8a00\u6a21\u578b\u5f15\u5bfc\u7684\u7279\u5f81\u4f18\u5316\uff0c\u5e76\u7ed3\u5408\u5bf9\u6bd4\u5b66\u4e60\u4e0e\u7c92\u5b50\u7fa4\u53c2\u6570\u81ea\u9002\u5e94\u8c03\u6574\u3002", "result": "\u5728USTC-TFC\u3001ISCX-VPN\u548cEdge-IIoTset\u6570\u636e\u96c6\u4e0a\u5206\u522b\u8fbe\u523098.87%\u300192.61%\u548c99.83%\u7684\u5206\u7c7b\u51c6\u786e\u7387\uff0c\u5e73\u5747\u63d0\u53473.7%\uff0c\u8bad\u7ec3\u65f6\u95f4\u51cf\u5c1141.9%\u3002", "conclusion": "DMLITE\u6709\u6548\u89e3\u51b3\u4e86\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4e0b\u52a0\u5bc6\u6d41\u91cf\u68c0\u6d4b\u7684\u96be\u9898\uff0c\u517c\u5177\u9ad8\u7cbe\u5ea6\u4e0e\u9ad8\u6548\u6027\uff0c\u5177\u5907\u5b9e\u9645\u90e8\u7f72\u6f5c\u529b\u3002"}}
