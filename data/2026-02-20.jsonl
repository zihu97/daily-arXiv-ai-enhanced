{"id": "2602.16952", "categories": ["cs.NI"], "pdf": "https://arxiv.org/pdf/2602.16952", "abs": "https://arxiv.org/abs/2602.16952", "authors": ["Mohammad Zangooei", "Bo Sun", "Noura Limam", "Raouf Boutaba"], "title": "HyRA: A Hybrid Resource Allocation Framework for RAN Slicing", "comment": "To appear in IEEE/IFIP NOMS 2026", "summary": "The advent of 5G and the emergence of 6G networks demand unprecedented flexibility and efficiency in Radio Access Network (RAN) resource management to satisfy diverse service-level agreements (SLAs). Existing RAN slicing frameworks predominantly rely on per-slice resource reservation, which ensures performance isolation but leads to inefficient utilization, particularly under bursty traffic. We introduce HyRA, a hybrid resource allocation framework for RAN slicing that combines dedicated per-slice allocations with shared resource pooling across slices. HyRA preserves performance isolation while improving resource efficiency by leveraging multiplexing gains in bursty traffic conditions. We formulate this design as a bi-level stochastic optimization problem, where the outer loop determines the dedicated and shared resource budgets and the inner loop performs per-UE scheduling under a novel water-filling approach. By using the sample-average approximation, the Karush-Kuhn-Tucker (KKT) conditions of the inner loop, and Big-M encoding, we transform the problem into a tractable mixed-integer program that standard optimization solvers can solve. Extensive simulations under diverse demand patterns, SLA configurations, and traffic burstiness show that HyRA achieves up to 50-75% spectrum savings compared to dedicated-only and shared-only baselines. These results highlight HyRA as a viable approach for resource-efficient, SLA-compliant RAN slicing in future mobile networks."}
{"id": "2602.16969", "categories": ["cs.NI"], "pdf": "https://arxiv.org/pdf/2602.16969", "abs": "https://arxiv.org/abs/2602.16969", "authors": ["Laasya Koduru", "Sylee Beltiukov", "Alexander Nguyen", "Eugene Vuong", "Jaber Daneshamooz", "Tejas Narechania", "Elizabeth Belding", "Arpit Gupta"], "title": "Robust and Extensible Measurement of Broadband Plans with BQT+", "comment": null, "summary": "Independent, street address-level broadband data is essential for evaluating Internet infrastructure investments, such as the $42B Broadband Equity, Access, and Deployment (BEAD) program. Evaluating these investments requires longitudinal visibility into broadband availability, quality, and affordability, including data on pre-disbursement baselines and changes in providers' advertised plans. While such data can be obtained through Internet Service Provider (ISP) web interfaces, these workloads impose three fundamental system requirements: robustness to frequent interface evolution, extensibility across hundreds of providers, and low technical overhead for non-expert users. Existing systems fail to meet these three essential requirements.\n  We present BQT+, a broadband plan measurement framework that replaces monolithic workflows with declarative state/action specifications. BQT+ models querying intent as an interaction state space, formalized as an abstract nondeterministic finite automaton (NFA), and selects execution paths at runtime to accommodate alternative interaction flows and localized interface changes. We show that BQT+ sustains longitudinal monitoring of 64 ISPs, supporting querying for over 100 ISPs. We apply it to two policy studies: constructing a BEAD pre-disbursement baseline and benchmarking broadband affordability across over 124,000 addresses in four states."}
{"id": "2602.17198", "categories": ["cs.NI"], "pdf": "https://arxiv.org/pdf/2602.17198", "abs": "https://arxiv.org/abs/2602.17198", "authors": ["Oscar Adamuz-Hinojosa", "Lanfranco Zanzi", "Vincenzo Sciancalepore", "Marco Di Renzo", "Xavier Costa-Pérez"], "title": "RIS Control through the Lens of Stochastic Network Calculus: An O-RAN Framework for Delay-Sensitive 6G Applications", "comment": null, "summary": "Reconfigurable Intelligent Surfaces (RIS) enable dynamic electromagnetic control for 6G networks, but existing control schemes lack responsiveness to fast-varying network conditions, limiting their applicability for ultra-reliable low latency communications. This work addresses uplink delay minimization in multi-RIS scenarios with heterogeneous per-user latency and reliability demands. We propose Delay-Aware RIS Orchestrator (DARIO), an O-RAN-compliant framework that dynamically assigns RIS devices to users within short time windows, adapting to traffic fluctuations to meet per-user delay and reliability targets. DARIO relies on a novel Stochastic Network Calculus (SNC) model to analytically estimate the delay bound for each possible user-RIS assignment under specific traffic and service dynamics. These estimations are used by DARIO to formulate a Nonlinear Integer Program (NIP), for which an online heuristic provides near-optimal performance with low computational overhead. Extensive evaluations with simulations and real traffic traces show consistent delay reductions up to 95.7% under high load or RIS availability."}
{"id": "2602.17209", "categories": ["cs.NI"], "pdf": "https://arxiv.org/pdf/2602.17209", "abs": "https://arxiv.org/abs/2602.17209", "authors": ["Alejandro Flores", "Danial Shafaie", "Konstantinos Ntontin", "Elli Kartsakli", "Symeon Chatzinotas"], "title": "Hierarchical Edge-Cloud Task Offloading in NTN for Remote Healthcare", "comment": null, "summary": "In this work, we study a hierarchical non-terrestrial network as an edge-cloud platform for remote computing of tasks generated by remote ad-hoc healthcare facility deployments, or internet of medical things (IoMT) devices. We consider a high altitude platform station (HAPS) to provide local multiaccess edge server (MEC) services to a set of remote ground medical devices, and a low-earth orbit (LEO) satellite, serving as a bridge to a remote cloud computing server through a ground gateway (GW), providing a large amount of computing resources to the HAPS. In this hierarchical system, the HAPS and the cloud server charges the ground users and the HAPS for the use of the spectrum and the computing of their tasks respectively. Each tier seeks to maximize their own utility in a selfish manner. To encourage the prompt computation of the tasks, a local delay cost is assumed. We formulate the optimal per-task cost at each tier that influences the corresponding offloading policies, and find the corresponding optimal bandwidth allocation."}
{"id": "2602.16858", "categories": ["cs.PF", "cs.AR"], "pdf": "https://arxiv.org/pdf/2602.16858", "abs": "https://arxiv.org/abs/2602.16858", "authors": ["Kathiravan Palaniappan"], "title": "GDEV-AI: A Generalized Evaluation of Deep Learning Inference Scaling and Architectural Saturation", "comment": "14 pages, 18 figures, 20 references", "summary": "The deployment of deep learning inference in production environments continues to grow, where throughput, latency, and hardware efficiency are critical. Although specialized accelerators are increasingly adopted, many inference workloads still run on CPU-only systems, particularly in legacy data centers and cost-sensitive environments. This study investigates the scalability limits of CPU-based inference for convolutional neural networks by benchmarking ResNet models across varying batch sizes on two hardware tiers: a legacy Intel Xeon E5-2403 v2 processor and a modern Intel Xeon 6 \"Granite Rapids\" platform.\n  Results show that legacy CPUs quickly reach throughput saturation, with limited scaling beyond small batch sizes due to instruction-level and memory constraints. In contrast, the Granite Rapids system leverages Intel Advanced Matrix Extensions (AMX) to achieve substantially higher throughput. However, oversubscription beyond physical core limits introduces execution contention and tail-latency amplification, revealing a performance degradation regime in modern architectures.\n  We introduce GDEV-AI, a reproducible benchmarking framework for analyzing scalability behavior and architectural saturation in CPU-based inference. By establishing a vendor-neutral baseline, this work provides empirical insight into performance bottlenecks and informs capacity planning in heterogeneous data center environments."}
{"id": "2602.16748", "categories": ["cs.SE", "cs.ET"], "pdf": "https://arxiv.org/pdf/2602.16748", "abs": "https://arxiv.org/abs/2602.16748", "authors": ["Md Asiful Islam", "Shanto Jouerder", "Md Sabit As Sami", "Afia Jahin Prema"], "title": "A Construction-Phase Digital Twin Framework for Quality Assurance and Decision Support in Civil Infrastructure Projects", "comment": null, "summary": "Quality assurance (QA) during construction often relies on inspection records and laboratory test results that become available days or weeks after work is completed. On large highway and bridge projects, this delay limits early intervention and increases the risk of rework, schedule impacts, and fragmented documentation. This study presents a construction-phase digital twin framework designed to support element-level QA and readiness-based decision making during active construction. The framework links inspection records, material production and placement data, early-age sensing, and predictive strength models to individual construction elements. By integrating these data streams, the system represents the evolving quality state of each element and supports structured release or hold decisions before standard-age test results are available. The approach does not replace established inspection and testing procedures. Instead, it supplements existing workflows by improving traceability and enabling earlier, data-informed quality assessments. Practical considerations related to data integration, contractual constraints, and implementation challenges are also discussed. The proposed framework provides a structured pathway for transitioning construction QA from delayed, document-driven review toward proactive, element-level decision support during construction."}
{"id": "2602.17114", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2602.17114", "abs": "https://arxiv.org/abs/2602.17114", "authors": ["Seemron Neupane", "Aashish Ghimire"], "title": "Low-Cost IoT-Enabled Tele-ECG Monitoring for Resource-Constrained Settings: System Design and Prototype", "comment": null, "summary": "With the availability of automation machinery and its superiority, are being slothful and inviting many diseases to invade them. The world still has so many places where people lack basic health facilities. Due to early detection and intervention, CDV can be cured to an extreme extent. It heavily reduces travel and associated costs. A remote ECG monitoring system enables community health workers to support and empower patients through telemedicine. However, there remains some financial and logistical burden. Heart disease cannot be taken lightly. These patients require regular health check-ups and the attention of health personnel in a short period if their health deteriorates suddenly and rapidly. Chronic diseases are extremely variable in their symptoms and evolution of treatment. Some, if not treated early, will end the patient's life. The trend of the INTERNET OF THINGS, IoT, is spreading massively. This paper focuses on the three main: the operator, the doctor, and the server over which the data is being sent."}
{"id": "2602.16903", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2602.16903", "abs": "https://arxiv.org/abs/2602.16903", "authors": ["Armando Castañeda", "Braulio Ramses Hernández Martínez"], "title": "Read-Modify-Writable Snapshots from Read/Write operations", "comment": null, "summary": "In the context of asynchronous concurrent shared-memory systems, a snapshot algorithm allows failure-prone processes to concurrently and atomically write on the entries of a shared array MEM , and also atomically read the whole array. Recently, Read-Modify-Writable (RMWable) snapshot was proposed, a variant of snapshot that allows processes to perform operations more complex than just read and write, specifically, each entry MEM[k] is an arbitrary readable object. The known RMWable snapshot algorithms heavily rely on powerful low-level operations such as compare&swap or load-link/store-conditional to correctly produce snapshots of MEM. Following the large body of research devoted to understand the limits of what can be solved using the simple read/write low-level operations, which are known to be strictly weaker than compare&swap and load-link/store-conditional, we explore if RMWable snapshots are possible using only read/write operations. We present two read/write RMWable snapshot algorithms, the first one in the standard concurrent shared-memory model where the number of processes n is finite and known in advance, and the second one in a variant of the standard model with unbounded concurrency, where there are infinitely many processes, but at any moment only finitely many processes participate in an execution."}
{"id": "2602.16726", "categories": ["cs.MA"], "pdf": "https://arxiv.org/pdf/2602.16726", "abs": "https://arxiv.org/abs/2602.16726", "authors": ["Hua Yan", "Heng Tan", "Yu Yang"], "title": "Guiding LLM-Based Human Mobility Simulation with Mobility Measures from Shared Data", "comment": null, "summary": "Large-scale human mobility simulation is critical for many science domains such as urban science, epidemiology, and transportation analysis. Recent works treat large language models (LLMs) as human agents to simulate realistic mobility trajectories by modeling individual-level cognitive processes. However, these approaches generate individual mobility trajectories independently, without any population-level coordination mechanism, and thus fail to capture the emergence of collective behaviors. To address this issue, we design M2LSimu, a mobility measures-guided multi-prompt adjustment framework that leverages mobility measures derived from shared data as guidance to refine individual-level prompts for realistic mobility generation. Our framework applies coarse-grained adjustment strategies guided by mobility measures, progressively enabling fine-grained individual-level adaptation while satisfying multiple population-level mobility objectives under a limited budget. Experiments show that M2LSimu significantly outperforms state-of-the-art LLM-based methods on two public datasets."}
{"id": "2602.17381", "categories": ["cs.NI"], "pdf": "https://arxiv.org/pdf/2602.17381", "abs": "https://arxiv.org/abs/2602.17381", "authors": ["François Provost", "Faisal Hawlader", "Mehdi Testouri", "Raphaël Frank"], "title": "End-to-End Latency Measurement Methodology for Connected and Autonomous Vehicle Teleoperation", "comment": null, "summary": "Connected and Autonomous Vehicles (CAVs) continue to evolve rapidly, and system latency remains one of their most critical performance parameters, particularly when vehicles are operated remotely. Existing latency-assessment methodologies focus predominantly on Glass-to-Glass (G2G) latency, defined as the delay between an event occurring in the operational environment, its capture by a camera, and its subsequent display to the remote operator. However, G2G latency accounts for only one component of the total delay experienced by the driver. The complementary component, Motion-to-Motion (M2M) latency, represents the delay between the initiation of a control input by the remote driver and the corresponding physical actuation by the vehicle. Together, M2M and G2G constitute the overall End-to-End (E2E) latency. This paper introduces a measurement framework capable of quantifying M2M, G2G, and E2E latencies using gyroscopes, a phototransistor, and two GPS-synchronized Raspberry Pi 5 units. The system employs low-pass filtering and threshold-based detection to identify steering-wheel motion on both the remote operator and vehicle sides. An interrupt is generated when the phototransistor detects the activation of an LED positioned within the camera's Field Of View (FOV). Initial measurements obtained from our teleoperated prototype vehicle over commercial 4G and 5G networks indicate an average E2E latency of approximately 500 ms (measurement precision +/- 4 ms). The M2M latency contributes up to 60% of this value."}
{"id": "2602.17282", "categories": ["cs.DC", "cs.PF", "eess.SY"], "pdf": "https://arxiv.org/pdf/2602.17282", "abs": "https://arxiv.org/abs/2602.17282", "authors": ["Boris Sedlak", "Víctor Casamayor Pujol", "Schahram Dustdar"], "title": "Visual Insights into Agentic Optimization of Pervasive Stream Processing Services", "comment": null, "summary": "Processing sensory data close to the data source, often involving Edge devices, promises low latency for pervasive applications, like smart cities. This commonly involves a multitude of processing services, executed with limited resources; this setup faces three problems: first, the application demand and the resource availability fluctuate, so the service execution must scale dynamically to sustain processing requirements (e.g., latency); second, each service permits different actions to adjust its operation, so they require individual scaling policies; third, without a higher-level mediator, services would cannibalize any resources of services co-located on the same device. This demo first presents a platform for context-aware autoscaling of stream processing services that allows developers to monitor and adjust the service execution across multiple service-specific parameters. We then connect a scaling agent to these interfaces that gradually builds an understanding of the processing environment by exploring each service's action space; the agent then optimizes the service execution according to this knowledge. Participants can revisit the demo contents as video summary and introductory poster, or build a custom agent by extending the artifact repository."}
{"id": "2602.16819", "categories": ["cs.SE", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.16819", "abs": "https://arxiv.org/abs/2602.16819", "authors": ["Yiqing Xie", "Emmy Liu", "Gaokai Zhang", "Nachiket Kotalwar", "Shubham Gandhi", "Sathwik Acharya", "Xingyao Wang", "Carolyn Rose", "Graham Neubig", "Daniel Fried"], "title": "Hybrid-Gym: Training Coding Agents to Generalize Across Tasks", "comment": null, "summary": "When assessing the quality of coding agents, predominant benchmarks focus on solving single issues on GitHub, such as SWE-Bench. In contrast, in real use, these agents solve more various and complex tasks that involve other skills such as exploring codebases, testing software, and designing architecture. In this paper, we first characterize some transferable skills that are shared across diverse tasks by decomposing trajectories into fine-grained components, and derive a set of principles for designing auxiliary training tasks to teach language models these skills. Guided by these principles, we propose a training environment, Hybrid-Gym, consisting of a set of scalable synthetic tasks, such as function localization and dependency search. Experiments show that agents trained on our synthetic tasks effectively generalize to diverse real-world tasks that are not present in training, improving a base model by 25.4% absolute gain on SWE-Bench Verified, 7.9% on SWT-Bench Verified, and 5.1% on Commit-0 Lite. Hybrid-Gym also complements datasets built for the downstream tasks (e.g., improving SWE-Play by 4.9% on SWT-Bench Verified). Code available at: https://github.com/yiqingxyq/Hybrid-Gym."}
{"id": "2602.17119", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2602.17119", "abs": "https://arxiv.org/abs/2602.17119", "authors": ["Zhenyu Bai", "Pranav Dangi", "Rohan Juneja", "Zhaoying Li", "Zhanglu Yan", "Huiying Lan", "Tulika Mitra"], "title": "A Data-Driven Dynamic Execution Orchestration Architecture", "comment": "ASPLOS 2026", "summary": "Domain-specific accelerators deliver exceptional performance on their target workloads through fabrication-time orchestrated datapaths. However, such specialized architectures often exhibit performance fragility when exposed to new kernels or irregular input patterns. In contrast, programmable architectures like FPGAs, CGRAs, and GPUs rely on compile-time orchestration to support a broader range of applications; but they are typically less efficient under irregular or sparse data. Pushing the boundaries of programmable architectures requires designs that can achieve efficiency and high-performance on par with specialized accelerators while retaining the agility of general-purpose architectures.\n  We introduce Canon, a parallel architecture that bridges the gap between specialized and general purpose architectures. Canon exploits data-level and instruction-level parallelism through its novel design. First, it employs a novel dynamic data-driven orchestration mechanism using programmable Finite State Machines (FSMs). These FSMs are programmed at compile time to encode high-level dataflow per state and translate incoming meta-information (e.g., sparse coordinates) into control instructions at runtime. Second, Canon introduces a time-lapsed SIMD execution in which instructions are issued across a row of processing elements over several cycles, creating a staggered pipelined execution. These innovations amortize control overhead, allowing dynamic instruction changes while constructing a continuously evolving dataflow that maximizes parallelism. Experimental evaluation shows that Canon delivers high performance across diverse data-agnostic and data-driven kernels while achieving efficiency comparable to specialized accelerators, yet retaining the flexibility of a general-purpose architecture."}
{"id": "2602.16936", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2602.16936", "abs": "https://arxiv.org/abs/2602.16936", "authors": ["Zikai Zhang", "Rui Hu", "Jiahao Xu"], "title": "Heterogeneous Federated Fine-Tuning with Parallel One-Rank Adaptation", "comment": "To appear in ICLR 2026", "summary": "Large Language Models (LLMs) have demonstrated remarkable effectiveness in adapting to downstream tasks through fine-tuning. Federated Learning (FL) extends this capability by enabling collaborative fine-tuning across distributed clients using Low-Rank Adaptation (LoRA), while preserving data privacy by avoiding raw data sharing. However, practical deployments face challenges when clients have heterogeneous resources and thus adopt different LoRA ranks, leading to substantial initialization and aggregation noise that undermines performance. To address these challenges, we propose Fed-PLoRA, a novel lightweight heterogeneous federated fine-tuning (FFT) framework. Fed-PLoRA introduces Parallel One-Rank Adaptation (PLoRA), a new LoRA variant that replaces the classic multi-rank LoRA module with multiple parallel one-rank modules, and a novel Select-N-Fold strategy that folds untrained PLoRA modules into the pre-trained weights before local training, thereby accommodating heterogeneous client resources. We provide a unified analysis of initialization and aggregation noise of Fed-PLoRA and demonstrate how it addresses the limitations of state-of-the-art methods. Extensive experiments on diverse LLM fine-tuning tasks demonstrate that Fed-PLoRA consistently outperforms existing methods in both accuracy and efficiency. The code is available at https://github.com/TNI-playground/Fed-PLoRA."}
{"id": "2602.16738", "categories": ["cs.MA", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.16738", "abs": "https://arxiv.org/abs/2602.16738", "authors": ["Rebin Saleh", "Khanh Pham Dinh", "Balázs Villányi", "Truong-Son Hy"], "title": "Self-Evolving Multi-Agent Network for Industrial IoT Predictive Maintenance", "comment": null, "summary": "Industrial IoT predictive maintenance requires systems capable of real-time anomaly detection without sacrificing interpretability or demanding excessive computational resources. Traditional approaches rely on static, offline-trained models that cannot adapt to evolving operational conditions, while LLM-based monolithic systems demand prohibitive memory and latency, rendering them impractical for on-site edge deployment. We introduce SEMAS, a self-evolving hierarchical multi-agent system that distributes specialized agents across Edge, Fog, and Cloud computational tiers. Edge agents perform lightweight feature extraction and pre-filtering; Fog agents execute diversified ensemble detection with dynamic consensus voting; and Cloud agents continuously optimize system policies via Proximal Policy Optimization (PPO) while maintaining asynchronous, non-blocking inference. The framework incorporates LLM-based response generation for explainability and federated knowledge aggregation for adaptive policy distribution. This architecture enables resource-aware specialization without sacrificing real-time performance or model interpretability. Empirical evaluation on two industrial benchmarks (Boiler Emulator and Wind Turbine) demonstrates that SEMAS achieves superior anomaly detection performance with exceptional stability under adaptation, sustains prediction accuracy across evolving operational contexts, and delivers substantial latency improvements enabling genuine real-time deployment. Ablation studies confirm that PPO-driven policy evolution, consensus voting, and federated aggregation each contribute materially to system effectiveness. These findings indicate that resource-aware, self-evolving 1multi-agent coordination is essential for production-ready industrial IoT predictive maintenance under strict latency and explainability constraints."}
{"id": "2602.17394", "categories": ["cs.NI", "cs.AI", "cs.SD"], "pdf": "https://arxiv.org/pdf/2602.17394", "abs": "https://arxiv.org/abs/2602.17394", "authors": ["Nuno Saavedra", "Pedro Ribeiro", "André Coelho", "Rui Campos"], "title": "Voice-Driven Semantic Perception for UAV-Assisted Emergency Networks", "comment": "7 pages, 4 figures", "summary": "Unmanned Aerial Vehicle (UAV)-assisted networks are increasingly foreseen as a promising approach for emergency response, providing rapid, flexible, and resilient communications in environments where terrestrial infrastructure is degraded or unavailable. In such scenarios, voice radio communications remain essential for first responders due to their robustness; however, their unstructured nature prevents direct integration with automated UAV-assisted network management. This paper proposes SIREN, an AI-driven framework that enables voice-driven perception for UAV-assisted networks. By integrating Automatic Speech Recognition (ASR) with Large Language Model (LLM)-based semantic extraction and Natural Language Processing (NLP) validation, SIREN converts emergency voice traffic into structured, machine-readable information, including responding units, location references, emergency severity, and Quality-of-Service (QoS) requirements. SIREN is evaluated using synthetic emergency scenarios with controlled variations in language, speaker count, background noise, and message complexity. The results demonstrate robust transcription and reliable semantic extraction across diverse operating conditions, while highlighting speaker diarization and geographic ambiguity as the main limiting factors. These findings establish the feasibility of voice-driven situational awareness for UAV-assisted networks and show a practical foundation for human-in-the-loop decision support and adaptive network management in emergency response operations."}
{"id": "2602.16997", "categories": ["cs.SE", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2602.16997", "abs": "https://arxiv.org/abs/2602.16997", "authors": ["Diego Firmenich", "Leandro Antonelli", "Bruno Pazos", "Fabricio Lozada", "Leonardo Morales"], "title": "Exploring LLMs for User Story Extraction from Mockups", "comment": "14 pages, 6 figures. Preprint of the paper published in the 28th Workshop on Requirements Engineering (WER 2025)", "summary": "User stories are one of the most widely used artifacts in the software industry to define functional requirements. In parallel, the use of high-fidelity mockups facilitates end-user participation in defining their needs. In this work, we explore how combining these techniques with large language models (LLMs) enables agile and automated generation of user stories from mockups. To this end, we present a case study that analyzes the ability of LLMs to extract user stories from high-fidelity mockups, both with and without the inclusion of a glossary of the Language Extended Lexicon (LEL) in the prompts. Our results demonstrate that incorporating the LEL significantly enhances the accuracy and suitability of the generated user stories. This approach represents a step forward in the integration of AI into requirements engineering, with the potential to improve communication between users and developers."}
{"id": "2602.17169", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2602.17169", "abs": "https://arxiv.org/abs/2602.17169", "authors": ["Yuhuan Xia", "Tun Li", "Hongji Zhou", "Xianfa Zhou", "Chong Chen", "Ruiyu Zhang"], "title": "SimulatorCoder: DNN Accelerator Simulator Code Generation and Optimization via Large Language Models", "comment": null, "summary": "This paper presents SimulatorCoder, an agent powered by large language models (LLMs), designed to generate and optimize deep neural network (DNN) accelerator simulators based on natural language descriptions. By integrating domain-specific prompt engineering including In-Context Learning (ICL), Chain-of-Thought (CoT) reasoning, and a multi-round feedback-verification flow, SimulatorCoder systematically transforms high-level functional requirements into efficient, executable, and architecture-aligned simulator code. Experiments based on the customized SCALE-Sim benchmark demonstrate that structured prompting and feedback mechanisms substantially improve both code generation accuracy and simulator performance. The resulting simulators not only maintain cycle-level fidelity with less than 1% error compared to manually implemented counterparts, but also consistently achieve lower simulation runtimes, highlighting the effectiveness of LLM-based methods in accelerating simulator development. Our code is available at https://github.com/xiayuhuan/SimulatorCoder."}
{"id": "2602.17254", "categories": ["cs.DC", "cs.NI"], "pdf": "https://arxiv.org/pdf/2602.17254", "abs": "https://arxiv.org/abs/2602.17254", "authors": ["Anton Juerss", "Vamsi Addanki", "Stefan Schmid"], "title": "Trivance: Latency-Optimal AllReduce by Shortcutting Multiport Networks", "comment": null, "summary": "AllReduce is a fundamental collective operation in distributed computing and a key performance bottleneck for large-scale training and inference. Its completion time is determined by the number of communication steps, which dominates latency-sensitive workloads, and the communication distance affecting both latency- and bandwidth-bound regimes. Direct-connect topologies, such as torus networks used in Google's TPUv4, are particularly prone to large communication distances due to limited bisection bandwidth. Latency-optimal algorithms such as Bruck's complete AllReduce in $\\log_3 n$ steps on a bidirectional ring, but incur large communication distances that result in substantial congestion. In contrast, recent approaches such as Swing reduce communication distance and congestion, but are inherently required to perform $\\log_2 n$ steps to complete AllReduce, sacrificing latency-optimality.\n  In this paper, we present Trivance, a novel AllReduce algorithm that completes within $\\log_3 n$ steps, while reducing congestion compared to Bruck's algorithm by a factor of three and preserving bandwidth-optimality. Trivance exploits both transmission ports of a bidirectional ring within each step to triple the communication distance along both directions simultaneously. Furthermore, by performing joint reductions, Trivance improves both the number of steps and network congestion. We further show that Trivance extends naturally to multidimensional torus networks, retaining its latency advantage while achieving performance comparable to bandwidth-optimal algorithms for large messages.\n  Our empirical evaluation shows that Trivance improves state-of-the-art approaches by 5-30% for message sizes up to 8\\,MiB, in high-bandwidth settings up to 32MiB and for 3D tori up to 128MiB. Throughout the evaluation, Trivance remains the best-performing latency-optimal algorithm."}
{"id": "2602.16873", "categories": ["cs.MA", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.16873", "abs": "https://arxiv.org/abs/2602.16873", "authors": ["Geunbin Yu"], "title": "AdaptOrch: Task-Adaptive Multi-Agent Orchestration in the Era of LLM Performance Convergence", "comment": "21 pages, 10 figures, 6 tables", "summary": "As large language models from diverse providers converge toward comparable benchmark performance, the traditional paradigm of selecting a single best model per task yields diminishing returns. We argue that orchestration topology -- the structural composition of how multiple agents are coordinated, parallelized, and synthesized -- now dominates system-level performance over individual model capability. We present AdaptOrch, a formal framework for task-adaptive multi-agent orchestration that dynamically selects among four canonical topologies (parallel, sequential, hierarchical, and hybrid) based on task dependency graphs and empirically derived domain characteristics. Our framework introduces three key contributions: (1) a Performance Convergence Scaling Law, formalizing conditions under which orchestration selection outweighs model selection; (2) a Topology Routing Algorithm that maps task decomposition DAGs to optimal orchestration patterns in O(|V| + |E|) time; and (3) an Adaptive Synthesis Protocol with provable termination guarantees and heuristic consistency scoring for parallel agent outputs. We validate AdaptOrch across coding (SWE-bench), reasoning (GPQA), and retrieval-augmented generation tasks, demonstrating that topology-aware orchestration achieves 12-23% improvement over static single-topology baselines, even when using identical underlying models. Our results establish orchestration design as a first-class optimization target independent of model scaling."}
{"id": "2602.17449", "categories": ["cs.NI"], "pdf": "https://arxiv.org/pdf/2602.17449", "abs": "https://arxiv.org/abs/2602.17449", "authors": ["Daniel Amir", "Ori Cohen", "Jakob Krebs", "Mark Silberstein"], "title": "ACOS: Arrays of Cheap Optical Switches", "comment": "17 pages, 12 figures", "summary": "Machine learning training places immense demands on cluster networks, motivating specialized architectures and co-design with parallelization strategies. Recent designs incorporating optical circuit switches (OCSes) are promising, offering improved cost, power efficiency, and long-term bandwidth scaling than packet switches. However, most existing approaches rely on costly high-radix OCSes and/or combine them with packet switches to achieve competitive performance at scale. Unfortunately, high-radix OCSes are both expensive and slow to reconfigure, limiting both scalability and performance.\n  We propose Arrays of Cheap Optical Switches (ACOS), which bring application co-design directly to the structure of the reconfigurable fabric. Using low-radix OCSes as building blocks, ACOS supports the forms of reconfiguration needed in training clusters including topology selection, workload adaptation, and failure resilience. The cost of ACOS scales with supported topologies and adaptations rather than with port count, breaking past the scalability barriers of current specialized ML networks. We show through simulation that ACOS-based deployments match the performance of fully provisioned packet-switched networks when training state-of-the-art LLMs at scale, while delivering significant cost savings using existing off-the-shelf OCSes, with strong bandwidth scaling and higher cost savings in the future."}
{"id": "2602.17018", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2602.17018", "abs": "https://arxiv.org/abs/2602.17018", "authors": ["Giovanni Rosa", "David Moreno-Lumbreras", "Raula Gaikovina Kula"], "title": "Not Only for Developers: Exploring Plugin Maintenance for Knowledge-Centric Communities", "comment": "Accepted to SANER2026", "summary": "The adoption of third-party libraries has become integral to modern software development, leading to large ecosystems such as PyPI, NPM, and Maven, where contributors typically share the technical expertise to sustain extensions. In communities that are not exclusively composed of developers, however, maintaining plugin ecosystems can present different challenges. In this early results paper, we study Obsidian, a knowledge--centric platform whose community is focused on writing, organization, and creativity--has built a substantial plugin ecosystem despite not being developer--centric. We investigate what kinds of plugins exist within this hybrid ecosystem and establish a foundation for understanding how they are maintained. Using repository mining and LLM-based topic modeling on a representative sample of 396 plugins, we identify six topics related to knowledge management and tooling, which is (i) dynamic editing and organization, (ii) interface and layouts, (iii) creative writing and productivity, (iv) knowledge sync solutions, (v) linking and script tools, and (vi) workflow enhancements tools. Furthermore, analysis of the Pull Requests from these plugins show that much software evolution has been performed on these ecosystem. These findings suggest that even in mixed communities, plugin ecosystems can develop recognizable engineering structures, motivating future work that highlight three different research directions with six research questions related to the health and sustainability of these non-developer ecosystems."}
{"id": "2602.17520", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2602.17520", "abs": "https://arxiv.org/abs/2602.17520", "authors": ["Yogeswar Reddy Thota", "Setareh Rafatirad", "Homayoun Houman", "Tooraj Nikoubin"], "title": "When Models Ignore Definitions: Measuring Semantic Override Hallucinations in LLM Reasoning", "comment": null, "summary": "Large language models (LLMs) demonstrate strong performance on standard digital logic and Boolean reasoning tasks, yet their reliability under locally redefined semantics remains poorly understood. In many formal settings, such as circuit specifications, examinations, and hardware documentation, operators and components are explicitly redefined within narrow scope. Correct reasoning in these contexts requires models to temporarily suppress globally learned conventions in favor of prompt-local definitions. In this work, we study a systematic failure mode we term semantic override, in which an LLM reverts to its pretrained default interpretation of operators or gate behavior despite explicit redefinition in the prompt. We also identify a related class of errors, assumption injection, where models commit to unstated hardware semantics when critical details are underspecified, rather than requesting clarification. We introduce a compact micro-benchmark of 30 logic and digital-circuit reasoning tasks designed as verifier-style traps, spanning Boolean algebra, operator overloading, redefined gates, and circuit-level semantics. Evaluating three frontier LLMs, we observe persistent noncompliance with local specifications, confident but incompatible assumptions, and dropped constraints even in elementary settings. Our findings highlight a gap between surface-level correctness and specification-faithful reasoning, motivating evaluation protocols that explicitly test local unlearning and semantic compliance in formal domains."}
{"id": "2602.17282", "categories": ["cs.DC", "cs.PF", "eess.SY"], "pdf": "https://arxiv.org/pdf/2602.17282", "abs": "https://arxiv.org/abs/2602.17282", "authors": ["Boris Sedlak", "Víctor Casamayor Pujol", "Schahram Dustdar"], "title": "Visual Insights into Agentic Optimization of Pervasive Stream Processing Services", "comment": null, "summary": "Processing sensory data close to the data source, often involving Edge devices, promises low latency for pervasive applications, like smart cities. This commonly involves a multitude of processing services, executed with limited resources; this setup faces three problems: first, the application demand and the resource availability fluctuate, so the service execution must scale dynamically to sustain processing requirements (e.g., latency); second, each service permits different actions to adjust its operation, so they require individual scaling policies; third, without a higher-level mediator, services would cannibalize any resources of services co-located on the same device. This demo first presents a platform for context-aware autoscaling of stream processing services that allows developers to monitor and adjust the service execution across multiple service-specific parameters. We then connect a scaling agent to these interfaces that gradually builds an understanding of the processing environment by exploring each service's action space; the agent then optimizes the service execution according to this knowledge. Participants can revisit the demo contents as video summary and introductory poster, or build a custom agent by extending the artifact repository."}
{"id": "2602.17078", "categories": ["cs.MA"], "pdf": "https://arxiv.org/pdf/2602.17078", "abs": "https://arxiv.org/abs/2602.17078", "authors": ["Xuefeng Wang", "Lei Zhang", "Henglin Pu", "Husheng Li", "Ahmed H. Qureshi"], "title": "Safe Continuous-time Multi-Agent Reinforcement Learning via Epigraph Form", "comment": "Accepted by ICLR 2026. 27 pages, 15 figures", "summary": "Multi-agent reinforcement learning (MARL) has made significant progress in recent years, but most algorithms still rely on a discrete-time Markov Decision Process (MDP) with fixed decision intervals. This formulation is often ill-suited for complex multi-agent dynamics, particularly in high-frequency or irregular time-interval settings, leading to degraded performance and motivating the development of continuous-time MARL (CT-MARL). Existing CT-MARL methods are mainly built on Hamilton-Jacobi-Bellman (HJB) equations. However, they rarely account for safety constraints such as collision penalties, since these introduce discontinuities that make HJB-based learning difficult. To address this challenge, we propose a continuous-time constrained MDP (CT-CMDP) formulation and a novel MARL framework that transforms discrete MDPs into CT-CMDPs via an epigraph-based reformulation. We then solve this by proposing a novel physics-informed neural network (PINN)-based actor-critic method that enables stable and efficient optimization in continuous time. We evaluate our approach on continuous-time safe multi-particle environments (MPE) and safe multi-agent MuJoCo benchmarks. Results demonstrate smoother value approximations, more stable training, and improved performance over safe MARL baselines, validating the effectiveness and robustness of our method."}
{"id": "2602.17534", "categories": ["cs.NI"], "pdf": "https://arxiv.org/pdf/2602.17534", "abs": "https://arxiv.org/abs/2602.17534", "authors": ["Sultan Çoğay", "T. Tolga Sari", "Muhammad Nadeem Ali", "Byung-Seo Kim", "Gökhan Seçinti"], "title": "HAP Networks for the Future: Applications in Sensing, Computing, and Communication", "comment": null, "summary": "High Altitude Platforms (HAPs) are a major advancement in non-terrestrial networks, offering broad coverage and unique capabilities. They form a vital link between satellite systems and terrestrial networks and play a key role in next-generation communication technologies. This study reviews HAP network applications, focusing on advanced airborne communications, integrated sensing, and airborne informatics. Our survey assesses the current state of HAP-centric applications by examining data processing, network performance, computational and storage requirements, economic feasibility, and regulatory challenges. The analysis highlights the evolving role of HAPs in global communication and identifies future research directions to support their deployment."}
{"id": "2602.17037", "categories": ["cs.SE", "cs.AI", "cs.HC", "cs.PL"], "pdf": "https://arxiv.org/pdf/2602.17037", "abs": "https://arxiv.org/abs/2602.17037", "authors": ["Rahul Nanda", "Chandra Maddila", "Smriti Jha", "Euna Mehnaz Khan", "Matteo Paltenghi", "Satish Chandra"], "title": "Wink: Recovering from Misbehaviors in Coding Agents", "comment": null, "summary": "Autonomous coding agents, powered by large language models (LLMs), are increasingly being adopted in the software industry to automate complex engineering tasks. However, these agents are prone to a wide range of misbehaviors, such as deviating from the user's instructions, getting stuck in repetitive loops, or failing to use tools correctly. These failures disrupt the development workflow and often require resource-intensive manual intervention. In this paper, we present a system for automatically recovering from agentic misbehaviors at scale. We first introduce a taxonomy of misbehaviors grounded in an analysis of production traffic, identifying three primary categories: Specification Drift, Reasoning Problems, and Tool Call Failures, which we find occur in about 30% of all agent trajectories.\n  To address these issues, we developed a lightweight, asynchronous self-intervention system named Wink. Wink observes agent trajectories and provides targeted course-correction guidance to nudge the agent back to a productive path. We evaluated our system on over 10,000 real world agent trajectories and found that it successfully resolves 90% of the misbehaviors that require a single intervention. Furthermore, a live A/B test in our production environment demonstrated that our system leads to a statistically significant reduction in Tool Call Failures, Tokens per Session and Engineer Interventions per Session. We present our experience designing and deploying this system, offering insights into the challenges of building resilient agentic systems at scale."}
{"id": "2602.16858", "categories": ["cs.PF", "cs.AR"], "pdf": "https://arxiv.org/pdf/2602.16858", "abs": "https://arxiv.org/abs/2602.16858", "authors": ["Kathiravan Palaniappan"], "title": "GDEV-AI: A Generalized Evaluation of Deep Learning Inference Scaling and Architectural Saturation", "comment": "14 pages, 18 figures, 20 references", "summary": "The deployment of deep learning inference in production environments continues to grow, where throughput, latency, and hardware efficiency are critical. Although specialized accelerators are increasingly adopted, many inference workloads still run on CPU-only systems, particularly in legacy data centers and cost-sensitive environments. This study investigates the scalability limits of CPU-based inference for convolutional neural networks by benchmarking ResNet models across varying batch sizes on two hardware tiers: a legacy Intel Xeon E5-2403 v2 processor and a modern Intel Xeon 6 \"Granite Rapids\" platform.\n  Results show that legacy CPUs quickly reach throughput saturation, with limited scaling beyond small batch sizes due to instruction-level and memory constraints. In contrast, the Granite Rapids system leverages Intel Advanced Matrix Extensions (AMX) to achieve substantially higher throughput. However, oversubscription beyond physical core limits introduces execution contention and tail-latency amplification, revealing a performance degradation regime in modern architectures.\n  We introduce GDEV-AI, a reproducible benchmarking framework for analyzing scalability behavior and architectural saturation in CPU-based inference. By establishing a vendor-neutral baseline, this work provides empirical insight into performance bottlenecks and informs capacity planning in heterogeneous data center environments."}
{"id": "2602.17318", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2602.17318", "abs": "https://arxiv.org/abs/2602.17318", "authors": ["Patrick Zojer", "Jonas Posner", "Taylan Özden"], "title": "Evaluating Malleable Job Scheduling in HPC Clusters using Real-World Workloads", "comment": null, "summary": "Optimizing resource utilization in high-performance computing (HPC) clusters is essential for maximizing both system efficiency and user satisfaction. However, traditional rigid job scheduling often results in underutilized resources and increased job waiting times.\n  This work evaluates the benefits of resource elasticity, where the job scheduler dynamically adjusts the resource allocation of malleable jobs at runtime. Using real workload traces from the Cori, Eagle, and Theta supercomputers, we simulate varying proportions (0-100%) of malleable jobs with the ElastiSim software.\n  We evaluate five job scheduling strategies, including a novel one that maintains malleable jobs at their preferred resource allocation when possible. Results show that, compared to fully rigid workloads, malleable jobs yield significant improvements across all key metrics. Considering the best-performing scheduling strategy for each supercomputer, job turnaround times decrease by 37-67%, job makespan by 16-65%, job wait times by 73-99%, and node utilization improves by 5-52%. Although improvements vary, gains remain substantial even at 20% malleable jobs.\n  This work highlights important correlations between workload characteristics (e.g., job runtimes and node requirements), malleability proportions, and scheduling strategies. These findings confirm the potential of malleability to address inefficiencies in current HPC practices and demonstrate that even limited adoption can provide substantial advantages, encouraging its integration into HPC resource management."}
{"id": "2602.17100", "categories": ["cs.MA"], "pdf": "https://arxiv.org/pdf/2602.17100", "abs": "https://arxiv.org/abs/2602.17100", "authors": ["Siyu Wang", "Ruotian Lu", "Zhihao Yang", "Yuchao Wang", "Yanzhou Zhang", "Lei Xu", "Qimin Xu", "Guojun Yin", "Cailian Chen", "Xinping Guan"], "title": "AgentConductor: Topology Evolution for Multi-Agent Competition-Level Code Generation", "comment": null, "summary": "Large language model(LLM)-driven multi-agent systems(MAS) coordinate specialized agents through predefined interaction topologies and have shown promise for complex tasks such as competition-level code generation. Recent studies demonstrate that carefully designed multi-agent workflows and communication graphs can significantly improve code generation performance by leveraging collaborative reasoning. However, existing methods neither adapt topology density to task difficulty nor iteratively refine the topology within an instance using execution feedback, which leads to redundant communication and performance bottlenecks. To address these issues, we propose AgentConductor: a reinforcement learning-optimized MAS with an LLM-based orchestrator agent as its core, which enables end-to-end feedback-driven dynamic generation of interaction topologies. For each query, AgentConductor infers agent roles and task difficulty, then constructs a task-adapted, density-aware layered directed acyclic graph (DAG) topology, underpinned by two key innovations. First, we design a novel topological density function that captures communication-aware mathematical characterizations of multi-agent interactions. Second, we adopt difficulty interval partitioning to avoid excessive pruning for precise topological density upper bound measurement per difficulty level and finer-grained control. Empirically, across three competition-level and two foundational code datasets, AgentConductor achieves state-of-the-art accuracy, outperforming the strongest baseline by up to 14.6% in pass@1 accuracy, 13% in density reduction, and 68% in token cost reduction."}
{"id": "2602.17619", "categories": ["cs.NI"], "pdf": "https://arxiv.org/pdf/2602.17619", "abs": "https://arxiv.org/abs/2602.17619", "authors": ["Jothi Prasanna Shanmuga Sundaram", "Magzhan Gabidolla", "Luis Fujarte", "Shawn Duong", "Jianlin Guo", "Toshiaki Koike-Akino", "Pu", "Wang", "Kieran Parsons", "Philip V. Orlik", "Takenori Sumi", "Yukimasa Nagai", "Miguel A. Carreira-Perpinan", "Alberto E. Cerpa"], "title": "EDRP: Enhanced Dynamic Relay Point Protocol for Data Dissemination in Multi-hop Wireless IoT Networks", "comment": null, "summary": "Emerging IoT applications are transitioning from battery-powered to grid-powered nodes. DRP, a contention-based data dissemination protocol, was developed for these applications. Traditional contention-based protocols resolve collisions through control packet exchanges, significantly reducing goodput. DRP mitigates this issue by employing a distributed delay timer mechanism that assigns transmission-start delays based on the average link quality between a sender and its children, prioritizing highly connected nodes for early transmission. However, our in-field experiments reveal that DRP is unable to accommodate real-world link quality fluctuations, leading to overlapping transmissions from multiple senders. This overlap triggers CSMA's random back-off delays, ultimately degrading the goodput performance.\n  To address these shortcomings, we first conduct a theoretical analysis that characterizes the design requirements induced by real-world link quality fluctuations and DRP's passive acknowledgments. Guided by this analysis, we design EDRP, which integrates two novel components: (i) Link-Quality Aware CSMA (LQ-CSMA) and (ii) a Machine Learning-based Block Size Selection (ML-BSS) algorithm for rateless codes. LQ-CSMA dynamically restricts the back-off delay range based on real-time link quality estimates, ensuring that nodes with stronger connectivity experience shorter delays. ML-BSS algorithm predicts future link quality conditions and optimally adjusts the block size for rateless coding, reducing overhead and enhancing goodput. In-field evaluations of EDRP demonstrate an average goodput improvement of 39.43\\% than the competing protocols."}
{"id": "2602.17091", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2602.17091", "abs": "https://arxiv.org/abs/2602.17091", "authors": ["Kan Watanabe", "Tatsuya Shirai", "Yutaro Kashiwa", "Hajimu Iida"], "title": "What to Cut? Predicting Unnecessary Methods in Agentic Code Generation", "comment": null, "summary": "Agentic Coding, powered by autonomous agents such as GitHub Copilot and Cursor, enables developers to generate code, tests, and pull requests from natural language instructions alone. While this accelerates implementation, it produces larger volumes of code per pull request, shifting the burden from implementers to reviewers. In practice, a notable portion of AI-generated code is eventually deleted during review, yet reviewers must still examine such code before deciding to remove it. No prior work has explored methods to help reviewers efficiently identify code that will be removed.In this paper, we propose a prediction model that identifies functions likely to be deleted during PR review. Our results show that functions deleted for different reasons exhibit distinct characteristics, and our model achieves an AUC of 87.1%. These findings suggest that predictive approaches can help reviewers prioritize their efforts on essential code."}
{"id": "2602.17541", "categories": ["cs.DC", "cs.DS"], "pdf": "https://arxiv.org/pdf/2602.17541", "abs": "https://arxiv.org/abs/2602.17541", "authors": ["Lelia Blin", "Sylvain Gay", "Isabella Ziccardi"], "title": "Informative Trains: A Memory-Efficient Journey to a Self-Stabilizing Leader Election Algorithm in Anonymous Graphs", "comment": null, "summary": "We study the self-stabilizing leader election problem in anonymous $n$-nodes networks. Achieving self-stabilization with low space memory complexity is particularly challenging, and designing space-optimal leader election algorithms remains an open problem for general graphs. In deterministic settings, it is known that $Ω(\\log \\log n)$ bits of memory per node are necessary [Blin et al., Disc. Math. \\& Theor. Comput. Sci., 2023], while in probabilistic settings the same lower bound holds for some values of $n$, but only for an unfair scheduler [Beauquier et al., PODC 1999]. Several deterministic and probabilistic protocols have been proposed in models ranging from the state model to the population protocols. However, to the best of our knowledge, existing solutions either require $Ω(\\log n)$ bits of memory per node for general worst case graphs, or achieve low state complexity only under restricted network topologies such as rings, trees, or bounded-degree graphs.\n  In this paper, we present a probabilistic self-stabilizing leader election algorithm for arbitrary anonymous networks that uses $O(\\log \\log n)$ bits of memory per node. Our algorithm operates in the state model under a synchronous scheduler and assumes knowledge of a global parameter $N = Θ(\\log n)$. We show that, under our protocol, the system converges almost surely to a stable configuration with a unique leader and stabilizes within $O(\\mathrm{poly}(n))$ rounds with high probability. To achieve $O(\\log \\log n)$ bits of memory, our algorithm keeps transmitting information after convergence, i.e. it does not verify the silence property. Moreover, like most works in the field, our algorithm does not provide explicit termination detection (i.e., nodes do not detect when the algorithm has converged)."}
{"id": "2602.17203", "categories": ["cs.MA", "cs.GT"], "pdf": "https://arxiv.org/pdf/2602.17203", "abs": "https://arxiv.org/abs/2602.17203", "authors": ["Yuhong Luo", "Daniel Schoepflin", "Xintong Wang"], "title": "Algorithmic Collusion at Test Time: A Meta-game Design and Evaluation", "comment": "AAMAS 2026. 31 pages", "summary": "The threat of algorithmic collusion, and whether it merits regulatory intervention, remains debated, as existing evaluations of its emergence often rely on long learning horizons, assumptions about counterparty rationality in adopting collusive strategies, and symmetry in hyperparameters and economic settings among players. To study collusion risk, we introduce a meta-game design for analyzing algorithmic behavior under test-time constraints. We model agents as possessing pretrained policies with distinct strategic characteristics (e.g., competitive, naively cooperative, robustly collusive), and formulate the problem as selecting a meta-strategy that combines a pretrained, initial policy with an in-game adaptation rule. We seek to examine whether collusion can emerge under rational choices and how agents co-adapt toward cooperation or competition. To this end, we sample normal-form empirical games over meta-strategy profiles, % across random initial game states, compute relevant game statistics (e.g., payoffs against individuals and regret against an equilibrium mixture of opponents), and construct empirical best-response graphs to uncover strategic relationships. We evaluate both reinforcement-learning and LLM-based strategies in repeated pricing games under symmetric and asymmetric cost settings, and present findings on the feasibility of algorithmic collusion and the effectiveness of pricing strategies in practical ``test-time'' environments.\n  The source code and the full paper with appendix are available at: https://github.com/chailab-rutgers/CollusionMetagame."}
{"id": "2602.17254", "categories": ["cs.DC", "cs.NI"], "pdf": "https://arxiv.org/pdf/2602.17254", "abs": "https://arxiv.org/abs/2602.17254", "authors": ["Anton Juerss", "Vamsi Addanki", "Stefan Schmid"], "title": "Trivance: Latency-Optimal AllReduce by Shortcutting Multiport Networks", "comment": null, "summary": "AllReduce is a fundamental collective operation in distributed computing and a key performance bottleneck for large-scale training and inference. Its completion time is determined by the number of communication steps, which dominates latency-sensitive workloads, and the communication distance affecting both latency- and bandwidth-bound regimes. Direct-connect topologies, such as torus networks used in Google's TPUv4, are particularly prone to large communication distances due to limited bisection bandwidth. Latency-optimal algorithms such as Bruck's complete AllReduce in $\\log_3 n$ steps on a bidirectional ring, but incur large communication distances that result in substantial congestion. In contrast, recent approaches such as Swing reduce communication distance and congestion, but are inherently required to perform $\\log_2 n$ steps to complete AllReduce, sacrificing latency-optimality.\n  In this paper, we present Trivance, a novel AllReduce algorithm that completes within $\\log_3 n$ steps, while reducing congestion compared to Bruck's algorithm by a factor of three and preserving bandwidth-optimality. Trivance exploits both transmission ports of a bidirectional ring within each step to triple the communication distance along both directions simultaneously. Furthermore, by performing joint reductions, Trivance improves both the number of steps and network congestion. We further show that Trivance extends naturally to multidimensional torus networks, retaining its latency advantage while achieving performance comparable to bandwidth-optimal algorithms for large messages.\n  Our empirical evaluation shows that Trivance improves state-of-the-art approaches by 5-30% for message sizes up to 8\\,MiB, in high-bandwidth settings up to 32MiB and for 3D tori up to 128MiB. Throughout the evaluation, Trivance remains the best-performing latency-optimal algorithm."}
{"id": "2602.17112", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2602.17112", "abs": "https://arxiv.org/abs/2602.17112", "authors": ["Arjun Ashok", "Nafiz Imtiaz Khan", "Swati Singhvi", "Stefan Stanciulescu", "Zhouhao Wang", "Vladimir Filkov"], "title": "Multi-Ecosystem Modeling of OSS Project Sustainability", "comment": "42 pages, 11 figures", "summary": "Many OSS projects join foundations such as Apache, Eclipse, and OSGeo, to aid their immediate plans and improve long-term prospects by getting governance advice, incubation support, and community-building mechanisms. But foundations differ in their policies, funding models, and support strategies. Moreover, since projects joining these foundations are diverse, coming at different lifecycle stages and having different needs, it can be challenging to decide on the appropriate project-foundation match and on the project-specific plan for sustainability.\n  Here, we present an empirical study and quantitative analysis of the sustainability of incubator projects in the Apache, Eclipse, and OSGeo foundations, and, additionally, of OSS projects from GitHub outside of foundations. We develop foundation-specific sustainability models and a project triage, based on projects' sociotechnical trace profiles, and demonstrate their effectiveness across the foundations. Our results show that our models with triage can effectively forecast sustainability outcomes not only within but across foundations. In addition, the generalizability of the framework allows us to apply the approach to GitHub projects outside the foundations. We complement our findings with actionable recovery strategies from previous work and apply them to case studies of failed incubator projects. Our study highlights the value of sociotechnical frameworks in characterizing and addressing software project sustainability issues."}
{"id": "2602.17552", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2602.17552", "abs": "https://arxiv.org/abs/2602.17552", "authors": ["Tripti Agarwal", "Sheng Di", "Xin Liang", "Zhaoyuan Su", "Yuxiao Li", "Ganesh Gopalakrishnan", "Hanqi Guo", "Franck Cappello"], "title": "TopoSZp: Lightweight Topology-Aware Error-controlled Compression for Scientific Data", "comment": "11 pages, 9 figures, 2 tables", "summary": "Error-bounded lossy compression is essential for managing the massive data volumes produced by large-scale HPC simulations. While state-of-the-art compressors such as SZ and ZFP provide strong numerical error guarantees, they often fail to preserve topological structures (example, minima, maxima, and saddle points) that are critical for scientific analysis. Existing topology-aware compressors address this limitation but incur substantial computational overhead. We present TopoSZp, a lightweight, topology-aware, error-controlled lossy compressor that preserves critical points and their relationships while maintaining high compression and decompression performance. Built on the high-throughput SZp compressor, TopoSZp integrates efficient critical point detection, local ordering preservation, and targeted saddle point refinement, all within a relaxed but strictly enforced error bound. Experimental results on real-world scientific datasets show that TopoSZp achieves 3 to 100 times fewer non-preserved critical points, introduces no false positives or incorrect critical point types, and delivers 100 to 10000 times faster compression and 10 to 500 times faster decompression compared to existing topology-aware compressors, while maintaining competitive compression ratios."}
{"id": "2602.17131", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2602.17131", "abs": "https://arxiv.org/abs/2602.17131", "authors": ["Yuki Takei", "Toshiaki Aoki", "Chaiyong Ragkhitwetsagul"], "title": "Quantifying Competitive Relationships Among Open-Source Software Projects", "comment": "12 pages, 3 figures, 7 tables. Accepted at MSR 2026", "summary": "Throughout the history of software, evolution has occurred in cycles of rise and fall driven by competition, and open-source software (OSS) is no exception. This cycle is accelerating, particularly in rapidly evolving domains such as web development and deep learning. However, the impact of competitive relationships among OSS projects on their survival remains unclear, and there are risks of losing a competitive edge to rivals. To address this, this study proposes a new automated method called ``Mutual Impact Analysis of OSS (MIAO)'' to quantify these competitive relationships. The proposed method employs a structural vector autoregressive model and impulse response functions, normally used in macroeconomic analysis, to analyze the interactions among OSS projects. In an empirical analysis involving mining and analyzing 187 OSS project groups, MIAO identified projects that were forced to cease development owing to competitive influences with up to 81\\% accuracy, and the resulting features supported predictive experiments that anticipate cessation one year ahead with up to 77\\% accuracy. This suggests that MIAO could be a valuable tool for OSS project maintainers to understand the dynamics of OSS ecosystems and predict the rise and fall of OSS projects."}
{"id": "2602.17610", "categories": ["cs.DC", "cs.DB"], "pdf": "https://arxiv.org/pdf/2602.17610", "abs": "https://arxiv.org/abs/2602.17610", "authors": ["Nicolau Manubens Gil"], "title": "Exploring Novel Data Storage Approaches for Large-Scale Numerical Weather Prediction", "comment": "PhD. thesis successfully defended at The University of Edinburgh on the 16th October 2025", "summary": "Driven by scientific and industry ambition, HPC and AI applications such as operational Numerical Weather Prediction (NWP) require processing and storing ever-increasing data volumes as fast as possible. Whilst POSIX distributed file systems and NVMe SSDs are currently a common HPC storage configuration providing I/O to applications, new storage solutions have proliferated or gained traction over the last decade with potential to address performance limitations POSIX file systems manifest at scale for certain I/O workloads.\n  This work has primarily aimed to assess the suitability and performance of two object storage systems -namely DAOS and Ceph- for the ECMWF's operational NWP as well as for HPC and AI applications in general. New software-level adapters have been developed which enable the ECMWF's NWP to leverage these systems, and extensive I/O benchmarking has been conducted on a few computer systems, comparing the performance delivered by the evaluated object stores to that of equivalent Lustre file system deployments on the same hardware. Challenges of porting to object storage and its benefits with respect to the traditional POSIX I/O approach have been discussed and, where possible, domain-agnostic performance analysis has been conducted, leading to insight also of relevance to I/O practitioners and the broader HPC community.\n  DAOS and Ceph have both demonstrated excellent performance, but DAOS stood out relative to Ceph and Lustre, providing superior scalability and flexibility for applications to perform I/O at scale as desired. This sets a promising outlook for DAOS and object storage, which might see greater adoption at HPC centres in the years to come, although not necessarily implying a shift away from POSIX-like I/O."}
{"id": "2602.17183", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.17183", "abs": "https://arxiv.org/abs/2602.17183", "authors": ["Kishan Maharaj", "Nandakishore Menon", "Ashita Saxena", "Srikanth Tamilselvam"], "title": "Robustness and Reasoning Fidelity of Large Language Models in Long-Context Code Question Answering", "comment": "11 pages, 4 Figures, 5 Tables, Work in Progress", "summary": "Large language models (LLMs) increasingly assist software engineering tasks that require reasoning over long code contexts, yet their robustness under varying input conditions remains unclear. We conduct a systematic study of long-context code question answering using controlled ablations that test sensitivity to answer format, distractors, and context scale. Extending LongCodeBench Python dataset with new COBOL and Java question-answer sets, we evaluate state-of-the-art models under three settings: (i) shuffled multiple-choice options, (ii) open-ended questions and (iii) needle-in-a-haystack contexts containing relevant and adversarially irrelevant information. Results show substantial performance drops in both shuffled multiple-choice options and open-ended questions, and brittle behavior in the presence of irrelevant cues. Our findings highlight limitations of current long-context evaluations and provide a broader benchmark for assessing code reasoning in both legacy and modern systems."}
{"id": "2602.17193", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2602.17193", "abs": "https://arxiv.org/abs/2602.17193", "authors": ["Juho Vepsäläinen"], "title": "The Case for HTML First Web Development", "comment": "28 pages, 5 figures, 5 tables, preprint", "summary": "Since its introduction in the early 90s, the web has become the largest application platform available globally. HyperText Markup Language (HTML) has been an essential part of the web since the beginning, as it allows defining webpages in a tree-like manner, including semantics and content. Although the web was never meant to be an application platform, it evolved as such, especially since the early 2000s, as web application frameworks became available. While the emergence of frameworks made it easier than ever to develop complex applications, it also put HTML on the back burner. As web standards caught up, especially with milestones such as HTML5, the gap between the web platform and frameworks was reduced. HTML First development emphasizes this shift and puts focus on literally using HTML first when possible, while encouraging minimalism familiar from the early days of the web. It seems HTML-oriented web development can provide clear benefits to developers, especially when it is combined with comple- mentary approaches, such as embracing hypermedia and moving a large part of application logic to the server side. In the context of the htmx project, it was observed that moving towards HTML can reduce the size of a codebase greatly while leading to maintenance and development benefits due to the increased conceptual simplicity. Holotype-based comparisons for content-oriented websites show performance benefits, and the same observation was confirmed by a small case study where the Yle website was converted to follow HTML First principles. In short, the HTML First approach seems to have clear advantages for web developers, while there are open questions related to the magnitude of the benefits and the alignment with the recent trend of AI-driven web development."}
{"id": "2602.17237", "categories": ["cs.SE", "cs.FL"], "pdf": "https://arxiv.org/pdf/2602.17237", "abs": "https://arxiv.org/abs/2602.17237", "authors": ["Tannaz Zameni", "Petra van den Bos", "Arend Rensink"], "title": "Disjunction Composition of BDD Transition Systems for Model-Based Testing", "comment": "Technical report with proofs", "summary": "We introduce a compositional approach to model-based test generation in Behavior-Driven Development (BDD). BDD is an agile methodology in which system behavior is specified through textual scenarios that, in our approach, are translated into transition systems used for model-based testing. This paper formally defines disjunction composition, to combine BDD transition systems that represent alternative system behaviors. Disjunction composition allows for modeling and testing the integrated behavior while ensuring that the testing power of the original set of scenarios is preserved. This is proved using a symbolic semantics for BDD transition systems, with the property that the symbolic equivalence of two BDD transition systems guarantees that they fail the same test cases. Also, we demonstrate the potential of disjunction composition by applying the composition in an industrial case study."}
{"id": "2602.17320", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2602.17320", "abs": "https://arxiv.org/abs/2602.17320", "authors": ["Stefano Lambiase", "Manuel De Stefano", "Fabio Palomba", "Filomena Ferrucci", "Andrea De Lucia"], "title": "Socio-Technical Well-Being of Quantum Software Communities: An Overview on Community Smells", "comment": null, "summary": "Quantum computing has gained significant attention due to its potential to solve computational problems beyond the capabilities of classical computers. With major corporations and academic institutions investing in quantum hardware and software, there has been a rise in the development of quantum-enabled systems, particularly within open-source communities. However, despite the promising nature of quantum technologies, these communities face critical socio-technical challenges, including the emergence of socio-technical anti-patterns known as community smells. These anti-patterns, prevalent in open-source environments, have the potential to negatively impact both product quality and community health by introducing technical debt and amplifying architectural and code smells. Despite the importance of these socio-technical factors, there remains a scarcity of research investigating their influence within quantum open-source communities. This work aims to address this gap by providing a first step in analyzing the socio-technical well-being of quantum communities through a cross-sectional study. By understanding the socio-technical dynamics at play, it is expected that foundational knowledge can be established to mitigate the risks associated with community smells and ensure the long-term sustainability of open-source quantum initiatives."}
{"id": "2602.17365", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2602.17365", "abs": "https://arxiv.org/abs/2602.17365", "authors": ["Yiming Guan", "Rui Yu", "John Zhang", "Lu Wang", "Chaoyun Zhang", "Liqun Li", "Bo Qiao", "Si Qin", "He Huang", "Fangkai Yang", "Pu Zhao", "Lukas Wutschitz", "Samuel Kessler", "Huseyin A Inan", "Robert Sim", "Saravan Rajmohan", "Qingwei Lin", "Dongmei Zhang"], "title": "Computer-Using World Model", "comment": "35 pages, 7 figures", "summary": "Agents operating in complex software environments benefit from reasoning about the consequences of their actions, as even a single incorrect user interface (UI) operation can derail long, artifact-preserving workflows. This challenge is particularly acute for computer-using scenarios, where real execution does not support counterfactual exploration, making large-scale trial-and-error learning and planning impractical despite the environment being fully digital and deterministic. We introduce the Computer-Using World Model (CUWM), a world model for desktop software that predicts the next UI state given the current state and a candidate action. CUWM adopts a two-stage factorization of UI dynamics: it first predicts a textual description of agent-relevant state changes, and then realizes these changes visually to synthesize the next screenshot. CUWM is trained on offline UI transitions collected from agents interacting with real Microsoft Office applications, and further refined with a lightweight reinforcement learning stage that aligns textual transition predictions with the structural requirements of computer-using environments. We evaluate CUWM via test-time action search, where a frozen agent uses the world model to simulate and compare candidate actions before execution. Across a range of Office tasks, world-model-guided test-time scaling improves decision quality and execution robustness."}
{"id": "2602.17426", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2602.17426", "abs": "https://arxiv.org/abs/2602.17426", "authors": ["Marco Autili", "Gianluca Filippone", "Mashal Afzal Memon", "Patrizio Pelliccione"], "title": "The Runtime Dimension of Ethics in Self-Adaptive Systems", "comment": null, "summary": "Self-adaptive systems increasingly operate in close interaction with humans, often sharing the same physical or virtual environments and making decisions with ethical implications at runtime. Current approaches typically encode ethics as fixed, rule-based constraints or as a single chosen ethical theory embedded at design time. This overlooks a fundamental property of human-system interaction settings: ethical preferences vary across individuals and groups, evolve with context, and may conflict, while still needing to remain within a legally and regulatorily defined hard-ethics envelope (e.g., safety and compliance constraints). This paper advocates a shift from static ethical rules to runtime ethical reasoning for self-adaptive systems, where ethical preferences are treated as runtime requirements that must be elicited, represented, and continuously revised as stakeholders and situations change. We argue that satisfying such requirements demands explicit ethics-based negotiation to manage ethical trade-offs among multiple humans who interact with, are represented by, or are affected by a system. We identify key challenges, ethical uncertainty, conflicts among ethical values (including human, societal, and environmental drivers), and multi-dimensional/multi-party/multi-driver negotiation, and outline research directions and questions toward ethically self-adaptive systems."}
{"id": "2602.17498", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2602.17498", "abs": "https://arxiv.org/abs/2602.17498", "authors": ["Julian Frattini", "Quim Motger"], "title": "Towards a Software Reference Architecture for Natural Language Processing Tools in Requirements Engineering", "comment": null, "summary": "Natural Language Processing (NLP) tools support requirements engineering (RE) tasks like requirements elicitation, classification, and validation. However, they are often developed from scratch despite functional overlaps, and abandoned after publication. This lack of interoperability and maintenance incurs unnecessary development effort, impedes tool comparison and benchmarking, complicates documentation, and diminishes the long-term sustainability of NLP4RE tools. To address these issues, we postulate a vision to transition from monolithic NLP4RE tools to an ecosystem of reusable, interoperable modules. We outline a research roadmap towards a software reference architecture (SRA) to realize this vision, elaborated following a standard methodological framework for SRA development. As an initial step, we conducted a stakeholder-driven focus group session to elicit generic system requirements for NLP4RE tools. This activity resulted in 36 key system requirements, further motivating the need for a dedicated SRA. Overall, the proposed vision, roadmap, and initial contribution pave the way towards improved development, reuse, and long-term maintenance of NLP4RE tools."}
