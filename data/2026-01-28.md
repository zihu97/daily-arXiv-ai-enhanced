<div id=toc></div>

# Table of Contents

- [cs.AR](#cs.AR) [Total: 2]
- [cs.SE](#cs.SE) [Total: 23]
- [cs.MA](#cs.MA) [Total: 1]
- [cs.DC](#cs.DC) [Total: 4]
- [cs.NI](#cs.NI) [Total: 3]


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [1] [M$^{\text{2}}$XFP: A Metadata-Augmented Microscaling Data Format for Efficient Low-bit Quantization](https://arxiv.org/abs/2601.19213)
*Weiming Hu,Zihan Zhang,Haoyan Zhang,Chen Zhang,Cong Guo,Yu Feng,Tianchi Hu,Guanglin Li,Guipeng Hu,Junsong Wang,Jingwen Leng*

Main category: cs.AR

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Existing low-bit Microscaling (MX) formats, such as MXFP4, often suffer from substantial accuracy degradation due to the use of a shared scaling factor with the Power-of-Two format. In this work, we explore strategies that introduce minimal metadata to recover accuracy lost during quantization while maintaining high bit efficiency across a wide range of large language models. We propose a complete algorithm-hardware co-design based on flexible metadata, featuring an online quantization with simple encoding. To support the proposed method efficiently, we implement a lightweight hardware unit and integrate it into the accelerator. Evaluation results demonstrate that our method substantially narrows the accuracy gap, achieving on average a 70.63% reduction in accuracy loss compared to MXFP4 and a 37.30% reduction relative to the latest NVFP4 on LLM benchmarks. Furthermore, our design delivers up to 1.91$\times$ speedup and 1.75$\times$ energy savings over state-of-the-art accelerators. Our code is available at https://github.com/SJTU-ReArch-Group/M2XFP_ASPLOS26.

</details>


### [2] [GenPairX: A Hardware-Algorithm Co-Designed Accelerator for Paired-End Read Mapping](https://arxiv.org/abs/2601.19384)
*Julien Eudine,Chu Li,Zhuo Cheng,Renzo Andri,Can Firtina,Mohammad Sadrosadati,Nika Mansouri Ghiasi,Konstantina Koliogeorgi,Anirban Nag,Arash Tavakkol,Haiyu Mao,Onur Mutlu,Shai Bergman,Ji Zhang*

Main category: cs.AR

TL;DR: 提出GenPairX硬件-算法协同加速器，针对高通量基因组测序中的双端读段映射瓶颈，通过联合过滤算法和专用硬件设计提高能效比。


<details>
  <summary>Details</summary>
Motivation: 现有读段映射方案存在性能瓶颈，双端读段的独立处理导致过滤效率低，传统动态规划计算开销大。

Method: 开发新颖过滤算法（联合评估双端读段提升过滤效率）和轻量级比对算法替代动态规划；设计两套专用硬件机制支持算法执行。

Result: 相比CPU及加速器方案，单位功耗吞吐量提升1575倍和1.43倍，且精度无损。

Conclusion: GenPairX通过软硬件协同设计显著优化双端读段映射效率，为基因组分析提供高性能低能耗解决方案。

Abstract: Genome sequencing has become a central focus in computational biology. A genome study typically begins with sequencing, which produces millions to billions of short DNA fragments known as reads. Read mapping aligns these reads to a reference genome. Read mapping for short reads comes in two forms: single-end and paired-end, with the latter being more prevalent due to its higher accuracy and support for advanced analysis. Read mapping remains a major performance bottleneck in genome analysis due to expensive dynamic programming. Prior efforts have attempted to mitigate this cost by employing filters to identify and potentially discard computationally expensive matches and leveraging hardware accelerators to speed up the computations. While partially effective, these approaches have limitations. In particular, existing filters are often ineffective for paired-end reads, as they evaluate each read independently and exhibit relatively low filtering ratios. In this work, we propose GenPairX, a hardware-algorithm co-designed accelerator that efficiently minimizes the computational load of paired-end read mapping while enhancing the throughput of memory-intensive operations. GenPairX introduces: (1) a novel filtering algorithm that jointly considers both reads in a pair to improve filtering effectiveness, and a lightweight alignment algorithm to replace most of the computationally expensive dynamic programming operations, and (2) two specialized hardware mechanisms to support the proposed algorithms. Our evaluations show that GenPairX delivers substantial performance improvements over state-of-the-art solutions, achieving 1575x and 1.43x higher throughput per watt compared to leading CPU-based and accelerator-based read mappers, respectively, all without compromising accuracy.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [3] [Reducing False Positives in Static Bug Detection with LLMs: An Empirical Study in Industry](https://arxiv.org/abs/2601.18844)
*Xueying Du,Jiayi Feng,Yi Zou,Wei Xu,Jie Ma,Wei Zhang,Sisi Liu,Xin Peng,Yiling Lou*

Main category: cs.SE

TL;DR: 静态分析工具误报率高导致效率低下，本研究在腾讯企业环境中验证LLM技术可高效消除94-98%误报并大幅降低成本。


<details>
  <summary>Details</summary>
Motivation: 当前静态分析工具在大型企业系统中误报率过高，需大量人工审查，而LLM技术在开源环境中虽有效，其实际工业应用效果不明，亟需实证验证。

Method: 利用腾讯定制化SAT工具在广告营销软件中构建433个警报数据集（含328误报/105真实），结合开发者访谈与数据分析，评估多种LLM误 سما篓减少技术。

Result: LLM混合技术可消除94-98%误报（召回率高），每个警报成本降至2.1-109.5秒和$0.0011-0.12美元，相较人工审查效率提升数个量级，但工业场景仍存局限性。

Conclusion: LLM技术具有工业级误报削减潜力与成本效益，为提升企业代码审查效率提供新方案，但需进一步解决实际场景限制。

Abstract: Static analysis tools (SATs) are widely adopted in both academia and industry for improving software quality, yet their practical use is often hindered by high false positive rates, especially in large-scale enterprise systems. These false alarms demand substantial manual inspection, creating severe inefficiencies in industrial code review. While recent work has demonstrated the potential of large language models (LLMs) for false alarm reduction on open-source benchmarks, their effectiveness in real-world enterprise settings remains unclear. To bridge this gap, we conduct the first comprehensive empirical study of diverse LLM-based false alarm reduction techniques in an industrial context at Tencent, one of the largest IT companies in China. Using data from Tencent's enterprise-customized SAT on its large-scale Advertising and Marketing Services software, we construct a dataset of 433 alarms (328 false positives, 105 true positives) covering three common bug types. Through interviewing developers and analyzing the data, our results highlight the prevalence of false positives, which wastes substantial manual effort (e.g., 10-20 minutes of manual inspection per alarm). Meanwhile, our results show the huge potential of LLMs for reducing false alarms in industrial settings (e.g., hybrid techniques of LLM and static analysis eliminate 94-98% of false positives with high recall). Furthermore, LLM-based techniques are cost-effective, with per-alarm costs as low as 2.1-109.5 seconds and $0.0011-$0.12, representing orders-of-magnitude savings compared to manual review. Finally, our case analysis further identifies key limitations of LLM-based false alarm reduction in industrial settings.

</details>


### [4] [MulVul: Retrieval-augmented Multi-Agent Code Vulnerability Detection via Cross-Model Prompt Evolution](https://arxiv.org/abs/2601.18847)
*Zihan Wu,Jie Xu,Yun Peng,Chun Yong Chong,Xiaohua Jia*

Main category: cs.SE

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Large Language Models (LLMs) struggle to automate real-world vulnerability detection due to two key limitations: the heterogeneity of vulnerability patterns undermines the effectiveness of a single unified model, and manual prompt engineering for massive weakness categories is unscalable.
  To address these challenges, we propose \textbf{MulVul}, a retrieval-augmented multi-agent framework designed for precise and broad-coverage vulnerability detection. MulVul adopts a coarse-to-fine strategy: a \emph{Router} agent first predicts the top-$k$ coarse categories and then forwards the input to specialized \emph{Detector} agents, which identify the exact vulnerability types. Both agents are equipped with retrieval tools to actively source evidence from vulnerability knowledge bases to mitigate hallucinations.
  Crucially, to automate the generation of specialized prompts, we design \emph{Cross-Model Prompt Evolution}, a prompt optimization mechanism where a generator LLM iteratively refines candidate prompts while a distinct executor LLM validates their effectiveness. This decoupling mitigates the self-correction bias inherent in single-model optimization.
  Evaluated on 130 CWE types, MulVul achieves 34.79\% Macro-F1, outperforming the best baseline by 41.5\%. Ablation studies validate cross-model prompt evolution, which boosts performance by 51.6\% over manual prompts by effectively handling diverse vulnerability patterns.

</details>


### [5] [Towards Safety-Compliant Transformer Architectures for Automotive Systems](https://arxiv.org/abs/2601.18850)
*Sven Kirchner,Nils Purschke,Chengdong Wu,Alois Knoll*

Main category: cs.SE

TL;DR: 提出一种Transformer在安全关键汽车系统中的集成框架，利用多模态传感器冗余提升容错性与鲁棒性，支持自动驾驶AI系统的安全认证。


<details>
  <summary>Details</summary>
Motivation: Transformer架构在视觉和语言任务中表现优异，但应用于汽车等安全关键领域存在可靠性挑战，需通过传感器多样性与冗余机制解决故障容忍问题。

Method: 设计多 ангольных编码器架构——各模态独立编码后，特征融合至共享潜在空间；当单一模态失效时，系统仍可维持运行并保证场景理解一致性。

Result: 在表征层嵌入冗余与多样性，实现多模态数据的有效融合，弥合深度学习与功能安全实践间的鸿沟，确保故障条件下的连续性操作。

Conclusion: 该框架为构建可认证的自动驾驶AI系统铺平道路，推动安全关键场景中Transformer技术的实际应用。

Abstract: Transformer-based architectures have shown remarkable performance in vision and language tasks but pose unique challenges for safety-critical applications. This paper presents a conceptual framework for integrating Transformers into automotive systems from a safety perspective. We outline how multimodal Foundation Models can leverage sensor diversity and redundancy to improve fault tolerance and robustness. Our proposed architecture combines multiple independent modality-specific encoders that fuse their representations into a shared latent space, supporting fail-operational behavior if one modality degrades. We demonstrate how different input modalities could be fused in order to maintain consistent scene understanding. By structurally embedding redundancy and diversity at the representational level, this approach bridges the gap between modern deep learning and established functional safety practices, paving the way for certifiable AI systems in autonomous driving.

</details>


### [6] [Tricky$^2$: Towards a Benchmark for Evaluating Human and LLM Error Interactions](https://arxiv.org/abs/2601.18949)
*Cole Granger,Dipin Khati,Daniel Rodriguez-Cardenas,Denys Poshyvanyk*

Main category: cs.SE

TL;DR: 本研究构建了Tricky²混合数据集，用于分析人类开发者和大型语言模型（LLM）在代码中引入错误的相互作用，支持错误分类、定位和修复任务的评估。


<details>
  <summary>Details</summary>
Motivation: 动机是LLM频繁集成到软件开发中时引入与人类错误不同的逻辑或数据误用错误，但两者如何互动缺乏研究，需要构建专用数据集填补空白。

Method: 方法：采用分类引导的提示框架，在TrickyBugs语料基础上注入GPT-5和OpenAI-oss-20b生成的错误，保留原始人类缺陷与程序结构，形成仅人、仅LLM和人+LLM的混合语料。

Result: 结果：数据集涵盖C++、Python和Java程序，支持分析混合错误行为、多缺陷修复鲁棒性和混合代码可靠性，并通过小规模基准评估展示了应用潜力。

Conclusion: 结论：Tricky²数据集为研究和工具开发提供了基准，有助于深入理解人机混合编码中的错误动态和修复挑战。

Abstract: Large language models (LLMs) are increasingly integrated into software development workflows, yet they often introduce subtle logic or data-misuse errors that differ from human bugs. To study how these two error types interact, we construct Tricky$^2$, a hybrid dataset that augments the existing TrickyBugs corpus of human-written defects with errors injected by both GPT-5 and OpenAI-oss-20b across C++, Python, and Java programs. Our approach uses a taxonomy-guided prompting framework to generate machine-originated bugs while preserving original human defects and program structure. The resulting corpus spans human-only, LLM-only, and human+LLM splits, enabling analysis of mixed-origin error behavior, multi-bug repair robustness, and reliability in hybrid human-machine code. This paper outlines the dataset construction pipeline and illustrates its use through small-scale baseline evaluations of classification, localization, and repair tasks.

</details>


### [7] [The Opaque Pointer Design Pattern in Python: Towards a Pythonic PIMPL for Modularity, Encapsulation, and Stability](https://arxiv.org/abs/2601.19065)
*Antonios Saravanos,John Pazarzis,Stavros Zervoudakis,Dongnanzi Zheng*

Main category: cs.SE

TL;DR: 本文提出一种Python版本PIMPL惯用语，通过不透明委托模式保护公共API稳定性，支持依赖隔离和重构。


<details>
  <summary>Details</summary>
Motivation: Python库维护时，用户可能依赖未公开的内部实现，导致重构风险和维护困难，需改进封装以提升长期可持续性。

Method: 引入Pythonic的委托模式：小型公共对象将行为委托至内部实现对象，并与现有封装技术（如模块间接、外观对象）进行整合和分析。

Result: 成功在Python代码库中隔离重依赖、支持懒导入和运行时后端选择，无需更改公共API。

Conclusion: 方法有益于减少风险并提供灵活性，但需权衡开销；论文提供实践指南，建议应用于大型长期库中。

Abstract: Python libraries often need to maintain a stable public API even as internal implementations evolve, gain new backends, or depend on heavy optional libraries. In Python, where internal objects are easy to inspect and import, users can come to rely on "reachable internals" that were never intended to be public, making refactoring risky and slowing long-term maintenance. This paper revisits the pointer-to-implementation (PIMPL) idiom from C++ and reinterprets it as a Pythonic pattern of opaque delegation: a small public object (or module) that delegates its behavior to a separate implementation object treated as internal. We situate this pattern within a broader taxonomy of encapsulation techniques in Python, relate it to existing practices such as module-level indirection, facade objects, and backend dispatch, and identify PIMPL-like structures already used in the standard library and the scientific Python ecosystem. We then show how a Pythonic PIMPL can be used in existing codebases to isolate heavy dependencies, support lazy imports, and enable runtime selection of alternative backends without changing the public API. Finally, we discuss the benefits and trade-offs of the approach and offer practical guidance on when the pattern is appropriate and how to apply it in large, long-lived Python libraries.

</details>


### [8] [Dynamic Cogeneration of Bug Reproduction Test in Agentic Program Repair](https://arxiv.org/abs/2601.19066)
*Runxiang Cheng,Michele Tufano,José Cambronero,Renyao Wei,Sherry Shi,Grant Uy,Pat Rondon,Franjo Ivančić*

Main category: cs.SE

TL;DR: 本文研究在智能程序修复中协同生成修复和错误重现测试的策略，证明其在不降低效果的前提下可减少系统工程开销。


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: 评估不同协同策略在120个Google报告的bug上的效果，开发考虑测试变更的补丁选择器选择合格修复和测试，并分析协同伴失败主因。

Result: 协同策略可为同量bug生成错误重现测试，且修复生成率不受损，节省修复与测试单独流程的维护协调成本。

Conclusion: 协同生成提升信任和效率，减少大规模工程负担，推动智能修复系统优化。

Abstract: Bug Reproduction Tests (BRTs) have been used in many agentic Automated Program Repair (APR) systems, primarily for validating promising fixes and aiding fix generation. In practice, when developers submit a patch, they often implement the BRT alongside the fix. Our experience deploying agentic APR reveals that developers similarly desire a BRT within AI-generated patches to increase their confidence. However, canonical APR systems tend to generate BRTs and fixes separately, or focus on producing only the fix in the final patch. In this paper, we study agentic APR in the context of cogeneration, where the APR agent is instructed to generate both a fix and a BRT in the same patch. We evaluate the effectiveness of different cogeneration strategies on 120 human-reported bugs at Google and characterize different cogeneration strategies by their influence on APR agent behavior. We develop and evaluate patch selectors that account for test change information to select patches with plausible fixes (and plausible BRTs). Finally, we analyze the root causes of failed cogeneration trajectories. Importantly, we show that cogeneration allows the APR agent to generate BRTs for at least as many bugs as a dedicated BRT agent, without compromising the generation rate of plausible fixes, thereby reducing engineering effort in maintaining and coordinating separate generation pipelines for fix and BRT at scale.

</details>


### [9] [HalluJudge: A Reference-Free Hallucination Detection for Context Misalignment in Code Review Automation](https://arxiv.org/abs/2601.19072)
*Kla Tantithamthavorn,Hong Yi Lin,Patanamon Thongtanunam,Wachiraphan Charoenwet,Minwoo Jeong,Ming Wu*

Main category: cs.SE

TL;DR: 论文提出HalluJudge方法，通过上下文对齐策略检测大型语言模型（LLM）在代码审查中生成评论时的幻觉问题，以减少不接地气的评论。


<details>
  <summary>Details</summary>
Motivation: 动机是LLM在自动化代码审查时产生的幻觉评论与实际代码不符，阻碍了AI在实际工作流程中的采纳，需要一种高效、可扩展的无参考检测方法。

Method: 方法设计了HalluJudge工具，采用四种关键策略（如直接评估和结构化多分支推理Tree-of-Thoughts），评估生成的审查评论的上下文对齐情况。

Result: 在Atlassian企业级软件项目中评估显示，HalluJudge的F1分数为0.85，平均成本0.009美元；67%的评估结果与实际在线生产中开发者对LLM生成评论的偏好一致。

Conclusion: 结论表明HalluJudge是一项成本效益高的保障措施，能减少开发者接触幻觉评论的风险，提升对AI辅助代码审查的信任度。

Abstract: Large Language models (LLMs) have shown strong capabilities in code review automation, such as review comment generation, yet they suffer from hallucinations -- where the generated review comments are ungrounded in the actual code -- poses a significant challenge to the adoption of LLMs in code review workflows. To address this, we explore effective and scalable methods for a hallucination detection in LLM-generated code review comments without the reference. In this work, we design HalluJudge that aims to assess the grounding of generated review comments based on the context alignment. HalluJudge includes four key strategies ranging from direct assessment to structured multi-branch reasoning (e.g., Tree-of-Thoughts). We conduct a comprehensive evaluation of these assessment strategies across Atlassian's enterprise-scale software projects to examine the effectiveness and cost-efficiency of HalluJudge. Furthermore, we analyze the alignment between HalluJudge's judgment and developer preference of the actual LLM-generated code review comments in the real-world production. Our results show that the hallucination assessment in HalluJudge is cost-effective with an F1 score of 0.85 and an average cost of $0.009. On average, 67% of the HalluJudge assessments are aligned with the developer preference of the actual LLM-generated review comments in the online production. Our results suggest that HalluJudge can serve as a practical safeguard to reduce developers' exposure to hallucinated comments, fostering trust in AI-assisted code reviews.

</details>


### [10] [Hybrid Fault-Driven Mutation Testing for Python](https://arxiv.org/abs/2601.19088)
*Saba Alimadadi,Golnaz Gharachorlu*

Main category: cs.SE

TL;DR: 研究者推出PyTation工具，引入七种针对Python反模式的新突变算子，通过混合静态和动态分析优化突变测试，减少等价突变，扩展故障覆盖范围。


<details>
  <summary>Details</summary>
Motivation: 现有突变测试技术在动态类型语言如Python中无法充分捕捉常见缺陷，需要专门的突变算子来补充通用工具。

Method: 基于Python常见反模式设计七种突变算子，结合静态和动态分析开发PyTation，减少等价突变体生成。

Result: 在13个开源Python应用上评估，PyTation生成独特突变体比例高，交叉杀死率和测试重叠率低，等价突变少，有效揭示测试套件不足。

Conclusion: PyTation的创新故障模型显著提升Python测试套件评估能力，其变异拓展性好且可行。

Abstract: Mutation testing is an effective technique for assessing the effectiveness of test suites by systematically injecting artificial faults into programs. However, existing mutation testing techniques fall short in capturing many types of common faults in dynamically typed languages like Python. In this paper, we introduce a novel set of seven mutation operators that are inspired by prevalent anti-patterns in Python programs, designed to complement the existing general-purpose operators and broaden the spectrum of simulated faults. We propose a mutation testing technique that utilizes a hybrid of static and dynamic analyses to mutate Python programs based on these operators while minimizing equivalent mutants. We implement our approach in a tool called PyTation and evaluate it on 13 open-source Python applications. Our results show that PyTation generates mutants that complement those from general-purpose tools, exhibiting distinct behaviour under test execution and uncovering inadequacies in high-coverage test suites. We further demonstrate that PyTation produces a high proportion of unique mutants, a low cross-kill rate, and a low test overlap ratio relative to baseline tools, highlighting its novel fault model. PyTation also incurs few equivalent mutants, aided by dynamic analysis heuristics.

</details>


### [11] [Detecting and Correcting Hallucinations in LLM-Generated Code via Deterministic AST Analysis](https://arxiv.org/abs/2601.19106)
*Dipin Khati,Daniel Rodriguez-Cardenas,Paul Pantzer,Denys Poshyvanyk*

Main category: cs.SE

TL;DR: 提出静态分析框架检测并自动修正代码生成中的知识冲突幻觉（KCHs），实现100%查准率与87.6%查全率，77%错误可自动修复。


<details>
  <summary>Details</summary>
Motivation: 现有概率性修复方法（如约束解码）对语义级代码幻觉（如无效API参数）不可靠，需确定性解决方案。

Method: 解析生成代码为AST，通过库自省动态构建知识库，基于确定性规则静态检测与修复API/标识符冲突。

Result: 在200个Python案例中：KCH检测F1值0.934（100%精度/87.6%召回率），77%识别错误成功自动修正。

Conclusion: 确定性后处理为概率修复的可行替代方案，为可信代码生成提供可靠路径。

Abstract: Large Language Models (LLMs) for code generation boost productivity but frequently introduce Knowledge Conflicting Hallucinations (KCHs), subtle, semantic errors, such as non-existent API parameters, that evade linters and cause runtime failures. Existing mitigations like constrained decoding or non-deterministic LLM-in-the-loop repair are often unreliable for these errors. This paper investigates whether a deterministic, static-analysis framework can reliably detect \textit{and} auto-correct KCHs. We propose a post-processing framework that parses generated code into an Abstract Syntax Tree (AST) and validates it against a dynamically-generated Knowledge Base (KB) built via library introspection. This non-executing approach uses deterministic rules to find and fix both API and identifier-level conflicts. On a manually-curated dataset of 200 Python snippets, our framework detected KCHs with 100\% precision and 87.6\% recall (0.934 F1-score), and successfully auto-corrected 77.0\% of all identified hallucinations. Our findings demonstrate that this deterministic post-processing approach is a viable and reliable alternative to probabilistic repair, offering a clear path toward trustworthy code generation.

</details>


### [12] [The Promise and Reality of Continuous Integration Caching: An Empirical Study of Travis CI Builds](https://arxiv.org/abs/2601.19146)
*Taher A. Ghaleb,Daniel Alencar da Costa,Ying Zou*

Main category: cs.SE

TL;DR: 研究实证分析CI缓存采用情况：仅30%项目启用，采用与项目成熟度相关；缺乏意识是主要障碍，缓存维护复杂且常出错。


<details>
  <summary>Details</summary>
Motivation: CI构建时间长影响ammed效率，缓存可加速但采用程度及挑战未知，需揭示实践障碍

Method: 大规模实证研究Travis CI，分析513,384次构建和1,279个GitHub项目；提交拉取请求到非采用项目并收集开发者反馈

Result: 30%项目启用缓存且早期采用受项目成熟度(如依赖数、提交量)影响；47%拉取请求被接受表明意识不足为首因；24%项目进行维护，97%构建会上传缓存，33%存在陈旧缓存

Conclusion: CI缓存非万能，需持续维护且复杂性超预期

Abstract: Continuous Integration (CI) provides early feedback by automatically building software, but long build durations can hinder developer productivity. CI services offer caching mechanisms to speed up builds by reusing infrequently changing artifacts, yet little is known about how caching is adopted in practice and what challenges it entails. In this paper, we conduct a large-scale empirical study of CI caching in Travis CI, analyzing 513,384 builds from 1,279 GitHub projects. We find that only 30% of projects adopt CI caching, and early adoption is strongly associated with project maturity, such as more dependencies, more commits, and longer CI lifespans. To understand why many projects do not adopt caching, we submitted pull requests enabling caching in non-adopting projects, and nearly half were accepted or merged. Developer feedback suggests that non- or late adoption mainly stems from limited awareness of CI caching support. We also examine cache maintenance and identify five common activities, performed by 24% of cache-enabled projects. Although one-third of projects see substantial build-time reductions, cache uploads occur in 97% of builds, and 33% of projects contain stale cached artifacts. Finally, our analysis of reported caching issues shows developers mainly struggle with corrupted or outdated caches or request broader caching features. Overall, CI caching does not help all projects, needs ongoing maintenance, and is more complex in practice than many developers expect.

</details>


### [13] [LLM-based Vulnerability Detection at Project Scale: An Empirical Study](https://arxiv.org/abs/2601.19239)
*Fengjie Li,Jiajun Jiang,Dongchi Chen,Yingfei Xiong*

Main category: cs.SE

TL;DR: 本研究首次在项目规模上对比LLM漏洞探测器与传统静态分析工具：LLM工具召回率低但能发现更独特漏洞；实际应用中二者均误报率高，LLM额外面临推理浅层和计算成本巨大问题。


<details>
  <summary>Details</summary>
Motivation: 旨在填补现有研究空白，通过实证评估项目规模下LLM与传统漏洞探测器的检测能力、实际可用性与效率，推动改进工具鲁棒性与实用性。

Method: 使用两种方法评估5种LLM工具和2种传统工具：内部基准测试222个真实漏洞（C/C++和Java）以衡量检测力；分析24个开源项目的385个警告手动检查实用性和故障根源。

Result: LLM工具基准测试召回低但发现更多独特漏洞；实践项目中二者警报量大但误报率高，实用性受限，故障主因是浅层过程间推理和源/汇对误识别；LLM有独特故障且计算成本极高（数十万至数亿令牌，运行数小时至数天）。

Conclusion: 当前LLM探测器在鲁棒性、可靠性和扩展性上存在严重缺陷，研究总结未来方向以开发更高效实用的漏洞检测工具。

Abstract: In this paper, we present the first comprehensive empirical study of specialized LLM-based detectors and compare them with traditional static analyzers at the project scale. Specifically, our study evaluates five latest and representative LLM-based methods and two traditional tools using: 1) an in-house benchmark of 222 known real-world vulnerabilities (C/C++ and Java) to assess detection capability, and 2) 24 active open-source projects, where we manually inspected 385 warnings to assess their practical usability and underlying root causes of failures. Our evaluation yields three key findings: First, while LLM-based detectors exhibit low recall on the in-house benchmark, they still uncover more unique vulnerabilities than traditional tools. Second, in open-source projects, both LLM-based and traditional tools generate substantial warnings but suffer from very high false discovery rates, hindering practical use. Our manual analysis further reveals shallow interprocedural reasoning and misidentified source/sink pairs as primary failure causes, with LLM-based tools exhibiting additional unique failures. Finally, LLM-based methods incurs substantial computational costs-hundreds of thousands to hundreds of millions of tokens and multi-hour to multi-day runtimes. Overall, our findings underscore critical limitations in the robustness, reliability, and scalability of current LLM-based detectors. We ultimately summarize a set of implications for future research toward more effective and practical project-scale vulnerability detection.

</details>


### [14] ["ENERGY STAR" LLM-Enabled Software Engineering Tools](https://arxiv.org/abs/2601.19260)
*Himon Thakur,Armin Moin*

Main category: cs.SE

TL;DR: 本文研究AI增强型软件工具（如IDE）的能量效率，特别针对基于LLM的代码生成。通过结合RAG和PETs技术，在多模型架构中测量能耗和时间，证明方法可行。


<details>
  <summary>Details</summary>
Motivation: 随着AI默认集成到软件开发工具中，可能导致能源消耗激增，研究旨在提高LLM代码生成的能效和质量，以优化软件开发生命周期。

Method: 结合检索增强生成(RAG)和提示工程技术(PETs)，测量不同参数规模（125M至7B）的LLM（如GPT-2、CodeLlama）的实时能耗与推理时间，使用综合框架验证ර。

Result: 在多种模型上成功验证核心想法，证明RAG结合PETs能提升代码生成质量和能效，并为未来深入分析提供了概念证明。

Conclusion: 该框架ånå为降低AI工具能耗提供了可行路径，强调持续优化LLM架构的重要性，以适应绿色软件工程趋势。

Abstract: The discussion around AI-Engineering, that is, Software Engineering (SE) for AI-enabled Systems, cannot ignore a crucial class of software systems that are increasingly becoming AI-enhanced: Those used to enable or support the SE process, such as Computer-Aided SE (CASE) tools and Integrated Development Environments (IDEs). In this paper, we study the energy efficiency of these systems. As AI becomes seamlessly available in these tools and, in many cases, is active by default, we are entering a new era with significant implications for energy consumption patterns throughout the Software Development Lifecycle (SDLC). We focus on advanced Machine Learning (ML) capabilities provided by Large Language Models (LLMs). Our proposed approach combines Retrieval-Augmented Generation (RAG) with Prompt Engineering Techniques (PETs) to enhance both the quality and energy efficiency of LLM-based code generation. We present a comprehensive framework that measures real-time energy consumption and inference time across diverse model architectures ranging from 125M to 7B parameters, including GPT-2, CodeLlama, Qwen 2.5, and DeepSeek Coder. These LLMs, chosen for practical reasons, are sufficient to validate the core ideas and provide a proof of concept for more in-depth future analysis.

</details>


### [15] [Whitespaces Don't Lie: Feature-Driven and Embedding-Based Approaches for Detecting Machine-Generated Code](https://arxiv.org/abs/2601.19264)
*Syed Mehedi Hasan Nirob,Shamim Ehsan,Moqsadur Rahman,Summit Haque*

Main category: cs.SE

TL;DR: 论文通过比较基于代码特征的轻量探测器和基于嵌入的CodeBERT探测器，展示出两者在区分人写与AI生成代码上性能优异，揭示了可解释性与泛化能力间的权衡。


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: 使用60万个样本数据集，对比基于代码缩进等风格特征的轻量探测器和CodeBERT预训练编码器的嵌入探测器。

Result: 持征模型ROC-AUC 0.995하면 PR-AUC 0.995하면 F1 0.971，嵌入模型ROC-AUC 0.994᳝ PR-AUC 0.994᳝ F1 0.965；缩进和空格特征最具区分力，嵌入捕获深层语义且精度稍高。

Conclusion: 该研究强调了探测器的可解释性与泛化能力平衡，为学术和工业场景部署代码来源检测提供实用指导。

Abstract: Large language models (LLMs) have made it remarkably easy to synthesize plausible source code from natural language prompts. While this accelerates software development and supports learning, it also raises new risks for academic integrity, authorship attribution, and responsible AI use. This paper investigates the problem of distinguishing human-written from machine-generated code by comparing two complementary approaches: feature-based detectors built from lightweight, interpretable stylometric and structural properties of code, and embedding-based detectors leveraging pretrained code encoders. Using a recent large-scale benchmark dataset of 600k human-written and AI-generated code samples, we find that feature-based models achieve strong performance (ROC-AUC 0.995, PR-AUC 0.995, F1 0.971), while embedding-based models with CodeBERT embeddings are also very competitive (ROC-AUC 0.994, PR-AUC 0.994, F1 0.965). Analysis shows that features tied to indentation and whitespace provide particularly discriminative cues, whereas embeddings capture deeper semantic patterns and yield slightly higher precision. These findings underscore the trade-offs between interpretability and generalization, offering practical guidance for deploying robust code-origin detection in academic and industrial contexts.

</details>


### [16] [Modeling Sampling Workflows for Code Repositories](https://arxiv.org/abs/2601.19316)
*Romain Lefeuvre,Maïwenn Le Goasteller,Jessie Galasso,Benoit Combemale,Quentin Perez,Houari Sahraoui*

Main category: cs.SE

TL;DR: 提出用于描述代码仓库抽样策略的领域特定语言（DSL），通过可组合算子提升研究结果的可推广性


<details>
  <summary>Details</summary>
Motivation: 软件工程中抽样策略设计常被低估，存在抽样代表性不足及结果可推广性评估困难两大挑战

Method: 开发基于Python的流式API实现DSL，支持通过统计指标对抽样工作流进行代表性验证

Result: 案例研究证明该DSL能准确建模文献中的抽样策略，并实现可推广性定量分析

Conclusion: DSL框架为软件工程实证研究提供了系统化的抽样策略描述与以下の影响评估方案

Abstract: Empirical software engineering research often depends on datasets of code repository artifacts, where sampling strategies are employed to enable large-scale analyses. The design and evaluation of these strategies are critical, as they directly influence the generalizability of research findings. However, sampling remains an underestimated aspect in software engineering research: we identify two main challenges related to (1) the design and representativeness of sampling approaches, and (2) the ability to reason about the implications of sampling decisions on generalizability. To address these challenges, we propose a Domain-Specific Language (DSL) to explicitly describe complex sampling strategies through composable sampling operators. This formalism supports both the specification and the reasoning about the generalizability of results based on the applied sampling strategies. We implement the DSL as a Python-based fluent API, and demonstrate how it facilitates representativeness reasoning using statistical indicators extracted from sampling workflows. We validate our approach through a case study of MSR papers involving code repository sampling. Our results show that the DSL can model the sampling strategies reported in recent literature.

</details>


### [17] [High-quality data augmentation for code comment classification](https://arxiv.org/abs/2601.19383)
*Thomas Borsani,Andrea Rosani,Giuseppe Di Fatta*

Main category: cs.SE

TL;DR: 本研究引入Q-SYNTH技术，通过合成过采样与增强提升代码注释分类性能，分类器改进2.56%。


<details>
  <summary>Details</summary>
Motivation: 现有注释分类数据集受手动标注限制，存在规模不足和类别不平衡问题，无法准确反映真实代码库分布。

Method: 提出基于高质量数据生成的合成过采样和增强技术Q-SYNTH，应用于NLBSE'26数据集增强。

Result: Q-SYNTH使基础分类器性能提升了2.56%。

Conclusion: Q-SYNTH方法有效提升了数据集质量和分类性能，显示出应用潜力。

Abstract: Code comments serve a crucial role in software development for documenting functionality, clarifying design choices, and assisting with issue tracking. They capture developers' insights about the surrounding source code, serving as an essential resource for both human comprehension and automated analysis. Nevertheless, since comments are in natural language, they present challenges for machine-based code understanding. To address this, recent studies have applied natural language processing (NLP) and deep learning techniques to classify comments according to developers' intentions. However, existing datasets for this task suffer from size limitations and class imbalance, as they rely on manual annotations and may not accurately represent the distribution of comments in real-world codebases. To overcome this issue, we introduce new synthetic oversampling and augmentation techniques based on high-quality data generation to enhance the NLBSE'26 challenge datasets. Our Synthetic Quality Oversampling Technique and Augmentation Technique (Q-SYNTH) yield promising results, improving the base classifier by $2.56\%$.

</details>


### [18] [Bridging the Socio-Emotional Gap: The Functional Dimension of Human-AI Collaboration for Software Engineering](https://arxiv.org/abs/2601.19387)
*Lekshmi Murali Rani,Richard Berntsson Svensson,Robert Feldt*

Main category: cs.SE

TL;DR: 研究探索软件工程师与GenAI协作中的社会情感智能缺口，发现从业者视AI为智力队友而非社交伙伴，主张通过功能性设计而非复制人类情感特征提升协作效率。


<details>
  <summary>Details</summary>
Motivation: 澄清AI系统中社会情感智能在人机协作中的作用，辨识当前协作中的潜在功能差距。

Method: 对10名软件从业者进行半结构化访谈，分析与人及AI队友合作时的社会情感智能期望与AI能力需求。

Result: 从业者认为AI合作存在功能性缺口（如无法协商责任或适应情境），但功能性等效技术（如内部认知和自适应学习）可达到类似人类情感协作成果。

Conclusion: 人机协作应聚焦功能性对齐而非模仿人类情感特征，以重新定义合作框架并优化效能。

Abstract: As GenAI models are adopted to support software engineers and their development teams, understanding effective human-AI collaboration (HAIC) is increasingly important. Socio-emotional intelligence (SEI) enhances collaboration among human teammates, but its role in HAIC remains unclear. Current AI systems lack SEI capabilities that humans bring to teamwork, creating a potential gap in collaborative dynamics. In this study, we investigate how software practitioners perceive the socio-emotional gap in HAIC and what capabilities AI systems require for effective collaboration. Through semi-structured interviews with 10 practitioners, we examine how they think about collaborating with human versus AI teammates, focusing on their SEI expectations and the AI capabilities they envision. Results indicate that practitioners currently view AI models as intellectual teammates rather than social partners and expect fewer SEI attributes from them than from human teammates. However, they see the socio-emotional gap not as AIs failure to exhibit SEI traits, but as a functional gap in collaborative capabilities (AIs inability to negotiate responsibilities, adapt contextually, or maintain sustained partnerships). We introduce the concept of functional equivalents: technical capabilities (internal cognition, contextual intelligence, adaptive learning, and collaborative intelligence) that achieve collaborative outcomes comparable to human SEI attributes. Our findings suggest that effective collaboration with AI for SE tasks may benefit from functional design rather than replicating human SEI traits for SE tasks, thereby redefining collaboration as functional alignment.

</details>


### [19] [From Scattered to Structured: A Vision for Automating Architectural Knowledge Management](https://arxiv.org/abs/2601.19548)
*Jan Keim,Angelika Kaplan*

Main category: cs.SE

TL;DR: 提出自动化知识管理管道，从异构软件制品中提取架构知识，解决不一致性问题，构建结构化知识库支持架构活动。


<details>
  <summary>Details</summary>
Motivation: 软件架构知识分散在各异制品中导致访问困难，系统演进引发不一致性，造成架构腐蚀和维护障碍。

Method: 开发针对性提取器、统一知识表示模式、实现一致性检查机制，集成检索增强生成技术实现对话式访问。

Result: 构建知识库可实现架构一致性检查、变更影响分析，并通过自然语言问答提升知识可及性。

Conclusion: 该方案系统化整合多源架构知识，增强软件维护效率与知识可用性，推动架构智能管理发展。

Abstract: Software architecture is inherently knowledge-centric. The architectural knowledge is distributed across heterogeneous software artifacts such as requirements documents, design diagrams, code, and documentation, making it difficult for developers to access and utilize this knowledge effectively. Moreover, as systems evolve, inconsistencies frequently emerge between these artifacts, leading to architectural erosion and impeding maintenance activities. We envision an automated pipeline that systematically extracts architectural knowledge from diverse artifacts, links them, identifies and resolves inconsistencies, and consolidates this knowledge into a structured knowledge base. This knowledge base enables critical activities such as architecture conformance checking and change impact analysis, while supporting natural language question-answering to improve access to architectural knowledge. To realize this vision, we plan to develop specialized extractors for different artifact types, design a unified knowledge representation schema, implement consistency checking mechanisms, and integrate retrieval-augmented generation techniques for conversational knowledge access.

</details>


### [20] [Toward Architecture-Aware Evaluation Metrics for LLM Agents](https://arxiv.org/abs/2601.19583)
*Débora Souza,Patrícia Machado*

Main category: cs.SE

TL;DR: 提出轻量级架构感知方法，将LLM智能体组件与行为、评估指标关联，解决现有评估体系碎片化问题。


<details>
  <summary>Details</summary>
Motivation: 现有LLM智能体评估集中于模型层面，忽视架构组件（如规划器、记忆模块）对行为的影响，导致诊断能力受限。

Method: 设计架构驱动的评估框架，建立组件-行为-指标的三元映射，明确评估目标与原理。

Result: 通过真实场景智能体案例验证了框架的有效性，实现更精准、透明、可操作的评估方案。

Conclusion: 该方法为LLM智能体提供了可诊断的评估范式，推动软件开发任务的系统化评测进程。

Abstract: LLM-based agents are becoming central to software engineering tasks, yet evaluating them remains fragmented and largely model-centric. Existing studies overlook how architectural components, such as planners, memory, and tool routers, shape agent behavior, limiting diagnostic power. We propose a lightweight, architecture-informed approach that links agent components to their observable behaviors and to the metrics capable of evaluating them. Our method clarifies what to measure and why, and we illustrate its application through real world agents, enabling more targeted, transparent, and actionable evaluation of LLM-based agents.

</details>


### [21] [The Competence Crisis: A Design Fiction on AI-Assisted Research in Software Engineering](https://arxiv.org/abs/2601.19628)
*Mairieli Wessel,Daniel Feitosa,Sangeeth Kochanthara*

Main category: cs.SE

TL;DR: 该愿景论文探讨生成式AI工具和发文压力对软件工程研究的影响，通过设计虚构方法分析潜在风险，提出对专业能力衰退和研究伦理的警示。


<details>
  <summary>Details</summary>
Motivation: 解决生成式AI普及和学术压力加剧背景下软件工程研究领域可能出现的技能退化、责任归属及学术信任危机等问题。

Method: 使用设计虚构作为方法论框架，基于社群调查构建近未来研究场景的推测性案例，将虚构情景作为分析工具而非预测模型。

Result: 揭示自动化工具可能削弱领域知识掌握能力、验证流程和导师培养机制，通过设定警示性案例引发对学术素养定义的反思。

Conclusion: 呼吁软件工程社群主动探讨未来研究中专业能力标准、责任分配机制与学习支持体系的改革路径。

Abstract: Rising publication pressure and the routine use of generative AI tools are reshaping how software engineering research is produced, assessed, and taught. While these developments promise efficiency, they also raise concerns about skill degradation, responsibility, and trust in scholarly outputs. This vision paper employs Design Fiction as a methodological lens to examine how such concerns might materialise if current practices persist. Drawing on themes reported in a recent community survey, we construct a speculative artifact situated in a near future research setting. The fiction is used as an analytical device rather than a forecast, enabling reflection on how automated assistance might impede domain knowledge competence, verification, and mentoring practices. By presenting an intentionally unsettling scenario, the paper invites discussion on how the software engineering research community in the future will define proficiency, allocate responsibility, and support learning.

</details>


### [22] [Using LLMs to Evaluate Architecture Documents: Results from a Digital Marketplace Environment](https://arxiv.org/abs/2601.19693)
*Frank Elberzhager,Matthias Gerbershagen,Joshua Ginkel*

Main category: cs.SE

TL;DR: 研究探讨了LLM在软件架构文档评估中的应用效果，发现文档质量越高，LLM与专家评估一致性越强，但存在不一致性需进一步研究。


<details>
  <summary>Details</summary>
Motivation: 生成式AI在软件工程中应用日益广泛，但LLM的实际效益不明确。本研究旨在探索LLM如何支持软件架构师改进架构文档质量。

Method: 在开发数字市场的研究项目中，使用多种LLM评估架构文档质量，并与人类软件架构师的评估结果进行对比分析。

Result: 架构文档质量显著影响LLM评估效果：文档质量越高，LLM与人类专家评估结果的一致性越强。

Conclusion: LLM在架构任务中潜力显著，但因结果存在不一致性，需深入分析后才能推广。

Abstract: Generative AI plays an increasing role during software engineering activities to make them, e.g., more efficient or provide better quality. However, it is often unclear how much benefit LLMs really provide. We concentrate on software architects and investigated how an LLM-supported evaluation of architecture documents can support software architects to improve such artefacts. In the context of a research project where a digital marketplace is developed and digital solutions should be analyzed, we used different LLMs to analyze the quality of architecture documents and compared the results with evaluations from software architects. We found out that the quality of the artifact has a strong influence on the quality of the LLM, i.e., the better the quality of the architecture document was, the more consistent were the LLM-based evaluation and the human expert evaluation. While using LLMs in this architecture task is promising, our results showed inconsistencies that need further analyses before generalizing them.

</details>


### [23] [AlignCoder: Aligning Retrieval with Target Intent for Repository-Level Code Completion](https://arxiv.org/abs/2601.19697)
*Tianyue Jiang,Yanli Wang,Yanlin Wang,Daya Guo,Ensheng Shi,Yuchi Ma,Jiachi Chen,Zibin Zheng*

Main category: cs.SE

TL;DR: AlignC第三个Coder：基于查询增强与强化学习的仓库级代码补全框架，提高代码LLMs的而他能力


<details>
  <summary>Details</summary>
Motivation: 现有代码大模型在库级代码补全中因无法充分理解仓库上下文而受限，传统检索增强生成存在两个问题：(1)检索过程中查询与目标代码语义不对齐；(2)检索方法无法有效利用推理信息。

Method: 提出AlignCoder框架：(1)查询增强机制通过多候选补全构建增强查询，桥接初始查询与目标代码的语义鸿沟；(2)使用强化学习训练AlignRetriever，使其能利用增强查询中的推理信息实现精准检索。

Result: 在CrossCodeEval和RepoEval数据集上测试5种基线代码LLMs，EM分数较基线提升18.1%，并在多种编程语言中展现出高泛化性

Conclusion: 该框架显著提升了库级代码补全效果，通过创新检索对齐机制解决了现有关键瓶颈

Abstract: Repository-level code completion remains a challenging task for existing code large language models (code LLMs) due to their limited understanding of repository-specific context and domain knowledge. While retrieval-augmented generation (RAG) approaches have shown promise by retrieving relevant code snippets as cross-file context, they suffer from two fundamental problems: misalignment between the query and the target code in the retrieval process, and the inability of existing retrieval methods to effectively utilize the inference information. To address these challenges, we propose AlignCoder, a repository-level code completion framework that introduces a query enhancement mechanism and a reinforcement learning based retriever training method. Our approach generates multiple candidate completions to construct an enhanced query that bridges the semantic gap between the initial query and the target code. Additionally, we employ reinforcement learning to train an AlignRetriever that learns to leverage inference information in the enhanced query for more accurate retrieval. We evaluate AlignCoder on two widely-used benchmarks (CrossCodeEval and RepoEval) across five backbone code LLMs, demonstrating an 18.1% improvement in EM score compared to baselines on the CrossCodeEval benchmark. The results show that our framework achieves superior performance and exhibits high generalizability across various code LLMs and programming languages.

</details>


### [24] [Future of Software Engineering Research: The SIGSOFT Perspective](https://arxiv.org/abs/2601.19731)
*Massimiliano Di Penta,Kelly Blincoe,Marsha Chechik,Claire Le Goues,David Lo,Emerson Murphy-Hill,Thomas Zimmermann*

Main category: cs.SE

TL;DR: 随着软件工程会议规模扩大，成本和格式过时带来参与障碍；基于调查数据，建议ACM SIGSOFT通过提高资金透明度、试验混合海报展示和扩大对欠代表地区的推广来解决，以保持社区包容性和可及性。


<details>
  <summary>Details</summary>
Motivation: 软件工程会议的成本上升和过时格式阻碍研究者参与，威胁社区的包容性和全球多样性，这对SE领域的成功至关重要。

Method: 通过分析调查数据，识别并提出了可操作措施。

Result: 确定了具体行动：提高会议资助透明度、试行混合式海报展示、向代表性不足地区扩张推广范围。

Conclusion: 实施这些变革后，SIGSOFT可确保软件工程社区持续开放和包容，维护其全球多样性基础。

Abstract: As software engineering conferences grow in size, rising costs and outdated formats are creating barriers to participation for many researchers. These barriers threaten the inclusivity and global diversity that have contributed to the success of the SE community. Based on survey data, we identify concrete actions the ACM Special Interest Group on Software Engineering (SIGSOFT) can take to address these challenges, including improving transparency around conference funding, experimenting with hybrid poster presentations, and expanding outreach to underrepresented regions. By implementing these changes, SIGSOFT can help ensure the software engineering community remains accessible and welcoming.

</details>


### [25] [Assessing Task-based Chatbots: Snapshot and Curated Datasets for Dialogflow](https://arxiv.org/abs/2601.19787)
*Elena Masserini,Diego Clerissi,Daniela Micucci,Leonardo Mariani*

Main category: cs.SE

TL;DR: 本研究介绍了TOFU-D和COD数据集用于聊天机器人质量与安全的实证研究，初步评估发现测试覆盖不足和安全漏洞问题，强调需系统性研究。


<details>
  <summary>Details</summary>
Motivation: 由于缺乏大型精选数据集，聊天机器人质量和可靠性研究受限；意在填补这一空白。

Method: 构建TOFU-D数据集（1788个Dialogflow聊天机器人示例）和COD精选子集（185个已验证示例），并使用Botium测试框架和Bandit静态分析器进行初步评估。

Result: 评估揭示了多个聊天机器人在 reunion测试覆盖方面存在 gaps 并频繁出现安全漏洞。

Conclusion: 发现凸显了聊天机器人质量与安全领域需系统性、多平台研究的必要性。

Abstract: In recent years, chatbots have gained widespread adoption thanks to their ability to assist users at any time and across diverse domains. However, the lack of large-scale curated datasets limits research on their quality and reliability. This paper presents TOFU-D, a snapshot of 1,788 Dialogflow chatbots from GitHub, and COD, a curated subset of TOFU-D including 185 validated chatbots. The two datasets capture a wide range of domains, languages, and implementation patterns, offering a sound basis for empirical studies on chatbot quality and security. A preliminary assessment using the Botium testing framework and the Bandit static analyzer revealed gaps in test coverage and frequent security vulnerabilities in several chatbots, highlighting the need for systematic, multi-Platform research on chatbot quality and security.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [26] [Reimagining Peer Review Process Through Multi-Agent Mechanism Design](https://arxiv.org/abs/2601.19778)
*Ahmad Farooq,Kamran Iqbal*

Main category: cs.MA

TL;DR: 立场论文指出软件工程领域同行评审因提交量剧增、激励失调和审稿人倦怠陷入危机，提出采用多智能体强化学习设计三大计算化解决方案：信用制投稿经济、智能分配审阅及混合验证机制。


<details>
  <summary>Details</summary>
Motivation: 社区调查揭示同行评审系统已'崩溃'，归因于激增的投稿量、激励错位及审稿疲劳，属机制设计缺陷，亟需计算化干预以实现可持续发展。

Method: 将研究社区建模为随机多智能体系统，应用强化学习设计激励兼容协议。具体方案：1) 信用体系调控投稿经济 2) 多智能体优化审稿分配 3) 混合方法验证评审一致性。含威胁模型分析和分阶段试点指标。

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: The software engineering research community faces a systemic crisis: peer review is failing under growing submissions, misaligned incentives, and reviewer fatigue. Community surveys reveal that researchers perceive the process as "broken." This position paper argues that these dysfunctions are mechanism design failures amenable to computational solutions. We propose modeling the research community as a stochastic multi-agent system and applying multi-agent reinforcement learning to design incentive-compatible protocols. We outline three interventions: a credit-based submission economy, MARL-optimized reviewer assignment, and hybrid verification of review consistency. We present threat models, equity considerations, and phased pilot metrics. This vision charts a research agenda toward sustainable peer review.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [27] [Trustworthy Scheduling for Big Data Applications](https://arxiv.org/abs/2601.18983)
*Dimitrios Tomaras,Vana Kalogeraki,Dimitrios Gunopulos*

Main category: cs.DC

TL;DR: X-Sched是基于解释性技术和机器学习的中间件，为容器环境提供可行的资源配置指导，弥补调度决策的透明度不足。


<details>
  <summary>Details</summary>
Motivation: 现有调度器优化任务执行时间和资源利用，但缺少决策过程透明性或如何满足服务等级目标的明确指导。

Method: 利用反事实解释结合随机森林等机器学习模型，高效识别最优资源配置。

Result: 实验验证结果表明，该方法在真实环境中具有高效率、有益性和实用性。

Conclusion: X-Sched确保了任务执行为cion时满足性能目标，并为用户提供清晰的决策见解。

Abstract: Recent advances in modern containerized execution environments have resulted in substantial benefits in terms of elasticity and more efficient utilization of computing resources. Although existing schedulers strive to optimize performance metrics like task execution times and resource utilization, they provide limited transparency into their decision-making processes or the specific actions developers must take to meet Service Level Objectives (SLOs). In this work, we propose X-Sched, a middleware that uses explainability techniques to generate actionable guidance on resource configurations that makes task execution in containerized environments feasible, under resource and time constraints. X-Sched addresses this gap by integrating counterfactual explanations with advanced machine learning models, such as Random Forests, to efficiently identify optimal configurations. This approach not only ensures that tasks are executed in line with performance goals but also gives users clear, actionable insights into the rationale behind scheduling decisions. Our experimental results validated with data from real-world execution environments, illustrate the efficiency, benefits and practicality of our approach.

</details>


### [28] [Axe: A Simple Unified Layout Abstraction for Machine Learning Compilers](https://arxiv.org/abs/2601.19092)
*Bohan Hou,Hongyi Jin,Guanjie Wang,Jinqi Chen,Yaxing Cai,Lijie Yang,Zihao Ye,Yaoyao Ding,Ruihang Lai,Tianqi Chen*

Main category: cs.DC

TL;DR: 提出Axe Layout硬件感知抽象，统一跨设备与线程的张量映射，高效扩展深度学习负载。


<details>
  <summary>Details</summary>
Motivation: 解决规模化深度学习负载中，在设备网格、内存层次和异构加速器上协调放置数据与计算的挑战，避免手动优化的复杂性。

Method: 设计Axe Layout，通过命名轴将逻辑张量坐标映射到多轴物理空间，整合切片、分片、复制和偏移；开发了多粒度分布感知DSL和编译器，在单内核中结合线程局部控制与集合算子。

Result: 实验证明，该方法在GPU设备、多设备环境和加速器后端上，性能接近手动优化内核，显著提升效率。

Conclusion: 该统一框架实现了跨异构硬件的一致张量管理与性能优化，支持深度学习的可扩展性和高效部署。

Abstract: Scaling modern deep learning workloads demands coordinated placement of data and compute across device meshes, memory hierarchies, and heterogeneous accelerators. We present Axe Layout, a hardware-aware abstraction that maps logical tensor coordinates to a multi-axis physical space via named axes. Axe unifies tiling, sharding, replication, and offsets across inter-device distribution and on-device layouts, enabling collective primitives to be expressed consistently from device meshes to threads. Building on Axe, we design a multi-granularity, distribution-aware DSL and compiler that composes thread-local control with collective operators in a single kernel. Experiments show that our unified approach can bring performance close to hand-tuned kernels on across latest GPU devices and multi-device environments and accelerator backends.

</details>


### [29] [KUBEDIRECT: Unleashing the Full Power of the Cluster Manager for Serverless Computing](https://arxiv.org/abs/2601.19160)
*Sheng Qi,Zhiquan Zhang,Xuanzhe Liu,Xin Jin*

Main category: cs.DC

TL;DR: KUBEDIRECT优化Kubernetes集群管理器，通过公共狭窄瓶颈实现高速Faas扩展和兼容性保持。


<details>
  <summary>Details</summary>
Motivation: Kubernetes在Faas实例突发扩展时因API服务器消息传递成为瓶颈，现有方案需重置设计牺牲兼容性。

Method: 利用Faas平台的公共狭窄瓶颈绕过API服务器进行直接消息传递，采用层级写回缓存管理短暂状态以确保一致性与收敛性。

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: FaaS platforms rely on cluster managers like Kubernetes for resource management. Kubernetes is popular due to its state-centric APIs that decouple the control plane into modular controllers. However, to scale out a burst of FaaS instances, message passing becomes the primary bottleneck as controllers have to exchange extensive state through the API Server. Existing solutions opt for a clean-slate redesign of cluster managers, but at the expense of compatibility with existing ecosystem and substantial engineering effort.
  We present KUBEDIRECT, a Kubernetes-based cluster manager for FaaS. We find that there exists a common narrow waist across FaaS platform that allows us to achieve both efficiency and external compatibility. Our insight is that the sequential structure of the narrow waist obviates the need for a single source of truth, allowing us to bypass the API Server and perform direct message passing for efficiency. However, our approach introduces a set of ephemeral states across controllers, making it challenging to enforce end-to-end semantics due to the absence of centralized coordination. KUBEDIRECT employs a novel state management scheme that leverages the narrow waist as a hierarchical write-back cache, ensuring consistency and convergence to the desired state. KUBEDIRECT can seamlessly integrate with Kubernetes, adding ~150 LoC per controller. Experiments show that KUBEDIRECT reduces serving latency by 26.7x over Knative, and has similar performance as the state-of-the-art clean-slate platform Dirigent.

</details>


### [30] [Revisiting Parameter Server in LLM Post-Training](https://arxiv.org/abs/2601.19362)
*Xinyi Wan,Penghui Qi,Guangxing Huang,Chaoyi Ruan,Min Lin,Jialin Li*

Main category: cs.DC

TL;DR: 提出按需通信(ODC)方法，通过点对点通信替换FSDP中的集体通信以解决LLM训练中因序列长度差异导致的负载不均衡问题，显著提升设备利用率。


<details>
  <summary>Details</summary>
Motivation: 大语言模型(LLM)训练后阶段存在序列长度高度差异导致负载不均衡，传统集体通信造成同步障碍和设备利用率下降。

Method: 将参数服务器范式整合到FSDP中，用点对点通信替代all-gather和reduce-scatter集体操作，实现每批次一次同步并WD解耦设备负载。

Result: 在多任务LLM训练中提升设备利用率与训练吞吐量，相较标准FSDP最高实现36%加速。

Conclusion: ODC有效解决LLM训练负载失衡问题，其开源实现验证了在任务中的优越适应性。

Abstract: Modern data parallel (DP) training favors collective communication over parameter servers (PS) for its simplicity and efficiency under balanced workloads. However, the balanced workload assumption no longer holds in large language model (LLM) post-training due to the high variance in sequence lengths. Under imbalanced workloads, collective communication creates synchronization barriers, leading to under-utilization of devices with smaller workloads. This change in training dynamics calls for a revisit of the PS paradigm for its robustness to such imbalance. We propose \textbf{On-Demand Communication (ODC)}, which adapts PS into Fully Sharded Data Parallel (FSDP) by replacing collective all-gather and reduce-scatter with direct point-to-point communication. Compared to FSDP, ODC reduces the synchronization barrier from once per layer to once per minibatch and decouples the workload on each device so that faster workers are not stalled. It also enables simpler and more effective load balancing at the minibatch level. Across diverse LLM post-training tasks, ODC consistently improves device utilization and training throughput, achieving up to a 36\% speedup over standard FSDP. These results demonstrate that ODC is a superior fit for the prevalent imbalanced workloads in LLM post-training. Our implementation of ODC and integration with FSDP is open-sourced at https://github.com/sail-sg/odc.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [31] [Intent2QoS: Language Model-Driven Automation of Traffic Shaping Configurations](https://arxiv.org/abs/2601.18974)
*Sudipta Acharya,Burak Kantarci*

Main category: cs.NI

TL;DR: 提出了一种自动化框架，可将高级流量整形意图自动转换为符合标准的Linux流量控制规则，使用排队论语义模型和规则校验实现了端到端流程。


<details>
  <summary>Details</summary>
Motivation: 现有流量控制配置依赖人工设置且需要专业知识，缺乏高层意图到可部署规则的自动化转换方法，亟需解决此技术瓶颈。

Method: 三阶段框架：先通过排队仿真构建语义模型；再用语言模型结合流量剖面生成子意图和规则；最后由规则校验器修正配置确保合规。

Result: LLaMA3在100条意图测试中表现最佳（语义相似度0.88，覆盖率0.87），领先其他模型超30%；AQM引导提示使变异性降低三倍。

Conclusion: 该方法首次实现从业务意图到标准流量控制配置的自动化转换，显著提升效率与准确性。

Abstract: Traffic shaping and Quality of Service (QoS) enforcement are critical for managing bandwidth, latency, and fairness in networks. These tasks often rely on low-level traffic control settings, which require manual setup and technical expertise. This paper presents an automated framework that converts high-level traffic shaping intents in natural or declarative language into valid and correct traffic control rules. To the best of our knowledge, we present the first end-to-end pipeline that ties intent translation in a queuing-theoretic semantic model and, with a rule-based critic, yields deployable Linux traffic control configuration sets. The framework has three steps: (1) a queuing simulation with priority scheduling and Active Queue Management (AQM) builds a semantic model; (2) a language model, using this semantic model and a traffic profile, generates sub-intents and configuration rules; and (3) a rule-based critic checks and adjusts the rules for correctness and policy compliance. We evaluate multiple language models by generating traffic control commands from business intents that comply with relevant standards for traffic control protocols. Experimental results on 100 intents show significant gains, with LLaMA3 reaching 0.88 semantic similarity and 0.87 semantic coverage, outperforming other models by over 30\. A thorough sensitivity study demonstrates that AQM-guided prompting reduces variability threefold compared to zero-shot baselines.

</details>


### [32] [Optimizing Network Topology Efficiency: A Resource-Centric Analysis of Non-Blocking Architectures](https://arxiv.org/abs/2601.19008)
*Jia Xu Wei,Wei Wei*

Main category: cs.NI

TL;DR: 本文重新定義網絡效率為資源成本導向，提出以硬件支出來優化拓撲設計，並比較直接與間接網絡的適用性。


<details>
  <summary>Details</summary>
Motivation: 傳統效率指標如延遲過度強調性能，忽略硬件成本；論文旨在建立資源為核心的效率模型，以解決網絡擴展中的效率誤判問題。

Method: 建模網絡成本為流量倍增係數（Hop Count）和路由器複雜度（Radix）的函數，結合接口成本（α）、交換成本（β）和網絡濃度比率分析。

Result: 中小規模網路下高基數直接網絡最優；大規模需間接拓撲（如Fat Trees）限制複雜度；冗余應公路上平行網絡實例（如多平面Star）而非內在路徑多樣性處理。

Conclusion: 效率最佳化由技術比率決定，強調結構化擴展關鍵資源限制下的網絡設計原則。

Abstract: In modern network design, "efficiency" is often conflated with raw performance metrics like latency or aggregate throughput. This paper proposes a resource-centric definition of efficiency, isolating the hardware cost required to maintain a non-blocking throughput constraint. By modeling network cost as a function of the Traffic Multiplier (Hop Count) and Router Complexity (Radix), we demonstrate that the optimal topology is determined by the technological ratio between link interface costs ($α$), crossbar switching costs ($β$), and the network concentration ratio. We conclude that while high-radix direct networks optimize efficiency at small to medium scales, indirect networks (e.g., Fat Trees) are required to cap router complexity at massive scales. Furthermore, we posit that redundancy is most efficiently handled via parallel network instances (e.g., multi-plane Star networks) rather than intrinsic topological path diversity.

</details>


### [33] [NET4EXA: Pioneering the Future of Interconnects for Supercomputing and AI](https://arxiv.org/abs/2601.19413)
*Michele Martinelli,Roberto Ammendola,Andrea Biagioni,Carlotta Chiarini,Ottorino Frezza,Francesca Lo Cicero,Alessandro Lonardo,Pier Stanislao Paolucci,Elena Pastorelli,Pierpaolo Perticaroli,Luca Pontisso,Cristian Rossi,Francesco Simula,Piero Vicini,David Colin,Grégoire Pichon,Alexandre Louvet,John Gliksberg,Claire Chen,Matteo Turisini,Andrea Monterubbiano,Jean-Philippe Nominé,Denis Dutoit,Hugo Taboada,Lilia Zaourar,Mohamed Benazouz,Angelos Bilas,Fabien Chaix,Manolis Katevenis,Nikolaos Chrysos,Evangelos Mageiropoulos,Christos Kozanitis,Thomas Moen,Steffen Persvold,Einar Rustad,Sandro Fiore,Fabrizio Granelli,Simone Pezzuto,Raffaello Potestio,Luca Tubiana,Philippe Velha,Flavio Vella,Daniele De Sensi,Salvatore Pontarelli*

Main category: cs.NI

TL;DR: NET4EXA开发下一代高性能互连BXIv3，用于HPC和AI系统，基于BXI技术，部署TRL 8试点并铺垫BXIv4。


<details>
  <summary>Details</summary>
Motivation: 应对训练大型语言模型等大规模基础设施的日益增长需求。

Method: 采用混合开发和协同设计方法，结合商用交换机技术与定制IP及基于FPGA的网卡。

Result: 计划集成TRL 8级试点系统，应用基准测试、科学应用和AI工作负载评估性能。

Conclusion: 支持2025年后超算系统部署，奠定BXIv4基础。

Abstract: NET4EXA aims to develop a next-generation high-performance interconnect for HPC and AI systems, addressing the increasing demands of large-scale infrastructures, such as those required for training Large Language Models. Building upon the proven BXI (Bull eXascale Interconnect) European technology used in TOP15 supercomputers, NET4EXA will deliver the new BXI release, BXIv3, a complete hardware and software interconnect solution, including switch and network interface components. The project will integrate a fully functional pilot system at TRL 8, ready for deployment into upcoming exascale and post-exascale systems from 2025 onward. Leveraging prior research from European initiatives like RED-SEA, the previous achievements of consortium partners and over 20 years of expertise from BULL, NET4EXA also lays the groundwork for the future generation of BXI, BXIv4, providing analysis and preliminary design. The project will use a hybrid development and co-design approach, combining commercial switch technology with custom IP and FPGA-based NICs. Performances of NET4EXA BXIv3 interconnect will be evaluated using a broad portfolio of benchmarks, scientific scalable applications, and AI workloads.

</details>
