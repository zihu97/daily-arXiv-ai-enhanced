{"id": "2510.21103", "categories": ["cs.NI", "cs.DC"], "pdf": "https://arxiv.org/pdf/2510.21103", "abs": "https://arxiv.org/abs/2510.21103", "authors": ["Zongyang Yuan", "Lailong Luo", "Qianzhen Zhang", "Bangbang Ren", "Deke Guo", "Richard T. B. Ma"], "title": "Sensing and Storing Less: A MARL-based Solution for Energy Saving in Edge Internet of Things", "comment": null, "summary": "As the number of Internet of Things (IoT) devices continuously grows and\napplication scenarios constantly enrich, the volume of sensor data experiences\nan explosive increase. However, substantial data demands considerable energy\nduring computation and transmission. Redundant deployment or mobile assistance\nis essential to cover the target area reliably with fault-prone sensors.\nConsequently, the ``butterfly effect\" may appear during the IoT operation,\nsince unreasonable data overlap could result in many duplicate data. To this\nend, we propose Senses, a novel online energy saving solution for edge IoT\nnetworks, with the insight of sensing and storing less at the network edge by\nadopting Muti-Agent Reinforcement Learning (MARL). Senses achieves data\nde-duplication by dynamically adjusting sensor coverage at the sensor level.\nFor exceptional cases where sensor coverage cannot be altered, Senses conducts\ndata partitioning and eliminates redundant data at the controller level.\nFurthermore, at the global level, considering the heterogeneity of IoT devices,\nSenses balances the operational duration among the devices to prolong the\noverall operational duration of edge IoT networks. We evaluate the performance\nof Senses through testbed experiments and simulations. The results show that\nSenses saves 11.37% of energy consumption on control devices and prolongs 20%\noverall operational duration of the IoT device network."}
{"id": "2510.21127", "categories": ["cs.NI", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.21127", "abs": "https://arxiv.org/abs/2510.21127", "authors": ["Bowei Tong", "Hui Kang", "Jiahui Li", "Geng Sun", "Jiacheng Wang", "Yaoqi Yang", "Bo Xu", "Dusit Niyato"], "title": "Enhanced Evolutionary Multi-Objective Deep Reinforcement Learning for Reliable and Efficient Wireless Rechargeable Sensor Networks", "comment": "15 pages, 9 figures, submited to TVT", "summary": "Despite rapid advancements in sensor networks, conventional battery-powered\nsensor networks suffer from limited operational lifespans and frequent\nmaintenance requirements that severely constrain their deployment in remote and\ninaccessible environments. As such, wireless rechargeable sensor networks\n(WRSNs) with mobile charging capabilities offer a promising solution to extend\nnetwork lifetime. However, WRSNs face critical challenges from the inherent\ntrade-off between maximizing the node survival rates and maximizing charging\nenergy efficiency under dynamic operational conditions. In this paper, we\ninvestigate a typical scenario where mobile chargers move and charge the\nsensor, thereby maintaining the network connectivity while minimizing the\nenergy waste. Specifically, we formulate a multi-objective optimization problem\nthat simultaneously maximizes the network node survival rate and mobile charger\nenergy usage efficiency across multiple time slots, which presents NP-hard\ncomputational complexity with long-term temporal dependencies that make\ntraditional optimization approaches ineffective. To address these challenges,\nwe propose an enhanced evolutionary multi-objective deep reinforcement learning\nalgorithm, which integrates a long short-term memory (LSTM)-based policy\nnetwork for temporal pattern recognition, a multilayer perceptron-based\nprospective increment model for future state prediction, and a time-varying\nPareto policy evaluation method for dynamic preference adaptation. Extensive\nsimulation results demonstrate that the proposed algorithm significantly\noutperforms existing approaches in balancing node survival rate and energy\nefficiency while generating diverse Pareto-optimal solutions. Moreover, the\nLSTM-enhanced policy network converges 25% faster than conventional networks,\nwith the time-varying evaluation method effectively adapting to dynamic\nconditions."}
{"id": "2510.21130", "categories": ["cs.NI"], "pdf": "https://arxiv.org/pdf/2510.21130", "abs": "https://arxiv.org/abs/2510.21130", "authors": ["Qi Deng", "Yinghao Zhang", "Yalin Liu", "Bishenghui Tao"], "title": "A Confidence-Constrained Cloud-Edge Collaborative Framework for Autism Spectrum Disorder Diagnosis", "comment": "10 pages, 2 figures", "summary": "Autism Spectrum Disorder (ASD) diagnosis systems in school environments\nincreasingly relies on IoT-enabled cameras, yet pure cloud processing raises\nprivacy and latency concerns while pure edge inference suffers from limited\naccuracy. We propose Confidence-Constrained Cloud-Edge Knowledge Distillation\n(C3EKD), a hierarchical framework that performs most inference at the edge and\nselectively uploads only low-confidence samples to the cloud. The cloud\nproduces temperature-scaled soft labels and distils them back to edge models\nvia a global loss aggregated across participating schools, improving\ngeneralization without centralizing raw data. On two public ASD facial-image\ndatasets, the proposed framework achieves a superior accuracy of 87.4\\%,\ndemonstrating its potential for scalable deployment in real-world applications."}
{"id": "2510.21048", "categories": ["cs.PF", "cs.DC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.21048", "abs": "https://arxiv.org/abs/2510.21048", "authors": ["Jiabo Shi", "Dimitrios Pezaros", "Yehia Elkhatib"], "title": "xMem: A CPU-Based Approach for Accurate Estimation of GPU Memory in Deep Learning Training Workloads", "comment": null, "summary": "The global scarcity of GPUs necessitates more sophisticated strategies for\nDeep Learning jobs in shared cluster environments. Accurate estimation of how\nmuch GPU memory a job will require is fundamental to enabling advanced\nscheduling and GPU sharing, which helps prevent out-of-memory (OOM) errors and\nresource underutilization. However, existing estimation methods have\nlimitations. Approaches relying on static analysis or historical data with\nmachine learning often fail to accurately capture runtime dynamics.\nFurthermore, direct GPU analysis consumes scarce resources, and some techniques\nrequire intrusive code modifications. Thus, the key challenge lies in precisely\nestimating dynamic memory requirements, including memory allocator nuances,\nwithout consuming GPU resources and non-intrusive code changes. To address this\nchallenge, we propose xMem, a novel framework that leverages CPU-only dynamic\nanalysis to accurately estimate peak GPU memory requirements a priori. We\nconducted a thorough evaluation of xMem against state-of-the-art solutions\nusing workloads from 25 different models, including architectures like\nConvolutional Neural Networks and Transformers. The analysis of 5209 runs,\nwhich includes ANOVA and Monte Carlo results, highlights xMem's benefits: it\ndecreases the median relative error by 91% and significantly reduces the\nprobability of estimation failure as safe OOM thresholds by 75%, meaning that\nthe estimated value can often be used directly without causing OOM. Ultimately,\nthese improvements lead to a 368% increase in memory conservation potential\nover current solutions."}
{"id": "2510.21141", "categories": ["cs.NI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.21141", "abs": "https://arxiv.org/abs/2510.21141", "authors": ["Haarika Manda", "Manshi Sagar", "Yogesh", "Kartikay Singh", "Cindy Zhao", "Tarun Mangla", "Phillipa Gill", "Elizabeth Belding", "Arpit Gupta"], "title": "TURBOTEST: Learning When Less is Enough through Early Termination of Internet Speed Tests", "comment": null, "summary": "Internet speed tests are indispensable for users, ISPs, and policymakers, but\ntheir static flooding-based design imposes growing costs: a single high-speed\ntest can transfer hundreds of megabytes, and collectively, platforms like\nOokla, M-Lab, and Fast.com generate petabytes of traffic each month. Reducing\nthis burden requires deciding when a test can be stopped early without\nsacrificing accuracy. We frame this as an optimal stopping problem and show\nthat existing heuristics-static thresholds, BBR pipe-full signals, or\nthroughput stability rules from Fast.com and FastBTS-capture only a narrow\nportion of the achievable accuracy-savings trade-off. This paper introduces\nTURBOTEST, a systematic framework for speed test termination that sits atop\nexisting platforms. The key idea is to decouple throughput prediction (Stage 1)\nfrom test termination (Stage 2): Stage 1 trains a regressor to estimate final\nthroughput from partial measurements, while Stage 2 trains a classifier to\ndecide when sufficient evidence has accumulated to stop. Leveraging richer\ntransport-level features (RTT, retransmissions, congestion window) alongside\nthroughput, TURBOTEST exposes a single tunable parameter for accuracy tolerance\nand includes a fallback mechanism for high-variability cases. Evaluation on\n173,000 M-Lab NDT speed tests (2024-2025) shows that TURBOTEST achieves nearly\n2-4x higher data savings than an approach based on BBR signals while reducing\nmedian error. These results demonstrate that adaptive ML-based termination can\ndeliver accurate, efficient, and deployable speed tests at scale."}
{"id": "2510.21405", "categories": ["cs.SE", "cs.AR", "cs.PF"], "pdf": "https://arxiv.org/pdf/2510.21405", "abs": "https://arxiv.org/abs/2510.21405", "authors": ["Aidan Dakhama", "W. B. Langdon", "Hector D. Menendez", "Karine Even-Mendoza"], "title": "GreenMalloc: Allocator Optimisation for Industrial Workloads", "comment": null, "summary": "We present GreenMalloc, a multi objective search-based framework for\nautomatically configuring memory allocators. Our approach uses NSGA II and\nrand_malloc as a lightweight proxy benchmarking tool. We efficiently explore\nallocator parameters from execution traces and transfer the best configurations\nto gem5, a large system simulator, in a case study on two allocators: the GNU\nC/CPP compiler's glibc malloc and Google's TCMalloc. Across diverse workloads,\nour empirical results show up to 4.1 percantage reduction in average heap usage\nwithout loss of runtime efficiency; indeed, we get a 0.25 percantage reduction."}
{"id": "2510.20844", "categories": ["cs.MA"], "pdf": "https://arxiv.org/pdf/2510.20844", "abs": "https://arxiv.org/abs/2510.20844", "authors": ["Jiawei Zhou", "Ruicheng Zhu", "Mengshi Chen", "Jianwei Wang", "Kai Wang"], "title": "\\textsc{autoresearcher}: Automating Knowledge-Grounded and Transparent Research Ideation with Multi-Agent Collaboration", "comment": null, "summary": "Effective research relies on organizing extensive information and stimulating\nnovel solutions. Agentic systems have recently emerged as a promising tool to\nautomate literature-based ideation. However, current systems often remain\nblack-box. Their outputs may appear plausible but weakly grounded, with limited\ntransparency or control for researchers. Our work introduces\n\\textsc{autoresearcher}, a multi-agent demo system for knowledge-grounded and\ntransparent ideation. Specifically, \\textsc{autoresearcher} integrates\nmeticulously designed four stages into a unified framework: (A) Structured\nKnowledge Curation, (B) Diversified Idea Generation, (C) Multi-stage Idea\nSelection, and (D) Expert Panel Review \\& Synthesis. Different from prior\npipelines, our system not only exposes intermediate reasoning states, execution\nlogs, and tunable agents for inspections, but also enables the generation of\nhypotheses that are both diverse and evidence-aligned. Our design is also\ndomain-agnostic: as long as literature sources exist, the same pipeline can be\ninstantiated in any scientific field. As an illustrative case, we demonstrate\n\\textsc{autoresearcher} on a graph-mining case study ($k$-truss breaking\nproblem), where it generates distinct, plausible hypotheses with evidence and\ncritiques. A live demo and source code are available at\nhttps://github.com/valleysprings/AutoResearcher."}
{"id": "2510.21031", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.21031", "abs": "https://arxiv.org/abs/2510.21031", "authors": ["Qinghua Lu", "Dehai Zhao", "Yue Liu", "Hao Zhang", "Liming Zhu", "Xiwei Xu", "Angela Shi", "Tristan Tan", "Rick Kazman"], "title": "AgentArcEval: An Architecture Evaluation Method for Foundation Model based Agents", "comment": null, "summary": "The emergence of foundation models (FMs) has enabled the development of\nhighly capable and autonomous agents, unlocking new application opportunities\nacross a wide range of domains. Evaluating the architecture of agents is\nparticularly important as the architectural decisions significantly impact the\nquality attributes of agents given their unique characteristics, including\ncompound architecture, autonomous and non-deterministic behaviour, and\ncontinuous evolution. However, these traditional methods fall short in\naddressing the evaluation needs of agent architecture due to the unique\ncharacteristics of these agents. Therefore, in this paper, we present\nAgentArcEval, a novel agent architecture evaluation method designed specially\nto address the complexities of FM-based agent architecture and its evaluation.\nMoreover, we present a catalogue of agent-specific general scenarios, which\nserves as a guide for generating concrete scenarios to design and evaluate the\nagent architecture. We demonstrate the usefulness of AgentArcEval and the\ncatalogue through a case study on the architecture evaluation of a real-world\ntax copilot, named Luna."}
{"id": "2510.20931", "categories": ["cs.DC", "cs.AR", "C.1.4; C.4"], "pdf": "https://arxiv.org/pdf/2510.20931", "abs": "https://arxiv.org/abs/2510.20931", "authors": ["Albert Reuther", "Peter Michaleas", "Michael Jones", "Vijay Gadepally", "Jeremy Kepner"], "title": "Lincoln AI Computing Survey (LAICS) and Trends", "comment": "12 pages, 7 figures, 2025 IEEE High Performance Extreme Computing\n  (HPEC) conference, September 2025", "summary": "In the past year, generative AI (GenAI) models have received a tremendous\namount of attention, which in turn has increased attention to computing systems\nfor training and inference for GenAI. Hence, an update to this survey is due.\nThis paper is an update of the survey of AI accelerators and processors from\npast seven years, which is called the Lincoln AI Computing Survey -- LAICS\n(pronounced \"lace\"). This multi-year survey collects and summarizes the current\ncommercial accelerators that have been publicly announced with peak performance\nand peak power consumption numbers. In the same tradition of past papers of\nthis survey, the performance and power values are plotted on a scatter graph,\nand a number of dimensions and observations from the trends on this plot are\nagain discussed and analyzed. Market segments are highlighted on the scatter\nplot, and zoomed plots of each segment are also included. A brief description\nof each of the new accelerators that have been added in the survey this year is\nincluded, and this update features a new categorization of computing\narchitectures that implement each of the accelerators."}
{"id": "2510.20981", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2510.20981", "abs": "https://arxiv.org/abs/2510.20981", "authors": ["Stefan Abi-Karam", "Rishov Sarkar", "Suhail Basalama", "Jason Cong", "Callie Hao"], "title": "FIFOAdvisor: A DSE Framework for Automated FIFO Sizing of High-Level Synthesis Designs", "comment": "Accepted and to be presented at ASP-DAC 2026", "summary": "Dataflow hardware designs enable efficient FPGA implementations via\nhigh-level synthesis (HLS), but correctly sizing first-in-first-out (FIFO)\nchannel buffers remains challenging. FIFO sizes are user-defined and balance\nlatency and area-undersized FIFOs cause stalls and potential deadlocks, while\noversized ones waste memory. Determining optimal sizes is non-trivial: existing\nmethods rely on restrictive assumptions, conservative over-allocation, or slow\nRTL simulations. We emphasize that runtime-based analyses (i.e., simulation)\nare the only reliable way to ensure deadlock-free FIFO optimization for\ndata-dependent designs.\n  We present FIFOAdvisor, a framework that automatically determines FIFO sizes\nin HLS designs. It leverages LightningSim, a 99.9\\% cycle-accurate simulator\nsupporting millisecond-scale incremental runs with new FIFO configurations.\nFIFO sizing is formulated as a dual-objective black-box optimization problem,\nand we explore heuristic and search-based methods to characterize the\nlatency-resource trade-off. FIFOAdvisor also integrates with Stream-HLS, a\nframework for optimizing affine dataflow designs lowered from C++, MLIR, or\nPyTorch, enabling deeper optimization of FIFOs in these workloads.\n  We evaluate FIFOAdvisor on Stream-HLS design benchmarks spanning linear\nalgebra and deep learning workloads. Our results reveal Pareto-optimal\nlatency-memory frontiers across optimization strategies. Compared to baseline\ndesigns, FIFOAdvisor achieves much lower memory usage with minimal delay\noverhead. Additionally, it delivers significant runtime speedups over\ntraditional HLS/RTL co-simulation, making it practical for rapid design space\nexploration. We further demonstrate its capability on a complex accelerator\nwith data-dependent control flow.\n  Code and results: https://github.com/sharc-lab/fifo-advisor"}
{"id": "2510.21162", "categories": ["cs.NI", "cs.CY"], "pdf": "https://arxiv.org/pdf/2510.21162", "abs": "https://arxiv.org/abs/2510.21162", "authors": ["Varshika Srinivasavaradhan", "Morgan Vigil-Hayes", "Ellen Zegura", "Elizabeth Belding"], "title": "Quality of Coverage (QoC): A New Paradigm for Quantifying Cellular Network Coverage Quality, Usability and Stability", "comment": null, "summary": "Current representations of cellular coverage are overly simplistic; they\nstate only the minimal level of available bandwidth (i.e., 35/3Mbps\ndownload/upload speed for 5G) and fail to incorporate a critical component of\nusability: network stability over space and time. Cellular coverage quality is\ncomplex given wireless propagation characteristics and relationships between\nnetwork load and (often limited) network capacity. A more fine-grained\ncharacterization is essential. We introduce Quality of Coverage (QoC), a novel\nmulti-dimensional set of key performance indicators (KPIs) that reflect actual\nmeasured performance quality, usability and stability. This representation of\nthe coverage of the cellular network more fully captures temporal and spatial\nusability and resilience. We motivate and define a set of QoC KPIs and use\nthree distinct datasets to analyze the ability of the KPIs to characterize\nnetwork behavior, demonstrating the ability of QoC to offer a more fine-grained\nand useful representation of cellular coverage than possible with current\nmetrics."}
{"id": "2510.21370", "categories": ["cs.MA", "cs.AI", "cs.CL", "cs.DL"], "pdf": "https://arxiv.org/pdf/2510.21370", "abs": "https://arxiv.org/abs/2510.21370", "authors": ["Zain Ul Abideen Tariq", "Mahmood Al-Zubaidi", "Uzair Shah", "Marco Agus", "Mowafa Househ"], "title": "HIKMA: Human-Inspired Knowledge by Machine Agents through a Multi-Agent Framework for Semi-Autonomous Scientific Conferences", "comment": null, "summary": "HIKMA Semi-Autonomous Conference is the first experiment in reimagining\nscholarly communication through an end-to-end integration of artificial\nintelligence into the academic publishing and presentation pipeline. This paper\npresents the design, implementation, and evaluation of the HIKMA framework,\nwhich includes AI dataset curation, AI-based manuscript generation, AI-assisted\npeer review, AI-driven revision, AI conference presentation, and AI archival\ndissemination. By combining language models, structured research workflows, and\ndomain safeguards, HIKMA shows how AI can support - not replace traditional\nscholarly practices while maintaining intellectual property protection,\ntransparency, and integrity. The conference functions as a testbed and proof of\nconcept, providing insights into the opportunities and challenges of AI-enabled\nscholarship. It also examines questions about AI authorship, accountability,\nand the role of human-AI collaboration in research."}
{"id": "2510.21094", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.21094", "abs": "https://arxiv.org/abs/2510.21094", "authors": ["Yao Lu", "Wanwei Liu", "Tanghaoran Zhang", "Kang Yang", "Yang Zhang", "Wenyu Xu", "Longfei Sun", "Xinjun Mao", "Shuzheng Gao", "Michael R. Lyu"], "title": "BDiff: Block-aware and Accurate Text-based Code Differencing", "comment": null, "summary": "Code differencing is a fundamental technique in software engineering practice\nand research. While researchers have proposed text-based differencing\ntechniques capable of identifying line changes over the past decade, existing\nmethods exhibit a notable limitation in identifying edit actions (EAs) that\noperate on text blocks spanning multiple lines. Such EAs are common in\ndevelopers' practice, such as moving a code block for conditional branching or\nduplicating a method definition block for overloading. Existing tools represent\nsuch block-level operations as discrete sequences of line-level EAs, compelling\ndevelopers to manually correlate them and thereby substantially impeding the\nefficiency of change comprehension. To address this issue, we propose BDiff, a\ntext-based differencing algorithm capable of identifying two types of\nblock-level EAs and five types of line-level EAs. Building on traditional\ndifferencing algorithms, we first construct a candidate set containing all\npossible line mappings and block mappings. Leveraging the Kuhn-Munkres\nalgorithm, we then compute the optimal mapping set that can minimize the size\nof the edit script (ES) while closely aligning with the original developer's\nintent. To validate the effectiveness of BDiff, we selected five\nstate-of-the-art tools, including large language models (LLMs), as baselines\nand adopted a combined qualitative and quantitative approach to evaluate their\nperformance in terms of ES size, result quality, and running time. Experimental\nresults show that BDiff produces higher-quality differencing results than\nbaseline tools while maintaining competitive runtime performance. Our\nexperiments also show the unreliability of LLMs in code differencing tasks\nregarding result quality and their infeasibility in terms of runtime\nefficiency. We have implemented a web-based visual differencing tool."}
{"id": "2510.21155", "categories": ["cs.DC", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.21155", "abs": "https://arxiv.org/abs/2510.21155", "authors": ["Dandan Liang", "Jianing Zhang", "Evan Chen", "Zhe Li", "Rui Li", "Haibo Yang"], "title": "Towards Straggler-Resilient Split Federated Learning: An Unbalanced Update Approach", "comment": null, "summary": "Split Federated Learning (SFL) enables scalable training on edge devices by\ncombining the parallelism of Federated Learning (FL) with the computational\noffloading of Split Learning (SL). Despite its great success, SFL suffers\nsignificantly from the well-known straggler issue in distributed learning\nsystems. This problem is exacerbated by the dependency between Split Server and\nclients: the Split Server side model update relies on receiving activations\nfrom clients. Such synchronization requirement introduces significant time\nlatency, making straggler a critical bottleneck to the scalability and\nefficiency of the system. To mitigate this problem, we propose MU-SplitFed, a\nstraggler-resilient SFL algorithm in zeroth-order optimization that decouples\ntraining progress from straggler delays via a simple yet effective unbalanced\nupdate mechanism.\n  By enabling the server to perform $\\tau$ local updates per client round,\nMU-SplitFed achieves a convergence rate of $O(\\sqrt{d/(\\tau T)})$ for\nnon-convex objectives, demonstrating a linear speedup of $\\tau$ in\ncommunication rounds. Experiments demonstrate that MU-SplitFed consistently\noutperforms baseline methods with the presence of stragglers and effectively\nmitigates their impact through adaptive tuning of $\\tau$. The code for this\nproject is available at https://github.com/Johnny-Zip/MU-SplitFed."}
{"id": "2510.21533", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2510.21533", "abs": "https://arxiv.org/abs/2510.21533", "authors": ["Misaki Kida", "Shimpei Sato"], "title": "Hardware-Efficient Accurate 4-bit Multiplier for Xilinx 7 Series FPGAs", "comment": "5 pages, 5 figures", "summary": "As IoT and edge inference proliferate,there is a growing need to\nsimultaneously optimize area and delay in lookup-table (LUT)-based multipliers\nthat implement large numbers of low-bitwidth operations in parallel. This paper\nproposes a hardwareefficientaccurate 4-bit multiplier design for AMD Xilinx\n7-series FPGAs using only 11 LUTs and two CARRY4 blocks. By reorganizing the\nlogic functions mapped to the LUTs, the proposed method reduces the LUT count\nby one compared with the prior 12-LUT design while also shortening the critical\npath. Evaluation confirms that the circuit attains minimal resource usage and a\ncritical-path delay of 2.750 ns."}
{"id": "2510.21580", "categories": ["cs.NI"], "pdf": "https://arxiv.org/pdf/2510.21580", "abs": "https://arxiv.org/abs/2510.21580", "authors": ["Tomas Lestayo Martinez", "Manuel Fernandez Veiega Veiga"], "title": "Source-Coded Online Algorithm for Multicast Subgraph Construction", "comment": null, "summary": "Multicast remains a fundamental mechanism for scalable content distribution,\nyet existing approaches face critical limitations. Traditional multicast trees\nsuffer from path redundancy and inefficient utilization of network resources,\nwhile network coding, although capacity-achieving, incurs significant\ncomputational overhead and deployment challenges. In this paper, we introduce a\nsource-coded multicast framework that exploits maximum-flow decomposition to\nconstruct multiple disjoint or partially overlapping paths from the source to\nall receivers. Our scheme incorporates a novel path redirection mechanism: when\nmultiple overlaps occur between receiver flows, downstream paths are realigned\nat the first intersection, ensuring loop-free delivery while maximizing overall\nthroughput. We develop algorithms for path construction, overlap detection, and\niterative refinement of multicast subgraphs, and analyze their computational\ncomplexity. Through extensive evaluation on synthetic and real network\ntopologies, we demonstrate that the proposed method consistently approaches the\nthroughput of network coding with substantially lower encoding and decoding\ncomplexity, while significantly outperforming multicast tree constructions in\nterms of fairness, robustness to link failures, and delivery efficiency. These\nresults position source-coded multicast as a practical and scalable solution\nfor next-generation networks requiring high-throughput and adaptive group\ncommunication."}
{"id": "2510.21566", "categories": ["cs.MA", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.21566", "abs": "https://arxiv.org/abs/2510.21566", "authors": ["Fangwen Wu", "Zheng Wu", "Jihong Wang", "Yunku Chen", "Ruiguang Pei", "Heyuan Huang", "Xin Liao", "Xingyu Lou", "Huarong Deng", "Zhihui Fu", "Weiwen Liu", "Zhuosheng Zhang", "Weinan Zhang", "Jun Wang"], "title": "ColorEcosystem: Powering Personalized, Standardized, and Trustworthy Agentic Service in massive-agent Ecosystem", "comment": null, "summary": "With the rapid development of (multimodal) large language model-based agents,\nthe landscape of agentic service management has evolved from single-agent\nsystems to multi-agent systems, and now to massive-agent ecosystems. Current\nmassive-agent ecosystems face growing challenges, including impersonal service\nexperiences, a lack of standardization, and untrustworthy behavior. To address\nthese issues, we propose ColorEcosystem, a novel blueprint designed to enable\npersonalized, standardized, and trustworthy agentic service at scale.\nConcretely, ColorEcosystem consists of three key components: agent carrier,\nagent store, and agent audit. The agent carrier provides personalized service\nexperiences by utilizing user-specific data and creating a digital twin, while\nthe agent store serves as a centralized, standardized platform for managing\ndiverse agentic services. The agent audit, based on the supervision of\ndeveloper and user activities, ensures the integrity and credibility of both\nservice providers and users. Through the analysis of challenges, transitional\nforms, and practical considerations, the ColorEcosystem is poised to power\npersonalized, standardized, and trustworthy agentic service across\nmassive-agent ecosystems. Meanwhile, we have also implemented part of\nColorEcosystem's functionality, and the relevant code is open-sourced at\nhttps://github.com/opas-lab/color-ecosystem."}
{"id": "2510.21106", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.21106", "abs": "https://arxiv.org/abs/2510.21106", "authors": ["Zhen Yang", "Hongyi Lin", "Xiao Yu", "Jacky Wai Keung", "Shuo Liu", "Pak Yuen Patrick Chan", "Yicheng Sun", "Fengji Zhang"], "title": "R2ComSync: Improving Code-Comment Synchronization with In-Context Learning and Reranking", "comment": null, "summary": "Code-Comment Synchronization (CCS) aims to synchronize the comments with code\nchanges in an automated fashion, thereby significantly reducing the workload of\ndevelopers during software maintenance and evolution. While previous studies\nhave proposed various solutions that have shown success, they often exhibit\nlimitations, such as a lack of generalization ability or the need for extensive\ntask-specific learning resources. This motivates us to investigate the\npotential of Large Language Models (LLMs) in this area. However, a pilot\nanalysis proves that LLMs fall short of State-Of-The-Art (SOTA) CCS approaches\nbecause (1) they lack instructive demonstrations for In-Context Learning (ICL)\nand (2) many correct-prone candidates are not prioritized.To tackle the above\nchallenges, we propose R2ComSync, an ICL-based code-Comment Synchronization\napproach enhanced with Retrieval and Re-ranking. Specifically, R2ComSync\ncarries corresponding two novelties: (1) Ensemble hybrid retrieval. It equally\nconsiders the similarity in both code-comment semantics and change patterns\nwhen retrieval, thereby creating ICL prompts with effective examples. (2)\nMulti-turn re-ranking strategy. We derived three significant rules through\nlarge-scale CCS sample analysis. Given the inference results of LLMs, it\nprogressively exploits three re-ranking rules to prioritize relatively\ncorrect-prone candidates. We evaluate R2ComSync using five recent LLMs on three\nCCS datasets covering both Java and Python programming languages, and make\ncomparisons with five SOTA approaches. Extensive experiments demonstrate the\nsuperior performance of R2ComSync against other approaches. Moreover, both\nquantitative and qualitative analyses provide compelling evidence that the\ncomments synchronized by our proposal exhibit significantly higher quality.}"}
{"id": "2510.21173", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2510.21173", "abs": "https://arxiv.org/abs/2510.21173", "authors": ["Víctor Rampérez", "Javier Soriano", "David Lizcano", "Shadi Aljawarneh", "Juan A. Lara"], "title": "From SLA to vendor-neutral metrics: An intelligent knowledge-based approach for multi-cloud SLA-based broker", "comment": null, "summary": "Cloud computing has been consolidated as a support for the vast majority of\ncurrent and emerging technologies. However, there are some barriers that\nprevent the exploitation of the full potential of this technology. First, the\nmajor cloud providers currently put the onus of implementing the mechanisms\nthat ensure compliance with the desired service levels on cloud consumers.\nHowever, consumers do not have the required expertise. Since each cloud\nprovider exports a different set of low-level metrics, the strategies defined\nto ensure compliance with the established service-level agreement (SLA) are\nbound to a particular cloud provider. This fosters provider lock-in and\nprevents consumers from benefiting from the advantages of multi-cloud\nenvironments. This paper presents a solution to the problem of automatically\ntranslating SLAs into objectives expressed as metrics that can be measured\nacross multiple cloud providers. First, we propose an intelligent\nknowledge-based system capable of automatically translating high-level SLAs\ndefined by cloud consumers into a set of conditions expressed as vendor-neutral\nmetrics, providing feedback to cloud consumers (intelligent tutoring system).\nSecondly, we present the set of vendor-neutral metrics and explain how they can\nbe measured for the different cloud providers. Finally, we report a validation\nbased on two use cases (IaaS and PaaS) in a multi-cloud environment formed by\nleading cloud providers. This evaluation has demonstrated that, thanks to the\ncomplementarity of the two solutions, cloud consumers can automatically and\ntransparently exploit the multi-cloud in many application domains, as endorsed\nby the cloud experts consulted in the course of this study."}
{"id": "2510.21547", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2510.21547", "abs": "https://arxiv.org/abs/2510.21547", "authors": ["Hangyu Zhang", "Sachin S. Sapatnekar"], "title": "Accelerating Electrostatics-based Global Placement with Enhanced FFT Computation", "comment": "ASPDAC 2025", "summary": "Global placement is essential for high-quality and efficient circuit\nplacement for complex modern VLSI designs. Recent advancements, such as\nelectrostatics-based analytic placement, have improved scalability and solution\nquality. This work demonstrates that using an accelerated FFT technique,\nAccFFT, for electric field computation significantly reduces runtime.\nExperimental results on standard benchmarks show significant improvements when\nincorporated into the ePlace-MS and Pplace-MS algorithms, e.g., a 5.78x speedup\nin FFT computation and a 32% total runtime improvement against ePlace-MS, with\n1.0% reduction of scaled half-perimeter wirelength after detailed placement."}
{"id": "2510.21373", "categories": ["cs.DC", "cs.NI"], "pdf": "https://arxiv.org/pdf/2510.21373", "abs": "https://arxiv.org/abs/2510.21373", "authors": ["Sankalpa Timilsina", "Susmit Shannigrahi"], "title": "LIDC: A Location Independent Multi-Cluster Computing Framework for Data Intensive Science", "comment": null, "summary": "Scientific communities are increasingly using geographically distributed\ncomputing platforms. The current methods of compute placement predominantly use\nlogically centralized controllers such as Kubernetes (K8s) to match tasks to\navailable resources. However, this centralized approach is unsuitable in\nmulti-organizational collaborations. Furthermore, workflows often need to use\nmanual configurations tailored for a single platform and cannot adapt to\ndynamic changes across infrastructure. Our work introduces a decentralized\ncontrol plane for placing computations on geographically dispersed compute\nclusters using semantic names. We assign semantic names to computations to\nmatch requests with named Kubernetes (K8s) service endpoints. We show that this\napproach provides multiple benefits. First, it allows placement of\ncomputational jobs to be independent of location, enabling any cluster with\nsufficient resources to execute the computation. Second, it facilitates dynamic\ncompute placement without requiring prior knowledge of cluster locations or\npredefined configurations."}
{"id": "2510.21405", "categories": ["cs.SE", "cs.AR", "cs.PF"], "pdf": "https://arxiv.org/pdf/2510.21405", "abs": "https://arxiv.org/abs/2510.21405", "authors": ["Aidan Dakhama", "W. B. Langdon", "Hector D. Menendez", "Karine Even-Mendoza"], "title": "GreenMalloc: Allocator Optimisation for Industrial Workloads", "comment": null, "summary": "We present GreenMalloc, a multi objective search-based framework for\nautomatically configuring memory allocators. Our approach uses NSGA II and\nrand_malloc as a lightweight proxy benchmarking tool. We efficiently explore\nallocator parameters from execution traces and transfer the best configurations\nto gem5, a large system simulator, in a case study on two allocators: the GNU\nC/CPP compiler's glibc malloc and Google's TCMalloc. Across diverse workloads,\nour empirical results show up to 4.1 percantage reduction in average heap usage\nwithout loss of runtime efficiency; indeed, we get a 0.25 percantage reduction."}
{"id": "2510.21183", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2510.21183", "abs": "https://arxiv.org/abs/2510.21183", "authors": ["Anwesha Mukherjee", "Rajkumar Buyya"], "title": "Generative Federated Learning for Smart Prediction and Recommendation Applications", "comment": null, "summary": "This paper proposes a generative adversarial network and federated\nlearning-based model to address various challenges of the smart prediction and\nrecommendation applications, such as high response time, compromised data\nprivacy, and data scarcity. The integration of the generative adversarial\nnetwork and federated learning is referred to as Generative Federated Learning\n(GFL). As a case study of the proposed model, a heart health monitoring\napplication is considered. The realistic synthetic datasets are generated using\nthe generated adversarial network-based proposed algorithm for improving data\ndiversity, data quality, and data augmentation, and remove the data scarcity\nand class imbalance issues. In this paper, we implement the centralized and\ndecentralized federated learning approaches in an edge computing paradigm. In\ncentralized federated learning, the edge nodes communicate with the central\nserver to build the global and personalized local models in a collaborative\nmanner. In the decentralized federated learning approach, the edge nodes\ncommunicate among themselves to exchange model updates for collaborative\ntraining. The comparative study shows that the proposed framework outperforms\nthe existing heart health monitoring applications. The results show that using\nthe proposed framework (i) the prediction accuracy is improved by 12% than the\nconventional framework, and (ii) the response time is reduced by 73% than the\nconventional cloud-only system."}
{"id": "2510.20931", "categories": ["cs.DC", "cs.AR", "C.1.4; C.4"], "pdf": "https://arxiv.org/pdf/2510.20931", "abs": "https://arxiv.org/abs/2510.20931", "authors": ["Albert Reuther", "Peter Michaleas", "Michael Jones", "Vijay Gadepally", "Jeremy Kepner"], "title": "Lincoln AI Computing Survey (LAICS) and Trends", "comment": "12 pages, 7 figures, 2025 IEEE High Performance Extreme Computing\n  (HPEC) conference, September 2025", "summary": "In the past year, generative AI (GenAI) models have received a tremendous\namount of attention, which in turn has increased attention to computing systems\nfor training and inference for GenAI. Hence, an update to this survey is due.\nThis paper is an update of the survey of AI accelerators and processors from\npast seven years, which is called the Lincoln AI Computing Survey -- LAICS\n(pronounced \"lace\"). This multi-year survey collects and summarizes the current\ncommercial accelerators that have been publicly announced with peak performance\nand peak power consumption numbers. In the same tradition of past papers of\nthis survey, the performance and power values are plotted on a scatter graph,\nand a number of dimensions and observations from the trends on this plot are\nagain discussed and analyzed. Market segments are highlighted on the scatter\nplot, and zoomed plots of each segment are also included. A brief description\nof each of the new accelerators that have been added in the survey this year is\nincluded, and this update features a new categorization of computing\narchitectures that implement each of the accelerators."}
{"id": "2510.21413", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.21413", "abs": "https://arxiv.org/abs/2510.21413", "authors": ["Seyedmoein Mohsenimofidi", "Matthias Galster", "Christoph Treude", "Sebastian Baltes"], "title": "Context Engineering for AI Agents in Open-Source Software", "comment": "6 pages, 1 figure, 2 tables", "summary": "GenAI-based coding assistants have disrupted software development. Their next\ngeneration is agent-based, operating with more autonomy and potentially without\nhuman oversight. One challenge is to provide AI agents with sufficient context\nabout the software projects they operate in. Like humans, AI agents require\ncontextual information to develop solutions that are in line with the target\narchitecture, interface specifications, coding guidelines, standard workflows,\nand other project-specific policies. Popular AI agents for software development\n(e.g., Claude Code) advocate for maintaining tool-specific version-controlled\nMarkdown files that cover aspects such as the project structure, building and\ntesting, or code style. The content of these files is automatically added to\neach prompt. AGENTS.md has emerged as a potential standard that consolidates\ntool-specific formats. However, little is known about whether and how\ndevelopers adopt this format. Therefore, in this paper, we present the results\nof a preliminary study investigating the adoption of AI configuration files in\n466 open-source software projects, what information developers provide in these\nfiles, how they present that information, and how they evolve over time. Our\nfindings indicate that there is no established structure yet, and that there is\na lot of variation in terms of how context is provided (descriptive,\nprescriptive, prohibitive, explanatory, conditional). We see great potential in\nstudying which modifications in structure or presentation can positively affect\nthe quality of the generated content. Finally, our analysis of commits that\nhave modified AGENTS.md files provides first insights into how projects\ncontinuously extend and maintain these files. We conclude the paper by\noutlining how the adoption of AI configuration files in provides a unique\nopportunity to study real-world prompt and context engineering."}
{"id": "2510.21304", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2510.21304", "abs": "https://arxiv.org/abs/2510.21304", "authors": ["Hagit Attiya", "Constantin Enea", "Enrique Román-Calvo"], "title": "Arbitration-Free Consistency is Available (and Vice Versa)", "comment": null, "summary": "The fundamental tension between \\emph{availability} and \\emph{consistency}\nshapes the design of distributed storage systems. Classical results capture\nextreme points of this trade-off: the CAP theorem shows that strong models like\nlinearizability preclude availability under partitions, while weak models like\ncausal consistency remain implementable without coordination. These theorems\napply to simple read-write interfaces, leaving open a precise explanation of\nthe combinations of object semantics and consistency models that admit\navailable implementations.\n  This paper develops a general semantic framework in which storage\nspecifications combine operation semantics and consistency models. The\nframework encompasses a broad range of objects (key-value stores, counters,\nsets, CRDTs, and transactional databases) and consistency models (from causal\nconsistency and sequential consistency to snapshot isolation and transactional\nand non-transactional SQL).\n  Within this framework, we prove the \\emph{Arbitration-Free Consistency} (AFC)\ntheorem, showing that an object specification within a consistency model admits\nan available implementation if and only if it is \\emph{arbitration-free}, that\nis, it does not require a total arbitration order to resolve visibility or read\ndependencies.\n  The AFC theorem unifies and generalizes previous results, revealing\narbitration-freedom as the fundamental property that delineates\ncoordination-free consistency from inherently synchronized behavior."}
{"id": "2510.21405", "categories": ["cs.SE", "cs.AR", "cs.PF"], "pdf": "https://arxiv.org/pdf/2510.21405", "abs": "https://arxiv.org/abs/2510.21405", "authors": ["Aidan Dakhama", "W. B. Langdon", "Hector D. Menendez", "Karine Even-Mendoza"], "title": "GreenMalloc: Allocator Optimisation for Industrial Workloads", "comment": null, "summary": "We present GreenMalloc, a multi objective search-based framework for\nautomatically configuring memory allocators. Our approach uses NSGA II and\nrand_malloc as a lightweight proxy benchmarking tool. We efficiently explore\nallocator parameters from execution traces and transfer the best configurations\nto gem5, a large system simulator, in a case study on two allocators: the GNU\nC/CPP compiler's glibc malloc and Google's TCMalloc. Across diverse workloads,\nour empirical results show up to 4.1 percantage reduction in average heap usage\nwithout loss of runtime efficiency; indeed, we get a 0.25 percantage reduction."}
{"id": "2510.21443", "categories": ["cs.SE", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.21443", "abs": "https://arxiv.org/abs/2510.21443", "authors": ["Mohammad Amin Zadenoori", "Vincenzo De Martino", "Jacek Dabrowski", "Xavier Franch", "Alessio Ferrari"], "title": "Does Model Size Matter? A Comparison of Small and Large Language Models for Requirements Classification", "comment": null, "summary": "[Context and motivation] Large language models (LLMs) show notable results in\nnatural language processing (NLP) tasks for requirements engineering (RE).\nHowever, their use is compromised by high computational cost, data sharing\nrisks, and dependence on external services. In contrast, small language models\n(SLMs) offer a lightweight, locally deployable alternative. [Question/problem]\nIt remains unclear how well SLMs perform compared to LLMs in RE tasks in terms\nof accuracy. [Results] Our preliminary study compares eight models, including\nthree LLMs and five SLMs, on requirements classification tasks using the\nPROMISE, PROMISE Reclass, and SecReq datasets. Our results show that although\nLLMs achieve an average F1 score of 2% higher than SLMs, this difference is not\nstatistically significant. SLMs almost reach LLMs performance across all\ndatasets and even outperform them in recall on the PROMISE Reclass dataset,\ndespite being up to 300 times smaller. We also found that dataset\ncharacteristics play a more significant role in performance than model size.\n[Contribution] Our study contributes with evidence that SLMs are a valid\nalternative to LLMs for requirements classification, offering advantages in\nprivacy, cost, and local deployability."}
{"id": "2510.21348", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2510.21348", "abs": "https://arxiv.org/abs/2510.21348", "authors": ["João A. Silva", "Hervé Paulino", "João M. Lourenço"], "title": "Parsley's Group Size Study", "comment": null, "summary": "Parsley is a resilient group-based Distributed Hash Table that incorporates a\npreemptive peer relocation technique and a dynamic data sharding mechanism to\nenhance robustness and balance. In addition to the hard limits on group size,\ndefined by minimum and maximum thresholds, Parsley introduces two soft limits\nthat define a target interval for maintaining stable group sizes. These soft\nboundaries allow the overlay to take proactive measures to prevent violations\nof the hard limits, improving system stability under churn. This work provides\nan in-depth analysis of the rationale behind the parameter values adopted for\nParsley's evaluation. Unlike related systems, which specify group size limits\nwithout justification, we conduct a systematic overlay characterization study\nto understand the effects of these parameters on performance and scalability.\nThe study examines topology operations, the behavior of large groups, and the\noverall trade-offs observed, offering a grounded explanation for the chosen\nconfiguration values."}
{"id": "2510.21451", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.21451", "abs": "https://arxiv.org/abs/2510.21451", "authors": ["Yinglong Zou", "Juan Zhai", "Chunrong Fang", "An Guo", "Jiawei Liu", "Zhenyu Chen"], "title": "Scalpel: Automotive Deep Learning Framework Testing via Assembling Model Components", "comment": null, "summary": "Deep learning (DL) plays a key role in autonomous driving systems. DL models\nsupport perception modules, equipped with tasks such as object detection and\nsensor fusion. These DL models enable vehicles to process multi-sensor inputs\nto understand complex surroundings. Deploying DL models in autonomous driving\nsystems faces stringent challenges, including real-time processing, limited\ncomputational resources, and strict power constraints. To address these\nchallenges, automotive DL frameworks (e.g., PaddleInference) have emerged to\noptimize inference efficiency. However, these frameworks encounter unique\nquality issues due to their more complex deployment environments, such as\ncrashes stemming from limited scheduled memory and incorrect memory allocation.\nUnfortunately, existing DL framework testing methods fail to detect these\nquality issues due to the failure in deploying generated test input models, as\nthese models lack three essential capabilities: (1) multi-input/output tensor\nprocessing, (2) multi-modal data processing, and (3) multi-level data feature\nextraction. These capabilities necessitate specialized model components, which\nexisting testing methods neglect during model generation. To bridge this gap,\nwe propose Scalpel, an automotive DL frameworks testing method that generates\ntest input models at the model component level. Scalpel generates models by\nassembling model components (heads, necks, backbones) to support capabilities\nrequired by autonomous driving systems. Specifically, Scalpel maintains and\nupdates a repository of model components, generating test inputs by selecting,\nmutating, and assembling them. Successfully generated models are added back to\nenrich the repository. Newly generated models are then deployed within the\nautonomous driving system to test automotive DL frameworks via differential\ntesting."}
{"id": "2510.21373", "categories": ["cs.DC", "cs.NI"], "pdf": "https://arxiv.org/pdf/2510.21373", "abs": "https://arxiv.org/abs/2510.21373", "authors": ["Sankalpa Timilsina", "Susmit Shannigrahi"], "title": "LIDC: A Location Independent Multi-Cluster Computing Framework for Data Intensive Science", "comment": null, "summary": "Scientific communities are increasingly using geographically distributed\ncomputing platforms. The current methods of compute placement predominantly use\nlogically centralized controllers such as Kubernetes (K8s) to match tasks to\navailable resources. However, this centralized approach is unsuitable in\nmulti-organizational collaborations. Furthermore, workflows often need to use\nmanual configurations tailored for a single platform and cannot adapt to\ndynamic changes across infrastructure. Our work introduces a decentralized\ncontrol plane for placing computations on geographically dispersed compute\nclusters using semantic names. We assign semantic names to computations to\nmatch requests with named Kubernetes (K8s) service endpoints. We show that this\napproach provides multiple benefits. First, it allows placement of\ncomputational jobs to be independent of location, enabling any cluster with\nsufficient resources to execute the computation. Second, it facilitates dynamic\ncompute placement without requiring prior knowledge of cluster locations or\npredefined configurations."}
{"id": "2510.21452", "categories": ["cs.SE", "cs.CR", "cs.SI", "J.4; K.4.2; K.6.5; D.2.9; D.4.6"], "pdf": "https://arxiv.org/pdf/2510.21452", "abs": "https://arxiv.org/abs/2510.21452", "authors": ["Thomas Welsh", "Kristófer Finnsson", "Brynjólfur Stefánsson", "Helmut Neukirchen"], "title": "Towards Socio-Technical Topology-Aware Adaptive Threat Detection in Software Supply Chains", "comment": "to be published in: The 12th International Conference on Social\n  Networks Analysis, Management and Security (SNAMS), IEEE", "summary": "Software supply chains (SSCs) are complex systems composed of dynamic,\nheterogeneous technical and social components which collectively achieve the\nproduction and maintenance of software artefacts. Attacks on SSCs are\nincreasing, yet pervasive vulnerability analysis is challenging due to their\ncomplexity. Therefore, threat detection must be targeted, to account for the\nlarge and dynamic structure, and adaptive, to account for its change and\ndiversity. While current work focuses on technical approaches for monitoring\nsupply chain dependencies and establishing component controls, approaches which\ninform threat detection through understanding the socio-technical dynamics are\nlacking. We outline a position and research vision to develop and investigate\nthe use of socio-technical models to support adaptive threat detection of SSCs.\nWe motivate this approach through an analysis of the XZ Utils attack whereby\nmalicious actors undermined the maintainers' trust via the project's GitHub and\nmailing lists. We highlight that monitoring technical and social data can\nidentify trends which indicate suspicious behaviour to then inform targeted and\nintensive vulnerability assessment. We identify challenges and research\ndirections to achieve this vision considering techniques for developer and\nsoftware analysis, decentralised adaptation and the need for a test bed for\nsoftware supply chain security research."}
{"id": "2510.21419", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2510.21419", "abs": "https://arxiv.org/abs/2510.21419", "authors": ["Sankalpa Timilsina", "Susmit Shannigrahi"], "title": "Learning to Schedule: A Supervised Learning Framework for Network-Aware Scheduling of Data-Intensive Workloads", "comment": null, "summary": "Distributed cloud environments hosting data-intensive applications often\nexperience slowdowns due to network congestion, asymmetric bandwidth, and\ninter-node data shuffling. These factors are typically not captured by\ntraditional host-level metrics like CPU or memory. Scheduling without\naccounting for these conditions can lead to poor placement decisions, longer\ndata transfers, and suboptimal job performance. We present a network-aware job\nscheduler that uses supervised learning to predict the completion time of\ncandidate jobs. Our system introduces a prediction-and-ranking mechanism that\ncollects real-time telemetry from all nodes, uses a trained supervised model to\nestimate job duration per node, and ranks them to select the best placement. We\nevaluate the scheduler on a geo-distributed Kubernetes cluster deployed on the\nFABRIC testbed by running network-intensive Spark workloads. Compared to the\ndefault Kubernetes scheduler, which makes placement decisions based on current\nresource availability alone, our proposed supervised scheduler achieved 34-54%\nhigher accuracy in selecting optimal nodes for job placement. The novelty of\nour work lies in the demonstration of supervised learning for real-time,\nnetwork-aware job scheduling on a multi-site cluster."}
{"id": "2510.21460", "categories": ["cs.SE", "cs.CY", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.21460", "abs": "https://arxiv.org/abs/2510.21460", "authors": ["Sean McGregor", "Victor Lu", "Vassil Tashev", "Armstrong Foundjem", "Aishwarya Ramasethu", "Sadegh AlMahdi Kazemi Zarkouei", "Chris Knotz", "Kongtao Chen", "Alicia Parrish", "Anka Reuel", "Heather Frase"], "title": "Risk Management for Mitigating Benchmark Failure Modes: BenchRisk", "comment": "19 pages, 7 figures, to be published in the 39th Conference on Neural\n  Information Processing Systems (NeurIPS 2025)", "summary": "Large language model (LLM) benchmarks inform LLM use decisions (e.g., \"is\nthis LLM safe to deploy for my use case and context?\"). However, benchmarks may\nbe rendered unreliable by various failure modes that impact benchmark bias,\nvariance, coverage, or people's capacity to understand benchmark evidence.\nUsing the National Institute of Standards and Technology's risk management\nprocess as a foundation, this research iteratively analyzed 26 popular\nbenchmarks, identifying 57 potential failure modes and 196 corresponding\nmitigation strategies. The mitigations reduce failure likelihood and/or\nseverity, providing a frame for evaluating \"benchmark risk,\" which is scored to\nprovide a metaevaluation benchmark: BenchRisk. Higher scores indicate that\nbenchmark users are less likely to reach an incorrect or unsupported conclusion\nabout an LLM. All 26 scored benchmarks present significant risk within one or\nmore of the five scored dimensions (comprehensiveness, intelligibility,\nconsistency, correctness, and longevity), which points to important open\nresearch directions for the field of LLM benchmarking. The BenchRisk workflow\nallows for comparison between benchmarks; as an open-source tool, it also\nfacilitates the identification and sharing of risks and their mitigations."}
{"id": "2510.21493", "categories": ["cs.DC", "94-08", "D.2.2"], "pdf": "https://arxiv.org/pdf/2510.21493", "abs": "https://arxiv.org/abs/2510.21493", "authors": ["Rüdiger Valk", "Daniel Moldt"], "title": "On Reduction and Synthesis of Petri's Cycloids", "comment": null, "summary": "Cycloids are particular Petri nets for modelling processes of actions and\nevents, belonging to the fundaments of Petri's general systems theory. Defined\nby four parameters they provide an algebraic formalism to describe strongly\nsynchronized sequential processes. To further investigate their structure,\nreduction systems of cycloids are defined in the style of rewriting systems and\nproperties of irreducible cycloids are proved. In particular the synthesis of\ncycloid parameters from their Petri net structure is derived, leading to an\nefficient method for a decision procedure for cycloid isomorphism."}
{"id": "2510.21513", "categories": ["cs.SE", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.21513", "abs": "https://arxiv.org/abs/2510.21513", "authors": ["Fernando Vallecillos Ruiz", "Max Hort", "Leon Moonen"], "title": "Wisdom and Delusion of LLM Ensembles for Code Generation and Repair", "comment": null, "summary": "Today's pursuit of a single Large Language Model (LMM) for all software\nengineering tasks is resource-intensive and overlooks the potential benefits of\ncomplementarity, where different models contribute unique strengths. However,\nthe degree to which coding LLMs complement each other and the best strategy for\nmaximizing an ensemble's potential are unclear, leaving practitioners without a\nclear path to move beyond single-model systems.\n  To address this gap, we empirically compare ten individual LLMs from five\nfamilies, and three ensembles of these LLMs across three software engineering\nbenchmarks covering code generation and program repair. We assess the\ncomplementarity between models and the performance gap between the best\nindividual model and the ensembles. Next, we evaluate various selection\nheuristics to identify correct solutions from an ensemble's candidate pool.\n  We find that the theoretical upperbound for an ensemble's performance can be\n83% above the best single model. Our results show that consensus-based\nstrategies for selecting solutions fall into a \"popularity trap,\" amplifying\ncommon but incorrect outputs. In contrast, a diversity-based strategy realizes\nup to 95% of this theoretical potential, and proves effective even in small\ntwo-model ensembles, enabling a cost-efficient way to enhance performance by\nleveraging multiple LLMs."}
{"id": "2510.21549", "categories": ["cs.DC", "cs.CC"], "pdf": "https://arxiv.org/pdf/2510.21549", "abs": "https://arxiv.org/abs/2510.21549", "authors": ["Marc Fuchs", "Fabian Kuhn"], "title": "Distributed $(Δ+1)$-Coloring in Graphs of Bounded Neighborhood Independence", "comment": null, "summary": "The distributed coloring problem is arguably one of the key problems studied\nin the area of distributed graph algorithms. The most standard variant of the\nproblem asks for a proper vertex coloring of a graph with $\\Delta+1$ colors,\nwhere $\\Delta$ is the maximum degree of the graph. Despite an immense amount of\nwork on distributed coloring problems in the distributed setting, determining\nthe deterministic complexity of $(\\Delta+1)$-coloring in the standard message\npassing model remains one of the most important open questions of the area. In\nthis paper, we aim to improve our understanding of the deterministic complexity\nof $(\\Delta+1)$-coloring as a function of $\\Delta$ in a special family of\ngraphs for which significantly faster algorithms are already known. The\nneighborhood independence $\\theta$ of a graph is the maximum number of pairwise\nnon-adjacent neighbors of some node of the graph. In general, in graphs of\nneighborhood independence $\\theta=O(1)$ (e.g., line graphs), it is known that\n$(\\Delta+1)$-coloring can be solved in $2^{O(\\sqrt{\\log\\Delta})}+O(\\log^* n)$\nrounds. In the present paper, we significantly improve this result, and we show\nthat in graphs of neighborhood independence $\\theta$, a $(\\Delta+1)$-coloring\ncan be computed in $(\\theta\\cdot\\log\\Delta)^{O(\\log\\log\\Delta /\n\\log\\log\\log\\Delta)}+O(\\log^* n)$ rounds and thus in quasipolylogarithmic time\nin $\\Delta$ as long as $\\theta$ is at most polylogarithmic in $\\Delta$. We also\nshow that the known approach that leads to a polylogarithmic in $\\Delta$\nalgorithm for $(2\\Delta-1)$-edge coloring already fails for edge colorings of\nhypergraphs of rank at least $3$."}
{"id": "2510.21516", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.21516", "abs": "https://arxiv.org/abs/2510.21516", "authors": ["Marvin Böcker", "Ralph Biggins", "Michael Schmeing"], "title": "Lights-Out: An Automated Ground Segment for unstaffed Satellite Operations", "comment": null, "summary": "We present our approach for a periodically unstaffed, fully automated ground\nsegment. The concept is in use for the first time on the German satellite\ncommunications mission Heinrich Hertz on behalf of the German Space Agency at\nDLR. Heinrich Hertz was launched in July 2023 and offers access to scientific\nand technical experiments to its users. The mission utilizes major automation\nconcepts for the satellite platform operations, allowing fully automated\noperations outside of office hours. The concept includes tracking, telemetry\nand commanding (TTC) of the satellite. Pre-planned and automatically executed\nschedules enable commanding without human interaction. The user mission\nschedule is planned separately from the main mission schedule and is\nautomatically de-conflicted. The automatic monitoring concept monitors the\nsystems of the satellite and all assets in the ground segment and triggers\nreactions in operator-configurable ways depending on the mission needs, for\nexample emergency notifications or automated execution of flight operation\nprocedures. Additionally, the concept also puts special emphasis on a\nself-service user portal that provides flexible access 24/7, even when the\ncontrol center is not staffed. The portal allows external users of the payload\nto schedule pre-defined experiments, monitor the live execution of the\nexperiment with browser-based displays and access ground station telemetry and\ndedicated RF test equipment during the time of their scheduled experiment.\nTasks can be planned long in advance as well as with a short reaction time\n(less than 1 minute), which allows, for example, the reconfiguration of the\npayload during a running experiment."}
{"id": "2510.21048", "categories": ["cs.PF", "cs.DC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.21048", "abs": "https://arxiv.org/abs/2510.21048", "authors": ["Jiabo Shi", "Dimitrios Pezaros", "Yehia Elkhatib"], "title": "xMem: A CPU-Based Approach for Accurate Estimation of GPU Memory in Deep Learning Training Workloads", "comment": null, "summary": "The global scarcity of GPUs necessitates more sophisticated strategies for\nDeep Learning jobs in shared cluster environments. Accurate estimation of how\nmuch GPU memory a job will require is fundamental to enabling advanced\nscheduling and GPU sharing, which helps prevent out-of-memory (OOM) errors and\nresource underutilization. However, existing estimation methods have\nlimitations. Approaches relying on static analysis or historical data with\nmachine learning often fail to accurately capture runtime dynamics.\nFurthermore, direct GPU analysis consumes scarce resources, and some techniques\nrequire intrusive code modifications. Thus, the key challenge lies in precisely\nestimating dynamic memory requirements, including memory allocator nuances,\nwithout consuming GPU resources and non-intrusive code changes. To address this\nchallenge, we propose xMem, a novel framework that leverages CPU-only dynamic\nanalysis to accurately estimate peak GPU memory requirements a priori. We\nconducted a thorough evaluation of xMem against state-of-the-art solutions\nusing workloads from 25 different models, including architectures like\nConvolutional Neural Networks and Transformers. The analysis of 5209 runs,\nwhich includes ANOVA and Monte Carlo results, highlights xMem's benefits: it\ndecreases the median relative error by 91% and significantly reduces the\nprobability of estimation failure as safe OOM thresholds by 75%, meaning that\nthe estimated value can often be used directly without causing OOM. Ultimately,\nthese improvements lead to a 368% increase in memory conservation potential\nover current solutions."}
{"id": "2510.21591", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.21591", "abs": "https://arxiv.org/abs/2510.21591", "authors": ["Oleksandr Kosenkov", "Ehsan Zabardast", "Davide Fucci", "Daniel Mendez", "Michael Unterkalmsteiner"], "title": "Privacy by Design: Aligning GDPR and Software Engineering Specifications with a Requirements Engineering Approach", "comment": null, "summary": "Context: Consistent requirements and system specifications are essential for\nthe compliance of software systems towards the General Data Protection\nRegulation (GDPR). Both artefacts need to be grounded in the original text and\nconjointly assure the achievement of privacy by design (PbD). Objectives: There\nis little understanding of the perspectives of practitioners on specification\nobjectives and goals to address PbD. Existing approaches do not account for the\ncomplex intersection between problem and solution space expressed in GDPR. In\nthis study we explore the demand for conjoint requirements and system\nspecification for PbD and suggest an approach to address this demand. Methods:\nWe reviewed secondary and related primary studies and conducted interviews with\npractitioners to (1) investigate the state-of-practice and (2) understand the\nunderlying specification objectives and goals (e.g., traceability). We\ndeveloped and evaluated an approach for requirements and systems specification\nfor PbD, and evaluated it against the specification objectives. Results: The\nrelationship between problem and solution space, as expressed in GDPR, is\ninstrumental in supporting PbD. We demonstrate how our approach, based on the\nmodeling GDPR content with original legal concepts, contributes to\nspecification objectives of capturing legal knowledge, supporting specification\ntransparency, and traceability. Conclusion: GDPR demands need to be addressed\nthroughout different levels of abstraction in the engineering lifecycle to\nachieve PbD. Legal knowledge specified in the GDPR text should be captured in\nspecifications to address the demands of different stakeholders and ensure\ncompliance. While our results confirm the suitability of our approach to\naddress practical needs, we also revealed specific needs for the future\neffective operationalization of the approach."}
{"id": "2510.21103", "categories": ["cs.NI", "cs.DC"], "pdf": "https://arxiv.org/pdf/2510.21103", "abs": "https://arxiv.org/abs/2510.21103", "authors": ["Zongyang Yuan", "Lailong Luo", "Qianzhen Zhang", "Bangbang Ren", "Deke Guo", "Richard T. B. Ma"], "title": "Sensing and Storing Less: A MARL-based Solution for Energy Saving in Edge Internet of Things", "comment": null, "summary": "As the number of Internet of Things (IoT) devices continuously grows and\napplication scenarios constantly enrich, the volume of sensor data experiences\nan explosive increase. However, substantial data demands considerable energy\nduring computation and transmission. Redundant deployment or mobile assistance\nis essential to cover the target area reliably with fault-prone sensors.\nConsequently, the ``butterfly effect\" may appear during the IoT operation,\nsince unreasonable data overlap could result in many duplicate data. To this\nend, we propose Senses, a novel online energy saving solution for edge IoT\nnetworks, with the insight of sensing and storing less at the network edge by\nadopting Muti-Agent Reinforcement Learning (MARL). Senses achieves data\nde-duplication by dynamically adjusting sensor coverage at the sensor level.\nFor exceptional cases where sensor coverage cannot be altered, Senses conducts\ndata partitioning and eliminates redundant data at the controller level.\nFurthermore, at the global level, considering the heterogeneity of IoT devices,\nSenses balances the operational duration among the devices to prolong the\noverall operational duration of edge IoT networks. We evaluate the performance\nof Senses through testbed experiments and simulations. The results show that\nSenses saves 11.37% of energy consumption on control devices and prolongs 20%\noverall operational duration of the IoT device network."}
