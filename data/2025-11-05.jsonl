{"id": "2511.02230", "categories": ["cs.OS", "cs.AI", "cs.NI"], "pdf": "https://arxiv.org/pdf/2511.02230", "abs": "https://arxiv.org/abs/2511.02230", "authors": ["Hanchen Li", "Qiuyang Mang", "Runyuan He", "Qizheng Zhang", "Huanzhi Mao", "Xiaokun Chen", "Alvin Cheung", "Joseph Gonzalez", "Ion Stoica"], "title": "Continuum: Efficient and Robust Multi-Turn LLM Agent Scheduling with KV Cache Time-to-Live", "comment": null, "summary": "Agentic LLM applications interleave LLM generation requests with tool calls.\nThese tool calls break the continuity of the workflow by creating pauses\nbetween LLM requests, bringing many challenges for the serving system,\nespecially under multi-turn scenarios. Each pause potentially causes KV cache\neviction and extra waiting time before entering the continuous batch for the\nfollowing LLM request. Since these pauses happen for each call, this problem\nbecomes increasingly severe as turn number grow for agentic programs. Previous\nworks either fail to incorporate information from the tool call, evicting KV\ncache that leads to repetitive prefill or loading, or ignore the continuity of\na multi-turn program, creating waiting time between turns that increases\nper-request latency.\n  We present Continuum, a serving system to optimize job completion time for\nmulti-turn agent workloads by combining tool-aware KV cache timeout with\nprogram-level scheduling. By predicting tool call durations in agentic\nworkflows, Continuum selectively pins the KV cache in GPU memory with a\ntime-to-live value based on total turn number. When combined with program-level\nfirst-come-first-serve, Continuum prevents scheduling bubbles, preserves\nmulti-turn continuity, and optimizes for throughput for complex agentic\nworkflows. By modeling the variability of tool call and agent program\ncontinuity, Continuum outperforms state-of-the-art baselines. Our evaluation on\nreal-world agentic workloads (SWE-Bench and BFCL) with Llama-3.1 8B/70B models\nshows that Continuum significantly improves the average job completion times,\nand remains performant across different hardware setups and DRAM offloading\nschemes. Preview code is available at:\nhttps://github.com/Hanchenli/vllm-continuum"}
{"id": "2511.01941", "categories": ["cs.SE", "cs.AI", "cs.CR"], "pdf": "https://arxiv.org/pdf/2511.01941", "abs": "https://arxiv.org/abs/2511.01941", "authors": ["Sogol Masoumzadeh"], "title": "Detecting Vulnerabilities from Issue Reports for Internet-of-Things", "comment": "ACCEPTED/To Appear in the Proceedings of the 40th IEEE/ACM\n  International Conference on Automated Software Engineering (ASE) 2025.\n  https://conf.researchr.org/details/ase-2025/ase-2025-student-research-competition/5/Detecting-Vulnerabilities-from-Issue-Reports-for-Internet-of-Things", "summary": "Timely identification of issue reports reflecting software vulnerabilities is\ncrucial, particularly for Internet-of-Things (IoT) where analysis is slower\nthan non-IoT systems. While Machine Learning (ML) and Large Language Models\n(LLMs) detect vulnerability-indicating issues in non-IoT systems, their IoT use\nremains unexplored. We are the first to tackle this problem by proposing two\napproaches: (1) combining ML and LLMs with Natural Language Processing (NLP)\ntechniques to detect vulnerability-indicating issues of 21 Eclipse IoT projects\nand (2) fine-tuning a pre-trained BERT Masked Language Model (MLM) on 11,000\nGitHub issues for classifying \\vul. Our best performance belongs to a Support\nVector Machine (SVM) trained on BERT NLP features, achieving an Area Under the\nreceiver operator characteristic Curve (AUC) of 0.65. The fine-tuned BERT\nachieves 0.26 accuracy, emphasizing the importance of exposing all data during\ntraining. Our contributions set the stage for accurately detecting IoT\nvulnerabilities from issue reports, similar to non-IoT systems."}
{"id": "2511.02132", "categories": ["cs.AR", "cs.DC", "cs.LG", "cs.PF"], "pdf": "https://arxiv.org/pdf/2511.02132", "abs": "https://arxiv.org/abs/2511.02132", "authors": ["Mansi Choudhary", "Karthik Sangaiah", "Sonali Singh", "Muhammad Osama", "Lisa Wu Wills", "Ganesh Dasika"], "title": "Optimizing Attention on GPUs by Exploiting GPU Architectural NUMA Effects", "comment": "11 pages, 14 figures", "summary": "The rise of disaggregated AI GPUs has exposed a critical bottleneck in\nlarge-scale attention workloads: non-uniform memory access (NUMA). As\nmulti-chiplet designs become the norm for scaling compute capabilities, memory\nlatency and bandwidth vary sharply across compute regions, undermining the\nperformance of traditional GPU kernel scheduling strategies that assume uniform\nmemory access. We identify how these NUMA effects distort locality in\nmulti-head attention (MHA) and present Swizzled Head-first Mapping, a\nspatially-aware scheduling strategy that aligns attention heads with GPU NUMA\ndomains to exploit intra-chiplet cache reuse. On AMD's MI300X architecture, our\nmethod achieves up to 50% higher performance over state-of-the-art attention\nalgorithms using conventional scheduling techniques and sustains consistently\nhigh L2 cache hit rates of 80-97%. These results demonstrate that NUMA-aware\nscheduling is now fundamental to achieving full efficiency on next-generation\ndisaggregated GPUs, offering a path forward for scalable AI training and\ninference."}
{"id": "2511.01860", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2511.01860", "abs": "https://arxiv.org/abs/2511.01860", "authors": ["Leszek Sliwko"], "title": "A Taxonomy of Schedulers -- Operating Systems, Clusters and Big Data Frameworks", "comment": "This is the accepted author's version of the paper. The final\n  published version is available in Global Journal of Computer Science and\n  Technology, 2019", "summary": "This review analyzes deployed and actively used workload schedulers'\nsolutions and presents a taxonomy in which those systems are divided into\nseveral hierarchical groups based on their architecture and design. While other\ntaxonomies do exist, this review has focused on the key design factors that\naffect the throughput and scalability of a given solution, as well as the\nincremental improvements which bettered such an architecture. This review gives\nspecial attention to Google's Borg, which is one of the most advanced and\npublished systems of this kind."}
{"id": "2511.01886", "categories": ["cs.NI", "cs.SY", "eess.SY", "nlin.CD"], "pdf": "https://arxiv.org/pdf/2511.01886", "abs": "https://arxiv.org/abs/2511.01886", "authors": ["Priya Ranjan"], "title": "Nonlinear Instabilities in Computer Network Dynamics", "comment": "PhD Thesis, 2003. Advisory Committee: Professor Eyad H. Abed ,\n  Chairman Assistant Professor Richard J. La (Co-advisor) Professor P. S.\n  Krishnaprasad Professor Armond M. Makowski Professor Mark I. Freidlin", "summary": "This work studies two types of computer networking models. The primary focus\nis to understand the different dynamical phenomena observed in practice due to\nthe presence of severe nonlinearities, delays and widely varying operating\nconditions. The first models considered are of senders running TCP\n(Transmission Control Protocol) and traffic passing through RED (Random Early\nDetection) gateways. Building on earlier work, a first order nonlinear\ndiscrete-time model is developed for the interaction scenario between transport\nprotocols like TCP and UDP (User Datagram Protocol) and Active Queuing\nManagement schemes like RED. It is shown that the dynamics resulting from the\ninteraction with TCP is consistent with various dynamical behaviors and\nparameter sensitivities observed in practice. Using bifurcation-theoretic ideas\nit is shown that TCP-RED type networks may lose their stability through a\nperiod doubling bifurcation followed by border collision bifurcations. The\nnonlinear dependence of the throughput function of TCP-type flows on drop\nprobability is found to be responsible for the period doubling bifurcation,\nwhereas limited buffer space and lack of sufficient damping results in border\ncollision bifurcations. A second class of models studied in this work deals\nwith optimal rate control in networks and are based on the rate-control\nframework proposed by Kelly. Using the results on delay-differential equation\nstability, the stability and its lack thereof is studied through an underlying\nmap which arises naturally in time delay systems. An invariance property of\nthis map is used to prove delay-independent stability and to compute bounds on\nperiodic oscillations."}
{"id": "2511.01912", "categories": ["cs.MA", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.01912", "abs": "https://arxiv.org/abs/2511.01912", "authors": ["Wenzhe Fan", "Ning Yan", "Masood Mortazavi"], "title": "EvoMem: Improving Multi-Agent Planning with Dual-Evolving Memory", "comment": null, "summary": "Planning has been a cornerstone of artificial intelligence for solving\ncomplex problems, and recent progress in LLM-based multi-agent frameworks have\nbegun to extend this capability. However, the role of human-like memory within\nthese frameworks remains largely unexplored. Understanding how agents\ncoordinate through memory is critical for natural language planning, where\niterative reasoning, constraint tracking, and error correction drive the\nsuccess. Inspired by working memory model in cognitive psychology, we present\nEvoMem, a multi-agent framework built on a dual-evolving memory mechanism. The\nframework consists of three agents (Constraint Extractor, Verifier, and Actor)\nand two memory modules: Constraint Memory (CMem), which evolves across queries\nby storing task-specific rules and constraints while remains fixed within a\nquery, and Query-feedback Memory (QMem), which evolves within a query by\naccumulating feedback across iterations for solution refinement. Both memory\nmodules are reset at the end of each query session. Evaluations on trip\nplanning, meeting planning, and calendar scheduling show consistent performance\nimprovements, highlighting the effectiveness of EvoMem. This success\nunderscores the importance of memory in enhancing multi-agent planning."}
{"id": "2511.02108", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.02108", "abs": "https://arxiv.org/abs/2511.02108", "authors": ["Steven Cho", "Stefano Ruberto", "Valerio Terragni"], "title": "Metamorphic Testing of Large Language Models for Natural Language Processing", "comment": null, "summary": "Using large language models (LLMs) to perform natural language processing\n(NLP) tasks has become increasingly pervasive in recent times. The versatile\nnature of LLMs makes them applicable to a wide range of such tasks. While the\nperformance of recent LLMs is generally outstanding, several studies have shown\nthat they can often produce incorrect results. Automatically identifying these\nfaulty behaviors is extremely useful for improving the effectiveness of LLMs.\nOne obstacle to this is the limited availability of labeled datasets, which\nnecessitates an oracle to determine the correctness of LLM behaviors.\nMetamorphic testing (MT) is a popular testing approach that alleviates this\noracle problem. At the core of MT are metamorphic relations (MRs), which define\nrelationships between the outputs of related inputs. MT can expose faulty\nbehaviors without the need for explicit oracles (e.g., labeled datasets). This\npaper presents the most comprehensive study of MT for LLMs to date. We\nconducted a literature review and collected 191 MRs for NLP tasks. We\nimplemented a representative subset (36 MRs) to conduct a series of experiments\nwith three popular LLMs, running approximately 560,000 metamorphic tests. The\nresults shed light on the capabilities and opportunities of MT for LLMs, as\nwell as its limitations."}
{"id": "2511.02196", "categories": ["cs.AR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.02196", "abs": "https://arxiv.org/abs/2511.02196", "authors": ["Liwei Ni", "Jiaxi Zhang", "Shenggen Zheng", "Junfeng Liu", "Xingyu Meng", "Biwei Xie", "Xingquan Li", "Huawei Li"], "title": "BoolSkeleton: Boolean Network Skeletonization via Homogeneous Pattern Reduction", "comment": null, "summary": "Boolean equivalence allows Boolean networks with identical functionality to\nexhibit diverse graph structures. This gives more room for exploration in logic\noptimization, while also posing a challenge for tasks involving consistency\nbetween Boolean networks. To tackle this challenge, we introduce BoolSkeleton,\na novel Boolean network skeletonization method that improves the consistency\nand reliability of design-specific evaluations. BoolSkeleton comprises two key\nsteps: preprocessing and reduction. In preprocessing, the Boolean network is\ntransformed into a defined Boolean dependency graph, where nodes are assigned\nthe functionality-related status. Next, the homogeneous and heterogeneous\npatterns are defined for the node-level pattern reduction step. Heterogeneous\npatterns are preserved to maintain critical functionality-related dependencies,\nwhile homogeneous patterns can be reduced. Parameter K of the pattern further\nconstrains the fanin size of these patterns, enabling fine-tuned control over\nthe granularity of graph reduction. To validate BoolSkeleton's effectiveness,\nwe conducted four analysis/downstream tasks around the Boolean network:\ncompression analysis, classification, critical path analysis, and timing\nprediction, demonstrating its robustness across diverse scenarios. Furthermore,\nit improves above 55% in the average accuracy compared to the original Boolean\nnetwork for the timing prediction task. These experiments underscore the\npotential of BoolSkeleton to enhance design consistency in logic synthesis."}
{"id": "2511.01861", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2511.01861", "abs": "https://arxiv.org/abs/2511.01861", "authors": ["Johan Messchendorp", "Mohammad Al-Turany", "Volker Friese", "Thorsten Kollegger", "Bastian Loeher", "Jochen Markert", "Andrew Mistry", "Thomas Neff", "Adrian Oeftiger", "Michael Papenbrock", "Stephane Pietri", "Shahab Sanjari", "Tobias Stockmanns"], "title": "Conceptual Design Report for FAIR Computing", "comment": "88 pages, Conceptual Design Report for FAIR Computing", "summary": "This Conceptual Design Report (CDR) presents the plans of the computing\ninfrastructure for research at FAIR, Darmstadt, Germany. It presents the\ncomputing requirements of the various research groups, the policies for the\ncomputing and storage infrastructure, the foreseen FAIR computing model\nincluding the open data, software and services policies and architecture for\nthe periods starting in 2028 with the \"first science (plus)\" phase to the\nmodularized start version of FAIR. The overall ambition is to create a\nfederated and centrally-orchestrated infrastructure serving the large diversity\nof the research lines present with sufficient scalability and flexibility to\ncope with future data challenges that will be present at FAIR."}
{"id": "2511.01989", "categories": ["cs.NI"], "pdf": "https://arxiv.org/pdf/2511.01989", "abs": "https://arxiv.org/abs/2511.01989", "authors": ["Tuğçe Bilen", "Mehmet Özdem"], "title": "A Modular DTaaS Architecture for Predictive Slice Management in 6G Systems", "comment": null, "summary": "The sixth generation (6G) of wireless networks will require fundamentally new\norchestration paradigms to meet stringent requirements for ultra-low latency,\nhigh reliability, and pervasive intelligence. Network slicing emerges as a key\nenabler to support diverse services with customized quality-of-service (QoS)\nguarantees. However, dynamic and fine-grained slice management poses\nsignificant challenges in terms of real-time provisioning, SLA assurance, and\ncross-layer observability. In this paper, we propose a novel Digital Twin as a\nService (DTaaS) framework that embeds per-slice digital twins (SDTs) into the\norchestration loop. Each SDT maintains a synchronized, real-time representation\nof its slice, leveraging multi-domain telemetry and deep sequential models to\npredict traffic evolution and SLA risks. The framework introduces modular\nintelligence layers, programmable interfaces, and edge-embedded decision-making\nto enable proactive provisioning, adaptive scaling, and closed-loop SLA\nassurance. Mathematical formulations for fidelity measurement, predictive\ncontrol, and optimization objectives are provided to ensure rigor and\ntransparency. Evaluation results demonstrate that DTaaS significantly improves\nSLA compliance ratio, reduces resource over-provisioning, and lowers average\nSLA violation probability, offering a scalable and reliable orchestration\napproach for 6G networks."}
{"id": "2511.02217", "categories": ["cs.MA", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.02217", "abs": "https://arxiv.org/abs/2511.02217", "authors": ["Manonmani Sekar", "Nasim Nezamoddini"], "title": "Optimizing Multi-Lane Intersection Performance in Mixed Autonomy Environments", "comment": null, "summary": "One of the main challenges in managing traffic at multilane intersections is\nensuring smooth coordination between human-driven vehicles (HDVs) and connected\nautonomous vehicles (CAVs). This paper presents a novel traffic signal control\nframework that combines Graph Attention Networks (GAT) with Soft Actor-Critic\n(SAC) reinforcement learning to address this challenge. GATs are used to model\nthe dynamic graph- structured nature of traffic flow to capture spatial and\ntemporal dependencies between lanes and signal phases. The proposed SAC is a\nrobust off-policy reinforcement learning algorithm that enables adaptive signal\ncontrol through entropy-optimized decision making. This design allows the\nsystem to coordinate the signal timing and vehicle movement simultaneously with\nobjectives focused on minimizing travel time, enhancing performance, ensuring\nsafety, and improving fairness between HDVs and CAVs. The model is evaluated\nusing a SUMO-based simulation of a four-way intersection and incorporating\ndifferent traffic densities and CAV penetration rates. The experimental results\ndemonstrate the effectiveness of the GAT-SAC approach by achieving a 24.1%\nreduction in average delay and up to 29.2% fewer traffic violations compared to\ntraditional methods. Additionally, the fairness ratio between HDVs and CAVs\nimproved to 1.59, indicating more equitable treatment across vehicle types.\nThese findings suggest that the GAT-SAC framework holds significant promise for\nreal-world deployment in mixed-autonomy traffic systems."}
{"id": "2511.02197", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.02197", "abs": "https://arxiv.org/abs/2511.02197", "authors": ["Shufan Wang", "Xing Hu", "Junkai Chen", "Zhiyuan Pan", "Xin Xia"], "title": "Open the Oyster: Empirical Evaluation and Improvement of Code Reasoning Confidence in LLMs", "comment": "13 pages, 4 figures", "summary": "With the widespread application of large language models (LLMs) in the field\nof code intelligence, increasing attention has been paid to the reliability and\ncontrollability of their outputs in code reasoning tasks. Confidence estimation\nserves as an effective and convenient approach for evaluating these aspects.\nThis paper proposes a confidence analysis and enhancement framework for LLMs\ntailored to code reasoning tasks. We conduct a comprehensive empirical study on\nthe confidence reliability of mainstream LLMs across different tasks, and\nfurther evaluate the effectiveness of techniques such as prompt strategy\noptimisation and mathematical calibration (e.g., Platt Scaling) in improving\nconfidence reliability. Our results show that DeepSeek-Reasoner achieves the\nbest performance across various tasks, outperforming other models by up to\n$0.680$, $0.636$, and $13.652$ in terms of ECE, Brier Score, and Performance\nScore, respectively. The hybrid strategy combining the reassess prompt strategy\nand Platt Scaling achieves improvements of up to $0.541$, $0.628$, and $15.084$\nover the original performance in the aforementioned three metrics. These\nresults indicate that models with reasoning capabilities demonstrate superior\nconfidence reliability, and that the hybrid strategy is the most effective in\nenhancing the confidence reliability of various models. Meanwhile, we elucidate\nthe impact of different task complexities, model scales, and strategies on\nconfidence performance, and highlight that the confidence of current LLMs in\ncomplex reasoning tasks still has considerable room for improvement. This study\nnot only provides a research foundation and technical reference for the\napplication of confidence in LLM-assisted software engineering, but also points\nthe way for future optimisation and engineering deployment of confidence\nmechanisms."}
{"id": "2511.02269", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2511.02269", "abs": "https://arxiv.org/abs/2511.02269", "authors": ["Takuto Ando", "Yu Eto", "Ayumu Takeuchi", "Yasuhiko Nakashima"], "title": "Energy-Efficient Hardware Acceleration of Whisper ASR on a CGLA", "comment": "This paper is accepted at The Thirteenth International Symposium on\n  Computing and Networking (CANDAR2025)", "summary": "The rise of generative AI for tasks like Automatic Speech Recognition (ASR)\nhas created a critical energy consumption challenge. While ASICs offer high\nefficiency, they lack the programmability to adapt to evolving algorithms. To\naddress this trade-off, we implement and evaluate Whisper's core computational\nkernel on the IMAX, a general-purpose Coarse-Grained Linear Arrays (CGLAs)\naccelerator. To our knowledge, this is the first work to execute a Whisper\nkernel on a CGRA and compare its performance against CPUs and GPUs. Using\nhardware/software co-design, we evaluate our system via an FPGA prototype and\nproject performance for a 28 nm ASIC. Our results demonstrate superior energy\nefficiency. The projected ASIC is 1.90x more energy-efficient than the NVIDIA\nJetson AGX Orin and 9.83x more than an NVIDIA RTX 4090 for the Q8_0 model. This\nwork positions CGLA as a promising platform for sustainable ASR on\npower-constrained edge devices."}
{"id": "2511.01862", "categories": ["cs.DC", "cs.CY"], "pdf": "https://arxiv.org/pdf/2511.01862", "abs": "https://arxiv.org/abs/2511.01862", "authors": ["Vanessa Sochat", "Daniel Milroy"], "title": "Possible Futures for Cloud Cost Models", "comment": "10 pages", "summary": "Cloud is now the leading software and computing hardware innovator, and is\nchanging the landscape of compute to one that is optimized for artificial\nintelligence and machine learning (AI/ML). Computing innovation was initially\ndriven to meet the needs of scientific computing. As industry and consumer\nusage of computing proliferated, there was a shift to satisfy a multipolar\ncustomer base. Demand for AI/ML now dominates modern computing and innovation\nhas centralized on cloud. As a result, cost and resource models designed to\nserve AI/ML use cases are not currently well suited for science. If resource\ncontention resulting from a unipole consumer makes access to contended\nresources harder for scientific users, a likely future is running scientific\nworkloads where they were not intended. In this article, we discuss the past,\ncurrent, and possible futures of cloud cost models for the continued support of\ndiscovery and science."}
{"id": "2511.02171", "categories": ["cs.NI"], "pdf": "https://arxiv.org/pdf/2511.02171", "abs": "https://arxiv.org/abs/2511.02171", "authors": ["Rodrigo Nunes", "André Melo", "Rafael Albarello", "Reinaldo Gomes", "Cesar Marcondes", "Lourenço Pereira Jr"], "title": "Permissioned Blockchain in Advanced Air Mobility: A Performance Analisys for UTM", "comment": "Submitted to IEEE International Conference on Communications 2026", "summary": "The rapid adoption of Uncrewed Aerial Vehicles (UAVs) has driven aviation\nauthorities to propose distributed Uncrewed Traffic Management (UTM)\narchitectures. Several studies have advocated blockchain as a promising\ntechnology to meet these requirements. However, since UTM is a safety-critical\nand highly regulated domain, compliance with standards and regulatory\nframeworks is as crucial as performance and security. This work benchmarks two\ndistributed architectures aligned with current regulatory frameworks: the Linux\nFoundation's InterUSS platform and a Hyperledger Fabric-based private ledger.\nOur findings reveal that blockchain-based systems require architectures\nspecifically designed for aeronautical performance constraints."}
{"id": "2511.02304", "categories": ["cs.MA", "cs.AI", "cs.CL", "cs.FL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.02304", "abs": "https://arxiv.org/abs/2511.02304", "authors": ["Beyazit Yalcinkaya", "Marcell Vazquez-Chanlatte", "Ameesh Shah", "Hanna Krasowski", "Sanjit A. Seshia"], "title": "Automata-Conditioned Cooperative Multi-Agent Reinforcement Learning", "comment": null, "summary": "We study the problem of learning multi-task, multi-agent policies for\ncooperative, temporal objectives, under centralized training, decentralized\nexecution. In this setting, using automata to represent tasks enables the\ndecomposition of complex tasks into simpler sub-tasks that can be assigned to\nagents. However, existing approaches remain sample-inefficient and are limited\nto the single-task case. In this work, we present Automata-Conditioned\nCooperative Multi-Agent Reinforcement Learning (ACC-MARL), a framework for\nlearning task-conditioned, decentralized team policies. We identify the main\nchallenges to ACC-MARL's feasibility in practice, propose solutions, and prove\nthe correctness of our approach. We further show that the value functions of\nlearned policies can be used to assign tasks optimally at test time.\nExperiments show emergent task-aware, multi-step coordination among agents,\ne.g., pressing a button to unlock a door, holding the door, and\nshort-circuiting tasks."}
{"id": "2511.02203", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2511.02203", "abs": "https://arxiv.org/abs/2511.02203", "authors": ["Gerhard Yu", "Mithila Sivakumar", "Alvine B. Belle", "Soude Ghari", "Song Wang", "Timothy C. Lethbridge"], "title": "LLMs as Judges: Toward The Automatic Review of GSN-compliant Assurance Cases", "comment": null, "summary": "Assurance cases allow verifying the correct implementation of certain\nnon-functional requirements of mission-critical systems, including their\nsafety, security, and reliability. They can be used in the specification of\nautonomous driving, avionics, air traffic control, and similar systems. They\naim to reduce risks of harm of all kinds including human mortality,\nenvironmental damage, and financial loss. However, assurance cases often tend\nto be organized as extensive documents spanning hundreds of pages, making their\ncreation, review, and maintenance error-prone, time-consuming, and tedious.\nTherefore, there is a growing need to leverage (semi-)automated techniques,\nsuch as those powered by generative AI and large language models (LLMs), to\nenhance efficiency, consistency, and accuracy across the entire assurance-case\nlifecycle. In this paper, we focus on assurance case review, a critical task\nthat ensures the quality of assurance cases and therefore fosters their\nacceptance by regulatory authorities. We propose a novel approach that\nleverages the \\textit{LLM-as-a-judge} paradigm to automate the review process.\nSpecifically, we propose new predicate-based rules that formalize\nwell-established assurance case review criteria, allowing us to craft LLM\nprompts tailored to the review task. Our experiments on several\nstate-of-the-art LLMs (GPT-4o, GPT-4.1, DeepSeek-R1, and Gemini 2.0 Flash) show\nthat, while most LLMs yield relatively good review capabilities, DeepSeek-R1\nand GPT-4.1 demonstrate superior performance, with DeepSeek-R1 ultimately\noutperforming GPT-4.1. However, our experimental results also suggest that\nhuman reviewers are still needed to refine the reviews LLMs yield."}
{"id": "2511.02285", "categories": ["cs.AR", "cs.PL", "cs.SE"], "pdf": "https://arxiv.org/pdf/2511.02285", "abs": "https://arxiv.org/abs/2511.02285", "authors": ["Zhuorui Zhao", "Bing Li", "Grace Li Zhang", "Ulf Schlichtmann"], "title": "VFocus: Better Verilog Generation from Large Language Model via Focused Reasoning", "comment": "accepted by SOCC 2025", "summary": "Large Language Models (LLMs) have shown impressive potential in generating\nVerilog codes, but ensuring functional correctness remains a challenge.\nExisting approaches often rely on self-consistency or simulation feedback to\nselect the best candidate, but they miss opportunities to focus LLM reasoning\non the most informative parts of the design. We propose VFocus, a three-stage\nframework that enhances Verilog generation by sharpening the focus of LLM\nreasoning onto critical decision points in the code generation process. In the\n\\textbf{pre-ranking stage}, VFocus generates multiple code candidates through\nLLM prompting, retries for syntactically valid outputs, and introduces a\n\\textit{Density-guided Filtering} to retain candidates that fall within the\n\"reasoning sweet spot\" for functional correctness. In the \\textbf{ranking\nstage}, we simulate each code candidate using an automatically generated\ntestbench and apply self-consistency-based clustering to identify the most\nconsistent outputs. Finally, in the \\textbf{post-ranking refinement stage},\nVFocus performs inconsistency mining on top-ranked candidates and invokes\nreasoning-augmented LLM prompts for candidate refinement. Experiments on the\nVerilogEval-Human benchmark show that VFocus significantly improves the pass@1\ncorrectness across multiple reasoning LLMs, demonstrating its effectiveness in\nenhancing Verilog generation for complex hardware design tasks."}
{"id": "2511.01863", "categories": ["cs.DC", "cs.DM"], "pdf": "https://arxiv.org/pdf/2511.01863", "abs": "https://arxiv.org/abs/2511.01863", "authors": ["Robert Fabian Lindermann", "Paul-Niklas Ken Kandora", "Simon Caspar Zeller", "Adrian Asmund Fessler", "Steffen Rebennack"], "title": "SPHERE: Spherical partitioning for large-scale routing optimization", "comment": null, "summary": "We study shortest-path routing in large weighted, undirected graphs, where\nexpanding search frontiers raise time and memory costs for exact solvers. We\npropose \\emph{SPHERE}, a source-target-aware heuristic that identifies an\n$s$-$t$ overlap: vertices that are close to both $s$ and $t$ in hop count.\nSelecting an anchor $a$ in this overlap partitions the task into two\nsubproblems with unchanged problem-topology, $s\\to a$ and $a\\to t$; if either\nremains large, the procedure recurses on its induced subgraph. Because the cut\nlies inside the overlap, concatenating the resulting subpaths yields a valid\n$s\\to t$ route without boundary repair. SPHERE is independent of the downstream\nsolver (e.g., Dijkstra) and exposes parallelism across subproblems. On large\nnetworks, it achieves faster runtimes and smaller optimality gaps than\nLouvain-based routing and a METIS-based pipeline, even on graphs with more than\na million nodes and edges, while also outperforming Dijkstra in runtime."}
{"id": "2511.02368", "categories": ["cs.NI"], "pdf": "https://arxiv.org/pdf/2511.02368", "abs": "https://arxiv.org/abs/2511.02368", "authors": ["Rushi Moliya", "Dhaval K. Patel", "Brijesh Soni", "Miguel López-Benítez"], "title": "Optimizing Multi-UAV 3D Deployment for Energy-Efficient Sensing over Uneven Terrains", "comment": "Accepted in part for presentation at IEEE Consumer Communications and\n  Networking Conference (CCNC), Las Vegas, USA, Jan. 2026. 6 pages, 4 figures", "summary": "In this work, we consider a multi-unmanned aerial vehicle (UAV) cooperative\nsensing system where UAVs are deployed to sense multiple targets in\nterrain-aware line of sight (LoS) conditions in uneven terrain equipped with\ndirectional antennas. To mitigate terrain-induced LoS blockages that degrade\ndetection performance, we incorporate a binary LoS indicator and propose a\nbounding volume hierarchy (BHV)-based adaptive scheme for efficient LoS\nevaluation. We formulate a bi-objective problem that maximizes the probability\nof cooperative detection with minimal hover energy constraints governing\nspatial, orientational, and safety constraints. To address the problem, which\nis inherently non-convex, we propose a hierarchical heuristic framework that\ncombines exploration through a genetic algorithm (GA) with per-UAV refinement\nvia particle swarm optimization (PSO), where a penalty-based fitness evaluation\nguides solutions toward feasibility, bounded within constraints. The proposed\nmethodology is an effective trade-off method of traversing through a complex\nsearch space and maintaining terrain-aware LoS connectivity and energy aware\ndeployment. Monte Carlo simulations on real-world terrain data show that the\nproposed GA+PSO framework improves detection probability by 37.02% and 36.5%\nfor 2 and 3 UAVs, respectively, while reducing average excess hover energy by\n45.0% and 48.9% compared to the PSO-only baseline. Relative to the\nnon-optimized scheme, it further achieves 59.5% and 54.2% higher detection\nprobability with 59.8% and 65.9% lower excess hover energy, thereby showing its\neffectiveness with a small number of UAVs over uneven terrain."}
{"id": "2511.02352", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2511.02352", "abs": "https://arxiv.org/abs/2511.02352", "authors": ["Sanket Mhatre", "Yasharth Bajpai", "Sumit Gulwani", "Emerson Murphy-Hill", "Gustavo Soares"], "title": "SWE-Sharp-Bench: A Reproducible Benchmark for C# Software Engineering Tasks", "comment": null, "summary": "AI coding agents have shown great progress on Python software engineering\nbenchmarks like SWE-Bench, and for other languages like Java and C in\nbenchmarks like Multi-SWE-Bench. However, C# -- a prominent enterprise language\nranking #5 in the TIOBE index -- remains absent from such benchmarks. We\nintroduce SWE-Sharp-Bench, a reproducible software engineering benchmark for\nC\\# featuring 150 instances from 17 repositories. Evaluating identical\nmodel-agent configurations across languages reveals a significant performance\ngap: while 70% of Python tasks in SWE-Bench Verified are solved, $only 40% of\nour C\\# tasks are resolved. We open-source SWE-Sharp-Bench and our entire\ncuration pipeline."}
{"id": "2511.02408", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2511.02408", "abs": "https://arxiv.org/abs/2511.02408", "authors": ["Takuto Ando", "Yusuke Inoue"], "title": "Facial Expression Recognition System Using DNN Accelerator with Multi-threading on FPGA", "comment": "This paper was published in the proceedings of the 2024 Twelfth\n  International Symposium on Computing and Networking Workshops (CANDARW)", "summary": "In this paper, we implement a stand-alone facial expression recognition\nsystem on an SoC FPGA with multi-threading using a Deep learning Processor Unit\n(DPU). The system consists of two steps: one for face detection step and one\nfor facial expression recognition. In the previous work, the Haar Cascade\ndetector was run on a CPU in the face detection step due to FPGA resource\nlimitations, but this detector is less accurate for profile and variable\nillumination condition images. Moreover, the previous work used a dedicated\ncircuit accelerator, so running a second DNN inference for face detection on\nthe FPGA would require the addition of a new accelerator. As an alternative to\nthis approach, we run the two inferences by DNN on a DPU, which is a\ngeneral-purpose CNN accelerator of the systolic array type. Our method for face\ndetection using DenseBox and facial expression recognition using CNN on the\nsame DPU enables the efficient use of FPGA resources while maintaining a small\ncircuit size. We also developed a multi-threading technique that improves the\noverall throughput while increasing the DPU utilization efficiency. With this\napproach, we achieved an overall system throughput of 25 FPS and a throughput\nper power consumption of 2.4 times."}
{"id": "2511.01866", "categories": ["cs.DC", "cs.AI", "cs.AR"], "pdf": "https://arxiv.org/pdf/2511.01866", "abs": "https://arxiv.org/abs/2511.01866", "authors": ["Benjamin Kubwimana", "Qijing Huang"], "title": "EdgeReasoning: Characterizing Reasoning LLM Deployment on Edge GPUs", "comment": "Published in the Proceedings of the 2025 IEEE International Symposium\n  on Workload Characterization (IISWC 2025)", "summary": "Edge intelligence paradigm is increasingly demanded by the emerging\nautonomous systems, such as robotics. Beyond ensuring privacy-preserving\noperation and resilience in connectivity-limited environments, edge deployment\noffers significant energy and cost advantages over cloud-based solutions.\nHowever, deploying large language models (LLMs) for reasoning tasks on edge\nGPUs faces critical challenges from strict latency constraints and limited\ncomputational resources. To navigate these constraints, developers must balance\nmultiple design factors - choosing reasoning versus non-reasoning\narchitectures, selecting appropriate model sizes, allocating token budgets, and\napplying test-time scaling strategies - to meet target latency and optimize\naccuracy. Yet guidance on optimal combinations of these variables remains\nscarce. In this work, we present EdgeReasoning, a comprehensive study\ncharacterizing the deployment of reasoning LLMs on edge GPUs. We systematically\nquantify latency-accuracy tradeoffs across various LLM architectures and model\nsizes. We systematically evaluate prompt-based and model-tuning-based\ntechniques for reducing reasoning token length while maintaining performance\nquality. We further profile test-time scaling methods with varying degrees of\nparallelism to maximize accuracy under strict latency budgets. Through these\nanalyses, EdgeReasoning maps the Pareto frontier of achievable accuracy-latency\nconfigurations, offering systematic guidance for optimal edge deployment of\nreasoning LLMs."}
{"id": "2511.02501", "categories": ["cs.NI", "cs.DC"], "pdf": "https://arxiv.org/pdf/2511.02501", "abs": "https://arxiv.org/abs/2511.02501", "authors": ["Mohan Liyanage", "Eldiyar Zhantileuov", "Ali Kadhum Idrees", "Rolf Schuster"], "title": "Lightweight Latency Prediction Scheme for Edge Applications: A Rational Modelling Approach", "comment": "Presented at the ICCS 2025 - 5th International Conference on Computer\n  Systems, Xian, China", "summary": "Accurately predicting end-to-end network latency is essential for enabling\nreliable task offloading in real-time edge computing applications. This paper\nintroduces a lightweight latency prediction scheme based on rational modelling\nthat uses features such as frame size, arrival rate, and link utilization,\neliminating the need for intrusive active probing. The model achieves\nstate-of-the-art prediction accuracy through extensive experiments and 5-fold\ncross-validation (MAE = 0.0115, R$^2$ = 0.9847) with competitive inference\ntime, offering a substantial trade-off between precision and efficiency\ncompared to traditional regressors and neural networks."}
{"id": "2511.02399", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.02399", "abs": "https://arxiv.org/abs/2511.02399", "authors": ["Junwei Liu", "Chen Xu", "Chong Wang", "Tong Bai", "Weitong Chen", "Kaseng Wong", "Yiling Lou", "Xin Peng"], "title": "EvoDev: An Iterative Feature-Driven Framework for End-to-End Software Development with LLM-based Agents", "comment": "14 pages, 6 figures", "summary": "Recent advances in large language model agents offer the promise of\nautomating end-to-end software development from natural language requirements.\nHowever, existing approaches largely adopt linear, waterfall-style pipelines,\nwhich oversimplify the iterative nature of real-world development and struggle\nwith complex, large-scale projects. To address these limitations, we propose\nEvoDev, an iterative software development framework inspired by feature-driven\ndevelopment. EvoDev decomposes user requirements into a set of user-valued\nfeatures and constructs a Feature Map, a directed acyclic graph that explicitly\nmodels dependencies between features. Each node in the feature map maintains\nmulti-level information, including business logic, design, and code, which is\npropagated along dependencies to provide context for subsequent development\niterations. We evaluate EvoDev on challenging Android development tasks and\nshow that it outperforms the best-performing baseline, Claude Code, by a\nsubstantial margin of 56.8%, while improving single-agent performance by\n16.0%-76.6% across different base LLMs, highlighting the importance of\ndependency modeling, context propagation, and workflow-aware agent design for\ncomplex software projects. Our work summarizes practical insights for designing\niterative, LLM-driven development frameworks and informs future training of\nbase LLMs to better support iterative software development."}
{"id": "2511.02494", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2511.02494", "abs": "https://arxiv.org/abs/2511.02494", "authors": ["Raul Murillo", "Julio Villalba-Moreno", "Alberto A. Del Barrio", "Guillermo Botella"], "title": "Digit-Recurrence Posit Division", "comment": "11 pages, 9 figures", "summary": "Posit arithmetic has emerged as a promising alternative to IEEE 754\nfloating-point representation, offering enhanced accuracy and dynamic range.\nHowever, division operations in posit systems remain challenging due to their\ninherent hardware complexity. In this work, we present posit division units\nbased on the digit-recurrence algorithm, marking the first implementation of\nradix-4 digit-recurrence techniques within this context. Our approach\nincorporates hardware-centric optimizations including redundant arithmetic,\non-the-fly quotient conversion, and operand scaling to streamline the division\nprocess while mitigating latency, area, and power overheads. Comprehensive\nsynthesis evaluations across multiple posit configurations demonstrate\nsignificant performance improvements, including more than 80% energy reduction\nwith small area overhead compared to existing methods, and a substantial\ndecrease in the number of iterations. These results underscore the potential of\nour adapted algorithm to enhance the efficiency of posit-based arithmetic\nunits."}
{"id": "2511.01871", "categories": ["cs.DC", "68M15, 93B12"], "pdf": "https://arxiv.org/pdf/2511.01871", "abs": "https://arxiv.org/abs/2511.01871", "authors": ["S. Tsiramua", "H. Meladze", "T. Davitashvili", "J. M. Sanchez", "F. Criado-Aldeanueva"], "title": "Structural Analysis of Multi-Core Processor and Reliability Evaluation Model", "comment": null, "summary": "In the present paper, the models of structural analysis and evaluation of\nefficiency indicators (reliability, fault tolerance, viability, and\nflexibility) of a multi core processor with variable structure, equipped with\nmulti functional cores, are considered. Using logical probabilistic methods,\nthe following has been developed: models for evaluating the reliability and\nfault tolerance of processor cores as multi functional elements; logical\nprobabilistic models of the shortest paths, flexibility, and performance\nconditions for successful operation of multi core processors based on multi\nfunctional cores; and models for estimating the reliability, fault tolerance,\nand lifetime of multi core processors considering all possible states of\nperformance. The results of the structural analysis of two core and four core\nprocessors and the trends of increasing the efficiency indicators of multi core\nprocessors are presented."}
{"id": "2511.02559", "categories": ["cs.NI"], "pdf": "https://arxiv.org/pdf/2511.02559", "abs": "https://arxiv.org/abs/2511.02559", "authors": ["Yao Wang", "Kexin Yu", "Wenyun Xu", "Kaiqiang Hu", "Ziyi Wang", "Lizhao You", "Qiang Su", "Dong Guo", "Haizhou Du", "Wanjian Feng", "Qingyu Song", "Linghe Kong", "Qiao Xiang", "Jiwu Shu"], "title": "Janus: Leveraging Incremental Computation for Efficient DNS Verification", "comment": null, "summary": "Existing DNS configuration verification tools face significant issues (e.g.,\ninefficient and lacking support for incremental verification). Inspired by the\nadvancements in recent work of distributed data plane verification and the\nresemblance be- tween the data plane and DNS configuration, we tackle the\nchallenge of DNS misconfiguration by introducing Janus, a DNS verification\ntool. Our key insight is that the process of a nameserver handling queries can\nbe transformed into a matching process on a match-action table. With this\ninsight, Janus consists of (1) an efficient data structure for partition query\nspace based on the behaviors, (2) a symbolic execution algorithm that specifies\nhow a single nameserver can efficiently cover all possible queries and ensure\nthe accuracy of verification, (3) a mechanism to support incremental\nverification with less computational effort. Extensive experiments on\nreal-world datasets (with over 6 million resource records) show that Janus\nachieves significant speedups, with peak improvements of up to 255.7x and a\nmaximum 6046x reduction in the number of LECs."}
{"id": "2511.02434", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2511.02434", "abs": "https://arxiv.org/abs/2511.02434", "authors": ["Dominik Fuchß", "Haoyu Liu", "Sophie Corallo", "Tobias Hey", "Jan Keim", "Johannes von Geisau", "Anne Koziolek"], "title": "Who's Who? LLM-assisted Software Traceability with Architecture Entity Recognition", "comment": null, "summary": "Identifying architecturally relevant entities in textual artifacts is crucial\nfor Traceability Link Recovery (TLR) between Software Architecture\nDocumentation (SAD) and source code. While Software Architecture Models (SAMs)\ncan bridge the semantic gap between these artifacts, their manual creation is\ntime-consuming. Large Language Models (LLMs) offer new capabilities for\nextracting architectural entities from SAD and source code to construct SAMs\nautomatically or establish direct trace links. This paper presents two\nLLM-based approaches: ExArch extracts component names as simple SAMs from SAD\nand source code to eliminate the need for manual SAM creation, while ArTEMiS\nidentifies architectural entities in documentation and matches them with\n(manually or automatically generated) SAM entities. Our evaluation compares\nagainst state-of-the-art approaches SWATTR, TransArC and ArDoCode. TransArC\nachieves strong performance (F1: 0.87) but requires manually created SAMs;\nExArch achieves comparable results (F1: 0.86) using only SAD and code. ArTEMiS\nis on par with the traditional heuristic-based SWATTR (F1: 0.81) and can\nsuccessfully replace it when integrated with TransArC. The combination of\nArTEMiS and ExArch outperforms ArDoCode, the best baseline without manual SAMs.\nOur results demonstrate that LLMs can effectively identify architectural\nentities in textual artifacts, enabling automated SAM generation and TLR,\nmaking architecture-code traceability more practical and accessible."}
{"id": "2511.02530", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2511.02530", "abs": "https://arxiv.org/abs/2511.02530", "authors": ["Takuto Ando", "Yu Eto", "Yasuhiko Nakashima"], "title": "Implementation and Evaluation of Stable Diffusion on a General-Purpose CGLA Accelerator", "comment": "This paper is accepted at 2025 IEEE 18th International Symposium on\n  Embedded Multicore/Many-core Systems-on-Chip (MCSoC)", "summary": "This paper presents the first implementation and in-depth evaluation of the\nprimary computational kernels from the stable-diffusion.cpp image generation\nframework on IMAX3, a general-purpose Coarse-Grained Reconfigurable Array\n(CGRA) accelerator. We designed IMAX3 as a versatile computational platform,\nand this work assesses its capabilities by executing a demanding image\ngeneration workload. We evaluate its performance on a current\nField-Programmable Gate Array (FPGA) prototype to establish a baseline and\nproject its potential for a future Application-Specific Integrated Circuit\n(ASIC) implementation. Our results demonstrate that, despite its\ngeneral-purpose architecture, IMAX3 achieves promising performance and power\nefficiency, particularly in its projected ASIC form. This work provides\nconcrete guidelines for future IMAX architectural designs and establishes a\nfoundation for developing next-generation, AI-specialized Coarse-Grained Linear\nArray (CGLA) accelerators by refining this versatile platform. Ultimately, this\nachievement contributes to the realization of energy-efficient, on-device,\nmulti-modal AI platforms."}
{"id": "2511.01872", "categories": ["cs.DC", "cs.LG", "cs.PL"], "pdf": "https://arxiv.org/pdf/2511.01872", "abs": "https://arxiv.org/abs/2511.01872", "authors": ["Etash Guha", "Tianxiao Jiang", "Andrew Deng", "Jian Zhang", "Muthu Annamalai"], "title": "Learned Cost Model for Placement on Reconfigurable Dataflow Hardware", "comment": "7 pages, 2 figures, 2 tables, DAC Conference style (2022)", "summary": "Mapping a dataflow-graph of an ML model onto a reconfigurable system is\ndifficult, as different mappings have different throughputs and consume\nresource constraints differently. To solve this, a model to evaluate the\nthroughput of mappings is necessary as measuring throughput completely is\nexpensive. Many use a hand-designed analytical model, relying on proxy features\nor intuition, introducing error. We provide a Learned Approach that predicts\nthroughput 31%-52% more accurately over a variety of graphs. In addition, our\napproach shows no accuracy degradation after removing performance annotations.\nWe show that using this approach results in 5.6% faster compiled graphs."}
{"id": "2511.02638", "categories": ["cs.NI"], "pdf": "https://arxiv.org/pdf/2511.02638", "abs": "https://arxiv.org/abs/2511.02638", "authors": ["Jinkun Zhang", "Stefan Vlaski", "Kin Leung"], "title": "Decentralized AI Service Placement, Selection and Routing in Mobile Networks", "comment": null, "summary": "The rapid development and usage of large-scale AI models by mobile users will\ndominate the traffic load in future communication networks. The advent of AI\ntechnology also facilitates a decentralized AI ecosystem where small\norganizations or even individuals can host AI services. In such scenarios, AI\nservice (models) placement, selection, and request routing decisions are\ntightly coupled, posing a challenging yet fundamental trade-off between service\nquality and service latency, especially when considering user mobility.\nExisting solutions for related problems in mobile edge computing (MEC) and\ndata-intensive networks fall short due to restrictive assumptions about network\nstructure or user mobility. To bridge this gap, we propose a decentralized\nframework that jointly optimizes AI service placement, selection, and request\nrouting. In the proposed framework, we use traffic tunneling to support user\nmobility without costly AI service migrations. To account for nonlinear queuing\ndelays, we formulate a nonconvex problem to optimize the trade-off between\nservice quality and end-to-end latency. We derive the node-level KKT conditions\nand develop a decentralized Frank--Wolfe algorithm with a novel messaging\nprotocol. Numerical evaluations validate the proposed approach and show\nsubstantial performance improvements over existing methods."}
{"id": "2511.02445", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2511.02445", "abs": "https://arxiv.org/abs/2511.02445", "authors": ["Eriks Klotins", "Magnus Ahlgren", "Nicolas Martin Vivaldi", "Even-Andre Karlsson"], "title": "When Continuous Delivery Is Not an Option: Practical Paths to Continuous Engineering in Complex Organizations", "comment": null, "summary": "Purpose: Continuous Software Engineering (CSE) promises improved efficiency,\nquality, and responsiveness in software-intensive organizations. However, fully\nadopting CSE is often constrained by complex products, legacy systems,\norganizational inertia, and regulatory requirements. In this paper, we examine\nfour industrial cases from the automation, automotive, retail, and chemical\nsectors to explore how such constraints shape CSE adoption in practice.\nMethods: We apply and extend a previously proposed CSE Industry Readiness Model\nto assess the current and potential levels of adoption in each case. Through\nexpert interviews and narrative synthesis, we identify common driving forces\nand adoption barriers, including organizational preparedness,\ncross-organizational dependencies, and limited customer demand for continuous\ndelivery. Results: Based on our findings, we propose an updated readiness model\nthat introduces additional levels of internal and external feedback,\ndistinguishes market- and organization-facing constraints, and better guides\npractitioners in setting realistic CSE adoption goals. Conclusions: Our results\nhighlight that while full end-to-end CSE adoption may not always be feasible,\nmeaningful internal improvements are still possible and beneficial. This study\nprovides empirically grounded guidance for organizations navigating partial or\nconstrained CSE transformations."}
{"id": "2511.01866", "categories": ["cs.DC", "cs.AI", "cs.AR"], "pdf": "https://arxiv.org/pdf/2511.01866", "abs": "https://arxiv.org/abs/2511.01866", "authors": ["Benjamin Kubwimana", "Qijing Huang"], "title": "EdgeReasoning: Characterizing Reasoning LLM Deployment on Edge GPUs", "comment": "Published in the Proceedings of the 2025 IEEE International Symposium\n  on Workload Characterization (IISWC 2025)", "summary": "Edge intelligence paradigm is increasingly demanded by the emerging\nautonomous systems, such as robotics. Beyond ensuring privacy-preserving\noperation and resilience in connectivity-limited environments, edge deployment\noffers significant energy and cost advantages over cloud-based solutions.\nHowever, deploying large language models (LLMs) for reasoning tasks on edge\nGPUs faces critical challenges from strict latency constraints and limited\ncomputational resources. To navigate these constraints, developers must balance\nmultiple design factors - choosing reasoning versus non-reasoning\narchitectures, selecting appropriate model sizes, allocating token budgets, and\napplying test-time scaling strategies - to meet target latency and optimize\naccuracy. Yet guidance on optimal combinations of these variables remains\nscarce. In this work, we present EdgeReasoning, a comprehensive study\ncharacterizing the deployment of reasoning LLMs on edge GPUs. We systematically\nquantify latency-accuracy tradeoffs across various LLM architectures and model\nsizes. We systematically evaluate prompt-based and model-tuning-based\ntechniques for reducing reasoning token length while maintaining performance\nquality. We further profile test-time scaling methods with varying degrees of\nparallelism to maximize accuracy under strict latency budgets. Through these\nanalyses, EdgeReasoning maps the Pareto frontier of achievable accuracy-latency\nconfigurations, offering systematic guidance for optimal edge deployment of\nreasoning LLMs."}
{"id": "2511.01881", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2511.01881", "abs": "https://arxiv.org/abs/2511.01881", "authors": ["Zhengxin Fang", "Hui Ma", "Gang Chen", "Rajkumar Buyya"], "title": "HGraphScale: Hierarchical Graph Learning for Autoscaling Microservice Applications in Container-based Cloud Computing", "comment": null, "summary": "Microservice architecture has become a dominant paradigm in application\ndevelopment due to its advantages of being lightweight, flexible, and\nresilient. Deploying microservice applications in the container-based cloud\nenables fine-grained elastic resource allocation. Autoscaling is an effective\napproach to dynamically adjust the resource provisioned to containers. However,\nthe intricate microservice dependencies and the deployment scheme of the\ncontainer-based cloud bring extra challenges of resource scaling. This article\nproposes a novel autoscaling approach named HGraphScale. In particular,\nHGraphScale captures microservice dependencies and the deployment scheme by a\nnewly designed hierarchical graph neural network, and makes effective scaling\nactions for rapidly changing user requests workloads. Extensive experiments\nbased on real-world traces of user requests are conducted to evaluate the\neffectiveness of HGraphScale. The experiment results show that the HGraphScale\noutperforms existing state-of-the-art autoscaling approaches by reducing at\nmost 80.16\\% of the average response time under a certain VM rental budget of\napplication providers."}
{"id": "2511.02692", "categories": ["cs.NI"], "pdf": "https://arxiv.org/pdf/2511.02692", "abs": "https://arxiv.org/abs/2511.02692", "authors": ["Keith Briggs", "Ibrahim Nur"], "title": "CRRM: A 5G system-level simulator", "comment": "9 pages, 5 figures", "summary": "System-level simulation is indispensable for developing and testing novel\nalgorithms for 5G and future wireless networks, yet a gap persists between the\nneeds of the machine learning re- search community and the available tooling.\nTo address this, we introduce the Cellular Radio Reference Model (CRRM), an\nopen-source, pure Python simulator we designed specifically for speed,\nusability, and direct integration with modern AI frameworks. The core\nscientific contribution of CRRM lies in its architecture, which departs from\ntraditional discrete-event simulation. We model the system as a set of\ninter-dependent computational blocks which form nodes in a directed graph. This\nenables a compute-on-demand mechanism we term smart update."}
{"id": "2511.02475", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2511.02475", "abs": "https://arxiv.org/abs/2511.02475", "authors": ["Jürgen Cito", "Dominik Bork"], "title": "Lost in Code Generation: Reimagining the Role of Software Models in AI-driven Software Engineering", "comment": null, "summary": "Generative AI enables rapid ``vibe coding,\" where natural language prompts\nyield working software systems. While this lowers barriers to software\ncreation, it also collapses the boundary between prototypes and engineered\nsoftware, leading to fragile systems that lack robustness, security, and\nmaintainability. We argue that this shift motivates a reimagining of software\nmodels. Rather than serving only as upfront blueprints, models can be recovered\npost-hoc from AI-generated code to restore comprehension, expose risks, and\nguide refinement. In this role, models serve as mediators between human intent,\nAI generation, and long-term system evolution, providing a path toward\nsustainable AI-driven software engineering."}
{"id": "2511.01888", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2511.01888", "abs": "https://arxiv.org/abs/2511.01888", "authors": ["Cynthia Marcelino", "Thomas Pusztai", "Stefan Nastic"], "title": "Roadrunner: Accelerating Data Delivery to WebAssembly-Based Serverless Functions", "comment": "26th International Middleware Conference (MIDDLEWARE 25)", "summary": "Serverless computing provides infrastructure management and elastic\nauto-scaling, therefore reducing operational overhead. By design serverless\nfunctions are stateless, which means they typically leverage external remote\nservices to store and exchange data. Transferring data over a network typically\ninvolves serialization and deserialization. These operations usually require\nmultiple data copies and transitions between user and kernel space, resulting\nin overhead from context switching and memory allocation, contributing\nsignificantly to increased latency and resource consumption. To address these\nissues, we present Roadrunner, a sidecar shim that enables near-zero copy and\nserialization-free data transfer between WebAssembly-based serverless\nfunctions. Roadrunner reduces the multiple copies between user space and kernel\nspace by mapping the function memory and moving the data along a dedicated\nvirtual data hose, bypassing the costly processes of serialization and\ndeserialization. This approach reduces data movement overhead and context\nswitching, achieving near-native latency performance for WebAssembly-based\nserverless functions. Our experimental results demonstrate that Roadrunner\nsignificantly improves the inter-function communication latency from 44% up to\n89%, reducing the serialization overhead in 97% of data transfer, and\nincreasing throughput by 69 times compared to state-of-the-art\nWebAssembly-based serverless functions."}
{"id": "2511.02703", "categories": ["cs.NI"], "pdf": "https://arxiv.org/pdf/2511.02703", "abs": "https://arxiv.org/abs/2511.02703", "authors": ["Mengyao Li", "Noah Ploch", "Sebastian Troia", "Carlo Spatocco", "Wolfgang Kellerer", "Guido Maier"], "title": "On the Optimization of Model Aggregation for Federated Learning at the Network Edge", "comment": "accepted by TNSM", "summary": "The rapid increase in connected devices has signifi- cantly intensified the\ncomputational and communication demands on modern telecommunication networks.\nTo address these chal- lenges, integrating advanced Machine Learning (ML)\ntechniques like Federated Learning (FL) with emerging paradigms such as\nMulti-access Edge Computing (MEC) and Software-Defined Wide Area Networks\n(SD-WANs) is crucial. This paper intro- duces online resource management\nstrategies specifically designed for FL model aggregation, utilizing\nintermediate aggregation at edge nodes. Our analysis highlights the benefits of\nincorporating edge aggregators to reduce network link congestion and maximize\nthe potential of edge computing nodes. However, the risk of network congestion\npersists. To mitigate this, we propose a novel aggregation approach that\ndeploys an aggregator overlay network. We present an Integer Linear Programming\n(ILP) model and a heuristic algorithm to optimize the routing within this\noverlay network. Our solution demonstrates improved adapt- ability to network\nresource utilization, significantly reducing FL training round failure rates by\nup to 15% while also alleviating cloud link congestion."}
{"id": "2511.02713", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2511.02713", "abs": "https://arxiv.org/abs/2511.02713", "authors": ["Qianru Meng", "Zhaochun Ren", "Joost Visser"], "title": "ReleaseEval: A Benchmark for Evaluating Language Models in Automated Release Note Generation", "comment": null, "summary": "Automated release note generation addresses the challenge of documenting\nfrequent software updates, where manual efforts are time-consuming and prone to\nhuman error. Although recent advances in language models further enhance this\nprocess, progress remains hindered by dataset limitations, including the lack\nof explicit licensing and limited reproducibility, and incomplete task design\nthat relies mainly on commit messages for summarization while overlooking\nfine-grained contexts such as commit hierarchies and code changes. To fill this\ngap, we introduce ReleaseEval, a reproducible and openly licensed benchmark\ndesigned to systematically evaluate language models for automated release note\ngeneration. ReleaseEval comprises 94,987 release notes from 3,369 repositories\nacross 6 programming languages, and supports three task settings with three\nlevels of input granularity: (1) commit2sum, which generates release notes from\ncommit messages; (2) tree2sum, which incorporates commit tree structures; and\n(3) diff2sum, which leverages fine-grained code diffs. Both automated and human\nevaluations show that large language models consistently outperform traditional\nbaselines across all tasks, achieving substantial gains on tree2sum, while\nstill struggling on diff2sum. These findings highlight LLMs' proficiency in\nleveraging structured information while revealing challenges in abstracting\nfrom long code diffs."}
{"id": "2511.01893", "categories": ["cs.DC", "cs.PF"], "pdf": "https://arxiv.org/pdf/2511.01893", "abs": "https://arxiv.org/abs/2511.01893", "authors": ["Bin Ma", "Viktor Nikitin", "Xi Wang", "Tekin Bicer", "Dong Li"], "title": "mLR: Scalable Laminography Reconstruction based on Memoization", "comment": null, "summary": "ADMM-FFT is an iterative method with high reconstruction accuracy for\nlaminography but suffers from excessive computation time and large memory\nconsumption. We introduce mLR, which employs memoization to replace the\ntime-consuming Fast Fourier Transform (FFT) operations based on an unique\nobservation that similar FFT operations appear in iterations of ADMM-FFT. We\nintroduce a series of techniques to make the application of memoization to\nADMM-FFT performance-beneficial and scalable. We also introduce variable\noffloading to save CPU memory and scale ADMM-FFT across GPUs within and across\nnodes. Using mLR, we are able to scale ADMM-FFT on an input problem of\n2Kx2Kx2K, which is the largest input problem laminography reconstruction has\never worked on with the ADMM-FFT solution on limited memory; mLR brings 52.8%\nperformance improvement on average (up to 65.4%), compared to the original\nADMM-FFT."}
{"id": "2511.02748", "categories": ["cs.NI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.02748", "abs": "https://arxiv.org/abs/2511.02748", "authors": ["Farhad Rezazadeh", "Hatim Chergui", "Merouane Debbah", "Houbing Song", "Dusit Niyato", "Lingjia Liu"], "title": "Agentic World Modeling for 6G: Near-Real-Time Generative State-Space Reasoning", "comment": "13 Pages, 3 Figures, 4 Tables", "summary": "We argue that sixth-generation (6G) intelligence is not fluent token\nprediction but the capacity to imagine and choose -- to simulate future\nscenarios, weigh trade-offs, and act with calibrated uncertainty. We reframe\nopen radio access network (O-RAN) near-real-time (Near-RT) control via\ncounterfactual dynamics and a world modeling (WM) paradigm that learns an\naction-conditioned generative state space. This enables quantitative \"what-if\"\nforecasting beyond large language models (LLMs) as the primary modeling\nprimitive. Actions such as physical resource blocks (PRBs) are treated as\nfirst-class control inputs in a causal world model, and both aleatoric and\nepistemic uncertainty are modeled for prediction and what-if analysis. An\nagentic, model predictive control (MPC)-based cross-entropy method (CEM)\nplanner operates over short horizons, using prior-mean rollouts within\ndata-driven PRB bounds to maximize a deterministic reward. The model couples\nmulti-scale structured state-space mixtures (MS3M) with a compact stochastic\nlatent to form WM-MS3M, summarizing key performance indicators (KPIs) histories\nand predicting next-step KPIs under hypothetical PRB sequences. On realistic\nO-RAN traces, WM-MS3M cuts mean absolute error (MAE) by 1.69% versus MS3M with\n32% fewer parameters and similar latency, and achieves 35-80% lower root mean\nsquared error (RMSE) than attention/hybrid baselines with 2.3-4.1x faster\ninference, enabling rare-event simulation and offline policy screening."}
{"id": "2511.02736", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2511.02736", "abs": "https://arxiv.org/abs/2511.02736", "authors": ["Madalena Sasportes", "Grischa Liebel", "Miguel Goulão"], "title": "Investigating the Experience of Autistic Individuals in Software Engineering", "comment": null, "summary": "Context: Autism spectrum disorder (ASD) leads to various issues in the\neveryday life of autistic individuals, often resulting in unemployment and\nmental health problems. To improve the inclusion of autistic adults, existing\nstudies have highlighted the strengths these individuals possess in comparison\nto non-autistic individuals, e.g., high attention to detail or excellent\nlogical reasoning skills. If fostered, these strengths could be valuable in\nsoftware engineering activities, such for identifying specific kinds of bugs in\ncode. However, existing work in SE has primarily studied the challenges of\nautistic individuals and possible accommodations, with little attention their\nstrengths. Objective: Our goal is to analyse the experiences of autistic\nindividuals in software engineering activities, such as code reviews, with a\nparticular emphasis on strengths. Methods: This study combines Social-Technical\nGrounded Theory through semi-structured interviews with 16 autistic software\nengineers and a survey with 49 respondents, including 5 autistic participants.\nWe compare the emerging themes with the theory by Gama et al. on the Effect of\nNeurodivergent Cognitive Dysfunctions in Software Engineering Performance.\nResults: Our results suggest that autistic software engineers are often skilled\nin logical thinking, attention to detail, and hyperfocus in programming; and\nthey enjoy learning new programming languages and programming-related\ntechnologies. Confirming previous work, they tend to prefer written\ncommunication and remote work. Finally, we report a high comfort level in\ninteracting with AI-based systems. Conclusions: Our findings extend existing\nwork by providing further evidence on the strengths of autistic software\nengineers."}
{"id": "2511.02034", "categories": ["cs.DC", "cs.ET", "cs.NI"], "pdf": "https://arxiv.org/pdf/2511.02034", "abs": "https://arxiv.org/abs/2511.02034", "authors": ["Shashank Motepalli", "Naman Garg", "Gengrui Zhang", "Hans-Arno Jacobsen"], "title": "GPoS: Geospatially-aware Proof of Stake", "comment": "Published in ACM TWEB", "summary": "Geospatial decentralization is essential for blockchains, ensuring regulatory\nresilience, robustness, and fairness. We empirically analyze five major Proof\nof Stake (PoS) blockchains: Aptos, Avalanche, Ethereum, Solana, and Sui,\nrevealing that a few geographic regions dominate consensus voting power,\nresulting in limited geospatial decentralization. To address this, we propose\nGeospatially aware Proof of Stake (GPoS), which integrates geospatial diversity\nwith stake-based voting power. Experimental evaluation demonstrates an average\n45% improvement in geospatial decentralization, as measured by the Gini\ncoefficient of Eigenvector centrality, while incurring minimal performance\noverhead in BFT protocols, including HotStuff and CometBFT. These results\ndemonstrate that GPoS can improve geospatial decentralization {while, in our\nexperiments, incurring minimal overhead} to consensus performance."}
{"id": "2511.02034", "categories": ["cs.DC", "cs.ET", "cs.NI"], "pdf": "https://arxiv.org/pdf/2511.02034", "abs": "https://arxiv.org/abs/2511.02034", "authors": ["Shashank Motepalli", "Naman Garg", "Gengrui Zhang", "Hans-Arno Jacobsen"], "title": "GPoS: Geospatially-aware Proof of Stake", "comment": "Published in ACM TWEB", "summary": "Geospatial decentralization is essential for blockchains, ensuring regulatory\nresilience, robustness, and fairness. We empirically analyze five major Proof\nof Stake (PoS) blockchains: Aptos, Avalanche, Ethereum, Solana, and Sui,\nrevealing that a few geographic regions dominate consensus voting power,\nresulting in limited geospatial decentralization. To address this, we propose\nGeospatially aware Proof of Stake (GPoS), which integrates geospatial diversity\nwith stake-based voting power. Experimental evaluation demonstrates an average\n45% improvement in geospatial decentralization, as measured by the Gini\ncoefficient of Eigenvector centrality, while incurring minimal performance\noverhead in BFT protocols, including HotStuff and CometBFT. These results\ndemonstrate that GPoS can improve geospatial decentralization {while, in our\nexperiments, incurring minimal overhead} to consensus performance."}
{"id": "2511.02810", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2511.02810", "abs": "https://arxiv.org/abs/2511.02810", "authors": ["Suddhasvatta Das", "Kevin Gary"], "title": "Formalizing Regression Testing for Agile and Continuous Integration Environments", "comment": "This is the first attempt to formalize regression testing in agile\n  context as a continuous/near-continuos activity. This formalization will help\n  practitioners and researchers to answer 'when', 'what' and 'how much'\n  question of regression testing in real world time constrained agile projects.\n  This work is currently under review with Software Quality Journal", "summary": "Software developed using modern agile practices delivers a stream of software\nversions that require continuous regression testing rather than testing once\nclose to the delivery or maintenance phase, as assumed by classical\nregression-testing theory. In this work, we formalize the phenomenon of\ncontinuous or near-continuous regression testing using successive builds as a\ntime-ordered chain, where each build contains the program, requirements, and\nthe accompanying tests. We also formalize the regression test window between\nany two builds, which captures the limited time budget available for regression\ntesting. As the time limit is set to infinity and the chain is closed to two\nbuilds, the model degenerates to retest-all, thereby preserving semantics for\nthe classical two-version case. The formalization is validated by directly\nrepresenting two state-of-the-art agile regression testing algorithms in terms\nof build-tuple operations without requiring auxiliary assumptions, followed by\nproof of the soundness and completeness of our formalization."}
{"id": "2511.02168", "categories": ["cs.DC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.02168", "abs": "https://arxiv.org/abs/2511.02168", "authors": ["Octavian Alexandru Trifan", "Karthik Sangaiah", "Muhammad Awad", "Muhammad Osama", "Sumanth Gudaparthi", "Alexandru Nicolau", "Alexander Veidenbaum", "Ganesh Dasika"], "title": "Eliminating Multi-GPU Performance Taxes: A Systems Approach to Efficient Distributed LLMs", "comment": null, "summary": "As large language models (LLMs) continue to scale, their workloads\nincreasingly rely on distributed execution across multiple GPUs. However, the\nconventional bulk synchronous parallel~(BSP) model used in such settings\nintroduces significant performance inefficiencies. To characterize these\nbottlenecks, we introduce the ''Three Taxes'' (Bulk Synchronous, Inter-Kernel\nData Locality, and Kernel Launch Overhead) as an analytical framework. We\npropose moving beyond the rigid BSP model to address key inefficiencies in\ndistributed GPU execution. By exploiting libraries like Iris for Triton, we\ngain access to in-kernel communication primitives that enable the design of\nnovel fine-grained programming patterns, offering greater flexibility and\nperformance than traditional BSP-based approaches. These patterns\nsystematically eliminate the three taxes by creating direct, tile-level\nproducer-consumer pipelines and replacing global barriers with fine-grained\ndataflow synchronization. Applying this methodology to critical kernels, from\nthe foundational All-Gather + general matrix multiplication operation to the\ncomplex Flash Decode algorithm, we observe a 10-20% speedup in end-to-end\nlatency over BSP-based approaches, establishing a more programmable and\nefficient paradigm for distributed LLM workloads."}
{"id": "2511.02230", "categories": ["cs.OS", "cs.AI", "cs.NI"], "pdf": "https://arxiv.org/pdf/2511.02230", "abs": "https://arxiv.org/abs/2511.02230", "authors": ["Hanchen Li", "Qiuyang Mang", "Runyuan He", "Qizheng Zhang", "Huanzhi Mao", "Xiaokun Chen", "Alvin Cheung", "Joseph Gonzalez", "Ion Stoica"], "title": "Continuum: Efficient and Robust Multi-Turn LLM Agent Scheduling with KV Cache Time-to-Live", "comment": null, "summary": "Agentic LLM applications interleave LLM generation requests with tool calls.\nThese tool calls break the continuity of the workflow by creating pauses\nbetween LLM requests, bringing many challenges for the serving system,\nespecially under multi-turn scenarios. Each pause potentially causes KV cache\neviction and extra waiting time before entering the continuous batch for the\nfollowing LLM request. Since these pauses happen for each call, this problem\nbecomes increasingly severe as turn number grow for agentic programs. Previous\nworks either fail to incorporate information from the tool call, evicting KV\ncache that leads to repetitive prefill or loading, or ignore the continuity of\na multi-turn program, creating waiting time between turns that increases\nper-request latency.\n  We present Continuum, a serving system to optimize job completion time for\nmulti-turn agent workloads by combining tool-aware KV cache timeout with\nprogram-level scheduling. By predicting tool call durations in agentic\nworkflows, Continuum selectively pins the KV cache in GPU memory with a\ntime-to-live value based on total turn number. When combined with program-level\nfirst-come-first-serve, Continuum prevents scheduling bubbles, preserves\nmulti-turn continuity, and optimizes for throughput for complex agentic\nworkflows. By modeling the variability of tool call and agent program\ncontinuity, Continuum outperforms state-of-the-art baselines. Our evaluation on\nreal-world agentic workloads (SWE-Bench and BFCL) with Llama-3.1 8B/70B models\nshows that Continuum significantly improves the average job completion times,\nand remains performant across different hardware setups and DRAM offloading\nschemes. Preview code is available at:\nhttps://github.com/Hanchenli/vllm-continuum"}
{"id": "2511.02827", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2511.02827", "abs": "https://arxiv.org/abs/2511.02827", "authors": ["Mohamed Almukhtar", "Anwar Ghammam", "Marouane Kessentini", "Hua Ming"], "title": "From Code Changes to Quality Gains: An Empirical Study in Python ML Systems with PyQu", "comment": "Accepted for publication in the proceedings of IEEE/ACM 48th\n  International Conference on Software Engineering", "summary": "In an era shaped by Generative Artificial Intelligence for code generation\nand the rising adoption of Python-based Machine Learning systems (MLS),\nsoftware quality has emerged as a major concern. As these systems grow in\ncomplexity and importance, a key obstacle lies in understanding exactly how\nspecific code changes affect overall quality-a shortfall aggravated by the lack\nof quality assessment tools and a clear mapping between ML systems code changes\nand their quality effects. Although prior work has explored code changes in\nMLS, it mostly stops at what the changes are, leaving a gap in our knowledge of\nthe relationship between code changes and the MLS quality. To address this gap,\nwe conducted a large-scale empirical study of 3,340 open-source Python ML\nprojects, encompassing more than 3.7 million commits and 2.7 trillion lines of\ncode. We introduce PyQu, a novel tool that leverages low level software metrics\nto identify quality-enhancing commits with an average accuracy, precision, and\nrecall of 0.84 and 0.85 of average F1 score. Using PyQu and a thematic\nanalysis, we identified 61 code changes, each demonstrating a direct impact on\nenhancing software quality, and we classified them into 13 categories based on\ncontextual characteristics. 41% of the changes are newly discovered by our\nstudy and have not been identified by state-of-the-art Python changes detection\ntools. Our work offers a vital foundation for researchers, practitioners,\neducators, and tool developers, advancing the quest for automated quality\nassessment and best practices in Python-based ML software."}
{"id": "2511.02248", "categories": ["cs.DC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.02248", "abs": "https://arxiv.org/abs/2511.02248", "authors": ["Xingqi Cui", "Chieh-Jan Mike Liang", "Jiarong Xing", "Haoran Qiu"], "title": "From Models to Operators: Rethinking Autoscaling Granularity for Large Generative Models", "comment": "16 pages, 13 figures", "summary": "Serving large generative models such as LLMs and multi- modal transformers\nrequires balancing user-facing SLOs (e.g., time-to-first-token,\ntime-between-tokens) with provider goals of efficiency and cost reduction.\nExisting solutions rely on static provisioning or model-level autoscaling, both\nof which treat the model as a monolith. This coarse-grained resource management\nleads to degraded performance or significant resource underutilization due to\npoor adaptability to dynamic inference traffic that is common online.\n  The root cause of this inefficiency lies in the internal structure of\ngenerative models: they are executed as graphs of interconnected operators.\nThrough detailed characterization and systematic analysis, we find that\noperators are heterogeneous in their compute and memory footprints and exhibit\ndiverse sensitivity to workload and resource factors such as batch size,\nsequence length, and traffic rate. This heterogeneity suggests that the\noperator, rather than the entire model, is the right granularity for scaling\ndecisions.\n  We propose an operator-level autoscaling framework, which allocates resources\nat finer (operator)-granularity, optimizing the scaling, batching, and\nplacement based on individual operator profiles. Evaluated on production-scale\ntraces, our approach preserves SLOs with up to 40% fewer GPUs and 35% less\nenergy, or under fixed resources achieves 1.6x higher throughput with 5% less\nenergy. These results show that the operator, rather than the model, is\nfundamentally a more effective unit for scaling large generative workloads."}
{"id": "2511.02285", "categories": ["cs.AR", "cs.PL", "cs.SE"], "pdf": "https://arxiv.org/pdf/2511.02285", "abs": "https://arxiv.org/abs/2511.02285", "authors": ["Zhuorui Zhao", "Bing Li", "Grace Li Zhang", "Ulf Schlichtmann"], "title": "VFocus: Better Verilog Generation from Large Language Model via Focused Reasoning", "comment": "accepted by SOCC 2025", "summary": "Large Language Models (LLMs) have shown impressive potential in generating\nVerilog codes, but ensuring functional correctness remains a challenge.\nExisting approaches often rely on self-consistency or simulation feedback to\nselect the best candidate, but they miss opportunities to focus LLM reasoning\non the most informative parts of the design. We propose VFocus, a three-stage\nframework that enhances Verilog generation by sharpening the focus of LLM\nreasoning onto critical decision points in the code generation process. In the\n\\textbf{pre-ranking stage}, VFocus generates multiple code candidates through\nLLM prompting, retries for syntactically valid outputs, and introduces a\n\\textit{Density-guided Filtering} to retain candidates that fall within the\n\"reasoning sweet spot\" for functional correctness. In the \\textbf{ranking\nstage}, we simulate each code candidate using an automatically generated\ntestbench and apply self-consistency-based clustering to identify the most\nconsistent outputs. Finally, in the \\textbf{post-ranking refinement stage},\nVFocus performs inconsistency mining on top-ranked candidates and invokes\nreasoning-augmented LLM prompts for candidate refinement. Experiments on the\nVerilogEval-Human benchmark show that VFocus significantly improves the pass@1\ncorrectness across multiple reasoning LLMs, demonstrating its effectiveness in\nenhancing Verilog generation for complex hardware design tasks."}
{"id": "2511.02257", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2511.02257", "abs": "https://arxiv.org/abs/2511.02257", "authors": ["Oguz Selvitopi", "Emin Ozturk", "Jie Chen", "Ponnuswamy Sadayappan", "Robert G. Edwards", "Aydın Buluç"], "title": "Fast Algorithms for Scheduling Many-body Correlation Functions on Accelerators", "comment": null, "summary": "Computation of correlation functions is a key operation in Lattice quantum\nchromodynamics (LQCD) simulations to extract nuclear physics observables. These\nfunctions involve many binary batch tensor contractions, each tensor possibly\noccupying hundreds of MBs of memory. Performing these contractions on GPU\naccelerators poses the challenge of scheduling them as to optimize tensor reuse\nand reduce data traffic. In this work we propose two fast novel scheduling\nalgorithms that reorder contractions to increase temporal locality via\ninput/intermediate tensor reuse. Our schedulers take advantage of\napplication-specific features, such as contractions being binary and locality\nwithin contraction trees, to optimize the objective of minimizing peak memory.\nWe integrate them into the LQCD analysis software suite Redstar and improve\ntime-to-solution. Our schedulers attain upto 2.1x improvement in peak memory,\nwhich is reflected by a reduction of upto 4.2x in evictions, upto 1.8x in data\ntraffic, resulting in upto 1.9x faster correlation function computation time."}
{"id": "2511.02293", "categories": ["cs.DC", "cs.CV", "C.2.4; I.2.10"], "pdf": "https://arxiv.org/pdf/2511.02293", "abs": "https://arxiv.org/abs/2511.02293", "authors": ["Taisuke Noguchi", "Takuya Azumi"], "title": "3D Point Cloud Object Detection on Edge Devices for Split Computing", "comment": "6 pages. This version includes minor lstlisting configuration\n  adjustments for successful compilation. No changes to content or layout.\n  Originally published at ACM/IEEE RAGE 2024", "summary": "The field of autonomous driving technology is rapidly advancing, with deep\nlearning being a key component. Particularly in the field of sensing, 3D point\ncloud data collected by LiDAR is utilized to run deep neural network models for\n3D object detection. However, these state-of-the-art models are complex,\nleading to longer processing times and increased power consumption on edge\ndevices. The objective of this study is to address these issues by leveraging\nSplit Computing, a distributed machine learning inference method. Split\nComputing aims to lessen the computational burden on edge devices, thereby\nreducing processing time and power consumption. Furthermore, it minimizes the\nrisk of data breaches by only transmitting intermediate data from the deep\nneural network model. Experimental results show that splitting after\nvoxelization reduces the inference time by 70.8% and the edge device execution\ntime by 90.0%. When splitting within the network, the inference time is reduced\nby up to 57.1%, and the edge device execution time is reduced by up to 69.5%."}
{"id": "2511.02647", "categories": ["cs.DC", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.02647", "abs": "https://arxiv.org/abs/2511.02647", "authors": ["Xiumei Deng", "Zehui Xiong", "Binbin Chen", "Dong In Kim", "Merouane Debbah", "H. Vincent Poor"], "title": "Federated Attention: A Distributed Paradigm for Collaborative LLM Inference over Edge Networks", "comment": null, "summary": "Large language models (LLMs) are proliferating rapidly at the edge,\ndelivering intelligent capabilities across diverse application scenarios.\nHowever, their practical deployment in collaborative scenarios confronts\nfundamental challenges: privacy vulnerabilities, communication overhead, and\ncomputational bottlenecks. To address these, we propose Federated Attention\n(FedAttn), which integrates the federated paradigm into the self-attention\nmechanism, creating a new distributed LLM inference framework that\nsimultaneously achieves privacy protection, communication efficiency, and\ncomputational efficiency. FedAttn enables participants to perform local\nself-attention over their own token representations while periodically\nexchanging and aggregating Key-Value (KV) matrices across multiple Transformer\nblocks, collaboratively generating LLM responses without exposing private\nprompts. Further, we identify a structural duality between contextual\nrepresentation refinement in FedAttn and parameter optimization in FL across\nprivate data, local computation, and global aggregation. This key insight\nprovides a principled foundation for systematically porting federated\noptimization techniques to collaborative LLM inference. Building on this\nframework, we theoretically analyze how local self-attention computation within\nparticipants and heterogeneous token relevance among participants shape error\npropagation dynamics across Transformer blocks. Moreover, we characterize the\nfundamental trade-off between response quality and communication/computation\nefficiency, which is governed by the synchronization interval and the number of\nparticipants. Experimental results validate our theoretical analysis, and\nreveal significant optimization opportunities through sparse attention and\nadaptive KV aggregation, highlighting FedAttn's potential to deliver\nscalability and efficiency in real-world edge deployments."}
{"id": "2511.02655", "categories": ["cs.DC", "cs.MS"], "pdf": "https://arxiv.org/pdf/2511.02655", "abs": "https://arxiv.org/abs/2511.02655", "authors": ["Johansell Villalobos", "Josef Ruzicka", "Silvio Rizzi"], "title": "Implementing Multi-GPU Scientific Computing Miniapps Across Performance Portable Frameworks", "comment": null, "summary": "Scientific computing in the exascale era demands increased computational\npower to solve complex problems across various domains. With the rise of\nheterogeneous computing architectures the need for vendor-agnostic, performance\nportability frameworks has been highlighted. Libraries like Kokkos have become\nessential for enabling high-performance computing applications to execute\nefficiently across different hardware platforms with minimal code changes. In\nthis direction, this paper presents preliminary time-to-solution results for\ntwo representative scientific computing applications: an N-body simulation and\na structured grid simulation. Both applications used a distributed memory\napproach and hardware acceleration through four performance portability\nframeworks: Kokkos, OpenMP, RAJA, and OCCA. Experiments conducted on a single\nnode of the Polaris supercomputer using four NVIDIA A100 GPUs revealed\nsignificant performance variability among frameworks. OCCA demonstrated faster\nexecution times for small-scale validation problems, likely due to JIT\ncompilation, however its lack of optimized reduction algorithms may limit\nscalability for larger simulations while using its out of the box API. OpenMP\nperformed poorly in the structured grid simulation most likely due to\ninefficiencies in inter-node data synchronization and communication. These\nfindings highlight the need for further optimization to maximize each\nframework's capabilities. Future work will focus on enhancing reduction\nalgorithms, data communication, memory management, as wells as performing\nscalability studies, and a comprehensive statistical analysis to evaluate and\ncompare framework performance."}
{"id": "2511.02743", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2511.02743", "abs": "https://arxiv.org/abs/2511.02743", "authors": ["Fedor Ryabinin", "Alexey Gotsman", "Pierre Sutra"], "title": "Making Democracy Work: Fixing and Simplifying Egalitarian Paxos (Extended Version)", "comment": "Extended version of a paper in OPODIS'25: International Conference on\n  Principles of Distributed Systems", "summary": "Classical state-machine replication protocols, such as Paxos, rely on a\ndistinguished leader process to order commands. Unfortunately, this approach\nmakes the leader a single point of failure and increases the latency for\nclients that are not co-located with it. As a response to these drawbacks,\nEgalitarian Paxos introduced an alternative, leaderless approach, that allows\nreplicas to order commands collaboratively. Not relying on a single leader\nallows the protocol to maintain non-zero throughput with up to $f$ crashes of\nany processes out of a total of $n = 2f+1$. The protocol furthermore allows any\nprocess to execute a command $c$ fast, in $2$ message delays, provided no more\nthan $e = \\lceil\\frac{f+1}{2}\\rceil$ other processes fail, and all concurrently\nsubmitted commands commute with $c$; the latter condition is often satisfied in\npractical systems.\n  Egalitarian Paxos has served as a foundation for many other replication\nprotocols. But unfortunately, the protocol is very complex, ambiguously\nspecified and suffers from nontrivial bugs. In this paper, we present EPaxos*\n-- a simpler and correct variant of Egalitarian Paxos. Our key technical\ncontribution is a simpler failure-recovery algorithm, which we have rigorously\nproved correct. Our protocol also generalizes Egalitarian Paxos to cover the\nwhole spectrum of failure thresholds $f$ and $e$ such that $n \\ge \\max\\{2e+f-1,\n2f+1\\}$ -- the number of processes that we show to be optimal."}
{"id": "2511.02132", "categories": ["cs.AR", "cs.DC", "cs.LG", "cs.PF"], "pdf": "https://arxiv.org/pdf/2511.02132", "abs": "https://arxiv.org/abs/2511.02132", "authors": ["Mansi Choudhary", "Karthik Sangaiah", "Sonali Singh", "Muhammad Osama", "Lisa Wu Wills", "Ganesh Dasika"], "title": "Optimizing Attention on GPUs by Exploiting GPU Architectural NUMA Effects", "comment": "11 pages, 14 figures", "summary": "The rise of disaggregated AI GPUs has exposed a critical bottleneck in\nlarge-scale attention workloads: non-uniform memory access (NUMA). As\nmulti-chiplet designs become the norm for scaling compute capabilities, memory\nlatency and bandwidth vary sharply across compute regions, undermining the\nperformance of traditional GPU kernel scheduling strategies that assume uniform\nmemory access. We identify how these NUMA effects distort locality in\nmulti-head attention (MHA) and present Swizzled Head-first Mapping, a\nspatially-aware scheduling strategy that aligns attention heads with GPU NUMA\ndomains to exploit intra-chiplet cache reuse. On AMD's MI300X architecture, our\nmethod achieves up to 50% higher performance over state-of-the-art attention\nalgorithms using conventional scheduling techniques and sustains consistently\nhigh L2 cache hit rates of 80-97%. These results demonstrate that NUMA-aware\nscheduling is now fundamental to achieving full efficiency on next-generation\ndisaggregated GPUs, offering a path forward for scalable AI training and\ninference."}
{"id": "2511.02501", "categories": ["cs.NI", "cs.DC"], "pdf": "https://arxiv.org/pdf/2511.02501", "abs": "https://arxiv.org/abs/2511.02501", "authors": ["Mohan Liyanage", "Eldiyar Zhantileuov", "Ali Kadhum Idrees", "Rolf Schuster"], "title": "Lightweight Latency Prediction Scheme for Edge Applications: A Rational Modelling Approach", "comment": "Presented at the ICCS 2025 - 5th International Conference on Computer\n  Systems, Xian, China", "summary": "Accurately predicting end-to-end network latency is essential for enabling\nreliable task offloading in real-time edge computing applications. This paper\nintroduces a lightweight latency prediction scheme based on rational modelling\nthat uses features such as frame size, arrival rate, and link utilization,\neliminating the need for intrusive active probing. The model achieves\nstate-of-the-art prediction accuracy through extensive experiments and 5-fold\ncross-validation (MAE = 0.0115, R$^2$ = 0.9847) with competitive inference\ntime, offering a substantial trade-off between precision and efficiency\ncompared to traditional regressors and neural networks."}
