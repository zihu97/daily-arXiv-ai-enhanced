{"id": "2511.05789", "categories": ["cs.NI"], "pdf": "https://arxiv.org/pdf/2511.05789", "abs": "https://arxiv.org/abs/2511.05789", "authors": ["Shanhao Zhan", "Zhang Liu", "Lianfen Huang", "Shaowei Shen", "Ziyang Bai", "Zhibin Gao", "Dusit Niyato"], "title": "Digital Twin-Assisted Task Offloading and Resource Allocation in ISAC-Enabled Internet of Vehicles", "comment": "17 pages,7 figures, transactions paper", "summary": "The convergence of the Internet of vehicles (IoV) and 6G networks is driving the evolution of next-generation intelligent transportation systems. However, IoV networks face persistent challenges, including low spectral efficiency in vehicular communications, difficulty in achieving dynamic and adaptive resource optimization, and the need for long-term stability under highly dynamic environments. In this paper, we study the problem of digital twin (DT)-assisted task offloading and resource allocation in integrated sensing and communication (ISAC)-enabled IoV networks. The objective is to minimize the long-term average system cost, defined as a weighted combination of delay and energy consumption, while ensuring queue stability over time. To address this, we employ an ISAC-enabled design and introduce two transmission modes (i.e., raw data transmission (DataT) and instruction transmission (InstrT)). The InstrT mode enables instruction-level transmission, thereby reducing data volume and improving spectral efficiency. We then employ Lyapunov optimization to decompose the long-term stochastic problem into per-slot deterministic problems, ensuring long-term queue stability. Building upon this, we propose a Lyapunov-driven DT-enhanced multi-agent proximal policy optimization (Ly-DTMPPO) algorithm, which leverages DT for global state awareness and intelligent decision-making within a centralized training and decentralized execution (CTDE) architecture. Extensive simulations verify that Ly-DTMPPO achieves superior performance compared with existing benchmarks.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6570\u5b57\u5b6a\u751f\uff08DT\uff09\u8f85\u52a9\u7684\u96c6\u6210\u611f\u77e5\u4e0e\u901a\u4fe1\uff08ISAC\uff09\u8f66\u8054\u7f51\u4efb\u52a1\u5378\u8f7d\u4e0e\u8d44\u6e90\u5206\u914d\u65b9\u6cd5\uff0c\u901a\u8fc7\u5f15\u5165\u6307\u4ee4\u7ea7\u4f20\u8f93\u6a21\u5f0f\u548cLyapunov\u4f18\u5316\u6846\u67b6\uff0c\u7ed3\u5408\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5Ly-DTMPPO\uff0c\u5728\u4fdd\u8bc1\u961f\u5217\u957f\u671f\u7a33\u5b9a\u7684\u524d\u63d0\u4e0b\u6709\u6548\u964d\u4f4e\u4e86\u7cfb\u7edf\u65f6\u5ef6\u4e0e\u80fd\u8017\u3002", "motivation": "\u8f66\u8054\u7f51\uff08IoV\uff09\u57286G\u65f6\u4ee3\u9762\u4e34\u9891\u8c31\u6548\u7387\u4f4e\u3001\u52a8\u6001\u8d44\u6e90\u4f18\u5316\u56f0\u96be\u4ee5\u53ca\u9ad8\u52a8\u6001\u73af\u5883\u4e0b\u957f\u671f\u7a33\u5b9a\u6027\u4e0d\u8db3\u7b49\u6311\u6218\uff0c\u4e9f\u9700\u4e00\u79cd\u517c\u987e\u6027\u80fd\u4e0e\u7a33\u5b9a\u6027\u7684\u667a\u80fd\u8d44\u6e90\u7ba1\u7406\u673a\u5236\u3002", "method": "\u4f5c\u8005\u6784\u5efa\u4e86\u4e00\u4e2aISAC\u8d4b\u80fd\u7684IoV\u7cfb\u7edf\u6a21\u578b\uff0c\u5f15\u5165\u539f\u59cb\u6570\u636e\u4f20\u8f93\uff08DataT\uff09\u548c\u6307\u4ee4\u4f20\u8f93\uff08InstrT\uff09\u4e24\u79cd\u6a21\u5f0f\uff0c\u5e76\u91c7\u7528Lyapunov\u4f18\u5316\u5c06\u957f\u671f\u968f\u673a\u95ee\u9898\u5206\u89e3\u4e3a\u6bcf\u65f6\u9699\u786e\u5b9a\u6027\u5b50\u95ee\u9898\uff1b\u5728\u6b64\u57fa\u7840\u4e0a\uff0c\u8bbe\u8ba1\u4e86Lyapunov\u9a71\u52a8\u7684DT\u589e\u5f3a\u578b\u591a\u667a\u80fd\u4f53\u8fd1\u7aef\u7b56\u7565\u4f18\u5316\u7b97\u6cd5\uff08Ly-DTMPPO\uff09\uff0c\u5229\u7528\u6570\u5b57\u5b6a\u751f\u5b9e\u73b0\u5168\u5c40\u72b6\u6001\u611f\u77e5\uff0c\u5e76\u5728\u96c6\u4e2d\u8bad\u7ec3-\u5206\u6563\u6267\u884c\uff08CTDE\uff09\u67b6\u6784\u4e0b\u8fdb\u884c\u667a\u80fd\u51b3\u7b56\u3002", "result": "\u4eff\u771f\u5b9e\u9a8c\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684Ly-DTMPPO\u7b97\u6cd5\u5728\u7cfb\u7edf\u6210\u672c\uff08\u65f6\u5ef6\u4e0e\u80fd\u8017\u52a0\u6743\u548c\uff09\u65b9\u9762\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u57fa\u51c6\u65b9\u6cd5\uff0c\u540c\u65f6\u4fdd\u969c\u4e86\u961f\u5217\u7684\u957f\u671f\u7a33\u5b9a\u6027\u3002", "conclusion": "\u878d\u5408\u6570\u5b57\u5b6a\u751f\u4e0eLyapunov\u4f18\u5316\u7684\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u80fd\u6709\u6548\u63d0\u5347ISAC\u8d4b\u80fd\u8f66\u8054\u7f51\u4e2d\u7684\u8d44\u6e90\u5229\u7528\u6548\u7387\u4e0e\u7cfb\u7edf\u7a33\u5b9a\u6027\uff0c\u4e3a\u672a\u6765\u667a\u80fd\u4ea4\u901a\u7cfb\u7edf\u63d0\u4f9b\u4e86\u53ef\u884c\u7684\u6280\u672f\u8def\u5f84\u3002"}}
{"id": "2511.05626", "categories": ["cs.SE", "cs.AI", "cs.DC"], "pdf": "https://arxiv.org/pdf/2511.05626", "abs": "https://arxiv.org/abs/2511.05626", "authors": ["Caetano Melone", "Daniel Nichols", "Konstantinos Parasyris", "Todd Gamblin", "Harshitha Menon"], "title": "LLMs as Packagers of HPC Software", "comment": null, "summary": "High performance computing (HPC) software ecosystems are inherently heterogeneous, comprising scientific applications that depend on hundreds of external packages, each with distinct build systems, options, and dependency constraints. Tools such as Spack automate dependency resolution and environment management, but their effectiveness relies on manually written build recipes. As these ecosystems grow, maintaining existing specifications and creating new ones becomes increasingly labor-intensive. While large language models (LLMs) have shown promise in code generation, automatically producing correct and maintainable Spack recipes remains a significant challenge. We present a systematic analysis of how LLMs and context-augmentation methods can assist in the generation of Spack recipes. To this end, we introduce SpackIt, an end-to-end framework that combines repository analysis, retrieval of relevant examples, and iterative refinement through diagnostic feedback. We apply SpackIt to a representative subset of 308 open-source HPC packages to assess its effectiveness and limitations. Our results show that SpackIt increases installation success from 20% in a zero-shot setting to over 80% in its best configuration, demonstrating the value of retrieval and structured feedback for reliable package synthesis.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86SpackIt\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u5408\u4ee3\u7801\u4ed3\u5e93\u5206\u6790\u3001\u793a\u4f8b\u68c0\u7d22\u548c\u8bca\u65ad\u53cd\u9988\u8fed\u4ee3\u4f18\u5316\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5927\u8bed\u8a00\u6a21\u578b\u81ea\u52a8\u751f\u6210Spack\u6784\u5efa\u914d\u65b9\u7684\u6210\u529f\u7387\uff0c\u4ece\u96f6\u6837\u672c\u768420%\u63d0\u9ad8\u5230\u6700\u4f73\u914d\u7f6e\u4e0b\u768480%\u4ee5\u4e0a\u3002", "motivation": "\u9ad8\u6027\u80fd\u8ba1\u7b97\uff08HPC\uff09\u8f6f\u4ef6\u751f\u6001\u9ad8\u5ea6\u5f02\u6784\uff0c\u4f9d\u8d56\u5927\u91cf\u5916\u90e8\u5305\uff0c\u5176\u6784\u5efa\u914d\u65b9\u9700\u624b\u52a8\u7f16\u5199\u4e14\u7ef4\u62a4\u6210\u672c\u9ad8\uff1b\u5c3d\u7ba1\u5927\u8bed\u8a00\u6a21\u578b\u5728\u4ee3\u7801\u751f\u6210\u65b9\u9762\u6709\u6f5c\u529b\uff0c\u4f46\u81ea\u52a8\u751f\u6210\u6b63\u786e\u4e14\u53ef\u7ef4\u62a4\u7684Spack\u914d\u65b9\u4ecd\u5177\u6311\u6218\u3002", "method": "\u63d0\u51faSpackIt\u7aef\u5230\u7aef\u6846\u67b6\uff0c\u6574\u5408\u4ed3\u5e93\u5206\u6790\u3001\u76f8\u5173\u793a\u4f8b\u68c0\u7d22\u548c\u57fa\u4e8e\u8bca\u65ad\u53cd\u9988\u7684\u8fed\u4ee3\u4f18\u5316\uff0c\u7528\u4e8e\u8f85\u52a9\u5927\u8bed\u8a00\u6a21\u578b\u751f\u6210Spack\u6784\u5efa\u914d\u65b9\u3002", "result": "\u5728308\u4e2a\u5f00\u6e90HPC\u5305\u7684\u6d4b\u8bd5\u96c6\u4e0a\uff0cSpackIt\u5c06\u5b89\u88c5\u6210\u529f\u7387\u4ece\u96f6\u6837\u672c\u8bbe\u7f6e\u768420%\u63d0\u5347\u81f3\u6700\u4f73\u914d\u7f6e\u4e0b\u768480%\u4ee5\u4e0a\u3002", "conclusion": "\u68c0\u7d22\u673a\u5236\u4e0e\u7ed3\u6784\u5316\u53cd\u9988\u80fd\u663e\u8457\u63d0\u5347\u5927\u8bed\u8a00\u6a21\u578b\u5728\u590d\u6742\u6784\u5efa\u7cfb\u7edf\u4e2d\u751f\u6210\u53ef\u9760\u8f6f\u4ef6\u5305\u914d\u65b9\u7684\u80fd\u529b\uff0c\u4e3a\u81ea\u52a8\u5316HPC\u8f6f\u4ef6\u751f\u6001\u7ef4\u62a4\u63d0\u4f9b\u4e86\u6709\u6548\u8def\u5f84\u3002"}}
{"id": "2511.06052", "categories": ["cs.DC", "cs.PF"], "pdf": "https://arxiv.org/pdf/2511.06052", "abs": "https://arxiv.org/abs/2511.06052", "authors": ["Philipp Schaad", "Tal Ben-Nun", "Patrick Iff", "Torsten Hoefler"], "title": "Inductive Loop Analysis for Practical HPC Application Optimization", "comment": null, "summary": "Scientific computing applications heavily rely on multi-level loop nests operating on multidimensional arrays. This presents multiple optimization opportunities from exploiting parallelism to reducing data movement through prefetching and improved register usage. HPC frameworks often delegate fine-grained data movement optimization to compilers, but their low-level representations hamper analysis of common patterns, such as strided data accesses and loop-carried dependencies. In this paper, we introduce symbolic, inductive loop optimization (SILO), a novel technique that models data accesses and dependencies as functions of loop nest strides. This abstraction enables the automatic parallelization of sequentially-dependent loops, as well as data movement optimizations including software prefetching and pointer incrementation to reduce register spills. We demonstrate SILO on fundamental kernels from scientific applications with a focus on atmospheric models and numerical solvers, achieving up to 12$\\times$ speedup over the state of the art.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aSILO\u7684\u65b0\u6280\u672f\uff0c\u901a\u8fc7\u5c06\u6570\u636e\u8bbf\u95ee\u548c\u4f9d\u8d56\u5173\u7cfb\u5efa\u6a21\u4e3a\u5faa\u73af\u6b65\u957f\u7684\u51fd\u6570\uff0c\u5b9e\u73b0\u4e86\u81ea\u52a8\u5e76\u884c\u5316\u548c\u6570\u636e\u79fb\u52a8\u4f18\u5316\uff0c\u5728\u79d1\u5b66\u8ba1\u7b97\u6838\u5fc3\u5e94\u7528\u4e2d\u53d6\u5f97\u4e86\u6700\u9ad812\u500d\u7684\u52a0\u901f\u3002", "motivation": "\u9ad8\u6027\u80fd\u8ba1\u7b97\u6846\u67b6\u901a\u5e38\u5c06\u7ec6\u7c92\u5ea6\u7684\u6570\u636e\u79fb\u52a8\u4f18\u5316\u4ea4\u7ed9\u7f16\u8bd1\u5668\u5904\u7406\uff0c\u4f46\u5176\u5e95\u5c42\u8868\u793a\u96be\u4ee5\u5206\u6790\u5e38\u89c1\u7684\u6a21\u5f0f\uff08\u5982\u8de8\u6b65\u6570\u636e\u8bbf\u95ee\u548c\u5faa\u73af\u95f4\u4f9d\u8d56\uff09\uff0c\u9650\u5236\u4e86\u4f18\u5316\u6548\u679c\u3002", "method": "\u63d0\u51fa\u7b26\u53f7\u5f52\u7eb3\u5faa\u73af\u4f18\u5316\uff08SILO\uff09\u65b9\u6cd5\uff0c\u5c06\u6570\u636e\u8bbf\u95ee\u4e0e\u4f9d\u8d56\u5173\u7cfb\u62bd\u8c61\u4e3a\u5faa\u73af\u5d4c\u5957\u6b65\u957f\u7684\u51fd\u6570\uff0c\u4ece\u800c\u652f\u6301\u81ea\u52a8\u5e76\u884c\u5316\u3001\u8f6f\u4ef6\u9884\u53d6\u548c\u6307\u9488\u9012\u589e\u7b49\u4f18\u5316\u3002", "result": "\u5728\u5927\u6c14\u6a21\u578b\u548c\u6570\u503c\u6c42\u89e3\u5668\u7b49\u79d1\u5b66\u8ba1\u7b97\u6838\u5fc3\u7a0b\u5e8f\u4e0a\u9a8c\u8bc1\u4e86SILO\u7684\u6709\u6548\u6027\uff0c\u76f8\u6bd4\u73b0\u6709\u6280\u672f\u6700\u9ad8\u83b7\u5f9712\u500d\u6027\u80fd\u63d0\u5347\u3002", "conclusion": "SILO\u901a\u8fc7\u9ad8\u5c42\u6b21\u62bd\u8c61\u6709\u6548\u89e3\u51b3\u4e86\u4f20\u7edf\u7f16\u8bd1\u5668\u5728\u591a\u5c42\u5faa\u73af\u5d4c\u5957\u4e2d\u96be\u4ee5\u8bc6\u522b\u548c\u4f18\u5316\u6570\u636e\u8bbf\u95ee\u6a21\u5f0f\u7684\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u79d1\u5b66\u8ba1\u7b97\u5e94\u7528\u7684\u6027\u80fd\u3002"}}
{"id": "2511.05663", "categories": ["cs.SE", "physics.acc-ph"], "pdf": "https://arxiv.org/pdf/2511.05663", "abs": "https://arxiv.org/abs/2511.05663", "authors": ["M. Gonzalez", "M. Acosta"], "title": "Accelerating Control Systems with GitOps: A Path to Automation and Reliability", "comment": null, "summary": "GitOps is a foundational approach for modernizing infrastructure by leveraging Git as the single source of truth for declarative configurations. The poster explores how GitOps transforms traditional control system infrastructure, services and applications by enabling fully automated, auditable, and version-controlled infrastructure management. Cloud-native and containerized environments are shifting the ecosystem not only in the IT industry but also within the computational science field, as is the case of CERN [1] and Diamond Light Source [2] among other Accelerator/Science facilities which are slowly shifting towards modern software and infrastructure paradigms. The ACORN project, which aims to modernize Fermilab's control system infrastructure and software is implementing proven best-practices and cutting-edge technology standards including GitOps, containerization, infrastructure as code and modern data pipelines for control system data acquisition and the inclusion of AI/ML in our accelerator complex.", "AI": {"tldr": "\u8be5\u8bba\u6587\u4ecb\u7ecd\u4e86GitOps\u5982\u4f55\u901a\u8fc7\u5c06Git\u4f5c\u4e3a\u58f0\u660e\u5f0f\u914d\u7f6e\u7684\u552f\u4e00\u771f\u5b9e\u6765\u6e90\uff0c\u63a8\u52a8\u63a7\u5236\u7cfb\u7edf\u7684\u57fa\u7840\u8bbe\u65bd\u73b0\u4ee3\u5316\uff0c\u5e76\u4ee5Fermilab\u7684ACORN\u9879\u76ee\u4e3a\u4f8b\uff0c\u5c55\u793a\u4e86GitOps\u3001\u5bb9\u5668\u5316\u3001\u57fa\u7840\u8bbe\u65bd\u5373\u4ee3\u7801\u53ca\u73b0\u4ee3\u6570\u636e\u7ba1\u9053\u7b49\u6280\u672f\u5728\u52a0\u901f\u5668\u63a7\u5236\u4f53\u7cfb\u4e2d\u7684\u5e94\u7528\u3002", "motivation": "\u4f20\u7edf\u63a7\u5236\u7cfb\u7edf\u57fa\u7840\u8bbe\u65bd\u96be\u4ee5\u6ee1\u8db3\u73b0\u4ee3\u79d1\u7814\u8bbe\u65bd\u5bf9\u81ea\u52a8\u5316\u3001\u53ef\u5ba1\u8ba1\u6027\u548c\u7248\u672c\u63a7\u5236\u7684\u9700\u6c42\uff0c\u56e0\u6b64\u9700\u8981\u501f\u52a9GitOps\u7b49\u4e91\u539f\u751f\u65b9\u6cd5\u5b9e\u73b0\u73b0\u4ee3\u5316\u8f6c\u578b\u3002", "method": "\u91c7\u7528GitOps\u3001\u5bb9\u5668\u5316\u3001\u57fa\u7840\u8bbe\u65bd\u5373\u4ee3\u7801\uff08IaC\uff09\u4ee5\u53ca\u73b0\u4ee3\u6570\u636e\u7ba1\u9053\u7b49\u4e1a\u754c\u6700\u4f73\u5b9e\u8df5\u4e0e\u524d\u6cbf\u6280\u672f\u6807\u51c6\uff0c\u7528\u4e8e\u63a7\u5236\u7cfb\u7edf\u7684\u57fa\u7840\u8bbe\u65bd\u4e0e\u8f6f\u4ef6\u73b0\u4ee3\u5316\u3002", "result": "ACORN\u9879\u76ee\u6210\u529f\u5c06GitOps\u7b49\u73b0\u4ee3\u8f6f\u4ef6\u5de5\u7a0b\u5b9e\u8df5\u5f15\u5165Fermilab\u52a0\u901f\u5668\u63a7\u5236\u7cfb\u7edf\uff0c\u4e3a\u63a7\u5236\u6570\u636e\u91c7\u96c6\u548cAI/ML\u96c6\u6210\u5960\u5b9a\u57fa\u7840\u3002", "conclusion": "GitOps\u53ca\u5176\u914d\u5957\u6280\u672f\u4e3a\u79d1\u7814\u673a\u6784\u7684\u63a7\u5236\u7cfb\u7edf\u73b0\u4ee3\u5316\u63d0\u4f9b\u4e86\u6709\u6548\u8def\u5f84\uff0c\u63d0\u5347\u4e86\u57fa\u7840\u8bbe\u65bd\u7684\u81ea\u52a8\u5316\u6c34\u5e73\u3001\u53ef\u8ffd\u6eaf\u6027\u4e0e\u53ef\u7ef4\u62a4\u6027\u3002"}}
{"id": "2511.05843", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2511.05843", "abs": "https://arxiv.org/abs/2511.05843", "authors": ["Hanzheng Lyu", "Shaokang Xie", "Jianyu Niu", "Mohammad Sadoghi", "Yinqian Zhang", "Cong Wang", "Ivan Beschastnikh", "Chen Feng"], "title": "HYDRA: Breaking the Global Ordering Barrier in Multi-BFT Consensus", "comment": null, "summary": "Multi-Byzantine Fault Tolerant (Multi-BFT) consensus, which runs multiple BFT instances in parallel, has recently emerged as a promising approach to overcome the leader bottleneck in classical BFT protocols. However, existing designs rely on a global ordering layer to serialize blocks across instances, an intuitive yet costly mechanism that constrains scalability, amplifies failure propagation, and complicates deployment. In this paper, we challenge this conventional wisdom. We present HYDRA, the first Multi-BFT consensus framework that eliminates global ordering altogether. HYDRA introduces an object-centric execution model that partitions transactions by their accessed objects, enabling concurrent yet deterministic execution across instances. To ensure consistency, HYDRA combines lightweight lock-based coordination with a deadlock resolution mechanism, achieving both scalability and correctness. We implement HYDRA and evaluate it on up to 128 replicas in both LAN and WAN environments. Experimental results show HYDRA outperforms several state-of-the-art Multi-BFT protocols in the presence of a straggler. These results demonstrate strong consistency and high performance by removing global ordering, opening a new direction toward scalable Multi-BFT consensus design.", "AI": {"tldr": "HYDRA is a novel Multi-BFT consensus framework that removes the need for global ordering by using an object-centric execution model with lightweight coordination, achieving better scalability and performance than existing approaches.", "motivation": "Existing Multi-BFT protocols rely on a global ordering layer to serialize blocks across parallel BFT instances, which limits scalability, increases failure propagation, and complicates deployment.", "method": "HYDRA partitions transactions by accessed objects and enables concurrent deterministic execution across BFT instances, using lightweight lock-based coordination and deadlock resolution to ensure consistency without global ordering.", "result": "Experiments with up to 128 replicas in LAN and WAN settings show HYDRA outperforms state-of-the-art Multi-BFT protocols, especially in the presence of stragglers, while maintaining strong consistency.", "conclusion": "Eliminating global ordering in Multi-BFT consensus is feasible and beneficial; HYDRA demonstrates that scalable, high-performance, and strongly consistent Multi-BFT can be achieved through object-centric execution and lightweight coordination."}}
{"id": "2511.06090", "categories": ["cs.SE", "cs.AI", "cs.PF"], "pdf": "https://arxiv.org/pdf/2511.06090", "abs": "https://arxiv.org/abs/2511.06090", "authors": ["Jeffrey Jian Ma", "Milad Hashemi", "Amir Yazdanbakhsh", "Kevin Swersky", "Ofir Press", "Enhui Li", "Vijay Janapa Reddi", "Parthasarathy Ranganathan"], "title": "SWE-fficiency: Can Language Models Optimize Real-World Repositories on Real Workloads?", "comment": "Data, code, and leaderboard are available at https://swefficiency.com/", "summary": "Optimizing the performance of large-scale software repositories demands expertise in code reasoning and software engineering (SWE) to reduce runtime while preserving program correctness. However, most benchmarks emphasize what to fix rather than how to fix code. We introduce \\textsc{SWE-fficiency}, a benchmark for evaluating repository-level performance optimization on real workloads. Our suite contains 498 tasks across nine widely used data-science, machine-learning, and HPC repositories (e.g., numpy, pandas, scipy): given a complete codebase and a slow workload, an agent must investigate code semantics, localize bottlenecks and relevant tests, and produce a patch that matches or exceeds expert speedup while passing the same unit tests. To enable this how-to-fix evaluation, our automated pipeline scrapes GitHub pull requests for performance-improving edits, combining keyword filtering, static analysis, coverage tooling, and execution validation to both confirm expert speedup baselines and identify relevant repository unit tests. Empirical evaluation of state-of-the-art agents reveals significant underperformance. On average, agents achieve less than 0.15x the expert speedup: agents struggle in localizing optimization opportunities, reasoning about execution across functions, and maintaining correctness in proposed edits. We release the benchmark and accompanying data pipeline to facilitate research on automated performance engineering and long-horizon software reasoning.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86 SWE-fficiency \u57fa\u51c6\uff0c\u7528\u4e8e\u8bc4\u4f30\u667a\u80fd\u4f53\u5728\u771f\u5b9e\u4ee3\u7801\u5e93\u4e2d\u8fdb\u884c\u6027\u80fd\u4f18\u5316\u7684\u80fd\u529b\uff0c\u5f3a\u8c03\u201c\u5982\u4f55\u4fee\u590d\u201d\u800c\u975e\u4ec5\u8bc6\u522b\u95ee\u9898\u3002\u8be5\u57fa\u51c6\u5305\u542b\u6765\u81ea9\u4e2a\u6d41\u884c\u4ed3\u5e93\u7684498\u9879\u4efb\u52a1\uff0c\u8981\u6c42\u667a\u80fd\u4f53\u5206\u6790\u8bed\u4e49\u3001\u5b9a\u4f4d\u74f6\u9888\u5e76\u751f\u6210\u6709\u6548\u8865\u4e01\u3002\u5b9e\u9a8c\u8868\u660e\u5f53\u524d\u5148\u8fdb\u667a\u80fd\u4f53\u5728\u6b64\u4efb\u52a1\u4e0a\u8868\u73b0\u4e0d\u4f73\uff0c\u5e73\u5747\u4ec5\u8fbe\u5230\u4e13\u5bb6\u63d0\u901f\u6548\u679c\u768415%\u3002", "motivation": "\u73b0\u6709\u57fa\u51c6\u591a\u5173\u6ce8\u201c\u4fee\u590d\u4ec0\u4e48\u201d\uff0c\u800c\u7f3a\u4e4f\u5bf9\u201c\u5982\u4f55\u4fee\u590d\u201d\u6027\u80fd\u95ee\u9898\u7684\u8bc4\u4f30\uff0c\u5c24\u5176\u5728\u5927\u89c4\u6a21\u8f6f\u4ef6\u4ed3\u5e93\u4e2d\u7f3a\u5c11\u8861\u91cf\u81ea\u52a8\u5316\u6027\u80fd\u4f18\u5316\u80fd\u529b\u7684\u6807\u51c6\u3002", "method": "\u6784\u5efa SWE-fficiency \u57fa\u51c6\uff0c\u901a\u8fc7\u81ea\u52a8\u5316\u6d41\u7a0b\u4ece GitHub PR \u4e2d\u63d0\u53d6\u6027\u80fd\u4f18\u5316\u63d0\u4ea4\uff0c\u7ed3\u5408\u5173\u952e\u8bcd\u8fc7\u6ee4\u3001\u9759\u6001\u5206\u6790\u3001\u8986\u76d6\u7387\u5de5\u5177\u548c\u6267\u884c\u9a8c\u8bc1\uff0c\u751f\u6210\u5305\u542b\u5b8c\u6574\u4ee3\u7801\u5e93\u3001\u6162\u901f\u5de5\u4f5c\u8d1f\u8f7d\u3001\u4e13\u5bb6\u63d0\u901f\u57fa\u7ebf\u548c\u76f8\u5173\u5355\u5143\u6d4b\u8bd5\u7684\u4efb\u52a1\u96c6\u3002", "result": "\u5bf9\u5f53\u524d\u5148\u8fdb\u667a\u80fd\u4f53\u7684\u8bc4\u4f30\u663e\u793a\u5176\u5e73\u5747\u4ec5\u5b9e\u73b0\u4e13\u5bb6\u63d0\u901f\u6548\u679c\u7684\u4e0d\u52300.15\u500d\uff0c\u5728\u5b9a\u4f4d\u4f18\u5316\u70b9\u3001\u8de8\u51fd\u6570\u6267\u884c\u63a8\u7406\u548c\u4fdd\u6301\u8865\u4e01\u6b63\u786e\u6027\u65b9\u9762\u5b58\u5728\u663e\u8457\u56f0\u96be\u3002", "conclusion": "SWE-fficiency \u4e3a\u81ea\u52a8\u5316\u6027\u80fd\u5de5\u7a0b\u548c\u957f\u7a0b\u8f6f\u4ef6\u63a8\u7406\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u57fa\u51c6\u548c\u6570\u636e\u7ba1\u9053\uff0c\u63ed\u793a\u4e86\u5f53\u524d\u667a\u80fd\u4f53\u5728\u5b9e\u9645\u6027\u80fd\u4f18\u5316\u4efb\u52a1\u4e2d\u7684\u4e0d\u8db3\u3002"}}
{"id": "2511.07176", "categories": ["cs.NI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2511.07176", "abs": "https://arxiv.org/abs/2511.07176", "authors": ["Hanlin Cai", "Houtianfu Wang", "Haofan Dong", "Kai Li", "Ozgur B. Akan"], "title": "Graph Representation-based Model Poisoning on the Heterogeneous Internet of Agents", "comment": "6 pages, 6 figures", "summary": "Internet of Agents (IoA) envisions a unified, agent-centric paradigm where heterogeneous large language model (LLM) agents can interconnect and collaborate at scale. Within this paradigm, federated learning (FL) serves as a key enabler that allows distributed LLM agents to co-train global models without centralizing data. However, the FL-enabled IoA system remains vulnerable to model poisoning attacks, and the prevailing distance and similarity-based defenses become fragile at billion-parameter scale and under heterogeneous data distributions. This paper proposes a graph representation-based model poisoning (GRMP) attack, which passively exploits observed benign local models to construct a parameter correlation graph and extends an adversarial variational graph autoencoder to capture and reshape higher-order dependencies. The GRMP attack synthesizes malicious local models that preserve benign-like statistics while embedding adversarial objectives, remaining elusive to detection at the server. Experiments demonstrate a gradual drop in system accuracy under the proposed attack and the ineffectiveness of the prevailing defense mechanism in detecting the attack, underscoring a severe threat to the ambitious IoA paradigm.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u56fe\u8868\u793a\u7684\u6a21\u578b\u6295\u6bd2\u653b\u51fb\uff08GRMP\uff09\uff0c\u901a\u8fc7\u6784\u5efa\u53c2\u6570\u76f8\u5173\u56fe\u5e76\u5229\u7528\u53d8\u5206\u56fe\u81ea\u7f16\u7801\u5668\u91cd\u5851\u9ad8\u9636\u4f9d\u8d56\u5173\u7cfb\uff0c\u5728\u8054\u90a6\u5b66\u4e60\u9a71\u52a8\u7684\u4e92\u8054\u7f51\u667a\u80fd\u4f53\uff08IoA\uff09\u7cfb\u7edf\u4e2d\u751f\u6210\u96be\u4ee5\u88ab\u68c0\u6d4b\u7684\u6076\u610f\u672c\u5730\u6a21\u578b\uff0c\u4ece\u800c\u5bf9\u7cfb\u7edf\u51c6\u786e\u6027\u9020\u6210\u6301\u7eed\u635f\u5bb3\uff0c\u5e76\u63ed\u793a\u4e86\u73b0\u6709\u9632\u5fa1\u673a\u5236\u5728\u5927\u89c4\u6a21\u5f02\u6784\u73af\u5883\u4e0b\u7684\u8106\u5f31\u6027\u3002", "motivation": "\u8054\u90a6\u5b66\u4e60\u867d\u80fd\u652f\u6301\u5206\u5e03\u5f0f\u5927\u8bed\u8a00\u6a21\u578b\u667a\u80fd\u4f53\u534f\u540c\u8bad\u7ec3\uff0c\u4f46\u5728\u4e92\u8054\u7f51\u667a\u80fd\u4f53\uff08IoA\uff09\u8303\u5f0f\u4e0b\u9762\u4e34\u6a21\u578b\u6295\u6bd2\u653b\u51fb\u7684\u98ce\u9669\uff0c\u800c\u73b0\u6709\u7684\u57fa\u4e8e\u8ddd\u79bb\u548c\u76f8\u4f3c\u6027\u7684\u9632\u5fa1\u65b9\u6cd5\u5728\u5341\u4ebf\u53c2\u6570\u89c4\u6a21\u548c\u6570\u636e\u5f02\u6784\u73af\u5883\u4e0b\u6548\u679c\u6709\u9650\uff0c\u4e9f\u9700\u7814\u7a76\u66f4\u9690\u853d\u3001\u66f4\u5177\u7834\u574f\u529b\u7684\u653b\u51fb\u65b9\u5f0f\u4ee5\u8bc4\u4f30\u7cfb\u7edf\u5b89\u5168\u6027\u3002", "method": "\u63d0\u51faGRMP\u653b\u51fb\u65b9\u6cd5\uff1a\u9996\u5148\u88ab\u52a8\u89c2\u5bdf\u826f\u6027\u672c\u5730\u6a21\u578b\uff0c\u6784\u5efa\u53c2\u6570\u76f8\u5173\u56fe\uff1b\u7136\u540e\u5f15\u5165\u5bf9\u6297\u6027\u53d8\u5206\u56fe\u81ea\u7f16\u7801\u5668\uff0c\u6355\u83b7\u5e76\u91cd\u5851\u53c2\u6570\u95f4\u7684\u9ad8\u9636\u4f9d\u8d56\u5173\u7cfb\uff1b\u6700\u7ec8\u5408\u6210\u5728\u7edf\u8ba1\u7279\u5f81\u4e0a\u7c7b\u4f3c\u826f\u6027\u6a21\u578b\u4f46\u5d4c\u5165\u5bf9\u6297\u76ee\u6807\u7684\u6076\u610f\u672c\u5730\u6a21\u578b\uff0c\u4ee5\u89c4\u907f\u670d\u52a1\u5668\u7aef\u68c0\u6d4b\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cGRMP\u653b\u51fb\u53ef\u5bfc\u81f4\u7cfb\u7edf\u51c6\u786e\u7387\u9010\u6b65\u4e0b\u964d\uff0c\u4e14\u73b0\u6709\u4e3b\u6d41\u9632\u5fa1\u673a\u5236\u96be\u4ee5\u6709\u6548\u8bc6\u522b\u8be5\u653b\u51fb\uff0c\u8bc1\u660e\u5176\u5bf9IoA\u7cfb\u7edf\u7684\u5b89\u5168\u6784\u6210\u4e25\u91cd\u5a01\u80c1\u3002", "conclusion": "GRMP\u653b\u51fb\u63ed\u793a\u4e86\u5f53\u524d\u8054\u90a6\u5b66\u4e60\u5728IoA\u573a\u666f\u4e0b\u5bf9\u9ad8\u7ea7\u6a21\u578b\u6295\u6bd2\u653b\u51fb\u7684\u9632\u5fa1\u4e0d\u8db3\uff0c\u5f3a\u8c03\u4e86\u5728\u5927\u89c4\u6a21\u5f02\u6784\u73af\u5883\u4e2d\u91cd\u65b0\u8bbe\u8ba1\u9c81\u68d2\u5b89\u5168\u673a\u5236\u7684\u5fc5\u8981\u6027\u3002"}}
{"id": "2511.05813", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2511.05813", "abs": "https://arxiv.org/abs/2511.05813", "authors": ["In-on Wiratsin", "Chaiyong Ragkhitwetsagul", "Matheus Paixao", "Denis De Sousa", "Pongpop Lapvikai", "Peter Haddawy"], "title": "An Empirical Study of Java Code Improvements Based on Stack Overflow Answer Edits", "comment": null, "summary": "Suboptimal code is prevalent in software systems. Developers often write low-quality code due to factors like technical knowledge gaps, insufficient experience, time pressure, management decisions, or personal factors. Once integrated, the accumulation of this suboptimal code leads to significant maintenance costs and technical debt.\n  Developers frequently consult external knowledge bases, such as API documentation and Q&A websites like Stack Overflow (SO), to aid their programming tasks. SO's crowdsourced, collaborative nature has created a vast repository of programming knowledge. Its community-curated content is constantly evolving, with new answers posted or existing ones edited.\n  In this paper, we present an empirical study of SO Java answer edits and their application to improving code in open-source projects. We use a modified code clone search tool to analyze SO code snippets with version history and apply it to open-source Java projects. This identifies outdated or unoptimized code and suggests improved alternatives. Analyzing 140,840 Java accepted answers from SOTorrent and 10,668 GitHub Java projects, we manually categorized SO answer edits and created pull requests to open-source projects with the suggested code improvements. Our results show that 6.91% of SO Java accepted answers have more than one revision (average of 2.82). Moreover, 49.24% of the code snippets in the answer edits are applicable to open-source projects, and 11 out of 36 proposed bug fixes based on these edits were accepted by the GitHub project maintainers.", "AI": {"tldr": "\u672c\u6587\u901a\u8fc7\u5b9e\u8bc1\u7814\u7a76\u5206\u6790\u4e86Stack Overflow\u4e0aJava\u7b54\u6848\u7684\u7f16\u8f91\u5386\u53f2\uff0c\u5e76\u5229\u7528\u8fd9\u4e9b\u7f16\u8f91\u6539\u8fdb\u5f00\u6e90\u9879\u76ee\u4e2d\u7684\u4ee3\u7801\u3002\u7814\u7a76\u53d1\u73b0\uff0c\u8fd1\u4e00\u534a\u7684\u7f16\u8f91\u4ee3\u7801\u7247\u6bb5\u9002\u7528\u4e8e\u5f00\u6e90\u9879\u76ee\uff0c\u4e14\u90e8\u5206\u57fa\u4e8e\u8fd9\u4e9b\u7f16\u8f91\u63d0\u51fa\u7684\u4fee\u590d\u5efa\u8bae\u5df2\u88ab\u9879\u76ee\u7ef4\u62a4\u8005\u91c7\u7eb3\u3002", "motivation": "\u8f6f\u4ef6\u7cfb\u7edf\u4e2d\u666e\u904d\u5b58\u5728\u6b21\u4f18\u4ee3\u7801\uff0c\u5bfc\u81f4\u9ad8\u6602\u7684\u7ef4\u62a4\u6210\u672c\u548c\u6280\u672f\u503a\u52a1\u3002\u5f00\u53d1\u8005\u5e38\u501f\u52a9Stack Overflow\u7b49\u5916\u90e8\u77e5\u8bc6\u5e93\u8f85\u52a9\u7f16\u7a0b\uff0c\u800c\u5176\u7b54\u6848\u5185\u5bb9\u4f1a\u4e0d\u65ad\u66f4\u65b0\u4f18\u5316\u3002\u4f5c\u8005\u5e0c\u671b\u63a2\u7d22\u5982\u4f55\u5229\u7528\u8fd9\u4e9b\u66f4\u65b0\u540e\u7684\u4f18\u8d28\u4ee3\u7801\u7247\u6bb5\u6765\u6539\u8fdb\u73b0\u6709\u5f00\u6e90\u9879\u76ee\u4e2d\u7684\u4f4e\u8d28\u91cf\u4ee3\u7801\u3002", "method": "\u4f5c\u8005\u4f7f\u7528\u6539\u8fdb\u7684\u4ee3\u7801\u514b\u9686\u641c\u7d22\u5de5\u5177\uff0c\u7ed3\u5408SOTorrent\u4e2d140,840\u4e2aJava\u88ab\u91c7\u7eb3\u7b54\u6848\u53ca\u5176\u7248\u672c\u5386\u53f2\uff0c\u5bf910,668\u4e2aGitHub Java\u9879\u76ee\u8fdb\u884c\u5206\u6790\u3002\u901a\u8fc7\u4eba\u5de5\u5206\u7c7b\u7b54\u6848\u7f16\u8f91\u7c7b\u578b\uff0c\u5e76\u5411\u5f00\u6e90\u9879\u76ee\u63d0\u4ea4\u62c9\u53d6\u8bf7\u6c42\uff08Pull Requests\uff09\u4ee5\u5efa\u8bae\u4ee3\u7801\u6539\u8fdb\u3002", "result": "\u7814\u7a76\u53d1\u73b06.91%\u7684Java\u88ab\u91c7\u7eb3\u7b54\u6848\u6709\u591a\u6b21\u4fee\u8ba2\uff08\u5e73\u57472.82\u6b21\uff09\uff0c\u5176\u4e2d49.24%\u7684\u7f16\u8f91\u4ee3\u7801\u7247\u6bb5\u53ef\u5e94\u7528\u4e8e\u5f00\u6e90\u9879\u76ee\u3002\u5728\u63d0\u4ea4\u768436\u4e2a\u57fa\u4e8e\u7f16\u8f91\u7684bug\u4fee\u590d\u5efa\u8bae\u4e2d\uff0c\u670911\u4e2a\u88ab\u9879\u76ee\u7ef4\u62a4\u8005\u63a5\u53d7\u3002", "conclusion": "Stack Overflow\u4e0a\u7684\u7b54\u6848\u7f16\u8f91\u8574\u542b\u5927\u91cf\u53ef\u590d\u7528\u7684\u4ee3\u7801\u6539\u8fdb\u673a\u4f1a\uff0c\u80fd\u6709\u6548\u5e2e\u52a9\u8bc6\u522b\u548c\u4fee\u590d\u5f00\u6e90\u9879\u76ee\u4e2d\u7684\u6b21\u4f18\u4ee3\u7801\uff0c\u8bc1\u660e\u793e\u533a\u534f\u4f5c\u77e5\u8bc6\u5bf9\u63d0\u5347\u8f6f\u4ef6\u8d28\u91cf\u5177\u6709\u5b9e\u9645\u4ef7\u503c\u3002"}}
{"id": "2511.06361", "categories": ["cs.MA", "cs.AI", "cs.GT"], "pdf": "https://arxiv.org/pdf/2511.06361", "abs": "https://arxiv.org/abs/2511.06361", "authors": ["Qi Shi", "Pavel Naumov"], "title": "A Graph-Theoretical Perspective on Law Design for Multiagent Systems", "comment": "The 40th AAAI Conference on Artificial Intelligence (AAAI-26)", "summary": "A law in a multiagent system is a set of constraints imposed on agents' behaviours to avoid undesirable outcomes. The paper considers two types of laws: useful laws that, if followed, completely eliminate the undesirable outcomes and gap-free laws that guarantee that at least one agent can be held responsible each time an undesirable outcome occurs. In both cases, we study the problem of finding a law that achieves the desired result by imposing the minimum restrictions.\n  We prove that, for both types of laws, the minimisation problem is NP-hard even in the simple case of one-shot concurrent interactions. We also show that the approximation algorithm for the vertex cover problem in hypergraphs could be used to efficiently approximate the minimum laws in both cases.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u4e2d\u4e24\u7c7b\u6cd5\u5f8b\uff08\u6709\u7528\u6cd5\u5f8b\u548c\u65e0\u6f0f\u6d1e\u6cd5\u5f8b\uff09\u7684\u6700\u5c0f\u5316\u95ee\u9898\uff0c\u8bc1\u660e\u5176\u4e3aNP\u96be\u95ee\u9898\uff0c\u5e76\u6307\u51fa\u53ef\u501f\u52a9\u8d85\u56fe\u9876\u70b9\u8986\u76d6\u7684\u8fd1\u4f3c\u7b97\u6cd5\u9ad8\u6548\u6c42\u89e3\u8fd1\u4f3c\u6700\u5c0f\u6cd5\u5f8b\u3002", "motivation": "\u5728\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u4e2d\uff0c\u9700\u8981\u901a\u8fc7\u65bd\u52a0\u884c\u4e3a\u7ea6\u675f\uff08\u5373\u201c\u6cd5\u5f8b\u201d\uff09\u6765\u907f\u514d\u4e0d\u826f\u7ed3\u679c\u3002\u4f5c\u8005\u65e8\u5728\u5bfb\u627e\u65e2\u80fd\u8fbe\u6210\u76ee\u6807\u53c8\u9650\u5236\u6700\u5c11\u7684\u6cd5\u5f8b\uff0c\u4ee5\u63d0\u9ad8\u7cfb\u7edf\u7684\u6548\u7387\u4e0e\u53ef\u884c\u6027\u3002", "method": "\u5206\u6790\u4e24\u7c7b\u6cd5\u5f8b\uff08\u6709\u7528\u6cd5\u5f8b\u548c\u65e0\u6f0f\u6d1e\u6cd5\u5f8b\uff09\u7684\u5f62\u5f0f\u5316\u5b9a\u4e49\uff0c\u5e76\u5c06\u6700\u5c0f\u5316\u95ee\u9898\u5f52\u7ea6\u4e3a\u8ba1\u7b97\u590d\u6742\u6027\u95ee\u9898\uff1b\u5229\u7528\u8d85\u56fe\u9876\u70b9\u8986\u76d6\u95ee\u9898\u7684\u8fd1\u4f3c\u7b97\u6cd5\u6765\u8fd1\u4f3c\u6c42\u89e3\u6700\u5c0f\u6cd5\u5f8b\u3002", "result": "\u8bc1\u660e\u4e86\u5728\u5355\u6b21\u5e76\u53d1\u4ea4\u4e92\u60c5\u5f62\u4e0b\uff0c\u4e24\u7c7b\u6cd5\u5f8b\u7684\u6700\u5c0f\u5316\u95ee\u9898\u5747\u4e3aNP-hard\uff1b\u540c\u65f6\u8868\u660e\u53ef\u4f7f\u7528\u8d85\u56fe\u9876\u70b9\u8986\u76d6\u7684\u8fd1\u4f3c\u7b97\u6cd5\u5bf9\u6700\u5c0f\u6cd5\u5f8b\u8fdb\u884c\u6709\u6548\u8fd1\u4f3c\u3002", "conclusion": "\u6700\u5c0f\u6cd5\u5f8b\u7684\u8bbe\u8ba1\u5728\u7406\u8bba\u4e0a\u662f\u56f0\u96be\u7684\uff08NP-hard\uff09\uff0c\u4f46\u53ef\u901a\u8fc7\u5df2\u6709\u8fd1\u4f3c\u7b97\u6cd5\u83b7\u5f97\u5b9e\u7528\u89e3\uff0c\u4e3a\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u4e2d\u7684\u89c4\u5219\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u7406\u8bba\u57fa\u7840\u4e0e\u53ef\u884c\u8def\u5f84\u3002"}}
{"id": "2511.06174", "categories": ["cs.AR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.06174", "abs": "https://arxiv.org/abs/2511.06174", "authors": ["Zifan He", "Shengyu Ye", "Rui Ma", "Yang Wang", "Jason Cong"], "title": "LUT-LLM: Efficient Large Language Model Inference with Memory-based Computations on FPGAs", "comment": null, "summary": "The rapid progress of large language models (LLMs) has advanced numerous applications, yet efficient single-batch inference remains vital for on-device intelligence. While FPGAs offer fine-grained data control and high energy efficiency, recent GPU optimizations have narrowed their advantage, especially under arithmetic-based computation. To overcome this, we leverage FPGAs' abundant on-chip memory to shift LLM inference from arithmetic- to memory-based computation through table lookups. We present LUT-LLM, the first FPGA accelerator enabling 1B+ LLM inference via vector-quantized memory operations. Our analysis identifies activation-weight co-quantization as the most effective scheme, supported by (1) bandwidth-aware parallel centroid search, (2) efficient 2D table lookups, and (3) a spatial-temporal hybrid design minimizing data caching. Implemented on an AMD V80 FPGA for a customized Qwen 3 1.7B model, LUT-LLM achieves 1.66x lower latency than AMD MI210 and 1.72x higher energy efficiency than NVIDIA A100, scaling to 32B models with 2.16x efficiency gain over A100.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faLUT-LLM\uff0c\u9996\u4e2a\u57fa\u4e8eFPGA\u7684\u52a0\u901f\u5668\uff0c\u901a\u8fc7\u5411\u91cf\u91cf\u5316\u5185\u5b58\u64cd\u4f5c\u5b9e\u73b010\u4ebf\u7ea7\u4ee5\u4e0a\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u63a8\u7406\uff0c\u5229\u7528\u67e5\u627e\u8868\u5c06\u8ba1\u7b97\u4ece\u7b97\u672f\u5bc6\u96c6\u578b\u8f6c\u4e3a\u5185\u5b58\u5bc6\u96c6\u578b\uff0c\u5728AMD V80 FPGA\u4e0a\u90e8\u7f72\u5b9a\u5236Qwen 3 1.7B\u6a21\u578b\uff0c\u76f8\u6bd4AMD MI210\u548cNVIDIA A100\u5206\u522b\u5b9e\u73b0\u4e86\u66f4\u4f4e\u5ef6\u8fdf\u4e0e\u66f4\u9ad8\u80fd\u6548\uff0c\u5e76\u53ef\u6269\u5c55\u81f332B\u6a21\u578b\u3002", "motivation": "\u5c3d\u7ba1GPU\u5728\u7b97\u672f\u8ba1\u7b97\u65b9\u9762\u4f18\u5316\u663e\u8457\uff0c\u524a\u5f31\u4e86FPGA\u5728\u80fd\u6548\u65b9\u9762\u7684\u4f20\u7edf\u4f18\u52bf\uff0c\u4f46FPGA\u62e5\u6709\u4e30\u5bcc\u7684\u7247\u4e0a\u5185\u5b58\u8d44\u6e90\u3002\u4f5c\u8005\u65e8\u5728\u5229\u7528\u8fd9\u4e00\u7279\u6027\uff0c\u5c06\u5927\u8bed\u8a00\u6a21\u578b\u63a8\u7406\u4ece\u4f9d\u8d56\u7b97\u672f\u8fd0\u7b97\u8f6c\u5411\u57fa\u4e8e\u5185\u5b58\u67e5\u627e\u7684\u8ba1\u7b97\u8303\u5f0f\uff0c\u4ee5\u63d0\u5347\u5728\u8bbe\u5907\u7aef\u5355\u6279\u6b21\u63a8\u7406\u7684\u6548\u7387\u3002", "method": "\u63d0\u51faLUT-LLM\u67b6\u6784\uff0c\u91c7\u7528\u6fc0\u6d3b\u503c\u4e0e\u6743\u91cd\u534f\u540c\u91cf\u5316\u7b56\u7565\uff0c\u5e76\u7ed3\u5408\u4e09\u9879\u5173\u952e\u6280\u672f\uff1a(1) \u5e26\u5bbd\u611f\u77e5\u7684\u5e76\u884c\u8d28\u5fc3\u641c\u7d22\uff1b(2) \u9ad8\u6548\u7684\u4e8c\u7ef4\u67e5\u8868\u64cd\u4f5c\uff1b(3) \u7a7a\u95f4-\u65f6\u95f4\u6df7\u5408\u8bbe\u8ba1\u4ee5\u6700\u5c0f\u5316\u6570\u636e\u7f13\u5b58\u5f00\u9500\u3002", "result": "\u5728AMD V80 FPGA\u4e0a\u5b9e\u73b0\u5b9a\u5236Qwen 3 1.7B\u6a21\u578b\u63a8\u7406\uff0c\u76f8\u6bd4AMD MI210\u964d\u4f4e1.66\u500d\u5ef6\u8fdf\uff0c\u76f8\u6bd4NVIDIA A100\u63d0\u53471.72\u500d\u80fd\u6548\uff1b\u8be5\u65b9\u6cd5\u53ef\u6269\u5c55\u81f332B\u6a21\u578b\uff0c\u5728A100\u4e0a\u83b7\u5f972.16\u500d\u7684\u80fd\u6548\u589e\u76ca\u3002", "conclusion": "LUT-LLM\u6210\u529f\u5c55\u793a\u4e86\u901a\u8fc7\u5185\u5b58\u4e3a\u4e2d\u5fc3\u7684\u8ba1\u7b97\u8303\u5f0f\u53ef\u5728FPGA\u4e0a\u9ad8\u6548\u8fd0\u884c\u5927\u89c4\u6a21\u8bed\u8a00\u6a21\u578b\uff0c\u663e\u8457\u4f18\u4e8e\u5f53\u524d\u4e3b\u6d41GPU\uff0c\u5728\u5ef6\u8fdf\u548c\u80fd\u6548\u65b9\u9762\u5747\u53d6\u5f97\u7a81\u7834\uff0c\u4e3a\u7aef\u4fa7\u5927\u6a21\u578b\u90e8\u7f72\u63d0\u4f9b\u4e86\u65b0\u8def\u5f84\u3002"}}
{"id": "2511.05820", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.05820", "abs": "https://arxiv.org/abs/2511.05820", "authors": ["Zishuo Xu", "Dezhong Yao", "Yao Wan"], "title": "WAR-Re: Web API Recommendation with Semantic Reasoning", "comment": null, "summary": "With the development of cloud computing, the number of Web APIs has increased dramatically, further intensifying the demand for efficient Web API recommendation. Despite the demonstrated success of previous Web API recommendation solutions, two critical challenges persist: 1) a fixed top-N recommendation that cannot accommodate the varying API cardinality requirements of different mashups, and 2) these methods output only ranked API lists without accompanying reasons, depriving users of understanding the recommendation. To address these challenges, we propose WAR-Re, an LLM-based model for Web API recommendation with semantic reasoning for justification. WAR-Re leverages special start and stop tokens to handle the first challenge and uses two-stage training: supervised fine-tuning and reinforcement learning via Group Relative Policy Optimization (GRPO) to enhance the model's ability in both tasks. Comprehensive experimental evaluations on the ProgrammableWeb dataset demonstrate that WAR-Re achieves a gain of up to 21.59\\% over the state-of-the-art baseline model in recommendation accuracy, while consistently producing high-quality semantic reasons for recommendations.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faWAR-Re\u6a21\u578b\uff0c\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u5b9e\u73b0\u53ef\u53d8\u6570\u91cf\u7684Web API\u63a8\u8350\uff0c\u5e76\u901a\u8fc7\u8bed\u4e49\u63a8\u7406\u751f\u6210\u63a8\u8350\u7406\u7531\uff0c\u5728\u51c6\u786e\u7387\u548c\u89e3\u91ca\u6027\u4e0a\u5747\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709Web API\u63a8\u8350\u65b9\u6cd5\u5b58\u5728\u4e24\u4e2a\u5173\u952e\u95ee\u9898\uff1a\u4e00\u662f\u56fa\u5b9a\u6570\u91cf\u7684Top-N\u63a8\u8350\u65e0\u6cd5\u9002\u5e94\u4e0d\u540cmashup\u5bf9API\u6570\u91cf\u7684\u5dee\u5f02\u5316\u9700\u6c42\uff1b\u4e8c\u662f\u4ec5\u63d0\u4f9b\u6392\u5e8f\u5217\u8868\u800c\u7f3a\u4e4f\u63a8\u8350\u7406\u7531\uff0c\u5f71\u54cd\u7528\u6237\u7406\u89e3\u4e0e\u4fe1\u4efb\u3002", "method": "\u63d0\u51faWAR-Re\u6a21\u578b\uff0c\u91c7\u7528\u7279\u6b8a\u8d77\u6b62\u6807\u8bb0\u5904\u7406\u53ef\u53d8\u63a8\u8350\u6570\u91cf\uff0c\u5e76\u7ed3\u5408\u4e24\u9636\u6bb5\u8bad\u7ec3\u7b56\u7565\u2014\u2014\u76d1\u7763\u5fae\u8c03\u4e0e\u57fa\u4e8eGroup Relative Policy Optimization\uff08GRPO\uff09\u7684\u5f3a\u5316\u5b66\u4e60\uff0c\u4ee5\u540c\u65f6\u63d0\u5347\u63a8\u8350\u51c6\u786e\u6027\u4e0e\u8bed\u4e49\u89e3\u91ca\u80fd\u529b\u3002", "result": "\u5728ProgrammableWeb\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cWAR-Re\u76f8\u6bd4\u5f53\u524d\u6700\u4f18\u57fa\u7ebf\u6a21\u578b\u5728\u63a8\u8350\u51c6\u786e\u7387\u4e0a\u6700\u9ad8\u63d0\u534721.59%\uff0c\u5e76\u80fd\u6301\u7eed\u751f\u6210\u9ad8\u8d28\u91cf\u7684\u8bed\u4e49\u63a8\u8350\u7406\u7531\u3002", "conclusion": "WAR-Re\u6709\u6548\u89e3\u51b3\u4e86Web API\u63a8\u8350\u4e2d\u6570\u91cf\u7075\u6d3b\u6027\u4e0e\u53ef\u89e3\u91ca\u6027\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u4e3a\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u4e86\u66f4\u53ef\u9760\u3001\u900f\u660e\u7684\u63a8\u8350\u65b9\u6848\u3002"}}
{"id": "2511.06448", "categories": ["cs.MA", "cs.AI", "cs.CL", "cs.SI"], "pdf": "https://arxiv.org/pdf/2511.06448", "abs": "https://arxiv.org/abs/2511.06448", "authors": ["Qibing Ren", "Zhijie Zheng", "Jiaxuan Guo", "Junchi Yan", "Lizhuang Ma", "Jing Shao"], "title": "When AI Agents Collude Online: Financial Fraud Risks by Collaborative LLM Agents on Social Platforms", "comment": "Code is available at https://github.com/zheng977/MutiAgent4Fraud", "summary": "In this work, we study the risks of collective financial fraud in large-scale multi-agent systems powered by large language model (LLM) agents. We investigate whether agents can collaborate in fraudulent behaviors, how such collaboration amplifies risks, and what factors influence fraud success. To support this research, we present MultiAgentFraudBench, a large-scale benchmark for simulating financial fraud scenarios based on realistic online interactions. The benchmark covers 28 typical online fraud scenarios, spanning the full fraud lifecycle across both public and private domains. We further analyze key factors affecting fraud success, including interaction depth, activity level, and fine-grained collaboration failure modes. Finally, we propose a series of mitigation strategies, including adding content-level warnings to fraudulent posts and dialogues, using LLMs as monitors to block potentially malicious agents, and fostering group resilience through information sharing at the societal level. Notably, we observe that malicious agents can adapt to environmental interventions. Our findings highlight the real-world risks of multi-agent financial fraud and suggest practical measures for mitigating them. Code is available at https://github.com/zheng977/MutiAgent4Fraud.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u7531\u5927\u8bed\u8a00\u6a21\u578b\u9a71\u52a8\u7684\u5927\u89c4\u6a21\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u4e2d\u96c6\u4f53\u91d1\u878d\u6b3a\u8bc8\u7684\u98ce\u9669\uff0c\u63d0\u51fa\u4e86\u5305\u542b28\u79cd\u5178\u578b\u5728\u7ebf\u6b3a\u8bc8\u573a\u666f\u7684\u57fa\u51c6MultiAgentFraudBench\uff0c\u5e76\u5206\u6790\u4e86\u5f71\u54cd\u6b3a\u8bc8\u6210\u529f\u7684\u5173\u952e\u56e0\u7d20\u53ca\u76f8\u5e94\u7684\u7f13\u89e3\u7b56\u7565\u3002", "motivation": "\u968f\u7740\u5927\u8bed\u8a00\u6a21\u578b\u9a71\u52a8\u7684\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u5728\u73b0\u5b9e\u4e16\u754c\u4e2d\u7684\u5e7f\u6cdb\u5e94\u7528\uff0c\u5176\u6f5c\u5728\u7684\u96c6\u4f53\u91d1\u878d\u6b3a\u8bc8\u98ce\u9669\u65e5\u76ca\u51f8\u663e\u3002\u73b0\u6709\u7814\u7a76\u7f3a\u4e4f\u5bf9\u591a\u667a\u80fd\u4f53\u534f\u540c\u6b3a\u8bc8\u884c\u4e3a\u53ca\u5176\u653e\u5927\u6548\u5e94\u7684\u7cfb\u7edf\u6027\u5206\u6790\uff0c\u4e5f\u7f3a\u5c11\u7528\u4e8e\u8bc4\u4f30\u548c\u7f13\u89e3\u6b64\u7c7b\u98ce\u9669\u7684\u5927\u89c4\u6a21\u57fa\u51c6\u3002", "method": "\u4f5c\u8005\u6784\u5efa\u4e86MultiAgentFraudBench\u57fa\u51c6\uff0c\u6a21\u62df\u8986\u76d6\u5b8c\u6574\u6b3a\u8bc8\u751f\u547d\u5468\u671f\u768428\u79cd\u771f\u5b9e\u5728\u7ebf\u91d1\u878d\u6b3a\u8bc8\u573a\u666f\uff1b\u901a\u8fc7\u8be5\u57fa\u51c6\u5206\u6790\u4ea4\u4e92\u6df1\u5ea6\u3001\u6d3b\u8dc3\u5ea6\u548c\u534f\u4f5c\u5931\u8d25\u6a21\u5f0f\u7b49\u5173\u952e\u56e0\u7d20\u5bf9\u6b3a\u8bc8\u6210\u529f\u7387\u7684\u5f71\u54cd\uff1b\u5e76\u63d0\u51fa\u5185\u5bb9\u7ea7\u8b66\u544a\u3001LLM\u76d1\u63a7\u5668\u548c\u7fa4\u4f53\u4fe1\u606f\u5171\u4eab\u7b49\u7f13\u89e3\u7b56\u7565\u3002", "result": "\u5b9e\u9a8c\u53d1\u73b0\u6076\u610f\u667a\u80fd\u4f53\u80fd\u591f\u534f\u540c\u5b9e\u65bd\u6b3a\u8bc8\uff0c\u4e14\u5176\u6210\u529f\u7387\u53d7\u591a\u79cd\u56e0\u7d20\u5f71\u54cd\uff1b\u6240\u63d0\u51fa\u7684\u7f13\u89e3\u7b56\u7565\uff08\u5982\u5185\u5bb9\u8b66\u544a\u3001LLM\u76d1\u63a7\uff09\u5728\u4e00\u5b9a\u7a0b\u5ea6\u4e0a\u6709\u6548\uff0c\u4f46\u6076\u610f\u667a\u80fd\u4f53\u53ef\u9002\u5e94\u73af\u5883\u5e72\u9884\uff1b\u7fa4\u4f53\u5c42\u9762\u7684\u4fe1\u606f\u5171\u4eab\u6709\u52a9\u4e8e\u63d0\u5347\u6574\u4f53\u6297\u6b3a\u8bc8\u80fd\u529b\u3002", "conclusion": "\u5927\u89c4\u6a21\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u5b58\u5728\u663e\u8457\u7684\u96c6\u4f53\u91d1\u878d\u6b3a\u8bc8\u98ce\u9669\uff0c\u9700\u7ed3\u5408\u6280\u672f\u624b\u6bb5\u4e0e\u793e\u4f1a\u673a\u5236\u8fdb\u884c\u7efc\u5408\u6cbb\u7406\u3002\u672c\u7814\u7a76\u4e3a\u7406\u89e3\u548c\u9632\u8303\u6b64\u7c7b\u98ce\u9669\u63d0\u4f9b\u4e86\u5b9e\u8bc1\u57fa\u7840\u548c\u5b9e\u7528\u5bf9\u7b56\u3002"}}
{"id": "2511.05821", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2511.05821", "abs": "https://arxiv.org/abs/2511.05821", "authors": ["Rujiphart Charatvaraphan", "Bunradar Chatchaiyadech", "Thitirat Sukijprasert", "Chaiyong Ragkhitwetsagul", "Morakot Choetkiertikul", "Raula Gaikovina Kula", "Thanwadee Sunetnanta", "Kenichi Matsumoto"], "title": "PyGress: Tool for Analyzing the Progression of Code Proficiency in Python OSS Projects", "comment": null, "summary": "Assessing developer proficiency in open-source software (OSS) projects is essential for understanding project dynamics, especially for expertise. This paper presents PyGress, a web-based tool designed to automatically evaluate and visualize Python code proficiency using pycefr, a Python code proficiency analyzer. By submitting a GitHub repository link, the system extracts commit histories, analyzes source code proficiency across CEFR-aligned levels (A1 to C2), and generates visual summaries of individual and project-wide proficiency. The PyGress tool visualizes per-contributor proficiency distribution and tracks project code proficiency progression over time. PyGress offers an interactive way to explore contributor coding levels in Python OSS repositories. The video demonstration of the PyGress tool can be found at https://youtu.be/hxoeK-ggcWk, and the source code of the tool is publicly available at https://github.com/MUICT-SERU/PyGress.", "AI": {"tldr": "PyGress \u662f\u4e00\u4e2a\u57fa\u4e8e\u7f51\u9875\u7684\u5de5\u5177\uff0c\u7528\u4e8e\u81ea\u52a8\u8bc4\u4f30\u548c\u53ef\u89c6\u5316 Python \u5f00\u6e90\u9879\u76ee\u4e2d\u5f00\u53d1\u8005\u7684\u4ee3\u7801\u719f\u7ec3\u5ea6\uff0c\u4f9d\u636e CEFR \u7b49\u7ea7\uff08A1\u2013C2\uff09\u5206\u6790\u63d0\u4ea4\u5386\u53f2\u5e76\u751f\u6210\u4e2a\u4eba\u4e0e\u9879\u76ee\u6574\u4f53\u7684\u719f\u7ec3\u5ea6\u53ef\u89c6\u5316\u3002", "motivation": "\u8bc4\u4f30\u5f00\u6e90\u8f6f\u4ef6\u9879\u76ee\u4e2d\u5f00\u53d1\u8005\u7684\u7f16\u7a0b\u719f\u7ec3\u5ea6\u5bf9\u4e8e\u7406\u89e3\u9879\u76ee\u52a8\u6001\u548c\u4e13\u5bb6\u5206\u5e03\u81f3\u5173\u91cd\u8981\u3002", "method": "\u901a\u8fc7\u7528\u6237\u63d0\u4ea4\u7684 GitHub \u4ed3\u5e93\u94fe\u63a5\uff0cPyGress \u63d0\u53d6\u63d0\u4ea4\u5386\u53f2\uff0c\u5229\u7528 pycefr \u5de5\u5177\u5bf9\u6e90\u4ee3\u7801\u8fdb\u884c CEFR \u7b49\u7ea7\u5206\u6790\uff0c\u5e76\u751f\u6210\u53ef\u89c6\u5316\u7ed3\u679c\u3002", "result": "\u8be5\u5de5\u5177\u80fd\u591f\u5c55\u793a\u6bcf\u4f4d\u8d21\u732e\u8005\u7684\u719f\u7ec3\u5ea6\u5206\u5e03\uff0c\u5e76\u8ffd\u8e2a\u9879\u76ee\u6574\u4f53\u4ee3\u7801\u719f\u7ec3\u5ea6\u968f\u65f6\u95f4\u7684\u53d8\u5316\u3002", "conclusion": "PyGress \u4e3a\u63a2\u7d22 Python \u5f00\u6e90\u9879\u76ee\u4e2d\u8d21\u732e\u8005\u7684\u7f16\u7801\u6c34\u5e73\u63d0\u4f9b\u4e86\u4e00\u79cd\u4ea4\u4e92\u5f0f\u624b\u6bb5\uff0c\u6709\u52a9\u4e8e\u66f4\u597d\u5730\u7406\u89e3\u9879\u76ee\u7684\u4eba\u624d\u7ed3\u6784\u4e0e\u53d1\u5c55\u8f68\u8ff9\u3002"}}
{"id": "2511.06313", "categories": ["cs.AR", "cs.AI", "cs.LG", "eess.SP"], "pdf": "https://arxiv.org/pdf/2511.06313", "abs": "https://arxiv.org/abs/2511.06313", "authors": ["Stef Cuyckens", "Xiaoling Yi", "Robin Geens", "Joren Dumoulin", "Martin Wiesner", "Chao Fang", "Marian Verhelst"], "title": "Precision-Scalable Microscaling Datapaths with Optimized Reduction Tree for Efficient NPU Integration", "comment": "To appear in the 31st Asia and South Pacific Design Automation Conference (ASP-DAC 2026, Invited Paper)", "summary": "Emerging continual learning applications necessitate next-generation neural processing unit (NPU) platforms to support both training and inference operations. The promising Microscaling (MX) standard enables narrow bit-widths for inference and large dynamic ranges for training. However, existing MX multiply-accumulate (MAC) designs face a critical trade-off: integer accumulation requires expensive conversions from narrow floating-point products, while FP32 accumulation suffers from quantization losses and costly normalization. To address these limitations, we propose a hybrid precision-scalable reduction tree for MX MACs that combines the benefits of both approaches, enabling efficient mixed-precision accumulation with controlled accuracy relaxation. Moreover, we integrate an 8x8 array of these MACs into the state-of-the-art (SotA) NPU integration platform, SNAX, to provide efficient control and data transfer to our optimized precision-scalable MX datapath. We evaluate our design both on MAC and system level and compare it to the SotA. Our integrated system achieves an energy efficiency of 657, 1438-1675, and 4065 GOPS/W, respectively, for MXINT8, MXFP8/6, and MXFP4, with a throughput of 64, 256, and 512 GOPS.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u6df7\u5408\u7cbe\u5ea6\u53ef\u6269\u5c55\u7684Microscaling\uff08MX\uff09\u4e58\u79ef\u7d2f\u52a0\uff08MAC\uff09\u67b6\u6784\uff0c\u89e3\u51b3\u4e86\u73b0\u6709MX MAC\u5728\u6574\u6570\u7d2f\u52a0\u4e0eFP32\u7d2f\u52a0\u4e4b\u95f4\u7684\u7cbe\u5ea6\u4e0e\u6548\u7387\u6743\u8861\u95ee\u9898\uff0c\u5e76\u5c06\u5176\u96c6\u6210\u5230SNAX\u795e\u7ecf\u5904\u7406\u5355\u5143\u5e73\u53f0\u4e2d\uff0c\u5728\u591a\u79cdMX\u683c\u5f0f\u4e0b\u5b9e\u73b0\u4e86\u9ad8\u80fd\u6548\u4e0e\u9ad8\u541e\u5410\u3002", "motivation": "\u65b0\u5174\u7684\u6301\u7eed\u5b66\u4e60\u5e94\u7528\u8981\u6c42\u4e0b\u4e00\u4ee3\u795e\u7ecf\u5904\u7406\u5355\u5143\uff08NPU\uff09\u540c\u65f6\u652f\u6301\u8bad\u7ec3\u548c\u63a8\u7406\u3002MX\u6807\u51c6\u867d\u80fd\u517c\u987e\u7a84\u4f4d\u5bbd\u63a8\u7406\u4e0e\u5927\u52a8\u6001\u8303\u56f4\u8bad\u7ec3\uff0c\u4f46\u73b0\u6709MX MAC\u8bbe\u8ba1\u5728\u7d2f\u52a0\u65b9\u5f0f\u4e0a\u5b58\u5728\u8f6c\u6362\u5f00\u9500\u5927\u6216\u91cf\u5316\u635f\u5931\u4e25\u91cd\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u6df7\u5408\u7cbe\u5ea6\u53ef\u6269\u5c55\u7684\u7f29\u51cf\u6811\u7ed3\u6784\u7528\u4e8eMX MAC\uff0c\u7ed3\u5408\u6574\u6570\u4e0e\u6d6e\u70b9\u7d2f\u52a0\u7684\u4f18\u52bf\uff0c\u5b9e\u73b0\u9ad8\u6548\u6df7\u5408\u7cbe\u5ea6\u7d2f\u52a0\uff1b\u5e76\u5c068x8 MAC\u9635\u5217\u96c6\u6210\u5230SNAX NPU\u5e73\u53f0\uff0c\u4f18\u5316\u6570\u636e\u901a\u8def\u4e0e\u63a7\u5236\u3002", "result": "\u5728MXINT8\u3001MXFP8/6\u548cMXFP4\u683c\u5f0f\u4e0b\uff0c\u7cfb\u7edf\u5206\u522b\u8fbe\u5230657\u30011438\u20131675\u548c4065 GOPS/W\u7684\u80fd\u6548\uff0c\u4ee5\u53ca64\u3001256\u548c512 GOPS\u7684\u541e\u5410\u91cf\uff0c\u4f18\u4e8e\u5f53\u524d\u6700\u5148\u8fdb\u6c34\u5e73\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u6df7\u5408\u7cbe\u5ea6\u53ef\u6269\u5c55MX MAC\u67b6\u6784\u6709\u6548\u5e73\u8861\u4e86\u7cbe\u5ea6\u4e0e\u6548\u7387\uff0c\u663e\u8457\u63d0\u5347\u4e86NPU\u5728\u6301\u7eed\u5b66\u4e60\u573a\u666f\u4e0b\u7684\u80fd\u6548\u4e0e\u6027\u80fd\uff0c\u4e3a\u652f\u6301\u8bad\u7ec3\u4e0e\u63a8\u7406\u4e00\u4f53\u5316\u7684\u4e0b\u4e00\u4ee3NPU\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848\u3002"}}
{"id": "2511.05824", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2511.05824", "abs": "https://arxiv.org/abs/2511.05824", "authors": ["Chaiyong Ragkhitwetsagul", "Morakot Choetkiertikul", "Srisupa Palakvangsa-Na-Ayudhya", "Thanwadee Sunetnanta", "Nattanee Satchanawakul"], "title": "The Impact of COVID-19 and Remote Work on Software Development in Thailand", "comment": null, "summary": "The COVID-19 pandemic impacted the way of working, including software development. During the pandemic, software companies were forced to work remotely, and many companies have been using such work arrangements. There are prior studies showing the benefits and drawbacks of remote work in software development during COVID-19. However, there is no study that targets Thailand, one of the growing software markets in Asia, specifically. This paper performs an empirical study of the effects of COVID-19 on software development in Thailand. We surveyed 194 Thai software developers regarding the challenges and benefits they faced while working remotely during the COVID-19 period. The results show no statistically significant changes in the productivity and well-being of Thai software developers before and after working remotely due to the pandemic. The results show that software developers in Thailand both received benefits and faced challenges from remote work during COVID-19, similar to results reported by other studies, but with some unique differences. This study can be beneficial to similar Asian countries or other low- and middle-income countries around the world.", "AI": {"tldr": "\u672c\u6587\u901a\u8fc7\u5bf9194\u540d\u6cf0\u56fd\u8f6f\u4ef6\u5f00\u53d1\u8005\u7684\u8c03\u67e5\uff0c\u7814\u7a76\u4e86\u65b0\u51a0\u75ab\u60c5\u5bf9\u6cf0\u56fd\u8fdc\u7a0b\u8f6f\u4ef6\u5f00\u53d1\u7684\u5f71\u54cd\uff0c\u53d1\u73b0\u8fdc\u7a0b\u5de5\u4f5c\u5bf9\u5f00\u53d1\u8005\u7684\u751f\u4ea7\u529b\u548c\u5e78\u798f\u611f\u65e0\u663e\u8457\u5f71\u54cd\uff0c\u540c\u65f6\u63ed\u793a\u4e86\u5176\u5e26\u6765\u7684\u6311\u6218\u4e0e\u76ca\u5904\uff0c\u5e76\u4e3a\u5176\u4ed6\u4e9a\u6d32\u53ca\u4e2d\u4f4e\u6536\u5165\u56fd\u5bb6\u63d0\u4f9b\u53c2\u8003\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u7f3a\u4e4f\u9488\u5bf9\u6cf0\u56fd\u8fd9\u4e00\u4e9a\u6d32\u65b0\u5174\u8f6f\u4ef6\u5e02\u573a\u5728\u65b0\u51a0\u75ab\u60c5\u671f\u95f4\u8fdc\u7a0b\u5de5\u4f5c\u7684\u4e13\u95e8\u63a2\u8ba8\uff0c\u56e0\u6b64\u672c\u6587\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u901a\u8fc7\u95ee\u5377\u8c03\u67e5194\u540d\u6cf0\u56fd\u8f6f\u4ef6\u5f00\u53d1\u8005\uff0c\u6536\u96c6\u4ed6\u4eec\u5728\u75ab\u60c5\u671f\u95f4\u8fdc\u7a0b\u5de5\u4f5c\u6240\u9762\u4e34\u7684\u6311\u6218\u4e0e\u83b7\u5f97\u7684\u76ca\u5904\uff0c\u5e76\u5206\u6790\u5176\u751f\u4ea7\u529b\u4e0e\u5e78\u798f\u611f\u7684\u53d8\u5316\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff0c\u6cf0\u56fd\u8f6f\u4ef6\u5f00\u53d1\u8005\u5728\u8fdc\u7a0b\u5de5\u4f5c\u524d\u540e\uff0c\u5176\u751f\u4ea7\u529b\u548c\u5e78\u798f\u611f\u6ca1\u6709\u7edf\u8ba1\u5b66\u4e0a\u7684\u663e\u8457\u53d8\u5316\uff1b\u8fdc\u7a0b\u5de5\u4f5c\u65e2\u5e26\u6765\u76ca\u5904\u4e5f\u5e26\u6765\u6311\u6218\uff0c\u4e0e\u5176\u4ed6\u7814\u7a76\u7ed3\u679c\u76f8\u4f3c\u4f46\u5b58\u5728\u4e00\u4e9b\u72ec\u7279\u5dee\u5f02\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u7406\u89e3\u65b0\u51a0\u75ab\u60c5\u4e0b\u6cf0\u56fd\u8f6f\u4ef6\u5f00\u53d1\u7684\u8fdc\u7a0b\u5de5\u4f5c\u5f71\u54cd\u63d0\u4f9b\u4e86\u5b9e\u8bc1\u4f9d\u636e\uff0c\u5e76\u53ef\u4e3a\u5176\u4ed6\u7c7b\u4f3c\u4e9a\u6d32\u56fd\u5bb6\u6216\u4e2d\u4f4e\u6536\u5165\u56fd\u5bb6\u63d0\u4f9b\u501f\u9274\u3002"}}
{"id": "2511.07071", "categories": ["cs.MA", "cs.RO"], "pdf": "https://arxiv.org/pdf/2511.07071", "abs": "https://arxiv.org/abs/2511.07071", "authors": ["Marcel M\u00fcller"], "title": "Multi-Agent Reinforcement Learning for Deadlock Handling among Autonomous Mobile Robots", "comment": "for associated repositories, see https://github.com/Nerozud/dl_reference_models and https://github.com/Nerozud/FTS_simpel", "summary": "This dissertation explores the application of multi-agent reinforcement learning (MARL) for handling deadlocks in intralogistics systems that rely on autonomous mobile robots (AMRs). AMRs enhance operational flexibility but also increase the risk of deadlocks, which degrade system throughput and reliability. Existing approaches often neglect deadlock handling in the planning phase and rely on rigid control rules that cannot adapt to dynamic operational conditions.\n  To address these shortcomings, this work develops a structured methodology for integrating MARL into logistics planning and operational control. It introduces reference models that explicitly consider deadlock-capable multi-agent pathfinding (MAPF) problems, enabling systematic evaluation of MARL strategies. Using grid-based environments and an external simulation software, the study compares traditional deadlock handling strategies with MARL-based solutions, focusing on PPO and IMPALA algorithms under different training and execution modes.\n  Findings reveal that MARL-based strategies, particularly when combined with centralized training and decentralized execution (CTDE), outperform rule-based methods in complex, congested environments. In simpler environments or those with ample spatial freedom, rule-based methods remain competitive due to their lower computational demands. These results highlight that MARL provides a flexible and scalable solution for deadlock handling in dynamic intralogistics scenarios, but requires careful tailoring to the operational context.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\uff08MARL\uff09\u5728\u81ea\u4e3b\u79fb\u52a8\u673a\u5668\u4eba\uff08AMR\uff09\u5185\u7269\u6d41\u7cfb\u7edf\u4e2d\u5904\u7406\u6b7b\u9501\u95ee\u9898\u7684\u5e94\u7528\uff0c\u63d0\u51fa\u7ed3\u5408\u96c6\u4e2d\u8bad\u7ec3\u4e0e\u5206\u6563\u6267\u884c\uff08CTDE\uff09\u7684MARL\u7b56\u7565\uff0c\u5728\u590d\u6742\u62e5\u5835\u73af\u5883\u4e2d\u4f18\u4e8e\u4f20\u7edf\u89c4\u5219\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u5185\u7269\u6d41\u7cfb\u7edf\u5728\u89c4\u5212\u9636\u6bb5\u5e38\u5ffd\u7565\u6b7b\u9501\u95ee\u9898\uff0c\u4f9d\u8d56\u65e0\u6cd5\u9002\u5e94\u52a8\u6001\u73af\u5883\u7684\u521a\u6027\u63a7\u5236\u89c4\u5219\uff0c\u96be\u4ee5\u4fdd\u969c\u7cfb\u7edf\u541e\u5410\u91cf\u4e0e\u53ef\u9760\u6027\u3002", "method": "\u6784\u5efa\u8003\u8651\u6b7b\u9501\u7684\u591a\u667a\u80fd\u4f53\u8def\u5f84\u89c4\u5212\uff08MAPF\uff09\u53c2\u8003\u6a21\u578b\uff0c\u5229\u7528\u57fa\u4e8e\u7f51\u683c\u7684\u73af\u5883\u548c\u5916\u90e8\u4eff\u771f\u8f6f\u4ef6\uff0c\u5bf9\u6bd4\u4f20\u7edf\u7b56\u7565\u4e0e\u57fa\u4e8ePPO\u548cIMPALA\u7b97\u6cd5\u7684MARL\u65b9\u6cd5\uff0c\u91cd\u70b9\u8bc4\u4f30\u4e0d\u540c\u8bad\u7ec3\u4e0e\u6267\u884c\u6a21\u5f0f\uff08\u5982CTDE\uff09\u4e0b\u7684\u6027\u80fd\u3002", "result": "\u5728\u590d\u6742\u62e5\u5835\u73af\u5883\u4e2d\uff0c\u91c7\u7528CTDE\u7684MARL\u7b56\u7565\u663e\u8457\u4f18\u4e8e\u89c4\u5219\u65b9\u6cd5\uff1b\u800c\u5728\u7a7a\u95f4\u5bbd\u677e\u7684\u7b80\u5355\u73af\u5883\u4e2d\uff0c\u89c4\u5219\u65b9\u6cd5\u56e0\u8ba1\u7b97\u5f00\u9500\u4f4e\u4ecd\u5177\u7ade\u4e89\u529b\u3002", "conclusion": "MARL\u4e3a\u52a8\u6001\u5185\u7269\u6d41\u7cfb\u7edf\u7684\u6b7b\u9501\u5904\u7406\u63d0\u4f9b\u4e86\u7075\u6d3b\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u4f46\u9700\u6839\u636e\u5177\u4f53\u8fd0\u884c\u73af\u5883\u8fdb\u884c\u9488\u5bf9\u6027\u8bbe\u8ba1\u3002"}}
{"id": "2511.06558", "categories": ["cs.AR", "cs.SE"], "pdf": "https://arxiv.org/pdf/2511.06558", "abs": "https://arxiv.org/abs/2511.06558", "authors": ["Akshay Revankar", "Charan Renganathan", "Sartaj Wariah"], "title": "Offloading Data Center Tax", "comment": null, "summary": "The data centers of today are running diverse workloads sharing many common lower level functions called tax components. Any optimization to any tax component will lead to performance improvements across the data center fleet. Typically, performance enhancements in tax components are achieved by offloading them to accelerators, however, it is not practical to offload every tax component. The goal of this paper is to identify opportunities to offload more than one tax component together. We focus on MongoDB which is a common microservice used in a large number of applications in the datacenter. We profile MongoDB running as part of the DeathStarBench benchmark suite, identifying its tax components and their microarchitectural implications. We make observations and suggestions based on the inferences made to offload a few of the tax components in this application.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u5982\u4f55\u901a\u8fc7\u8054\u5408\u5378\u8f7d\u591a\u4e2a\u201c\u7a0e\u7ec4\u4ef6\u201d\uff08tax components\uff09\u6765\u4f18\u5316\u6570\u636e\u4e2d\u5fc3\u4e2dMongoDB\u5fae\u670d\u52a1\u7684\u6027\u80fd\uff0c\u57fa\u4e8eDeathStarBench\u57fa\u51c6\u6d4b\u8bd5\u5206\u6790\u5176\u5fae\u67b6\u6784\u7279\u5f81\u5e76\u63d0\u51fa\u5378\u8f7d\u5efa\u8bae\u3002", "motivation": "\u73b0\u4ee3\u6570\u636e\u4e2d\u5fc3\u8fd0\u884c\u591a\u79cd\u5de5\u4f5c\u8d1f\u8f7d\uff0c\u8fd9\u4e9b\u8d1f\u8f7d\u5171\u4eab\u8bb8\u591a\u5e95\u5c42\u901a\u7528\u529f\u80fd\uff08\u79f0\u4e3a\u7a0e\u7ec4\u4ef6\uff09\uff0c\u4f18\u5316\u8fd9\u4e9b\u7ec4\u4ef6\u53ef\u5e26\u6765\u6574\u4f53\u6027\u80fd\u63d0\u5347\uff1b\u7136\u800c\uff0c\u5e76\u975e\u6240\u6709\u7ec4\u4ef6\u90fd\u9002\u5408\u5378\u8f7d\u5230\u52a0\u901f\u5668\uff0c\u56e0\u6b64\u9700\u8bc6\u522b\u53ef\u8054\u5408\u5378\u8f7d\u7684\u673a\u4f1a\u3002", "method": "\u4f5c\u8005\u4ee5MongoDB\u4e3a\u7814\u7a76\u5bf9\u8c61\uff0c\u5728DeathStarBench\u57fa\u51c6\u5957\u4ef6\u4e2d\u5bf9\u5176\u8fdb\u884c\u5256\u6790\uff0c\u8bc6\u522b\u5176\u7a0e\u7ec4\u4ef6\u53ca\u5176\u5fae\u67b6\u6784\u5f71\u54cd\uff0c\u5e76\u636e\u6b64\u63a8\u65ad\u51fa\u53ef\u8054\u5408\u5378\u8f7d\u7684\u7ec4\u4ef6\u3002", "result": "\u8bba\u6587\u8bc6\u522b\u4e86MongoDB\u4e2d\u7684\u82e5\u5e72\u7a0e\u7ec4\u4ef6\uff0c\u5e76\u57fa\u4e8e\u5fae\u67b6\u6784\u5206\u6790\u63d0\u51fa\u4e86\u53ef\u884c\u7684\u8054\u5408\u5378\u8f7d\u5efa\u8bae\u3002", "conclusion": "\u901a\u8fc7\u8054\u5408\u5378\u8f7d\u591a\u4e2a\u7a0e\u7ec4\u4ef6\uff0c\u53ef\u5728\u5b9e\u9645\u7cfb\u7edf\u4e2d\u5b9e\u73b0\u66f4\u9ad8\u6548\u7684\u6027\u80fd\u4f18\u5316\uff0c\u5c24\u5176\u9002\u7528\u4e8e\u5e7f\u6cdb\u90e8\u7f72\u7684\u5fae\u670d\u52a1\u5982MongoDB\u3002"}}
{"id": "2511.05825", "categories": ["cs.SE", "cs.SI"], "pdf": "https://arxiv.org/pdf/2511.05825", "abs": "https://arxiv.org/abs/2511.05825", "authors": ["Boyang Liu"], "title": "Design and Implementation of Data Acquisition and Analysis System for Programming Debugging Process Based On VS Code Plug-In", "comment": null, "summary": "In order to meet the needs of students' programming debugging ability training, this paper designs and implements a data acquisition and analysis system for programming debugging process based on VS Code plug-in, which aims to solve the limitation of traditional assessment methods that are difficult to fully evaluate students' debugging ability. The system supports a variety of programming languages, integrates debugging tasks and data acquisition functions, captures students' debugging behavior in the local editor in real time, and uploads the data to the platform database to realize the whole process monitoring and feedback, provides accurate debugging guidance for teachers, and improves the teaching effect. In terms of data analysis, the system proposed a debugging behavior analysis model based on abstract syntax tree, combined with node annotation, sequence recognition and cluster analysis and other technologies, to automatically track the context of students' debugging process and accurately identify key features in the debugging path. Through this tool, the system realizes the intelligent identification and labeling of the debugging direction and behavior pattern, and improves the refinement level of debugging data analysis. In this research system, a complex debugging scenario of multi-file and multi-task is introduced into the debugging problem design, which optimizes the multi-dimensional capturing ability of debugging data and lays a foundation for accurate debugging behavior analysis. Through several practical teaching tests, the feasibility and stability of the system are verified, which proves that it can effectively support procedural evaluation in programming debugging teaching, and provides a new direction for debugging behavior analysis research.", "AI": {"tldr": "\u672c\u6587\u8bbe\u8ba1\u5e76\u5b9e\u73b0\u4e86\u4e00\u4e2a\u57fa\u4e8eVS Code\u63d2\u4ef6\u7684\u7f16\u7a0b\u8c03\u8bd5\u8fc7\u7a0b\u6570\u636e\u91c7\u96c6\u4e0e\u5206\u6790\u7cfb\u7edf\uff0c\u901a\u8fc7\u62bd\u8c61\u8bed\u6cd5\u6811\u7b49\u6280\u672f\u5bf9\u5b66\u751f\u7684\u8c03\u8bd5\u884c\u4e3a\u8fdb\u884c\u667a\u80fd\u8bc6\u522b\u4e0e\u5206\u6790\uff0c\u652f\u6301\u591a\u8bed\u8a00\u3001\u591a\u6587\u4ef6\u3001\u591a\u4efb\u52a1\u7684\u590d\u6742\u8c03\u8bd5\u573a\u666f\uff0c\u9a8c\u8bc1\u4e86\u5176\u5728\u7f16\u7a0b\u6559\u5b66\u4e2d\u5bf9\u5b66\u751f\u8c03\u8bd5\u80fd\u529b\u8bc4\u4f30\u7684\u6709\u6548\u6027\u3002", "motivation": "\u4f20\u7edf\u8bc4\u4f30\u65b9\u6cd5\u96be\u4ee5\u5168\u9762\u8bc4\u4ef7\u5b66\u751f\u7684\u7f16\u7a0b\u8c03\u8bd5\u80fd\u529b\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u80fd\u591f\u5b9e\u65f6\u91c7\u96c6\u548c\u5206\u6790\u5b66\u751f\u8c03\u8bd5\u884c\u4e3a\u7684\u7cfb\u7edf\uff0c\u4ee5\u63d0\u4f9b\u66f4\u7cbe\u51c6\u7684\u6559\u5b66\u53cd\u9988\u3002", "method": "\u5f00\u53d1\u57fa\u4e8eVS Code\u63d2\u4ef6\u7684\u6570\u636e\u91c7\u96c6\u7cfb\u7edf\uff0c\u7ed3\u5408\u62bd\u8c61\u8bed\u6cd5\u6811\uff08AST\uff09\u3001\u8282\u70b9\u6807\u6ce8\u3001\u5e8f\u5217\u8bc6\u522b\u548c\u805a\u7c7b\u5206\u6790\u7b49\u6280\u672f\u6784\u5efa\u8c03\u8bd5\u884c\u4e3a\u5206\u6790\u6a21\u578b\uff0c\u5b9e\u73b0\u5bf9\u5b66\u751f\u8c03\u8bd5\u8fc7\u7a0b\u7684\u4e0a\u4e0b\u6587\u8ffd\u8e2a\u4e0e\u5173\u952e\u7279\u5f81\u8bc6\u522b\u3002", "result": "\u7cfb\u7edf\u5728\u5b9e\u9645\u6559\u5b66\u6d4b\u8bd5\u4e2d\u8868\u73b0\u51fa\u826f\u597d\u7684\u53ef\u884c\u6027\u4e0e\u7a33\u5b9a\u6027\uff0c\u80fd\u6709\u6548\u652f\u6301\u7f16\u7a0b\u8c03\u8bd5\u6559\u5b66\u4e2d\u7684\u8fc7\u7a0b\u6027\u8bc4\u4ef7\uff0c\u5e76\u63d0\u5347\u4e86\u8c03\u8bd5\u6570\u636e\u5206\u6790\u7684\u7cbe\u7ec6\u5316\u6c34\u5e73\u3002", "conclusion": "\u8be5\u7cfb\u7edf\u4e3a\u7f16\u7a0b\u8c03\u8bd5\u80fd\u529b\u7684\u8bc4\u4f30\u63d0\u4f9b\u4e86\u65b0\u65b9\u6cd5\uff0c\u63a8\u52a8\u4e86\u8c03\u8bd5\u884c\u4e3a\u5206\u6790\u7814\u7a76\u7684\u53d1\u5c55\uff0c\u5177\u6709\u826f\u597d\u7684\u6559\u5b66\u5e94\u7528\u524d\u666f\u3002"}}
{"id": "2511.06187", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2511.06187", "abs": "https://arxiv.org/abs/2511.06187", "authors": ["Mathew Joseph", "Tanush Savadi", "Abel Souza"], "title": "LiteCast: A Lightweight Forecaster for Carbon Optimizations", "comment": null, "summary": "Over recent decades, electricity demand has experienced sustained growth through widespread electrification of transportation and the accelerated expansion of Artificial Intelligence (AI). Grids have managed the resulting surges by scaling generation capacity, incorporating additional resources such as solar and wind, and implementing demand-response mechanisms. Altogether, these policies influence a region's carbon intensity by affecting its energy mix. To mitigate the environmental impacts of consumption, carbon-aware optimizations often rely on long-horizon, high-accuracy forecasts of the grid's carbon intensity that typically use compute intensive models with extensive historical energy mix data. In addition to limiting scalability, accuracy improvements do not necessarily translate into proportional increases in savings. Highlighting the need for more efficient forecasting strategies, we argue that carbon forecasting solutions can achieve the majority of savings without requiring highly precise and complex predictions. Instead, it is the preservation of the ranking of forecasts relative to the ground-truth that drives realized savings. In this paper, we present LiteCast, a lightweight time series forecasting method capable of quickly modeling a region's energy mix to estimate its carbon intensity. LiteCast requires only a few days of historical energy and weather data, delivering fast forecasts that can quickly adapt to sudden changes in the electrical grid. Our evaluation in 50 worldwide regions under various real-world workloads shows that LiteCast outperforms state-of-the-art forecasters, delivering 20% higher savings with near-optimal performance, achieving 97% of the maximum attainable average savings, while remaining lightweight, efficient to run, and adaptive to new data.", "AI": {"tldr": "LiteCast \u662f\u4e00\u79cd\u8f7b\u91cf\u7ea7\u78b3\u5f3a\u5ea6\u9884\u6d4b\u65b9\u6cd5\uff0c\u4ec5\u9700\u5c11\u91cf\u5386\u53f2\u6570\u636e\u5373\u53ef\u5feb\u901f\u9002\u5e94\u7535\u7f51\u53d8\u5316\uff0c\u5728\u5168\u740350\u4e2a\u5730\u533a\u5b9e\u6d4b\u4e2d\u6bd4\u73b0\u6709\u65b9\u6cd5\u8282\u770120%\u78b3\u6392\u653e\uff0c\u8fbe\u5230\u8fd1\u4f3c\u6700\u4f18\u6548\u679c\u3002", "motivation": "\u73b0\u6709\u9ad8\u7cbe\u5ea6\u78b3\u5f3a\u5ea6\u9884\u6d4b\u6a21\u578b\u8ba1\u7b97\u5f00\u9500\u5927\u3001\u6269\u5c55\u6027\u5dee\uff0c\u4e14\u7cbe\u5ea6\u63d0\u5347\u672a\u5fc5\u5e26\u6765\u663e\u8457\u8282\u80fd\u589e\u76ca\uff1b\u4f5c\u8005\u8ba4\u4e3a\u4fdd\u6301\u9884\u6d4b\u503c\u4e0e\u771f\u5b9e\u503c\u7684\u76f8\u5bf9\u6392\u5e8f\u66f4\u80fd\u9a71\u52a8\u5b9e\u9645\u8282\u80fd\uff0c\u56e0\u6b64\u63d0\u51fa\u66f4\u9ad8\u6548\u8f7b\u91cf\u7684\u9884\u6d4b\u7b56\u7565\u3002", "method": "\u63d0\u51fa LiteCast\uff0c\u4e00\u79cd\u8f7b\u91cf\u7ea7\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u65b9\u6cd5\uff0c\u4ec5\u4f7f\u7528\u51e0\u5929\u7684\u5386\u53f2\u80fd\u6e90\u4e0e\u5929\u6c14\u6570\u636e\uff0c\u5feb\u901f\u5efa\u6a21\u533a\u57df\u80fd\u6e90\u7ed3\u6784\u4ee5\u4f30\u7b97\u78b3\u5f3a\u5ea6\uff0c\u5e76\u80fd\u8fc5\u901f\u9002\u5e94\u7535\u7f51\u7a81\u53d8\u3002", "result": "\u572850\u4e2a\u5168\u7403\u5730\u533a\u7684\u591a\u79cd\u771f\u5b9e\u8d1f\u8f7d\u4e0b\u8bc4\u4f30\u8868\u660e\uff0cLiteCast \u6bd4\u5f53\u524d\u6700\u5148\u8fdb\u7684\u9884\u6d4b\u5668\u591a\u5b9e\u73b020%\u7684\u78b3\u8282\u7701\uff0c\u8fbe\u5230\u6700\u5927\u53ef\u5b9e\u73b0\u5e73\u5747\u8282\u7701\u768497%\uff0c\u540c\u65f6\u4fdd\u6301\u8f7b\u91cf\u3001\u9ad8\u6548\u548c\u5bf9\u65b0\u6570\u636e\u7684\u9002\u5e94\u6027\u3002", "conclusion": "LiteCast \u8bc1\u660e\u4e86\u65e0\u9700\u590d\u6742\u9ad8\u7cbe\u5ea6\u6a21\u578b\u4e5f\u80fd\u5b9e\u73b0\u63a5\u8fd1\u6700\u4f18\u7684\u78b3\u611f\u77e5\u8c03\u5ea6\u6548\u679c\uff0c\u4e3a\u78b3\u5f3a\u5ea6\u9884\u6d4b\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u3001\u5b9e\u7528\u7684\u65b0\u8303\u5f0f\u3002"}}
{"id": "2511.06149", "categories": ["cs.SE", "cs.MA"], "pdf": "https://arxiv.org/pdf/2511.06149", "abs": "https://arxiv.org/abs/2511.06149", "authors": ["Dominique Briechle", "Mohammed Fahad Ali", "Marit Briechle-Mathiszig", "Tobias Geger", "Robert Werner", "Andreas Rausch"], "title": "The Lifecycle Workbench -- A Configurable Framework for Digitized Product Maintenance Services", "comment": "This preprint has not undergone peer review or any post-submission improvements or corrections. Published at ECSA 2025 Tracks and Workshops", "summary": "The global production of electric goods is at an all-time high, causing negative environmental and health impacts as well as a continuing depletion of natural resources. Considering the worsening global climate change, a transition of current industrial processes is necessary to tackle the above-mentioned factors. To address this urgent issue, socio-economic systems like the Circular Economy (CE) provide options to reallocate the use of resources and products on a global scale. Especially in terms of product lifecycle-prolonging, this system provides suitable approaches to alter the current modes of product handling by society and industry alike, based on the condition of the products. Although the importance and benefits of sustainable services enabling these options are widely known, users tend to shy away from using them. One of the reasons is the missing reliability in terms of the knowledge of the costs associated with a particular service. This uncertainty in expected pricing can, therefore, lower the willingness of potential clients. However, not only clients struggle with the boundary conditions of such services. On the part of the potential providers of services, the monetary risk is often caused by the incapability to detect the condition of a product in advance. This can result on the provider side in a severe economic loss if this possibility is not covered by the service price or through the mass of items, which could allow equalization of serval service operations. To address these weak points in current service execution, the authors propose the \\textit{Lifecycle Workbench (LCW)}-ecosystem, which features digital representations to enhance the reliability of service pricing as well as the assessment of the condition of items, assemblies, and parts in the Circular Economy domain.", "AI": {"tldr": "\u672c\u6587\u9488\u5bf9\u5faa\u73af\u7ecf\u6d4e\u4e2d\u670d\u52a1\u5b9a\u4ef7\u4e0d\u53ef\u9760\u548c\u4ea7\u54c1\u72b6\u6001\u96be\u4ee5\u8bc4\u4f30\u7684\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u4e00\u4e2a\u540d\u4e3a\u201c\u751f\u547d\u5468\u671f\u5de5\u4f5c\u53f0\uff08LCW\uff09\u201d\u7684\u751f\u6001\u7cfb\u7edf\uff0c\u901a\u8fc7\u6570\u5b57\u5316\u624b\u6bb5\u63d0\u5347\u670d\u52a1\u5b9a\u4ef7\u7684\u53ef\u9760\u6027\u4e0e\u4ea7\u54c1\u72b6\u51b5\u8bc4\u4f30\u7684\u51c6\u786e\u6027\u3002", "motivation": "\u5f53\u524d\u7535\u5b50\u4ea7\u54c1\u751f\u4ea7\u5e26\u6765\u7684\u73af\u5883\u4e0e\u5065\u5eb7\u95ee\u9898\u65e5\u76ca\u4e25\u91cd\uff0c\u4e9f\u9700\u5411\u5faa\u73af\u7ecf\u6d4e\u8f6c\u578b\u3002\u7136\u800c\uff0c\u7528\u6237\u56e0\u670d\u52a1\u6210\u672c\u4e0d\u786e\u5b9a\u800c\u4e0d\u613f\u4f7f\u7528\u53ef\u6301\u7eed\u670d\u52a1\uff0c\u670d\u52a1\u63d0\u4f9b\u65b9\u4e5f\u56e0\u65e0\u6cd5\u63d0\u524d\u5224\u65ad\u4ea7\u54c1\u72b6\u6001\u800c\u9762\u4e34\u7ecf\u6d4e\u98ce\u9669\u3002", "method": "\u4f5c\u8005\u63d0\u51fa\u201c\u751f\u547d\u5468\u671f\u5de5\u4f5c\u53f0\uff08LCW\uff09\u201d\u751f\u6001\u7cfb\u7edf\uff0c\u5229\u7528\u6570\u5b57\u8868\u5f81\u6280\u672f\u6765\u589e\u5f3a\u5faa\u73af\u7ecf\u6d4e\u4e2d\u670d\u52a1\u5b9a\u4ef7\u7684\u53ef\u9760\u6027\uff0c\u5e76\u6539\u8fdb\u5bf9\u7269\u54c1\u3001\u7ec4\u4ef6\u548c\u96f6\u4ef6\u72b6\u6001\u7684\u8bc4\u4f30\u3002", "result": "\u8be5\u7cfb\u7edf\u6709\u671b\u89e3\u51b3\u5f53\u524d\u5faa\u73af\u7ecf\u6d4e\u670d\u52a1\u6267\u884c\u4e2d\u7684\u4e24\u5927\u5f31\u70b9\uff1a\u5ba2\u6237\u5bf9\u670d\u52a1\u4ef7\u683c\u7684\u4e0d\u786e\u5b9a\u6027\u4ee5\u53ca\u670d\u52a1\u63d0\u4f9b\u65b9\u5bf9\u4ea7\u54c1\u72b6\u6001\u5224\u65ad\u7684\u56f0\u96be\u3002", "conclusion": "LCW\u751f\u6001\u7cfb\u7edf\u901a\u8fc7\u6570\u5b57\u5316\u624b\u6bb5\u63d0\u9ad8\u4e86\u5faa\u73af\u7ecf\u6d4e\u4e2d\u670d\u52a1\u7684\u53ef\u9760\u6027\u548c\u53ef\u884c\u6027\uff0c\u6709\u52a9\u4e8e\u63a8\u52a8\u53ef\u6301\u7eed\u670d\u52a1\u7684\u5e7f\u6cdb\u5e94\u7528\u548c\u5de5\u4e1a\u6d41\u7a0b\u7684\u7eff\u8272\u8f6c\u578b\u3002"}}
{"id": "2511.06565", "categories": ["cs.AR", "cs.CL", "cs.DC", "cs.PL"], "pdf": "https://arxiv.org/pdf/2511.06565", "abs": "https://arxiv.org/abs/2511.06565", "authors": ["Arnab A Purkayastha", "Jay Tharwani", "Shobhit Aggarwal"], "title": "FPGA or GPU? Analyzing comparative research for application-specific guidance", "comment": "7 pages", "summary": "The growing complexity of computational workloads has amplified the need for efficient and specialized hardware accelerators. Field Programmable Gate Arrays (FPGAs) and Graphics Processing Units (GPUs) have emerged as prominent solutions, each excelling in specific domains. Although there is substantial research comparing FPGAs and GPUs, most of the work focuses primarily on performance metrics, offering limited insight into the specific types of applications that each accelerator benefits the most. This paper aims to bridge this gap by synthesizing insights from various research articles to guide users in selecting the appropriate accelerator for domain-specific applications. By categorizing the reviewed studies and analyzing key performance metrics, this work highlights the strengths, limitations, and ideal use cases for FPGAs and GPUs. The findings offer actionable recommendations, helping researchers and practitioners navigate trade-offs in performance, energy efficiency, and programmability.", "AI": {"tldr": "\u672c\u6587\u7efc\u8ff0\u5e76\u6bd4\u8f83\u4e86FPGA\u4e0eGPU\u5728\u4e0d\u540c\u5e94\u7528\u9886\u57df\u4e2d\u7684\u9002\u7528\u6027\uff0c\u8d85\u8d8a\u5355\u7eaf\u7684\u6027\u80fd\u6307\u6807\uff0c\u4e3a\u7528\u6237\u5728\u7279\u5b9a\u573a\u666f\u4e0b\u9009\u62e9\u5408\u9002\u7684\u786c\u4ef6\u52a0\u901f\u5668\u63d0\u4f9b\u6307\u5bfc\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u591a\u805a\u7126\u4e8eFPGA\u4e0eGPU\u7684\u6027\u80fd\u5bf9\u6bd4\uff0c\u7f3a\u4e4f\u5bf9\u5404\u81ea\u6700\u9002\u5408\u5e94\u7528\u573a\u666f\u7684\u6df1\u5165\u5206\u6790\uff0c\u672c\u6587\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u7efc\u5408\u5206\u6790\u591a\u7bc7\u7814\u7a76\u6587\u732e\uff0c\u5bf9\u5e94\u7528\u8fdb\u884c\u5206\u7c7b\uff0c\u5e76\u8bc4\u4f30\u5173\u952e\u6027\u80fd\u6307\u6807\uff0c\u4ee5\u8bc6\u522bFPGA\u548cGPU\u7684\u4f18\u52bf\u3001\u5c40\u9650\u53ca\u7406\u60f3\u4f7f\u7528\u573a\u666f\u3002", "result": "\u660e\u786e\u4e86FPGA\u548cGPU\u5728\u6027\u80fd\u3001\u80fd\u6548\u548c\u53ef\u7f16\u7a0b\u6027\u65b9\u9762\u7684\u6743\u8861\uff0c\u63d0\u51fa\u4e86\u9488\u5bf9\u4e0d\u540c\u9886\u57df\u5e94\u7528\u7684\u52a0\u901f\u5668\u9009\u62e9\u5efa\u8bae\u3002", "conclusion": "FPGA\u548cGPU\u5404\u6709\u4f18\u52bf\uff0c\u5e94\u6839\u636e\u5177\u4f53\u5e94\u7528\u9700\u6c42\u9009\u62e9\uff1b\u672c\u6587\u63d0\u4f9b\u7684\u6307\u5bfc\u6709\u52a9\u4e8e\u7814\u7a76\u4eba\u5458\u548c\u5de5\u7a0b\u5e08\u505a\u51fa\u66f4\u4f18\u51b3\u7b56\u3002"}}
{"id": "2511.05862", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2511.05862", "abs": "https://arxiv.org/abs/2511.05862", "authors": ["Xinlong Zhao", "Tong Jia", "Minghua He", "Ying Li", "Gang Huang"], "title": "ZeroLog: Zero-Label Generalizable Cross-System Log-based Anomaly Detection", "comment": "12 pages, 17 figures, and 3 tables; accepted by ISSRE 2025", "summary": "Log-based anomaly detection is an important task in ensuring the stability and reliability of software systems. One of the key problems in this task is the lack of labeled logs. Existing works usually leverage large-scale labeled logs from mature systems to train an anomaly detection model of a target system based on the idea of transfer learning. However, these works still require a certain number of labeled logs from the target system. In this paper, we take a step forward and study a valuable yet underexplored setting: zero-label cross-system log-based anomaly detection, that is, no labeled logs are available in the target system. Specifically, we propose ZeroLog, a system-agnostic representation meta-learning method that enables cross-system log-based anomaly detection under zero-label conditions. To achieve this, we leverage unsupervised domain adaptation to perform adversarial training between the source and target domains, aiming to learn system-agnostic general feature representations. By employing meta-learning, the learned representations are further generalized to the target system without any target labels. Experimental results on three public log datasets from different systems show that ZeroLog reaches over 80% F1-score without labels, comparable to state-of-the-art cross-system methods trained with labeled logs, and outperforms existing methods under zero-label conditions.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faZeroLog\uff0c\u4e00\u79cd\u65e0\u9700\u76ee\u6807\u7cfb\u7edf\u6807\u7b7e\u7684\u8de8\u7cfb\u7edf\u65e5\u5fd7\u5f02\u5e38\u68c0\u6d4b\u65b9\u6cd5\uff0c\u901a\u8fc7\u65e0\u76d1\u7763\u57df\u81ea\u9002\u5e94\u4e0e\u5143\u5b66\u4e60\u5b9e\u73b0\u7cfb\u7edf\u65e0\u5173\u7684\u7279\u5f81\u8868\u793a\uff0c\u5728\u96f6\u6807\u7b7e\u6761\u4ef6\u4e0b\u8fbe\u5230\u4e0e\u4f7f\u7528\u6807\u7b7e\u7684\u5148\u8fdb\u65b9\u6cd5\u76f8\u5f53\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u8de8\u7cfb\u7edf\u65e5\u5fd7\u5f02\u5e38\u68c0\u6d4b\u65b9\u6cd5\u4ecd\u9700\u5c11\u91cf\u76ee\u6807\u7cfb\u7edf\u6807\u6ce8\u65e5\u5fd7\uff0c\u800c\u5b9e\u9645\u4e2d\u76ee\u6807\u7cfb\u7edf\u5f80\u5f80\u7f3a\u4e4f\u4efb\u4f55\u6807\u7b7e\u3002\u56e0\u6b64\uff0c\u4f5c\u8005\u63a2\u7d22\u96f6\u6807\u7b7e\u6761\u4ef6\u4e0b\u7684\u8de8\u7cfb\u7edf\u65e5\u5fd7\u5f02\u5e38\u68c0\u6d4b\u8fd9\u4e00\u66f4\u5177\u6311\u6218\u6027\u4e14\u672a\u88ab\u5145\u5206\u7814\u7a76\u7684\u8bbe\u5b9a\u3002", "method": "\u63d0\u51faZeroLog\u65b9\u6cd5\uff0c\u7ed3\u5408\u65e0\u76d1\u7763\u57df\u81ea\u9002\u5e94\uff08\u901a\u8fc7\u6e90\u57df\u4e0e\u76ee\u6807\u57df\u4e4b\u95f4\u7684\u5bf9\u6297\u8bad\u7ec3\uff09\u548c\u5143\u5b66\u4e60\uff0c\u4ee5\u5b66\u4e60\u7cfb\u7edf\u65e0\u5173\u7684\u901a\u7528\u7279\u5f81\u8868\u793a\uff0c\u5e76\u5728\u65e0\u76ee\u6807\u6807\u7b7e\u7684\u60c5\u51b5\u4e0b\u6cdb\u5316\u5230\u76ee\u6807\u7cfb\u7edf\u3002", "result": "\u5728\u4e09\u4e2a\u4e0d\u540c\u7cfb\u7edf\u7684\u516c\u5f00\u65e5\u5fd7\u6570\u636e\u96c6\u4e0a\u5b9e\u9a8c\u8868\u660e\uff0cZeroLog\u5728\u65e0\u6807\u7b7e\u6761\u4ef6\u4e0bF1\u5206\u6570\u8d85\u8fc780%\uff0c\u6027\u80fd\u5ab2\u7f8e\u4f7f\u7528\u6807\u7b7e\u8bad\u7ec3\u7684\u6700\u5148\u8fdb\u8de8\u7cfb\u7edf\u65b9\u6cd5\uff0c\u5e76\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u96f6\u6807\u7b7e\u65b9\u6cd5\u3002", "conclusion": "ZeroLog\u6709\u6548\u89e3\u51b3\u4e86\u96f6\u6807\u7b7e\u8de8\u7cfb\u7edf\u65e5\u5fd7\u5f02\u5e38\u68c0\u6d4b\u95ee\u9898\uff0c\u8bc1\u660e\u4e86\u7cfb\u7edf\u65e0\u5173\u8868\u793a\u4e0e\u5143\u5b66\u4e60\u7ed3\u5408\u5728\u8be5\u4efb\u52a1\u4e2d\u7684\u6f5c\u529b\uff0c\u4e3a\u5b9e\u9645\u90e8\u7f72\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848\u3002"}}
{"id": "2511.05882", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2511.05882", "abs": "https://arxiv.org/abs/2511.05882", "authors": ["Xinlong Zhao", "Tong Jia", "Minghua He", "Ying Li"], "title": "Generality Is Not Enough: Zero-Label Cross-System Log-Based Anomaly Detection via Knowledge-Level Collaboration", "comment": "5 pages, 2 figures, 1 table", "summary": "Log-based anomaly detection is crucial for ensuring software system stability. However, the scarcity of labeled logs limits rapid deployment to new systems. Cross-system transfer has become an important research direction. State-of-the-art approaches perform well with a few labeled target logs, but limitations remain: small-model methods transfer general knowledge but overlook mismatches with the target system's proprietary knowledge; LLM-based methods can capture proprietary patterns but rely on a few positive examples and incur high inference cost. Existing LLM-small model collaborations route 'simple logs' to the small model and 'complex logs' to the LLM based on output uncertainty. In zero-label cross-system settings, supervised sample complexity is unavailable, and such routing does not consider knowledge separation. To address this, we propose GeneralLog, a novel LLM-small model collaborative method for zero-label cross-system log anomaly detection. GeneralLog dynamically routes unlabeled logs, letting the LLM handle 'proprietary logs' and the small model 'general logs,' enabling cross-system generalization without labeled target logs. Experiments on three public log datasets show that GeneralLog achieves over 90% F1-score under a fully zero-label setting, significantly outperforming existing methods.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faGeneralLog\uff0c\u4e00\u79cd\u5728\u65e0\u76ee\u6807\u6807\u7b7e\u573a\u666f\u4e0b\u7ed3\u5408\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u4e0e\u5c0f\u6a21\u578b\u7684\u534f\u4f5c\u65b9\u6cd5\uff0c\u901a\u8fc7\u52a8\u6001\u8def\u7531\u5c06\u201c\u901a\u7528\u65e5\u5fd7\u201d\u4ea4\u7531\u5c0f\u6a21\u578b\u5904\u7406\u3001\u201c\u4e13\u6709\u65e5\u5fd7\u201d\u4ea4\u7531LLM\u5904\u7406\uff0c\u5b9e\u73b0\u8de8\u7cfb\u7edf\u7684\u96f6\u6837\u672c\u65e5\u5fd7\u5f02\u5e38\u68c0\u6d4b\uff0c\u5728\u4e09\u4e2a\u516c\u5f00\u6570\u636e\u96c6\u4e0aF1\u5206\u6570\u8d85\u8fc790%\u3002", "motivation": "\u73b0\u6709\u8de8\u7cfb\u7edf\u65e5\u5fd7\u5f02\u5e38\u68c0\u6d4b\u65b9\u6cd5\u5728\u96f6\u6807\u7b7e\u8bbe\u7f6e\u4e0b\u9762\u4e34\u6311\u6218\uff1a\u5c0f\u6a21\u578b\u5ffd\u7565\u76ee\u6807\u7cfb\u7edf\u4e13\u6709\u77e5\u8bc6\uff0c\u800c\u5927\u8bed\u8a00\u6a21\u578b\u4f9d\u8d56\u5c11\u91cf\u6b63\u6837\u672c\u4e14\u63a8\u7406\u6210\u672c\u9ad8\uff1b\u5f53\u524d\u534f\u4f5c\u7b56\u7565\u672a\u8003\u8651\u77e5\u8bc6\u5206\u79bb\uff0c\u65e0\u6cd5\u6709\u6548\u5e94\u7528\u4e8e\u65e0\u76d1\u7763\u8fc1\u79fb\u573a\u666f\u3002", "method": "\u63d0\u51faGeneralLog\u6846\u67b6\uff0c\u5728\u65e0\u76ee\u6807\u6807\u7b7e\u6761\u4ef6\u4e0b\u52a8\u6001\u5206\u914d\u65e5\u5fd7\uff1a\u5229\u7528\u5c0f\u6a21\u578b\u5904\u7406\u901a\u7528\u6a21\u5f0f\u65e5\u5fd7\uff0c\u5927\u8bed\u8a00\u6a21\u578b\u5904\u7406\u5305\u542b\u7cfb\u7edf\u4e13\u6709\u77e5\u8bc6\u7684\u65e5\u5fd7\uff0c\u5b9e\u73b0\u77e5\u8bc6\u89e3\u8026\u4e0e\u9ad8\u6548\u534f\u4f5c\u3002", "result": "\u5728\u4e09\u4e2a\u516c\u5f00\u65e5\u5fd7\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cGeneralLog\u5728\u5b8c\u5168\u65e0\u6807\u7b7e\u8bbe\u7f6e\u4e0bF1\u5206\u6570\u8d85\u8fc790%\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "GeneralLog\u901a\u8fc7\u5408\u7406\u7684\u77e5\u8bc6\u5206\u5de5\u4e0e\u52a8\u6001\u8def\u7531\u673a\u5236\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u96f6\u6807\u7b7e\u8de8\u7cfb\u7edf\u65e5\u5fd7\u5f02\u5e38\u68c0\u6d4b\u4e2d\u7684\u77e5\u8bc6\u8fc1\u79fb\u4e0e\u6548\u7387\u95ee\u9898\uff0c\u4e3a\u5b9e\u9645\u90e8\u7f72\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848\u3002"}}
{"id": "2511.06345", "categories": ["cs.DC", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.06345", "abs": "https://arxiv.org/abs/2511.06345", "authors": ["Kelun Lei", "Hailong Yang", "Huaitao Zhang", "Xin You", "Kaige Zhang", "Zhongzhi Luan", "Yi Liu", "Depei Qian"], "title": "PRAGMA: A Profiling-Reasoned Multi-Agent Framework for Automatic Kernel Optimization", "comment": null, "summary": "Designing high-performance kernels requires expert-level tuning and a deep understanding of hardware characteristics. Recent advances in large language models (LLMs) have enabled automated kernel generation, yet most existing systems rely solely on correctness or execution time feedback, lacking the ability to reason about low-level performance bottlenecks. In this paper, we introduce PRAGMA, a profile-guided AI kernel generation framework that integrates execution feedback and fine-grained hardware profiling into the reasoning loop. PRAGMA enables LLMs to identify performance bottlenecks, preserve historical best versions, and iteratively refine code quality. We evaluate PRAGMA on KernelBench, covering GPU and CPU backends. Results show that PRAGMA consistently outperforms baseline AIKG without profiling enabled and achieves 2.81$\\times$ and 2.30$\\times$ averaged speedups against Torch on CPU and GPU platforms, respectively.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faPRAGMA\uff0c\u4e00\u79cd\u7ed3\u5408\u6267\u884c\u53cd\u9988\u4e0e\u7ec6\u7c92\u5ea6\u786c\u4ef6\u6027\u80fd\u5206\u6790\u7684AI\u6838\u51fd\u6570\u751f\u6210\u6846\u67b6\uff0c\u663e\u8457\u63d0\u5347\u751f\u6210\u6838\u51fd\u6570\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u81ea\u52a8\u6838\u51fd\u6570\u751f\u6210\u65b9\u6cd5\u5927\u591a\u4ec5\u4f9d\u8d56\u6b63\u786e\u6027\u6216\u6267\u884c\u65f6\u95f4\u53cd\u9988\uff0c\u7f3a\u4e4f\u5bf9\u5e95\u5c42\u6027\u80fd\u74f6\u9888\u7684\u63a8\u7406\u80fd\u529b\u3002", "method": "PRAGMA\u5c06\u6267\u884c\u53cd\u9988\u548c\u7ec6\u7c92\u5ea6\u786c\u4ef6\u6027\u80fd\u5206\u6790\u6574\u5408\u5230\u5927\u8bed\u8a00\u6a21\u578b\u7684\u63a8\u7406\u5faa\u73af\u4e2d\uff0c\u4f7f\u5176\u80fd\u591f\u8bc6\u522b\u6027\u80fd\u74f6\u9888\u3001\u4fdd\u7559\u5386\u53f2\u6700\u4f18\u7248\u672c\u5e76\u8fed\u4ee3\u4f18\u5316\u4ee3\u7801\u3002", "result": "\u5728KernelBench\u57fa\u51c6\u4e0a\uff0cPRAGMA\u5728CPU\u548cGPU\u5e73\u53f0\u4e0a\u5206\u522b\u6bd4Torch\u5e73\u5747\u63d0\u901f2.81\u500d\u548c2.30\u500d\uff0c\u5e76\u4f18\u4e8e\u672a\u542f\u7528\u6027\u80fd\u5206\u6790\u7684\u57fa\u7ebfAIKG\u65b9\u6cd5\u3002", "conclusion": "PRAGMA\u901a\u8fc7\u5f15\u5165\u786c\u4ef6\u6027\u80fd\u5206\u6790\u663e\u8457\u63d0\u5347\u4e86AI\u81ea\u52a8\u751f\u6210\u9ad8\u6027\u80fd\u6838\u51fd\u6570\u7684\u80fd\u529b\uff0c\u9a8c\u8bc1\u4e86\u5176\u5728\u591a\u786c\u4ef6\u5e73\u53f0\u4e0a\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2511.07257", "categories": ["cs.SE", "cs.MA"], "pdf": "https://arxiv.org/pdf/2511.07257", "abs": "https://arxiv.org/abs/2511.07257", "authors": ["Hanya Elhashemy", "Youssef Lotfy", "Yongjian Tang"], "title": "Bridging the Prototype-Production Gap: A Multi-Agent System for Notebooks Transformation", "comment": null, "summary": "The increasing adoption of Jupyter notebooks in data science and machine learning workflows has created a gap between exploratory code development and production-ready software systems. While notebooks excel at iterative development and visualization, they often lack proper software engineering principles, making their transition to production environments challenging. This paper presents Codelevate, a novel multi-agent system that automatically transforms Jupyter notebooks into well-structured, maintainable Python code repositories. Our system employs three specialized agents - Architect, Developer, and Structure - working in concert through a shared dependency tree to ensure architectural coherence and code quality. Our experimental results validate Codelevate's capability to bridge the prototype-to-production gap through autonomous code transformation, yielding quantifiable improvements in code quality metrics while preserving computational semantics.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faCodelevate\uff0c\u4e00\u4e2a\u57fa\u4e8e\u591a\u667a\u80fd\u4f53\u7684\u7cfb\u7edf\uff0c\u80fd\u81ea\u52a8\u5c06Jupyter Notebook\u8f6c\u6362\u4e3a\u7ed3\u6784\u826f\u597d\u3001\u53ef\u7ef4\u62a4\u7684Python\u4ee3\u7801\u5e93\uff0c\u6709\u6548\u5f25\u5408\u4ece\u539f\u578b\u5230\u751f\u4ea7\u7684\u5dee\u8ddd\u3002", "motivation": "Jupyter Notebook\u5728\u6570\u636e\u79d1\u5b66\u548c\u673a\u5668\u5b66\u4e60\u4e2d\u5e7f\u6cdb\u4f7f\u7528\uff0c\u4f46\u5176\u7f3a\u4e4f\u8f6f\u4ef6\u5de5\u7a0b\u89c4\u8303\uff0c\u96be\u4ee5\u76f4\u63a5\u7528\u4e8e\u751f\u4ea7\u73af\u5883\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u81ea\u52a8\u5316\u65b9\u6cd5\u5c06\u5176\u8f6c\u5316\u4e3a\u9ad8\u8d28\u91cf\u3001\u53ef\u7ef4\u62a4\u7684\u4ee3\u7801\u3002", "method": "\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u5305\u542bArchitect\u3001Developer\u548cStructure\u4e09\u4e2a\u4e13\u7528\u667a\u80fd\u4f53\u7684\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\uff0c\u901a\u8fc7\u5171\u4eab\u4f9d\u8d56\u6811\u534f\u540c\u5de5\u4f5c\uff0c\u5b9e\u73b0Notebook\u5230\u7ed3\u6784\u5316Python\u9879\u76ee\u7684\u81ea\u52a8\u8f6c\u6362\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cCodelevate\u80fd\u5728\u4fdd\u6301\u8ba1\u7b97\u8bed\u4e49\u4e0d\u53d8\u7684\u524d\u63d0\u4e0b\uff0c\u663e\u8457\u63d0\u5347\u4ee3\u7801\u8d28\u91cf\u6307\u6807\uff0c\u6210\u529f\u5b9e\u73b0\u4ece\u539f\u578b\u5230\u751f\u4ea7\u4ee3\u7801\u7684\u81ea\u52a8\u5316\u8f6c\u5316\u3002", "conclusion": "Codelevate\u6709\u6548\u89e3\u51b3\u4e86Jupyter Notebook\u5411\u751f\u4ea7\u7ea7\u4ee3\u7801\u8fc1\u79fb\u7684\u96be\u9898\uff0c\u4e3a\u6570\u636e\u79d1\u5b66\u5de5\u4f5c\u6d41\u7684\u5de5\u7a0b\u5316\u63d0\u4f9b\u4e86\u53ef\u884c\u8def\u5f84\u3002"}}
{"id": "2511.06736", "categories": ["cs.AR", "cs.OS"], "pdf": "https://arxiv.org/pdf/2511.06736", "abs": "https://arxiv.org/abs/2511.06736", "authors": ["Arsalan Ali Malik", "John Buchanan", "Aydin Aysu"], "title": "Preemption-Enhanced Benchmark Suite for FPGAs", "comment": "13 Pages, 4 Figures, 4 Tables", "summary": "Field-Programmable Gate Arrays (FPGAs) have become essential in cloud computing due to their reconfigurability, energy efficiency, and ability to accelerate domain-specific workloads. As FPGA adoption grows, research into task scheduling and preemption techniques has intensified. However, the field lacks a standardized benchmarking framework for consistent and reproducible evaluation. Many existing studies propose innovative scheduling or preemption mechanisms but often rely on proprietary or synthetic benchmarks, limiting generalizability and making comparison difficult. This methodical fragmentation hinders effective evaluation of scheduling strategies and preemption in multi-tenant FPGA environments.\n  This paper presents the first open-source preemption-enabled benchmark suite for evaluating FPGA preemption strategies and testing new scheduling algorithms, without requiring users to create preemption workloads from scratch. The suite includes 27 diverse applications spanning cryptography, AI/ML, computation-intensive workloads, communication systems, and multimedia processing. Each benchmark integrates comprehensive context-saving and restoration mechanisms, facilitating reproducible research and consistent comparisons. Our suite not only simplifies testing FPGA scheduling policies but also benefits OS research by enabling the evaluation of scheduling fairness, resource allocation efficiency, and context-switching performance in multi-tenant FPGA systems, ultimately supporting the development of better operating systems and scheduling policies for FPGA-based environments. We also provide guidelines for adding new benchmarks, enabling future research to expand and refine FPGA preemption and scheduling evaluation.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u9996\u4e2a\u5f00\u6e90\u7684\u652f\u6301\u62a2\u5360\u7684 FPGA \u57fa\u51c6\u6d4b\u8bd5\u5957\u4ef6\uff0c\u5305\u542b 27 \u4e2a\u6db5\u76d6\u591a\u4e2a\u9886\u57df\u7684\u5e94\u7528\uff0c\u5185\u7f6e\u4e0a\u4e0b\u6587\u4fdd\u5b58\u4e0e\u6062\u590d\u673a\u5236\uff0c\u65e8\u5728\u6807\u51c6\u5316 FPGA \u62a2\u5360\u7b56\u7565\u4e0e\u8c03\u5ea6\u7b97\u6cd5\u7684\u8bc4\u4f30\u3002", "motivation": "\u5f53\u524d FPGA \u8c03\u5ea6\u4e0e\u62a2\u5360\u7814\u7a76\u7f3a\u4e4f\u7edf\u4e00\u3001\u53ef\u590d\u73b0\u7684\u57fa\u51c6\u6d4b\u8bd5\u6846\u67b6\uff0c\u591a\u6570\u7814\u7a76\u4f9d\u8d56\u79c1\u6709\u6216\u5408\u6210\u57fa\u51c6\uff0c\u9650\u5236\u4e86\u7ed3\u679c\u7684\u901a\u7528\u6027\u548c\u53ef\u6bd4\u6027\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u5f00\u6e90\u7684\u9884\u96c6\u6210\u62a2\u5360\u529f\u80fd\u7684\u57fa\u51c6\u6d4b\u8bd5\u5957\u4ef6\uff0c\u5305\u542b 27 \u4e2a\u591a\u6837\u5316\u5e94\u7528\uff0c\u5e76\u63d0\u4f9b\u4e0a\u4e0b\u6587\u4fdd\u5b58/\u6062\u590d\u673a\u5236\u53ca\u65b0\u589e\u57fa\u51c6\u7684\u6307\u5357\u3002", "result": "\u8be5\u5957\u4ef6\u652f\u6301\u5bf9 FPGA \u62a2\u5360\u7b56\u7565\u548c\u8c03\u5ea6\u7b97\u6cd5\u8fdb\u884c\u4e00\u81f4\u3001\u53ef\u590d\u73b0\u7684\u8bc4\u4f30\uff0c\u6709\u52a9\u4e8e\u64cd\u4f5c\u7cfb\u7edf\u7814\u7a76\u4e2d\u7684\u516c\u5e73\u6027\u3001\u8d44\u6e90\u5206\u914d\u6548\u7387\u548c\u4e0a\u4e0b\u6587\u5207\u6362\u6027\u80fd\u5206\u6790\u3002", "conclusion": "\u8be5\u57fa\u51c6\u5957\u4ef6\u586b\u8865\u4e86 FPGA \u8c03\u5ea6\u4e0e\u62a2\u5360\u7814\u7a76\u4e2d\u7684\u6807\u51c6\u5316\u7a7a\u767d\uff0c\u4e3a\u672a\u6765 FPGA \u64cd\u4f5c\u7cfb\u7edf\u548c\u8c03\u5ea6\u7b56\u7565\u7684\u53d1\u5c55\u63d0\u4f9b\u4e86\u6709\u529b\u652f\u6491\u3002"}}
{"id": "2511.06599", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2511.06599", "abs": "https://arxiv.org/abs/2511.06599", "authors": ["Siddharth Agarwal", "Maria A. Rodriguez", "Rajkumar Buyya"], "title": "Saarthi: An End-to-End Intelligent Platform for Optimising Distributed Serverless Workloads", "comment": "12 pages, 9 figures, 1 table, 2 algorithms", "summary": "FaaS offers significant advantages with its infrastructure abstraction, on-demand execution, and attractive no idle resource pricing for modern cloud applications. Despite these benefits, challenges such as startup latencies, static configurations, sub-optimal resource allocation and scheduling still exist due to coupled resource offering and workload-agnostic generic scheduling behaviour. These issues often lead to inconsistent function performance and unexpected operational costs for users and service providers. This paper introduces Saarthi, a novel, end-to-end serverless framework that intelligently manages the dynamic resource needs of function workloads, representing a significant step toward self-driving serverless platforms. Unlike platforms that rely on static resource configurations, Saarthi is input-aware, allowing it to intelligently anticipate resource requirements based on the characteristics of an incoming request payload. This input-driven approach reinforces function right-sizing and enables smart request orchestration across available function configurations. Saarthi further integrates a proactive fault-tolerant redundancy mechanism and employs a multi-objective Integer Linear Programming (ILP) model to maintain an optimal function quantity. This optimisation aims to maximise system throughput while simultaneously reducing overall operational costs. We validate the effectiveness of Saarthi by implementing it as a framework atop OpenFaaS. Our results demonstrate Saarthi's ability to achieve up to 1.45x better throughput, 1.84x reduced costs, while maintaining up to 98.3% service level targets with an overhead of up to 0.2 seconds as compared to the baseline OpenFaaS.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86Saarthi\uff0c\u4e00\u79cd\u65b0\u578b\u7aef\u5230\u7aef\u65e0\u670d\u52a1\u5668\u6846\u67b6\uff0c\u901a\u8fc7\u8f93\u5165\u611f\u77e5\u7684\u8d44\u6e90\u9884\u6d4b\u3001\u667a\u80fd\u8bf7\u6c42\u7f16\u6392\u548c\u591a\u76ee\u6807\u4f18\u5316\uff0c\u5728\u63d0\u5347\u541e\u5410\u91cf\u7684\u540c\u65f6\u663e\u8457\u964d\u4f4e\u6210\u672c\uff0c\u5e76\u4fdd\u969c\u670d\u52a1\u6c34\u5e73\u3002", "motivation": "\u5f53\u524dFaaS\u5e73\u53f0\u5b58\u5728\u542f\u52a8\u5ef6\u8fdf\u9ad8\u3001\u8d44\u6e90\u914d\u7f6e\u9759\u6001\u3001\u8c03\u5ea6\u7b56\u7565\u4e0e\u5de5\u4f5c\u8d1f\u8f7d\u65e0\u5173\u7b49\u95ee\u9898\uff0c\u5bfc\u81f4\u51fd\u6570\u6027\u80fd\u4e0d\u7a33\u5b9a\u548c\u8fd0\u8425\u6210\u672c\u4e0d\u53ef\u63a7\uff0c\u4e9f\u9700\u4e00\u79cd\u80fd\u52a8\u6001\u9002\u914d\u8d1f\u8f7d\u9700\u6c42\u7684\u667a\u80fd\u65e0\u670d\u52a1\u5668\u6846\u67b6\u3002", "method": "Saarthi\u91c7\u7528\u8f93\u5165\u611f\u77e5\u673a\u5236\u9884\u6d4b\u51fd\u6570\u8d44\u6e90\u9700\u6c42\uff0c\u5b9e\u73b0\u7cbe\u51c6\u7684\u51fd\u6570\u8d44\u6e90\u914d\u7f6e\uff1b\u7ed3\u5408\u4e3b\u52a8\u5bb9\u9519\u5197\u4f59\u673a\u5236\uff0c\u5e76\u5229\u7528\u591a\u76ee\u6807\u6574\u6570\u7ebf\u6027\u89c4\u5212\uff08ILP\uff09\u6a21\u578b\u4f18\u5316\u51fd\u6570\u5b9e\u4f8b\u6570\u91cf\uff0c\u4ee5\u6700\u5927\u5316\u541e\u5410\u91cf\u5e76\u6700\u5c0f\u5316\u6210\u672c\u3002", "result": "\u5728OpenFaaS\u4e0a\u5b9e\u73b0\u7684\u5b9e\u9a8c\u8868\u660e\uff0cSaarthi\u76f8\u6bd4\u57fa\u7ebf\u53ef\u63d0\u53471.45\u500d\u541e\u5410\u91cf\uff0c\u964d\u4f4e1.84\u500d\u6210\u672c\uff0c\u540c\u65f6\u7ef4\u6301\u9ad8\u8fbe98.3%\u7684\u670d\u52a1\u6c34\u5e73\u76ee\u6807\uff0c\u4ec5\u5f15\u5165\u6700\u591a0.2\u79d2\u7684\u5f00\u9500\u3002", "conclusion": "Saarthi\u901a\u8fc7\u8f93\u5165\u9a71\u52a8\u7684\u8d44\u6e90\u7ba1\u7406\u4e0e\u667a\u80fd\u8c03\u5ea6\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u73b0\u6709FaaS\u5e73\u53f0\u5728\u6027\u80fd\u4e0e\u6210\u672c\u4e4b\u95f4\u7684\u6743\u8861\u95ee\u9898\uff0c\u4e3a\u8fc8\u5411\u81ea\u9a71\u52a8\u65e0\u670d\u52a1\u5668\u5e73\u53f0\u63d0\u4f9b\u4e86\u53ef\u884c\u8def\u5f84\u3002"}}
{"id": "2511.06605", "categories": ["cs.DC", "cs.AR"], "pdf": "https://arxiv.org/pdf/2511.06605", "abs": "https://arxiv.org/abs/2511.06605", "authors": ["Suchita Pati", "Mahzabeen Islam", "Shaizeen Aga", "Mohamed Assem Ibrahim"], "title": "DMA Collectives for Efficient ML Communication Offloads", "comment": null, "summary": "Offloading machine learning (ML) communication collectives to direct memory access (DMA) engines has emerged as an interesting and low-cost solution to efficiently overlap computation and communication in inference and training. Doing so delivers superior concurrent performance by freeing up all GPU cores for computation and also lowers interference in the memory sub-system (caches). While DMA collectives show strong promise, prior works have only studied them in limited context (bandwidth-bound transfer sizes only, performance-only).\n  To address this, we provide a comprehensive performance, power/energy and synchronization costs analysis of offloading ML communication collectives (all-gather, all-to-all) to DMA engines on state-of-the-art AMD Instinct MI300X GPUs. Our analysis reveals that, compared to the state-of-the-art RCCL communication collectives library, DMA collectives are at-par or better for large sizes (10s of MB to GB) in terms of both performance (16% better) and power (32% better). However, they significantly lag for latency-bound small sizes; 4.5X and 2.5X slower for all-gather and all-to-all, respectively. We provide a detailed latency breakdown of a DMA transfer and identify that DMA command scheduling and synchronization costs can limit DMA collective performance. To tackle this, we harness existing DMA architecture innovations, hitherto untapped, to build optimized DMA collectives and demonstrate their efficacy on real hardware. Our optimized implementations considerably close the performance gap for DMA collectives at smaller sizes (30% slower and 20% faster all-gather and all-to-all, respectively) and further improves performance (by 7%) and power savings at larger sizes (3-10%). Overall, this work represents a significant step toward making DMA collectives suitable for adoption in mainstream collective libraries.", "AI": {"tldr": "\u672c\u6587\u5bf9\u5728AMD MI300X GPU\u4e0a\u5c06\u673a\u5668\u5b66\u4e60\u901a\u4fe1\u96c6\u5408\u64cd\u4f5c\uff08\u5982all-gather\u3001all-to-all\uff09\u5378\u8f7d\u5230DMA\u5f15\u64ce\u7684\u6027\u80fd\u3001\u529f\u8017/\u80fd\u8017\u53ca\u540c\u6b65\u5f00\u9500\u8fdb\u884c\u4e86\u5168\u9762\u5206\u6790\uff0c\u53d1\u73b0DMA\u5728\u5927\u6570\u636e\u91cf\u65f6\u4f18\u4e8e\u73b0\u6709RCCL\u5e93\uff0c\u4f46\u5728\u5c0f\u6570\u636e\u91cf\u65f6\u5ef6\u8fdf\u8f83\u9ad8\uff1b\u901a\u8fc7\u5229\u7528\u73b0\u6709DMA\u67b6\u6784\u521b\u65b0\u4f18\u5316\u540e\uff0c\u663e\u8457\u7f29\u5c0f\u4e86\u5c0f\u6570\u636e\u91cf\u4e0b\u7684\u6027\u80fd\u5dee\u8ddd\uff0c\u5e76\u8fdb\u4e00\u6b65\u63d0\u5347\u4e86\u5927\u5c3a\u5bf8\u4e0b\u7684\u6027\u80fd\u4e0e\u80fd\u6548\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u4ec5\u5728\u6709\u9650\u573a\u666f\u4e0b\uff08\u5982\u4ec5\u5173\u6ce8\u5e26\u5bbd\u53d7\u9650\u7684\u5927\u6570\u636e\u4f20\u8f93\u548c\u6027\u80fd\uff09\u8bc4\u4f30DMA\u901a\u4fe1\u96c6\u5408\u64cd\u4f5c\uff0c\u7f3a\u4e4f\u5bf9\u5176\u5728\u6027\u80fd\u3001\u529f\u8017\u548c\u540c\u6b65\u5f00\u9500\u7b49\u65b9\u9762\u7684\u5168\u9762\u5206\u6790\uff0c\u963b\u788d\u4e86\u5176\u5728\u4e3b\u6d41\u96c6\u5408\u901a\u4fe1\u5e93\u4e2d\u7684\u5e94\u7528\u3002", "method": "\u5728AMD Instinct MI300X GPU\u4e0a\u5bf9DMA\u5f15\u64ce\u5378\u8f7dML\u901a\u4fe1\u96c6\u5408\u64cd\u4f5c\u8fdb\u884c\u5168\u9762\u8bc4\u4f30\uff0c\u5305\u62ec\u6027\u80fd\u3001\u529f\u8017/\u80fd\u8017\u548c\u540c\u6b65\u6210\u672c\u5206\u6790\uff1b\u968f\u540e\u5229\u7528\u672a\u88ab\u5145\u5206\u5229\u7528\u7684DMA\u67b6\u6784\u7279\u6027\u8bbe\u8ba1\u4f18\u5316\u65b9\u6848\uff0c\u5e76\u5728\u771f\u5b9e\u786c\u4ef6\u4e0a\u9a8c\u8bc1\u5176\u6548\u679c\u3002", "result": "\u76f8\u6bd4RCCL\u5e93\uff0c\u539f\u59cbDMA\u5b9e\u73b0\u5728\u5927\u5c3a\u5bf8\u6570\u636e\uff08\u51e0\u5341MB\u81f3GB\u7ea7\uff09\u4e0a\u6027\u80fd\u63d0\u534716%\u3001\u529f\u8017\u964d\u4f4e32%\uff0c\u4f46\u5728\u5c0f\u5c3a\u5bf8\u6570\u636e\u4e0a\u5ef6\u8fdf\u663e\u8457\u66f4\u9ad8\uff08all-gather\u61624.5\u500d\uff0call-to-all\u61622.5\u500d\uff09\uff1b\u4f18\u5316\u540e\u7684DMA\u5b9e\u73b0\u5728\u5c0f\u5c3a\u5bf8\u4e0a\u5927\u5e45\u7f29\u5c0f\u5dee\u8ddd\uff08all-gather\u616230%\uff0call-to-all\u5feb20%\uff09\uff0c\u5e76\u5728\u5927\u5c3a\u5bf8\u4e0a\u8fdb\u4e00\u6b65\u63d0\u5347\u6027\u80fd7%\u3001\u8282\u80fd3\u201310%\u3002", "conclusion": "\u901a\u8fc7\u7cfb\u7edf\u6027\u5206\u6790\u4e0e\u9488\u5bf9\u6027\u4f18\u5316\uff0cDMA\u901a\u4fe1\u96c6\u5408\u64cd\u4f5c\u5728\u5404\u7c7b\u6570\u636e\u89c4\u6a21\u4e0b\u5747\u5c55\u73b0\u51fa\u5b9e\u7528\u6f5c\u529b\uff0c\u4e3a\u5c06\u5176\u96c6\u6210\u5230\u4e3b\u6d41\u96c6\u5408\u901a\u4fe1\u5e93\u8fc8\u51fa\u4e86\u5173\u952e\u4e00\u6b65\u3002"}}
{"id": "2511.06838", "categories": ["cs.AR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.06838", "abs": "https://arxiv.org/abs/2511.06838", "authors": ["Yuzong Chen", "Chao Fang", "Xilai Dai", "Yuheng Wu", "Thierry Tambe", "Marian Verhelst", "Mohamed S. Abdelfattah"], "title": "P3-LLM: An Integrated NPU-PIM Accelerator for LLM Inference Using Hybrid Numerical Formats", "comment": "Preprint. Under review", "summary": "The substantial memory bandwidth and computational demand of large language models (LLMs) present critical challenges for efficient inference. To tackle this, the literature has explored heterogeneous systems that combine neural processing units (NPUs) with DRAM-based processing-in-memory (PIM) for LLM acceleration. However, existing high-precision (e.g., FP16) PIM compute units incur significant area and power overhead in DRAM technology, limiting the effective computation throughput. In this paper, we introduce P3-LLM, a novel NPU-PIM integrated accelerator for LLM inference using hybrid numerical formats. Our approach is threefold: First, we propose a flexible mixed-precision quantization scheme, which leverages hybrid numerical formats to quantize different LLM operands with high compression efficiency and minimal accuracy loss. Second, we architect an efficient PIM accelerator co-design for P3-LLM, featuring lightweight compute units to support our hybrid numerical formats. The enhanced PIM compute units significantly boost the computation throughput under iso-area constraints. Third, we optimize the low-precision dataflow of different LLM modules by applying operator fusion to minimize the overhead of runtime dequantization. Our evaluation on a diverse set of representative LLMs and tasks demonstrates that P3-LLM achieves state-of-the-art quantization accuracy in terms of both KV-cache-only quantization and weight-activation quantization. Combining the proposed quantization scheme with PIM architecture co-design, P3-LLM yields an average of $4.9\\times$, $2.0\\times$, and $3.4\\times$ speedups over the state-of-the-art LLM accelerators HBM-PIM, Ecco, and Pimba, respectively. Our quantization code is available at https://github.com/yc2367/P3-LLM.git", "AI": {"tldr": "\u672c\u6587\u63d0\u51faP3-LLM\uff0c\u4e00\u79cd\u7ed3\u5408NPU\u4e0eDRAM\u578b\u5b58\u5185\u8ba1\u7b97\uff08PIM\uff09\u7684\u6df7\u5408\u7cbe\u5ea6\u5927\u8bed\u8a00\u6a21\u578b\u63a8\u7406\u52a0\u901f\u5668\uff0c\u901a\u8fc7\u7075\u6d3b\u7684\u6df7\u5408\u6570\u503c\u683c\u5f0f\u91cf\u5316\u3001\u8f7b\u91cf\u7ea7PIM\u8ba1\u7b97\u5355\u5143\u8bbe\u8ba1\u548c\u4f4e\u7cbe\u5ea6\u6570\u636e\u6d41\u4f18\u5316\uff0c\u5728\u4fdd\u6301\u9ad8\u7cbe\u5ea6\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u541e\u5410\u91cf\uff0c\u76f8\u6bd4\u73b0\u6709\u65b9\u6848\u5e73\u5747\u63d0\u901f2.0\u20134.9\u500d\u3002", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b\u63a8\u7406\u9762\u4e34\u9ad8\u5185\u5b58\u5e26\u5bbd\u548c\u8ba1\u7b97\u9700\u6c42\u7684\u6311\u6218\uff1b\u73b0\u6709\u57fa\u4e8e\u9ad8\u7cbe\u5ea6\uff08\u5982FP16\uff09\u7684PIM\u8ba1\u7b97\u5355\u5143\u5728DRAM\u5de5\u827a\u4e0b\u9762\u79ef\u548c\u529f\u8017\u5f00\u9500\u5927\uff0c\u9650\u5236\u4e86\u6709\u6548\u8ba1\u7b97\u541e\u5410\u91cf\u3002", "method": "\u63d0\u51fa\u4e09\u65b9\u9762\u65b9\u6cd5\uff1a1\uff09\u7075\u6d3b\u7684\u6df7\u5408\u7cbe\u5ea6\u91cf\u5316\u65b9\u6848\uff0c\u4f7f\u7528\u6df7\u5408\u6570\u503c\u683c\u5f0f\u5bf9\u4e0d\u540c\u64cd\u4f5c\u6570\u8fdb\u884c\u9ad8\u6548\u538b\u7f29\u5e76\u4fdd\u6301\u7cbe\u5ea6\uff1b2\uff09\u4e3a\u652f\u6301\u8be5\u683c\u5f0f\u8bbe\u8ba1\u8f7b\u91cf\u7ea7PIM\u52a0\u901f\u5668\u67b6\u6784\uff1b3\uff09\u901a\u8fc7\u7b97\u5b50\u878d\u5408\u4f18\u5316\u4f4e\u7cbe\u5ea6\u6570\u636e\u6d41\uff0c\u51cf\u5c11\u8fd0\u884c\u65f6\u53cd\u91cf\u5316\u5f00\u9500\u3002", "result": "\u5728\u591a\u79cd\u4ee3\u8868\u6027\u5927\u8bed\u8a00\u6a21\u578b\u548c\u4efb\u52a1\u4e0a\u8bc4\u4f30\u8868\u660e\uff0cP3-LLM\u5728KV\u7f13\u5b58\u91cf\u5316\u548c\u6743\u91cd-\u6fc0\u6d3b\u91cf\u5316\u65b9\u9762\u5747\u8fbe\u5230\u5f53\u524d\u6700\u4f18\u7cbe\u5ea6\uff0c\u5e76\u76f8\u8f83HBM-PIM\u3001Ecco\u548cPimba\u7b49\u5148\u8fdb\u52a0\u901f\u5668\u5206\u522b\u5b9e\u73b0\u5e73\u57474.9\u00d7\u30012.0\u00d7\u548c3.4\u00d7\u7684\u52a0\u901f\u6bd4\u3002", "conclusion": "P3-LLM\u901a\u8fc7\u6df7\u5408\u6570\u503c\u683c\u5f0f\u91cf\u5316\u4e0ePIM\u67b6\u6784\u534f\u540c\u8bbe\u8ba1\uff0c\u5728\u4fdd\u8bc1\u7cbe\u5ea6\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u4e86\u5927\u8bed\u8a00\u6a21\u578b\u63a8\u7406\u6548\u7387\uff0c\u4e3a\u9ad8\u6548\u80fdLLM\u52a0\u901f\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2511.06103", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2511.06103", "abs": "https://arxiv.org/abs/2511.06103", "authors": ["Miguel Goul\u00e3o", "Vasco Amaral", "Marjan Mernik"], "title": "Quality in model-driven engineering: a tertiary study", "comment": null, "summary": "Model-driven engineering (MDE) is believed to have a significant impact in software quality. However, researchers and practitioners may have a hard time locating consolidated evidence on this impact, as the available information is scattered in several different publications. Our goal is to aggregate consolidated findings on quality in MDE, facilitating the work of researchers and practitioners in learning about the coverage and main findings of existing work as well as identifying relatively unexplored niches of research that need further attention. We performed a tertiary study on quality in MDE, in order to gain a better understanding of its most prominent findings and existing challenges, as reported in the literature. We identified 22 systematic literature reviews and mapping studies and the most relevant quality attributes addressed by each of those studies, in the context of MDE. Maintainability is clearly the most often studied and reported quality attribute impacted by MDE. Eighty out of 83 research questions in the selected secondary studies have a structure that is more often associated with mapping existing research than with answering more concrete research questions (e.g., comparing two alternative MDE approaches with respect to their impact on a specific quality attribute). We briefly outline the main contributions of each of the selected literature reviews. In the collected studies, we observed a broad coverage of software product quality, although frequently accompanied by notes on how much more empirical research is needed to further validate existing claims. Relatively, little attention seems to be devoted to the impact of MDE on the quality in use of products developed using MDE.", "AI": {"tldr": "\u672c\u6587\u901a\u8fc7\u4e09\u7ea7\u7814\u7a76\u7efc\u8ff0\u4e86\u6a21\u578b\u9a71\u52a8\u5de5\u7a0b\uff08MDE\uff09\u5bf9\u8f6f\u4ef6\u8d28\u91cf\u5f71\u54cd\u7684\u73b0\u6709\u7cfb\u7edf\u6027\u6587\u732e\uff0c\u53d1\u73b0\u53ef\u7ef4\u62a4\u6027\u662f\u6700\u5e38\u88ab\u7814\u7a76\u7684\u8d28\u91cf\u5c5e\u6027\uff0c\u4f46\u591a\u6570\u7814\u7a76\u4fa7\u91cd\u4e8e\u6620\u5c04\u73b0\u6709\u6210\u679c\u800c\u975e\u56de\u7b54\u5177\u4f53\u95ee\u9898\uff0c\u4e14\u7f3a\u4e4f\u5bf9\u201c\u4f7f\u7528\u8d28\u91cf\u201d\u7684\u5173\u6ce8\u548c\u66f4\u591a\u5b9e\u8bc1\u652f\u6301\u3002", "motivation": "\u7814\u7a76\u4eba\u5458\u548c\u4ece\u4e1a\u8005\u96be\u4ee5\u83b7\u53d6\u5173\u4e8eMDE\u5bf9\u8f6f\u4ef6\u8d28\u91cf\u5f71\u54cd\u7684\u96c6\u4e2d\u8bc1\u636e\uff0c\u56e0\u4e3a\u76f8\u5173\u7814\u7a76\u5206\u6563\u5728\u5927\u91cf\u6587\u732e\u4e2d\u3002\u4f5c\u8005\u65e8\u5728\u6574\u5408\u5df2\u6709\u7814\u7a76\u6210\u679c\uff0c\u5e2e\u52a9\u8bc6\u522b\u7814\u7a76\u8986\u76d6\u8303\u56f4\u3001\u4e3b\u8981\u53d1\u73b0\u53ca\u5c1a\u5f85\u63a2\u7d22\u7684\u7814\u7a76\u7a7a\u767d\u3002", "method": "\u5f00\u5c55\u4e86\u4e00\u9879\u5173\u4e8eMDE\u4e2d\u8d28\u91cf\u5f71\u54cd\u7684\u4e09\u7ea7\u7814\u7a76\uff0c\u7b5b\u9009\u5e76\u5206\u6790\u4e8622\u7bc7\u7cfb\u7edf\u6027\u6587\u732e\u7efc\u8ff0\u548c\u6620\u5c04\u7814\u7a76\uff0c\u63d0\u53d6\u5176\u4e2d\u6d89\u53ca\u7684\u4e3b\u8981\u8d28\u91cf\u5c5e\u6027\u53ca\u5176\u7814\u7a76\u95ee\u9898\u7ed3\u6784\uff0c\u5e76\u603b\u7ed3\u5404\u7efc\u8ff0\u7684\u6838\u5fc3\u8d21\u732e\u3002", "result": "\u53ef\u7ef4\u62a4\u6027\u662fMDE\u4e2d\u6700\u5e38\u88ab\u62a5\u544a\u7684\u8d28\u91cf\u5c5e\u6027\uff1b83\u4e2a\u7814\u7a76\u95ee\u9898\u4e2d\u670980\u4e2a\u504f\u5411\u4e8e\u7814\u7a76\u6620\u5c04\u800c\u975e\u5177\u4f53\u6bd4\u8f83\uff1b\u73b0\u6709\u7814\u7a76\u867d\u5e7f\u6cdb\u8986\u76d6\u4ea7\u54c1\u8d28\u91cf\uff0c\u4f46\u666e\u904d\u547c\u5401\u66f4\u591a\u5b9e\u8bc1\u9a8c\u8bc1\uff1b\u5bf9\u201c\u4f7f\u7528\u8d28\u91cf\u201d\u7684\u5173\u6ce8\u8f83\u5c11\u3002", "conclusion": "\u5c3d\u7ba1MDE\u5728\u8f6f\u4ef6\u4ea7\u54c1\u8d28\u91cf\u65b9\u9762\u5df2\u6709\u8f83\u5e7f\u7814\u7a76\u8986\u76d6\uff0c\u5c24\u5176\u662f\u53ef\u7ef4\u62a4\u6027\uff0c\u4f46\u4ecd\u9700\u66f4\u591a\u5b9e\u8bc1\u5de5\u4f5c\u6765\u9a8c\u8bc1\u73b0\u6709\u4e3b\u5f20\uff0c\u5e76\u52a0\u5f3a\u5bf9\u4f7f\u7528\u8d28\u91cf\u7b49\u8f83\u5c11\u5173\u6ce8\u9886\u57df\u7684\u7814\u7a76\u3002"}}
{"id": "2511.06907", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2511.06907", "abs": "https://arxiv.org/abs/2511.06907", "authors": ["Ilias Papalamprou", "Dimosthenis Masouros", "Ioannis Loudaros", "Francky Catthoor", "Dimitrios Soudris"], "title": "Optimizing GEMM for Energy and Performance on Versal ACAP Architectures", "comment": null, "summary": "General Matrix Multiplication (GEMM) is a fundamental operation in many scientific workloads, signal processing, and particularly deep learning. It is often a bottleneck for performance and energy efficiency, especially in edge environments with tight resource and power constraints. AMD's Versal ACAP offers heterogeneous components (AIEs, PL, PS) that can address these challenges, but mapping GEMM across them is complex, with prior works largely overlooking energy-performance trade-offs. In this paper, we propose an automated framework for Versal ACAP that generates GEMM mappings optimized for either performance or energy efficiency. Unlike prior analytical approaches, our method leverages a Machine Learning (ML) model, trained on approximately 6000 on-board experiments of different GEMM mappings, to guide Design Space Exploration, yielding more efficient designs. Evaluation on the Versal VCK190 shows geomean improvements of 1.23x (up to 2.5x) in throughput and 1.25x (up to 2.7x) in energy efficiency over state-of-the-art frameworks.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u673a\u5668\u5b66\u4e60\u7684\u81ea\u52a8\u5316\u6846\u67b6\uff0c\u7528\u4e8e\u5728AMD Versal ACAP\u4e0a\u4f18\u5316\u901a\u7528\u77e9\u9635\u4e58\u6cd5\uff08GEMM\uff09\u7684\u6027\u80fd\u6216\u80fd\u6548\u6620\u5c04\uff0c\u76f8\u6bd4\u73b0\u6709\u65b9\u6cd5\u5728\u541e\u5410\u91cf\u548c\u80fd\u6548\u65b9\u9762\u5206\u522b\u5e73\u5747\u63d0\u53471.23\u500d\u548c1.25\u500d\u3002", "motivation": "GEMM\u662f\u4f17\u591a\u79d1\u5b66\u8ba1\u7b97\u548c\u6df1\u5ea6\u5b66\u4e60\u4efb\u52a1\u4e2d\u7684\u6838\u5fc3\u64cd\u4f5c\uff0c\u5e38\u6210\u4e3a\u6027\u80fd\u4e0e\u80fd\u6548\u74f6\u9888\uff0c\u5c24\u5176\u5728\u8d44\u6e90\u53d7\u9650\u7684\u8fb9\u7f18\u8bbe\u5907\u4e0a\u3002\u5c3d\u7ba1AMD Versal ACAP\u5177\u5907\u5f02\u6784\u8ba1\u7b97\u5355\u5143\u53ef\u5e94\u5bf9\u8be5\u6311\u6218\uff0c\u4f46\u5982\u4f55\u5728\u5176\u4e0a\u9ad8\u6548\u6620\u5c04GEMM\u5e76\u6743\u8861\u6027\u80fd\u4e0e\u80fd\u8017\u4ecd\u7f3a\u4e4f\u6709\u6548\u65b9\u6cd5\u3002", "method": "\u4f5c\u8005\u6784\u5efa\u4e86\u4e00\u4e2a\u57fa\u4e8e\u673a\u5668\u5b66\u4e60\u7684\u81ea\u52a8\u5316\u8bbe\u8ba1\u7a7a\u95f4\u63a2\u7d22\u6846\u67b6\uff0c\u5229\u7528\u7ea66000\u6b21\u677f\u7ea7\u5b9e\u9a8c\u6570\u636e\u8bad\u7ec3\u6a21\u578b\uff0c\u4ee5\u6307\u5bfcGEMM\u5728Versal ACAP\u5f02\u6784\u67b6\u6784\uff08AIE\u3001PL\u3001PS\uff09\u4e0a\u7684\u6620\u5c04\u4f18\u5316\u3002", "result": "\u5728Versal VCK190\u5e73\u53f0\u4e0a\u7684\u8bc4\u4f30\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u76f8\u6bd4\u5f53\u524d\u6700\u5148\u8fdb\u7684\u6846\u67b6\uff0c\u5728\u541e\u5410\u91cf\u4e0a\u5e73\u5747\u63d0\u53471.23\u500d\uff08\u6700\u9ad82.5\u500d\uff09\uff0c\u5728\u80fd\u6548\u4e0a\u5e73\u5747\u63d0\u53471.25\u500d\uff08\u6700\u9ad82.7\u500d\u3002", "conclusion": "\u7ed3\u5408\u5b9e\u6d4b\u6570\u636e\u9a71\u52a8\u7684\u673a\u5668\u5b66\u4e60\u6a21\u578b\u80fd\u66f4\u6709\u6548\u5730\u6307\u5bfcGEMM\u5728\u5f02\u6784\u786c\u4ef6\u4e0a\u7684\u6620\u5c04\u4f18\u5316\uff0c\u5728\u6027\u80fd\u4e0e\u80fd\u6548\u65b9\u9762\u5747\u663e\u8457\u4f18\u4e8e\u4f20\u7edf\u5206\u6790\u65b9\u6cd5\u3002"}}
{"id": "2511.06110", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2511.06110", "abs": "https://arxiv.org/abs/2511.06110", "authors": ["Mafalda Santos", "Catarina Gralha", "Miguel Goul\u00e3o", "Jo\u00e3o Ara\u00fajo", "Ana Moreira"], "title": "On the impact of semantic transparency on understanding and reviewing social goal models", "comment": "preprint", "summary": "Context: i* is one of the most influential languages in the Requirements Engineering research community. Perhaps due to its complexity and low adoption in industry, it became a natural candidate for studies aiming at improving its concrete syntax and the stakeholders' ability to correctly interpret i* models.\n  Objectives: We evaluate the impact of semantic transparency on understanding and reviewing i* models, in the presence of a language key. Methods: We performed a quasi-experiment comparing the standard i* concrete syntax with an alternative that has an increased semantic transparency. We asked 57 novice participants to perform understanding and reviewing tasks on i* models, and measured their accuracy, speed and ease, using metrics of task success, time and effort, collected with eye-tracking and participants' feedback.\n  Results: We found no evidence of improved accuracy or speed attributable to the alternative concrete syntax. Although participants' perceived ease was similar, they devoted significantly less visual effort to the model and the provided language key, when using the alternative concrete syntax.\n  Conclusions: The context provided by the model and language key may mitigate the i* symbol recognition deficit reported in previous works. However, the alternative concrete syntax required a significantly lower visual effort.", "AI": {"tldr": "\u672c\u7814\u7a76\u8bc4\u4f30\u4e86\u8bed\u4e49\u900f\u660e\u5ea6\u5bf9\u7406\u89e3\u4e0e\u8bc4\u5ba1i*\u6a21\u578b\u7684\u5f71\u54cd\uff0c\u53d1\u73b0\u5c3d\u7ba1\u66ff\u4ee3\u6027\u5177\u4f53\u8bed\u6cd5\u672a\u663e\u8457\u63d0\u5347\u51c6\u786e\u7387\u6216\u901f\u5ea6\uff0c\u4f46\u663e\u8457\u964d\u4f4e\u4e86\u89c6\u89c9\u8d1f\u62c5\u3002", "motivation": "i*\u8bed\u8a00\u56e0\u5176\u590d\u6742\u6027\u548c\u5de5\u4e1a\u754c\u4f4e\u91c7\u7528\u7387\uff0c\u6210\u4e3a\u6539\u8fdb\u5176\u5177\u4f53\u8bed\u6cd5\u548c\u63d0\u5347\u5229\u76ca\u76f8\u5173\u8005\u6b63\u786e\u89e3\u8bfb\u6a21\u578b\u80fd\u529b\u7684\u7814\u7a76\u5bf9\u8c61\u3002", "method": "\u901a\u8fc7\u51c6\u5b9e\u9a8c\u6bd4\u8f83\u6807\u51c6i*\u8bed\u6cd5\u4e0e\u8bed\u4e49\u900f\u660e\u5ea6\u66f4\u9ad8\u7684\u66ff\u4ee3\u8bed\u6cd5\uff0c57\u540d\u65b0\u624b\u53c2\u4e0e\u8005\u5b8c\u6210\u7406\u89e3\u4e0e\u8bc4\u5ba1\u4efb\u52a1\uff0c\u4f7f\u7528\u773c\u52a8\u8ffd\u8e2a\u548c\u53cd\u9988\u6536\u96c6\u4efb\u52a1\u6210\u529f\u7387\u3001\u65f6\u95f4\u548c\u52aa\u529b\u7a0b\u5ea6\u7b49\u6307\u6807\u3002", "result": "\u66ff\u4ee3\u8bed\u6cd5\u672a\u663e\u8457\u63d0\u9ad8\u51c6\u786e\u7387\u6216\u901f\u5ea6\uff0c\u611f\u77e5\u6613\u7528\u6027\u76f8\u4f3c\uff0c\u4f46\u4f7f\u7528\u8be5\u8bed\u6cd5\u65f6\u53c2\u4e0e\u8005\u5728\u6a21\u578b\u548c\u8bed\u8a00\u952e\u4e0a\u7684\u89c6\u89c9\u52aa\u529b\u663e\u8457\u51cf\u5c11\u3002", "conclusion": "\u6a21\u578b\u548c\u8bed\u8a00\u952e\u63d0\u4f9b\u7684\u4e0a\u4e0b\u6587\u53ef\u80fd\u7f13\u89e3\u5148\u524d\u7814\u7a76\u4e2d\u62a5\u544a\u7684i*\u7b26\u53f7\u8bc6\u522b\u7f3a\u9677\uff0c\u4e14\u66ff\u4ee3\u8bed\u6cd5\u663e\u8457\u964d\u4f4e\u4e86\u89c6\u89c9\u52aa\u529b\u3002"}}
{"id": "2511.06824", "categories": ["cs.DC", "cs.CE"], "pdf": "https://arxiv.org/pdf/2511.06824", "abs": "https://arxiv.org/abs/2511.06824", "authors": ["Xin Yao", "Yang Liu", "Jin Jiang", "Yesen Chen", "Zhilong Chen", "Hongkang Dong", "Xiaofeng Wei", "Teng Zhang", "Dongyun Wang"], "title": "A GPU-boosted high-performance multi-working condition joint analysis framework for predicting dynamics of textured axial piston pump", "comment": null, "summary": "Accurate simulation to dynamics of axial piston pump (APP) is essential for its design, manufacture and maintenance. However, limited by computation capacity of CPU device and traditional solvers, conventional iteration methods are inefficient in complicated case with textured surface requiring refined mesh, and could not handle simulation during multiple periods. To accelerate Picard iteration for predicting dynamics of APP, a GPU-boosted high-performance Multi-working condition joint Analysis Framework (GMAF) is designed, which adopts Preconditioned Conjugate Gradient method (PCG) using Approximate Symmetric Successive Over-Relaxation preconditioner (ASSOR). GMAF abundantly utilizes GPU device via elevating computational intensity and expanding scale of massive parallel computation. Therefore, it possesses novel performance in analyzing dynamics of both smooth and textured APPs during multiple periods, as the establishment and solution to joint algebraic system for pressure field are accelerated magnificently, as well as numerical integral for force and moment due to oil flow. Compared with asynchronized convergence strategy pursuing local convergence, synchronized convergence strategy targeting global convergence is adopted in PCG solver for the joint system. Revealed by corresponding results, oil force in axial direction and moment in circumferential directly respond to input pressure, while other components evolve in sinusoidal patterns. Specifically, force and moment due to normal pressure instantly reach their steady state initially, while ones due to viscous shear stress evolve during periods. After simulating dynamics of APP and pressure distribution via GMAF, the promotion of pressure capacity and torsion resistance due to textured surface is revealed numerically, as several 'steps' exist in the pressure field corresponding to textures.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eGPU\u52a0\u901f\u7684\u591a\u5de5\u51b5\u8054\u5408\u5206\u6790\u6846\u67b6\uff08GMAF\uff09\uff0c\u7528\u4e8e\u9ad8\u6548\u6a21\u62df\u8f74\u5411\u67f1\u585e\u6cf5\uff08APP\uff09\u5728\u5149\u6ed1\u548c\u7ec7\u6784\u8868\u9762\u4e0b\u7684\u591a\u5468\u671f\u52a8\u529b\u5b66\u884c\u4e3a\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8ba1\u7b97\u6548\u7387\u4e0e\u7cbe\u5ea6\u3002", "motivation": "\u4f20\u7edfCPU\u8ba1\u7b97\u80fd\u529b\u548c\u8fed\u4ee3\u65b9\u6cd5\u5728\u5904\u7406\u5177\u6709\u7ec7\u6784\u8868\u9762\u3001\u9700\u8981\u7cbe\u7ec6\u7f51\u683c\u7684\u590d\u6742\u8f74\u5411\u67f1\u585e\u6cf5\u52a8\u529b\u5b66\u4eff\u771f\u65f6\u6548\u7387\u4f4e\u4e0b\uff0c\u96be\u4ee5\u652f\u6301\u591a\u5468\u671f\u6a21\u62df\u3002", "method": "\u8bbe\u8ba1\u4e86GPU\u52a0\u901f\u7684\u9ad8\u6027\u80fd\u591a\u5de5\u51b5\u8054\u5408\u5206\u6790\u6846\u67b6\uff08GMAF\uff09\uff0c\u91c7\u7528\u5e26\u8fd1\u4f3c\u5bf9\u79f0\u9010\u6b21\u8d85\u677e\u5f1b\uff08ASSOR\uff09\u9884\u6761\u4ef6\u5b50\u7684\u9884\u6761\u4ef6\u5171\u8f6d\u68af\u5ea6\u6cd5\uff08PCG\uff09\uff0c\u5e76\u91c7\u7528\u9762\u5411\u5168\u5c40\u6536\u655b\u7684\u540c\u6b65\u6536\u655b\u7b56\u7565\uff0c\u5145\u5206\u5229\u7528GPU\u8fdb\u884c\u5927\u89c4\u6a21\u5e76\u884c\u8ba1\u7b97\u3002", "result": "GMAF\u663e\u8457\u52a0\u901f\u4e86\u538b\u529b\u573a\u4ee3\u6570\u7cfb\u7edf\u7684\u6784\u5efa\u4e0e\u6c42\u89e3\u4ee5\u53ca\u6cb9\u6d41\u5f15\u8d77\u7684\u529b\u548c\u529b\u77e9\u7684\u6570\u503c\u79ef\u5206\uff1b\u4eff\u771f\u7ed3\u679c\u663e\u793a\u8f74\u5411\u6cb9\u529b\u548c\u5468\u5411\u529b\u77e9\u76f4\u63a5\u54cd\u5e94\u8f93\u5165\u538b\u529b\uff0c\u5176\u4ed6\u5206\u91cf\u5448\u6b63\u5f26\u53d8\u5316\uff1b\u7ec7\u6784\u8868\u9762\u53ef\u63d0\u5347\u538b\u529b\u627f\u8f7d\u80fd\u529b\u548c\u6297\u626d\u6027\u80fd\uff0c\u5e76\u5728\u538b\u529b\u573a\u4e2d\u5f62\u6210\u5bf9\u5e94\u201c\u53f0\u9636\u201d\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684GMAF\u6846\u67b6\u80fd\u9ad8\u6548\u3001\u51c6\u786e\u5730\u6a21\u62df\u8f74\u5411\u67f1\u585e\u6cf5\u5728\u591a\u5468\u671f\u4e0b\u7684\u52a8\u529b\u5b66\u7279\u6027\uff0c\u5c24\u5176\u9002\u7528\u4e8e\u5e26\u7ec7\u6784\u8868\u9762\u7684\u590d\u6742\u5de5\u51b5\uff0c\u4e3a\u6cf5\u7684\u8bbe\u8ba1\u4e0e\u4f18\u5316\u63d0\u4f9b\u4e86\u6709\u529b\u5de5\u5177\u3002"}}
{"id": "2511.06186", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2511.06186", "abs": "https://arxiv.org/abs/2511.06186", "authors": ["Lakshmi Priya Bodepudi", "Yutong Zhao", "Ming Quan Fu", "Yuanyuan Wu", "Sen He", "Yu Zhao"], "title": "Diagnosing and Resolving Android Applications Building Issues: An Empirical Study", "comment": "11 pages", "summary": "Building Android applications reliably remains a persistent challenge due to complex dependencies, diverse configurations, and the rapid evolution of the Android ecosystem. This study conducts an empirical analysis of 200 open-source Android projects written in Java and Kotlin to diagnose and resolve build failures. Through a five-phase process encompassing data collection, build execution, failure classification, repair strategy design, and LLM-assisted evaluation, we identified four primary types of build errors: environment issues, dependency and Gradle task errors, configuration problems, and syntax/API incompatibilities. Among the 135 projects that initially failed to build, our diagnostic and repair strategy enabled developers to resolve 102 cases (75.56%), significantly reducing troubleshooting effort. We further examined the potential of Large Language Models, such as GPT-5, to assist in error diagnosis, achieving a 53.3% success rate in suggesting viable fixes. An analysis of project attributes revealed that build success is influenced by programming language, project age, and app size. These findings provide practical insights into improving Android build reliability and advancing AI-assisted software maintenance.", "AI": {"tldr": "\u8be5\u7814\u7a76\u5bf9200\u4e2a\u5f00\u6e90Android\u9879\u76ee\u8fdb\u884c\u5b9e\u8bc1\u5206\u6790\uff0c\u8bc6\u522b\u51fa\u56db\u7c7b\u4e3b\u8981\u6784\u5efa\u9519\u8bef\uff0c\u5e76\u901a\u8fc7\u8bca\u65ad\u4e0e\u4fee\u590d\u7b56\u7565\u6210\u529f\u89e3\u51b375.56%\u7684\u5931\u8d25\u6848\u4f8b\uff1b\u540c\u65f6\u8bc4\u4f30\u4e86\u5927\u8bed\u8a00\u6a21\u578b\uff08\u5982GPT-5\uff09\u5728\u9519\u8bef\u8bca\u65ad\u4e2d\u7684\u6f5c\u529b\uff0c\u6210\u529f\u7387\u8fbe53.3%\u3002", "motivation": "\u7531\u4e8e\u590d\u6742\u7684\u4f9d\u8d56\u5173\u7cfb\u3001\u591a\u6837\u7684\u914d\u7f6e\u4ee5\u53caAndroid\u751f\u6001\u7cfb\u7edf\u7684\u5feb\u901f\u6f14\u8fdb\uff0c\u53ef\u9760\u5730\u6784\u5efaAndroid\u5e94\u7528\u4ecd\u662f\u4e00\u9879\u6301\u7eed\u6311\u6218\uff0c\u4e9f\u9700\u7cfb\u7edf\u6027\u65b9\u6cd5\u8bca\u65ad\u548c\u4fee\u590d\u6784\u5efa\u5931\u8d25\u95ee\u9898\u3002", "method": "\u91c7\u7528\u4e94\u9636\u6bb5\u6d41\u7a0b\uff1a\u6570\u636e\u6536\u96c6\u3001\u6784\u5efa\u6267\u884c\u3001\u5931\u8d25\u5206\u7c7b\u3001\u4fee\u590d\u7b56\u7565\u8bbe\u8ba1\u548c\u5927\u8bed\u8a00\u6a21\u578b\u8f85\u52a9\u8bc4\u4f30\uff0c\u5bf9200\u4e2aJava/Kotlin\u7f16\u5199\u7684\u5f00\u6e90Android\u9879\u76ee\u8fdb\u884c\u5b9e\u8bc1\u5206\u6790\u3002", "result": "\u5728135\u4e2a\u521d\u59cb\u6784\u5efa\u5931\u8d25\u7684\u9879\u76ee\u4e2d\uff0c\u4fee\u590d\u7b56\u7565\u6210\u529f\u89e3\u51b3\u4e86102\u4e2a\uff0875.56%\uff09\uff1bLLM\u8f85\u52a9\u8bca\u65ad\u7684\u6210\u529f\u7387\u4e3a53.3%\uff1b\u9879\u76ee\u5c5e\u6027\u5206\u6790\u8868\u660e\u6784\u5efa\u6210\u529f\u7387\u53d7\u7f16\u7a0b\u8bed\u8a00\u3001\u9879\u76ee\u5e74\u9f84\u548c\u5e94\u7528\u89c4\u6a21\u5f71\u54cd\u3002", "conclusion": "\u7814\u7a76\u4e3a\u63d0\u5347Android\u6784\u5efa\u53ef\u9760\u6027\u63d0\u4f9b\u4e86\u5b9e\u7528\u89c1\u89e3\uff0c\u5e76\u5c55\u793a\u4e86AI\uff08\u7279\u522b\u662f\u5927\u8bed\u8a00\u6a21\u578b\uff09\u5728\u8f6f\u4ef6\u7ef4\u62a4\u4e2d\u7684\u8f85\u52a9\u6f5c\u529b\u3002"}}
{"id": "2511.07229", "categories": ["cs.DC", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.07229", "abs": "https://arxiv.org/abs/2511.07229", "authors": ["Jaehong Cho", "Hyunmin Choi", "Jongse Park"], "title": "LLMServingSim2.0: A Unified Simulator for Heterogeneous Hardware and Serving Techniques in LLM Infrastructure", "comment": "4 pages, 3 figures", "summary": "This paper introduces LLMServingSim2.0, a system simulator designed for exploring heterogeneous hardware in large-scale LLM serving systems. LLMServingSim2.0 addresses two key limitations of its predecessor: (1) integrating hardware models into system-level simulators is non-trivial due to the lack of a clear abstraction, and (2) existing simulators support only a narrow subset of serving techniques, leaving no infrastructure that captures the breadth of approaches in modern LLM serving. To overcome these issues, LLMServingSim2.0 adopts trace-driven performance modeling, accompanied by an operator-level latency profiler, enabling the integration of new accelerators with a single command. It further embeds up-to-date serving techniques while exposing flexible interfaces for request routing, cache management, and scheduling policies. In a TPU case study, our profiler requires 18.5x fewer LoC and outperforms the predecessor's hardware-simulator integration, demonstrating LLMServingSim2.0's low-effort hardware extensibility. Our experiments further show that LLMServingSim2.0 reproduces GPU-based LLM serving with 1.9% error, while maintaining practical simulation time, making it a comprehensive platform for both hardware developers and LLM service providers.", "AI": {"tldr": "LLMServingSim2.0 \u662f\u4e00\u4e2a\u7528\u4e8e\u5927\u89c4\u6a21\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u670d\u52a1\u7cfb\u7edf\u7684\u5f02\u6784\u786c\u4ef6\u4eff\u771f\u5e73\u53f0\uff0c\u901a\u8fc7\u5f15\u5165\u57fa\u4e8e\u8f68\u8ff9\u7684\u6027\u80fd\u5efa\u6a21\u548c\u7b97\u5b50\u7ea7\u5ef6\u8fdf\u5206\u6790\u5668\uff0c\u663e\u8457\u7b80\u5316\u4e86\u65b0\u786c\u4ef6\u52a0\u901f\u5668\u7684\u96c6\u6210\uff0c\u5e76\u652f\u6301\u591a\u79cd\u73b0\u4ee3 LLM \u670d\u52a1\u6280\u672f\uff0c\u5177\u5907\u9ad8\u7cbe\u5ea6\u4e0e\u4f4e\u5f00\u9500\u3002", "motivation": "\u73b0\u6709 LLM \u670d\u52a1\u7cfb\u7edf\u4eff\u771f\u5668\u5b58\u5728\u4e24\u5927\u5c40\u9650\uff1a\u4e00\u662f\u7f3a\u4e4f\u6e05\u6670\u7684\u62bd\u8c61\u4f7f\u5f97\u5c06\u786c\u4ef6\u6a21\u578b\u96c6\u6210\u5230\u7cfb\u7edf\u7ea7\u4eff\u771f\u5668\u4e2d\u56f0\u96be\uff1b\u4e8c\u662f\u4ec5\u652f\u6301\u6709\u9650\u7684\u670d\u52a1\u6280\u672f\uff0c\u65e0\u6cd5\u6db5\u76d6\u73b0\u4ee3 LLM \u670d\u52a1\u4e2d\u7684\u591a\u6837\u5316\u65b9\u6cd5\u3002", "method": "LLMServingSim2.0 \u91c7\u7528\u57fa\u4e8e\u8f68\u8ff9\u7684\u6027\u80fd\u5efa\u6a21\u65b9\u6cd5\uff0c\u5e76\u7ed3\u5408\u7b97\u5b50\u7ea7\u5ef6\u8fdf\u5206\u6790\u5668\uff0c\u5b9e\u73b0\u4e00\u952e\u5f0f\u65b0\u52a0\u901f\u5668\u96c6\u6210\uff1b\u540c\u65f6\u5d4c\u5165\u6700\u65b0\u7684\u670d\u52a1\u6280\u672f\uff0c\u5e76\u63d0\u4f9b\u7075\u6d3b\u7684\u8bf7\u6c42\u8def\u7531\u3001\u7f13\u5b58\u7ba1\u7406\u548c\u8c03\u5ea6\u7b56\u7565\u63a5\u53e3\u3002", "result": "\u5728 TPU \u6848\u4f8b\u7814\u7a76\u4e2d\uff0c\u5176\u5206\u6790\u5668\u4ee3\u7801\u91cf\u51cf\u5c11 18.5 \u500d\uff0c\u4e14\u4f18\u4e8e\u524d\u4ee3\u786c\u4ef6-\u4eff\u771f\u5668\u96c6\u6210\u65b9\u5f0f\uff1b\u5728 GPU \u573a\u666f\u4e0b\uff0c\u4eff\u771f\u8bef\u5dee\u4ec5\u4e3a 1.9%\uff0c\u540c\u65f6\u4fdd\u6301\u5408\u7406\u7684\u4eff\u771f\u8017\u65f6\u3002", "conclusion": "LLMServingSim2.0 \u63d0\u4f9b\u4e86\u4e00\u4e2a\u9ad8\u6548\u3001\u51c6\u786e\u4e14\u6613\u4e8e\u6269\u5c55\u7684\u4eff\u771f\u5e73\u53f0\uff0c\u9002\u7528\u4e8e\u786c\u4ef6\u5f00\u53d1\u8005\u548c LLM \u670d\u52a1\u63d0\u4f9b\u5546\uff0c\u80fd\u591f\u5168\u9762\u652f\u6301\u5f02\u6784\u786c\u4ef6\u63a2\u7d22\u4e0e\u670d\u52a1\u7b56\u7565\u8bc4\u4f30\u3002"}}
{"id": "2511.06227", "categories": ["cs.SE", "cs.AI", "cs.CE"], "pdf": "https://arxiv.org/pdf/2511.06227", "abs": "https://arxiv.org/abs/2511.06227", "authors": ["Anamul Haque Mollah", "Ahmed Aljohani", "Hyunsook Do"], "title": "Assertion-Aware Test Code Summarization with Large Language Models", "comment": "Accepted for publication at 2nd ACM International Conference on AI-powered Software (AIware 2025)", "summary": "Unit tests often lack concise summaries that convey test intent, especially in auto-generated or poorly documented codebases. Large Language Models (LLMs) offer a promising solution, but their effectiveness depends heavily on how they are prompted. Unlike generic code summarization, test-code summarization poses distinct challenges because test methods validate expected behavior through assertions rather than implementing functionality. This paper presents a new benchmark of 91 real-world Java test cases paired with developer-written summaries and conducts a controlled ablation study to investigate how test code-related components-such as the method under test (MUT), assertion messages, and assertion semantics-affect the performance of LLM-generated test summaries. We evaluate four code LLMs (Codex, Codestral, DeepSeek, and Qwen-Coder) across seven prompt configurations using n-gram metrics (BLEU, ROUGE-L, METEOR), semantic similarity (BERTScore), and LLM-based evaluation. Results show that prompting with assertion semantics improves summary quality by an average of 0.10 points (2.3%) over full MUT context (4.45 vs. 4.35) while requiring fewer input tokens. Codex and Qwen-Coder achieve the highest alignment with human-written summaries, while DeepSeek underperforms despite high lexical overlap. The replication package is publicly available at https://doi.org/10. 5281/zenodo.17067550", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u4e2a\u5305\u542b91\u4e2a\u771f\u5b9eJava\u6d4b\u8bd5\u7528\u4f8b\u53ca\u5176\u5f00\u53d1\u8005\u64b0\u5199\u6458\u8981\u7684\u65b0\u57fa\u51c6\uff0c\u5e76\u901a\u8fc7\u6d88\u878d\u5b9e\u9a8c\u7814\u7a76\u4e86\u4e0d\u540c\u63d0\u793a\u7b56\u7565\uff08\u5982\u88ab\u6d4b\u65b9\u6cd5\u3001\u65ad\u8a00\u8bed\u4e49\u7b49\uff09\u5bf9\u5927\u8bed\u8a00\u6a21\u578b\u751f\u6210\u6d4b\u8bd5\u6458\u8981\u6548\u679c\u7684\u5f71\u54cd\u3002\u7ed3\u679c\u8868\u660e\uff0c\u4ec5\u4f7f\u7528\u65ad\u8a00\u8bed\u4e49\u5373\u53ef\u5728\u51cf\u5c11\u8f93\u5165\u957f\u5ea6\u7684\u540c\u65f6\u63d0\u5347\u6458\u8981\u8d28\u91cf\uff0c\u5176\u4e2dCodex\u548cQwen-Coder\u8868\u73b0\u6700\u4f73\u3002", "motivation": "\u5355\u5143\u6d4b\u8bd5\u901a\u5e38\u7f3a\u4e4f\u7b80\u6d01\u660e\u4e86\u7684\u6458\u8981\u6765\u8868\u8fbe\u6d4b\u8bd5\u610f\u56fe\uff0c\u5c24\u5176\u5728\u81ea\u52a8\u751f\u6210\u6216\u6587\u6863\u4e0d\u8db3\u7684\u4ee3\u7801\u5e93\u4e2d\u66f4\u4e3a\u660e\u663e\u3002\u5c3d\u7ba1\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u4ee3\u7801\u6458\u8981\u65b9\u9762\u5c55\u73b0\u51fa\u6f5c\u529b\uff0c\u4f46\u5176\u5728\u6d4b\u8bd5\u4ee3\u7801\u6458\u8981\u4efb\u52a1\u4e2d\u7684\u6709\u6548\u6027\u9ad8\u5ea6\u4f9d\u8d56\u4e8e\u63d0\u793a\u65b9\u5f0f\uff0c\u800c\u8be5\u4efb\u52a1\u56e0\u6d4b\u8bd5\u65b9\u6cd5\u901a\u8fc7\u65ad\u8a00\u9a8c\u8bc1\u884c\u4e3a\u800c\u975e\u5b9e\u73b0\u529f\u80fd\uff0c\u5177\u6709\u72ec\u7279\u6311\u6218\u3002", "method": "\u6784\u5efa\u5305\u542b91\u4e2a\u771f\u5b9eJava\u6d4b\u8bd5\u7528\u4f8b\u53ca\u5bf9\u5e94\u4eba\u5de5\u6458\u8981\u7684\u57fa\u51c6\u6570\u636e\u96c6\uff1b\u8bbe\u8ba1\u4e03\u79cd\u63d0\u793a\u914d\u7f6e\uff0c\u7cfb\u7edf\u6027\u5730\u5f15\u5165\u88ab\u6d4b\u65b9\u6cd5\uff08MUT\uff09\u3001\u65ad\u8a00\u6d88\u606f\u548c\u65ad\u8a00\u8bed\u4e49\u7b49\u7ec4\u4ef6\uff1b\u5728\u56db\u4e2a\u4ee3\u7801\u5927\u6a21\u578b\uff08Codex\u3001Codestral\u3001DeepSeek\u3001Qwen-Coder\uff09\u4e0a\u8fdb\u884c\u53d7\u63a7\u6d88\u878d\u5b9e\u9a8c\uff1b\u91c7\u7528n-gram\u6307\u6807\uff08BLEU\u3001ROUGE-L\u3001METEOR\uff09\u3001\u8bed\u4e49\u76f8\u4f3c\u5ea6\uff08BERTScore\uff09\u53ca\u57fa\u4e8eLLM\u7684\u8bc4\u4f30\u65b9\u6cd5\u7efc\u5408\u8861\u91cf\u6458\u8981\u8d28\u91cf\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u4ec5\u63d0\u4f9b\u65ad\u8a00\u8bed\u4e49\u7684\u63d0\u793a\u65b9\u5f0f\u6bd4\u5b8c\u6574MUT\u4e0a\u4e0b\u6587\u5e73\u5747\u63d0\u53470.10\u5206\uff082.3%\uff09\uff0c\u4e14\u6240\u9700\u8f93\u5165token\u66f4\u5c11\uff1bCodex\u548cQwen-Coder\u751f\u6210\u7684\u6458\u8981\u4e0e\u4eba\u5de5\u6458\u8981\u4e00\u81f4\u6027\u6700\u9ad8\uff0c\u800cDeepSeek\u5c3d\u7ba1\u8bcd\u6c47\u91cd\u53e0\u5ea6\u9ad8\uff0c\u6574\u4f53\u8868\u73b0\u5374\u8f83\u5dee\u3002", "conclusion": "\u65ad\u8a00\u8bed\u4e49\u662f\u63d0\u5347\u6d4b\u8bd5\u4ee3\u7801\u6458\u8981\u8d28\u91cf\u7684\u5173\u952e\u56e0\u7d20\uff0c\u5408\u7406\u8bbe\u8ba1\u63d0\u793a\u7b56\u7565\u53ef\u5728\u964d\u4f4e\u8ba1\u7b97\u5f00\u9500\u7684\u540c\u65f6\u63d0\u9ad8\u6458\u8981\u51c6\u786e\u6027\uff1b\u4e0d\u540c\u6a21\u578b\u5728\u6d4b\u8bd5\u6458\u8981\u4efb\u52a1\u4e2d\u8868\u73b0\u5dee\u5f02\u663e\u8457\uff0c\u9700\u7ed3\u5408\u591a\u79cd\u8bc4\u4f30\u7ef4\u5ea6\u5168\u9762\u8861\u91cf\u6027\u80fd\u3002"}}
{"id": "2511.06251", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.06251", "abs": "https://arxiv.org/abs/2511.06251", "authors": ["Mingde Xu", "Zhen Yang", "Wenyi Hong", "Lihang Pan", "Xinyue Fan", "Yan Wang", "Xiaotao Gu", "Bin Xu", "Jie Tang"], "title": "WebVIA: A Web-based Vision-Language Agentic Framework for Interactive and Verifiable UI-to-Code Generation", "comment": "36 pages, 30 figures", "summary": "User interface (UI) development requires translating design mockups into functional code, a process that remains repetitive and labor-intensive. While recent Vision-Language Models (VLMs) automate UI-to-Code generation, they generate only static HTML/CSS/JavaScript layouts lacking interactivity. To address this, we propose WebVIA, the first agentic framework for interactive UI-to-Code generation and validation. The framework comprises three components: 1) an exploration agent to capture multi-state UI screenshots; 2) a UI2Code model that generates executable interactive code; 3) a validation module that verifies the interactivity. Experiments demonstrate that WebVIA-Agent achieves more stable and accurate UI exploration than general-purpose agents (e.g., Gemini-2.5-Pro). In addition, our fine-tuned WebVIA-UI2Code models exhibit substantial improvements in generating executable and interactive HTML/CSS/JavaScript code, outperforming their base counterparts across both interactive and static UI2Code benchmarks. Our code and models are available at \\href{https://zheny2751-dotcom.github.io/webvia.github.io/}{\\texttt{https://webvia.github.io}}.", "AI": {"tldr": "WebVIA \u662f\u9996\u4e2a\u9762\u5411\u4ea4\u4e92\u5f0f UI \u751f\u6210\u7684\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u901a\u8fc7\u63a2\u7d22\u3001\u4ee3\u7801\u751f\u6210\u4e0e\u9a8c\u8bc1\u4e09\u9636\u6bb5\u6d41\u7a0b\uff0c\u663e\u8457\u63d0\u5347 UI \u5230\u53ef\u6267\u884c\u4ea4\u4e92\u4ee3\u7801\u7684\u751f\u6210\u8d28\u91cf\u3002", "motivation": "\u73b0\u6709\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLMs\uff09\u5728 UI-to-Code \u4efb\u52a1\u4e2d\u4ec5\u80fd\u751f\u6210\u9759\u6001\u5e03\u5c40\uff0c\u7f3a\u4e4f\u4ea4\u4e92\u6027\uff0c\u800c\u4eba\u5de5\u5b9e\u73b0\u4ea4\u4e92\u529f\u80fd\u8fc7\u7a0b\u91cd\u590d\u4e14\u8017\u65f6\u3002", "method": "\u63d0\u51fa WebVIA \u6846\u67b6\uff0c\u5305\u542b\uff1a1\uff09\u7528\u4e8e\u6355\u83b7\u591a\u72b6\u6001 UI \u622a\u56fe\u7684\u63a2\u7d22\u667a\u80fd\u4f53\uff1b2\uff09\u751f\u6210\u53ef\u6267\u884c\u4ea4\u4e92\u4ee3\u7801\u7684 UI2Code \u6a21\u578b\uff1b3\uff09\u9a8c\u8bc1\u4ea4\u4e92\u6027\u7684\u6a21\u5757\u3002", "result": "WebVIA-Agent \u5728 UI \u63a2\u7d22\u4e0a\u6bd4\u901a\u7528\u667a\u80fd\u4f53\uff08\u5982 Gemini-2.5-Pro\uff09\u66f4\u7a33\u5b9a\u51c6\u786e\uff1b\u5fae\u8c03\u540e\u7684 WebVIA-UI2Code \u6a21\u578b\u5728\u4ea4\u4e92\u6027\u548c\u9759\u6001 UI \u751f\u6210\u4efb\u52a1\u4e0a\u5747\u4f18\u4e8e\u57fa\u7ebf\u6a21\u578b\u3002", "conclusion": "WebVIA \u9996\u6b21\u5b9e\u73b0\u4e86\u7aef\u5230\u7aef\u7684\u4ea4\u4e92\u5f0f UI \u81ea\u52a8\u751f\u6210\u4e0e\u9a8c\u8bc1\uff0c\u663e\u8457\u63a8\u8fdb\u4e86 UI \u5f00\u53d1\u81ea\u52a8\u5316\u6c34\u5e73\u3002"}}
{"id": "2511.06352", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2511.06352", "abs": "https://arxiv.org/abs/2511.06352", "authors": ["Sara Mahdavi Hezavehi", "Danny Weyns", "Paris Avgeriou"], "title": "State of the Art on Self-adaptive Systems: An Essay", "comment": "12", "summary": "In this essay, we introduce the basic concepts necessary to lay out the foundation for our PhD research on uncertainty and risk-aware adaptation, and discuss relevant related research.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u535a\u58eb\u7814\u7a76\u4e2d\u5173\u4e8e\u4e0d\u786e\u5b9a\u6027\u4e0e\u98ce\u9669\u611f\u77e5\u9002\u5e94\u7684\u57fa\u7840\u6982\u5ff5\u53ca\u76f8\u5173\u7814\u7a76\u3002", "motivation": "\u4e3a\u5f00\u5c55\u5173\u4e8e\u4e0d\u786e\u5b9a\u6027\u4e0e\u98ce\u9669\u611f\u77e5\u9002\u5e94\u7684\u535a\u58eb\u7814\u7a76\u5960\u5b9a\u7406\u8bba\u57fa\u7840\u3002", "method": "\u7efc\u8ff0\u5e76\u8ba8\u8bba\u76f8\u5173\u9886\u57df\u7684\u57fa\u7840\u6982\u5ff5\u548c\u5df2\u6709\u7814\u7a76\u3002", "result": "\u68b3\u7406\u4e86\u4e0d\u786e\u5b9a\u6027\u4e0e\u98ce\u9669\u611f\u77e5\u9002\u5e94\u9886\u57df\u4e2d\u7684\u5173\u952e\u6982\u5ff5\u548c\u76f8\u5173\u5de5\u4f5c\u3002", "conclusion": "\u8be5\u6587\u4e3a\u540e\u7eed\u6df1\u5165\u7814\u7a76\u4e0d\u786e\u5b9a\u6027\u4e0e\u98ce\u9669\u611f\u77e5\u9002\u5e94\u63d0\u4f9b\u4e86\u7406\u8bba\u94fa\u57ab\u3002"}}
{"id": "2511.06362", "categories": ["cs.SE", "cs.AI", "cs.CY"], "pdf": "https://arxiv.org/pdf/2511.06362", "abs": "https://arxiv.org/abs/2511.06362", "authors": ["Anastasiia Birillo", "Aleksei Rostovskii", "Yaroslav Golubev", "Hieke Keuning"], "title": "Understanding Student Interaction with AI-Powered Next-Step Hints: Strategies and Challenges", "comment": "Accepted to SIGCSE'26. 7 pages, 3 figures", "summary": "Automated feedback generation plays a crucial role in enhancing personalized learning experiences in computer science education. Among different types of feedback, next-step hint feedback is particularly important, as it provides students with actionable steps to progress towards solving programming tasks. This study investigates how students interact with an AI-driven next-step hint system in an in-IDE learning environment. We gathered and analyzed a dataset from 34 students solving Kotlin tasks, containing detailed hint interaction logs. We applied process mining techniques and identified 16 common interaction scenarios. Semi-structured interviews with 6 students revealed strategies for managing unhelpful hints, such as adapting partial hints or modifying code to generate variations of the same hint. These findings, combined with our publicly available dataset, offer valuable opportunities for future research and provide key insights into student behavior, helping improve hint design for enhanced learning support.", "AI": {"tldr": "\u672c\u7814\u7a76\u901a\u8fc7\u5206\u679034\u540d\u5b66\u751f\u5728IDE\u4e2d\u4f7f\u7528AI\u9a71\u52a8\u7684\u4e0b\u4e00\u6b65\u63d0\u793a\u7cfb\u7edf\u7684\u4ea4\u4e92\u65e5\u5fd7\uff0c\u7ed3\u54086\u540d\u5b66\u751f\u7684\u8bbf\u8c08\uff0c\u8bc6\u522b\u51fa16\u79cd\u5e38\u89c1\u4ea4\u4e92\u573a\u666f\uff0c\u5e76\u63ed\u793a\u4e86\u5b66\u751f\u5e94\u5bf9\u65e0\u6548\u63d0\u793a\u7684\u7b56\u7565\uff0c\u4e3a\u6539\u8fdb\u7f16\u7a0b\u6559\u80b2\u4e2d\u7684\u63d0\u793a\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u4f9d\u636e\u3002", "motivation": "\u5728\u8ba1\u7b97\u673a\u79d1\u5b66\u6559\u80b2\u4e2d\uff0c\u81ea\u52a8\u5316\u53cd\u9988\uff08\u5c24\u5176\u662f\u4e0b\u4e00\u6b65\u63d0\u793a\uff09\u5bf9\u63d0\u5347\u4e2a\u6027\u5316\u5b66\u4e60\u4f53\u9a8c\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u5c1a\u7f3a\u4e4f\u5bf9\u5b66\u751f\u5982\u4f55\u4e0e\u8fd9\u7c7b\u7cfb\u7edf\u4e92\u52a8\u7684\u6df1\u5165\u7406\u89e3\u3002", "method": "\u6536\u96c634\u540d\u5b66\u751f\u89e3\u51b3Kotlin\u4efb\u52a1\u65f6\u7684\u63d0\u793a\u4ea4\u4e92\u65e5\u5fd7\uff0c\u5e94\u7528\u8fc7\u7a0b\u6316\u6398\u6280\u672f\u8bc6\u522b\u5e38\u89c1\u4ea4\u4e92\u6a21\u5f0f\uff0c\u5e76\u5bf96\u540d\u5b66\u751f\u8fdb\u884c\u534a\u7ed3\u6784\u5316\u8bbf\u8c08\u4ee5\u4e86\u89e3\u5176\u5e94\u5bf9\u7b56\u7565\u3002", "result": "\u8bc6\u522b\u51fa16\u79cd\u5e38\u89c1\u7684\u63d0\u793a\u4ea4\u4e92\u573a\u666f\uff1b\u5b66\u751f\u4f1a\u91c7\u7528\u5982\u8c03\u6574\u90e8\u5206\u63d0\u793a\u6216\u4fee\u6539\u4ee3\u7801\u4ee5\u751f\u6210\u76f8\u540c\u63d0\u793a\u53d8\u4f53\u7b49\u7b56\u7565\u6765\u5e94\u5bf9\u65e0\u5e2e\u52a9\u7684\u63d0\u793a\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u548c\u516c\u5f00\u6570\u636e\u96c6\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u57fa\u7840\uff0c\u5e76\u6709\u52a9\u4e8e\u4f18\u5316AI\u63d0\u793a\u7cfb\u7edf\u7684\u8bbe\u8ba1\uff0c\u4ece\u800c\u66f4\u597d\u5730\u652f\u6301\u5b66\u751f\u5b66\u4e60\u3002"}}
{"id": "2511.06367", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2511.06367", "abs": "https://arxiv.org/abs/2511.06367", "authors": ["Sara Mahdavi Hezavehi", "Danny Weyns", "Paris Avgeriou"], "title": "Methodological Considerations for Self-adaptive Systems: An Essay", "comment": "15", "summary": "In this essay, we provide an overview of methodological considerations necessary to lay out the foundation for our PhD research on uncertainty and risk-aware adaptation.", "AI": {"tldr": "\u672c\u6587\u6982\u8ff0\u4e86\u4e3a\u535a\u58eb\u7814\u7a76\u5960\u5b9a\u57fa\u7840\u6240\u9700\u7684\u65b9\u6cd5\u8bba\u8003\u91cf\uff0c\u805a\u7126\u4e8e\u4e0d\u786e\u5b9a\u6027\u4e0e\u98ce\u9669\u611f\u77e5\u9002\u5e94\u3002", "motivation": "\u4e3a\u5f00\u5c55\u5173\u4e8e\u4e0d\u786e\u5b9a\u6027\u4e0e\u98ce\u9669\u611f\u77e5\u9002\u5e94\u7684\u535a\u58eb\u7814\u7a76\uff0c\u9700\u660e\u786e\u76f8\u5173\u65b9\u6cd5\u8bba\u57fa\u7840\u3002", "method": "\u7efc\u8ff0\u5e76\u5206\u6790\u652f\u6491\u8be5\u7814\u7a76\u4e3b\u9898\u7684\u5173\u952e\u65b9\u6cd5\u8bba\u8981\u7d20\u3002", "result": "\u63d0\u51fa\u4e86\u9002\u7528\u4e8e\u4e0d\u786e\u5b9a\u6027\u4e0e\u98ce\u9669\u611f\u77e5\u9002\u5e94\u7814\u7a76\u7684\u65b9\u6cd5\u8bba\u6846\u67b6\u3002", "conclusion": "\u786e\u7acb\u6e05\u6670\u7684\u65b9\u6cd5\u8bba\u57fa\u7840\u5bf9\u63a8\u8fdb\u4e0d\u786e\u5b9a\u6027\u4e0e\u98ce\u9669\u611f\u77e5\u9002\u5e94\u7814\u7a76\u81f3\u5173\u91cd\u8981\u3002"}}
{"id": "2511.06428", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.06428", "abs": "https://arxiv.org/abs/2511.06428", "authors": ["Samuel Ferino", "Rashina Hoda", "John Grundy", "Christoph Treude"], "title": "Walking the Tightrope of LLMs for Software Development: A Practitioners' Perspective", "comment": null, "summary": "Background: Large Language Models emerged with the potential of provoking a revolution in software development (e.g., automating processes, workforce transformation). Although studies have started to investigate the perceived impact of LLMs for software development, there is a need for empirical studies to comprehend how to balance forward and backward effects of using LLMs. Objective: We investigated how LLMs impact software development and how to manage the impact from a software developer's perspective. Method: We conducted 22 interviews with software practitioners across 3 rounds of data collection and analysis, between October (2024) and September (2025). We employed socio-technical grounded theory (STGT) for data analysis to rigorously analyse interview participants' responses. Results: We identified the benefits (e.g., maintain software development flow, improve developers' mental model, and foster entrepreneurship) and disadvantages (e.g., negative impact on developers' personality and damage to developers' reputation) of using LLMs at individual, team, organisation, and society levels; as well as best practices on how to adopt LLMs. Conclusion: Critically, we present the trade-offs that software practitioners, teams, and organisations face in working with LLMs. Our findings are particularly useful for software team leaders and IT managers to assess the viability of LLMs within their specific context.", "AI": {"tldr": "\u8be5\u7814\u7a76\u901a\u8fc722\u6b21\u8bbf\u8c08\uff0c\u91c7\u7528\u793e\u4f1a\u6280\u672f\u624e\u6839\u7406\u8bba\u5206\u6790\u4e86\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5bf9\u8f6f\u4ef6\u5f00\u53d1\u5728\u4e2a\u4f53\u3001\u56e2\u961f\u3001\u7ec4\u7ec7\u548c\u793e\u4f1a\u5c42\u9762\u7684\u5229\u5f0a\uff0c\u5e76\u63d0\u51fa\u4e86\u91c7\u7eb3LLM\u7684\u6700\u4f73\u5b9e\u8df5\u4e0e\u6743\u8861\u5efa\u8bae\u3002", "motivation": "\u5c3d\u7ba1\u5df2\u6709\u7814\u7a76\u5f00\u59cb\u63a2\u8ba8\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5bf9\u8f6f\u4ef6\u5f00\u53d1\u7684\u611f\u77e5\u5f71\u54cd\uff0c\u4f46\u4ecd\u7f3a\u4e4f\u5b9e\u8bc1\u7814\u7a76\u6765\u7406\u89e3\u5982\u4f55\u5e73\u8861\u5176\u6b63\u5411\u4e0e\u8d1f\u5411\u6548\u5e94\u3002", "method": "\u57282024\u5e7410\u6708\u81f32025\u5e749\u6708\u671f\u95f4\uff0c\u7814\u7a76\u8005\u8fdb\u884c\u4e86\u4e09\u8f6e\u6570\u636e\u6536\u96c6\u4e0e\u5206\u6790\uff0c\u5171\u5bf922\u540d\u8f6f\u4ef6\u4ece\u4e1a\u8005\u8fdb\u884c\u8bbf\u8c08\uff0c\u5e76\u91c7\u7528\u793e\u4f1a\u6280\u672f\u624e\u6839\u7406\u8bba\uff08STGT\uff09\u5bf9\u8bbf\u8c08\u6570\u636e\u8fdb\u884c\u4e25\u8c28\u5206\u6790\u3002", "result": "\u7814\u7a76\u8bc6\u522b\u51fa\u4f7f\u7528LLM\u5728\u4e2a\u4f53\u3001\u56e2\u961f\u3001\u7ec4\u7ec7\u548c\u793e\u4f1a\u5c42\u9762\u7684\u76ca\u5904\uff08\u5982\u7ef4\u6301\u5f00\u53d1\u6d41\u7a0b\u3001\u63d0\u5347\u5f00\u53d1\u8005\u5fc3\u667a\u6a21\u578b\u3001\u4fc3\u8fdb\u521b\u4e1a\uff09\u548c\u5f0a\u7aef\uff08\u5982\u5bf9\u5f00\u53d1\u8005\u4e2a\u6027\u7684\u8d1f\u9762\u5f71\u54cd\u53ca\u58f0\u8a89\u635f\u5bb3\uff09\uff0c\u5e76\u603b\u7ed3\u4e86\u91c7\u7eb3LLM\u7684\u6700\u4f73\u5b9e\u8df5\u3002", "conclusion": "\u7814\u7a76\u63ed\u793a\u4e86\u8f6f\u4ef6\u4ece\u4e1a\u8005\u3001\u56e2\u961f\u548c\u7ec4\u7ec7\u5728\u4f7f\u7528LLM\u65f6\u9762\u4e34\u7684\u6743\u8861\uff0c\u5176\u53d1\u73b0\u5bf9\u8f6f\u4ef6\u56e2\u961f\u8d1f\u8d23\u4eba\u548cIT\u7ba1\u7406\u8005\u8bc4\u4f30LLM\u5728\u5176\u7279\u5b9a\u60c5\u5883\u4e0b\u7684\u53ef\u884c\u6027\u5177\u6709\u91cd\u8981\u53c2\u8003\u4ef7\u503c\u3002"}}
{"id": "2511.06501", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2511.06501", "abs": "https://arxiv.org/abs/2511.06501", "authors": ["Antu Saha", "Mehedi Sun", "Oscar Chaparro"], "title": "Automatically Identifying Solution-Related Content in Issue Report Discussions with Language Models", "comment": "34 pages, 4 figures", "summary": "During issue resolution, software developers rely on issue reports to discuss solutions for defects, feature requests, and other changes. These discussions contain proposed solutions-from design changes to code implementations-as well as their evaluations. Locating solution-related content is essential for investigating reopened issues, addressing regressions, reusing solutions, and understanding code change rationale. Manually understanding long discussions to identify such content can be difficult and time-consuming.\n  This paper automates solution identification using language models as supervised classifiers. We investigate three applications-embeddings, prompting, and fine-tuning-across three classifier types: traditional ML models (MLMs), pre-trained language models (PLMs), and large language models (LLMs). Using 356 Mozilla Firefox issues, we created a dataset to train and evaluate six MLMs, four PLMs, and two LLMs across 68 configurations.\n  Results show that MLMs with LLM embeddings outperform TF-IDF features, prompting underperforms, and fine-tuned LLMs achieve the highest performance, with LLAMAft reaching 0.716 F1 score. Ensembles of the best models further improve results (0.737 F1). Misclassifications often arise from misleading clues or missing context, highlighting the need for context-aware classifiers. Models trained on Mozilla transfer to other projects, with a small amount of project-specific data, further enhancing results. This work supports software maintenance, issue understanding, and solution reuse.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4f7f\u7528\u8bed\u8a00\u6a21\u578b\u81ea\u52a8\u8bc6\u522b\u8f6f\u4ef6\u95ee\u9898\u62a5\u544a\u4e2d\u7684\u89e3\u51b3\u65b9\u6848\u5185\u5bb9\uff0c\u6bd4\u8f83\u4e86\u5d4c\u5165\u3001\u63d0\u793a\u548c\u5fae\u8c03\u4e09\u79cd\u65b9\u6cd5\u5728\u4f20\u7edf\u673a\u5668\u5b66\u4e60\u6a21\u578b\uff08MLMs\uff09\u3001\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\uff08PLMs\uff09\u548c\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u4e0a\u7684\u8868\u73b0\u3002\u5b9e\u9a8c\u57fa\u4e8e356\u4e2aMozilla Firefox\u95ee\u9898\u6784\u5efa\u6570\u636e\u96c6\uff0c\u7ed3\u679c\u8868\u660e\u5fae\u8c03\u540e\u7684LLM\uff08\u5982LLAMAft\uff09\u6548\u679c\u6700\u4f73\uff08F1\u8fbe0.716\uff09\uff0c\u96c6\u6210\u6a21\u578b\u8fdb\u4e00\u6b65\u63d0\u5347\u81f30.737\uff1b\u6a21\u578b\u5177\u5907\u8de8\u9879\u76ee\u8fc1\u79fb\u80fd\u529b\uff0c\u5c11\u91cf\u76ee\u6807\u9879\u76ee\u6570\u636e\u5373\u53ef\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u5728\u8f6f\u4ef6\u5f00\u53d1\u4e2d\uff0c\u5f00\u53d1\u8005\u4f9d\u8d56\u95ee\u9898\u62a5\u544a\u8ba8\u8bba\u7f3a\u9677\u4fee\u590d\u3001\u529f\u80fd\u8bf7\u6c42\u7b49\u53d8\u66f4\u65b9\u6848\uff0c\u8fd9\u4e9b\u8ba8\u8bba\u5305\u542b\u5927\u91cf\u89e3\u51b3\u65b9\u6848\u53ca\u5176\u8bc4\u4f30\u4fe1\u606f\u3002\u624b\u52a8\u4ece\u5197\u957f\u8ba8\u8bba\u4e2d\u5b9a\u4f4d\u89e3\u51b3\u65b9\u6848\u5185\u5bb9\u8d39\u65f6\u8d39\u529b\uff0c\u56e0\u6b64\u4e9f\u9700\u81ea\u52a8\u5316\u65b9\u6cd5\u4ee5\u652f\u6301\u95ee\u9898\u91cd\u5f00\u8c03\u67e5\u3001\u56de\u5f52\u5206\u6790\u3001\u65b9\u6848\u590d\u7528\u53ca\u4ee3\u7801\u53d8\u66f4\u7406\u89e3\u3002", "method": "\u672c\u6587\u5c06\u89e3\u51b3\u65b9\u6848\u8bc6\u522b\u4efb\u52a1\u5efa\u6a21\u4e3a\u76d1\u7763\u5206\u7c7b\u95ee\u9898\uff0c\u7cfb\u7edf\u8bc4\u4f30\u4e09\u79cd\u5e94\u7528\u65b9\u5f0f\uff08\u5d4c\u5165\u3001\u63d0\u793a\u3001\u5fae\u8c03\uff09\u5728\u4e09\u7c7b\u6a21\u578b\uff08MLMs\u3001PLMs\u3001LLMs\uff09\u4e0a\u7684\u6548\u679c\u3002\u57fa\u4e8e356\u4e2aMozilla Firefox\u95ee\u9898\u6784\u5efa\u6807\u6ce8\u6570\u636e\u96c6\uff0c\u8bad\u7ec3\u5e76\u8bc4\u4f306\u79cdMLMs\u30014\u79cdPLMs\u548c2\u79cdLLMs\u517168\u79cd\u914d\u7f6e\uff0c\u5e76\u6d4b\u8bd5\u6a21\u578b\u8de8\u9879\u76ee\u8fc1\u79fb\u80fd\u529b\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff1aMLMs\u7ed3\u5408LLM\u5d4c\u5165\u4f18\u4e8eTF-IDF\u7279\u5f81\uff1b\u63d0\u793a\u65b9\u6cd5\u8868\u73b0\u4e0d\u4f73\uff1b\u5fae\u8c03LLM\u6548\u679c\u6700\u4f18\uff0c\u5176\u4e2dLLAMAft\u8fbe\u52300.716 F1\u5206\u6570\uff1b\u6700\u4f73\u6a21\u578b\u96c6\u6210\u540eF1\u63d0\u5347\u81f30.737\uff1b\u9519\u8bef\u5206\u7c7b\u4e3b\u8981\u6e90\u4e8e\u8bef\u5bfc\u6027\u7ebf\u7d22\u6216\u4e0a\u4e0b\u6587\u7f3a\u5931\uff1b\u6a21\u578b\u5728\u5c11\u91cf\u76ee\u6807\u9879\u76ee\u6570\u636e\u4e0b\u53ef\u6709\u6548\u8fc1\u79fb\u5230\u5176\u4ed6\u9879\u76ee\u3002", "conclusion": "\u5fae\u8c03\u5927\u8bed\u8a00\u6a21\u578b\u662f\u81ea\u52a8\u8bc6\u522b\u95ee\u9898\u62a5\u544a\u4e2d\u89e3\u51b3\u65b9\u6848\u5185\u5bb9\u7684\u6709\u6548\u65b9\u6cd5\uff0c\u5177\u5907\u826f\u597d\u6027\u80fd\u4e0e\u8de8\u9879\u76ee\u9002\u5e94\u6027\u3002\u8be5\u5de5\u4f5c\u6709\u52a9\u4e8e\u63d0\u5347\u8f6f\u4ef6\u7ef4\u62a4\u6548\u7387\u3001\u95ee\u9898\u7406\u89e3\u6df1\u5ea6\u548c\u89e3\u51b3\u65b9\u6848\u590d\u7528\u80fd\u529b\uff0c\u672a\u6765\u9700\u8fdb\u4e00\u6b65\u589e\u5f3a\u6a21\u578b\u5bf9\u4e0a\u4e0b\u6587\u7684\u611f\u77e5\u80fd\u529b\u3002"}}
{"id": "2511.06552", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.06552", "abs": "https://arxiv.org/abs/2511.06552", "authors": ["Mostafijur Rahman Akhond", "Saikat Chakraborty", "Gias Uddin"], "title": "LLM For Loop Invariant Generation and Fixing: How Far Are We?", "comment": "This work has been submitted to the IEEE for possible publication", "summary": "A loop invariant is a property of a loop that remains true before and after each execution of the loop. The identification of loop invariants is a critical step to support automated program safety assessment. Recent advancements in Large Language Models (LLMs) have demonstrated potential in diverse software engineering (SE) and formal verification tasks. However, we are not aware of the performance of LLMs to infer loop invariants. We report an empirical study of both open-source and closed-source LLMs of varying sizes to assess their proficiency in inferring inductive loop invariants for programs and in fixing incorrect invariants. Our findings reveal that while LLMs exhibit some utility in inferring and repairing loop invariants, their performance is substantially enhanced when supplemented with auxiliary information such as domain knowledge and illustrative examples. LLMs achieve a maximum success rate of 78\\% in generating, but are limited to 16\\% in repairing the invariant.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u63a8\u65ad\u548c\u4fee\u590d\u7a0b\u5e8f\u5faa\u73af\u4e0d\u53d8\u5f0f\u65b9\u9762\u7684\u8868\u73b0\uff0c\u53d1\u73b0\u5176\u5728\u751f\u6210\u4efb\u52a1\u4e2d\u6700\u9ad8\u6210\u529f\u7387\u8fbe78%\uff0c\u4f46\u5728\u4fee\u590d\u4efb\u52a1\u4e2d\u4ec5\u8fbe16%\uff1b\u8f85\u52a9\u4fe1\u606f\u5982\u9886\u57df\u77e5\u8bc6\u548c\u793a\u4f8b\u80fd\u663e\u8457\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u5faa\u73af\u4e0d\u53d8\u5f0f\u5bf9\u81ea\u52a8\u5316\u7a0b\u5e8f\u5b89\u5168\u6027\u9a8c\u8bc1\u81f3\u5173\u91cd\u8981\uff0c\u800c\u76ee\u524d\u5c1a\u4e0d\u6e05\u695a\u5927\u8bed\u8a00\u6a21\u578b\u5728\u63a8\u65ad\u5faa\u73af\u4e0d\u53d8\u5f0f\u65b9\u9762\u7684\u5b9e\u9645\u80fd\u529b\u3002", "method": "\u5bf9\u591a\u79cd\u5f00\u6e90\u4e0e\u95ed\u6e90\u3001\u4e0d\u540c\u89c4\u6a21\u7684\u5927\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u5b9e\u8bc1\u7814\u7a76\uff0c\u8bc4\u4f30\u5176\u5728\u63a8\u65ad\u5f52\u7eb3\u6027\u5faa\u73af\u4e0d\u53d8\u5f0f\u53ca\u4fee\u590d\u9519\u8bef\u4e0d\u53d8\u5f0f\u65b9\u9762\u7684\u8868\u73b0\u3002", "result": "\u5927\u8bed\u8a00\u6a21\u578b\u5728\u751f\u6210\u5faa\u73af\u4e0d\u53d8\u5f0f\u65b9\u9762\u6700\u9ad8\u6210\u529f\u7387\u4e3a78%\uff0c\u4f46\u5728\u4fee\u590d\u4efb\u52a1\u4e2d\u4ec5\u4e3a16%\uff1b\u5f15\u5165\u9886\u57df\u77e5\u8bc6\u548c\u793a\u4f8b\u7b49\u8f85\u52a9\u4fe1\u606f\u53ef\u663e\u8457\u63d0\u5347\u5176\u6027\u80fd\u3002", "conclusion": "\u5927\u8bed\u8a00\u6a21\u578b\u5728\u5faa\u73af\u4e0d\u53d8\u5f0f\u63a8\u65ad\u4e0e\u4fee\u590d\u4e2d\u5177\u6709\u4e00\u5b9a\u6f5c\u529b\uff0c\u4f46\u9700\u4f9d\u8d56\u989d\u5916\u8f85\u52a9\u4fe1\u606f\u624d\u80fd\u6709\u6548\u63d0\u5347\u51c6\u786e\u7387\uff0c\u5c24\u5176\u5728\u4fee\u590d\u4efb\u52a1\u4e2d\u8868\u73b0\u4ecd\u6709\u9650\u3002"}}
{"id": "2511.06661", "categories": ["cs.SE", "cs.CR"], "pdf": "https://arxiv.org/pdf/2511.06661", "abs": "https://arxiv.org/abs/2511.06661", "authors": ["Tapti Palit", "Seyedhamed Ghavamnia", "Michalis Polychronakis"], "title": "PhaseSeed: Precise Call Graph Construction for Split-Phase Applications using Dynamic Seeding", "comment": null, "summary": "Precise and sound call graph construction is crucial for many software security mechanisms. Unfortunately, traditional static pointer analysis techniques used to generate application call graphs suffer from imprecision. These techniques are agnostic to the application's architecture and are designed for broad applicability. To mitigate this precision problem, we propose PhaseSeed, a novel technique that improves the accuracy of pointer analysis for split-phase applications, which have distinct initialization and processing phases. PhaseSeed analyzes the initialization phase dynamically, collecting the points-to relationships established at runtime. At the end of the initialization phase, it then seeds this information to a static analysis stage that performs pointer analysis for all code that stays in scope during the processing phase, improving precision. Our observations show that, given the same runtime configuration options, the points-to relationships established during the initialization phase remain constant across multiple runs. Therefore, PhaseSeed is sound with respect to a given initial configuration. We apply PhaseSeed to three security mechanisms: control flow integrity (CFI), software debloating, and system call filtering. PhaseSeed provides up to 92.6% precision improvement for CFI compared to static call graph construction techniques, and filters nine additional security-critical system calls when used to generate Seccomp profiles.", "AI": {"tldr": "PhaseSeed is a novel technique that enhances pointer analysis precision for split-phase applications by dynamically analyzing the initialization phase and seeding runtime points-to information into static analysis, significantly improving security mechanisms like CFI, debloating, and system call filtering.", "motivation": "Traditional static pointer analysis techniques are imprecise because they ignore application architecture and aim for broad applicability, which undermines the effectiveness of security mechanisms relying on accurate call graphs.", "method": "PhaseSeed dynamically analyzes the initialization phase of split-phase applications to collect runtime points-to relationships, then seeds this information into a subsequent static pointer analysis focused on code active during the processing phase.", "result": "PhaseSeed improves CFI precision by up to 92.6% over traditional static methods and enables Seccomp profiles to filter nine additional security-critical system calls; it is sound under a fixed runtime configuration.", "conclusion": "By leveraging phase-specific dynamic information to guide static analysis, PhaseSeed significantly boosts the precision and effectiveness of software security mechanisms dependent on accurate call graphs."}}
{"id": "2511.06701", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.06701", "abs": "https://arxiv.org/abs/2511.06701", "authors": ["Karen Sargsyan"], "title": "Structural Enforcement of Statistical Rigor in AI-Driven Discovery: A Functional Architecture", "comment": null, "summary": "Sequential statistical protocols require meticulous state management and robust error handling -- challenges naturally suited to functional programming. We present a functional architecture for structural enforcement of statistical rigor in automated research systems (AI-Scientists). These LLM-driven systems risk generating spurious discoveries through dynamic hypothesis testing. We introduce the Research monad, a Haskell eDSL that enforces sequential statistical protocols (e.g., Online FDR (false discovery rate) control) using a monad transformer stack. To address risks in hybrid architectures where LLMs generate imperative code, we employ Declarative Scaffolding -- generating rigid harnesses that structurally constrain execution and prevent methodological errors like data leakage. We validate this approach through large-scale simulation (N=2000 hypotheses) and an end-to-end case study, demonstrating essential defense-in-depth for automated science integrity.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u51fd\u6570\u5f0f\u7f16\u7a0b\u7684\u67b6\u6784\uff0c\u5229\u7528Haskell\u4e2d\u7684Research monad\u548c\u58f0\u660e\u5f0f\u811a\u624b\u67b6\uff0c\u4ee5\u7ed3\u6784\u5316\u65b9\u5f0f\u4fdd\u969cAI\u9a71\u52a8\u79d1\u7814\u7cfb\u7edf\u4e2d\u7684\u7edf\u8ba1\u4e25\u8c28\u6027\uff0c\u9632\u6b62\u52a8\u6001\u5047\u8bbe\u68c0\u9a8c\u5bfc\u81f4\u7684\u865a\u5047\u53d1\u73b0\u3002", "motivation": "AI\u9a71\u52a8\u7684\u81ea\u52a8\u5316\u79d1\u7814\u7cfb\u7edf\uff08AI-Scientists\uff09\u5728\u52a8\u6001\u5047\u8bbe\u68c0\u9a8c\u4e2d\u5bb9\u6613\u4ea7\u751f\u865a\u5047\u53d1\u73b0\uff0c\u7f3a\u4e4f\u5bf9\u987a\u5e8f\u7edf\u8ba1\u534f\u8bae\uff08\u5982\u5728\u7ebfFDR\u63a7\u5236\uff09\u7684\u7ed3\u6784\u6027\u4fdd\u969c\uff0c\u4e9f\u9700\u4e00\u79cd\u80fd\u5f3a\u5236\u6267\u884c\u7edf\u8ba1\u4e25\u8c28\u6027\u7684\u65b9\u6cd5\u3002", "method": "\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u540d\u4e3aResearch monad\u7684Haskell\u5d4c\u5165\u5f0f\u9886\u57df\u7279\u5b9a\u8bed\u8a00\uff08eDSL\uff09\uff0c\u901a\u8fc7monad\u53d8\u6362\u5668\u6808\u5f3a\u5236\u5b9e\u65bd\u987a\u5e8f\u7edf\u8ba1\u534f\u8bae\uff1b\u540c\u65f6\u91c7\u7528\u58f0\u660e\u5f0f\u811a\u624b\u67b6\uff08Declarative Scaffolding\uff09\u751f\u6210\u4e25\u683c\u7ea6\u675f\u6267\u884c\u6d41\u7a0b\u7684\u4ee3\u7801\u6846\u67b6\uff0c\u9632\u6b62\u6570\u636e\u6cc4\u9732\u7b49\u65b9\u6cd5\u8bba\u9519\u8bef\u3002", "result": "\u901a\u8fc7\u5927\u89c4\u6a21\u6a21\u62df\uff08N=2000\u4e2a\u5047\u8bbe\uff09\u548c\u7aef\u5230\u7aef\u6848\u4f8b\u7814\u7a76\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u5c55\u793a\u4e86\u5176\u5728\u4fdd\u969c\u81ea\u52a8\u5316\u79d1\u7814\u5b8c\u6574\u6027\u65b9\u9762\u7684\u7eb5\u6df1\u9632\u5fa1\u80fd\u529b\u3002", "conclusion": "\u51fd\u6570\u5f0f\u7f16\u7a0b\u7ed3\u5408\u58f0\u660e\u5f0f\u811a\u624b\u67b6\u80fd\u6709\u6548\u7ed3\u6784\u5316\u5730\u4fdd\u969cAI\u79d1\u7814\u7cfb\u7edf\u7684\u7edf\u8ba1\u4e25\u8c28\u6027\uff0c\u662f\u5b9e\u73b0\u53ef\u9760\u81ea\u52a8\u5316\u79d1\u5b66\u7684\u91cd\u8981\u8def\u5f84\u3002"}}
{"id": "2511.06762", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2511.06762", "abs": "https://arxiv.org/abs/2511.06762", "authors": ["Rui Lu", "Lyuye Zhang", "Kaixuan Li", "Min Zhang", "Yixiang Chen"], "title": "Minimizing Breaking Changes and Redundancy in Mitigating Technical Lag for Java Projects", "comment": "ICSE26 accepted paper", "summary": "Re-using open-source software (OSS) can avoid reinventing the wheel, but failing to keep it up-to-date can lead to missing new features and persistent bugs or vulnerabilities that have already been resolved. The use of outdated OSS libraries introduces technical lag, necessitating timely upgrades. However, maintaining up-to-date libraries is challenging, as it may introduce incompatibility issues that break the project or redundant dependencies that unnecessarily increase the size of the project. These issues discourage developers from upgrading libraries, highlighting the need for a fully automated solution that balances version upgrades, reduces technical lag, ensures compatibility, and avoids redundant dependencies.\n  To this end, we propose DepUpdater, which ensures that upgrades minimize technical lag as much as possible while avoiding incompatibility issues and redundant dependencies. The comparison with existing dependency management tools demonstrates that DepUpdater more effectively reduces technical lag while ensuring compatibility and pruning redundant dependencies. Additionally, an ablation study highlights the potential benefits of considering pruning requirements during upgrades to mitigate incompatibility issues. Finally, leveraging DepUpdater, we investigate the impact of transitive dependency upgrades on client compatibility, providing insights for future research.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86DepUpdater\uff0c\u4e00\u79cd\u81ea\u52a8\u5316\u4f9d\u8d56\u66f4\u65b0\u5de5\u5177\uff0c\u5728\u6700\u5c0f\u5316\u6280\u672f\u6ede\u540e\u7684\u540c\u65f6\u907f\u514d\u4e0d\u517c\u5bb9\u95ee\u9898\u548c\u5197\u4f59\u4f9d\u8d56\u3002", "motivation": "\u5f00\u53d1\u8005\u5728\u5347\u7ea7\u5f00\u6e90\u8f6f\u4ef6\u5e93\u65f6\u9762\u4e34\u517c\u5bb9\u6027\u95ee\u9898\u548c\u5197\u4f59\u4f9d\u8d56\u7684\u6311\u6218\uff0c\u5bfc\u81f4\u4e0d\u613f\u53ca\u65f6\u66f4\u65b0\uff0c\u4ece\u800c\u4ea7\u751f\u6280\u672f\u6ede\u540e\uff1b\u56e0\u6b64\u9700\u8981\u4e00\u4e2a\u80fd\u81ea\u52a8\u5e73\u8861\u5347\u7ea7\u3001\u517c\u5bb9\u6027\u548c\u4f9d\u8d56\u7cbe\u7b80\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u63d0\u51faDepUpdater\u5de5\u5177\uff0c\u7efc\u5408\u8003\u8651\u7248\u672c\u5347\u7ea7\u3001\u517c\u5bb9\u6027\u4fdd\u969c\u4e0e\u5197\u4f59\u4f9d\u8d56\u526a\u679d\uff0c\u901a\u8fc7\u5bf9\u6bd4\u5b9e\u9a8c\u548c\u6d88\u878d\u7814\u7a76\u9a8c\u8bc1\u5176\u6709\u6548\u6027\u3002", "result": "DepUpdater\u76f8\u6bd4\u73b0\u6709\u4f9d\u8d56\u7ba1\u7406\u5de5\u5177\u80fd\u66f4\u6709\u6548\u5730\u51cf\u5c11\u6280\u672f\u6ede\u540e\u3001\u786e\u4fdd\u517c\u5bb9\u6027\u5e76\u526a\u679d\u5197\u4f59\u4f9d\u8d56\uff1b\u6d88\u878d\u7814\u7a76\u663e\u793a\u5728\u5347\u7ea7\u8fc7\u7a0b\u4e2d\u8003\u8651\u526a\u679d\u6709\u52a9\u4e8e\u7f13\u89e3\u4e0d\u517c\u5bb9\u95ee\u9898\uff1b\u6b64\u5916\u8fd8\u63a2\u8ba8\u4e86\u4f20\u9012\u4f9d\u8d56\u5347\u7ea7\u5bf9\u5ba2\u6237\u7aef\u517c\u5bb9\u6027\u7684\u5f71\u54cd\u3002", "conclusion": "DepUpdater\u4e3a\u5f00\u6e90\u4f9d\u8d56\u81ea\u52a8\u66f4\u65b0\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u65b9\u6cd5\uff0c\u517c\u987e\u6280\u672f\u6ede\u540e\u51cf\u5c11\u3001\u517c\u5bb9\u6027\u7ef4\u62a4\u548c\u4f9d\u8d56\u7cbe\u7b80\uff0c\u4e3a\u672a\u6765\u76f8\u5173\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2511.06864", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2511.06864", "abs": "https://arxiv.org/abs/2511.06864", "authors": ["Pallav Jain", "Yuvraj Agrawal", "Ashutosh Nigam", "Pushpak Patil"], "title": "MetricSynth: Framework for Aggregating DORA and KPI Metrics Across Multi-Platform Engineering", "comment": null, "summary": "In modern, large-scale software development, engineering leaders face the significant challenge of gaining a holistic and data-driven view of team performance and system health. Data is often siloed across numerous disparate tools, making manual report generation time-consuming and prone to inconsistencies. This paper presents the architecture and implementation of a centralized framework designed to provide near-real-time visibility into developer experience (DevEx) and Key Performance Indicator (KPI) metrics for a software ecosystem. By aggregating data from various internal tools and platforms, the system computes and visualizes metrics across key areas such as Developer Productivity, Quality, and Operational Efficiency. The architecture features a cron-based data ingestion layer, a dual-schema data storage approach, a processing engine for metric pre-computation, a proactive alerting system, and utilizes the open-source BI tool Metabase for visualization, all secured with role-based access control (RBAC). The implementation resulted in a significant reduction in manual reporting efforts, saving an estimated 20 person-hours per week, and enabled faster, data-driven bottleneck identification. Finally, we evaluate the system's scalability and discuss its trade-offs, positioning it as a valuable contribution to engineering intelligence platforms.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u5e76\u5b9e\u73b0\u4e86\u4e00\u4e2a\u96c6\u4e2d\u5f0f\u6846\u67b6\uff0c\u7528\u4e8e\u6574\u5408\u591a\u6e90\u5f00\u53d1\u6570\u636e\uff0c\u5b9e\u65f6\u53ef\u89c6\u5316\u5f00\u53d1\u8005\u4f53\u9a8c\uff08DevEx\uff09\u548c\u5173\u952e\u7ee9\u6548\u6307\u6807\uff08KPI\uff09\uff0c\u663e\u8457\u51cf\u5c11\u4eba\u5de5\u62a5\u544a\u5de5\u4f5c\u91cf\u5e76\u63d0\u5347\u51b3\u7b56\u6548\u7387\u3002", "motivation": "\u73b0\u4ee3\u5927\u578b\u8f6f\u4ef6\u5f00\u53d1\u4e2d\uff0c\u6570\u636e\u5206\u6563\u5728\u591a\u4e2a\u5de5\u5177\u4e2d\uff0c\u5bfc\u81f4\u56e2\u961f\u7ee9\u6548\u4e0e\u7cfb\u7edf\u5065\u5eb7\u72b6\u51b5\u96be\u4ee5\u83b7\u5f97\u7edf\u4e00\u3001\u5b9e\u65f6\u3001\u6570\u636e\u9a71\u52a8\u7684\u89c6\u56fe\uff0c\u4eba\u5de5\u751f\u6210\u62a5\u544a\u8017\u65f6\u4e14\u6613\u51fa\u9519\u3002", "method": "\u6784\u5efa\u4e00\u4e2a\u5305\u542b\u5b9a\u65f6\u6570\u636e\u6444\u53d6\u5c42\u3001\u53cc\u6a21\u5f0f\u6570\u636e\u5b58\u50a8\u3001\u6307\u6807\u9884\u8ba1\u7b97\u5f15\u64ce\u3001\u4e3b\u52a8\u544a\u8b66\u673a\u5236\u548c\u57fa\u4e8eMetabase\u7684\u53ef\u89c6\u5316\u754c\u9762\u7684\u96c6\u4e2d\u5f0f\u67b6\u6784\uff0c\u5e76\u901a\u8fc7\u57fa\u4e8e\u89d2\u8272\u7684\u8bbf\u95ee\u63a7\u5236\u4fdd\u969c\u5b89\u5168\u3002", "result": "\u7cfb\u7edf\u90e8\u7f72\u540e\u6bcf\u5468\u8282\u7701\u7ea620\u4eba\u65f6\u7684\u624b\u52a8\u62a5\u544a\u5de5\u4f5c\uff0c\u5e76\u652f\u6301\u66f4\u5feb\u5730\u8bc6\u522b\u5f00\u53d1\u74f6\u9888\uff1b\u540c\u65f6\u8bc4\u4f30\u4e86\u7cfb\u7edf\u7684\u53ef\u6269\u5c55\u6027\u53ca\u5176\u8bbe\u8ba1\u6743\u8861\u3002", "conclusion": "\u8be5\u6846\u67b6\u6709\u6548\u63d0\u5347\u4e86\u5de5\u7a0b\u667a\u80fd\u5e73\u53f0\u7684\u6570\u636e\u6574\u5408\u4e0e\u6d1e\u5bdf\u80fd\u529b\uff0c\u4e3a\u5f00\u53d1\u8005\u4f53\u9a8c\u548c\u6548\u80fd\u5ea6\u91cf\u63d0\u4f9b\u4e86\u5b9e\u7528\u3001\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2511.06885", "categories": ["cs.SE", "cs.SI"], "pdf": "https://arxiv.org/pdf/2511.06885", "abs": "https://arxiv.org/abs/2511.06885", "authors": ["Davis Byamugisha", "Francis Kamuganga", "Adones Rukundo", "John Businge"], "title": "A Collaborative Model for Improving Information Sharing among Cancer Care Groups using Software Engineering Principles", "comment": null, "summary": "Effective treatment of cancer requires early diagnosis which involves the patient's awareness of the early signs and symptoms, leading to a consultation with a health provider, who would then promptly refer the patient for confirmation of the diagnosis and thereafter treatment. However, this is not always the case because of delays arising from limited skilled manpower and health information management systems that are neither integrated nor organized in their design hence leading to information gap among care groups. Existing methods focus on using accumulated data to support decision making, enhancing the sharing of secondary data while others exclude some critical stakeholders like patient caretakers and administrators thus, leaving an information gap that creates delays and miscommunication during case management. We however notice some similarities between cancer treatment and software engineering information management especially when progress history needs to be maintained (versioning).\n  We analyze the similarities and propose a model for information sharing among cancer care groups using the software engineering principles approach. We model for reducing delays and improving coordination among care groups in cancer case management. Model design was guided by software engineering principles adopted in GitHub version control system for bug fixing in open-source code projects. Any-Logic simulation software was used to mimic the model realism in a virtual environment. Results show that bug resolution principles from software engineering and GitHub version control system can be adopted to coordinate collaboration and information sharing among care groups in a cancer case management environment while involving all stakeholders to improve care treatment outcomes, ensure early diagnosis and increase patient's survival chances.", "AI": {"tldr": "\u672c\u6587\u501f\u9274\u8f6f\u4ef6\u5de5\u7a0b\u4e2d\u7684\u7248\u672c\u63a7\u5236\u7406\u5ff5\uff08\u7279\u522b\u662fGitHub\u7684bug\u4fee\u590d\u673a\u5236\uff09\uff0c\u63d0\u51fa\u4e00\u79cd\u7528\u4e8e\u764c\u75c7\u8bca\u7597\u56e2\u961f\u95f4\u4fe1\u606f\u5171\u4eab\u4e0e\u534f\u4f5c\u7684\u65b0\u6a21\u578b\uff0c\u4ee5\u51cf\u5c11\u8bca\u7597\u5ef6\u8fdf\u3001\u63d0\u5347\u65e9\u671f\u8bca\u65ad\u7387\u548c\u60a3\u8005\u751f\u5b58\u7387\u3002", "motivation": "\u5f53\u524d\u764c\u75c7\u8bca\u7597\u8fc7\u7a0b\u4e2d\u5b58\u5728\u56e0\u533b\u7597\u4eba\u529b\u4e0d\u8db3\u53ca\u4fe1\u606f\u7cfb\u7edf\u4e0d\u6574\u5408\u800c\u5bfc\u81f4\u7684\u4fe1\u606f\u65ad\u5c42\uff0c\u73b0\u6709\u65b9\u6cd5\u672a\u80fd\u5145\u5206\u7eb3\u5165\u6240\u6709\u5173\u952e\u5229\u76ca\u76f8\u5173\u8005\uff08\u5982\u60a3\u8005\u7167\u62a4\u8005\u548c\u7ba1\u7406\u4eba\u5458\uff09\uff0c\u9020\u6210\u8bca\u7597\u5ef6\u8bef\u4e0e\u6c9f\u901a\u969c\u788d\u3002", "method": "\u4f5c\u8005\u5206\u6790\u764c\u75c7\u6cbb\u7597\u4e0e\u8f6f\u4ef6\u5de5\u7a0b\u5728\u4fe1\u606f\u7ba1\u7406\u4e0a\u7684\u76f8\u4f3c\u6027\uff0c\u57fa\u4e8eGitHub\u7248\u672c\u63a7\u5236\u7cfb\u7edf\u4e2d\u7684bug\u4fee\u590d\u539f\u5219\u8bbe\u8ba1\u4fe1\u606f\u5171\u4eab\u6a21\u578b\uff0c\u5e76\u5229\u7528AnyLogic\u4eff\u771f\u8f6f\u4ef6\u5728\u865a\u62df\u73af\u5883\u4e2d\u9a8c\u8bc1\u6a21\u578b\u6548\u679c\u3002", "result": "\u4eff\u771f\u7ed3\u679c\u8868\u660e\uff0c\u91c7\u7528\u8f6f\u4ef6\u5de5\u7a0b\u4e2d\u7684bug\u4fee\u590d\u4e0e\u7248\u672c\u63a7\u5236\u673a\u5236\u53ef\u6709\u6548\u4fc3\u8fdb\u764c\u75c7\u8bca\u7597\u56e2\u961f\u4e4b\u95f4\u7684\u534f\u4f5c\u4e0e\u4fe1\u606f\u5171\u4eab\uff0c\u6db5\u76d6\u6240\u6709\u5229\u76ca\u76f8\u5173\u8005\uff0c\u4ece\u800c\u6539\u5584\u6cbb\u7597\u7ed3\u679c\u3001\u5b9e\u73b0\u65e9\u671f\u8bca\u65ad\u5e76\u63d0\u9ad8\u60a3\u8005\u751f\u5b58\u51e0\u7387\u3002", "conclusion": "\u5c06\u8f6f\u4ef6\u5de5\u7a0b\u4e2d\u7684\u4fe1\u606f\u7ba1\u7406\u539f\u5219\u5e94\u7528\u4e8e\u764c\u75c7\u8bca\u7597\u6d41\u7a0b\uff0c\u6709\u52a9\u4e8e\u6784\u5efa\u9ad8\u6548\u3001\u534f\u540c\u3001\u5168\u5458\u53c2\u4e0e\u7684\u4fe1\u606f\u5171\u4eab\u673a\u5236\uff0c\u663e\u8457\u63d0\u5347\u764c\u75c7\u8bca\u7597\u6548\u7387\u4e0e\u60a3\u8005\u9884\u540e\u3002"}}
{"id": "2511.07017", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.07017", "abs": "https://arxiv.org/abs/2511.07017", "authors": ["Ruida Hu", "Xinchen Wang", "Xin-Cheng Wen", "Zhao Zhang", "Bo Jiang", "Pengfei Gao", "Chao Peng", "Cuiyun Gao"], "title": "Benchmarking LLMs for Fine-Grained Code Review with Enriched Context in Practice", "comment": null, "summary": "Code review is a cornerstone of software quality assurance, and recent advances in Large Language Models (LLMs) have shown promise in automating this process. However, existing benchmarks for LLM-based code review face three major limitations. (1) Lack of semantic context: most benchmarks provide only code diffs without textual information such as issue descriptions, which are crucial for understanding developer intent. (2) Data quality issues: without rigorous validation, many samples are noisy-e.g., reviews on outdated or irrelevant code-reducing evaluation reliability. (3) Coarse granularity: most benchmarks operate at the file or commit level, overlooking the fine-grained, line-level reasoning essential for precise review.\n  We introduce ContextCRBench, a high-quality, context-rich benchmark for fine-grained LLM evaluation in code review. Our construction pipeline comprises: (1) Raw Data Crawling, collecting 153.7K issues and pull requests from top-tier repositories; (2) Comprehensive Context Extraction, linking issue-PR pairs for textual context and extracting the full surrounding function or class for code context; and (3) Multi-stage Data Filtering, combining rule-based and LLM-based validation to remove outdated, malformed, or low-value samples, resulting in 67,910 context-enriched entries.\n  ContextCRBench supports three evaluation scenarios aligned with the review workflow: (1) hunk-level quality assessment, (2) line-level defect localization, and (3) line-level comment generation. Evaluating eight leading LLMs (four closed-source and four open-source) reveals that textual context yields greater performance gains than code context alone, while current LLMs remain far from human-level review ability. Deployed at ByteDance, ContextCRBench drives a self-evolving code review system, improving performance by 61.98% and demonstrating its robustness and industrial utility.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86ContextCRBench\uff0c\u4e00\u4e2a\u9ad8\u8d28\u91cf\u3001\u5bcc\u542b\u4e0a\u4e0b\u6587\u7684\u7ec6\u7c92\u5ea6\u4ee3\u7801\u5ba1\u67e5\u57fa\u51c6\uff0c\u901a\u8fc7\u5f15\u5165\u95ee\u9898\u63cf\u8ff0\u7b49\u6587\u672c\u4e0a\u4e0b\u6587\u548c\u5b8c\u6574\u7684\u4ee3\u7801\u4e0a\u4e0b\u6587\uff0c\u5e76\u7ed3\u5408\u591a\u9636\u6bb5\u6570\u636e\u8fc7\u6ee4\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u57fa\u51c6\u7f3a\u4e4f\u8bed\u4e49\u4e0a\u4e0b\u6587\u3001\u6570\u636e\u8d28\u91cf\u5dee\u548c\u7c92\u5ea6\u7c97\u7cd9\u7684\u95ee\u9898\u3002\u8be5\u57fa\u51c6\u652f\u6301\u4e09\u4e2a\u8bc4\u4f30\u573a\u666f\uff0c\u5e76\u5728\u5b9e\u9645\u5de5\u4e1a\u90e8\u7f72\u4e2d\u663e\u8457\u63d0\u5347\u4e86\u4ee3\u7801\u5ba1\u67e5\u7cfb\u7edf\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u4ee3\u7801\u5ba1\u67e5\u57fa\u51c6\u5b58\u5728\u4e09\u5927\u5c40\u9650\uff1a\u7f3a\u4e4f\u8bed\u4e49\u4e0a\u4e0b\u6587\uff08\u5982\u95ee\u9898\u63cf\u8ff0\uff09\u3001\u6570\u636e\u8d28\u91cf\u4e0d\u9ad8\uff08\u5305\u542b\u8fc7\u65f6\u6216\u65e0\u5173\u6837\u672c\uff09\u4ee5\u53ca\u7c92\u5ea6\u8fc7\u7c97\uff08\u4ec5\u5728\u6587\u4ef6\u6216\u63d0\u4ea4\u7ea7\u522b\uff09\uff0c\u65e0\u6cd5\u652f\u6301\u7ec6\u7c92\u5ea6\u7684\u7cbe\u51c6\u5ba1\u67e5\u3002", "method": "\u6784\u5efaContextCRBench\u57fa\u51c6\uff0c\u5305\u62ec\u4e09\u4e2a\u6b65\u9aa4\uff1a(1) \u4ece\u9876\u7ea7\u4ee3\u7801\u4ed3\u5e93\u722c\u53d6153.7K\u4e2a\u95ee\u9898\u548c\u62c9\u53d6\u8bf7\u6c42\uff1b(2) \u63d0\u53d6\u7efc\u5408\u4e0a\u4e0b\u6587\uff0c\u5c06\u95ee\u9898\u4e0ePR\u914d\u5bf9\u4ee5\u83b7\u53d6\u6587\u672c\u4e0a\u4e0b\u6587\uff0c\u5e76\u63d0\u53d6\u5b8c\u6574\u51fd\u6570\u6216\u7c7b\u4f5c\u4e3a\u4ee3\u7801\u4e0a\u4e0b\u6587\uff1b(3) \u91c7\u7528\u57fa\u4e8e\u89c4\u5219\u548cLLM\u7684\u591a\u9636\u6bb5\u8fc7\u6ee4\uff0c\u4fdd\u755967,910\u4e2a\u9ad8\u8d28\u91cf\u3001\u5bcc\u542b\u4e0a\u4e0b\u6587\u7684\u6837\u672c\u3002\u8be5\u57fa\u51c6\u652f\u6301hunk\u7ea7\u8d28\u91cf\u8bc4\u4f30\u3001\u884c\u7ea7\u7f3a\u9677\u5b9a\u4f4d\u548c\u884c\u7ea7\u8bc4\u8bba\u751f\u6210\u4e09\u79cd\u8bc4\u4f30\u573a\u666f\u3002", "result": "\u5728\u516b\u4e2a\u4e3b\u6d41LLM\u4e0a\u7684\u8bc4\u4f30\u8868\u660e\uff0c\u6587\u672c\u4e0a\u4e0b\u6587\u6bd4\u4ec5\u4f7f\u7528\u4ee3\u7801\u4e0a\u4e0b\u6587\u66f4\u80fd\u63d0\u5347\u6a21\u578b\u6027\u80fd\uff0c\u4f46\u5f53\u524dLLM\u8ddd\u79bb\u4eba\u7c7b\u6c34\u5e73\u4ecd\u6709\u8f83\u5927\u5dee\u8ddd\u3002\u8be5\u57fa\u51c6\u5df2\u5728\u5b57\u8282\u8df3\u52a8\u90e8\u7f72\uff0c\u9a71\u52a8\u81ea\u8fdb\u5316\u4ee3\u7801\u5ba1\u67e5\u7cfb\u7edf\uff0c\u6027\u80fd\u63d0\u534761.98%\u3002", "conclusion": "ContextCRBench\u662f\u4e00\u4e2a\u9ad8\u8d28\u91cf\u3001\u7ec6\u7c92\u5ea6\u4e14\u5bcc\u542b\u4e0a\u4e0b\u6587\u7684\u4ee3\u7801\u5ba1\u67e5\u57fa\u51c6\uff0c\u6709\u6548\u5f25\u8865\u4e86\u73b0\u6709\u57fa\u51c6\u7684\u4e0d\u8db3\uff0c\u5728\u5b66\u672f\u8bc4\u4f30\u548c\u5de5\u4e1a\u5e94\u7528\u4e2d\u5747\u5c55\u73b0\u51fa\u663e\u8457\u4ef7\u503c\u548c\u5b9e\u7528\u6027\u3002"}}
