<div id=toc></div>

# Table of Contents

- [cs.AR](#cs.AR) [Total: 2]
- [cs.DC](#cs.DC) [Total: 2]
- [cs.SE](#cs.SE) [Total: 11]
- [cs.NI](#cs.NI) [Total: 1]
- [cs.MA](#cs.MA) [Total: 1]


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [1] [DISCA: A Digital In-memory Stochastic Computing Architecture Using A Compressed Bent-Pyramid Format](https://arxiv.org/abs/2511.17265)
*Shady Agwa,Yikang Shen,Shiwei Wang,Themis Prodromakis*

Main category: cs.AR

TL;DR: 本文提出了一种新型数字存内随机计算架构DISCA，通过压缩的准随机Bent-Pyramid数据格式，在保持数字系统可扩展性与可靠性的同时，显著提升边缘AI中矩阵运算的能效。


<details>
  <summary>Details</summary>
Motivation: 传统冯·诺依曼架构受限于“内存墙”和摩尔定律终结，难以满足边缘AI应用（如机器人、无人机）对高效能、低功耗硬件的需求；现有存内计算方案（模拟或数字）因设计局限未能充分发挥潜力。

Method: 提出DISCA架构，采用压缩版准随机Bent-Pyramid数据格式，结合数字存内随机计算，在180nm CMOS工艺下进行后布局建模验证。

Result: 在500 MHz频率下，DISCA实现每比特3.59 TOPS/W的能效，相比同类架构在矩阵乘法任务上能效提升数个数量级。

Conclusion: DISCA兼具模拟计算的简洁性和数字系统的可扩展性与可靠性，是面向边缘AI高效能硬件的有力解决方案。

Abstract: Nowadays, we are witnessing an Artificial Intelligence revolution that dominates the technology landscape in various application domains, such as healthcare, robotics, automotive, security, and defense. Massive-scale AI models, which mimic the human brain's functionality, typically feature millions and even billions of parameters through data-intensive matrix multiplication tasks. While conventional Von-Neumann architectures struggle with the memory wall and the end of Moore's Law, these AI applications are migrating rapidly towards the edge, such as in robotics and unmanned aerial vehicles for surveillance, thereby adding more constraints to the hardware budget of AI architectures at the edge. Although in-memory computing has been proposed as a promising solution for the memory wall, both analog and digital in-memory computing architectures suffer from substantial degradation of the proposed benefits due to various design limitations. We propose a new digital in-memory stochastic computing architecture, DISCA, utilizing a compressed version of the quasi-stochastic Bent-Pyramid data format. DISCA inherits the same computational simplicity of analog computing, while preserving the same scalability, productivity, and reliability of digital systems. Post-layout modeling results of DISCA show an energy efficiency of 3.59 TOPS/W per bit at 500 MHz using a commercial 180nm CMOS technology. Therefore, DISCA significantly improves the energy efficiency for matrix multiplication workloads by orders of magnitude if scaled and compared to its counterpart architectures.

</details>


### [2] [NX-CGRA: A Programmable Hardware Accelerator for Core Transformer Algorithms on Edge Devices](https://arxiv.org/abs/2511.17235)
*Rohit Prasad*

Main category: cs.AR

TL;DR: 本文提出了NX-CGRA，一种面向边缘设备的可编程硬件加速器，利用粗粒度可重构阵列（CGRA）架构高效支持多种Transformer推理算法，在性能、能效和灵活性之间取得良好平衡。


<details>
  <summary>Details</summary>
Motivation: 边缘端Transformer工作负载日益多样化和复杂化，对性能、能效和架构灵活性提出了更高要求，现有固定功能加速器难以适应广泛的应用场景。

Method: 采用软件驱动的CGRA架构，通过可重构硬件支持线性和非线性Transformer推理操作，实现对不同算子模式的高效执行。

Result: 在真实Transformer模型衍生的基准测试中，NX-CGRA展现出高整体效率和优越的能效-面积权衡表现。

Conclusion: NX-CGRA是一种可扩展且适应性强的硬件解决方案，适用于资源受限的边缘设备部署Transformer模型。

Abstract: The increasing diversity and complexity of transformer workloads at the edge present significant challenges in balancing performance, energy efficiency, and architectural flexibility. This paper introduces NX-CGRA, a programmable hardware accelerator designed to support a range of transformer inference algorithms, including both linear and non-linear functions. Unlike fixed-function accelerators optimized for narrow use cases, NX-CGRA employs a coarse-grained reconfigurable array (CGRA) architecture with software-driven programmability, enabling efficient execution across varied kernel patterns. The architecture is evaluated using representative benchmarks derived from real-world transformer models, demonstrating high overall efficiency and favorable energy-area tradeoffs across different classes of operations. These results indicate the potential of NX-CGRA as a scalable and adaptable hardware solution for edge transformer deployment under constrained power and silicon budgets.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [3] [MicroMoE: Fine-Grained Load Balancing for Mixture-of-Experts with Token Scheduling](https://arxiv.org/abs/2511.16947)
*Chenqi Zhao,Wenfei Wu,Linhai Song,Yuchen Xu*

Main category: cs.DC

TL;DR: 本文提出了MicroEP并行策略和基于其的MicroMoE系统，通过高效的跨GPU令牌调度，在每个微批次中实现细粒度负载均衡，显著提升MoE模型训练吞吐量。


<details>
  <summary>Details</summary>
Motivation: Mixture-of-Experts（MoE）模型虽能降低计算资源消耗，但其动态特性导致专家间负载不均衡，严重影响训练效率；现有方法或牺牲精度，或引入额外开销，难以实现细粒度负载均衡。

Method: 提出MicroEP并行策略，通过在GPU之间高效调度token，实现在每个微批次中的最优负载均衡；并在此基础上构建了分布式MoE训练系统MicroMoE。

Result: 实验表明，MicroMoE相比当前最先进的系统，端到端训练吞吐量最高提升47.6%，并在GPU间几乎始终实现最优负载均衡。

Conclusion: MicroEP策略和MicroMoE系统有效解决了MoE训练中的细粒度负载均衡问题，在不牺牲模型精度的前提下显著提升了训练效率。

Abstract: Mixture-of-Experts (MoE) has emerged as a promising approach to scale up deep learning models due to its significant reduction in computational resources. However, the dynamic nature of MoE leads to load imbalance among experts, severely impacting training efficiency. While previous research has attempted to address the load balancing challenge, existing solutions either compromise model accuracy or introduce additional system overhead. As a result, they fail to achieve fine-grained load balancing, which is crucial to optimizing training efficiency.
  We propose MicroEP, a novel parallelization strategy to achieve fine-grained load balancing in MoE systems. MicroEP is capable of achieving optimal load balancing in every micro-batch through efficient token scheduling across GPUs. Furthermore, we propose MicroMoE, an efficient distributed MoE training system with MicroEP's load balancing capabilities. Our experimental results demonstrate that MicroMoE improves the end-to-end training throughput by up to 47.6% compared with the state-of-the-art system, and almost consistently achieves optimal load balance among GPUs.

</details>


### [4] [Modeling Anomaly Detection in Cloud Services: Analysis of the Properties that Impact Latency and Resource Consumption](https://arxiv.org/abs/2511.17119)
*Gabriel Job Antunes Grabher,Fumio Machida,Thomas Ropars*

Main category: cs.DC

TL;DR: 本文研究了云服务中性能异常检测器的关键特性（如精确率、召回率和检测频率）如何影响性能与成本之间的权衡，并发现高频检测下高精确率已足够，而低频检测时高召回率更为关键。


<details>
  <summary>Details</summary>
Motivation: 在云服务中，异常检测虽有助于维持性能目标，但其误判会导致不必要的资源开销。因此，有必要明确哪些检测器特性对优化性能与成本的权衡最为关键。

Method: 作者使用随机奖励网（Stochastic Reward Nets）对由性能异常检测器监控的云服务进行建模，并分析检测器的精确率、召回率和检测频率对服务平均延迟和资源消耗的影响。

Result: 研究表明，并非总是需要同时实现高精确率和高召回率：若检测频率高，仅高精确率即可获得良好的性能-成本平衡；若检测频率低，则高召回率更为重要。

Conclusion: 性能异常检测器的设计应根据其运行频率权衡精确率与召回率，以实现最优的性能与资源成本平衡。

Abstract: Detecting and resolving performance anomalies in Cloud services is crucial for maintaining desired performance objectives. Scaling actions triggered by an anomaly detector help achieve target latency at the cost of extra resource consumption. However, performance anomaly detectors make mistakes. This paper studies which characteristics of performance anomaly detection are important to optimize the trade-off between performance and cost. Using Stochastic Reward Nets, we model a Cloud service monitored by a performance anomaly detector. Using our model, we study the impact of detector characteristics, namely precision, recall and inspection frequency, on the average latency and resource consumption of the monitored service. Our results show that achieving a high precision and a high recall is not always necessary. If detection can be run frequently, a high precision is enough to obtain a good performance-to-cost trade-off, but if the detector is run infrequently, recall becomes the most important.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [5] [Multi-Agent Code Verification with Compound Vulnerability Detection](https://arxiv.org/abs/2511.16708)
*Shreshth Rajan*

Main category: cs.SE

TL;DR: 本文提出CodeX-Verify，一种多智能体系统，通过四个专门的智能体协同检测代码中的各类缺陷，在不依赖测试执行的情况下以更快速度达到与现有最佳方法相当的查全率（76.1%），并揭示多漏洞叠加带来的风险远超传统模型预测。


<details>
  <summary>Details</summary>
Motivation: 大语言模型生成的代码存在显著缺陷：SWE-bench中29.6%的“已解决”补丁实际失败，BaxBench中62%的解决方案包含漏洞，而现有工具仅能捕获65%的缺陷且误报率达35%。因此亟需更高效、准确的自动验证机制。

Method: 构建名为CodeX-Verify的多智能体系统，包含四个专精于不同类型缺陷检测的智能体；通过数学证明和实证分析（智能体间相关性p=0.05–0.25）验证多智能体组合的有效性；在99个带标签样本和300个真实补丁上评估性能。

Result: 系统在99个样本上检出76.1%的缺陷，优于单一智能体（最高提升39.7个百分点），最佳双智能体组合达79.3%准确率；处理Claude Sonnet 4.5生成的补丁平均耗时低于200ms；发现多漏洞（如SQL注入+凭证泄露）叠加风险达传统模型预测的15倍。

Conclusion: 多智能体协同验证能显著提升LLM生成代码的缺陷检测效果，兼具高准确率、低延迟和无需执行测试的优势，适用于生产环境，并揭示了复合漏洞风险被严重低估的问题。

Abstract: LLMs generate buggy code: 29.6% of SWE-bench "solved" patches fail, 62% of BaxBench solutions have vulnerabilities, and existing tools only catch 65% of bugs with 35% false positives. We built CodeX-Verify, a multi-agent system that uses four specialized agents to detect different types of bugs. We prove mathematically that combining agents with different detection patterns finds more bugs than any single agent when the agents look for different problems, confirmed by measuring agent correlation of p = 0.05--0.25. We also show that multiple vulnerabilities in the same code create exponentially more risk than previously thought--SQL injection plus exposed credentials creates 15x more danger (risk 300 vs. 20) than traditional models predict. Testing on 99 code samples with verified labels shows our system catches 76.1% of bugs, matching the best existing method while running faster and without test execution. We tested 15 different agent combinations and found that using multiple agents improves accuracy by 39.7 percentage points (from 32.8% to 72.4%) compared to single agents, with gains of +14.9pp, +13.5pp, and +11.2pp for agents 2, 3, and 4. The best two-agent combination reaches 79.3% accuracy. Testing on 300 real patches from Claude Sonnet 4.5 runs in under 200ms per sample, making this practical for production use.

</details>


### [6] [Is the Cure Still Worse Than the Disease? Test Overfitting by LLMs in Automated Program Repair](https://arxiv.org/abs/2511.16858)
*Toufique Ahmed,Jatin Ganhotra,Avraham Shinnar,Martin Hirzel*

Main category: cs.SE

TL;DR: 本文研究了在当前大型语言模型时代，自动化程序修复中测试过拟合问题是否仍然存在，使用SWE-bench仓库级任务进行实验分析。


<details>
  <summary>Details</summary>
Motivation: 以往研究表明，自动化程序修复容易产生仅通过已知测试但无法通过隐藏测试的修复代码（即测试过拟合问题）。随着大语言模型的发展，有必要重新评估该问题是否依然显著。

Method: 通过在SWE-bench的仓库级任务上进行实验，评估当前自动化程序修复方法在测试过拟合方面的表现。

Result: 实验结果表明，测试过拟合问题在当前的自动化程序修复实践中仍然普遍存在。

Conclusion: 尽管大语言模型带来了新的修复能力，测试过拟合仍是自动化程序修复中的关键挑战，需进一步研究更鲁棒的评估与修复策略。

Abstract: Automated program repair has been shown to be susceptible to generating repaired code that passes on seen tests but fails on a hold-out set of hidden tests. This problem, dubbed test overfitting, has been identified and studied before the rise of large language models. We experimentally study how much test overfitting is still a problem today, using repository-level SWE-bench tasks.

</details>


### [7] [MOOT: a Repository of Many Multi-Objective Optimization Tasks](https://arxiv.org/abs/2511.16882)
*Tim Menzies,Tao Chen,Yulong Ye,Kishan Kumar Ganguly,Amirali Rayegan,Srinath Srinivasan,Andre Lustosa*

Main category: cs.SE

TL;DR: 本文介绍了MOOT，一个包含120多个来自软件工程研究的多目标优化任务的开源仓库，旨在支持对软件工程中多目标权衡问题的研究。


<details>
  <summary>Details</summary>
Motivation: 软件工程师常需在相互冲突的目标之间做权衡（如速度与成本、安全性与可用性等），但现有研究和工业实践缺乏有效工具来探索这些权衡，限制了最优决策的实现。

Method: 构建并发布MOOT仓库，整合来自近期软件工程论文的多目标优化任务，涵盖软件配置、云调优、项目健康等多个领域，并以MIT许可证开放获取。

Result: MOOT提供了丰富的数据集，支持数十个新颖的研究问题，并鼓励社区贡献以持续扩展任务库。

Conclusion: MOOT为软件工程中的多目标优化研究提供了重要基础设施，有望推动该领域的进一步发展。

Abstract: Software engineers must make decisions that trade off competing goals (faster vs. cheaper, secure vs. usable, accurate vs. interpretable, etc.). Despite MSR's proven techniques for exploring such goals, researchers still struggle with these trade-offs. Similarly, industrial practitioners deliver sub-optimal products since they lack the tools needed to explore these trade-offs.
  To enable more research in this important area, we introduce MOOT, a repository of multi-objective optimization tasks taken from recent SE research papers. MOOT's tasks cover software configuration, cloud tuning, project health, process modeling, hyperparameter optimization, and more. Located at github.com/timm/moot, MOOT's current 120+ tasks are freely available under an MIT license (and we invite community contributions). As shown here, this data enables dozens of novel research questions.

</details>


### [8] [ReVul-CoT: Towards Effective Software Vulnerability Assessment with Retrieval-Augmented Generation and Chain-of-Thought Prompting](https://arxiv.org/abs/2511.17027)
*Zhijie Chen,Xiang Chen,Ziming Li,Jiacheng Xue,Chaoyang Gao*

Main category: cs.SE

TL;DR: 本文提出了一种结合检索增强生成（RAG）与思维链（CoT）提示的新框架ReVul-CoT，用于提升大语言模型在软件漏洞评估（SVA）中的表现。通过从本地知识库动态检索权威漏洞信息并引导模型进行逐步推理，ReVul-CoT在多个指标上显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型在软件漏洞评估中存在两个主要局限：缺乏领域特定知识，以及依赖浅层模式匹配而非深层上下文推理，难以准确理解复杂代码语义及其安全影响。

Method: 提出ReVul-CoT框架，结合RAG与CoT提示。RAG模块从整合了NVD、CWE等权威来源的本地知识库中动态检索相关漏洞信息和代码片段；基于DeepSeek-V3.1模型，利用CoT提示引导模型对可利用性、影响范围等因素进行分步推理。

Result: 在包含12,070个漏洞的数据集上评估，ReVul-CoT在MCC指标上比现有最优基线提升16.50%–42.26%，在Accuracy、F1-score和MCC上分别提升10.43%、15.86%和16.50%。消融实验验证了动态检索、知识融合与CoT推理的有效性。

Conclusion: 将RAG与CoT提示相结合能显著提升大语言模型在软件漏洞评估中的性能，为未来研究提供了有前景的方向。

Abstract: Context: Software Vulnerability Assessment (SVA) plays a vital role in evaluating and ranking vulnerabilities in software systems to ensure their security and reliability. Objective: Although Large Language Models (LLMs) have recently shown remarkable potential in SVA, they still face two major limitations. First, most LLMs are trained on general-purpose corpora and thus lack domain-specific knowledge essential for effective SVA. Second, they tend to rely on shallow pattern matching instead of deep contextual reasoning, making it challenging to fully comprehend complex code semantics and their security implications. Method: To alleviate these limitations, we propose a novel framework ReVul-CoT that integrates Retrieval-Augmented Generation (RAG) with Chain-of-Thought (COT) prompting. In ReVul-CoT, the RAG module dynamically retrieves contextually relevant information from a constructed local knowledge base that consolidates vulnerability data from authoritative sources (such as NVD and CWE), along with corresponding code snippets and descriptive information. Building on DeepSeek-V3.1, CoT prompting guides the LLM to perform step-by-step reasoning over exploitability, impact scope, and related factors Results: We evaluate ReVul-CoT on a dataset of 12,070 vulnerabilities. Experimental results show that ReVul-CoT outperforms state-of-the-art SVA baselines by 16.50%-42.26% in terms of MCC, and outperforms the best baseline by 10.43%, 15.86%, and 16.50% in Accuracy, F1-score, and MCC, respectively. Our ablation studies further validate the contributions of considering dynamic retrieval, knowledge integration, and CoT-based reasoning. Conclusion: Our results demonstrate that combining RAG with CoT prompting significantly enhances LLM-based SVA and points out promising directions for future research.

</details>


### [9] [UI-CUBE: Enterprise-Grade Computer Use Agent Benchmarking Beyond Task Accuracy to Operational Reliability](https://arxiv.org/abs/2511.17131)
*Horia Cristescu,Charles Park,Trong Canh Nguyen,Sergiu Talmacel,Alexandru-Gabriel Ilie,Stefan Adam*

Main category: cs.SE

TL;DR: UI-CUBE is a new benchmark with 226 tasks that reveals current Computer Use Agents (CUAs) perform well on simple UI tasks but fail dramatically on complex enterprise workflows, exposing fundamental architectural limitations in memory, planning, and state coordination.


<details>
  <summary>Details</summary>
Motivation: Existing CUA benchmarks focus on task completion and functional correctness but neglect operational reliability needed for real-world enterprise deployment; there's a need for a more rigorous evaluation of CUAs' readiness in production environments.

Method: The authors introduce UI-CUBE, a systematic benchmark with 226 tasks across two tiers: simple UI interactions (136 tasks), and complex workflows including copy-paste (50) and enterprise scenarios (40). It features interface variation, multi-resolution testing, and automated validation via application state. Five state-of-the-art CUAs and human evaluators are tested.

Result: State-of-the-art CUAs achieve 67–85% success on simple tasks (vs. 97.9% human performance) but only 9–19% on complex workflows. Humans without prior experience score 61.2% on complex tasks, revealing a sharp performance cliff. CUAs reach 68–87% of human performance on simple tasks but only 15–32% on complex ones.

Conclusion: Current CUAs cannot reliably automate complex, multi-step enterprise workflows due to core architectural flaws—not just training or prompting issues. UI-CUBE serves as a diagnostic tool for enterprise-readiness and guides future development toward production-capable agents.

Abstract: While current Computer Use Agent (CUA) benchmarks measure task completion effectively, they provide limited assessment of enterprise deployment readiness, emphasizing functional correctness over the operational reliability required for production systems. We present UI-CUBE (UiPath Computer Use BEnchmark), a systematic benchmark comprising 226 tasks across two difficulty tiers designed to expose fundamental architectural limitations in current CUAs. Our evaluation covers simple UI interactions (136 tasks) and complex workflows including copy-paste tasks (50 tasks) and enterprise application scenarios (40 tasks), with systematic interface variation coverage, multi-resolution testing and automated validation of task success through the application state. Evaluation of five state-of-the-art models reveals a sharp capability cliff rather than gradual performance degradation. Simple UI interactions achieve 67-85% success rates (compared to 97.9% human performance), but complex workflows drop precipitously to 9-19%. Human evaluators with no prior application experience achieve only 61.2% on complex tasks despite near-perfect performance on simple tasks, establishing realistic performance ceilings. This discontinuous performance pattern -- where agents achieve 68-87% of human performance on simple tasks but only 15-32% on complex workflows -- indicates fundamental architectural limitations in memory management, hierarchical planning, and state coordination rather than incremental capability gaps addressable through better training or prompting. UI-CUBE functions as an enterprise-readiness diagnostic, revealing that while current CUAs can manipulate individual interface elements, they cannot yet function as reliable workflow automation tools. These findings provide architectural insights essential for developing production-ready CUAs capable of managing complex, multi-step enterprise processes.

</details>


### [10] [SlsReuse: LLM-Powered Serverless Function Reuse](https://arxiv.org/abs/2511.17262)
*Jinfeng Wen,Yuehan Sun*

Main category: cs.SE

TL;DR: 本文提出了SlsReuse，首个基于大语言模型（LLM）的无服务器函数复用框架，通过构建可复用函数库、利用提示工程学习异构函数的统一语义表示，并结合意图感知发现与多级剪枝策略，在任务查询中显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 无服务器计算虽降低了运维开销，但对新手开发者而言，从零开发函数需适应多样化的平台特定编程风格，过程耗时且易错。现有函数推荐方法因任务描述与异构函数实现之间存在语义鸿沟而效果有限。

Method: SlsReuse首先构建可复用函数仓库作为知识基础；其次通过少样本提示工程学习异构函数的统一语义增强表示，涵盖代码意图、目标平台、编程语言和云服务；最后针对自然语言任务查询，采用意图感知发现、多级剪枝和相似度匹配进行函数推荐。

Result: 在包含110个任务查询的数据集上评估，基于ChatGPT-4o的SlsReuse实现了91.20%的Recall@10，比当前最优基线高出24.53个百分点。

Conclusion: SlsReuse有效弥合了任务需求与无服务器函数实现之间的语义鸿沟，显著提升了函数推荐的准确性和实用性，为无服务器开发提供了高效复用的新范式。

Abstract: Serverless computing has rapidly emerged as a popular cloud computing paradigm. It enables developers to implement function-level tasks, i.e., serverless functions, without managing infrastructure. While reducing operational overhead, it poses challenges, especially for novice developers. Developing functions from scratch requires adapting to heterogeneous, platform-specific programming styles, making the process time-consuming and error-prone. Function reuse offers a promising solution to address these challenges. However, research on serverless computing lacks a dedicated approach for function recommendation. Existing techniques from traditional contexts remain insufficient due to the semantic gap between task descriptions and heterogeneous function implementations. Advances in large language models (LLMs), pre-trained on large-scale corpora, create opportunities to bridge this gap by aligning developer requirements with function semantics.
  This paper presents SlsReuse, the first LLM-powered framework for serverless function reuse. Specifically, SlsReuse first constructs a reusable function repository serving as a foundational knowledge base. Then, it learns unified semantic-enhanced representations of heterogeneous functions through effective prompt engineering with few-shot prompting, capturing implicit code intent, target platforms, programming languages, and cloud services. Finally, given a natural language task query, SlsReuse performs intent-aware discovery combined with a multi-level pruning strategy and similarity matching. We evaluate SlsReuse on a curated dataset of 110 task queries. Built on ChatGPT-4o, one of the most representative LLMs, SlsReuse achieves Recall@10 of 91.20%, exceeding the state-of-the-art baseline by 24.53 percentage points.

</details>


### [11] [Detecting Performance-Relevant Changes in Configurable Software Systems](https://arxiv.org/abs/2511.17271)
*Sebastian Böhm,Florian Sattler,Norbert Siegmund,Sven Apel*

Main category: cs.SE

TL;DR: ConfFLARE 是一种通过识别与性能相关代码的数据流交互来检测性能回归并选择关键配置进行测试的新方法，可显著减少性能测试所需配置数量，平均节省70%以上的测试时间。


<details>
  <summary>Details</summary>
Motivation: 软件系统的性能具有易变性，频繁的性能分析成本高昂，尤其在可配置系统中需测试大量配置；传统配置采样方法无法保证完整性，可能遗漏仅影响少数配置的性能退化。

Method: ConfFLARE 通过分析代码变更与性能相关代码之间的数据流交互，识别参与这些交互的软件特性，并据此选择相关配置子集进行性能剖析。

Result: 在合成和真实软件系统上的实验表明，ConfFLARE 几乎能正确检测所有性能回归，在除两个案例外的所有情况下都能准确识别相关特性，平均减少79%（合成）和70%（真实）的测试配置，大幅节省测试时间。

Conclusion: ConfFLARE 能有效降低性能测试成本，同时保持对性能回归的高检测率，是一种优于传统配置采样方法的实用解决方案。

Abstract: Performance is a volatile property of a software system and frequent performance profiling is required to keep the knowledge about a software system's performance behavior up to date. Repeating all performance measurements after every revision is a cost-intensive task, especially in the presence of configurability, where one has to measure multiple configurations to obtain a comprehensive picture. Configuration sampling is a common approach to control the measurement cost. However, it cannot guarantee completeness and might miss performance regressions, especially if they only affect few configurations. As an alternative to solve the cost reduction problem, we present ConfFLARE: ConfFLARE estimates whether a change potentially impacts performance by identifying data-flow interactions with performance-relevant code and extracts which software features participate in such interactions. Based on these features, we can select a subset of relevant configurations to focus performance profiling efforts on. In a study conducted on both, synthetic and real-world software systems, ConfFLARE correctly detects performance regressions in almost all cases and identifies relevant features in all but two cases, reducing the number of configurations to be tested on average by $79\%$ for synthetic and by $70\%$ for real-world regression scenarios saving hours of performance testing time.

</details>


### [12] [Framework Matters: Energy Efficiency of UI Automation Testing Frameworks](https://arxiv.org/abs/2511.17303)
*Timmie M. R. Lagermann,Kristina Sophia Carter,Su Mei Gwen Ho,Luís Cruz,Kerstin Eder,Maja H. Kirkeby*

Main category: cs.SE

TL;DR: 该研究评估了四个Web UI自动化测试框架在执行常见UI操作时的能耗表现，发现不同框架在不同操作下的能效差异显著，最高可达六倍，表明开发者可根据具体操作选择更节能的框架。


<details>
  <summary>Details</summary>
Motivation: 为指导开发人员进行节能的UI自动化测试设计，需了解不同测试框架在执行各类UI操作时的能耗特性。

Method: 在受控的客户端-服务器环境中，使用外部功率计对四种UI自动化测试框架（Puppeteer、Selenium、Nightwatch等）重复执行七种常见UI操作（如点击、滚动、输入文本等）各35次，测量并比较每次操作的能耗。

Result: Puppeteer在多数操作（左键/右键/双击、复选框、文本输入）中最节能；Selenium在刷新和滚动操作中表现最佳；Nightwatch整体能效最差；相同操作在不同框架间能耗差异最高达六倍。

Conclusion: 通过提供UI自动化测试框架的能耗透明度，开发者可针对特定UI操作做出更节能的选择，从而实现能源感知的测试设计。

Abstract: We examine per action energy consumption across four web user interface (UI) automation testing frameworks to determine whether consistent tendencies can guide energy-aware test design. Using a controlled client-server setup with external power metering, we repeat each UI action (refresh, click variants, checkbox, drag&drop, input-text, scroll) 35 times. Across each of the actions, energy costs vary by both framework and action. Puppeteer is the most efficient for left-click, right-click, double-click, checkbox, and input-text; Selenium is the most efficient for refresh and scroll; Nightwatch is generally the least energy efficient. The energy cost of performing the same action varied by up to a factor of six depending on the framework. This indicates that providing transparency of energy consumption for UI automation testing frameworks allows developers to make informed, energy-aware decisions when testing a specific UI action.

</details>


### [13] [Agentic Program Verification](https://arxiv.org/abs/2511.17330)
*Haoxin Tu,Huan Zhao,Yahui Song,Mehtab Zafar,Ruijie Meng,Abhik Roychoudhury*

Main category: cs.SE

TL;DR: 本文提出了首个用于程序验证的大语言模型（LLM）智能体 AutoRocq，它通过与 Rocq 定理证明器的迭代交互实现无需大量训练的自动证明生成，并在 SV-COMP 和 Linux 内核模块上验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型自动生成代码的普及，亟需一种能对生成代码进行自动验证的方法。虽然通用数学推理可用于程序推理，但程序推理具有更强的结构性和上下文丰富性，因此需要专门设计的 AI 推理机制。

Method: 提出 AutoRocq 智能体，该智能体不依赖于大量证明样本的训练，而是通过与 Rocq（原 Coq）定理证明器的迭代交互，在运行中学习并不断优化证明过程，最终生成经 Rocq 验证的证明推导。

Result: 在 SV-COMP 基准测试和 Linux 内核模块上的实验表明，AutoRocq 在自动化程序验证方面表现出良好效果。

Conclusion: AutoRocq 展示了将 LLM 智能体与定理证明器结合用于程序验证的可行性，未来可与 AI 编码智能体集成，形成“生成—验证”闭环，推动可信自动化编程的发展。

Abstract: Automatically generated code is gaining traction recently, owing to the prevalence of Large Language Models (LLMs). Further, the AlphaProof initiative has demonstrated the possibility of using AI for general mathematical reasoning. Reasoning about computer programs (software) can be accomplished via general mathematical reasoning; however, it tends to be more structured and richer in contexts. This forms an attractive proposition, since then AI agents can be used to reason about voluminous code that gets generated by AI.
  In this work, we present a first LLM agent, AutoRocq, for conducting program verification. Unlike past works, which rely on extensive training of LLMs on proof examples, our agent learns on-the-fly and improves the proof via an iterative refinement loop. The iterative improvement of the proof is achieved by the proof agent communicating with the Rocq (formerly Coq) theorem prover to get additional context and feedback. The final result of the iteration is a proof derivation checked by the Rocq theorem prover. In this way, our proof construction involves autonomous collaboration between the proof agent and the theorem prover. This autonomy facilitates the search for proofs and decision-making in deciding on the structure of the proof tree.
  Experimental evaluation on SV-COMP benchmarks and on Linux kernel modules shows promising efficacy in achieving automated program verification. As automation in code generation becomes more widespread, we posit that our proof agent can be potentially integrated with AI coding agents to achieve a generate and validate loop, thus moving closer to the vision of trusted automatic programming.

</details>


### [14] [Exploring Scientific Debt: Harnessing AI for SATD Identification in Scientific Software](https://arxiv.org/abs/2511.17368)
*Eric L. Melin,Ahmed Musa Awon,Nasir U. Eisty,Neil A. Ernst,Shurui Zhou*

Main category: cs.SE

TL;DR: 该研究发现科学软件（SSW）中的自认技术债（SATD）远高于通用软件，并验证了基于Transformer的模型在识别SATD方面的优越性能，强调了管理SATD对保障科研结果可靠性的重要性。


<details>
  <summary>Details</summary>
Motivation: 科学软件中广泛存在自认技术债（SATD），可能威胁科研结果的准确性与可复现性，但SATD与科学软件之间的关系尚未被充分研究，亟需深入理解其特性及影响。

Method: 分析27个涵盖多领域和编程语言的科学与通用开源仓库中的代码注释；在67,066条标注数据上微调并比较10种参数规模从1亿到70亿的Transformer模型，以识别SATD。

Result: 科学软件中的科学债务是通用软件的9.25倍，SATD总量是4.93倍；所提出的最佳模型在SATD识别任务上优于现有方法。

Conclusion: 科学软件中的SATD具有独特性和高发性，对软件质量和科研可信度构成显著影响；研究结果有助于开发者和科研人员制定更有效的技术债管理策略，维护科学发现的完整性。

Abstract: Developers often leave behind clues in their code, admitting where it falls short, known as Self-Admitted Technical Debt (SATD). In the world of Scientific Software (SSW), where innovation moves fast and collaboration is key, such debt is not just common but deeply impactful. As research relies on accurate and reproducible results, accumulating SATD can threaten the very foundations of scientific discovery. Yet, despite its significance, the relationship between SATD and SSW remains largely unexplored, leaving a crucial gap in understanding how to manage SATD in this critical domain. This study explores SATD in SSW repositories, comparing SATD in scientific versus general-purpose open-source software and evaluating transformer-based models for SATD identification. We analyzed SATD in 27 scientific and general-purpose repositories across multiple domains and languages. We fine-tuned and compared 10 transformer-based models (100M-7B parameters) on 67,066 labeled code comments. SSW contains 9.25x more Scientific Debt and 4.93x more SATD than general-purpose software due to complex computations, domain constraints, and evolving research needs. Furthermore, our best model outperforms existing ones. This study uncovers how SATD in SSW differs from general software, revealing its impact on quality and scientific validity. By recognizing these challenges, developers and researchers can adopt smarter strategies to manage debt and safeguard the integrity of scientific discovery.

</details>


### [15] [CREST: Improving Interpretability and Effectiveness of Troubleshooting at Ericsson through Criterion-Specific Trouble Report Retrieval](https://arxiv.org/abs/2511.17417)
*Soroush Javdan,Pragash Krishnamoorthy,Olga Baysal*

Main category: cs.SE

TL;DR: 本文提出CREST方法，通过为故障报告（TR）中的不同观察标准训练专用检索模型，并集成其输出，从而提升检索效果、校准能力和可解释性。


<details>
  <summary>Details</summary>
Motivation: 电信行业快速发展要求高效的故障排查流程；现有TR数据复杂且量大，单一检索模型难以有效处理多维标准，影响故障定位效率。

Method: 提出CREST框架，采用针对不同TR字段（如故障现象、日志等）的专用检索模型，在初始检索和重排序两阶段流程中分别建模，并融合各模型输出以获得综合相关性评分。

Result: 在爱立信内部TR数据子集上的实验表明，CREST在关键评估指标上显著优于单一模型方法，验证了多标准建模的有效性。

Conclusion: 针对不同TR观察标准使用专用模型能显著提升检索性能与可解释性，有助于加快软件故障排查和维护。

Abstract: The rapid evolution of the telecommunication industry necessitates efficient troubleshooting processes to maintain network reliability, software maintainability, and service quality. Trouble Reports (TRs), which document issues in Ericsson's production system, play a critical role in facilitating the timely resolution of software faults. However, the complexity and volume of TR data, along with the presence of diverse criteria that reflect different aspects of each fault, present challenges for retrieval systems. Building on prior work at Ericsson, which utilized a two-stage workflow, comprising Initial Retrieval (IR) and Re-Ranking (RR) stages, this study investigates different TR observation criteria and their impact on the performance of retrieval models. We propose \textbf{CREST} (\textbf{C}riteria-specific \textbf{R}etrieval via \textbf{E}nsemble of \textbf{S}pecialized \textbf{T}R models), a criterion-driven retrieval approach that leverages specialized models for different TR fields to improve both effectiveness and interpretability, thereby enabling quicker fault resolution and supporting software maintenance. CREST utilizes specialized models trained on specific TR criteria and aggregates their outputs to capture diverse and complementary signals. This approach leads to enhanced retrieval accuracy, better calibration of predicted scores, and improved interpretability by providing relevance scores for each criterion, helping users understand why specific TRs were retrieved. Using a subset of Ericsson's internal TRs, this research demonstrates that criterion-specific models significantly outperform a single model approach across key evaluation metrics. This highlights the importance of all targeted criteria used in this study for optimizing the performance of retrieval systems.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [16] [One Walk is All You Need: Data-Efficient 3D RF Scene Reconstruction with Human Movements](https://arxiv.org/abs/2511.16966)
*Yiheng Bian,Zechen Li,Lanqing Yang,Hao Pan,Yezhou Wang,Longyuan Ge,Jeffery Wu,Ruiheng Liu,Yongjian Fu,Yichao chen,Guangtao xue*

Main category: cs.NI

TL;DR: 本文提出一种新方法，利用单次短暂的人类行走视频即可高效、高保真地重建被遮挡的静态3D辐射场场景，将人体运动视为有用信号而非噪声，并基于复合3D高斯泼溅实现，显著优于现有需要大量静态采样的SOTA方法。


<details>
  <summary>Details</summary>
Motivation: 传统3D辐射场重建依赖大量静态测量，将人体运动视为需滤除的噪声，导致数据采集过程繁琐低效；本文旨在突破这一瓶颈，探索如何利用非结构化的人体运动作为信息源，实现快速、数据高效的高质量重建。

Method: 设计了一个基于复合3D高斯泼溅（3DGS）的因子分解框架，从原始射频流中联合学习动态人体运动效应与静态场景几何结构，仅需单次60秒随意行走的数据进行训练。

Result: 在仅使用一次60秒人类行走数据的情况下，模型重建静态场景的SSIM达到0.96，比当前最先进的重度采样方法高出12%。

Conclusion: 将人体运动转化为重建信号可有效消除传统数据采集瓶颈，为实时、即兴的未知环境3D射频建图开辟了新路径。

Abstract: Reconstructing 3D Radiance Field (RF) scenes through opaque obstacles is a long-standing goal, yet it is fundamentally constrained by a laborious data acquisition process requiring thousands of static measurements, which treats human motion as noise to be filtered. This work introduces a new paradigm with a core objective: to perform fast, data-efficient, and high-fidelity RF reconstruction of occluded 3D static scenes, using only a single, brief human walk. We argue that this unstructured motion is not noise, but is in fact an information-rich signal available for reconstruction. To achieve this, we design a factorization framework based on composite 3D Gaussian Splatting (3DGS) that learns to model the dynamic effects of human motion from the persistent static scene geometry within a raw RF stream. Trained on just a single 60-second casual walk, our model reconstructs the full static scene with a Structural Similarity Index (SSIM) of 0.96, remarkably outperforming heavily-sampled state-of-the-art (SOTA) by 12%. By transforming the human movements into its valuable signals, our method eliminates the data acquisition bottleneck and paves the way for on-the-fly 3D RF mapping of unseen environments.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [17] [Optimizing PyTorch Inference with LLM-Based Multi-Agent Systems](https://arxiv.org/abs/2511.16964)
*Kirill Nagaitsev,Luka Grbcic,Samuel Williams,Costin Iancu*

Main category: cs.MA

TL;DR: 本文提出了一种用于比较多智能体PyTorch优化系统的逻辑框架，发现以探索为主的策略配合错误修复智能体效果最佳，且优化步骤的粒度与性能相关；最优实现在H100 GPU上于KernelBench基准测试中平均提速2.88倍。


<details>
  <summary>Details</summary>
Motivation: 传统AI推理系统在GPU硬件上的性能优化依赖于手工编写内核或专用编译器，而近期研究表明基于大语言模型（LLM）的多智能体系统可有效完成此类调优任务。然而，这类多智能体系统在此任务中的运行机制尚不明确，亟需系统性分析。

Method: 构建一个逻辑框架用于比较不同多智能体PyTorch优化系统，并通过实验评估不同策略（如偏向利用或探索）与错误修复智能体的组合效果，同时考察优化步骤粒度对性能的影响。

Result: 实验表明，偏向利用（exploit-heavy）的策略在搭配错误修复智能体时表现最佳，且优化性能与优化步骤的粒度密切相关；最优实现方案在H100 GPU上对KernelBench中涵盖多种PyTorch模型架构的任务平均实现了2.88倍的加速。

Conclusion: 多智能体系统在GPU代码优化方面具有显著潜力，其性能受策略类型和优化粒度影响，合理设计智能体协作机制可有效提升AI推理效率，减少对手工调优或专用编译器的依赖。

Abstract: Maximizing performance on available GPU hardware is an ongoing challenge for modern AI inference systems. Traditional approaches include writing custom GPU kernels and using specialized model compilers to tune high-level code for specific GPU targets. Recent work shows that LLM-based multi-agent systems can effectively perform such tuning, often outperforming existing compilers and eliminating the need for manual kernel development. However, the dynamics of multi-agent systems for this task remain unexplored. In this work, we present a logical framework for comparing multi-agent PyTorch optimization systems. Our evaluation shows that exploit-heavy strategies perform best when paired with error-fixing agents, and that performance correlates with the granularity of optimization steps. The best implementation achieves an average 2.88x speedup on an H100 GPU across diverse tasks in KernelBench, a benchmark suite covering a range of machine learning architectures in PyTorch.

</details>
