<div id=toc></div>

# Table of Contents

- [cs.DC](#cs.DC) [Total: 14]
- [cs.OS](#cs.OS) [Total: 1]
- [cs.AR](#cs.AR) [Total: 3]
- [cs.MA](#cs.MA) [Total: 3]
- [cs.SE](#cs.SE) [Total: 8]


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [1] [Optimizations on Graph-Level for Domain Specific Computations in Julia and Application to QED](https://arxiv.org/abs/2511.19456)
*Anton Reinhard,Simeon Ehrig,René Widera,Michael Bussmann,Uwe Hernandez Acosta*

Main category: cs.DC

TL;DR: 本文提出了一种基于Julia语言的软件框架，通过扩展有向无环图（DAG）调度理论并结合领域知识，能够自动动态生成静态调度和编译代码，以优化复杂科学计算任务在异构硬件上的执行效率，并以量子电动力学中多粒子散射矩阵元计算为例进行验证。


<details>
  <summary>Details</summary>
Motivation: 科学计算中的复杂问题通常由多个子任务组成，这些子任务对计算资源的需求差异较大。为实现最优效率，需对每个子任务进行分析，并将其调度到最适合的硬件上执行，同时兼顾并行性、任务依赖性和设备间数据传输速度等因素。

Method: 作者构建了一个用Julia编写的软件框架，该框架在传统DAG调度基础上引入领域特定信息和理论扩展，从而自动生成静态调度且经过编译的高效代码。

Result: 该框架成功应用于量子电动力学中涉及多个外粒子的散射过程矩阵元计算，展示了其在实际复杂科学计算场景中的可行性和优化能力。

Conclusion: 通过将领域知识融入DAG调度理论，所提出的框架能有效提升异构硬件环境下复杂科学计算任务的执行效率，为高性能科学计算提供了新的自动化工具。

Abstract: Complex computational problems in science often consist of smaller parts that can have largely distinct compute requirements from one another. For optimal efficiency, analyzing each subtask and scheduling it on the best-suited hardware would be necessary. Other considerations must be taken into account, too, such as parallelism, dependencies between different subtasks, and data transfer speeds between devices. To achieve this, directed acyclic graphs are often employed to represent these problems and enable utilizing as much hardware as possible on a given machine. In this paper, we present a software framework written in Julia capable of automatically and dynamically producing statically scheduled and compiled code. We lay theoretical foundations and add domain-specific information about the computation to the existing concepts of DAG scheduling, enabling optimizations that would otherwise be impossible. To illustrate the theory we implement an example application: the computation of matrix elements for scattering processes with many external particles in quantum electrodynamics.

</details>


### [2] [Temperature in SLMs: Impact on Incident Categorization in On-Premises Environments](https://arxiv.org/abs/2511.19464)
*Marcio Pohlmann,Alex Severo,Gefté Almeida,Diego Kreutz,Tiago Heinrich,Lourenço Pereira*

Main category: cs.DC

TL;DR: 本文研究本地部署的小型语言模型（SLM）是否可用于安全运营中心（SOC）和计算机安全事件响应团队（CSIRT）的自动化事件分类，评估了21个不同参数规模的模型，发现模型参数数量和GPU性能是关键因素，而温度超参数影响较小。


<details>
  <summary>Details</summary>
Motivation: 由于使用云端大语言模型（LLM）进行事件分类存在成本、延迟和保密性风险，因此探索在本地运行的小型语言模型（SLM）能否有效应对这一挑战。

Method: 评估了21个参数规模从1B到20B不等的语言模型，在两种不同架构下调整温度超参数，并测量执行时间和分类精度。

Result: 实验结果表明，温度对模型性能影响不大，而模型参数数量和GPU容量是决定性能的关键因素。

Conclusion: 本地部署的SLM在具备足够参数规模和GPU资源的情况下，有望替代云LLM用于安全事件的自动化分类，兼顾效率与数据保密性。

Abstract: SOCs and CSIRTs face increasing pressure to automate incident categorization, yet the use of cloud-based LLMs introduces costs, latency, and confidentiality risks. We investigate whether locally executed SLMs can meet this challenge. We evaluated 21 models ranging from 1B to 20B parameters, varying the temperature hyperparameter and measuring execution time and precision across two distinct architectures. The results indicate that temperature has little influence on performance, whereas the number of parameters and GPU capacity are decisive factors.

</details>


### [3] [Asynchronous Cooperative Optimization of a Capacitated Vehicle Routing Problem Solution](https://arxiv.org/abs/2511.19445)
*Luca Accorsi,Demetrio Laganà,Federico Michelotto,Roberto Musmanno,Daniele Vigo*

Main category: cs.DC

TL;DR: 本文提出了一种并行共享内存框架FILO2$^x$，用于高效求解带容量约束的车辆路径问题（CVRP），在几乎无需同步且无需显式分解的情况下，通过多线程并发优化同一解的不同局部区域，显著缩短求解时间，同时保持与原FILO2算法相当的解质量。


<details>
  <summary>Details</summary>
Motivation: 为提升大规模CVRP实例的求解效率，充分利用现代多核计算资源，克服单线程算法在处理超大规模问题时的时间瓶颈。

Method: 基于FILO2算法设计其单轨迹并行版本FILO2$^x$，利用FILO2局部优化的特性，在共享内存架构下允许多个求解器异步、并发地优化同一解的不同区域，实现迭代级并行。

Result: 实验表明，FILO2$^x$相比原始FILO2显著缩短了求解时间，且在客户规模从数百到数十万的实例上均能保持相近的最终解质量。

Conclusion: FILO2$^x$通过轻量级同步和隐式并行策略，有效提升了CVRP求解效率，验证了在不牺牲解质量的前提下利用并行计算加速局部搜索算法的可行性。

Abstract: We propose a parallel shared-memory schema to cooperatively optimize the solution of a Capacitated Vehicle Routing Problem instance with minimal synchronization effort and without the need for an explicit decomposition. To this end, we design FILO2$^x$ as a single-trajectory parallel adaptation of the FILO2 algorithm originally proposed for extremely large-scale instances and described in Accorsi and Vigo (2024). Using the locality of the FILO2 optimization applications, in FILO2$^x$ several possibly unrelated solution areas are concurrently asynchronously optimized. The overall search trajectory emerges as an iteration-based parallelism obtained by the simultaneous optimization of the same underlying solution performed by several solvers. Despite the high efficiency exhibited by the single-threaded FILO2 algorithm, the computational results show that, by better exploiting the available computing resources, FILO2$^x$ can greatly enhance the resolution time compared to the original approach, still maintaining a similar final solution quality for instances ranging from hundreds to hundreds of thousands customers.

</details>


### [4] [AI-driven Predictive Shard Allocation for Scalable Next Generation Blockchains](https://arxiv.org/abs/2511.19450)
*M. Zeeshan Haider,Tayyaba Noreen,M. D. Assuncao,Kaiwen Zhang*

Main category: cs.DC

TL;DR: 本文提出了一种名为预测性分片分配协议（PSAP）的动态智能分片方法，通过结合时序工作负载预测模型与安全约束强化学习控制器，实现对账户和交易的前瞻性分片分配，在保证拜占庭安全的同时显著提升吞吐量、降低延迟和跨分片开销。


<details>
  <summary>Details</summary>
Motivation: 传统区块链分片方案多采用静态或启发式分配策略，易导致负载不均、拥塞和过多跨分片通信，削弱了分片带来的可扩展性优势。因此，亟需一种能动态、智能地进行分片分配的方法以优化性能并保障安全性。

Method: PSAP协议融合了时序工作负载预测（TWF）模型与安全约束的近端策略优化（Safe-PPO）强化学习控制器，支持多区块提前预测和自适应分片重配置；同时通过同步量化运行时和安全门机制（限制权益集中度、迁移Gas和利用率阈值）确保验证节点间确定性推理与系统安全。

Result: 在Ethereum、NEAR和Hyperledger Fabric等异构数据集上的实验表明，相比现有动态分片基线方法，PSAP最多可实现2倍吞吐量提升、35%延迟降低和20%跨分片开销减少。

Conclusion: 预测性、确定性且兼顾安全性的分片分配策略是构建下一代高可扩展区块链系统的重要方向，PSAP为此提供了有效解决方案。

Abstract: Sharding has emerged as a key technique to address blockchain scalability by partitioning the ledger into multiple shards that process transactions in parallel. Although this approach improves throughput, static or heuristic shard allocation often leads to workload skew, congestion, and excessive cross-shard communication diminishing the scalability benefits of sharding. To overcome these challenges, we propose the Predictive Shard Allocation Protocol (PSAP), a dynamic and intelligent allocation framework that proactively assigns accounts and transactions to shards based on workload forecasts. PSAP integrates a Temporal Workload Forecasting (TWF) model with a safety-constrained reinforcement learning (Safe-PPO) controller, jointly enabling multi-block-ahead prediction and adaptive shard reconfiguration. The protocol enforces deterministic inference across validators through a synchronized quantized runtime and a safety gate that limits stake concentration, migration gas, and utilization thresholds. By anticipating hotspot formation and executing bounded, atomic migrations, PSAP achieves stable load balance while preserving Byzantine safety. Experimental evaluation on heterogeneous datasets, including Ethereum, NEAR, and Hyperledger Fabric mapped via address-clustering heuristics, demonstrates up to 2x throughput improvement, 35\% lower latency, and 20\% reduced cross-shard overhead compared to existing dynamic sharding baselines. These results confirm that predictive, deterministic, and security-aware shard allocation is a promising direction for next-generation scalable blockchain systems.

</details>


### [5] [AVS: A Computational and Hierarchical Storage System for Autonomous Vehicles](https://arxiv.org/abs/2511.19453)
*Yuxin Wang,Yuankai He,Weisong Shi*

Main category: cs.DC

TL;DR: 本文提出了AVS，一种面向自动驾驶车辆的新型车载存储系统，通过计算与存储协同设计，实现高效的数据写入、快速选择性检索和显著的存储空间压缩。


<details>
  <summary>Details</summary>
Motivation: 当前自动驾驶车辆产生的海量异构数据（如每天14TB）缺乏高效的通用存储与查询系统，现有车载数据记录器和存储栈无法满足新兴第三方应用对高效存取的需求。

Method: AVS采用分层布局协同设计计算与存储，包括模态感知的数据缩减与压缩、基于冷热数据分层的每日归档机制，以及轻量级元数据索引层；并通过在真实L4自动驾驶轨迹和嵌入式硬件上的系统级基准测试进行验证。

Result: 原型系统在有限资源下实现了可预测的实时数据摄入、快速的选择性数据检索以及显著的存储占用减少。

Conclusion: 该研究证明了将存储作为自动驾驶系统一等组件的重要性，并为未来更可扩展、更长期部署的车载存储系统提供了设计思路与实践基础。

Abstract: Autonomous vehicles (AVs) are evolving into mobile computing platforms, equipped with powerful processors and diverse sensors that generate massive heterogeneous data, for example 14 TB per day. Supporting emerging third-party applications calls for a general-purpose, queryable onboard storage system. Yet today's data loggers and storage stacks in vehicles fail to deliver efficient data storage and retrieval. This paper presents AVS, an Autonomous Vehicle Storage system that co-designs computation with a hierarchical layout: modality-aware reduction and compression, hot-cold tiering with daily archival, and a lightweight metadata layer for indexing. The design is grounded with system-level benchmarks on AV data that cover SSD and HDD filesystems and embedded indexing, and is validated on embedded hardware with real L4 autonomous driving traces. The prototype delivers predictable real-time ingest, fast selective retrieval, and substantial footprint reduction under modest resource budgets. The work also outlines observations and next steps toward more scalable and longer deployments to motivate storage as a first-class component in AV stacks.

</details>


### [6] [SparOA: Sparse and Operator-aware Hybrid Scheduling for Edge DNN Inference](https://arxiv.org/abs/2511.19457)
*Ziyang Zhang,Jie Liu,Luca Mottola*

Main category: cs.DC

TL;DR: SparOA is a CPU-GPU hybrid inference framework that uses sparsity and computational intensity to schedule DNN operators, achieving faster inference and lower energy consumption than existing methods.


<details>
  <summary>Details</summary>
Motivation: Deep neural networks face performance and efficiency challenges on edge devices; existing approaches either lose accuracy, are costly, or ignore operator-level characteristics.

Method: SparOA employs (1) a threshold predictor for sparsity and computational intensity, (2) a reinforcement learning-based scheduler for dynamic resource allocation, and (3) a hybrid inference engine with asynchronous execution and batch optimization.

Result: SparOA achieves 1.22–1.31× average speedup over baselines, up to 50.7× faster than CPU-only inference, and reduces energy-per-inference by 7%–16% compared to state-of-the-art co-execution methods.

Conclusion: By jointly considering sparsity and computational intensity at the operator level, SparOA effectively balances performance and energy efficiency in hybrid CPU-GPU inference for edge deployment.

Abstract: The resource demands of deep neural network (DNN) models introduce significant performance challenges, especially when deployed on resource-constrained edge devices. Existing solutions like model compression often sacrifice accuracy, while specialized hardware remains costly and inflexible. Hybrid inference methods, however, typically overlook how operator characteristics impact performance. In this work, we present SparOA, a CPU-GPU hybrid inference framework, which leverages both sparsity and computational intensity to optimize operator scheduling. SparOA embraces aforementioned challenges through three key components: (1) a threshold predictor that accurately determines optimal sparsity and computational intensity thresholds; (2) a reinforcement learning-based scheduler that dynamically optimizes resource allocation based on real-time hardware states; and (3) a hybrid inference engine that enhances efficiency through asynchronous execution and batch size optimization.Extensive results show that SparOA achieves an average speedup of 1.22-1.31x compared to all baselines, and outperforms the CPU-Only by up to 50.7x. Also, SparOA achieves optimal energy-per-inference, consuming 7\%-16\% less energy than the SOTA co-execution baseline.

</details>


### [7] [Systemic approach for modeling a generic smart grid](https://arxiv.org/abs/2511.19460)
*Sofiane Ben Amor,Guillaume Guerard,Loup-Noé Levy*

Main category: cs.DC

TL;DR: 本文提出了一种智能电网的骨干模型，通过分布式优化实现生产与消费调度，在保证灵活性和可扩展性的同时，用于验证不同场景下的假设。


<details>
  <summary>Details</summary>
Motivation: 传统计算方法难以应对智能电网带来的复杂跨学科建模与仿真挑战，亟需一种系统性的集成建模方法。

Method: 构建一个包含电力系统、能源市场、需求侧管理等多要素的智能电网骨干模型，并采用子系统的分布式优化方法进行仿真。

Result: 该模型能够有效模拟异构系统，在人类尺度模型之前验证假设，并成功实现灵活且可扩展的生产与消费调度。

Conclusion: 所提出的骨干模型为智能电网不同场景的测试提供了有效工具，展示了分布式优化在提升系统灵活性和可扩展性方面的潜力。

Abstract: Smart grid technological advances present a recent class of complex interdisciplinary modeling and increasingly difficult simulation problems to solve using traditional computational methods. To simulate a smart grid requires a systemic approach to integrated modeling of power systems, energy markets, demand-side management, and much other resources and assets that are becoming part of the current paradigm of the power grid. This paper presents a backbone model of a smart grid to test alternative scenarios for the grid. This tool simulates disparate systems to validate assumptions before the human scale model. Thanks to a distributed optimization of subsystems, the production and consumption scheduling is achieved while maintaining flexibility and scalability.

</details>


### [8] [Urban Buildings Energy Consumption Estimation Using HPC: A Case Study of Bologna](https://arxiv.org/abs/2511.19463)
*Aldo Canfora,Eleonora Bergamaschi,Riccardo Mioli,Federico Battini,Mirko Degli Esposti,Giorgio Pedrazzi,Chiara Dellacasa*

Main category: cs.DC

TL;DR: 本文提出了一种结合EnergyPlus模拟、高性能计算和开放地理空间数据的城市建筑能耗建模（UBEM）流程，用于估算意大利博洛尼亚市约25,000栋建筑的能耗，整个模拟在Cineca的Leonardo超级计算机上不到30分钟完成。


<details>
  <summary>Details</summary>
Motivation: 城市建筑能耗建模对于理解和预测城市尺度的能源消耗至关重要，但传统方法在精度与效率之间难以兼顾。因此，作者旨在构建一个高效且基于真实数据的UBEM流程。

Method: 该研究整合了EnergyPlus能耗模拟工具、高性能计算（HPC）平台以及开放地理空间数据集。建筑几何信息来自博洛尼亚开放数据门户并辅以LiDAR测量；非几何属性（如建材、保温性能、窗户参数）则依据地区建筑规范和欧洲TABULA数据库设定。所有模拟在Cineca的Leonardo超级计算机上运行。

Result: 成功在不到30分钟内完成了对博洛尼亚约25,000栋建筑的能耗模拟，展示了该UBEM流程在大规模城市尺度上的高效性与可行性。

Conclusion: 所提出的UBEM流程有效结合了高精度数据源与高性能计算能力，为城市级建筑能耗评估提供了一个可扩展、高效且数据驱动的解决方案。

Abstract: Urban Building Energy Modeling (UBEM) plays a central role in understanding and forecasting energy consumption at the city scale. In this work, we present a UBEM pipeline that integrates EnergyPlus simulations, high-performance computing (HPC), and open geospatial datasets to estimate the energy demand of buildings in Bologna, Italy. Geometric information including building footprints and heights was obtained from the Bologna Open Data portal and enhanced with aerial LiDAR measurements. Non-geometric attributes such as construction materials, insulation characteristics, and window performance were derived from regional building regulations and the European TABULA database. The computation was carried out on Leonardo, the Cineca-hosted supercomputer, enabling the simulation of approximately 25,000 buildings in under 30 minutes.

</details>


### [9] [Towards a future space-based, highly scalable AI infrastructure system design](https://arxiv.org/abs/2511.19468)
*Blaise Agüera y Arcas,Travis Beals,Maria Biggs,Jessica V. Bloom,Thomas Fischbacher,Konstantin Gromov,Urs Köster,Rishiraj Pravahan,James Manyika*

Main category: cs.DC

TL;DR: 本文探讨了在太空中利用太阳能卫星集群构建可扩展的机器学习计算系统，通过搭载TPU芯片、星间激光通信和高精度机器学习模型控制编队飞行，实现高效AI算力部署，并分析了辐射耐受性与未来发射成本。


<details>
  <summary>Details</summary>
Motivation: 鉴于AI作为基础性通用技术将持续增长对算力和能源的需求，而太阳是太阳系中最丰富的能源，因此有必要探索如何高效利用太阳能支持未来的AI基础设施。

Method: 提出一种基于近地轨道卫星群的空间机器学习计算架构，卫星配备太阳能阵列、自由空间光通信链路和Google TPU芯片；采用紧密编队飞行（如81颗卫星组成半径1公里的集群），并使用高精度机器学习模型控制大规模星座；同时对Trillium TPU进行辐射测试以评估其在太空环境中的可靠性。

Result: Trillium TPU在模拟5年任务寿命的总电离剂量下未出现永久性故障，并对其位翻转错误进行了表征；此外，学习曲线分析预测到2030年代中期，低地球轨道发射成本可能降至约200美元/公斤。

Conclusion: 利用近地轨道太阳能卫星集群构建AI计算基础设施在技术和经济上具有可行性，有望成为满足未来AI能源与算力需求的重要路径。

Abstract: If AI is a foundational general-purpose technology, we should anticipate that demand for AI compute -- and energy -- will continue to grow. The Sun is by far the largest energy source in our solar system, and thus it warrants consideration how future AI infrastructure could most efficiently tap into that power. This work explores a scalable compute system for machine learning in space, using fleets of satellites equipped with solar arrays, inter-satellite links using free-space optics, and Google tensor processing unit (TPU) accelerator chips. To facilitate high-bandwidth, low-latency inter-satellite communication, the satellites would be flown in close proximity. We illustrate the basic approach to formation flight via a 81-satellite cluster of 1 km radius, and describe an approach for using high-precision ML-based models to control large-scale constellations. Trillium TPUs are radiation tested. They survive a total ionizing dose equivalent to a 5 year mission life without permanent failures, and are characterized for bit-flip errors. Launch costs are a critical part of overall system cost; a learning curve analysis suggests launch to low-Earth orbit (LEO) may reach $\lesssim$\$200/kg by the mid-2030s.

</details>


### [10] [Federated Learning Framework for Scalable AI in Heterogeneous HPC and Cloud Environments](https://arxiv.org/abs/2511.19479)
*Sangam Ghimire,Paribartan Timalsina,Nirjal Bhurtel,Bishal Neupane,Bigyan Byanju Shrestha,Subarna Bhattarai,Prajwal Gaire,Jessica Thapa,Sudan Jha*

Main category: cs.DC

TL;DR: 本文提出了一种适用于混合高性能计算（HPC）与云环境的联邦学习框架，有效应对系统异构性、通信开销和资源调度等挑战，在保障模型精度与数据隐私的同时，展现出良好的可扩展性、容错性和收敛性。


<details>
  <summary>Details</summary>
Motivation: 随着对可扩展且注重隐私的AI系统需求增长，联邦学习成为一种有前景的解决方案；然而，在混合HPC与云环境中，硬件异构性、通信限制和非均匀数据分布带来了新的复杂性，亟需高效应对。

Method: 设计并实现了一个专为混合HPC与云环境优化的联邦学习框架，重点解决系统异构性、通信开销和资源调度问题，同时确保模型准确性和数据隐私。

Result: 在混合测试平台上实验表明，该框架在非独立同分布（non-IID）数据和多样化硬件条件下仍具有优异的可扩展性、容错性和收敛性能。

Conclusion: 联邦学习在现代分布式计算环境中具备实际应用潜力，能够构建高效、可扩展且隐私保护的AI系统。

Abstract: As the demand grows for scalable and privacy-aware AI systems, Federated Learning (FL) has emerged as a promising solution, allowing decentralized model training without moving raw data. At the same time, the combination of high-performance computing (HPC) and cloud infrastructure offers vast computing power but introduces new complexities, especially when dealing with heterogeneous hardware, communication limits, and non-uniform data. In this work, we present a federated learning framework built to run efficiently across mixed HPC and cloud environments. Our system addresses key challenges such as system heterogeneity, communication overhead, and resource scheduling, while maintaining model accuracy and data privacy. Through experiments on a hybrid testbed, we demonstrate strong performance in terms of scalability, fault tolerance, and convergence, even under non-Independent and Identically Distributed (non-IID) data distributions and varied hardware. These results highlight the potential of federated learning as a practical approach to building scalable Artificial Intelligence (AI) systems in modern, distributed computing settings.

</details>


### [11] [Enabling Scientific Workflow Scheduling Research in Non-Uniform Memory Access Architectures](https://arxiv.org/abs/2511.19832)
*Aurelio Vivas,Harold Castro*

Main category: cs.DC

TL;DR: 本文提出了nFlows，一个面向NUMA架构的高性能计算（HPC）系统的工作流执行运行时系统，用于建模、裸机执行、仿真和验证数据密集型工作流的调度算法。


<details>
  <summary>Details</summary>
Motivation: 当前大多数工作流调度策略主要针对网格或云环境设计，缺乏对HPC系统中非统一内存访问（NUMA）架构的考虑。现代HPC节点包含多个NUMA域和异构内存（如HBM和DRAM），并常将加速器和网卡连接到特定NUMA节点，导致数据访问延迟差异显著，任务与数据放置复杂化。

Method: 设计并实现了一个名为nFlows的NUMA感知工作流执行运行时系统，支持构建仿真模型并在物理系统上直接执行，从而研究NUMA效应对调度的影响。

Result: nFlows能够支持NUMA感知调度算法的设计、数据移动行为分析、性能瓶颈识别以及内存中工作流执行的探索。

Conclusion: nFlows填补了现有工作流调度方法在NUMA感知方面的空白，为在基于NUMA的HPC系统上高效执行数据密集型工作流提供了有效工具和方法。

Abstract: Data-intensive scientific workflows increasingly rely on high-performance computing (HPC) systems, complementing traditional Grid and Cloud platforms. However, workflow scheduling on HPC infrastructures remains challenging due to the prevalence of non-uniform memory access (NUMA) architectures. These systems require schedulers to account for data locality not only across distributed environments but also within each node. Modern HPC nodes integrate multiple NUMA domains and heterogeneous memory regions, such as high-bandwidth memory (HBM) and DRAM, and frequently attach accelerators (GPUs or FPGAs) and network interface cards (NICs) to specific NUMA nodes. This design increases the variability of data-access latency and complicates the placement of both tasks and data. Despite these constraints, most workflow scheduling strategies were originally developed for Grid or Cloud environments and rarely incorporate NUMA-aware considerations. To address this gap, this work introduces nFlows, a NUMA-aware Workflow Execution Runtime System that enables the modeling, bare-metal execution, simulation, and validation of scheduling algorithms for data-intensive workflows on NUMA-based HPC systems. The system's design, implementation, and validation methodology are presented. nFlows supports the construction of simulation models and their direct execution on physical systems, enabling studies of NUMA effects on scheduling, the design of NUMA-aware algorithms, the analysis of data-movement behavior, the identification of performance bottlenecks, and the exploration of in-memory workflow execution.

</details>


### [12] [PolarStore: High-Performance Data Compression for Large-Scale Cloud-Native Databases](https://arxiv.org/abs/2511.19949)
*Qingda Hu,Xinjun Yang,Feifei Li,Junru Li,Ya Lin,Yuqi Zhou,Yicong Zhu,Junwei Zhang,Rongbiao Xie,Ling Zhou,Bin Wu,Wenchao Zhou*

Main category: cs.DC

TL;DR: PolarStore 是一种面向云原生关系数据库的压缩共享存储系统，结合硬件与软件压缩，在保持性能的同时实现 3.55 倍压缩率和约 60% 的存储成本降低。


<details>
  <summary>Details</summary>
Motivation: 现有 RDBMS 中的压缩方法在性能开销（软件方案）和灵活性不足（硬件方案）之间存在明显权衡，而云原生数据库用户对存储成本高度敏感，亟需兼顾高效压缩与高性能的解决方案。

Method: PolarStore 采用双层压缩机制：在 PolarCSD 硬件中进行存储内压缩，同时辅以轻量级软件压缩；并引入面向数据库的 I/O 路径优化、硬件稳定性改进及压缩感知调度策略。

Result: PolarStore 已部署于 PolarDB 的数千台存储服务器上，管理超 100 PB 数据，实现 3.55 倍压缩率，降低约 60% 存储成本，且性能与未压缩集群相当。

Conclusion: 通过软硬协同设计与数据库感知优化，PolarStore 成功在云原生 RDBMS 中实现了高性价比的存储压缩，为大规模数据库系统提供了可行的低成本高效率存储方案。

Abstract: In recent years, resource elasticity and cost optimization have become essential for RDBMSs. While cloud-native RDBMSs provide elastic computing resources via disaggregated computing and storage, storage costs remain a critical user concern. Consequently, data compression emerges as an effective strategy to reduce storage costs. However, existing compression approaches in RDBMSs present a stark trade-off: software-based approaches incur significant performance overheads, while hardware-based alternatives lack the flexibility required for diverse database workloads. In this paper, we present PolarStore, a compressed shared storage system for cloud-native RDBMSs. PolarStore employs a dual-layer compression mechanism that combines in-storage compression in PolarCSD hardware with lightweight compression in software. This design leverages the strengths of both approaches. PolarStore also incorporates database-oriented optimizations to maintain high performance on critical I/O paths. Drawing from large-scale deployment experiences, we also introduce hardware improvements for PolarCSD to ensure host-level stability and propose a compression-aware scheduling scheme to improve cluster-level space efficiency. PolarStore is currently deployed on thousands of storage servers within PolarDB, managing over 100 PB of data. It achieves a compression ratio of 3.55 and reduces storage costs by approximately 60%. Remarkably, these savings are achieved while maintaining performance comparable to uncompressed clusters.

</details>


### [13] [QiMeng-Kernel: Macro-Thinking Micro-Coding Paradigm for LLM-Based High-Performance GPU Kernel Generation](https://arxiv.org/abs/2511.20100)
*Xinguo Zhu,Shaohui Peng,Jiaming Guo,Yunji Chen,Qi Guo,Yuanbo Wen,Hang Qin,Ruizhi Chen,Qirui Zhou,Ke Gao,Yanjun Wu,Chen Zhao,Ling Li*

Main category: cs.DC

TL;DR: 本文提出了一种名为“宏观思考微观编码”（MTMC）的分层框架，用于高效、正确地自动生成高性能GPU内核。该方法将优化策略与具体实现解耦，通过强化学习引导轻量级LLM探索高层优化策略，并利用通用LLM逐步实现这些策略，从而在准确性和运行速度上显著优于现有大模型和专家手工优化方案。


<details>
  <summary>Details</summary>
Motivation: 开发高性能GPU内核对AI和科学计算至关重要，但传统方法依赖专家手工编写，可移植性差。尽管大语言模型（LLM）有望实现自动化，但现有方法在生成完整底层代码时面临正确性与效率难以兼顾的根本矛盾，根源在于需同时探索庞大的优化策略空间和实现细节空间。

Method: 提出Macro Thinking Micro Coding（MTMC）框架：Macro Thinking阶段使用强化学习引导轻量级LLM高效探索并学习最大化硬件利用率的高层语义优化策略；Micro Coding阶段则利用通用LLM根据Macro Thinking提出的逐步优化建议增量式地生成低层实现代码，避免一次性生成整个内核带来的错误。

Result: 在KernelBench上，MTMC在Level 1-2达到近100%准确率，Level 3达70%，比当前最优的通用和领域微调LLM高出50%以上，并实现最高7.3倍于LLM、2.2倍于专家优化PyTorch Eager内核的加速；在更具挑战性的TritonBench上，准确率达59.64%，加速比高达34倍。

Conclusion: MTMC通过将优化策略与实现细节分层处理，有效解决了LLM在GPU内核生成中正确性与效率难以兼顾的问题，在多个基准测试中显著超越现有方法，为高性能计算中的自动代码生成提供了新范式。

Abstract: Developing high-performance GPU kernels is critical for AI and scientific computing, but remains challenging due to its reliance on expert crafting and poor portability. While LLMs offer promise for automation, both general-purpose and finetuned LLMs suffer from two fundamental and conflicting limitations: correctness and efficiency. The key reason is that existing LLM-based approaches directly generate the entire optimized low-level programs, requiring exploration of an extremely vast space encompassing both optimization policies and implementation codes. To address the challenge of exploring an intractable space, we propose Macro Thinking Micro Coding (MTMC), a hierarchical framework inspired by the staged optimization strategy of human experts. It decouples optimization strategy from implementation details, ensuring efficiency through high-level strategy and correctness through low-level implementation. Specifically, Macro Thinking employs reinforcement learning to guide lightweight LLMs in efficiently exploring and learning semantic optimization strategies that maximize hardware utilization. Micro Coding leverages general-purpose LLMs to incrementally implement the stepwise optimization proposals from Macro Thinking, avoiding full-kernel generation errors. Together, they effectively navigate the vast optimization space and intricate implementation details, enabling LLMs for high-performance GPU kernel generation. Comprehensive results on widely adopted benchmarks demonstrate the superior performance of MTMC on GPU kernel generation in both accuracy and running time. On KernelBench, MTMC achieves near 100% and 70% accuracy at Levels 1-2 and 3, over 50% than SOTA general-purpose and domain-finetuned LLMs, with up to 7.3x speedup over LLMs, and 2.2x over expert-optimized PyTorch Eager kernels. On the more challenging TritonBench, MTMC attains up to 59.64% accuracy and 34x speedup.

</details>


### [14] [Beluga: A CXL-Based Memory Architecture for Scalable and Efficient LLM KVCache Management](https://arxiv.org/abs/2511.20172)
*Xinjun Yang,Qingda Hu,Junru Li,Feifei Li,Yuqi Zhou,Yicong Zhu,Qiuru Lin,Jian Dai,Yang Kong,Jiayu Zhang,Guoqiang Xu,Qiang Liu*

Main category: cs.DC

TL;DR: 本文提出Beluga，一种基于CXL的新型内存架构，使GPU和CPU能通过CXL交换机共享大规模内存池，显著降低LLM推理中KVCache访问的延迟并提升吞吐量。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型（LLM）规模和上下文长度的增长，GPU高带宽内存（HBM）容量不足，需依赖主机内存（CPU DRAM）存储KVCache；然而CPU内存通道数量限制了DRAM扩展，而现有基于RDMA的分离式内存方案存在高延迟、协议复杂和同步开销大等问题。

Method: 利用新兴的CXL技术构建共享内存池，通过CXL交换机实现GPU与CPU对大规模内存的原生load/store访问，设计并实现了面向LLM推理KVCache管理的系统Beluga-KVCache。

Result: 在vLLM推理引擎中，相比基于RDMA的方案，Beluga-KVCache将首Token时间（TTFT）减少89.6%，吞吐量提升7.35倍。

Conclusion: Beluga是首个支持GPU通过CXL交换机直接访问大规模内存池的系统，为GPU低延迟共享访问海量内存资源提供了有效解决方案。

Abstract: The rapid increase in LLM model sizes and the growing demand for long-context inference have made memory a critical bottleneck in GPU-accelerated serving systems. Although high-bandwidth memory (HBM) on GPUs offers fast access, its limited capacity necessitates reliance on host memory (CPU DRAM) to support larger working sets such as the KVCache. However, the maximum DRAM capacity is constrained by the limited number of memory channels per CPU socket. To overcome this limitation, current systems often adopt RDMA-based disaggregated memory pools, which introduce significant challenges including high access latency, complex communication protocols, and synchronization overhead. Fortunately, the emerging CXL technology introduces new opportunities in KVCache design. In this paper, we propose Beluga, a novel memory architecture that enables GPUs and CPUs to access a shared, large-scale memory pool through CXL switches. By supporting native load/store access semantics over the CXL fabric, our design delivers near-local memory latency, while reducing programming complexity and minimizing synchronization overhead. We conduct a systematic characterization of a commercial CXL switch-based memory pool and propose a set of design guidelines. Based on Beluga, we design and implement Beluga-KVCache, a system tailored for managing the large-scale KVCache in LLM inference. Beluga-KVCache achieves an 89.6% reduction in Time-To-First-Token (TTFT) and 7.35x throughput improvement in the vLLM inference engine compared to RDMA-based solutions. To the best of our knowledge, Beluga is the first system that enables GPUs to directly access large-scale memory pools through CXL switches, marking a significant step toward low-latency, shared access to vast memory resources by GPUs.

</details>


<div id='cs.OS'></div>

# cs.OS [[Back]](#toc)

### [15] [SARA: A Stall-Aware Memory Allocation Strategy for Mixed-Criticality Systems](https://arxiv.org/abs/2511.19991)
*Meng-Chia Lee,Wen Sheng Lim,Yuan-Hao Chang,Tei-Wei Kuo*

Main category: cs.OS

TL;DR: 本文提出了一种名为SARA的停顿感知实时内存分配器，通过为软实时任务分配刚好满足截止期限的内存，并优化剩余内存供非实时应用使用，在内存受限的边缘设备上实现了软实时任务高截止命中率（97.13%）和非实时应用吞吐量最高22.32倍的提升。


<details>
  <summary>Details</summary>
Motivation: 边缘设备因成本、尺寸和功耗限制导致内存容量有限，引发内存竞争和频繁页面交换，造成存储I/O延迟与性能下降。现有内存分配策略难以在软实时任务与非实时应用之间取得平衡，且在高内存与I/O压力下会出现不可预测的系统级长停顿（long stalls），进一步恶化性能。

Method: SARA基于PSI指标衡量内存不足引起的延迟对软实时任务各周期作业执行时间的影响，动态分配刚好满足截止期限的最小内存；同时定义并检测长停顿，主动丢弃受影响的作业以减少执行中断。

Result: 实验表明，即使在内存容量仅为峰值需求60%的情况下，SARA仍能实现软实时任务平均97.13%的截止命中率，并使非实时应用吞吐量最高提升22.32倍。

Conclusion: SARA有效解决了内存受限混合关键性边缘设备中的内存分配难题，在保障软实时任务时效性的同时显著提升非实时应用性能，具有良好的实用价值。

Abstract: The memory capacity in edge devices is often limited due to constraints on cost, size, and power. Consequently, memory competition leads to inevitable page swapping in memory-constrained mixed-criticality edge devices, causing slow storage I/O and thus performance degradation. In such scenarios, inefficient memory allocation disrupts the balance between application performance, causing soft real-time (soft RT) tasks to miss deadlines or preventing non-real-time (non-RT) applications from optimizing throughput. Meanwhile, we observe unpredictable, long system-level stalls (called long stalls) under high memory and I/O pressure, which further degrade performance. In this work, we propose a Stall-Aware Real-Time Memory Allocator (SARA), which discovers opportunities for performance balance by allocating just enough memory to soft RT tasks to meet deadlines and, at the same time, optimizing the remaining memory for non-RT applications. To minimize the memory usage of soft RT tasks while meeting real-time requirements, SARA leverages our insight into how latency, caused by memory insufficiency and measured by our proposed PSI-based metric, affects the execution time of each soft RT job, where a job runs per period and a soft RT task consists of multiple periods. Moreover, SARA detects long stalls using our definition and proactively drops affected jobs, minimizing stalls in task execution. Experiments show that SARA achieves an average of 97.13% deadline hit ratio for soft RT tasks and improves non-RT application throughput by up to 22.32x over existing approaches, even with memory capacity limited to 60% of peak demand.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [16] [CAMformer: Associative Memory is All You Need](https://arxiv.org/abs/2511.19740)
*Tergel Molom-Ochir,Benjamin F. Morris,Mark Horton,Chiyue Wei,Cong Guo,Brady Taylor,Peter Liu,Shan X. Wang,Deliang Fan,Hai Helen Li,Yiran Chen*

Main category: cs.AR

TL;DR: CAMformer 是一种新型加速器，通过将注意力机制重构为关联记忆操作，并利用电压域二值注意力内容可寻址存储器（BA-CAM）实现常数时间的相似性搜索，显著提升能效、吞吐量并减少面积开销。


<details>
  <summary>Details</summary>
Motivation: 传统 Transformer 的注意力机制存在二次计算复杂度问题，导致在扩展时面临能耗高、延迟大和硬件开销大的挑战。

Method: 提出 CAMformer 架构，将注意力视为关联记忆操作，使用 BA-CAM 在模拟电压域中通过电荷共享实现相似性计算，替代传统数字运算；同时结合分层两阶段 top-k 过滤、流水线执行和高精度上下文建模。

Result: 在 BERT 和 Vision Transformer 任务上，CAMformer 相比当前最先进的加速器实现了超过 10 倍的能效提升、最高 4 倍的吞吐量提升以及 6–8 倍的面积缩减，同时保持近乎无损的模型准确率。

Conclusion: CAMformer 通过硬件-算法协同设计，有效解决了注意力机制的可扩展性瓶颈，在性能、能效与精度之间取得了优异平衡。

Abstract: Transformers face scalability challenges due to the quadratic cost of attention, which involves dense similarity computations between queries and keys. We propose CAMformer, a novel accelerator that reinterprets attention as an associative memory operation and computes attention scores using a voltage-domain Binary Attention Content Addressable Memory (BA-CAM). This enables constant-time similarity search through analog charge sharing, replacing digital arithmetic with physical similarity sensing. CAMformer integrates hierarchical two-stage top-k filtering, pipelined execution, and high-precision contextualization to achieve both algorithmic accuracy and architectural efficiency. Evaluated on BERT and Vision Transformer workloads, CAMformer achieves over 10x energy efficiency, up to 4x higher throughput, and 6-8x lower area compared to state-of-the-art accelerators--while maintaining near-lossless accuracy.

</details>


### [17] [Pickle Prefetcher: Programmable and Scalable Last-Level Cache Prefetcher](https://arxiv.org/abs/2511.19973)
*Hoa Nguyen,Pongstorn Maidee,Jason Lowe-Power,Alireza Kaviani*

Main category: cs.AR

TL;DR: 本文提出了一种名为Pickle Prefetcher的可编程、可扩展的末级缓存（LLC）预取器，通过软件定义预取策略来有效处理不规则内存访问模式，在图应用等场景中显著优于传统预取技术。


<details>
  <summary>Details</summary>
Motivation: 现代高性能架构中的大容量末级缓存（LLC）在处理具有高局部性的工作负载时能降低平均内存访问延迟，但在面对不规则内存访问模式时反而可能增加延迟。现有的基于预测的预取器难以有效应对这类不规则访问模式，尤其在现代应用中更为普遍，因此需要一种更灵活高效的预取机制。

Method: Pickle Prefetcher摒弃了传统的静态启发式或复杂预测算法，转而提供一个简单的软件编程接口，允许软件自定义预取策略，而不扩展指令集架构（ISA）。该方法将硬件预测逻辑的复杂性转移为软件可编程性，使预取器能适应多种访问模式，并将硬件资源集中用于调度和及时发出预取请求。

Result: 在gem5全系统仿真中，Pickle Prefetcher在GAPBS的广度优先搜索（BFS）实现上相比基线系统最高提速1.74倍；与私有缓存预取器结合使用时，相比仅使用私有缓存预取器的系统最高提速1.40倍。

Conclusion: Pickle Prefetcher通过软硬件协同设计，有效解决了不规则内存访问模式下的预取难题，在保持硬件资源高效利用的同时显著提升了性能，尤其适用于图计算等应用场景。

Abstract: Modern high-performance architectures employ large last-level caches (LLCs). While large LLCs can reduce average memory access latency for workloads with a high degree of locality, they can also increase latency for workloads with irregular memory access patterns. Prefetchers are widely used to reduce memory latency by prefetching data into the cache hierarchy before it is accessed by the core. However, existing prediction-based prefetchers often struggle with irregular memory access patterns, which are especially prevalent in modern applications. This paper introduces the Pickle Prefetcher, a programmable and scalable LLC prefetcher designed to handle independent irregular memory access patterns effectively. Instead of relying on static heuristics or complex prediction algorithms, Pickle Prefetcher allows software to define its own prefetching strategies using a simple programming interface without expanding the instruction set architecture (ISA). By trading the logic complexity of hardware prediction for software programmability, Pickle Prefetcher can adapt to a wide range of access patterns without requiring extensive hardware resources for prediction. This allows the prefetcher to dedicate its resources to scheduling and issuing timely prefetch requests. Graph applications are an example where the memory access pattern is irregular but easily predictable by software. Through extensive evaluations of the Pickle Prefetcher on gem5 full-system simulations, we demonstrate tha Pickle Prefetcher significantly outperforms traditional prefetching techniques. Our results show that Pickle Prefetcher achieves speedups of up to 1.74x on the GAPBS breadth-first search (BFS) implementation over a baseline system. When combined with private cache prefetchers, Pickle Prefetcher provides up to a 1.40x speedup over systems using only private cache prefetchers.

</details>


### [18] [R3A: Reliable RTL Repair Framework with Multi-Agent Fault Localization and Stochastic Tree-of-Thoughts Patch Generation](https://arxiv.org/abs/2511.20090)
*Zizhang Luo,Fan Cui,Kexing Zhou,Runlin Guo,Mile Xia,Hongyuan Hou,Yun Lian*

Main category: cs.AR

TL;DR: 本文提出R3A，一种基于大语言模型（LLM）的自动RTL修复框架，通过随机思维树方法和多智能体故障定位提升修复可靠性，在RTL-repair数据集上修复了90.6%的bug，显著优于传统方法和其他LLM方法。


<details>
  <summary>Details</summary>
Motivation: 传统自动程序修复方法依赖固定模板，适用范围有限；而大语言模型虽能理解代码语义，但在RTL修复中因输入冗长和内在随机性导致结果不可靠。

Method: R3A采用随机思维树（stochastic Tree-Of-Thoughts）方法控制补丁生成智能体，并结合启发式函数在探索与利用之间取得平衡；同时引入多智能体故障定位机制，为补丁生成提供可靠起点。

Result: R3A在给定时间内修复了RTL-repair数据集中90.6%的bug，比传统方法和其他LLM方法多覆盖45%的bug，平均pass@5率达86.7%。

Conclusion: R3A显著提升了基于LLM的RTL自动修复的可靠性与修复能力，验证了其在硬件设计与验证中的实用价值。

Abstract: Repairing RTL bugs is crucial for hardware design and verification. Traditional automatic program repair (APR) methods define dedicated search spaces to locate and fix bugs with program synthesis. However, they heavily rely on fixed templates and can only deal with limited bugs. As an alternative, Large Language Models with the ability to understand code semantics can be explored for RTL repair. However, they suffer from unreliable outcomes due to inherent randomness and long input contexts of RTL code and waveform. To address these challenges, we propose R3A, an LLM-based automatic RTL program repair framework upon the basic model to improve reliability. R3A proposes the stochastic Tree-Of-Thoughts method to control a patch generation agent to explore a validated solution for the bug. The algorithm samples search states according to a heuristic function to balance between exploration and exploitation for a reliable outcome. Besides, R3A proposes a multi-agent fault localization method to find fault candidates as the starting points for the patch generation agent, further increasing the reliability. Experiments show R3A can fix 90.6% of bugs in the RTL-repair dataset within a given time limit, which covers 45% more bugs than traditional methods and other LLM-based approaches, while achieving an 86.7% pass@5 rate on average, showing a high reliability.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [19] [An Adaptive, Data-Integrated Agent-Based Modeling Framework for Explainable and Contestable Policy Design](https://arxiv.org/abs/2511.19726)
*Roberto Garrone*

Main category: cs.MA

TL;DR: 本文提出了一种通用的自适应多智能体学习框架，整合动态机制、信息论诊断、因果模型、先验生成与无监督行为识别方法，用于分析学习型智能体与自适应控制如何共同塑造系统演化轨迹。


<details>
  <summary>Details</summary>
Motivation: 现有许多多智能体仿真研究仍采用静态决策规则和固定控制参数，无法充分反映真实系统中反馈、适应性与非平稳性的特点，因此需要一个更通用且可解释的自适应分析框架。

Method: 该框架结合了：(i) 四种区分静态/自适应智能体与固定/自适应系统参数的动态机制；(ii) 基于信息熵率、统计复杂度和预测信息的信息论诊断工具；(iii) 显式干预语义的结构因果模型；(iv) 从聚合或样本数据生成智能体层级先验的方法；(v) 识别涌现行为模式的无监督技术。

Result: 该框架提供了一个领域无关的架构，可用于系统比较非平衡、振荡或漂移动态下的稳定性、性能与可解释性，并配有数学定义、计算算子和实验设计模板。

Conclusion: 所提出的框架为构建可解释、可质疑的多智能体决策过程提供了结构化方法，有助于深入理解自适应智能体与控制系统之间的协同演化机制。

Abstract: Multi-agent systems often operate under feedback, adaptation, and non-stationarity, yet many simulation studies retain static decision rules and fixed control parameters. This paper introduces a general adaptive multi-agent learning framework that integrates: (i) four dynamic regimes distinguishing static versus adaptive agents and fixed versus adaptive system parameters; (ii) information-theoretic diagnostics (entropy rate, statistical complexity, and predictive information) to assess predictability and structure; (iii) structural causal models for explicit intervention semantics; (iv) procedures for generating agent-level priors from aggregate or sample data; and (v) unsupervised methods for identifying emergent behavioral regimes. The framework offers a domain-neutral architecture for analyzing how learning agents and adaptive controls jointly shape system trajectories, enabling systematic comparison of stability, performance, and interpretability across non-equilibrium, oscillatory, or drifting dynamics. Mathematical definitions, computational operators, and an experimental design template are provided, yielding a structured methodology for developing explainable and contestable multi-agent decision processes.

</details>


### [20] [Complex Instruction Following with Diverse Style Policies in Football Games](https://arxiv.org/abs/2511.19885)
*Chenglu Sun,Shuo Shen,Haonan Hu,Wei Zhou,Chen Chen*

Main category: cs.MA

TL;DR: 本文提出了一种名为LCDSP的新范式，通过结合多样化风格训练（DST）和风格解释器（SI），使智能体能在复杂多智能体环境中理解和执行高层语言指令。


<details>
  <summary>Details</summary>
Motivation: 现有语言控制强化学习方法在处理复杂、多智能体环境中的高层或抽象指令（如足球比赛中的战术指令）时仍面临挑战。

Method: 提出Language-Controlled Diverse Style Policies (LCDSP)，包含两个核心组件：Diverse Style Training (DST) 方法用于训练能展现多种行为风格的单一策略；Style Interpreter (SI) 用于将高层语言指令快速准确地转化为对应风格参数。

Result: 在5v5足球环境中进行的大量实验表明，LCDSP能够有效理解抽象战术指令并准确执行所需的多样化行为风格。

Conclusion: LCDSP在复杂多智能体场景中展现出理解和执行高层语言指令的强大能力，具有应用于现实复杂任务的潜力。

Abstract: Despite advancements in language-controlled reinforcement learning (LC-RL) for basic domains and straightforward commands (e.g., object manipulation and navigation), effectively extending LC-RL to comprehend and execute high-level or abstract instructions in complex, multi-agent environments, such as football games, remains a significant challenge. To address this gap, we introduce Language-Controlled Diverse Style Policies (LCDSP), a novel LC-RL paradigm specifically designed for complex scenarios. LCDSP comprises two key components: a Diverse Style Training (DST) method and a Style Interpreter (SI). The DST method efficiently trains a single policy capable of exhibiting a wide range of diverse behaviors by modulating agent actions through style parameters (SP). The SI is designed to accurately and rapidly translate high-level language instructions into these corresponding SP. Through extensive experiments in a complex 5v5 football environment, we demonstrate that LCDSP effectively comprehends abstract tactical instructions and accurately executes the desired diverse behavioral styles, showcasing its potential for complex, real-world applications.

</details>


### [21] [EnergyTwin: A Multi-Agent System for Simulating and Coordinating Energy Microgrids](https://arxiv.org/abs/2511.20590)
*Jakub Muszyński,Ignacy Walużenicz,Patryk Zan,Zofia Wrona,Maria Ganzha,Marcin Paprzycki,Costin Bădică*

Main category: cs.MA

TL;DR: EnergyTwin 是一个结合物理模型与基于预测的滚动规划的多智能体微电网仿真平台，用于提升本地能源自给率、电池储备和系统韧性。


<details>
  <summary>Details</summary>
Motivation: 现有微电网仿真工具要么缺乏对去中心化决策的支持，要么缺乏物理基础；需要一种能同时兼顾物理真实性和分布式决策的新方法。

Method: 提出 EnergyTwin，一个基于智能体的微电网仿真环境，将物理建模与基于预测的滚动规划及基于合约的协商机制相结合，每个资产作为智能体与中央智能体交互。

Result: 在校园微电网场景中验证表明，该方法提升了本地能源自给率、维持更高电池储备，并减少低韧性运行状态的发生。

Conclusion: EnergyTwin 为研究具有韧性和协商驱动特性的微电网提供了可行且可扩展的数字孪生平台。

Abstract: Microgrids are deployed to reduce purchased grid energy, limit exposure to volatile tariffs, and ensure service continuity during disturbances. This requires coordinating heterogeneous distributed energy resources across multiple time scales and under variable conditions. Among existing tools, typically, power-system simulators capture physical behaviour but assume centralized control, while multi-agent frameworks model decentralized decision-making but represent energy with no physical grounding. In this context, the EnergyTwin is introduced, an agent-based microgrid simulation environment that couples physically grounded models with forecast-informed, rolling-horizon planning, and negotiations. Each asset is modeled as an agent, interacting with a central agent that obtains forecasts, formulates predictions, and allocates energy through contract-based interactions. EnergyTwin targets tertiary-layer decision making and is extensible for digital-twin use. Its feasibility was evaluated in a university campus microgrid scenario where multiple planning strategies were compared. Achieved results show that forecast-driven rolling-horizon planning increases local energy self-sufficiency, maintains higher battery reserves, and reduces exposure to low-resilience operating states. They demonstrate also potential of EnergyTwin as platform supporting research on resilient, negotiation-driven microgrids.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [22] [Z-Space: A Multi-Agent Tool Orchestration Framework for Enterprise-Grade LLM Automation](https://arxiv.org/abs/2511.19483)
*Qingsong He,Jing Nan,Jiayu Jiao,Liangjie Tang,Xiaodong Xu,Mengmeng Sun,Qingyao Wang,Minghui Yan*

Main category: cs.SE

TL;DR: 本文提出Z-Space框架，通过多智能体协作与融合子空间加权算法，在大规模工具库中高效精准匹配用户意图与工具，显著降低大模型调用开销并提升工具调用准确率。


<details>
  <summary>Details</summary>
Motivation: 在企业级MCP服务快速增长背景下，现有方法难以在数千异构工具中高效准确地匹配目标功能，存在语义割裂、上下文膨胀和推理延迟高等问题。

Method: Z-Space框架包含三个核心组件：(1)基于意图解析模型的结构化用户查询理解；(2)基于融合子空间加权算法（FSWW）的无参细粒度工具过滤模块；(3)支持动态规划与容错执行的推理执行智能体。

Result: 在饿了么技术部门部署后，系统在淘天、高德、盒马等多个业务单元的大规模测试数据生成场景中，将工具推理平均token消耗降低96.26%，工具调用准确率达92%。

Conclusion: Z-Space有效提升了智能测试数据生成系统的效率与可靠性，为大规模工具调用场景提供了实用解决方案。

Abstract: Large Language Models can break through knowledge and timeliness limitations by invoking external tools within the Model Context Protocol framework to achieve automated execution of complex tasks. However, with the rapid growth of enterprise-scale MCP services, efficiently and accurately matching target functionalities among thousands of heterogeneous tools has become a core challenge restricting system practicality. Existing approaches generally rely on full-prompt injection or static semantic retrieval, facing issues including semantic disconnection between user queries and tool descriptions, context inflation in LLM input, and high inference latency. To address these challenges, this paper proposes Z-Space, a data-generation-oriented multi-agent collaborative tool invocation framework Z-Space. The Z-Space framework establishes a multi-agent collaborative architecture and tool filtering algorithm: (1) A structured semantic understanding of user queries is achieved through an intent parsing model; (2) A tool filtering module (FSWW) based on fused subspace weighted algorithm realizes fine-grained semantic alignment between intents and tools without parameter tuning; (3) An inference execution agent is constructed to support dynamic planning and fault-tolerant execution for multi-step tasks. This framework has been deployed in the Eleme platform's technical division, serving large-scale test data generation scenarios across multiple business units including Taotian, Gaode, and Hema. Production data demonstrates that the system reduces average token consumption in tool inference by 96.26\% while achieving a 92\% tool invocation accuracy rate, significantly enhancing the efficiency and reliability of intelligent test data generation systems.

</details>


### [23] [stable-pretraining-v1: Foundation Model Research Made Simple](https://arxiv.org/abs/2511.19484)
*Randall Balestriero,Hugues Van Assel,Sami BuGhanem,Lucas Maes*

Main category: cs.SE

TL;DR: stable-pretraining 是一个基于 PyTorch 等框架构建的模块化、高性能自监督学习库，旨在提升研究灵活性与迭代速度，支持细粒度日志记录，并已用于生成新研究见解。


<details>
  <summary>Details</summary>
Motivation: 当前基础模型和自监督学习研究受限于复杂代码库、重复实现及大规模实验的工程负担。

Method: 开发 stable-pretraining 库，整合探针、崩溃检测指标、增强流水线和可扩展评估流程，并强调全面日志记录以支持调试与复现。

Result: 该库成功用于深度表征探查和 CLIP 在合成数据微调下的退化分析等新研究发现。

Conclusion: stable-pretraining 通过降低入门门槛并支持大规模实验，有望加速基础模型研究的进展。

Abstract: Foundation models and self-supervised learning (SSL) have become central to modern AI, yet research in this area remains hindered by complex codebases, redundant re-implementations, and the heavy engineering burden of scaling experiments. We present stable-pretraining, a modular, extensible, and performance-optimized library built on top of PyTorch, Lightning, Hugging Face, and TorchMetrics. Unlike prior toolkits focused narrowly on reproducing state-of-the-art results, stable-pretraining is designed for flexibility and iteration speed: it unifies essential SSL utilities--including probes, collapse detection metrics, augmentation pipelines, and extensible evaluation routines--within a coherent and reliable framework. A central design principle is logging everything, enabling fine-grained visibility into training dynamics that makes debugging, monitoring, and reproducibility seamless. We validate the library by demonstrating its ability to generate new research insights with minimal overhead, including depthwise representation probing and the analysis of CLIP degradation under synthetic data finetuning. By lowering barriers to entry while remaining scalable to large experiments, stable-pretraining aims to accelerate discovery and expand the possibilities of foundation model research.

</details>


### [24] [Evolution without an Oracle: Driving Effective Evolution with LLM Judges](https://arxiv.org/abs/2511.19489)
*Zhe Zhao,Yuheng Yang,Haibin Wen,Xiaojie Qiu,Zaixi Zhang,Qingfu Zhang*

Main category: cs.SE

TL;DR: 本文提出MADE框架，通过将模糊指令分解为可验证的子需求，使进化计算能在仅依赖大语言模型主观评判的环境中有效运行，显著提升复杂任务中的性能表现。


<details>
  <summary>Details</summary>
Motivation: 现有结合大语言模型与进化计算的方法依赖于可计算的目标函数（Oracle），限制了其在无明确评估标准的开放领域中的应用。本文旨在探索是否可以在完全依赖主观评判（由大语言模型提供）的环境中实现有效的进化优化。

Method: 提出MADE（Multi-Agent Decomposed Evolution）框架，利用“问题规范”将模糊指令分解为具体、可验证的子需求，从而降低大语言模型主观评价的噪声，形成稳定精准的选择压力。

Result: 在DevAI和InfoBench等复杂基准测试中，MADE在软件需求满足度上从39.9%提升至61.9%（提升超50%），并在复杂指令遵循任务中达到95%的完美通过率。

Conclusion: 该研究验证了一种范式转变：从优化“可计算指标”转向优化“可描述的品质”，使得进化优化能够应用于缺乏客观真值的广泛开放领域。

Abstract: The integration of Large Language Models (LLMs) with Evolutionary Computation (EC) has unlocked new frontiers in scientific discovery but remains shackled by a fundamental constraint: the reliance on an Oracle--an objective, machine-computable fitness function. This paper breaks this barrier by asking: Can evolution thrive in a purely subjective landscape governed solely by LLM judges? We introduce MADE (Multi-Agent Decomposed Evolution), a framework that tames the inherent noise of subjective evaluation through "Problem Specification." By decomposing vague instructions into specific, verifiable sub-requirements, MADE transforms high-variance LLM feedback into stable, precise selection pressure. The results are transformative: across complex benchmarks like DevAI and InfoBench, MADE outperforms strong baselines by over 50% in software requirement satisfaction (39.9% to 61.9%) and achieves a 95% perfect pass rate on complex instruction following. This work validates a fundamental paradigm shift: moving from optimizing "computable metrics" to "describable qualities," thereby unlocking evolutionary optimization for the vast open-ended domains where no ground truth exists.

</details>


### [25] [CodeR3: A GenAI-Powered Workflow Repair and Revival Ecosystem](https://arxiv.org/abs/2511.19510)
*Asif Zaman,Kallol Naha,Khalid Belhajjame,Hasan M. Jamil*

Main category: cs.SE

TL;DR: 本文提出了一种名为CodeR³的新型遗留工作流迁移系统，利用生成式AI将已失效的Taverna工作流自动转换为现代工作流技术（如Snakemake和VisFlow），并结合可视化分析、服务自动替换与人工验证，显著减少人工解析负担，同时指出关键任务仍需领域专家参与。


<details>
  <summary>Details</summary>
Motivation: 科学工作流蕴含宝贵的领域知识和计算方法，但大量已发表的工作流随时间推移而失效，尤其在Taverna等已停用的旧系统中问题尤为严重，亟需有效手段恢复其可用性。

Method: 开发CodeR³系统，结合生成式AI分析失效工作流特征，将其迁移到现代工作流平台，并集成逐步分析可视化、自动化服务替换及人机协同验证机制。

Result: 通过多个Taverna工作流复兴案例，验证了该方法的可行性；自动化显著降低了工作流解析和服务识别的人工成本，但服务替换与数据验证仍依赖人类专家；计划构建众包平台支持社区协作复兴与验证工作流。

Conclusion: 本文提出了一种兼顾自动化效率与人类判断的工作流复兴框架，为解决科学工作流衰减问题提供了可行路径。

Abstract: Scientific workflows encode valuable domain expertise and computational methodologies. Yet studies consistently show that a significant proportion of published workflows suffer from decay over time. This problem is particularly acute for legacy workflow systems like Taverna, where discontinued services, obsolete dependencies, and system retirement render previously functional workflows unusable. We present a novel legacy workflow migration system, called CodeR$^3$ (stands for Code Repair, Revival and Reuse), that leverages generative AI to analyze the characteristics of decayed workflows, reproduce them into modern workflow technologies like Snakemake and VisFlow. Our system additionally integrates stepwise workflow analysis visualization, automated service substitution, and human-in-the-loop validation. Through several case studies of Taverna workflow revival, we demonstrate the feasibility of this approach while identifying key challenges that require human oversight. Our findings reveal that automation significantly reduces manual effort in workflow parsing and service identification. However, critical tasks such as service substitution and data validation still require domain expertise. Our result will be a crowdsourcing platform that enables the community to collaboratively revive decayed workflows and validate the functionality and correctness of revived workflows. This work contributes a framework for workflow revival that balances automation efficiency with necessary human judgment.

</details>


### [26] [Agint: Agentic Graph Compilation for Software Engineering Agents](https://arxiv.org/abs/2511.19635)
*Abhi Chivukula,Jay Somasundaram,Vijay Somasundaram*

Main category: cs.SE

TL;DR: Agint 是一个基于图的智能体编译器与运行时系统，通过将自然语言指令分层转化为带类型的、具有效应感知的代码 DAG，解决了当前 LLM 编码智能体在上下文管理、延迟、可靠性、可复现性和可扩展性方面的挑战。


<details>
  <summary>Details</summary>
Motivation: 当前基于大语言模型（LLM）的编码智能体在实际应用中仍面临上下文管理困难、延迟高、可靠性不足、结果不可复现以及难以扩展等问题，亟需一种结合编译器技术与智能体架构的新范式来提升开发效率与系统性能。

Method: Agint 引入显式的类型层级（文本→数据→规范→代码），基于语义图变换构建类型化、效应感知的代码有向无环图（DAG），并采用混合 LLM 与函数驱动的即时（JIT）运行时。系统提供 dagify、dagent、schemagin 和 datagin 等 Unix 风格的可组合工具链，并支持 CLI 与 GUI（Agint Flow）双模式交互。

Result: Agint 实现了动态图优化、可复现且可优化的执行、推测性评估以及与现有开发者工具的互操作性；其类型化图绑定提升了可靠性，支持并发代码库的构造性组合，从而实现更低延迟、更高吞吐量、更高效上下文利用，并允许使用更小更快的模型进行加速开发。

Conclusion: Agint 通过融合自然语言、编译器方法与开发者工具链，构建了一个支持团队协作、可扩展、可复现的新型编码智能体框架，为从原型到生产级代码的无缝过渡提供了可行路径。

Abstract: LLM-based coding agents are increasingly common but still face challenges in context management, latency, reliability, reproducibility, and scalability. We present Agint, an agentic graph compiler, interpreter, and runtime that incrementally and hierarchically converts natural-language instructions into typed, effect-aware code DAGs. Agint introduces explicit type floors (text to data to spec to code) grounded in semantic graph transformations and a hybrid LLM and function-based JIT runtime. This enables dynamic graph refinement, reproducible and optimizable execution, speculative evaluation, and interoperability with existing developer tools. Agint's typed graph bindings improve reliability and allow concurrent composition of concurrent codebases by construction, supporting accelerated development with smaller and faster models, lower latency, efficient context utilization, and higher throughput. Hierarchical compilation allows scalable graph edits, while the graph structure supports reproducibility and efficient parallel generation. Agint provides a composable unix-style toolchain: dagify (DAG compiler), dagent (hybrid JIT runtime), schemagin (schema generator), and datagin (data transformer) for realtime, low-latency code and dataflow creation. Human developers and coding agents refine graphs through the Agint CLI, while non-technical users use Agint Flow GUI for visual editing, conversational refinement, and debugging to promote prototype agentic workflows to production code. This continuous co-creation model allows teams to prototype quickly, refine seamlessly, and deploy reliably, bridging natural language, compiler methods, and developer tooling to enable a new generation of composable, team-centric coding agents at scale.

</details>


### [27] [CodeFuse-CommitEval: Towards Benchmarking LLM's Power on Commit Message and Code Change Inconsistency Detection](https://arxiv.org/abs/2511.19875)
*Qingyu Zhang,Puzhuo Liu,Peng Di,Chenxiong Qian*

Main category: cs.SE

TL;DR: 本文提出了首个用于检测提交信息与代码差异不一致（MCI）的基准CODEFUSE-COMMITEVAL，基于ApacheCM数据集构建，并通过规则引导生成七类不一致样本。研究评估了六种开源大语言模型在原始设置及三种增强策略下的表现，发现模型对不一致提交的识别优于一致提交，且不同增强方法效果各异。


<details>
  <summary>Details</summary>
Motivation: 提交信息常存在质量低下或与代码差异不一致的问题（即MCI），这会误导审查者、阻碍维护、污染研究数据并可能掩盖安全补丁。目前尚无专门用于评估MCI检测模型的基准。

Method: 基于ApacheCM数据集，通过规则引导变异生成七类不一致消息，并采用双重验证确保正负样本质量；使用该标注数据集，在原始设置及少样本提示、思维链和扩展上下文三种增强策略下评估六种开源大语言模型的表现。

Result: 模型检测不一致提交的效果优于一致提交（平均召回率85.95%，精确率80.28%，特异性63.8%）；gpt-oss-20B整体表现最佳但token消耗超两倍；增强策略效果因模型大小而异，不同类型不一致的可检测性存在差异。

Conclusion: CODEFUSE-COMMITEVAL为MCI检测提供了严谨的评估基础，揭示了捕捉高层语义差距需更丰富的上下文和更平衡的数据。

Abstract: Version control relies on commit messages to convey the rationale for code changes, but these messages are often low quality and, more critically, inconsistent with their diffs-known as message-code inconsistency (MCI). MCIs mislead reviewers, hinder maintenance, contaminate research datasets, and may obscure security patches. Yet, no dedicated benchmark exists to evaluate models for MCI detection. We introduce CODEFUSE-COMMITEVAL, the first benchmark designed for MCI detection using large language models (LLMs). Built on the ApacheCM dataset for diversity and quality, we generate seven types of inconsistent messages through rule-guided mutations of originally consistent commits and apply two-fold validation to verify both positive and negative samples. Using this labeled dataset of message-diff pairs, we evaluate six state-of-the-art open-source LLMs under a vanilla setting and with three augmentation strategies: few-shot prompting, chain-of-thought, and extended context. Results show models detect inconsistent commits more reliably than consistent ones (average Recall 85.95%, Precision 80.28%, Specificity 63.8%); gpt-oss-20B performs best overall but uses over twice the tokens of others. Augmentation effects vary: adjacent context helps larger models but adds noise for smaller ones; few-shot improves accuracy and reduces token use, yet increases universally incorrect predictions; chain-of-thought boosts precision and specificity at the cost of recall and higher token consumption. Type-wise analysis reveals higher detectability for component, file-path, and operation inconsistencies, but lower accuracy and higher token cost for intent-level "purpose" inconsistencies. CODEFUSE-COMMITEVAL provides a rigorous foundation for measuring, comparing, and advancing MCI detection, highlighting the need for richer context and balanced data to capture high-level semantic gaps.

</details>


### [28] [LLMs for Automated Unit Test Generation and Assessment in Java: The AgoneTest Framework](https://arxiv.org/abs/2511.20403)
*Andrea Lops,Fedelucio Narducci,Azzurra Ragone,Michelantonio Trizio,Claudio Barto*

Main category: cs.SE

TL;DR: 本文提出了AgoneTest，一个用于评估大语言模型（LLM）生成Java单元测试的自动化框架，并引入Classes2Test数据集和综合评估指标，在现实条件下比较不同LLM及提示策略的效果。


<details>
  <summary>Details</summary>
Motivation: 单元测试是软件开发中重要但资源密集的环节，现有研究缺乏对LLM生成测试在真实场景下系统性评估的标准化方法。

Method: 构建AgoneTest框架，整合Classes2Test数据集与高级评估指标（如变异得分和测试异味），通过端到端流水线对LLM生成的测试进行标准化评估。

Result: 实验表明，在可编译的测试子集中，LLM生成的测试在覆盖率和缺陷检测方面可媲美甚至超越人工编写测试；增强型提示策略能提升测试质量。

Conclusion: AgoneTest揭示了LLM在软件测试中的潜力，为未来模型设计、提示工程和测试实践提供了参考。

Abstract: Unit testing is an essential but resource-intensive step in software development, ensuring individual code units function correctly. This paper introduces AgoneTest, an automated evaluation framework for Large Language Model-generated (LLM) unit tests in Java. AgoneTest does not aim to propose a novel test generation algorithm; rather, it supports researchers and developers in comparing different LLMs and prompting strategies through a standardized end-to-end evaluation pipeline under realistic conditions. We introduce the Classes2Test dataset, which maps Java classes under test to their corresponding test classes, and a framework that integrates advanced evaluation metrics, such as mutation score and test smells, for a comprehensive assessment. Experimental results show that, for the subset of tests that compile, LLM-generated tests can match or exceed human-written tests in terms of coverage and defect detection. Our findings also demonstrate that enhanced prompting strategies contribute to test quality. AgoneTest clarifies the potential of LLMs in software testing and offers insights for future improvements in model design, prompt engineering, and testing practices.

</details>


### [29] [Translating Large-Scale C Repositories to Idiomatic Rust](https://arxiv.org/abs/2511.20617)
*Saman Dehghan,Tianran Sun,Tianxiang Wu,Zihan Li,Reyhaneh Jabbarvand*

Main category: cs.SE

TL;DR: 本文提出了Rustine，一种高效且全自动的C到Rust代码翻译流水线，在保证可扩展性的同时显著提升了生成代码的安全性、惯用性和可读性。


<details>
  <summary>Details</summary>
Motivation: 现有C到Rust翻译方法难以兼顾质量与可扩展性：基于转译的方法虽可处理大型项目，但生成代码安全性差、不惯用且可读性低；而基于大语言模型（LLM）的方法因依赖前沿模型导致成本过高，难以规模化。

Method: 提出Rustine，一个全自动的仓库级C到Rust翻译流水线，旨在高效生成安全、惯用且可编译的Rust代码。

Result: 在23个C程序（27至13,200行代码）上评估，Rustine能为所有程序生成可编译的Rust代码，功能等价率达87%（通过1,063,099/1,221,192个断言），平均函数和行覆盖率分别为74.7%和72.2%。相比六种现有技术，其翻译结果更安全、更惯用、更可读；当未完全通过测试时，开发者平均仅需4.5小时即可借助Rustine完成调试。

Conclusion: Rustine在保持可扩展性的同时，显著优于现有C到Rust翻译方法，在安全性、惯用性和可读性方面取得良好平衡，并有效支持人工调试。

Abstract: Existing C to Rust translation techniques fail to balance quality and scalability: transpilation-based approaches scale to large projects but produce code with poor safety, idiomaticity, and readability. In contrast, LLM-based techniques are prohibitively expensive due to their reliance on frontier models (without which they cannot reliably generate compilable translations), thus limiting scalability. This paper proposes Rustine, a fully automated pipeline for effective and efficient repository-level C to idiomatic safe Rust translation. Evaluating on a diverse set of 23 C programs, ranging from 27 to 13,200 lines of code, Rustine can generate fully compilable Rust code for all and achieve 87% functional equivalence (passing 1,063,099 assertions out of 1,221,192 in test suites with average function and line coverage of 74.7% and 72.2%). Compared to six prior repository-level C to Rust translation techniques, the translations by Rustine are overall safer (fewer raw pointers, pointer arithmetic, and unsafe constructs), more idiomatic (fewer Rust linter violations), and more readable. When the translations cannot pass all tests to fulfill functional equivalence, human developers were able to complete the task in 4.5 hours, on average, using Rustine as debugging support.

</details>
