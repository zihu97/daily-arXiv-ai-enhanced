<div id=toc></div>

# Table of Contents

- [cs.AR](#cs.AR) [Total: 1]
- [cs.MA](#cs.MA) [Total: 1]
- [cs.DC](#cs.DC) [Total: 3]
- [cs.PF](#cs.PF) [Total: 2]
- [cs.SE](#cs.SE) [Total: 13]


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [1] [Annotated PIM Bibliography](https://arxiv.org/abs/2601.09002)
*Peter M. Kogge*

Main category: cs.AR

TL;DR: 本文旨在提供一份涵盖过去60年PIM技术发展的注释书目，以支持即将发表的文章。


<details>
  <summary>Details</summary>
Motivation: 澄清PIM并非全新技术，而是有长期历史背景，并系统整理相关文献。

Method: 通过文献综述和注释书目方法，全面覆盖PIM及相关术语的历史发展。

Result: 成功构建了跨越60年的PIM技术文献体系，为后续研究提供基础。

Conclusion: PIM虽近年受关注，但其技术渊源深远，需从历史角度全面理解其演进。

Abstract: Processing in Memory (PIM) and similar terms such as Compute In Memory (CIM), Logic in Memory (LIM), In Memory Computing (IMC), and Near Memory Computing (NMC) have gained attention recently as a potentially ``revolutionary new'' technique. The truth, however, is that many examples of the technology go back over 60 years. This document attempts to provide an annotated bibliography of PIM technology that attempts to cover the whole time-frame, and is organized to augment a forth-coming article.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [2] [MACRO-LLM: LLM-Empowered Multi-Agent Collaborative Reasoning under Spatiotemporal Partial Observability](https://arxiv.org/abs/2601.09295)
*Handi Chen,Running Zhao,Xiuzhe Wu,Edith C. H. Ngai*

Main category: cs.MA

TL;DR: MACRO-LLM 提出一种应对时空局部可观测性的多智能体协作推理框架，提升分布式大语言模型智能体的协调能力。


<details>
  <summary>Details</summary>
Motivation: 解决分布式大语言模型智能体因物理分散导致的局部感知与有限时间视野问题。

Method: 通过 CoProposer、Negotiator 和 Introspector 三个模块分别处理时间不确定性、空间短视和策略持续优化。

Result: 在自适应巡航控制与疫情管控两项长周期任务中表现优异，显著提升协作鲁棒性。

Conclusion: 该框架有效缓解时空局部可观测性限制，为复杂现实场景中的多智能体协作提供新方案。

Abstract: Large Language Model (LLM) agents deployed in complex real-world scenarios typically operate as spatially distributed entities. However, this physical dispersion constrains agents to limited local perception and finite temporal horizons. We characterize this bottleneck as spatiotemporal partial observability. Given such fragmented awareness, distributed agents struggle to coordinate efficiently. To bridge this gap, we introduce MACRO-LLM, LLM-empowered multi-agent collaborative reasoning under spatiotemporal partial observability. The architecture addresses spatiotemporal constraints via three modules: (1) the CoProposer mitigates temporal uncertainty by verifying candidate actions via predictive rollouts; (2) the Negotiator overcomes spatial myopia by resolving conflicts through mean-field statistical aggregation; and (3) the Introspector ensures continuous adaptation by analyzing historical experience to refine strategies via semantic gradient descent. Extensive evaluations on two complex long-horizon tasks, cooperative adaptive cruise control and pandemic control, demonstrate that our framework effectively mitigates spatiotemporal partial observability through spatial and temporal strategies, enabling robust coordination.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [3] [A Machine Learning Approach Towards Runtime Optimisation of Matrix Multiplication](https://arxiv.org/abs/2601.09114)
*Yufan Xia,Marco De La Pierre,Amanda S. Barnard,Giuseppe Maria Junior Barca*

Main category: cs.DC

TL;DR: 本文提出一种基于机器学习的线性代数库ADSALA，可动态优化GEMM多线程性能，在Intel与AMD平台上实现25%-40%加速。


<details>
  <summary>Details</summary>
Motivation: 现代多核共享内存系统中，难以手动确定最优线程数以最小化GEMM运行时间。

Method: 构建ADSALA库，利用机器学习模型根据训练数据动态选择最优线程数。

Result: 在Intel Cascade Lake与AMD Zen 3架构上，相比传统BLAS实现获得25%-40%性能提升（内存使用<100MB时）。

Conclusion: 机器学习驱动的线程数自适应策略能有效提升多核环境下GEMM运算效率。

Abstract: The GEneral Matrix Multiplication (GEMM) is one of the essential algorithms in scientific computing. Single-thread GEMM implementations are well-optimised with techniques like blocking and autotuning. However, due to the complexity of modern multi-core shared memory systems, it is challenging to determine the number of threads that minimises the multi-thread GEMM runtime. We present a proof-of-concept approach to building an Architecture and Data-Structure Aware Linear Algebra (ADSALA) software library that uses machine learning to optimise the runtime performance of BLAS routines. More specifically, our method uses a machine learning model on-the-fly to automatically select the optimal number of threads for a given GEMM task based on the collected training data. Test results on two different HPC node architectures, one based on a two-socket Intel Cascade Lake and the other on a two-socket AMD Zen 3, revealed a 25 to 40 per cent speedup compared to traditional GEMM implementations in BLAS when using GEMM of memory usage within 100 MB.

</details>


### [4] [Transaction-Driven Dynamic Reconfiguration for Certificate-Based Payment Systems](https://arxiv.org/abs/2601.09146)
*Lingkang Shangguan*

Main category: cs.DC

TL;DR: 提出基于拜占庭一致广播的交易驱动动态重配置协议PDCC，避免全局交易排序以提升性能。


<details>
  <summary>Details</summary>
Motivation: 现代支付系统需在不牺牲性能的前提下实现平滑的动态重配置。

Method: 结合用户nonce交易排序与周期性全系统共识机制，设计PDCC协议。

Result: PDCC可在不影响原系统性能的情况下完成平滑重配置。

Conclusion: 该协议为高性能支付系统提供了可行的动态配置更新方案。

Abstract: We present a transaction-driven dynamic reconfiguration protocol in Modern payment systems based on Byzantine Consistent Broadcast which can achieve high performance by avoiding global transaction ordering. We demonstrate the fundamental paradigm of modern payment systems, which combines user nonce based transactions ordering with periodic system-wide consensus mechanisms. Building on this foundation, we design PDCC(Payment Dynamic Config Change), which can lead a smooth reconfiguration process without impacting the original system's performance.

</details>


### [5] [LatencyPrism: Online Non-intrusive Latency Sculpting for SLO-Guaranteed LLM Inference](https://arxiv.org/abs/2601.09258)
*Du Yin,Jiayi Ren,Xiayu Sun,Tianyao Zhou,Haizhu Zhou,Ruiyan Ma,Danyang Zhang*

Main category: cs.DC

TL;DR: LatencyPrism 是首个零侵入、多平台的推理延迟分析系统，支持实时监控与异常告警，无需代码修改或服务重启。


<details>
  <summary>Details</summary>
Motivation: 现有AI性能分析工具因侵入性强、硬件绑定等问题，难以应对分布式异构推理环境中的实时延迟分析需求。

Method: 提出 LatencyPrism 系统，通过非侵入式设计，在不中断服务前提下实现流水线级延迟分解、毫秒级异常告警与SLO保障。

Result: 系统已在数千XPU上部署超六个月，实现批次级低开销监控，异常检测F1-score达0.98，并支持根因分析。

Conclusion: LatencyPrism 有效解决异构推理环境中延迟监控与SLO保障难题，具备高精度、低开销和强适应性。

Abstract: LLM inference latency critically determines user experience and operational costs, directly impacting throughput under SLO constraints. Even brief latency spikes degrade service quality despite acceptable average performance. However, distributed inference environments featuring diverse software frameworks and XPU architectures combined with dynamic workloads make latency analysis challenging. Constrained by intrusive designs that necessitate service restarts or even suspension, and by hardware-bound implementations that fail to adapt to heterogeneous inference environments, existing AI profiling methods are often inadequate for real-time production analysis.
  We present LatencyPrism, the first zero-intrusion multi-platform latency sculpting system. It aims to break down the inference latency across pipeline, proactively alert on inference latency anomalies, and guarantee adherence to SLOs, all without requiring code modifications or service restarts. LatencyPrism has been deployed across thousands of XPUs for over six months. It enables low-overhead real-time monitoring at batch level with alerts triggered in milliseconds. This approach distinguishes between workload-driven latency variations and anomalies indicating underlying issues with an F1-score of 0.98. We also conduct extensive experiments and investigations into root cause analysis to demonstrate LatencyPrism's capability.

</details>


<div id='cs.PF'></div>

# cs.PF [[Back]](#toc)

### [6] [Revisiting Disaggregated Large Language Model Serving for Performance and Energy Implications](https://arxiv.org/abs/2601.08833)
*Jiaxi Li,Yue Zhu,Eun Kyung Lee,Klara Nahrstedt*

Main category: cs.PF

TL;DR: 本文重新评估了在不同KV缓存传输介质和优化策略下，预填充-解码分离服务的性能与能效表现。


<details>
  <summary>Details</summary>
Motivation: 现有研究缺乏对不同KV缓存传输路径及优化技术（如缓存复用、频率调节）在性能与能耗方面的系统性基准测试。

Method: 通过GPU性能剖析与动态电压频率调节（DVFS），对比不同部署架构下的性能-能耗帕累托前沿，并引入新的同地部署基线进行评估。

Result: 预填充-解码分离的性能优势非必然，依赖请求负载与传输介质；分离虽支持独立调频，但因固有高能耗无法实现节能。

Conclusion: 预填充-解码分离架构需谨慎部署，其性能增益受限且无显著节能效果，实际应用应综合权衡负载特征与硬件配置。

Abstract: Different from traditional Large Language Model (LLM) serving that colocates the prefill and decode stages on the same GPU, disaggregated serving dedicates distinct GPUs to prefill and decode workload. Once the prefill GPU completes its task, the KV cache must be transferred to the decode GPU. While existing works have proposed various KV cache transfer paths across different memory and storage tiers, there remains a lack of systematic benchmarking that compares their performance and energy efficiency. Meanwhile, although optimization techniques such as KV cache reuse and frequency scaling have been utilized for disaggregated serving, their performance and energy implications have not been rigorously benchmarked. In this paper, we fill this research gap by re-evaluating prefill-decode disaggregation under different KV transfer mediums and optimization strategies. Specifically, we include a new colocated serving baseline and evaluate disaggregated setups under different KV cache transfer paths. Through GPU profiling using dynamic voltage and frequency scaling (DVFS), we identify and compare the performance-energy Pareto frontiers across all setups to evaluate the potential energy savings enabled by disaggregation. Our results show that performance benefits from prefill-decode disaggregation are not guaranteed and depend on the request load and KV transfer mediums. In addition, stage-wise independent frequency scaling enabled by disaggregation does not lead to energy saving due to inherently higher energy consumption of disaggregated serving.

</details>


### [7] [LookAhead: The Optimal Non-decreasing Index Policy for a Time-Varying Holding Cost problem](https://arxiv.org/abs/2601.08960)
*Keerthana Gurushankar,Zhouzi Li,Mor Harchol-Balter,Alan Scheller-Wolf*

Main category: cs.PF

TL;DR: 本文提出了一种名为LookAhead的最优调度策略，用于解决两类别M/M/1队列中的时变持有成本问题。


<details>
  <summary>Details</summary>
Motivation: 现有研究缺乏在非渐近条件下对时变持有成本问题的最优解，本文旨在填补这一空白。

Method: 通过引入“前瞻量”概念，设计一种基于未来成本而非当前成本的调度策略。

Result: 成功推导出适用于该特殊情形的最优非递减索引策略及对应的前瞻量。

Conclusion: LookAhead策略为时变持有成本问题提供了首个理论最优解，具有实际应用价值。

Abstract: In practice, the cost of delaying a job can grow as the job waits. Such behavior is modeled by the Time-Varying Holding Cost (TVHC) problem, where each job's instantaneous holding cost increases with its current age (a job's age is the time since it arrived). The goal of the TVHC problem is to find a scheduling policy that minimizes the time-average total holding cost across all jobs.
  However, no optimality results are known for the TVHC problem outside of the asymptotic regime. In this paper, we study a simple yet still challenging special case: A two-class M/M/1 queue in which class 1 jobs incur a non-decreasing, time-varying holding cost and class 2 jobs incur a constant holding cost.
  Our main contribution is deriving the first optimal (non-decreasing) index policy for this special case of the TVHC problem. Our optimal policy, called LookAhead, stems from the following idea: Rather than considering each job's current holding cost when making scheduling decisions, we should look at their cost some $X$ time into the future, where this $X$ is intuitively called the ``lookahead amount." This paper derives that optimal lookahead amount.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [8] [LAUDE: LLM-Assisted Unit Test Generation and Debugging of Hardware DEsigns](https://arxiv.org/abs/2601.08856)
*Deeksha Nandal,Riccardo Revalor,Soham Dan,Debjit Pal*

Main category: cs.SE

TL;DR: LAUDE是一个结合大语言模型与硬件设计语义理解的统一单元测试生成与调试框架，显著提升测试覆盖率与调试效率。


<details>
  <summary>Details</summary>
Motivation: 硬件设计中单元测试开发和调试过程复杂耗时，需要深度功能理解与创造力，亟需自动化辅助工具。

Method: 利用大语言模型的思维链推理能力，结合设计源码语义与执行信息，通过提示工程增强测试生成与调试能力。

Result: 在VerilogEval数据集上，LAUDE对组合与时序设计的缺陷检出率达100%与93%，调试成功率分别达93%与84%。

Conclusion: LAUDE有效提升了硬件设计单元测试的自动化水平，在测试生成与缺陷调试方面表现卓越。

Abstract: Unit tests are critical in the hardware design lifecycle to ensure that component design modules are functionally correct and conform to the specification before they are integrated at the system level. Thus developing unit tests targeting various design features requires deep understanding of the design functionality and creativity. When one or more unit tests expose a design failure, the debugging engineer needs to diagnose, localize, and debug the failure to ensure design correctness, which is often a painstaking and intense process. In this work, we introduce LAUDE, a unified unit-test generation and debugging framework for hardware designs that cross-pollinates the semantic understanding of the design source code with the Chain-of-Thought (CoT) reasoning capabilities of foundational Large-Language Models (LLMs). LAUDE integrates prompt engineering and design execution information to enhance its unit test generation accuracy and code debuggability. We apply LAUDE with closed- and open-source LLMs to a large corpus of buggy hardware design codes derived from the VerilogEval dataset, where generated unit tests detected bugs in up to 100% and 93% of combinational and sequential designs and debugged up to 93% and 84% of combinational and sequential designs, respectively.

</details>


### [9] [Revisiting Software Engineering Education in the Era of Large Language Models: A Curriculum Adaptation and Academic Integrity Framework](https://arxiv.org/abs/2601.08857)
*Mustafa Degerli*

Main category: cs.SE

TL;DR: 本文探讨大语言模型（LLM）对软件工程教育的理论影响，提出整合LLM的教学框架，并呼吁实证研究验证其效果。


<details>
  <summary>Details</summary>
Motivation: 当前软件工程教学仍强调手动编码能力，与LLM驱动的行业实践脱节，引发评估有效性与技能培养的担忧。

Method: 采用概念性研究方法，构建理论框架分析LLM如何改变核心能力，并针对土耳其计算机工程课程设计教学模型。

Result: 框架表明软件工程能力重心从构建转向批判、验证与人机协作；传统防抄袭机制不足，需转向过程透明模式。

Conclusion: 本文为课程改革提供结构化理论方案，但需后续纵向实证研究验证干预措施的长期学习成效。

Abstract: The integration of Large Language Models (LLMs), such as ChatGPT and GitHub Copilot, into professional workflows is increasingly reshaping software engineering practices. These tools have lowered the cost of code generation, explanation, and testing, while introducing new forms of automation into routine development tasks. In contrast, most of the software engineering and computer engineering curricula remain closely aligned with pedagogical models that equate manual syntax production with technical competence. This growing misalignment raises concerns regarding assessment validity, learning outcomes, and the development of foundational skills. Adopting a conceptual research approach, this paper proposes a theoretical framework for analyzing how generative AI alters core software engineering competencies and introduces a pedagogical design model for LLM-integrated education. Attention is given to computer engineering programs in Turkey, where centralized regulation, large class sizes, and exam-oriented assessment practices amplify these challenges. The framework delineates how problem analysis, design, implementation, and testing increasingly shift from construction toward critique, validation, and human-AI stewardship. In addition, the paper argues that traditional plagiarism-centric integrity mechanisms are becoming insufficient, motivating a transition toward a process transparency model. While this work provides a structured proposal for curriculum adaptation, it remains a theoretical contribution; the paper concludes by outlining the need for longitudinal empirical studies to evaluate these interventions and their long-term impacts on learning.

</details>


### [10] [Adaptive Trust Metrics for Multi-LLM Systems: Enhancing Reliability in Regulated Industries](https://arxiv.org/abs/2601.08858)
*Tejaswini Bollikonda*

Main category: cs.SE

TL;DR: 本文提出了一种用于多LLM生态系统的自适应信任度量框架，以提升在受监管环境中的模型可靠性。


<details>
  <summary>Details</summary>
Motivation: LLM在医疗、金融和法律等敏感领域的应用引发了对信任、问责和可靠性的担忧。

Method: 通过分析系统行为、评估多个LLM的不确定性并实施动态监控管道来构建框架。

Result: 在金融合规和医疗诊断案例中验证了该框架的实际适用性。

Conclusion: 自适应信任度量是推动受监管行业安全、可扩展AI应用的基础。

Abstract: Large Language Models (LLMs) are increasingly deployed in sensitive domains such as healthcare, finance, and law, yet their integration raises pressing concerns around trust, accountability, and reliability. This paper explores adaptive trust metrics for multi LLM ecosystems, proposing a framework for quantifying and improving model reliability under regulated constraints. By analyzing system behaviors, evaluating uncertainty across multiple LLMs, and implementing dynamic monitoring pipelines, the study demonstrates practical pathways for operational trustworthiness. Case studies from financial compliance and healthcare diagnostics illustrate the applicability of adaptive trust metrics in real world settings. The findings position adaptive trust measurement as a foundational enabler for safe and scalable AI adoption in regulated industries.

</details>


### [11] [Bridging the Gap: Empowering Small Models in Reliable OpenACC-based Parallelization via GEPA-Optimized Prompting](https://arxiv.org/abs/2601.08884)
*Samyak Jhaveri,Cristina V. Lopes*

Main category: cs.SE

TL;DR: 通过GEPA框架优化提示词，显著提升小规模LLM生成OpenACC指令的编译成功率与GPU加速性能，实现高效低成本的自动并行化。


<details>
  <summary>Details</summary>
Motivation: 降低GPU卸载编程门槛，解决LLM直接生成OpenACC指令时语法错误、不可编译或性能不佳的问题。

Method: 采用遗传帕累托（GEPA）框架，基于专家示例和结构化反馈迭代进化提示词，优化指令生成。

Result: GPT-4.1 Nano编译成功率从66.7%升至93.3%，GPT-5 Nano达100%，且21%程序获得超越CPU基准的GPU加速。

Conclusion: 提示词优化可有效释放小型LLM潜力，为HPC工作流提供稳定、高效的低成本自动并行化方案。

Abstract: OpenACC lowers the barrier to GPU offloading, but writing high-performing pragma remains complex, requiring deep domain expertise in memory hierarchies, data movement, and parallelization strategies. Large Language Models (LLMs) present a promising potential solution for automated parallel code generation, but naive prompting often results in syntactically incorrect directives, uncompilable code, or performance that fails to exceed CPU baselines. We present a systematic prompt optimization approach to enhance OpenACC pragma generation without the prohibitive computational costs associated with model post-training. Leveraging the GEPA (GEnetic-PAreto) framework, we iteratively evolve prompts through a reflective feedback loop. This process utilizes crossover and mutation of instructions, guided by expert-curated gold examples and structured feedback based on clause- and clause parameter-level mismatches between the gold and predicted pragma. In our evaluation on the PolyBench suite, we observe an increase in compilation success rates for programs annotated with OpenACC pragma generated using the optimized prompts compared to those annotated using the simpler initial prompt, particularly for the "nano"-scale models. Specifically, with optimized prompts, the compilation success rate for GPT-4.1 Nano surged from 66.7% to 93.3%, and for GPT-5 Nano improved from 86.7% to 100%, matching or surpassing the capabilities of their significantly larger, more expensive versions. Beyond compilation, the optimized prompts resulted in a 21% increase in the number of programs that achieve functional GPU speedups over CPU baselines. These results demonstrate that prompt optimization effectively unlocks the potential of smaller, cheaper LLMs in writing stable and effective GPU-offloading directives, establishing a cost-effective pathway to automated directive-based parallelization in HPC workflows.

</details>


### [12] [EZInput: A Cross-Environment Python Library for Easy UI Generation in Scientific Computing](https://arxiv.org/abs/2601.08859)
*Bruno M. Saraiva,Iván Hidalgo-Cenalmor,António D. Brito,Damián Martínez,Tayla Shakespeare,Guillaume Jacquemet,Ricardo Henriques*

Main category: cs.SE

TL;DR: EZInput 是一个跨运行环境的 Python 库，通过自动生成图形界面和持久化参数配置，降低算法使用门槛并提升科研可复现性。


<details>
  <summary>Details</summary>
Motivation: 解决科研人员在不同环境中重复配置参数、缺乏统一接口、难以记录与共享设置的问题。

Method: 采用声明式规范系统，自动检测环境并渲染界面，支持 Jupyter、Colab 和终端，通过 YAML 文件实现参数持久化与验证。

Result: 实现了‘一次编写，到处运行’的架构，用户无需编程即可操作算法，参数配置可跨平台复用并增强工作流可复现性。

Conclusion: EZInput 有效弥合了算法开发者与终端用户之间的技术鸿沟，提升了科学计算工具的易用性、一致性与可复现性。

Abstract: Researchers face a persistent barrier when applying computational algorithms with parameter configuration typically demanding programming skills, interfaces differing across environments, and settings rarely persisting between sessions. This fragmentation forces repetitive input, slows iterative exploration, and undermines reproducibility because parameter choices are difficult to record, share, and reuse. We present EZInput, a cross-runtime environment Python library enabling algorithm developers to automatically generate graphical user interfaces that make their computational tools accessible to end-users without programming expertise. EZInput employs a declarative specification system where developers define input requirements and validation constraints once; the library then handles environment detection, interface rendering, parameter validation, and session persistence across Jupyter notebooks, Google Colab, and terminal environments. This "write once, run anywhere" architecture enables researchers to prototype in notebooks and deploy identical parameter configurations for batch execution on remote systems without code changes or manual transcription. Parameter persistence, inspired by ImageJ/FIJI and adapted to Python workflows, saves and restores user configurations via lightweight YAML files, eliminating redundant input and producing shareable records that enhance reproducibility. EZInput supports diverse input types essential for scientific computing and it also includes built-in validation that ensures data integrity and clear feedback that reduces user friction.

</details>


### [13] [Build Code is Still Code: Finding the Antidote for Pipeline Poisoning](https://arxiv.org/abs/2601.08995)
*Brent Pappas,Paul Gazzillo*

Main category: cs.SE

TL;DR: 提出开发阶段隔离策略以防御构建系统投毒，原型工具Foreman成功检测XZ Utils攻击。


<details>
  <summary>Details</summary>
Motivation: 构建系统易受投毒攻击且现有安全工具忽视其防护，需新方法保障软件供应链安全。

Method: 通过建模构建自动化行为权限，实现开发阶段隔离，并开发原型工具Foreman。

Result: Foreman成功识别XZ Utils攻击中的恶意测试文件，验证方法有效性。

Conclusion: 未来应普及构建系统安全检查工具，使其与程序代码检查工具同等重要。

Abstract: Open source C code underpins society's computing infrastructure. Decades of work has helped harden C code against attackers, but C projects do not consist of only C code. C projects also contain build system code for automating development tasks like compilation, testing, and packaging. These build systems are critcal to software supply chain security and vulnerable to being poisoned, with the XZ Utils and SolarWinds attacks being recent examples. Existing techniques try to harden software supply chains by verifying software dependencies, but such methods ignore the build system itself. Similarly, classic software security checkers only analyze and monitor program code, not build system code. Moreover, poisoned build systems can easily circumvent tools for detecting program code vulnerabilities by disabling such checks. We present development phase isolation, a novel strategy for hardening build systems against poisoning by modeling the information and behavior permissions of build automation as if it were program code. We have prototyped this approach as a tool called Foreman, which successfully detects and warns about the poisoned test files involved in the XZ Utils attack. We outline our future plans to protect against pipeline poisoning by automatically checking development phase isolation. We envision a future where build system security checkers are as prevalent as program code checkers.

</details>


### [14] [On the Flakiness of LLM-Generated Tests for Industrial and Open-Source Database Management Systems](https://arxiv.org/abs/2601.08998)
*Alexander Berndt,Thomas Bach,Rainer Gemulla,Marcus Kessel,Sebastian Baltes*

Main category: cs.SE

TL;DR: 研究分析了LLM生成测试用例的不稳定性问题，发现其略高于现有测试，并揭示主要原因为依赖未保证顺序的集合。


<details>
  <summary>Details</summary>
Motivation: 探究LLM生成测试中不稳定性的普遍性及根本原因，以指导开发者合理使用LLM进行测试生成。

Method: 针对四个数据库系统，使用GPT-4o和Mistral-Large-Instruct-2407扩增测试套件，并通过人工检查分析不稳定性根源。

Result: 115个不稳定测试中63%源于'无序集合'；LLM会通过提示上下文将现有测试的不稳定性传递给新生成测试，且闭源系统中更显著。

Conclusion: 开发者应预期LLM生成测试可能出现的特定不稳定性类型，并在使用LLM时提供定制化上下文以减少问题。

Abstract: Flaky tests are a common problem in software testing. They produce inconsistent results when executed multiple times on the same code, invalidating the assumption that a test failure indicates a software defect. Recent work on LLM-based test generation has identified flakiness as a potential problem with generated tests. However, its prevalence and underlying causes are unclear. We examined the flakiness of LLM-generated tests in the context of four relational database management systems: SAP HANA, DuckDB, MySQL, and SQLite. We amplified test suites with two LLMs, GPT-4o and Mistral-Large-Instruct-2407, to assess the flakiness of the generated test cases. Our results suggest that generated tests have a slightly higher proportion of flaky tests compared to existing tests. Based on a manual inspection, we found that the most common root cause of flakiness was the reliance of a test on a certain order that is not guaranteed ("unordered collection"), which was present in 72 of 115 flaky tests (63%). Furthermore, both LLMs transferred the flakiness from the existing tests to the newly generated tests via the provided prompt context. Our experiments suggest that flakiness transfer is more prevalent in closed-source systems such as SAP HANA than in open-source systems. Our study informs developers on what types of flakiness to expect from LLM-generated tests. It also highlights the importance of providing LLMs with tailored context when employing LLMs for test generation.

</details>


### [15] [SafePlanner: Testing Safety of the Automated Driving System Plan Model](https://arxiv.org/abs/2601.09171)
*Dohyun Kim,Sanggu Han,Sangmin Woo,Joonha Jang,Jaehoon Kim,Changhun Song,Yongdae Kim*

Main category: cs.SE

TL;DR: SafePlanner 是一种针对自动驾驶系统规划模块的系统性测试框架，通过结构分析与引导式模糊测试发现安全关键缺陷。


<details>
  <summary>Details</summary>
Motivation: 解决自动驾驶规划模块中测试场景缺乏结构性语义及难以检测危险行为的问题。

Method: 对规划模型代码进行结构分析，提取可行场景转换，结合NPC行为构建测试场景，并采用引导式模糊测试探索行为空间。

Result: 在百度Apollo上生成20635个测试用例，发现520个危险行为（归为15类根因），修复4类后无副作用，功能覆盖率达83.63%，决策覆盖率达63.22%。

Conclusion: SafePlanner 在缺陷发现数量与测试效率上优于基线方法，能有效提升自动驾驶规划模块的安全性。

Abstract: In this work, we present SafePlanner, a systematic testing framework for identifying safety-critical flaws in the Plan model of Automated Driving Systems (ADS). SafePlanner targets two core challenges: generating structurally meaningful test scenarios and detecting hazardous planning behaviors. To maximize coverage, SafePlanner performs a structural analysis of the Plan model implementation - specifically, its scene-transition logic and hierarchical control flow - and uses this insight to extract feasible scene transitions from code. It then composes test scenarios by combining these transitions with non-player vehicle (NPC) behaviors. Guided fuzzing is applied to explore the behavioral space of the Plan model under these scenarios. We evaluate SafePlanner on Baidu Apollo, a production-grade level 4 ADS. It generates 20635 test cases and detects 520 hazardous behaviors, grouped into 15 root causes through manual analysis. For four of these, we applied patches based on our analysis; the issues disappeared, and no apparent side effects were observed. SafePlanner achieves 83.63 percent function and 63.22 percent decision coverage on the Plan model, outperforming baselines in both bug discovery and efficiency.

</details>


### [16] [Towards a Metadata Schema for Energy Research Software](https://arxiv.org/abs/2601.09456)
*Stephan Ferenz,Oliver Werth,Astrid Nieße*

Main category: cs.SE

TL;DR: 本文为能源研究软件开发了一种元数据模式，通过需求分析和用户测试验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 许多领域（如能源研究）缺乏成熟的元数据模式，影响研究软件的可发现性和可重用性。

Method: 基于需求分析设计元数据模式，并通过用户测试进行评估。

Result: 该模式在形式化与互操作性之间取得平衡，同时满足能源研究人员的具体需求；良好的信息展示对元数据创建至关重要。

Conclusion: 设计适用于特定领域的元数据模式面临挑战但也充满机遇，本研究为此提供了有益见解。

Abstract: Domain-specific metadata schemas are essential to improve the findability and reusability of research software and to follow the FAIR4RS principles. However, many domains, including energy research, lack established metadata schemas. To address this gap, we developed a metadata schema for energy research software based on a requirement analysis and evaluated it through user testing. Our results show that the schema balances the need for formalization and interoperability, while also meeting the specific needs of energy researchers. Meanwhile, the testing showed that a good presentation of the required information is key to enable researchers to create the required metadata. This paper provides insights into the challenges and opportunities of designing a metadata schema for energy research software.

</details>


### [17] [Analyzing GitHub Issues and Pull Requests in nf-core Pipelines: Insights into nf-core Pipeline Repositories](https://arxiv.org/abs/2601.09612)
*Khairul Alam,Banani Roy*

Main category: cs.SE

TL;DR: 本研究通过分析nf-core社区25,173个问题与拉取请求，识别出13项关键挑战，并发现标签和代码片段显著提升解决效率，为提升科学工作流的可用性与可持续性提供实证依据。


<details>
  <summary>Details</summary>
Motivation: 了解nf-core用户在开发与维护标准化计算流程中所面临的实际困难，以改进其可用性、可持续性和可复现性。

Method: 采用BERTopic建模分析海量议题数据，结合统计方法评估解决效率影响因素，如标签与代码片段的作用。

Result: 89.38%的问题最终关闭，半数在三天内解决；工具开发与仓库维护挑战最大；标签（δ=0.94）和代码片段（δ=0.50）显著提高解决概率。

Conclusion: 研究揭示了nf-core协作开发中的核心痛点与高效管理实践，为优化科学工作流系统的设计与支持机制提供了可操作的洞见。

Abstract: Scientific Workflow Management Systems (SWfMSs) such as Nextflow have become essential software frameworks for conducting reproducible, scalable, and portable computational analyses in data-intensive fields like genomics, transcriptomics, and proteomics. Building on Nextflow, the nf-core community curates standardized, peer-reviewed pipelines that follow strict testing, documentation, and governance guidelines. Despite its broad adoption, little is known about the challenges users face during the development and maintenance of these pipelines. This paper presents an empirical study of 25,173 issues and pull requests from these pipelines to uncover recurring challenges, management practices, and perceived difficulties. Using BERTopic modeling, we identify 13 key challenges, including pipeline development and integration, bug fixing, integrating genomic data, managing CI configurations, and handling version updates. We then examine issue resolution dynamics, showing that 89.38\% of issues and pull requests are eventually closed, with half resolved within three days. Statistical analysis reveals that the presence of labels (large effect, $δ$ = 0.94) and code snippets (medium effect, $δ$ = 0.50) significantly improve resolution likelihood. Further analysis reveals that tool development and repository maintenance poses the most significant challenges, followed by testing pipelines and CI configurations, and debugging containerized pipelines. Overall, this study provides actionable insights into the collaborative development and maintenance of nf-core pipelines, highlighting opportunities to enhance their usability, sustainability, and reproducibility.

</details>


### [18] [SysPro: Reproducing System-level Concurrency Bugs from Bug Reports](https://arxiv.org/abs/2601.09616)
*Tarannum Shaila Zaman,Zhihui Yan,Chen Wang,Chadni Islam,Jiangfan Shi,Tingting Yu*

Main category: cs.SE

TL;DR: SysPro自动从bug报告中提取系统调用信息并生成输入数据，通过动态插桩高效复现系统级并发bug。


<details>
  <summary>Details</summary>
Motivation: 现有工具难以从非结构化自然语言报告中提取细节并控制系统调用交错顺序，导致复现并发bug困难。

Method: 结合信息检索、正则匹配与分类分区法生成输入数据，定位源码中系统调用位置，并通过动态插桩实现精确交错控制。

Result: 在真实基准测试中，SysPro能有效且高效地定位并复现系统级并发bug。

Conclusion: SysPro为从自然语言报告中自动化复现系统级并发bug提供了一种实用解决方案。

Abstract: Reproducing system-level concurrency bugs requires both input data and the precise interleaving order of system calls. This process is challenging because such bugs are non-deterministic, and bug reports often lack the detailed information needed. Additionally, the unstructured nature of reports written in natural language makes it difficult to extract necessary details. Existing tools are inadequate to reproduce these bugs due to their inability to manage the specific interleaving at the system call level. To address these challenges, we propose SysPro, a novel approach that automatically extracts relevant system call names from bug reports and identifies their locations in the source code. It generates input data by utilizing information retrieval, regular expression matching, and the category-partition method. This extracted input and interleaving data are then used to reproduce bugs through dynamic source code instrumentation. Our empirical study on real-world benchmarks demonstrates that SysPro is both effective and efficient at localizing and reproducing system-level concurrency bugs from bug reports.

</details>


### [19] [How well LLM-based test generation techniques perform with newer LLM versions?](https://arxiv.org/abs/2601.09695)
*Michael Konstantinou,Renzo Degiovanni,Mike Papadakis*

Main category: cs.SE

TL;DR: 本文研究发现，使用当前版本的大型语言模型（LLM）直接生成单元测试，在覆盖率和变异分数等指标上优于四种现有先进方法，且成本相当；进一步提出优先针对类再补充未覆盖方法的策略，可减少约20%的LLM调用次数。


<details>
  <summary>Details</summary>
Motivation: 现有基于LLM的测试生成技术常与过时基线比较，可能高估其效果，需验证在当前更强LLM下是否仍具优势。

Method: 复现四种先进工具（HITS、SymPrompt、TestSpark、CoverUp），并在393个类、3657个方法上对比其与纯LLM方法的效果与效率。

Result: 纯LLM方法在行覆盖率（+17.72%）、分支覆盖率（+19.80%）、变异分数（+20.92%）均优于现有方法，且成本相近；按类优先生成再补漏策略可减少20%请求量。

Conclusion: 当前LLM能力已足以支撑高效测试生成，工程化增强组件未必必要；优化调用粒度可显著降低成本。

Abstract: The rapid evolution of Large Language Models (LLMs) has strongly impacted software engineering, leading to a growing number of studies on automated unit test generation. However, the standalone use of LLMs without post-processing has proven insufficient, often producing tests that fail to compile or achieve high coverage. Several techniques have been proposed to address these issues, reporting improvements in test compilation and coverage. While important, LLM-based test generation techniques have been evaluated against relatively weak baselines (for todays' standards), i.e., old LLM versions and relatively weak prompts, which may exacerbate the performance contribution of the approaches. In other words, stronger (newer) LLMs may obviate any advantage these techniques bring. We investigate this issue by replicating four state-of-the-art LLM-based test generation tools, HITS, SymPrompt, TestSpark, and CoverUp that include engineering components aimed at guiding the test generation process through compilation and execution feedback, and evaluate their relative effectiveness and efficiency over a plain LLM test generation method. We integrate current LLM versions in all approaches and run an experiment on 393 classes and 3,657 methods. Our results show that the plain LLM approach can outperform previous state-of-the-art approaches in all test effectiveness metrics we used: line coverage (by 17.72%), branch coverage (by 19.80%) and mutation score (by 20.92%), and it does so at a comparable cost (LLM queries). We also observe that the granularity at which the plain LLM is applied has a significant impact on the cost. We therefore propose targeting first the program classes, where test generation is more efficient, and then the uncovered methods to reduce the number of LLM requests. This strategy achieves comparable (slightly higher) effectiveness while requiring about 20% fewer LLM requests.

</details>


### [20] [ShortCoder: Knowledge-Augmented Syntax Optimization for Token-Efficient Code Generation](https://arxiv.org/abs/2601.09703)
*Sicong Liu,Yanxian Huang,Mingwei Liu,Jiachi Chen,Ensheng Shi,Yuchi Ma,Hongyu Zhang,Yin Zhang,Yanlin Wang*

Main category: cs.SE

TL;DR: ShortCoder通过语法简化规则和混合数据合成方法，提升代码生成效率并保持语义一致性。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型在代码生成阶段存在资源消耗高、效率低的问题，需优化生成过程。

Method: 提出基于AST的Python语法简化规则、混合数据合成管道及精调策略，构建ShorterCodeBench语料库并训练模型。

Result: 在HumanEval上生成效率提升18.1%-37.8%，同时保持代码功能与可读性。

Conclusion: ShortCoder有效平衡了代码简洁性与功能性，为高效代码生成提供新方案。

Abstract: Code generation tasks aim to automate the conversion of user requirements into executable code, significantly reducing manual development efforts and enhancing software productivity. The emergence of large language models (LLMs) has significantly advanced code generation, though their efficiency is still impacted by certain inherent architectural constraints. Each token generation necessitates a complete inference pass, requiring persistent retention of contextual information in memory and escalating resource consumption. While existing research prioritizes inference-phase optimizations such as prompt compression and model quantization, the generation phase remains underexplored. To tackle these challenges, we propose a knowledge-infused framework named ShortCoder, which optimizes code generation efficiency while preserving semantic equivalence and readability. In particular, we introduce: (1) ten syntax-level simplification rules for Python, derived from AST-preserving transformations, achieving 18.1% token reduction without functional compromise; (2) a hybrid data synthesis pipeline integrating rule-based rewriting with LLM-guided refinement, producing ShorterCodeBench, a corpus of validated tuples of original code and simplified code with semantic consistency; (3) a fine-tuning strategy that injects conciseness awareness into the base LLMs. Extensive experimental results demonstrate that ShortCoder consistently outperforms state-of-the-art methods on HumanEval, achieving an improvement of 18.1%-37.8% in generation efficiency over previous methods while ensuring the performance of code generation.

</details>
