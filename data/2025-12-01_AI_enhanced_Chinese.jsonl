{"id": "2511.22334", "categories": ["cs.PF", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.22334", "abs": "https://arxiv.org/abs/2511.22334", "authors": ["Pablo Prieto", "Pablo Abad"], "title": "Edge Deployment of Small Language Models, a comprehensive comparison of CPU, GPU and NPU backends", "comment": "8 pages, 9 figures", "summary": "Edge computing processes data where it is generated, enabling faster decisions, lower bandwidth usage, and improved privacy. However, edge devices typically operate under strict constraints on processing power, memory, and energy consumption, making them unsuitable for large language models (LLMs). Fortunately, Small Language Models (SLMs) offer lightweight alternatives that bring AI inference to resource-constrained environments by significantly reducing computational cost while remaining suitable for specialization and customization. In this scenario, selecting the hardware platform that best balances performance and efficiency for SLM inference is challenging due to strict resource limitations. To address this issue, this study evaluates the inference performance and energy efficiency of commercial CPUs (Intel and ARM), GPUs (NVIDIA), and NPUs (RaiderChip) for running SLMs. GPUs, the usual platform of choice, are compared against commercial NPUs and recent multi-core CPUs. While NPUs leverage custom hardware designs optimized for computation, modern CPUs increasingly incorporate dedicated features targeting language-model workloads. Using a common execution framework and a suite of state-of-the-art SLMs, we analyze both maximum achievable performance and processing and energy efficiency across commercial solutions available for each platform. The results indicate that specialized backends outperform general-purpose CPUs, with NPUs achieving the highest performance by a wide margin. Bandwidth normalization proves essential for fair cross-architecture comparisons. Although low-power ARM processors deliver competitive results when energy usage is considered, metrics that combine performance and power (such as EDP) again highlight NPUs as the dominant architecture. These findings show that designs optimized for both efficiency and performance offer a clear advantage for edge workloads.", "AI": {"tldr": "\u8be5\u7814\u7a76\u8bc4\u4f30\u4e86\u5728\u8fb9\u7f18\u8bbe\u5907\u4e0a\u8fd0\u884c\u5c0f\u578b\u8bed\u8a00\u6a21\u578b\uff08SLMs\uff09\u65f6\uff0c\u4e0d\u540c\u786c\u4ef6\u5e73\u53f0\uff08\u5305\u62ecCPU\u3001GPU\u548cNPU\uff09\u7684\u63a8\u7406\u6027\u80fd\u4e0e\u80fd\u6548\u8868\u73b0\uff0c\u53d1\u73b0\u4e13\u7528\u786c\u4ef6\uff08\u5c24\u5176\u662fNPU\uff09\u5728\u6027\u80fd\u548c\u80fd\u6548\u7efc\u5408\u6307\u6807\u4e0a\u663e\u8457\u4f18\u4e8e\u901a\u7528CPU\u3002", "motivation": "\u8fb9\u7f18\u8bbe\u5907\u8d44\u6e90\u53d7\u9650\uff0c\u96be\u4ee5\u652f\u6301\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u800c\u5c0f\u578b\u8bed\u8a00\u6a21\u578b\uff08SLMs\uff09\u867d\u9002\u5408\u90e8\u7f72\uff0c\u4f46\u5982\u4f55\u5728\u6709\u9650\u8d44\u6e90\u4e0b\u9009\u62e9\u6700\u4f18\u786c\u4ef6\u5e73\u53f0\u4ecd\u5177\u6311\u6218\u3002", "method": "\u5728\u7edf\u4e00\u6267\u884c\u6846\u67b6\u4e0b\uff0c\u4f7f\u7528\u591a\u79cd\u5148\u8fdbSLMs\uff0c\u5728\u5546\u7528CPU\uff08Intel\u3001ARM\uff09\u3001GPU\uff08NVIDIA\uff09\u548cNPU\uff08RaiderChip\uff09\u5e73\u53f0\u4e0a\u8bc4\u4f30\u5176\u63a8\u7406\u6027\u80fd\u4e0e\u80fd\u6548\uff0c\u5e76\u91c7\u7528\u5e26\u5bbd\u5f52\u4e00\u5316\u8fdb\u884c\u516c\u5e73\u8de8\u67b6\u6784\u6bd4\u8f83\u3002", "result": "NPU\u5728\u6027\u80fd\u4e0a\u5927\u5e45\u9886\u5148\uff1b\u4f4e\u529f\u8017ARM\u5904\u7406\u5668\u5728\u80fd\u6548\u65b9\u9762\u8868\u73b0\u826f\u597d\uff0c\u4f46\u7efc\u5408\u6027\u80fd\u4e0e\u529f\u8017\u7684\u6307\u6807\uff08\u5982EDP\uff09\u4ecd\u663e\u793aNPU\u6700\u5177\u4f18\u52bf\u3002", "conclusion": "\u9488\u5bf9\u6548\u7387\u4e0e\u6027\u80fd\u5171\u540c\u4f18\u5316\u7684\u4e13\u7528\u786c\u4ef6\uff08\u5982NPU\uff09\u5728\u8fb9\u7f18\u8ba1\u7b97\u573a\u666f\u4e2d\u5177\u6709\u660e\u663e\u4f18\u52bf\uff0c\u66f4\u9002\u5408\u90e8\u7f72SLMs\u3002"}}
{"id": "2511.21936", "categories": ["cs.NI", "eess.SY"], "pdf": "https://arxiv.org/pdf/2511.21936", "abs": "https://arxiv.org/abs/2511.21936", "authors": ["T. Rebolo", "A. Grilo", "C. Ribeiro"], "title": "Secure Command, Control and Communications Systems (C3) for Army UxVs", "comment": "13 pages", "summary": "Unmanned Vehicles (UxVs) are increasingly used in modern military operations for reconnaissance, surveillance, and strike missions, enhancing situational awareness while reducing risk to personnel. Their affordability and rapid deployment have encouraged the adoption of commercial solutions. However, many rely on insecure protocols such as MAVLink, which lack authentication and encryption mechanisms. This paper designed, implemented, and evaluated a new secure command-and-control architecture that ensures confidentiality, integrity, and authentication (CIA) while supporting real-time control delegation between Ground Control Stations (GCSs). The proposed solution, named New Command and Control System (NC2S), enforces a zero-trust model integrating hierarchical credential-based privileges to regulate access and control among Tactical Commanders (TC), GCSs, and UxVs. It employs mutual Transport Layer Security (mTLS) with Elliptic Curve Digital Signature Algorithm (ECDSA) certificates and Elliptic Curve Diffie-Hellman (ECDH) key exchange, while message integrity is ensured through Hash-based Message Authentication Codes (HMAC). Multiple lightweight protocols were developed for credential management, key renewal, and control handover. The NC2S prototype was experimentally validated over Wi-Fi and Rohde&Schwarz HR-5000H tactical radios. Results showed that HR-5000H links introduce latencies roughly two orders of magnitude higher than broadband technologies (e.g., Wi-Fi or 5G&Beyond technologies) but are still able to maintain stable communication with minimal message loss, making them suitable for the NC2S links among TC terminals and GCSs.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u5e76\u5b9e\u73b0\u4e86\u4e00\u79cd\u540d\u4e3aNC2S\u7684\u5b89\u5168\u65e0\u4eba\u8f7d\u5177\u6307\u6325\u63a7\u5236\u7cfb\u7edf\uff0c\u91c7\u7528\u96f6\u4fe1\u4efb\u6a21\u578b\u548c\u57fa\u4e8e\u8bc1\u4e66\u7684\u5206\u5c42\u6743\u9650\u673a\u5236\uff0c\u901a\u8fc7mTLS\u3001ECDSA\u3001ECDH\u548cHMAC\u4fdd\u969c\u901a\u4fe1\u7684\u673a\u5bc6\u6027\u3001\u5b8c\u6574\u6027\u548c\u8ba4\u8bc1\u6027\uff0c\u5e76\u5728Wi-Fi\u4e0e\u6218\u672f\u65e0\u7ebf\u7535\u4e0a\u9a8c\u8bc1\u4e86\u5176\u53ef\u884c\u6027\u3002", "motivation": "\u5f53\u524d\u519b\u7528\u65e0\u4eba\u8f7d\u5177\u5e7f\u6cdb\u4f7f\u7528\u5982MAVLink\u7b49\u7f3a\u4e4f\u8ba4\u8bc1\u4e0e\u52a0\u5bc6\u673a\u5236\u7684\u4e0d\u5b89\u5168\u534f\u8bae\uff0c\u5b58\u5728\u5b89\u5168\u98ce\u9669\uff1b\u540c\u65f6\u9700\u652f\u6301\u5730\u9762\u63a7\u5236\u7ad9\u4e4b\u95f4\u7684\u5b9e\u65f6\u63a7\u5236\u6743\u79fb\u4ea4\uff0c\u56e0\u6b64\u4e9f\u9700\u4e00\u79cd\u517c\u987e\u5b89\u5168\u6027\u4e0e\u5b9e\u65f6\u6027\u7684\u65b0\u578b\u6307\u6325\u63a7\u5236\u67b6\u6784\u3002", "method": "\u8bbe\u8ba1\u5e76\u5b9e\u73b0\u4e86NC2S\u7cfb\u7edf\uff0c\u91c7\u7528\u96f6\u4fe1\u4efb\u67b6\u6784\uff0c\u7ed3\u5408\u5206\u5c42\u51ed\u8bc1\u6743\u9650\u7ba1\u7406\uff0c\u5229\u7528mTLS\u914d\u5408ECDSA\u8bc1\u4e66\u548cECDH\u5bc6\u94a5\u4ea4\u6362\u4fdd\u969c\u901a\u4fe1\u5b89\u5168\uff0c\u5e76\u901a\u8fc7HMAC\u786e\u4fdd\u6d88\u606f\u5b8c\u6574\u6027\uff1b\u540c\u65f6\u5f00\u53d1\u4e86\u8f7b\u91cf\u7ea7\u534f\u8bae\u7528\u4e8e\u51ed\u8bc1\u7ba1\u7406\u3001\u5bc6\u94a5\u66f4\u65b0\u548c\u63a7\u5236\u6743\u4ea4\u63a5\uff0c\u5e76\u5728Wi-Fi\u4e0eHR-5000H\u6218\u672f\u65e0\u7ebf\u7535\u4e0a\u8fdb\u884c\u539f\u578b\u6d4b\u8bd5\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cHR-5000H\u6218\u672f\u65e0\u7ebf\u7535\u94fe\u8def\u7684\u5ef6\u8fdf\u6bd4\u5bbd\u5e26\u6280\u672f\uff08\u5982Wi-Fi\u62165G\uff09\u9ad8\u7ea6\u4e24\u4e2a\u6570\u91cf\u7ea7\uff0c\u4f46\u4ecd\u80fd\u7ef4\u6301\u7a33\u5b9a\u901a\u4fe1\u4e14\u6d88\u606f\u4e22\u5931\u6781\u5c11\uff0c\u9002\u7528\u4e8eTC\u7ec8\u7aef\u4e0eGCS\u4e4b\u95f4\u7684NC2S\u94fe\u8def\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684NC2S\u67b6\u6784\u6709\u6548\u89e3\u51b3\u4e86\u73b0\u6709\u65e0\u4eba\u8f7d\u5177\u6307\u6325\u63a7\u5236\u7cfb\u7edf\u4e2d\u7684\u5b89\u5168\u7f3a\u9677\uff0c\u5728\u4fdd\u969cCIA\u4e09\u8981\u7d20\u7684\u540c\u65f6\u652f\u6301\u5b9e\u65f6\u63a7\u5236\u6743\u79fb\u4ea4\uff0c\u4e14\u5728\u6218\u672f\u901a\u4fe1\u73af\u5883\u4e0b\u5177\u5907\u5b9e\u7528\u53ef\u884c\u6027\u3002"}}
{"id": "2511.22442", "categories": ["cs.PF", "cs.AI", "cs.CV", "cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2511.22442", "abs": "https://arxiv.org/abs/2511.22442", "authors": ["S\u00e9bastien Pi\u00e9rard", "Adrien Deli\u00e8ge", "Marc Van Droogenbroeck"], "title": "What Is the Optimal Ranking Score Between Precision and Recall? We Can Always Find It and It Is Rarely $F_1$", "comment": null, "summary": "Ranking methods or models based on their performance is of prime importance but is tricky because performance is fundamentally multidimensional. In the case of classification, precision and recall are scores with probabilistic interpretations that are both important to consider and complementary. The rankings induced by these two scores are often in partial contradiction. In practice, therefore, it is extremely useful to establish a compromise between the two views to obtain a single, global ranking. Over the last fifty years or so,it has been proposed to take a weighted harmonic mean, known as the F-score, F-measure, or $F_\u03b2$. Generally speaking, by averaging basic scores, we obtain a score that is intermediate in terms of values. However, there is no guarantee that these scores lead to meaningful rankings and no guarantee that the rankings are good tradeoffs between these base scores. Given the ubiquity of $F_\u03b2$ scores in the literature, some clarification is in order. Concretely: (1) We establish that $F_\u03b2$-induced rankings are meaningful and define a shortest path between precision- and recall-induced rankings. (2) We frame the problem of finding a tradeoff between two scores as an optimization problem expressed with Kendall rank correlations. We show that $F_1$ and its skew-insensitive version are far from being optimal in that regard. (3) We provide theoretical tools and a closed-form expression to find the optimal value for $\u03b2$ for any distribution or set of performances, and we illustrate their use on six case studies.", "AI": {"tldr": "\u672c\u6587\u91cd\u65b0\u5ba1\u89c6\u4e86\u5e7f\u6cdb\u4f7f\u7528\u7684F\u03b2\u5206\u6570\u5728\u6392\u5e8f\u6a21\u578b\u6027\u80fd\u65f6\u7684\u6709\u6548\u6027\uff0c\u6307\u51fa\u5176\u867d\u5e38\u7528\u4e8e\u8c03\u548c\u7cbe\u786e\u7387\u4e0e\u53ec\u56de\u7387\u4e4b\u95f4\u7684\u77db\u76fe\u6392\u5e8f\uff0c\u4f46\u5e76\u4e0d\u603b\u80fd\u63d0\u4f9b\u6700\u4f18\u6298\u4e2d\u3002\u4f5c\u8005\u901a\u8fc7Kendall\u79e9\u76f8\u5173\u6784\u5efa\u4f18\u5316\u6846\u67b6\uff0c\u8bc1\u660eF1\u7b49\u5e38\u7528\u6307\u6807\u5e76\u975e\u7406\u60f3\u9009\u62e9\uff0c\u5e76\u63d0\u51fa\u4e00\u79cd\u901a\u7528\u65b9\u6cd5\u4ee5\u95ed\u5f0f\u89e3\u786e\u5b9a\u5bf9\u4efb\u610f\u6027\u80fd\u5206\u5e03\u6700\u4f18\u7684\u03b2\u503c\u3002", "motivation": "\u5728\u5206\u7c7b\u4efb\u52a1\u4e2d\uff0c\u7cbe\u786e\u7387\u548c\u53ec\u56de\u7387\u662f\u4e24\u4e2a\u4e92\u8865\u4f46\u5e38\u76f8\u4e92\u51b2\u7a81\u7684\u91cd\u8981\u6307\u6807\uff0c\u5b9e\u8df5\u4e2d\u5e38\u4f7f\u7528F\u03b2\u5206\u6570\uff08\u5982F1\uff09\u5bf9\u5176\u52a0\u6743\u8c03\u548c\u5e73\u5747\u4ee5\u83b7\u5f97\u5355\u4e00\u6392\u5e8f\u3002\u7136\u800c\uff0c\u8fd9\u79cd\u5e73\u5747\u662f\u5426\u80fd\u4ea7\u751f\u6709\u610f\u4e49\u4e14\u5e73\u8861\u7684\u6392\u5e8f\u5c1a\u7f3a\u4e4f\u7406\u8bba\u4fdd\u8bc1\uff0c\u4e9f\u9700\u6f84\u6e05F\u03b2\u5728\u6392\u5e8f\u4e2d\u7684\u4f5c\u7528\u53ca\u5176\u6700\u4f18\u53c2\u6570\u9009\u62e9\u3002", "method": "\u4f5c\u8005\u5c06\u5bfb\u627e\u4e24\u4e2a\u6307\u6807\u95f4\u6298\u4e2d\u6392\u5e8f\u7684\u95ee\u9898\u5f62\u5f0f\u5316\u4e3a\u57fa\u4e8eKendall\u79e9\u76f8\u5173\u7684\u4f18\u5316\u95ee\u9898\uff1b\u901a\u8fc7\u7406\u8bba\u5206\u6790\u5efa\u7acbF\u03b2\u8bf1\u5bfc\u6392\u5e8f\u4e0e\u7cbe\u786e\u7387\u3001\u53ec\u56de\u7387\u6392\u5e8f\u4e4b\u95f4\u7684\u5173\u7cfb\uff1b\u5e76\u63a8\u5bfc\u51fa\u9488\u5bf9\u4efb\u610f\u6027\u80fd\u5206\u5e03\u53ef\u8ba1\u7b97\u6700\u4f18\u03b2\u503c\u7684\u95ed\u5f0f\u8868\u8fbe\u5f0f\u3002", "result": "\u7814\u7a76\u53d1\u73b0F1\u53ca\u5176\u5bf9\u504f\u659c\u4e0d\u654f\u611f\u7684\u53d8\u4f53\u5728\u6298\u4e2d\u6392\u5e8f\u65b9\u9762\u8fdc\u975e\u6700\u4f18\uff1b\u6240\u63d0\u51fa\u7684\u7406\u8bba\u5de5\u5177\u80fd\u5728\u516d\u4e2a\u6848\u4f8b\u7814\u7a76\u4e2d\u6709\u6548\u786e\u5b9a\u6700\u4f18\u03b2\uff0c\u4ece\u800c\u751f\u6210\u66f4\u5408\u7406\u7684\u5168\u5c40\u6392\u5e8f\u3002", "conclusion": "F\u03b2\u5206\u6570\u867d\u88ab\u5e7f\u6cdb\u4f7f\u7528\uff0c\u4f46\u5176\u9ed8\u8ba4\u53c2\u6570\uff08\u5982\u03b2=1\uff09\u672a\u5fc5\u80fd\u63d0\u4f9b\u7cbe\u786e\u7387\u4e0e\u53ec\u56de\u7387\u4e4b\u95f4\u7684\u6700\u4f73\u6743\u8861\uff1b\u901a\u8fc7\u672c\u6587\u63d0\u51fa\u7684\u4f18\u5316\u6846\u67b6\u548c\u95ed\u5f0f\u89e3\uff0c\u53ef\u4e3a\u5177\u4f53\u5e94\u7528\u573a\u666f\u5b9a\u5236\u6700\u4f18\u03b2\uff0c\u4ece\u800c\u83b7\u5f97\u66f4\u6709\u610f\u4e49\u7684\u6a21\u578b\u6392\u5e8f\u3002"}}
{"id": "2511.22467", "categories": ["cs.PF", "cs.RO"], "pdf": "https://arxiv.org/pdf/2511.22467", "abs": "https://arxiv.org/abs/2511.22467", "authors": ["Fran\u00e7ois Provost", "Faisal Hawlader", "Mehdi Testouri", "Rapha\u00ebl Frank"], "title": "Motion-to-Motion Latency Measurement Framework for Connected and Autonomous Vehicle Teleoperation", "comment": null, "summary": "Latency is a key performance factor for the teleoperation of Connected and Autonomous Vehicles (CAVs). It affects how quickly an operator can perceive changes in the driving environment and apply corrective actions. Most existing work focuses on Glass-to-Glass (G2G) latency, which captures delays only in the video pipeline. However, there is no standard method for measuring Motion-to-Motion (M2M) latency, defined as the delay between the physical steering movement of the remote operator and the corresponding steering motion in the vehicle. This paper presents an M2M latency measurement framework that uses Hall-effect sensors and two synchronized Raspberry Pi~5 devices. The system records interrupt-based timestamps on both sides to estimate M2M latency, independently of the underlying teleoperation architecture. Precision tests show an accuracy of 10--15~ms, while field results indicate that actuator delays dominate M2M latency, with median values above 750~ms.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u6d4b\u91cf\u8fdc\u7a0b\u64cd\u63a7\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u4e2d\u201c\u52a8\u4f5c\u5230\u52a8\u4f5c\u201d\uff08M2M\uff09\u5ef6\u8fdf\u7684\u6846\u67b6\uff0c\u5229\u7528\u970d\u5c14\u6548\u5e94\u4f20\u611f\u5668\u548c\u4e24\u4e2a\u540c\u6b65\u7684\u6811\u8393\u6d3e5\u8bbe\u5907\uff0c\u5b9e\u73b0\u4e8610\u201315\u6beb\u79d2\u7684\u6d4b\u91cf\u7cbe\u5ea6\uff0c\u5e76\u53d1\u73b0\u6267\u884c\u5668\u5ef6\u8fdf\u662fM2M\u5ef6\u8fdf\u7684\u4e3b\u8981\u6765\u6e90\uff0c\u4e2d\u4f4d\u6570\u8d85\u8fc7750\u6beb\u79d2\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u4e3b\u8981\u5173\u6ce8\u89c6\u9891\u4f20\u8f93\u7aef\u5230\u7aef\uff08G2G\uff09\u5ef6\u8fdf\uff0c\u7f3a\u4e4f\u5bf9\u8fdc\u7a0b\u64cd\u4f5c\u4e2d\u7269\u7406\u52a8\u4f5c\u5230\u8f66\u8f86\u54cd\u5e94\uff08M2M\uff09\u5ef6\u8fdf\u7684\u6807\u51c6\u6d4b\u91cf\u65b9\u6cd5\uff0c\u800c\u8be5\u5ef6\u8fdf\u76f4\u63a5\u5f71\u54cd\u64cd\u4f5c\u54cd\u5e94\u6027\u80fd\u3002", "method": "\u91c7\u7528\u970d\u5c14\u6548\u5e94\u4f20\u611f\u5668\u4e0e\u4e24\u4e2a\u65f6\u95f4\u540c\u6b65\u7684\u6811\u8393\u6d3e5\u8bbe\u5907\uff0c\u5728\u64cd\u4f5c\u7aef\u548c\u8f66\u8f86\u7aef\u5206\u522b\u8bb0\u5f55\u57fa\u4e8e\u4e2d\u65ad\u7684\u65f6\u95f4\u6233\uff0c\u4ece\u800c\u72ec\u7acb\u4e8e\u5177\u4f53\u8fdc\u7a0b\u64cd\u63a7\u67b6\u6784\u5730\u4f30\u7b97M2M\u5ef6\u8fdf\u3002", "result": "\u7cfb\u7edf\u7cbe\u5ea6\u6d4b\u8bd5\u8fbe\u523010\u201315\u6beb\u79d2\uff1b\u5b9e\u5730\u6d4b\u8bd5\u8868\u660e\u6267\u884c\u5668\u5ef6\u8fdf\u4e3b\u5bfc\u4e86M2M\u5ef6\u8fdf\uff0c\u4e2d\u4f4d\u5ef6\u8fdf\u8d85\u8fc7750\u6beb\u79d2\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684M2M\u5ef6\u8fdf\u6d4b\u91cf\u6846\u67b6\u6709\u6548\u4e14\u5177\u6709\u9ad8\u7cbe\u5ea6\uff0c\u63ed\u793a\u4e86\u5f53\u524d\u8fdc\u7a0b\u64cd\u63a7\u7cfb\u7edf\u4e2d\u6267\u884c\u5668\u5ef6\u8fdf\u662f\u6027\u80fd\u74f6\u9888\uff0c\u4e3a\u672a\u6765\u7cfb\u7edf\u4f18\u5316\u63d0\u4f9b\u4e86\u4f9d\u636e\u3002"}}
{"id": "2511.22551", "categories": ["cs.AR", "cs.ET", "cs.PF"], "pdf": "https://arxiv.org/pdf/2511.22551", "abs": "https://arxiv.org/abs/2511.22551", "authors": ["Elham Cheshmikhani", "Hamed Farbeh", "Hossein Asad"], "title": "3RSeT: Read Disturbance Rate Reduction in STT-MRAM Caches by Selective Tag Comparison", "comment": null, "summary": "Recent development in memory technologies has introduced Spin-Transfer Torque Magnetic RAM (STT-MRAM) as the most promising replacement for SRAMs in on-chip cache memories. Besides its lower leakage power, higher density, immunity to radiation-induced particles, and non-volatility, an unintentional bit flip during read operation, referred to as read disturbance error, is a severe reliability challenge in STT-MRAM caches. One major source of read disturbance error in STT-MRAM caches is simultaneous accesses to all tags for parallel comparison operation in a cache set, which has not been addressed in previous work. This paper first demonstrates that high read accesses to tag array extremely increase the read disturbance rate and then proposes a low-cost scheme, so-called Read Disturbance Rate Reduction in STT-MRAM Caches by Selective Tag Comparison (3RSeT), to reduce the error rate by eliminating a significant portion of tag reads. 3RSeT proactively disables the tags that have no chance for hit, using low significant bits of the tags on each access request. Our evaluations using gem5 full-system cycle-accurate simulator show that 3RSeT reduces the read disturbance rate in the tag array by 71.8%, which results in 3.6x improvement in Mean Time To Failure (MTTF). In addition, the energy consumption is reduced by 62.1% without compromising performance and with less than 0.4% area overhead.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a3RSeT\u7684\u4f4e\u6210\u672c\u65b9\u6848\uff0c\u901a\u8fc7\u9009\u62e9\u6027\u6807\u7b7e\u6bd4\u8f83\u663e\u8457\u964d\u4f4eSTT-MRAM\u7f13\u5b58\u4e2d\u7684\u8bfb\u5e72\u6270\u9519\u8bef\u7387\uff0c\u4ece\u800c\u63d0\u5347\u53ef\u9760\u6027\u3001\u964d\u4f4e\u80fd\u8017\uff0c\u5e76\u51e0\u4e4e\u4e0d\u5f71\u54cd\u6027\u80fd\u548c\u9762\u79ef\u3002", "motivation": "STT-MRAM\u4f5c\u4e3aSRAM\u7684\u6f5c\u5728\u66ff\u4ee3\u54c1\u867d\u5177\u591a\u9879\u4f18\u52bf\uff0c\u4f46\u5176\u5728\u7f13\u5b58\u4e2d\u5e76\u884c\u6807\u7b7e\u8bfb\u53d6\u64cd\u4f5c\u4f1a\u5f15\u53d1\u4e25\u91cd\u7684\u8bfb\u5e72\u6270\u9519\u8bef\uff0c\u800c\u73b0\u6709\u7814\u7a76\u5c1a\u672a\u89e3\u51b3\u6b64\u95ee\u9898\u3002", "method": "\u63d0\u51fa3RSeT\u65b9\u6cd5\uff0c\u5728\u6bcf\u6b21\u8bbf\u95ee\u8bf7\u6c42\u65f6\u5229\u7528\u6807\u7b7e\u4f4e\u4f4d\u4fe1\u606f\u9884\u5148\u7981\u7528\u4e0d\u53ef\u80fd\u547d\u4e2d\u7684\u6807\u7b7e\uff0c\u4ece\u800c\u51cf\u5c11\u4e0d\u5fc5\u8981\u7684\u6807\u7b7e\u8bfb\u53d6\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c3RSeT\u5c06\u6807\u7b7e\u9635\u5217\u7684\u8bfb\u5e72\u6270\u7387\u964d\u4f4e71.8%\uff0c\u5e73\u5747\u6545\u969c\u95f4\u9694\u65f6\u95f4\uff08MTTF\uff09\u63d0\u53473.6\u500d\uff0c\u80fd\u8017\u964d\u4f4e62.1%\uff0c\u6027\u80fd\u65e0\u635f\u4e14\u9762\u79ef\u5f00\u9500\u4f4e\u4e8e0.4%\u3002", "conclusion": "3RSeT\u662f\u4e00\u79cd\u9ad8\u6548\u3001\u4f4e\u5f00\u9500\u7684\u65b9\u6848\uff0c\u80fd\u663e\u8457\u7f13\u89e3STT-MRAM\u7f13\u5b58\u4e2d\u7684\u8bfb\u5e72\u6270\u95ee\u9898\uff0c\u63d0\u5347\u7cfb\u7edf\u53ef\u9760\u6027\u548c\u80fd\u6548\u3002"}}
{"id": "2511.22924", "categories": ["cs.MA", "cs.AI", "cs.CR"], "pdf": "https://arxiv.org/pdf/2511.22924", "abs": "https://arxiv.org/abs/2511.22924", "authors": ["Kaixiang Wang", "Zhaojiacheng Zhou", "Bunyod Suvonov", "Jiong Lou", "Jie LI"], "title": "AgentShield: Make MAS more secure and efficient", "comment": null, "summary": "Large Language Model (LLM)-based Multi-Agent Systems (MAS) offer powerful cooperative reasoning but remain vulnerable to adversarial attacks, where compromised agents can undermine the system's overall performance. Existing defenses either depend on single trusted auditors, creating single points of failure, or sacrifice efficiency for robustness. To resolve this tension, we propose \\textbf{AgentShield}, a distributed framework for efficient, decentralized auditing. AgentShield introduces a novel three-layer defense: \\textbf{(i) Critical Node Auditing} prioritizes high-influence agents via topological analysis; \\textbf{(ii) Light Token Auditing} implements a cascade protocol using lightweight sentry models for rapid discriminative verification; and \\textbf{(iii) Two-Round Consensus Auditing} triggers heavyweight arbiters only upon uncertainty to ensure global agreement. This principled design optimizes the robustness-efficiency trade-off. Experiments demonstrate that AgentShield achieves a 92.5\\% recovery rate and reduces auditing overhead by over 70\\% compared to existing methods, maintaining high collaborative accuracy across diverse MAS topologies and adversarial scenarios.", "AI": {"tldr": "AgentShield \u662f\u4e00\u79cd\u7528\u4e8e\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u7684\u9ad8\u6548\u53bb\u4e2d\u5fc3\u5316\u5ba1\u8ba1\u6846\u67b6\uff0c\u901a\u8fc7\u4e09\u5c42\u9632\u5fa1\u673a\u5236\u5728\u4fdd\u969c\u7cfb\u7edf\u9c81\u68d2\u6027\u7684\u540c\u65f6\u663e\u8457\u964d\u4f4e\u5ba1\u8ba1\u5f00\u9500\u3002", "motivation": "\u73b0\u6709\u9488\u5bf9\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u7684\u9632\u5fa1\u65b9\u6cd5\u5b58\u5728\u5355\u70b9\u6545\u969c\u6216\u6548\u7387\u4f4e\u4e0b\u95ee\u9898\uff0c\u4e9f\u9700\u4e00\u79cd\u517c\u987e\u9c81\u68d2\u6027\u4e0e\u6548\u7387\u7684\u53bb\u4e2d\u5fc3\u5316\u5ba1\u8ba1\u65b9\u6848\u3002", "method": "\u63d0\u51fa AgentShield \u6846\u67b6\uff0c\u5305\u542b\u4e09\u5c42\u9632\u5fa1\u673a\u5236\uff1a(i) \u57fa\u4e8e\u62d3\u6251\u5206\u6790\u7684\u5173\u952e\u8282\u70b9\u5ba1\u8ba1\uff1b(ii) \u4f7f\u7528\u8f7b\u91cf\u54e8\u5175\u6a21\u578b\u7684\u7ea7\u8054\u5f0f\u8f7b\u91cf\u4ee4\u724c\u5ba1\u8ba1\uff1b(iii) \u5728\u4e0d\u786e\u5b9a\u6027\u65f6\u89e6\u53d1\u91cd\u578b\u4ef2\u88c1\u5668\u7684\u4e24\u8f6e\u5171\u8bc6\u5ba1\u8ba1\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cAgentShield \u8fbe\u5230 92.5% \u7684\u6062\u590d\u7387\uff0c\u5ba1\u8ba1\u5f00\u9500\u6bd4\u73b0\u6709\u65b9\u6cd5\u964d\u4f4e 70% \u4ee5\u4e0a\uff0c\u5e76\u5728\u591a\u79cd\u591a\u667a\u80fd\u4f53\u62d3\u6251\u7ed3\u6784\u548c\u5bf9\u6297\u573a\u666f\u4e0b\u4fdd\u6301\u9ad8\u534f\u4f5c\u51c6\u786e\u7387\u3002", "conclusion": "AgentShield \u6709\u6548\u5e73\u8861\u4e86\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u4e2d\u9c81\u68d2\u6027\u4e0e\u6548\u7387\u4e4b\u95f4\u7684\u77db\u76fe\uff0c\u4e3a\u62b5\u5fa1\u5bf9\u6297\u653b\u51fb\u63d0\u4f9b\u4e86\u5b9e\u7528\u4e14\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2511.21769", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2511.21769", "abs": "https://arxiv.org/abs/2511.21769", "authors": ["Royer David Estrada-Esponda", "Gerardo Matturro", "Jose Reinaldo Sabogal-Pinilla"], "title": "Technical knowledge and soft skills in software startups within the Colombian entrepreneurial ecosystem", "comment": "30 pages", "summary": "The technical knowledge and soft skills of entrepreneurial team members significantly impact the early stages of software startups. It is widely recognized that the success or failure of a startup is determined by the quality of the individuals who constitute the founding team. This article presents the findings of a study conducted within the Colombian entrepreneurial ecosystem, focusing on which technical knowledge and soft skills are the most valued by founding teams of software startups, and how the needs for knowledge and skills evolve as the startup grows. A survey of software startup representatives revealed that the most valued knowledge includes requirements engineering, software testing, project planning and management, agile methodologies, marketing, business model definition, and budgeting. The most valued soft skills are typically communication, leadership, and teamwork. The outcomes of this work are relevant to software entrepreneurs, incubators, and researchers.", "AI": {"tldr": "\u8be5\u7814\u7a76\u8c03\u67e5\u4e86\u54e5\u4f26\u6bd4\u4e9a\u8f6f\u4ef6\u521d\u521b\u4f01\u4e1a\u521b\u59cb\u56e2\u961f\u6700\u91cd\u89c6\u7684\u6280\u672f\u77e5\u8bc6\u548c\u8f6f\u6280\u80fd\uff0c\u5e76\u63a2\u8ba8\u4e86\u8fd9\u4e9b\u9700\u6c42\u5982\u4f55\u968f\u4f01\u4e1a\u53d1\u5c55\u800c\u53d8\u5316\u3002", "motivation": "\u521d\u521b\u4f01\u4e1a\u7684\u6210\u8d25\u5f88\u5927\u7a0b\u5ea6\u4e0a\u53d6\u51b3\u4e8e\u521b\u59cb\u56e2\u961f\u6210\u5458\u7684\u8d28\u91cf\uff0c\u56e0\u6b64\u6709\u5fc5\u8981\u660e\u786e\u5728\u8f6f\u4ef6\u521d\u521b\u4f01\u4e1a\u65e9\u671f\u9636\u6bb5\u54ea\u4e9b\u6280\u672f\u77e5\u8bc6\u548c\u8f6f\u6280\u80fd\u6700\u4e3a\u5173\u952e\u3002", "method": "\u901a\u8fc7\u5bf9\u54e5\u4f26\u6bd4\u4e9a\u8f6f\u4ef6\u521d\u521b\u4f01\u4e1a\u4ee3\u8868\u8fdb\u884c\u95ee\u5377\u8c03\u67e5\uff0c\u8bc6\u522b\u6700\u53d7\u91cd\u89c6\u7684\u6280\u672f\u77e5\u8bc6\u548c\u8f6f\u6280\u80fd\u3002", "result": "\u6700\u53d7\u91cd\u89c6\u7684\u6280\u672f\u77e5\u8bc6\u5305\u62ec\u9700\u6c42\u5de5\u7a0b\u3001\u8f6f\u4ef6\u6d4b\u8bd5\u3001\u9879\u76ee\u89c4\u5212\u4e0e\u7ba1\u7406\u3001\u654f\u6377\u65b9\u6cd5\u3001\u5e02\u573a\u8425\u9500\u3001\u5546\u4e1a\u6a21\u5f0f\u5b9a\u4e49\u548c\u9884\u7b97\u7f16\u5236\uff1b\u6700\u53d7\u91cd\u89c6\u7684\u8f6f\u6280\u80fd\u662f\u6c9f\u901a\u3001\u9886\u5bfc\u529b\u548c\u56e2\u961f\u5408\u4f5c\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u5bf9\u8f6f\u4ef6\u521b\u4e1a\u8005\u3001\u5b75\u5316\u5668\u548c\u7814\u7a76\u4eba\u5458\u5177\u6709\u53c2\u8003\u4ef7\u503c\uff0c\u6709\u52a9\u4e8e\u7406\u89e3\u521d\u521b\u4f01\u4e1a\u5728\u4e0d\u540c\u6210\u957f\u9636\u6bb5\u5bf9\u4eba\u624d\u80fd\u529b\u7684\u9700\u6c42\u53d8\u5316\u3002"}}
{"id": "2511.21844", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2511.21844", "abs": "https://arxiv.org/abs/2511.21844", "authors": ["Murat Yaslioglu"], "title": "A Sustainable and Reward Incentivized High-Performance Cluster Computing for Artificial Intelligence: A Novel Bayesian-Time-Decay Trust Mechanism in Blockchain", "comment": null, "summary": "In an age where sustainability is of paramount importance, the significance of both high-performance computing and intelligent algorithms cannot be understated. Yet, these domains often demand hefty computational power, translating to substantial energy usage and potentially sidelining less robust computing systems. It's evident that we need an approach that is more encompassing, scalable, and eco-friendly for intelligent algorithm development and implementation. The strategy we present in this paper offers a compelling answer to these issues. We unveil a fresh framework that seamlessly melds high-performance cluster computing with intelligent algorithms, all within a blockchain infrastructure. This promotes both efficiency and a broad-based participation. At its core, our design integrates an evolved proof-of-work consensus process, which links computational efforts directly to rewards for producing blocks. This ensures both optimal resource use and participation from a wide spectrum of computational capacities. Additionally, our approach incorporates a dynamic 'trust rating' that evolves based on a track record of accurate block validations. This rating determines the likelihood of a node being chosen for block generation, creating a merit-based system that recognizes and rewards genuine and precise contributions. To level the playing field further, we suggest a statistical 'draw' system, allowing even less powerful nodes a chance to be part of the block creation process.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u878d\u5408\u9ad8\u6027\u80fd\u96c6\u7fa4\u8ba1\u7b97\u3001\u667a\u80fd\u7b97\u6cd5\u4e0e\u533a\u5757\u94fe\u7684\u65b0\u6846\u67b6\uff0c\u901a\u8fc7\u6539\u8fdb\u7684\u5de5\u4f5c\u91cf\u8bc1\u660e\u673a\u5236\u3001\u52a8\u6001\u4fe1\u4efb\u8bc4\u5206\u548c\u7edf\u8ba1\u62bd\u7b7e\u7cfb\u7edf\uff0c\u5b9e\u73b0\u9ad8\u6548\u3001\u516c\u5e73\u4e14\u73af\u4fdd\u7684\u667a\u80fd\u7b97\u6cd5\u90e8\u7f72\u3002", "motivation": "\u9ad8\u6027\u80fd\u8ba1\u7b97\u548c\u667a\u80fd\u7b97\u6cd5\u901a\u5e38\u9700\u8981\u5927\u91cf\u8ba1\u7b97\u8d44\u6e90\uff0c\u5bfc\u81f4\u9ad8\u80fd\u8017\u5e76\u6392\u65a5\u4f4e\u6027\u80fd\u8bbe\u5907\uff0c\u56e0\u6b64\u4e9f\u9700\u4e00\u79cd\u66f4\u5177\u5305\u5bb9\u6027\u3001\u53ef\u6269\u5c55\u6027\u548c\u73af\u4fdd\u6027\u7684\u65b0\u65b9\u6cd5\u3002", "method": "\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u57fa\u4e8e\u533a\u5757\u94fe\u7684\u6846\u67b6\uff0c\u6574\u5408\u6539\u8fdb\u7684PoW\u5171\u8bc6\u673a\u5236\uff08\u5c06\u8ba1\u7b97\u6295\u5165\u4e0e\u533a\u5757\u5956\u52b1\u6302\u94a9\uff09\u3001\u52a8\u6001\u201c\u4fe1\u4efb\u8bc4\u5206\u201d\uff08\u6839\u636e\u8282\u70b9\u9a8c\u8bc1\u51c6\u786e\u6027\u8c03\u6574\u5176\u88ab\u9009\u4e2d\u6982\u7387\uff09\u4ee5\u53ca\u7edf\u8ba1\u201c\u62bd\u7b7e\u201d\u673a\u5236\uff08\u4f7f\u4f4e\u7b97\u529b\u8282\u70b9\u4e5f\u6709\u673a\u4f1a\u53c2\u4e0e\u51fa\u5757\uff09\u3002", "result": "\u8be5\u6846\u67b6\u63d0\u5347\u4e86\u8d44\u6e90\u5229\u7528\u6548\u7387\uff0c\u652f\u6301\u5e7f\u6cdb\u8ba1\u7b97\u80fd\u529b\u7684\u8bbe\u5907\u53c2\u4e0e\uff0c\u5e76\u901a\u8fc7\u4fe1\u4efb\u673a\u5236\u548c\u62bd\u7b7e\u673a\u5236\u4fdd\u969c\u4e86\u7cfb\u7edf\u7684\u516c\u5e73\u6027\u4e0e\u53ef\u9760\u6027\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u6846\u67b6\u4e3a\u6784\u5efa\u9ad8\u6548\u3001\u5305\u5bb9\u4e14\u53ef\u6301\u7eed\u7684\u667a\u80fd\u7b97\u6cd5\u751f\u6001\u7cfb\u7edf\u63d0\u4f9b\u4e86\u53ef\u884c\u8def\u5f84\uff0c\u517c\u987e\u6027\u80fd\u3001\u516c\u5e73\u4e0e\u73af\u4fdd\u76ee\u6807\u3002"}}
{"id": "2511.21788", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2511.21788", "abs": "https://arxiv.org/abs/2511.21788", "authors": ["Md. Raihan Tapader", "Md. Mostafizer Rahman", "Ariful Islam Shiplu", "Md Faizul Ibne Amin", "Yutaka Watanobe"], "title": "Code Refactoring with LLM: A Comprehensive Evaluation With Few-Shot Settings", "comment": null, "summary": "In today's world, the focus of programmers has shifted from writing complex, error-prone code to prioritizing simple, clear, efficient, and sustainable code that makes programs easier to understand. Code refactoring plays a critical role in this transition by improving structural organization and optimizing performance. However, existing refactoring methods are limited in their ability to generalize across multiple programming languages and coding styles, as they often rely on manually crafted transformation rules. The objectives of this study are to (i) develop an Large Language Models (LLMs)-based framework capable of performing accurate and efficient code refactoring across multiple languages (C, C++, C#, Python, Java), (ii) investigate the impact of prompt engineering (Temperature, Different shot algorithm) and instruction fine-tuning on refactoring effectiveness, and (iii) evaluate the quality improvements (Compilability, Correctness, Distance, Similarity, Number of Lines, Token, Character, Cyclomatic Complexity) in refactored code through empirical metrics and human assessment. To accomplish these goals, we propose a fine-tuned prompt-engineering-based model combined with few-shot learning for multilingual code refactoring. Experimental results indicate that Java achieves the highest overall correctness up to 99.99% the 10-shot setting, records the highest average compilability of 94.78% compared to the original source code and maintains high similarity (Approx. 53-54%) and thus demonstrates a strong balance between structural modifications and semantic preservation. Python exhibits the lowest structural distance across all shots (Approx. 277-294) while achieving moderate similarity ( Approx. 44-48%) that indicates consistent and minimally disruptive refactoring.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u591a\u8bed\u8a00\u4ee3\u7801\u91cd\u6784\u6846\u67b6\uff0c\u7ed3\u5408\u63d0\u793a\u5de5\u7a0b\u4e0e\u5c11\u6837\u672c\u5b66\u4e60\uff0c\u5728C\u3001C++\u3001C#\u3001Python\u548cJava\u4e0a\u5b9e\u73b0\u9ad8\u6548\u91cd\u6784\uff0c\u5e76\u901a\u8fc7\u591a\u79cd\u6307\u6807\u8bc4\u4f30\u91cd\u6784\u8d28\u91cf\u3002\u5b9e\u9a8c\u8868\u660e\uff0cJava\u5728\u6b63\u786e\u6027\u548c\u53ef\u7f16\u8bd1\u6027\u65b9\u9762\u8868\u73b0\u6700\u4f73\uff0cPython\u5219\u5c55\u73b0\u51fa\u6700\u5c0f\u7684\u7ed3\u6784\u6539\u52a8\u3002", "motivation": "\u73b0\u6709\u4ee3\u7801\u91cd\u6784\u65b9\u6cd5\u4f9d\u8d56\u4eba\u5de5\u7f16\u5199\u7684\u8f6c\u6362\u89c4\u5219\uff0c\u96be\u4ee5\u8de8\u7f16\u7a0b\u8bed\u8a00\u548c\u7f16\u7801\u98ce\u683c\u6cdb\u5316\u3002\u968f\u7740\u5bf9\u7b80\u6d01\u3001\u6e05\u6670\u3001\u9ad8\u6548\u548c\u53ef\u6301\u7eed\u4ee3\u7801\u7684\u9700\u6c42\u589e\u52a0\uff0c\u4e9f\u9700\u4e00\u79cd\u901a\u7528\u4e14\u9ad8\u6548\u7684\u81ea\u52a8\u5316\u91cd\u6784\u65b9\u6848\u3002", "method": "\u63d0\u51fa\u4e00\u4e2a\u7ed3\u5408\u6307\u4ee4\u5fae\u8c03\u3001\u63d0\u793a\u5de5\u7a0b\uff08\u5982Temperature\u63a7\u5236\u548c\u4e0d\u540cshot\u7b97\u6cd5\uff09\u4e0e\u5c11\u6837\u672c\u5b66\u4e60\u7684LLM\u6846\u67b6\uff0c\u7528\u4e8e\u591a\u8bed\u8a00\u4ee3\u7801\u91cd\u6784\uff0c\u5e76\u901a\u8fc7\u7f16\u8bd1\u6027\u3001\u6b63\u786e\u6027\u3001\u7ed3\u6784\u8ddd\u79bb\u3001\u76f8\u4f3c\u5ea6\u7b49\u591a\u4e2a\u6307\u6807\u8fdb\u884c\u8bc4\u4f30\u3002", "result": "Java\u572810-shot\u8bbe\u7f6e\u4e0b\u8fbe\u523099.99%\u7684\u6b63\u786e\u7387\u548c94.78%\u7684\u5e73\u5747\u53ef\u7f16\u8bd1\u6027\uff0c\u540c\u65f6\u4fdd\u6301\u7ea653-54%\u7684\u76f8\u4f3c\u5ea6\uff1bPython\u5728\u6240\u6709shot\u8bbe\u7f6e\u4e2d\u7ed3\u6784\u8ddd\u79bb\u6700\u4f4e\uff08\u7ea6277-294\uff09\uff0c\u76f8\u4f3c\u5ea6\u4e3a44-48%\uff0c\u8868\u660e\u5176\u91cd\u6784\u4e00\u81f4\u6027\u9ad8\u4e14\u5e72\u6270\u5c0f\u3002", "conclusion": "\u57fa\u4e8eLLM\u7684\u63d0\u793a\u5de5\u7a0b\u4e0e\u5c11\u6837\u672c\u5b66\u4e60\u65b9\u6cd5\u80fd\u6709\u6548\u5b9e\u73b0\u591a\u8bed\u8a00\u4ee3\u7801\u91cd\u6784\uff0c\u5728\u4fdd\u6301\u8bed\u4e49\u4e00\u81f4\u7684\u540c\u65f6\u63d0\u5347\u4ee3\u7801\u8d28\u91cf\uff0c\u5c24\u5176\u5728Java\u548cPython\u4e2d\u8868\u73b0\u7a81\u51fa\uff0c\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\u7684\u53ef\u884c\u6027\u4e0e\u4f18\u52bf\u3002"}}
{"id": "2511.21859", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2511.21859", "abs": "https://arxiv.org/abs/2511.21859", "authors": ["Hagit Attiya", "Armando Casta\u00f1eda", "Dhrubajyoti Ghosh", "Thomas Nowak"], "title": "Equivalence and Separation between Heard-Of and Asynchronous Message-Passing Models", "comment": "18 pages", "summary": "We revisit the relationship between two fundamental models of distributed computation: the asynchronous message-passing model with up to $f$ crash failures ($\\operatorname{AMP}_f$) and the Heard-Of model with up to $f$ message omissions ($\\operatorname{HO}_f$). We show that for $n > 2f$, the two models are equivalent with respect to the solvability of colorless tasks, and that for colored tasks the equivalence holds only when $f = 1$ (and $n > 2$). The separation for larger $f$ arises from the presence of silenced processes in $\\operatorname{HO}_f$, which may lead to incompatible decisions. The proofs proceed through bidirectional simulations between $\\operatorname{AMP}_f$ and $\\operatorname{HO}_f$ via an intermediate model that captures this notion of silencing. The results extend to randomized protocols against a non-adaptive adversary, indicating that the expressive limits of canonical rounds are structural rather than probabilistic. Together, these results delineate precisely where round-based abstractions capture asynchronous computation, and where they do not.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u5f02\u6b65\u6d88\u606f\u4f20\u9012\u6a21\u578b\uff08AMP_f\uff09\u4e0eHeard-Of\u6a21\u578b\uff08HO_f\uff09\u5728\u5bb9\u9519\u5206\u5e03\u5f0f\u8ba1\u7b97\u4e2d\u7684\u7b49\u4ef7\u6027\uff0c\u53d1\u73b0\u5bf9\u4e8e\u65e0\u8272\u4efb\u52a1\u5728 n > 2f \u65f6\u4e24\u8005\u7b49\u4ef7\uff0c\u800c\u5bf9\u4e8e\u6709\u8272\u4efb\u52a1\u4ec5\u5728 f = 1 \u4e14 n > 2 \u65f6\u7b49\u4ef7\u3002", "motivation": "\u5398\u6e05\u4e24\u79cd\u4e3b\u6d41\u5206\u5e03\u5f0f\u8ba1\u7b97\u6a21\u578b\uff08AMP_f \u548c HO_f\uff09\u5728\u53ef\u89e3\u6027\u65b9\u9762\u7684\u5173\u7cfb\uff0c\u7279\u522b\u662f\u5b83\u4eec\u5728\u5904\u7406\u4e0d\u540c\u4efb\u52a1\u7c7b\u578b\uff08\u65e0\u8272\u4e0e\u6709\u8272\uff09\u548c\u6545\u969c\u6570\u91cf\u4e0b\u7684\u8868\u8fbe\u80fd\u529b\u5dee\u5f02\u3002", "method": "\u901a\u8fc7\u6784\u5efa\u53cc\u5411\u6a21\u62df\uff0c\u5e76\u5f15\u5165\u4e00\u4e2a\u80fd\u523b\u753b\u201c\u9759\u9ed8\u8fdb\u7a0b\u201d\u73b0\u8c61\u7684\u4e2d\u95f4\u6a21\u578b\uff0c\u5728 AMP_f \u4e0e HO_f \u4e4b\u95f4\u5efa\u7acb\u8054\u7cfb\uff1b\u540c\u65f6\u5c06\u5206\u6790\u6269\u5c55\u81f3\u975e\u81ea\u9002\u5e94\u654c\u624b\u4e0b\u7684\u968f\u673a\u534f\u8bae\u3002", "result": "\u8bc1\u660e\u4e86\u5728 n > 2f \u6761\u4ef6\u4e0b\uff0cAMP_f \u4e0e HO_f \u5bf9\u65e0\u8272\u4efb\u52a1\u7b49\u4ef7\uff1b\u5bf9\u6709\u8272\u4efb\u52a1\uff0c\u4ec5\u5f53 f = 1 \u4e14 n > 2 \u65f6\u7b49\u4ef7\u3002\u66f4\u5927\u7684 f \u503c\u4f1a\u5bfc\u81f4 HO_f \u4e2d\u56e0\u9759\u9ed8\u8fdb\u7a0b\u800c\u4ea7\u751f\u4e0d\u517c\u5bb9\u51b3\u7b56\uff0c\u7834\u574f\u7b49\u4ef7\u6027\u3002", "conclusion": "\u8f6e\u6b21\u62bd\u8c61\uff08round-based abstractions\uff09\u662f\u5426\u80fd\u51c6\u786e\u523b\u753b\u5f02\u6b65\u8ba1\u7b97\u5177\u6709\u7ed3\u6784\u6027\u9650\u5236\uff1a\u5bf9\u65e0\u8272\u4efb\u52a1\u5728\u591a\u6570\u6d3e\u6761\u4ef6\u4e0b\u6210\u7acb\uff0c\u4f46\u5bf9\u6709\u8272\u4efb\u52a1\u5219\u4ec5\u5728\u6781\u4f4e\u6545\u969c\u6570\u4e0b\u6210\u7acb\uff0c\u4e14\u8be5\u9650\u5236\u4e0e\u662f\u5426\u4f7f\u7528\u968f\u673a\u5316\u65e0\u5173\u3002"}}
{"id": "2511.22421", "categories": ["cs.NI"], "pdf": "https://arxiv.org/pdf/2511.22421", "abs": "https://arxiv.org/abs/2511.22421", "authors": ["Hanshuai Cui", "Zhiqing Tang", "Zhi Yao", "Weijia Ji", "Wei Zhao"], "title": "Semantic-Aware Caching for Efficient Image Generation in Edge Computing", "comment": null, "summary": "Text-to-image generation employing diffusion models has attained significant popularity due to its capability to produce high-quality images that adhere to textual prompts. However, the integration of diffusion models faces critical challenges into resource-constrained mobile and edge environments because it requires multiple denoising steps from the original random noise. A practical way to speed up denoising is to initialize the process with a noised reference image that is similar to the target, since both images share similar layouts, structures, and details, allowing for fewer denoising steps. Based on this idea, we present CacheGenius, a hybrid image generation system in edge computing that accelerates generation by combining text-toimage and image-to-image workflows. It generates images from user text prompts using cached reference images. CacheGenius introduces a semantic-aware classified storage scheme and a request-scheduling algorithm that ensures semantic alignment between references and targets. To ensure sustained performance, it employs a cache maintenance policy that proactively evicts obsolete entries via correlation analysis. Evaluated in a distributed edge computing system, CacheGenius reduces generation latency by 41% and computational costs by 48% relative to baselines, while maintaining competitive evaluation metrics.", "AI": {"tldr": "CacheGenius \u662f\u4e00\u79cd\u9762\u5411\u8fb9\u7f18\u8ba1\u7b97\u7684\u6df7\u5408\u56fe\u50cf\u751f\u6210\u7cfb\u7edf\uff0c\u901a\u8fc7\u7ed3\u5408\u6587\u672c\u5230\u56fe\u50cf\u4e0e\u56fe\u50cf\u5230\u56fe\u50cf\u6d41\u7a0b\uff0c\u5e76\u5229\u7528\u8bed\u4e49\u5bf9\u9f50\u7684\u7f13\u5b58\u53c2\u8003\u56fe\u50cf\uff0c\u663e\u8457\u51cf\u5c11\u6269\u6563\u6a21\u578b\u5728\u79fb\u52a8\u7aef\u7684\u751f\u6210\u5ef6\u8fdf\u548c\u8ba1\u7b97\u5f00\u9500\u3002", "motivation": "\u6269\u6563\u6a21\u578b\u867d\u80fd\u751f\u6210\u9ad8\u8d28\u91cf\u56fe\u50cf\uff0c\u4f46\u5176\u591a\u6b65\u53bb\u566a\u8fc7\u7a0b\u5728\u8d44\u6e90\u53d7\u9650\u7684\u79fb\u52a8\u548c\u8fb9\u7f18\u8bbe\u5907\u4e0a\u6548\u7387\u4f4e\u4e0b\uff0c\u4e9f\u9700\u52a0\u901f\u65b9\u6848\u3002", "method": "\u63d0\u51fa CacheGenius \u7cfb\u7edf\uff0c\u91c7\u7528\u8bed\u4e49\u611f\u77e5\u7684\u5206\u7c7b\u5b58\u50a8\u7b56\u7565\u3001\u8bf7\u6c42\u8c03\u5ea6\u7b97\u6cd5\u4ee5\u786e\u4fdd\u53c2\u8003\u56fe\u50cf\u4e0e\u76ee\u6807\u8bed\u4e49\u5bf9\u9f50\uff0c\u5e76\u5f15\u5165\u57fa\u4e8e\u76f8\u5173\u6027\u5206\u6790\u7684\u7f13\u5b58\u7ef4\u62a4\u7b56\u7565\u4e3b\u52a8\u6dd8\u6c70\u8fc7\u65f6\u6761\u76ee\u3002", "result": "\u5728\u5206\u5e03\u5f0f\u8fb9\u7f18\u8ba1\u7b97\u73af\u5883\u4e2d\uff0c\u76f8\u6bd4\u57fa\u7ebf\u65b9\u6cd5\uff0cCacheGenius \u5c06\u751f\u6210\u5ef6\u8fdf\u964d\u4f4e 41%\uff0c\u8ba1\u7b97\u6210\u672c\u51cf\u5c11 48%\uff0c\u540c\u65f6\u4fdd\u6301\u6709\u7ade\u4e89\u529b\u7684\u8bc4\u4f30\u6307\u6807\u3002", "conclusion": "CacheGenius \u6709\u6548\u63d0\u5347\u4e86\u6269\u6563\u6a21\u578b\u5728\u8fb9\u7f18\u73af\u5883\u4e2d\u7684\u63a8\u7406\u6548\u7387\uff0c\u4e3a\u8d44\u6e90\u53d7\u9650\u573a\u666f\u4e0b\u7684\u9ad8\u6548\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848\u3002"}}
{"id": "2511.22267", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2511.22267", "abs": "https://arxiv.org/abs/2511.22267", "authors": ["Yuyang Zou", "Youwei Xiao", "Yansong Xu", "Chenyun Yin", "Yuhao Luo", "Yitian Sun", "Ruifan Xu", "Renze Chen", "Yun Liang"], "title": "Aquas: Enhancing Domain Specialization through Holistic Hardware-Software Co-Optimization based on MLIR", "comment": null, "summary": "Application-Specific Instruction-Set Processors (ASIPs) built on the RISC-V architecture offer specialization opportunities for various applications. However, existing frameworks from the open-source RISC-V ecosystem suffer from limited performance due to restricted hardware synthesis and rigid compiler support. To address these challenges, we introduce Aquas, a holistic hardware-software co-design framework built upon MLIR. Aquas enhances ASIP synthesis with fast memory access capability via a burst DMA engine and advanced high-level synthesis (HLS) optimizations. On the compiler side, we propose an e-graph based retargetable approach with a novel matching engine for efficient instruction matching. Evaluation demonstrates up to 9.27x speedup on real-world workloads, including point cloud processing and LLM inference.", "AI": {"tldr": "Aquas \u662f\u4e00\u4e2a\u57fa\u4e8e MLIR \u7684\u8f6f\u786c\u4ef6\u534f\u540c\u8bbe\u8ba1\u6846\u67b6\uff0c\u901a\u8fc7\u589e\u5f3a\u7684\u786c\u4ef6\u5408\u6210\uff08\u5982\u7a81\u53d1 DMA \u5f15\u64ce\u548c\u9ad8\u7ea7 HLS \u4f18\u5316\uff09\u4e0e\u57fa\u4e8e e-graph \u7684\u53ef\u91cd\u5b9a\u5411\u7f16\u8bd1\u5668\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347 RISC-V ASIP \u7684\u6027\u80fd\uff0c\u5728\u771f\u5b9e\u5de5\u4f5c\u8d1f\u8f7d\u4e0a\u5b9e\u73b0\u6700\u9ad8 9.27 \u500d\u52a0\u901f\u3002", "motivation": "\u73b0\u6709\u5f00\u6e90 RISC-V \u751f\u6001\u4e2d\u7684\u6846\u67b6\u56e0\u786c\u4ef6\u7efc\u5408\u80fd\u529b\u53d7\u9650\u548c\u7f16\u8bd1\u5668\u652f\u6301\u50f5\u5316\uff0c\u96be\u4ee5\u5145\u5206\u53d1\u6325 ASIP \u5728\u7279\u5b9a\u5e94\u7528\u4e2d\u7684\u6027\u80fd\u6f5c\u529b\u3002", "method": "\u63d0\u51fa Aquas \u6846\u67b6\uff1a\u786c\u4ef6\u65b9\u9762\u5f15\u5165\u7a81\u53d1 DMA \u5f15\u64ce\u548c\u9ad8\u7ea7 HLS \u4f18\u5316\u4ee5\u52a0\u5feb\u5185\u5b58\u8bbf\u95ee\uff1b\u8f6f\u4ef6\u65b9\u9762\u91c7\u7528\u57fa\u4e8e e-graph \u7684\u53ef\u91cd\u5b9a\u5411\u7f16\u8bd1\u65b9\u6cd5\uff0c\u5e76\u8bbe\u8ba1\u65b0\u578b\u5339\u914d\u5f15\u64ce\u4ee5\u9ad8\u6548\u5b8c\u6210\u6307\u4ee4\u5339\u914d\u3002", "result": "\u5728\u70b9\u4e91\u5904\u7406\u548c\u5927\u8bed\u8a00\u6a21\u578b\u63a8\u7406\u7b49\u771f\u5b9e\u5de5\u4f5c\u8d1f\u8f7d\u4e0a\uff0cAquas \u5b9e\u73b0\u4e86\u6700\u9ad8 9.27 \u500d\u7684\u6027\u80fd\u52a0\u901f\u3002", "conclusion": "Aquas \u6709\u6548\u89e3\u51b3\u4e86\u73b0\u6709 RISC-V ASIP \u6846\u67b6\u5728\u6027\u80fd\u548c\u7f16\u8bd1\u7075\u6d3b\u6027\u65b9\u9762\u7684\u74f6\u9888\uff0c\u5c55\u793a\u4e86\u8f6f\u786c\u4ef6\u534f\u540c\u8bbe\u8ba1\u5728\u4e13\u7528\u5904\u7406\u5668\u5f00\u53d1\u4e2d\u7684\u5de8\u5927\u6f5c\u529b\u3002"}}
{"id": "2511.21877", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.21877", "abs": "https://arxiv.org/abs/2511.21877", "authors": ["Nenad Petrovic", "Norbert Kroth", "Axel Torschmied", "Yinglei Song", "Fengjunjie Pan", "Vahid Zolfaghari", "Nils Purschke", "Sven Kirchner", "Chengdong Wu", "Andre Schamschurko", "Yi Zhang", "Alois Knoll"], "title": "LLM-Empowered Event-Chain Driven Code Generation for ADAS in SDV systems", "comment": null, "summary": "This paper presents an event-chain-driven, LLM-empowered workflow for generating validated, automotive code from natural-language requirements. A Retrieval-Augmented Generation (RAG) layer retrieves relevant signals from large and evolving Vehicle Signal Specification (VSS) catalogs as code generation prompt context, reducing hallucinations and ensuring architectural correctness. Retrieved signals are mapped and validated before being transformed into event chains that encode causal and timing constraints. These event chains guide and constrain LLM-based code synthesis, ensuring behavioral consistency and real-time feasibility. Based on our initial findings from the emergency braking case study, with the proposed approach, we managed to achieve valid signal usage and consistent code generation without LLM retraining.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u4e8b\u4ef6\u94fe\u9a71\u52a8\u3001\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u589e\u5f3a\u7684\u5de5\u4f5c\u6d41\uff0c\u7528\u4e8e\u4ece\u81ea\u7136\u8bed\u8a00\u9700\u6c42\u751f\u6210\u7ecf\u8fc7\u9a8c\u8bc1\u7684\u6c7d\u8f66\u4ee3\u7801\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u4ece\u81ea\u7136\u8bed\u8a00\u9700\u6c42\u76f4\u63a5\u751f\u6210\u6c7d\u8f66\u4ee3\u7801\u65f6\u53ef\u80fd\u51fa\u73b0\u7684\u5e7b\u89c9\u95ee\u9898\u548c\u67b6\u6784\u4e0d\u4e00\u81f4\u6027\uff0c\u786e\u4fdd\u751f\u6210\u4ee3\u7801\u7684\u884c\u4e3a\u6b63\u786e\u6027\u548c\u5b9e\u65f6\u53ef\u884c\u6027\u3002", "method": "\u91c7\u7528\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u5c42\u4ece\u4e0d\u65ad\u6f14\u5316\u7684\u8f66\u8f86\u4fe1\u53f7\u89c4\u8303\uff08VSS\uff09\u76ee\u5f55\u4e2d\u68c0\u7d22\u76f8\u5173\u4fe1\u53f7\u4f5c\u4e3a\u63d0\u793a\u4e0a\u4e0b\u6587\uff1b\u5c06\u68c0\u7d22\u5230\u7684\u4fe1\u53f7\u6620\u5c04\u5e76\u9a8c\u8bc1\u540e\u8f6c\u6362\u4e3a\u7f16\u7801\u56e0\u679c\u4e0e\u65f6\u5e8f\u7ea6\u675f\u7684\u4e8b\u4ef6\u94fe\uff1b\u5229\u7528\u8fd9\u4e9b\u4e8b\u4ef6\u94fe\u5f15\u5bfc\u5e76\u7ea6\u675fLLM\u8fdb\u884c\u4ee3\u7801\u5408\u6210\u3002", "result": "\u5728\u7d27\u6025\u5236\u52a8\u6848\u4f8b\u7814\u7a76\u4e2d\uff0c\u8be5\u65b9\u6cd5\u5b9e\u73b0\u4e86\u6709\u6548\u7684\u4fe1\u53f7\u4f7f\u7528\u548c\u4e00\u81f4\u7684\u4ee3\u7801\u751f\u6210\uff0c\u4e14\u65e0\u9700\u5bf9LLM\u8fdb\u884c\u91cd\u65b0\u8bad\u7ec3\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u63d0\u5347\u4ece\u81ea\u7136\u8bed\u8a00\u9700\u6c42\u751f\u6210\u6c7d\u8f66\u4ee3\u7801\u7684\u51c6\u786e\u6027\u4e0e\u53ef\u9760\u6027\uff0c\u540c\u65f6\u907f\u514d\u4e86\u5bf9\u5927\u8bed\u8a00\u6a21\u578b\u7684\u518d\u8bad\u7ec3\u9700\u6c42\u3002"}}
{"id": "2511.21862", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2511.21862", "abs": "https://arxiv.org/abs/2511.21862", "authors": ["Siyu Wu", "Zihan Tang", "Yuting Zeng", "Hui Chen", "Guiguang Ding", "Tongxuan Liu", "Ke Zhang", "Hailong Yang"], "title": "OOCO: Latency-disaggregated Architecture for Online-Offline Co-locate LLM Serving", "comment": null, "summary": "Large Language Models (LLMs) are increasingly deployed in both latency-sensitive online services and cost-sensitive offline workloads. Co-locating these workloads on shared serving instances can improve resource utilization, but directly applying this approach to Prefill/Decode (P/D) disaggregated systems introduces severe load imbalance, as fluctuating request mixes alter the intrinsic P/D ratio. Existing dynamic adjustment techniques cannot keep up with the bursty traffic patterns of online services.\n  We propose a latency-constraint disaggregated architecture, which separates cluster resources into latency-strict and latency-relaxed pools based on task latency requirements. This design enables flexible placement of offline decode tasks, mitigating P/D imbalance while preserving online performance. To fully exploit this flexibility, we propose (1) a bottleneck-based scheduler guided by a Roofline-based performance model for performance bottleneck based scheduling, and (2) a fast preemption mechanism that strictly enforces Service Level Objectives (SLOs) for online requests.\n  Experiments on real-world traces show that compared to existing offline system approaches, our method improves offline throughput by up to 3x, while maintaining online request SLOs.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u5ef6\u8fdf\u7ea6\u675f\u7684\u89e3\u8026\u67b6\u6784\uff0c\u901a\u8fc7\u5206\u79bb\u5ef6\u8fdf\u654f\u611f\u4e0e\u975e\u654f\u611f\u4efb\u52a1\u6c60\uff0c\u5e76\u7ed3\u5408\u74f6\u9888\u611f\u77e5\u8c03\u5ea6\u5668\u548c\u5feb\u901f\u62a2\u5360\u673a\u5236\uff0c\u5728\u4fdd\u969c\u5728\u7ebf\u8bf7\u6c42SLO\u7684\u540c\u65f6\u5c06\u79bb\u7ebf\u541e\u5410\u63d0\u5347\u6700\u9ad83\u500d\u3002", "motivation": "\u5728Prefill/Decode\u89e3\u8026\u7cfb\u7edf\u4e2d\u76f4\u63a5\u5171\u7f6e\u5ef6\u8fdf\u654f\u611f\u7684\u5728\u7ebf\u670d\u52a1\u4e0e\u6210\u672c\u654f\u611f\u7684\u79bb\u7ebf\u4efb\u52a1\u4f1a\u5bfc\u81f4\u4e25\u91cd\u7684\u8d1f\u8f7d\u4e0d\u5747\u8861\uff0c\u73b0\u6709\u52a8\u6001\u8c03\u6574\u65b9\u6cd5\u96be\u4ee5\u5e94\u5bf9\u5728\u7ebf\u6d41\u91cf\u7a81\u53d1\u6027\u3002", "method": "\u8bbe\u8ba1\u5ef6\u8fdf\u7ea6\u675f\u89e3\u8026\u67b6\u6784\uff0c\u5c06\u96c6\u7fa4\u8d44\u6e90\u5212\u5206\u4e3a\u5ef6\u8fdf\u4e25\u683c\u4e0e\u5bbd\u677e\u4e24\u7c7b\u6c60\uff1b\u5f15\u5165\u57fa\u4e8eRoofline\u6a21\u578b\u7684\u74f6\u9888\u611f\u77e5\u8c03\u5ea6\u5668\uff0c\u5e76\u5b9e\u73b0\u5feb\u901f\u62a2\u5360\u673a\u5236\u4ee5\u4fdd\u969c\u5728\u7ebfSLO\u3002", "result": "\u5728\u771f\u5b9e\u8f68\u8ff9\u5b9e\u9a8c\u4e2d\uff0c\u76f8\u6bd4\u73b0\u6709\u79bb\u7ebf\u7cfb\u7edf\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u5728\u6ee1\u8db3\u5728\u7ebf\u8bf7\u6c42SLO\u7684\u524d\u63d0\u4e0b\uff0c\u79bb\u7ebf\u541e\u5410\u91cf\u6700\u9ad8\u63d0\u53473\u500d\u3002", "conclusion": "\u6240\u63d0\u67b6\u6784\u6709\u6548\u7f13\u89e3\u4e86P/D\u8d1f\u8f7d\u5931\u8861\u95ee\u9898\uff0c\u5728\u4fdd\u969c\u5728\u7ebf\u670d\u52a1\u8d28\u91cf\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u4e86\u79bb\u7ebf\u4efb\u52a1\u7684\u8d44\u6e90\u5229\u7528\u6548\u7387\u3002"}}
{"id": "2511.22348", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2511.22348", "abs": "https://arxiv.org/abs/2511.22348", "authors": ["Shuao Jia", "Zichao Ling", "Chen Bai", "Kang Zhao", "Jianwang Zhai"], "title": "FADiff: Fusion-Aware Differentiable Optimization for DNN Scheduling on Tensor Accelerators", "comment": "7 pages, 4 figures", "summary": "Efficient deployment of Deep Neural Networks (DNNs), such as Large Language Models (LLMs), on tensor accelerators is essential for maximizing computational efficiency in modern AI systems. However, achieving this is challenging due to the enormous and complex design space created by the interaction of intra-layer mapping and inter-layer fusion. In this work, we present FADiff, a gradient-based optimization framework capable of automatically identifying high-quality intra-layer mapping and inter-layer fusion strategies to accelerate inference for DNN workloads. We first construct a unified and differentiable analytical cost model, which accurately predicts the energy and latency of both single-layer mappings and various layer fusion strategies. Then, by encoding discrete constraints into the loss function, we employ a gradient-based approach to efficiently explore the vast design space, determining the optimal joint strategy for mapping and fusion. Experimental results demonstrate the superiority of FADiff, achieving better optimization in terms of energy and latency compared to existing methods.", "AI": {"tldr": "FADiff is a gradient-based optimization framework that jointly optimizes intra-layer mapping and inter-layer fusion for efficient DNN inference on tensor accelerators, outperforming existing methods in energy and latency.", "motivation": "Efficient deployment of DNNs like LLMs on tensor accelerators is hindered by the complex design space arising from the interaction between intra-layer mapping and inter-layer fusion.", "method": "FADiff constructs a unified differentiable analytical cost model to predict energy and latency, then uses gradient-based optimization with discrete constraints encoded in the loss function to explore the joint mapping-fusion design space.", "result": "Experiments show FADiff achieves superior energy and latency optimization compared to existing approaches.", "conclusion": "FADiff effectively automates the discovery of high-quality mapping and fusion strategies, significantly improving DNN inference efficiency on tensor accelerators."}}
{"id": "2511.21878", "categories": ["cs.SE", "cs.PL"], "pdf": "https://arxiv.org/pdf/2511.21878", "abs": "https://arxiv.org/abs/2511.21878", "authors": ["Kaiyao Ke", "Ali Reza Ibrahimzada", "Rangeet Pan", "Saurabh Sinha", "Reyhaneh Jabbarvand"], "title": "Advancing Automated In-Isolation Validation in Repository-Level Code Translation", "comment": null, "summary": "Repository-level code translation aims to migrate entire repositories across programming languages while preserving functionality automatically. Despite advancements in repository-level code translation, validating the translations remains challenging. This paper proposes TRAM, which combines context-aware type resolution with mock-based in-isolation validation to achieve high-quality translations between programming languages. Prior to translation, TRAM retrieves API documentation and contextual code information for each variable type in the source language. It then prompts a large language model (LLM) with retrieved contextual information to resolve type mappings across languages with precise semantic interpretations. Using the automatically constructed type mapping, TRAM employs a custom serialization/deserialization workflow that automatically constructs equivalent mock objects in the target language. This enables each method fragment to be validated in isolation, without the high cost of using agents for translation validation, or the heavy manual effort required by existing approaches that rely on language interoperability. TRAM demonstrates state-of-the-art performance in Java-to-Python translation, underscoring the effectiveness of its integration of RAG-based type resolution with reliable in-isolation validation.", "AI": {"tldr": "TRAM \u662f\u4e00\u79cd\u7528\u4e8e\u4ed3\u5e93\u7ea7\u4ee3\u7801\u7ffb\u8bd1\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u7ed3\u5408\u4e0a\u4e0b\u6587\u611f\u77e5\u7684\u7c7b\u578b\u89e3\u6790\u4e0e\u57fa\u4e8e\u6a21\u62df\u5bf9\u8c61\u7684\u9694\u79bb\u9a8c\u8bc1\uff0c\u5b9e\u73b0\u4e86\u9ad8\u8d28\u91cf\u7684\u8de8\u8bed\u8a00\u7ffb\u8bd1\uff0c\u5c24\u5176\u5728 Java \u5230 Python \u7684\u7ffb\u8bd1\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u5f53\u524d\u4ed3\u5e93\u7ea7\u4ee3\u7801\u7ffb\u8bd1\u7f3a\u4e4f\u9ad8\u6548\u3001\u81ea\u52a8\u5316\u7684\u9a8c\u8bc1\u673a\u5236\uff0c\u73b0\u6709\u65b9\u6cd5\u8981\u4e48\u4f9d\u8d56\u9ad8\u6210\u672c\u7684\u667a\u80fd\u4f53\u9a8c\u8bc1\uff0c\u8981\u4e48\u9700\u8981\u5927\u91cf\u4eba\u5de5\u5e72\u9884\uff0c\u56e0\u6b64\u4e9f\u9700\u4e00\u79cd\u66f4\u53ef\u9760\u4e14\u81ea\u52a8\u5316\u7684\u9a8c\u8bc1\u65b9\u6848\u3002", "method": "TRAM \u5728\u7ffb\u8bd1\u524d\u68c0\u7d22\u6e90\u8bed\u8a00\u4e2d\u5404\u53d8\u91cf\u7c7b\u578b\u7684 API \u6587\u6863\u548c\u4e0a\u4e0b\u6587\u4ee3\u7801\u4fe1\u606f\uff0c\u5e76\u5229\u7528\u8fd9\u4e9b\u4fe1\u606f\u5f15\u5bfc\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u8fdb\u884c\u7cbe\u51c6\u7684\u8de8\u8bed\u8a00\u7c7b\u578b\u6620\u5c04\uff1b\u968f\u540e\uff0c\u901a\u8fc7\u81ea\u5b9a\u4e49\u7684\u5e8f\u5217\u5316/\u53cd\u5e8f\u5217\u5316\u6d41\u7a0b\uff0c\u5728\u76ee\u6807\u8bed\u8a00\u4e2d\u81ea\u52a8\u751f\u6210\u7b49\u6548\u7684\u6a21\u62df\u5bf9\u8c61\uff0c\u4ece\u800c\u5b9e\u73b0\u65b9\u6cd5\u7247\u6bb5\u7684\u9694\u79bb\u9a8c\u8bc1\u3002", "result": "TRAM \u5728 Java \u5230 Python \u7684\u4ed3\u5e93\u7ea7\u4ee3\u7801\u7ffb\u8bd1\u4efb\u52a1\u4e2d\u8fbe\u5230\u5f53\u524d\u6700\u4f18\u6027\u80fd\uff0c\u9a8c\u8bc1\u4e86\u5176\u7ed3\u5408 RAG \u578b\u7c7b\u578b\u89e3\u6790\u4e0e\u9694\u79bb\u9a8c\u8bc1\u673a\u5236\u7684\u6709\u6548\u6027\u3002", "conclusion": "TRAM \u901a\u8fc7\u4e0a\u4e0b\u6587\u611f\u77e5\u7684\u7c7b\u578b\u89e3\u6790\u4e0e\u81ea\u52a8\u5316\u9694\u79bb\u9a8c\u8bc1\uff0c\u663e\u8457\u63d0\u5347\u4e86\u4ed3\u5e93\u7ea7\u4ee3\u7801\u7ffb\u8bd1\u7684\u8d28\u91cf\u4e0e\u53ef\u9a8c\u8bc1\u6027\uff0c\u4e3a\u8de8\u8bed\u8a00\u8fc1\u79fb\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u3001\u4f4e\u4eba\u5de5\u5e72\u9884\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2511.21958", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2511.21958", "abs": "https://arxiv.org/abs/2511.21958", "authors": ["Yiyan Zhai", "Bintang Dwi Marthen", "Sarath Balivada", "Vamsi Sudhakar Bojji", "Eric Knauft", "Jitender Rohilla", "Jiaqi Zuo", "Quanxing Liu", "Maxime Austruy", "Wenguang Wang", "Juncheng Yang"], "title": "Clock2Q+: A Simple and Efficient Replacement Algorithm for Metadata Cache in VMware vSAN", "comment": "12 pages, 14 figures", "summary": "Cache replacement algorithms are critical building blocks of storage systems. This paper examines the characteristics of metadata caches and argues that they inherently exhibit correlated references, even when the corresponding data accesses do not contain correlated references. The presence of correlated references reduces the effectiveness of cache replacement algorithms because these references are often mistakenly categorized as hot blocks. Clock2Q+ is specifically designed for metadata caches and has been implemented in vSAN and VDFS, two flagship storage products of VMware by Broadcom. Similar to S3-FIFO, Clock2Q+ uses three queues; however, Clock2Q+ introduces a correlation window in the Small FIFO queue, where blocks in this window do not set the reference bit. This simple enhancement allows Clock2Q+ to outperform state-of-the-art replacement algorithms. Compared to S3-FIFO, the second-best performing algorithm, Clock2Q+ achieves up to a 28.5% lower miss ratio on metadata traces. Clock2Q+ possesses the essential properties required for large-scale storage systems: it has low CPU overhead on cache hits, low memory overhead, scales efficiently to multiple CPUs, and is both easy to tune and implement. Additionally, Clock2Q+ outperforms state-of-the-art cache replacement algorithms on data traces as well.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u4e13\u4e3a\u5143\u6570\u636e\u7f13\u5b58\u8bbe\u8ba1\u7684\u65b0\u578b\u7f13\u5b58\u66ff\u6362\u7b97\u6cd5Clock2Q+\uff0c\u901a\u8fc7\u5728Small FIFO\u961f\u5217\u4e2d\u5f15\u5165\u76f8\u5173\u6027\u7a97\u53e3\u6765\u6709\u6548\u5904\u7406\u5143\u6570\u636e\u8bbf\u95ee\u4e2d\u7684\u76f8\u5173\u5f15\u7528\u95ee\u9898\uff0c\u5728\u5143\u6570\u636e\u548c\u6570\u636e\u8f68\u8ff9\u4e0a\u5747\u4f18\u4e8e\u73b0\u6709\u5148\u8fdb\u7b97\u6cd5\u3002", "motivation": "\u5143\u6570\u636e\u7f13\u5b58\u4e2d\u5b58\u5728\u56fa\u6709\u7684\u76f8\u5173\u5f15\u7528\uff0c\u5373\u4f7f\u5bf9\u5e94\u7684\u6570\u636e\u8bbf\u95ee\u6ca1\u6709\u76f8\u5173\u6027\uff1b\u8fd9\u4e9b\u76f8\u5173\u5f15\u7528\u5e38\u88ab\u8bef\u5224\u4e3a\u70ed\u5757\uff0c\u4ece\u800c\u964d\u4f4e\u4f20\u7edf\u7f13\u5b58\u66ff\u6362\u7b97\u6cd5\u7684\u6548\u679c\u3002", "method": "Clock2Q+\u57fa\u4e8eS3-FIFO\u7684\u4e09\u961f\u5217\u7ed3\u6784\uff0c\u4f46\u5728Small FIFO\u961f\u5217\u4e2d\u5f15\u5165\u4e00\u4e2a\u201c\u76f8\u5173\u6027\u7a97\u53e3\u201d\uff0c\u8be5\u7a97\u53e3\u5185\u7684\u5757\u4e0d\u4f1a\u8bbe\u7f6e\u5f15\u7528\u4f4d\uff0c\u4ee5\u6b64\u533a\u5206\u771f\u6b63\u70ed\u5757\u4e0e\u76f8\u5173\u5f15\u7528\u3002", "result": "\u5728\u5143\u6570\u636e\u8f68\u8ff9\u4e0a\uff0cClock2Q+\u76f8\u6bd4\u6b21\u4f18\u7b97\u6cd5S3-FIFO\u6700\u591a\u964d\u4f4e28.5%\u7684\u7f3a\u5931\u7387\uff1b\u540c\u65f6\u5728\u6570\u636e\u8f68\u8ff9\u4e0a\u4e5f\u4f18\u4e8e\u5f53\u524d\u6700\u5148\u8fdb\u7684\u7f13\u5b58\u66ff\u6362\u7b97\u6cd5\uff0c\u5e76\u5177\u5907\u4f4eCPU\u5f00\u9500\u3001\u4f4e\u5185\u5b58\u5f00\u9500\u3001\u826f\u597d\u7684\u591a\u6838\u6269\u5c55\u6027\u53ca\u6613\u4e8e\u8c03\u4f18\u548c\u5b9e\u73b0\u7b49\u4f18\u70b9\u3002", "conclusion": "Clock2Q+\u662f\u4e00\u79cd\u9ad8\u6548\u3001\u5b9e\u7528\u4e14\u9002\u7528\u4e8e\u5927\u89c4\u6a21\u5b58\u50a8\u7cfb\u7edf\u7684\u7f13\u5b58\u66ff\u6362\u7b97\u6cd5\uff0c\u7279\u522b\u9002\u5408\u5904\u7406\u5143\u6570\u636e\u7f13\u5b58\u4e2d\u7684\u76f8\u5173\u5f15\u7528\u95ee\u9898\uff0c\u5e76\u5df2\u5728VMware by Broadcom\u7684vSAN\u548cVDFS\u4ea7\u54c1\u4e2d\u90e8\u7f72\u3002"}}
{"id": "2511.23278", "categories": ["cs.NI", "cs.CR", "cs.DC"], "pdf": "https://arxiv.org/pdf/2511.23278", "abs": "https://arxiv.org/abs/2511.23278", "authors": ["Jhonatan Tavori", "Anat Bremler-Barr", "Hanoch Levy", "Ofek Lavi"], "title": "RetryGuard: Preventing Self-Inflicted Retry Storms in Cloud Microservices Applications", "comment": null, "summary": "Modern cloud applications are built on independent, diverse microservices, offering scalability, flexibility, and usage-based billing. However, the structural design of these varied services, along with their reliance on auto-scalers for dynamic internet traffic, introduces significant coordination challenges. As we demonstrate in this paper, common default retry patterns used between misaligned services can turn into retry storms which drive up resource usage and costs, leading to self-inflicted Denial-of-Wallet (DoW) scenarios. To overcome these problems we introduce RetryGuard, a distributed framework for productive control of retry patterns across interdependent microservices. By managing retry policy on a per-service basis and making parallel decisions, RetryGuard prevents retry storms, curbs resource contention, and mitigates escalating operational costs. RetryGuard makes its decisions based on an analytic model that captures the relationships among retries, throughput (rejections), delays, and costs. Experimental results show that RetryGuard significantly reduces resource usage and costs compared to AWS standard and advanced retry policies. We further demonstrate its scalability and superior performance in a more complex Kubernetes deployment with the Istio service mesh, where it achieves substantial improvements.", "AI": {"tldr": "RetryGuard \u662f\u4e00\u79cd\u5206\u5e03\u5f0f\u6846\u67b6\uff0c\u7528\u4e8e\u63a7\u5236\u5fae\u670d\u52a1\u95f4\u7684\u91cd\u8bd5\u884c\u4e3a\uff0c\u9632\u6b62\u91cd\u8bd5\u98ce\u66b4\u3001\u8d44\u6e90\u4e89\u7528\u548c\u6210\u672c\u6fc0\u589e\uff0c\u5728 AWS \u548c Kubernetes+Istio \u73af\u5883\u4e2d\u5747\u663e\u8457\u964d\u4f4e\u8d44\u6e90\u4f7f\u7528\u4e0e\u8fd0\u8425\u6210\u672c\u3002", "motivation": "\u73b0\u4ee3\u4e91\u5e94\u7528\u4f9d\u8d56\u591a\u6837\u5316\u7684\u5fae\u670d\u52a1\u548c\u81ea\u52a8\u6269\u7f29\u5bb9\u673a\u5236\uff0c\u4f46\u670d\u52a1\u95f4\u9ed8\u8ba4\u7684\u91cd\u8bd5\u7b56\u7565\u5728\u6d41\u91cf\u6ce2\u52a8\u4e0b\u6613\u5f15\u53d1\u91cd\u8bd5\u98ce\u66b4\uff0c\u5bfc\u81f4\u8d44\u6e90\u6d6a\u8d39\u548c\u201c\u94b1\u5305\u62d2\u7edd\u670d\u52a1\u201d\uff08Denial-of-Wallet\uff09\u95ee\u9898\u3002", "method": "\u63d0\u51fa RetryGuard \u6846\u67b6\uff0c\u57fa\u4e8e\u5206\u6790\u6a21\u578b\u5bf9\u6bcf\u4e2a\u670d\u52a1\u72ec\u7acb\u7ba1\u7406\u91cd\u8bd5\u7b56\u7565\uff0c\u901a\u8fc7\u5e76\u884c\u51b3\u7b56\u534f\u8c03\u5fae\u670d\u52a1\u95f4\u7684\u91cd\u8bd5\u884c\u4e3a\uff0c\u7efc\u5408\u8003\u8651\u91cd\u8bd5\u3001\u541e\u5410\u91cf\u3001\u5ef6\u8fdf\u4e0e\u6210\u672c\u4e4b\u95f4\u7684\u5173\u7cfb\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cRetryGuard \u76f8\u6bd4 AWS \u6807\u51c6\u548c\u9ad8\u7ea7\u91cd\u8bd5\u7b56\u7565\u5927\u5e45\u964d\u4f4e\u8d44\u6e90\u4f7f\u7528\u548c\u6210\u672c\uff0c\u5e76\u5728 Kubernetes \u4e0e Istio \u670d\u52a1\u7f51\u683c\u7684\u590d\u6742\u90e8\u7f72\u4e2d\u5c55\u73b0\u51fa\u826f\u597d\u7684\u53ef\u6269\u5c55\u6027\u4e0e\u6027\u80fd\u4f18\u52bf\u3002", "conclusion": "RetryGuard \u80fd\u6709\u6548\u7f13\u89e3\u5fae\u670d\u52a1\u67b6\u6784\u4e2d\u7531\u91cd\u8bd5\u98ce\u66b4\u5f15\u53d1\u7684\u6210\u672c\u4e0e\u6027\u80fd\u95ee\u9898\uff0c\u4e3a\u4e91\u539f\u751f\u7cfb\u7edf\u63d0\u4f9b\u4e86\u4e00\u79cd\u5b9e\u7528\u4e14\u9ad8\u6548\u7684\u91cd\u8bd5\u63a7\u5236\u65b9\u6848\u3002"}}
{"id": "2511.21920", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.21920", "abs": "https://arxiv.org/abs/2511.21920", "authors": ["Apu Kumar Chakroborti", "Yi Ding", "Lipeng Wan"], "title": "Toward Automated and Trustworthy Scientific Analysis and Visualization with LLM-Generated Code", "comment": null, "summary": "As modern science becomes increasingly data-intensive, the ability to analyze and visualize large-scale, complex datasets is critical to accelerating discovery. However, many domain scientists lack the programming expertise required to develop custom data analysis workflows, creating barriers to timely and effective insight. Large language models (LLMs) offer a promising solution by generating executable code from natural language descriptions. In this paper, we investigate the trustworthiness of open-source LLMs in autonomously producing Python scripts for scientific data analysis and visualization. We construct a benchmark suite of domain-inspired prompts that reflect real-world research tasks and systematically evaluate the executability and correctness of the generated code. Our findings show that, without human intervention, the reliability of LLM-generated code is limited, with frequent failures caused by ambiguous prompts and the models' insufficient understanding of domain-specific contexts. To address these challenges, we design and assess three complementary strategies: data-aware prompt disambiguation, retrieval-augmented prompt enhancement, and iterative error repair. While these methods significantly improve execution success rates and output quality, further refinement is needed. This work highlights both the promise and current limitations of LLM-driven automation in scientific workflows and introduces actionable techniques and a reusable benchmark for building more inclusive, accessible, and trustworthy AI-assisted research tools.", "AI": {"tldr": "\u672c\u6587\u8bc4\u4f30\u5f00\u6e90\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u81ea\u52a8\u751f\u6210\u79d1\u5b66\u6570\u636e\u5206\u6790\u4e0e\u53ef\u89c6\u5316Python\u811a\u672c\u65b9\u9762\u7684\u53ef\u9760\u6027\uff0c\u53d1\u73b0\u5176\u5728\u65e0\u5e72\u9884\u60c5\u51b5\u4e0b\u5b58\u5728\u6267\u884c\u5931\u8d25\u95ee\u9898\uff0c\u5e76\u63d0\u51fa\u4e09\u79cd\u6539\u8fdb\u7b56\u7565\u4ee5\u63d0\u5347\u751f\u6210\u4ee3\u7801\u7684\u53ef\u6267\u884c\u6027\u4e0e\u6b63\u786e\u6027\u3002", "motivation": "\u73b0\u4ee3\u79d1\u5b66\u7814\u7a76\u65e5\u76ca\u4f9d\u8d56\u5927\u89c4\u6a21\u590d\u6742\u6570\u636e\uff0c\u4f46\u8bb8\u591a\u9886\u57df\u79d1\u5b66\u5bb6\u7f3a\u4e4f\u7f16\u7a0b\u80fd\u529b\uff0c\u96be\u4ee5\u9ad8\u6548\u6784\u5efa\u5206\u6790\u6d41\u7a0b\uff1b\u5927\u8bed\u8a00\u6a21\u578b\u867d\u53ef\u901a\u8fc7\u81ea\u7136\u8bed\u8a00\u751f\u6210\u4ee3\u7801\uff0c\u4f46\u5176\u5728\u79d1\u5b66\u573a\u666f\u4e0b\u7684\u53ef\u4fe1\u5ea6\u5c1a\u4e0d\u660e\u786e\u3002", "method": "\u6784\u5efa\u53cd\u6620\u771f\u5b9e\u79d1\u7814\u4efb\u52a1\u7684\u63d0\u793a\u57fa\u51c6\u96c6\uff0c\u7cfb\u7edf\u8bc4\u4f30LLM\u751f\u6210\u4ee3\u7801\u7684\u53ef\u6267\u884c\u6027\u4e0e\u6b63\u786e\u6027\uff1b\u5e76\u8bbe\u8ba1\u4e09\u79cd\u7b56\u7565\uff1a\u6570\u636e\u611f\u77e5\u7684\u63d0\u793a\u6d88\u6b67\u3001\u68c0\u7d22\u589e\u5f3a\u7684\u63d0\u793a\u4f18\u5316\u548c\u8fed\u4ee3\u9519\u8bef\u4fee\u590d\u3002", "result": "\u672a\u7ecf\u4eba\u5de5\u5e72\u9884\u65f6\uff0cLLM\u751f\u6210\u7684\u4ee3\u7801\u53ef\u9760\u6027\u6709\u9650\uff0c\u5e38\u56e0\u63d0\u793a\u6a21\u7cca\u6216\u7f3a\u4e4f\u9886\u57df\u7406\u89e3\u800c\u5931\u8d25\uff1b\u6240\u63d0\u4e09\u79cd\u7b56\u7565\u663e\u8457\u63d0\u5347\u4e86\u4ee3\u7801\u6267\u884c\u6210\u529f\u7387\u548c\u8f93\u51fa\u8d28\u91cf\u3002", "conclusion": "LLM\u5728\u79d1\u5b66\u5de5\u4f5c\u6d41\u81ea\u52a8\u5316\u4e2d\u5177\u6709\u6f5c\u529b\uff0c\u4f46\u4ecd\u5b58\u5728\u5c40\u9650\uff1b\u672c\u6587\u63d0\u51fa\u7684\u7b56\u7565\u548c\u53ef\u590d\u7528\u57fa\u51c6\u4e3a\u6784\u5efa\u66f4\u53ef\u4fe1\u3001\u6613\u7528\u7684AI\u8f85\u52a9\u79d1\u7814\u5de5\u5177\u63d0\u4f9b\u4e86\u53ef\u884c\u8def\u5f84\u3002"}}
{"id": "2511.22889", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2511.22889", "abs": "https://arxiv.org/abs/2511.22889", "authors": ["Fang Li"], "title": "The Immutable Tensor Architecture: A Pure Dataflow Approach for Secure, Energy-Efficient AI Inference", "comment": "Code and data can be found here: https://github.com/fanglioc/ita-fpga-prototype", "summary": "The deployment of Large Language Models (LLMs) on consumer edge devices is throttled by the \"Memory Wall\" -- the prohibitive bandwidth and energy cost of fetching gigabytes of model weights from DRAM for every token generated. Current architectures (GPUs, NPUs) treat model weights as mutable software data, incurring massive energy penalties to maintain general-purpose programmability. We propose The Immutable Tensor Architecture (ITA), a paradigm shift that treats model weights not as data, but as physical circuit topology. By encoding parameters directly into the metal interconnects and logic of mature-node ASICs (28nm/40nm), ITA eliminates the memory hierarchy entirely. We present a \"Split-Brain\" system design where a host CPU manages dynamic KV-cache operations while the ITA ASIC acts as a stateless, ROM-embedded dataflow engine.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u201c\u4e0d\u53ef\u53d8\u5f20\u91cf\u67b6\u6784\u201d\uff08ITA\uff09\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u5c06\u5927\u8bed\u8a00\u6a21\u578b\u6743\u91cd\u56fa\u5316\u5230ASIC\u7684\u7269\u7406\u7535\u8def\u4e2d\uff0c\u6d88\u9664\u5185\u5b58\u5899\u95ee\u9898\uff0c\u4ece\u800c\u5728\u8fb9\u7f18\u8bbe\u5907\u4e0a\u9ad8\u6548\u90e8\u7f72\u5927\u8bed\u8a00\u6a21\u578b\u3002", "motivation": "\u5f53\u524d\u5728\u6d88\u8d39\u7ea7\u8fb9\u7f18\u8bbe\u5907\u4e0a\u90e8\u7f72\u5927\u8bed\u8a00\u6a21\u578b\u53d7\u9650\u4e8e\u201c\u5185\u5b58\u5899\u201d\u95ee\u9898\uff0c\u5373\u6bcf\u6b21\u751f\u6210token\u90fd\u9700\u8981\u4eceDRAM\u4e2d\u8bfb\u53d6\u5927\u91cf\u6a21\u578b\u6743\u91cd\uff0c\u5e26\u6765\u9ad8\u6602\u7684\u5e26\u5bbd\u548c\u80fd\u8017\u5f00\u9500\uff1b\u73b0\u6709\u786c\u4ef6\u67b6\u6784\u5c06\u6743\u91cd\u89c6\u4e3a\u53ef\u53d8\u6570\u636e\uff0c\u4e3a\u4fdd\u6301\u901a\u7528\u53ef\u7f16\u7a0b\u6027\u800c\u4ed8\u51fa\u5de8\u5927\u80fd\u8017\u4ee3\u4ef7\u3002", "method": "\u63d0\u51fa\u4e0d\u53ef\u53d8\u5f20\u91cf\u67b6\u6784\uff08ITA\uff09\uff0c\u5c06\u6a21\u578b\u6743\u91cd\u76f4\u63a5\u7f16\u7801\u8fdb\u6210\u719f\u5de5\u827a\u8282\u70b9\uff08\u598228nm/40nm\uff09ASIC\u7684\u91d1\u5c5e\u4e92\u8fde\u4e0e\u903b\u8f91\u7535\u8def\u4e2d\uff0c\u4f7f\u6743\u91cd\u6210\u4e3a\u7269\u7406\u7535\u8def\u62d3\u6251\u7684\u4e00\u90e8\u5206\uff1b\u540c\u65f6\u91c7\u7528\u201cSplit-Brain\u201d\u7cfb\u7edf\u8bbe\u8ba1\uff0c\u7531\u4e3b\u673aCPU\u5904\u7406\u52a8\u6001KV\u7f13\u5b58\uff0cITA ASIC\u4f5c\u4e3a\u65e0\u72b6\u6001\u3001\u5185\u5d4cROM\u7684\u6570\u636e\u6d41\u5f15\u64ce\u3002", "result": "\u8be5\u65b9\u6cd5\u5b8c\u5168\u6d88\u9664\u4e86\u4f20\u7edf\u5185\u5b58\u5c42\u6b21\u7ed3\u6784\uff0c\u663e\u8457\u964d\u4f4e\u80fd\u8017\u4e0e\u5e26\u5bbd\u9700\u6c42\uff0c\u4e3a\u5728\u8fb9\u7f18\u8bbe\u5907\u4e0a\u9ad8\u6548\u8fd0\u884c\u5927\u8bed\u8a00\u6a21\u578b\u63d0\u4f9b\u4e86\u65b0\u8def\u5f84\u3002", "conclusion": "\u5c06\u6a21\u578b\u6743\u91cd\u56fa\u5316\u4e3a\u786c\u4ef6\u62d3\u6251\u7684ITA\u67b6\u6784\u662f\u4e00\u79cd\u6709\u524d\u666f\u7684\u8303\u5f0f\u8f6c\u53d8\uff0c\u80fd\u6709\u6548\u7a81\u7834\u5185\u5b58\u5899\u9650\u5236\uff0c\u63a8\u52a8\u5927\u8bed\u8a00\u6a21\u578b\u5728\u8d44\u6e90\u53d7\u9650\u8fb9\u7f18\u8bbe\u5907\u4e0a\u7684\u5b9e\u9645\u90e8\u7f72\u3002"}}
{"id": "2511.21956", "categories": ["cs.SE", "physics.acc-ph"], "pdf": "https://arxiv.org/pdf/2511.21956", "abs": "https://arxiv.org/abs/2511.21956", "authors": ["M. Polzin", "M. Guzman"], "title": "Beyond Like-for-Like: A User-centered Approach to Modernizing Legacy Applications", "comment": "The 20th International Conference on Accelerator and Large Experimental Physics Control Systems", "summary": "When modernizing a legacy application, it is easy to fall back on a like-for-like replica with new tools and updated design stylings, but this is an opportunity to explore making a more intuitive application that supports user tasks and efficiency. Rather than having a blank canvas-unburdened by legacy tech debt-to create a new application, you are working with an existing application that is integral to accelerator operations and one that expert users are already familiar with. Due to this, you might assume people will prefer the like-for-like, but you could be carrying forward the pain points, processes that are inefficient, and ultimately wind up with an application that no one wants to use because it doesn't solve existing problems. Getting users involved can make all the difference in your approach to modernizing a legacy application that caters to both newer and expert users. It also can bridge the gap between like-for-like and introducing new GUI design. Having a legacy application doesn't have to make the modernized one difficult to develop, as the existing application is a tool in how you move forward with the new application. It provides insight into areas that an application with a clean slate doesn't give you.", "AI": {"tldr": "\u5728\u73b0\u4ee3\u5316\u9057\u7559\u5e94\u7528\u7a0b\u5e8f\u65f6\uff0c\u4e0d\u5e94\u4ec5\u505a\u8868\u9762\u7ffb\u65b0\uff0c\u800c\u5e94\u901a\u8fc7\u7528\u6237\u53c2\u4e0e\u4f18\u5316\u7528\u6237\u4f53\u9a8c\uff0c\u517c\u987e\u65b0\u8001\u7528\u6237\u9700\u6c42\u3002", "motivation": "\u907f\u514d\u5728\u73b0\u4ee3\u5316\u8fc7\u7a0b\u4e2d\u7b80\u5355\u590d\u5236\u65e7\u7cfb\u7edf\uff0c\u4ece\u800c\u5ef6\u7eed\u5176\u4f4e\u6548\u6d41\u7a0b\u548c\u75db\u70b9\uff0c\u5bfc\u81f4\u65b0\u7cfb\u7edf\u65e0\u6cd5\u771f\u6b63\u6ee1\u8db3\u7528\u6237\u9700\u6c42\u3002", "method": "\u901a\u8fc7\u8ba9\u7528\u6237\uff08\u5305\u62ec\u65b0\u7528\u6237\u548c\u4e13\u5bb6\u7528\u6237\uff09\u79ef\u6781\u53c2\u4e0e\uff0c\u7ed3\u5408\u73b0\u6709\u9057\u7559\u7cfb\u7edf\u7684\u6d1e\u5bdf\uff0c\u6307\u5bfc\u65b0\u5e94\u7528\u7684\u8bbe\u8ba1\u4e0e\u5f00\u53d1\u3002", "result": "\u80fd\u591f\u5f25\u5408\u201c\u9010\u5b57\u590d\u5236\u201d\u4e0e\u5f15\u5165\u5168\u65b0GUI\u8bbe\u8ba1\u4e4b\u95f4\u7684\u5dee\u8ddd\uff0c\u6253\u9020\u66f4\u76f4\u89c2\u3001\u9ad8\u6548\u4e14\u88ab\u7528\u6237\u63a5\u53d7\u7684\u5e94\u7528\u7a0b\u5e8f\u3002", "conclusion": "\u9057\u7559\u5e94\u7528\u672c\u8eab\u53ef\u4f5c\u4e3a\u65b0\u5f00\u53d1\u7684\u5b9d\u8d35\u53c2\u8003\uff0c\u901a\u8fc7\u7528\u6237\u53c2\u4e0e\u53ef\u6709\u6548\u63d0\u5347\u73b0\u4ee3\u5316\u6210\u679c\u7684\u5b9e\u7528\u6027\u548c\u53ef\u7528\u6027\u3002"}}
{"id": "2511.22010", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2511.22010", "abs": "https://arxiv.org/abs/2511.22010", "authors": ["Provakar Mondal", "Eli Tilevich"], "title": "An Empirical Study of Cross-Language Interoperability in Replicated Data Systems", "comment": null, "summary": "BACKGROUND: Modern distributed systems replicate data across multiple execution sites. Business requirements and resource constraints often necessitate mixing different languages across replica sites. To facilitate the management of replicated data, modern software engineering practices integrate special-purpose replicated data libraries (RDLs) that provide read-write access to the data and ensure its synchronization. Irrespective of the implementation languages, an RDL typically uses a single language or offers bindings to a designated one. Hence, integrating existing RDLs in multilingual environments requires special-purpose code, whose software quality and performance characteristics are poorly understood.\n  AIMS: We aim to bridge this knowledge gap to understand the software quality and performance characteristics of RDL integration in multilingual environments.\n  METHOD: We conduct an empirical study of two key strategies for integrating RDLs in the context of multilingual replicated data systems: foreign-function interface (FFI) and a common data format (CDF); we measure and compare their respective software metrics and performance to understand their suitability for the task at hand.\n  RESULTS: Our results reveal that adopting CDF for cross-language interaction offers software quality, latency, memory consumption, and throughput advantages. We further validate our findings by (1) creating a CDF-based RDL for mixing compiled, interpreted, and managed languages; and (2) enhancing our RDL with plug-in extensibility that enables adding functionality in a single language while maintaining integration within a multilingual environment.\n  CONCLUSIONS: With modern distributed systems utilizing multiple languages, our findings provide novel insights for designing RDLs in multilingual replicated data systems.", "AI": {"tldr": "\u672c\u6587\u901a\u8fc7\u5b9e\u8bc1\u7814\u7a76\u6bd4\u8f83\u4e86\u5728\u591a\u8bed\u8a00\u590d\u5236\u6570\u636e\u7cfb\u7edf\u4e2d\u96c6\u6210\u590d\u5236\u6570\u636e\u5e93\uff08RDL\uff09\u7684\u4e24\u79cd\u7b56\u7565\u2014\u2014\u5916\u90e8\u51fd\u6570\u63a5\u53e3\uff08FFI\uff09\u548c\u901a\u7528\u6570\u636e\u683c\u5f0f\uff08CDF\uff09\uff0c\u53d1\u73b0CDF\u5728\u8f6f\u4ef6\u8d28\u91cf\u3001\u5ef6\u8fdf\u3001\u5185\u5b58\u6d88\u8017\u548c\u541e\u5410\u91cf\u65b9\u9762\u66f4\u5177\u4f18\u52bf\uff0c\u5e76\u636e\u6b64\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u652f\u6301\u591a\u8bed\u8a00\u4e14\u5177\u6709\u63d2\u4ef6\u6269\u5c55\u80fd\u529b\u7684RDL\u3002", "motivation": "\u73b0\u4ee3\u5206\u5e03\u5f0f\u7cfb\u7edf\u5e38\u5728\u591a\u4e2a\u6267\u884c\u7ad9\u70b9\u95f4\u590d\u5236\u6570\u636e\uff0c\u4e14\u56e0\u4e1a\u52a1\u548c\u8d44\u6e90\u9650\u5236\u9700\u6df7\u5408\u4f7f\u7528\u591a\u79cd\u7f16\u7a0b\u8bed\u8a00\u3002\u73b0\u6709RDL\u901a\u5e38\u4ec5\u652f\u6301\u5355\u4e00\u8bed\u8a00\u6216\u63d0\u4f9b\u7279\u5b9a\u8bed\u8a00\u7ed1\u5b9a\uff0c\u5728\u591a\u8bed\u8a00\u73af\u5883\u4e2d\u96c6\u6210\u9700\u989d\u5916\u4ee3\u7801\uff0c\u4f46\u5176\u8f6f\u4ef6\u8d28\u91cf\u548c\u6027\u80fd\u7279\u5f81\u5c1a\u4e0d\u660e\u786e\u3002", "method": "\u5f00\u5c55\u5b9e\u8bc1\u7814\u7a76\uff0c\u5bf9\u6bd4FFI\u548cCDF\u4e24\u79cdRDL\u96c6\u6210\u7b56\u7565\uff0c\u6d4b\u91cf\u5e76\u6bd4\u8f83\u5b83\u4eec\u7684\u8f6f\u4ef6\u6307\u6807\u4e0e\u6027\u80fd\u8868\u73b0\uff1b\u8fdb\u4e00\u6b65\u6784\u5efa\u57fa\u4e8eCDF\u7684RDL\u4ee5\u652f\u6301\u7f16\u8bd1\u578b\u3001\u89e3\u91ca\u578b\u548c\u6258\u7ba1\u8bed\u8a00\uff0c\u5e76\u52a0\u5165\u63d2\u4ef6\u6269\u5c55\u673a\u5236\u3002", "result": "CDF\u5728\u8de8\u8bed\u8a00\u4ea4\u4e92\u4e2d\u5c55\u73b0\u51fa\u66f4\u4f18\u7684\u8f6f\u4ef6\u8d28\u91cf\u3001\u66f4\u4f4e\u7684\u5ef6\u8fdf\u3001\u66f4\u5c11\u7684\u5185\u5b58\u5360\u7528\u548c\u66f4\u9ad8\u7684\u541e\u5410\u91cf\uff1b\u6240\u6784\u5efa\u7684CDF-based RDL\u6210\u529f\u652f\u6301\u591a\u8bed\u8a00\u6df7\u5408\uff0c\u5e76\u901a\u8fc7\u63d2\u4ef6\u673a\u5236\u5b9e\u73b0\u5355\u8bed\u8a00\u529f\u80fd\u6269\u5c55\u800c\u4e0d\u7834\u574f\u591a\u8bed\u8a00\u96c6\u6210\u3002", "conclusion": "\u7814\u7a76\u4e3a\u591a\u8bed\u8a00\u590d\u5236\u6570\u636e\u7cfb\u7edf\u4e2dRDL\u7684\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u65b0\u89c1\u89e3\uff0c\u8868\u660e\u91c7\u7528\u901a\u7528\u6570\u636e\u683c\u5f0f\u662f\u63d0\u5347\u8f6f\u4ef6\u8d28\u91cf\u4e0e\u7cfb\u7edf\u6027\u80fd\u7684\u6709\u6548\u9014\u5f84\u3002"}}
{"id": "2511.23011", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2511.23011", "abs": "https://arxiv.org/abs/2511.23011", "authors": ["Yanjing Wang", "Lizhou Wu", "Sunfeng Gao", "Yibo Tang", "Junhui Luo", "Zicong Wang", "Yang Ou", "Dezun Dong", "Nong Xiao", "Mingche Lai"], "title": "Cohet: A CXL-Driven Coherent Heterogeneous Computing Framework with Hardware-Calibrated Full-System Simulation", "comment": "Accepted by HPCA 2026", "summary": "Conventional heterogeneous computing systems built on PCIe interconnects suffer from inefficient fine-grained host-device interactions and complex programming models. In recent years, many proprietary and open cache-coherent interconnect standards have emerged, among which compute express link (CXL) prevails in the open-standard domain after acquiring several competing solutions. Although CXL-based coherent heterogeneous computing holds the potential to fundamentally transform the collaborative computing mode of CPUs and XPUs, research in this direction remains hampered by the scarcity of available CXL-supported platforms, immature software/hardware ecosystems, and unclear application prospects. This paper presents Cohet, the first CXL-driven coherent heterogeneous computing framework. Cohet decouples the compute and memory resources to form unbiased CPU and XPU pools which share a single unified and coherent memory pool. It exposes a standard malloc/mmap interface to both CPU and XPU compute threads, leaving the OS dealing with smart memory allocation and management of heterogeneous resources. To facilitate Cohet research, we also present a full-system cycle-level simulator named SimCXL, which is capable of modeling all CXL sub-protocols and device types. SimCXL has been rigorously calibrated against a real CXL testbed with various CXL memory and accelerators, showing an average simulation error of 3%. Our evaluation reveals that CXL.cache reduces latency by 68% and increases bandwidth by 14.4x compared to DMA transfers at cacheline granularity. Building upon these insights, we demonstrate the benefits of Cohet with two killer apps, which are remote atomic operation (RAO) and remote procedure call (RPC). Compared to PCIe-NIC design, CXL-NIC achieves a 5.5 to 40.2x speedup for RAO offloading and an average speedup of 1.86x for RPC (de)serialization offloading.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86Cohet\u2014\u2014\u9996\u4e2a\u57fa\u4e8eCXL\u7684\u7f13\u5b58\u4e00\u81f4\u6027\u5f02\u6784\u8ba1\u7b97\u6846\u67b6\uff0c\u901a\u8fc7\u89e3\u8026\u8ba1\u7b97\u4e0e\u5185\u5b58\u8d44\u6e90\u6784\u5efa\u7edf\u4e00\u4e00\u81f4\u5185\u5b58\u6c60\uff0c\u5e76\u63d0\u4f9b\u6807\u51c6\u5185\u5b58\u63a5\u53e3\uff1b\u540c\u65f6\u5f00\u53d1\u4e86\u9ad8\u7cbe\u5ea6\u5168\u7cfb\u7edf\u6a21\u62df\u5668SimCXL\uff0c\u5b9e\u9a8c\u8868\u660eCXL\u76f8\u6bd4PCIe\u5728\u7ec6\u7c92\u5ea6\u6570\u636e\u4f20\u8f93\u3001\u8fdc\u7a0b\u539f\u5b50\u64cd\u4f5c\u548c\u8fdc\u7a0b\u8fc7\u7a0b\u8c03\u7528\u7b49\u573a\u666f\u4e2d\u6027\u80fd\u663e\u8457\u63d0\u5347\u3002", "motivation": "\u4f20\u7edf\u57fa\u4e8ePCIe\u7684\u5f02\u6784\u8ba1\u7b97\u7cfb\u7edf\u5b58\u5728\u7ec6\u7c92\u5ea6\u4e3b\u673a-\u8bbe\u5907\u4ea4\u4e92\u6548\u7387\u4f4e\u548c\u7f16\u7a0b\u6a21\u578b\u590d\u6742\u7684\u95ee\u9898\uff1b\u5c3d\u7ba1CXL\u7b49\u7f13\u5b58\u4e00\u81f4\u4e92\u8fde\u6807\u51c6\u5174\u8d77\uff0c\u4f46\u53d7\u9650\u4e8e\u786c\u4ef6\u5e73\u53f0\u7a00\u7f3a\u3001\u8f6f\u786c\u4ef6\u751f\u6001\u4e0d\u6210\u719f\u53ca\u5e94\u7528\u573a\u666f\u4e0d\u660e\uff0c\u76f8\u5173\u7814\u7a76\u8fdb\u5c55\u7f13\u6162\u3002", "method": "\u63d0\u51faCohet\u6846\u67b6\uff0c\u5c06CPU\u4e0eXPU\u7684\u8ba1\u7b97\u548c\u5185\u5b58\u8d44\u6e90\u89e3\u8026\uff0c\u6784\u5efa\u5171\u4eab\u7684\u7edf\u4e00\u7f13\u5b58\u4e00\u81f4\u5185\u5b58\u6c60\uff0c\u5e76\u5411\u8ba1\u7b97\u7ebf\u7a0b\u66b4\u9732\u6807\u51c6malloc/mmap\u63a5\u53e3\uff1b\u540c\u65f6\u5f00\u53d1\u652f\u6301\u6240\u6709CXL\u5b50\u534f\u8bae\u548c\u8bbe\u5907\u7c7b\u578b\u7684\u5468\u671f\u7ea7\u5168\u7cfb\u7edf\u6a21\u62df\u5668SimCXL\u7528\u4e8e\u7814\u7a76\u9a8c\u8bc1\u3002", "result": "\u8bc4\u4f30\u663e\u793a\uff0c\u76f8\u6bd4DMA\u4f20\u8f93\uff0cCXL.cache\u5728\u7f13\u5b58\u884c\u7c92\u5ea6\u4e0b\u5ef6\u8fdf\u964d\u4f4e68%\uff0c\u5e26\u5bbd\u63d0\u534714.4\u500d\uff1b\u5728\u8fdc\u7a0b\u539f\u5b50\u64cd\u4f5c\uff08RAO\uff09\u548c\u8fdc\u7a0b\u8fc7\u7a0b\u8c03\u7528\uff08RPC\uff09\u4e24\u4e2a\u5178\u578b\u5e94\u7528\u4e2d\uff0cCXL-NIC\u76f8\u6bd4PCIe-NIC\u5206\u522b\u5b9e\u73b05.5\u201340.2\u500d\u548c\u5e73\u57471.86\u500d\u7684\u52a0\u901f\u3002", "conclusion": "CXL\u9a71\u52a8\u7684\u7f13\u5b58\u4e00\u81f4\u5f02\u6784\u8ba1\u7b97\u80fd\u663e\u8457\u63d0\u5347\u7ec6\u7c92\u5ea6\u534f\u540c\u6548\u7387\uff0cCohet\u6846\u67b6\u4e0eSimCXL\u6a21\u62df\u5668\u4e3a\u8be5\u65b9\u5411\u63d0\u4f9b\u4e86\u53ef\u884c\u7684\u7cfb\u7edf\u67b6\u6784\u4e0e\u7814\u7a76\u5de5\u5177\uff0c\u9a8c\u8bc1\u4e86CXL\u5728\u65b0\u578b\u5f02\u6784\u8ba1\u7b97\u4e2d\u7684\u5de8\u5927\u6f5c\u529b\u3002"}}
{"id": "2511.21964", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2511.21964", "abs": "https://arxiv.org/abs/2511.21964", "authors": ["Ali Sayedsalehi", "Peter C. Rigby", "Audris Mockus"], "title": "DRS-OSS: LLM-Driven Diff Risk Scoring Tool for PR Risk Prediction", "comment": "8 pages, 4 figures, includes system architecture diagrams, Web UI screenshots, GitHub App examples, and an appendix with API endpoints. Full replication package and demo materials available", "summary": "In large-scale open-source projects, hundreds of pull requests land daily, each a potential source of regressions. Diff Risk Scoring (DRS) estimates the likelihood that a diff will introduce a defect, enabling better review prioritization, test planning, and CI/CD gating. We present DRS-OSS, an open-source DRS system equipped with a public API, web UI, and GitHub plugin. DRS-OSS uses a fine-tuned Llama 3.1 8B sequence classifier trained on the ApacheJIT dataset, consuming long-context representations that combine commit messages, structured diffs, and change metrics. Through parameter-efficient adaptation, 4-bit QLoRA, and DeepSpeed ZeRO-3 CPU offloading, we train 22k-token contexts on a single 20 GB GPU. On the ApacheJIT benchmark, DRS-OSS achieves state-of-the-art performance (F1 = 0.64, ROC-AUC = 0.89). Simulations show that gating only the riskiest 30% of commits can prevent up to 86.4% of defect-inducing changes. The system integrates with developer workflows through an API gateway, a React dashboard, and a GitHub App that posts risk labels on pull requests. We release the full replication package, fine-tuning scripts, deployment artifacts, code, demo video, and public website.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86DRS-OSS\uff0c\u4e00\u4e2a\u5f00\u6e90\u7684\u5dee\u5f02\u98ce\u9669\u8bc4\u5206\u7cfb\u7edf\uff0c\u5229\u7528\u5fae\u8c03\u540e\u7684Llama 3.1 8B\u6a21\u578b\u5bf9\u4ee3\u7801\u63d0\u4ea4\u8fdb\u884c\u7f3a\u9677\u98ce\u9669\u9884\u6d4b\uff0c\u5728ApacheJIT\u6570\u636e\u96c6\u4e0a\u8fbe\u5230SOTA\u6027\u80fd\uff0c\u5e76\u901a\u8fc7GitHub\u63d2\u4ef6\u7b49\u5de5\u5177\u96c6\u6210\u5230\u5f00\u53d1\u8005\u5de5\u4f5c\u6d41\u4e2d\u3002", "motivation": "\u5728\u5927\u89c4\u6a21\u5f00\u6e90\u9879\u76ee\u4e2d\uff0c\u6bcf\u5929\u6709\u5927\u91cf\u62c9\u53d6\u8bf7\u6c42\u5408\u5e76\uff0c\u53ef\u80fd\u5f15\u5165\u56de\u5f52\u7f3a\u9677\u3002\u9700\u8981\u4e00\u79cd\u673a\u5236\u6765\u8bc4\u4f30\u6bcf\u4e2a\u4ee3\u7801\u53d8\u66f4\u5f15\u5165\u7f3a\u9677\u7684\u53ef\u80fd\u6027\uff0c\u4ee5\u4f18\u5316\u4ee3\u7801\u5ba1\u67e5\u3001\u6d4b\u8bd5\u89c4\u5212\u548cCI/CD\u6d41\u7a0b\u3002", "method": "\u57fa\u4e8eApacheJIT\u6570\u636e\u96c6\uff0c\u4f7f\u7528\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\uff084-bit QLoRA\uff09\u3001DeepSpeed ZeRO-3 CPU\u5378\u8f7d\u7b49\u6280\u672f\uff0c\u5728\u5355\u5f2020GB GPU\u4e0a\u8bad\u7ec3\u652f\u630122k token\u4e0a\u4e0b\u6587\u7684Llama 3.1 8B\u5e8f\u5217\u5206\u7c7b\u5668\uff1b\u8f93\u5165\u5305\u62ec\u63d0\u4ea4\u4fe1\u606f\u3001\u7ed3\u6784\u5316diff\u548c\u53d8\u66f4\u6307\u6807\uff1b\u7cfb\u7edf\u63d0\u4f9bAPI\u3001Web\u754c\u9762\u548cGitHub\u63d2\u4ef6\u3002", "result": "\u5728ApacheJIT\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cDRS-OSS\u53d6\u5f97F1=0.64\u3001ROC-AUC=0.89\u7684SOTA\u7ed3\u679c\uff1b\u6a21\u62df\u663e\u793a\uff0c\u4ec5\u5bf9\u98ce\u9669\u6700\u9ad8\u768430%\u63d0\u4ea4\u8fdb\u884c\u62e6\u622a\uff0c\u53ef\u9632\u6b62\u9ad8\u8fbe86.4%\u7684\u7f3a\u9677\u5f15\u5165\u3002", "conclusion": "DRS-OSS\u6709\u6548\u5b9e\u73b0\u4e86\u9ad8\u7cbe\u5ea6\u7684\u4ee3\u7801\u53d8\u66f4\u98ce\u9669\u9884\u6d4b\uff0c\u5177\u5907\u826f\u597d\u7684\u5de5\u7a0b\u5b9e\u7528\u6027\u4e0e\u5f00\u6e90\u751f\u6001\u96c6\u6210\u80fd\u529b\uff0c\u5df2\u516c\u5f00\u53d1\u5e03\u5b8c\u6574\u590d\u73b0\u5305\u548c\u90e8\u7f72\u8d44\u6e90\u3002"}}
{"id": "2511.22333", "categories": ["cs.DC", "cs.CL"], "pdf": "https://arxiv.org/pdf/2511.22333", "abs": "https://arxiv.org/abs/2511.22333", "authors": ["Jinjun Yi", "Zhixin Zhao", "Yitao Hu", "Ke Yan", "Weiwei Sun", "Hao Wang", "Laiping Zhao", "Yuhao Zhang", "Wenxin Li", "Keqiu Li"], "title": "PAT: Accelerating LLM Decoding via Prefix-Aware Attention with Resource Efficient Multi-Tile Kernel", "comment": "Accepted by ASPLOS'26", "summary": "LLM serving is increasingly dominated by decode attention, which is a memory-bound operation due to massive KV cache loading from global memory. Meanwhile, real-world workloads exhibit substantial, hierarchical shared prefixes across requests (e.g., system prompts, tools/templates, RAG). Existing attention implementations fail to fully exploit prefix sharing: *one-query-per-CTA* execution repeatedly loads shared prefix KV cache, while *one-size-fits-all* tiling leaves on-chip resources idle and exacerbates bubbles for uneven KV lengths. These choices amplify memory bandwidth pressure and stall memory-bound decode attention.\n  This paper introduces PAT, a prefix-aware attention kernel implementation for LLM decoding that organizes execution with a pack-forward-merge paradigm. PAT packs queries by shared prefix to reduce repeated memory accesses, runs a customized multi-tile kernel to achieve high resource efficiency. It further applies practical multi-stream forwarding and KV splitting to reduce resource bubbles. The final merge performs online softmax with negligible overhead. We implement PAT as an off-the-shelf plugin for vLLM. Evaluation on both real-world and synthetic workloads shows that PAT reduces attention latency by 67.4% on average and TPOT by 13.6-83.4% under the same configurations against state-of-the-art attention kernels.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faPAT\uff0c\u4e00\u79cd\u9762\u5411LLM\u89e3\u7801\u7684\u524d\u7f00\u611f\u77e5\u6ce8\u610f\u529b\u6838\uff0c\u901a\u8fc7\u6253\u5305\u5171\u4eab\u524d\u7f00\u67e5\u8be2\u3001\u5b9a\u5236\u591a\u5206\u5757\u6838\u53ca\u591a\u6d41\u8f6c\u53d1\u7b49\u6280\u672f\uff0c\u663e\u8457\u964d\u4f4e\u6ce8\u610f\u529b\u5ef6\u8fdf\u548cTPOT\u3002", "motivation": "\u73b0\u6709\u6ce8\u610f\u529b\u5b9e\u73b0\u672a\u80fd\u5145\u5206\u5229\u7528\u771f\u5b9e\u8d1f\u8f7d\u4e2d\u5927\u91cf\u5b58\u5728\u7684\u5206\u5c42\u5171\u4eab\u524d\u7f00\uff08\u5982\u7cfb\u7edf\u63d0\u793a\u3001\u5de5\u5177\u6a21\u677f\u3001RAG\uff09\uff0c\u5bfc\u81f4\u91cd\u590d\u52a0\u8f7dKV\u7f13\u5b58\u3001\u7247\u4e0a\u8d44\u6e90\u95f2\u7f6e\u53ca\u5185\u5b58\u5e26\u5bbd\u538b\u529b\u589e\u5927\uff0c\u4ece\u800c\u62d6\u6162\u5185\u5b58\u53d7\u9650\u7684\u89e3\u7801\u6ce8\u610f\u529b\u64cd\u4f5c\u3002", "method": "\u63d0\u51faPAT\u65b9\u6cd5\uff0c\u91c7\u7528\u201c\u6253\u5305-\u524d\u5411-\u5408\u5e76\u201d\u8303\u5f0f\uff1a\u6309\u5171\u4eab\u524d\u7f00\u6253\u5305\u67e5\u8be2\u4ee5\u51cf\u5c11\u91cd\u590d\u5185\u5b58\u8bbf\u95ee\uff1b\u8fd0\u884c\u5b9a\u5236\u5316\u591a\u5206\u5757\u6838\u63d0\u5347\u8d44\u6e90\u5229\u7528\u7387\uff1b\u7ed3\u5408\u591a\u6d41\u524d\u5411\u4e0eKV\u5207\u5206\u51cf\u5c11\u8d44\u6e90\u6c14\u6ce1\uff1b\u6700\u540e\u901a\u8fc7\u5728\u7ebfsoftmax\u5b8c\u6210\u5408\u5e76\uff0c\u5f00\u9500\u53ef\u5ffd\u7565\u3002\u8be5\u65b9\u6cd5\u4ee5\u5373\u63d2\u5373\u7528\u63d2\u4ef6\u5f62\u5f0f\u96c6\u6210\u5230vLLM\u4e2d\u3002", "result": "\u5728\u771f\u5b9e\u4e0e\u5408\u6210\u8d1f\u8f7d\u4e0a\u7684\u8bc4\u4f30\u8868\u660e\uff0c\u76f8\u6bd4\u5f53\u524d\u6700\u5148\u8fdb\u7684\u6ce8\u610f\u529b\u6838\uff0cPAT\u5728\u76f8\u540c\u914d\u7f6e\u4e0b\u5e73\u5747\u964d\u4f4e\u6ce8\u610f\u529b\u5ef6\u8fdf67.4%\uff0cTPOT\u964d\u4f4e13.6%\u201383.4%\u3002", "conclusion": "PAT\u901a\u8fc7\u9ad8\u6548\u5229\u7528\u8bf7\u6c42\u95f4\u7684\u5171\u4eab\u524d\u7f00\u4fe1\u606f\uff0c\u5728\u4e0d\u6539\u53d8\u6a21\u578b\u7ed3\u6784\u7684\u524d\u63d0\u4e0b\u663e\u8457\u63d0\u5347\u4e86LLM\u89e3\u7801\u9636\u6bb5\u7684\u6ce8\u610f\u529b\u8ba1\u7b97\u6548\u7387\uff0c\u6709\u6548\u7f13\u89e3\u4e86\u5185\u5b58\u74f6\u9888\u95ee\u9898\u3002"}}
{"id": "2511.23203", "categories": ["cs.AR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.23203", "abs": "https://arxiv.org/abs/2511.23203", "authors": ["Jordi Fornt", "Pau Fontova-Must\u00e9", "Adrian Gras", "Omar Lahyani", "Mart\u00ed Caro", "Jaume Abella", "Francesc Moll", "Josep Altet"], "title": "GAVINA: flexible aggressive undervolting for bit-serial mixed-precision DNN acceleration", "comment": "Presented in the 2025 IEEE/ACM International Symposium on Low Power Electronics and Design (ISLPED). Conference proceedings pending to be published", "summary": "Voltage overscaling, or undervolting, is an enticing approximate technique in the context of energy-efficient Deep Neural Network (DNN) acceleration, given the quadratic relationship between power and voltage. Nevertheless, its very high error rate has thwarted its general adoption. Moreover, recent undervolting accelerators rely on 8-bit arithmetic and cannot compete with state-of-the-art low-precision (<8b) architectures. To overcome these issues, we propose a new technique called Guarded Aggressive underVolting (GAV), which combines the ideas of undervolting and bit-serial computation to create a flexible approximation method based on aggressively lowering the supply voltage on a select number of least significant bit combinations. Based on this idea, we implement GAVINA (GAV mIxed-precisioN Accelerator), a novel architecture that supports arbitrary mixed precision and flexible undervolting, with an energy efficiency of up to 89 TOP/sW in its most aggressive configuration. By developing an error model of GAVINA, we show that GAV can achieve an energy efficiency boost of 20% via undervolting, with negligible accuracy degradation on ResNet-18.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aGAV\uff08Guarded Aggressive underVolting\uff09\u7684\u65b0\u6280\u672f\uff0c\u7ed3\u5408\u7535\u538b\u4e0b\u7f29\uff08undervolting\uff09\u4e0e\u4f4d\u4e32\u884c\u8ba1\u7b97\uff0c\u5728\u4fdd\u8bc1\u7cbe\u5ea6\u51e0\u4e4e\u65e0\u635f\u7684\u524d\u63d0\u4e0b\u663e\u8457\u63d0\u5347\u80fd\u6548\uff0c\u5e76\u57fa\u4e8e\u6b64\u6784\u5efa\u4e86\u652f\u6301\u4efb\u610f\u6df7\u5408\u7cbe\u5ea6\u7684\u52a0\u901f\u5668GAVINA\uff0c\u6700\u9ad8\u80fd\u6548\u8fbe89 TOP/sW\u3002", "motivation": "\u7535\u538b\u4e0b\u7f29\u867d\u80fd\u663e\u8457\u964d\u4f4e\u529f\u8017\uff0c\u4f46\u56e0\u9519\u8bef\u7387\u8fc7\u9ad8\u800c\u96be\u4ee5\u5e7f\u6cdb\u5e94\u7528\uff1b\u540c\u65f6\u73b0\u6709\u4e0b\u7f29\u52a0\u901f\u5668\u591a\u57fa\u4e8e8\u4f4d\u8fd0\u7b97\uff0c\u65e0\u6cd5\u4e0e\u5f53\u524d\u4e3b\u6d41\u7684\u4f4e\u7cbe\u5ea6\uff08<8\u4f4d\uff09\u67b6\u6784\u7ade\u4e89\u3002", "method": "\u63d0\u51faGAV\u65b9\u6cd5\uff0c\u901a\u8fc7\u5728\u9009\u5b9a\u7684\u6700\u4f4e\u6709\u6548\u4f4d\u7ec4\u5408\u4e0a\u6fc0\u8fdb\u5730\u964d\u4f4e\u4f9b\u7535\u7535\u538b\uff0c\u7ed3\u5408\u4f4d\u4e32\u884c\u8ba1\u7b97\u5b9e\u73b0\u7075\u6d3b\u8fd1\u4f3c\uff1b\u5e76\u636e\u6b64\u8bbe\u8ba1\u652f\u6301\u4efb\u610f\u6df7\u5408\u7cbe\u5ea6\u548c\u7075\u6d3b\u7535\u538b\u4e0b\u7f29\u7684GAVINA\u52a0\u901f\u5668\u3002", "result": "GAVINA\u5728\u6700\u6fc0\u8fdb\u914d\u7f6e\u4e0b\u80fd\u6548\u9ad8\u8fbe89 TOP/sW\uff1b\u901a\u8fc7\u8bef\u5dee\u5efa\u6a21\u8868\u660e\uff0c\u5728ResNet-18\u4e0a\u4f7f\u7528GAV\u53ef\u5b9e\u73b020%\u7684\u80fd\u6548\u63d0\u5347\uff0c\u4e14\u7cbe\u5ea6\u635f\u5931\u53ef\u5ffd\u7565\u3002", "conclusion": "GAV\u901a\u8fc7\u7ed3\u5408\u7535\u538b\u4e0b\u7f29\u4e0e\u4f4d\u4e32\u884c\u8ba1\u7b97\uff0c\u5728\u7ef4\u6301\u6a21\u578b\u7cbe\u5ea6\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u80fd\u6548\uff0c\u4e3a\u4f4e\u529f\u8017DNN\u52a0\u901f\u63d0\u4f9b\u4e86\u4e00\u79cd\u53ef\u884c\u4e14\u9ad8\u6548\u7684\u65b0\u65b9\u6848\u3002"}}
{"id": "2511.22118", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2511.22118", "abs": "https://arxiv.org/abs/2511.22118", "authors": ["Yihan Dai", "Dimitrios Stamatios Bouras", "Haoxiang Jia", "Sergey Mechtaev"], "title": "Statistical Independence Aware Caching for LLM Workflows", "comment": null, "summary": "Large language models (LLMs) inference is both expensive and slow. Local caching of responses offers a practical solution to reduce the cost and latency of LLM queries. In research contexts, caching also enhances reproducibility and provides flexibility for experimentation. However, naive reuse of cached responses compromises statistical independence, a critical property for probabilistic workflows. In applications of LLM for code, it underpins performance metrics such as Pass@k and uncertainty estimation, as well as algorithms like program repair loops and retries. Existing LLM caching systems lack ways to enforce statistical independence constraints. To address this, we introduce Mnimi, a cache design pattern that supports modular LLM workflows while ensuring statistical integrity at the component level. Its core innovation lies in encapsulating statistical constraints within the type of LLM references, allowing users to manage and transform these types according to the scope and requirements of their algorithm. We implemented this design pattern in Python using a combination of decorators and iterators over infinite sequences. A case study on SpecFix, an recent automated program specification repair system, highlights how Mnimi improves reproducibility, ease of debugging, time and cost efficiency while preserving statistical correctness.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faMnimi\uff0c\u4e00\u79cd\u65b0\u578b\u7f13\u5b58\u8bbe\u8ba1\u6a21\u5f0f\uff0c\u5728\u4fdd\u8bc1\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u63a8\u7406\u4e2d\u7edf\u8ba1\u72ec\u7acb\u6027\u7684\u540c\u65f6\uff0c\u63d0\u5347\u53ef\u590d\u73b0\u6027\u3001\u8c03\u8bd5\u4fbf\u5229\u6027\u53ca\u6210\u672c\u6548\u7387\u3002", "motivation": "\u73b0\u6709LLM\u7f13\u5b58\u65b9\u6cd5\u5728\u91cd\u7528\u54cd\u5e94\u65f6\u4f1a\u7834\u574f\u7edf\u8ba1\u72ec\u7acb\u6027\uff0c\u5f71\u54cd\u57fa\u4e8e\u6982\u7387\u7684\u5de5\u4f5c\u6d41\uff08\u5982Pass@k\u3001\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\u548c\u7a0b\u5e8f\u4fee\u590d\u7b49\uff09\u7684\u6b63\u786e\u6027\uff0c\u4e9f\u9700\u4e00\u79cd\u517c\u987e\u7f13\u5b58\u6548\u76ca\u4e0e\u7edf\u8ba1\u5b8c\u6574\u6027\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u4f5c\u8005\u63d0\u51faMnimi\u7f13\u5b58\u8bbe\u8ba1\u6a21\u5f0f\uff0c\u901a\u8fc7\u5c06\u7edf\u8ba1\u7ea6\u675f\u5c01\u88c5\u5728LLM\u5f15\u7528\u7c7b\u578b\u4e2d\uff0c\u5e76\u7ed3\u5408Python\u88c5\u9970\u5668\u4e0e\u65e0\u9650\u5e8f\u5217\u8fed\u4ee3\u5668\u5b9e\u73b0\uff0c\u4f7f\u7528\u6237\u80fd\u6309\u7b97\u6cd5\u9700\u6c42\u7ba1\u7406\u4e0e\u8f6c\u6362\u8fd9\u4e9b\u7c7b\u578b\u3002", "result": "\u5728SpecFix\u81ea\u52a8\u7a0b\u5e8f\u89c4\u8303\u4fee\u590d\u7cfb\u7edf\u4e0a\u7684\u6848\u4f8b\u7814\u7a76\u8868\u660e\uff0cMnimi\u5728\u4fdd\u6301\u7edf\u8ba1\u6b63\u786e\u6027\u7684\u540c\u65f6\uff0c\u663e\u8457\u63d0\u5347\u4e86\u53ef\u590d\u73b0\u6027\u3001\u8c03\u8bd5\u4fbf\u6377\u6027\u4ee5\u53ca\u65f6\u95f4\u548c\u6210\u672c\u6548\u7387\u3002", "conclusion": "Mnimi\u6709\u6548\u89e3\u51b3\u4e86LLM\u7f13\u5b58\u4e2d\u7edf\u8ba1\u72ec\u7acb\u6027\u7f3a\u5931\u7684\u95ee\u9898\uff0c\u4e3a\u6a21\u5757\u5316LLM\u5de5\u4f5c\u6d41\u63d0\u4f9b\u4e86\u4e00\u79cd\u517c\u987e\u6548\u7387\u4e0e\u7edf\u8ba1\u5b8c\u6574\u6027\u7684\u5b9e\u7528\u65b9\u6848\u3002"}}
{"id": "2511.22186", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2511.22186", "abs": "https://arxiv.org/abs/2511.22186", "authors": ["Chayanid Termphaiboon", "Raula Gaikovina Kula", "Youmei Fan", "Morakot Choetkiertikul", "Chaiyong Ragkhitwetsagul", "Thanwadee Sunetnanta", "Kenichi Matsumoto"], "title": "Exploring the SECURITY.md in the Dependency Chain: Preliminary Analysis of the PyPI Ecosystem", "comment": "8 pages, 5 figures, accepted to ISE 2025 (International Workshop on Intelligent Software Engineering)", "summary": "Security policies, such as SECURITY.md files, are now common in open-source projects. They help guide responsible vulnerability reporting and build trust among users and contributors. Despite their growing use, it is still unclear how these policies influence the structure and evolution of software dependencies. Software dependencies are external packages or libraries that a project relies on, and their interconnected nature affects both functionality and security. This study explores the relationship between security policies and dependency management in PyPI projects. We analyzed projects with and without a SECURITY.md file by examining their dependency trees and tracking how dependencies change over time. The analysis shows that projects with a security policy tend to rely on a broader set of direct dependencies, while overall depth and transitive dependencies remain similar. Historically, projects created after the introduction of SECURITY.md, particularly later adopters, show more frequent dependency updates. These results suggest that security policies are linked to more modular and feature-rich projects, and highlight the role of SECURITY.md in promoting proactive dependency management and reducing risks in the software supply chain.", "AI": {"tldr": "\u8be5\u7814\u7a76\u53d1\u73b0\uff0c\u62e5\u6709 SECURITY.md \u5b89\u5168\u7b56\u7565\u7684 PyPI \u9879\u76ee\u503e\u5411\u4e8e\u4f7f\u7528\u66f4\u591a\u76f4\u63a5\u4f9d\u8d56\u9879\uff0c\u5e76\u66f4\u9891\u7e41\u5730\u66f4\u65b0\u4f9d\u8d56\uff0c\u8868\u660e\u5b89\u5168\u7b56\u7565\u4e0e\u66f4\u4e3b\u52a8\u7684\u4f9d\u8d56\u7ba1\u7406\u76f8\u5173\u3002", "motivation": "\u5c3d\u7ba1\u5f00\u6e90\u9879\u76ee\u4e2d SECURITY.md \u7b49\u5b89\u5168\u7b56\u7565\u65e5\u76ca\u666e\u53ca\uff0c\u4f46\u5176\u5bf9\u8f6f\u4ef6\u4f9d\u8d56\u7ed3\u6784\u548c\u6f14\u5316\u7684\u5f71\u54cd\u5c1a\u4e0d\u660e\u786e\u3002\u672c\u7814\u7a76\u65e8\u5728\u63ed\u793a\u5b89\u5168\u7b56\u7565\u4e0e\u4f9d\u8d56\u7ba1\u7406\u4e4b\u95f4\u7684\u5173\u7cfb\u3002", "method": "\u5206\u6790\u5305\u542b\u4e0e\u4e0d\u5305\u542b SECURITY.md \u6587\u4ef6\u7684 PyPI \u9879\u76ee\u7684\u4f9d\u8d56\u6811\uff0c\u5e76\u8ffd\u8e2a\u5176\u968f\u65f6\u95f4\u53d8\u5316\u7684\u4f9d\u8d56\u66f4\u65b0\u884c\u4e3a\u3002", "result": "\u6709\u5b89\u5168\u7b56\u7565\u7684\u9879\u76ee\u4f7f\u7528\u66f4\u5e7f\u6cdb\u7684\u76f4\u63a5\u4f9d\u8d56\uff0c\u4f46\u6574\u4f53\u4f9d\u8d56\u6df1\u5ea6\u548c\u4f20\u9012\u4f9d\u8d56\u76f8\u4f3c\uff1b\u540e\u671f\u91c7\u7528 SECURITY.md \u7684\u9879\u76ee\u8868\u73b0\u51fa\u66f4\u9891\u7e41\u7684\u4f9d\u8d56\u66f4\u65b0\u3002", "conclusion": "SECURITY.md \u4e0e\u66f4\u6a21\u5757\u5316\u3001\u529f\u80fd\u4e30\u5bcc\u7684\u9879\u76ee\u76f8\u5173\uff0c\u5e76\u5728\u63a8\u52a8\u4e3b\u52a8\u4f9d\u8d56\u7ba1\u7406\u3001\u964d\u4f4e\u8f6f\u4ef6\u4f9b\u5e94\u94fe\u98ce\u9669\u65b9\u9762\u53d1\u6325\u79ef\u6781\u4f5c\u7528\u3002"}}
{"id": "2511.22481", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2511.22481", "abs": "https://arxiv.org/abs/2511.22481", "authors": ["Jun Wang", "Yunxiang Yao", "Wenwei Kuang", "Runze Mao", "Zhenhao Sun", "Zhuang Tao", "Ziyang Zhang", "Dengyu Li", "Jiajun Chen", "Zhili Wang", "Kai Cui", "Congzhi Cai", "Longwen Lan", "Ken Zhang"], "title": "OmniInfer: System-Wide Acceleration Techniques for Optimizing LLM Serving Throughput and Latency", "comment": "Project page: [this https URL](https://gitee.com/omniai/omniinfer)", "summary": "Large Language Models drive a wide range of modern AI applications but impose substantial challenges on large-scale serving systems due to intensive computation, strict latency constraints, and throughput bottlenecks. We introduce OmniInfer, a unified system-level acceleration framework designed to maximize end-to-end serving efficiency through fine-grained optimization of expert placement, cache compression, and scheduling. OmniInfer integrates three complementary components: OmniPlacement for load-aware Mixture-of-Experts scheduling, OmniAttn for sparse attention acceleration, and OmniProxy for disaggregation-aware request scheduling. Built atop vLLM, OmniInfer delivers system-wide performance gains through adaptive resource disaggregation, efficient sparsity exploitation, and global coordination across prefill and decode phases. Evaluated on DeepSeek-R1 within a 10-node Ascend 910C cluster, OmniInfer achieves 616 QPM, where the unified framework reduces TPOT by 36\\%, and the superimposition of OmniProxy further slashes TTFT by 38\\%. The project is open-sourced at [this https URL](https://gitee.com/omniai/omniinfer).", "AI": {"tldr": "OmniInfer \u662f\u4e00\u4e2a\u9762\u5411\u5927\u8bed\u8a00\u6a21\u578b\u63a8\u7406\u7684\u7edf\u4e00\u7cfb\u7edf\u7ea7\u52a0\u901f\u6846\u67b6\uff0c\u901a\u8fc7\u4e13\u5bb6\u653e\u7f6e\u3001\u7f13\u5b58\u538b\u7f29\u548c\u8c03\u5ea6\u7684\u7ec6\u7c92\u5ea6\u4f18\u5316\uff0c\u663e\u8457\u63d0\u5347\u7aef\u5230\u7aef\u63a8\u7406\u6548\u7387\u3002", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b\u5728\u5b9e\u9645\u90e8\u7f72\u4e2d\u9762\u4e34\u8ba1\u7b97\u5bc6\u96c6\u3001\u5ef6\u8fdf\u654f\u611f\u548c\u541e\u5410\u74f6\u9888\u7b49\u6311\u6218\uff0c\u4e9f\u9700\u9ad8\u6548\u7684\u7cfb\u7edf\u7ea7\u4f18\u5316\u65b9\u6848\u4ee5\u63d0\u5347\u63a8\u7406\u670d\u52a1\u6027\u80fd\u3002", "method": "OmniInfer \u6784\u5efa\u4e8e vLLM \u4e4b\u4e0a\uff0c\u6574\u5408\u4e09\u4e2a\u6838\u5fc3\u7ec4\u4ef6\uff1aOmniPlacement\uff08\u8d1f\u8f7d\u611f\u77e5\u7684 MoE \u8c03\u5ea6\uff09\u3001OmniAttn\uff08\u7a00\u758f\u6ce8\u610f\u529b\u52a0\u901f\uff09\u548c OmniProxy\uff08\u9762\u5411\u8d44\u6e90\u89e3\u8026\u7684\u8bf7\u6c42\u8c03\u5ea6\uff09\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u8d44\u6e90\u89e3\u8026\u3001\u9ad8\u6548\u5229\u7528\u7a00\u758f\u6027\u548c\u5168\u5c40\u534f\u8c03\u9884\u586b\u5145\u4e0e\u89e3\u7801\u9636\u6bb5\u6765\u4f18\u5316\u6574\u4f53\u6027\u80fd\u3002", "result": "\u5728 10 \u8282\u70b9 Ascend 910C \u96c6\u7fa4\u4e0a\u4f7f\u7528 DeepSeek-R1 \u6a21\u578b\u8bc4\u4f30\uff0cOmniInfer \u8fbe\u5230 616 QPM\uff0c\u6574\u4f53\u6846\u67b6\u964d\u4f4e TPOT 36%\uff0c\u53e0\u52a0 OmniProxy \u540e\u8fdb\u4e00\u6b65\u51cf\u5c11 TTFT 38%\u3002", "conclusion": "OmniInfer \u6709\u6548\u63d0\u5347\u4e86\u5927\u8bed\u8a00\u6a21\u578b\u63a8\u7406\u7cfb\u7edf\u7684\u7aef\u5230\u7aef\u6548\u7387\uff0c\u5c55\u793a\u4e86\u7cfb\u7edf\u7ea7\u534f\u540c\u4f18\u5316\u5728\u5b9e\u9645\u90e8\u7f72\u4e2d\u7684\u5de8\u5927\u6f5c\u529b\uff0c\u5e76\u5df2\u5f00\u6e90\u3002"}}
{"id": "2511.22409", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2511.22409", "abs": "https://arxiv.org/abs/2511.22409", "authors": ["Polydoros Giannouris", "Sophia Ananiadou"], "title": "NOMAD: A Multi-Agent LLM System for UML Class Diagram Generation from Natural Language Requirements", "comment": null, "summary": "Large Language Models (LLMs) are increasingly utilised in software engineering, yet their ability to generate structured artefacts such as UML diagrams remains underexplored. In this work we present NOMAD, a cognitively inspired, modular multi-agent framework that decomposes UML generation into a series of role-specialised subtasks. Each agent handles a distinct modelling activity, such as entity extraction, relationship classification, and diagram synthesis, mirroring the goal-directed reasoning processes of an engineer. This decomposition improves interpretability and allows for targeted verification strategies. We evaluate NOMAD through a mixed design: a large case study (Northwind) for in-depth probing and error analysis, and human-authored UML exercises for breadth and realism. NOMAD outperforms all selected baselines, while revealing persistent challenges in fine-grained attribute extraction. Building on these observations, we introduce the first systematic taxonomy of errors in LLM-generated UML diagrams, categorising structural, relationship, and semantic/logical. Finally, we examine verification as a design probe, showing its mixed effects and outlining adaptive strategies as promising directions. Together, these contributions position NOMAD as both an effective framework for UML class diagram generation and a lens onto the broader research challenges of reliable language-to-model workflows.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86NOMAD\uff0c\u4e00\u4e2a\u53d7\u8ba4\u77e5\u542f\u53d1\u7684\u591a\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u7528\u4e8e\u751f\u6210UML\u7c7b\u56fe\u3002\u8be5\u6846\u67b6\u5c06\u5efa\u6a21\u4efb\u52a1\u5206\u89e3\u4e3a\u591a\u4e2a\u89d2\u8272\u4e13\u7528\u5b50\u4efb\u52a1\uff0c\u63d0\u5347\u4e86\u53ef\u89e3\u91ca\u6027\u4e0e\u9a8c\u8bc1\u80fd\u529b\uff0c\u5728\u5b9e\u9a8c\u4e2d\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\uff0c\u5e76\u9996\u6b21\u7cfb\u7edf\u5730\u5bf9LLM\u751f\u6210UML\u4e2d\u7684\u9519\u8bef\u8fdb\u884c\u4e86\u5206\u7c7b\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u8f6f\u4ef6\u5de5\u7a0b\u4e2d\u7684\u5e94\u7528\u65e5\u76ca\u5e7f\u6cdb\uff0c\u4f46\u5176\u751f\u6210\u5982UML\u56fe\u7b49\u7ed3\u6784\u5316\u4ea7\u7269\u7684\u80fd\u529b\u5c1a\u672a\u5f97\u5230\u5145\u5206\u63a2\u7d22\u3002\u73b0\u6709\u65b9\u6cd5\u7f3a\u4e4f\u5bf9\u5efa\u6a21\u8fc7\u7a0b\u7684\u5206\u89e3\u3001\u53ef\u89e3\u91ca\u6027\u548c\u6709\u6548\u9a8c\u8bc1\u673a\u5236\u3002", "method": "\u63d0\u51faNOMAD\u6846\u67b6\uff0c\u91c7\u7528\u6a21\u5757\u5316\u591a\u667a\u80fd\u4f53\u67b6\u6784\uff0c\u5c06UML\u751f\u6210\u4efb\u52a1\u5206\u89e3\u4e3a\u5b9e\u4f53\u62bd\u53d6\u3001\u5173\u7cfb\u5206\u7c7b\u548c\u56fe\u8868\u5408\u6210\u7b49\u5b50\u4efb\u52a1\uff0c\u6bcf\u4e2a\u667a\u80fd\u4f53\u4e13\u7cbe\u4e8e\u7279\u5b9a\u5efa\u6a21\u6d3b\u52a8\uff1b\u901a\u8fc7\u6df7\u5408\u8bc4\u4f30\u8bbe\u8ba1\uff08\u5305\u62ecNorthwind\u6848\u4f8b\u7814\u7a76\u548c\u4eba\u5de5\u7f16\u5199\u7684UML\u7ec3\u4e60\uff09\u8fdb\u884c\u9a8c\u8bc1\uff0c\u5e76\u6784\u5efa\u4e86LLM\u751f\u6210UML\u9519\u8bef\u7684\u7cfb\u7edf\u5206\u7c7b\u6cd5\u3002", "result": "NOMAD\u5728\u5b9e\u9a8c\u4e2d\u4f18\u4e8e\u6240\u6709\u9009\u5b9a\u57fa\u7ebf\uff0c\u63ed\u793a\u4e86\u7ec6\u7c92\u5ea6\u5c5e\u6027\u63d0\u53d6\u65b9\u9762\u7684\u6301\u7eed\u6311\u6218\uff0c\u5e76\u9996\u6b21\u63d0\u51fa\u4e86LLM\u751f\u6210UML\u56fe\u7684\u9519\u8bef\u5206\u7c7b\u4f53\u7cfb\uff08\u7ed3\u6784\u3001\u5173\u7cfb\u3001\u8bed\u4e49/\u903b\u8f91\u4e09\u7c7b\uff09\uff1b\u540c\u65f6\u63a2\u8ba8\u4e86\u9a8c\u8bc1\u673a\u5236\u7684\u8bbe\u8ba1\u4f5c\u7528\u53ca\u5176\u9002\u5e94\u6027\u7b56\u7565\u3002", "conclusion": "NOMAD\u4e0d\u4ec5\u662f\u4e00\u4e2a\u9ad8\u6548\u7684UML\u7c7b\u56fe\u751f\u6210\u6846\u67b6\uff0c\u4e5f\u4e3a\u8bed\u8a00\u5230\u6a21\u578b\u5de5\u4f5c\u6d41\u7684\u53ef\u9760\u6027\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u7684\u89c6\u89d2\u548c\u57fa\u7840\u3002"}}
{"id": "2511.22779", "categories": ["cs.DC", "physics.med-ph", "physics.optics"], "pdf": "https://arxiv.org/pdf/2511.22779", "abs": "https://arxiv.org/abs/2511.22779", "authors": ["Shijie Yan", "Douglas Dwyer", "David R. Kaeli", "Qianqian Fang"], "title": "Accelerating mesh-based Monte Carlo simulations using contemporary graphics ray-tracing hardware", "comment": null, "summary": "Significance: Monte Carlo (MC) methods are the gold-standard for modeling light-tissue interactions due to their accuracy. Mesh-based MC (MMC) offers enhanced precision for complex tissue structures using tetrahedral mesh models. Despite significant speedups achieved on graphics processing units (GPUs), MMC performance remains hindered by the computational cost of frequent ray-boundary intersection tests.\n  Aim: We propose a highly accelerated MMC algorithm, RT-MMC, that leverages the hardware-accelerated ray traversal and intersection capabilities of ray-tracing cores (RT-cores) on modern GPUs.\n  Approach: Implemented using NVIDIA's OptiX platform, RT-MMC extends graphics ray-tracing pipelines towards volumetric ray-tracing in turbid media, eliminating the need for challenging tetrahedral mesh generation while delivering significant speed improvements through hardware acceleration. It also intrinsically supports wide-field sources without complex mesh retesselation.\n  Results: RT-MMC demonstrates excellent agreement with traditional software-ray-tracing MMC algorithms while achieving 1.5x to 4.5x speedups across multiple GPU architectures. These performance gains significantly enhance the practicality of MMC for routine simulations.\n  Conclusion: Migration from software- to hardware-based ray-tracing not only greatly simplifies MMC simulation workflows, but also results in significant speedups that are expected to increase further as ray-tracing hardware rapidly gains adoption. Adoption of graphics ray-tracing pipelines in quantitative MMC simulations enables leveraging of emerging hardware resources and benefits a wide range of biophotonics applications.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eGPU\u5149\u7ebf\u8ffd\u8e2a\u6838\u5fc3\uff08RT-core\uff09\u7684\u52a0\u901f\u7248\u7f51\u683c\u8499\u7279\u5361\u6d1b\u7b97\u6cd5RT-MMC\uff0c\u901a\u8fc7\u786c\u4ef6\u52a0\u901f\u663e\u8457\u63d0\u5347\u6a21\u62df\u5149\u5728\u590d\u6742\u7ec4\u7ec7\u4e2d\u4f20\u64ad\u7684\u901f\u5ea6\uff0c\u540c\u65f6\u7b80\u5316\u5de5\u4f5c\u6d41\u7a0b\u5e76\u4fdd\u6301\u9ad8\u7cbe\u5ea6\u3002", "motivation": "\u4f20\u7edf\u7f51\u683c\u8499\u7279\u5361\u6d1b\uff08MMC\uff09\u65b9\u6cd5\u867d\u7cbe\u5ea6\u9ad8\uff0c\u4f46\u53d7\u9650\u4e8e\u9891\u7e41\u7684\u5149\u7ebf-\u8fb9\u754c\u76f8\u4ea4\u8ba1\u7b97\uff0c\u5373\u4f7f\u5728GPU\u4e0a\u8fd0\u884c\u4ecd\u6548\u7387\u4e0d\u8db3\uff0c\u4e9f\u9700\u66f4\u9ad8\u6548\u7684\u52a0\u901f\u7b56\u7565\u3002", "method": "\u5229\u7528NVIDIA OptiX\u5e73\u53f0\uff0c\u5c06\u56fe\u5f62\u5b66\u4e2d\u7684\u5149\u7ebf\u8ffd\u8e2a\u7ba1\u7ebf\u6269\u5c55\u81f3\u6d51\u6d4a\u4ecb\u8d28\u4e2d\u7684\u4f53\u5149\u7ebf\u8ffd\u8e2a\uff0c\u501f\u52a9\u73b0\u4ee3GPU\u7684RT-core\u5b9e\u73b0\u786c\u4ef6\u52a0\u901f\uff0c\u907f\u514d\u590d\u6742\u7684\u56db\u9762\u4f53\u7f51\u683c\u751f\u6210\uff0c\u5e76\u5929\u7136\u652f\u6301\u5bbd\u573a\u5149\u6e90\u3002", "result": "RT-MMC\u4e0e\u4f20\u7edf\u8f6f\u4ef6\u5149\u7ebf\u8ffd\u8e2aMMC\u7ed3\u679c\u9ad8\u5ea6\u4e00\u81f4\uff0c\u5728\u591a\u79cdGPU\u67b6\u6784\u4e0a\u5b9e\u73b01.5\u500d\u81f34.5\u500d\u7684\u901f\u5ea6\u63d0\u5347\uff0c\u663e\u8457\u589e\u5f3aMMC\u5728\u5e38\u89c4\u6a21\u62df\u4e2d\u7684\u5b9e\u7528\u6027\u3002", "conclusion": "\u4ece\u8f6f\u4ef6\u8f6c\u5411\u786c\u4ef6\u5149\u7ebf\u8ffd\u8e2a\u4e0d\u4ec5\u5927\u5e45\u7b80\u5316MMC\u6a21\u62df\u6d41\u7a0b\uff0c\u8fd8\u5e26\u6765\u663e\u8457\u52a0\u901f\u6548\u679c\uff0c\u968f\u7740\u5149\u7ebf\u8ffd\u8e2a\u786c\u4ef6\u666e\u53ca\uff0c\u8be5\u65b9\u6cd5\u5c06\u63a8\u52a8\u751f\u7269\u5149\u5b50\u5b66\u9886\u57df\u5e7f\u6cdb\u5e94\u7528\u3002"}}
{"id": "2511.22513", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2511.22513", "abs": "https://arxiv.org/abs/2511.22513", "authors": ["J\u00e9r\u00f4me Pfeiffer", "Nicolai Maisch", "Sebastian Friedl", "Matthias Milan Strljic", "Armin Lechler", "Oliver Riedel", "Andreas Wortmann"], "title": "Declarative Policy Control for Data Spaces: A DSL-Based Approach for Manufacturing-X", "comment": null, "summary": "The growing adoption of federated data spaces, such as in the GAIA-X and the International Data Spaces (IDS) initiative, promises secure and sovereign data sharing across organizational boundaries in Industry 4.0. In manufacturing ecosystems, this enables use cases, such as cross-factory process optimization, predictive maintenance, and supplier integration. Frameworks and standards, such as the Asset Administration Shell (AAS), Eclipse Dataspace Connector (EDC), ID-Link and Open Platform Communications Unified Architecture (OPC UA) provide a strong foundation to realize this ecosystem. However, a major open challenge is the practical description and enforcement of context-dependent data usage policies using these base technologies - especially by domain experts without software engineering backgrounds. Therefore, this article proposes a method for leveraging domain-specific languages (DSLs) to enable declarative, human-readable, and machine-executable policy definitions for sovereign data sharing via data space connectors. The DSL empowers domain experts to specify fine-grained data governance requirements - such as restricting access to data from specific production batches or enforcing automatic deletion after a defined retention period - without writing imperative code.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u9886\u57df\u7279\u5b9a\u8bed\u8a00\uff08DSL\uff09\u7684\u65b9\u6cd5\uff0c\u4f7f\u975e\u8f6f\u4ef6\u5de5\u7a0b\u80cc\u666f\u7684\u9886\u57df\u4e13\u5bb6\u80fd\u591f\u4ee5\u58f0\u660e\u5f0f\u3001\u53ef\u8bfb\u4e14\u53ef\u673a\u5668\u6267\u884c\u7684\u65b9\u5f0f\u5b9a\u4e49\u4e0a\u4e0b\u6587\u76f8\u5173\u7684\u6570\u636e\u4f7f\u7528\u7b56\u7565\uff0c\u7528\u4e8e\u4e3b\u6743\u6570\u636e\u5171\u4eab\u3002", "motivation": "\u5f53\u524d\u5728\u5de5\u4e1a4.0\u4e2d\u5e7f\u6cdb\u91c7\u7528\u7684\u8054\u90a6\u6570\u636e\u7a7a\u95f4\uff08\u5982GAIA-X\u548cIDS\uff09\u7f3a\u4e4f\u5bf9\u4e0a\u4e0b\u6587\u76f8\u5173\u6570\u636e\u4f7f\u7528\u7b56\u7565\u7684\u5b9e\u7528\u63cf\u8ff0\u4e0e\u6267\u884c\u673a\u5236\uff0c\u5c24\u5176\u96be\u4ee5\u88ab\u975e\u8f6f\u4ef6\u5de5\u7a0b\u80cc\u666f\u7684\u9886\u57df\u4e13\u5bb6\u4f7f\u7528\u3002", "method": "\u5229\u7528\u9886\u57df\u7279\u5b9a\u8bed\u8a00\uff08DSL\uff09\u6784\u5efa\u4e00\u79cd\u58f0\u660e\u5f0f\u3001\u4eba\u7c7b\u53ef\u8bfb\u4e14\u673a\u5668\u53ef\u6267\u884c\u7684\u7b56\u7565\u5b9a\u4e49\u65b9\u6cd5\uff0c\u901a\u8fc7\u6570\u636e\u7a7a\u95f4\u8fde\u63a5\u5668\u5b9e\u73b0\u7ec6\u7c92\u5ea6\u7684\u6570\u636e\u6cbb\u7406\u89c4\u5219\u8bbe\u5b9a\u3002", "result": "\u8be5\u65b9\u6cd5\u5141\u8bb8\u9886\u57df\u4e13\u5bb6\u65e0\u9700\u7f16\u5199\u547d\u4ee4\u5f0f\u4ee3\u7801\u5373\u53ef\u6307\u5b9a\u5982\u9650\u5236\u7279\u5b9a\u751f\u4ea7\u6279\u6b21\u6570\u636e\u8bbf\u95ee\u6216\u8bbe\u7f6e\u81ea\u52a8\u5220\u9664\u4fdd\u7559\u671f\u7b49\u7cbe\u7ec6\u6570\u636e\u6cbb\u7406\u8981\u6c42\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684DSL\u65b9\u6cd5\u6709\u6548\u5f25\u5408\u4e86\u73b0\u6709\u6570\u636e\u7a7a\u95f4\u6280\u672f\u4e0e\u5b9e\u9645\u4e1a\u52a1\u9700\u6c42\u4e4b\u95f4\u7684\u9e3f\u6c9f\uff0c\u63d0\u5347\u4e86\u4e3b\u6743\u6570\u636e\u5171\u4eab\u7684\u53ef\u7528\u6027\u4e0e\u53ef\u63a7\u6027\u3002"}}
{"id": "2511.22880", "categories": ["cs.DC", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.22880", "abs": "https://arxiv.org/abs/2511.22880", "authors": ["Shashwat Jaiswal", "Shrikara Arun", "Anjaly Parayil", "Ankur Mallick", "Spyros Mastorakis", "Alind Khare", "Chloi Alverti", "Renee St Amant", "Chetan Bansal", "Victor R\u00fchle", "Josep Torrellas"], "title": "Serving Heterogeneous LoRA Adapters in Distributed LLM Inference Systems", "comment": null, "summary": "Low-Rank Adaptation (LoRA) has become the de facto method for parameter-efficient fine-tuning of large language models (LLMs), enabling rapid adaptation to diverse domains. In production, LoRA-based models are served at scale, creating multi-tenant environments with hundreds of adapters sharing a base model. However, state-of-the-art serving systems co-batch heterogeneous adapters without accounting for rank (size) variability, leading to severe performance skew, which ultimately requires adding more GPUs to satisfy service-level objectives (SLOs). Existing optimizations, focused on loading, caching, and kernel execution, ignore this heterogeneity, leaving GPU resources underutilized. We present LoRAServe, a workload-aware dynamic adapter placement and routing framework designed to tame rank diversity in LoRA serving. By dynamically rebalancing adapters across GPUs and leveraging GPU Direct RDMA for remote access, LoRAServe maximizes throughput and minimizes tail latency under real-world workload drift. Evaluations on production traces from Company X show that LoRAServe elicits up to 2$\\times$ higher throughput, up to 9$\\times$ lower TTFT, while using up to 50% fewer GPUs under SLO constraints compared to state-of-the-art systems.", "AI": {"tldr": "LoRAServe \u662f\u4e00\u79cd\u9762\u5411 LoRA \u670d\u52a1\u7684\u52a8\u6001\u9002\u914d\u5668\u653e\u7f6e\u4e0e\u8def\u7531\u6846\u67b6\uff0c\u901a\u8fc7\u611f\u77e5\u5de5\u4f5c\u8d1f\u8f7d\u5e76\u5229\u7528 GPU Direct RDMA \u6280\u672f\uff0c\u5728\u771f\u5b9e\u751f\u4ea7\u73af\u5883\u4e2d\u663e\u8457\u63d0\u5347\u541e\u5410\u91cf\u3001\u964d\u4f4e\u9996 token \u5ef6\u8fdf\uff0c\u5e76\u51cf\u5c11\u6240\u9700 GPU \u6570\u91cf\u3002", "motivation": "\u73b0\u6709 LoRA \u670d\u52a1\u7cfb\u7edf\u5728\u591a\u79df\u6237\u73af\u5883\u4e0b\u5bf9\u4e0d\u540c\u79e9\uff08rank\uff09\u7684\u9002\u914d\u5668\u8fdb\u884c\u5171\u6279\u5904\u7406\u65f6\u672a\u8003\u8651\u5176\u5927\u5c0f\u5dee\u5f02\uff0c\u5bfc\u81f4\u6027\u80fd\u4e25\u91cd\u4e0d\u5747\u8861\uff0c\u8fdb\u800c\u9700\u589e\u52a0 GPU \u8d44\u6e90\u4ee5\u6ee1\u8db3\u670d\u52a1\u7b49\u7ea7\u76ee\u6807\uff08SLO\uff09\uff0c\u800c\u5f53\u524d\u4f18\u5316\u624b\u6bb5\u5ffd\u7565\u4e86\u8fd9\u79cd\u5f02\u6784\u6027\uff0c\u9020\u6210 GPU \u8d44\u6e90\u5229\u7528\u7387\u4f4e\u4e0b\u3002", "method": "\u63d0\u51fa LoRAServe \u6846\u67b6\uff0c\u901a\u8fc7\u52a8\u6001\u5730\u5728 GPU \u95f4\u91cd\u65b0\u5e73\u8861\u9002\u914d\u5668\u5206\u5e03\uff0c\u5e76\u7ed3\u5408 GPU Direct RDMA \u5b9e\u73b0\u8fdc\u7a0b\u8bbf\u95ee\uff0c\u4ece\u800c\u5728\u9762\u5bf9\u771f\u5b9e\u5de5\u4f5c\u8d1f\u8f7d\u53d8\u5316\u65f6\u6700\u5927\u5316\u541e\u5410\u91cf\u5e76\u6700\u5c0f\u5316\u5c3e\u90e8\u5ef6\u8fdf\u3002", "result": "\u5728 Company X \u7684\u751f\u4ea7\u8f68\u8ff9\u8bc4\u4f30\u4e2d\uff0cLoRAServe \u76f8\u6bd4\u73b0\u6709\u6700\u5148\u8fdb\u7cfb\u7edf\uff0c\u5728\u6ee1\u8db3 SLO \u7684\u524d\u63d0\u4e0b\uff0c\u541e\u5410\u91cf\u6700\u9ad8\u63d0\u5347 2 \u500d\uff0c\u9996 token \u5ef6\u8fdf\uff08TTFT\uff09\u6700\u591a\u964d\u4f4e 9 \u500d\uff0c\u5e76\u6700\u591a\u8282\u7701 50% \u7684 GPU \u8d44\u6e90\u3002", "conclusion": "LoRAServe \u6709\u6548\u89e3\u51b3\u4e86 LoRA \u591a\u79df\u6237\u670d\u52a1\u4e2d\u56e0\u9002\u914d\u5668\u79e9\u5f02\u6784\u5e26\u6765\u7684\u8d44\u6e90\u5229\u7528\u4e0e\u6027\u80fd\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u7cfb\u7edf\u6548\u7387\u548c\u53ef\u6269\u5c55\u6027\u3002"}}
{"id": "2511.22726", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2511.22726", "abs": "https://arxiv.org/abs/2511.22726", "authors": ["Ethan Friesen", "Sasha Morton-Salmon", "Md Nahidul Islam Opu", "Shahidul Islam", "Shaiful Chowdhury"], "title": "The Repeat Offenders: Characterizing and Predicting Extremely Bug-Prone Source Methods", "comment": null, "summary": "Identifying the small subset of source code that repeatedly attracts bugs is critical for reducing long-term maintenance effort. We define ExtremelyBuggy methods as those involved in more than one bug fix and present the first large-scale study of their prevalence, characteristics, and predictability. Using a dataset of over 1.25 million methods from 98 open-source Java projects, we find that ExtremelyBuggy methods constitute only a tiny fraction of all methods, yet frequently account for a disproportionately large share of bugs. At their inception, these methods are significantly larger, more complex, less readable, and less maintainable than both singly-buggy and non-buggy methods. However, despite these measurable differences, a comprehensive evaluation of five machine learning models shows that early prediction of ExtremelyBuggy methods remains highly unreliable due to data imbalance, project heterogeneity, and the fact that many bugs emerge through subsequent evolution rather than initial implementation. To complement these quantitative findings, we conduct a thematic analysis of 265 ExtremelyBuggy methods, revealing recurring visual issues (e.g., confusing control flow, poor readability), contextual roles (e.g., core logic, data transformation, external resource handling), and common defect patterns (e.g., faulty conditionals, fragile error handling, misuse of variables). These results highlight the need for richer, evolution-aware representations of code and provide actionable insights for practitioners seeking to prioritize high-risk methods early in the development lifecycle.", "AI": {"tldr": "\u8be5\u8bba\u6587\u5bf998\u4e2a\u5f00\u6e90Java\u9879\u76ee\u4e2d\u8d85\u8fc7125\u4e07\u65b9\u6cd5\u8fdb\u884c\u4e86\u5927\u89c4\u6a21\u7814\u7a76\uff0c\u53d1\u73b0\u201c\u6781\u7f3a\u9677\u65b9\u6cd5\u201d\uff08\u5373\u591a\u6b21\u5f15\u53d1bug\u4fee\u590d\u7684\u65b9\u6cd5\uff09\u867d\u5360\u6bd4\u6781\u5c0f\uff0c\u5374\u5bfc\u81f4\u4e86\u5927\u91cfbug\uff1b\u8fd9\u4e9b\u65b9\u6cd5\u5728\u521d\u59cb\u9636\u6bb5\u5c31\u8868\u73b0\u51fa\u66f4\u5927\u3001\u66f4\u590d\u6742\u3001\u53ef\u8bfb\u6027\u548c\u53ef\u7ef4\u62a4\u6027\u66f4\u5dee\u7b49\u7279\u5f81\uff0c\u4f46\u56e0\u6570\u636e\u4e0d\u5e73\u8861\u3001\u9879\u76ee\u5f02\u8d28\u6027\u53ca\u7f3a\u9677\u968f\u6f14\u5316\u4ea7\u751f\u7b49\u539f\u56e0\uff0c\u65e9\u671f\u9884\u6d4b\u4ecd\u4e0d\u53ef\u9760\uff1b\u901a\u8fc7\u4e3b\u9898\u5206\u6790\u63ed\u793a\u4e86\u5176\u5e38\u89c1\u89c6\u89c9\u95ee\u9898\u3001\u4e0a\u4e0b\u6587\u89d2\u8272\u548c\u7f3a\u9677\u6a21\u5f0f\uff0c\u4e3a\u9ad8\u98ce\u9669\u65b9\u6cd5\u7684\u65e9\u671f\u8bc6\u522b\u63d0\u4f9b\u4e86\u5b9e\u8df5\u6307\u5bfc\u3002", "motivation": "\u8bc6\u522b\u53cd\u590d\u5f15\u53d1\u7f3a\u9677\u7684\u5c11\u91cf\u6e90\u4ee3\u7801\u5b50\u96c6\u5bf9\u4e8e\u964d\u4f4e\u957f\u671f\u7ef4\u62a4\u6210\u672c\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u76ee\u524d\u7f3a\u4e4f\u5bf9\u6b64\u7c7b\u201c\u6781\u7f3a\u9677\u65b9\u6cd5\u201d\u7684\u7cfb\u7edf\u6027\u7814\u7a76\u3002", "method": "\u7ed3\u5408\u5927\u89c4\u6a21\u5b9a\u91cf\u5206\u6790\uff08\u57fa\u4e8e1.25M+ Java\u65b9\u6cd5\uff09\u4e0e\u5b9a\u6027\u4e3b\u9898\u5206\u6790\uff08265\u4e2a\u6781\u7f3a\u9677\u65b9\u6cd5\uff09\uff0c\u8bc4\u4f30\u4e94\u79cd\u673a\u5668\u5b66\u4e60\u6a21\u578b\u7684\u9884\u6d4b\u80fd\u529b\uff0c\u5e76\u5f52\u7eb3\u5176\u7ed3\u6784\u6027\u3001\u4e0a\u4e0b\u6587\u548c\u7f3a\u9677\u6a21\u5f0f\u7279\u5f81\u3002", "result": "\u6781\u7f3a\u9677\u65b9\u6cd5\u5360\u6bd4\u6781\u5c0f\u4f46\u8d21\u732e\u5927\u91cfbug\uff1b\u521d\u59cb\u9636\u6bb5\u5df2\u5177\u663e\u8457\u4e0d\u826f\u7279\u5f81\uff1b\u73b0\u6709ML\u6a21\u578b\u96be\u4ee5\u53ef\u9760\u9884\u6d4b\uff1b\u4e3b\u9898\u5206\u6790\u63ed\u793a\u4e86\u4e09\u7c7b\u5171\u6027\uff1a\u6df7\u4e71\u63a7\u5236\u6d41\u7b49\u89c6\u89c9\u95ee\u9898\u3001\u6838\u5fc3\u903b\u8f91\u7b49\u89d2\u8272\u3001\u6761\u4ef6\u9519\u8bef\u7b49\u7f3a\u9677\u6a21\u5f0f\u3002", "conclusion": "\u9700\u53d1\u5c55\u80fd\u6355\u6349\u4ee3\u7801\u6f14\u5316\u4fe1\u606f\u7684\u66f4\u4e30\u5bcc\u8868\u793a\u65b9\u6cd5\uff0c\u5f53\u524d\u9759\u6001\u7279\u5f81\u4e0d\u8db3\u4ee5\u652f\u6491\u53ef\u9760\u65e9\u671f\u9884\u6d4b\uff1b\u7814\u7a76\u7ed3\u679c\u53ef\u5e2e\u52a9\u5f00\u53d1\u8005\u4f18\u5148\u5ba1\u67e5\u9ad8\u98ce\u9669\u65b9\u6cd5\u3002"}}
{"id": "2511.22921", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2511.22921", "abs": "https://arxiv.org/abs/2511.22921", "authors": ["Hengyuan Liu", "Xia Song", "Yong Liu", "Zheng Li"], "title": "MBFL-DKMR: Improving Mutation-based Fault Localization through Denoising-based Kill Matrix Refinement", "comment": null, "summary": "Software debugging is a critical and time-consuming aspect of software development, with fault localization being a fundamental step that significantly impacts debugging efficiency. Mutation-Based Fault Localization (MBFL) has gained prominence due to its robust theoretical foundations and fine-grained analysis capabilities. However, recent studies have identified a critical challenge: noise phenomena, specifically the false kill relationships between mutants and tests, which significantly degrade localization effectiveness. While several approaches have been proposed to rectify the final localization results, they do not directly address the underlying noise. In this paper, we propose a novel approach to refine the kill matrix, a core data structure capturing mutant-test relationships in MBFL, by treating it as a signal that contains both meaningful fault-related patterns and high-frequency noise. Inspired by signal processing theory, we introduce DKMR (Denoising-based Kill Matrix Refinement), which employs two key stages: (1) signal enhancement through hybrid matrix construction to improve the signal-to-noise ratio for better denoising, and (2) signal denoising via frequency domain filtering to suppress noise while preserving fault-related patterns. Building on this foundation, we develop MBFL-DKMR, a fault localization framework that utilizes the refined matrix with fuzzy values for suspiciousness calculation. Our evaluation on Defects4J v2.0.0 demonstrates that MBFL-DKMR effectively mitigates the noise and outperforms the state-of-the-art MBFL techniques. Specifically, MBFL-DKMR achieves 129 faults localized at Top-1 compared to 85 for BLMu and 103 for Delta4Ms, with negligible additional computational overhead (0.11 seconds, 0.001\\% of total time). This work highlights the potential of signal processing techniques to enhance the effectiveness of MBFL by refining the kill matrix.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u4fe1\u53f7\u53bb\u566a\u601d\u60f3\u7684\u65b0\u65b9\u6cd5DKMR\uff0c\u7528\u4e8e\u4f18\u5316\u53d8\u5f02\u4f53-\u6d4b\u8bd5\u5173\u7cfb\u77e9\u9635\uff08kill matrix\uff09\uff0c\u4ece\u800c\u63d0\u5347\u57fa\u4e8e\u53d8\u5f02\u7684\u6545\u969c\u5b9a\u4f4d\uff08MBFL\uff09\u7684\u51c6\u786e\u6027\u3002\u8be5\u65b9\u6cd5\u901a\u8fc7\u6df7\u5408\u77e9\u9635\u6784\u5efa\u589e\u5f3a\u4fe1\u53f7\uff0c\u5e76\u5728\u9891\u57df\u4e2d\u6ee4\u9664\u9ad8\u9891\u566a\u58f0\uff0c\u5728Defects4J\u6570\u636e\u96c6\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u6280\u672f\u3002", "motivation": "\u73b0\u6709MBFL\u65b9\u6cd5\u53d7\u201c\u865a\u5047\u6740\u6b7b\u201d\uff08false kill\uff09\u566a\u58f0\u5f71\u54cd\uff0c\u5bfc\u81f4\u5b9a\u4f4d\u6548\u679c\u4e0b\u964d\uff1b\u5f53\u524d\u7814\u7a76\u591a\u805a\u7126\u4e8e\u4fee\u6b63\u6700\u7ec8\u7ed3\u679c\uff0c\u800c\u975e\u4ece\u6e90\u5934\u51c0\u5316\u6838\u5fc3\u6570\u636e\u7ed3\u6784\u2014\u2014kill matrix\u3002", "method": "\u5c06kill matrix\u89c6\u4e3a\u542b\u566a\u58f0\u4fe1\u53f7\uff0c\u63d0\u51faDKMR\u6846\u67b6\uff1a\u9996\u5148\u901a\u8fc7\u6df7\u5408\u77e9\u9635\u6784\u9020\u589e\u5f3a\u4fe1\u566a\u6bd4\uff0c\u518d\u5229\u7528\u9891\u57df\u6ee4\u6ce2\u8fdb\u884c\u53bb\u566a\uff1b\u5728\u6b64\u57fa\u7840\u4e0a\u6784\u5efaMBFL-DKMR\u6846\u67b6\uff0c\u4f7f\u7528\u53bb\u566a\u540e\u7684\u6a21\u7cca\u503c\u77e9\u9635\u8ba1\u7b97\u53ef\u7591\u5ea6\u3002", "result": "\u5728Defects4J v2.0.0\u4e0a\uff0cMBFL-DKMR\u5728Top-1\u5b9a\u4f4d129\u4e2a\u6545\u969c\uff0c\u4f18\u4e8eBLMu\uff0885\u4e2a\uff09\u548cDelta4Ms\uff08103\u4e2a\uff09\uff0c\u4e14\u989d\u5916\u5f00\u9500\u6781\u4f4e\uff08\u4ec50.11\u79d2\uff0c\u5360\u603b\u65f6\u95f40.001%\uff09\u3002", "conclusion": "\u5c06\u4fe1\u53f7\u5904\u7406\u4e2d\u7684\u53bb\u566a\u601d\u60f3\u5f15\u5165MBFL\u53ef\u6709\u6548\u63d0\u5347\u6545\u969c\u5b9a\u4f4d\u7cbe\u5ea6\uff0c\u9a8c\u8bc1\u4e86\u5bf9kill matrix\u8fdb\u884c\u6e90\u5934\u51c0\u5316\u7684\u6709\u6548\u6027\u4e0e\u53ef\u884c\u6027\u3002"}}
{"id": "2511.23050", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2511.23050", "abs": "https://arxiv.org/abs/2511.23050", "authors": ["Nikita Repnkiov", "Vladimir Faerman"], "title": "Software for Studying CASCADE Error Correction Protocols in Quantum Communications", "comment": "Reported in Omsk State Technical University, November 13", "summary": "This article addresses the development of quantum communication methods in the context of emerging quantum computing threats and emphasizes the importance of key reconciliation in quantum communication systems. The study focuses on the CASCADE protocol and the design of a software prototype intended for research and educational purposes. A parallel error-correction algorithm based on the actor model was implemented, improving the efficiency of key reconciliation and reducing the amount of exchanged data. Evaluation of the prototype revealed limitations, including the computational cost of message passing, complexity of error handling, and code redundancy due to iterative development. Experimental results confirmed the correct implementation of the core CASCADE algorithms and informed the design of future improvements. Proposed enhancements include redesigning the system architecture, developing interfaces for exporting intermediate data, defining the communication channel as a separate component, and expanding tools for systematic verification and comparative analysis of blind key-reconciliation methods.", "AI": {"tldr": "\u672c\u6587\u9488\u5bf9\u91cf\u5b50\u8ba1\u7b97\u5a01\u80c1\u4e0b\u7684\u91cf\u5b50\u901a\u4fe1\u7cfb\u7edf\uff0c\u7814\u7a76\u4e86CASCADE\u534f\u8bae\u5e76\u5f00\u53d1\u4e86\u4e00\u4e2a\u7528\u4e8e\u79d1\u7814\u4e0e\u6559\u5b66\u7684\u8f6f\u4ef6\u539f\u578b\uff0c\u901a\u8fc7\u57fa\u4e8eActor\u6a21\u578b\u7684\u5e76\u884c\u7ea0\u9519\u7b97\u6cd5\u63d0\u5347\u4e86\u5bc6\u94a5\u534f\u8c03\u6548\u7387\uff0c\u5e76\u63d0\u51fa\u4e86\u82e5\u5e72\u6539\u8fdb\u65b9\u5411\u3002", "motivation": "\u968f\u7740\u91cf\u5b50\u8ba1\u7b97\u7684\u53d1\u5c55\uff0c\u4f20\u7edf\u52a0\u5bc6\u65b9\u6cd5\u9762\u4e34\u5a01\u80c1\uff0c\u56e0\u6b64\u9700\u8981\u53d1\u5c55\u5b89\u5168\u7684\u91cf\u5b50\u901a\u4fe1\u6280\u672f\uff1b\u800c\u5bc6\u94a5\u534f\u8c03\u4f5c\u4e3a\u5176\u4e2d\u5173\u952e\u73af\u8282\uff0c\u4e9f\u9700\u9ad8\u6548\u4e14\u53ef\u9a8c\u8bc1\u7684\u5b9e\u73b0\u65b9\u6848\u3002", "method": "\u8bbe\u8ba1\u5e76\u5b9e\u73b0\u4e86\u4e00\u4e2a\u57fa\u4e8eActor\u6a21\u578b\u7684\u5e76\u884c\u9519\u8bef\u6821\u6b63\u7b97\u6cd5\u8f6f\u4ef6\u539f\u578b\uff0c\u7528\u4e8e\u6a21\u62df\u548c\u7814\u7a76CASCADE\u5bc6\u94a5\u534f\u8c03\u534f\u8bae\u3002", "result": "\u539f\u578b\u9a8c\u8bc1\u4e86CASCADE\u6838\u5fc3\u7b97\u6cd5\u7684\u6b63\u786e\u6027\uff0c\u63d0\u9ad8\u4e86\u5bc6\u94a5\u534f\u8c03\u6548\u7387\u5e76\u51cf\u5c11\u4e86\u901a\u4fe1\u6570\u636e\u91cf\uff0c\u4f46\u4e5f\u66b4\u9732\u51fa\u6d88\u606f\u4f20\u9012\u5f00\u9500\u5927\u3001\u9519\u8bef\u5904\u7406\u590d\u6742\u53ca\u4ee3\u7801\u5197\u4f59\u7b49\u95ee\u9898\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u91cf\u5b50\u5bc6\u94a5\u534f\u8c03\u63d0\u4f9b\u4e86\u53ef\u884c\u7684\u8f6f\u4ef6\u5b9e\u73b0\u57fa\u7840\uff0c\u672a\u6765\u53ef\u901a\u8fc7\u67b6\u6784\u91cd\u6784\u3001\u63a5\u53e3\u6269\u5c55\u548c\u7cfb\u7edf\u5316\u9a8c\u8bc1\u5de5\u5177\u8fdb\u4e00\u6b65\u4f18\u5316\u3002"}}
{"id": "2511.23157", "categories": ["cs.SE", "cs.HC"], "pdf": "https://arxiv.org/pdf/2511.23157", "abs": "https://arxiv.org/abs/2511.23157", "authors": ["Hana Kataoka", "Jialong Li", "Yutaka Matsuno"], "title": "Amplifiers or Equalizers? A Longitudinal Study of LLM Evolution in Software Engineering Project-Based Learning", "comment": "Accepted by ICSE-SEET (ACM/IEEE 48th International Conference on Software Engineering: Software Engineering Education and Training)", "summary": "As LLMs reshape software development, integrating LLM-augmented practices into SE education has become imperative. While existing studies explore LLMs' educational use in introductory programming or isolated SE tasks, their impact in more open-ended Project-Based Learning (PBL) remains unexplored. This paper introduces a two-year longitudinal study comparing a 2024 (using early free LLMs, $n$=48) and 2025 (using the latest paid LLMs, $n$=46) cohort. Our findings suggest the latest powerful LLMs' dual role: they act as \"equalizers,\" boosting average performance even for programming-weak students, providing opportunities for more authentic SE practices; yet also as \"amplifiers,\" dramatically widening absolute performance gaps, creating new pedagogical challenges for addressing educational inequities.", "AI": {"tldr": "\u672c\u6587\u901a\u8fc7\u4e24\u5e74\u7eb5\u5411\u7814\u7a76\uff0c\u63a2\u8ba8\u4e86\u6700\u65b0\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5728\u8f6f\u4ef6\u5de5\u7a0b\u9879\u76ee\u5f0f\u5b66\u4e60\u4e2d\u7684\u53cc\u91cd\u4f5c\u7528\uff1a\u65e2\u63d0\u5347\u6574\u4f53\u5b66\u751f\u8868\u73b0\uff08\u201c\u5747\u8861\u5668\u201d\u6548\u5e94\uff09\uff0c\u53c8\u6269\u5927\u6210\u7ee9\u5dee\u8ddd\uff08\u201c\u653e\u5927\u5668\u201d\u6548\u5e94\uff09\u3002", "motivation": "\u968f\u7740\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u91cd\u5851\u8f6f\u4ef6\u5f00\u53d1\uff0c\u5c06\u5176\u6574\u5408\u8fdb\u8f6f\u4ef6\u5de5\u7a0b\uff08SE\uff09\u6559\u80b2\u53d8\u5f97\u8feb\u5207\u3002\u73b0\u6709\u7814\u7a76\u591a\u805a\u7126\u4e8e\u5165\u95e8\u7f16\u7a0b\u6216\u5b64\u7acb\u7684SE\u4efb\u52a1\uff0c\u7f3a\u4e4f\u5bf9\u66f4\u5f00\u653e\u7684\u9879\u76ee\u5f0f\u5b66\u4e60\uff08PBL\uff09\u73af\u5883\u4e2dLLM\u5f71\u54cd\u7684\u63a2\u7d22\u3002", "method": "\u5f00\u5c55\u4e3a\u671f\u4e24\u5e74\u7684\u7eb5\u5411\u7814\u7a76\uff0c\u6bd4\u8f832024\u5e74\uff08\u4f7f\u7528\u65e9\u671f\u514d\u8d39LLM\uff0cn=48\uff09\u4e0e2025\u5e74\uff08\u4f7f\u7528\u6700\u65b0\u4ed8\u8d39LLM\uff0cn=46\uff09\u4e24\u4e2a\u5b66\u751f\u7fa4\u4f53\u5728SE\u9879\u76ee\u5f0f\u5b66\u4e60\u4e2d\u7684\u8868\u73b0\u5dee\u5f02\u3002", "result": "\u6700\u65b0\u5f3a\u5927\u7684LLM\u5177\u6709\u53cc\u91cd\u4f5c\u7528\uff1a\u4e00\u65b9\u9762\u4f5c\u4e3a\u201c\u5747\u8861\u5668\u201d\uff0c\u63d0\u5347\u5305\u62ec\u7f16\u7a0b\u80fd\u529b\u8f83\u5f31\u5b66\u751f\u5728\u5185\u7684\u6574\u4f53\u5e73\u5747\u8868\u73b0\uff0c\u4fc3\u8fdb\u66f4\u771f\u5b9e\u7684SE\u5b9e\u8df5\uff1b\u53e6\u4e00\u65b9\u9762\u4f5c\u4e3a\u201c\u653e\u5927\u5668\u201d\uff0c\u663e\u8457\u62c9\u5927\u5b66\u751f\u4e4b\u95f4\u7684\u7edd\u5bf9\u6210\u7ee9\u5dee\u8ddd\u3002", "conclusion": "\u5c3d\u7ba1\u5148\u8fdbLLM\u6709\u52a9\u4e8e\u63d0\u5347\u8f6f\u4ef6\u5de5\u7a0b\u6559\u80b2\u7684\u6574\u4f53\u6c34\u5e73\uff0c\u4f46\u5176\u52a0\u5267\u6559\u80b2\u4e0d\u5e73\u7b49\u7684\u73b0\u8c61\u4e5f\u5e26\u6765\u65b0\u7684\u6559\u5b66\u6311\u6218\uff0c\u9700\u5728\u6559\u5b66\u8bbe\u8ba1\u4e2d\u52a0\u4ee5\u5e94\u5bf9\u3002"}}
{"id": "2511.23159", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.23159", "abs": "https://arxiv.org/abs/2511.23159", "authors": ["Bertrand Meyer"], "title": "AI for software engineering: from probable to provable", "comment": null, "summary": "Vibe coding, the much-touted use of AI techniques for programming, faces two overwhelming obstacles: the difficulty of specifying goals (\"prompt engineering\" is a form of requirements engineering, one of the toughest disciplines of software engineering); and the hallucination phenomenon. Programs are only useful if they are correct or very close to correct.\n  The solution? Combine the creativity of artificial intelligence with the rigor of formal specification methods and the power of formal program verification, supported by modern proof tools.", "AI": {"tldr": "Vibe coding \u9762\u4e34\u76ee\u6807\u8bbe\u5b9a\u56f0\u96be\u548c\u5e7b\u89c9\u95ee\u9898\uff0c\u89e3\u51b3\u65b9\u6cd5\u662f\u7ed3\u5408\u4eba\u5de5\u667a\u80fd\u7684\u521b\u9020\u529b\u3001\u5f62\u5f0f\u5316\u89c4\u7ea6\u65b9\u6cd5\u7684\u4e25\u8c28\u6027\u4ee5\u53ca\u73b0\u4ee3\u8bc1\u660e\u5de5\u5177\u652f\u6301\u7684\u5f62\u5f0f\u5316\u7a0b\u5e8f\u9a8c\u8bc1\u3002", "motivation": "\u5f53\u524d\u57fa\u4e8e AI \u7684\u7f16\u7a0b\u65b9\u6cd5\uff08\u5982 vibe coding\uff09\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u53d7\u9650\u4e8e\u4e24\u4e2a\u4e3b\u8981\u95ee\u9898\uff1a\u4e00\u662f\u96be\u4ee5\u51c6\u786e\u8868\u8fbe\u7f16\u7a0b\u76ee\u6807\uff08\u5373\u63d0\u793a\u5de5\u7a0b\u672c\u8d28\u4e0a\u5c5e\u4e8e\u9700\u6c42\u5de5\u7a0b\uff0c\u800c\u9700\u6c42\u5de5\u7a0b\u672c\u8eab\u662f\u8f6f\u4ef6\u5de5\u7a0b\u4e2d\u6700\u56f0\u96be\u7684\u9886\u57df\u4e4b\u4e00\uff09\uff1b\u4e8c\u662f AI \u751f\u6210\u4ee3\u7801\u65f6\u5bb9\u6613\u4ea7\u751f\u201c\u5e7b\u89c9\u201d\uff0c\u5bfc\u81f4\u7a0b\u5e8f\u4e0d\u6b63\u786e\u6216\u4e0d\u53ef\u9760\u3002", "method": "\u5c06\u4eba\u5de5\u667a\u80fd\u7684\u521b\u9020\u6027\u80fd\u529b\u4e0e\u5f62\u5f0f\u5316\u89c4\u7ea6\u65b9\u6cd5\u7684\u4e25\u8c28\u6027\u76f8\u7ed3\u5408\uff0c\u5e76\u5229\u7528\u73b0\u4ee3\u5f62\u5f0f\u5316\u9a8c\u8bc1\u5de5\u5177\u5bf9\u751f\u6210\u7684\u7a0b\u5e8f\u8fdb\u884c\u4e25\u683c\u9a8c\u8bc1\u3002", "result": "\u8be5\u65b9\u6cd5\u6709\u671b\u514b\u670d vibe coding \u4e2d\u7684\u76ee\u6807\u6a21\u7cca\u6027\u548c\u5e7b\u89c9\u95ee\u9898\uff0c\u4ece\u800c\u751f\u6210\u65e2\u5177\u521b\u9020\u6027\u53c8\u9ad8\u5ea6\u53ef\u9760\u7684\u7a0b\u5e8f\u3002", "conclusion": "\u8981\u4f7f AI \u7f16\u7a0b\u771f\u6b63\u5b9e\u7528\uff0c\u5fc5\u987b\u878d\u5408\u5f62\u5f0f\u5316\u65b9\u6cd5\u4e0e AI \u6280\u672f\uff0c\u4ee5\u786e\u4fdd\u7a0b\u5e8f\u7684\u6b63\u786e\u6027\u4e0e\u53ef\u9760\u6027\u3002"}}
{"id": "2511.23213", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2511.23213", "abs": "https://arxiv.org/abs/2511.23213", "authors": ["Samuele Doria", "Eleonora Losiouk"], "title": "GAPS: Guiding Dynamic Android Analysis with Static Path Synthesis", "comment": null, "summary": "Dynamically resolving method reachability in Android applications remains a critical and largely unsolved problem. Despite notable advancements in GUI testing and static call graph construction, current tools are insufficient for reliably driving execution toward specific target methods, especially those not embedded in a graphical component (e.g., libraries' methods), a capability essential for tasks such as vulnerability validation, debugging, and behavioral analysis.\n  We present GAPS (Graph-based Automated Path Synthesizer), the first system that integrates static, method-guided call graph analysis with dynamic, interaction-driven execution. GAPS performs a lightweight backward traversal of the call graph, guided by data-flow analysis, to reconstruct paths reaching the target methods. These paths are then translated into instructions that guide runtime app exploration.\n  On the AndroTest benchmark, GAPS statically identifies paths to reach 88.24\\% of the target methods in just 4.27 seconds per app and dynamically reaches 57.44\\% of them. In contrast, state-of-the-art dynamic interaction tools show significantly lower reachability over three runs: APE, one of the best model-based GUI testers, achieves 12.82\\%, while GoalExplorer, a hybrid analysis tool, reaches 9.69\\%, and Guardian, an LLM-based UI automator, reaches 17.12\\%. Static analysis tools also fall short: FlowDroid and DroidReach identify paths to reach 58.81\\% and 9.48\\% of the targets, requiring 35.06 seconds and 23.46 seconds per app, respectively.\n  Finally, an evaluation on the 50 most downloaded real-world apps demonstrates GAPS's practical utility in analyzing security-critical code under a realistic scenario. With an average static analysis time of 278.9 seconds, GAPS statically reconstructs paths to 62.03\\% of the target methods and dynamically reaches 59.86\\% of them.", "AI": {"tldr": "GAPS \u662f\u9996\u4e2a\u7ed3\u5408\u9759\u6001\u65b9\u6cd5\u5bfc\u5411\u8c03\u7528\u56fe\u5206\u6790\u4e0e\u52a8\u6001\u4ea4\u4e92\u9a71\u52a8\u6267\u884c\u7684\u7cfb\u7edf\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u5de5\u5177\u5728 Android \u5e94\u7528\u4e2d\u52a8\u6001\u89e6\u8fbe\u76ee\u6807\u65b9\u6cd5\u7684\u80fd\u529b\u3002", "motivation": "\u5f53\u524d Android \u5e94\u7528\u4e2d\u52a8\u6001\u89e3\u6790\u65b9\u6cd5\u53ef\u8fbe\u6027\u4ecd\u662f\u4e00\u4e2a\u5173\u952e\u4e14\u672a\u5145\u5206\u89e3\u51b3\u7684\u95ee\u9898\uff0c\u5c24\u5176\u5bf9\u4e8e\u975e GUI \u7ec4\u4ef6\uff08\u5982\u5e93\u65b9\u6cd5\uff09\u7684\u76ee\u6807\u65b9\u6cd5\uff0c\u73b0\u6709 GUI \u6d4b\u8bd5\u548c\u9759\u6001\u8c03\u7528\u56fe\u6784\u5efa\u5de5\u5177\u96be\u4ee5\u53ef\u9760\u9a71\u52a8\u6267\u884c\u81f3\u8fd9\u4e9b\u65b9\u6cd5\uff0c\u800c\u8fd9\u5bf9\u6f0f\u6d1e\u9a8c\u8bc1\u3001\u8c03\u8bd5\u548c\u884c\u4e3a\u5206\u6790\u81f3\u5173\u91cd\u8981\u3002", "method": "GAPS \u91c7\u7528\u8f7b\u91cf\u7ea7\u7684\u3001\u7531\u6570\u636e\u6d41\u5206\u6790\u5f15\u5bfc\u7684\u8c03\u7528\u56fe\u53cd\u5411\u904d\u5386\uff0c\u91cd\u6784\u901a\u5f80\u76ee\u6807\u65b9\u6cd5\u7684\u8def\u5f84\uff0c\u5e76\u5c06\u8fd9\u4e9b\u8def\u5f84\u8f6c\u5316\u4e3a\u8fd0\u884c\u65f6\u5e94\u7528\u63a2\u7d22\u7684\u6307\u4ee4\uff0c\u4ece\u800c\u6574\u5408\u9759\u6001\u5206\u6790\u4e0e\u52a8\u6001\u6267\u884c\u3002", "result": "\u5728 AndroTest \u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cGAPS \u9759\u6001\u8bc6\u522b 88.24% \u76ee\u6807\u65b9\u6cd5\u8def\u5f84\uff08\u6bcf\u5e94\u7528\u4ec5\u9700 4.27 \u79d2\uff09\uff0c\u52a8\u6001\u89e6\u8fbe 57.44%\uff1b\u8fdc\u8d85 APE\uff0812.82%\uff09\u3001GoalExplorer\uff089.69%\uff09\u548c Guardian\uff0817.12%\uff09\u7b49\u52a8\u6001\u5de5\u5177\uff0c\u4e5f\u4f18\u4e8e FlowDroid\uff0858.81%\uff09\u548c DroidReach\uff089.48%\uff09\u7b49\u9759\u6001\u5de5\u5177\u3002\u5728 50 \u4e2a\u70ed\u95e8\u771f\u5b9e\u5e94\u7528\u4e2d\uff0cGAPS \u5e73\u5747\u9759\u6001\u5206\u6790\u8017\u65f6 278.9 \u79d2\uff0c\u9759\u6001\u8def\u5f84\u8986\u76d6\u7387\u8fbe 62.03%\uff0c\u52a8\u6001\u89e6\u8fbe\u7387\u8fbe 59.86%\u3002", "conclusion": "GAPS \u5728\u9759\u6001\u8def\u5f84\u91cd\u5efa\u548c\u52a8\u6001\u65b9\u6cd5\u89e6\u8fbe\u65b9\u9762\u5747\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u6280\u672f\uff0c\u5c55\u73b0\u51fa\u5728\u771f\u5b9e\u573a\u666f\u4e0b\u5206\u6790\u5b89\u5168\u5173\u952e\u4ee3\u7801\u7684\u5f3a\u5927\u5b9e\u7528\u6027\u3002"}}
{"id": "2511.23302", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2511.23302", "abs": "https://arxiv.org/abs/2511.23302", "authors": ["Hengyuan Liu", "Zheng Li", "Donghua Wang", "Yankai Wu", "Xiang Chen", "Yong Liu"], "title": "FLIMs: Fault Localization Interference Mutants, Definition, Recognition and Mitigation", "comment": null, "summary": "Mutation-based Fault Localization (MBFL) has been widely explored for automated software debugging, leveraging artificial mutants to identify faulty code entities. However, MBFL faces significant challenges due to interference mutants generated from non-faulty code entities but can be killed by failing tests. These mutants mimic the test sensitivity behaviors of real faulty code entities and weaken the effectiveness of fault localization. To address this challenge, we introduce the concept of Fault Localization Interference Mutants (FLIMs) and conduct a theoretical analysis based on the Reachability, Infection, Propagation, and Revealability (RIPR) model, identifying four distinct interference causes. Building on this, we propose a novel approach to semantically recognize and mitigate FLIMs using LLM-based semantic analysis, enhanced by fine-tuning techniques and confidence estimation strategies to address LLM output instability. The recognized FLIMs are then mitigated by refining the suspiciousness scores calculated from MBFL techniques. We integrate FLIM recognition and mitigation into the MBFL workflow, developing MBFL-FLIM, a fault localization framework that enhances MBFL's effectiveness by reducing misleading interference while preserving real fault-revealing information. Our empirical experiments on the Defects4J benchmark with 395 program versions using eight LLMs demonstrate MBFL-FLIM's superiority over traditional SBFL and MBFL methods, advanced dynamic feature-based approaches, and recent LLM-based fault localization techniques. Specifically, MBFL-FLIM achieves an average improvement of 44 faults in the Top-1 metric, representing a significant enhancement over baseline methods. Further evaluation confirms MBFL-FLIM's robust performance in multi-fault scenarios, with ablation experiments validating the contributions of the fine-tuning and confidence estimation components.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faMBFL-FLIM\u6846\u67b6\uff0c\u901a\u8fc7\u5f15\u5165\u6545\u969c\u5b9a\u4f4d\u5e72\u6270\u53d8\u5f02\u4f53\uff08FLIMs\uff09\u7684\u6982\u5ff5\uff0c\u5e76\u5229\u7528\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u8bed\u4e49\u5206\u6790\u8bc6\u522b\u548c\u7f13\u89e3\u8fd9\u4e9b\u5e72\u6270\u53d8\u5f02\u4f53\uff0c\u4ece\u800c\u63d0\u5347\u57fa\u4e8e\u53d8\u5f02\u7684\u6545\u969c\u5b9a\u4f4d\uff08MBFL\uff09\u7684\u51c6\u786e\u6027\u3002", "motivation": "\u4f20\u7edfMBFL\u65b9\u6cd5\u56e0\u975e\u6545\u969c\u4ee3\u7801\u751f\u6210\u7684\u5e72\u6270\u53d8\u5f02\u4f53\uff08FLIMs\uff09\u800c\u6548\u679c\u53d7\u9650\uff0c\u8fd9\u4e9b\u53d8\u5f02\u4f53\u80fd\u88ab\u5931\u8d25\u6d4b\u8bd5\u6740\u6b7b\uff0c\u6a21\u4eff\u771f\u5b9e\u6545\u969c\u884c\u4e3a\uff0c\u8bef\u5bfc\u5b9a\u4f4d\u7ed3\u679c\u3002", "method": "\u57fa\u4e8eRIPR\u6a21\u578b\u7406\u8bba\u5206\u6790FLIMs\u6210\u56e0\uff0c\u7ed3\u5408\u5fae\u8c03\u4e0e\u7f6e\u4fe1\u5ea6\u4f30\u8ba1\u7b56\u7565\uff0c\u5229\u7528LLM\u8fdb\u884c\u8bed\u4e49\u5206\u6790\u4ee5\u8bc6\u522bFLIMs\uff0c\u5e76\u5728MBFL\u6d41\u7a0b\u4e2d\u8c03\u6574\u53ef\u7591\u5ea6\u5206\u6570\u4ee5\u51cf\u8f7b\u5176\u5e72\u6270\u3002", "result": "\u5728Defects4J\u57fa\u51c6\u4e0a\u5bf9395\u4e2a\u7a0b\u5e8f\u7248\u672c\u7684\u5b9e\u9a8c\u8868\u660e\uff0cMBFL-FLIM\u5728Top-1\u6307\u6807\u4e0a\u5e73\u5747\u591a\u5b9a\u4f4d44\u4e2a\u6545\u969c\uff0c\u4f18\u4e8e\u4f20\u7edfSBFL\u3001MBFL\u3001\u52a8\u6001\u7279\u5f81\u65b9\u6cd5\u53ca\u73b0\u6709LLM\u6545\u969c\u5b9a\u4f4d\u6280\u672f\uff0c\u4e14\u5728\u591a\u6545\u969c\u573a\u666f\u4e0b\u8868\u73b0\u7a33\u5065\u3002", "conclusion": "\u901a\u8fc7\u8bed\u4e49\u8bc6\u522b\u4e0e\u7f13\u89e3FLIMs\uff0cMBFL-FLIM\u663e\u8457\u63d0\u5347\u4e86MBFL\u7684\u6545\u969c\u5b9a\u4f4d\u7cbe\u5ea6\u548c\u9c81\u68d2\u6027\uff0c\u9a8c\u8bc1\u4e86\u6240\u63d0\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2511.23321", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2511.23321", "abs": "https://arxiv.org/abs/2511.23321", "authors": ["Yifei Wang", "Jacky Keung", "Zhenyu Mao", "Jingyu Zhang", "Yuchen Cao"], "title": "Chart2Code-MoLA: Efficient Multi-Modal Code Generation via Adaptive Expert Routing", "comment": null, "summary": "Chart-to-code generation is a critical task in automated data visualization, translating complex chart structures into executable programs. While recent Multi-modal Large Language Models (MLLMs) improve chart representation, existing approaches still struggle to achieve cross-type generalization, memory efficiency, and modular design. To address these challenges, this paper proposes C2C-MoLA, a multimodal framework that synergizes Mixture of Experts (MoE) with Low-Rank Adaptation (LoRA). The MoE component uses a complexity-aware routing mechanism with domain-specialized experts and load-balanced sparse gating, dynamically allocating inputs based on learnable structural metrics like element count and chart complexity. LoRA enables parameter-efficient updates for resource-conscious tuning, further supported by a tailored training strategy that aligns routing stability with semantic accuracy. Experiments on Chart2Code-160k show that the proposed model improves generation accuracy by up to 17%, reduces peak GPU memory by 18%, and accelerates convergence by 20%, when compared to standard fine-tuning and LoRA-only baselines, particularly on complex charts. Ablation studies validate optimal designs, such as 8 experts and rank-8 LoRA, and confirm scalability for real-world multimodal code generation.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faC2C-MoLA\u6846\u67b6\uff0c\u7ed3\u5408\u6df7\u5408\u4e13\u5bb6\uff08MoE\uff09\u4e0e\u4f4e\u79e9\u81ea\u9002\u5e94\uff08LoRA\uff09\uff0c\u5728\u56fe\u8868\u5230\u4ee3\u7801\u751f\u6210\u4efb\u52a1\u4e2d\u663e\u8457\u63d0\u5347\u51c6\u786e\u7387\u3001\u964d\u4f4e\u663e\u5b58\u6d88\u8017\u5e76\u52a0\u5feb\u6536\u655b\u901f\u5ea6\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u56fe\u8868\u5230\u4ee3\u7801\u751f\u6210\u4efb\u52a1\u4e2d\u96be\u4ee5\u517c\u987e\u8de8\u7c7b\u578b\u6cdb\u5316\u80fd\u529b\u3001\u5185\u5b58\u6548\u7387\u548c\u6a21\u5757\u5316\u8bbe\u8ba1\uff0c\u4e9f\u9700\u66f4\u9ad8\u6548\u4e14\u53ef\u6269\u5c55\u7684\u591a\u6a21\u6001\u6a21\u578b\u67b6\u6784\u3002", "method": "\u63d0\u51faC2C-MoLA\u6846\u67b6\uff0c\u878d\u5408Mixture of Experts\uff08MoE\uff09\u4e0eLow-Rank Adaptation\uff08LoRA\uff09\u3002MoE\u91c7\u7528\u57fa\u4e8e\u56fe\u8868\u590d\u6742\u5ea6\u7684\u8def\u7531\u673a\u5236\uff0c\u5305\u542b\u9886\u57df\u4e13\u7528\u4e13\u5bb6\u548c\u8d1f\u8f7d\u5747\u8861\u7a00\u758f\u95e8\u63a7\uff1bLoRA\u7528\u4e8e\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\uff0c\u5e76\u914d\u5408\u7a33\u5b9a\u8def\u7531\u4e0e\u8bed\u4e49\u5bf9\u9f50\u7684\u8bad\u7ec3\u7b56\u7565\u3002", "result": "\u5728Chart2Code-160k\u6570\u636e\u96c6\u4e0a\uff0c\u76f8\u6bd4\u6807\u51c6\u5fae\u8c03\u548c\u4ec5LoRA\u57fa\u7ebf\uff0c\u8be5\u6a21\u578b\u751f\u6210\u51c6\u786e\u7387\u6700\u9ad8\u63d0\u534717%\uff0c\u5cf0\u503cGPU\u5185\u5b58\u51cf\u5c1118%\uff0c\u6536\u655b\u901f\u5ea6\u52a0\u5feb20%\uff0c\u5c24\u5176\u5728\u590d\u6742\u56fe\u8868\u4e0a\u8868\u73b0\u7a81\u51fa\u3002\u6d88\u878d\u5b9e\u9a8c\u9a8c\u8bc1\u4e868\u4e2a\u4e13\u5bb6\u548crank-8 LoRA\u4e3a\u6700\u4f18\u914d\u7f6e\u3002", "conclusion": "C2C-MoLA\u901a\u8fc7MoE\u4e0eLoRA\u7684\u534f\u540c\u8bbe\u8ba1\uff0c\u5728\u56fe\u8868\u5230\u4ee3\u7801\u751f\u6210\u4efb\u52a1\u4e2d\u5b9e\u73b0\u4e86\u9ad8\u51c6\u786e\u6027\u3001\u4f4e\u8d44\u6e90\u6d88\u8017\u4e0e\u826f\u597d\u53ef\u6269\u5c55\u6027\uff0c\u4e3a\u591a\u6a21\u6001\u7a0b\u5e8f\u751f\u6210\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
