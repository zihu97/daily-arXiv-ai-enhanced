<div id=toc></div>

# Table of Contents

- [cs.PF](#cs.PF) [Total: 1]
- [cs.AR](#cs.AR) [Total: 3]
- [cs.DC](#cs.DC) [Total: 7]
- [cs.SE](#cs.SE) [Total: 5]
- [cs.MA](#cs.MA) [Total: 1]


<div id='cs.PF'></div>

# cs.PF [[Back]](#toc)

### [1] [The Configuration Wall: Characterization and Elimination of Accelerator Configuration Overhead](https://arxiv.org/abs/2511.10397)
*Josse Van Delm,Anton Lydike,Joren Dumoulin,Jonas Crols,Xiaoling Yi,Ryan Antonio,Jackson Woodruff,Tobias Grosser,Marian Verhelst*

Main category: cs.PF

TL;DR: 本文提出了一种扩展的roofline模型来识别系统是否受配置瓶颈限制，并引入一种领域特定的编译器抽象与优化方法，通过MLIR框架实现，在OpenGeMM系统上实现了2倍的几何平均性能提升，有效缓解“配置墙”问题。


<details>
  <summary>Details</summary>
Motivation: 随着硬件加速器复杂度增加，CPU在控制设置和同步上的开销显著上升，导致系统性能受限于配置过程，形成所谓的“配置墙”。现有方案缺乏普适性且未能深入揭示性能下降的根本原因。

Method: 作者首先提出一种适用于广泛场景的roofline模型变体，用于量化系统是否处于配置受限状态；随后设计了一种领域特定的编译器抽象及配套优化流程，并在MLIR编译器框架中实现，以自动生成高效代码。

Result: 在开源的OpenGeMM系统上实验表明，该方法通过消除冗余配置周期并自动隐藏剩余配置开销，实现了2倍的几何平均性能提升。

Conclusion: 本研究揭示了加速器性能受配置机制影响的关键机理，为绕过“配置墙”提供了通用且自动化的编译器解决方案，具有良好的有效性与普适性。

Abstract: Contemporary compute platforms increasingly offload compute kernels from CPU to integrated hardware accelerators to reach maximum performance per Watt. Unfortunately, the time the CPU spends on setup control and synchronization has increased with growing accelerator complexity. For systems with complex accelerators, this means that performance can be configuration-bound. Faster accelerators are more severely impacted by this overlooked performance drop, which we call the configuration wall. Prior work evidences this wall and proposes ad-hoc solutions to reduce configuration overhead. However, these solutions are not universally applicable, nor do they offer comprehensive insights into the underlying causes of performance degradation. In this work, we first introduce a widely-applicable variant of the well-known roofline model to quantify when system performance is configuration-bound. To move systems out of the performance-bound region, we subsequently propose a domain-specific compiler abstraction and associated optimization passes. We implement the abstraction and passes in the MLIR compiler framework to run optimized binaries on open-source architectures to prove its effectiveness and generality. Experiments demonstrate a geomean performance boost of 2x on the open-source OpenGeMM system, by eliminating redundant configuration cycles and by automatically hiding the remaining configuration cycles. Our work provides key insights in how accelerator performance is affected by setup mechanisms, thereby facilitating automatic code generation for circumventing the configuration wall.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [2] [History-Aware Trajectory k-Anonymization Using an FPGA-Based Hardware Accelerator for Real-Time Location Services](https://arxiv.org/abs/2511.09688)
*Hiroshi Nakano,Hiroaki Nishi*

Main category: cs.AR

TL;DR: 本文提出了一种结合历史轨迹信息的k-匿名化方法，并在FPGA上实现了支持该方法的硬件架构，在保证实时性的同时提升了轨迹数据的效用和保真度。


<details>
  <summary>Details</summary>
Motivation: 先前基于最短路径的轨迹匿名化方法无法准确反映用户真实出行行为，导致匿名化后数据效用降低，因此需要引入历史行为信息以提升实用性。

Method: 提出一种历史感知的轨迹k-匿名化方法，设计了集成并行历史轨迹搜索与最短路径查找的FPGA硬件架构，并采用自定义定点计数模块对历史数据贡献进行加权。

Result: 新架构在FPGA上实现实时处理超过6000条记录/秒，相比仅使用最短路径的方法，数据保留率提升最多1.2%，并更有效地保留主干道路信息。

Conclusion: 该研究实现了在LBS严格延迟约束下兼顾隐私保护与行为准确性的高保真轨迹匿名化，是轨迹隐私保护技术的重要进展。

Abstract: Our previous work established the feasibility of FPGA-based real-time trajectory anonymization, a critical task for protecting user privacy in modern location-based services (LBS). However, that pioneering approach relied exclusively on shortest-path computations, which can fail to capture re- alistic travel behavior and thus reduce the utility of the anonymized data. To address this limitation, this paper introduces a novel, history-aware trajectory k-anonymization methodology and presents an advanced FPGA-based hardware architecture to implement it. Our proposed architecture uniquely integrates par- allel history-based trajectory searches with conventional shortest- path finding, using a custom fixed-point counting module to ac- curately weigh contributions from historical data. This approach enables the system to prioritize behaviorally common routes over geometrically shorter but less-traveled paths. The FPGA implementation demonstrates that our new architecture achieves a real-time throughput of over 6,000 records/s, improves data retention by up to 1.2% compared to our previous shortest-path- only design, and preserves major arterial roads more effectively. These results signify a key advancement, enabling high-fidelity, history-aware anonymization that preserves both privacy and behavioral accuracy under the strict latency constraints of LBS.

</details>


### [3] [AssertMiner: Module-Level Spec Generation and Assertion Mining using Static Analysis Guided LLMs](https://arxiv.org/abs/2511.10007)
*Hongqin Lyu,Yonghao Wang,Jiaxin Zhou,Zhiteng Chao,Tiancheng Wang,Huawei Li*

Main category: cs.AR

TL;DR: AssertMiner is a module-level assertion generation framework that uses static AST-derived information to guide LLMs in generating detailed, high-quality assertions, improving verification coverage and error detection over existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing assertion generation methods focus only on top-level specifications and neglect micro-architectural implementation details where design errors are more common, creating a need for module-level assertion generation.

Method: AssertMiner leverages abstract syntax tree (AST) analysis to extract structural information—such as module call graphs, I/O tables, and dataflow graphs—and uses this to guide large language models (LLMs) in mining module-level assertions.

Result: AssertMiner outperforms prior methods like AssertLLM and Spec2Assertion in generating high-quality module-level assertions and enhances structural coverage and error detection when integrated with them.

Conclusion: By focusing on module-level details through AST-guided LLM prompting, AssertMiner enables more comprehensive and efficient hardware verification.

Abstract: Assertion-based verification (ABV) is a key approach to checking whether a logic design complies with its architectural specifications. Existing assertion generation methods based on design specifications typically produce only top-level assertions, overlooking verification needs on the implementation details in the modules at the micro-architectural level, where design errors occur more frequently. To address this limitation, we present AssertMiner, a module-level assertion generation framework that leverages static information generated from abstract syntax tree (AST) to assist LLMs in mining assertions. Specifically, it performs AST-based structural extraction to derive the module call graph, I/O table, and dataflow graph, guiding the LLM to generate module-level specifications and mine module-level assertions. Our evaluation demonstrates that AssertMiner outperforms existing methods such as AssertLLM and Spec2Assertion in generating high-quality assertions for modules. When integrated with these methods, AssertMiner can enhance the structural coverage and significantly improve the error detection capability, enabling a more comprehensive and efficient verification process.

</details>


### [4] [Beamspace Equalization for mmWave Massive MIMO: Algorithms and VLSI Implementations](https://arxiv.org/abs/2511.10563)
*Seyed Hadi Mirfarshbafan,Christoph Studer*

Main category: cs.AR

TL;DR: 本文研究了面向毫米波大规模MIMO系统的波束域数据检测算法与VLSI架构，提出了一种新型复稀疏自适应均衡器（CSPADE），在22nm工艺下实现显著的功耗、能效与面积优势，并达到当前最高的吞吐量。


<details>
  <summary>Details</summary>
Motivation: 大规模多用户MIMO和毫米波通信虽是未来无线系统的关键技术，但其基带处理硬件成本和功耗过高；利用毫米波信道稀疏性进行波束域处理可降低复杂度，因此需开发高效的数据检测算法与低功耗VLSI架构。

Method: 提出新的波束域数据检测算法CSPADE，并设计对应的全并行与基于MAC的串行VLSI架构，在22nm FDSOI工艺下进行实现与评估。

Result: 全并行CSPADE架构相比天线域均衡器最多节省54%功耗，且在现有大规模MIMO检测器中吞吐量最高、能效与面积效率更优；基于MAC的串行架构则最多节省66%功耗。

Conclusion: 所提出的CSPADE算法及其VLSI架构在功耗、吞吐量、能效和面积方面均优于现有方案，为毫米波大规模MIMO系统提供了高效可行的基带处理解决方案。

Abstract: Massive multiuser multiple-input multiple-output (MIMO) and millimeter-wave (mmWave) communication are key physical layer technologies in future wireless systems. Their deployment, however, is expected to incur excessive baseband processing hardware cost and power consumption. Beamspace processing leverages the channel sparsity at mmWave frequencies to reduce baseband processing complexity. In this paper, we review existing beamspace data detection algorithms and propose new algorithms as well as corresponding VLSI architectures that reduce data detection power. We present VLSI implementation results for the proposed architectures in a 22nm FDSOI process. Our results demonstrate that a fully-parallelized implementation of the proposed complex sparsity-adaptive equalizer (CSPADE) achieves up to 54% power savings compared to antenna-domain equalization. Furthermore, our fully-parallelized designs achieve the highest reported throughput among existing massive MIMO data detectors, while achieving better energy and area efficiency. We also present a sequential multiply-accumulate (MAC)-based architecture for CSPADE, which enables even higher power savings, i.e., up to 66%, compared to a MAC-based antenna-domain equalizer.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [5] [Optimizing CPU Cache Utilization in Cloud VMs with Accurate Cache Abstraction](https://arxiv.org/abs/2511.09956)
*Mani Tofigh,Edward Guo,Weiwei Jia,Xiaoning Ding,Jianchen Shan*

Main category: cs.DC

TL;DR: 本文指出云虚拟机中缓存优化常因缺乏对缓存分配的可见性与控制而失效，并提出名为CacheX的新方案，通过驱逐集在无需硬件或虚拟机监控器支持的情况下探测细粒度缓存信息，进而实现LLC争用感知的任务调度和虚拟颜色感知的页缓存管理，显著提升公有云VM中的缓存利用率。


<details>
  <summary>Details</summary>
Motivation: 在公有云环境中，虚拟机无法获知CPU缓存的分配细节（如是否被分区或共享），也无法通过页面放置策略影响缓存使用，因为内存到缓存的映射对虚拟机是隐藏的，导致传统缓存优化方法效果有限。

Method: 提出CacheX系统，利用驱逐集（eviction sets）在虚拟机内部探测精确且细粒度的缓存抽象，无需依赖硬件或Hypervisor支持；并基于所获取的信息实现两种新技术：末级缓存（LLC）争用感知的任务调度和虚拟颜色感知的页缓存管理。

Result: 在x86 Linux内核中实现并评估CacheX，结果表明其能有效提升多种工作负载在公有云虚拟机中的缓存利用率。

Conclusion: CacheX通过在虚拟机内部自主探测缓存结构，克服了云环境中缓存信息不可见的问题，为提升缓存性能提供了实用且无需底层支持的新途径。

Abstract: This paper shows that cache-based optimizations are often ineffective in cloud virtual machines (VMs) due to limited visibility into and control over provisioned caches. In public clouds, CPU caches can be partitioned or shared among VMs, but a VM is unaware of cache provisioning details. Moreover, a VM cannot influence cache usage via page placement policies, as memory-to-cache mappings are hidden. The paper proposes a novel solution, CacheX, which probes accurate and fine-grained cache abstraction within VMs using eviction sets without requiring hardware or hypervisor support, and showcases the utility of the probed information with two new techniques: LLC contention-aware task scheduling and virtual color-aware page cache management. Our evaluation of CacheX's implementation in x86 Linux kernel demonstrates that it can effectively improve cache utilization for various workloads in public cloud VMs.

</details>


### [6] [Ksurf-Drone: Attention Kalman Filter for Contextual Bandit Optimization in Cloud Resource Allocation](https://arxiv.org/abs/2511.09766)
*Michael Dang'ana,Yuqiu Zhang,Hans-Arno Jacobsen*

Main category: cs.DC

TL;DR: 本文评估了Ksurf在高波动云环境下作为Drone编排器中上下文多臂赌博机目标函数模型的性能，结果表明其显著降低了延迟方差并节省了资源成本。


<details>
  <summary>Details</summary>
Motivation: 云数据中心中容器化基础设施面临配置搜索空间庞大和云环境不确定性高的挑战，现有编排方法（如Drone）在高度可变的工作负载下准确性受限。

Method: 将Ksurf这一方差最小化估计方法集成到Drone编排器中，作为上下文多臂赌博机的目标函数模型，用于高波动性工作负载下的资源估计与编排。

Result: Ksurf在p95和p99分别降低41%和47%的延迟方差，减少4%的CPU使用率、7MB主节点内存占用，并在VarBench基准测试中实现平均7%的Worker Pod数量成本节约。

Conclusion: Ksurf能有效提升高波动云环境中资源编排的准确性和效率，显著降低延迟方差与资源开销，具有实际部署价值。

Abstract: Resource orchestration and configuration parameter search are key concerns for container-based infrastructure in cloud data centers. Large configuration search space and cloud uncertainties are often mitigated using contextual bandit techniques for resource orchestration including the state-of-the-art Drone orchestrator. Complexity in the cloud provider environment due to varying numbers of virtual machines introduces variability in workloads and resource metrics, making orchestration decisions less accurate due to increased nonlinearity and noise. Ksurf, a state-of-the-art variance-minimizing estimator method ideal for highly variable cloud data, enables optimal resource estimation under conditions of high cloud variability.
  This work evaluates the performance of Ksurf on estimation-based resource orchestration tasks involving highly variable workloads when employed as a contextual multi-armed bandit objective function model for cloud scenarios using Drone. Ksurf enables significantly lower latency variance of $41\%$ at p95 and $47\%$ at p99, demonstrates a $4\%$ reduction in CPU usage and 7 MB reduction in master node memory usage on Kubernetes, resulting in a $7\%$ cost savings in average worker pod count on VarBench Kubernetes benchmark.

</details>


### [7] [MoFa: A Unified Performance Modeling Framework for LLM Pretraining](https://arxiv.org/abs/2511.09837)
*Lu Zhao,Rong Shi,Shaoqing Zhang,Shangchao Su,Ziqing Yin,Zhiyan Cui,Hongfeng Sun,Baoguo He,Yueqiang Chen,Liang Dong,Xiyuan Li,Lingbin Wang,Lijun Ma,Qiang Huang,Ting Liu,Chong Wang,Can Wei*

Main category: cs.DC

TL;DR: 本文提出了MoFa，一种融合多维优化特征与容错机制的大模型预训练性能建模框架，显著提升了预测准确性，并为系统设计提供先验指导。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型参数规模激增至万亿级别，预训练需在数千至上万设备组成的集群上进行分布式计算。现有性能建模方法未能全面考虑主流优化技术，且忽略了长期训练中容错机制（如检查点恢复）带来的显著开销，导致调优成本高昂。

Method: 提出MoFa框架，包含增强的成本模型以精确刻画关键优化效果，并基于历史集群可靠性数据构建容错模型；同时开发了基于MoFa的调优系统，用于探索不同场景下的最优预训练性能和潜在瓶颈。

Result: 大量实验表明，MoFa在多种场景下均能实现高精度性能预测；调优实验系统性揭示了不同配置下影响预训练性能的关键因素。

Conclusion: MoFa有效解决了现有建模方法忽略优化特征与容错开销的问题，为大模型预训练系统的高效设计与部署提供了可靠的先验指导。

Abstract: The exponential growth in LLM scales, with parameters soaring from billions to trillions, has necessitated distributed pretraining across large clusters comprising thousands to tens of thousands of devices. While hybrid parallelization strategies enable such pretraining, the vast combinatorial strategy space introduces significant optimization challenges. Traditional manual tuning methods incur prohibitive trial-and-error costs, and existing performance modeling approaches exhibit critical limitations: they fail to comprehensively account for prevalent optimization features and ignore the substantial overhead imposed by essential fault tolerance mechanisms like checkpoint recovery in long-duration pretraining. To address these gaps, we propose MoFa, a novel pretraining performance modeling framework that unifies multi-dimensional optimization features and fault tolerance. MoFa incorporates an enhanced cost model to accurately capture the effects of key optimizations and integrates a fault tolerance model based on historical cluster reliability data. Besides, a MoFa-based tuning system is developed to explore optimal pretraining performance and potential bottlenecks in various scenarios. Extensive modeling evaluations demonstrate that MoFa can achieve high prediction accuracy across various scenarios. In addition, through comprehensive tuning experiments, our framework systematically reveals the key factors influencing pretraining performance under different configurations, which provides solid a priori guidance for LLM pretraining system design and deployment.

</details>


### [8] [Lit Silicon: A Case Where Thermal Imbalance Couples Concurrent Execution in Multiple GPUs](https://arxiv.org/abs/2511.09861)
*Marco Kurzynski,Shaizeen Aga,Di Wu*

Main category: cs.DC

TL;DR: 该论文揭示了多GPU系统中由热不平衡引发的“Lit Silicon效应”——即热耦合导致的GPU性能差异会通过计算通信重叠（C3）机制放大，造成节点级性能下降；作者提出了分析模型与轻量级检测缓解方案，在AMD MI300X系统上实现了最高6%的性能提升和4%的功耗降低。


<details>
  <summary>Details</summary>
Motivation: 现代数据中心广泛采用GPU系统，但其在节点和集群层面存在显著性能波动，尤其影响大语言模型等AI工作负载。现有研究未充分关注热效应对多GPU协同性能的影响，因此需要深入分析并解决这一问题。

Method: 作者首先分析单节点多GPU系统在LLM训练中的内核级性能变化，发现其与计算通信重叠（C3）密切相关；进而提出“Lit Silicon效应”概念，并构建性能与功耗分析模型；最后设计了三种基于功率管理的缓解策略：GPU热设计功耗下的功耗优化、节点级GPU功耗限制下的性能优化，以及节点级CPU功耗转移下的性能优化。

Result: 在两个AMD Instinct™ MI300X GPU系统上，使用两种LLM训练框架进行实验，结果表明所提方法可实现最高6%的性能提升和4%的功耗降低，具备大规模数据中心部署潜力。

Conclusion: Lit Silicon效应是多GPU系统中不可忽视的性能瓶颈，通过引入轻量级的节点级功耗管理机制，可在几乎无额外开销的情况下显著提升系统效率，具有重要的实际应用价值。

Abstract: GPU systems are increasingly powering modern datacenters at scale. Despite being highly performant, GPU systems suffer from performance variation at the node and cluster levels. Such performance variation significantly impacts both high-performance computing and artificial intelligence workloads, such as cutting-edge large language models (LLMs). We analyze the performance of a single-node multi-GPU system running LLM training, and observe that the kernel-level performance variation is highly correlated with concurrent computation communication (C3), a technique to overlap computation and communication across GPUs for performance gains. We then take a further step to reason that thermally induced straggling coupling with C3 impacts performance variation, coined as the Lit Silicon effect. Lit Silicon describes that in a multi-GPU node, thermal imbalance across GPUs introduces node-level straggler GPUs, which in turn slow down the leader GPUs. Lit Silicon leads to node-level performance variation and inefficiency, impacting the entire datacenter from the bottom up. We propose analytical performance and power models for Lit Silicon, to understand the potential system-level gains. We further design simple detection and mitigation techniques to effectively address the Lit Silicon problem, and evaluate three different power management solutions, including power optimization under GPU thermal design power, performance optimization under node-level GPU power capping, and performance optimization under node-level CPU power sloshing. We conduct experiments on two workloads on two AMD InstinctTM MI300X GPU systems under two LLM training frameworks, and observe up to 6% performance and 4% power improvements, potentially saving hundreds of millions of dollars in datacenters. Our solution is almost free lunch and can be effortlessly adopted in datacenters as a new node-level power management layer.

</details>


### [9] [Selection of Supervised Learning-based Sparse Matrix Reordering Algorithms](https://arxiv.org/abs/2511.10180)
*Tao Tang,Youfu Jiang,Yingbo Cui,Jianbin Fang,Peng Zhang,Lin Peng,Chun Huang*

Main category: cs.DC

TL;DR: 本文提出了一种基于监督学习的稀疏矩阵重排序算法选择模型，能根据矩阵特征自动预测最优重排序算法，在Florida数据集上相比仅使用AMD算法平均提速1.45倍，求解时间减少55.37%。


<details>
  <summary>Details</summary>
Motivation: 传统稀疏矩阵重排序算法选择依赖暴力搜索或经验知识，难以适应不同稀疏矩阵结构，缺乏智能化和自适应能力。

Method: 构建监督学习模型，学习稀疏矩阵特征与常用重排序算法之间的关联，实现自动智能的算法选择。

Result: 在Florida稀疏矩阵数据集上的实验表明，该模型能准确预测各类矩阵的最优重排序算法，相比仅使用AMD算法，平均求解时间减少55.37%，平均加速比达1.45。

Conclusion: 基于监督学习的稀疏矩阵重排序算法选择方法能有效提升求解效率，具有良好的自适应性和实用性。

Abstract: Sparse matrix ordering is a vital optimization technique often employed for solving large-scale sparse matrices. Its goal is to minimize the matrix bandwidth by reorganizing its rows and columns, thus enhancing efficiency. Conventional methods for algorithm selection usually depend on brute-force search or empirical knowledge, lacking the ability to adjust to diverse sparse matrix structures.As a result, we have introduced a supervised learning-based model for choosing sparse matrix reordering algorithms. This model grasps the correlation between matrix characteristics and commonly utilized reordering algorithms, facilitating the automated and intelligent selection of the suitable sparse matrix reordering algorithm. Experiments conducted on the Florida sparse matrix dataset reveal that our model can accurately predict the optimal reordering algorithm for various matrices, leading to a 55.37% reduction in solution time compared to solely using the AMD reordering algorithm, with an average speedup ratio of 1.45.

</details>


### [10] [Workload Schedulers -- Genesis, Algorithms and Differences](https://arxiv.org/abs/2511.10258)
*Leszek Sliwko,Vladimir Getov*

Main category: cs.DC

TL;DR: 本文提出了一种对现代工作负载调度器进行分类的新方法，描述了操作系统进程调度器、集群系统作业调度器和大数据调度器三类调度器的演进与特征，并总结了它们之间的异同及设计策略的共通点。


<details>
  <summary>Details</summary>
Motivation: 为了更好地理解和组织现代调度器的发展脉络，作者旨在对不同类型的调度器进行系统性分类和比较，揭示其设计策略的共性与差异。

Method: 通过回顾和分析操作系统进程调度器、集群作业调度器和大数据调度器从早期到现代的演进过程，比较其算法使用与功能特性。

Result: 明确了三类调度器在发展路径、应用场景和算法特性上的区别，并指出其调度策略设计在本地与分布式系统中的相似关注点。

Conclusion: 尽管调度器类型各异，但其调度策略的设计核心在本地和分布式系统中存在显著共性，为未来调度器研究提供统一视角。

Abstract: This paper presents a novel approach to categorization of modern workload schedulers. We provide descriptions of three classes of schedulers: Operating Systems Process Schedulers, Cluster Systems Jobs Schedulers and Big Data Schedulers. We describe their evolution from early adoptions to modern implementations, considering both the use and features of algorithms. In summary, we discuss differences between all presented classes of schedulers and discuss their chronological development. In conclusion we highlight similarities in the focus of scheduling strategies design, applicable to both local and distributed systems.

</details>


### [11] [Scalable Synthesis of distributed LLM workloads through Symbolic Tensor Graphs](https://arxiv.org/abs/2511.10480)
*Changhai Man,Joongun Park,Hanjiang Wu,Huan Xu,Srinivas Sridharan,Tushar Krishna*

Main category: cs.DC

TL;DR: 本文提出了STAGE框架，用于生成高保真度的LLM执行轨迹，以支持大规模分布式系统中对LLM工作负载的建模与优化。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖真实系统采集执行轨迹，但大规模基础设施仅限大型云厂商访问，且现有平台的轨迹难以适配未来更大规模的系统配置，因此需要一种可扩展且表达能力强的建模机制。

Method: 提出Symbolic Tensor grAph GEnerator（STAGE）框架，通过合成高保真执行轨迹，在张量级别上准确建模计算、内存和通信行为，并支持多种并行策略。

Result: STAGE能够生成覆盖超过32K GPU的高保真LLM执行轨迹，同时保持张量级精度，并支持广泛的LLM架构与系统配置探索。

Conclusion: STAGE为分布式机器学习系统研究提供了可扩展、高保真的建模工具，将公开发布以促进相关研究。

Abstract: Optimizing the performance of large language models (LLMs) on large-scale AI training and inference systems requires a scalable and expressive mechanism to model distributed workload execution. Such modeling is essential for pre-deployment system-level optimizations (e.g., parallelization strategies) and design-space explorations. While recent efforts have proposed collecting execution traces from real systems, access to large-scale infrastructure remains limited to major cloud providers. Moreover, traces obtained from existing platforms cannot be easily adapted to study future larger-scale system configurations. We introduce Symbolic Tensor grAph GEnerator(STAGE), a framework that synthesizes high-fidelity execution traces to accurately model LLM workloads. STAGE supports a comprehensive set of parallelization strategies, allowing users to systematically explore a wide spectrum of LLM architectures and system configurations. STAGE demonstrates its scalability by synthesizing high-fidelity LLM traces spanning over 32K GPUs, while preserving tensor-level accuracy in compute, memory, and communication. STAGE will be publicly available to facilitate further research in distributed machine learning systems.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [12] [Evaluating Software Process Models for Multi-Agent Class-Level Code Generation](https://arxiv.org/abs/2511.09794)
*Wasique Islam Shafin,Md Nakhla Rafi,Zhenhao Li,Tse-Hsun Chen*

Main category: cs.SE

TL;DR: 该研究探讨了多智能体大语言模型（LLM）在类级代码生成中如何受软件开发流程结构和角色分工影响，发现瀑布式协作虽提升代码可维护性，但通常降低功能正确性，并改变错误类型分布。


<details>
  <summary>Details</summary>
Motivation: 现有研究多关注单智能体、函数级代码生成，缺乏对多智能体协作下软件过程结构（如需求、设计、实现、测试阶段）如何影响LLM代码生成质量的理解。

Method: 在ClassEval基准的100个Python任务上，使用GPT-4o-mini、DeepSeek-Chat和Claude-3.5-Haiku三个LLM，模拟瀑布式开发流程（需求、设计、实现、测试）进行多智能体协作实验。

Result: 瀑布式多智能体工作流提升了代码的结构性与可维护性，但多数模型功能正确性显著下降（GPT-4o-mini -37.8%，DeepSeek-Chat -39.8%），仅Claude-3.5-Haiku提升9.5%；错误类型从结构性缺失转向语义与验证错误；测试阶段对结果影响最大。

Conclusion: 软件过程结构深刻影响多智能体LLM的推理、协作与失败模式，在流程纪律性与灵活问题解决之间存在固有权衡，需谨慎设计多智能体协作机制以平衡代码质量维度。

Abstract: Modern software systems require code that is not only functional but also maintainable and well-structured. Although Large Language Models (LLMs) are increasingly used to automate software development, most studies focus on isolated, single-agent function-level generation. This work examines how process structure and role specialization shape multi-agent LLM workflows for class-level code generation. We simulate a Waterfall-style development cycle covering Requirement, Design, Implementation, and Testing using three LLMs (GPT-4o-mini, DeepSeek-Chat, and Claude-3.5-Haiku) on 100 Python tasks from the ClassEval benchmark. Our findings show that multi-agent workflows reorganize, rather than consistently enhance, model performance. Waterfall-style collaboration produces cleaner and more maintainable code but often reduces functional correctness (-37.8\% for GPT-4o-mini and -39.8\% for DeepSeek-Chat), with Claude-3.5-Haiku as a notable exception (+9.5\%). Importantly, process constraints shift failure characteristics: structural issues such as missing code decrease, while semantic and validation errors become more frequent. Among all stages, Testing exerts the strongest influence by improving verification coverage but also introducing new reasoning failures, whereas Requirement and Design have comparatively modest effects. Overall, this study provides empirical evidence that software process structure fundamentally alters how LLMs reason, collaborate, and fail, revealing inherent trade-offs between rigid workflow discipline and flexible problem-solving in multi-agent code generation.

</details>


### [13] [EnvTrace: Simulation-Based Semantic Evaluation of LLM Code via Execution Trace Alignment -- Demonstrated at Synchrotron Beamlines](https://arxiv.org/abs/2511.09964)
*Noah van der Vleuten,Anthony Flores,Shray Mathur,Max Rakitin,Thomas Hopkins,Kevin G. Yager,Esther H. R. Tsai*

Main category: cs.SE

TL;DR: 本文提出EnvTrace方法，通过基于仿真的执行轨迹评估语义代码等价性，用于评测大语言模型在仪器控制任务中的表现，并结合数字孪生实现安全高效的控制代码验证。


<details>
  <summary>Details</summary>
Motivation: 标准的无状态算法基准无法充分评估大语言模型在物理系统控制任务中的行为，因此需要一种能捕捉系统动态行为的评测方法。

Method: 提出EnvTrace方法，利用数字孪生仿真环境对模型生成的控制代码执行轨迹进行比对，通过轨迹对齐从多个行为维度评估功能正确性。

Result: 对30多个大语言模型的评估表明，部分顶尖模型在快速生成仪器控制代码方面已接近人类水平。

Conclusion: 该研究为大语言模型与数字孪生协同构建自主具身智能系统奠定了基础，其中大模型提供直观控制与智能编排，数字孪生提供高保真、安全的验证环境。

Abstract: Evaluating large language models (LLMs) for instrument control requires methods that go beyond standard, stateless algorithmic benchmarks, since the behavior of physical systems cannot be fully captured by unit tests alone. Here we introduce EnvTrace, a simulation-based method that evaluates execution traces to assess semantic code equivalence. EnvTrace is demonstrated with a beamline control-logic digital twin to facilitate the evaluation of instrument control code, with the digital twin itself also enabling the pre-execution validation of live experiments. Over 30 LLMs were evaluated using trace alignment to generate a multi-faceted score for functional correctness across key behavioral dimensions, showing that many top-tier models can approach human-level performance in rapid control-code generation. This is a first step toward a broader vision where LLMs and digital twins work symbiotically: LLMs providing intuitive control and agentic orchestration, and digital twins offering safe and high-fidelity environments, paving the way towards autonomous embodied AI.

</details>


### [14] [Continuous Benchmark Generation for Evaluating Enterprise-scale LLM Agents](https://arxiv.org/abs/2511.10049)
*Divyanshu Saxena,Rishikesh Maurya,Xiaoxuan Ou,Gagan Somashekar,Shachee Mishra Gupta,Arun Iyer,Yu Kang,Chetan Bansal,Aditya Akella,Saravan Rajmohan*

Main category: cs.SE

TL;DR: 本文提出一种动态生成基准的方法，用于评估在企业环境中持续演化的AI智能体，通过少量半结构化文档和大语言模型自动生成可维护的评估基准。


<details>
  <summary>Details</summary>
Motivation: 传统固定基准在企业级AI智能体评估中存在不足，难以应对服务与需求的持续变化及真实样本稀缺的问题。

Method: 利用开发者提供的少量半结构化文档表达高层意图，结合先进大语言模型（LLM）自动生成适应需求变化的评估基准。

Result: 该方法构建了一个可维护的评估框架，在大型公共企业的服务迁移案例中实现了对AI智能体性能的快速反馈和针对性改进。

Conclusion: 所提出的动态基准生成流程能有效支持企业级AI智能体的鲁棒评估与持续优化。

Abstract: The rapid adoption of AI agents across domains has made systematic evaluation crucial for ensuring their usefulness and successful production deployment. Evaluation of AI agents typically involves using a fixed set of benchmarks and computing multiple evaluation metrics for the agent. While sufficient for simple coding tasks, these benchmarks fall short for enterprise-scale agents, where services and requirements evolve continuously and ground-truth examples are sparse. We propose a process of benchmark generation that helps evolve the benchmarks as the requirements change and perform robust evaluation of evolving AI agents. We instantiate this approach for a case study of service migration from one deployment platform to another at a large public enterprise. Our approach relies on semi-structured documents where developers express the high-level intent, and uses state-of-the-art LLMs to generate benchmarks from just a small number of such documents. Overall, this process results in a maintainable evaluation framework, enabling rapid feedback on agent performance and facilitating targeted improvements.

</details>


### [15] [Quality Assurance of LLM-generated Code: Addressing Non-Functional Quality Characteristics](https://arxiv.org/abs/2511.10271)
*Xin Sun,Daniel Ståhl,Kristian Sandahl,Christoph Kessler*

Main category: cs.SE

TL;DR: 该研究基于ISO/IEC 25010质量模型，通过文献综述、行业研讨会和实证分析，揭示了大语言模型（LLM）生成代码在非功能性质量（如安全性、可维护性和性能效率）方面的现状与挑战，发现学术关注、行业需求与模型表现之间存在错位，并呼吁将质量保障机制整合进LLM代码生成流程。


<details>
  <summary>Details</summary>
Motivation: 当前对大语言模型生成代码的评估主要关注功能正确性（是否通过测试），而忽视其非功能性质量（如可维护性、安全性、性能等）。为系统理解并评估这些质量维度，作者结合学术研究与工业实践展开综合调查。

Method: 研究采用三种互补方法：（1）系统综述108篇相关论文；（2）举办两场涵盖多家机构从业者的行业研讨会；（3）使用三个LLM对真实软件问题生成补丁，并对其在安全性、可维护性和性能效率方面进行实证分析。

Result: 文献中学术界主要关注安全性和性能效率，而可维护性等维度被忽视；工业界则更重视可维护性与可读性，并担忧生成代码加剧技术债。实证结果表明，LLM生成的功能正确补丁在某一质量维度上的提升常以牺牲其他维度为代价，且不同模型与优化策略在运行时和内存表现上差异显著。

Conclusion: 学术关注点、工业优先级与LLM实际表现之间存在明显不匹配，亟需在LLM代码生成流程中嵌入质量保障机制，以确保生成代码不仅功能正确，而且具备高质量的非功能性属性。

Abstract: In recent years, LLMs have been widely integrated into software engineering workflows, supporting tasks like code generation. However, while these models often generate functionally correct outputs, we still lack a systematic understanding and evaluation of their non-functional qualities. Existing studies focus mainly on whether generated code passes the tests rather than whether it passes with quality. Guided by the ISO/IEC 25010 quality model, this study conducted three complementary investigations: a systematic review of 108 papers, two industry workshops with practitioners from multiple organizations, and an empirical analysis of patching real-world software issues using three LLMs. Motivated by insights from both the literature and practitioners, the empirical study examined the quality of generated patches on security, maintainability, and performance efficiency. Across the literature, we found that security and performance efficiency dominate academic attention, while maintainability and other qualities are understudied. In contrast, industry experts prioritize maintainability and readability, warning that generated code may accelerate the accumulation of technical debt. In our evaluation of functionally correct patches generated by three LLMs, improvements in one quality dimension often come at the cost of others. Runtime and memory results further show high variance across models and optimization strategies. Overall, our findings reveal a mismatch between academic focus, industry priorities, and model performance, highlighting the urgent need to integrate quality assurance mechanisms into LLM code generation pipelines to ensure that future generated code not only passes tests but truly passes with quality.

</details>


### [16] [A Large-Scale Collection Of (Non-)Actionable Static Code Analysis Reports](https://arxiv.org/abs/2511.10323)
*Dávid Kószó,Tamás Aladics,Rudolf Ferenc,Péter Hegedűs*

Main category: cs.SE

TL;DR: 本文提出了一种新方法，用于收集和分类静态代码分析（SCA）警告，并构建了一个包含超过100万条Java代码警告的大规模数据集NASCAR，以区分可操作与不可操作的警告，缓解“警报疲劳”问题。


<details>
  <summary>Details</summary>
Motivation: 静态代码分析工具常产生大量不可操作的警告，导致开发者出现“警报疲劳”，而现有研究缺乏大规模标注数据集（尤其是Java），阻碍了相关机器学习模型的发展。

Method: 提出一种新颖的方法论来收集和分类SCA警告，区分其是否可操作，并基于此方法构建名为NASCAR的大规模Java警告数据集。

Result: 成功构建并公开了一个包含超100万条Java SCA警告的数据集NASCAR，同时开源了相关生成工具，支持后续研究。

Conclusion: 该工作填补了Java领域SCA警告数据集的空白，有助于提升SCA工具的准确性和可用性，并为缓解警报疲劳提供基础支持。

Abstract: Static Code Analysis (SCA) tools, while invaluable for identifying potential coding problems, functional bugs, or vulnerabilities, often generate an overwhelming number of warnings, many of which are non-actionable. This overload of alerts leads to ``alert fatigue'', a phenomenon where developers become desensitized to warnings, potentially overlooking critical issues and ultimately hindering productivity and code quality. Analyzing these warnings and training machine learning models to identify and filter them requires substantial datasets, which are currently scarce, particularly for Java. This scarcity impedes efforts to improve the accuracy and usability of SCA tools and mitigate the effects of alert fatigue. In this paper, we address this gap by introducing a novel methodology for collecting and categorizing SCA warnings, effectively distinguishing actionable from non-actionable ones. We further leverage this methodology to generate a large-scale dataset of over 1 million entries of Java source code warnings, named NASCAR: (Non-)Actionable Static Code Analysis Reports. To facilitate follow-up research in this domain, we make both the dataset and the tools used to generate it publicly available.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [17] [Behavior Modeling for Training-free Building of Private Domain Multi Agent System](https://arxiv.org/abs/2511.10283)
*Won Ik Cho,Woonghee Han,Kyung Seo Ki,Young Min Kim*

Main category: cs.MA

TL;DR: 本文提出了一种无需训练和数据生成的私有领域多智能体对话系统框架，通过行为建模与文档驱动的方式，实现对私有工具和动态上下文的可扩展适配。


<details>
  <summary>Details</summary>
Motivation: 在私有领域部署基于大语言模型的智能体系统面临工具格式异构、领域术语复杂、API访问受限及治理困难等挑战，而传统依赖微调和合成数据的方法成本高、泛化差且易损害通用性能。

Method: 该框架采用行为建模与文档驱动策略，仅包含协调器、工具调用智能体和通用聊天智能体，通过结构化工具规范和领域指导指令集成工具，避免了持续训练和数据生成。

Result: 该方法支持轻量级多智能体部署、利用API规范作为检索资源，并可生成用于评估的合成对话，在私有对话生态中可持续地对齐智能体行为与领域知识。

Conclusion: 所提框架为私有领域多智能体系统提供了一种高效、可扩展且无需训练的解决方案，有效应对了工具集成与领域适配的现实挑战。

Abstract: The rise of agentic systems that combine orchestration, tool use, and conversational capabilities, has been more visible by the recent advent of large language models (LLMs). While open-domain frameworks exist, applying them in private domains remains difficult due to heterogeneous tool formats, domain-specific jargon, restricted accessibility of APIs, and complex governance. Conventional solutions, such as fine-tuning on synthetic dialogue data, are burdensome and brittle under domain shifts, and risk degrading general performance. In this light, we introduce a framework for private-domain multi-agent conversational systems that avoids training and data generation by adopting behavior modeling and documentation. Our design simply assumes an orchestrator, a tool-calling agent, and a general chat agent, with tool integration defined through structured specifications and domain-informed instructions. This approach enables scalable adaptation to private tools and evolving contexts without continual retraining. The framework supports practical use cases, including lightweight deployment of multi-agent systems, leveraging API specifications as retrieval resources, and generating synthetic dialogue for evaluation -- providing a sustainable method for aligning agent behavior with domain expertise in private conversational ecosystems.

</details>
