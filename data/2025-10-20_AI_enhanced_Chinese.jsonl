{"id": "2510.15237", "categories": ["cs.PF"], "pdf": "https://arxiv.org/pdf/2510.15237", "abs": "https://arxiv.org/abs/2510.15237", "authors": ["Yee Lam Elim Thompson", "Jonathan Fergus", "Jonathan Chung", "Jana G. Delfino", "Weijie Chen", "Gary M. Levine", "Frank W. Samuelson"], "title": "Impact of AI-Triage on Radiologist Report Turnaround Time: Real-World Time-Savings and Insights from Model Predictions", "comment": null, "summary": "Objective: To quantify the impact of workflow parameters on time-savings in\nreport turnaround time (TAT) due to an AI-triage device that prioritized\npulmonary embolism (PE) in chest CT pulmonary angiography (CTPA) exams.\nMethods: This retrospective study analyzed 11252 adult CTPA exams conducted for\nsuspected PE at a single tertiary academic medical center. Data was divided\ninto two periods: pre-AI and post-AI. For PE-positive exams, TAT - defined as\nthe duration from patient scan completion to the first preliminary report\ncompletion - was compared between the two periods. Time-savings were reported\nseparately for work-hour and off-hour cohorts. To characterize radiologist\nworkflow, 527234 records were retrieved from the PACS and workflow parameters\nsuch as exam inter-arrival time and radiologist read-time extracted. These\nparameters were input into a computational model to predict time-savings\nfollowing deployment of an AI-triage device and to study the impact of workflow\nparameters. Results: The pre-AI dataset included 4694 chest CTPA exams with\n13.3% being PE-positive. The post-AI dataset comprised 6558 exams with 16.2%\nbeing PE-positive. The mean TAT for pre-AI and post-AI during work hours are\n68.9 [95% CI\" 55.0, 82.8] and 46.7 [38.1, 55.2] minutes respectively, and those\nduring off-hours are 44.8 [33.7, 55.9] and 42.0 [33.6, 50.3] minutes.\nClinically-observed time-savings during work hours (22.2 [95% CI: 5.85, 38.6]\nminutes) were significant (p=0.004), while off-hour (2.82 [-11.1, 16.7]\nminutes) were not (p=0.345). Observed time-savings aligned with model\npredictions (29.6 [95% range: 23.2, 38.1] minutes for work hours; 2.10 [1.76,\n2.58] minutes for off-hours). Discussion: Consideration and quantification of\nclinical workflow contribute to an accurate assessment of the expected\ntime-savings in TAT following deployment of an AI-triage device.", "AI": {"tldr": "\u8be5\u7814\u7a76\u901a\u8fc7\u56de\u987e\u6027\u5206\u6790\u8bc4\u4f30AI\u5206\u8bca\u7cfb\u7edf\u5728\u80ba\u6813\u585e\uff08PE\uff09\u80f8\u90e8CT\u80ba\u52a8\u8109\u9020\u5f71\uff08CTPA\uff09\u68c0\u67e5\u4e2d\u5bf9\u62a5\u544a\u5468\u8f6c\u65f6\u95f4\uff08TAT\uff09\u7684\u8282\u7701\u6548\u679c\uff0c\u5e76\u7ed3\u5408\u5de5\u4f5c\u6d41\u53c2\u6570\u5efa\u7acb\u8ba1\u7b97\u6a21\u578b\u9884\u6d4b\u65f6\u95f4\u8282\u7701\uff0c\u53d1\u73b0\u5de5\u4f5c\u65f6\u95f4\u5185\u7684TAT\u663e\u8457\u7f29\u77ed\uff0c\u800c\u6a21\u578b\u9884\u6d4b\u4e0e\u5b9e\u9645\u89c2\u5bdf\u7ed3\u679c\u4e00\u81f4\u3002", "motivation": "\u51c6\u786e\u8bc4\u4f30AI\u5206\u8bca\u8bbe\u5907\u5728\u4e34\u5e8a\u5b9e\u8df5\u4e2d\u5bf9\u62a5\u544a\u5468\u8f6c\u65f6\u95f4\u7684\u5b9e\u9645\u5f71\u54cd\uff0c\u9700\u8003\u8651\u5e76\u91cf\u5316\u653e\u5c04\u79d1\u533b\u751f\u7684\u5de5\u4f5c\u6d41\u53c2\u6570\uff0c\u4ee5\u907f\u514d\u9ad8\u4f30\u6216\u4f4e\u4f30AI\u5e26\u6765\u7684\u6548\u76ca\u3002", "method": "\u56de\u987e\u6027\u5206\u679011252\u4f8b\u7591\u4f3cPE\u7684\u6210\u4ebaCTPA\u68c0\u67e5\uff0c\u5206\u4e3aAI\u90e8\u7f72\u524d\u540e\u4e24\u4e2a\u9636\u6bb5\uff1b\u63d0\u53d6PACS\u7cfb\u7edf\u4e2d527234\u6761\u8bb0\u5f55\u4ee5\u83b7\u53d6\u5de5\u4f5c\u6d41\u53c2\u6570\uff08\u5982\u68c0\u67e5\u5230\u8fbe\u95f4\u9694\u3001\u8bfb\u7247\u65f6\u95f4\u7b49\uff09\uff0c\u5e76\u6784\u5efa\u8ba1\u7b97\u6a21\u578b\u9884\u6d4bAI\u5206\u8bca\u540e\u7684\u65f6\u95f4\u8282\u7701\u6548\u679c\uff1b\u6bd4\u8f83PE\u9633\u6027\u75c5\u4f8b\u5728\u5de5\u4f5c\u65f6\u95f4\u548c\u975e\u5de5\u4f5c\u65f6\u95f4\u7684TAT\u5dee\u5f02\u3002", "result": "AI\u90e8\u7f72\u540e\uff0c\u5de5\u4f5c\u65f6\u95f4\u5185PE\u9633\u6027\u75c5\u4f8b\u7684\u5e73\u5747TAT\u4ece68.9\u5206\u949f\u964d\u81f346.7\u5206\u949f\uff0c\u8282\u770122.2\u5206\u949f\uff08p=0.004\uff09\uff0c\u5177\u6709\u7edf\u8ba1\u5b66\u610f\u4e49\uff1b\u975e\u5de5\u4f5c\u65f6\u95f4\u8282\u77012.82\u5206\u949f\uff08p=0.345\uff09\uff0c\u4e0d\u663e\u8457\uff1b\u6a21\u578b\u9884\u6d4b\u7ed3\u679c\u4e0e\u5b9e\u9645\u89c2\u5bdf\u9ad8\u5ea6\u4e00\u81f4\u3002", "conclusion": "\u5c06\u4e34\u5e8a\u5de5\u4f5c\u6d41\u53c2\u6570\u7eb3\u5165\u8bc4\u4f30\u6846\u67b6\uff0c\u6709\u52a9\u4e8e\u66f4\u51c6\u786e\u5730\u9884\u6d4b\u548c\u7406\u89e3AI\u5206\u8bca\u7cfb\u7edf\u5728\u771f\u5b9e\u4e34\u5e8a\u73af\u5883\u4e2d\u7684\u65f6\u95f4\u8282\u7701\u6548\u679c\uff0c\u5c24\u5176\u5728\u5de5\u4f5c\u65f6\u95f4\u5185\u7684\u6548\u76ca\u66f4\u4e3a\u663e\u8457\u3002"}}
{"id": "2510.15744", "categories": ["cs.AR", "cs.PF"], "pdf": "https://arxiv.org/pdf/2510.15744", "abs": "https://arxiv.org/abs/2510.15744", "authors": ["Haocong Luo", "Ataberk Olgun", "Maria Makeenkova", "F. Nisa Bostanci", "Geraldo F. Oliveira", "A. Giray Yaglikci", "Onur Mutlu"], "title": "Cleaning up the Mess", "comment": null, "summary": "A MICRO 2024 best paper runner-up publication (the Mess paper) with all three\nartifact badges awarded (including \"Reproducible\") proposes a new benchmark to\nevaluate real and simulated memory system performance. In this paper, we\ndemonstrate that the Ramulator 2.0 simulation results reported in the Mess\npaper are incorrect and, at the time of the publication of the Mess paper,\nirreproducible. We find that the authors of Mess paper made multiple trivial\nhuman errors in both the configuration and usage of the simulators. We show\nthat by correctly configuring Ramulator 2.0, Ramulator 2.0's simulated memory\nsystem performance actually resembles real system characteristics well, and\nthus a key claimed contribution of the Mess paper is factually incorrect. We\nalso identify that the DAMOV simulation results in the Mess paper use wrong\nsimulation statistics that are unrelated to the simulated DRAM performance.\nMoreover, the Mess paper's artifact repository lacks the necessary sources to\nfully reproduce all the Mess paper's results.\n  Our work corrects the Mess paper's errors regarding Ramulator 2.0 and\nidentifies important issues in the Mess paper's memory simulator evaluation\nmethodology. We emphasize the importance of both carefully and rigorously\nvalidating simulation results and contacting simulator authors and developers,\nin true open source spirit, to ensure these simulators are used with correct\nconfigurations and as intended. We encourage the computer architecture\ncommunity to correct the Mess paper's errors. This is necessary to prevent the\npropagation of inaccurate and misleading results, and to maintain the\nreliability of the scientific record. Our investigation also opens up questions\nabout the integrity of the review and artifact evaluation processes. To aid\nfuture work, our source code and scripts are openly available at https:\n//github.com/CMU-SAFARI/ramulator2/tree/mess.", "AI": {"tldr": "\u8be5\u8bba\u6587\u6307\u51faMICRO 2024\u6700\u4f73\u8bba\u6587\u4e9a\u519b\u201cMess\u201d\u5728\u4f7f\u7528Ramulator 2.0\u548cDAMOV\u6a21\u62df\u5668\u65f6\u5b58\u5728\u914d\u7f6e\u548c\u4f7f\u7528\u9519\u8bef\uff0c\u5bfc\u81f4\u5176\u5173\u952e\u7ed3\u8bba\u4e0d\u6210\u7acb\uff0c\u5e76\u5f3a\u8c03\u4e86\u4e25\u8c28\u9a8c\u8bc1\u6a21\u62df\u7ed3\u679c\u53ca\u4e0e\u6a21\u62df\u5668\u5f00\u53d1\u8005\u534f\u4f5c\u7684\u91cd\u8981\u6027\u3002", "motivation": "\u7ea0\u6b63Mess\u8bba\u6587\u4e2d\u5173\u4e8e\u5185\u5b58\u7cfb\u7edf\u6a21\u62df\u5668\uff08Ramulator 2.0\u548cDAMOV\uff09\u7684\u9519\u8bef\u4f7f\u7528\uff0c\u9632\u6b62\u9519\u8bef\u7ed3\u8bba\u5728\u5b66\u672f\u754c\u4f20\u64ad\uff0c\u7ef4\u62a4\u79d1\u7814\u8bb0\u5f55\u7684\u53ef\u9760\u6027\uff0c\u5e76\u53cd\u601d\u8bc4\u5ba1\u4e0e\u6210\u679c\u590d\u73b0\u8bc4\u4f30\u6d41\u7a0b\u7684\u5b8c\u6574\u6027\u3002", "method": "\u901a\u8fc7\u590d\u73b0Mess\u8bba\u6587\u7684\u5b9e\u9a8c\uff0c\u68c0\u67e5\u5176\u6a21\u62df\u5668\u914d\u7f6e\u4e0e\u4f7f\u7528\u65b9\u5f0f\uff0c\u53d1\u73b0\u5176\u5728Ramulator 2.0\u548cDAMOV\u4e2d\u7684\u914d\u7f6e\u9519\u8bef\u548c\u7edf\u8ba1\u6307\u6807\u8bef\u7528\uff0c\u5e76\u57fa\u4e8e\u6b63\u786e\u914d\u7f6e\u91cd\u65b0\u8fd0\u884c\u5b9e\u9a8c\u8fdb\u884c\u5bf9\u6bd4\u9a8c\u8bc1\u3002", "result": "\u6b63\u786e\u914d\u7f6eRamulator 2.0\u540e\uff0c\u5176\u6a21\u62df\u7ed3\u679c\u80fd\u8f83\u597d\u53cd\u6620\u771f\u5b9e\u7cfb\u7edf\u6027\u80fd\uff0c\u63a8\u7ffb\u4e86Mess\u8bba\u6587\u7684\u5173\u952e\u4e3b\u5f20\uff1b\u540c\u65f6\u53d1\u73b0DAMOV\u7ed3\u679c\u4f7f\u7528\u4e86\u4e0eDRAM\u6027\u80fd\u65e0\u5173\u7684\u9519\u8bef\u7edf\u8ba1\u91cf\uff0c\u4e14Mess\u8bba\u6587\u7684\u6210\u679c\u4ed3\u5e93\u7f3a\u5c11\u5b8c\u6574\u590d\u73b0\u6240\u9700\u4ee3\u7801\u3002", "conclusion": "Mess\u8bba\u6587\u5173\u4e8e\u5185\u5b58\u6a21\u62df\u5668\u6027\u80fd\u8bc4\u4f30\u7684\u7ed3\u8bba\u56e0\u4eba\u4e3a\u914d\u7f6e\u9519\u8bef\u800c\u4e0d\u6210\u7acb\uff1b\u7814\u7a76\u5f3a\u8c03\u4e86\u4e25\u683c\u9a8c\u8bc1\u6a21\u62df\u7ed3\u679c\u3001\u4e0e\u5de5\u5177\u5f00\u53d1\u8005\u534f\u4f5c\u7684\u91cd\u8981\u6027\uff0c\u5e76\u547c\u5401\u793e\u533a\u7ea0\u6b63\u9519\u8bef\uff0c\u4ee5\u7ef4\u62a4\u79d1\u7814\u53ef\u4fe1\u5ea6\u3002"}}
{"id": "2510.15004", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.15004", "abs": "https://arxiv.org/abs/2510.15004", "authors": ["Zhiming Zhang", "Qingfu Zhu", "Xianzhen Luo", "Yixuan Wang", "Bohan Li", "Wanxiang Che"], "title": "Automated Snippet-Alignment Data Augmentation for Code Translation", "comment": null, "summary": "Code translation aims to translate the code from its source language to the\ntarget language and is used in various software development scenarios. Recent\ndevelopments in Large Language Models (LLMs) have showcased their capabilities\nin code translation, and parallel corpora play a crucial role in training\nmodels for code translation. Parallel corpora can be categorized into\nprogram-alignment (PA) and snippet-alignment (SA) data. Although PA data has\ncomplete context and is suitable for semantic alignment learning, it may not\nprovide adequate fine-grained training signals due to its extended length,\nwhile the brevity of SA data enables more fine-grained alignment learning. Due\nto limited parallel corpora, researchers explore several augmentation methods\nfor code translation. Previous studies mainly focus on augmenting PA data. In\nthis paper, we propose a data augmentation method that leverages LLMs to\ngenerate SA data automatically. To fully leverage both PA data and SA data, we\nexplore a simple yet effective two-stage training strategy, which consistently\nenhances model performance compared to fine-tuning solely on PA data.\nExperiments on TransCoder-test demonstrate that our augmented SA data combined\nwith the two-stage training approach yields consistent improvements over the\nbaseline, achieving a maximum gain of 3.78% on pass@k.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\u81ea\u52a8\u751f\u6210\u4ee3\u7801\u7247\u6bb5\u5bf9\u9f50\uff08SA\uff09\u6570\u636e\u7684\u6570\u636e\u589e\u5f3a\u65b9\u6cd5\uff0c\u5e76\u7ed3\u5408\u7a0b\u5e8f\u5bf9\u9f50\uff08PA\uff09\u6570\u636e\uff0c\u91c7\u7528\u4e24\u9636\u6bb5\u8bad\u7ec3\u7b56\u7565\u63d0\u5347\u4ee3\u7801\u7ffb\u8bd1\u6027\u80fd\uff0c\u5728TransCoder-test\u4e0a\u6700\u9ad8\u63d0\u53473.78%\u7684pass@k\u6307\u6807\u3002", "motivation": "\u73b0\u6709\u4ee3\u7801\u7ffb\u8bd1\u7814\u7a76\u4e3b\u8981\u4f9d\u8d56\u7a0b\u5e8f\u5bf9\u9f50\uff08PA\uff09\u6570\u636e\u8fdb\u884c\u6570\u636e\u589e\u5f3a\uff0c\u4f46PA\u6570\u636e\u56e0\u957f\u5ea6\u8f83\u957f\u96be\u4ee5\u63d0\u4f9b\u7ec6\u7c92\u5ea6\u7684\u8bad\u7ec3\u4fe1\u53f7\uff1b\u800c\u7247\u6bb5\u5bf9\u9f50\uff08SA\uff09\u6570\u636e\u867d\u9002\u5408\u7ec6\u7c92\u5ea6\u5b66\u4e60\uff0c\u5374\u56e0\u7a00\u7f3a\u800c\u672a\u88ab\u5145\u5206\u5229\u7528\u3002\u56e0\u6b64\uff0c\u4f5c\u8005\u5e0c\u671b\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\u81ea\u52a8\u751f\u6210SA\u6570\u636e\uff0c\u5e76\u7ed3\u5408PA\u4e0eSA\u6570\u636e\u63d0\u5347\u6a21\u578b\u6027\u80fd\u3002", "method": "\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u81ea\u52a8\u751f\u6210\u7247\u6bb5\u5bf9\u9f50\uff08SA\uff09\u7684\u5e73\u884c\u8bed\u6599\uff0c\u5e76\u8bbe\u8ba1\u4e00\u79cd\u4e24\u9636\u6bb5\u8bad\u7ec3\u7b56\u7565\uff1a\u5148\u5728PA\u6570\u636e\u4e0a\u9884\u8bad\u7ec3\uff0c\u518d\u5728\u589e\u5f3a\u540e\u7684SA\u6570\u636e\u4e0a\u5fae\u8c03\uff0c\u4ee5\u517c\u987e\u8bed\u4e49\u5bf9\u9f50\u4e0e\u7ec6\u7c92\u5ea6\u5b66\u4e60\u3002", "result": "\u5728TransCoder-test\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684SA\u6570\u636e\u589e\u5f3a\u65b9\u6cd5\u7ed3\u5408\u4e24\u9636\u6bb5\u8bad\u7ec3\u7b56\u7565\uff0c\u76f8\u6bd4\u4ec5\u4f7f\u7528PA\u6570\u636e\u5fae\u8c03\u7684\u57fa\u7ebf\u6a21\u578b\uff0c\u5728pass@k\u6307\u6807\u4e0a\u6700\u9ad8\u63d0\u53473.78%\u3002", "conclusion": "\u7ed3\u5408\u81ea\u52a8\u751f\u6210\u7684SA\u6570\u636e\u4e0e\u4e24\u9636\u6bb5\u8bad\u7ec3\u7b56\u7565\u80fd\u6709\u6548\u63d0\u5347\u4ee3\u7801\u7ffb\u8bd1\u6a21\u578b\u7684\u6027\u80fd\uff0c\u9a8c\u8bc1\u4e86\u7ec6\u7c92\u5ea6\u5bf9\u9f50\u6570\u636e\u5728\u4ee3\u7801\u7ffb\u8bd1\u4efb\u52a1\u4e2d\u7684\u4ef7\u503c\u3002"}}
{"id": "2510.15095", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2510.15095", "abs": "https://arxiv.org/abs/2510.15095", "authors": ["Md Sabbir Hossain Polak", "David Troendle", "Byunghyun Jang"], "title": "Hive Hash Table: A Warp-Cooperative, Dynamically Resizable Hash Table for GPUs", "comment": null, "summary": "Hash tables are essential building blocks in data-intensive applications, yet\nexisting GPU implementations often struggle with concurrent updates, high load\nfactors, and irregular memory access patterns. We present Hive hash table, a\nhigh-performance, warp-cooperative and dynamically resizable GPU hash table\nthat adapts to varying workloads without global rehashing.\n  Hive hash table makes three key contributions. First, a cache-aligned packed\nbucket layout stores key-value pairs as 64-bit words, enabling coalesced memory\naccess and atomic updates via single-CAS operations. Second, warp-synchronous\nconcurrency protocols - Warp-Aggregated-Bitmask-Claim (WABC) and\nWarp-Cooperative Match-and-Elect (WCME) - reduce contention to one atomic\noperation per warp while ensuring lock-free progress. Third, a\nload-factor-aware dynamic resizing strategy expands or contracts capacity in\nwarp-parallel K-bucket batches using linear hashing, maintaining balanced\noccupancy. To handle insertions under heavy contention, Hive hash table employs\na four-step strategy: replace, claim-and-commit, bounded cuckoo eviction, and\noverflow-stash fallback. This design provides lock-free fast paths and bounded\nrecovery cost under contention determined by a fixed eviction depth, while\neliminating ABA hazards during concurrent updates.\n  Experimental evaluation on an NVIDIA RTX 4090 shows Hive hash table sustains\nload factors up to 95% while delivering 1.5-2x higher throughput than\nstate-of-the-art GPU hash tables (Slab-Hash, DyCuckoo, WarpCore) under mixed\ninsert-delete-lookup workloads. On balanced workload, Hive hash table reaches\n3.5 billion updates/s and nearly 4 billion lookups/s, demonstrating scalability\nand efficiency for GPU-accelerated data processing.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86Hive\u54c8\u5e0c\u8868\uff0c\u4e00\u79cd\u9ad8\u6027\u80fd\u3001\u7ebf\u7a0b\u675f\u534f\u540c\u4e14\u52a8\u6001\u53ef\u8c03\u6574\u5927\u5c0f\u7684GPU\u54c8\u5e0c\u8868\uff0c\u5728\u9ad8\u8d1f\u8f7d\u56e0\u5b50\u548c\u9ad8\u5e76\u53d1\u4e0b\u4ecd\u80fd\u4fdd\u6301\u9ad8\u6548\u6027\u80fd\uff0c\u65e0\u9700\u5168\u5c40\u91cd\u54c8\u5e0c\u3002", "motivation": "\u73b0\u6709GPU\u54c8\u5e0c\u8868\u5728\u5e76\u53d1\u66f4\u65b0\u3001\u9ad8\u8d1f\u8f7d\u56e0\u5b50\u548c\u4e0d\u89c4\u5219\u5185\u5b58\u8bbf\u95ee\u65b9\u9762\u5b58\u5728\u6027\u80fd\u74f6\u9888\uff0c\u96be\u4ee5\u9002\u5e94\u591a\u53d8\u7684\u5de5\u4f5c\u8d1f\u8f7d\u3002", "method": "Hive\u54c8\u5e0c\u8868\u91c7\u7528\u4e09\u9879\u5173\u952e\u6280\u672f\uff1a1\uff09\u7f13\u5b58\u5bf9\u9f50\u7684\u7d27\u51d1\u6876\u5e03\u5c40\uff0c\u4ee564\u4f4d\u5b57\u5b58\u50a8\u952e\u503c\u5bf9\uff0c\u652f\u6301\u5408\u5e76\u5185\u5b58\u8bbf\u95ee\u548c\u5355CAS\u539f\u5b50\u66f4\u65b0\uff1b2\uff09\u7ebf\u7a0b\u675f\u540c\u6b65\u5e76\u53d1\u534f\u8bae\uff08WABC\u548cWCME\uff09\uff0c\u6bcf\u7ebf\u7a0b\u675f\u4ec5\u9700\u4e00\u6b21\u539f\u5b50\u64cd\u4f5c\u5e76\u4fdd\u8bc1\u65e0\u9501\u8fdb\u5c55\uff1b3\uff09\u57fa\u4e8e\u8d1f\u8f7d\u56e0\u5b50\u7684\u52a8\u6001\u6269\u5bb9\u7b56\u7565\uff0c\u5229\u7528\u7ebf\u6027\u54c8\u5e0c\u4ee5\u7ebf\u7a0b\u675f\u5e76\u884c\u65b9\u5f0f\u6309K\u6876\u6279\u6b21\u8c03\u6574\u5bb9\u91cf\u3002\u6b64\u5916\uff0c\u901a\u8fc7\u56db\u6b65\u63d2\u5165\u7b56\u7565\uff08\u66ff\u6362\u3001\u58f0\u660e\u63d0\u4ea4\u3001\u6709\u754c\u5e03\u8c37\u9e1f\u9a71\u9010\u3001\u6ea2\u51fa\u6682\u5b58\u56de\u9000\uff09\u5904\u7406\u9ad8\u7ade\u4e89\u573a\u666f\u3002", "result": "\u5728NVIDIA RTX 4090\u4e0a\uff0cHive\u54c8\u5e0c\u8868\u5728\u6df7\u5408\u63d2\u5165-\u5220\u9664-\u67e5\u627e\u8d1f\u8f7d\u4e0b\u652f\u6301\u9ad8\u8fbe95%\u7684\u8d1f\u8f7d\u56e0\u5b50\uff0c\u541e\u5410\u91cf\u6bd4\u5f53\u524d\u6700\u5148\u8fdb\u7684GPU\u54c8\u5e0c\u8868\uff08Slab-Hash\u3001DyCuckoo\u3001WarpCore\uff09\u9ad81.5\u20132\u500d\uff1b\u5728\u5747\u8861\u8d1f\u8f7d\u4e0b\uff0c\u66f4\u65b0\u901f\u5ea6\u8fbe35\u4ebf\u6b21/\u79d2\uff0c\u67e5\u627e\u63a5\u8fd140\u4ebf\u6b21/\u79d2\u3002", "conclusion": "Hive\u54c8\u5e0c\u8868\u901a\u8fc7\u521b\u65b0\u7684\u5185\u5b58\u5e03\u5c40\u3001\u5e76\u53d1\u534f\u8bae\u548c\u52a8\u6001\u8c03\u6574\u673a\u5236\uff0c\u5728GPU\u4e0a\u5b9e\u73b0\u4e86\u9ad8\u8d1f\u8f7d\u56e0\u5b50\u4e0b\u7684\u9ad8\u6027\u80fd\u4e0e\u53ef\u6269\u5c55\u6027\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6848\uff0c\u9002\u7528\u4e8eGPU\u52a0\u901f\u7684\u6570\u636e\u5bc6\u96c6\u578b\u5e94\u7528\u3002"}}
{"id": "2510.15494", "categories": ["cs.SE", "cs.AI", "cs.PF"], "pdf": "https://arxiv.org/pdf/2510.15494", "abs": "https://arxiv.org/abs/2510.15494", "authors": ["Lirong Yi", "Gregory Gay", "Philipp Leitner"], "title": "An Experimental Study of Real-Life LLM-Proposed Performance Improvements", "comment": null, "summary": "Large Language Models (LLMs) can generate code, but can they generate fast\ncode? In this paper, we study this question using a dataset of 65 real-world\ntasks mined from open-source Java programs. We specifically select tasks where\ndevelopers achieved significant speedups, and employ an automated pipeline to\ngenerate patches for these issues using two leading LLMs under four prompt\nvariations. By rigorously benchmarking the results against the baseline and\nhuman-authored solutions, we demonstrate that LLM-generated code indeed\nimproves performance over the baseline in most cases. However, patches proposed\nby human developers outperform LLM fixes by a statistically significant margin,\nindicating that LLMs often fall short of finding truly optimal solutions. We\nfurther find that LLM solutions are semantically identical or similar to the\ndeveloper optimization idea in approximately two-thirds of cases, whereas they\npropose a more original idea in the remaining one-third. However, these\noriginal ideas only occasionally yield substantial performance gains.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u662f\u5426\u80fd\u751f\u6210\u9ad8\u6548\u7684\u4ee3\u7801\uff0c\u901a\u8fc7\u5bf965\u4e2a\u771f\u5b9eJava\u4f18\u5316\u4efb\u52a1\u7684\u5b9e\u9a8c\u53d1\u73b0\uff0cLLM\u751f\u6210\u7684\u4ee3\u7801\u901a\u5e38\u4f18\u4e8e\u539f\u59cb\u57fa\u7ebf\uff0c\u4f46\u663e\u8457\u900a\u8272\u4e8e\u4eba\u7c7b\u5f00\u53d1\u8005\u7f16\u5199\u7684\u4f18\u5316\u4ee3\u7801\uff1b\u7ea6\u4e09\u5206\u4e4b\u4e8c\u7684LLM\u65b9\u6848\u4e0e\u4eba\u7c7b\u601d\u8def\u76f8\u4f3c\uff0c\u5176\u4f59\u4e3a\u539f\u521b\u4f46\u8f83\u5c11\u5e26\u6765\u663e\u8457\u6027\u80fd\u63d0\u5347\u3002", "motivation": "\u63a2\u7a76\u5927\u8bed\u8a00\u6a21\u578b\u5728\u751f\u6210\u9ad8\u6027\u80fd\u4ee3\u7801\u65b9\u9762\u7684\u80fd\u529b\uff0c\u7279\u522b\u662f\u5728\u771f\u5b9e\u4e16\u754c\u4e2d\u5f00\u53d1\u8005\u5df2\u5b9e\u73b0\u663e\u8457\u52a0\u901f\u7684\u573a\u666f\u4e0b\uff0cLLM\u662f\u5426\u80fd\u590d\u73b0\u6216\u8d85\u8d8a\u4eba\u7c7b\u4f18\u5316\u6548\u679c\u3002", "method": "\u4ece\u5f00\u6e90Java\u9879\u76ee\u4e2d\u6536\u96c665\u4e2a\u771f\u5b9e\u6027\u80fd\u4f18\u5316\u4efb\u52a1\uff0c\u4f7f\u7528\u4e24\u79cd\u4e3b\u6d41LLM\u5728\u56db\u79cd\u63d0\u793a\u7b56\u7565\u4e0b\u81ea\u52a8\u751f\u6210\u8865\u4e01\uff0c\u5e76\u901a\u8fc7\u4e25\u683c\u57fa\u51c6\u6d4b\u8bd5\u5c06\u5176\u4e0e\u539f\u59cb\u4ee3\u7801\u548c\u4eba\u7c7b\u4f18\u5316\u65b9\u6848\u8fdb\u884c\u6027\u80fd\u5bf9\u6bd4\u3002", "result": "LLM\u751f\u6210\u7684\u4ee3\u7801\u5728\u5927\u591a\u6570\u60c5\u51b5\u4e0b\u4f18\u4e8e\u57fa\u7ebf\uff0c\u4f46\u4eba\u7c7b\u5f00\u53d1\u8005\u65b9\u6848\u5728\u6027\u80fd\u4e0a\u663e\u8457\u4f18\u4e8eLLM\uff1b\u7ea666%\u7684LLM\u65b9\u6848\u4e0e\u4eba\u7c7b\u4f18\u5316\u601d\u8def\u8bed\u4e49\u76f8\u4f3c\uff0c\u5176\u4f59\u4e3a\u539f\u521b\u4f46\u4ec5\u5076\u5c14\u5e26\u6765\u663e\u8457\u6027\u80fd\u63d0\u5347\u3002", "conclusion": "\u5c3d\u7ba1LLM\u80fd\u5728\u4e00\u5b9a\u7a0b\u5ea6\u4e0a\u751f\u6210\u6027\u80fd\u6539\u8fdb\u7684\u4ee3\u7801\uff0c\u4f46\u5176\u5728\u53d1\u73b0\u771f\u6b63\u6700\u4f18\u89e3\u65b9\u9762\u4ecd\u660e\u663e\u843d\u540e\u4e8e\u4eba\u7c7b\u5f00\u53d1\u8005\uff0c\u539f\u521b\u4f18\u5316\u601d\u8def\u7684\u6709\u6548\u6027\u4e5f\u8f83\u4e3a\u6709\u9650\u3002"}}
{"id": "2510.15079", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.15079", "abs": "https://arxiv.org/abs/2510.15079", "authors": ["Changshu Liu", "Yang Chen", "Reyhaneh Jabbarvand"], "title": "Assessing Coherency and Consistency of Code Execution Reasoning by Large Language Models", "comment": null, "summary": "This paper proposes CES, a task to evaluate the abilities of LLMs in\nsimulating program execution and using that reasoning in programming tasks.\nBesides measuring the correctness of variable predictions during execution\nsimulation, CES introduces the notion of coherence to determine whether the\nsimulation complies with commonsense execution logic, even if the predicted\nvalues along the simulations are incorrect. This enables CES to rule out\nsuspiciously correct output predictions due to reasoning shortcuts,\nhallucinations, or potential data leakage. CES also introduces a novel metric\nto measure reasoning consistency across tests with the same or different prime\npath coverage in a spectrum: strong, weak, and random. Evaluating 16 LLMs\n(including three reasoning LLMs) using CES indicates 81.42% coherent execution\nsimulation on HumanEval, 46.92% and 53.08% of which result in correct and\nincorrect output predictions. Frontier LLMs such as GPT-4 and DeepSeek-R1 have\nthe most incoherent execution reasoning, mostly due to natural language\nshortcuts. Despite relatively coherent execution simulation, LLMs' reasoning\nperformance across different tests is inconsistent, mostly random (48.87%) or\nweak (45.37%), potentially explaining their weakness in programming tasks that\nrequire path-sensitive program analysis to succeed. We also compare CES with\nbug prediction/localization/repair, which intuitively requires control- and\ndata-flow awareness. We observe that LLMs barely incorporate execution\nreasoning into their analysis for bug-related tasks, and their success is\nprimarily due to inherent abilities in pattern matching or natural language\nshortcuts, if not data leakage. Without reasoning, there is a threat to the\ngeneralizability of LLMs in dealing with unseen bugs or patterns in different\ncontexts. CES can be used to vet the suspicious success of LLMs in these tasks\nsystematically.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86CES\u4efb\u52a1\uff0c\u7528\u4e8e\u8bc4\u4f30\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u6a21\u62df\u7a0b\u5e8f\u6267\u884c\u53ca\u5176\u5728\u7f16\u7a0b\u4efb\u52a1\u4e2d\u63a8\u7406\u80fd\u529b\u7684\u8868\u73b0\uff0c\u5f15\u5165\u201c\u4e00\u81f4\u6027\u201d\u6982\u5ff5\u548c\u65b0\u7684\u63a8\u7406\u4e00\u81f4\u6027\u5ea6\u91cf\u6807\u51c6\uff0c\u53d1\u73b0\u5c3d\u7ba1\u90e8\u5206\u6a21\u578b\u8f93\u51fa\u6b63\u786e\uff0c\u4f46\u5176\u6267\u884c\u6a21\u62df\u5e38\u7f3a\u4e4f\u903b\u8f91\u4e00\u81f4\u6027\uff0c\u4e14\u5728\u4e0d\u540c\u6d4b\u8bd5\u4e2d\u63a8\u7406\u8868\u73b0\u4e0d\u7a33\u5b9a\u3002", "motivation": "\u73b0\u6709\u8bc4\u4f30\u65b9\u6cd5\u96be\u4ee5\u5224\u65adLLMs\u5728\u7f16\u7a0b\u4efb\u52a1\u4e2d\u7684\u6b63\u786e\u8f93\u51fa\u662f\u5426\u6e90\u4e8e\u771f\u5b9e\u7684\u6267\u884c\u63a8\u7406\uff0c\u8fd8\u662f\u4f9d\u8d56\u81ea\u7136\u8bed\u8a00\u6377\u5f84\u3001\u5e7b\u89c9\u6216\u6570\u636e\u6cc4\u9732\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u80fd\u8861\u91cf\u6267\u884c\u6a21\u62df\u903b\u8f91\u4e00\u81f4\u6027\u7684\u65b0\u8bc4\u4f30\u6846\u67b6\u3002", "method": "CES\u4efb\u52a1\u901a\u8fc7\u8bc4\u4f30\u53d8\u91cf\u9884\u6d4b\u7684\u6b63\u786e\u6027\u4e0e\u6267\u884c\u6a21\u62df\u7684\u201c\u4e00\u81f4\u6027\u201d\uff08\u5373\u662f\u5426\u7b26\u5408\u5e38\u8bc6\u6027\u6267\u884c\u903b\u8f91\uff09\u6765\u5224\u65adLLMs\u7684\u63a8\u7406\u8d28\u91cf\uff0c\u5e76\u5f15\u5165\u8986\u76d6\u4e0d\u540c\u4e3b\u8def\u5f84\u7684\u6d4b\u8bd5\u7528\u4f8b\uff0c\u6309\u5f3a\u3001\u5f31\u3001\u968f\u673a\u4e09\u7c7b\u8861\u91cf\u63a8\u7406\u4e00\u81f4\u6027\uff1b\u540c\u65f6\u5c06CES\u4e0e\u7f3a\u9677\u9884\u6d4b/\u5b9a\u4f4d/\u4fee\u590d\u4efb\u52a1\u8fdb\u884c\u5bf9\u6bd4\u5206\u6790\u3002", "result": "\u5728HumanEval\u4e0a\uff0c16\u4e2aLLMs\u5e73\u5747\u8fbe\u523081.42%\u7684\u4e00\u81f4\u6027\u6267\u884c\u6a21\u62df\uff0c\u5176\u4e2d46.92%\u5e26\u6765\u6b63\u786e\u8f93\u51fa\uff0c53.08%\u4ecd\u8f93\u51fa\u9519\u8bef\uff1bGPT-4\u548cDeepSeek-R1\u7b49\u524d\u6cbf\u6a21\u578b\u4e00\u81f4\u6027\u6700\u5dee\uff1b\u6a21\u578b\u5728\u4e0d\u540c\u6d4b\u8bd5\u95f4\u7684\u63a8\u7406\u4e00\u81f4\u6027\u591a\u4e3a\u968f\u673a\uff0848.87%\uff09\u6216\u5f31\uff0845.37%\uff09\uff1b\u5728\u7f3a\u9677\u76f8\u5173\u4efb\u52a1\u4e2d\uff0cLLMs\u4e3b\u8981\u4f9d\u8d56\u6a21\u5f0f\u5339\u914d\u6216\u8bed\u8a00\u6377\u5f84\uff0c\u800c\u975e\u6267\u884c\u63a8\u7406\u3002", "conclusion": "LLMs\u5728\u7f16\u7a0b\u4efb\u52a1\u4e2d\u7684\u6210\u529f\u5e38\u7f3a\u4e4f\u53ef\u9760\u7684\u6267\u884c\u63a8\u7406\u652f\u6301\uff0cCES\u53ef\u6709\u6548\u8bc6\u522b\u6b64\u7c7b\u201c\u53ef\u7591\u6210\u529f\u201d\uff0c\u63ed\u793a\u6a21\u578b\u5728\u8def\u5f84\u654f\u611f\u5206\u6790\u4e0a\u7684\u4e0d\u8db3\uff0c\u5f3a\u8c03\u63d0\u5347\u903b\u8f91\u4e00\u81f4\u6027\u548c\u6cdb\u5316\u80fd\u529b\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2510.15122", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2510.15122", "abs": "https://arxiv.org/abs/2510.15122", "authors": ["Fran\u00e7ois Ezard", "Can Umut Ileri", "J\u00e9r\u00e9mie Decouchant"], "title": "NEMO: Faster Parallel Execution for Highly Contended Blockchain Workloads (Full version)", "comment": "This is the full version of a paper that will appear at the 7th\n  Conference on Blockchain Research & Applications for Innovative Networks and\n  Services (BRAINS 2025)", "summary": "Following the design of more efficient blockchain consensus algorithms, the\nexecution layer has emerged as the new performance bottleneck of blockchains,\nespecially under high contention. Current parallel execution frameworks either\nrely on optimistic concurrency control (OCC) or on pessimistic concurrency\ncontrol (PCC), both of which see their performance decrease when workloads are\nhighly contended, albeit for different reasons. In this work, we present NEMO,\na new blockchain execution engine that combines OCC with the object data model\nto address this challenge. NEMO introduces four core innovations: (i) a greedy\ncommit rule for transactions using only owned objects; (ii) refined handling of\ndependencies to reduce re-executions; (iii) the use of incomplete but\nstatically derivable read/write hints to guide execution; and (iv) a\npriority-based scheduler that favors transactions that unblock others. Through\nsimulated execution experiments, we demonstrate that NEMO significantly reduces\nredundant computation and achieves higher throughput than representative\napproaches. For example, with 16 workers NEMO's throughput is up to 42% higher\nthan the one of Block-STM, the state-of-the-art OCC approach, and 61% higher\nthan the pessimistic concurrency control baseline used.", "AI": {"tldr": "NEMO \u662f\u4e00\u79cd\u7ed3\u5408\u4e50\u89c2\u5e76\u53d1\u63a7\u5236\uff08OCC\uff09\u4e0e\u5bf9\u8c61\u6570\u636e\u6a21\u578b\u7684\u65b0\u578b\u533a\u5757\u94fe\u6267\u884c\u5f15\u64ce\uff0c\u901a\u8fc7\u56db\u9879\u6838\u5fc3\u521b\u65b0\u663e\u8457\u63d0\u5347\u9ad8\u7ade\u4e89\u8d1f\u8f7d\u4e0b\u7684\u541e\u5410\u91cf\uff0c\u76f8\u6bd4\u73b0\u6709\u65b9\u6cd5\u5982 Block-STM \u548c\u60b2\u89c2\u5e76\u53d1\u63a7\u5236\u57fa\u7ebf\u5206\u522b\u63d0\u5347\u6700\u9ad8 42% \u548c 61%\u3002", "motivation": "\u5f53\u524d\u533a\u5757\u94fe\u7684\u6267\u884c\u5c42\u5728\u9ad8\u7ade\u4e89\u573a\u666f\u4e0b\u6210\u4e3a\u6027\u80fd\u74f6\u9888\uff0c\u800c\u73b0\u6709\u7684\u4e50\u89c2\uff08OCC\uff09\u548c\u60b2\u89c2\uff08PCC\uff09\u5e76\u53d1\u63a7\u5236\u65b9\u6cd5\u5728\u9ad8\u7ade\u4e89\u5de5\u4f5c\u8d1f\u8f7d\u4e0b\u6027\u80fd\u5747\u663e\u8457\u4e0b\u964d\uff0c\u4e9f\u9700\u66f4\u9ad8\u6548\u7684\u6267\u884c\u5f15\u64ce\u3002", "method": "NEMO \u5f15\u5165\u56db\u5927\u521b\u65b0\uff1a(i) \u4ec5\u4f7f\u7528\u81ea\u6709\u5bf9\u8c61\u7684\u4e8b\u52a1\u91c7\u7528\u8d2a\u5fc3\u63d0\u4ea4\u89c4\u5219\uff1b(ii) \u4f18\u5316\u4f9d\u8d56\u5904\u7406\u4ee5\u51cf\u5c11\u91cd\u6267\u884c\uff1b(iii) \u5229\u7528\u9759\u6001\u53ef\u63a8\u5bfc\u4f46\u4e0d\u5b8c\u6574\u7684\u8bfb/\u5199\u63d0\u793a\u6307\u5bfc\u6267\u884c\uff1b(iv) \u91c7\u7528\u4f18\u5148\u7ea7\u8c03\u5ea6\u5668\u4f18\u5148\u6267\u884c\u80fd\u89e3\u9664\u5176\u4ed6\u4e8b\u52a1\u963b\u585e\u7684\u4e8b\u52a1\u3002", "result": "\u6a21\u62df\u5b9e\u9a8c\u8868\u660e\uff0cNEMO \u663e\u8457\u51cf\u5c11\u4e86\u5197\u4f59\u8ba1\u7b97\uff0c\u5728 16 \u4e2a\u5de5\u4f5c\u7ebf\u7a0b\u4e0b\uff0c\u541e\u5410\u91cf\u6bd4 Block-STM \u9ad8\u51fa\u6700\u591a 42%\uff0c\u6bd4\u60b2\u89c2\u5e76\u53d1\u63a7\u5236\u57fa\u7ebf\u9ad8\u51fa\u6700\u591a 61%\u3002", "conclusion": "NEMO \u6709\u6548\u89e3\u51b3\u4e86\u9ad8\u7ade\u4e89\u573a\u666f\u4e0b\u533a\u5757\u94fe\u6267\u884c\u5c42\u7684\u6027\u80fd\u74f6\u9888\u95ee\u9898\uff0c\u901a\u8fc7\u7ed3\u5408 OCC \u4e0e\u5bf9\u8c61\u6a21\u578b\u53ca\u56db\u9879\u5173\u952e\u6280\u672f\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u5e76\u53d1\u63a7\u5236\u65b9\u6cd5\u3002"}}
{"id": "2510.15408", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.15408", "abs": "https://arxiv.org/abs/2510.15408", "authors": ["Mohit", "Kuljit Kaur Chahal"], "title": "Community Engagement and the Lifespan of Open-Source Software Projects", "comment": null, "summary": "Open-source software (OSS) projects depend on community engagement (CE) for\nlongevity. However, CE's quantifiable impact on project dynamics and lifespan\nis underexplored. Objectives: This study defines CE in OSS, identifies key\nmetrics, and evaluates their influence on project dynamics (releases, commits,\nbranches) and lifespan. Methods: We analyzed 33,946 GitHub repositories,\ndefining and operationalizing CE with validated per-month metrics (issues,\ncomments, watchers, stargazers). Non-parametric tests and correlations assessed\nrelationships with project dynamics and lifespan across quartiles. Results: CE\nmetrics significantly associate with project dynamics, with stronger\ncorrelations in highly engaged projects. For lifespan, a complex pattern\nemerged: per-month CE rates are highest in younger projects, declining with\nage. Yet, a subset of long-lived projects maintains exceptionally high\nactivity. Initial CE bursts appear crucial for establishment, while sustained\nhigh engagement drives extreme longevity. Active issue engagement's influence\nintensifies with age, but passive attention's declines. Conclusion: CE\ndynamically drives OSS project longevity and development. Our findings\nestablish validated CE metrics and offer deeper insights into how diverse\ncommunity activity patterns contribute to project longevity.", "AI": {"tldr": "\u672c\u7814\u7a76\u901a\u8fc7\u5206\u679033,946\u4e2aGitHub\u4ed3\u5e93\uff0c\u5b9a\u4e49\u5e76\u9a8c\u8bc1\u4e86\u5f00\u6e90\u8f6f\u4ef6\u9879\u76ee\u4e2d\u7684\u793e\u533a\u53c2\u4e0e\uff08CE\uff09\u6307\u6807\uff0c\u53d1\u73b0CE\u4e0e\u9879\u76ee\u52a8\u6001\uff08\u5982\u53d1\u5e03\u3001\u63d0\u4ea4\u7b49\uff09\u663e\u8457\u76f8\u5173\uff0c\u4e14\u5bf9\u9879\u76ee\u5bff\u547d\u5177\u6709\u590d\u6742\u4f46\u5173\u952e\u7684\u5f71\u54cd\uff1a\u521d\u671f\u9ad8\u53c2\u4e0e\u6709\u52a9\u4e8e\u9879\u76ee\u5efa\u7acb\uff0c\u800c\u6301\u7eed\u9ad8\u53c2\u4e0e\u5219\u63a8\u52a8\u6781\u957f\u5bff\u547d\uff1b\u6d3b\u8dc3\u7684\u95ee\u9898\u4e92\u52a8\u968f\u9879\u76ee\u5e74\u9f84\u589e\u957f\u4f5c\u7528\u589e\u5f3a\uff0c\u800c\u88ab\u52a8\u5173\u6ce8\u5219\u51cf\u5f31\u3002", "motivation": "\u5f00\u6e90\u8f6f\u4ef6\u9879\u76ee\u7684\u957f\u671f\u5b58\u7eed\u4f9d\u8d56\u793e\u533a\u53c2\u4e0e\uff0c\u4f46\u793e\u533a\u53c2\u4e0e\u5bf9\u9879\u76ee\u52a8\u6001\u548c\u5bff\u547d\u7684\u53ef\u91cf\u5316\u5f71\u54cd\u5c1a\u672a\u88ab\u5145\u5206\u7814\u7a76\u3002", "method": "\u7814\u7a76\u5206\u6790\u4e8633,946\u4e2aGitHub\u4ed3\u5e93\uff0c\u5b9a\u4e49\u5e76\u64cd\u4f5c\u5316\u793e\u533a\u53c2\u4e0e\u6307\u6807\uff08\u5305\u62ec\u6bcf\u6708\u7684\u95ee\u9898\u6570\u3001\u8bc4\u8bba\u6570\u3001\u5173\u6ce8\u8005\u548c\u661f\u6807\u6570\uff09\uff0c\u5e76\u901a\u8fc7\u975e\u53c2\u6570\u68c0\u9a8c\u548c\u76f8\u5173\u6027\u5206\u6790\u8bc4\u4f30\u8fd9\u4e9b\u6307\u6807\u4e0e\u9879\u76ee\u52a8\u6001\u53ca\u5bff\u547d\u5728\u4e0d\u540c\u56db\u5206\u4f4d\u6570\u4e2d\u7684\u5173\u7cfb\u3002", "result": "\u793e\u533a\u53c2\u4e0e\u6307\u6807\u4e0e\u9879\u76ee\u52a8\u6001\u663e\u8457\u76f8\u5173\uff0c\u4e14\u5728\u9ad8\u53c2\u4e0e\u9879\u76ee\u4e2d\u76f8\u5173\u6027\u66f4\u5f3a\uff1b\u5c31\u9879\u76ee\u5bff\u547d\u800c\u8a00\uff0c\u5e74\u8f7b\u9879\u76ee\u6bcf\u6708\u793e\u533a\u53c2\u4e0e\u7387\u6700\u9ad8\uff0c\u968f\u5e74\u9f84\u589e\u957f\u800c\u4e0b\u964d\uff0c\u4f46\u90e8\u5206\u957f\u5bff\u9879\u76ee\u4ecd\u4fdd\u6301\u6781\u9ad8\u6d3b\u8dc3\u5ea6\uff1b\u521d\u671f\u53c2\u4e0e\u7206\u53d1\u5bf9\u9879\u76ee\u5efa\u7acb\u81f3\u5173\u91cd\u8981\uff0c\u6301\u7eed\u9ad8\u53c2\u4e0e\u63a8\u52a8\u6781\u7aef\u957f\u5bff\uff1b\u6d3b\u8dc3\u95ee\u9898\u4e92\u52a8\u7684\u5f71\u54cd\u968f\u9879\u76ee\u5e74\u9f84\u589e\u5f3a\uff0c\u800c\u88ab\u52a8\u5173\u6ce8\u7684\u5f71\u54cd\u51cf\u5f31\u3002", "conclusion": "\u793e\u533a\u53c2\u4e0e\u52a8\u6001\u5730\u9a71\u52a8\u5f00\u6e90\u8f6f\u4ef6\u9879\u76ee\u7684\u957f\u671f\u53d1\u5c55\u548c\u5bff\u547d\uff0c\u672c\u7814\u7a76\u786e\u7acb\u4e86\u7ecf\u8fc7\u9a8c\u8bc1\u7684\u793e\u533a\u53c2\u4e0e\u6307\u6807\uff0c\u5e76\u6df1\u5165\u63ed\u793a\u4e86\u4e0d\u540c\u7c7b\u578b\u793e\u533a\u6d3b\u52a8\u5bf9\u9879\u76ee\u5bff\u547d\u7684\u8d21\u732e\u673a\u5236\u3002"}}
{"id": "2510.15147", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2510.15147", "abs": "https://arxiv.org/abs/2510.15147", "authors": ["Aditya Bhosale", "Kavitha Chandrasekar", "Laxmikant Kale", "Sara Kokkila-Schumacher"], "title": "An Elastic Job Scheduler for HPC Applications on the Cloud", "comment": null, "summary": "The last few years have seen an increase in adoption of the cloud for running\nHPC applications. The pay-as-you-go cost model of these cloud resources has\nnecessitated the development of specialized programming models and schedulers\nfor HPC jobs for efficient utilization of cloud resources. A key aspect of\nefficient utilization is the ability to rescale applications on the fly to\nmaximize the utilization of cloud resources. Most commonly used parallel\nprogramming models like MPI have traditionally not supported autoscaling either\nin a cloud environment or on supercomputers. While more recent work has been\ndone to implement this functionality in MPI, it is still nascent and requires\nadditional programmer effort. Charm++ is a parallel programming model that\nnatively supports dynamic rescaling through its migratable objects paradigm. In\nthis paper, we present a Kubernetes operator to run Charm++ applications on a\nKubernetes cluster. We then present a priority-based elastic job scheduler that\ncan dynamically rescale jobs based on the state of a Kubernetes cluster to\nmaximize cluster utilization while minimizing response time for high-priority\njobs. We show that our elastic scheduler, with the ability to rescale HPC jobs\nwith minimal overhead, demonstrates significant performance improvements over\ntraditional static schedulers.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eKubernetes\u7684\u5f39\u6027\u8c03\u5ea6\u5668\uff0c\u7ed3\u5408Charm++\u7684\u52a8\u6001\u91cd\u7f29\u653e\u80fd\u529b\uff0c\u5b9e\u73b0\u9ad8\u6027\u80fd\u8ba1\u7b97\uff08HPC\uff09\u4f5c\u4e1a\u5728\u4e91\u73af\u5883\u4e2d\u7684\u9ad8\u6548\u8d44\u6e90\u5229\u7528\u548c\u4f4e\u54cd\u5e94\u5ef6\u8fdf\u3002", "motivation": "\u4f20\u7edfMPI\u7b49\u5e76\u884c\u7f16\u7a0b\u6a21\u578b\u7f3a\u4e4f\u5bf9\u52a8\u6001\u6269\u7f29\u5bb9\u7684\u652f\u6301\uff0c\u96be\u4ee5\u9ad8\u6548\u5229\u7528\u4e91\u8d44\u6e90\uff1b\u800c\u4e91\u73af\u5883\u4e2d\u6309\u9700\u4ed8\u8d39\u7684\u6a21\u5f0f\u8981\u6c42HPC\u5e94\u7528\u5177\u5907\u5f39\u6027\u8c03\u5ea6\u80fd\u529b\u4ee5\u63d0\u5347\u8d44\u6e90\u5229\u7528\u7387\u3002", "method": "\u5229\u7528Charm++\u539f\u751f\u652f\u6301\u52a8\u6001\u91cd\u7f29\u653e\u7684\u7279\u6027\uff0c\u5f00\u53d1\u4e86\u4e00\u4e2aKubernetes Operator\u6765\u90e8\u7f72Charm++\u5e94\u7528\uff0c\u5e76\u8bbe\u8ba1\u4e86\u4e00\u79cd\u57fa\u4e8e\u4f18\u5148\u7ea7\u7684\u5f39\u6027\u4f5c\u4e1a\u8c03\u5ea6\u5668\uff0c\u6839\u636e\u96c6\u7fa4\u72b6\u6001\u52a8\u6001\u8c03\u6574\u4f5c\u4e1a\u89c4\u6a21\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684\u5f39\u6027\u8c03\u5ea6\u5668\u5728\u6700\u5c0f\u5316\u9ad8\u4f18\u5148\u7ea7\u4f5c\u4e1a\u54cd\u5e94\u65f6\u95f4\u7684\u540c\u65f6\uff0c\u663e\u8457\u4f18\u4e8e\u4f20\u7edf\u9759\u6001\u8c03\u5ea6\u5668\uff0c\u5728\u8d44\u6e90\u5229\u7528\u7387\u548c\u6027\u80fd\u65b9\u9762\u5747\u6709\u660e\u663e\u63d0\u5347\u3002", "conclusion": "\u7ed3\u5408Charm++\u4e0eKubernetes\u7684\u5f39\u6027\u8c03\u5ea6\u65b9\u6848\u80fd\u6709\u6548\u652f\u6301HPC\u5e94\u7528\u5728\u4e91\u73af\u5883\u4e2d\u7684\u52a8\u6001\u8d44\u6e90\u8c03\u6574\uff0c\u4e3a\u9ad8\u6548\u3001\u7075\u6d3b\u7684\u4e91\u4e0aHPC\u63d0\u4f9b\u4e86\u53ef\u884c\u8def\u5f84\u3002"}}
{"id": "2510.15480", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.15480", "abs": "https://arxiv.org/abs/2510.15480", "authors": ["Muslim Chochlov", "Gul Aftab Ahmed", "James Vincent Patten", "Yuanhua Han", "Guoxian Lu", "David Gregg", "Jim Buckley"], "title": "Selecting and Combining Large Language Models for Scalable Code Clone Detection", "comment": null, "summary": "Source code clones pose risks ranging from intellectual property violations\nto unintended vulnerabilities. Effective and efficient scalable clone\ndetection, especially for diverged clones, remains challenging. Large language\nmodels (LLMs) have recently been applied to clone detection tasks. However, the\nrapid emergence of LLMs raises questions about optimal model selection and\npotential LLM-ensemble efficacy.\n  This paper addresses the first question by identifying 76 LLMs and filtering\nthem down to suitable candidates for large-scale clone detection. The\ncandidates were evaluated on two public industrial datasets, BigCloneBench, and\na commercial large-scale dataset. No uniformly 'best-LLM' emerged, though\nCodeT5+110M, CuBERT and SPTCode were top-performers. Analysis of LLM-candidates\nsuggested that smaller embedding sizes, smaller tokenizer vocabularies and\ntailored datasets are advantageous. On commercial large-scale dataset a\ntop-performing CodeT5+110M achieved 39.71\\% precision: twice the precision of\npreviously used CodeBERT.\n  To address the second question, this paper explores ensembling of the\nselected LLMs: effort-effective approach to improving effectiveness. Results\nsuggest the importance of score normalization and favoring ensembling methods\nlike maximum or sum over averaging. Also, findings indicate that ensembling\napproach can be statistically significant and effective on larger datasets: the\nbest-performing ensemble achieved even higher precision of 46.91\\% over\nindividual LLM on the commercial large-scale code.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u4ee3\u7801\u514b\u9686\u68c0\u6d4b\u4e2d\u7684\u9002\u7528\u6027\u4e0e\u96c6\u6210\u7b56\u7565\uff0c\u53d1\u73b0CodeT5+110M\u3001CuBERT\u548cSPTCode\u8868\u73b0\u6700\u4f73\uff0c\u5e76\u8bc1\u660e\u901a\u8fc7\u5408\u7406\u7684\u96c6\u6210\u65b9\u6cd5\u53ef\u8fdb\u4e00\u6b65\u63d0\u5347\u68c0\u6d4b\u7cbe\u5ea6\u3002", "motivation": "\u4ee3\u7801\u514b\u9686\u53ef\u80fd\u5e26\u6765\u77e5\u8bc6\u4ea7\u6743\u4fb5\u72af\u548c\u5b89\u5168\u6f0f\u6d1e\u7b49\u95ee\u9898\uff0c\u800c\u73b0\u6709\u65b9\u6cd5\u5728\u68c0\u6d4b\u9ad8\u5ea6\u53d8\u5f02\u7684\u514b\u9686\u4ee3\u7801\u65f6\u4ecd\u9762\u4e34\u6548\u7387\u4e0e\u6548\u679c\u7684\u6311\u6218\u3002\u5c3d\u7ba1\u5927\u8bed\u8a00\u6a21\u578b\u88ab\u7528\u4e8e\u8be5\u4efb\u52a1\uff0c\u4f46\u5982\u4f55\u9009\u62e9\u5408\u9002\u6a21\u578b\u53ca\u662f\u5426\u53ef\u901a\u8fc7\u96c6\u6210\u63d0\u5347\u6027\u80fd\u5c1a\u4e0d\u660e\u786e\u3002", "method": "\u4f5c\u8005\u7b5b\u9009\u51fa76\u4e2aLLM\u5e76\u8bc4\u4f30\u5176\u5728BigCloneBench\u548c\u4e00\u4e2a\u5546\u4e1a\u5927\u89c4\u6a21\u6570\u636e\u96c6\u4e0a\u7684\u8868\u73b0\uff1b\u968f\u540e\u63a2\u7d22\u4e0d\u540c\u96c6\u6210\u7b56\u7565\uff08\u5982\u6700\u5927\u503c\u3001\u6c42\u548c\u3001\u5e73\u5747\uff09\u5bf9\u68c0\u6d4b\u6548\u679c\u7684\u5f71\u54cd\uff0c\u5e76\u5206\u6790\u6a21\u578b\u7ed3\u6784\u7279\u5f81\uff08\u5982\u5d4c\u5165\u5927\u5c0f\u3001\u8bcd\u8868\u89c4\u6a21\uff09\u4e0e\u6027\u80fd\u7684\u5173\u7cfb\u3002", "result": "\u6ca1\u6709\u5355\u4e00\u201c\u6700\u4f73\u201dLLM\uff0c\u4f46CodeT5+110M\u3001CuBERT\u548cSPTCode\u8868\u73b0\u7a81\u51fa\uff1bCodeT5+110M\u5728\u5546\u4e1a\u6570\u636e\u96c6\u4e0a\u8fbe\u523039.71%\u7684\u7cbe\u5ea6\uff0c\u662f\u6b64\u524dCodeBERT\u7684\u4e24\u500d\uff1b\u96c6\u6210\u65b9\u6cd5\u4e2d\u6700\u5927\u503c\u6216\u6c42\u548c\u4f18\u4e8e\u5e73\u5747\uff0c\u6700\u4f73\u96c6\u6210\u65b9\u6848\u5728\u5546\u4e1a\u6570\u636e\u96c6\u4e0a\u7cbe\u5ea6\u8fbe46.91%\u3002", "conclusion": "\u9488\u5bf9\u4ee3\u7801\u514b\u9686\u68c0\u6d4b\u4efb\u52a1\uff0c\u8f83\u5c0f\u7684\u5d4c\u5165\u7ef4\u5ea6\u3001\u8f83\u5c0f\u7684\u8bcd\u8868\u53ca\u4e13\u7528\u8bad\u7ec3\u6570\u636e\u6709\u52a9\u4e8e\u63d0\u5347LLM\u6027\u80fd\uff1b\u5408\u7406\u96c6\u6210\u591a\u4e2aLLM\u53ef\u663e\u8457\u63d0\u9ad8\u68c0\u6d4b\u7cbe\u5ea6\uff0c\u5c24\u5176\u5728\u5927\u89c4\u6a21\u6570\u636e\u96c6\u4e0a\u6548\u679c\u66f4\u4f73\u3002"}}
{"id": "2510.15802", "categories": ["cs.NI", "C.2.2; C.2.3; C.2.5"], "pdf": "https://arxiv.org/pdf/2510.15802", "abs": "https://arxiv.org/abs/2510.15802", "authors": ["Tommaso Bonato", "Sepehr Abdous", "Abdul Kabbani", "Ahmad Ghalayini", "Nadeen Gebara", "Terry Lam", "Anup Agarwal", "Tiancheng Chen", "Zhuolong Yu", "Konstantin Taranov", "Mahmoud Elhaddad", "Daniele De Sensi", "Soudeh Ghorbani", "Torsten Hoefler"], "title": "Uno: A One-Stop Solution for Inter- and Intra-Datacenter Congestion Control and Reliable Connectivity", "comment": null, "summary": "Cloud computing and AI workloads are driving unprecedented demand for\nefficient communication within and across datacenters. However, the coexistence\nof intra- and inter-datacenter traffic within datacenters plus the disparity\nbetween the RTTs of intra- and inter-datacenter networks complicates congestion\nmanagement and traffic routing. Particularly, faster congestion responses of\nintra-datacenter traffic causes rate unfairness when competing with slower\ninter-datacenter flows. Additionally, inter-datacenter messages suffer from\nslow loss recovery and, thus, require reliability. Existing solutions overlook\nthese challenges and handle inter- and intra-datacenter congestion with\nseparate control loops or at different granularities. We propose Uno, a unified\nsystem for both inter- and intra-DC environments that integrates a transport\nprotocol for rapid congestion reaction and fair rate control with a load\nbalancing scheme that combines erasure coding and adaptive routing. Our\nfindings show that Uno significantly improves the completion times of both\ninter- and intra-DC flows compared to state-of-the-art methods such as Gemini.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aUno\u7684\u7edf\u4e00\u7cfb\u7edf\uff0c\u7528\u4e8e\u540c\u65f6\u4f18\u5316\u6570\u636e\u4e2d\u5fc3\u5185\u90e8\u548c\u8de8\u6570\u636e\u4e2d\u5fc3\u7684\u6d41\u91cf\u4f20\u8f93\uff0c\u901a\u8fc7\u6574\u5408\u5feb\u901f\u62e5\u585e\u54cd\u5e94\u7684\u4f20\u8f93\u534f\u8bae\u4e0e\u7ed3\u5408\u7ea0\u5220\u7801\u548c\u81ea\u9002\u5e94\u8def\u7531\u7684\u8d1f\u8f7d\u5747\u8861\u65b9\u6848\uff0c\u663e\u8457\u63d0\u5347\u4e86\u4e24\u7c7b\u6d41\u91cf\u7684\u5b8c\u6210\u65f6\u95f4\u3002", "motivation": "\u73b0\u6709\u65b9\u6848\u5728\u5904\u7406\u6570\u636e\u4e2d\u5fc3\u5185\u90e8\u4e0e\u8de8\u6570\u636e\u4e2d\u5fc3\u6d41\u91cf\u65f6\u5b58\u5728\u4e0d\u8db3\uff1a\u7531\u4e8e\u4e24\u7c7b\u6d41\u91cf\u7684\u5f80\u8fd4\u65f6\u5ef6\uff08RTT\uff09\u5dee\u5f02\uff0c\u5bfc\u81f4\u62e5\u585e\u63a7\u5236\u4e0d\u516c\u5e73\uff1b\u8de8\u6570\u636e\u4e2d\u5fc3\u6d41\u91cf\u8fd8\u9762\u4e34\u4e22\u5305\u6062\u590d\u6162\u548c\u53ef\u9760\u6027\u9700\u6c42\u9ad8\u7684\u95ee\u9898\u3002\u5f53\u524d\u65b9\u6cd5\u901a\u5e38\u91c7\u7528\u5206\u79bb\u7684\u63a7\u5236\u73af\u8def\u6216\u4e0d\u540c\u7c92\u5ea6\u5904\u7406\uff0c\u65e0\u6cd5\u6709\u6548\u5e94\u5bf9\u8fd9\u4e9b\u6311\u6218\u3002", "method": "\u63d0\u51faUno\u7cfb\u7edf\uff0c\u7edf\u4e00\u5904\u7406\u6570\u636e\u4e2d\u5fc3\u5185\u4e0e\u8de8\u6570\u636e\u4e2d\u5fc3\u6d41\u91cf\u3002\u8be5\u7cfb\u7edf\u7ed3\u5408\u4e00\u79cd\u652f\u6301\u5feb\u901f\u62e5\u585e\u54cd\u5e94\u548c\u516c\u5e73\u901f\u7387\u63a7\u5236\u7684\u4f20\u8f93\u534f\u8bae\uff0c\u4ee5\u53ca\u4e00\u79cd\u878d\u5408\u7ea0\u5220\u7801\u4e0e\u81ea\u9002\u5e94\u8def\u7531\u7684\u8d1f\u8f7d\u5747\u8861\u673a\u5236\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u76f8\u6bd4Gemini\u7b49\u73b0\u6709\u5148\u8fdb\u65b9\u6cd5\uff0cUno\u663e\u8457\u7f29\u77ed\u4e86\u6570\u636e\u4e2d\u5fc3\u5185\u90e8\u548c\u8de8\u6570\u636e\u4e2d\u5fc3\u6d41\u91cf\u7684\u5b8c\u6210\u65f6\u95f4\u3002", "conclusion": "Uno\u901a\u8fc7\u7edf\u4e00\u67b6\u6784\u6709\u6548\u89e3\u51b3\u4e86\u6570\u636e\u4e2d\u5fc3\u5185\u4e0e\u8de8\u6570\u636e\u4e2d\u5fc3\u6d41\u91cf\u5728\u62e5\u585e\u63a7\u5236\u3001\u516c\u5e73\u6027\u548c\u53ef\u9760\u6027\u65b9\u9762\u7684\u5173\u952e\u6311\u6218\uff0c\u4e3a\u4e91\u548cAI\u5de5\u4f5c\u8d1f\u8f7d\u63d0\u4f9b\u4e86\u66f4\u9ad8\u6548\u7684\u901a\u4fe1\u652f\u6301\u3002"}}
{"id": "2510.15330", "categories": ["cs.DC", "cs.AI", "cs.CL", "cs.NI"], "pdf": "https://arxiv.org/pdf/2510.15330", "abs": "https://arxiv.org/abs/2510.15330", "authors": ["Tella Rajashekhar Reddy", "Atharva Deshmukh", "Karan Tandon", "Rohan Gandhi", "Anjaly Parayil", "Debopam Bhattacherjee"], "title": "BeLLMan: Controlling LLM Congestion", "comment": "To be presented at FAISYS 2025", "summary": "Large language model (LLM) applications are blindfolded to the infrastructure\nunderneath and generate tokens autoregressively, indifferent to the system\nload, thus risking inferencing latency inflation and poor user experience. Our\nfirst-cut controller, named beLLMan, enables the LLM infrastructure to actively\nand progressively signal the first-party LLM application to adjust the output\nlength in response to changing system load. On a real testbed with H100 GPUs,\nbeLLMan helps keep inferencing latency under control (upto 8X lower end-to-end\nlatency) and reduces energy consumption by 25% (while serving 19% more\nrequests) during periods of congestion for a summarization workload.", "AI": {"tldr": "beLLMan is a controller that enables LLM applications to dynamically adjust output length based on system load, significantly reducing latency and energy consumption while increasing request throughput.", "motivation": "LLM applications currently generate tokens without considering underlying system load, leading to high inference latency and poor user experience during congestion.", "method": "The authors propose beLLMan, a controller that actively signals the LLM application to adjust output length in response to real-time system load changes.", "result": "On an H100 GPU testbed, beLLMan reduces end-to-end latency by up to 8\u00d7, cuts energy consumption by 25%, and serves 19% more requests during congestion for a summarization task.", "conclusion": "Dynamic output-length adaptation guided by infrastructure feedback (via beLLMan) effectively mitigates latency and energy inefficiencies in LLM inference under load."}}
{"id": "2510.15512", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.15512", "abs": "https://arxiv.org/abs/2510.15512", "authors": ["Wachiraphan Charoenwet", "Patanamon Thongtanunam", "Van-Thuan Pham", "Christoph Treude"], "title": "Enhancing Code Review through Fuzzing and Likely Invariants", "comment": null, "summary": "Many software projects employ manual code review to gatekeep defects and\nvulnerabilities in the code before integration. However, reviewers often work\nunder time pressure and rely primarily on static inspection, leaving the\ndynamic aspects of the program unexplored. Dynamic analyses could reveal such\nbehaviors, but they are rarely integrated into reviews. Among them, fuzzing is\ntypically applied later to uncover crashing bugs. Yet its ability to exercise\ncode with diverse inputs makes it promising for exposing non-crashing, but\nunexpected, behaviors earlier. Still, without suitable mechanisms to analyze\nprogram behaviors, the rich data produced during fuzzing remains inaccessible\nto reviewers, limiting its practical value in this context.\n  We hypothesize that unexpected variations in program behaviors could signify\npotential bugs. The impact of code changes can be automatically captured at\nruntime. Representing program behavior as likely invariants, dynamic properties\nconsistently observed at specific program points, can provide practical signals\nof behavioral changes. Such signals offer a way to distinguish between intended\nchanges and unexpected behavioral shifts from code changes.\n  We present FuzzSight, a framework that leverages likely invariants from\nnon-crashing fuzzing inputs to highlight behavioral differences across program\nversions. By surfacing such differences, it provides insights into which code\nblocks may need closer attention. In our evaluation, FuzzSight flagged 75% of\nregression bugs and up to 80% of vulnerabilities uncovered by 24-hour fuzzing.\nIt also outperformed SAST in identifying buggy code blocks, achieving ten times\nhigher detection rates with fewer false alarms. In summary, FuzzSight\ndemonstrates the potential and value of leveraging fuzzing and invariant\nanalysis for early-stage code review, bridging static inspection with dynamic\nbehavioral insights.", "AI": {"tldr": "FuzzSight \u662f\u4e00\u4e2a\u7ed3\u5408\u6a21\u7cca\u6d4b\u8bd5\u4e0e\u52a8\u6001\u4e0d\u53d8\u91cf\u5206\u6790\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u5728\u4ee3\u7801\u5ba1\u67e5\u9636\u6bb5\u65e9\u671f\u8bc6\u522b\u7531\u4ee3\u7801\u53d8\u66f4\u5f15\u53d1\u7684\u975e\u5d29\u6e83\u578b\u884c\u4e3a\u5f02\u5e38\uff0c\u4ece\u800c\u5e2e\u52a9\u53d1\u73b0\u56de\u5f52\u7f3a\u9677\u548c\u6f0f\u6d1e\u3002", "motivation": "\u4f20\u7edf\u4ee3\u7801\u5ba1\u67e5\u4e3b\u8981\u4f9d\u8d56\u9759\u6001\u68c0\u67e5\uff0c\u96be\u4ee5\u8986\u76d6\u7a0b\u5e8f\u7684\u52a8\u6001\u884c\u4e3a\uff1b\u800c\u6a21\u7cca\u6d4b\u8bd5\u867d\u80fd\u751f\u6210\u591a\u6837\u8f93\u5165\uff0c\u4f46\u5176\u4ea7\u751f\u7684\u52a8\u6001\u884c\u4e3a\u6570\u636e\u7f3a\u4e4f\u6709\u6548\u5206\u6790\u673a\u5236\uff0c\u96be\u4ee5\u878d\u5165\u4ee3\u7801\u5ba1\u67e5\u6d41\u7a0b\u3002", "method": "\u63d0\u51fa FuzzSight \u6846\u67b6\uff0c\u901a\u8fc7\u975e\u5d29\u6e83\u578b\u6a21\u7cca\u6d4b\u8bd5\u8f93\u5165\u63d0\u53d6\u7a0b\u5e8f\u5728\u7279\u5b9a\u4f4d\u7f6e\u7684\u201c\u53ef\u80fd\u4e0d\u53d8\u91cf\u201d\uff08likely invariants\uff09\uff0c\u4ee5\u6b64\u8868\u5f81\u7a0b\u5e8f\u884c\u4e3a\uff0c\u5e76\u5bf9\u6bd4\u4e0d\u540c\u7248\u672c\u95f4\u7684\u884c\u4e3a\u5dee\u5f02\uff0c\u8bc6\u522b\u6f5c\u5728\u5f02\u5e38\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cFuzzSight \u80fd\u68c0\u6d4b\u5230 75% \u7684\u56de\u5f52\u7f3a\u9677\u548c\u9ad8\u8fbe 80% \u7684\u6f0f\u6d1e\uff1b\u76f8\u6bd4\u9759\u6001\u5206\u6790\u5de5\u5177\uff08SAST\uff09\uff0c\u5176\u5728\u5b9a\u4f4d\u7f3a\u9677\u4ee3\u7801\u5757\u65b9\u9762\u68c0\u6d4b\u7387\u9ad8\u5341\u500d\u4e14\u8bef\u62a5\u66f4\u5c11\u3002", "conclusion": "\u5c06\u6a21\u7cca\u6d4b\u8bd5\u4e0e\u4e0d\u53d8\u91cf\u5206\u6790\u7ed3\u5408\uff0c\u53ef\u6709\u6548\u8865\u5145\u9759\u6001\u4ee3\u7801\u5ba1\u67e5\uff0c\u4e3a\u65e9\u671f\u53d1\u73b0\u975e\u5d29\u6e83\u578b\u884c\u4e3a\u5f02\u5e38\u63d0\u4f9b\u5b9e\u7528\u4e14\u9ad8\u6548\u7684\u52a8\u6001\u6d1e\u5bdf\u3002"}}
{"id": "2510.15355", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2510.15355", "abs": "https://arxiv.org/abs/2510.15355", "authors": ["Tim Kraus", "Axel Sauer", "Ingo Feldner"], "title": "Cloud-Enabled Virtual Prototypes", "comment": "8 pages, 5 figures, Published in DVCon Europe 2025", "summary": "The rapid evolution of embedded systems, along with the growing variety and\ncomplexity of AI algorithms, necessitates a powerful hardware/software\nco-design methodology based on virtual prototyping technologies. The market\noffers a diverse range of simulation solutions, each with its unique\ntechnological approach and therefore strengths and weaknesses. Additionally,\nwith the increasing availability of remote on-demand computing resources and\ntheir adaptation throughout the industry, the choice of the host infrastructure\nfor execution opens even more new possibilities for operational strategies.\nThis work explores the dichotomy between local and cloud-based simulation\nenvironments, focusing on the trade-offs between scalability and privacy. We\ndiscuss how the setup of the compute infrastructure impacts the performance of\nthe execution and security of data involved in the process. Furthermore, we\nhighlight the development workflow associated with embedded AI and the critical\nrole of efficient simulations in optimizing these algorithms. With the proposed\nsolution, we aim to sustainably improve trust in remote simulations and\nfacilitate the adoption of virtual prototyping practices.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u4e86\u672c\u5730\u4e0e\u4e91\u7aef\u4eff\u771f\u73af\u5883\u5728\u5d4c\u5165\u5f0fAI\u5f00\u53d1\u4e2d\u7684\u6743\u8861\uff0c\u91cd\u70b9\u5173\u6ce8\u53ef\u6269\u5c55\u6027\u4e0e\u6570\u636e\u9690\u79c1\u4e4b\u95f4\u7684\u5e73\u8861\uff0c\u5e76\u63d0\u51fa\u63d0\u5347\u8fdc\u7a0b\u4eff\u771f\u53ef\u4fe1\u5ea6\u7684\u65b9\u6848\u3002", "motivation": "\u5d4c\u5165\u5f0f\u7cfb\u7edf\u548cAI\u7b97\u6cd5\u65e5\u76ca\u590d\u6742\uff0c\u9700\u8981\u9ad8\u6548\u7684\u8f6f\u786c\u4ef6\u534f\u540c\u8bbe\u8ba1\u65b9\u6cd5\uff0c\u800c\u865a\u62df\u539f\u578b\u6280\u672f\u662f\u5173\u952e\uff1b\u540c\u65f6\uff0c\u968f\u7740\u8fdc\u7a0b\u8ba1\u7b97\u8d44\u6e90\u666e\u53ca\uff0c\u5982\u4f55\u9009\u62e9\u5408\u9002\u7684\u4eff\u771f\u57fa\u7840\u8bbe\u65bd\u6210\u4e3a\u91cd\u8981\u95ee\u9898\u3002", "method": "\u5206\u6790\u672c\u5730\u4e0e\u4e91\u4eff\u771f\u73af\u5883\u5728\u6027\u80fd\u3001\u6570\u636e\u5b89\u5168\u53ca\u5f00\u53d1\u6d41\u7a0b\u4e2d\u7684\u5dee\u5f02\uff0c\u8bc4\u4f30\u8ba1\u7b97\u57fa\u7840\u8bbe\u65bd\u8bbe\u7f6e\u5bf9\u4eff\u771f\u6267\u884c\u6548\u7387\u4e0e\u6570\u636e\u5b89\u5168\u7684\u5f71\u54cd\u3002", "result": "\u660e\u786e\u4e86\u672c\u5730\u4e0e\u4e91\u7aef\u4eff\u771f\u5728\u53ef\u6269\u5c55\u6027\u4e0e\u9690\u79c1\u4fdd\u62a4\u65b9\u9762\u7684\u4f18\u52a3\uff0c\u5f3a\u8c03\u9ad8\u6548\u4eff\u771f\u5bf9\u5d4c\u5165\u5f0fAI\u7b97\u6cd5\u4f18\u5316\u7684\u5173\u952e\u4f5c\u7528\u3002", "conclusion": "\u901a\u8fc7\u5408\u7406\u9009\u62e9\u548c\u914d\u7f6e\u4eff\u771f\u57fa\u7840\u8bbe\u65bd\uff0c\u53ef\u63d0\u5347\u8fdc\u7a0b\u4eff\u771f\u7684\u53ef\u4fe1\u5ea6\uff0c\u63a8\u52a8\u865a\u62df\u539f\u578b\u6280\u672f\u5728\u5d4c\u5165\u5f0fAI\u5f00\u53d1\u4e2d\u7684\u53ef\u6301\u7eed\u5e94\u7528\u3002"}}
{"id": "2510.15565", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.15565", "abs": "https://arxiv.org/abs/2510.15565", "authors": ["Vinicius Moraes de Jesus", "Andre Georghton Cardoso Pacheco"], "title": "Colepp: uma ferramenta multiplataforma para coleta de dados de dispositivos vestiveis", "comment": "in Portuguese language", "summary": "The widespread adoption of wearable devices such as smartwatches and fitness\ntrackers has fueled the demand for reliable physiological and movement data\ncollection tools. However, challenges such as limited access to large,\nhigh-quality public datasets and a lack of control over data collection\nconditions hinder the development of robust algorithms. This work presents\nColepp, an open-source, cross-platform tool designed to collect and synchronize\ndata from multiple wearable devices, including heart rate (via ECG and PPG) and\nmotion signals (accelerometer and gyroscope). The system integrates a\nsmartphone as a central hub, receiving data from a Polar H10 chest strap and a\nWear OS smartwatch, and exporting synchronized datasets in CSV format. Through\na custom synchronization protocol and user-friendly interface, Colepp\nfacilitates the generation of customizable, real-world datasets suitable for\napplications such as human activity recognition and heart rate estimation. A\nuse case shows the effectiveness of the tool in producing consistent and\nsynchronized signals.", "AI": {"tldr": "Colepp \u662f\u4e00\u4e2a\u5f00\u6e90\u8de8\u5e73\u53f0\u5de5\u5177\uff0c\u7528\u4e8e\u4ece\u591a\u79cd\u53ef\u7a7f\u6234\u8bbe\u5907\u540c\u6b65\u91c7\u96c6\u5fc3\u7387\u548c\u8fd0\u52a8\u6570\u636e\uff0c\u5e76\u901a\u8fc7\u667a\u80fd\u624b\u673a\u6574\u5408\u751f\u6210\u53ef\u7528\u4e8e\u4eba\u4f53\u6d3b\u52a8\u8bc6\u522b\u7b49\u4efb\u52a1\u7684\u9ad8\u8d28\u91cf\u6570\u636e\u96c6\u3002", "motivation": "\u73b0\u6709\u53ef\u7a7f\u6234\u8bbe\u5907\u6570\u636e\u91c7\u96c6\u9762\u4e34\u7f3a\u4e4f\u5927\u89c4\u6a21\u9ad8\u8d28\u91cf\u516c\u5f00\u6570\u636e\u96c6\u53ca\u6570\u636e\u91c7\u96c6\u6761\u4ef6\u4e0d\u53ef\u63a7\u7684\u95ee\u9898\uff0c\u9650\u5236\u4e86\u9c81\u68d2\u7b97\u6cd5\u7684\u5f00\u53d1\u3002", "method": "\u5f00\u53d1 Colepp \u7cfb\u7edf\uff0c\u5229\u7528\u667a\u80fd\u624b\u673a\u4f5c\u4e3a\u4e2d\u5fc3\u67a2\u7ebd\uff0c\u540c\u6b65\u91c7\u96c6 Polar H10 \u80f8\u5e26\uff08ECG\uff09\u548c Wear OS \u667a\u80fd\u624b\u8868\uff08PPG\u3001\u52a0\u901f\u5ea6\u8ba1\u3001\u9640\u87ba\u4eea\uff09\u7684\u6570\u636e\uff0c\u5e76\u901a\u8fc7\u81ea\u5b9a\u4e49\u540c\u6b65\u534f\u8bae\u5bfc\u51fa\u4e3a CSV \u683c\u5f0f\u3002", "result": "\u901a\u8fc7\u5b9e\u9645\u7528\u4f8b\u9a8c\u8bc1\uff0cColepp \u80fd\u6709\u6548\u751f\u6210\u4e00\u81f4\u4e14\u540c\u6b65\u7684\u751f\u7406\u4e0e\u8fd0\u52a8\u4fe1\u53f7\u6570\u636e\u96c6\u3002", "conclusion": "Colepp \u4e3a\u7814\u7a76\u8005\u63d0\u4f9b\u4e86\u4e00\u4e2a\u7075\u6d3b\u3001\u6613\u7528\u7684\u5de5\u5177\uff0c\u6709\u52a9\u4e8e\u63a8\u52a8\u57fa\u4e8e\u53ef\u7a7f\u6234\u8bbe\u5907\u7684\u4eba\u4f53\u6d3b\u52a8\u8bc6\u522b\u4e0e\u5fc3\u7387\u4f30\u8ba1\u7b97\u6cd5\u7684\u53d1\u5c55\u3002"}}
{"id": "2510.15473", "categories": ["cs.DC", "math.PR"], "pdf": "https://arxiv.org/pdf/2510.15473", "abs": "https://arxiv.org/abs/2510.15473", "authors": ["Petra Berenbrink", "Robert Els\u00e4sser", "Tom Friedetzky", "Hamed Hosseinpour", "Dominik Kaaser", "Peter Kling", "Thomas Sauerwald"], "title": "(Almost) Perfect Discrete Iterative Load Balancing", "comment": null, "summary": "We consider discrete, iterative load balancing via matchings on arbitrary\ngraphs. Initially each node holds a certain number of tokens, defining the load\nof the node, and the objective is to redistribute the tokens such that\neventually each node has approximately the same number of tokens. We present\nresults for a general class of simple local balancing schemes where the tokens\nare balanced via matchings. In each round the process averages the tokens of\nany two matched nodes. If the sum of their tokens is odd, the node to receive\nthe one excess token is selected at random. Our class covers three popular\nmodels: in the matching model a new matching is generated randomly in each\nround, in the balancing circuit model a fixed sequence of matchings is applied\nperiodically, and in the asynchronous model the load is balanced over a\nrandomly chosen edge.\n  We measure the quality of a load vector by its discrepancy, defined as the\ndifference between the maximum and minimum load across all nodes. As our main\nresult we show that with high probability our discrete balancing scheme reaches\na discrepancy of $3$ in a number of rounds which asymptotically matches the\nspectral bound for continuous load balancing with fractional load.\n  This result improves and tightens a long line of previous works, by not only\nachieving a small constant discrepancy (instead of a non-explicit, large\nconstant) but also holding for arbitrary instead of regular graphs. The result\nalso demonstrates that in the general model we consider, discrete load\nbalancing is no harder than continuous load balancing.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u5728\u4efb\u610f\u56fe\u4e0a\u901a\u8fc7\u5339\u914d\u8fdb\u884c\u79bb\u6563\u8fed\u4ee3\u8d1f\u8f7d\u5747\u8861\u7684\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u4e00\u7c7b\u901a\u7528\u7684\u5c40\u90e8\u5747\u8861\u65b9\u6848\uff0c\u5e76\u8bc1\u660e\u8be5\u65b9\u6848\u5728\u9ad8\u6982\u7387\u4e0b\u53ef\u5728\u4e0e\u8fde\u7eed\u8d1f\u8f7d\u5747\u8861\u8c31\u754c\u76f8\u5339\u914d\u7684\u8f6e\u6570\u5185\u8fbe\u5230\u6700\u5927\u4e0e\u6700\u5c0f\u8d1f\u8f7d\u5dee\u503c\u4e3a3\u7684\u5747\u8861\u72b6\u6001\u3002", "motivation": "\u8d1f\u8f7d\u5747\u8861\u662f\u5206\u5e03\u5f0f\u8ba1\u7b97\u4e2d\u7684\u57fa\u672c\u95ee\u9898\uff0c\u73b0\u6709\u5de5\u4f5c\u591a\u5173\u6ce8\u6b63\u5219\u56fe\u6216\u4ec5\u80fd\u4fdd\u8bc1\u8f83\u5927\u7684\u5e38\u6570\u504f\u5dee\uff0c\u672c\u6587\u65e8\u5728\u8bbe\u8ba1\u9002\u7528\u4e8e\u4efb\u610f\u56fe\u4e14\u80fd\u8fbe\u5230\u5c0f\u5e38\u6570\u504f\u5dee\u7684\u79bb\u6563\u8d1f\u8f7d\u5747\u8861\u7b97\u6cd5\u3002", "method": "\u4f5c\u8005\u8003\u8651\u4e00\u7c7b\u57fa\u4e8e\u5339\u914d\u7684\u5c40\u90e8\u5747\u8861\u673a\u5236\uff0c\u6bcf\u8f6e\u5bf9\u5339\u914d\u8282\u70b9\u7684\u4ee4\u724c\u6570\u53d6\u5e73\u5747\uff08\u5947\u6570\u65f6\u968f\u673a\u5206\u914d\u4f59\u6570\uff09\uff0c\u6db5\u76d6\u5339\u914d\u6a21\u578b\u3001\u5e73\u8861\u7535\u8def\u6a21\u578b\u548c\u5f02\u6b65\u6a21\u578b\u4e09\u79cd\u7ecf\u5178\u8bbe\u5b9a\uff0c\u5e76\u901a\u8fc7\u5206\u6790\u8d1f\u8f7d\u5411\u91cf\u7684\u504f\u5dee\uff08\u6700\u5927\u4e0e\u6700\u5c0f\u8d1f\u8f7d\u4e4b\u5dee\uff09\u6765\u8bc4\u4f30\u6027\u80fd\u3002", "result": "\u5728\u9ad8\u6982\u7387\u4e0b\uff0c\u6240\u63d0\u51fa\u7684\u79bb\u6563\u5747\u8861\u65b9\u6848\u53ef\u5728\u8f6e\u6570\u4e0a\u6e10\u8fd1\u5339\u914d\u8fde\u7eed\u8d1f\u8f7d\u5747\u8861\u7684\u8c31\u754c\uff0c\u5e76\u8fbe\u5230\u504f\u5dee\u4e3a3\u7684\u5747\u8861\u72b6\u6001\uff0c\u4f18\u4e8e\u4ee5\u5f80\u5de5\u4f5c\u4e14\u9002\u7528\u4e8e\u4efb\u610f\u56fe\u3002", "conclusion": "\u8be5\u7814\u7a76\u8bc1\u660e\u5728\u6240\u8003\u8651\u7684\u901a\u7528\u6a21\u578b\u4e2d\uff0c\u79bb\u6563\u8d1f\u8f7d\u5747\u8861\u5e76\u4e0d\u6bd4\u8fde\u7eed\u8d1f\u8f7d\u5747\u8861\u66f4\u96be\uff0c\u4e14\u9996\u6b21\u5728\u4efb\u610f\u56fe\u4e0a\u5b9e\u73b0\u4e86\u5c0f\u5e38\u6570\u504f\u5dee\u7684\u9ad8\u6548\u5747\u8861\u3002"}}
{"id": "2510.15585", "categories": ["cs.SE", "cs.CL", "cs.PL", "F.2.2; I.2.7"], "pdf": "https://arxiv.org/pdf/2510.15585", "abs": "https://arxiv.org/abs/2510.15585", "authors": ["Dr Simon Thorne", "Dr Advait Sarkar"], "title": "Leveraging Test Driven Development with Large Language Models for Reliable and Verifiable Spreadsheet Code Generation: A Research Framework", "comment": "16 pages", "summary": "Large Language Models (LLMs), such as ChatGPT, are increasingly leveraged for\ngenerating both traditional software code and spreadsheet logic. Despite their\nimpressive generative capabilities, these models frequently exhibit critical\nissues such as hallucinations, subtle logical inconsistencies, and syntactic\nerrors, risks particularly acute in high stakes domains like financial\nmodelling and scientific computations, where accuracy and reliability are\nparamount. This position paper proposes a structured research framework that\nintegrates the proven software engineering practice of Test-Driven Development\n(TDD) with Large Language Model (LLM) driven generation to enhance the\ncorrectness of, reliability of, and user confidence in generated outputs. We\nhypothesise that a \"test first\" methodology provides both technical constraints\nand cognitive scaffolding, guiding LLM outputs towards more accurate,\nverifiable, and comprehensible solutions. Our framework, applicable across\ndiverse programming contexts, from spreadsheet formula generation to scripting\nlanguages such as Python and strongly typed languages like Rust, includes an\nexplicitly outlined experimental design with clearly defined participant\ngroups, evaluation metrics, and illustrative TDD based prompting examples. By\nemphasising test driven thinking, we aim to improve computational thinking,\nprompt engineering skills, and user engagement, particularly benefiting\nspreadsheet users who often lack formal programming training yet face serious\nconsequences from logical errors. We invite collaboration to refine and\nempirically evaluate this approach, ultimately aiming to establish responsible\nand reliable LLM integration in both educational and professional development\npractices.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u5c06\u6d4b\u8bd5\u9a71\u52a8\u5f00\u53d1\uff08TDD\uff09\u4e0e\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u751f\u6210\u76f8\u7ed3\u5408\uff0c\u4ee5\u63d0\u5347\u751f\u6210\u4ee3\u7801\uff08\u5305\u62ec\u7535\u5b50\u8868\u683c\u516c\u5f0f\u548c\u591a\u79cd\u7f16\u7a0b\u8bed\u8a00\uff09\u7684\u51c6\u786e\u6027\u3001\u53ef\u9760\u6027\u548c\u7528\u6237\u4fe1\u5fc3\uff0c\u5c24\u5176\u9762\u5411\u7f3a\u4e4f\u7f16\u7a0b\u8bad\u7ec3\u4f46\u5bf9\u903b\u8f91\u6b63\u786e\u6027\u8981\u6c42\u9ad8\u7684\u7528\u6237\u7fa4\u4f53\u3002", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b\u5728\u751f\u6210\u4ee3\u7801\u65f6\u5b58\u5728\u5e7b\u89c9\u3001\u903b\u8f91\u4e0d\u4e00\u81f4\u548c\u8bed\u6cd5\u9519\u8bef\u7b49\u95ee\u9898\uff0c\u5728\u91d1\u878d\u5efa\u6a21\u548c\u79d1\u5b66\u8ba1\u7b97\u7b49\u9ad8\u98ce\u9669\u9886\u57df\u5c24\u4e3a\u4e25\u91cd\uff0c\u4e9f\u9700\u63d0\u5347\u5176\u8f93\u51fa\u7684\u53ef\u9760\u6027\u4e0e\u53ef\u9a8c\u8bc1\u6027\u3002", "method": "\u63d0\u51fa\u4e00\u4e2a\u7ed3\u5408\u6d4b\u8bd5\u9a71\u52a8\u5f00\u53d1\uff08TDD\uff09\u4e0eLLM\u751f\u6210\u7684\u7ed3\u6784\u5316\u7814\u7a76\u6846\u67b6\uff0c\u91c7\u7528\u201c\u5148\u5199\u6d4b\u8bd5\u201d\u7684\u65b9\u6cd5\u5bf9LLM\u8f93\u51fa\u8fdb\u884c\u6280\u672f\u7ea6\u675f\u548c\u8ba4\u77e5\u5f15\u5bfc\uff0c\u5e76\u8bbe\u8ba1\u4e86\u5305\u542b\u53c2\u4e0e\u8005\u5206\u7ec4\u3001\u8bc4\u4f30\u6307\u6807\u548c\u63d0\u793a\u793a\u4f8b\u7684\u5b9e\u9a8c\u65b9\u6848\u3002", "result": "\u8be5\u6846\u67b6\u9002\u7528\u4e8e\u7535\u5b50\u8868\u683c\u3001Python\u3001Rust\u7b49\u591a\u79cd\u7f16\u7a0b\u573a\u666f\uff0c\u6709\u671b\u63d0\u5347\u7528\u6237\u7684\u8ba1\u7b97\u601d\u7ef4\u3001\u63d0\u793a\u5de5\u7a0b\u80fd\u529b\u548c\u53c2\u4e0e\u5ea6\uff0c\u5c24\u5176\u5e2e\u52a9\u975e\u4e13\u4e1a\u7f16\u7a0b\u7528\u6237\u51cf\u5c11\u903b\u8f91\u9519\u8bef\u3002", "conclusion": "\u901a\u8fc7\u5f3a\u8c03\u6d4b\u8bd5\u9a71\u52a8\u601d\u7ef4\uff0c\u8be5\u65b9\u6cd5\u6709\u671b\u5728\u6559\u80b2\u548c\u4e13\u4e1a\u5f00\u53d1\u5b9e\u8df5\u4e2d\u5b9e\u73b0\u66f4\u8d1f\u8d23\u4efb\u3001\u53ef\u9760\u7684LLM\u96c6\u6210\uff0c\u4f5c\u8005\u547c\u5401\u5408\u4f5c\u4ee5\u8fdb\u4e00\u6b65\u5b8c\u5584\u548c\u5b9e\u8bc1\u8bc4\u4f30\u8be5\u6846\u67b6\u3002"}}
{"id": "2510.15485", "categories": ["cs.DC", "cs.DB", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2510.15485", "abs": "https://arxiv.org/abs/2510.15485", "authors": ["D\u0101vis Ka\u017eemaks", "Laurens Versluis", "Burcu Kulahcioglu Ozkan", "J\u00e9r\u00e9mie Decouchant"], "title": "Balancing Fairness and Performance in Multi-User Spark Workloads with Dynamic Scheduling (extended version)", "comment": "This paper is an extended version of a paper accepted at the ACM\n  Symposium on Cloud Computing (SoCC'25) that contains a proof of correctness", "summary": "Apache Spark is a widely adopted framework for large-scale data processing.\nHowever, in industrial analytics environments, Spark's built-in schedulers,\nsuch as FIFO and fair scheduling, struggle to maintain both user-level fairness\nand low mean response time, particularly in long-running shared applications.\nExisting solutions typically focus on job-level fairness which unintentionally\nfavors users who submit more jobs. Although Spark offers a built-in fair\nscheduler, it lacks adaptability to dynamic user workloads and may degrade\noverall job performance. We present the User Weighted Fair Queuing (UWFQ)\nscheduler, designed to minimize job response times while ensuring equitable\nresource distribution across users and their respective jobs. UWFQ simulates a\nvirtual fair queuing system and schedules jobs based on their estimated finish\ntimes under a bounded fairness model. To further address task skew and reduce\npriority inversions, which are common in Spark workloads, we introduce runtime\npartitioning, a method that dynamically refines task granularity based on\nexpected runtime. We implement UWFQ within the Spark framework and evaluate its\nperformance using multi-user synthetic workloads and Google cluster traces. We\nshow that UWFQ reduces the average response time of small jobs by up to 74%\ncompared to existing built-in Spark schedulers and to state-of-the-art fair\nscheduling algorithms.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u7528\u6237\u52a0\u6743\u516c\u5e73\u961f\u5217\uff08UWFQ\uff09\u7684\u65b0\u578b\u8c03\u5ea6\u5668\uff0c\u7528\u4e8eApache Spark\uff0c\u65e8\u5728\u5728\u591a\u7528\u6237\u5171\u4eab\u73af\u5883\u4e2d\u540c\u65f6\u5b9e\u73b0\u7528\u6237\u7ea7\u516c\u5e73\u6027\u548c\u4f4e\u5e73\u5747\u54cd\u5e94\u65f6\u95f4\u3002UWFQ\u901a\u8fc7\u6a21\u62df\u865a\u62df\u516c\u5e73\u961f\u5217\u5e76\u57fa\u4e8e\u4f30\u8ba1\u5b8c\u6210\u65f6\u95f4\u8c03\u5ea6\u4f5c\u4e1a\uff0c\u7ed3\u5408\u8fd0\u884c\u65f6\u52a8\u6001\u5206\u533a\u7b56\u7565\u7f13\u89e3\u4efb\u52a1\u503e\u659c\u548c\u4f18\u5148\u7ea7\u53cd\u8f6c\u95ee\u9898\uff0c\u5728\u5b9e\u9a8c\u4e2d\u5c06\u5c0f\u4f5c\u4e1a\u7684\u5e73\u5747\u54cd\u5e94\u65f6\u95f4\u6700\u591a\u964d\u4f4e\u4e8674%\u3002", "motivation": "\u73b0\u6709Spark\u5185\u7f6e\u8c03\u5ea6\u5668\uff08\u5982FIFO\u548c\u516c\u5e73\u8c03\u5ea6\uff09\u5728\u5de5\u4e1a\u5206\u6790\u73af\u5883\u4e2d\u96be\u4ee5\u517c\u987e\u7528\u6237\u7ea7\u516c\u5e73\u6027\u4e0e\u4f4e\u5e73\u5747\u54cd\u5e94\u65f6\u95f4\uff0c\u5c24\u5176\u5728\u957f\u671f\u8fd0\u884c\u7684\u5171\u4eab\u5e94\u7528\u4e2d\u3002\u5f53\u524d\u65b9\u6848\u591a\u5173\u6ce8\u4f5c\u4e1a\u7ea7\u516c\u5e73\uff0c\u65e0\u610f\u4e2d\u504f\u5411\u63d0\u4ea4\u66f4\u591a\u4f5c\u4e1a\u7684\u7528\u6237\uff0c\u4e14\u7f3a\u4e4f\u5bf9\u52a8\u6001\u7528\u6237\u8d1f\u8f7d\u7684\u9002\u5e94\u6027\u3002", "method": "\u63d0\u51fa\u7528\u6237\u52a0\u6743\u516c\u5e73\u961f\u5217\uff08UWFQ\uff09\u8c03\u5ea6\u5668\uff0c\u6a21\u62df\u865a\u62df\u516c\u5e73\u961f\u5217\uff0c\u57fa\u4e8e\u4f5c\u4e1a\u5728\u6709\u754c\u516c\u5e73\u6a21\u578b\u4e0b\u7684\u4f30\u8ba1\u5b8c\u6210\u65f6\u95f4\u8fdb\u884c\u8c03\u5ea6\uff1b\u540c\u65f6\u5f15\u5165\u8fd0\u884c\u65f6\u5206\u533a\u65b9\u6cd5\uff0c\u6839\u636e\u4efb\u52a1\u9884\u671f\u8fd0\u884c\u65f6\u95f4\u52a8\u6001\u8c03\u6574\u4efb\u52a1\u7c92\u5ea6\uff0c\u4ee5\u7f13\u89e3\u4efb\u52a1\u503e\u659c\u548c\u4f18\u5148\u7ea7\u53cd\u8f6c\u3002", "result": "\u5728Spark\u6846\u67b6\u4e2d\u5b9e\u73b0UWFQ\uff0c\u5e76\u901a\u8fc7\u591a\u7528\u6237\u5408\u6210\u5de5\u4f5c\u8d1f\u8f7d\u548cGoogle\u96c6\u7fa4\u8f68\u8ff9\u8fdb\u884c\u8bc4\u4f30\uff0c\u7ed3\u679c\u8868\u660eUWFQ\u76f8\u6bd4\u73b0\u6709Spark\u5185\u7f6e\u8c03\u5ea6\u5668\u548c\u5148\u8fdb\u516c\u5e73\u8c03\u5ea6\u7b97\u6cd5\uff0c\u6700\u591a\u53ef\u5c06\u5c0f\u4f5c\u4e1a\u7684\u5e73\u5747\u54cd\u5e94\u65f6\u95f4\u964d\u4f4e74%\u3002", "conclusion": "UWFQ\u6709\u6548\u89e3\u51b3\u4e86Spark\u5728\u591a\u7528\u6237\u5171\u4eab\u573a\u666f\u4e0b\u516c\u5e73\u6027\u4e0e\u54cd\u5e94\u65f6\u95f4\u4e4b\u95f4\u7684\u6743\u8861\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5c0f\u4f5c\u4e1a\u7684\u54cd\u5e94\u6027\u80fd\uff0c\u540c\u65f6\u4fdd\u969c\u4e86\u7528\u6237\u95f4\u7684\u8d44\u6e90\u516c\u5e73\u5206\u914d\u3002"}}
{"id": "2510.15642", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.15642", "abs": "https://arxiv.org/abs/2510.15642", "authors": ["Sian Brooke"], "title": "Interact and React: Exploring Gender Patterns in Development and the Impact on Innovation and Robustness of a User Interface Tool", "comment": "Published in AoIR 2025", "summary": "In open-source software design, the inclusion of women is often highlighted\nsimply to remind programmers that women exist. Yet, little attention is given\nto how greater gender diversity, specifically women's participation, could\nfundamentally alter development patterns. To understand the potential impact of\ngender inclusion, this study investigates React, a widely used JavaScript\nlibrary for building user interfaces with an active contributor community. I\nexamine gender differences in metrics of robustness and innovation, as well as\nshifts in contribution patterns leading up to major version releases over 11\nyears of the React project. My results show that the exclusion of women is\ndetrimental to software as women contribute significantly more to feature\nenhancement and dependency management. By exploring how gender influences\ninnovation and robustness in the development of React, the study offers\ncritical insights into how increasing gender diversity could lead to more\ninclusive, innovative, and robust software.", "AI": {"tldr": "\u8be5\u7814\u7a76\u5206\u6790\u4e86React\u9879\u76ee11\u5e74\u6765\u7684\u8d21\u732e\u6570\u636e\uff0c\u53d1\u73b0\u5973\u6027\u5728\u529f\u80fd\u589e\u5f3a\u548c\u4f9d\u8d56\u7ba1\u7406\u65b9\u9762\u8d21\u732e\u663e\u8457\u66f4\u591a\uff0c\u8868\u660e\u6027\u522b\u591a\u6837\u6027\u6709\u52a9\u4e8e\u63d0\u5347\u8f6f\u4ef6\u7684\u521b\u65b0\u6027\u4e0e\u7a33\u5065\u6027\u3002", "motivation": "\u63a2\u8ba8\u6027\u522b\u591a\u6837\u6027\uff08\u5c24\u5176\u662f\u5973\u6027\u53c2\u4e0e\uff09\u5982\u4f55\u6839\u672c\u6027\u5730\u6539\u53d8\u5f00\u6e90\u8f6f\u4ef6\u7684\u5f00\u53d1\u6a21\u5f0f\uff0c\u800c\u4e0d\u4ec5\u505c\u7559\u5728\u8c61\u5f81\u6027\u5305\u5bb9\u5c42\u9762\u3002", "method": "\u5bf9React\u9879\u76ee11\u5e74\u95f4\u7684\u8d21\u732e\u6570\u636e\u8fdb\u884c\u5206\u6790\uff0c\u6bd4\u8f83\u4e0d\u540c\u6027\u522b\u5728\u7a33\u5065\u6027\u3001\u521b\u65b0\u6027\u6307\u6807\u53ca\u91cd\u5927\u7248\u672c\u53d1\u5e03\u524d\u7684\u8d21\u732e\u6a21\u5f0f\u5dee\u5f02\u3002", "result": "\u5973\u6027\u5728\u529f\u80fd\u589e\u5f3a\u548c\u4f9d\u8d56\u7ba1\u7406\u65b9\u9762\u8d21\u732e\u663e\u8457\u66f4\u591a\uff0c\u6392\u9664\u5973\u6027\u5bf9\u8f6f\u4ef6\u8d28\u91cf\u6709\u5bb3\u3002", "conclusion": "\u63d0\u5347\u6027\u522b\u591a\u6837\u6027\u53ef\u4fc3\u8fdb\u8f6f\u4ef6\u66f4\u5177\u5305\u5bb9\u6027\u3001\u521b\u65b0\u6027\u548c\u7a33\u5065\u6027\u3002"}}
{"id": "2510.15690", "categories": ["cs.SE", "cs.CR"], "pdf": "https://arxiv.org/pdf/2510.15690", "abs": "https://arxiv.org/abs/2510.15690", "authors": ["Shiwen Ou", "Yuwei Li", "Lu Yu", "Chengkun Wei", "Tingke Wen", "Qiangpu Chen", "Yu Chen", "Haizhi Tang", "Zulie Pan"], "title": "MirrorFuzz: Leveraging LLM and Shared Bugs for Deep Learning Framework APIs Fuzzing", "comment": "Accepted for publication in IEEE Transactions on Software Engineering\n  (TSE), 2025", "summary": "Deep learning (DL) frameworks serve as the backbone for a wide range of\nartificial intelligence applications. However, bugs within DL frameworks can\ncascade into critical issues in higher-level applications, jeopardizing\nreliability and security. While numerous techniques have been proposed to\ndetect bugs in DL frameworks, research exploring common API patterns across\nframeworks and the potential risks they entail remains limited. Notably, many\nDL frameworks expose similar APIs with overlapping input parameters and\nfunctionalities, rendering them vulnerable to shared bugs, where a flaw in one\nAPI may extend to analogous APIs in other frameworks. To address this\nchallenge, we propose MirrorFuzz, an automated API fuzzing solution to discover\nshared bugs in DL frameworks. MirrorFuzz operates in three stages: First,\nMirrorFuzz collects historical bug data for each API within a DL framework to\nidentify potentially buggy APIs. Second, it matches each buggy API in a\nspecific framework with similar APIs within and across other DL frameworks.\nThird, it employs large language models (LLMs) to synthesize code for the API\nunder test, leveraging the historical bug data of similar APIs to trigger\nanalogous bugs across APIs. We implement MirrorFuzz and evaluate it on four\npopular DL frameworks (TensorFlow, PyTorch, OneFlow, and Jittor). Extensive\nevaluation demonstrates that MirrorFuzz improves code coverage by 39.92\\% and\n98.20\\% compared to state-of-the-art methods on TensorFlow and PyTorch,\nrespectively. Moreover, MirrorFuzz discovers 315 bugs, 262 of which are newly\nfound, and 80 bugs are fixed, with 52 of these bugs assigned CNVD IDs.", "AI": {"tldr": "MirrorFuzz \u662f\u4e00\u79cd\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u81ea\u52a8\u5316 API \u6a21\u7cca\u6d4b\u8bd5\u5de5\u5177\uff0c\u7528\u4e8e\u53d1\u73b0\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\u4e2d\u7684\u5171\u4eab\u6f0f\u6d1e\uff0c\u663e\u8457\u63d0\u5347\u4ee3\u7801\u8986\u76d6\u7387\u5e76\u6210\u529f\u68c0\u6d4b\u51fa\u5927\u91cf\u65b0\u6f0f\u6d1e\u3002", "motivation": "\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\u4e2d\u7684\u6f0f\u6d1e\u53ef\u80fd\u5f15\u53d1\u4e0a\u5c42\u5e94\u7528\u7684\u4e25\u91cd\u95ee\u9898\uff0c\u800c\u73b0\u6709\u7814\u7a76\u8f83\u5c11\u5173\u6ce8\u8de8\u6846\u67b6\u7684\u901a\u7528 API \u6a21\u5f0f\u53ca\u5176\u6f5c\u5728\u98ce\u9669\u3002\u7531\u4e8e\u591a\u4e2a\u6846\u67b6\u63d0\u4f9b\u529f\u80fd\u548c\u53c2\u6570\u76f8\u4f3c\u7684 API\uff0c\u4e00\u4e2a\u6846\u67b6\u4e2d\u7684\u6f0f\u6d1e\u53ef\u80fd\u5728\u5176\u4ed6\u6846\u67b6\u4e2d\u91cd\u590d\u51fa\u73b0\uff0c\u56e0\u6b64\u4e9f\u9700\u4e00\u79cd\u80fd\u7cfb\u7edf\u6027\u53d1\u73b0\u8fd9\u7c7b\u5171\u4eab\u6f0f\u6d1e\u7684\u65b9\u6cd5\u3002", "method": "MirrorFuzz \u5206\u4e09\u6b65\u5de5\u4f5c\uff1a\u9996\u5148\u6536\u96c6\u5404 API \u7684\u5386\u53f2\u6f0f\u6d1e\u6570\u636e\u4ee5\u8bc6\u522b\u6f5c\u5728\u6613\u9519 API\uff1b\u5176\u6b21\u5728\u6846\u67b6\u5185\u53ca\u8de8\u6846\u67b6\u5339\u914d\u76f8\u4f3c API\uff1b\u6700\u540e\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\u7ed3\u5408\u5386\u53f2\u6f0f\u6d1e\u4fe1\u606f\u751f\u6210\u6d4b\u8bd5\u4ee3\u7801\uff0c\u89e6\u53d1\u76f8\u4f3c API \u4e2d\u7684\u540c\u7c7b\u6f0f\u6d1e\u3002", "result": "\u5728 TensorFlow\u3001PyTorch\u3001OneFlow \u548c Jittor \u56db\u4e2a\u4e3b\u6d41\u6846\u67b6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cMirrorFuzz \u76f8\u6bd4\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5\u5728 TensorFlow \u548c PyTorch \u4e0a\u5206\u522b\u63d0\u5347\u4ee3\u7801\u8986\u76d6\u7387 39.92% \u548c 98.20%\uff1b\u5171\u53d1\u73b0 315 \u4e2a\u6f0f\u6d1e\uff0c\u5176\u4e2d 262 \u4e2a\u4e3a\u65b0\u6f0f\u6d1e\uff0c80 \u4e2a\u5df2\u88ab\u4fee\u590d\uff0c52 \u4e2a\u83b7\u5f97 CNVD \u7f16\u53f7\u3002", "conclusion": "MirrorFuzz \u80fd\u6709\u6548\u8bc6\u522b\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\u4e2d\u56e0 API \u76f8\u4f3c\u6027\u5bfc\u81f4\u7684\u5171\u4eab\u6f0f\u6d1e\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u6a21\u7cca\u6d4b\u8bd5\u65b9\u6cd5\uff0c\u4e3a\u63d0\u5347 DL \u6846\u67b6\u7684\u53ef\u9760\u6027\u4e0e\u5b89\u5168\u6027\u63d0\u4f9b\u4e86\u5b9e\u7528\u5de5\u5177\u3002"}}
{"id": "2510.15767", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.15767", "abs": "https://arxiv.org/abs/2510.15767", "authors": ["Rathi Adarshi Rammohan", "Moritz Meier", "Dennis K\u00fcster", "Tanja Schultz"], "title": "EASELAN: An Open-Source Framework for Multimodal Biosignal Annotation and Data Management", "comment": null, "summary": "Recent advancements in machine learning and adaptive cognitive systems are\ndriving a growing demand for large and richly annotated multimodal data. A\nprominent example of this trend are fusion models, which increasingly\nincorporate multiple biosignals in addition to traditional audiovisual\nchannels. This paper introduces the EASELAN annotation framework to improve\nannotation workflows designed to address the resulting rising complexity of\nmultimodal and biosignals datasets. It builds on the robust ELAN tool by adding\nnew components tailored to support all stages of the annotation pipeline: From\nstreamlining the preparation of annotation files to setting up additional\nchannels, integrated version control with GitHub, and simplified\npost-processing. EASELAN delivers a seamless workflow designed to integrate\nbiosignals and facilitate rich annotations to be readily exported for further\nanalyses and machine learning-supported model training. The EASELAN framework\nis successfully applied to a high-dimensional biosignals collection initiative\non human everyday activities (here, table setting) for cognitive robots within\nthe DFG-funded Collaborative Research Center 1320 Everyday Activity Science and\nEngineering (EASE). In this paper we discuss the opportunities, limitations,\nand lessons learned when using EASELAN for this initiative. To foster research\non biosignal collection, annotation, and processing, the code of EASELAN is\npublicly available(https://github.com/cognitive-systems-lab/easelan), along\nwith the EASELAN-supported fully annotated Table Setting Database.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86EASELAN\u6807\u6ce8\u6846\u67b6\uff0c\u901a\u8fc7\u6269\u5c55ELAN\u5de5\u5177\u4ee5\u652f\u6301\u591a\u6a21\u6001\u4e0e\u751f\u7269\u4fe1\u53f7\u6570\u636e\u7684\u9ad8\u6548\u6807\u6ce8\uff0c\u6db5\u76d6\u4ece\u6570\u636e\u51c6\u5907\u3001\u591a\u901a\u9053\u8bbe\u7f6e\u3001\u7248\u672c\u63a7\u5236\u5230\u540e\u5904\u7406\u7684\u5168\u6d41\u7a0b\uff0c\u5e76\u5df2\u6210\u529f\u5e94\u7528\u4e8e\u65e5\u5e38\u6d3b\u52a8\uff08\u5982\u6446\u684c\uff09\u7684\u9ad8\u7ef4\u751f\u7269\u4fe1\u53f7\u6570\u636e\u96c6\u6784\u5efa\u3002", "motivation": "\u968f\u7740\u878d\u5408\u6a21\u578b\u5bf9\u591a\u6a21\u6001\u53ca\u751f\u7269\u4fe1\u53f7\u6570\u636e\u9700\u6c42\u7684\u589e\u957f\uff0c\u4f20\u7edf\u6807\u6ce8\u5de5\u5177\u96be\u4ee5\u5e94\u5bf9\u65e5\u76ca\u590d\u6742\u7684\u6570\u636e\u6807\u6ce8\u4efb\u52a1\uff0c\u4e9f\u9700\u66f4\u9ad8\u6548\u3001\u96c6\u6210\u5316\u7684\u6807\u6ce8\u6d41\u7a0b\u652f\u6301\u3002", "method": "\u5728ELAN\u5de5\u5177\u57fa\u7840\u4e0a\u5f00\u53d1EASELAN\u6846\u67b6\uff0c\u65b0\u589e\u652f\u6301\u6807\u6ce8\u6587\u4ef6\u51c6\u5907\u3001\u591a\u901a\u9053\u914d\u7f6e\u3001GitHub\u96c6\u6210\u7248\u672c\u63a7\u5236\u53ca\u7b80\u5316\u540e\u5904\u7406\u7b49\u529f\u80fd\uff0c\u5b9e\u73b0\u751f\u7269\u4fe1\u53f7\u4e0e\u591a\u6a21\u6001\u6570\u636e\u7684\u65e0\u7f1d\u6807\u6ce8\u4e0e\u5bfc\u51fa\u3002", "result": "EASELAN\u5df2\u6210\u529f\u5e94\u7528\u4e8eDFG\u8d44\u52a9\u7684EASE\u9879\u76ee\u4e2d\u7684\u4eba\u7c7b\u65e5\u5e38\u6d3b\u52a8\uff08\u6446\u684c\uff09\u9ad8\u7ef4\u751f\u7269\u4fe1\u53f7\u6570\u636e\u91c7\u96c6\u4e0e\u6807\u6ce8\uff0c\u6784\u5efa\u4e86\u5b8c\u6574\u7684\u6807\u6ce8\u6570\u636e\u5e93\uff0c\u5e76\u516c\u5f00\u4e86\u4ee3\u7801\u4e0e\u6570\u636e\u96c6\u3002", "conclusion": "EASELAN\u663e\u8457\u63d0\u5347\u4e86\u591a\u6a21\u6001\u4e0e\u751f\u7269\u4fe1\u53f7\u6570\u636e\u7684\u6807\u6ce8\u6548\u7387\u4e0e\u8d28\u91cf\uff0c\u4e3a\u8ba4\u77e5\u673a\u5668\u4eba\u7b49\u9886\u57df\u63d0\u4f9b\u4e86\u53ef\u590d\u7528\u7684\u6807\u6ce8\u57fa\u7840\u8bbe\u65bd\uff0c\u540c\u65f6\u901a\u8fc7\u5f00\u6e90\u4fc3\u8fdb\u76f8\u5173\u7814\u7a76\u53d1\u5c55\u3002"}}
{"id": "2510.15794", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.15794", "abs": "https://arxiv.org/abs/2510.15794", "authors": ["Rachna Raj", "Diego Elias Costa"], "title": "Towards Supporting Open Source Library Maintainers with Community-Based Analytics", "comment": null, "summary": "Open-source software (OSS) is a pillar of modern software development. Its\nsuccess depends on the dedication of maintainers who work constantly to keep\ntheir libraries stable, adapt to changing needs, and support a growing\ncommunity. Yet, they receive little to no continuous feedback on how the\nprojects that rely on their libraries actually use their APIs. We believe that\ngaining these insights can help maintainers make better decisions, such as\nrefining testing strategies, understanding the impact of changes, and guiding\nthe evolution of their libraries more effectively. We propose the use of\ncommunity-based analytics to analyze how an OSS library is used across its\ndependent ecosystem. We conduct an empirical study of 10 popular Java libraries\nand each with their respective dependent ecosystem of 50 projects. Our results\nreveal that while library developers offer a wide range of API methods, only\n16% on average are actively used by their dependent ecosystem. Moreover, only\n74% of the used API methods are partially or fully covered by their library\ntest suite. We propose two metrics to help developers evaluate their test suite\naccording to the APIs used by their community, and we conduct a survey on\nopen-source practitioners to assess the practical value of these insights in\nguiding maintenance decisions.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u901a\u8fc7\u793e\u533a\u4f7f\u7528\u6570\u636e\u5206\u6790\u5f00\u6e90\u5e93\u7684\u5b9e\u9645API\u4f7f\u7528\u60c5\u51b5\uff0c\u53d1\u73b0\u4ec5\u670916%\u7684API\u88ab\u4f9d\u8d56\u9879\u76ee\u5e7f\u6cdb\u4f7f\u7528\uff0c\u4e14\u5176\u4e2d\u4ec574%\u88ab\u5e93\u81ea\u8eab\u7684\u6d4b\u8bd5\u8986\u76d6\uff0c\u5e76\u63d0\u51fa\u4e86\u4e24\u4e2a\u65b0\u6307\u6807\u5e2e\u52a9\u7ef4\u62a4\u8005\u4f18\u5316\u6d4b\u8bd5\u4e0e\u7ef4\u62a4\u7b56\u7565\u3002", "motivation": "\u5f00\u6e90\u8f6f\u4ef6\u7ef4\u62a4\u8005\u7f3a\u4e4f\u5bf9\u5176API\u5728\u4f9d\u8d56\u9879\u76ee\u4e2d\u5b9e\u9645\u4f7f\u7528\u60c5\u51b5\u7684\u6301\u7eed\u53cd\u9988\uff0c\u9650\u5236\u4e86\u5176\u5728\u6d4b\u8bd5\u3001\u53d8\u66f4\u5f71\u54cd\u8bc4\u4f30\u548c\u5e93\u6f14\u8fdb\u65b9\u9762\u7684\u51b3\u7b56\u80fd\u529b\u3002", "method": "\u5bf910\u4e2a\u6d41\u884c\u7684Java\u5f00\u6e90\u5e93\u53ca\u5176\u5404\u81ea50\u4e2a\u4f9d\u8d56\u9879\u76ee\u8fdb\u884c\u5b9e\u8bc1\u7814\u7a76\uff0c\u5206\u6790API\u4f7f\u7528\u60c5\u51b5\u548c\u6d4b\u8bd5\u8986\u76d6\u7387\uff0c\u5e76\u63d0\u51fa\u4e24\u4e2a\u65b0\u6307\u6807\uff1b\u540c\u65f6\u901a\u8fc7\u95ee\u5377\u8c03\u67e5\u8bc4\u4f30\u8fd9\u4e9b\u6d1e\u5bdf\u5bf9\u7ef4\u62a4\u5b9e\u8df5\u7684\u4ef7\u503c\u3002", "result": "\u5e73\u5747\u4ec5\u670916%\u7684API\u65b9\u6cd5\u88ab\u4f9d\u8d56\u751f\u6001\u7cfb\u7edf\u5b9e\u9645\u4f7f\u7528\uff0c\u4e14\u8fd9\u4e9b\u88ab\u4f7f\u7528\u7684API\u4e2d\u4ec5\u670974%\u88ab\u5e93\u7684\u6d4b\u8bd5\u5957\u4ef6\u90e8\u5206\u6216\u5b8c\u5168\u8986\u76d6\u3002", "conclusion": "\u793e\u533a\u9a71\u52a8\u7684\u4f7f\u7528\u6570\u636e\u5206\u6790\u80fd\u4e3a\u5f00\u6e90\u5e93\u7ef4\u62a4\u8005\u63d0\u4f9b\u6709\u4ef7\u503c\u7684\u53cd\u9988\uff0c\u6709\u52a9\u4e8e\u4f18\u5316\u6d4b\u8bd5\u7b56\u7565\u548c\u6307\u5bfc\u5e93\u7684\u6f14\u8fdb\u3002"}}
{"id": "2510.15698", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2510.15698", "abs": "https://arxiv.org/abs/2510.15698", "authors": ["Sebastian Brandt", "Tim G\u00f6ttlicher"], "title": "A Post-Quantum Lower Bound for the Distributed Lov\u00e1sz Local Lemma", "comment": "46 pages, 3 figures", "summary": "In this work, we study the Lov\\'asz local lemma (LLL) problem in the area of\ndistributed quantum computing, which has been the focus of attention of recent\nadvances in quantum computing [STOC'24, STOC'25, STOC'25]. We prove a lower\nbound of $2^{\\Omega(\\log^* n)}$ for the complexity of the distributed LLL in\nthe quantum-LOCAL model. More specifically, we obtain our lower bound already\nfor a very well-studied special case of the LLL, called sinkless orientation,\nin a stronger model than quantum-LOCAL, called the randomized online-LOCAL\nmodel. As a consequence, we obtain the same lower bounds for sinkless\norientation and the distributed LLL also in a variety of other models studied\nacross different research communities.\n  Our work provides the first superconstant lower bound for sinkless\norientation and the distributed LLL in all of these models, addressing recently\nstated open questions. Moreover, to obtain our results, we develop an entirely\nnew lower bound technique that we believe has the potential to become the first\ngeneric technique for proving post-quantum lower bounds for many of the most\nimportant problems studied in the context of locality.", "AI": {"tldr": "\u672c\u6587\u5728\u5206\u5e03\u5f0f\u91cf\u5b50\u8ba1\u7b97\u80cc\u666f\u4e0b\u7814\u7a76\u4e86Lov\u00e1sz\u5c40\u90e8\u5f15\u7406\uff08LLL\uff09\u95ee\u9898\uff0c\u8bc1\u660e\u4e86\u5728\u91cf\u5b50-LOCAL\u6a21\u578b\u4e2d\u5206\u5e03\u5f0fLLL\u7684\u590d\u6742\u5ea6\u4e0b\u754c\u4e3a $2^{\\Omega(\\log^* n)}$\uff0c\u8be5\u4e0b\u754c\u751a\u81f3\u9002\u7528\u4e8e\u66f4\u5f3a\u7684\u968f\u673a\u5728\u7ebf-LOCAL\u6a21\u578b\u4e2d\u7684sinkless orientation\u95ee\u9898\uff0c\u5e76\u4e3a\u591a\u4e2a\u76f8\u5173\u6a21\u578b\u63d0\u4f9b\u4e86\u9996\u4e2a\u8d85\u5e38\u6570\u4e0b\u754c\uff0c\u540c\u65f6\u63d0\u51fa\u4e86\u4e00\u79cd\u5168\u65b0\u7684\u4e0b\u754c\u8bc1\u660e\u6280\u672f\u3002", "motivation": "\u8fd1\u671f\u91cf\u5b50\u8ba1\u7b97\u7684\u8fdb\u5c55\u4fc3\u4f7f\u7814\u7a76\u8005\u5173\u6ce8\u5206\u5e03\u5f0f\u91cf\u5b50\u8ba1\u7b97\u4e2d\u7684Lov\u00e1sz\u5c40\u90e8\u5f15\u7406\uff08LLL\uff09\u95ee\u9898\uff0c\u800c\u6b64\u524d\u5728\u591a\u79cd\u6a21\u578b\u4e2d\u7f3a\u4e4f\u5bf9LLL\u53ca\u5176\u7279\u4f8bsinkless orientation\u7684\u8d85\u5e38\u6570\u4e0b\u754c\u7ed3\u679c\uff0c\u56e0\u6b64\u672c\u6587\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7406\u8bba\u7a7a\u767d\u5e76\u56de\u5e94\u8fd1\u671f\u63d0\u51fa\u7684\u5f00\u653e\u95ee\u9898\u3002", "method": "\u4f5c\u8005\u901a\u8fc7\u5728\u6bd4\u91cf\u5b50-LOCAL\u66f4\u5f3a\u7684\u968f\u673a\u5728\u7ebf-LOCAL\u6a21\u578b\u4e2d\u5206\u6790sinkless orientation\u8fd9\u4e00\u7ecf\u5178LLL\u7279\u4f8b\uff0c\u6784\u5efa\u4e86\u4e00\u4e2a\u5168\u65b0\u7684\u4e0b\u754c\u8bc1\u660e\u6280\u672f\uff0c\u4ece\u800c\u63a8\u5bfc\u51fa\u5206\u5e03\u5f0fLLL\u5728\u591a\u79cd\u6a21\u578b\u4e2d\u7684\u590d\u6742\u5ea6\u4e0b\u754c\u3002", "result": "\u8bc1\u660e\u4e86\u5728\u91cf\u5b50-LOCAL\u6a21\u578b\u53ca\u66f4\u5e7f\u6cdb\u7684\u6a21\u578b\u4e2d\uff0csinkless orientation\u548c\u5206\u5e03\u5f0fLLL\u7684\u590d\u6742\u5ea6\u4e0b\u754c\u4e3a $2^{\\Omega(\\log^* n)}$\uff0c\u8fd9\u662f\u8fd9\u4e9b\u6a21\u578b\u4e2d\u9996\u4e2a\u8d85\u5e38\u6570\u4e0b\u754c\u3002", "conclusion": "\u672c\u6587\u4e0d\u4ec5\u89e3\u51b3\u4e86\u5173\u4e8e\u5206\u5e03\u5f0fLLL\u548csinkless orientation\u4e0b\u754c\u7684\u5f00\u653e\u95ee\u9898\uff0c\u8fd8\u63d0\u51fa\u4e86\u4e00\u79cd\u6709\u6f5c\u529b\u6210\u4e3a\u901a\u7528\u65b9\u6cd5\u7684\u65b0\u4e0b\u754c\u6280\u672f\uff0c\u53ef\u7528\u4e8e\u672a\u6765\u4f17\u591a\u5c40\u90e8\u6027\u76f8\u5173\u95ee\u9898\u7684\u540e\u91cf\u5b50\u4e0b\u754c\u7814\u7a76\u3002"}}
{"id": "2510.15755", "categories": ["cs.DC", "cs.OS"], "pdf": "https://arxiv.org/pdf/2510.15755", "abs": "https://arxiv.org/abs/2510.15755", "authors": ["Atsushi Koshiba", "Charalampos Mainas", "Pramod Bhatotia"], "title": "Funky: Cloud-Native FPGA Virtualization and Orchestration", "comment": "17 pages, ACM Symposium on Cloud Computing (SoCC'25)", "summary": "The adoption of FPGAs in cloud-native environments is facing impediments due\nto FPGA limitations and CPU-oriented design of orchestrators, as they lack\nvirtualization, isolation, and preemption support for FPGAs. Consequently,\ncloud providers offer no orchestration services for FPGAs, leading to low\nscalability, flexibility, and resiliency.\n  This paper presents Funky, a full-stack FPGA-aware orchestration engine for\ncloud-native applications. Funky offers primary orchestration services for FPGA\nworkloads to achieve high performance, utilization, scalability, and fault\ntolerance, accomplished by three contributions: (1) FPGA virtualization for\nlightweight sandboxes, (2) FPGA state management enabling task preemption and\ncheckpointing, and (3) FPGA-aware orchestration components following the\nindustry-standard CRI/OCI specifications.\n  We implement and evaluate Funky using four x86 servers with Alveo U50 FPGA\ncards. Our evaluation highlights that Funky allows us to port 23 OpenCL\napplications from the Xilinx Vitis and Rosetta benchmark suites by modifying\n3.4% of the source code while keeping the OCI image sizes 28.7 times smaller\nthan AMD's FPGA-accessible Docker containers. In addition, Funky incurs only\n7.4% performance overheads compared to native execution, while providing\nvirtualization support with strong hypervisor-enforced isolation and\ncloud-native orchestration for a set of distributed FPGAs. Lastly, we evaluate\nFunky's orchestration services in a large-scale cluster using Google production\ntraces, showing its scalability, fault tolerance, and scheduling efficiency.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86Funky\uff0c\u4e00\u79cd\u9762\u5411\u4e91\u539f\u751f\u73af\u5883\u7684\u5168\u6808FPGA\u611f\u77e5\u7f16\u6392\u5f15\u64ce\uff0c\u901a\u8fc7FPGA\u865a\u62df\u5316\u3001\u72b6\u6001\u7ba1\u7406\u548c\u7b26\u5408CRI/OCI\u6807\u51c6\u7684\u7f16\u6392\u7ec4\u4ef6\uff0c\u89e3\u51b3\u4e86\u5f53\u524d\u4e91\u73af\u5883\u4e2dFPGA\u7f3a\u4e4f\u865a\u62df\u5316\u3001\u9694\u79bb\u548c\u62a2\u5360\u652f\u6301\u7684\u95ee\u9898\u3002", "motivation": "\u7531\u4e8eFPGA\u81ea\u8eab\u9650\u5236\u4ee5\u53ca\u73b0\u6709\u7f16\u6392\u5668\u4ee5CPU\u4e3a\u4e2d\u5fc3\u7684\u8bbe\u8ba1\uff0c\u5f53\u524d\u4e91\u73af\u5883\u7f3a\u4e4f\u5bf9FPGA\u7684\u865a\u62df\u5316\u3001\u9694\u79bb\u548c\u62a2\u5360\u652f\u6301\uff0c\u5bfc\u81f4\u4e91\u670d\u52a1\u5546\u65e0\u6cd5\u63d0\u4f9bFPGA\u7f16\u6392\u670d\u52a1\uff0c\u9650\u5236\u4e86FPGA\u5728\u4e91\u539f\u751f\u573a\u666f\u4e2d\u7684\u53ef\u6269\u5c55\u6027\u3001\u7075\u6d3b\u6027\u548c\u5f39\u6027\u3002", "method": "Funky\u901a\u8fc7\u4e09\u9879\u5173\u952e\u6280\u672f\u5b9e\u73b0FPGA\u611f\u77e5\u7f16\u6392\uff1a(1) \u8f7b\u91cf\u7ea7\u6c99\u7bb1\u7684FPGA\u865a\u62df\u5316\uff1b(2) \u652f\u6301\u4efb\u52a1\u62a2\u5360\u548c\u68c0\u67e5\u70b9\u7684FPGA\u72b6\u6001\u7ba1\u7406\uff1b(3) \u9075\u5faaCRI/OCI\u884c\u4e1a\u6807\u51c6\u7684FPGA\u611f\u77e5\u7f16\u6392\u7ec4\u4ef6\u3002", "result": "\u5728\u56db\u53f0\u914d\u5907Alveo U50 FPGA\u5361\u7684x86\u670d\u52a1\u5668\u4e0a\u8bc4\u4f30\u8868\u660e\uff0cFunky\u4ec5\u9700\u4fee\u65393.4%\u7684\u6e90\u4ee3\u7801\u5373\u53ef\u79fb\u690d23\u4e2aOpenCL\u5e94\u7528\uff0cOCI\u955c\u50cf\u4f53\u79ef\u6bd4AMD\u65b9\u6848\u5c0f28.7\u500d\uff0c\u6027\u80fd\u5f00\u9500\u4ec57.4%\uff0c\u5e76\u652f\u6301\u5f3a\u9694\u79bb\u865a\u62df\u5316\u548c\u5206\u5e03\u5f0fFPGA\u7f16\u6392\u3002\u57fa\u4e8eGoogle\u751f\u4ea7\u8ffd\u8e2a\u7684\u5927\u89c4\u6a21\u96c6\u7fa4\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u53ef\u6269\u5c55\u6027\u3001\u5bb9\u9519\u6027\u548c\u8c03\u5ea6\u6548\u7387\u3002", "conclusion": "Funky\u6709\u6548\u89e3\u51b3\u4e86\u4e91\u539f\u751f\u73af\u5883\u4e2dFPGA\u7f16\u6392\u7684\u5173\u952e\u6311\u6218\uff0c\u5728\u4fdd\u6301\u63a5\u8fd1\u539f\u751f\u6027\u80fd\u7684\u540c\u65f6\uff0c\u63d0\u4f9b\u4e86\u8f7b\u91cf\u3001\u5b89\u5168\u3001\u53ef\u6269\u5c55\u7684FPGA\u8d44\u6e90\u7ba1\u7406\u80fd\u529b\uff0c\u4e3aFPGA\u5728\u4e91\u4e2d\u7684\u5e7f\u6cdb\u5e94\u7528\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
