{"id": "2511.02230", "categories": ["cs.OS", "cs.AI", "cs.NI"], "pdf": "https://arxiv.org/pdf/2511.02230", "abs": "https://arxiv.org/abs/2511.02230", "authors": ["Hanchen Li", "Qiuyang Mang", "Runyuan He", "Qizheng Zhang", "Huanzhi Mao", "Xiaokun Chen", "Alvin Cheung", "Joseph Gonzalez", "Ion Stoica"], "title": "Continuum: Efficient and Robust Multi-Turn LLM Agent Scheduling with KV Cache Time-to-Live", "comment": null, "summary": "Agentic LLM applications interleave LLM generation requests with tool calls.\nThese tool calls break the continuity of the workflow by creating pauses\nbetween LLM requests, bringing many challenges for the serving system,\nespecially under multi-turn scenarios. Each pause potentially causes KV cache\neviction and extra waiting time before entering the continuous batch for the\nfollowing LLM request. Since these pauses happen for each call, this problem\nbecomes increasingly severe as turn number grow for agentic programs. Previous\nworks either fail to incorporate information from the tool call, evicting KV\ncache that leads to repetitive prefill or loading, or ignore the continuity of\na multi-turn program, creating waiting time between turns that increases\nper-request latency.\n  We present Continuum, a serving system to optimize job completion time for\nmulti-turn agent workloads by combining tool-aware KV cache timeout with\nprogram-level scheduling. By predicting tool call durations in agentic\nworkflows, Continuum selectively pins the KV cache in GPU memory with a\ntime-to-live value based on total turn number. When combined with program-level\nfirst-come-first-serve, Continuum prevents scheduling bubbles, preserves\nmulti-turn continuity, and optimizes for throughput for complex agentic\nworkflows. By modeling the variability of tool call and agent program\ncontinuity, Continuum outperforms state-of-the-art baselines. Our evaluation on\nreal-world agentic workloads (SWE-Bench and BFCL) with Llama-3.1 8B/70B models\nshows that Continuum significantly improves the average job completion times,\nand remains performant across different hardware setups and DRAM offloading\nschemes. Preview code is available at:\nhttps://github.com/Hanchenli/vllm-continuum", "AI": {"tldr": "Continuum \u662f\u4e00\u79cd\u9762\u5411\u591a\u8f6e\u667a\u80fd\u4f53\u5de5\u4f5c\u8d1f\u8f7d\u7684 LLM \u63a8\u7406\u670d\u52a1\u7cfb\u7edf\uff0c\u901a\u8fc7\u5de5\u5177\u8c03\u7528\u611f\u77e5\u7684 KV \u7f13\u5b58\u8d85\u65f6\u673a\u5236\u4e0e\u7a0b\u5e8f\u7ea7\u8c03\u5ea6\u7b56\u7565\uff0c\u663e\u8457\u51cf\u5c11\u4efb\u52a1\u5b8c\u6210\u65f6\u95f4\u5e76\u63d0\u5347\u541e\u5410\u91cf\u3002", "motivation": "\u73b0\u6709 LLM \u670d\u52a1\u7cfb\u7edf\u5728\u5904\u7406\u5305\u542b\u5de5\u5177\u8c03\u7528\u7684\u591a\u8f6e\u667a\u80fd\u4f53\u5e94\u7528\u65f6\uff0c\u56e0\u5de5\u5177\u8c03\u7528\u9020\u6210\u7684\u6d41\u7a0b\u4e2d\u65ad\u5bfc\u81f4 KV \u7f13\u5b58\u9891\u7e41\u9a71\u9010\u548c\u8bf7\u6c42\u95f4\u7b49\u5f85\u65f6\u95f4\u589e\u52a0\uff0c\u4ece\u800c\u4e25\u91cd\u5f71\u54cd\u5ef6\u8fdf\u4e0e\u6548\u7387\u3002", "method": "Continuum \u7ed3\u5408\u5de5\u5177\u8c03\u7528\u6301\u7eed\u65f6\u95f4\u9884\u6d4b\uff0c\u52a8\u6001\u4e3a KV \u7f13\u5b58\u8bbe\u7f6e\u57fa\u4e8e\u8f6e\u6b21\u7684\u5b58\u6d3b\u65f6\u95f4\uff08TTL\uff09\uff0c\u5e76\u5728\u7a0b\u5e8f\u7ea7\u522b\u91c7\u7528\u5148\u5230\u5148\u670d\u52a1\u8c03\u5ea6\u7b56\u7565\uff0c\u4ee5\u4fdd\u6301\u591a\u8f6e\u8fde\u7eed\u6027\u5e76\u907f\u514d\u8c03\u5ea6\u7a7a\u6ce1\u3002", "result": "\u5728 SWE-Bench \u548c BFCL \u7b49\u771f\u5b9e\u667a\u80fd\u4f53\u5de5\u4f5c\u8d1f\u8f7d\u4e0a\uff0c\u4f7f\u7528 Llama-3.1 8B/70B \u6a21\u578b\u7684\u5b9e\u9a8c\u8868\u660e\uff0cContinuum \u663e\u8457\u7f29\u77ed\u4e86\u5e73\u5747\u4efb\u52a1\u5b8c\u6210\u65f6\u95f4\uff0c\u5e76\u5728\u4e0d\u540c\u786c\u4ef6\u914d\u7f6e\u548c DRAM \u5378\u8f7d\u65b9\u6848\u4e0b\u5747\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "Continuum \u901a\u8fc7\u5efa\u6a21\u5de5\u5177\u8c03\u7528\u53d8\u5f02\u6027\u4e0e\u667a\u80fd\u4f53\u7a0b\u5e8f\u8fde\u7eed\u6027\uff0c\u6709\u6548\u4f18\u5316\u4e86\u591a\u8f6e\u667a\u80fd\u4f53\u5de5\u4f5c\u8d1f\u8f7d\u7684\u670d\u52a1\u6027\u80fd\uff0c\u4f18\u4e8e\u5f53\u524d\u6700\u5148\u8fdb\u7684\u57fa\u7ebf\u7cfb\u7edf\u3002"}}
{"id": "2511.01941", "categories": ["cs.SE", "cs.AI", "cs.CR"], "pdf": "https://arxiv.org/pdf/2511.01941", "abs": "https://arxiv.org/abs/2511.01941", "authors": ["Sogol Masoumzadeh"], "title": "Detecting Vulnerabilities from Issue Reports for Internet-of-Things", "comment": "ACCEPTED/To Appear in the Proceedings of the 40th IEEE/ACM\n  International Conference on Automated Software Engineering (ASE) 2025.\n  https://conf.researchr.org/details/ase-2025/ase-2025-student-research-competition/5/Detecting-Vulnerabilities-from-Issue-Reports-for-Internet-of-Things", "summary": "Timely identification of issue reports reflecting software vulnerabilities is\ncrucial, particularly for Internet-of-Things (IoT) where analysis is slower\nthan non-IoT systems. While Machine Learning (ML) and Large Language Models\n(LLMs) detect vulnerability-indicating issues in non-IoT systems, their IoT use\nremains unexplored. We are the first to tackle this problem by proposing two\napproaches: (1) combining ML and LLMs with Natural Language Processing (NLP)\ntechniques to detect vulnerability-indicating issues of 21 Eclipse IoT projects\nand (2) fine-tuning a pre-trained BERT Masked Language Model (MLM) on 11,000\nGitHub issues for classifying \\vul. Our best performance belongs to a Support\nVector Machine (SVM) trained on BERT NLP features, achieving an Area Under the\nreceiver operator characteristic Curve (AUC) of 0.65. The fine-tuned BERT\nachieves 0.26 accuracy, emphasizing the importance of exposing all data during\ntraining. Our contributions set the stage for accurately detecting IoT\nvulnerabilities from issue reports, similar to non-IoT systems.", "AI": {"tldr": "\u672c\u6587\u9996\u6b21\u63a2\u7d22\u4e86\u5728\u7269\u8054\u7f51\uff08IoT\uff09\u9879\u76ee\u4e2d\u5229\u7528\u673a\u5668\u5b66\u4e60\uff08ML\uff09\u4e0e\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7ed3\u5408\u81ea\u7136\u8bed\u8a00\u5904\u7406\uff08NLP\uff09\u6280\u672f\uff0c\u4ece\u95ee\u9898\u62a5\u544a\u4e2d\u8bc6\u522b\u6f5c\u5728\u6f0f\u6d1e\u3002\u4f5c\u8005\u63d0\u51fa\u4e86\u4e24\u79cd\u65b9\u6cd5\uff1a\u4e00\u662f\u7ed3\u5408ML\u3001LLMs\u4e0eNLP\u5206\u679021\u4e2aEclipse IoT\u9879\u76ee\u7684\u95ee\u9898\u62a5\u544a\uff1b\u4e8c\u662f\u57fa\u4e8e11,000\u6761GitHub\u95ee\u9898\u5fae\u8c03BERT\u63a9\u7801\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u6f0f\u6d1e\u5206\u7c7b\u3002\u5b9e\u9a8c\u8868\u660e\uff0c\u57fa\u4e8eBERT\u7279\u5f81\u8bad\u7ec3\u7684SVM\u6a21\u578b\u8868\u73b0\u6700\u4f73\uff08AUC\u4e3a0.65\uff09\uff0c\u800c\u5fae\u8c03\u540e\u7684BERT\u51c6\u786e\u7387\u4ec5\u4e3a0.26\uff0c\u7a81\u663e\u8bad\u7ec3\u65f6\u6570\u636e\u8986\u76d6\u7684\u91cd\u8981\u6027\u3002", "motivation": "\u5f53\u524d\u9488\u5bf9\u975eIoT\u7cfb\u7edf\u7684\u6f0f\u6d1e\u68c0\u6d4b\u5df2\u6709ML\u548cLLM\u65b9\u6cd5\uff0c\u4f46IoT\u7cfb\u7edf\u56e0\u5206\u6790\u901f\u5ea6\u8f83\u6162\u4e14\u7f3a\u4e4f\u76f8\u5173\u7814\u7a76\uff0c\u4e9f\u9700\u4e13\u95e8\u7684\u65b9\u6cd5\u6765\u53ca\u65f6\u8bc6\u522b\u5176\u95ee\u9898\u62a5\u544a\u4e2d\u7684\u5b89\u5168\u6f0f\u6d1e\u3002", "method": "\u63d0\u51fa\u4e24\u79cd\u65b9\u6cd5\uff1a(1) \u7ed3\u5408ML\u3001LLMs\u4e0eNLP\u6280\u672f\u5206\u679021\u4e2aEclipse IoT\u9879\u76ee\u7684\u95ee\u9898\u62a5\u544a\uff1b(2) \u572811,000\u6761GitHub\u95ee\u9898\u4e0a\u5fae\u8c03\u9884\u8bad\u7ec3BERT MLM\u6a21\u578b\u7528\u4e8e\u6f0f\u6d1e\u5206\u7c7b\u3002", "result": "\u57fa\u4e8eBERT NLP\u7279\u5f81\u8bad\u7ec3\u7684SVM\u6a21\u578b\u53d6\u5f97\u6700\u4f73\u6027\u80fd\uff08AUC=0.65\uff09\uff1b\u5fae\u8c03\u540e\u7684BERT\u6a21\u578b\u51c6\u786e\u7387\u4ec5\u4e3a0.26\uff0c\u8868\u660e\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u66b4\u9732\u5168\u90e8\u6570\u636e\u7684\u91cd\u8981\u6027\u3002", "conclusion": "\u672c\u7814\u7a76\u9996\u6b21\u5728IoT\u9886\u57df\u63a2\u7d22\u5229\u7528ML\u4e0eLLMs\u4ece\u95ee\u9898\u62a5\u544a\u4e2d\u68c0\u6d4b\u6f0f\u6d1e\uff0c\u4e3a\u672a\u6765\u5b9e\u73b0\u4e0e\u975eIoT\u7cfb\u7edf\u76f8\u5f53\u7684IoT\u6f0f\u6d1e\u8bc6\u522b\u7cbe\u5ea6\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2511.02132", "categories": ["cs.AR", "cs.DC", "cs.LG", "cs.PF"], "pdf": "https://arxiv.org/pdf/2511.02132", "abs": "https://arxiv.org/abs/2511.02132", "authors": ["Mansi Choudhary", "Karthik Sangaiah", "Sonali Singh", "Muhammad Osama", "Lisa Wu Wills", "Ganesh Dasika"], "title": "Optimizing Attention on GPUs by Exploiting GPU Architectural NUMA Effects", "comment": "11 pages, 14 figures", "summary": "The rise of disaggregated AI GPUs has exposed a critical bottleneck in\nlarge-scale attention workloads: non-uniform memory access (NUMA). As\nmulti-chiplet designs become the norm for scaling compute capabilities, memory\nlatency and bandwidth vary sharply across compute regions, undermining the\nperformance of traditional GPU kernel scheduling strategies that assume uniform\nmemory access. We identify how these NUMA effects distort locality in\nmulti-head attention (MHA) and present Swizzled Head-first Mapping, a\nspatially-aware scheduling strategy that aligns attention heads with GPU NUMA\ndomains to exploit intra-chiplet cache reuse. On AMD's MI300X architecture, our\nmethod achieves up to 50% higher performance over state-of-the-art attention\nalgorithms using conventional scheduling techniques and sustains consistently\nhigh L2 cache hit rates of 80-97%. These results demonstrate that NUMA-aware\nscheduling is now fundamental to achieving full efficiency on next-generation\ndisaggregated GPUs, offering a path forward for scalable AI training and\ninference.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u540d\u4e3a\u201cSwizzled Head-first Mapping\u201d\u7684NUMA\u611f\u77e5\u8c03\u5ea6\u7b56\u7565\uff0c\u901a\u8fc7\u5c06\u591a\u5934\u6ce8\u610f\u529b\u673a\u5236\u4e2d\u7684\u6ce8\u610f\u529b\u5934\u4e0eGPU\u7684NUMA\u57df\u5bf9\u9f50\uff0c\u663e\u8457\u63d0\u5347\u7f13\u5b58\u5229\u7528\u7387\u548c\u6027\u80fd\uff0c\u5728AMD MI300X\u4e0a\u76f8\u6bd4\u73b0\u6709\u65b9\u6cd5\u6700\u9ad8\u63d0\u534750%\u6027\u80fd\u3002", "motivation": "\u968f\u7740AI GPU\u5411\u591a\u82af\u7c92\u67b6\u6784\u53d1\u5c55\uff0c\u975e\u7edf\u4e00\u5185\u5b58\u8bbf\u95ee\uff08NUMA\uff09\u5bfc\u81f4\u4f20\u7edf\u5047\u8bbe\u7edf\u4e00\u5185\u5b58\u8bbf\u95ee\u7684GPU\u8c03\u5ea6\u7b56\u7565\u6548\u7387\u4e0b\u964d\uff0c\u5c24\u5176\u5728\u5927\u89c4\u6a21\u6ce8\u610f\u529b\u8ba1\u7b97\u4e2d\u6210\u4e3a\u6027\u80fd\u74f6\u9888\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u7a7a\u95f4\u611f\u77e5\u7684\u8c03\u5ea6\u7b56\u7565\u2014\u2014Swizzled Head-first Mapping\uff0c\u5c06\u591a\u5934\u6ce8\u610f\u529b\u4e2d\u7684\u6ce8\u610f\u529b\u5934\u6620\u5c04\u5230\u5bf9\u5e94\u7684GPU NUMA\u57df\uff0c\u4ee5\u5229\u7528\u7247\u5185\u7f13\u5b58\u590d\u7528\u3002", "result": "\u5728AMD MI300X\u67b6\u6784\u4e0a\uff0c\u8be5\u65b9\u6cd5\u76f8\u6bd4\u5f53\u524d\u6700\u5148\u8fdb\u7684\u6ce8\u610f\u529b\u7b97\u6cd5\u6700\u9ad8\u5b9e\u73b050%\u7684\u6027\u80fd\u63d0\u5347\uff0c\u5e76\u4fdd\u630180-97%\u7684\u9ad8L2\u7f13\u5b58\u547d\u4e2d\u7387\u3002", "conclusion": "NUMA\u611f\u77e5\u8c03\u5ea6\u5df2\u6210\u4e3a\u5728\u65b0\u4e00\u4ee3\u89e3\u805a\u5f0fGPU\u4e0a\u5b9e\u73b0\u9ad8\u6548AI\u8bad\u7ec3\u4e0e\u63a8\u7406\u7684\u5173\u952e\u6280\u672f\u3002"}}
{"id": "2511.01860", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2511.01860", "abs": "https://arxiv.org/abs/2511.01860", "authors": ["Leszek Sliwko"], "title": "A Taxonomy of Schedulers -- Operating Systems, Clusters and Big Data Frameworks", "comment": "This is the accepted author's version of the paper. The final\n  published version is available in Global Journal of Computer Science and\n  Technology, 2019", "summary": "This review analyzes deployed and actively used workload schedulers'\nsolutions and presents a taxonomy in which those systems are divided into\nseveral hierarchical groups based on their architecture and design. While other\ntaxonomies do exist, this review has focused on the key design factors that\naffect the throughput and scalability of a given solution, as well as the\nincremental improvements which bettered such an architecture. This review gives\nspecial attention to Google's Borg, which is one of the most advanced and\npublished systems of this kind.", "AI": {"tldr": "\u672c\u6587\u7efc\u8ff0\u4e86\u5f53\u524d\u90e8\u7f72\u5e76\u5e7f\u6cdb\u4f7f\u7528\u7684\u8d1f\u8f7d\u8c03\u5ea6\u5668\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u67b6\u6784\u4e0e\u8bbe\u8ba1\u7684\u5206\u5c42\u5206\u7c7b\u6cd5\uff0c\u5e76\u91cd\u70b9\u5206\u6790\u4e86\u5f71\u54cd\u7cfb\u7edf\u541e\u5410\u91cf\u4e0e\u53ef\u6269\u5c55\u6027\u7684\u5173\u952e\u8bbe\u8ba1\u56e0\u7d20\u53ca\u67b6\u6784\u6539\u8fdb\uff0c\u7279\u522b\u5173\u6ce8\u4e86Google\u7684Borg\u7cfb\u7edf\u3002", "motivation": "\u73b0\u6709\u8c03\u5ea6\u7cfb\u7edf\u79cd\u7c7b\u7e41\u591a\uff0c\u7f3a\u4e4f\u805a\u7126\u4e8e\u541e\u5410\u91cf\u4e0e\u53ef\u6269\u5c55\u6027\u5173\u952e\u8bbe\u8ba1\u56e0\u7d20\u7684\u7cfb\u7edf\u6027\u5206\u7c7b\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u65b0\u7684\u5206\u7c7b\u65b9\u6cd5\u6765\u6df1\u5165\u7406\u89e3\u8fd9\u4e9b\u7cfb\u7edf\u7684\u67b6\u6784\u6f14\u8fdb\u3002", "method": "\u901a\u8fc7\u5206\u6790\u5df2\u90e8\u7f72\u548c\u5b9e\u9645\u4f7f\u7528\u7684\u8d1f\u8f7d\u8c03\u5ea6\u5668\uff0c\u6784\u5efa\u4e00\u4e2a\u57fa\u4e8e\u67b6\u6784\u548c\u8bbe\u8ba1\u7684\u5c42\u6b21\u5316\u5206\u7c7b\u4f53\u7cfb\uff0c\u5e76\u805a\u7126\u4e8e\u5f71\u54cd\u6027\u80fd\u7684\u5173\u952e\u8bbe\u8ba1\u8981\u7d20\u548c\u589e\u91cf\u5f0f\u67b6\u6784\u6539\u8fdb\u3002", "result": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u65b0\u7684\u8c03\u5ea6\u7cfb\u7edf\u5206\u7c7b\u6cd5\uff0c\u5e76\u5bf9\u5305\u62ecGoogle Borg\u5728\u5185\u7684\u5148\u8fdb\u8c03\u5ea6\u7cfb\u7edf\u8fdb\u884c\u4e86\u6df1\u5165\u5256\u6790\uff0c\u63ed\u793a\u4e86\u63d0\u5347\u541e\u5410\u91cf\u4e0e\u53ef\u6269\u5c55\u6027\u7684\u8bbe\u8ba1\u7b56\u7565\u3002", "conclusion": "\u8be5\u7efc\u8ff0\u4e3a\u7406\u89e3\u548c\u8bbe\u8ba1\u9ad8\u6027\u80fd\u3001\u53ef\u6269\u5c55\u7684\u8d1f\u8f7d\u8c03\u5ea6\u7cfb\u7edf\u63d0\u4f9b\u4e86\u7ed3\u6784\u5316\u89c6\u89d2\uff0c\u5f3a\u8c03\u4e86\u5173\u952e\u67b6\u6784\u9009\u62e9\u5bf9\u7cfb\u7edf\u6027\u80fd\u7684\u91cd\u8981\u5f71\u54cd\u3002"}}
{"id": "2511.01912", "categories": ["cs.MA", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.01912", "abs": "https://arxiv.org/abs/2511.01912", "authors": ["Wenzhe Fan", "Ning Yan", "Masood Mortazavi"], "title": "EvoMem: Improving Multi-Agent Planning with Dual-Evolving Memory", "comment": null, "summary": "Planning has been a cornerstone of artificial intelligence for solving\ncomplex problems, and recent progress in LLM-based multi-agent frameworks have\nbegun to extend this capability. However, the role of human-like memory within\nthese frameworks remains largely unexplored. Understanding how agents\ncoordinate through memory is critical for natural language planning, where\niterative reasoning, constraint tracking, and error correction drive the\nsuccess. Inspired by working memory model in cognitive psychology, we present\nEvoMem, a multi-agent framework built on a dual-evolving memory mechanism. The\nframework consists of three agents (Constraint Extractor, Verifier, and Actor)\nand two memory modules: Constraint Memory (CMem), which evolves across queries\nby storing task-specific rules and constraints while remains fixed within a\nquery, and Query-feedback Memory (QMem), which evolves within a query by\naccumulating feedback across iterations for solution refinement. Both memory\nmodules are reset at the end of each query session. Evaluations on trip\nplanning, meeting planning, and calendar scheduling show consistent performance\nimprovements, highlighting the effectiveness of EvoMem. This success\nunderscores the importance of memory in enhancing multi-agent planning.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86EvoMem\uff0c\u4e00\u79cd\u57fa\u4e8e\u53cc\u6f14\u5316\u8bb0\u5fc6\u673a\u5236\u7684\u591a\u667a\u80fd\u4f53\u89c4\u5212\u6846\u67b6\uff0c\u901a\u8fc7\u7ea6\u675f\u8bb0\u5fc6\uff08CMem\uff09\u548c\u67e5\u8be2\u53cd\u9988\u8bb0\u5fc6\uff08QMem\uff09\u63d0\u5347\u81ea\u7136\u8bed\u8a00\u89c4\u5212\u4efb\u52a1\u4e2d\u7684\u534f\u8c03\u4e0e\u63a8\u7406\u80fd\u529b\uff0c\u5728\u591a\u4e2a\u89c4\u5212\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u591a\u667a\u80fd\u4f53\u6846\u67b6\u5728\u89c4\u5212\u4efb\u52a1\u4e2d\u7f3a\u4e4f\u5bf9\u7c7b\u4eba\u8bb0\u5fc6\u673a\u5236\u7684\u63a2\u7d22\uff0c\u800c\u8bb0\u5fc6\u5bf9\u4e8e\u8fed\u4ee3\u63a8\u7406\u3001\u7ea6\u675f\u8ffd\u8e2a\u548c\u9519\u8bef\u4fee\u6b63\u81f3\u5173\u91cd\u8981\u3002", "method": "EvoMem\u6846\u67b6\u5305\u542b\u4e09\u4e2a\u667a\u80fd\u4f53\uff08\u7ea6\u675f\u63d0\u53d6\u5668\u3001\u9a8c\u8bc1\u5668\u548c\u6267\u884c\u5668\uff09\u548c\u4e24\u4e2a\u8bb0\u5fc6\u6a21\u5757\uff1a\u8de8\u67e5\u8be2\u6f14\u5316\u7684\u7ea6\u675f\u8bb0\u5fc6\uff08CMem\uff09\u548c\u67e5\u8be2\u5185\u6f14\u5316\u7684\u53cd\u9988\u8bb0\u5fc6\uff08QMem\uff09\uff0c\u4e24\u8005\u5728\u6bcf\u6b21\u67e5\u8be2\u7ed3\u675f\u540e\u91cd\u7f6e\u3002", "result": "\u5728\u65c5\u884c\u89c4\u5212\u3001\u4f1a\u8bae\u5b89\u6392\u548c\u65e5\u7a0b\u8c03\u5ea6\u4efb\u52a1\u4e2d\uff0cEvoMem\u5747\u5c55\u73b0\u51fa\u4e00\u81f4\u7684\u6027\u80fd\u63d0\u5347\u3002", "conclusion": "\u8bb0\u5fc6\u673a\u5236\u5bf9\u63d0\u5347\u591a\u667a\u80fd\u4f53\u89c4\u5212\u80fd\u529b\u5177\u6709\u5173\u952e\u4f5c\u7528\uff0cEvoMem\u6709\u6548\u6a21\u62df\u4e86\u4eba\u7c7b\u5de5\u4f5c\u8bb0\u5fc6\u6a21\u578b\uff0c\u589e\u5f3a\u4e86\u667a\u80fd\u4f53\u95f4\u7684\u534f\u8c03\u4e0e\u89c4\u5212\u6548\u679c\u3002"}}
{"id": "2511.02108", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.02108", "abs": "https://arxiv.org/abs/2511.02108", "authors": ["Steven Cho", "Stefano Ruberto", "Valerio Terragni"], "title": "Metamorphic Testing of Large Language Models for Natural Language Processing", "comment": null, "summary": "Using large language models (LLMs) to perform natural language processing\n(NLP) tasks has become increasingly pervasive in recent times. The versatile\nnature of LLMs makes them applicable to a wide range of such tasks. While the\nperformance of recent LLMs is generally outstanding, several studies have shown\nthat they can often produce incorrect results. Automatically identifying these\nfaulty behaviors is extremely useful for improving the effectiveness of LLMs.\nOne obstacle to this is the limited availability of labeled datasets, which\nnecessitates an oracle to determine the correctness of LLM behaviors.\nMetamorphic testing (MT) is a popular testing approach that alleviates this\noracle problem. At the core of MT are metamorphic relations (MRs), which define\nrelationships between the outputs of related inputs. MT can expose faulty\nbehaviors without the need for explicit oracles (e.g., labeled datasets). This\npaper presents the most comprehensive study of MT for LLMs to date. We\nconducted a literature review and collected 191 MRs for NLP tasks. We\nimplemented a representative subset (36 MRs) to conduct a series of experiments\nwith three popular LLMs, running approximately 560,000 metamorphic tests. The\nresults shed light on the capabilities and opportunities of MT for LLMs, as\nwell as its limitations.", "AI": {"tldr": "\u672c\u6587\u5bf9\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u53d8\u5f62\u6d4b\u8bd5\uff08MT\uff09\u8fdb\u884c\u4e86\u8fc4\u4eca\u4e3a\u6b62\u6700\u5168\u9762\u7684\u7814\u7a76\uff0c\u6536\u96c6\u4e86191\u4e2a\u9002\u7528\u4e8e\u81ea\u7136\u8bed\u8a00\u5904\u7406\u4efb\u52a1\u7684\u53d8\u5f62\u5173\u7cfb\uff08MR\uff09\uff0c\u5e76\u9009\u53d6\u5176\u4e2d36\u4e2a\u5728\u4e09\u4e2a\u4e3b\u6d41LLM\u4e0a\u6267\u884c\u7ea656\u4e07\u6b21\u6d4b\u8bd5\uff0c\u7cfb\u7edf\u8bc4\u4f30\u4e86MT\u5728\u68c0\u6d4bLLM\u9519\u8bef\u884c\u4e3a\u65b9\u9762\u7684\u6f5c\u529b\u4e0e\u5c40\u9650\u3002", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b\u867d\u5728NLP\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5e38\u4ea7\u751f\u9519\u8bef\u7ed3\u679c\uff1b\u7531\u4e8e\u7f3a\u4e4f\u6807\u6ce8\u6570\u636e\u4f5c\u4e3a\u5224\u65ad\u6b63\u786e\u6027\u7684\u201c\u9884\u8a00\u673a\u201d\uff0c\u81ea\u52a8\u8bc6\u522b\u8fd9\u4e9b\u9519\u8bef\u884c\u4e3a\u9762\u4e34\u6311\u6218\u3002\u53d8\u5f62\u6d4b\u8bd5\u53ef\u7f13\u89e3\u8fd9\u4e00\u9884\u8a00\u673a\u95ee\u9898\uff0c\u56e0\u6b64\u6709\u5fc5\u8981\u7cfb\u7edf\u7814\u7a76\u5176\u5728LLM\u4e2d\u7684\u5e94\u7528\u3002", "method": "\u4f5c\u8005\u901a\u8fc7\u6587\u732e\u7efc\u8ff0\u6536\u96c6191\u4e2a\u9002\u7528\u4e8eNLP\u4efb\u52a1\u7684\u53d8\u5f62\u5173\u7cfb\uff08MR\uff09\uff0c\u4ece\u4e2d\u9009\u53d636\u4e2a\u4ee3\u8868\u6027MR\uff0c\u5728\u4e09\u4e2a\u6d41\u884c\u7684\u5927\u8bed\u8a00\u6a21\u578b\u4e0a\u6267\u884c\u7ea6560,000\u6b21\u53d8\u5f62\u6d4b\u8bd5\u5b9e\u9a8c\u3002", "result": "\u5b9e\u9a8c\u63ed\u793a\u4e86\u53d8\u5f62\u6d4b\u8bd5\u5728\u68c0\u6d4b\u5927\u8bed\u8a00\u6a21\u578b\u9519\u8bef\u884c\u4e3a\u65b9\u9762\u7684\u6709\u6548\u6027\u3001\u9002\u7528\u573a\u666f\u53ca\u5176\u5f53\u524d\u5c40\u9650\u6027\u3002", "conclusion": "\u53d8\u5f62\u6d4b\u8bd5\u662f\u4e00\u79cd\u6709\u524d\u666f\u4e14\u5b9e\u7528\u7684\u65b9\u6cd5\uff0c\u53ef\u7528\u4e8e\u5728\u65e0\u6807\u6ce8\u6570\u636e\u60c5\u51b5\u4e0b\u8bc4\u4f30\u548c\u6539\u8fdb\u5927\u8bed\u8a00\u6a21\u578b\u7684\u53ef\u9760\u6027\uff0c\u4f46\u5176\u6548\u679c\u53d7\u9650\u4e8e\u6240\u9009\u53d8\u5f62\u5173\u7cfb\u7684\u8d28\u91cf\u4e0e\u8986\u76d6\u8303\u56f4\u3002"}}
{"id": "2511.01861", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2511.01861", "abs": "https://arxiv.org/abs/2511.01861", "authors": ["Johan Messchendorp", "Mohammad Al-Turany", "Volker Friese", "Thorsten Kollegger", "Bastian Loeher", "Jochen Markert", "Andrew Mistry", "Thomas Neff", "Adrian Oeftiger", "Michael Papenbrock", "Stephane Pietri", "Shahab Sanjari", "Tobias Stockmanns"], "title": "Conceptual Design Report for FAIR Computing", "comment": "88 pages, Conceptual Design Report for FAIR Computing", "summary": "This Conceptual Design Report (CDR) presents the plans of the computing\ninfrastructure for research at FAIR, Darmstadt, Germany. It presents the\ncomputing requirements of the various research groups, the policies for the\ncomputing and storage infrastructure, the foreseen FAIR computing model\nincluding the open data, software and services policies and architecture for\nthe periods starting in 2028 with the \"first science (plus)\" phase to the\nmodularized start version of FAIR. The overall ambition is to create a\nfederated and centrally-orchestrated infrastructure serving the large diversity\nof the research lines present with sufficient scalability and flexibility to\ncope with future data challenges that will be present at FAIR.", "AI": {"tldr": "\u8be5\u6982\u5ff5\u8bbe\u8ba1\u62a5\u544a\u6982\u8ff0\u4e86\u5fb7\u56fd\u8fbe\u59c6\u65bd\u5854\u7279FAIR\u8bbe\u65bd\u4ece2028\u5e74\u201c\u9996\u6b21\u79d1\u5b66\uff08\u52a0\uff09\u201d\u9636\u6bb5\u8d77\u7684\u8ba1\u7b97\u57fa\u7840\u8bbe\u65bd\u89c4\u5212\uff0c\u65e8\u5728\u6784\u5efa\u4e00\u4e2a\u517c\u5177\u8054\u90a6\u5316\u4e0e\u96c6\u4e2d\u534f\u8c03\u3001\u53ef\u6269\u5c55\u4e14\u7075\u6d3b\u7684\u8ba1\u7b97\u4e0e\u5b58\u50a8\u67b6\u6784\uff0c\u4ee5\u652f\u6301\u591a\u6837\u5316\u7684\u79d1\u7814\u9700\u6c42\u548c\u672a\u6765\u6570\u636e\u6311\u6218\u3002", "motivation": "\u4e3a\u5e94\u5bf9FAIR\u8bbe\u65bd\u591a\u6837\u5316\u7684\u79d1\u7814\u9700\u6c42\u53ca\u672a\u6765\u6570\u636e\u5904\u7406\u6311\u6218\uff0c\u9700\u5efa\u7acb\u4e00\u4e2a\u65e2\u80fd\u6ee1\u8db3\u5404\u7814\u7a76\u56e2\u961f\u8ba1\u7b97\u4e0e\u5b58\u50a8\u8981\u6c42\uff0c\u53c8\u5177\u5907\u8db3\u591f\u53ef\u6269\u5c55\u6027\u4e0e\u7075\u6d3b\u6027\u7684\u7edf\u4e00\u57fa\u7840\u8bbe\u65bd\u3002", "method": "\u5236\u5b9a\u6db5\u76d6\u5f00\u653e\u6570\u636e\u3001\u8f6f\u4ef6\u4e0e\u670d\u52a1\u653f\u7b56\u7684FAIR\u8ba1\u7b97\u6a21\u578b\uff0c\u8bbe\u8ba1\u8054\u90a6\u5316\u4f46\u7531\u4e2d\u5fc3\u534f\u8c03\u7684\u8ba1\u7b97\u4e0e\u5b58\u50a8\u57fa\u7840\u8bbe\u65bd\u67b6\u6784\uff0c\u5e76\u660e\u786e\u76f8\u5173\u653f\u7b56\u4e0e\u6280\u672f\u8def\u7ebf\u3002", "result": "\u63d0\u51fa\u4e86\u9762\u54112028\u5e74\u53ca\u4ee5\u540e\u9636\u6bb5\u7684FAIR\u8ba1\u7b97\u57fa\u7840\u8bbe\u65bd\u6574\u4f53\u65b9\u6848\uff0c\u5305\u62ec\u9700\u6c42\u5206\u6790\u3001\u653f\u7b56\u6846\u67b6\u548c\u7cfb\u7edf\u67b6\u6784\u3002", "conclusion": "\u901a\u8fc7\u6784\u5efa\u8054\u90a6\u5316\u4e0e\u96c6\u4e2d\u534f\u8c03\u76f8\u7ed3\u5408\u7684\u8ba1\u7b97\u57fa\u7840\u8bbe\u65bd\uff0c\u53ef\u6709\u6548\u652f\u6491FAIR\u591a\u5b66\u79d1\u7814\u7a76\u5e76\u5e94\u5bf9\u672a\u6765\u7684\u6570\u636e\u5bc6\u96c6\u578b\u79d1\u7814\u6311\u6218\u3002"}}
{"id": "2511.02197", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.02197", "abs": "https://arxiv.org/abs/2511.02197", "authors": ["Shufan Wang", "Xing Hu", "Junkai Chen", "Zhiyuan Pan", "Xin Xia"], "title": "Open the Oyster: Empirical Evaluation and Improvement of Code Reasoning Confidence in LLMs", "comment": "13 pages, 4 figures", "summary": "With the widespread application of large language models (LLMs) in the field\nof code intelligence, increasing attention has been paid to the reliability and\ncontrollability of their outputs in code reasoning tasks. Confidence estimation\nserves as an effective and convenient approach for evaluating these aspects.\nThis paper proposes a confidence analysis and enhancement framework for LLMs\ntailored to code reasoning tasks. We conduct a comprehensive empirical study on\nthe confidence reliability of mainstream LLMs across different tasks, and\nfurther evaluate the effectiveness of techniques such as prompt strategy\noptimisation and mathematical calibration (e.g., Platt Scaling) in improving\nconfidence reliability. Our results show that DeepSeek-Reasoner achieves the\nbest performance across various tasks, outperforming other models by up to\n$0.680$, $0.636$, and $13.652$ in terms of ECE, Brier Score, and Performance\nScore, respectively. The hybrid strategy combining the reassess prompt strategy\nand Platt Scaling achieves improvements of up to $0.541$, $0.628$, and $15.084$\nover the original performance in the aforementioned three metrics. These\nresults indicate that models with reasoning capabilities demonstrate superior\nconfidence reliability, and that the hybrid strategy is the most effective in\nenhancing the confidence reliability of various models. Meanwhile, we elucidate\nthe impact of different task complexities, model scales, and strategies on\nconfidence performance, and highlight that the confidence of current LLMs in\ncomplex reasoning tasks still has considerable room for improvement. This study\nnot only provides a research foundation and technical reference for the\napplication of confidence in LLM-assisted software engineering, but also points\nthe way for future optimisation and engineering deployment of confidence\nmechanisms.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9762\u5411\u4ee3\u7801\u63a8\u7406\u4efb\u52a1\u7684\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7f6e\u4fe1\u5ea6\u5206\u6790\u4e0e\u589e\u5f3a\u6846\u67b6\uff0c\u901a\u8fc7\u5b9e\u8bc1\u7814\u7a76\u8bc4\u4f30\u4e3b\u6d41LLM\u5728\u4e0d\u540c\u4efb\u52a1\u4e2d\u7684\u7f6e\u4fe1\u53ef\u9760\u6027\uff0c\u5e76\u9a8c\u8bc1\u4e86\u63d0\u793a\u7b56\u7565\u4f18\u5316\u4e0ePlatt Scaling\u7b49\u6280\u672f\u7684\u6709\u6548\u6027\u3002\u7ed3\u679c\u8868\u660e\uff0cDeepSeek-Reasoner\u8868\u73b0\u6700\u4f18\uff0c\u800c\u7ed3\u5408\u91cd\u8bc4\u4f30\u63d0\u793a\u4e0ePlatt Scaling\u7684\u6df7\u5408\u7b56\u7565\u63d0\u5347\u6700\u663e\u8457\u3002", "motivation": "\u968f\u7740\u5927\u8bed\u8a00\u6a21\u578b\u5728\u4ee3\u7801\u667a\u80fd\u9886\u57df\u7684\u5e7f\u6cdb\u5e94\u7528\uff0c\u5176\u5728\u4ee3\u7801\u63a8\u7406\u4efb\u52a1\u4e2d\u8f93\u51fa\u7684\u53ef\u9760\u6027\u548c\u53ef\u63a7\u6027\u65e5\u76ca\u53d7\u5230\u5173\u6ce8\uff0c\u800c\u7f6e\u4fe1\u5ea6\u4f30\u8ba1\u662f\u8bc4\u4f30\u8fd9\u4e9b\u7279\u6027\u7684\u6709\u6548\u624b\u6bb5\u3002", "method": "\u5bf9\u4e3b\u6d41\u5927\u8bed\u8a00\u6a21\u578b\u5728\u591a\u79cd\u4ee3\u7801\u63a8\u7406\u4efb\u52a1\u4e2d\u7684\u7f6e\u4fe1\u53ef\u9760\u6027\u8fdb\u884c\u7cfb\u7edf\u6027\u5b9e\u8bc1\u7814\u7a76\uff0c\u5e76\u8bc4\u4f30\u63d0\u793a\u7b56\u7565\u4f18\u5316\u548c\u6570\u5b66\u6821\u51c6\uff08\u5982Platt Scaling\uff09\u7b49\u6280\u672f\u5bf9\u63d0\u5347\u7f6e\u4fe1\u53ef\u9760\u6027\u7684\u6548\u679c\u3002", "result": "DeepSeek-Reasoner\u5728ECE\u3001Brier Score\u548cPerformance Score\u4e09\u9879\u6307\u6807\u4e0a\u5206\u522b\u9886\u5148\u6700\u591a0.680\u30010.636\u548c13.652\uff1b\u6df7\u5408\u7b56\u7565\u76f8\u8f83\u539f\u59cb\u6a21\u578b\u6700\u591a\u63d0\u53470.541\u30010.628\u548c15.084\u3002\u7814\u7a76\u8868\u660e\u5177\u5907\u63a8\u7406\u80fd\u529b\u7684\u6a21\u578b\u7f6e\u4fe1\u53ef\u9760\u6027\u66f4\u4f18\uff0c\u4e14\u6df7\u5408\u7b56\u7565\u6548\u679c\u6700\u4f73\u3002", "conclusion": "\u5f53\u524d\u5927\u8bed\u8a00\u6a21\u578b\u5728\u590d\u6742\u63a8\u7406\u4efb\u52a1\u4e2d\u7684\u7f6e\u4fe1\u5ea6\u4ecd\u6709\u8f83\u5927\u63d0\u5347\u7a7a\u95f4\uff1b\u672c\u7814\u7a76\u4e3aLLM\u8f85\u52a9\u8f6f\u4ef6\u5de5\u7a0b\u4e2d\u7684\u7f6e\u4fe1\u673a\u5236\u5e94\u7528\u63d0\u4f9b\u4e86\u57fa\u7840\u548c\u6280\u672f\u53c2\u8003\uff0c\u5e76\u6307\u660e\u4e86\u672a\u6765\u4f18\u5316\u4e0e\u5de5\u7a0b\u90e8\u7f72\u65b9\u5411\u3002"}}
{"id": "2511.02269", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2511.02269", "abs": "https://arxiv.org/abs/2511.02269", "authors": ["Takuto Ando", "Yu Eto", "Ayumu Takeuchi", "Yasuhiko Nakashima"], "title": "Energy-Efficient Hardware Acceleration of Whisper ASR on a CGLA", "comment": "This paper is accepted at The Thirteenth International Symposium on\n  Computing and Networking (CANDAR2025)", "summary": "The rise of generative AI for tasks like Automatic Speech Recognition (ASR)\nhas created a critical energy consumption challenge. While ASICs offer high\nefficiency, they lack the programmability to adapt to evolving algorithms. To\naddress this trade-off, we implement and evaluate Whisper's core computational\nkernel on the IMAX, a general-purpose Coarse-Grained Linear Arrays (CGLAs)\naccelerator. To our knowledge, this is the first work to execute a Whisper\nkernel on a CGRA and compare its performance against CPUs and GPUs. Using\nhardware/software co-design, we evaluate our system via an FPGA prototype and\nproject performance for a 28 nm ASIC. Our results demonstrate superior energy\nefficiency. The projected ASIC is 1.90x more energy-efficient than the NVIDIA\nJetson AGX Orin and 9.83x more than an NVIDIA RTX 4090 for the Q8_0 model. This\nwork positions CGLA as a promising platform for sustainable ASR on\npower-constrained edge devices.", "AI": {"tldr": "\u672c\u6587\u9996\u6b21\u5728CGRA\u67b6\u6784IMAX\u4e0a\u5b9e\u73b0\u5e76\u8bc4\u4f30\u4e86Whisper\u6a21\u578b\u7684\u6838\u5fc3\u8ba1\u7b97\u5185\u6838\uff0c\u901a\u8fc7\u8f6f\u786c\u4ef6\u534f\u540c\u8bbe\u8ba1\uff0c\u5728FPGA\u539f\u578b\u4e0a\u9a8c\u8bc1\u5e76\u9884\u6d4b28 nm ASIC\u6027\u80fd\uff0c\u7ed3\u679c\u8868\u660e\u5176\u5728\u80fd\u6548\u65b9\u9762\u663e\u8457\u4f18\u4e8eGPU\u548cCPU\uff0c\u4e3a\u8fb9\u7f18\u8bbe\u5907\u4e0a\u7684\u53ef\u6301\u7eed\u81ea\u52a8\u8bed\u97f3\u8bc6\u522b\u63d0\u4f9b\u4e86\u65b0\u65b9\u6848\u3002", "motivation": "\u751f\u6210\u5f0fAI\uff08\u5982\u81ea\u52a8\u8bed\u97f3\u8bc6\u522b\uff09\u7684\u5174\u8d77\u5e26\u6765\u4e86\u4e25\u91cd\u7684\u80fd\u8017\u95ee\u9898\u3002\u4e13\u7528\u96c6\u6210\u7535\u8def\uff08ASIC\uff09\u867d\u9ad8\u6548\u4f46\u7f3a\u4e4f\u53ef\u7f16\u7a0b\u6027\uff0c\u96be\u4ee5\u9002\u5e94\u7b97\u6cd5\u6f14\u8fdb\u3002\u56e0\u6b64\uff0c\u4e9f\u9700\u4e00\u79cd\u517c\u987e\u80fd\u6548\u4e0e\u7075\u6d3b\u6027\u7684\u786c\u4ef6\u5e73\u53f0\u3002", "method": "\u5728\u901a\u7528\u7c97\u7c92\u5ea6\u7ebf\u6027\u9635\u5217\uff08CGLA\uff09\u52a0\u901f\u5668IMAX\u4e0a\u5b9e\u73b0Whisper\u6838\u5fc3\u8ba1\u7b97\u5185\u6838\uff0c\u91c7\u7528\u8f6f\u786c\u4ef6\u534f\u540c\u8bbe\u8ba1\u65b9\u6cd5\uff0c\u901a\u8fc7FPGA\u539f\u578b\u8fdb\u884c\u8bc4\u4f30\uff0c\u5e76\u5916\u63a8\u81f328 nm ASIC\u5de5\u827a\u4e0b\u7684\u6027\u80fd\u8868\u73b0\u3002", "result": "\u9884\u6d4b\u7684ASIC\u5b9e\u73b0\u76f8\u6bd4NVIDIA Jetson AGX Orin\u80fd\u6548\u63d0\u53471.90\u500d\uff0c\u76f8\u6bd4RTX 4090\u63d0\u53479.83\u500d\uff08\u9488\u5bf9Q8_0\u6a21\u578b\uff09\uff0c\u5c55\u73b0\u51fa\u5353\u8d8a\u7684\u80fd\u6548\u4f18\u52bf\u3002", "conclusion": "CGLA\u67b6\u6784\u662f\u4e00\u79cd\u6709\u524d\u666f\u7684\u5e73\u53f0\uff0c\u80fd\u591f\u5728\u529f\u8017\u53d7\u9650\u7684\u8fb9\u7f18\u8bbe\u5907\u4e0a\u5b9e\u73b0\u9ad8\u80fd\u6548\u3001\u53ef\u6301\u7eed\u7684\u81ea\u52a8\u8bed\u97f3\u8bc6\u522b\u3002"}}
{"id": "2511.01862", "categories": ["cs.DC", "cs.CY"], "pdf": "https://arxiv.org/pdf/2511.01862", "abs": "https://arxiv.org/abs/2511.01862", "authors": ["Vanessa Sochat", "Daniel Milroy"], "title": "Possible Futures for Cloud Cost Models", "comment": "10 pages", "summary": "Cloud is now the leading software and computing hardware innovator, and is\nchanging the landscape of compute to one that is optimized for artificial\nintelligence and machine learning (AI/ML). Computing innovation was initially\ndriven to meet the needs of scientific computing. As industry and consumer\nusage of computing proliferated, there was a shift to satisfy a multipolar\ncustomer base. Demand for AI/ML now dominates modern computing and innovation\nhas centralized on cloud. As a result, cost and resource models designed to\nserve AI/ML use cases are not currently well suited for science. If resource\ncontention resulting from a unipole consumer makes access to contended\nresources harder for scientific users, a likely future is running scientific\nworkloads where they were not intended. In this article, we discuss the past,\ncurrent, and possible futures of cloud cost models for the continued support of\ndiscovery and science.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u4e86\u4e91\u8ba1\u7b97\u6210\u672c\u6a21\u578b\u5982\u4f55\u4ece\u670d\u52a1\u79d1\u5b66\u8ba1\u7b97\u6f14\u53d8\u4e3a\u4ee5AI/ML\u4e3a\u4e2d\u5fc3\uff0c\u5e76\u5206\u6790\u4e86\u8fd9\u79cd\u8f6c\u53d8\u5bf9\u79d1\u7814\u5de5\u4f5c\u8d1f\u8f7d\u5e26\u6765\u7684\u6311\u6218\u53ca\u672a\u6765\u53ef\u80fd\u7684\u89e3\u51b3\u65b9\u6848\u3002", "motivation": "\u968f\u7740AI/ML\u9700\u6c42\u4e3b\u5bfc\u73b0\u4ee3\u8ba1\u7b97\uff0c\u4e91\u8d44\u6e90\u548c\u6210\u672c\u6a21\u578b\u65e5\u76ca\u504f\u5411\u5355\u4e00\u6d88\u8d39\u8005\uff08AI/ML\uff09\uff0c\u5bfc\u81f4\u79d1\u7814\u7528\u6237\u5728\u7ade\u4e89\u8d44\u6e90\u65f6\u5904\u4e8e\u4e0d\u5229\u5730\u4f4d\uff0c\u53ef\u80fd\u88ab\u8feb\u5728\u975e\u7406\u60f3\u73af\u5883\u4e2d\u8fd0\u884c\u79d1\u5b66\u5de5\u4f5c\u8d1f\u8f7d\u3002", "method": "\u6587\u7ae0\u901a\u8fc7\u56de\u987e\u4e91\u8ba1\u7b97\u6210\u672c\u6a21\u578b\u7684\u5386\u53f2\u6f14\u53d8\uff0c\u5206\u6790\u5f53\u524d\u4ee5AI/ML\u4e3a\u4e2d\u5fc3\u7684\u8d44\u6e90\u5206\u914d\u673a\u5236\uff0c\u5e76\u63a2\u8ba8\u5176\u5bf9\u79d1\u5b66\u8ba1\u7b97\u7684\u5f71\u54cd\uff0c\u8fdb\u800c\u63d0\u51fa\u5bf9\u672a\u6765\u4e91\u6210\u672c\u6a21\u578b\u652f\u6301\u79d1\u7814\u7684\u53ef\u80fd\u8def\u5f84\u3002", "result": "\u6307\u51fa\u5f53\u524d\u4e91\u6210\u672c\u6a21\u578b\u4e0d\u9002\u5408\u79d1\u5b66\u8ba1\u7b97\u9700\u6c42\uff0c\u82e5\u4e0d\u52a0\u4ee5\u8c03\u6574\uff0c\u79d1\u7814\u5de5\u4f5c\u5c06\u9762\u4e34\u8d44\u6e90\u83b7\u53d6\u56f0\u96be\u548c\u8fd0\u884c\u73af\u5883\u4e0d\u9002\u914d\u7684\u95ee\u9898\u3002", "conclusion": "\u4e3a\u6301\u7eed\u652f\u6301\u79d1\u5b66\u53d1\u73b0\uff0c\u4e91\u6210\u672c\u6a21\u578b\u9700\u91cd\u65b0\u8bbe\u8ba1\uff0c\u4ee5\u517c\u987eAI/ML\u4e0e\u79d1\u7814\u7b49\u591a\u5143\u7528\u6237\u7684\u9700\u6c42\uff0c\u907f\u514d\u8d44\u6e90\u5206\u914d\u8fc7\u5ea6\u96c6\u4e2d\u4e8e\u5355\u4e00\u7528\u9014\u3002"}}
{"id": "2511.02171", "categories": ["cs.NI"], "pdf": "https://arxiv.org/pdf/2511.02171", "abs": "https://arxiv.org/abs/2511.02171", "authors": ["Rodrigo Nunes", "Andr\u00e9 Melo", "Rafael Albarello", "Reinaldo Gomes", "Cesar Marcondes", "Louren\u00e7o Pereira Jr"], "title": "Permissioned Blockchain in Advanced Air Mobility: A Performance Analisys for UTM", "comment": "Submitted to IEEE International Conference on Communications 2026", "summary": "The rapid adoption of Uncrewed Aerial Vehicles (UAVs) has driven aviation\nauthorities to propose distributed Uncrewed Traffic Management (UTM)\narchitectures. Several studies have advocated blockchain as a promising\ntechnology to meet these requirements. However, since UTM is a safety-critical\nand highly regulated domain, compliance with standards and regulatory\nframeworks is as crucial as performance and security. This work benchmarks two\ndistributed architectures aligned with current regulatory frameworks: the Linux\nFoundation's InterUSS platform and a Hyperledger Fabric-based private ledger.\nOur findings reveal that blockchain-based systems require architectures\nspecifically designed for aeronautical performance constraints.", "AI": {"tldr": "\u672c\u6587\u8bc4\u4f30\u4e86\u4e24\u79cd\u7b26\u5408\u822a\u7a7a\u76d1\u7ba1\u6846\u67b6\u7684\u5206\u5e03\u5f0f\u65e0\u4eba\u673a\u4ea4\u901a\u7ba1\u7406\uff08UTM\uff09\u67b6\u6784\u2014\u2014InterUSS\u5e73\u53f0\u548c\u57fa\u4e8eHyperledger Fabric\u7684\u79c1\u6709\u8d26\u672c\uff0c\u53d1\u73b0\u533a\u5757\u94fe\u7cfb\u7edf\u9700\u4e13\u95e8\u8bbe\u8ba1\u4ee5\u6ee1\u8db3\u822a\u7a7a\u6027\u80fd\u7ea6\u675f\u3002", "motivation": "\u968f\u7740\u65e0\u4eba\u673a\uff08UAV\uff09\u7684\u5feb\u901f\u666e\u53ca\uff0c\u822a\u7a7a\u7ba1\u7406\u673a\u6784\u63d0\u51fa\u4e86\u5206\u5e03\u5f0f\u65e0\u4eba\u673a\u4ea4\u901a\u7ba1\u7406\uff08UTM\uff09\u67b6\u6784\uff1b\u5c3d\u7ba1\u533a\u5757\u94fe\u88ab\u89c6\u4e3a\u6709\u524d\u666f\u7684\u6280\u672f\uff0c\u4f46UTM\u4f5c\u4e3a\u5b89\u5168\u5173\u952e\u4e14\u9ad8\u5ea6\u76d1\u7ba1\u7684\u9886\u57df\uff0c\u5fc5\u987b\u540c\u65f6\u6ee1\u8db3\u6027\u80fd\u3001\u5b89\u5168\u4e0e\u5408\u89c4\u8981\u6c42\u3002", "method": "\u5bf9\u4e24\u79cd\u7b26\u5408\u73b0\u884c\u76d1\u7ba1\u6846\u67b6\u7684\u5206\u5e03\u5f0fUTM\u67b6\u6784\uff08Linux\u57fa\u91d1\u4f1a\u7684InterUSS\u5e73\u53f0\u548c\u57fa\u4e8eHyperledger Fabric\u7684\u79c1\u6709\u8d26\u672c\uff09\u8fdb\u884c\u57fa\u51c6\u6d4b\u8bd5\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff0c\u57fa\u4e8e\u533a\u5757\u94fe\u7684\u7cfb\u7edf\u9700\u8981\u9488\u5bf9\u822a\u7a7a\u6027\u80fd\u7ea6\u675f\u8fdb\u884c\u4e13\u95e8\u67b6\u6784\u8bbe\u8ba1\u3002", "conclusion": "\u5728\u5b89\u5168\u5173\u952e\u7684UTM\u73af\u5883\u4e2d\uff0c\u4ec5\u9760\u901a\u7528\u533a\u5757\u94fe\u6280\u672f\u4e0d\u8db3\u4ee5\u6ee1\u8db3\u9700\u6c42\uff0c\u5fc5\u987b\u7ed3\u5408\u822a\u7a7a\u9886\u57df\u7684\u6027\u80fd\u548c\u5408\u89c4\u8981\u6c42\u8fdb\u884c\u5b9a\u5236\u5316\u67b6\u6784\u8bbe\u8ba1\u3002"}}
{"id": "2511.02304", "categories": ["cs.MA", "cs.AI", "cs.CL", "cs.FL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.02304", "abs": "https://arxiv.org/abs/2511.02304", "authors": ["Beyazit Yalcinkaya", "Marcell Vazquez-Chanlatte", "Ameesh Shah", "Hanna Krasowski", "Sanjit A. Seshia"], "title": "Automata-Conditioned Cooperative Multi-Agent Reinforcement Learning", "comment": null, "summary": "We study the problem of learning multi-task, multi-agent policies for\ncooperative, temporal objectives, under centralized training, decentralized\nexecution. In this setting, using automata to represent tasks enables the\ndecomposition of complex tasks into simpler sub-tasks that can be assigned to\nagents. However, existing approaches remain sample-inefficient and are limited\nto the single-task case. In this work, we present Automata-Conditioned\nCooperative Multi-Agent Reinforcement Learning (ACC-MARL), a framework for\nlearning task-conditioned, decentralized team policies. We identify the main\nchallenges to ACC-MARL's feasibility in practice, propose solutions, and prove\nthe correctness of our approach. We further show that the value functions of\nlearned policies can be used to assign tasks optimally at test time.\nExperiments show emergent task-aware, multi-step coordination among agents,\ne.g., pressing a button to unlock a door, holding the door, and\nshort-circuiting tasks.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aACC-MARL\u7684\u65b0\u6846\u67b6\uff0c\u7528\u4e8e\u5728\u96c6\u4e2d\u8bad\u7ec3\u3001\u5206\u6563\u6267\u884c\u7684\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u4e2d\uff0c\u5229\u7528\u81ea\u52a8\u673a\u5bf9\u590d\u6742\u534f\u4f5c\u4efb\u52a1\u8fdb\u884c\u5206\u89e3\u4e0e\u6761\u4ef6\u5316\u5b66\u4e60\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u7684\u591a\u4efb\u52a1\u7b56\u7565\u5b66\u4e60\u548c\u6d4b\u8bd5\u65f6\u7684\u4efb\u52a1\u6700\u4f18\u5206\u914d\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u4f7f\u7528\u81ea\u52a8\u673a\u8868\u793a\u4efb\u52a1\u65f6\u4ecd\u5b58\u5728\u6837\u672c\u6548\u7387\u4f4e\u3001\u4ec5\u9650\u4e8e\u5355\u4efb\u52a1\u7b49\u95ee\u9898\uff0c\u96be\u4ee5\u5e94\u5bf9\u591a\u4efb\u52a1\u3001\u591a\u667a\u80fd\u4f53\u534f\u4f5c\u573a\u666f\u4e2d\u7684\u590d\u6742\u65f6\u5e8f\u76ee\u6807\u3002", "method": "\u63d0\u51faAutomata-Conditioned Cooperative Multi-Agent Reinforcement Learning\uff08ACC-MARL\uff09\u6846\u67b6\uff0c\u901a\u8fc7\u81ea\u52a8\u673a\u5c06\u590d\u6742\u4efb\u52a1\u5206\u89e3\u4e3a\u5b50\u4efb\u52a1\uff0c\u5e76\u8bbe\u8ba1\u4efb\u52a1\u6761\u4ef6\u5316\u7684\u53bb\u4e2d\u5fc3\u5316\u56e2\u961f\u7b56\u7565\uff1b\u540c\u65f6\u89e3\u51b3\u8be5\u6846\u67b6\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u5173\u952e\u6311\u6218\uff0c\u5e76\u8bc1\u660e\u5176\u6b63\u786e\u6027\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u6240\u5b66\u7b56\u7565\u80fd\u5b9e\u73b0\u4efb\u52a1\u611f\u77e5\u7684\u591a\u6b65\u534f\u8c03\u884c\u4e3a\uff08\u5982\u6309\u6309\u94ae\u5f00\u95e8\u3001\u6276\u95e8\u3001\u8df3\u8fc7\u4efb\u52a1\u7b49\uff09\uff0c\u4e14\u5176\u4ef7\u503c\u51fd\u6570\u53ef\u7528\u4e8e\u6d4b\u8bd5\u9636\u6bb5\u7684\u4efb\u52a1\u6700\u4f18\u5206\u914d\u3002", "conclusion": "ACC-MARL\u6709\u6548\u63d0\u5347\u4e86\u591a\u4efb\u52a1\u591a\u667a\u80fd\u4f53\u534f\u4f5c\u7b56\u7565\u7684\u5b66\u4e60\u6548\u7387\u4e0e\u6cdb\u5316\u80fd\u529b\uff0c\u4e3a\u590d\u6742\u65f6\u5e8f\u4efb\u52a1\u7684\u81ea\u52a8\u5206\u89e3\u4e0e\u5206\u914d\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848\u3002"}}
{"id": "2511.02203", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2511.02203", "abs": "https://arxiv.org/abs/2511.02203", "authors": ["Gerhard Yu", "Mithila Sivakumar", "Alvine B. Belle", "Soude Ghari", "Song Wang", "Timothy C. Lethbridge"], "title": "LLMs as Judges: Toward The Automatic Review of GSN-compliant Assurance Cases", "comment": null, "summary": "Assurance cases allow verifying the correct implementation of certain\nnon-functional requirements of mission-critical systems, including their\nsafety, security, and reliability. They can be used in the specification of\nautonomous driving, avionics, air traffic control, and similar systems. They\naim to reduce risks of harm of all kinds including human mortality,\nenvironmental damage, and financial loss. However, assurance cases often tend\nto be organized as extensive documents spanning hundreds of pages, making their\ncreation, review, and maintenance error-prone, time-consuming, and tedious.\nTherefore, there is a growing need to leverage (semi-)automated techniques,\nsuch as those powered by generative AI and large language models (LLMs), to\nenhance efficiency, consistency, and accuracy across the entire assurance-case\nlifecycle. In this paper, we focus on assurance case review, a critical task\nthat ensures the quality of assurance cases and therefore fosters their\nacceptance by regulatory authorities. We propose a novel approach that\nleverages the \\textit{LLM-as-a-judge} paradigm to automate the review process.\nSpecifically, we propose new predicate-based rules that formalize\nwell-established assurance case review criteria, allowing us to craft LLM\nprompts tailored to the review task. Our experiments on several\nstate-of-the-art LLMs (GPT-4o, GPT-4.1, DeepSeek-R1, and Gemini 2.0 Flash) show\nthat, while most LLMs yield relatively good review capabilities, DeepSeek-R1\nand GPT-4.1 demonstrate superior performance, with DeepSeek-R1 ultimately\noutperforming GPT-4.1. However, our experimental results also suggest that\nhuman reviewers are still needed to refine the reviews LLMs yield.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u81ea\u52a8\u5316\u65b9\u6cd5\uff0c\u7528\u4e8e\u8bc4\u5ba1\u4fdd\u969c\u6848\u4f8b\uff08assurance cases\uff09\uff0c\u901a\u8fc7\u5c06\u8bc4\u5ba1\u6807\u51c6\u5f62\u5f0f\u5316\u4e3a\u8c13\u8bcd\u89c4\u5219\u5e76\u7ed3\u5408\u201cLLM-as-a-judge\u201d\u8303\u5f0f\uff0c\u63d0\u5347\u8bc4\u5ba1\u6548\u7387\u4e0e\u51c6\u786e\u6027\uff1b\u5b9e\u9a8c\u8868\u660eDeepSeek-R1\u548cGPT-4.1\u8868\u73b0\u6700\u4f73\uff0c\u4f46\u4eba\u7c7b\u4e13\u5bb6\u4ecd\u9700\u53c2\u4e0e\u4f18\u5316\u8bc4\u5ba1\u7ed3\u679c\u3002", "motivation": "\u4fdd\u969c\u6848\u4f8b\u5728\u5173\u952e\u7cfb\u7edf\u4e2d\u7528\u4e8e\u9a8c\u8bc1\u5b89\u5168\u3001\u53ef\u9760\u6027\u548c\u5b89\u5168\u6027\u7b49\u975e\u529f\u80fd\u6027\u9700\u6c42\uff0c\u4f46\u5176\u6587\u6863\u5197\u957f\u590d\u6742\uff0c\u5bfc\u81f4\u521b\u5efa\u3001\u8bc4\u5ba1\u548c\u7ef4\u62a4\u8fc7\u7a0b\u6613\u51fa\u9519\u4e14\u8017\u65f6\uff0c\u4e9f\u9700\u501f\u52a9\u81ea\u52a8\u5316\u6280\u672f\uff08\u5982\u5927\u8bed\u8a00\u6a21\u578b\uff09\u63d0\u9ad8\u6574\u4e2a\u751f\u547d\u5468\u671f\u7684\u6548\u7387\u4e0e\u4e00\u81f4\u6027\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u201cLLM-as-a-judge\u201d\u8303\u5f0f\u7684\u81ea\u52a8\u5316\u8bc4\u5ba1\u65b9\u6cd5\uff0c\u5c06\u5df2\u6709\u7684\u4fdd\u969c\u6848\u4f8b\u8bc4\u5ba1\u51c6\u5219\u5f62\u5f0f\u5316\u4e3a\u8c13\u8bcd\u89c4\u5219\uff0c\u5e76\u636e\u6b64\u8bbe\u8ba1\u9488\u5bf9\u6027\u7684LLM\u63d0\u793a\uff1b\u5728\u591a\u4e2a\u4e3b\u6d41\u5927\u8bed\u8a00\u6a21\u578b\uff08GPT-4o\u3001GPT-4.1\u3001DeepSeek-R1\u3001Gemini 2.0 Flash\uff09\u4e0a\u8fdb\u884c\u5b9e\u9a8c\u8bc4\u4f30\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\u5927\u591a\u6570LLM\u5177\u5907\u8f83\u597d\u7684\u8bc4\u5ba1\u80fd\u529b\uff0c\u5176\u4e2dDeepSeek-R1\u548cGPT-4.1\u8868\u73b0\u7a81\u51fa\uff0cDeepSeek-R1\u6700\u7ec8\u4f18\u4e8eGPT-4.1\uff1b\u4f46LLM\u751f\u6210\u7684\u8bc4\u5ba1\u7ed3\u679c\u4ecd\u9700\u4eba\u5de5\u8fdb\u4e00\u6b65\u4f18\u5316\u3002", "conclusion": "\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\u53ef\u6709\u6548\u8f85\u52a9\u4fdd\u969c\u6848\u4f8b\u7684\u81ea\u52a8\u5316\u8bc4\u5ba1\uff0c\u663e\u8457\u63d0\u5347\u6548\u7387\u548c\u4e00\u81f4\u6027\uff0c\u4f46\u5f53\u524d\u6280\u672f\u5c1a\u4e0d\u80fd\u5b8c\u5168\u66ff\u4ee3\u4eba\u7c7b\u4e13\u5bb6\uff0c\u4eba\u673a\u534f\u540c\u4ecd\u662f\u4fdd\u969c\u8bc4\u5ba1\u8d28\u91cf\u7684\u5173\u952e\u3002"}}
{"id": "2511.02285", "categories": ["cs.AR", "cs.PL", "cs.SE"], "pdf": "https://arxiv.org/pdf/2511.02285", "abs": "https://arxiv.org/abs/2511.02285", "authors": ["Zhuorui Zhao", "Bing Li", "Grace Li Zhang", "Ulf Schlichtmann"], "title": "VFocus: Better Verilog Generation from Large Language Model via Focused Reasoning", "comment": "accepted by SOCC 2025", "summary": "Large Language Models (LLMs) have shown impressive potential in generating\nVerilog codes, but ensuring functional correctness remains a challenge.\nExisting approaches often rely on self-consistency or simulation feedback to\nselect the best candidate, but they miss opportunities to focus LLM reasoning\non the most informative parts of the design. We propose VFocus, a three-stage\nframework that enhances Verilog generation by sharpening the focus of LLM\nreasoning onto critical decision points in the code generation process. In the\n\\textbf{pre-ranking stage}, VFocus generates multiple code candidates through\nLLM prompting, retries for syntactically valid outputs, and introduces a\n\\textit{Density-guided Filtering} to retain candidates that fall within the\n\"reasoning sweet spot\" for functional correctness. In the \\textbf{ranking\nstage}, we simulate each code candidate using an automatically generated\ntestbench and apply self-consistency-based clustering to identify the most\nconsistent outputs. Finally, in the \\textbf{post-ranking refinement stage},\nVFocus performs inconsistency mining on top-ranked candidates and invokes\nreasoning-augmented LLM prompts for candidate refinement. Experiments on the\nVerilogEval-Human benchmark show that VFocus significantly improves the pass@1\ncorrectness across multiple reasoning LLMs, demonstrating its effectiveness in\nenhancing Verilog generation for complex hardware design tasks.", "AI": {"tldr": "VFocus \u662f\u4e00\u4e2a\u4e09\u9636\u6bb5\u6846\u67b6\uff0c\u901a\u8fc7\u805a\u7126\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5728\u5173\u952e\u51b3\u7b56\u70b9\u4e0a\u7684\u63a8\u7406\uff0c\u63d0\u5347\u751f\u6210 Verilog \u4ee3\u7801\u7684\u529f\u80fd\u6b63\u786e\u6027\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u5229\u7528 LLM \u751f\u6210 Verilog \u4ee3\u7801\u65f6\u96be\u4ee5\u786e\u4fdd\u529f\u80fd\u6b63\u786e\u6027\uff0c\u4e14\u672a\u80fd\u6709\u6548\u5f15\u5bfc\u6a21\u578b\u5173\u6ce8\u8bbe\u8ba1\u4e2d\u6700\u5173\u952e\u7684\u90e8\u5206\u3002", "method": "VFocus \u5305\u542b\u4e09\u4e2a\u9636\u6bb5\uff1a1\uff09\u9884\u6392\u5e8f\u9636\u6bb5\uff0c\u901a\u8fc7\u63d0\u793a\u751f\u6210\u591a\u4e2a\u5019\u9009\u4ee3\u7801\uff0c\u7ed3\u5408\u8bed\u6cd5\u91cd\u8bd5\u548c\u5bc6\u5ea6\u5f15\u5bfc\u8fc7\u6ee4\u4fdd\u7559\u9ad8\u6f5c\u529b\u5019\u9009\uff1b2\uff09\u6392\u5e8f\u9636\u6bb5\uff0c\u4f7f\u7528\u81ea\u52a8\u751f\u6210\u7684\u6d4b\u8bd5\u5e73\u53f0\u4eff\u771f\u5019\u9009\u4ee3\u7801\uff0c\u5e76\u57fa\u4e8e\u81ea\u4e00\u81f4\u6027\u805a\u7c7b\u9009\u51fa\u6700\u4e00\u81f4\u7ed3\u679c\uff1b3\uff09\u540e\u6392\u5e8f\u4f18\u5316\u9636\u6bb5\uff0c\u5bf9\u9ad8\u6392\u540d\u5019\u9009\u8fdb\u884c\u4e0d\u4e00\u81f4\u6027\u6316\u6398\uff0c\u5e76\u8c03\u7528\u589e\u5f3a\u63a8\u7406\u7684 LLM \u63d0\u793a\u8fdb\u884c\u4f18\u5316\u3002", "result": "\u5728 VerilogEval-Human \u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cVFocus \u663e\u8457\u63d0\u5347\u4e86\u591a\u4e2a\u63a8\u7406\u578b LLM \u7684 pass@1 \u6b63\u786e\u7387\u3002", "conclusion": "VFocus \u80fd\u6709\u6548\u589e\u5f3a LLM \u5728\u590d\u6742\u786c\u4ef6\u8bbe\u8ba1\u4efb\u52a1\u4e2d\u751f\u6210 Verilog \u4ee3\u7801\u7684\u6b63\u786e\u6027\uff0c\u901a\u8fc7\u805a\u7126\u5173\u952e\u63a8\u7406\u70b9\u63d0\u5347\u6574\u4f53\u6027\u80fd\u3002"}}
{"id": "2511.02368", "categories": ["cs.NI"], "pdf": "https://arxiv.org/pdf/2511.02368", "abs": "https://arxiv.org/abs/2511.02368", "authors": ["Rushi Moliya", "Dhaval K. Patel", "Brijesh Soni", "Miguel L\u00f3pez-Ben\u00edtez"], "title": "Optimizing Multi-UAV 3D Deployment for Energy-Efficient Sensing over Uneven Terrains", "comment": "Accepted in part for presentation at IEEE Consumer Communications and\n  Networking Conference (CCNC), Las Vegas, USA, Jan. 2026. 6 pages, 4 figures", "summary": "In this work, we consider a multi-unmanned aerial vehicle (UAV) cooperative\nsensing system where UAVs are deployed to sense multiple targets in\nterrain-aware line of sight (LoS) conditions in uneven terrain equipped with\ndirectional antennas. To mitigate terrain-induced LoS blockages that degrade\ndetection performance, we incorporate a binary LoS indicator and propose a\nbounding volume hierarchy (BHV)-based adaptive scheme for efficient LoS\nevaluation. We formulate a bi-objective problem that maximizes the probability\nof cooperative detection with minimal hover energy constraints governing\nspatial, orientational, and safety constraints. To address the problem, which\nis inherently non-convex, we propose a hierarchical heuristic framework that\ncombines exploration through a genetic algorithm (GA) with per-UAV refinement\nvia particle swarm optimization (PSO), where a penalty-based fitness evaluation\nguides solutions toward feasibility, bounded within constraints. The proposed\nmethodology is an effective trade-off method of traversing through a complex\nsearch space and maintaining terrain-aware LoS connectivity and energy aware\ndeployment. Monte Carlo simulations on real-world terrain data show that the\nproposed GA+PSO framework improves detection probability by 37.02% and 36.5%\nfor 2 and 3 UAVs, respectively, while reducing average excess hover energy by\n45.0% and 48.9% compared to the PSO-only baseline. Relative to the\nnon-optimized scheme, it further achieves 59.5% and 54.2% higher detection\nprobability with 59.8% and 65.9% lower excess hover energy, thereby showing its\neffectiveness with a small number of UAVs over uneven terrain.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u7ed3\u5408\u9057\u4f20\u7b97\u6cd5\uff08GA\uff09\u4e0e\u7c92\u5b50\u7fa4\u4f18\u5316\uff08PSO\uff09\u7684\u5206\u5c42\u542f\u53d1\u5f0f\u6846\u67b6\uff0c\u7528\u4e8e\u5728\u590d\u6742\u5730\u5f62\u4e2d\u4f18\u5316\u591a\u65e0\u4eba\u673a\u534f\u540c\u611f\u77e5\u7cfb\u7edf\u7684\u63a2\u6d4b\u6982\u7387\u4e0e\u60ac\u505c\u80fd\u8017\uff0c\u5728\u4fdd\u8bc1\u5730\u5f62\u611f\u77e5\u89c6\u8ddd\uff08LoS\uff09\u8fde\u901a\u6027\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u5728\u4e0d\u5e73\u5766\u5730\u5f62\u4e2d\uff0c\u5730\u5f62\u906e\u6321\u4f1a\u7834\u574f\u65e0\u4eba\u673a\u4e0e\u76ee\u6807\u4e4b\u95f4\u7684\u89c6\u8ddd\uff08LoS\uff09\u8fde\u63a5\uff0c\u4ece\u800c\u964d\u4f4e\u534f\u540c\u63a2\u6d4b\u6027\u80fd\uff1b\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u540c\u65f6\u517c\u987e\u63a2\u6d4b\u6982\u7387\u6700\u5927\u5316\u4e0e\u80fd\u91cf\u6548\u7387\uff0c\u4e14\u7f3a\u4e4f\u9ad8\u6548\u7684LoS\u8bc4\u4f30\u673a\u5236\u3002", "method": "\u5f15\u5165\u4e8c\u5143LoS\u6307\u793a\u5668\u5e76\u91c7\u7528\u57fa\u4e8e\u5305\u56f4\u4f53\u5c42\u6b21\u7ed3\u6784\uff08BVH\uff09\u7684\u81ea\u9002\u5e94\u65b9\u6848\u8fdb\u884c\u9ad8\u6548LoS\u8bc4\u4f30\uff1b\u6784\u5efa\u4e00\u4e2a\u53cc\u76ee\u6807\u4f18\u5316\u95ee\u9898\uff0c\u5e76\u8bbe\u8ba1\u5206\u5c42\u542f\u53d1\u5f0f\u6846\u67b6\uff1a\u5916\u5c42\u4f7f\u7528\u9057\u4f20\u7b97\u6cd5\u8fdb\u884c\u5168\u5c40\u63a2\u7d22\uff0c\u5185\u5c42\u5bf9\u6bcf\u67b6\u65e0\u4eba\u673a\u4f7f\u7528\u7c92\u5b50\u7fa4\u4f18\u5316\u8fdb\u884c\u7cbe\u7ec6\u8c03\u6574\uff0c\u7ed3\u5408\u57fa\u4e8e\u60e9\u7f5a\u7684\u9002\u5e94\u5ea6\u51fd\u6570\u786e\u4fdd\u89e3\u6ee1\u8db3\u7a7a\u95f4\u3001\u671d\u5411\u548c\u5b89\u5168\u7ea6\u675f\u3002", "result": "\u5728\u771f\u5b9e\u5730\u5f62\u6570\u636e\u4e0a\u7684\u8499\u7279\u5361\u6d1b\u4eff\u771f\u8868\u660e\uff0c\u76f8\u6bd4\u4ec5\u4f7f\u7528PSO\u7684\u57fa\u7ebf\u65b9\u6cd5\uff0c\u6240\u63d0GA+PSO\u6846\u67b6\u57282\u67b6\u548c3\u67b6\u65e0\u4eba\u673a\u573a\u666f\u4e0b\u5206\u522b\u5c06\u63a2\u6d4b\u6982\u7387\u63d0\u534737.02%\u548c36.5%\uff0c\u5e73\u5747\u591a\u4f59\u60ac\u505c\u80fd\u8017\u964d\u4f4e45.0%\u548c48.9%\uff1b\u76f8\u6bd4\u975e\u4f18\u5316\u65b9\u6848\uff0c\u63a2\u6d4b\u6982\u7387\u63d0\u534759.5%\u548c54.2%\uff0c\u591a\u4f59\u80fd\u8017\u964d\u4f4e59.8%\u548c65.9%\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684GA+PSO\u5206\u5c42\u4f18\u5316\u6846\u67b6\u5728\u5c11\u91cf\u65e0\u4eba\u673a\u548c\u590d\u6742\u5730\u5f62\u6761\u4ef6\u4e0b\uff0c\u80fd\u6709\u6548\u5e73\u8861\u63a2\u6d4b\u6027\u80fd\u4e0e\u80fd\u8017\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u9a8c\u8bc1\u4e86\u5176\u5728\u5730\u5f62\u611f\u77e5\u534f\u540c\u611f\u77e5\u4efb\u52a1\u4e2d\u7684\u5b9e\u7528\u6027\u548c\u6709\u6548\u6027\u3002"}}
{"id": "2511.02408", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2511.02408", "abs": "https://arxiv.org/abs/2511.02408", "authors": ["Takuto Ando", "Yusuke Inoue"], "title": "Facial Expression Recognition System Using DNN Accelerator with Multi-threading on FPGA", "comment": "This paper was published in the proceedings of the 2024 Twelfth\n  International Symposium on Computing and Networking Workshops (CANDARW)", "summary": "In this paper, we implement a stand-alone facial expression recognition\nsystem on an SoC FPGA with multi-threading using a Deep learning Processor Unit\n(DPU). The system consists of two steps: one for face detection step and one\nfor facial expression recognition. In the previous work, the Haar Cascade\ndetector was run on a CPU in the face detection step due to FPGA resource\nlimitations, but this detector is less accurate for profile and variable\nillumination condition images. Moreover, the previous work used a dedicated\ncircuit accelerator, so running a second DNN inference for face detection on\nthe FPGA would require the addition of a new accelerator. As an alternative to\nthis approach, we run the two inferences by DNN on a DPU, which is a\ngeneral-purpose CNN accelerator of the systolic array type. Our method for face\ndetection using DenseBox and facial expression recognition using CNN on the\nsame DPU enables the efficient use of FPGA resources while maintaining a small\ncircuit size. We also developed a multi-threading technique that improves the\noverall throughput while increasing the DPU utilization efficiency. With this\napproach, we achieved an overall system throughput of 25 FPS and a throughput\nper power consumption of 2.4 times.", "AI": {"tldr": "\u672c\u6587\u5728SoC FPGA\u4e0a\u5229\u7528\u6df1\u5ea6\u5b66\u4e60\u5904\u7406\u5668\u5355\u5143\uff08DPU\uff09\u5b9e\u73b0\u4e86\u4e00\u4e2a\u591a\u7ebf\u7a0b\u7684\u72ec\u7acb\u9762\u90e8\u8868\u60c5\u8bc6\u522b\u7cfb\u7edf\uff0c\u901a\u8fc7\u5728\u540c\u4e00\u4e2a\u901a\u7528CNN\u52a0\u901f\u5668\u4e0a\u8fd0\u884cDenseBox\u4eba\u8138\u68c0\u6d4b\u548cCNN\u8868\u60c5\u8bc6\u522b\uff0c\u63d0\u9ad8\u4e86FPGA\u8d44\u6e90\u5229\u7528\u7387\u548c\u7cfb\u7edf\u541e\u5410\u91cf\u3002", "motivation": "\u5148\u524d\u5de5\u4f5c\u56e0FPGA\u8d44\u6e90\u9650\u5236\u800c\u5728CPU\u4e0a\u8fd0\u884cHaar Cascade\u4eba\u8138\u68c0\u6d4b\u5668\uff0c\u5bfc\u81f4\u5bf9\u4fa7\u8138\u548c\u5149\u7167\u53d8\u5316\u56fe\u50cf\u68c0\u6d4b\u7cbe\u5ea6\u8f83\u4f4e\uff1b\u540c\u65f6\u82e5\u4f7f\u7528\u4e13\u7528\u7535\u8def\u52a0\u901f\u5668\u8fdb\u884c\u4e24\u6b21DNN\u63a8\u7406\uff0c\u5219\u9700\u589e\u52a0\u65b0\u7684\u786c\u4ef6\u6a21\u5757\uff0c\u4e0d\u591f\u9ad8\u6548\u3002", "method": "\u91c7\u7528\u901a\u7528\u578b\u8109\u52a8\u9635\u5217\u7ed3\u6784\u7684DPU\uff0c\u5728\u5176\u4e0a\u540c\u65f6\u8fd0\u884c\u57fa\u4e8eDenseBox\u7684\u4eba\u8138\u68c0\u6d4b\u548c\u57fa\u4e8eCNN\u7684\u8868\u60c5\u8bc6\u522b\uff0c\u5e76\u5f00\u53d1\u591a\u7ebf\u7a0b\u6280\u672f\u4ee5\u63d0\u5347DPU\u5229\u7528\u7387\u548c\u6574\u4f53\u541e\u5410\u91cf\u3002", "result": "\u7cfb\u7edf\u5b9e\u73b0\u4e8625 FPS\u7684\u6574\u4f53\u541e\u5410\u91cf\uff0c\u5355\u4f4d\u529f\u8017\u541e\u5410\u91cf\u63d0\u5347\u4e862.4\u500d\u3002", "conclusion": "\u901a\u8fc7\u5728\u5355\u4e00DPU\u4e0a\u96c6\u6210\u4e24\u4e2aDNN\u63a8\u7406\u4efb\u52a1\u5e76\u7ed3\u5408\u591a\u7ebf\u7a0b\u8c03\u5ea6\uff0c\u8be5\u65b9\u6cd5\u5728\u4fdd\u6301\u5c0f\u7535\u8def\u89c4\u6a21\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u4e86\u7cfb\u7edf\u6548\u7387\u4e0e\u6027\u80fd\u3002"}}
{"id": "2511.01866", "categories": ["cs.DC", "cs.AI", "cs.AR"], "pdf": "https://arxiv.org/pdf/2511.01866", "abs": "https://arxiv.org/abs/2511.01866", "authors": ["Benjamin Kubwimana", "Qijing Huang"], "title": "EdgeReasoning: Characterizing Reasoning LLM Deployment on Edge GPUs", "comment": "Published in the Proceedings of the 2025 IEEE International Symposium\n  on Workload Characterization (IISWC 2025)", "summary": "Edge intelligence paradigm is increasingly demanded by the emerging\nautonomous systems, such as robotics. Beyond ensuring privacy-preserving\noperation and resilience in connectivity-limited environments, edge deployment\noffers significant energy and cost advantages over cloud-based solutions.\nHowever, deploying large language models (LLMs) for reasoning tasks on edge\nGPUs faces critical challenges from strict latency constraints and limited\ncomputational resources. To navigate these constraints, developers must balance\nmultiple design factors - choosing reasoning versus non-reasoning\narchitectures, selecting appropriate model sizes, allocating token budgets, and\napplying test-time scaling strategies - to meet target latency and optimize\naccuracy. Yet guidance on optimal combinations of these variables remains\nscarce. In this work, we present EdgeReasoning, a comprehensive study\ncharacterizing the deployment of reasoning LLMs on edge GPUs. We systematically\nquantify latency-accuracy tradeoffs across various LLM architectures and model\nsizes. We systematically evaluate prompt-based and model-tuning-based\ntechniques for reducing reasoning token length while maintaining performance\nquality. We further profile test-time scaling methods with varying degrees of\nparallelism to maximize accuracy under strict latency budgets. Through these\nanalyses, EdgeReasoning maps the Pareto frontier of achievable accuracy-latency\nconfigurations, offering systematic guidance for optimal edge deployment of\nreasoning LLMs.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86EdgeReasoning\uff0c\u7cfb\u7edf\u7814\u7a76\u4e86\u5728\u8fb9\u7f18GPU\u4e0a\u90e8\u7f72\u7528\u4e8e\u63a8\u7406\u7684\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u65f6\u7684\u5ef6\u8fdf\u4e0e\u51c6\u786e\u7387\u6743\u8861\uff0c\u8bc4\u4f30\u4e86\u4e0d\u540c\u67b6\u6784\u3001\u6a21\u578b\u89c4\u6a21\u3001\u63d0\u793a\u7b56\u7565\u548c\u6d4b\u8bd5\u65f6\u7f29\u653e\u65b9\u6cd5\uff0c\u5e76\u7ed8\u5236\u4e86\u51c6\u786e\u7387-\u5ef6\u8fdf\u7684\u5e15\u7d2f\u6258\u524d\u6cbf\uff0c\u4e3a\u8fb9\u7f18\u7aef\u9ad8\u6548\u90e8\u7f72\u63a8\u7406\u578bLLM\u63d0\u4f9b\u6307\u5bfc\u3002", "motivation": "\u5728\u673a\u5668\u4eba\u7b49\u65b0\u5174\u81ea\u4e3b\u7cfb\u7edf\u4e2d\uff0c\u8fb9\u7f18\u667a\u80fd\u65e5\u76ca\u91cd\u8981\u3002\u5c3d\u7ba1\u8fb9\u7f18\u90e8\u7f72\u5177\u5907\u9690\u79c1\u4fdd\u62a4\u3001\u8fde\u63a5\u9c81\u68d2\u6027\u4ee5\u53ca\u80fd\u6548\u548c\u6210\u672c\u4f18\u52bf\uff0c\u4f46\u5c06\u5927\u8bed\u8a00\u6a21\u578b\u7528\u4e8e\u8fb9\u7f18GPU\u4e0a\u7684\u63a8\u7406\u4efb\u52a1\u4ecd\u9762\u4e34\u4e25\u82db\u7684\u5ef6\u8fdf\u9650\u5236\u548c\u6709\u9650\u8ba1\u7b97\u8d44\u6e90\u7684\u6311\u6218\u3002\u76ee\u524d\u7f3a\u4e4f\u5bf9\u5982\u4f55\u7ec4\u5408\u591a\u79cd\u8bbe\u8ba1\u56e0\u7d20\uff08\u5982\u6a21\u578b\u67b6\u6784\u9009\u62e9\u3001\u89c4\u6a21\u3001token\u9884\u7b97\u548c\u6d4b\u8bd5\u65f6\u7f29\u653e\u7b56\u7565\uff09\u4ee5\u5b9e\u73b0\u6700\u4f73\u6027\u80fd\u7684\u7cfb\u7edf\u6027\u6307\u5bfc\u3002", "method": "\u4f5c\u8005\u7cfb\u7edf\u6027\u5730\u91cf\u5316\u4e86\u4e0d\u540cLLM\u67b6\u6784\u548c\u6a21\u578b\u89c4\u6a21\u4e0b\u7684\u5ef6\u8fdf-\u51c6\u786e\u7387\u6743\u8861\uff1b\u8bc4\u4f30\u4e86\u57fa\u4e8e\u63d0\u793a\u548c\u57fa\u4e8e\u6a21\u578b\u5fae\u8c03\u7684\u6280\u672f\u5728\u51cf\u5c11\u63a8\u7406token\u957f\u5ea6\u7684\u540c\u65f6\u4fdd\u6301\u6027\u80fd\u7684\u6548\u679c\uff1b\u5e76\u5206\u6790\u4e86\u4e0d\u540c\u5e76\u884c\u5ea6\u7684\u6d4b\u8bd5\u65f6\u7f29\u653e\u65b9\u6cd5\u5728\u4e25\u683c\u5ef6\u8fdf\u7ea6\u675f\u4e0b\u63d0\u5347\u51c6\u786e\u7387\u7684\u80fd\u529b\u3002", "result": "\u7814\u7a76\u7ed8\u5236\u4e86\u5728\u8fb9\u7f18GPU\u4e0a\u90e8\u7f72\u63a8\u7406\u578bLLM\u65f6\u53ef\u5b9e\u73b0\u7684\u51c6\u786e\u7387\u4e0e\u5ef6\u8fdf\u4e4b\u95f4\u7684\u5e15\u7d2f\u6258\u524d\u6cbf\uff0c\u660e\u786e\u4e86\u5404\u79cd\u8bbe\u8ba1\u9009\u62e9\u5bf9\u6027\u80fd\u7684\u5f71\u54cd\uff0c\u5e76\u63d0\u4f9b\u4e86\u4f18\u5316\u8fb9\u7f18\u90e8\u7f72\u7684\u7cfb\u7edf\u6027\u6307\u5357\u3002", "conclusion": "EdgeReasoning\u4e3a\u5728\u8d44\u6e90\u53d7\u9650\u7684\u8fb9\u7f18\u8bbe\u5907\u4e0a\u9ad8\u6548\u90e8\u7f72\u63a8\u7406\u578b\u5927\u8bed\u8a00\u6a21\u578b\u63d0\u4f9b\u4e86\u5168\u9762\u7684\u5b9e\u8bc1\u5206\u6790\u548c\u5b9e\u7528\u6307\u5bfc\uff0c\u6709\u52a9\u4e8e\u5f00\u53d1\u8005\u5728\u5ef6\u8fdf\u548c\u51c6\u786e\u7387\u4e4b\u95f4\u505a\u51fa\u6700\u4f18\u6743\u8861\u3002"}}
{"id": "2511.02399", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.02399", "abs": "https://arxiv.org/abs/2511.02399", "authors": ["Junwei Liu", "Chen Xu", "Chong Wang", "Tong Bai", "Weitong Chen", "Kaseng Wong", "Yiling Lou", "Xin Peng"], "title": "EvoDev: An Iterative Feature-Driven Framework for End-to-End Software Development with LLM-based Agents", "comment": "14 pages, 6 figures", "summary": "Recent advances in large language model agents offer the promise of\nautomating end-to-end software development from natural language requirements.\nHowever, existing approaches largely adopt linear, waterfall-style pipelines,\nwhich oversimplify the iterative nature of real-world development and struggle\nwith complex, large-scale projects. To address these limitations, we propose\nEvoDev, an iterative software development framework inspired by feature-driven\ndevelopment. EvoDev decomposes user requirements into a set of user-valued\nfeatures and constructs a Feature Map, a directed acyclic graph that explicitly\nmodels dependencies between features. Each node in the feature map maintains\nmulti-level information, including business logic, design, and code, which is\npropagated along dependencies to provide context for subsequent development\niterations. We evaluate EvoDev on challenging Android development tasks and\nshow that it outperforms the best-performing baseline, Claude Code, by a\nsubstantial margin of 56.8%, while improving single-agent performance by\n16.0%-76.6% across different base LLMs, highlighting the importance of\ndependency modeling, context propagation, and workflow-aware agent design for\ncomplex software projects. Our work summarizes practical insights for designing\niterative, LLM-driven development frameworks and informs future training of\nbase LLMs to better support iterative software development.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86EvoDev\uff0c\u4e00\u79cd\u53d7\u7279\u6027\u9a71\u52a8\u5f00\u53d1\u542f\u53d1\u7684\u8fed\u4ee3\u5f0f\u8f6f\u4ef6\u5f00\u53d1\u6846\u67b6\uff0c\u901a\u8fc7\u6784\u5efa\u663e\u5f0f\u5efa\u6a21\u7279\u6027\u7684\u4f9d\u8d56\u5173\u7cfb\u7684Feature Map\uff0c\u5e76\u5728\u591a\u5c42\u7ea7\u4fe1\u606f\u95f4\u4f20\u64ad\u4e0a\u4e0b\u6587\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5927\u8bed\u8a00\u6a21\u578b\u5728\u590d\u6742Android\u9879\u76ee\u4e2d\u7684\u81ea\u52a8\u5316\u5f00\u53d1\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u81ea\u52a8\u5316\u8f6f\u4ef6\u5f00\u53d1\u65b9\u6cd5\u591a\u91c7\u7528\u7ebf\u6027\u3001\u7011\u5e03\u5f0f\u7684\u6d41\u7a0b\uff0c\u65e0\u6cd5\u6709\u6548\u5e94\u5bf9\u771f\u5b9e\u4e16\u754c\u4e2d\u8fed\u4ee3\u6027\u5f3a\u3001\u89c4\u6a21\u590d\u6742\u7684\u8f6f\u4ef6\u9879\u76ee\u5f00\u53d1\u9700\u6c42\u3002", "method": "EvoDev\u5c06\u7528\u6237\u9700\u6c42\u5206\u89e3\u4e3a\u6709\u4ef7\u503c\u7684\u7279\u6027\u96c6\u5408\uff0c\u6784\u5efa\u4e00\u4e2a\u6709\u5411\u65e0\u73af\u56fe\uff08Feature Map\uff09\u4ee5\u663e\u5f0f\u5efa\u6a21\u7279\u6027\u95f4\u7684\u4f9d\u8d56\u5173\u7cfb\uff1b\u6bcf\u4e2a\u8282\u70b9\u7ef4\u62a4\u4e1a\u52a1\u903b\u8f91\u3001\u8bbe\u8ba1\u548c\u4ee3\u7801\u7b49\u591a\u5c42\u4fe1\u606f\uff0c\u5e76\u6cbf\u4f9d\u8d56\u5173\u7cfb\u4f20\u64ad\u4e0a\u4e0b\u6587\uff0c\u7528\u4e8e\u6307\u5bfc\u540e\u7eed\u8fed\u4ee3\u5f00\u53d1\u3002", "result": "\u5728\u5177\u6709\u6311\u6218\u6027\u7684Android\u5f00\u53d1\u4efb\u52a1\u4e0a\uff0cEvoDev\u76f8\u6bd4\u6700\u4f73\u57fa\u7ebfClaude Code\u6027\u80fd\u63d0\u534756.8%\uff0c\u5e76\u5728\u4e0d\u540c\u57fa\u7840\u5927\u8bed\u8a00\u6a21\u578b\u4e0a\u5c06\u5355\u667a\u80fd\u4f53\u6027\u80fd\u63d0\u534716.0%\u201376.6%\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\uff0c\u5728\u590d\u6742\u8f6f\u4ef6\u9879\u76ee\u4e2d\uff0c\u4f9d\u8d56\u5efa\u6a21\u3001\u4e0a\u4e0b\u6587\u4f20\u64ad\u548c\u5de5\u4f5c\u6d41\u611f\u77e5\u7684\u667a\u80fd\u4f53\u8bbe\u8ba1\u81f3\u5173\u91cd\u8981\uff1b\u8be5\u5de5\u4f5c\u4e3a\u8bbe\u8ba1\u8fed\u4ee3\u5f0fLLM\u9a71\u52a8\u5f00\u53d1\u6846\u67b6\u63d0\u4f9b\u4e86\u5b9e\u8df5\u6d1e\u89c1\uff0c\u5e76\u4e3a\u672a\u6765\u57fa\u7840\u5927\u6a21\u578b\u7684\u8bad\u7ec3\u6307\u660e\u4e86\u65b9\u5411\u3002"}}
{"id": "2511.02494", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2511.02494", "abs": "https://arxiv.org/abs/2511.02494", "authors": ["Raul Murillo", "Julio Villalba-Moreno", "Alberto A. Del Barrio", "Guillermo Botella"], "title": "Digit-Recurrence Posit Division", "comment": "11 pages, 9 figures", "summary": "Posit arithmetic has emerged as a promising alternative to IEEE 754\nfloating-point representation, offering enhanced accuracy and dynamic range.\nHowever, division operations in posit systems remain challenging due to their\ninherent hardware complexity. In this work, we present posit division units\nbased on the digit-recurrence algorithm, marking the first implementation of\nradix-4 digit-recurrence techniques within this context. Our approach\nincorporates hardware-centric optimizations including redundant arithmetic,\non-the-fly quotient conversion, and operand scaling to streamline the division\nprocess while mitigating latency, area, and power overheads. Comprehensive\nsynthesis evaluations across multiple posit configurations demonstrate\nsignificant performance improvements, including more than 80% energy reduction\nwith small area overhead compared to existing methods, and a substantial\ndecrease in the number of iterations. These results underscore the potential of\nour adapted algorithm to enhance the efficiency of posit-based arithmetic\nunits.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u57fa\u6570-4\u6570\u5b57\u9012\u5f52\u7b97\u6cd5\u7684posit\u9664\u6cd5\u5668\uff0c\u901a\u8fc7\u786c\u4ef6\u4f18\u5316\u663e\u8457\u964d\u4f4e\u4e86\u80fd\u8017\u548c\u8fed\u4ee3\u6b21\u6570\uff0c\u5728\u5c0f\u9762\u79ef\u5f00\u9500\u4e0b\u5b9e\u73b0\u4e86\u9ad8\u6548posit\u9664\u6cd5\u8fd0\u7b97\u3002", "motivation": "Posit\u7b97\u672f\u867d\u5728\u7cbe\u5ea6\u548c\u52a8\u6001\u8303\u56f4\u4e0a\u4f18\u4e8eIEEE 754\u6d6e\u70b9\u6570\uff0c\u4f46\u5176\u9664\u6cd5\u8fd0\u7b97\u56e0\u786c\u4ef6\u590d\u6742\u6027\u800c\u9762\u4e34\u6311\u6218\uff0c\u4e9f\u9700\u9ad8\u6548\u5b9e\u73b0\u65b9\u6848\u3002", "method": "\u91c7\u7528\u57fa\u6570-4\u6570\u5b57\u9012\u5f52\u7b97\u6cd5\uff0c\u5e76\u7ed3\u5408\u5197\u4f59\u7b97\u672f\u3001\u5728\u7ebf\u5546\u8f6c\u6362\u548c\u64cd\u4f5c\u6570\u7f29\u653e\u7b49\u786c\u4ef6\u5bfc\u5411\u4f18\u5316\u6280\u672f\u8bbe\u8ba1posit\u9664\u6cd5\u5355\u5143\u3002", "result": "\u5728\u591a\u79cdposit\u914d\u7f6e\u4e0b\uff0c\u76f8\u6bd4\u73b0\u6709\u65b9\u6cd5\uff0c\u80fd\u8017\u964d\u4f4e\u8d85\u8fc780%\uff0c\u8fed\u4ee3\u6b21\u6570\u663e\u8457\u51cf\u5c11\uff0c\u4e14\u4ec5\u5e26\u6765\u8f83\u5c0f\u7684\u9762\u79ef\u5f00\u9500\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u4f18\u5316\u7b97\u6cd5\u6709\u6548\u63d0\u5347\u4e86posit\u7b97\u672f\u5355\u5143\u4e2d\u9664\u6cd5\u8fd0\u7b97\u7684\u6548\u7387\uff0c\u5c55\u793a\u4e86\u5176\u5728\u5b9e\u9645\u786c\u4ef6\u5b9e\u73b0\u4e2d\u7684\u6f5c\u529b\u3002"}}
{"id": "2511.01871", "categories": ["cs.DC", "68M15, 93B12"], "pdf": "https://arxiv.org/pdf/2511.01871", "abs": "https://arxiv.org/abs/2511.01871", "authors": ["S. Tsiramua", "H. Meladze", "T. Davitashvili", "J. M. Sanchez", "F. Criado-Aldeanueva"], "title": "Structural Analysis of Multi-Core Processor and Reliability Evaluation Model", "comment": null, "summary": "In the present paper, the models of structural analysis and evaluation of\nefficiency indicators (reliability, fault tolerance, viability, and\nflexibility) of a multi core processor with variable structure, equipped with\nmulti functional cores, are considered. Using logical probabilistic methods,\nthe following has been developed: models for evaluating the reliability and\nfault tolerance of processor cores as multi functional elements; logical\nprobabilistic models of the shortest paths, flexibility, and performance\nconditions for successful operation of multi core processors based on multi\nfunctional cores; and models for estimating the reliability, fault tolerance,\nand lifetime of multi core processors considering all possible states of\nperformance. The results of the structural analysis of two core and four core\nprocessors and the trends of increasing the efficiency indicators of multi core\nprocessors are presented.", "AI": {"tldr": "\u672c\u6587\u57fa\u4e8e\u903b\u8f91\u6982\u7387\u65b9\u6cd5\uff0c\u6784\u5efa\u4e86\u591a\u6838\u5904\u7406\u5668\uff08\u542b\u591a\u529f\u80fd\u6838\u5fc3\uff09\u5728\u53ef\u9760\u6027\u3001\u5bb9\u9519\u6027\u3001\u53ef\u7528\u6027\u4e0e\u7075\u6d3b\u6027\u7b49\u65b9\u9762\u7684\u7ed3\u6784\u5206\u6790\u4e0e\u6548\u7387\u8bc4\u4f30\u6a21\u578b\uff0c\u5e76\u5bf9\u53cc\u6838\u4e0e\u56db\u6838\u5904\u7406\u5668\u8fdb\u884c\u4e86\u5b9e\u4f8b\u5206\u6790\uff0c\u63ed\u793a\u4e86\u6548\u7387\u6307\u6807\u968f\u7ed3\u6784\u53d8\u5316\u7684\u8d8b\u52bf\u3002", "motivation": "\u4e3a\u63d0\u5347\u591a\u6838\u5904\u7406\u5668\u5728\u590d\u6742\u5e94\u7528\u573a\u666f\u4e0b\u7684\u6027\u80fd\u8868\u73b0\uff0c\u9700\u7cfb\u7edf\u8bc4\u4f30\u5176\u53ef\u9760\u6027\u3001\u5bb9\u9519\u6027\u3001\u53ef\u7528\u6027\u548c\u7075\u6d3b\u6027\u7b49\u5173\u952e\u6548\u7387\u6307\u6807\uff0c\u800c\u73b0\u6709\u65b9\u6cd5\u7f3a\u4e4f\u5bf9\u591a\u529f\u80fd\u6838\u5fc3\u53ca\u53ef\u53d8\u7ed3\u6784\u7684\u7efc\u5408\u5efa\u6a21\u3002", "method": "\u91c7\u7528\u903b\u8f91\u6982\u7387\u65b9\u6cd5\uff0c\u5efa\u7acb\u591a\u529f\u80fd\u6838\u5fc3\u7684\u53ef\u9760\u6027\u4e0e\u5bb9\u9519\u6027\u8bc4\u4f30\u6a21\u578b\uff1b\u6784\u5efa\u57fa\u4e8e\u6700\u77ed\u8def\u5f84\u548c\u8fd0\u884c\u6761\u4ef6\u7684\u7075\u6d3b\u6027\u4e0e\u6027\u80fd\u903b\u8f91\u6982\u7387\u6a21\u578b\uff1b\u5e76\u7efc\u5408\u6240\u6709\u8fd0\u884c\u72b6\u6001\uff0c\u4f30\u7b97\u591a\u6838\u5904\u7406\u5668\u7684\u6574\u4f53\u53ef\u9760\u6027\u3001\u5bb9\u9519\u80fd\u529b\u4e0e\u5bff\u547d\u3002", "result": "\u5bf9\u53cc\u6838\u4e0e\u56db\u6838\u5904\u7406\u5668\u8fdb\u884c\u4e86\u7ed3\u6784\u5206\u6790\uff0c\u5c55\u793a\u4e86\u6240\u63d0\u6a21\u578b\u7684\u6709\u6548\u6027\uff0c\u5e76\u63ed\u793a\u4e86\u968f\u7740\u6838\u5fc3\u6570\u91cf\u589e\u52a0\uff0c\u591a\u6838\u5904\u7406\u5668\u5404\u9879\u6548\u7387\u6307\u6807\u7684\u53d8\u5316\u8d8b\u52bf\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u903b\u8f91\u6982\u7387\u6a21\u578b\u80fd\u6709\u6548\u652f\u6301\u5bf9\u53ef\u53d8\u7ed3\u6784\u591a\u6838\u5904\u7406\u5668\u7684\u6548\u7387\u8bc4\u4f30\uff0c\u4e3a\u8bbe\u8ba1\u9ad8\u53ef\u9760\u3001\u9ad8\u5bb9\u9519\u3001\u9ad8\u7075\u6d3b\u6027\u7684\u591a\u6838\u7cfb\u7edf\u63d0\u4f9b\u7406\u8bba\u4f9d\u636e\u3002"}}
{"id": "2511.02559", "categories": ["cs.NI"], "pdf": "https://arxiv.org/pdf/2511.02559", "abs": "https://arxiv.org/abs/2511.02559", "authors": ["Yao Wang", "Kexin Yu", "Wenyun Xu", "Kaiqiang Hu", "Ziyi Wang", "Lizhao You", "Qiang Su", "Dong Guo", "Haizhou Du", "Wanjian Feng", "Qingyu Song", "Linghe Kong", "Qiao Xiang", "Jiwu Shu"], "title": "Janus: Leveraging Incremental Computation for Efficient DNS Verification", "comment": null, "summary": "Existing DNS configuration verification tools face significant issues (e.g.,\ninefficient and lacking support for incremental verification). Inspired by the\nadvancements in recent work of distributed data plane verification and the\nresemblance be- tween the data plane and DNS configuration, we tackle the\nchallenge of DNS misconfiguration by introducing Janus, a DNS verification\ntool. Our key insight is that the process of a nameserver handling queries can\nbe transformed into a matching process on a match-action table. With this\ninsight, Janus consists of (1) an efficient data structure for partition query\nspace based on the behaviors, (2) a symbolic execution algorithm that specifies\nhow a single nameserver can efficiently cover all possible queries and ensure\nthe accuracy of verification, (3) a mechanism to support incremental\nverification with less computational effort. Extensive experiments on\nreal-world datasets (with over 6 million resource records) show that Janus\nachieves significant speedups, with peak improvements of up to 255.7x and a\nmaximum 6046x reduction in the number of LECs.", "AI": {"tldr": "Janus \u662f\u4e00\u79cd\u65b0\u578b DNS \u914d\u7f6e\u9a8c\u8bc1\u5de5\u5177\uff0c\u901a\u8fc7\u5c06\u57df\u540d\u670d\u52a1\u5668\u67e5\u8be2\u5904\u7406\u5efa\u6a21\u4e3a\u5339\u914d-\u52a8\u4f5c\u8868\u64cd\u4f5c\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u3001\u652f\u6301\u589e\u91cf\u9a8c\u8bc1\u7684 DNS \u914d\u7f6e\u68c0\u67e5\uff0c\u5728\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u663e\u8457\u63d0\u5347\u4e86\u9a8c\u8bc1\u901f\u5ea6\u5e76\u51cf\u5c11\u4e86\u8ba1\u7b97\u5f00\u9500\u3002", "motivation": "\u73b0\u6709 DNS \u914d\u7f6e\u9a8c\u8bc1\u5de5\u5177\u6548\u7387\u4f4e\u4e0b\u4e14\u4e0d\u652f\u6301\u589e\u91cf\u9a8c\u8bc1\uff0c\u96be\u4ee5\u5e94\u5bf9\u5927\u89c4\u6a21\u90e8\u7f72\u4e2d\u7684\u914d\u7f6e\u9519\u8bef\u95ee\u9898\u3002", "method": "Janus \u5c06\u540d\u79f0\u670d\u52a1\u5668\u5904\u7406\u67e5\u8be2\u7684\u8fc7\u7a0b\u8f6c\u5316\u4e3a\u5339\u914d-\u52a8\u4f5c\u8868\u4e0a\u7684\u5339\u914d\u8fc7\u7a0b\uff0c\u91c7\u7528\u57fa\u4e8e\u884c\u4e3a\u5212\u5206\u67e5\u8be2\u7a7a\u95f4\u7684\u6570\u636e\u7ed3\u6784\u3001\u7b26\u53f7\u6267\u884c\u7b97\u6cd5\u4ee5\u8986\u76d6\u6240\u6709\u53ef\u80fd\u67e5\u8be2\uff0c\u5e76\u8bbe\u8ba1\u4e86\u652f\u6301\u589e\u91cf\u9a8c\u8bc1\u7684\u673a\u5236\u3002", "result": "\u5728\u5305\u542b\u8d85\u8fc7 600 \u4e07\u6761\u8d44\u6e90\u8bb0\u5f55\u7684\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\uff0cJanus \u5b9e\u73b0\u4e86\u6700\u9ad8 255.7 \u500d\u7684\u901f\u5ea6\u63d0\u5347\u548c\u6700\u591a 6046 \u500d\u7684 LEC \u6570\u91cf\u51cf\u5c11\u3002", "conclusion": "Janus \u6709\u6548\u89e3\u51b3\u4e86 DNS \u914d\u7f6e\u9a8c\u8bc1\u4e2d\u7684\u6548\u7387\u4e0e\u53ef\u6269\u5c55\u6027\u95ee\u9898\uff0c\u4e3a\u5927\u89c4\u6a21 DNS \u7cfb\u7edf\u63d0\u4f9b\u4e86\u5b9e\u7528\u4e14\u9ad8\u6548\u7684\u9a8c\u8bc1\u65b9\u6848\u3002"}}
{"id": "2511.02434", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2511.02434", "abs": "https://arxiv.org/abs/2511.02434", "authors": ["Dominik Fuch\u00df", "Haoyu Liu", "Sophie Corallo", "Tobias Hey", "Jan Keim", "Johannes von Geisau", "Anne Koziolek"], "title": "Who's Who? LLM-assisted Software Traceability with Architecture Entity Recognition", "comment": null, "summary": "Identifying architecturally relevant entities in textual artifacts is crucial\nfor Traceability Link Recovery (TLR) between Software Architecture\nDocumentation (SAD) and source code. While Software Architecture Models (SAMs)\ncan bridge the semantic gap between these artifacts, their manual creation is\ntime-consuming. Large Language Models (LLMs) offer new capabilities for\nextracting architectural entities from SAD and source code to construct SAMs\nautomatically or establish direct trace links. This paper presents two\nLLM-based approaches: ExArch extracts component names as simple SAMs from SAD\nand source code to eliminate the need for manual SAM creation, while ArTEMiS\nidentifies architectural entities in documentation and matches them with\n(manually or automatically generated) SAM entities. Our evaluation compares\nagainst state-of-the-art approaches SWATTR, TransArC and ArDoCode. TransArC\nachieves strong performance (F1: 0.87) but requires manually created SAMs;\nExArch achieves comparable results (F1: 0.86) using only SAD and code. ArTEMiS\nis on par with the traditional heuristic-based SWATTR (F1: 0.81) and can\nsuccessfully replace it when integrated with TransArC. The combination of\nArTEMiS and ExArch outperforms ArDoCode, the best baseline without manual SAMs.\nOur results demonstrate that LLMs can effectively identify architectural\nentities in textual artifacts, enabling automated SAM generation and TLR,\nmaking architecture-code traceability more practical and accessible.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e24\u79cd\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u65b9\u6cd5ExArch\u548cArTEMiS\uff0c\u7528\u4e8e\u81ea\u52a8\u8bc6\u522b\u8f6f\u4ef6\u67b6\u6784\u6587\u6863\u548c\u6e90\u4ee3\u7801\u4e2d\u7684\u67b6\u6784\u76f8\u5173\u5b9e\u4f53\uff0c\u4ee5\u652f\u6301\u67b6\u6784\u6a21\u578b\u81ea\u52a8\u751f\u6210\u548c\u8ffd\u8e2a\u94fe\u63a5\u6062\u590d\uff0c\u6548\u679c\u5ab2\u7f8e\u6216\u4f18\u4e8e\u73b0\u6709\u5148\u8fdb\u65b9\u6cd5\u3002", "motivation": "\u624b\u52a8\u521b\u5efa\u8f6f\u4ef6\u67b6\u6784\u6a21\u578b\uff08SAM\uff09\u8017\u65f6\u4e14\u7e41\u7410\uff0c\u800c\u67b6\u6784\u6587\u6863\u4e0e\u6e90\u4ee3\u7801\u4e4b\u95f4\u7684\u8bed\u4e49\u9e3f\u6c9f\u963b\u788d\u4e86\u8ffd\u8e2a\u94fe\u63a5\u6062\u590d\uff08TLR\uff09\u3002\u5229\u7528LLM\u81ea\u52a8\u63d0\u53d6\u67b6\u6784\u5b9e\u4f53\u53ef\u964d\u4f4e\u4eba\u5de5\u6210\u672c\u5e76\u63d0\u5347TLR\u7684\u53ef\u884c\u6027\u3002", "method": "\u63d0\u51fa\u4e24\u79cdLLM\u9a71\u52a8\u7684\u65b9\u6cd5\uff1aExArch\u4ece\u67b6\u6784\u6587\u6863\u548c\u6e90\u4ee3\u7801\u4e2d\u63d0\u53d6\u7ec4\u4ef6\u540d\u4ee5\u81ea\u52a8\u751f\u6210\u7b80\u5316SAM\uff1bArTEMiS\u8bc6\u522b\u6587\u6863\u4e2d\u7684\u67b6\u6784\u5b9e\u4f53\u5e76\u4e0e\u5df2\u6709SAM\uff08\u624b\u52a8\u6216\u81ea\u52a8\u751f\u6210\uff09\u8fdb\u884c\u5339\u914d\u3002", "result": "ExArch\u5728\u65e0\u9700\u4eba\u5de5SAM\u7684\u60c5\u51b5\u4e0b\u8fbe\u5230F1 0.86\uff0c\u63a5\u8fd1\u9700\u4eba\u5de5SAM\u7684TransArC\uff08F1 0.87\uff09\uff1bArTEMiS\u8868\u73b0\u4e0eSWATTR\u76f8\u5f53\uff08F1 0.81\uff09\uff0c\u5e76\u4e0eTransArC\u7ed3\u5408\u4f7f\u7528\u6548\u679c\u66f4\u4f73\uff1bExArch+ArTEMiS\u7ec4\u5408\u4f18\u4e8e\u65e0\u9700\u4eba\u5de5SAM\u7684\u6700\u4f73\u57fa\u7ebfArDoCode\u3002", "conclusion": "\u5927\u8bed\u8a00\u6a21\u578b\u80fd\u6709\u6548\u8bc6\u522b\u6587\u672c\u5236\u54c1\u4e2d\u7684\u67b6\u6784\u5b9e\u4f53\uff0c\u652f\u6301\u81ea\u52a8\u5316SAM\u6784\u5efa\u4e0e\u8ffd\u8e2a\u94fe\u63a5\u6062\u590d\uff0c\u663e\u8457\u63d0\u5347\u67b6\u6784-\u4ee3\u7801\u53ef\u8ffd\u6eaf\u6027\u7684\u5b9e\u7528\u6027\u4e0e\u53ef\u53ca\u6027\u3002"}}
{"id": "2511.02530", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2511.02530", "abs": "https://arxiv.org/abs/2511.02530", "authors": ["Takuto Ando", "Yu Eto", "Yasuhiko Nakashima"], "title": "Implementation and Evaluation of Stable Diffusion on a General-Purpose CGLA Accelerator", "comment": "This paper is accepted at 2025 IEEE 18th International Symposium on\n  Embedded Multicore/Many-core Systems-on-Chip (MCSoC)", "summary": "This paper presents the first implementation and in-depth evaluation of the\nprimary computational kernels from the stable-diffusion.cpp image generation\nframework on IMAX3, a general-purpose Coarse-Grained Reconfigurable Array\n(CGRA) accelerator. We designed IMAX3 as a versatile computational platform,\nand this work assesses its capabilities by executing a demanding image\ngeneration workload. We evaluate its performance on a current\nField-Programmable Gate Array (FPGA) prototype to establish a baseline and\nproject its potential for a future Application-Specific Integrated Circuit\n(ASIC) implementation. Our results demonstrate that, despite its\ngeneral-purpose architecture, IMAX3 achieves promising performance and power\nefficiency, particularly in its projected ASIC form. This work provides\nconcrete guidelines for future IMAX architectural designs and establishes a\nfoundation for developing next-generation, AI-specialized Coarse-Grained Linear\nArray (CGLA) accelerators by refining this versatile platform. Ultimately, this\nachievement contributes to the realization of energy-efficient, on-device,\nmulti-modal AI platforms.", "AI": {"tldr": "\u672c\u6587\u9996\u6b21\u5728\u901a\u7528\u7c97\u7c92\u5ea6\u53ef\u91cd\u6784\u9635\u5217\uff08CGRA\uff09\u52a0\u901f\u5668IMAX3\u4e0a\u5b9e\u73b0\u4e86stable-diffusion.cpp\u56fe\u50cf\u751f\u6210\u6846\u67b6\u7684\u6838\u5fc3\u8ba1\u7b97\u5185\u6838\uff0c\u5e76\u5bf9\u5176\u6027\u80fd\u8fdb\u884c\u4e86\u6df1\u5165\u8bc4\u4f30\uff0c\u7ed3\u679c\u8868\u660eIMAX3\u5728ASIC\u5b9e\u73b0\u4e0b\u5177\u6709\u826f\u597d\u7684\u6027\u80fd\u4e0e\u80fd\u6548\u8868\u73b0\u3002", "motivation": "\u8bc4\u4f30\u901a\u7528CGRA\u67b6\u6784IMAX3\u5728\u6267\u884c\u9ad8\u8d1f\u8f7d\u56fe\u50cf\u751f\u6210\u4efb\u52a1\uff08\u5982Stable Diffusion\uff09\u65f6\u7684\u6027\u80fd\u6f5c\u529b\uff0c\u4e3a\u672a\u6765\u9762\u5411AI\u7684\u4e13\u7528\u52a0\u901f\u5668\u8bbe\u8ba1\u63d0\u4f9b\u4f9d\u636e\u3002", "method": "\u5728IMAX3\u7684FPGA\u539f\u578b\u4e0a\u90e8\u7f72\u5e76\u8fd0\u884cstable-diffusion.cpp\u7684\u5173\u952e\u8ba1\u7b97\u5185\u6838\uff0c\u901a\u8fc7\u5b9e\u6d4b\u5efa\u7acb\u6027\u80fd\u57fa\u7ebf\uff0c\u5e76\u5916\u63a8\u5176\u5728ASIC\u5b9e\u73b0\u4e0b\u7684\u9884\u671f\u8868\u73b0\u3002", "result": "\u5c3d\u7ba1IMAX3\u662f\u901a\u7528\u67b6\u6784\uff0c\u4f46\u5728FPGA\u539f\u578b\u4e0a\u5df2\u5c55\u73b0\u51fa\u826f\u597d\u6027\u80fd\uff0c\u4e14\u5728\u9884\u6d4b\u7684ASIC\u7248\u672c\u4e2d\u5177\u6709\u663e\u8457\u7684\u6027\u80fd\u548c\u80fd\u6548\u4f18\u52bf\u3002", "conclusion": "\u672c\u7814\u7a76\u9a8c\u8bc1\u4e86IMAX3\u4f5c\u4e3a\u591a\u529f\u80fd\u8ba1\u7b97\u5e73\u53f0\u7684\u53ef\u884c\u6027\uff0c\u4e3a\u4e0b\u4e00\u4ee3\u9762\u5411\u591a\u6a21\u6001AI\u7684\u4e13\u7528CGLA\u52a0\u901f\u5668\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u5177\u4f53\u6307\u5bfc\u548c\u57fa\u7840\u3002"}}
{"id": "2511.02445", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2511.02445", "abs": "https://arxiv.org/abs/2511.02445", "authors": ["Eriks Klotins", "Magnus Ahlgren", "Nicolas Martin Vivaldi", "Even-Andre Karlsson"], "title": "When Continuous Delivery Is Not an Option: Practical Paths to Continuous Engineering in Complex Organizations", "comment": null, "summary": "Purpose: Continuous Software Engineering (CSE) promises improved efficiency,\nquality, and responsiveness in software-intensive organizations. However, fully\nadopting CSE is often constrained by complex products, legacy systems,\norganizational inertia, and regulatory requirements. In this paper, we examine\nfour industrial cases from the automation, automotive, retail, and chemical\nsectors to explore how such constraints shape CSE adoption in practice.\nMethods: We apply and extend a previously proposed CSE Industry Readiness Model\nto assess the current and potential levels of adoption in each case. Through\nexpert interviews and narrative synthesis, we identify common driving forces\nand adoption barriers, including organizational preparedness,\ncross-organizational dependencies, and limited customer demand for continuous\ndelivery. Results: Based on our findings, we propose an updated readiness model\nthat introduces additional levels of internal and external feedback,\ndistinguishes market- and organization-facing constraints, and better guides\npractitioners in setting realistic CSE adoption goals. Conclusions: Our results\nhighlight that while full end-to-end CSE adoption may not always be feasible,\nmeaningful internal improvements are still possible and beneficial. This study\nprovides empirically grounded guidance for organizations navigating partial or\nconstrained CSE transformations.", "AI": {"tldr": "\u672c\u6587\u901a\u8fc7\u56db\u4e2a\u5de5\u4e1a\u6848\u4f8b\u7814\u7a76\uff0c\u5206\u6790\u4e86\u5728\u590d\u6742\u4ea7\u54c1\u3001\u9057\u7559\u7cfb\u7edf\u548c\u7ec4\u7ec7\u60ef\u6027\u7b49\u7ea6\u675f\u4e0b\u6301\u7eed\u8f6f\u4ef6\u5de5\u7a0b\uff08CSE\uff09\u7684\u5b9e\u9645\u91c7\u7eb3\u60c5\u51b5\uff0c\u5e76\u636e\u6b64\u66f4\u65b0\u4e86CSE\u884c\u4e1a\u5c31\u7eea\u6a21\u578b\uff0c\u4ee5\u5e2e\u52a9\u7ec4\u7ec7\u8bbe\u5b9a\u66f4\u73b0\u5b9e\u7684CSE\u76ee\u6807\u3002", "motivation": "\u5168\u9762\u91c7\u7528\u6301\u7eed\u8f6f\u4ef6\u5de5\u7a0b\uff08CSE\uff09\u5e38\u53d7\u9650\u4e8e\u590d\u6742\u4ea7\u54c1\u3001\u9057\u7559\u7cfb\u7edf\u3001\u7ec4\u7ec7\u60ef\u6027\u548c\u6cd5\u89c4\u8981\u6c42\uff0c\u56e0\u6b64\u9700\u8981\u7406\u89e3\u8fd9\u4e9b\u7ea6\u675f\u5982\u4f55\u5f71\u54cd\u5b9e\u9645\u91c7\u7eb3\u8fc7\u7a0b\uff0c\u5e76\u4e3a\u5b9e\u8df5\u8005\u63d0\u4f9b\u53ef\u884c\u6307\u5bfc\u3002", "method": "\u5e94\u7528\u5e76\u6269\u5c55\u5df2\u6709\u7684CSE\u884c\u4e1a\u5c31\u7eea\u6a21\u578b\uff0c\u7ed3\u5408\u4e13\u5bb6\u8bbf\u8c08\u4e0e\u53d9\u4e8b\u7efc\u5408\u65b9\u6cd5\uff0c\u5bf9\u81ea\u52a8\u5316\u3001\u6c7d\u8f66\u3001\u96f6\u552e\u548c\u5316\u5de5\u56db\u4e2a\u884c\u4e1a\u7684\u6848\u4f8b\u8fdb\u884c\u8bc4\u4f30\u4e0e\u5206\u6790\u3002", "result": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u66f4\u65b0\u7248\u7684\u5c31\u7eea\u6a21\u578b\uff0c\u589e\u52a0\u4e86\u5185\u5916\u90e8\u53cd\u9988\u5c42\u7ea7\uff0c\u533a\u5206\u4e86\u9762\u5411\u5e02\u573a\u4e0e\u7ec4\u7ec7\u7684\u7ea6\u675f\uff0c\u5e76\u8bc6\u522b\u51fa\u7ec4\u7ec7\u51c6\u5907\u5ea6\u3001\u8de8\u7ec4\u7ec7\u4f9d\u8d56\u548c\u5ba2\u6237\u5bf9\u6301\u7eed\u4ea4\u4ed8\u9700\u6c42\u6709\u9650\u7b49\u5173\u952e\u9a71\u52a8\u56e0\u7d20\u4e0e\u969c\u788d\u3002", "conclusion": "\u5c3d\u7ba1\u7aef\u5230\u7aef\u7684CSE\u5168\u9762\u91c7\u7eb3\u672a\u5fc5\u603b\u53ef\u884c\uff0c\u4f46\u5728\u7ea6\u675f\u6761\u4ef6\u4e0b\u4ecd\u53ef\u5b9e\u73b0\u6709\u610f\u4e49\u7684\u5185\u90e8\u6539\u8fdb\uff1b\u672c\u7814\u7a76\u4e3a\u9762\u4e34\u90e8\u5206\u6216\u53d7\u9650CSE\u8f6c\u578b\u7684\u7ec4\u7ec7\u63d0\u4f9b\u4e86\u5b9e\u8bc1\u6307\u5bfc\u3002"}}
{"id": "2511.02475", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2511.02475", "abs": "https://arxiv.org/abs/2511.02475", "authors": ["J\u00fcrgen Cito", "Dominik Bork"], "title": "Lost in Code Generation: Reimagining the Role of Software Models in AI-driven Software Engineering", "comment": null, "summary": "Generative AI enables rapid ``vibe coding,\" where natural language prompts\nyield working software systems. While this lowers barriers to software\ncreation, it also collapses the boundary between prototypes and engineered\nsoftware, leading to fragile systems that lack robustness, security, and\nmaintainability. We argue that this shift motivates a reimagining of software\nmodels. Rather than serving only as upfront blueprints, models can be recovered\npost-hoc from AI-generated code to restore comprehension, expose risks, and\nguide refinement. In this role, models serve as mediators between human intent,\nAI generation, and long-term system evolution, providing a path toward\nsustainable AI-driven software engineering.", "AI": {"tldr": "\u751f\u6210\u5f0fAI\u867d\u52a0\u901f\u4e86\u8f6f\u4ef6\u5f00\u53d1\uff0c\u4f46\u4e5f\u6a21\u7cca\u4e86\u539f\u578b\u4e0e\u5de5\u7a0b\u5316\u8f6f\u4ef6\u7684\u754c\u9650\uff0c\u5bfc\u81f4\u7cfb\u7edf\u8106\u5f31\uff1b\u6587\u7ae0\u4e3b\u5f20\u901a\u8fc7\u4e8b\u540e\u6062\u590d\u6a21\u578b\u6765\u91cd\u5efa\u7406\u89e3\u3001\u63ed\u793a\u98ce\u9669\u5e76\u5f15\u5bfc\u4f18\u5316\uff0c\u4f7f\u6a21\u578b\u6210\u4e3a\u8fde\u63a5\u4eba\u7c7b\u610f\u56fe\u3001AI\u751f\u6210\u4e0e\u7cfb\u7edf\u6f14\u5316\u7684\u4e2d\u4ecb\u3002", "motivation": "\u751f\u6210\u5f0fAI\u5e26\u6765\u7684\u201c\u6c1b\u56f4\u7f16\u7801\u201d\u867d\u964d\u4f4e\u4e86\u8f6f\u4ef6\u521b\u5efa\u95e8\u69db\uff0c\u5374\u4f7f\u539f\u578b\u4e0e\u5de5\u7a0b\u5316\u8f6f\u4ef6\u8fb9\u754c\u6d88\u5931\uff0c\u9020\u6210\u7cfb\u7edf\u7f3a\u4e4f\u9c81\u68d2\u6027\u3001\u5b89\u5168\u6027\u548c\u53ef\u7ef4\u62a4\u6027\uff0c\u4e9f\u9700\u91cd\u65b0\u601d\u8003\u8f6f\u4ef6\u6a21\u578b\u7684\u89d2\u8272\u3002", "method": "\u63d0\u51fa\u5c06\u8f6f\u4ef6\u6a21\u578b\u4eceAI\u751f\u6210\u7684\u4ee3\u7801\u4e2d\u4e8b\u540e\u6062\u590d\uff0c\u800c\u975e\u4ec5\u4f5c\u4e3a\u524d\u671f\u84dd\u56fe\uff0c\u4ee5\u6b64\u91cd\u5efa\u5bf9\u7cfb\u7edf\u7684\u7406\u89e3\u3001\u66b4\u9732\u6f5c\u5728\u98ce\u9669\u5e76\u6307\u5bfc\u540e\u7eed\u6539\u8fdb\u3002", "result": "\u6a21\u578b\u53ef\u4f5c\u4e3a\u4eba\u7c7b\u610f\u56fe\u3001AI\u751f\u6210\u7ed3\u679c\u4e0e\u7cfb\u7edf\u957f\u671f\u6f14\u5316\u4e4b\u95f4\u7684\u4e2d\u4ecb\uff0c\u4e3a\u53ef\u6301\u7eed\u7684AI\u9a71\u52a8\u8f6f\u4ef6\u5de5\u7a0b\u63d0\u4f9b\u8def\u5f84\u3002", "conclusion": "\u901a\u8fc7\u8d4b\u4e88\u8f6f\u4ef6\u6a21\u578b\u65b0\u7684\u4e2d\u4ecb\u89d2\u8272\uff0c\u53ef\u5728\u4fdd\u7559\u751f\u6210\u5f0fAI\u6548\u7387\u7684\u540c\u65f6\uff0c\u63d0\u5347AI\u751f\u6210\u8f6f\u4ef6\u7684\u53ef\u9760\u6027\u4e0e\u53ef\u6301\u7eed\u6027\u3002"}}
{"id": "2511.02713", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2511.02713", "abs": "https://arxiv.org/abs/2511.02713", "authors": ["Qianru Meng", "Zhaochun Ren", "Joost Visser"], "title": "ReleaseEval: A Benchmark for Evaluating Language Models in Automated Release Note Generation", "comment": null, "summary": "Automated release note generation addresses the challenge of documenting\nfrequent software updates, where manual efforts are time-consuming and prone to\nhuman error. Although recent advances in language models further enhance this\nprocess, progress remains hindered by dataset limitations, including the lack\nof explicit licensing and limited reproducibility, and incomplete task design\nthat relies mainly on commit messages for summarization while overlooking\nfine-grained contexts such as commit hierarchies and code changes. To fill this\ngap, we introduce ReleaseEval, a reproducible and openly licensed benchmark\ndesigned to systematically evaluate language models for automated release note\ngeneration. ReleaseEval comprises 94,987 release notes from 3,369 repositories\nacross 6 programming languages, and supports three task settings with three\nlevels of input granularity: (1) commit2sum, which generates release notes from\ncommit messages; (2) tree2sum, which incorporates commit tree structures; and\n(3) diff2sum, which leverages fine-grained code diffs. Both automated and human\nevaluations show that large language models consistently outperform traditional\nbaselines across all tasks, achieving substantial gains on tree2sum, while\nstill struggling on diff2sum. These findings highlight LLMs' proficiency in\nleveraging structured information while revealing challenges in abstracting\nfrom long code diffs.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86ReleaseEval\uff0c\u4e00\u4e2a\u53ef\u590d\u73b0\u4e14\u5f00\u6e90\u8bb8\u53ef\u7684\u57fa\u51c6\u6570\u636e\u96c6\uff0c\u7528\u4e8e\u7cfb\u7edf\u8bc4\u4f30\u8bed\u8a00\u6a21\u578b\u5728\u81ea\u52a8\u751f\u6210\u53d1\u5e03\u8bf4\u660e\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\uff0c\u6db5\u76d6\u4e09\u79cd\u4e0d\u540c\u7c92\u5ea6\u8f93\u5165\uff08\u63d0\u4ea4\u4fe1\u606f\u3001\u63d0\u4ea4\u6811\u7ed3\u6784\u548c\u4ee3\u7801\u5dee\u5f02\uff09\uff0c\u53d1\u73b0\u5927\u8bed\u8a00\u6a21\u578b\u5728\u5229\u7528\u7ed3\u6784\u5316\u4fe1\u606f\u65b9\u9762\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5728\u5904\u7406\u7ec6\u7c92\u5ea6\u4ee3\u7801\u5dee\u5f02\u65f6\u4ecd\u6709\u6311\u6218\u3002", "motivation": "\u5f53\u524d\u81ea\u52a8\u53d1\u5e03\u8bf4\u660e\u751f\u6210\u9762\u4e34\u6570\u636e\u96c6\u7f3a\u4e4f\u660e\u786e\u8bb8\u53ef\u3001\u53ef\u590d\u73b0\u6027\u5dee\u4ee5\u53ca\u4efb\u52a1\u8bbe\u8ba1\u4e0d\u5b8c\u6574\uff08\u4ec5\u4f9d\u8d56\u63d0\u4ea4\u4fe1\u606f\u800c\u5ffd\u7565\u63d0\u4ea4\u5c42\u7ea7\u548c\u4ee3\u7801\u53d8\u66f4\u7b49\u7ec6\u7c92\u5ea6\u4e0a\u4e0b\u6587\uff09\u7b49\u95ee\u9898\u3002", "method": "\u6784\u5efaReleaseEval\u57fa\u51c6\uff0c\u5305\u542b\u6765\u81ea3,369\u4e2a\u4ed3\u5e93\u768494,987\u6761\u53d1\u5e03\u8bf4\u660e\uff0c\u8986\u76d66\u79cd\u7f16\u7a0b\u8bed\u8a00\uff0c\u5e76\u5b9a\u4e49\u4e09\u79cd\u4efb\u52a1\u8bbe\u7f6e\uff1acommit2sum\uff08\u57fa\u4e8e\u63d0\u4ea4\u4fe1\u606f\uff09\u3001tree2sum\uff08\u7ed3\u5408\u63d0\u4ea4\u6811\u7ed3\u6784\uff09\u548cdiff2sum\uff08\u5229\u7528\u4ee3\u7801\u5dee\u5f02\uff09\u3002", "result": "\u81ea\u52a8\u4e0e\u4eba\u5de5\u8bc4\u4f30\u5747\u8868\u660e\uff0c\u5927\u8bed\u8a00\u6a21\u578b\u5728\u6240\u6709\u4efb\u52a1\u4e0a\u4f18\u4e8e\u4f20\u7edf\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5728tree2sum\u4efb\u52a1\u4e2d\u63d0\u5347\u663e\u8457\uff0c\u4f46\u5728diff2sum\u4efb\u52a1\u4e0a\u4ecd\u8868\u73b0\u4e0d\u4f73\u3002", "conclusion": "\u5927\u8bed\u8a00\u6a21\u578b\u64c5\u957f\u5229\u7528\u7ed3\u6784\u5316\u4fe1\u606f\u751f\u6210\u53d1\u5e03\u8bf4\u660e\uff0c\u4f46\u5728\u4ece\u957f\u4ee3\u7801\u5dee\u5f02\u4e2d\u62bd\u8c61\u5173\u952e\u4fe1\u606f\u65b9\u9762\u4ecd\u5b58\u5728\u56f0\u96be\uff0c\u672a\u6765\u9700\u8fdb\u4e00\u6b65\u6539\u8fdb\u5bf9\u7ec6\u7c92\u5ea6\u4ee3\u7801\u53d8\u66f4\u7684\u7406\u89e3\u80fd\u529b\u3002"}}
{"id": "2511.01893", "categories": ["cs.DC", "cs.PF"], "pdf": "https://arxiv.org/pdf/2511.01893", "abs": "https://arxiv.org/abs/2511.01893", "authors": ["Bin Ma", "Viktor Nikitin", "Xi Wang", "Tekin Bicer", "Dong Li"], "title": "mLR: Scalable Laminography Reconstruction based on Memoization", "comment": null, "summary": "ADMM-FFT is an iterative method with high reconstruction accuracy for\nlaminography but suffers from excessive computation time and large memory\nconsumption. We introduce mLR, which employs memoization to replace the\ntime-consuming Fast Fourier Transform (FFT) operations based on an unique\nobservation that similar FFT operations appear in iterations of ADMM-FFT. We\nintroduce a series of techniques to make the application of memoization to\nADMM-FFT performance-beneficial and scalable. We also introduce variable\noffloading to save CPU memory and scale ADMM-FFT across GPUs within and across\nnodes. Using mLR, we are able to scale ADMM-FFT on an input problem of\n2Kx2Kx2K, which is the largest input problem laminography reconstruction has\never worked on with the ADMM-FFT solution on limited memory; mLR brings 52.8%\nperformance improvement on average (up to 65.4%), compared to the original\nADMM-FFT.", "AI": {"tldr": "\u672c\u6587\u63d0\u51famLR\u65b9\u6cd5\uff0c\u901a\u8fc7\u8bb0\u5fc6\u5316\u6280\u672f\u4f18\u5316ADMM-FFT\u7b97\u6cd5\u4e2d\u91cd\u590d\u7684FFT\u64cd\u4f5c\uff0c\u5e76\u7ed3\u5408\u53d8\u91cf\u5378\u8f7d\u7b56\u7565\uff0c\u5728\u6709\u9650\u5185\u5b58\u4e0b\u5b9e\u73b0\u4e86\u5bf92K\u00d72K\u00d72K\u89c4\u6a21\u5c42\u6790\u6210\u50cf\u91cd\u5efa\u95ee\u9898\u7684\u9ad8\u6548\u6c42\u89e3\uff0c\u5e73\u5747\u6027\u80fd\u63d0\u534752.8%\u3002", "motivation": "ADMM-FFT\u867d\u5728\u5c42\u6790\u6210\u50cf\u91cd\u5efa\u4e2d\u5177\u6709\u9ad8\u7cbe\u5ea6\uff0c\u4f46\u5b58\u5728\u8ba1\u7b97\u65f6\u95f4\u8fc7\u957f\u548c\u5185\u5b58\u6d88\u8017\u5927\u7684\u95ee\u9898\uff0c\u9650\u5236\u4e86\u5176\u5728\u5927\u89c4\u6a21\u95ee\u9898\u4e2d\u7684\u5e94\u7528\u3002", "method": "\u5f15\u5165\u8bb0\u5fc6\u5316\uff08memoization\uff09\u6280\u672f\u66ff\u4ee3ADMM-FFT\u8fed\u4ee3\u8fc7\u7a0b\u4e2d\u91cd\u590d\u4e14\u8017\u65f6\u7684FFT\u64cd\u4f5c\uff0c\u5e76\u8bbe\u8ba1\u4e00\u7cfb\u5217\u4f18\u5316\u7b56\u7565\u4f7f\u8bb0\u5fc6\u5316\u5728\u6027\u80fd\u548c\u53ef\u6269\u5c55\u6027\u4e0a\u6709\u6548\uff1b\u540c\u65f6\u91c7\u7528\u53d8\u91cf\u5378\u8f7d\uff08variable offloading\uff09\u6280\u672f\u8282\u7701CPU\u5185\u5b58\uff0c\u652f\u6301\u8de8GPU\u4e43\u81f3\u8de8\u8282\u70b9\u6269\u5c55\u3002", "result": "mLR\u6210\u529f\u5c06ADMM-FFT\u6269\u5c55\u81f32K\u00d72K\u00d72K\u8f93\u5165\u89c4\u6a21\uff0c\u8fd9\u662f\u76ee\u524d\u5728\u6709\u9650\u5185\u5b58\u6761\u4ef6\u4e0b\u4f7f\u7528ADMM-FFT\u8fdb\u884c\u5c42\u6790\u6210\u50cf\u91cd\u5efa\u7684\u6700\u5927\u95ee\u9898\u89c4\u6a21\uff0c\u5e73\u5747\u6027\u80fd\u63d0\u534752.8%\uff08\u6700\u9ad8\u8fbe65.4%\uff09\u3002", "conclusion": "\u901a\u8fc7\u8bb0\u5fc6\u5316\u4e0e\u53d8\u91cf\u5378\u8f7d\u76f8\u7ed3\u5408\uff0cmLR\u663e\u8457\u63d0\u5347\u4e86ADMM-FFT\u7684\u6548\u7387\u548c\u53ef\u6269\u5c55\u6027\uff0c\u4f7f\u5176\u9002\u7528\u4e8e\u66f4\u5927\u89c4\u6a21\u7684\u5c42\u6790\u6210\u50cf\u91cd\u5efa\u4efb\u52a1\u3002"}}
{"id": "2511.02736", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2511.02736", "abs": "https://arxiv.org/abs/2511.02736", "authors": ["Madalena Sasportes", "Grischa Liebel", "Miguel Goul\u00e3o"], "title": "Investigating the Experience of Autistic Individuals in Software Engineering", "comment": null, "summary": "Context: Autism spectrum disorder (ASD) leads to various issues in the\neveryday life of autistic individuals, often resulting in unemployment and\nmental health problems. To improve the inclusion of autistic adults, existing\nstudies have highlighted the strengths these individuals possess in comparison\nto non-autistic individuals, e.g., high attention to detail or excellent\nlogical reasoning skills. If fostered, these strengths could be valuable in\nsoftware engineering activities, such for identifying specific kinds of bugs in\ncode. However, existing work in SE has primarily studied the challenges of\nautistic individuals and possible accommodations, with little attention their\nstrengths. Objective: Our goal is to analyse the experiences of autistic\nindividuals in software engineering activities, such as code reviews, with a\nparticular emphasis on strengths. Methods: This study combines Social-Technical\nGrounded Theory through semi-structured interviews with 16 autistic software\nengineers and a survey with 49 respondents, including 5 autistic participants.\nWe compare the emerging themes with the theory by Gama et al. on the Effect of\nNeurodivergent Cognitive Dysfunctions in Software Engineering Performance.\nResults: Our results suggest that autistic software engineers are often skilled\nin logical thinking, attention to detail, and hyperfocus in programming; and\nthey enjoy learning new programming languages and programming-related\ntechnologies. Confirming previous work, they tend to prefer written\ncommunication and remote work. Finally, we report a high comfort level in\ninteracting with AI-based systems. Conclusions: Our findings extend existing\nwork by providing further evidence on the strengths of autistic software\nengineers.", "AI": {"tldr": "\u8be5\u7814\u7a76\u901a\u8fc7\u8bbf\u8c08\u548c\u95ee\u5377\u8c03\u67e5\uff0c\u53d1\u73b0\u81ea\u95ed\u75c7\u8f6f\u4ef6\u5de5\u7a0b\u5e08\u5728\u903b\u8f91\u601d\u7ef4\u3001\u7ec6\u8282\u5173\u6ce8\u3001\u7f16\u7a0b\u4e2d\u7684\u9ad8\u5ea6\u4e13\u6ce8\u7b49\u65b9\u9762\u5177\u6709\u663e\u8457\u4f18\u52bf\uff0c\u5e76\u504f\u597d\u4e66\u9762\u6c9f\u901a\u3001\u8fdc\u7a0b\u5de5\u4f5c\u53ca\u4e0eAI\u7cfb\u7edf\u4e92\u52a8\u3002", "motivation": "\u73b0\u6709\u8f6f\u4ef6\u5de5\u7a0b\u7814\u7a76\u591a\u805a\u7126\u4e8e\u81ea\u95ed\u75c7\u4e2a\u4f53\u9762\u4e34\u7684\u6311\u6218\u548c\u6240\u9700\u652f\u6301\uff0c\u800c\u5ffd\u89c6\u4e86\u5176\u72ec\u7279\u4f18\u52bf\uff1b\u672c\u7814\u7a76\u65e8\u5728\u6df1\u5165\u63a2\u7d22\u81ea\u95ed\u75c7\u8f6f\u4ef6\u5de5\u7a0b\u5e08\u5728\u8f6f\u4ef6\u5de5\u7a0b\u6d3b\u52a8\uff08\u5982\u4ee3\u7801\u5ba1\u67e5\uff09\u4e2d\u7684\u4f18\u52bf\u4f53\u9a8c\u3002", "method": "\u7ed3\u5408\u793e\u4f1a\u6280\u672f\u624e\u6839\u7406\u8bba\uff0c\u5bf916\u540d\u81ea\u95ed\u75c7\u8f6f\u4ef6\u5de5\u7a0b\u5e08\u8fdb\u884c\u534a\u7ed3\u6784\u5316\u8bbf\u8c08\uff0c\u5e76\u8f85\u4ee5\u5305\u542b49\u540d\u53d7\u8bbf\u8005\uff08\u5176\u4e2d5\u540d\u4e3a\u81ea\u95ed\u75c7\u4eba\u58eb\uff09\u7684\u95ee\u5377\u8c03\u67e5\uff0c\u5c06\u7ed3\u679c\u4e0eGama\u7b49\u4eba\u5173\u4e8e\u795e\u7ecf\u591a\u6837\u6027\u8ba4\u77e5\u529f\u80fd\u969c\u788d\u5bf9\u8f6f\u4ef6\u5de5\u7a0b\u8868\u73b0\u5f71\u54cd\u7684\u7406\u8bba\u8fdb\u884c\u5bf9\u6bd4\u5206\u6790\u3002", "result": "\u81ea\u95ed\u75c7\u8f6f\u4ef6\u5de5\u7a0b\u5e08\u666e\u904d\u5c55\u73b0\u51fa\u903b\u8f91\u601d\u7ef4\u80fd\u529b\u5f3a\u3001\u6ce8\u91cd\u7ec6\u8282\u3001\u7f16\u7a0b\u65f6\u80fd\u9ad8\u5ea6\u4e13\u6ce8\u7b49\u4f18\u52bf\uff1b\u4ed6\u4eec\u4e50\u4e8e\u5b66\u4e60\u65b0\u7f16\u7a0b\u8bed\u8a00\u548c\u6280\u672f\uff0c\u504f\u597d\u4e66\u9762\u6c9f\u901a\u4e0e\u8fdc\u7a0b\u5de5\u4f5c\uff0c\u5e76\u5bf9\u4e0eAI\u7cfb\u7edf\u4ea4\u4e92\u611f\u5230\u9ad8\u5ea6\u8212\u9002\u3002", "conclusion": "\u672c\u7814\u7a76\u901a\u8fc7\u5b9e\u8bc1\u8fdb\u4e00\u6b65\u8bc1\u5b9e\u4e86\u81ea\u95ed\u75c7\u8f6f\u4ef6\u5de5\u7a0b\u5e08\u7684\u4f18\u52bf\uff0c\u62d3\u5c55\u4e86\u73b0\u6709\u6587\u732e\u5bf9\u795e\u7ecf\u591a\u6837\u6027\u5728\u8f6f\u4ef6\u5de5\u7a0b\u4e2d\u79ef\u6781\u4f5c\u7528\u7684\u7406\u89e3\u3002"}}
{"id": "2511.02034", "categories": ["cs.DC", "cs.ET", "cs.NI"], "pdf": "https://arxiv.org/pdf/2511.02034", "abs": "https://arxiv.org/abs/2511.02034", "authors": ["Shashank Motepalli", "Naman Garg", "Gengrui Zhang", "Hans-Arno Jacobsen"], "title": "GPoS: Geospatially-aware Proof of Stake", "comment": "Published in ACM TWEB", "summary": "Geospatial decentralization is essential for blockchains, ensuring regulatory\nresilience, robustness, and fairness. We empirically analyze five major Proof\nof Stake (PoS) blockchains: Aptos, Avalanche, Ethereum, Solana, and Sui,\nrevealing that a few geographic regions dominate consensus voting power,\nresulting in limited geospatial decentralization. To address this, we propose\nGeospatially aware Proof of Stake (GPoS), which integrates geospatial diversity\nwith stake-based voting power. Experimental evaluation demonstrates an average\n45% improvement in geospatial decentralization, as measured by the Gini\ncoefficient of Eigenvector centrality, while incurring minimal performance\noverhead in BFT protocols, including HotStuff and CometBFT. These results\ndemonstrate that GPoS can improve geospatial decentralization {while, in our\nexperiments, incurring minimal overhead} to consensus performance.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5730\u7406\u611f\u77e5\u6743\u76ca\u8bc1\u660e\u673a\u5236\uff08GPoS\uff09\uff0c\u901a\u8fc7\u7ed3\u5408\u5730\u7406\u4f4d\u7f6e\u591a\u6837\u6027\u4e0e\u6743\u76ca\u6295\u7968\u6743\uff0c\u5728\u4e3b\u6d41PoS\u533a\u5757\u94fe\u4e2d\u663e\u8457\u63d0\u5347\u5730\u7406\u53bb\u4e2d\u5fc3\u5316\u6c34\u5e73\uff0c\u540c\u65f6\u5bf9\u5171\u8bc6\u6027\u80fd\u5f71\u54cd\u6781\u5c0f\u3002", "motivation": "\u73b0\u6709\u4e3b\u6d41\u6743\u76ca\u8bc1\u660e\uff08PoS\uff09\u533a\u5757\u94fe\u5728\u5171\u8bc6\u6295\u7968\u6743\u4e0a\u9ad8\u5ea6\u96c6\u4e2d\u4e8e\u5c11\u6570\u5730\u7406\u533a\u57df\uff0c\u5bfc\u81f4\u5730\u7406\u53bb\u4e2d\u5fc3\u5316\u7a0b\u5ea6\u4e0d\u8db3\uff0c\u5f71\u54cd\u7cfb\u7edf\u7684\u76d1\u7ba1\u97e7\u6027\u3001\u9c81\u68d2\u6027\u548c\u516c\u5e73\u6027\u3002", "method": "\u4f5c\u8005\u5b9e\u8bc1\u5206\u6790\u4e86Aptos\u3001Avalanche\u3001Ethereum\u3001Solana\u548cSui\u4e94\u4e2a\u4e3b\u6d41PoS\u533a\u5757\u94fe\u7684\u5730\u7406\u5206\u5e03\uff0c\u5e76\u63d0\u51fa\u4e86GPoS\u673a\u5236\uff0c\u5c06\u5730\u7406\u4f4d\u7f6e\u591a\u6837\u6027\u7eb3\u5165\u6743\u76ca\u6295\u7968\u6743\u8ba1\u7b97\u4e2d\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cGPoS\u5e73\u5747\u53ef\u5c06\u57fa\u4e8e\u7279\u5f81\u5411\u91cf\u4e2d\u5fc3\u6027\u7684\u57fa\u5c3c\u7cfb\u6570\u8861\u91cf\u7684\u5730\u7406\u53bb\u4e2d\u5fc3\u5316\u6c34\u5e73\u63d0\u534745%\uff0c\u4e14\u5728HotStuff\u548cCometBFT\u7b49BFT\u534f\u8bae\u4e2d\u4ec5\u5e26\u6765\u6781\u5c0f\u7684\u6027\u80fd\u5f00\u9500\u3002", "conclusion": "GPoS\u80fd\u6709\u6548\u589e\u5f3a\u533a\u5757\u94fe\u7684\u5730\u7406\u53bb\u4e2d\u5fc3\u5316\uff0c\u540c\u65f6\u51e0\u4e4e\u4e0d\u5f71\u54cd\u5171\u8bc6\u6027\u80fd\uff0c\u4e3a\u6784\u5efa\u66f4\u5177\u97e7\u6027\u4e0e\u516c\u5e73\u6027\u7684\u533a\u5757\u94fe\u7cfb\u7edf\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848\u3002"}}
{"id": "2511.02810", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2511.02810", "abs": "https://arxiv.org/abs/2511.02810", "authors": ["Suddhasvatta Das", "Kevin Gary"], "title": "Formalizing Regression Testing for Agile and Continuous Integration Environments", "comment": "This is the first attempt to formalize regression testing in agile\n  context as a continuous/near-continuos activity. This formalization will help\n  practitioners and researchers to answer 'when', 'what' and 'how much'\n  question of regression testing in real world time constrained agile projects.\n  This work is currently under review with Software Quality Journal", "summary": "Software developed using modern agile practices delivers a stream of software\nversions that require continuous regression testing rather than testing once\nclose to the delivery or maintenance phase, as assumed by classical\nregression-testing theory. In this work, we formalize the phenomenon of\ncontinuous or near-continuous regression testing using successive builds as a\ntime-ordered chain, where each build contains the program, requirements, and\nthe accompanying tests. We also formalize the regression test window between\nany two builds, which captures the limited time budget available for regression\ntesting. As the time limit is set to infinity and the chain is closed to two\nbuilds, the model degenerates to retest-all, thereby preserving semantics for\nthe classical two-version case. The formalization is validated by directly\nrepresenting two state-of-the-art agile regression testing algorithms in terms\nof build-tuple operations without requiring auxiliary assumptions, followed by\nproof of the soundness and completeness of our formalization.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u63cf\u8ff0\u654f\u6377\u5f00\u53d1\u4e2d\u8fde\u7eed\u56de\u5f52\u6d4b\u8bd5\u7684\u5f62\u5f0f\u5316\u6a21\u578b\uff0c\u5c06\u8f6f\u4ef6\u6784\u5efa\u5e8f\u5217\u5efa\u6a21\u4e3a\u65f6\u95f4\u6709\u5e8f\u94fe\uff0c\u5e76\u5b9a\u4e49\u4e86\u6784\u5efa\u95f4\u7684\u56de\u5f52\u6d4b\u8bd5\u7a97\u53e3\uff1b\u8be5\u6a21\u578b\u5728\u7279\u5b9a\u6761\u4ef6\u4e0b\u53ef\u9000\u5316\u4e3a\u7ecf\u5178\u56de\u5f52\u6d4b\u8bd5\u573a\u666f\uff0c\u5e76\u901a\u8fc7\u8868\u793a\u73b0\u6709\u7b97\u6cd5\u9a8c\u8bc1\u4e86\u5176\u6b63\u786e\u6027\u4e0e\u5b8c\u5907\u6027\u3002", "motivation": "\u4f20\u7edf\u56de\u5f52\u6d4b\u8bd5\u7406\u8bba\u5047\u8bbe\u8f6f\u4ef6\u4ec5\u5728\u4ea4\u4ed8\u6216\u7ef4\u62a4\u9636\u6bb5\u8fdb\u884c\u4e00\u6b21\u6d4b\u8bd5\uff0c\u800c\u73b0\u4ee3\u654f\u6377\u5f00\u53d1\u5b9e\u8df5\u8981\u6c42\u5bf9\u6301\u7eed\u53d1\u5e03\u7684\u7248\u672c\u8fdb\u884c\u8fde\u7eed\u56de\u5f52\u6d4b\u8bd5\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u65b0\u7684\u5f62\u5f0f\u5316\u65b9\u6cd5\u6765\u51c6\u786e\u523b\u753b\u8fd9\u4e00\u73b0\u8c61\u3002", "method": "\u4f5c\u8005\u5c06\u8fde\u7eed\u56de\u5f52\u6d4b\u8bd5\u5efa\u6a21\u4e3a\u4e00\u4e2a\u65f6\u95f4\u6709\u5e8f\u7684\u6784\u5efa\u94fe\uff0c\u6bcf\u4e2a\u6784\u5efa\u5305\u542b\u7a0b\u5e8f\u3001\u9700\u6c42\u548c\u5bf9\u5e94\u6d4b\u8bd5\uff0c\u5e76\u5f15\u5165\u201c\u56de\u5f52\u6d4b\u8bd5\u7a97\u53e3\u201d\u6982\u5ff5\u4ee5\u53cd\u6620\u6709\u9650\u7684\u6d4b\u8bd5\u65f6\u95f4\u9884\u7b97\uff1b\u968f\u540e\u901a\u8fc7\u5c06\u4e24\u79cd\u5148\u8fdb\u7684\u654f\u6377\u56de\u5f52\u6d4b\u8bd5\u7b97\u6cd5\u76f4\u63a5\u8868\u8fbe\u4e3a\u6784\u5efa\u5143\u7ec4\u64cd\u4f5c\uff0c\u9a8c\u8bc1\u8be5\u5f62\u5f0f\u5316\u6a21\u578b\u7684\u5408\u7406\u6027\u3002", "result": "\u6240\u63d0\u51fa\u7684\u5f62\u5f0f\u5316\u6a21\u578b\u80fd\u591f\u51c6\u786e\u8868\u793a\u73b0\u6709\u654f\u6377\u56de\u5f52\u6d4b\u8bd5\u7b97\u6cd5\uff0c\u4e14\u5728\u65f6\u95f4\u9884\u7b97\u65e0\u9650\u3001\u6784\u5efa\u94fe\u9000\u5316\u4e3a\u4e24\u4e2a\u7248\u672c\u65f6\uff0c\u53ef\u8fd8\u539f\u4e3a\u7ecf\u5178\u7684\u201c\u5168\u90e8\u91cd\u6d4b\u201d\u7b56\u7565\uff0c\u8bc1\u660e\u4e86\u6a21\u578b\u7684\u8bed\u4e49\u4e00\u81f4\u6027\uff1b\u540c\u65f6\u5b8c\u6210\u4e86\u8be5\u5f62\u5f0f\u5316\u4f53\u7cfb\u7684soundness\u4e0ecompleteness\u8bc1\u660e\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u8fde\u7eed\u56de\u5f52\u6d4b\u8bd5\u63d0\u4f9b\u4e86\u4e00\u4e2a\u4e25\u8c28\u4e14\u901a\u7528\u7684\u5f62\u5f0f\u5316\u6846\u67b6\uff0c\u65e2\u517c\u5bb9\u7ecf\u5178\u7406\u8bba\uff0c\u53c8\u80fd\u6709\u6548\u652f\u6301\u73b0\u4ee3\u654f\u6377\u5f00\u53d1\u73af\u5883\u4e0b\u7684\u6d4b\u8bd5\u5b9e\u8df5\u3002"}}
{"id": "2511.02168", "categories": ["cs.DC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.02168", "abs": "https://arxiv.org/abs/2511.02168", "authors": ["Octavian Alexandru Trifan", "Karthik Sangaiah", "Muhammad Awad", "Muhammad Osama", "Sumanth Gudaparthi", "Alexandru Nicolau", "Alexander Veidenbaum", "Ganesh Dasika"], "title": "Eliminating Multi-GPU Performance Taxes: A Systems Approach to Efficient Distributed LLMs", "comment": null, "summary": "As large language models (LLMs) continue to scale, their workloads\nincreasingly rely on distributed execution across multiple GPUs. However, the\nconventional bulk synchronous parallel~(BSP) model used in such settings\nintroduces significant performance inefficiencies. To characterize these\nbottlenecks, we introduce the ''Three Taxes'' (Bulk Synchronous, Inter-Kernel\nData Locality, and Kernel Launch Overhead) as an analytical framework. We\npropose moving beyond the rigid BSP model to address key inefficiencies in\ndistributed GPU execution. By exploiting libraries like Iris for Triton, we\ngain access to in-kernel communication primitives that enable the design of\nnovel fine-grained programming patterns, offering greater flexibility and\nperformance than traditional BSP-based approaches. These patterns\nsystematically eliminate the three taxes by creating direct, tile-level\nproducer-consumer pipelines and replacing global barriers with fine-grained\ndataflow synchronization. Applying this methodology to critical kernels, from\nthe foundational All-Gather + general matrix multiplication operation to the\ncomplex Flash Decode algorithm, we observe a 10-20% speedup in end-to-end\nlatency over BSP-based approaches, establishing a more programmable and\nefficient paradigm for distributed LLM workloads.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u8d85\u8d8a\u4f20\u7edfBSP\u6a21\u578b\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u5229\u7528Triton\u4e2d\u7684Iris\u7b49\u5e93\u5b9e\u73b0\u7ec6\u7c92\u5ea6\u901a\u4fe1\u4e0e\u540c\u6b65\uff0c\u7cfb\u7edf\u6027\u6d88\u9664\u201c\u4e09\u5927\u5f00\u9500\u201d\uff0c\u5728\u5206\u5e03\u5f0f\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5173\u952e\u7b97\u5b50\u4e0a\u5b9e\u73b0\u4e8610-20%\u7684\u7aef\u5230\u7aef\u5ef6\u8fdf\u63d0\u5347\u3002", "motivation": "\u4f20\u7edfBSP\u6a21\u578b\u5728\u5206\u5e03\u5f0fGPU\u6267\u884c\u4e2d\u5f15\u5165\u663e\u8457\u6027\u80fd\u74f6\u9888\uff0c\u4f5c\u8005\u65e8\u5728\u901a\u8fc7\u65b0\u7f16\u7a0b\u8303\u5f0f\u89e3\u51b3\u8fd9\u4e9b\u4f4e\u6548\u95ee\u9898\u3002", "method": "\u5f15\u5165\u201c\u4e09\u5927\u5f00\u9500\u201d\u5206\u6790\u6846\u67b6\uff0c\u5e76\u57fa\u4e8eIris for Triton\u63d0\u4f9b\u7684\u6838\u5185\u901a\u4fe1\u539f\u8bed\uff0c\u8bbe\u8ba1\u7ec6\u7c92\u5ea6\u7684\u751f\u4ea7\u8005-\u6d88\u8d39\u8005\u6d41\u6c34\u7ebf\u548c\u6570\u636e\u6d41\u540c\u6b65\u673a\u5236\uff0c\u66ff\u4ee3\u5168\u5c40\u5c4f\u969c\u548c\u7c97\u7c92\u5ea6\u540c\u6b65\u3002", "result": "\u5728All-Gather+GEMM\u548cFlash Decode\u7b49\u5173\u952e\u7b97\u5b50\u4e0a\uff0c\u76f8\u6bd4BSP\u65b9\u6cd5\u83b7\u5f9710-20%\u7684\u7aef\u5230\u7aef\u5ef6\u8fdf\u52a0\u901f\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u5206\u5e03\u5f0fLLM\u5de5\u4f5c\u8d1f\u8f7d\u63d0\u4f9b\u4e86\u4e00\u79cd\u66f4\u9ad8\u6548\u3001\u66f4\u53ef\u7f16\u7a0b\u7684\u65b0\u8303\u5f0f\uff0c\u6709\u6548\u514b\u670d\u4e86\u4f20\u7edfBSP\u6a21\u578b\u7684\u5c40\u9650\u6027\u3002"}}
{"id": "2511.02827", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2511.02827", "abs": "https://arxiv.org/abs/2511.02827", "authors": ["Mohamed Almukhtar", "Anwar Ghammam", "Marouane Kessentini", "Hua Ming"], "title": "From Code Changes to Quality Gains: An Empirical Study in Python ML Systems with PyQu", "comment": "Accepted for publication in the proceedings of IEEE/ACM 48th\n  International Conference on Software Engineering", "summary": "In an era shaped by Generative Artificial Intelligence for code generation\nand the rising adoption of Python-based Machine Learning systems (MLS),\nsoftware quality has emerged as a major concern. As these systems grow in\ncomplexity and importance, a key obstacle lies in understanding exactly how\nspecific code changes affect overall quality-a shortfall aggravated by the lack\nof quality assessment tools and a clear mapping between ML systems code changes\nand their quality effects. Although prior work has explored code changes in\nMLS, it mostly stops at what the changes are, leaving a gap in our knowledge of\nthe relationship between code changes and the MLS quality. To address this gap,\nwe conducted a large-scale empirical study of 3,340 open-source Python ML\nprojects, encompassing more than 3.7 million commits and 2.7 trillion lines of\ncode. We introduce PyQu, a novel tool that leverages low level software metrics\nto identify quality-enhancing commits with an average accuracy, precision, and\nrecall of 0.84 and 0.85 of average F1 score. Using PyQu and a thematic\nanalysis, we identified 61 code changes, each demonstrating a direct impact on\nenhancing software quality, and we classified them into 13 categories based on\ncontextual characteristics. 41% of the changes are newly discovered by our\nstudy and have not been identified by state-of-the-art Python changes detection\ntools. Our work offers a vital foundation for researchers, practitioners,\neducators, and tool developers, advancing the quest for automated quality\nassessment and best practices in Python-based ML software.", "AI": {"tldr": "\u8be5\u7814\u7a76\u901a\u8fc7\u5927\u89c4\u6a21\u5b9e\u8bc1\u5206\u67903,340\u4e2a\u5f00\u6e90Python\u673a\u5668\u5b66\u4e60\u9879\u76ee\uff0c\u5f00\u53d1\u4e86\u540d\u4e3aPyQu\u7684\u65b0\u5de5\u5177\uff0c\u7528\u4e8e\u8bc6\u522b\u63d0\u5347\u8f6f\u4ef6\u8d28\u91cf\u7684\u4ee3\u7801\u53d8\u66f4\uff0c\u5e76\u5f52\u7eb3\u51fa61\u79cd\u5bf9\u8d28\u91cf\u6709\u76f4\u63a5\u5f71\u54cd\u7684\u53d8\u66f4\u6a21\u5f0f\uff0c\u5176\u4e2d41%\u4e3a\u6b64\u524d\u672a\u88ab\u53d1\u73b0\u7684\u65b0\u7c7b\u578b\u3002", "motivation": "\u968f\u7740\u751f\u6210\u5f0fAI\u5728\u4ee3\u7801\u751f\u6210\u4e2d\u7684\u5e7f\u6cdb\u5e94\u7528\u4ee5\u53caPython\u673a\u5668\u5b66\u4e60\u7cfb\u7edf\u7684\u666e\u53ca\uff0c\u8f6f\u4ef6\u8d28\u91cf\u6210\u4e3a\u5173\u952e\u95ee\u9898\u3002\u73b0\u6709\u7814\u7a76\u7f3a\u4e4f\u5bf9\u4ee3\u7801\u53d8\u66f4\u5982\u4f55\u5177\u4f53\u5f71\u54cd\u673a\u5668\u5b66\u4e60\u7cfb\u7edf\u8d28\u91cf\u7684\u6df1\u5165\u7406\u89e3\uff0c\u4e14\u7f3a\u5c11\u6709\u6548\u7684\u8d28\u91cf\u8bc4\u4f30\u5de5\u5177\u548c\u6e05\u6670\u7684\u6620\u5c04\u5173\u7cfb\u3002", "method": "\u7814\u7a76\u5bf9\u8d85\u8fc73.7\u767e\u4e07\u6b21\u63d0\u4ea4\u548c2.7\u4e07\u4ebf\u884c\u4ee3\u7801\u8fdb\u884c\u5927\u89c4\u6a21\u5b9e\u8bc1\u5206\u6790\uff0c\u5f00\u53d1\u4e86\u57fa\u4e8e\u5e95\u5c42\u8f6f\u4ef6\u5ea6\u91cf\u7684PyQu\u5de5\u5177\uff0c\u7ed3\u5408\u4e3b\u9898\u5206\u6790\u8bc6\u522b\u5e76\u5206\u7c7b\u5f71\u54cd\u8d28\u91cf\u7684\u4ee3\u7801\u53d8\u66f4\u3002", "result": "PyQu\u5728\u8bc6\u522b\u8d28\u91cf\u63d0\u5347\u578b\u63d0\u4ea4\u65b9\u9762\u8fbe\u5230\u5e73\u57470.84\u7684\u51c6\u786e\u7387\u3001\u7cbe\u786e\u7387\u548c\u53ec\u56de\u7387\uff0cF1\u5206\u6570\u5e73\u5747\u4e3a0.85\uff1b\u5171\u8bc6\u522b\u51fa61\u79cd\u6709\u6548\u4ee3\u7801\u53d8\u66f4\uff0c\u5f52\u4e3a13\u7c7b\uff0c\u5176\u4e2d41%\u4e3a\u65b0\u53d1\u73b0\u3002", "conclusion": "\u672c\u7814\u7a76\u4e3aPython\u673a\u5668\u5b66\u4e60\u7cfb\u7edf\u7684\u81ea\u52a8\u5316\u8d28\u91cf\u8bc4\u4f30\u3001\u6700\u4f73\u5b9e\u8df5\u5236\u5b9a\u53ca\u5de5\u5177\u5f00\u53d1\u63d0\u4f9b\u4e86\u91cd\u8981\u57fa\u7840\uff0c\u586b\u8865\u4e86\u4ee3\u7801\u53d8\u66f4\u4e0e\u8f6f\u4ef6\u8d28\u91cf\u5173\u7cfb\u7684\u8ba4\u77e5\u7a7a\u767d\u3002"}}
{"id": "2511.02248", "categories": ["cs.DC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.02248", "abs": "https://arxiv.org/abs/2511.02248", "authors": ["Xingqi Cui", "Chieh-Jan Mike Liang", "Jiarong Xing", "Haoran Qiu"], "title": "From Models to Operators: Rethinking Autoscaling Granularity for Large Generative Models", "comment": "16 pages, 13 figures", "summary": "Serving large generative models such as LLMs and multi- modal transformers\nrequires balancing user-facing SLOs (e.g., time-to-first-token,\ntime-between-tokens) with provider goals of efficiency and cost reduction.\nExisting solutions rely on static provisioning or model-level autoscaling, both\nof which treat the model as a monolith. This coarse-grained resource management\nleads to degraded performance or significant resource underutilization due to\npoor adaptability to dynamic inference traffic that is common online.\n  The root cause of this inefficiency lies in the internal structure of\ngenerative models: they are executed as graphs of interconnected operators.\nThrough detailed characterization and systematic analysis, we find that\noperators are heterogeneous in their compute and memory footprints and exhibit\ndiverse sensitivity to workload and resource factors such as batch size,\nsequence length, and traffic rate. This heterogeneity suggests that the\noperator, rather than the entire model, is the right granularity for scaling\ndecisions.\n  We propose an operator-level autoscaling framework, which allocates resources\nat finer (operator)-granularity, optimizing the scaling, batching, and\nplacement based on individual operator profiles. Evaluated on production-scale\ntraces, our approach preserves SLOs with up to 40% fewer GPUs and 35% less\nenergy, or under fixed resources achieves 1.6x higher throughput with 5% less\nenergy. These results show that the operator, rather than the model, is\nfundamentally a more effective unit for scaling large generative workloads.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9762\u5411\u7b97\u5b50\u7ea7\u522b\u7684\u81ea\u52a8\u6269\u7f29\u6846\u67b6\uff0c\u901a\u8fc7\u7ec6\u7c92\u5ea6\u8d44\u6e90\u5206\u914d\uff0c\u5728\u6ee1\u8db3\u7528\u6237\u670d\u52a1\u7b49\u7ea7\u76ee\u6807\uff08SLO\uff09\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u5927\u751f\u6210\u6a21\u578b\u63a8\u7406\u7684\u8d44\u6e90\u6548\u7387\u548c\u80fd\u6548\u3002", "motivation": "\u73b0\u6709\u5927\u6a21\u578b\u670d\u52a1\u65b9\u6848\u91c7\u7528\u9759\u6001\u8d44\u6e90\u914d\u7f6e\u6216\u6a21\u578b\u7ea7\u81ea\u52a8\u6269\u7f29\uff0c\u5c06\u6a21\u578b\u89c6\u4e3a\u6574\u4f53\uff0c\u65e0\u6cd5\u9002\u5e94\u5728\u7ebf\u63a8\u7406\u4e2d\u52a8\u6001\u53d8\u5316\u7684\u6d41\u91cf\uff0c\u5bfc\u81f4\u6027\u80fd\u4e0b\u964d\u6216\u8d44\u6e90\u5229\u7528\u7387\u4f4e\u4e0b\u3002", "method": "\u4f5c\u8005\u901a\u8fc7\u5bf9\u751f\u6210\u6a21\u578b\u5185\u90e8\u7b97\u5b50\u7684\u5f02\u6784\u6027\u8fdb\u884c\u7cfb\u7edf\u5206\u6790\uff0c\u63d0\u51fa\u4ee5\u7b97\u5b50\u4e3a\u5355\u4f4d\u8fdb\u884c\u81ea\u52a8\u6269\u7f29\uff0c\u6839\u636e\u5404\u7b97\u5b50\u7684\u8ba1\u7b97\u3001\u5185\u5b58\u7279\u5f81\u53ca\u5bf9\u6279\u5927\u5c0f\u3001\u5e8f\u5217\u957f\u5ea6\u548c\u6d41\u91cf\u7684\u654f\u611f\u6027\uff0c\u4f18\u5316\u5176\u6269\u7f29\u3001\u6279\u5904\u7406\u548c\u90e8\u7f72\u7b56\u7565\u3002", "result": "\u5728\u751f\u4ea7\u7ea7\u8d1f\u8f7d\u4e0b\u8bc4\u4f30\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u6ee1\u8db3SLO\u7684\u524d\u63d0\u4e0b\u53ef\u51cf\u5c11\u6700\u591a40%\u7684GPU\u4f7f\u7528\u91cf\u548c35%\u7684\u80fd\u8017\uff1b\u5728\u56fa\u5b9a\u8d44\u6e90\u4e0b\u53ef\u5b9e\u73b01.6\u500d\u541e\u5410\u91cf\u5e76\u964d\u4f4e5%\u80fd\u8017\u3002", "conclusion": "\u7b97\u5b50\u800c\u975e\u6574\u4e2a\u6a21\u578b\uff0c\u662f\u6269\u7f29\u5927\u89c4\u6a21\u751f\u6210\u6a21\u578b\u5de5\u4f5c\u8d1f\u8f7d\u66f4\u6709\u6548\u7684\u57fa\u672c\u5355\u5143\u3002"}}
{"id": "2511.02257", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2511.02257", "abs": "https://arxiv.org/abs/2511.02257", "authors": ["Oguz Selvitopi", "Emin Ozturk", "Jie Chen", "Ponnuswamy Sadayappan", "Robert G. Edwards", "Ayd\u0131n Bulu\u00e7"], "title": "Fast Algorithms for Scheduling Many-body Correlation Functions on Accelerators", "comment": null, "summary": "Computation of correlation functions is a key operation in Lattice quantum\nchromodynamics (LQCD) simulations to extract nuclear physics observables. These\nfunctions involve many binary batch tensor contractions, each tensor possibly\noccupying hundreds of MBs of memory. Performing these contractions on GPU\naccelerators poses the challenge of scheduling them as to optimize tensor reuse\nand reduce data traffic. In this work we propose two fast novel scheduling\nalgorithms that reorder contractions to increase temporal locality via\ninput/intermediate tensor reuse. Our schedulers take advantage of\napplication-specific features, such as contractions being binary and locality\nwithin contraction trees, to optimize the objective of minimizing peak memory.\nWe integrate them into the LQCD analysis software suite Redstar and improve\ntime-to-solution. Our schedulers attain upto 2.1x improvement in peak memory,\nwhich is reflected by a reduction of upto 4.2x in evictions, upto 1.8x in data\ntraffic, resulting in upto 1.9x faster correlation function computation time.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e24\u79cd\u65b0\u9896\u7684\u8c03\u5ea6\u7b97\u6cd5\uff0c\u901a\u8fc7\u4f18\u5316\u5f20\u91cf\u91cd\u7528\u4ee5\u63d0\u5347\u683c\u70b9\u91cf\u5b50\u8272\u52a8\u529b\u5b66\uff08LQCD\uff09\u4e2d\u5173\u8054\u51fd\u6570\u8ba1\u7b97\u7684\u5185\u5b58\u6548\u7387\u548c\u8ba1\u7b97\u901f\u5ea6\u3002", "motivation": "\u5728LQCD\u6a21\u62df\u4e2d\uff0c\u5173\u8054\u51fd\u6570\u7684\u8ba1\u7b97\u6d89\u53ca\u5927\u91cf\u5927\u89c4\u6a21\u5f20\u91cf\u6536\u7f29\u64cd\u4f5c\uff0c\u5728GPU\u4e0a\u6267\u884c\u65f6\u9762\u4e34\u5982\u4f55\u4f18\u5316\u5f20\u91cf\u91cd\u7528\u3001\u51cf\u5c11\u6570\u636e\u4f20\u8f93\u5e76\u964d\u4f4e\u5cf0\u503c\u5185\u5b58\u4f7f\u7528\u7684\u6311\u6218\u3002", "method": "\u8bbe\u8ba1\u4e24\u79cd\u5feb\u901f\u8c03\u5ea6\u7b97\u6cd5\uff0c\u5229\u7528LQCD\u5e94\u7528\u7279\u6709\u7684\u4e8c\u5143\u6536\u7f29\u7ed3\u6784\u548c\u6536\u7f29\u6811\u4e2d\u7684\u5c40\u90e8\u6027\u7279\u5f81\uff0c\u5bf9\u6536\u7f29\u64cd\u4f5c\u8fdb\u884c\u91cd\u6392\u5e8f\uff0c\u4ee5\u589e\u5f3a\u65f6\u95f4\u5c40\u90e8\u6027\u5e76\u6700\u5c0f\u5316\u5cf0\u503c\u5185\u5b58\u5360\u7528\uff1b\u5e76\u5c06\u7b97\u6cd5\u96c6\u6210\u5230Redstar\u8f6f\u4ef6\u5957\u4ef6\u4e2d\u3002", "result": "\u8c03\u5ea6\u5668\u6700\u591a\u5b9e\u73b02.1\u500d\u7684\u5cf0\u503c\u5185\u5b58\u964d\u4f4e\uff0c\u5bf9\u5e94\u7f13\u5b58\u9a71\u9010\u51cf\u5c11\u8fbe4.2\u500d\u3001\u6570\u636e\u6d41\u91cf\u51cf\u5c11\u8fbe1.8\u500d\uff0c\u5e76\u4f7f\u5173\u8054\u51fd\u6570\u8ba1\u7b97\u901f\u5ea6\u6700\u9ad8\u63d0\u53471.9\u500d\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u8c03\u5ea6\u7b97\u6cd5\u663e\u8457\u63d0\u5347\u4e86LQCD\u5173\u8054\u51fd\u6570\u8ba1\u7b97\u7684\u6027\u80fd\u548c\u5185\u5b58\u6548\u7387\uff0c\u9a8c\u8bc1\u4e86\u9488\u5bf9\u9886\u57df\u7279\u5b9a\u7279\u5f81\u8fdb\u884c\u8c03\u5ea6\u4f18\u5316\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2511.02647", "categories": ["cs.DC", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.02647", "abs": "https://arxiv.org/abs/2511.02647", "authors": ["Xiumei Deng", "Zehui Xiong", "Binbin Chen", "Dong In Kim", "Merouane Debbah", "H. Vincent Poor"], "title": "Federated Attention: A Distributed Paradigm for Collaborative LLM Inference over Edge Networks", "comment": null, "summary": "Large language models (LLMs) are proliferating rapidly at the edge,\ndelivering intelligent capabilities across diverse application scenarios.\nHowever, their practical deployment in collaborative scenarios confronts\nfundamental challenges: privacy vulnerabilities, communication overhead, and\ncomputational bottlenecks. To address these, we propose Federated Attention\n(FedAttn), which integrates the federated paradigm into the self-attention\nmechanism, creating a new distributed LLM inference framework that\nsimultaneously achieves privacy protection, communication efficiency, and\ncomputational efficiency. FedAttn enables participants to perform local\nself-attention over their own token representations while periodically\nexchanging and aggregating Key-Value (KV) matrices across multiple Transformer\nblocks, collaboratively generating LLM responses without exposing private\nprompts. Further, we identify a structural duality between contextual\nrepresentation refinement in FedAttn and parameter optimization in FL across\nprivate data, local computation, and global aggregation. This key insight\nprovides a principled foundation for systematically porting federated\noptimization techniques to collaborative LLM inference. Building on this\nframework, we theoretically analyze how local self-attention computation within\nparticipants and heterogeneous token relevance among participants shape error\npropagation dynamics across Transformer blocks. Moreover, we characterize the\nfundamental trade-off between response quality and communication/computation\nefficiency, which is governed by the synchronization interval and the number of\nparticipants. Experimental results validate our theoretical analysis, and\nreveal significant optimization opportunities through sparse attention and\nadaptive KV aggregation, highlighting FedAttn's potential to deliver\nscalability and efficiency in real-world edge deployments.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u8054\u90a6\u6ce8\u610f\u529b\uff08FedAttn\uff09\u6846\u67b6\uff0c\u5c06\u8054\u90a6\u5b66\u4e60\u601d\u60f3\u878d\u5165\u5927\u8bed\u8a00\u6a21\u578b\u7684\u81ea\u6ce8\u610f\u529b\u673a\u5236\u4e2d\uff0c\u5728\u4fdd\u62a4\u7528\u6237\u9690\u79c1\u7684\u540c\u65f6\u63d0\u5347\u901a\u4fe1\u4e0e\u8ba1\u7b97\u6548\u7387\uff0c\u652f\u6301\u8fb9\u7f18\u8bbe\u5907\u4e0a\u7684\u534f\u4f5c\u5f0f\u5927\u6a21\u578b\u63a8\u7406\u3002", "motivation": "\u5728\u8fb9\u7f18\u8bbe\u5907\u4e0a\u90e8\u7f72\u5927\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u534f\u4f5c\u63a8\u7406\u65f6\uff0c\u9762\u4e34\u9690\u79c1\u6cc4\u9732\u3001\u901a\u4fe1\u5f00\u9500\u5927\u548c\u8ba1\u7b97\u74f6\u9888\u4e09\u5927\u6311\u6218\uff0c\u4e9f\u9700\u4e00\u79cd\u517c\u987e\u9690\u79c1\u3001\u6548\u7387\u4e0e\u6027\u80fd\u7684\u65b0\u65b9\u6cd5\u3002", "method": "\u63d0\u51faFedAttn\u6846\u67b6\uff0c\u5141\u8bb8\u53c2\u4e0e\u8005\u5728\u672c\u5730\u6267\u884c\u81ea\u6ce8\u610f\u529b\u8ba1\u7b97\uff0c\u5e76\u5468\u671f\u6027\u5730\u8de8\u591a\u4e2aTransformer\u5c42\u4ea4\u6362\u548c\u805a\u5408Key-Value\u77e9\u9635\uff1b\u540c\u65f6\u63ed\u793a\u5176\u4e0e\u8054\u90a6\u5b66\u4e60\u5728\u7ed3\u6784\u4e0a\u7684\u5bf9\u5076\u6027\uff0c\u4ece\u800c\u5f15\u5165\u8054\u90a6\u4f18\u5316\u6280\u672f\uff0c\u5e76\u5206\u6790\u8bef\u5dee\u4f20\u64ad\u4e0e\u6548\u7387-\u8d28\u91cf\u6743\u8861\u3002", "result": "\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u7406\u8bba\u5206\u6790\u7684\u6b63\u786e\u6027\uff0c\u8868\u660e\u901a\u8fc7\u7a00\u758f\u6ce8\u610f\u529b\u548c\u81ea\u9002\u5e94KV\u805a\u5408\u53ef\u663e\u8457\u4f18\u5316\u6027\u80fd\uff0cFedAttn\u5728\u4fdd\u8bc1\u54cd\u5e94\u8d28\u91cf\u7684\u540c\u65f6\u6709\u6548\u63d0\u5347\u4e86\u901a\u4fe1\u4e0e\u8ba1\u7b97\u6548\u7387\u3002", "conclusion": "FedAttn\u4e3a\u8fb9\u7f18\u73af\u5883\u4e0b\u534f\u4f5c\u5f0f\u5927\u8bed\u8a00\u6a21\u578b\u63a8\u7406\u63d0\u4f9b\u4e86\u4e00\u4e2a\u517c\u987e\u9690\u79c1\u4fdd\u62a4\u3001\u901a\u4fe1\u6548\u7387\u548c\u8ba1\u7b97\u6548\u7387\u7684\u53ef\u884c\u6846\u67b6\uff0c\u5177\u6709\u826f\u597d\u7684\u53ef\u6269\u5c55\u6027\u548c\u5b9e\u9645\u90e8\u7f72\u6f5c\u529b\u3002"}}
{"id": "2511.02655", "categories": ["cs.DC", "cs.MS"], "pdf": "https://arxiv.org/pdf/2511.02655", "abs": "https://arxiv.org/abs/2511.02655", "authors": ["Johansell Villalobos", "Josef Ruzicka", "Silvio Rizzi"], "title": "Implementing Multi-GPU Scientific Computing Miniapps Across Performance Portable Frameworks", "comment": null, "summary": "Scientific computing in the exascale era demands increased computational\npower to solve complex problems across various domains. With the rise of\nheterogeneous computing architectures the need for vendor-agnostic, performance\nportability frameworks has been highlighted. Libraries like Kokkos have become\nessential for enabling high-performance computing applications to execute\nefficiently across different hardware platforms with minimal code changes. In\nthis direction, this paper presents preliminary time-to-solution results for\ntwo representative scientific computing applications: an N-body simulation and\na structured grid simulation. Both applications used a distributed memory\napproach and hardware acceleration through four performance portability\nframeworks: Kokkos, OpenMP, RAJA, and OCCA. Experiments conducted on a single\nnode of the Polaris supercomputer using four NVIDIA A100 GPUs revealed\nsignificant performance variability among frameworks. OCCA demonstrated faster\nexecution times for small-scale validation problems, likely due to JIT\ncompilation, however its lack of optimized reduction algorithms may limit\nscalability for larger simulations while using its out of the box API. OpenMP\nperformed poorly in the structured grid simulation most likely due to\ninefficiencies in inter-node data synchronization and communication. These\nfindings highlight the need for further optimization to maximize each\nframework's capabilities. Future work will focus on enhancing reduction\nalgorithms, data communication, memory management, as wells as performing\nscalability studies, and a comprehensive statistical analysis to evaluate and\ncompare framework performance.", "AI": {"tldr": "\u672c\u6587\u5728Polaris\u8d85\u7b97\u5355\u8282\u70b9\u4e0a\u4f7f\u7528\u56db\u79cd\u6027\u80fd\u53ef\u79fb\u690d\u6846\u67b6\uff08Kokkos\u3001OpenMP\u3001RAJA\u548cOCCA\uff09\u5bf9N\u4f53\u6a21\u62df\u548c\u7ed3\u6784\u7f51\u683c\u6a21\u62df\u8fdb\u884c\u521d\u6b65\u6027\u80fd\u8bc4\u4f30\uff0c\u53d1\u73b0\u5404\u6846\u67b6\u5728\u4e0d\u540c\u573a\u666f\u4e0b\u8868\u73b0\u5dee\u5f02\u663e\u8457\uff0c\u6307\u51fa\u9700\u8fdb\u4e00\u6b65\u4f18\u5316\u4ee5\u63d0\u5347\u53ef\u6269\u5c55\u6027\u548c\u6548\u7387\u3002", "motivation": "\u968f\u7740\u5f02\u6784\u8ba1\u7b97\u67b6\u6784\u7684\u5174\u8d77\uff0c\u79d1\u5b66\u8ba1\u7b97\u9700\u8981\u80fd\u5728\u4e0d\u540c\u786c\u4ef6\u5e73\u53f0\u4e0a\u9ad8\u6548\u8fd0\u884c\u4e14\u4ee3\u7801\u6539\u52a8\u6700\u5c0f\u7684\u6027\u80fd\u53ef\u79fb\u690d\u6846\u67b6\uff0c\u56e0\u6b64\u6709\u5fc5\u8981\u8bc4\u4f30\u4e3b\u6d41\u6846\u67b6\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u8868\u73b0\u3002", "method": "\u5728\u914d\u5907\u56db\u5757NVIDIA A100 GPU\u7684Polaris\u8d85\u7b97\u5355\u8282\u70b9\u4e0a\uff0c\u91c7\u7528\u5206\u5e03\u5f0f\u5185\u5b58\u65b9\u6cd5\uff0c\u5229\u7528Kokkos\u3001OpenMP\u3001RAJA\u548cOCCA\u56db\u79cd\u6846\u67b6\u5206\u522b\u5b9e\u73b0N\u4f53\u6a21\u62df\u548c\u7ed3\u6784\u7f51\u683c\u6a21\u62df\uff0c\u5e76\u6bd4\u8f83\u5176\u6c42\u89e3\u65f6\u95f4\u3002", "result": "OCCA\u5728\u5c0f\u89c4\u6a21\u9a8c\u8bc1\u95ee\u9898\u4e2d\u56e0JIT\u7f16\u8bd1\u8868\u73b0\u51fa\u66f4\u5feb\u6267\u884c\u65f6\u95f4\uff0c\u4f46\u5176\u9ed8\u8ba4API\u7f3a\u4e4f\u4f18\u5316\u7684\u5f52\u7ea6\u7b97\u6cd5\u53ef\u80fd\u9650\u5236\u5927\u89c4\u6a21\u6a21\u62df\u7684\u53ef\u6269\u5c55\u6027\uff1bOpenMP\u5728\u7ed3\u6784\u7f51\u683c\u6a21\u62df\u4e2d\u8868\u73b0\u8f83\u5dee\uff0c\u53ef\u80fd\u6e90\u4e8e\u8282\u70b9\u95f4\u6570\u636e\u540c\u6b65\u4e0e\u901a\u4fe1\u6548\u7387\u4f4e\u4e0b\u3002", "conclusion": "\u5404\u6027\u80fd\u53ef\u79fb\u690d\u6846\u67b6\u5728\u4e0d\u540c\u5e94\u7528\u4e2d\u8868\u73b0\u5404\u5f02\uff0c\u9700\u9488\u5bf9\u5f52\u7ea6\u7b97\u6cd5\u3001\u6570\u636e\u901a\u4fe1\u548c\u5185\u5b58\u7ba1\u7406\u7b49\u65b9\u9762\u8fdb\u4e00\u6b65\u4f18\u5316\uff0c\u5e76\u5f00\u5c55\u53ef\u6269\u5c55\u6027\u7814\u7a76\u4e0e\u5168\u9762\u7edf\u8ba1\u5206\u6790\u4ee5\u5145\u5206\u6316\u6398\u5176\u6f5c\u529b\u3002"}}
{"id": "2511.02743", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2511.02743", "abs": "https://arxiv.org/abs/2511.02743", "authors": ["Fedor Ryabinin", "Alexey Gotsman", "Pierre Sutra"], "title": "Making Democracy Work: Fixing and Simplifying Egalitarian Paxos (Extended Version)", "comment": "Extended version of a paper in OPODIS'25: International Conference on\n  Principles of Distributed Systems", "summary": "Classical state-machine replication protocols, such as Paxos, rely on a\ndistinguished leader process to order commands. Unfortunately, this approach\nmakes the leader a single point of failure and increases the latency for\nclients that are not co-located with it. As a response to these drawbacks,\nEgalitarian Paxos introduced an alternative, leaderless approach, that allows\nreplicas to order commands collaboratively. Not relying on a single leader\nallows the protocol to maintain non-zero throughput with up to $f$ crashes of\nany processes out of a total of $n = 2f+1$. The protocol furthermore allows any\nprocess to execute a command $c$ fast, in $2$ message delays, provided no more\nthan $e = \\lceil\\frac{f+1}{2}\\rceil$ other processes fail, and all concurrently\nsubmitted commands commute with $c$; the latter condition is often satisfied in\npractical systems.\n  Egalitarian Paxos has served as a foundation for many other replication\nprotocols. But unfortunately, the protocol is very complex, ambiguously\nspecified and suffers from nontrivial bugs. In this paper, we present EPaxos*\n-- a simpler and correct variant of Egalitarian Paxos. Our key technical\ncontribution is a simpler failure-recovery algorithm, which we have rigorously\nproved correct. Our protocol also generalizes Egalitarian Paxos to cover the\nwhole spectrum of failure thresholds $f$ and $e$ such that $n \\ge \\max\\{2e+f-1,\n2f+1\\}$ -- the number of processes that we show to be optimal.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86EPaxos*\uff0c\u4e00\u79cd\u66f4\u7b80\u5355\u4e14\u6b63\u786e\u7684Egalitarian Paxos\u53d8\u4f53\uff0c\u901a\u8fc7\u7b80\u5316\u6545\u969c\u6062\u590d\u7b97\u6cd5\u5e76\u6269\u5c55\u5176\u9002\u7528\u7684\u5bb9\u9519\u9608\u503c\u8303\u56f4\uff0c\u89e3\u51b3\u4e86\u539f\u534f\u8bae\u590d\u6742\u3001\u6a21\u7cca\u548c\u5b58\u5728\u7f3a\u9677\u7684\u95ee\u9898\u3002", "motivation": "Egalitarian Paxos\u867d\u907f\u514d\u4e86\u4f20\u7edfPaxos\u4e2d\u5355\u70b9\u9886\u5bfc\u8005\u5e26\u6765\u7684\u6027\u80fd\u74f6\u9888\u548c\u5355\u70b9\u6545\u969c\u95ee\u9898\uff0c\u4f46\u5176\u534f\u8bae\u672c\u8eab\u8fc7\u4e8e\u590d\u6742\u3001\u89c4\u8303\u6a21\u7cca\u4e14\u5b58\u5728\u4e25\u91cd\u9519\u8bef\uff0c\u4e9f\u9700\u4e00\u4e2a\u66f4\u6e05\u6670\u3001\u6b63\u786e\u4e14\u6613\u4e8e\u5b9e\u73b0\u7684\u66ff\u4ee3\u65b9\u6848\u3002", "method": "\u4f5c\u8005\u8bbe\u8ba1\u4e86EPaxos*\uff0c\u6838\u5fc3\u8d21\u732e\u662f\u4e00\u4e2a\u66f4\u7b80\u5355\u7684\u6545\u969c\u6062\u590d\u7b97\u6cd5\uff0c\u5e76\u5bf9\u8be5\u7b97\u6cd5\u8fdb\u884c\u4e86\u4e25\u683c\u7684\u5f62\u5f0f\u5316\u6b63\u786e\u6027\u8bc1\u660e\uff1b\u540c\u65f6\u5c06\u534f\u8bae\u63a8\u5e7f\u5230\u66f4\u4e00\u822c\u7684\u5bb9\u9519\u53c2\u6570\u8303\u56f4\uff08\u6ee1\u8db3 n \u2265 max{2e+f\u22121, 2f+1}\uff09\u3002", "result": "EPaxos*\u5728\u4fdd\u6301Egalitarian Paxos\u65e0\u9886\u5bfc\u8005\u3001\u4f4e\u5ef6\u8fdf\u548c\u9ad8\u541e\u5410\u4f18\u52bf\u7684\u540c\u65f6\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u534f\u8bae\u590d\u6742\u6027\uff0c\u6d88\u9664\u4e86\u539f\u6709\u7f3a\u9677\uff0c\u5e76\u8bc1\u660e\u4e86\u6240\u7528\u8fdb\u7a0b\u6570\u5728\u7ed9\u5b9a\u5bb9\u9519\u8981\u6c42\u4e0b\u662f\u6700\u4f18\u7684\u3002", "conclusion": "EPaxos*\u662f\u5bf9Egalitarian Paxos\u7684\u6709\u6548\u6539\u8fdb\uff0c\u63d0\u4f9b\u4e86\u4e00\u4e2a\u66f4\u7b80\u6d01\u3001\u6b63\u786e\u4e14\u901a\u7528\u7684\u65e0\u9886\u5bfc\u8005\u72b6\u6001\u673a\u590d\u5236\u534f\u8bae\uff0c\u4e3a\u540e\u7eed\u76f8\u5173\u7814\u7a76\u548c\u5b9e\u8df5\u63d0\u4f9b\u4e86\u53ef\u9760\u57fa\u7840\u3002"}}
