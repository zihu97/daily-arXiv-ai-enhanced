<div id=toc></div>

# Table of Contents

- [cs.MA](#cs.MA) [Total: 3]
- [cs.AR](#cs.AR) [Total: 3]
- [cs.DC](#cs.DC) [Total: 6]
- [cs.SE](#cs.SE) [Total: 15]


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [1] [OptiMA: A Transaction-Based Framework with Throughput Optimization for Very Complex Multi-Agent Systems](https://arxiv.org/abs/2511.03761)
*Umut Çalıkyılmaz,Nitin Nayak,Jinghua Groppe,Sven Groppe*

Main category: cs.MA

TL;DR: 本文针对复杂多智能体系统（VCMAS）在规模扩大时可能出现的容错性差和性能瓶颈问题，提出了一种基于事务的框架OptiMA，并引入事务调度机制，实验表明该框架可支持上百个智能体运行，且调度策略带来超过16%的性能提升。


<details>
  <summary>Details</summary>
Motivation: 随着多智能体系统模型日益复杂，系统面临两大挑战：对故障的敏感性增加以及性能瓶颈。为应对这些问题，作者提出新的设计框架以增强系统的鲁棒性和效率。

Method: 作者构建了一个基于事务的多智能体系统设计框架（OptiMA），并在此基础上集成了事务调度机制，以提升系统整体性能与可靠性。

Result: OptiMA框架成功支持了包含上百个智能体的复杂系统运行，事务调度带来了最高超过16%的性能提升；同时作者还对调度问题进行了理论分析，并提供了实用工具供后续研究使用。

Conclusion: 基于事务的设计方法结合事务调度能有效应对大规模多智能体系统的容错性与性能挑战，为未来复杂多智能体系统的研究与实现提供了可行路径和工具支持。

Abstract: In recent years, the research of multi-agent systems has taken a direction to
explore larger and more complex models to fulfill sophisticated tasks. We point
out two possible pitfalls that might be caused by increasing complexity;
susceptibilities to faults, and performance bottlenecks. To prevent the former
threat, we propose a transaction-based framework to design very complex
multi-agent systems (VCMAS). To address the second threat, we offer to
integrate transaction scheduling into the proposed framework. We implemented
both of these ideas to develop the OptiMA framework and show that it is able to
facilitate the execution of VCMAS with more than a hundred agents. We also
demonstrate the effect of transaction scheduling on such a system by showing
improvements up to more than 16\%. Furthermore, we also performed a theoretical
analysis on the transaction scheduling problem and provided practical tools
that can be used for future research on it.

</details>


### [2] [ASAP: an Agentic Solution to Auto-optimize Performance of Large-Scale LLM Training](https://arxiv.org/abs/2511.03844)
*Yuran Ding,Xinwei Chen,Xiaofan Zhang,Zongwei Zhou*

Main category: cs.MA

TL;DR: 本文提出了ASAP，一种用于自动优化大规模大语言模型（LLM）训练性能的多智能体系统，通过结合LLM推理、性能分析工具和专家知识，实现对分布式训练中分片配置的自动诊断与优化，显著提升训练效率。


<details>
  <summary>Details</summary>
Motivation: 现有LLM训练优化方法依赖耗时的手动调优或资源密集型的黑盒搜索，难以适应快速发展的LLM领域，导致开发缓慢和资源利用率低。

Method: 提出ASAP多智能体系统，包含Coordinator、Analyzer和Proposal三个智能体，融合LLM推理能力、性能剖析工具、roofline分析以及专家最佳实践知识库，自动诊断性能瓶颈并推荐带解释的优化分片策略。

Result: 实验表明，ASAP生成的分片配置可减少最多28%的训练步时，并提升1.43倍吞吐量；若结合人工进一步优化，吞吐量可达2.58倍。

Conclusion: ASAP为大规模LLM训练提供了一种可扩展且可解释的AI辅助性能工程方法，有效提升分布式训练效率。

Abstract: Optimizing large-language model (LLM) training on distributed domain-specific
accelerator systems presents significant challenges due to its complex
optimization space. Existing optimization methods, however, rely on
time-consuming manual tuning or resource-intensive black-box searches, which
struggle to keep pace with the rapidly evolving LLM domain, leading to slow
development and underutilized resources. To address this, we introduce ASAP, an
Agentic Solution to Auto-optimize Performance of Large-Scale LLM Training. It
is a multi-agent system, featuring Coordinator, Analyzer, and Proposal agents,
which integrates LLM reasoning with insights from performance profiling tools,
roofline analysis, and a knowledge base of best practices and successful past
optimizations from human experts. Our proposed design can automate the
diagnosis of performance bottlenecks and recommend optimized sharding
configurations with reasoning, thus effectively improving the efficiency of
distributed LLM training. Experiments have shown that the ASAP-generated
sharding configurations can contribute up to 28% training step time reduction
and 1.43 times throughput improvement. When combined with additional
optimization from human experts, throughput can be further increased to 2.58
times. The proposed ASAP promises to provide a scalable and explainable
methodology for AI-assisted performance engineering in large-scale LLM
training.

</details>


### [3] [Multi-Agent Collaborative Framework For Math Problem Generation](https://arxiv.org/abs/2511.03958)
*Kia Karbasi,Kevin Hong,Mohammad Amin Samadi,Gregory Pottie*

Main category: cs.MA

TL;DR: 本文提出一种基于多智能体协作的自动数学问题生成框架，通过推理时计算动态调整问题的认知难度与清晰度，在多项元评估指标上展现出更优的教育内容生成质量。


<details>
  <summary>Details</summary>
Motivation: 现有预训练语言模型在自动生成数学教育问题时难以精确控制问题复杂度和认知需求，限制了其在智能辅导系统中的应用。

Method: 引入一个协作式多智能体框架，在推理阶段通过多个智能体迭代优化生成的问题-答案对，以平衡问题的复杂性与认知要求。

Result: 在相关性、重要性、清晰度、难度匹配和可答性五个元评估标准上，该方法生成的问题质量显著提升，实现了认知挑战与清晰度之间的更好平衡。

Conclusion: 协作式多智能体工作流能生成更具教学价值、可控性更强的教育内容，有助于推动自动化教育内容生成和自适应学习环境的发展。

Abstract: Automatic question generation (AQG) for mathematics education remains an
elusive goal for Intelligent Tutoring Systems and educators. While pre-trained
transformer-based language models have significantly advanced natural language
generation, they often struggle to precisely control problem complexity and
cognitive demands. In this paper, we introduce a collaborative multi-agent
framework as a novel method of incorporating inference-time computation into
AQG. This approach leverages multiple agents that iteratively refine generated
question-answer pairs to better balance complexity and cognitive demand. We
evaluate the generated questions on five meta-evaluation criteria: relevance,
importance, clarity, difficulty matching, answerability, to assess the system's
ability to control the required complexity and quality of the questions.
Preliminary evaluations show that this collaborative multi-agent framework
elevates the quality of generated educational content by fostering a more
nuanced balance between cognitive challenge and clarity. These promising
outcomes suggest that integrating collaborative multi-agent workflows can yield
more controlled, pedagogically valuable content that can help advance automated
educational content generation and adaptive learning environments.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [4] [From Minutes to Seconds: Redefining the Five-Minute Rule for AI-Era Memory Hierarchies](https://arxiv.org/abs/2511.03944)
*Tong Zhang,Vikram Sharma Mailthody,Fei Sun,Linsen Ma,Chris J. Newburn,Teresa Zhang,Yang Liu,Jiangpeng Li,Hao Zhong,Wen-Mei Hwu*

Main category: cs.AR

TL;DR: 本文重新审视了经典的五分钟规则，结合现代AI平台的硬件特性（如GPU主机和高性能SSD），提出DRAM与闪存之间的缓存阈值已从分钟级缩短至秒级，并构建了一个考虑成本、性能约束和工作负载的可行性感知分析框架。


<details>
  <summary>Details</summary>
Motivation: 原始的五分钟规则仅基于存储与内存的经济性，忽略了主机成本、DRAM带宽/容量限制、SSD物理性能模型以及实际工作负载特征，难以适用于现代AI系统。

Method: 从第一性原理出发，整合主机成本、DRAM带宽与容量、基于物理的SSD性能与成本模型，并将其嵌入一个考虑约束条件和工作负载的分析框架；同时开发了校准后的SSD模拟器MQSim-Next以支持验证与敏感性分析。

Result: 发现对于现代AI平台，尤其是配备超高IOPS SSD的GPU中心化主机，DRAM到闪存的缓存阈值缩短至几秒；并展示了两个软件系统设计案例，说明该范式转变带来的新设计空间。

Conclusion: 将经典的五分钟规则转化为一个可行、可操作的分析与配置框架，为AI时代内存层次结构的研究奠定基础。

Abstract: In 1987, Jim Gray and Gianfranco Putzolu introduced the five-minute rule, a
simple, storage-memory-economics-based heuristic for deciding when data should
live in DRAM rather than on storage. Subsequent revisits to the rule largely
retained that economics-only view, leaving host costs, feasibility limits, and
workload behavior out of scope. This paper revisits the rule from first
principles, integrating host costs, DRAM bandwidth/capacity, and
physics-grounded models of SSD performance and cost, and then embedding these
elements in a constraint- and workload-aware framework that yields actionable
provisioning guidance. We show that, for modern AI platforms, especially
GPU-centric hosts paired with ultra-high-IOPS SSDs engineered for fine-grained
random access, the DRAM-to-flash caching threshold collapses from minutes to a
few seconds. This shift reframes NAND flash memory as an active data tier and
exposes a broad research space across the hardware-software stack. We further
introduce MQSim-Next, a calibrated SSD simulator that supports validation and
sensitivity analysis and facilitates future architectural and system research.
Finally, we present two concrete case studies that showcase the software system
design space opened by such memory hierarchy paradigm shift. Overall, we turn a
classical heuristic into an actionable, feasibility-aware analysis and
provisioning framework and set the stage for further research on AI-era memory
hierarchy.

</details>


### [5] [Disaggregated Architectures and the Redesign of Data Center Ecosystems: Scheduling, Pooling, and Infrastructure Trade-offs](https://arxiv.org/abs/2511.04104)
*Chao Guo,Jiahe Xu,Moshe Zukerman*

Main category: cs.AR

TL;DR: 本文综述了数据中心硬件解耦的动机与最新进展，探讨其对整个数据中心生态系统（包括应用设计、资源调度、硬件配置、冷却和供电优化）的潜在影响，并通过数值研究揭示相关挑战。


<details>
  <summary>Details</summary>
Motivation: 传统数据中心服务器架构存在资源利用率低、扩展性差等问题，硬件解耦旨在将计算、存储、内存等资源池化，提升灵活性与效率，从而重塑数据中心生态。

Method: 文章通过综述现有研究与工业实践，分析硬件解耦架构的关键技术与挑战，并辅以数值研究来量化和说明若干核心问题。

Result: 研究表明硬件解耦在资源利用、系统可扩展性和能效方面具有显著潜力，但其实际部署仍面临互连延迟、管理复杂性等挑战。

Conclusion: 硬件解耦有望深刻改变数据中心的设计与运营方式，尽管尚存技术障碍，但其带来的系统级优化机会值得持续深入研究。

Abstract: Hardware disaggregation seeks to transform Data Center (DC) resources from
traditional server fleets into unified resource pools. Despite existing
challenges that may hinder its full realization, significant progress has been
made in both industry and academia. In this article, we provide an overview of
the motivations and recent advancements in hardware disaggregation. We further
discuss the research challenges and opportunities associated with disaggregated
architectures, focusing on aspects that have received limited attention. We
argue that hardware disaggregation has the potential to reshape the entire DC
ecosystem, impacting application design, resource scheduling, hardware
configuration, cooling, and power system optimization. Additionally, we present
a numerical study to illustrate several key aspects of these challenges.

</details>


### [6] [AIM: Software and Hardware Co-design for Architecture-level IR-drop Mitigation in High-performance PIM](https://arxiv.org/abs/2511.04321)
*Yuanpeng Zhang,Xing Hu,Xi Chen,Zhihang Yuan,Cong Li,Jingchen Zhu,Zhao Wang,Chenguang Zhang,Xin Si,Wei Gao,Qiang Wu,Runsheng Wang,Guangyu Sun*

Main category: cs.AR

TL;DR: 本文提出AIM，一种软硬件协同设计的架构级IR-drop缓解方案，用于高性能SRAM存内计算（PIM）芯片。通过建立工作负载与IR-drop之间的关联模型，并结合软件优化与硬件动态调节机制，在保持计算精度的同时显著降低IR-drop，提升能效与性能。


<details>
  <summary>Details</summary>
Motivation: 高性能SRAM PIM因复杂电路和高频率运行导致严重的IR-drop问题，传统后端电路级缓解方法代价高昂且牺牲PPA（功耗、性能、面积），亟需一种更高效的架构级解决方案。

Method: 提出AIM框架：首先基于PIM的位串行和原位数据流特性，引入Rtog和HR指标以量化工作负载与IR-drop的关系；然后提出LHR和WDS进行软件层面的架构级IR-drop优化；接着开发IR-Booster机制，融合软件HR信息与硬件IR监测，动态调整PIM宏的电压-频率对；最后设计HR感知的任务映射策略，实现软硬件协同优化。

Result: 在7nm工艺、256-TOPS的PIM芯片上进行后仿真实验，结果显示AIM最多可降低69.2%的IR-drop，带来2.29倍的能效提升和1.152倍的性能加速。

Conclusion: AIM通过软硬件协同的架构级设计，在不牺牲计算精度的前提下有效缓解了高性能PIM中的IR-drop问题，显著提升了能效与性能，为未来高密度存内计算系统提供了可行的优化路径。

Abstract: SRAM Processing-in-Memory (PIM) has emerged as the most promising
implementation for high-performance PIM, delivering superior computing density,
energy efficiency, and computational precision. However, the pursuit of higher
performance necessitates more complex circuit designs and increased operating
frequencies, which exacerbate IR-drop issues. Severe IR-drop can significantly
degrade chip performance and even threaten reliability. Conventional
circuit-level IR-drop mitigation methods, such as back-end optimizations, are
resource-intensive and often compromise power, performance, and area (PPA). To
address these challenges, we propose AIM, comprehensive software and hardware
co-design for architecture-level IR-drop mitigation in high-performance PIM.
Initially, leveraging the bit-serial and in-situ dataflow processing properties
of PIM, we introduce Rtog and HR, which establish a direct correlation between
PIM workloads and IR-drop. Building on this foundation, we propose LHR and WDS,
enabling extensive exploration of architecture-level IR-drop mitigation while
maintaining computational accuracy through software optimization. Subsequently,
we develop IR-Booster, a dynamic adjustment mechanism that integrates
software-level HR information with hardware-based IR-drop monitoring to adapt
the V-f pairs of the PIM macro, achieving enhanced energy efficiency and
performance. Finally, we propose the HR-aware task mapping method, bridging
software and hardware designs to achieve optimal improvement. Post-layout
simulation results on a 7nm 256-TOPS PIM chip demonstrate that AIM achieves up
to 69.2% IR-drop mitigation, resulting in 2.29x energy efficiency improvement
and 1.152x speedup.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [7] [OMPILOT: Harnessing Transformer Models for Auto Parallelization to Shared Memory Computing Paradigms](https://arxiv.org/abs/2511.03866)
*Arijit Bhattacharjee,Ali TehraniJamsaz,Le Chen,Niranjan Hasabnis,Mihai Capota,Nesreen Ahmed,Ali Jannesari*

Main category: cs.DC

TL;DR: 本文提出了OMPILOT，一种专用于将C++代码翻译为OpenMP并行代码的领域特定编码器-解码器Transformer模型，并引入新评估指标OMPBLEU。


<details>
  <summary>Details</summary>
Motivation: 传统规则系统在代码翻译中准确性和灵活性不足，而现有大语言模型虽有进展，但在函数级并行语义建模和OpenMP特定构造评估方面仍有局限。

Method: OMPILOT采用定制的预训练目标融合并行构造语义，结合无监督与监督学习策略，在函数级别进行代码翻译；同时提出OMPBLEU指标评估OpenMP翻译质量。

Result: OMPILOT能有效实现C++到OpenMP的函数级翻译，提升并行化准确性与鲁棒性，OMPBLEU指标更贴合OpenMP构造的评估需求。

Conclusion: 该工作展示了领域特定大模型在代码并行化翻译中的潜力，并为专用翻译任务提供了新的建模范式与评估方法。

Abstract: Recent advances in large language models (LLMs) have significantly
accelerated progress in code translation, enabling more accurate and efficient
transformation across programming languages. While originally developed for
natural language processing, LLMs have shown strong capabilities in modeling
programming language syntax and semantics, outperforming traditional rule-based
systems in both accuracy and flexibility. These models have streamlined
cross-language conversion, reduced development overhead, and accelerated legacy
code migration. In this paper, we introduce OMPILOT, a novel domain-specific
encoder-decoder transformer tailored for translating C++ code into OpenMP,
enabling effective shared-memory parallelization. OMPILOT leverages custom
pre-training objectives that incorporate the semantics of parallel constructs
and combines both unsupervised and supervised learning strategies to improve
code translation robustness. Unlike previous work that focused primarily on
loop-level transformations, OMPILOT operates at the function level to capture a
wider semantic context. To evaluate our approach, we propose OMPBLEU, a novel
composite metric specifically crafted to assess the correctness and quality of
OpenMP parallel constructs, addressing limitations in conventional translation
metrics.

</details>


### [8] [Stochastic Modeling for Energy-Efficient Edge Infrastructure](https://arxiv.org/abs/2511.03941)
*Fabio Diniz Rossi*

Main category: cs.DC

TL;DR: 本文提出一种基于马尔可夫链的随机建模方法，用于分析边缘计算中的电源状态转换，并通过AI驱动的预测性功耗调节策略提升能效。


<details>
  <summary>Details</summary>
Motivation: 边缘计算虽支持低延迟实时应用，但其设备分布广、能源有限，带来电源管理挑战，亟需更高效的能耗优化方法。

Method: 采用马尔可夫链对边缘设备的电源状态转换进行建模，推导稳态概率并评估能耗；结合蒙特卡洛仿真与敏感性分析验证模型，并比较AI预测性调节能耗与传统反应式方法的效果。

Result: 仿真结果表明理论与实证高度一致；AI预测性电源管理能减少不必要的状态切换，优化异构边缘节点间的工作负载分配，降低设备间能耗差异，提升整体能效与系统响应能力。

Conclusion: AI驱动的预测性电源管理策略能显著提升边缘计算环境下的能源效率和自适应协调能力。

Abstract: Edge Computing enables low-latency processing for real-time applications but
introduces challenges in power management due to the distributed nature of edge
devices and their limited energy resources. This paper proposes a stochastic
modeling approach using Markov Chains to analyze power state transitions in
Edge Computing. By deriving steady-state probabilities and evaluating energy
consumption, we demonstrate the benefits of AI-driven predictive power scaling
over conventional reactive methods. Monte Carlo simulations validate the model,
showing strong alignment between theoretical and empirical results. Sensitivity
analysis highlights how varying transition probabilities affect power
efficiency, confirming that predictive scaling minimizes unnecessary
transitions and improves overall system responsiveness. Our findings suggest
that AI-based power management strategies significantly enhance energy
efficiency by anticipating workload demands and optimizing state transitions.
Experimental results indicate that AI-based power management optimizes workload
distribution across heterogeneous edge nodes, reducing energy consumption
disparities between devices, improving overall efficiency, and enhancing
adaptive power coordination in multi-node environments.

</details>


### [9] [Parallel Spawning Strategies for Dynamic-Aware MPI Applications](https://arxiv.org/abs/2511.04268)
*Iker Martín-Álvarez,José I. Aliaga,Maribel Castillo,Sergio Iserte*

Main category: cs.DC

TL;DR: 本文提出了一种新颖的并行启动策略，通过所有进程在重分配前协作启动，显著降低MPI应用在动态资源调整（尤其是收缩）时的开销，在保持扩展性能的同时将收缩成本降低至少20倍。


<details>
  <summary>Details</summary>
Motivation: 动态资源管理对高性能计算系统至关重要，但现有MPI应用的可塑性方法存在重配置成本高、无法有效释放节点等问题，亟需更高效的解决方案。

Method: 提出一种新型并行启动策略：所有进程在资源重分配前协同参与启动过程，并改进收缩机制以彻底释放不再需要的节点。

Result: 该策略在扩展时最多仅有1.25倍开销，同时将收缩操作的成本降低至少20倍，并在同构与异构系统及共享资源环境中均得到验证。

Conclusion: 所提方法有效克服了现有MPI可塑性技术的局限性，显著提升了动态资源调整效率，有助于减少作业等待时间和系统整体完工时间。

Abstract: Dynamic resource management is an increasingly important capability of High
Performance Computing systems, as it enables jobs to adjust their resource
allocation at runtime. This capability has been shown to reduce workload
makespan, substantially decrease job waiting times and improve overall system
utilization. In this context, malleability refers to the ability of
applications to adapt to new resource allocations during execution. Although
beneficial, malleability incurs significant reconfiguration costs, making the
reduction of these costs an important research topic.
  Some existing methods for MPI applications respawn the entire application,
which is an expensive solution that avoids the reuse of original processes.
Other MPI methods reuse them, but fail to fully release unneeded processes when
shrinking, since some ranks within the same communicator remain active across
nodes, preventing the application from returning those nodes to the system.
This work overcomes both limitations by proposing a novel parallel spawning
strategy, in which all processes cooperate in spawning before redistribution,
thereby reducing execution time. Additionally, it removes shrinkage
limitations, allowing better adaptation of parallel systems to workload and
reducing their makespan. As a result, it preserves competitive expansion times
with at most a $1.25\times$ overhead, while enabling fast shrink operations
that reduce their cost by at least $20\times$. This strategy has been validated
on both homogeneous and heterogeneous systems and can also be applied in
shared-resource environments.

</details>


### [10] [Enabling Dynamic Sparsity in Quantized LLM Inference](https://arxiv.org/abs/2511.04477)
*Rongxiang Wang,Kangyuan Shu,Felix Xiaozhu Lin*

Main category: cs.DC

TL;DR: 本文提出了一种在低比特量化下实现动态稀疏推理的方法，通过zigzag量化布局、专用GEMV核和紧凑运行时机制，在保持精度的同时在多种硬件上实现最高1.55倍的解码加速。


<details>
  <summary>Details</summary>
Motivation: 在终端设备上部署大语言模型（LLMs）面临内存和计算资源受限的问题；尽管LLMs内部激活具有动态稀疏性可减少计算，但其与主流的分组量化方法不兼容，阻碍了高效推理。

Method: 提出三种关键技术：(1) 与激活稀疏性一致且提升GPU内存局部性的zigzag量化布局；(2) 针对该布局设计的专用GEMV核以充分利用并行计算单元；(3) 开销极小的稀疏索引收集运行时机制。

Result: 在多种模型规模和硬件配置下，该方法在保持与稠密量化推理相当精度的前提下，实现最高1.55倍的解码吞吐量提升。

Conclusion: 结构化稀疏性与量化可在商用GPU上有效共存，所提方法显著提升了终端设备上LLM推理效率。

Abstract: Deploying large language models (LLMs) on end-user devices is gaining
importance due to benefits in responsiveness, privacy, and operational cost.
Yet the limited memory and compute capability of mobile and desktop GPUs make
efficient execution difficult. Recent observations suggest that the internal
activations of LLMs are often dynamically sparse, meaning that for each input,
only part of the network contributes significantly to the output. Such sparsity
could reduce computation, but it interacts poorly with group-wise quantization,
which remains the dominant approach for fitting LLMs onto resource-constrained
hardware. To reconcile these two properties, this study proposes a set of
techniques that realize dynamic sparse inference under low-bit quantization.
The method features: (1) a zigzag-patterned quantization layout that organizes
weights in a way consistent with activation sparsity and improves GPU memory
locality; (2) a specialized GEMV kernel designed for this layout to fully
utilize parallel compute units; and (3) a compact runtime mechanism that
gathers sparse indices with minimal overhead. Across several model scales and
hardware configurations, the approach achieves up to 1.55x faster decoding
throughput while maintaining accuracy comparable to dense quantized inference,
showing that structured sparsity and quantization can effectively coexist on
commodity GPUs.

</details>


### [11] [A New Probabilistic Mobile Byzantine Failure Model for Self-Protecting Systems](https://arxiv.org/abs/2511.04523)
*Silvia Bonomi,Giovanni Farina,Roy Friedman,Eviatar B. Procaccia,Sebastien Tixeuil*

Main category: cs.DC

TL;DR: 本文提出一种基于MAPE-K架构的自保护分布式系统模型，引入新的概率性移动拜占庭故障（MBF）机制，用于刻画动态攻击行为，并通过数学分析与仿真研究系统在攻击传播与自恢复之间的动态平衡。


<details>
  <summary>Details</summary>
Motivation: 现有基于拜占庭进程的容错模型难以准确反映现实中的动态攻击场景，因此需要更贴近实际的建模方法来提升分布式系统的自保护能力。

Method: 在MAPE-K自适应架构的分析组件中嵌入一种新的概率性移动拜占庭故障（MBF）模型，结合数学分析与仿真实验，研究攻击传播速率与系统自恢复速率对系统状态演化的影响。

Result: 论文推导了系统中拜占庭节点数量越过安全阈值或恢复至安全状态所需的时间，并通过仿真验证了所提模型在不同参数设置下的系统行为。

Conclusion: 所提出的概率性MBF模型能有效刻画动态攻击过程，为自保护分布式系统提供理论支持和实用指导，增强其在面对复杂威胁时的适应性和韧性。

Abstract: Modern distributed systems face growing security threats, as attackers
continuously enhance their skills and vulnerabilities span across the entire
system stack, from hardware to the application layer. In the system design
phase, fault tolerance techniques can be employed to safeguard systems. From a
theoretical perspective, an attacker attempting to compromise a system can be
abstracted by considering the presence of Byzantine processes in the system.
Although this approach enhances the resilience of the distributed system, it
introduces certain limitations regarding the accuracy of the model in
reflecting real-world scenarios. In this paper, we consider a self-protecting
distributed system based on the \emph{Monitoring-Analyse-Plan-Execute over a
shared Knowledge} (MAPE-K) architecture, and we propose a new probabilistic
Mobile Byzantine Failure (MBF) that can be plugged into the Analysis component.
Our new model captures the dynamics of evolving attacks and can be used to
drive the self-protection and reconfiguration strategy. We analyze
mathematically the time that it takes until the number of Byzantine nodes
crosses given thresholds, or for the system to self-recover back into a safe
state, depending on the rates of Byzantine infection spreading \emph{vs.} the
rate of self-recovery. We also provide simulation results that illustrate the
behavior of the system under such assumptions.

</details>


### [12] [Resolving Conflicts with Grace: Dynamically Concurrent Universality](https://arxiv.org/abs/2511.04631)
*Petr Kuznetsov,Nathan Josia Schrodt*

Main category: cs.DC

TL;DR: 本文提出动态并发的概念，通过根据系统当前状态动态调整同步机制，仅在必要时使用强同步原语，并给出一种动态并发的通用构造方法。


<details>
  <summary>Details</summary>
Motivation: 传统分布式计算中，同步是可扩展性的主要障碍。当并发操作因顺序依赖而发生冲突时，通常采用静态同步策略，但很多冲突仅在罕见状态下才出现，因此需要一种能根据系统状态动态检测和处理冲突的方法。

Method: 定义“动态并发”概念：操作仅在当前系统状态下确实需要与并发操作仲裁时，才使用强同步原语；并基于此构建一个动态并发的通用构造。

Result: 提出了动态并发的形式化定义，并实现了一个支持该特性的通用构造，能够在减少不必要同步的同时保证正确性。

Conclusion: 动态并发能够有效缓解同步带来的可扩展性问题，通过状态感知的冲突检测机制，在保证正确性的前提下提升系统性能。

Abstract: Synchronization is the major obstacle to scalability in distributed
computing. Concurrent operations on the shared data engage in synchronization
when they encounter a \emph{conflict}, i.e., their effects depend on the order
in which they are applied. Ideally, one would like to detect conflicts in a
\emph{dynamic} manner, i.e., adjusting to the current system state. Indeed, it
is very common that two concurrent operations conflict only in some rarely
occurring states. In this paper, we define the notion of \emph{dynamic
concurrency}: an operation employs strong synchronization primitives only if it
\emph{has} to arbitrate with concurrent operations, given the current system
state. We then present a dynamically concurrent universal construction.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [13] [Tutorial Debriefing: Applied Statistical Causal Inference in Requirements Engineering](https://arxiv.org/abs/2511.03875)
*Julian Frattini,Hans-Martin Heyn,Robert Feldt,Richard Torkar*

Main category: cs.SE

TL;DR: 本文探讨了软件工程研究如何通过因果推断将研究成果有效转化为实践，尤其在无法进行随机对照试验时，强调利用观察性数据进行统计因果推断的重要性。


<details>
  <summary>Details</summary>
Motivation: 软件工程研究旨在改善软件开发者与用户的实践和体验，但其成果（如工具、流程、指南）的价值依赖于能否证明其对性能指标具有因果效应。然而，随机对照试验常因法律、伦理或实际限制不可行，因此需要可靠的替代方法。

Method: 提出在无法开展随机对照试验的情况下，采用基于观察性数据的统计因果推断（SCI）方法来验证软件工程干预措施的有效性。

Result: 文章指出统计因果推断可作为评估软件工程实践中干预效果的可行且可靠的方法，为知识转化提供实证基础。

Conclusion: 为了确保软件工程研究成果真正有益于实践，研究者应重视并应用统计因果推断方法，尤其是在无法进行随机实验的情境下。

Abstract: As any scientific discipline, the software engineering (SE) research
community strives to contribute to the betterment of the target population of
our research: software producers and consumers. We will only achieve this
betterment if we manage to transfer the knowledge acquired during research into
practice. This transferal of knowledge may come in the form of tools,
processes, and guidelines for software developers. However, the value of these
contributions hinges on the assumption that applying them causes an improvement
of the development process, user experience, or other performance metrics. Such
a promise requires evidence of causal relationships between an exposure or
intervention (i.e., the contributed tool, process or guideline) and an outcome
(i.e., performance metrics). A straight-forward approach to obtaining this
evidence is via controlled experiments in which a sample of a population is
randomly divided into a group exposed to the new tool, process, or guideline,
and a control group. However, such randomized control trials may not be
legally, ethically, or logistically feasible. In these cases, we need a
reliable process for statistical causal inference (SCI) from observational
data.

</details>


### [14] [Collaborative Agents for Automated Program Repair in Ruby](https://arxiv.org/abs/2511.03925)
*Nikta Akbarpour,Mahdieh Sadat Benis,Fatemeh Hendijani Fard,Ali Ouni,Mohamed Aymen Saied*

Main category: cs.SE

TL;DR: 本文提出了RAMP，一种轻量级的多智能体框架，用于Ruby语言的自动化程序修复。该方法通过测试驱动的反馈机制迭代优化修复方案，在XCodeEval基准上以67%的pass@1表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于大语言模型的自动化程序修复（APR）方法计算成本高且主要聚焦于少数编程语言，而广泛用于Web开发的Ruby语言在APR研究中被忽视。

Method: RAMP采用协作式多智能体架构，通过生成针对性测试、错误反思和候选修复迭代优化，无需依赖大规模多语言修复数据库或昂贵微调，仅依靠轻量级提示和测试反馈直接作用于Ruby代码。

Result: 在XCodeEval基准测试中，RAMP在Ruby任务上达到67%的pass@1准确率，通常在五次迭代内收敛；消融实验证明测试生成与自我反思是性能关键；该方法对错误答案、编译错误和运行时错误尤为有效。

Conclusion: RAMP为多智能体修复策略提供了新见解，并为将大语言模型调试工具扩展到研究较少的语言奠定了基础。

Abstract: Automated Program Repair (APR) has advanced rapidly with Large Language
Models (LLMs), but most existing methods remain computationally expensive, and
focused on a small set of languages. Ruby, despite its widespread use in web
development and the persistent challenges faced by its developers, has received
little attention in APR research. In this paper, we introduce RAMP, a novel
lightweight framework that formulates program repair as a feedback-driven,
iterative process for Ruby. RAMP employs a team of collaborative agents that
generate targeted tests, reflect on errors, and refine candidate fixes until a
correct solution is found. Unlike prior approaches, RAMP is designed to avoid
reliance on large multilingual repair databases or costly fine-tuning, instead
operating directly on Ruby through lightweight prompting and test-driven
feedback. Evaluation on the XCodeEval benchmark shows that RAMP achieves a
pass@1 of 67% on Ruby, outper-forming prior approaches. RAMP converges quickly
within five iterations, and ablation studies confirm that test generation and
self-reflection are key drivers of its performance. Further analysis shows that
RAMP is particularly effective at repairing wrong answers, compilation errors,
and runtime errors. Our approach provides new insights into multi-agent repair
strategies, and establishes a foundation for extending LLM-based debugging
tools to under-studied languages.

</details>


### [15] [PEFA-AI: Advancing Open-source LLMs for RTL generation using Progressive Error Feedback Agentic-AI](https://arxiv.org/abs/2511.03934)
*Athma Narayanan,Mahesh Subedar,Omesh Tickoo*

Main category: cs.SE

TL;DR: 本文提出了一种多智能体协同流程，结合专用大语言模型（LLM）与硬件仿真工具，通过渐进式错误反馈机制（PEFA）自动生成可综合、功能正确的寄存器传输级（RTL）代码，在无需人工干预的情况下实现了当前最优的RTL生成效果。


<details>
  <summary>Details</summary>
Motivation: 现有自然语言到RTL的自动转换方法在功能正确性、可综合性及自动化程度方面仍有不足，亟需一种能自我纠错、高效且无需人工干预的生成流程。

Method: 构建一个多智能体系统，集成专用LLM与硬件仿真工具，并引入渐进式错误反馈系统（PEFA），通过迭代反馈逐步提升生成复杂度，确保生成的RTL通过编译、功能验证和可综合性检查。

Result: 在两个开源NL-to-RTL数据集上进行评估，该方法显著优于现有技术，达到最高的通过率，同时在token使用效率上表现优异，并缩小了开源与闭源LLM之间的性能差距。

Conclusion: 所提出的基于PEFA的多智能体流程为RTL自动生成提供了一种高效、可靠且完全自动化的解决方案，代表了该领域的新前沿。

Abstract: We present an agentic flow consisting of multiple agents that combine
specialized LLMs and hardware simulation tools to collaboratively complete the
complex task of Register Transfer Level (RTL) generation without human
intervention. A key feature of the proposed flow is the progressive error
feedback system of agents (PEFA), a self-correcting mechanism that leverages
iterative error feedback to progressively increase the complexity of the
approach. The generated RTL includes checks for compilation, functional
correctness, and synthesizable constructs. To validate this adaptive approach
to code generation, benchmarking is performed using two opensource natural
language-to-RTL datasets. We demonstrate the benefits of the proposed approach
implemented on an open source agentic framework, using both open- and
closed-source LLMs, effectively bridging the performance gap between them.
Compared to previously published methods, our approach sets a new benchmark,
providing state-of-the-art pass rates while being efficient in token counts.

</details>


### [16] [PSD2Code: Automated Front-End Code Generation from Design Files via Multimodal Large Language Models](https://arxiv.org/abs/2511.04012)
*Yongxi Chen,Lei Chen*

Main category: cs.SE

TL;DR: 本文提出PSD2Code，一种基于多模态的从PSD设计稿生成生产级React+SCSS代码的新方法，通过Parse-Align-Generate流程显著提升代码质量与视觉保真度。


<details>
  <summary>Details</summary>
Motivation: 现有设计到代码生成方法常存在结构不一致、资源错位和缺乏生产就绪性等问题，亟需更可靠且工业可用的解决方案。

Method: 提出ParseAlignGenerate流水线，从PSD文件中提取层级结构、图层属性和元数据，结合约束对齐策略与结构化提示，引导大语言模型生成高保真、可部署的前端代码。

Result: 在代码相似性、视觉保真度和生产就绪性等多个指标上显著优于现有方法，并在不同大语言模型上表现出良好的模型无关性。

Conclusion: 将结构化设计信息与多模态大语言模型结合，能有效实现工业级前端代码自动生成，推动设计驱动的自动化前端开发。

Abstract: Design-to-code generation has emerged as a promising approach to bridge the
gap between design prototypes and deployable frontend code. However, existing
methods often suffer from structural inconsistencies, asset misalignment, and
limited production readiness. This paper presents PSD2Code, a novel multi-modal
approach that leverages PSD file parsing and asset alignment to generate
production-ready React+SCSS code. Our method introduces a ParseAlignGenerate
pipeline that extracts hierarchical structures, layer properties, and metadata
from PSD files, providing large language models with precise spatial
relationships and semantic groupings for frontend code generation. The system
employs a constraint-based alignment strategy that ensures consistency between
generated elements and design resources, while a structured prompt construction
enhances controllability and code quality. Comprehensive evaluation
demonstrates significant improvements over existing methods across multiple
metrics including code similarity, visual fidelity, and production readiness.
The method exhibits strong model independence across different large language
models, validating the effectiveness of integrating structured design
information with multimodal large language models for industrial-grade code
generation, marking an important step toward design-driven automated frontend
development.

</details>


### [17] [Specification-Guided Vulnerability Detection with Large Language Models](https://arxiv.org/abs/2511.04014)
*Hao Zhu,Jia Li,Cuiyun Gao,Jiaru Qian,Yihong Dong,Huanyu Liu,Lecheng Wang,Ziliang Wang,Xiaolong Hu,Ge Li*

Main category: cs.SE

TL;DR: 本文提出VulInstruct方法，通过从历史漏洞中提取安全规范（包括通用规范和领域特定规范），引导大语言模型进行漏洞检测，显著提升了检测性能与推理能力，并发现了一个新的高危漏洞。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在漏洞检测任务中表现有限，难以区分漏洞代码与修复后的代码，主要原因是缺乏对安全规范（即代码应如何行为以保持安全）的理解，而这类知识在训练数据中通常未显式体现。

Method: VulInstruct是一种基于安全规范引导的方法，从两个角度构建规范知识库：(i) 跨项目的高质量补丁中提取通用安全规范；(ii) 从目标代码相关仓库中重复出现的违规模式中提取领域特定规范。该方法检索相关历史案例和规范，使大语言模型能够基于预期的安全行为进行推理，而非依赖表面模式。

Result: 在PrimeVul数据集上，VulInstruct相比基线方法F1分数提升32.7%（达45.0%），召回率提升50.8%（达37.7%），并能独检出24.3%的漏洞（是最佳基线的2.4倍）；在成对评估中相对提升32.3%；此外还发现了一个新的高危漏洞CVE-2025-56538。

Conclusion: VulInstruct通过引入安全规范知识，有效增强了大语言模型在漏洞检测中的推理能力和实际应用价值，为现实世界中的漏洞发现提供了新思路。

Abstract: Large language models (LLMs) have achieved remarkable progress in code
understanding tasks. However, they demonstrate limited performance in
vulnerability detection and struggle to distinguish vulnerable code from
patched code. We argue that LLMs lack understanding of security specifications
-- the expectations about how code should behave to remain safe. When code
behavior differs from these expectations, it becomes a potential vulnerability.
However, such knowledge is rarely explicit in training data, leaving models
unable to reason about security flaws. We propose VulInstruct, a
specification-guided approach that systematically extracts security
specifications from historical vulnerabilities to detect new ones. VulInstruct
constructs a specification knowledge base from two perspectives: (i) General
specifications from high-quality patches across projects, capturing fundamental
safe behaviors; and (ii) Domain-specific specifications from repeated
violations in particular repositories relevant to the target code. VulInstruct
retrieves relevant past cases and specifications, enabling LLMs to reason about
expected safe behaviors rather than relying on surface patterns. We evaluate
VulInstruct under strict criteria requiring both correct predictions and valid
reasoning. On PrimeVul, VulInstruct achieves 45.0% F1-score (32.7% improvement)
and 37.7% recall (50.8% improvement) compared to baselines, while uniquely
detecting 24.3% of vulnerabilities -- 2.4x more than any baseline. In pair-wise
evaluation, VulInstruct achieves 32.3% relative improvement. VulInstruct also
discovered a previously unknown high-severity vulnerability (CVE-2025-56538) in
production code, demonstrating practical value for real-world vulnerability
discovery. All code and supplementary materials are available at
https://github.com/zhuhaopku/VulInstruct-temp.

</details>


### [18] [LLM-Driven Adaptive Source-Sink Identification and False Positive Mitigation for Static Analysis](https://arxiv.org/abs/2511.04023)
*Shiyin Lin*

Main category: cs.SE

TL;DR: AdaTaint 是一个结合大语言模型（LLM）与符号验证的污点分析框架，通过神经符号推理自适应推断源/汇规范并过滤误报，在保持运行开销可控的同时显著降低静态分析中的假阳性率并提升召回率。


<details>
  <summary>Details</summary>
Motivation: 传统静态分析在发现软件漏洞时受限于不完整的源-汇规范和大量误报，现有纯LLM方法缺乏确定性和程序事实依据。

Method: 提出 AdaTaint 框架，利用 LLM 自适应推断源/汇规范，并通过神经符号推理将模型建议与程序事实及约束验证相结合，实现兼具适应性与确定性的污点分析。

Result: 在 Juliet 1.3、SV-COMP 风格 C 基准和三个大型真实项目上的实验表明，AdaTaint 相比 CodeQL、Joern 和纯 LLM 方法，平均减少 43.7% 的假阳性，召回率提升 11.2%，且运行时开销可控。

Conclusion: 将 LLM 推理与符号验证相结合，是实现更准确、可靠静态漏洞分析的有效路径。

Abstract: Static analysis is effective for discovering software vulnerabilities but
notoriously suffers from incomplete source--sink specifications and excessive
false positives (FPs). We present \textsc{AdaTaint}, an LLM-driven taint
analysis framework that adaptively infers source/sink specifications and
filters spurious alerts through neuro-symbolic reasoning. Unlike LLM-only
detectors, \textsc{AdaTaint} grounds model suggestions in program facts and
constraint validation, ensuring both adaptability and determinism.
  We evaluate \textsc{AdaTaint} on Juliet 1.3, SV-COMP-style C benchmarks, and
three large real-world projects. Results show that \textsc{AdaTaint} reduces
false positives by \textbf{43.7\%} on average and improves recall by
\textbf{11.2\%} compared to state-of-the-art baselines (CodeQL, Joern, and
LLM-only pipelines), while maintaining competitive runtime overhead. These
findings demonstrate that combining LLM inference with symbolic validation
offers a practical path toward more accurate and reliable static vulnerability
analysis.

</details>


### [19] [Benchmarking and Studying the LLM-based Agent System in End-to-End Software Development](https://arxiv.org/abs/2511.04064)
*Zhengran Zeng,Yixin Li,Rui Xie,Wei Ye,Shikun Zhang*

Main category: cs.SE

TL;DR: 本文提出了一个更具挑战性的端到端软件开发基准E2EDevBench和一个结合测试用例与大语言模型驱动需求验证的混合评估框架，通过控制变量实验发现当前最先进的智能体仅能完成约50%的需求，其性能高度依赖任务分解与协作架构，并指出需求遗漏和自我验证不足是主要瓶颈。


<details>
  <summary>Details</summary>
Motivation: 现有基于大语言模型（LLM）的自主软件开发智能体缺乏科学评估，主要受限于过于简化的基准测试以及不同智能体架构之间因实现差异导致的不公平比较。

Method: 构建动态维护、贴近真实场景的E2EDevBench基准；提出融合测试用例功能评估与细粒度LLM需求验证的混合评估框架；在统一基础平台上实现三种代表性智能体架构，开展控制变量实证研究以隔离工作流设计的影响。

Result: 最先进的智能体在E2EDevBench上仅能满足约50%的需求；任务分解与协作架构对性能影响显著；主要失败原因在于需求遗漏和自我验证能力不足。

Conclusion: 本研究提供了更真实的基准、全面的评估框架和对当前软件开发智能体能力与核心挑战的关键洞察，为未来提升需求理解与规划能力的研究指明方向。

Abstract: The development of LLM-based autonomous agents for end-to-end software
development represents a significant paradigm shift in software engineering.
However, the scientific evaluation of these systems is hampered by significant
challenges, including overly simplistic benchmarks and the difficulty of
conducting fair comparisons between different agent architectures due to
confounding implementation variables. To address these limitations, we first
construct a challenging and dynamically curated E2EDevBench to simulate
realistic development scenarios. Second, we propose a hybrid evaluation
framework that combines test-case-based functional assessment with
fine-grained, LLM-based requirement verification. Using this framework, we
conduct a controlled empirical study on three representative agent
architectures implemented upon a unified foundation to isolate the impact of
workflow design. Our findings reveal that state-of-the-art agents can fulfill
approximately 50\% of requirements on \bench{}, but their success is critically
dependent on the architectural strategy for task decomposition and
collaboration. Furthermore, our analysis indicates that the primary bottleneck
is the omission of requirements and inadequate self-verification. This work
provides the community with a more realistic benchmark, a comprehensive
evaluation framework, and crucial insights into the current capabilities and
core challenges of software development agents, guiding future research toward
enhancing requirement comprehension and planning.

</details>


### [20] [How Natural Language Proficiency Shapes GenAI Code for Software Engineering Tasks](https://arxiv.org/abs/2511.04115)
*Ruksit Rojpaisarnkit,Youmei Fan,Kenichi Matsumoto,Raula Gaikovina Kula*

Main category: cs.SE

TL;DR: 本文研究了自然语言提示的英语熟练度对大语言模型（LLM）生成代码质量和正确性的影响，发现更高英语水平的提示能持续提升代码正确性。


<details>
  <summary>Details</summary>
Motivation: 尽管已有大量研究关注提示结构，但自然语言熟练度对LLM生成代码质量的影响尚未充分探索。本文旨在验证英语熟练度本身是否独立于提示技术影响代码生成效果。

Method: 基于HumanEval数据集，对164个编程任务的提示语从基础到高级系统性地调整英语熟练度，并评估所生成代码的熟练度与正确性。

Result: LLM默认使用中等（B2）自然语言水平；虽然对代码熟练度的影响因模型而异，但高熟练度提示在所有模型中均显著提高了代码正确性。

Conclusion: 自然语言熟练度是控制代码生成的关键因素，有助于开发者优化AI输出并提升解决方案的可靠性。

Abstract: With the widespread adoption of Foundation Model (FM)-powered tools in
software engineering, the natural language prompt has become a critical
interface between developers and Large Language Models (LLMs). While much
research has focused on prompt structure, the natural language proficiency is
an underexplored factor that can influence the quality of generated code. This
paper investigates whether the English language proficiency itself independent
of the prompting technique affects the proficiency and correctness of code
generated by LLMs. Using the HumanEval dataset, we systematically varied the
English proficiency of prompts from basic to advanced for 164 programming tasks
and measured the resulting code proficiency and correctness. Our findings show
that LLMs default to an intermediate (B2) natural language level. While the
effect on the resulting code proficiency was model-dependent, we found that
higher-proficiency prompts consistently yielded more correct code across all
models. These results demonstrate that natural language proficiency is a key
lever for controlling code generation, helping developers tailor AI output and
improve the reliability of solutions.

</details>


### [21] [Are We Aligned? A Preliminary Investigation of the Alignment of Responsible AI Values between LLMs and Human Judgment](https://arxiv.org/abs/2511.04157)
*Asma Yamani,Malak Baslyman,Moataz Ahmed*

Main category: cs.SE

TL;DR: 该研究评估了23个大语言模型（LLMs）在负责任AI价值观方面与两类人群（美国代表性样本和AI从业者）的一致性，发现LLMs更贴近AI从业者的偏好，但在声明的价值观与实际需求优先级之间存在不一致。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型在软件工程任务中的广泛应用，其在负责任AI价值观上是否与人类判断一致成为关键问题。本研究旨在系统评估LLMs在价值观偏好上与不同人群的对齐程度，并揭示潜在风险。

Method: 研究通过四项任务评估23个LLMs：(T1) 选择关键负责任AI价值观；(T2) 评估这些价值观在特定情境中的重要性；(T3) 解决价值观之间的权衡；(T4) 对体现这些价值观的软件需求进行优先级排序。比较LLMs与美国代表性样本及AI从业者之间的对齐情况。

Result: LLMs整体上更贴近AI从业者而非美国代表性样本的价值观，强调公平、隐私、透明、安全和问责。然而，在前三个任务中声明的价值观与第四个任务中实际的需求优先级之间存在不一致，显示出“言行不一”的问题。

Conclusion: 依赖LLMs进行需求工程存在实际风险，需辅以人工监督；研究呼吁建立系统化方法来对齐、解释和监控AI辅助软件开发中的价值观一致性。

Abstract: Large Language Models (LLMs) are increasingly employed in software
engineering tasks such as requirements elicitation, design, and evaluation,
raising critical questions regarding their alignment with human judgments on
responsible AI values. This study investigates how closely LLMs' value
preferences align with those of two human groups: a US-representative sample
and AI practitioners. We evaluate 23 LLMs across four tasks: (T1) selecting key
responsible AI values, (T2) rating their importance in specific contexts, (T3)
resolving trade-offs between competing values, and (T4) prioritizing software
requirements that embody those values. The results show that LLMs generally
align more closely with AI practitioners than with the US-representative
sample, emphasizing fairness, privacy, transparency, safety, and
accountability. However, inconsistencies appear between the values that LLMs
claim to uphold (Tasks 1-3) and the way they prioritize requirements (Task 4),
revealing gaps in faithfulness between stated and applied behavior. These
findings highlight the practical risk of relying on LLMs in requirements
engineering without human oversight and motivate the need for systematic
approaches to benchmark, interpret, and monitor value alignment in AI-assisted
software development.

</details>


### [22] [Explaining Software Vulnerabilities with Large Language Models](https://arxiv.org/abs/2511.04179)
*Oshando Johnson,Alexandra Fomina,Ranjith Krishnamurthy,Vaibhav Chaudhari,Rohith Kumar Shanmuganathan,Eric Bodden*

Main category: cs.SE

TL;DR: 本文提出SAFE，一种结合大语言模型（GPT-4o）的IDE插件，用于提升静态应用安全测试（SAST）工具的可解释性，帮助初级至中级开发者更好地理解和修复安全漏洞。


<details>
  <summary>Details</summary>
Motivation: 现有SAST工具生成的警告信息过于通用，缺乏对漏洞成因、影响和修复策略的清晰说明，导致开发者难以理解或忽视关键漏洞。

Method: 开发SAFE插件，集成GPT-4o，在IDE中自动生成针对SAST检测出的漏洞的详细解释，包括原因、影响和缓解措施。

Result: 专家用户研究表明，SAFE生成的解释显著提升了初级至中级开发者对安全漏洞的理解与处理能力。

Conclusion: 利用大语言模型增强SAST工具的可解释性是一种有效手段，能显著改善其可用性，尤其对经验较少的开发者具有重要帮助。

Abstract: The prevalence of security vulnerabilities has prompted companies to adopt
static application security testing (SAST) tools for vulnerability detection.
Nevertheless, these tools frequently exhibit usability limitations, as their
generic warning messages do not sufficiently communicate important information
to developers, resulting in misunderstandings or oversight of critical
findings. In light of recent developments in Large Language Models (LLMs) and
their text generation capabilities, our work investigates a hybrid approach
that uses LLMs to tackle the SAST explainability challenges. In this paper, we
present SAFE, an Integrated Development Environment (IDE) plugin that leverages
GPT-4o to explain the causes, impacts, and mitigation strategies of
vulnerabilities detected by SAST tools. Our expert user study findings indicate
that the explanations generated by SAFE can significantly assist beginner to
intermediate developers in understanding and addressing security
vulnerabilities, thereby improving the overall usability of SAST tools.

</details>


### [23] [GITER: A Git-Based Declarative Exchange Model Using Kubernetes-Style Custom Resources](https://arxiv.org/abs/2511.04182)
*Christos Tranoris*

Main category: cs.SE

TL;DR: 本文提出了一种基于 Git 的轻量级、可审计的异步信息交换方法，利用 Kubernetes Operator 和自定义资源（CR）模式，将 Git 作为分布式实体间的通信媒介，扩展了 GitOps 的应用场景。


<details>
  <summary>Details</summary>
Motivation: 传统 API 和消息中间件在跨域、跨组织或隔离环境下的协作中存在耦合度高、审计困难等问题，作者旨在通过 Git 提供一种更透明、可追溯且松耦合的通信机制。

Method: 采用 Git 作为协调媒介，结合 Kubernetes Operator 和自定义资源（CR），通过共享仓库中的 spec 字段表示期望状态、status 字段反映实际状态，实现声明式通信。

Result: 该方法成功将 GitOps 扩展至跨域和隔离环境的信息交换场景，具备版本控制、提交签名和访问控制等原生特性，提升了系统的透明性、可追溯性和可复现性。

Conclusion: Git 可作为声明式通信的基础架构，在保持系统自治与松耦合的同时，支持安全、可审计的异步信息交换，适用于多种复杂协作场景。

Abstract: This paper introduces a lightweight and auditable method for asynchronous
information exchange between distributed entities using Git as the coordination
medium. The proposed approach replaces traditional APIs and message brokers
with a Git-based communication model built on the principles of Kubernetes
Operators and Custom Resources (CRs). Each participating entity, designated as
a Publisher or Consumer, interacts through a shared repository that serves as a
single source of truth, where the spec field captures the desired state and the
status field reflects the observed outcome. This pattern extends GitOps beyond
infrastructure management to support cross-domain, inter-organizational, and
air-gapped collaboration scenarios. By leveraging Git native features
(versioning, commit signing, and access control) the model ensures
transparency, traceability, and reproducibility while preserving loose coupling
and autonomy between systems. The paper discusses architectural principles,
implementation considerations, and comparisons with RESTful and broker-based
integrations, highlighting both the advantages and trade-offs of adopting Git
as a declarative communication substrate.

</details>


### [24] [A Tool for Benchmarking Large Language Models' Robustness in Assessing the Realism of Driving Scenarios](https://arxiv.org/abs/2511.04267)
*Jiahui Wu,Chengjie Lu,Aitor Arrieta,Shaukat Ali*

Main category: cs.SE

TL;DR: 本文提出了DriveRLR，一种用于评估大语言模型（LLM）在判断驾驶场景真实性方面鲁棒性的基准工具。该工具通过生成变异场景并构建文本提示，测试LLM对真实感的判断能力，并在DeepScenario数据集上验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有仿真测试中缺乏有效评估驾驶场景真实性的方法，而大语言模型展现出强大的推理与泛化能力，有望用于该任务，因此需要一个系统性基准来评估LLM在此类任务中的鲁棒性。

Method: DriveRLR通过生成变异的驾驶场景变体并构造相应的文本提示，输入给大语言模型，从而评估其判断场景真实性的能力与鲁棒性。

Result: 在DeepScenario数据集上对GPT-5、Llama 4 Maverick和Mistral Small 3.2三种先进LLM的测试表明，DriveRLR能有效揭示不同模型在鲁棒性上的差异。

Conclusion: DriveRLR不仅可作为评估LLM鲁棒性的有效基准，还可作为实际应用组件，例如作为目标函数引导场景生成，支持基于仿真的自动驾驶系统测试流程。

Abstract: In recent years, autonomous driving systems have made significant progress,
yet ensuring their safety remains a key challenge. To this end, scenario-based
testing offers a practical solution, and simulation-based methods have gained
traction due to the high cost and risk of real-world testing. However,
evaluating the realism of simulated scenarios remains difficult, creating
demand for effective assessment methods. Recent advances show that Large
Language Models (LLMs) possess strong reasoning and generalization
capabilities, suggesting their potential in assessing scenario realism through
scenario-related textual prompts. Motivated by this, we propose DriveRLR, a
benchmark tool to assess the robustness of LLMs in evaluating the realism of
driving scenarios. DriveRLR generates mutated scenario variants, constructs
prompts, which are then used to assess a given LLM's ability and robustness in
determining the realism of driving scenarios. We validate DriveRLR on the
DeepScenario dataset using three state-of-the-art LLMs: GPT-5, Llama 4
Maverick, and Mistral Small 3.2. Results show that DriveRLR effectively reveals
differences in the robustness of various LLMs, demonstrating its effectiveness
and practical value in scenario realism assessment. Beyond LLM robustness
evaluation, DriveRLR can serve as a practical component in applications such as
an objective function to guide scenario generation, supporting simulation-based
ADS testing workflows.

</details>


### [25] [Where Do LLMs Still Struggle? An In-Depth Analysis of Code Generation Benchmarks](https://arxiv.org/abs/2511.04355)
*Amir Molzam Sharifloo,Maedeh Heydari,Parsa Kazerooni,Daniel Maninger,Mira Mezini*

Main category: cs.SE

TL;DR: 该论文分析了大型语言模型（LLMs）在代码生成任务中的失败案例，识别出其在四个主流基准测试中普遍难以解决的任务，并揭示了导致失败的四种常见弱点模式及任务本身的复杂性因素。


<details>
  <summary>Details</summary>
Motivation: 当前代码生成领域的基准测试和排行榜虽能对LLMs进行性能排序，但缺乏对其失败任务的深入洞察，而这些信息对于理解模型局限性和指导未来改进至关重要。

Method: 作者考察了四个流行基准中的代码生成任务，筛选出主流LLMs最常失败的任务；通过分析解决方案代码的静态复杂度，并系统检查114个LLMs持续表现不佳的任务，归纳失败原因。

Result: 研究发现了LLMs在代码生成中的四种反复出现的弱点模式，以及基准任务中常见的导致失败的复杂性因素。

Conclusion: 为提升LLMs的代码生成能力，需关注其在特定类型任务中的系统性弱点，并重新审视现有基准任务的设计是否合理。

Abstract: Large Language Models (LLMs) have achieved remarkable success in code
generation, and the race to improve their performance has become a central
focus of AI research. Benchmarks and leaderboards are increasingly popular,
offering quantitative rankings of LLMs. However, they provide limited insight
into the tasks that LLMs consistently fail to solve - information that is
crucial for understanding current limitations and guiding the development of
more capable models. To address this gap, we examined code generation tasks
across four popular benchmarks, identifying those that major LLMs are most
likely to fail. To understand the causes of these failures, we investigated
whether the static complexity of solution code contributes to them, followed by
a systematic inspection of 114 tasks that LLMs consistently struggled with. Our
analysis revealed four recurring patterns of weaknesses in LLMs, as well as
common complications within benchmark tasks that most often lead to failure.

</details>


### [26] [EDIT-Bench: Evaluating LLM Abilities to Perform Real-World Instructed Code Edits](https://arxiv.org/abs/2511.04486)
*Wayne Chi,Valerie Chen,Ryan Shar,Aditya Mittal,Jenny Liang,Wei-Lin Chiang,Anastasios Nikolas Angelopoulos,Ion Stoica,Graham Neubig,Ameet Talwalkar,Chris Donahue*

Main category: cs.SE

TL;DR: 本文提出了 EDIT-Bench，一个基于真实用户指令和代码上下文的代码编辑能力评测基准，包含545个多样化的真实场景问题，评估了40个大语言模型，发现仅有5个模型得分超过60%，并揭示了上下文信息对模型表现的重要影响。


<details>
  <summary>Details</summary>
Motivation: 现有代码编辑能力评测基准多依赖人工构造数据，缺乏对真实使用场景的覆盖，难以有效评估大语言模型在实际开发中根据用户指令修改代码的能力。

Method: 构建 EDIT-Bench 基准，收集真实世界中的用户指令与代码上下文，涵盖多种自然语言、编程语言及多样化用例，并引入依赖代码上下文、高亮代码和光标位置的问题；在此基础上对40个大语言模型进行系统评估。

Result: EDIT-Bench 对当前模型具有挑战性，仅5个模型得分超过60%；模型在不同指令类别上的表现存在差异；上下文信息的丰富程度显著影响任务成功率，性能差异最高达11%。

Conclusion: 真实上下文对代码编辑任务至关重要，EDIT-Bench 为评估大语言模型的代码编辑能力提供了更贴近实际应用的基准。

Abstract: Instructed code editing, where LLMs directly modify a developer's existing
code based on a user instruction, is becoming a widely used interaction mode in
AI coding assistants. However, few benchmarks directly evaluate this capability
and current datasets often rely on artificial sources. We introduce EDIT-Bench,
a benchmark for evaluating LLM code editing capabilities grounded in real-world
usage, i.e., user instructions and code contexts collected in the wild.
EDIT-Bench comprises of 545 problems, multiple natural and programming
languages, and a diverse set of real-world use cases, ranging from resolving
errors to adding features. EDIT-Bench introduces context-dependent problems
that require the model to understand code context, highlighted code, and cursor
position in addition to the user instruction. We evaluate 40 diverse LLMs and
observe that EDIT-Bench is a challenging set of problems where only 5 models
score over 60%. We find that model performance varies across different
categories of user instructions. Further, we find that varying levels of
contextual information greatly affect task success rate, with performance
varying up to 11%, indicating the importance of evaluating with realistic
context.

</details>


### [27] [Microservices Is Dying, A New Method for Module Division Based on Universal Interfaces](https://arxiv.org/abs/2511.04548)
*Qing Wang,Yong Zhang*

Main category: cs.SE

TL;DR: 本文提出一种衡量模块独立性的方法，并基于此设计了一种新的系统架构EIGHT，旨在消除模块间依赖，实现即使在单体应用中也能动态加载、卸载或修改模块，从而探索超越微服务与单体架构的新路径。


<details>
  <summary>Details</summary>
Motivation: 尽管微服务在物理上隔离了模块，但未能阻止依赖的传播和扩散。为了解决模块间耦合问题，需要从根源上评估模块变更的影响并确保模块独立性。

Method: 提出一种计算模块独立性的概念方法，推导出模块独立的必要条件，并据此设计通用接口模式作为模块间的通用边界；在此基础上构建名为EIGHT的平台架构。

Result: 实现了EIGHT架构，验证了只要保证模块独立性，即使是单进程内的单体应用也能在运行时动态加载、卸载或修改任意部分。

Conclusion: 该架构为日益复杂的系统提供了一条超越传统微服务与单体架构的新路径。

Abstract: Although microservices have physically isolated modules, they have failed to
prevent the propagation and diffusion of dependencies. To trace the root cause
of the inter-module coupling, this paper, starting from the impact assessment
approach for module changes, proposes a conceptual method for calculating
module independence and utilizes this method to derive the necessary conditions
for module independence. Then, a new system design philosophy and software
engineering methodology is proposed, aimed at eliminating dependencies between
modules. A specific pattern is employed to design a set of universal
interfaces, serving as a universal boundary between modules. Subsequently, this
method is used to implement a platform architecture named EIGHT, demonstrating
that, as long as module independence is guaranteed, even a monolithic
application within a single process can dynamically load, unload, or modify any
part at runtime. Finally, the paper concludes that this architecture aims to
explore a novel path for increasingly complex systems, beyond microservice and
monolithic architectures.

</details>
