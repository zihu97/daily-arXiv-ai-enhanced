{"id": "2511.17265", "categories": ["cs.AR", "cs.AI", "cs.ET", "cs.PF"], "pdf": "https://arxiv.org/pdf/2511.17265", "abs": "https://arxiv.org/abs/2511.17265", "authors": ["Shady Agwa", "Yikang Shen", "Shiwei Wang", "Themis Prodromakis"], "title": "DISCA: A Digital In-memory Stochastic Computing Architecture Using A Compressed Bent-Pyramid Format", "comment": "6 pages, 5 figures", "summary": "Nowadays, we are witnessing an Artificial Intelligence revolution that dominates the technology landscape in various application domains, such as healthcare, robotics, automotive, security, and defense. Massive-scale AI models, which mimic the human brain's functionality, typically feature millions and even billions of parameters through data-intensive matrix multiplication tasks. While conventional Von-Neumann architectures struggle with the memory wall and the end of Moore's Law, these AI applications are migrating rapidly towards the edge, such as in robotics and unmanned aerial vehicles for surveillance, thereby adding more constraints to the hardware budget of AI architectures at the edge. Although in-memory computing has been proposed as a promising solution for the memory wall, both analog and digital in-memory computing architectures suffer from substantial degradation of the proposed benefits due to various design limitations. We propose a new digital in-memory stochastic computing architecture, DISCA, utilizing a compressed version of the quasi-stochastic Bent-Pyramid data format. DISCA inherits the same computational simplicity of analog computing, while preserving the same scalability, productivity, and reliability of digital systems. Post-layout modeling results of DISCA show an energy efficiency of 3.59 TOPS/W per bit at 500 MHz using a commercial 180nm CMOS technology. Therefore, DISCA significantly improves the energy efficiency for matrix multiplication workloads by orders of magnitude if scaled and compared to its counterpart architectures.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u578b\u6570\u5b57\u5b58\u5185\u968f\u673a\u8ba1\u7b97\u67b6\u6784DISCA\uff0c\u901a\u8fc7\u538b\u7f29\u7684\u51c6\u968f\u673aBent-Pyramid\u6570\u636e\u683c\u5f0f\uff0c\u5728\u4fdd\u6301\u6570\u5b57\u7cfb\u7edf\u53ef\u6269\u5c55\u6027\u4e0e\u53ef\u9760\u6027\u7684\u540c\u65f6\uff0c\u663e\u8457\u63d0\u5347\u8fb9\u7f18AI\u4e2d\u77e9\u9635\u8fd0\u7b97\u7684\u80fd\u6548\u3002", "motivation": "\u4f20\u7edf\u51af\u00b7\u8bfa\u4f9d\u66fc\u67b6\u6784\u53d7\u9650\u4e8e\u201c\u5185\u5b58\u5899\u201d\u548c\u6469\u5c14\u5b9a\u5f8b\u7ec8\u7ed3\uff0c\u96be\u4ee5\u6ee1\u8db3\u8fb9\u7f18AI\u5e94\u7528\uff08\u5982\u673a\u5668\u4eba\u3001\u65e0\u4eba\u673a\uff09\u5bf9\u9ad8\u6548\u80fd\u3001\u4f4e\u529f\u8017\u786c\u4ef6\u7684\u9700\u6c42\uff1b\u73b0\u6709\u5b58\u5185\u8ba1\u7b97\u65b9\u6848\uff08\u6a21\u62df\u6216\u6570\u5b57\uff09\u56e0\u8bbe\u8ba1\u5c40\u9650\u672a\u80fd\u5145\u5206\u53d1\u6325\u6f5c\u529b\u3002", "method": "\u63d0\u51faDISCA\u67b6\u6784\uff0c\u91c7\u7528\u538b\u7f29\u7248\u51c6\u968f\u673aBent-Pyramid\u6570\u636e\u683c\u5f0f\uff0c\u7ed3\u5408\u6570\u5b57\u5b58\u5185\u968f\u673a\u8ba1\u7b97\uff0c\u5728180nm CMOS\u5de5\u827a\u4e0b\u8fdb\u884c\u540e\u5e03\u5c40\u5efa\u6a21\u9a8c\u8bc1\u3002", "result": "\u5728500 MHz\u9891\u7387\u4e0b\uff0cDISCA\u5b9e\u73b0\u6bcf\u6bd4\u72793.59 TOPS/W\u7684\u80fd\u6548\uff0c\u76f8\u6bd4\u540c\u7c7b\u67b6\u6784\u5728\u77e9\u9635\u4e58\u6cd5\u4efb\u52a1\u4e0a\u80fd\u6548\u63d0\u5347\u6570\u4e2a\u6570\u91cf\u7ea7\u3002", "conclusion": "DISCA\u517c\u5177\u6a21\u62df\u8ba1\u7b97\u7684\u7b80\u6d01\u6027\u548c\u6570\u5b57\u7cfb\u7edf\u7684\u53ef\u6269\u5c55\u6027\u4e0e\u53ef\u9760\u6027\uff0c\u662f\u9762\u5411\u8fb9\u7f18AI\u9ad8\u6548\u80fd\u786c\u4ef6\u7684\u6709\u529b\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2511.16947", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2511.16947", "abs": "https://arxiv.org/abs/2511.16947", "authors": ["Chenqi Zhao", "Wenfei Wu", "Linhai Song", "Yuchen Xu"], "title": "MicroMoE: Fine-Grained Load Balancing for Mixture-of-Experts with Token Scheduling", "comment": "19 pages", "summary": "Mixture-of-Experts (MoE) has emerged as a promising approach to scale up deep learning models due to its significant reduction in computational resources. However, the dynamic nature of MoE leads to load imbalance among experts, severely impacting training efficiency. While previous research has attempted to address the load balancing challenge, existing solutions either compromise model accuracy or introduce additional system overhead. As a result, they fail to achieve fine-grained load balancing, which is crucial to optimizing training efficiency.\n  We propose MicroEP, a novel parallelization strategy to achieve fine-grained load balancing in MoE systems. MicroEP is capable of achieving optimal load balancing in every micro-batch through efficient token scheduling across GPUs. Furthermore, we propose MicroMoE, an efficient distributed MoE training system with MicroEP's load balancing capabilities. Our experimental results demonstrate that MicroMoE improves the end-to-end training throughput by up to 47.6% compared with the state-of-the-art system, and almost consistently achieves optimal load balance among GPUs.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86MicroEP\u5e76\u884c\u7b56\u7565\u548c\u57fa\u4e8e\u5176\u7684MicroMoE\u7cfb\u7edf\uff0c\u901a\u8fc7\u9ad8\u6548\u7684\u8de8GPU\u4ee4\u724c\u8c03\u5ea6\uff0c\u5728\u6bcf\u4e2a\u5fae\u6279\u6b21\u4e2d\u5b9e\u73b0\u7ec6\u7c92\u5ea6\u8d1f\u8f7d\u5747\u8861\uff0c\u663e\u8457\u63d0\u5347MoE\u6a21\u578b\u8bad\u7ec3\u541e\u5410\u91cf\u3002", "motivation": "Mixture-of-Experts\uff08MoE\uff09\u6a21\u578b\u867d\u80fd\u964d\u4f4e\u8ba1\u7b97\u8d44\u6e90\u6d88\u8017\uff0c\u4f46\u5176\u52a8\u6001\u7279\u6027\u5bfc\u81f4\u4e13\u5bb6\u95f4\u8d1f\u8f7d\u4e0d\u5747\u8861\uff0c\u4e25\u91cd\u5f71\u54cd\u8bad\u7ec3\u6548\u7387\uff1b\u73b0\u6709\u65b9\u6cd5\u6216\u727a\u7272\u7cbe\u5ea6\uff0c\u6216\u5f15\u5165\u989d\u5916\u5f00\u9500\uff0c\u96be\u4ee5\u5b9e\u73b0\u7ec6\u7c92\u5ea6\u8d1f\u8f7d\u5747\u8861\u3002", "method": "\u63d0\u51faMicroEP\u5e76\u884c\u7b56\u7565\uff0c\u901a\u8fc7\u5728GPU\u4e4b\u95f4\u9ad8\u6548\u8c03\u5ea6token\uff0c\u5b9e\u73b0\u5728\u6bcf\u4e2a\u5fae\u6279\u6b21\u4e2d\u7684\u6700\u4f18\u8d1f\u8f7d\u5747\u8861\uff1b\u5e76\u5728\u6b64\u57fa\u7840\u4e0a\u6784\u5efa\u4e86\u5206\u5e03\u5f0fMoE\u8bad\u7ec3\u7cfb\u7edfMicroMoE\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cMicroMoE\u76f8\u6bd4\u5f53\u524d\u6700\u5148\u8fdb\u7684\u7cfb\u7edf\uff0c\u7aef\u5230\u7aef\u8bad\u7ec3\u541e\u5410\u91cf\u6700\u9ad8\u63d0\u534747.6%\uff0c\u5e76\u5728GPU\u95f4\u51e0\u4e4e\u59cb\u7ec8\u5b9e\u73b0\u6700\u4f18\u8d1f\u8f7d\u5747\u8861\u3002", "conclusion": "MicroEP\u7b56\u7565\u548cMicroMoE\u7cfb\u7edf\u6709\u6548\u89e3\u51b3\u4e86MoE\u8bad\u7ec3\u4e2d\u7684\u7ec6\u7c92\u5ea6\u8d1f\u8f7d\u5747\u8861\u95ee\u9898\uff0c\u5728\u4e0d\u727a\u7272\u6a21\u578b\u7cbe\u5ea6\u7684\u524d\u63d0\u4e0b\u663e\u8457\u63d0\u5347\u4e86\u8bad\u7ec3\u6548\u7387\u3002"}}
{"id": "2511.16964", "categories": ["cs.MA", "cs.AI", "cs.DC"], "pdf": "https://arxiv.org/pdf/2511.16964", "abs": "https://arxiv.org/abs/2511.16964", "authors": ["Kirill Nagaitsev", "Luka Grbcic", "Samuel Williams", "Costin Iancu"], "title": "Optimizing PyTorch Inference with LLM-Based Multi-Agent Systems", "comment": null, "summary": "Maximizing performance on available GPU hardware is an ongoing challenge for modern AI inference systems. Traditional approaches include writing custom GPU kernels and using specialized model compilers to tune high-level code for specific GPU targets. Recent work shows that LLM-based multi-agent systems can effectively perform such tuning, often outperforming existing compilers and eliminating the need for manual kernel development. However, the dynamics of multi-agent systems for this task remain unexplored. In this work, we present a logical framework for comparing multi-agent PyTorch optimization systems. Our evaluation shows that exploit-heavy strategies perform best when paired with error-fixing agents, and that performance correlates with the granularity of optimization steps. The best implementation achieves an average 2.88x speedup on an H100 GPU across diverse tasks in KernelBench, a benchmark suite covering a range of machine learning architectures in PyTorch.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u6bd4\u8f83\u591a\u667a\u80fd\u4f53PyTorch\u4f18\u5316\u7cfb\u7edf\u7684\u903b\u8f91\u6846\u67b6\uff0c\u53d1\u73b0\u4ee5\u63a2\u7d22\u4e3a\u4e3b\u7684\u7b56\u7565\u914d\u5408\u9519\u8bef\u4fee\u590d\u667a\u80fd\u4f53\u6548\u679c\u6700\u4f73\uff0c\u4e14\u4f18\u5316\u6b65\u9aa4\u7684\u7c92\u5ea6\u4e0e\u6027\u80fd\u76f8\u5173\uff1b\u6700\u4f18\u5b9e\u73b0\u5728H100 GPU\u4e0a\u4e8eKernelBench\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5e73\u5747\u63d0\u901f2.88\u500d\u3002", "motivation": "\u4f20\u7edfAI\u63a8\u7406\u7cfb\u7edf\u5728GPU\u786c\u4ef6\u4e0a\u7684\u6027\u80fd\u4f18\u5316\u4f9d\u8d56\u4e8e\u624b\u5de5\u7f16\u5199\u5185\u6838\u6216\u4e13\u7528\u7f16\u8bd1\u5668\uff0c\u800c\u8fd1\u671f\u7814\u7a76\u8868\u660e\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u53ef\u6709\u6548\u5b8c\u6210\u6b64\u7c7b\u8c03\u4f18\u4efb\u52a1\u3002\u7136\u800c\uff0c\u8fd9\u7c7b\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u5728\u6b64\u4efb\u52a1\u4e2d\u7684\u8fd0\u884c\u673a\u5236\u5c1a\u4e0d\u660e\u786e\uff0c\u4e9f\u9700\u7cfb\u7edf\u6027\u5206\u6790\u3002", "method": "\u6784\u5efa\u4e00\u4e2a\u903b\u8f91\u6846\u67b6\u7528\u4e8e\u6bd4\u8f83\u4e0d\u540c\u591a\u667a\u80fd\u4f53PyTorch\u4f18\u5316\u7cfb\u7edf\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u8bc4\u4f30\u4e0d\u540c\u7b56\u7565\uff08\u5982\u504f\u5411\u5229\u7528\u6216\u63a2\u7d22\uff09\u4e0e\u9519\u8bef\u4fee\u590d\u667a\u80fd\u4f53\u7684\u7ec4\u5408\u6548\u679c\uff0c\u540c\u65f6\u8003\u5bdf\u4f18\u5316\u6b65\u9aa4\u7c92\u5ea6\u5bf9\u6027\u80fd\u7684\u5f71\u54cd\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u504f\u5411\u5229\u7528\uff08exploit-heavy\uff09\u7684\u7b56\u7565\u5728\u642d\u914d\u9519\u8bef\u4fee\u590d\u667a\u80fd\u4f53\u65f6\u8868\u73b0\u6700\u4f73\uff0c\u4e14\u4f18\u5316\u6027\u80fd\u4e0e\u4f18\u5316\u6b65\u9aa4\u7684\u7c92\u5ea6\u5bc6\u5207\u76f8\u5173\uff1b\u6700\u4f18\u5b9e\u73b0\u65b9\u6848\u5728H100 GPU\u4e0a\u5bf9KernelBench\u4e2d\u6db5\u76d6\u591a\u79cdPyTorch\u6a21\u578b\u67b6\u6784\u7684\u4efb\u52a1\u5e73\u5747\u5b9e\u73b0\u4e862.88\u500d\u7684\u52a0\u901f\u3002", "conclusion": "\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u5728GPU\u4ee3\u7801\u4f18\u5316\u65b9\u9762\u5177\u6709\u663e\u8457\u6f5c\u529b\uff0c\u5176\u6027\u80fd\u53d7\u7b56\u7565\u7c7b\u578b\u548c\u4f18\u5316\u7c92\u5ea6\u5f71\u54cd\uff0c\u5408\u7406\u8bbe\u8ba1\u667a\u80fd\u4f53\u534f\u4f5c\u673a\u5236\u53ef\u6709\u6548\u63d0\u5347AI\u63a8\u7406\u6548\u7387\uff0c\u51cf\u5c11\u5bf9\u624b\u5de5\u8c03\u4f18\u6216\u4e13\u7528\u7f16\u8bd1\u5668\u7684\u4f9d\u8d56\u3002"}}
{"id": "2511.17119", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2511.17119", "abs": "https://arxiv.org/abs/2511.17119", "authors": ["Gabriel Job Antunes Grabher", "Fumio Machida", "Thomas Ropars"], "title": "Modeling Anomaly Detection in Cloud Services: Analysis of the Properties that Impact Latency and Resource Consumption", "comment": null, "summary": "Detecting and resolving performance anomalies in Cloud services is crucial for maintaining desired performance objectives. Scaling actions triggered by an anomaly detector help achieve target latency at the cost of extra resource consumption. However, performance anomaly detectors make mistakes. This paper studies which characteristics of performance anomaly detection are important to optimize the trade-off between performance and cost. Using Stochastic Reward Nets, we model a Cloud service monitored by a performance anomaly detector. Using our model, we study the impact of detector characteristics, namely precision, recall and inspection frequency, on the average latency and resource consumption of the monitored service. Our results show that achieving a high precision and a high recall is not always necessary. If detection can be run frequently, a high precision is enough to obtain a good performance-to-cost trade-off, but if the detector is run infrequently, recall becomes the most important.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u4e91\u670d\u52a1\u4e2d\u6027\u80fd\u5f02\u5e38\u68c0\u6d4b\u5668\u7684\u5173\u952e\u7279\u6027\uff08\u5982\u7cbe\u786e\u7387\u3001\u53ec\u56de\u7387\u548c\u68c0\u6d4b\u9891\u7387\uff09\u5982\u4f55\u5f71\u54cd\u6027\u80fd\u4e0e\u6210\u672c\u4e4b\u95f4\u7684\u6743\u8861\uff0c\u5e76\u53d1\u73b0\u9ad8\u9891\u68c0\u6d4b\u4e0b\u9ad8\u7cbe\u786e\u7387\u5df2\u8db3\u591f\uff0c\u800c\u4f4e\u9891\u68c0\u6d4b\u65f6\u9ad8\u53ec\u56de\u7387\u66f4\u4e3a\u5173\u952e\u3002", "motivation": "\u5728\u4e91\u670d\u52a1\u4e2d\uff0c\u5f02\u5e38\u68c0\u6d4b\u867d\u6709\u52a9\u4e8e\u7ef4\u6301\u6027\u80fd\u76ee\u6807\uff0c\u4f46\u5176\u8bef\u5224\u4f1a\u5bfc\u81f4\u4e0d\u5fc5\u8981\u7684\u8d44\u6e90\u5f00\u9500\u3002\u56e0\u6b64\uff0c\u6709\u5fc5\u8981\u660e\u786e\u54ea\u4e9b\u68c0\u6d4b\u5668\u7279\u6027\u5bf9\u4f18\u5316\u6027\u80fd\u4e0e\u6210\u672c\u7684\u6743\u8861\u6700\u4e3a\u5173\u952e\u3002", "method": "\u4f5c\u8005\u4f7f\u7528\u968f\u673a\u5956\u52b1\u7f51\uff08Stochastic Reward Nets\uff09\u5bf9\u7531\u6027\u80fd\u5f02\u5e38\u68c0\u6d4b\u5668\u76d1\u63a7\u7684\u4e91\u670d\u52a1\u8fdb\u884c\u5efa\u6a21\uff0c\u5e76\u5206\u6790\u68c0\u6d4b\u5668\u7684\u7cbe\u786e\u7387\u3001\u53ec\u56de\u7387\u548c\u68c0\u6d4b\u9891\u7387\u5bf9\u670d\u52a1\u5e73\u5747\u5ef6\u8fdf\u548c\u8d44\u6e90\u6d88\u8017\u7684\u5f71\u54cd\u3002", "result": "\u7814\u7a76\u8868\u660e\uff0c\u5e76\u975e\u603b\u662f\u9700\u8981\u540c\u65f6\u5b9e\u73b0\u9ad8\u7cbe\u786e\u7387\u548c\u9ad8\u53ec\u56de\u7387\uff1a\u82e5\u68c0\u6d4b\u9891\u7387\u9ad8\uff0c\u4ec5\u9ad8\u7cbe\u786e\u7387\u5373\u53ef\u83b7\u5f97\u826f\u597d\u7684\u6027\u80fd-\u6210\u672c\u5e73\u8861\uff1b\u82e5\u68c0\u6d4b\u9891\u7387\u4f4e\uff0c\u5219\u9ad8\u53ec\u56de\u7387\u66f4\u4e3a\u91cd\u8981\u3002", "conclusion": "\u6027\u80fd\u5f02\u5e38\u68c0\u6d4b\u5668\u7684\u8bbe\u8ba1\u5e94\u6839\u636e\u5176\u8fd0\u884c\u9891\u7387\u6743\u8861\u7cbe\u786e\u7387\u4e0e\u53ec\u56de\u7387\uff0c\u4ee5\u5b9e\u73b0\u6700\u4f18\u7684\u6027\u80fd\u4e0e\u8d44\u6e90\u6210\u672c\u5e73\u8861\u3002"}}
{"id": "2511.16708", "categories": ["cs.SE", "cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2511.16708", "abs": "https://arxiv.org/abs/2511.16708", "authors": ["Shreshth Rajan"], "title": "Multi-Agent Code Verification with Compound Vulnerability Detection", "comment": "18 pages, 3 figures, 9 tables", "summary": "LLMs generate buggy code: 29.6% of SWE-bench \"solved\" patches fail, 62% of BaxBench solutions have vulnerabilities, and existing tools only catch 65% of bugs with 35% false positives. We built CodeX-Verify, a multi-agent system that uses four specialized agents to detect different types of bugs. We prove mathematically that combining agents with different detection patterns finds more bugs than any single agent when the agents look for different problems, confirmed by measuring agent correlation of p = 0.05--0.25. We also show that multiple vulnerabilities in the same code create exponentially more risk than previously thought--SQL injection plus exposed credentials creates 15x more danger (risk 300 vs. 20) than traditional models predict. Testing on 99 code samples with verified labels shows our system catches 76.1% of bugs, matching the best existing method while running faster and without test execution. We tested 15 different agent combinations and found that using multiple agents improves accuracy by 39.7 percentage points (from 32.8% to 72.4%) compared to single agents, with gains of +14.9pp, +13.5pp, and +11.2pp for agents 2, 3, and 4. The best two-agent combination reaches 79.3% accuracy. Testing on 300 real patches from Claude Sonnet 4.5 runs in under 200ms per sample, making this practical for production use.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faCodeX-Verify\uff0c\u4e00\u79cd\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\uff0c\u901a\u8fc7\u56db\u4e2a\u4e13\u95e8\u7684\u667a\u80fd\u4f53\u534f\u540c\u68c0\u6d4b\u4ee3\u7801\u4e2d\u7684\u5404\u7c7b\u7f3a\u9677\uff0c\u5728\u4e0d\u4f9d\u8d56\u6d4b\u8bd5\u6267\u884c\u7684\u60c5\u51b5\u4e0b\u4ee5\u66f4\u5feb\u901f\u5ea6\u8fbe\u5230\u4e0e\u73b0\u6709\u6700\u4f73\u65b9\u6cd5\u76f8\u5f53\u7684\u67e5\u5168\u7387\uff0876.1%\uff09\uff0c\u5e76\u63ed\u793a\u591a\u6f0f\u6d1e\u53e0\u52a0\u5e26\u6765\u7684\u98ce\u9669\u8fdc\u8d85\u4f20\u7edf\u6a21\u578b\u9884\u6d4b\u3002", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b\u751f\u6210\u7684\u4ee3\u7801\u5b58\u5728\u663e\u8457\u7f3a\u9677\uff1aSWE-bench\u4e2d29.6%\u7684\u201c\u5df2\u89e3\u51b3\u201d\u8865\u4e01\u5b9e\u9645\u5931\u8d25\uff0cBaxBench\u4e2d62%\u7684\u89e3\u51b3\u65b9\u6848\u5305\u542b\u6f0f\u6d1e\uff0c\u800c\u73b0\u6709\u5de5\u5177\u4ec5\u80fd\u6355\u83b765%\u7684\u7f3a\u9677\u4e14\u8bef\u62a5\u7387\u8fbe35%\u3002\u56e0\u6b64\u4e9f\u9700\u66f4\u9ad8\u6548\u3001\u51c6\u786e\u7684\u81ea\u52a8\u9a8c\u8bc1\u673a\u5236\u3002", "method": "\u6784\u5efa\u540d\u4e3aCodeX-Verify\u7684\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\uff0c\u5305\u542b\u56db\u4e2a\u4e13\u7cbe\u4e8e\u4e0d\u540c\u7c7b\u578b\u7f3a\u9677\u68c0\u6d4b\u7684\u667a\u80fd\u4f53\uff1b\u901a\u8fc7\u6570\u5b66\u8bc1\u660e\u548c\u5b9e\u8bc1\u5206\u6790\uff08\u667a\u80fd\u4f53\u95f4\u76f8\u5173\u6027p=0.05\u20130.25\uff09\u9a8c\u8bc1\u591a\u667a\u80fd\u4f53\u7ec4\u5408\u7684\u6709\u6548\u6027\uff1b\u572899\u4e2a\u5e26\u6807\u7b7e\u6837\u672c\u548c300\u4e2a\u771f\u5b9e\u8865\u4e01\u4e0a\u8bc4\u4f30\u6027\u80fd\u3002", "result": "\u7cfb\u7edf\u572899\u4e2a\u6837\u672c\u4e0a\u68c0\u51fa76.1%\u7684\u7f3a\u9677\uff0c\u4f18\u4e8e\u5355\u4e00\u667a\u80fd\u4f53\uff08\u6700\u9ad8\u63d0\u534739.7\u4e2a\u767e\u5206\u70b9\uff09\uff0c\u6700\u4f73\u53cc\u667a\u80fd\u4f53\u7ec4\u5408\u8fbe79.3%\u51c6\u786e\u7387\uff1b\u5904\u7406Claude Sonnet 4.5\u751f\u6210\u7684\u8865\u4e01\u5e73\u5747\u8017\u65f6\u4f4e\u4e8e200ms\uff1b\u53d1\u73b0\u591a\u6f0f\u6d1e\uff08\u5982SQL\u6ce8\u5165+\u51ed\u8bc1\u6cc4\u9732\uff09\u53e0\u52a0\u98ce\u9669\u8fbe\u4f20\u7edf\u6a21\u578b\u9884\u6d4b\u768415\u500d\u3002", "conclusion": "\u591a\u667a\u80fd\u4f53\u534f\u540c\u9a8c\u8bc1\u80fd\u663e\u8457\u63d0\u5347LLM\u751f\u6210\u4ee3\u7801\u7684\u7f3a\u9677\u68c0\u6d4b\u6548\u679c\uff0c\u517c\u5177\u9ad8\u51c6\u786e\u7387\u3001\u4f4e\u5ef6\u8fdf\u548c\u65e0\u9700\u6267\u884c\u6d4b\u8bd5\u7684\u4f18\u52bf\uff0c\u9002\u7528\u4e8e\u751f\u4ea7\u73af\u5883\uff0c\u5e76\u63ed\u793a\u4e86\u590d\u5408\u6f0f\u6d1e\u98ce\u9669\u88ab\u4e25\u91cd\u4f4e\u4f30\u7684\u95ee\u9898\u3002"}}
{"id": "2511.17235", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2511.17235", "abs": "https://arxiv.org/abs/2511.17235", "authors": ["Rohit Prasad"], "title": "NX-CGRA: A Programmable Hardware Accelerator for Core Transformer Algorithms on Edge Devices", "comment": "This paper has been accepted for publication at the Design, Automation and Test in Europe (DATE) Conference 2026. 2026 IEEE. Personal use of this material is permitted", "summary": "The increasing diversity and complexity of transformer workloads at the edge present significant challenges in balancing performance, energy efficiency, and architectural flexibility. This paper introduces NX-CGRA, a programmable hardware accelerator designed to support a range of transformer inference algorithms, including both linear and non-linear functions. Unlike fixed-function accelerators optimized for narrow use cases, NX-CGRA employs a coarse-grained reconfigurable array (CGRA) architecture with software-driven programmability, enabling efficient execution across varied kernel patterns. The architecture is evaluated using representative benchmarks derived from real-world transformer models, demonstrating high overall efficiency and favorable energy-area tradeoffs across different classes of operations. These results indicate the potential of NX-CGRA as a scalable and adaptable hardware solution for edge transformer deployment under constrained power and silicon budgets.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86NX-CGRA\uff0c\u4e00\u79cd\u9762\u5411\u8fb9\u7f18\u8bbe\u5907\u7684\u53ef\u7f16\u7a0b\u786c\u4ef6\u52a0\u901f\u5668\uff0c\u5229\u7528\u7c97\u7c92\u5ea6\u53ef\u91cd\u6784\u9635\u5217\uff08CGRA\uff09\u67b6\u6784\u9ad8\u6548\u652f\u6301\u591a\u79cdTransformer\u63a8\u7406\u7b97\u6cd5\uff0c\u5728\u6027\u80fd\u3001\u80fd\u6548\u548c\u7075\u6d3b\u6027\u4e4b\u95f4\u53d6\u5f97\u826f\u597d\u5e73\u8861\u3002", "motivation": "\u8fb9\u7f18\u7aefTransformer\u5de5\u4f5c\u8d1f\u8f7d\u65e5\u76ca\u591a\u6837\u5316\u548c\u590d\u6742\u5316\uff0c\u5bf9\u6027\u80fd\u3001\u80fd\u6548\u548c\u67b6\u6784\u7075\u6d3b\u6027\u63d0\u51fa\u4e86\u66f4\u9ad8\u8981\u6c42\uff0c\u73b0\u6709\u56fa\u5b9a\u529f\u80fd\u52a0\u901f\u5668\u96be\u4ee5\u9002\u5e94\u5e7f\u6cdb\u7684\u5e94\u7528\u573a\u666f\u3002", "method": "\u91c7\u7528\u8f6f\u4ef6\u9a71\u52a8\u7684CGRA\u67b6\u6784\uff0c\u901a\u8fc7\u53ef\u91cd\u6784\u786c\u4ef6\u652f\u6301\u7ebf\u6027\u548c\u975e\u7ebf\u6027Transformer\u63a8\u7406\u64cd\u4f5c\uff0c\u5b9e\u73b0\u5bf9\u4e0d\u540c\u7b97\u5b50\u6a21\u5f0f\u7684\u9ad8\u6548\u6267\u884c\u3002", "result": "\u5728\u771f\u5b9eTransformer\u6a21\u578b\u884d\u751f\u7684\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cNX-CGRA\u5c55\u73b0\u51fa\u9ad8\u6574\u4f53\u6548\u7387\u548c\u4f18\u8d8a\u7684\u80fd\u6548-\u9762\u79ef\u6743\u8861\u8868\u73b0\u3002", "conclusion": "NX-CGRA\u662f\u4e00\u79cd\u53ef\u6269\u5c55\u4e14\u9002\u5e94\u6027\u5f3a\u7684\u786c\u4ef6\u89e3\u51b3\u65b9\u6848\uff0c\u9002\u7528\u4e8e\u8d44\u6e90\u53d7\u9650\u7684\u8fb9\u7f18\u8bbe\u5907\u90e8\u7f72Transformer\u6a21\u578b\u3002"}}
{"id": "2511.16858", "categories": ["cs.SE", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.16858", "abs": "https://arxiv.org/abs/2511.16858", "authors": ["Toufique Ahmed", "Jatin Ganhotra", "Avraham Shinnar", "Martin Hirzel"], "title": "Is the Cure Still Worse Than the Disease? Test Overfitting by LLMs in Automated Program Repair", "comment": null, "summary": "Automated program repair has been shown to be susceptible to generating repaired code that passes on seen tests but fails on a hold-out set of hidden tests. This problem, dubbed test overfitting, has been identified and studied before the rise of large language models. We experimentally study how much test overfitting is still a problem today, using repository-level SWE-bench tasks.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u5728\u5f53\u524d\u5927\u578b\u8bed\u8a00\u6a21\u578b\u65f6\u4ee3\uff0c\u81ea\u52a8\u5316\u7a0b\u5e8f\u4fee\u590d\u4e2d\u6d4b\u8bd5\u8fc7\u62df\u5408\u95ee\u9898\u662f\u5426\u4ecd\u7136\u5b58\u5728\uff0c\u4f7f\u7528SWE-bench\u4ed3\u5e93\u7ea7\u4efb\u52a1\u8fdb\u884c\u5b9e\u9a8c\u5206\u6790\u3002", "motivation": "\u4ee5\u5f80\u7814\u7a76\u8868\u660e\uff0c\u81ea\u52a8\u5316\u7a0b\u5e8f\u4fee\u590d\u5bb9\u6613\u4ea7\u751f\u4ec5\u901a\u8fc7\u5df2\u77e5\u6d4b\u8bd5\u4f46\u65e0\u6cd5\u901a\u8fc7\u9690\u85cf\u6d4b\u8bd5\u7684\u4fee\u590d\u4ee3\u7801\uff08\u5373\u6d4b\u8bd5\u8fc7\u62df\u5408\u95ee\u9898\uff09\u3002\u968f\u7740\u5927\u8bed\u8a00\u6a21\u578b\u7684\u53d1\u5c55\uff0c\u6709\u5fc5\u8981\u91cd\u65b0\u8bc4\u4f30\u8be5\u95ee\u9898\u662f\u5426\u4f9d\u7136\u663e\u8457\u3002", "method": "\u901a\u8fc7\u5728SWE-bench\u7684\u4ed3\u5e93\u7ea7\u4efb\u52a1\u4e0a\u8fdb\u884c\u5b9e\u9a8c\uff0c\u8bc4\u4f30\u5f53\u524d\u81ea\u52a8\u5316\u7a0b\u5e8f\u4fee\u590d\u65b9\u6cd5\u5728\u6d4b\u8bd5\u8fc7\u62df\u5408\u65b9\u9762\u7684\u8868\u73b0\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u6d4b\u8bd5\u8fc7\u62df\u5408\u95ee\u9898\u5728\u5f53\u524d\u7684\u81ea\u52a8\u5316\u7a0b\u5e8f\u4fee\u590d\u5b9e\u8df5\u4e2d\u4ecd\u7136\u666e\u904d\u5b58\u5728\u3002", "conclusion": "\u5c3d\u7ba1\u5927\u8bed\u8a00\u6a21\u578b\u5e26\u6765\u4e86\u65b0\u7684\u4fee\u590d\u80fd\u529b\uff0c\u6d4b\u8bd5\u8fc7\u62df\u5408\u4ecd\u662f\u81ea\u52a8\u5316\u7a0b\u5e8f\u4fee\u590d\u4e2d\u7684\u5173\u952e\u6311\u6218\uff0c\u9700\u8fdb\u4e00\u6b65\u7814\u7a76\u66f4\u9c81\u68d2\u7684\u8bc4\u4f30\u4e0e\u4fee\u590d\u7b56\u7565\u3002"}}
{"id": "2511.16966", "categories": ["cs.NI"], "pdf": "https://arxiv.org/pdf/2511.16966", "abs": "https://arxiv.org/abs/2511.16966", "authors": ["Yiheng Bian", "Zechen Li", "Lanqing Yang", "Hao Pan", "Yezhou Wang", "Longyuan Ge", "Jeffery Wu", "Ruiheng Liu", "Yongjian Fu", "Yichao chen", "Guangtao xue"], "title": "One Walk is All You Need: Data-Efficient 3D RF Scene Reconstruction with Human Movements", "comment": null, "summary": "Reconstructing 3D Radiance Field (RF) scenes through opaque obstacles is a long-standing goal, yet it is fundamentally constrained by a laborious data acquisition process requiring thousands of static measurements, which treats human motion as noise to be filtered. This work introduces a new paradigm with a core objective: to perform fast, data-efficient, and high-fidelity RF reconstruction of occluded 3D static scenes, using only a single, brief human walk. We argue that this unstructured motion is not noise, but is in fact an information-rich signal available for reconstruction. To achieve this, we design a factorization framework based on composite 3D Gaussian Splatting (3DGS) that learns to model the dynamic effects of human motion from the persistent static scene geometry within a raw RF stream. Trained on just a single 60-second casual walk, our model reconstructs the full static scene with a Structural Similarity Index (SSIM) of 0.96, remarkably outperforming heavily-sampled state-of-the-art (SOTA) by 12%. By transforming the human movements into its valuable signals, our method eliminates the data acquisition bottleneck and paves the way for on-the-fly 3D RF mapping of unseen environments.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u65b0\u65b9\u6cd5\uff0c\u5229\u7528\u5355\u6b21\u77ed\u6682\u7684\u4eba\u7c7b\u884c\u8d70\u89c6\u9891\u5373\u53ef\u9ad8\u6548\u3001\u9ad8\u4fdd\u771f\u5730\u91cd\u5efa\u88ab\u906e\u6321\u7684\u9759\u60013D\u8f90\u5c04\u573a\u573a\u666f\uff0c\u5c06\u4eba\u4f53\u8fd0\u52a8\u89c6\u4e3a\u6709\u7528\u4fe1\u53f7\u800c\u975e\u566a\u58f0\uff0c\u5e76\u57fa\u4e8e\u590d\u54083D\u9ad8\u65af\u6cfc\u6e85\u5b9e\u73b0\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u9700\u8981\u5927\u91cf\u9759\u6001\u91c7\u6837\u7684SOTA\u65b9\u6cd5\u3002", "motivation": "\u4f20\u7edf3D\u8f90\u5c04\u573a\u91cd\u5efa\u4f9d\u8d56\u5927\u91cf\u9759\u6001\u6d4b\u91cf\uff0c\u5c06\u4eba\u4f53\u8fd0\u52a8\u89c6\u4e3a\u9700\u6ee4\u9664\u7684\u566a\u58f0\uff0c\u5bfc\u81f4\u6570\u636e\u91c7\u96c6\u8fc7\u7a0b\u7e41\u7410\u4f4e\u6548\uff1b\u672c\u6587\u65e8\u5728\u7a81\u7834\u8fd9\u4e00\u74f6\u9888\uff0c\u63a2\u7d22\u5982\u4f55\u5229\u7528\u975e\u7ed3\u6784\u5316\u7684\u4eba\u4f53\u8fd0\u52a8\u4f5c\u4e3a\u4fe1\u606f\u6e90\uff0c\u5b9e\u73b0\u5feb\u901f\u3001\u6570\u636e\u9ad8\u6548\u7684\u9ad8\u8d28\u91cf\u91cd\u5efa\u3002", "method": "\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u57fa\u4e8e\u590d\u54083D\u9ad8\u65af\u6cfc\u6e85\uff083DGS\uff09\u7684\u56e0\u5b50\u5206\u89e3\u6846\u67b6\uff0c\u4ece\u539f\u59cb\u5c04\u9891\u6d41\u4e2d\u8054\u5408\u5b66\u4e60\u52a8\u6001\u4eba\u4f53\u8fd0\u52a8\u6548\u5e94\u4e0e\u9759\u6001\u573a\u666f\u51e0\u4f55\u7ed3\u6784\uff0c\u4ec5\u9700\u5355\u6b2160\u79d2\u968f\u610f\u884c\u8d70\u7684\u6570\u636e\u8fdb\u884c\u8bad\u7ec3\u3002", "result": "\u5728\u4ec5\u4f7f\u7528\u4e00\u6b2160\u79d2\u4eba\u7c7b\u884c\u8d70\u6570\u636e\u7684\u60c5\u51b5\u4e0b\uff0c\u6a21\u578b\u91cd\u5efa\u9759\u6001\u573a\u666f\u7684SSIM\u8fbe\u52300.96\uff0c\u6bd4\u5f53\u524d\u6700\u5148\u8fdb\u7684\u91cd\u5ea6\u91c7\u6837\u65b9\u6cd5\u9ad8\u51fa12%\u3002", "conclusion": "\u5c06\u4eba\u4f53\u8fd0\u52a8\u8f6c\u5316\u4e3a\u91cd\u5efa\u4fe1\u53f7\u53ef\u6709\u6548\u6d88\u9664\u4f20\u7edf\u6570\u636e\u91c7\u96c6\u74f6\u9888\uff0c\u4e3a\u5b9e\u65f6\u3001\u5373\u5174\u7684\u672a\u77e5\u73af\u58833D\u5c04\u9891\u5efa\u56fe\u5f00\u8f9f\u4e86\u65b0\u8def\u5f84\u3002"}}
{"id": "2511.16882", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2511.16882", "abs": "https://arxiv.org/abs/2511.16882", "authors": ["Tim Menzies", "Tao Chen", "Yulong Ye", "Kishan Kumar Ganguly", "Amirali Rayegan", "Srinath Srinivasan", "Andre Lustosa"], "title": "MOOT: a Repository of Many Multi-Objective Optimization Tasks", "comment": null, "summary": "Software engineers must make decisions that trade off competing goals (faster vs. cheaper, secure vs. usable, accurate vs. interpretable, etc.). Despite MSR's proven techniques for exploring such goals, researchers still struggle with these trade-offs. Similarly, industrial practitioners deliver sub-optimal products since they lack the tools needed to explore these trade-offs.\n  To enable more research in this important area, we introduce MOOT, a repository of multi-objective optimization tasks taken from recent SE research papers. MOOT's tasks cover software configuration, cloud tuning, project health, process modeling, hyperparameter optimization, and more. Located at github.com/timm/moot, MOOT's current 120+ tasks are freely available under an MIT license (and we invite community contributions). As shown here, this data enables dozens of novel research questions.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86MOOT\uff0c\u4e00\u4e2a\u5305\u542b120\u591a\u4e2a\u6765\u81ea\u8f6f\u4ef6\u5de5\u7a0b\u7814\u7a76\u7684\u591a\u76ee\u6807\u4f18\u5316\u4efb\u52a1\u7684\u5f00\u6e90\u4ed3\u5e93\uff0c\u65e8\u5728\u652f\u6301\u5bf9\u8f6f\u4ef6\u5de5\u7a0b\u4e2d\u591a\u76ee\u6807\u6743\u8861\u95ee\u9898\u7684\u7814\u7a76\u3002", "motivation": "\u8f6f\u4ef6\u5de5\u7a0b\u5e08\u5e38\u9700\u5728\u76f8\u4e92\u51b2\u7a81\u7684\u76ee\u6807\u4e4b\u95f4\u505a\u6743\u8861\uff08\u5982\u901f\u5ea6\u4e0e\u6210\u672c\u3001\u5b89\u5168\u6027\u4e0e\u53ef\u7528\u6027\u7b49\uff09\uff0c\u4f46\u73b0\u6709\u7814\u7a76\u548c\u5de5\u4e1a\u5b9e\u8df5\u7f3a\u4e4f\u6709\u6548\u5de5\u5177\u6765\u63a2\u7d22\u8fd9\u4e9b\u6743\u8861\uff0c\u9650\u5236\u4e86\u6700\u4f18\u51b3\u7b56\u7684\u5b9e\u73b0\u3002", "method": "\u6784\u5efa\u5e76\u53d1\u5e03MOOT\u4ed3\u5e93\uff0c\u6574\u5408\u6765\u81ea\u8fd1\u671f\u8f6f\u4ef6\u5de5\u7a0b\u8bba\u6587\u7684\u591a\u76ee\u6807\u4f18\u5316\u4efb\u52a1\uff0c\u6db5\u76d6\u8f6f\u4ef6\u914d\u7f6e\u3001\u4e91\u8c03\u4f18\u3001\u9879\u76ee\u5065\u5eb7\u7b49\u591a\u4e2a\u9886\u57df\uff0c\u5e76\u4ee5MIT\u8bb8\u53ef\u8bc1\u5f00\u653e\u83b7\u53d6\u3002", "result": "MOOT\u63d0\u4f9b\u4e86\u4e30\u5bcc\u7684\u6570\u636e\u96c6\uff0c\u652f\u6301\u6570\u5341\u4e2a\u65b0\u9896\u7684\u7814\u7a76\u95ee\u9898\uff0c\u5e76\u9f13\u52b1\u793e\u533a\u8d21\u732e\u4ee5\u6301\u7eed\u6269\u5c55\u4efb\u52a1\u5e93\u3002", "conclusion": "MOOT\u4e3a\u8f6f\u4ef6\u5de5\u7a0b\u4e2d\u7684\u591a\u76ee\u6807\u4f18\u5316\u7814\u7a76\u63d0\u4f9b\u4e86\u91cd\u8981\u57fa\u7840\u8bbe\u65bd\uff0c\u6709\u671b\u63a8\u52a8\u8be5\u9886\u57df\u7684\u8fdb\u4e00\u6b65\u53d1\u5c55\u3002"}}
{"id": "2511.17027", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2511.17027", "abs": "https://arxiv.org/abs/2511.17027", "authors": ["Zhijie Chen", "Xiang Chen", "Ziming Li", "Jiacheng Xue", "Chaoyang Gao"], "title": "ReVul-CoT: Towards Effective Software Vulnerability Assessment with Retrieval-Augmented Generation and Chain-of-Thought Prompting", "comment": null, "summary": "Context: Software Vulnerability Assessment (SVA) plays a vital role in evaluating and ranking vulnerabilities in software systems to ensure their security and reliability. Objective: Although Large Language Models (LLMs) have recently shown remarkable potential in SVA, they still face two major limitations. First, most LLMs are trained on general-purpose corpora and thus lack domain-specific knowledge essential for effective SVA. Second, they tend to rely on shallow pattern matching instead of deep contextual reasoning, making it challenging to fully comprehend complex code semantics and their security implications. Method: To alleviate these limitations, we propose a novel framework ReVul-CoT that integrates Retrieval-Augmented Generation (RAG) with Chain-of-Thought (COT) prompting. In ReVul-CoT, the RAG module dynamically retrieves contextually relevant information from a constructed local knowledge base that consolidates vulnerability data from authoritative sources (such as NVD and CWE), along with corresponding code snippets and descriptive information. Building on DeepSeek-V3.1, CoT prompting guides the LLM to perform step-by-step reasoning over exploitability, impact scope, and related factors Results: We evaluate ReVul-CoT on a dataset of 12,070 vulnerabilities. Experimental results show that ReVul-CoT outperforms state-of-the-art SVA baselines by 16.50%-42.26% in terms of MCC, and outperforms the best baseline by 10.43%, 15.86%, and 16.50% in Accuracy, F1-score, and MCC, respectively. Our ablation studies further validate the contributions of considering dynamic retrieval, knowledge integration, and CoT-based reasoning. Conclusion: Our results demonstrate that combining RAG with CoT prompting significantly enhances LLM-based SVA and points out promising directions for future research.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u4e0e\u601d\u7ef4\u94fe\uff08CoT\uff09\u63d0\u793a\u7684\u65b0\u6846\u67b6ReVul-CoT\uff0c\u7528\u4e8e\u63d0\u5347\u5927\u8bed\u8a00\u6a21\u578b\u5728\u8f6f\u4ef6\u6f0f\u6d1e\u8bc4\u4f30\uff08SVA\uff09\u4e2d\u7684\u8868\u73b0\u3002\u901a\u8fc7\u4ece\u672c\u5730\u77e5\u8bc6\u5e93\u52a8\u6001\u68c0\u7d22\u6743\u5a01\u6f0f\u6d1e\u4fe1\u606f\u5e76\u5f15\u5bfc\u6a21\u578b\u8fdb\u884c\u9010\u6b65\u63a8\u7406\uff0cReVul-CoT\u5728\u591a\u4e2a\u6307\u6807\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u5f53\u524d\u5927\u8bed\u8a00\u6a21\u578b\u5728\u8f6f\u4ef6\u6f0f\u6d1e\u8bc4\u4f30\u4e2d\u5b58\u5728\u4e24\u4e2a\u4e3b\u8981\u5c40\u9650\uff1a\u7f3a\u4e4f\u9886\u57df\u7279\u5b9a\u77e5\u8bc6\uff0c\u4ee5\u53ca\u4f9d\u8d56\u6d45\u5c42\u6a21\u5f0f\u5339\u914d\u800c\u975e\u6df1\u5c42\u4e0a\u4e0b\u6587\u63a8\u7406\uff0c\u96be\u4ee5\u51c6\u786e\u7406\u89e3\u590d\u6742\u4ee3\u7801\u8bed\u4e49\u53ca\u5176\u5b89\u5168\u5f71\u54cd\u3002", "method": "\u63d0\u51faReVul-CoT\u6846\u67b6\uff0c\u7ed3\u5408RAG\u4e0eCoT\u63d0\u793a\u3002RAG\u6a21\u5757\u4ece\u6574\u5408\u4e86NVD\u3001CWE\u7b49\u6743\u5a01\u6765\u6e90\u7684\u672c\u5730\u77e5\u8bc6\u5e93\u4e2d\u52a8\u6001\u68c0\u7d22\u76f8\u5173\u6f0f\u6d1e\u4fe1\u606f\u548c\u4ee3\u7801\u7247\u6bb5\uff1b\u57fa\u4e8eDeepSeek-V3.1\u6a21\u578b\uff0c\u5229\u7528CoT\u63d0\u793a\u5f15\u5bfc\u6a21\u578b\u5bf9\u53ef\u5229\u7528\u6027\u3001\u5f71\u54cd\u8303\u56f4\u7b49\u56e0\u7d20\u8fdb\u884c\u5206\u6b65\u63a8\u7406\u3002", "result": "\u5728\u5305\u542b12,070\u4e2a\u6f0f\u6d1e\u7684\u6570\u636e\u96c6\u4e0a\u8bc4\u4f30\uff0cReVul-CoT\u5728MCC\u6307\u6807\u4e0a\u6bd4\u73b0\u6709\u6700\u4f18\u57fa\u7ebf\u63d0\u534716.50%\u201342.26%\uff0c\u5728Accuracy\u3001F1-score\u548cMCC\u4e0a\u5206\u522b\u63d0\u534710.43%\u300115.86%\u548c16.50%\u3002\u6d88\u878d\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u52a8\u6001\u68c0\u7d22\u3001\u77e5\u8bc6\u878d\u5408\u4e0eCoT\u63a8\u7406\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u5c06RAG\u4e0eCoT\u63d0\u793a\u76f8\u7ed3\u5408\u80fd\u663e\u8457\u63d0\u5347\u5927\u8bed\u8a00\u6a21\u578b\u5728\u8f6f\u4ef6\u6f0f\u6d1e\u8bc4\u4f30\u4e2d\u7684\u6027\u80fd\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u6709\u524d\u666f\u7684\u65b9\u5411\u3002"}}
{"id": "2511.17131", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.17131", "abs": "https://arxiv.org/abs/2511.17131", "authors": ["Horia Cristescu", "Charles Park", "Trong Canh Nguyen", "Sergiu Talmacel", "Alexandru-Gabriel Ilie", "Stefan Adam"], "title": "UI-CUBE: Enterprise-Grade Computer Use Agent Benchmarking Beyond Task Accuracy to Operational Reliability", "comment": "18 pages, 8 figures, 5 tables. Benchmark comprising 226 tasks across two difficulty tiers. Code and benchmark available at https://github.com/UiPath/uipath_enterprise_benchmark", "summary": "While current Computer Use Agent (CUA) benchmarks measure task completion effectively, they provide limited assessment of enterprise deployment readiness, emphasizing functional correctness over the operational reliability required for production systems. We present UI-CUBE (UiPath Computer Use BEnchmark), a systematic benchmark comprising 226 tasks across two difficulty tiers designed to expose fundamental architectural limitations in current CUAs. Our evaluation covers simple UI interactions (136 tasks) and complex workflows including copy-paste tasks (50 tasks) and enterprise application scenarios (40 tasks), with systematic interface variation coverage, multi-resolution testing and automated validation of task success through the application state. Evaluation of five state-of-the-art models reveals a sharp capability cliff rather than gradual performance degradation. Simple UI interactions achieve 67-85% success rates (compared to 97.9% human performance), but complex workflows drop precipitously to 9-19%. Human evaluators with no prior application experience achieve only 61.2% on complex tasks despite near-perfect performance on simple tasks, establishing realistic performance ceilings. This discontinuous performance pattern -- where agents achieve 68-87% of human performance on simple tasks but only 15-32% on complex workflows -- indicates fundamental architectural limitations in memory management, hierarchical planning, and state coordination rather than incremental capability gaps addressable through better training or prompting. UI-CUBE functions as an enterprise-readiness diagnostic, revealing that while current CUAs can manipulate individual interface elements, they cannot yet function as reliable workflow automation tools. These findings provide architectural insights essential for developing production-ready CUAs capable of managing complex, multi-step enterprise processes.", "AI": {"tldr": "UI-CUBE is a new benchmark with 226 tasks that reveals current Computer Use Agents (CUAs) perform well on simple UI tasks but fail dramatically on complex enterprise workflows, exposing fundamental architectural limitations in memory, planning, and state coordination.", "motivation": "Existing CUA benchmarks focus on task completion and functional correctness but neglect operational reliability needed for real-world enterprise deployment; there's a need for a more rigorous evaluation of CUAs' readiness in production environments.", "method": "The authors introduce UI-CUBE, a systematic benchmark with 226 tasks across two tiers: simple UI interactions (136 tasks), and complex workflows including copy-paste (50) and enterprise scenarios (40). It features interface variation, multi-resolution testing, and automated validation via application state. Five state-of-the-art CUAs and human evaluators are tested.", "result": "State-of-the-art CUAs achieve 67\u201385% success on simple tasks (vs. 97.9% human performance) but only 9\u201319% on complex workflows. Humans without prior experience score 61.2% on complex tasks, revealing a sharp performance cliff. CUAs reach 68\u201387% of human performance on simple tasks but only 15\u201332% on complex ones.", "conclusion": "Current CUAs cannot reliably automate complex, multi-step enterprise workflows due to core architectural flaws\u2014not just training or prompting issues. UI-CUBE serves as a diagnostic tool for enterprise-readiness and guides future development toward production-capable agents."}}
{"id": "2511.17262", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2511.17262", "abs": "https://arxiv.org/abs/2511.17262", "authors": ["Jinfeng Wen", "Yuehan Sun"], "title": "SlsReuse: LLM-Powered Serverless Function Reuse", "comment": null, "summary": "Serverless computing has rapidly emerged as a popular cloud computing paradigm. It enables developers to implement function-level tasks, i.e., serverless functions, without managing infrastructure. While reducing operational overhead, it poses challenges, especially for novice developers. Developing functions from scratch requires adapting to heterogeneous, platform-specific programming styles, making the process time-consuming and error-prone. Function reuse offers a promising solution to address these challenges. However, research on serverless computing lacks a dedicated approach for function recommendation. Existing techniques from traditional contexts remain insufficient due to the semantic gap between task descriptions and heterogeneous function implementations. Advances in large language models (LLMs), pre-trained on large-scale corpora, create opportunities to bridge this gap by aligning developer requirements with function semantics.\n  This paper presents SlsReuse, the first LLM-powered framework for serverless function reuse. Specifically, SlsReuse first constructs a reusable function repository serving as a foundational knowledge base. Then, it learns unified semantic-enhanced representations of heterogeneous functions through effective prompt engineering with few-shot prompting, capturing implicit code intent, target platforms, programming languages, and cloud services. Finally, given a natural language task query, SlsReuse performs intent-aware discovery combined with a multi-level pruning strategy and similarity matching. We evaluate SlsReuse on a curated dataset of 110 task queries. Built on ChatGPT-4o, one of the most representative LLMs, SlsReuse achieves Recall@10 of 91.20%, exceeding the state-of-the-art baseline by 24.53 percentage points.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86SlsReuse\uff0c\u9996\u4e2a\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u65e0\u670d\u52a1\u5668\u51fd\u6570\u590d\u7528\u6846\u67b6\uff0c\u901a\u8fc7\u6784\u5efa\u53ef\u590d\u7528\u51fd\u6570\u5e93\u3001\u5229\u7528\u63d0\u793a\u5de5\u7a0b\u5b66\u4e60\u5f02\u6784\u51fd\u6570\u7684\u7edf\u4e00\u8bed\u4e49\u8868\u793a\uff0c\u5e76\u7ed3\u5408\u610f\u56fe\u611f\u77e5\u53d1\u73b0\u4e0e\u591a\u7ea7\u526a\u679d\u7b56\u7565\uff0c\u5728\u4efb\u52a1\u67e5\u8be2\u4e2d\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u65e0\u670d\u52a1\u5668\u8ba1\u7b97\u867d\u964d\u4f4e\u4e86\u8fd0\u7ef4\u5f00\u9500\uff0c\u4f46\u5bf9\u65b0\u624b\u5f00\u53d1\u8005\u800c\u8a00\uff0c\u4ece\u96f6\u5f00\u53d1\u51fd\u6570\u9700\u9002\u5e94\u591a\u6837\u5316\u7684\u5e73\u53f0\u7279\u5b9a\u7f16\u7a0b\u98ce\u683c\uff0c\u8fc7\u7a0b\u8017\u65f6\u4e14\u6613\u9519\u3002\u73b0\u6709\u51fd\u6570\u63a8\u8350\u65b9\u6cd5\u56e0\u4efb\u52a1\u63cf\u8ff0\u4e0e\u5f02\u6784\u51fd\u6570\u5b9e\u73b0\u4e4b\u95f4\u5b58\u5728\u8bed\u4e49\u9e3f\u6c9f\u800c\u6548\u679c\u6709\u9650\u3002", "method": "SlsReuse\u9996\u5148\u6784\u5efa\u53ef\u590d\u7528\u51fd\u6570\u4ed3\u5e93\u4f5c\u4e3a\u77e5\u8bc6\u57fa\u7840\uff1b\u5176\u6b21\u901a\u8fc7\u5c11\u6837\u672c\u63d0\u793a\u5de5\u7a0b\u5b66\u4e60\u5f02\u6784\u51fd\u6570\u7684\u7edf\u4e00\u8bed\u4e49\u589e\u5f3a\u8868\u793a\uff0c\u6db5\u76d6\u4ee3\u7801\u610f\u56fe\u3001\u76ee\u6807\u5e73\u53f0\u3001\u7f16\u7a0b\u8bed\u8a00\u548c\u4e91\u670d\u52a1\uff1b\u6700\u540e\u9488\u5bf9\u81ea\u7136\u8bed\u8a00\u4efb\u52a1\u67e5\u8be2\uff0c\u91c7\u7528\u610f\u56fe\u611f\u77e5\u53d1\u73b0\u3001\u591a\u7ea7\u526a\u679d\u548c\u76f8\u4f3c\u5ea6\u5339\u914d\u8fdb\u884c\u51fd\u6570\u63a8\u8350\u3002", "result": "\u5728\u5305\u542b110\u4e2a\u4efb\u52a1\u67e5\u8be2\u7684\u6570\u636e\u96c6\u4e0a\u8bc4\u4f30\uff0c\u57fa\u4e8eChatGPT-4o\u7684SlsReuse\u5b9e\u73b0\u4e8691.20%\u7684Recall@10\uff0c\u6bd4\u5f53\u524d\u6700\u4f18\u57fa\u7ebf\u9ad8\u51fa24.53\u4e2a\u767e\u5206\u70b9\u3002", "conclusion": "SlsReuse\u6709\u6548\u5f25\u5408\u4e86\u4efb\u52a1\u9700\u6c42\u4e0e\u65e0\u670d\u52a1\u5668\u51fd\u6570\u5b9e\u73b0\u4e4b\u95f4\u7684\u8bed\u4e49\u9e3f\u6c9f\uff0c\u663e\u8457\u63d0\u5347\u4e86\u51fd\u6570\u63a8\u8350\u7684\u51c6\u786e\u6027\u548c\u5b9e\u7528\u6027\uff0c\u4e3a\u65e0\u670d\u52a1\u5668\u5f00\u53d1\u63d0\u4f9b\u4e86\u9ad8\u6548\u590d\u7528\u7684\u65b0\u8303\u5f0f\u3002"}}
{"id": "2511.17271", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2511.17271", "abs": "https://arxiv.org/abs/2511.17271", "authors": ["Sebastian B\u00f6hm", "Florian Sattler", "Norbert Siegmund", "Sven Apel"], "title": "Detecting Performance-Relevant Changes in Configurable Software Systems", "comment": null, "summary": "Performance is a volatile property of a software system and frequent performance profiling is required to keep the knowledge about a software system's performance behavior up to date. Repeating all performance measurements after every revision is a cost-intensive task, especially in the presence of configurability, where one has to measure multiple configurations to obtain a comprehensive picture. Configuration sampling is a common approach to control the measurement cost. However, it cannot guarantee completeness and might miss performance regressions, especially if they only affect few configurations. As an alternative to solve the cost reduction problem, we present ConfFLARE: ConfFLARE estimates whether a change potentially impacts performance by identifying data-flow interactions with performance-relevant code and extracts which software features participate in such interactions. Based on these features, we can select a subset of relevant configurations to focus performance profiling efforts on. In a study conducted on both, synthetic and real-world software systems, ConfFLARE correctly detects performance regressions in almost all cases and identifies relevant features in all but two cases, reducing the number of configurations to be tested on average by $79\\%$ for synthetic and by $70\\%$ for real-world regression scenarios saving hours of performance testing time.", "AI": {"tldr": "ConfFLARE \u662f\u4e00\u79cd\u901a\u8fc7\u8bc6\u522b\u4e0e\u6027\u80fd\u76f8\u5173\u4ee3\u7801\u7684\u6570\u636e\u6d41\u4ea4\u4e92\u6765\u68c0\u6d4b\u6027\u80fd\u56de\u5f52\u5e76\u9009\u62e9\u5173\u952e\u914d\u7f6e\u8fdb\u884c\u6d4b\u8bd5\u7684\u65b0\u65b9\u6cd5\uff0c\u53ef\u663e\u8457\u51cf\u5c11\u6027\u80fd\u6d4b\u8bd5\u6240\u9700\u914d\u7f6e\u6570\u91cf\uff0c\u5e73\u5747\u8282\u770170%\u4ee5\u4e0a\u7684\u6d4b\u8bd5\u65f6\u95f4\u3002", "motivation": "\u8f6f\u4ef6\u7cfb\u7edf\u7684\u6027\u80fd\u5177\u6709\u6613\u53d8\u6027\uff0c\u9891\u7e41\u7684\u6027\u80fd\u5206\u6790\u6210\u672c\u9ad8\u6602\uff0c\u5c24\u5176\u5728\u53ef\u914d\u7f6e\u7cfb\u7edf\u4e2d\u9700\u6d4b\u8bd5\u5927\u91cf\u914d\u7f6e\uff1b\u4f20\u7edf\u914d\u7f6e\u91c7\u6837\u65b9\u6cd5\u65e0\u6cd5\u4fdd\u8bc1\u5b8c\u6574\u6027\uff0c\u53ef\u80fd\u9057\u6f0f\u4ec5\u5f71\u54cd\u5c11\u6570\u914d\u7f6e\u7684\u6027\u80fd\u9000\u5316\u3002", "method": "ConfFLARE \u901a\u8fc7\u5206\u6790\u4ee3\u7801\u53d8\u66f4\u4e0e\u6027\u80fd\u76f8\u5173\u4ee3\u7801\u4e4b\u95f4\u7684\u6570\u636e\u6d41\u4ea4\u4e92\uff0c\u8bc6\u522b\u53c2\u4e0e\u8fd9\u4e9b\u4ea4\u4e92\u7684\u8f6f\u4ef6\u7279\u6027\uff0c\u5e76\u636e\u6b64\u9009\u62e9\u76f8\u5173\u914d\u7f6e\u5b50\u96c6\u8fdb\u884c\u6027\u80fd\u5256\u6790\u3002", "result": "\u5728\u5408\u6210\u548c\u771f\u5b9e\u8f6f\u4ef6\u7cfb\u7edf\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cConfFLARE \u51e0\u4e4e\u80fd\u6b63\u786e\u68c0\u6d4b\u6240\u6709\u6027\u80fd\u56de\u5f52\uff0c\u5728\u9664\u4e24\u4e2a\u6848\u4f8b\u5916\u7684\u6240\u6709\u60c5\u51b5\u4e0b\u90fd\u80fd\u51c6\u786e\u8bc6\u522b\u76f8\u5173\u7279\u6027\uff0c\u5e73\u5747\u51cf\u5c1179%\uff08\u5408\u6210\uff09\u548c70%\uff08\u771f\u5b9e\uff09\u7684\u6d4b\u8bd5\u914d\u7f6e\uff0c\u5927\u5e45\u8282\u7701\u6d4b\u8bd5\u65f6\u95f4\u3002", "conclusion": "ConfFLARE \u80fd\u6709\u6548\u964d\u4f4e\u6027\u80fd\u6d4b\u8bd5\u6210\u672c\uff0c\u540c\u65f6\u4fdd\u6301\u5bf9\u6027\u80fd\u56de\u5f52\u7684\u9ad8\u68c0\u6d4b\u7387\uff0c\u662f\u4e00\u79cd\u4f18\u4e8e\u4f20\u7edf\u914d\u7f6e\u91c7\u6837\u65b9\u6cd5\u7684\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2511.17303", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2511.17303", "abs": "https://arxiv.org/abs/2511.17303", "authors": ["Timmie M. R. Lagermann", "Kristina Sophia Carter", "Su Mei Gwen Ho", "Lu\u00eds Cruz", "Kerstin Eder", "Maja H. Kirkeby"], "title": "Framework Matters: Energy Efficiency of UI Automation Testing Frameworks", "comment": "10 pages, 6 figures, submitted to The 41st ACM/SIGAPP Symposium On Applied Computing (SAC2026)", "summary": "We examine per action energy consumption across four web user interface (UI) automation testing frameworks to determine whether consistent tendencies can guide energy-aware test design. Using a controlled client-server setup with external power metering, we repeat each UI action (refresh, click variants, checkbox, drag&drop, input-text, scroll) 35 times. Across each of the actions, energy costs vary by both framework and action. Puppeteer is the most efficient for left-click, right-click, double-click, checkbox, and input-text; Selenium is the most efficient for refresh and scroll; Nightwatch is generally the least energy efficient. The energy cost of performing the same action varied by up to a factor of six depending on the framework. This indicates that providing transparency of energy consumption for UI automation testing frameworks allows developers to make informed, energy-aware decisions when testing a specific UI action.", "AI": {"tldr": "\u8be5\u7814\u7a76\u8bc4\u4f30\u4e86\u56db\u4e2aWeb UI\u81ea\u52a8\u5316\u6d4b\u8bd5\u6846\u67b6\u5728\u6267\u884c\u5e38\u89c1UI\u64cd\u4f5c\u65f6\u7684\u80fd\u8017\u8868\u73b0\uff0c\u53d1\u73b0\u4e0d\u540c\u6846\u67b6\u5728\u4e0d\u540c\u64cd\u4f5c\u4e0b\u7684\u80fd\u6548\u5dee\u5f02\u663e\u8457\uff0c\u6700\u9ad8\u53ef\u8fbe\u516d\u500d\uff0c\u8868\u660e\u5f00\u53d1\u8005\u53ef\u6839\u636e\u5177\u4f53\u64cd\u4f5c\u9009\u62e9\u66f4\u8282\u80fd\u7684\u6846\u67b6\u3002", "motivation": "\u4e3a\u6307\u5bfc\u5f00\u53d1\u4eba\u5458\u8fdb\u884c\u8282\u80fd\u7684UI\u81ea\u52a8\u5316\u6d4b\u8bd5\u8bbe\u8ba1\uff0c\u9700\u4e86\u89e3\u4e0d\u540c\u6d4b\u8bd5\u6846\u67b6\u5728\u6267\u884c\u5404\u7c7bUI\u64cd\u4f5c\u65f6\u7684\u80fd\u8017\u7279\u6027\u3002", "method": "\u5728\u53d7\u63a7\u7684\u5ba2\u6237\u7aef-\u670d\u52a1\u5668\u73af\u5883\u4e2d\uff0c\u4f7f\u7528\u5916\u90e8\u529f\u7387\u8ba1\u5bf9\u56db\u79cdUI\u81ea\u52a8\u5316\u6d4b\u8bd5\u6846\u67b6\uff08Puppeteer\u3001Selenium\u3001Nightwatch\u7b49\uff09\u91cd\u590d\u6267\u884c\u4e03\u79cd\u5e38\u89c1UI\u64cd\u4f5c\uff08\u5982\u70b9\u51fb\u3001\u6eda\u52a8\u3001\u8f93\u5165\u6587\u672c\u7b49\uff09\u540435\u6b21\uff0c\u6d4b\u91cf\u5e76\u6bd4\u8f83\u6bcf\u6b21\u64cd\u4f5c\u7684\u80fd\u8017\u3002", "result": "Puppeteer\u5728\u591a\u6570\u64cd\u4f5c\uff08\u5de6\u952e/\u53f3\u952e/\u53cc\u51fb\u3001\u590d\u9009\u6846\u3001\u6587\u672c\u8f93\u5165\uff09\u4e2d\u6700\u8282\u80fd\uff1bSelenium\u5728\u5237\u65b0\u548c\u6eda\u52a8\u64cd\u4f5c\u4e2d\u8868\u73b0\u6700\u4f73\uff1bNightwatch\u6574\u4f53\u80fd\u6548\u6700\u5dee\uff1b\u76f8\u540c\u64cd\u4f5c\u5728\u4e0d\u540c\u6846\u67b6\u95f4\u80fd\u8017\u5dee\u5f02\u6700\u9ad8\u8fbe\u516d\u500d\u3002", "conclusion": "\u901a\u8fc7\u63d0\u4f9bUI\u81ea\u52a8\u5316\u6d4b\u8bd5\u6846\u67b6\u7684\u80fd\u8017\u900f\u660e\u5ea6\uff0c\u5f00\u53d1\u8005\u53ef\u9488\u5bf9\u7279\u5b9aUI\u64cd\u4f5c\u505a\u51fa\u66f4\u8282\u80fd\u7684\u9009\u62e9\uff0c\u4ece\u800c\u5b9e\u73b0\u80fd\u6e90\u611f\u77e5\u7684\u6d4b\u8bd5\u8bbe\u8ba1\u3002"}}
{"id": "2511.17330", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2511.17330", "abs": "https://arxiv.org/abs/2511.17330", "authors": ["Haoxin Tu", "Huan Zhao", "Yahui Song", "Mehtab Zafar", "Ruijie Meng", "Abhik Roychoudhury"], "title": "Agentic Program Verification", "comment": "21 pages, 8 figures", "summary": "Automatically generated code is gaining traction recently, owing to the prevalence of Large Language Models (LLMs). Further, the AlphaProof initiative has demonstrated the possibility of using AI for general mathematical reasoning. Reasoning about computer programs (software) can be accomplished via general mathematical reasoning; however, it tends to be more structured and richer in contexts. This forms an attractive proposition, since then AI agents can be used to reason about voluminous code that gets generated by AI.\n  In this work, we present a first LLM agent, AutoRocq, for conducting program verification. Unlike past works, which rely on extensive training of LLMs on proof examples, our agent learns on-the-fly and improves the proof via an iterative refinement loop. The iterative improvement of the proof is achieved by the proof agent communicating with the Rocq (formerly Coq) theorem prover to get additional context and feedback. The final result of the iteration is a proof derivation checked by the Rocq theorem prover. In this way, our proof construction involves autonomous collaboration between the proof agent and the theorem prover. This autonomy facilitates the search for proofs and decision-making in deciding on the structure of the proof tree.\n  Experimental evaluation on SV-COMP benchmarks and on Linux kernel modules shows promising efficacy in achieving automated program verification. As automation in code generation becomes more widespread, we posit that our proof agent can be potentially integrated with AI coding agents to achieve a generate and validate loop, thus moving closer to the vision of trusted automatic programming.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u9996\u4e2a\u7528\u4e8e\u7a0b\u5e8f\u9a8c\u8bc1\u7684\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u667a\u80fd\u4f53 AutoRocq\uff0c\u5b83\u901a\u8fc7\u4e0e Rocq \u5b9a\u7406\u8bc1\u660e\u5668\u7684\u8fed\u4ee3\u4ea4\u4e92\u5b9e\u73b0\u65e0\u9700\u5927\u91cf\u8bad\u7ec3\u7684\u81ea\u52a8\u8bc1\u660e\u751f\u6210\uff0c\u5e76\u5728 SV-COMP \u548c Linux \u5185\u6838\u6a21\u5757\u4e0a\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u968f\u7740\u5927\u8bed\u8a00\u6a21\u578b\u81ea\u52a8\u751f\u6210\u4ee3\u7801\u7684\u666e\u53ca\uff0c\u4e9f\u9700\u4e00\u79cd\u80fd\u5bf9\u751f\u6210\u4ee3\u7801\u8fdb\u884c\u81ea\u52a8\u9a8c\u8bc1\u7684\u65b9\u6cd5\u3002\u867d\u7136\u901a\u7528\u6570\u5b66\u63a8\u7406\u53ef\u7528\u4e8e\u7a0b\u5e8f\u63a8\u7406\uff0c\u4f46\u7a0b\u5e8f\u63a8\u7406\u5177\u6709\u66f4\u5f3a\u7684\u7ed3\u6784\u6027\u548c\u4e0a\u4e0b\u6587\u4e30\u5bcc\u6027\uff0c\u56e0\u6b64\u9700\u8981\u4e13\u95e8\u8bbe\u8ba1\u7684 AI \u63a8\u7406\u673a\u5236\u3002", "method": "\u63d0\u51fa AutoRocq \u667a\u80fd\u4f53\uff0c\u8be5\u667a\u80fd\u4f53\u4e0d\u4f9d\u8d56\u4e8e\u5927\u91cf\u8bc1\u660e\u6837\u672c\u7684\u8bad\u7ec3\uff0c\u800c\u662f\u901a\u8fc7\u4e0e Rocq\uff08\u539f Coq\uff09\u5b9a\u7406\u8bc1\u660e\u5668\u7684\u8fed\u4ee3\u4ea4\u4e92\uff0c\u5728\u8fd0\u884c\u4e2d\u5b66\u4e60\u5e76\u4e0d\u65ad\u4f18\u5316\u8bc1\u660e\u8fc7\u7a0b\uff0c\u6700\u7ec8\u751f\u6210\u7ecf Rocq \u9a8c\u8bc1\u7684\u8bc1\u660e\u63a8\u5bfc\u3002", "result": "\u5728 SV-COMP \u57fa\u51c6\u6d4b\u8bd5\u548c Linux \u5185\u6838\u6a21\u5757\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cAutoRocq \u5728\u81ea\u52a8\u5316\u7a0b\u5e8f\u9a8c\u8bc1\u65b9\u9762\u8868\u73b0\u51fa\u826f\u597d\u6548\u679c\u3002", "conclusion": "AutoRocq \u5c55\u793a\u4e86\u5c06 LLM \u667a\u80fd\u4f53\u4e0e\u5b9a\u7406\u8bc1\u660e\u5668\u7ed3\u5408\u7528\u4e8e\u7a0b\u5e8f\u9a8c\u8bc1\u7684\u53ef\u884c\u6027\uff0c\u672a\u6765\u53ef\u4e0e AI \u7f16\u7801\u667a\u80fd\u4f53\u96c6\u6210\uff0c\u5f62\u6210\u201c\u751f\u6210\u2014\u9a8c\u8bc1\u201d\u95ed\u73af\uff0c\u63a8\u52a8\u53ef\u4fe1\u81ea\u52a8\u5316\u7f16\u7a0b\u7684\u53d1\u5c55\u3002"}}
{"id": "2511.17368", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2511.17368", "abs": "https://arxiv.org/abs/2511.17368", "authors": ["Eric L. Melin", "Ahmed Musa Awon", "Nasir U. Eisty", "Neil A. Ernst", "Shurui Zhou"], "title": "Exploring Scientific Debt: Harnessing AI for SATD Identification in Scientific Software", "comment": "11 pages, 2 figures, 6 tables", "summary": "Developers often leave behind clues in their code, admitting where it falls short, known as Self-Admitted Technical Debt (SATD). In the world of Scientific Software (SSW), where innovation moves fast and collaboration is key, such debt is not just common but deeply impactful. As research relies on accurate and reproducible results, accumulating SATD can threaten the very foundations of scientific discovery. Yet, despite its significance, the relationship between SATD and SSW remains largely unexplored, leaving a crucial gap in understanding how to manage SATD in this critical domain. This study explores SATD in SSW repositories, comparing SATD in scientific versus general-purpose open-source software and evaluating transformer-based models for SATD identification. We analyzed SATD in 27 scientific and general-purpose repositories across multiple domains and languages. We fine-tuned and compared 10 transformer-based models (100M-7B parameters) on 67,066 labeled code comments. SSW contains 9.25x more Scientific Debt and 4.93x more SATD than general-purpose software due to complex computations, domain constraints, and evolving research needs. Furthermore, our best model outperforms existing ones. This study uncovers how SATD in SSW differs from general software, revealing its impact on quality and scientific validity. By recognizing these challenges, developers and researchers can adopt smarter strategies to manage debt and safeguard the integrity of scientific discovery.", "AI": {"tldr": "\u8be5\u7814\u7a76\u53d1\u73b0\u79d1\u5b66\u8f6f\u4ef6\uff08SSW\uff09\u4e2d\u7684\u81ea\u8ba4\u6280\u672f\u503a\uff08SATD\uff09\u8fdc\u9ad8\u4e8e\u901a\u7528\u8f6f\u4ef6\uff0c\u5e76\u9a8c\u8bc1\u4e86\u57fa\u4e8eTransformer\u7684\u6a21\u578b\u5728\u8bc6\u522bSATD\u65b9\u9762\u7684\u4f18\u8d8a\u6027\u80fd\uff0c\u5f3a\u8c03\u4e86\u7ba1\u7406SATD\u5bf9\u4fdd\u969c\u79d1\u7814\u7ed3\u679c\u53ef\u9760\u6027\u7684\u91cd\u8981\u6027\u3002", "motivation": "\u79d1\u5b66\u8f6f\u4ef6\u4e2d\u5e7f\u6cdb\u5b58\u5728\u81ea\u8ba4\u6280\u672f\u503a\uff08SATD\uff09\uff0c\u53ef\u80fd\u5a01\u80c1\u79d1\u7814\u7ed3\u679c\u7684\u51c6\u786e\u6027\u4e0e\u53ef\u590d\u73b0\u6027\uff0c\u4f46SATD\u4e0e\u79d1\u5b66\u8f6f\u4ef6\u4e4b\u95f4\u7684\u5173\u7cfb\u5c1a\u672a\u88ab\u5145\u5206\u7814\u7a76\uff0c\u4e9f\u9700\u6df1\u5165\u7406\u89e3\u5176\u7279\u6027\u53ca\u5f71\u54cd\u3002", "method": "\u5206\u679027\u4e2a\u6db5\u76d6\u591a\u9886\u57df\u548c\u7f16\u7a0b\u8bed\u8a00\u7684\u79d1\u5b66\u4e0e\u901a\u7528\u5f00\u6e90\u4ed3\u5e93\u4e2d\u7684\u4ee3\u7801\u6ce8\u91ca\uff1b\u572867,066\u6761\u6807\u6ce8\u6570\u636e\u4e0a\u5fae\u8c03\u5e76\u6bd4\u8f8310\u79cd\u53c2\u6570\u89c4\u6a21\u4ece1\u4ebf\u523070\u4ebf\u7684Transformer\u6a21\u578b\uff0c\u4ee5\u8bc6\u522bSATD\u3002", "result": "\u79d1\u5b66\u8f6f\u4ef6\u4e2d\u7684\u79d1\u5b66\u503a\u52a1\u662f\u901a\u7528\u8f6f\u4ef6\u76849.25\u500d\uff0cSATD\u603b\u91cf\u662f4.93\u500d\uff1b\u6240\u63d0\u51fa\u7684\u6700\u4f73\u6a21\u578b\u5728SATD\u8bc6\u522b\u4efb\u52a1\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "\u79d1\u5b66\u8f6f\u4ef6\u4e2d\u7684SATD\u5177\u6709\u72ec\u7279\u6027\u548c\u9ad8\u53d1\u6027\uff0c\u5bf9\u8f6f\u4ef6\u8d28\u91cf\u548c\u79d1\u7814\u53ef\u4fe1\u5ea6\u6784\u6210\u663e\u8457\u5f71\u54cd\uff1b\u7814\u7a76\u7ed3\u679c\u6709\u52a9\u4e8e\u5f00\u53d1\u8005\u548c\u79d1\u7814\u4eba\u5458\u5236\u5b9a\u66f4\u6709\u6548\u7684\u6280\u672f\u503a\u7ba1\u7406\u7b56\u7565\uff0c\u7ef4\u62a4\u79d1\u5b66\u53d1\u73b0\u7684\u5b8c\u6574\u6027\u3002"}}
{"id": "2511.17417", "categories": ["cs.SE", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.17417", "abs": "https://arxiv.org/abs/2511.17417", "authors": ["Soroush Javdan", "Pragash Krishnamoorthy", "Olga Baysal"], "title": "CREST: Improving Interpretability and Effectiveness of Troubleshooting at Ericsson through Criterion-Specific Trouble Report Retrieval", "comment": null, "summary": "The rapid evolution of the telecommunication industry necessitates efficient troubleshooting processes to maintain network reliability, software maintainability, and service quality. Trouble Reports (TRs), which document issues in Ericsson's production system, play a critical role in facilitating the timely resolution of software faults. However, the complexity and volume of TR data, along with the presence of diverse criteria that reflect different aspects of each fault, present challenges for retrieval systems. Building on prior work at Ericsson, which utilized a two-stage workflow, comprising Initial Retrieval (IR) and Re-Ranking (RR) stages, this study investigates different TR observation criteria and their impact on the performance of retrieval models. We propose \\textbf{CREST} (\\textbf{C}riteria-specific \\textbf{R}etrieval via \\textbf{E}nsemble of \\textbf{S}pecialized \\textbf{T}R models), a criterion-driven retrieval approach that leverages specialized models for different TR fields to improve both effectiveness and interpretability, thereby enabling quicker fault resolution and supporting software maintenance. CREST utilizes specialized models trained on specific TR criteria and aggregates their outputs to capture diverse and complementary signals. This approach leads to enhanced retrieval accuracy, better calibration of predicted scores, and improved interpretability by providing relevance scores for each criterion, helping users understand why specific TRs were retrieved. Using a subset of Ericsson's internal TRs, this research demonstrates that criterion-specific models significantly outperform a single model approach across key evaluation metrics. This highlights the importance of all targeted criteria used in this study for optimizing the performance of retrieval systems.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faCREST\u65b9\u6cd5\uff0c\u901a\u8fc7\u4e3a\u6545\u969c\u62a5\u544a\uff08TR\uff09\u4e2d\u7684\u4e0d\u540c\u89c2\u5bdf\u6807\u51c6\u8bad\u7ec3\u4e13\u7528\u68c0\u7d22\u6a21\u578b\uff0c\u5e76\u96c6\u6210\u5176\u8f93\u51fa\uff0c\u4ece\u800c\u63d0\u5347\u68c0\u7d22\u6548\u679c\u3001\u6821\u51c6\u80fd\u529b\u548c\u53ef\u89e3\u91ca\u6027\u3002", "motivation": "\u7535\u4fe1\u884c\u4e1a\u5feb\u901f\u53d1\u5c55\u8981\u6c42\u9ad8\u6548\u7684\u6545\u969c\u6392\u67e5\u6d41\u7a0b\uff1b\u73b0\u6709TR\u6570\u636e\u590d\u6742\u4e14\u91cf\u5927\uff0c\u5355\u4e00\u68c0\u7d22\u6a21\u578b\u96be\u4ee5\u6709\u6548\u5904\u7406\u591a\u7ef4\u6807\u51c6\uff0c\u5f71\u54cd\u6545\u969c\u5b9a\u4f4d\u6548\u7387\u3002", "method": "\u63d0\u51faCREST\u6846\u67b6\uff0c\u91c7\u7528\u9488\u5bf9\u4e0d\u540cTR\u5b57\u6bb5\uff08\u5982\u6545\u969c\u73b0\u8c61\u3001\u65e5\u5fd7\u7b49\uff09\u7684\u4e13\u7528\u68c0\u7d22\u6a21\u578b\uff0c\u5728\u521d\u59cb\u68c0\u7d22\u548c\u91cd\u6392\u5e8f\u4e24\u9636\u6bb5\u6d41\u7a0b\u4e2d\u5206\u522b\u5efa\u6a21\uff0c\u5e76\u878d\u5408\u5404\u6a21\u578b\u8f93\u51fa\u4ee5\u83b7\u5f97\u7efc\u5408\u76f8\u5173\u6027\u8bc4\u5206\u3002", "result": "\u5728\u7231\u7acb\u4fe1\u5185\u90e8TR\u6570\u636e\u5b50\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cCREST\u5728\u5173\u952e\u8bc4\u4f30\u6307\u6807\u4e0a\u663e\u8457\u4f18\u4e8e\u5355\u4e00\u6a21\u578b\u65b9\u6cd5\uff0c\u9a8c\u8bc1\u4e86\u591a\u6807\u51c6\u5efa\u6a21\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u9488\u5bf9\u4e0d\u540cTR\u89c2\u5bdf\u6807\u51c6\u4f7f\u7528\u4e13\u7528\u6a21\u578b\u80fd\u663e\u8457\u63d0\u5347\u68c0\u7d22\u6027\u80fd\u4e0e\u53ef\u89e3\u91ca\u6027\uff0c\u6709\u52a9\u4e8e\u52a0\u5feb\u8f6f\u4ef6\u6545\u969c\u6392\u67e5\u548c\u7ef4\u62a4\u3002"}}
