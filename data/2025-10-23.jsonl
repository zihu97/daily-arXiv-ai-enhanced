{"id": "2510.19765", "categories": ["cs.OS", "cs.PF", "cs.PL"], "pdf": "https://arxiv.org/pdf/2510.19765", "abs": "https://arxiv.org/abs/2510.19765", "authors": ["Vinay Banakar", "Suli Yang", "Kan Wu", "Andrea C. Arpaci-Dusseau", "Remzi H. Arpaci-Dusseau", "Kimberly Keeton"], "title": "Tidying Up the Address Space", "comment": null, "summary": "Memory tiering in datacenters does not achieve its full potential due to\nhotness fragmentation -- the intermingling of hot and cold objects within\nmemory pages. This fragmentation prevents page-based reclamation systems from\ndistinguishing truly hot pages from pages containing mostly cold objects,\nfundamentally limiting memory efficiency despite highly skewed accesses. We\nintroduce address-space engineering: dynamically reorganizing application\nvirtual address spaces to create uniformly hot and cold regions that any\npage-level tiering backend can manage effectively. HADES demonstrates this\nfrontend/backend approach through a compiler-runtime system that tracks and\nmigrates objects based on access patterns, requiring minimal developer\nintervention. Evaluations across ten data structures achieve up to 70% memory\nreduction with 3% performance overhead, showing that address space engineering\nenables existing reclamation systems to reclaim memory aggressively without\nperformance degradation."}
{"id": "2510.19783", "categories": ["cs.NI", "cs.PF"], "pdf": "https://arxiv.org/pdf/2510.19783", "abs": "https://arxiv.org/abs/2510.19783", "authors": ["Miguel Sánchez de la Rosa", "Francisco J. andújar", "Jesus Escudero-Sahuquillo", "José L. Sánchez", "Francisco J. Alfaro-Cortés"], "title": "On the Power Saving in High-Speed Ethernet-based Networks for Supercomputers and Data Centers", "comment": "Submitted to The Journal of Systems Architecture. Currently under\n  revision", "summary": "The increase in computation and storage has led to a significant growth in\nthe scale of systems powering applications and services, raising concerns about\nsustainability and operational costs. In this paper, we explore power-saving\ntechniques in high-performance computing (HPC) and datacenter networks, and\ntheir relation with performance degradation. From this premise, we propose\nleveraging Energy Efficient Ethernet (EEE), with the flexibility to extend to\nconventional Ethernet or upcoming Ethernet-derived interconnect versions of BXI\nand Omnipath.\n  We analyze the PerfBound proposal, identifying possible improvements and\nmodeling it into a simulation framework. Through different experiments, we\nexamine its impact on performance and determine the most appropriate\ninterconnect. We also study traffic patterns generated by selected HPC and\nmachine learning applications to evaluate the behavior of power-saving\ntechniques.\n  From these experiments, we provide an analysis of how applications affect\nsystem and network energy consumption. Based on this, we disclose the weakness\nof dynamic power-down mechanisms and propose an approach that improves energy\nreduction with minimal or no performance penalty. To our knowledge, this is the\nfirst power management proposal tailored to future Ethernet-based HPC\narchitectures, with promising results."}
{"id": "2510.19072", "categories": ["cs.MA", "cs.AI", "cs.RO"], "pdf": "https://arxiv.org/pdf/2510.19072", "abs": "https://arxiv.org/abs/2510.19072", "authors": ["Tomoki Arita", "Keisuke Okumura"], "title": "Local Guidance for Configuration-Based Multi-Agent Pathfinding", "comment": "10 pages", "summary": "Guidance is an emerging concept that improves the empirical performance of\nreal-time, sub-optimal multi-agent pathfinding (MAPF) methods. It offers\nadditional information to MAPF algorithms to mitigate congestion on a global\nscale by considering the collective behavior of all agents across the entire\nworkspace. This global perspective helps reduce agents' waiting times, thereby\nimproving overall coordination efficiency. In contrast, this study explores an\nalternative approach: providing local guidance in the vicinity of each agent.\nWhile such localized methods involve recomputation as agents move and may\nappear computationally demanding, we empirically demonstrate that supplying\ninformative spatiotemporal cues to the planner can significantly improve\nsolution quality without exceeding a moderate time budget. When applied to\nLaCAM, a leading configuration-based solver, this form of guidance establishes\na new performance frontier for MAPF."}
{"id": "2510.19322", "categories": ["cs.NI", "cs.AI", "cs.DC"], "pdf": "https://arxiv.org/pdf/2510.19322", "abs": "https://arxiv.org/abs/2510.19322", "authors": ["Changbo Wu", "Zhuolong Yu", "Gongming Zhao", "Hongli Xu"], "title": "Enabling Reconfiguration-Communication Overlap for Collective Communication in Optical Networks", "comment": null, "summary": "Collective communication (CC) is widely adopted for large-scale distributed\nmachine learning (DML) training workloads. DML's predictable traffic pattern\nprovides a great oppotunity for applying optical network technology. Existing\noptical interconnects-based CC schemes adopt ``one-shot network\nreconfiguration'', which provisions static high-capacity topologies for an\nentire collective operation -- sometimes for a full training iteration.\nHowever, this approach faces significant scalability limitations when\nsupporting more complex and efficient CC algorithms required for modern\nworkloads: the ``one-shot'' strategies either demand excessive resource\noverprovisioning or suffer performance degradation due to rigid resource\nallocation.\n  To address these challenges, we propose SWOT, a demand-aware optical network\nframework. SWOT employs ``intra-collective reconfiguration'' and can\ndynamically align network resources with CC traffic patterns. SWOT incorporates\na novel scheduling technique that overlaps optical switch reconfigurations with\nongoing transmissions, and improves communication efficiency. SWOT introduce a\nlightweight collective communication shim that enables coordinated optical\nnetwork configuration and transmission scheduling while supporting seamless\nintegration with existing CC libraries. Our simulation results demonstrate\nSWOT's significant performance improvements."}
{"id": "2510.19765", "categories": ["cs.OS", "cs.PF", "cs.PL"], "pdf": "https://arxiv.org/pdf/2510.19765", "abs": "https://arxiv.org/abs/2510.19765", "authors": ["Vinay Banakar", "Suli Yang", "Kan Wu", "Andrea C. Arpaci-Dusseau", "Remzi H. Arpaci-Dusseau", "Kimberly Keeton"], "title": "Tidying Up the Address Space", "comment": null, "summary": "Memory tiering in datacenters does not achieve its full potential due to\nhotness fragmentation -- the intermingling of hot and cold objects within\nmemory pages. This fragmentation prevents page-based reclamation systems from\ndistinguishing truly hot pages from pages containing mostly cold objects,\nfundamentally limiting memory efficiency despite highly skewed accesses. We\nintroduce address-space engineering: dynamically reorganizing application\nvirtual address spaces to create uniformly hot and cold regions that any\npage-level tiering backend can manage effectively. HADES demonstrates this\nfrontend/backend approach through a compiler-runtime system that tracks and\nmigrates objects based on access patterns, requiring minimal developer\nintervention. Evaluations across ten data structures achieve up to 70% memory\nreduction with 3% performance overhead, showing that address space engineering\nenables existing reclamation systems to reclaim memory aggressively without\nperformance degradation."}
{"id": "2510.18895", "categories": ["cs.SE", "cs.AI", "cs.HC"], "pdf": "https://arxiv.org/pdf/2510.18895", "abs": "https://arxiv.org/abs/2510.18895", "authors": ["Santhosh Kumar Ravindran"], "title": "CosmoCore Affective Dream-Replay Reinforcement Learning for Code Generation", "comment": "12 pages", "summary": "We introduce CosmoCore, a neuroscience-inspired reinforcement learning (RL)\narchitecture that integrates affective signals to enhance code generation in\nlarge language models (LLMs). Motivated by human and animal learning where\nembarrassment from mistakes drives rapid correction, as observed in training a\npuppy to avoid repeating errors after a single scolding CosmoCore tags code\ngeneration trajectories with valence and surprise using a lightweight\nmulti-layer perceptron (MLP). High-negative valence (cringe) episodes, such as\nbuggy code outputs, are prioritized in a Dream Queue for five-fold replay\nduring off-policy updates, while low-surprise successes are pruned to prevent\noverconfidence and buffer bloat. Evaluated on code generation benchmarks like\nHumanEval and BigCodeBench, alongside simulations with a custom data pipeline\nenvironment, CosmoCore reduces hallucinated code (e.g., syntax errors or\nlogical bugs) by 48\\% and accelerates self-correction by 45\\%. Local\nexperiments using Hugging Face models in a PySpark environment validate these\ngains, with code snippets provided for replication. Ablations confirm valence\ntagging boosts curiosity in exploration, and pruning mitigates inefficiency.\nThis framework extends RL from human feedback (RLHF) for more emotionally aware\ncode assistants, with applications in IDEs and data pipelines. Code and the\ncustom mini-world simulation are released."}
{"id": "2510.19260", "categories": ["cs.AR", "cs.ET", "eess.IV"], "pdf": "https://arxiv.org/pdf/2510.19260", "abs": "https://arxiv.org/abs/2510.19260", "authors": ["Mukul Lokhande", "Narendra Singh Dhakad", "Seema Chouhan", "Akash Sankhe", "Santosh Kumar Vishvakarma"], "title": "Res-DPU: Resource-shared Digital Processing-in-memory Unit for Edge-AI Workloads", "comment": null, "summary": "Processing-in-memory (PIM) has emerged as the go to solution for addressing\nthe von Neumann bottleneck in edge AI accelerators. However, state-of-the-art\n(SoTA) digital PIM approaches suffer from low compute density, primarily due to\nthe use of bulky bit cells and transistor-heavy adder trees, which impose\nlimitations on macro scalability and energy efficiency. This work introduces\nRes-DPU, a resource-shared digital PIM unit, with a dual-port 5T SRAM latch and\nshared 2T AND compute logic. This reflects the per-bit multiplication cost to\njust 5.25T and reduced the transistor count of the PIM array by up to 56% over\nthe SoTA works. Furthermore, a Transistor-Reduced 2D Interspersed Adder Tree\n(TRAIT) with FA-7T and PG-FA-26T helps reduce the power consumption of the\nadder tree by up to 21.35% and leads to improved energy efficiency by 59%\ncompared to conventional 28T RCA designs. We propose a Cycle-controlled\nIterative Approximate-Accurate Multiplication (CIA2M) approach, enabling\nrun-time accuracy-latency trade-offs without requiring error-correction\ncircuitry. The 16 KB REP-DPIM macro achieves 0.43 TOPS throughput and 87.22\nTOPS/W energy efficiency in TSMC 65nm CMOS, with 96.85% QoR for ResNet-18 or\nVGG-16 on CIFAR-10, including 30% pruning. The proposed results establish a\nRes-DPU module for highly scalable and energy-efficient real-time edge AI\naccelerators."}
{"id": "2510.18893", "categories": ["cs.DC", "cs.AI", "cs.SE", "I.2.11; D.2.11"], "pdf": "https://arxiv.org/pdf/2510.18893", "abs": "https://arxiv.org/abs/2510.18893", "authors": ["Sergey Pugachev"], "title": "CodeCRDT: Observation-Driven Coordination for Multi-Agent LLM Code Generation", "comment": "11 pages, 3 figures", "summary": "Multi-agent LLM systems fail to realize parallel speedups due to costly\ncoordination. We present CodeCRDT, an observation-driven coordination pattern\nwhere agents coordinate by monitoring a shared state with observable updates\nand deterministic convergence, rather than explicit message passing. Using\nConflict-Free Replicated Data Types (CRDTs), CodeCRDT enables lock-free,\nconflict-free concurrent code generation with strong eventual consistency.\nEvaluation across 600 trials (6 tasks, 50 runs per mode) shows both benefits\nand trade-offs: up to 21.1% speedup on some tasks, up to 39.4% slowdown on\nothers, and 100% convergence with zero merge failures. The study formalizes\nobservation-driven coordination for stochastic LLM agents, revealing semantic\nconflict rates (5-10%) and quality-performance tradeoffs, and provides\nempirical characterization of when parallel coordination succeeds versus fails\nbased on task structure."}
{"id": "2510.19327", "categories": ["cs.MA", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.19327", "abs": "https://arxiv.org/abs/2510.19327", "authors": ["Usama Antuley", "Shahbaz Siddiqui", "Sufian Hameed", "Waqas Arif", "Subhan Shah", "Syed Attique Shah"], "title": "SORA-ATMAS: Adaptive Trust Management and Multi-LLM Aligned Governance for Future Smart Cities", "comment": null, "summary": "The rapid evolution of smart cities has increased the reliance on intelligent\ninterconnected services to optimize infrastructure, resources, and citizen\nwell-being. Agentic AI has emerged as a key enabler by supporting autonomous\ndecision-making and adaptive coordination, allowing urban systems to respond in\nreal time to dynamic conditions. Its benefits are evident in areas such as\ntransportation, where the integration of traffic data, weather forecasts, and\nsafety sensors enables dynamic rerouting and a faster response to hazards.\nHowever, its deployment across heterogeneous smart city ecosystems raises\ncritical governance, risk, and compliance (GRC) challenges, including\naccountability, data privacy, and regulatory alignment within decentralized\ninfrastructures. Evaluation of SORA-ATMAS with three domain agents (Weather,\nTraffic, and Safety) demonstrated that its governance policies, including a\nfallback mechanism for high-risk scenarios, effectively steer multiple LLMs\n(GPT, Grok, DeepSeek) towards domain-optimized, policy-aligned outputs,\nproducing an average MAE reduction of 35% across agents. Results showed stable\nweather monitoring, effective handling of high-risk traffic plateaus 0.85, and\nadaptive trust regulation in Safety/Fire scenarios 0.65. Runtime profiling of a\n3-agent deployment confirmed scalability, with throughput between 13.8-17.2\nrequests per second, execution times below 72~ms, and governance delays under\n100 ms, analytical projections suggest maintained performance at larger scales.\nCross-domain rules ensured safe interoperability, with traffic rerouting\npermitted only under validated weather conditions. These findings validate\nSORA-ATMAS as a regulation-aligned, context-aware, and verifiable governance\nframework that consolidates distributed agent outputs into accountable,\nreal-time decisions, offering a resilient foundation for smart-city management."}
{"id": "2510.19783", "categories": ["cs.NI", "cs.PF"], "pdf": "https://arxiv.org/pdf/2510.19783", "abs": "https://arxiv.org/abs/2510.19783", "authors": ["Miguel Sánchez de la Rosa", "Francisco J. andújar", "Jesus Escudero-Sahuquillo", "José L. Sánchez", "Francisco J. Alfaro-Cortés"], "title": "On the Power Saving in High-Speed Ethernet-based Networks for Supercomputers and Data Centers", "comment": "Submitted to The Journal of Systems Architecture. Currently under\n  revision", "summary": "The increase in computation and storage has led to a significant growth in\nthe scale of systems powering applications and services, raising concerns about\nsustainability and operational costs. In this paper, we explore power-saving\ntechniques in high-performance computing (HPC) and datacenter networks, and\ntheir relation with performance degradation. From this premise, we propose\nleveraging Energy Efficient Ethernet (EEE), with the flexibility to extend to\nconventional Ethernet or upcoming Ethernet-derived interconnect versions of BXI\nand Omnipath.\n  We analyze the PerfBound proposal, identifying possible improvements and\nmodeling it into a simulation framework. Through different experiments, we\nexamine its impact on performance and determine the most appropriate\ninterconnect. We also study traffic patterns generated by selected HPC and\nmachine learning applications to evaluate the behavior of power-saving\ntechniques.\n  From these experiments, we provide an analysis of how applications affect\nsystem and network energy consumption. Based on this, we disclose the weakness\nof dynamic power-down mechanisms and propose an approach that improves energy\nreduction with minimal or no performance penalty. To our knowledge, this is the\nfirst power management proposal tailored to future Ethernet-based HPC\narchitectures, with promising results."}
{"id": "2510.18923", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.18923", "abs": "https://arxiv.org/abs/2510.18923", "authors": ["Eduard Frankford", "Tobias Antensteiner", "Michael Vierhauser", "Clemens Sauerwein", "Vivien Wallner", "Iris Groher", "Reinhold Plösch", "Ruth Breu"], "title": "A Survey on Feedback Types in Automated Programming Assessment Systems", "comment": null, "summary": "With the recent rapid increase in digitization across all major industries,\nacquiring programming skills has increased the demand for introductory\nprogramming courses. This has further resulted in universities integrating\nprogramming courses into a wide range of curricula, including not only\ntechnical studies but also business and management fields of study.\n  Consequently, additional resources are needed for teaching, grading, and\ntutoring students with diverse educational backgrounds and skills. As part of\nthis, Automated Programming Assessment Systems (APASs) have emerged, providing\nscalable and high-quality assessment systems with efficient evaluation and\ninstant feedback. Commonly, APASs heavily rely on predefined unit tests for\ngenerating feedback, often limiting the scope and level of detail of feedback\nthat can be provided to students. With the rise of Large Language Models (LLMs)\nin recent years, new opportunities have emerged as these technologies can\nenhance feedback quality and personalization.\n  To investigate how different feedback mechanisms in APASs are perceived by\nstudents, and how effective they are in supporting problem-solving, we have\nconducted a large-scale study with over 200 students from two different\nuniversities. Specifically, we compare baseline Compiler Feedback, standard\nUnit Test Feedback, and advanced LLM-based Feedback regarding perceived quality\nand impact on student performance.\n  Results indicate that while students rate unit test feedback as the most\nhelpful, AI-generated feedback leads to significantly better performances.\nThese findings suggest combining unit tests and AI-driven guidance to optimize\nautomated feedback mechanisms and improve learning outcomes in programming\neducation."}
{"id": "2510.19577", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2510.19577", "abs": "https://arxiv.org/abs/2510.19577", "authors": ["Zuoming Fu", "Alex Manley", "Mohammad Alian"], "title": "gem5 Co-Pilot: AI Assistant Agent for Architectural Design Space Exploration", "comment": "Accepted by CAMS25, October, 2025, Seoul, Republic of Korea", "summary": "Generative AI is increasing the productivity of software and hardware\ndevelopment across many application domains. In this work, we utilize the power\nof Large Language Models (LLMs) to develop a co-pilot agent for assisting gem5\nusers with automating design space exploration. Computer architecture design\nspace exploration is complex and time-consuming, given that numerous parameter\nsettings and simulation statistics must be analyzed before improving the\ncurrent design. The emergence of LLMs has significantly accelerated the\nanalysis of long-text data as well as smart decision making, two key functions\nin a successful design space exploration task. In this project, we first build\ngem5 Co-Pilot, an AI agent assistant for gem5, which comes with a webpage-GUI\nfor smooth user interaction, agent automation, and result summarization. We\nalso implemented a language for design space exploration, as well as a Design\nSpace Database (DSDB). With DSDB, gem5 Co-Pilot effectively implements a\nRetrieval Augmented Generation system for gem5 design space exploration. We\nexperiment on cost-constraint optimization with four cost ranges and compare\nour results with two baseline models. Results show that gem5 Co-Pilot can\nquickly identify optimal parameters for specific design constraints based on\nperformance and cost, with limited user interaction."}
{"id": "2510.18897", "categories": ["cs.DC", "cs.AI", "cs.DB", "cs.SE"], "pdf": "https://arxiv.org/pdf/2510.18897", "abs": "https://arxiv.org/abs/2510.18897", "authors": ["Jacopo Tagliabue"], "title": "AI for Distributed Systems Design: Scalable Cloud Optimization Through Repeated LLMs Sampling And Simulators", "comment": "Pre-print IAAA workshop submission", "summary": "We explore AI-driven distributed-systems policy design by combining\nstochastic code generation from large language models (LLMs) with deterministic\nverification in a domain-specific simulator. Using a Function-as-a-Service\nruntime (Bauplan) and its open-source simulator (Eudoxia) as a case study, we\nframe scheduler design as an iterative generate-and-verify loop: an LLM\nproposes a Python policy, the simulator evaluates it on standardized traces,\nand structured feedback steers subsequent generations. This setup preserves\ninterpretability while enabling targeted search over a large design space. We\ndetail the system architecture and report preliminary results on throughput\nimprovements across multiple models. Beyond early gains, we discuss the limits\nof the current setup and outline next steps; in particular, we conjecture that\nAI will be crucial for scaling this methodology by helping to bootstrap new\nsimulators."}
{"id": "2510.19386", "categories": ["cs.MA", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.19386", "abs": "https://arxiv.org/abs/2510.19386", "authors": ["Ning Li", "Qiqiang Lin", "Zheng Wu", "Xiaoyun Mo", "Weiming Zhang", "Yin Zhao", "Xiangmou Qu", "Jiamu Zhou", "Jun Wang", "Congmin Zheng", "Yuanyi Song", "Hongjiang Chen", "Heyuan Huang", "Jihong Wang", "Jiaxin Yin", "Jingwei Yu", "Junwei Liao", "Qiuying Peng", "Xingyu Lou", "Jun Wang", "Weiwen Liu", "Zhuosheng Zhang", "Weinan Zhang"], "title": "ColorAgent: Building A Robust, Personalized, and Interactive OS Agent", "comment": null, "summary": "With the advancements in hardware, software, and large language model\ntechnologies, the interaction between humans and operating systems has evolved\nfrom the command-line interface to the rapidly emerging AI agent interactions.\nBuilding an operating system (OS) agent capable of executing user instructions\nand faithfully following user desires is becoming a reality. In this technical\nreport, we present ColorAgent, an OS agent designed to engage in long-horizon,\nrobust interactions with the environment while also enabling personalized and\nproactive user interaction. To enable long-horizon interactions with the\nenvironment, we enhance the model's capabilities through step-wise\nreinforcement learning and self-evolving training, while also developing a\ntailored multi-agent framework that ensures generality, consistency, and\nrobustness. In terms of user interaction, we explore personalized user intent\nrecognition and proactive engagement, positioning the OS agent not merely as an\nautomation tool but as a warm, collaborative partner. We evaluate ColorAgent on\nthe AndroidWorld and AndroidLab benchmarks, achieving success rates of 77.2%\nand 50.7%, respectively, establishing a new state of the art. Nonetheless, we\nnote that current benchmarks are insufficient for a comprehensive evaluation of\nOS agents and propose further exploring directions in future work, particularly\nin the areas of evaluation paradigms, agent collaboration, and security. Our\ncode is available at https://github.com/MadeAgents/mobile-use."}
{"id": "2510.19262", "categories": ["cs.DC", "cs.NI"], "pdf": "https://arxiv.org/pdf/2510.19262", "abs": "https://arxiv.org/abs/2510.19262", "authors": ["Heng Xu", "Zhiwei Yu", "Chengze Du", "Ying Zhou", "Letian Li", "Haojie Wang", "Weiqiang Cheng", "Jialong Li"], "title": "RailS: Load Balancing for All-to-All Communication in Distributed Mixture-of-Experts Training", "comment": null, "summary": "Training Mixture-of-Experts (MoE) models introduces sparse and highly\nimbalanced all-to-all communication that dominates iteration time. Conventional\nload-balancing methods fail to exploit the deterministic topology of Rail\narchitectures, leaving multi-NIC bandwidth underutilized. We present RailS, a\ndistributed load-balancing framework that minimizes all-to-all completion time\nin MoE training. RailS leverages the Rail topology's symmetry to prove that\nuniform sending ensures uniform receiving, transforming global coordination\ninto local scheduling. Each node independently executes a Longest Processing\nTime First (LPT) spraying scheduler to proactively balance traffic using local\ninformation. RailS activates N parallel rails for fine-grained, topology-aware\nmultipath transmission. Across synthetic and real-world MoE workloads, RailS\nimproves bus bandwidth by 20%--78% and reduces completion time by 17%--78%. For\nMixtral workloads, it shortens iteration time by 18%--40% and achieves\nnear-optimal load balance, fully exploiting architectural parallelism in\ndistributed training."}
{"id": "2510.19035", "categories": ["cs.SE", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2510.19035", "abs": "https://arxiv.org/abs/2510.19035", "authors": ["Amirreza Hosseini", "Amro M. Farid"], "title": "Extending Resource Constrained Project Scheduling to Mega-Projects with Model-Based Systems Engineering & Hetero-functional Graph Theory", "comment": null, "summary": "Within the project management context, project scheduling serves as an\nindispensable component, functioning as a fundamental tool for planning,\nmonitoring, controlling, and managing projects more broadly. Although the\nresource-constrained project scheduling problem (RCPSP) lies at the core of\nproject management activities, it remains largely disconnected from the broader\nliterature on model-based systems engineering (MBSE), thereby limiting its\nintegration into the design and management of complex systems. The original\ncontribution of this paper is twofold. First, the paper seeks to reconcile the\nRCPSP with the broader literature and vocabulary of model-based systems\nengineering and hetero-functional graph theory (HFGT). A concrete translation\npipeline from an activity-on-node network to a SysML activity diagram, and then\nto an operand net is constructed. Using this representation, it specializes the\nhetero-functional network minimum-cost flow (HFNMCF) formulation to the RCPSP\ncontext as a systematic means of HFGT for quantitative analysis and proves that\nthe RCPSP is recoverable as a special case of a broader model. Secondly, on an\nillustrative instance with renewable and non-renewable operands, the\nspecialized HFNMCF, while producing similar schedules, yields explicit\nexplanations of the project states that enable richer monitoring and control.\nOverall, the framework preserves the strengths of the classical RCPSP while\naccommodating real-world constraints and enterprise-level decision processes\nencountered in large, complex megaprojects."}
{"id": "2510.19012", "categories": ["cs.DC", "cs.DB", "cs.PL", "cs.SE"], "pdf": "https://arxiv.org/pdf/2510.19012", "abs": "https://arxiv.org/abs/2510.19012", "authors": ["Ivan Borodii", "Illia Fedorovych", "Halyna Osukhivska", "Diana Velychko", "Roman Butsii"], "title": "Comparative analysis of large data processing in Apache Spark using Java, Python and Scala", "comment": "CITI 2025, 3rd International Workshop on Computer Information\n  Technologies in Industry 4.0, June 11-12, 2025, Ternopil, Ukraine. The\n  article includes 10 pages, 5 figures, 9 tables", "summary": "During the study, the results of a comparative analysis of the process of\nhandling large datasets using the Apache Spark platform in Java, Python, and\nScala programming languages were obtained. Although prior works have focused on\nindividual stages, comprehensive comparisons of full ETL workflows across\nprogramming languages using Apache Iceberg remain limited. The analysis was\nperformed by executing several operations, including downloading data from CSV\nfiles, transforming and loading it into an Apache Iceberg analytical table. It\nwas found that the performance of the Spark algorithm varies significantly\ndepending on the amount of data and the programming language used. When\nprocessing a 5-megabyte CSV file, the best result was achieved in Python: 6.71\nseconds, which is superior to Scala's score of 9.13 seconds and Java's time of\n9.62 seconds. For processing a large CSV file of 1.6 gigabytes, all programming\nlanguages demonstrated similar results: the fastest performance was showed in\nPython: 46.34 seconds, while Scala and Java showed results of 47.72 and 50.56\nseconds, respectively. When performing a more complex operation that involved\ncombining two CSV files into a single dataset for further loading into an\nApache Iceberg table, Scala demonstrated the highest performance, at 374.42\nseconds. Java processing was completed in 379.8 seconds, while Python was the\nleast efficient, with a runtime of 398.32 seconds. It follows that the\nprogramming language significantly affects the efficiency of data processing by\nthe Apache Spark algorithm, with Scala and Java being more productive for\nprocessing large amounts of data and complex operations, while Python\ndemonstrates an advantage in working with small amounts of data. The results\nobtained can be useful for optimizing data handling processes depending on\nspecific performance requirements and the amount of information being\nprocessed."}
{"id": "2510.19497", "categories": ["cs.MA", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.19497", "abs": "https://arxiv.org/abs/2510.19497", "authors": ["Trung-Dung Vu", "Benoit Gaudou", "Kamaldeep Singh Oberoi"], "title": "Modeling realistic human behavior using generative agents in a multimodal transport system: Software architecture and Application to Toulouse", "comment": null, "summary": "Modeling realistic human behaviour to understand people's mode choices in\norder to propose personalised mobility solutions remains challenging. This\npaper presents an architecture for modeling realistic human mobility behavior\nin complex multimodal transport systems, demonstrated through a case study in\nToulouse, France. We apply Large Language Models (LLMs) within an agent-based\nsimulation to capture decision-making in a real urban setting. The framework\nintegrates the GAMA simulation platform with an LLM-based generative agent,\nalong with General Transit Feed Specification (GTFS) data for public transport,\nand OpenTripPlanner for multimodal routing. GAMA platform models the\ninteractive transport environment, providing visualization and dynamic agent\ninteractions while eliminating the need to construct the simulation environment\nfrom scratch. This design enables a stronger focus on developing generative\nagents and evaluating their performance in transport decision-making processes.\nOver a simulated month, results show that agents not only make context-aware\ntransport decisions but also form habits over time. We conclude that combining\nLLMs with agent-based simulation offers a promising direction for advancing\nintelligent transportation systems and personalised multimodal mobility\nsolutions. We also discuss some limitations of this approach and outline future\nwork on scaling to larger regions, integrating real-time data, and refining\nmemory models."}
{"id": "2510.19725", "categories": ["cs.DC", "cs.NI", "C.2.m; G.2.m"], "pdf": "https://arxiv.org/pdf/2510.19725", "abs": "https://arxiv.org/abs/2510.19725", "authors": ["Jingfan Meng", "Tianji Yang", "Jun Xu"], "title": "CommonSense: Efficient Set Intersection (SetX) Protocol Based on Compressed Sensing", "comment": null, "summary": "In the set reconciliation (\\textsf{SetR}) problem, two parties Alice and Bob,\nholding sets $\\mathsf{A}$ and $\\mathsf{B}$, communicate to learn the symmetric\ndifference $\\mathsf{A} \\Delta \\mathsf{B}$. In this work, we study a related but\nunder-explored problem: set intersection (\\textsf{SetX})~\\cite{Ozisik2019},\nwhere both parties learn $\\mathsf{A} \\cap \\mathsf{B}$ instead. However,\nexisting solutions typically reuse \\textsf{SetR} protocols due to the absence\nof dedicated \\textsf{SetX} protocols and the misconception that \\textsf{SetR}\nand \\textsf{SetX} have comparable costs. Observing that \\textsf{SetX} is\nfundamentally cheaper than \\textsf{SetR}, we developed a multi-round\n\\textsf{SetX} protocol that outperforms the information-theoretic lower bound\nof \\textsf{SetR} problem. In our \\textsf{SetX} protocol, Alice sends Bob a\ncompressed sensing (CS) sketch of $\\mathsf{A}$ to help Bob identify his unique\nelements (those in $\\mathsf{B \\setminus A}$). This solves the \\textsf{SetX}\nproblem, if $\\mathsf{A} \\subseteq \\mathsf{B}$. Otherwise, Bob sends a CS sketch\nof the residue (a set of elements he cannot decode) back to Alice for her to\ndecode her unique elements (those in $\\mathsf{A \\setminus B}$). As such, Alice\nand Bob communicate back and forth %with a set membership filter (SMF) of\nestimated $\\mathsf{B \\setminus A}$. Alice updates $\\mathsf{A}$ and\ncommunication repeats until both parties agrees on $\\mathsf{A} \\cap\n\\mathsf{B}$. On real world datasets, experiments show that our $\\mathsf{SetX}$\nprotocol reduces the communication cost by 8 to 10 times compared to the\nIBLT-based $\\mathsf{SetR}$ protocol."}
{"id": "2510.19089", "categories": ["cs.SE", "D.2.4; K.6.3"], "pdf": "https://arxiv.org/pdf/2510.19089", "abs": "https://arxiv.org/abs/2510.19089", "authors": ["Érik Martin-Dorel"], "title": "Docker-based CI/CD for Rocq/OCaml projects", "comment": "26 pages, 17 figures, 3 tables, 16 references", "summary": "This paper presents three closely-related software projects, namely:\ndocker-coq, docker-coq-action, and docker-keeper. It aims at two objectives:\nprovide a high-level description of the available features -- to foster the use\nof a Docker-based CI/CD for Rocq (formerly known as Coq) or OCaml projects --\nand document the underlying requirements and the main design choices of these\nthree DevOps tools -- to help their future maintainers."}
{"id": "2510.19151", "categories": ["cs.DC", "cs.DS"], "pdf": "https://arxiv.org/pdf/2510.19151", "abs": "https://arxiv.org/abs/2510.19151", "authors": ["Seri Khoury", "Manish Purohit", "Aaron Schild", "Joshua Wang"], "title": "On the Randomized Locality of Matching Problems in Regular Graphs", "comment": "DISC 2025. Abstract modified for arXiv", "summary": "The main goal in distributed symmetry-breaking is to understand the locality\nof problems; i.e., the radius of the neighborhood that a node needs to explore\nin order to arrive at its part of a global solution. In this work, we study the\nlocality of matching problems in the family of regular graphs, which is one of\nthe main benchmarks for establishing lower bounds on the locality of\nsymmetry-breaking problems, as well as for obtaining classification results.\nFor approximate matching, we develop randomized algorithms to show that $(1 +\n\\epsilon)$-approximate matching in regular graphs is truly local; i.e., the\nlocality depends only on $\\epsilon$ and is independent of all other graph\nparameters. Furthermore, as long as the degree $\\Delta$ is not very small\n(namely, as long as $\\Delta \\geq \\text{poly}(1/\\epsilon)$), this dependence is\nonly logarithmic in $1/\\epsilon$. This stands in sharp contrast to maximal\nmatching in regular graphs which requires some dependence on the number of\nnodes $n$ or the degree $\\Delta$. We show matching lower bounds for both\nresults. For maximal matching, our techniques further allow us to establish a\nstrong separation between the node-averaged complexity and worst-case\ncomplexity of maximal matching in regular graphs, by showing that the former is\nonly $O(1)$. Central to our main technical contribution is a novel\nmartingale-based analysis for the $\\approx 40$-year-old algorithm by Luby. In\nparticular, our analysis shows that applying one round of Luby's algorithm on\nthe line graph of a $\\Delta$-regular graph results in an almost\n$\\Delta/2$-regular graph."}
{"id": "2510.19567", "categories": ["cs.MA"], "pdf": "https://arxiv.org/pdf/2510.19567", "abs": "https://arxiv.org/abs/2510.19567", "authors": ["Takahiro Suzuki", "Keisuke Okumura"], "title": "Polynomial-time Configuration Generator for Connected Unlabeled Multi-Agent Pathfinding", "comment": null, "summary": "We consider Connected Unlabeled Multi-Agent Pathfinding (CUMAPF), a variant\nof MAPF where the agents must maintain connectivity at all times. This problem\nis fundamental to swarm robotics applications like self-reconfiguration and\nmarching, where standard MAPF is insufficient as it does not guarantee the\nrequired connectivity between agents. While unlabeled MAPF is tractable in\noptimization, CUMAPF is NP-hard even on highly restricted graph classes. To\ntackle this challenge, we propose PULL, a complete and polynomial-time\nalgorithm with a simple design. It is based on a rule-based one-step function\nthat computes a subsequent configuration that preserves connectivity and\nadvances towards the target configuration. PULL is lightweight, and runs in\n$O(n^2)$ time per step in 2D grid, where $n$ is the number of agents. Our\nexperiments further demonstrate its practical performance: PULL finds\ncompetitive solution qualities against trivial solutions for hundreds of\nagents, in randomly generated instances. Furthermore, we develop an eventually\noptimal solver that integrates PULL into an existing search-based MAPF\nalgorithm, providing a valuable tool for small-scale instances."}
{"id": "2510.19237", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.19237", "abs": "https://arxiv.org/abs/2510.19237", "authors": ["Dongming Jin", "Zhi Jin", "Xiaohong Chen", "Zheng Fang", "Linyu Li", "Shengxin Zhao", "Chuihui Wang", "Hongbin Xiao"], "title": "Automated Concern Extraction from Textual Requirements of Cyber-Physical Systems: A Multi-solution Study", "comment": "27 pages, 3 figures", "summary": "Cyber-physical systems (CPSs) are characterized by a deep integration of the\ninformation space and the physical world, which makes the extraction of\nrequirements concerns more challenging. Some automated solutions for\nrequirements concern extraction have been proposed to alleviate the burden on\nrequirements engineers. However, evaluating the effectiveness of these\nsolutions, which relies on fair and comprehensive benchmarks, remains an open\nquestion. To address this gap, we propose ReqEBench, a new CPSs requirements\nconcern extraction benchmark, which contains 2,721 requirements from 12\nreal-world CPSs. ReqEBench offers four advantages. It aligns with real-world\nCPSs requirements in multiple dimensions, e.g., scale and complexity. It covers\ncomprehensive concerns related to CPSs requirements. It undergoes a rigorous\nannotation process. It covers multiple application domains of CPSs, e.g.,\naerospace and healthcare. We conducted a comparative study on three types of\nautomated requirements concern extraction solutions and revealed their\nperformance in real-world CPSs using our ReqEBench. We found that the highest\nF1 score of GPT-4 is only 0.24 in entity concern extraction. We further analyze\nfailure cases of popular LLM-based solutions, summarize their shortcomings, and\nprovide ideas for improving their capabilities. We believe ReqEBench will\nfacilitate the evaluation and development of automated requirements concern\nextraction."}
{"id": "2510.19225", "categories": ["cs.DC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.19225", "abs": "https://arxiv.org/abs/2510.19225", "authors": ["Yongji Wu", "Xueshen Liu", "Haizhong Zheng", "Juncheng Gu", "Beidi Chen", "Z. Morley Mao", "Arvind Krishnamurthy", "Ion Stoica"], "title": "RLBoost: Harvesting Preemptible Resources for Cost-Efficient Reinforcement Learning on LLMs", "comment": null, "summary": "Reinforcement learning (RL) has become essential for unlocking advanced\nreasoning capabilities in large language models (LLMs). RL workflows involve\ninterleaving rollout and training stages with fundamentally different resource\nrequirements. Rollout typically dominates overall execution time, yet scales\nefficiently through multiple independent instances. In contrast, training\nrequires tightly-coupled GPUs with full-mesh communication. Existing RL\nframeworks fall into two categories: co-located and disaggregated\narchitectures. Co-located ones fail to address this resource tension by forcing\nboth stages to share the same GPUs. Disaggregated architectures, without\nmodifications of well-established RL algorithms, suffer from resource\nunder-utilization. Meanwhile, preemptible GPU resources, i.e., spot instances\non public clouds and spare capacity in production clusters, present significant\ncost-saving opportunities for accelerating RL workflows, if efficiently\nharvested for rollout.\n  In this paper, we present RLBoost, a systematic solution for cost-efficient\nRL training that harvests preemptible GPU resources. Our key insight is that\nrollout's stateless and embarrassingly parallel nature aligns perfectly with\npreemptible and often fragmented resources. To efficiently utilize these\nresources despite frequent and unpredictable availability changes, RLBoost\nadopts a hybrid architecture with three key techniques: (1) adaptive rollout\noffload to dynamically adjust workloads on the reserved (on-demand) cluster,\n(2) pull-based weight transfer that quickly provisions newly available\ninstances, and (3) token-level response collection and migration for efficient\npreemption handling and continuous load balancing. Extensive experiments show\nRLBoost increases training throughput by 1.51x-1.97x while improving cost\nefficiency by 28%-49% compared to using only on-demand GPU resources."}
{"id": "2510.19692", "categories": ["cs.SE", "cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2510.19692", "abs": "https://arxiv.org/abs/2510.19692", "authors": ["Rashina Hoda"], "title": "Toward Agentic Software Engineering Beyond Code: Framing Vision, Values, and Vocabulary", "comment": "5 pages", "summary": "Agentic AI is poised to usher in a seismic paradigm shift in Software\nEngineering (SE). As technologists rush head-along to make agentic AI a\nreality, SE researchers are driven to establish agentic SE as a research area.\nWhile early visions of agentic SE are primarily focused on code-related\nactivities, early empirical evidence calls for a consideration of a range of\nsocio-technical concerns to make it work in practice. This paper contributes to\nthe emerging community vision by: (a) recommending an expansion of its scope\nbeyond code, toward a 'whole of process' vision, grounding it in SE foundations\nand evolution and emerging agentic SE frameworks, (b) proposing a preliminary\nset of values and principles to guide efforts, and (c) sharing guidance on\ndesigning/using well-defined vocabulary for agentic SE. It is hoped that these\nideas will encourage community collaborations and steer the SE community\ntowards laying strong foundations of agentic SE so its not only inevitable but\nalso deliberate and desirable in the long run."}
{"id": "2510.19240", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.19240", "abs": "https://arxiv.org/abs/2510.19240", "authors": ["Behnam Agahi", "Hamed Farbeh"], "title": "A General Solution for the Implementation of CI/CD in Embedded Linux Development", "comment": null, "summary": "With the growing use of embedded systems in various industries, the need for\nautomated platforms for the development and deployment of customized\nLinux-based operating systems has become more important. This research was\nconducted with the aim of designing and implementing an integrated and\nreproducible infrastructure for the development, building, and testing of a\nLinux-based operating system using the Yocto Project. The proposed structure\nwas implemented based on a three-layer architecture consisting of the main\nYocto repositories, a custom layer (meta-custom), and a coordinating manifest\nlayer to ensure version synchronization, scalability, and reproducibility.\nThree sample projects, including libhelloworld, helloworld, and the kernel\nmodule hello mod, were developed and integrated into the build process.\nContinuous Integration and Continuous Deployment pipelines were implemented\nwith GitLab CI and combined with an isolated Docker environment to automate and\nstreamline the build and testing workflows. Using a local cache server\ncontaining hashserv, downloads and sstate cache significantly reduced the build\ntime. The functionality and stability of the system were verified through six\nboot test scenarios in the QEMU simulator. The results show that the proposed\ndesign not only ensures reproducibility but also can be extended to advanced\napplications such as continuous deployment of real-time Linux versions. Future\nrecommendations include expanding automated tests, implementing system\nmonitoring with Prometheus and Grafana, using distributed builds, optimizing\nwith Docker multi-stage builds, and enabling continuous deployment of real-time\nLinux changes to provide a stable and scalable model for industrial and\nresearch projects in embedded systems with a rapid and reliable development\ncycle."}
{"id": "2510.19262", "categories": ["cs.DC", "cs.NI"], "pdf": "https://arxiv.org/pdf/2510.19262", "abs": "https://arxiv.org/abs/2510.19262", "authors": ["Heng Xu", "Zhiwei Yu", "Chengze Du", "Ying Zhou", "Letian Li", "Haojie Wang", "Weiqiang Cheng", "Jialong Li"], "title": "RailS: Load Balancing for All-to-All Communication in Distributed Mixture-of-Experts Training", "comment": null, "summary": "Training Mixture-of-Experts (MoE) models introduces sparse and highly\nimbalanced all-to-all communication that dominates iteration time. Conventional\nload-balancing methods fail to exploit the deterministic topology of Rail\narchitectures, leaving multi-NIC bandwidth underutilized. We present RailS, a\ndistributed load-balancing framework that minimizes all-to-all completion time\nin MoE training. RailS leverages the Rail topology's symmetry to prove that\nuniform sending ensures uniform receiving, transforming global coordination\ninto local scheduling. Each node independently executes a Longest Processing\nTime First (LPT) spraying scheduler to proactively balance traffic using local\ninformation. RailS activates N parallel rails for fine-grained, topology-aware\nmultipath transmission. Across synthetic and real-world MoE workloads, RailS\nimproves bus bandwidth by 20%--78% and reduces completion time by 17%--78%. For\nMixtral workloads, it shortens iteration time by 18%--40% and achieves\nnear-optimal load balance, fully exploiting architectural parallelism in\ndistributed training."}
{"id": "2510.19254", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.19254", "abs": "https://arxiv.org/abs/2510.19254", "authors": ["Chong Chen", "Jiachi Chen", "Lingfeng Bao", "David Lo", "Yanlin Wang", "Zhenyu Shan", "Ting Chen", "Guangqiang Yin", "Jianxing Yu", "Zibin Zheng"], "title": "Trace: Securing Smart Contract Repository Against Access Control Vulnerability", "comment": null, "summary": "Smart contract vulnerabilities, particularly improper Access Control that\nallows unauthorized execution of restricted functions, have caused billions of\ndollars in losses. GitHub hosts numerous smart contract repositories containing\nsource code, documentation, and configuration files-these serve as intermediate\ndevelopment artifacts that must be compiled and packaged before deployment.\nThird-party developers often reference, reuse, or fork code from these\nrepositories during custom development. However, if the referenced code\ncontains vulnerabilities, it can introduce significant security risks. Existing\ntools for detecting smart contract vulnerabilities are limited in their ability\nto handle complex repositories, as they typically require the target contract\nto be compilable to generate an abstract representation for further analysis.\nThis paper presents TRACE, a tool designed to secure non-compilable smart\ncontract repositories against access control vulnerabilities. TRACE employs\nLLMs to locate sensitive functions involving critical operations (e.g.,\ntransfer) within the contract and subsequently completes function snippets into\na fully compilable contract. TRACE constructs a function call graph from the\nabstract syntax tree (AST) of the completed contract. It uses the control flow\ngraph (CFG) of each function as node information. The nodes of the sensitive\nfunctions are then analyzed to detect Access Control vulnerabilities.\nExperimental results demonstrate that TRACE outperforms state-of-the-art tools\non an open-sourced CVE dataset, detecting 14 out of 15 CVEs. In addition, it\nachieves 89.2% precision on 5,000 recent on-chain contracts, far exceeding the\nbest existing tool at 76.9%. On 83 real-world repositories, TRACE achieves\n87.0% precision, significantly surpassing DeepSeek-R1's 14.3%."}
{"id": "2510.19301", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2510.19301", "abs": "https://arxiv.org/abs/2510.19301", "authors": ["Ziheng Deng", "Xue Liu", "Jiantong Jiang", "Yankai Li", "Qingxu Deng", "Xiaochun Yang"], "title": "FLASH Viterbi: Fast and Adaptive Viterbi Decoding for Modern Data Systems", "comment": "Accepted for ICDE 2026", "summary": "The Viterbi algorithm is a key operator for structured sequence inference in\nmodern data systems, with applications in trajectory analysis, online\nrecommendation, and speech recognition. As these workloads increasingly migrate\nto resource-constrained edge platforms, standard Viterbi decoding remains\nmemory-intensive and computationally inflexible. Existing methods typically\ntrade decoding time for space efficiency, but often incur significant runtime\noverhead and lack adaptability to various system constraints. This paper\npresents FLASH Viterbi, a Fast, Lightweight, Adaptive, and Hardware-Friendly\nViterbi decoding operator that enhances adaptability and resource efficiency.\nFLASH Viterbi combines a non-recursive divide-and-conquer strategy with pruning\nand parallelization techniques to enhance both time and memory efficiency,\nmaking it well-suited for resource-constrained data systems.To further decouple\nspace complexity from the hidden state space size, we present FLASH-BS Viterbi,\na dynamic beam search variant built on a memory-efficient data structure. Both\nproposed algorithms exhibit strong adaptivity to diverse deployment scenarios\nby dynamically tuning internal parameters.To ensure practical deployment on\nedge devices, we also develop FPGA-based hardware accelerators for both\nalgorithms, demonstrating high throughput and low resource usage. Extensive\nexperiments show that our algorithms consistently outperform existing baselines\nin both decoding time and memory efficiency, while preserving adaptability and\nhardware-friendly characteristics essential for modern data systems. All codes\nare publicly available at https://github.com/Dzh-16/FLASH-Viterbi."}
{"id": "2510.19274", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.19274", "abs": "https://arxiv.org/abs/2510.19274", "authors": ["Saurabh Chauhan", "Zeeshan Rasheed", "Malik Abdul Sami", "Kai-Kristian Kemell", "Muhammad Waseem", "Zheying Zhang", "Jussi Rasku", "Mika Saari", "Pekka Abrahamsson"], "title": "From Specification to Service: Accelerating API-First Development Using Multi-Agent Systems", "comment": "9 Figures, 6Tables", "summary": "This paper presents a system that uses Large Language Models (LLMs)-based\nagents to automate the API-first development of RESTful microservices. This\nsystem helps to create an OpenAPI specification, generate server code from it,\nand refine the code through a feedback loop that analyzes execution logs and\nerror messages. The integration of log analysis enables the LLM to detect and\naddress issues efficiently, reducing the number of iterations required to\nproduce functional and robust services. This study's main goal is to advance\nAPI-first development automation for RESTful web services and test the\ncapability of LLM-based multi-agent systems in supporting the API-first\ndevelopment approach. To test the proposed system's potential, we utilized the\nPRAB benchmark. The results indicate that if we keep the OpenAPI specification\nsmall and focused, LLMs are capable of generating complete functional code with\nbusiness logic that aligns to the specification. The code for the system is\npublicly available at https://github.com/sirbh/code-gen"}
{"id": "2510.19470", "categories": ["cs.DC", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.19470", "abs": "https://arxiv.org/abs/2510.19470", "authors": ["Weihao Yang", "Hao Huang", "Donglei Wu", "Ningke Li", "Yanqi Pan", "Qiyang Zheng", "Wen Xia", "Shiyi Li", "Qiang Wang"], "title": "HybridEP: Scaling Expert Parallelism to Cross-Datacenter Scenario via Hybrid Expert/Data Transmission", "comment": null, "summary": "Mixture-of-Experts (MoE) has become a popular architecture for scaling large\nmodels. However, the rapidly growing scale outpaces model training on a single\nDC, driving a shift toward a more flexible, cross-DC training paradigm. Under\nthis, Expert Parallelism (EP) of MoE faces significant scalability issues due\nto the limited cross-DC bandwidth. Specifically, existing EP optimizations\nattempt to overlap data communication and computation, which has little benefit\nin low-bandwidth scenarios due to a much longer data communication time.\nTherefore, the trends of cross-DC EP scaling is fast becoming a critical\nroadblock to the continued growth of MoE models.\n  To address this, we propose HybridEP, a modeling-guided framework to optimize\nEP under constrained bandwidth. Our key idea is to dynamically transform the\nspatial placement of experts to reduce data communication traffic and\nfrequency, thereby minimizing EP's communication overheads. However, it is\nnon-trivial to find the optimal solution because it complicates the original\ncommunication pattern by mixing data and expert communication. We therefore\nbuild a stream-based model to determine the optimal transmission ratio. Guided\nby this, we incorporate two techniques: (1) domain-based partition to construct\nthe mapping between hybrid patterns and specific communication topology at GPU\nlevel, and (2) parameter-efficient migration to further refine this topology by\nreducing expert transmission overhead and enlarging the domain size. Combining\nall these designs, HybridEP can be considered as a more general EP with better\nscalability. Experimental results show that HybridEP outperforms existing\nstate-of-the-art MoE training systems by up to 5.6x under constrained\nbandwidth. We further compare HybridEP and EP on large-scale simulations.\nHybridEP achieves up to 1.45x speedup with 1k DCs under different bandwidths."}
{"id": "2510.19281", "categories": ["cs.SE", "cs.CR", "cs.PL"], "pdf": "https://arxiv.org/pdf/2510.19281", "abs": "https://arxiv.org/abs/2510.19281", "authors": ["Shubham Joshi"], "title": "An Empirical Study of Bitwise Operators Intuitiveness through Performance Metrics", "comment": "15 pages, 10 tables, 9 Figures", "summary": "Objectives: This study aims to investigate the readability and\nunderstandability of bitwise operators in programming, with the main hypothesis\nthat there will be a difference in the performance metrics (response time and\nerror rate) between participants exposed to various bitwise operators related\nquestions and those who are not.\n  Participants: Participants in this human research study include people\nwithout programming background, novice programmers, and university students\nwith varying programming experience (from freshmen to PhD level). There were 23\nparticipants for this study.\n  Study Methods: This study uses an Within-Subjects Experimental Design to\nassess how people with diverse programming backgrounds understand and use\nbitwise operators. Participants complete tasks in JavaScript program, and their\ntask completion time and accuracy of the tasks are recorded for analysis.\n  Findings: The results indicate that operators can be one of the factors\npredicting response time, with a small but significant effect, with R-squared\n0.032, (1, 494) = 16.5, p < .001. Additionally, some operators like OR, NOT,\nand Left Shift showed statistical significance in task completion times\ncompared to other operators.\n  Conclusions: While the complexity of bitwise operators did not generally\nresult in longer task completion times, certain operators were found to be less\nintuitive, suggesting the need for further investigation and potential redesign\nfor improved understandability."}
{"id": "2510.19617", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2510.19617", "abs": "https://arxiv.org/abs/2510.19617", "authors": ["Eric Ding"], "title": "Propius: A Platform for Collaborative Machine Learning across the Edge and the Cloud", "comment": null, "summary": "Collaborative Machine Learning is a paradigm in the field of distributed\nmachine learning, designed to address the challenges of data privacy,\ncommunication overhead, and model heterogeneity. There have been significant\nadvancements in optimization and communication algorithm design and ML hardware\nthat enables fair, efficient and secure collaborative ML training. However,\nless emphasis is put on collaborative ML infrastructure development. Developers\nand researchers often build server-client systems for a specific collaborative\nML use case, which is not scalable and reusable. As the scale of collaborative\nML grows, the need for a scalable, efficient, and ideally multi-tenant resource\nmanagement system becomes more pressing. We propose a novel system, Propius,\nthat can adapt to the heterogeneity of client machines, and efficiently manage\nand control the computation flow between ML jobs and edge resources in a\nscalable fashion. Propius is comprised of a control plane and a data plane. The\ncontrol plane enables efficient resource sharing among multiple collaborative\nML jobs and supports various resource sharing policies, while the data plane\nimproves the scalability of collaborative ML model sharing and result\ncollection. Evaluations show that Propius outperforms existing resource\nmanagement techniques and frameworks in terms of resource utilization (up to\n$1.88\\times$), throughput (up to $2.76$), and job completion time (up to\n$1.26\\times$)."}
{"id": "2510.19393", "categories": ["cs.SE", "cs.CR"], "pdf": "https://arxiv.org/pdf/2510.19393", "abs": "https://arxiv.org/abs/2510.19393", "authors": ["Stefan Schott", "Serena Elisa Ponta", "Wolfram Fischer", "Jonas Klauke", "Eric Bodden"], "title": "Bytecode-centric Detection of Known-to-be-vulnerable Dependencies in Java Projects", "comment": "To be published in: ICSE 2026 Proceedings", "summary": "On average, 71% of the code in typical Java projects comes from open-source\nsoftware (OSS) dependencies, making OSS dependencies the dominant component of\nmodern software code bases. This high degree of OSS reliance comes with a\nconsiderable security risk of adding known security vulnerabilities to a code\nbase. To remedy this risk, researchers and companies have developed various\ndependency scanners, which try to identify inclusions of known-to-be-vulnerable\nOSS dependencies. However, there are still challenges that modern dependency\nscanners do not overcome, especially when it comes to dependency modifications,\nsuch as re-compilations, re-bundlings or re-packagings, which are common in the\nJava ecosystem. To overcome these challenges, we present Jaralyzer, a\nbytecode-centric dependency scanner for Java. Jaralyzer does not rely on the\nmetadata or the source code of the included OSS dependencies being available\nbut directly analyzes a dependency's bytecode. Our evaluation across 56 popular\nOSS components demonstrates that Jaralyzer outperforms other popular dependency\nscanners in detecting vulnerabilities within modified dependencies. It is the\nonly scanner capable of identifying vulnerabilities across all the above\nmentioned types of modifications. But even when applied to unmodified\ndependencies, Jaralyzer outperforms the current state-of-the-art code-centric\nscanner Eclipse Steady by detecting 28 more true vulnerabilities and yielding\n29 fewer false warnings."}
{"id": "2510.19689", "categories": ["cs.DC", "cs.AI", "cs.LG", "C.2.4; H.3.4; I.2.6"], "pdf": "https://arxiv.org/pdf/2510.19689", "abs": "https://arxiv.org/abs/2510.19689", "authors": ["Guilin Zhang", "Wulan Guo", "Ziqi Tan", "Srinivas Vippagunta", "Suchitra Raman", "Shreeshankar Chatterjee", "Ju Lin", "Shang Liu", "Mary Schladenhauffen", "Jeffrey Luo", "Hailong Jiang"], "title": "Serverless GPU Architecture for Enterprise HR Analytics: A Production-Scale BDaaS Implementation", "comment": "10 pages, 7 figures, 4 tables. Accepted to IEEE BigData 2025", "summary": "Industrial and government organizations increasingly depend on data-driven\nanalytics for workforce, finance, and regulated decision processes, where\ntimeliness, cost efficiency, and compliance are critical. Distributed\nframeworks such as Spark and Flink remain effective for massive-scale batch or\nstreaming analytics but introduce coordination complexity and auditing\noverheads that misalign with moderate-scale, latency-sensitive inference.\nMeanwhile, cloud providers now offer serverless GPUs, and models such as TabNet\nenable interpretable tabular ML, motivating new deployment blueprints for\nregulated environments. In this paper, we present a production-oriented Big\nData as a Service (BDaaS) blueprint that integrates a single-node serverless\nGPU runtime with TabNet. The design leverages GPU acceleration for throughput,\nserverless elasticity for cost reduction, and feature-mask interpretability for\nIL4/FIPS compliance. We conduct benchmarks on the HR, Adult, and BLS datasets,\ncomparing our approach against Spark and CPU baselines. Our results show that\nGPU pipelines achieve up to 4.5x higher throughput, 98x lower latency, and 90%\nlower cost per 1K inferences compared to Spark baselines, while compliance\nmechanisms add only ~5.7 ms latency with p99 < 22 ms. Interpretability remains\nstable under peak load, ensuring reliable auditability. Taken together, these\nfindings provide a compliance-aware benchmark, a reproducible Helm-packaged\nblueprint, and a decision framework that demonstrate the practicality of\nsecure, interpretable, and cost-efficient serverless GPU analytics for\nregulated enterprise and government settings."}
{"id": "2510.19438", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.19438", "abs": "https://arxiv.org/abs/2510.19438", "authors": ["Linfeng Liang", "Chenkai Tan", "Yao Deng", "Yingfeng Cai", "T. Y Chen", "Xi Zheng"], "title": "AutoMT: A Multi-Agent LLM Framework for Automated Metamorphic Testing of Autonomous Driving Systems", "comment": null, "summary": "Autonomous Driving Systems (ADS) are safety-critical, where failures can be\nsevere. While Metamorphic Testing (MT) is effective for fault detection in ADS,\nexisting methods rely heavily on manual effort and lack automation. We present\nAutoMT, a multi-agent MT framework powered by Large Language Models (LLMs) that\nautomates the extraction of Metamorphic Relations (MRs) from local traffic\nrules and the generation of valid follow-up test cases. AutoMT leverages LLMs\nto extract MRs from traffic rules in Gherkin syntax using a predefined\nontology. A vision-language agent analyzes scenarios, and a search agent\nretrieves suitable MRs from a RAG-based database to generate follow-up cases\nvia computer vision. Experiments show that AutoMT achieves up to 5 x higher\ntest diversity in follow-up case generation compared to the best baseline\n(manual expert-defined MRs) in terms of validation rate, and detects up to\n20.55% more behavioral violations. While manual MT relies on a fixed set of\npredefined rules, AutoMT automatically extracts diverse metamorphic relations\nthat augment real-world datasets and help uncover corner cases often missed\nduring in-field testing and data collection. Its modular architecture\nseparating MR extraction, filtering, and test generation supports integration\ninto industrial pipelines and potentially enables simulation-based testing to\nsystematically cover underrepresented or safety-critical scenarios."}
{"id": "2510.19725", "categories": ["cs.DC", "cs.NI", "C.2.m; G.2.m"], "pdf": "https://arxiv.org/pdf/2510.19725", "abs": "https://arxiv.org/abs/2510.19725", "authors": ["Jingfan Meng", "Tianji Yang", "Jun Xu"], "title": "CommonSense: Efficient Set Intersection (SetX) Protocol Based on Compressed Sensing", "comment": null, "summary": "In the set reconciliation (\\textsf{SetR}) problem, two parties Alice and Bob,\nholding sets $\\mathsf{A}$ and $\\mathsf{B}$, communicate to learn the symmetric\ndifference $\\mathsf{A} \\Delta \\mathsf{B}$. In this work, we study a related but\nunder-explored problem: set intersection (\\textsf{SetX})~\\cite{Ozisik2019},\nwhere both parties learn $\\mathsf{A} \\cap \\mathsf{B}$ instead. However,\nexisting solutions typically reuse \\textsf{SetR} protocols due to the absence\nof dedicated \\textsf{SetX} protocols and the misconception that \\textsf{SetR}\nand \\textsf{SetX} have comparable costs. Observing that \\textsf{SetX} is\nfundamentally cheaper than \\textsf{SetR}, we developed a multi-round\n\\textsf{SetX} protocol that outperforms the information-theoretic lower bound\nof \\textsf{SetR} problem. In our \\textsf{SetX} protocol, Alice sends Bob a\ncompressed sensing (CS) sketch of $\\mathsf{A}$ to help Bob identify his unique\nelements (those in $\\mathsf{B \\setminus A}$). This solves the \\textsf{SetX}\nproblem, if $\\mathsf{A} \\subseteq \\mathsf{B}$. Otherwise, Bob sends a CS sketch\nof the residue (a set of elements he cannot decode) back to Alice for her to\ndecode her unique elements (those in $\\mathsf{A \\setminus B}$). As such, Alice\nand Bob communicate back and forth %with a set membership filter (SMF) of\nestimated $\\mathsf{B \\setminus A}$. Alice updates $\\mathsf{A}$ and\ncommunication repeats until both parties agrees on $\\mathsf{A} \\cap\n\\mathsf{B}$. On real world datasets, experiments show that our $\\mathsf{SetX}$\nprotocol reduces the communication cost by 8 to 10 times compared to the\nIBLT-based $\\mathsf{SetR}$ protocol."}
{"id": "2510.19460", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.19460", "abs": "https://arxiv.org/abs/2510.19460", "authors": ["Thomas I. Strasser", "Edmund Widl", "Carlos Ayon Mac Gregor", "Mirko Ginocchi", "Rene Kuchenbuch"], "title": "Mapping and Evolving Interoperability Testing in European Energy Systems: The int:net Perspective", "comment": "2025 IEEE PES Innovative Smart Grid Technologies Conference Europe\n  (ISGT Europe)", "summary": "The ongoing transformation of the European energy landscape, driven by the\nintegration of renewable energy sources, digital technologies, and\ndecentralized systems, requires a high degree of interoperability across\ndiverse components and systems. Ensuring that these elements can exchange\ninformation and operate together reliably is essential for achieving a secure,\nflexible, and efficient energy supply infrastructure. While several initiatives\nhave contributed to the development of smart grid testing infrastructures, they\ndo not provide a dedicated or comprehensive focus on interoperability testing.\nA structured and harmonized overview of interoperability testing capabilities\nacross Europe is therefore still missing. This work therefore presents a novel\ncontribution by analyzing the European interoperability testing facility\nlandscape through a structured survey of 30 facilities. It provides a\ncategorized inventory of testing infrastructures, applied methodologies, and\nreference test cases, and introduces a blueprint for the development of future\ntesting environments. The findings contribute to the establishment of a\ncoordinated European ecosystem for interoperability testing, supporting\ncollaboration, innovation, and alignment with the goals of the energy\ntransition."}
{"id": "2510.19805", "categories": ["cs.DC", "cs.DB"], "pdf": "https://arxiv.org/pdf/2510.19805", "abs": "https://arxiv.org/abs/2510.19805", "authors": ["Carl-Johan Fauvelle Munck af Rosensch\"old", "Feras M. Awaysheh", "Ahmad Awad"], "title": "Next Generation Cloud-native In-Memory Stores: From Redis to Valkey and Beyond", "comment": "10 pages, 5 figures, 2 algorithms, 4 tables", "summary": "In-memory key-value datastores have become indispensable building blocks of\nmodern cloud-native infrastructures, yet their evolution faces scalability,\ncompatibility, and sustainability constraints. The current literature lacks an\nexperimental evaluation of state-of-the-art tools in the domain. This study\naddressed this timely gap by benchmarking Redis alternatives and systematically\nevaluating Valkey, KeyDB, and Garnet under realistic workloads within\nKubernetes deployments. The results demonstrate clear trade-offs among the\nbenchmarked data systems. Our study presents a comprehensive performance and\nviability assessment of the emerging in-memory key-value stores. Metrics\ninclude throughput, tail latency, CPU and memory efficiency, and migration\ncomplexity. We highlight trade-offs between performance, compatibility, and\nlong-term viability, including project maturity, community support, and\nsustained development."}
{"id": "2510.19593", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.19593", "abs": "https://arxiv.org/abs/2510.19593", "authors": ["Aoyang Fang", "Haowen Yang", "Haoze Dong", "Qisheng Lu", "Junjielong Xu", "Pinjia He"], "title": "A Goal-Driven Survey on Root Cause Analysis", "comment": null, "summary": "Root Cause Analysis (RCA) is a crucial aspect of incident management in\nlarge-scale cloud services. While the term root cause analysis or RCA has been\nwidely used, different studies formulate the task differently. This is because\nthe term \"RCA\" implicitly covers tasks with distinct underlying goals. For\ninstance, the goal of localizing a faulty service for rapid triage is\nfundamentally different from identifying a specific functional bug for a\ndefinitive fix. However, previous surveys have largely overlooked these\ngoal-based distinctions, conventionally categorizing papers by input data types\n(e.g., metric-based vs. trace-based methods). This leads to the grouping of\nworks with disparate objectives, thereby obscuring the true progress and gaps\nin the field. Meanwhile, the typical audience of an RCA survey is either laymen\nwho want to know the goals and big picture of the task or RCA researchers who\nwant to figure out past research under the same task formulation. Thus, an RCA\nsurvey that organizes the related papers according to their goals is in high\ndemand. To this end, this paper presents a goal-driven framework that\neffectively categorizes and integrates 135 papers on RCA in the context of\ncloud incident management based on their diverse goals, spanning the period\nfrom 2014 to 2025. In addition to the goal-driven categorization, it discusses\nthe ultimate goal of all RCA papers as an umbrella covering different RCA\nformulations. Moreover, the paper discusses open challenges and future\ndirections in RCA."}
{"id": "2510.19322", "categories": ["cs.NI", "cs.AI", "cs.DC"], "pdf": "https://arxiv.org/pdf/2510.19322", "abs": "https://arxiv.org/abs/2510.19322", "authors": ["Changbo Wu", "Zhuolong Yu", "Gongming Zhao", "Hongli Xu"], "title": "Enabling Reconfiguration-Communication Overlap for Collective Communication in Optical Networks", "comment": null, "summary": "Collective communication (CC) is widely adopted for large-scale distributed\nmachine learning (DML) training workloads. DML's predictable traffic pattern\nprovides a great oppotunity for applying optical network technology. Existing\noptical interconnects-based CC schemes adopt ``one-shot network\nreconfiguration'', which provisions static high-capacity topologies for an\nentire collective operation -- sometimes for a full training iteration.\nHowever, this approach faces significant scalability limitations when\nsupporting more complex and efficient CC algorithms required for modern\nworkloads: the ``one-shot'' strategies either demand excessive resource\noverprovisioning or suffer performance degradation due to rigid resource\nallocation.\n  To address these challenges, we propose SWOT, a demand-aware optical network\nframework. SWOT employs ``intra-collective reconfiguration'' and can\ndynamically align network resources with CC traffic patterns. SWOT incorporates\na novel scheduling technique that overlaps optical switch reconfigurations with\nongoing transmissions, and improves communication efficiency. SWOT introduce a\nlightweight collective communication shim that enables coordinated optical\nnetwork configuration and transmission scheduling while supporting seamless\nintegration with existing CC libraries. Our simulation results demonstrate\nSWOT's significant performance improvements."}
{"id": "2510.19600", "categories": ["cs.SE", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.19600", "abs": "https://arxiv.org/abs/2510.19600", "authors": ["Qianli Ma", "Siyu Wang", "Yilin Chen", "Yinhao Tang", "Yixiang Yang", "Chang Guo", "Bingjie Gao", "Zhening Xing", "Yanan Sun", "Zhipeng Zhang"], "title": "Human-Agent Collaborative Paper-to-Page Crafting for Under $0.1", "comment": null, "summary": "In the quest for scientific progress, communicating research is as vital as\nthe discovery itself. Yet, researchers are often sidetracked by the manual,\nrepetitive chore of building project webpages to make their dense papers\naccessible. While automation has tackled static slides and posters, the\ndynamic, interactive nature of webpages has remained an unaddressed challenge.\nTo bridge this gap, we reframe the problem, arguing that the solution lies not\nin a single command, but in a collaborative, hierarchical process. We introduce\n$\\textbf{AutoPage}$, a novel multi-agent system that embodies this philosophy.\nAutoPage deconstructs paper-to-page creation into a coarse-to-fine pipeline\nfrom narrative planning to multimodal content generation and interactive\nrendering. To combat AI hallucination, dedicated \"Checker\" agents verify each\nstep against the source paper, while optional human checkpoints ensure the\nfinal product aligns perfectly with the author's vision, transforming the\nsystem from a mere tool into a powerful collaborative assistant. To rigorously\nvalidate our approach, we also construct $\\textbf{PageBench}$, the first\nbenchmark for this new task. Experiments show AutoPage not only generates\nhigh-quality, visually appealing pages but does so with remarkable efficiency\nin under 15 minutes for less than \\$0.1. Code and dataset will be released at\n$\\href{https://mqleet.github.io/AutoPage_ProjectPage/}{Webpage}$."}
{"id": "2510.19615", "categories": ["cs.SE", "cs.CR"], "pdf": "https://arxiv.org/pdf/2510.19615", "abs": "https://arxiv.org/abs/2510.19615", "authors": ["Zhiping Zhou", "Xiaohong Li", "Ruitao Feng", "Yao Zhang", "Yuekang Li", "Wenbu Feng", "Yunqian Wang", "Yuqing Li"], "title": "FidelityGPT: Correcting Decompilation Distortions with Retrieval Augmented Generation", "comment": null, "summary": "Decompilation converts machine code into human-readable form, enabling\nanalysis and debugging without source code. However, fidelity issues often\ndegrade the readability and semantic accuracy of decompiled output. Existing\nmethods, such as variable renaming or structural simplification, provide\npartial improvements but lack robust detection and correction, particularly for\ncomplex closed-source binaries. We present FidelityGPT, a framework that\nenhances decompiled code accuracy and readability by systematically detecting\nand correcting semantic distortions. FidelityGPT introduces distortion-aware\nprompt templates tailored to closed-source settings and integrates\nRetrieval-Augmented Generation (RAG) with a dynamic semantic intensity\nalgorithm to locate distorted lines and retrieve semantically similar code from\na database. A variable dependency algorithm further mitigates long-context\nlimitations by analyzing redundant variables and integrating their dependencies\ninto the prompt context. Evaluated on 620 function pairs from a binary\nsimilarity benchmark, FidelityGPT achieved an average detection accuracy of 89%\nand a precision of 83%. Compared to the state-of-the-art DeGPT (Fix Rate 83%,\nCorrected Fix Rate 37%), FidelityGPT attained 94% FR and 64% CFR, demonstrating\nsignificant gains in accuracy and readability. These results highlight its\npotential to advance LLM-based decompilation and reverse engineering."}
{"id": "2510.19692", "categories": ["cs.SE", "cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2510.19692", "abs": "https://arxiv.org/abs/2510.19692", "authors": ["Rashina Hoda"], "title": "Toward Agentic Software Engineering Beyond Code: Framing Vision, Values, and Vocabulary", "comment": "5 pages", "summary": "Agentic AI is poised to usher in a seismic paradigm shift in Software\nEngineering (SE). As technologists rush head-along to make agentic AI a\nreality, SE researchers are driven to establish agentic SE as a research area.\nWhile early visions of agentic SE are primarily focused on code-related\nactivities, early empirical evidence calls for a consideration of a range of\nsocio-technical concerns to make it work in practice. This paper contributes to\nthe emerging community vision by: (a) recommending an expansion of its scope\nbeyond code, toward a 'whole of process' vision, grounding it in SE foundations\nand evolution and emerging agentic SE frameworks, (b) proposing a preliminary\nset of values and principles to guide efforts, and (c) sharing guidance on\ndesigning/using well-defined vocabulary for agentic SE. It is hoped that these\nideas will encourage community collaborations and steer the SE community\ntowards laying strong foundations of agentic SE so its not only inevitable but\nalso deliberate and desirable in the long run."}
{"id": "2510.19747", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.19747", "abs": "https://arxiv.org/abs/2510.19747", "authors": ["Priyaranjan Pattnayak", "Hussain Bohra"], "title": "Review of Tools for Zero-Code LLM Based Application Development", "comment": "Accepted in 6th World Conference on Artificial Intelligence: Advances\n  and Applications (WCAIAA 2025)", "summary": "Large Language Models (LLMs) are transforming software creation by enabling\nzero code development platforms. Our survey reviews recent platforms that let\nusers build applications without writing code, by leveraging LLMs as the brains\nof the development process. We adopt a broad survey methodology, categorizing\nplatforms based on key dimensions such as interface style, backend integration,\noutput type, and extensibility. We analyze both dedicated LLM based app\nbuilders (OpenAI's custom GPTs, Bolt.new, Dust.tt, Flowise, Cognosys) and\ngeneral no code platforms (e.g., Bubble, Glide) that integrate LLM\ncapabilities. We present a taxonomy categorizing these platforms by their\ninterface (conversational, visual, etc.), supported LLM backends, output type\n(chatbot, full application, workflow), and degree of extensibility. Core\nfeatures such as autonomous agents, memory management, workflow orchestration,\nand API integrations are in scope of the survey. We provide a detailed\ncomparison, highlighting each platform's strengths and limitations. Trade offs\n(customizability, scalability, vendor lock-in) are discussed in comparison with\ntraditional and low code development approaches. Finally, we outline future\ndirections, including multimodal interfaces, on device LLMs, and improved\norchestration for democratizing app creation with AI. Our findings indicate\nthat while zero code LLM platforms greatly reduce the barrier to creating AI\npowered applications, they still face challenges in flexibility and\nreliability. Overall, the landscape is rapidly evolving, offering exciting\nopportunities to empower non programmers to create sophisticated software."}
{"id": "2510.19777", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.19777", "abs": "https://arxiv.org/abs/2510.19777", "authors": ["S M Sadrul Islam Asif", "James Chen", "Earl T. Barr", "Mark Marron"], "title": "BOSQTGEN: Breaking the Sound Barrier in Test Generation", "comment": null, "summary": "Modern software is increasingly built by composing APIs, elevating the API\ncontract to a critical role. Inadequate contracts, however, lead to mismatched\nexpectations and failures, creating a pressing need for robust conformance\ntesting. Current test generation techniques are hindered by key challenges:\npolyglot systems, source code inaccessibility, a cost-reliability trade-off,\nand, most critically, the difficulty of generating structured inputs.\n  We introduce BOSQTGEN, a novel black-box methodology and tool for API test\ngeneration. BOSQTGEN utilizes a novel approach for decomposing API\nspecifications into primitives, using LLMs to suggest coherent strata for them,\nand employing combinatorial testing to efficiently sample over these values.\nThis approach ensures coverage of critical interactions while avoiding the\nredundancy of random sampling.\n  The resulting BOSQTGEN system achieves an average of 82% code coverage on\nRESTful benchmarks, often a 20% or more increase over prior state-of-the-art\nsystems and nearing parity with hand-written test suites. Providing a fully\nAPI-driven approach to test generation, enables developers to automatically\ncreate high-quality test cases for validation or test-driven development."}
{"id": "2510.18893", "categories": ["cs.DC", "cs.AI", "cs.SE", "I.2.11; D.2.11"], "pdf": "https://arxiv.org/pdf/2510.18893", "abs": "https://arxiv.org/abs/2510.18893", "authors": ["Sergey Pugachev"], "title": "CodeCRDT: Observation-Driven Coordination for Multi-Agent LLM Code Generation", "comment": "11 pages, 3 figures", "summary": "Multi-agent LLM systems fail to realize parallel speedups due to costly\ncoordination. We present CodeCRDT, an observation-driven coordination pattern\nwhere agents coordinate by monitoring a shared state with observable updates\nand deterministic convergence, rather than explicit message passing. Using\nConflict-Free Replicated Data Types (CRDTs), CodeCRDT enables lock-free,\nconflict-free concurrent code generation with strong eventual consistency.\nEvaluation across 600 trials (6 tasks, 50 runs per mode) shows both benefits\nand trade-offs: up to 21.1% speedup on some tasks, up to 39.4% slowdown on\nothers, and 100% convergence with zero merge failures. The study formalizes\nobservation-driven coordination for stochastic LLM agents, revealing semantic\nconflict rates (5-10%) and quality-performance tradeoffs, and provides\nempirical characterization of when parallel coordination succeeds versus fails\nbased on task structure."}
{"id": "2510.18897", "categories": ["cs.DC", "cs.AI", "cs.DB", "cs.SE"], "pdf": "https://arxiv.org/pdf/2510.18897", "abs": "https://arxiv.org/abs/2510.18897", "authors": ["Jacopo Tagliabue"], "title": "AI for Distributed Systems Design: Scalable Cloud Optimization Through Repeated LLMs Sampling And Simulators", "comment": "Pre-print IAAA workshop submission", "summary": "We explore AI-driven distributed-systems policy design by combining\nstochastic code generation from large language models (LLMs) with deterministic\nverification in a domain-specific simulator. Using a Function-as-a-Service\nruntime (Bauplan) and its open-source simulator (Eudoxia) as a case study, we\nframe scheduler design as an iterative generate-and-verify loop: an LLM\nproposes a Python policy, the simulator evaluates it on standardized traces,\nand structured feedback steers subsequent generations. This setup preserves\ninterpretability while enabling targeted search over a large design space. We\ndetail the system architecture and report preliminary results on throughput\nimprovements across multiple models. Beyond early gains, we discuss the limits\nof the current setup and outline next steps; in particular, we conjecture that\nAI will be crucial for scaling this methodology by helping to bootstrap new\nsimulators."}
{"id": "2510.19012", "categories": ["cs.DC", "cs.DB", "cs.PL", "cs.SE"], "pdf": "https://arxiv.org/pdf/2510.19012", "abs": "https://arxiv.org/abs/2510.19012", "authors": ["Ivan Borodii", "Illia Fedorovych", "Halyna Osukhivska", "Diana Velychko", "Roman Butsii"], "title": "Comparative analysis of large data processing in Apache Spark using Java, Python and Scala", "comment": "CITI 2025, 3rd International Workshop on Computer Information\n  Technologies in Industry 4.0, June 11-12, 2025, Ternopil, Ukraine. The\n  article includes 10 pages, 5 figures, 9 tables", "summary": "During the study, the results of a comparative analysis of the process of\nhandling large datasets using the Apache Spark platform in Java, Python, and\nScala programming languages were obtained. Although prior works have focused on\nindividual stages, comprehensive comparisons of full ETL workflows across\nprogramming languages using Apache Iceberg remain limited. The analysis was\nperformed by executing several operations, including downloading data from CSV\nfiles, transforming and loading it into an Apache Iceberg analytical table. It\nwas found that the performance of the Spark algorithm varies significantly\ndepending on the amount of data and the programming language used. When\nprocessing a 5-megabyte CSV file, the best result was achieved in Python: 6.71\nseconds, which is superior to Scala's score of 9.13 seconds and Java's time of\n9.62 seconds. For processing a large CSV file of 1.6 gigabytes, all programming\nlanguages demonstrated similar results: the fastest performance was showed in\nPython: 46.34 seconds, while Scala and Java showed results of 47.72 and 50.56\nseconds, respectively. When performing a more complex operation that involved\ncombining two CSV files into a single dataset for further loading into an\nApache Iceberg table, Scala demonstrated the highest performance, at 374.42\nseconds. Java processing was completed in 379.8 seconds, while Python was the\nleast efficient, with a runtime of 398.32 seconds. It follows that the\nprogramming language significantly affects the efficiency of data processing by\nthe Apache Spark algorithm, with Scala and Java being more productive for\nprocessing large amounts of data and complex operations, while Python\ndemonstrates an advantage in working with small amounts of data. The results\nobtained can be useful for optimizing data handling processes depending on\nspecific performance requirements and the amount of information being\nprocessed."}
