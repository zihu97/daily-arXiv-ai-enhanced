{"id": "2510.15872", "categories": ["cs.AR", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.15872", "abs": "https://arxiv.org/abs/2510.15872", "authors": ["Yun-Da Tsai", "Chang-Yu Chao", "Liang-Yeh Shen", "Tsung-Han Lin", "Haoyu Yang", "Mark Ho", "Yi-Chen Lu", "Wen-Hao Liu", "Shou-De Lin", "Haoxing Ren"], "title": "Multimodal Chip Physical Design Engineer Assistant", "comment": null, "summary": "Modern chip physical design relies heavily on Electronic Design Automation\n(EDA) tools, which often struggle to provide interpretable feedback or\nactionable guidance for improving routing congestion. In this work, we\nintroduce a Multimodal Large Language Model Assistant (MLLMA) that bridges this\ngap by not only predicting congestion but also delivering human-interpretable\ndesign suggestions. Our method combines automated feature generation through\nMLLM-guided genetic prompting with an interpretable preference learning\nframework that models congestion-relevant tradeoffs across visual, tabular, and\ntextual inputs. We compile these insights into a \"Design Suggestion Deck\" that\nsurfaces the most influential layout features and proposes targeted\noptimizations. Experiments on the CircuitNet benchmark demonstrate that our\napproach outperforms existing models on both accuracy and explainability.\nAdditionally, our design suggestion guidance case study and qualitative\nanalyses confirm that the learned preferences align with real-world design\nprinciples and are actionable for engineers. This work highlights the potential\nof MLLMs as interactive assistants for interpretable and context-aware physical\ndesign optimization.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u52a9\u624b\uff08MLLMA\uff09\uff0c\u7528\u4e8e\u9884\u6d4b\u82af\u7247\u7269\u7406\u8bbe\u8ba1\u4e2d\u7684\u5e03\u7ebf\u62e5\u585e\u5e76\u63d0\u4f9b\u53ef\u89e3\u91ca\u7684\u8bbe\u8ba1\u5efa\u8bae\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u7535\u5b50\u8bbe\u8ba1\u81ea\u52a8\u5316\uff08EDA\uff09\u5de5\u5177\u5728\u5e03\u7ebf\u62e5\u585e\u95ee\u9898\u4e0a\u7f3a\u4e4f\u53ef\u89e3\u91ca\u7684\u53cd\u9988\u548c\u53ef\u64cd\u4f5c\u7684\u4f18\u5316\u6307\u5bfc\uff0c\u9650\u5236\u4e86\u5de5\u7a0b\u5e08\u7684\u51b3\u7b56\u6548\u7387\u3002", "method": "\u7ed3\u5408\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5f15\u5bfc\u7684\u9057\u4f20\u63d0\u793a\u8fdb\u884c\u81ea\u52a8\u7279\u5f81\u751f\u6210\uff0c\u5e76\u6784\u5efa\u4e00\u4e2a\u53ef\u89e3\u91ca\u7684\u504f\u597d\u5b66\u4e60\u6846\u67b6\uff0c\u6574\u5408\u89c6\u89c9\u3001\u8868\u683c\u548c\u6587\u672c\u8f93\u5165\uff0c\u751f\u6210\u201c\u8bbe\u8ba1\u5efa\u8bae\u5361\u201d\u4ee5\u7a81\u51fa\u5173\u952e\u5e03\u5c40\u7279\u5f81\u5e76\u63d0\u51fa\u9488\u5bf9\u6027\u4f18\u5316\u3002", "result": "\u5728CircuitNet\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u8be5\u65b9\u6cd5\u5728\u51c6\u786e\u6027\u548c\u53ef\u89e3\u91ca\u6027\u65b9\u9762\u5747\u4f18\u4e8e\u73b0\u6709\u6a21\u578b\uff1b\u6848\u4f8b\u7814\u7a76\u548c\u5b9a\u6027\u5206\u6790\u8868\u660e\u5176\u5efa\u8bae\u7b26\u5408\u5b9e\u9645\u8bbe\u8ba1\u539f\u5219\u4e14\u5bf9\u5de5\u7a0b\u5e08\u5177\u6709\u53ef\u64cd\u4f5c\u6027\u3002", "conclusion": "\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u6709\u6f5c\u529b\u4f5c\u4e3a\u4ea4\u4e92\u5f0f\u52a9\u624b\uff0c\u5b9e\u73b0\u53ef\u89e3\u91ca\u3001\u4e0a\u4e0b\u6587\u611f\u77e5\u7684\u7269\u7406\u8bbe\u8ba1\u4f18\u5316\u3002"}}
{"id": "2510.15878", "categories": ["cs.AR", "cs.OS", "cs.PF"], "pdf": "https://arxiv.org/pdf/2510.15878", "abs": "https://arxiv.org/abs/2510.15878", "authors": ["David A. Roberts"], "title": "Putting the Context back into Memory", "comment": null, "summary": "Requests arriving at main memory are often different from what programmers\ncan observe or estimate by using CPU-based monitoring. Hardware cache\nprefetching, memory request scheduling and interleaving cause a loss of\nobservability that limits potential data movement and tiering optimizations. In\nresponse, memory-side telemetry hardware like page access heat map units (HMU)\nand page prefetchers were proposed to inform Operating Systems with accurate\nusage data. However, it is still hard to map memory activity to software\nprogram functions and objects because of the decoupled nature of host\nprocessors and memory devices. Valuable program context is stripped out from\nthe memory bus, leaving only commands, addresses and data. Programmers have\nexpert knowledge of future data accesses, priorities, and access to processor\nstate, which could be useful hints for runtime memory device optimization. This\npaper makes context visible at memory devices by encoding any user-visible\nstate as detectable packets in the memory read address stream, in a\nnondestructive manner without significant capacity overhead, drivers or special\naccess privileges. We prototyped an end-to-end system with metadata injection\nthat can be reliably detected and decoded from a memory address trace, either\nby a host processor, or a memory module. We illustrate a use case with precise\ncode execution markers and object address range tracking. In the future, real\ntime metadata decoding with near-memory computing (NMC) could provide\ncustomized telemetry and statistics to users, or act on application hints to\nperform functions like prioritizing requests, remapping data and reconfiguring\ndevices.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u5728\u5185\u5b58\u8bfb\u5730\u5740\u6d41\u4e2d\u65e0\u635f\u7f16\u7801\u7528\u6237\u53ef\u89c1\u7a0b\u5e8f\u72b6\u6001\u7684\u65b9\u6cd5\uff0c\u4f7f\u5185\u5b58\u8bbe\u5907\u80fd\u591f\u83b7\u53d6\u7a0b\u5e8f\u4e0a\u4e0b\u6587\u4fe1\u606f\uff0c\u4ece\u800c\u652f\u6301\u66f4\u7cbe\u7ec6\u7684\u5185\u5b58\u4f18\u5316\u3002", "motivation": "\u7531\u4e8e\u786c\u4ef6\u9884\u53d6\u3001\u8c03\u5ea6\u548c\u4ea4\u7ec7\u7b49\u56e0\u7d20\uff0c\u4e3b\u5b58\u8bf7\u6c42\u4e0e\u7a0b\u5e8f\u5458\u89c2\u5bdf\u5230\u7684\u884c\u4e3a\u5b58\u5728\u5dee\u5f02\uff0c\u5bfc\u81f4\u7a0b\u5e8f\u4e0a\u4e0b\u6587\u4fe1\u606f\u5728\u5185\u5b58\u603b\u7ebf\u4e0a\u4e22\u5931\uff0c\u9650\u5236\u4e86\u6570\u636e\u79fb\u52a8\u548c\u5206\u5c42\u4f18\u5316\u7684\u6f5c\u529b\u3002", "method": "\u901a\u8fc7\u5728\u5185\u5b58\u8bfb\u5730\u5740\u6d41\u4e2d\u4ee5\u975e\u7834\u574f\u6027\u65b9\u5f0f\u5d4c\u5165\u7528\u6237\u53ef\u89c1\u72b6\u6001\u4f5c\u4e3a\u53ef\u68c0\u6d4b\u7684\u6570\u636e\u5305\uff0c\u65e0\u9700\u989d\u5916\u9a71\u52a8\u6216\u7279\u6743\uff0c\u5b9e\u73b0\u7a0b\u5e8f\u4e0a\u4e0b\u6587\u5728\u5185\u5b58\u8bbe\u5907\u7aef\u7684\u53ef\u89c1\u6027\uff0c\u5e76\u6784\u5efa\u7aef\u5230\u7aef\u539f\u578b\u7cfb\u7edf\u8fdb\u884c\u9a8c\u8bc1\u3002", "result": "\u539f\u578b\u7cfb\u7edf\u80fd\u591f\u4ece\u5185\u5b58\u5730\u5740\u8f68\u8ff9\u4e2d\u53ef\u9760\u5730\u68c0\u6d4b\u548c\u89e3\u7801\u5143\u6570\u636e\uff0c\u5c55\u793a\u4e86\u7cbe\u786e\u7684\u4ee3\u7801\u6267\u884c\u6807\u8bb0\u548c\u5bf9\u8c61\u5730\u5740\u8303\u56f4\u8ddf\u8e2a\u7b49\u5e94\u7528\u573a\u666f\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u8fd1\u5185\u5b58\u8ba1\u7b97\u63d0\u4f9b\u4e86\u83b7\u53d6\u5b9a\u5236\u9065\u6d4b\u6570\u636e\u548c\u54cd\u5e94\u5e94\u7528\u63d0\u793a\u7684\u65b0\u9014\u5f84\uff0c\u672a\u6765\u53ef\u7528\u4e8e\u8bf7\u6c42\u4f18\u5148\u7ea7\u6392\u5e8f\u3001\u6570\u636e\u91cd\u6620\u5c04\u548c\u8bbe\u5907\u91cd\u914d\u7f6e\u7b49\u4f18\u5316\u3002"}}
{"id": "2510.15880", "categories": ["cs.AR", "cs.ET"], "pdf": "https://arxiv.org/pdf/2510.15880", "abs": "https://arxiv.org/abs/2510.15880", "authors": ["Philip Emma", "Eren Kurshan"], "title": "Opportunities and Challenges for 3D Systems and Their Design", "comment": "IEEE Design and Computers", "summary": "Although it is not a new concept, 3D integration increasingly receives\nwidespread interest and focus as lithographic scaling becomes more challenging,\nand as the ability to make miniature vias greatly improves. Like Moores law, 3D\nintegration improves density. With improvements in packaging density, however,\ncome the challenges associated with its inherently higher power density. And\nthough it acts somewhat as a scaling accelerator, the vertical integration also\nposes new challenges to design and manufacturing technologies. The placement of\ncircuits, vias, and macros in the planes of a 3D stack must be co-designed\nacross layers (or must conform to new standards) so that, when assembled, they\nhave correct spatial correspondence. Each layer, although perhaps being a mere\nfunctional slice through a system (and we can slice the system in many\ndifferent ways), must be independently testable so that we can systematically\ntest and diagnose subsystems before and after final assembly. When those layers\nare assembled, they must come together in a way that enables a sensible yield\nand facilitates testing the finished product. To make the most of 3D\nintegration, we should articulate the leverages of 3D systems (other\nresearchers offer a more complete treatment elsewhere). Then we can enumerate\nand elucidate many of the new challenges posed by the design, assembly, and\ntest of 3D systems.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u4e863D\u96c6\u6210\u6280\u672f\u5728\u5ef6\u7eed\u6469\u5c14\u5b9a\u5f8b\u3001\u63d0\u5347\u96c6\u6210\u5bc6\u5ea6\u65b9\u9762\u7684\u6f5c\u529b\uff0c\u540c\u65f6\u7cfb\u7edf\u5206\u6790\u4e86\u5176\u5728\u529f\u8017\u5bc6\u5ea6\u3001\u8de8\u5c42\u534f\u540c\u8bbe\u8ba1\u3001\u53ef\u6d4b\u6027\u53ca\u5236\u9020\u826f\u7387\u7b49\u65b9\u9762\u5e26\u6765\u7684\u65b0\u6311\u6218\u3002", "motivation": "\u968f\u7740\u5149\u523b\u5de5\u827a\u7f29\u653e\u96be\u5ea6\u52a0\u5927\u548c\u5fae\u578b\u901a\u5b54\u5236\u9020\u80fd\u529b\u63d0\u5347\uff0c3D\u96c6\u6210\u6280\u672f\u91cd\u65b0\u53d7\u5230\u5e7f\u6cdb\u5173\u6ce8\uff1b\u7136\u800c\uff0c\u5176\u9ad8\u96c6\u6210\u5bc6\u5ea6\u4e5f\u5e26\u6765\u4e86\u529f\u8017\u3001\u8bbe\u8ba1\u534f\u540c\u3001\u6d4b\u8bd5\u548c\u5236\u9020\u7b49\u65b9\u9762\u7684\u65b0\u95ee\u9898\uff0c\u4e9f\u9700\u7cfb\u7edf\u6027\u7814\u7a76\u3002", "method": "\u672c\u6587\u901a\u8fc7\u7efc\u8ff0\u548c\u5206\u6790\u73b0\u6709\u7814\u7a76\uff0c\u68b3\u74063D\u96c6\u6210\u7cfb\u7edf\u7684\u4f18\u52bf\uff0c\u5e76\u7cfb\u7edf\u9610\u8ff0\u5176\u5728\u8bbe\u8ba1\u3001\u7ec4\u88c5\u548c\u6d4b\u8bd5\u9636\u6bb5\u6240\u9762\u4e34\u7684\u5173\u952e\u6311\u6218\u3002", "result": "\u6587\u7ae0\u660e\u786e\u4e863D\u96c6\u6210\u5728\u63d0\u5347\u5bc6\u5ea6\u65b9\u9762\u7684\u4f18\u52bf\uff0c\u5e76\u8bc6\u522b\u51fa\u5305\u62ec\u8de8\u5c42\u534f\u540c\u8bbe\u8ba1\u3001\u5404\u5c42\u72ec\u7acb\u53ef\u6d4b\u6027\u3001\u7ec4\u88c5\u826f\u7387\u63a7\u5236\u4ee5\u53ca\u6210\u54c1\u6d4b\u8bd5\u7b49\u6838\u5fc3\u6311\u6218\u3002", "conclusion": "\u8981\u5145\u5206\u53d1\u63253D\u96c6\u6210\u7684\u6f5c\u529b\uff0c\u5fc5\u987b\u5168\u9762\u7406\u89e3\u5176\u4f18\u52bf\uff0c\u5e76\u7cfb\u7edf\u5e94\u5bf9\u8bbe\u8ba1\u3001\u5236\u9020\u4e0e\u6d4b\u8bd5\u4e2d\u7684\u65b0\u6311\u6218\uff0c\u63a8\u52a8\u76f8\u5173\u6807\u51c6\u4e0e\u65b9\u6cd5\u7684\u53d1\u5c55\u3002"}}
{"id": "2510.15882", "categories": ["cs.AR", "cs.AI", "cs.DC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.15882", "abs": "https://arxiv.org/abs/2510.15882", "authors": ["Ao Shen", "Rui Zhang", "Junping Zhao"], "title": "FlexLink: Boosting your NVLink Bandwidth by 27% without accuracy concern", "comment": null, "summary": "As large language models (LLMs) continue to scale, multi-node deployment has\nbecome a necessity. Consequently, communication has become a critical\nperformance bottleneck. Current intra-node communication libraries, like NCCL,\ntypically make use of a single interconnect such as NVLink. This approach\ncreates performance ceilings, especially on hardware like the H800 GPU where\nthe primary interconnect's bandwidth can become a bottleneck, and leaves other\nhardware resources like PCIe and Remote Direct Memory Access (RDMA)-capable\nNetwork Interface Cards (NICs) largely idle during intensive workloads. We\npropose FlexLink, the first collective communication framework to the best of\nour knowledge designed to systematically address this by aggregating these\nheterogeneous links-NVLink, PCIe, and RDMA NICs-into a single, high-performance\ncommunication fabric. FlexLink employs an effective two-stage adaptive load\nbalancing strategy that dynamically partitions communication traffic across all\navailable links, ensuring that faster interconnects are not throttled by slower\nones. On an 8-GPU H800 server, our design improves the bandwidth of collective\noperators such as AllReduce and AllGather by up to 26% and 27% over the NCCL\nbaseline, respectively. This gain is achieved by offloading 2-22% of the total\ncommunication traffic to the previously underutilized PCIe and RDMA NICs.\nFlexLink provides these improvements as a lossless, drop-in replacement\ncompatible with the NCCL API, ensuring easy adoption.", "AI": {"tldr": "FlexLink \u662f\u4e00\u79cd\u65b0\u578b\u96c6\u4f53\u901a\u4fe1\u6846\u67b6\uff0c\u901a\u8fc7\u805a\u5408 NVLink\u3001PCIe \u548c RDMA NIC \u7b49\u5f02\u6784\u94fe\u8def\uff0c\u52a8\u6001\u5206\u914d\u901a\u4fe1\u6d41\u91cf\uff0c\u5728 H800 GPU \u4e0a\u5c06 AllReduce \u548c AllGather \u7684\u5e26\u5bbd\u5206\u522b\u63d0\u5347\u6700\u591a 26% \u548c 27%\uff0c\u4e14\u517c\u5bb9 NCCL API\uff0c\u53ef\u65e0\u635f\u66ff\u6362\u3002", "motivation": "\u968f\u7740\u5927\u8bed\u8a00\u6a21\u578b\u89c4\u6a21\u6269\u5927\uff0c\u591a\u8282\u70b9\u90e8\u7f72\u6210\u4e3a\u5fc5\u9700\uff0c\u901a\u4fe1\u6210\u4e3a\u6027\u80fd\u74f6\u9888\u3002\u73b0\u6709\u901a\u4fe1\u5e93\uff08\u5982 NCCL\uff09\u4ec5\u4f7f\u7528\u5355\u4e00\u4e92\u8fde\uff08\u5982 NVLink\uff09\uff0c\u5728 H800 \u7b49\u786c\u4ef6\u4e0a\u9020\u6210\u5e26\u5bbd\u74f6\u9888\uff0c\u540c\u65f6 PCIe \u548c RDMA NIC \u7b49\u8d44\u6e90\u672a\u88ab\u5145\u5206\u5229\u7528\u3002", "method": "\u63d0\u51fa FlexLink \u6846\u67b6\uff0c\u901a\u8fc7\u4e24\u9636\u6bb5\u81ea\u9002\u5e94\u8d1f\u8f7d\u5747\u8861\u7b56\u7565\uff0c\u52a8\u6001\u5c06\u901a\u4fe1\u6d41\u91cf\u5206\u914d\u5230\u6240\u6709\u53ef\u7528\u94fe\u8def\uff08NVLink\u3001PCIe\u3001RDMA NIC\uff09\uff0c\u907f\u514d\u9ad8\u901f\u94fe\u8def\u88ab\u4f4e\u901f\u94fe\u8def\u62d6\u7d2f\uff0c\u5e76\u4f5c\u4e3a NCCL API \u7684\u65e0\u635f\u66ff\u4ee3\u65b9\u6848\u5b9e\u73b0\u3002", "result": "\u5728 8-GPU H800 \u670d\u52a1\u5668\u4e0a\uff0cFlexLink \u76f8\u6bd4 NCCL \u57fa\u7ebf\u5c06 AllReduce \u548c AllGather \u7684\u5e26\u5bbd\u5206\u522b\u63d0\u5347\u6700\u591a 26% \u548c 27%\uff0c\u5e76\u5c06 2\u201322% \u7684\u901a\u4fe1\u6d41\u91cf\u5378\u8f7d\u81f3\u539f\u5148\u672a\u5145\u5206\u5229\u7528\u7684 PCIe \u548c RDMA NIC\u3002", "conclusion": "FlexLink \u6709\u6548\u5229\u7528\u5f02\u6784\u4e92\u8fde\u8d44\u6e90\uff0c\u663e\u8457\u63d0\u5347\u591a GPU \u7cfb\u7edf\u4e2d\u7684\u96c6\u4f53\u901a\u4fe1\u6027\u80fd\uff0c\u4e14\u5177\u5907\u826f\u597d\u7684\u517c\u5bb9\u6027\u548c\u6613\u90e8\u7f72\u6027\uff0c\u4e3a\u5927\u89c4\u6a21 LLM \u8bad\u7ec3\u63d0\u4f9b\u4e86\u5b9e\u7528\u7684\u901a\u4fe1\u4f18\u5316\u65b9\u6848\u3002"}}
{"id": "2510.15884", "categories": ["cs.AR", "cs.MS"], "pdf": "https://arxiv.org/pdf/2510.15884", "abs": "https://arxiv.org/abs/2510.15884", "authors": ["Faizan A Khattak", "Mantas Mikaitis"], "title": "Generalized Methodology for Determining Numerical Features of Hardware Floating-Point Matrix Multipliers: Part I", "comment": "Accepted for IEEE HPEC 2025", "summary": "Numerical features of matrix multiplier hardware units in NVIDIA and AMD data\ncentre GPUs have recently been studied. Features such as rounding,\nnormalisation, and internal precision of the accumulators are of interest. In\nthis paper, we extend the methodology for analysing those features, to\nconsumer-grade NVIDIA GPUs by implementing an architecture-independent test\nscheme for various input and output precision formats. Unlike current\napproaches, the proposed test vector generation method neither performs an\nexhaustive search nor relies on hard-coded {constants that are device-specific,\nyet remains applicable to a wide range of mixed-precision formats. We have\napplied the scheme to the RTX-3060 (Ampere architecture), and Ada RTX-1000 (Ada\nLovelace architecture) graphics cards and determined numerical features of\nmatrix multipliers for binary16, TensorFloat32, and bfloat16 input floating\npoint formats and binary16 and binary32 IEEE 754 output formats. Our\nmethodology allowed us to determine that} the numerical features of RTX-3060, a\nconsumer-grade GPU, are identical to those of the A100, a data centre GPU. We\ndo not expect our code to require any changes for performing analysis of matrix\nmultipliers on newer NVIDIA GPUs, Hopper or Blackwell, and their future\nsuccessors, and any input/output format combination, including the latest 8-bit\nfloating-point formats.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u67b6\u6784\u65e0\u5173\u7684\u6d4b\u8bd5\u65b9\u6848\uff0c\u7528\u4e8e\u5206\u6790\u6d88\u8d39\u7ea7NVIDIA GPU\u4e2d\u77e9\u9635\u4e58\u6cd5\u5668\u7684\u6570\u503c\u7279\u6027\uff08\u5982\u820d\u5165\u3001\u5f52\u4e00\u5316\u548c\u7d2f\u52a0\u5668\u5185\u90e8\u7cbe\u5ea6\uff09\uff0c\u9002\u7528\u4e8e\u591a\u79cd\u6df7\u5408\u7cbe\u5ea6\u683c\u5f0f\uff0c\u5e76\u9a8c\u8bc1\u4e86RTX-3060\u4e0e\u6570\u636e\u4e2d\u5fc3GPU A100\u5728\u6570\u503c\u7279\u6027\u4e0a\u7684\u4e00\u81f4\u6027\u3002", "motivation": "\u5f53\u524d\u5bf9GPU\u77e9\u9635\u4e58\u6cd5\u5668\u6570\u503c\u7279\u6027\u7684\u7814\u7a76\u4e3b\u8981\u96c6\u4e2d\u5728\u6570\u636e\u4e2d\u5fc3\u7ea7\u786c\u4ef6\uff0c\u7f3a\u4e4f\u9002\u7528\u4e8e\u6d88\u8d39\u7ea7GPU\u4e14\u4e0d\u4f9d\u8d56\u8bbe\u5907\u7279\u5b9a\u5e38\u91cf\u6216\u7a77\u4e3e\u641c\u7d22\u7684\u901a\u7528\u5206\u6790\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u67b6\u6784\u65e0\u5173\u7684\u6d4b\u8bd5\u5411\u91cf\u751f\u6210\u65b9\u6cd5\uff0c\u4e0d\u8fdb\u884c\u7a77\u4e3e\u641c\u7d22\uff0c\u4e5f\u4e0d\u4f9d\u8d56\u786c\u7f16\u7801\u7684\u8bbe\u5907\u7279\u5b9a\u5e38\u91cf\uff0c\u53ef\u9002\u7528\u4e8e\u591a\u79cd\u8f93\u5165/\u8f93\u51fa\u7cbe\u5ea6\u683c\u5f0f\uff0c\u5305\u62ecbinary16\u3001TensorFloat32\u3001bfloat16\u7b49\u3002", "result": "\u6210\u529f\u5e94\u7528\u4e8eRTX-3060\uff08Ampere\uff09\u548cAda RTX-1000\uff08Ada Lovelace\uff09\u663e\u5361\uff0c\u53d1\u73b0RTX-3060\u7684\u77e9\u9635\u4e58\u6cd5\u5668\u6570\u503c\u7279\u6027\u4e0eA100\u6570\u636e\u4e2d\u5fc3GPU\u5b8c\u5168\u4e00\u81f4\u3002", "conclusion": "\u6240\u63d0\u65b9\u6cd5\u5177\u6709\u826f\u597d\u7684\u901a\u7528\u6027\u548c\u524d\u77bb\u6027\uff0c\u65e0\u9700\u4fee\u6539\u5373\u53ef\u7528\u4e8e\u5206\u6790\u672a\u6765NVIDIA GPU\uff08\u5982Hopper\u3001Blackwell\uff09\u53ca\u65b0\u578b8\u4f4d\u6d6e\u70b9\u683c\u5f0f\u7684\u77e9\u9635\u4e58\u6cd5\u5668\u6570\u503c\u7279\u6027\u3002"}}
{"id": "2510.15885", "categories": ["cs.AR", "cs.OS"], "pdf": "https://arxiv.org/pdf/2510.15885", "abs": "https://arxiv.org/abs/2510.15885", "authors": ["Dingcui Yu", "Zonghuan Yan", "Jialin Liu", "Yumiao Zhao", "Yanyun Wang", "Xinghui Duan", "Yina Lv", "Liang Shi"], "title": "ConZone+: Practical Zoned Flash Storage Emulation for Consumer Devices", "comment": null, "summary": "To facilitate the understanding and efficient enhancement of software and\nhardware design for consumer-grade zoned flash storage, ConZone is proposed as\nthe first emulator designed to model the resource constraints and architectural\nfeatures typical of such systems. It incorporates essential components commonly\ndeployed in consumer-grade devices, including limited logical to physical\nmapping caches, constrained write buffers, and hybrid flash media management.\nHowever, ConZone cannot be mounted with the file system due to the lack of\nin-place update capability, which is required by the metadata area of F2FS. To\nimprove the usability of the emulator, ConZone+ extends ConZone with support\nfor a block interface. We also provide a script to help the deployment and\nintroduces several enhancements over the original version. Users can explore\nthe internal architecture of consumer-grade zoned flash storage and integrate\ntheir optimizations with system software using ConZone+. We validate the\naccuracy of ConZone+ by comparing a hardware architecture representative of\nconsumer-grade zoned flash storage and comparing it with the state-of-the-art.\nIn addition, we conduct several case studies using ConZone+ to investigate the\ndesign of zoned storage and explore the inadequacies of the current file\nsystem.", "AI": {"tldr": "ConZone+ \u662f\u4e00\u4e2a\u9762\u5411\u6d88\u8d39\u7ea7\u5206\u533a\u57df\u95ea\u5b58\u5b58\u50a8\u7684\u589e\u5f3a\u578b\u6a21\u62df\u5668\uff0c\u901a\u8fc7\u5f15\u5165\u5757\u63a5\u53e3\u652f\u6301\u6587\u4ef6\u7cfb\u7edf\u6302\u8f7d\uff0c\u63d0\u5347\u4e86\u539f\u59cb ConZone \u7684\u53ef\u7528\u6027\uff0c\u5e76\u53ef\u7528\u4e8e\u7814\u7a76\u5b58\u50a8\u67b6\u6784\u4e0e\u6587\u4ef6\u7cfb\u7edf\u9002\u914d\u95ee\u9898\u3002", "motivation": "\u73b0\u6709 ConZone \u6a21\u62df\u5668\u56e0\u7f3a\u4e4f\u539f\u5730\u66f4\u65b0\u80fd\u529b\uff0c\u65e0\u6cd5\u652f\u6301\u4f9d\u8d56\u8be5\u7279\u6027\u7684\u6587\u4ef6\u7cfb\u7edf\uff08\u5982 F2FS\uff09\u6302\u8f7d\uff0c\u9650\u5236\u4e86\u5176\u5728\u7cfb\u7edf\u8f6f\u4ef6\u534f\u540c\u4f18\u5316\u4e2d\u7684\u5e94\u7528\u3002", "method": "\u5728 ConZone \u57fa\u7840\u4e0a\u6269\u5c55\u5757\u63a5\u53e3\u652f\u6301\uff0c\u63d0\u4f9b\u90e8\u7f72\u811a\u672c\u5e76\u5f15\u5165\u591a\u9879\u6539\u8fdb\uff0c\u4f7f\u6a21\u62df\u5668\u80fd\u4e0e\u6587\u4ef6\u7cfb\u7edf\u96c6\u6210\uff0c\u540c\u65f6\u4fdd\u7559\u5bf9\u6d88\u8d39\u7ea7\u5206\u533a\u57df\u95ea\u5b58\u5173\u952e\u7279\u6027\u7684\u5efa\u6a21\u80fd\u529b\u3002", "result": "\u901a\u8fc7\u4e0e\u771f\u5b9e\u786c\u4ef6\u53ca\u73b0\u6709\u5148\u8fdb\u65b9\u6848\u5bf9\u6bd4\u9a8c\u8bc1\u4e86 ConZone+ \u7684\u51c6\u786e\u6027\uff0c\u5e76\u5229\u7528\u5176\u5f00\u5c55\u591a\u4e2a\u6848\u4f8b\u7814\u7a76\uff0c\u63ed\u793a\u4e86\u5f53\u524d\u6587\u4ef6\u7cfb\u7edf\u5728\u5206\u533a\u57df\u5b58\u50a8\u8bbe\u8ba1\u4e2d\u7684\u4e0d\u8db3\u3002", "conclusion": "ConZone+ \u6709\u6548\u63d0\u5347\u4e86\u6d88\u8d39\u7ea7\u5206\u533a\u57df\u95ea\u5b58\u6a21\u62df\u5668\u7684\u5b9e\u7528\u6027\uff0c\u4e3a\u8f6f\u786c\u4ef6\u534f\u540c\u4f18\u5316\u548c\u6587\u4ef6\u7cfb\u7edf\u9002\u914d\u7814\u7a76\u63d0\u4f9b\u4e86\u6709\u529b\u5de5\u5177\u3002"}}
{"id": "2510.15927", "categories": ["cs.AR", "cs.DC", "cs.PF"], "pdf": "https://arxiv.org/pdf/2510.15927", "abs": "https://arxiv.org/abs/2510.15927", "authors": ["Krystian Chmielewski", "Jaros\u0142aw \u0141awnicki", "Uladzislau Lukyanau", "Tadeusz Kobus", "Maciej Maciejewski"], "title": "UPMEM Unleashed: Software Secrets for Speed", "comment": null, "summary": "Developing kernels for Processing-In-Memory (PIM) platforms poses unique\nchallenges in data management and parallel programming on limited processing\nunits. Although software development kits (SDKs) for PIM, such as the UPMEM\nSDK, provide essential tools, these emerging platforms still leave significant\nroom for performance optimization. In this paper, we reveal surprising\ninefficiencies in UPMEM software stack and play with non-standard programming\ntechniques. By making simple modifications to the assembly generated by the\nUPMEM compiler, we achieve speedups of 1.6-2x in integer addition and 1.4-5.9x\nin integer multiplication, depending on the data type. We also demonstrate that\nbit-serial processing of low precision data is a viable option for UPMEM: in\nINT4 bit-serial dot-product calculation, UPMEM can achieve over 2.7x speedup\nover the baseline. Minor API extensions for PIM allocation that account for the\nnon-uniform memory access (NUMA) architecture of the server further improve the\nconsistency and throughput of host-PIM data transfers by up to 2.9x. Finally,\nwe show that, when the matrix is preloaded into PIM, our optimized kernels\noutperform a dual-socket CPU server by over 3x for INT8 generalized\nmatrix-vector multiplication (GEMV) and by 10x for INT4 GEMV. Our optimized\nINT8 GEMV kernel outperforms the baseline 3.5x.", "AI": {"tldr": "\u672c\u6587\u901a\u8fc7\u975e\u6807\u51c6\u7f16\u7a0b\u6280\u672f\u4f18\u5316UPMEM PIM\u5e73\u53f0\u7684\u8f6f\u4ef6\u6808\uff0c\u5728\u6574\u6570\u8fd0\u7b97\u3001\u4f4e\u7cbe\u5ea6\u4f4d\u4e32\u884c\u5904\u7406\u53ca\u5185\u5b58\u5206\u914d\u7b49\u65b9\u9762\u663e\u8457\u63d0\u5347\u6027\u80fd\uff0c\u4f7fINT8\u548cINT4\u7684GEMV\u5206\u522b\u6bd4CPU\u5feb3\u500d\u548c10\u500d\u3002", "motivation": "PIM\u5e73\u53f0\uff08\u5982UPMEM\uff09\u867d\u6709SDK\u652f\u6301\uff0c\u4f46\u5728\u6570\u636e\u7ba1\u7406\u548c\u5e76\u884c\u7f16\u7a0b\u65b9\u9762\u4ecd\u5b58\u5728\u6027\u80fd\u74f6\u9888\uff0c\u4e9f\u9700\u4f18\u5316\u3002", "method": "\u5bf9UPMEM\u7f16\u8bd1\u5668\u751f\u6210\u7684\u6c47\u7f16\u4ee3\u7801\u8fdb\u884c\u7b80\u5355\u4fee\u6539\uff1b\u91c7\u7528\u4f4d\u4e32\u884c\u5904\u7406\u4f4e\u7cbe\u5ea6\u6570\u636e\uff1b\u6269\u5c55API\u4ee5\u9002\u914d\u670d\u52a1\u5668\u7684NUMA\u67b6\u6784\u3002", "result": "\u6574\u6570\u52a0\u6cd5\u63d0\u901f1.6\u20132\u500d\uff0c\u4e58\u6cd5\u63d0\u901f1.4\u20135.9\u500d\uff1bINT4\u4f4d\u4e32\u884c\u70b9\u79ef\u63d0\u901f2.7\u500d\u4ee5\u4e0a\uff1b\u4e3b\u673a-PIM\u6570\u636e\u4f20\u8f93\u541e\u5410\u63d0\u5347\u8fbe2.9\u500d\uff1b\u9884\u52a0\u8f7d\u77e9\u9635\u4e0b\uff0cINT8\u548cINT4 GEMV\u5206\u522b\u6bd4\u53cc\u8defCPU\u5feb3\u500d\u548c10\u500d\u3002", "conclusion": "\u901a\u8fc7\u8f6f\u786c\u4ef6\u534f\u540c\u7684\u975e\u4f20\u7edf\u4f18\u5316\u624b\u6bb5\uff0c\u53ef\u663e\u8457\u91ca\u653eUPMEM PIM\u5e73\u53f0\u7684\u8ba1\u7b97\u6f5c\u529b\uff0c\u5c24\u5176\u5728\u4f4e\u7cbe\u5ea6\u8ba1\u7b97\u573a\u666f\u4e2d\u4f18\u52bf\u660e\u663e\u3002"}}
{"id": "2510.16242", "categories": ["cs.SE", "cs.DL"], "pdf": "https://arxiv.org/pdf/2510.16242", "abs": "https://arxiv.org/abs/2510.16242", "authors": ["Eva Maxfield Brown", "Isaac Slaughter", "Nicholas Weber"], "title": "Code Contribution and Credit in Science", "comment": null, "summary": "Software development has become essential to scientific research, but its\nrelationship to traditional metrics of scholarly credit remains poorly\nunderstood. We develop a dataset of approximately 140,000 paired research\narticles and code repositories, as well as a predictive model that matches\nresearch article authors with software repository developer accounts. We use\nthis data to investigate how software development activities influence credit\nallocation in collaborative scientific settings. Our findings reveal\nsignificant patterns distinguishing software contributions from traditional\nauthorship credit. We find that nearly 30% of articles include non-author code\ncontributors- individuals who participated in software development but received\nno formal authorship recognition. While code-contributing authors show a modest\n$\\sim$4.2% increase in article citations, this effect becomes non-significant\nwhen controlling for domain, article type, and open access status. First\nauthors are significantly more likely to be code contributors than other author\npositions. Notably, we identify a negative relationship between coding\nfrequency and scholarly impact metrics. Authors who contribute code more\nfrequently exhibit progressively lower h-indices than non-coding colleagues,\neven when controlling for publication count, author position, domain, and\narticle type. These results suggest a disconnect between software contributions\nand credit, highlighting important implications for institutional reward\nstructures and science policy.", "AI": {"tldr": "\u8be5\u7814\u7a76\u901a\u8fc7\u5339\u914d\u7ea614\u4e07\u7bc7\u79d1\u7814\u8bba\u6587\u4e0e\u4ee3\u7801\u4ed3\u5e93\uff0c\u53d1\u73b0\u8fd130%\u7684\u8bba\u6587\u5305\u542b\u672a\u88ab\u5217\u4e3a\u4f5c\u8005\u7684\u4ee3\u7801\u8d21\u732e\u8005\uff0c\u4e14\u9891\u7e41\u53c2\u4e0e\u7f16\u7801\u7684\u4f5c\u8005\u5176\u5b66\u672f\u5f71\u54cd\u529b\u6307\u6807\uff08\u5982h\u6307\u6570\uff09\u53cd\u800c\u66f4\u4f4e\uff0c\u8868\u660e\u8f6f\u4ef6\u8d21\u732e\u4e0e\u4f20\u7edf\u5b66\u672f\u8ba4\u53ef\u4e4b\u95f4\u5b58\u5728\u8131\u8282\u3002", "motivation": "\u63a2\u7d22\u8f6f\u4ef6\u5f00\u53d1\u5728\u79d1\u7814\u4e2d\u7684\u4f5c\u7528\u53ca\u5176\u4e0e\u4f20\u7edf\u5b66\u672f\u8ba4\u53ef\u673a\u5236\uff08\u5982\u4f5c\u8005\u7f72\u540d\u548c\u5f15\u7528\uff09\u4e4b\u95f4\u7684\u5173\u7cfb\uff0c\u4ee5\u7406\u89e3\u5f53\u524d\u5b66\u672f\u5956\u52b1\u4f53\u7cfb\u662f\u5426\u5145\u5206\u8ba4\u53ef\u8f6f\u4ef6\u8d21\u732e\u3002", "method": "\u6784\u5efa\u5305\u542b\u7ea614\u4e07\u5bf9\u79d1\u7814\u8bba\u6587\u4e0e\u4ee3\u7801\u4ed3\u5e93\u7684\u6570\u636e\u96c6\uff0c\u5f00\u53d1\u9884\u6d4b\u6a21\u578b\u5c06\u8bba\u6587\u4f5c\u8005\u4e0e\u4ee3\u7801\u4ed3\u5e93\u5f00\u53d1\u8005\u8d26\u6237\u5339\u914d\uff0c\u5e76\u901a\u8fc7\u7edf\u8ba1\u5206\u6790\u63a2\u7a76\u8f6f\u4ef6\u8d21\u732e\u4e0e\u5b66\u672f\u5f71\u54cd\u529b\u6307\u6807\u4e4b\u95f4\u7684\u5173\u7cfb\u3002", "result": "\u8fd130%\u7684\u8bba\u6587\u5305\u542b\u672a\u7f72\u540d\u7684\u4ee3\u7801\u8d21\u732e\u8005\uff1b\u4ee3\u7801\u8d21\u732e\u4f5c\u8005\u7684\u5f15\u7528\u91cf\u4ec5\u5c0f\u5e45\u589e\u52a0\uff08\u7ea64.2%\uff09\uff0c\u4f46\u5728\u63a7\u5236\u9886\u57df\u3001\u6587\u7ae0\u7c7b\u578b\u548c\u5f00\u653e\u83b7\u53d6\u72b6\u6001\u540e\u4e0d\u663e\u8457\uff1b\u7b2c\u4e00\u4f5c\u8005\u66f4\u53ef\u80fd\u662f\u4ee3\u7801\u8d21\u732e\u8005\uff1b\u9891\u7e41\u7f16\u7801\u8005\u7684h\u6307\u6570\u663e\u8457\u4f4e\u4e8e\u975e\u7f16\u7801\u8005\uff0c\u5373\u4f7f\u63a7\u5236\u4e86\u53d1\u8868\u6570\u91cf\u3001\u4f5c\u8005\u4f4d\u7f6e\u3001\u9886\u57df\u548c\u6587\u7ae0\u7c7b\u578b\u3002", "conclusion": "\u8f6f\u4ef6\u8d21\u732e\u5728\u5f53\u524d\u5b66\u672f\u8bc4\u4ef7\u4f53\u7cfb\u4e2d\u672a\u83b7\u5f97\u5145\u5206\u8ba4\u53ef\uff0c\u751a\u81f3\u4e0e\u4f20\u7edf\u5b66\u672f\u5f71\u54cd\u529b\u6307\u6807\u5448\u8d1f\u76f8\u5173\uff0c\u8fd9\u63d0\u793a\u9700\u8981\u6539\u9769\u673a\u6784\u5956\u52b1\u673a\u5236\u548c\u79d1\u7814\u653f\u7b56\uff0c\u4ee5\u66f4\u516c\u5e73\u5730\u627f\u8ba4\u8f6f\u4ef6\u5f00\u53d1\u5728\u79d1\u7814\u4e2d\u7684\u4ef7\u503c\u3002"}}
{"id": "2510.17158", "categories": ["cs.DC", "cs.PF", "cs.SE"], "pdf": "https://arxiv.org/pdf/2510.17158", "abs": "https://arxiv.org/abs/2510.17158", "authors": ["Daniel Nichols", "Konstantinos Parasyris", "Charles Jekel", "Abhinav Bhatele", "Harshitha Menon"], "title": "Integrating Performance Tools in Model Reasoning for GPU Kernel Optimization", "comment": null, "summary": "Language models are now prevalent in software engineering with many\ndevelopers using them to automate tasks and accelerate their development. While\nlanguage models have been tremendous at accomplishing complex software\nengineering tasks, there are still many areas where they fail to deliver\ndesirable results, for instance code performance related tasks. Tasks like\noptimization depend on many complex data from the environment, hardware, etc.\nthat are not directly represented in source code. Recent efforts have seen\nlarge improvements in general code modeling tasks using chain-of-thought style\nreasoning, but these models still fail to comprehend how the environment\ninteracts with code performance. In this paper we propose a methodology to\ntrain language models that can interact with performance tools during their\nreasoning process. We then demonstrate how this methodology can be used to\ntrain a state-of-the-art GPU kernel optimization model.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\u7684\u65b9\u6cd5\uff0c\u4f7f\u5176\u5728\u63a8\u7406\u8fc7\u7a0b\u4e2d\u80fd\u4e0e\u6027\u80fd\u5de5\u5177\u4ea4\u4e92\uff0c\u5e76\u6210\u529f\u5e94\u7528\u4e8eGPU\u5185\u6838\u4f18\u5316\u4efb\u52a1\u3002", "motivation": "\u5f53\u524d\u8bed\u8a00\u6a21\u578b\u5728\u4ee3\u7801\u6027\u80fd\u76f8\u5173\u4efb\u52a1\uff08\u5982\u4f18\u5316\uff09\u4e0a\u8868\u73b0\u4e0d\u4f73\uff0c\u56e0\u4e3a\u8fd9\u4e9b\u4efb\u52a1\u4f9d\u8d56\u4e8e\u6e90\u4ee3\u7801\u4e2d\u672a\u76f4\u63a5\u4f53\u73b0\u7684\u73af\u5883\u548c\u786c\u4ef6\u7b49\u590d\u6742\u6570\u636e\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\u7684\u65b9\u6cd5\uff0c\u4f7f\u5176\u5728\u63a8\u7406\u8fc7\u7a0b\u4e2d\u80fd\u591f\u4e0e\u6027\u80fd\u5de5\u5177\u8fdb\u884c\u4ea4\u4e92\u3002", "result": "\u8be5\u65b9\u6cd5\u88ab\u7528\u4e8e\u8bad\u7ec3\u4e00\u4e2a\u5148\u8fdb\u7684GPU\u5185\u6838\u4f18\u5316\u6a21\u578b\uff0c\u5c55\u793a\u4e86\u5176\u6709\u6548\u6027\u3002", "conclusion": "\u901a\u8fc7\u8ba9\u8bed\u8a00\u6a21\u578b\u5728\u63a8\u7406\u4e2d\u7ed3\u5408\u6027\u80fd\u5de5\u5177\uff0c\u53ef\u4ee5\u663e\u8457\u63d0\u5347\u5176\u5728\u4ee3\u7801\u6027\u80fd\u4f18\u5316\u7b49\u590d\u6742\u4efb\u52a1\u4e0a\u7684\u8868\u73b0\u3002"}}
{"id": "2510.16357", "categories": ["cs.SE", "cs.LG", "cs.PL"], "pdf": "https://arxiv.org/pdf/2510.16357", "abs": "https://arxiv.org/abs/2510.16357", "authors": ["Jugal Gajjar", "Kamalasankari Subramaniakuppusamy"], "title": "MLCPD: A Unified Multi-Language Code Parsing Dataset with Universal AST Schema", "comment": "12 pages, 7 figures, 4 tables, 2 algorithms, and 34 references.\n  HuggingFace:\n  https://huggingface.co/datasets/jugalgajjar/MultiLang-Code-Parser-Dataset\n  GitHub: https://github.com/JugalGajjar/MultiLang-Code-Parser-Dataset", "summary": "We introduce the MultiLang Code Parser Dataset (MLCPD), a large-scale,\nlanguage-agnostic dataset unifying syntactic and structural representations of\ncode across ten major programming languages. MLCPD contains over seven million\nparsed source files normalized under our proposed universal Abstract Syntax\nTree (AST) schema, enabling consistent cross-language reasoning, structural\nlearning, and multilingual software analysis. Unlike existing corpora that\nfocus purely on token-level code or isolated parsers, MLCPD provides both\nhierarchical tree representations and rich metadata for every file, ensuring\nlossless syntactic coverage and structural uniformity. Each entry includes a\nnormalized schema, language-level metadata, and abstracted node semantics\nstored in Parquet format for scalable retrieval. Empirical analyses reveal\nstrong cross-language structural regularities-demonstrating that syntactic\ngraphs from languages as diverse as Python, Java, and Go can be aligned under a\nshared schema. We release the dataset publicly on Hugging Face and the\naccompanying codebase on GitHub, which includes complete pipelines for dataset\nreproduction, grammar compilation, and a visualization tool for exploring the\nunified AST across languages. Together, these resources establish MLCPD as an\nopen, reproducible foundation for future research in cross-language\nrepresentation learning and program analysis.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u591a\u8bed\u8a00\u4ee3\u7801\u89e3\u6790\u5668\u6570\u636e\u96c6\uff08MLCPD\uff09\uff0c\u8fd9\u662f\u4e00\u4e2a\u6db5\u76d6\u5341\u79cd\u4e3b\u6d41\u7f16\u7a0b\u8bed\u8a00\u3001\u5305\u542b\u8d85\u8fc7\u4e03\u767e\u4e07\u4e2a\u6e90\u6587\u4ef6\u7684\u5927\u89c4\u6a21\u8bed\u8a00\u65e0\u5173\u6570\u636e\u96c6\uff0c\u91c7\u7528\u7edf\u4e00\u7684\u62bd\u8c61\u8bed\u6cd5\u6811\uff08AST\uff09\u6a21\u5f0f\uff0c\u652f\u6301\u8de8\u8bed\u8a00\u7684\u7ed3\u6784\u5316\u4ee3\u7801\u7406\u89e3\u548c\u5206\u6790\u3002", "motivation": "\u73b0\u6709\u4ee3\u7801\u8bed\u6599\u5e93\u901a\u5e38\u4ec5\u5173\u6ce8\u8bcd\u5143\u7ea7\u522b\u7684\u4ee3\u7801\u6216\u5b64\u7acb\u7684\u89e3\u6790\u5668\uff0c\u7f3a\u4e4f\u7edf\u4e00\u7684\u7ed3\u6784\u8868\u793a\u548c\u8de8\u8bed\u8a00\u4e00\u81f4\u6027\uff0c\u9650\u5236\u4e86\u591a\u8bed\u8a00\u7a0b\u5e8f\u5206\u6790\u548c\u8868\u793a\u5b66\u4e60\u7684\u53d1\u5c55\u3002", "method": "\u6784\u5efa\u5e76\u53d1\u5e03MLCPD\u6570\u636e\u96c6\uff0c\u63d0\u51fa\u4e00\u79cd\u901a\u7528\u7684\u62bd\u8c61\u8bed\u6cd5\u6811\uff08AST\uff09\u6a21\u5f0f\uff0c\u5bf9\u5341\u79cd\u7f16\u7a0b\u8bed\u8a00\u7684\u6e90\u4ee3\u7801\u8fdb\u884c\u89e3\u6790\u548c\u6807\u51c6\u5316\uff0c\u4fdd\u7559\u5c42\u6b21\u5316\u6811\u7ed3\u6784\u548c\u4e30\u5bcc\u7684\u5143\u6570\u636e\uff0c\u5e76\u4ee5Parquet\u683c\u5f0f\u5b58\u50a8\uff1b\u540c\u65f6\u63d0\u4f9b\u5b8c\u6574\u7684\u6570\u636e\u751f\u6210\u3001\u8bed\u6cd5\u7f16\u8bd1\u548c\u53ef\u89c6\u5316\u5de5\u5177\u94fe\u3002", "result": "\u5b9e\u8bc1\u5206\u6790\u8868\u660e\uff0cPython\u3001Java\u3001Go\u7b49\u5dee\u5f02\u663e\u8457\u7684\u8bed\u8a00\u5728\u8be5\u7edf\u4e00AST\u6a21\u5f0f\u4e0b\u5c55\u73b0\u51fa\u5f3a\u7ed3\u6784\u89c4\u5f8b\u6027\uff0c\u9a8c\u8bc1\u4e86\u8de8\u8bed\u8a00\u5bf9\u9f50\u7684\u53ef\u884c\u6027\u3002", "conclusion": "MLCPD\u4e3a\u8de8\u8bed\u8a00\u4ee3\u7801\u8868\u793a\u5b66\u4e60\u548c\u7a0b\u5e8f\u5206\u6790\u63d0\u4f9b\u4e86\u5f00\u653e\u3001\u53ef\u590d\u73b0\u7684\u57fa\u7840\u8d44\u6e90\uff0c\u63a8\u52a8\u4e86\u591a\u8bed\u8a00\u8f6f\u4ef6\u7406\u89e3\u7684\u7814\u7a76\u3002"}}
{"id": "2510.16284", "categories": ["cs.DC", "cs.MS", "cs.NA", "math.NA", "stat.CO", "65Y05, 65C60, 62F40", "F.2.2; G.3; D.1.3"], "pdf": "https://arxiv.org/pdf/2510.16284", "abs": "https://arxiv.org/abs/2510.16284", "authors": ["Di Zhang"], "title": "Communication-Efficient and Memory-Aware Parallel Bootstrapping using MPI", "comment": "6 pages", "summary": "Bootstrapping is a powerful statistical resampling technique for estimating\nthe sampling distribution of an estimator. However, its computational cost\nbecomes prohibitive for large datasets or a high number of resamples. This\npaper presents a theoretical analysis and design of parallel bootstrapping\nalgorithms using the Message Passing Interface (MPI). We address two key\nchallenges: high communication overhead and memory constraints in distributed\nenvironments. We propose two novel strategies: 1) Local Statistic Aggregation,\nwhich drastically reduces communication by transmitting sufficient statistics\ninstead of full resampled datasets, and 2) Synchronized Pseudo-Random Number\nGeneration, which enables distributed resampling when the entire dataset cannot\nbe stored on a single process. We develop analytical models for communication\nand computation complexity, comparing our methods against naive baseline\napproaches. Our analysis demonstrates that the proposed methods offer\nsignificant reductions in communication volume and memory usage, facilitating\nscalable parallel bootstrapping on large-scale systems.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e24\u79cd\u57fa\u4e8eMPI\u7684\u5e76\u884c\u81ea\u52a9\u6cd5\uff08bootstrapping\uff09\u7b56\u7565\uff0c\u901a\u8fc7\u672c\u5730\u7edf\u8ba1\u91cf\u805a\u5408\u548c\u540c\u6b65\u4f2a\u968f\u673a\u6570\u751f\u6210\uff0c\u663e\u8457\u964d\u4f4e\u901a\u4fe1\u5f00\u9500\u4e0e\u5185\u5b58\u5360\u7528\uff0c\u5b9e\u73b0\u5927\u89c4\u6a21\u6570\u636e\u4e0b\u7684\u9ad8\u6548\u5e76\u884c\u81ea\u52a9\u6cd5\u3002", "motivation": "\u4f20\u7edf\u81ea\u52a9\u6cd5\u5728\u5927\u6570\u636e\u96c6\u6216\u9ad8\u91cd\u91c7\u6837\u6b21\u6570\u4e0b\u8ba1\u7b97\u6210\u672c\u8fc7\u9ad8\uff0c\u96be\u4ee5\u5728\u5206\u5e03\u5f0f\u73af\u5883\u4e2d\u9ad8\u6548\u6269\u5c55\uff0c\u4e3b\u8981\u53d7\u9650\u4e8e\u901a\u4fe1\u5f00\u9500\u5927\u548c\u5185\u5b58\u9650\u5236\u3002", "method": "\u63d0\u51fa\u4e24\u79cd\u65b0\u7b56\u7565\uff1a1\uff09\u672c\u5730\u7edf\u8ba1\u91cf\u805a\u5408\uff0c\u901a\u8fc7\u4f20\u8f93\u5145\u5206\u7edf\u8ba1\u91cf\u800c\u975e\u5b8c\u6574\u91cd\u91c7\u6837\u6570\u636e\u4ee5\u51cf\u5c11\u901a\u4fe1\uff1b2\uff09\u540c\u6b65\u4f2a\u968f\u673a\u6570\u751f\u6210\uff0c\u652f\u6301\u5728\u5355\u8fdb\u7a0b\u65e0\u6cd5\u5b58\u50a8\u5168\u91cf\u6570\u636e\u65f6\u8fdb\u884c\u5206\u5e03\u5f0f\u91cd\u91c7\u6837\u3002\u540c\u65f6\u5efa\u7acb\u901a\u4fe1\u4e0e\u8ba1\u7b97\u590d\u6742\u5ea6\u7684\u5206\u6790\u6a21\u578b\u3002", "result": "\u6240\u63d0\u65b9\u6cd5\u76f8\u6bd4\u6734\u7d20\u57fa\u7ebf\u663e\u8457\u51cf\u5c11\u4e86\u901a\u4fe1\u91cf\u548c\u5185\u5b58\u4f7f\u7528\uff0c\u63d0\u5347\u4e86\u5e76\u884c\u81ea\u52a9\u6cd5\u5728\u5927\u89c4\u6a21\u7cfb\u7edf\u4e2d\u7684\u53ef\u6269\u5c55\u6027\u3002", "conclusion": "\u901a\u8fc7\u4f18\u5316\u901a\u4fe1\u4e0e\u5185\u5b58\u4f7f\u7528\uff0c\u672c\u6587\u63d0\u51fa\u7684\u5e76\u884c\u81ea\u52a9\u6cd5\u7b56\u7565\u6709\u6548\u89e3\u51b3\u4e86\u5927\u89c4\u6a21\u6570\u636e\u4e0b\u81ea\u52a9\u6cd5\u7684\u8ba1\u7b97\u74f6\u9888\uff0c\u4e3a\u9ad8\u6548\u7edf\u8ba1\u63a8\u65ad\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848\u3002"}}
{"id": "2510.16034", "categories": ["cs.MA", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.16034", "abs": "https://arxiv.org/abs/2510.16034", "authors": ["Bo Li", "Junwei Ma", "Kai Yin", "Yiming Xiao", "Chia-Wei Hsu", "Ali Mostafavi"], "title": "Disaster Management in the Era of Agentic AI Systems: A Vision for Collective Human-Machine Intelligence for Augmented Resilience", "comment": null, "summary": "The escalating frequency and severity of disasters routinely overwhelm\ntraditional response capabilities, exposing critical vulnerability in disaster\nmanagement. Current practices are hindered by fragmented data streams, siloed\ntechnologies, resource constraints, and the erosion of institutional memory,\nwhich collectively impede timely and effective decision making. This study\nintroduces Disaster Copilot, a vision for a multi-agent artificial intelligence\nsystem designed to overcome these systemic challenges by unifying specialized\nAI tools within a collaborative framework. The proposed architecture utilizes a\ncentral orchestrator to coordinate diverse sub-agents, each specializing in\ncritical domains such as predictive risk analytics, situational awareness, and\nimpact assessment. By integrating multi-modal data, the system delivers a\nholistic, real-time operational picture and serve as the essential AI backbone\nrequired to advance Disaster Digital Twins from passive models to active,\nintelligent environments. Furthermore, it ensures functionality in\nresource-limited environments through on-device orchestration and incorporates\nmechanisms to capture institutional knowledge, mitigating the impact of staff\nturnover. We detail the system architecture and propose a three-phased roadmap\nemphasizing the parallel growth of technology, organizational capacity, and\nhuman-AI teaming. Disaster Copilot offers a transformative vision, fostering\ncollective human-machine intelligence to build more adaptive, data-driven and\nresilient communities.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u201c\u707e\u96be\u526f\u9a7e\u201d\uff08Disaster Copilot\uff09\u2014\u2014\u4e00\u79cd\u591a\u667a\u80fd\u4f53AI\u7cfb\u7edf\uff0c\u901a\u8fc7\u6574\u5408\u9884\u6d4b\u5206\u6790\u3001\u6001\u52bf\u611f\u77e5\u4e0e\u5f71\u54cd\u8bc4\u4f30\u7b49\u4e13\u7528AI\u6a21\u5757\uff0c\u5728\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4e2d\u5b9e\u73b0\u534f\u540c\u51b3\u7b56\uff0c\u63a8\u52a8\u707e\u96be\u6570\u5b57\u5b6a\u751f\u4ece\u9759\u6001\u6a21\u578b\u5411\u4e3b\u52a8\u667a\u80fd\u73af\u5883\u6f14\u8fdb\u3002", "motivation": "\u4f20\u7edf\u707e\u5bb3\u5e94\u5bf9\u80fd\u529b\u5e38\u56e0\u6570\u636e\u788e\u7247\u5316\u3001\u6280\u672f\u5b64\u5c9b\u3001\u8d44\u6e90\u9650\u5236\u53ca\u673a\u6784\u8bb0\u5fc6\u6d41\u5931\u800c\u96be\u4ee5\u6709\u6548\u54cd\u5e94\u65e5\u76ca\u9891\u53d1\u548c\u4e25\u91cd\u7684\u707e\u5bb3\u4e8b\u4ef6\uff0c\u4e9f\u9700\u4e00\u79cd\u7cfb\u7edf\u6027\u89e3\u51b3\u65b9\u6848\u63d0\u5347\u51b3\u7b56\u6548\u7387\u4e0e\u97e7\u6027\u3002", "method": "\u8bbe\u8ba1\u4e00\u4e2a\u591a\u667a\u80fd\u4f53AI\u67b6\u6784\uff0c\u7531\u4e2d\u592e\u534f\u8c03\u5668\u7edf\u7b79\u591a\u4e2a\u4e13\u4e1a\u5b50\u667a\u80fd\u4f53\uff08\u5982\u98ce\u9669\u9884\u6d4b\u3001\u6001\u52bf\u611f\u77e5\u3001\u5f71\u54cd\u8bc4\u4f30\uff09\uff0c\u878d\u5408\u591a\u6a21\u6001\u6570\u636e\uff0c\u652f\u6301\u8bbe\u5907\u7aef\u90e8\u7f72\uff0c\u5e76\u5d4c\u5165\u673a\u6784\u77e5\u8bc6\u4fdd\u5b58\u673a\u5236\u3002", "result": "\u8be5\u7cfb\u7edf\u53ef\u63d0\u4f9b\u5b9e\u65f6\u6574\u4f53\u4f5c\u6218\u89c6\u56fe\uff0c\u652f\u6491\u707e\u96be\u6570\u5b57\u5b6a\u751f\u7684\u667a\u80fd\u5316\u5347\u7ea7\uff0c\u5e76\u5728\u8d44\u6e90\u53d7\u9650\u6761\u4ef6\u4e0b\u4fdd\u6301\u529f\u80fd\uff0c\u540c\u65f6\u7f13\u89e3\u4eba\u5458\u6d41\u52a8\u5e26\u6765\u7684\u77e5\u8bc6\u65ad\u5c42\u95ee\u9898\u3002", "conclusion": "Disaster Copilot\u901a\u8fc7\u4eba\u673a\u534f\u540c\u667a\u80fd\uff0c\u4e3a\u6784\u5efa\u66f4\u5177\u9002\u5e94\u6027\u3001\u6570\u636e\u9a71\u52a8\u548c\u97e7\u6027\u7684\u793e\u533a\u63d0\u4f9b\u4e86\u53d8\u9769\u6027\u613f\u666f\uff0c\u5e76\u63d0\u51fa\u6280\u672f\u3001\u7ec4\u7ec7\u80fd\u529b\u4e0e\u4eba\u673a\u534f\u4f5c\u4e09\u9636\u6bb5\u5e76\u884c\u53d1\u5c55\u8def\u7ebf\u56fe\u3002"}}
{"id": "2510.15887", "categories": ["cs.AR", "C.1.0; B.7.1"], "pdf": "https://arxiv.org/pdf/2510.15887", "abs": "https://arxiv.org/abs/2510.15887", "authors": ["Hyun Woo Kang", "Ji Woong Choi"], "title": "basic_RV32s: An Open-Source Microarchitectural Roadmap for RISC-V RV32I", "comment": "2 pages, 3 figures. Accepted to ISOCC 2025 (submitted 14 Jul. 2025;\n  accepted 8 Aug. 2025). To appear in the Proceedings of ISOCC 2025; oral\n  presentation on 17 Oct. 2025 (conference opens 15 Oct 2025). Camera-ready\n  version. Project repository: https://github.com/RISC-KC/basic_rv32s", "summary": "This paper introduces BASIC_RV32s, an open-source framework providing a\npractical microarchitectural roadmap for the RISC-V RV32I architecture,\naddressing the gap between theoretical knowledge and hardware implementation.\nFollowing the classic Patterson and Hennessy methodology, the design evolves\nfrom a basic single-cycle core to a 5-stage pipelined core design with full\nhazard forwarding, dynamic branch prediction, and exception handling. For\nverification, the final core design is integrated into a System-on-Chip (SoC)\nwith Universal Asynchronous Receiver-Transmitter (UART) communication\nimplemented on a Xilinx Artix-7 Field-Programmable Gate Array (FPGA), achieving\n1.09 Dhrystone million instructions per second per megahertz (DMIPS/MHz) at 50\nMHz. By releasing all Register-Transfer Level (RTL) source code, signal-level\nlogic block diagrams, and development logs under MIT license on GitHub,\nBASIC_RV32s offers a reproducible instructional pathway for the open-source\nhardware ecosystem.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86BASIC_RV32s\uff0c\u4e00\u4e2a\u5f00\u6e90\u7684RISC-V RV32I\u5fae\u67b6\u6784\u5b9e\u73b0\u6846\u67b6\uff0c\u4ece\u5355\u5468\u671f\u6838\u5fc3\u9010\u6b65\u6f14\u8fdb\u4e3a\u5177\u5907\u5b8c\u6574\u6d41\u6c34\u7ebf\u3001\u524d\u9012\u3001\u52a8\u6001\u5206\u652f\u9884\u6d4b\u548c\u5f02\u5e38\u5904\u7406\u7684\u4e94\u7ea7\u6d41\u6c34\u7ebf\u6838\u5fc3\uff0c\u5e76\u5728FPGA\u4e0a\u9a8c\u8bc1\uff0c\u540c\u65f6\u5f00\u6e90\u5168\u90e8RTL\u4ee3\u7801\u4e0e\u6587\u6863\u3002", "motivation": "\u5f25\u5408\u7406\u8bba\u77e5\u8bc6\u4e0eRISC-V\u786c\u4ef6\u5b9e\u73b0\u4e4b\u95f4\u7684\u5dee\u8ddd\uff0c\u4e3a\u5f00\u6e90\u786c\u4ef6\u793e\u533a\u63d0\u4f9b\u53ef\u590d\u73b0\u7684\u6559\u5b66\u8def\u5f84\u3002", "method": "\u57fa\u4e8ePatterson\u548cHennessy\u7684\u7ecf\u5178\u65b9\u6cd5\uff0c\u4ece\u5355\u5468\u671f\u6838\u5fc3\u9010\u6b65\u6784\u5efa\u4e94\u7ea7\u6d41\u6c34\u7ebf\u6838\u5fc3\uff0c\u5305\u542b\u524d\u9012\u3001\u52a8\u6001\u5206\u652f\u9884\u6d4b\u548c\u5f02\u5e38\u5904\u7406\uff1b\u6700\u7ec8\u96c6\u6210\u5230SoC\u5e76\u5728Xilinx Artix-7 FPGA\u4e0a\u901a\u8fc7UART\u8fdb\u884c\u9a8c\u8bc1\u3002", "result": "\u572850 MHz\u9891\u7387\u4e0b\u5b9e\u73b01.09 DMIPS/MHz\u6027\u80fd\uff0c\u5e76\u5728GitHub\u4e0a\u4ee5MIT\u8bb8\u53ef\u8bc1\u5f00\u6e90\u5168\u90e8RTL\u4ee3\u7801\u3001\u903b\u8f91\u6846\u56fe\u548c\u5f00\u53d1\u65e5\u5fd7\u3002", "conclusion": "BASIC_RV32s\u4e3aRISC-V\u6559\u5b66\u548c\u5f00\u6e90\u786c\u4ef6\u5f00\u53d1\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5b8c\u6574\u3001\u53ef\u590d\u73b0\u4e14\u5b9e\u7528\u7684\u5fae\u67b6\u6784\u5b9e\u73b0\u8303\u4f8b\u3002"}}
{"id": "2510.16384", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.16384", "abs": "https://arxiv.org/abs/2510.16384", "authors": ["Yuwei Zhao", "Yuan-An Xiao", "Qianyu Xiao", "Zhao Zhang", "Yingfei Xiong"], "title": "SemOpt: LLM-Driven Code Optimization via Rule-Based Analysis", "comment": null, "summary": "Automated code optimization aims to improve performance in programs by\nrefactoring code, and recent studies focus on utilizing LLMs for the\noptimization. Typical existing approaches mine optimization commits from\nopen-source codebases to construct a large-scale knowledge base, then employ\ninformation retrieval techniques such as BM25 to retrieve relevant optimization\nexamples for hotspot code locations, thereby guiding LLMs to optimize these\nhotspots. However, since semantically equivalent optimizations can manifest in\nsyntactically dissimilar code snippets, current retrieval methods often fail to\nidentify pertinent examples, leading to suboptimal optimization performance.\nThis limitation significantly reduces the effectiveness of existing\noptimization approaches.\n  To address these limitations, we propose SemOpt, a novel framework that\nleverages static program analysis to precisely identify optimizable code\nsegments, retrieve the corresponding optimization strategies, and generate the\noptimized results. SemOpt consists of three key components: (1) A strategy\nlibrary builder that extracts and clusters optimization strategies from\nreal-world code modifications. (2) A rule generator that generates Semgrep\nstatic analysis rules to capture the condition of applying the optimization\nstrategy. (3) An optimizer that utilizes the strategy library to generate\noptimized code results. All the three components are powered by LLMs.\n  On our benchmark containing 151 optimization tasks, SemOpt demonstrates its\neffectiveness under different LLMs by increasing the number of successful\noptimizations by 1.38 to 28 times compared to the baseline. Moreover, on\npopular large-scale C/C++ projects, it can improve individual performance\nmetrics by 5.04% to 218.07%, demonstrating its practical utility.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faSemOpt\uff0c\u4e00\u79cd\u7ed3\u5408\u9759\u6001\u7a0b\u5e8f\u5206\u6790\u4e0e\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u81ea\u52a8\u5316\u4ee3\u7801\u4f18\u5316\u6846\u67b6\uff0c\u901a\u8fc7\u6784\u5efa\u4f18\u5316\u7b56\u7565\u5e93\u3001\u751f\u6210Semgrep\u89c4\u5219\u5e76\u6307\u5bfcLLM\u8fdb\u884c\u4f18\u5316\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u548c\u771f\u5b9eC/C++\u9879\u76ee\u4e2d\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8eLLM\u7684\u4ee3\u7801\u4f18\u5316\u65b9\u6cd5\u4f9d\u8d56BM25\u7b49\u4fe1\u606f\u68c0\u7d22\u6280\u672f\u4ece\u5f00\u6e90\u63d0\u4ea4\u4e2d\u83b7\u53d6\u4f18\u5316\u793a\u4f8b\uff0c\u4f46\u7531\u4e8e\u8bed\u4e49\u7b49\u4ef7\u7684\u4f18\u5316\u53ef\u80fd\u5728\u8bed\u6cd5\u4e0a\u5dee\u5f02\u8f83\u5927\uff0c\u5bfc\u81f4\u68c0\u7d22\u5931\u8d25\uff0c\u5f71\u54cd\u4f18\u5316\u6548\u679c\u3002", "method": "SemOpt\u5305\u542b\u4e09\u4e2aLLM\u9a71\u52a8\u7684\u6838\u5fc3\u7ec4\u4ef6\uff1a(1) \u4ece\u771f\u5b9e\u4ee3\u7801\u53d8\u66f4\u4e2d\u63d0\u53d6\u5e76\u805a\u7c7b\u4f18\u5316\u7b56\u7565\u7684\u7b56\u7565\u5e93\u6784\u5efa\u5668\uff1b(2) \u751f\u6210Semgrep\u9759\u6001\u5206\u6790\u89c4\u5219\u4ee5\u8bc6\u522b\u9002\u7528\u4f18\u5316\u6761\u4ef6\u7684\u89c4\u5219\u751f\u6210\u5668\uff1b(3) \u5229\u7528\u7b56\u7565\u5e93\u751f\u6210\u4f18\u5316\u4ee3\u7801\u7684\u4f18\u5316\u5668\u3002", "result": "\u5728\u5305\u542b151\u4e2a\u4f18\u5316\u4efb\u52a1\u7684\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cSemOpt\u76f8\u6bd4\u57fa\u7ebf\u65b9\u6cd5\u5c06\u6210\u529f\u4f18\u5316\u6570\u91cf\u63d0\u53471.38\u81f328\u500d\uff1b\u5728\u4e3b\u6d41C/C++\u9879\u76ee\u4e2d\uff0c\u6027\u80fd\u6307\u6807\u63d0\u53475.04%\u81f3218.07%\u3002", "conclusion": "SemOpt\u901a\u8fc7\u7ed3\u5408\u9759\u6001\u5206\u6790\u4e0eLLM\uff0c\u6709\u6548\u514b\u670d\u4e86\u73b0\u6709\u68c0\u7d22\u65b9\u6cd5\u5728\u8bed\u4e49-\u8bed\u6cd5\u4e0d\u4e00\u81f4\u95ee\u9898\u4e0a\u7684\u5c40\u9650\u6027\uff0c\u663e\u8457\u63d0\u5347\u4e86\u81ea\u52a8\u5316\u4ee3\u7801\u4f18\u5316\u7684\u6548\u679c\u548c\u5b9e\u7528\u6027\u3002"}}
{"id": "2510.16187", "categories": ["cs.MA", "cs.AI", "cs.RO"], "pdf": "https://arxiv.org/pdf/2510.16187", "abs": "https://arxiv.org/abs/2510.16187", "authors": ["Rupal Nigam", "Niket Parikh", "Hamid Osooli", "Mikihisa Yuasa", "Jacob Heglund", "Huy T. Tran"], "title": "Zero-Shot Coordination in Ad Hoc Teams with Generalized Policy Improvement and Difference Rewards", "comment": "10 pages, 8 figures", "summary": "Real-world multi-agent systems may require ad hoc teaming, where an agent\nmust coordinate with other previously unseen teammates to solve a task in a\nzero-shot manner. Prior work often either selects a pretrained policy based on\nan inferred model of the new teammates or pretrains a single policy that is\nrobust to potential teammates. Instead, we propose to leverage all pretrained\npolicies in a zero-shot transfer setting. We formalize this problem as an ad\nhoc multi-agent Markov decision process and present a solution that uses two\nkey ideas, generalized policy improvement and difference rewards, for efficient\nand effective knowledge transfer between different teams. We empirically\ndemonstrate that our algorithm, Generalized Policy improvement for Ad hoc\nTeaming (GPAT), successfully enables zero-shot transfer to new teams in three\nsimulated environments: cooperative foraging, predator-prey, and Overcooked. We\nalso demonstrate our algorithm in a real-world multi-robot setting.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aGPAT\u7684\u65b0\u7b97\u6cd5\uff0c\u901a\u8fc7\u5e7f\u4e49\u7b56\u7565\u6539\u8fdb\u548c\u5dee\u5f02\u5956\u52b1\uff0c\u5728\u65e0\u9700\u989d\u5916\u8bad\u7ec3\u7684\u60c5\u51b5\u4e0b\u5b9e\u73b0\u5bf9\u65b0\u56e2\u961f\u7684\u96f6\u6837\u672c\u8fc1\u79fb\uff0c\u6709\u6548\u63d0\u5347\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u5728\u672a\u77e5\u961f\u53cb\u73af\u5883\u4e0b\u7684\u534f\u4f5c\u80fd\u529b\u3002", "motivation": "\u73b0\u5b9e\u4e16\u754c\u4e2d\u7684\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u5e38\u9700\u4e0e\u672a\u77e5\u961f\u53cb\u5373\u65f6\u534f\u4f5c\u5b8c\u6210\u4efb\u52a1\uff08\u5373\u5373\u5174\u7ec4\u961f\uff09\uff0c\u73b0\u6709\u65b9\u6cd5\u8981\u4e48\u57fa\u4e8e\u5bf9\u65b0\u961f\u53cb\u7684\u63a8\u65ad\u9009\u62e9\u9884\u8bad\u7ec3\u7b56\u7565\uff0c\u8981\u4e48\u8bad\u7ec3\u5bf9\u5404\u7c7b\u961f\u53cb\u90fd\u9c81\u68d2\u7684\u5355\u4e00\u7b56\u7565\uff0c\u4f46\u8fd9\u4e9b\u65b9\u6cd5\u672a\u80fd\u5145\u5206\u5229\u7528\u6240\u6709\u9884\u8bad\u7ec3\u7b56\u7565\u7684\u77e5\u8bc6\u3002", "method": "\u5c06\u5373\u5174\u7ec4\u961f\u95ee\u9898\u5f62\u5f0f\u5316\u4e3a\u5373\u5174\u591a\u667a\u80fd\u4f53\u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0b\uff0c\u5e76\u7ed3\u5408\u5e7f\u4e49\u7b56\u7565\u6539\u8fdb\uff08Generalized Policy Improvement\uff09\u4e0e\u5dee\u5f02\u5956\u52b1\uff08Difference Rewards\uff09\u673a\u5236\uff0c\u5b9e\u73b0\u4e0d\u540c\u56e2\u961f\u95f4\u9ad8\u6548\u6709\u6548\u7684\u77e5\u8bc6\u8fc1\u79fb\u3002", "result": "\u5728\u4e09\u4e2a\u6a21\u62df\u73af\u5883\uff08\u5408\u4f5c\u89c5\u98df\u3001\u6355\u98df\u8005-\u730e\u7269\u3001Overcooked\uff09\u53ca\u4e00\u4e2a\u771f\u5b9e\u591a\u673a\u5668\u4eba\u7cfb\u7edf\u4e2d\uff0cGPAT\u7b97\u6cd5\u6210\u529f\u5b9e\u73b0\u4e86\u5bf9\u65b0\u56e2\u961f\u7684\u96f6\u6837\u672c\u8fc1\u79fb\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684GPAT\u7b97\u6cd5\u80fd\u591f\u6709\u6548\u5229\u7528\u591a\u4e2a\u9884\u8bad\u7ec3\u7b56\u7565\uff0c\u5728\u65e0\u9700\u5fae\u8c03\u7684\u60c5\u51b5\u4e0b\u5feb\u901f\u9002\u5e94\u672a\u77e5\u961f\u53cb\uff0c\u663e\u8457\u63d0\u5347\u5373\u5174\u7ec4\u961f\u573a\u666f\u4e0b\u7684\u534f\u4f5c\u6027\u80fd\u3002"}}
{"id": "2510.15888", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2510.15888", "abs": "https://arxiv.org/abs/2510.15888", "authors": ["Konstantinos Kafousis"], "title": "Limited Read-Write/Set Hardware Transactional Memory without modifying the ISA or the Coherence Protocol", "comment": null, "summary": "Hardware Transactional Memory (HTM) allows lock-free programming as easy as\nwith traditional coarse-grain locks or similar, while benefiting from the\nperformance advantages of fine-grained locking. Many HTM implementations have\nbeen proposed, but they have not received widespread adoption because of their\nhigh hardware complexity, their need for additions to the Instruction Set\nArchitecture (ISA), and often for modifications to the cache coherence\nprotocol.\n  We show that HTM can be implemented without adding new instructions -- merely\nby extending the semantics of two existing, Load-Linked and Store-Conditional.\nAlso, our proposed design does not modify or extend standard coherence\nprotocols. We further propose to drastically simplify the implementation of HTM\n-- confined to modifications in the L1 Data Cache only -- by restricting it to\napplications where the write set plus the read set of each transaction do not\nexceed a small number of cache lines. We also propose two alternative\nmechanisms to guarantee forward progress, both based on detecting retrial\nattempts.\n  We simulated our proposed design in Gem5, and we used it to implement several\npopular concurrent data structures, showing that a maximum of eight (8) words\n(cache lines) suffice for the write plus read sets. We provide a detailed\nexplanation of selected implementations, clarifying the intended usage of our\nHTM from a programmer's perspective. We evaluated our HTM under varying\ncontention levels to explore its scalability limits. The results indicate that\nour HTM provides good performance in concurrent data structures when contention\nis spread across multiple nodes: in such cases, the percentage of aborts\nrelative to successful commits is very low. In the atomic fetch-and-increment\nbenchmark for multiple shared counters, the results show that, under\nlow-congestion, our HTM improves performance relative to the TTS lock.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u9700\u65b0\u589e\u6307\u4ee4\u3001\u4e0d\u4fee\u6539\u7f13\u5b58\u4e00\u81f4\u6027\u534f\u8bae\u7684\u786c\u4ef6\u4e8b\u52a1\u5185\u5b58\uff08HTM\uff09\u5b9e\u73b0\u65b9\u6cd5\uff0c\u4ec5\u901a\u8fc7\u6269\u5c55\u73b0\u6709Load-Linked/Store-Conditional\u6307\u4ee4\u8bed\u4e49\uff0c\u5e76\u5c06\u5b9e\u73b0\u9650\u5236\u5728L1\u6570\u636e\u7f13\u5b58\u4e2d\uff0c\u9002\u7528\u4e8e\u8bfb\u5199\u96c6\u4e0d\u8d85\u8fc7\u5c11\u91cf\u7f13\u5b58\u884c\u7684\u573a\u666f\u3002\u5b9e\u9a8c\u8868\u660e\u8be5\u65b9\u6cd5\u5728\u4f4e\u4e89\u7528\u548c\u591a\u8282\u70b9\u5206\u6563\u4e89\u7528\u60c5\u51b5\u4e0b\u6027\u80fd\u826f\u597d\u3002", "motivation": "\u73b0\u6709HTM\u5b9e\u73b0\u56e0\u786c\u4ef6\u590d\u6742\u5ea6\u9ad8\u3001\u9700\u6269\u5c55\u6307\u4ee4\u96c6\u67b6\u6784\uff08ISA\uff09\u53ca\u4fee\u6539\u7f13\u5b58\u4e00\u81f4\u6027\u534f\u8bae\u800c\u96be\u4ee5\u5e7f\u6cdb\u5e94\u7528\uff0c\u4f5c\u8005\u65e8\u5728\u8bbe\u8ba1\u4e00\u79cd\u66f4\u7b80\u5355\u3001\u517c\u5bb9\u6027\u66f4\u5f3a\u7684HTM\u65b9\u6848\u3002", "method": "\u901a\u8fc7\u6269\u5c55\u73b0\u6709Load-Linked\u548cStore-Conditional\u6307\u4ee4\u7684\u8bed\u4e49\u5b9e\u73b0HTM\uff0c\u4e0d\u5f15\u5165\u65b0\u6307\u4ee4\uff1b\u4e0d\u4fee\u6539\u6807\u51c6\u7f13\u5b58\u4e00\u81f4\u6027\u534f\u8bae\uff1b\u5c06HTM\u5b9e\u73b0\u9650\u5236\u5728L1\u6570\u636e\u7f13\u5b58\u4e2d\uff0c\u5e76\u9650\u5b9a\u4e8b\u52a1\u7684\u8bfb\u5199\u96c6\u4e0d\u8d85\u8fc7\u5c11\u91cf\u7f13\u5b58\u884c\uff1b\u63d0\u51fa\u4e24\u79cd\u57fa\u4e8e\u91cd\u8bd5\u68c0\u6d4b\u7684\u524d\u5411\u8fdb\u5c55\u4fdd\u969c\u673a\u5236\u3002", "result": "\u5728Gem5\u4e2d\u6a21\u62df\u5b9e\u73b0\uff0c\u7528\u4e8e\u591a\u4e2a\u5e76\u53d1\u6570\u636e\u7ed3\u6784\uff0c\u7ed3\u679c\u8868\u660e\u8bfb\u5199\u96c6\u6700\u591a\u53ea\u97008\u4e2a\u7f13\u5b58\u884c\uff1b\u5728\u591a\u8282\u70b9\u5206\u6563\u4e89\u7528\u4e0b\u4e8b\u52a1\u4e2d\u6b62\u7387\u4f4e\uff1b\u5728\u4f4e\u62e5\u585e\u7684\u539f\u5b50fetch-and-increment\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u6027\u80fd\u4f18\u4e8eTTS\u9501\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684HTM\u8bbe\u8ba1\u5728\u4fdd\u6301\u786c\u4ef6\u7b80\u6d01\u6027\u548c\u517c\u5bb9\u6027\u7684\u540c\u65f6\uff0c\u5728\u5178\u578b\u5e76\u53d1\u6570\u636e\u7ed3\u6784\u4e2d\u5c55\u73b0\u51fa\u826f\u597d\u7684\u6027\u80fd\u548c\u53ef\u6269\u5c55\u6027\uff0c\u5c24\u5176\u9002\u7528\u4e8e\u8bfb\u5199\u96c6\u8f83\u5c0f\u4e14\u4e89\u7528\u5206\u6563\u7684\u573a\u666f\u3002"}}
{"id": "2510.17395", "categories": ["cs.NI"], "pdf": "https://arxiv.org/pdf/2510.17395", "abs": "https://arxiv.org/abs/2510.17395", "authors": ["Dmitry Bankov", "Artem Krasilov", "Artem Otmakhov", "Aleksei Shashin", "Evgeny Khorov"], "title": "Enhancing 5G V2X Mode 2 for Sporadic Traffic", "comment": null, "summary": "The emerging road safety and autonomous vehicle applications require timely\nand reliable data delivery between vehicles and between vehicles and\ninfrastructure. To satisfy this demand, 3GPP develops a 5G\nVehicle-to-Everything (V2X) technology. Depending on the served traffic type,\n5G V2X specifications propose two channel access methods: (i) Mode 1, according\nto which a base station allocates resources to users, and (ii) Mode 2,\naccording to which users autonomously select resources for their transmissions.\nIn the paper, we consider a scenario with sporadic traffic, e.g., a vehicle\ngenerates a packet at a random time moment when it detects a dangerous\nsituation, which imposes strict requirements on delay and reliability. To\nsatisfy strict delay requirements, vehicles use Mode 2. We analyze the\nperformance of Mode 2 for sporadic traffic and propose several approaches to\nimprove it. Simulation results show that the proposed approaches can increase\nthe system capacity by up to 40% with a low impact on complexity.", "AI": {"tldr": "\u672c\u6587\u9488\u5bf95G\u8f66\u8054\u7f51\uff08V2X\uff09\u4e2d\u7a81\u53d1\u6027\u5b89\u5168\u7c7b\u4e1a\u52a1\uff0c\u5206\u6790\u4e86Mode 2\uff08\u81ea\u4e3b\u8d44\u6e90\u9009\u62e9\uff09\u7684\u6027\u80fd\uff0c\u5e76\u63d0\u51fa\u4e86\u82e5\u5e72\u6539\u8fdb\u65b9\u6cd5\uff0c\u5728\u51e0\u4e4e\u4e0d\u589e\u52a0\u590d\u6742\u5ea6\u7684\u524d\u63d0\u4e0b\u5c06\u7cfb\u7edf\u5bb9\u91cf\u63d0\u5347\u9ad8\u8fbe40%\u3002", "motivation": "\u65b0\u5174\u7684\u9053\u8def\u5b89\u5168\u4e0e\u81ea\u52a8\u9a7e\u9a76\u5e94\u7528\u8981\u6c42\u8f66\u8f86\u4e0e\u8f66\u8f86\u3001\u8f66\u8f86\u4e0e\u57fa\u7840\u8bbe\u65bd\u4e4b\u95f4\u5b9e\u73b0\u4f4e\u65f6\u5ef6\u3001\u9ad8\u53ef\u9760\u7684\u6570\u636e\u4f20\u8f93\u3002\u5bf9\u4e8e\u7a81\u53d1\u6027\u5b89\u5168\u4e8b\u4ef6\uff08\u5982\u5371\u9669\u60c5\u51b5\u68c0\u6d4b\uff09\uff0c\u4f20\u7edf\u8c03\u5ea6\u65b9\u5f0f\u96be\u4ee5\u6ee1\u8db3\u4e25\u82db\u7684\u65f6\u5ef6\u8981\u6c42\uff0c\u56e0\u6b64\u9700\u4f18\u5316\u57fa\u4e8eMode 2\u7684\u81ea\u4e3b\u8d44\u6e90\u5206\u914d\u673a\u5236\u3002", "method": "\u5206\u67905G V2X\u4e2dMode 2\u5728\u7a81\u53d1\u6d41\u91cf\u573a\u666f\u4e0b\u7684\u6027\u80fd\u8868\u73b0\uff0c\u5e76\u63d0\u51fa\u82e5\u5e72\u6539\u8fdb\u7b56\u7565\u4ee5\u63d0\u5347\u7cfb\u7edf\u5bb9\u91cf\u548c\u53ef\u9760\u6027\u3002", "result": "\u4eff\u771f\u7ed3\u679c\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684\u6539\u8fdb\u65b9\u6cd5\u53ef\u5728\u590d\u6742\u5ea6\u589e\u52a0\u6781\u5c11\u7684\u60c5\u51b5\u4e0b\uff0c\u5c06\u7cfb\u7edf\u5bb9\u91cf\u6700\u591a\u63d0\u534740%\u3002", "conclusion": "\u9488\u5bf9\u7a81\u53d1\u6027\u9ad8\u53ef\u9760\u4f4e\u65f6\u5ef6V2X\u901a\u4fe1\uff0c\u4f18\u5316Mode 2\u673a\u5236\u80fd\u663e\u8457\u63d0\u5347\u7cfb\u7edf\u6027\u80fd\uff0c\u4e3a\u672a\u6765\u8f66\u8054\u7f51\u5b89\u5168\u5e94\u7528\u63d0\u4f9b\u6709\u6548\u652f\u6301\u3002"}}
{"id": "2510.16395", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.16395", "abs": "https://arxiv.org/abs/2510.16395", "authors": ["Xin Peng", "Chong Wang"], "title": "Code Digital Twin: Empowering LLMs with Tacit Knowledge for Complex Software Development", "comment": null, "summary": "Recent advances in large language models (LLMs) have demonstrated strong\ncapabilities in software engineering tasks, raising expectations of\nrevolutionary productivity gains. However, enterprise software development is\nlargely driven by incremental evolution, where challenges extend far beyond\nroutine coding and depend critically on tacit knowledge, including design\ndecisions at different levels and historical trade-offs. To achieve effective\nAI-powered support for complex software development, we should align emerging\nAI capabilities with the practical realities of enterprise development. To this\nend, we systematically identify challenges from both software and LLM\nperspectives. Alongside these challenges, we outline opportunities where AI and\nstructured knowledge frameworks can enhance decision-making in tasks such as\nissue localization and impact analysis. To address these needs, we propose the\nCode Digital Twin, a living framework that models both the physical and\nconceptual layers of software, preserves tacit knowledge, and co-evolves with\nthe codebase. By integrating hybrid knowledge representations, multi-stage\nextraction pipelines, incremental updates, LLM-empowered applications, and\nhuman-in-the-loop feedback, the Code Digital Twin transforms fragmented\nknowledge into explicit and actionable representations. Our vision positions it\nas a bridge between AI advancements and enterprise software realities,\nproviding a concrete roadmap toward sustainable, intelligent, and resilient\ndevelopment and evolution of ultra-complex systems.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u201c\u4ee3\u7801\u6570\u5b57\u5b6a\u751f\u201d\uff08Code Digital Twin\uff09\u6846\u67b6\uff0c\u901a\u8fc7\u6574\u5408\u6df7\u5408\u77e5\u8bc6\u8868\u793a\u3001\u591a\u9636\u6bb5\u77e5\u8bc6\u62bd\u53d6\u3001\u589e\u91cf\u66f4\u65b0\u3001\u5927\u8bed\u8a00\u6a21\u578b\u5e94\u7528\u4e0e\u4eba\u673a\u534f\u540c\u53cd\u9988\uff0c\u5c06\u4f01\u4e1a\u8f6f\u4ef6\u5f00\u53d1\u4e2d\u7684\u9690\u6027\u77e5\u8bc6\u663e\u6027\u5316\uff0c\u4ee5\u5f25\u5408\u5927\u8bed\u8a00\u6a21\u578b\u80fd\u529b\u4e0e\u4f01\u4e1a\u7ea7\u8f6f\u4ef6\u5f00\u53d1\u5b9e\u9645\u9700\u6c42\u4e4b\u95f4\u7684\u9e3f\u6c9f\u3002", "motivation": "\u4f01\u4e1a\u8f6f\u4ef6\u5f00\u53d1\u4e3b\u8981\u4f9d\u8d56\u6e10\u8fdb\u5f0f\u6f14\u8fdb\uff0c\u6d89\u53ca\u5927\u91cf\u9690\u6027\u77e5\u8bc6\uff08\u5982\u8bbe\u8ba1\u51b3\u7b56\u548c\u5386\u53f2\u6743\u8861\uff09\uff0c\u800c\u5f53\u524d\u5927\u8bed\u8a00\u6a21\u578b\u867d\u5728\u7f16\u7801\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u5374\u96be\u4ee5\u5e94\u5bf9\u8fd9\u7c7b\u590d\u6742\u73b0\u5b9e\u6311\u6218\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u80fd\u5c06AI\u80fd\u529b\u4e0e\u4f01\u4e1a\u5f00\u53d1\u5b9e\u8df5\u5bf9\u9f50\u7684\u65b0\u6846\u67b6\u3002", "method": "\u63d0\u51fa\u201c\u4ee3\u7801\u6570\u5b57\u5b6a\u751f\u201d\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u5efa\u6a21\u8f6f\u4ef6\u7684\u7269\u7406\u5c42\u4e0e\u6982\u5ff5\u5c42\uff0c\u901a\u8fc7\u6df7\u5408\u77e5\u8bc6\u8868\u793a\u3001\u591a\u9636\u6bb5\u62bd\u53d6\u7ba1\u9053\u3001\u589e\u91cf\u66f4\u65b0\u673a\u5236\u3001\u5927\u8bed\u8a00\u6a21\u578b\u9a71\u52a8\u7684\u5e94\u7528\u4ee5\u53ca\u4eba\u5728\u73af\u8def\u53cd\u9988\uff0c\u5b9e\u73b0\u9690\u6027\u77e5\u8bc6\u7684\u6301\u7eed\u6355\u83b7\u4e0e\u663e\u6027\u5316\u3002", "result": "\u8be5\u6846\u67b6\u80fd\u5c06\u788e\u7247\u5316\u7684\u5f00\u53d1\u77e5\u8bc6\u8f6c\u5316\u4e3a\u660e\u786e\u4e14\u53ef\u64cd\u4f5c\u7684\u8868\u793a\u5f62\u5f0f\uff0c\u652f\u6301\u95ee\u9898\u5b9a\u4f4d\u3001\u5f71\u54cd\u5206\u6790\u7b49\u5173\u952e\u4efb\u52a1\uff0c\u63d0\u5347AI\u5728\u590d\u6742\u4f01\u4e1a\u8f6f\u4ef6\u5f00\u53d1\u4e2d\u7684\u5b9e\u7528\u6027\u3002", "conclusion": "\u4ee3\u7801\u6570\u5b57\u5b6a\u751f\u4e3a\u8fde\u63a5AI\u6280\u672f\u8fdb\u6b65\u4e0e\u4f01\u4e1a\u8f6f\u4ef6\u5f00\u53d1\u73b0\u5b9e\u63d0\u4f9b\u4e86\u53ef\u884c\u8def\u5f84\uff0c\u662f\u5b9e\u73b0\u8d85\u590d\u6742\u7cfb\u7edf\u53ef\u6301\u7eed\u3001\u667a\u80fd\u548c\u97e7\u6027\u6f14\u8fdb\u7684\u5173\u952e\u57fa\u7840\u8bbe\u65bd\u3002"}}
{"id": "2510.16418", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2510.16418", "abs": "https://arxiv.org/abs/2510.16418", "authors": ["Jian Ma", "Xinchen Lyu", "Jun Jiang", "Longhao Zou", "Chenshan Ren", "Qimei Cui", "Xiaofeng Tao"], "title": "FourierCompress: Layer-Aware Spectral Activation Compression for Efficient and Accurate Collaborative LLM Inference", "comment": null, "summary": "Collaborative large language model (LLM) inference enables real-time,\nprivacy-preserving AI services on resource-constrained edge devices by\npartitioning computational workloads between client devices and edge servers.\nHowever, this paradigm is severely hindered by communication bottlenecks caused\nby the transmission of high-dimensional intermediate activations, exacerbated\nby the autoregressive decoding structure of LLMs, where bandwidth consumption\nscales linearly with output length. Existing activation compression methods\nstruggle to simultaneously achieve high compression ratios, low reconstruction\nerror, and computational efficiency. This paper proposes FourierCompress, a\nnovel, layer-aware activation compression framework that exploits the\nfrequency-domain sparsity of LLM activations. We rigorously demonstrate that\nactivations from the first Transformer layer exhibit strong smoothness and\nenergy concentration in the low-frequency domain, making them highly amenable\nto near-lossless compression via the Fast Fourier Transform (FFT).\nFourierCompress transforms activations into the frequency domain, retains only\na compact block of low-frequency coefficients, and reconstructs the signal at\nthe server using conjugate symmetry, enabling seamless hardware acceleration on\nDSPs and FPGAs. Extensive experiments on Llama 3 and Qwen2.5 models across 10\ncommonsense reasoning datasets demonstrate that FourierCompress preserves\nperformance remarkably close to the uncompressed baseline, outperforming Top-k,\nQR, and SVD. FourierCompress bridges the gap between communication efficiency\n(an average 7.6x reduction in activation size), near-lossless inference (less\nthan 0.3% average accuracy loss), and significantly faster compression\n(achieving over 32x reduction in compression time compared to Top-k via\nhardware acceleration) for edge-device LLM inference.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aFourierCompress\u7684\u65b0\u578b\u6fc0\u6d3b\u538b\u7f29\u6846\u67b6\uff0c\u5229\u7528LLM\u6fc0\u6d3b\u5728\u9891\u57df\u4e2d\u7684\u7a00\u758f\u6027\uff0c\u901a\u8fc7\u5feb\u901f\u5085\u91cc\u53f6\u53d8\u6362\uff08FFT\uff09\u5bf9\u4f4e\u9891\u7cfb\u6570\u8fdb\u884c\u538b\u7f29\uff0c\u5728\u8fb9\u7f18\u8bbe\u5907\u4e0a\u5b9e\u73b0\u9ad8\u6548\u3001\u8fd1\u65e0\u635f\u7684\u5927\u6a21\u578b\u534f\u540c\u63a8\u7406\u3002", "motivation": "\u534f\u540c\u5927\u8bed\u8a00\u6a21\u578b\u63a8\u7406\u5728\u8fb9\u7f18\u8bbe\u5907\u4e0a\u9762\u4e34\u901a\u4fe1\u74f6\u9888\u95ee\u9898\uff0c\u4e3b\u8981\u7531\u4e8e\u81ea\u56de\u5f52\u89e3\u7801\u7ed3\u6784\u5bfc\u81f4\u4e2d\u95f4\u6fc0\u6d3b\u6570\u636e\u4f20\u8f93\u91cf\u968f\u8f93\u51fa\u957f\u5ea6\u7ebf\u6027\u589e\u957f\uff0c\u73b0\u6709\u538b\u7f29\u65b9\u6cd5\u96be\u4ee5\u517c\u987e\u9ad8\u538b\u7f29\u7387\u3001\u4f4e\u91cd\u5efa\u8bef\u5dee\u548c\u8ba1\u7b97\u6548\u7387\u3002", "method": "FourierCompress\u5c06\u6fc0\u6d3b\u8f6c\u6362\u5230\u9891\u57df\uff0c\u4fdd\u7559\u7d27\u51d1\u7684\u4f4e\u9891\u7cfb\u6570\u5757\uff0c\u5e76\u5229\u7528\u5171\u8f6d\u5bf9\u79f0\u6027\u5728\u670d\u52a1\u5668\u7aef\u91cd\u5efa\u4fe1\u53f7\uff1b\u8be5\u65b9\u6cd5\u7279\u522b\u9002\u7528\u4e8e\u7b2c\u4e00\u5c42Transformer\u8f93\u51fa\u7684\u5e73\u6ed1\u4e14\u80fd\u91cf\u96c6\u4e2d\u7684\u6fc0\u6d3b\uff0c\u652f\u6301DSP\u548cFPGA\u786c\u4ef6\u52a0\u901f\u3002", "result": "\u5728Llama 3\u548cQwen2.5\u6a21\u578b\u53ca10\u4e2a\u5e38\u8bc6\u63a8\u7406\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cFourierCompress\u76f8\u6bd4Top-k\u3001QR\u548cSVD\u7b49\u65b9\u6cd5\uff0c\u5728\u4fdd\u6301\u6027\u80fd\u63a5\u8fd1\u65e0\u538b\u7f29\u57fa\u7ebf\u7684\u540c\u65f6\uff0c\u5e73\u5747\u51cf\u5c117.6\u500d\u6fc0\u6d3b\u6570\u636e\u91cf\u3001\u5e73\u5747\u7cbe\u5ea6\u635f\u5931\u4f4e\u4e8e0.3%\uff0c\u5e76\u901a\u8fc7\u786c\u4ef6\u52a0\u901f\u5b9e\u73b0\u538b\u7f29\u65f6\u95f4\u51cf\u5c11\u8d8532\u500d\u3002", "conclusion": "FourierCompress\u6709\u6548\u5e73\u8861\u4e86\u901a\u4fe1\u6548\u7387\u3001\u8fd1\u65e0\u635f\u63a8\u7406\u7cbe\u5ea6\u4e0e\u538b\u7f29\u901f\u5ea6\uff0c\u4e3a\u8d44\u6e90\u53d7\u9650\u8fb9\u7f18\u8bbe\u5907\u4e0a\u7684\u5927\u8bed\u8a00\u6a21\u578b\u534f\u540c\u63a8\u7406\u63d0\u4f9b\u4e86\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.16221", "categories": ["cs.MA", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2510.16221", "abs": "https://arxiv.org/abs/2510.16221", "authors": ["Qinshuang Wei", "Vaibhav Srivastava", "Vijay Gupta"], "title": "Heterogeneous Multi-Agent Task-Assignment with Uncertain Execution Times and Preferences", "comment": "14 pages", "summary": "While sequential task assignment for a single agent has been widely studied,\nsuch problems in a multi-agent setting, where the agents have heterogeneous\ntask preferences or capabilities, remain less well-characterized. We study a\nmulti-agent task assignment problem where a central planner assigns recurring\ntasks to multiple members of a team over a finite time horizon. For any given\ntask, the members have heterogeneous capabilities in terms of task completion\ntimes, task resource consumption (which can model variables such as energy or\nattention), and preferences in terms of the rewards they collect upon task\ncompletion. We assume that the reward, execution time, and resource consumption\nfor each member to complete any task are stochastic with unknown distributions.\nThe goal of the planner is to maximize the total expected reward that the team\nreceives over the problem horizon while ensuring that the resource consumption\nrequired for any assigned task is within the capability of the agent. We\npropose and analyze a bandit algorithm for this problem. Since the bandit\nalgorithm relies on solving an optimal task assignment problem repeatedly, we\nanalyze the achievable regret in two cases: when we can solve the optimal task\nassignment exactly and when we can solve it only approximately.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u591a\u667a\u80fd\u4f53\u4efb\u52a1\u5206\u914d\u95ee\u9898\uff0c\u5176\u4e2d\u667a\u80fd\u4f53\u5728\u5b8c\u6210\u4efb\u52a1\u65f6\u5177\u6709\u5f02\u6784\u7684\u80fd\u529b\u4e0e\u504f\u597d\uff0c\u4e14\u4efb\u52a1\u76f8\u5173\u7684\u5956\u52b1\u3001\u6267\u884c\u65f6\u95f4\u548c\u8d44\u6e90\u6d88\u8017\u5747\u4e3a\u672a\u77e5\u5206\u5e03\u7684\u968f\u673a\u53d8\u91cf\u3002\u4f5c\u8005\u63d0\u51fa\u4e00\u79cdbandit\u7b97\u6cd5\uff0c\u5728\u6709\u9650\u65f6\u95f4\u8303\u56f4\u5185\u6700\u5927\u5316\u56e2\u961f\u603b\u671f\u671b\u5956\u52b1\uff0c\u540c\u65f6\u6ee1\u8db3\u8d44\u6e90\u7ea6\u675f\uff0c\u5e76\u5206\u6790\u4e86\u5728\u7cbe\u786e\u4e0e\u8fd1\u4f3c\u6c42\u89e3\u6700\u4f18\u4efb\u52a1\u5206\u914d\u4e0b\u7684\u7d2f\u79ef\u9057\u61be\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u591a\u96c6\u4e2d\u4e8e\u5355\u667a\u80fd\u4f53\u7684\u5e8f\u5217\u4efb\u52a1\u5206\u914d\uff0c\u800c\u5bf9\u5177\u6709\u5f02\u6784\u80fd\u529b\u4e0e\u504f\u597d\u7684\u591a\u667a\u80fd\u4f53\u4efb\u52a1\u5206\u914d\u95ee\u9898\u7f3a\u4e4f\u6df1\u5165\u63a2\u8ba8\uff0c\u5c24\u5176\u5728\u4efb\u52a1\u5956\u52b1\u3001\u6267\u884c\u65f6\u95f4\u548c\u8d44\u6e90\u6d88\u8017\u5747\u4e3a\u672a\u77e5\u968f\u673a\u53d8\u91cf\u7684\u60c5\u51b5\u4e0b\u3002", "method": "\u63d0\u51fa\u5e76\u5206\u6790\u4e00\u79cdbandit\u7b97\u6cd5\uff0c\u8be5\u7b97\u6cd5\u5728\u6bcf\u4e2a\u65f6\u95f4\u6b65\u91cd\u590d\u6c42\u89e3\u5e26\u8d44\u6e90\u7ea6\u675f\u7684\u6700\u4f18\u4efb\u52a1\u5206\u914d\u95ee\u9898\uff0c\u5e76\u5206\u522b\u8003\u8651\u7cbe\u786e\u6c42\u89e3\u4e0e\u8fd1\u4f3c\u6c42\u89e3\u4e24\u79cd\u60c5\u5f62\u3002", "result": "\u5206\u6790\u4e86\u6240\u63d0bandit\u7b97\u6cd5\u5728\u7cbe\u786e\u548c\u8fd1\u4f3c\u6c42\u89e3\u6700\u4f18\u4efb\u52a1\u5206\u914d\u65f6\u7684\u7d2f\u79ef\u9057\u61be\uff08regret\uff09\u6027\u80fd\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u4e3a\u5177\u6709\u5f02\u6784\u80fd\u529b\u548c\u8d44\u6e90\u7ea6\u675f\u7684\u591a\u667a\u80fd\u4f53\u968f\u673a\u4efb\u52a1\u5206\u914d\u95ee\u9898\u63d0\u4f9b\u4e86\u7406\u8bba\u6846\u67b6\u548c\u7b97\u6cd5\uff0c\u5e76\u5bf9\u4e0d\u540c\u6c42\u89e3\u7cbe\u5ea6\u4e0b\u7684\u6027\u80fd\u8fdb\u884c\u4e86\u9057\u61be\u5206\u6790\u3002"}}
{"id": "2510.15893", "categories": ["cs.AR", "cs.AI", "cs.DC", "cs.LG", "68M10, 68M14", "B.4.3; C.2.4; C.4; I.2"], "pdf": "https://arxiv.org/pdf/2510.15893", "abs": "https://arxiv.org/abs/2510.15893", "authors": ["Mikhail Bernadskiy", "Peter Carson", "Thomas Graham", "Taylor Groves", "Ho John Lee", "Eric Yeh"], "title": "Accelerating Frontier MoE Training with 3D Integrated Optics", "comment": "12 pages, 11 figures. To be published in Hot Interconnects 2025", "summary": "The unabated growth in AI workload demands is driving the need for concerted\nadvances in compute, memory, and interconnect performance. As traditional\nsemiconductor scaling slows, high-speed interconnects have emerged as the new\nscaling engine, enabling the creation of larger logical GPUs by linking many\nGPUs into a single, low-latency, high-bandwidth compute domain. While initial\nscale-up fabrics leveraged copper interconnects for their power and cost\nadvantages, the maximum reach of passive electrical interconnects\n(approximately 1 meter) effectively limits the scale-up domain to within a\nsingle rack. The advent of 3D-stacked optics and logic offers a transformative,\npower-efficient scale-up solution for connecting hundreds of GPU packages\n(thousands of GPUs) across multiple data center racks. This work explores the\ndesign tradeoffs of scale-up technologies and demonstrates how frontier LLMs\nnecessitate novel photonic solutions to achieve aggressive power and\nperformance targets. We model the benefits of 3D CPO (Passage) enabled GPUs and\nswitches within the scale-up domain when training Frontier Mixture of Experts\n(MoE) models exceeding one trillion parameters. Our results show that the\nsubstantial increases in bandwidth and radix enabled by 3D CPO allow for an 8X\nincrease in scale-up capability. This affords new opportunities for\nmulti-dimensional parallelism within the scale-up domain and results in a 2.7X\nreduction in time-to-train, unlocking unprecedented model scaling.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u4e86\u5728AI\u5de5\u4f5c\u8d1f\u8f7d\u6301\u7eed\u589e\u957f\u7684\u80cc\u666f\u4e0b\uff0c\u5229\u75283D\u5171\u5c01\u88c5\u5149\u5b66\uff08CPO\uff09\u6280\u672f\u7a81\u7834\u4f20\u7edf\u7535\u4e92\u8fde\u9650\u5236\uff0c\u5b9e\u73b0\u8de8\u673a\u67b6GPU\u5927\u89c4\u6a21\u4e92\u8054\uff0c\u4ece\u800c\u663e\u8457\u63d0\u5347\u5927\u6a21\u578b\u8bad\u7ec3\u6548\u7387\u3002", "motivation": "\u4f20\u7edf\u534a\u5bfc\u4f53\u5de5\u827a\u7f29\u653e\u653e\u7f13\uff0c\u800cAI\u5de5\u4f5c\u8d1f\u8f7d\u5bf9\u7b97\u529b\u3001\u5185\u5b58\u548c\u4e92\u8fde\u6027\u80fd\u7684\u9700\u6c42\u4e0d\u65ad\u589e\u957f\uff0c\u4e9f\u9700\u65b0\u7684\u4e92\u8fde\u6280\u672f\u6765\u6269\u5c55GPU\u903b\u8f91\u89c4\u6a21\uff0c\u5c24\u5176\u662f\u5728\u8bad\u7ec3\u8d85\u5927\u89c4\u6a21\u6a21\u578b\uff08\u5982\u4e07\u4ebf\u53c2\u6570MoE\u6a21\u578b\uff09\u65f6\u3002", "method": "\u5206\u6790\u94dc\u4e92\u8fde\u4e0e3D\u5806\u53e0\u5149\u4e92\u8fde\uff083D CPO\uff09\u5728\u6269\u5c55\u6027\u3001\u529f\u8017\u548c\u5e26\u5bbd\u7b49\u65b9\u9762\u7684\u6743\u8861\uff0c\u5e76\u5bf9\u57fa\u4e8e3D CPO\u7684GPU\u548c\u4ea4\u6362\u673a\u5728\u8bad\u7ec3\u524d\u6cbfMoE\u6a21\u578b\u4e2d\u7684\u6027\u80fd\u8fdb\u884c\u5efa\u6a21\u4e0e\u8bc4\u4f30\u3002", "result": "3D CPO\u6280\u672f\u53ef\u5c06\u6269\u5c55\u80fd\u529b\u63d0\u53478\u500d\uff0c\u652f\u6301\u591a\u7ef4\u5e76\u884c\uff0c\u4f7f\u8bad\u7ec3\u65f6\u95f4\u51cf\u5c112.7\u500d\uff0c\u663e\u8457\u63d0\u5347\u6a21\u578b\u6269\u5c55\u80fd\u529b\u3002", "conclusion": "3D\u5171\u5c01\u88c5\u5149\u5b66\u4e92\u8fde\u662f\u6ee1\u8db3\u524d\u6cbf\u5927\u6a21\u578b\u4e25\u82db\u6027\u80fd\u4e0e\u529f\u8017\u76ee\u6807\u7684\u5173\u952e\u6280\u672f\uff0c\u80fd\u6709\u6548\u7a81\u7834\u5355\u673a\u67b6\u9650\u5236\uff0c\u5b9e\u73b0\u8de8\u673a\u67b6\u5927\u89c4\u6a21GPU\u4e92\u8054\uff0c\u5927\u5e45\u63d0\u5347\u8bad\u7ec3\u6548\u7387\u3002"}}
{"id": "2510.17410", "categories": ["cs.NI"], "pdf": "https://arxiv.org/pdf/2510.17410", "abs": "https://arxiv.org/abs/2510.17410", "authors": ["Dmitry Bankov", "Artem Krasilov", "Artem Otmakhov", "Pavel Savlukovich", "Evgeny Khorov"], "title": "Is It Worth to Use Feedback Channel in 5G V2X Platoon Scenarios?", "comment": null, "summary": "5G Vehicle-to-Everything (V2X) is a new technology developed by 3GPP to\nsupport inter-vehicle communication. In contrast to 4G V2X which allows only\nbroadcast communication, 5G V2X enables groupcast and unicast communication.\nSuch types of communication are needed for new V2X scenarios: platooning,\nextended sensors, remote driving, etc. To improve the data transmission\nreliability and assist in the selection of the transmission parameters in these\nscenarios, 5G V2X introduces a feedback channel that allows receivers to send\nacknowledgments in response to data packets. However, some part of the overall\nresource shall be allocated for the feedback channel, which reduces the amount\nof channel resources available for data transmission. In this paper, we\nconsider a scenario with a platoon, which generates groupcast traffic, and\nsurrounding vehicles, which generate legacy broadcast traffic. Using extensive\nsimulations in NS-3, we analyze how the usage of the feedback channel\ninfluences the overall system capacity. Our results show that depending on the\nplatoon size, groupcast, and broadcast traffic intensities, and their quality\nof service requirements, the usage of the feedback channel can in some cases\nsignificantly increase the system capacity (up to 2x), while in other cases it\nalmost halves the system capacity. We explain the reasons for such effects and\ndiscuss how to adaptively select the feedback channel parameters.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e865G V2X\u4e2d\u53cd\u9988\u4fe1\u9053\u5bf9\u7cfb\u7edf\u5bb9\u91cf\u7684\u5f71\u54cd\uff0c\u53d1\u73b0\u5176\u5728\u67d0\u4e9b\u573a\u666f\u4e0b\u53ef\u4f7f\u7cfb\u7edf\u5bb9\u91cf\u63d0\u5347\u81f32\u500d\uff0c\u800c\u5728\u5176\u4ed6\u60c5\u51b5\u4e0b\u5219\u53ef\u80fd\u964d\u4f4e\u8fd1\u4e00\u534a\uff0c\u5e76\u63d0\u51fa\u4e86\u81ea\u9002\u5e94\u9009\u62e9\u53cd\u9988\u4fe1\u9053\u53c2\u6570\u7684\u5efa\u8bae\u3002", "motivation": "5G V2X\u5f15\u5165\u53cd\u9988\u4fe1\u9053\u4ee5\u63d0\u5347\u6570\u636e\u4f20\u8f93\u53ef\u9760\u6027\u5e76\u8f85\u52a9\u4f20\u8f93\u53c2\u6570\u9009\u62e9\uff0c\u4f46\u53cd\u9988\u4fe1\u9053\u5360\u7528\u8d44\u6e90\u4f1a\u51cf\u5c11\u53ef\u7528\u4e8e\u6570\u636e\u4f20\u8f93\u7684\u8d44\u6e90\uff0c\u56e0\u6b64\u9700\u8bc4\u4f30\u5176\u5bf9\u7cfb\u7edf\u6574\u4f53\u5bb9\u91cf\u7684\u5f71\u54cd\u3002", "method": "\u901a\u8fc7NS-3\u4eff\u771f\u5e73\u53f0\uff0c\u5728\u5305\u542b\u7f16\u961f\u8f66\u8f86\uff08\u4ea7\u751f\u7ec4\u64ad\u6d41\u91cf\uff09\u548c\u5468\u56f4\u8f66\u8f86\uff08\u4ea7\u751f\u4f20\u7edf\u5e7f\u64ad\u6d41\u91cf\uff09\u7684\u573a\u666f\u4e2d\uff0c\u5206\u6790\u53cd\u9988\u4fe1\u9053\u4f7f\u7528\u5bf9\u7cfb\u7edf\u5bb9\u91cf\u7684\u5f71\u54cd\u3002", "result": "\u4eff\u771f\u7ed3\u679c\u8868\u660e\uff0c\u53cd\u9988\u4fe1\u9053\u5bf9\u7cfb\u7edf\u5bb9\u91cf\u7684\u5f71\u54cd\u9ad8\u5ea6\u4f9d\u8d56\u4e8e\u7f16\u961f\u89c4\u6a21\u3001\u7ec4\u64ad\u4e0e\u5e7f\u64ad\u6d41\u91cf\u5f3a\u5ea6\u53ca\u670d\u52a1\u8d28\u91cf\u9700\u6c42\uff1a\u5728\u67d0\u4e9b\u6761\u4ef6\u4e0b\u7cfb\u7edf\u5bb9\u91cf\u53ef\u63d0\u5347\u81f32\u500d\uff0c\u800c\u5728\u5176\u4ed6\u6761\u4ef6\u4e0b\u5219\u53ef\u80fd\u4e0b\u964d\u8fd150%\u3002", "conclusion": "\u53cd\u9988\u4fe1\u9053\u7684\u4f7f\u7528\u5bf95G V2X\u7cfb\u7edf\u5bb9\u91cf\u5177\u6709\u53cc\u91cd\u5f71\u54cd\uff0c\u5e94\u6839\u636e\u5177\u4f53\u573a\u666f\u52a8\u6001\u8c03\u6574\u5176\u53c2\u6570\u4ee5\u4f18\u5316\u7cfb\u7edf\u6027\u80fd\u3002"}}
{"id": "2510.16433", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.16433", "abs": "https://arxiv.org/abs/2510.16433", "authors": ["Tatsuya Shirai", "Olivier Nourry", "Yutaro Kashiwa", "Kenji Fujiwara", "Yasutaka Kamei", "Hajimu Iida"], "title": "Large-Scale Empirical Analysis of Continuous Fuzzing: Insights from 1 Million Fuzzing Sessions", "comment": null, "summary": "Software vulnerabilities are constantly being reported and exploited in\nsoftware products, causing significant impacts on society. In recent years, the\nmain approach to vulnerability detection, fuzzing, has been integrated into the\ncontinuous integration process to run in short and frequent cycles. This\ncontinuous fuzzing allows for fast identification and remediation of\nvulnerabilities during the development process. Despite adoption by thousands\nof projects, however, it is unclear how continuous fuzzing contributes to\nvulnerability detection. This study aims to elucidate the role of continuous\nfuzzing in vulnerability detection. Specifically, we investigate the coverage\nand the total number of fuzzing sessions when fuzzing bugs are discovered. We\ncollect issue reports, coverage reports, and fuzzing logs from OSS-Fuzz, an\nonline service provided by Google that performs fuzzing during continuous\nintegration. Through an empirical study of a total of approximately 1.12\nmillion fuzzing sessions from 878 projects participating in OSS-Fuzz, we reveal\nthat (i) a substantial number of fuzzing bugs exist prior to the integration of\ncontinuous fuzzing, leading to a high detection rate in the early stages; (ii)\ncode coverage continues to increase as continuous fuzzing progresses; and (iii)\nchanges in coverage contribute to the detection of fuzzing bugs. This study\nprovides empirical insights into how continuous fuzzing contributes to fuzzing\nbug detection, offering practical implications for future strategies and tool\ndevelopment in continuous fuzzing.", "AI": {"tldr": "\u672c\u6587\u901a\u8fc7\u5206\u6790OSS-Fuzz\u5e73\u53f0\u4e0a878\u4e2a\u9879\u76ee\u7684\u7ea6112\u4e07\u6b21\u6a21\u7cca\u6d4b\u8bd5\u4f1a\u8bdd\uff0c\u63ed\u793a\u4e86\u6301\u7eed\u6a21\u7cca\u6d4b\u8bd5\u5728\u6f0f\u6d1e\u68c0\u6d4b\u4e2d\u7684\u4f5c\u7528\uff1a\u65e9\u671f\u68c0\u6d4b\u7387\u9ad8\u3001\u4ee3\u7801\u8986\u76d6\u7387\u6301\u7eed\u589e\u957f\uff0c\u4e14\u8986\u76d6\u7387\u53d8\u5316\u6709\u52a9\u4e8e\u53d1\u73b0\u6f0f\u6d1e\u3002", "motivation": "\u5c3d\u7ba1\u6301\u7eed\u6a21\u7cca\u6d4b\u8bd5\u5df2\u88ab\u5e7f\u6cdb\u91c7\u7528\uff0c\u4f46\u5176\u5728\u6f0f\u6d1e\u68c0\u6d4b\u4e2d\u7684\u5b9e\u9645\u8d21\u732e\u5c1a\u4e0d\u660e\u786e\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7\u5b9e\u8bc1\u7814\u7a76\u9610\u660e\u6301\u7eed\u6a21\u7cca\u6d4b\u8bd5\u5982\u4f55\u5f71\u54cd\u6f0f\u6d1e\u7684\u53d1\u73b0\u3002", "method": "\u4f5c\u8005\u4eceGoogle\u63d0\u4f9b\u7684OSS-Fuzz\u5e73\u53f0\u6536\u96c6\u4e86\u95ee\u9898\u62a5\u544a\u3001\u8986\u76d6\u7387\u62a5\u544a\u548c\u6a21\u7cca\u6d4b\u8bd5\u65e5\u5fd7\uff0c\u5bf9878\u4e2a\u5f00\u6e90\u9879\u76ee\u5171\u8ba1\u7ea6112\u4e07\u6b21\u6a21\u7cca\u6d4b\u8bd5\u4f1a\u8bdd\u8fdb\u884c\u4e86\u5b9e\u8bc1\u5206\u6790\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff1a(i) \u5927\u91cf\u6a21\u7cca\u6d4b\u8bd5\u6f0f\u6d1e\u5728\u96c6\u6210\u6301\u7eed\u6a21\u7cca\u6d4b\u8bd5\u524d\u5df2\u5b58\u5728\uff0c\u5bfc\u81f4\u65e9\u671f\u68c0\u6d4b\u7387\u9ad8\uff1b(ii) \u968f\u7740\u6301\u7eed\u6a21\u7cca\u6d4b\u8bd5\u8fdb\u884c\uff0c\u4ee3\u7801\u8986\u76d6\u7387\u6301\u7eed\u63d0\u5347\uff1b(iii) \u8986\u76d6\u7387\u7684\u53d8\u5316\u6709\u52a9\u4e8e\u53d1\u73b0\u65b0\u7684\u6a21\u7cca\u6d4b\u8bd5\u6f0f\u6d1e\u3002", "conclusion": "\u672c\u7814\u7a76\u4e3a\u6301\u7eed\u6a21\u7cca\u6d4b\u8bd5\u5728\u6f0f\u6d1e\u68c0\u6d4b\u4e2d\u7684\u4f5c\u7528\u63d0\u4f9b\u4e86\u5b9e\u8bc1\u4f9d\u636e\uff0c\u5e76\u5bf9\u672a\u6765\u6301\u7eed\u6a21\u7cca\u6d4b\u8bd5\u7b56\u7565\u548c\u5de5\u5177\u5f00\u53d1\u5177\u6709\u5b9e\u9645\u6307\u5bfc\u610f\u4e49\u3002"}}
{"id": "2510.16497", "categories": ["cs.DC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.16497", "abs": "https://arxiv.org/abs/2510.16497", "authors": ["Pacome Simon Mbonimpa", "Diane Tuyizere", "Azizuddin Ahmed Biyabani", "Ozan K. Tonguz"], "title": "Edge-Based Speech Transcription and Synthesis for Kinyarwanda and Swahili Languages", "comment": null, "summary": "This paper presents a novel framework for speech transcription and synthesis,\nleveraging edge-cloud parallelism to enhance processing speed and accessibility\nfor Kinyarwanda and Swahili speakers. It addresses the scarcity of powerful\nlanguage processing tools for these widely spoken languages in East African\ncountries with limited technological infrastructure. The framework utilizes the\nWhisper and SpeechT5 pre-trained models to enable speech-to-text (STT) and\ntext-to-speech (TTS) translation. The architecture uses a cascading mechanism\nthat distributes the model inference workload between the edge device and the\ncloud, thereby reducing latency and resource usage, benefiting both ends. On\nthe edge device, our approach achieves a memory usage compression of 9.5% for\nthe SpeechT5 model and 14% for the Whisper model, with a maximum memory usage\nof 149 MB. Experimental results indicate that on a 1.7 GHz CPU edge device with\na 1 MB/s network bandwidth, the system can process a 270-character text in less\nthan a minute for both speech-to-text and text-to-speech transcription. Using\nreal-world survey data from Kenya, it is shown that the cascaded edge-cloud\narchitecture proposed could easily serve as an excellent platform for STT and\nTTS transcription with good accuracy and response time.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9762\u5411\u4e1c\u975e\u8bed\u8a00\uff08\u5362\u65fa\u8fbe\u8bed\u548c\u65af\u74e6\u5e0c\u91cc\u8bed\uff09\u7684\u8fb9\u7f18-\u4e91\u534f\u540c\u8bed\u97f3\u8f6c\u5199\u4e0e\u5408\u6210\u6846\u67b6\uff0c\u901a\u8fc7\u5728\u8fb9\u7f18\u8bbe\u5907\u548c\u4e91\u7aef\u4e4b\u95f4\u5206\u914dWhisper\u548cSpeechT5\u6a21\u578b\u7684\u63a8\u7406\u4efb\u52a1\uff0c\u5728\u6709\u9650\u8d44\u6e90\u4e0b\u5b9e\u73b0\u4f4e\u5ef6\u8fdf\u3001\u4f4e\u5185\u5b58\u5360\u7528\u7684\u9ad8\u6548\u8bed\u97f3\u5904\u7406\u3002", "motivation": "\u4e1c\u975e\u56fd\u5bb6\u5e7f\u6cdb\u4f7f\u7528\u5362\u65fa\u8fbe\u8bed\u548c\u65af\u74e6\u5e0c\u91cc\u8bed\uff0c\u4f46\u53d7\u9650\u4e8e\u6280\u672f\u57fa\u7840\u8bbe\u65bd\uff0c\u7f3a\u4e4f\u9ad8\u6548\u7684\u8bed\u97f3\u5904\u7406\u5de5\u5177\u3002\u672c\u6587\u65e8\u5728\u89e3\u51b3\u8fd9\u4e9b\u8bed\u8a00\u5728\u8bed\u97f3\u8f6c\u5199\uff08STT\uff09\u548c\u8bed\u97f3\u5408\u6210\uff08TTS\uff09\u65b9\u9762\u7684\u8d44\u6e90\u532e\u4e4f\u95ee\u9898\uff0c\u63d0\u5347\u53ef\u8bbf\u95ee\u6027\u548c\u5904\u7406\u6548\u7387\u3002", "method": "\u91c7\u7528\u9884\u8bad\u7ec3\u6a21\u578bWhisper\uff08\u7528\u4e8eSTT\uff09\u548cSpeechT5\uff08\u7528\u4e8eTTS\uff09\uff0c\u6784\u5efa\u4e00\u79cd\u7ea7\u8054\u5f0f\u8fb9\u7f18-\u4e91\u5e76\u884c\u67b6\u6784\uff0c\u5c06\u6a21\u578b\u63a8\u7406\u4efb\u52a1\u5728\u8fb9\u7f18\u8bbe\u5907\u4e0e\u4e91\u7aef\u4e4b\u95f4\u52a8\u6001\u5206\u914d\uff0c\u4ee5\u964d\u4f4e\u5ef6\u8fdf\u548c\u8d44\u6e90\u6d88\u8017\u3002\u5728\u8fb9\u7f18\u7aef\u5bf9\u6a21\u578b\u8fdb\u884c\u538b\u7f29\u4f18\u5316\uff0c\u51cf\u5c11\u5185\u5b58\u5360\u7528\u3002", "result": "\u57281.7 GHz CPU\u30011 MB/s\u5e26\u5bbd\u7684\u8fb9\u7f18\u8bbe\u5907\u4e0a\uff0c\u7cfb\u7edf\u53ef\u5728\u4e00\u5206\u949f\u5185\u5b8c\u6210270\u5b57\u7b26\u6587\u672c\u7684STT\u548cTTS\u5904\u7406\uff1bSpeechT5\u548cWhisper\u6a21\u578b\u5185\u5b58\u5360\u7528\u5206\u522b\u538b\u7f299.5%\u548c14%\uff0c\u6700\u5927\u5185\u5b58\u4f7f\u7528\u4e3a149 MB\u3002\u57fa\u4e8e\u80af\u5c3c\u4e9a\u771f\u5b9e\u8c03\u67e5\u6570\u636e\u9a8c\u8bc1\u4e86\u8be5\u67b6\u6784\u7684\u51c6\u786e\u6027\u4e0e\u54cd\u5e94\u901f\u5ea6\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u7ea7\u8054\u8fb9\u7f18-\u4e91\u67b6\u6784\u80fd\u6709\u6548\u652f\u6301\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4e0b\u5362\u65fa\u8fbe\u8bed\u548c\u65af\u74e6\u5e0c\u91cc\u8bed\u7684\u9ad8\u8d28\u91cf\u8bed\u97f3\u8f6c\u5199\u4e0e\u5408\u6210\uff0c\u5177\u5907\u826f\u597d\u7684\u5b9e\u7528\u6027\u4e0e\u53ef\u6269\u5c55\u6027\u3002"}}
{"id": "2510.16635", "categories": ["cs.MA", "cs.AI", "cs.CL", "cs.HC", "cs.IR"], "pdf": "https://arxiv.org/pdf/2510.16635", "abs": "https://arxiv.org/abs/2510.16635", "authors": ["Wonduk Seo", "Juhyeon Lee", "Junseo Koh", "Hyunjin An", "Jian Park", "Seunghyun Lee", "Haihua Chen", "Yi Bu"], "title": "Prompt Optimization via Retrieved Reasoning Assets and Multi-Agent Analysis", "comment": "Preprint", "summary": "Prompt optimization has emerged as an effective alternative to retraining for\nimproving the performance of Large Language Models (LLMs). However, most\nexisting approaches treat evaluation as a black box, relying solely on\nnumerical scores while offering limited insight into why a prompt succeeds or\nfails. They also depend heavily on trial-and-error refinements, which are\ndifficult to interpret and control. In this paper, we introduce MA-SAPO, a\nMulti-Agent framework for Score-Aware Prompt Optimization. Compared to prior\nmethods, MA-SAPO explicitly couples evaluation outcomes with structured\nreasoning to guide systematic edits. The framework specifically consists of two\nstages: during the Reasoning Phase, agents collaboratively explain metric\nscores, diagnose weaknesses, and synthesize targeted refinements that are\nstored as reusable reasoning assets; during the Test Phase, agents retrieve\nthese assets to analyze optimized prompts and apply only evidence-grounded\nedits. By turning evaluation signals into interpretable reasoning chains,\nMA-SAPO produces prompt refinements that are more transparent, auditable, and\ncontrollable. Experiments on the HelpSteer1/2 benchmarks demonstrate consistent\nimprovements over single-pass prompting, retrieval-augmented baselines, and\nprior multi-agent strategies, validating the effectiveness of our approach.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faMA-SAPO\uff0c\u4e00\u79cd\u591a\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u901a\u8fc7\u5c06\u8bc4\u4f30\u5206\u6570\u4e0e\u7ed3\u6784\u5316\u63a8\u7406\u7ed3\u5408\uff0c\u5b9e\u73b0\u53ef\u89e3\u91ca\u3001\u53ef\u63a7\u7684\u63d0\u793a\u4f18\u5316\u3002", "motivation": "\u73b0\u6709\u63d0\u793a\u4f18\u5316\u65b9\u6cd5\u5c06\u8bc4\u4f30\u89c6\u4e3a\u9ed1\u7bb1\uff0c\u4ec5\u4f9d\u8d56\u6570\u503c\u5206\u6570\uff0c\u7f3a\u4e4f\u5bf9\u63d0\u793a\u6210\u529f\u6216\u5931\u8d25\u539f\u56e0\u7684\u6df1\u5165\u7406\u89e3\uff0c\u4e14\u4f9d\u8d56\u96be\u4ee5\u89e3\u91ca\u548c\u63a7\u5236\u7684\u8bd5\u9519\u5f0f\u6539\u8fdb\u3002", "method": "MA-SAPO\u5305\u542b\u4e24\u4e2a\u9636\u6bb5\uff1a\u63a8\u7406\u9636\u6bb5\u4e2d\uff0c\u591a\u4e2a\u667a\u80fd\u4f53\u534f\u4f5c\u89e3\u91ca\u8bc4\u4f30\u5206\u6570\u3001\u8bca\u65ad\u5f31\u70b9\u5e76\u751f\u6210\u53ef\u590d\u7528\u7684\u7ed3\u6784\u5316\u63a8\u7406\u8d44\u4ea7\uff1b\u6d4b\u8bd5\u9636\u6bb5\u4e2d\uff0c\u667a\u80fd\u4f53\u68c0\u7d22\u8fd9\u4e9b\u8d44\u4ea7\uff0c\u4ec5\u5e94\u7528\u6709\u8bc1\u636e\u652f\u6301\u7684\u7f16\u8f91\u3002", "result": "\u5728HelpSteer1/2\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cMA-SAPO\u5728\u6027\u80fd\u4e0a\u6301\u7eed\u4f18\u4e8e\u5355\u6b21\u63d0\u793a\u3001\u68c0\u7d22\u589e\u5f3a\u57fa\u7ebf\u548c\u73b0\u6709\u591a\u79cd\u591a\u667a\u80fd\u4f53\u7b56\u7565\u3002", "conclusion": "\u901a\u8fc7\u5c06\u8bc4\u4f30\u4fe1\u53f7\u8f6c\u5316\u4e3a\u53ef\u89e3\u91ca\u7684\u63a8\u7406\u94fe\uff0cMA-SAPO\u5b9e\u73b0\u4e86\u66f4\u900f\u660e\u3001\u53ef\u5ba1\u8ba1\u548c\u53ef\u63a7\u7684\u63d0\u793a\u4f18\u5316\uff0c\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2510.15897", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2510.15897", "abs": "https://arxiv.org/abs/2510.15897", "authors": ["Kien Le Trung", "Truong-Son Hy"], "title": "DiffPlace: A Conditional Diffusion Framework for Simultaneous VLSI Placement Beyond Sequential Paradigms", "comment": null, "summary": "Chip placement, the task of determining optimal positions of circuit modules\non a chip canvas, is a critical step in the VLSI design flow that directly\nimpacts performance, power consumption, and routability. Traditional methods\nrely on analytical optimization or reinforcement learning, which struggle with\nhard placement constraints or require expensive online training for each new\ncircuit design. To address these limitations, we introduce DiffPlace, a\nframework that formulates chip placement as a conditional denoising diffusion\nprocess, enabling transferable placement policies that generalize to unseen\ncircuit netlists without retraining. DiffPlace leverages the generative\ncapabilities of diffusion models to efficiently explore the vast space of\nplacement while conditioning on circuit connectivity and relative quality\nmetrics to identify optimal solutions globally. Our approach combines\nenergy-guided sampling with constrained manifold diffusion to ensure placement\nlegality, achieving extremely low overlap across all experimental scenarios.\nOur method bridges the gap between optimization-based and learning-based\napproaches, offering a practical path toward automated, high-quality chip\nplacement for modern VLSI design. Our source code is publicly available at:\nhttps://github.com/HySonLab/DiffPlace/", "AI": {"tldr": "DiffPlace \u662f\u4e00\u79cd\u57fa\u4e8e\u6761\u4ef6\u53bb\u566a\u6269\u6563\u8fc7\u7a0b\u7684\u82af\u7247\u5e03\u5c40\u65b0\u65b9\u6cd5\uff0c\u65e0\u9700\u9488\u5bf9\u6bcf\u4e2a\u65b0\u7535\u8def\u91cd\u65b0\u8bad\u7ec3\uff0c\u5373\u53ef\u5728\u672a\u89c1\u8fc7\u7684\u7535\u8def\u7f51\u8868\u4e0a\u5b9e\u73b0\u9ad8\u8d28\u91cf\u3001\u5408\u6cd5\u7684\u5e03\u5c40\u3002", "motivation": "\u4f20\u7edf\u82af\u7247\u5e03\u5c40\u65b9\u6cd5\u5728\u5904\u7406\u786c\u6027\u5e03\u5c40\u7ea6\u675f\u65f6\u5b58\u5728\u56f0\u96be\uff0c\u6216\u9700\u4e3a\u6bcf\u4e2a\u65b0\u7535\u8def\u8bbe\u8ba1\u8fdb\u884c\u6602\u8d35\u7684\u5728\u7ebf\u8bad\u7ec3\uff0c\u96be\u4ee5\u9ad8\u6548\u6cdb\u5316\u3002", "method": "\u5c06\u82af\u7247\u5e03\u5c40\u5efa\u6a21\u4e3a\u6761\u4ef6\u53bb\u566a\u6269\u6563\u8fc7\u7a0b\uff0c\u5229\u7528\u6269\u6563\u6a21\u578b\u7684\u751f\u6210\u80fd\u529b\uff0c\u5728\u8003\u8651\u7535\u8def\u8fde\u63a5\u6027\u548c\u76f8\u5bf9\u8d28\u91cf\u6307\u6807\u7684\u6761\u4ef6\u4e0b\u5168\u5c40\u63a2\u7d22\u5e03\u5c40\u7a7a\u95f4\uff0c\u5e76\u7ed3\u5408\u80fd\u91cf\u5f15\u5bfc\u91c7\u6837\u4e0e\u7ea6\u675f\u6d41\u5f62\u6269\u6563\u4ee5\u786e\u4fdd\u5e03\u5c40\u5408\u6cd5\u6027\u3002", "result": "\u5728\u6240\u6709\u5b9e\u9a8c\u573a\u666f\u4e2d\u5747\u5b9e\u73b0\u4e86\u6781\u4f4e\u7684\u6a21\u5757\u91cd\u53e0\uff0c\u8868\u660e\u8be5\u65b9\u6cd5\u80fd\u6709\u6548\u751f\u6210\u9ad8\u8d28\u91cf\u4e14\u5408\u6cd5\u7684\u82af\u7247\u5e03\u5c40\u3002", "conclusion": "DiffPlace \u6210\u529f\u878d\u5408\u4e86\u57fa\u4e8e\u4f18\u5316\u4e0e\u57fa\u4e8e\u5b66\u4e60\u7684\u65b9\u6cd5\u4f18\u52bf\uff0c\u4e3a\u73b0\u4ee3 VLSI \u8bbe\u8ba1\u63d0\u4f9b\u4e86\u4e00\u6761\u5b9e\u7528\u7684\u81ea\u52a8\u5316\u9ad8\u8d28\u91cf\u82af\u7247\u5e03\u5c40\u8def\u5f84\u3002"}}
{"id": "2510.16502", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.16502", "abs": "https://arxiv.org/abs/2510.16502", "authors": ["Sebasti\u00e1n Pizard", "Ramiro Moreira", "Federico Galiano", "Ignacio Sastre", "Lorena Etcheverry"], "title": "On the Use of Large Language Models for Qualitative Synthesis", "comment": null, "summary": "Large language models (LLMs) show promise for supporting systematic reviews\n(SR), even complex tasks such as qualitative synthesis (QS). However, applying\nthem to a stage that is unevenly reported and variably conducted carries\nimportant risks: misuse can amplify existing weaknesses and erode confidence in\nthe SR findings. To examine the challenges of using LLMs for QS, we conducted a\ncollaborative autoethnography involving two trials. We evaluated each trial for\nmethodological rigor and practical usefulness, and interpreted the results\nthrough a technical lens informed by how LLMs are built and their current\nlimitations.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u4e86\u5c06\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7528\u4e8e\u7cfb\u7edf\u7efc\u8ff0\u4e2d\u7684\u5b9a\u6027\u7efc\u5408\uff08QS\uff09\u6240\u9762\u4e34\u7684\u6311\u6218\uff0c\u901a\u8fc7\u4e24\u6b21\u8bd5\u9a8c\u7684\u534f\u4f5c\u81ea\u6c11\u65cf\u5fd7\u7814\u7a76\uff0c\u8bc4\u4f30\u5176\u65b9\u6cd5\u4e25\u8c28\u6027\u4e0e\u5b9e\u7528\u4ef7\u503c\uff0c\u5e76\u7ed3\u5408LLMs\u7684\u6280\u672f\u7279\u6027\u4e0e\u5c40\u9650\u6027\u8fdb\u884c\u89e3\u8bfb\u3002", "motivation": "\u7531\u4e8e\u5b9a\u6027\u7efc\u5408\u5728\u7cfb\u7edf\u7efc\u8ff0\u4e2d\u62a5\u544a\u4e0d\u4e00\u81f4\u3001\u6267\u884c\u65b9\u5f0f\u591a\u6837\uff0c\u76f4\u63a5\u5e94\u7528\u5927\u8bed\u8a00\u6a21\u578b\u5b58\u5728\u8bef\u7528\u98ce\u9669\uff0c\u53ef\u80fd\u653e\u5927\u73b0\u6709\u7f3a\u9677\u5e76\u524a\u5f31\u7efc\u8ff0\u7ed3\u679c\u7684\u53ef\u4fe1\u5ea6\uff0c\u56e0\u6b64\u6709\u5fc5\u8981\u6df1\u5165\u63a2\u8ba8\u5176\u5e94\u7528\u6311\u6218\u3002", "method": "\u91c7\u7528\u534f\u4f5c\u81ea\u6c11\u65cf\u5fd7\u65b9\u6cd5\uff0c\u5f00\u5c55\u4e24\u9879\u8bd5\u9a8c\uff0c\u8bc4\u4f30\u5176\u65b9\u6cd5\u5b66\u4e25\u8c28\u6027\u548c\u5b9e\u9645\u6548\u7528\uff0c\u5e76\u7ed3\u5408\u5927\u8bed\u8a00\u6a21\u578b\u7684\u6280\u672f\u6784\u5efa\u539f\u7406\u4e0e\u5f53\u524d\u5c40\u9650\u8fdb\u884c\u6280\u672f\u6027\u89e3\u8bfb\u3002", "result": "\u7814\u7a76\u63ed\u793a\u4e86\u5728\u5b9a\u6027\u7efc\u5408\u4e2d\u4f7f\u7528\u5927\u8bed\u8a00\u6a21\u578b\u6240\u9762\u4e34\u7684\u5177\u4f53\u6311\u6218\uff0c\u5305\u62ec\u65b9\u6cd5\u9002\u914d\u6027\u3001\u7ed3\u679c\u53ef\u9760\u6027\u53ca\u6280\u672f\u5c40\u9650\u5e26\u6765\u7684\u6f5c\u5728\u98ce\u9669\u3002", "conclusion": "\u5728\u5c06\u5927\u8bed\u8a00\u6a21\u578b\u5e94\u7528\u4e8e\u7cfb\u7edf\u7efc\u8ff0\u7684\u5b9a\u6027\u7efc\u5408\u9636\u6bb5\u65f6\uff0c\u9700\u8c28\u614e\u5bf9\u5f85\u5176\u6280\u672f\u5c40\u9650\uff0c\u907f\u514d\u56e0\u8bef\u7528\u800c\u635f\u5bb3\u7efc\u8ff0\u8d28\u91cf\uff1b\u672a\u6765\u5e94\u52a0\u5f3a\u65b9\u6cd5\u89c4\u8303\u4e0e\u6a21\u578b\u9002\u914d\u6027\u7814\u7a76\u3002"}}
{"id": "2510.16606", "categories": ["cs.DC", "cs.NI"], "pdf": "https://arxiv.org/pdf/2510.16606", "abs": "https://arxiv.org/abs/2510.16606", "authors": ["Ertza Warraich", "Ali Imran", "Annus Zulfiqar", "Shay Vargaftik", "Sonia Fahmy", "Muhammad Shahbaz"], "title": "Reimagining RDMA Through the Lens of ML", "comment": "4 pages", "summary": "As distributed machine learning (ML) workloads scale to thousands of GPUs\nconnected by ultra-high-speed inter-connects, tail latency in collective\ncommunication has emerged as a primary bottleneck. Prior RDMA designs, like\nRoCE, IRN, and SRNIC, enforce strict reliability and in-order delivery, relying\non retransmissions and packet sequencing to ensure correctness. While effective\nfor general-purpose workloads, these mechanisms introduce complexity and\nlatency that scale poorly, where even rare packet losses or delays can\nconsistently degrade system performance. We introduce Celeris, a\ndomain-specific RDMA transport that revisits traditional reliability guarantees\nbased on ML's tolerance for lost or partial data. Celeris removes\nretransmissions and in-order delivery from the RDMA NIC, enabling best-effort\ntransport that exploits the robustness of ML workloads. It retains congestion\ncontrol (e.g., DCQCN) and manages communication with software-level mechanisms\nsuch as adaptive timeouts and data prioritization, while shifting loss recovery\nto the ML pipeline (e.g., using the Hadamard Transform). Early results show\nthat Celeris reduces 99th-percentile latency by up to 2.3x, cuts BRAM usage by\n67%, and nearly doubles NIC resilience to faults -- delivering a resilient,\nscalable transport tailored for ML at cluster scale.", "AI": {"tldr": "Celeris \u662f\u4e00\u79cd\u9762\u5411\u5206\u5e03\u5f0f\u673a\u5668\u5b66\u4e60\u7684\u65b0\u578b RDMA \u4f20\u8f93\u65b9\u6848\uff0c\u901a\u8fc7\u653e\u5f03\u4f20\u7edf RDMA \u4e2d\u7684\u91cd\u4f20\u548c\u6709\u5e8f\u4ea4\u4ed8\u673a\u5236\uff0c\u5229\u7528 ML \u5bf9\u6570\u636e\u4e22\u5931\u7684\u5bb9\u5fcd\u6027\uff0c\u663e\u8457\u964d\u4f4e\u5c3e\u90e8\u5ef6\u8fdf\u5e76\u63d0\u5347\u53ef\u6269\u5c55\u6027\u3002", "motivation": "\u5728\u5927\u89c4\u6a21\u5206\u5e03\u5f0f\u673a\u5668\u5b66\u4e60\u4e2d\uff0c\u4f20\u7edf RDMA\uff08\u5982 RoCE\u3001IRN\u3001SRNIC\uff09\u4f9d\u8d56\u91cd\u4f20\u548c\u5305\u6392\u5e8f\u6765\u4fdd\u8bc1\u53ef\u9760\u6027\u548c\u6709\u5e8f\u6027\uff0c\u4f46\u8fd9\u4e9b\u673a\u5236\u5728\u9762\u5bf9\u7f55\u89c1\u4e22\u5305\u6216\u5ef6\u8fdf\u65f6\u4f1a\u5bfc\u81f4\u5c3e\u90e8\u5ef6\u8fdf\u663e\u8457\u4e0a\u5347\uff0c\u6210\u4e3a\u6027\u80fd\u74f6\u9888\u3002", "method": "Celeris \u79fb\u9664\u4e86 RDMA \u7f51\u5361\u4e2d\u7684\u91cd\u4f20\u548c\u6709\u5e8f\u4ea4\u4ed8\u673a\u5236\uff0c\u91c7\u7528\u5c3d\u529b\u800c\u4e3a\u7684\u4f20\u8f93\u65b9\u5f0f\uff0c\u5e76\u4fdd\u7559\u62e5\u585e\u63a7\u5236\uff08\u5982 DCQCN\uff09\uff1b\u540c\u65f6\u901a\u8fc7\u8f6f\u4ef6\u5c42\u673a\u5236\uff08\u5982\u81ea\u9002\u5e94\u8d85\u65f6\u3001\u6570\u636e\u4f18\u5148\u7ea7\uff09\u548c ML \u7ba1\u9053\u4e2d\u7684\u5bb9\u9519\u65b9\u6cd5\uff08\u5982 Hadamard \u53d8\u6362\uff09\u5904\u7406\u4e22\u5305\u95ee\u9898\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cCeleris \u5c06\u7b2c 99 \u767e\u5206\u4f4d\u5ef6\u8fdf\u6700\u591a\u964d\u4f4e 2.3 \u500d\uff0cBRAM \u4f7f\u7528\u51cf\u5c11 67%\uff0c\u7f51\u5361\u5bf9\u6545\u969c\u7684\u5bb9\u5fcd\u5ea6\u63a5\u8fd1\u7ffb\u500d\u3002", "conclusion": "Celeris \u901a\u8fc7\u91cd\u65b0\u8bbe\u8ba1 RDMA \u53ef\u9760\u6027\u4fdd\u969c\u673a\u5236\uff0c\u4e3a\u5927\u89c4\u6a21\u673a\u5668\u5b66\u4e60\u63d0\u4f9b\u4e86\u4f4e\u5ef6\u8fdf\u3001\u9ad8\u53ef\u6269\u5c55\u4e14\u5177\u6709\u5bb9\u9519\u80fd\u529b\u7684\u901a\u4fe1\u4f20\u8f93\u65b9\u6848\u3002"}}
{"id": "2510.16850", "categories": ["cs.MA", "cs.DC", "cs.RO"], "pdf": "https://arxiv.org/pdf/2510.16850", "abs": "https://arxiv.org/abs/2510.16850", "authors": ["Uday Gopan", "Manjari Kulkarni", "Lakshasri S", "Kashish Mittal", "Sriram Radhakrishna", "Aditya Naskar", "Rameshwar DL"], "title": "DiRAC - Distributed Robot Awareness and Consensus", "comment": null, "summary": "DiRAC is a scalable, distributed framework designed to enable efficient task\nassignment and path planning in very large robotic swarms. It introduces a\nnovel zone-partitioned architecture with dynamically elected leaders and a\ntick-synchronized consensus protocol that yields strong consistency and\ndeterministic outcomes. For path planning, DiRAC uses a novel algorithm, a\nforce-based decentralized planner for real-time collision resolution. Validated\nwithin ROS 2 middleware through preliminary simulation, DiRAC demonstrates\narchitectural scalability and modular efficiency in simulated warehouse\nenvironments, laying the groundwork for real-world deployment in large-scale\nindustrial and logistics domains.", "AI": {"tldr": "DiRAC is a scalable, distributed framework for task assignment and path planning in large robotic swarms, featuring zone partitioning, dynamic leader election, tick-synchronized consensus, and a force-based decentralized planner, validated via ROS 2 simulations in warehouse scenarios.", "motivation": "To address the challenges of coordination, scalability, and real-time collision avoidance in very large robotic swarms for industrial and logistics applications.", "method": "DiRAC employs a zone-partitioned architecture with dynamically elected leaders and a tick-synchronized consensus protocol for strong consistency, combined with a novel force-based decentralized path planning algorithm for real-time collision resolution.", "result": "Preliminary ROS 2 simulations in warehouse environments show that DiRAC achieves architectural scalability and modular efficiency.", "conclusion": "DiRAC provides a promising foundation for deploying large-scale robotic swarms in real-world industrial and logistics settings."}}
{"id": "2510.15899", "categories": ["cs.AR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.15899", "abs": "https://arxiv.org/abs/2510.15899", "authors": ["Kiran Thorat", "Jiahui Zhao", "Yaotian Liu", "Amit Hasan", "Hongwu Peng", "Xi Xie", "Bin Lei", "Caiwen Ding"], "title": "LLM-VeriPPA: Power, Performance, and Area Optimization aware Verilog Code Generation with Large Language Models", "comment": null, "summary": "Large Language Models (LLMs) are gaining prominence in various fields, thanks\nto their ability to generate high- quality content from human instructions.\nThis paper delves into the field of chip design using LLMs, specifically in\nPower- Performance-Area (PPA) optimization and the generation of accurate\nVerilog codes for circuit designs. We introduce a novel framework VeriPPA\ndesigned to optimize PPA and generate Verilog code using LLMs. Our method\nincludes a two-stage process where the first stage focuses on improving the\nfunctional and syntactic correctness of the generated Verilog codes, while the\nsecond stage focuses on optimizing the Verilog codes to meet PPA constraints of\ncircuit designs, a crucial element of chip design. Our framework achieves an\n81.37% success rate in syntactic correctness and 62.06% in functional\ncorrectness for code genera- tion, outperforming current state-of-the-art\n(SOTA) methods. On the RTLLM dataset. On the VerilogEval dataset, our framework\nachieves 99.56% syntactic correctness and 43.79% functional correctness, also\nsurpassing SOTA, which stands at 92.11% for syntactic correctness and 33.57%\nfor functional correctness. Furthermore, Our framework able to optimize the PPA\nof the designs. These results highlight the potential of LLMs in handling\ncomplex technical areas and indicate an encouraging development in the\nautomation of chip design processes.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aVeriPPA\u7684\u65b0\u6846\u67b6\uff0c\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5728\u82af\u7247\u8bbe\u8ba1\u4e2d\u4f18\u5316\u529f\u8017-\u6027\u80fd-\u9762\u79ef\uff08PPA\uff09\u5e76\u751f\u6210\u51c6\u786e\u7684Verilog\u4ee3\u7801\u3002\u8be5\u6846\u67b6\u91c7\u7528\u4e24\u9636\u6bb5\u65b9\u6cd5\uff0c\u5206\u522b\u63d0\u5347Verilog\u4ee3\u7801\u7684\u529f\u80fd\u4e0e\u8bed\u6cd5\u6b63\u786e\u6027\uff0c\u5e76\u4f18\u5316PPA\u3002\u5b9e\u9a8c\u8868\u660e\uff0cVeriPPA\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u5747\u4f18\u4e8e\u5f53\u524d\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\u3002", "motivation": "\u82af\u7247\u8bbe\u8ba1\u4e2d\u7684\u529f\u8017-\u6027\u80fd-\u9762\u79ef\uff08PPA\uff09\u4f18\u5316\u548c\u9ad8\u8d28\u91cfVerilog\u4ee3\u7801\u751f\u6210\u662f\u590d\u6742\u4e14\u8017\u65f6\u7684\u4efb\u52a1\u3002\u73b0\u6709\u65b9\u6cd5\u5728\u529f\u80fd\u4e0e\u8bed\u6cd5\u6b63\u786e\u6027\u65b9\u9762\u4ecd\u6709\u4e0d\u8db3\u3002\u672c\u6587\u65e8\u5728\u63a2\u7d22\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5728\u8fd9\u4e00\u9ad8\u6280\u672f\u95e8\u69db\u9886\u57df\u7684\u5e94\u7528\u6f5c\u529b\uff0c\u63d0\u5347\u81ea\u52a8\u5316\u82af\u7247\u8bbe\u8ba1\u7684\u6548\u7387\u4e0e\u51c6\u786e\u6027\u3002", "method": "\u63d0\u51faVeriPPA\u6846\u67b6\uff0c\u91c7\u7528\u4e24\u9636\u6bb5\u6d41\u7a0b\uff1a\u7b2c\u4e00\u9636\u6bb5\u63d0\u5347\u751f\u6210Verilog\u4ee3\u7801\u7684\u8bed\u6cd5\u4e0e\u529f\u80fd\u6b63\u786e\u6027\uff1b\u7b2c\u4e8c\u9636\u6bb5\u5bf9\u4ee3\u7801\u8fdb\u884cPPA\u4f18\u5316\u4ee5\u6ee1\u8db3\u7535\u8def\u8bbe\u8ba1\u7ea6\u675f\u3002\u8be5\u65b9\u6cd5\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u5b9e\u73b0\uff0c\u5e76\u5728RTLLM\u548cVerilogEval\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u8bc4\u4f30\u3002", "result": "\u5728RTLLM\u6570\u636e\u96c6\u4e0a\uff0cVeriPPA\u5b9e\u73b0\u4e8681.37%\u7684\u8bed\u6cd5\u6b63\u786e\u7387\u548c62.06%\u7684\u529f\u80fd\u6b63\u786e\u7387\uff1b\u5728VerilogEval\u6570\u636e\u96c6\u4e0a\uff0c\u8bed\u6cd5\u6b63\u786e\u7387\u8fbe99.56%\uff0c\u529f\u80fd\u6b63\u786e\u7387\u4e3a43.79%\uff0c\u5747\u4f18\u4e8e\u5f53\u524dSOTA\u65b9\u6cd5\uff08\u8bed\u6cd592.11%\uff0c\u529f\u80fd33.57%\uff09\u3002\u6b64\u5916\uff0c\u6846\u67b6\u8fd8\u80fd\u6709\u6548\u4f18\u5316\u8bbe\u8ba1\u7684PPA\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\uff0c\u5927\u8bed\u8a00\u6a21\u578b\u5728\u590d\u6742\u6280\u672f\u9886\u57df\u5982\u82af\u7247\u8bbe\u8ba1\u4e2d\u5177\u6709\u663e\u8457\u6f5c\u529b\uff0cVeriPPA\u6846\u67b6\u5728Verilog\u4ee3\u7801\u751f\u6210\u4e0ePPA\u4f18\u5316\u65b9\u9762\u53d6\u5f97\u4e86\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u7684\u6210\u679c\uff0c\u4e3a\u82af\u7247\u8bbe\u8ba1\u81ea\u52a8\u5316\u63d0\u4f9b\u4e86\u65b0\u7684\u53ef\u884c\u8def\u5f84\u3002"}}
{"id": "2510.16579", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.16579", "abs": "https://arxiv.org/abs/2510.16579", "authors": ["Wendk\u00fbuni C. Ou\u00e9draogo", "Yinghua Li", "Xueqi Dang", "Pawel Borsukiewicz", "Xin Zhou", "Anil Koyuncu", "Jacques Klein", "David Lo", "Tegawend\u00e9 F. Bissyand\u00e9"], "title": "Human-Aligned Code Readability Assessment with Large Language Models", "comment": null, "summary": "Code readability is crucial for software comprehension and maintenance, yet\ndifficult to assess at scale. Traditional static metrics often fail to capture\nthe subjective, context-sensitive nature of human judgments. Large Language\nModels (LLMs) offer a scalable alternative, but their behavior as readability\nevaluators remains underexplored. We introduce CoReEval, the first large-scale\nbenchmark for evaluating LLM-based code readability assessment, comprising over\n1.4 million model-snippet-prompt evaluations across 10 state of the art LLMs.\nThe benchmark spans 3 programming languages (Java, Python, CUDA), 2 code types\n(functional code and unit tests), 4 prompting strategies (ZSL, FSL, CoT, ToT),\n9 decoding settings, and developer-guided prompts tailored to junior and senior\npersonas. We compare LLM outputs against human annotations and a validated\nstatic model, analyzing numerical alignment (MAE, Pearson's, Spearman's) and\njustification quality (sentiment, aspect coverage, semantic clustering). Our\nfindings show that developer-guided prompting grounded in human-defined\nreadability dimensions improves alignment in structured contexts, enhances\nexplanation quality, and enables lightweight personalization through persona\nframing. However, increased score variability highlights trade-offs between\nalignment, stability, and interpretability. CoReEval provides a robust\nfoundation for prompt engineering, model alignment studies, and human in the\nloop evaluation, with applications in education, onboarding, and CI/CD\npipelines where LLMs can serve as explainable, adaptable reviewers.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86 CoReEval\uff0c\u9996\u4e2a\u7528\u4e8e\u8bc4\u4f30\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u4ee3\u7801\u53ef\u8bfb\u6027\u5224\u65ad\u80fd\u529b\u7684\u5927\u89c4\u6a21\u57fa\u51c6\uff0c\u6db5\u76d610\u4e2aLLM\u30013\u79cd\u8bed\u8a00\u3001\u591a\u79cd\u63d0\u793a\u7b56\u7565\u548c\u5f00\u53d1\u8005\u89d2\u8272\u8bbe\u5b9a\uff0c\u53d1\u73b0\u57fa\u4e8e\u4eba\u7c7b\u5b9a\u4e49\u7ef4\u5ea6\u7684\u5f15\u5bfc\u5f0f\u63d0\u793a\u80fd\u63d0\u5347LLM\u4e0e\u4eba\u7c7b\u5224\u65ad\u7684\u4e00\u81f4\u6027\u548c\u89e3\u91ca\u8d28\u91cf\u3002", "motivation": "\u4f20\u7edf\u9759\u6001\u6307\u6807\u96be\u4ee5\u51c6\u786e\u53cd\u6620\u4ee3\u7801\u53ef\u8bfb\u6027\u7684\u4eba\u7c7b\u4e3b\u89c2\u5224\u65ad\uff0c\u800c\u5927\u8bed\u8a00\u6a21\u578b\u867d\u5177\u6f5c\u529b\uff0c\u4f46\u5176\u5728\u53ef\u8bfb\u6027\u8bc4\u4f30\u4e2d\u7684\u8868\u73b0\u5c1a\u7f3a\u4e4f\u7cfb\u7edf\u7814\u7a76\u3002", "method": "\u6784\u5efa\u5305\u542b140\u4e07\u6b21\u8bc4\u4f30\u7684 CoReEval \u57fa\u51c6\uff0c\u8986\u76d610\u4e2aLLM\u30013\u79cd\u7f16\u7a0b\u8bed\u8a00\u30012\u7c7b\u4ee3\u7801\u30014\u79cd\u63d0\u793a\u7b56\u7565\u30019\u79cd\u89e3\u7801\u8bbe\u7f6e\u53ca\u9762\u5411\u521d\u7ea7/\u9ad8\u7ea7\u5f00\u53d1\u8005\u7684\u4eba\u8bbe\u63d0\u793a\uff1b\u901a\u8fc7\u4e0e\u4eba\u5de5\u6807\u6ce8\u548c\u9759\u6001\u6a21\u578b\u5bf9\u6bd4\uff0c\u4ece\u6570\u503c\u4e00\u81f4\u6027\uff08MAE\u3001Pearson\u3001Spearman\uff09\u548c\u89e3\u91ca\u8d28\u91cf\uff08\u60c5\u611f\u3001\u7ef4\u5ea6\u8986\u76d6\u3001\u8bed\u4e49\u805a\u7c7b\uff09\u4e24\u65b9\u9762\u8fdb\u884c\u5206\u6790\u3002", "result": "\u5f15\u5bfc\u5f0f\u63d0\u793a\uff08\u5c24\u5176\u662f\u7ed3\u5408\u4eba\u7c7b\u5b9a\u4e49\u7684\u53ef\u8bfb\u6027\u7ef4\u5ea6\u548c\u5f00\u53d1\u8005\u4eba\u8bbe\uff09\u5728\u7ed3\u6784\u5316\u4e0a\u4e0b\u6587\u4e2d\u663e\u8457\u63d0\u5347LLM\u4e0e\u4eba\u7c7b\u5224\u65ad\u7684\u4e00\u81f4\u6027\u4e0e\u89e3\u91ca\u8d28\u91cf\uff0c\u4f46\u8bc4\u5206\u53d8\u5f02\u6027\u589e\u52a0\uff0c\u63ed\u793a\u4e86\u5bf9\u9f50\u6027\u3001\u7a33\u5b9a\u6027\u4e0e\u53ef\u89e3\u91ca\u6027\u4e4b\u95f4\u7684\u6743\u8861\u3002", "conclusion": "CoReEval \u4e3aLLM\u5728\u4ee3\u7801\u53ef\u8bfb\u6027\u8bc4\u4f30\u4e2d\u7684\u63d0\u793a\u5de5\u7a0b\u3001\u6a21\u578b\u5bf9\u9f50\u53ca\u4eba\u673a\u534f\u540c\u8bc4\u4f30\u63d0\u4f9b\u4e86\u575a\u5b9e\u57fa\u7840\uff0c\u9002\u7528\u4e8e\u6559\u80b2\u3001\u65b0\u4eba\u5f15\u5bfc\u548cCI/CD\u7b49\u573a\u666f\uff0c\u4f7fLLM\u6210\u4e3a\u53ef\u89e3\u91ca\u3001\u53ef\u5b9a\u5236\u7684\u4ee3\u7801\u8bc4\u5ba1\u5de5\u5177\u3002"}}
{"id": "2510.16890", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2510.16890", "abs": "https://arxiv.org/abs/2510.16890", "authors": ["Ji\u0159\u00ed Klepl", "Martin Kruli\u0161", "Maty\u00e1\u0161 Brabec"], "title": "Layout-Agnostic MPI Abstraction for Distributed Computing in Modern C++", "comment": "This preprint has not undergone peer review or any post-submission\n  improvements or corrections. The Version of Record of this contribution is\n  published in Recent Advances in the Message Passing Interface (EuroMPI 2025),\n  and is available online at https://doi.org/10.1007/978-3-032-07194-1_3", "summary": "Message Passing Interface (MPI) has been a well-established technology in the\ndomain of distributed high-performance computing for several decades. However,\none of its greatest drawbacks is a rather ancient pure-C interface. It lacks\nmany useful features of modern languages (namely C++), like basic type-checking\nor support for generic code design. In this paper, we propose a novel\nabstraction for MPI, which we implemented as an extension of the C++ Noarr\nlibrary. It follows Noarr paradigms (first-class layout and traversal\nabstraction) and offers layout-agnostic design of MPI applications. We also\nimplemented a layout-agnostic distributed GEMM kernel as a case study to\ndemonstrate the usability and syntax of the proposed abstraction. We show that\nthe abstraction achieves performance comparable to the state-of-the-art MPI C++\nbindings while allowing for a more flexible design of distributed applications.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eC++ Noarr\u5e93\u7684\u65b0\u578bMPI\u62bd\u8c61\uff0c\u901a\u8fc7\u5e03\u5c40\u65e0\u5173\u7684\u8bbe\u8ba1\u63d0\u5347MPI\u5e94\u7528\u7684\u7075\u6d3b\u6027\uff0c\u5e76\u5728\u4fdd\u6301\u4e0e\u73b0\u6709MPI C++\u7ed1\u5b9a\u76f8\u5f53\u6027\u80fd\u7684\u540c\u65f6\uff0c\u6539\u5584\u4e86\u7c7b\u578b\u5b89\u5168\u548c\u6cdb\u578b\u652f\u6301\u3002", "motivation": "MPI\u957f\u671f\u4f7f\u7528\u53e4\u8001\u7684\u7eafC\u63a5\u53e3\uff0c\u7f3a\u4e4f\u73b0\u4ee3\u8bed\u8a00\uff08\u5982C++\uff09\u7684\u7279\u6027\uff0c\u4f8b\u5982\u7c7b\u578b\u68c0\u67e5\u548c\u6cdb\u578b\u4ee3\u7801\u8bbe\u8ba1\u652f\u6301\uff0c\u9650\u5236\u4e86\u5f00\u53d1\u6548\u7387\u4e0e\u4ee3\u7801\u5b89\u5168\u6027\u3002", "method": "\u4f5c\u8005\u5c06MPI\u62bd\u8c61\u4f5c\u4e3aC++ Noarr\u5e93\u7684\u6269\u5c55\u5b9e\u73b0\uff0c\u9075\u5faaNoarr\u7684\u8303\u5f0f\uff08\u5982\u4e00\u7b49\u5e03\u5c40\u548c\u904d\u5386\u62bd\u8c61\uff09\uff0c\u63d0\u4f9b\u5e03\u5c40\u65e0\u5173\u7684MPI\u5e94\u7528\u8bbe\u8ba1\u65b9\u5f0f\uff0c\u5e76\u901a\u8fc7\u4e00\u4e2a\u5e03\u5c40\u65e0\u5173\u7684\u5206\u5e03\u5f0fGEMM\u5185\u6838\u4f5c\u4e3a\u6848\u4f8b\u7814\u7a76\u3002", "result": "\u6240\u63d0\u51fa\u7684\u62bd\u8c61\u5728\u6027\u80fd\u4e0a\u4e0e\u5f53\u524d\u6700\u5148\u8fdb\u7684MPI C++\u7ed1\u5b9a\u76f8\u5f53\uff0c\u540c\u65f6\u63d0\u4f9b\u4e86\u66f4\u7075\u6d3b\u7684\u5206\u5e03\u5f0f\u5e94\u7528\u8bbe\u8ba1\u80fd\u529b\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u6210\u529f\u5730\u5c06\u73b0\u4ee3C++\u7279\u6027\u5f15\u5165MPI\u7f16\u7a0b\uff0c\u901a\u8fc7Noarr\u5e93\u5b9e\u73b0\u4e86\u4e00\u79cd\u517c\u5177\u6027\u80fd\u4e0e\u7075\u6d3b\u6027\u7684MPI\u62bd\u8c61\uff0c\u6709\u52a9\u4e8e\u63d0\u5347\u9ad8\u6027\u80fd\u8ba1\u7b97\u4e2d\u5206\u5e03\u5f0f\u5e94\u7528\u7684\u5f00\u53d1\u6548\u7387\u4e0e\u53ef\u7ef4\u62a4\u6027\u3002"}}
{"id": "2510.16978", "categories": ["cs.MA"], "pdf": "https://arxiv.org/pdf/2510.16978", "abs": "https://arxiv.org/abs/2510.16978", "authors": ["Dheeraj Chintapalli", "Rikhil Tanugula", "Sunkalp Chandra"], "title": "Lark: Biologically Inspired Neuroevolution for Multi-Stakeholder LLM Agents", "comment": "39th Conference on Neural Information Processing Systems (NeurIPS\n  2025) Workshop: NeurIPS 2025 Workshop on Efficient Reasoning", "summary": "We present Lark, a biologically inspired decision-making framework that\ncouples LLM-driven reasoning with an evolutionary, stakeholder-aware\nMulti-Agent System (MAS). To address verbosity and stakeholder trade-offs, we\nintegrate four mechanisms: (i) plasticity, which applies concise adjustments to\ncandidate solutions; (ii) duplication and maturation, which copy\nhigh-performing candidates and specialize them into new modules; (iii)\nranked-choice stakeholder aggregation using influence-weighted Borda scoring;\nand (iv) compute awareness via token-based penalties that reward brevity. The\nsystem iteratively proposes diverse strategies, applies plasticity tweaks,\nsimulates stakeholder evaluations, aggregates preferences, selects top\ncandidates, and performs duplication/maturation while factoring compute cost\ninto final scores. In a controlled evaluation over 30 rounds comparing 14\nsystems, Lark Full achieves a mean rank of 2.55 (95% CI [2.17, 2.93]) and a\nmean composite score of 29.4/50 (95% CI [26.34, 32.46]), finishing Top-3 in 80%\nof rounds while remaining cost competitive with leading commercial models\n($0.016 per task). Paired Wilcoxon tests confirm that all four mechanisms\ncontribute significantly as ablating duplication/maturation yields the largest\ndeficit ({\\Delta}Score = 3.5, Cohen's d_z = 2.53, p < 0.001), followed by\nplasticity ({\\Delta}Score = 3.4, d_z = 1.86), ranked-choice voting\n({\\Delta}Score = 2.4, d_z = 1.20), and token penalties ({\\Delta}Score = 2.2,\nd_z = 1.63). Rather than a formal Markov Decision Process with constrained\noptimization, Lark is a practical, compute-aware neuroevolutionary loop that\nscales stakeholder-aligned strategy generation and makes trade-offs transparent\nthrough per-step metrics. Our work presents proof-of-concept findings and\ninvites community feedback as we expand toward real-world validation studies.", "AI": {"tldr": "Lark \u662f\u4e00\u4e2a\u53d7\u751f\u7269\u542f\u53d1\u7684\u51b3\u7b56\u6846\u67b6\uff0c\u7ed3\u5408\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u63a8\u7406\u4e0e\u8fdb\u5316\u5f0f\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\uff0c\u901a\u8fc7\u56db\u79cd\u673a\u5236\uff08\u53ef\u5851\u6027\u8c03\u6574\u3001\u590d\u5236\u6210\u719f\u3001\u52a0\u6743\u6392\u5e8f\u6295\u7968\u3001\u8ba1\u7b97\u6210\u672c\u611f\u77e5\uff09\u5b9e\u73b0\u7b80\u6d01\u3001\u9ad8\u6548\u3001\u517c\u987e\u591a\u65b9\u5229\u76ca\u7684\u7b56\u7565\u751f\u6210\uff0c\u5728\u5b9e\u9a8c\u4e2d\u8868\u73b0\u4f18\u5f02\u4e14\u6210\u672c\u53ef\u63a7\u3002", "motivation": "\u73b0\u6709\u51b3\u7b56\u7cfb\u7edf\u5728\u5904\u7406\u591a\u65b9\u5229\u76ca\u6743\u8861\u65f6\u5f80\u5f80\u5197\u957f\u4f4e\u6548\uff0c\u7f3a\u4e4f\u5bf9\u8ba1\u7b97\u6210\u672c\u548c\u5229\u76ca\u76f8\u5173\u8005\u504f\u597d\u7684\u900f\u660e\u6574\u5408\u3002Lark \u65e8\u5728\u6784\u5efa\u4e00\u4e2a\u517c\u987e\u7b80\u6d01\u6027\u3001\u591a\u65b9\u504f\u597d\u548c\u8ba1\u7b97\u6548\u7387\u7684\u5b9e\u7528\u578b\u51b3\u7b56\u6846\u67b6\u3002", "method": "Lark \u6846\u67b6\u878d\u5408 LLM \u63a8\u7406\u4e0e\u8fdb\u5316\u5f0f\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\uff0c\u5f15\u5165\u56db\u79cd\u6838\u5fc3\u673a\u5236\uff1a(i) \u53ef\u5851\u6027\uff08\u5bf9\u5019\u9009\u65b9\u6848\u8fdb\u884c\u7b80\u6d01\u8c03\u6574\uff09\uff1b(ii) \u590d\u5236\u4e0e\u6210\u719f\uff08\u590d\u5236\u9ad8\u6027\u80fd\u65b9\u6848\u5e76\u7279\u5316\u4e3a\u65b0\u6a21\u5757\uff09\uff1b(iii) \u57fa\u4e8e\u5f71\u54cd\u529b\u52a0\u6743\u7684\u6392\u5e8f\u9009\u62e9\u6295\u7968\uff08Borda \u8ba1\u5206\uff09\uff1b(iv) \u57fa\u4e8e token \u7684\u8ba1\u7b97\u6210\u672c\u60e9\u7f5a\u673a\u5236\u3002\u7cfb\u7edf\u901a\u8fc7\u8fed\u4ee3\u751f\u6210\u7b56\u7565\u3001\u6a21\u62df\u8bc4\u4f30\u3001\u805a\u5408\u504f\u597d\u3001\u9009\u62e9\u4e0e\u8fdb\u5316\uff0c\u5e76\u5c06\u8ba1\u7b97\u6210\u672c\u7eb3\u5165\u6700\u7ec8\u8bc4\u5206\u3002", "result": "\u572830\u8f6e\u5bf9\u7167\u5b9e\u9a8c\u4e2d\uff0cLark Full \u5e73\u5747\u6392\u540d2.55\uff0895% CI [2.17, 2.93]\uff09\uff0c\u5e73\u5747\u7efc\u5408\u5f97\u520629.4/50\uff0895% CI [26.34, 32.46]\uff09\uff0c80%\u7684\u8f6e\u6b21\u8fdb\u5165\u524d\u4e09\uff0c\u5355\u4efb\u52a1\u6210\u672c\u4ec5$0.016\u3002\u6d88\u878d\u5b9e\u9a8c\u8bc1\u660e\u56db\u4e2a\u673a\u5236\u5747\u663e\u8457\u6709\u6548\uff0c\u5176\u4e2d\u590d\u5236/\u6210\u719f\u673a\u5236\u5f71\u54cd\u6700\u5927\uff08\u0394Score = 3.5, p < 0.001\uff09\u3002", "conclusion": "Lark \u63d0\u4f9b\u4e86\u4e00\u79cd\u5b9e\u7528\u3001\u900f\u660e\u4e14\u8ba1\u7b97\u9ad8\u6548\u7684\u795e\u7ecf\u8fdb\u5316\u51b3\u7b56\u5faa\u73af\uff0c\u80fd\u6709\u6548\u751f\u6210\u517c\u987e\u591a\u65b9\u5229\u76ca\u7684\u7b56\u7565\u3002\u8be5\u7814\u7a76\u4e3a\u6982\u5ff5\u9a8c\u8bc1\uff0c\u672a\u6765\u5c06\u62d3\u5c55\u81f3\u771f\u5b9e\u573a\u666f\u9a8c\u8bc1\u3002"}}
{"id": "2510.15902", "categories": ["cs.AR", "cs.ET"], "pdf": "https://arxiv.org/pdf/2510.15902", "abs": "https://arxiv.org/abs/2510.15902", "authors": ["Shuhang Zhang", "Jelena Radulovic", "Thorsten Dworzak"], "title": "Fully Automated Verification Framework for Configurable IPs: From Requirements to Results", "comment": "DVCon Europe 2025", "summary": "The increasing competition in the semiconductor industry has created\nsignificant pressure to reduce chip prices while maintaining quality and\nreliability. Functional verification, particularly for configurable IPs, is a\nmajor contributor to development costs due to its complexity and\nresource-intensive nature. To address this, we propose a fully automated\nframework for requirements driven functional verification. The framework\nautomates key processes, including vPlan generation, testbench creation,\nregression execution, and reporting in a requirements management tool,\ndrastically reducing verification effort. This approach accelerates development\ncycles, minimizes human error, and enhances coverage, offering a scalable and\nefficient solution to the challenges of verifying configurable IPs.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5168\u81ea\u52a8\u7684\u9700\u6c42\u9a71\u52a8\u529f\u80fd\u9a8c\u8bc1\u6846\u67b6\uff0c\u7528\u4e8e\u964d\u4f4e\u53ef\u914d\u7f6eIP\u9a8c\u8bc1\u7684\u6210\u672c\u4e0e\u590d\u6742\u6027\u3002", "motivation": "\u534a\u5bfc\u4f53\u884c\u4e1a\u7ade\u4e89\u52a0\u5267\uff0c\u9700\u5728\u4fdd\u8bc1\u8d28\u91cf\u4e0e\u53ef\u9760\u6027\u7684\u540c\u65f6\u964d\u4f4e\u82af\u7247\u4ef7\u683c\uff0c\u800c\u53ef\u914d\u7f6eIP\u7684\u529f\u80fd\u9a8c\u8bc1\u56e0\u590d\u6742\u4e14\u8d44\u6e90\u5bc6\u96c6\uff0c\u6210\u4e3a\u5f00\u53d1\u6210\u672c\u7684\u4e3b\u8981\u6765\u6e90\u3002", "method": "\u6784\u5efa\u4e00\u4e2a\u5168\u81ea\u52a8\u6846\u67b6\uff0c\u96c6\u6210vPlan\u751f\u6210\u3001\u6d4b\u8bd5\u5e73\u53f0\u521b\u5efa\u3001\u56de\u5f52\u6267\u884c\u548c\u9700\u6c42\u7ba1\u7406\u5de5\u5177\u4e2d\u7684\u62a5\u544a\u7b49\u5173\u952e\u6d41\u7a0b\u3002", "result": "\u663e\u8457\u51cf\u5c11\u9a8c\u8bc1\u5de5\u4f5c\u91cf\uff0c\u52a0\u5feb\u5f00\u53d1\u5468\u671f\uff0c\u964d\u4f4e\u4eba\u4e3a\u9519\u8bef\uff0c\u5e76\u63d0\u5347\u8986\u76d6\u7387\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u53ef\u914d\u7f6eIP\u9a8c\u8bc1\u63d0\u4f9b\u4e86\u4e00\u79cd\u53ef\u6269\u5c55\u4e14\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.16665", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.16665", "abs": "https://arxiv.org/abs/2510.16665", "authors": ["Mohamed Sami Rakha", "Andriy Miranskyy", "Daniel Alencar da Costa"], "title": "Contrasting the Hyperparameter Tuning Impact Across Software Defect Prediction Scenarios", "comment": "Accepted to IEEE Transactions on Software Engineering", "summary": "Software defect prediction (SDP) is crucial for delivering high-quality\nsoftware products. Recent research has indicated that prediction performance\nimprovements in SDP are achievable by applying hyperparameter tuning to a\nparticular SDP scenario. However, the positive impact resulting from the\nhyperparameter tuning step may differ based on the targeted SDP scenario.\nComparing the impact of hyperparameter tuning across SDP scenarios is necessary\nto provide comprehensive insights and enhance the robustness, generalizability,\nand, eventually, the practicality of SDP modeling for quality assurance.\n  Therefore, in this study, we contrast the impact of hyperparameter tuning\nacross two pivotal and consecutive SDP scenarios: (1) Inner Version Defect\nPrediction (IVDP) and (2) Cross Version Defect Prediction (CVDP). The main\ndistinctions between the two scenarios lie in the scope of defect prediction\nand the selected evaluation setups. This study's experiments use common\nevaluation setups, 28 machine learning (ML) algorithms, 53 post-release\nsoftware datasets, two tuning algorithms, and five optimization metrics. We\napply statistical analytics to compare the SDP performance impact differences\nby investigating the overall impact, the single ML algorithm impact, and\nvariations across different software dataset sizes.\n  The results indicate that the SDP gains within the IVDP scenario are\nsignificantly larger than those within the CVDP scenario. The results reveal\nthat asserting performance gains for up to 24 out of 28 ML algorithms may not\nhold across multiple SDP scenarios. Furthermore, we found that small software\ndatasets are more susceptible to larger differences in performance impacts.\nOverall, the study findings recommend software engineering researchers and\npractitioners to consider the effect of the selected SDP scenario when\nexpecting performance gains from hyperparameter tuning.", "AI": {"tldr": "\u672c\u7814\u7a76\u6bd4\u8f83\u4e86\u8d85\u53c2\u6570\u8c03\u4f18\u5728\u4e24\u79cd\u8f6f\u4ef6\u7f3a\u9677\u9884\u6d4b\uff08SDP\uff09\u573a\u666f\u2014\u2014\u7248\u672c\u5185\u7f3a\u9677\u9884\u6d4b\uff08IVDP\uff09\u548c\u8de8\u7248\u672c\u7f3a\u9677\u9884\u6d4b\uff08CVDP\uff09\u4e2d\u7684\u6548\u679c\u5dee\u5f02\uff0c\u53d1\u73b0IVDP\u4e2d\u8c03\u4f18\u5e26\u6765\u7684\u6027\u80fd\u63d0\u5347\u663e\u8457\u9ad8\u4e8eCVDP\uff0c\u4e14\u5c0f\u89c4\u6a21\u6570\u636e\u96c6\u5bf9\u8c03\u4f18\u6548\u679c\u66f4\u654f\u611f\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u8868\u660e\u8d85\u53c2\u6570\u8c03\u4f18\u53ef\u63d0\u5347SDP\u6027\u80fd\uff0c\u4f46\u5176\u6548\u679c\u53ef\u80fd\u56e0SDP\u573a\u666f\u4e0d\u540c\u800c\u5f02\u3002\u4e3a\u589e\u5f3aSDP\u6a21\u578b\u7684\u9c81\u68d2\u6027\u3001\u6cdb\u5316\u80fd\u529b\u548c\u5b9e\u7528\u6027\uff0c\u6709\u5fc5\u8981\u7cfb\u7edf\u6bd4\u8f83\u8c03\u4f18\u5728\u4e0d\u540cSDP\u573a\u666f\u4e0b\u7684\u5f71\u54cd\u3002", "method": "\u7814\u7a76\u91c7\u7528\u4e24\u79cd\u8c03\u4f18\u7b97\u6cd5\u300128\u79cd\u673a\u5668\u5b66\u4e60\u7b97\u6cd5\u300153\u4e2a\u53d1\u5e03\u540e\u8f6f\u4ef6\u6570\u636e\u96c6\u548c\u4e94\u79cd\u4f18\u5316\u6307\u6807\uff0c\u5728IVDP\u548cCVDP\u4e24\u79cd\u573a\u666f\u4e0b\u8fdb\u884c\u5b9e\u9a8c\uff0c\u5e76\u901a\u8fc7\u7edf\u8ba1\u5206\u6790\u6bd4\u8f83\u8c03\u4f18\u5bf9\u6574\u4f53\u6027\u80fd\u3001\u5355\u4e2a\u7b97\u6cd5\u53ca\u4e0d\u540c\u6570\u636e\u96c6\u89c4\u6a21\u4e0b\u7684\u5f71\u54cd\u5dee\u5f02\u3002", "result": "IVDP\u573a\u666f\u4e2d\u8d85\u53c2\u6570\u8c03\u4f18\u5e26\u6765\u7684\u6027\u80fd\u63d0\u5347\u663e\u8457\u5927\u4e8eCVDP\uff1b28\u79cd\u7b97\u6cd5\u4e2d\u670924\u79cd\u7684\u6027\u80fd\u589e\u76ca\u65e0\u6cd5\u5728\u4e0d\u540cSDP\u573a\u666f\u95f4\u4e00\u81f4\u4fdd\u6301\uff1b\u5c0f\u89c4\u6a21\u8f6f\u4ef6\u6570\u636e\u96c6\u5bf9\u8c03\u4f18\u6548\u679c\u7684\u5dee\u5f02\u66f4\u654f\u611f\u3002", "conclusion": "\u8f6f\u4ef6\u5de5\u7a0b\u7814\u7a76\u4eba\u5458\u548c\u4ece\u4e1a\u8005\u5728\u671f\u671b\u901a\u8fc7\u8d85\u53c2\u6570\u8c03\u4f18\u83b7\u5f97\u6027\u80fd\u63d0\u5347\u65f6\uff0c\u5e94\u5145\u5206\u8003\u8651\u6240\u9009SDP\u573a\u666f\u7684\u5f71\u54cd\u3002"}}
{"id": "2510.16896", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2510.16896", "abs": "https://arxiv.org/abs/2510.16896", "authors": ["Yiming Hu"], "title": "FTI-TMR: A Fault Tolerance and Isolation Algorithm for Interconnected Multicore Systems", "comment": null, "summary": "Two-Phase Triple Modular Redundancy TMR divides redundancy operations into\ntwo stages, omitting part of the computation during fault-free operation to\nreduce energy consumption. However, it becomes ineffective under permanent\nfaults, limiting its reliability in critical systems. To address this,\nReactive-TMR (R-TMR) introduces permanent fault isolation mechanisms for faulty\ncores, tolerating both transient and permanent faults. Yet, its reliance on\nadditional hardware increases system complexity and reduces fault tolerance\nwhen multiple cores or auxiliary modules fail. This paper proposes an\nintegrated fault-tolerant architecture for interconnected multicore systems. By\nconstructing a stability metric to identify reliable machines and performing\nperiodic diagnostics, the method enables permanent fault isolation and adaptive\ntask scheduling without extra hardware. Experimental results show that it\nreduces task workload by approximately 30% compared to baseline TMR and\nachieves superior fault coverage and isolation accuracy, significantly\nimproving both reliability and energy efficiency.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u4e92\u8fde\u591a\u6838\u7cfb\u7edf\u7684\u96c6\u6210\u5bb9\u9519\u67b6\u6784\uff0c\u901a\u8fc7\u6784\u5efa\u7a33\u5b9a\u6027\u5ea6\u91cf\u548c\u5468\u671f\u6027\u8bca\u65ad\uff0c\u5728\u65e0\u9700\u989d\u5916\u786c\u4ef6\u7684\u60c5\u51b5\u4e0b\u5b9e\u73b0\u6c38\u4e45\u6027\u6545\u969c\u9694\u79bb\u4e0e\u81ea\u9002\u5e94\u4efb\u52a1\u8c03\u5ea6\uff0c\u76f8\u6bd4\u4f20\u7edfTMR\u65b9\u6cd5\u964d\u4f4e\u7ea630%\u4efb\u52a1\u8d1f\u8f7d\uff0c\u5e76\u663e\u8457\u63d0\u5347\u53ef\u9760\u6027\u4e0e\u80fd\u6548\u3002", "motivation": "\u4f20\u7edf\u4e24\u9636\u6bb5\u4e09\u6a21\u5197\u4f59\uff08TMR\uff09\u5728\u65e0\u6545\u969c\u65f6\u8282\u80fd\u4f46\u65e0\u6cd5\u5e94\u5bf9\u6c38\u4e45\u6027\u6545\u969c\uff1b\u800cReactive-TMR\u867d\u80fd\u5904\u7406\u6c38\u4e45\u6027\u6545\u969c\uff0c\u5374\u4f9d\u8d56\u989d\u5916\u786c\u4ef6\uff0c\u589e\u52a0\u7cfb\u7edf\u590d\u6742\u6027\u4e14\u5728\u591a\u4e2a\u6838\u5fc3\u6216\u8f85\u52a9\u6a21\u5757\u5931\u6548\u65f6\u5bb9\u9519\u80fd\u529b\u4e0b\u964d\u3002\u56e0\u6b64\uff0c\u4e9f\u9700\u4e00\u79cd\u65e0\u9700\u989d\u5916\u786c\u4ef6\u3001\u517c\u5177\u9ad8\u53ef\u9760\u6027\u548c\u80fd\u6548\u7684\u5bb9\u9519\u67b6\u6784\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u96c6\u6210\u5bb9\u9519\u67b6\u6784\uff0c\u901a\u8fc7\u6784\u5efa\u7a33\u5b9a\u6027\u5ea6\u91cf\u8bc6\u522b\u53ef\u9760\u8ba1\u7b97\u5355\u5143\uff0c\u5e76\u7ed3\u5408\u5468\u671f\u6027\u8bca\u65ad\u673a\u5236\uff0c\u5b9e\u73b0\u6c38\u4e45\u6027\u6545\u969c\u9694\u79bb\u4e0e\u81ea\u9002\u5e94\u4efb\u52a1\u8c03\u5ea6\uff0c\u65e0\u9700\u5f15\u5165\u989d\u5916\u786c\u4ef6\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u76f8\u6bd4\u57fa\u7ebfTMR\u964d\u4f4e\u7ea630%\u7684\u4efb\u52a1\u8d1f\u8f7d\uff0c\u5728\u6545\u969c\u8986\u76d6\u7387\u548c\u9694\u79bb\u51c6\u786e\u7387\u65b9\u9762\u8868\u73b0\u66f4\u4f18\u3002", "conclusion": "\u6240\u63d0\u67b6\u6784\u5728\u4e0d\u589e\u52a0\u786c\u4ef6\u5f00\u9500\u7684\u524d\u63d0\u4e0b\uff0c\u6709\u6548\u63d0\u5347\u4e86\u4e92\u8fde\u591a\u6838\u7cfb\u7edf\u7684\u53ef\u9760\u6027\u4e0e\u80fd\u6548\uff0c\u514b\u670d\u4e86\u73b0\u6709TMR\u65b9\u6848\u5728\u6c38\u4e45\u6027\u6545\u969c\u5904\u7406\u65b9\u9762\u7684\u5c40\u9650\u6027\u3002"}}
{"id": "2510.17004", "categories": ["cs.MA", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.17004", "abs": "https://arxiv.org/abs/2510.17004", "authors": ["Eleftherios Tzanis", "Michail E. Klontzas"], "title": "ReclAIm: A multi-agent framework for degradation-aware performance tuning of medical imaging AI", "comment": "25 pages, 4 figures", "summary": "Ensuring the long-term reliability of AI models in clinical practice requires\ncontinuous performance monitoring and corrective actions when degradation\noccurs. Addressing this need, this manuscript presents ReclAIm, a multi-agent\nframework capable of autonomously monitoring, evaluating, and fine-tuning\nmedical image classification models. The system, built on a large language\nmodel core, operates entirely through natural language interaction, eliminating\nthe need for programming expertise. ReclAIm successfully trains, evaluates, and\nmaintains consistent performance of models across MRI, CT, and X-ray datasets.\nOnce ReclAIm detects significant performance degradation, it autonomously\nexecutes state-of-the-art fine-tuning procedures that substantially reduce the\nperformance gap. In cases with performance drops of up to -41.1% (MRI\nInceptionV3), ReclAIm managed to readjust performance metrics within 1.5% of\nthe initial model results. ReclAIm enables automated, continuous maintenance of\nmedical imaging AI models in a user-friendly and adaptable manner that\nfacilitates broader adoption in both research and clinical environments.", "AI": {"tldr": "ReclAIm \u662f\u4e00\u4e2a\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u591a\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u80fd\u901a\u8fc7\u81ea\u7136\u8bed\u8a00\u4ea4\u4e92\u81ea\u52a8\u76d1\u63a7\u3001\u8bc4\u4f30\u5e76\u5fae\u8c03\u533b\u5b66\u5f71\u50cf\u5206\u7c7b\u6a21\u578b\uff0c\u5728\u65e0\u9700\u7f16\u7a0b\u6280\u80fd\u7684\u60c5\u51b5\u4e0b\u5b9e\u73b0\u6a21\u578b\u6027\u80fd\u7684\u6301\u7eed\u7ef4\u62a4\u3002", "motivation": "\u5728\u4e34\u5e8a\u5b9e\u8df5\u4e2d\uff0cAI\u6a21\u578b\u7684\u957f\u671f\u53ef\u9760\u6027\u4f9d\u8d56\u4e8e\u6301\u7eed\u7684\u6027\u80fd\u76d1\u63a7\u548c\u6027\u80fd\u4e0b\u964d\u65f6\u7684\u53ca\u65f6\u4fee\u6b63\uff0c\u800c\u73b0\u6709\u65b9\u6cd5\u901a\u5e38\u9700\u8981\u4e13\u4e1a\u7f16\u7a0b\u6280\u80fd\uff0c\u9650\u5236\u4e86\u5176\u5e7f\u6cdb\u5e94\u7528\u3002", "method": "ReclAIm \u5229\u7528\u591a\u667a\u80fd\u4f53\u67b6\u6784\u548c\u5927\u8bed\u8a00\u6a21\u578b\u6838\u5fc3\uff0c\u901a\u8fc7\u81ea\u7136\u8bed\u8a00\u4ea4\u4e92\u5b9e\u73b0\u5bf9\u533b\u5b66\u56fe\u50cf\u5206\u7c7b\u6a21\u578b\u7684\u81ea\u52a8\u76d1\u63a7\u3001\u8bc4\u4f30\u548c\u5fae\u8c03\uff0c\u652f\u6301 MRI\u3001CT \u548c X \u5149\u7b49\u591a\u79cd\u5f71\u50cf\u6570\u636e\u3002", "result": "\u5728\u9762\u5bf9\u9ad8\u8fbe -41.1% \u7684\u6027\u80fd\u4e0b\u964d\uff08\u5982 MRI InceptionV3 \u6a21\u578b\uff09\u65f6\uff0cReclAIm \u80fd\u81ea\u52a8\u6267\u884c\u5148\u8fdb\u5fae\u8c03\u7b56\u7565\uff0c\u5c06\u6027\u80fd\u6062\u590d\u81f3\u521d\u59cb\u6c34\u5e73\u7684 1.5% \u4ee5\u5185\u3002", "conclusion": "ReclAIm \u63d0\u4f9b\u4e86\u4e00\u79cd\u7528\u6237\u53cb\u597d\u3001\u65e0\u9700\u7f16\u7a0b\u7684\u81ea\u52a8\u5316\u65b9\u6848\uff0c\u53ef\u6709\u6548\u7ef4\u6301\u533b\u5b66\u5f71\u50cf AI \u6a21\u578b\u7684\u957f\u671f\u6027\u80fd\uff0c\u6709\u52a9\u4e8e\u5176\u5728\u79d1\u7814\u548c\u4e34\u5e8a\u73af\u5883\u4e2d\u7684\u5e7f\u6cdb\u90e8\u7f72\u3002"}}
{"id": "2510.16779", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.16779", "abs": "https://arxiv.org/abs/2510.16779", "authors": ["Xiaoyu Guo", "Minggu Wang", "Jianjun Zhao"], "title": "QuanBench: Benchmarking Quantum Code Generation with Large Language Models", "comment": "This paper was accepted by ASE2025", "summary": "Large language models (LLMs) have demonstrated good performance in general\ncode generation; however, their capabilities in quantum code generation remain\ninsufficiently studied. This paper presents QuanBench, a benchmark for\nevaluating LLMs on quantum code generation. QuanBench includes 44 programming\ntasks that cover quantum algorithms, state preparation, gate decomposition, and\nquantum machine learning. Each task has an executable canonical solution and is\nevaluated by functional correctness (Pass@K) and quantum semantic equivalence\n(Process Fidelity). We evaluate several recent LLMs, including general-purpose\nand code-specialized models. The results show that current LLMs have limited\ncapability in generating the correct quantum code, with overall accuracy below\n40% and frequent semantic errors. We also analyze common failure cases, such as\noutdated API usage, circuit construction errors, and incorrect algorithm logic.\nQuanBench provides a basis for future work on improving quantum code generation\nwith LLMs.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86QuanBench\uff0c\u4e00\u4e2a\u7528\u4e8e\u8bc4\u4f30\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u91cf\u5b50\u4ee3\u7801\u751f\u6210\u80fd\u529b\u4e0a\u7684\u57fa\u51c6\uff0c\u5305\u542b44\u4e2a\u6db5\u76d6\u91cf\u5b50\u7b97\u6cd5\u3001\u6001\u5236\u5907\u3001\u95e8\u5206\u89e3\u548c\u91cf\u5b50\u673a\u5668\u5b66\u4e60\u7684\u7f16\u7a0b\u4efb\u52a1\uff0c\u5e76\u901a\u8fc7\u529f\u80fd\u6b63\u786e\u6027\u548c\u91cf\u5b50\u8bed\u4e49\u7b49\u4ef7\u6027\u8fdb\u884c\u8bc4\u4f30\u3002\u5b9e\u9a8c\u8868\u660e\u5f53\u524dLLMs\u5728\u8be5\u4efb\u52a1\u4e0a\u8868\u73b0\u6709\u9650\uff0c\u51c6\u786e\u7387\u4f4e\u4e8e40%\uff0c\u5e76\u5b58\u5728\u591a\u79cd\u5e38\u89c1\u9519\u8bef\u3002", "motivation": "\u5c3d\u7ba1\u5927\u8bed\u8a00\u6a21\u578b\u5728\u901a\u7528\u4ee3\u7801\u751f\u6210\u65b9\u9762\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5176\u5728\u91cf\u5b50\u4ee3\u7801\u751f\u6210\u65b9\u9762\u7684\u80fd\u529b\u5c1a\u672a\u5f97\u5230\u5145\u5206\u7814\u7a76\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u4e2a\u4e13\u95e8\u7684\u57fa\u51c6\u6765\u8bc4\u4f30\u548c\u63a8\u52a8\u8be5\u9886\u57df\u7684\u53d1\u5c55\u3002", "method": "\u6784\u5efaQuanBench\u57fa\u51c6\uff0c\u5305\u542b44\u4e2a\u5177\u6709\u53ef\u6267\u884c\u6807\u51c6\u89e3\u7684\u91cf\u5b50\u7f16\u7a0b\u4efb\u52a1\uff0c\u91c7\u7528\u529f\u80fd\u6b63\u786e\u6027\uff08Pass@K\uff09\u548c\u91cf\u5b50\u8bed\u4e49\u7b49\u4ef7\u6027\uff08\u8fc7\u7a0b\u4fdd\u771f\u5ea6\uff09\u4f5c\u4e3a\u8bc4\u4f30\u6307\u6807\uff0c\u5e76\u5bf9\u591a\u4e2a\u901a\u7528\u548c\u4ee3\u7801\u4e13\u7528\u7684\u5927\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u6d4b\u8bd5\u3002", "result": "\u5f53\u524d\u5927\u8bed\u8a00\u6a21\u578b\u5728\u91cf\u5b50\u4ee3\u7801\u751f\u6210\u4efb\u52a1\u4e0a\u7684\u6574\u4f53\u51c6\u786e\u7387\u4f4e\u4e8e40%\uff0c\u5e38\u51fa\u73b0API\u8fc7\u65f6\u3001\u7535\u8def\u6784\u5efa\u9519\u8bef\u548c\u7b97\u6cd5\u903b\u8f91\u9519\u8bef\u7b49\u8bed\u4e49\u95ee\u9898\u3002", "conclusion": "QuanBench\u4e3a\u672a\u6765\u63d0\u5347\u5927\u8bed\u8a00\u6a21\u578b\u5728\u91cf\u5b50\u4ee3\u7801\u751f\u6210\u65b9\u9762\u7684\u80fd\u529b\u63d0\u4f9b\u4e86\u8bc4\u4f30\u57fa\u7840\u548c\u7814\u7a76\u65b9\u5411\u3002"}}
{"id": "2510.16933", "categories": ["cs.DC", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.16933", "abs": "https://arxiv.org/abs/2510.16933", "authors": ["Maty\u00e1\u0161 Brabec", "Ji\u0159\u00ed Klepl", "Michal T\u00f6pfer", "Martin Kruli\u0161"], "title": "Tutoring LLM into a Better CUDA Optimizer", "comment": "This preprint has not undergone peer review or any post-submission\n  improvements or corrections. The Version of Record of this contribution is\n  published in Euro-Par 2025: Parallel Processing, Part II, and is available\n  online at https://doi.org/10.1007/978-3-031-99857-7_18", "summary": "Recent leaps in large language models (LLMs) caused a revolution in\nprogramming tools (like GitHub Copilot) that can help with code generation,\ndebugging, and even performance optimization. In this paper, we focus on the\ncapabilities of the most recent reasoning models to generate optimized CUDA\ncode for predefined, well-known tasks. Our objective is to determine which\ntypes of code optimizations and parallel patterns the LLMs can perform by\nthemselves and whether they can be improved by tutoring (providing more\ndetailed hints and guidelines in the prompt). The generated solutions were\nevaluated both automatically (for correctness and speedup) and manually (code\nreviews) to provide a more detailed perspective. We also tried an interactive\napproach where the LLM can fix its previous mistakes within a session. The\nresults indicate that LLMs are quite skilled coders; however, they require\ntutoring to reach optimized solutions provided by parallel computing experts.", "AI": {"tldr": "\u672c\u6587\u8bc4\u4f30\u4e86\u6700\u65b0\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u751f\u6210\u4f18\u5316CUDA\u4ee3\u7801\u65b9\u9762\u7684\u80fd\u529b\uff0c\u53d1\u73b0\u5176\u867d\u5177\u5907\u8f83\u5f3a\u7f16\u7801\u80fd\u529b\uff0c\u4f46\u9700\u901a\u8fc7\u63d0\u793a\u4e2d\u7684\u8be6\u7ec6\u6307\u5bfc\uff08\u201c\u8f85\u5bfc\u201d\uff09\u624d\u80fd\u8fbe\u5230\u4e13\u5bb6\u7ea7\u4f18\u5316\u6c34\u5e73\u3002", "motivation": "\u63a2\u7d22\u5f53\u524d\u5927\u8bed\u8a00\u6a21\u578b\u5728\u751f\u6210\u9488\u5bf9\u5df2\u77e5\u4efb\u52a1\u7684\u4f18\u5316CUDA\u4ee3\u7801\u65b9\u9762\u7684\u6f5c\u529b\uff0c\u7279\u522b\u662f\u5176\u81ea\u4e3b\u5b9e\u73b0\u4ee3\u7801\u4f18\u5316\u4e0e\u5e76\u884c\u6a21\u5f0f\u7684\u80fd\u529b\uff0c\u4ee5\u53ca\u901a\u8fc7\u63d0\u793a\u4e2d\u7684\u8f85\u5bfc\u662f\u5426\u80fd\u8fdb\u4e00\u6b65\u63d0\u5347\u6027\u80fd\u3002", "method": "\u901a\u8fc7\u81ea\u52a8\u8bc4\u4f30\uff08\u6b63\u786e\u6027\u4e0e\u52a0\u901f\u6bd4\uff09\u548c\u4eba\u5de5\u4ee3\u7801\u5ba1\u67e5\uff0c\u6d4b\u8bd5LLM\u5728\u6709\u65e0\u8f85\u5bfc\u63d0\u793a\u4e0b\u751f\u6210\u7684CUDA\u4ee3\u7801\u8d28\u91cf\uff0c\u5e76\u5c1d\u8bd5\u4ea4\u4e92\u5f0f\u65b9\u6cd5\u8ba9\u6a21\u578b\u5728\u4f1a\u8bdd\u4e2d\u4fee\u6b63\u9519\u8bef\u3002", "result": "LLM\u5c55\u73b0\u51fa\u8f83\u5f3a\u7684\u7f16\u7801\u80fd\u529b\uff0c\u4f46\u5728\u65e0\u8f85\u5bfc\u60c5\u51b5\u4e0b\u96be\u4ee5\u8fbe\u5230\u4e13\u5bb6\u7ea7\u4f18\u5316\u6548\u679c\uff1b\u901a\u8fc7\u63d0\u4f9b\u66f4\u8be6\u7ec6\u7684\u63d0\u793a\u53ef\u663e\u8457\u63d0\u5347\u5176\u751f\u6210\u4ee3\u7801\u7684\u4f18\u5316\u6c34\u5e73\u3002", "conclusion": "\u5927\u8bed\u8a00\u6a21\u578b\u5728CUDA\u4ee3\u7801\u751f\u6210\u65b9\u9762\u5177\u6709\u6f5c\u529b\uff0c\u4f46\u8981\u5b9e\u73b0\u4e13\u5bb6\u7ea7\u6027\u80fd\u4f18\u5316\uff0c\u4ecd\u9700\u4f9d\u8d56\u63d0\u793a\u4e2d\u7684\u660e\u786e\u6307\u5bfc\u6216\u4ea4\u4e92\u5f0f\u4fee\u6b63\u673a\u5236\u3002"}}
{"id": "2510.17401", "categories": ["cs.MA", "I.2.11"], "pdf": "https://arxiv.org/pdf/2510.17401", "abs": "https://arxiv.org/abs/2510.17401", "authors": ["David Aguilera-Luzon", "Dave de Jonge", "Javier Larrosa"], "title": "MiCRO for Multilateral Negotiations", "comment": "Extended version of short-paper presented at PRIMA2025", "summary": "Recently, a very simple new bilateral negotiation strategy called MiCRO was\nintroduced that does not make use of any kind of opponent modeling or machine\nlearning techniques and that does not require fine-tuning of any parameters.\nDespite its simplicity, it was shown that MiCRO performs similar to -- or even\nbetter than -- most state-of-the-art negotiation strategies. This lead its\nauthors to argue that the benchmark domains on which negotiation algorithms are\ntypically tested may be too simplistic. However, one question that was left\nopen, was how MiCRO could be generalized to multilateral negotiations. In this\npaper we fill this gap by introducing a multilateral variant of MiCRO. We\ncompare it with the winners of the Automated Negotiating Agents Competitions\n(ANAC) of 2015, 2017 and 2018 and show that it outperforms them. Furthermore,\nwe perform an empirical game-theoretical analysis to show that our new version\nof MiCRO forms an empirical Nash equilibrium.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86MiCRO\u7b56\u7565\u7684\u591a\u8fb9\u8c08\u5224\u53d8\u4f53\uff0c\u5e76\u5728\u4e0eANAC\u7ade\u8d5b\u4f18\u80dc\u7b56\u7565\u7684\u5bf9\u6bd4\u4e2d\u8868\u73b0\u66f4\u4f18\uff0c\u4e14\u6784\u6210\u7ecf\u9a8c\u7eb3\u4ec0\u5747\u8861\u3002", "motivation": "MiCRO\u7b56\u7565\u5728\u53cc\u8fb9\u8c08\u5224\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5176\u5982\u4f55\u63a8\u5e7f\u5230\u591a\u8fb9\u8c08\u5224\u5c1a\u4e0d\u660e\u786e\uff1b\u4f5c\u8005\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u63d0\u51faMiCRO\u7684\u591a\u8fb9\u53d8\u4f53\uff0c\u5e76\u4e0eANAC 2015\u30012017\u548c2018\u5e74\u7684\u4f18\u80dc\u7b56\u7565\u8fdb\u884c\u6bd4\u8f83\uff0c\u540c\u65f6\u8fdb\u884c\u7ecf\u9a8c\u535a\u5f08\u8bba\u5206\u6790\u3002", "result": "\u65b0\u63d0\u51fa\u7684\u591a\u8fb9MiCRO\u7b56\u7565\u4f18\u4e8e\u4ee5\u5f80ANAC\u7ade\u8d5b\u4e2d\u7684\u4f18\u80dc\u7b56\u7565\uff0c\u5e76\u6784\u6210\u7ecf\u9a8c\u7eb3\u4ec0\u5747\u8861\u3002", "conclusion": "MiCRO\u7b56\u7565\u53ef\u6210\u529f\u63a8\u5e7f\u81f3\u591a\u8fb9\u8c08\u5224\u573a\u666f\uff0c\u4e14\u5728\u6027\u80fd\u548c\u535a\u5f08\u7a33\u5b9a\u6027\u65b9\u9762\u8868\u73b0\u4f18\u5f02\u3002"}}
{"id": "2510.16786", "categories": ["cs.SE", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.16786", "abs": "https://arxiv.org/abs/2510.16786", "authors": ["Pengfei Gao", "Chao Peng"], "title": "More with Less: An Empirical Study of Turn-Control Strategies for Efficient Coding Agents", "comment": null, "summary": "LLM-powered coding agents, which operate in iterative loops (turns) to solve\nsoftware engineering tasks, are becoming increasingly powerful. However, their\npractical deployment is hindered by significant and unpredictable costs. This\nchallenge arises from a combination of factors: quadratically growing token\ncounts with each turn, the high price of models, the large number of turns\nrequired for real-world tasks, and the tendency of agents to take inefficient\nor unnecessary actions. While existing research focuses on optimizing\nindividual turns, the strategic control of the total number of turns remains an\nunderexplored area for managing agent performance and cost. To address this\ngap, we conduct a comprehensive empirical study on SWE-bench using three\nstate-of-the-art models and evaluate the impact of three distinct turn-control\nstrategies: an unrestricted baseline, a fixed-turn limit with reminders, and a\nnovel dynamic-turn strategy that grants extensions on-demand. Our findings\nfirst reveal a fundamental trade-off in the unrestricted setting, where no\nsingle model excels across performance, cost, and turn efficiency. We then show\nthat a fixed-turn limit, specifically at the 75th percentile of the baseline,\nserves as a \"sweet spot\", substantially reducing costs (by 24%-68%) with\nminimal impact on solve rates. Most significantly, the dynamic-turn strategy\nconsistently outperforms fixed-limit approaches, achieving comparable or better\nsolve rates while further reducing costs by an additional 12%-24% by\nintelligently allocating resources only to tasks that need them. This work\nprovides the first systematic analysis of turn-control strategies, offering\nsimple yet effective guidelines for developers to balance cost and efficacy. We\ndemonstrate that dynamic resource allocation is a superior, easy-to-implement\napproach for deploying powerful yet economically viable coding agents.", "AI": {"tldr": "\u672c\u6587\u7cfb\u7edf\u7814\u7a76\u4e86\u5927\u8bed\u8a00\u6a21\u578b\u9a71\u52a8\u7684\u7f16\u7801\u667a\u80fd\u4f53\u5728\u6267\u884c\u8f6f\u4ef6\u5de5\u7a0b\u4efb\u52a1\u65f6\u7684\u8f6e\u6b21\u63a7\u5236\u7b56\u7565\uff0c\u53d1\u73b0\u52a8\u6001\u8f6e\u6b21\u5206\u914d\u7b56\u7565\u5728\u4fdd\u6301\u751a\u81f3\u63d0\u5347\u4efb\u52a1\u89e3\u51b3\u7387\u7684\u540c\u65f6\u663e\u8457\u964d\u4f4e\u6210\u672c\uff0c\u4f18\u4e8e\u56fa\u5b9a\u8f6e\u6b21\u9650\u5236\u548c\u65e0\u9650\u5236\u57fa\u7ebf\u3002", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b\u9a71\u52a8\u7684\u7f16\u7801\u667a\u80fd\u4f53\u5728\u5b9e\u9645\u90e8\u7f72\u4e2d\u9762\u4e34\u6210\u672c\u9ad8\u4e14\u4e0d\u53ef\u9884\u6d4b\u7684\u95ee\u9898\uff0c\u4e3b\u8981\u6e90\u4e8e\u6bcf\u8f6e\u8fed\u4ee3\u4e2dtoken\u6570\u91cf\u7684\u4e8c\u6b21\u589e\u957f\u3001\u6a21\u578b\u4ef7\u683c\u9ad8\u6602\u3001\u5b8c\u6210\u771f\u5b9e\u4efb\u52a1\u6240\u9700\u8f6e\u6b21\u591a\u4ee5\u53ca\u667a\u80fd\u4f53\u5e38\u6267\u884c\u4f4e\u6548\u6216\u4e0d\u5fc5\u8981\u64cd\u4f5c\u3002\u73b0\u6709\u7814\u7a76\u591a\u805a\u7126\u4e8e\u5355\u8f6e\u4f18\u5316\uff0c\u800c\u5bf9\u6574\u4f53\u8f6e\u6b21\u7684\u6218\u7565\u63a7\u5236\u7f3a\u4e4f\u63a2\u7d22\u3002", "method": "\u5728SWE-bench\u57fa\u51c6\u4e0a\uff0c\u4f7f\u7528\u4e09\u4e2a\u6700\u5148\u8fdb\u7684\u6a21\u578b\uff0c\u8bc4\u4f30\u4e09\u79cd\u8f6e\u6b21\u63a7\u5236\u7b56\u7565\uff1a\u65e0\u9650\u5236\u57fa\u7ebf\u3001\u5e26\u63d0\u9192\u7684\u56fa\u5b9a\u8f6e\u6b21\u9650\u5236\uff08\u8bbe\u4e3a\u57fa\u7ebf\u7b2c75\u767e\u5206\u4f4d\uff09\u3001\u4ee5\u53ca\u4e00\u79cd\u65b0\u9896\u7684\u6309\u9700\u52a8\u6001\u6269\u5c55\u8f6e\u6b21\u7684\u7b56\u7565\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u65e0\u9650\u5236\u8bbe\u7f6e\u4e0b\u6a21\u578b\u5728\u6027\u80fd\u3001\u6210\u672c\u548c\u8f6e\u6b21\u6548\u7387\u4e4b\u95f4\u5b58\u5728\u6839\u672c\u6743\u8861\uff1b\u56fa\u5b9a\u8f6e\u6b21\u9650\u5236\uff0875\u767e\u5206\u4f4d\uff09\u53ef\u5927\u5e45\u964d\u4f4e\u6210\u672c\uff0824%-68%\uff09\u4e14\u5bf9\u89e3\u51b3\u7387\u5f71\u54cd\u5f88\u5c0f\uff1b\u52a8\u6001\u8f6e\u6b21\u7b56\u7565\u8868\u73b0\u6700\u4f18\uff0c\u5728\u4fdd\u6301\u6216\u63d0\u5347\u89e3\u51b3\u7387\u7684\u540c\u65f6\uff0c\u6bd4\u56fa\u5b9a\u9650\u5236\u8fdb\u4e00\u6b65\u8282\u770112%-24%\u7684\u6210\u672c\u3002", "conclusion": "\u52a8\u6001\u8d44\u6e90\u5206\u914d\u662f\u4e00\u79cd\u4f18\u8d8a\u4e14\u6613\u4e8e\u5b9e\u73b0\u7684\u65b9\u6cd5\uff0c\u80fd\u6709\u6548\u5e73\u8861\u7f16\u7801\u667a\u80fd\u4f53\u7684\u6027\u80fd\u4e0e\u6210\u672c\uff0c\u4e3a\u5f00\u53d1\u8005\u63d0\u4f9b\u4e86\u7b80\u5355\u800c\u6709\u6548\u7684\u90e8\u7f72\u6307\u5bfc\u3002"}}
{"id": "2510.16946", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2510.16946", "abs": "https://arxiv.org/abs/2510.16946", "authors": ["Erfan Darzi", "Aldo Pareja", "Shreeanant Bharadwaj"], "title": "Host-Side Telemetry for Performance Diagnosis in Cloud and HPC GPU Infrastructure", "comment": null, "summary": "Diagnosing GPU tail latency spikes in cloud and HPC infrastructure is\ncritical for maintaining performance predictability and resource utilization,\nyet existing monitoring tools lack the granularity for root cause analysis in\nshared computing environments. We introduce an eBPF-based telemetry system that\nprovides unified host-side monitoring of GPU workloads, correlating\neBPF-derived host metrics with GPU-internal events for holistic system\nobservability. The system achieves 81--88\\% diagnostic accuracy, detects spikes\nwithin 5 seconds, and completes root cause analysis in 6--8 seconds, operating\nwith 1.21\\% CPU overhead at 100Hz sampling. Evaluated on distributed learning\nworkloads, the system identifies root causes including NIC contention, PCIe\npressure, and CPU interference, enabling operational debugging for multi-tenant\nGPU infrastructure without requiring cluster-wide instrumentation.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eeBPF\u7684\u9065\u6d4b\u7cfb\u7edf\uff0c\u7528\u4e8e\u8bca\u65ad\u4e91\u548c\u9ad8\u6027\u80fd\u8ba1\u7b97\u73af\u5883\u4e2dGPU\u5c3e\u90e8\u5ef6\u8fdf\u5c16\u5cf0\u95ee\u9898\uff0c\u901a\u8fc7\u5173\u8054\u4e3b\u673a\u6307\u6807\u4e0eGPU\u5185\u90e8\u4e8b\u4ef6\u5b9e\u73b0\u9ad8\u7cbe\u5ea6\u3001\u4f4e\u5f00\u9500\u7684\u6839\u56e0\u5206\u6790\u3002", "motivation": "\u73b0\u6709\u76d1\u63a7\u5de5\u5177\u5728\u5171\u4eab\u8ba1\u7b97\u73af\u5883\u4e2d\u7f3a\u4e4f\u7ec6\u7c92\u5ea6\u7684\u6839\u56e0\u5206\u6790\u80fd\u529b\uff0c\u96be\u4ee5\u6709\u6548\u8bca\u65adGPU\u5c3e\u90e8\u5ef6\u8fdf\u5c16\u5cf0\uff0c\u5f71\u54cd\u6027\u80fd\u53ef\u9884\u6d4b\u6027\u548c\u8d44\u6e90\u5229\u7528\u7387\u3002", "method": "\u8bbe\u8ba1\u5e76\u5b9e\u73b0\u4e86\u4e00\u4e2a\u57fa\u4e8eeBPF\u7684\u9065\u6d4b\u7cfb\u7edf\uff0c\u7edf\u4e00\u76d1\u63a7GPU\u5de5\u4f5c\u8d1f\u8f7d\uff0c\u5c06eBPF\u83b7\u53d6\u7684\u4e3b\u673a\u6307\u6807\u4e0eGPU\u5185\u90e8\u4e8b\u4ef6\u8fdb\u884c\u5173\u8054\uff0c\u4ee5\u5b9e\u73b0\u7cfb\u7edf\u7ea7\u7684\u6574\u4f53\u53ef\u89c2\u6d4b\u6027\u3002", "result": "\u8be5\u7cfb\u7edf\u5728\u5206\u5e03\u5f0f\u5b66\u4e60\u5de5\u4f5c\u8d1f\u8f7d\u4e2d\u5b9e\u73b0\u4e8681\u201388%\u7684\u8bca\u65ad\u51c6\u786e\u7387\uff0c\u53ef\u57285\u79d2\u5185\u68c0\u6d4b\u5230\u5ef6\u8fdf\u5c16\u5cf0\uff0c\u5e76\u57286\u20138\u79d2\u5185\u5b8c\u6210\u6839\u56e0\u5206\u6790\uff0cCPU\u5f00\u9500\u4ec5\u4e3a1.21%\uff08100Hz\u91c7\u6837\uff09\uff0c\u6210\u529f\u8bc6\u522b\u51faNIC\u4e89\u7528\u3001PCIe\u538b\u529b\u548cCPU\u5e72\u6270\u7b49\u6839\u56e0\u3002", "conclusion": "\u8be5eBPF\u9065\u6d4b\u7cfb\u7edf\u80fd\u591f\u5728\u65e0\u9700\u96c6\u7fa4\u8303\u56f4\u63d2\u6869\u7684\u60c5\u51b5\u4e0b\uff0c\u6709\u6548\u652f\u6301\u591a\u79df\u6237GPU\u57fa\u7840\u8bbe\u65bd\u7684\u8fd0\u884c\u65f6\u8c03\u8bd5\uff0c\u663e\u8457\u63d0\u5347GPU\u5c3e\u90e8\u5ef6\u8fdf\u95ee\u9898\u7684\u8bca\u65ad\u6548\u7387\u4e0e\u7cbe\u5ea6\u3002"}}
{"id": "2510.15907", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2510.15907", "abs": "https://arxiv.org/abs/2510.15907", "authors": ["Era Thaqi", "Dennis Eigner", "Arman Ferdowsi", "Ulrich Schmid"], "title": "Symbolic Timing Analysis of Digital Circuits Using Analytic Delay Functions", "comment": null, "summary": "We propose a novel approach to symbolic timing analysis for digital\nintegrated circuits based on recently developed analytic delay formulas for\n2-input NOR, NAND, and Muller-C gates by Ferdowsi et al. (NAHS 2025). Given a\nfixed order of the transitions of all input and internal signals of a circuit,\nour framework computes closed-form analytic delay expressions for all the\ninternal signal transition times that depend on (i) the symbolic transition\ntimes of the relevant input signals and (ii) the model parameters of the\nrelevant gates. The resulting formulas facilitate per-transition timing\nanalysis without any simulation, by instantiating the symbolic input transition\ntimes and the gate parameters. More importantly, however, they also enable an\n\\emph{analytic} study of the dependencies of certain timing properties on input\nsignals and gate parameters. For instance, differentiating a symbolic delay\nexpression with respect to a gate parameter or input transition time enables\nsensitivity analysis. As a proof of concept, we implement our approach using\nthe computer algebra system SageMath and apply it to the NOR-gate version of\nthe c17 slack benchmark circuit.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u89e3\u6790\u5ef6\u8fdf\u516c\u5f0f\u7684\u7b26\u53f7\u65f6\u5e8f\u5206\u6790\u65b0\u65b9\u6cd5\uff0c\u9002\u7528\u4e8e\u6570\u5b57\u96c6\u6210\u7535\u8def\uff0c\u65e0\u9700\u4eff\u771f\u5373\u53ef\u8fdb\u884c\u9010\u8df3\u53d8\u65f6\u5e8f\u5206\u6790\uff0c\u5e76\u652f\u6301\u5bf9\u8f93\u5165\u4fe1\u53f7\u548c\u95e8\u53c2\u6570\u7684\u89e3\u6790\u654f\u611f\u6027\u7814\u7a76\u3002", "motivation": "\u4f20\u7edf\u65f6\u5e8f\u5206\u6790\u4f9d\u8d56\u4eff\u771f\uff0c\u96be\u4ee5\u9ad8\u6548\u7814\u7a76\u65f6\u5e8f\u7279\u6027\u5bf9\u8f93\u5165\u4fe1\u53f7\u548c\u95e8\u53c2\u6570\u7684\u4f9d\u8d56\u5173\u7cfb\uff1b\u4f5c\u8005\u65e8\u5728\u901a\u8fc7\u89e3\u6790\u8868\u8fbe\u5f0f\u5b9e\u73b0\u65e0\u9700\u4eff\u771f\u7684\u7b26\u53f7\u65f6\u5e8f\u5206\u6790\u3002", "method": "\u5229\u7528Ferdowsi\u7b49\u4eba\u63d0\u51fa\u76842\u8f93\u5165NOR\u3001NAND\u548cMuller-C\u95e8\u7684\u89e3\u6790\u5ef6\u8fdf\u516c\u5f0f\uff0c\u5728\u7ed9\u5b9a\u6240\u6709\u8f93\u5165\u548c\u5185\u90e8\u4fe1\u53f7\u8df3\u53d8\u987a\u5e8f\u7684\u524d\u63d0\u4e0b\uff0c\u63a8\u5bfc\u51fa\u5185\u90e8\u4fe1\u53f7\u8df3\u53d8\u65f6\u95f4\u7684\u95ed\u5f0f\u89e3\u6790\u8868\u8fbe\u5f0f\uff0c\u8be5\u8868\u8fbe\u5f0f\u4f9d\u8d56\u4e8e\u8f93\u5165\u4fe1\u53f7\u7684\u7b26\u53f7\u8df3\u53d8\u65f6\u95f4\u548c\u95e8\u6a21\u578b\u53c2\u6570\uff0c\u5e76\u901a\u8fc7SageMath\u5b9e\u73b0\u3002", "result": "\u5728c17 slack\u57fa\u51c6\u7535\u8def\u7684NOR\u95e8\u7248\u672c\u4e0a\u6210\u529f\u5e94\u7528\u8be5\u65b9\u6cd5\uff0c\u9a8c\u8bc1\u4e86\u5176\u53ef\u884c\u6027\uff1b\u6240\u5f97\u516c\u5f0f\u652f\u6301\u76f4\u63a5\u5b9e\u4f8b\u5316\u8fdb\u884c\u65f6\u5e8f\u5206\u6790\uff0c\u5e76\u53ef\u901a\u8fc7\u6c42\u5bfc\u5b9e\u73b0\u654f\u611f\u6027\u5206\u6790\u3002", "conclusion": "\u6240\u63d0\u65b9\u6cd5\u4e3a\u6570\u5b57\u7535\u8def\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u7684\u7b26\u53f7\u65f6\u5e8f\u5206\u6790\u624b\u6bb5\uff0c\u4e0d\u4ec5\u907f\u514d\u4e86\u4eff\u771f\u5f00\u9500\uff0c\u8fd8\u652f\u6301\u5bf9\u65f6\u5e8f\u7279\u6027\u7684\u89e3\u6790\u7814\u7a76\uff0c\u5177\u6709\u826f\u597d\u7684\u7406\u8bba\u548c\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2510.16809", "categories": ["cs.SE", "cs.AI", "cs.CL", "cs.PL", "68T50, 68N30, 68W40", "I.2.7; D.2.7; I.2.6"], "pdf": "https://arxiv.org/pdf/2510.16809", "abs": "https://arxiv.org/abs/2510.16809", "authors": ["Amirkia Rafiei Oskooei", "Kaan Baturalp Cosdan", "Husamettin Isiktas", "Mehmet S. Aktas"], "title": "When Many-Shot Prompting Fails: An Empirical Study of LLM Code Translation", "comment": null, "summary": "Large Language Models (LLMs) with vast context windows offer new avenues for\nin-context learning (ICL), where providing many examples (\"many-shot\"\nprompting) is often assumed to enhance performance. We investigate this\nassumption for the complex task of code translation. Through a large-scale\nempirical study of over 90,000 translations, we systematically evaluate the\nimpact of scaling in-context examples from zero-shot to many-shot\nconfigurations of up to 625 examples, with prompts spanning from approximately\n100,000 to 800,000 tokens. Our findings reveal a \"many-shot paradox\": while\nstatic similarity metrics may modestly improve with more examples, functional\ncorrectness consistently peaks with few-shot prompting (5-25 examples).\nProviding substantially more examples often degrades this crucial functional\nperformance. This study highlights that for code translation, the quality of a\nfew well-chosen examples outweighs sheer quantity, challenging the universal\nefficacy of \"more is better\" for ICL and underscoring the task-dependent nature\nof optimal prompting strategies. Our results have significant implications for\neffectively leveraging LLMs in software engineering.", "AI": {"tldr": "\u672c\u6587\u901a\u8fc7\u5927\u89c4\u6a21\u5b9e\u9a8c\u53d1\u73b0\uff0c\u5728\u4ee3\u7801\u7ffb\u8bd1\u4efb\u52a1\u4e2d\uff0c\u4f7f\u7528\u5c11\u91cf\u7cbe\u5fc3\u6311\u9009\u7684\u793a\u4f8b\uff085-25\u4e2a\uff09\u6548\u679c\u6700\u4f73\uff0c\u800c\u589e\u52a0\u793a\u4f8b\u6570\u91cf\u53cd\u800c\u4f1a\u964d\u4f4e\u529f\u80fd\u6b63\u786e\u6027\uff0c\u63ed\u793a\u4e86\u201c\u591a\u793a\u4f8b\u6096\u8bba\u201d\u3002", "motivation": "\u63a2\u7a76\u5728\u4e0a\u4e0b\u6587\u5b66\u4e60\uff08ICL\uff09\u4e2d\uff0c\u589e\u52a0\u793a\u4f8b\u6570\u91cf\u662f\u5426\u603b\u80fd\u63d0\u5347\u5927\u8bed\u8a00\u6a21\u578b\u5728\u4ee3\u7801\u7ffb\u8bd1\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\u3002", "method": "\u5bf9\u8d85\u8fc790,000\u6b21\u4ee3\u7801\u7ffb\u8bd1\u8fdb\u884c\u5927\u89c4\u6a21\u5b9e\u8bc1\u7814\u7a76\uff0c\u7cfb\u7edf\u8bc4\u4f30\u4ece\u96f6\u6837\u672c\u5230\u591a\u8fbe625\u4e2a\u793a\u4f8b\uff08\u63d0\u793a\u957f\u5ea6\u8fbe80\u4e07token\uff09\u7684\u6027\u80fd\u53d8\u5316\u3002", "result": "\u529f\u80fd\u6b63\u786e\u6027\u57285-25\u4e2a\u793a\u4f8b\u65f6\u8fbe\u5230\u5cf0\u503c\uff0c\u66f4\u591a\u793a\u4f8b\u53cd\u800c\u5bfc\u81f4\u6027\u80fd\u4e0b\u964d\uff1b\u9759\u6001\u76f8\u4f3c\u6027\u6307\u6807\u867d\u7565\u6709\u63d0\u5347\uff0c\u4f46\u65e0\u6cd5\u53cd\u6620\u5b9e\u9645\u529f\u80fd\u8868\u73b0\u3002", "conclusion": "\u5728\u4ee3\u7801\u7ffb\u8bd1\u4efb\u52a1\u4e2d\uff0c\u793a\u4f8b\u8d28\u91cf\u8fdc\u80dc\u4e8e\u6570\u91cf\uff0c\u201c\u8d8a\u591a\u8d8a\u597d\u201d\u7684\u5047\u8bbe\u5e76\u4e0d\u6210\u7acb\uff0c\u6700\u4f18\u63d0\u793a\u7b56\u7565\u5177\u6709\u4efb\u52a1\u4f9d\u8d56\u6027\u3002"}}
{"id": "2510.15908", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2510.15908", "abs": "https://arxiv.org/abs/2510.15908", "authors": ["Hana Chitsaz", "Johnson Umeike", "Amirmahdi Namjoo", "Babak N. Safa", "Bahar Asgari"], "title": "Belenos: Bottleneck Evaluation to Link Biomechanics to Novel Computing Optimizations", "comment": null, "summary": "Finite element simulations are essential in biomechanics, enabling detailed\nmodeling of tissues and organs. However, architectural inefficiencies in\ncurrent hardware and software stacks limit performance and scalability,\nespecially for iterative tasks like material parameter identification. As a\nresult, workflows often sacrifice fidelity for tractability. Reconfigurable\nhardware, such as FPGAs, offers a promising path to domain-specific\nacceleration without the cost of ASICs, but its potential in biomechanics\nremains underexplored. This paper presents Belenos, a comprehensive workload\ncharacterization of finite element biomechanics using FEBio, a widely adopted\nsimulator, gem5 sensitivity studies, and VTune analysis. VTune results reveal\nthat smaller workloads experience moderate front-end stalls, typically around\n13.1%, whereas larger workloads are dominated by significant back-end\nbottlenecks, with backend-bound cycles ranging from 59.9% to over 82.2%.\nComplementary gem5 sensitivity studies identify optimal hardware configurations\nfor Domain-Specific Accelerators (DSA), showing that suboptimal pipeline,\nmemory, or branch predictor settings can degrade performance by up to 37.1%.\nThese findings underscore the need for architecture-aware co-design to\nefficiently support biomechanical simulation workloads.", "AI": {"tldr": "\u672c\u6587\u901a\u8fc7FEBio\u3001gem5\u548cVTune\u5bf9\u6709\u9650\u5143\u751f\u7269\u529b\u5b66\u5de5\u4f5c\u8d1f\u8f7d\u8fdb\u884c\u7efc\u5408\u8868\u5f81\uff0c\u63ed\u793a\u4e86\u5f53\u524d\u8f6f\u786c\u4ef6\u67b6\u6784\u5728\u5904\u7406\u6b64\u7c7b\u4efb\u52a1\u65f6\u7684\u6027\u80fd\u74f6\u9888\uff0c\u5e76\u6307\u51fa\u9762\u5411\u9886\u57df\u7684\u67b6\u6784\u534f\u540c\u8bbe\u8ba1\u5bf9\u63d0\u5347\u6548\u7387\u81f3\u5173\u91cd\u8981\u3002", "motivation": "\u5f53\u524d\u751f\u7269\u529b\u5b66\u6709\u9650\u5143\u6a21\u62df\u53d7\u9650\u4e8e\u8f6f\u786c\u4ef6\u67b6\u6784\u6548\u7387\u4f4e\u4e0b\uff0c\u96be\u4ee5\u517c\u987e\u8ba1\u7b97\u7cbe\u5ea6\u4e0e\u53ef\u6269\u5c55\u6027\uff0c\u5c24\u5176\u5728\u6750\u6599\u53c2\u6570\u8bc6\u522b\u7b49\u8fed\u4ee3\u4efb\u52a1\u4e2d\u8868\u73b0\u660e\u663e\uff0c\u4e9f\u9700\u63a2\u7d22\u66f4\u9ad8\u6548\u7684\u52a0\u901f\u65b9\u6848\u3002", "method": "\u7ed3\u5408\u5e7f\u6cdb\u4f7f\u7528\u7684FEBio\u6a21\u62df\u5668\u3001gem5\u67b6\u6784\u654f\u611f\u6027\u7814\u7a76\u548cVTune\u6027\u80fd\u5206\u6790\u5de5\u5177\uff0c\u5bf9\u6709\u9650\u5143\u751f\u7269\u529b\u5b66\u5de5\u4f5c\u8d1f\u8f7d\u8fdb\u884c\u7cfb\u7edf\u8868\u5f81\uff0c\u8bc4\u4f30\u4e0d\u540c\u89c4\u6a21\u4efb\u52a1\u7684\u6027\u80fd\u74f6\u9888\u53ca\u786c\u4ef6\u914d\u7f6e\u5f71\u54cd\u3002", "result": "VTune\u5206\u6790\u663e\u793a\u5c0f\u89c4\u6a21\u4efb\u52a1\u5b58\u5728\u7ea613.1%\u7684\u524d\u7aef\u505c\u987f\uff0c\u800c\u5927\u89c4\u6a21\u4efb\u52a1\u4ee5\u540e\u7aef\u74f6\u9888\u4e3a\u4e3b\uff0859.9%\u201382.2%\uff09\uff1bgem5\u7814\u7a76\u8868\u660e\u6b21\u4f18\u7684\u6d41\u6c34\u7ebf\u3001\u5185\u5b58\u6216\u5206\u652f\u9884\u6d4b\u914d\u7f6e\u53ef\u5bfc\u81f4\u9ad8\u8fbe37.1%\u7684\u6027\u80fd\u4e0b\u964d\u3002", "conclusion": "\u4e3a\u9ad8\u6548\u652f\u6301\u751f\u7269\u529b\u5b66\u6a21\u62df\uff0c\u9700\u91c7\u7528\u67b6\u6784\u611f\u77e5\u7684\u8f6f\u786c\u4ef6\u534f\u540c\u8bbe\u8ba1\uff0c\u5145\u5206\u53d1\u6325\u53ef\u91cd\u6784\u786c\u4ef6\uff08\u5982FPGA\uff09\u5728\u9886\u57df\u4e13\u7528\u52a0\u901f\u4e2d\u7684\u6f5c\u529b\u3002"}}
{"id": "2510.16823", "categories": ["cs.SE", "cs.CR"], "pdf": "https://arxiv.org/pdf/2510.16823", "abs": "https://arxiv.org/abs/2510.16823", "authors": ["Yue Liu", "Zhenchang Xing", "Shidong Pan", "Chakkrit Tantithamthavorn"], "title": "When AI Takes the Wheel: Security Analysis of Framework-Constrained Program Generation", "comment": null, "summary": "In recent years, the AI wave has grown rapidly in software development. Even\nnovice developers can now design and generate complex framework-constrained\nsoftware systems based on their high-level requirements with the help of Large\nLanguage Models (LLMs). However, when LLMs gradually \"take the wheel\" of\nsoftware development, developers may only check whether the program works. They\noften miss security problems hidden in how the generated programs are\nimplemented.\n  In this work, we investigate the security properties of framework-constrained\nprograms generated by state-of-the-art LLMs. We focus specifically on Chrome\nextensions due to their complex security model involving multiple privilege\nboundaries and isolated components. To achieve this, we built ChromeSecBench, a\ndataset with 140 prompts based on known vulnerable extensions. We used these\nprompts to instruct nine state-of-the-art LLMs to generate complete Chrome\nextensions, and then analyzed them for vulnerabilities across three dimensions:\nscenario types, model differences, and vulnerability categories. Our results\nshow that LLMs produced vulnerable programs at alarmingly high rates (18%-50%),\nparticularly in Authentication & Identity and Cookie Management scenarios (up\nto 83% and 78% respectively). Most vulnerabilities exposed sensitive browser\ndata like cookies, history, or bookmarks to untrusted code. Interestingly, we\nfound that advanced reasoning models performed worse, generating more\nvulnerabilities than simpler models. These findings highlight a critical gap\nbetween LLMs' coding skills and their ability to write secure\nframework-constrained programs.", "AI": {"tldr": "\u8be5\u7814\u7a76\u53d1\u73b0\uff0c\u5f53\u524d\u4e3b\u6d41\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u751f\u6210\u53d7\u6846\u67b6\u7ea6\u675f\u7684Chrome\u6269\u5c55\u7a0b\u5e8f\u65f6\uff0c\u5b58\u5728\u4e25\u91cd\u7684\u5b89\u5168\u6f0f\u6d1e\u95ee\u9898\uff0c\u6f0f\u6d1e\u7387\u9ad8\u8fbe18%-50%\uff0c\u5c24\u5176\u5728\u8eab\u4efd\u8ba4\u8bc1\u548cCookie\u7ba1\u7406\u573a\u666f\u4e2d\u66f4\u4e3a\u7a81\u51fa\uff0c\u4e14\u9ad8\u7ea7\u63a8\u7406\u6a21\u578b\u53cd\u800c\u8868\u73b0\u66f4\u5dee\u3002", "motivation": "\u968f\u7740LLMs\u5728\u8f6f\u4ef6\u5f00\u53d1\u4e2d\u7684\u5e7f\u6cdb\u5e94\u7528\uff0c\u5f00\u53d1\u8005\u8d8a\u6765\u8d8a\u4f9d\u8d56\u5176\u751f\u6210\u590d\u6742\u7a0b\u5e8f\uff0c\u4f46\u5f80\u5f80\u5ffd\u89c6\u4e86\u5176\u4e2d\u6f5c\u85cf\u7684\u5b89\u5168\u95ee\u9898\uff0c\u5c24\u5176\u662f\u5728\u5177\u6709\u590d\u6742\u5b89\u5168\u6a21\u578b\u7684\u6846\u67b6\uff08\u5982Chrome\u6269\u5c55\uff09\u4e2d\u3002\u56e0\u6b64\uff0c\u6709\u5fc5\u8981\u7cfb\u7edf\u8bc4\u4f30LLMs\u751f\u6210\u4ee3\u7801\u7684\u5b89\u5168\u6027\u3002", "method": "\u6784\u5efa\u5305\u542b140\u4e2a\u63d0\u793a\u7684ChromeSecBench\u6570\u636e\u96c6\uff0c\u57fa\u4e8e\u5df2\u77e5\u6f0f\u6d1e\u6269\u5c55\uff1b\u4f7f\u75289\u4e2a\u4e3b\u6d41LLMs\u751f\u6210\u5b8c\u6574Chrome\u6269\u5c55\uff1b\u4ece\u573a\u666f\u7c7b\u578b\u3001\u6a21\u578b\u5dee\u5f02\u548c\u6f0f\u6d1e\u7c7b\u522b\u4e09\u4e2a\u7ef4\u5ea6\u5206\u6790\u751f\u6210\u4ee3\u7801\u4e2d\u7684\u5b89\u5168\u6f0f\u6d1e\u3002", "result": "LLMs\u751f\u6210\u7684\u7a0b\u5e8f\u6f0f\u6d1e\u7387\u9ad8\u8fbe18%-50%\uff0c\u5728Authentication & Identity\u548cCookie Management\u573a\u666f\u4e2d\u5206\u522b\u9ad8\u8fbe83%\u548c78%\uff1b\u591a\u6570\u6f0f\u6d1e\u5bfc\u81f4\u654f\u611f\u6d4f\u89c8\u5668\u6570\u636e\u6cc4\u9732\uff1b\u9ad8\u7ea7\u63a8\u7406\u6a21\u578b\u53cd\u800c\u6bd4\u7b80\u5355\u6a21\u578b\u4ea7\u751f\u66f4\u591a\u6f0f\u6d1e\u3002", "conclusion": "\u5c3d\u7ba1LLMs\u5177\u5907\u5f3a\u5927\u7684\u7f16\u7801\u80fd\u529b\uff0c\u4f46\u5728\u7f16\u5199\u5b89\u5168\u7684\u6846\u67b6\u7ea6\u675f\u7a0b\u5e8f\u65b9\u9762\u5b58\u5728\u663e\u8457\u4e0d\u8db3\uff0c\u4e9f\u9700\u63d0\u5347\u5176\u5bf9\u5b89\u5168\u6a21\u578b\u548c\u6743\u9650\u8fb9\u754c\u7684\u7406\u89e3\u80fd\u529b\u3002"}}
{"id": "2510.17639", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2510.17639", "abs": "https://arxiv.org/abs/2510.17639", "authors": ["Alkida Balliu", "Sebastian Brandt", "Ole Gabsdil", "Dennis Olivetti", "Jukka Suomela"], "title": "On the Universality of Round Elimination Fixed Points", "comment": null, "summary": "Recent work on distributed graph algorithms [e.g. STOC 2022, ITCS 2022, PODC\n2020] has drawn attention to the following open question: are round elimination\nfixed points a universal technique for proving lower bounds? That is, given a\nlocally checkable problem $\\Pi$ that requires at least $\\Omega(\\log n)$ rounds\nin the deterministic LOCAL model, can we always find a relaxation $\\Pi'$ of\n$\\Pi$ that is a nontrivial fixed point for the round elimination technique [see\nSTOC 2016, PODC 2019]? If yes, then a key part of distributed computational\ncomplexity would be also decidable.\n  The key obstacle so far has been a certain family of homomorphism problems\n[ITCS 2022], which require $\\Omega(\\log n)$ rounds, but the only known proof is\nbased on Marks' technique [J.AMS 2016].\n  We develop a new technique for constructing round elimination lower bounds\nsystematically. Using so-called tripotent inputs we show that the\naforementioned homomorphism problems indeed admit a lower bound proof that is\nbased on round elimination fixed points. Hence we eliminate the only known\nobstacle for the universality of round elimination.\n  Yet we also present a new obstacle: we show that there are some problems with\ninputs that require $\\Omega(\\log n)$ rounds, yet there is no proof that is\nbased on relaxations to nontrivial round elimination fixed points. Hence round\nelimination cannot be a universal technique for problems with inputs (but it\nmight be universal for problems without inputs).\n  We also prove the first fully general lower bound theorem that is applicable\nto any problem, with or without inputs, that is a fixed point in round\nelimination. Prior results of this form were only able to handle certain very\nrestricted inputs.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u8f6e\u6b21\u6d88\u9664\uff08round elimination\uff09\u4e0d\u52a8\u70b9\u662f\u5426\u80fd\u4f5c\u4e3a\u8bc1\u660e\u5206\u5e03\u5f0f\u56fe\u7b97\u6cd5\u4e2d\u5c40\u90e8\u53ef\u68c0\u9a8c\u95ee\u9898\u4e0b\u754c\u7684\u901a\u7528\u6280\u672f\u3002\u4f5c\u8005\u901a\u8fc7\u65b0\u65b9\u6cd5\u8bc1\u660e\u4e86\u6b64\u524d\u552f\u4e00\u969c\u788d\uff08\u540c\u6001\u95ee\u9898\uff09\u786e\u5b9e\u53ef\u901a\u8fc7\u8f6e\u6b21\u6d88\u9664\u5904\u7406\uff0c\u4f46\u540c\u65f6\u63ed\u793a\u4e86\u65b0\u969c\u788d\uff1a\u67d0\u4e9b\u5e26\u8f93\u5165\u7684\u95ee\u9898\u867d\u9700\u03a9(log n)\u8f6e\uff0c\u5374\u65e0\u6cd5\u901a\u8fc7\u677e\u5f1b\u5230\u975e\u5e73\u51e1\u8f6e\u6b21\u6d88\u9664\u4e0d\u52a8\u70b9\u6765\u8bc1\u660e\u4e0b\u754c\uff0c\u56e0\u6b64\u8f6e\u6b21\u6d88\u9664\u5bf9\u5e26\u8f93\u5165\u95ee\u9898\u5e76\u975e\u901a\u7528\uff0c\u4f46\u53ef\u80fd\u5bf9\u65e0\u8f93\u5165\u95ee\u9898\u901a\u7528\u3002\u6b64\u5916\uff0c\u4f5c\u8005\u8fd8\u7ed9\u51fa\u4e86\u9996\u4e2a\u9002\u7528\u4e8e\u4efb\u610f\u8f6e\u6b21\u6d88\u9664\u4e0d\u52a8\u70b9\u95ee\u9898\uff08\u65e0\u8bba\u662f\u5426\u5e26\u8f93\u5165\uff09\u7684\u901a\u7528\u4e0b\u754c\u5b9a\u7406\u3002", "motivation": "\u63a2\u7a76\u8f6e\u6b21\u6d88\u9664\u4e0d\u52a8\u70b9\u662f\u5426\u662f\u8bc1\u660e\u5206\u5e03\u5f0fLOCAL\u6a21\u578b\u4e2d\u5c40\u90e8\u53ef\u68c0\u9a8c\u95ee\u9898\u03a9(log n)\u8f6e\u4e0b\u754c\u7684\u901a\u7528\u65b9\u6cd5\uff0c\u8fd9\u5bf9\u5224\u65ad\u5206\u5e03\u5f0f\u8ba1\u7b97\u590d\u6742\u6027\u4e2d\u5173\u952e\u90e8\u5206\u662f\u5426\u53ef\u5224\u5b9a\u5177\u6709\u91cd\u8981\u610f\u4e49\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u201c\u4e09\u5e42\u8f93\u5165\u201d\uff08tripotent inputs\uff09\u7684\u65b0\u6280\u672f\uff0c\u7cfb\u7edf\u6027\u5730\u6784\u5efa\u8f6e\u6b21\u6d88\u9664\u4e0b\u754c\uff1b\u5229\u7528\u8be5\u65b9\u6cd5\u5206\u6790\u540c\u6001\u95ee\u9898\uff0c\u5e76\u6784\u9020\u53cd\u4f8b\u4ee5\u63ed\u793a\u8f6e\u6b21\u6d88\u9664\u7684\u5c40\u9650\u6027\uff1b\u540c\u65f6\u8bc1\u660e\u4e00\u4e2a\u9002\u7528\u4e8e\u4efb\u610f\u8f6e\u6b21\u6d88\u9664\u4e0d\u52a8\u70b9\u95ee\u9898\u7684\u901a\u7528\u4e0b\u754c\u5b9a\u7406\u3002", "result": "1\uff09\u8bc1\u660e\u4e86\u6b64\u524d\u552f\u4e00\u969c\u788d\uff08ITCS 2022\u4e2d\u7684\u540c\u6001\u95ee\u9898\uff09\u786e\u5b9e\u5b58\u5728\u57fa\u4e8e\u8f6e\u6b21\u6d88\u9664\u4e0d\u52a8\u70b9\u7684\u4e0b\u754c\u8bc1\u660e\uff1b2\uff09\u53d1\u73b0\u65b0\u969c\u788d\uff1a\u5b58\u5728\u67d0\u4e9b\u5e26\u8f93\u5165\u7684\u95ee\u9898\u867d\u9700\u03a9(log n)\u8f6e\uff0c\u4f46\u65e0\u6cd5\u901a\u8fc7\u677e\u5f1b\u5230\u975e\u5e73\u51e1\u8f6e\u6b21\u6d88\u9664\u4e0d\u52a8\u70b9\u6765\u8bc1\u660e\u4e0b\u754c\uff1b3\uff09\u5efa\u7acb\u4e86\u9996\u4e2a\u9002\u7528\u4e8e\u4efb\u610f\u8f6e\u6b21\u6d88\u9664\u4e0d\u52a8\u70b9\u95ee\u9898\uff08\u542b\u6216\u4e0d\u542b\u8f93\u5165\uff09\u7684\u901a\u7528\u4e0b\u754c\u5b9a\u7406\u3002", "conclusion": "\u8f6e\u6b21\u6d88\u9664\u4e0d\u52a8\u70b9\u4e0d\u662f\u5904\u7406\u6240\u6709\u5e26\u8f93\u5165\u95ee\u9898\u7684\u901a\u7528\u4e0b\u754c\u8bc1\u660e\u6280\u672f\uff0c\u4f46\u53ef\u80fd\u5bf9\u65e0\u8f93\u5165\u95ee\u9898\u5177\u6709\u666e\u9002\u6027\uff1b\u672c\u6587\u65e2\u6d88\u9664\u4e86\u65e7\u969c\u788d\uff0c\u53c8\u63ed\u793a\u4e86\u65b0\u9650\u5236\uff0c\u5e76\u63a8\u8fdb\u4e86\u8f6e\u6b21\u6d88\u9664\u7406\u8bba\u7684\u901a\u7528\u6027\u8fb9\u754c\u3002"}}
{"id": "2510.15910", "categories": ["cs.AR", "hep-ex"], "pdf": "https://arxiv.org/pdf/2510.15910", "abs": "https://arxiv.org/abs/2510.15910", "authors": ["Marvin Fuchs", "Lukas Scheller", "Timo Muscheid", "Oliver Sander", "Luis E. Ardila-Perez"], "title": "SoCks - Simplifying Firmware and Software Integration for Heterogeneous SoCs", "comment": "26 pages, single-column, 13 figures, 2 tables", "summary": "Modern heterogeneous System-on-Chip (SoC) devices integrate advanced\ncomponents into a single package, offering powerful capabilities while also\nintroducing significant complexity. To manage these sophisticated devices,\nfirmware and software developers need powerful development tools. However, as\nthese tools become increasingly complex, they often lack adequate support,\nresulting in a steep learning curve and challenging troubleshooting. To address\nthis, this work introduces System-on-Chip blocks (SoCks), a flexible and\nexpandable build framework that reduces complexity by partitioning the SoC\nimage into high-level units called blocks. SoCks builds each firmware and\nsoftware block in an encapsulated way, independently from other components of\nthe image, thereby reducing dependencies to a minimum. While some information\nexchange between the blocks is unavoidable to ensure seamless runtime\nintegration, this interaction is standardized via interfaces. A small number of\ndependencies and well-defined interfaces simplify the reuse of existing block\nimplementations and facilitate seamless substitution between versions-for\ninstance, when choosing root file systems for the embedded Linux operating\nsystem. Additionally, this approach facilitates the establishment of a\ndecentralized and partially automated development flow through Continuous\nIntegration and Continuous Delivery (CI/CD). Measurement results demonstrate\nthat SoCks can build a complete SoC image up to three times faster than\nestablished tools.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aSoCks\u7684\u6a21\u5757\u5316\u6784\u5efa\u6846\u67b6\uff0c\u901a\u8fc7\u5c06SoC\u955c\u50cf\u5212\u5206\u4e3a\u72ec\u7acb\u5c01\u88c5\u7684\u201c\u5757\u201d\uff0c\u663e\u8457\u964d\u4f4e\u5f00\u53d1\u590d\u6742\u6027\uff0c\u63d0\u5347\u6784\u5efa\u901f\u5ea6\u5e76\u652f\u6301\u7075\u6d3b\u590d\u7528\u4e0eCI/CD\u96c6\u6210\u3002", "motivation": "\u73b0\u4ee3\u5f02\u6784SoC\u8bbe\u5907\u65e5\u76ca\u590d\u6742\uff0c\u5f00\u53d1\u5de5\u5177\u7f3a\u4e4f\u8db3\u591f\u652f\u6301\uff0c\u5bfc\u81f4\u5b66\u4e60\u66f2\u7ebf\u9661\u5ced\u3001\u8c03\u8bd5\u56f0\u96be\uff0c\u4e9f\u9700\u4e00\u79cd\u80fd\u7b80\u5316\u5f00\u53d1\u6d41\u7a0b\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u5f15\u5165SoCks\u6784\u5efa\u6846\u67b6\uff0c\u5c06SoC\u955c\u50cf\u5212\u5206\u4e3a\u9ad8\u9636\u201c\u5757\u201d\uff0c\u6bcf\u4e2a\u5757\u72ec\u7acb\u5c01\u88c5\u6784\u5efa\uff0c\u901a\u8fc7\u6807\u51c6\u5316\u63a5\u53e3\u8fdb\u884c\u5fc5\u8981\u4ea4\u4e92\uff0c\u6700\u5c0f\u5316\u4f9d\u8d56\u3002", "result": "SoCks\u53ef\u5c06\u5b8c\u6574SoC\u955c\u50cf\u7684\u6784\u5efa\u901f\u5ea6\u63d0\u5347\u81f3\u4f20\u7edf\u5de5\u5177\u7684\u4e09\u500d\uff0c\u5e76\u652f\u6301\u5757\u7684\u590d\u7528\u3001\u7248\u672c\u66ff\u6362\u53caCI/CD\u6d41\u7a0b\u3002", "conclusion": "SoCks\u901a\u8fc7\u6a21\u5757\u5316\u548c\u63a5\u53e3\u6807\u51c6\u5316\u6709\u6548\u964d\u4f4e\u4e86SoC\u5f00\u53d1\u590d\u6742\u6027\uff0c\u63d0\u9ad8\u4e86\u6784\u5efa\u6548\u7387\u548c\u5f00\u53d1\u6d41\u7a0b\u7684\u81ea\u52a8\u5316\u6c34\u5e73\u3002"}}
{"id": "2510.17056", "categories": ["cs.SE", "cs.HC"], "pdf": "https://arxiv.org/pdf/2510.17056", "abs": "https://arxiv.org/abs/2510.17056", "authors": ["Luis F. G. Campos", "Leonardo C. Marques", "Walter T. Nakamura"], "title": "Will AI also replace inspectors? Investigating the potential of generative AIs in usability inspection", "comment": "Accepted and to be published in SBQS25 - Brazilian Symposium on\n  Software Quality 2025", "summary": "Usability inspection is a well-established technique for identifying\ninteraction issues in software interfaces, thereby contributing to improved\nproduct quality. However, it is a costly process that requires time and\nspecialized knowledge from inspectors. With advances in Artificial Intelligence\n(AI), new opportunities have emerged to support this task, particularly through\ngenerative models capable of interpreting interfaces and performing inspections\nmore efficiently. This study examines the performance of generative AIs in\nidentifying usability problems, comparing them to those of experienced human\ninspectors. A software prototype was evaluated by four specialists and two AI\nmodels (GPT-4o and Gemini 2.5 Flash), using metrics such as precision, recall,\nand F1-score. While inspectors achieved the highest levels of precision and\noverall coverage, the AIs demonstrated high individual performance and\ndiscovered many novel defects, but with a higher rate of false positives and\nredundant reports. The combination of AIs and human inspectors produced the\nbest results, revealing their complementarity. These findings suggest that AI,\nin its current stage, cannot replace human inspectors but can serve as a\nvaluable augmentation tool to improve efficiency and expand defect coverage.\nThe results provide evidence based on quantitative analysis to inform the\ndiscussion on the role of AI in usability inspections, pointing to viable paths\nfor its complementary use in software quality assessment contexts.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u751f\u6210\u5f0fAI\uff08GPT-4o\u548cGemini 2.5 Flash\uff09\u5728\u53ef\u7528\u6027\u68c0\u67e5\u4e2d\u7684\u8868\u73b0\uff0c\u53d1\u73b0\u5176\u867d\u4e0d\u80fd\u66ff\u4ee3\u4eba\u7c7b\u4e13\u5bb6\uff0c\u4f46\u53ef\u4f5c\u4e3a\u6709\u6548\u8f85\u52a9\u5de5\u5177\u63d0\u5347\u7f3a\u9677\u53d1\u73b0\u6548\u7387\u548c\u8986\u76d6\u8303\u56f4\u3002", "motivation": "\u53ef\u7528\u6027\u68c0\u67e5\u6210\u672c\u9ad8\u4e14\u4f9d\u8d56\u4e13\u5bb6\u77e5\u8bc6\uff0c\u800c\u4eba\u5de5\u667a\u80fd\u7684\u53d1\u5c55\u4e3a\u63d0\u5347\u8be5\u8fc7\u7a0b\u6548\u7387\u63d0\u4f9b\u4e86\u65b0\u673a\u4f1a\u3002", "method": "\u901a\u8fc7\u5bf9\u6bd4\u56db\u4f4d\u4eba\u7c7b\u4e13\u5bb6\u4e0e\u4e24\u4e2aAI\u6a21\u578b\uff08GPT-4o\u548cGemini 2.5 Flash\uff09\u5bf9\u540c\u4e00\u8f6f\u4ef6\u539f\u578b\u7684\u53ef\u7528\u6027\u95ee\u9898\u68c0\u6d4b\u7ed3\u679c\uff0c\u91c7\u7528\u7cbe\u786e\u7387\u3001\u53ec\u56de\u7387\u548cF1\u5206\u6570\u7b49\u6307\u6807\u8fdb\u884c\u8bc4\u4f30\u3002", "result": "\u4eba\u7c7b\u4e13\u5bb6\u5728\u7cbe\u786e\u7387\u548c\u6574\u4f53\u8986\u76d6\u65b9\u9762\u8868\u73b0\u6700\u4f73\uff1bAI\u6a21\u578b\u867d\u6709\u8f83\u9ad8\u4e2a\u4f53\u8868\u73b0\u5e76\u53d1\u73b0\u8bb8\u591a\u65b0\u7f3a\u9677\uff0c\u4f46\u5b58\u5728\u8f83\u591a\u8bef\u62a5\u548c\u91cd\u590d\u62a5\u544a\uff1b\u4eba\u673a\u7ed3\u5408\u6548\u679c\u6700\u4f18\u3002", "conclusion": "\u5f53\u524dAI\u5c1a\u4e0d\u80fd\u53d6\u4ee3\u4eba\u7c7b\u68c0\u67e5\u5458\uff0c\u4f46\u53ef\u4f5c\u4e3a\u6709\u4ef7\u503c\u7684\u8f85\u52a9\u5de5\u5177\uff0c\u63d0\u5347\u53ef\u7528\u6027\u68c0\u67e5\u7684\u6548\u7387\u4e0e\u7f3a\u9677\u8986\u76d6\u8303\u56f4\u3002"}}
{"id": "2510.17110", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.17110", "abs": "https://arxiv.org/abs/2510.17110", "authors": ["Xiaoyu Guo", "Shinobu Saito", "Jianjun Zhao"], "title": "M2QCode: A Model-Driven Framework for Generating Multi-Platform Quantum Programs", "comment": "This paper was accepted by ASE2025", "summary": "With the growing interest in quantum computing, the emergence of quantum\nsupremacy has marked a pivotal milestone in the field. As a result, numerous\nquantum programming languages (QPLs) have been introduced to support the\ndevelopment of quantum algorithms. However, the application of Model-Driven\nDevelopment (MDD) in quantum system engineering remains largely underexplored.\nThis paper presents an MDD-based approach to support the structured design and\nimplementation of quantum systems. Our framework enables the automatic\ngeneration of quantum code for multiple QPLs, thereby enhancing development\nefficiency and consistency across heterogeneous quantum platforms. The\neffectiveness and practicality of our approach have been demonstrated through\nmultiple case studies.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6a21\u578b\u9a71\u52a8\u5f00\u53d1\uff08MDD\uff09\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u7ed3\u6784\u5316\u8bbe\u8ba1\u548c\u5b9e\u73b0\u91cf\u5b50\u7cfb\u7edf\uff0c\u5e76\u80fd\u81ea\u52a8\u751f\u6210\u591a\u79cd\u91cf\u5b50\u7f16\u7a0b\u8bed\u8a00\u7684\u4ee3\u7801\uff0c\u63d0\u5347\u5f00\u53d1\u6548\u7387\u4e0e\u8de8\u5e73\u53f0\u4e00\u81f4\u6027\u3002", "motivation": "\u5c3d\u7ba1\u91cf\u5b50\u8ba1\u7b97\u65e5\u76ca\u53d7\u5230\u5173\u6ce8\u4e14\u5df2\u5b9e\u73b0\u91cf\u5b50\u4f18\u8d8a\u6027\uff0c\u4f46\u6a21\u578b\u9a71\u52a8\u5f00\u53d1\uff08MDD\uff09\u5728\u91cf\u5b50\u7cfb\u7edf\u5de5\u7a0b\u4e2d\u7684\u5e94\u7528\u4ecd\u9c9c\u6709\u7814\u7a76\u3002", "method": "\u63d0\u51fa\u4e00\u4e2a\u57fa\u4e8eMDD\u7684\u6846\u67b6\uff0c\u652f\u6301\u91cf\u5b50\u7cfb\u7edf\u7684\u7ed3\u6784\u5316\u8bbe\u8ba1\uff0c\u5e76\u80fd\u81ea\u52a8\u751f\u6210\u591a\u79cd\u91cf\u5b50\u7f16\u7a0b\u8bed\u8a00\uff08QPLs\uff09\u7684\u4ee3\u7801\u3002", "result": "\u901a\u8fc7\u591a\u4e2a\u6848\u4f8b\u7814\u7a76\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\u7684\u6709\u6548\u6027\u548c\u5b9e\u7528\u6027\u3002", "conclusion": "\u8be5MDD\u65b9\u6cd5\u80fd\u6709\u6548\u63d0\u5347\u91cf\u5b50\u7cfb\u7edf\u5f00\u53d1\u7684\u6548\u7387\u548c\u8de8\u5e73\u53f0\u4e00\u81f4\u6027\uff0c\u4e3a\u91cf\u5b50\u8f6f\u4ef6\u5de5\u7a0b\u63d0\u4f9b\u4e86\u65b0\u7684\u53ef\u884c\u8def\u5f84\u3002"}}
{"id": "2510.15917", "categories": ["cs.AR", "cs.AI", "cs.DC"], "pdf": "https://arxiv.org/pdf/2510.15917", "abs": "https://arxiv.org/abs/2510.15917", "authors": ["Shai Bergman", "Won Wook Song", "Lukas Cavigelli", "Konstantin Berestizshevsky", "Ke Zhou", "Ji Zhang"], "title": "Intent-Driven Storage Systems: From Low-Level Tuning to High-Level Understanding", "comment": null, "summary": "Existing storage systems lack visibility into workload intent, limiting their\nability to adapt to the semantics of modern, large-scale data-intensive\napplications. This disconnect leads to brittle heuristics and fragmented,\nsiloed optimizations. To address these limitations, we propose Intent-Driven\nStorage Systems (IDSS), a vision for a new paradigm where large language models\n(LLMs) infer workload and system intent from unstructured signals to guide\nadaptive and cross-layer parameter reconfiguration. IDSS provides holistic\nreasoning for competing demands, synthesizing safe and efficient decisions\nwithin policy guardrails. We present four design principles for integrating\nLLMs into storage control loops and propose a corresponding system\narchitecture. Initial results on FileBench workloads show that IDSS can improve\nIOPS by up to 2.45X by interpreting intent and generating actionable\nconfigurations for storage components such as caching and prefetching. These\nfindings suggest that, when constrained by guardrails and embedded within\nstructured workflows, LLMs can function as high-level semantic optimizers,\nbridging the gap between application goals and low-level system control. IDSS\npoints toward a future in which storage systems are increasingly adaptive,\nautonomous, and aligned with dynamic workload demands.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u610f\u56fe\u9a71\u52a8\u5b58\u50a8\u7cfb\u7edf\uff08IDSS\uff09\uff0c\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u4ece\u975e\u7ed3\u6784\u5316\u4fe1\u53f7\u4e2d\u63a8\u65ad\u5de5\u4f5c\u8d1f\u8f7d\u610f\u56fe\uff0c\u5b9e\u73b0\u8de8\u5c42\u81ea\u9002\u5e94\u914d\u7f6e\uff0c\u5728FileBench\u6d4b\u8bd5\u4e2dIOPS\u6700\u9ad8\u63d0\u53472.45\u500d\u3002", "motivation": "\u73b0\u6709\u5b58\u50a8\u7cfb\u7edf\u7f3a\u4e4f\u5bf9\u5de5\u4f5c\u8d1f\u8f7d\u610f\u56fe\u7684\u611f\u77e5\u80fd\u529b\uff0c\u5bfc\u81f4\u4f9d\u8d56\u8106\u5f31\u7684\u542f\u53d1\u5f0f\u65b9\u6cd5\u548c\u5b64\u7acb\u7684\u4f18\u5316\u7b56\u7565\uff0c\u96be\u4ee5\u9002\u5e94\u73b0\u4ee3\u5927\u89c4\u6a21\u6570\u636e\u5bc6\u96c6\u578b\u5e94\u7528\u7684\u8bed\u4e49\u9700\u6c42\u3002", "method": "\u5f15\u5165\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5230\u5b58\u50a8\u7cfb\u7edf\u7684\u63a7\u5236\u56de\u8def\u4e2d\uff0c\u901a\u8fc7\u63a8\u65ad\u5de5\u4f5c\u8d1f\u8f7d\u4e0e\u7cfb\u7edf\u610f\u56fe\uff0c\u7ed3\u5408\u7b56\u7565\u7ea6\u675f\uff0c\u5b9e\u73b0\u5bf9\u7f13\u5b58\u3001\u9884\u53d6\u7b49\u7ec4\u4ef6\u7684\u81ea\u9002\u5e94\u914d\u7f6e\uff1b\u63d0\u51fa\u56db\u9879\u8bbe\u8ba1\u539f\u5219\u548c\u76f8\u5e94\u7cfb\u7edf\u67b6\u6784\u3002", "result": "\u5728FileBench\u5de5\u4f5c\u8d1f\u8f7d\u4e0a\u7684\u521d\u6b65\u5b9e\u9a8c\u8868\u660e\uff0cIDSS\u53ef\u5c06IOPS\u63d0\u5347\u6700\u591a2.45\u500d\uff0c\u9a8c\u8bc1\u4e86LLM\u5728\u7b56\u7565\u7ea6\u675f\u4e0b\u53ef\u4f5c\u4e3a\u9ad8\u5c42\u8bed\u4e49\u4f18\u5316\u5668\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u5f53LLM\u88ab\u5d4c\u5165\u7ed3\u6784\u5316\u5de5\u4f5c\u6d41\u5e76\u53d7\u7b56\u7565\u7ea6\u675f\u65f6\uff0c\u53ef\u6709\u6548\u5f25\u5408\u5e94\u7528\u76ee\u6807\u4e0e\u5e95\u5c42\u7cfb\u7edf\u63a7\u5236\u4e4b\u95f4\u7684\u8bed\u4e49\u9e3f\u6c9f\uff0c\u63a8\u52a8\u5b58\u50a8\u7cfb\u7edf\u5411\u66f4\u81ea\u9002\u5e94\u3001\u81ea\u4e3b\u548c\u610f\u56fe\u5bf9\u9f50\u7684\u65b9\u5411\u53d1\u5c55\u3002"}}
{"id": "2510.17130", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.17130", "abs": "https://arxiv.org/abs/2510.17130", "authors": ["Shuzheng Gao", "Chaozheng Wang", "Cuiyun Gao", "Michael R. Lyu"], "title": "SEER: Enhancing Chain-of-Thought Code Generation through Self-Exploring Deep Reasoning", "comment": "The paper was completed in Feb. 2025, submitted to ICSE 2026 in Mar.\n  2025, received a major revision in Jun. 2025, and was finally accepted in\n  Oct. 2025", "summary": "Code generation, the task of creating executable programs from natural\nlanguage requirements, has recently seen tremendous advances through\nChain-of-Thought (CoT) reasoning, which enables Large Language Models (LLMs) to\ndevelop high-level reasoning plans before writing code. Recent research has\nproposed various methods to enhance models' CoT reasoning for code generation\nsuch as prompt engineering and supervised fine-tuning. However, existing\napproaches still face three critical limitations: (1) limited exploration of\ndiverse reasoning paths, which constrains generalization across various\nprogramming scenarios, (2) lack of quality assessment for intermediate\nreasoning steps, which hampers the reliability of the generated plans and code,\nand (3) the potential negative impact of \"overthinking\", potentially leading to\nunnecessarily complex and incorrect solutions. To address these limitations, we\nframe CoT code generation as a decision making problem and present SEER, a\nSElf-Exploring deep Reasoning framework that enables accurate and adaptive\nreasoning for code generation. SEER introduces three key components: (1)\nDiverse reasoning path exploration, which aims at exploring diverse reasoning\npaths and annotating intermediate steps without relying on manual experts or\nclosed-source proprietary models; (2) Reasoning quality-aware model training,\nwhich trains a policy model for generating candidate reasoning steps and a\nvalue model for assessing their quality; and (3) Adaptive CoT reasoning, which\ndynamically switches between direct generation and step-by-step reasoning for\ndifferent problems.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faSEER\u6846\u67b6\uff0c\u901a\u8fc7\u591a\u6837\u5316\u63a8\u7406\u8def\u5f84\u63a2\u7d22\u3001\u63a8\u7406\u8d28\u91cf\u611f\u77e5\u8bad\u7ec3\u548c\u81ea\u9002\u5e94CoT\u63a8\u7406\uff0c\u63d0\u5347\u5927\u8bed\u8a00\u6a21\u578b\u5728\u4ee3\u7801\u751f\u6210\u4efb\u52a1\u4e2d\u7684\u51c6\u786e\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u601d\u7ef4\u94fe\uff08CoT\uff09\u7684\u4ee3\u7801\u751f\u6210\u65b9\u6cd5\u5b58\u5728\u4e09\u5927\u5c40\u9650\uff1a\u63a8\u7406\u8def\u5f84\u591a\u6837\u6027\u4e0d\u8db3\u3001\u4e2d\u95f4\u63a8\u7406\u6b65\u9aa4\u7f3a\u4e4f\u8d28\u91cf\u8bc4\u4f30\u3001\u4ee5\u53ca\u201c\u8fc7\u5ea6\u601d\u8003\u201d\u53ef\u80fd\u5bfc\u81f4\u590d\u6742\u4e14\u9519\u8bef\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u63d0\u51faSEER\u6846\u67b6\uff0c\u5305\u542b\u4e09\u4e2a\u6838\u5fc3\u7ec4\u4ef6\uff1a(1) \u591a\u6837\u5316\u63a8\u7406\u8def\u5f84\u63a2\u7d22\uff0c\u81ea\u52a8\u6807\u6ce8\u4e2d\u95f4\u6b65\u9aa4\uff1b(2) \u63a8\u7406\u8d28\u91cf\u611f\u77e5\u7684\u6a21\u578b\u8bad\u7ec3\uff0c\u5305\u62ec\u751f\u6210\u5019\u9009\u63a8\u7406\u6b65\u9aa4\u7684\u7b56\u7565\u6a21\u578b\u548c\u8bc4\u4f30\u5176\u8d28\u91cf\u7684\u4ef7\u503c\u6a21\u578b\uff1b(3) \u81ea\u9002\u5e94CoT\u63a8\u7406\uff0c\u6839\u636e\u95ee\u9898\u52a8\u6001\u9009\u62e9\u76f4\u63a5\u751f\u6210\u6216\u9010\u6b65\u63a8\u7406\u3002", "result": "SEER\u6709\u6548\u63d0\u5347\u4e86\u4ee3\u7801\u751f\u6210\u7684\u51c6\u786e\u6027\u4e0e\u9002\u5e94\u6027\uff0c\u7f13\u89e3\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u6cdb\u5316\u6027\u3001\u53ef\u9760\u6027\u53ca\u590d\u6742\u6027\u65b9\u9762\u7684\u4e0d\u8db3\u3002", "conclusion": "\u5c06CoT\u4ee3\u7801\u751f\u6210\u89c6\u4e3a\u51b3\u7b56\u95ee\u9898\uff0c\u5e76\u901a\u8fc7SEER\u6846\u67b6\u5b9e\u73b0\u66f4\u51c6\u786e\u3001\u53ef\u9760\u548c\u9ad8\u6548\u7684\u63a8\u7406\u8fc7\u7a0b\uff0c\u4e3a\u4ee3\u7801\u751f\u6210\u4efb\u52a1\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2510.15926", "categories": ["cs.AR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.15926", "abs": "https://arxiv.org/abs/2510.15926", "authors": ["Ye Qiao", "Zhiheng Chen", "Yifan Zhang", "Yian Wang", "Sitao Huang"], "title": "TeLLMe v2: An Efficient End-to-End Ternary LLM Prefill and Decode Accelerator with Table-Lookup Matmul on Edge FPGAs", "comment": null, "summary": "With the emergence of wearable devices and other embedded systems, deploying\nlarge language models (LLMs) on edge platforms has become an urgent need.\nHowever, this is challenging because of their high computational and memory\ndemands. Although recent low-bit quantization methods (e.g., BitNet, DeepSeek)\ncompress weights to as low as 1.58~bits with minimal accuracy loss, edge\ndeployment is still constrained by limited on-chip resources, power budgets,\nand the often-neglected long latency of the prefill stage. We present\n\\textbf{TeLLMe}, the first table-lookup-based ternary LLM accelerator for\nlow-power edge FPGAs that fully supports both prefill and autoregressive\ndecoding using 1.58-bit weights and 8-bit activations. TeLLMe incorporates\nseveral novel techniques, including (1) a table-lookup-based ternary matrix\nmultiplication (TLMM) engine utilizing grouped activations and online\nprecomputation for low resource utilization and high throughput; (2) a\nfine-grained analytic URAM-based weight buffer management scheme for efficient\nloading and compute engine access; (3) a streaming dataflow architecture that\nfuses floating-point element-wise operations with linear computations to hide\nlatency; (4) a reversed-reordered prefill stage attention with fused attention\noperations for high memory efficiency; and (5) a resource-efficient specialized\ndecoding stage attention. Under a 5~W power budget, TeLLMe delivers up to\n25~tokens/s decoding throughput and 0.45--0.96~s time-to-first-token (TTFT) for\n64--128 token prompts, marking a significant energy-efficiency advancement in\nLLM inference on edge FPGAs.", "AI": {"tldr": "TeLLMe \u662f\u4e00\u79cd\u4e13\u4e3a\u4f4e\u529f\u8017\u8fb9\u7f18 FPGA \u8bbe\u8ba1\u7684\u57fa\u4e8e\u67e5\u8868\u7684\u4e09\u5143\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u52a0\u901f\u5668\uff0c\u652f\u6301 1.58 \u4f4d\u6743\u91cd\u548c 8 \u4f4d\u6fc0\u6d3b\uff0c\u663e\u8457\u63d0\u5347\u80fd\u6548\u5e76\u964d\u4f4e\u9884\u586b\u5145\u548c\u81ea\u56de\u5f52\u89e3\u7801\u9636\u6bb5\u7684\u5ef6\u8fdf\u3002", "motivation": "\u5728\u53ef\u7a7f\u6234\u8bbe\u5907\u548c\u5d4c\u5165\u5f0f\u7cfb\u7edf\u5174\u8d77\u7684\u80cc\u666f\u4e0b\uff0c\u5c06\u5927\u8bed\u8a00\u6a21\u578b\u90e8\u7f72\u5230\u8fb9\u7f18\u5e73\u53f0\u6210\u4e3a\u8feb\u5207\u9700\u6c42\uff0c\u4f46\u53d7\u9650\u4e8e\u9ad8\u8ba1\u7b97\u4e0e\u5185\u5b58\u5f00\u9500\u3001\u7247\u4e0a\u8d44\u6e90\u6709\u9650\u3001\u529f\u8017\u9884\u7b97\u7d27\u5f20\u4ee5\u53ca\u9884\u586b\u5145\u9636\u6bb5\u7684\u9ad8\u5ef6\u8fdf\u3002", "method": "TeLLMe \u5f15\u5165\u591a\u9879\u521b\u65b0\u6280\u672f\uff1a(1) \u57fa\u4e8e\u67e5\u8868\u7684\u4e09\u5143\u77e9\u9635\u4e58\u6cd5\u5f15\u64ce\uff08TLMM\uff09\uff0c\u7ed3\u5408\u5206\u7ec4\u6fc0\u6d3b\u4e0e\u5728\u7ebf\u9884\u8ba1\u7b97\uff1b(2) \u57fa\u4e8e\u5206\u6790\u7684\u7ec6\u7c92\u5ea6 URAM \u6743\u91cd\u7f13\u51b2\u7ba1\u7406\uff1b(3) \u878d\u5408\u6d6e\u70b9\u9010\u5143\u7d20\u64cd\u4f5c\u4e0e\u7ebf\u6027\u8ba1\u7b97\u7684\u6d41\u5f0f\u6570\u636e\u6d41\u67b6\u6784\uff1b(4) \u91cd\u6392\u53cd\u5411\u9884\u586b\u5145\u6ce8\u610f\u529b\u673a\u5236\uff1b(5) \u8d44\u6e90\u9ad8\u6548\u7684\u4e13\u7528\u89e3\u7801\u6ce8\u610f\u529b\u6a21\u5757\u3002", "result": "\u5728 5W \u529f\u8017\u9650\u5236\u4e0b\uff0cTeLLMe \u5b9e\u73b0\u6700\u9ad8 25 tokens/s \u7684\u89e3\u7801\u541e\u5410\u91cf\uff0c\u5bf9 64\u2013128 token \u7684\u63d0\u793a\u8bcd\u5b9e\u73b0 0.45\u20130.96 \u79d2\u7684\u9996 token \u5ef6\u8fdf\uff08TTFT\uff09\u3002", "conclusion": "TeLLMe \u5728\u8fb9\u7f18 FPGA \u4e0a\u5b9e\u73b0\u4e86\u9ad8\u6548\u80fd\u7684\u5927\u8bed\u8a00\u6a21\u578b\u63a8\u7406\uff0c\u663e\u8457\u63a8\u8fdb\u4e86\u4f4e\u529f\u8017\u8fb9\u7f18\u8bbe\u5907\u4e0a LLM \u90e8\u7f72\u7684\u53ef\u884c\u6027\u3002"}}
{"id": "2510.17142", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.17142", "abs": "https://arxiv.org/abs/2510.17142", "authors": ["Xiaoxue Ren", "Jun Wan", "Yun Peng", "Zhongxin Liu", "Ming Liang", "Dajun Chen", "Wei Jiang", "Yong Li"], "title": "PEACE: Towards Efficient Project-Level Efficiency Optimization via Hybrid Code Editing", "comment": null, "summary": "Large Language Models (LLMs) have demonstrated significant capability in code\ngeneration, but their potential in code efficiency optimization remains\nunderexplored. Previous LLM-based code efficiency optimization approaches\nexclusively focus on function-level optimization and overlook interaction\nbetween functions, failing to generalize to real-world development scenarios.\nCode editing techniques show great potential for conducting project-level\noptimization, yet they face challenges associated with invalid edits and\nsuboptimal internal functions. To address these gaps, we propose Peace, a novel\nhybrid framework for Project-level code Efficiency optimization through\nAutomatic Code Editing, which also ensures the overall correctness and\nintegrity of the project. Peace integrates three key phases: dependency-aware\noptimizing function sequence construction, valid associated edits\nidentification, and efficiency optimization editing iteration. To rigorously\nevaluate the effectiveness of Peace, we construct PeacExec, the first benchmark\ncomprising 146 real-world optimization tasks from 47 high-impact GitHub Python\nprojects, along with highly qualified test cases and executable environments.\nExtensive experiments demonstrate Peace's superiority over the state-of-the-art\nbaselines, achieving a 69.2% correctness rate (pass@1), +46.9% opt rate, and\n0.840 speedup in execution efficiency. Notably, our Peace outperforms all\nbaselines by significant margins, particularly in complex optimization tasks\nwith multiple functions. Moreover, extensive experiments are also conducted to\nvalidate the contributions of each component in Peace, as well as the rationale\nand effectiveness of our hybrid framework design.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86Peace\uff0c\u4e00\u4e2a\u7528\u4e8e\u9879\u76ee\u7ea7\u4ee3\u7801\u6548\u7387\u4f18\u5316\u7684\u81ea\u52a8\u4ee3\u7801\u7f16\u8f91\u6df7\u5408\u6846\u67b6\uff0c\u901a\u8fc7\u4f9d\u8d56\u611f\u77e5\u7684\u51fd\u6570\u5e8f\u5217\u6784\u5efa\u3001\u6709\u6548\u5173\u8054\u7f16\u8f91\u8bc6\u522b\u548c\u6548\u7387\u4f18\u5316\u8fed\u4ee3\uff0c\u5728\u65b0\u6784\u5efa\u7684\u57fa\u51c6PeacExec\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u4ee3\u7801\u6548\u7387\u4f18\u5316\u65b9\u6cd5\u4ec5\u5173\u6ce8\u51fd\u6570\u7ea7\u4f18\u5316\uff0c\u5ffd\u7565\u4e86\u51fd\u6570\u95f4\u7684\u4ea4\u4e92\uff0c\u96be\u4ee5\u9002\u7528\u4e8e\u771f\u5b9e\u5f00\u53d1\u573a\u666f\uff1b\u800c\u4ee3\u7801\u7f16\u8f91\u6280\u672f\u867d\u6709\u9879\u76ee\u7ea7\u4f18\u5316\u6f5c\u529b\uff0c\u5374\u9762\u4e34\u65e0\u6548\u7f16\u8f91\u548c\u6b21\u4f18\u5185\u90e8\u51fd\u6570\u7b49\u95ee\u9898\u3002", "method": "Peace\u6846\u67b6\u5305\u542b\u4e09\u4e2a\u5173\u952e\u9636\u6bb5\uff1a\u4f9d\u8d56\u611f\u77e5\u7684\u4f18\u5316\u51fd\u6570\u5e8f\u5217\u6784\u5efa\u3001\u6709\u6548\u5173\u8054\u7f16\u8f91\u8bc6\u522b\u3001\u6548\u7387\u4f18\u5316\u7f16\u8f91\u8fed\u4ee3\uff0c\u5e76\u5728\u65b0\u6784\u5efa\u7684\u9879\u76ee\u7ea7\u57fa\u51c6PeacExec\u4e0a\u8fdb\u884c\u8bc4\u4f30\u3002", "result": "Peace\u5728PeacExec\u57fa\u51c6\u4e0a\u8fbe\u523069.2%\u7684\u6b63\u786e\u7387\uff08pass@1\uff09\uff0c\u4f18\u5316\u6210\u529f\u7387\u63d0\u534746.9%\uff0c\u6267\u884c\u6548\u7387\u52a0\u901f\u6bd4\u8fbe0.840\uff0c\u5c24\u5176\u5728\u591a\u51fd\u6570\u590d\u6742\u4f18\u5316\u4efb\u52a1\u4e2d\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u3002", "conclusion": "Peace\u901a\u8fc7\u6df7\u5408\u6846\u67b6\u8bbe\u8ba1\u6709\u6548\u5b9e\u73b0\u4e86\u9879\u76ee\u7ea7\u4ee3\u7801\u6548\u7387\u4f18\u5316\uff0c\u5728\u4fdd\u8bc1\u9879\u76ee\u6b63\u786e\u6027\u548c\u5b8c\u6574\u6027\u7684\u540c\u65f6\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u9a8c\u8bc1\u4e86\u5176\u5404\u7ec4\u4ef6\u7684\u6709\u6548\u6027\u548c\u6574\u4f53\u8bbe\u8ba1\u7684\u5408\u7406\u6027\u3002"}}
{"id": "2510.17163", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.17163", "abs": "https://arxiv.org/abs/2510.17163", "authors": ["Shuzheng Gao", "Eric John Li", "Man Ho Lam", "Jingyu Xiao", "Yuxuan Wan", "Chaozheng Wang", "Ng Man Tik", "Michael R. Lyu"], "title": "TREAT: A Code LLMs Trustworthiness / Reliability Evaluation and Testing Framework", "comment": null, "summary": "Large foundation models are fundamentally transforming the software\nengineering landscape, demonstrating exceptional capabilities across diverse\ntasks such as code generation, debugging, and testing. Despite this rapid\nprogress, a significant gap remains in how to comprehensively evaluate these\nmodels' trustworthiness in real-world software engineering scenarios. Existing\nbenchmarks suffer from limited task scope and fail to incorporate critical\nevaluation aspects such as the robustness and reliability of models. To bridge\nthis gap, we present an evaluation framework called TREAT (Code LLMs\nTrustworthiness / Reliability Evaluation And Testing) that provides a holistic\nassessment of model performance in code intelligence tasks. Our evaluation\nframework addresses key limitations in existing approaches with four main\nimprovements: (1) Multi-Task Holistic Evaluation that spans diverse software\nengineering activities rather than limited coding tasks; (2) Multi-Language and\nMulti-Modality Assessment that extends beyond traditional single-language,\ntext-only benchmarks to include multi-modality coding tasks; (3) Robustness\nAssessment that evaluates model reliability under semantically-preserving code\ntransformations; and (4) Rigorous Evaluation Methodology that enhances the\ntrustworthiness of evaluation results through diverse evaluation prompts and\nadaptive solution extraction. Based on this evaluation framework, we assess 26\nstate-of-the-art models and uncover both their strengths and limitations,\nyielding several key insights:(1) Current models show substantial performance\nvariation across programming tasks; (2) Multi-modal language models demonstrate\nspecific performance limitations in UI code generation and edit;", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86TREAT\u8bc4\u4f30\u6846\u67b6\uff0c\u7528\u4e8e\u5168\u9762\u8bc4\u4f30\u4ee3\u7801\u5927\u6a21\u578b\u5728\u8f6f\u4ef6\u5de5\u7a0b\u4efb\u52a1\u4e2d\u7684\u53ef\u4fe1\u6027\u4e0e\u53ef\u9760\u6027\uff0c\u6db5\u76d6\u591a\u4efb\u52a1\u3001\u591a\u8bed\u8a00\u3001\u591a\u6a21\u6001\u53ca\u9c81\u68d2\u6027\u7b49\u65b9\u9762\uff0c\u5e76\u5bf926\u4e2a\u524d\u6cbf\u6a21\u578b\u8fdb\u884c\u4e86\u7cfb\u7edf\u8bc4\u4f30\u3002", "motivation": "\u73b0\u6709\u4ee3\u7801\u5927\u6a21\u578b\u8bc4\u4f30\u57fa\u51c6\u4efb\u52a1\u8303\u56f4\u6709\u9650\uff0c\u7f3a\u4e4f\u5bf9\u6a21\u578b\u9c81\u68d2\u6027\u548c\u53ef\u9760\u6027\u7684\u7efc\u5408\u8003\u91cf\uff0c\u96be\u4ee5\u53cd\u6620\u5176\u5728\u771f\u5b9e\u8f6f\u4ef6\u5de5\u7a0b\u573a\u666f\u4e2d\u7684\u53ef\u4fe1\u8868\u73b0\u3002", "method": "\u63d0\u51faTREAT\u8bc4\u4f30\u6846\u67b6\uff0c\u5305\u542b\u56db\u5927\u6539\u8fdb\uff1a(1) \u591a\u4efb\u52a1\u6574\u4f53\u8bc4\u4f30\uff1b(2) \u591a\u8bed\u8a00\u4e0e\u591a\u6a21\u6001\u8bc4\u4f30\uff1b(3) \u9c81\u68d2\u6027\u8bc4\u4f30\uff08\u901a\u8fc7\u8bed\u4e49\u4fdd\u6301\u7684\u4ee3\u7801\u53d8\u6362\uff09\uff1b(4) \u4e25\u8c28\u7684\u8bc4\u4f30\u65b9\u6cd5\uff08\u591a\u6837\u63d0\u793a\u4e0e\u81ea\u9002\u5e94\u89e3\u63d0\u53d6\uff09\u3002", "result": "\u5bf926\u4e2a\u5148\u8fdb\u6a21\u578b\u7684\u8bc4\u4f30\u663e\u793a\uff1a(1) \u6a21\u578b\u5728\u4e0d\u540c\u7f16\u7a0b\u4efb\u52a1\u4e2d\u6027\u80fd\u5dee\u5f02\u663e\u8457\uff1b(2) \u591a\u6a21\u6001\u8bed\u8a00\u6a21\u578b\u5728UI\u4ee3\u7801\u751f\u6210\u4e0e\u7f16\u8f91\u65b9\u9762\u5b58\u5728\u660e\u663e\u5c40\u9650\u3002", "conclusion": "TREAT\u6846\u67b6\u4e3a\u4ee3\u7801\u5927\u6a21\u578b\u7684\u53ef\u4fe1\u6027\u8bc4\u4f30\u63d0\u4f9b\u4e86\u66f4\u5168\u9762\u3001\u53ef\u9760\u7684\u65b9\u6cd5\uff0c\u63ed\u793a\u4e86\u5f53\u524d\u6a21\u578b\u7684\u4f18\u52bf\u4e0e\u4e0d\u8db3\uff0c\u4e3a\u672a\u6765\u6a21\u578b\u5f00\u53d1\u4e0e\u8bc4\u4f30\u63d0\u4f9b\u4e86\u91cd\u8981\u53c2\u8003\u3002"}}
{"id": "2510.17164", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.17164", "abs": "https://arxiv.org/abs/2510.17164", "authors": ["Maria Deolinda Santana", "Cleyton Magalhaes", "Ronnie de Souza Santos"], "title": "Software Testing with Large Language Models: An Interview Study with Practitioners", "comment": null, "summary": "\\textit{Background:} The use of large language models in software testing is\ngrowing fast as they support numerous tasks, from test case generation to\nautomation, and documentation. However, their adoption often relies on informal\nexperimentation rather than structured guidance. \\textit{Aims:} This study\ninvestigates how software testing professionals use LLMs in practice to propose\na preliminary, practitioner-informed guideline to support their integration\ninto testing workflows. \\textit{Method:} We conducted a qualitative study with\n15 software testers from diverse roles and domains. Data were collected through\nsemi-structured interviews and analyzed using grounded theory-based processes\nfocused on thematic analysis. \\textit{Results:} Testers described an iterative\nand reflective process that included defining testing objectives, applying\nprompt engineering strategies, refining prompts, evaluating outputs, and\nlearning over time. They emphasized the need for human oversight and careful\nvalidation, especially due to known limitations of LLMs such as hallucinations\nand inconsistent reasoning. \\textit{Conclusions:} LLM adoption in software\ntesting is growing, but remains shaped by evolving practices and caution around\nrisks. This study offers a starting point for structuring LLM use in testing\ncontexts and invites future research to refine these practices across teams,\ntools, and tasks.", "AI": {"tldr": "\u672c\u7814\u7a76\u901a\u8fc7\u8bbf\u8c0815\u540d\u8f6f\u4ef6\u6d4b\u8bd5\u4eba\u5458\uff0c\u63ed\u793a\u4e86\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5728\u8f6f\u4ef6\u6d4b\u8bd5\u4e2d\u7684\u5b9e\u9645\u5e94\u7528\u6a21\u5f0f\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u5957\u521d\u6b65\u7684\u3001\u4ee5\u5b9e\u8df5\u8005\u7ecf\u9a8c\u4e3a\u57fa\u7840\u7684\u4f7f\u7528\u6307\u5357\u3002", "motivation": "\u5f53\u524d\u5927\u8bed\u8a00\u6a21\u578b\u5728\u8f6f\u4ef6\u6d4b\u8bd5\u4e2d\u7684\u5e94\u7528\u7f3a\u4e4f\u7cfb\u7edf\u6027\u6307\u5bfc\uff0c\u591a\u4f9d\u8d56\u975e\u6b63\u5f0f\u5b9e\u9a8c\uff0c\u4e9f\u9700\u57fa\u4e8e\u4ece\u4e1a\u8005\u7ecf\u9a8c\u7684\u7ed3\u6784\u5316\u6307\u5357\u4ee5\u652f\u6301\u5176\u6709\u6548\u6574\u5408\u3002", "method": "\u91c7\u7528\u5b9a\u6027\u7814\u7a76\u65b9\u6cd5\uff0c\u5bf9\u6765\u81ea\u4e0d\u540c\u89d2\u8272\u548c\u9886\u57df\u768415\u540d\u8f6f\u4ef6\u6d4b\u8bd5\u4eba\u5458\u8fdb\u884c\u534a\u7ed3\u6784\u5316\u8bbf\u8c08\uff0c\u5e76\u57fa\u4e8e\u624e\u6839\u7406\u8bba\u8fdb\u884c\u4e3b\u9898\u5206\u6790\u3002", "result": "\u6d4b\u8bd5\u4eba\u5458\u91c7\u7528\u5305\u542b\u76ee\u6807\u5b9a\u4e49\u3001\u63d0\u793a\u5de5\u7a0b\u3001\u8f93\u51fa\u8bc4\u4f30\u4e0e\u6301\u7eed\u5b66\u4e60\u7684\u8fed\u4ee3\u6d41\u7a0b\uff0c\u5e76\u5f3a\u8c03\u4eba\u7c7b\u76d1\u7763\u548c\u9a8c\u8bc1\u7684\u91cd\u8981\u6027\uff0c\u4ee5\u5e94\u5bf9LLM\u7684\u5e7b\u89c9\u548c\u63a8\u7406\u4e0d\u4e00\u81f4\u7b49\u95ee\u9898\u3002", "conclusion": "LLM\u5728\u8f6f\u4ef6\u6d4b\u8bd5\u4e2d\u7684\u5e94\u7528\u6b63\u5728\u589e\u957f\uff0c\u4f46\u5b9e\u8df5\u4ecd\u5904\u4e8e\u6f14\u8fdb\u9636\u6bb5\u4e14\u9700\u8c28\u614e\u5e94\u5bf9\u98ce\u9669\uff1b\u672c\u7814\u7a76\u4e3a\u7ed3\u6784\u5316\u4f7f\u7528LLM\u63d0\u4f9b\u4e86\u521d\u6b65\u6846\u67b6\uff0c\u5e76\u547c\u5401\u672a\u6765\u7814\u7a76\u8fdb\u4e00\u6b65\u5b8c\u5584\u76f8\u5173\u5b9e\u8df5\u3002"}}
{"id": "2510.16040", "categories": ["cs.AR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.16040", "abs": "https://arxiv.org/abs/2510.16040", "authors": ["Tianhua Xia", "Sai Qian Zhang"], "title": "Kelle: Co-design KV Caching and eDRAM for Efficient LLM Serving in Edge Computing", "comment": null, "summary": "Running Large Language Models (LLMs) on edge devices is crucial for reducing\nlatency, improving real-time processing, and enhancing privacy. By performing\ninference directly on the device, data does not need to be sent to the cloud,\nensuring faster responses and reducing reliance on network connectivity.\nHowever, implementing LLMs on edge devices presents challenges, particularly\nwith managing key-value (KV) caches, which plays a pivotal role in LLM serving.\nAs the input text lengthens, the size of the KV cache increases linearly with\nthe sequence length, leading to a significant memory footprint and data access\ncosts. On the other hand, edge devices have limited memory and computational\npower, making it hard to store and efficiently access the large caches needed\nfor LLM inference.\n  To mitigate the substantial overhead caused by KV cache, we propose using\nembedded DRAM (eDRAM) as the primary storage for LLM serving in edge device,\nwhich offers higher storage density compared to SRAM. However, to ensure data\nintegrity, eDRAM needs periodic refresh operations, which are power-intensive.\nTo reduce eDRAM costs and improve overall system performance, we\npropose~\\textit{Kelle}, a software-hardware co-design solution optimized for\ndeploying LLMs on eDRAM-based edge systems. Combined with our fine-grained\nmemory eviction, recomputation, and refresh control algorithms, the\n\\textit{Kelle} accelerator delivers a $3.9\\times$ speedup and $4.5\\times$\nenergy savings compared to existing baseline solutions.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aKelle\u7684\u8f6f\u786c\u4ef6\u534f\u540c\u8bbe\u8ba1\u65b9\u6cd5\uff0c\u901a\u8fc7\u5c06\u5d4c\u5165\u5f0fDRAM\uff08eDRAM\uff09\u4f5c\u4e3a\u8fb9\u7f18\u8bbe\u5907\u4e0a\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u63a8\u7406\u4e2d\u952e\u503c\uff08KV\uff09\u7f13\u5b58\u7684\u4e3b\u8981\u5b58\u50a8\u4ecb\u8d28\uff0c\u5e76\u7ed3\u5408\u7ec6\u7c92\u5ea6\u7684\u5185\u5b58\u9a71\u9010\u3001\u91cd\u8ba1\u7b97\u548c\u5237\u65b0\u63a7\u5236\u7b97\u6cd5\uff0c\u5728\u663e\u8457\u964d\u4f4e\u5185\u5b58\u5f00\u9500\u7684\u540c\u65f6\uff0c\u5b9e\u73b0\u4e863.9\u500d\u7684\u901f\u5ea6\u63d0\u5347\u548c4.5\u500d\u7684\u80fd\u8017\u8282\u7701\u3002", "motivation": "\u5728\u8fb9\u7f18\u8bbe\u5907\u4e0a\u8fd0\u884c\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5bf9\u964d\u4f4e\u5ef6\u8fdf\u3001\u63d0\u5347\u5b9e\u65f6\u6027\u548c\u9690\u79c1\u4fdd\u62a4\u81f3\u5173\u91cd\u8981\uff0c\u4f46KV\u7f13\u5b58\u968f\u8f93\u5165\u5e8f\u5217\u957f\u5ea6\u7ebf\u6027\u589e\u957f\uff0c\u5e26\u6765\u5de8\u5927\u7684\u5185\u5b58\u5360\u7528\u548c\u8bbf\u95ee\u5f00\u9500\uff0c\u800c\u8fb9\u7f18\u8bbe\u5907\u8d44\u6e90\u53d7\u9650\uff0c\u96be\u4ee5\u9ad8\u6548\u652f\u6301\u6b64\u7c7b\u7f13\u5b58\u9700\u6c42\u3002", "method": "\u63d0\u51fa\u4f7f\u7528\u5d4c\u5165\u5f0fDRAM\uff08eDRAM\uff09\u66ff\u4ee3\u4f20\u7edfSRAM\u4f5c\u4e3aKV\u7f13\u5b58\u7684\u4e3b\u5b58\u50a8\uff0c\u5e76\u8bbe\u8ba1\u540d\u4e3aKelle\u7684\u8f6f\u786c\u4ef6\u534f\u540c\u52a0\u901f\u65b9\u6848\uff0c\u96c6\u6210\u7ec6\u7c92\u5ea6\u7684\u5185\u5b58\u9a71\u9010\u3001\u91cd\u8ba1\u7b97\u548c\u5237\u65b0\u63a7\u5236\u7b97\u6cd5\uff0c\u4ee5\u4f18\u5316eDRAM\u7684\u4f7f\u7528\u6548\u7387\u548c\u6570\u636e\u5b8c\u6574\u6027\u3002", "result": "Kelle\u65b9\u6848\u76f8\u6bd4\u73b0\u6709\u57fa\u7ebf\u65b9\u6cd5\u5b9e\u73b0\u4e863.9\u500d\u7684\u63a8\u7406\u901f\u5ea6\u63d0\u5347\u548c4.5\u500d\u7684\u80fd\u6548\u63d0\u5347\u3002", "conclusion": "\u901a\u8fc7\u8f6f\u786c\u4ef6\u534f\u540c\u8bbe\u8ba1\uff0cKelle\u6709\u6548\u7f13\u89e3\u4e86\u8fb9\u7f18\u8bbe\u5907\u4e0aLLM\u63a8\u7406\u4e2dKV\u7f13\u5b58\u5e26\u6765\u7684\u5185\u5b58\u4e0e\u80fd\u8017\u74f6\u9888\uff0c\u4e3a\u5728\u8d44\u6e90\u53d7\u9650\u8bbe\u5907\u4e0a\u9ad8\u6548\u90e8\u7f72\u5927\u8bed\u8a00\u6a21\u578b\u63d0\u4f9b\u4e86\u53ef\u884c\u8def\u5f84\u3002"}}
{"id": "2510.17184", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.17184", "abs": "https://arxiv.org/abs/2510.17184", "authors": ["Nicolas Robert", "Fabien Gandon", "Maxime Lefran\u00e7ois"], "title": "OLIVAW: ACIMOV's GitHub robot assisting agile collaborative ontology development", "comment": null, "summary": "Agile and collaborative approaches to ontologies design are crucial because\nthey contribute to making them userdriven, up-to-date, and able to evolve\nalongside the systems they support, hence proper continuous validation tooling\nis required to ensure ontologies match developers' requirements all along their\ndevelopment. We propose OLIVAW (Ontology Long-lived Integration Via ACIMOV\nWorkflow), a tool supporting the ACIMOV methodology on GitHub. It relies on W3C\nStandards to assist the development of modular ontologies through GitHub\nComposite Actions, pre-commit hooks, or a command line interface. OLIVAW was\ntested on several ontology projects to ensure its usefulness, genericity and\nreusability. A template repository is available for a quick start. OLIVAW is", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86OLIVAW\u5de5\u5177\uff0c\u57fa\u4e8eACIMOV\u65b9\u6cd5\u8bba\uff0c\u5229\u7528GitHub\u548cW3C\u6807\u51c6\u652f\u6301\u6a21\u5757\u5316\u672c\u4f53\u7684\u654f\u6377\u5f00\u53d1\u4e0e\u6301\u7eed\u9a8c\u8bc1\u3002", "motivation": "\u4e3a\u652f\u6301\u7528\u6237\u9a71\u52a8\u3001\u6301\u7eed\u66f4\u65b0\u5e76\u80fd\u968f\u7cfb\u7edf\u6f14\u5316\u7684\u672c\u4f53\u8bbe\u8ba1\uff0c\u9700\u8981\u5408\u9002\u7684\u6301\u7eed\u9a8c\u8bc1\u5de5\u5177\u6765\u786e\u4fdd\u672c\u4f53\u59cb\u7ec8\u7b26\u5408\u5f00\u53d1\u8005\u9700\u6c42\u3002", "method": "\u5f00\u53d1\u4e86OLIVAW\u5de5\u5177\uff0c\u7ed3\u5408ACIMOV\u65b9\u6cd5\u8bba\uff0c\u5728GitHub\u4e0a\u901a\u8fc7Composite Actions\u3001pre-commit hooks\u6216\u547d\u4ee4\u884c\u63a5\u53e3\uff0c\u5229\u7528W3C\u6807\u51c6\u8f85\u52a9\u6a21\u5757\u5316\u672c\u4f53\u5f00\u53d1\u3002", "result": "OLIVAW\u5728\u591a\u4e2a\u672c\u4f53\u9879\u76ee\u4e2d\u8fdb\u884c\u4e86\u6d4b\u8bd5\uff0c\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3001\u901a\u7528\u6027\u548c\u53ef\u91cd\u7528\u6027\uff0c\u5e76\u63d0\u4f9b\u4e86\u6a21\u677f\u4ed3\u5e93\u4ee5\u4f9b\u5feb\u901f\u542f\u52a8\u3002", "conclusion": "OLIVAW\u4e3a\u672c\u4f53\u7684\u654f\u6377\u534f\u4f5c\u5f00\u53d1\u63d0\u4f9b\u4e86\u5b9e\u7528\u3001\u901a\u7528\u4e14\u53ef\u590d\u7528\u7684\u6301\u7eed\u9a8c\u8bc1\u652f\u6301\uff0c\u6709\u52a9\u4e8e\u63d0\u5347\u672c\u4f53\u4e0e\u7cfb\u7edf\u9700\u6c42\u7684\u4e00\u81f4\u6027\u3002"}}
{"id": "2510.16487", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2510.16487", "abs": "https://arxiv.org/abs/2510.16487", "authors": ["Giovanni Agosta", "Stefano Cherubin", "Derek Christ", "Francesco Conti", "Asbj\u00f8rn Djupdal", "Matthias Jung", "Georgios Keramidas", "Roberto Passerone", "Paolo Rech", "Elisa Ricci", "Philippe Velha", "Flavio Vella", "Kasim Sinan Yildirim", "Nils Wilbert"], "title": "Architecture, Simulation and Software Stack to Support Post-CMOS Accelerators: The ARCHYTAS Project", "comment": null, "summary": "ARCHYTAS aims to design and evaluate non-conventional hardware accelerators,\nin particular, optoelectronic, volatile and non-volatile processing-in-memory,\nand neuromorphic, to tackle the power, efficiency, and scalability bottlenecks\nof AI with an emphasis on defense use cases (e.g., autonomous vehicles,\nsurveillance drones, maritime and space platforms). In this paper, we present\nthe system architecture and software stack that ARCHYTAS will develop to\nintegrate and support those accelerators, as well as the simulation software\nneeded for early prototyping of the full system and its components.", "AI": {"tldr": "ARCHYTAS\u5f00\u53d1\u9762\u5411\u56fd\u9632AI\u5e94\u7528\u7684\u975e\u4f20\u7edf\u786c\u4ef6\u52a0\u901f\u5668\uff08\u5982\u5149\u7535\u3001\u5b58\u5185\u8ba1\u7b97\u3001\u795e\u7ecf\u5f62\u6001\uff09\uff0c\u5e76\u6784\u5efa\u914d\u5957\u7684\u7cfb\u7edf\u67b6\u6784\u3001\u8f6f\u4ef6\u6808\u4e0e\u4eff\u771f\u5de5\u5177\u3002", "motivation": "\u4f20\u7edfAI\u786c\u4ef6\u5728\u529f\u8017\u3001\u6548\u7387\u548c\u53ef\u6269\u5c55\u6027\u65b9\u9762\u9762\u4e34\u74f6\u9888\uff0c\u5c24\u5176\u5728\u56fd\u9632\u573a\u666f\uff08\u5982\u65e0\u4eba\u8f66\u3001\u4fa6\u5bdf\u65e0\u4eba\u673a\u3001\u6d77\u7a7a\u5e73\u53f0\uff09\u4e2d\u4e9f\u9700\u66f4\u9ad8\u6548\u7684\u8ba1\u7b97\u65b9\u6848\u3002", "method": "\u8bbe\u8ba1\u5e76\u96c6\u6210\u591a\u79cd\u975e\u4f20\u7edf\u786c\u4ef6\u52a0\u901f\u5668\uff0c\u5f00\u53d1\u7edf\u4e00\u7684\u7cfb\u7edf\u67b6\u6784\u3001\u8f6f\u4ef6\u6808\u53ca\u65e9\u671f\u4eff\u771f\u5de5\u5177\u4ee5\u652f\u6301\u5168\u7cfb\u7edf\u539f\u578b\u9a8c\u8bc1\u3002", "result": "\u63d0\u51fa\u4e86\u4e00\u5957\u5b8c\u6574\u7684\u8f6f\u786c\u4ef6\u534f\u540c\u6846\u67b6\uff0c\u7528\u4e8e\u652f\u6301\u9762\u5411\u56fd\u9632AI\u5e94\u7528\u7684\u65b0\u578b\u52a0\u901f\u5668\u5f00\u53d1\u4e0e\u8bc4\u4f30\u3002", "conclusion": "ARCHYTAS\u901a\u8fc7\u8f6f\u786c\u4ef6\u534f\u540c\u8bbe\u8ba1\uff0c\u4e3a\u56fd\u9632\u9886\u57dfAI\u7cfb\u7edf\u63d0\u4f9b\u9ad8\u6548\u3001\u4f4e\u529f\u8017\u3001\u53ef\u6269\u5c55\u7684\u975e\u4f20\u7edf\u8ba1\u7b97\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.17376", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.17376", "abs": "https://arxiv.org/abs/2510.17376", "authors": ["Yongmin Li", "Jia Li", "Ge Li", "Zhi Jin"], "title": "AdapTrack: Constrained Decoding without Distorting LLM's Output Intent", "comment": "to be published in ICSE 2026", "summary": "Language model-based code generation and completion tools have been widely\nadopted, but they may sometimes produce code that does not meet necessary\nconstraints, such as syntactic correctness or API existence. Constrained\ndecoding techniques are developed to help the model generate code adhering to\nthe constraints by greedily eliminating generation options that violate\nconstraints at each step of the generation process. However, there is a severe\nlimitation of constrained decoding, that it distorts the model's output intent,\nforcing it to produce code that may satisfy the constraint but does not match\nthe development intent and is therefore incorrect. In response to this\nchallenge, we propose AdapTrack. By incorporating backtracking into the\ngeneration process, AdapTrack avoids distorting the output intent of the model,\nthereby producing results that are not only constraint-compliant but also more\nsemantically aligned with model's output intent. On our synthetic API\ncompletion dataset, AdapTrack can achieve up to 360.87% improvement compared to\nconstrained decoding; on the real-world API completion dataset we collect that\nexhibits similar issues, AdapTrack can achieve up to 38.93% improvement over\nconstrained decoding; in general code genration benchmarks, compared to\nconstrained decoding, AdapTrack can achieve up to 7.84% improvement on\nHumanEval, and up to 6.42% improvement on MBPP. This indicates that, simply by\nbetter adhering to the model's output intent, AdapTrack can achieve significant\nimprovements. We provide a theoretical proof that the distribution produced by\nAdapTrack aligns with the model's distribution given the generated tokens,\nthereby ensuring that the model's output intent is not distorted. Experiments\non DSL problems show that, compared to existing methods, our approach can\nprovide generation results that are more consistent with the language model's\ndistribution.", "AI": {"tldr": "AdapTrack \u662f\u4e00\u79cd\u7ed3\u5408\u56de\u6eaf\u673a\u5236\u7684\u4ee3\u7801\u751f\u6210\u65b9\u6cd5\uff0c\u901a\u8fc7\u907f\u514d\u7ea6\u675f\u89e3\u7801\u5bf9\u6a21\u578b\u8f93\u51fa\u610f\u56fe\u7684\u626d\u66f2\uff0c\u5728\u6ee1\u8db3\u8bed\u6cd5\u548c API \u7b49\u7ea6\u675f\u7684\u540c\u65f6\uff0c\u663e\u8457\u63d0\u5347\u751f\u6210\u4ee3\u7801\u7684\u8bed\u4e49\u6b63\u786e\u6027\u548c\u4e0e\u6a21\u578b\u539f\u59cb\u610f\u56fe\u7684\u4e00\u81f4\u6027\u3002", "motivation": "\u73b0\u6709\u7ea6\u675f\u89e3\u7801\u65b9\u6cd5\u5728\u5f3a\u5236\u6ee1\u8db3\u7ea6\u675f\uff08\u5982\u8bed\u6cd5\u6b63\u786e\u6027\u3001API \u5b58\u5728\u6027\uff09\u65f6\u4f1a\u626d\u66f2\u8bed\u8a00\u6a21\u578b\u7684\u539f\u59cb\u8f93\u51fa\u610f\u56fe\uff0c\u5bfc\u81f4\u751f\u6210\u7684\u4ee3\u7801\u867d\u7b26\u5408\u7ea6\u675f\u4f46\u8bed\u4e49\u4e0a\u4e0d\u7b26\u5408\u5f00\u53d1\u9700\u6c42\u3002", "method": "\u63d0\u51fa AdapTrack \u65b9\u6cd5\uff0c\u5728\u751f\u6210\u8fc7\u7a0b\u4e2d\u5f15\u5165\u56de\u6eaf\u673a\u5236\uff0c\u4f7f\u5f97\u6a21\u578b\u5728\u6ee1\u8db3\u7ea6\u675f\u7684\u540c\u65f6\u4fdd\u7559\u5176\u539f\u59cb\u8f93\u51fa\u610f\u56fe\uff1b\u5e76\u901a\u8fc7\u7406\u8bba\u8bc1\u660e\u8be5\u65b9\u6cd5\u751f\u6210\u7684\u5206\u5e03\u4e0e\u6a21\u578b\u539f\u59cb\u5206\u5e03\u4e00\u81f4\u3002", "result": "\u5728\u5408\u6210\u548c\u771f\u5b9e API \u8865\u5168\u6570\u636e\u96c6\u4e0a\uff0cAdapTrack \u76f8\u6bd4\u7ea6\u675f\u89e3\u7801\u5206\u522b\u63d0\u5347 360.87% \u548c 38.93%\uff1b\u5728 HumanEval \u548c MBPP \u57fa\u51c6\u4e0a\u5206\u522b\u63d0\u5347 7.84% \u548c 6.42%\uff1b\u5728 DSL \u4efb\u52a1\u4e2d\u4e5f\u8868\u73b0\u51fa\u66f4\u5f3a\u7684\u4e00\u81f4\u6027\u3002", "conclusion": "AdapTrack \u901a\u8fc7\u56de\u6eaf\u673a\u5236\u6709\u6548\u7f13\u89e3\u4e86\u7ea6\u675f\u89e3\u7801\u5bf9\u6a21\u578b\u610f\u56fe\u7684\u626d\u66f2\u95ee\u9898\uff0c\u5728\u591a\u79cd\u4ee3\u7801\u751f\u6210\u4efb\u52a1\u4e2d\u663e\u8457\u4f18\u4e8e\u4f20\u7edf\u7ea6\u675f\u89e3\u7801\u65b9\u6cd5\uff0c\u540c\u65f6\u7406\u8bba\u548c\u5b9e\u9a8c\u5747\u9a8c\u8bc1\u4e86\u5176\u5bf9\u6a21\u578b\u539f\u59cb\u5206\u5e03\u7684\u4fdd\u771f\u6027\u3002"}}
{"id": "2510.16622", "categories": ["cs.AR", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2510.16622", "abs": "https://arxiv.org/abs/2510.16622", "authors": ["Kazi Ababil Azam", "Hasan Masum", "Masfiqur Rahaman", "A. B. M. Alim Al Islam"], "title": "Towards Intelligent Traffic Signaling in Dhaka City Based on Vehicle Detection and Congestion Optimization", "comment": "10 pages, Submitted to IEEE Transactions on Intelligent\n  Transportation Systems (T-ITS)", "summary": "The vehicular density in urbanizing cities of developing countries such as\nDhaka, Bangladesh result in a lot of traffic congestion, causing poor on-road\nexperiences. Traffic signaling is a key component in effective traffic\nmanagement for such situations, but the advancements in intelligent traffic\nsignaling have been exclusive to developed countries with structured traffic.\nThe non-lane-based, heterogeneous traffic of Dhaka City requires a contextual\napproach. This study focuses on the development of an intelligent traffic\nsignaling system feasible in the context of developing countries such as\nBangladesh. We propose a pipeline leveraging Real Time Streaming Protocol\n(RTSP) feeds, a low resources system Raspberry Pi 4B processing, and a state of\nthe art YOLO-based object detection model trained on the Non-lane-based and\nHeterogeneous Traffic (NHT-1071) dataset to detect and classify heterogeneous\ntraffic. A multi-objective optimization algorithm, NSGA-II, then generates\noptimized signal timings, minimizing waiting time while maximizing vehicle\nthroughput. We test our implementation in a five-road intersection at Palashi,\nDhaka, demonstrating the potential to significantly improve traffic management\nin similar situations. The developed testbed paves the way for more contextual\nand effective Intelligent Traffic Signaling (ITS) solutions for developing\nareas with complicated traffic dynamics such as Dhaka City.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9002\u7528\u4e8e\u53d1\u5c55\u4e2d\u56fd\u5bb6\u975e\u8f66\u9053\u5316\u5f02\u6784\u4ea4\u901a\u73af\u5883\u7684\u667a\u80fd\u4ea4\u901a\u4fe1\u53f7\u7cfb\u7edf\uff0c\u7ed3\u5408RTSP\u89c6\u9891\u6d41\u3001\u6811\u8393\u6d3e4B\u548c\u57fa\u4e8eYOLO\u7684\u68c0\u6d4b\u6a21\u578b\uff0c\u5e76\u901a\u8fc7NSGA-II\u591a\u76ee\u6807\u4f18\u5316\u7b97\u6cd5\u52a8\u6001\u8c03\u6574\u4fe1\u53f7\u706f\u65f6\u957f\uff0c\u5728\u8fbe\u5361\u5e02\u5b9e\u9645\u8def\u53e3\u6d4b\u8bd5\u4e2d\u663e\u8457\u63d0\u5347\u4e86\u4ea4\u901a\u6548\u7387\u3002", "motivation": "\u53d1\u5c55\u4e2d\u56fd\u5bb6\u5982\u5b5f\u52a0\u62c9\u56fd\u8fbe\u5361\u5e02\u7684\u4ea4\u901a\u5177\u6709\u975e\u8f66\u9053\u5316\u3001\u9ad8\u5ea6\u5f02\u6784\u7684\u7279\u70b9\uff0c\u4f20\u7edf\u667a\u80fd\u4ea4\u901a\u4fe1\u53f7\u7cfb\u7edf\u4e3b\u8981\u9762\u5411\u53d1\u8fbe\u56fd\u5bb6\u7684\u7ed3\u6784\u5316\u4ea4\u901a\uff0c\u96be\u4ee5\u9002\u7528\uff0c\u56e0\u6b64\u4e9f\u9700\u4e00\u79cd\u5951\u5408\u672c\u5730\u4ea4\u901a\u7279\u5f81\u7684\u667a\u80fd\u4fe1\u53f7\u63a7\u5236\u65b9\u6848\u3002", "method": "\u7cfb\u7edf\u91c7\u7528RTSP\u89c6\u9891\u6d41\u8f93\u5165\uff0c\u5229\u7528\u8d44\u6e90\u53d7\u9650\u7684\u6811\u8393\u6d3e4B\u5e73\u53f0\u8fd0\u884c\u57fa\u4e8eYOLO\u7684\u76ee\u6807\u68c0\u6d4b\u6a21\u578b\uff08\u5728NHT-1071\u6570\u636e\u96c6\u4e0a\u8bad\u7ec3\uff09\uff0c\u8bc6\u522b\u5e76\u5206\u7c7b\u5f02\u6784\u4ea4\u901a\u53c2\u4e0e\u8005\uff1b\u968f\u540e\u4f7f\u7528NSGA-II\u591a\u76ee\u6807\u4f18\u5316\u7b97\u6cd5\uff0c\u4ee5\u6700\u5c0f\u5316\u7b49\u5f85\u65f6\u95f4\u548c\u6700\u5927\u5316\u8f66\u8f86\u901a\u884c\u91cf\u4e3a\u76ee\u6807\uff0c\u751f\u6210\u4f18\u5316\u7684\u4fe1\u53f7\u914d\u65f6\u65b9\u6848\u3002", "result": "\u5728\u8fbe\u5361Palashi\u4e94\u8def\u4ea4\u53c9\u53e3\u7684\u5b9e\u9645\u6d4b\u8bd5\u8868\u660e\uff0c\u6240\u63d0\u7cfb\u7edf\u80fd\u663e\u8457\u6539\u5584\u6b64\u7c7b\u590d\u6742\u4ea4\u901a\u73af\u5883\u4e0b\u7684\u7ba1\u7406\u6548\u7387\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u53d1\u5c55\u4e2d\u56fd\u5bb6\u590d\u6742\u4ea4\u901a\u573a\u666f\u63d0\u4f9b\u4e86\u53ef\u884c\u3001\u4f4e\u6210\u672c\u4e14\u9ad8\u6548\u7684\u667a\u80fd\u4ea4\u901a\u4fe1\u53f7\u89e3\u51b3\u65b9\u6848\uff0c\u5177\u6709\u826f\u597d\u7684\u63a8\u5e7f\u6f5c\u529b\u3002"}}
{"id": "2510.17430", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.17430", "abs": "https://arxiv.org/abs/2510.17430", "authors": ["Kuniaki Kudo", "Sherine Devi"], "title": "Scalable CI/CD for Legacy Modernization: An Industrial Experience Addressing Internal Challenges Related to the 2025 Japan Cliff", "comment": null, "summary": "We have developed a Scalable CI/CD Pipeline to address internal challenges\nrelated to Japan 2025 cliff problem, a critical issue where the mass end of\nservice life of legacy core IT systems threatens to significantly increase the\nmaintenance cost and black box nature of these system also leads to difficult\nupdate moreover replace, which leads to lack of progress in Digital\nTransformation (DX). If not addressed, Japan could potentially lose up to 12\ntrillion yen per year after 2025, which is 3 times more than the cost in\nprevious years. Asahi also faced the same internal challenges regarding legacy\nsystem, where manual maintenance workflows and limited QA environment have left\ncritical systems outdated and difficult to update. Middleware and OS version\nhave remained unchanged for years, leading to now its nearing end of service\nlife which require huge maintenance cost and effort to continue its operation.\nTo address this problem, we have developed and implemented a Scalable CI/CD\nPipeline where isolated development environments can be created and deleted\ndynamically and is scalable as needed. This Scalable CI/CD Pipeline incorporate\nGitHub for source code control and branching, Jenkins for pipeline automation,\nAmazon Web Services for scalable environment, and Docker for environment\ncontainerization. This paper presents the design and architecture of the\nScalable CI/CD Pipeline, with the implementation along with some use cases.\nThrough Scalable CI/CD, developers can freely and safely test maintenance\nprocedures and do experiments with new technology in their own environment,\nreducing maintenance cost and drive Digital Transformation (DX).\n  key words: 2025 Japan Cliff, Scalable CI/CD, DevOps, Legacy IT Modernization.", "AI": {"tldr": "\u672c\u6587\u9488\u5bf9\u65e5\u672c2025\u5e74\u201cIT\u60ac\u5d16\u201d\u95ee\u9898\uff0c\u63d0\u51fa\u5e76\u5b9e\u73b0\u4e86\u4e00\u79cd\u53ef\u6269\u5c55\u7684CI/CD\u6d41\u6c34\u7ebf\uff0c\u901a\u8fc7\u52a8\u6001\u521b\u5efa\u9694\u79bb\u5f00\u53d1\u73af\u5883\uff0c\u7ed3\u5408GitHub\u3001Jenkins\u3001AWS\u548cDocker\u7b49\u6280\u672f\uff0c\u6709\u6548\u964d\u4f4e\u8001\u65e7\u7cfb\u7edf\u7ef4\u62a4\u6210\u672c\u5e76\u63a8\u52a8\u6570\u5b57\u5316\u8f6c\u578b\u3002", "motivation": "\u65e5\u672c\u5927\u91cf\u6838\u5fc3IT\u7cfb\u7edf\u5373\u5c06\u57282025\u5e74\u8fbe\u5230\u670d\u52a1\u5bff\u547d\u7ec8\u70b9\uff0c\u5bfc\u81f4\u7ef4\u62a4\u6210\u672c\u6fc0\u589e\u3001\u7cfb\u7edf\u96be\u4ee5\u66f4\u65b0\u6216\u66ff\u6362\uff0c\u963b\u788d\u6570\u5b57\u5316\u8f6c\u578b\uff1bAsahi\u516c\u53f8\u5185\u90e8\u4e5f\u9762\u4e34\u7c7b\u4f3c\u95ee\u9898\uff0c\u5305\u62ec\u624b\u52a8\u7ef4\u62a4\u6d41\u7a0b\u3001\u7f3a\u4e4fQA\u73af\u5883\u53ca\u4e2d\u95f4\u4ef6/\u64cd\u4f5c\u7cfb\u7edf\u957f\u671f\u672a\u66f4\u65b0\u3002", "method": "\u6784\u5efa\u5e76\u5b9e\u65bd\u4e00\u79cd\u53ef\u6269\u5c55\u7684CI/CD\u6d41\u6c34\u7ebf\uff0c\u5229\u7528GitHub\u8fdb\u884c\u4ee3\u7801\u7ba1\u7406\u3001Jenkins\u5b9e\u73b0\u81ea\u52a8\u5316\u3001AWS\u63d0\u4f9b\u53ef\u6269\u5c55\u57fa\u7840\u8bbe\u65bd\u3001Docker\u5b9e\u73b0\u73af\u5883\u5bb9\u5668\u5316\uff0c\u652f\u6301\u52a8\u6001\u521b\u5efa\u548c\u9500\u6bc1\u9694\u79bb\u7684\u5f00\u53d1\u6d4b\u8bd5\u73af\u5883\u3002", "result": "\u5f00\u53d1\u8005\u53ef\u5728\u72ec\u7acb\u73af\u5883\u4e2d\u5b89\u5168\u5730\u6d4b\u8bd5\u7ef4\u62a4\u6d41\u7a0b\u548c\u5c1d\u8bd5\u65b0\u6280\u672f\uff0c\u663e\u8457\u964d\u4f4e\u7ef4\u62a4\u6210\u672c\uff0c\u5e76\u6709\u6548\u63a8\u52a8\u4f01\u4e1a\u6570\u5b57\u5316\u8f6c\u578b\uff08DX\uff09\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u53ef\u6269\u5c55CI/CD\u6d41\u6c34\u7ebf\u4e3a\u5e94\u5bf92025\u5e74\u65e5\u672cIT\u60ac\u5d16\u95ee\u9898\u63d0\u4f9b\u4e86\u4e00\u79cd\u53ef\u884c\u4e14\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u6709\u52a9\u4e8e\u8001\u65e7IT\u7cfb\u7edf\u7684\u73b0\u4ee3\u5316\u548c\u6301\u7eed\u6f14\u8fdb\u3002"}}
{"id": "2510.17251", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2510.17251", "abs": "https://arxiv.org/abs/2510.17251", "authors": ["Chengxi Li", "Yang Sun", "Lei Chen", "Yiwen Wang", "Mingxuan Yuan", "Evangeline F. Y. Young"], "title": "SmaRTLy: RTL Optimization with Logic Inferencing and Structural Rebuilding", "comment": null, "summary": "This paper proposes smaRTLy: a new optimization technique for multiplexers in\nRegister-Transfer Level (RTL) logic synthesis. Multiplexer trees are very\ncommon in RTL designs, and traditional tools like Yosys optimize them by\ntraversing the tree and monitoring control port values. However, this method\ndoes not fully exploit the intrinsic logical relationships among signals or the\npotential for structural optimization. To address these limitations, we develop\ninnovative strategies to remove redundant multiplexer trees and restructure the\nremaining ones, significantly reducing the overall gate count. We evaluate\nsmaRTLy on the IWLS-2005 and RISC-V benchmarks, achieving an additional 8.95%\nreduction in AIG area compared to Yosys. We also evaluate smaRTLy on an\nindustrial benchmark in the scale of millions of gates, results show that\nsmaRTLy can remove 47.2% more AIG area than Yosys. These results demonstrate\nthe effectiveness of our logic inferencing and structural rebuilding techniques\nin enhancing the RTL optimization process, leading to more efficient hardware\ndesigns.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3asmaRTLy\u7684\u65b0\u578b\u5bc4\u5b58\u5668\u4f20\u8f93\u7ea7\uff08RTL\uff09\u591a\u8def\u590d\u7528\u5668\u4f18\u5316\u6280\u672f\uff0c\u901a\u8fc7\u903b\u8f91\u63a8\u7406\u548c\u7ed3\u6784\u91cd\u6784\u663e\u8457\u51cf\u5c11\u95e8\u6570\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u4f18\u4e8e\u4f20\u7edf\u5de5\u5177Yosys\u3002", "motivation": "\u4f20\u7edfRTL\u7efc\u5408\u5de5\u5177\uff08\u5982Yosys\uff09\u5728\u4f18\u5316\u591a\u8def\u590d\u7528\u5668\u6811\u65f6\u4ec5\u901a\u8fc7\u904d\u5386\u6811\u5e76\u76d1\u63a7\u63a7\u5236\u7aef\u53e3\u503c\uff0c\u672a\u80fd\u5145\u5206\u5229\u7528\u4fe1\u53f7\u95f4\u7684\u5185\u5728\u903b\u8f91\u5173\u7cfb\u548c\u7ed3\u6784\u4f18\u5316\u6f5c\u529b\u3002", "method": "\u5f00\u53d1\u4e86\u521b\u65b0\u7b56\u7565\uff0c\u7528\u4e8e\u6d88\u9664\u5197\u4f59\u7684\u591a\u8def\u590d\u7528\u5668\u6811\u5e76\u91cd\u6784\u5269\u4f59\u7ed3\u6784\uff0c\u4ece\u800c\u51cf\u5c11\u6574\u4f53\u95e8\u6570\u3002", "result": "\u5728IWLS-2005\u548cRISC-V\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u76f8\u6bd4Yosys\u989d\u5916\u51cf\u5c11\u4e868.95%\u7684AIG\u9762\u79ef\uff1b\u5728\u767e\u4e07\u95e8\u7ea7\u5de5\u4e1a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u6bd4Yosys\u591a\u51cf\u5c11\u4e8647.2%\u7684AIG\u9762\u79ef\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u903b\u8f91\u63a8\u7406\u4e0e\u7ed3\u6784\u91cd\u5efa\u6280\u672f\u80fd\u6709\u6548\u63d0\u5347RTL\u4f18\u5316\u6548\u679c\uff0c\u5b9e\u73b0\u66f4\u9ad8\u6548\u7684\u786c\u4ef6\u8bbe\u8ba1\u3002"}}
