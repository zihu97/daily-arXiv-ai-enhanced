<div id=toc></div>

# Table of Contents

- [cs.AR](#cs.AR) [Total: 1]
- [cs.OS](#cs.OS) [Total: 2]
- [cs.NI](#cs.NI) [Total: 5]
- [cs.DC](#cs.DC) [Total: 5]
- [cs.SE](#cs.SE) [Total: 17]


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [1] [HERO-Sign: Hierarchical Tuning and Efficient Compiler-Time GPU Optimizations for SPHINCS+ Signature Generation](https://arxiv.org/abs/2512.23969)
*Yaoyun Zhou,Qian Wang*

Main category: cs.AR

TL;DR: HERO Sign通过分层调优和编译器优化显著加速了SPHINCS+在GPU上的签名生成。


<details>
  <summary>Details</summary>
Motivation: 现有GPU优化未能充分挖掘SPHINCS+的并行潜力或缺乏细粒度编译定制。

Method: 提出Tree Fusion策略与自适应编译，结合任务图减少核启动开销。

Result: 在RTX 4090等GPU上实现1.24-3.13倍吞吐提升，核启动延迟降低两个数量级。

Conclusion: HERO Sign高效利用GPU架构特性，为后量子签名提供实用化加速方案。

Abstract: SPHINCS+ is a stateless hash-based signature scheme that provides strong post quantum security, but its signature generation is slow due to intensive hash computations. GPUs offer massive parallelism that can potentially accelerate SPHINCS+ signatures. However, existing GPU-based optimizations either fail to fully exploit the inherent parallelism of SPHINCS+'s Merkle tree structure or lack fine-grained, compiler-level customization across its diverse computational kernels. This paper proposes HERO Sign, a GPU-accelerated SPHINCS+ implementation that adopts hierarchical tuning and efficient compiler time optimizations. HERO Sign reexamines the parallelization opportunities enabled by data independence across SPHINCS+ components, including FORS, MSS, and WOTS+. It introduces a Tree Fusion strategy for FORS, which contains a large number of independent branches. The fusion strategy is guided by an automated Tree Tuning search algorithm that adapts fusion schemes to different GPU architectures. To further improve performance, HERO Sign employs an adaptive compilation strategy that accounts for the varying effectiveness of compiler optimizations across SPHINCS+ kernels such as FORS Sign, TREE Sign, and WOTS+ Sign. During compilation, the strategy automatically selects between PTX and native code paths to maximize efficiency. For batched signature generation, HERO Sign optimizes kernel-level overlapping using a task graph-based construction to reduce multi-stream idle time and kernel launch overhead. Experimental results show that, compared to state of the art GPU implementations, HERO Sign achieves throughput improvements of 1.28-3.13, 1.28-2.92, and 1.24-2.60 under the SPHINCS+ 128f, 192f, and 256f parameter sets on RTX 4090. Similar gains are observed on A100, H100, and GTX 2080, along with a two orders of magnitude reduction in kernel launch latency.

</details>


<div id='cs.OS'></div>

# cs.OS [[Back]](#toc)

### [2] [MSched: GPU Multitasking via Proactive Memory Scheduling](https://arxiv.org/abs/2512.24637)
*Weihang Shen,Yinqiu Chen,Rong Chen,Haibo Chen*

Main category: cs.OS

TL;DR: MSched通过预测GPU工作集并优化页面迁移，显著提升内存过载场景下的性能。


<details>
  <summary>Details</summary>
Motivation: HBM容量限制导致GPU任务性能下降，传统按需分页因工作集大和局部性差造成严重延迟。

Method: 提出基于内核参数预测工作集的模板方法，并协同任务调度与内存管理实现全局最优页面布局。

Result: 在科学计算、深度学习和大语言模型负载中，性能分别提升11.05倍和57.88倍。

Conclusion: MSched有效缓解GPU内存瓶颈，为大规模任务提供高效内存管理方案。

Abstract: The limited HBM capacity has become the primary bottleneck for hosting an increasing number of larger-scale GPU tasks. While demand paging extends capacity via host DRAM, it incurs up to 78x slowdown due to the massive working sets and poor locality of GPU workloads. We observe, however, that GPU memory access patterns are inherently predictable via kernel launch arguments and their asynchronous execution nature. Leveraging this, we propose MSched, an OS-level scheduler that extends GPU context switching to include proactive working set preparation, thereby coalescing fragmented, eventual, and expensive page faults into a single efficient migration. MSched employs a template-based approach to predict working sets with near-perfect accuracy and proposes a co-design between task scheduler and memory manager to enforce a globally optimal page placement policy. Evaluation demonstrates that MSched outperforms demand paging by up to 11.05x for scientific and deep learning workloads, and 57.88x for LLM under memory oversubscription.

</details>


### [3] [Vulcan: Instance-Optimal Systems Heuristics Through LLM-Driven Search](https://arxiv.org/abs/2512.25065)
*Rohit Dwivedula,Divyanshu Saxena,Sujay Yadalam,Daehyeok Kim,Aditya Akella*

Main category: cs.OS

TL;DR: Vulcan利用大语言模型自动生成针对特定工作负载和硬件优化的资源管理启发式算法，显著超越人工设计的算法性能。


<details>
  <summary>Details</summary>
Motivation: 传统资源管理依赖手工设计的启发式方法，开发成本高且难以适应不断变化的硬件和工作负载环境。

Method: 通过将策略与机制分离，Vulcan使用进化搜索在LLM生成的代码中寻找高性能策略，并采用任务无关接口简化生成过程。

Result: 在缓存淘汰和内存分层任务中，Vulcan生成的启发式算法分别比现有最优人工算法提升69%和7.9%性能。

Conclusion: Vulcan展示了利用LLM自动合成实例最优启发式算法的可行性与优越性，为系统资源管理提供新范式。

Abstract: Resource-management tasks in modern operating and distributed systems continue to rely primarily on hand-designed heuristics for tasks such as scheduling, caching, or active queue management. Designing performant heuristics is an expensive, time-consuming process that we are forced to continuously go through due to the constant flux of hardware, workloads and environments.
  We propose a new alternative: synthesizing instance-optimal heuristics -- specialized for the exact workloads and hardware where they will be deployed -- using code-generating large language models (LLMs). To make this synthesis tractable, Vulcan separates policy and mechanism through LLM-friendly, task-agnostic interfaces. With these interfaces, users specify the inputs and objectives of their desired policy, while Vulcan searches for performant policies via evolutionary search over LLM-generated code. This interface is expressive enough to capture a wide range of system policies, yet sufficiently constrained to allow even small, inexpensive LLMs to generate correct and executable code.
  We use Vulcan to synthesize performant heuristics for cache eviction and memory tiering, and find that these heuristics outperform all human-designed state-of-the-art algorithms by upto 69% and 7.9% in performance for each of these tasks respectively.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [4] [Road Rules for Radio: Why Your Wi-Fi Got Better](https://arxiv.org/abs/2512.23901)
*Bradley Fang,Michael Roger*

Main category: cs.NI

TL;DR: 本文通过文献综述和公路类比，系统梳理WiFi关键技术演进，聚焦七大核心问题，并展望WiFi 8标准对可靠性的强化。


<details>
  <summary>Details</summary>
Motivation: 厘清近年繁杂的WiFi技术进展，帮助读者全面理解其发展脉络与未来方向。

Method: 围绕带宽、续航、碰撞、干扰等七个维度展开文献综述，辅以公路类比增强可读性，并引用既有研究支撑技术论点。

Result: 清晰呈现各问题的历史解决方案与现存局限，同时阐释基于IEEE 802.11bn的WiFi 8如何转向可靠性优先。

Conclusion: WiFi 8标志着从追求速率向保障可靠性的战略转变，本文为非专业读者提供了系统且易懂的技术演进图景。

Abstract: WiFi allows for the connection of devices and people around the globe. It has proven to be a monumental and revolutionary tool that keeps the world connected. However, recent WiFi advancements are numerous and at times confusing. WiFi has grown significantly over the years, yet few understand the scope and scale of WiFi progression as a whole. This paper tackles that problem, providing a broad literature review on the advancements of key WiFi features to date. This paper will center on seven key areas of focus: (1) bandwidth, (2) battery life, (3) traffic collisions, (4) interference, (5) data-intensive transmissions, (6) numerous devices, and (7) peak throughput/modulation. Each section will focus on WiFi's problems, how those problems were fixed, as well as the limitations of existing solutions. Moreover, the paper explains the role of new unreleased technologies in these seven areas. This includes exploring the upcoming WiFi 8 standard based on the IEEE 802.11bn "Ultra High Reliability" (UHR) specification and how it builds upon current specifications. Compared to previous specifications, WiFi 8 marks a stronger and more significant shift toward prioritizing reliability over pure data rates. Beyond a sole literature review, this paper uses a novel analogy. A road/highway analogy will be integrated throughout the paper to facilitate understanding of networking mechanisms. This paper is approachable and is written such that someone with very little WiFi knowledge should come away with a strong understanding of WiFi. As is typical of literature review papers, technical claims will be grounded in prior work.

</details>


### [5] [Beyond Dedicated-Active: A General Reliability Provisioning Framework for SFC Placement in Fog Computing](https://arxiv.org/abs/2512.24049)
*Negin Doostar,Mohammad Reza Heidarpour,Amir Khorsandi*

Main category: cs.NI

TL;DR: 本文提出基于可靠性理论的SFC部署框架，结合四种冗余策略，在异构雾服务器上优化延迟与成本，同时满足可靠性约束。


<details>
  <summary>Details</summary>
Motivation: 物联网设备激增导致传统云架构压力增大，雾计算虽能降低延迟，但资源异构性影响可靠性，需智能冗余与部署方案。

Method: 构建整数非线性规划模型，采用遗传算法求解，比较共享/专用、主动/备用四种冗余策略。

Result: 仿真表明，共享-备用冗余策略相比传统专用-主动方案性能提升最高达84%。

Conclusion: 所提框架能有效平衡雾环境中SFC部署的延迟、成本与可靠性需求，共享-备用策略表现最优。

Abstract: The explosive growth of Internet of Things (IoT) devices has strained traditional cloud infrastructures, highlighting the need for low-latency and energy-efficient alternatives. Fog computing addresses this by placing computation near the network edge. However, limited and heterogeneous fog resources pose reliability challenges, especially for mission-critical applications. On the other hand, to improve flexibility, applications are deployed as Service Function Chains (SFCs), where each function runs as a Virtual Network Function (VNF). While scalable, this approach is more failure-prone than monolithic deployments, necessitating intelligent redundancy and placement strategies. This paper addresses the reliability-aware SFC placement problem over heterogeneous fog servers through the lens of reliability theory. We explore four redundancy strategies, combining shared vs. dedicated and active vs. standby modes, and propose a general framework to minimize latency and cost while meeting reliability and deadline constraints. The problem is formulated as an Integer Non-Linear Program (INLP), and two genetic algorithm (GA)-based solutions are developed. Simulation results show that shared-standby redundancy outperforms the conventional dedicated-active approach by up to 84%.

</details>


### [6] [Privacy-Preserving Semantic Communications via Multi-Task Learning and Adversarial Perturbations](https://arxiv.org/abs/2512.24452)
*Yalin E. Sagduyu,Tugba Erpek,Aylin Yener,Sennur Ulukus*

Main category: cs.NI

TL;DR: 提出一种基于深度学习的语义通信框架，通过最小-最大优化和对抗扰动层，在保障合法接收方性能的同时有效降低窃听者的语义信息泄露。


<details>
  <summary>Details</summary>
Motivation: 解决语义通信中敏感信息可能被窃听者获取的问题，提升无线系统安全性。

Method: 采用深度学习架构，结合最小-最大优化策略与对抗扰动层，训练合法收发器与窃听者模型，以平衡任务性能与隐私保护。

Result: 在MNIST和CIFAR-10数据集上验证，该方法显著降低窃听者推理能力，同时不损害合法接收方的语义准确性和重建质量。

Conclusion: 该框架支持端到端可调隐私保护，适用于现实无线环境中对抗自适应攻击者的语义通信系统设计。

Abstract: Semantic communications conveys task-relevant meaning rather than focusing solely on message reconstruction, improving bandwidth efficiency and robustness for next-generation wireless systems. However, learned semantic representations can still leak sensitive information to unintended receivers (eavesdroppers). This paper presents a deep learning-based semantic communication framework that jointly supports multiple receiver tasks while explicitly limiting semantic leakage to an eavesdropper. The legitimate link employs a learned encoder at the transmitter, while the receiver trains decoders for semantic inference and data reconstruction. The security problem is formulated via an iterative min-max optimization in which an eavesdropper is trained to improve its semantic inference, while the legitimate transmitter-receiver pair is trained to preserve task performance while reducing the eavesdropper's success. We also introduce an auxiliary layer that superimposes a cooperative, adversarially crafted perturbation on the transmitted waveform to degrade semantic leakage to an eavesdropper. Performance is evaluated over Rayleigh fading channels with additive white Gaussian noise using MNIST and CIFAR-10 datasets. Semantic accuracy and reconstruction quality improve with increasing latent dimension, while the min-max mechanism reduces the eavesdropper's inference performance significantly without degrading the legitimate receiver. The perturbation layer is successful in reducing semantic leakage even when the legitimate link is trained only for its own task. This comprehensive framework motivates semantic communication designs with tunable, end-to-end privacy against adaptive adversaries in realistic wireless settings.

</details>


### [7] [Hierarchical Online Optimization Approach for IRS-enabled Low-altitude MEC in Vehicular Networks](https://arxiv.org/abs/2512.24659)
*Yixian Wang,Geng Sun,Zemin Sun,Jiacheng Wang,Changyuan Zhao,Daxin Tian,Dusit Niyato,Shiwen Mao*

Main category: cs.NI

TL;DR: 本文提出了一种智能反射面辅助的低空多接入边缘计算架构，并设计分层在线优化算法以降低任务延迟与能耗。


<details>
  <summary>Details</summary>
Motivation: 提升受遮挡环境下的空地通信质量，优化任务卸载与资源分配效率。

Method: 构建多目标优化问题，采用Stackelberg博弈建模，结合匹配机制与GDMTD3强化学习算法求解。

Result: 相比现有最优基准方法，平均任务延迟降低2.5%，能耗降低3.1%，且具备良好收敛性与鲁棒性。

Conclusion: 所提架构与算法在动态环境中高效稳定，显著提升边缘计算性能。

Abstract: In this paper, we propose an intelligent reflecting surface (IRS)-enabled low-altitude multi-access edge computing (MEC) architecture, where an aerial MEC server cooperates with a terrestrial MEC server to provide computing services, while hybrid IRSs (i.e., building-installed and UAV-carried IRSs) are deployed to enhance the air-ground connectivity under blockage. Based on this architecture, we formulate a multi-objective optimization problem (MOOP) to minimize the task completion delay and energy consumption by jointly optimizing task offloading, UAV trajectory control, IRS phase-shift configuration, and computation resource allocation. The considered problem is NP-hard, and thus we propose a hierarchical online optimization approach (HOOA) to efficiently solve the problem. Specifically, we reformulate the MOOP as a Stackelberg game, where MEC servers collectively act as the leader to determine the system-level decisions, while the vehicles act as followers to make individual decisions. At the follower level, we present a many-to-one matching mechanism to generate feasible discrete decisions. At the leader level, we propose a generative diffusion model-enhanced twin delayed deep deterministic policy gradient (GDMTD3) algorithm integrated with a Karush-Kuhn-Tucker (KKT)-based method, which is a deep reinforcement learning (DRL)-based approach, to determine the continuous decisions. Simulation results demonstrate that the proposed HOOA achieves significant improvements, which reduces average task completion delay by 2.5% and average energy consumption by 3.1% compared with the best-performing benchmark approach and state-of-the-art DRL algorithm, respectively. Moreover, the proposed HOOA exhibits superior convergence stability while maintaining strong robustness and scalability in dynamic environments.

</details>


### [8] [Analyzing Communication Predictability in LLM Training](https://arxiv.org/abs/2512.24750)
*Wenxue Li,Xiangzhou Liu,Yuxuan Li,Yilun Jin,Zhenghang Ren,Xudong Liao,Han Tian,Bo Ren,Zhizhen Zhong,Guyue Liu,Ying Zhang,Kai Chen*

Main category: cs.NI

TL;DR: 本文系统性地研究了分布式训练中的通信可预测性，提出分析模型并开发ConfigTuner工具优化LLM训练性能。


<details>
  <summary>Details</summary>
Motivation: 现有研究缺乏对通信可预测性的系统理解，仅依赖在线剖析进行运行时优化。

Method: 分析典型LLM的通信流量模式与开销影响因素，建立通信开销解析模型，并基于此构建配置调优工具ConfigTuner。

Result: ConfigTuner相较Megatron-LM提升最高1.36倍吞吐量，相较Alpa生成相同配置但搜索复杂度显著降低。

Conclusion: 系统建模通信可预测性可有效指导分布式训练配置优化，显著提升训练效率。

Abstract: Effective communication is essential in distributed training, with predictability being one of its most significant characteristics. However, existing studies primarily focus on exploiting predictability through online profiling for runtime optimization, without a systematic understanding of it. In this work, we aim to systematically formulate communication predictability in distributed training, particularly in Large Language Models (LLMs) that utilize hybrid parallelism. Our analysis focuses on both traffic patterns and communication overhead. Specifically, we investigate predictable traffic patterns in typical LLMs and evaluate how various factors influence GPU utilization and effective bandwidth (two critical variables affecting communication overhead). Furthermore, we develop an analytical formulation to estimate communication overhead in LLM training, which is validated with high accuracy against empirical data. Leveraging this formulation, we propose a configuration tuning tool, ConfigTuner, to optimize training performance. Compared to Megatron-LM, the training configurations optimized by ConfigTuner demonstrate up to a 1.36$\times$ increase in throughput. Compared to Alpa, ConfigTuner generates the same configuration suggestion while significantly reducing the search complexity.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [9] [Squeezing Edge Performance: A Sensitivity-Aware Container Management for Heterogeneous Tasks](https://arxiv.org/abs/2512.23952)
*Yongmin Zhang,Pengyu Huang,Mingyi Dong,Jing Yao*

Main category: cs.DC

TL;DR: 本文提出了一种基于容器的资源管理框架CRMS，通过非线性拟合建模与凸优化，在单边缘节点上联合优化延迟与能耗，显著优于启发式方法。


<details>
  <summary>Details</summary>
Motivation: 边缘计算中任务异构性与资源受限导致调度困难，需高效资源管理方案支持多样化应用。

Method: 通过实验构建CPU/内存分配与延迟关系的非线性模型，建立MINLP问题并分解为凸子问题，采用两阶段容器资源管理方案求解。

Result: 仿真显示CRMS降低延迟超14%，提升能效，具备多项式时间复杂度和准动态执行能力。

Conclusion: CRMS为异构边缘环境提供实用、可扩展的资源优化方案，有效应对动态负载挑战。

Abstract: Edge computing enables latency-critical applications to process data close to end devices, yet task heterogeneity and limited resources pose significant challenges to efficient orchestration. This paper presents a measurement-driven, container-based resource management framework for intra-node optimization on a single edge server hosting multiple heterogeneous applications. Extensive profiling experiments are conducted to derive a nonlinear fitting model that characterizes the relationship among CPU/memory allocations and processing latency across diverse workloads, enabling reliable estimation of performance under varying configurations and providing quantitative support for subsequent optimization. Using this model and a queueing-based delay formulation, we formulate a mixed-integer nonlinear programming (MINLP) problem to jointly minimize system latency and power consumption, which is shown to be NP-hard. The problem is decomposed into tractable convex subproblems and solved through a two-stage container-based resource management scheme (CRMS) combining convex optimization and greedy refinement. The proposed scheme achieves polynomial-time complexity and supports quasi-dynamic execution under global resource constraints. Simulation results demonstrate that CRMS reduces latency by over 14\% and improves energy efficiency compared with heuristic and search-based baselines, offering a practical and scalable solution for heterogeneous edge environments with dynamic workload characteristics.

</details>


### [10] [PackKV: Reducing KV Cache Memory Footprint through LLM-Aware Lossy Compression](https://arxiv.org/abs/2512.24449)
*Bo Jiang,Taolue Yang,Youyuan Liu,Xubin He,Sheng Di,Sian Jin*

Main category: cs.DC

TL;DR: PackKV是一种高效的KV缓存管理框架，通过定制压缩算法和系统架构协同设计，在保持高计算效率的同时显著降低内存占用并提升吞吐量。


<details>
  <summary>Details</summary>
Motivation: 长上下文推理中KV缓存内存需求巨大，限制了Transformer大模型的实际应用。

Method: 提出PackKV框架，结合专为KV缓存数据设计的有损压缩技术与系统架构协同优化。

Result: 相比现有量化方法，K缓存内存减少率提高153.2%，V缓存提高179.6%；在A100和RTX Pro 6000上，K/V吞吐量分别平均提升75.7%和171.7%。

Conclusion: PackKV有效缓解长序列生成中的内存瓶颈，同时大幅提升执行效率，适用于延迟敏感和吞吐量敏感场景。

Abstract: Transformer-based large language models (LLMs) have demonstrated remarkable potential across a wide range of practical applications. However, long-context inference remains a significant challenge due to the substantial memory requirements of the key-value (KV) cache, which can scale to several gigabytes as sequence length and batch size increase. In this paper, we present \textbf{PackKV}, a generic and efficient KV cache management framework optimized for long-context generation. %, which synergistically supports both latency-critical and throughput-critical inference scenarios. PackKV introduces novel lossy compression techniques specifically tailored to the characteristics of KV cache data, featuring a careful co-design of compression algorithms and system architecture. Our approach is compatible with the dynamically growing nature of the KV cache while preserving high computational efficiency. Experimental results show that, under the same and minimum accuracy drop as state-of-the-art quantization methods, PackKV achieves, on average, \textbf{153.2}\% higher memory reduction rate for the K cache and \textbf{179.6}\% for the V cache. Furthermore, PackKV delivers extremely high execution throughput, effectively eliminating decompression overhead and accelerating the matrix-vector multiplication operation. Specifically, PackKV achieves an average throughput improvement of \textbf{75.7}\% for K and \textbf{171.7}\% for V across A100 and RTX Pro 6000 GPUs, compared to cuBLAS matrix-vector multiplication kernels, while demanding less GPU memory bandwidth. Code available on https://github.com/BoJiang03/PackKV

</details>


### [11] [Understanding LLM Checkpoint/Restore I/O Strategies and Patterns](https://arxiv.org/abs/2512.24511)
*Mikaila J. Gossman,Avinash Maurya,Bogdan Nicolae,Jon C. Calhoun*

Main category: cs.DC

TL;DR: 本文研究了在大规模语言模型训练中使用liburing优化检查点I/O性能，提出聚合与对齐策略，显著提升吞吐量。


<details>
  <summary>Details</summary>
Motivation: 随着LLM规模扩大，检查点/恢复成为关键操作，但其I/O性能受存储栈瓶颈限制，亟需优化。

Method: 通过微基准测试分析liburing在缓冲与直接I/O下的聚合、对齐与I/O合并效果。

Result: 相比现有引擎，本方法写入吞吐量最高提升3.9倍（对比DataStates-LLM）和7.6倍（对比TorchSnapshot）。

Conclusion: 现代文件系统下，聚合与I/O合并策略对提升LLM检查点性能至关重要。

Abstract: As LLMs and foundation models scale, checkpoint/restore has become a critical pattern for training and inference. With 3D parallelism (tensor, pipeline, data), checkpointing involves many processes, each managing numerous tensors of varying shapes and sizes, that must be persisted frequently to stable storage (e.g., parallel file systems). This turns checkpoint/restore into a big-data I/O problem characterized by volume, variety, and velocity. The workflow must traverse the full storage stack -- from GPU memory through host memory and local storage to external repositories -- whose tiers differ by orders of magnitude in performance, creating bottlenecks under concurrency even with asynchronous flush/prefetch. Kernel-accelerated I/O libraries such as \texttt{liburing} may mitigate these issues versus POSIX, but their effectiveness for LLM checkpointing remains underexplored. We develop microbenchmarks to quantify trade-offs when using \texttt{liburing}, evaluating how aggregation, alignment, and I/O coalescing interact under buffered and direct I/O. We find that uncoalesced small-buffer operations halve throughput relative to synthetic workloads, while file system-aware aggregation restores bandwidth and reduces metadata overhead. Compared to state-of-the-art LLM checkpointing engines, our approach achieves up to $3.9\times$ higher write throughput than DataStates-LLM and $7.6\times$ higher than TorchSnapshot. These results highlight the need for aggregation and coalescing strategies that align with modern file systems and I/O backends.

</details>


### [12] [Distributed Bilevel Optimization with Dual Pruning for Resource-limited Clients](https://arxiv.org/abs/2512.24667)
*Mingyi Li,Xiao Zhang,Ruisheng Zheng,Hongjian Shi,Yuan Yuan,Xiuzhen Cheng,Dongxiao Yu*

Main category: cs.DC

TL;DR: 提出首个资源自适应分布式双层优化框架，显著提升低资源客户端的计算效率。


<details>
  <summary>Details</summary>
Motivation: 大规模模型发展下，传统双层优化算法无法直接用于低资源客户端，因其计算开销过大。

Method: 设计无二阶超梯度估计器的资源自适应框架，支持客户端根据资源优化子模型。

Result: 理论证明RABO与RAFBO均达到最优收敛速率$O(1/\sqrt{C_x^{\ast}Q})$，实验验证其高效性。

Conclusion: 该框架在保证收敛性的同时，有效降低资源消耗，适用于实际低资源场景。

Abstract: With the development of large-scale models, traditional distributed bilevel optimization algorithms cannot be applied directly in low-resource clients. The key reason lies in the excessive computation involved in optimizing both the lower- and upper-level functions. Thus, we present the first resource-adaptive distributed bilevel optimization framework with a second-order free hypergradient estimator, which allows each client to optimize the submodels adapted to the available resources. Due to the coupled influence of partial outer parameters x and inner parameters y, it's challenging to theoretically analyze the upper bound regarding the globally averaged hypergradient for full model parameters. The error bound of inner parameter also needs to be reformulated since the local partial training. The provable theorems show that both RABO and RAFBO can achieve an asymptotically optimal convergence rate of $O(1/\sqrt{C_x^{\ast}Q})$, which is dominated by the minimum coverage of the outer parameter $C_x^{\ast}$. Extensive experiments on two different tasks demonstrate the effectiveness and computation efficiency of our proposed methods.

</details>


### [13] [AI-Driven Cloud Resource Optimization for Multi-Cluster Environments](https://arxiv.org/abs/2512.24914)
*Vinoth Punniyamoorthy,Akash Kumar Agarwal,Bikesh Kumar,Abhirup Mazumder,Kabilan Kannan,Sumit Saha*

Main category: cs.DC

TL;DR: 本文提出一种基于AI的多集群云系统自适应资源优化框架，通过预测学习与策略感知决策提升资源效率和系统稳定性。


<details>
  <summary>Details</summary>
Motivation: 现有资源管理方法反应迟缓且以单集群为中心，难以应对动态负载下的全局优化需求，导致资源利用率低、适应延迟和运维开销大。

Method: 整合预测学习、策略感知决策与持续反馈机制，利用跨集群遥测数据和历史执行模式动态调整资源分配。

Result: 原型实现表明，相比传统方法，该框架提升了资源效率，加快了负载波动下的稳定速度，并降低了性能波动。

Conclusion: 智能自适应基础设施管理是构建可扩展、高韧性云平台的关键推动力。

Abstract: Modern cloud-native systems increasingly rely on multi-cluster deployments to support scalability, resilience, and geographic distribution. However, existing resource management approaches remain largely reactive and cluster-centric, limiting their ability to optimize system-wide behavior under dynamic workloads. These limitations result in inefficient resource utilization, delayed adaptation, and increased operational overhead across distributed environments. This paper presents an AI-driven framework for adaptive resource optimization in multi-cluster cloud systems. The proposed approach integrates predictive learning, policy-aware decision-making, and continuous feedback to enable proactive and coordinated resource management across clusters. By analyzing cross-cluster telemetry and historical execution patterns, the framework dynamically adjusts resource allocation to balance performance, cost, and reliability objectives. A prototype implementation demonstrates improved resource efficiency, faster stabilization during workload fluctuations, and reduced performance variability compared to conventional reactive approaches. The results highlight the effectiveness of intelligent, self-adaptive infrastructure management as a key enabler for scalable and resilient cloud platforms.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [14] [AgenticTCAD: A LLM-based Multi-Agent Framework for Automated TCAD Code Generation and Device Optimization](https://arxiv.org/abs/2512.23742)
*Guangxi Fan,Tianliang Ma,Xuguang Sun,Xun Wang,Kain Lu Low,Leilai Shao*

Main category: cs.SE

TL;DR: 提出AgenticTCAD框架，通过多智能体与自然语言驱动实现2纳米纳米片场效应管的自动化设计，大幅缩短优化时间。


<details>
  <summary>Details</summary>
Motivation: 解决TCAD仿真领域开源资源稀缺导致语言模型难以生成有效代码的问题，提升器件设计效率。

Method: 构建专家精选开源TCAD数据集并微调专用模型，开发自然语言驱动的多智能体框架AgenticTCAD。

Result: 在2纳米NS-FET设计中，4.2小时内达成IRDS-2024标准，远快于人工7.1天。

Conclusion: AgenticTCAD显著加速先进节点器件设计，验证了自动化与AI协同优化在DTCO中的巨大潜力。

Abstract: With the continued scaling of advanced technology nodes, the design-technology co-optimization (DTCO) paradigm has become increasingly critical, rendering efficient device design and optimization essential. In the domain of TCAD simulation, however, the scarcity of open-source resources hinders language models from generating valid TCAD code. To overcome this limitation, we construct an open-source TCAD dataset curated by experts and fine-tune a domain-specific model for TCAD code generation. Building on this foundation, we propose AgenticTCAD, a natural language - driven multi-agent framework that enables end-to-end automated device design and optimization. Validation on a 2 nm nanosheet FET (NS-FET) design shows that AgenticTCAD achieves the International Roadmap for Devices and Systems (IRDS)-2024 device specifications within 4.2 hours, whereas human experts required 7.1 days with commercial tools.

</details>


### [15] [Hybrid-Code: A Privacy-Preserving, Redundant Multi-Agent Framework for Reliable Local Clinical Coding](https://arxiv.org/abs/2512.23743)
*Yunguo Yu*

Main category: cs.SE

TL;DR: Hybrid-Code 是一种本地部署的神经符号多智能体框架，通过冗余与验证机制确保临床编码自动化在医疗环境中的可靠性与隐私性。


<details>
  <summary>Details</summary>
Motivation: 云端大语言模型存在隐私泄露和延迟问题，不适合院内部署，需构建兼顾可靠性与隐私保护的本地化方案。

Method: 系统包含 Coder 与 Auditor 两个智能体：Coder 使用 BioMistral-7B 进行语义推理，失败时回退至关键词匹配；Auditor 基于知识库验证编码并提供证据支持。

Result: 在 MIMIC-III 数据集上实现零幻觉编码、24.47% 验证率、34.11% 覆盖率，86%+ 模型利用率，75.53% 无效编码被过滤，且患者数据不出院内防火墙。

Conclusion: 在医疗生产系统中，通过冗余机制保障可靠性比单纯追求模型性能更重要，本框架有效克服了AI落地的关键障碍。

Abstract: Clinical coding automation using cloud-based Large Language Models (LLMs) poses privacy risks and latency bottlenecks, rendering them unsuitable for on-premise healthcare deployment. We introduce Hybrid-Code, a hybrid neuro-symbolic multi-agent framework for local clinical coding that ensures production reliability through redundancy and verification. Our system comprises two agents: a Coder that attempts language model-based semantic reasoning using BioMistral-7B but falls back to deterministic keyword matching when model output is unreliable, ensuring pipeline completion; and an Auditor that verifies codes against a 257-code knowledge base and clinical evidence. Evaluating on 1,000 MIMIC-III discharge summaries, we demonstrate no hallucinated codes among accepted outputs within the knowledge base, 24.47% verification rate, and 34.11% coverage (95% CI: 31.2%--37.0%) with 86%+ language model utilization. The Auditor filtered invalid format codes and provided evidence-based quality control (75.53% rejection rate) while ensuring no patient data leaves the hospital firewall. The hybrid architecture -- combining language model semantic understanding (when successful), deterministic fallback (when the model fails), and symbolic verification (always active) -- ensures both reliability and privacy preservation, addressing critical barriers to AI adoption in healthcare. Our key finding is that reliability through redundancy is more valuable than pure model performance in production healthcare systems, where system failures are unacceptable.

</details>


### [16] [DEFT: Differentiable Automatic Test Pattern Generation](https://arxiv.org/abs/2512.23746)
*Wei Li,Yan Zou,Yixin Liang,José Moura,Shawn Blanton*

Main category: cs.SE

TL;DR: DEFT是一种新的ATPG方法，通过将离散问题转化为连续优化任务，显著提升对难检测故障的覆盖率。


<details>
  <summary>Details</summary>
Motivation: 现代集成电路复杂性导致测试模式增长，多数模式针对少量难检测故障，需更有效的ATPG算法。

Method: DEFT采用数学重参数化对齐连续目标与离散语义，结合CUDA内核和梯度归一化实现高效稳定优化。

Result: 在两个工业基准上，DEFT在相同预算下平均提升HTD故障检测率21.1%和48.9%，并支持部分赋值模式生成。

Conclusion: DEFT是现有启发式ATPG的有效补充，具备实用性和扩展潜力。

Abstract: Modern IC complexity drives test pattern growth, with the majority of patterns targeting a small set of hard-to-detect (HTD) faults. This motivates new ATPG algorithms to improve test effectiveness specifically for HTD faults. This paper presents DEFT (Differentiable Automatic Test Pattern Generation), a new ATPG approach that reformulates the discrete ATPG problem as a continuous optimization task. DEFT introduces a mathematically grounded reparameterization that aligns the expected continuous objective with discrete fault-detection semantics, enabling reliable gradient-based pattern generation. To ensure scalability and stability on deep circuit graphs, DEFT integrates a custom CUDA kernel for efficient forward-backward propagation and applies gradient normalization to mitigate vanishing gradients. Compared to a leading commercial tool on two industrial benchmarks, DEFT improves HTD fault detection by 21.1% and 48.9% on average under the same pattern budget and comparable runtime. DEFT also supports practical ATPG settings such as partial assignment pattern generation, producing patterns with 19.3% fewer 0/1 bits while still detecting 35% more faults. These results indicate DEFT is a promising and effective ATPG engine, offering a valuable complement to existing heuristic.

</details>


### [17] [State-of-the-art Small Language Coder Model: Mify-Coder](https://arxiv.org/abs/2512.23747)
*Abhinav Parmar,Abhisek Panigrahi,Abhishek Kumar Dwivedi,Abhishek Bhattacharya,Adarsh Ramachandra,Aditya Choudhary,Aditya Garg,Aditya Raj,Alankrit Bhatt,Alpesh Yadav,Anant Vishnu,Ananthu Pillai,Ankush Kumar,Aryan Patnaik,Aswatha Narayanan S,Avanish Raj Singh,Bhavya Shree Gadda,Brijesh Pankajbhai Kachhadiya,Buggala Jahnavi,Chidurala Nithin Krishna,Chintan Shah,Chunduru Akshaya,Debarshi Banerjee,Debrup Dey,Deepa R.,Deepika B G,Faiz ur Rahman,Gagan Gayari,Gudhi Jagadeesh Kumar Naidu,Gursimar Singh,Harshal Tyagi,Harshini K,James Mani Vathalloor,Jayarama Nettar,Jayashree Gajjam,Joe Walter Sugil George,Kamalakara Sri Krishna Tadepalli,Kamalkumar Rathinasamy,Karan Chaurasia,Karthikeyan S,Kashish Arora,Kaushal Desai,Khushboo Buwade,Kiran Manjrekar,Malikireddy Venkata Sai Likhitha,Manjunath A,Mitali Mahavir Bedmutha,Mohammed Rafee Tarafdar,Nikhil Tiwari,Nikitha K Gigi,Pavan Ravikumar,Pendyala Swarnanjali,Piyush Anand,Prakash Chandrasekar,Prasanna Bhalchandra Gawade,Prasanth Sivan,Preeti Khurana,Priyanshi Babbar,Rajab Ali Mondal,Rajesh Kumar Vissapragada,Rajeshwari Ganesan,Rajeswari Koppisetti,Ramjee R.,Ramkumar Thiruppathisamy,Rani G. S.,S Reka,Samarth Gupta,Sandeep Reddy Kothakota,Sarathy K,Sathyanarayana Sampath Kumar,Saurabh Kumar,Shashank Khasare,Shenbaga Devi Venkatesh Kumar,Shiva Rama Krishna Parvatham,Shoeb Shaikh,Shrishanmathi A,Shubham Pathak,Sree Samhita Koppaka,Sreenivasa Raghavan K S,Sreeram Venkatasubramanian,Suprabha Desai Bojja,Swetha R,Syed Ahmed,Chinmai Harshitha Thota,Tushar Yadav,Veeravelly Kusumitha,V V S S Prasanth Patnaik,Vidya Sri Sesetti,Vijayakeerthi K,Vikram Raj Bakshi,Vinay K K,Vinoth Kumar Loganathan,Vipin Tiwari,Vivek Kumar Shrivastav,V Venkata Sri Datta Charan,Wasim Akhtar Khan*

Main category: cs.SE

TL;DR: Mify-Coder是一个2.5B参数的代码模型，通过高效训练策略在标准编码任务中超越更大模型，支持桌面部署。


<details>
  <summary>Details</summary>
Motivation: 探索小规模模型能否通过高质量数据与训练策略达到前沿大模型的代码能力与安全性。

Method: 基于Mify-2.5B，结合人工精选与智能体生成的合成数据，辅以LLM质量过滤，在单一训练流程中优化CPT-SFT目标与数据混合策略。

Result: 在多个基准测试中表现优于更大基线模型，同时保持高准确率与安全性，量化版本可在普通桌面运行。

Conclusion: 严谨的数据与算力管理可使小型模型在代码生成与智能体工作流中媲美前沿大模型。

Abstract: We present Mify-Coder, a 2.5B-parameter code model trained on 4.2T tokens using a compute-optimal strategy built on the Mify-2.5B foundation model. Mify-Coder achieves comparable accuracy and safety while significantly outperforming much larger baseline models on standard coding and function-calling benchmarks, demonstrating that compact models can match frontier-grade models in code generation and agent-driven workflows. Our training pipeline combines high-quality curated sources with synthetic data generated through agentically designed prompts, refined iteratively using enterprise-grade evaluation datasets. LLM-based quality filtering further enhances data density, enabling frugal yet effective training. Through disciplined exploration of CPT-SFT objectives, data mixtures, and sampling dynamics, we deliver frontier-grade code intelligence within a single continuous training trajectory. Empirical evidence shows that principled data and compute discipline allow smaller models to achieve competitive accuracy, efficiency, and safety compliance. Quantized variants of Mify-Coder enable deployment on standard desktop environments without requiring specialized hardware.

</details>


### [18] [A Magnified View into Heterogeneous-ISA Thread Migration Performance without State Transformation](https://arxiv.org/abs/2512.24530)
*Nikolaos Mavrogeorgis,Christos Vasiladiotis,Pei Mu,Amir Khordadi,Björn Franke,Antonio Barbalace*

Main category: cs.SE

TL;DR: Unifico是一个新型多ISA编译器，通过统一栈布局、ABI和虚拟地址空间，消除异构ISA迁移时的运行时开销，显著优于现有方案。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏支持异构ISA目标的编译工具链，阻碍了该领域的研究进展，尤其需要高效解决迁移时状态转换问题。

Method: 基于LLVM实现Unifico编译器，针对x86-64与ARMv8架构，确保执行期间栈布局一致，避免运行时栈转换，并维护统一ABI与虚拟地址空间。

Result: 在NAS基准测试中，Unifico平均仅引入低于6%（高端）或10%（低端）性能开销，相比Popcorn编译器将二进制体积开销从约200%降至约10%，并完全消除栈转换开销。

Conclusion: Unifico有效解决了异构ISA迁移中的关键性能瓶颈，其设计具备进一步优化潜力，为异构计算研究提供了实用编译基础设施。

Abstract: Heterogeneous-ISA processor designs have attracted considerable research interest. However, unlike their homogeneous-ISA counterparts, explicit software support for bridging ISA heterogeneity is required. The lack of a compilation toolchain ready to support heterogeneous-ISA targets has been a major factor hindering research in this exciting emerging area. For any such compiler, "getting right" the mechanics involved in state transformation upon migration and doing this efficiently is of critical importance. In particular, any runtime conversion of the current program stack from one architecture to another would be prohibitively expensive. In this paper, we design and develop Unifico, a new multi-ISA compiler that generates binaries that maintain the same stack layout during their execution on either architecture. Unifico avoids the need for runtime stack transformation, thus eliminating overheads associated with ISA migration. Additional responsibilities of the Unifico compiler backend include maintenance of a uniform ABI and virtual address space across ISAs. Unifico is implemented using the LLVM compiler infrastructure, and we are currently targeting the x86-64 and ARMv8 ISAs. We have evaluated Unifico across a range of compute-intensive NAS benchmarks and show its minimal impact on overall execution time, where less than 6% (10%) overhead is introduced on average for high-end (low-end) processors. We also analyze the performance impact of Unifico's key design features and demonstrate that they can be further optimized to mitigate this impact. When compared against the state-of-the-art Popcorn compiler, Unifico reduces binary size overhead from ~200% to ~10%, whilst eliminating the stack transformation overhead during ISA migration.

</details>


### [19] [A Systematic Mapping on Software Fairness: Focus, Trends and Industrial Context](https://arxiv.org/abs/2512.23782)
*Kessia Nepomuceno,Fabio Petrillo*

Main category: cs.SE

TL;DR: 本文系统梳理了软件工程中公平性研究的现状，指出当前研究多集中于算法与后处理阶段，缺乏早期干预与产业落地，呼吁全生命周期整合及产学合作。


<details>
  <summary>Details</summary>
Motivation: 软件系统中的公平性日益重要，但现有研究分散且工业适用性低，亟需系统性综述以指导未来方向。

Method: 构建分类框架，对95篇文献进行系统映射分析，从研究趋势、焦点与工业可行性三维度评估。

Result: 研究呈增长趋势，但偏重算法与群体公平，忽视个体公平与根源分析；学术主导，TRL偏低，产业转化遥远。

Conclusion: 需在软件开发生命周期各阶段融入公平性考量，并加强产学协作，以推动公平性研究的实际应用。

Abstract: Context: Fairness in systems has emerged as a critical concern in software engineering, garnering increasing attention as the field has advanced in recent years. While several guidelines have been proposed to address fairness, achieving a comprehensive understanding of research solutions for ensuring fairness in software systems remains challenging. Objectives: This paper presents a systematic literature mapping to explore and categorize current advancements in fairness solutions within software engineering, focusing on three key dimensions: research trends, research focus, and viability in industrial contexts. Methods: We develop a classification framework to organize research on software fairness from a fresh perspective, applying it to 95 selected studies and analyzing their potential for industrial adoption. Results: Our findings reveal that software fairness research is expanding, yet it remains heavily focused on methods and algorithms. It primarily focuses on post-processing and group fairness, with less emphasis on early-stage interventions, individual fairness metrics, and understanding bias root causes. Additionally fairness research remains largely academic, with limited industry collaboration and low to medium Technology Readiness Level (TRL), indicating that industrial transferability remains distant. Conclusion: Our results underscore the need to incorporate fairness considerations across all stages of the software development life-cycle and to foster greater collaboration between academia and industry. This analysis provides a comprehensive overview of the field, offering a foundation to guide future research and practical applications of fairness in software systems.

</details>


### [20] [From Illusion to Insight: Change-Aware File-Level Software Defect Prediction Using Agentic AI](https://arxiv.org/abs/2512.23875)
*Mohsen Hesamolhokama,Behnam Rohani,Amirahmad Shafiee,MohammadAmin Fazli,Jafar Habibi*

Main category: cs.SE

TL;DR: 本文提出一种基于LLM的多智能体辩论框架，用于改进文件级缺陷预测中的变化感知能力，解决传统方法因标签持久性偏差导致的评估失真问题。


<details>
  <summary>Details</summary>
Motivation: 传统文件级缺陷预测方法依赖静态快照，忽视代码变更，导致评估结果虚高，无法有效捕捉关键缺陷引入场景。

Method: 将缺陷预测重构为变化感知任务，利用LLM驱动的多智能体辩论机制对连续版本间的代码变更进行推理。

Result: 在多个PROMISE项目上实验表明，新方法在各类演化子集上表现更均衡，显著提升对缺陷引入的敏感度。

Conclusion: 当前缺陷预测评估存在根本缺陷，实际应用中必须引入变化感知推理机制。

Abstract: Much of the reported progress in file-level software defect prediction (SDP) is, in reality, nothing but an illusion of accuracy. Over the last decades, machine learning and deep learning models have reported increasing performance across software versions. However, since most files persist across releases and retain their defect labels, standard evaluation rewards label-persistence bias rather than reasoning about code changes. To address this issue, we reformulate SDP as a change-aware prediction task, in which models reason over code changes of a file within successive project versions, rather than relying on static file snapshots. Building on this formulation, we propose an LLM-driven, change-aware, multi-agent debate framework. Our experiments on multiple PROMISE projects show that traditional models achieve inflated F1, while failing on rare but critical defect-transition cases. In contrast, our change-aware reasoning and multi-agent debate framework yields more balanced performance across evolution subsets and significantly improves sensitivity to defect introductions. These results highlight fundamental flaws in current SDP evaluation practices and emphasize the need for change-aware reasoning in practical defect prediction. The source code is publicly available.

</details>


### [21] [Coding With AI: From a Reflection on Industrial Practices to Future Computer Science and Software Engineering Education](https://arxiv.org/abs/2512.23982)
*Hung-Fu Chang,MohammadShokrolah Shirazi,Lizhou Cao,Supannika Koolmanojwong Mobasser*

Main category: cs.SE

TL;DR: 该论文探讨了大语言模型（LLM）在工业界编程实践中的应用、风险与工作流变革，并提出计算机教育需转向问题解决、架构思维与代码审查等能力培养。


<details>
  <summary>Details</summary>
Motivation: 现有研究多聚焦个体或教育场景下的AI辅助编程，缺乏对工业从业者实际体验与挑战的系统分析。

Method: 对2024年末至2025年间发布的57个精选YouTube视频进行定性分析，比较传统与LLM编程方式，识别风险并刻画工作流演变。

Result: 发现LLM显著提升生产力并降低入门门槛，但也引发代码质量、安全性、伦理及基础技能退化等担忧，开发瓶颈转向代码审查环节。

Conclusion: 建议计算机教育课程应融入LLM工具，强化问题解决、架构设计与项目实践，以适应行业快速变革。

Abstract: Recent advances in large language models (LLMs) have introduced new paradigms in software development, including vibe coding, AI-assisted coding, and agentic coding, fundamentally reshaping how software is designed, implemented, and maintained. Prior research has primarily examined AI-based coding at the individual level or in educational settings, leaving industrial practitioners' perspectives underexplored. This paper addresses this gap by investigating how LLM coding tools are used in professional practice, the associated concerns and risks, and the resulting transformations in development workflows, with particular attention to implications for computing education. We conducted a qualitative analysis of 57 curated YouTube videos published between late 2024 and 2025, capturing reflections and experiences shared by practitioners. Following a filtering and quality assessment process, the selected sources were analyzed to compare LLM-based and traditional programming, identify emerging risks, and characterize evolving workflows. Our findings reveal definitions of AI-based coding practices, notable productivity gains, and lowered barriers to entry. Practitioners also report a shift in development bottlenecks toward code review and concerns regarding code quality, maintainability, security vulnerabilities, ethical issues, erosion of foundational problem-solving skills, and insufficient preparation of entry-level engineers. Building on these insights, we discuss implications for computer science and software engineering education and argue for curricular shifts toward problem-solving, architectural thinking, code review, and early project-based learning that integrates LLM tools. This study offers an industry-grounded perspective on AI-based coding and provides guidance for aligning educational practices with rapidly evolving professional realities.

</details>


### [22] [Developing controlled natural language for formal specification patterns using AI assistants](https://arxiv.org/abs/2512.24159)
*Natalia Garanina,Vladimir Zyubin,Igor Anureev*

Main category: cs.SE

TL;DR: 提出一种基于AI助手系统化构建需求控制自然语言的方法，适用于事件驱动的时序需求。


<details>
  <summary>Details</summary>
Motivation: 为提升需求表达的准确性与形式化程度，解决自然语言模糊性问题。

Method: 分三阶段：构建通用模板、AI生成语料库并精简属性、语法结构分析形成受控语言。

Result: 方法在事件驱动时序需求场景中验证有效。

Conclusion: 该方法能有效支持从形式化模板到受控自然语言的需求构造。

Abstract: Using an AI assistant, we developed a method for systematically constructing controlled natural language for requirements based on formal specification patterns containing logical attributes. The method involves three stages: 1) compiling a generalized natural language requirement pattern that utilizes all attributes of the formal specification template; 2) generating, using the AI assistant, a corpus of natural language requirement patterns, reduced by partially evaluating attributes (the developed prompt utilizes the generalized template, attribute definitions, and specific formal semantics of the requirement patterns); and 3) formalizing the syntax of the controlled natural language based on an analysis of the grammatical structure of the resulting patterns. The method has been tested for event-driven temporal requirements.

</details>


### [23] ["Game Changer" or "Overenthusiastic Drunk Acquaintance"? Generative AI Use by Blind and Low Vision Software Professionals in the Workplace](https://arxiv.org/abs/2512.24462)
*Yoonha Cha,Victoria Jackson,Lauren Shu,Stacy Branham,André van der Hoek*

Main category: cs.SE

TL;DR: 研究探讨了生成式AI对盲人及低视力软件从业者的影响，发现其在提升效率与可访问性的同时也带来更高风险。


<details>
  <summary>Details</summary>
Motivation: 填补生成式AI在盲人及低视力软件从业者群体中研究视角的空白。

Method: 对39名盲人及低视力软件从业者进行半结构化访谈的定性研究。

Result: 受访者利用生成式AI提升工作效率和可访问性，但更易受幻觉影响且受组织政策限制。

Conclusion: 盲人及低视力从业者需权衡生成式AI带来的高回报与高风险以决定是否使用。

Abstract: The software development workplace poses numerous technical and collaborative accessibility challenges for blind and low vision software professionals (BLVSPs). Though Generative AI (GenAI) is increasingly adopted within the software development industry and has been a rapidly growing topic of interest in research, to date, the unique perspectives of BLVSPs have yet to be consulted. We report on a qualitative study involving 39 semi-structured interviews with BLVSPs about what the introduction of GenAI has meant for their work. We found that BLVSPs used GenAI for many software development tasks, resulting in benefits such as increased productivity and accessibility. However, significant costs were also accompanied by GenAI use as they were more vulnerable to hallucinations than their sighted colleagues. Sometimes, organizational policies prevented use. Based on our findings, we discuss the higher-risks and higher-returns that BLVSPs had to carefully weigh when deciding whether and when to use GenAI tools for work.

</details>


### [24] [Localized Calibrated Uncertainty in Code Language Models](https://arxiv.org/abs/2512.24560)
*David Gros,Prem Devanbu*

Main category: cs.SE

TL;DR: 本文提出了一种定位大语言模型生成代码中与用户意图偏差部分的技术，并通过构建数据集评估不同方法的校准概率性能。


<details>
  <summary>Details</summary>
Motivation: 大语言模型生成的代码可能偏离用户意图，需监督和编辑，因此需要技术来定位这些偏差。

Method: 构建包含“最小意图对齐补丁”的数据集，使用测试用例验证程序正确性，并比较白盒探测、黑盒反思及自一致性方法的校准效果。

Result: 小监督模型的探针在校准误差和Brier技能评分上表现良好，且在仅训练于代码的情况下对自然语言错误也有一定泛化能力。

Conclusion: 所提技术在校准概率估计和跨领域泛化方面具有潜力，有助于AI监督与控制。

Abstract: Large Language models (LLMs) can generate complicated source code from natural language prompts. However, LLMs can generate output that deviates from what the user wants, requiring supervision and editing. To support this process, we offer techniques to localize where generations might be misaligned from user intent. We first create a dataset of "Minimal Intent Aligning Patches" of repaired LLM generated programs. Each program uses test cases to verify correctness. After creating a dataset of programs, we measure how well various techniques can assign a well-calibrated probability to indicate which parts of code will be edited in a minimal patch (i.e., give a probability that corresponds with empirical odds it is edited). We compare white-box probing (where we propose a technique for efficient arbitrary-span querying), against black-box reflective and self-consistency based approaches. We find probes with a small supervisor model can achieve low calibration error and Brier Skill Score of approx 0.2 estimating edited lines on code generated by models many orders of magnitude larger. We discuss the generalizability of the techniques, and the connections to AI oversight and control, finding a probe trained only on code shows some signs of generalizing to natural language errors if new probability scaling is allowed.

</details>


### [25] [On the Effectiveness of Training Data Optimization for LLM-based Code Generation: An Empirical Study](https://arxiv.org/abs/2512.24570)
*Shiqi Kuang,Zhao Tian,Tao Xiao,Dong Wang,Junjie Chen*

Main category: cs.SE

TL;DR: 本文首次大规模评估了五种常用训练数据优化技术及其组合在LLM代码生成中的效果，发现数据合成对功能正确性最有效，而与重构结合时整体性能最佳。


<details>
  <summary>Details</summary>
Motivation: 现有研究缺乏对训练数据优化技术及其组合在代码生成中效果的系统性评估，本文旨在填补这一空白。

Method: 在三个基准和四个LLM上，实证分析五种主流数据优化技术及其两两组合的效果，并进行细粒度归因分析。

Result: 数据合成提升功能正确性和减少代码异味最显著，但可维护性弱于重构、清洗和选择；多数组合不能进一步提升功能正确性，但能改善代码质量；合成+重构组合表现最优。

Conclusion: 本研究为LLM代码生成的数据优化策略提供了首个系统性实证基础和实用指导，推动未来研究与部署。

Abstract: Large language models (LLMs) have achieved remarkable progress in code generation, largely driven by the availability of high-quality code datasets for effective training. To further improve data quality, numerous training data optimization techniques have been proposed; however, their overall effectiveness has not been systematically evaluated. To bridge this gap, we conduct the first large-scale empirical study, examining five widely-used training data optimization techniques and their pairwise combinations for LLM-based code generation across three benchmarks and four LLMs. Our results show that data synthesis is the most effective technique for improving functional correctness and reducing code smells, although it performs relatively worse on code maintainability compared to data refactoring, cleaning, and selection. Regarding combinations, we find that most combinations do not further improve functional correctness but can effectively enhance code quality (code smells and maintainability). Among all combinations, data synthesis combined with data refactoring achieves the strongest overall performance. Furthermore, our fine-grained analysis reinforces these findings and provides deeper insights into how individual techniques and their combinations influence code generation effectiveness. Overall, this work represents a first step toward a systematic understanding of training data optimization and combination strategies, offering practical guidance for future research and deployment in LLM-based code generation.

</details>


### [26] [A Tale of 1001 LoC: Potential Runtime Error-Guided Specification Synthesis for Verifying Large-Scale Programs](https://arxiv.org/abs/2512.24594)
*Zhongyi Wang,Tengjie Lin,Mingshuai Chen,Haokun Li,Mingqi Yang,Xiao Yi,Shengchao Qin,Yixing Luo,Xiaofeng Li,Bin Gu,Liqiang Lu,Jianwei Yin*

Main category: cs.SE

TL;DR: Preguss框架通过结合静态分析与演绎验证，显著提升大规模程序形式化验证的自动化程度，减少人工验证工作量达80.6%~88.9%。


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型在形式化验证中因长上下文推理限制和复杂跨过程规范推断困难导致的可扩展性差问题。

Method: 提出Preguss框架，采用分治策略，结合运行时错误引导的验证单元构建与优先级排序，以及LLM辅助的单元级跨过程规范合成。

Result: 在真实千行级代码程序上实现高度自动化的运行时错误自由验证，性能大幅超越现有LLM方法。

Conclusion: Preguss有效提升了形式化验证的自动化水平，显著降低人工干预需求，为大规模软硬件系统验证提供实用解决方案。

Abstract: Fully automated verification of large-scale software and hardware systems is arguably the holy grail of formal methods. Large language models (LLMs) have recently demonstrated their potential for enhancing the degree of automation in formal verification by, e.g., generating formal specifications as essential to deductive verification, yet exhibit poor scalability due to long-context reasoning limitations and, more importantly, the difficulty of inferring complex, interprocedural specifications. This paper presents Preguss -- a modular, fine-grained framework for automating the generation and refinement of formal specifications. Preguss synergizes between static analysis and deductive verification by steering two components in a divide-and-conquer fashion: (i) potential runtime error-guided construction and prioritization of verification units, and (ii) LLM-aided synthesis of interprocedural specifications at the unit level. We show that Preguss substantially outperforms state-of-the-art LLM-based approaches and, in particular, it enables highly automated RTE-freeness verification for real-world programs with over a thousand LoC, with a reduction of 80.6%~88.9% human verification effort.

</details>


### [27] [DynaFix: Iterative Automated Program Repair Driven by Execution-Level Dynamic Information](https://arxiv.org/abs/2512.24635)
*Zhili Huang,Ling Xu,Chao Liu,Weifeng Sun,Xu Zhang,Yan Lei,Meng Yan,Hongyu Zhang*

Main category: cs.SE

TL;DR: DynaFix是一种利用运行时动态信息迭代修复程序缺陷的新方法，显著提升了自动程序修复的效果和效率。


<details>
  <summary>Details</summary>
Motivation: 现有基于大语言模型的自动程序修复方法多依赖静态分析或粗粒度反馈，难以模拟人类逐步调试过程，限制了复杂缺陷的修复能力。

Method: DynaFix在每轮修复中捕获变量状态、控制流路径和调用栈等执行级动态信息，将其结构化后引导LLM生成候选补丁，并通过失败补丁的重新执行持续收集新信息进行迭代优化。

Result: 在Defects4J v1.2和v2.0基准上，DynaFix修复了186个单函数缺陷，比现有最优方法提升10%，其中38个为此前未修复缺陷；最多35次尝试内完成修复，搜索空间减少70%。

Conclusion: DynaFix通过细粒度动态信息驱动的迭代机制，有效模拟人类调试行为，在修复能力和效率上均优于现有方法。

Abstract: Automated Program Repair (APR) aims to automatically generate correct patches for buggy programs. Recent approaches leveraging large language models (LLMs) have shown promise but face limitations. Most rely solely on static analysis, ignoring runtime behaviors. Some attempt to incorporate dynamic signals, but these are often restricted to training or fine-tuning, or injected only once into the repair prompt, without iterative use. This fails to fully capture program execution. Current iterative repair frameworks typically rely on coarse-grained feedback, such as pass/fail results or exception types, and do not leverage fine-grained execution-level information effectively. As a result, models struggle to simulate human stepwise debugging, limiting their effectiveness in multi-step reasoning and complex bug repair.
  To address these challenges, we propose DynaFix, an execution-level dynamic information-driven APR method that iteratively leverages runtime information to refine the repair process. In each repair round, DynaFix captures execution-level dynamic information such as variable states, control-flow paths, and call stacks, transforming them into structured prompts to guide LLMs in generating candidate patches. If a patch fails validation, DynaFix re-executes the modified program to collect new execution information for the next attempt. This iterative loop incrementally improves patches based on updated feedback, similar to the stepwise debugging practices of human developers. We evaluate DynaFix on the Defects4J v1.2 and v2.0 benchmarks. DynaFix repairs 186 single-function bugs, a 10% improvement over state-of-the-art baselines, including 38 bugs previously unrepaired. It achieves correct patches within at most 35 attempts, reducing the patch search space by 70% compared with existing methods, thereby demonstrating both effectiveness and efficiency in repairing complex bugs.

</details>


### [28] [Characterizing Bugs and Quality Attributes in Quantum Software: A Large-Scale Empirical Study](https://arxiv.org/abs/2512.24656)
*Mir Mohammad Yousuf,Shabir Ahmad Sofi*

Main category: cs.SE

TL;DR: 首次大规模纵向分析123个开源量子项目中的软件缺陷，揭示全栈库和编译器最易出错，自动化测试可降低约60%缺陷发生率。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏对真实量子项目中缺陷如何产生及影响质量的实证研究，需系统性分析以指导量子软件工程实践。

Method: 结合仓库挖掘、静态代码分析、问题元数据提取与规则分类框架，分析2012-2024年间32,296个已验证缺陷报告。

Result: 全栈库与编译器因电路与转译问题最易出错；模拟器主要受测量与噪声建模错误影响；自动化测试显著减少缺陷并加速修复。

Conclusion: 本研究首次提供量子软件缺陷的大规模数据驱动特征，为改进测试、文档与可维护性实践提供实证依据。

Abstract: Quantum Software Engineering (QSE) is essential for ensuring the reliability and maintainability of hybrid quantum-classical systems, yet empirical evidence on how bugs emerge and affect quality in real-world quantum projects remains limited. This study presents the first ecosystem-scale longitudinal analysis of software defects across 123 open source quantum repositories from 2012 to 2024, spanning eight functional categories, including full-stack libraries, simulators, annealing, algorithms, compilers, assembly, cryptography, and experimental computing. Using a mixed method approach combining repository mining, static code analysis, issue metadata extraction, and a validated rule-based classification framework, we analyze 32,296 verified bug reports. Results show that full-stack libraries and compilers are the most defect-prone categories due to circuit, gate, and transpilation-related issues, while simulators are mainly affected by measurement and noise modeling errors. Classical bugs primarily impact usability and interoperability, whereas quantum-specific bugs disproportionately degrade performance, maintainability, and reliability. Longitudinal analysis indicates ecosystem maturation, with defect densities peaking between 2017 and 2021 and declining thereafter. High-severity defects cluster in cryptography, experimental computing, and compiler toolchains. Repositories employing automated testing detect more defects and resolve issues faster. A negative binomial regression further shows that automated testing is associated with an approximate 60 percent reduction in expected defect incidence. Overall, this work provides the first large-scale data-driven characterization of quantum software defects and offers empirical guidance for improving testing, documentation, and maintainability practices in QSE.

</details>


### [29] [Feature Slice Matching for Precise Bug Detection](https://arxiv.org/abs/2512.24858)
*Ke Ma,Jianjun Huang,Wei You,Bin Liang,Jingzheng Wu,Yanjun Wu,Yuanjun Gong*

Main category: cs.SE

TL;DR: MATUS通过端到端方法提取语义特征切片，有效抑制目标噪声以提升相似性度量的精准性，成功在Linux内核中发现31个未知漏洞。


<details>
  <summary>Details</summary>
Motivation: 现有方法难以消除目标代码中的噪声干扰，影响基于相似性度量的缺陷检测效果。

Method: 提出MATUS框架，利用缺陷代码先验知识引导目标切片，提取语义特征切片并基于向量相似性比较，最终审计确认潜在缺陷。

Result: 实验表明MATUS在真实项目中具有较高检测效率，已在Linux内核中发现31个未知缺陷，全部获开发者确认，其中11个被分配CVE编号。

Conclusion: MATUS能有效缓解目标噪声干扰，显著提升基于相似性的缺陷检测精度与实用性。

Abstract: Measuring the function similarity to detect bugs is effective, but the statements unrelated to the bugs can impede the performance due to the noise interference. Suppressing the noise interference in existing works does not manage the tough job, i.e., eliminating the noise in the targets. In this paper, we propose MATUS to mitigate the target noise for precise bug detection based on similarity measurement. Feature slices are extracted from both the buggy query and the targets to represent the semantic feature of (potential) bug logics. In particular, MATUS guides the target slicing with the prior knowledge from the buggy code, in an end-to-end way to pinpoint the slicing criterion in the targets. All feature slices are embedded and compared based on the vector similarity. Buggy candidates are audited to confirm unknown bugs in the targets. Experiments show that MATUS holds advantages in bug detection for real-world projects with acceptable efficiency. In total, MATUS has spotted 31 unknown bugs in the Linux kernel. All of them have been confirmed by the kernel developers, and 11 have been assigned CVEs.

</details>


### [30] [Securing High-Concurrency Ticket Sales: A Framework Based on Microservice](https://arxiv.org/abs/2512.24941)
*Zhiyong Zhang,Xiaoyan Zhang,Xiaoqi Li*

Main category: cs.SE

TL;DR: 本文设计并开发了一个基于微服务架构的铁路购票系统，以应对高并发场景下的稳定性和数据一致性挑战。


<details>
  <summary>Details</summary>
Motivation: 传统聚合架构在节假日等高峰期无法满足大量用户同时访问的需求，系统需要更高的容错性和性能。

Method: 采用B/S架构和Spring Cloud框架，结合多种中间件组件与安全设计方法，实现系统功能如实时查询、动态座位更新、在线选座与购票等。

Result: 系统测试表明，在高并发环境下具有良好的稳定性和响应能力，能有效解决线下购票排队与信息延迟等问题。

Conclusion: 该系统实现了从查询、预订到支付、退票的全流程线上化，兼顾安全性、稳定性与高性能，提升了用户体验与服务效率。

Abstract: The railway ticketing system is one of the most important public service infrastructure. In peak periods such as holidays, it is often faced with the challenge of high concurrency scenarios because of a large number of users accessing at the same time. The traditional aggregation architecture can not meet the peak user requirements because of its insufficient fault tolerance and low ability. Therefore, the system needs to use microservice architecture for development, and add multiple security methods to ensure that the system can have good stability and data consistency under high concurrency scenarios, and can respond quickly to user requests. This paper introduces the use of B/S architecture and Spring Cloud to design and develop a railway ticket purchase system that can maintain stability and reliability under high concurrency scenarios, and formulate multiple security design methods for the system. This system integrates a range of functions, such as real-time train inquiries, dynamic seat updates, online seat selection, and ticket purchasing, effectively addressing common problems associated with offline ticket purchasing, such as long queues and delayed information. It enables a complete online process from inquiry and booking to payment and refunds. Furthermore, the "add passenger" function allows users to purchase tickets for others, extending the convenience of online ticketing to people with limited internet access. The system design prioritizes security and stability, while also focusing on high performance, and achieves these goals through a carefully designed architecture and the integration of multiple middleware components. After the completion of the system development, the core interface of the system is tested, and then the results are analyzed. The test data proves that the system has good ability and stability under high concurrency.

</details>
