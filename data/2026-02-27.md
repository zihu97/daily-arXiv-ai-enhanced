<div id=toc></div>

# Table of Contents

- [cs.AR](#cs.AR) [Total: 1]
- [cs.SE](#cs.SE) [Total: 13]
- [cs.DC](#cs.DC) [Total: 15]


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [1] [FHECore: Rethinking GPU Microarchitecture for Fully Homomorphic Encryption](https://arxiv.org/abs/2602.22229)
*Lohit Daksha,Seyda Guzelhan,Kaustubh Shivdikar,Carlos Agulló Domingo,Óscar Vera Lopez,Gilbert Jonatan,Hubert Dymarkowski,Aymane El Jerari,José Cano,José L. Abellán,John Kim,David Kaeli,Ajay Joshi*

Main category: cs.AR

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Fully Homomorphic Encryption (FHE) enables computation directly on encrypted data but incurs massive computational and memory overheads, often exceeding plaintext execution by several orders of magnitude. While custom ASIC accelerators can mitigate these costs, their long time-to-market and the rapid evolution of FHE algorithms threaten their long-term relevance. GPUs, by contrast, offer scalability, programmability, and widespread availability, making them an attractive platform for FHE. However, modern GPUs are increasingly specialized for machine learning workloads, emphasizing low-precision datatypes (e.g., INT$8$, FP$8$) that are fundamentally mismatched to the wide-precision modulo arithmetic required by FHE. Essentially, while GPUs offer ample parallelism, their functional units, like Tensor Cores, are not suited for wide-integer modulo arithmetic required by FHE schemes such as CKKS. Despite this constraint, researchers have attempted to map FHE primitives on Tensor Cores by segmenting wide integers into low-precision (INT$8$) chunks.
  To overcome these bottlenecks, we propose FHECore, a specialized functional unit integrated directly into the GPU's Streaming Multiprocessor. Our design is motivated by a key insight: the two dominant contributors to latency$-$Number Theoretic Transform and Base Conversion$-$can be formulated as modulo-linear transformations. This allows them to be mapped on a common hardware unit that natively supports wide-precision modulo-multiply-accumulate operations. Our simulations demonstrate that FHECore reduces dynamic instruction count by a geometric mean of $2.41\times$ for CKKS primitives and $1.96\times$ for end-to-end workloads. These reductions translate to performance speedups of $1.57\times$ and $2.12\times$, respectively$-$including a $50\%$ reduction in bootstrapping latency$-$all while inuring a modest $2.4\%$ area overhead.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [2] [EmpiRE-Compass: A Neuro-Symbolic Dashboard for Sustainable and Dynamic Knowledge Exploration, Synthesis, and Reuse](https://arxiv.org/abs/2602.22276)
*Oliver Karras,Amirreza Alasti,Lena John,Sushant Aggarwal,Yücel Celik*

Main category: cs.SE

TL;DR: 本文介绍了EmpiRE-Compass，一个神经符号仪表板，旨在通过语义化结构化文献综述底层数据于研究知识图谱中，并利用大语言模型提供便捷的访问、复制和重用功能，从而促进文献综述的可持续性。


<details>
  <summary>Details</summary>
Motivation: 软件工程和需求工程领域的二级研究（尤其是文献综述）数量激增，生成式人工智能虽能快速产出文献综述，但常以牺牲质量、严谨性和透明度为代价。同时，二级研究往往未能共享底层数据和工件，限制了复制和重用。

Method: 开发EmpiRE-Compass神经符号仪表板，采用模块化系统设计和工作流程，支持专门定制的自定义能力问题。该工具结合研究知识图谱(RKGs)和大语言模型(LLMs)，提供探索性可视分析、神经符号合成和可重用知识三大核心功能。

Result: EmpiRE-Compass已在线免费提供，配有演示视频，源代码和文档作为开源项目发布。默认LLM(GPT-4o mini)限制每个IP地址每天25次请求以控制运营成本。该工具通过统一RKGs和LLMs，降低了技术壁垒，促进了透明度和可复现性。

Conclusion: Conclusion extraction failed

Abstract: Software engineering (SE) and requirements engineering (RE) face a significant increase in secondary studies, particularly literature reviews (LRs), due to the ever-growing number of scientific publications. Generative artificial intelligence (GenAI) exacerbates this trend by producing LRs rapidly but often at the expense of quality, rigor, and transparency. At the same time, secondary studies often fail to share underlying data and artifacts, limiting replication and reuse. This paper introduces EmpiRE-Compass, a neuro-symbolic dashboard designed to lower barriers for accessing, replicating, and reusing LR data. Its overarching goal is to demonstrate how LRs can become more sustainable by semantically structuring their underlying data in research knowledge graphs (RKGs) and by leveraging large language models (LLMs) for easy and dynamic access, replication, and reuse. Building on two RE use cases, we developed EmpiRE-Compass with a modular system design and workflows for curated and custom competency questions. The dashboard is freely available online, accompanied by a demonstration video. To manage operational costs, a limit of 25 requests per IP address per day applies to the default LLM (GPT-4o mini). All source code and documentation are released as an open-source project to foster reuse, adoption, and extension. EmpiRE-Compass provides three core capabilities: (1) Exploratory visual analytics for curated competency questions; (2) Neuro-symbolic synthesis for custom competency questions; and (3) Reusable knowledge with all queries, analyses, and results openly available. By unifying RKGs and LLMs in a neuro-symbolic dashboard, EmpiRE-Compass advances sustainable LRs in RE, SE, and beyond. It lowers technical barriers, fosters transparency and reproducibility, and enables collaborative, continuously updated, and reusable LRs

</details>


### [3] [The Ethos of the PEERfect REVIEWer: Scientific Care and Collegial Welfare](https://arxiv.org/abs/2602.22292)
*Oliver Karras*

Main category: cs.SE

TL;DR: 该论文提出'PEERfect REVIEWer'理念，强调同行评议应兼顾科学严谨性和同行关怀，通过16条实用指南帮助审稿人以建设性、支持性的方式进行评议。


<details>
  <summary>Details</summary>
Motivation: 传统同行评议过于强调科学严谨性，而缺乏支持所有参与者（作者、共同审稿人、组织者和编辑）的同理心，不利于学术界的共同进步和福祉。

Method: 作者基于十年学术经验、与同事的专业交流、文献研究以及对自身撰写和收到的评议的分析，提炼出'PEERfect REVIEWer'理念及其指导方针。

Result: PEERfect REVIEWer理念帮助审稿人保持高科学标准，同时以建设性、支持性、尊重性和及时性的方式进行同行评议，证明科学严谨性和同理心是互补力量。

Conclusion: 同行评议者应重新审视自己的角色，不仅是质量把关者，更是每位同行学术旅程中的伙伴，科学严谨性和同理心同等重要，共同促进真正有影响力的同行评议实践。

Abstract: Peer review remains a cornerstone in academia, yet it frequently falls short in fostering joint progress and well-being. While peer review primarily emphasizes scientific rigor, it often lacks the empathy essential for supporting and encouraging all peers involved. In this experience report, I aim to highlight that peer review is a practice that demands both scientific care for quality and collegial welfare for the joint progress and well-being of all peers involved, including authors, co-reviewers, workshop or conference organizers, and journal editors. Drawing on my ten years of experience in academia, I propose the ethos of the PEERfect REVIEWer, grounded in the two core values: Scientific care and collegial welfare. Through reflection shaped by professional exchanges with colleagues, consideration of literature, and an examination of both self-authored and received reviews, I formulated an accompanying guideline with 16 practical recommendations to guide reviewers in their actions to achieve these two values. The ethos of the PEERfect REVIEWer and its accompanying guideline help reviewers in upholding high scientific standards and conducting peer review in a constructive, supportive, respectful, and timely manner. They demonstrate that scientific rigor and empathy are complementary forces that promote impactful peer review practice. By placing scientific care and collegial welfare at the core of peer review, this experience report reaffirms the importance of scientific rigor while also advocating for greater attention to empathy. It invites reviewers to reconsider their role not merely as gatekeepers but as partners in the academic journey of each peer involved. The PEERfect REVIEWer is both a caretaker of quality and a steward of joint progress and well-being - as truly impactful peer review practice requires scientific rigor and empathy in equal measure.

</details>


### [4] [EyeLayer: Integrating Human Attention Patterns into LLM-Based Code Summarization](https://arxiv.org/abs/2602.22368)
*Jiahao Zhang,Yifan Zhang,Kevin Leach,Yu Huang*

Main category: cs.SE

TL;DR: EyeLayer是一种轻量级注意力增强模块，通过融入人类专家的眼动模式作为先验知识，提升大语言模型在代码摘要任务上的表现。


<details>
  <summary>Details</summary>
Motivation: 探索人类在代码理解方面的专业知识是否能够指导和增强大语言模型在代码摘要任务中的表现。

Method: EyeLayer使用多模态高斯混合模型建模人类阅读代码时的注意力模式，根据学习到的参数(μ_i, σ_i^2)重新分布token嵌入，捕获开发者关注的位置和强度。

Result: 在多种模型架构(如LLaMA-3.2、Qwen3和CodeBERT)上，EyeLayer consistently超越强微调基线，在BLEU-4指标上最高提升13.17%。

Conclusion: 人类眼动模式编码了互补的注意力信号，能够增强大语言模型的语义聚焦能力，并有效迁移到不同模型架构的代码摘要任务中。

Abstract: Code summarization is the task of generating natural language descriptions of source code, which is critical for software comprehension and maintenance. While large language models (LLMs) have achieved remarkable progress on this task, an open question remains: can human expertise in code understanding further guide and enhance these models? We propose EyeLayer, a lightweight attention-augmentation module that incorporates human eye-gaze patterns, as a proxy of human expertise, into LLM-based code summarization. EyeLayer models human attention during code reading via a Multimodal Gaussian Mixture, redistributing token embeddings based on learned parameters (μ_i, σ_i^2) that capture where and how intensively developers focus. This design enables learning generalizable attention priors from eye-tracking data and incorporating them into LLMs seamlessly, without disturbing existing representations. We evaluate EyeLayer across diverse model families (i.e., LLaMA-3.2, Qwen3, and CodeBERT) covering different scales and architectures. EyeLayer consistently outperforms strong fine-tuning baselines across standard metrics, achieving gains of up to 13.17% on BLEU-4. These results demonstrate that human gaze patterns encode complementary attention signals that enhance the semantic focus of LLMs and transfer effectively across diverse models for code summarization.

</details>


### [5] [Contextual Memory Virtualisation: DAG-Based State Management and Structurally Lossless Trimming for LLM Agents](https://arxiv.org/abs/2602.22402)
*Cosmo Santoni*

Main category: cs.SE

TL;DR: 本文提出CMV系统，通过虚拟化上下文内存并采用三阶段无损修剪算法，将LLM会话建模为DAG，在保留关键信息的同时平均减少20%的token使用量，最多减少86%，显著提高长对话的效率和成本效益。


<details>
  <summary>Details</summary>
Motivation: LLM在长推理任务中积累的重要状态信息因上下文窗口限制而在有损压缩中丢失，现有方法无法有效重用和管理这些积累的理解，影响长期推理任务的效果和成本。

Method: CMV系统借鉴操作系统虚拟内存概念，将会话历史建模为有向无环图(DAG)，实现版本控制的状态管理。采用三阶段结构无损修剪算法，保留用户消息和助手响应原文，同时去除机械性膨胀内容。

Result: 在76个真实编码会话评估中，算法平均减少20%的token使用量，最多减少86%。混合工具使用会话中效果最佳，平均减少39%，10轮内达到收支平衡。在提示缓存下保持经济可行性。

Conclusion: CMV系统通过虚拟化上下文内存和结构无损修剪，有效解决了LLM长会话中的状态管理和token效率问题，特别是在混合工具使用场景中表现出显著优势，为实际应用提供了经济高效的解决方案。

Abstract: As large language models engage in extended reasoning tasks, they accumulate significant state -- architectural mappings, trade-off decisions, codebase conventions -- within the context window. This understanding is lost when sessions reach context limits and undergo lossy compaction. We propose Contextual Memory Virtualisation (CMV), a system that treats accumulated LLM understanding as version-controlled state. Borrowing from operating system virtual memory, CMV models session history as a Directed Acyclic Graph (DAG) with formally defined snapshot, branch, and trim primitives that enable context reuse across independent parallel sessions. We introduce a three-pass structurally lossless trimming algorithm that preserves every user message and assistant response verbatim while reducing token counts by a mean of 20% and up to 86% for sessions with significant overhead by stripping mechanical bloat such as raw tool outputs, base64 images, and metadata. A single-user case-study evaluation across 76 real-world coding sessions demonstrates that trimming remains economically viable under prompt caching, with the strongest gains in mixed tool-use sessions, which average 39% reduction and reach break-even within 10 turns. A reference implementation is available at https://github.com/CosmoNaught/claude-code-cmv.

</details>


### [6] [XMENTOR: A Rank-Aware Aggregation Approach for Human-Centered Explainable AI in Just-in-Time Software Defect Prediction](https://arxiv.org/abs/2602.22403)
*Saumendu Roy,Banani Roy,Chanchal Roy,Richard Bassey*

Main category: cs.SE

TL;DR: 论文介绍了XMENTOR，一种统一多种XAI方法冲突解释的VS Code插件，通过自适应阈值、排名和符号一致性策略提供单一连贯视图。用户研究显示90%参与者偏好聚合解释，因其减少了混淆并增强了调试支持。


<details>
  <summary>Details</summary>
Motivation: ML缺陷预测模型提高软件质量，但 opaque reasoning 创造HCI挑战，开发者难以信任不可解释的模型。现有XAI方法(LIME、SHAP、BreakDown)一起使用时产生冲突解释，增加混淆、挫败感和认知负荷。

Method: 提出XMENTOR，一种以人为中心、排名感知的聚合方法，实现为VS Code插件。通过自适应阈值、排名和符号一致性以及备用策略，将多个后验解释统一为单一连贯视图，保持清晰度而不使用户不堪重负。

Result: 用户研究中，近90%参与者更喜欢聚合解释，指出其减少混淆，并为调试和缺陷审查的日常任务提供更强支持。

Conclusion: 结合多种解释并将其嵌入开发者工作流程可提高可解释性、可用性和信任度。

Abstract: Machine learning (ML)-based defect prediction models can improve software quality. However, their opaque reasoning creates an HCI challenge because developers struggle to trust models they cannot interpret. Explainable AI (XAI) methods such as LIME, SHAP, and BreakDown aim to provide transparency, but when used together, they often produce conflicting explanations that increase confusion, frustration, and cognitive load. To address this usability challenge, we introduce XMENTOR, a human-centered, rank-aware aggregation method implemented as a VS Code plugin. XMENTOR unifies multiple post-hoc explanations into a single, coherent view by applying adaptive thresholding, rank and sign agreement, and fallback strategies to preserve clarity without overwhelming users. In a user study, nearly 90% of the participants preferred aggregated explanations, citing reduced confusion and stronger support for daily tasks of debugging and review of defects. Our findings show how combining explanations and embedding them into developer workflows can enhance interpretability, usability, and trust.

</details>


### [7] [Automating the Detection of Requirement Dependencies Using Large Language Models](https://arxiv.org/abs/2602.22456)
*Ikram Darif,Feifei Niu,Manel Abdellatif,Lionel C. Briand,Ramesh S.,Arun Adiththan*

Main category: cs.SE

TL;DR: 本文提出了LEREDD，一种基于大型语言模型的自动化需求依赖检测方法，结合检索增强生成(RAG)和上下文学习(ICL)技术。该方法在需求依赖分类任务上取得了高准确率(0.93)和F1分数(0.84)，显著优于现有基线方法，特别是在细粒度依赖类型检测方面。作者还提供了包含813个需求对的标注数据集以支持可复现性。


<details>
  <summary>Details</summary>
Motivation: 软件需求之间通过各种类型相互关联，识别这些依赖关系对软件开发中的关键决策至关重要。然而，现代软件系统中大量复杂耦合的需求、自然语言需求的模糊性以及需求的频繁变化，使得需求依赖检测具有挑战性。因此，这一任务常被忽视或手动执行，效率低下且易出错。

Method: 作者提出LEREDD，一种基于大型语言模型(LLM)的需求依赖自动检测方法，利用检索增强生成(RAG)和上下文学习(ICL)技术，能够直接从自然语言需求中识别多种依赖类型。该方法设计用于区分依赖和非依赖需求，并特别关注细粒度依赖类型的检测。

Result: LEREDD在两个最先进基线方法上的实证评估显示：需求依赖和非依赖分类准确率达0.93，F1分数为0.84(非依赖案例平均为0.96)。在检测细粒度依赖类型方面，LEREDD显著优于基线方法，特别是对于Requires依赖，F1分数相对提升分别达到94.87%和105.41%。

Conclusion: LEREDD证明了结合RAG和ICL技术的LLM在需求依赖检测任务上的有效性，为需求工程领域提供了有价值的自动化解决方案。此外，作者提供的包含三个不同系统813个需求对的标注数据集促进了该领域的可复现性和未来研究。

Abstract: Requirements are inherently interconnected through various types of dependencies. Identifying these dependencies is essential, as they underpin critical decisions and influence a range of activities throughout software development. However, this task is challenging, particularly in modern software systems, given the high volume of complex, coupled requirements. These challenges are further exacerbated by the ambiguity of Natural Language (NL) requirements and their constant change. Consequently, requirement dependency detection is often overlooked or performed manually. Large Language Models (LLMs) exhibit strong capabilities in NL processing, presenting a promising avenue for requirement-related tasks. While they have shown to enhance various requirements engineering tasks, their effectiveness in identifying requirement dependencies remains unexplored. In this paper, we introduce LEREDD, an LLM-based approach for automated detection of requirement dependencies that leverages Retrieval-Augmented Generation (RAG) and In-Context Learning (ICL). It is designed to identify diverse dependency types directly from NL requirements. We empirically evaluate LEREDD against two state-of-the-art baselines. The results show that LEREDD provides highly accurate classification of dependent and non-dependent requirements, achieving an accuracy of 0.93, and an F1 score of 0.84, with the latter averaging 0.96 for non-dependent cases. LEREDD outperforms zero-shot LLMs and baselines, particularly in detecting fine-grained dependency types, where it yields average relative gains of 94.87% and 105.41% in F1 scores for the Requires dependency over the baselines. We also provide an annotated dataset of requirement dependencies encompassing 813 requirement pairs across three distinct systems to support reproducibility and future research.

</details>


### [8] [RandSet: Randomized Corpus Reduction for Fuzzing Seed Scheduling](https://arxiv.org/abs/2602.22729)
*Yuchong Xie,Kaikai Zhang,Yu Liu,Rundong Yang,Ping Chen,Shuai Wang,Dongdong She*

Main category: cs.SE

TL;DR: RandSet是一种随机化语料库缩减技术，通过将语料库缩减问题转化为集合覆盖问题来同时实现语料库大小缩减和多样化种子选择，最小化运行时开销，在三个流行模糊测试器上实现了比现有技术更优的性能。


<details>
  <summary>Details</summary>
Motivation: 现有模糊测试面临种子爆炸问题，即维持巨大语料库时无法有效选择有前途的种子。现有工作专注于种子优先级排序但无法解决根本问题，因为语料库规模仍然巨大。

Method: RandSet将语料库缩减表述为集合覆盖问题，计算一个覆盖整个语料库所有特征的随机化子集，从该子集而非整个语料库中调度种子，通过引入随机性同时实现多样化的种子选择和低运行成本。

Result: RandSet在独立程序上实现了16.58%的覆盖率提升，在FuzzBench上最高提升3.57%，在Magma上比最先进技术触发多达7个额外的真实错误，同时仅引入1.17%-3.93%的开销。平均子集比例为独立程序的4.03%和FuzzBench的5.99%。

Conclusion: RandSet通过随机化语料库缩减技术有效缓解了种子爆炸问题，在保持多样性的同时显著提高了模糊测试效率，是一种实用的语料库缩减解决方案。

Abstract: Seed explosion is a fundamental problem in fuzzing seed scheduling, where a fuzzer maintains a huge corpus and fails to choose promising seeds. Existing works focus on seed prioritization but still suffer from seed explosion since corpus size remains huge. We tackle this from a new perspective: corpus reduction, i.e., computing a seed corpus subset. However, corpus reduction could lead to poor seed diversity and large runtime overhead. Prior techniques like cull_queue, AFL-Cmin, and MinSet suffer from poor diversity or prohibitive overhead, making them unsuitable for high-frequency seed scheduling.
  We propose RandSet, a novel randomized corpus reduction technique that reduces corpus size and yields diverse seed selection simultaneously with minimal overhead. Our key insight is introducing randomness into corpus reduction to enjoy two benefits of a randomized algorithm: randomized output (diverse seed selection) and low runtime cost. Specifically, we formulate corpus reduction as a set cover problem and compute a randomized subset covering all features of the entire corpus. We then schedule seeds from this small, randomized subset rather than the entire corpus, effectively mitigating seed explosion.
  We implement RandSet on three popular fuzzers: AFL++, LibAFL, and Centipede, and evaluate it on standalone programs, FuzzBench, and Magma. Results show RandSet achieves significantly more diverse seed selection than other reduction techniques, with average subset ratios of 4.03% and 5.99% on standalone and FuzzBench programs. RandSet achieves a 16.58% coverage gain on standalone programs and up to 3.57% on FuzzBench in AFL++, triggers up to 7 more ground-truth bugs than the state-of-the-art on Magma, while introducing only 1.17%-3.93% overhead.

</details>


### [9] [Evaluating and Improving Automated Repository-Level Rust Issue Resolution with LLM-based Agents](https://arxiv.org/abs/2602.22764)
*Jiahong Xiang,Wenxiao He,Xihua Wang,Hongliang Tian,Yuqun Zhang*

Main category: cs.SE

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: The Rust programming language presents a steep learning curve and significant coding challenges, making the automation of issue resolution essential for its broader adoption. Recently, LLM-powered code agents have shown remarkable success in resolving complex software engineering tasks, yet their application to Rust has been limited by the absence of a large-scale, repository-level benchmark. To bridge this gap, we introduce Rust-SWE-bench, a benchmark comprising 500 real-world, repository-level software engineering tasks from 34 diverse and popular Rust repositories. We then perform a comprehensive study on Rust-SWE-bench with four representative agents and four state-of-the-art LLMs to establish a foundational understanding of their capabilities and limitations in the Rust ecosystem. Our extensive study reveals that while ReAct-style agents are promising, i.e., resolving up to 21.2% of issues, they are limited by two primary challenges: comprehending repository-wide code structure and complying with Rust's strict type and trait semantics. We also find that issue reproduction is rather critical for task resolution. Inspired by these findings, we propose RUSTFORGER, a novel agentic approach that integrates an automated test environment setup with a Rust metaprogramming-driven dynamic tracing strategy to facilitate reliable issue reproduction and dynamic analysis. The evaluation shows that RUSTFORGER using Claude-Sonnet-3.7 significantly outperforms all baselines, resolving 28.6% of tasks on Rust-SWE-bench, i.e., a 34.9% improvement over the strongest baseline, and, in aggregate, uniquely solves 46 tasks that no other agent could solve across all adopted advanced LLMs.

</details>


### [10] [Productivity and Collaboration in Hybrid Agile Teams: An Interview Study](https://arxiv.org/abs/2602.22835)
*Elisabeth Mo,Jefferson Seide Molléri,Asle Fagerstrøm*

Main category: cs.SE

TL;DR: 该研究通过访谈三个挪威敏捷团队，发现混合工作减少了非正式互动，造成参与不均，并增加对数字工具的依赖，而信任、沟通和工具支持是团队有效性的中介因素。


<details>
  <summary>Details</summary>
Motivation: 疫情后混合工作已成为现实，改变了敏捷团队交付价值、协作和适应的方式，需要了解混合环境如何影响敏捷团队的生产力和协作。

Method: 通过对三个挪威敏捷团队进行九次访谈，采用定性研究方法收集混合工作环境中的团队体验数据。

Result: 混合工作减少了非正式互动，造成参与不均，增加对数字工具的依赖，敏捷仪式成为团队定位的锚点，信任、沟通和工具支持是团队有效性的中介因素。

Conclusion: 混合敏捷工作是一个不断发展领域，需要量身定制的结构来支持包容性、团队凝聚力和可持续绩效。

Abstract: Hybrid work has become a reality post-pandemic, transforming how Agile teams deliver value, collaborate, and adapt. This study investigate how hybrid settings influence productivity and collaboration through nine interviews with three Norwegian Agile teams. Our findings show that hybrid work reduces informal interaction, creates uneven participation, and increases reliance on digital tools. Agile ceremonies became alignment anchors, while trust, communication, and tool support mediate team effectiveness. Hybrid Agile work is an evolving field that requires tailored structures to support inclusion, team cohesion, and sustainable performance.

</details>


### [11] [Managing Uncertainty in LLM-based Multi-Agent System Operation](https://arxiv.org/abs/2602.23005)
*Man Zhang,Tao Yue,Yihua He*

Main category: cs.SE

TL;DR: 该论文提出了一个基于生命周期的多智能体不确定性管理框架，用于解决LLM多智能体系统在安全关键领域中的系统级风险问题。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注模型级别的不确定性，而忽视了智能体协调、数据管道、人机交互和运行时控制逻辑等系统级风险传播问题。

Method: 区分认识论和本体论不确定性，提出包含表示、识别、演化和适应四个机制的全生命周期不确定性管理框架，实现结构化运行时治理和受控适应。

Result: 通过真实世界的心脏超声多智能体系统验证，证明了该方法在提高诊断推理可靠性和可诊断性方面的有效性。

Conclusion: 该方法超越了以模型为中心的方法，为其他安全关键领域的LLM多智能体系统提供了有原则的运行控制和运行保证。

Abstract: Applying LLM-based multi-agent software systems in safety-critical domains such as lifespan echocardiography introduces system-level risks that cannot be addressed by improving model accuracy alone. During system operation, beyond individual LLM behavior, uncertainty propagates through agent coordination, data pipelines, human-in-the-loop interaction, and runtime control logic. Yet existing work largely treats uncertainty at the model level rather than as a first-class software engineering concern. This paper approaches uncertainty from both system-level and runtime perspectives. We first differentiate epistemological and ontological uncertainties in the context of LLM-based multi-agent software system operation. Building on this foundation, we propose a lifecycle-based uncertainty management framework comprising four mechanisms: representation, identification, evolution, and adaptation. The uncertainty lifecycle governs how uncertainties emerge, transform, and are mitigated across architectural layers and execution phases, enabling structured runtime governance and controlled adaptation. We demonstrate the feasibility of the framework using a real-world LLM-based multi-agent echocardiographic software system developed in clinical collaboration, showing improved reliability and diagnosability in diagnostic reasoning. The proposed approach generalizes to other safety-critical LLM-based multi-agent software systems, supporting principled operational control and runtime assurance beyond model-centric methods.

</details>


### [12] [CL4SE: A Context Learning Benchmark For Software Engineering Tasks](https://arxiv.org/abs/2602.23047)
*Haichuan Hu,Ye Shang,Guoqing Xie,Congqing He,Quanjun Zhang*

Main category: cs.SE

TL;DR: CL4SE是首个软件工程领域上下文学习的标准化评估框架，包含四种SE特定上下文类型的大规模数据集和评估方法，显示上下文学习平均提升性能24.7%。


<details>
  <summary>Details</summary>
Motivation: 现有研究缺乏软件工程特定上下文类型的系统分类和专用基准来量化不同上下文在核心SE工作流程中的异构效应。

Method: 提出CL4SE基准，包含四种SE导向上下文类型(可解释示例、项目特定上下文、程序决策上下文、正负上下文)，映射到四个代表性任务，构建了来自30多个开源项目的13,000多个样本数据集，评估了五个主流LLM在九个指标上的表现。

Result: 上下文学习在所有任务中平均性能提升24.7%，程序上下文将代码审查性能提升高达33%，混合正负上下文将补丁评估提高30%，项目特定上下文将代码摘要BLEU提高14.78%，可解释示例将代码生成PASS@1提高5.72%。

Conclusion: CL4SE建立了SE上下文学习的首个标准化评估框架，提供了任务特定上下文设计的可操作经验见解，并发布了大规模数据集以促进该领域的可重复研究。

Abstract: Context engineering has emerged as a pivotal paradigm for unlocking the potential of Large Language Models (LLMs) in Software Engineering (SE) tasks, enabling performance gains at test time without model fine-tuning. Despite its success, existing research lacks a systematic taxonomy of SE-specific context types and a dedicated benchmark to quantify the heterogeneous effects of different contexts across core SE workflows. To address this gap, we propose CL4SE (Context Learning for Software Engineering), a comprehensive benchmark featuring a fine-grained taxonomy of four SE-oriented context types (interpretable examples, project-specific context, procedural decision-making context, and positive & negative context), each mapped to a representative task (code generation, code summarization, code review, and patch correctness assessment). We construct high-quality datasets comprising over 13,000 samples from more than 30 open-source projects and evaluate five mainstream LLMs across nine metrics. Extensive experiments demonstrate that context learning yields an average performance improvement of 24.7% across all tasks. Specifically, procedural context boosts code review performance by up to 33% (Qwen3-Max), mixed positive-negative context improves patch assessment by 30% (DeepSeek-V3), project-specific context increases code summarization BLEU by 14.78% (GPT-Oss-120B), and interpretable examples enhance code generation PASS@1 by 5.72% (DeepSeek-V3). CL4SE establishes the first standardized evaluation framework for SE context learning, provides actionable empirical insights into task-specific context design, and releases a large-scale dataset to facilitate reproducible research in this domain.

</details>


### [13] [LLM-Powered Silent Bug Fuzzing in Deep Learning Libraries via Versatile and Controlled Bug Transfer](https://arxiv.org/abs/2602.23065)
*Kunpeng Zhang,Dongwei Xiao,Daoyuan Wu,Jiali Zhao,Yuanyi Lin,Tongtong Xu,Shaohua Wang,Shuai Wang*

Main category: cs.SE

TL;DR: TransFuzz利用大型语言模型(LLM)从历史报告中提取上下文感知的bug模式，通过功能匹配API，合成测试用例和自定义验证器，实现了深度学习库中静默bug的主动检测，发现了79个新bug(12个CVE)。


<details>
  <summary>Details</summary>
Motivation: 深度学习库在关键应用中被广泛使用，但现有的模糊测试技术难以检测静默bug，因为缺乏有效的测试程序和相应的验证器。

Method: 利用大型语言模型进行bug迁移：从历史问题中提取上下文感知bug模式，使用基于功能的嵌入匹配语义相关的API，合成带有自定义验证器的测试用例，并通过LLM驱动的自验证模块确保迁移的可靠性。

Result: 在PyTorch、TensorFlow和MindSpore三个主流深度学习库中发现了79个先前未知的bug，其中12个已被确认为CVE，覆盖了10种bug类型。

Conclusion: TransFuzz展示了将深度学习库bug发现能力迁移的有效性和通用性，通过利用大型语言模型和bug转移技术，显著提升了深度学习库中静默bug的检测能力。

Abstract: Deep learning (DL) libraries are widely used in critical applications, where even subtle silent bugs can lead to serious consequences. While existing DL fuzzing techniques have made progress in detecting crashes, they inherently struggle to detect silent bugs due to the lack of effective test programs and corresponding oracles.
  Building on the observation that historical bug reports contain rich, underutilized information about silent bugs, we leverage large language models (LLMs) to perform versatile yet controlled bug transfer for silent bug fuzzing. Specifically, our approach uses LLMs to extract context-aware bug patterns from historical issues, match semantically related Application Programming Interfaces (APIs) using functionality-based embeddings, and synthesize test cases with customized oracles. This enables proactive detection of silent bugs by transferring high-risk contexts and oracle designs from known buggy APIs to functionally similar target APIs. To ensure the reliability of our context-aware bug transfer, we introduce an LLM-powered self-validation module that systematically evaluates the validity of each transferred bug instance. We implement this methodology in a tool named TransFuzz and evaluate it on three mainstream DL libraries: PyTorch, TensorFlow, and MindSpore. TransFuzz successfully discovers 79 previously unknown bugs (12 confirmed as Common Vulnerabilities and Exposures (CVEs)) in 10 bug types, demonstrating its effectiveness and generalizability in migrating DL library bug discovery capabilities.

</details>


### [14] [Utilizing LLMs for Industrial Process Automation](https://arxiv.org/abs/2602.23331)
*Salim Fares*

Main category: cs.SE

TL;DR: 该研究探索将大型语言模型(LLMs)应用于工业流程自动化领域的专用编程语言，相较于通用编程语言这一方向尚未得到充分研究。


<details>
  <summary>Details</summary>
Motivation: 尽管LLMs在软件工程中的应用日益受到关注，但大多数研究集中在Python等通用编程语言上，而专用于工业自动化领域的编程语言(通常仅在专有环境中使用)与LLMs的结合应用尚未得到充分探索。

Method: 研究旨在将LLMs集成到工业开发流程中，解决实际编程任务(如为机械臂生成运动例程)并加速制造系统的开发周期。

Result: 摘要中未明确提及研究结果(可能论文展示了在工业环境中应用LLMs的发现)。

Conclusion: 摘要中未明确提及研究结论(可能论文总结了LLMs在工业自动化软件开发中的有效性和潜力)。

Abstract: A growing number of publications address the best practices to use Large Language Models (LLMs) for software engineering in recent years. However, most of this work focuses on widely-used general purpose programming languages like Python due to their widespread usage training data. The utility of LLMs for software within the industrial process automation domain, with highly-specialized languages that are typically only used in proprietary contexts, remains underexplored. This research aims to utilize and integrate LLMs in the industrial development process, solving real-life programming tasks (e.g., generating a movement routine for a robotic arm) and accelerating the development cycles of manufacturing systems.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [15] [Engineered Simultaneity: The Physical Impossibility of Consolidated Price Discovery Across Spacelike-Separated Exchanges](https://arxiv.org/abs/2602.22350)
*Paul Borrill*

Main category: cs.DC

TL;DR: 该论文引入'工程化同时性'概念，指出美国全国最佳买卖报价(NBBO)是典型案例，由于交易所间距离造成的光传播延迟(143-3,940微秒)，NBBO具有参考系依赖性，高频交易公司利用数据访问时差(50:1优势)每年获利约50亿美元。


<details>
  <summary>Details</summary>
Motivation: 研究金融市场中同时性概念的应用问题，揭示NBBO系统因忽略相对论效应而存在的结构缺陷。

Method: 通过分析NBBO系统的物理约束(交易所间距、光传播延迟)，比较不同数据访问渠道的时延差异，应用赖尔的范畴错误理论进行概念分析。

Result: 证明NBBO是参考系依赖的，不存在框架无关的价格排序，高频交易公司利用这一现象创造信息不对称，年获利约50亿美元。

Conclusion: NBBO系统在同时性概念的应用上存在范畴错误，这种设计缺陷导致市场结构不平等，需要重新考虑金融市场的同时性定义和数据访问规则。

Abstract: We introduce the concept of engineered simultaneity: a system design that (1) requires comparing events at spacelike-separated locations, (2) implements this comparison via an implicit simultaneity convention, and (3) represents the result as objective rather than conventional. The United States National Best Bid and Offer (NBBO), mandated by SEC Regulation NMS Rule 611, is shown to be an instance of engineered simultaneity. We prove that the NBBO is frame-dependent: its value depends on the reference frame in which "current" prices are defined. Since the exchanges that generate quote data are separated by distances of 43-1,180 km, light-travel times of 143-3,940 microseconds create unavoidable windows during which no frame-independent price ordering exists. High-frequency trading firms exploit this window by accessing exchange data via direct feeds (latency ~tens of microseconds) while the consolidated Securities Information Processor operates at ~1,128 microseconds -- a ratio exceeding 50:1. We demonstrate that this constitutes a category mistake in the sense of Ryle: the NBBO applies the concept of "simultaneity" in a domain where it has no frame-independent meaning. The resulting information asymmetry extracts approximately $5 billion annually from other market participants.

</details>


### [16] [DIAL: Decentralized I/O AutoTuning via Learned Client-side Local Metrics for Parallel File System](https://arxiv.org/abs/2602.22392)
*Md Hasanur Rashid,Xinyi Li,Youbiao He,Forrest Sheng Bao,Dong Dai*

Main category: cs.DC

TL;DR: 本文提出DIAL方法，通过使用本地指标和机器学习，实现并行文件系统的去中心化I/O自动调优，避免了全局监控的开销，实现了更细粒度的动态调优。


<details>
  <summary>Details</summary>
Motivation: 现有的并行文件系统I/O调优方法严重依赖全局运行时指标和复杂的应用I/O模式建模，导致开销过大，限制了实际系统中细粒度动态调优的实现。

Method: DIAL采用去中心化方法，将每个I/O客户端视为独立单元，仅使用可观测的本地指标进行配置调优，利用机器学习模型使多个可调单元能够做出独立但协同的决策，及时响应全局存储系统状态变化。

Result: DIAL能够在不依赖全局指标的情况下，实现应用的全局I/O性能提升，同时减少监控开销，支持更细粒度和动态的调优。

Conclusion: DIAL为并行文件系统提供了一种轻量级、高效的I/O调优方法，通过本地指标和机器学习实现了全局性能优化，克服了传统调优方法的开销问题。

Abstract: Enabling efficient, high-performance data access in parallel file systems (PFS) is critical for today's high-performance computing systems. PFS client-side I/O heavily impacts the final I/O performance delivered to individual applications and the entire system. Autotuning the key client-side I/O behaviors has been extensively studied and shows promising results. However, existing work has heavily relied on extensive number of global runtime metrics to monitor and accurate modeling of applications' I/O patterns. Such heavy overheads significantly limit the ability to enable fine-grained, dynamic tuning in practical systems. In this study, we propose DIAL (Decentralized I/O AutoTuning via Learned Client-side Local Metrics) which takes a drastically different approach. Instead of trying to extract the global I/O patterns of applications, DIAL takes a decentralized approach, treating each I/O client as an independent unit and tuning configurations using only its locally observable metrics. With the help of machine learning models, DIAL enables multiple tunable units to make independent but collective decisions, reacting to what is happening in the global storage systems in a timely manner and achieving better I/O performance globally for the application.

</details>


### [17] [AdapTBF: Decentralized Bandwidth Control via Adaptive Token Borrowing for HPC Storage](https://arxiv.org/abs/2602.22409)
*Md Hasanur Rashid,Dong Dai*

Main category: cs.DC

TL;DR: 该论文提出AdapTBF，一种在HPC环境中改进I/O带宽控制的方案，通过自适应借用和借出机制，在保证公平性的同时提高存储效率。


<details>
  <summary>Details</summary>
Motivation: 现代HPC应用共享全局存储系统时，某些应用可能消耗与其计算资源不成比例的存储带宽，导致资源浪费，而传统的固定比例限制(TBF)无法有效处理突发I/O模式。

Method: 基于Lustre并行文件系统实现AdapTBF，采用分散式带宽控制方法，允许应用根据需求动态借用和借出带宽资源，而非严格按比例限制。

Result: AdapTBF有效管理了I/O带宽分配，即使在极端条件下也能保持高存储利用率，同时确保应用间的公平性，提高了整体系统效率。

Conclusion: AdapTBF通过自适应带宽控制机制，成功解决了传统I/O限制方法在处理突发I/O时的效率问题，为HPC环境中的存储资源共享提供了更有效的解决方案。

Abstract: Modern high-performance computing (HPC) applications run on compute resources but share global storage systems. This design can cause problems when applications consume a disproportionate amount of storage bandwidth relative to their allocated compute resources. For example, an application running on a single compute node can issue many small, random writes and consume excessive I/O bandwidth from a storage server. This can hinder larger jobs that write to the same storage server and are allocated many compute nodes, resulting in significant resource waste.
  A straightforward solution is to limit each application's I/O bandwidth on storage servers in proportion to its allocated compute resources. This approach has been implemented in parallel file systems using Token Bucket Filter (TBF). However, strict proportional limits often reduce overall I/O efficiency because HPC applications generate short, bursty I/O. Limiting bandwidth can waste server capacity when applications are idle or prevent applications from temporarily using higher bandwidth during bursty phases.
  We argue that I/O control should maximize per-application performance and overall storage efficiency while ensuring fairness (e.g., preventing small jobs from blocking large-scale ones). We propose AdapTBF, which builds on TBF in modern parallel file systems (e.g., Lustre) and introduces a decentralized bandwidth control approach using adaptive borrowing and lending. We detail the algorithm, implement AdapTBF in Lustre, and evaluate it using synthetic workloads modeled after real-world scenarios. Results show that AdapTBF manages I/O bandwidth effectively while maintaining high storage utilization, even under extreme conditions.

</details>


### [18] [CARAT: Client-Side Adaptive RPC and Cache Co-Tuning for Parallel File Systems](https://arxiv.org/abs/2602.22423)
*Md Hasanur Rashid,Nathan R. Tallent,Forrest Sheng Bao,Dong Dai*

Main category: cs.DC

TL;DR: CARAT是一种机器学习引导的框架，能够对并行文件系统参数进行可扩展的在线调优，实现了高达3倍的性能提升。


<details>
  <summary>Details</summary>
Motivation: 现有的并行文件系统自动调优框架缺乏可扩展性、自适应能力和在线操作能力。

Method: CARAT利用本地可观测指标，通过机器学习协同调优客户端RPC和缓存参数，使每个客户端能够独立做出智能调优决策。

Result: 在动态I/O模式、真实HPC工作负载和多客户端部署的评估中，CARAT比默认或静态配置实现了高达3倍的性能提升。

Conclusion: CARAT的可扩展性和轻量级设计使其适合在现有并行文件系统中广泛部署，为各种数据密集型应用程序带来性能提升。

Abstract: Tuning parallel file system in High-Performance Computing (HPC) systems remains challenging due to the complex I/O paths, diverse I/O patterns, and dynamic system conditions. While existing autotuning frameworks have shown promising results in tuning PFS parameters based on applications' I/O patterns, they lack scalability, adaptivity, and the ability to operate online. In this work, focusing on scalable online tuning, we present CARAT, an ML-guided framework to co-tune client-side RPC and caching parameters of PFS, leveraging only locally observable metrics. Unlike global or pattern-dependent approaches, CARAT enables each client to make independent and intelligent tuning decisions online, responding to real-time changes in both application I/O behaviors and system states. We then prototyped CARAT using Lustre and evaluated it extensively across dynamic I/O patterns, real-world HPC workloads, and multi-client deployments. The results demonstrated that CARAT can achieve up to 3x performance improvement over the default or static configurations, validating the effectiveness and generality of our approach. Due to its scalability and lightweight, we believe CARAT has the potential to be widely deployed into existing PFS and benefit various data-intensive applications.

</details>


### [19] [GetBatch: Distributed Multi-Object Retrieval for ML Data Loading](https://arxiv.org/abs/2602.22434)
*Alex Aizman,Abhishek Gaikwad,Piotr Żelasko*

Main category: cs.DC

TL;DR: 论文提出GetBatch API，将批量数据检索提升为存储一级操作，替代多个独立GET请求，显著提升机器学习训练数据获取效率。


<details>
  <summary>Details</summary>
Motivation: 机器学习训练需从分布式存储获取大量数据样本，数千个独立GET请求的请求开销主导了数据传输时间，成为性能瓶颈。

Method: 设计GetBatch对象存储API，实现确定性的、容错的流式批量数据检索操作，替代传统多个GET请求。

Result: 对小对象实现高达15倍吞吐量提升；生产环境中P95批量检索延迟降低2倍，P99尾部延迟降低3.7倍。

Conclusion: GetBatch通过批量处理优化数据检索流程，显著减少机器学习训练的数据获取开销，提升训练效率。

Abstract: Machine learning training pipelines consume data in batches. A single training step may require thousands of samples drawn from shards distributed across a storage cluster. Issuing thousands of individual GET requests incurs per-request overhead that often dominates data transfer time. To solve this problem, we introduce GetBatch - a new object store API that elevates batch retrieval to a first-class storage operation, replacing independent GET operations with a single deterministic, fault-tolerant streaming execution. GetBatch achieves up to 15x throughput improvement for small objects and, in a production training workload, reduces P95 batch retrieval latency by 2x and P99 per-object tail latency by 3.7x compared to individual GET requests.

</details>


### [20] [veScale-FSDP: Flexible and High-Performance FSDP at Scale](https://arxiv.org/abs/2602.22437)
*Zezhou Wang,Youjie Li,Zhiqi Lin,Jiacheng Yang,Cong Xie,Guanyu Feng,Zheng Zhong,Ziyue Huang,Hongyu Zhu,Zhi Zhang,Yanghua Peng,Xin Liu*

Main category: cs.DC

TL;DR: veScale-FSDP是一款重新设计的FSDP系统，通过灵活的分片格式和结构感知算法实现了更高的吞吐量和更低的内存使用，同时能够扩展到数万GPU。


<details>
  <summary>Details</summary>
Motivation: 现有FSDP系统在结构化训练方法和非元素级优化器方面存在局限性，固定分片格式与块结构计算冲突，且通信和内存效率低下，限制了扩展能力。

Method: 引入RaggedShard灵活分片格式和结构感知规划算法，保持FSDP所需的高效数据布局，原生支持块量化训练和非元素级优化器。

Result: 相比现有FSDP系统，吞吐量提高5-66%，内存使用降低16-30%，并能高效扩展到数万GPU。

Conclusion: veScale-FSDP在保持灵活性的同时提供了卓越的性能和扩展性，适用于大规模模型训练。

Abstract: Fully Sharded Data Parallel (FSDP), also known as ZeRO, is widely used for training large-scale models, featuring its flexibility and minimal intrusion on model code. However, current FSDP systems struggle with structure-aware training methods (e.g., block-wise quantized training) and with non-element-wise optimizers (e.g., Shampoo and Muon) used in cutting-edge models (e.g., Gemini, Kimi K2). FSDP's fixed element- or row-wise sharding formats conflict with the block-structured computations. In addition, today's implementations fall short in communication and memory efficiency, limiting scaling to tens of thousands of GPUs. We introduce veScale-FSDP, a redesigned FSDP system that couples a flexible sharding format, RaggedShard, with a structure-aware planning algorithm to deliver both flexibility and performance at scale. veScale-FSDP natively supports efficient data placement required by FSDP, empowering block-wise quantization and non-element-wise optimizers. As a result, veScale-FSDP achieves 5~66% higher throughput and 16~30% lower memory usage than existing FSDP systems, while scaling efficiently to tens of thousands of GPUs.

</details>


### [21] [Fault-tolerant Reduce and Allreduce operations based on correction](https://arxiv.org/abs/2602.22445)
*Martin Kuettler,Hermann Haertig*

Main category: cs.DC

TL;DR: 该论文提出了一种容错的Reduce算法，通过在树形阶段前添加校正通信阶段，实现了对进程故障的容忍，并将此方法与Broadcast结合形成Allreduce算法。


<details>
  <summary>Details</summary>
Motivation: 开发能够处理分布式系统中进程故障的容错集体操作算法，特别是针对Reduce操作。

Method: 采用与Broadcast相反的顺序，在树形通信阶段前实现类似校正的通信阶段，以实现容错Reduce操作。

Result: 提供了具有已证明语义的容错Reduce算法，并将Broadcast与Reduce结合形成了Allreduce算法。

Conclusion: 成功将容错技术从Broadcast扩展到Reduce操作，使集体计算在可能存在进程故障的系统中更加可靠。

Abstract: Implementations of Broadcast based on some information dissemination algorithm -- e.g., gossip or tree-based communication -- followed by a correction algorithm has been proposed previously. This work describes an approach to apply a similar idea to Reduce. In it, a correction-like communication phase precedes a tree-based phase. This provides a Reduce algorithm which is tolerant to a number of failed processes. Semantics of the resulting algorithm are provided and proven.
  Based on these results, Broadcast and Reduce are combined to provide Allreduce.

</details>


### [22] [FuxiShuffle: An Adaptive and Resilient Shuffle Service for Distributed Data Processing on Alibaba Cloud](https://arxiv.org/abs/2602.22580)
*Yuhao Lin,Zhipeng Tang,Jiayan Tong,Junqing Xiao,Bin Lu,Yuhang Li,Chao Li,Zhiguo Zhang,Junhua Wang,Hao Luo,James Cheng,Chuang Hu,Jiawei Jiang,Xiao Yan*

Main category: cs.DC

TL;DR: FuxiShuffle是一种为超大规模分布式计算环境设计的数据混洗服务，通过动态适应和高效故障恢复机制，显著减少了作业完成时间和资源消耗。


<details>
  <summary>Details</summary>
Motivation: 现有混洗系统无法适应高度动态的作业特性和集群资源条件，且在故障发生时容错机制被动低效。

Method: FuxiShuffle实现了基于运行时信息的动态混洗模式选择、下游工作器的进度感知调度、自动确定每个数据块的最适备份策略、多副本故障转移、精细内存管理和不丢失计算进度的增量恢复机制。

Result: 与基线系统相比，FuxiShuffle显著减少了端到端作业完成时间和总体资源消耗，微观实验验证了其在提高适应性和故障恢复能力方面的有效性。

Conclusion: FuxiShuffle为超大规模生产环境提供了一种高效、自适应和容错的数据混洗服务解决方案。

Abstract: Shuffle exchanges intermediate results between upstream and downstream operators in distributed data processing and is usually the bottleneck due to factors such as small random I/Os and network contention. Several systems have been designed to improve shuffle efficiency, but from our experiences of running ultra-large clusters at Alibaba Cloud MaxCompute platform, we observe that they can not adapt to highly dynamic job characteristics and cluster resource conditions, and their fault tolerance mechanisms are passive and inefficient when failures are inevitable. To tackle their limitations, we design and implement FuxiShuffle as a general data shuffle service for the ultra-large production environment of MaxCompute, featuring good adaptability and efficient failure resilience. Specifically, to achieve good adaptability, FuxiShuffle dynamically selects the shuffle mode based on runtime information, conducts progress-aware scheduling for the downstream workers, and automatically determines the most suitable backup strategy for each shuffle data chunk. To make failure resilience efficient, FuxiShuffle actively ensures data availability with multi-replica failover, prevents memory overflow with careful memory management, and employs an incremental recovery mechanism that does not lose computation progress. Our experiments show that, compared to baseline systems, FuxiShuffle significantly reduces not only end-to-end job completion time but also aggregate resource consumption. Micro experiments suggest that our designs are effective in improving adaptability and failure resilience.

</details>


### [23] [FLYING SERVING: On-the-Fly Parallelism Switching for Large Language Model Serving](https://arxiv.org/abs/2602.22593)
*Shouwei Gao,Junqi Yin,Feiyi Wang,Wenqian Dong*

Main category: cs.DC

TL;DR: Flying Serving是一种vLLM-based系统，实现了在线数据并行(DP)与张量并行(TP)的无缝切换，无需重启引擎工作器，显著提升了LLM服务的性能和适应性。


<details>
  <summary>Details</summary>
Motivation: 现有LLM服务系统通常采用静态并行配置，难以适应非平稳流量和混合请求需求，无法同时满足高吞吐量、低延迟和大上下文容量的要求。

Method: Flying Serving通过四项关键技术实现DP-TP在线切换：零拷贝模型权重管理器、KV缓存适配器、预初始化通信器池和无死锁调度器，虚拟化状态以避免数据迁移。

Result: 在三种流行LLM和实际服务场景中，Flying Serving在高负载下性能提升最高达4.79倍，低负载下提升3.47倍，同时支持延迟敏感和内存密集型请求。

Conclusion: Flying Serving通过动态并行策略切换，有效解决了LLM服务系统在吞吐量、延迟和上下文容量之间的权衡问题，实现了更高效的资源利用和服务质量。

Abstract: Production LLM serving must simultaneously deliver high throughput, low latency, and sufficient context capacity under non-stationary traffic and mixed request requirements. Data parallelism (DP) maximizes throughput by running independent replicas, while tensor parallelism (TP) reduces per-request latency and pools memory for long-context inference. However, existing serving stacks typically commit to a static parallelism configuration at deployment; adapting to bursts, priorities, or long-context requests is often disruptive and slow. We present Flying Serving, a vLLM-based system that enables online DP-TP switching without restarting engine workers. Flying Serving makes reconfiguration practical by virtualizing the state that would otherwise force data movement: (i) a zero-copy Model Weights Manager that exposes TP shard views on demand, (ii) a KV Cache Adaptor that preserves request KV state across DP/TP layouts, (iii) an eagerly initialized Communicator Pool to amortize collective setup, and (iv) a deadlock-free scheduler that coordinates safe transitions under execution skew. Across three popular LLMs and realistic serving scenarios, Flying Serving improves performance by up to $4.79\times$ under high load and $3.47\times$ under low load while supporting latency- and memory-driven requests.

</details>


### [24] [Distributed LLM Pretraining During Renewable Curtailment Windows: A Feasibility Study](https://arxiv.org/abs/2602.22760)
*Philipp Wiesner,Soeren Becker,Brett Cornick,Dominik Scheinert,Alexander Acker,Odej Kao*

Main category: cs.DC

TL;DR: 该研究提出了一种系统，在可再生能源削减期间利用分布式GPU集群训练大语言模型，显著降低碳排放同时保持训练质量。


<details>
  <summary>Details</summary>
Motivation: 训练大语言模型需要大量计算和能源，同时可再生能源经常产生超过电网可吸收的电力，导致削减浪费。将训练与削减窗口对齐可利用清洁且廉价的电力。

Method: 开发了一个在地理位置分布的GPU集群上执行全参数LLM训练的系统，在区域削减窗口期间训练，弹性切换本地单站点训练和联邦多站点同步，使用Flower联邦学习框架，基于真实边际碳强度轨迹确定削减期。

Result: 在三个集群上成功训练了561M参数的transformer模型，削减感知调度在保持训练质量的同时，将运营排放降低到单站点基线的5-12%。

Conclusion: 该系统证明了LLM训练可以与可再生能源削减期对齐，显著降低碳排放，弹性联邦学习方法允许根据站点可用性动态调整。

Abstract: Training large language models (LLMs) requires substantial compute and energy. At the same time, renewable energy sources regularly produce more electricity than the grid can absorb, leading to curtailment, the deliberate reduction of clean generation that would otherwise go to waste. These periods represent an opportunity: if training is aligned with curtailment windows, LLMs can be pretrained using electricity that is both clean and cheap. This technical report presents a system that performs full-parameter LLM training across geo-distributed GPU clusters during regional curtailment windows, elastically switching between local single-site training and federated multi-site synchronization as sites become available or unavailable. Our prototype trains a 561M-parameter transformer model across three clusters using the Flower federated learning framework, with curtailment periods derived from real-world marginal carbon intensity traces. Preliminary results show that curtailment-aware scheduling preserves training quality while reducing operational emissions to 5-12% of single-site baselines.

</details>


### [25] [An Artificial Intelligence Framework for Joint Structural-Temporal Load Forecasting in Cloud Native Platforms](https://arxiv.org/abs/2602.22780)
*Qingyuan Zhang*

Main category: cs.DC

TL;DR: 提出了一种面向微服务拓扑的联合负载预测框架，通过服务调用图和分层表示建模多尺度负载模式，改进云原生环境中的资源评估和调度


<details>
  <summary>Details</summary>
Motivation: 云原生环境中微服务调用关系复杂、多尺度负载波动叠加、跨服务影响显著，传统负载预测方法难以应对这些挑战

Method: 将系统表示为时变服务调用图与多元负载序列的耦合实体，构建实例、服务和集群层级的分层负载表示，引入结构先验到注意力计算中，采用多目标回归策略联合优化

Result: 单因素敏感性分析证实了多粒度融合和结构注入的必要性，并明确了有效配置范围

Conclusion: 该框架为云环境中的容量评估、资源编排和运行时态势理解提供了可重用的建模范式和实施路径

Abstract: This study targets cloud native environments where microservice invocation relations are complex, load fluctuations are multi-scale and superimposed, and cross-service impacts are significant. We propose a structured temporal joint load prediction framework oriented to microservice topology. The method represents the system as a coupled entity of a time-evolving service invocation graph and multivariate load sequences. It constructs neighborhood-aggregated and global summarized views based on service level observations. This forms layered load representations across instance, service, and cluster levels. A unified sequence encoder models multi-scale historical context. To strengthen the expression of invocation dependencies, the framework introduces a lightweight structural prior into attention computation. This enables more effective capture of load propagation and accumulation along invocation chains, while maintaining consistent modeling of local bursts and overall trends. The training objective adopts a multi-objective regression strategy that jointly optimizes service level and cluster level predictions to improve cross-granularity stability. We further conduct single-factor sensitivity analyses on key structural and training hyperparameters. We systematically examine the effects of time window length, encoding depth, and regularization strength. The results support the necessity of multi-granularity fusion and structural injection and clarify their effective configuration ranges. Overall, the framework provides a reusable modeling paradigm and implementation path for capacity assessment, resource orchestration, and runtime situational understanding in cloud environments.

</details>


### [26] [Workload Buoyancy: Keeping Apps Afloat by Identifying Shared Resource Bottlenecks](https://arxiv.org/abs/2602.22852)
*Oliver Larsson,Thijs Metsch,Cristian Klein,Erik Elmroth*

Main category: cs.DC

TL;DR: 本文提出'浮力'(buoyancy)新抽象概念，通过整合应用级指标与系统级资源争用洞察，在多租户系统中提供更全面的工作负载性能表征，较传统指标提升19.3%的瓶颈识别能力。


<details>
  <summary>Details</summary>
Motivation: 多租户、硬件异构计算环境中的工作负载编排面临挑战，传统CPU利用率等简单指标无法捕获资源争用和噪声邻居效应导致的复杂性能动态，引发难以诊断的性能退化。

Method: 开发'浮力'抽象概念，整合应用级指标与共享资源争用的系统级洞察，显式捕获多资源瓶颈和余量，提供直观、可扩展且通用的性能动态表征方法。

Result: 通过代表性多租户工作负载评估，浮力比传统启发式方法平均提高19.3%的瓶颈指示能力，可作为传统性能指标的替代品，提升可观测性并支持更明智的调度和优化决策。

Conclusion: 浮力使多租户系统能够实现资源感知和应用感知的工作负载编排，改善异构平台上的性能优化效果，为复杂计算环境提供了有效的性能管理解决方案。

Abstract: Modern multi-tenant, hardware-heterogeneous computing environments pose significant challenges for effective workload orchestration. Simple heuristics for assessing workload performance, such as CPU utilization or application-level metrics, are often insufficient to capture the complex performance dynamics arising from resource contention and noisy-neighbor effects. In such environments, performance bottlenecks may emerge in any shared system resource, leading to unexpected and difficult-to-diagnose degradation.
  This paper introduces buoyancy, a novel abstraction for characterizing workload performance in multi-tenant systems. Unlike traditional approaches, buoyancy integrates application-level metrics with system-level insights of shared resource contention to provide a holistic view of performance dynamics. By explicitly capturing bottlenecks and headroom across multiple resources, buoyancy facilitates resource-aware and application-aware orchestration in a manner that is intuitive, extensible, and generalizable across heterogeneous platforms. We evaluate buoyancy using representative multi-tenant workloads to illustrate its ability to expose performance-limiting resource interactions. Buoyancy provides a 19.3% better indication of bottlenecks compared to traditional heuristics on average. We additionally show how buoyancy can act as a drop-in replacement for conventional performance metrics, enabling improved observability and more informed scheduling and optimization decisions.

</details>


### [27] [A Simple Distributed Deterministic Planar Separator](https://arxiv.org/abs/2602.22916)
*Yaseen Abd-Elhaleem,Michal Dory,Oren Weimann*

Main category: cs.DC

TL;DR: 该论文提出了一种简单确定性的平面图平衡分隔算法，在Õ(D)轮内找到大小为O(D)的分隔符，相比之前复杂的随机化算法或复杂的确定性算法更为简洁高效。


<details>
  <summary>Details</summary>
Motivation: 分布式模型中需要确定性算法来避免随机化，而现有的O(D)大小分隔符算法是随机化的，确定性算法则过于复杂。

Method: 每个简单地将自身权重转移到其所在的一个任意面上，这是一种直接且简单的权重转移方法。

Result: 实现了与先前复杂算法相同的Õ(D)轮时间复杂度，但算法更简单，且直接对平面图上多种经典问题的分布式算法进行了去随机化。

Conclusion: 该确定性分隔符算法不仅简化了方法，还为平面图上的多个优化问题提供了更高效的确定性解决方案。

Abstract: A balanced separator of a graph $G$ is a set of vertices whose removal disconnects the graph into connected components that are a constant factor smaller than $G$. Lipton and Tarjan [FOCS'77] famously proved that every planar graph admits a balanced separator of size $O(\sqrt{n})$, as well as a balanced separator of size $O(D)$ that is a simple path (where $D$ is $G$'s diameter). In the centralized setting, both separators can be found in linear time. In the distributed setting, $D$ is a universal lower bound for the round complexity of solving many optimization problems, so, separators of size $O(D)$ are preferable.
  It was not until [DISC'17] that a distributed algorithm was devised by Ghaffari and Parter to compute such an $O(D)$-size separator in $\tilde O(D)$ rounds, by adapting the Lipton-Tarjan algorithm to the distributed model. Since then, this algorithm was used in several distributed algorithms for planar graphs, e.g., [GP, DISC'17], [LP, STOC'19], [AEDPW, PODC'25]. However, the algorithm is randomized, deeming the algorithms that use it to be randomized as well. Obtaining a deterministic algorithm remained an interesting open question until [PODC'25], when a (complex) deterministic separator algorithm was given by Jauregui, Montealegre and Rapaport.
  We present a much simpler deterministic separator algorithm with the same (near-optimal) $\tilde O(D)$-round complexity. While previous works devised either complicated or randomized ways of transferring weights from vertices to faces of $G$, we show that a straightforward way also works: Each vertex simply transfers its weight to one arbitrary face it lies on. That's it!
  We note that a deterministic separator algorithm directly derandomizes the state-of-the-art distributed algorithms for classical problems on planar graphs such as single-source shortest-paths, maximum-flow, directed global min-cut, and reachability.

</details>


### [28] [LLMServingSim 2.0: A Unified Simulator for Heterogeneous and Disaggregated LLM Serving Infrastructure](https://arxiv.org/abs/2602.23036)
*Jaehong Cho,Hyunmin Choi,Guseul Heo,Jongse Park*

Main category: cs.DC

TL;DR: 该论文提出了LLMServingSim 2.0，一个统一系统级模拟器，用于模拟异构和分布式大语言模型服务基础设施中的运行时硬件-软件交互，具有高精度（0.97%误差）和合理模拟时间（约10分钟）。


<details>
  <summary>Details</summary>
Motivation: 现有模拟器无法在统一的运行时驱动框架中联合建模异构硬件和分布式服务技术，使得难以理解硬件-软件交互如何影响大语言模型服务性能。

Method: LLMServingSim 2.0将服务决策和硬件行为嵌入单一运行时循环，实现批处理、路由、卸载、内存和电力的交互感知建模。它通过基于配置文件的建模支持新兴加速器的可扩展集成。

Result: 该模拟器与真实部署相比，重现了关键性能、内存和电力指标，平均误差为0.97%，即使在复杂配置下也能保持约10分钟的模拟时间。

Conclusion: LLMServingSim 2.0为硬件创新和服务系统设计提供了实际桥梁， enabling 系统化探索和下一代大语言模型服务基础设施的协同设计。

Abstract: Large language model (LLM) serving infrastructures are undergoing a shift toward heterogeneity and disaggregation. Modern deployments increasingly integrate diverse accelerators and near-memory processing technologies, introducing significant hardware heterogeneity, while system software increasingly separates computation, memory, and model components across distributed resources to improve scalability and efficiency. As a result, LLM serving performance is no longer determined by hardware or software choices in isolation, but by their runtime interaction through scheduling, data movement, and interconnect behavior. However, understanding these interactions remains challenging, as existing simulators lack the ability to jointly model heterogeneous hardware and disaggregated serving techniques within a unified, runtime-driven framework.
  This paper presents LLMServingSim 2.0, a unified system-level simulator designed to make runtime-driven hardware-software interactions in heterogeneous and disaggregated LLM serving infrastructures explicit and analyzable. LLMServingSim 2.0 embeds serving decisions and hardware behavior into a single runtime loop, enabling interaction-aware modeling of batching, routing, offloading, memory, and power. The simulator supports extensible integration of emerging accelerators and memory systems through profile-based modeling, while capturing dynamic serving behavior and system-level effects. We validate LLMServingSim 2.0 against real deployments, showing that it reproduces key performance, memory, and power metrics with an average error of 0.97%, while maintaining simulation times of around 10 minutes even for complex configurations. These results demonstrate that LLMServingSim 2.0 provides a practical bridge between hardware innovation and serving-system design, enabling systematic exploration and co-design for next-generation LLM serving infrastructures.

</details>


### [29] [STELLAR: Storage Tuning Engine Leveraging LLM Autonomous Reasoning for High Performance Parallel File Systems](https://arxiv.org/abs/2602.23220)
*Chris Egersdoerfer,Philip Carns,Shane Snyder,Robert Ross,Dong Dai*

Main category: cs.DC

TL;DR: STELLAR是一个基于大语言模型的自助式I/O调优系统，能在前五次尝试中为并行文件系统选择接近最优的参数配置，显著优于需要数十万次迭代才能收敛的传统自动调优方法。


<details>
  <summary>Details</summary>
Motivation: I/O性能调优在数据密集型科学计算中至关重要，但大规模存储系统的调优过程复杂、成本高且需要大量专业人力，使大多数领域科学家难以访问。

Method: STELLAR利用大语言模型实现自主端到端智能调优，通过六个步骤：(1)从软件手册中提取可调参数；(2)分析应用程序生成的I/O跟踪日志；(3)选择初始调优策略；(4)在真实系统上重新运行应用程序并收集I/O性能反馈；(5)调整调优策略并重复调优周期；(6)反思并总结调优经验为未来优化可重用知识。系统整合了检索增强生成(RAG)、工具执行、基于LLM的推理和多智能体设计。

Result: STELLAR几乎总是能在前五次尝试中为并行文件系统选择接近最优的参数配置，即使是对于之前未见过的应用程序。系统评估了每个组件对优化结果的影响，为其他优化领域的类似系统提供了设计见解。

Conclusion: STELLAR的架构和实证结果为复杂系统优化提供了一种有前景的方法，特别适用于搜索空间大、探索成本高的问题，同时使领域科学家能够以最少的额外资源访问I/O调优能力。

Abstract: I/O performance is crucial to efficiency in data-intensive scientific computing; but tuning large-scale storage systems is complex, costly, and notoriously manpower-intensive, making it inaccessible for most domain scientists. To address this problem, we propose STELLAR, an autonomous tuner for high-performance parallel file systems. Our evaluations show that STELLAR almost always selects near-optimal parameter configurations for parallel file systems within the first five attempts, even for previously unseen applications.
  STELLAR differs fundamentally from traditional autotuning methods, which often require hundreds of thousands of iterations to converge. Powered by large language models (LLMs), STELLAR enables autonomous end-to-end agentic tuning by (1) accurately extracting tunable parameters from software manuals, (2) analyzing I/O trace logs generated by applications, (3) selecting initial tuning strategies, (4) rerunning applications on real systems and collecting I/O performance feedback, (5) adjusting tuning strategies and repeating the tuning cycle, and (6) reflecting on and summarizing tuning experiences into reusable knowledge for future optimizations. STELLAR integrates retrieval-augmented generation (RAG), tool execution, LLM-based reasoning, and a multiagent design to stabilize reasoning and combat hallucinations.
  We evaluate the impact of each component on optimization outcomes, providing design insights for similar systems in other optimization domains. STELLAR's architecture and empirical results highlight a promising approach to complex system optimization, especially for problems with large search spaces and high exploration costs, while making I/O tuning more accessible to domain scientists with minimal added resources.

</details>
