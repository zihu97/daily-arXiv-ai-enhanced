{"id": "2511.02952", "categories": ["cs.NI"], "pdf": "https://arxiv.org/pdf/2511.02952", "abs": "https://arxiv.org/abs/2511.02952", "authors": ["Zhenzhou Qi", "Yuncheng Yao", "Yiming Li", "Chung-Hsuan Tung", "Junyao Zheng", "Danyang Zhuo", "Tingjun Chen"], "title": "DecodeX: Exploring and Benchmarking of LDPC Decoding across CPU, GPU, and ASIC Platforms", "comment": null, "summary": "Emerging virtualized radio access networks (vRANs) demand flexible and\nefficient baseband processing across heterogeneous compute substrates. In this\npaper, we present DecodeX, a unified benchmarking framework for evaluating\nlow-density parity-check (LDPC) decoding acceleration across different hardware\nplatforms. DecodeX integrates a comprehensive suite of LDPC decoder\nimplementations, including kernels, APIs, and test vectors for CPUs (FlexRAN),\nGPUs (Aerial and Sionna-RK), and ASIC (ACC100), and can be readily extended to\nadditional architectures and configurations. Using DecodeX, we systematically\ncharacterize how different platforms orchestrate computation-from threading and\nmemory management to data movement and accelerator offload-and quantify the\nresulting decoding latency under varying Physical layer parameters. Our\nobservations reveal distinct trade-offs in parallel efficiency and offload\noverhead, showing that accelerator gains strongly depend on data-movement and\nworkload granularity. Building on these insights, we discuss how cross-platform\nbenchmarking can inform adaptive scheduling and co-design for future\nheterogeneous vRANs, enabling scalable and energy-efficient baseband processing\nfor NextG wireless systems."}
{"id": "2511.03039", "categories": ["cs.NI", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2511.03039", "abs": "https://arxiv.org/abs/2511.03039", "authors": ["Yiming Zheng", "Haoran Qi", "Lirui Yu", "Zhan Shu", "Qing Zhao"], "title": "Distributed Incast Detection in Data Center Networks", "comment": null, "summary": "Incast traffic in data centers can lead to severe performance degradation,\nsuch as packet loss and increased latency. Effectively addressing incast\nrequires prompt and accurate detection. Existing solutions, including MA-ECN,\nBurstRadar and Pulser, typically rely on fixed thresholds of switch port egress\nqueue lengths or their gradients to identify microburst caused by incast flows.\nHowever, these queue length related methods often suffer from delayed detection\nand high error rates. In this study, we propose a distributed incast detection\nmethod for data center networks at the switch-level, leveraging a probabilistic\nhypothesis test with an optimal detection threshold. By analyzing the arrival\nintervals of new flows, our algorithm can immediately determine if a flow is\npart of an incast traffic from its initial packet. The experimental results\ndemonstrate that our method offers significant improvements over existing\napproaches in both detection speed and inference accuracy."}
{"id": "2511.03081", "categories": ["cs.NI"], "pdf": "https://arxiv.org/pdf/2511.03081", "abs": "https://arxiv.org/abs/2511.03081", "authors": ["Pragya Sharma", "Amanda Xiang", "Abbas Kiani", "John Kaippallimalil", "Tony Saboorian", "Haining Wang"], "title": "CRSF: Enabling QoS-Aware Beyond-Connectivity Service Sharing in 6G Local Networks", "comment": null, "summary": "Sixth-generation (6G) networks are envisioned to support interconnected local\nsubnetworks that can share specialized, beyond-connectivity services. However,\na standardized architecture for discovering and selecting these services across\nnetwork boundaries has not existed yet. To address this gap, this paper\nintroduces the Central Repository and Selection Function (CRSF), a novel\nnetwork function for the 6G core that facilitates efficient inter-subnetwork\nservice discovery and selection. We formulate the selection process as a\nQoS-aware optimization problem designed to balance service quality metrics with\nuser-defined priorities. We evaluate our system model through simulations for a\nsensing service scenario and observe a consistently higher aggregate Quality of\nService (QoS) compared to the baseline selection strategy. The proposed CRSF\nprovides a foundational and extensible mechanism for building standardized,\ncollaborative, and service-centric interconnected networks essential for the 6G\nera."}
{"id": "2511.02854", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.02854", "abs": "https://arxiv.org/abs/2511.02854", "authors": ["Yixiang Chen", "Tianshi Zheng", "Shijue Huang", "Zhitao He", "Yi R. Fung"], "title": "SELF-REDRAFT: Eliciting Intrinsic Exploration-Exploitation Balance in Test-Time Scaling for Code Generation", "comment": "15 pages, 8 figures,2 tables", "summary": "Test-time scaling without interpreter feedback is essential for real-world\ncode generation scenarios where test cases are not readily available. While\nexisting paradigms often rely on either greedy exploitation (i.e., iterative\nrefinement) or stochastic exploration (i.e., relying on sample-based voting or\nreranking mechanisms), the balance between these two dimensions remains\nunderexplored. To investigate the LLM's intrinsic ability to balance\nexploitation and exploration, we introduce SELF-REDRAFT, a framework built upon\nSelf-Refine that encourages the model to propose new drafts for solutions that\nare fundamentally flawed. Our results show that SELF-REDRAFT consistently\nachieves better performance than Self-Refine when converged under the same\nmaximum number of iterations. Still, we observe that significant room for\nimprovement remains, largely due to two core aspects of current self-redraft\ncapabilities: constrained capacity for generating instructive feedback and\nfragile discriminative judgment. We also find that balancing strategies vary\nnotably across different LLMs, reflecting distinct, model-specific behaviors.\nOverall, our study establishes a baseline for intrinsic\nexploration-exploitation balancing in test-time scaling and identifies feedback\nand discrimination as key areas with potential for future advances."}
{"id": "2511.03116", "categories": ["cs.NI"], "pdf": "https://arxiv.org/pdf/2511.03116", "abs": "https://arxiv.org/abs/2511.03116", "authors": ["Moinak Ghoshal", "Imran Khan", "Phuc Dinh", "Z. Jonny Kong", "Omar Basit", "Sizhe Wang", "Yufei Feng", "Y. Charlie Hu", "Dimitrios Koutsonikolas"], "title": "Handover Configurations in Operational 5G Networks: Diversity, Evolution, and Impact on Performance", "comment": null, "summary": "Mobility management in cellular networks, especially the handover (HO)\nprocess, plays a key role in providing seamless and ubiquitous Internet access.\nThe wide-scale deployment of 5G and the resulting co-existence of 4G/5G in the\npast six years have significantly changed the landscape of all mobile network\noperators and made the HO process much more complex than before. While several\nrecent works have studied the impact of HOs on user experience, why and how HOs\noccur and how HO configurations affect performance in 5G operational networks\nremains largely unknown. Through four cross-country driving trips across the US\nspread out over a 27-month period, we conduct an in-depth measurement study of\nHO configurations across all three major US operators. Our study reveals (a)\nnew types of HOs and new HO events used by operators to handle these new types\nof HOs, (b) overly aggressive HO configurations that result in unnecessarily\nhigh signaling overhead, (c) large diversity in HO configuration parameter\nvalues, which also differ across operators, but significantly lower diversity\nin 5G compared to LTE, and (d) sub-optimal HO configurations/decisions leading\nto poor pre- or post-HO performance. Our findings have many implications for\nmobile operators, as they keep fine-tuning their 5G HO configurations."}
{"id": "2511.02859", "categories": ["cs.SE", "D.2.9"], "pdf": "https://arxiv.org/pdf/2511.02859", "abs": "https://arxiv.org/abs/2511.02859", "authors": ["Bianca Leech", "Ridewaan Hanslo"], "title": "The Evolution of Agile and Hybrid Project Management Methodologies: A Systematic Literature Review", "comment": "10 pages, 5 images, 1 table, 7th World Symposium on Software\n  Engineering (WSSE 2025)", "summary": "The rapid evolution of IT projects has driven the transformation of project\nmanagement methodologies, from traditional waterfall approaches to agile\nframeworks and, more recently, hybrid models. This systematic literature review\ninvestigates the evolution of agile methodologies into hybrid frameworks,\nanalysing their implementation challenges and success factors. We identify key\ntrends through PRISMA-guided analysis of peer-reviewed studies from the last 8\nyears. Hybrid methodologies emerge from agile limitations in large-scale and\nregulated environments, combining iterative flexibility with structured\ngovernance. Agile has several implementation challenges, leading to hybrid\nmethods, and the success hinges on leadership support, tailored process\nintegration, and continuous improvement mechanisms. The study explores the need\nfor contextual adaptation over rigid frameworks, offering practical insights\nfor organisations navigating hybrid transitions."}
{"id": "2511.03079", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2511.03079", "abs": "https://arxiv.org/abs/2511.03079", "authors": ["Changhong Li", "Biswajit Basu", "Shreejith Shanker"], "title": "LogicSparse: Enabling Engine-Free Unstructured Sparsity for Quantised Deep-learning Accelerators", "comment": "Accepted by ICFPT 2025", "summary": "FPGAs have been shown to be a promising platform for deploying Quantised\nNeural Networks (QNNs) with high-speed, low-latency, and energy-efficient\ninference. However, the complexity of modern deep-learning models limits the\nperformance on resource-constrained edge devices. While quantisation and\npruning alleviate these challenges, unstructured sparsity remains\nunderexploited due to irregular memory access. This work introduces a framework\nthat embeds unstructured sparsity into dataflow accelerators, eliminating the\nneed for dedicated sparse engines and preserving parallelism. A hardware-aware\npruning strategy is introduced to improve efficiency and design flow further.\nOn LeNet-5, the framework attains 51.6 x compression and 1.23 x throughput\nimprovement using only 5.12% of LUTs, effectively exploiting unstructured\nsparsity for QNN acceleration."}
{"id": "2511.03029", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2511.03029", "abs": "https://arxiv.org/abs/2511.03029", "authors": ["Kajol Kulkarni", "Samuel Kemmler", "Anna Schwarz", "Gulcin Gedik", "Yanxiang Chen", "Dimitrios Papageorgiou", "Ioannis Kavroulakis", "Roman Iakymchuk"], "title": "Harvesting energy consumption on European HPC systems: Sharing Experience from the CEEC project", "comment": "10 pages, 11 figures, conference", "summary": "Energy efficiency has emerged as a central challenge for modern\nhigh-performance computing (HPC) systems, where escalating computational\ndemands and architectural complexity have led to significant energy footprints.\nThis paper presents the collective experience of the EuroHPC JU Center of\nExcellence in Exascale CFD (CEEC) in measuring, analyzing, and optimizing\nenergy consumption across major European HPC systems. We briefly review key\nmethodologies and tools for energy measurement as well as define metrics for\nreporting results. Through case studies using representative CFD applications\n(waLBerla, FLEXI/GAL{\\AE}XI, Neko, and NekRS), we evaluate energy-to-solution\nand time-to-solution metrics on diverse architectures, including CPU- and\nGPU-based partitions of LUMI, MareNostrum5, MeluXina, and JUWELS Booster. Our\nresults highlight the advantages of accelerators and mixed-precision techniques\nfor reducing energy consumption while maintaining computational accuracy.\nFinally, we advocate the need to facilitate energy measurements on HPC systems\nin order to raise awareness, teach the community, and take actions toward more\nsustainable exascale computing."}
{"id": "2511.03586", "categories": ["cs.PF", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.03586", "abs": "https://arxiv.org/abs/2511.03586", "authors": ["Andrei Ivanov", "Siyuan Shen", "Gioele Gottardo", "Marcin Chrapek", "Afif Boudaoud", "Timo Schneider", "Luca Benini", "Torsten Hoefler"], "title": "PerfDojo: Automated ML Library Generation for Heterogeneous Architectures", "comment": null, "summary": "The increasing complexity of machine learning models and the proliferation of\ndiverse hardware architectures (CPUs, GPUs, accelerators) make achieving\noptimal performance a significant challenge. Heterogeneity in instruction sets,\nspecialized kernel requirements for different data types and model features\n(e.g., sparsity, quantization), and architecture-specific optimizations\ncomplicate performance tuning. Manual optimization is resource-intensive, while\nexisting automatic approaches often rely on complex hardware-specific\nheuristics and uninterpretable intermediate representations, hindering\nperformance portability. We introduce PerfLLM, a novel automatic optimization\nmethodology leveraging Large Language Models (LLMs) and Reinforcement Learning\n(RL). Central to this is PerfDojo, an environment framing optimization as an RL\ngame using a human-readable, mathematically-inspired code representation that\nguarantees semantic validity through transformations. This allows effective\noptimization without prior hardware knowledge, facilitating both human analysis\nand RL agent training. We demonstrate PerfLLM's ability to achieve significant\nperformance gains across diverse CPU (x86, Arm, RISC-V) and GPU architectures."}
{"id": "2511.03094", "categories": ["cs.MA"], "pdf": "https://arxiv.org/pdf/2511.03094", "abs": "https://arxiv.org/abs/2511.03094", "authors": ["Longling Geng", "Edward Y. Chang"], "title": "ALAS: Transactional and Dynamic Multi-Agent LLM Planning", "comment": null, "summary": "Large language models enable flexible multi-agent planning but remain fragile\nin practice: verification is often circular, state changes are not tracked for\nrepair, and small faults trigger costly global recomputation. We present ALAS,\na stateful, disruption-aware framework that separates planning from\nnon-circular validation, records a versioned execution log for grounded checks\nand restore points, and performs localized repair that preserves work in\nprogress. The validator operates independently of the planning LLM with fresh,\nbounded context, avoiding self-check loops and mid-context attrition. The\nrepair protocol edits only the minimal affected region under explicit policies\n(retry, catch, timeout, backoff, idempotency keys, compensation, loop guards)\ndefined in a canonical workflow IR that maps to Amazon States Language and Argo\nWorkflows. On job-shop scheduling suites (DMU, TA) across five classical\nbenchmarks, ALAS matches or exceeds strong single-LLM and multi-agent\nbaselines, achieving 83.7% success, reducing token usage by 60%, and running\n1.82times faster under comparable settings. A minimal reliability study shows\nthat the validator detects injected structural faults with low overhead, and\nthat localized repair contains runtime perturbations with a bounded edit radius\nand less makespan degradation than global recompute. Results indicate that the\ncombination of validator isolation, versioned execution logs, and localized\nrepair provides measurable efficiency, feasibility, and scalability for\nmulti-agent LLM planning. Code and seeds will be released."}
{"id": "2511.03159", "categories": ["cs.NI"], "pdf": "https://arxiv.org/pdf/2511.03159", "abs": "https://arxiv.org/abs/2511.03159", "authors": ["Shuting Qiu", "Fang Dong", "Siyu Tan", "Ruiting Zhou", "Dian Shen", "Patrick P. C. Lee", "Qilin Fan"], "title": "Joint Optimization of DNN Model Caching and Request Routing in Mobile Edge Computing", "comment": null, "summary": "Mobile edge computing (MEC) can pre-cache deep neural networks (DNNs) near\nend-users, providing low-latency services and improving users' quality of\nexperience (QoE). However, caching all DNN models at edge servers with limited\ncapacity is difficult, and the impact of model loading time on QoE remains\nunderexplored. Hence, we introduce dynamic DNNs in edge scenarios,\ndisassembling a complete DNN model into interrelated submodels for more\nfine-grained and flexible model caching and request routing solutions. This\nraises the pressing issue of jointly deciding request routing and submodel\ncaching for dynamic DNNs to balance model inference precision and loading\nlatency for QoE optimization. In this paper, we study the joint dynamic model\ncaching and request routing problem in MEC networks, aiming to maximize user\nrequest inference precision under constraints of server resources, latency, and\nmodel loading time. To tackle this problem, we propose CoCaR, an offline\nalgorithm based on linear programming and random rounding that leverages\ndynamic DNNs to optimize caching and routing schemes, achieving near-optimal\nperformance. Furthermore, we develop an online variant of CoCaR, named\nCoCaR-OL, enabling effective adaptation to dynamic and unpredictable online\nrequest patterns. The simulation results demonstrate that the proposed CoCaR\nimproves the average inference precision of user requests by 46\\% compared to\nstate-of-the-art baselines. In addition, in online scenarios, CoCaR-OL achieves\nan improvement of no less than 32.3\\% in user QoE over competitive baselines."}
{"id": "2511.02866", "categories": ["cs.SE", "cs.AI", "cs.AR", "cs.CR"], "pdf": "https://arxiv.org/pdf/2511.02866", "abs": "https://arxiv.org/abs/2511.02866", "authors": ["Ahmad Tahmasivand", "Noureldin Zahran", "Saba Al-Sayouri", "Mohammed Fouda", "Khaled N. Khasawneh"], "title": "LM-Fix: Lightweight Bit-Flip Detection and Rapid Recovery Framework for Language Models", "comment": "Accepted at IEEE ICCD 2025. Code: https://github.com/ata990/lm-fix.\n  Detects over 94 percent single-bit flips (near 100 percent multi-bit) with\n  about 1 to 7.7 percent overhead; recovery is over 100x faster than a full\n  reload. Keywords: LLMs, bit-flip, fault injection, reliability, security,\n  Rowhammer, SDC, Jailbreaking, Attack, Defense, GPU DRAM faults", "summary": "This paper presents LM-Fix, a lightweight detection and rapid recovery\nframework for faults in large language models (LLMs). Existing integrity\napproaches are often heavy or slow for modern LLMs. LM-Fix runs a short\ntest-vector pass and uses hash-guided checks to detect bit-flip faults, then\nrepairs them locally without a full reload. Across multiple models, it detects\nover 94% of single-bit flips at TVL=200 and nearly 100% of multi-bit flips with\napproximately 1% to 7.7% runtime overhead; recovery is more than 100x faster\nthan reloading. These results show a practical, low-overhead solution to keep\nLLMs reliable in production"}
{"id": "2511.03203", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2511.03203", "abs": "https://arxiv.org/abs/2511.03203", "authors": ["Deyang Yu", "Chenchen Liu", "Chuanjie Zhang", "Xiao Fang", "Weisheng Zhao"], "title": "An Event-Driven Spiking Compute-In-Memory Macro based on SOT-MRAM", "comment": "5 pages, 7 figures. Under review for ISCAS", "summary": "The application of Magnetic Random-Access Memory (MRAM) in\ncomputing-in-memory (CIM) has gained significant attention. However, existing\ndesigns often suffer from high energy consumption due to their reliance on\ncomplex analog circuits for computation. In this work, we present a Spin-Orbit-\nTorque MRAM(SOT-MRAM)-based CIM macro that employs an event-driven spiking\nprocessing for high energy efficiency. The SOT-MRAM crossbar adopts a hybrid\nseries-parallel cell structure to efficiently support matrix-vector\nmultiplication (MVM). Signal information is (en) decoded as spikes using\nlightweight circuits, eliminating the need for conventional area- and\npowerintensive analog circuits. The SOT-MRAM macro is designed and evaluated in\n28nm technology, and experimental results show that it achieves a peak energy\nefficiency of 243.6 TOPS/W, significantly outperforming existing designs."}
{"id": "2511.03286", "categories": ["cs.DC", "cs.MA", "cs.SE", "cs.SI"], "pdf": "https://arxiv.org/pdf/2511.03286", "abs": "https://arxiv.org/abs/2511.03286", "authors": ["Ehud Shapiro"], "title": "Characterising Global Platforms: Centralised, Decentralised, Federated, and Grassroots", "comment": null, "summary": "Global digital platforms are software systems designed to serve entire\npopulations, with some already serving billions of people. We propose atomic\ntransactions-based multiagent transition systems and protocols as a formal\nframework to study them; introduce essential agents -- minimal sets of agents\nthe removal of which makes communication impossible; and show that the\ncardinality of essential agents partitions all global platforms into four\nclasses:\n  1. Centralised -- one (the server)\n  2. Decentralised -- finite $>1$ (bootstrap nodes)\n  3. Federated -- infinite but not universal (all servers)\n  4. Grassroots -- universal (all agents)\n  Our illustrative formal example is a global social network, for which we\nprovide centralised, decentralised, federated, and grassroots specifications\nvia multiagent atomic transactions, and prove they satisfy basic correctness\nproperties. We discuss informally additional global platforms -- currencies,\n``sharing economy'' apps, AI, and more. While this may be the first\ncharacterisation of centralised, decentralised, and federated global platforms,\ngrassroots platforms have been formally defined previously, but using different\nnotions. Here, we prove that their original definition implies that all agents\nare essential, placing grassroots platforms in a distinct class within the\nbroader formal context that includes all global platforms. This work provides\nthe first mathematical framework for classifying any global platform --\nexisting or imagined -- by providing a multiagent atomic-transactions\nspecification of it and determining the cardinality of the minimal set of\nessential agents in the ensuing multiagent protocol. It thus"}
{"id": "2511.03348", "categories": ["cs.MA", "68T05"], "pdf": "https://arxiv.org/pdf/2511.03348", "abs": "https://arxiv.org/abs/2511.03348", "authors": ["Changxi Zhu", "Mehdi Dastani", "Shihan Wang"], "title": "Learning Communication Skills in Multi-task Multi-agent Deep Reinforcement Learning", "comment": "20 pages, 10 figures", "summary": "In multi-agent deep reinforcement learning (MADRL), agents can communicate\nwith one another to perform a task in a coordinated manner. When multiple tasks\nare involved, agents can also leverage knowledge from one task to improve\nlearning in other tasks. In this paper, we propose Multi-task Communication\nSkills (MCS), a MADRL with communication method that learns and performs\nmultiple tasks simultaneously, with agents interacting through learnable\ncommunication protocols. MCS employs a Transformer encoder to encode\ntask-specific observations into a shared message space, capturing shared\ncommunication skills among agents. To enhance coordination among agents, we\nintroduce a prediction network that correlates messages with the actions of\nsender agents in each task. We adapt three multi-agent benchmark environments\nto multi-task settings, where the number of agents as well as the observation\nand action spaces vary across tasks. Experimental results demonstrate that MCS\nachieves better performance than multi-task MADRL baselines without\ncommunication, as well as single-task MADRL baselines with and without\ncommunication."}
{"id": "2511.03312", "categories": ["cs.NI", "cs.SE"], "pdf": "https://arxiv.org/pdf/2511.03312", "abs": "https://arxiv.org/abs/2511.03312", "authors": ["Jiali Xu", "Valeria Loscri", "Romain Rouvoy"], "title": "Integrity Under Siege: A Rogue gNodeB's Manipulation of 5G Network Slice Allocation", "comment": "15 pages, 11 figures, Elsevier journal paper layout", "summary": "The advent of 5G networks, with network slicing as a cornerstone technology,\npromises customized, high-performance services, but also introduces novel\nattack surfaces beyond traditional threats. This article investigates a\ncritical and underexplored integrity vulnerability: the manipulation of network\nslice allocation to compromise Quality of Service (QoS) and resource integrity.\nWe introduce a threat model, grounded in a risk analysis of permissible yet\ninsecure configurations like null-ciphering (5G-EA0), demonstrating how a rogue\ngNodeB acting as a Man-in-the-Middle can exploit protocol weaknesses to forge\nslice requests and hijack a User Equipment's (UE) connection. Through a\ncomprehensive experimental evaluation on a 5G testbed, we demonstrate the\nattack's versatile and severe impacts. Our findings show this integrity breach\ncan manifest as obvious QoS degradation, such as a 95% bandwidth reduction and\n150% latency increase when forcing UE to a suboptimal slice, or as stealthy\nslice manipulation that is indistinguishable from benign network operation and\ngenerates no core network errors. Furthermore, we validate a systemic resource\ncontamination attack where redirecting a crowd of UE orchestrates a\nDenial-of-Service, causing packet loss to exceed 60% and inducing measurable\nCPU saturation (~80%) on core network User Plane Functions (UPFs). Based on\nthese results, we discuss the profound implications for Service Level\nAgreements (SLAs) and critical infrastructure. We propose concrete, cross-layer\nmitigation strategies for network operators as future work, underscoring the\nurgent need to secure the integrity of dynamic resource management in 5G\nnetworks."}
{"id": "2511.02869", "categories": ["cs.SE", "cs.AI", "cs.PL"], "pdf": "https://arxiv.org/pdf/2511.02869", "abs": "https://arxiv.org/abs/2511.02869", "authors": ["Amirreza Esmaeili", "Fahd Seddik", "Yongyi Ji", "Fatemeh Fard", "Fuxiang Chen"], "title": "Analysis of AdvFusion: Adapter-based Multilingual Learning for Code Large Language Models", "comment": null, "summary": "Programming languages can benefit from one another by utilizing a language\nmodel for software engineering tasks. Full fine-tuning and Parameter Efficient\nFine-Tuning (PEFT) of Code Language Models (Code-LMs) has been explored for\nmultilingual knowledge transfer. AdapterFusion is a PEFT architecture that aims\nto enhance task performance by leveraging information from multiple programming\nlanguages, but primarily focuses on the target programming language.\n  In our previous work, we proposed AdvFusion, a novel PEFT-based approach that\neffectively learns from other programming languages before adapting to the\ntarget task. Though previous experiments showed that AdvFusion outperformed\nAdapterFusion and LoRA, it was applied on pre-trained Code-LMs and was limited\nto only two tasks, code summarization and method name prediction. In this\nstudy, we expanded our work and investigated AdvFusion on Code Large Language\nModels (Code-LLMs), considering three new tasks: code generation, code\ntranslation, and commit message generation. We observed that different\nCode-LLMs/tasks exhibit different characteristics. In code generation,\nAdvFusion outperformed AdapterFusion but not other PEFT methods (LoRA,\nCompacter, and TaskAdapter). In commit message generation, AdapterFusion\nperformed better than AdvFusion, and contrary to code generation, we found that\nthe other PEFT methods do not have better performance. In code translation,\nAdvFusion performed worse than AdapterFusion overall, with the performance gap\nmarginally widening as the model size increases. However, consistent with code\ngeneration, other PEFT methods showed better performance."}
{"id": "2511.03427", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2511.03427", "abs": "https://arxiv.org/abs/2511.03427", "authors": ["Florentia Afentaki", "Maha Shatta", "Konstantinos Balaskas", "Georgios Panagopoulos", "Georgios Zervakis", "Mehdi B. Tahoori"], "title": "Design and Optimization of Mixed-Kernel Mixed-Signal SVMs for Flexible Electronics", "comment": "Accepted for publication at IEEE Design Automation & Testing in\n  Europe (DATE 2026)", "summary": "Flexible Electronics (FE) have emerged as a promising alternative to\nsilicon-based technologies, offering on-demand low-cost fabrication,\nconformality, and sustainability. However, their large feature sizes severely\nlimit integration density, imposing strict area and power constraints, thus\nprohibiting the realization of Machine Learning (ML) circuits, which can\nsignificantly enhance the capabilities of relevant near-sensor applications.\nSupport Vector Machines (SVMs) offer high accuracy in such applications at\nrelatively low computational complexity, satisfying FE technologies'\nconstraints. Existing SVM designs rely solely on linear or Radial Basis\nFunction (RBF) kernels, forcing a trade-off between hardware costs and\naccuracy. Linear kernels, implemented digitally, minimize overhead but\nsacrifice performance, while the more accurate RBF kernels are prohibitively\nlarge in digital, and their analog realization contains inherent functional\napproximation. In this work, we propose the first mixed-kernel and mixed-signal\nSVM design in FE, which unifies the advantages of both implementations and\nbalances the cost/accuracy trade-off. To that end, we introduce a\nco-optimization approach that trains our mixed-kernel SVMs and maps binary SVM\nclassifiers to the appropriate kernel (linear/RBF) and domain (digital/analog),\naiming to maximize accuracy whilst reducing the number of costly RBF\nclassifiers. Our designs deliver 7.7% higher accuracy than state-of-the-art\nsingle-kernel linear SVMs, and reduce area and power by 108x and 17x on average\ncompared to digital RBF implementations."}
{"id": "2511.03293", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2511.03293", "abs": "https://arxiv.org/abs/2511.03293", "authors": ["Hai Huang", "Xuhong Qiang", "Weisheng Zhao", "Chenchen Liu"], "title": "UMDAM: A Unified Data Layout and DRAM Address Mapping for Heterogenous NPU-PIM", "comment": "5 pages, 5 figures, under review for IEEE ISCAS", "summary": "Large Language Models (LLMs) are increasingly deployed on edge devices with\nNeural Processing Units (NPUs), yet the decode phase remains memory-intensive,\nlimiting performance. Processing-in-Memory (PIM) offers a promising solution,\nbut co-executing NPU-PIM systems face challenges such as data layout\nmismatches, bandwidth loss, and redundant storage. To address these issues, we\npropose UMDAM, a unified memory-affinity data layout and DRAM address mapping\nscheme tailored for NPU-PIM co-execution. UMDAM employs a column-major,\ntile-based layout and a configurable DRAM mapping strategy to ensure\ncompatibility with NPU computation while maximizing PIM efficiency -- without\nintroducing extra memory overhead or bandwidth loss. Comprehensive evaluations\non OPT models demonstrate that UMDAM reduces time-to-first-token (TTFT) by up\nto 3.0x and time-to-last-token (TTLT) by 2.18x, significantly improving\nend-to-end LLM inference efficiency on edge devices."}
{"id": "2511.03286", "categories": ["cs.DC", "cs.MA", "cs.SE", "cs.SI"], "pdf": "https://arxiv.org/pdf/2511.03286", "abs": "https://arxiv.org/abs/2511.03286", "authors": ["Ehud Shapiro"], "title": "Characterising Global Platforms: Centralised, Decentralised, Federated, and Grassroots", "comment": null, "summary": "Global digital platforms are software systems designed to serve entire\npopulations, with some already serving billions of people. We propose atomic\ntransactions-based multiagent transition systems and protocols as a formal\nframework to study them; introduce essential agents -- minimal sets of agents\nthe removal of which makes communication impossible; and show that the\ncardinality of essential agents partitions all global platforms into four\nclasses:\n  1. Centralised -- one (the server)\n  2. Decentralised -- finite $>1$ (bootstrap nodes)\n  3. Federated -- infinite but not universal (all servers)\n  4. Grassroots -- universal (all agents)\n  Our illustrative formal example is a global social network, for which we\nprovide centralised, decentralised, federated, and grassroots specifications\nvia multiagent atomic transactions, and prove they satisfy basic correctness\nproperties. We discuss informally additional global platforms -- currencies,\n``sharing economy'' apps, AI, and more. While this may be the first\ncharacterisation of centralised, decentralised, and federated global platforms,\ngrassroots platforms have been formally defined previously, but using different\nnotions. Here, we prove that their original definition implies that all agents\nare essential, placing grassroots platforms in a distinct class within the\nbroader formal context that includes all global platforms. This work provides\nthe first mathematical framework for classifying any global platform --\nexisting or imagined -- by providing a multiagent atomic-transactions\nspecification of it and determining the cardinality of the minimal set of\nessential agents in the ensuing multiagent protocol. It thus"}
{"id": "2511.02874", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2511.02874", "abs": "https://arxiv.org/abs/2511.02874", "authors": ["Jannatul Shefa", "Taylan G. Topcu"], "title": "An Analysis of Early-Stage Functional Safety Analysis Methods and Their Integration into Model-Based Systems Engineering", "comment": null, "summary": "As systems become increasingly complex, conducting effective safety analysis\nin the earlier phases of a system's lifecycle is essential to identify and\nmitigate risks before they escalate. To that end, this paper investigates the\ncapabilities of key safety analysis techniques, namely: Failure Mode and\nEffects Analysis (FMEA), Functional Hazard Analysis (FHA), and Functional\nFailure Identification and Propagation (FFIP), along with the current state of\nthe literature in terms of their integration into Model-Based Systems\nEngineering (MBSE). A two-phase approach is adopted. The first phase is focused\non contrasting FMEA, FHA, and FFIP techniques, examining their procedures,\nalong with a documentation of their relative strengths and limitations. Our\nanalysis highlights FFIP's capability in identifying emergent system behaviors,\nsecond-order effects, and fault propagation; thus, suggesting it is better\nsuited for the safety needs of modern interconnected systems. Second, we review\nthe existing research on the efforts to integrate each of these methods into\nMBSE. We find that MBSE integration efforts primarily focus on FMEA, and\nintegration of FHA and FFIP is nascent. Additionally, FMEA-MBSE integration\nefforts could be organized into four categories: model-to-model transformation,\nuse of external customized algorithms, built-in MBSE packages, and manual use\nof standard MBSE diagrams. While our findings indicate a variety of MBSE\nintegration approaches, there is no universally established framework or\nstandard. This leaves room for an integration approach that could support the\nongoing Digital Engineering transformation efforts by enabling a more\nsynergistic lifecycle safety management methods and tools."}
{"id": "2511.02866", "categories": ["cs.SE", "cs.AI", "cs.AR", "cs.CR"], "pdf": "https://arxiv.org/pdf/2511.02866", "abs": "https://arxiv.org/abs/2511.02866", "authors": ["Ahmad Tahmasivand", "Noureldin Zahran", "Saba Al-Sayouri", "Mohammed Fouda", "Khaled N. Khasawneh"], "title": "LM-Fix: Lightweight Bit-Flip Detection and Rapid Recovery Framework for Language Models", "comment": "Accepted at IEEE ICCD 2025. Code: https://github.com/ata990/lm-fix.\n  Detects over 94 percent single-bit flips (near 100 percent multi-bit) with\n  about 1 to 7.7 percent overhead; recovery is over 100x faster than a full\n  reload. Keywords: LLMs, bit-flip, fault injection, reliability, security,\n  Rowhammer, SDC, Jailbreaking, Attack, Defense, GPU DRAM faults", "summary": "This paper presents LM-Fix, a lightweight detection and rapid recovery\nframework for faults in large language models (LLMs). Existing integrity\napproaches are often heavy or slow for modern LLMs. LM-Fix runs a short\ntest-vector pass and uses hash-guided checks to detect bit-flip faults, then\nrepairs them locally without a full reload. Across multiple models, it detects\nover 94% of single-bit flips at TVL=200 and nearly 100% of multi-bit flips with\napproximately 1% to 7.7% runtime overhead; recovery is more than 100x faster\nthan reloading. These results show a practical, low-overhead solution to keep\nLLMs reliable in production"}
{"id": "2511.03533", "categories": ["cs.DC", "cs.SE"], "pdf": "https://arxiv.org/pdf/2511.03533", "abs": "https://arxiv.org/abs/2511.03533", "authors": ["Nils Japke", "Furat Hamdan", "Diana Baumann", "David Bermbach"], "title": "Investigating the Impact of Isolation on Synchronized Benchmarks", "comment": "Accepted for publication in 2025 IEEE/ACM 18th International\n  Conference on Utility and Cloud Computing", "summary": "Benchmarking in cloud environments suffers from performance variability from\nmulti-tenant resource contention. Duet benchmarking mitigates this by running\ntwo workload versions concurrently on the same VM, exposing them to identical\nexternal interference. However, intra-VM contention between synchronized\nworkloads necessitates additional isolation mechanisms.\n  This work evaluates three such strategies: cgroups and CPU pinning, Docker\ncontainers, and Firecracker MicroVMs. We compare all strategies with an\nunisolated baseline experiment, by running benchmarks with a duet setup\nalongside a noise generator. This noise generator \"steals\" compute resources to\ndegrade performance measurements.\n  All experiments showed different latency distributions while under the\neffects of noise generation, but results show that process isolation generally\nlowered false positives, except for our experiments with Docker containers.\nEven though Docker containers rely internally on cgroups and CPU pinning, they\nwere more susceptible to performance degradation due to noise influence.\nTherefore, we recommend to use process isolation for synchronized workloads,\nwith the exception of Docker containers."}
{"id": "2511.02876", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2511.02876", "abs": "https://arxiv.org/abs/2511.02876", "authors": ["Anjali Chouhan", "Sruti Srinivasa Ragavan", "Amey Karkare"], "title": "CS Educator challenges and their solutions : A systematic mapping study", "comment": null, "summary": "Computer Science (CS) education is expanding rapidly, but educators continue\nto face persistent challenges in teaching and learning environments.Despite\ngrowing interest, limited systematic work exists to categorize and synthesize\nthe specific challenges faced by CS educators and the remedies adopted in\nresponse.This is problematic because it remains unclear which areas have been\nthoroughly addressed and which still lack sufficient scholarly attention. In\nthis study, we conducted a structured literature review of peer-reviewed\nresearch papers published over the last five years, focusing on challenges and\nremedies across ten categorized themes, including pedagogical, emotional,\ntechnological, and institutional dimensions.Our analysis revealed recurring\nissues in areas such as assessment practices, teacher training, classroom\nmanagement, and emotional well-being, along with various strategies such as\nprofessional development programs and policy interventions adopted to mitigate\nthem while also revealing several areas that have received insufficient\nattention.This review offers a consolidated understanding of the CS education\nlandscape, providing valuable insights for researchers, curriculum designers,\nand policymakers aiming to improve teaching effectiveness and educator support."}
{"id": "2511.03609", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2511.03609", "abs": "https://arxiv.org/abs/2511.03609", "authors": ["Cameron Calk", "Emmanuel Godard"], "title": "Stone Duality Proofs for Colorless Distributed Computability Theorems", "comment": null, "summary": "We introduce a new topological encoding by spectral spaces of executions of\n  round-based full-information adversaries, a model of distributed computations\nthat is functorially presented and that\n  contains many message adversaries. We give a characterization of the\nsolvability of colorless tasks against compact adversaries.\n  Message adversaries are distributed\n  models that are known to be very expressive despite being\n  round-based and crash-free. Colorless tasks are\n  an important class of distributed tasks. For a colorless task, the\n  specification does not depend upon the multiplicity of input or\n  output values, like the ubiquitous agreement tasks.\n  Therefore, our result is a significant\n  step toward unifying topological methods in distributed computing.\n  The main insight is to consider global states obtained after finite\nexecutions of a distributed protocol\n  not as abstract\n  simplicial complexes as previously done, but as spectral\n  spaces, considering the Alexandrov topology on the faces poset. Given\n  an adversary $\\mathcal M$ with a set of inputs $\\mathcal I$,\n  we define a limit object $\\Pi^\\infty_\\mathcal M(\\mathcal I)$\n  by projective limit in the category of spectral spaces. We derive a new\ngeneral distributed computability\n  theorem using Stone duality: there exists an algorithm solving a colorless\ntask $(\\mathcal I,\\mathcal O,\\Delta)$\n  against the compact adversary $\\mathcal M$ if and only if there exists a\nspectral\n  map $f:\\Pi^\\infty_\\mathcal M(\\mathcal I)\\longrightarrow\\mathcal O$ compatible\nwith $\\Delta$.\n  From this general characterization are derived many known colorless\ncomputability\n  theorems.\n  Quite surprisingly, colored and uncolored models have the same\n  computability power (they solve the same tasks). Our new proofs give\n  topological reasons for this equivalence, previously known through\n  algorithmic reductions."}
{"id": "2511.02885", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.02885", "abs": "https://arxiv.org/abs/2511.02885", "authors": ["Gwendal Jouneaux", "Jordi Cabot"], "title": "AgentSLA : Towards a Service Level Agreement for AI Agents", "comment": null, "summary": "AI components are increasingly becoming a key element of all types of\nsoftware systems to enhance their functionality. These AI components are often\nimplemented as AI Agents, offering more autonomy than a plain integration of\nLarge Language Models (LLMs), moving from a Model-as-a-Service paradigm to an\nAgent-as-a-Service one, bringing new challenges to the development of smart\nsoftware systems. Indeed, while support for the design, implementation, and\ndeployment of those agents exist, the specification of Quality of Service (QoS)\nand definition of Service Level Agreements (SLAs) aspects for those agents,\nimportant to ensure the quality of the resulting systems, remains an open\nchallenge. Part of this is due to the difficulty to clearly define quality in\nthe context of AI components, resulting in a lack of consensus on how to best\napproach Quality Assurance (QA) for these types of systems. To address this\nchallenge, this paper proposes both a quality model for AI agents based on the\nISO/IEC 25010 standard, and a domain specific language to support the\ndefinition of SLAs for the services provided by these AI agents."}
{"id": "2511.03662", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2511.03662", "abs": "https://arxiv.org/abs/2511.03662", "authors": ["Yannis Coutouly", "Emmanuel Godard"], "title": "A General Input-Dependent Colorless Computability Theorem and Applications to Core-Dependent Adversaries", "comment": "OPODIS-25 version", "summary": "Distributed computing tasks can be presented with a triple $(\\I,\\Ou,\\Delta)$.\nThe solvability of a colorless task on the Iterated Immediate Snapshot model\n(IIS) has been characterized by the Colorless Computability Theorem\n\\cite[Th.4.3.1]{HKRbook}. A recent paper~\\cite{CG-24} generalizes this theorem\nfor any message adversaries $\\ma \\subseteq IIS$ by geometric methods. In 2001,\nMost\\'efaoui, Rajsbaum, Raynal, and Roy \\cite{condbased} introduced\n\\emph{condition-based adversaries}. This setting considers a particular\nadversary that will be applied only to a subset of input configurations. In\nthis setting, they studied the $k$-set agreement task with condition-based\n$t$-resilient adversaries and obtained a sufficient condition on the conditions\nthat make $k$-Set Agreement solvable. In this paper we have three\ncontributions:\n  -We generalize the characterization of~\\cite{CG-24} to \\emph{input-dependent}\nadversaries, which means that the adversaries can change depending on the input\nconfiguration.\n  - We show that core-resilient adversaries of $IIS_n$ have the same\ncomputability power as the core-resilient adversaries of $IIS_n$ where crashes\nonly happen at the start.\n  - Using the two previous contributions, we provide a necessary and sufficient\ncharacterization of the condition-based, core-dependent adversaries that can\nsolve $k$-Set Agreement. We also distinguish four settings that may appear when\npresenting a distributed task as $(\\I,\\Ou,\\Delta)$. Finally, in a later\nsection, we present structural properties on the carrier map $\\Delta$. Such\nproperties allow simpler proof, without changing the computability power of the\ntask. Most of the proofs in this article leverage the topological framework\nused in distributed computing by using simple geometric constructions."}
{"id": "2511.02922", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2511.02922", "abs": "https://arxiv.org/abs/2511.02922", "authors": ["Yunhan Qiao", "Christopher Hundhausen", "Summit Haque", "Md Istiak Hossain Shihab"], "title": "Comprehension-Performance Gap in GenAI-Assisted Brownfield Programming: A Replication and Extension", "comment": "12 pages", "summary": "Code comprehension is essential for brownfield programming tasks, in which\ndevelopers maintain and enhance legacy code bases. Generative AI (GenAI) coding\nassistants such as GitHub Copilot have been shown to improve developer\nproductivity, but their impact on code understanding is less clear. We\nreplicate and extend a previous study by exploring both performance and\ncomprehension in GenAI-assisted brownfield programming tasks. In a\nwithin-subjects experimental study, 18 computer science graduate students\ncompleted feature implementation tasks with and without Copilot. Results show\nthat Copilot significantly reduced task time and increased the number of test\ncases passed. However, comprehension scores did not differ across conditions,\nrevealing a comprehension-performance gap: participants passed more test cases\nwith Copilot, but did not demonstrate greater understanding of the legacy\ncodebase. Moreover, we failed to find a correlation between comprehension and\ntask performance. These findings suggest that while GenAI tools can accelerate\nprogramming progress in a legacy codebase, such progress may come without an\nimproved understanding of that codebase. We consider the implications of these\nfindings for programming education and GenAI tool design."}
{"id": "2511.02927", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2511.02927", "abs": "https://arxiv.org/abs/2511.02927", "authors": ["Rafael Baez", "Alejandro Olivas", "Nathan K. Diamond", "Marcelo Frias", "Yannic Noller", "Saeid Tizpaz-Niari"], "title": "Risk Estimation in Differential Fuzzing via Extreme Value Theory", "comment": "In Proceedings of the 40th IEEE/ACM International Conference on\n  Automated Software Engineering (ASE 25), 13 Pages, 4 Figures, 5 Tables", "summary": "Differential testing is a highly effective technique for automatically\ndetecting software bugs and vulnerabilities when the specifications involve an\nanalysis over multiple executions simultaneously. Differential fuzzing, in\nparticular, operates as a guided randomized search, aiming to find (similar)\ninputs that lead to a maximum difference in software outputs or their\nbehaviors. However, fuzzing, as a dynamic analysis, lacks any guarantees on the\nabsence of bugs: from a differential fuzzing campaign that has observed no bugs\n(or a minimal difference), what is the risk of observing a bug (or a larger\ndifference) if we run the fuzzer for one or more steps?\n  This paper investigates the application of Extreme Value Theory (EVT) to\naddress the risk of missing or underestimating bugs in differential fuzzing.\nThe key observation is that differential fuzzing as a random process resembles\nthe maximum distribution of observed differences. Hence, EVT, a branch of\nstatistics dealing with extreme values, is an ideal framework to analyze the\ntail of the differential fuzzing campaign to contain the risk. We perform\nexperiments on a set of real-world Java libraries and use differential fuzzing\nto find information leaks via side channels in these libraries. We first\nexplore the feasibility of EVT for this task and the optimal hyperparameters\nfor EVT distributions. We then compare EVT-based extrapolation against baseline\nstatistical methods like Markov's as well as Chebyshev's inequalities, and the\nBayes factor. EVT-based extrapolations outperform the baseline techniques in\n14.3% of cases and tie with the baseline in 64.2% of cases. Finally, we\nevaluate the accuracy and performance gains of EVT-enabled differential fuzzing\nin real-world Java libraries, where we reported an average saving of tens of\nmillions of bytecode executions by an early stop."}
{"id": "2511.03026", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2511.03026", "abs": "https://arxiv.org/abs/2511.03026", "authors": ["Logan Murphy", "Torin Viger", "Alessio Di Sandro", "Aren A. Babikian", "Marsha Chechik"], "title": "Assurance Case Development for Evolving Software Product Lines: A Formal Approach", "comment": null, "summary": "In critical software engineering, structured assurance cases (ACs) are used\nto demonstrate how key system properties are supported by evidence (e.g., test\nresults, proofs). Creating rigorous ACs is particularly challenging in the\ncontext of software product lines (SPLs), i.e, sets of software products with\noverlapping but distinct features and behaviours. Since SPLs can encompass very\nlarge numbers of products, developing a rigorous AC for each product\nindividually is infeasible. Moreover, if the SPL evolves, e.g., by the\nmodification or introduction of features, it can be infeasible to assess the\nimpact of this change. Instead, the development and maintenance of ACs ought to\nbe lifted such that a single AC can be developed for the entire SPL\nsimultaneously, and be analyzed for regression in a variability-aware fashion.\nIn this article, we describe a formal approach to lifted AC development and\nregression analysis. We formalize a language of variability-aware ACs for SPLs\nand study the lifting of template-based AC development. We also define a\nregression analysis to determine the effects of SPL evolutions on\nvariability-aware ACs. We describe a model-based assurance management tool\nwhich implements these techniques, and illustrate our contributions by\ndeveloping an AC for a product line of medical devices."}
{"id": "2511.03103", "categories": ["cs.SE", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.03103", "abs": "https://arxiv.org/abs/2511.03103", "authors": ["Rafael Jos Moura", "Maria Gizele Nascimento", "Fumio Machida", "Ermeson Andrade"], "title": "Adaptive Detection of Software Aging under Workload Shift", "comment": "SIMP\\'OSIO EM SISTEMAS COMPUTACIONAIS DE ALTO DESEMPENHO (SSCAD)", "summary": "Software aging is a phenomenon that affects long-running systems, leading to\nprogressive performance degradation and increasing the risk of failures. To\nmitigate this problem, this work proposes an adaptive approach based on machine\nlearning for software aging detection in environments subject to dynamic\nworkload conditions. We evaluate and compare a static model with adaptive\nmodels that incorporate adaptive detectors, specifically the Drift Detection\nMethod (DDM) and Adaptive Windowing (ADWIN), originally developed for concept\ndrift scenarios and applied in this work to handle workload shifts. Experiments\nwith simulated sudden, gradual, and recurring workload transitions show that\nstatic models suffer a notable performance drop when applied to unseen workload\nprofiles, whereas the adaptive model with ADWIN maintains high accuracy,\nachieving an F1-Score above 0.93 in all analyzed scenarios."}
{"id": "2511.03136", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2511.03136", "abs": "https://arxiv.org/abs/2511.03136", "authors": ["Kexing Ji", "Shiyun Fu", "Cuiyun Gao", "Yujia Chen", "Zezhou Yang", "Chaozheng Wang", "Yuetang Deng"], "title": "Automated Prompt Generation for Code Intelligence: An Empirical study and Experience in WeChat", "comment": "Accepted by ASE 2025 Industry Track", "summary": "Large Code Models (LCMs) show potential in code intelligence, but their\neffectiveness is greatly influenced by prompt quality. Current prompt design is\nmostly manual, which is time-consuming and highly dependent on specific LCMs\nand tasks. While automated prompt generation (APG) exists in NLP, it is\nunderexplored for code intelligence. This creates a gap, as automating the\nprompt process is essential for developers facing diverse tasks and black-box\nLCMs.\n  To mitigate this, we empirically investigate two important parts of APG:\nInstruction Generation (IG) and Multi-Step Reasoning (MSR). IG provides a\ntask-related description to instruct LCMs, while MSR guides them to produce\nlogical steps before the final answer. We evaluate widely-used APG methods for\neach part on four open-source LCMs and three code intelligence tasks: code\ntranslation (PL-PL), code summarization (PL-NL), and API recommendation\n(NL-PL).Experimental results indicate that both IG and MSR dramatically enhance\nperformance compared to basic prompts. Based on these results, we propose a\nnovel APG approach combining the best methods of the two parts. Experiments\nshow our approach achieves average improvements of 28.38% in CodeBLEU (code\ntranslation), 58.11% in ROUGE-L (code summarization), and 84.53% in\nSuccessRate@1 (API recommendation) over basic prompts. To validate its\neffectiveness in an industrial scenario, we evaluate our approach on\nWeChat-Bench, a proprietary dataset, achieving an average MRR improvement of\n148.89% for API recommendation."}
{"id": "2511.03153", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.03153", "abs": "https://arxiv.org/abs/2511.03153", "authors": ["Khouloud Oueslati", "Maxime Lamothe", "Foutse Khomh"], "title": "RefAgent: A Multi-agent LLM-based Framework for Automatic Software Refactoring", "comment": null, "summary": "Large Language Models (LLMs) have substantially influenced various software\nengineering tasks. Indeed, in the case of software refactoring, traditional\nLLMs have shown the ability to reduce development time and enhance code\nquality. However, these LLMs often rely on static, detailed instructions for\nspecific tasks. In contrast, LLM-based agents can dynamically adapt to evolving\ncontexts and autonomously make decisions by interacting with software tools and\nexecuting workflows. In this paper, we explore the potential of LLM-based\nagents in supporting refactoring activities. Specifically, we introduce\nRefAgent, a multi-agent LLM-based framework for end-to-end software\nrefactoring. RefAgent consists of specialized agents responsible for planning,\nexecuting, testing, and iteratively refining refactorings using self-reflection\nand tool-calling capabilities. We evaluate RefAgent on eight open-source Java\nprojects, comparing its effectiveness against a single-agent approach, a\nsearch-based refactoring tool, and historical developer refactorings. Our\nassessment focuses on: (1) the impact of generated refactorings on software\nquality, (2) the ability to identify refactoring opportunities, and (3) the\ncontribution of each LLM agent through an ablation study. Our results show that\nRefAgent achieves a median unit test pass rate of 90%, reduces code smells by a\nmedian of 52.5%, and improves key quality attributes (e.g., reusability) by a\nmedian of 8.6%. Additionally, it closely aligns with developer refactorings and\nthe search-based tool in identifying refactoring opportunities, attaining a\nmedian F1-score of 79.15% and 72.7%, respectively. Compared to single-agent\napproaches, RefAgent improves the median unit test pass rate by 64.7% and the\nmedian compilation success rate by 40.1%. These findings highlight the promise\nof multi-agent architectures in advancing automated software refactoring."}
{"id": "2511.03182", "categories": ["cs.SE", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.03182", "abs": "https://arxiv.org/abs/2511.03182", "authors": ["Vinaik Chhetri", "A. B Siddique", "Umar Farooq"], "title": "Understanding Robustness of Model Editing in Code LLMs: An Empirical Study", "comment": "26 pages, 2 figures, 15 tables", "summary": "Large language models (LLMs) are increasingly used in software development.\nHowever, while LLMs remain static after pretraining, programming languages and\nAPIs continue to evolve, leading to the generation of deprecated or\nincompatible code that undermines reliability. Retraining LLMs from scratch to\nreflect such changes is computationally expensive, making model editing a\npromising lightweight alternative that updates only a small subset of\nparameters. Despite its potential, it remains unclear whether model editing\nyields genuine syntactic and semantic adaptations or merely superficial fixes.\nIn this work, we present a systematic study of five state-of-the-art model\nediting methods: Constrained Fine-Tuning (FT), GRACE, MEMIT, PMET, and ROME. We\napply these methods to three leading open-source code LLMs, CodeLlama,\nCodeQwen1.5, and DeepSeek-Coder, under controlled API deprecation scenarios.\nOur evaluation covers both instant and sequential editing settings, using three\ndisjoint evaluation sets designed to assess reliability, generalization, and\nspecificity. We measure model correctness at three levels: successful\ncompilation, partial test case pass, and full test pass. Our findings show that\ninstant edits consistently degrade model performance, with syntactic validity\ndropping by up to 86 percentage points and functional correctness declining by\n45 points even in the best-performing setting. Sequential edits further amplify\nthis degradation, and in some cases, model performance collapses entirely.\nAcross all models, most passing generations relied on workarounds rather than\ncorrectly adopting the intended changes, while faulty adoptions that result in\ntest failures or compilation errors were significantly more frequent. Correct\nadoptions, where the model correctly integrates the intended change, occurred\nin only about 6% of cases."}
{"id": "2511.03404", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2511.03404", "abs": "https://arxiv.org/abs/2511.03404", "authors": ["Qianhui Zhao", "Li Zhang", "Fang Liu", "Junhang Cheng", "Chengru Wu", "Junchen Ai", "Qiaoyuanhe Meng", "Lichen Zhang", "Xiaoli Lian", "Shubin Song", "Yuanping Guo"], "title": "Towards Realistic Project-Level Code Generation via Multi-Agent Collaboration and Semantic Architecture Modeling", "comment": null, "summary": "In recent years, Large Language Models (LLMs) have achieved remarkable\nprogress in automated code generation. In real-world software engineering, the\ngrowing demand for rapid iteration and continuous delivery underscores the\nimportance of project-level code generation, where LLMs are expected to\ngenerate complete software projects directly from complex user requirements.\nAlthough existing studies have made initial explorations, they still face key\nlimitations, including unrealistic datasets and unreliable evaluation metrics\nthat fail to reflect real-world complexity, the semantic gap between\nhuman-written requirements and machine-interpretable structures, and\ndifficulties in managing hierarchical dependencies and maintaining quality\nthroughout the generation process. To address these limitations, we first\nintroduce CodeProjectEval, a project-level code generation dataset built from\n18 real-world repositories with 12.7 files and 2,388.6 lines of code per task\non average, supplemented with documentation and executable test cases for\nautomatic evaluation. We further propose ProjectGen, a multi-agent framework\nthat decomposes projects into architecture design, skeleton generation, and\ncode filling stages with iterative refinement and memory-based context\nmanagement. Within this framework, we introduce the Semantic Software\nArchitecture Tree (SSAT), a structured and semantically rich representation\nthat effectively bridges user requirements and source code implementation.\nExperiments show that ProjectGen achieves state-of-the-art performance, passing\n52/124 test cases on the small-scale project-level code generation dataset\nDevBench, a 57% improvement over the baseline approaches, and 310 test cases on\nCodeProjectEval, representing an improvement of roughly tenfold compared to the\nbaselines."}
{"id": "2511.03421", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.03421", "abs": "https://arxiv.org/abs/2511.03421", "authors": ["Shihai Wang", "Tao Chen"], "title": "Light over Heavy: Automated Performance Requirements Quantification with Linguistic Inducement", "comment": "accepted by ICSE 2026", "summary": "Elicited performance requirements need to be quantified for compliance in\ndifferent engineering tasks, e.g., configuration tuning and performance\ntesting. Much existing work has relied on manual quantification, which is\nexpensive and error-prone due to the imprecision. In this paper, we present\nLQPR, a highly efficient automatic approach for performance requirements\nquantification.LQPR relies on a new theoretical framework that converts\nquantification as a classification problem. Despite the prevalent applications\nof Large Language Models (LLMs) for requirement analytics, LQPR takes a\ndifferent perspective to address the classification: we observed that\nperformance requirements can exhibit strong patterns and are often\nshort/concise, therefore we design a lightweight linguistically induced\nmatching mechanism. We compare LQPR against nine state-of-the-art\nlearning-based approaches over diverse datasets, demonstrating that it is\nranked as the sole best for 75% or more cases with two orders less cost. Our\nwork proves that, at least for performance requirement quantification,\nspecialized methods can be more suitable than the general LLM-driven\napproaches."}
{"id": "2511.03517", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2511.03517", "abs": "https://arxiv.org/abs/2511.03517", "authors": ["Wencheng Ye", "Yan Liu"], "title": "U2F: Encouraging SWE-Agent to Seize Novelty without Losing Feasibility", "comment": null, "summary": "Large language models (LLMs) have shown strong capabilities in software\nengineering tasks, yet most existing LLM-based SWE-Agents mainly tackle\nwell-defined problems using conventional methods, often overlooking alternative\nor innovative solutions beyond their predefined frameworks. This limitation is\nevident in open-world software environments, where emerging challenges\ntranscend established paradigms.\n  We propose U2F (Unknown Unknowns to Functional solutions), a\ncognitive-inspired, uncertainty-embracing multi-agent framework that\nsystematically surfaces \"Unknown Unknowns\" - novel solution pathways absent\nfrom initial formulations but holding innovative potential. U2F consists of two\nkey components: (1) a Discovery-Exploration-Integration agent system for\nuncovering and synthesizing potential solutions, and (2) cognitive enhancement\nmechanisms across three dimensions: cross-domain analogical reasoning, reverse\nthinking, and external validation, which strategically reframe and extend\nconventional solution boundaries.\n  Applied to 218 real-world software enabler stories curated from authentic\nengineering tasks, U2F achieved notable improvements: human experts reported a\n14 percent increase in overall novelty, 51 percent improvement in semantic\nnovelty, and stable feasibility (4.02/5.0), corroborated by an LLM-based\nevaluator. These results highlight the potential of embracing uncertainty as a\ncatalyst for innovation in software engineering."}
{"id": "2511.03549", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.03549", "abs": "https://arxiv.org/abs/2511.03549", "authors": ["Ziv Nevo", "Orna Raz", "Karen Yorav"], "title": "Uncovering Code Insights: Leveraging GitHub Artifacts for Deeper Code Understanding", "comment": "7 pages, 6 figures, to be published in AISM 2025, see\n  https://aism25.github.io/aism25/", "summary": "Understanding the purpose of source code is a critical task in software\nmaintenance, onboarding, and modernization. While large language models (LLMs)\nhave shown promise in generating code explanations, they often lack grounding\nin the broader software engineering context. We propose a novel approach that\nleverages natural language artifacts from GitHub -- such as pull request\ndescriptions, issue descriptions and discussions, and commit messages -- to\nenhance LLM-based code understanding. Our system consists of three components:\none that extracts and structures relevant GitHub context, another that uses\nthis context to generate high-level explanations of the code's purpose, and a\nthird that validates the explanation. We implemented this as a standalone tool,\nas well as a server within the Model Context Protocol (MCP), enabling\nintegration with other AI-assisted development tools. Our main use case is that\nof enhancing a standard LLM-based code explanation with code insights that our\nsystem generates. To evaluate explanations' quality, we conducted a small scale\nuser study, with developers of several open projects, as well as developers of\nproprietary projects. Our user study indicates that when insights are generated\nthey often are helpful and non trivial, and are free from hallucinations."}
{"id": "2511.03690", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.03690", "abs": "https://arxiv.org/abs/2511.03690", "authors": ["Xingyao Wang", "Simon Rosenberg", "Juan Michelini", "Calvin Smith", "Hoang Tran", "Engel Nyst", "Rohit Malhotra", "Xuhui Zhou", "Valerie Chen", "Robert Brennan", "Graham Neubig"], "title": "The OpenHands Software Agent SDK: A Composable and Extensible Foundation for Production Agents", "comment": null, "summary": "Agents are now used widely in the process of software development, but\nbuilding production-ready software engineering agents is a complex task.\nDeploying software agents effectively requires flexibility in implementation\nand experimentation, reliable and secure execution, and interfaces for users to\ninteract with agents. In this paper, we present the OpenHands Software Agent\nSDK, a toolkit for implementing software development agents that satisfy these\ndesiderata. This toolkit is a complete architectural redesign of the agent\ncomponents of the popular OpenHands framework for software development agents,\nwhich has 64k+ GitHub stars. To achieve flexibility, we design a simple\ninterface for implementing agents that requires only a few lines of code in the\ndefault case, but is easily extensible to more complex, full-featured agents\nwith features such as custom tools, memory management, and more. For security\nand reliability, it delivers seamless local-to-remote execution portability,\nintegrated REST/WebSocket services. For interaction with human users, it can\nconnect directly to a variety of interfaces, such as visual workspaces (VS\nCode, VNC, browser), command-line interfaces, and APIs. Compared with existing\nSDKs from OpenAI, Claude, and Google, OpenHands uniquely integrates native\nsandboxed execution, lifecycle control, model-agnostic multi-LLM routing, and\nbuilt-in security analysis. Empirical results on SWE-Bench Verified and GAIA\nbenchmarks demonstrate strong performance. Put together, these elements allow\nthe OpenHands Software Agent SDK to provide a practical foundation for\nprototyping, unlocking new classes of custom applications, and reliably\ndeploying agents at scale."}
{"id": "2511.03286", "categories": ["cs.DC", "cs.MA", "cs.SE", "cs.SI"], "pdf": "https://arxiv.org/pdf/2511.03286", "abs": "https://arxiv.org/abs/2511.03286", "authors": ["Ehud Shapiro"], "title": "Characterising Global Platforms: Centralised, Decentralised, Federated, and Grassroots", "comment": null, "summary": "Global digital platforms are software systems designed to serve entire\npopulations, with some already serving billions of people. We propose atomic\ntransactions-based multiagent transition systems and protocols as a formal\nframework to study them; introduce essential agents -- minimal sets of agents\nthe removal of which makes communication impossible; and show that the\ncardinality of essential agents partitions all global platforms into four\nclasses:\n  1. Centralised -- one (the server)\n  2. Decentralised -- finite $>1$ (bootstrap nodes)\n  3. Federated -- infinite but not universal (all servers)\n  4. Grassroots -- universal (all agents)\n  Our illustrative formal example is a global social network, for which we\nprovide centralised, decentralised, federated, and grassroots specifications\nvia multiagent atomic transactions, and prove they satisfy basic correctness\nproperties. We discuss informally additional global platforms -- currencies,\n``sharing economy'' apps, AI, and more. While this may be the first\ncharacterisation of centralised, decentralised, and federated global platforms,\ngrassroots platforms have been formally defined previously, but using different\nnotions. Here, we prove that their original definition implies that all agents\nare essential, placing grassroots platforms in a distinct class within the\nbroader formal context that includes all global platforms. This work provides\nthe first mathematical framework for classifying any global platform --\nexisting or imagined -- by providing a multiagent atomic-transactions\nspecification of it and determining the cardinality of the minimal set of\nessential agents in the ensuing multiagent protocol. It thus"}
{"id": "2511.03312", "categories": ["cs.NI", "cs.SE"], "pdf": "https://arxiv.org/pdf/2511.03312", "abs": "https://arxiv.org/abs/2511.03312", "authors": ["Jiali Xu", "Valeria Loscri", "Romain Rouvoy"], "title": "Integrity Under Siege: A Rogue gNodeB's Manipulation of 5G Network Slice Allocation", "comment": "15 pages, 11 figures, Elsevier journal paper layout", "summary": "The advent of 5G networks, with network slicing as a cornerstone technology,\npromises customized, high-performance services, but also introduces novel\nattack surfaces beyond traditional threats. This article investigates a\ncritical and underexplored integrity vulnerability: the manipulation of network\nslice allocation to compromise Quality of Service (QoS) and resource integrity.\nWe introduce a threat model, grounded in a risk analysis of permissible yet\ninsecure configurations like null-ciphering (5G-EA0), demonstrating how a rogue\ngNodeB acting as a Man-in-the-Middle can exploit protocol weaknesses to forge\nslice requests and hijack a User Equipment's (UE) connection. Through a\ncomprehensive experimental evaluation on a 5G testbed, we demonstrate the\nattack's versatile and severe impacts. Our findings show this integrity breach\ncan manifest as obvious QoS degradation, such as a 95% bandwidth reduction and\n150% latency increase when forcing UE to a suboptimal slice, or as stealthy\nslice manipulation that is indistinguishable from benign network operation and\ngenerates no core network errors. Furthermore, we validate a systemic resource\ncontamination attack where redirecting a crowd of UE orchestrates a\nDenial-of-Service, causing packet loss to exceed 60% and inducing measurable\nCPU saturation (~80%) on core network User Plane Functions (UPFs). Based on\nthese results, we discuss the profound implications for Service Level\nAgreements (SLAs) and critical infrastructure. We propose concrete, cross-layer\nmitigation strategies for network operators as future work, underscoring the\nurgent need to secure the integrity of dynamic resource management in 5G\nnetworks."}
{"id": "2511.03533", "categories": ["cs.DC", "cs.SE"], "pdf": "https://arxiv.org/pdf/2511.03533", "abs": "https://arxiv.org/abs/2511.03533", "authors": ["Nils Japke", "Furat Hamdan", "Diana Baumann", "David Bermbach"], "title": "Investigating the Impact of Isolation on Synchronized Benchmarks", "comment": "Accepted for publication in 2025 IEEE/ACM 18th International\n  Conference on Utility and Cloud Computing", "summary": "Benchmarking in cloud environments suffers from performance variability from\nmulti-tenant resource contention. Duet benchmarking mitigates this by running\ntwo workload versions concurrently on the same VM, exposing them to identical\nexternal interference. However, intra-VM contention between synchronized\nworkloads necessitates additional isolation mechanisms.\n  This work evaluates three such strategies: cgroups and CPU pinning, Docker\ncontainers, and Firecracker MicroVMs. We compare all strategies with an\nunisolated baseline experiment, by running benchmarks with a duet setup\nalongside a noise generator. This noise generator \"steals\" compute resources to\ndegrade performance measurements.\n  All experiments showed different latency distributions while under the\neffects of noise generation, but results show that process isolation generally\nlowered false positives, except for our experiments with Docker containers.\nEven though Docker containers rely internally on cgroups and CPU pinning, they\nwere more susceptible to performance degradation due to noise influence.\nTherefore, we recommend to use process isolation for synchronized workloads,\nwith the exception of Docker containers."}
