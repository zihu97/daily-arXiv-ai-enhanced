<div id=toc></div>

# Table of Contents

- [cs.AR](#cs.AR) [Total: 9]
- [cs.MA](#cs.MA) [Total: 6]
- [cs.PF](#cs.PF) [Total: 2]
- [cs.NI](#cs.NI) [Total: 1]
- [cs.SE](#cs.SE) [Total: 17]
- [cs.DC](#cs.DC) [Total: 10]
- [cs.OS](#cs.OS) [Total: 1]


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [1] [Compass: Mapping Space Exploration for Multi-Chiplet Accelerators Targeting LLM Inference Serving Workloads](https://arxiv.org/abs/2512.06093)
*Boyu Li,Zongwei Zhu,Yi Xiong,Qianyue Cao,Jiawei Geng,Xiaonan Zhang,Xi Li*

Main category: cs.AR

TL;DR: 提出Compass框架，通过计算执行图编码和遗传算法优化映射，显著降低LLM推理能耗延迟积。


<details>
  <summary>Details</summary>
Motivation: 现有加速器映射方法无法有效支持LLM推理中混合请求类型与可变序列长度的动态特性。

Method: 采用计算执行图编码方案解耦微批次与层，并结合遗传算法实现高效映射搜索。

Result: 相比现有最优方案，平均EDP降低63.12%。

Conclusion: 该框架能灵活适配异构芯片并高效支持LLM动态推理需求。

Abstract: Large Language Models (LLMs) impose massive computational demands, driving the need for scalable multi-chiplet accelerators. However, existing mapping space exploration efforts for such accelerators primarily focus on traditional CNN/Transformer workloads and fail to adequately support the dynamic behaviors of mixed request types and variable sequence lengths in real-world LLM inference serving. To bridge this gap, we first propose a computation execution graph-based mapping encoding scheme that decouples micro-batches and layers, enabling fine-grained execution control on heterogeneous chiplets and flexibly representing various parallelism strategies. Second, building upon this scheme, we develop the Compass framework, which integrates an evaluation engine and a genetic algorithm-based mapping generation engine to achieve efficient mapping search. Compared to state-of-the-art works, our solution achieves an average EDP reduction of 63.12%.

</details>


### [2] [Hardware Software Optimizations for Fast Model Recovery on Reconfigurable Architectures](https://arxiv.org/abs/2512.06113)
*Bin Xu,Ayan Banerjee,Sandeep Gupta*

Main category: cs.AR

TL;DR: MERINDA是一个基于FPGA的模型恢复加速框架，通过流式数据流水线和硬件感知设计显著提升效率。


<details>
  <summary>Details</summary>
Motivation: 解决GPU在模型恢复任务中因迭代依赖、内核启动开销等问题导致的低效执行。

Method: 采用BRAM分块、定点计算核、LUT与进位链加法器并行等技术构建流式数据通路，减少片外访存与同步瓶颈。

Result: 在典型工作负载上比FPGA基线方案减少最多6.3倍周期数，实现实时性能。

Conclusion: MERINDA能高效支持物理AI与实时数字孪生中的模型恢复需求。

Abstract: Model Recovery (MR) is a core primitive for physical AI and real-time digital twins, but GPUs often execute MR inefficiently due to iterative dependencies, kernel-launch overheads, underutilized memory bandwidth, and high data-movement latency. We present MERINDA, an FPGA-accelerated MR framework that restructures computation as a streaming dataflow pipeline. MERINDA exploits on-chip locality through BRAM tiling, fixed-point kernels, and the concurrent use of LUT fabric and carry-chain adders to expose fine-grained spatial parallelism while minimizing off-chip traffic. This hardware-aware formulation removes synchronization bottlenecks and sustains high throughput across the iterative updates in MR. On representative MR workloads, MERINDA delivers up to 6.3x fewer cycles than an FPGA-based LTC baseline, enabling real-time performance for time-critical physical systems.

</details>


### [3] [From PyTorch to Calyx: An Open-Source Compiler Toolchain for ML Accelerators](https://arxiv.org/abs/2512.06177)
*Jiahan Xie,Evan Williams,Adrian Sampson*

Main category: cs.AR

TL;DR: 开源编译器工具链将PyTorch模型转为可综合SystemVerilog，支持FPGA高效实现。


<details>
  <summary>Details</summary>
Motivation: 提升ML模型在FPGA上的部署效率，替代闭源工业工具。

Method: 结合Allo、Calyx与CIRCT，实现内存分区优化与并行加速。

Result: 生成硬件设计性能媲美Vitis HLS等闭源工具。

Conclusion: 该工具链有效支持端到端ML模型到FPGA硬件的自动化编译。

Abstract: We present an end-to-end open-source compiler toolchain that targets synthesizable SystemVerilog from ML models written in PyTorch. Our toolchain leverages the accelerator design language Allo, the hardware intermediate representation (IR) Calyx, and the CIRCT project under LLVM. We also implement a set of compiler passes for memory partitioning, enabling effective parallelism in memory-intensive ML workloads. Experimental results demonstrate that our compiler can effectively generate optimized FPGA-implementable hardware designs that perform reasonably well against closed-source industry-grade tools such as Vitis HLS.

</details>


### [4] [Análisis de rendimiento y eficiencia energética en el cluster Raspberry Pi Cronos](https://arxiv.org/abs/2512.07622)
*Martha Semken,Mariano Vargas,Ignacio Tula,Giuliana Zorzoli,Andrés Rojas Paredes*

Main category: cs.AR

TL;DR: 本文评估了由树莓派4和3b组成的Cronos集群在教育用途下的计算性能与能效，通过HPL基准测试发现6节点同构配置可达6.91 GFLOPS，异构节点则降低稳定性和效率，并提供了GFLOPS/W能效比。


<details>
  <summary>Details</summary>
Motivation: 探索低成本ARM集群在教育和科研场景中的实际性能与能耗表现，为类似系统设计提供实证依据。

Method: 使用HPL基准，在Slurm资源管理和Open MPI并行通信环境下，测试不同节点配置的扩展性、稳定性及功耗。

Result: 6个树莓派4节点同构集群最高达6.91 GFLOPS；混入树莓派3b会降低稳定性和效率；测得整机功耗并计算了GFLOPS/W能效指标。

Conclusion: 该研究为低成本ARM集群在教学与科研中的设计与部署提供了实用参考，强调同构配置更优且需关注能效平衡。

Abstract: This article presents an evaluation of the computational performance and energy efficiency of the Cronos cluster, composed of Raspberry Pi4 and 3b microcomputers designed for educational purposes. Experimental tests were performed using the High Performance Linpack (HPL) benchmark, under a resource management environment configured with Slurm and parallel communication via Open MPI. The study focuses on analyzing scalability, stability, and power consumption during the execution of computationally intensive workloads, considering different node configurations. The results show that the cluster achieves a performance of up to 6.91 GFLOPS in homogeneous configurations of 6 Raspberry Pi 4 nodes, and that the use of heterogeneous nodes (including Raspberry Pi 3b) can negatively impact stability and efficiency. Additionally, the total electrical consumption of the system was measured during the runs, allowing for the estimation of the performance-to-consumption ratio (GFLOPS/W) as a comparative metric. This study constitutes a concrete contribution to the design, evaluation, and utilization of low-cost ARM clusters in educational and research contexts.

</details>


### [5] [SparsePixels: Efficient Convolution for Sparse Data on FPGAs](https://arxiv.org/abs/2512.06208)
*Ho Fung Tsoi,Dylan Rankin,Vladimir Loncar,Philip Harris*

Main category: cs.AR

TL;DR: SparsePixels框架通过选择性计算稀疏图像中的活跃像素，显著加速FPGA上的CNN推理，适用于微秒级延迟需求的场景。


<details>
  <summary>Details</summary>
Motivation: 标准CNN在FPGA上处理稀疏图像时因遍历所有像素导致高延迟，而实际语义信息仅占少数像素，造成大量无效计算。

Method: 提出SparsePixels框架，构建一类特殊CNN，仅保留并计算活跃像素子集，结合量化感知训练与HLS实现FPGA部署。

Result: 在LArTPC中微子数据集上实现73倍推理加速（48.665μs→0.665μs），资源占用低，性能损失仅百分比级别；类似稀疏数据集亦获至少10倍加速。

Conclusion: 该框架可有效支持现代实验如LHC触发与数据采集系统的高效快速算法开发。

Abstract: Inference of standard CNNs on FPGAs often incurs high latency and a long initiation interval due to the deep nested loops required to densely convolve every input pixel regardless of its feature value, especially when the image size is large. However, in some image data, input features can be spatially sparse, and semantic information may occupy only a small fraction of the input pixels. In this case most computation would be wasted on empty regions. In this work, we introduce SparsePixels, a framework for efficient convolution for spatially sparse image data on FPGAs, targeting fast inference applications in constrained environments with latency requirements of microseconds or below. Our approach implements a special class of CNNs that selectively retain and compute on a small subset of pixels that are active while ignoring the rest. We show that, for example, in a neutrino physics dataset for identifying neutrino interactions in LArTPC images that have around 4k input pixels but are naturally very sparse, a standard CNN with a compact size of 4k parameters incurs an inference latency of 48.665 $μ$s on an FPGA, whereas a sparse CNN of the same base architecture computing on less than 1% of the input pixels results in a $\times 73$ inference speedup to 0.665 $μ$s, with resource utilization well within on-chip budgets, trading only a small percent-level performance loss. At least one-order-of magnitude speedups with comparable performance are also demonstrated in similar datasets with sparse image patterns. This work aims to benefit future algorithm developments for fast and efficient data readout in modern experiments such as the trigger and data acquisition systems at the CERN Large Hadron Collider. For easy adoption, we have developed a library to support building sparse CNNs with quantization-aware training, as well as an HLS implementation for FPGA deployment.

</details>


### [6] [A 33.6-136.2 TOPS/W Nonlinear Analog Computing-In-Memory Macro for Multi-bit LSTM Accelerator in 65 nm CMOS](https://arxiv.org/abs/2512.06362)
*Junyi Yang,Xinyu Luo,Ye Ke,Zheng Wang,Hongyang Shang,Shuai Dong,Zhengnan Fu,Xiaofeng Yang,Hongjie Liu,Arindam Basu*

Main category: cs.AR

TL;DR: 提出一种结合可重构非线性存内模拟-数字转换器的LSTM加速器，显著提升能效与面积效率。


<details>
  <summary>Details</summary>
Motivation: 解决模拟存内计算中非线性操作数字化导致能效受限的问题。

Method: 采用双9T位单元、RUDC技术及双电源6T-SRAM阵列，在模拟域直接计算非线性激活。

Result: 实现平均误差<1 LSB，系统能效提升2.2倍，面积效率提升1.6倍，关键词识别准确率达92.0%。

Conclusion: 该设计有效提升LSTM加速器能效，兼具高精度与鲁棒性，适用于边缘智能场景。

Abstract: The energy efficiency of analog computing-in-memory (ACIM) accelerator for recurrent neural networks, particularly long short-term memory (LSTM) network, is limited by the high proportion of nonlinear (NL) operations typically executed digitally. To address this, we propose an LSTM accelerator incorporating an ACIM macro with reconfigurable (1-5 bit) nonlinear in-memory (NLIM) analog-to-digital converter (ADC) to compute NL activations directly in the analog domain using: 1) a dual 9T bitcell with decoupled read/write paths for signed inputs and ternary weight operations; 2) a read-word-line underdrive Cascode (RUDC) technique achieving 2.8X higher read-bitline dynamic range than single-transistor designs (1.4X better over conventional Cascode structure with 7X lower current variation); 3) a dual-supply 6T-SRAM array for efficient multi-bit weight operations and reducing both bitcell count (7.8X) and latency (4X) for 5-bit weight operations. We experimentally demonstrate 5-bit NLIM ADC for approximating NL activations in LSTM cells, achieving average error <1 LSB. Simulation confirms the robustness of NLIM ADC against temperature variations thanks to the replica bias strategy. Our design achieves 92.0% on-chip inference accuracy for a 12-class keyword-spotting task while demonstrating 2.2X higher system-level normalized energy efficiency and 1.6X better normalized area efficiency than state-of-the-art works. The results combine physical measurements of a macro unit-accounting for the majority of LSTM operations (99% linear and 80% nonlinear operations)-with simulations of the remaining components, including additional LSTM and fully connected layers.

</details>


### [7] [ArchPower: Dataset for Architecture-Level Power Modeling of Modern CPU Design](https://arxiv.org/abs/2512.06854)
*Qijun Zhang,Yao Lu,Mengming Li,Shang Liu,Zhiyao Xie*

Main category: cs.AR

TL;DR: ArchPower是首个开源的架构级处理器功耗建模数据集，包含200个样本、100多个架构特征及细粒度功耗标签，旨在解决ML功耗模型的数据稀缺问题。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏开源且贴近真实设计场景的处理器架构级功耗数据集，制约了ML功耗模型的发展与验证。

Method: 通过复杂真实的设计流程，从25种CPU配置执行8种工作负载中采集架构特征与仿真功耗标签，构建包含组件与功耗类型分解的细粒度数据集。

Result: 成功构建并开源包含200样本的ArchPower数据集，每个样本含100+特征和11组件×4功耗类型的详细标签。

Conclusion: ArchPower填补了架构级功耗建模领域开源数据空白，有助于推动高精度ML功耗模型的研究与应用。

Abstract: Power is the primary design objective of large-scale integrated circuits (ICs), especially for complex modern processors (i.e., CPUs). Accurate CPU power evaluation requires designers to go through the whole time-consuming IC implementation process, easily taking months. At the early design stage (e.g., architecture-level), classical power models are notoriously inaccurate. Recently, ML-based architecture-level power models have been proposed to boost accuracy, but the data availability is a severe challenge. Currently, there is no open-source dataset for this important ML application. A typical dataset generation process involves correct CPU design implementation and repetitive execution of power simulation flows, requiring significant design expertise, engineering effort, and execution time. Even private in-house datasets often fail to reflect realistic CPU design scenarios. In this work, we propose ArchPower, the first open-source dataset for architecture-level processor power modeling. We go through complex and realistic design flows to collect the CPU architectural information as features and the ground-truth simulated power as labels. Our dataset includes 200 CPU data samples, collected from 25 different CPU configurations when executing 8 different workloads. There are more than 100 architectural features in each data sample, including both hardware and event parameters. The label of each sample provides fine-grained power information, including the total design power and the power for each of the 11 components. Each power value is further decomposed into four fine-grained power groups: combinational logic power, sequential logic power, memory power, and clock power. ArchPower is available at https://github.com/hkust-zhiyao/ArchPower.

</details>


### [8] [DCO: Dynamic Cache Orchestration for LLM Accelerators through Predictive Management](https://arxiv.org/abs/2512.07312)
*Zhongchun Zhou,Chengtao Lai,Yuhang Gu,Wei Zhang*

Main category: cs.AR

TL;DR: 本文提出一种基于共享缓存与应用感知策略的多核AI加速器设计，显著提升性能并简化编程。


<details>
  <summary>Details</summary>
Motivation: 避免使用复杂的分层暂存内存架构，降低软件开发复杂度。

Method: 利用软件栈中的数据流信息指导缓存替换、旁路决策和缓解缓存颠簸机制。

Result: 相比传统缓存架构最高实现1.80倍加速，RTL实现在15nm工艺下面积仅0.064mm²，支持2GHz频率。

Conclusion: 共享缓存设计有助于未来AI加速器系统的开发。

Abstract: The rapid adoption of large language models (LLMs) is pushing AI accelerators toward increasingly powerful and specialized designs. Instead of further complicating software development with deeply hierarchical scratchpad memories (SPMs) and their asynchronous management, we investigate the opposite point of the design spectrum: a multi-core AI accelerator equipped with a shared system-level cache and application-aware management policies, which keeps the programming effort modest. Our approach exploits dataflow information available in the software stack to guide cache replacement (including dead-block prediction), in concert with bypass decisions and mechanisms that alleviate cache thrashing.
  We assess the proposal using a cycle-accurate simulator and observe substantial performance gains (up to 1.80x speedup) compared with conventional cache architectures. In addition, we build and validate an analytical model that takes into account the actual overlapping behaviors to extend the measurement results of our policies to real-world larger-scale workloads. Experiment results show that when functioning together, our bypassing and thrashing mitigation strategies can handle scenarios both with and without inter-core data sharing and achieve remarkable speedups.
  Finally, we implement the design in RTL and the area of our design is $\mathbf{0.064mm^2}$ with 15nm process, which can run at 2 GHz clock frequency. Our findings explore the potential of the shared cache design to assist the development of future AI accelerator systems.

</details>


### [9] [aLEAKator: HDL Mixed-Domain Simulation for Masked Hardware \& Software Formal Verification](https://arxiv.org/abs/2512.07520)
*Noé Amiot,Quentin L. Meunier,Karine Heydemann,Emmanuelle Encrenaz*

Main category: cs.AR

TL;DR: aLEAKator是一个开源框架，用于从HDL描述中自动形式化验证掩码加密加速器和CPU上运行的软件。


<details>
  <summary>Details</summary>
Motivation: 现有验证方法受限于小型硬件模块、小规模程序、有限泄漏模型或需硬件先验知识，难以应对复杂泄漏模型下的安全验证挑战。

Method: 引入混合域仿真，支持多种1-探针泄漏模型，允许可变信号粒度，无需限于1位线，并支持查找表及无需目标CPU架构先验知识。

Result: 验证了包括完整一阶掩码AES在不同CPU上的实现，结果与现有工具和实际测量一致。

Conclusion: aLEAKator提供了一种通用且精确的掩码实现安全验证方法，适用于硬件和软件场景。

Abstract: Verifying the security of masked hardware and software implementations, under advanced leakage models, remains a significant challenge, especially then accounting for glitches, transitions and CPU micro-architectural specifics. Existing verification approaches are either restricted to small hardware gadgets, small programs on CPUs such as Sboxes, limited leakage models, or require hardware-specific prior knowledge. In this work, we present aLEAKator, an open-source framework for the automated formal verification of masked cryptographic accelerators and software running on CPUs from their HDL descriptions. Our method introduces mixed-domain simulation, enabling precise modeling and verification under various (including robust and relaxed) 1-probing leakage models, and supports variable signal granularity without being restricted to 1-bit wires. aLEAKator also supports verification in the presence of lookup tables, and does not require prior knowledge of the target CPU architecture. Our approach is validated against existing tools and real-world measurements while providing innovative results such as the verification of a full, first-order masked AES on various CPUs

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [10] [AI-Generated Compromises for Coalition Formation: Modeling, Simulation, and a Textual Case Study](https://arxiv.org/abs/2512.05983)
*Eyal Briman,Ehud Shapiro,Nimrod Talmon*

Main category: cs.MA

TL;DR: 本文提出了一种结合NLP与LLM的AI模型，用于在语义空间中生成支持多数且优于现状的妥协提案，以促进大规模民主文本协作编辑。


<details>
  <summary>Details</summary>
Motivation: 解决在联盟形成过程中如何有效寻找妥协提案的问题，特别是在民主化文本协作场景中传统工具的不足。

Method: 构建包含有限理性与不确定性的整体模型，利用自然语言处理与大语言模型建立语义度量空间，并设计算法推荐妥协点。

Result: 通过模拟多种联盟形成过程，验证了所提算法在协助民主化文本编辑（如起草社区章程）中的有效性。

Conclusion: AI技术有望显著提升大规模群体在文本协作中的协商效率与民主性，弥补现有工具的局限。

Abstract: The challenge of finding compromises between agent proposals is fundamental to AI sub-fields such as argumentation, mediation, and negotiation. Building on this tradition, Elkind et al. (2021) introduced a process for coalition formation that seeks majority-supported proposals preferable to the status quo, using a metric space where each agent has an ideal point. The crucial step in this iterative process involves identifying compromise proposals around which agent coalitions can unite. How to effectively find such compromise proposals, however, remains an open question. We address this gap by formalizing a holistic model that encompasses agent bounded rationality and uncertainty and developing AI models to generate such compromise proposals. We focus on the domain of collaboratively writing text documents -- e.g., to enable the democratic creation of a community constitution. We apply NLP (Natural Language Processing) techniques and utilize LLMs (Large Language Models) to create a semantic metric space for text and develop algorithms to suggest suitable compromise points. To evaluate the effectiveness of our algorithms, we simulate various coalition formation processes and demonstrate the potential of AI to facilitate large-scale democratic text editing, such as collaboratively drafting a constitution, an area where traditional tools are limited.

</details>


### [11] [HiveMind: Contribution-Guided Online Prompt Optimization of LLM Multi-Agent Systems](https://arxiv.org/abs/2512.06432)
*Yihan Xia,Taotao Wang,Shengli Zhang,Zhangyuhua Weng,Bin Cao,Soung Chang Liew*

Main category: cs.MA

TL;DR: HiveMind框架通过贡献分析实现LLM多智能体协作的自适应优化，提出DAG-Shapley算法高效计算Shapley值，显著降低计算成本并保持高精度。


<details>
  <summary>Details</summary>
Motivation: 现有LLM多智能体系统缺乏对个体效能的评估与在线优化机制，难以支持复杂协作场景中的动态调整。

Method: 引入基于Shapley值的贡献度量，并设计DAG-Shapley算法，利用有向无环图结构剪枝无效联盟，复用中间输出以减少冗余计算。

Result: 在股票交易场景中性能优于静态基线，DAG-Shapley减少80%以上LLM调用且保持与完整Shapley值相当的归因精度。

Conclusion: HiveMind为多智能体协作提供了高效、可扩展的在线优化方案，推动了LLM智能体系统在现实场景中的落地应用。

Abstract: Recent advances in LLM-based multi-agent systems have demonstrated remarkable capabilities in complex decision-making scenarios such as financial trading and software engineering. However, evaluating each individual agent's effectiveness and online optimization of underperforming agents remain open challenges. To address these issues, we present HiveMind, a self-adaptive framework designed to optimize LLM multi-agent collaboration through contribution analysis. At its core, HiveMind introduces Contribution-Guided Online Prompt Optimization (CG-OPO), which autonomously refines agent prompts based on their quantified contributions. We first propose the Shapley value as a grounded metric to quantify each agent's contribution, thereby identifying underperforming agents in a principled manner for automated prompt refinement. To overcome the computational complexity of the classical Shapley value, we present DAG-Shapley, a novel and efficient attribution algorithm that leverages the inherent Directed Acyclic Graph structure of the agent workflow to axiomatically prune non-viable coalitions. By hierarchically reusing intermediate outputs of agents in the DAG, our method further reduces redundant computations, and achieving substantial cost savings without compromising the theoretical guarantees of Shapley values. Evaluated in a multi-agent stock-trading scenario, HiveMind achieves superior performance compared to static baselines. Notably, DAG-Shapley reduces LLM calls by over 80\% while maintaining attribution accuracy comparable to full Shapley values, establishing a new standard for efficient credit assignment and enabling scalable, real-world optimization of multi-agent collaboration.

</details>


### [12] [Analyzing Collision Rates in Large-Scale Mixed Traffic Control via Multi-Agent Reinforcement Learning](https://arxiv.org/abs/2512.06645)
*Muyang Fan*

Main category: cs.MA

TL;DR: 该研究探讨了多智能体强化学习在混合交通控制中影响碰撞率的关键因素，为提升交通安全与效率提供实践指导。


<details>
  <summary>Details</summary>
Motivation: 解决人类驾驶车辆与自动驾驶车辆在动态不确定环境下的碰撞风险问题，提高混合交通系统的安全性。

Method: 通过受控仿真实验，分析车辆总数、信号灯配置及转向策略三个维度对碰撞率的影响。

Result: 碰撞率受交通密度、信号协调程度和转向控制设计显著影响。

Conclusion: 研究结果有助于优化基于MARL的混合交通控制系统，在保障效率的同时提升安全性和鲁棒性。

Abstract: Vehicle collisions remain a major challenge in large-scale mixed traffic systems, especially when human-driven vehicles (HVs) and robotic vehicles (RVs) interact under dynamic and uncertain conditions. Although Multi-Agent Reinforcement Learning (MARL) offers promising capabilities for traffic signal control, ensuring safety in such environments remains difficult. As a direct indicator of traffic risk, the collision rate must be well understood and incorporated into traffic control design. This study investigates the primary factors influencing collision rates in a MARL-governed Mixed Traffic Control (MTC) network. We examine three dimensions: total vehicle count, signalized versus unsignalized intersection configurations, and turning-movement strategies. Through controlled simulation experiments, we evaluate how each factor affects collision likelihood. The results show that collision rates are sensitive to traffic density, the level of signal coordination, and turning-control design. These findings provide practical insights for improving the safety and robustness of MARL-based mixed traffic control systems, supporting the development of intelligent transportation systems in which both efficiency and safety are jointly optimized.

</details>


### [13] [Characterizing Lane-Changing Behavior in Mixed Traffic](https://arxiv.org/abs/2512.07219)
*Sungyong Chung,Alireza Talebpour,Samer H. Hamdar*

Main category: cs.MA

TL;DR: 本研究通过博弈论框架分析混合交通中车道变换行为，发现自动驾驶车辆比人工驾驶车辆更倾向于合作，并且重复交互会促进合作行为增加。


<details>
  <summary>Details</summary>
Motivation: 为了确保混合交通中的安全与效率，需要深入理解自动驾驶车辆存在时的车道变换行为及其互动模式。

Method: 利用Waymo开放运动数据集中的轨迹数据，采用k-means聚类和量化响应均衡框架建模，并结合演化博弈理论分析合作行为演变。

Result: 约4%和11%的车道变换事件中存在社会困境，主要为猎鹿博弈或囚徒困境；蒙特卡洛模拟表明，无论自动驾驶渗透率如何，重复交互均能提升合作行为。

Conclusion: 自动驾驶车辆在车道变换中表现出更高合作性，且长期重复交互有助于促进整体交通环境的合作演化。

Abstract: Characterizing and understanding lane-changing behavior in the presence of automated vehicles (AVs) is crucial to ensuring safety and efficiency in mixed traffic. Accordingly, this study aims to characterize the interactions between the lane-changing vehicle (active vehicle) and the vehicle directly impacted by the maneuver in the target lane (passive vehicle). Utilizing real-world trajectory data from the Waymo Open Motion Dataset (WOMD), this study explores patterns in lane-changing behavior and provides insight into how these behaviors evolve under different AV market penetration rates (MPRs). In particular, we propose a game-theoretic framework to analyze cooperative and defective behaviors in mixed traffic, applied to the 7,636 observed lane-changing events in the WOMD. First, we utilize k-means clustering to classify vehicles as cooperative or defective, revealing that the proportions of cooperative AVs are higher than those of HDVs in both active and passive roles. Next, we jointly estimate the utilities of active and passive vehicles to model their behaviors using the quantal response equilibrium framework. Empirical payoff tables are then constructed based on these utilities. Using these payoffs, we analyze the presence of social dilemmas and examine the evolution of cooperative behaviors using evolutionary game theory. Our results reveal the presence of social dilemmas in approximately 4% and 11% of lane-changing events for active and passive vehicles, respectively, with most classified as Stag Hunt or Prisoner's Dilemma (Chicken Game rarely observed). Moreover, the Monte Carlo simulation results show that repeated lane-changing interactions consistently lead to increased cooperative behavior over time, regardless of the AV penetration rate.

</details>


### [14] [Understanding LLM Agent Behaviours via Game Theory: Strategy Recognition, Biases and Multi-Agent Dynamics](https://arxiv.org/abs/2512.07462)
*Trung-Kiet Huynh,Duy-Minh Dao-Sy,Thanh-Bang Cao,Phong-Hao Le,Hong-Dan Nguyen,Phu-Quy Nguyen-Lam,Minh-Luan Nguyen-Vo,Hong-Phat Pham,Phu-Hoa Pham,Thien-Kim Than,Chi-Nguyen Tran,Huy Tran,Gia-Thoai Tran-Le,Alessio Buscemi,Le Hong Trang,The Anh Han*

Main category: cs.MA

TL;DR: 本文扩展FAIRGAME框架，通过新设计的博弈环境评估大语言模型在重复社会困境中的策略行为，揭示其合作倾向与语言、架构相关性，为AI治理提供方法论基础。


<details>
  <summary>Details</summary>
Motivation: 理解大语言模型在交互与多智能体系统中的策略行为，对AI安全、协调及社会经济基础设施设计至关重要。

Method: 扩展FAIRGAME框架，引入收益缩放囚徒困境与动态收益公共物品博弈，并用监督分类模型解析行为意图。

Result: 发现LLM行为具有一致性特征：激励敏感合作、跨语言差异、终局趋向背叛；语言框架影响可媲美架构差异。

Conclusion: 该研究为审计LLM作为策略智能体提供了统一方法论，并揭示其合作偏差，对AI治理与多智能体系统设计有直接意义。

Abstract: As Large Language Models (LLMs) increasingly operate as autonomous decision-makers in interactive and multi-agent systems and human societies, understanding their strategic behaviour has profound implications for safety, coordination, and the design of AI-driven social and economic infrastructures. Assessing such behaviour requires methods that capture not only what LLMs output, but the underlying intentions that guide their decisions. In this work, we extend the FAIRGAME framework to systematically evaluate LLM behaviour in repeated social dilemmas through two complementary advances: a payoff-scaled Prisoners Dilemma isolating sensitivity to incentive magnitude, and an integrated multi-agent Public Goods Game with dynamic payoffs and multi-agent histories. These environments reveal consistent behavioural signatures across models and languages, including incentive-sensitive cooperation, cross-linguistic divergence and end-game alignment toward defection. To interpret these patterns, we train traditional supervised classification models on canonical repeated-game strategies and apply them to FAIRGAME trajectories, showing that LLMs exhibit systematic, model- and language-dependent behavioural intentions, with linguistic framing at times exerting effects as strong as architectural differences. Together, these findings provide a unified methodological foundation for auditing LLMs as strategic agents and reveal systematic cooperation biases with direct implications for AI governance, collective decision-making, and the design of safe multi-agent systems.

</details>


### [15] [Understanding Individual Decision-Making in Multi-Agent Reinforcement Learning: A Dynamical Systems Approach](https://arxiv.org/abs/2512.07588)
*James Rudd-Jones,María Pérez-Ortiz,Mirco Musolesi*

Main category: cs.MA

TL;DR: 本文提出一种新视角，将多智能体强化学习系统建模为耦合随机动力系统，以更准确分析个体行为的稳定性和敏感性。


<details>
  <summary>Details</summary>
Motivation: 传统方法依赖平均场近似忽略随机性，导致分析预测与实际轨迹不符，难以满足实际部署中的安全需求。

Method: 利用动力系统理论工具，将MARL建模为耦合随机动力系统，捕捉智能体交互与环境特性。

Result: 首次能严谨研究MARL动力学并考虑其固有随机性，加深对系统行为的理解。

Conclusion: 该框架为多智能体学习过程的设计与控制提供了实用见解，尤其适用于高安全性要求场景。

Abstract: Analysing learning behaviour in Multi-Agent Reinforcement Learning (MARL) environments is challenging, in particular with respect to \textit{individual} decision-making. Practitioners frequently tend to study or compare MARL algorithms from a qualitative perspective largely due to the inherent stochasticity in practical algorithms arising from random dithering exploration strategies, environment transition noise, and stochastic gradient updates to name a few. Traditional analytical approaches, such as replicator dynamics, often rely on mean-field approximations to remove stochastic effects, but this simplification, whilst able to provide general overall trends, might lead to dissonance between analytical predictions and actual realisations of individual trajectories. In this paper, we propose a novel perspective on MARL systems by modelling them as \textit{coupled stochastic dynamical systems}, capturing both agent interactions and environmental characteristics. Leveraging tools from dynamical systems theory, we analyse the stability and sensitivity of agent behaviour at individual level, which are key dimensions for their practical deployments, for example, in presence of strict safety requirements. This framework allows us, for the first time, to rigorously study MARL dynamics taking into consideration their inherent stochasticity, providing a deeper understanding of system behaviour and practical insights for the design and control of multi-agent learning processes.

</details>


<div id='cs.PF'></div>

# cs.PF [[Back]](#toc)

### [16] [Predictive Modeling of I/O Performance for Machine Learning Training Pipelines: A Data-Driven Approach to Storage Optimization](https://arxiv.org/abs/2512.06699)
*Karthik Prabhakar*

Main category: cs.PF

TL;DR: 本文提出一种基于机器学习的方法，通过XGBoost模型预测I/O性能并推荐最优存储配置，显著缩短ML训练管道的调优时间。


<details>
  <summary>Details</summary>
Motivation: 现代机器学习训练常受限于数据I/O瓶颈，导致GPU利用率低下，需高效预测与优化存储配置。

Method: 系统性基准测试收集141组数据，涵盖多种存储后端、数据格式与访问模式，并评估7种回归与3种分类模型，最终选用XGBoost进行预测。

Result: XGBoost模型达到R²=0.991，平均误差仅11.8%，关键特征为吞吐量指标与批大小。

Conclusion: 该数据驱动方法可将配置时间从数天缩短至数分钟，具备可复现性与扩展性，适用于其他ML系统资源管理问题。

Abstract: Modern machine learning training is increasingly bottlenecked by data I/O rather than compute. GPUs often sit idle at below 50% utilization waiting for data. This paper presents a machine learning approach to predict I/O performance and recommend optimal storage configurations for ML training pipelines. We collected 141 observations through systematic benchmarking across different storage backends (NVMe SSD, network-attached storage, in-memory filesystems), data formats, and access patterns, covering both low-level I/O operations and full training pipelines. After evaluating seven regression models and three classification approaches, XGBoost achieved the best performance with R-squared of 0.991, predicting I/O throughput within 11.8% error on average. Feature importance analysis revealed that throughput metrics and batch size are the primary performance drivers. This data-driven approach can reduce configuration time from days of trial-and-error to minutes of predictive recommendation. The methodology is reproducible and extensible to other resource management problems in ML systems. Code and data are available at https://github.com/knkarthik01/gpu_storage_ml_project

</details>


### [17] [AFarePart: Accuracy-aware Fault-resilient Partitioner for DNN Edge Accelerators](https://arxiv.org/abs/2512.07449)
*Mukta Debnath,Krishnendu Guha,Debasri Saha,Amlan Chakrabarti,Susmita Sur-Kolay*

Main category: cs.PF

TL;DR: 本文提出了一种兼顾准确率、容错性的多目标DNN划分框架，显著提升模型在故障环境下的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 分布式资源受限平台中DNN划分的可靠性问题尚未充分研究，尤其在安全关键场景下亟需解决。

Method: 采用NSGA-II算法进行多目标优化，将故障条件下的精度下降作为核心指标，并通过运行时故障注入与反馈机制优先选择容错划分方案。

Result: 在AlexNet、SqueezeNet和ResNet18上验证，容错能力最高提升27.7%，性能开销增加极小。

Conclusion: 将容错性纳入DNN划分框架，可为易错环境中的稳健AI推理奠定基础。

Abstract: Deep Neural Networks (DNNs) are increasingly deployed across distributed and resource-constrained platforms, such as System-on-Chip (SoC) accelerators and edge-cloud systems. DNNs are often partitioned and executed across heterogeneous processing units to optimize latency and energy. However, the reliability of these partitioned models under hardware faults and communication errors remains a critical yet underexplored topic, especially in safety-critical applications. In this paper, we propose an accuracy-aware, fault-resilient DNN partitioning framework targeting multi-objective optimization using NSGA-II, where accuracy degradation under fault conditions is introduced as a core metric alongside energy and latency. Our framework performs runtime fault injection during optimization and utilizes a feedback loop to prioritize fault-tolerant partitioning. We evaluate our approach on benchmark CNNs including AlexNet, SqueezeNet and ResNet18 on hardware accelerators, and demonstrate up to 27.7% improvement in fault tolerance with minimal increase in performance overhead. Our results highlight the importance of incorporating resilience into DNN partitioning, and thereby paving the way for robust AI inference in error-prone environments.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [18] [WaggleNet: A LoRa and MQTT-Based Monitoring System for Internal and External Beehive Conditions](https://arxiv.org/abs/2512.07408)
*Minju Jeon,Jiyun Kim,Sewon Kim,Seongmin Park,Bo Zhang,Anthony H. Smith*

Main category: cs.NI

TL;DR: WaggleNet是一种低成本、双范围监测系统，通过LoRa-MQTT架构同步采集蜂箱内外环境数据，支持精准养蜂。


<details>
  <summary>Details</summary>
Motivation: 现有智能蜂箱系统忽视外部环境因素，且成本高、扩展性差，无法满足实际养蜂需求。

Method: 部署约15美元的模块化节点，结合温湿度、光照、GPS传感器与LoRa-MQTT网关，实现内外数据采集与云端传输。

Result: 实测在110米视距和95米遮挡环境下实现100%数据包送达，端到端延迟低于5秒，稳定运行两个月。

Conclusion: WaggleNet填补了蜂群内外环境协同监测空白，助力资源受限场景下的数据驱动型养蜂管理。

Abstract: Bee populations are declining globally due to habitat loss, pesticide exposure, and climate change, threatening agricultural productivity and food security. While existing smart beehive systems monitor internal conditions, they typically overlook external environmental factors that significantly influence colony health, and are constrained by high cost, limited scalability, and inadequate contextual analysis. We present WaggleNet, a novel dual-scope monitoring system that simultaneously captures both internal hive conditions and external environmental parameters using a cost-effective LoRa-MQTT architecture. Our system deploys modular worker nodes ($\sim$\$15 each) equipped with temperature, humidity, light, and GPS sensors both inside and around beehives. A master node functions as a LoRa-MQTT gateway, forwarding data to a cloud server with a mobile application interface. Field experiments confirmed reliable operation with 100\% packet delivery over 110 meters in line-of-sight conditions and 95 meters in obstructed environments, including successful deployment inside wooden hive structures. Our system demonstrated stable end-to-end latency under 5 seconds and continuous operation over a two-month period across diverse environmental conditions. By bridging the gap between internal and external monitoring, WaggleNet enables contextual anomaly detection and supports data-driven precision beekeeping in resource-constrained settings.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [19] [Auto-SPT: Automating Semantic Preserving Transformations for Code](https://arxiv.org/abs/2512.06042)
*Ashish Hooda,Mihai Christodorescu,Chuangang Ren,Aaron Wilson,Kassem Fawaz,Somesh Jha*

Main category: cs.SE

TL;DR: Auto-SPT 是一种基于大语言模型的框架，用于自动生成语义保持变换（SPT），以增强代码克隆检测模型对真实世界代码变换的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有代码克隆检测模型多在干净结构化数据上训练，难以应对真实代码中常见的语义保持变换，导致性能下降。

Method: 利用大语言模型自动生成多样化的 SPT，并组合生成强变换，用于构建合成数据集或增强训练数据。

Result: 实验证明 Auto-SPT 生成的 SPT 比现有方法更丰富多样，能显著降低当前最优检测器性能，同时提升模型对对抗性变换的鲁棒性。

Conclusion: Auto-SPT 有效弥合了训练与测试数据间的语义鸿沟，为构建鲁棒代码克隆检测系统提供了新途径。

Abstract: Machine learning (ML) models for code clone detection determine whether two pieces of code are semantically equivalent, which in turn is a key building block for software-engineering tasks like refactoring and security tasks like vulnerability and malware detection. While these models are predominantly trained on clean, structured code datasets, real-world code often undergoes a variety of semantic-preserving transformations, including refactoring, minification, automated formatting, and compiler optimizations. To address this critical gap between training and test data, we propose Auto-SPT, a novel framework to automatically construct synthetic-data generators for code. Auto-SPT is designed to produce Semantic Preserving Transformations (SPTs) that alter a program's syntactic structure while preserving its functionality and is instantiated on top of Large Language Models (LLMs). In particular, we use LLMs to craft a diverse set of SPTs, generate strong implementations for these SPTs, and compose them to result into strong transformations. Our formal analysis shows that the diversity of SPTs impacts the strength of their composition. We then empirically demonstrate that Auto-SPT generates more diverse SPTs than existing approaches and these SPTs significantly drop the performance of state-of-the-art code clone detectors. Further experiments show Auto-SPT can be used to enhance code datasets for training, to produce code-clone detection models that are robust to real-world, adversarial code transformations.

</details>


### [20] [Beyond Prototyping: Autonomous, Enterprise-Grade Frontend Development from Pixel to Production via a Specialized Multi-Agent Framework](https://arxiv.org/abs/2512.06046)
*Ramprasath Ganesaraja,Swathika N,Saravanan AP,Kamalkumar Rathinasamy,Chetana Amancharla,Rahul Das,Sahil Dilip Panse,Aditya Batwe,Dileep Vijayan,Veena Ashok,Thanushree A P,Kausthubh J Rao,Alden Olivero,Roshan,Rajeshwar Reddy Manthena,Asmitha Yuga Sre A,Harsh Tripathi,Suganya Selvaraj,Vito Chin,Kasthuri Rangan Bhaskar,Kasthuri Rangan Bhaskar,Venkatraman R,Sajit Vijayakumar*

Main category: cs.SE

TL;DR: AI4UI是一个面向企业级前端开发的自主智能体框架，通过设计阶段嵌入语法与后期专家微调实现全流程自动化，显著提升交付效率与代码质量。


<details>
  <summary>Details</summary>
Motivation: 解决通用代码助手在企业生产环境中安全性、合规性、可维护性不足的问题，满足企业对高质量UI代码的严苛需求。

Method: 引入Figma语法规则、领域知识图谱、安全代码集成策略、专家架构模板及多角色协同工作流，结合人机协作机制实现设计到代码的全自动转化。

Result: 在多项基准测试中表现优异：平台兼容性97.24%、编译成功率87.10%、安全合规86.98%，并获200名专家盲测认可，交付周期从数月缩短至数周。

Conclusion: AI4UI在企业级前端自动化开发中具备领先竞争力，兼顾效率、质量与合规，是迈向高可靠AI工程化的重要实践。

Abstract: We present AI4UI, a framework of autonomous front-end development agents purpose-built to meet the rigorous requirements of enterprise-grade application delivery. Unlike general-purpose code assistants designed for rapid prototyping, AI4UI focuses on production readiness delivering secure, scalable, compliant, and maintainable UI code integrated seamlessly into enterprise workflows. AI4UI operates with targeted human-in-the-loop involvement: at the design stage, developers embed a Gen-AI-friendly grammar into Figma prototypes to encode requirements for precise interpretation; and at the post processing stage, domain experts refine outputs for nuanced design adjustments, domain-specific optimizations, and compliance needs. Between these stages, AI4UI runs fully autonomously, converting designs into engineering-ready UI code. Technical contributions include a Figma grammar for autonomous interpretation, domain-aware knowledge graphs, a secure abstract/package code integration strategy, expertise driven architecture templates, and a change-oriented workflow coordinated by specialized agent roles. In large-scale benchmarks against industry baselines and leading competitor systems, AI4UI achieved 97.24% platform compatibility, 87.10% compilation success, 86.98% security compliance, 78.00% feature implementation success, 73.50% code-review quality, and 73.36% UI/UX consistency. In blind preference studies with 200 expert evaluators, AI4UI emerged as one of the leaders demonstrating strong competitive standing among leading solutions. Operating asynchronously, AI4UI generates thousands of validated UI screens in weeks rather than months, compressing delivery timeline

</details>


### [21] [Reinforcement Learning Integrated Agentic RAG for Software Test Cases Authoring](https://arxiv.org/abs/2512.06060)
*Mohanakrishnan Hariharan*

Main category: cs.SE

TL;DR: 该论文提出了一种结合强化学习与自主智能体的框架，用于在质量工程流程中持续优化从需求文档自动生成测试用例的能力。


<details>
  <summary>Details</summary>
Motivation: 传统基于大语言模型的系统依赖静态知识库，无法随时间持续提升性能，限制了自动化测试的发展。

Method: 采用强化学习算法（PPO和DQN）驱动的智能体，结合混合向量-图谱知识库，依据QE反馈、缺陷发现等指标动态优化测试生成策略。

Result: 在苹果企业项目实验中，测试生成准确率提升2.4%，缺陷检出率提高10.8%。

Conclusion: 该框架构建了由QE专家驱动的知识持续优化闭环，在增强而非取代人工测试能力的同时，实现测试用例质量的渐进式提升。

Abstract: This paper introduces a framework that integrates reinforcement learning (RL) with autonomous agents to enable continuous improvement in the automated process of software test cases authoring from business requirement documents within Quality Engineering (QE) workflows. Conventional systems employing Large Language Models (LLMs) generate test cases from static knowledge bases, which fundamentally limits their capacity to enhance performance over time. Our proposed Reinforcement Infused Agentic RAG (Retrieve, Augment, Generate) framework overcomes this limitation by employing AI agents that learn from QE feedback, assessments, and defect discovery outcomes to automatically improve their test case generation strategies. The system combines specialized agents with a hybrid vector-graph knowledge base that stores and retrieves software testing knowledge. Through advanced RL algorithms, specifically Proximal Policy Optimization (PPO) and Deep Q-Networks (DQN), these agents optimize their behavior based on QE-reported test effectiveness, defect detection rates, and workflow metrics. As QEs execute AI-generated test cases and provide feedback, the system learns from this expert guidance to improve future iterations. Experimental validation on enterprise Apple projects yielded substantive improvements: a 2.4% increase in test generation accuracy (from 94.8% to 97.2%), and a 10.8% improvement in defect detection rates. The framework establishes a continuous knowledge refinement loop driven by QE expertise, resulting in progressively superior test case quality that enhances, rather than replaces, human testing capabilities.

</details>


### [22] [Toward Patch Robustness Certification and Detection for Deep Learning Systems Beyond Consistent Samples](https://arxiv.org/abs/2512.06123)
*Qilin Zhou,Zhengyuan Wei,Haipeng Wang,Zhuo Wang,W. K. Chan*

Main category: cs.SE

TL;DR: HiCert是一种新型的基于掩码的认证检测技术，能全面认证对抗性补丁攻击中的不一致和一致样本，显著提升认证效果。


<details>
  <summary>Details</summary>
Motivation: 现有认证检测方法对误分类或预测不一致的样本无效，亟需改进以增强防御能力。

Method: 通过分析有害样本与良性样本之间的形式化关系，设定最大置信度边界，系统性地认证不一致和一致样本。

Result: 实验表明HiCert在认证更多良性样本、提高无警告样本准确率及降低假沉默率方面达到新SOTA性能。

Conclusion: HiCert是首个能提供全面补丁鲁棒性认证的方案，显著优于现有方法。

Abstract: Patch robustness certification is an emerging kind of provable defense technique against adversarial patch attacks for deep learning systems. Certified detection ensures the detection of all patched harmful versions of certified samples, which mitigates the failures of empirical defense techniques that could (easily) be compromised. However, existing certified detection methods are ineffective in certifying samples that are misclassified or whose mutants are inconsistently pre icted to different labels. This paper proposes HiCert, a novel masking-based certified detection technique. By focusing on the problem of mutants predicted with a label different from the true label with our formal analysis, HiCert formulates a novel formal relation between harmful samples generated by identified loopholes and their benign counterparts. By checking the bound of the maximum confidence among these potentially harmful (i.e., inconsistent) mutants of each benign sample, HiCert ensures that each harmful sample either has the minimum confidence among mutants that are predicted the same as the harmful sample itself below this bound, or has at least one mutant predicted with a label different from the harmful sample itself, formulated after two novel insights. As such, HiCert systematically certifies those inconsistent samples and consistent samples to a large extent. To our knowledge, HiCert is the first work capable of providing such a comprehensive patch robustness certification for certified detection. Our experiments show the high effectiveness of HiCert with a new state-of the-art performance: It certifies significantly more benign samples, including those inconsistent and consistent, and achieves significantly higher accuracy on those samples without warnings and a significantly lower false silent ratio.

</details>


### [23] [Systematically Thinking about the Complexity of Code Structuring Exercises at Introductory Level](https://arxiv.org/abs/2512.06178)
*Georgiana Haldeman,Peter Ohmann,Paul Denny*

Main category: cs.SE

TL;DR: 本文提出一个评估代码结构任务复杂性的框架，以帮助学生在程序设计中掌握分解与抽象技能。


<details>
  <summary>Details</summary>
Motivation: 随着生成式AI降低语法关注、提升高层次代码推理的重要性，有必要在编程课程中明确教授分解与抽象。

Method: 构建包含重复性、代码模式和数据依赖三个维度的复杂度评估框架，并提供配套示例任务与交互工具。

Result: 该框架能有效支持教育者设计循序渐进的DA训练任务，提升学生在过程式编程中的抽象能力。

Conclusion: 该框架为系统化教学分解与抽象提供了实用基础，有助于强化计算思维核心能力的培养。

Abstract: Decomposition and abstraction is an essential component of computational thinking, yet it is not always emphasized in introductory programming courses. In addition, as generative AI further reduces the focus on syntax and increases the importance of higher-level code reasoning, there is renewed opportunity to teach DA explicitly. In this paper, we introduce a framework for systematically assessing the complexity of code structuring tasks, where students must identify and separate meaningful abstractions within existing, unstructured code. The framework defines three dimensions of task complexity, each with multiple levels: repetition, code pattern, and data dependency. To support practical use, we provide example tasks mapped to these levels and offer an interactive tool for generating and exploring DA problems. The framework is designed to support the development of educational tasks that build students' skills with DA in the procedural paradigm.

</details>


### [24] [CFCEval: Evaluating Security Aspects in Code Generated by Large Language Models](https://arxiv.org/abs/2512.06248)
*Cheng Cheng,Jinqiu Yang*

Main category: cs.SE

TL;DR: CFCEval是一个用于评估代码生成大模型质量和安全性的新框架，通过引入MLVBench基准和ELRM指标，更全面准确地衡量生成代码的性能。


<details>
  <summary>Details</summary>
Motivation: 现有代码大模型评估方法存在数据集偏差和CodeBLEU指标缺陷，难以全面评估生成代码的质量与安全性。

Method: 提出CFCEval框架，构建MLVBench基准消除数据偏差，设计ELRM指标评估代码相关性，并从编程质量、漏洞修复能力、转换后修复能力和相关性四个维度进行评估。

Result: 实验证明CFCEval在质量和安全性评估上更有效，且ELRM指标比CodeBLEU更贴近人类判断。

Conclusion: CFCEval为未来代码大模型评估提供了更可靠的方法论基础。

Abstract: Code-focused Large Language Models (LLMs), such as CodeX and Star-Coder, have demonstrated remarkable capabilities in enhancing developer productivity through context-aware code generation. However, evaluating the quality and security of LLM-generated code remains a significant challenge. Existing evaluation protocols for Code LLMs lack both methodological rigor and comprehensive scope. A key limitation is dataset bias, which arises from unintentional overlap between training and testing data. Furthermore, while CodeBLEU, a BLEU-based metric, is widely used to assess code similarity, it suffers from critical shortcomings, including imprecise tokenization, structural limitations, and low reference diversity. To address these challenges, we introduce CFCEval, a novel framework for evaluating the quality and security of code generated by LLMs. CFCEval mitigates dataset bias by creating a new benchmark, MLVBench, and incorporates ELRM, a new metric designed to assess the relevance between reference code and generated code. CFCEval evaluates generated code across four dimensions: programming quality, vulnerability-fixing capability, post-transformation fixing capability, and relevance. Our experiments show that CFCEval not only captures both quality and security aspects of generated code more effectively but also that its ELRM aligns more closely with human judgments than CodeBLEU, thus paving the way for future advancements in Code LLMs evaluation.

</details>


### [25] [Translating PL/I Macro Procedures into Java Using Automatic Templatization and Large Language Models](https://arxiv.org/abs/2512.06448)
*Takaaki Tateishi,Yasuharu Katsuno*

Main category: cs.SE

TL;DR: 提出一种基于符号执行的模板化方法，提升LLM对PL/I宏程序到Java的翻译效果。


<details>
  <summary>Details</summary>
Motivation: 解决PL/I宏程序因字符串操作特性导致LLM翻译困难的问题。

Method: 通过符号执行生成含占位符的代码模板作为中间表示，辅助LLM生成可读、可维护的Java代码。

Result: 在10个PL/I宏程序上的初步实验表明，该方法能成功生成行为一致的Java程序。

Conclusion: 模板化方法有效提升LLM在复杂宏程序翻译任务中的准确性和实用性。

Abstract: Modernizing legacy enterprise systems often involves translating PL/I programs into modern languages such as Java. This task becomes significantly more complex when PL/I macro procedures are involved. The PL/I macro procedures are considered string-manipulating programs that generate PL/I code, and they make automated translation more complex. Recently, large language models (LLMs) have been explored for automated code translation. However, LLM-based code translation struggles to translate the PL/I macro procedures to Java programs that reproduce the behavior of the plain PL/I code generated by the original PL/I macro procedures.
  This paper proposes a novel method called templatization, which uses symbolic execution to generate code templates (code with named placeholders) as an intermediate representation. In this approach, symbolic values are treated as parts of macro-generated code. By symbolically executing macro procedures and generating code templates, our approach facilitates LLMs to generate readable and maintainable Java code. Our preliminary experiment on ten PL/I macro procedures shows that the LLM-based translation through templatization successfully generates Java programs that reproduce the behavior of the macro-generated PL/I programs.

</details>


### [26] [Leveraging LLMs to support co-evolution between definitions and instances of textual DSLs](https://arxiv.org/abs/2512.06836)
*Weixing Zhang,Regina Hebig,Daniel Strüber*

Main category: cs.SE

TL;DR: 本研究探讨了基于大语言模型（LLM）的方案在语法与实例协同演化中的可行性，特别关注其在处理文本实例时保留注释和布局等辅助信息的能力。


<details>
  <summary>Details</summary>
Motivation: 文本DSL在语法演进时易丢失注释和布局等重要信息，现有模型驱动方法不适用于文本语法，因此需探索LLM解决方案。

Method: 使用Claude-3.5和GPT-4o两个先进语言模型，在七种案例语言上进行实验评估。

Result: LLM在小规模、实例尺寸有限的情况下表现良好，但在大规模实例中面临显著可扩展性挑战。

Conclusion: LLM方案对部分实际场景可行，但其扩展性限制为未来研究提供了重要启示。

Abstract: Software languages evolve over time for various reasons, such as the addition of new features. When the language's grammar definition evolves, textual instances that originally conformed to the grammar become outdated. For DSLs in a model-driven engineering context, there exists a plethora of techniques to co-evolve models with the evolving metamodel. However, these techniques are not geared to support DSLs with a textual syntax -- applying them to textual language definitions and instances may lead to the loss of information from the original instances, such as comments and layout information, which are valuable for software comprehension and maintenance. This study explores the potential of Large Language Model (LLM)-based solutions in achieving grammar and instance co-evolution, with attention to their ability to preserve auxiliary information when directly processing textual instances. By applying two advanced language models, Claude-3.5 and GPT-4o, and conducting experiments across seven case languages, we evaluated the feasibility and limitations of this approach. Our results indicate a good ability of the considered LLMs for migrating textual instances in small-scale cases with limited instance size, which are representative of a subset of cases encountered in practice. In addition, we observe significant challenges with the scalability of LLM-based solutions to larger instances, leading to insights that are useful for informing future research.

</details>


### [27] [MINES: Explainable Anomaly Detection through Web API Invariant Inference](https://arxiv.org/abs/2512.06906)
*Wenjie Zhang,Yun Lin,Chun Fung Amos Kwok,Xiwen Teoh,Xiaofei Xie,Frank Liauw,Hongyu Zhang,Jin Song Dong*

Main category: cs.SE

TL;DR: MINES通过从API签名推断可解释的不变量，以检测Web应用中的异常行为，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现代Web应用常因API暴露而遭受攻击或非法访问，导致系统异常，但现有日志分析方法易受噪声干扰，难以精准识别异常。

Method: MINES将API签名转换为表结构增强数据库模式，利用LLM推断潜在约束，并结合正常日志筛选生成不变量，最终转化为Python代码验证运行时日志。

Result: 在多个基准测试中，MINES对异常的召回率高且几乎无误报，性能超越LogRobust、LogFormer和WebNorm等基线方法。

Conclusion: MINES提供了一种新颖且高效的Web应用异常检测方案，具备高精度与强解释性，达到当前最优水平。

Abstract: Detecting the anomalies of web applications, important infrastructures for running modern companies and governments, is crucial for providing reliable web services. Many modern web applications operate on web APIs (e.g., RESTful, SOAP, and WebSockets), their exposure invites intended attacks or unintended illegal visits, causing abnormal system behaviors. However, such anomalies can share very similar logs with normal logs, missing crucial information (which could be in database) for log discrimination. Further, log instances can be also noisy, which can further mislead the state-of-the-art log learning solutions to learn spurious correlation, resulting superficial models and rules for anomaly detection. In this work, we propose MINES which infers explainable API invariants for anomaly detection from the schema level instead of detailed raw log instances, which can (1) significantly discriminate noise in logs to identify precise normalities and (2) detect abnormal behaviors beyond the instrumented logs. Technically, MINES (1) converts API signatures into table schema to enhance the original database shema; and (2) infers the potential database constraints on the enhanced database schema to capture the potential relationships between APIs and database tables. MINES uses LLM for extracting potential relationship based on two given table structures; and use normal log instances to reject and accept LLM-generated invariants. Finally, MINES translates the inferred constraints into invariants to generate Python code for verifying the runtime logs. We extensively evaluate MINES on web-tamper attacks on the benchmarks of TrainTicket, NiceFish, Gitea, Mastodon, and NextCloud against baselines such as LogRobust, LogFormer, and WebNorm. The results show that MINES achieves high recall for the anomalies while introducing almost zero false positives, indicating a new state-of-the-art.

</details>


### [28] [Multi-Docker-Eval: A `Shovel of the Gold Rush' Benchmark on Automatic Environment Building for Software Engineering](https://arxiv.org/abs/2512.06915)
*Kelin Fu,Tianyu Liu,Zeyu Shang,Yingwei Ma,Jian Yang,Jiaheng Liu,Kaigui Bian*

Main category: cs.SE

TL;DR: 提出Multi-Docker-Eval基准，评估LLM在自动化环境配置中的表现，发现当前模型成功率低，且模型规模非决定因素。


<details>
  <summary>Details</summary>
Motivation: 解决软件工程自动化中环境配置的瓶颈问题，提供可靠评估标准。

Method: 构建涵盖40个真实仓库、9种编程语言的Multi-Docker-Eval基准，评估主流LLM与智能体框架的表现。

Result: 当前模型最高成功率仅37.7%，开源模型如DeepSeek-V3.1和Kimi-K2表现优异，智能体框架与编程语言显著影响结果。

Conclusion: 为构建可扩展的全自动软件工程流水线提供实用指导。

Abstract: Automated environment configuration is a critical bottleneck in scaling software engineering (SWE) automation. To provide a reliable evaluation standard for this task, we present Multi-Docker-Eval benchmark. It includes 40 real-world repositories spanning 9 programming languages and measures both success in achieving executable states and efficiency under realistic constraints. Our extensive evaluation of state-of-the-art LLMs and agent frameworks reveals key insights: (1) the overall success rate of current models is low (F2P at most 37.7%), with environment construction being the primary bottleneck; (2) model size and reasoning length are not decisive factors, and open-source models like DeepSeek-V3.1 and Kimi-K2 are competitive in both efficiency and effectiveness; (3) agent framework and programming language also have significantly influence on success rate. These findings provide actionable guidelines for building scalable, fully automated SWE pipelines.

</details>


### [29] [RisConFix: LLM-based Automated Repair of Risk-Prone Drone Configurations](https://arxiv.org/abs/2512.07122)
*Liping Han,Tingting Nie,Le Yu,Mingzhe Hu,Tao Yue*

Main category: cs.SE

TL;DR: RisConFix利用大语言模型实时修复无人机风险配置，提升飞行稳定性。


<details>
  <summary>Details</summary>
Motivation: 某些推荐参数组合仍可能导致飞行不稳定，降低无人机鲁棒性，需实时修复机制。

Method: 基于LLM分析参数与飞行状态关系，生成修正参数并迭代验证直至稳定。

Result: 在ArduPilot测试中修复成功率最高达97%，平均修复次数仅1.17次。

Conclusion: RisConFix能高效实时修复风险配置，显著增强无人机运行鲁棒性。

Abstract: Flight control software is typically designed with numerous configurable parameters governing multiple functionalities, enabling flexible adaptation to mission diversity and environmental uncertainty. Although developers and manufacturers usually provide recommendations for these parameters to ensure safe and stable operations, certain combinations of parameters with recommended values may still lead to unstable flight behaviors, thereby degrading the drone's robustness. To this end, we propose a Large Language Model (LLM) based approach for real-time repair of risk-prone configurations (named RisConFix) that degrade drone robustness. RisConFix continuously monitors the drone's operational state and automatically triggers a repair mechanism once abnormal flight behaviors are detected. The repair mechanism leverages an LLM to analyze relationships between configuration parameters and flight states, and then generates corrective parameter updates to restore flight stability. To ensure the validity of the updated configuration, RisConFix operates as an iterative process; it continuously monitors the drone's flight state and, if an anomaly persists after applying an update, automatically triggers the next repair cycle. We evaluated RisConFix through a case study of ArduPilot (with 1,421 groups of misconfigurations). Experimental results show that RisConFix achieved a best repair success rate of 97% and an optimal average number of repairs of 1.17, demonstrating its capability to effectively and efficiently repair risk-prone configurations in real time.

</details>


### [30] [Towards Benchmarking Design Pattern Detection Under Obfuscation: Reproducing and Evaluating Attention-Based Detection Method](https://arxiv.org/abs/2512.07193)
*Manthan Shenoy,Andreas Rausch*

Main category: cs.SE

TL;DR: 该论文评估了基于注意力机制的分类器在设计模式检测中的语义鲁棒性，发现其过度依赖表层语法特征，在代码混淆后性能显著下降。


<details>
  <summary>Details</summary>
Motivation: 现有设计模式检测工具可能依赖非语义特征，需评估其真实语义泛化能力。

Method: 复现DPDAtt方法，并构建混淆版语料库（替换标识符和字符串，保留结构逻辑），测试分类器性能变化。

Result: 混淆后分类器性能大幅下降，表明其严重依赖表层语法而非深层语义。

Conclusion: 需开发更鲁棒的设计模式检测工具，并推荐所构建混淆语料作为评估基准。

Abstract: This paper investigates the semantic robustness of attention-based classifiers for design pattern detection, particularly focusing on their reliance on structural and behavioral semantics. We reproduce the DPDAtt, an attention-based design pattern detection approach using learning-based classifiers, and evaluate its performance under obfuscation. To this end, we curate an obfuscated version of the DPDAtt Corpus, where the name identifiers in code such as class names, method names, etc., and string literals like print statements and comment blocks are replaced while preserving control flow, inheritance, and logic. Our findings reveal that these trained classifiers in DPDAtt depend significantly on superficial syntactic features, leading to substantial misclassification when such cues are removed through obfuscation. This work highlights the need for more robust detection tools capable of capturing deeper semantic meanings in source code. We propose our curated Obfuscated corpus (containing 34 Java source files) as a reusable proof-of-concept benchmark for evaluating state-of-the-art design pattern detectors on their true semantic generalization capabilities.

</details>


### [31] [The Human Need for Storytelling: Reflections on Qualitative Software Engineering Research With a Focus Group of Experts](https://arxiv.org/abs/2512.07293)
*Roberto Verdecchia,Justus Bogner*

Main category: cs.SE

TL;DR: 本文回顾了定性研究在软件工程领域的发展历程、重要性及未来展望。


<details>
  <summary>Details</summary>
Motivation: 探讨定性研究在以定量为主导的软件工程研究中的价值与挑战。

Method: 通过专家焦点小组讨论的形式进行反思与分析。

Result: 揭示了定性研究的演进过程、当前实践障碍及未来可能方向。

Conclusion: 定性研究虽面临挑战，但在软件工程领域具有不可替代的重要性。

Abstract: From its first adoption in the late 80s, qualitative research has slowly but steadily made a name for itself in what was, and perhaps still is, the predominantly quantitative software engineering (SE) research landscape. As part of our regular column on empirical software engineering (ACM SIGSOFT SEN-ESE), we reflect on the state of qualitative SE research with a focus group of experts. Among other things, we discuss why qualitative SE research is important, how it evolved over time, common impediments faced while practicing it today, and what the future of qualitative SE research might look like. Joining the conversation are Rashina Hoda (Monash University, Australia), Carolyn Seaman (University of Maryland, United States), and Klaas Stol (University College Cork, Ireland). The content of this paper is a faithful account of our conversation from October 25, 2025, which we moderated and edited for our column.

</details>


### [32] [Do LLMs Trust the Code They Write?](https://arxiv.org/abs/2512.07404)
*Francisco Ribeiro,Claudio Spiess,Prem Devanbu,Sarah Nadi*

Main category: cs.SE

TL;DR: 通过挖掘大语言模型内部的正确性表征，提升代码生成的可靠性，无需执行测试即可筛选高质量代码。


<details>
  <summary>Details</summary>
Motivation: 大语言模型生成的代码常因输出概率与正确性不相关而错误，需改进其可靠性。

Method: 对比同一编程任务中正确与错误代码的隐藏状态，提取内部正确性表征，并在四个模型上验证其效果。

Result: 该方法优于标准对数似然排序和模型口头置信度，能有效筛选更高质量代码。

Conclusion: 利用内部表征可增强代码生成系统，提高自动生成代码的可信度。

Abstract: Despite the effectiveness of large language models (LLMs) for code generation, they often output incorrect code. One reason is that model output probabilities are often not well-correlated with correctness, and reflect only the final output of the generation process. Inspired by findings that LLMs internally encode concepts like truthfulness, this paper explores if LLMs similarly represent code correctness. Specifically, we identify a correctness representation inside LLMs by contrasting the hidden states between pairs of correct and incorrect code for the same programming tasks. By experimenting on four LLMs, we show that exploiting this extracted correctness representation outperforms standard log-likelihood ranking, as well as verbalized model confidence. Furthermore, we explore how this internal correctness signal can be used to select higher-quality code samples, without requiring test execution. Ultimately, this work demonstrates how leveraging internal representations can enhance code generation systems and make LLMs more reliable, thus improving confidence in automatically generated code.

</details>


### [33] [AutoICE: Automatically Synthesizing Verifiable C Code via LLM-driven Evolution](https://arxiv.org/abs/2512.07501)
*Weilin Luo,Xueyi Liang,Haotian Deng,Yanan Liu,Hai Wan*

Main category: cs.SE

TL;DR: AutoICE是一种基于大语言模型的进化搜索方法，用于从自然语言需求自动生成可验证的C代码，显著优于现有技术。


<details>
  <summary>Details</summary>
Motivation: 现有方法因缺乏领域特定语料和难以形式化隐含知识，导致语法与语义错误频发。

Method: 提出多样化个体初始化、协同交叉操作和自省式变异机制，以支持多样化的迭代更新并挖掘隐含知识。

Result: 在标准数据集上验证成功率达90.36%，在开发者友好数据集上达88.33%，均显著超越当前最优方法。

Conclusion: AutoICE有效提升了从自然语言生成可验证代码的准确率与实用性，推动了形式化方法的普及。

Abstract: Automatically synthesizing verifiable code from natural language requirements ensures software correctness and reliability while significantly lowering the barrier to adopting the techniques of formal methods. With the rise of large language models (LLMs), long-standing efforts at autoformalization have gained new momentum. However, existing approaches suffer from severe syntactic and semantic errors due to the scarcity of domain-specific pre-training corpora and often fail to formalize implicit knowledge effectively. In this paper, we propose AutoICE, an LLM-driven evolutionary search for synthesizing verifiable C code. It introduces the diverse individual initialization and the collaborative crossover to enable diverse iterative updates, thereby mitigating error propagation inherent in single-agent iterations. Besides, it employs the self-reflective mutation to facilitate the discovery of implicit knowledge. Evaluation results demonstrate the effectiveness of AutoICE: it successfully verifies $90.36$\% of code, outperforming the state-of-the-art (SOTA) approach. Besides, on a developer-friendly dataset variant, AutoICE achieves a $88.33$\% verification success rate, significantly surpassing the $65$\% success rate of the SOTA approach.

</details>


### [34] [Understanding Privacy Risks in Code Models Through Training Dynamics: A Causal Approach](https://arxiv.org/abs/2512.07814)
*Hua Yang,Alejandro Velasco,Sen Fang,Bowen Xu,Denys Poshyvanyk*

Main category: cs.SE

TL;DR: 该研究首次揭示代码大模型中不同类型的个人身份信息（PII）泄露风险存在因果差异，易学类型如IP地址更易泄露，而密码等难学类型泄露较少，为构建类型感知防御提供依据。


<details>
  <summary>Details</summary>
Motivation: 现有研究将PII视为单一类别，忽视不同类型间的异质风险，本文旨在探究不同PII类型在代码大模型中的学习与泄露风险是否存在因果关系。

Method: 构建多类型PII数据集，微调不同规模模型，计算真实PII数据的训练动态，并建立结构因果模型估计可学习性对泄露的因果效应。

Result: 泄露风险因PII类型显著不同：易学类型（如IP地址）泄露率高，难学类型（如密钥、密码）泄露少，模糊类型表现混合。

Conclusion: 首次提供因果证据表明泄露风险依赖于PII类型，建议开发类型感知和可学习性感知的防御机制以提升代码大模型隐私保护。

Abstract: Large language models for code (LLM4Code) have greatly improved developer productivity but also raise privacy concerns due to their reliance on open-source repositories containing abundant personally identifiable information (PII). Prior work shows that commercial models can reproduce sensitive PII, yet existing studies largely treat PII as a single category and overlook the heterogeneous risks among different types. We investigate whether distinct PII types vary in their likelihood of being learned and leaked by LLM4Code, and whether this relationship is causal. Our methodology includes building a dataset with diverse PII types, fine-tuning representative models of different scales, computing training dynamics on real PII data, and formulating a structural causal model to estimate the causal effect of learnability on leakage. Results show that leakage risks differ substantially across PII types and correlate with their training dynamics: easy-to-learn instances such as IP addresses exhibit higher leakage, while harder types such as keys and passwords leak less frequently. Ambiguous types show mixed behaviors. This work provides the first causal evidence that leakage risks are type-dependent and offers guidance for developing type-aware and learnability-aware defenses for LLM4Code.

</details>


### [35] [Studying the Role of Reusing Crowdsourcing Knowledge in Software Development](https://arxiv.org/abs/2512.07824)
*Rabe Abdalkareem*

Main category: cs.SE

TL;DR: 研究探讨了重用众包知识对软件项目的影响，发现其虽能提升开发效率，但也带来依赖过载和维护成本增加等问题，并提出通过持续集成改进来缓解风险。


<details>
  <summary>Details</summary>
Motivation: 当前众包平台如Stack Overflow缺乏对软件质量影响的实证研究，尤其不清楚开发者如何使用这些知识。

Method: 在Stack Overflow和npm等知名众包平台上进行大规模实证研究，分析代码重用行为及其对项目质量的影响，并探索持续集成（CI）作为质量保障手段的效果。

Result: 重用众包知识可辅助开发实践，但会导致依赖过载和维护负担；引入并优化CI有助于提高生产力并节省资源。

Conclusion: 应基于数据驱动决策，在利用众包知识的同时，通过改进CI等方法降低其对软件质量的负面影响。

Abstract: Crowdsourcing platforms, such as Stack Overflow, have changed and impacted the software development practice. In these platforms, developers share and reuse their software development and programming experience. Therefore, a plethora of research work focused on crowdsourcing in software engineering and showed that, among other things, crowdsourced development tends to increase developers' productivity and reduce time-to-market. However, in crowdsourcing, the empirical studies of software quality are lacking, and simple questions, such as what developers use the crowdsourcing knowledge for, are unanswered.
  Therefore, our research focused on studying the impact of reusing crowdsourcing knowledge on software projects. To do so, we conduct several large-scale empirical studies on some of the well-known crowdsourcing platforms, including Stack Overflow and npm. Our results showed that reusing knowledge from these crowdsourcing platforms has the potential to assist software development practice, specifically in the form of reusing crowdsourced code. However, using such knowledge affects the quality of the software in several aspects, such as making the software projects suffer from dependency overhead and increasing the maintenance effort. Based on these findings, we use the gained knowledge to make sound data-driven decisions where we examine software quality assurance methods to mitigate the risk of relying on crowd sourcing knowledge in software development. We examine the use of continuous integration (CI). Our analysis showed how CI can be improved to increase developers' productivity and save their resources.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [36] [Vec-LUT: Vector Table Lookup for Parallel Ultra-Low-Bit LLM Inference on Edge Devices](https://arxiv.org/abs/2512.06443)
*Xiangyu Li,Chengyu Yin,Weijun Wang,Jianyu Wei,Ting Cao,Yunxin Liu*

Main category: cs.DC

TL;DR: 提出向量LUT方法，优化超低位宽LLM在边缘设备上的并行推理效率。


<details>
  <summary>Details</summary>
Motivation: 解决基于标量LUT的推理在多token场景下内存带宽利用率低的问题。

Method: 引入向量LUT范式、以向量LUT为中心的张量布局和缓存感知流式查找技术。

Result: 在5种边缘设备上实现最高4.2倍性能提升，并集成进llama.cpp。

Conclusion: 向量LUT显著提升超低位宽LLM推理效率，推动端侧智能普及。

Abstract: Large language models (LLMs) are increasingly deployed on edge devices. To meet strict resource constraints, real-world deployment has pushed LLM quantization from 8-bit to 4-bit, 2-bit, and now 1.58-bit. Combined with lookup table (LUT)-based inference, CPUs run these ultra-low-bit LLMs even faster than NPUs, opening new opportunities for ubiquitous on-device intelligence.
  However, this paper identifies that LUT-based inference underutilizes memory bandwidth during parallel inference, which is required for prefilling, test-time scaling, and other multi-token scenarios. The root cause is the scalar LUT paradigm, which performs repetitive and non-contiguous memory accesses for each token.
  To solve the issue, we propose vector LUT, a new lookup paradigm that constructs a unified LUT across parallel tokens, and performs a single $1 \rightarrow N$ lookup per index. To realize it efficiently, we further introduce (1) Vector LUT-Centric Tensor Layout, and (2) Cache-Aware Streamed Lookup techniques. Evaluations on 5 edge devices across 3 LLMs show that Vec-LUT outperforms state-of-the-art baselines by up to $4.2\times$. Our implementation is integrated into llama.cpp. The code is available at https://github.com/Cipherxzc/vlut.cpp.

</details>


### [37] [Cloud Revolution: Tracing the Origins and Rise of Cloud Computing](https://arxiv.org/abs/2512.06800)
*Deepa Gurung,S M Zia Ur Rashid,Zain ul Abdeen,Suman Rath*

Main category: cs.DC

TL;DR: 本文回顾云计算发展历程，分析其技术经济影响及当前挑战，并探讨边缘计算、AI优化架构和量子计算等新趋势如何塑造未来云环境。


<details>
  <summary>Details</summary>
Motivation: 重新审视云计算的历史演进及其对组织计算模式的深远影响，同时探讨当前局限与未来发展方向。

Method: 通过历史回顾与趋势分析，结合技术经济视角，系统梳理云计算发展脉络与新兴变革力量。

Result: 揭示云计算已从基础技术演变为全球性生态体系，正面临安全、合规与供应商锁定等挑战，同时受边缘融合、AI架构与量子服务等趋势驱动转型。

Conclusion: 云计算正处于快速演变阶段，其未来发展取决于在可扩展性、开放性与信任之间取得平衡。

Abstract: The history behind the development of cloud computing is more than several decades of technological progress in the fields of virtualization, distributed systems, and high-speed networking, but its current application is much broader than the underlying technologies that made it possible. This paper reexamines the historical evolution of the field, including the initial ideas of resource sharing and utility-based computing approaches and the development of hyperscale data centers and modern globally federated cloud ecosystems. We also analyze the technological and economic forces and point to the way cloud platforms altered the organizational computing habits, decreasing the entrance-level to the data-intensive and computation-heavy apps. The study also takes into account the ongoing limitations which have come with the large-scale adoption of clouds which include exposure to security due to the weaknesses in configuration, particular establishment regulations, and structural reliance on the single vendors. Lastly, we address some of the new trends that are transforming the cloud environment, including the convergence of edge and cloud infrastructure, the increased prominence of AI-optimised architectures and the initial adoption of quantum computing services. Collectively, the developments above describe an emerging but quickly changing paradigm with its future direction being determined by a strike of balancing between scalability, openness, and trust.

</details>


### [38] [Optimizing video analytics inference pipelines: a case study](https://arxiv.org/abs/2512.07009)
*Saeid Ghafouri,Yuming Ding,Katerine Diaz Chito,Jesús Martinez del Rincón,Niamh O'Connell,Hans Vandierendonck*

Main category: cs.DC

TL;DR: 通过系统级优化，实现家禽福利监测系统的2倍加速，同时保持模型精度。


<details>
  <summary>Details</summary>
Motivation: 精准畜牧监测需要高分辨率视频和近实时处理，计算负载大，需降低成本和提高可扩展性。

Method: 采用多级并行化、GPU加速代码替换CPU代码、向量化聚类和内存高效后处理等优化方法。

Result: 在真实农场视频数据上测试，管道性能提升高达2倍，且不损失模型精度。

Conclusion: 该研究为农业及其他大规模视频分析应用提供了构建高吞吐、低延迟推理系统的实用策略。

Abstract: Cost-effective and scalable video analytics are essential for precision livestock monitoring, where high-resolution footage and near-real-time monitoring needs from commercial farms generates substantial computational workloads. This paper presents a comprehensive case study on optimizing a poultry welfare monitoring system through system-level improvements across detection, tracking, clustering, and behavioral analysis modules. We introduce a set of optimizations, including multi-level parallelization, Optimizing code with substituting CPU code with GPU-accelerated code, vectorized clustering, and memory-efficient post-processing. Evaluated on real-world farm video footage, these changes deliver up to a 2x speedup across pipelines without compromising model accuracy. Our findings highlight practical strategies for building high-throughput, low-latency video inference systems that reduce infrastructure demands in agricultural and smart sensing deployments as well as other large-scale video analytics applications.

</details>


### [39] [ContinuumConductor : Decentralized Process Mining on the Edge-Cloud Continuum](https://arxiv.org/abs/2512.07280)
*Hendrik Reiter,Janick Edinger,Martin Kabierski,Agnes Koschmider,Olaf Landsiedel,Arvid Lepsien,Xixi Lu,Andrea Marrella,Estefania Serral,Stefan Schulte,Florian Tschorsch,Matthias Weidlich,Wilhelm Hasselbring*

Main category: cs.DC

TL;DR: 本文提出了一种在边缘-云连续体中实现去中心化流程挖掘的框架ContinuumConductor，以支持隐私保护、响应迅速且资源高效的流程分析。


<details>
  <summary>Details</summary>
Motivation: 现代工业物联网系统多运行于分布式、资源受限的边缘-云计算架构，传统集中式流程挖掘难以满足其需求。

Method: 设计分层决策框架ContinuumConductor，指导预处理、关联与发现等任务在边缘或云端执行，并分析各步骤去中心化与集中化的权衡。

Result: 在内河港口实际用例中验证了框架有效性，实现了计算感知的流程优化。

Conclusion: 本研究为信息物理系统与工业物联网中的计算感知流程挖掘奠定了基础。

Abstract: Process mining traditionally assumes centralized event data collection and analysis. However, modern Industrial Internet of Things systems increasingly operate over distributed, resource-constrained edge-cloud infrastructures. This paper proposes a structured approach for decentralizing process mining by enabling event data to be mined directly within the IoT systems edge-cloud continuum. We introduce ContinuumConductor a layered decision framework that guides when to perform process mining tasks such as preprocessing, correlation, and discovery centrally or decentrally. Thus, enabling privacy, responsive and resource-efficient process mining. For each step in the process mining pipeline, we analyze the trade-offs of decentralization versus centralization across these layers and propose decision criteria. We demonstrate ContinuumConductor at a real-world use-case of process optimazition in inland ports. Our contributions lay the foundation for computing-aware process mining in cyber-physical and IIoT systems.

</details>


### [40] [Venus: An Efficient Edge Memory-and-Retrieval System for VLM-based Online Video Understanding](https://arxiv.org/abs/2512.07344)
*Shengyuan Ye,Bei Ouyang,Tianyi Qian,Liekang Zeng,Mu Yuan,Xiaowen Chu,Weijie Hong,Xu Chen*

Main category: cs.DC

TL;DR: Venus 是一种面向设备端的内存与检索系统，旨在提升在线视频理解的效率，通过边缘-云分离架构实现低延迟和高精度。


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言模型在部署时忽视了系统开销，导致实际应用中性能瓶颈。

Method: 提出两阶段架构：摄入阶段通过场景分割和聚类构建分层内存；查询阶段采用阈值渐进采样算法优化关键帧选择。

Result: 相比现有方法，Venus 实现15倍至131倍的响应延迟加速，并保持相当或更优的推理精度。

Conclusion: Venus 有效平衡了系统成本与推理准确性，支持秒级实时响应，适合实际部署。

Abstract: Vision-language models (VLMs) have demonstrated impressive multimodal comprehension capabilities and are being deployed in an increasing number of online video understanding applications. While recent efforts extensively explore advancing VLMs' reasoning power in these cases, deployment constraints are overlooked, leading to overwhelming system overhead in real-world deployments. To address that, we propose Venus, an on-device memory-and-retrieval system for efficient online video understanding. Venus proposes an edge-cloud disaggregated architecture that sinks memory construction and keyframe retrieval from cloud to edge, operating in two stages. In the ingestion stage, Venus continuously processes streaming edge videos via scene segmentation and clustering, where the selected keyframes are embedded with a multimodal embedding model to build a hierarchical memory for efficient storage and retrieval. In the querying stage, Venus indexes incoming queries from memory, and employs a threshold-based progressive sampling algorithm for keyframe selection that enhances diversity and adaptively balances system cost and reasoning accuracy. Our extensive evaluation shows that Venus achieves a 15x-131x speedup in total response latency compared to state-of-the-art methods, enabling real-time responses within seconds while maintaining comparable or even superior reasoning accuracy.

</details>


### [41] [Communication-Efficient Serving for Video Diffusion Models with Latent Parallelism](https://arxiv.org/abs/2512.07350)
*Zhiyuan Wu,Shuai Wang,Li Chen,Kaihui Gao,Dan Li,Yanyu Ren,Qiming Zhang,Yong Wang*

Main category: cs.DC

TL;DR: Latent Parallelism (LP) 是首个专为视频扩散模型设计的并行策略，通过在潜在空间动态旋转划分维度显著降低通信开销，同时保持生成质量。


<details>
  <summary>Details</summary>
Motivation: 视频扩散模型因三维时空注意力计算导致内存消耗剧增，传统并行策略存在严重通信瓶颈，亟需高效解决方案。

Method: 提出 Latent Parallelism，动态旋转潜在空间的时间、高度和宽度划分维度，配合边界对齐与位置感知重建机制实现低通信并行推理。

Result: 在三个基准测试中，LP 相比基线方法最高降低 97% 通信开销，且生成质量相当。

Conclusion: LP 是一种非侵入式插件方案，可无缝集成现有并行策略，实现高效可扩展的视频生成服务。

Abstract: Video diffusion models (VDMs) perform attention computation over the 3D spatio-temporal domain. Compared to large language models (LLMs) processing 1D sequences, their memory consumption scales cubically, necessitating parallel serving across multiple GPUs. Traditional parallelism strategies partition the computational graph, requiring frequent high-dimensional activation transfers that create severe communication bottlenecks. To tackle this issue, we exploit the local spatio-temporal dependencies inherent in the diffusion denoising process and propose Latent Parallelism (LP), the first parallelism strategy tailored for VDM serving. \textcolor{black}{LP decomposes the global denoising problem into parallelizable sub-problems by dynamically rotating the partitioning dimensions (temporal, height, and width) within the compact latent space across diffusion timesteps, substantially reducing the communication overhead compared to prevailing parallelism strategies.} To ensure generation quality, we design a patch-aligned overlapping partition strategy that matches partition boundaries with visual patches and a position-aware latent reconstruction mechanism for smooth stitching. Experiments on three benchmarks demonstrate that LP reduces communication overhead by up to 97\% over baseline methods while maintaining comparable generation quality. As a non-intrusive plug-in paradigm, LP can be seamlessly integrated with existing parallelism strategies, enabling efficient and scalable video generation services.

</details>


### [42] [Otus Supercomputer](https://arxiv.org/abs/2512.07401)
*Sadaf Ehtesabi,Manoar Hossain,Tobias Kenter,Andreas Krawinkel,Holger Nitsche,Lukas Ostermann,Christian Plessl,Heinrich Riebler,Stefan Rohde,Robert Schade,Michael Schwarz,Jens Simon,Nils Winnwa,Alex Wiens,Xin Wu*

Main category: cs.DC

TL;DR: Otus是德国帕德博恩大学高性能计算集群，性能约为Noctua 2的两倍，具备CPU、GPU与FPGA三种节点，在Top500和Green500榜单中表现优异。


<details>
  <summary>Details</summary>
Motivation: 为科学用户提供系统洞察，并为其他HPC中心提供参考，推动高效能与节能运算发展。

Method: 介绍Otus硬件架构、软件环境、系统集成及数据中心能效设计，并持续更新最新配置与实测数据。

Result: Otus在Top500中CPU分区排名第164、GPU分区第255；在Green500中GPU分区高居第5，展现卓越能效。

Conclusion: Otus作为NHR计划关键设施，兼顾高性能与绿色节能，为科研与HPC运营提供重要范例。

Abstract: Otus is a high-performance computing cluster that was launched in 2025 and is operated by the Paderborn Center for Parallel Computing (PC2) at Paderborn University in Germany. The system is part of the National High Performance Computing (NHR) initiative. Otus complements the previous supercomputer Noctua 2, offering approximately twice the computing power while retaining the three node types that were characteristic of Noctua 2: 1) CPU compute nodes with different memory capacities, 2) high-end GPU nodes, and 3) HPC-grade FPGA nodes. On the Top500 list, which ranks the 500 most powerful supercomputers in the world, Otus is in position 164 with the CPU partition and in position 255 with the GPU partition (June 2025). On the Green500 list, ranking the 500 most energy-efficient supercomputers in the world, Otus is in position 5 with the GPU partition (June 2025).
  This article provides a comprehensive overview of the system in terms of its hardware, software, system integration, and its overall integration into the data center building to ensure energy-efficient operation. The article aims to provide unique insights for scientists using the system and for other centers operating HPC clusters. The article will be continuously updated to reflect the latest system setup and measurements.

</details>


### [43] [A Performance Analyzer for a Public Cloud's ML-Augmented VM Allocator](https://arxiv.org/abs/2512.07750)
*Roozbeh Bostandoost,Pooria Namyar,Siva Kesava Reddy Kakarla,Ryan Beckett,Santiago Segarra,Eli Cortez,Ankur Mallick,Kevin Hsieh,Rodrigo Fonseca,Mohammad Hajiesmaili,Behnaz Arzani*

Main category: cs.DC

TL;DR: SANJESH是一种用于分析云系统中多个机器学习模型对端到端性能影响的工具，通过双层优化快速定位性能瓶颈。


<details>
  <summary>Details</summary>
Motivation: 现有云系统缺乏工具来理解多个机器学习模型及其交互如何影响整体性能。

Method: 提出SANJESH工具，采用双层优化方法并引入新颖机制加速求解。

Result: 在VM放置场景中，SANJESH发现某些模型导致的性能比模拟方法检测结果差约4倍。

Conclusion: SANJESH能有效揭示多模型交互对系统性能的影响，帮助优化云系统效率。

Abstract: Many operational cloud systems use one or more machine learning models that help them achieve better efficiency and performance. But operators do not have tools to help them understand how each model and the interaction between them affect the end-to-end system performance. SANJESH is such a tool. SANJESH supports a diverse set of performance-related queries which we answer through a bi-level optimization. We invent novel mechanisms to solve this optimization more quickly. These techniques allow us to solve an optimization which prior work failed to solve even after $24$ hours.
  As a proof of concept, we apply SANJESH to an example production system that uses multiple ML models to optimize virtual machine (VM) placement. These models impact how many servers the operators uses to host VMs and the frequency with which it has to live-migrate them because the servers run out of resources. SANJESH finds scenarios where these models cause $~4\times$ worse performance than what simulation-based approaches detect.

</details>


### [44] [Designing Co-operation in Systems of Hierarchical, Multi-objective Schedulers for Stream Processing](https://arxiv.org/abs/2512.07792)
*Animesh Dangwal,Yufeng Jiang,Charlie Arnold,Jun Fan,Mohamed Bassem,Aish Rajagopal*

Main category: cs.DC

TL;DR: 本文探讨了在Meta构建和设计专注于关键计算资源负载均衡的流处理系统，并展示了如何将新调度器集成到现有层次结构中以实现高效协作。


<details>
  <summary>Details</summary>
Motivation: 随着应用复杂性和用户需求的增长，原先只需最小负载均衡的基础设施部分现在需要更健壮和主动的负载管理。

Method: 通过设计专注于关键计算资源和应用属性的负载均衡系统，并将新调度器集成到现有调度器层次结构中，实现多层次协同工作。

Result: 成功实现了多个调度器在各自基础设施层级上的有效负载均衡，提升了系统的整体性能与可扩展性。

Conclusion: 该方法增强了流处理框架应对复杂负载的能力，为大规模实时数据处理提供了更稳健的支持。

Abstract: Stream processing is a computing paradigm that supports real-time data processing for a wide variety of applications. At Meta, it's used across the company for various tasks such as deriving product insights, providing and improving user services, and enabling AI at scale for our ever-growing user base. Meta's current stream processing framework supports processing TerraBytes(TBs) of data in mere seconds. This is enabled by our efficient schedulers and multi-layered infrastructure, which allocate workloads across various compute resources, working together in hierarchies across various parts of the infrastructure. But with the ever growing complexity of applications, and user needs, areas of the infrastructure that previously required minimal load balancing, now must be made more robust and proactive to application load. In our work we explore how to build and design such a system that focuses on load balancing over key compute resources and properties of these applications. We also showcase how to integrate new schedulers into the hierarchy of the existing ones, allowing multiple schedulers to work together and perform load balancing, at their infrastructure level, effectively.

</details>


### [45] [Quantifying the Carbon Reduction of DAG Workloads: A Job Shop Scheduling Perspective](https://arxiv.org/abs/2512.07799)
*Roozbeh Bostandoost,Adam Lechowicz,Walid A. Hanafy,Prashant Shenoy,Mohammad Hajiesmaili*

Main category: cs.DC

TL;DR: 该论文研究了依赖感知的碳感知调度方法，以优化数据中心批处理作业的碳排放。


<details>
  <summary>Details</summary>
Motivation: 现有调度器忽略作业内部任务结构，导致碳效率低下，作者希望通过利用任务依赖关系提升碳减排效果。

Method: 将问题建模为柔性作业车间调度变体，并使用离线求解器计算碳排放和能耗节省的上限。

Result: 相比仅优化完成时间的基线，平均减少25%碳排放；允许双倍完成时间可近似翻倍碳节省，但异构服务器下可能增加能耗。

Conclusion: 作业结构和服务器数量显著影响碳减排潜力，碳、能耗与完成时间之间存在权衡关系。

Abstract: Carbon-aware schedulers aim to reduce the operational carbon footprint of data centers by running flexible workloads during periods of low carbon intensity. Most schedulers treat workloads as single monolithic tasks, ignoring that many jobs, like video encoding or offline inference, consist of smaller tasks with specific dependencies and resource needs; however, knowledge of this structure enables opportunities for greater carbon efficiency.
  We quantify the maximum benefit of a dependency-aware approach for batch workloads. We model the problem as a flexible job-shop scheduling variant and use an offline solver to compute upper bounds on carbon and energy savings. Results show up to $25\%$ lower carbon emissions on average without increasing the optimal makespan (total job completion time) compared to a makespan-only baseline. Although in heterogeneous server setup, these schedules may use more energy than energy-optimal ones. Our results also show that allowing twice the optimal makespan nearly doubles the carbon savings, underscoring the tension between carbon, energy, and makespan. We also highlight key factors such as job structure and server count influence the achievable carbon reductions.

</details>


<div id='cs.OS'></div>

# cs.OS [[Back]](#toc)

### [46] [Defending Event-Triggered Systems against Out-of-Envelope Environments](https://arxiv.org/abs/2512.06331)
*Marcus Völp,Mohammad Ibrahim Alkoudsi,Azin Bayrami Asl,Kristin Krüger,Julio Rodrigues Mendonca da Neto,Gerhard Fohler*

Main category: cs.OS

TL;DR: 本文探讨实时系统在超出设计环境条件时的鲁棒性，比较时间触发与事件触发范式，并提出通过任务重要性提升事件触发系统的防御能力。


<details>
  <summary>Details</summary>
Motivation: 实时系统在超出安全操作环境时可能丧失时效性与安全性，需研究如何增强其鲁棒性。

Method: 引入“重要性”概念，独立于优先级与关键性，指导任务调度；类比混合关键性调度方法。

Result: 事件触发系统可通过重要性机制有效抵御中断风暴等超出设计假设的环境行为。

Conclusion: 时间触发系统并非天然免疫环境越界行为，事件触发系统经适当设计可具备同等甚至更强的鲁棒性。

Abstract: The design of real-time systems is based on assumptions about environmental conditions in which they will operate. We call this their safe operational envelope. Violation of these assumptions, i.e., out-of-envelope environments, can jeopardize timeliness and safety of real-time systems, e.g., by overwhelming them with interrupt storms. A long-lasting debate has been going on over which design paradigm, the time- or event-triggered, is more robust against such behavior. In this work, we investigate the claim that time-triggered systems are immune against out-of-envelope behavior and how event-triggered systems can be constructed to defend against being overwhelmed by interrupt showers. We introduce importance (independently of priority and criticality) as a means to express which tasks should still be scheduled in case environmental design assumptions cease to hold, draw parallels to mixed-criticality scheduling, and demonstrate how event-triggered systems can defend against out-of-envelope behavior.

</details>
