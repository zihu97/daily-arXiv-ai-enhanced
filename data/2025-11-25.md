<div id=toc></div>

# Table of Contents

- [cs.MA](#cs.MA) [Total: 6]
- [cs.PF](#cs.PF) [Total: 2]
- [cs.NI](#cs.NI) [Total: 7]
- [cs.OS](#cs.OS) [Total: 1]
- [cs.SE](#cs.SE) [Total: 29]
- [cs.DC](#cs.DC) [Total: 8]
- [cs.AR](#cs.AR) [Total: 6]


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [1] [A novel strategy for multi-resource load balancing in agent-based systems](https://arxiv.org/abs/2511.17580)
*Leszek Sliwko,Aleksander Zgrzywa*

Main category: cs.MA

TL;DR: 本文提出了一种基于智能体的多资源负载均衡策略，利用智能体的社会行为与自适应能力优化复杂企业架构的结构，并通过实验验证了该方法的有效性。


<details>
  <summary>Details</summary>
Motivation: 为帮助系统设计者优化复杂企业架构的结构，需要一种能够动态调整并实现负载均衡的智能方法。

Method: 采用基于智能体的系统，结合智能体的社会行为和自适应能力，使其具备自我评估功能，从而确定给定配置下的最优设置。

Result: 所提出的智能体系统已成功实现，并通过实验展示了其在多资源负载均衡方面的效果。

Conclusion: 该多资源负载均衡策略能有效支持复杂企业架构的优化，具有实际应用价值。

Abstract: The paper presents a multi-resource load balancing strategy which can be utilised within an agent-based system. This approach can assist system designers in their attempts to optimise the structure for complex enterprise architectures. In this system, the social behaviour of the agent and its adaptation abilities are applied to determine an optimal setup for a given configuration. All the methods have been developed to allow the agent's self-assessment. The proposed agent system has been implemented and the experiment results are presented here.

</details>


### [2] [From Competition to Coordination: Market Making as a Scalable Framework for Safe and Aligned Multi-Agent LLM Systems](https://arxiv.org/abs/2511.17621)
*Brendan Gho,Suman Muppavarapu,Afnan Shaik,Tyson Tsay,James Begin,Kevin Zhu,Archana Vaidheeswaran,Vasu Sharma*

Main category: cs.MA

TL;DR: 本文提出一种基于市场机制的多智能体大语言模型协调框架，通过结构化经济交换促进智能体在无需外部监督的情况下实现可信、透明且可验证的集体推理，在多项任务中准确率提升最高达10%。


<details>
  <summary>Details</summary>
Motivation: 随着基础模型越来越多地作为交互式智能体部署于多智能体系统中，其集体行为在可信度、透明性和问责制方面带来新挑战。传统协调机制（如集中监管或对抗性裁决）难以扩展且常掩盖决策过程。

Method: 引入一种市场制造框架，将智能体互动组织为结构化的经济交换：每个智能体作为市场参与者，更新并交易概率信念，以收敛至共享且真实的结论；该方法通过将局部激励与集体认知目标对齐，实现自组织、可验证的推理。

Result: 在事实推理、伦理判断和常识推断任务上的实验表明，基于市场的协调方法相比单次推理基线准确率最高提升10%，同时保留中间推理步骤的可解释性与透明性。

Conclusion: 经济协调原则能够有效实现多智能体大语言模型系统中的问责性与鲁棒性，为构建可自我修正、具有社会责任感并在真实场景中维持信任与监督能力的人工智能系统提供了可扩展路径。

Abstract: As foundation models are increasingly deployed as interacting agents in multi-agent systems, their collective behavior raises new challenges for trustworthiness, transparency, and accountability. Traditional coordination mechanisms, such as centralized oversight or adversarial adjudication, struggle to scale and often obscure how decisions emerge. We introduce a market-making framework for multi-agent large language model (LLM) coordination that organizes agent interactions as structured economic exchanges. In this setup, each agent acts as a market participant, updating and trading probabilistic beliefs, to converge toward shared, truthful outcomes. By aligning local incentives with collective epistemic goals, the framework promotes self-organizing, verifiable reasoning without requiring external enforcement. Empirically, we evaluate this approach across factual reasoning, ethical judgment, and commonsense inference tasks. Market-based coordination yields accuracy gains of up to 10% over single-shot baselines while preserving interpretability and transparency of intermediate reasoning steps. Beyond these improvements, our findings demonstrate that economic coordination principles can operationalize accountability and robustness in multi-agent LLM systems, offering a scalable pathway toward self-correcting, socially responsible AI capable of maintaining trust and oversight in real world deployment scenarios.

</details>


### [3] [Iterative Negotiation and Oversight: A Case Study in Decentralized Air Traffic Management](https://arxiv.org/abs/2511.17625)
*Jaehan Im,John-Paul Clarke,Ufuk Topcu,David Fridovich-Keil*

Main category: cs.MA

TL;DR: 本文提出了一种结合税收式监督机制的迭代协商框架，用于在无中心协调者的非合作多智能体系统中实现兼顾系统效率与公平性的共识，并在有限时间内收敛。


<details>
  <summary>Details</summary>
Motivation: 在去中心化的多智能体系统中，非合作智能体因偏好冲突难以达成共识，现有方法缺乏对系统级目标（如效率和公平）的形式化保证。

Method: 提出一种迭代协商与监督框架，在基于资产交易的去中心化协商机制基础上引入类税收的监督干预，引导协商结果趋向高效与公平，并调控收敛速度。

Result: 理论分析证明了该框架可在有限时间内终止，并推导出系统效率与收敛速度受中央干预程度影响的边界；在美国空管路径重规划案例中验证了其有效性。

Conclusion: 所提框架为非合作多智能体系统提供了一种通用的去中心化协调机制，在保障隐私的同时兼顾系统级目标。

Abstract: Achieving consensus among noncooperative agents remains challenging in decentralized multi-agent systems, where agents often have conflicting preferences. Existing coordination methods enable agents to reach consensus without a centralized coordinator, but do not provide formal guarantees on system-level objectives such as efficiency or fairness. To address this limitation, we propose an iterative negotiation and oversight framework that augments a decentralized negotiation mechanism with taxation-like oversight. The framework builds upon the trading auction for consensus, enabling noncooperative agents with conflicting preferences to negotiate through asset trading while preserving valuation privacy. We introduce an oversight mechanism, which implements a taxation-like intervention that guides decentralized negotiation toward system-efficient and equitable outcomes while also regulating how fast the framework converges. We establish theoretical guarantees of finite-time termination and derive bounds linking system efficiency and convergence rate to the level of central intervention. A case study based on the collaborative trajectory options program, a rerouting initiative in U.S. air traffic management, demonstrates that the framework can reliably achieve consensus among noncooperative airspace sector managers, and reveals how the level of intervention regulates the relationship between system efficiency and convergence speed. Taken together, the theoretical and experimental results indicate that the proposed framework provides a general mechanism for decentralized coordination in noncooperative multi-agent systems while safeguarding system-level objectives.

</details>


### [4] [Episodic Memory in Agentic Frameworks: Suggesting Next Tasks](https://arxiv.org/abs/2511.17775)
*Sandro Rama Fiorini,Leonardo G. Azevedo,Raphael M. Thiago,Valesca M. de Sousa,Anton B. Labate,Viviane Torres da Silva*

Main category: cs.MA

TL;DR: 本文提出一种基于情景记忆架构的方法，通过存储和检索历史工作流来辅助大语言模型驱动的智能体推荐科学工作流中的下一步操作，以减少模型幻觉并避免依赖稀缺的专有数据进行微调。


<details>
  <summary>Details</summary>
Motivation: 在科学工作流中，大语言模型（LLM）驱动的智能体虽能促进人机协同创作，但其推荐下一步操作时易产生幻觉，且依赖稀缺的专有数据进行微调，因此需要一种不完全依赖LLM的可靠推荐机制。

Method: 提出一种情景记忆架构，用于存储和检索过往的工作流；通过将当前工作流与历史序列匹配，使智能体能够基于以往模式推荐合理的后续任务。

Result: 该方法使智能体能够依据历史工作流中的实际模式推荐下一步任务，从而提升推荐的合理性和可靠性。

Conclusion: 利用情景记忆架构可有效辅助LLM智能体在科学工作流中进行下一步推荐，降低幻觉风险并减少对专有训练数据的依赖。

Abstract: Agentic frameworks powered by Large Language Models (LLMs) can be useful tools in scientific workflows by enabling human-AI co-creation. A key challenge is recommending the next steps during workflow creation without relying solely on LLMs, which risk hallucination and require fine-tuning with scarce proprietary data. We propose an episodic memory architecture that stores and retrieves past workflows to guide agents in suggesting plausible next tasks. By matching current workflows with historical sequences, agents can recommend steps based on prior patterns.

</details>


### [5] [DISPATCH -- Decentralized Informed Spatial Planning and Assignment of Tasks for Cooperative Heterogeneous Agents](https://arxiv.org/abs/2511.17915)
*Yao Liu,Sampad Mohanty,Elizabeth Ondula,Bhaskar Krishnamachari*

Main category: cs.MA

TL;DR: 本文研究了在部分可观测、异构多智能体系统中的空间任务分配问题，提出两种基于Eisenberg-Gale（EG）均衡的新算法——EG-MARL和一种随机在线优化机制，在保证效率的同时实现公平分配。


<details>
  <summary>Details</summary>
Motivation: 现有贪婪分配策略虽高效但导致任务间服务不公平；而多数公平分配方法依赖集中式协调或忽略部分可观测下的公平性。因此，需在去中心化、部分可观测场景下兼顾效率与公平。

Method: 建立EG均衡凸规划与去中心化多智能体学习之间的联系，提出两种算法：(i) 基于多智能体强化学习的EG-MARL，利用集中式公平分配算法（EG和偏好感知匈牙利法）指导训练；(ii) 一种随机在线优化机制，在任务逐步出现时进行引导探索和子集公平分配。

Result: 实验表明，两种算法在不同团队规模和任务设定下均能维持EG均衡所定义的公平-效率平衡。EG-MARL接近集中式协调效果并减少行驶距离，随机在线机制则实现实时分配且保持良好公平性。

Conclusion: 空间感知的EG公式可有效指导具有异构能力的智能体在去中心化环境下的协调，实现兼顾公平与效率的任务分配。

Abstract: Spatial task allocation in systems such as multi-robot delivery or ride-sharing requires balancing efficiency with fair service across tasks. Greedy assignment policies that match each agent to its highest-preference or lowest-cost task can maximize efficiency but often create inequities: some tasks receive disproportionately favorable service (e.g., shorter delays or better matches), while others face long waits or poor allocations.
  We study fairness in heterogeneous multi-agent systems where tasks vary in preference alignment and urgency. Most existing approaches either assume centralized coordination or largely ignore fairness under partial observability. Distinct from this prior work, we establish a connection between the Eisenberg-Gale (EG) equilibrium convex program and decentralized, partially observable multi-agent learning. Building on this connection, we develop two equilibrium-informed algorithms that integrate fairness and efficiency: (i) a multi-agent reinforcement learning (MARL) framework, EG-MARL, whose training is guided by centralized fair assignment algorithms (EG and a preference-aware Hungarian method); and (ii) a stochastic online optimization mechanism that performs guided exploration and subset-based fair assignment as tasks are discovered.
  We evaluate our frameworks across a range of team sizes and assignment formulations against centralized EG, Hungarian, and Min-Max Distance baselines. Both algorithms preserve the fairness-efficiency balance of the Eisenberg-Gale equilibrium under partial observability. EG-MARL achieves near-centralized coordination and reduced travel distances, while the stochastic online mechanism enables real-time allocation with competitive fairness. Together, these results demonstrate that spatially aware EG formulations can effectively guide decentralized coordination in agents with heterogeneous capabilities.

</details>


### [6] [VIL2C: Value-of-Information Aware Low-Latency Communication for Multi-Agent Reinforcement Learning](https://arxiv.org/abs/2511.19146)
*Qian Zhang,Zhuo Sun,Yao Zhang,Zhiwen Yu,Bin Guo,Jun Zhang*

Main category: cs.MA

TL;DR: 本文提出了一种面向多智能体强化学习（MARL）系统的低延迟通信方案VIL2C，通过引入信息价值（VoI）度量和自适应接收机制，在存在通信延迟的现实场景中优化资源分配，显著提升了系统性能。


<details>
  <summary>Details</summary>
Motivation: 实际MARL系统中的通信延迟会导致动作决策滞后和信息过时，尤其在自动驾驶等时间敏感应用中严重制约性能提升，因此亟需一种能主动应对延迟影响的通信机制。

Method: 提出VIL2C方案：1）定义信息价值（VoI）指标以量化延迟消息的重要性；2）设计渐进式消息接收机制，根据已接收消息动态调整接收时长；3）推导基于VoI的最优资源分配策略。

Result: 大量实验表明，VIL2C在多种通信条件下均优于现有方法，其优势源于高VoI消息的低延迟传输和通过自适应接收机制消除不必要的等待时间。

Conclusion: VIL2C通过结合信息价值感知与自适应通信调度，有效缓解了通信延迟对MARL性能的负面影响，为时间敏感型多智能体系统提供了实用解决方案。

Abstract: Inter-agent communication serves as an effective mechanism for enhancing performance in collaborative multi-agent reinforcement learning(MARL) systems. However, the inherent communication latency in practical systems induces both action decision delays and outdated information sharing, impeding MARL performance gains, particularly in time-critical applications like autonomous driving. In this work, we propose a Value-of-Information aware Low-latency Communication(VIL2C) scheme that proactively adjusts the latency distribution to mitigate its effects in MARL systems. Specifically, we define a Value of Information (VOI) metric to quantify the importance of delayed message transmission based on each delayed message's importance. Moreover, we propose a progressive message reception mechanism to adaptively adjust the reception duration based on received messages. We derive the optimized VoI aware resource allocation and theoretically prove the performance advantage of the proposed VIL2C scheme. Extensive experiments demonstrate that VIL2C outperforms existing approaches under various communication conditions. These gains are attributed to the low-latency transmission of high-VoI messages via resource allocation and the elimination of unnecessary waiting periods via adaptive reception duration.

</details>


<div id='cs.PF'></div>

# cs.PF [[Back]](#toc)

### [7] [GROOT: General-Purpose Automatic Parameter Tuning Across Layers, Domains, and Use Cases](https://arxiv.org/abs/2511.17922)
*Robert Krahn,Josia Mädler,Christoph Seidl,Christof Fetzer*

Main category: cs.PF

TL;DR: 本文提出 Groot，一种通用配置调优工具，适用于多领域、多目标优化，支持自定义技术栈，并在真实场景中有效提升性能与降低资源消耗。


<details>
  <summary>Details</summary>
Motivation: 现有参数调优工具通常局限于特定领域、固定优化目标或特定技术层，难以满足专业创新企业（SIVs）在多变应用场景、成本-性能权衡及自托管定制技术栈下的调优需求。

Method: 设计并实现 Groot，一个通用配置调优器，其特点包括：领域无关性、支持多目标优化、兼容不同自定义技术栈，以及对参数类型和取值范围不做强假设。

Result: 在真实用例和基准测试中，Groot 能稳定提升系统性能并减少资源消耗，适用于 SIVs 的典型部署场景。

Conclusion: Groot 有效解决了现有调优工具在通用性、灵活性和易用性方面的不足，为资源受限且技术栈多样的 SIVs 提供了实用的配置优化方案。

Abstract: Modern software systems are executed on a runtime stack with layers (virtualization, storage, trusted execution, etc.) each incurring an execution and/or monetary cost, which may be mitigated by finding suitable parameter configurations. While specialized parameter tuners exist, they are tied to a particular domain or use case, fixed in type and number of optimization goals, or focused on a specific layer or technology. These limitations pose significant adoption hurdles for specialized and innovative ventures (SIVs) that address a variety of domains and use cases, operate under strict cost-performance constraints requiring tradeoffs, and rely on self-hosted servers with custom technology stacks while having little data or expertise to set up and operate specialized tuners. In this paper, we present Groot - a general-purpose configuration tuner designed to a) be explicitly agnostic of a particular domain or use case, b) balance multiple potentially competing optimization goals, c) support different custom technology setups, and d) make minimal assumptions about parameter types, ranges, or suitable values. Our evaluation on both real-world use cases and benchmarks shows that Groot reliably improves performance and reduces resource consumption in scenarios representative for SIVs.

</details>


### [8] [Low-Rank GEMM: Efficient Matrix Multiplication via Low-Rank Approximation with FP8 Acceleration](https://arxiv.org/abs/2511.18674)
*Alfredo Metere*

Main category: cs.PF

TL;DR: 本文提出Low-Rank GEMM方法，利用低秩矩阵近似结合FP8精度和智能核选择，在NVIDIA RTX 4090上实现高达378 TFLOPS的性能，相比PyTorch FP32在大矩阵上提速7.8倍并节省75%内存。


<details>
  <summary>Details</summary>
Motivation: 传统大矩阵乘法具有立方计算复杂度（如$\mathcal{O}(n^3)$），难以满足现代机器学习对高效计算的需求，亟需一种兼顾计算效率、内存占用与硬件加速能力的新方法。

Method: 采用低秩矩阵近似技术，结合FP8精度计算与自动化的内核选择策略（如SVD或随机SVD），根据矩阵特性与硬件能力动态优化分解方式与精度。

Result: 在NVIDIA RTX 4090上，对尺寸达$N=20480$的矩阵实现最高378 TFLOPS性能；相比PyTorch FP32，内存节省75%，速度提升7.8倍；当$N\geq10240$时，性能超越传统cuBLAS实现。

Conclusion: Low-Rank GEMM通过内存带宽优化而非计算捷径，在大矩阵场景下实现了显著的性能与效率提升，成为适用于现代GPU硬件的高效矩阵乘法新范式。

Abstract: Large matrix multiplication is a cornerstone of modern machine learning workloads, yet traditional approaches suffer from cubic computational complexity (e.g., $\mathcal{O}(n^3)$ for a matrix of size $n\times n$). We present Low-Rank GEMM, a novel approach that leverages low-rank matrix approximations to achieve sub-quadratic complexity while maintaining hardware-accelerated performance through FP8 precision and intelligent kernel selection. On a NVIDIA RTX 4090, our implementation achieves up to 378 TFLOPS on matrices up to $N=20480$, providing 75\% memory savings and $7.8\times$ speedup over PyTorch FP32 for large matrices. The system automatically adapts to hardware capabilities, selecting optimal decomposition methods (SVD, randomized SVD) and precision levels based on matrix characteristics and available accelerators. Comprehensive benchmarking on NVIDIA RTX 4090 demonstrates that Low-Rank GEMM becomes the fastest approach for matrices $N\geq10240$, surpassing traditional cuBLAS implementations through memory bandwidth optimization rather than computational shortcuts.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [9] [AURA: Adaptive Unified Reasoning and Automation with LLM-Guided MARL for NextG Cellular Networks](https://arxiv.org/abs/2511.17506)
*Narjes Nourzad,Mingyu Zong,Bhaskar Krishnamachari*

Main category: cs.NI

TL;DR: AURA is a hybrid framework combining cloud-based large language models (LLMs) for strategic planning and multi-agent reinforcement learning (MARL) at base stations for local execution, improving 6G network resilience and reducing latency.


<details>
  <summary>Details</summary>
Motivation: Next-generation cellular networks need to handle dynamic traffic with high performance, but LLMs are too slow for real-time use and MARL struggles with large-scale coordination. AURA addresses these limitations by integrating both approaches.

Method: AURA uses cloud-based LLMs to generate high-level objectives and subgoals based on environmental understanding, while base stations act as MARL agents that execute these goals locally. A trust mechanism balances external guidance with local learning, and batched communication reduces latency.

Result: In a simulated 6G environment, AURA reduces dropped handoff requests by over 50% under normal and high traffic and decreases system failures. Agents rely on LLM input in fewer than 60% of decisions, showing effective augmentation without full dependency.

Conclusion: Integrating LLM reasoning with MARL adaptability offers a scalable and efficient solution for real-time management of NextG cellular networks, balancing global strategy with local responsiveness while mitigating latency and hallucination risks.

Abstract: Next-generation (NextG) cellular networks are expected to manage dynamic traffic while sustaining high performance. Large language models (LLMs) provide strategic reasoning for 6G planning, but their computational cost and latency limit real-time use. Multi-agent reinforcement learning (MARL) supports localized adaptation, yet coordination at scale remains challenging. We present AURA, a framework that integrates cloud-based LLMs for high-level planning with base stations modeled as MARL agents for local decision-making. The LLM generates objectives and subgoals from its understanding of the environment and reasoning capabilities, while agents at base stations execute these objectives autonomously, guided by a trust mechanism that balances local learning with external input. To reduce latency, AURA employs batched communication so that agents update the LLM's view of the environment and receive improved feedback. In a simulated 6G scenario, AURA improves resilience, reducing dropped handoff requests by more than half under normal and high traffic and lowering system failures. Agents use LLM input in fewer than 60\% of cases, showing that guidance augments rather than replaces local adaptability, thereby mitigating latency and hallucination risks. These results highlight the promise of combining LLM reasoning with MARL adaptability for scalable, real-time NextG network management.

</details>


### [10] [RI-PIENO -- Revised and Improved Petrol-Filling Itinerary Estimation aNd Optimization](https://arxiv.org/abs/2511.17517)
*Marco Savarese,Antonio De Blasi,Carmine Zaccagnino,Giacomo Salici,Silvia Cascianelli,Roberto Vezzani,Carlo Augusto Grazia*

Main category: cs.NI

TL;DR: RI-PIENO 是一种融合车内传感器数据与外部地理和油价信息的动态加油路径优化系统，通过物联网云/雾服务构建时变有向无环图，实现比现有方法更优的成本节省与路径效率。


<details>
  <summary>Details</summary>
Motivation: 现有加油路径优化方案多关注车际通信或车内监控，缺乏能动态适应驾驶者出行模式的集成框架。

Method: 基于 PIENO 框架提出 RI-PIENO，结合车内传感数据与外部地理及油价信息，利用 IoT 云/雾服务构建动态时变有向无环图，将加油决策建模为持续自适应过程。

Result: 在多驾驶员、多周的真实通勤模拟中，RI-PIENO 相较以往方法实现了显著的成本节约和更高的路径效率。

Conclusion: RI-PIENO 能有效整合新兴路侧基础设施与 V2X 通信，适用于下一代物联网与车联网生态系统的可扩展部署。

Abstract: Efficient energy provisioning is a fundamental requirement for modern transportation systems, making refueling path optimization a critical challenge. Existing solutions often focus either on inter-vehicle communication or intra-vehicle monitoring, leveraging Intelligent Transportation Systems, Digital Twins, and Software-Defined Internet of Vehicles with Cloud/Fog/Edge infrastructures. However, integrated frameworks that adapt dynamically to driver mobility patterns are still underdeveloped. Building on our previous PIENO framework, we present RI-PIENO (Revised and Improved Petrol-filling Itinerary Estimation aNd Optimization), a system that combines intra-vehicle sensor data with external geospatial and fuel price information, processed via IoT-enabled Cloud/Fog services. RI-PIENO models refueling as a dynamic, time-evolving directed acyclic graph that reflects both habitual daily trips and real-time vehicular inputs, transforming the system from a static recommendation tool into a continuously adaptive decision engine. We validate RI-PIENO in a daily-commute use case through realistic multi-driver, multi-week simulations, showing that it achieves significant cost savings and more efficient routing compared to previous approaches. The framework is designed to leverage emerging roadside infrastructure and V2X communication, supporting scalable deployment within next-generation IoT and vehicular networking ecosystems.

</details>


### [11] [Serv-Drishti: An Interactive Serverless Function Request Simulation Engine and Visualiser](https://arxiv.org/abs/2511.17518)
*Siddharth Agarwal,Maria A. Rodriguez,Rajkumar Buyya*

Main category: cs.NI

TL;DR: Serv-Drishti 是一个开源的交互式模拟工具，用于可视化和分析无服务器计算平台中请求路由、冷启动、函数扩展和资源管理等关键行为。


<details>
  <summary>Details</summary>
Motivation: 无服务器计算的快速普及要求深入理解其底层运行机制，尤其是请求路由、冷启动、函数扩展和资源管理等方面的复杂性。

Method: 开发并实现了一个名为 Serv-Drishti 的交互式模拟器，支持可配置的平台参数、多种请求路由与函数放置策略，以及故障模拟模块，通过实时图表和数据导出进行系统行为分析。

Result: Serv-Drishti 能够有效模拟请求在无服务器平台中的完整流程，并支持在不同负载和故障条件下对系统性能进行比较分析。

Conclusion: Serv-Drishti 为无服务器架构的研究、教学和设计分析提供了一个强大且灵活的工具，有助于深入理解其内部工作机制。

Abstract: The rapid adoption of serverless computing necessitates a deeper understanding of its underlying operational mechanics, particularly concerning request routing, cold starts, function scaling, and resource management. This paper presents Serv-Drishti, an interactive, open-source simulation tool designed to demystify these complex behaviours. Serv-Drishti simulates and visualises the journey of a request through a representative serverless platform, from the API Gateway and intelligent Request Dispatcher to dynamic Function Instances on resource-constrained Compute Nodes. Unlike simple simulators, Serv-Drishti provides a robust framework for comparative analysis. It features configurable platform parameters, multiple request routing and function placement strategies, and a comprehensive failure simulation module. This allows users to not only observe but also rigorously analyse system responses under various loads and fault conditions. The tool generates real-time performance graphs and provides detailed data exports, establishing it as a valuable resource for research, education, and the design analysis of serverless architectures.

</details>


### [12] [Joint Edge Server Deployment and Computation Offloading: A Multi-Timescale Stochastic Programming Framework](https://arxiv.org/abs/2511.17524)
*Huaizhe Liu,Jiaqi Wu,Zhizongkai Wang,Bin Cao,Lin Gao*

Main category: cs.NI

TL;DR: 本文针对移动边缘计算中边缘服务器部署、服务放置与任务卸载的联合优化问题，提出一种多时间尺度随机规划框架，区分长期战略决策与短期战术决策，并设计相应算法求解。


<details>
  <summary>Details</summary>
Motivation: 传统方法将边缘服务器部署、服务放置和任务卸载视为同等决策，忽略了它们在信息实现时间上的差异：边缘服务器部署需在信息完全获知前做出且不可频繁变更，而服务放置与任务卸载可在信息完全获知后实时调整。这种时间耦合性未被现有研究充分考虑。

Method: 作者引入随机规划（SP）框架，分为战略层（基于不完整随机信息决定边缘服务器部署）和战术层（基于完整信息决定服务放置与任务卸载）。为处理两层决策的时间尺度差异，构建多时间尺度SP框架：大时间尺度（周期）用于战略层，小时间尺度（时隙）用于战术层，并分别采用马尔可夫近似算法和基于Lyapunov的算法进行求解。

Result: 该方法有效应对了边缘计算中不同决策层级间的时间耦合与信息不确定性问题，实现了对边缘服务器部署、服务放置与任务卸载的联合优化。

Conclusion: 所提出的多时间尺度随机规划框架能更真实地建模实际部署场景中的信息动态性与决策约束，为B5G/6G时代AI驱动应用的QoS保障提供了可行的优化方案。

Abstract: Mobile Edge Computing (MEC) is a promising approach for enhancing the quality-of-service (QoS) of AI-enabled applications in the B5G/6G era, by bringing computation capability closer to end-users at the network edge. In this work, we investigate the joint optimization of edge server (ES) deployment, service placement, and computation task offloading under the stochastic information scenario. Traditional approaches often treat these decisions as equal, disregarding the differences in information realization. However, in practice, the ES deployment decision must be made in advance and remain unchanged, prior to the complete realization of information, whereas the decisions regarding service placement and computation task offloading can be made and adjusted in real-time after information is fully realized. To address such temporal coupling between decisions and information realization, we introduce the stochastic programming (SP) framework, which involves a strategic-layer for deciding ES deployment based on (incomplete) stochastic information and a tactical-layer for deciding service placement and task offloading based on complete information realization. The problem is challenging due to the different timescales of two layers' decisions. To overcome this challenge, we propose a multi-timescale SP framework, which includes a large timescale (called period) for strategic-layer decision-making and a small timescale (called slot) for tactical-layer decision making. Moreover, we design a Lyapunov-based algorithm to solve the tactical-layer problem at each time slot, and a Markov approximation algorithm to solve the strategic-layer problem in every time period.

</details>


### [13] [SFusion: Energy and Coding Fusion for Ultra-Robust Low-SNR LoRa Networks](https://arxiv.org/abs/2511.18484)
*Weiwei Chen,Huaxuan Xiao,Jiefeng Zhang,Xianjin Xia,Shuai Wang,Xianjun Deng,Dan Zeng*

Main category: cs.NI

TL;DR: SFusion 是一种新型软件编码框架，通过联合利用信号级聚合与编码级冗余，显著提升 LoRa 在极弱信号下的解码能力，相比 SF12 最多获得 15dB 增益。


<details>
  <summary>Details</summary>
Motivation: 传统 LoRa 物理层在信号级解调与编码级纠错之间存在分离，且扩频因子（SF）受限，导致在城市级部署中面对极弱信号时鲁棒性不足。

Method: SFusion 在信号低于可解码阈值时，使用 2^m 个 SFk 符号构造等效的准 SF(k+m) 符号以累积能量；当部分解码可行时，采用机会式解码策略直接跨符号融合 IQ 信号进行纠错。

Result: 实验表明，SFusion 相比 SF12 最多提升 15dB，比现有最先进方案最多提升 13dB。

Conclusion: SFusion 有效增强了 LoRa 在城市级物联网应用中的通信鲁棒性，尤其适用于极弱信号环境。

Abstract: LoRa has become a cornerstone for city-wide IoT applications due to its long-range, low-power communication. It achieves extended transmission by spreading symbols over multiple samples, with redundancy controlled by the Spreading Factor (SF), and further error resilience provided by Forward Error Correction (FEC). However, practical limits on SF and the separation between signal-level demodulation and coding-level error correction in conventional LoRa PHY leave it vulnerable under extremely weak signals - common in city-scale deployments. To address this, we present SFusion, a software-based coding framework that jointly leverages signal-level aggregation and coding-level redundancy to enhance LoRa's robustness. When signals fall below the decodable threshold, SFusion encodes a quasi-SF(k +m) symbol using 2^m SFk symbols to boost processing gain through energy accumulation. Once partial decoding becomes feasible with energy aggregation, an opportunistic decoding strategy directly combines IQ signals across symbols to recover errors. Extensive evaluations show that SFusion achieves up to 15dB gain over SF12 and up to 13dB improvement over state-of-the-art solutions.

</details>


### [14] [Diffusion Model-Enhanced Environment Reconstruction in ISAC](https://arxiv.org/abs/2511.19044)
*Nguyen Duc Minh Quang,Chang Liu,Shuangyang Li,Hoai-Nam Vu,Derrick Wing Kwan Ng,Wei Xiang*

Main category: cs.NI

TL;DR: 本文提出了一种噪声-稀疏性感知扩散模型（NSADM）后处理框架，用于提升ISAC系统中环境重建的点云质量和去噪效果。


<details>
  <summary>Details</summary>
Motivation: ISAC系统初始获得的环境重建结果因点云高度稀疏和噪声方差大而质量较差，亟需有效方法提升重建精度。

Method: 利用扩散模型强大的数据恢复能力，结合空间特征与噪声的可加性，设计NSADM后处理框架以增强点云密度并去除噪声。

Result: 仿真实验表明，所提方法在Chamfer距离和均方根误差指标上显著优于现有的基于模型和深度学习的方法。

Conclusion: NSADM能有效改善ISAC系统中环境重建的点云质量，为高分辨率环境感知提供了可行方案。

Abstract: Recently, environment reconstruction (ER) in integrated sensing and communication (ISAC) systems has emerged as a promising approach for achieving high-resolution environmental perception. However, the initial results obtained from ISAC systems are coarse and often unsatisfactory due to the high sparsity of the point clouds and significant noise variance. To address this problem, we propose a noise-sparsity-aware diffusion model (NSADM) post-processing framework. Leveraging the powerful data recovery capabilities of diffusion models, the proposed scheme exploits spatial features and the additive nature of noise to enhance point cloud density and denoise the initial input. Simulation results demonstrate that the proposed method significantly outperforms existing model-based and deep learning-based approaches in terms of Chamfer distance and root mean square error.

</details>


### [15] [Agent Discovery in Internet of Agents: Challenges and Solutions](https://arxiv.org/abs/2511.19113)
*Shaolong Guo,Yuntao Wang,Zhou Su,Yanghe Pan,Qinnan Hu,Tom H. Luan*

Main category: cs.NI

TL;DR: 本文提出了一种面向互联网智能体（IoA）的两阶段能力发现框架，通过自主能力声明与任务驱动的能力发现机制，结合语义建模、可扩展索引和记忆增强技术，提升了大规模智能体协作中的能力匹配效率与可扩展性。


<details>
  <summary>Details</summary>
Motivation: 在互联网智能体（IoA）范式中，数十亿异构且上下文相关的智能体需高效发现彼此能力以协同完成复杂任务，但现有方法在能力表示、可扩展发现及长期性能方面面临挑战。

Method: 提出一个两阶段能力发现框架：第一阶段为自主能力声明，使智能体能可信地发布机器可解释的能力描述；第二阶段为任务驱动的能力发现，支持上下文感知的搜索、排序与组合。该框架融合了语义能力建模、可扩展且可更新的索引机制以及记忆增强的持续发现策略。

Result: 仿真实验表明，所提方法显著提升了能力发现的性能与可扩展性。

Conclusion: 该研究为IoA中的大规模智能体协作提供了有效的能力发现机制，并提出了未来研究路线图及若干开放问题与潜在方向。

Abstract: Rapid advances in large language models and agentic AI are driving the emergence of the Internet of Agents (IoA), a paradigm where billions of autonomous software and embodied agents interact, coordinate, and collaborate to accomplish complex tasks. A key prerequisite for such large-scale collaboration is agent capability discovery, where agents identify, advertise, and match one another's capabilities under dynamic tasks. Agent's capability in IoA is inherently heterogeneous and context-dependent, raising challenges in capability representation, scalable discovery, and long-term performance. To address these issues, this paper introduces a novel two-stage capability discovery framework. The first stage, autonomous capability announcement, allows agents to credibly publish machine-interpretable descriptions of their abilities. The second stage, task-driven capability discovery, enables context-aware search, ranking, and composition to locate and assemble suitable agents for specific tasks. Building on this framework, we propose a novel scheme that integrates semantic capability modeling, scalable and updatable indexing, and memory-enhanced continual discovery. Simulation results demonstrate that our approach enhances discovery performance and scalability. Finally, we outline a research roadmap and highlight open problems and promising directions for future IoA.

</details>


<div id='cs.OS'></div>

# cs.OS [[Back]](#toc)

### [16] [Crash-Consistent Checkpointing for AI Training on macOS/APFS](https://arxiv.org/abs/2511.18323)
*Juha Jeon*

Main category: cs.OS

TL;DR: 该论文研究了在 macOS/APFS 上进行 AI 训练时的检查点安装协议与完整性验证机制，提出了三种具有不同持久性保证的写入模式，并设计了一种基于 SHA-256 的格式无关完整性保护机制，在大量崩溃与损坏注入实验中实现了接近 100% 的损坏检测率且无误报，同时量化了可靠性与性能之间的权衡。


<details>
  <summary>Details</summary>
Motivation: 深度学习训练依赖周期性检查点以从故障中恢复，但不安全的检查点安装可能导致磁盘上留下损坏文件，影响训练可靠性。因此，需要研究更安全、可靠的检查点安装协议和完整性验证方法。

Method: 实现三种写入模式（unsafe、atomic_nodirsync、atomic_dirsync），提供递增的持久性保证；设计一种格式无关的完整性保护机制，使用 SHA-256 校验和并支持自动回滚；通过崩溃注入（430 次 unsafe 模式试验）和损坏注入（1600 次 atomic 模式试验）进行实验评估。

Result: 完整性保护机制在实验中检测到 99.8%-100% 的损坏，且无假阳性；相比 unsafe 基线，atomic_nodirsync 性能开销为 56.5%-108.4%，atomic_dirsync 为 84.2%-570.6%。

Conclusion: 该研究量化了 AI 训练中检查点机制在可靠性与性能之间的权衡，为生产环境中部署高可靠 AI 基础设施提供了实践指导。

Abstract: Deep learning training relies on periodic checkpoints to recover from failures, but unsafe checkpoint installation can leave corrupted files on disk. This paper presents an experimental study of checkpoint installation protocols and integrity validation for AI training on macOS/APFS. We implement three write modes with increasing durability guarantees: unsafe (baseline, no fsync), atomic_nodirsync (file-level durability via fsync()), and atomic_dirsync (file + directory durability). We design a format-agnostic integrity guard using SHA-256 checksums with automatic rollback. Through controlled experiments including crash injection (430 unsafe-mode trials) and corruption injection (1,600 atomic-mode trials), we demonstrate that the integrity guard detects 99.8-100% of corruptions with zero false positives. Performance overhead is 56.5-108.4% for atomic_nodirsync and 84.2-570.6% for atomic_dirsync relative to the unsafe baseline. Our findings quantify the reliability-performance trade-offs and provide deployment guidance for production AI infrastructure.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [17] [The Software Engineering Simulations Lab: Agentic AI for RE Quality Simulations](https://arxiv.org/abs/2511.17762)
*Henning Femmer,Ivan Esau*

Main category: cs.SE

TL;DR: 本文提出利用基于智能体的AI仿真来补充需求工程（RE）研究，通过模拟软件工程过程生成经验数据，以应对高质量需求缺陷实证数据稀缺的问题，并适应AI参与需求消费的新趋势。


<details>
  <summary>Details</summary>
Motivation: 当前需求工程中的质量评估仍主要依赖直觉和轶事证据，缺乏大量实证数据支持；同时，随着AI越来越多地参与开发过程，传统面向人类的需求表达方式可能不再最优，亟需新的研究手段来探索适用于人机共读的需求质量因素。

Method: 作者提出在需求工程研究中引入基于智能体的AI仿真方法，通过构建标准化智能体，在随机、动态、事件驱动的定性仿真环境中复现软件工程流程，从而高效生成关于需求缺陷影响的经验数据。

Result: 研究开发了一个初步概念、研究路线图、原型系统及可行性研究，结果表明即使采用简单实现也能运行有效仿真，验证了该方法的技术可行性并展示了其在RE研究中的潜力。

Conclusion: 基于智能体的AI仿真是对需求工程研究工具箱的有益补充，尽管在模拟人类行为方面存在局限，但其高效性和简易性使其值得进一步技术优化与广泛应用。

Abstract: Context and motivation. Quality in Requirements Engineering (RE) is still predominantly anecdotal and intuition-driven. Creating a solid requirements quality model requires broad sets of empirical evidence to evaluate quality factors and their context. Problem. However, empirical data on the detailed effects of requirements quality defects is scarce, since it is costly to obtain. Furthermore, with the advent of AI-based development, the requirements quality factors may change: Requirements are no longer only consumed by humans, but increasingly also by AI agents, which might lead to a different efficient and effective requirements style. Principal ideas. We propose to extend the RE research toolbox with Agentic AI simulations, in which software engineering (SE) processes are replicated by standardized agents in stochastic, dynamic, event-driven, qualitative simulations. We argue that their speed and simplicity makes them a valuable addition to RE research, although limitations in replicating human behavior need to be studied and understood. Contribution. This paper contributes a first concept, a research roadmap, a prototype, and a first feasibility study for RE simulations with agentic AI. Study results indicate that even a naive implementation leads to executable simulations, encouraging technical improvements along with broader application in RE research.

</details>


### [18] [Validating API Design Requirements for Interoperability: A Static Analysis Approach Using OpenAPI](https://arxiv.org/abs/2511.17836)
*Edwin Sundberg,Thea Ekmark,Workneh Yilma Ayele*

Main category: cs.SE

TL;DR: 本文提出了一种名为S.E.O.R.A的可配置规则引擎，用于在API设计早期自动检测OpenAPI规范中的结构违规，从而支持企业级API的质量、互操作性和治理。


<details>
  <summary>Details</summary>
Motivation: 当前API设计质量评估主要依赖手动且临时的方式，尤其在开发早期缺乏系统化支持，难以保障API的一致性、互操作性及与企业架构的对齐。

Method: 采用设计科学研究方法（DSR），通过文献综述提炼75条API设计规则，并构建一个可配置的规则引擎，允许组织根据自身需求启用、禁用或定制规则；通过结构化实验和行业专家的主题分析进行评估。

Result: S.E.O.R.A能有效支持非功能性API需求的早期验证，提供可追溯的反馈，自动化原本需人工检查的设计合规性任务，并与需求工程和质量保证流程良好集成。

Conclusion: 该工作将API设计原则转化为可验证约束并嵌入实用工具中，提升了API设计的一致性与可复用性，未来将探索IDE集成、规则扩展及在敏捷开发中的持续合规支持。

Abstract: RESTful APIs are central in developing interoperable, modular, and maintainable software systems in enterprises today. Also, it is essential to support system evolution, service interoperability, and governance across organizational boundaries to ensure good quality and consistency of these APIs. However, evaluating API design quality, which is part of non-functional requirement tasks, remains a largely manual and ad hoc process, particularly during early development. Using a Design Science Research (DSR) methodology, we elicited user needs, identified 75 API design rules using a literature review, and implemented a configurable rule engine to detect structural violations in OpenAPI specifications. The proposed tool supports organizational adaptability by allowing rules to be customized, enabled, or disabled, enabling integration of domain-specific standards. The evaluation was conducted through structured experiments and thematic analysis involving industry experts. API quality validation contributes to aligning technical designs with requirements and enterprise architecture by strengthening interoperability and governance between enterprise systems. The results show that S.E.O.R.A facilitates early validation of non-functional API requirements, provides actionable and traceable feedback, and aligns well with requirements elicitation and quality assurance processes. It improves the API design process by automating checks that would otherwise require manual inspection, thus supporting consistent and reusable conformance practices. This work contributes to requirements engineering by operationalizing design principles as verifiable constraints and embedding them into a practical validation tool. Future directions include IDE integration, expanded rule coverage, and real-world deployment to support continuous compliance in agile API development lifecycles.

</details>


### [19] [A Low-Code Methodology for Developing AI Kiosks: a Case Study with the DIZEST Platform](https://arxiv.org/abs/2511.17853)
*SunMin Moon,Jangwon Gim,Chaerin Kim,Yeeun Kim,YoungJoo Kim,Kang Choi*

Main category: cs.SE

TL;DR: 本文提出一种基于DIZEST低代码平台的AI集成方法，以解决传统信息亭系统在集成性、灵活性和协作性方面的不足，并通过案例研究验证其在互操作性、用户体验和部署灵活性方面的优势。


<details>
  <summary>Details</summary>
Motivation: 现代信息亭系统面临缺乏集成、结构僵化、性能瓶颈及缺少协作框架等挑战，亟需一种更灵活高效的架构来支持AI功能的快速部署与协同。

Method: 采用DIZEST低代码平台，通过直观的工作流设计实现AI模块的无缝集成，并与Jupyter Notebook、ComfyUI和Orange3等现有平台进行对比分析。

Result: DIZEST在关键评估指标上表现优于现有平台；照片信息亭案例研究表明该方法显著提升了系统的互操作性、用户体验和部署灵活性。

Conclusion: DIZEST低代码架构为信息亭系统提供了一种高效、灵活且易于集成AI能力的解决方案，具有良好的应用前景和推广价值。

Abstract: This paper presents a comprehensive study on enhancing kiosk systems through a low-code architecture, with a focus on AI-based implementations. Modern kiosk systems are confronted with significant challenges, including a lack of integration, structural rigidity, performance bottlenecks, and the absence of collaborative frameworks. To overcome these limitations, we propose a DIZEST-based approach methodology, a specialized low-code platform that enables intuitive workflow design and seamless AI integration. Through a comparative analysis with existing platforms, including Jupyter Notebook, ComfyUI, and Orange3, we demonstrate that DIZEST delivers superior performance across key evaluation criteria. Our photo kiosk case study further validates the effectiveness of this approach in improving interoperability, enhancing user experience, and increasing deployment flexibility.

</details>


### [20] [Enhancing Automated Program Repair via Faulty Token Localization and Quality-Aware Patch Refinement](https://arxiv.org/abs/2511.18001)
*Jiaolong Kong,Xiaofei Xie,Yiheng Xiong,Yuekun Wang,Jian Wang*

Main category: cs.SE

TL;DR: 本文提出了TokenRepair，一种结合内部反思与外部反馈的两层细化框架，通过定位可疑代码令牌并进行针对性修复，显著提升了自动程序修复的性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于大语言模型的自动程序修复方法主要依赖粗粒度的外部反馈（如测试结果），缺乏细粒度的内部信号来识别补丁失败原因或错误代码位置，导致修复效率低、错误传播和性能不佳。

Method: TokenRepair首先通过分析上下文感知的令牌级不确定性波动进行内部反思，识别补丁中可疑或低置信度的令牌；然后采用思维链引导重写策略仅对这些局部令牌进行精细化修正；同时引入质量感知的外部反馈机制，在迭代前过滤低质量补丁。

Result: 在Defects4J 1.2上成功修复88个缺陷，在HumanEval-Java上修复139个，相比现有方法在Defects4J 1.2上提升8.2%至34.9%，在HumanEval-Java上提升3.3%至16.1%。

Conclusion: TokenRepair通过融合细粒度内部反思与质量感知外部反馈，实现了更高效、精准的自动程序修复，达到了当前最优的修复效果。

Abstract: Large language models (LLMs) have recently demonstrated strong potential for automated program repair (APR). However, existing LLM-based techniques primarily rely on coarse-grained external feedback (e.g.,test results) to guide iterative patch generation, while lacking fine-grained internal signals that reveal why a patch fails or which parts of the generated code are likely incorrect. This limitation often leads to inefficient refinement, error propagation, and suboptimal repair performance. In this work, we propose TokenRepair, a novel two-level refinement framework that enhances APR by integrating internal reflection for localizing potentially faulty tokens with external feedback for quality-aware patch refinement. Specifically, TokenRepair first performs internal reflection by analyzing context-aware token-level uncertainty fluctuations to identify suspicious or low-confidence tokens within a patch. It then applies Chain-of-Thought guided rewriting to refine only these localized tokens, enabling targeted and fine-grained correction. To further stabilize the iterative repair loop, TokenRepair incorporates a quality-aware external feedback mechanism that evaluates patch quality and filters out low-quality candidates before refinement. Experimental results show that TokenRepair achieves new state-of-the-art repair performance, correctly fixing 88 bugs on Defects4J 1.2 and 139 bugs on HumanEval-Java, demonstrating substantial improvements ranging from 8.2% to 34.9% across all models on Defects4J 1.2 and from 3.3% to 16.1% on HumanEval-Java.

</details>


### [21] [Event-Chain Analysis for Automated Driving and ADAS Systems: Ensuring Safety and Meeting Regulatory Timing Requirements](https://arxiv.org/abs/2511.18092)
*Sebastian Dingler,Philip Rehkop,Florian Mayer,Ralf Muenzenberger*

Main category: cs.SE

TL;DR: 本文提出一种基于事件链建模的白盒方法，用于在架构层面推导、建模和验证自动驾驶系统的端到端时序约束，以满足国际法规对系统响应时间的严格要求。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶系统（ADS）和高级驾驶辅助系统（ADAS）需同时满足高功能性与严格的时序约束，而现有黑盒方法难以提供足够的透明度来证明系统架构符合国际法规（如UN、NCAP、ISO、NHTSA等）对反应时间的要求。

Method: 采用基于事件链建模（Event-Chain Modeling）的白盒方法，对从感知、规划到执行和人机交互的每个功能组件进行时序行为分析，并在架构层面进行端到端时序约束的建模与仿真验证。

Result: 通过案例研究验证了该方法能早期识别合规问题、系统化优化参数，并通过概率分析生成定量证据，从而提升法规合规性、优化系统设计并支持基于模型的安全分析。

Conclusion: 事件链中心的方法为自动驾驶系统提供了一种透明、可验证且符合法规要求的时序分析框架，有助于在开发早期确保系统满足安全与合规性标准。

Abstract: Automated Driving Systems (ADS), including Advanced Driver Assistance Systems (ADAS), must fulfill not only high functional expectations but also stringent timing constraints mandated by international regulations and standards. Regulatory frameworks such as UN regulations, NCAP standards, ISO norms, and NHTSA guidelines impose strict bounds on system reaction times to ensure safe vehicle operation. This paper presents a structured, White-Box methodology based on Event-Chain Modeling to address these timing challenges. Unlike Black-Box approaches, Event-Chain Analysis offers transparent insights into the timing behavior of each functional component - from perception and planning to actuation and human interaction. This perspective is also aligned with multiple regulations, which require that homologation dossiers provide evidence that the chosen system architecture is suitable to ensure compliance with the specified requirements. Our methodology enables the derivation, modeling, and validation of end-to-end timing constraints at the architectural level and facilitates early verification through simulation. Through a detailed case study, we demonstrate how this Event-Chain-centric approach enhances regulatory compliance, optimizes system design, and supports model-based safety analysis techniques, with results showing early identification of compliance issues, systematic parameter optimization, and quantitative evidence generation through probabilistic analysis.

</details>


### [22] [Towards a General Framework for HTN Modeling with LLMs](https://arxiv.org/abs/2511.18165)
*Israel Puerta-Merino,Carlos Núñez-Molina,Pablo Mesejo,Juan Fernández-Olivares*

Main category: cs.SE

TL;DR: 本文提出了L2HP，一个支持大语言模型（LLM）生成分层规划（HP）模型的框架，并通过实验发现LLM在生成HP模型时面临比非分层自动规划（AP）更大的挑战，尤其是在语法有效性方面。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型在自动生成分层规划（HP）模型方面的能力远不如在非分层自动规划（AP）中成熟，存在明显的研究空白。

Method: 作者扩展了现有的L2P库，提出L2HP框架，使其支持HP模型生成，并在PlanBench数据集上对LLM在AP与HP建模能力方面进行了对比实验。

Result: 实验结果显示，在PlanBench数据集中，LLM生成的HP和AP模型的解析成功率相近（约36%），但HP模型的语法有效性显著更低（1% vs. 20%）。

Conclusion: 分层规划对大语言模型提出了独特挑战，需进一步研究以提升其生成HP模型的质量。

Abstract: The use of Large Language Models (LLMs) for generating Automated Planning (AP) models has been widely explored; however, their application to Hierarchical Planning (HP) is still far from reaching the level of sophistication observed in non-hierarchical architectures. In this work, we try to address this gap. We present two main contributions. First, we propose L2HP, an extension of L2P (a library to LLM-driven PDDL models generation) that support HP model generation and follows a design philosophy of generality and extensibility. Second, we apply our framework to perform experiments where we compare the modeling capabilities of LLMs for AP and HP. On the PlanBench dataset, results show that parsing success is limited but comparable in both settings (around 36\%), while syntactic validity is substantially lower in the hierarchical case (1\% vs. 20\% of instances). These findings underscore the unique challenges HP presents for LLMs, highlighting the need for further research to improve the quality of generated HP models.

</details>


### [23] [Establishing Traceability Links between Release Notes & Software Artifacts: Practitioners' Perspectives](https://arxiv.org/abs/2511.18187)
*Sristy Sumana Nath,Banani Roy,Munima Jahan*

Main category: cs.SE

TL;DR: 本文针对开源项目中发布说明与开发工件（如PR、提交、问题）之间追溯链接缺失或损坏的问题，提出基于大语言模型（LLM）的方法自动建立追溯链接，并构建了包含3500个验证实例的基准数据集。结合时间邻近特征，Gemini 1.5 Pro在PR追溯任务中达到0.73的Precision@1。用户调查显示，84%的开源开发者认为维护追溯性“有些重要”或“非常重要”。


<details>
  <summary>Details</summary>
Motivation: 在开源环境中，由于贡献者远程异步协作，发布说明与开发工件之间的追溯链接常常缺失或出错，影响技术债务管理和软件可维护性。实证研究发现47%的发布工件缺少追溯链接，12%存在断链，亟需自动化解决方案。

Method: 首先分析发布说明中的What、Why、How信息，并与PR、提交和问题对齐；然后构建包含3500个经过筛选和验证的追溯链接实例的基准数据集；接着采用基于大语言模型（如Gemini 1.5 Pro）的方法，结合时间邻近特征，自动建立发布说明与PR、提交、问题之间的三类追溯链接。

Result: 在PR追溯任务中，结合时间邻近特征的LLM方法实现了0.73的Precision@1；对33名开源从业者的在线调查显示，16%认为追溯性维护“非常重要”，68%认为“有些重要”。

Conclusion: 基于大语言模型的方法能有效恢复发布说明与开发工件间的追溯链接，具有较高的准确率和良好的实际应用潜力，有助于提升开源项目的可维护性和技术债务管理能力。

Abstract: Maintaining traceability links between software release notes and corresponding development artifacts, e.g., pull requests (PRs), commits, and issues, is essential for managing technical debt and ensuring maintainability. However, in open-source environments where contributors work remotely and asynchronously, establishing and maintaining these links is often error-prone, time-consuming, and frequently overlooked. Our empirical study of GitHub repositories revealed that 47% of release artifacts lacked traceability links, and 12% contained broken links. To address this gap, we first analyzed release notes to identify their What, Why, and How information and assessed how these align with PRs, commits, and issues. We curated a benchmark dataset consisting of 3,500 filtered and validated traceability link instances. Then, we implemented LLM-based approaches to automatically establish traceability links of three pairs between release note contents & PRs, release note contents & PRs and release note contents & issues. By combining the time proximity feature, the LLM-based approach, e.g., Gemini 1.5 Pro, achieved a high Precision@1 value of 0.73 for PR traceability recovery. To evaluate the usability and adoption potential of this approach, we conducted an online survey involving 33 open-source practitioners. 16% of respondents rated as very important, and 68% as somewhat important for traceability maintenance.

</details>


### [24] [LLM Assisted Coding with Metamorphic Specification Mutation Agent](https://arxiv.org/abs/2511.18249)
*Mostafijur Rahman Akhond,Gias Uddin*

Main category: cs.SE

TL;DR: 本文提出CodeMetaAgent（CMA），一种基于蜕变关系（MRs）驱动的大语言模型（LLM）智能体，通过在生成过程中引入MRs来系统化地优化任务描述并生成语义约束的测试用例，从而提升LLM在软件工程中的代码生成准确性和覆盖率。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在软件工程中的可靠性常因用户输入规范不明确或不一致而受损，现有方法多将蜕变关系用于事后验证，缺乏在生成阶段引导LLM的能力。

Method: 将蜕变关系与LLM结合，在代码生成前利用MRs对任务规范进行系统性精炼，并生成语义等价且受约束的测试用例，以减少由模糊规范引起的输出变异性。

Result: 在HumanEval-Pro、MBPP-Pro和SWE-Bench_Lite数据集上评估，使用GPT-4o、Mistral Large、GPT-OSS和Qwen3-Coder模型，代码生成准确率最高提升17%，代码覆盖率最高达99.81%。

Conclusion: 蜕变关系可作为简单而有效的引导机制，显著提升LLM在软件开发中的生成一致性与可靠性。

Abstract: Metamorphic Relations (MRs) serve as a foundational mechanism for generating semantically equivalent mutations. Software engineering has advanced significantly in recent years with the advent of Large Language Models (LLMs). However, the reliability of LLMs in software engineering is often compromised by ambiguities and inconsistencies due to improper user specification. To address this challenge, we present CodeMetaAgent (CMA), a metamorphic relation-driven LLM agent that systematically refines task specifications and generates semantically constrained test cases. Our proposed framework uses MRs with LLMs to improve generation consistency and reduce variability caused by specifications, unlike the traditional use of MRs as post validations. Our framework has been evaluated on the HumanEval-Pro, MBPP-Pro, and SWE-Bench_Lite datasets using the GPT-4o, Mistral Large, GPT-OSS, and Qwen3-Coder models. It improved code generation accuracy by up to 17% and achieved code coverage gains of up to 99.81%. These results show that metamorphic relations can be a simple but effective guide in assisting LLM-based software development.

</details>


### [25] [A Needle in a Haystack: Intent-driven Reusable Artifacts Recommendation with LLMs](https://arxiv.org/abs/2511.18343)
*Dongming Jin,Zhi Jin,Xiaohong Chen,Zheng Fang,Linyu Li,Yuanpeng He,Jia Li,Yirang Zhang,Yingtao Fang*

Main category: cs.SE

TL;DR: 本文针对开源软件开发中可复用构件推荐的挑战，构建了意图驱动的基准数据集IntentRecBench，并对大语言模型（LLMs）与传统方法进行了系统评估。研究发现LLMs虽优于传统方法，但仍存在精度低和推理成本高的问题。为此，作者提出TreeRec框架，利用特征树引导的语义组织方式提升推荐效果和效率，实验证明其在多个生态系统中具有良好的通用性和实用性。


<details>
  <summary>Details</summary>
Motivation: 在开源软件开发中，尽管可复用构件能提高效率和可靠性，但面对大量候选构件，开发者难以快速找到符合需求的构件。现有基于检索和学习的方法以及新兴的大语言模型（LLMs）尚未被充分评估其在该任务中的有效性，尤其缺乏统一的基准和对LLMs局限性的深入分析。

Method: 作者首先构建了一个覆盖三个主流开源生态系统的意图驱动构件推荐基准IntentRecBench；然后对五种主流LLMs和六种传统方法进行精度与效率的综合比较；在此基础上，提出TreeRec框架，通过LLM驱动的语义抽象将构件组织成层次化语义树，以实现意图-功能对齐并降低推理开销。

Result: 实验表明，虽然LLMs整体优于传统方法，但在大规模候选空间下仍面临精度不足和推理成本高的问题；而TreeRec能显著提升多种LLMs在不同生态系统中的推荐性能，展现出良好的泛化能力和实际部署潜力。

Conclusion: 本研究揭示了当前LLMs在构件推荐任务中的优势与局限，并通过提出的TreeRec框架有效缓解了这些问题，为未来基于语义结构化和大模型的软件构件推荐提供了新思路和实用工具。

Abstract: In open source software development, the reuse of existing artifacts has been widely adopted to avoid redundant implementation work. Reusable artifacts are considered more efficient and reliable than developing software components from scratch. However, when faced with a large number of reusable artifacts, developers often struggle to find artifacts that can meet their expected needs. To reduce this burden, retrieval-based and learning-based techniques have been proposed to automate artifact recommendations. Recently, Large Language Models (LLMs) have shown the potential to understand intentions, perform semantic alignment, and recommend usable artifacts. Nevertheless, their effectiveness has not been thoroughly explored. To fill this gap, we construct an intent-driven artifact recommendation benchmark named IntentRecBench, covering three representative open source ecosystems. Using IntentRecBench, we conduct a comprehensive comparative study of five popular LLMs and six traditional approaches in terms of precision and efficiency. Our results show that although LLMs outperform traditional methods, they still suffer from low precision and high inference cost due to the large candidate space. Inspired by the ontology-based semantic organization in software engineering, we propose TreeRec, a feature tree-guided recommendation framework to mitigate these issues. TreeRec leverages LLM-based semantic abstraction to organize artifacts into a hierarchical semantic tree, enabling intent and function alignment and reducing reasoning time. Extensive experiments demonstrate that TreeRec consistently improves the performance of diverse LLMs across ecosystems, highlighting its generalizability and potential for practical deployment.

</details>


### [26] [Evaluating perturbation robustnessof generative systems that use COBOL code inputs](https://arxiv.org/abs/2511.18488)
*Samuel Ackerman,Wesam Ibraheem,Orna Raz,Marcel Zalmanovici*

Main category: cs.SE

TL;DR: 本文提出一个评估以COBOL代码为输入的大型语言模型（LLM）系统鲁棒性的框架，通过构建COBOL代码扰动方法和扩展基准数据集，结合可视化工具分析系统对语义不变输入变化的敏感性，并支持后续系统优化。


<details>
  <summary>Details</summary>
Motivation: 当前包含大型语言模型的系统对不改变语义的微小输入变化过于敏感，影响实用性；而COBOL作为大量关键业务系统的语言，其代码通常无法用于模型训练，使得针对COBOL输入的系统鲁棒性评估尤为必要且具有挑战性。

Method: 开发一套COBOL段落与完整程序的扰动方法库，构建任务特定的变体扩展基准数据集；通过计算系统输出在个体与聚合指标上的变化来评估鲁棒性，并设计动态表格与图表可视化仪表板辅助调试与归因分析。

Result: 成功构建了可用于评估COBOL相关任务（如COBOL-Java翻译）中LLM系统鲁棒性的扰动数据集与评估流程，并提供了可视化工具帮助识别输入变化导致性能下降的根本原因。

Conclusion: 该框架有效支持对基于LLM的COBOL处理系统的鲁棒性评估与调试，其方法可推广至其他代码生成或解释任务，并可通过预处理等手段进一步提升系统稳定性。

Abstract: Systems incorporating large language models (LLMs) as a component are known to be sensitive (i.e., non-robust) to minor input variations that do not change the meaning of the input; such sensitivity may reduce the system's usefulness. Here, we present a framework to evaluate robustness of systems using COBOL code as input; our application is translation between COBOL and Java programming languages, but the approach extends to other tasks such as code generation or explanation. Targeting robustness of systems with COBOL as input is essential yet challenging. Many business-critical applications are written in COBOL, yet these are typically proprietary legacy applications and their code is unavailable to LLMs for training. We develop a library of COBOL paragraph and full-program perturbation methods, and create variant-expanded versions of a benchmark dataset of examples for a specific task. The robustness of the LLM-based system is evaluated by measuring changes in values of individual and aggregate metrics calculated on the system's outputs. Finally, we present a series of dynamic table and chart visualization dashboards that assist in debugging the system's outputs, and monitoring and understanding root causes of the system's sensitivity to input variation. These tools can be further used to improve the system by, for instance, indicating variations that should be handled by pre-processing steps.

</details>


### [27] [HQPEF-Py: Metrics, Python Patterns, and Guidance for Evaluating Hybrid Quantum Programs](https://arxiv.org/abs/2511.18506)
*Michael Adjei Osei,Sidney Shapiro*

Main category: cs.SE

TL;DR: 该论文提出了一种面向端到端混合量子工作流的评估方法，包括工作流感知的量子就绪等级（QRL）、带质量约束的归一化加速比（用于量子效用UQ），以及混合流水线的时间与漂移审计，并提供了相应的Python参考实现。


<details>
  <summary>Details</summary>
Motivation: 现有量子程序评估多聚焦于孤立设备或算法，缺乏对端到端混合量子工作流的整体评价体系。

Method: 基于HQPEF框架，形式化定义了工作流感知的QRL评分、在质量约束下的归一化加速比（UQ）以及混合流水线的时序与漂移审计方法，并通过Python实现示例说明如何结合主流经典与量子求解器进行可复现、预算匹配的评估。

Result: 提出了三项新指标与审计流程，并配套提供简洁的Python参考实现，支持在Qiskit或PennyLane等平台上实例化这些评估方法。

Conclusion: 该工作为混合量子程序的端到端评估提供了系统性框架和实用工具，有助于更全面地衡量量子计算的实际效用与成熟度。

Abstract: We study how to evaluate hybrid quantum programs as end-to-end workflows rather than as isolated devices or algorithms. Building on the Hybrid Quantum Program Evaluation Framework (HQPEF), we formalize a workflow-aware Quantum Readiness Level (QRL) score; define a normalized speedup under quality constraints for the Utility of Quantumness (UQ); and provide a timing-and-drift audit for hybrid pipelines. We complement these definitions with concise Python reference implementations that illustrate how to instantiate the metrics and audit procedures with state-of-the-art classical and quantum solvers (e.g., via Qiskit or PennyLane), while preserving matched-budget discipline and reproducibility.

</details>


### [28] [End-to-End Automated Logging via Multi-Agent Framework](https://arxiv.org/abs/2511.18528)
*Renyi Zhong,Yintong Huo,Wenwei Gu,Yichen Li,Michael R. Lyu*

Main category: cs.SE

TL;DR: 本文提出 Autologger，一个结合分类器与多智能体系统的混合框架，用于自动化完成“是否记录日志”、“在哪里记录”以及“记录什么”三个关键决策，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 开发者在软件日志实践中面临过度记录（成本高）和记录不足（风险大）的双重困境，而现有自动日志工具往往忽略“是否记录”这一根本决策，且难以处理日志生成的复合性问题。

Method: Autologger 框架首先使用微调后的分类器（Judger）判断方法是否需要新增日志；若需要，则激活包含 Locator（决定日志位置）和 Generator（决定日志内容）的多智能体系统，并结合程序分析与检索工具协同工作。

Result: 在三个成熟开源项目的大规模语料上评估表明，Autologger 在“是否记录”任务上达到 96.63% 的 F1 分数；端到端设置下，其生成日志的整体质量比最强基线提升 16.13%（基于 LLM-as-a-judge 评分），且能稳定提升多种骨干 LLM 的性能。

Conclusion: Autologger 有效解决了自动日志生成中的核心挑战，提供了一个通用、高效且高质量的端到端日志插入解决方案。

Abstract: Software logging is critical for system observability, yet developers face a dual crisis of costly overlogging and risky underlogging. Existing automated logging tools often overlook the fundamental whether-to-log decision and struggle with the composite nature of logging. In this paper, we propose Autologger, a novel hybrid framework that addresses the complete the end-to-end logging pipeline. Autologger first employs a fine-tuned classifier, the Judger, to accurately determine if a method requires new logging statements. If logging is needed, a multi-agent system is activated. The system includes specialized agents: a Locator dedicated to determining where to log, and a Generator focused on what to log. These agents work together, utilizing our designed program analysis and retrieval tools. We evaluate Autologger on a large corpus from three mature open-source projects against state-of-the-art baselines. Our results show that Autologger achieves 96.63\% F1-score on the crucial whether-to-log decision. In an end-to-end setting, Autologger improves the overall quality of generated logging statements by 16.13\% over the strongest baseline, as measured by an LLM-as-a-judge score. We also demonstrate that our framework is generalizable, consistently boosting the performance of various backbone LLMs.

</details>


### [29] [From Code Foundation Models to Agents and Applications: A Practical Guide to Code Intelligence](https://arxiv.org/abs/2511.18538)
*Jian Yang,Wei Zhang,Shark Liu,Jiajun Wu,Shawn Guo,Yizhi Li*

Main category: cs.SE

TL;DR: 本文系统综述了代码大语言模型（Code LLMs）的全生命周期，涵盖数据构建、预训练、监督微调、强化学习及高级提示方法，并对比通用与专用模型性能，分析学术研究与工业实践之间的差距，同时通过一系列实验深入探讨训练策略与模型设计的关键因素。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型在代码生成任务中性能显著提升并被广泛应用于商业工具，亟需系统性梳理其技术演进、关键方法与实际部署挑战，弥合学术研究与真实软件开发场景之间的鸿沟。

Method: 作者通过综合文献分析与一系列探针实验，系统考察代码LLM从数据准备到后训练各阶段的技术路径，评估主流通用和专用模型的表现，并对预训练、微调和强化学习等环节进行消融与对比实验。

Result: 研究揭示了不同训练阶段（如预训练数据规模、微调策略、RL应用）对模型性能的影响，明确了架构选择、超参敏感性及数据质量的关键作用，并指出现有基准测试在代码正确性、安全性及上下文理解等方面与实际需求存在差距。

Conclusion: 代码大语言模型虽取得显著进展，但要实现可靠的实际部署仍需解决正确性、安全性和大规模代码上下文整合等问题；未来研究应聚焦于贴近真实开发流程的任务设计与评估体系。

Abstract: Large language models (LLMs) have fundamentally transformed automated software development by enabling direct translation of natural language descriptions into functional code, driving commercial adoption through tools like Github Copilot (Microsoft), Cursor (Anysphere), Trae (ByteDance), and Claude Code (Anthropic). While the field has evolved dramatically from rule-based systems to Transformer-based architectures, achieving performance improvements from single-digit to over 95\% success rates on benchmarks like HumanEval. In this work, we provide a comprehensive synthesis and practical guide (a series of analytic and probing experiments) about code LLMs, systematically examining the complete model life cycle from data curation to post-training through advanced prompting paradigms, code pre-training, supervised fine-tuning, reinforcement learning, and autonomous coding agents. We analyze the code capability of the general LLMs (GPT-4, Claude, LLaMA) and code-specialized LLMs (StarCoder, Code LLaMA, DeepSeek-Coder, and QwenCoder), critically examining the techniques, design decisions, and trade-offs. Further, we articulate the research-practice gap between academic research (e.g., benchmarks and tasks) and real-world deployment (e.g., software-related code tasks), including code correctness, security, contextual awareness of large codebases, and integration with development workflows, and map promising research directions to practical needs. Last, we conduct a series of experiments to provide a comprehensive analysis of code pre-training, supervised fine-tuning, and reinforcement learning, covering scaling law, framework selection, hyperparameter sensitivity, model architectures, and dataset comparisons.

</details>


### [30] [Strategic Decision Framework for Enterprise LLM Adoption](https://arxiv.org/abs/2511.18589)
*Michael Trusov,Minha Hwang,Zainab Jamal,Swarup Chandra*

Main category: cs.SE

TL;DR: 本文提出一个六步决策框架，帮助企业在采用大语言模型（LLM）时从应用场景选择到部署全过程做出明智决策，兼顾安全性、合规性与业务目标。


<details>
  <summary>Details</summary>
Motivation: 企业在快速采用大语言模型（LLM）过程中面临缺乏明确指导的问题，尤其在数据安全、开发路径、基础设施和部署策略等关键决策上存在挑战。

Method: 基于对成功与失败案例的深入访谈和分析，构建了一个系统性的六步决策框架，并结合B2B和B2C场景中的实际案例提供实践指导。

Result: 该框架帮助不同行业的组织（如医疗、金融、软件公司）在保障数据安全与合规的前提下，有效将LLM能力与业务目标对齐，实现安全高效的集成。

Conclusion: 通过结构化的决策流程和真实案例支持，企业可更自信地推进LLM应用，在客户自动化服务、内容生成和高级分析等场景中实现价值。

Abstract: Organizations are rapidly adopting Large Language Models (LLMs) to transform their operations, yet they lack clear guidance on key decisions for adoption and implementation. While LLMs offer powerful capabilities in content generation, assisted coding, and process automation, businesses face critical challenges in data security, LLM solution development approach, infrastructure requirements, and deployment strategies. Healthcare providers must protect patient data while leveraging LLMs for medical analysis, financial institutions need to balance automated customer service with regulatory compliance, and software companies seek to enhance development productivity while maintaining code security.
  This article presents a systematic six-step decision framework for LLM adoption, helping organizations navigate from initial application selection to final deployment. Based on extensive interviews and analysis of successful and failed implementations, our framework provides practical guidance for business leaders to align technological capabilities with business objectives. Through key decision points and real-world examples from both B2B and B2C contexts, organizations can make informed decisions about LLM adoption while ensuring secure and efficient integration across various use cases, from customer service automation to content creation and advanced analytics.

</details>


### [31] [From Reviewers' Lens: Understanding Bug Bounty Report Invalid Reasons with LLMs](https://arxiv.org/abs/2511.18608)
*Jiangrui Zheng,Yingming Zhou,Ali Abdullah Ahmad,Hanqing Yao,Xueqing Liu*

Main category: cs.SE

TL;DR: 本文研究了AI生成漏洞报告在众测平台上的有效性问题，发现大语言模型虽整体准确率高，但在识别无效报告方面表现不佳；通过构建信息泄露类漏洞的拒绝原因分类体系并结合检索增强生成（RAG）框架，显著提升了无效报告识别的一致性与公平性，并揭示了报告者声誉对评审结果的潜在影响。


<details>
  <summary>Details</summary>
Motivation: 随着AI生成漏洞报告的兴起，大量报告被标记为无效，但缺乏对其无效原因的系统理解。为提升报告质量、减轻评审负担，亟需有效预测并解释无效报告的原因。

Method: 作者收集了9,942份公开的众测报告（含1,400份无效报告），评估多种大语言模型（如GPT-5、DeepSeek和微调RoBERTa）在识别无效报告上的表现；随后构建信息泄露漏洞的拒绝原因分类体系，并将其整合进检索增强生成（RAG）框架以改进识别效果；同时分析报告者声誉等非内容因素对评审决策的影响。

Result: 大语言模型在整体准确率上表现良好，但普遍难以识别无效报告，存在过度接受倾向；引入基于分类体系的RAG方法显著提升了无效报告分类的一致性并减少了偏见；此外，高声誉报告者在边界案例中更可能获得有利评审结果。

Conclusion: 识别无效漏洞报告仍具挑战性，将大语言模型与结构化的评审知识相结合，有助于实现更透明、一致的漏洞报告评审流程。

Abstract: Bug bounty platforms (e.g., HackerOne, BugCrowd) leverage crowd-sourced vulnerability discovery to improve continuous coverage, reduce the cost of discovery, and serve as an integral complement to internal red teams. With the rise of AI-generated bug reports, little work exists to help bug hunters understand why these reports are labeled as invalid. To improve report quality and reduce reviewers' burden, it is critical to predict invalid reports and interpret invalid reasons.
  In this work, we conduct an empirical study with the purpose of helping bug hunters understand the validity of reports. We collect a dataset of 9,942 disclosed bug bounty reports, including 1,400 invalid reports, and evaluate whether state-of-the-art large language models can identify invalid reports. While models such as GPT-5, DeepSeek, and a fine-tuned RoBERTa achieve strong overall accuracy, they consistently struggle to detect invalid cases, showing a tendency to over-accept reports. To improve invalidity detection, we build a taxonomy of rejection reasons for Information Disclosure vulnerabilities and incorporate it into a retrieval-augmented generation (RAG) framework. This approach substantially improves classification consistency and reduces bias. We also examine whether reviewer decisions may be influenced by factors beyond the content of the report. Our analysis shows that reporters with higher reputations tend to receive more favorable outcomes in borderline cases, suggesting that perceived expertise can influence review judgments.
  Overall, our findings highlight the challenges of invalid report identification and show that combining LLMs with structured reviewer knowledge can support more transparent and consistent vulnerability report review.

</details>


### [32] [Leveraging Discrete Choice Experiments for User-Centric Requirements Prioritization in mHealth Applications](https://arxiv.org/abs/2511.18625)
*Wei Wang,Hourieh Khalajzadeh,John Grundy,Anuradha Madugalla,Humphrey O. Obie*

Main category: cs.SE

TL;DR: 本研究通过离散选择实验（DCE）量化慢性病患者对移动健康（mHealth）应用自适应界面设计的偏好，发现可控性、低频次和小幅度的适配更易被接受，而常用功能和照护者介入可能降低适配价值，并揭示了不同人群间的偏好差异。


<details>
  <summary>Details</summary>
Motivation: 尽管自适应用户界面（AUIs）可提升mHealth应用的用户体验，但其采纳仍面临障碍。为推动广泛接受，需深入理解用户在适配设计中的偏好与权衡。

Method: 研究采用离散选择实验（DCE），招募186名患有慢性病并使用mHealth应用的参与者，评估其在六个属性不同水平组合下的选择偏好；使用混合Logit模型分析偏好异质性，并按年龄、性别、健康状况和应对机制进行子群分析。

Result: 结果表明，保持可用性的同时提供对适配的控制权、较低频率的适配以及小幅改动是促进采纳的关键因素；而高频使用的功能和照护者参与会削弱适配的感知价值；不同人口统计和行为子群体间存在显著偏好差异。

Conclusion: 该研究通过数据驱动方法揭示了mHealth自适应设计中的关键用户偏好与权衡，为未来自适应应用开发及软件工程中的需求优先级设定提供了实证依据和指导。

Abstract: Mobile health (mHealth) applications are widely used for chronic disease management, but usability and accessibility challenges persist due to the diverse needs of users. Adaptive User Interfaces (AUIs) offer a personalized solution to enhance user experience, yet barriers to adoption remain. Understanding user preferences and trade-offs is essential to ensure widespread acceptance of adaptation designs. This study identifies key factors influencing user preferences and trade-offs in mHealth adaptation design. A Discrete Choice Experiment (DCE) was conducted with 186 participants who have chronic diseases and use mHealth applications. Participants were asked to select preferred adaptation designs from choices featuring six attributes with varying levels. A mixed logit model was used to analyze preference heterogeneity and determine the factors most likely influencing adoption. Additionally, subgroup analyses were performed to explore differences by age, gender, health conditions, and coping mechanisms. Maintaining usability while ensuring controllability over adaptations, infrequent adaptations, and small-scale changes are key factors that facilitate the adoption of adaptive mHealth app designs. In contrast, frequently used functions and caregiver involvement can diminish the perceived value of such adaptations. This study employs a data-driven approach to quantify user preferences, identify key trade-offs, and reveal variations across demographic and behavioral subgroups through preference heterogeneity modeling. Furthermore, our results offer valuable guidance for developing future adaptive mHealth applications and lay the groundwork for continued exploration into requirements prioritization within the field of software engineering.

</details>


### [33] [ChroniUXMag: A Persona-Driven Framework for Inclusive mHealth Requirements Engineering](https://arxiv.org/abs/2511.18634)
*Wei Wang,Devi Karolita,Hourieh Khalajzadeh,John Grundy,Anuradha Madugalla,Humphrey O. Obie*

Main category: cs.SE

TL;DR: 本文提出了ChroniUXMag框架，用于在移动健康（mHealth）应用的需求工程中系统地识别和评估包容性需求，以应对慢性病患者在使用mHealth时因个体差异而面临的可及性与持续参与挑战。


<details>
  <summary>Details</summary>
Motivation: 传统需求工程方法忽视了慢性病患者在健康进展、依从性和照护支持变化下动态演化的多样化需求，导致mHealth应用在可访问性、包容性和用户持续参与方面存在不足。

Method: 基于InclusiveMag和GenderMag原则，通过两阶段流程开发ChroniUXMag框架：第一阶段通过文献综述、焦点小组、访谈和大规模调查识别包容性维度；第二阶段将这些维度整合为代表不同健康状况、态度和数字行为的用户画像，并嵌入改良的认知走查表中。

Result: 研究识别出13个反映mHealth使用社会技术复杂性的包容性维度（如信任、数字素养、依赖性和文化背景），并通过画像驱动的评估方法揭示传统可用性测试常忽略的包容性障碍。

Conclusion: ChroniUXMag为mHealth需求工程提供了一种可复现、循证的包容性嵌入方法，未来将在真实设计场景中通过实践者主导的评估进一步验证其第三阶段“应用”。

Abstract: Mobile health (mHealth) applications are increasingly adopted for chronic disease management, yet they face persistent challenges related to accessibility, inclusivity, and sustained engagement. Patients' needs evolve dynamically with their health progression, adherence, and caregiver support, creating unique requirements engineering (RE) challenges that traditional approaches often overlook. This study introduces ChroniUXMag, a framework for eliciting and analysing inclusivity requirements in mHealth design. Building on InclusiveMag and GenderMag principles, the framework aims to help researchers and practitioners systematically capture and evaluate factors that influence how individuals with chronic conditions perceive, trust, and interact with mHealth systems. The framework was developed through two stages of the InclusiveMag process. In the first stage, inclusivity facets were identified through a systematic literature review, focus groups, interviews, and a large-scale survey. In the second stage, these facets were synthesised into personas representing diverse health situations, attitudes, and digital practices, and integrated into an adapted cognitive walkthrough form. Thirteen facets were identified that capture the socio-technical complexity of mHealth use, including trust, digital literacy, dependency, and cultural context. These facets support structured, persona-driven evaluations that reveal inclusivity barriers often missed by traditional usability assessments. ChroniUXMag contributes to RE by offering a reproducible, evidence-based approach for embedding inclusivity into mHealth requirements. Future work will extend the third stage Apply through practitioner-led evaluation in real-world design contexts.

</details>


### [34] [Summary-Mediated Repair: Can LLMs use code summarisation as a tool for program repair?](https://arxiv.org/abs/2511.18782)
*Lukas Twist*

Main category: cs.SE

TL;DR: 本文提出一种仅通过提示实现的程序修复方法——摘要介导修复（summary-mediated repair），利用自然语言代码摘要作为中间步骤，尤其采用错误感知型诊断摘要，在多个大语言模型和基准测试中平均比直接修复基线多修复5%的错误。


<details>
  <summary>Details</summary>
Motivation: 大语言模型（LLMs）尽管在基准测试中表现优异，但生成的代码常包含难以察觉的实现级错误；而它们在代码摘要任务中能较好地捕捉高层意图，可能忽略低层错误。受此启发，作者探索将代码摘要作为程序修复的中间步骤。

Method: 提出“摘要介导修复”流程：先让LLM生成代码的自然语言摘要（尤其是错误感知型诊断摘要），再基于该摘要进行修复，无需额外训练，仅依赖提示工程。

Result: 在8个生产级LLM及HumanEvalPack、MBPP两个函数级基准上评估，错误感知摘要平均比直接修复基线多修复5%的未见错误，最高可修复65%的错误，但整体提升有限且依赖具体模型。

Conclusion: 代码摘要可作为廉价、人类可解释的诊断工具，有效集成到程序修复流程中，但并非万能解决方案。

Abstract: Large Language Models (LLMs) often produce code with subtle implementation-level bugs despite strong benchmark performance. These errors are hard for LLMs to spot and can have large behavioural effects; yet when asked to summarise code, LLMs can frequently surface high-level intent and sometimes overlook this low-level noise. Motivated by this, we propose summary-mediated repair, a prompt-only pipeline for program repair that leverages natural-language code summarisation as an explicit intermediate step, extending previous work that has already shown code summarisation to be a useful intermediary for downstream tasks. We evaluate our method across eight production-grade LLMs on two function level benchmarks (HumanEvalPack and MBPP), comparing several summary styles against a direct repair baseline. Error-aware diagnostic summaries consistently yield the largest gains - repairing up to 65% of unseen errors, on average of 5% more than the baseline - though overall improvements are modest and LLM-dependent. Our results position summaries as a cheap, human-interpretable diagnostic artefact that can be integrated into program-repair pipelines rather than a stand-alone fix-all.

</details>


### [35] [Optimizing LLM Code Suggestions: Feedback-Driven Timing with Lightweight State Bounds](https://arxiv.org/abs/2511.18842)
*Mohammad Nour Al Awad,Sergey Ivanov,Olga Tikhonova*

Main category: cs.SE

TL;DR: 提出了一种自适应延迟机制，根据开发者实时反馈动态调整代码补全建议的呈现时机，在提升建议接受率的同时显著减少无效推理调用。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型（LLM）驱动的代码自动补全系统在何时展示建议方面缺乏有效策略，常导致打断开发者或浪费计算资源。

Method: 结合近期建议接受率的逻辑变换、有界延迟范围以及对开发者认知状态的高层二元预测，构建自适应延迟机制。

Result: 在为期两个月的真实部署中，建议接受率从无延迟时的4.9%提升至自适应延迟下的18.6%，同时“未读即拒”比例从8.3%降至0.36%，无效推理调用减少75%。

Conclusion: 所提自适应时机机制显著提升了LLM代码助手的效率与成本效益，兼顾用户体验与系统资源利用。

Abstract: Large Language Models (LLMs) have transformed code auto-completion by generating context-aware suggestions. Yet, deciding when to present these suggestions remains underexplored, often leading to interruptions or wasted inference calls. We propose an adaptive timing mechanism that dynamically adjusts the delay before offering a suggestion based on real-time developer feedback. Our suggested method combines a logistic transform of recent acceptance rates with a bounded delay range, anchored by a high-level binary prediction of the developer's cognitive state. In a two-month deployment with professional developers, our system improved suggestion acceptance from 4.9% with no delay to 15.4% with static delays, and to 18.6% with adaptive timing-while reducing blind rejections (rejections without being read) from 8.3% to 0.36%. Together, these improvements increase acceptance and substantially reduce wasted inference calls by 75%, making LLM-based code assistants more efficient and cost-effective in practice.

</details>


### [36] [Pre-Filtering Code Suggestions using Developer Behavioral Telemetry to Optimize LLM-Assisted Programming](https://arxiv.org/abs/2511.18849)
*Mohammad Nour Al Awad,Sergey Ivanov,Olga Tikhonova*

Main category: cs.SE

TL;DR: 本文提出一种轻量级预过滤模型，利用开发者实时行为信号（如打字速度、文件导航和编辑活动）预测代码建议的接受概率，在不查看代码或提示内容的前提下，显著提升大语言模型代码建议的接受率并减少无效调用。


<details>
  <summary>Details</summary>
Motivation: 当前集成在代码编辑器中的大语言模型（LLM）生成的代码建议常被用户忽略，造成计算资源浪费、延迟增加和不必要的干扰，亟需一种高效机制提前判断建议是否值得生成。

Method: 开发一个轻量级预过滤模型，仅基于调用LLM前的实时编辑器遥测数据（如打字速度、文件导航和编辑活动）预测建议被接受的可能性，并在Visual Studio Code插件中部署验证。

Result: 在为期四个月的真实使用环境中，该方法将代码建议接受率从18.4%提升至34.2%，同时减少了35%的低价值LLM调用。

Conclusion: 仅依靠开发者行为信号即可有效提升LLM辅助编程的用户体验与系统效率，表明时机感知且保护隐私的适配机制具有重要价值。

Abstract: Large Language Models (LLMs) are increasingly integrated into code editors to provide AI-powered code suggestions. Yet many of these suggestions are ignored, resulting in wasted computation, increased latency, and unnecessary interruptions. We introduce a lightweight pre-filtering model that predicts the likelihood of suggestion acceptance before invoking the LLM, using only real-time developer telemetry such as typing speed, file navigation, and editing activity. Deployed in a production-grade Visual Studio Code plugin over four months of naturalistic use, our approach nearly doubled acceptance rates (18.4% -> 34.2%) while suppressing 35% of low-value LLM calls. These findings demonstrate that behavioral signals alone can meaningfully improve both user experience and system efficiency in LLM-assisted programming, highlighting the value of timing-aware, privacy-preserving adaptation mechanisms. The filter operates solely on pre-invocation editor telemetry and never inspects code or prompts.

</details>


### [37] [Time Travel: LLM-Assisted Semantic Behavior Localization with Git Bisect](https://arxiv.org/abs/2511.18854)
*Yujing Wang,Weize Hong*

Main category: cs.SE

TL;DR: 本文提出了一种将大语言模型（LLM）融入 Git bisect 过程的新框架，用于语义故障定位，在多个开源项目中显著提升了成功率并减少了平均 bisect 时间。


<details>
  <summary>Details</summary>
Motivation: 传统 Git bisect 方法依赖确定性谓词和二元失败状态，但在现代软件开发中常因测试不稳定、非单调回归和语义分歧而失效，因此需要更鲁棒的语义故障定位方法。

Method: 通过结构化思维链推理增强 bisect 遍历过程，使用弱监督流程减少标注开销，并基于人工校正与自一致性过滤对 DeepSeekCoderV2 模型进行 QLoRA 微调。

Result: 在多个开源项目上的实验表明，该方法将 bisect 成功率从 74.2% 提升至 80.6%，最多可将平均 bisect 时间减少一半。

Conclusion: 该研究验证了 LLM 在 commit 级别行为分析中的有效性，并讨论了适用于此任务的时间推理、提示设计和微调策略。

Abstract: We present a novel framework that integrates Large Language Models (LLMs) into the Git bisect process for semantic fault localization. Traditional bisect assumes deterministic predicates and binary failure states assumptions often violated in modern software development due to flaky tests, nonmonotonic regressions, and semantic divergence from upstream repositories. Our system augments bisect traversal with structured chain of thought reasoning, enabling commit by commit analysis under noisy conditions. We evaluate multiple open source and proprietary LLMs for their suitability and fine tune DeepSeekCoderV2 using QLoRA on a curated dataset of semantically labeled diffs. We adopt a weak supervision workflow to reduce annotation overhead, incorporating human in the loop corrections and self consistency filtering. Experiments across multiple open source projects show a 6.4 point absolute gain in success rate from 74.2 to 80.6 percent, leading to significantly fewer failed traversals and by experiment up to 2x reduction in average bisect time. We conclude with discussions on temporal reasoning, prompt design, and finetuning strategies tailored for commit level behavior analysis.

</details>


### [38] [VecIntrinBench: Benchmarking Cross-Architecture Intrinsic Code Migration for RISC-V Vector](https://arxiv.org/abs/2511.18867)
*Liutong Han,Chu Kang,Mingjie Xing,Yanjun Wu*

Main category: cs.SE

TL;DR: 本文提出了VecIntrinBench，这是首个支持RISC-V Vector（RVV）扩展的内建函数基准测试集，用于评估跨架构内建函数迁移能力，并通过实验比较了基于规则和大语言模型（LLM）的迁移方法，发现先进LLM在效果和性能上具有优势。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏针对RISC-V Vector（RVV）架构的内建函数迁移能力评估基准，阻碍了高性能算法库在RISC-V生态中的迁移与适配。

Method: 构建包含50个函数级任务的基准测试集VecIntrinBench，涵盖标量、RVV、Arm Neon和x86内建函数实现，并设计功能与性能测试用例；在此基础上系统评估基于规则和基于大语言模型的代码迁移方法。

Result: 实验表明，先进的大语言模型在RISC-V内建函数迁移任务中能达到与基于规则方法相当的正确性，并在性能方面表现更优；同时揭示了LLM在该领域的改进方向。

Conclusion: VecIntrinBench填补了RVV内建函数迁移评估的空白，为社区提供了有效工具；研究结果支持LLM在跨架构代码迁移中的应用潜力，并为未来研究指明方向。

Abstract: Intrinsic functions are specialized functions provided by the compiler that efficiently operate on architecture-specific hardware, allowing programmers to write optimized code in a high-level language that fully exploits hardware features. Using intrinsics to vectorize core code blocks is a standard optimization method in high-performance libraries, often requiring specific vector optimization implementations for multiple mainstream architectures. The promising RISC-V software ecosystem has a significant demand for algorithm library migration and adaptation. Translating existing intrinsic functions to RISC-V Vector (RVV) intrinsic functions across architectures is currently a mainstream approach. Rule-based intrinsic mapping methods and LLM-based code generation can help developers address the code migration challenge. However, existing intrinsic code benchmarks focus on mainstream SIMD intrinsics and lack support for the emerging RISC-V architecture. There is currently no benchmark that comprehensively evaluates the intrinsic migration capabilities for the RVV extension. To fill this gap, we propose VecIntrinBench, the first intrinsic benchmark encompassing RVV extensions. It includes 50 function-level tasks from open source repositories, implemented as scalars, RVV intrinsics, Arm Neon intrinsics, and x86 intrinsics, along with comprehensive functional and performance test cases. We systematically evaluated various code migration approaches on VecIntrinBench, yielding a series of insightful findings. The results demonstrate that advanced Large Language Models (LLMs) achieve a similar effect as rule-based mapping approaches for RISC-V code migration, while also delivering superior performance. We further analyze the reasons and identify future directions for LLM development in the code migration field. The VecIntrinBench is open-sourced to benefit the broader community and developers.

</details>


### [39] [Optimization-Aware Test Generation for Deep Learning Compilers](https://arxiv.org/abs/2511.18918)
*Qingchao Shen,Zan Wang,Haoyang Ma,Yongqiang Tian,Lili Huang,Zibo Xiao,Junjie Chen,Shing-Chi Cheung*

Main category: cs.SE

TL;DR: 本文提出OATest，一种用于生成面向优化的计算图的新方法，通过结合文档测试中的模式与种子图，并采用边重用和辅助层添加策略，有效提升对深度学习编译器（如TVM和ONNXRuntime）的测试覆盖与缺陷检测能力。


<details>
  <summary>Details</summary>
Motivation: 现有测试方法难以有效覆盖深度学习编译器的核心优化阶段，因其缺乏生成“优化感知”测试用例的能力，从而影响编译器的可靠性与安全性验证。

Method: OATest从已有文档化测试中提取优化模式，将其融合到种子计算图中；通过边重用策略增强模式与上下文的关联以确保优化感知性，并采用辅助层添加策略解决生成图的有效性问题；最后利用两个差异测试预言对TVM和ONNXRuntime进行评估。

Result: 实验表明，OATest在代码覆盖率和缺陷发现数量上优于现有最先进方法，在TVM和ONNXRuntime中发现了58个未知漏洞，其中36个已被开发者确认或修复。

Conclusion: OATest能有效生成优化感知的计算图，显著提升深度学习编译器测试的广度与深度，为保障其可靠性与安全性提供了有力支持。

Abstract: Deep Learning (DL) compilers have been widely utilized to optimize DL models for efficient deployment across various hardware. Due to their vital role in the DL ecosystem, ensuring their reliability and security is critical. However, existing approaches have limitations in testing optimization stages, which is the core functionality of DL compilers, due to the difficulty in generating optimization-aware tests. In this paper, we proposed OATest, a novel approach for synthesizing optimization-aware computational graphs. The approach combines patterns extracted from documented tests for optimization and incorporates them into seed computational graphs, enabling broader exploration of optimization paths. To guarantee the optimization-awareness of generated graphs, OATest introduces the edges reusing strategy to establish strong connections between patterns and contexts. Additionally, to solve the validity challenge for the generated graphs, OATest employs an auxiliary layers addition strategy to resolve broken constraints. Equipped with two distinct test oracles, OATest applies differential testing to evaluate the two widely used DL compilers (i.e., TVM and ONNXRuntime). Our experimental results show that OATest outperforms the state-of-the-art method by detecting more bugs and achieving higher code coverage in TVM and ONNXRutimes. Additionally, OATest uncovers 58 previously unknown bugs, 36 of which have been confirmed or fixed by developers.

</details>


### [40] [LLM-Driven Kernel Evolution: Automating Driver Updates in Linux](https://arxiv.org/abs/2511.18924)
*Arina Kharlamova,Jiawen Liu,Tianyi Zhang,Xinrui Yang,Humaid Alqasimi,Youcheng Sun,Chun Jason Xue*

Main category: cs.SE

TL;DR: 本文提出了DRIVEBENCH（一个包含Linux内核与驱动协同演化案例的可执行语料库）和AUTODRIVER（一个基于大语言模型的闭环驱动维护自动化系统），通过提示工程、多智能体协作、静态分析和迭代验证，实现对因内核更新而失效的驱动程序的自动修复。


<details>
  <summary>Details</summary>
Motivation: Linux内核的持续演进通过API/ABI变更、语义变化和安全强化更新导致驱动程序失效，亟需自动化手段支持驱动的持续维护与协同演化。

Method: 构建包含235个已验证案例的DRIVEBENCH语料库（覆盖内核v5.10–v6.10），并开发AUTODRIVER系统，该系统结合大语言模型、提示工程、多智能体协作、静态分析和迭代验证，自动生成符合内核规范的驱动补丁。

Result: 在55个测试案例中，AUTODRIVER达到56.4%的编译成功率；基于QEMU的启动验证表明，大多数成功编译的补丁能正确保留驱动初始化功能。

Conclusion: DRIVEBENCH和AUTODRIVER为驱动程序的自动化维护提供了可复现的研究基础和实用工具，有助于实现驱动与Linux内核的安全、持续协同演化。

Abstract: Linux kernel evolution breaks drivers through API/ABI changes, semantic shifts, and security-hardening updates. We introduce DRIVEBENCH, an executable corpus of kernel$\rightarrow$driver co-evolution cases, and AUTODRIVER, a closed-loop, LLM-driven system for automating driver maintenance. The system integrates prompt engineering, multi-agent collaboration, static analysis, and iterative validation to ensure that generated patches are not only syntactically correct but also functionally and semantically consistent with kernel conventions. The corpus spans v5.10-v6.10 with 235 validated cases drawn from 612 candidates. In evaluation across 55 cases, AUTODRIVER achieves 56.4% compilation success; QEMU-based boot verification indicates that compiled patches preserve driver initialization in most instances. By releasing DRIVEBENCH and tooling, we enable reproducible research and a practical route to continuous, safe co-evolution of drivers with the Linux kernel.

</details>


### [41] [LLMAID: Identifying AI Capabilities in Android Apps with LLMs](https://arxiv.org/abs/2511.19059)
*Pei Liu,Terry Zhuo,Jiawei Deng,Thong James,Shidong Pan,Sherry Xu,Zhenchang Xing,Qinghua Lu,Xiaoning Du,Hongyu Zhang*

Main category: cs.SE

TL;DR: 本文提出LLMAID，一种基于大语言模型的自动化方法，用于高效识别和总结移动应用中的AI功能，在准确率和召回率上均超过90%，并发现计算机视觉是当前Android应用中最主要的AI能力。


<details>
  <summary>Details</summary>
Motivation: 现有识别移动应用中AI能力的方法依赖人工检查和基于规则的启发式方法，成本高、耗时长且难以适应先进AI技术，亟需更高效、自动化的解决方案。

Method: 提出LLMAID框架，包含四个任务：候选提取、知识库交互、AI能力分析与检测、AI服务摘要生成，并利用大语言模型实现对Android应用中AI组件的自动识别与总结。

Result: 在4,201个Android应用上的实验表明，LLMAID比现有基于规则的方法多识别242%的真实AI应用，检测精度和召回率均超90%；用户研究表明开发者更偏好其生成的AI服务摘要；实证分析显示54.80%的AI功能集中在计算机视觉，其中目标检测最常见（25.19%）。

Conclusion: LLMAID有效解决了现有方法的局限性，为大规模识别和理解移动应用中的AI能力提供了高效、准确的自动化工具，有助于提升透明度并支持监管与开发实践。

Abstract: Recent advancements in artificial intelligence (AI) and its widespread integration into mobile software applications have received significant attention, highlighting the growing prominence of AI capabilities in modern software systems. However, the inherent hallucination and reliability issues of AI continue to raise persistent concerns. Consequently, application users and regulators increasingly ask critical questions such as: Does the application incorporate AI capabilities? and What specific types of AI functionalities are embedded? Preliminary efforts have been made to identify AI capabilities in mobile software; however, existing approaches mainly rely on manual inspection and rule-based heuristics. These methods are not only costly and time-consuming but also struggle to adapt advanced AI techniques.
  To address the limitations of existing methods, we propose LLMAID (Large Language Model for AI Discovery). LLMAID includes four main tasks: (1) candidate extraction, (2) knowledge base interaction, (3) AI capability analysis and detection, and (4) AI service summarization. We apply LLMAID to a dataset of 4,201 Android applications and demonstrate that it identifies 242% more real-world AI apps than state-of-the-art rule-based approaches. Our experiments show that LLM4AID achieves high precision and recall, both exceeding 90%, in detecting AI-related components. Additionally, a user study indicates that developers find the AI service summaries generated by LLMAID to be more informative and preferable to the original app descriptions. Finally, we leverage LLMAID to perform an empirical analysis of AI capabilities across Android apps. The results reveal a strong concentration of AI functionality in computer vision (54.80%), with object detection emerging as the most common task (25.19%).

</details>


### [42] [LLMs-Powered Real-Time Fault Injection: An Approach Toward Intelligent Fault Test Cases Generation](https://arxiv.org/abs/2511.19132)
*Mohammad Abboush,Ahmad Hatahet,Andreas Rausch*

Main category: cs.SE

TL;DR: 本文提出了一种基于大语言模型（LLM）的故障测试用例自动生成方法，用于汽车软件系统的实时故障注入测试，显著提升了测试效率与安全性。


<details>
  <summary>Details</summary>
Motivation: 当前故障注入（FI）测试方法在识别故障类型、位置和时间等属性时依赖人工操作，过程耗时、昂贵且劳动密集，尤其在复杂系统中问题更为突出。

Method: 利用大语言模型（特别是gpt-4o）从功能安全需求（FSRs）中自动生成具有代表性和覆盖性的故障测试用例，并在硬件在环系统中进行实时验证。

Result: 所提方法在FSR分类和故障测试用例生成方面分别达到了88%和97.5%的F1分数，并在高保真汽车系统模型上成功实施了实时测试。

Conclusion: 该方法有效优化了汽车软件系统的实时故障注入测试流程，在降低成本的同时提升了复杂安全关键系统的安全性。

Abstract: A well-known testing method for the safety evaluation and real-time validation of automotive software systems (ASSs) is Fault Injection (FI). In accordance with the ISO 26262 standard, the faults are introduced artificially for the purpose of analyzing the safety properties and verifying the safety mechanisms during the development phase. However, the current FI method and tools have a significant limitation in that they require manual identification of FI attributes, including fault type, location and time. The more complex the system, the more expensive, time-consuming and labour-intensive the process. To address the aforementioned challenge, a novel Large Language Models (LLMs)-assisted fault test cases (TCs) generation approach for utilization during real-time FI tests is proposed in this paper. To this end, considering the representativeness and coverage criteria, the applicability of various LLMs to create fault TCs from the functional safety requirements (FSRs) has been investigated. Through the validation results of LLMs, the superiority of the proposed approach utilizing gpt-4o in comparison to other state-of-the-art models has been demonstrated. Specifically, the proposed approach exhibits high performance in terms of FSRs classification and fault TCs generation with F1-score of 88% and 97.5%, respectively. To illustrate the proposed approach, the generated fault TCs were executed in real time on a hardware-in-the-loop system, where a high-fidelity automotive system model served as a case study. This novel approach offers a means of optimizing the real-time testing process, thereby reducing costs while simultaneously enhancing the safety properties of complex safety-critical ASSs.

</details>


### [43] [Synthesizing Test Cases for Narrowing Specification Candidates](https://arxiv.org/abs/2511.19177)
*Alcino Cunha,Nuno Macedo*

Main category: cs.SE

TL;DR: 本文提出了一种基于测试用例生成的技术，用于从多个候选形式化规约中选出最优的一个；通过用户对生成测试用例的反馈，可将候选集缩小至最多一个规约，并实现了两种求解器驱动的算法（最优与非最优），实验表明两者在实际问题中均具实用性。


<details>
  <summary>Details</summary>
Motivation: 在形式化方法实践中，常常存在多个语义相近但行为不同的规约候选，如何从中选择最符合用户意图的一个是一个关键挑战。现有方法缺乏有效机制来引导用户在多个候选之间做出准确选择。

Method: 提出两种基于求解器的算法：一种生成最小测试套件，另一种不保证最小性但可扩展性更强；通过用户对测试用例是否“可接受”的分类反馈，逐步排除不符合预期的规约候选，最终收敛到至多一个候选。

Result: 原型工具在大量问题上的评估表明，最优算法对许多实际问题足够高效，而非最优算法可处理多达数十个候选规约，同时仍生成规模合理的测试套件。

Conclusion: 该技术为在多个形式化规约之间进行选择提供了一种实用且可扩展的解决方案，尤其适用于Alloy等轻量级形式化规约语言的场景。

Abstract: This paper proposes a technique to help choose the best formal specification candidate among a set of alternatives. Given a set of specifications, our technique generates a suite of test cases that, once classified by the user as desirable or not, narrows down the set of candidates to at most one specification. Two alternative solver-based algorithms are proposed, one that generates a minimal test suite, and another that does not ensure minimality. Both algorithms were implemented in a prototype that can be used generate test suites to help choose among alternative Alloy specifications. Our evaluation of this prototype against a large set of problems showed that the optimal algorithm is efficient enough for many practical problems, and that the non-optimal algorithm can scale up to dozens of candidate specifications while still generating reasonably sized test suites.

</details>


### [44] [SLMFix: Leveraging Small Language Models for Error Fixing with Reinforcement Learning](https://arxiv.org/abs/2511.19422)
*David Jiahao Fu,Aryan Gupta,Aaron Councilman,David Grove,Yu-Xiong Wang,Vikram Adve*

Main category: cs.SE

TL;DR: 本文提出SLMFix，一种利用强化学习微调的小语言模型（SLM）来修复大语言模型（LLM）生成代码中的语法错误的新方法，显著提升了在低资源领域特定语言（DSL）上的代码生成质量。


<details>
  <summary>Details</summary>
Motivation: 大语言模型（LLM）在代码生成中常产生语法错误，尤其在低资源编程语言上表现不佳；同时，传统微调成本高昂，难以在资源受限条件下有效应用。

Method: 提出SLMFix框架，使用强化学习对小语言模型（SLM）进行微调，通过结合静态验证器和静态语义相似度指标计算奖励信号，用于修复LLM生成代码中的语法错误。

Result: 实验表明，SLMFix在多个领域特定语言（DSL）上具有良好的泛化能力，在静态验证器上的通过率超过95%，且优于监督微调方法，即使在7B规模模型和低资源语言上也表现优异。

Conclusion: SLMFix为提升LLM在低资源DSL上的代码生成质量提供了一种高效、低成本的替代方案，展示了强化学习在程序修复任务中的潜力。

Abstract: Recent advancements in large language models (LLMs) have shown very impressive capabilities in code generation across many programming languages. However, even state-of-the-art LLMs generate programs that contains syntactic errors and fail to complete the given tasks, especially for low-resource programming languages (LRPLs). In addition, high training cost makes finetuning LLMs unaffordable with constrained computational resources, further undermining the effectiveness of LLMs for code generation. In this work, we propose SLMFix, a novel code generation pipeline that leverages a small language model (SLM) finetuned using reinforcement learning (RL) techniques to fix syntactic errors in LLM-generated programs to improve the quality of LLM-generated programs for domain-specific languages (DSLs). In specific, we applied RL on the SLM for the program repair task using a reward calculated using both a static validator and a static semantic similarity metric. Our experimental results demonstrate the effectiveness and generalizability of our approach across multiple DSLs, achieving more than 95% pass rate on the static validator. Notably, SLMFix brings substantial improvement to the base model and outperforms supervised finetuning approach even for 7B models on a LRPL, showing the potential of our approach as an alternative to traditional finetuning approaches.

</details>


### [45] [Prompt Less, Smile More: MTP with Semantic Engineering in Lieu of Prompt Engineering](https://arxiv.org/abs/2511.19427)
*Jayanaka L. Dantanarayana,Savini Kashmira,Thakee Nathees,Zichen Zhang,Krisztian Flautner,Lingjia Tang,Jason Mars*

Main category: cs.SE

TL;DR: 本文提出了一种名为“语义工程”的轻量级方法，通过在程序中嵌入自然语言上下文（SemTexts）来增强代码语义，使基于大语言模型的系统能更准确地反映开发者意图，显著提升提示生成质量，同时减少人工提示设计的工作量。


<details>
  <summary>Details</summary>
Motivation: 现有基于代码静态语义的方法（如MTP）无法充分表达真实应用场景中的上下文线索、开发者意图和领域特定推理，限制了AI集成编程的效果。

Method: 引入语义工程方法，在Jac编程语言中实现语义上下文注解（SemTexts），允许开发者将自然语言上下文直接嵌入程序结构，并将其整合进MTP框架用于提示生成。

Result: 实验表明，语义工程显著提升了提示保真度，在性能上接近传统提示工程，但所需开发者工作量大幅减少。

Conclusion: 语义工程是一种高效且实用的方法，能够在不依赖完整手动提示设计的前提下，提升AI集成编程中LLM对开发者意图的理解与表达能力。

Abstract: AI-Integrated programming is emerging as a foundational paradigm for building intelligent systems with large language models (LLMs). Recent approaches such as Meaning Typed Programming (MTP) automate prompt generation by leveraging the semantics already present in code. However, many real-world applications depend on contextual cues, developer intent, and domain-specific reasoning that extend beyond what static code semantics alone can express. To address this limitation, we introduce Semantic Engineering, a lightweight method for enriching program semantics so that LLM-based systems can more accurately reflect developer intent without requiring full manual prompt design. We present Semantic Context Annotations (SemTexts), a language-level mechanism that allows developers to embed natural-language context directly into program constructs. Integrated into the Jac programming language, Semantic Engineering extends MTP to incorporate these enriched semantics during prompt generation. We further introduce a benchmark suite designed to reflect realistic AI-Integrated application scenarios. Our evaluation shows that Semantic Engineering substantially improves prompt fidelity, achieving performance comparable to Prompt Engineering while requiring significantly less developer effort.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [46] [Pier: Efficient Large Language Model pretraining with Relaxed Global Communication](https://arxiv.org/abs/2511.17849)
*Shuyuan Fan,Zhao Zhang*

Main category: cs.DC

TL;DR: Pier 是一种高效且可扩展的优化器，通过减少全局通信开销，在保持模型性能的同时显著加速大语言模型（如 GPT 系列）的预训练过程。


<details>
  <summary>Details</summary>
Motivation: 大语言模型预训练中的全局通信（如 all-reduce 和 allgather）是主要性能瓶颈，亟需更高效的优化策略来降低通信开销并提升训练速度。

Method: Pier 基于 DiLoCo 框架，在处理器组内使用内层优化器，在需要全局通信时使用外层优化器，并引入动量预热（momentum warmup）和动量衰减（momentum decay）技术以保持收敛性和模型性能。同时采用高效可扩展的系统架构支持复杂并行策略。

Result: 在 GPT-2 XL 模型上，Pier 在 256 块 A100 GPU 上实现 2.7x–3.7x 的训练加速，在 64 块 GH200 Superchips 上实现 1.2x–1.9x 加速；在 GPT-2 7B 模型上结合数据并行与张量并行，训练时间减少 54.5%，且未损失验证损失或下游任务性能。

Conclusion: Pier 能有效缓解大语言模型预训练中的全局通信瓶颈，在不牺牲模型质量的前提下显著提升训练效率，具备良好的可扩展性与实用性。

Abstract: Global communication, such as all-reduce and allgather, is the prominent performance bottleneck in large language model (LLM) pretraining. To address this issue, we present Pier, an efficient and scalable optimizer with relaxed global communication. Pier is built upon DiLoCo, which leverages an inner optimizer within groups of processors and an outer optimizer that requires global communication. To preserve the convergence and model performance, Pier incorporates two key techniques for the outer optimizer: momentum warmup and momentum decay. Pier employs an efficient and scalable system architecture to enable complex parallelization strategies in LLM pretraining. We examine the model performance and runtime reduction of Pier using the GPT model family (e.g., small, medium, XL, and 7B) and the OpenWebText dataset with a suite of thirteen downstream tasks. With data parallel strategy, Pier speeds up GPT-2 XL training by up to 2.7x-3.7x on 256 NVIDIA A100 GPUs and 1.2x-1.9x on 64 GH200 Superchips, respectively, without degradation of validation loss or downstream task performance. With data parallel and tensor parallel, Pier reduces the time cost GPT-2 7B model training by 54.5% on 128 A100s.

</details>


### [47] [SAGkit: A Python SAG Toolkit for Response Time Analysis of Hybrid-Triggered Jobs](https://arxiv.org/abs/2511.17882)
*Ruide Cao,Zhuyun Qi,Qinyang He,Chenxi Ling,Yi Wang,Guoming Tang*

Main category: cs.DC

TL;DR: 本文提出了SAGkit，一个基于调度抽象图（SAG）框架的Python工具包，用于对混合触发任务进行精确且可持续的响应时间分析（RTA），有效应对传统RTA方法在非抢占式系统中因状态空间爆炸带来的挑战。


<details>
  <summary>Details</summary>
Motivation: 现代分布式控制系统中的延迟敏感型应用对实时性保证和鲁棒性要求越来越高，而传统响应时间分析方法在处理具有释放抖动和执行时间变化的非抢占式系统时面临状态空间爆炸问题。

Method: 引入SAGkit工具包，基于调度抽象图（SAG）框架，通过允许在SAG基础上缺失任务实例，实现对混合触发任务的精确响应时间分析。

Result: 实验表明，SAGkit在保持可接受的运行时间和内存开销的同时，实现了精确的响应时间分析。

Conclusion: SAGkit是一个轻量级、开源的工具包，能够支持研究人员分析复杂的分布式控制系统，并为后续开发提供基础。

Abstract: For distributed control systems, modern latency-critical applications are increasingly demanding real-time guarantees and robustness. Response-time analysis (RTA) is useful for this purpose, as it helps analyze and guarantee timing bounds. However, conventional RTA methods struggle with the state-space explosion problem, especially in non-preemptive systems with release jitter and execution time variations. In this paper, we introduce SAGkit, a Python toolkit that implements the schedule-abstraction graph (SAG) framework. SAGkit novelly enables exact and sustainable RTA of hybrid-triggered jobs by allowing job absence on the SAG basis. Our experiments demonstrate that SAGkit achieves exactness with acceptable runtime and memory overhead. This lightweight toolkit empowers researchers to analyze complex distributed control systems and is open-access for further development.

</details>


### [48] [MIDAS: Adaptive Proxy Middleware for Mitigating Metadata Hotspots in HPC I/O at Scale](https://arxiv.org/abs/2511.18124)
*Sangam Ghimire,Nigam Niraula,Nirjal Bhurtel,Paribartan Timalsina,Bishal Neupane,James Bhattarai,Sudan Jha*

Main category: cs.DC

TL;DR: MIDAS 是一种自适应中间件，通过透明部署在客户端与元数据服务器之间，结合命名空间感知负载均衡、协作缓存和自稳定控制环，在不修改内核或存储后端的前提下显著缓解元数据热点问题。


<details>
  <summary>Details</summary>
Motivation: 元数据热点是高性能计算（HPC）和云存储环境中可扩展 I/O 的主要障碍，现有方法如静态命名空间划分、后端特定扩展或内核级修改往往过于僵化、侵入性强或在动态负载下不稳定。

Method: MIDAS 采用三种机制：(i) 基于实时遥测信息增强一致性哈希的命名空间感知负载均衡器；(ii) 通过租约、失效或自适应超时维持后端语义的协作缓存层；(iii) 动态调节路由激进程度和缓存寿命的自稳定控制环。

Result: 实验表明，相比轮询调度，MIDAS 平均队列长度减少约 23%，最坏情况热点降低高达 80%。

Conclusion: MIDAS 展示了一种稳定性感知、中间件驱动的策略可在不依赖特定后端的情况下提升元数据管理效率，从而在突发负载场景中实现更好的可扩展性、更可预测的尾延迟和更强的整体性能。

Abstract: Metadata hotspots remain one of the key obstacles to scalable Input/Output (I/O) in both High-Performance Computing (HPC) and cloud-scale storage environments. Situations such as job start-ups, checkpoint storms, or heavily skewed namespace access can trigger thousands of concurrent metadata requests against a small subset of servers. The result is long queues, inflated tail latencies, and reduced system throughput. Prior efforts including static namespace partitioning, backend-specific extensions, and kernel-level modifications address parts of the problem, but they often prove too rigid, intrusive to deploy, or unstable under shifting workloads. We present MIDAS, an adaptive middleware layer that operates transparently between clients and metadata servers, requiring no changes to kernels or storage backends. The design brings together three mechanisms: (i) a namespace-aware load balancer that enhances consistent hashing with power-of-d sampling informed by live telemetry, (ii) a cooperative caching layer that preserves backend semantics through leases, invalidations, or adaptive timeouts, and (iii) a self-stabilizing control loop that dynamically adjusts routing aggressiveness and cache lifetimes while avoiding oscillations under bursty workloads. Analysis of the model and controlled experiments show that MIDAS reduces average queue lengths by roughly 23% and mitigates worst-case hotspots by up to 80% when compared to round-robin scheduling. These findings highlight that a stability-aware, middleware-based strategy can provide backend-agnostic improvements to metadata management, enabling better scalability in bursty scenarios, more predictable tail latencies, and stronger overall system performance.

</details>


### [49] [Simulating Dynamic Cloud Marketspaces: Modeling Spot Instance Behavior and Scheduling with CloudSim Plus](https://arxiv.org/abs/2511.18137)
*Christoph Goldgruber,Benedikt Pittl,Erich Schikuta*

Main category: cs.DC

TL;DR: 本文扩展了CloudSim Plus仿真框架以支持真实的竞价实例生命周期管理，并在此基础上改进了HLEM-VMP虚拟机分配算法，验证其在动态云环境中减少中断次数与持续时间的有效性。


<details>
  <summary>Details</summary>
Motivation: 当前公有云中广泛采用的动态定价模型（如竞价实例）带来了成本优势，但也引入了现有调度算法和仿真工具未能充分应对的不确定性和波动性。

Method: 扩展CloudSim Plus仿真框架以支持竞价实例的中断、终止、休眠和重新分配等生命周期行为；基于Google Cluster Trace数据集进行大规模仿真实验；改进并评估HLEM-VMP算法在动态竞价市场条件下的性能。

Result: 改进后的HLEM-VMP算法相比基线策略显著减少了竞价实例中断次数和最大中断持续时间。

Conclusion: 本研究不仅提供了模拟动态云环境的有效仿真框架，还为虚拟机分配策略在成本效益与可靠性之间的权衡提供了分析依据，有助于提升云资源管理的鲁棒性。

Abstract: The increasing reliance on dynamic pricing models, such as spot instances, in public cloud environments presents new challenges for workload scheduling and reliability. While these models offer cost advantages, they introduce volatility and uncertainty that are not fully addressed by current allocation algorithms or simulation tools. This work contributes to the modeling and evaluation of such environments by extending the CloudSim Plus simulation framework to support realistic spot instance lifecycle management, including interruption, termination, hibernation, and reallocation. The enhanced simulator is validated using synthetic scenarios and large-scale simulations based on the Google Cluster Trace dataset. Building on this foundation, the HLEM-VMP allocation algorithm, originally proposed in earlier research, was adapted to operate under dynamic spot market conditions. Its performance was evaluated against baseline allocation strategies to assess its efficiency and resilience in volatile workload environments. The comparison demonstrated a reduction in the number of spot instance interruptions as well as a decrease in the maximum interruption duration. Overall, this work provides both a simulation framework for simulating dynamic cloud behavior and analytical insights into virtual machine allocation performance and market risk, contributing to more robust and cost-effective resource management in cloud computing.

</details>


### [50] [AVERY: Adaptive VLM Split Computing through Embodied Self-Awareness for Efficient Disaster Response Systems](https://arxiv.org/abs/2511.18151)
*Rajat Bhattacharjya,Sing-Yao Wu,Hyunwoo Oh,Chaewon Nam,Suyeon Koo,Mohsen Imani,Elaheh Bozorgzadeh,Nikil Dutt*

Main category: cs.DC

TL;DR: AVERY is an adaptive split-computing framework that enables efficient deployment of Vision-Language Models (VLMs) on UAVs in disaster response by splitting VLM processing into dual streams—context and insight—and dynamically managing compression based on network conditions and operator intent, achieving higher accuracy and lower energy use.


<details>
  <summary>Details</summary>
Motivation: UAVs in disaster zones need queryable semantic intelligence beyond what on-board CNNs offer, but VLMs are too resource-intensive for on-device use and cloud offloading struggles under low-bandwidth conditions.

Method: AVERY introduces a cognitive-inspired dual-stream split computing approach: a high-frequency, low-resolution “context stream” for real-time awareness and a low-frequency, high-fidelity “insight stream” for deep analysis, managed by a lightweight on-board controller that adapts compression models based on network state and user intent.

Result: In edge-cloud evaluations with LISA-7B under variable networks, AVERY achieved 11.2% higher accuracy than raw image compression and 93.98% lower energy consumption versus full-edge execution.

Conclusion: AVERY effectively balances accuracy and efficiency to enable real-time, queryable VLM intelligence on resource-constrained UAVs operating in dynamic, low-bandwidth disaster environments.

Abstract: Unmanned Aerial Vehicles (UAVs) in disaster response require complex, queryable intelligence that on-board CNNs cannot provide. While Vision-Language Models (VLMs) offer this semantic reasoning, their high resource demands make on-device deployment infeasible, and naive cloud offloading fails under the low-bandwidth networks common in disaster zones. We present AVERY, a framework that enables VLM deployment through adaptive split computing. We advance the split computing paradigm beyond traditional depth-wise partitioning by introducing a functional, cognitive-inspired dual-stream split that separates the VLM into a high-frequency, low-resolution "context stream" for real-time awareness and a low-frequency, high-fidelity "insight stream" for deep analysis. A lightweight, self-aware on-board controller manages this architecture, monitoring network conditions and operator intent to dynamically select from pre-trained compression models, navigating the fundamental accuracy-throughput trade-off. Evaluated using the VLM LISA-7B across an edge-cloud scenario under fluctuating network conditions, AVERY consistently outperforms static configurations, achieving 11.2% higher accuracy than raw image compression and 93.98% lower energy consumption compared to full-edge execution, thereby enhancing mission efficiency and enabling real-time, queryable intelligence on resource-constrained platforms in dynamic environments.

</details>


### [51] [An Online Fragmentation-Aware GPU Scheduler for Multi-Tenant MIG-based Clouds](https://arxiv.org/abs/2511.18906)
*Marco Zambianco,Lorenzo Fasol,Roberto Doriguzzi-Corin*

Main category: cs.DC

TL;DR: 本文提出了一种面向MIG（多实例GPU）云环境的新型调度框架，通过引入碎片度量指标和贪心算法，在线优化GPU资源分配，有效减少碎片并提升工作负载接纳率。


<details>
  <summary>Details</summary>
Motivation: MIG虽能提供强隔离性和安全性，但其固定分区方式在多租户动态负载场景下易导致严重的GPU资源碎片，造成资源利用率低下，限制可调度工作负载数量。

Method: 作者设计了一个碎片度量指标，并基于该指标构建了一个贪心调度算法，在工作负载到达时选择使碎片增长最小的GPU和MIG切片进行分配，适用于在线、工作负载不可预知的场景。

Result: 在多种负载分布下，所提方法相比多个基线策略，在重负载条件下平均多调度10%的工作负载，且使用的GPU数量与基准方法相当。

Conclusion: 该调度框架有效缓解了MIG环境中的GPU碎片问题，在不增加硬件开销的前提下显著提升了系统吞吐量和资源利用效率。

Abstract: The explosive growth of AI applications has created unprecedented demand for GPU resources. Cloud providers meet this demand through GPU-as-a-Service platforms that offer rentable GPU resources for running AI workloads. In this context, the sharing of GPU resources between different tenants is essential to maximize the number of scheduled workloads. Among the various GPU sharing technologies, NVIDIA's Multi-Instance GPU (MIG) stands out by partitioning GPUs at hardware level into isolated slices with dedicated compute and memory, ensuring strong tenant isolation, preventing resource contention, and enhancing security. Despite these advantages, MIG's fixed partitioning introduces scheduling rigidity, leading to severe GPU fragmentation in multi-tenant environments, where workloads are continuously deployed and terminated. Fragmentation leaves GPUs underutilized, limiting the number of workloads that can be accommodated. To overcome this challenge, we propose a novel scheduling framework for MIG-based clouds that maximizes workload acceptance while mitigating fragmentation in an online, workload-agnostic setting. We introduce a fragmentation metric to quantify resource inefficiency and guide allocation decisions. Building on this metric, our greedy scheduling algorithm selects GPUs and MIG slices that minimize fragmentation growth for each incoming workload. We evaluate our approach against multiple baseline strategies under diverse workload distributions. Results demonstrate that our method consistently achieves higher workload acceptance rates, leading to an average 10% increase in the number of scheduled workloads in heavy load conditions, while using approximately the same number of GPUs as the benchmark methods.

</details>


### [52] [AME: An Efficient Heterogeneous Agentic Memory Engine for Smartphones](https://arxiv.org/abs/2511.19192)
*Xinkui Zhao,Qingyu Ma,Yifan Zhang,Hengxuan Lou,Guanjie Cheng,Shuiguang Deng,Jianwei Yin*

Main category: cs.DC

TL;DR: 本文提出了AME（Agentic Memory Engine），一种专为智能手机SoC设计的端侧向量数据库引擎，通过硬件感知的高效矩阵流水线和软硬件协同的任务调度机制，在满足隐私与响应性需求的同时，显著提升了查询吞吐量、索引构建速度和插入性能。


<details>
  <summary>Details</summary>
Motivation: 现有向量数据库主要面向服务器环境，直接移植到智能手机上时存在两大问题：一是与移动端SoC资源限制（如带宽、片上内存、数据类型等）不匹配；二是无法有效支持端侧智能体持续学习场景下频繁的插入、删除与索引维护等混合负载。

Method: AME采用两项关键技术：(1) 硬件感知的高效矩阵流水线，充分利用计算单元并利用多级片上存储以维持高吞吐；(2) 面向硬件与工作负载的调度策略，协调查询、插入和索引重建任务以降低延迟。

Result: 在Snapdragon 8系列SoC上基于HotpotQA的实验表明，AME在相同召回率下查询吞吐提升最高1.4倍，索引构建速度提升最高7倍，并发查询下的插入吞吐提升最高6倍。

Conclusion: AME通过软硬件协同设计，有效解决了现有向量数据库在移动端部署时的性能瓶颈，为端侧智能体提供了高效、低延迟的持续记忆能力。

Abstract: On-device agents on smartphones increasingly require continuously evolving memory to support personalized, context-aware, and long-term behaviors. To meet both privacy and responsiveness demands, user data is embedded as vectors and stored in a vector database for fast similarity search. However, most existing vector databases target server-class environments. When ported directly to smartphones, two gaps emerge: (G1) a mismatch between mobile SoC constraints and vector-database assumptions, including tight bandwidth budgets, limited on-chip memory, and stricter data type and layout constraints; and (G2) a workload mismatch, because on-device usage resembles a continuously learning memory, in which queries must coexist with frequent inserts, deletions, and ongoing index maintenance. To address these challenges, we propose AME, an on-device Agentic Memory Engine co-designed with modern smartphone SoCs. AME introduces two key techniques: (1) a hardware-aware, high-efficiency matrix pipeline that maximizes compute-unit utilization and exploits multi-level on-chip storage to sustain high throughput; and (2) a hardware- and workload-aware scheduling scheme that coordinates querying, insertion, and index rebuilding to minimize latency. We implement AME on Snapdragon 8-series SoCs and evaluate it on HotpotQA. In our experiments, AME improves query throughput by up to 1.4x at matched recall, achieves up to 7x faster index construction, and delivers up to 6x higher insertion throughput under concurrent query workloads.

</details>


### [53] [Constant-Size Certificates for Leader Election in Chordal Graphs and Related Classes](https://arxiv.org/abs/2511.19208)
*Jérémie Chalopin,Maria Kokkou*

Main category: cs.DC

TL;DR: 本文针对分布式计算中的领导者选举和生成树构造问题，提出了适用于特定图类（如弦图、无K4可拆解图）的常数大小局部认证方案，并进一步提出一种将任意认证方案自动转化为静默自稳定算法的通用方法。


<details>
  <summary>Details</summary>
Motivation: 在分布式系统中，如何让每个节点仅通过其局部信息高效验证全局解的正确性是一个核心挑战。现有工作缺乏针对特定图结构（如弦图和可拆解图）的局部认证方案，且缺乏将认证方案自动转化为容错算法的通用机制。

Method: 作者为领导者选举和生成树构造设计了每条边仅需常数大小证书的局部认证方案，其中每个节点只能访问其一跳邻域内的信息。此外，提出了一种通用转换算法，在Gouda公平调度器假设下，通过仅增加一个状态即可将任意认证方案转化为静默自稳定算法。

Result: 成功为弦图和无K4可拆解图上的领导者选举以及给定根的可拆解图上的生成树构造提供了首个常数大小局部认证方案；其中弦图方案还能保证无环定向。同时，提出的转换算法能有效实现认证方案到自稳定算法的自动转化。

Conclusion: 本工作不仅首次为特定图类提供了局部认证方案，揭示了其结构特性对验证问题的帮助，还提出了一个具有独立价值的通用转换技术，连接了局部认证与自稳定计算两个领域。

Abstract: In distributed computing a certification scheme consists of a set of states and conditions over those states that enable each node of a graph to efficiently verify the correctness of a solution to a given problem. This work focuses on two fundamental problems: leader election and spanning tree construction. For each problem, we present a constant-size (per edge), local certification scheme, where the conditions available to each node can only refer to the graph induced by its one-hop neighborhood. In particular, we provide certification schemes for leader election in chordal and $K_4$-free dismantlable graphs and for spanning tree construction in dismantlable graphs, assuming a root is given. For chordal graphs, our leader election certification scheme additionally ensures an acyclic orientation, a property that is not generally verifiable using constant-size certificates in arbitrary graphs. To the best of our knowledge, these are the first local certification results tailored to these graph classes, potentially highlighting structural properties useful for verifying additional problems. Finally, we propose an algorithm that automatically transforms any certification scheme into a silent self-stabilizing algorithm (i.e., an algorithm that automatically recovers from faults) by adding only one extra state to the set of states of the certification scheme, assuming a Gouda fair scheduler. This transformation may be of independent interest.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [54] [Optimized Memory Tagging on AmpereOne Processors](https://arxiv.org/abs/2511.17773)
*Shiv Kaushik,Mahesh Madhav,Nagi Aboulenein,Jason Bessette,Sandeep Brahmadathan,Ben Chaffin,Matthew Erler,Stephan Jourdan,Thomas Maciukenas,Ramya Masti,Jon Perry,Massimo Sutera,Scott Tetrick,Bret Toll,David Turley,Carl Worth,Atiq Bajwa*

Main category: cs.AR

TL;DR: 本文介绍了AmpereOne处理器对ARM AArch64架构中内存标记扩展（MTE）的高效实现，该实现无内存容量开销，并在数据中心工作负载中仅带来个位数性能影响，为云环境中的内存安全提供了实用解决方案。


<details>
  <summary>Details</summary>
Motivation: 内存安全漏洞仍是C/C++等指针语言编写软件面临的主要安全威胁，现有编译器和ISA扩展方案因开销大或适用性有限而难以广泛部署。

Method: 采用ARM AArch64的Memory Tagging Extension（MTE），在AmpereOne处理器上实现同步标记检查，无需额外内存存储标签，并分析软硬件栈以识别优化点。

Result: AmpereOne处理器支持MTE且无内存容量开销，在多种数据中心工作负载下同步模式仅带来个位数百分比的性能损耗，并指出应用内存管理是当前主要开销来源。

Conclusion: AmpereOne的MTE实现结合高效的硬件基础与明确的软件优化路径，非常适合在生产云环境中部署，以有效缓解内存安全问题。

Abstract: Memory-safety escapes continue to form the launching pad for a wide range of security attacks, especially for the substantial base of deployed software that is coded in pointer-based languages such as C/C++. Although compiler and Instruction Set Architecture (ISA) extensions have been introduced to address elements of this issue, the overhead and/or comprehensive applicability have limited broad production deployment. The Memory Tagging Extension (MTE) to the ARM AArch64 Instruction Set Architecture is a valuable tool to address memory-safety escapes; when used in synchronous tag-checking mode, MTE provides deterministic detection and prevention of sequential buffer overflow attacks, and probabilistic detection and prevention of exploits resulting from temporal use-after-free pointer programming bugs. The AmpereOne processor, launched in 2024, is the first datacenter processor to support MTE. Its optimized MTE implementation uniquely incurs no memory capacity overhead for tag storage and provides synchronous tag-checking with single-digit performance impact across a broad range of datacenter class workloads. Furthermore, this paper analyzes the complete hardware-software stack, identifying application memory management as the primary remaining source of overhead and highlighting clear opportunities for software optimization. The combination of an efficient hardware foundation and a clear path for software improvement makes the MTE implementation of the AmpereOne processor highly attractive for deployment in production cloud environments.

</details>


### [55] [HDDB: Efficient In-Storage SQL Database Search Using Hyperdimensional Computing on Ferroelectric NAND Flash](https://arxiv.org/abs/2511.18234)
*Quanling Zhao,Yanru Chen,Runyang Tian,Sumukh Pinge,Weihong Xu,Augusto Vega,Steven Holmes,Saransh Gupta,Tajana Rosing*

Main category: cs.AR

TL;DR: 本文提出HDDB，一种结合超维计算（HDC）与铁电NAND（FeNAND）存储器的软硬件协同设计，用于在存储内高效执行SQL谓词评估与分析任务，显著降低延迟与能耗，并具备对高噪声存储设备的强鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 传统SQL数据库在处理大规模事实表时面临高能耗与高延迟问题，而新兴的高密度FeNAND存储器虽支持存内计算但存在较高原始误码率；作者旨在利用HDC对噪声的容忍能力，构建一个低功耗、低延迟且能适应高噪声存储环境的数据库处理系统。

Method: 提出HDDB架构，将标准SQL数据表编码为HDC向量，并将谓词过滤与聚合操作转化为可在FeNAND多层单元（MLC）中执行的高效HDC运算，利用HDC固有的冗余性避免显式纠错开销。

Result: 在TPC-DS事实表上的实验表明，HDDB相比传统CPU/GPU数据库引擎最高可实现80.6倍的延迟降低和12,636倍的能耗降低，并能在高达10%随机损坏的TLC单元下保持正确结果。

Conclusion: HDDB为面向高噪声、内存为中心的数据库处理提供了一个实用且高效的解决方案，展示了HDC与新型非易失存储技术结合的巨大潜力。

Abstract: Hyperdimensional Computing (HDC) encodes information and data into high-dimensional distributed vectors that can be manipulated using simple bitwise operations and similarity searches, offering parallelism, low-precision hardware friendliness, and strong robustness to noise. These properties are a natural fit for SQL database workloads dominated by predicate evaluation and scans, which demand low energy and low latency over large fact tables. Notably, HDC's noise-tolerance maps well onto emerging ferroelectric NAND (FeNAND) memories, which provide ultra-high density and in-storage compute capability but suffer from elevated raw bit-error rates. In this work, we propose HDDB, a hardware-software co-design that combines HDC with FeNAND multi-level cells (MLC) to perform in-storage SQL predicate evaluation and analytics with massive parallelism and minimal data movement. Particularly, we introduce novel HDC encoding techniques for standard SQL data tables and formulate predicate-based filtering and aggregation as highly efficient HDC operations that can happen in-storage. By exploiting the intrinsic redundancy of HDC, HDDB maintains correct predicate and decode outcomes under substantial device noise (up to 10% randomly corrupted TLC cells) without explicit error-correction overheads. Experiments on TPC-DS fact tables show that HDDB achieves up to 80.6x lower latency and 12,636x lower energy consumption compared to conventional CPU/GPU SQL database engines, suggesting that HDDB provides a practical substrate for noise-robust, memory-centric database processing.

</details>


### [56] [Evaluation of NVENC Split-Frame Encoding (SFE) for UHD Video Transcoding](https://arxiv.org/abs/2511.18687)
*Kasidis Arunruangsirilert,Jiro Katto*

Main category: cs.AR

TL;DR: 本文评估了NVIDIA的Split-Frame Encoding（SFE）技术在超高清视频转码中的性能表现，发现其在几乎不损失率失真（RD）性能的前提下，将近实时编码吞吐量提升近一倍，并支持4K高质量预设和8K实时编码，同时不增加甚至降低端到端延迟。


<details>
  <summary>Details</summary>
Motivation: 随着消费设备能够以4K/8K超高清分辨率录制视频，对高性能视频转码器的需求日益增长。NVIDIA推出的Split-Frame Encoding（SFE）技术利用高端GPU中的多个NVENC编码单元并行处理单帧图像，以提高吞吐量，但可能带来率失真性能损失。因此，有必要系统评估SFE在吞吐量、RD性能、功耗与延迟等方面的权衡。

Method: 本文使用标准测试序列，在支持SFE的NVIDIA GPU上对超高清视频进行实验，评估SFE对率失真性能、编码吞吐量、功耗和端到端延迟的影响。

Result: 实验结果表明，在实时应用场景中，SFE几乎将编码吞吐量提升一倍，而RD性能损失可忽略；这使得4K视频可使用更高质量的编码预设，并使8K实时编码成为可能。此外，SFE在4K下不引入额外延迟，在8K下甚至能降低延迟。

Conclusion: SFE是一种高效可行的技术，能够在保持良好视频质量的同时显著提升超高清视频的实时编码吞吐量，适用于数据中心大规模部署，是实现高吞吐、低延迟UHD视频转码的关键技术。

Abstract: NVIDIA Encoder (NVENC) features in modern NVIDIA GPUs, offer significant advantages over software encoders by providing comparable Rate-Distortion (RD) performance while consuming considerably less power. The increasing capability of consumer devices to capture footage in Ultra High-Definition (UHD) at 4K and 8K resolutions necessitates high-performance video transcoders for internet-based delivery. To address this demand, NVIDIA introduced Split-Frame Encoding (SFE), a technique that leverages multiple on-die NVENC chips available in high-end GPUs. SFE splits a single UHD frame for parallel encoding across these physical encoders and subsequently stitches the results, which significantly improves encoding throughput. However, this approach is known to incur an RD performance penalty. The widespread adoption of NVIDIA GPUs in data centers, driven by the rise of Generative AI, means NVENC is poised to play a critical role in transcoding UHD video. To better understand the performance-efficiency tradeoff of SFE, this paper evaluates SFE's impact on RD performance, encoding throughput, power consumption, and end-to-end latency using standardized test sequences. The results show that for real-time applications, SFE nearly doubles encoding throughput with a negligible RD performance penalty, which enables the use of higher-quality presets for 4K and makes real-time 8K encoding feasible, effectively offsetting the minor RD penalty. Moreover, SFE adds no latency at 4K and can reduce it at 8K, positioning it as a key enabler for high-throughput, real-time UHD transcoding.

</details>


### [57] [Evaluation of GPU Video Encoder for Low-Latency Real-Time 4K UHD Encoding](https://arxiv.org/abs/2511.18688)
*Kasidis Arunruangsirilert,Jiro Katto*

Main category: cs.AR

TL;DR: 本文评估了NVIDIA、Intel和AMD GPU上的低延迟硬件编码模式，在端到端延迟和率失真（RD）性能方面进行了分析，发现超低延迟模式可在几乎不影响画质的前提下将延迟降至83毫秒。


<details>
  <summary>Details</summary>
Motivation: 随着4K UHD视频流在直播、电视服务和云游戏等场景中的普及，对低延迟高质量视频编码的需求激增。现代GPU集成了支持HEVC和AV1等先进编解码器的专用硬件编码器，并提供低延迟和超低延迟调优选项。为迎接6G时代，有必要深入理解这些编码模式的性能权衡。

Method: 作者从率失真性能和端到端延迟两个维度，系统评估了NVIDIA、Intel和AMD GPU上的低延迟与超低延迟硬件编码模式，并将其与常规延迟硬件编码及主流软件编码器进行对比。

Result: 硬件编码器相比软件方案显著降低端到端延迟，且RD性能略优；标准低延迟模式质量-延迟权衡较差，而超低延迟模式可将延迟降至83ms（5帧）且无额外RD损失；此外，硬件编码器延迟对质量预设不敏感，可兼顾高质量与低延迟。

Conclusion: GPU硬件编码器，尤其是其超低延迟模式，在保持高画质的同时实现了极低延迟，适用于未来对实时性要求严苛的应用场景，如6G时代的互动媒体服务。

Abstract: The demand for high-quality, real-time video streaming has grown exponentially, with 4K Ultra High Definition (UHD) becoming the new standard for many applications such as live broadcasting, TV services, and interactive cloud gaming. This trend has driven the integration of dedicated hardware encoders into modern Graphics Processing Units (GPUs). Nowadays, these encoders support advanced codecs like HEVC and AV1 and feature specialized Low-Latency and Ultra Low-Latency tuning, targeting end-to-end latencies of < 2 seconds and < 500 ms, respectively. As the demand for such capabilities grows toward the 6G era, a clear understanding of their performance implications is essential. In this work, we evaluate the low-latency encoding modes on GPUs from NVIDIA, Intel, and AMD from both Rate-Distortion (RD) performance and latency perspectives. The results are then compared against both the normal-latency tuning of hardware encoders and leading software encoders. Results show hardware encoders achieve significantly lower E2E latency than software solutions with slightly better RD performance. While standard Low-Latency tuning yields a poor quality-latency trade-off, the Ultra Low-Latency mode reduces E2E latency to 83 ms (5 frames) without additional RD impact. Furthermore, hardware encoder latency is largely insensitive to quality presets, enabling high-quality, low-latency streams without compromise.

</details>


### [58] [Splatonic: Architecture Support for 3D Gaussian Splatting SLAM via Sparse Processing](https://arxiv.org/abs/2511.18755)
*Xiaotong Huang,He Zhu,Tianrui Ma,Yuxiang Xiong,Fangxin Liu,Zhezhi He,Yiming Gan,Zihan Liu,Jingwen Leng,Yu Feng,Minyi Guo*

Main category: cs.AR

TL;DR: 本文提出了Splatonic，一种面向资源受限设备的稀疏高效实时3D高斯泼溅SLAM（3DGS-SLAM）算法与硬件协同设计方案，通过自适应稀疏像素采样和新型像素级渲染管线，在保持精度的同时显著提升速度并降低能耗。


<details>
  <summary>Details</summary>
Motivation: 现有3DGS-SLAM算法因计算开销大（尤其在跟踪阶段），难以部署于移动平台；亟需兼顾效率、精度与能效的解决方案。

Method: 提出自适应稀疏像素采样算法以减少渲染像素数量，并设计基于像素的渲染管线（包括高斯并行渲染和预判α检测）以提升移动GPU利用率；进一步构建流水线架构优化投影与聚合阶段的新瓶颈。

Result: 在四种3DGS-SLAM算法上评估表明，Splatonic相比商用移动GPU最高实现274.9倍加速和4738.5倍能效提升，相比当前最先进的加速器也有25.2倍加速和241.1倍能效提升，且精度相当。

Conclusion: Splatonic通过算法-硬件协同设计有效解决了3DGS-SLAM在移动端部署的性能与能效瓶颈，为高保真实时SLAM在资源受限设备上的应用提供了可行路径。

Abstract: 3D Gaussian splatting (3DGS) has emerged as a promising direction for SLAM due to its high-fidelity reconstruction and rapid convergence. However, 3DGS-SLAM algorithms remain impractical for mobile platforms due to their high computational cost, especially for their tracking process.
  This work introduces Splatonic, a sparse and efficient real-time 3DGS-SLAM algorithm-hardware co-design for resource-constrained devices. Inspired by classical SLAMs, we propose an adaptive sparse pixel sampling algorithm that reduces the number of rendered pixels by up to 256$\times$ while retaining accuracy. To unlock this performance potential on mobile GPUs, we design a novel pixel-based rendering pipeline that improves hardware utilization via Gaussian-parallel rendering and preemptive $α$-checking. Together, these optimizations yield up to 121.7$\times$ speedup on the bottleneck stages and 14.6$\times$ end-to-end speedup on off-the-shelf GPUs. To further address new bottlenecks introduced by our rendering pipeline, we propose a pipelined architecture that simplifies the overall design while addressing newly emerged bottlenecks in projection and aggregation. Evaluated across four 3DGS-SLAM algorithms, Splatonic achieves up to 274.9$\times$ speedup and 4738.5$\times$ energy savings over mobile GPUs and up to 25.2$\times$ speedup and 241.1$\times$ energy savings over state-of-the-art accelerators, all with comparable accuracy.

</details>


### [59] [HeLEx: A Heterogeneous Layout Explorer for Spatial Elastic Coarse-Grained Reconfigurable Arrays](https://arxiv.org/abs/2511.19366)
*Alan Jia Bao Du,Tarek S. Abdelrahman*

Main category: cs.AR

TL;DR: HeLEx 是一个用于优化异构弹性粗粒度可重构阵列（CGRAs）功能布局的框架，通过分支限界搜索显著减少处理单元中的操作数量，在保持映射成功的同时大幅降低面积和功耗。


<details>
  <summary>Details</summary>
Motivation: 现有 CGRA 设计通常采用全功能布局，导致资源冗余、面积和功耗过高；亟需一种能根据具体数据流图自动优化功能布局的方法。

Method: HeLEx 框架从全功能布局出发，利用分支限界（branch-and-bound）搜索策略逐步移除处理单元（PE）中不必要的操作，同时确保输入的数据流图（DFGs）仍能成功映射到优化后的 CGRA 上。

Result: 在 12 个 DFG 和 9 种 CGRA 尺寸的实验中，HeLEx 平均减少 68.7% 的操作数，面积减少近 70%，功耗降低超 51%；其结果平均仅比理论最小 CGRA 多出 6.2% 的操作，并优于现有两种先进框架最多达 2.6 倍。

Conclusion: HeLEx 能高效生成接近理论最优的异构 CGRA 功能布局，在显著节省硬件资源的同时保持良好的映射能力，优于当前主流方法。

Abstract: We present HeLEx, a framework for determining the functional layout of heterogeneous spatially-configured elastic Coarse-Grained Reconfigurable Arrays (CGRAs). Given a collection of input data flow graphs (DFGs) and a target CGRA, the framework starts with a full layout in which every processing element (PE) supports every operation in the DFGs. It then employs a branch-and-bound (BB) search to eliminate operations out of PEs, ensuring that the input DFGs successfully map onto the resulting CGRAs, eventually returning an optimized heterogeneous CGRA. Experimental evaluation with 12 DFGs and 9 target CGRA sizes reveals that the framework reduces the number of operations by 68.7% on average, resulting in a reduction of CGRA area by almost 70% and of power by over 51%, all compared to the initial full layout. HeLEx generates CGRAs that are on average only within 6.2% of theoretically minimum CGRAs that support exactly the number of operations needed by the input DFGs. A comparison with functional layouts produced by two state-of-the-art frameworks indicates that HeLEx achieves better reduction in the number of operations, by up to 2.6X.

</details>
