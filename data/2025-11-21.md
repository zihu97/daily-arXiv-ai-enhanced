<div id=toc></div>

# Table of Contents

- [cs.SE](#cs.SE) [Total: 11]
- [cs.MA](#cs.MA) [Total: 1]
- [cs.DC](#cs.DC) [Total: 6]
- [cs.NI](#cs.NI) [Total: 1]


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [1] [Technique to Baseline QE Artefact Generation Aligned to Quality Metrics](https://arxiv.org/abs/2511.15733)
*Eitan Farchi,Kiran Nayak,Papia Ghosh Majumdar,Saritha Route*

Main category: cs.SE

TL;DR: 本文提出一种结合大语言模型（LLM）生成、反向生成与基于评分标准的迭代优化方法，用于系统化评估和提升质量工程（QE）制品（如需求、测试用例、BDD场景）的质量，并在12个项目中验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型在自动生成质量工程制品方面展现出潜力，但如何确保这些生成内容的质量仍是一个关键挑战。

Method: 该方法融合了LLM驱动的生成、反向生成以及由评分标准引导的迭代优化，从清晰性、完整性、一致性和可测试性四个维度对QE制品进行评估和改进。

Result: 在12个项目的实验中，反向生成的制品不仅优于低质量输入，而且在输入质量较高时也能维持高标准。

Conclusion: 所提出的框架实现了可扩展且可靠的QE制品验证机制，在自动化与质量问责之间建立了有效桥梁。

Abstract: Large Language Models (LLMs) are transforming Quality Engineering (QE) by automating the generation of artefacts such as requirements, test cases, and Behavior Driven Development (BDD) scenarios. However, ensuring the quality of these outputs remains a challenge. This paper presents a systematic technique to baseline and evaluate QE artefacts using quantifiable metrics. The approach combines LLM-driven generation, reverse generation , and iterative refinement guided by rubrics technique for clarity, completeness, consistency, and testability. Experimental results across 12 projects show that reverse-generated artefacts can outperform low-quality inputs and maintain high standards when inputs are strong. The framework enables scalable, reliable QE artefact validation, bridging automation with accountability.

</details>


### [2] [Rethinking Kernel Program Repair: Benchmarking and Enhancing LLMs with RGym](https://arxiv.org/abs/2511.15757)
*Kareem Shehada,Yifan Wu,Wyatt D. Feng,Adithya Iyer,Gryphon Kumfert,Yangruibo Ding,Zhiyun Qian*

Main category: cs.SE

TL;DR: 本文提出了RGym，一个轻量级、平台无关的Linux内核自动程序修复（APR）评估框架，并基于该框架设计了一个高效低成本的APR流程，在143个真实内核缺陷上实现了最高43.36%的修复成功率，单次修复成本低于0.2美元。


<details>
  <summary>Details</summary>
Motivation: 现有自动程序修复（APR）基准（如SWE-Bench）主要关注用户空间应用，忽视了内核空间调试与修复的复杂性；而Linux内核因其单体内核结构、并发性和底层硬件交互等特点，给APR带来独特挑战。此前方法如KGym和CrashFixer或成功率低，或依赖昂贵复杂的云基础设施。

Method: 作者构建了RGym框架，可在本地普通硬件上运行，并提出一种结合专用定位技术（如调用栈和归责提交）的简单有效APR流程，避免KGym中不切实际的预言机使用；同时通过消融实验分析定位策略、提示结构和模型选择的影响，并引入基于反馈的重试机制。

Result: 在143个经过筛选验证的Linux内核缺陷数据集上，使用GPT-5 Thinking模型达到最高43.36%的通过率，单缺陷修复成本低于0.20美元；消融实验表明所提定位策略和反馈重试显著提升修复效果。

Conclusion: RGym为Linux内核APR提供了一个轻量、低成本且高效的评估与修复方案，证明在本地硬件上实现高性能内核修复是可行的，同时揭示了定位信息和模型交互策略对修复成功率的关键作用。

Abstract: Large Language Models (LLMs) have revolutionized automated program repair (APR) but current benchmarks like SWE-Bench predominantly focus on userspace applications and overlook the complexities of kernel-space debugging and repair. The Linux kernel poses unique challenges due to its monolithic structure, concurrency, and low-level hardware interactions. Prior efforts such as KGym and CrashFixer have highlighted the difficulty of APR in this domain, reporting low success rates or relying on costly and complex pipelines and pricey cloud infrastructure. In this work, we introduce RGym, a lightweight, platform-agnostic APR evaluation framework for the Linux kernel designed to operate on local commodity hardware. Built on RGym, we propose a simple yet effective APR pipeline leveraging specialized localization techniques (e.g., call stacks and blamed commits) to overcome the unrealistic usage of oracles in KGym. We test on a filtered and verified dataset of 143 bugs. Our method achieves up to a 43.36% pass rate with GPT-5 Thinking while maintaining a cost of under $0.20 per bug. We further conduct an ablation study to analyze contributions from our proposed localization strategy, prompt structure, and model choice, and demonstrate that feedback-based retries can significantly enhance success rates.

</details>


### [3] [A Causal Perspective on Measuring, Explaining and Mitigating Smells in \llm-Generated Code](https://arxiv.org/abs/2511.15817)
*Alejandro Velasco,Daniel Rodriguez-Cardenas,Dipin Khati,David N. Palacio,Luftar Rahman Alif,Denys Poshyvanyk*

Main category: cs.SE

TL;DR: 本文系统研究了大语言模型（LLMs）生成代码中的“代码异味”问题，提出并验证了一种名为PSC（Propensity Smelly Score）的指标，用于衡量和解释异味倾向，并据此探索缓解策略。研究发现提示设计和模型架构对异味产生具有决定性影响，且PSC有助于开发者评估代码质量。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型在软件工程中被广泛采用，但其生成代码常包含不良编程实践，即“代码异味”，影响代码的可读性、可维护性和设计完整性。现有研究多聚焦于检测或修复异味，缺乏对其在生成过程中如何及何时出现的系统理解。

Method: 作者基于PSC（一种估计生成特定类型异味可能性的概率指标），将其作为因果分析工具，系统测量并解释不同因素（如生成策略、模型规模、架构和提示设计）对代码结构质量的影响，并提出缓解策略。此外，通过用户研究验证PSC对开发者判断的支持作用。

Result: 研究发现提示设计和模型架构对异味倾向起决定性作用；提出的缓解策略能有效减少异味发生；用户研究表明PSC有助于开发者理解和评估模型生成代码的质量。

Conclusion: 本研究为将质量感知评估整合到大语言模型代码生成的评估与部署流程中奠定了基础，强调了在模型使用中考虑结构性质量问题的重要性。

Abstract: Recent advances in large language models (LLMs) have accelerated their adoption in software engineering contexts. However, concerns persist about the structural quality of the code they produce. In particular, LLMs often replicate poor coding practices, introducing code smells (i.e., patterns that hinder readability, maintainability, or design integrity). Although prior research has examined the detection or repair of smells, we still lack a clear understanding of how and when these issues emerge in generated code.
  This paper addresses this gap by systematically measuring, explaining and mitigating smell propensity in LLM-generated code. We build on the Propensity Smelly Score (PSC), a probabilistic metric that estimates the likelihood of generating particular smell types, and establish its robustness as a signal of structural quality. Using PSC as an instrument for causal analysis, we identify how generation strategy, model size, model architecture and prompt formulation shape the structural properties of generated code. Our findings show that prompt design and architectural choices play a decisive role in smell propensity and motivate practical mitigation strategies that reduce its occurrence. A user study further demonstrates that PSC helps developers interpret model behavior and assess code quality, providing evidence that smell propensity signals can support human judgement. Taken together, our work lays the groundwork for integrating quality-aware assessments into the evaluation and deployment of LLMs for code.

</details>


### [4] [AI-Enabled Orchestration of Event-Driven Business Processes in Workday ERP for Healthcare Enterprises](https://arxiv.org/abs/2511.15852)
*Monu Sharma*

Main category: cs.SE

TL;DR: 本文提出了一种在Workday ERP中嵌入AI能力的事件驱动型编排框架，通过机器学习触发器、异常检测和流程挖掘技术，实现医疗环境中财务与供应链工作流的智能同步，显著提升了运营效率与决策准确性。


<details>
  <summary>Details</summary>
Motivation: 传统ERP系统的工作流逻辑难以适应医疗行业高度数据驱动和事件密集的运营环境，亟需更具适应性的智能解决方案。

Method: 在Workday ERP平台中构建AI驱动的事件驱动编排框架，结合机器学习触发器、异常检测和流程挖掘分析，自动响应库存耗尽、付款延迟和患者需求波动等运营事件。

Result: 多机构案例分析表明，该框架显著提高了流程效率、成本可见性和决策准确性，并增强了系统的运营韧性、治理能力和可扩展性。

Conclusion: 将AI能力嵌入Workday的事件驱动架构可有效提升医疗企业ERP系统的智能化水平，为下一代医疗自动化策略提供参考模型。

Abstract: The adoption of cloud-based Enterprise Resource Planning (ERP) platforms such as Workday has transformed healthcare operations by integrating financial, supply-chain, and workforce processes into a unified ecosystem. However, traditional workflow logic in ERP systems often lacks the adaptability required to manage event-driven and data-intensive healthcare environments.
  This study proposes an AI-enabled event-driven orchestration framework within Workday ERP that intelligently synchronizes financial and supply-chain workflows across distributed healthcare entities. The framework employs machine-learning triggers, anomaly detection, and process mining analytics to anticipate and automate responses to operational events such as inventory depletion, payment delays, or patient demand fluctuations. A multi-organization case analysis demonstrates measurable gains in process efficiency, cost visibility, and decision accuracy.
  Results confirm that embedding AI capabilities into Workday's event-based architecture enhances operational resilience, governance, and scalability. The proposed model contributes to the broader understanding of intelligent ERP integration and establishes a reference for next-generation automation strategies in healthcare enterprises.

</details>


### [5] [RE for AI in Practice: Managing Data Annotation Requirements for AI Autonomous Driving Systems](https://arxiv.org/abs/2511.15859)
*Hina Saeeda,Mazen Mohamad,Eric Knauss,Jennifer Horkoff,Ali Nouri*

Main category: cs.SE

TL;DR: 本研究通过19次访谈揭示了自动驾驶AI感知系统中数据标注需求的五大挑战（模糊性、边缘案例复杂性、需求演变、不一致性、资源限制）和三类最佳实践（伦理合规、改进指南、嵌入式质量保障），并阐明了标注需求如何影响整个AI系统开发流程。


<details>
  <summary>Details</summary>
Motivation: 高质量的数据标注需求对开发安全可靠的自动驾驶AI感知系统至关重要，但其制定与管理尚未得到充分研究，导致不一致、安全隐患和监管问题。

Method: 对来自六家国际公司和四家研究机构的19名从业者进行半结构化访谈，并采用主题分析法提炼关键发现。

Result: 识别出五大关键挑战和三类最佳实践，揭示了标注需求、标注实践、数据质量和AI系统性能之间的关键相互关系。

Conclusion: 本研究首次提供了基于实证的标注需求改进建议，为提升标注质量、合规性和系统可靠性提供可行洞见，并推动了AI领域的软件工程与需求工程发展。

Abstract: High-quality data annotation requirements are crucial for the development of safe and reliable AI-enabled perception systems (AIePS) in autonomous driving. Although these requirements play a vital role in reducing bias and enhancing performance, their formulation and management remain underexplored, leading to inconsistencies, safety risks, and regulatory concerns. Our study investigates how annotation requirements are defined and used in practice, the challenges in ensuring their quality, practitioner-recommended improvements, and their impact on AIePS development and performance. We conducted $19$ semi-structured interviews with participants from six international companies and four research organisations. Our thematic analysis reveals five main key challenges: ambiguity, edge case complexity, evolving requirements, inconsistencies, and resource constraints and three main categories of best practices, including ensuring compliance with ethical standards, improving data annotation requirements guidelines, and embedded quality assurance for data annotation requirements. We also uncover critical interrelationships between annotation requirements, annotation practices, annotated data quality, and AIePS performance and development, showing how requirement flaws propagate through the AIePS development pipeline. To the best of our knowledge, this study is the first to offer empirically grounded guidance on improving annotation requirements, offering actionable insights to enhance annotation quality, regulatory compliance, and system reliability. It also contributes to the emerging fields of Software Engineering (SE for AI) and Requirements Engineering (RE for AI) by bridging the gap between RE and AI in a timely and much-needed manner.

</details>


### [6] [InfCode: Adversarial Iterative Refinement of Tests and Patches for Reliable Software Issue Resolution](https://arxiv.org/abs/2511.16004)
*KeFan Li,Mengfei Wang,Hengzhi Zhang,Zhichao Li,Yuan Yuan,Mu Li,Xiang Gao,Hailong Sun,Chunming Hu,Weifeng Lv*

Main category: cs.SE

TL;DR: InfCode 是一个基于对抗多智能体的自动化框架，通过测试生成器与代码补丁生成器的对抗交互，迭代优化测试与补丁，并在容器化环境中实现仓库级问题修复，在 SWE-bench Verified 上达到 79.4% 的新 SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有基于智能体或流水线的方法常依赖不充分的测试用例，导致生成的补丁虽通过验证却未真正修复缺陷；因此需要一种能进行仓库级推理、准确诊断并提供强验证信号的方法。

Method: 提出 InfCode 框架，包含测试补丁生成器、代码补丁生成器和选择器三个智能体，在容器化环境中通过对抗机制迭代优化测试与补丁，最终由选择器选出最可靠的修复方案。

Result: 在 SWE-bench Lite 和 SWE-bench Verified 基准上使用 DeepSeek-V3 和 Claude 4.5 Sonnet 等模型进行实验，InfCode 显著优于现有强基线方法，在 SWE-bench Verified 上达到 79.4% 的解决率。

Conclusion: InfCode 通过对抗多智能体机制有效提升了仓库级软件问题自动修复的准确性与可靠性，是当前该任务上的最优方法，并已开源。

Abstract: Large language models have advanced software engineering automation, yet resolving real-world software issues remains difficult because it requires repository-level reasoning, accurate diagnostics, and strong verification signals. Existing agent-based and pipeline-based methods often rely on insufficient tests, which can lead to patches that satisfy verification but fail to fix the underlying defect. We present InfCode, an adversarial multi-agent framework for automated repository-level issue resolution. InfCode iteratively refines both tests and patches through adversarial interaction between a Test Patch Generator and a Code Patch Generator, while a Selector agent identifies the most reliable fix. The framework runs inside a containerized environment that supports realistic repository inspection, modification, and validation. Experiments on SWE-bench Lite and SWE-bench Verified using models such as DeepSeek-V3 and Claude 4.5 Sonnet show that InfCode consistently outperforms strong baselines. It achieves 79.4% performance on SWE-bench Verified, establishing a new state-of-the-art. We have released InfCode as an open-source project at https://github.com/Tokfinity/InfCode.

</details>


### [7] [The Future of Development Environments with AI Foundation Models: NII Shonan Meeting 222 Report](https://arxiv.org/abs/2511.16092)
*Xing Hu,Raula Gaikovina Kula,Christoph Treude*

Main category: cs.SE

TL;DR: 本文报告了Shonan Meeting 222上33位专家对生成式人工智能（GenAI）在集成开发环境（IDE）中影响的讨论，探讨其带来的挑战与机遇。


<details>
  <summary>Details</summary>
Motivation: 随着GenAI在代码生成、测试、审查和修复等任务中表现卓越，有必要深入理解其如何改变开发者与IDE之间的人机交互方式。

Method: 组织来自软件工程、人工智能和人机交互领域的33位专家召开Shonan Meeting 222，通过研讨形式分析GenAI对IDE的影响。

Result: 会议识别出GenAI在提升抽象层次、改变编程范式方面的潜力，并明确了当前面临的关键挑战与未来研究机会。

Conclusion: GenAI有望重塑IDE中的人机协作模式，但需跨学科合作解决技术、交互与伦理等方面的挑战。

Abstract: Generative Artificial Intelligence (GenAI) models are achieving remarkable performance in various tasks, including code generation, testing, code review, and program repair. The ability to increase the level of abstraction away from writing code has the potential to change the Human-AI interaction within the integrated development environment (IDE). To explore the impact of GenAI on IDEs, 33 experts from the Software Engineering, Artificial Intelligence, and Human-Computer Interaction domains gathered to discuss challenges and opportunities at Shonan Meeting 222. This is the report

</details>


### [8] [Domain-constrained Synthesis of Inconsistent Key Aspects in Textual Vulnerability Descriptions](https://arxiv.org/abs/2511.16123)
*Linyi Han,Shidong Pan,Zhenchang Xing,Sofonias Yitagesu,Xiaowang Zhang,Zhiyong Feng,Jiamou Sun,Qing Huang*

Main category: cs.SE

TL;DR: 本文提出了一种基于领域约束大语言模型的文本漏洞描述（TVD）统一框架，通过提取、自评估和融合三个阶段有效缓解不同来源TVD中的关键信息不一致问题，并开发了可视化工具Digest Labels以提升可用性。


<details>
  <summary>Details</summary>
Motivation: 不同漏洞库中的文本漏洞描述（TVD）存在关键信息不一致的问题，影响安全分析师对漏洞的全面理解；现有方法在对齐外部知识库时往往丢失有价值的信息，难以生成综合性表示。

Method: 提出一个三阶段的领域约束LLM合成框架：1）基于规则模板的提取，确保捕获所有关键细节；2）利用领域锚词进行自评估，衡量语义差异；3）基于信息熵进行融合，调和不一致性并优先保留相关信息。同时开发了可视化工具Digest Labels。

Result: 该框架将关键方面增强的F1分数从0.82提升至0.87，并使理解和处理效率提升超过30%；人工评估表明Digest Labels显著提高了TVD的可用性。

Conclusion: 所提出的框架有效解决了TVD中的不一致性问题，提升了漏洞信息的综合表达与实用性，为安全分析提供了更高效、准确的支持。

Abstract: Textual Vulnerability Descriptions (TVDs) are crucial for security analysts to understand and address software vulnerabilities. However, the key aspect inconsistencies in TVDs from different repositories pose challenges for achieving a comprehensive understanding of vulnerabilities. Existing approaches aim to mitigate inconsistencies by aligning TVDs with external knowledge bases, but they often discard valuable information and fail to synthesize comprehensive representations. In this paper, we propose a domain-constrained LLM-based synthesis framework for unifying key aspects of TVDs. Our framework consists of three stages: 1) Extraction, guided by rule-based templates to ensure all critical details are captured; 2) Self-evaluation, using domain-specific anchor words to assess semantic variability across sources; and 3) Fusion, leveraging information entropy to reconcile inconsistencies and prioritize relevant details. This framework improves synthesis performance, increasing the F1 score for key aspect augmentation from 0.82 to 0.87, while enhancing comprehension and efficiency by over 30\%. We further develop Digest Labels, a practical tool for visualizing TVDs, which human evaluations show significantly boosts usability.

</details>


### [9] [Beyond Code Similarity: Benchmarking the Plausibility, Efficiency, and Complexity of LLM-Generated Smart Contracts](https://arxiv.org/abs/2511.16224)
*Francesco Salzano,Simone Scalabrino,Rocco Oliveto,Simone Scalabrino*

Main category: cs.SE

TL;DR: 该论文评估了大语言模型（LLM）在生成Solidity智能合约代码时的功能正确性与非功能性属性（如Gas消耗、复杂度等），发现尽管生成代码在语义上与真实合约高度相似，但功能正确率仅为20%-26%；引入检索增强生成（RAG）可将正确率提升至最多45%，但仍需专家验证才能用于生产环境。


<details>
  <summary>Details</summary>
Motivation: 现有研究缺乏对LLM生成的Solidity智能合约在关键功能和非功能属性（如安全性、Gas消耗、确定性）方面的全面评估，而这些属性对智能合约至关重要。

Method: 在零样本和检索增强生成（RAG）两种设置下，对四种前沿大语言模型在500个真实函数上进行基准测试；采用代码相似性指标、语义嵌入、自动化测试、Gas分析及认知与圈复杂度分析等多维度评估方法。

Result: LLM生成的代码语义相似度高，但功能正确率低（20%-26%）；代码复杂度和Gas消耗显著更低，常因省略验证逻辑所致；RAG显著提升功能正确率（最高达45%），并生成更简洁高效的代码。

Conclusion: 语义相似性不等于功能合理性，当前LLM生成的智能合约尚不能直接用于生产，即使使用RAG也需专家仔细验证；实现可靠、生产级的智能合约自动生成仍面临重大挑战。

Abstract: Smart Contracts are critical components of blockchain ecosystems, with Solidity as the dominant programming language. While LLMs excel at general-purpose code generation, the unique constraints of Smart Contracts, such as gas consumption, security, and determinism, raise open questions about the reliability of LLM-generated Solidity code. Existing studies lack a comprehensive evaluation of these critical functional and non-functional properties. We benchmark four state-of-the-art models under zero-shot and retrieval-augmented generation settings across 500 real-world functions. Our multi-faceted assessment employs code similarity metrics, semantic embeddings, automated test execution, gas profiling, and cognitive and cyclomatic complexity analysis. Results show that while LLMs produce code with high semantic similarity to real contracts, their functional correctness is low: only 20% to 26% of zero-shot generations behave identically to ground-truth implementations under testing. The generated code is consistently simpler, with significantly lower complexity and gas consumption, often due to omitted validation logic. Retrieval-Augmented Generation markedly improves performance, boosting functional correctness by up to 45% and yielding more concise and efficient code. Our findings reveal a significant gap between semantic similarity and functional plausibility in LLM-generated Smart Contracts. We conclude that while RAG is a powerful enhancer, achieving robust, production-ready code generation remains a substantial challenge, necessitating careful expert validation.

</details>


### [10] [Data Annotation Quality Problems in AI-Enabled Perception System Development](https://arxiv.org/abs/2511.16410)
*Hina Saeeda,Tommy Johansson,Mazen Mohamad,Eric Knauss*

Main category: cs.SE

TL;DR: 本研究通过多组织案例分析，识别并分类了自动驾驶AI感知系统数据标注中的18种常见错误类型，并验证了该分类法在工业实践中的实用性，为提升标注质量和构建可信AI系统提供了诊断工具与指导。


<details>
  <summary>Details</summary>
Motivation: 数据标注对自动驾驶AI感知系统的开发至关重要但极易出错，而业界缺乏对标注错误如何在多组织汽车供应链中产生和传播的实证洞察。

Method: 开展涉及六家公司和四家研究机构的多组织案例研究，基于20位专家的19次半结构化访谈（共50小时转录文本），采用六阶段主题分析方法构建标注错误分类体系。

Result: 提出了包含18种标注错误类型的分类法，涵盖完整性、准确性和一致性三个数据质量维度，并经行业从业者验证，可用于根本原因分析、供应商评估、新人培训及标注指南优化。

Conclusion: 将标注质量问题视为生命周期与供应链问题，本研究为面向AI的软件工程（SE4AI）提供了共享术语、诊断工具集和可操作建议，有助于构建更可靠可信的AI感知系统。

Abstract: Data annotation is essential but highly error-prone in the development of AI-enabled perception systems (AIePS) for automated driving, and its quality directly influences model performance, safety, and reliability. However, the industry lacks empirical insights into how annotation errors emerge and spread across the multi-organisational automotive supply chain. This study addresses this gap through a multi-organisation case study involving six companies and four research institutes across Europe and the UK. Based on 19 semi-structured interviews with 20 experts (50 hours of transcripts) and a six-phase thematic analysis, we develop a taxonomy of 18 recurring annotation error types across three data-quality dimensions: completeness (e.g., attribute omission, missing feedback loops, edge-case omissions, selection bias), accuracy (e.g., mislabelling, bounding-box inaccuracies, granularity mismatches, bias-driven errors), and consistency (e.g., inter-annotator disagreement, ambiguous instructions, misaligned hand-offs, cross-modality inconsistencies). The taxonomy was validated with industry practitioners, who reported its usefulness for root-cause analysis, supplier quality reviews, onboarding, and improving annotation guidelines. They described it as a failure-mode catalogue similar to FMEA. By conceptualising annotation quality as a lifecycle and supply-chain issue, this study contributes to SE4AI by offering a shared vocabulary, diagnostic toolset, and actionable guidance for building trustworthy AI-enabled perception systems.

</details>


### [11] [Green Resilience of Cyber-Physical Systems: Doctoral Dissertation](https://arxiv.org/abs/2511.16593)
*Diaeddin Rimawi*

Main category: cs.SE

TL;DR: 本文研究在线协作人工智能系统（OL-CAIS）在遭遇干扰事件时如何在恢复性能（韧性）与降低能耗（绿色性）之间取得平衡，提出了GResilience框架及多种智能体策略，并通过实验验证其有效性。


<details>
  <summary>Details</summary>
Motivation: OL-CAIS作为一类信息物理系统，在与人类协作学习过程中易受干扰事件影响，导致性能下降；决策者需在恢复性能的同时控制能源消耗，因此需要有效方法来协调韧性与绿色性之间的权衡。

Method: 作者将OL-CAIS运行状态建模为稳态、干扰态和终态三类，提出GResilience框架，包含基于多目标优化的单智能体、博弈论的双智能体和强化学习的RL智能体三种恢复策略，并设计了衡量韧性和绿色性的指标体系。通过真实与仿真实验（协作机器人从人类示范中学习物体分类）进行评估。

Result: 实验表明所提出的韧性模型能准确捕捉干扰期间的性能变化；GResilience策略可缩短恢复时间、稳定性能并减少对人类依赖，其中RL智能体效果最佳，但伴随轻微CO₂排放增加；重复干扰会导致灾难性遗忘，而所提策略有助于维持系统稳定性；容器化执行可使CO₂排放减半。

Conclusion: 本研究提供了用于保障OL-CAIS绿色恢复的模型、度量方法与策略，为在韧性与绿色性之间实现有效平衡提供了可行路径。

Abstract: Cyber-physical systems (CPS) combine computational and physical components. Online Collaborative AI System (OL-CAIS) is a type of CPS that learn online in collaboration with humans to achieve a common goal, which makes it vulnerable to disruptive events that degrade performance. Decision-makers must therefore restore performance while limiting energy impact, creating a trade-off between resilience and greenness. This research addresses how to balance these two properties in OL-CAIS. It aims to model resilience for automatic state detection, develop agent-based policies that optimize the greenness-resilience trade-off, and understand catastrophic forgetting to maintain performance consistency. We model OL-CAIS behavior through three operational states: steady, disruptive, and final. To support recovery during disruptions, we introduce the GResilience framework, which provides recovery strategies through multi-objective optimization (one-agent), game-theoretic decision-making (two-agent), and reinforcement learning (RL-agent). We also design a measurement framework to quantify resilience and greenness. Empirical evaluation uses real and simulated experiments with a collaborative robot learning object classification from human demonstrations. Results show that the resilience model captures performance transitions during disruptions, and that GResilience policies improve green recovery by shortening recovery time, stabilizing performance, and reducing human dependency. RL-agent policies achieve the strongest results, although with a marginal increase in CO2 emissions. We also observe catastrophic forgetting after repeated disruptions, while our policies help maintain steadiness. A comparison with containerized execution shows that containerization cuts CO2 emissions by half. Overall, this research provides models, metrics, and policies that ensure the green recovery of OL-CAIS.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [12] [The Subtle Art of Defection: Understanding Uncooperative Behaviors in LLM based Multi-Agent Systems](https://arxiv.org/abs/2511.15862)
*Devang Kulshreshtha,Wanyu Du,Raghav Jain,Srikanth Doss,Hang Su,Sandesh Swamy,Yanjun Qi*

Main category: cs.MA

TL;DR: 本文提出一个新框架，用于模拟和分析不合作行为如何破坏基于大语言模型的多智能体系统，并通过资源管理实验验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有文献缺乏对多智能体系统中不合作行为的系统性分类与动态模拟方法，难以评估其对系统稳定性的影响。

Method: 构建包含两部分的框架：(1) 基于博弈论的不合作行为分类体系；(2) 多阶段动态仿真流程，随智能体状态演化生成并优化不合作行为。在协作资源管理场景中，通过生存时间和资源滥用率等指标评估系统稳定性。

Result: 框架生成不合作行为的准确率达96.7%（经人工验证）；实验显示，合作智能体可维持100%系统稳定性（12轮内无资源滥用），而不合作行为可在1至7轮内导致系统迅速崩溃。

Conclusion: 不合作行为对多智能体系统的集体性能具有显著破坏性，研究强调了设计更具韧性的多智能体系统的必要性。

Abstract: This paper introduces a novel framework for simulating and analyzing how uncooperative behaviors can destabilize or collapse LLM-based multi-agent systems. Our framework includes two key components: (1) a game theory-based taxonomy of uncooperative agent behaviors, addressing a notable gap in the existing literature; and (2) a structured, multi-stage simulation pipeline that dynamically generates and refines uncooperative behaviors as agents' states evolve. We evaluate the framework via a collaborative resource management setting, measuring system stability using metrics such as survival time and resource overuse rate. Empirically, our framework achieves 96.7% accuracy in generating realistic uncooperative behaviors, validated by human evaluations. Our results reveal a striking contrast: cooperative agents maintain perfect system stability (100% survival over 12 rounds with 0% resource overuse), while any uncooperative behavior can trigger rapid system collapse within 1 to 7 rounds. These findings demonstrate that uncooperative agents can significantly degrade collective outcomes, highlighting the need for designing more resilient multi-agent systems.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [13] [Efficient Chromosome Parallelization for Precision Medicine Genomic Workflows](https://arxiv.org/abs/2511.15977)
*Daniel Mas Montserrat,Ray Verma,Míriam Barrabés,Francisco M. de la Vega,Carlos D. Bustamante,Alexander G. Ioannidis*

Main category: cs.DC

TL;DR: 本文提出多种自适应、内存高效的染色体级生物信息学工作流并行化机制，包括基于符号回归的内存预测模型、动态调度器和静态调度策略，以优化大规模基因组工作流的内存使用和执行效率。


<details>
  <summary>Details</summary>
Motivation: 大规模基因组工作流在精准医学中处理的数据量巨大，常导致内存峰值高、磁盘I/O密集以及因内存不足而任务失败；传统静态资源分配方法难以应对不同染色体任务间内存需求的差异，造成资源利用率低和运行时间长。

Method: 1）构建符号回归模型估算每个染色体任务的内存消耗，并引入插值偏差以保守地减少内存过度分配；2）设计动态调度器，利用多项式回归预测内存使用，并将任务打包建模为背包问题以最优批处理作业；3）提出静态调度器，优化染色体处理顺序以降低峰值内存同时保持吞吐量。

Result: 在模拟和真实基因组流程上的评估表明，所提方法能有效减少内存溢出、均衡线程负载，并加快端到端执行速度。

Conclusion: 所提出的自适应内存高效调度机制显著提升了大规模基因组工作流的性能与资源利用率，展示了其在优化此类计算密集型任务中的潜力。

Abstract: Large-scale genomic workflows used in precision medicine can process datasets spanning tens to hundreds of gigabytes per sample, leading to high memory spikes, intensive disk I/O, and task failures due to out-of-memory errors. Simple static resource allocation methods struggle to handle the variability in per-chromosome RAM demands, resulting in poor resource utilization and long runtimes. In this work, we propose multiple mechanisms for adaptive, RAM-efficient parallelization of chromosome-level bioinformatics workflows. First, we develop a symbolic regression model that estimates per-chromosome memory consumption for a given task and introduces an interpolating bias to conservatively minimize over-allocation. Second, we present a dynamic scheduler that adaptively predicts RAM usage with a polynomial regression model, treating task packing as a Knapsack problem to optimally batch jobs based on predicted memory requirements. Additionally, we present a static scheduler that optimizes chromosome processing order to minimize peak memory while preserving throughput. Our proposed methods, evaluated on simulations and real-world genomic pipelines, provide new mechanisms to reduce memory overruns and balance load across threads. We thereby achieve faster end-to-end execution, showcasing the potential to optimize large-scale genomic workflows.

</details>


### [14] [Can Asymmetric Tile Buffering Be Beneficial?](https://arxiv.org/abs/2511.16041)
*Chengyue Wang,Wesley Pang,Xinrui Wu,Gregory Jun,Luis Romero,Endri Taka,Diana Marculescu,Tony Nowatzki,Pranathi Vasireddy,Joseph Melber,Deming Chen,Jason Cong*

Main category: cs.DC

TL;DR: 本文提出了一种名为非对称分块缓存（ATB）的新技术，通过解耦输入与输出操作数的分块维度，显著提升了通用矩阵乘法（GEMM）在AI加速器上的性能，并在AMD XDNA2 AI引擎上实现了最高4.54倍的加速。


<details>
  <summary>Details</summary>
Motivation: 传统GEMM实现采用对称分块缓存策略，限制了算术强度和性能优化空间。作者旨在探索一种更灵活的分块方法以突破这一限制。

Method: 提出非对称分块缓存（ATB）技术，并构建包含算术强度增益与核切换开销的性能模型，用于指导ATB分块因子的选择。

Result: 在AMD XDNA2 AI Engine上，ATB将混合精度BFP16–BF16 GEMM性能从4.8 TFLOPS提升至24.6 TFLOPS，提速达4.54倍，创下该平台新纪录。

Conclusion: 非对称分块缓存是一种实用且高效的GEMM优化技术，通过合理权衡计算强度与调度开销，可显著提升AI硬件上的矩阵乘性能。

Abstract: General matrix multiplication (GEMM) is the computational backbone of modern AI workloads, and its efficiency is critically dependent on effective tiling strategies. Conventional approaches employ symmetric tile buffering, where the buffered tile size of the input $A$ along the dimension $M$ matches the output tile size of $C$.
  In this paper, we introduce asymmetric tile buffering (ATB), a simple but powerful technique that decouples the buffered tile dimensions of the input and output operands. We show, for the first time, that ATB is both practical and highly beneficial. To explain this effect, we develop a performance model that incorporates both the benefits of ATB (higher arithmetic intensity) and its overheads (higher kernel switching costs), providing insight into how to select effective ATB tiling factors. As a case study, we apply ATB to AMD's latest XDNA2 AI Engine (AIE), achieving up to a 4.54x speedup, from 4.8 to 24.6 TFLOPS on mixed-precision BFP16--BF16 GEMM, establishing a new performance record for XDNA2 AIE.

</details>


### [15] [Optimizing Communication in Byzantine Agreement Protocols with Slim-HBBFT](https://arxiv.org/abs/2511.15957)
*Nasit S Sony,Xianzhong Ding*

Main category: cs.DC

TL;DR: Slim-HBBFT 是一种新型原子广播协议，通过仅处理部分参与方的请求并引入优先可证明广播（P-PB）机制，将通信复杂度降低 O(n) 倍，同时保证异步通用子集协议所需的安全性。


<details>
  <summary>Details</summary>
Motivation: 传统异步拜占庭容错协议要求所有参与方广播其请求，即使最终只就某一方或部分请求达成一致，也会造成高昂通信开销；尤其当请求重复时，这种代价不合理。因此需要更高效的原子广播协议。

Method: 提出 Slim-HBBFT 协议，核心是优先可证明广播（P-PB）协议，仅对选定参与方生成广播证明，并以此构建原子广播协议。

Result: Slim-HBBFT 在保持安全性的前提下，将通信复杂度降低了 O(n) 倍，并满足异步通用子集协议的属性。

Conclusion: Slim-HBBFT 有效优化了异步拜占庭原子广播的效率，在请求重复或无需全网广播场景下具有显著优势，同时通过严格安全分析验证其可靠性。

Abstract: Byzantine agreement protocols in asynchronous networks have received renewed interest because they do not rely on network behavior to achieve termination. Conventional asynchronous Byzantine agreement protocols require every party to broadcast its requests (e.g., transactions), and at the end of the protocol, parties agree on one party's request. If parties agree on one party's requests while exchanging every party's request, the protocol becomes expensive. These protocols are used to design an atomic broadcast (ABC) protocol where parties agree on $\langle n-f \rangle$ parties' requests (assuming $n=3f+1$, where $n$ is the total number of parties, and $f$ is the number of Byzantine parties). Although the parties agree on a subset of requests in the ABC protocol, if the requests do not vary (are duplicated), investing in a costly protocol is not justified. We propose Slim-HBBFT, an atomic broadcast protocol that considers requests from a fraction of $n$ parties and improves communication complexity by a factor of $O(n)$. At the core of our design is a prioritized provable-broadcast (P-PB) protocol that generates proof of broadcast only for selected parties. We use the P-PB protocol to design the Slim-HBBFT atomic broadcast protocol. Additionally, we conduct a comprehensive security analysis to demonstrate that Slim-HBBFT satisfies the properties of the Asynchronous Common Subset protocol, ensuring robust security and reliability.

</details>


### [16] [Mitigating Shared Storage Congestion Using Control Theory](https://arxiv.org/abs/2511.16177)
*Thomas Collignon,Kouds Halitim,Raphaël Bleuse,Sophie Cerf,Bogdan Robu,Éric Rutten,Lionel Seinturier,Alexandre van Kempen*

Main category: cs.DC

TL;DR: 本文提出一种基于控制理论的自适应方法，通过动态调节客户端I/O速率来缓解HPC系统中的I/O拥塞问题，在真实测试平台上验证了其可减少最多20%总运行时间并降低尾部延迟。


<details>
  <summary>Details</summary>
Motivation: 传统I/O栈优化方法通常针对特定工作负载、依赖专家知识，难以泛化；在共享HPC环境中，资源争用导致性能不可预测，出现减速和超时问题。

Method: 基于控制理论设计自适应控制器，利用少量运行时系统负载指标动态调节客户端I/O速率，以减轻拥塞并提升性能稳定性。

Result: 在多节点集群的真实测试平台上评估表明，该方法有效缓解I/O拥塞，最多减少20%总运行时间，同时降低尾部延迟并保持性能稳定。

Conclusion: 所提出的自适应I/O调控方法能有效应对HPC环境中的性能波动问题，具有良好的实用性和泛化能力。

Abstract: Efficient data access in High-Performance Computing (HPC) systems is essential to the performance of intensive computing tasks. Traditional optimizations of the I/O stack aim to improve peak performance but are often workload specific and require deep expertise, making them difficult to generalize or re-use. In shared HPC environments, resource congestion can lead to unpredictable performance, causing slowdowns and timeouts. To address these challenges, we propose a self-adaptive approach based on Control Theory to dynamically regulate client-side I/O rates. Our approach leverages a small set of runtime system load metrics to reduce congestion and enhance performance stability. We implement a controller in a multi-node cluster and evaluate it on a real testbed under a representative workload. Experimental results demonstrate that our method effectively mitigates I/O congestion, reducing total runtime by up to 20% and lowering tail latency, while maintaining stable performance.

</details>


### [17] [Fast LLM Post-training via Decoupled and Best-of-N Speculation](https://arxiv.org/abs/2511.16193)
*Rongxin Cheng,Kai Zhou,Xingda Wei,Siyuan Liu,Mingcong Han,Mingjing Ai,Yeju Zhou,Baoquan Zhong,Wencong Xiao,Xin Liu,Rong Chen,Haibo Chen*

Main category: cs.DC

TL;DR: SpecActor通过动态解耦推测和动态Best-of-N推测方法，显著加速大语言模型后训练中的rollout过程，在不增加额外计算资源的情况下，比常规基线快1.3–1.7倍。


<details>
  <summary>Details</summary>
Motivation: 大语言模型后训练中rollout阶段耗时严重，而现有推测解码方法在大批量训练场景下效率低下且难以保证推测准确性。

Method: 提出SpecActor框架，包含（1）动态解耦推测执行方法，提升GPU计算效率以适配大批量场景；（2）动态Best-of-N推测方法，根据rollout进度动态选择并组合不同草稿模型，提高推测准确性。

Result: SpecActor比常规后训练基线快1.3–1.7倍，比直接应用推测解码的方法快1.3–1.5倍。

Conclusion: SpecActor有效解决了推测rollout中的两大核心挑战，在不增加计算开销的前提下显著提升了训练效率。

Abstract: Rollout dominates the training time in large language model (LLM) post-training, where the trained model is used to generate tokens given a batch of prompts. SpecActor achieves fast rollout with speculative decoding that deploys a fast path (e.g., a smaller model) to accelerate the unparallelizable generation, while the correctness is guaranteed by fast parallel verification of the outputs with the original model. SpecActor addresses two foundational challenges in speculative rollout by (1) a \emph{dynamic decoupled speculation} execution method that maximizes the GPU computational efficiency to realize speedup for large-batch execution -- a configuration common in training but unfriendly to speculative execution and (2) a \emph{dynamic Best-of-N speculation} method that selects and combines different drafting methods according to the rollout progress. It substantially improves the speculation accuracy even when the best drafting method is unknown a priori, meanwhile without requiring adding extra computation resources. {\sys} is {1.3--1.7}\,$\times$ faster than common post-training baselines, and is {1.3--1.5}\,$\times$ faster compared to naively adopting speculative decoding for rollout.

</details>


### [18] [Optimizing Federated Learning in the Era of LLMs: Message Quantization and Streaming](https://arxiv.org/abs/2511.16450)
*Ziyue Xu,Zhihong Zhang,Holger R. Roth,Chester Chen,Yan Cheng,Andrew Feng*

Main category: cs.DC

TL;DR: 本文针对大语言模型（LLM）在联邦学习（FL）中面临的通信开销和资源限制问题，提出通过消息量化和容器/文件流式传输两种技术优化NVIDIA FLARE框架，以提升FL在LLM场景下的效率与可扩展性。


<details>
  <summary>Details</summary>
Motivation: 大语言模型参数量庞大，导致联邦学习中的通信和内存压力显著增加，亟需高效传输与处理机制以支持实际部署。

Method: 在NVIDIA FLARE开源SDK基础上，引入消息量化以减小传输数据量，并采用容器/文件流式传输技术优化内存管理。

Result: 所提方法显著提升了联邦学习在大语言模型场景下的鲁棒性、效率和可扩展性，并更好地兼容现有工作流。

Conclusion: 通过量化与流式传输技术，有效缓解了大语言模型在联邦学习中的通信与资源瓶颈，为实际应用提供了可行方案。

Abstract: Federated Learning (FL) offers a promising solution for training machine learning models across distributed data sources while preserving data privacy. However, FL faces critical challenges related to communication overhead and local resource constraints, especially in the era of Large Language Models (LLMs) with billions of parameters. The sheer size of these models exacerbates both memory and communication constraints, making efficient transmission and processing essential for practical deployment. NVIDIA FLARE, an open-source SDK for federated learning, addresses these challenges by introducing advanced communication capabilities. Building upon existing solutions for large object streaming, we enhance FL workflows for LLMs through two key techniques: message quantization and container/file streaming. Quantization reduces message size, while streaming enables efficient memory management, improving scalability and integration with existing workflows. These advancements significantly enhance the robustness and efficiency of FL with LLMs, ensuring better performance in real-world federated learning scenarios.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [19] [Green Distributed AI Training: Orchestrating Compute Across Renewable-Powered Micro Datacenters](https://arxiv.org/abs/2511.16182)
*Giuseppe Tomei,Andrea Mayer,Giuseppe Alcini,Stefano Salsano*

Main category: cs.NI

TL;DR: 本文提出一种基于可再生能源的分布式微数据中心架构，通过可行性感知的实时迁移机制，使AI工作负载动态追踪绿色能源盈余，在降低非可再生能耗的同时提升性能稳定性。


<details>
  <summary>Details</summary>
Motivation: 当前集中式数据中心在能源比例性和地理灵活性方面难以适配以间歇性可再生能源为主的能源格局，导致大量零碳能源被弃用。

Method: 构建一个形式化的可行性域模型，将检查点大小、广域带宽和可再生窗口时长关联起来，并据此设计可行性感知的编排框架，实现AI工作负载的动态迁移。

Result: 基于真实轨迹的评估表明，该方法能同时减少非可再生能源使用并提高性能稳定性，优于纯能耗驱动策略；扩展部分还探讨了面向未来的可再生能源感知AI基础设施架构。

Conclusion: 可行性感知迁移是构建未来AI计算范式的关键基石，可使AI执行具备流动性、地理适应性，并与可再生能源供应对齐。

Abstract: The accelerating expansion of AI workloads is colliding with an energy landscape increasingly dominated by intermittent renewable generation. While vast quantities of zero-carbon energy are routinely curtailed, today's centralized datacenter architectures remain poorly matched to this reality in both energy proportionality and geographic flexibility. This work envisions a shift toward a distributed fabric of renewable-powered micro-datacenters that dynamically follow the availability of surplus green energy through live workload migration.
  At the core of this vision lies a formal feasibility-domain model that delineates when migratory AI computation is practically achievable. By explicitly linking checkpoint size, wide-area bandwidth, and renewable-window duration, the model reveals that migration is almost always energetically justified, and that time-not energy-is the dominant constraint shaping feasibility. This insight enables the design of a feasibility-aware orchestration framework that transforms migration from a best-effort heuristic into a principled control mechanism. Trace-driven evaluation shows that such orchestration can simultaneously reduce non-renewable energy use and improve performance stability, overcoming the tradeoffs of purely energy-driven strategies.
  Beyond the immediate feasibility analysis, the extended version explores the architectural horizon of renewable-aware AI infrastructures. It examines the role of emerging ultra-efficient GPU-enabled edge platforms, anticipates integration with grid-level control and demand-response ecosystems, and outlines paths toward supporting partially migratable and distributed workloads. The work positions feasibility-aware migration as a foundational building block for a future computing paradigm in which AI execution becomes fluid, geographically adaptive, and aligned with renewable energy availability.

</details>
