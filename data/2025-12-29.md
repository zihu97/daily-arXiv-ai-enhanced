<div id=toc></div>

# Table of Contents

- [cs.NI](#cs.NI) [Total: 1]
- [cs.MA](#cs.MA) [Total: 2]
- [cs.AR](#cs.AR) [Total: 1]
- [cs.OS](#cs.OS) [Total: 1]
- [cs.SE](#cs.SE) [Total: 12]
- [cs.DC](#cs.DC) [Total: 7]


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [1] [Physics-informed Diffusion Models for Multi-scale Prediction of Reference Signal Received Power in Wireless Networks](https://arxiv.org/abs/2512.21475)
*Xiaoqian Qi,Haoye Chai,Yue Wang,Zhaocheng Wang,Yong Li*

Main category: cs.NI

TL;DR: 提出Channel-Diff框架，结合物理先验与扩散模型，显著提升RSRP预测精度与泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有RSRP预测方法因忽略大小尺度衰减耦合及缺乏物理先验，导致精度与可解释性不足。

Method: 构建物理信息引导的条件扩散模型，融合大尺度传播建模、遮挡建模与多径传播建模，设计两阶段训练机制实现多尺度物理知识融合。

Result: 相比基线方法，预测精度提升25.15%-37.19%，同时具备优异的迁移性与训练效率。

Conclusion: Channel-Diff有效结合物理建模与数据驱动，为无线信道预测提供高精度、可解释的新范式。

Abstract: The Reference Signal Received Power (RSRP) is a crucial factor that determines communication performance in mobile networks. Accurately predicting the RSRP can help network operators perceive user experiences and maximize throughput by optimizing wireless resources. However, existing research into RSRP prediction has limitations in accuracy and verisimilitude. Theoretical derivations and existing data-driven methods consider only easily quantifiable Large-Scale (LS) information, and struggle to effectively capture the intertwined LS and Small-Scale (SS) signal attenuation characteristics of the wireless channel. Moreover, the lack of prior physical knowledge leads to weak accuracy, interpretability, and transferability. In this paper, we propose a novel RSRP prediction framework, Channel-Diff. This framework physically models LS and SS attenuation using multimodal conditions and employs physics-informed conditional diffusion models as the prediction network. Channel-Diff extracts prior physical information that characterises the signal propagation process from network parameters and multi-attribute maps of the urban spatial environment. It provides LS physical priors through large-scale propagation modelling and shadow-occlusion modelling, and SS physical priors through multipath propagation modelling and urban microenvironment feature extraction. We design a physical-prior-guided two-stage training scheme with a noise prior guidance mechanism, enabling effective fusion of multi-scale physical knowledge with the diffusion models. Evaluations demonstrate Channel-Diff exhibits excellent performance on RSRP prediction, achieving at least 25.15%-37.19% improvement in accuracy relative to baseline methods. Additionally, the model also demonstrated outstanding performance in terms of transferability and training efficiency.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [2] [The AI Committee: A Multi-Agent Framework for Automated Validation and Remediation of Web-Sourced Data](https://arxiv.org/abs/2512.21481)
*Sunith Vallabhaneni,Thomas Berkane,Maimuna Majumder*

Main category: cs.MA

TL;DR: 本文提出了一种名为AI Committee的多智能体系统，用于自动化验证和修复从网页收集的研究数据集，显著提升了数据完整性和准确性。


<details>
  <summary>Details</summary>
Motivation: 现有基于大语言模型的网页智能体在数据收集中常出现幻觉、遗漏或语义误解等问题，难以手动检测和修正，亟需自动化解决方案。

Method: 设计了一个模型无关的多智能体系统，各智能体分别负责数据质量保障流程中的不同任务，如来源审查、事实核查、数据修复等，并结合上下文学习、链式推理与自校正机制。

Result: 在三个真实数据集上测试表明，该系统跨模型通用性强，数据完整性最高达78.7%，精确度达100%，优于基线方法。

Conclusion: AI Committee有效解决了网页数据采集中的质量问题，作为开源工具可广泛支持科研社区。

Abstract: Many research areas rely on data from the web to gain insights and test their methods. However, collecting comprehensive research datasets often demands manually reviewing many web pages to identify and record relevant data points, which is labor-intensive and susceptible to error. While the emergence of large language models (LLM)-powered web agents has begun to automate parts of this process, they often struggle to ensure the validity of the data they collect. Indeed, these agents exhibit several recurring failure modes - including hallucinating or omitting values, misinterpreting page semantics, and failing to detect invalid information - which are subtle and difficult to detect and correct manually. To address this, we introduce the AI Committee, a novel model-agnostic multi-agent system that automates the process of validating and remediating web-sourced datasets. Each agent is specialized in a distinct task in the data quality assurance pipeline, from source scrutiny and fact-checking to data remediation and integrity validation. The AI Committee leverages various LLM capabilities - including in-context learning for dataset adaptation, chain-of-thought reasoning for complex semantic validation, and a self-correction loop for data remediation - all without task-specific training. We demonstrate the effectiveness of our system by applying it to three real-world datasets, showing that it generalizes across LLMs and significantly outperforms baseline approaches, achieving data completeness up to 78.7% and precision up to 100%. We additionally conduct an ablation study demonstrating the contribution of each agent to the Committee's performance. This work is released as an open-source tool for the research community.

</details>


### [3] [MASFIN: A Multi-Agent System for Decomposed Financial Reasoning and Forecasting](https://arxiv.org/abs/2512.21878)
*Marc S. Montalvo,Hamed Yaghoobian*

Main category: cs.MA

TL;DR: MASFIN是一个结合大语言模型与金融数据的多智能体框架，通过偏差缓解机制提升金融预测的透明性与可复现性，在八周测试中实现7.33%累计收益。


<details>
  <summary>Details</summary>
Motivation: 解决传统量化方法中的幸存者偏差及AI驱动方法在信号整合、可复现性和计算效率方面的不足。

Method: 构建模块化多智能体框架MASFIN，集成GPT-4.1-nano模型，结合结构化财务指标与非结构化新闻数据，优化短期投资组合分配。

Result: 八周评估期内累计回报率达7.33%，六周内超越三大股指基准，但波动性较高。

Conclusion: 偏差感知的生成式AI框架在金融预测中展现潜力，模块化多智能体设计有助于推动量化金融向实用、透明和可复现方向发展。

Abstract: Recent advances in large language models (LLMs) are transforming data-intensive domains, with finance representing a high-stakes environment where transparent and reproducible analysis of heterogeneous signals is essential. Traditional quantitative methods remain vulnerable to survivorship bias, while many AI-driven approaches struggle with signal integration, reproducibility, and computational efficiency. We introduce MASFIN, a modular multi-agent framework that integrates LLMs with structured financial metrics and unstructured news, while embedding explicit bias-mitigation protocols. The system leverages GPT-4.1-nano for reproducability and cost-efficient inference and generates weekly portfolios of 15-30 equities with allocation weights optimized for short-term performance. In an eight-week evaluation, MASFIN delivered a 7.33% cumulative return, outperforming the S&P 500, NASDAQ-100, and Dow Jones benchmarks in six of eight weeks, albeit with higher volatility. These findings demonstrate the promise of bias-aware, generative AI frameworks for financial forecasting and highlight opportunities for modular multi-agent design to advance practical, transparent, and reproducible approaches in quantitative finance.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [4] [Prefill vs. Decode Bottlenecks: SRAM-Frequency Tradeoffs and the Memory-Bandwidth Ceiling](https://arxiv.org/abs/2512.22066)
*Hannah Atmer,Yuan Yao,Thiemo Voigt,Stefanos Kaxiras*

Main category: cs.AR

TL;DR: 优化LLM推理能效需平衡SRAM大小与运行频率，小缓存（32KB-64KB）配合高频率（1200MHz-1400MHz）可实现最佳能耗延迟积。


<details>
  <summary>Details</summary>
Motivation: 降低大语言模型部署的能耗成本与环境影响，指导高效硬件加速器设计。

Method: 结合OpenRAM、LLMCompass与ScaleSIM进行能耗建模、延迟仿真与计算强度分析。

Result: SRAM容量主导总能耗，高频率虽提升预填充阶段性能，但在解码阶段受限于内存带宽；高频率反而因缩短执行时间降低静态能耗，整体更节能。

Conclusion: 推荐采用小缓存+高频率配置，在满足内存带宽约束下实现最优能效与延迟平衡，为数据中心LLM加速器提供架构设计依据。

Abstract: Energy consumption dictates the cost and environmental impact of deploying Large Language Models. This paper investigates the impact of on-chip SRAM size and operating frequency on the energy efficiency and performance of LLM inference, focusing on the distinct behaviors of the compute-bound prefill and memory-bound decode phases. Our simulation methodology combines OpenRAM for energy modeling, LLMCompass for latency simulation, and ScaleSIM for systolic array operational intensity. Our findings show that total energy use is predominantly determined by SRAM size in both phases, with larger buffers significantly increasing static energy due to leakage, which is not offset by corresponding latency benefits. We quantitatively explore the memory-bandwidth bottleneck, demonstrating that while high operating frequencies reduce prefill latency, their positive impact on memory-bound decode latency is capped by the external memory bandwidth. Counter-intuitively, high compute frequency can reduce total energy by reducing execution time and consequently decreasing static energy consumption more than the resulting dynamic power increase. We identify an optimal hardware configuration for the simulated workload: high operating frequencies (1200MHz-1400MHz) and a small local buffer size of 32KB to 64KB. This combination achieves the best energy-delay product, balancing low latency with high energy efficiency. Furthermore, we demonstrate how memory bandwidth acts as a performance ceiling, and that increasing compute frequency only yields performance gains up to the point where the workload becomes memory-bound. This analysis provides concrete architectural insights for designing energy-efficient LLM accelerators, especially for datacenters aiming to minimize their energy overhead.

</details>


<div id='cs.OS'></div>

# cs.OS [[Back]](#toc)

### [5] [LEFT-RS: A Lock-Free Fault-Tolerant Resource Sharing Protocol for Multicore Real-Time Systems](https://arxiv.org/abs/2512.21701)
*Nan Chen,Xiaotian Dai,Tong Cheng,Alan Burns,Iain Bate,Shuai Zhao*

Main category: cs.OS

TL;DR: 提出LEFT-RS协议，通过无锁并发访问提升多核实时系统资源容错性与调度性能。


<details>
  <summary>Details</summary>
Motivation: 现有锁定协议无法有效应对临界区瞬态故障，传统容错技术增加阻塞，需更高效方案。

Method: 设计LEFT-RS协议，允许多任务并行进入临界区、并发读取全局资源，故障时提前完成正常任务以减少开销。

Result: 实验表明LEFT-RS平均提升84.5%可调度性，显著优于现有方法。

Conclusion: LEFT-RS在保证时序前提下，有效提升多核实时系统资源访问效率与容错能力。

Abstract: Emerging real-time applications have driven the transition to multicore embedded systems, where tasks must share resources due to functional demands and limited availability. These resources, whether local or global, are protected within critical sections to prevent race conditions, with locking protocols ensuring both exclusive access and timing requirements. However, transient faults occurring within critical sections can disrupt execution and propagate errors across multiple tasks. Conventional locking protocols fail to address such faults, and integrating traditional fault tolerance techniques often increases blocking. Recent approaches improve fault recovery through parallel replica execution; however, challenges remain due to sequential accessing, coordination overhead, and susceptibility to common-mode faults. In this paper, we propose a Lock-frEe Fault-Tolerant Resource Sharing (LEFT-RS) protocol for multicore real-time systems. LEFT-RS allows tasks to concurrently access and read global resources while entering their critical sections in parallel. Each task can complete its access earlier upon successful execution if other tasks experience faults, thereby improving the efficiency of resource usage. Our design also limits the overhead and enhances fault resilience. We present a comprehensive worst-case response time analysis to ensure timing guarantees. Extensive evaluation results demonstrate that our method significantly outperforms existing approaches, achieving up to an 84.5% improvement in schedulability on average.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [6] [Understanding the Role of Large Language Models in Software Engineering: Evidence from an Industry Survey](https://arxiv.org/abs/2512.21347)
*Vítor Mateus de Brito,Kleinner Farias*

Main category: cs.SE

TL;DR: 本研究通过调查46位行业从业者，探讨大语言模型在软件工程中的实际应用与影响，揭示其提升效率的同时也带来认知依赖与安全风险。


<details>
  <summary>Details</summary>
Motivation: 理解大语言模型如何融入开发者日常工作，弥合学术研究与工业实践之间的差距。

Method: 对46名具有不同教育背景和经验水平的行业专业人士进行问卷调查，收集实证数据。

Result: 受访者普遍认可LLM在加速技术问题解决、改进文档支持和代码标准化方面的价值，但也担忧认知依赖、安全风险和技术自主性丧失。

Conclusion: 需以批判性和监督方式使用LLM工具，为开发者和研究者提供负责任、安全且高效的采纳路径，并推动未来对其认知、伦理与组织影响的研究。

Abstract: The rapid advancement of Large Language Models (LLMs) is reshaping software engineering by profoundly influencing coding, documentation, and system maintenance practices. As these tools become deeply embedded in developers' daily workflows, understanding how they are used has become essential. This paper reports an empirical study of LLM adoption in software engineering, based on a survey of 46 industry professionals with diverse educational backgrounds and levels of experience. The results reveal positive perceptions of LLMs, particularly regarding faster resolution of technical questions, improved documentation support, and enhanced source code standardization. However, respondents also expressed concerns about cognitive dependence, security risks, and the potential erosion of technical autonomy. These findings underscore the need for critical and supervised use of LLM-based tools. By grounding the discussion in empirical evidence from industry practice, this study bridges the gap between academic discourse and real-world software development. The results provide actionable insights for developers and researchers seeking to adopt and evolve LLM-based technologies in a more effective, responsible, and secure manner, while also motivating future research on their cognitive, ethical, and organizational implications.

</details>


### [7] [CosmoCore-Evo: Evolutionary Dream-Replay Reinforcement Learning for Adaptive Code Generation](https://arxiv.org/abs/2512.21351)
*Santhosh Kumar Ravindran*

Main category: cs.SE

TL;DR: CosmoCore-Evo 结合进化算法与强化学习，在代码生成任务中提升适应性与新颖性，显著优于原版及主流基线。


<details>
  <summary>Details</summary>
Motivation: 受人类进化中自然选择启发，旨在突破现有模型在分布偏移环境下的行为固化问题。

Method: 将RL轨迹视为基因组，在梦境重放阶段施加变异与选择，引入企业级适应度函数评估效率、合规与扩展性。

Result: 在多个基准测试中，方案新颖性提升35%，适应速度加快25%。

Conclusion: 进化机制有效弥合LLM智能体的感知鸿沟，增强其应对动态环境的能力。

Abstract: Building on the affective dream-replay reinforcement learning framework of CosmoCore, we introduce CosmoCore-Evo, an extension that incorporates evolutionary algorithms to enhance adaptability and novelty in code generation tasks. Inspired by anthropological aspects of human evolution, such as natural selection and adaptation in early hominids, CosmoCore-Evo treats RL trajectories as ``genomes'' that undergo mutation and selection during the nocturnal replay phase. This mechanism allows agents to break free from trained patterns, fostering emergent behaviors and improved performance in distribution-shifted environments, such as changing APIs or novel libraries. We augment the Dream Queue with evolutionary operations, including mutation of high-fitness trajectories and enterprise-tuned fitness functions that incorporate efficiency, compliance, and scalability metrics. Evaluated on extended benchmarks including HumanEval variants with shifts, BigCodeBench, and a custom PySpark pipeline simulation, CosmoCore-Evo achieves up to 35% higher novelty in solutions and 25% faster adaptation compared to the original CosmoCore and baselines like PPO and REAMER. Ablations confirm the role of evolutionary components in bridging the sentient gap for LLM agents. Code for replication, including a toy simulation, is provided.

</details>


### [8] [Multi-Agent LLM Committees for Autonomous Software Beta Testing](https://arxiv.org/abs/2512.21352)
*Sumanth Bharadwaj Hachalli Karanam,Dhiwahar Adhithya Kennady*

Main category: cs.SE

TL;DR: 提出多智能体委员会框架，通过视觉增强的LLM协作与三轮投票机制，显著提升Web应用测试成功率与效率。


<details>
  <summary>Details</summary>
Motivation: 解决人工测试成本高、单智能体LLM易产生幻觉和行为不一致的问题。

Method: 采用多样化的视觉增强LLM智能体，结合角色驱动行为差异与三轮投票协议，在Web应用中系统化探索并达成共识。

Result: 在84次实验中实现89.5%任务成功率，2-4智能体配置达91.7%-100%，优于单智能体基线78.0%；动作级成功率达93.1%，延迟中位数0.71秒；在WebShop和OWASP基准分别达74.7%和82.0%成功率，F1分数0.91。

Conclusion: 该框架显著提升自动化测试性能，支持CI/CD集成，具备开源可复现性与实用部署价值。

Abstract: Manual software beta testing is costly and time-consuming, while single-agent large language model (LLM) approaches suffer from hallucinations and inconsistent behavior. We propose a multi-agent committee framework in which diverse vision-enabled LLMs collaborate through a three-round voting protocol to reach consensus on testing actions. The framework combines model diversity, persona-driven behavioral variation, and visual user interface understanding to systematically explore web applications. Across 84 experimental runs with 9 testing personas and 4 scenarios, multi-agent committees achieve an 89.5 percent overall task success rate. Configurations with 2 to 4 agents reach 91.7 to 100 percent success, compared to 78.0 percent for single-agent baselines, yielding improvements of 13.7 to 22.0 percentage points. At the action level, the system attains a 93.1 percent success rate with a median per-action latency of 0.71 seconds, enabling real-time and continuous integration testing. Vision-enabled agents successfully identify user interface elements, with navigation and reporting achieving 100 percent success and form filling achieving 99.2 percent success. We evaluate the framework on WebShop and OWASP benchmarks, achieving 74.7 percent success on WebShop compared to a 50.1 percent published GPT-3 baseline, and 82.0 percent success on OWASP Juice Shop security testing with coverage of 8 of the 10 OWASP Top 10 vulnerability categories. Across 20 injected regressions, the committee achieves an F1 score of 0.91 for bug detection, compared to 0.78 for single-agent baselines. The open-source implementation enables reproducible research and practical deployment of LLM-based software testing in CI/CD pipelines.

</details>


### [9] [What Makes a GitHub Issue Ready for Copilot?](https://arxiv.org/abs/2512.21426)
*Mohammed Sayagh*

Main category: cs.SE

TL;DR: 本文提出32项详细标准，用于评估GitHub议题质量，以提升AI助手（如Copilot）生成可合并PR的成功率，并构建可解释的机器学习模型预测议题合并可能性，AUC达72%。


<details>
  <summary>Details</summary>
Motivation: 当前GitHub议题常因描述不清或范围模糊导致AI助手无法生成有效实现，现有最佳实践建议过于笼统，亟需系统化质量评估标准。

Method: 通过对比最终合并与关闭的PR所对应的议题，提炼32项质量指标，构建可解释机器学习模型预测议题是否可能导致合并PR。

Result: 模型中位AUC为72%，发现成功合并的议题通常更简短、范围明确、含清晰指引及关联工件提示；含外部依赖或API引用的议题合并率较低。

Conclusion: 高质量议题撰写应作为AI协作时代的一等软件工程活动，本研究提供可操作的质量指标，为未来相关研究奠定基础。

Abstract: AI-agents help developers in different coding tasks, such as developing new features, fixing bugs, and reviewing code. Developers can write a Github issue and assign it to an AI-agent like Copilot for implementation. Based on the issue and its related discussion, the AI-agent performs a plan for the implementation, and executes it. However, the performance of AI-agents and LLMs heavily depends on the input they receive. For instance, a GitHub issue that is unclear or not well scoped might not lead to a successful implementation that will eventually be merged. GitHub Copilot provides a set of best practice recommendations that are limited and high-level. In this paper, we build a set of 32 detailed criteria that we leverage to measure the quality of GitHub issues to make them suitable for AI-agents. We compare the GitHub issues that lead to a merged pull request versus closed pull request. Then, we build an interpretable machine learning model to predict the likelihood of a GitHub issue resulting in a merged pull request. We observe that pull requests that end up being merged are those originating from issues that are shorter, well scoped, with clear guidance and hints about the relevant artifacts for an issue, and with guidance on how to perform the implementation. Issues with external references including configuration, context setup, dependencies or external APIs are associated with lower merge rates. We built an interpretable machine learning model to help users identify how to improve a GitHub issue to increase the chances of the issue resulting in a merged pull request by Copilot. Our model has a median AUC of 72\%. Our results shed light on quality metrics relevant for writing GitHub issues and motivate future studies further investigate the writing of GitHub issues as a first-class software engineering activity in the era of AI-teammates.

</details>


### [10] [Cerberus: Multi-Agent Reasoning and Coverage-Guided Exploration for Static Detection of Runtime Errors](https://arxiv.org/abs/2512.21431)
*Hridya Dhulipala,Xiaokai Rong,Tien N. Nguyen*

Main category: cs.SE

TL;DR: Cerberus是一种无需执行代码即可预测并检测运行时错误的测试框架，结合LLM生成高覆盖率测试用例，显著提升错误发现效率。


<details>
  <summary>Details</summary>
Motivation: 在软件开发中，需在不实际运行代码的情况下提前检测在线代码片段中的运行时异常，以保障代码集成安全。

Method: 提出两阶段反馈循环框架Cerberus，利用LLM生成触发错误的输入，并预测代码覆盖与错误，第一阶段兼顾覆盖率与错误检测，第二阶段专注错误检测。

Result: 实验证明Cerberus相比传统和学习型测试方法，在完整或不完整代码片段上能更高效生成高覆盖率测试用例，发现更多运行时错误。

Conclusion: Cerberus为无执行环境下的代码异常检测提供了一种高效、智能的新方案，优于直接提示LLM同时完成覆盖与错误检测的做法。

Abstract: In several software development scenarios, it is desirable to detect runtime errors and exceptions in code snippets without actual execution. A typical example is to detect runtime exceptions in online code snippets before integrating them into a codebase. In this paper, we propose Cerberus, a novel predictive, execution-free coverage-guided testing framework. Cerberus uses LLMs to generate the inputs that trigger runtime errors and to perform code coverage prediction and error detection without code execution. With a two-phase feedback loop, Cerberus first aims to both increasing code coverage and detecting runtime errors, then shifts to focus only detecting runtime errors when the coverage reaches 100% or its maximum, enabling it to perform better than prompting the LLMs for both purposes. Our empirical evaluation demonstrates that Cerberus performs better than conventional and learning-based testing frameworks for (in)complete code snippets by generating high-coverage test cases more efficiently, leading to the discovery of more runtime errors.

</details>


### [11] [Fuzzwise: Intelligent Initial Corpus Generation for Fuzzing](https://arxiv.org/abs/2512.21440)
*Hridya Dhulipala,Xiaokai Rong,Aashish Yadavally,Tien N. Nguyen*

Main category: cs.SE

TL;DR: FuzzWise 使用多智能体 LLM 框架，在生成初始种子语料时同步评估覆盖率，减少冗余测试用例并提升效率。


<details>
  <summary>Details</summary>
Motivation: 传统灰盒模糊测试需分阶段生成与精简语料，效率低且耗资源。

Method: 采用两个 LLM 智能体：一个生成测试用例，另一个预测代码覆盖率，避免实际执行。

Result: 相比基线方法，FuzzWise 生成更少测试用例，却实现更高覆盖率、更多错误触发，且更省时高效。

Conclusion: FuzzWise 能有效优化初始语料构建过程，兼顾质量与效率，适用于执行受限场景。

Abstract: In mutation-based greybox fuzzing, generating high-quality input seeds for the initial corpus is essential for effective fuzzing. Rather than conducting separate phases for generating a large corpus and subsequently minimizing it, we propose FuzzWise which integrates them into one process to generate the optimal initial corpus of seeds (ICS). FuzzWise leverages a multi-agent framework based on Large Language Models (LLMs). The first LLM agent generates test cases for the target program. The second LLM agent, which functions as a predictive code coverage module, assesses whether each generated test case will enhance the overall coverage of the current corpus. The streamlined process allows each newly generated test seed to be immediately evaluated for its contribution to the overall coverage. FuzzWise employs a predictive approach using an LLM and eliminates the need for actual execution, saving computational resources and time, particularly in scenarios where the execution is not desirable or even impossible. Our empirical evaluation demonstrates that FuzzWise generates significantly fewer test cases than baseline methods. Despite the lower number of test cases, FuzzWise achieves high code coverage and triggers more runtime errors compared to the baselines. Moreover, it is more time-efficient and coverage-efficient in producing an initial corpus catching more errors.

</details>


### [12] [Code Clone Refactoring in C# with Lambda Expressions](https://arxiv.org/abs/2512.21511)
*Takuto Kawamoto,Yoshiki Higo*

Main category: cs.SE

TL;DR: 提出一种针对C#的使用Lambda表达式进行代码克隆合并的Extract Method重构方法，并通过实验验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有行为参数化研究多集中于Java，缺乏对其他语言如C#的针对性探索，而语言特性影响重构效果。

Method: 基于Lambda表达式设计C#专用重构技术，利用NiCad检测出的代码克隆对进行评估与实际重构尝试。

Result: 在2217个克隆对中，35.0%被判定为可重构，其中28.9%成功完成重构。

Conclusion: 该方法在C#中具备一定实用性，表明编程语言特性对Extract Method重构策略有显著影响。

Abstract: "Extract Method" refactoring is a technique for consolidating code clones. Parameterization approaches are used to extract a single method from multiple code clones that contain differences. This approach parameterizes expressions and behaviors within a method. In particular, behavior parameterization has been extensively studied in Java programs, but little research has been conducted on other programming languages.
  Lambda expressions can be used to parameterize behaviors, but the specifications of each programming language significantly affect the applicability of this technique. Therefore, the optimal "Extract Method" approach may vary depending on the programming language.
  In this study, we propose a C#-specific technique that uses lambda expressions to analyze and consolidate code clones. We evaluated our proposed method by applying it to code clones detected by the NiCad clone detector and measuring how many of them could be successfully consolidated.
  In total, 2,217 clone pairs from 22 projects were included in our evaluation. For the clone pairs determined to be refactorable, we also attempted refactoring actually. The proposed approach determined that 35.0% of all clone pairs were suitable for refactoring. Among these, 28.9% were successfully refactored.

</details>


### [13] [Co-Evolution of Types and Dependencies: Towards Repository-Level Type Inference for Python Code](https://arxiv.org/abs/2512.21591)
*Shuo Sun,Shixin Zhang,Jiwei Yan,Jun Yan,Jian Zhang*

Main category: cs.SE

TL;DR: 提出基于LLM的\methodName方法，通过类型与依赖协同演化实现仓库级类型推断，显著提升推断准确率并减少工具引入的新错误。


<details>
  <summary>Details</summary>
Motivation: Python动态类型机制导致大型项目运行时类型错误频发，现有工具难以处理跨过程依赖，亟需仓库级类型推断方案。

Method: 构建实体依赖图（EDG）建模仓库内对象与类型依赖，迭代优化类型与依赖关系，并结合类型检查器实时校正推断结果。

Result: 在12个复杂Python仓库上，\methodName取得TypeSim 0.89和TypeExact 0.84，较最强基线相对提升27%和40%，且减少92.7%工具引入的新类型错误。

Conclusion: \methodName为现实Python项目提供了高精度、低错误的自动化类型标注能力，是迈向可靠类型推断的重要突破。

Abstract: Python's dynamic typing mechanism, while promoting flexibility, is a significant source of runtime type errors that plague large-scale software, which inspires the automatic type inference techniques. Existing type inference tools have achieved advances in type inference within isolated code snippets. However, repository-level type inference remains a significant challenge, primarily due to the complex inter-procedural dependencies that are difficult to model and resolve. To fill this gap, we present \methodName, a novel approach based on LLMs that achieves repository-level type inference through the co-evolution of types and dependencies. \methodName~constructs an Entity Dependency Graph (EDG) to model the objects and type dependencies across the repository. During the inference process, it iteratively refines types and dependencies in EDG for accurate type inference. Our key innovations are: (1) an EDG model designed to capture repository-level type dependencies; (2) an iterative type inference approach where types and dependencies co-evolve in each iteration; and (3) a type-checker-in-the-loop strategy that validates and corrects inferences on-the-fly, thereby reducing error propagation. When evaluated on 12 complex Python repositories, \methodName~significantly outperformed prior works, achieving a \textit{TypeSim} score of 0.89 and a \textit{TypeExact} score of 0.84, representing a 27\% and 40\% relative improvement over the strongest baseline. More importantly, \methodName~removed new type errors introduced by the tool by 92.7\%. This demonstrates a significant leap towards automated, reliable type annotation for real-world Python development.

</details>


### [14] [The State of the SBOM Tool Ecosystems: A Comparative Analysis of SPDX and CycloneDX](https://arxiv.org/abs/2512.21781)
*Abdul Ali Bangash,Tongxu Ge,Zhimin Zhao,Arshdeep Singh,Zitao Wang,Bram Adams*

Main category: cs.SE

TL;DR: 本文定量比较了SPDX与CycloneDX两种SBOM格式的工具生态成熟度、社区活跃度及项目健康指标，揭示各自优势并提出协同改进方向。


<details>
  <summary>Details</summary>
Motivation: 评估SBOM两大主流格式的生态系统差异，为开发者和实践者提供选型与优化依据。

Method: 分析170个公开工具用例、641个工具健康指标、36,990条开源问题报告及Top 250项目数据。

Result: CycloneDX项目开发者参与度更高，SPDX生态更成熟且工具更丰富。

Conclusion: 两种生态互补性强，存在相互增强机会，可指导未来工具开发与社区协作。

Abstract: A Software Bill of Materials (SBOM) provides transparency by documenting software component metadata and dependencies. However, SBOM adoption depends on tool ecosystems. With two dominant formats: SPDX and CycloneDX - the ecosystems vary significantly in maturity, tool support, and community engagement. We conduct a quantitative comparison of use cases for 170 publicly advertised SBOM tools, identifying enhancement areas for each format. We compare health metrics of both ecosystems (171 CycloneDX versus 470 SPDX tools) to evaluate robustness and maturity. We quantitatively compare 36,990 issue reports from open-source tools to identify challenges and development opportunities. Finally, we investigate the top 250 open-source projects using each tool ecosystem and compare their health metrics. Our findings reveal distinct characteristics: projects using CycloneDX tools demonstrate higher developer engagement and certain health indicators, while SPDX tools benefit from a more mature ecosystem with broader tool availability and established industry adoption. This research provides insights for developers, contributors, and practitioners regarding complementary strengths of these ecosystems and identifies opportunities for mutual enhancement.

</details>


### [15] [Proceedings First Workshop on Adaptable Cloud Architectures](https://arxiv.org/abs/2512.22054)
*Giuseppe De Palma,Saverio Giallorenzo*

Main category: cs.SE

TL;DR: WACA 2025会议论文集，与DisCoTec 2025联合举办。


<details>
  <summary>Details</summary>
Motivation: 推动可适应云架构的研究与交流。

Method: 组织专题研讨会并收录相关论文。

Result: 汇集了关于可适应云架构的最新研究成果。

Conclusion: 为分布式计算技术领域提供有价值的参考。

Abstract: This volume contains the post-proceedings of the Workshop on Adaptable Cloud Architectures (WACA 2025), held on June 20, 2025, in Lille, France, co-located with DisCoTec 2025 - 20th International Federated Conference on Distributed Computing Techniques.

</details>


### [16] [A Story About Cohesion and Separation: Label-Free Metric for Log Parser Evaluation](https://arxiv.org/abs/2512.21811)
*Qiaolin Qin,Jianchen Zhao,Heng Li,Weiyi Shang,Ettore Merlo*

Main category: cs.SE

TL;DR: 提出一种无标签的日志解析器评估指标PMSS，通过中位数轮廓分析和Levenshtein距离评估解析性能，与现有标签指标高度相关，适用于无标注或标注不一致场景。


<details>
  <summary>Details</summary>
Motivation: 现有评估指标依赖标注数据，导致评估受限且结论不一致，需开发无需标签的通用评估方法。

Method: 设计PMSS指标，结合中位数轮廓分析与Levenshtein距离，在近线性时间复杂度内评估日志解析器的分组与模板质量。

Result: 在Loghub 2.0数据集上验证，PMSS与FGA/FTA显著正相关（Spearman系数分别为0.648和0.587），高PMSS得分解析器与高FGA得分解析器平均差异仅2.1%。

Conclusion: PMSS为无标注或标注不一致时提供可靠评估替代方案，并给出实际选型指导。

Abstract: Log parsing converts log messages into structured event templates, allowing for automated log analysis and reducing manual inspection effort. To select the most compatible parser for a specific system, multiple evaluation metrics are commonly used for performance comparisons. However, existing evaluation metrics heavily rely on labeled log data, which limits prior studies to a fixed set of datasets and hinders parser evaluations and selections in the industry. Further, we discovered that different versions of ground-truth used in existing studies can lead to inconsistent performance conclusions. Motivated by these challenges, we propose a novel label-free template-level metric, PMSS (parser medoid silhouette score), to evaluate log parser performance. PMSS evaluates both parser grouping and template quality with medoid silhouette analysis and Levenshtein distance within a near-linear time complexity in general. To understand its relationship with label-based template-level metrics, FGA and FTA, we compared their evaluation outcomes for six log parsers on the standard corrected Loghub 2.0 dataset. Our results indicate that log parsers achieving the highest PMSS or FGA exhibit comparable performance, differing by only 2.1% on average in terms of the FGA score; the difference is 9.8% for FTA. PMSS is also significantly (p<1e-8) and positively correlated to both FGA and FTA: the Spearman's rho correlation coefficient of PMSS-FGA and PMSS-FTA are respectively 0.648 and 0.587, close to the coefficient between FGA and FTA (0.670). We further extended our discussion on how to interpret the conclusions from different metrics, identifying challenges in using PMSS, and provided guidelines on conducting parser selections with our metric. PMSS provides a valuable evaluation alternative when ground-truths are inconsistent or labels are unavailable.

</details>


### [17] [HALF: Process Hollowing Analysis Framework for Binary Programs with the Assistance of Kernel Modules](https://arxiv.org/abs/2512.22043)
*Zhangbo Long,Letian Sha,Jiaye Pan,Dongpeng Xu,Yifei Huang,Fu Xiao*

Main category: cs.SE

TL;DR: 提出一种基于内核模块和进程镂空技术的二进制程序分析框架，提升细粒度分析的可用性与性能。


<details>
  <summary>Details</summary>
Motivation: 现有细粒度二进制分析存在部署困难、内存占用高、性能开销大等问题，需适配新场景如内存漏洞利用和沙箱逃逸恶意代码分析。

Method: 结合内核模块扩展传统动态二进制插桩能力，通过进程镂空技术在容器进程中构建解耦分析环境，复用现有平台功能并降低对目标程序影响。

Result: 在Windows平台实现原型，通过大量基准与真实程序实验验证框架有效性与性能，并成功分析实际漏洞利用与恶意代码。

Conclusion: 该框架显著提升细粒度二进制分析的实用性与效率，具备实际应用价值。

Abstract: Binary program analysis is still very important in system security. There are many practical achievements in binary code analysis, but fine-grained analysis such as dynamic taint analysis, is constantly studied due to the problem of deployability, high memory usage, and performance overhead, so as to better adapt to the new analysis scenario, such as memory corruption exploits and sandbox evasion malware. This paper presents a new binary program analysis framework, in order to improve the usability and performance of fine-grained analysis. The framework mainly uses the kernel module to further expand the analysis capability of the traditional dynamic binary instrumentation. Then, based on the idea of decoupling analysis, the analysis environment is constructed in the container process through process hollowing techniques in a new way. It can reuse the functions of the existing dynamic binary instrumentation platforms and also reduce the impact on the execution of the target program. The prototype is implemented on the Windows platform. The validity and performance of the framework are verified by a large number of experiments with benchmark and actual programs. The effectiveness of the framework is also verified by the analysis of actual exploit programs and malicious code, demonstrating the value of the practical application.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [18] [Harnessing Data Spaces to Build Intelligent Smart City Infrastructures Across the Cloud-Edge Continuum](https://arxiv.org/abs/2512.21340)
*Dimitrios Amaxilatis,Themistoklis Sarantakos,Nikolaos Tsironis,Souvik Sengupta,Kostas Ramantas,Jhofre Ojeda*

Main category: cs.DC

TL;DR: 智能城市采用以数据为中心的架构来提升城市服务效率、可持续性和韧性。


<details>
  <summary>Details</summary>
Motivation: 提升城市服务的效率、可持续性和韧性。

Method: 采用以数据为中心的架构。

Result: 增强城市管理能力与服务水平。

Conclusion: 数据驱动方法对智能城市发展具有关键作用。

Abstract: Smart cities are increasingly adopting data-centric architectures to enhance the efficiency, sustainability, and resilience of urban services.

</details>


### [19] [Efficient MoE Inference with Fine-Grained Scheduling of Disaggregated Expert Parallelism](https://arxiv.org/abs/2512.21487)
*Xinglin Pan,Shaohuai Shi,Wenxiang Lin,Yuxin Wang,Zhenheng Tang,Wei Wang,Xiaowen Chu*

Main category: cs.DC

TL;DR: FinDEP是一种面向MoE模型推理的细粒度任务调度算法，通过优化计算与通信任务划分及调度策略，在多GPU系统上显著提升吞吐量。


<details>
  <summary>Details</summary>
Motivation: 解决现有DEP方法在共享专家支持和任务调度效率上的不足，以缓解MoE推理中内存密集与计算资源利用率低的问题。

Method: 提出三项创新：1）将计算/通信划分为更小任务以实现细粒度流水；2）构建支持可变粒度与顺序的调度优化模型；3）设计高效求解器应对大规模搜索空间。

Result: 在DeepSeek-V2与Qwen3-MoE上，FinDEP相较现有方法最高提升1.61倍吞吐量，在32-GPU系统上实现最高1.24倍加速。

Conclusion: FinDEP有效提升了MoE模型在分布式GPU环境下的推理效率，为大规模专家模型部署提供了实用调度方案。

Abstract: The mixture-of-experts (MoE) architecture scales model size with sublinear computational increase but suffers from memory-intensive inference due to KV caches and sparse expert activation. Recent disaggregated expert parallelism (DEP) distributes attention and experts to dedicated GPU groups but lacks support for shared experts and efficient task scheduling, limiting performance.
  We propose FinDEP, a fine-grained task scheduling algorithm for DEP that maximizes task overlap to improve MoE inference throughput. FinDEP introduces three innovations: 1) partitioning computation/communication into smaller tasks for fine-grained pipelining, 2) formulating a scheduling optimization supporting variable granularity and ordering, and 3) developing an efficient solver for this large search space.
  Experiments on four GPU systems with DeepSeek-V2 and Qwen3-MoE show FinDEP improves throughput by up to 1.61x over prior methods, achieving up to 1.24x speedup on a 32-GPU system.

</details>


### [20] [nncase: An End-to-End Compiler for Efficient LLM Deployment on Heterogeneous Storage Architectures](https://arxiv.org/abs/2512.21571)
*Hui Guo,Qihang Zheng,Chenghai Huo,Dongliang Guo,Haoqi Yang,Yang Zhang*

Main category: cs.DC

TL;DR: nncase是一个开源的端到端编译框架，通过e-graph重写引擎和三大自动化模块优化LLM在异构硬件上的部署性能。


<details>
  <summary>Details</summary>
Motivation: 解决传统编译器因内存架构异构导致的工作流碎片化与高适配成本问题。

Method: 基于e-graph的项重写引擎，结合自动向量化、自动分布式策略搜索与自动调度模块，并辅以缓存感知代码生成。

Result: 在Qwen3模型上优于MLC LLM与Intel IPEX，在CPU上性能媲美手工优化的llama.cpp。

Conclusion: 自动化编译可有效实现高性能LLM部署，nncase为异构目标提供统一优化方案。

Abstract: The efficient deployment of large language models (LLMs) is hindered by memory architecture heterogeneity, where traditional compilers suffer from fragmented workflows and high adaptation costs. We present nncase, an open-source, end-to-end compilation framework designed to unify optimization across diverse targets. Central to nncase is an e-graph-based term rewriting engine that mitigates the phase ordering problem, enabling global exploration of computation and data movement strategies. The framework integrates three key modules: Auto Vectorize for adapting to heterogeneous computing units, Auto Distribution for searching parallel strategies with cost-aware communication optimization, and Auto Schedule for maximizing on-chip cache locality. Furthermore, a buffer-aware Codegen phase ensures efficient kernel instantiation. Evaluations show that nncase outperforms mainstream frameworks like MLC LLM and Intel IPEX on Qwen3 series models and achieves performance comparable to the hand-optimized llama.cpp on CPUs, demonstrating the viability of automated compilation for high-performance LLM deployment. The source code is available at https://github.com/kendryte/nncase.

</details>


### [21] [Optimizing Resource Allocation for Geographically-Distributed Inference by Large Language Models](https://arxiv.org/abs/2512.21884)
*Tingyang Sun,Ting He,Bo Ji,Parimal Parag*

Main category: cs.DC

TL;DR: 本文首次系统研究分布式大语言模型推理中的资源分配问题，提出性能预测模型与优化算法，显著降低推理延迟。


<details>
  <summary>Details</summary>
Motivation: 降低分布式LLM推理的资源分配复杂性，提升性能并减少对高端GPU的依赖。

Method: 建立性能预测模型，将块放置与请求路由建模为混合整数线性规划，设计多项式复杂度离线与在线算法。

Result: 实验与仿真表明，相比现有方案显著减少推理时间，同时提供轻量CPU模拟器支持无GPU环境研究。

Conclusion: 所提方法在多种地理分布场景下有效优化资源分配，具备理论保证与实用价值。

Abstract: Large language models have demonstrated extraordinary performance in many AI tasks but are expensive to use, even after training, due to their requirement of high-end GPUs. Recently, a distributed system called PETALS was developed to lower the barrier for deploying LLMs by splitting the model blocks across multiple servers with low-end GPUs distributed over the Internet, which was much faster than swapping the model parameters between the GPU memory and other cheaper but slower local storage media. However, the performance of such a distributed system critically depends on the resource allocation, and how to do so optimally remains unknown. In this work, we present the first systematic study of the resource allocation problem in distributed LLM inference, with focus on two important decisions: block placement and request routing. Our main results include: experimentally validated performance models that can predict the inference performance under given block placement and request routing decisions, a formulation of the offline optimization of block placement and request routing as a mixed integer linear programming problem together with the NP-hardness proof and a polynomial-complexity algorithm with guaranteed performance, and an adaptation of the offline algorithm for the online setting with the same performance guarantee under bounded load. Through both experiments and experimentally-validated simulations, we have verified that the proposed solution can substantially reduce the inference time compared to the state-of-the-art solution in diverse settings with geographically-distributed servers. As a byproduct, we have also developed a light-weighted CPU-only simulator capable of predicting the performance of distributed LLM inference on GPU servers, which can evaluate large deployments and facilitate future research for researchers with limited GPU access.

</details>


### [22] [BLEST: Blazingly Efficient BFS using Tensor Cores](https://arxiv.org/abs/2512.21967)
*Deniz Elbek,Kamer Kaya*

Main category: cs.DC

TL;DR: BLEST 是一种利用 GPU Tensor Cores 加速广度优先搜索（BFS）的框架，通过位图结构、负载均衡策略与稀疏矩阵优化，在真实图数据上显著超越现有方案。


<details>
  <summary>Details</summary>
Motivation: 传统 BFS 在 GPU 上难以高效利用专为密集计算设计的 Tensor Cores，导致性能受限；BLEST 旨在解决这一问题。

Method: 提出 BVSS 结构实现 warp 级负载均衡，结合图重排序、批处理 SpMSpV 模式与内核融合技术，适配稀疏图计算并减少冗余与同步开销。

Result: 在多种真实图数据集上，BLEST 平均比 BerryBees、Gunrock 和 GSWITCH 快 3.58 倍、4.64 倍和 4.9 倍。

Conclusion: BLEST 成功将稀疏图遍历映射至高吞吐 Tensor Core 单元，显著提升 BFS 性能，为不规则图计算提供新范式。

Abstract: Breadth-First Search (BFS) is a fundamental graph kernel that underpins a wide range of applications. While modern GPUs provide specialised Matrix-Multiply-Accumulate (MMA) units, e.g., Tensor Cores (TC), with extremely high throughput, they target dense operations, making it non-trivial to exploit them for irregular, unstructured graph computations. In particular, fully utilising them for a BFS requires an efficient mapping of the edge operations onto TCs while avoiding redundancy, load imbalance, and synchronisation. We present BLEST, a TC-accelerated framework that reformulates the pull-based BFS pipeline around a bitmap-oriented structure and a carefully engineered execution layout. BLEST introduces Binarised Virtual Slice Sets (BVSS) to enforce warp-level load balancing and to eliminate frontier-oblivious work assignment. To improve both memory efficiency and update locality across diverse graphs, we apply two complementary graph reordering strategies: a compression-oriented ordering for social-like graphs and a bandwidth-reducing ordering for non-social graphs. At the compute level, we develop a batched SpMSpV multiplication pattern that uses the bitwise TC tiles to handle dot products without wasting output entries, thereby reducing the number of required MMA calls. Finally, BLEST combines kernel fusion with a lazy vertex update scheme to reduce host-side synchronisation, mitigate atomic overheads, and improve cache locality. Experiments show that BLEST delivers, on average, $3.58\times$, $4.64\times$ and $4.9\times$ speedup over BerryBees, Gunrock, and GSWITCH, respectively, across a broad set of real-world graphs.

</details>


### [23] [FUSCO: High-Performance Distributed Data Shuffling via Transformation-Communication Fusion](https://arxiv.org/abs/2512.22036)
*Zhuoran Zhu,Chunyang Zhu,Hao Lin,Xu Fu,Yiming Zhou,Quanlu Zhang,Zhenhua Li,Feng Qian,Chao Yu,Boxun Li,Guohao Dai,Yu Wang*

Main category: cs.DC

TL;DR: FUSCO是一个专为大规模Mixture-of-Experts模型设计的高效通信库，通过融合数据转换与通信显著降低数据分发开销。


<details>
  <summary>Details</summary>
Motivation: 现有通信库在处理MoE模型的数据分发时效率低下，导致通信开销占整体运行时间过半。

Method: FUSCO基于专家主导与设备主导数据布局冲突的观察，结合细粒度布局分析、流水线通信引擎及轻量级规划与负载均衡机制实现高效通信。

Result: 相比NCCL和DeepEP，FUSCO在基准测试中分别提速最高达3.84倍和2.01倍；端到端任务中训练延迟降低1.17-1.39倍，推理首token生成延迟降低1.09-1.25倍。

Conclusion: FUSCO有效解决了MoE模型通信瓶颈，显著提升训练与推理效率。

Abstract: Large-scale Mixture-of-Experts (MoE) models rely on \emph{expert parallelism} for efficient training and inference, which splits experts across devices and necessitates distributed data shuffling to route each token to its assigned experts. However, existing communication libraries handle this shuffling poorly; its overhead can account for over half of end-to-end runtime. We present FUSCO, an MoE-friendly communication library that achieves efficient and lightweight data shuffling through fused data transformation and communication, based on the key observation that MoE's expert-major data layout conflicts with the device-major layout expected by communication operations. FUSCO captures the fine-grained data layout, which is then interpreted by a pipelined communication engine that performs the required shuffling efficiently along the communication path. Lightweight planning and load-balancing mechanisms complement the engine by eliminating redundant communication and dispersing traffic. Evaluations on representative benchmarks illustrate that FUSCO achieves up to 3.84$\times$ and 2.01$\times$ speedups over NCCL and DeepEP (the state-of-the-art MoE communication library), respectively. In end-to-end MoE tasks, compared to NCCL and DeepEP, FUSCO reduces the training latency by 1.17-1.39$\times$ and 1.10-1.19$\times$, and lowers the first-token generation latency in inference by 1.09-1.25$\times$ and 1.06-1.16$\times$.

</details>


### [24] [Agentic Structured Graph Traversal for Root Cause Analysis of Code-related Incidents in Cloud Applications](https://arxiv.org/abs/2512.22113)
*Shengkun Cui,Rahul Krishna,Saurabh Jha,Ravishankar K. Iyer*

Main category: cs.DC

TL;DR: PRAXIS是一种基于LLM的云事故诊断框架，通过服务依赖图与代码依赖图联合遍历，显著提升根因分析准确率并降低计算开销。


<details>
  <summary>Details</summary>
Motivation: 云事故造成巨大经济损失，现有方法在代码与配置类根因诊断上效果有限，亟需更高效精准的自动化方案。

Method: 构建服务依赖图(SDG)与程序依赖图(PDG)，利用LLM作为智能策略驱动跨图结构化遍历，实现故障定位与解释。

Result: 相比ReAct基线，PRAXIS将RCA准确率提升最高3.1倍，同时减少3.8倍token消耗，并在30个真实事故案例中验证有效性。

Conclusion: PRAXIS为云环境下的代码与配置类事故提供了高效、低耗、可解释的自动化根因分析新范式。

Abstract: Cloud incidents pose major operational challenges in production, with unresolved production cloud incidents cost on average over $2M per hour. Prior research identifies code- and configuration-related issues as the predominant category of root causes in cloud incidents. This paper introduces PRAXIS, an orchestrator that manages and deploys an agentic workflow for diagnosing code- and configuration-caused cloud incidents. PRAXIS employs an LLM-driven structured traversal over two types of graph: (1) a service dependency graph (SDG) that captures microservice-level dependencies; and (2) a hammock-block program dependence graph (PDG) that captures code-level dependencies for each microservice. Together, these graphs encode microservice- and code-level dependencies and the LLM acts as a traversal policy over these graphs, moving between services and code dependencies to localize and explain failures. Compared to state-of-the-art ReAct baselines, PRAXIS improves RCA accuracy by up to 3.1x while reducing token consumption by 3.8x. PRAXIS is demonstrated on a set of 30 comprehensive real-world incidents that is being compiled into an RCA benchmark.

</details>
