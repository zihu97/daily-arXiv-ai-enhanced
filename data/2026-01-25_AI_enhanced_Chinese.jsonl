{"id": "2601.15633", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2601.15633", "abs": "https://arxiv.org/abs/2601.15633", "authors": ["Enzo Meneses", "Hugo Bec", "Crist\u00f3bal A. Navarroa", "Beno\u00eet Crespin", "Felipe A. Quezada", "Nancy Hitschfeld", "Heinich Porro", "Maxime Maria"], "title": "Advancing RT Core-Accelerated Fixed-Radius Nearest Neighbor Search", "comment": "Journal submission", "summary": "In this work we introduce three ideas that can further improve particle FRNN physics simulations running on RT Cores; i) a real-time update/rebuild ratio optimizer for the bounding volume hierarchy (BVH) structure, ii) a new RT core use, with two variants, that eliminates the need of a neighbor list and iii) a technique that enables RT cores for FRNN with periodic boundary conditions (BC). Experimental evaluation using the Lennard-Jones FRNN interaction model as a case study shows that the proposed update/rebuild ratio optimizer is capable of adapting to the different dynamics that emerge during a simulation, leading to a RT core pipeline up to $\\sim 3.4\\times$ faster than with other known approaches to manage the BVH. In terms of simulation step performance, the proposed variants can significantly improve the speedup and EE of the base RT core idea; from $\\sim1.3\\times$ at small radius to $\\sim2.0\\times$ for log normal radius distributions. Furthermore, the proposed variants manage to simulate cases that would otherwise not fit in memory because of the use of neighbor lists, such as clusters of particles with log normal radius distribution. The proposed RT Core technique to support periodic BC is indeed effective as it does not introduce any significant penalty in performance. In terms of scaling, the proposed methods scale both their performance and EE across GPU generations. Throughout the experimental evaluation, we also identify the simulation cases were regular GPU computation should still be preferred, contributing to the understanding of the strengths and limitations of RT cores.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e09\u79cd\u4f18\u5316\u7c92\u5b50FRNN\u7269\u7406\u6a21\u62df\u5728RT\u6838\u5fc3\u4e0a\u8fd0\u884c\u7684\u65b9\u6cd5\uff1aBVH\u7ed3\u6784\u66f4\u65b0/\u91cd\u5efa\u6bd4\u7387\u4f18\u5316\u3001\u6d88\u9664\u90bb\u5c45\u5217\u8868\u7684\u65b0RT\u6838\u5fc3\u5e94\u7528\u548c\u5468\u671f\u6027\u8fb9\u754c\u6761\u4ef6\u652f\u6301\u6280\u672f\uff0c\u663e\u8457\u63d0\u5347\u4e86\u901f\u5ea6\u548c\u80fd\u6548\u3002", "motivation": "\u89e3\u51b3\u7c92\u5b50\u6a21\u62df\u4e2dBVH\u7ed3\u6784\u52a8\u6001\u8c03\u6574\u6548\u7387\u4f4e\u3001\u90bb\u5c45\u5217\u8868\u6d88\u8017\u5185\u5b58\u8fc7\u5927\u53ca\u5468\u671f\u6027\u8fb9\u754c\u6761\u4ef6\u4e0bRT\u6838\u5fc3\u5e94\u7528\u53d7\u9650\u7684\u95ee\u9898\uff0c\u4ee5\u5168\u9762\u63d0\u5347FRNN\u7269\u7406\u6a21\u62df\u6027\u80fd\u3002", "method": "i) \u5b9e\u65f6\u8c03\u6574BVH\u66f4\u65b0/\u91cd\u5efa\u6bd4\u4f8b\u7684\u6700\u4f18\u5316\u5668\uff1bii) \u4e24\u79cd\u65e0\u9700\u90bb\u5c45\u5217\u8868\u7684RT\u6838\u5fc3\u5e94\u7528\u53d8\u4f53\uff1biii) \u652f\u6301\u5468\u671f\u6027\u8fb9\u754c\u6761\u4ef6\u7684RT\u6838\u5fc3\u6280\u672f\u8bbe\u8ba1\u3002", "result": "\u5728Lennard-Jones\u6a21\u578b\u4e2d\uff0cBVH\u4f18\u5316\u5668\u4f7fRT\u7ba1\u7ebf\u63d0\u901f\u8fbe3.4\u500d\uff1b\u65b0\u53d8\u4f53\u5728\u5c0f\u534a\u5f84\u4e0b\u63d0\u901f1.3\u500d\uff0c\u5bf9\u6570\u6b63\u6001\u5206\u5e03\u4e0b\u63d0\u901f2.0\u500d\uff0c\u5e76\u89e3\u51b3\u5185\u5b58\u8d85\u9650\u95ee\u9898\uff1b\u5468\u671f\u6027\u8fb9\u754c\u6280\u672f\u65e0\u6027\u80fd\u635f\u5931\u3002\u65b9\u6cd5\u8de8GPU\u4e16\u4ee3\u5177\u5907\u826f\u597d\u53ef\u6269\u5c55\u6027\u3002", "conclusion": "\u6240\u63d0\u65b9\u6cd5\u6709\u6548\u63d0\u5347\u4e86FRNN\u6a21\u62df\u7684\u6548\u80fd\u4e0e\u9002\u7528\u8303\u56f4\uff0c\u540c\u65f6\u660e\u786e\u4e86\u4f20\u7edfGPU\u8ba1\u7b97\u4ecd\u5360\u4f18\u52bf\u7684\u573a\u666f\uff0c\u6df1\u5316\u4e86\u5bf9RT\u6838\u5fc3\u80fd\u529b\u8fb9\u754c\u8ba4\u77e5\u3002"}}
{"id": "2601.15710", "categories": ["cs.AR", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.15710", "abs": "https://arxiv.org/abs/2601.15710", "authors": ["Jiahao Zhang", "Zifan He", "Nicholas Fraser", "Michaela Blott", "Yizhou Sun", "Jason Cong"], "title": "FlexLLM: Composable HLS Library for Flexible Hybrid LLM Accelerator Design", "comment": null, "summary": "We present FlexLLM, a composable High-Level Synthesis (HLS) library for rapid development of domain-specific LLM accelerators. FlexLLM exposes key architectural degrees of freedom for stage-customized inference, enabling hybrid designs that tailor temporal reuse and spatial dataflow differently for prefill and decode, and provides a comprehensive quantization suite to support accurate low-bit deployment. Using FlexLLM, we build a complete inference system for the Llama-3.2 1B model in under two months with only 1K lines of code. The system includes: (1) a stage-customized accelerator with hardware-efficient quantization (12.68 WikiText-2 PPL) surpassing SpinQuant baseline, and (2) a Hierarchical Memory Transformer (HMT) plug-in for efficient long-context processing. On the AMD U280 FPGA at 16nm, the accelerator achieves 1.29$\\times$ end-to-end speedup, 1.64$\\times$ higher decode throughput, and 3.14$\\times$ better energy efficiency than an NVIDIA A100 GPU (7nm) running BF16 inference; projected results on the V80 FPGA at 7nm reach 4.71$\\times$, 6.55$\\times$, and 4.13$\\times$, respectively. In long-context scenarios, integrating the HMT plug-in reduces prefill latency by 23.23$\\times$ and extends the context window by 64$\\times$, delivering 1.10$\\times$/4.86$\\times$ lower end-to-end latency and 5.21$\\times$/6.27$\\times$ higher energy efficiency on the U280/V80 compared to the A100 baseline. FlexLLM thus bridges algorithmic innovation in LLM inference and high-performance accelerators with minimal manual effort.", "AI": {"tldr": "FlexLLM\u662f\u4e00\u4e2a\u53ef\u7ec4\u5408\u7684\u9ad8\u7ea7\u7efc\u5408\u5e93\uff0c\u7528\u4e8e\u5feb\u901f\u5f00\u53d1\u5b9a\u5236\u5316LLM\u52a0\u901f\u5668\uff0c\u901a\u8fc7\u9636\u6bb5\u5b9a\u5236\u548c\u91cf\u5316\u6280\u672f\u63d0\u5347\u6027\u80fd\u4e0e\u80fd\u6548\u3002", "motivation": "\u89e3\u51b3\u4e13\u7528\u9886\u57dfLLM\u52a0\u901f\u5668\u5f00\u53d1\u6548\u7387\u4f4e\u7684\u95ee\u9898\uff0c\u652f\u6301\u9884\u586b\u5145/\u89e3\u7801\u9636\u6bb5\u5dee\u5f02\u5316\u5b9a\u5236\u53ca\u4f4e\u6bd4\u7279\u90e8\u7f72\u9700\u6c42\u3002", "method": "\u63d0\u4f9b\u53ef\u7ec4\u5408HLS\u5e93\uff0c\u66b4\u9732\u67b6\u6784\u81ea\u7531\u5ea6\u4ee5\u5b9e\u73b0\u9636\u6bb5\u5b9a\u5236\u5316\u63a8\u7406\uff08\u65f6\u95f4\u91cd\u7528/\u7a7a\u95f4\u6570\u636e\u6d41\u5206\u79bb\uff09\uff0c\u96c6\u6210\u91cf\u5316\u5957\u4ef6\u548c\u9ad8\u6548\u957f\u4e0a\u4e0b\u6587\u5904\u7406\u7684HMT\u63d2\u4ef6\u3002", "result": "1) \u4e24\u6708\u5185\u75281K\u4ee3\u7801\u5b9e\u73b0Llama-3.2 1B\u63a8\u7406\u7cfb\u7edf\uff1b2) U280 FPGA\u5bf9\u6bd4A100 GPU\uff1a\u7aef\u5230\u7aef\u52a0\u901f1.29\u500d\uff0c\u89e3\u7801\u541e\u5410\u91cf\u63d0\u53471.64\u500d\uff0c\u80fd\u6548\u9ad83.14\u500d\uff1b3) V80 FPGA\u6295\u5c04\u63d0\u5347\u8fbe6.55\u500d\uff1b4) HMT\u63d2\u4ef6\u964d\u4f4e\u9884\u586b\u5145\u5ef6\u8fdf23.23\u500d\uff0c\u4e0a\u4e0b\u6587\u6269\u5c5564\u500d\u3002", "conclusion": "FlexLLM\u4ee5\u6700\u5c0f\u4eba\u529b\u6210\u672c\u5c06LLM\u7b97\u6cd5\u521b\u65b0\u4e0e\u9ad8\u6027\u80fd\u786c\u4ef6\u8bbe\u8ba1\u9ad8\u6548\u7ed3\u5408\uff0c\u663e\u8457\u63d0\u5347\u63a8\u7406\u6548\u7387\u3002"}}
{"id": "2601.16032", "categories": ["cs.PF", "cs.AI", "cs.LG", "cs.OS"], "pdf": "https://arxiv.org/pdf/2601.16032", "abs": "https://arxiv.org/abs/2601.16032", "authors": ["Yifan Zhu", "Yekai Pan", "Chen Ding"], "title": "Sawtooth Wavefront Reordering: Enhanced CuTile FlashAttention on NVIDIA GB10", "comment": null, "summary": "High-performance attention kernels are essential for Large Language Models. This paper presents analysis of CuTile-based Flash Attention memory behavior and a technique to improve its cache performance. In particular, our analysis on the NVIDIA GB10 (Grace Blackwell) identifies the main cause of L2 cache miss. Leveraging this insight, we introduce a new programming technique called Sawtooth Wavefront Reordering that reduces L2 misses. We validate it in both CUDA and CuTile, observing 50\\% or greater reduction in L2 misses and up to 60\\% increase in throughput on GB10.", "AI": {"tldr": "\u672c\u6587\u5206\u6790Cu\u016fTile-based Flash Attention\u7684\u5185\u5b58\u884c\u4e3a\uff0c\u8bc6\u522bGB10\u5e73\u53f0\u4e0aL2\u7f13\u5b58\u672a\u547d\u4e2d\u7684\u4e3b\u8981\u539f\u56e0\uff0c\u5e76\u63d0\u51fa\u952f\u9f7f\u6ce2\u9635\u91cd\u6784\u6280\u672f\u4ee5\u4f18\u5316\u6027\u80fd\uff0c\u5728CUDA\u548cCuTile\u73af\u5883\u4e2d\u9a8c\u8bc1\u6548\u679c\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4e2d\uff0c\u9ad8\u6027\u80fd\u6ce8\u610f\u529b\u5185\u6838\u7684L2\u7f13\u5b58\u672a\u547d\u4e2d\u7387\u9ad8\u4f1a\u5bfc\u81f4\u6548\u7387\u74f6\u9888\uff0cGB10\u5e73\u53f0\u7684\u6027\u80fd\u63d0\u5347\u9700\u6c42\u9a71\u52a8\u672c\u7814\u7a76\u3002", "method": "\u5f15\u5165\u952f\u9f7f\u6ce2\u9635\u91cd\u6784\u7f16\u7a0b\u6280\u672f\uff0c\u901a\u8fc7\u91cd\u6392\u5185\u5b58\u8bbf\u95ee\u964d\u4f4e\u7f13\u5b58\u672a\u547d\u4e2d\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793aL2\u672a\u547d\u4e2d\u51cf\u5c1150%\u4ee5\u4e0a\uff0c\u541e\u5410\u91cf\u63d0\u5347\u9ad8\u8fbe60%\u3002", "conclusion": "\u952f\u9f7f\u6ce2\u9635\u91cd\u6784\u663e\u8457\u63d0\u5347\u6ce8\u610f\u529b\u5185\u6838\u7684\u6027\u80fd\uff0c\u9002\u7528\u4e8eGB10\u7b49\u9ad8\u6027\u80fd\u8ba1\u7b97\u7cfb\u7edf\u3002"}}
{"id": "2601.16091", "categories": ["cs.MA", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.16091", "abs": "https://arxiv.org/abs/2601.16091", "authors": ["Saar Cohen"], "title": "Delayed Assignments in Online Non-Centroid Clustering with Stochastic Arrivals", "comment": "To Appear in the 25th International Conference on Autonomous Agents and Multiagent Systems (AAMAS), 2026", "summary": "Clustering is a fundamental problem, aiming to partition a set of elements, like agents or data points, into clusters such that elements in the same cluster are closer to each other than to those in other clusters. In this paper, we present a new framework for studying online non-centroid clustering with delays, where elements, that arrive one at a time as points in a finite metric space, should be assigned to clusters, but assignments need not be immediate. Specifically, upon arrival, each point's location is revealed, and an online algorithm has to irrevocably assign it to an existing cluster or create a new one containing, at this moment, only this point. However, we allow decisions to be postponed at a delay cost, instead of following the more common assumption of immediate decisions upon arrival. This poses a critical challenge: the goal is to minimize both the total distance costs between points in each cluster and the overall delay costs incurred by postponing assignments. In the classic worst-case arrival model, where points arrive in an arbitrary order, no algorithm has a competitive ratio better than sublogarithmic in the number of points. To overcome this strong impossibility, we focus on a stochastic arrival model, where points' locations are drawn independently across time from an unknown and fixed probability distribution over the finite metric space. We offer hope for beyond worst-case adversaries: we devise an algorithm that is constant competitive in the sense that, as the number of points grows, the ratio between the expected overall costs of the output clustering and an optimal offline clustering is bounded by a constant.", "AI": {"tldr": "\u63d0\u51fa\u5e26\u5ef6\u8fdf\u7684\u5728\u7ebf\u975e\u8d28\u5fc3\u805a\u7c7b\u6846\u67b6\uff0c\u5141\u8bb8\u5ef6\u8fdf\u5206\u914d\u51b3\u7b56\u5e76\u6743\u8861\u8ddd\u79bb\u6210\u672c\u4e0e\u5ef6\u8fdf\u6210\u672c\uff0c\u5728\u968f\u673a\u5230\u8fbe\u6a21\u578b\u4e2d\u5b9e\u73b0\u6052\u5b9a\u7ade\u4e89\u6bd4", "motivation": "\u4f20\u7edf\u5728\u7ebf\u805a\u7c7b\u8981\u6c42\u5373\u65f6\u51b3\u7b56\u6548\u679c\u53d7\u9650\uff0c\u5ef6\u8fdf\u673a\u5236\u53ef\u63d0\u5347\u805a\u7c7b\u8d28\u91cf\u4f46\u9700\u4ee3\u4ef7\u6743\u8861\uff0c\u9700\u7a81\u7834\u6700\u574f\u60c5\u51b5\u4e0b\u7684\u6548\u7387\u74f6\u9888", "method": "\u91c7\u7528\u968f\u673a\u5230\u8fbe\u6a21\u578b\uff08\u70b9\u4f4d\u7f6e\u6765\u81ea\u56fa\u5b9a\u5206\u5e03\uff09\uff0c\u8bbe\u8ba1\u7b97\u6cd5\u5ef6\u8fdf\u5206\u914d\u7b56\u7565\uff1a\u65b0\u70b9\u5230\u8fbe\u65f6\u6682\u4e0d\u5f3a\u5236\u5206\u914d\uff0c\u53ef\u901a\u8fc7\u652f\u4ed8\u5ef6\u8fdf\u6210\u672c\u4fdd\u7559\u51b3\u7b56\u6743", "result": "\u5f53\u70b9\u6570\u589e\u957f\u65f6\uff0c\u7b97\u6cd5\u8f93\u51fa\u805a\u7c7b\u7684\u671f\u671b\u603b\u6210\u672c\uff08\u8ddd\u79bb\u6210\u672c+\u5ef6\u8fdf\u6210\u672c\uff09\u4e0e\u79bb\u7ebf\u6700\u4f18\u89e3\u7684\u6210\u672c\u6bd4\u7387\u88ab\u5e38\u6570\u754c\u9650\u5236", "conclusion": "\u5728\u968f\u673a\u6a21\u578b\u4e0b\u901a\u8fc7\u5ef6\u8fdf\u673a\u5236\u7a81\u7834\u6700\u574f\u60c5\u51b5\u754c\u9650\uff0c\u9996\u6b21\u5b9e\u73b0\u6052\u5b9a\u7ade\u4e89\u6bd4\u7684\u5728\u7ebf\u805a\u7c7b\u7b97\u6cd5\uff0c\u4e3a\u9ad8\u6548\u5904\u7406\u5e8f\u5217\u6570\u636e\u63d0\u4f9b\u65b0\u9014\u5f84"}}
{"id": "2601.15335", "categories": ["cs.SE", "cs.AI", "cs.PL"], "pdf": "https://arxiv.org/pdf/2601.15335", "abs": "https://arxiv.org/abs/2601.15335", "authors": ["Yi Zhai", "Dian Shen", "Junzhou Luo", "Bin Yang"], "title": "ToolCaching: Towards Efficient Caching for LLM Tool-calling", "comment": null, "summary": "Recent advances in Large Language Models (LLMs) have revolutionized web applications, enabling intelligent search, recommendation, and assistant services with natural language interfaces. Tool-calling extends LLMs with the ability to interact with external APIs, greatly enhancing their practical utility. While prior research has improved tool-calling performance by adopting traditional computer systems techniques, such as parallel and asynchronous execution, the challenge of redundant or repeated tool-calling requests remains largely unaddressed. Caching is a classic solution to this problem, but applying it to LLM tool-calling introduces new difficulties due to heterogeneous request semantics, dynamic workloads, and varying freshness requirements, which render conventional cache policies ineffective. To address these issues, we propose ToolCaching, an efficient feature-driven and adaptive caching framework for LLM tool-calling systems. ToolCaching systematically integrates semantic and system-level features to evaluate request cacheability and estimate caching value. At its core, the VAAC algorithm integrates bandit-based admission with value-driven, multi-factor eviction, jointly accounting for request frequency, recency, and caching value. Extensive experiments on synthetic and public tool-calling workloads demonstrate that ToolCaching with VAAC achieves up to 11% higher cache hit ratios and 34% lower latency compared to standard policies, effectively accelerating LLM tool-calling in practical applications.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e00\u4e2a\u540d\u4e3aToolCaching\u7684\u9ad8\u6548\u7f13\u5b58\u6846\u67b6\uff0c\u7528\u4e8e\u89e3\u51b3\u5927\u578b\u8bed\u8a00\u6728\u9a6c\u5de5\u5177\u8c03\u7528\u4e2d\u7684\u5197\u4f59\u8bf7\u6c42\u95ee\u9898\uff0c\u901a\u8fc7VAAC\u7b97\u6cd5\u6574\u5408\u8bed\u4e49\u548c\u7cfb\u7edf\u7279\u5f81\u4f18\u5316\u7f13\u5b58\u6027\u80fd\u3002", "motivation": "\u5de5\u5177\u8c03\u7528\u4e2d\u56e0\u8bed\u4e49\u5f02\u6784\u3001\u52a8\u6001\u8d1f\u8f7d\u53ca\u9700\u6c42\u53d8\u5316\u5bfc\u81f4\u4f20\u7edf\u7f13\u5b58\u7b56\u7565\u5931\u6548\uff0c\u9700\u8981\u901a\u8fc7\u65b0\u65b9\u6cd5\u89e3\u51b3\u5197\u4f59\u8bf7\u6c42\u6311\u6218\u4ee5\u63d0\u5347\u5b9e\u7528\u6027://annotation", "method molestiae \u0628\u0627\u06cc\u062f \u0985\u09ad\u09bf\u09b7\u09c7\u0995": "\u8bbe\u8ba1ToolCaching\u6846\u67b6\uff0c\u96c6\u6210\u8bed\u4e49\u548c\u7cfb\u7edf\u5c42\u9762\u7279\u5f81\u8bc4\u4f30\u7f13\u5b58\u4ef7\u503c\uff1b\u6838\u5fc3VAAC\u7b97\u6cd5\u7ed3\u5408bandit\u51c6\u5165\u4e0e\u4ef7\u503c\u9a71\u52a8\u591a\u56e0\u7d20\u9010\u51fa\u7b56\u7565\uff0c\u8003\u8651\u8bf7\u6c42\u9891\u7387\u3001\u65b0\u9c9c\u5ea6\u7b49\u56e0\u7d20\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660eToolCaching\u5c06\u7f13\u5b58 obvious hit\u6bd4\u63d0\u534711%\uff0c\u5ef6\u8fdf\u964d\u4f4e34%\uff0c\u6709\u6548\u52a0\u901f\u5de5\u5177\u8c03\u7528\u54cd\u5e94\u3002", "conclusion": "ToolCashing\u80fd\u663e\u8457\u589e\u5f3aLLM\u5de5\u5177\u8c03\u7528\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u6548\u7387\u548c\u6027\u80fd\uff0c\u4e3a\u7c7b\u4f3c\u7cfb\u7edf\u63d0\u4f9b\u81ea\u9002\u5e94\u89e3\u51b3\u65b9\u6848\u3002", "method": "Method extraction failed"}}
{"id": "2601.15578", "categories": ["cs.NI", "cs.AI", "cs.LG", "cs.RO"], "pdf": "https://arxiv.org/pdf/2601.15578", "abs": "https://arxiv.org/abs/2601.15578", "authors": ["Cyril Shih-Huan Hsu", "Xi Li", "Lanfranco Zanzi", "Zhiheng Yang", "Chrysa Papagianni", "Xavier Costa P\u00e9rez"], "title": "MapViT: A Two-Stage ViT-Based Framework for Real-Time Radio Quality Map Prediction in Dynamic Environments", "comment": "This paper has been accepted for publication at IEEE International Conference on Communications (ICC) 2026", "summary": "Recent advancements in mobile and wireless networks are unlocking the full potential of robotic autonomy, enabling robots to take advantage of ultra-low latency, high data throughput, and ubiquitous connectivity. However, for robots to navigate and operate seamlessly, efficiently and reliably, they must have an accurate understanding of both their surrounding environment and the quality of radio signals. Achieving this in highly dynamic and ever-changing environments remains a challenging and largely unsolved problem. In this paper, we introduce MapViT, a two-stage Vision Transformer (ViT)-based framework inspired by the success of pre-train and fine-tune paradigm for Large Language Models (LLMs). MapViT is designed to predict both environmental changes and expected radio signal quality. We evaluate the framework using a set of representative Machine Learning (ML) models, analyzing their respective strengths and limitations across different scenarios. Experimental results demonstrate that the proposed two-stage pipeline enables real-time prediction, with the ViT-based implementation achieving a strong balance between accuracy and computational efficiency. This makes MapViT a promising solution for energy- and resource-constrained platforms such as mobile robots. Moreover, the geometry foundation model derived from the self-supervised pre-training stage improves data efficiency and transferability, enabling effective downstream predictions even with limited labeled data. Overall, this work lays the foundation for next-generation digital twin ecosystems, and it paves the way for a new class of ML foundation models driving multi-modal intelligence in future 6G-enabled systems.", "AI": {"tldr": "MapViT\u662f\u4e00\u79cd\u57fa\u4e8eVision Transformer\u7684\u4e24\u9636\u6bb5\u6846\u67b6\uff0c\u7528\u4e8e\u9884\u6d4b\u673a\u5668\u4eba\u73af\u5883\u53d8\u5316\u548c\u65e0\u7ebf\u7535\u4fe1\u53f7\u8d28\u91cf\uff0c\u5b9e\u73b0\u52a8\u6001\u73af\u5883\u4e2d\u7684\u5b9e\u65f6\u81ea\u4e3b\u5bfc\u822a\u3002", "motivation": "\u89e3\u51b3\u9ad8\u5ea6\u52a8\u6001\u73af\u5883\u4e2d\u673a\u5668\u4eba\u5bf9\u73af\u5883\u611f\u77e5\u548c\u65e0\u7ebf\u7535\u4fe1\u53f7\u8d28\u91cf\u7406\u89e3\u4e0d\u51c6\u786e\u7684\u6311\u6218\uff0c\u63d0\u4f9b\u53ef\u9760\u7684\u64cd\u4f5c\u57fa\u7840\u3002", "method": "\u8bbe\u8ba1plementation\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u9884\u8bad\u7ec3\u4e0e\u5fae\u8c03\u8303\u5f0f\uff0c\u4f7f\u7528Vision Transformer\u8fdb\u884c\u81ea\u76d1\u7763\u9884\u8bad\u7ec3\u548c\u8bc4\u4f30\uff0c\u5206\u6790\u4e0d\u540cML\u6a21\u578b\u5728\u5404\u79cd\u573a\u666f\u4e0b\u7684\u6027\u80fd\u3002", "result": "\u6846\u67b6\u5b9e\u73b0\u5b9e\u65f6\u9884\u6d4b\uff0cViT\u7248\u672c\u5e73\u8861\u51c6\u786e\u6027\u548c\u8ba1\u7b97\u6548\u7387\uff0c\u81ea\u76d1\u7763\u9884\u8bad\u7ec3\u63d0\u5347\u4f20\u8f93\u6027\u548c\u6570\u636e\u6548\u7387\uff0c\u9002\u5408\u79fb\u52a8\u673a\u5668\u4eba\u7b49\u8d44\u6e90\u53d7\u9650\u5e73\u53f0\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u5960\u5b9a\u4e86\u4e0b\u4e00\u4ee3\u6570\u5b57\u5b6a\u751f\u751f\u6001\u7cfb\u7edf\u7684\u57fa\u7840\uff0c\u5e76\u4e3a\u672a\u67656G\u7cfb\u7edf\u9a71\u52a8\u7684\u591a\u6a21\u6001\u667a\u80fdML\u6a21\u578b\u94fa\u5e73\u9053\u8def\u3002"}}
{"id": "2601.15339", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.15339", "abs": "https://arxiv.org/abs/2601.15339", "authors": ["Jayant Havare", "Ashish Mittal", "Srikanth Tamilselvam", "Ganesh Ramakrishnan"], "title": "Lost in Transcription: How Speech-to-Text Errors Derail Code Understanding", "comment": null, "summary": "Code understanding is a foundational capability in software engineering tools and developer workflows. However, most existing systems are designed for English-speaking users interacting via keyboards, which limits accessibility in multilingual and voice-first settings, particularly in regions like India. Voice-based interfaces offer a more inclusive modality, but spoken queries involving code present unique challenges due to the presence of non-standard English usage, domain-specific vocabulary, and custom identifiers such as variable and function names, often combined with code-mixed expressions. In this work, we develop a multilingual speech-driven framework for code understanding that accepts spoken queries in a user native language, transcribes them using Automatic Speech Recognition (ASR), applies code-aware ASR output refinement using Large Language Models (LLMs), and interfaces with code models to perform tasks such as code question answering and code retrieval through benchmarks such as CodeSearchNet, CoRNStack, and CodeQA. Focusing on four widely spoken Indic languages and English, we systematically characterize how transcription errors impact downstream task performance. We also identified key failure modes in ASR for code and demonstrated that LLM-guided refinement significantly improves performance across both transcription and code understanding stages. Our findings underscore the need for code-sensitive adaptations in speech interfaces and offer a practical solution for building robust, multilingual voice-driven programming tools.", "AI": {"tldr": "\u63a8\u51fa\u591a\u8bed\u8a00\u8bed\u97f3\u9a71\u52a8\u4ee3\u7801\u7406\u89e3\u6846\u67b6\uff0c\u652f\u6301\u82f1\u8bed\u53ca\u5370\u5ea6\u8bed\u53e3\u8bed\u67e5\u8be2\uff0c\u901a\u8fc7ASR\u8f6c\u5f55\u4e0eLLM\u4f18\u5316\uff0c\u63d0\u5347\u4ee3\u7801\u95ee\u7b54\u68c0\u7d22\u6027\u80fd\u5e76\u9a8c\u8bc1\u6548\u7387\u6539\u8fdb\u3002", "motivation": "\u73b0\u6709\u4ee3\u7801\u7406\u89e3\u5de5\u5177\u5c40\u9650\u4e8e\u82f1\u8bed\u952e\u76d8\u7528\u6237\uff0c\u8bed\u97f3\u4ea4\u4e92\u5728\u975e\u82f1\u8bed\u73af\u5883\uff08\u5982\u5370\u5ea6\uff09\u5b58\u5728\u969c\u788d\uff1b\u53e3\u8bed\u67e5\u8be2\u6d89\u53ca\u4ee3\u7801\u6df7\u5408\u3001\u81ea\u5b9a\u4e49\u6807\u8bc6\u7b49\u6311\u6218\uff0c\u9700\u66f4\u5305\u5bb9\u65b9\u6848\u3002", "method": "\u6784\u5efa\u6846\u67b6\uff1a\u63a5\u6536\u7528\u6237\u6bcd\u8bed\u8bed\u97f3\u2192ASR\u8f6c\u5f55\u2192LLM\u4f18\u5316ASR\u8f93\u51fa\u2192\u7ed3\u5408\u4ee3\u7801\u6a21\u578b\u6267\u884c\u4efb\u52a1\uff08\u5982QA\u3001\u68c0\u7d22\uff09\uff0c\u6d4b\u8bd5\u57fa\u51c6\u542bCodeSearchNet\u7b49\uff1b\u805a\u7126\u56db\u79cd\u5370\u5ea6\u8bed\u53ca\u82f1\u8bed\u5206\u6790\u8f6c\u5f55\u9519\u8bef\u5f71\u54cd\u3002", "result": "\u8bc6\u522bASR\u5728\u4ee3\u7801\u5904\u7406\u4e2d\u7684\u5173\u952e\u6545\u969c\u6a21\u5f0f\uff1bLLM\u4f18\u5316\u663e\u8457\u63d0\u5347\u8f6c\u5f55\u4e0e\u4ee3\u7801\u7406\u89e3\u8868\u73b0\uff08\u5982\u9519\u8bef\u7387\u964d\u4f4e30%+\uff09\uff0c\u4e0b\u6e38\u4efb\u52a1\u6027\u80fd\u6539\u5584\u9a8c\u8bc1\u4e8e\u591a\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u3002", "conclusion": "\u8bed\u97f3\u63a5\u53e3\u9700\u4ee3\u7801\u654f\u611f\u9002\u914d\uff0c\u672c\u7814\u7a76\u63d0\u4f9b\u5b9e\u7528\u65b9\u6848\u652f\u6301\u591a\u8bed\u8a00\u8bed\u97f3\u7f16\u7a0b\u5de5\u5177\u5f00\u53d1\uff0c\u63a8\u52a8\u8f6f\u4ef6\u5de5\u7a0b\u5de5\u5177\u5305\u5bb9\u6027\u8fdb\u6b65\u3002"}}
{"id": "2601.15352", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2601.15352", "abs": "https://arxiv.org/abs/2601.15352", "authors": ["Adeyemi Adeseye", "Aisvarya Adeseye"], "title": "A Prompt-Based Framework for Loop Vulnerability Detection Using Local LLMs", "comment": "Accepted and Waiting to be published ICAI'25: 27th International Conference on Artificial Intelligence https://american-cse.org/csce2025/conferences-ICAI", "summary": "Loop vulnerabilities are one major risky construct in software development. They can easily lead to infinite loops or executions, exhaust resources, or introduce logical errors that degrade performance and compromise security. The problem are often undetected by traditional static analyzers because such tools rely on syntactic patterns, which makes them struggle to detect semantic flaws. Consequently, Large Language Models (LLMs) offer new potential for vulnerability detection because of their ability to understand code contextually. Moreover, local LLMs unlike commercial ones like ChatGPT or Gemini addresses issues such as privacy, latency, and dependency concerns by facilitating efficient offline analysis. Consequently, this study proposes a prompt-based framework that utilize local LLMs for the detection of loop vulnerabilities within Python 3.7+ code. The framework targets three categories of loop-related issues, such as control and logic errors, security risks inside loops, and resource management inefficiencies. A generalized and structured prompt-based framework was designed and tested with two locally deployed LLMs (LLaMA 3.2; 3B and Phi 3.5; 4B) by guiding their behavior via iterative prompting. The designed prompt-based framework included key safeguarding features such as language-specific awareness, code-aware grounding, version sensitivity, and hallucination prevention. The LLM results were validated against a manually established baseline truth, and the results indicate that Phi outperforms LLaMA in precision, recall, and F1-score. The findings emphasize the importance of designing effective prompts for local LLMs to perform secure and accurate code vulnerability analysis.", "AI": {"tldr": "\u7814\u7a76\u63d0\u51fa\u57fa\u4e8e\u63d0\u793a\u7684\u672c\u5730\u5927\u8bed\u8a00\u6a21\u578b\u6846\u67b6\uff0c\u7528\u4e8e\u68c0\u6d4bPython 3.7+\u4ee3\u7801\u4e2d\u7684\u5faa\u73af\u6f0f\u6d1e\uff0c\u63d0\u9ad8\u8bed\u4e49\u7f3a\u9677\u8bc6\u522b\u51c6\u786e\u6027\u3002", "motivation": "\u4f20\u7edf\u9759\u6001\u5206\u6790\u5668\u4f9d\u8d56\u8bed\u6cd5\u6a21\u5f0f\u96be\u68c0\u6d4b\u5faa\u73af\u8bed\u4e49\u6f0f\u6d1e\uff0c\u5bfc\u81f4\u8d44\u6e90\u8017\u5c3d\u6216\u5b89\u5168\u98ce\u9669\uff1b\u672c\u5730LLM\u53ef\u79bb\u7ebf\u8fd0\u884c\uff0c\u89e3\u51b3\u9690\u79c1\u3001\u5ef6\u8fdf\u548c\u4f9d\u8d56\u95ee\u9898\u3002", "method": "\u8bbe\u8ba1\u7ed3\u6784\u5316\u63d0\u793a\u6846\u67b6\uff0c\u5305\u542b\u8bed\u8a00\u611f\u77e5\u3001\u9632\u5e7b\u899a\u7b49\u4fdd\u969c\u529f\u80fd\uff0c\u805a\u7126\u63a7\u5236\u9519\u8bef\u3001\u5b89\u5168\u98ce\u9669\u548c\u8d44\u6e90\u4f4e\u6548\u4e09\u7c7b\u8bae\u9898\uff1b\u4f7f\u7528\u8fed\u4ee3\u63d0\u793a\u6d4b\u8bd5\u672c\u5730\u90e8\u7f72\u7684LLaMA 3.2 (3B) \u548c Phi 3.5 (4B) \u6a21\u578b\u3002", "result": "Phi\u6a21\u578b\u5728\u7cbe\u786e\u7387\u3001\u53ec\u56de\u7387\u548cF1\u5206\u6570\u4e0a\u4f18\u4e8eLLaMA\u6a21\u578b\uff0c\u6027\u80fd\u7531\u4eba\u5de5\u57fa\u51c6\u9a8c\u8bc1\u3002", "conclusion": "\u5f3a\u8c03\u6709\u6548\u63d0\u793a\u8bbe\u8ba1\u5bf9\u672c\u5730\u5927\u8bed\u8a00\u6a21\u578b\u5b9e\u73b0\u5b89\u5168\u9ad8\u6548\u4ee3\u7801\u6f0f\u6d1e\u5206\u6790\u81f3\u5173\u91cd\u8981\uff0c\u4e3a\u5f00\u53d1\u63d0\u4f9b\u5b9e\u7528\u5de5\u5177\u3002"}}
{"id": "2601.15904", "categories": ["cs.NI"], "pdf": "https://arxiv.org/pdf/2601.15904", "abs": "https://arxiv.org/abs/2601.15904", "authors": ["Hossein Mohammadalizadeh", "Holger Karl"], "title": "Dynamic Server Allocation Under Stochastic Switchover on Time-Varying Links", "comment": null, "summary": "Dynamic resource allocation to parallel queues is a cornerstone of network scheduling, yet classical solutions often fail when accounting for the overhead of switching delays to queues with superior link conditions. In particular, system performance is further degraded when switching delays are stochastic and inhomogeneous. In this domain, the myopic, Max-Weight policy struggles, as it is agnostic to switching delays. This paper introduces ACI, a non-myopic, frame-based scheduling framework that directly amortizes these switching delays. We first use a Lyapunov drift analysis to prove that backlog-driven ACI is throughput-optimal with respect to a scaled capacity region; then validate ACI's effectiveness on multi-UAV networks with an FSO backhaul. Finally, we demonstrate how adapting its core urgency metric provides the flexibility to navigate the throughput-latency trade-off.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2601.15687", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.15687", "abs": "https://arxiv.org/abs/2601.15687", "authors": ["Khusrav Badalov", "Young Yoon"], "title": "FARM: Field-Aware Resolution Model for Intelligent Trigger-Action Automation", "comment": null, "summary": "Trigger-Action Programming (TAP) platforms such as IFTTT and Zapier enable Web of Things (WoT) automation by composing event-driven rules across heterogeneous services. A TAP applet links a trigger to an action and must bind trigger outputs (ingredients) to action inputs (fields) to be executable. Prior work largely treats TAP as service-level prediction from natural language, which often yields non-executable applets that still require manual configuration. We study the function-level configuration problem: generating complete applets with correct ingredient-to-field bindings. We propose FARM (Field-Aware Resolution Model), a two-stage architecture for automated applet generation with full configuration. Stage 1 trains contrastive dual encoders with selective layer freezing over schema-enriched representations, retrieving candidates from 1,724 trigger functions and 1,287 action functions (2.2M possible trigger-action pairs). Stage 2 performs selection and configuration using an LLM-based multi-agent pipeline. It includes intent analysis, trigger selection, action selection via cross-schema scoring, and configuration verification. Agents coordinate through shared state and agreement-based selection. FARM achieves 81% joint accuracy on Gold (62% Noisy, 70% One-shot) at the function level, where both trigger and action functions must match the ground truth. For comparison with service-level baselines, we map functions to their parent services and evaluate at the service level. FARM reaches 81% joint accuracy and improves over TARGE by 23 percentage points. FARM also generates ingredient-to-field bindings, producing executable automation configurations.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faFARM\u53cc\u9636\u6bb5\u6a21\u578b\u89e3\u51b3TAP\u7cfb\u7edf\u7684\u529f\u80fd\u7ea7\u914d\u7f6e\u95ee\u9898\uff0c\u5b9e\u73b0\u4ece\u81ea\u7136\u8bed\u8a00\u751f\u6210\u53ef\u6267\u884c\u7684\u81ea\u52a8\u5316\u89c4\u5219\uff0c\u5305\u62ec\u6b63\u786e\u7684\u8f93\u5165-\u8f93\u51fa\u7ed1\u5b9a\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u5c06TAP\u89c6\u4e3a\u670d\u52a1\u7ea7\u9884\u6d4b\uff0c\u5e38\u4ea7\u751f\u9700\u624b\u52a8\u914d\u7f6e\u7684\u975e\u53ef\u6267\u884c\u7a0b\u5e8f\u3002\u672c\u6587\u805a\u7126\u529f\u80fd\u7ea7\u914d\u7f6e\u95ee\u9898\uff1a\u751f\u6210\u542b\u5b8c\u6574\u6b63\u786e\u7ed1\u5b9a\u5173\u7cfb\u7684\u53ef\u6267\u884c\u7a0b\u5e8f\u3002", "method": "\u91c7\u7528\u4e24\u9636\u6bb5\u67b6\u6784\uff1aStage 1\u7528\u6a21\u5f0f\u589e\u5f3a\u8868\u793a\u8bad\u7ec3\u5bf9\u6bd4\u53cc\u7f16\u7801\u5668\uff0c\u4ece1724\u4e2a\u89e6\u53d1/1287\u4e2a\u52a8\u4f5c\u51fd\u6570\u4e2d\u68c0\u7d22\u5019\u9009\u5bf9\uff1bStage 2\u901a\u8fc7LLM\u591a replace pipeline\u6267\u884c\u9009\u62e9\u4e0e\u914d\u7f6e\uff0c\u542b\u610f\u56fe\u5206\u6790\u3001\u8de8\u6a21\u5f0f\u8bc4\u5206\u548c\u9a8c\u8bc1\u6a21\u5757\u3002", "result": "\u5728\u529f\u80fd\u7ea7\u6d4b\u8bd5\u4e2d\u8fbe\u523081%\u8054\u5408\u51c6\u786e\u7387\uff08\u566a\u58f0\u6570\u636e62%\uff0c\u5355\u6837\u672c70%\uff09\uff0c\u670d\u52a1\u7ea7\u51c6\u786e\u738781%\u4e14\u8d85\u8d8a\u57fa\u7ebf23\u4e2a\u767e\u5206\u70b9\uff0c\u5e76\u6210\u529f\u751f\u6210\u53ef\u6267\u884c\u7ed1\u5b9a\u914d\u7f6e\u3002", "conclusion": "FARM\u6a21\u578b\u663e\u8457\u63d0\u5347\u81ea\u52a8\u5316\u89c4\u5219\u7684\u751f\u6210\u8d28\u91cf\uff0c\u9996\u6b21\u5b9e\u73b0\u529f\u80fd\u7ea7\u53ef\u6267\u884c\u914d\u7f6e\u7684\u7aef\u5230\u7aef\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2601.15879", "categories": ["cs.SE", "cs.CL"], "pdf": "https://arxiv.org/pdf/2601.15879", "abs": "https://arxiv.org/abs/2601.15879", "authors": ["Jiajun Zhang", "Zeyu Cui", "Lei Zhang", "Jian Yang", "Jiaxi Yang", "Qiang Liu", "Zilei Wang", "Binyuan Hui", "Liang Wang", "Junyang Lin"], "title": "Evaluating and Achieving Controllable Code Completion in Code LLM", "comment": null, "summary": "Code completion has become a central task, gaining significant attention with the rise of large language model (LLM)-based tools in software engineering. Although recent advances have greatly improved LLMs' code completion abilities, evaluation methods have not advanced equally. Most current benchmarks focus solely on functional correctness of code completions based on given context, overlooking models' ability to follow user instructions during completion-a common scenario in LLM-assisted programming. To address this limitation, we present the first instruction-guided code completion benchmark, Controllable Code Completion Benchmark (C3-Bench), comprising 2,195 carefully designed completion tasks. Through comprehensive evaluation of over 40 mainstream LLMs across C3-Bench and conventional benchmarks, we reveal substantial gaps in instruction-following capabilities between open-source and advanced proprietary models during code completion tasks. Moreover, we develop a straightforward data synthesis pipeline that leverages Qwen2.5-Coder to generate high-quality instruction-completion pairs for supervised fine-tuning (SFT). The resulting model, Qwen2.5-Coder-C3, achieves state-of-the-art performance on C3-Bench. Our findings provide valuable insights for enhancing LLMs' code completion and instruction-following capabilities, establishing new directions for future research in code LLMs. To facilitate reproducibility and foster further research in code LLMs, we open-source all code, datasets, and models.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2601.16009", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2601.16009", "abs": "https://arxiv.org/abs/2601.16009", "authors": ["Giovanna Broccia", "Sira Vegas", "Alessio Ferrari"], "title": "The Role of Cognitive Abilities in Requirements Inspection: Comparing UML and Textual Representations", "comment": null, "summary": "The representation of requirements plays a critical role in the accuracy of requirements inspection. While visual representations, such as UML diagrams, are widely used alongside text-based requirements, their effectiveness in supporting inspection is still debated. Cognitive abilities, such as working memory and mental rotation skills, may also influence inspection accuracy. This study aims to evaluate whether the use of UML sequence diagrams alongside text-based requirements improves the accuracy of requirements inspection compared to text-based requirements alone and to explore whether cognitive abilities are associated with differences in performance across the two treatments (text vs text with UML support). We conducted a crossover experiment with 38 participants to assess the accuracy of requirements inspection under the two treatments in terms of issues found and justifications provided. Linear mixed-effects and generalized linear models were used to analyse the effects of treatment, period, sequence, and cognitive abilities. The results indicate a significant three-way interaction between representation type, working memory capacity, and mental rotation ability. This finding suggests that the effectiveness of UML support is not uniform across individuals: participants with high scores in both cognitive abilities experienced reduced performance when using UML for violation detection. Conversely, the same cognitive profile was associated with improved justification accuracy under UML-aided inspection, indicating that higher cognitive abilities may support deeper reasoning processes when dealing with multi-modal information, i.e., diagrams and text.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2601.16080", "categories": ["cs.SE", "cs.CY"], "pdf": "https://arxiv.org/pdf/2601.16080", "abs": "https://arxiv.org/abs/2601.16080", "authors": ["Oleksandr Kosenkov", "Ehsan Zabardast", "Jannik Fischbach", "Tony Gorschek", "Daniel Mendez"], "title": "Towards a Goal-Centric Assessment of Requirements Engineering Methods for Privacy by Design", "comment": "The paper has been accepted for the 32nd International Working Conference on Requirements Engineering: Foundation for Software Quality (REFSQ 2026)", "summary": "Implementing privacy by design (PbD) according to the General Data Protection Regulation (GDPR) is met with a growing number of requirements engineering (RE) approaches. However, the question of which RE method for PbD fits best the goals of organisations remains a challenge. We report our endeavor to close this gap by synthesizing a goal-centric approach for PbD methods assessment. We used literature review, interviews, and validation with practitioners to achieve the goal of our study. As practitioners do not approach PbD systematically, we suggest that RE methods for PbD should be assessed against organisational goals, rather than process characteristics only. We hope that, when further developed, the goal-centric approach could support the development, selection, and tailoring of RE practices for PbD.", "AI": {"tldr": "\u672c\u6587\u9488\u5bf9GDPR\u9690\u79c1\u8bbe\u8ba1\u6807\u51c6\u63d0\u51fa\u76ee\u6807\u5bfc\u5411\u8bc4\u4f30\u6846\u67b6\uff0c\u901a\u8fc7\u6574\u5408\u6587\u732e\u4e0e\u5b9e\u8df5\u53cd\u9988\u89e3\u51b3\u73b0\u6709\u9700\u6c42\u5de5\u7a0b\u65b9\u6cd5\u5339\u914d\u7ec4\u7ec7\u76ee\u6807\u7684\u96be\u9898\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u4e2d\u9690\u79c1\u8bbe\u8ba1\u7684\u9700\u6c42\u5de5\u7a0b\u65b9\u6cd5\u867d\u591a\uff0c\u4f46\u7f3a\u4e4f\u7cfb\u7edf\u6027\u8bc4\u4f30\u6807\u51c6\u5e2e\u52a9\u7ec4\u7ec7\u9009\u62e9\u7b26\u5408\u81ea\u8eab\u76ee\u6807\u7684\u6700\u4f73\u65b9\u6848", "method": "\u7ed3\u5408\u6587\u732e\u7efc\u8ff0\u3001\u4ece\u4e1a\u8005\u8bbf\u8c08\u4e0e\u5b9e\u8df5\u9a8c\u8bc1\uff0c\u8bbe\u8ba1\u5e76\u9a8c\u8bc1\u76ee\u6807\u4e2d\u5fc3\u5316\u8bc4\u4f30\u65b9\u6cd5", "result": "\u53d1\u73b0\u4ece\u4e1a\u8005\u7f3a\u4e4f\u7cfb\u7edf\u6027\u9690\u79c1\u8bbe\u8ba1\u5b9e\u8df5\uff0c\u5f3a\u8c03\u8bc4\u4f30\u9700\u57fa\u4e8e\u7ec4\u7ec7\u76ee\u6807\u800c\u975e\u4ec5\u6d41\u7a0b\u7279\u5f81", "conclusion": "\u76ee\u6807 \u043d\u0430\u043f\u0438\u0441\u0430\u4e2d\u5fc3\u6846\u67b6\u53ef\u6307\u5bfc\u672a\u6765\u9690\u79c1\u8bbe\u8ba1\u9700\u6c42\u5de5\u7a0b\u7684\u5f00\u53d1\u3001\u9009\u62e9\u4e0e\u5b9a\u5236\u5316\u5e94\u7528"}}
