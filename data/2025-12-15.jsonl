{"id": "2512.11223", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2512.11223", "abs": "https://arxiv.org/abs/2512.11223", "authors": ["Sasara Shimizu", "Yoshiki Higo"], "title": "Coverage Isn't Enough: SBFL-Driven Insights into Manually Created vs. Automatically Generated Tests", "comment": null, "summary": "The testing phase is an essential part of software development, but manually creating test cases can be time-consuming. Consequently, there is a growing need for more efficient testing methods. To reduce the burden on developers, various automated test generation tools have been developed, and several studies have been conducted to evaluate the effectiveness of the tests they produce. However, most of these studies focus primarily on coverage metrics, and only a few examine how well the tests support fault localization-particularly using artificial faults introduced through mutation testing. In this study, we compare the SBFL (Spectrum-Based Fault Localization) score and code coverage of automatically generated tests with those of manually created tests. The SBFL score indicates how accurately faults can be localized using SBFL techniques. By employing SBFL score as an evaluation metric-an approach rarely used in prior studies on test generation-we aim to provide new insights into the respective strengths and weaknesses of manually created and automatically generated tests. Our experimental results show that automatically generated tests achieve higher branch coverage than manually created tests, but their SBFL score is lower, especially for code with deeply nested structures. These findings offer guidance on how to effectively combine automatically generated and manually created testing approaches."}
{"id": "2512.11398", "categories": ["cs.SE", "cs.MA"], "pdf": "https://arxiv.org/pdf/2512.11398", "abs": "https://arxiv.org/abs/2512.11398", "authors": ["Qiuming Luo", "Yanming Lei", "Kunzhong Wu", "Yixuan Cao", "Chengjian Liu"], "title": "AutoFSM: A Multi-agent Framework for FSM Code Generation with IR and SystemC-Based Testing", "comment": "This version corrects a typo in the section title (\"Intruction\" -> \"Introduction\") that appears in the published version", "summary": "With the rapid advancement of large language models (LLMs) in code generation, their applications in hardware design are receiving growing attention. However, existing LLMs face several challenges when generating Verilog code for finite state machine (FSM) control logic, including frequent syntax errors, low debugging efficiency, and heavy reliance on test benchmarks. To address these challenges, this paper proposes AutoFSM, a multi-agent collaborative framework designed for FSM code generation tasks. AutoFSM introduces a structurally clear intermediate representation (IR) to reduce syntax error rate during code generation and provides a supporting toolchain to enable automatic translation from IR to Verilog. Furthermore, AutoFSM is the first to integrate SystemC-based modeling with automatic testbench generation, thereby improving debugging efficiency and feedback quality. To systematically evaluate the framework's performance, we construct SKT-FSM, the first hierarchical FSM benchmark in the field, comprising 67 FSM samples across different complexity levels. Experimental results show that, under the same base LLM, AutoFSM consistently outperforms the open-source framework MAGE on the SKT-FSM benchmark, achieving up to an 11.94% improvement in pass rate and up to a 17.62% reduction in syntax error rate. These results demonstrate the potential of combining LLMs with structured IR and automated testing to improve the reliability and scalability of register-transfer level (RTL) code generation."}
{"id": "2512.11402", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.11402", "abs": "https://arxiv.org/abs/2512.11402", "authors": ["Aryan Gupta", "Y. Raghu Reddy"], "title": "REMODEL-LLM: Transforming C code to Java using LLMs", "comment": null, "summary": "The automated translation of C code to Java code is a notoriously difficult task, fraught with challenges stemming from fundamental paradigm shifts (procedural vs. Object Oriented), memory models (manual pointers vs. Garbage Collection), and incompatible data types. This paper investigates the efficacy of 19 small, quantized LLMs (under 20 billion parameters) for the C to Java translation task. We use a novel, hybrid pipeline that leverages Abstract Syntax Trees (ASTs) for semantic decomposition and employs a highly constrained, rule based prompting strategy. The results are stark: a clear multi tiered performance divide emerged. The vast majority of models (Tier 3, e.g., llama3.1, gemma3, starcoder2) failed 100\\% of the tests, proving incapable of generating even basic, runnable Java boilerplate. A small middle tier (Tier 2, e.g., mistral-nemo and mistral) produced runnable code but was plagued by dangerous semantic failures and wrong translations. Only three models (Tier 1: phi4, deepseek-coder-v2, codeqwen) proved viable, passing over 50\\% of the test suite. Even these top models failed on the most complex C concepts, such as function pointers, sizeof, and enum logic, revealing a hard ceiling for the reasoning capabilities of current quantized models."}
{"id": "2512.11482", "categories": ["cs.SE", "cs.AI", "cs.CR"], "pdf": "https://arxiv.org/pdf/2512.11482", "abs": "https://arxiv.org/abs/2512.11482", "authors": ["Melih Catal", "Pooja Rani", "Harald C. Gall"], "title": "Towards Privacy-Preserving Code Generation: Differentially Private Code Language Models", "comment": null, "summary": "Large language models specialized for code (CodeLLMs) have demonstrated remarkable capabilities in generating code snippets, documentation, and test cases. However, despite their promising capabilities, CodeLLMs can inadvertently memorize and reproduce snippets from their training data, which poses risks of privacy breaches and intellectual property violations. These risks restrict the deployment of CodeLLMs in sensitive domains and limit their training datasets to publicly available sources. To mitigate the memorization risk without compromising their task performance, we apply Differential Privacy (DP) to CodeLLMs. To the best of our knowledge, this is the first comprehensive study that systematically evaluates the effectiveness of DP in CodeLLMs. DP adds calibrated noise to the training process to protect individual data points while still allowing the model to learn useful patterns. To this end, we first identify and understand the driving reasons of the memorization behaviour of the CodeLLMs during their fine-tuning. Then, to address this issue, we empirically evaluate the effect of DP on mitigating memorization while preserving code generation capabilities. Our findings show that DP substantially reduces memorization in CodeLLMs across all the tested snippet types. The snippet types most prone to memorization are also the most effectively mitigated by DP. Furthermore, we observe that DP slightly increases perplexity but preserves, and can even enhance, the code generation capabilities of CodeLLMs, which makes it feasible to apply DP in practice without significantly compromising model utility. Finally, we analyze the impact of DP on training efficiency and energy consumption, finding that DP does not significantly affect training time or energy usage, making it a practical choice for privacy-preserving CodeLLMs training."}
{"id": "2512.10974", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2512.10974", "abs": "https://arxiv.org/abs/2512.10974", "authors": ["Sohan Kumar Pande", "Sanjaya Kumar Panda", "Preeti Ranjan Sahu"], "title": "An Efficient Approach for Energy Conservation in Cloud Computing Environment", "comment": null, "summary": "Recent trends of technology have explored a numerous applications of cloud services, which require a significant amount of energy. In the present scenario, most of the energy sources are limited and have a greenhouse effect on the environment. Therefore, it is the need of the hour that the energy consumed by the cloud service providers must be reduced and it is a great challenge to the research community to develop energy-efficient algorithms. To design the same, some researchers tried to maximize the average resource utilization, whereas some researchers tried to minimize the makespan. However, they have not considered different types of resources that are present in the physical machines. In this paper, we propose a task scheduling algorithm, which tries to improve utilization of resources (like CPU, disk, I/O) explicitly, which in turn increases the utilization of active resources. For this, the proposed algorithm uses a fitness value, which is a function of CPU, disk and I/O utilization, and processing time of the task. To demonstrate the performance of the proposed algorithm, extensive simulations are performed on both proposed algorithm and existing algorithm MaxUtil using synthetic datasets. From the simulation results, it can be observed that the proposed algorithm is a better energy-efficient algorithm and consumes less energy than the MaxUtil algorithm."}
{"id": "2512.11094", "categories": ["cs.NI"], "pdf": "https://arxiv.org/pdf/2512.11094", "abs": "https://arxiv.org/abs/2512.11094", "authors": ["Shengkai Lin", "Kairui Zhou", "Yibo Wu", "Hongtao Zhang", "Qinwei Yang", "Wei Zhang", "Arvind Krishnamurthy", "Shizhen Zhao"], "title": "SHIFT: An RDMA Failure-Resilient Layer for Distributed Training", "comment": null, "summary": "With gang scheduling in large-scale distributed Large Language Model training, a single network anomaly can propagate and cause complete task failure. The frequency of such anomalies increases with network scale. However, existing fault-tolerance mechanisms, such as checkpointing and runtime resilience methods, primarily operate at the application layer and inevitably cause disruptions in training progress.\n  We propose to address this challenge by introducing fault tolerance at the Remote Direct Memory Access (RDMA) layer and integrating it with existing application-layer techniques. We present SHIFT, a fault-resilient layer over RDMA that enables seamless redirection of RDMA traffic across different intra-host NICs. By allowing applications to continue execution in the presence of network anomalies until the next checkpoint, SHIFT effectively minimizes training progress loss. SHIFT is designed to be application-agnostic, transparent to applications, and low-overhead.\n  Through a carefully designed failure state machine and control flow, unmodified applications such as PyTorch with NCCL can run with RDMA-level fault tolerance. Experimental results demonstrate that SHIFT introduces minimal data path overhead while ensuring application continuity under network failures."}
{"id": "2512.11247", "categories": ["cs.MA", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.11247", "abs": "https://arxiv.org/abs/2512.11247", "authors": ["Iftekharul Islam", "Weizi Li"], "title": "Multi-Objective Reinforcement Learning for Large-Scale Mixed Traffic Control", "comment": null, "summary": "Effective mixed traffic control requires balancing efficiency, fairness, and safety. Existing approaches excel at optimizing efficiency and enforcing safety constraints but lack mechanisms to ensure equitable service, resulting in systematic starvation of vehicles on low-demand approaches. We propose a hierarchical framework combining multi-objective reinforcement learning for local intersection control with strategic routing for network-level coordination. Our approach introduces a Conflict Threat Vector that provides agents with explicit risk signals for proactive conflict avoidance, and a queue parity penalty that ensures equitable service across all traffic streams. Extensive experiments on a real-world network across different robot vehicle (RV) penetration rates demonstrate substantial improvements: up to 53% reductions in average wait time, up to 86% reductions in maximum starvation, and up to 86\\% reduction in conflict rate compared to baselines, while maintaining fuel efficiency. Our analysis reveals that strategic routing effectiveness scales with RV penetration, becoming increasingly valuable at higher autonomy levels. The results demonstrate that multi-objective optimization through well-curated reward functions paired with strategic RV routing yields significant benefits in fairness and safety metrics critical for equitable mixed-autonomy deployment."}
{"id": "2512.11550", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2512.11550", "abs": "https://arxiv.org/abs/2512.11550", "authors": ["Yifan Zhang", "Zhiheng Chen", "Ye Qiao", "Sitao Huang"], "title": "PD-Swap: Prefill-Decode Logic Swapping for End-to-End LLM Inference on Edge FPGAs via Dynamic Partial Reconfiguration", "comment": null, "summary": "Aggressively quantized large language models (LLMs), such as BitNet-style 1.58-bit Transformers with ternary weights, make it feasible to deploy generative AI on low-power edge FPGAs. However, as prompts grow to tens of thousands of tokens, edge hardware performance drops sharply with sequence length due to quadratic prefill cost and rapidly increasing KV-cache bandwidth demands, making inference latency of longer context length a first-order system concern. Recent studies on LLMs expose a fundamental prefill-decode asymmetry: prefill is compute-bound and dominated by dense matrix-matrix operations, whereas decoding is memory-bandwidth-bound and dominated by KV-cache traffic. A static accelerator must provision resources and a single dataflow for both regimes, leading to duplicated attention logic, underutilized fabric, and tight LUT/URAM limits that cap model size and usable context. We propose a prefill--decode disaggregated LLM accelerator, PD-Swap, that uses Dynamic Partial Reconfiguration (DPR) to time-multiplex the attention module on edge FPGAs. The core table-lookup ternary matrix multiplication and weight-buffering engines remain static, while the attention subsystem is a reconfigurable partition with two phase-specialized architectures: a compute-heavy, token-parallel prefill engine and a bandwidth-optimized, KV-cache-centric decoding engine. A roofline-inspired model and design space exploration jointly optimize reconfigurable-region size, parallelism under reconfiguration and routability constraints, and reconfiguration latency is hidden by computation latency. PD-Swap achieves up to 27~tokens/s decoding throughput, outperforming prior state-of-the-art works by 1.3x--2.1x (larger gains at longer context lengths), without extra area cost."}
{"id": "2512.11527", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2512.11527", "abs": "https://arxiv.org/abs/2512.11527", "authors": ["Xi Wang", "Shuo Shi", "Chenyu Wu"], "title": "Mini-SFC: A Comprehensive Simulation Framework for Orchestration and Management of Service Function Chains", "comment": null, "summary": "In the continuously evolving cloud computing and network environment, service function chain (SFC) plays a crucial role in implementing complex services in the network with its flexible deployment capabilities. To address the limitations of existing SFC simulation tools, this paper introduces Mini-SFC, a modular simulation framework that supports both numerical and container-based virtual simulations, while also supporting online dynamic topology adjustments. As an open-source platform emphasizing user-friendliness, Mini-SFC facilitates rapid algorithm verification and realistic service deployment validation. By simplifying module design and providing standardized solver interfaces, Mini-SFC significantly shortens the learning curve for researchers and enhances the flexibility and scalability required for advanced SFC management and optimization. For readers interested in exploring or utilizing Mini-SFC, more information is available on the official project page."}
{"id": "2512.10977", "categories": ["cs.DC", "cs.AR", "cs.PL"], "pdf": "https://arxiv.org/pdf/2512.10977", "abs": "https://arxiv.org/abs/2512.10977", "authors": ["Alec M. Hammond", "Aram Markosyan", "Aman Dontula", "Simon Mahns", "Zacharias Fisches", "Dmitrii Pedchenko", "Keyur Muzumdar", "Natacha Supper", "Mark Saroufim", "Joe Isaacson", "Laura Wang", "Warren Hunt", "Kaustubh Gondkar", "Roman Levenstein", "Gabriel Synnaeve", "Richard Li", "Jacob Kahn", "Ajit Mathews"], "title": "Agentic Operator Generation for ML ASICs", "comment": null, "summary": "We present TritorX, an agentic AI system designed to generate functionally correct Triton PyTorch ATen kernels at scale for emerging accelerator platforms. TritorX integrates open-source large language models with a custom linter, JIT compilation, and a PyTorch OpInfo-based test harness. This pipeline is compatible with both real Meta Training and Inference Accelerator (MTIA) silicon and in hardware simulation environments for next-generation devices. In contrast to previous kernel-generation approaches that prioritize performance for a limited set of high-usage kernels, TritorX prioritizes coverage. Our system emphasizes correctness and generality across the entire operator set, including diverse data types, shapes, and argument patterns. In our experiments, TritorX successfully generated kernels and wrappers for 481 unique ATen operators that pass all corresponding PyTorch OpInfo tests (over 20,000 in total). TritorX paves the way for overnight generation of complete PyTorch ATen backends for new accelerator platforms."}
{"id": "2512.11156", "categories": ["cs.NI"], "pdf": "https://arxiv.org/pdf/2512.11156", "abs": "https://arxiv.org/abs/2512.11156", "authors": ["Mostafa Abdollahi", "Wenjun Yang", "Jianping Pan"], "title": "BIER-Star: Stateless Geographic Multicast for Scalable Satellite-Terrestrial Integration", "comment": "Accepted to present at IEEE CCNC 2026", "summary": "The rapid expansion of LEO satellite constellations has enabled an integrated terrestrial network and non-terrestrial network (TN-NTN), connecting diverse users such as aircraft, ships, and remote communities. These networks increasingly need a scalable and efficient multicast protocol for critical applications like emergency alerts, large-scale software updates, and real-time broadcasting. However, traditional multicast protocols, such as IP-based multicast and software-defined multicast approaches, introduce significant control overhead and struggle to adapt to the dynamic and mobile nature of satellite topologies. This paper presents BIER-Star, a stateless multicast protocol designed for the integrated TN-NTN. BIER-Star uses a two-layer geospatial gridding scheme (i.e., H3) to encode destinations as Earth- and space-cell identifiers rather than per-terminal addresses. This cell-based abstraction shortens the header bitstring, simplifies forwarding, and eliminates per-flow state and complex signaling. Our simulations indicate that BIER-Star reduces header size versus BIER and avoids geographic path-finding failures seen in greedy methods."}
{"id": "2512.11689", "categories": ["cs.MA"], "pdf": "https://arxiv.org/pdf/2512.11689", "abs": "https://arxiv.org/abs/2512.11689", "authors": ["Manuela Chacon-Chamorro", "Juan Sebastián Pinzón", "Rubén Manrique", "Luis Felipe Giraldo", "Nicanor Quijano"], "title": "Evaluating Cooperative Resilience in Multiagent Systems: A Comparison Between Humans and LLMs", "comment": "Supplementary material in https://github.com/mavivi95/resilience_humans_vs_LLM/blob/main/Supplementary_File.pdf", "summary": "This paper presents a comparative analysis of cooperative resilience in multi-agent systems, defined as the ability to anticipate, resist, recover from, and transform to disruptive events that affect collective well-being. We focus on mixed-motive social dilemmas instantiated as a \\textit{Tragedy of the Commons} environment from the Melting Pot suite, where we systematically compare human groups and Large Language Model (LLM)-based agents, each evaluated with and without explicit communication. Cooperative resilience is assessed under a continuously disruptive condition induced by a persistent unsustainable consumption bot, together with intermittent environmental shocks implemented as stochastic removal of shared resources across scenarios. This experimental design establishes a benchmark for cooperative resilience across agent architectures and interaction modalities, constituting a key step toward systematically comparing humans and LLM-based agents. Using this framework, we find that human groups with communication achieve the highest cooperative resilience compared to all other groups. Communication also improves the resilience of LLM agents, but their performance remains below human levels. Motivated by the performance of humans, we further examine a long-horizon setting with harsher environmental conditions, where humans sustain the shared resource and maintain high resilience in diverse disruption scenarios. Together, these results suggest that human decision-making under adverse social conditions can inform the design of artificial agents that promote prosocial and resilient behaviors."}
{"id": "2512.10977", "categories": ["cs.DC", "cs.AR", "cs.PL"], "pdf": "https://arxiv.org/pdf/2512.10977", "abs": "https://arxiv.org/abs/2512.10977", "authors": ["Alec M. Hammond", "Aram Markosyan", "Aman Dontula", "Simon Mahns", "Zacharias Fisches", "Dmitrii Pedchenko", "Keyur Muzumdar", "Natacha Supper", "Mark Saroufim", "Joe Isaacson", "Laura Wang", "Warren Hunt", "Kaustubh Gondkar", "Roman Levenstein", "Gabriel Synnaeve", "Richard Li", "Jacob Kahn", "Ajit Mathews"], "title": "Agentic Operator Generation for ML ASICs", "comment": null, "summary": "We present TritorX, an agentic AI system designed to generate functionally correct Triton PyTorch ATen kernels at scale for emerging accelerator platforms. TritorX integrates open-source large language models with a custom linter, JIT compilation, and a PyTorch OpInfo-based test harness. This pipeline is compatible with both real Meta Training and Inference Accelerator (MTIA) silicon and in hardware simulation environments for next-generation devices. In contrast to previous kernel-generation approaches that prioritize performance for a limited set of high-usage kernels, TritorX prioritizes coverage. Our system emphasizes correctness and generality across the entire operator set, including diverse data types, shapes, and argument patterns. In our experiments, TritorX successfully generated kernels and wrappers for 481 unique ATen operators that pass all corresponding PyTorch OpInfo tests (over 20,000 in total). TritorX paves the way for overnight generation of complete PyTorch ATen backends for new accelerator platforms."}
{"id": "2512.11589", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2512.11589", "abs": "https://arxiv.org/abs/2512.11589", "authors": ["Lukas Twist"], "title": "A Study of Library Usage in Agent-Authored Pull Requests", "comment": "5 pages, 3 tables", "summary": "Coding agents are becoming increasingly capable of completing end-to-end software engineering workflows that previously required a human developer, including raising pull requests (PRs) to propose their changes. However, we still know little about how these agents use libraries when generating code, a core part of real-world software development. To fill this gap, we study 26,760 agent-authored PRs from the AIDev dataset to examine three questions: how often do agents import libraries, how often do they introduce new dependencies (and with what versioning), and which specific libraries do they choose? We find that agents often import libraries (29.5% of PRs) but rarely add new dependencies (1.3% of PRs); and when they do, they follow strong versioning practices (75.0% specify a version), an improvement on direct LLM usage where versions are rarely mentioned. Generally, agents draw from a surprisingly diverse set of external libraries, contrasting with the limited \"library preferences\" seen in prior non-agentic LLM studies. Our results offer an early empirical view into how AI coding agents interact with today's software ecosystems."}
{"id": "2512.10979", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2512.10979", "abs": "https://arxiv.org/abs/2512.10979", "authors": ["Sima Attar-Khorasani", "Lincoln Sherpa", "Matthias Lieber", "Siavash Ghiasvand"], "title": "Seamless Transitions: A Comprehensive Review of Live Migration Technologies", "comment": "35 pages, 0 figures", "summary": "Live migration, a technology enabling seamless transition of operational computational entities between various hosts while preserving continuous functionality and client connectivity, has been the subject of extensive research. However, existing reviews often overlook critical technical aspects and practical challenges integral to the usage of live migration techniques in real-world scenarios. This work bridges this gap by integrating the aspects explored in existing reviews together with a comprehensive analysis of live migration technologies across multiple dimensions, with focus on migration techniques, migration units, and infrastructure characteristics. Despite efforts to make live migration widely accessible, its reliance on multiple system factors can create challenges. In certain cases, the complexities and resource demands outweigh the benefits, making its implementation hard to justify. The focus of this work is mainly on container based and virtual machine-based migration technologies, examining the current state of the art and the disparity in adoption between these two approaches. Furthermore, this work explores the impact of migration objectives and operational constraints on the usability and efficacy of existing technologies. By outlining current technical challenges and providing guidelines for future research and development directions, this work serves a dual purpose: first, to equip enthusiasts with a valuable resource on live migration, and second, to contribute to the advancement of live migration technologies and their practical implementation across diverse computing environments."}
{"id": "2512.11230", "categories": ["cs.NI"], "pdf": "https://arxiv.org/pdf/2512.11230", "abs": "https://arxiv.org/abs/2512.11230", "authors": ["Antoine Bernard", "Antoine Legrain", "Maroua Ben Attia", "Abdo Shabah"], "title": "Distributed Resource Allocation and Application Deployment in Mesh Edge Networks", "comment": "7 pages, 4 figures, 2 tables, 10 equations, workshop, STWiMob", "summary": "Virtual Network Embedding (VNE) approaches typically assume static or slowly-changing network topologies, but emerging applications require deployment in mobile environments where traditional methods become insufficient. This work extends VNE to constrained mesh networks of mobile edge devices, addressing the unique challenges of rapid topology changes and limited resources. We develop models incorporating device capabilities, connectivity, mobility and energy constraints to evaluate optimal deployment strategies for mobile edge environments. Our approach handles the dynamic nature of mobile networks through three allocation strategies: an integer linear program for optimal allocation, a greedy heuristic for immediate deployment, and a multi-objective genetic algorithm for balanced optimization. Our initial evaluation analyzes application acceptance rates, resource utilization, and latency performance under resource limitations. Results demonstrate improvements over traditional approaches, providing a foundation for VNE deployment in highly mobile environments."}
{"id": "2512.11398", "categories": ["cs.SE", "cs.MA"], "pdf": "https://arxiv.org/pdf/2512.11398", "abs": "https://arxiv.org/abs/2512.11398", "authors": ["Qiuming Luo", "Yanming Lei", "Kunzhong Wu", "Yixuan Cao", "Chengjian Liu"], "title": "AutoFSM: A Multi-agent Framework for FSM Code Generation with IR and SystemC-Based Testing", "comment": "This version corrects a typo in the section title (\"Intruction\" -> \"Introduction\") that appears in the published version", "summary": "With the rapid advancement of large language models (LLMs) in code generation, their applications in hardware design are receiving growing attention. However, existing LLMs face several challenges when generating Verilog code for finite state machine (FSM) control logic, including frequent syntax errors, low debugging efficiency, and heavy reliance on test benchmarks. To address these challenges, this paper proposes AutoFSM, a multi-agent collaborative framework designed for FSM code generation tasks. AutoFSM introduces a structurally clear intermediate representation (IR) to reduce syntax error rate during code generation and provides a supporting toolchain to enable automatic translation from IR to Verilog. Furthermore, AutoFSM is the first to integrate SystemC-based modeling with automatic testbench generation, thereby improving debugging efficiency and feedback quality. To systematically evaluate the framework's performance, we construct SKT-FSM, the first hierarchical FSM benchmark in the field, comprising 67 FSM samples across different complexity levels. Experimental results show that, under the same base LLM, AutoFSM consistently outperforms the open-source framework MAGE on the SKT-FSM benchmark, achieving up to an 11.94% improvement in pass rate and up to a 17.62% reduction in syntax error rate. These results demonstrate the potential of combining LLMs with structured IR and automated testing to improve the reliability and scalability of register-transfer level (RTL) code generation."}
{"id": "2512.10980", "categories": ["cs.DC", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.10980", "abs": "https://arxiv.org/abs/2512.10980", "authors": ["Akhmadillo Mamirov"], "title": "Reducing Fragmentation and Starvation in GPU Clusters through Dynamic Multi-Objective Scheduling", "comment": null, "summary": "GPU clusters have become essential for training and deploying modern AI systems, yet real deployments continue to report average utilization near 50%. This inefficiency is largely caused by fragmentation, heterogeneous workloads, and the limitations of static scheduling policies. This work presents a systematic evaluation of these issues and introduces three specialized dynamic schedulers: Hybrid Priority (HPS), Predictive Backfill (PBS), and Smart Batch (SBS). These schedulers are designed to improve utilization, fairness, and overall throughput in multi-tenant GPU clusters. We evaluate all schedulers using a controlled simulation of 1,000 AI jobs on a 64-GPU, 8-node cluster that includes a realistic mix of training, inference, and research workloads. Static baselines (FIFO, SJF, Shortest, Shortest-GPU) achieve 45 to 67% GPU utilization and 12.5 to 18.3 jobs per hour and experience severe starvation, with as many as 156 jobs waiting longer than 30 minutes. The dynamic schedulers significantly outperform these policies. HPS achieves the highest utilization (78.2%), highest throughput (25.8 jobs per hour), and the lowest fairness variance among dynamic methods (457), reducing starvation to 12 jobs. PBS improves fragmentation handling and reaches 76.1% utilization, while SBS increases efficiency for structurally similar jobs and reaches 74.6% utilization. Across all key metrics, including throughput, job wait times, fairness variance, and starvation, dynamic multi-objective schedulers consistently outperform single-objective heuristics. These results show that targeted and transparent scheduling strategies can meaningfully increase GPU efficiency in heterogeneous AI clusters and provide a practical foundation for future production scheduling frameworks."}
{"id": "2512.11667", "categories": ["cs.NI"], "pdf": "https://arxiv.org/pdf/2512.11667", "abs": "https://arxiv.org/abs/2512.11667", "authors": ["Gabriel Almeida", "João Paulo Esper", "Cleverson Nahum", "Audebaro Klautau", "Kleber Vieira Cardoso"], "title": "Toward Scalable VR-Cloud Gaming: An Attention-aware Adaptive Resource Allocation Framework for 6G Networks", "comment": null, "summary": "Virtual Reality Cloud Gaming (VR-CG) represents a demanding class of immersive applications, requiring high bandwidth, ultra-low latency, and intelligent resource management to ensure optimal user experience. In this paper, we propose a scalable and QoE-aware multi-stage optimization framework for resource allocation in VR-CG over 6G networks. Our solution decomposes the joint resource allocation problem into three interdependent stages: (i) user association and communication resource allocation; (ii) VR-CG game engine placement with adaptive multipath routing; and (iii) attention-aware scheduling and wireless resource allocation based on motion-to-photon latency. For each stage, we design specialized heuristic algorithms that achieve near-optimal performance while significantly reducing computational time. We introduce a novel user-centric QoE model based on visual attention to virtual objects, guiding adaptive resolution and frame rate selection. A dataset-driven evaluation demonstrates that, when compared against state-of-the-art approaches, our framework improves QoE by up to 50\\%, reduces communication resource usage by 75\\%, and achieves up to 35\\% cost savings, while maintaining an average optimality gap of 5\\%. Our proposed heuristics solve large-scale scenarios in under 0.1 seconds, highlighting their potential for real-time deployment in next-generation mobile networks."}
{"id": "2512.10987", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2512.10987", "abs": "https://arxiv.org/abs/2512.10987", "authors": ["Sumit Chongder"], "title": "Evaluation Framework for Centralized and Decentralized Aggregation Algorithm in Federated Systems", "comment": "13 pages, 14 figures, 2 tables", "summary": "In recent years, the landscape of federated learning has witnessed significant advancements, particularly in decentralized methodologies. This research paper presents a comprehensive comparison of Centralized Hierarchical Federated Learning (HFL) with Decentralized Aggregated Federated Learning (AFL) and Decentralized Continual Federated Learning (CFL) architectures. While HFL, in its centralized approach, faces challenges such as communication bottlenecks and privacy concerns due to centralized data aggregation, AFL and CFL provide promising alternatives by distributing computation and aggregation processes across devices. Through evaluation of Fashion MNIST and MNIST datasets, this study demonstrates the advantages of decentralized methodologies, showcasing how AFL and CFL outperform HFL in precision, recall, F1 score, and balanced accuracy. The analysis highlights the importance of decentralized aggregation mechanisms in AFL and CFL, which effectively enables collaborative model training across distributed devices. This comparative study contributes valuable insights into the evolving landscape of federated learning, guiding researchers and practitioners towards decentralized methodologies for enhanced performance in collaborative model training scenarios."}
{"id": "2512.10990", "categories": ["cs.DC", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.10990", "abs": "https://arxiv.org/abs/2512.10990", "authors": ["Jianli Jin", "Ziyang Lin", "Qianli Dong", "Yi Chen", "Jayanth Srinivasa", "Myungjin Lee", "Zhaowei Tan", "Fan Lai"], "title": "Dora: QoE-Aware Hybrid Parallelism for Distributed Edge AI", "comment": null, "summary": "With the proliferation of edge AI applications, satisfying user quality of experience (QoE) requirements, such as model inference latency, has become a first class objective, as these models operate in resource constrained settings and directly interact with users. Yet, modern AI models routinely exceed the resource capacity of individual devices, necessitating distributed execution across heterogeneous devices over variable and contention prone networks. Existing planners for hybrid (e.g., data and pipeline) parallelism largely optimize for throughput or device utilization, overlooking QoE, leading to severe resource inefficiency (e.g., unnecessary energy drain) or QoE violations under runtime dynamics.\n  We present Dora, a framework for QoE aware hybrid parallelism in distributed edge AI training and inference. Dora jointly optimizes heterogeneous computation, contention prone networks, and multi dimensional QoE objectives via three key mechanisms: (i) a heterogeneity aware model partitioner that determines and assigns model partitions across devices, forming a compact set of QoE compliant plans; (ii) a contention aware network scheduler that further refines these candidate plans by maximizing compute communication overlap; and (iii) a runtime adapter that adaptively composes multiple plans to maximize global efficiency while respecting overall QoEs. Across representative edge deployments, including smart homes, traffic analytics, and small edge clusters, Dora achieves 1.1--6.3 times faster execution and, alternatively, reduces energy consumption by 21--82 percent, all while maintaining QoE under runtime dynamics."}
{"id": "2512.11200", "categories": ["cs.DC", "cs.LG", "cs.PL"], "pdf": "https://arxiv.org/pdf/2512.11200", "abs": "https://arxiv.org/abs/2512.11200", "authors": ["Adilet Metinov", "Gulida M. Kudakeeva", "Gulnara D. Kabaeva"], "title": "Theoretical Foundations of GPU-Native Compilation for Rapid Code Iteration", "comment": "9 pages , 2 tables", "summary": "Current AI code generation systems suffer from significant latency bottlenecks due to CPU-GPU data transfers during compilation, execution, and testing phases. We establish theoretical foundations for three complementary approaches to GPU-native compilation that eliminate these transfers: (1) parallel traditional compilation adapted for GPU execution, (2) neural compilation using learned sequence-to-sequence translation with probabilistic verification, and (3) hybrid architectures combining both strategies. We derive latency and energy bounds demonstrating potential speedups of 10-100x for code iteration cycles. Our analysis shows that traditional GPU compilation provides 2-5x improvements through transfer elimination, neural compilation achieves 10-100x speedups via massive parallelism, and hybrid approaches offer practical deployment paths with guaranteed correctness. We formalize the probabilistic verification framework that enables trading compilation accuracy for parallel exploration, and discuss implications for self-improving AI systems and future analog computing substrates."}
{"id": "2512.11306", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2512.11306", "abs": "https://arxiv.org/abs/2512.11306", "authors": ["Tianyuan Wu", "Lunxi Cao", "Yining Wei", "Wei Gao", "Yuheng Zhao", "Dakai An", "Shaopan Xiong", "Zhiqiang Lv", "Ju Huang", "Siran Yang", "Yinghao Yu", "Jiamang Wang", "Lin Qu", "Wei Wang"], "title": "RollMux: Phase-Level Multiplexing for Disaggregated RL Post-Training", "comment": "17 pages, 15 figures", "summary": "Rollout-training disaggregation is emerging as the standard architecture for Reinforcement Learning (RL) post-training, where memory-bound rollout and compute-bound training are physically disaggregated onto purpose-built clusters to maximize hardware efficiency. However, the strict synchronization required by on-policy algorithms introduces severe dependency bubbles, forcing one cluster to idle while the dependent phase is running on the other. We present RollMux, a cluster scheduling framework that reclaims these bubbles through cross-cluster orchestration. RollMux is built on the insight that the structural idleness of one job can be effectively utilized by the active phase of another. To realize this, we introduce the co-execution group abstraction, which partitions the cluster into isolated locality domains. This abstraction enables a two-tier scheduling architecture: an inter-group scheduler that optimizes job placement using conservative stochastic planning, and an intra-group scheduler that orchestrates a provably optimal round-robin schedule. The group abstraction also imposes a residency constraint, ensuring that massive model states remain cached in host memory to enable \"warm-star\" context switching. We evaluate RollMux on a production-scale testbed with 328 H20 and 328 H800 GPUs. RollMux improves cost efficiency by 1.84x over standard disaggregation and 1.38x over state-of-the-art co-located baselines, all while achieving 100% SLO attainment."}
{"id": "2512.11512", "categories": ["cs.DC", "cs.SI"], "pdf": "https://arxiv.org/pdf/2512.11512", "abs": "https://arxiv.org/abs/2512.11512", "authors": ["Patrick D. Manya", "Eugene M. Mbuyi", "Gothy T. Ngoie", "Jordan F. Masakuna"], "title": "Enhanced Pruning for Distributed Closeness Centrality under Multi-Packet Messaging", "comment": "9 pages, 7 figures and 3 tables. Published in a local annal at the University of Kinshasa, although the annal is not indexed", "summary": "Identifying central nodes using closeness centrality is a critical task in analyzing large-scale complex networks, yet its decentralized computation remains challenging due to high communication overhead. Existing distributed approximation techniques, such as pruning, often fail to fully mitigate the cost of exchanging numerous data packets in large network settings. In this paper, we introduce a novel enhancement to the distributed pruning method specifically designed to overcome this communication bottleneck. Our core contribution is a technique that leverages multi-packet messaging, allowing nodes to batch and transmit larger, consolidated data blocks. This approach significantly reduces the number of exchanged messages and minimizes data loss without compromising the accuracy of the centrality estimates. We demonstrate that our multi-packet approach substantially outperforms the original pruning technique in both message efficiency (fewer overall messages) and computation time, preserving the core approximation properties of the baseline method. While we observe a manageable trade-off in increased per-node memory usage and local overhead, our findings show that this is outweighed by the gains in communication efficiency, particularly for very large networks and complex packet structures. Our work offers a more scalable and efficient solution for decentralized closeness centrality computation, promising a significant step forward for large-scale network analysis."}
{"id": "2512.11532", "categories": ["cs.DC", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2512.11532", "abs": "https://arxiv.org/abs/2512.11532", "authors": ["Chong Tang", "Hao Dai", "Jagmohan Chauhan"], "title": "Parallax: Runtime Parallelization for Operator Fallbacks in Heterogeneous Edge Systems", "comment": null, "summary": "The growing demand for real-time DNN applications on edge devices necessitates faster inference of increasingly complex models. Although many devices include specialized accelerators (e.g., mobile GPUs), dynamic control-flow operators and unsupported kernels often fall back to CPU execution. Existing frameworks handle these fallbacks poorly, leaving CPU cores idle and causing high latency and memory spikes. We introduce Parallax, a framework that accelerates mobile DNN inference without model refactoring or custom operator implementations. Parallax first partitions the computation DAG to expose parallelism, then employs branch-aware memory management with dedicated arenas and buffer reuse to reduce runtime footprint. An adaptive scheduler executes branches according to device memory constraints, meanwhile, fine-grained subgraph control enables heterogeneous inference of dynamic models. By evaluating on five representative DNNs across three different mobile devices, Parallax achieves up to 46% latency reduction, maintains controlled memory overhead (26.5% on average), and delivers up to 30% energy savings compared with state-of-the-art frameworks, offering improvements aligned with the responsiveness demands of real-time mobile inference."}
{"id": "2512.11634", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2512.11634", "abs": "https://arxiv.org/abs/2512.11634", "authors": ["Elia Palme", "Juan Pablo Dorsch", "Ali Khosravi", "Giovanni Pizzi", "Francesco Pagnamenta", "Andrea Ceriani", "Eirini Koutsaniti", "Rafael Sarmiento", "Ivano Bonesana", "Alejandro Dabin"], "title": "FirecREST v2: lessons learned from redesigning an API for scalable HPC resource access", "comment": "6 pages, 2 figures. Presented at Cray User Group 2025 conference at New York, USA (May 4-8, 2025)", "summary": "Introducing FirecREST v2, the next generation of our open-source RESTful API for programmatic access to HPC resources. FirecREST v2 delivers a 100x performance improvement over its predecessor. This paper explores the lessons learned from redesigning FirecREST from the ground up, with a focus on integrating enhanced security and high throughput as core requirements.\n  We provide a detailed account of our systematic performance testing methodology, highlighting common bottlenecks in proxy-based APIs with intensive I/O operations. Key design and architectural changes that enabled these performance gains are presented. Finally, we demonstrate the impact of these improvements, supported by independent peer validation, and discuss opportunities for further improvements."}
{"id": "2512.11643", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2512.11643", "abs": "https://arxiv.org/abs/2512.11643", "authors": ["Manideep Reddy Chinthareddy"], "title": "Stateless Snowflake: A Cloud-Agnostic Distributed ID Generator Using Network-Derived Identity", "comment": "12 pages, 3 tables, 1 figure", "summary": "Snowflake-style distributed ID generators are the industry standard for producing k-ordered, unique identifiers at scale. However, the traditional requirement for manually assigned or centrally coordinated worker IDs introduces significant friction in modern container-orchestrated environments (e.g., Kubernetes), where workloads are ephemeral and autoscaled. In such systems, maintaining stable worker identities requires complex stateful sets or external coordination services (e.g., ZooKeeper), negating the operational benefits of stateless microservices.\n  This paper presents a cloud-agnostic, container-native ID generation protocol that eliminates the dependency on explicit worker IDs. By deriving node uniqueness deterministically from ephemeral network properties - specifically the container's private IPv4 address - the proposed method removes the need for centralized coordination. We introduce a modified bit-allocation scheme (1-41-16-6) that accommodates 16 bits of network-derived entropy while preserving strict monotonicity. We validate the approach across AWS, GCP, and Azure environments. Evaluation results demonstrate that while the design has a theoretical single-node ceiling of approximately 64,000 TPS, in practical microservice deployments the network I/O dominates latency, resulting in end-to-end performance (approximately 31,000 TPS on a 3-node cluster) comparable to classic stateful generators while offering effectively unbounded horizontal scalability."}
{"id": "2512.11727", "categories": ["cs.DC", "cs.LG", "cs.NI"], "pdf": "https://arxiv.org/pdf/2512.11727", "abs": "https://arxiv.org/abs/2512.11727", "authors": ["Yuze He", "Ferdi Kossmann", "Srinivasan Seshan", "Peter Steenkiste"], "title": "ECCO: Leveraging Cross-Camera Correlations for Efficient Live Video Continuous Learning", "comment": null, "summary": "Recent advances in video analytics address real-time data drift by continuously retraining specialized, lightweight DNN models for individual cameras. However, the current practice of retraining a separate model for each camera suffers from high compute and communication costs, making it unscalable. We present ECCO, a new video analytics framework designed for resource-efficient continuous learning. The key insight is that the data drift, which necessitates model retraining, often shows temporal and spatial correlations across nearby cameras. By identifying cameras that experience similar drift and retraining a shared model for them, ECCO can substantially reduce the associated compute and communication costs. Specifically, ECCO introduces: (i) a lightweight grouping algorithm that dynamically forms and updates camera groups; (ii) a GPU allocator that dynamically assigns GPU resources across different groups to improve retraining accuracy and ensure fairness; and (iii) a transmission controller at each camera that configures frame sampling and coordinates bandwidth sharing with other cameras based on its assigned GPU resources. We conducted extensive evaluations on three distinctive datasets for two vision tasks. Compared to leading baselines, ECCO improves retraining accuracy by 6.7%-18.1% using the same compute and communication resources, or supports 3.3 times more concurrent cameras at the same accuracy."}
{"id": "2512.11775", "categories": ["cs.DC", "cs.CR", "cs.NI"], "pdf": "https://arxiv.org/pdf/2512.11775", "abs": "https://arxiv.org/abs/2512.11775", "authors": ["Ayush Nainwal", "Atharva Kamble", "Nitin Awathare"], "title": "Hypergraph based Multi-Party Payment Channel", "comment": null, "summary": "Public blockchains inherently offer low throughput and high latency, motivating off-chain scalability solutions such as Payment Channel Networks (PCNs). However, existing PCNs suffer from liquidity fragmentation-funds locked in one channel cannot be reused elsewhere-and channel depletion, both of which limit routing efficiency and reduce transaction success rates. Multi-party channel (MPC) constructions mitigate these issues, but they typically rely on leaders or coordinators, creating single points of failure and providing only limited flexibility for inter-channel payments.\n  We introduce Hypergraph-based Multi-Party Payment Channels (H-MPCs), a new off-chain construction that replaces bilateral channels with collectively funded hyperedges. These hyperedges enable fully concurrent, leaderless intra- and inter-hyperedge payments through verifiable, proposer-ordered DAG updates, offering significantly greater flexibility and concurrency than prior designs.\n  Our implementation on a 150-node network demonstrates a transaction success rate of approximately 94% without HTLC expiry or routing failures, highlighting the robustness of H-MPCs."}
