<div id=toc></div>

# Table of Contents

- [cs.MA](#cs.MA) [Total: 2]
- [cs.DC](#cs.DC) [Total: 14]
- [cs.NI](#cs.NI) [Total: 1]
- [cs.AR](#cs.AR) [Total: 1]
- [cs.SE](#cs.SE) [Total: 4]
- [cs.PF](#cs.PF) [Total: 1]


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [1] [Communication Enhances LLMs' Stability in Strategic Thinking](https://arxiv.org/abs/2602.06081)
*Nunzio Lore,Babak Heydari*

Main category: cs.MA

TL;DR: 预游戏消息可减少大型语言模型在策略游戏中的行为可变性，提升多智能体系统可预测性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在策略任务中表现出显著的上下文依赖性可变性，破坏多智能体行为的可预测性。本研究评估了类似廉价谈判的预游戏消息机制对策略稳定性的影响。

Method: 采用模拟级自举重采样和非参数推断方法，通过LOWESS回归比较有消息和无消息条件下的合作轨迹，分析囚徒困境游戏中模型行为。

Result: 多数模型-上下文组合中观察到轨迹噪声显著减少；稳定作用在不同提示和解码机制中持续，强弱取决于模型选择和语境，高基线波动模型获益最大；沟通很少导致有害不稳定，仅少数例外。

Conclusion: 廉价谈判式沟通可作为低成本实用工具，提升多智能体LLM系统策略行为的可预测性和可靠性。

Abstract: Large Language Models (LLMs) often exhibit pronounced context-dependent variability that undermines predictable multi-agent behavior in tasks requiring strategic thinking. Focusing on models that range from 7 to 9 billion parameters in size engaged in a ten-round repeated Prisoner's Dilemma, we evaluate whether short, costless pre-play messages emulating the cheap-talk paradigm affect strategic stability. Our analysis uses simulation-level bootstrap resampling and nonparametric inference to compare cooperation trajectories fitted with LOWESS regression across both the messaging and the no-messaging condition. We demonstrate consistent reductions in trajectory noise across a majority of the model-context pairings being studied. The stabilizing effect persists across multiple prompt variants and decoding regimes, though its magnitude depends on model choice and contextual framing, with models displaying higher baseline volatility gaining the most. While communication rarely produces harmful instability, we document a few context-specific exceptions and identify the limited domains in which communication harms stability. These findings position cheap-talk style communication as a low-cost, practical tool for improving the predictability and reliability of strategic behavior in multi-agent LLM systems.

</details>


### [2] [Sample-Efficient Policy Space Response Oracles with Joint Experience Best Response](https://arxiv.org/abs/2602.06599)
*Ariyan Bighashdel,Thiago D. Simão,Frans A. Oliehoek*

Main category: cs.MA

TL;DR: 提出JBR方法改进PSRO，通过复用联合数据集一次性计算所有智能体最优响应，大幅提升多智能体强化学习的样本效率。


<details>
  <summary>Details</summary>
Motivation: 解决PSRO在多智能体场景中因独立计算最优响应导致的训练开销过大问题，尤其在智能体众多或仿真成本高时难以应用。

Method: JBR核心是单次采样当前元策略轨迹并复用数据集，同步计算所有智能体最优响应。针对离线RL的分布偏移问题，提出三种改进：(1)带安全策略的保守JBR；(2)理论可证保障的探索增强JBR；(3)融合JBR与独立更新的混合BR。

Result: 实验表明：探索增强JBR取得最佳精度-效率平衡，混合BR以极低样本成本اصل近PSRO性能。

Conclusion: JBR显著提升PSRO在大规模策略学习中的实用性，同时保持均衡在该性.children_buttonhasIcon

Abstract: Multi-agent reinforcement learning (MARL) offers a scalable alternative to exact game-theoretic analysis but suffers from non-stationarity and the need to maintain diverse populations of strategies that capture non-transitive interactions. Policy Space Response Oracles (PSRO) address these issues by iteratively expanding a restricted game with approximate best responses (BRs), yet per-agent BR training makes it prohibitively expensive in many-agent or simulator-expensive settings. We introduce Joint Experience Best Response (JBR), a drop-in modification to PSRO that collects trajectories once under the current meta-strategy profile and reuses this joint dataset to compute BRs for all agents simultaneously. This amortizes environment interaction and improves the sample efficiency of best-response computation. Because JBR converts BR computation into an offline RL problem, we propose three remedies for distribution-shift bias: (i) Conservative JBR with safe policy improvement, (ii) Exploration-Augmented JBR that perturbs data collection and admits theoretical guarantees, and (iii) Hybrid BR that interleaves JBR with periodic independent BR updates. Across benchmark multi-agent environments, Exploration-Augmented JBR achieves the best accuracy-efficiency trade-off, while Hybrid BR attains near-PSRO performance at a fraction of the sample cost. Overall, JBR makes PSRO substantially more practical for large-scale strategic learning while preserving equilibrium robustness.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [3] [Quantifying Energy-Efficient Edge Intelligence: Inference-time Scaling Laws for Heterogeneous Computing](https://arxiv.org/abs/2602.06057)
*Satyam Kumar,Saurabh Jha*

Main category: cs.DC

TL;DR: 本文介绍了QEIL框架，通过异构协作硬件和比例定律优化边缘设备上的大语言模型推理效率。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在资源受限的边缘设备上进行推理存在高延迟，现有方案过度依赖云基础设施。

Method: 建立五个架构不可知的比例定律定理，集成异构工作负载分配、硬件感知路由和性能能量权衡量化指标，通过统一编排器和渐进样本复用实现优化。

Result: 在125M至2.6B参数的五个模型家族中测试：pass@k覆盖率提高7-10.5个百分点，能耗降低35.6-78.2%，平均功率降68%，延迟改善15.8%，且精度无损失。

Conclusion: 推理时间比例定律具有普适性和架构不可知性，异构边缘编排是能源受限智能系统的最优策略。

Abstract: Large language model inference on resource constrained edge devices remains a major challenge for low latency intelligent systems, as existing solutions depend heavily on cloud or datacenter infrastructure. This work introduces QEIL, Quantifying Edge Intelligence via Inference time Scaling Laws, a unified framework for efficient local LLM inference using principled scaling laws and heterogeneous orchestration across CPU, GPU, and NPU accelerators. We derive five architecture agnostic theorems that characterize how inference efficiency scales with model size, sample budget, and device level constraints. QEIL integrates three optimization dimensions. First, inference time scaling laws show that heterogeneous workload distribution achieves superlinear efficiency gains that are not observed in homogeneous execution. Second, hardware aware routing is enabled through analytical cost models that account for compute throughput, memory bandwidth, power consumption, and thermal limits. Third, performance energy trade offs are quantified using novel metrics including Intelligence Per Watt, Energy Coverage Efficiency, and Price Power Performance. A unified orchestrator combines these components through progressive sample multiplexing to improve coverage. Extensive evaluation across five model families from 125M to 2.6B parameters demonstrates consistent gains, including 7 to 10.5 percentage point improvement in pass at k coverage, 35.6 to 78.2 percent energy reduction, 68 percent average power reduction enabling edge thermal budgets, 15.8 percent latency improvement, and zero accuracy loss. Results confirm that inference time scaling laws are universal and architecture agnostic, establishing heterogeneous edge orchestration as the optimal strategy for energy constrained intelligent systems.

</details>


### [4] [Mapping Gemma3 onto an Edge Dataflow Architecture](https://arxiv.org/abs/2602.06063)
*Shouyu Du,Miaoxiang Yu,Zhiheng Ni,Jillian Cai,Qing Yang,Tao Wei,Zhenyu Xu*

Main category: cs.DC

TL;DR: 本文介绍在AMD Ryzen AI NPU上部署Gemma3语言视觉模型的端到端方案，提出硬件优化技术，显著提升预填充和解码速度及能效。


<details>
  <summary>Details</summary>
Motivation: 证明现代NPU能实现边缘设备上低功耗LLM/VLM推理，并提供变换器模型在瓦片式数据流加速器上的可扩展蓝图。

Method: Method extraction failed

Result: 相比集成GPU：预填充快5.2倍、解码快4.8倍；相比CPU：预填充快33.5倍、解码快2.2倍；能效提升分别达67.2倍（vs GPU）和222.9倍（vs CPU）。

Conclusion: NPU支持高效边缘推理，方法为变换器模型在数据流加速器上的映射提供通用框架。

Abstract: We present the first end-to-end deployment of the Gemma3 family of large language and vision models on a tiled edge dataflow architecture (AMD Ryzen AI NPU). Our work introduces a set of hardware-aware techniques. For prefill, we introduce an efficient dequantization engine, optimize tiled matrix multiplication kernels, and propose FlowQKV, a chunked, pipelined attention mechanism. For decoding, we introduce FusedDQP, which fuses dequantization and projection into a single kernel, and FlowKV, which re-structures attention to sustain high memory bandwidth utilization. Together with a compact Q4NX 4-bit quantization format, these methods yield up to $5.2\times$ faster prefill and $4.8\times$ faster decoding versus the iGPU, and $33.5\times$ and $2.2\times$ over the CPU, respectively. Power efficiency improves by as much as $67.2\times$ and $222.9\times$ compared to the iGPU and CPU. The proposed approach demonstrates that modern NPUs can deliver practical, low-power LLM and VLM inference at the edge, and provides a generalizable blueprint for mapping transformer-based models onto tiled dataflow accelerators.

</details>


### [5] [iScheduler: Reinforcement Learning-Driven Continual Optimization for Large-Scale Resource Investment Problems](https://arxiv.org/abs/2602.06064)
*Yi-Xiang Hu,Yuke Wang,Feng Wu,Zirui Huang,Shuli Zeng,Xiang-Yang Li*

Main category: cs.DC

TL;DR: 本文提出iScheduler，一种基于强化学习的迭代调度框架，用于高效求解资源投资问题（RIP），在工业规模基准（L-RIPLIB）上显著加快调度速度和动态更新。


<details>
  <summary>Details</summary>
Motivation: 现有方法如混合整数规划和约束规划在处理大规模任务（2500-10000个任务）时速度过慢，且动态更新865.要求低延迟重构。本研究旨在开发一个快速支持实时重新配置的调度解决方案。

Method: iScheduler将RIP建模为马尔可夫决策过程分解子问题，通过序贯过程选择构建调度计划；支持重用未更改进程，仅重调度影响我们可以变速部分，提升优化效率。

Result: 在L-RIPLIB数据集（1000个实例）上实验，iScheduler实现与商业基准相似的资源成本，同时将可达性时间最高减少43倍。

Conclusion: 该框架证明强化学习驱动的迭代方法能有效处理大规模任务调度问题，加速优化并支持实时资源重构。

Abstract: Scheduling precedence-constrained tasks under shared renewable resources is central to modern computing platforms. The Resource Investment Problem (RIP) models this setting by minimizing the cost of provisioned renewable resources under precedence and timing constraints. Exact mixed-integer programming and constraint programming become impractically slow on large instances, and dynamic updates require schedule revisions under tight latency budgets. We present iScheduler, a reinforcement-learning-driven iterative scheduling framework that formulates RIP solving as a Markov decision process over decomposed subproblems and constructs schedules through sequential process selection. The framework accelerates optimization and supports reconfiguration by reusing unchanged process schedules and rescheduling only affected processes. We also release L-RIPLIB, an industrial-scale benchmark derived from cloud-platform workloads with 1,000 instances of 2,500-10,000 tasks. Experiments show that iScheduler attains competitive resource costs while reducing time to feasibility by up to 43$\times$ against strong commercial baselines.

</details>


### [6] [HQP: Sensitivity-Aware Hybrid Quantization and Pruning for Ultra-Low-Latency Edge AI Inference](https://arxiv.org/abs/2602.06069)
*Dinesh Gopalan,Ratul Ali*

Main category: cs.DC

TL;DR: 本研究同步了混合量化和修剪（HQP）框架， jovens通过灵敏度认知剪枝和量化协同加速模型，以在边缘计算中实现高保真实时推理，同时严格控制精度损失。


<details>
  <summary>Details</summary>
Motivation: 分布式边缘云计算部署中，对高保真实时推理的需求激增，面临的严重延迟和能耗约束亟需高效模型优化来解决，以确保在资源有限环境下维持性能。

Method: 采用灵敏度认知的结构化剪枝算法：动态权重敏感度指标源自Fisher信息矩阵的高效近似，迭代移除冗余滤波顾；严格基于最高可容忍精度降幅Δax条件后，执行8位训练后量化，增强模型对量化错误和硬件优化的鲁棒性。

Result: 在NVIDIA Jetson边缘平台上，使用MobileNetV3和ResNet-18进行测试：实现峰值性能提升，推理加速达3.12倍，模型大小减少55%，并将精度降幅控制低于1.5%。

Conclusion: 与传统单目标压缩技术相比，HQP框架被验证为更优越的硬件无关解决方案，适合在资源受限的边缘基础设施中部署超低延迟AI系统。

Abstract: The escalating demand for high-fidelity, real-time inference in distributed edge-cloud environments necessitates aggressive model optimization to counteract severe latency and energy constraints. This paper introduces the Hybrid Quantization and Pruning (HQP) framework, a novel, integrated methodology designed to achieve synergistic model acceleration while adhering to strict quality guarantees. We detail a sensitivity-aware structural pruning algorithm that employs a dynamic weight sensitivity metric, derived from a highly efficient approximation of the Fisher Information Matrix (FIM), to guide the iterative removal of redundant filters. This pruning is strictly conditional, enforcing an adherence to a maximum permissible accuracy drop (Delta ax) before the model proceeds to 8-bit post-training quantization. This rigorous coordination is critical, as it ensures the resultant sparse model structure is maximally robust to quantization error and hardware-specific kernel optimization. Exhaustive evaluation across heterogeneous NVIDIA Jetson edge platforms, utilizing resource-efficient architectures like MobileNetV3 and ResNet-18, demonstrates that the HQP framework achieves a peak performance gain of 3.12 times inference speedup and a 55 percent model size reduction, while rigorously containing the accuracy drop below the 1.5 percent constraint. A comprehensive comparative analysis against conventional single-objective compression techniques validates the HQP framework as a superior, hardware-agnostic solution for deploying ultra-low-latency AI in resource-limited edge infrastructures.

</details>


### [7] [Computationally Efficient Laplacian CL-colME](https://arxiv.org/abs/2602.06070)
*Nikola Stankovic*

Main category: cs.DC

TL;DR: 提出新型CL-colME算法，基于拉普拉斯共识替代归一化矩阵的计算密集型步骤，在保持分布式协作平均估计精度的同时提升效率。


<details>
  <summary>Details</summary>
Motivation: 现有C-colME框架依赖双随机平均矩阵收敛至理论解，但归一化过程计算开销高昂，需优化计算效率。

Method: 设计CL-colME新变体，采用拉普拉斯共识机制消除归一化需求，保留核心收敛特性

Result: 仿真实验表明CL-colME维持C-colME的收敛性与 medically accuracy，同时askell显著降低计算复杂度

Conclusion: 该算法为分布式协作计算提供高效替代方案，在保持精度的前提下改进系统扩展性

Abstract: Decentralized collaborative mean estimation (colME) is a fundamental task in heterogeneous networks. Its graph-based variants B-colME and C-colME achieve high scalability of the problem. This paper evaluates the consensus-based C-colME framework, which relies on doubly stochastic averaging matrices to ensure convergence to the oracle solution. We propose CL-colME, a novel variant utilizing Laplacian-based consensus to avoid the computationally expensive normalization processes. Simulation results show that the proposed CL-colME maintains the convergence behavior and accuracy of C-colME while improving computational efficiency.

</details>


### [8] [FlashSketch: Sketch-Kernel Co-Design for Fast Sparse Sketching on GPUs](https://arxiv.org/abs/2602.06071)
*Rajat Vadiraj Dwaraknath,Sungyoon Kim,Mert Pilanci*

Main category: cs.DC

TL;DR: 提出BlockPerm-SJLT稀疏草图与FlashSketch内核协同设计，通过可调参数平衡GPU效率与鲁棒性，实现1.7倍加速的GPU高效稀疏草图计算。


<details>
  <summary>Details</summary>
Motivation: 传统稀疏草图因随机稀疏性导致GPU内存访问不规则，降低带宽利用率，需解决GPU效率与草图鲁棒性间的矛盾。

Method: 设计BlockPerm-SJLT稀疏结构草图，配套FlashSketch优化CUDA内核，引入可调参数调控GPU效率与鲁棒性权衡

Result: 理论证明满足无意识子空间嵌入(OSE)保证，实验在RandNLA基准和GraSS流水线中验证，质量-速度帕累托前沿提升，相对SOTA加速1.7倍(几何平均)。

Conclusion: BlockPerm-SJLT与FlashSketch协同设计突破GPU稀疏草图性能瓶颈，为高效随机数值线性代数提供新解决方案。

Abstract: Sparse sketches such as the sparse Johnson-Lindenstrauss transform are a core primitive in randomized numerical linear algebra because they leverage random sparsity to reduce the arithmetic cost of sketching, while still offering strong approximation guarantees. Their random sparsity, however, is at odds with efficient implementations on modern GPUs, since it leads to irregular memory access patterns that degrade memory bandwidth utilization. Motivated by this tension, we pursue a sketch-kernel co-design approach: we design a new family of sparse sketches, BlockPerm-SJLT, whose sparsity structure is chosen to enable FlashSketch, a corresponding optimized CUDA kernel that implements these sketches efficiently. The design of BlockPerm-SJLT introduces a tunable parameter that explicitly trades off the tension between GPU-efficiency and sketching robustness. We provide theoretical guarantees for BlockPerm-SJLT under the oblivious subspace embedding (OSE) framework, and also analyze the effect of the tunable parameter on sketching quality. We empirically evaluate FlashSketch on standard RandNLA benchmarks, as well as an end-to-end ML data attribution pipeline called GraSS. FlashSketch pushes the Pareto frontier of sketching quality versus speed, across a range of regimes and tasks, and achieves a global geomean speedup of roughly 1.7x over the prior state-of-the-art GPU sketches.

</details>


### [9] [PackInfer: Compute- and I/O-Efficient Attention for Batched LLM Inference](https://arxiv.org/abs/2602.06072)
*Rui Ning,Wei Zhang,Fan Lai*

Main category: cs.DC

TL;DR: PackInfer提出了一种面向大语言模型异构批量推理的核级注意力框架，通过请求分组打包与I/O感知调度提升硬件利用率。


<details>
  <summary>Details</summary>
Motivation: 生产环境中批处理请求序列长度高度异构，导致计算与I/O不均衡、资源利用率低下，传统优化方法（如FlashAttention）无法有效解决该问题。

Method: 1. 将批处理请求编排为负载均衡的执行组；2. 通过合并请求到统一核启动饱和GPU；3. 在打包查询-键区域构建注意力核消除冗余计算；4. I/O感知分组对齐共享前缀请求并重组KV缓存布局。

Result: 在实际工测试中：推理延迟降低13.0-20.1%，相比FlashAttention吞吐量提升20%。

Conclusion: PackInfer有效解决了批量推理中的计算与内存瓶颈，显著提升GPU资源利用率和推理效率。

Abstract: Attention efficiency is critical to large language model (LLM) inference. While prior advances optimize attention execution for individual requests (e.g., FlashAttention), production LLM serving relies on batching requests with highly heterogeneous sequence lengths for high serving throughput. This mismatch induces severe computation and I/O imbalance, exacerbates stragglers, and underutilizes GPU resources. We present PackInfer, a kernel-level attention framework that enables compute- and I/O-aware execution for heterogeneous batched inference. PackInfer orchestrates batched requests into load-balanced execution groups, effectively saturating GPU utilization by packing multiple requests into unified kernel launches. By constructing attention kernels directly over packed query-key regions, PackInfer eliminates redundant computation and balances thread-block execution. It then incorporates I/O-aware grouping that co-locates shared-prefix requests and reorganizes KV caches into group-contiguous layouts, reducing memory fragmentation and redundant data movement as generation evolves. Evaluations on real-world workloads show that PackInfer reduces inference latency by 13.0-20.1%, and improves throughput by 20% compared to the state-of-the-art FlashAttention.

</details>


### [10] [Experimental Analysis of Server-Side Caching for Web Performance](https://arxiv.org/abs/2602.06074)
*Mohammad Umar,Bharat Tripathi*

Main category: cs.DC

TL;DR: 本文通过实验研究了小型Web应用中简单内存缓存的性能影响，结果显示缓存能显著降低响应时间，适用于教育环境和小规模应用。


<details>
  <summary>Details</summary>
Motivation: 现有文献缺乏对小型Web应用中内存缓存效果的实验研究，本文旨在填补这一空白。

Method: 采用轻量级Web服务器框架，在相同环境下比较无缓存与带固定生存时间的内存缓存的配置，通过重复HTTP请求测量响应时间。

Result: 缓存请求的响应时间显著降低。

Conclusion: 简单服务器端缓存有效提升性能，特别适合注重简洁性和可复现性的教育场景和小型应用。

Abstract: Performance in web applications is a key aspect of user experience and system scalability. Among the different techniques used to improve web application performance, caching has been widely used. While caching has been widely explored in web performance optimization literature, there is a lack of experimental work that explores the effect of simple inmemory caching in small-scale web applications. This paper fills this research gap by experimentally comparing the performance of two server-side web application configurations: one without caching and another with in-memory caching and a fixed time-tolive. The performance evaluation was conducted using a lightweight web server framework, and response times were measured using repeated HTTP requests under identical environmental conditions. The results show a significant reduction in response time for cached requests, and the findings of this paper provide valuable insights into the effectiveness of simple server-side caching in improving web application performance making it suitable for educational environments and small-scale web applications where simplicity and reproducibility are critical.

</details>


### [11] [Canzona: A Unified, Asynchronous, and Load-Balanced Framework for Distributed Matrix-based Optimizers](https://arxiv.org/abs/2602.06079)
*Liangyu Wang,Siqi Zhang,Junjie Wang,Yiming Dong,Bo Zheng,Zihan Qiu,Shengkun Tang,Di Wang,Rui Men,Dayiheng Liu*

Main category: cs.DC

TL;DR: 提出Canzona框架解决大模型分布式训练中矩阵优化器更新与张量分片冲突的问题。


<details>
  <summary>Details</summary>
Motivation: 矩阵优化器在大模型训练中高效但需全局更新，与分布式框架张量分片冲突；现有同步方案存在计算冗余，分层优化违反通信约束。

Method: Canzona框架：1) 解耦逻辑优化与物理分布/models；2) 数据并行：α平衡静态分片保障原子性；3) 张量并行：微组调度异步计算批次更新/隐藏重构开销。

Result: Qwen3模型(32B)在256张GPU测试：端到端迭代加速1.57倍，优化器步长时间降低5.8倍。

Conclusion: Canzona在保持并行架构效率的同时，显著提升训练性能，解决优化器分片冲突问题。

Abstract: The scaling of Large Language Models (LLMs) drives interest in matrix-based optimizers (e.g., Shampoo, Muon, SOAP) for their convergence efficiency; yet their requirement for holistic updates conflicts with the tensor fragmentation in distributed frameworks like Megatron. Existing solutions are suboptimal: synchronous approaches suffer from computational redundancy, while layer-wise partitioning fails to reconcile this conflict without violating the geometric constraints of efficient communication primitives. To bridge this gap, we propose Canzona, a Unified, Asynchronous, and Load-Balanced framework that decouples logical optimizer assignment from physical parameter distribution. For Data Parallelism, we introduce an alpha-Balanced Static Partitioning strategy that respects atomicity while neutralizing the load imbalance. For Tensor Parallelism, we design an Asynchronous Compute pipeline utilizing Micro-Group Scheduling to batch fragmented updates and hide reconstruction overhead. Extensive evaluations on the Qwen3 model family (up to 32B parameters) on 256 GPUs demonstrate that our approach preserves the efficiency of established parallel architectures, achieving a 1.57x speedup in end-to-end iteration time and reducing optimizer step latency by 5.8x compared to the baseline.

</details>


### [12] [LAAFD: LLM-based Agents for Accelerated FPGA Design](https://arxiv.org/abs/2602.06085)
*Maxim Moraru,Kamalavasan Kamalakkannan,Jered Dominguez-Trujillo,Patrick Diehl,Atanu Barai,Julien Loiseau,Zachary Kent Baker,Howard Pritchard,Galen M Shipman*

Main category: cs.DC

TL;DR: LAAFD利用大型语言模型自动转换C++为Vitis HLS内核，降低FPGA开发门槛同时ificación保持高性能。


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: 通过LLM驱动智能工作流，全自动实施流水线优化、向量化及数据流分区，并结合协同仿真与综合反馈迭代优化周期性能。

Result: 在15个HPC典型核空气上达手工优化基准99.9%性能；模板计算任务中与SODA性能相当但内核可读性更优。

Conclusion: 该技术显著降低FPGA加速的专业知识壁垒，且在效率无损前提下提升开发可及性。

Abstract: FPGAs offer high performance, low latency, and energy efficiency for accelerated computing, yet adoption in scientific and edge settings is limited by the specialized hardware expertise required. High-level synthesis (HLS) boosts productivity over HDLs, but competitive designs still demand hardware-aware optimizations and careful dataflow design. We introduce LAAFD, an agentic workflow that uses large language models to translate general-purpose C++ into optimized Vitis HLS kernels. LAAFD automates key transfor mations: deep pipelining, vectorization, and dataflow partitioning and closes the loop with HLS co-simulation and synthesis feedback to verify correctness while iteratively improving execution time in cycles. Over a suite of 15 kernels representing common compute patterns in HPC, LAFFD achieves 99.9% geomean performance when compared to the hand tuned baseline for Vitis HLS. For stencil workloads, LAAFD matches the performance of SODA, a state-of-the-art DSL-based HLS code generator for stencil solvers, while yielding more readable kernels. These results suggest LAAFD substantially lowers the expertise barrier to FPGA acceleration without sacrificing efficiency.

</details>


### [13] [BouquetFL: Emulating diverse participant hardware in Federated Learning](https://arxiv.org/abs/2602.06498)
*Arno Geimer*

Main category: cs.DC

TL;DR: BouquetFL框架通过在单机上模拟硬件异构 knocked性，实现无须多设备的联邦学习硬件差异研究。


<details>
  <summary>Details</summary>
Motivation: 现有联邦学习研究多在统一硬件假设下进行模拟，忽略实际部署中设备硬件差异，导致实验结果与真实场景存在距离。

Method: 通过程序化资源限制模拟不同硬件配置，提供消费者设备和小型实验室设备的多样化硬件profile，并支持基于真实硬件流行度定制联邦环境。

Result: 实现在单机上可控复现硬件异构场景，降低了研究门槛，使实验条件更贴近实际部署。

Conclusion: BouquetFL填补了联邦学习硬件异构性实验方法论空白，为研究看在高度异构联邦提供实用工具。

Abstract: In Federated Learning (FL), multiple parties collaboratively train a shared Machine Learning model to encapsulate all private knowledge without exchange of information. While it has seen application in several industrial projects, most FL research considers simulations on a central machine, without considering potential hardware heterogeneity between the involved parties. In this paper, we present BouquetFL, a framework designed to address this methodological gap by simulating heterogeneous client hardware on a single physical machine. By programmatically emulating diverse hardware configurations through resource restriction, BouquetFL enables controlled FL experimentation under realistic hardware diversity. Our tool provides an accessible way to study system heterogeneity in FL without requiring multiple physical devices, thereby bringing experimental practice closer to practical deployment conditions. The target audience are FL researchers studying highly heterogeneous federations. We include a wide range of profiles derived from commonly available consumer and small-lab devices, as well as a custom hardware sampler built on real-world hardware popularity, allowing users to configure the federation according to their preference.

</details>


### [14] [FCDP: Fully Cached Data Parallel for Communication-Avoiding Large-Scale Training](https://arxiv.org/abs/2602.06499)
*Gyeongseo Park,Eungyeong Lee,Song-woo Sok,Myung-Hoon Cha,Kwangwon Koh,Baik-Song An,Hongyeon Kim,Ki-Dong Kang*

Main category: cs.DC

TL;DR: FCDP是一种优化方法，通过在主机内存中缓存参数并减少节点间通信，显著提升低带宽集群上十亿参数模型的训练吞吐量，同时保持ZeRO-3的内存效率。


<details>
  <summary>Details</summary>
Motivation: ZeRO-3在商用硬件上面临严重的节点间通信瓶颈，现有优化方法如MiCS和ZeRO++因GPU内存缓存导致内存不足，而ZeRO-Offload和ZeRO-Infinity则因主机内存卸载引发PCIe开销且吞吐量降低。因此，需要一种在带宽受限集群中高效利用主机内存的解决方案。

Method: FCDP利用主机内存作为快速缓存层，在前向传播阶段将参数缓存在主机内存中，并在反向传播阶段通过快速的节点内多机通信复用这些参数，从而将节点间通信减少50%。对于参数高效微调(PEFT)，它仅通信可训练参数以最大化缓存效果，使节点间流量降低超过99%。

Result: 在商用集群环境中，FCDP相比ZeRO-3吞吐量提升最高100倍，相比ZeRO++-ton提升51倍，同时维持ZeRO-3的最大批处理规模。

Conclusion: FCDP有效解决了带宽受限集群的训练瓶颈，大幅提升吞吐量，证明了主机内存作为高速缓存层的潜力，适用于大规模参密集型模型的实战部署。

Abstract: Training billion-parameter models requires distributing model states across GPUs using fully sharded data parallel (i.e., ZeRO-3). While ZeRO-3 succeeds on clusters with high-bandwidth NVLink and InfiniBand interconnects, researchers with commodity hardware face severe inter-node all-gather bottlenecks. Existing optimizations take two approaches: GPU memory caching (MiCS, ZeRO++) trades memory capacity for reduced communication, triggering out-of-memory failures on large models; host memory offloading (ZeRO-Offload, ZeRO-Infinity) extends capacity but degrades throughput due to PCIe overhead. We observe that on bandwidth-limited clusters, host memory can serve not as an overflow tier but as a fast caching layer that outperforms inter-node communication. Based on this insight, we propose FCDP, which eliminates redundant inter-node communication while preserving ZeRO-3's minimal GPU memory footprint. FCDP caches forward-pass parameters in host memory and reuses them during the backward pass via fast intra-node all-gather, reducing inter-node all-gather by 50%. For parameter-efficient fine-tuning (PEFT), FCDP selectively communicates only trainable parameters to maximize caching, reducing inter-node traffic by over 99%. In our commodity cluster setup, FCDP achieves up to 100x higher throughput than ZeRO-3 and 51x higher than ZeRO++, while maintaining ZeRO-3's maximum batch size.

</details>


### [15] [DualMap: Enabling Both Cache Affinity and Load Balancing for Distributed LLM Serving](https://arxiv.org/abs/2602.06502)
*Ying Yuan,Pengfei Zuo,Bo Wang,Zhangyu Chen,Zhipeng Tan,Zhou Yu*

Main category: cs.DC

TL;DR: DualMap是一种双映射调度策略，通过智能实例映射和负载感知机制，在LLM服务中同时优化KV缓存重用与负载均衡。


<details>
  <summary>Details</summary>
Motivation: 现有LLM服务调度器无法有效协调KV缓存亲和性（通过重用提示前缀来减少TTFT和成本）与负载平衡调度（均衡分布请求），导致服务效率低下和冲突加剧。

Method: 提出DualMap：1）使用两个独立哈希函数映射请求到候选实例；2）基于系统状态智能选择更好实例，结合SLO感知路由（优先缓存亲和性）、热点感知重平衡（动态迁移请求）和轻量级双哈希环扩展（支持快速实例伸缩）。

Result: 实验显示，在相同TTFT SLO约束下，DualMap将有效请求容量提升至多2.25倍，优于当前最优工作。

Conclusion: DualMap以统一框架解决缓存重用与负载平衡冲突，显著提升服务性能和成本效益，适应真实动态负载场景。

Abstract: In LLM serving, reusing the KV cache of prompts across requests is critical for reducing TTFT and serving costs. Cache-affinity scheduling, which co-locates requests with the same prompt prefix to maximize KV cache reuse, often conflicts with load-balancing scheduling that distributes requests evenly across compute instances. Existing schedulers fail to reconcile this trade-off as they operate within a single mapping space, typically applying cache-affinity routing to a subset of requests and load-balanced routing to the rest, without a unified solution to achieve both goals. To address this limitation, we propose DualMap, a dual-mapping scheduling strategy for distributed LLM serving that achieves both cache affinity and load balancing. Its key idea is to map each request to two candidate instances via two independent hash functions based on the request prompt, then intelligently select the better candidate based on current system states. This design increases the likelihood that requests with shared prefixes are co-located, while evenly dispersing distinct prefixes across the cluster via ``the power of two choices''. To make DualMap robust under dynamic and skewed real-world workloads, we incorporate three techniques: 1) SLO-aware request routing, which prioritizes cache affinity but switches to load-aware scheduling when TTFT exceeds the SLO, enhancing load balance without sacrificing cache reuse; 2) hotspot-aware rebalancing, which dynamically migrates requests from overloaded to underloaded instances, mitigating hotspots and rebalancing the system; 3) lightweight dual-hash-ring scaling, which leverages a dual-hash-ring mapping to support fast and low-overhead instance scaling without costly global remapping. Experiments on real-world workloads show that DualMap improves effective request capacity by up to 2.25$\times$ under the same TTFT SLO constraints compared with SOTA work.

</details>


### [16] [Reinforcement Learning-Based Dynamic Management of Structured Parallel Farm Skeletons on Serverless Platforms](https://arxiv.org/abs/2602.06555)
*Lanpei Li,Massimo Coppola,Malio Li,Valerio Besozzi,Jack Bell,Vincenzo Lomonaco*

Main category: cs.DC

TL;DR: 提出在无服务器平台 Eukaryote 上动态管理并行处理骨架的框架，通过AI驱动扩缩容提升Farm模式的性能和服务质量。


<details>
  <summary>Details</summary>
Motivation: 在维持骨架模式可编程性优势的同时，为无服务器环境提供类似高性能计算的性能和弹性，解决工作池自动扩缩容这一QoS感知的资源管理问题。

Method: 整合可复用Farm模板与31 Gymnasium监控控制层，收集队列、时序和QoS指标，采用强化学习策略管理并行度，在OpenFaaS上对比AI控制与传统反应式方法。

Result: AI管理能更好适应平台限制，相较于纯模型方法提高服务质量（QoS），同时保持高效资源利用和稳定扩缩行为。

Conclusion: 人工智能驱动的方法在优化性能和资源效率方面优于模型静态调控，证实其在无服务器环境动态管理中的有效性。

Abstract: We present a framework for dynamic management of structured parallel processing skeletons on serverless platforms. Our goal is to bring HPC-like performance and resilience to serverless and continuum environments while preserving the programmability benefits of skeletons. As a first step, we focus on the well known Farm pattern and its implementation on the open-source OpenFaaS platform, treating autoscaling of the worker pool as a QoS-aware resource management problem. The framework couples a reusable farm template with a Gymnasium-based monitoring and control layer that exposes queue, timing, and QoS metrics to both reactive and learning-based controllers. We investigate the effectiveness of AI-driven dynamic scaling for managing the farm's degree of parallelism via the scalability of serverless functions on OpenFaaS. In particular, we discuss the autoscaling model and its training, and evaluate two reinforcement learning (RL) policies against a baseline of reactive management derived from a simple farm performance model. Our results show that AI-based management can better accommodate platform-specific limitations than purely model-based performance steering, improving QoS while maintaining efficient resource usage and stable scaling behaviour.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [17] [Makespan Minimization in Split Learning: From Theory to Practice](https://arxiv.org/abs/2602.06693)
*Robert Ganian,Fionn Mc Inerney,Dimitra Tsigkari*

Main category: cs.NI

TL;DR: 本文针对异构物联网设备的分裂学习，提出最小化训练时间的解决方案；首先在同质任务下开发5-近似算法，然后在异质任务设定中设计优于现有方法的启发式算法。


<details>
  <summary>Details</summary>
Motivation: 动机：强调在分布式机器学习中，异构IoT设备需将计算卸载到助手节点，核心挑战在于通过客户端-助手分配和任务调度最小化训练时间，尤其是处理内存约束和变量成本场景。

Method: 方法：使用复杂性理论证明同质任务下无高效算法，并提出多项式时间5-近似算法；扩展至异质任务时，改进了该算法开发新启发式orporationorization，并通过大规模实验与现有方法进行对比评估。

Result: 结果：同质任务下排除了精确多项式时间算法和近似方案，但提供了5-近似算法；异质任务下证明除非P=NP否则无近似算法，启发式算法在实验中显著优于先前方法。

Conclusion: 结论：尽管异质任务的计算复杂性极高，基于同质算法的启发式方法在实际应用中表现卓越，为分裂学习部署提供了有效解决方案。

Abstract: Split learning recently emerged as a solution for distributed machine learning with heterogeneous IoT devices, where clients can offload part of their training to computationally-powerful helpers. The core challenge in split learning is to minimize the training time by jointly devising the client-helper assignment and the schedule of tasks at the helpers. We first study the model where each helper has a memory cardinality constraint on how many clients it may be assigned, which represents the case of homogeneous tasks. Through complexity theory, we rule out exact polynomial-time algorithms and approximation schemes even for highly restricted instances of this problem. We complement these negative results with a non-trivial polynomial-time 5-approximation algorithm. Building on this, we then focus on the more general heterogeneous task setting considered by Tirana et al. [INFOCOM 2024], where helpers have memory capacity constraints and clients have variable memory costs. In this case, we prove that, unless P=NP, the problem cannot admit a polynomial-time approximation algorithm for any approximation factor. However, by adapting our aforementioned 5-approximation algorithm, we develop a novel heuristic for the heterogeneous task setting and show that it outperforms heuristics from prior works through extensive experiments.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [18] [D-Legion: A Scalable Many-Core Architecture for Accelerating Matrix Multiplication in Quantized LLMs](https://arxiv.org/abs/2602.06252)
*Ahmed J. Abdelmaksoud,Cristian Sestito,Shiwei Wang,Themis Prodromakis*

Main category: cs.AR

TL;DR: 提出D-Legion架构，一种可扩展多核系统，采用自适应精度脉动阵列加速量化大型语言模型的矩阵乘法运算。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型的性能增益依赖高计算和内存需求，量化模型潜力显著，需开发专用架构以加速执行。

Method: 构建多Legion结构，每个含自适应精度脉动阵列核心，支持稀疏/稠密矩阵乘法；通过块结构稀疏化、并行累加器降低psum内存访问，优化调度提升数据复用；探索Legion/核粒度确定最佳配置。

Result: 在BitNet模型中，延迟降低8.2倍，内存节省提升3.8倍，psum内存节省提升3倍；8 Legions架构峰值吞吐量135.68 TOPS；32 Legions版本相比TPUv4i降低延迟2.5倍、提升吞吐量2.3倍和内存节省2.7倍。

Conclusion: D-Legion高效加速量化LLM训练，在扩展性、延迟、吞吐量及内存效率方面超越现有技术，具有应用潜力。

Abstract: The performance gains obtained by large language models (LLMs) are closely linked to their substantial computational and memory requirements. Quantized LLMs offer significant advantages with extremely quantized models, motivating the development of specialized architectures to accelerate their workloads. This paper proposes D-Legion, a novel scalable many-core architecture, designed using many adaptive-precision systolic array cores, to accelerate matrix multiplication in quantized LLMs. The proposed architecture consists of a set of Legions where each Legion has a group of adaptive-precision systolic arrays. D-Legion supports multiple computation modes, including quantized sparse and dense matrix multiplications. The block structured sparsity is exploited within a fully-sparse, or partially-sparse windows. In addition, memory accesses of partial summations (psums) are spatially reduced through parallel accumulators. Furthermore, data reuse is maximized through optimized scheduling techniques by multicasting matrix tiles across the Legions. A comprehensive design space exploration is performed in terms of Legion/core granularity to determine the optimal Legion configuration. Moreover, D-Legion is evaluated on attention workloads from two BitNet models, delivering up to 8.2$\times$ lower latency, up to 3.8$\times$ higher memory savings, and up to 3$\times$ higher psum memory savings compared to state-of-the-art work. D-Legion, with eight Legions and 64 total cores, achieves a peak throughput of 135,68 TOPS at a frequency of 1 GHz. A scaled version of D-Legion, with 32 Legions, is compared to Google TPUv4i, achieving up to 2.5$\times$ lower total latency, up to 2.3$\times$ higher total throughput, and up to 2.7$\times$ higher total memory savings.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [19] [Scaling Mobile Chaos Testing with AI-Driven Test Execution](https://arxiv.org/abs/2602.06223)
*Juan Marcano,Ashish Samant,Kai Song,Lingchao Chen,Kaelan Mikowicz,Tim Smyth,Mengdie Zhang,Ali Zamani,Arturo Bravo Rovirosa,Sowjanya Puligadda,Srikanth Prodduturi,Mayank Bansal*

Main category: cs.SE

TL;DR: 论文提出自动化移动混沌测试系统，集成LLM移动测试平台与服务级故障注入，解决传统方法无法应对移动端大规模组合测试的问题。


<details>
  <summary>Details</summary>
Motivation: 大规模分布式系统中移动应用易受后端服务故障影响，传统混沌工程因用户流、位置、故障场景组合爆炸而无法实现规模化移动端测试。

Method: 结合DragonCrawl（基于LLM的移动测试平台）与uHavoc（服务层故障注入系统），利用自适应AI驱动测试执行覆盖复杂场景，避免手动编写海量测试用例。

Result: 202ELS4年Q1以来，对Uber三大应用执行超18万次测试，识别23个弹性风险（70%为架构依赖违规，12个严重问题阻碍核心功能），自动化根因分析将调试时间从小时缩短至分钟，精准度达88%。

Conclusion: 系统验证了生产级移动韧性持续测试可行性，故障注入下保持99%可靠性，并发现仅通过移动混沌测试可检测的崩溃问题。

Abstract: Mobile applications in large-scale distributed systems are susceptible to backend service failures, yet traditional chaos engineering approaches cannot scale mobile testing due to the combinatorial explosion of flows, locations, and failure scenarios that need validation. We present an automated mobile chaos testing system that integrates DragonCrawl, an LLM-based mobile testing platform, with uHavoc, a service-level fault injection system. The key insight is that adaptive AI-driven test execution can navigate mobile applications under degraded backend conditions, eliminating the need to manually write test cases for each combination of user flow, city, and failure type. Since Q1 2024, our system has executed over 180,000 automated chaos tests across 47 critical flows in Uber's Rider, Driver, and Eats applications, representing approximately 39,000 hours of manual testing effort that would be impractical at this scale. We identified 23 resilience risks, with 70% being architectural dependency violations where non-critical service failures degraded core user flows. Twelve issues were severe enough to prevent trip requests or food orders. Two caused application crashes detectable only through mobile chaos testing, not backend testing alone. Automated root cause analysis reduced debugging time from hours to minutes, achieving 88% precision@5 in attributing mobile failures to specific backend services. This paper presents the system design, evaluates its performance under fault injection (maintaining 99% test reliability), and reports operational experience demonstrating that continuous mobile resilience validation is achievable at production scale.

</details>


### [20] [Code vs Serialized AST Inputs for LLM-Based Code Summarization: An Empirical Study](https://arxiv.org/abs/2602.06671)
*Shijia Dong,Haoruo Zhao,Paul Harvey*

Main category: cs.SE

TL;DR: 提出AST序列化方法AST(NIT)，在LLM代码摘要中整合完整语法树结构，减少输入长度和训练时间，性能媲美现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前基于大语言模型的代码摘要方法主要依赖原始代码或部分语法树信号，未能充分利用完整语法树表示的潜力，制约了摘要质量的提升。

Method: 设计AST(NIT)方法——通过语法树增强和序列化技术，保留词法细节并将结构信息编码为LLM兼容序列。

Result: 在CodeXGLUE Python数据集上使用LLaMA-3.1-8B验证：序列化后输入长度减少40%，训练时间缩短35%，摘要质量与基准方法相当（BLEU-4得分差值<0.5）

Conclusion: AST(NIT)证明完整语法树序列化能有效提升LLM代码摘要效率，为软件维护提供更轻量化的解决方案。

Abstract: Summarizing source code into natural language descriptions (code summarization) helps developers better understand program functionality and reduce the burden of software maintenance. Abstract Syntax Trees (ASTs), as opposed to source code, have been shown to improve summarization quality in traditional encoder-decoder-based code summarization models. However, most large language model (LLM)-based code summarization methods rely on raw code or only incorporate partial AST signals, meaning that the potential of complete AST representation has not been fully explored for LLMs. This paper presents AST(NIT), an AST augmentation and serialization method that preserves lexical details and encodes structural information into LLM-compatible sequences. Experiments with the LLaMA-3.1-8B model on the CodeXGLUE Python dataset show that the proposed serialized ASTs reduce the length of LLM inputs, require shorter training times, and achieve summarization quality comparable to existing approaches.

</details>


### [21] [Using Large Language Models to Support Automation of Failure Management in CI/CD Pipelines: A Case Study in SAP HANA](https://arxiv.org/abs/2602.06709)
*Duong Bui,Stefan Grintz,Alexander Berndt,Thomas Bach*

Main category: cs.SE

TL;DR: 研究在SAP HANA工业项目中评估基于大语言模型的CI/CD故障自动管理系统，历史故障数据对提升准确率贡献最大。


<details>
  <summary>Details</summary>
Motivation: 手动管理CI/CD流水线故障耗时且效率低下，传统自动化受限于非结构化信息处理能力。为此，探索LLM在工业场景（SAP HANA）中的自动化故障管理潜力。

Method: 构建LLM系统并提供三类领域知识（流水线信息、故障管理指南、历史故障数据），通过消融实验分析各类知识对解决方案准确性的影响。

Result: 历史故障数据贡献最显著：92.1%案例生成精准解决方案；提供领域知识时错误定位准确率达97.4%（无知识时为84.2%）。

Conclusion: LLM结合历史故障数据能高效自动化CI/CD故障管理，具有显著工业应用前景。

Abstract: CI/CD pipeline failure management is time-consuming when performed manually. Automating this process is non-trivial because the information required for effective failure management is unstructured and cannot be automatically processed by traditional programs. With their ability to process unstructured data, large language models (LLMs) have shown promising results for automated failure management by previous work. Following these studies, we evaluated whether an LLM-based system could automate failure management in a CI/CD pipeline in the context of a large industrial software project, namely SAP HANA. We evaluated the ability of the LLM-based system to identify the error location and to propose exact solutions that contain no unnecessary actions. To support the LLM in generating exact solutions, we provided it with different types of domain knowledge, including pipeline information, failure management instructions, and data from historical failures. We conducted an ablation study to determine which type of domain knowledge contributed most to solution accuracy. The results show that data from historical failures contributed the most to the system's accuracy, enabling it to produce exact solutions in 92.1% of cases in our dataset. The system correctly identified the error location with 97.4% accuracy when provided with domain knowledge, compared to 84.2% accuracy without it. In conclusion, our findings indicate that LLMs, when provided with data from historical failures, represent a promising approach for automating CI/CD pipeline failure management.

</details>


### [22] [Statistical-Based Metric Threshold Setting Method for Software Fault Prediction in Firmware Projects: An Industrial Experience](https://arxiv.org/abs/2602.06831)
*Marco De Luca,Domenico Amalfitano,Anna Rita Fasolino,Porfirio Tramontana*

Main category: cs.SE

TL;DR: 本文提出了一种可解释的软件度量阈值设定方法，通过静态分析工具提取工业嵌入式固件指标，建立跨项目的故障预测模型。


<details>
  <summary>Details</summary>
Motivation: 嵌入式固件在安全关键领域（如ISO 26262标准）需要高可靠性保障，但现有AI故障预测模型缺乏可解释性，阻碍工业应用。

Method: 利用Coverity和Understand工具分析三个工业级C语言嵌入式项目，通过统计假设检验识别关键软件指标，建立可跨项目复用的经验阈值模型。

Result: 实验验证阈值能高精度识别故障高危函数（high precision），避免了重新训练的需求，适用于同类软件系统。

Conclusion: 该方法提供了解释性强的工业级故障预测方案，为开发者提供可执行的软件质量评估工具，是黑盒AI模型的实用替代方案。

Abstract: Ensuring software quality in embedded firmware is critical, especially in safety-critical domains where compliance with functional safety standards (ISO 26262) requires strong guarantees of software reliability. While machine learning-based fault prediction models have demonstrated high accuracy, their lack of interpretability limits their adoption in industrial settings. Developers need actionable insights that can be directly employed in software quality assurance processes and guide defect mitigation strategies. In this paper, we present a structured process for defining context-specific software metric thresholds suitable for integration into fault detection workflows in industrial settings. Our approach supports cross-project fault prediction by deriving thresholds from one set of projects and applying them to independently developed firmware, thereby enabling reuse across similar software systems without retraining or domain-specific tuning. We analyze three real-world C-embedded firmware projects provided by an industrial partner, using Coverity and Understand static analysis tools to extract software metrics. Through statistical analysis and hypothesis testing, we identify discriminative metrics and derived empirical threshold values capable of distinguishing faulty from non-faulty functions. The derived thresholds are validated through an experimental evaluation, demonstrating their effectiveness in identifying fault-prone functions with high precision. The results confirm that the derived thresholds can serve as an interpretable solution for fault prediction, aligning with industry standards and SQA practices. This approach provides a practical alternative to black-box AI models, allowing developers to systematically assess software quality, take preventive actions, and integrate metric-based fault prediction into industrial development workflows to mitigate software faults.

</details>


<div id='cs.PF'></div>

# cs.PF [[Back]](#toc)

### [23] [End-to-End Throughput Benchmarking of Portable Deterministic CNN-Based Signal Processing Pipelines](https://arxiv.org/abs/2602.06216)
*Christiaan Boerkamp,Akhil John Thomas*

Main category: cs.PF

TL;DR: 本文提出了一种用于评估端到端确定性信号处理管道性能的基准测试方法，该方法针对相控阵工作负载（如超声成像），评估从RF到图像的完整管道在各种加速平台上的表现。


<details>
  <summary>Details</summary>
Motivation: 动机是现代AI加速器上开发可移植且可认证的信号处理实现，以避免硬件特定的重构，同时保持高性能。

Method: 方法包括在现实中执行条件下，使用持续输入吞吐量（MB/s）、有效帧率（FPS）、单次运行增量能耗和峰值内存使用指标报告性能，并在NVIDIA RTX 5090 GPU和Google TPU v5e-1等异构平台上基准测试无需训练且无需修改的CNN管道。

Result: 结果显示不同算子实现方式（如动态索引、全CNN表达和稀疏矩阵基础）对架构间的性能和可移植性产生显著影响。

Conclusion: 结论是该基准方法支持创建高效可移植的信号处理管道，满足多样AI加速器的需求，无需牺牲性能。

Abstract: This paper presents a benchmarking methodology for evaluating end-to-end performance of deterministic signal-processing pipelines expressed using CNN-compatible primitives. The benchmark targets phased-array workloads such as ultrasound imaging and evaluates complete RF-to-image pipelines under realistic execution conditions. Performance is reported using sustained input throughput (MB/s), effective frame rate (FPS), and, where available, incremental energy per run and peak memory usage. Using this methodology, we benchmark a single deterministic, training-free CNN-based signal-processing pipeline executed unmodified across heterogeneous accelerator platforms, including an NVIDIA RTX 5090 GPU and a Google TPU v5e-1. The results demonstrate how different operator formulations (dynamic indexing, fully CNN-expressed, and sparse-matrix-based) impact performance and portability across architectures. This work is motivated by the need for portable, certifiable signal-processing implementations that avoid hardware-specific refactoring while retaining high performance on modern AI accelerators.

</details>
