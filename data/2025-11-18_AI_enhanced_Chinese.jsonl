{"id": "2511.11789", "categories": ["cs.MA", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.11789", "abs": "https://arxiv.org/abs/2511.11789", "authors": ["Jiayi Li", "Xiao Liu", "Yansong Feng"], "title": "From Single to Societal: Analyzing Persona-Induced Bias in Multi-Agent Interactions", "comment": "AAAI-2026", "summary": "Large Language Model (LLM)-based multi-agent systems are increasingly used to simulate human interactions and solve collaborative tasks. A common practice is to assign agents with personas to encourage behavioral diversity. However, this raises a critical yet underexplored question: do personas introduce biases into multi-agent interactions? This paper presents a systematic investigation into persona-induced biases in multi-agent interactions, with a focus on social traits like trustworthiness (how an agent's opinion is received by others) and insistence (how strongly an agent advocates for its opinion). Through a series of controlled experiments in collaborative problem-solving and persuasion tasks, we reveal that (1) LLM-based agents exhibit biases in both trustworthiness and insistence, with personas from historically advantaged groups (e.g., men and White individuals) perceived as less trustworthy and demonstrating less insistence; and (2) agents exhibit significant in-group favoritism, showing a higher tendency to conform to others who share the same persona. These biases persist across various LLMs, group sizes, and numbers of interaction rounds, highlighting an urgent need for awareness and mitigation to ensure the fairness and reliability of multi-agent systems.", "AI": {"tldr": "\u672c\u6587\u7cfb\u7edf\u7814\u7a76\u4e86\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u4e2d\u89d2\u8272\u8bbe\u5b9a\uff08personas\uff09\u5f15\u53d1\u7684\u504f\u89c1\u95ee\u9898\uff0c\u53d1\u73b0\u5177\u6709\u5386\u53f2\u4e0a\u4f18\u52bf\u7fa4\u4f53\u7279\u5f81\u7684\u89d2\u8272\u5728\u4fe1\u4efb\u5ea6\u548c\u575a\u6301\u6027\u65b9\u9762\u8868\u73b0\u51fa\u7cfb\u7edf\u6027\u504f\u5dee\uff0c\u5e76\u5b58\u5728\u660e\u663e\u7684\u5185\u7fa4\u4f53\u504f\u597d\u3002", "motivation": "\u5f53\u524d\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u5e38\u901a\u8fc7\u8d4b\u4e88\u667a\u80fd\u4f53\u89d2\u8272\u4ee5\u589e\u5f3a\u884c\u4e3a\u591a\u6837\u6027\uff0c\u4f46\u89d2\u8272\u662f\u5426\u5f15\u5165\u504f\u89c1\u5c1a\u672a\u88ab\u5145\u5206\u7814\u7a76\u3002\u4f5c\u8005\u65e8\u5728\u63ed\u793a\u89d2\u8272\u8bbe\u5b9a\u5bf9\u591a\u667a\u80fd\u4f53\u4ea4\u4e92\u4e2d\u793e\u4f1a\u5c5e\u6027\uff08\u5982\u4fe1\u4efb\u5ea6\u4e0e\u575a\u6301\u6027\uff09\u7684\u5f71\u54cd\uff0c\u4ee5\u8bc4\u4f30\u5176\u516c\u5e73\u6027\u98ce\u9669\u3002", "method": "\u901a\u8fc7\u5728\u534f\u4f5c\u95ee\u9898\u89e3\u51b3\u4e0e\u8bf4\u670d\u4efb\u52a1\u4e2d\u8bbe\u8ba1\u53d7\u63a7\u5b9e\u9a8c\uff0c\u5206\u6790\u4e0d\u540c\u89d2\u8272\uff08\u5982\u6027\u522b\u3001\u79cd\u65cf\uff09\u5bf9LLM\u667a\u80fd\u4f53\u5728\u4fe1\u4efb\u5ea6\u548c\u575a\u6301\u6027\u4e0a\u7684\u5f71\u54cd\uff0c\u5e76\u8003\u5bdf\u6a21\u578b\u3001\u7fa4\u4f53\u89c4\u6a21\u548c\u4ea4\u4e92\u8f6e\u6b21\u7b49\u56e0\u7d20\u4e0b\u7684\u504f\u89c1\u7a33\u5b9a\u6027\u3002", "result": "(1) \u5177\u6709\u5386\u53f2\u4e0a\u4f18\u52bf\u7fa4\u4f53\u7279\u5f81\uff08\u5982\u7537\u6027\u3001\u767d\u4eba\uff09\u7684\u89d2\u8272\u88ab\u5176\u4ed6\u667a\u80fd\u4f53\u89c6\u4e3a\u66f4\u503c\u5f97\u4fe1\u8d56\u4e14\u66f4\u5177\u575a\u6301\u6027\uff1b(2) \u667a\u80fd\u4f53\u663e\u8457\u503e\u5411\u4e8e\u987a\u4ece\u4e0e\u5176\u89d2\u8272\u76f8\u540c\uff08\u540c\u7fa4\u4f53\uff09\u7684\u5176\u4ed6\u667a\u80fd\u4f53\uff0c\u8868\u73b0\u51fa\u5185\u7fa4\u4f53\u504f\u597d\u3002\u8fd9\u4e9b\u504f\u89c1\u5728\u4e0d\u540c\u6a21\u578b\u548c\u5b9e\u9a8c\u8bbe\u7f6e\u4e0b\u5747\u6301\u7eed\u5b58\u5728\u3002", "conclusion": "\u89d2\u8272\u8bbe\u5b9a\u4f1a\u5728\u591a\u667a\u80fd\u4f53\u4ea4\u4e92\u4e2d\u5f15\u5165\u7cfb\u7edf\u6027\u504f\u89c1\uff0c\u53ef\u80fd\u5f71\u54cd\u7cfb\u7edf\u7684\u516c\u5e73\u6027\u4e0e\u53ef\u9760\u6027\uff0c\u4e9f\u9700\u5f15\u8d77\u91cd\u89c6\u5e76\u5f00\u53d1\u76f8\u5e94\u7684\u7f13\u89e3\u7b56\u7565\u3002"}}
{"id": "2511.11854", "categories": ["cs.MA", "math.NA"], "pdf": "https://arxiv.org/pdf/2511.11854", "abs": "https://arxiv.org/abs/2511.11854", "authors": ["Vahid Hemmati", "Yonas Ayalew", "Ahmad Mohammadi", "Reza Ahmari", "Parham Kebria", "Abdollah Homaifar", "Mehrdad Saif"], "title": "Conflict-Free Flight Scheduling Using Strategic Demand Capacity Balancing for Urban Air Mobility Operations", "comment": null, "summary": "In this paper, we propose a conflict-free multi- agent flight scheduling that ensures robust separation in con- strained airspace for Urban Air Mobility (UAM) operations application. First, we introduce Pairwise Conflict Avoidance (PCA) based on delayed departures, leveraging kinematic principles to maintain safe distances. Next, we expand PCA to multi-agent scenarios, formulating an optimization approach that systematically determines departure times under increasing traffic densities. Performance metrics, such as average delay, assess the effectiveness of our solution. Through numerical simulations across diverse multi-agent environments and real- world UAM use cases, our method demonstrates a significant reduction in total delay while ensuring collision-free operations. This approach provides a scalable framework for emerging urban air mobility systems.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u51b2\u7a81\u7684\u591a\u667a\u80fd\u4f53\u98de\u884c\u8c03\u5ea6\u65b9\u6cd5\uff0c\u901a\u8fc7\u57fa\u4e8e\u5ef6\u8fdf\u8d77\u98de\u7684\u6210\u5bf9\u51b2\u7a81\u907f\u514d\uff08PCA\uff09\u7b56\u7565\uff0c\u5728\u57ce\u5e02\u7a7a\u4e2d\u4ea4\u901a\uff08UAM\uff09\u53d7\u9650\u7a7a\u57df\u4e2d\u5b9e\u73b0\u5b89\u5168\u95f4\u9694\uff0c\u5e76\u6269\u5c55\u81f3\u591a\u667a\u80fd\u4f53\u573a\u666f\uff0c\u663e\u8457\u964d\u4f4e\u603b\u5ef6\u8bef\u540c\u65f6\u4fdd\u8bc1\u65e0\u78b0\u649e\u8fd0\u884c\u3002", "motivation": "\u4e3a\u5e94\u5bf9\u57ce\u5e02\u7a7a\u4e2d\u4ea4\u901a\uff08UAM\uff09\u5728\u53d7\u9650\u7a7a\u57df\u4e2d\u65e5\u76ca\u589e\u957f\u7684\u98de\u884c\u5668\u5bc6\u5ea6\u6240\u5e26\u6765\u7684\u51b2\u7a81\u98ce\u9669\uff0c\u4e9f\u9700\u4e00\u79cd\u53ef\u6269\u5c55\u4e14\u9ad8\u6548\u7684\u98de\u884c\u8c03\u5ea6\u65b9\u6cd5\uff0c\u4ee5\u786e\u4fdd\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u4e2d\u7684\u5b89\u5168\u5206\u79bb\u4e0e\u8fd0\u884c\u6548\u7387\u3002", "method": "\u9996\u5148\u63d0\u51fa\u57fa\u4e8e\u5ef6\u8fdf\u8d77\u98de\u7684\u6210\u5bf9\u51b2\u7a81\u907f\u514d\uff08PCA\uff09\u673a\u5236\uff0c\u5229\u7528\u8fd0\u52a8\u5b66\u539f\u7406\u7ef4\u6301\u5b89\u5168\u8ddd\u79bb\uff1b\u968f\u540e\u5c06\u5176\u6269\u5c55\u81f3\u591a\u667a\u80fd\u4f53\u573a\u666f\uff0c\u6784\u5efa\u4f18\u5316\u6a21\u578b\u4ee5\u7cfb\u7edf\u6027\u5730\u786e\u5b9a\u5404\u98de\u884c\u5668\u7684\u8d77\u98de\u65f6\u95f4\uff0c\u5e76\u901a\u8fc7\u5e73\u5747\u5ef6\u8bef\u7b49\u6307\u6807\u8bc4\u4f30\u6027\u80fd\u3002", "result": "\u6570\u503c\u4eff\u771f\u8868\u660e\uff0c\u5728\u591a\u79cd\u591a\u667a\u80fd\u4f53\u73af\u5883\u548c\u771f\u5b9eUAM\u5e94\u7528\u573a\u666f\u4e0b\uff0c\u8be5\u65b9\u6cd5\u5728\u786e\u4fdd\u65e0\u78b0\u649e\u8fd0\u884c\u7684\u540c\u65f6\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u603b\u5ef6\u8bef\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u65b0\u5174\u7684\u57ce\u5e02\u7a7a\u4e2d\u4ea4\u901a\u7cfb\u7edf\u63d0\u4f9b\u4e86\u4e00\u4e2a\u53ef\u6269\u5c55\u3001\u9ad8\u6548\u4e14\u5b89\u5168\u7684\u98de\u884c\u8c03\u5ea6\u6846\u67b6\u3002"}}
{"id": "2511.11992", "categories": ["cs.MA", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.11992", "abs": "https://arxiv.org/abs/2511.11992", "authors": ["Hung Du", "Hy Nguyen", "Srikanth Thudumu", "Rajesh Vasa", "Kon Mouzakis"], "title": "Goal-Oriented Multi-Agent Reinforcement Learning for Decentralized Agent Teams", "comment": "Accepted poster at the IEEE Consumer Communications & Networking Conference (CCNC) 2026", "summary": "Connected and autonomous vehicles across land, water, and air must often operate in dynamic, unpredictable environments with limited communication, no centralized control, and partial observability. These real-world constraints pose significant challenges for coordination, particularly when vehicles pursue individual objectives. To address this, we propose a decentralized Multi-Agent Reinforcement Learning (MARL) framework that enables vehicles, acting as agents, to communicate selectively based on local goals and observations. This goal-aware communication strategy allows agents to share only relevant information, enhancing collaboration while respecting visibility limitations. We validate our approach in complex multi-agent navigation tasks featuring obstacles and dynamic agent populations. Results show that our method significantly improves task success rates and reduces time-to-goal compared to non-cooperative baselines. Moreover, task performance remains stable as the number of agents increases, demonstrating scalability. These findings highlight the potential of decentralized, goal-driven MARL to support effective coordination in realistic multi-vehicle systems operating across diverse domains.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u53bb\u4e2d\u5fc3\u5316\u7684\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u76ee\u6807\u611f\u77e5\u7684\u901a\u4fe1\u7b56\u7565\uff0c\u4f7f\u9646\u3001\u6c34\u3001\u7a7a\u81ea\u4e3b\u8f7d\u5177\u5728\u53d7\u9650\u73af\u5883\u4e0b\u9ad8\u6548\u534f\u4f5c\uff0c\u663e\u8457\u63d0\u5347\u4efb\u52a1\u6210\u529f\u7387\u4e0e\u53ef\u6269\u5c55\u6027\u3002", "motivation": "\u73b0\u5b9e\u4e16\u754c\u4e2d\uff0c\u8de8\u57df\u81ea\u4e3b\u8f7d\u5177\u5e38\u9762\u4e34\u52a8\u6001\u4e0d\u53ef\u9884\u6d4b\u73af\u5883\u3001\u901a\u4fe1\u53d7\u9650\u3001\u65e0\u4e2d\u5fc3\u63a7\u5236\u548c\u90e8\u5206\u53ef\u89c2\u6d4b\u6027\u7b49\u6311\u6218\uff0c\u5c24\u5176\u5728\u5404\u8f7d\u5177\u8ffd\u6c42\u4e2a\u4f53\u76ee\u6807\u65f6\u96be\u4ee5\u6709\u6548\u534f\u8c03\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u53bb\u4e2d\u5fc3\u5316\u7684\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\uff08MARL\uff09\u6846\u67b6\uff0c\u5f15\u5165\u57fa\u4e8e\u5c40\u90e8\u76ee\u6807\u4e0e\u89c2\u6d4b\u7684\u76ee\u6807\u611f\u77e5\u901a\u4fe1\u673a\u5236\uff0c\u4f7f\u667a\u80fd\u4f53\u4ec5\u5171\u4eab\u76f8\u5173\u4fe1\u606f\u4ee5\u589e\u5f3a\u534f\u4f5c\u3002", "result": "\u5728\u5305\u542b\u969c\u788d\u7269\u548c\u52a8\u6001\u667a\u80fd\u4f53\u6570\u91cf\u7684\u590d\u6742\u5bfc\u822a\u4efb\u52a1\u4e2d\uff0c\u8be5\u65b9\u6cd5\u76f8\u6bd4\u975e\u534f\u4f5c\u57fa\u7ebf\u663e\u8457\u63d0\u9ad8\u4e86\u4efb\u52a1\u6210\u529f\u7387\u5e76\u7f29\u77ed\u4e86\u5230\u8fbe\u76ee\u6807\u65f6\u95f4\uff0c\u4e14\u5728\u667a\u80fd\u4f53\u6570\u91cf\u589e\u52a0\u65f6\u6027\u80fd\u4fdd\u6301\u7a33\u5b9a\u3002", "conclusion": "\u53bb\u4e2d\u5fc3\u5316\u3001\u76ee\u6807\u9a71\u52a8\u7684MARL\u6846\u67b6\u80fd\u6709\u6548\u652f\u6301\u8de8\u57df\u591a\u8f7d\u5177\u7cfb\u7edf\u5728\u73b0\u5b9e\u6761\u4ef6\u4e0b\u7684\u534f\u8c03\uff0c\u5177\u5907\u826f\u597d\u7684\u5b9e\u7528\u6027\u4e0e\u53ef\u6269\u5c55\u6027\u3002"}}
{"id": "2511.11999", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2511.11999", "abs": "https://arxiv.org/abs/2511.11999", "authors": ["Zeyu Lu", "Peng Zhang", "Chun Yong Chong", "Shan Gao", "Yibiao Yang", "Yanhui Li", "Lin Chen", "Yuming Zhou"], "title": "WITNESS: A lightweight and practical approach to fine-grained predictive mutation testing", "comment": null, "summary": "Existing fine-grained predictive mutation testing studies predominantly rely on deep learning, which faces two critical limitations in practice: (1) Exorbitant computational costs. The deep learning models adopted in these studies demand significant computational resources for training and inference acceleration. This introduces high costs and undermines the cost-reduction goal of predictive mutation testing. (2) Constrained applicability. Although modern mutation testing tools generate mutants both inside and outside methods, current fine-grained predictive mutation testing approaches handle only inside-method mutants. As a result, they cannot predict outside-method mutants, limiting their applicability in real-world scenarios. We propose WITNESS, a new fine-grained predictive mutation testing approach. WITNESS adopts a twofold design: (1) With collected features from both inside-method and outside-method mutants, WITNESS is suitable for all generated mutants. (2) Instead of using computationally expensive deep learning, WITNESS employs lightweight classical machine learning models for training and prediction. This makes it more cost-effective and enabling straightforward explanations of the decision-making processes behind the adopted models. Evaluations on Defects4J projects show that WITNESS consistently achieves state-of-the-art predictive performance across different scenarios. Additionally, WITNESS significantly enhances the efficiency of kill matrix prediction. Post-hoc analysis reveals that features incorporating information from before and after the mutation are the most important among those used in WITNESS. Test case prioritization based on the predicted kill matrix shows that WITNESS delivers results much closer to those obtained by using the actual kill matrix, outperforming baseline approaches.", "AI": {"tldr": "WITNESS \u662f\u4e00\u79cd\u65b0\u7684\u7ec6\u7c92\u5ea6\u9884\u6d4b\u6027\u53d8\u5f02\u6d4b\u8bd5\u65b9\u6cd5\uff0c\u91c7\u7528\u8f7b\u91cf\u7ea7\u7ecf\u5178\u673a\u5668\u5b66\u4e60\u6a21\u578b\uff0c\u652f\u6301\u65b9\u6cd5\u5185\u5916\u7684\u53d8\u5f02\u4f53\u9884\u6d4b\uff0c\u5728\u4fdd\u6301\u9ad8\u6027\u80fd\u7684\u540c\u65f6\u663e\u8457\u964d\u4f4e\u8ba1\u7b97\u6210\u672c\uff0c\u5e76\u63d0\u5347\u6740\u77e9\u9635\u9884\u6d4b\u6548\u7387\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u7ec6\u7c92\u5ea6\u9884\u6d4b\u6027\u53d8\u5f02\u6d4b\u8bd5\u65b9\u6cd5\u5b58\u5728\u8ba1\u7b97\u6210\u672c\u9ad8\u6602\u548c\u9002\u7528\u8303\u56f4\u53d7\u9650\uff08\u4ec5\u5904\u7406\u65b9\u6cd5\u5185\u53d8\u5f02\u4f53\uff09\u4e24\u5927\u95ee\u9898\uff0c\u96be\u4ee5\u6ee1\u8db3\u5b9e\u9645\u5e94\u7528\u4e2d\u5bf9\u6210\u672c\u6548\u76ca\u548c\u5168\u9762\u6027\u7684\u9700\u6c42\u3002", "method": "WITNESS \u91c7\u7528\u53cc\u91cd\u8bbe\u8ba1\uff1a(1) \u6536\u96c6\u65b9\u6cd5\u5185\u5916\u53d8\u5f02\u4f53\u7684\u7279\u5f81\uff0c\u5b9e\u73b0\u5bf9\u6240\u6709\u53d8\u5f02\u4f53\u7684\u9884\u6d4b\uff1b(2) \u4f7f\u7528\u8f7b\u91cf\u7ea7\u7ecf\u5178\u673a\u5668\u5b66\u4e60\u6a21\u578b\u66ff\u4ee3\u6df1\u5ea6\u5b66\u4e60\uff0c\u4ee5\u964d\u4f4e\u6210\u672c\u5e76\u589e\u5f3a\u6a21\u578b\u53ef\u89e3\u91ca\u6027\u3002", "result": "\u5728 Defects4J \u9879\u76ee\u4e0a\u7684\u8bc4\u4f30\u8868\u660e\uff0cWITNESS \u5728\u4e0d\u540c\u573a\u666f\u4e0b\u5747\u8fbe\u5230\u6700\u5148\u8fdb\u7684\u9884\u6d4b\u6027\u80fd\uff0c\u663e\u8457\u63d0\u5347\u6740\u77e9\u9635\u9884\u6d4b\u6548\u7387\uff1b\u540e\u9a8c\u5206\u6790\u663e\u793a\u5305\u542b\u53d8\u5f02\u524d\u540e\u4fe1\u606f\u7684\u7279\u5f81\u6700\u4e3a\u91cd\u8981\uff1b\u57fa\u4e8e\u9884\u6d4b\u6740\u77e9\u9635\u7684\u6d4b\u8bd5\u7528\u4f8b\u4f18\u5148\u7ea7\u6392\u5e8f\u7ed3\u679c\u66f4\u63a5\u8fd1\u771f\u5b9e\u6740\u77e9\u9635\uff0c\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "WITNESS \u6709\u6548\u514b\u670d\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u8ba1\u7b97\u6210\u672c\u548c\u9002\u7528\u6027\u65b9\u9762\u7684\u5c40\u9650\uff0c\u5728\u4fdd\u8bc1\u9ad8\u9884\u6d4b\u6027\u80fd\u7684\u540c\u65f6\u63d0\u5347\u4e86\u6548\u7387\u4e0e\u53ef\u89e3\u91ca\u6027\uff0c\u5177\u6709\u66f4\u5f3a\u7684\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2511.11586", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2511.11586", "abs": "https://arxiv.org/abs/2511.11586", "authors": ["Ao Zhou", "Jianlei Yang", "Tong Qiao", "Yingjie Qi", "Xinming Wei", "Cenlin Duan", "Weisheng Zhao", "Chunming Hu"], "title": "ACE-GNN: Adaptive GNN Co-Inference with System-Aware Scheduling in Dynamic Edge Environments", "comment": "This paper is accepted by the Journal of IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems", "summary": "The device-edge co-inference paradigm effectively bridges the gap between the high resource demands of Graph Neural Networks (GNNs) and limited device resources, making it a promising solution for advancing edge GNN applications. Existing research enhances GNN co-inference by leveraging offline model splitting and pipeline parallelism (PP), which enables more efficient computation and resource utilization during inference. However, the performance of these static deployment methods is significantly affected by environmental dynamics such as network fluctuations and multi-device access, which remain unaddressed. We present ACE-GNN, the first Adaptive GNN Co-inference framework tailored for dynamic Edge environments, to boost system performance and stability. ACE-GNN achieves performance awareness for complex multi-device access edge systems via system-level abstraction and two novel prediction methods, enabling rapid runtime scheme optimization. Moreover, we introduce a data parallelism (DP) mechanism in the runtime optimization space, enabling adaptive scheduling between PP and DP to leverage their distinct advantages and maintain stable system performance. Also, an efficient batch inference strategy and specialized communication middleware are implemented to further improve performance. Extensive experiments across diverse applications and edge settings demonstrate that ACE-GNN achieves a speedup of up to 12.7x and an energy savings of 82.3% compared to GCoDE, as well as 11.7 better energy efficiency than Fograph.", "AI": {"tldr": "ACE-GNN is an adaptive GNN co-inference framework for dynamic edge environments that combines system-level abstraction, novel prediction methods, and hybrid parallelism (pipeline and data) to significantly improve speed, energy efficiency, and stability over existing approaches.", "motivation": "Existing GNN co-inference methods based on static model splitting and pipeline parallelism suffer performance degradation in dynamic edge environments due to network fluctuations and multi-device access scenarios, which they fail to adapt to.", "method": "ACE-GNN introduces system-level abstraction and two novel prediction methods for runtime performance awareness, integrates a data parallelism mechanism alongside pipeline parallelism for adaptive scheduling, and employs an efficient batch inference strategy with specialized communication middleware.", "result": "Experiments show ACE-GNN achieves up to 12.7\u00d7 speedup and 82.3% energy savings compared to GCoDE, and 11.7\u00d7 better energy efficiency than Fograph across diverse edge settings and applications.", "conclusion": "ACE-GNN effectively addresses the limitations of static co-inference methods by enabling adaptive, high-performance, and energy-efficient GNN inference in dynamic edge computing environments."}}
{"id": "2511.13109", "categories": ["cs.PF"], "pdf": "https://arxiv.org/pdf/2511.13109", "abs": "https://arxiv.org/abs/2511.13109", "authors": ["Fabian B\u00f6hm", "Nils Kohl", "Harald K\u00f6stler", "Ulrich R\u00fcde"], "title": "Large-scale Multigrid with Adaptive Galerkin Coarsening", "comment": null, "summary": "We propose a robust, adaptive coarse-grid correction scheme for matrix-free geometric multigrid targeting PDEs with strongly varying coefficients. The method combines uniform geometric coarsening of the underlying grid with heterogeneous coarse-grid operators: Galerkin coarse grid approximation is applied locally in regions with large coefficient gradients, while lightweight, direct coarse grid approximation is used elsewhere. This selective application ensures that local Galerkin operators are computed and stored only where necessary, minimizing memory requirements while maintaining robust convergence. We demonstrate the method on a suite of sinker benchmark problems for the generalized Stokes equation, including grid-aligned and unaligned viscosity jumps, smoothly varying viscosity functions with large gradients, and different viscosity evaluation techniques. We analytically quantify the solver's memory consumption and demonstrate its efficiency by solving Stokes problems with $10^{10}$ degrees of freedom, viscosity jumps of $10^{6}$ magnitude, and more than 100{,}000 parallel processes.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9488\u5bf9\u5f3a\u53d8\u7cfb\u6570\u504f\u5fae\u5206\u65b9\u7a0b\u7684\u81ea\u9002\u5e94\u7c97\u7f51\u683c\u6821\u6b63\u65b9\u6cd5\uff0c\u7ed3\u5408\u4e86\u51e0\u4f55\u5747\u5300\u7c97\u5316\u4e0e\u5c40\u90e8Galerkin\u7b97\u5b50\uff0c\u5728\u4fdd\u8bc1\u6536\u655b\u6027\u7684\u540c\u65f6\u663e\u8457\u964d\u4f4e\u5185\u5b58\u5f00\u9500\uff0c\u5e76\u5728\u5341\u4ebf\u81ea\u7531\u5ea6\u89c4\u6a21\u7684\u95ee\u9898\u4e0a\u9a8c\u8bc1\u4e86\u5176\u9ad8\u6548\u6027\u3002", "motivation": "\u4f20\u7edf\u591a\u91cd\u7f51\u683c\u65b9\u6cd5\u5728\u5904\u7406\u5177\u6709\u5f3a\u53d8\u7cfb\u6570\uff08\u5982\u7c98\u5ea6\u8df3\u8dc3\uff09\u7684\u504f\u5fae\u5206\u65b9\u7a0b\u65f6\uff0c\u5f80\u5f80\u96be\u4ee5\u517c\u987e\u6536\u655b\u9c81\u68d2\u6027\u4e0e\u5185\u5b58\u6548\u7387\u3002\u5168\u5c40\u4f7f\u7528Galerkin\u7c97\u5316\u867d\u51c6\u786e\u4f46\u5185\u5b58\u5f00\u9500\u5927\uff0c\u800c\u7b80\u5355\u7c97\u5316\u5219\u53ef\u80fd\u4e27\u5931\u6536\u655b\u6027\u3002\u56e0\u6b64\uff0c\u4e9f\u9700\u4e00\u79cd\u65e2\u80fd\u4fdd\u6301\u6536\u655b\u6027\u80fd\u53c8\u80fd\u63a7\u5236\u5185\u5b58\u6d88\u8017\u7684\u81ea\u9002\u5e94\u7c97\u7f51\u683c\u7b56\u7565\u3002", "method": "\u8be5\u65b9\u6cd5\u5728\u51e0\u4f55\u591a\u91cd\u7f51\u683c\u6846\u67b6\u4e0b\uff0c\u5bf9\u5e95\u5c42\u7f51\u683c\u8fdb\u884c\u5747\u5300\u7c97\u5316\uff0c\u4f46\u5728\u7c97\u7f51\u683c\u7b97\u5b50\u6784\u5efa\u65f6\u91c7\u7528\u5f02\u6784\u7b56\u7565\uff1a\u5728\u7cfb\u6570\u68af\u5ea6\u5927\u7684\u533a\u57df\u5c40\u90e8\u4f7f\u7528Galerkin\u8fd1\u4f3c\uff0c\u5728\u5176\u4f59\u533a\u57df\u5219\u91c7\u7528\u8f7b\u91cf\u7ea7\u7684\u76f4\u63a5\u7c97\u5316\u3002\u8fd9\u79cd\u9009\u62e9\u6027\u5e94\u7528\u4ec5\u5728\u5fc5\u8981\u5904\u8ba1\u7b97\u5e76\u5b58\u50a8\u5c40\u90e8Galerkin\u7b97\u5b50\u3002", "result": "\u5728\u5e7f\u4e49Stokes\u65b9\u7a0b\u7684\u4e00\u7cfb\u5217\u201csinker\u201d\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u5305\u62ec\u5bf9\u9f50/\u975e\u5bf9\u9f50\u7c98\u5ea6\u8df3\u8dc3\u3001\u5149\u6ed1\u5927\u68af\u5ea6\u7c98\u5ea6\u51fd\u6570\u7b49\u60c5\u5f62\u3002\u8be5\u65b9\u6cd5\u6210\u529f\u6c42\u89e3\u4e86\u542b10^10\u81ea\u7531\u5ea6\u3001\u7c98\u5ea6\u8df3\u8dc3\u8fbe10^6\u3001\u5e76\u884c\u8fdb\u7a0b\u8d8510\u4e07\u7684\u5927\u89c4\u6a21\u95ee\u9898\uff0c\u5e76\u5b9a\u91cf\u5206\u6790\u4e86\u5185\u5b58\u6d88\u8017\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u81ea\u9002\u5e94\u7c97\u7f51\u683c\u6821\u6b63\u65b9\u6848\u5728\u5f3a\u53d8\u7cfb\u6570PDE\u95ee\u9898\u4e2d\u5b9e\u73b0\u4e86\u9c81\u68d2\u6536\u655b\u4e0e\u4f4e\u5185\u5b58\u5360\u7528\u7684\u826f\u597d\u5e73\u8861\uff0c\u7279\u522b\u9002\u7528\u4e8e\u5927\u89c4\u6a21\u5e76\u884c\u8ba1\u7b97\u73af\u5883\u4e0b\u7684\u77e9\u9635\u81ea\u7531\u51e0\u4f55\u591a\u91cd\u7f51\u683c\u6c42\u89e3\u5668\u3002"}}
{"id": "2511.11895", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2511.11895", "abs": "https://arxiv.org/abs/2511.11895", "authors": ["Thorben Schey", "Khaled Karoonlatifi", "Michael Weyrich", "Andrey Morozov"], "title": "Uncertainty-Guided Live Measurement Sequencing for Fast SAR ADC Linearity Testing", "comment": "9 pages, 8 figures. this is the preprint version of the paper accepted for publication at ICCAD 2025", "summary": "This paper introduces a novel closed-loop testing methodology for efficient linearity testing of high-resolution Successive Approximation Register (SAR) Analog-to-Digital Converters (ADCs). Existing test strategies, including histogram-based approaches, sine wave testing, and model-driven reconstruction, often rely on dense data acquisition followed by offline post-processing, which increases overall test time and complexity. To overcome these limitations, we propose an adaptive approach that utilizes an iterative behavioral model refined by an Extended Kalman Filter (EKF) in real time, enabling direct estimation of capacitor mismatch parameters that determine INL behavior. Our algorithm dynamically selects measurement points based on current model uncertainty, maximizing information gain with respect to parameter confidence and narrowing sampling intervals as estimation progresses. By providing immediate feedback and adaptive targeting, the proposed method eliminates the need for large-scale data collection and post-measurement analysis. Experimental results demonstrate substantial reductions in total test time and computational overhead, highlighting the method's suitability for integration in production environments.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6269\u5c55\u5361\u5c14\u66fc\u6ee4\u6ce2\u5668\uff08EKF\uff09\u7684\u95ed\u73af\u81ea\u9002\u5e94\u6d4b\u8bd5\u65b9\u6cd5\uff0c\u7528\u4e8e\u9ad8\u6548\u6d4b\u8bd5\u9ad8\u5206\u8fa8\u7387SAR ADC\u7684\u7ebf\u6027\u5ea6\uff0c\u663e\u8457\u51cf\u5c11\u4e86\u6d4b\u8bd5\u65f6\u95f4\u548c\u8ba1\u7b97\u5f00\u9500\u3002", "motivation": "\u73b0\u6709SAR ADC\u7ebf\u6027\u5ea6\u6d4b\u8bd5\u65b9\u6cd5\uff08\u5982\u76f4\u65b9\u56fe\u6cd5\u3001\u6b63\u5f26\u6ce2\u6d4b\u8bd5\u548c\u6a21\u578b\u9a71\u52a8\u91cd\u5efa\uff09\u901a\u5e38\u4f9d\u8d56\u5bc6\u96c6\u6570\u636e\u91c7\u96c6\u548c\u79bb\u7ebf\u540e\u5904\u7406\uff0c\u5bfc\u81f4\u6d4b\u8bd5\u65f6\u95f4\u957f\u3001\u590d\u6742\u5ea6\u9ad8\u3002", "method": "\u91c7\u7528\u57fa\u4e8e\u6269\u5c55\u5361\u5c14\u66fc\u6ee4\u6ce2\u5668\uff08EKF\uff09\u7684\u8fed\u4ee3\u884c\u4e3a\u6a21\u578b\uff0c\u5728\u7ebf\u5b9e\u65f6\u4f30\u8ba1\u51b3\u5b9aINL\u7279\u6027\u7684\u7535\u5bb9\u5931\u914d\u53c2\u6570\uff0c\u5e76\u6839\u636e\u5f53\u524d\u6a21\u578b\u4e0d\u786e\u5b9a\u6027\u52a8\u6001\u9009\u62e9\u6d4b\u91cf\u70b9\uff0c\u4ee5\u6700\u5927\u5316\u53c2\u6570\u4f30\u8ba1\u7684\u4fe1\u606f\u589e\u76ca\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u6240\u63d0\u65b9\u6cd5\u5927\u5e45\u51cf\u5c11\u4e86\u603b\u6d4b\u8bd5\u65f6\u95f4\u548c\u8ba1\u7b97\u8d1f\u62c5\uff0c\u65e0\u9700\u5927\u89c4\u6a21\u6570\u636e\u91c7\u96c6\u548c\u540e\u671f\u5206\u6790\u3002", "conclusion": "\u8be5\u81ea\u9002\u5e94\u95ed\u73af\u6d4b\u8bd5\u65b9\u6cd5\u9ad8\u6548\u3001\u5b9e\u7528\uff0c\u9002\u7528\u4e8e\u9ad8\u5206\u8fa8\u7387SAR ADC\u5728\u91cf\u4ea7\u73af\u5883\u4e2d\u7684\u96c6\u6210\u6d4b\u8bd5\u3002"}}
{"id": "2511.12599", "categories": ["cs.MA"], "pdf": "https://arxiv.org/pdf/2511.12599", "abs": "https://arxiv.org/abs/2511.12599", "authors": ["Bijia Liu", "Ronghao Dang"], "title": "FINRS: A Risk-Sensitive Trading Framework for Real Financial Markets", "comment": "LLM Applications, LLM Agents, Financial Technology", "summary": "Large language models (LLMs) have shown strong reasoning capabilities and are increasingly explored for financial trading. Existing LLM-based trading agents, however, largely focus on single-step prediction and lack integrated mechanisms for risk management, which reduces their effectiveness in volatile markets. We introduce FinRS, a risk-sensitive trading framework that combines hierarchical market analysis, dual-decision agents, and multi-timescale reward reflection to align trading actions with both return objectives and downside risk constraints. Experiments on multiple stocks and market conditions show that FinRS achieves superior profitability and stability compared to state-of-the-art methods.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faFinRS\uff0c\u4e00\u79cd\u7ed3\u5408\u591a\u5c42\u6b21\u5e02\u573a\u5206\u6790\u3001\u53cc\u51b3\u7b56\u667a\u80fd\u4f53\u548c\u591a\u65f6\u95f4\u5c3a\u5ea6\u5956\u52b1\u53cd\u9988\u7684\u98ce\u9669\u654f\u611f\u578b\u4ea4\u6613\u6846\u67b6\uff0c\u5728\u76c8\u5229\u6027\u548c\u7a33\u5b9a\u6027\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u4ea4\u6613\u667a\u80fd\u4f53\u4e3b\u8981\u5173\u6ce8\u5355\u6b65\u9884\u6d4b\uff0c\u7f3a\u4e4f\u6574\u5408\u7684\u98ce\u9669\u7ba1\u7406\u673a\u5236\uff0c\u5728\u6ce2\u52a8\u5e02\u573a\u4e2d\u6548\u679c\u6709\u9650\u3002", "method": "\u63d0\u51faFinRS\u6846\u67b6\uff0c\u878d\u5408\u591a\u5c42\u6b21\u5e02\u573a\u5206\u6790\u3001\u53cc\u51b3\u7b56\u667a\u80fd\u4f53\u548c\u591a\u65f6\u95f4\u5c3a\u5ea6\u5956\u52b1\u53cd\u9988\uff0c\u4ee5\u517c\u987e\u6536\u76ca\u76ee\u6807\u4e0e\u4e0b\u884c\u98ce\u9669\u7ea6\u675f\u3002", "result": "\u5728\u591a\u79cd\u80a1\u7968\u548c\u5e02\u573a\u6761\u4ef6\u4e0b\u5b9e\u9a8c\u8868\u660e\uff0cFinRS\u5728\u76c8\u5229\u80fd\u529b\u548c\u7a33\u5b9a\u6027\u65b9\u9762\u4f18\u4e8e\u5f53\u524d\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\u3002", "conclusion": "FinRS\u901a\u8fc7\u5f15\u5165\u98ce\u9669\u654f\u611f\u673a\u5236\u663e\u8457\u63d0\u5347\u4e86\u5927\u8bed\u8a00\u6a21\u578b\u5728\u91d1\u878d\u4ea4\u6613\u4e2d\u7684\u5b9e\u9645\u8868\u73b0\uff0c\u4e3a\u6784\u5efa\u66f4\u7a33\u5065\u7684\u667a\u80fd\u4ea4\u6613\u7cfb\u7edf\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2511.12127", "categories": ["cs.NI"], "pdf": "https://arxiv.org/pdf/2511.12127", "abs": "https://arxiv.org/abs/2511.12127", "authors": ["Md Rahat Hasan", "Kazi Ahmed Akbar Munim", "Md. Forkan Uddin"], "title": "Joint Optimization of RU Allocation and C-SR in Multi-AP Coordinated Wi-Fi Systems", "comment": null, "summary": "We formulate an optimization problem for joint RU allocation and C-SR to maximize the throughput of a multi-AP coordinated WiFi system. The optimization problem is found to be a non-linear integer programming problem. We solve the problem for several network scenarios using an optimization tool. The joint design significantly improves throughput compared to a non-coordinated system. To reduce computational complexity, we also provide a heuristic solution to the problem. The proposed heuristic achieves throughput comparable to that of the computationally expensive optimization tool based solution approach.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u8054\u5408RU\u5206\u914d\u4e0eC-SR\u7684\u4f18\u5316\u65b9\u6cd5\uff0c\u4ee5\u63d0\u5347\u591aAP\u534f\u540cWiFi\u7cfb\u7edf\u7684\u541e\u5410\u91cf\uff0c\u5e76\u8bbe\u8ba1\u4e86\u4e00\u79cd\u4f4e\u590d\u6742\u5ea6\u7684\u542f\u53d1\u5f0f\u7b97\u6cd5\uff0c\u5728\u6027\u80fd\u63a5\u8fd1\u6700\u4f18\u89e3\u7684\u540c\u65f6\u663e\u8457\u964d\u4f4e\u8ba1\u7b97\u5f00\u9500\u3002", "motivation": "\u4e3a\u63d0\u5347\u591aAP\u534f\u540cWiFi\u7cfb\u7edf\u7684\u541e\u5410\u91cf\uff0c\u9700\u540c\u65f6\u4f18\u5316RU\u5206\u914d\u4e0eC-SR\u673a\u5236\uff0c\u4f46\u8be5\u95ee\u9898\u5177\u6709\u975e\u7ebf\u6027\u6574\u6570\u89c4\u5212\u7279\u6027\uff0c\u8ba1\u7b97\u590d\u6742\u5ea6\u9ad8\u3002", "method": "\u6784\u5efa\u8054\u5408RU\u5206\u914d\u4e0eC-SR\u7684\u975e\u7ebf\u6027\u6574\u6570\u89c4\u5212\u4f18\u5316\u6a21\u578b\uff0c\u5e76\u5229\u7528\u4f18\u5316\u5de5\u5177\u6c42\u89e3\uff1b\u540c\u65f6\u63d0\u51fa\u4e00\u79cd\u542f\u53d1\u5f0f\u7b97\u6cd5\u4ee5\u964d\u4f4e\u8ba1\u7b97\u590d\u6742\u5ea6\u3002", "result": "\u8054\u5408\u8bbe\u8ba1\u65b9\u6848\u76f8\u6bd4\u975e\u534f\u540c\u7cfb\u7edf\u663e\u8457\u63d0\u5347\u4e86\u541e\u5410\u91cf\uff1b\u6240\u63d0\u542f\u53d1\u5f0f\u7b97\u6cd5\u5728\u541e\u5410\u91cf\u6027\u80fd\u4e0a\u63a5\u8fd1\u57fa\u4e8e\u4f18\u5316\u5de5\u5177\u7684\u7cbe\u786e\u89e3\u3002", "conclusion": "\u8054\u5408RU\u5206\u914d\u4e0eC-SR\u80fd\u6709\u6548\u63d0\u5347\u7cfb\u7edf\u541e\u5410\u91cf\uff0c\u4e14\u6240\u63d0\u51fa\u7684\u542f\u53d1\u5f0f\u65b9\u6cd5\u5728\u4fdd\u8bc1\u6027\u80fd\u7684\u540c\u65f6\u5927\u5e45\u964d\u4f4e\u4e86\u8ba1\u7b97\u590d\u6742\u5ea6\u3002"}}
{"id": "2511.13450", "categories": ["cs.PF", "cs.ET"], "pdf": "https://arxiv.org/pdf/2511.13450", "abs": "https://arxiv.org/abs/2511.13450", "authors": ["\u00c1lvaro Corrochano L\u00f3pez", "Carlos Garc\u00eda S\u00e1nchez"], "title": "Evaluation of Domain-Specific Architectures for General-Purpose Applications in Apple Silicon", "comment": "11 pages, IEEE Format, IPDPS Submission (In revision), 12 figures, 8 tables", "summary": "The rise of AI and its growing computational demands have driven the integration of domain-specific accelerators (such as GPUs, TPUs, and NPUs) across the entire computing infrastructure. Following the precedent set by the GPGPU which popularized GPUs for general-purpose tasks, this research asks whether this phenomenon can be replicated with specialized accelerators like NPUs in new contexts. This paper evaluates the potential of the Apple Neural Engine (ANE) designed for high energy efficiency in Machine Learning workloads, in the context of general-purpose HPC applications. We evaluate the performance and energy consumption of classic HPC algorithms such as GEMM, Jacobi or Multigrid methods on Apple's ANE across the M1 and the latest M4 architectures. Results confirm that, when algorithms are properly adapted, the ANE achieves competitive performance (up to 3.8 TFlops on the M4-Pro, comparable to the GPU's 4.7 TFlops on the same SoC for GEMM operation) while demonstrating significantly superior energy efficiency (e.g., GEMM consumes 5.2 Watts on the ANE versus 24 Watts on GPU counterpart in M4 architectures).", "AI": {"tldr": "\u672c\u6587\u8bc4\u4f30\u4e86\u82f9\u679c\u795e\u7ecf\u5f15\u64ce\uff08ANE\uff09\u5728\u901a\u7528\u9ad8\u6027\u80fd\u8ba1\u7b97\uff08HPC\uff09\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\u4e0e\u80fd\u6548\uff0c\u53d1\u73b0\u7ecf\u9002\u5f53\u9002\u914d\u540e\uff0cANE\u5728GEMM\u7b49\u7ecf\u5178\u7b97\u6cd5\u4e0a\u53ef\u5b9e\u73b0\u4e0eGPU\u76f8\u5f53\u7684\u6027\u80fd\uff0c\u540c\u65f6\u663e\u8457\u4f18\u4e8eGPU\u7684\u80fd\u6548\u8868\u73b0\u3002", "motivation": "\u968f\u7740AI\u8ba1\u7b97\u9700\u6c42\u589e\u957f\uff0c\u9886\u57df\u4e13\u7528\u52a0\u901f\u5668\uff08\u5982GPU\u3001TPU\u3001NPU\uff09\u88ab\u5e7f\u6cdb\u90e8\u7f72\u3002\u53d7GPGPU\u5c06GPU\u7528\u4e8e\u901a\u7528\u8ba1\u7b97\u7684\u542f\u53d1\uff0c\u672c\u6587\u63a2\u7d22\u662f\u5426\u53ef\u5c06\u4e13\u4e3a\u673a\u5668\u5b66\u4e60\u8bbe\u8ba1\u7684NPU\uff08\u5982ANE\uff09\u62d3\u5c55\u5e94\u7528\u4e8e\u901a\u7528HPC\u573a\u666f\u3002", "method": "\u5728Apple M1\u548cM4\u82af\u7247\u4e0a\uff0c\u5bf9GEMM\u3001Jacobi\u548cMultigrid\u7b49\u7ecf\u5178HPC\u7b97\u6cd5\u8fdb\u884c\u9002\u914d\uff0c\u5e76\u5728ANE\u4e0a\u8fd0\u884c\uff0c\u8bc4\u4f30\u5176\u6027\u80fd\uff08\u5982TFlops\uff09\u548c\u80fd\u8017\uff08\u74e6\u7279\uff09\u5e76\u4e0e\u540c\u5e73\u53f0GPU\u8fdb\u884c\u5bf9\u6bd4\u3002", "result": "\u7ecf\u9002\u914d\u540e\uff0cANE\u5728M4-Pro\u4e0aGEMM\u6027\u80fd\u8fbe3.8 TFlops\uff08\u63a5\u8fd1\u540c\u5e73\u53f0GPU\u76844.7 TFlops\uff09\uff0c\u800c\u529f\u8017\u4ec5\u4e3a5.2\u74e6\uff0c\u8fdc\u4f4e\u4e8eGPU\u768424\u74e6\uff0c\u5c55\u73b0\u51fa\u5353\u8d8a\u7684\u80fd\u6548\u4f18\u52bf\u3002", "conclusion": "\u82f9\u679c\u795e\u7ecf\u5f15\u64ce\u5728\u9002\u914d\u540e\u53ef\u7528\u4e8e\u901a\u7528HPC\u4efb\u52a1\uff0c\u5728\u4fdd\u6301\u7ade\u4e89\u529b\u6027\u80fd\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u80fd\u6548\uff0c\u8868\u660e\u4e13\u7528AI\u52a0\u901f\u5668\u6709\u6f5c\u529b\u6269\u5c55\u81f3\u66f4\u5e7f\u6cdb\u7684\u8ba1\u7b97\u9886\u57df\u3002"}}
{"id": "2511.11917", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2511.11917", "abs": "https://arxiv.org/abs/2511.11917", "authors": ["Thorben Schey", "Khaled Karoonlatifi", "Michael Weyrich", "Andrey Morozov"], "title": "Advanced Strategies for Uncertainty-Guided Live Measurement Sequencing in Fast, Robust SAR ADC Linearity Testing", "comment": "6 pages, 5 figures, this is the preprint version of the paper accepted for publication at ATS 2025", "summary": "This paper builds on our Uncertainty-Guided Live Measurement Sequencing (UGLMS) method. UGLMS is a closed-loop test strategy that adaptively selects SAR ADC code edges based on model uncertainty and refines a behavioral mismatch model in real time via an Extended Kalman Filter (EKF), eliminating full-range sweeps and offline post-processing. We introduce an enhanced UGLMS that delivers significantly faster test runtimes while maintaining estimation accuracy. First, a rank-1 EKF update replaces costly matrix inversions with efficient vector operations, and a measurement-aligned covariance-inflation strategy accelerates convergence under unexpected innovations. Second, we extend the static mismatch model with a low-order carrier polynomial to capture systematic nonlinearities beyond pure capacitor mismatch. Third, a trace-based termination adapts test length to convergence, preventing premature stops and redundant iterations. Simulations show the enhanced UGLMS reconstructs full Integral- and Differential-Non-Linearity (INL/DNL) in just 36 ms for 16-bit and under 70 ms for 18-bit ADCs (120 ms with the polynomial extension). Combining the faster convergence from covariance inflation with reduced per-iteration runtime from the rank-1 EKF update, the method reaches equal accuracy 8x faster for 16-bit ADCs. These improvements enable real-time, production-ready SAR ADC linearity testing.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u589e\u5f3a\u578b\u4e0d\u786e\u5b9a\u6027\u5f15\u5bfc\u7684\u5b9e\u65f6\u6d4b\u91cf\u5e8f\u5217\u65b9\u6cd5\uff08UGLMS\uff09\uff0c\u901a\u8fc7\u79e9-1 EKF\u66f4\u65b0\u3001\u534f\u65b9\u5dee\u81a8\u80c0\u7b56\u7565\u3001\u4f4e\u9636\u8f7d\u6ce2\u591a\u9879\u5f0f\u6269\u5c55\u548c\u57fa\u4e8e\u8ff9\u7684\u7ec8\u6b62\u673a\u5236\uff0c\u663e\u8457\u52a0\u5febSAR ADC\u7ebf\u6027\u5ea6\u6d4b\u8bd5\u901f\u5ea6\uff0c\u5728\u4fdd\u6301\u7cbe\u5ea6\u7684\u540c\u65f6\u5c0616\u4f4dADC\u6d4b\u8bd5\u63d0\u901f8\u500d\uff0c\u5b9e\u73b0\u6beb\u79d2\u7ea7INL/DNL\u91cd\u5efa\u3002", "motivation": "\u4f20\u7edfSAR ADC\u7ebf\u6027\u5ea6\u6d4b\u8bd5\u4f9d\u8d56\u5168\u8303\u56f4\u626b\u63cf\u548c\u79bb\u7ebf\u540e\u5904\u7406\uff0c\u8017\u65f6\u4e14\u96be\u4ee5\u7528\u4e8e\u751f\u4ea7\u73af\u5883\uff1b\u73b0\u6709UGLMS\u867d\u5df2\u5b9e\u73b0\u95ed\u73af\u81ea\u9002\u5e94\u6d4b\u8bd5\uff0c\u4f46\u4ecd\u5b58\u5728\u8ba1\u7b97\u5f00\u9500\u5927\u3001\u6536\u655b\u6162\u53ca\u6a21\u578b\u8868\u8fbe\u80fd\u529b\u6709\u9650\u7b49\u95ee\u9898\u3002", "method": "1\uff09\u91c7\u7528\u79e9-1 EKF\u66f4\u65b0\u66ff\u4ee3\u77e9\u9635\u6c42\u9006\uff0c\u964d\u4f4e\u8ba1\u7b97\u590d\u6742\u5ea6\uff1b2\uff09\u5f15\u5165\u4e0e\u6d4b\u91cf\u5bf9\u9f50\u7684\u534f\u65b9\u5dee\u81a8\u80c0\u7b56\u7565\u4ee5\u52a0\u901f\u6536\u655b\uff1b3\uff09\u5728\u9759\u6001\u5931\u914d\u6a21\u578b\u4e2d\u52a0\u5165\u4f4e\u9636\u8f7d\u6ce2\u591a\u9879\u5f0f\u4ee5\u5efa\u6a21\u7cfb\u7edf\u975e\u7ebf\u6027\uff1b4\uff09\u8bbe\u8ba1\u57fa\u4e8e\u534f\u65b9\u5dee\u8ff9\u7684\u81ea\u9002\u5e94\u7ec8\u6b62\u51c6\u5219\u52a8\u6001\u63a7\u5236\u6d4b\u8bd5\u957f\u5ea6\u3002", "result": "\u4eff\u771f\u8868\u660e\uff0c\u589e\u5f3a\u578bUGLMS\u53ef\u572836 ms\u5185\u5b8c\u621016\u4f4dADC\u300170 ms\u5185\u5b8c\u621018\u4f4dADC\u7684\u5b8c\u6574INL/DNL\u91cd\u5efa\uff08\u542b\u591a\u9879\u5f0f\u6269\u5c55\u65f6\u4e3a120 ms\uff09\uff0c\u76f8\u6bd4\u539f\u65b9\u6cd5\u5728\u76f8\u540c\u7cbe\u5ea6\u4e0b\u6d4b\u8bd5\u901f\u5ea6\u63d0\u53478\u500d\u3002", "conclusion": "\u6240\u63d0\u589e\u5f3a\u578bUGLMS\u663e\u8457\u63d0\u5347\u4e86SAR ADC\u7ebf\u6027\u5ea6\u6d4b\u8bd5\u7684\u6548\u7387\u4e0e\u5b9e\u7528\u6027\uff0c\u5177\u5907\u5b9e\u65f6\u6027\u548c\u91cf\u4ea7\u9002\u7528\u6027\uff0c\u4e3a\u9ad8\u7cbe\u5ea6ADC\u7684\u5feb\u901f\u5728\u7ebf\u6d4b\u8bd5\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2511.12960", "categories": ["cs.MA"], "pdf": "https://arxiv.org/pdf/2511.12960", "abs": "https://arxiv.org/abs/2511.12960", "authors": ["Daivik Patel", "Shrenik Patel"], "title": "ENGRAM: Effective, Lightweight Memory Orchestration for Conversational Agents", "comment": null, "summary": "Large language models (LLMs) deployed in user-facing applications require long-horizon consistency: the ability to remember prior interactions, respect user preferences, and ground reasoning in past events. However, contemporary memory systems often adopt complex architectures such as knowledge graphs, multi-stage retrieval pipelines, and OS-style schedulers, which introduce engineering complexity and reproducibility challenges. We present ENGRAM, a lightweight memory system that organizes conversation into three canonical memory types (episodic, semantic, and procedural) through a single router and retriever. Each user turn is converted into typed memory records with normalized schemas and embeddings and stored in a database. At query time, the system retrieves top-k dense neighbors for each type, merges results with simple set operations, and provides the most relevant evidence as context to the model. ENGRAM attains state-of-the-art results on LoCoMo, a multi-session conversational QA benchmark for long-horizon memory, and exceeds the full-context baseline by 15 points on LongMemEval while using only about 1% of the tokens. These results show that careful memory typing and straightforward dense retrieval can enable effective long-term memory management in language models without requiring complex architectures.", "AI": {"tldr": "ENGRAM \u662f\u4e00\u79cd\u8f7b\u91cf\u7ea7\u8bb0\u5fc6\u7cfb\u7edf\uff0c\u901a\u8fc7\u5c06\u5bf9\u8bdd\u5185\u5bb9\u5212\u5206\u4e3a\u4e09\u79cd\u6807\u51c6\u8bb0\u5fc6\u7c7b\u578b\uff08\u60c5\u666f\u3001\u8bed\u4e49\u548c\u7a0b\u5e8f\u6027\uff09\uff0c\u5229\u7528\u5355\u4e00\u8def\u7531\u4e0e\u68c0\u7d22\u673a\u5236\uff0c\u5728\u663e\u8457\u51cf\u5c11 token \u4f7f\u7528\u7684\u540c\u65f6\uff0c\u5728\u957f\u65f6\u8bb0\u5fc6\u4efb\u52a1\u4e0a\u8fbe\u5230\u5f53\u524d\u6700\u4f18\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u9762\u5411\u7528\u6237\u7684\u5927\u8bed\u8a00\u6a21\u578b\u8bb0\u5fc6\u7cfb\u7edf\u67b6\u6784\u590d\u6742\uff08\u5982\u77e5\u8bc6\u56fe\u8c31\u3001\u591a\u9636\u6bb5\u68c0\u7d22\u7b49\uff09\uff0c\u5e26\u6765\u5de5\u7a0b\u5b9e\u73b0\u548c\u53ef\u590d\u73b0\u6027\u96be\u9898\uff0c\u4e9f\u9700\u4e00\u79cd\u66f4\u7b80\u6d01\u9ad8\u6548\u7684\u8bb0\u5fc6\u7ba1\u7406\u65b9\u6848\u3002", "method": "ENGRAM \u5c06\u6bcf\u8f6e\u7528\u6237\u4ea4\u4e92\u8f6c\u5316\u4e3a\u5177\u6709\u6807\u51c6\u5316\u6a21\u5f0f\u548c\u5d4c\u5165\u7684\u7c7b\u578b\u5316\u8bb0\u5fc6\u8bb0\u5f55\uff0c\u5b58\u50a8\u4e8e\u6570\u636e\u5e93\u4e2d\uff1b\u67e5\u8be2\u65f6\u5bf9\u6bcf\u79cd\u8bb0\u5fc6\u7c7b\u578b\u8fdb\u884c\u7a20\u5bc6\u68c0\u7d22\uff0c\u901a\u8fc7\u7b80\u5355\u96c6\u5408\u64cd\u4f5c\u878d\u5408\u7ed3\u679c\uff0c\u5e76\u5c06\u6700\u76f8\u5173\u8bc1\u636e\u4f5c\u4e3a\u4e0a\u4e0b\u6587\u8f93\u5165\u6a21\u578b\u3002", "result": "\u5728 LoCoMo \u57fa\u51c6\u4e0a\u8fbe\u5230 SOTA\uff0c\u5728 LongMemEval \u4e0a\u4ee5\u4ec5\u7ea6 1% \u7684 token \u4f7f\u7528\u91cf\u8d85\u8d8a\u5168\u4e0a\u4e0b\u6587\u57fa\u7ebf 15 \u5206\u3002", "conclusion": "\u7cbe\u7ec6\u7684\u8bb0\u5fc6\u7c7b\u578b\u5212\u5206\u7ed3\u5408\u7b80\u5355\u7684\u7a20\u5bc6\u68c0\u7d22\u673a\u5236\uff0c\u5373\u53ef\u5b9e\u73b0\u9ad8\u6548\u957f\u671f\u8bb0\u5fc6\u7ba1\u7406\uff0c\u65e0\u9700\u4f9d\u8d56\u590d\u6742\u67b6\u6784\u3002"}}
{"id": "2511.12229", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2511.12229", "abs": "https://arxiv.org/abs/2511.12229", "authors": ["Zhipeng Xue", "Zhipeng Gao", "Tongtong Xu", "Xing Hu", "Xin Xia", "Shanping Li"], "title": "Actionable Warning Is Not Enough: Recommending Valid Actionable Warnings with Weak Supervision", "comment": null, "summary": "The use of static analysis tools has gained increasing popularity among developers in the last few years. However, the widespread adoption of static analysis tools is hindered by their high false alarm rates. Previous studies have introduced the concept of actionable warnings and built a machine-learning method to distinguish actionable warnings from false alarms. However, according to our empirical observation, the current assumption used for actionable warning(s) collection is rather shaky and inaccurate, leading to a large number of invalid actionable warnings. To address this problem, in this study, we build the first large actionable warning dataset by mining 68,274 reversions from Top-500 GitHub C repositories, we then take one step further by assigning each actionable warning a weak label regarding its likelihood of being a real bug. Following that, we propose a two-stage framework called ACWRecommender to automatically recommend the actionable warnings with high probability to be real bugs (AWHB). Our approach warms up the pre-trained model UniXcoder by identifying actionable warnings task (coarse-grained detection stage) and rerank AWHB to the top by weakly supervised learning (fine-grained reranking stage). Experimental results show that our proposed model outperforms several baselines by a large margin in terms of nDCG and MRR for AWHB recommendation. Moreover, we ran our tool on 6 randomly selected projects and manually checked the top-ranked warnings from 2,197 reported warnings, we reported top-10 recommended warnings to developers, 27 of them were already confirmed by developers as real bugs. Developers can quickly find real bugs among the massive amount of reported warnings, which verifies the practical usage of our tool.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aACWRecommender\u7684\u4e24\u9636\u6bb5\u6846\u67b6\uff0c\u901a\u8fc7\u6784\u5efa\u9996\u4e2a\u5927\u89c4\u6a21\u53ef\u64cd\u4f5c\u8b66\u544a\u6570\u636e\u96c6\u5e76\u5f15\u5165\u5f31\u76d1\u7763\u5b66\u4e60\uff0c\u6709\u6548\u63d0\u5347\u9759\u6001\u5206\u6790\u5de5\u5177\u4e2d\u771f\u5b9e\u7f3a\u9677\u8b66\u544a\u7684\u63a8\u8350\u51c6\u786e\u7387\u3002", "motivation": "\u73b0\u6709\u9759\u6001\u5206\u6790\u5de5\u5177\u56e0\u9ad8\u8bef\u62a5\u7387\u800c\u96be\u4ee5\u88ab\u5e7f\u6cdb\u91c7\u7528\uff0c\u4e14\u4ee5\u5f80\u7814\u7a76\u4e2d\u5bf9\u201c\u53ef\u64cd\u4f5c\u8b66\u544a\u201d\u7684\u5b9a\u4e49\u548c\u6536\u96c6\u5047\u8bbe\u4e0d\u51c6\u786e\uff0c\u5bfc\u81f4\u5927\u91cf\u65e0\u6548\u6807\u7b7e\uff0c\u5f71\u54cd\u4e86\u673a\u5668\u5b66\u4e60\u6a21\u578b\u7684\u6548\u679c\u3002", "method": "\u4f5c\u8005\u4eceGitHub\u4e0aTop-500\u7684C\u8bed\u8a00\u9879\u76ee\u4e2d\u6316\u639868,274\u4e2a\u56de\u6eda\u63d0\u4ea4\uff0c\u6784\u5efa\u9996\u4e2a\u5927\u89c4\u6a21\u53ef\u64cd\u4f5c\u8b66\u544a\u6570\u636e\u96c6\uff0c\u5e76\u4e3a\u6bcf\u4e2a\u8b66\u544a\u5206\u914d\u8868\u793a\u5176\u4e3a\u771f\u5b9e\u7f3a\u9677\u53ef\u80fd\u6027\u7684\u5f31\u6807\u7b7e\uff1b\u968f\u540e\u63d0\u51fa\u4e24\u9636\u6bb5\u6846\u67b6ACWRecommender\uff1a\u7b2c\u4e00\u9636\u6bb5\u4f7f\u7528\u9884\u8bad\u7ec3\u6a21\u578bUniXcoder\u8fdb\u884c\u7c97\u7c92\u5ea6\u53ef\u64cd\u4f5c\u8b66\u544a\u8bc6\u522b\uff0c\u7b2c\u4e8c\u9636\u6bb5\u901a\u8fc7\u5f31\u76d1\u7763\u5b66\u4e60\u5bf9\u9ad8\u53ef\u80fd\u6027\u771f\u5b9e\u7f3a\u9677\u8b66\u544a\uff08AWHB\uff09\u8fdb\u884c\u7ec6\u7c92\u5ea6\u91cd\u6392\u5e8f\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cACWRecommender\u5728nDCG\u548cMRR\u6307\u6807\u4e0a\u663e\u8457\u4f18\u4e8e\u591a\u4e2a\u57fa\u7ebf\u6a21\u578b\uff1b\u57286\u4e2a\u968f\u673a\u9879\u76ee\u4e2d\u4eba\u5de5\u9a8c\u8bc1\u4e862,197\u4e2a\u8b66\u544a\uff0c\u5411\u5f00\u53d1\u8005\u62a5\u544a\u7684\u524d10\u4e2a\u63a8\u8350\u8b66\u544a\u4e2d\u670927\u4e2a\u88ab\u786e\u8ba4\u4e3a\u771f\u5b9e\u7f3a\u9677\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u80fd\u6709\u6548\u5e2e\u52a9\u5f00\u53d1\u8005\u4ece\u6d77\u91cf\u9759\u6001\u5206\u6790\u8b66\u544a\u4e2d\u5feb\u901f\u5b9a\u4f4d\u771f\u5b9e\u7f3a\u9677\uff0c\u5177\u6709\u826f\u597d\u7684\u5b9e\u7528\u4ef7\u503c\u3002"}}
{"id": "2511.11601", "categories": ["cs.DC", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.11601", "abs": "https://arxiv.org/abs/2511.11601", "authors": ["Elliott Wen", "Sean Ma", "Ewan Tempero", "Jens Dietrich", "Daniel Luo", "Jiaxing Shen", "Kaiqi Zhao", "Bruce Sham", "Yousong Song", "Jiayi Hua", "Jia Hong"], "title": "Mind the Gap: Revealing Inconsistencies Across Heterogeneous AI Accelerators", "comment": null, "summary": "While NVIDIA remains the dominant provider of AI accelerators within cloud data center, emerging vendors such as AMD, Intel, Mac, and Huawei offer cost-effective alternatives with claims of compatibility and performance. This paper presents the first empirical study investigating divergence in machine learning model across heterogeneous AI accelerators. Utilizing an automated pipeline, we synthesize over 100,000 variant models derived from 4,000 real-world models and execute them across five different enterprise-grade accelerators. Our findings suggest that newer AI platforms from Mac and Huawei support at least 17\\% fewer operators than NVIDIA. These platforms also exhibit a higher rate of output discrepancies (exceeding 5\\%), which stem from differences in operator implementations, handling of exceptional numerical values, and instruction scheduling. They are also more susceptible to failures during model compilation-based acceleration, and in some cases, the compiled models produce outputs that differ noticeably from those generated using the standard execution mode. In addition, we identify 7 implementation flaws in PyTorch and 40 platform-specific issues across vendors. These results underscore the challenges of achieving consistent machine learning behavior in an increasingly diverse hardware ecosystem.", "AI": {"tldr": "\u672c\u6587\u9996\u6b21\u5b9e\u8bc1\u7814\u7a76\u4e86\u5f02\u6784AI\u52a0\u901f\u5668\u4e0a\u673a\u5668\u5b66\u4e60\u6a21\u578b\u7684\u884c\u4e3a\u5dee\u5f02\uff0c\u53d1\u73b0Mac\u548c\u534e\u4e3a\u7b49\u65b0\u5174\u5e73\u53f0\u5728\u7b97\u5b50\u652f\u6301\u3001\u6570\u503c\u4e00\u81f4\u6027\u53ca\u7f16\u8bd1\u7a33\u5b9a\u6027\u65b9\u9762\u663e\u8457\u843d\u540e\u4e8eNVIDIA\uff0c\u5e76\u63ed\u793a\u4e86PyTorch\u53ca\u5404\u5382\u5546\u5e73\u53f0\u4e2d\u7684\u591a\u4e2a\u5b9e\u73b0\u7f3a\u9677\u3002", "motivation": "\u968f\u7740AMD\u3001Intel\u3001Mac\u548c\u534e\u4e3a\u7b49\u5382\u5546\u63a8\u51fa\u58f0\u79f0\u517c\u5bb9\u4e14\u5177\u6027\u4ef7\u6bd4\u7684AI\u52a0\u901f\u5668\uff0c\u4e1a\u754c\u4e9f\u9700\u4e86\u89e3\u8fd9\u4e9b\u5f02\u6784\u786c\u4ef6\u5728\u5b9e\u9645\u673a\u5668\u5b66\u4e60\u4efb\u52a1\u4e2d\u662f\u5426\u80fd\u63d0\u4f9b\u4e0eNVIDIA\u4e00\u81f4\u7684\u884c\u4e3a\uff0c\u4ee5\u8bc4\u4f30\u5176\u53ef\u9760\u6027\u548c\u90e8\u7f72\u98ce\u9669\u3002", "method": "\u6784\u5efa\u81ea\u52a8\u5316\u6d4b\u8bd5\u7ba1\u9053\uff0c\u4ece4,000\u4e2a\u771f\u5b9e\u6a21\u578b\u751f\u6210\u8d85\u8fc710\u4e07\u4e2a\u53d8\u4f53\uff0c\u5728\u4e94\u79cd\u4f01\u4e1a\u7ea7AI\u52a0\u901f\u5668\u4e0a\u6267\u884c\uff0c\u7cfb\u7edf\u6bd4\u8f83\u7b97\u5b50\u652f\u6301\u5ea6\u3001\u8f93\u51fa\u4e00\u81f4\u6027\u3001\u5f02\u5e38\u6570\u503c\u5904\u7406\u3001\u7f16\u8bd1\u7a33\u5b9a\u6027\u7b49\u65b9\u9762\u3002", "result": "Mac\u548c\u534e\u4e3a\u5e73\u53f0\u81f3\u5c11\u5c11\u652f\u630117%\u7684\u7b97\u5b50\uff0c\u8f93\u51fa\u5dee\u5f02\u7387\u8d855%\uff1b\u5728\u7f16\u8bd1\u52a0\u901f\u9636\u6bb5\u66f4\u6613\u5931\u8d25\uff0c\u4e14\u7f16\u8bd1\u540e\u6a21\u578b\u8f93\u51fa\u53ef\u80fd\u660e\u663e\u504f\u79bb\u6807\u51c6\u6267\u884c\u6a21\u5f0f\uff1b\u53d1\u73b0PyTorch\u4e2d7\u4e2a\u5b9e\u73b0\u7f3a\u9677\u53ca\u5404\u5e73\u53f0\u517140\u4e2a\u7279\u5b9a\u95ee\u9898\u3002", "conclusion": "\u5728\u65e5\u76ca\u591a\u6837\u5316\u7684AI\u786c\u4ef6\u751f\u6001\u4e2d\uff0c\u5b9e\u73b0\u4e00\u81f4\u7684\u673a\u5668\u5b66\u4e60\u884c\u4e3a\u4ecd\u9762\u4e34\u4e25\u5cfb\u6311\u6218\uff0c\u65b0\u5174\u52a0\u901f\u5668\u5728\u517c\u5bb9\u6027\u3001\u6570\u503c\u7a33\u5b9a\u6027\u548c\u5de5\u7a0b\u5b9e\u73b0\u65b9\u9762\u5c1a\u4e0d\u6210\u719f\u3002"}}
{"id": "2511.12035", "categories": ["cs.AR", "cs.CV"], "pdf": "https://arxiv.org/pdf/2511.12035", "abs": "https://arxiv.org/abs/2511.12035", "authors": ["Wenxuan Miao", "Yulin Sun", "Aiyue Chen", "Jing Lin", "Yiwu Yao", "Yiming Gan", "Jieru Zhao", "Jingwen Leng", "Mingyi Guo", "Yu Feng"], "title": "TIMERIPPLE: Accelerating vDiTs by Understanding the Spatio-Temporal Correlations in Latent Space", "comment": null, "summary": "The recent surge in video generation has shown the growing demand for high-quality video synthesis using large vision models. Existing video generation models are predominantly based on the video diffusion transformer (vDiT), however, they suffer from substantial inference delay due to self-attention. While prior studies have focused on reducing redundant computations in self-attention, they often overlook the inherent spatio-temporal correlations in video streams and directly leverage sparsity patterns from large language models to reduce attention computations.\n  In this work, we take a principled approach to accelerate self-attention in vDiTs by leveraging the spatio-temporal correlations in the latent space. We show that the attention patterns within vDiT are primarily due to the dominant spatial and temporal correlations at the token channel level. Based on this insight, we propose a lightweight and adaptive reuse strategy that approximates attention computations by reusing partial attention scores of spatially or temporally correlated tokens along individual channels. We demonstrate that our method achieves significantly higher computational savings (85\\%) compared to state-of-the-art techniques over 4 vDiTs, while preserving almost identical video quality ($<$0.06\\% loss on VBench).", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u8f7b\u91cf\u7ea7\u81ea\u9002\u5e94\u91cd\u7528\u7b56\u7565\uff0c\u901a\u8fc7\u5229\u7528\u89c6\u9891\u6269\u6563Transformer\uff08vDiT\uff09\u4e2d\u6f5c\u5728\u7a7a\u95f4\u7684\u65f6\u7a7a\u76f8\u5173\u6027\uff0c\u5728\u4fdd\u6301\u51e0\u4e4e\u76f8\u540c\u89c6\u9891\u8d28\u91cf\u7684\u524d\u63d0\u4e0b\uff0c\u663e\u8457\u51cf\u5c11\u81ea\u6ce8\u610f\u529b\u8ba1\u7b97\u5f00\u9500\uff08\u8282\u770185%\u8ba1\u7b97\u91cf\uff09\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8evDiT\u7684\u89c6\u9891\u751f\u6210\u6a21\u578b\u56e0\u81ea\u6ce8\u610f\u529b\u673a\u5236\u5bfc\u81f4\u63a8\u7406\u5ef6\u8fdf\u4e25\u91cd\uff1b\u4ee5\u5f80\u52a0\u901f\u65b9\u6cd5\u5ffd\u7565\u4e86\u89c6\u9891\u6d41\u56fa\u6709\u7684\u65f6\u7a7a\u76f8\u5173\u6027\uff0c\u76f4\u63a5\u5957\u7528\u5927\u8bed\u8a00\u6a21\u578b\u7684\u7a00\u758f\u6a21\u5f0f\uff0c\u6548\u679c\u6709\u9650\u3002", "method": "\u5206\u6790vDiT\u4e2d\u81ea\u6ce8\u610f\u529b\u6a21\u5f0f\uff0c\u53d1\u73b0\u5176\u4e3b\u8981\u6e90\u4e8etoken\u901a\u9053\u5c42\u9762\u7684\u4e3b\u5bfc\u65f6\u7a7a\u76f8\u5173\u6027\uff1b\u636e\u6b64\u8bbe\u8ba1\u4e00\u79cd\u8f7b\u91cf\u4e14\u81ea\u9002\u5e94\u7684\u6ce8\u610f\u529b\u5206\u6570\u91cd\u7528\u7b56\u7565\uff0c\u5bf9\u65f6\u7a7a\u76f8\u5173token\u5728\u5404\u901a\u9053\u4e0a\u7684\u90e8\u5206\u6ce8\u610f\u529b\u5206\u6570\u8fdb\u884c\u590d\u7528\uff0c\u4ee5\u8fd1\u4f3c\u5b8c\u6574\u8ba1\u7b97\u3002", "result": "\u57284\u79cdvDiT\u6a21\u578b\u4e0a\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u76f8\u6bd4\u5f53\u524d\u6700\u4f18\u6280\u672f\u53ef\u8282\u770185%\u7684\u8ba1\u7b97\u91cf\uff0c\u540c\u65f6\u89c6\u9891\u8d28\u91cf\u635f\u5931\u6781\u5c0f\uff08VBench\u6307\u6807\u4e0b\u964d\u4e0d\u52300.06%\uff09\u3002", "conclusion": "\u5229\u7528vDiT\u6f5c\u5728\u7a7a\u95f4\u4e2d\u7684\u65f6\u7a7a\u76f8\u5173\u6027\uff0c\u53ef\u9ad8\u6548\u52a0\u901f\u81ea\u6ce8\u610f\u529b\u8ba1\u7b97\uff0c\u5b9e\u73b0\u9ad8\u4fdd\u771f\u89c6\u9891\u751f\u6210\u4e0e\u663e\u8457\u63a8\u7406\u52a0\u901f\u7684\u517c\u987e\u3002"}}
{"id": "2511.12987", "categories": ["cs.MA"], "pdf": "https://arxiv.org/pdf/2511.12987", "abs": "https://arxiv.org/abs/2511.12987", "authors": ["Daivik Patel", "Shrenik Patel"], "title": "Reuse, Don't Recompute: Efficient Large Reasoning Model Inference via Memory Orchestration", "comment": null, "summary": "Large reasoning models (LRMs) achieve strong accuracy through test-time scaling, generating longer chains of thought or sampling multiple solutions, but at steep costs in tokens and latency. We argue that memory is a core ingredient for efficient reasoning: when evidence already exists, models should think less by reusing structured memory instead of recomputing derivations. We present ENGRAM-R, an inference-time memory layer that integrates typed retrieval with compact fact card representations and explicit citation control. On the LoCoMo benchmark, ENGRAM-R reduces input tokens by 85% and reasoning tokens by 75% compared to full context while maintaining high accuracy. On a multi-hop slice of the LongMemEval benchmark, it achieves similar efficiency with substantial accuracy gains. These results show that memory is not only critical for long-horizon correctness but also a practical lever for efficient reasoning under tight compute, memory, and latency budgets.", "AI": {"tldr": "ENGRAM-R \u662f\u4e00\u79cd\u63a8\u7406\u65f6\u8bb0\u5fc6\u5c42\uff0c\u901a\u8fc7\u7ed3\u6784\u5316\u8bb0\u5fc6\u91cd\u7528\u663e\u8457\u51cf\u5c11\u5927\u63a8\u7406\u6a21\u578b\u7684 token \u4f7f\u7528\u548c\u5ef6\u8fdf\uff0c\u540c\u65f6\u4fdd\u6301\u751a\u81f3\u63d0\u5347\u51c6\u786e\u6027\u3002", "motivation": "\u5927\u63a8\u7406\u6a21\u578b\uff08LRMs\uff09\u5728\u6d4b\u8bd5\u65f6\u901a\u8fc7\u751f\u6210\u66f4\u957f\u7684\u601d\u7ef4\u94fe\u6216\u91c7\u6837\u591a\u4e2a\u89e3\u6765\u63d0\u9ad8\u51c6\u786e\u6027\uff0c\u4f46\u4ee3\u4ef7\u662f\u9ad8\u6602\u7684 token \u6d88\u8017\u548c\u5ef6\u8fdf\u3002\u4f5c\u8005\u8ba4\u4e3a\uff0c\u82e5\u5df2\u6709\u8bc1\u636e\u5b58\u5728\uff0c\u6a21\u578b\u5e94\u901a\u8fc7\u590d\u7528\u7ed3\u6784\u5316\u8bb0\u5fc6\u800c\u975e\u91cd\u590d\u8ba1\u7b97\u6765\u5b9e\u73b0\u9ad8\u6548\u63a8\u7406\u3002", "method": "\u63d0\u51fa ENGRAM-R\uff0c\u4e00\u79cd\u63a8\u7406\u65f6\u8bb0\u5fc6\u5c42\uff0c\u7ed3\u5408\u7c7b\u578b\u5316\u68c0\u7d22\u3001\u7d27\u51d1\u7684\u4e8b\u5b9e\u5361\u7247\u8868\u793a\u548c\u663e\u5f0f\u5f15\u7528\u63a7\u5236\uff0c\u4ee5\u590d\u7528\u5148\u524d\u63a8\u5bfc\u7ed3\u679c\u3002", "result": "\u5728 LoCoMo \u57fa\u51c6\u4e0a\uff0c\u76f8\u6bd4\u5b8c\u6574\u4e0a\u4e0b\u6587\uff0cENGRAM-R \u51cf\u5c11 85% \u7684\u8f93\u5165 token \u548c 75% \u7684\u63a8\u7406 token\uff0c\u540c\u65f6\u4fdd\u6301\u9ad8\u51c6\u786e\u7387\uff1b\u5728 LongMemEval \u7684\u591a\u8df3\u5b50\u96c6\u4e0a\uff0c\u4e5f\u5b9e\u73b0\u4e86\u7c7b\u4f3c\u6548\u7387\u5e76\u663e\u8457\u63d0\u5347\u51c6\u786e\u7387\u3002", "conclusion": "\u8bb0\u5fc6\u4e0d\u4ec5\u662f\u5b9e\u73b0\u957f\u671f\u63a8\u7406\u6b63\u786e\u6027\u7684\u5173\u952e\uff0c\u4e5f\u662f\u5728\u8ba1\u7b97\u3001\u5185\u5b58\u548c\u5ef6\u8fdf\u53d7\u9650\u6761\u4ef6\u4e0b\u63d0\u5347\u63a8\u7406\u6548\u7387\u7684\u5b9e\u7528\u624b\u6bb5\u3002"}}
{"id": "2511.12276", "categories": ["cs.SE", "cs.PL"], "pdf": "https://arxiv.org/pdf/2511.12276", "abs": "https://arxiv.org/abs/2511.12276", "authors": ["L. Thomas van Binsbergen", "Christopher A. Esterhuyse", "Tim M\u00fcller"], "title": "Reflections on the design, applications and implementations of the normative specification language eFLINT", "comment": "27 pages", "summary": "Checking the compliance of software against laws, regulations and contracts is increasingly important and costly as the embedding of software into societal practices is getting more pervasive. Moreover, the digitalised services provided by governmental organisations and companies are governed by an increasing amount of laws and regulations, requiring highly adaptable compliance practices. A potential solution is to automate compliance using software. However, automating compliance is difficult for various reasons. Legal practices involve subjective processes such as interpretation and qualification. New laws and regulations come into effect regularly and laws and regulations, as well as their interpretations, are subjected to constant revision. In addition, computational reasoning with laws requires a cross-disciplinary process involving both legal and software expertise.\n  This paper reflects on the domain-specific software language eFLINT developed to experiment with novel solutions. The language combines declarative and procedural elements to reason about situations and scenarios respectively, explicates and formalises connections between legal concepts and computational concepts, and is designed to automate compliance checks both before, during and after a software system runs. The various goals and applications areas for the language give rise to (conflicting) requirements. This paper reflects on the current design of the language by recalling various applications, the requirements they imposed, and subsequent design decisions. As such, this paper reports on results and insights of an investigation that can benefit language developers within the field of automated compliance.", "AI": {"tldr": "\u672c\u6587\u56de\u987e\u4e86\u9886\u57df\u7279\u5b9a\u8bed\u8a00 eFLINT \u7684\u8bbe\u8ba1\u4e0e\u5e94\u7528\uff0c\u8be5\u8bed\u8a00\u65e8\u5728\u901a\u8fc7\u7ed3\u5408\u58f0\u660e\u5f0f\u4e0e\u8fc7\u7a0b\u5f0f\u5143\u7d20\uff0c\u5b9e\u73b0\u5bf9\u6cd5\u5f8b\u5408\u89c4\u6027\u7684\u81ea\u52a8\u5316\u68c0\u67e5\uff0c\u5e76\u63a2\u8ba8\u5176\u5728\u4e0d\u540c\u5e94\u7528\u573a\u666f\u4e0b\u9762\u4e34\u7684\u51b2\u7a81\u6027\u9700\u6c42\u53ca\u8bbe\u8ba1\u51b3\u7b56\u3002", "motivation": "\u968f\u7740\u8f6f\u4ef6\u5728\u793e\u4f1a\u5b9e\u8df5\u4e2d\u65e5\u76ca\u666e\u53ca\uff0c\u786e\u4fdd\u5176\u7b26\u5408\u6cd5\u5f8b\u6cd5\u89c4\u548c\u5408\u540c\u53d8\u5f97\u6108\u53d1\u91cd\u8981\u4e14\u6210\u672c\u9ad8\u6602\u3002\u73b0\u6709\u5408\u89c4\u5b9e\u8df5\u9700\u9ad8\u5ea6\u9002\u5e94\u4e0d\u65ad\u53d8\u5316\u7684\u6cd5\u5f8b\u73af\u5883\uff0c\u800c\u81ea\u52a8\u5316\u5408\u89c4\u9762\u4e34\u6cd5\u5f8b\u89e3\u91ca\u4e3b\u89c2\u6027\u3001\u6cd5\u89c4\u9891\u7e41\u66f4\u65b0\u53ca\u8de8\u5b66\u79d1\u534f\u4f5c\u7b49\u6311\u6218\u3002", "method": "\u5f00\u53d1\u5e76\u53cd\u601d\u9886\u57df\u7279\u5b9a\u8bed\u8a00 eFLINT\uff0c\u8be5\u8bed\u8a00\u878d\u5408\u58f0\u660e\u5f0f\u4e0e\u8fc7\u7a0b\u5f0f\u7f16\u7a0b\u8303\u5f0f\uff0c\u663e\u5f0f\u5efa\u7acb\u6cd5\u5f8b\u6982\u5ff5\u4e0e\u8ba1\u7b97\u6982\u5ff5\u4e4b\u95f4\u7684\u8054\u7cfb\uff0c\u652f\u6301\u5728\u8f6f\u4ef6\u8fd0\u884c\u524d\u3001\u4e2d\u3001\u540e\u5404\u9636\u6bb5\u8fdb\u884c\u5408\u89c4\u6027\u68c0\u67e5\u3002", "result": "\u901a\u8fc7\u591a\u4e2a\u5b9e\u9645\u5e94\u7528\u573a\u666f\u5206\u6790 eFLINT \u6240\u9762\u4e34\u7684\u591a\u6837\u5316\uff08\u751a\u81f3\u51b2\u7a81\u7684\uff09\u9700\u6c42\uff0c\u603b\u7ed3\u4e86\u76f8\u5e94\u7684\u8bed\u8a00\u8bbe\u8ba1\u51b3\u7b56\uff0c\u4e3a\u81ea\u52a8\u5316\u5408\u89c4\u9886\u57df\u7684\u8bed\u8a00\u5f00\u53d1\u8005\u63d0\u4f9b\u4e86\u7ecf\u9a8c\u4e0e\u6d1e\u89c1\u3002", "conclusion": "eFLINT \u7684\u8bbe\u8ba1\u63a2\u7d22\u8868\u660e\uff0c\u5728\u81ea\u52a8\u5316\u5408\u89c4\u9886\u57df\uff0c\u8bed\u8a00\u9700\u517c\u987e\u6cd5\u5f8b\u4e0e\u8f6f\u4ef6\u5de5\u7a0b\u7684\u53cc\u91cd\u9700\u6c42\uff1b\u672c\u6587\u7684\u7ecf\u9a8c\u53ef\u4e3a\u672a\u6765\u5408\u89c4\u8bed\u8a00\u7684\u8bbe\u8ba1\u63d0\u4f9b\u6709\u76ca\u53c2\u8003\u3002"}}
{"id": "2511.12152", "categories": ["cs.AR", "eess.SP"], "pdf": "https://arxiv.org/pdf/2511.12152", "abs": "https://arxiv.org/abs/2511.12152", "authors": ["Jianyi Yu", "Yuxuan Wang", "Xiang Fu", "Fei Qiao", "Ying Wang", "Rui Yuan", "Liyuan Liu", "Cong Shi"], "title": "A digital SRAM-based compute-in-memory macro for weight-stationary dynamic matrix multiplication in Transformer attention score computation", "comment": null, "summary": "Compute-in-memory (CIM) techniques are widely employed in energy-efficient artificial intelligent (AI) processors. They alleviate power and latency bottlenecks caused by extensive data movements between compute and storage units. This work proposes a digital CIM macro to compute Transformer attention. To mitigate dynamic matrix multiplication that is unsuitable for the common weight-stationary CIM paradigm, we reformulate the attention score computation process based on a combined QK-weight matrix, so that inputs can be directly fed to CIM cells to obtain the score results. Moreover, the involved binomial matrix multiplication operation is decomposed into 4 groups of bit-serial shifting and additions, without costly physical multipliers in the CIM. We maximize the energy efficiency of the CIM circuit through zero-value bit-skipping, data-driven word line activation, read-write separate 6T cells and bit-alternating 14T/28T adders. The proposed CIM macro was implemented using a 65-nm process. It occupied only 0.35 mm2 area, and delivered a 42.27 GOPS peak performance with 1.24 mW power consumption at a 1.0 V power supply and a 100 MHz clock frequency, resulting in 34.1 TOPS/W energy efficiency and 120.77 GOPS/mm2 area efficiency. When compared to the CPU and GPU, our CIM macro is 25x and 13x more energy efficient on practical tasks, respectively. Compared with other Transformer-CIMs, our design exhibits at least 7x energy efficiency and at least 2x area efficiency improvements when scaled to the same technology node, showcasing its potential for edge-side intelligent applications.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u8ba1\u7b97Transformer\u6ce8\u610f\u529b\u673a\u5236\u7684\u6570\u5b57\u5b58\u5185\u8ba1\u7b97\uff08CIM\uff09\u5b8f\u5355\u5143\uff0c\u901a\u8fc7\u91cd\u6784\u6ce8\u610f\u529b\u5206\u6570\u8ba1\u7b97\u6d41\u7a0b\u5e76\u4f18\u5316\u7535\u8def\u8bbe\u8ba1\uff0c\u572865nm\u5de5\u827a\u4e0b\u5b9e\u73b0\u4e86\u9ad8\u80fd\u6548\u548c\u9ad8\u9762\u79ef\u6548\u7387\u3002", "motivation": "\u4f20\u7edfAI\u5904\u7406\u5668\u4e2d\u8ba1\u7b97\u4e0e\u5b58\u50a8\u5355\u5143\u95f4\u9891\u7e41\u7684\u6570\u636e\u642c\u8fd0\u5bfc\u81f4\u529f\u8017\u548c\u5ef6\u8fdf\u74f6\u9888\uff0c\u800c\u73b0\u6709CIM\u67b6\u6784\u96be\u4ee5\u9ad8\u6548\u652f\u6301Transformer\u4e2d\u7684\u52a8\u6001\u77e9\u9635\u4e58\u6cd5\u64cd\u4f5c\u3002", "method": "\u5c06\u6ce8\u610f\u529b\u5206\u6570\u8ba1\u7b97\u91cd\u6784\u4e3a\u57fa\u4e8e\u7ec4\u5408QK\u6743\u91cd\u77e9\u9635\u7684\u5f62\u5f0f\uff0c\u4f7f\u8f93\u5165\u53ef\u76f4\u63a5\u9001\u5165CIM\u5355\u5143\uff1b\u5c06\u4e8c\u9879\u5f0f\u77e9\u9635\u4e58\u6cd5\u5206\u89e3\u4e3a4\u7ec4\u6bd4\u7279\u4e32\u884c\u79fb\u4f4d\u4e0e\u52a0\u6cd5\u64cd\u4f5c\uff0c\u5e76\u91c7\u7528\u96f6\u503c\u6bd4\u7279\u8df3\u8fc7\u3001\u6570\u636e\u9a71\u52a8\u5b57\u7ebf\u6fc0\u6d3b\u3001\u8bfb\u5199\u5206\u79bb6T\u5355\u5143\u53ca\u6bd4\u7279\u4ea4\u66ff14T/28T\u52a0\u6cd5\u5668\u7b49\u6280\u672f\u63d0\u5347\u80fd\u6548\u3002", "result": "\u572865nm\u5de5\u827a\u4e0b\u5b9e\u73b00.35 mm\u00b2\u9762\u79ef\uff0c\u5cf0\u503c\u6027\u80fd\u8fbe42.27 GOPS\uff0c\u529f\u8017\u4ec51.24 mW\uff081.0 V, 100 MHz\uff09\uff0c\u80fd\u6548\u8fbe34.1 TOPS/W\uff0c\u9762\u79ef\u6548\u7387\u8fbe120.77 GOPS/mm\u00b2\uff1b\u76f8\u6bd4CPU/GPU\u5206\u522b\u63d0\u534725\u500d\u548c13\u500d\u80fd\u6548\uff0c\u76f8\u8f83\u5176\u4ed6Transformer-CIM\u65b9\u6848\u81f3\u5c11\u63d0\u53477\u500d\u80fd\u6548\u4e0e2\u500d\u9762\u79ef\u6548\u7387\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684CIM\u5b8f\u5355\u5143\u5728\u80fd\u6548\u548c\u9762\u79ef\u6548\u7387\u65b9\u9762\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6848\uff0c\u5c55\u73b0\u51fa\u5728\u8fb9\u7f18\u667a\u80fd\u5e94\u7528\u4e2d\u7684\u5de8\u5927\u6f5c\u529b\u3002"}}
{"id": "2511.13233", "categories": ["cs.MA"], "pdf": "https://arxiv.org/pdf/2511.13233", "abs": "https://arxiv.org/abs/2511.13233", "authors": ["Jun Sashihara", "Yukihisa Fujita", "Kota Nakamura", "Masahiro Kuwahara", "Teruaki Hayashi"], "title": "LLM-based Multi-Agent System for Simulating Strategic and Goal-Oriented Data Marketplaces", "comment": "10 pages, 12 figures", "summary": "Data marketplaces, which mediate the purchase and exchange of data from third parties, have attracted growing attention for reducing the cost and effort of data collection while enabling the trading of diverse datasets. However, a systematic understanding of the interactions between market participants, data, and regulations remains limited. To address this gap, we propose a Large Language Model-based Multi-Agent System (LLM-MAS) for data marketplaces. In our framework, buyer and seller agents powered by LLMs operate with explicit objectives and autonomously perform strategic actions, such as planning, searching, purchasing, pricing, and updating data. These agents can reason about market dynamics, forecast future demand, and adjust strategies accordingly. Unlike conventional model-based simulations, which are typically constrained to predefined rules, LLM-MAS supports broader and more adaptive behavior selection through natural language reasoning. We evaluated the framework via simulation experiments using three distribution-based metrics: (1) the number of purchases per dataset, (2) the number of purchases per buyer, and (3) the number of repeated purchases of the same dataset. The results demonstrate that LLM-MAS more faithfully reproduces trading patterns observed in real data marketplaces compared to traditional approaches, and further captures the emergence and evolution of market trends.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\uff08LLM-MAS\uff09\u7528\u4e8e\u6a21\u62df\u6570\u636e\u5e02\u573a\u4e2d\u7684\u4e70\u5356\u884c\u4e3a\uff0c\u901a\u8fc7\u8d4b\u4e88\u4e70\u5bb6\u548c\u5356\u5bb6\u667a\u80fd\u4f53\u81ea\u4e3b\u51b3\u7b56\u4e0e\u7b56\u7565\u8c03\u6574\u80fd\u529b\uff0c\u66f4\u771f\u5b9e\u5730\u518d\u73b0\u4e86\u5b9e\u9645\u6570\u636e\u5e02\u573a\u7684\u4ea4\u6613\u6a21\u5f0f\u548c\u8d8b\u52bf\u6f14\u5316\u3002", "motivation": "\u73b0\u6709\u5bf9\u6570\u636e\u5e02\u573a\u4e2d\u53c2\u4e0e\u8005\u3001\u6570\u636e\u4e0e\u76d1\u7ba1\u4e4b\u95f4\u4e92\u52a8\u5173\u7cfb\u7684\u7406\u89e3\u4ecd\u4e0d\u7cfb\u7edf\uff0c\u4f20\u7edf\u57fa\u4e8e\u89c4\u5219\u7684\u6a21\u62df\u65b9\u6cd5\u96be\u4ee5\u6355\u6349\u590d\u6742\u4e14\u52a8\u6001\u7684\u5e02\u573a\u884c\u4e3a\u3002", "method": "\u6784\u5efa\u4e00\u4e2a\u7531\u5927\u8bed\u8a00\u6a21\u578b\u9a71\u52a8\u7684\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\uff08LLM-MAS\uff09\uff0c\u5176\u4e2d\u4e70\u5bb6\u548c\u5356\u5bb6\u667a\u80fd\u4f53\u5177\u5907\u660e\u786e\u76ee\u6807\uff0c\u80fd\u81ea\u4e3b\u6267\u884c\u89c4\u5212\u3001\u641c\u7d22\u3001\u8d2d\u4e70\u3001\u5b9a\u4ef7\u548c\u66f4\u65b0\u6570\u636e\u7b49\u7b56\u7565\u6027\u884c\u4e3a\uff0c\u5e76\u901a\u8fc7\u81ea\u7136\u8bed\u8a00\u63a8\u7406\u9002\u5e94\u5e02\u573a\u53d8\u5316\u3002", "result": "\u901a\u8fc7\u4e09\u9879\u5206\u5e03\u6307\u6807\uff08\u6bcf\u6570\u636e\u96c6\u8d2d\u4e70\u6b21\u6570\u3001\u6bcf\u4f4d\u4e70\u5bb6\u8d2d\u4e70\u6b21\u6570\u3001\u540c\u4e00\u6570\u636e\u96c6\u91cd\u590d\u8d2d\u4e70\u6b21\u6570\uff09\u7684\u4eff\u771f\u5b9e\u9a8c\u8868\u660e\uff0cLLM-MAS\u76f8\u6bd4\u4f20\u7edf\u65b9\u6cd5\u66f4\u80fd\u771f\u5b9e\u590d\u73b0\u771f\u5b9e\u6570\u636e\u5e02\u573a\u7684\u4ea4\u6613\u6a21\u5f0f\uff0c\u5e76\u80fd\u6355\u6349\u5e02\u573a\u8d8b\u52bf\u7684\u6d8c\u73b0\u4e0e\u6f14\u5316\u3002", "conclusion": "LLM-MAS\u4e3a\u6570\u636e\u5e02\u573a\u7814\u7a76\u63d0\u4f9b\u4e86\u4e00\u4e2a\u66f4\u5177\u9002\u5e94\u6027\u548c\u8868\u73b0\u529b\u7684\u6a21\u62df\u6846\u67b6\uff0c\u6709\u52a9\u4e8e\u6df1\u5165\u7406\u89e3\u5e02\u573a\u52a8\u6001\u53ca\u5176\u6f14\u5316\u673a\u5236\u3002"}}
{"id": "2511.12288", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2511.12288", "abs": "https://arxiv.org/abs/2511.12288", "authors": ["Yihan Dai", "Sijie Liang", "Haotian Xu", "Peichu Xie", "Sergey Mechtaev"], "title": "Reducing Hallucinations in LLM-Generated Code via Semantic Triangulation", "comment": null, "summary": "When generating code from natural language prompts, an LLM samples programs from a probability distribution, many of which might be incorrect. Sample consensus techniques - such as majority voting or validation against generated tests or specifications - aim to identify a correct program in the sample or abstain if none is valid. However, existing methods often fail to select a correct solution when its sampling probability is low, or when the problem permits multiple valid but non-equivalent solutions. Additionally, they often fail to abstain when no correct solution is present in the sample. To overcome these limitations, we introduce semantic triangulation, which transforms a programming problem in a way that non-trivially alters its semantics while preserving an exact, verifiable mapping between solutions before and after transformation. We theoretically establish that verifying consistency across such problem transformations increases confidence that generated programs reflect accurate generalization rather than spurious statistical correlations, enabling more reliable sample consensus and abstention. On the LiveCodeBench and CodeElo benchmarks, using GPT-4o and DeepSeek-V3 models, semantic triangulation increases reliability of generated code by 21% compared to the method that selects only high-confidence solutions with the probability threshold 0.5, while being able to pinpoint correct solutions at sampling probabilities as low as 0.14. Apart from that, it is also the only approach to consistently form true consensus on tasks with multiple valid but non-equivalent solutions.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u8bed\u4e49\u4e09\u89d2\u6d4b\u91cf\u65b9\u6cd5\uff0c\u901a\u8fc7\u53d8\u6362\u7f16\u7a0b\u95ee\u9898\u7684\u8bed\u4e49\u5e76\u4fdd\u7559\u89e3\u4e4b\u95f4\u7684\u53ef\u9a8c\u8bc1\u6620\u5c04\uff0c\u63d0\u5347\u5927\u8bed\u8a00\u6a21\u578b\u751f\u6210\u4ee3\u7801\u7684\u53ef\u9760\u6027\uff0c\u5728\u4f4e\u91c7\u6837\u6982\u7387\u548c\u591a\u89e3\u573a\u666f\u4e0b\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u5171\u8bc6\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u91c7\u6837\u7684\u5171\u8bc6\u65b9\u6cd5\u5728\u6b63\u786e\u7a0b\u5e8f\u91c7\u6837\u6982\u7387\u4f4e\u3001\u5b58\u5728\u591a\u4e2a\u975e\u7b49\u4ef7\u6709\u6548\u89e3\u6216\u6837\u672c\u4e2d\u65e0\u6b63\u786e\u89e3\u65f6\u8868\u73b0\u4e0d\u4f73\uff0c\u96be\u4ee5\u53ef\u9760\u9009\u62e9\u6b63\u786e\u7a0b\u5e8f\u6216\u9002\u65f6\u5f03\u6743\u3002", "method": "\u5f15\u5165\u8bed\u4e49\u4e09\u89d2\u6d4b\u91cf\uff1a\u5bf9\u7f16\u7a0b\u95ee\u9898\u8fdb\u884c\u975e\u5e73\u51e1\u8bed\u4e49\u53d8\u6362\uff0c\u540c\u65f6\u4fdd\u6301\u53d8\u6362\u524d\u540e\u89e3\u4e4b\u95f4\u7cbe\u786e\u4e14\u53ef\u9a8c\u8bc1\u7684\u6620\u5c04\u5173\u7cfb\uff0c\u5e76\u901a\u8fc7\u9a8c\u8bc1\u4e0d\u540c\u53d8\u6362\u4e0b\u751f\u6210\u7a0b\u5e8f\u7684\u4e00\u81f4\u6027\u6765\u5224\u65ad\u5176\u6cdb\u5316\u80fd\u529b\u3002", "result": "\u5728LiveCodeBench\u548cCodeElo\u57fa\u51c6\u4e0a\uff0c\u4f7f\u7528GPT-4o\u548cDeepSeek-V3\u6a21\u578b\uff0c\u8be5\u65b9\u6cd5\u76f8\u6bd4\u4ec5\u9009\u62e9\u7f6e\u4fe1\u5ea6\u9ad8\u4e8e0.5\u7684\u65b9\u6848\uff0c\u4ee3\u7801\u53ef\u9760\u6027\u63d0\u534721%\uff0c\u53ef\u5728\u91c7\u6837\u6982\u7387\u4f4e\u81f30.14\u65f6\u8bc6\u522b\u6b63\u786e\u89e3\uff0c\u5e76\u80fd\u6709\u6548\u5904\u7406\u591a\u6709\u6548\u89e3\u4efb\u52a1\u3002", "conclusion": "\u8bed\u4e49\u4e09\u89d2\u6d4b\u91cf\u80fd\u663e\u8457\u63d0\u5347\u6837\u672c\u5171\u8bc6\u7684\u53ef\u9760\u6027\u4e0e\u5f03\u6743\u80fd\u529b\uff0c\u5c24\u5176\u9002\u7528\u4e8e\u4f4e\u6982\u7387\u6b63\u786e\u89e3\u548c\u591a\u89e3\u573a\u666f\uff0c\u4e3a\u4ee3\u7801\u751f\u6210\u63d0\u4f9b\u66f4\u7a33\u5065\u7684\u9a8c\u8bc1\u673a\u5236\u3002"}}
{"id": "2511.11605", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2511.11605", "abs": "https://arxiv.org/abs/2511.11605", "authors": ["David Balaban", "Adrian Micl\u0103u\u015f"], "title": "PACE Solver Description: twin_width_fmi", "comment": null, "summary": "In this paper we present \\texttt{twin\\_width\\_fmi}'s solver for the heuristic track of PACE's 2025 competition on Minimum Dominating Set.\n  As a baseline, we implement \\texttt{greedy-ln}, a standard greedy dominating-set heuristic that repeatedly selects the vertex that newly dominates the largest number of currently undominated vertices. We then use this greedy solution as the starting point for a simulated annealing local search: we attempt vertex removals and exchanges and accept worsening moves with decaying probability, in order to escape local minima while preserving domination.\n  Our best-performing component, which we ultimately submitted, is \\texttt{hedom5}. The design of \\texttt{hedom5} is inspired by recent iterative-greedy style domination heuristics~\\cite{IterativeGreedy22} that alternate between constructive steps, pruning, and focused repair rather than relying on a single pass. In \\texttt{hedom5}, the input graph is first stored in a compact CSR structure and simplified using fast reductions such as forcing neighbors of leaves and handling isolates. We then run a lazy gain-based greedy stage using a priority queue: each candidate vertex is scored by how many currently undominated vertices its closed neighborhood would newly dominate, and scores are only recomputed when necessary. After this constructive phase, we perform an aggressive backward pruning pass that iterates over the chosen dominators in reverse insertion order and deletes any vertex whose closed neighborhood is still fully dominated by the remaining set. Finally, we run a budgeted 1-swap local improvement step that attempts to replace a dominator by an alternative vertex that covers all of its uniquely covered vertices, thereby reducing the size of the dominating set. A brief safety patch at the end guarantees full domination.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u7528\u4e8ePACE 2025\u6700\u5c0f\u652f\u914d\u96c6\u7ade\u8d5b\u542f\u53d1\u5f0f\u8d5b\u9053\u7684\u6c42\u89e3\u5668twin_width_fmi\uff0c\u5176\u6838\u5fc3\u7ec4\u4ef6hedom5\u7ed3\u5408\u4e86\u8d2a\u5fc3\u6784\u9020\u3001\u526a\u679d\u548c\u5c40\u90e8\u4ea4\u6362\u4f18\u5316\u7b56\u7565\uff0c\u5728\u4fdd\u8bc1\u5b8c\u5168\u652f\u914d\u7684\u524d\u63d0\u4e0b\u6709\u6548\u51cf\u5c0f\u652f\u914d\u96c6\u89c4\u6a21\u3002", "motivation": "\u4e3a\u5e94\u5bf9PACE 2025\u7ade\u8d5b\u4e2d\u6700\u5c0f\u652f\u914d\u96c6\u95ee\u9898\u7684\u6311\u6218\uff0c\u9700\u8bbe\u8ba1\u9ad8\u6548\u542f\u53d1\u5f0f\u7b97\u6cd5\u4ee5\u5728\u5408\u7406\u65f6\u95f4\u5185\u83b7\u5f97\u9ad8\u8d28\u91cf\u8fd1\u4f3c\u89e3\u3002", "method": "\u7b97\u6cd5hedom5\u9996\u5148\u5bf9\u56fe\u8fdb\u884c\u9884\u5904\u7406\uff08\u5982\u53f6\u8282\u70b9\u90bb\u5c45\u5f3a\u5236\u3001\u5b64\u7acb\u70b9\u5904\u7406\uff09\uff0c\u7136\u540e\u91c7\u7528\u57fa\u4e8e\u6536\u76ca\u7684\u61d2\u66f4\u65b0\u8d2a\u5fc3\u7b56\u7565\u6784\u5efa\u521d\u59cb\u652f\u914d\u96c6\uff1b\u63a5\u7740\u901a\u8fc7\u9006\u5e8f\u526a\u679d\u79fb\u9664\u5197\u4f59\u9876\u70b9\uff0c\u5e76\u6267\u884c\u9884\u7b97\u9650\u5236\u4e0b\u76841-\u4ea4\u6362\u5c40\u90e8\u641c\u7d22\u4ee5\u8fdb\u4e00\u6b65\u7f29\u5c0f\u89e3\u89c4\u6a21\uff0c\u6700\u540e\u52a0\u5165\u5b89\u5168\u8865\u4e01\u786e\u4fdd\u652f\u914d\u5b8c\u6574\u6027\u3002", "result": "\u6240\u63d0\u4ea4\u7684hedom5\u65b9\u6cd5\u5728\u7ade\u8d5b\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u80fd\u6709\u6548\u751f\u6210\u8f83\u5c0f\u89c4\u6a21\u7684\u652f\u914d\u96c6\u3002", "conclusion": "\u7ed3\u5408\u8fed\u4ee3\u8d2a\u5fc3\u3001\u526a\u679d\u4e0e\u5c40\u90e8\u641c\u7d22\u7684\u6df7\u5408\u7b56\u7565\u5728\u6700\u5c0f\u652f\u914d\u96c6\u95ee\u9898\u4e0a\u5177\u6709\u663e\u8457\u4f18\u52bf\uff0c\u4e3a\u540e\u7eed\u7814\u7a76\u63d0\u4f9b\u4e86\u6709\u6548\u6846\u67b6\u3002"}}
{"id": "2511.12286", "categories": ["cs.AR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.12286", "abs": "https://arxiv.org/abs/2511.12286", "authors": ["Khyati Kiyawat", "Zhenxing Fan", "Yasas Seneviratne", "Morteza Baradaran", "Akhil Shekar", "Zihan Xia", "Mingu Kang", "Kevin Skadron"], "title": "Sangam: Chiplet-Based DRAM-PIM Accelerator with CXL Integration for LLM Inferencing", "comment": null, "summary": "Large Language Models (LLMs) are becoming increasingly data-intensive due to growing model sizes, and they are becoming memory-bound as the context length and, consequently, the key-value (KV) cache size increase. Inference, particularly the decoding phase, is dominated by memory-bound GEMV or flat GEMM operations with low operational intensity (OI), making it well-suited for processing-in-memory (PIM) approaches. However, existing in/near-memory solutions face critical limitations such as reduced memory capacity due to the high area cost of integrating processing elements (PEs) within DRAM chips, and limited PE capability due to the constraints of DRAM fabrication technology. This work presents a chiplet-based memory module that addresses these limitations by decoupling logic and memory into chiplets fabricated in heterogeneous technology nodes and connected via an interposer. The logic chiplets sustain high bandwidth access to the DRAM chiplets, which house the memory banks, and enable the integration of advanced processing components such as systolic arrays and SRAM-based buffers to accelerate memory-bound GEMM kernels, capabilities that were not feasible in prior PIM architectures. We propose Sangam, a CXL-attached PIM-chiplet based memory module that can either act as a drop-in replacement for GPUs or co-executes along side the GPUs. Sangam achieves speedup of 3.93, 4.22, 2.82x speedup in end-to-end query latency, 10.3, 9.5, 6.36x greater decoding throughput, and order of magnitude energy savings compared to an H100 GPU for varying input size, output length, and batch size on LLaMA 2-7B, Mistral-7B, and LLaMA 3-70B, respectively.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8echiplet\u7684\u5185\u5b58\u6a21\u5757Sangam\uff0c\u901a\u8fc7\u5f02\u6784\u96c6\u6210\u903b\u8f91\u4e0e\u5b58\u50a8\u82af\u7247\uff0c\u5229\u7528CXL\u63a5\u53e3\u5b9e\u73b0\u5bf9\u5927\u8bed\u8a00\u6a21\u578b\u63a8\u7406\u4e2d\u5185\u5b58\u5bc6\u96c6\u578b\u64cd\u4f5c\u7684\u9ad8\u6548\u52a0\u901f\uff0c\u5728\u5ef6\u8fdf\u3001\u541e\u5410\u91cf\u548c\u80fd\u6548\u65b9\u9762\u663e\u8457\u4f18\u4e8eH100 GPU\u3002", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b\u63a8\u7406\u53d7\u5185\u5b58\u5e26\u5bbd\u9650\u5236\uff0c\u73b0\u6709\u5b58\u5185\u8ba1\u7b97\uff08PIM\uff09\u65b9\u6848\u53d7\u9650\u4e8eDRAM\u5185\u96c6\u6210\u5904\u7406\u5355\u5143\u5e26\u6765\u7684\u5bb9\u91cf\u51cf\u5c11\u548c\u8ba1\u7b97\u80fd\u529b\u4e0d\u8db3\u3002", "method": "\u8bbe\u8ba1\u4e00\u79cdchiplet\u67b6\u6784\u7684\u5185\u5b58\u6a21\u5757Sangam\uff0c\u5c06\u903b\u8f91\u4e0e\u5b58\u50a8\u5206\u79bb\u5e76\u91c7\u7528\u5f02\u6784\u5de5\u827a\u5236\u9020\uff0c\u901a\u8fc7\u4e2d\u4ecb\u5c42\u4e92\u8fde\uff1b\u903b\u8f91chiplet\u5305\u542b\u8109\u52a8\u9635\u5217\u548cSRAM\u7f13\u5b58\uff0c\u7528\u4e8e\u52a0\u901fGEMM\u7c7b\u5185\u5b58\u5bc6\u96c6\u578b\u64cd\u4f5c\uff0c\u5e76\u901a\u8fc7CXL\u63a5\u53e3\u8fde\u63a5\u4e3b\u673a\uff0c\u53ef\u66ff\u4ee3\u6216\u534f\u540cGPU\u5de5\u4f5c\u3002", "result": "\u5728LLaMA 2-7B\u3001Mistral-7B\u548cLLaMA 3-70B\u6a21\u578b\u4e0a\uff0c\u76f8\u6bd4H100 GPU\uff0cSangam\u5206\u522b\u5b9e\u73b0\u4e86\u6700\u9ad83.93\u500d\u67e5\u8be2\u5ef6\u8fdf\u964d\u4f4e\u300110.3\u500d\u89e3\u7801\u541e\u5410\u63d0\u5347\u53ca\u6570\u91cf\u7ea7\u7ea7\u522b\u7684\u80fd\u6548\u6539\u8fdb\u3002", "conclusion": "\u901a\u8fc7chiplet\u4e0eCXL\u6280\u672f\u7ed3\u5408\u7684PIM\u67b6\u6784\u80fd\u6709\u6548\u7a81\u7834\u4f20\u7edf\u5b58\u5185\u8ba1\u7b97\u7684\u5c40\u9650\u6027\uff0c\u4e3a\u5927\u8bed\u8a00\u6a21\u578b\u63a8\u7406\u63d0\u4f9b\u9ad8\u80fd\u6548\u3001\u9ad8\u6027\u80fd\u7684\u786c\u4ef6\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2511.13445", "categories": ["cs.MA"], "pdf": "https://arxiv.org/pdf/2511.13445", "abs": "https://arxiv.org/abs/2511.13445", "authors": ["Clemens Anzinger", "Jiehua Chen", "Christian Hatschka", "Manuel Sorge", "Alexander Temper"], "title": "How Hard is it to Explain Preferences Using Few Boolean Attributes?", "comment": "Accepted at AAAI 2026", "summary": "We study the computational complexity of explaining preference data through Boolean attribute models (BAMs), motivated by extensive research involving attribute models and their promise in understanding preference structure and enabling more efficient decision-making processes. In a BAM, each alternative has a subset of Boolean attributes, each voter cares about a subset of attributes, and voters prefer alternatives with more of their desired attributes. In the BAM problem, we are given a preference profile and a number k, and want to know whether there is a Boolean k-attribute model explaining the profile.\n  We establish a complexity dichotomy for the number of attributes k: BAM is linear-time solvable for $k \\le 2$ but NP-complete for $k \\ge 3$. The problem remains hard even when preference orders have length two. On the positive side, BAM becomes fixed-parameter tractable when parameterized by the number of alternatives m. For the special case of two voters, we provide a linear-time algorithm.\n  We also analyze variants where partial information is given: When voter preferences over attributes are known (BAM WITH CARES) or when alternative attributes are specified (BAM WITH HAS), we show that for most parameters BAM WITH CARES is more difficult whereas BAM WITH HAS is more tractable except for being NP-hard even for one voter.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u7528\u5e03\u5c14\u5c5e\u6027\u6a21\u578b\uff08BAM\uff09\u89e3\u91ca\u504f\u597d\u6570\u636e\u7684\u8ba1\u7b97\u590d\u6742\u6027\uff0c\u8bc1\u660e\u5f53\u5c5e\u6027\u6570k\u22642\u65f6\u95ee\u9898\u53ef\u5728\u7ebf\u6027\u65f6\u95f4\u5185\u6c42\u89e3\uff0c\u800ck\u22653\u65f6\u4e3aNP\u5b8c\u5168\uff1b\u540c\u65f6\u5206\u6790\u4e86\u90e8\u5206\u4fe1\u606f\u5df2\u77e5\u60c5\u5f62\u4e0b\u7684\u4e24\u4e2a\u53d8\u4f53\u95ee\u9898\u7684\u590d\u6742\u6027\u3002", "motivation": "\u5c5e\u6027\u6a21\u578b\u5728\u7406\u89e3\u504f\u597d\u7ed3\u6784\u548c\u63d0\u5347\u51b3\u7b56\u6548\u7387\u65b9\u9762\u5177\u6709\u6f5c\u529b\uff0c\u56e0\u6b64\u6709\u5fc5\u8981\u7814\u7a76\u7528\u5e03\u5c14\u5c5e\u6027\u6a21\u578b\u89e3\u91ca\u7ed9\u5b9a\u504f\u597d\u6570\u636e\u7684\u8ba1\u7b97\u590d\u6742\u6027\u3002", "method": "\u901a\u8fc7\u590d\u6742\u6027\u5206\u6790\uff0c\u5bf9\u4e0d\u540c\u53c2\u6570\uff08\u5982\u5c5e\u6027\u6570\u91cfk\u3001\u5907\u9009\u9879\u6570\u91cfm\u3001\u6295\u7968\u8005\u6570\u91cf\u7b49\uff09\u4e0b\u7684BAM\u95ee\u9898\u53ca\u5176\u53d8\u4f53\uff08BAM WITH CARES\u548cBAM WITH HAS\uff09\u8fdb\u884c\u5206\u7c7b\uff0c\u91c7\u7528\u5f52\u7ea6\u548c\u7b97\u6cd5\u8bbe\u8ba1\u65b9\u6cd5\u3002", "result": "\u5efa\u7acb\u4e86\u5173\u4e8e\u5c5e\u6027\u6570k\u7684\u590d\u6742\u6027\u4e8c\u5206\u6cd5\uff1ak\u22642\u65f6\u7ebf\u6027\u65f6\u95f4\u53ef\u89e3\uff0ck\u22653\u65f6NP\u5b8c\u5168\uff1b\u5373\u4f7f\u504f\u597d\u5e8f\u957f\u5ea6\u4e3a2\u4ecd\u4e3aNP\u96be\uff1b\u4ee5\u5907\u9009\u9879\u6570m\u4e3a\u53c2\u6570\u65f6\u56fa\u5b9a\u53c2\u6570\u53ef\u89e3\uff1b\u4e24\u4eba\u60c5\u5f62\u5b58\u5728\u7ebf\u6027\u65f6\u95f4\u7b97\u6cd5\uff1b\u53d8\u4f53\u95ee\u9898\u4e2d\uff0cBAM WITH CARES\u901a\u5e38\u66f4\u96be\uff0c\u800cBAM WITH HAS\u9664\u5355\u6295\u7968\u8005\u60c5\u5f62\u5916\u66f4\u6613\u5904\u7406\u3002", "conclusion": "\u5e03\u5c14\u5c5e\u6027\u6a21\u578b\u89e3\u91ca\u504f\u597d\u6570\u636e\u7684\u95ee\u9898\u5728\u4e00\u822c\u60c5\u51b5\u4e0b\u8ba1\u7b97\u56f0\u96be\uff0c\u4f46\u5728\u7279\u5b9a\u53c2\u6570\u6216\u9650\u5236\u6761\u4ef6\u4e0b\u5177\u5907\u9ad8\u6548\u7b97\u6cd5\uff0c\u4e3a\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u4e86\u7406\u8bba\u4f9d\u636e\u3002"}}
{"id": "2511.12349", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2511.12349", "abs": "https://arxiv.org/abs/2511.12349", "authors": ["Divya Kiran Kadiyala", "Alexandros Daglis"], "title": "Pushing the Memory Bandwidth Wall with CXL-enabled Idle I/O Bandwidth Harvesting", "comment": null, "summary": "The continual increase of cores on server-grade CPUs raises demands on memory systems, which are constrained by limited off-chip pin and data transfer rate scalability. As a result, high-end processors typically feature lower memory bandwidth per core, at the detriment of memory-intensive workloads. We propose alleviating this challenge by improving the utility of the CPU's limited pins. In a typical CPU design process, the available pins are apportioned between memory and I/O traffic, each accounting for about half of the total off-chip bandwidth availability. Consequently, unless both memory and I/O are simultaneously highly utilized, such fragmentation leads to underutilization of the valuable off-chip bandwidth resources. An ideal architecture would offer I/O and memory bandwidth fungibility, allowing use of the aggregate off-chip bandwidth in the form required by each workload.\n  In this work, we introduce SURGE, a software-supported architectural technique that boosts memory bandwidth availability by salvaging idle I/O bandwidth resources. SURGE leverages the capability of versatile interconnect technologies like CXL to dynamically multiplex memory and I/O traffic over the same processor interface. We demonstrate that SURGE-enhanced architectures can accelerate memory-intensive workloads on bandwidth-constrained servers by up to 1.3x.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aSURGE\u7684\u8f6f\u786c\u4ef6\u534f\u540c\u67b6\u6784\u6280\u672f\uff0c\u901a\u8fc7\u52a8\u6001\u590d\u7528\u95f2\u7f6e\u7684I/O\u5e26\u5bbd\u6765\u63d0\u5347\u5185\u5b58\u5e26\u5bbd\uff0c\u4ece\u800c\u7f13\u89e3\u591a\u6838\u670d\u52a1\u5668\u4e2d\u6bcf\u6838\u5185\u5b58\u5e26\u5bbd\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u5728\u5e26\u5bbd\u53d7\u9650\u7684\u670d\u52a1\u5668\u4e0a\u53ef\u5c06\u5185\u5b58\u5bc6\u96c6\u578b\u5de5\u4f5c\u8d1f\u8f7d\u52a0\u901f\u6700\u591a1.3\u500d\u3002", "motivation": "\u968f\u7740\u670d\u52a1\u5668\u7ea7CPU\u6838\u5fc3\u6570\u91cf\u6301\u7eed\u589e\u52a0\uff0c\u5185\u5b58\u7cfb\u7edf\u9762\u4e34\u5f15\u811a\u6570\u91cf\u548c\u6570\u636e\u4f20\u8f93\u901f\u7387\u6269\u5c55\u6027\u6709\u9650\u7684\u74f6\u9888\uff0c\u5bfc\u81f4\u9ad8\u7aef\u5904\u7406\u5668\u6bcf\u6838\u53ef\u7528\u5185\u5b58\u5e26\u5bbd\u4e0b\u964d\uff0c\u5f71\u54cd\u5185\u5b58\u5bc6\u96c6\u578b\u5e94\u7528\u6027\u80fd\u3002\u4f20\u7edf\u8bbe\u8ba1\u5c06\u5f15\u811a\u56fa\u5b9a\u5206\u914d\u7ed9\u5185\u5b58\u548cI/O\uff0c\u9020\u6210\u5e26\u5bbd\u8d44\u6e90\u788e\u7247\u5316\u548c\u5229\u7528\u7387\u4f4e\u4e0b\u3002", "method": "\u63d0\u51faSURGE\u67b6\u6784\uff0c\u5229\u7528CXL\u7b49\u591a\u529f\u80fd\u4e92\u8fde\u6280\u672f\uff0c\u5728\u8f6f\u4ef6\u652f\u6301\u4e0b\u52a8\u6001\u590d\u7528\u5185\u5b58\u4e0eI/O\u6d41\u91cf\u5171\u7528\u540c\u4e00\u5904\u7406\u5668\u63a5\u53e3\uff0c\u5b9e\u73b0\u5185\u5b58\u4e0eI/O\u5e26\u5bbd\u7684\u7075\u6d3b\u8c03\u914d\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u91c7\u7528SURGE\u7684\u67b6\u6784\u5728\u5e26\u5bbd\u53d7\u9650\u7684\u670d\u52a1\u5668\u4e0a\u53ef\u5c06\u5185\u5b58\u5bc6\u96c6\u578b\u5de5\u4f5c\u8d1f\u8f7d\u6027\u80fd\u6700\u9ad8\u63d0\u53471.3\u500d\u3002", "conclusion": "\u901a\u8fc7\u5b9e\u73b0\u5185\u5b58\u4e0eI/O\u5e26\u5bbd\u7684\u53ef\u4e92\u6362\u6027\uff0cSURGE\u6709\u6548\u63d0\u5347\u4e86\u670d\u52a1\u5668\u5185\u5b58\u7cfb\u7edf\u7684\u6574\u4f53\u5e26\u5bbd\u5229\u7528\u7387\uff0c\u4e3a\u672a\u6765\u591a\u6838\u5904\u7406\u5668\u7684\u5185\u5b58\u74f6\u9888\u95ee\u9898\u63d0\u4f9b\u4e86\u53ef\u884c\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2511.11621", "categories": ["cs.DC", "cs.AI", "cs.NI"], "pdf": "https://arxiv.org/pdf/2511.11621", "abs": "https://arxiv.org/abs/2511.11621", "authors": ["Pedro Antunes", "Ana Rita Ortigoso", "Gabriel Vieira", "Daniel Fuentes", "Lu\u00eds Fraz\u00e3o", "Nuno Costa", "Ant\u00f3nio Pereira"], "title": "AIvailable: A Software-Defined Architecture for LLM-as-a-Service on Heterogeneous and Legacy GPUs", "comment": null, "summary": "The rise of Large Language Models (LLM) has increased the need for scalable, high-performance inference systems, yet most existing frameworks assume homogeneous, resource-rich hardware, often unrealistic in academic, or resource-constrained settings. We introduce AIvailable, a low-cost, highly available LLM-as-a-Service (LLMaaS) platform, that uses a software-defined approach for running LLMs across heterogeneous and legacy GPU nodes, including NVIDIA and AMD devices, with a focus on fully utilizing each node's VRAM. AIvailable operates as a fully GPU-accelerated inference without CPU fallbacks, featuring a unified client interface that allows seamless interaction with all deployed LLMs through a single logical unit. The architecture comprises four main components: the Client Interface for user access, the Service Frontend for secure request routing and load balancing, the SDAI Controller for orchestration, deployment, and monitoring, and the Service Backend of heterogeneous GPU nodes executing workloads. By abstracting GPU-specific details and providing dynamic, VRAM-aware allocation and reallocation of models, AIvailable ensures efficient use of resources and resilience against failures or workload fluctuations. Targeting academic labs, private companies, and other constrained organizations, it supports diverse open LLMs helping democratize generative AI through the repurposing of legacy GPUs.", "AI": {"tldr": "AIvailable is a low-cost, highly available LLM-as-a-Service platform that enables efficient inference of large language models across heterogeneous and legacy GPU hardware (including NVIDIA and AMD) by maximizing VRAM utilization and providing a unified client interface.", "motivation": "Existing LLM inference frameworks assume homogeneous, resource-rich environments, which are impractical for academic or resource-constrained settings; there is a need for scalable solutions that can leverage heterogeneous and legacy GPU infrastructure.", "method": "AIvailable uses a software-defined architecture with four components\u2014Client Interface, Service Frontend, SDAI Controller, and Service Backend\u2014to orchestrate LLM inference across diverse GPU nodes without CPU fallbacks, employing VRAM-aware dynamic model allocation and abstraction of GPU-specific details.", "result": "The platform enables fully GPU-accelerated, resilient LLM inference on mixed legacy hardware, efficiently utilizing VRAM and supporting seamless deployment and interaction with diverse open-source LLMs.", "conclusion": "AIvailable democratizes access to generative AI by making LLM inference feasible and cost-effective in resource-constrained environments through intelligent use of heterogeneous and legacy GPU resources."}}
{"id": "2511.12543", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2511.12543", "abs": "https://arxiv.org/abs/2511.12543", "authors": ["Burak Karaduman", "Baris Tekin Tezel", "Moharram Challenger"], "title": "High-level reasoning while low-level actuation in Cyber-Physical Systems: How efficient is it?", "comment": null, "summary": "The increasing complexity of industrial information-integration systems demands software technologies that enable intelligent behaviour, real-time response, and efficient development. Although many programming languages and frameworks exist, engineers still lack sufficient empirical evidence to guide the choice of tools for advanced industrial applications. This study addresses that need by measuring and comparing worst-case execution time (WCET) and development time across six languages and frameworks: C++, Java, Jade, Jason, and fuzzy Jason BDI with both loosely and tightly coupled integration. These technologies reflect a progression from procedural and object-oriented programming to agent-based frameworks capable of symbolic and fuzzy reasoning.\n  Rather than relying on broad concepts such as paradigms or orientations, the study adopts a developer-centred approach grounded in measurable outcomes. The structured comparison examines how rising abstraction levels and reasoning capabilities affect both development effort and runtime behaviour. By analysing these dimensions, the study highlights concrete trade-offs between engineering workload and execution efficiency.\n  The findings show how abstraction and reasoning mechanisms shape system performance and developer productivity, offering practical insight for designing intelligent, agent-based solutions that must operate under real-time constraints and complex decision-making requirements. Overall, the study contributes evidence-based guidance for selecting software technologies in industrial informatization, supporting improved integration efficiency, maintainability, and responsiveness, and laying groundwork for future research on the interplay between language features, development dynamics, and runtime behaviour in cyber-physical and smart manufacturing systems.", "AI": {"tldr": "\u8be5\u7814\u7a76\u901a\u8fc7\u5b9e\u8bc1\u6bd4\u8f83\u516d\u79cd\u7f16\u7a0b\u8bed\u8a00\u548c\u6846\u67b6\uff08C++\u3001Java\u3001Jade\u3001Jason\u3001\u677e\u8026\u5408\u4e0e\u7d27\u8026\u5408\u7684\u6a21\u7ccaJason BDI\uff09\u5728\u6700\u574f\u60c5\u51b5\u6267\u884c\u65f6\u95f4\uff08WCET\uff09\u548c\u5f00\u53d1\u65f6\u95f4\u4e0a\u7684\u8868\u73b0\uff0c\u63ed\u793a\u4e86\u62bd\u8c61\u5c42\u6b21\u4e0e\u63a8\u7406\u80fd\u529b\u5bf9\u5f00\u53d1\u6548\u7387\u548c\u8fd0\u884c\u65f6\u6027\u80fd\u7684\u5177\u4f53\u6743\u8861\uff0c\u4e3a\u5de5\u4e1a\u667a\u80fd\u7cfb\u7edf\u7684\u6280\u672f\u9009\u578b\u63d0\u4f9b\u4f9d\u636e\u3002", "motivation": "\u5de5\u4e1a\u4fe1\u606f\u96c6\u6210\u7cfb\u7edf\u65e5\u76ca\u590d\u6742\uff0c\u4e9f\u9700\u652f\u6301\u667a\u80fd\u884c\u4e3a\u3001\u5b9e\u65f6\u54cd\u5e94\u548c\u9ad8\u6548\u5f00\u53d1\u7684\u8f6f\u4ef6\u6280\u672f\uff0c\u4f46\u5de5\u7a0b\u5e08\u7f3a\u4e4f\u5173\u4e8e\u5148\u8fdb\u5de5\u4e1a\u5e94\u7528\u4e2d\u5de5\u5177\u9009\u62e9\u7684\u5b9e\u8bc1\u4f9d\u636e\u3002", "method": "\u91c7\u7528\u4ee5\u5f00\u53d1\u8005\u4e3a\u4e2d\u5fc3\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u6d4b\u91cf\u548c\u5bf9\u6bd4\u516d\u79cd\u8bed\u8a00/\u6846\u67b6\u5728WCET\u548c\u5f00\u53d1\u65f6\u95f4\u4e24\u4e2a\u7ef4\u5ea6\u7684\u8868\u73b0\uff0c\u5206\u6790\u4e0d\u540c\u62bd\u8c61\u5c42\u6b21\u548c\u63a8\u7406\u673a\u5236\u5bf9\u7cfb\u7edf\u6027\u80fd\u4e0e\u5f00\u53d1\u5de5\u4f5c\u91cf\u7684\u5f71\u54cd\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff0c\u968f\u7740\u62bd\u8c61\u7a0b\u5ea6\u548c\u63a8\u7406\u80fd\u529b\u7684\u63d0\u5347\uff0c\u5f00\u53d1\u6548\u7387\u63d0\u9ad8\u4f46\u8fd0\u884c\u65f6\u6027\u80fd\uff08\u5982WCET\uff09\u53ef\u80fd\u4e0b\u964d\uff0c\u660e\u786e\u4e86\u5de5\u7a0b\u5de5\u4f5c\u91cf\u4e0e\u6267\u884c\u6548\u7387\u4e4b\u95f4\u7684\u5177\u4f53\u6743\u8861\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u5de5\u4e1a\u4fe1\u606f\u5316\u4e2d\u8f6f\u4ef6\u6280\u672f\u7684\u9009\u62e9\u63d0\u4f9b\u4e86\u57fa\u4e8e\u8bc1\u636e\u7684\u6307\u5bfc\uff0c\u6709\u52a9\u4e8e\u63d0\u5347\u7cfb\u7edf\u96c6\u6210\u7684\u6548\u7387\u3001\u53ef\u7ef4\u62a4\u6027\u548c\u54cd\u5e94\u80fd\u529b\uff0c\u5e76\u4e3a\u672a\u6765\u5728\u4fe1\u606f\u7269\u7406\u7cfb\u7edf\u548c\u667a\u80fd\u5236\u9020\u4e2d\u8bed\u8a00\u7279\u6027\u4e0e\u5f00\u53d1\u53ca\u8fd0\u884c\u884c\u4e3a\u4e4b\u95f4\u5173\u7cfb\u7684\u7814\u7a76\u5960\u5b9a\u57fa\u7840\u3002"}}
{"id": "2511.11612", "categories": ["cs.DC", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.11612", "abs": "https://arxiv.org/abs/2511.11612", "authors": ["Aasish Kumar Sharma", "Julian Kunkel"], "title": "Evaluating Large Language Models for Workload Mapping and Scheduling in Heterogeneous HPC Systems", "comment": "14 pages, 4 figures, 2 tables. Evaluation study on LLM-based reasoning for HPC scheduling. Published in Research in Academic Engineering Journal (RAEJ), 2025", "summary": "Large language models (LLMs) are increasingly explored for their reasoning capabilities, yet their ability to perform structured, constraint-based optimization from natural language remains insufficiently understood. This study evaluates twenty-one publicly available LLMs on a representative heterogeneous high-performance computing (HPC) workload mapping and scheduling problem. Each model received the same textual description of system nodes, task requirements, and scheduling constraints, and was required to assign tasks to nodes, compute the total makespan, and explain its reasoning. A manually derived analytical optimum of nine hours and twenty seconds served as the ground truth reference. Three models exactly reproduced the analytical optimum while satisfying all constraints, twelve achieved near-optimal results within two minutes of the reference, and six produced suboptimal schedules with arithmetic or dependency errors. All models generated feasible task-to-node mappings, though only about half maintained strict constraint adherence. Nineteen models produced partially executable verification code, and eighteen provided coherent step-by-step reasoning, demonstrating strong interpretability even when logical errors occurred. Overall, the results define the current capability boundary of LLM reasoning in combinatorial optimization: leading models can reconstruct optimal schedules directly from natural language, but most still struggle with precise timing, data transfer arithmetic, and dependency enforcement. These findings highlight the potential of LLMs as explainable co-pilots for optimization and decision-support tasks rather than autonomous solvers.", "AI": {"tldr": "\u8be5\u7814\u7a76\u8bc4\u4f30\u4e8621\u4e2a\u5f00\u6e90\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u81ea\u7136\u8bed\u8a00\u63cf\u8ff0\u7684\u9ad8\u6027\u80fd\u8ba1\u7b97\u4efb\u52a1\u8c03\u5ea6\u95ee\u9898\u4e0a\u7684\u8868\u73b0\uff0c\u53d1\u73b0\u5c11\u6570\u6a21\u578b\u80fd\u7cbe\u786e\u590d\u73b0\u6700\u4f18\u89e3\uff0c\u591a\u6570\u63a5\u8fd1\u6700\u4f18\u4f46\u5b58\u5728\u7b97\u672f\u6216\u4f9d\u8d56\u9519\u8bef\uff0c\u8868\u660eLLMs\u66f4\u9002\u5408\u4f5c\u4e3a\u53ef\u89e3\u91ca\u7684\u8f85\u52a9\u51b3\u7b56\u5de5\u5177\u800c\u975e\u81ea\u4e3b\u6c42\u89e3\u5668\u3002", "motivation": "\u63a2\u7d22\u5927\u8bed\u8a00\u6a21\u578b\u662f\u5426\u80fd\u591f\u4ece\u81ea\u7136\u8bed\u8a00\u4e2d\u6267\u884c\u7ed3\u6784\u5316\u7684\u7ea6\u675f\u4f18\u5316\u4efb\u52a1\uff0c\u7279\u522b\u662f\u5728\u9ad8\u6027\u80fd\u8ba1\u7b97\u5de5\u4f5c\u8d1f\u8f7d\u6620\u5c04\u4e0e\u8c03\u5ea6\u8fd9\u4e00\u5178\u578b\u7ec4\u5408\u4f18\u5316\u95ee\u9898\u4e2d\u7684\u80fd\u529b\u8fb9\u754c\u3002", "method": "\u541121\u4e2a\u516c\u5f00\u53ef\u7528\u7684\u5927\u8bed\u8a00\u6a21\u578b\u63d0\u4f9b\u76f8\u540c\u7684\u7cfb\u7edf\u8282\u70b9\u3001\u4efb\u52a1\u9700\u6c42\u548c\u8c03\u5ea6\u7ea6\u675f\u7684\u6587\u672c\u63cf\u8ff0\uff0c\u8981\u6c42\u5176\u5206\u914d\u4efb\u52a1\u3001\u8ba1\u7b97\u603b\u5b8c\u5de5\u65f6\u95f4\uff08makespan\uff09\u5e76\u89e3\u91ca\u63a8\u7406\u8fc7\u7a0b\uff0c\u5e76\u4ee5\u4eba\u5de5\u63a8\u5bfc\u76849\u5c0f\u65f620\u79d2\u6700\u4f18\u89e3\u4f5c\u4e3a\u57fa\u51c6\u8fdb\u884c\u8bc4\u4f30\u3002", "result": "\u4e09\u4e2a\u6a21\u578b\u7cbe\u786e\u590d\u73b0\u4e86\u6700\u4f18\u89e3\uff1b\u5341\u4e8c\u4e2a\u6a21\u578b\u7ed3\u679c\u5728\u6700\u4f18\u89e3\u4e24\u5206\u949f\u5185\uff1b\u516d\u4e2a\u6a21\u578b\u56e0\u7b97\u672f\u6216\u4f9d\u8d56\u9519\u8bef\u800c\u6b21\u4f18\uff1b\u6240\u6709\u6a21\u578b\u5747\u751f\u6210\u53ef\u884c\u6620\u5c04\uff0c\u4f46\u4ec5\u7ea6\u4e00\u534a\u4e25\u683c\u9075\u5b88\u7ea6\u675f\uff1b\u5341\u4e5d\u4e2a\u6a21\u578b\u751f\u6210\u90e8\u5206\u53ef\u6267\u884c\u9a8c\u8bc1\u4ee3\u7801\uff0c\u5341\u516b\u4e2a\u63d0\u4f9b\u8fde\u8d2f\u63a8\u7406\u6b65\u9aa4\u3002", "conclusion": "\u5f53\u524d\u5927\u8bed\u8a00\u6a21\u578b\u5728\u7ec4\u5408\u4f18\u5316\u4efb\u52a1\u4e2d\u5c55\u73b0\u51fa\u4e00\u5b9a\u80fd\u529b\uff0c\u9876\u5c16\u6a21\u578b\u53ef\u76f4\u63a5\u4ece\u81ea\u7136\u8bed\u8a00\u91cd\u5efa\u6700\u4f18\u8c03\u5ea6\u65b9\u6848\uff0c\u4f46\u591a\u6570\u4ecd\u96be\u4ee5\u7cbe\u786e\u5904\u7406\u65f6\u5e8f\u3001\u6570\u636e\u4f20\u8f93\u8ba1\u7b97\u548c\u4f9d\u8d56\u5173\u7cfb\uff0c\u56e0\u6b64\u66f4\u9002\u5408\u4f5c\u4e3a\u5177\u5907\u53ef\u89e3\u91ca\u6027\u7684\u534f\u540c\u51b3\u7b56\u52a9\u624b\uff0c\u800c\u975e\u5b8c\u5168\u81ea\u4e3b\u7684\u4f18\u5316\u6c42\u89e3\u5668\u3002"}}
{"id": "2511.12544", "categories": ["cs.AR", "cs.ET", "cs.LG", "eess.IV"], "pdf": "https://arxiv.org/pdf/2511.12544", "abs": "https://arxiv.org/abs/2511.12544", "authors": ["Mukul Lokhande", "Akash Sankhe", "S. V. Jaya Chand", "Santosh Kumar Vishvakarma"], "title": "FERMI-ML: A Flexible and Resource-Efficient Memory-In-Situ SRAM Macro for TinyML acceleration", "comment": null, "summary": "The growing demand for low-power and area-efficient TinyML inference on AIoT devices necessitates memory architectures that minimise data movement while sustaining high computational efficiency. This paper presents FERMI-ML, a Flexible and Resource-Efficient Memory-In-Situ (MIS) SRAM macro designed for TinyML acceleration. The proposed 9T XNOR-based RX9T bit-cell integrates a 5T storage cell with a 4T XNOR compute unit, enabling variable-precision MAC and CAM operations within the same array. A 22-transistor (C22T) compressor-tree-based accumulator facilitates logarithmic 1-64-bit MAC computation with reduced delay and power compared to conventional adder trees. The 4 KB macro achieves dual functionality for in-situ computation and CAM-based lookup operations, supporting Posit-4 or FP-4 precision. Post-layout results at 65 nm show operation at 350 MHz with 0.9 V, delivering a throughput of 1.93 TOPS and an energy efficiency of 364 TOPS/W, while maintaining a Quality-of-Result (QoR) above 97.5% with InceptionV4 and ResNet-18. FERMI-ML thus demonstrates a compact, reconfigurable, and energy-aware digital Memory-In-Situ macro capable of supporting mixed-precision TinyML workloads.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86FERMI-ML\uff0c\u4e00\u79cd\u9762\u5411TinyML\u7684\u7075\u6d3b\u3001\u8d44\u6e90\u9ad8\u6548\u7684\u5b58\u5185\u8ba1\u7b97SRAM\u5b8f\u67b6\u6784\uff0c\u652f\u6301\u53ef\u53d8\u7cbe\u5ea6MAC\u548cCAM\u64cd\u4f5c\uff0c\u572865 nm\u5de5\u827a\u4e0b\u5b9e\u73b0\u4e86\u9ad8\u80fd\u6548\u4e0e\u9ad8\u541e\u5410\u91cf\u3002", "motivation": "\u4e3a\u6ee1\u8db3AIoT\u8bbe\u5907\u5bf9\u4f4e\u529f\u8017\u3001\u9762\u79ef\u9ad8\u6548TinyML\u63a8\u7406\u7684\u9700\u6c42\uff0c\u9700\u51cf\u5c11\u6570\u636e\u79fb\u52a8\u5e76\u7ef4\u6301\u9ad8\u8ba1\u7b97\u6548\u7387\uff0c\u56e0\u6b64\u9700\u8981\u65b0\u578b\u5185\u5b58\u67b6\u6784\u3002", "method": "\u8bbe\u8ba1\u4e86\u4e00\u79cd\u57fa\u4e8e9T XNOR\u7684RX9T\u4f4d\u5355\u5143\uff0c\u96c6\u62105T\u5b58\u50a8\u5355\u5143\u4e0e4T XNOR\u8ba1\u7b97\u5355\u5143\uff0c\u5e76\u91c7\u752822\u7ba1\u6676\u4f53\u7ba1\u538b\u7f29\u6811\u7d2f\u52a0\u5668\uff08C22T\uff09\u5b9e\u73b01\u201364\u4f4d\u5bf9\u6570\u7ea7MAC\u8fd0\u7b97\uff1b\u6784\u5efa4 KB SRAM\u5b8f\uff0c\u652f\u6301Posit-4/FP-4\u7cbe\u5ea6\u4e0b\u7684\u5b58\u5185\u8ba1\u7b97\u4e0eCAM\u67e5\u627e\u3002", "result": "\u572865 nm\u5de5\u827a\u4e0b\uff0c\u8be5\u5b8f\u5de5\u4f5c\u9891\u7387\u8fbe350 MHz\uff080.9 V\uff09\uff0c\u541e\u5410\u91cf1.93 TOPS\uff0c\u80fd\u6548364 TOPS/W\uff0cInceptionV4\u4e0eResNet-18\u6a21\u578bQoR\u8d85\u8fc797.5%\u3002", "conclusion": "FERMI-ML\u662f\u4e00\u79cd\u7d27\u51d1\u3001\u53ef\u91cd\u6784\u4e14\u8282\u80fd\u7684\u6570\u5b57\u5b58\u5185\u8ba1\u7b97\u5b8f\uff0c\u9002\u7528\u4e8e\u6df7\u5408\u7cbe\u5ea6TinyML\u4efb\u52a1\u3002"}}
{"id": "2511.13614", "categories": ["cs.MA", "cs.CE", "q-fin.CP", "q-fin.TR"], "pdf": "https://arxiv.org/pdf/2511.13614", "abs": "https://arxiv.org/abs/2511.13614", "authors": ["Jerick Shi", "Burton Hollifield"], "title": "Market-Dependent Communication in Multi-Agent Alpha Generation", "comment": null, "summary": "Multi-strategy hedge funds face a fundamental organizational choice: should analysts generating trading strategies communicate, and if so, how? We investigate this using 5-agent LLM-based trading systems across 450 experiments spanning 21 months, comparing five organizational structures from isolated baseline to collaborative and competitive conversation. We show that communication improves performance, but optimal communication design depends on market characteristics. Competitive conversation excels in volatile technology stocks, while collaborative conversation dominates stable general stocks. Finance stocks resist all communication interventions. Surprisingly, all structures, including isolated agents, converge to similar strategy alignments, challenging assumptions that transparency causes harmful diversity loss. Performance differences stem from behavioral mechanisms: competitive agents focus on stock-level allocation while collaborative agents develop technical frameworks. Conversation quality scores show zero correlation with returns. These findings demonstrate that optimal communication design must match market volatility characteristics, and sophisticated discussions don't guarantee better performance.", "AI": {"tldr": "\u8be5\u7814\u7a76\u901a\u8fc751\u4e2a\u6708\u5185450\u6b21\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u591a\u667a\u80fd\u4f53\u4ea4\u6613\u5b9e\u9a8c\uff0c\u53d1\u73b0\u6c9f\u901a\u80fd\u63d0\u5347\u5bf9\u51b2\u57fa\u91d1\u7b56\u7565\u8868\u73b0\uff0c\u4f46\u6700\u4f18\u6c9f\u901a\u65b9\u5f0f\u53d6\u51b3\u4e8e\u5e02\u573a\u7279\u6027\uff1a\u7ade\u4e89\u6027\u5bf9\u8bdd\u5728\u6ce2\u52a8\u5927\u7684\u79d1\u6280\u80a1\u4e2d\u8868\u73b0\u6700\u4f73\uff0c\u534f\u4f5c\u6027\u5bf9\u8bdd\u5728\u7a33\u5b9a\u80a1\u7968\u4e2d\u5360\u4f18\uff0c\u800c\u91d1\u878d\u80a1\u5219\u5bf9\u6240\u6709\u6c9f\u901a\u5e72\u9884\u65e0\u54cd\u5e94\uff1b\u6b64\u5916\uff0c\u7b56\u7565\u8d8b\u540c\u73b0\u8c61\u666e\u904d\u5b58\u5728\uff0c\u4e14\u5bf9\u8bdd\u8d28\u91cf\u4e0e\u6536\u76ca\u65e0\u5173\u3002", "motivation": "\u63a2\u8ba8\u591a\u7b56\u7565\u5bf9\u51b2\u57fa\u91d1\u5185\u90e8\u5206\u6790\u5e08\u662f\u5426\u5e94\u6c9f\u901a\u53ca\u5982\u4f55\u6c9f\u901a\u8fd9\u4e00\u7ec4\u7ec7\u8bbe\u8ba1\u95ee\u9898\uff0c\u4ee5\u4f18\u5316\u4ea4\u6613\u7b56\u7565\u8868\u73b0\u3002", "method": "\u6784\u5efa\u5305\u542b5\u4e2a\u667a\u80fd\u4f53\u7684LLM\u4ea4\u6613\u7cfb\u7edf\uff0c\u572821\u4e2a\u6708\u5185\u8fdb\u884c450\u6b21\u5b9e\u9a8c\uff0c\u6bd4\u8f83\u4e94\u79cd\u7ec4\u7ec7\u7ed3\u6784\uff08\u4ece\u5b64\u7acb\u57fa\u7ebf\u5230\u534f\u4f5c\u4e0e\u7ade\u4e89\u6027\u5bf9\u8bdd\uff09\u5728\u4e0d\u540c\u80a1\u7968\u5e02\u573a\u73af\u5883\u4e0b\u7684\u8868\u73b0\u3002", "result": "\u6c9f\u901a\u603b\u4f53\u4e0a\u63d0\u5347\u7ee9\u6548\uff0c\u4f46\u6548\u679c\u4f9d\u8d56\u4e8e\u5e02\u573a\u7c7b\u578b\uff1a\u7ade\u4e89\u6027\u5bf9\u8bdd\u5728\u79d1\u6280\u80a1\u4e2d\u8868\u73b0\u6700\u4f73\uff0c\u534f\u4f5c\u6027\u5bf9\u8bdd\u5728\u4e00\u822c\u7a33\u5b9a\u80a1\u7968\u4e2d\u66f4\u4f18\uff0c\u91d1\u878d\u80a1\u5219\u4e0d\u53d7\u6c9f\u901a\u5f71\u54cd\uff1b\u6240\u6709\u7ed3\u6784\u4e0b\u7b56\u7565\u8d8b\u4e8e\u4e00\u81f4\uff0c\u4e14\u5bf9\u8bdd\u8d28\u91cf\u4e0e\u56de\u62a5\u65e0\u76f8\u5173\u6027\u3002", "conclusion": "\u6700\u4f18\u6c9f\u901a\u8bbe\u8ba1\u9700\u5339\u914d\u5e02\u573a\u6ce2\u52a8\u7279\u5f81\uff0c\u590d\u6742\u6216\u9ad8\u8d28\u91cf\u7684\u8ba8\u8bba\u5e76\u4e0d\u5fc5\u7136\u5e26\u6765\u66f4\u597d\u7ee9\u6548\uff0c\u7b56\u7565\u8d8b\u540c\u5e76\u975e\u4ec5\u7531\u900f\u660e\u5ea6\u5bfc\u81f4\u3002"}}
{"id": "2511.12576", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2511.12576", "abs": "https://arxiv.org/abs/2511.12576", "authors": ["Mohammad Meymani", "Hamed Jelodar", "Parisa Hamedi", "Roozbeh Razavi-Far", "Ali A. Ghorbani"], "title": "Can Small GenAI Language Models Rival Large Language Models in Understanding Application Behavior?", "comment": null, "summary": "Generative AI (GenAI) models, particularly large language models (LLMs), have transformed multiple domains, including natural language processing, software analysis, and code understanding. Their ability to analyze and generate code has enabled applications such as source code summarization, behavior analysis, and malware detection. In this study, we systematically evaluate the capabilities of both small and large GenAI language models in understanding application behavior, with a particular focus on malware detection as a representative task. While larger models generally achieve higher overall accuracy, our experiments show that small GenAI models maintain competitive precision and recall, offering substantial advantages in computational efficiency, faster inference, and deployment in resource-constrained environments. We provide a detailed comparison across metrics such as accuracy, precision, recall, and F1-score, highlighting each model's strengths, limitations, and operational feasibility. Our findings demonstrate that small GenAI models can effectively complement large ones, providing a practical balance between performance and resource efficiency in real-world application behavior analysis.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7cfb\u7edf\u8bc4\u4f30\u4e86\u5927\u5c0f\u578b\u751f\u6210\u5f0fAI\u8bed\u8a00\u6a21\u578b\u5728\u5e94\u7528\u7a0b\u5e8f\u884c\u4e3a\u7406\u89e3\uff08\u4ee5\u6076\u610f\u8f6f\u4ef6\u68c0\u6d4b\u4e3a\u4ee3\u8868\u4efb\u52a1\uff09\u4e2d\u7684\u8868\u73b0\uff0c\u53d1\u73b0\u5c0f\u578b\u6a21\u578b\u5728\u4fdd\u6301\u8f83\u9ad8\u51c6\u786e\u7387\u7684\u540c\u65f6\uff0c\u5728\u8ba1\u7b97\u6548\u7387\u548c\u90e8\u7f72\u53ef\u884c\u6027\u65b9\u9762\u5177\u6709\u663e\u8457\u4f18\u52bf\u3002", "motivation": "\u63a2\u7d22\u5c0f\u578b\u4e0e\u5927\u578b\u751f\u6210\u5f0fAI\u8bed\u8a00\u6a21\u578b\u5728\u5e94\u7528\u884c\u4e3a\u7406\u89e3\uff08\u7279\u522b\u662f\u6076\u610f\u8f6f\u4ef6\u68c0\u6d4b\uff09\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\u6743\u8861\uff0c\u4ee5\u8bc4\u4f30\u5c0f\u578b\u6a21\u578b\u662f\u5426\u80fd\u5728\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4e2d\u63d0\u4f9b\u5b9e\u7528\u4e14\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u5bf9\u591a\u79cd\u5c0f\u578b\u548c\u5927\u578b\u751f\u6210\u5f0fAI\u8bed\u8a00\u6a21\u578b\u5728\u6076\u610f\u8f6f\u4ef6\u68c0\u6d4b\u4efb\u52a1\u4e2d\u8fdb\u884c\u7cfb\u7edf\u6027\u8bc4\u4f30\uff0c\u6bd4\u8f83\u5176\u5728\u51c6\u786e\u7387\u3001\u7cbe\u786e\u7387\u3001\u53ec\u56de\u7387\u548cF1\u5206\u6570\u7b49\u6307\u6807\u4e0a\u7684\u8868\u73b0\uff0c\u5e76\u5206\u6790\u5176\u8ba1\u7b97\u6548\u7387\u4e0e\u90e8\u7f72\u53ef\u884c\u6027\u3002", "result": "\u5927\u578b\u6a21\u578b\u6574\u4f53\u51c6\u786e\u7387\u66f4\u9ad8\uff0c\u4f46\u5c0f\u578b\u6a21\u578b\u5728\u7cbe\u786e\u7387\u548c\u53ec\u56de\u7387\u65b9\u9762\u8868\u73b0\u5177\u6709\u7ade\u4e89\u529b\uff0c\u540c\u65f6\u5177\u5907\u66f4\u5feb\u7684\u63a8\u7406\u901f\u5ea6\u548c\u66f4\u4f4e\u7684\u8d44\u6e90\u6d88\u8017\u3002", "conclusion": "\u5c0f\u578b\u751f\u6210\u5f0fAI\u6a21\u578b\u80fd\u591f\u5728\u5b9e\u9645\u5e94\u7528\u884c\u4e3a\u5206\u6790\u4e2d\u6709\u6548\u8865\u5145\u5927\u578b\u6a21\u578b\uff0c\u5728\u6027\u80fd\u4e0e\u8d44\u6e90\u6548\u7387\u4e4b\u95f4\u53d6\u5f97\u5b9e\u7528\u5e73\u8861\u3002"}}
{"id": "2511.11614", "categories": ["cs.DC", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.11614", "abs": "https://arxiv.org/abs/2511.11614", "authors": ["Arturo Ur\u00edas Jim\u00e9nez"], "title": "Beyond the GPU: The Strategic Role of FPGAs in the Next Wave of AI", "comment": null, "summary": "AI acceleration has been dominated by GPUs, but the growing need for lower latency, energy efficiency, and fine-grained hardware control exposes the limits of fixed architectures. In this context, Field-Programmable Gate Arrays (FPGAs) emerge as a reconfigurable platform that allows mapping AI algorithms directly into device logic. Their ability to implement parallel pipelines for convolutions, attention mechanisms, and post-processing with deterministic timing and reduced power consumption makes them a strategic option for workloads that demand predictable performance and deep customization.\n  Unlike CPUs and GPUs, whose architecture is immutable, an FPGA can be reconfigured in the field to adapt its physical structure to a specific model, integrate as a SoC with embedded processors, and run inference near the sensor without sending raw data to the cloud. This reduces latency and required bandwidth, improves privacy, and frees GPUs from specialized tasks in data centers. Partial reconfiguration and compilation flows from AI frameworks are shortening the path from prototype to deployment, enabling hardware--algorithm co-design.", "AI": {"tldr": "FPGAs offer a reconfigurable, energy-efficient alternative to GPUs for AI acceleration by enabling custom hardware implementations of models with low latency, deterministic timing, and privacy benefits.", "motivation": "The limitations of fixed-architecture accelerators like GPUs\u2014such as high latency, limited energy efficiency, and lack of fine-grained control\u2014motivate the exploration of more flexible platforms for AI workloads.", "method": "The paper discusses leveraging FPGAs to map AI algorithms directly into hardware logic, utilizing their reconfigurability to implement parallel pipelines for operations like convolutions and attention mechanisms, and integrating them as SoCs with embedded processors.", "result": "FPGAs enable deterministic, low-latency inference near sensors, reduce bandwidth and cloud dependency, enhance privacy, and offload specialized tasks from data center GPUs, with improved deployment workflows via partial reconfiguration and AI framework compilation.", "conclusion": "FPGAs are a strategic platform for AI acceleration where performance predictability, customization, and efficiency are critical, especially in edge and privacy-sensitive applications."}}
{"id": "2511.12616", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2511.12616", "abs": "https://arxiv.org/abs/2511.12616", "authors": ["Arya Parameshwara"], "title": "SynapticCore-X: A Modular Neural Processing Architecture for Low-Cost FPGA Acceleration", "comment": "10 pages, 7 figures, conference-style formatting", "summary": "This paper presents SynapticCore-X, a modular and resource-efficient neural processing architecture optimized for deployment on low-cost FPGA platforms. The design integrates a lightweight RV32IMC RISC-V control core with a configurable neural compute tile that supports fused matrix, activation, and data-movement operations. Unlike existing FPGA accelerators that rely on heavyweight IP blocks, SynapticCore-X provides a fully open-source SystemVerilog microarchitecture with tunable parallelism, scratchpad memory depth, and DMA burst behavior, enabling rapid exploration of hardware-software co-design trade-offs. We document an automated, reproducible Vivado build pipeline that achieves timing closure at 100 MHz on the Zynq-7020 while consuming only 6.1% LUTs, 32.5% DSPs, and 21.4% BRAMs. Hardware validation on PYNQ-Z2 confirms correct register-level execution, deterministic control-path behavior, and cycle-accurate performance for matrix and convolution kernels. SynapticCore-X demonstrates that energy-efficient NPU-like acceleration can be prototyped on commodity educational FPGAs, lowering the entry barrier for academic and open-hardware research in neural microarchitectures.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86SynapticCore-X\uff0c\u4e00\u79cd\u6a21\u5757\u5316\u3001\u8d44\u6e90\u9ad8\u6548\u7684\u795e\u7ecf\u5904\u7406\u67b6\u6784\uff0c\u4e13\u4e3a\u4f4e\u6210\u672cFPGA\u5e73\u53f0\u4f18\u5316\uff0c\u7ed3\u5408\u8f7b\u91cf\u7ea7RISC-V\u6838\u5fc3\u4e0e\u53ef\u914d\u7f6e\u795e\u7ecf\u8ba1\u7b97\u5355\u5143\uff0c\u652f\u6301\u5f00\u6e90\u3001\u53ef\u8c03\u53c2\u6570\u548c\u9ad8\u6548\u786c\u4ef6\u9a8c\u8bc1\u3002", "motivation": "\u73b0\u6709FPGA\u52a0\u901f\u5668\u901a\u5e38\u4f9d\u8d56\u91cd\u91cf\u7ea7IP\u6a21\u5757\uff0c\u7f3a\u4e4f\u5f00\u653e\u6027\u548c\u7075\u6d3b\u6027\uff0c\u96be\u4ee5\u5728\u4f4e\u6210\u672c\u5e73\u53f0\u4e0a\u8fdb\u884c\u795e\u7ecf\u5fae\u67b6\u6784\u7814\u7a76\uff1b\u4f5c\u8005\u65e8\u5728\u964d\u4f4e\u5b66\u672f\u754c\u548c\u5f00\u6e90\u786c\u4ef6\u793e\u533a\u5728\u795e\u7ecf\u5904\u7406\u5668\u8bbe\u8ba1\u4e0a\u7684\u95e8\u69db\u3002", "method": "\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u96c6\u6210RV32IMC RISC-V\u63a7\u5236\u6838\u5fc3\u4e0e\u53ef\u914d\u7f6e\u795e\u7ecf\u8ba1\u7b97\u5355\u5143\u7684\u67b6\u6784\uff0c\u91c7\u7528\u5b8c\u5168\u5f00\u6e90\u7684SystemVerilog\u5b9e\u73b0\uff0c\u5e76\u63d0\u4f9b\u53ef\u8c03\u8282\u7684\u5e76\u884c\u5ea6\u3001\u6682\u5b58\u5b58\u50a8\u6df1\u5ea6\u548cDMA\u7a81\u53d1\u884c\u4e3a\uff1b\u540c\u65f6\u6784\u5efa\u4e86\u81ea\u52a8\u5316\u7684Vivado\u6784\u5efa\u6d41\u7a0b\u3002", "result": "\u5728Zynq-7020\u4e0a\u5b9e\u73b0100 MHz\u65f6\u5e8f\u6536\u655b\uff0c\u4ec5\u5360\u75286.1% LUT\u300132.5% DSP\u548c21.4% BRAM\uff1b\u5728PYNQ-Z2\u4e0a\u5b8c\u6210\u786c\u4ef6\u9a8c\u8bc1\uff0c\u786e\u8ba4\u5bc4\u5b58\u5668\u7ea7\u6267\u884c\u6b63\u786e\u6027\u3001\u63a7\u5236\u8def\u5f84\u786e\u5b9a\u6027\u53ca\u77e9\u9635/\u5377\u79ef\u6838\u7684\u5468\u671f\u7cbe\u786e\u6027\u80fd\u3002", "conclusion": "SynapticCore-X\u8bc1\u660e\u4e86\u5728\u666e\u901a\u6559\u80b2\u7ea7FPGA\u4e0a\u4e5f\u80fd\u5b9e\u73b0\u7c7bNPU\u7684\u80fd\u6548\u52a0\u901f\uff0c\u4e3a\u795e\u7ecf\u5fae\u67b6\u6784\u7684\u5b66\u672f\u4e0e\u5f00\u6e90\u7814\u7a76\u63d0\u4f9b\u4e86\u4f4e\u95e8\u69db\u3001\u9ad8\u7075\u6d3b\u6027\u7684\u539f\u578b\u5e73\u53f0\u3002"}}
{"id": "2511.12635", "categories": ["cs.SE", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.12635", "abs": "https://arxiv.org/abs/2511.12635", "authors": ["Lech Madeyski", "Barbara Kitchenham", "Martin Shepperd"], "title": "LLM4SCREENLIT: Recommendations on Assessing the Performance of Large Language Models for Screening Literature in Systematic Reviews", "comment": "19 pages, 4 figures", "summary": "Context: Large language models (LLMs) are released faster than users' ability to evaluate them rigorously. When LLMs underpin research, such as identifying relevant literature for systematic reviews (SRs), robust empirical assessment is essential. Objective: We identify and discuss key challenges in assessing LLM performance for selecting relevant literature, identify good (evaluation) practices, and propose recommendations. Method: Using a recent large-scale study as an example, we identify problems with the use of traditional metrics for assessing the performance of Gen-AI tools for identifying relevant literature in SRs. We analyzed 27 additional papers investigating this issue, extracted the performance metrics, and found both good practices and widespread problems, especially with the use and reporting of performance metrics for SR screening. Results: Major weaknesses included: i) a failure to use metrics that are robust to imbalanced data and do not directly indicate whether results are better than chance, e.g., the use of Accuracy, ii) a failure to consider the impact of lost evidence when making claims concerning workload savings, and iii) pervasive failure to report the full confusion matrix (or performance metrics from which it can be reconstructed) which is essential for future meta-analyses. On the positive side, we extract good (evaluation) practices on which our recommendations for researchers and practitioners, as well as policymakers, are built. Conclusions: SR screening evaluations should prioritize lost evidence/recall alongside chance-anchored and cost-sensitive Weighted MCC (WMCC) metric, report complete confusion matrices, treat unclassifiable outputs as referred-back positives for assessment, adopt leakage-aware designs with non-LLM baselines and open artifacts, and ground conclusions in cost-benefit analysis where FNs carry higher penalties than FPs.", "AI": {"tldr": "\u8be5\u8bba\u6587\u6307\u51fa\u5f53\u524d\u5728\u7cfb\u7edf\u7efc\u8ff0\u6587\u732e\u7b5b\u9009\u4e2d\u8bc4\u4f30\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u6027\u80fd\u65f6\u5b58\u5728\u4e25\u91cd\u65b9\u6cd5\u8bba\u7f3a\u9677\uff0c\u5982\u8bef\u7528\u51c6\u786e\u7387\u7b49\u4e0d\u9002\u7528\u4e8e\u4e0d\u5e73\u8861\u6570\u636e\u7684\u6307\u6807\u3001\u5ffd\u89c6\u9057\u6f0f\u8bc1\u636e\u7684\u5f71\u54cd\u3001\u672a\u62a5\u544a\u5b8c\u6574\u6df7\u6dc6\u77e9\u9635\u7b49\uff0c\u5e76\u636e\u6b64\u63d0\u51fa\u6539\u8fdb\u8bc4\u4f30\u5b9e\u8df5\u7684\u5177\u4f53\u5efa\u8bae\u3002", "motivation": "\u7531\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u53d1\u5e03\u901f\u5ea6\u8fdc\u8d85\u7528\u6237\u4e25\u8c28\u8bc4\u4f30\u7684\u80fd\u529b\uff0c\u5c24\u5176\u5728\u652f\u6491\u7cfb\u7edf\u7efc\u8ff0\u7b49\u5173\u952e\u7814\u7a76\u4efb\u52a1\u65f6\uff0c\u4e9f\u9700\u5efa\u7acb\u7a33\u5065\u7684\u8bc4\u4f30\u6846\u67b6\u4ee5\u786e\u4fdd\u5176\u53ef\u9760\u6027\u548c\u6709\u6548\u6027\u3002", "method": "\u4ee5\u4e00\u9879\u8fd1\u671f\u5927\u89c4\u6a21\u7814\u7a76\u4e3a\u4f8b\uff0c\u5206\u6790\u4f20\u7edf\u8bc4\u4f30\u6307\u6807\u5728\u7cfb\u7edf\u7efc\u8ff0\u6587\u732e\u7b5b\u9009\u4efb\u52a1\u4e2d\u7684\u9002\u7528\u6027\u95ee\u9898\uff1b\u540c\u65f6\u7cfb\u7edf\u56de\u987e27\u7bc7\u76f8\u5173\u8bba\u6587\uff0c\u63d0\u53d6\u5e76\u8bc4\u4f30\u5176\u6240\u91c7\u7528\u7684\u6027\u80fd\u6307\u6807\uff0c\u8bc6\u522b\u826f\u597d\u5b9e\u8df5\u4e0e\u5e38\u89c1\u95ee\u9898\u3002", "result": "\u53d1\u73b0\u4e3b\u8981\u95ee\u9898\u5305\u62ec\uff1a\u4f7f\u7528\u5bf9\u4e0d\u5e73\u8861\u6570\u636e\u4e0d\u9c81\u68d2\u4e14\u65e0\u6cd5\u53cd\u6620\u4f18\u4e8e\u968f\u673a\u6c34\u5e73\u7684\u6307\u6807\uff08\u5982\u51c6\u786e\u7387\uff09\u3001\u5ffd\u89c6\u9057\u6f0f\u8bc1\u636e\u5bf9\u5de5\u4f5c\u91cf\u8282\u7701\u4e3b\u5f20\u7684\u5f71\u54cd\u3001\u666e\u904d\u672a\u62a5\u544a\u5b8c\u6574\u6df7\u6dc6\u77e9\u9635\u3002\u540c\u65f6\u4e5f\u603b\u7ed3\u51fa\u82e5\u5e72\u826f\u597d\u8bc4\u4f30\u5b9e\u8df5\u3002", "conclusion": "\u7cfb\u7edf\u7efc\u8ff0\u7b5b\u9009\u8bc4\u4f30\u5e94\u4f18\u5148\u5173\u6ce8\u9057\u6f0f\u8bc1\u636e/\u53ec\u56de\u7387\u548c\u52a0\u6743\u9a6c\u4fee\u65af\u76f8\u5173\u7cfb\u6570\uff08WMCC\uff09\uff0c\u62a5\u544a\u5b8c\u6574\u6df7\u6dc6\u77e9\u9635\uff0c\u5c06\u4e0d\u53ef\u5206\u7c7b\u8f93\u51fa\u89c6\u4e3a\u9700\u4eba\u5de5\u590d\u6838\u7684\u9633\u6027\u7ed3\u679c\uff0c\u91c7\u7528\u9632\u4fe1\u606f\u6cc4\u9732\u8bbe\u8ba1\u5e76\u5305\u542b\u975eLLM\u57fa\u7ebf\uff0c\u4e14\u7ed3\u8bba\u5e94\u57fa\u4e8e\u5047\u9634\u6027\u4ee3\u4ef7\u66f4\u9ad8\u7684\u6210\u672c\u6548\u76ca\u5206\u6790\u3002"}}
{"id": "2511.11617", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2511.11617", "abs": "https://arxiv.org/abs/2511.11617", "authors": ["Wendong Xu", "Chujie Chen", "He Xiao", "Kuan Li", "Jing Xiong", "Chen Zhang", "Wenyong Zhou", "Chaofan Tao", "Yang Bai", "Bei Yu", "Ngai Wong"], "title": "AnchorTP: Resilient LLM Inference with State-Preserving Elastic Tensor Parallelism", "comment": "accpeted paper by Design, Automation and Test in Europe Conference (DATE'26). 8 pages in total with 6 figures and 2 tables", "summary": "Large Language Model (LLM) inference services demand exceptionally high availability and low latency, yet multi-GPU Tensor Parallelism (TP) makes them vulnerable to single-GPU failures. We present AnchorTP, a state-preserving elastic TP framework for fast recovery. It (i) enables Elastic Tensor Parallelism (ETP) with unequal-width partitioning over any number of GPUs and compatibility with Mixture-of-Experts (MoE), and (ii) preserves model parameters and KV caches in GPU memory via a daemon decoupled from the inference process. To minimize downtime, we propose a bandwidth-aware planner based on a Continuous Minimal Migration (CMM) algorithm that minimizes reload bytes under a byte-cost dominance assumption, and an execution scheduler that pipelines P2P transfers with reloads. These components jointly restore service quickly with minimal data movement and without changing service interfaces. In typical failure scenarios, AnchorTP reduces Time to First Success (TFS) by up to 11x and Time to Peak (TTP) by up to 59% versus restart-and-reload.", "AI": {"tldr": "AnchorTP \u662f\u4e00\u79cd\u652f\u6301\u72b6\u6001\u4fdd\u5b58\u7684\u5f39\u6027\u5f20\u91cf\u5e76\u884c\u6846\u67b6\uff0c\u901a\u8fc7\u4e0d\u7b49\u5bbd\u5212\u5206\u3001\u89e3\u8026\u5b88\u62a4\u8fdb\u7a0b\u548c\u5e26\u5bbd\u611f\u77e5\u8fc1\u79fb\u89c4\u5212\uff0c\u5728 GPU \u6545\u969c\u65f6\u5b9e\u73b0\u5feb\u901f\u6062\u590d\uff0c\u663e\u8457\u964d\u4f4e\u670d\u52a1\u4e2d\u65ad\u65f6\u95f4\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u63a8\u7406\u670d\u52a1\u5bf9\u9ad8\u53ef\u7528\u6027\u548c\u4f4e\u5ef6\u8fdf\u8981\u6c42\u6781\u9ad8\uff0c\u4f46\u591a GPU \u5f20\u91cf\u5e76\u884c\uff08TP\uff09\u6613\u53d7\u5355 GPU \u6545\u969c\u5f71\u54cd\uff0c\u73b0\u6709\u65b9\u6cd5\u6062\u590d\u6162\u3001\u5f00\u9500\u5927\u3002", "method": "\u63d0\u51fa AnchorTP \u6846\u67b6\uff1a(i) \u652f\u6301\u4efb\u610f GPU \u6570\u91cf\u7684\u5f39\u6027\u5f20\u91cf\u5e76\u884c\uff08ETP\uff09\u4e0e MoE \u517c\u5bb9\uff1b(ii) \u901a\u8fc7\u4e0e\u63a8\u7406\u89e3\u8026\u7684\u5b88\u62a4\u8fdb\u7a0b\u5728 GPU \u5185\u5b58\u4e2d\u4fdd\u7559\u6a21\u578b\u53c2\u6570\u548c KV \u7f13\u5b58\uff1b(iii) \u8bbe\u8ba1\u57fa\u4e8e\u8fde\u7eed\u6700\u5c0f\u8fc1\u79fb\uff08CMM\uff09\u7b97\u6cd5\u7684\u5e26\u5bbd\u611f\u77e5\u89c4\u5212\u5668\u548c\u6267\u884c\u8c03\u5ea6\u5668\uff0c\u4ee5\u51cf\u5c11\u6570\u636e\u91cd\u8f7d\u91cf\u5e76\u6d41\u6c34\u5316\u70b9\u5bf9\u70b9\u4f20\u8f93\u3002", "result": "\u5728\u5178\u578b\u6545\u969c\u573a\u666f\u4e0b\uff0c\u76f8\u6bd4\u91cd\u542f\u91cd\u8f7d\u65b9\u6848\uff0cAnchorTP \u5c06\u9996\u6b21\u6210\u529f\u54cd\u5e94\u65f6\u95f4\uff08TFS\uff09\u6700\u591a\u7f29\u77ed 11 \u500d\uff0c\u8fbe\u5230\u5cf0\u503c\u541e\u5410\u65f6\u95f4\uff08TTP\uff09\u6700\u591a\u51cf\u5c11 59%\u3002", "conclusion": "AnchorTP \u80fd\u5728\u4e0d\u6539\u53d8\u670d\u52a1\u63a5\u53e3\u7684\u524d\u63d0\u4e0b\uff0c\u4ee5\u6700\u5c0f\u6570\u636e\u8fc1\u79fb\u5b9e\u73b0\u5feb\u901f\u670d\u52a1\u6062\u590d\uff0c\u663e\u8457\u63d0\u5347 LLM \u63a8\u7406\u7cfb\u7edf\u7684\u5bb9\u9519\u80fd\u529b\u4e0e\u53ef\u7528\u6027\u3002"}}
{"id": "2511.12823", "categories": ["cs.SE", "cs.LG", "cs.PL"], "pdf": "https://arxiv.org/pdf/2511.12823", "abs": "https://arxiv.org/abs/2511.12823", "authors": ["Sajed Jalil", "Shuvo Saha", "Hossain Mohammad Seym"], "title": "Enhancing LLM Code Generation Capabilities through Test-Driven Development and Code Interpreter", "comment": "AACL-IJCNLP 2025 Workshop BLP Shared Task 2, 6 pages, 7 figures, 3 tables", "summary": "Over the past few years, improving LLM code generation capabilities has been a key focus in NLP research. Despite Bengali having 242 million native speakers worldwide, it receives little attention when it comes to training LLMs. More recently, various fine-tuning and augmented generation techniques have been employed to significantly enhance code generation performance. However, they require considerable expertise and resources to utilize effectively as an end user. The goal of our work is to democratize access to powerful code generation tools in resource-constrained emerging markets, enabling users to leverage them in their native language.\n  We introduce a novel approach that combines Test-Driven Development (TDD) and Code Interpreter (CI), utilizing open-weight models, which improves the baseline accuracy for code generation with Bengali prompts and achieves an overall accuracy of 85%. Our approach requires no finetuning and proves that even the smallest models in the same family can attain up to 98% accuracy compared to the largest models. All of our results are publicly shared in GitHub for validation and reproducibility.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u7ed3\u5408\u6d4b\u8bd5\u9a71\u52a8\u5f00\u53d1\uff08TDD\uff09\u4e0e\u4ee3\u7801\u89e3\u91ca\u5668\uff08CI\uff09\u7684\u65b0\u65b9\u6cd5\uff0c\u5229\u7528\u5f00\u6e90\u6a21\u578b\u5728\u65e0\u9700\u5fae\u8c03\u7684\u60c5\u51b5\u4e0b\u663e\u8457\u63d0\u5347\u9488\u5bf9\u5b5f\u52a0\u62c9\u8bed\u63d0\u793a\u7684\u4ee3\u7801\u751f\u6210\u51c6\u786e\u7387\uff0c\u6700\u9ad8\u53ef\u8fbe85%\uff0c\u5c0f\u6a21\u578b\u6027\u80fd\u53ef\u8fbe\u5927\u6a21\u578b\u768498%\u3002", "motivation": "\u5c3d\u7ba1\u5b5f\u52a0\u62c9\u8bed\u62e5\u67092.42\u4ebf\u6bcd\u8bed\u4f7f\u7528\u8005\uff0c\u4f46\u5728\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u8bad\u7ec3\u4e2d\u957f\u671f\u88ab\u5ffd\u89c6\uff1b\u73b0\u6709\u4ee3\u7801\u751f\u6210\u6280\u672f\u4f9d\u8d56\u5927\u91cf\u4e13\u4e1a\u77e5\u8bc6\u548c\u8d44\u6e90\uff0c\u96be\u4ee5\u5728\u8d44\u6e90\u53d7\u9650\u7684\u65b0\u5174\u5e02\u573a\u666e\u53ca\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7\u672c\u571f\u8bed\u8a00\u8d4b\u80fd\u7528\u6237\uff0c\u63a8\u52a8\u4ee3\u7801\u751f\u6210\u5de5\u5177\u7684\u6c11\u4e3b\u5316\u3002", "method": "\u7ed3\u5408\u6d4b\u8bd5\u9a71\u52a8\u5f00\u53d1\uff08TDD\uff09\u4e0e\u4ee3\u7801\u89e3\u91ca\u5668\uff08CI\uff09\uff0c\u4f7f\u7528\u5f00\u6e90\u6743\u91cd\u6a21\u578b\u5904\u7406\u5b5f\u52a0\u62c9\u8bed\u63d0\u793a\u8fdb\u884c\u4ee3\u7801\u751f\u6210\uff0c\u65e0\u9700\u5fae\u8c03\u3002", "result": "\u5728\u5b5f\u52a0\u62c9\u8bed\u63d0\u793a\u4e0b\uff0c\u4ee3\u7801\u751f\u6210\u6574\u4f53\u51c6\u786e\u7387\u8fbe\u523085%\uff1b\u540c\u7cfb\u5217\u6700\u5c0f\u6a21\u578b\u6027\u80fd\u53ef\u8fbe\u6700\u5927\u6a21\u578b\u768498%\u3002\u6240\u6709\u7ed3\u679c\u5df2\u5728GitHub\u516c\u5f00\u4ee5\u4f9b\u9a8c\u8bc1\u548c\u590d\u73b0\u3002", "conclusion": "\u6240\u63d0\u65b9\u6cd5\u6709\u6548\u63d0\u5347\u4e86\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4e0b\u57fa\u4e8e\u672c\u5730\u8bed\u8a00\u7684\u4ee3\u7801\u751f\u6210\u80fd\u529b\uff0c\u8bc1\u660e\u4e86\u65e0\u9700\u5fae\u8c03\u7684\u5c0f\u578b\u5f00\u6e90\u6a21\u578b\u4e5f\u80fd\u5b9e\u73b0\u63a5\u8fd1\u9876\u7ea7\u5927\u6a21\u578b\u7684\u6027\u80fd\uff0c\u6709\u52a9\u4e8e\u63a8\u52a8\u4ee3\u7801\u751f\u6210\u6280\u672f\u5728\u4f4e\u8d44\u6e90\u8bed\u8a00\u793e\u533a\u7684\u666e\u53ca\u3002"}}
{"id": "2511.12930", "categories": ["cs.AR", "cs.CV"], "pdf": "https://arxiv.org/pdf/2511.12930", "abs": "https://arxiv.org/abs/2511.12930", "authors": ["Changhun Oh", "Seongryong Oh", "Jinwoo Hwang", "Yoonsung Kim", "Hardik Sharma", "Jongse Park"], "title": "Neo: Real-Time On-Device 3D Gaussian Splatting with Reuse-and-Update Sorting Acceleration", "comment": null, "summary": "3D Gaussian Splatting (3DGS) rendering in real-time on resource-constrained devices is essential for delivering immersive augmented and virtual reality (AR/VR) experiences. However, existing solutions struggle to achieve high frame rates, especially for high-resolution rendering. Our analysis identifies the sorting stage in the 3DGS rendering pipeline as the major bottleneck due to its high memory bandwidth demand. This paper presents Neo, which introduces a reuse-and-update sorting algorithm that exploits temporal redundancy in Gaussian ordering across consecutive frames, and devises a hardware accelerator optimized for this algorithm. By efficiently tracking and updating Gaussian depth ordering instead of re-sorting from scratch, Neo significantly reduces redundant computations and memory bandwidth pressure. Experimental results show that Neo achieves up to 10.0x and 5.6x higher throughput than state-of-the-art edge GPU and ASIC solution, respectively, while reducing DRAM traffic by 94.5% and 81.3%. These improvements make high-quality and low-latency on-device 3D rendering more practical.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faNeo\u7cfb\u7edf\uff0c\u901a\u8fc7\u4e00\u79cd\u5229\u7528\u5e27\u95f4\u9ad8\u65af\u6392\u5e8f\u5197\u4f59\u6027\u7684\u91cd\u7528\u4e0e\u66f4\u65b0\u6392\u5e8f\u7b97\u6cd5\u53ca\u914d\u5957\u786c\u4ef6\u52a0\u901f\u5668\uff0c\u663e\u8457\u63d0\u53473D\u9ad8\u65af\u6cfc\u6e85\uff083DGS\uff09\u6e32\u67d3\u5728\u8d44\u6e90\u53d7\u9650\u8bbe\u5907\u4e0a\u7684\u5b9e\u65f6\u6027\u80fd\u3002", "motivation": "\u73b0\u67093DGS\u6e32\u67d3\u65b9\u6848\u5728\u9ad8\u5206\u8fa8\u7387\u4e0b\u96be\u4ee5\u5728\u8d44\u6e90\u53d7\u9650\u8bbe\u5907\u4e0a\u5b9e\u73b0\u5b9e\u65f6\u9ad8\u5e27\u7387\uff0c\u4e3b\u8981\u74f6\u9888\u5728\u4e8e\u6392\u5e8f\u9636\u6bb5\u5bf9\u5185\u5b58\u5e26\u5bbd\u7684\u9ad8\u9700\u6c42\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u91cd\u7528\u4e0e\u66f4\u65b0\u7684\u6392\u5e8f\u7b97\u6cd5\uff0c\u5229\u7528\u8fde\u7eed\u5e27\u4e4b\u95f4\u9ad8\u65af\u987a\u5e8f\u7684\u65f6\u95f4\u5197\u4f59\u6027\uff0c\u5e76\u8bbe\u8ba1\u4e13\u7528\u786c\u4ef6\u52a0\u901f\u5668\uff0c\u907f\u514d\u6bcf\u5e27\u4ece\u5934\u6392\u5e8f\uff0c\u4ec5\u8ddf\u8e2a\u548c\u66f4\u65b0\u6df1\u5ea6\u987a\u5e8f\u3002", "result": "Neo\u76f8\u8f83\u5f53\u524d\u6700\u5148\u8fdb\u7684\u8fb9\u7f18GPU\u548cASIC\u65b9\u6848\uff0c\u541e\u5410\u91cf\u5206\u522b\u63d0\u534710.0\u500d\u548c5.6\u500d\uff0c\u540c\u65f6DRAM\u6d41\u91cf\u51cf\u5c1194.5%\u548c81.3%\u3002", "conclusion": "Neo\u663e\u8457\u964d\u4f4e\u4e863DGS\u6e32\u67d3\u7684\u8ba1\u7b97\u4e0e\u5185\u5b58\u5f00\u9500\uff0c\u4f7f\u9ad8\u8d28\u91cf\u3001\u4f4e\u5ef6\u8fdf\u7684\u7aef\u4fa73D\u6e32\u67d3\u66f4\u5177\u53ef\u884c\u6027\u3002"}}
{"id": "2511.12856", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2511.12856", "abs": "https://arxiv.org/abs/2511.12856", "authors": ["Anuradha Madugalla", "Jixuan Dong", "Kai Lyne Loi", "Matthew Crossman", "John Grundy"], "title": "Human-Centred Requirements Engineering for Critical Systems: Insights from Disaster Early Warning Applications", "comment": null, "summary": "Critical systems, such as those used in healthcare, defence, and disaster management, demand rigorous requirements engineering to ensure safety and reliability. Yet, much of this rigour has traditionally focused on technical assurance, often overlooking the human and social contexts in which these systems operate. This paper argues that considering human-centric aspects is an essential dimension of dependability, and presents a human-centred RE process designed to integrate social responsibility into critical system development. Drawing from a literature review, we identified a set of guidelines for designing software for vulnerable communities and translated these into sixty-two functional and non-functional requirements. These requirements were operationalised through the design of an adaptive early warning system prototype, which was subsequently evaluated through six interviews and eight cognitive walkthroughs to validate their relevance and applicability. The findings demonstrate that human-centric requirements, when addressed early, enhance the usability and accessibility of systems for all users. The paper concludes by positioning human-centricity not as an ethical add-on but as a defining quality of safe and equitable critical systems.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u4ee5\u4eba\u4e3a\u672c\u7684\u9700\u6c42\u5de5\u7a0b\uff08RE\uff09\u6d41\u7a0b\uff0c\u5c06\u793e\u4f1a\u8d23\u4efb\u878d\u5165\u5173\u952e\u7cfb\u7edf\u5f00\u53d1\u4e2d\uff0c\u901a\u8fc7\u6587\u732e\u7efc\u8ff0\u63d0\u70bc\u51fa62\u9879\u529f\u80fd\u6027\u4e0e\u975e\u529f\u80fd\u6027\u9700\u6c42\uff0c\u5e76\u5728\u65e9\u671f\u9884\u8b66\u7cfb\u7edf\u539f\u578b\u4e2d\u5b9e\u73b0\uff0c\u7ecf\u7528\u6237\u8bc4\u4f30\u9a8c\u8bc1\u5176\u63d0\u5347\u7cfb\u7edf\u53ef\u7528\u6027\u4e0e\u53ef\u53ca\u6027\uff0c\u4e3b\u5f20\u4ee5\u4eba\u4e3a\u672c\u662f\u5173\u952e\u7cfb\u7edf\u5b89\u5168\u4e0e\u516c\u5e73\u7684\u6838\u5fc3\u5c5e\u6027\u3002", "motivation": "\u4f20\u7edf\u5173\u952e\u7cfb\u7edf\u7684\u9700\u6c42\u5de5\u7a0b\u8fc7\u4e8e\u4fa7\u91cd\u6280\u672f\u4fdd\u969c\uff0c\u5ffd\u89c6\u4e86\u7cfb\u7edf\u8fd0\u884c\u4e2d\u7684\u4eba\u7c7b\u4e0e\u793e\u4f1a\u80cc\u666f\uff1b\u4f5c\u8005\u8ba4\u4e3a\uff0c\u4ee5\u4eba\u4e3a\u672c\u7684\u8003\u91cf\u662f\u7cfb\u7edf\u53ef\u9760\u6027\u4e0d\u53ef\u6216\u7f3a\u7684\u7ef4\u5ea6\uff0c\u5c24\u5176\u5728\u670d\u52a1\u5f31\u52bf\u7fa4\u4f53\u65f6\u66f4\u9700\u7eb3\u5165\u793e\u4f1a\u8d23\u4efb\u3002", "method": "\u901a\u8fc7\u6587\u732e\u7efc\u8ff0\u8bc6\u522b\u9762\u5411\u5f31\u52bf\u7fa4\u4f53\u7684\u8f6f\u4ef6\u8bbe\u8ba1\u6307\u5357\uff0c\u5e76\u5c06\u5176\u8f6c\u5316\u4e3a62\u9879\u529f\u80fd\u4e0e\u975e\u529f\u80fd\u9700\u6c42\uff1b\u968f\u540e\u8bbe\u8ba1\u4e00\u4e2a\u81ea\u9002\u5e94\u65e9\u671f\u9884\u8b66\u7cfb\u7edf\u539f\u578b\uff0c\u5e76\u901a\u8fc76\u6b21\u8bbf\u8c08\u548c8\u6b21\u8ba4\u77e5\u8d70\u67e5\u8fdb\u884c\u8bc4\u4f30\u3002", "result": "\u8bc4\u4f30\u7ed3\u679c\u8868\u660e\uff0c\u5728\u9700\u6c42\u9636\u6bb5\u65e9\u671f\u7eb3\u5165\u4ee5\u4eba\u4e3a\u672c\u7684\u8981\u6c42\uff0c\u80fd\u663e\u8457\u63d0\u5347\u7cfb\u7edf\u5bf9\u6240\u6709\u7528\u6237\u7684\u53ef\u7528\u6027\u4e0e\u53ef\u8bbf\u95ee\u6027\u3002", "conclusion": "\u4ee5\u4eba\u4e3a\u672c\u4e0d\u5e94\u88ab\u89c6\u4e3a\u4f26\u7406\u9644\u52a0\u9879\uff0c\u800c\u5e94\u4f5c\u4e3a\u5b89\u5168\u3001\u516c\u5e73\u7684\u5173\u952e\u7cfb\u7edf\u7684\u6838\u5fc3\u8d28\u91cf\u5c5e\u6027\u3002"}}
{"id": "2511.12884", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2511.12884", "abs": "https://arxiv.org/abs/2511.12884", "authors": ["Worawalan Chatlatanagulchai", "Hao Li", "Yutaro Kashiwa", "Brittany Reid", "Kundjanasith Thonglek", "Pattara Leelaprute", "Arnon Rungsawang", "Bundit Manaskasemsak", "Bram Adams", "Ahmed E. Hassan", "Hajimu Iida"], "title": "Agent READMEs: An Empirical Study of Context Files for Agentic Coding", "comment": null, "summary": "Agentic coding tools receive goals written in natural language as input, break them down into specific tasks, and write or execute the actual code with minimal human intervention. Central to this process are agent context files (\"READMEs for agents\") that provide persistent, project-level instructions. In this paper, we conduct the first large-scale empirical study of 2,303 agent context files from 1,925 repositories to characterize their structure, maintenance, and content. We find that these files are not static documentation but complex, difficult-to-read artifacts that evolve like configuration code, maintained through frequent, small additions. Our content analysis of 16 instruction types shows that developers prioritize functional context, such as build and run commands (62.3%), implementation details (69.9%), and architecture (67.7%). We also identify a significant gap: non-functional requirements like security (14.5%) and performance (14.5%) are rarely specified. These findings indicate that while developers use context files to make agents functional, they provide few guardrails to ensure that agent-written code is secure or performant, highlighting the need for improved tooling and practices.", "AI": {"tldr": "\u672c\u6587\u9996\u6b21\u5bf92303\u4e2a\u667a\u80fd\u4f53\u4e0a\u4e0b\u6587\u6587\u4ef6\u8fdb\u884c\u4e86\u5927\u89c4\u6a21\u5b9e\u8bc1\u7814\u7a76\uff0c\u53d1\u73b0\u5f00\u53d1\u8005\u4e3b\u8981\u5173\u6ce8\u529f\u80fd\u6027\u5185\u5bb9\uff08\u5982\u6784\u5efa\u547d\u4ee4\u3001\u5b9e\u73b0\u7ec6\u8282\u548c\u67b6\u6784\uff09\uff0c\u800c\u5f88\u5c11\u6307\u5b9a\u5b89\u5168\u6027\u4e0e\u6027\u80fd\u7b49\u975e\u529f\u80fd\u6027\u9700\u6c42\uff0c\u8868\u660e\u5f53\u524d\u5de5\u5177\u5728\u4fdd\u969c\u4ee3\u7801\u8d28\u91cf\u65b9\u9762\u5b58\u5728\u660e\u663e\u4e0d\u8db3\u3002", "motivation": "\u7406\u89e3\u667a\u80fd\u4f53\u4e0a\u4e0b\u6587\u6587\u4ef6\u7684\u5b9e\u9645\u4f7f\u7528\u60c5\u51b5\u53ca\u5176\u5728\u6307\u5bfc\u667a\u80fd\u7f16\u7801\u5de5\u5177\u4e2d\u7684\u4f5c\u7528\uff0c\u5c24\u5176\u662f\u8bc6\u522b\u5f53\u524d\u5b9e\u8df5\u4e2d\u53ef\u80fd\u5b58\u5728\u7684\u5b89\u5168\u6027\u548c\u6027\u80fd\u4fdd\u969c\u7f3a\u5931\u95ee\u9898\u3002", "method": "\u5bf9\u6765\u81ea1925\u4e2a\u4ee3\u7801\u4ed3\u5e93\u76842303\u4e2a\u667a\u80fd\u4f53\u4e0a\u4e0b\u6587\u6587\u4ef6\u8fdb\u884c\u5927\u89c4\u6a21\u5b9e\u8bc1\u5206\u6790\uff0c\u5305\u62ec\u7ed3\u6784\u3001\u7ef4\u62a4\u6a21\u5f0f\u548c\u5185\u5bb9\u7c7b\u578b\uff0c\u5e76\u5bf916\u79cd\u6307\u4ee4\u7c7b\u578b\u8fdb\u884c\u5206\u7c7b\u7edf\u8ba1\u3002", "result": "\u4e0a\u4e0b\u6587\u6587\u4ef6\u5e76\u975e\u9759\u6001\u6587\u6863\uff0c\u800c\u662f\u7c7b\u4f3c\u914d\u7f6e\u4ee3\u7801\u7684\u52a8\u6001\u4ea7\u7269\uff1b62.3%\u5305\u542b\u6784\u5efa/\u8fd0\u884c\u547d\u4ee4\uff0c69.9%\u542b\u5b9e\u73b0\u7ec6\u8282\uff0c67.7%\u63cf\u8ff0\u67b6\u6784\uff1b\u4f46\u4ec514.5%\u63d0\u53ca\u5b89\u5168\u6027\u6216\u6027\u80fd\u3002", "conclusion": "\u5f53\u524d\u5f00\u53d1\u8005\u4e3b\u8981\u5229\u7528\u4e0a\u4e0b\u6587\u6587\u4ef6\u786e\u4fdd\u667a\u80fd\u4f53\u529f\u80fd\u6b63\u786e\u6027\uff0c\u5374\u5ffd\u89c6\u4e86\u5bf9\u5176\u751f\u6210\u4ee3\u7801\u7684\u5b89\u5168\u6027\u548c\u6027\u80fd\u7ea6\u675f\uff0c\u4e9f\u9700\u6539\u8fdb\u76f8\u5173\u5de5\u5177\u4e0e\u5b9e\u8df5\u4ee5\u63d0\u4f9b\u66f4\u5168\u9762\u7684\u4fdd\u969c\u3002"}}
{"id": "2511.13343", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2511.13343", "abs": "https://arxiv.org/abs/2511.13343", "authors": ["A Cormier", "David Roqui", "Fabrice Surma", "Martin Labour\u00e9", "Jean-Marc Vallet", "Odile Guillon", "N Grozavu", "Ann Bourg\u00e8s"], "title": "Coliseum project: Correlating climate change data with the behavior of heritage materials", "comment": null, "summary": "Heritage materials are already affected by climate change, and increasing climatic variations reduces the lifespan of monuments. As weathering depends on many factors, it is also difficult to link its progression to climatic changes. To predict weathering, it is essential to gather climatic data while simultaneously monitoring the progression of deterioration. The multimodal nature of collected data (images, text{\\ldots}) makes correlations difficult, particularly on different time scales. To address this issue, the COLISEUM project proposes a methodology for collecting data in three French sites to predict heritage material behaviour using artificial intelligence computer models. Over time, prediction models will allow the prediction of future material behaviours using known data from different climate change scenarios by the IPCC (Intergovernmental Panel on Climate Change). Thus, a climate monitoring methodology has been set up in three cultural sites in France: Notre-Dame cathedral in Strasbourg ( 67), Bibracte archaeological site (71), and the Saint-Pierre chapel in Villefranche-sur-Mer (06). Each site has a different climate and specific materials. In situ, microclimatic sensors continuously record variations parameters over time. The state of alteration is monitored at regular intervals by means of chemical analyses, cartographic measurements and scientific imaging campaigns. To implement weathering models, data is gathered in alteration matrix by mean of a calculated weathering index. This article presents the instrumentation methodology, the initial diagnostic and the first results with the example of Strasbourg Cathedral site.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86COLISEUM\u9879\u76ee\u5728\u6cd5\u56fd\u4e09\u4e2a\u6587\u5316\u9057\u4ea7\u5730\uff08\u65af\u7279\u62c9\u65af\u5821\u5723\u6bcd\u9662\u3001\u6bd4\u5e03\u62c9\u514b\u7279\u8003\u53e4\u9057\u5740\u548c\u7ef4\u5c14\u5f17\u6717\u4ec0-\u82cf\u5c14-\u6885\u5c14\u5723\u76ae\u57c3\u5c14\u5c0f\u6559\u5802\uff09\u5f00\u5c55\u7684\u591a\u6a21\u6001\u6570\u636e\u91c7\u96c6\u4e0e\u6c14\u5019\u76d1\u6d4b\u65b9\u6cd5\uff0c\u65e8\u5728\u901a\u8fc7\u4eba\u5de5\u667a\u80fd\u6a21\u578b\u9884\u6d4b\u6c14\u5019\u53d8\u5316\u5bf9\u9057\u4ea7\u6750\u6599\u52a3\u5316\u7684\u5f71\u54cd\uff0c\u5e76\u5c55\u793a\u4e86\u65af\u7279\u62c9\u65af\u5821\u5927\u6559\u5802\u7684\u521d\u6b65\u8bca\u65ad\u4e0e\u7ed3\u679c\u3002", "motivation": "\u6c14\u5019\u53d8\u5316\u6b63\u52a0\u901f\u9057\u4ea7\u6750\u6599\u7684\u52a3\u5316\uff0c\u4f46\u7531\u4e8e\u98ce\u5316\u8fc7\u7a0b\u53d7\u591a\u79cd\u56e0\u7d20\u5f71\u54cd\u4e14\u6570\u636e\u5177\u6709\u591a\u6a21\u6001\u7279\u6027\uff0c\u96be\u4ee5\u5efa\u7acb\u5176\u4e0e\u6c14\u5019\u53d8\u5316\u4e4b\u95f4\u7684\u660e\u786e\u5173\u8054\u3002\u56e0\u6b64\uff0c\u4e9f\u9700\u4e00\u79cd\u7cfb\u7edf\u6027\u65b9\u6cd5\u6765\u6574\u5408\u6c14\u5019\u4e0e\u52a3\u5316\u6570\u636e\uff0c\u4ee5\u9884\u6d4b\u672a\u6765\u4e0d\u540c\u6c14\u5019\u60c5\u666f\u4e0b\u9057\u4ea7\u6750\u6599\u7684\u884c\u4e3a\u3002", "method": "\u5728\u4e09\u4e2a\u5177\u6709\u4e0d\u540c\u6c14\u5019\u548c\u6750\u6599\u7279\u5f81\u7684\u6cd5\u56fd\u6587\u5316\u9057\u4ea7\u5730\u90e8\u7f72\u5fae\u6c14\u5019\u4f20\u611f\u5668\uff0c\u6301\u7eed\u8bb0\u5f55\u73af\u5883\u53c2\u6570\uff1b\u540c\u65f6\u5b9a\u671f\u901a\u8fc7\u5316\u5b66\u5206\u6790\u3001\u6d4b\u7ed8\u6d4b\u91cf\u548c\u79d1\u5b66\u6210\u50cf\u624b\u6bb5\u76d1\u6d4b\u6750\u6599\u52a3\u5316\u72b6\u6001\uff1b\u5c06\u591a\u6e90\u6570\u636e\u6574\u5408\u4e3a\u52a3\u5316\u77e9\u9635\uff0c\u5e76\u8ba1\u7b97\u98ce\u5316\u6307\u6570\uff0c\u7528\u4e8e\u6784\u5efa\u57fa\u4e8e\u4eba\u5de5\u667a\u80fd\u7684\u9884\u6d4b\u6a21\u578b\u3002", "result": "\u6587\u7ae0\u5c55\u793a\u4e86\u65af\u7279\u62c9\u65af\u5821\u5927\u6559\u5802\u7ad9\u70b9\u7684\u4eea\u5668\u90e8\u7f72\u65b9\u6cd5\u3001\u521d\u59cb\u8bca\u65ad\u7ed3\u679c\u53ca\u521d\u6b65\u6570\u636e\u5206\u6790\uff0c\u9a8c\u8bc1\u4e86\u6240\u63d0\u591a\u6a21\u6001\u6570\u636e\u91c7\u96c6\u4e0e\u6574\u5408\u65b9\u6cd5\u7684\u53ef\u884c\u6027\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u9884\u6d4b\u6c14\u5019\u53d8\u5316\u5bf9\u6587\u5316\u9057\u4ea7\u6750\u6599\u7684\u957f\u671f\u5f71\u54cd\u63d0\u4f9b\u4e86\u53ef\u884c\u7684\u6280\u672f\u8def\u5f84\uff0c\u672a\u6765\u53ef\u7ed3\u5408IPCC\u6c14\u5019\u60c5\u666f\u8fdb\u4e00\u6b65\u4f18\u5316AI\u6a21\u578b\uff0c\u652f\u6301\u9057\u4ea7\u4fdd\u62a4\u51b3\u7b56\u3002"}}
{"id": "2511.12950", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2511.12950", "abs": "https://arxiv.org/abs/2511.12950", "authors": ["Zirui Chen", "Zhipeng Xue", "Jiayuan Zhou", "Xing Hu", "Xin Xia", "Xiaohu Yang"], "title": "Diffploit: Facilitating Cross-Version Exploit Migration for Open Source Library Vulnerabilities", "comment": null, "summary": "Exploits are commonly used to demonstrate the presence of library vulnerabilities and validate their impact across different versions. However, their direct application to alternative versions often fails due to breaking changes introduced during evolution. These failures stem from both changes in triggering conditions (e.g., API refactorings) and broken dynamic environments (e.g., build or runtime errors), which are challenging to interpret and adapt manually. Existing techniques primarily focus on code-level trace alignment through fuzzing, which is both time-consuming and insufficient for handling environment-level failures. Moreover, they often fall short when dealing with complicated triggering condition changes across versions. To overcome this, we propose Diffploit, an iterative, diff-driven exploit migration method structured around two key modules: the Context Module and the Migration Module. The Context Module dynamically constructs contexts derived from analyzing behavioral discrepancies between the target and reference versions, which capture the failure symptom and its related diff hunks. Leveraging these contexts, the Migration Module guides an LLM-based adaptation through an iterative feedback loop, balancing exploration of diff candidates and gradual refinement to resolve reproduction failures effectively. We evaluate Diffploit on a large-scale dataset containing 102 Java CVEs and 689 version-migration tasks across 79 libraries. Diffploit successfully migrates 84.2% exploits, outperforming the change-aware test repair tool TARGET by 52.0% and the rule-based tool in IDEA by 61.6%. Beyond technical effectiveness, Diffploit identifies 5 CVE reports with incorrect affected version ranges, three of which have been confirmed. It also discovers 111 unreported vulnerable versions in the GitHub Advisory Database.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faDiffploit\uff0c\u4e00\u79cd\u57fa\u4e8e\u5dee\u5f02\u9a71\u52a8\u7684\u8fed\u4ee3\u5f0f\u6f0f\u6d1e\u5229\u7528\u8fc1\u79fb\u65b9\u6cd5\uff0c\u901a\u8fc7\u4e0a\u4e0b\u6587\u6a21\u5757\u548c\u8fc1\u79fb\u6a21\u5757\u534f\u540c\u5de5\u4f5c\uff0c\u6709\u6548\u89e3\u51b3\u56e0\u5e93\u7248\u672c\u6f14\u5316\u5bfc\u81f4\u7684\u6f0f\u6d1e\u5229\u7528\u5931\u8d25\u95ee\u9898\uff0c\u5728\u5927\u89c4\u6a21\u5b9e\u9a8c\u4e2d\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u5de5\u5177\uff0c\u5e76\u53d1\u73b0\u591a\u4e2aCVE\u62a5\u544a\u9519\u8bef\u548c\u672a\u8bb0\u5f55\u7684\u6f0f\u6d1e\u7248\u672c\u3002", "motivation": "\u73b0\u6709\u6f0f\u6d1e\u5229\u7528\u8fc1\u79fb\u65b9\u6cd5\u4e3b\u8981\u4f9d\u8d56\u4ee3\u7801\u7ea7\u8ffd\u8e2a\u5bf9\u9f50\uff0c\u8017\u65f6\u4e14\u96be\u4ee5\u5904\u7406\u73af\u5883\u7ea7\u5931\u8d25\uff0c\u5c24\u5176\u5728\u9762\u5bf9\u590d\u6742\u89e6\u53d1\u6761\u4ef6\u53d8\u5316\u65f6\u6548\u679c\u4e0d\u4f73\u3002\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u80fd\u540c\u65f6\u5e94\u5bf9\u89e6\u53d1\u6761\u4ef6\u53d8\u66f4\u548c\u52a8\u6001\u73af\u5883\u7834\u574f\u7684\u9ad8\u6548\u8fc1\u79fb\u65b9\u6cd5\u3002", "method": "Diffploit\u5305\u542b\u4e24\u4e2a\u6838\u5fc3\u6a21\u5757\uff1a\u4e0a\u4e0b\u6587\u6a21\u5757\u901a\u8fc7\u5206\u6790\u76ee\u6807\u7248\u672c\u4e0e\u53c2\u8003\u7248\u672c\u95f4\u7684\u884c\u4e3a\u5dee\u5f02\uff0c\u52a8\u6001\u6784\u5efa\u5305\u542b\u5931\u8d25\u75c7\u72b6\u53ca\u76f8\u5173\u5dee\u5f02\u7247\u6bb5\u7684\u4e0a\u4e0b\u6587\uff1b\u8fc1\u79fb\u6a21\u5757\u5219\u5229\u7528\u8fd9\u4e9b\u4e0a\u4e0b\u6587\uff0c\u901a\u8fc7\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u8fed\u4ee3\u53cd\u9988\u673a\u5236\uff0c\u5e73\u8861\u5dee\u5f02\u5019\u9009\u63a2\u7d22\u4e0e\u9010\u6b65\u4f18\u5316\uff0c\u5b9e\u73b0\u6f0f\u6d1e\u5229\u7528\u7684\u6709\u6548\u8fc1\u79fb\u3002", "result": "\u5728\u5305\u542b102\u4e2aJava CVE\u548c689\u4e2a\u7248\u672c\u8fc1\u79fb\u4efb\u52a1\u7684\u5927\u89c4\u6a21\u6570\u636e\u96c6\u4e0a\uff0cDiffploit\u6210\u529f\u8fc1\u79fb\u4e8684.2%\u7684\u6f0f\u6d1e\u5229\u7528\uff0c\u6bd4TARGET\u5de5\u5177\u9ad852.0%\uff0c\u6bd4IDEA\u4e2d\u7684\u89c4\u5219\u5de5\u5177\u9ad861.6%\u3002\u6b64\u5916\uff0c\u8fd8\u8bc6\u522b\u51fa5\u4e2aCVE\u53d7\u5f71\u54cd\u7248\u672c\u8303\u56f4\u9519\u8bef\uff08\u5176\u4e2d3\u4e2a\u5df2\u786e\u8ba4\uff09\uff0c\u5e76\u53d1\u73b0GitHub Advisory Database\u4e2d111\u4e2a\u672a\u62a5\u544a\u7684\u6f0f\u6d1e\u7248\u672c\u3002", "conclusion": "Diffploit\u901a\u8fc7\u7ed3\u5408\u5dee\u5f02\u5206\u6790\u4e0eLLM\u9a71\u52a8\u7684\u8fed\u4ee3\u8fc1\u79fb\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8de8\u7248\u672c\u6f0f\u6d1e\u5229\u7528\u8fc1\u79fb\u7684\u6548\u7387\u4e0e\u51c6\u786e\u6027\uff0c\u4e0d\u4ec5\u9a8c\u8bc1\u4e86\u5176\u6280\u672f\u4f18\u8d8a\u6027\uff0c\u8fd8\u5728\u5b9e\u9645\u5b89\u5168\u6570\u636e\u5e93\u4e2d\u53d1\u73b0\u4e86\u91cd\u8981\u6f0f\u6d1e\u4fe1\u606f\u9519\u8bef\u4e0e\u9057\u6f0f\u3002"}}
{"id": "2511.13676", "categories": ["cs.AR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.13676", "abs": "https://arxiv.org/abs/2511.13676", "authors": ["Hyunwoo Oh", "KyungIn Nam", "Rajat Bhattacharjya", "Hanning Chen", "Tamoghno Das", "Sanggeon Yun", "Suyeon Jang", "Andrew Ding", "Nikil Dutt", "Mohsen Imani"], "title": "T-SAR: A Full-Stack Co-design for CPU-Only Ternary LLM Inference via In-Place SIMD ALU Reorganization", "comment": "Accepted to DATE 2026", "summary": "Recent advances in LLMs have outpaced the computational and memory capacities of edge platforms that primarily employ CPUs, thereby challenging efficient and scalable deployment. While ternary quantization enables significant resource savings, existing CPU solutions rely heavily on memory-based lookup tables (LUTs) which limit scalability, and FPGA or GPU accelerators remain impractical for edge use. This paper presents T-SAR, the first framework to achieve scalable ternary LLM inference on CPUs by repurposing the SIMD register file for dynamic, in-register LUT generation with minimal hardware modifications. T-SAR eliminates memory bottlenecks and maximizes data-level parallelism, delivering 5.6-24.5x and 1.1-86.2x improvements in GEMM latency and GEMV throughput, respectively, with only 3.2% power and 1.4% area overheads in SIMD units. T-SAR achieves up to 2.5-4.9x the energy efficiency of an NVIDIA Jetson AGX Orin, establishing a practical approach for efficient LLM inference on edge platforms.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86T-SAR\uff0c\u9996\u4e2a\u5728CPU\u4e0a\u5b9e\u73b0\u53ef\u6269\u5c55\u4e09\u503c\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u63a8\u7406\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u590d\u7528SIMD\u5bc4\u5b58\u5668\u6587\u4ef6\u52a8\u6001\u751f\u6210\u67e5\u627e\u8868\uff0c\u663e\u8457\u63d0\u5347\u80fd\u6548\u4e0e\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u8fb9\u7f18\u8bbe\u5907\u4e3b\u8981\u4f9d\u8d56CPU\uff0c\u96be\u4ee5\u627f\u8f7d\u65e5\u76ca\u5e9e\u5927\u7684LLM\uff1b\u867d\u7136\u4e09\u503c\u91cf\u5316\u53ef\u8282\u7701\u8d44\u6e90\uff0c\u4f46\u5f53\u524dCPU\u65b9\u6848\u4f9d\u8d56\u5185\u5b58\u4e2d\u7684\u67e5\u627e\u8868\uff08LUT\uff09\uff0c\u9650\u5236\u4e86\u53ef\u6269\u5c55\u6027\uff0c\u800cFPGA\u6216GPU\u52a0\u901f\u5668\u53c8\u4e0d\u9002\u5408\u8fb9\u7f18\u573a\u666f\u3002", "method": "T-SAR\u901a\u8fc7\u5c11\u91cf\u786c\u4ef6\u4fee\u6539\uff0c\u5c06SIMD\u5bc4\u5b58\u5668\u6587\u4ef6\u7528\u4e8e\u52a8\u6001\u3001\u5bc4\u5b58\u5668\u5185\u7684LUT\u751f\u6210\uff0c\u6d88\u9664\u5185\u5b58\u74f6\u9888\u5e76\u6700\u5927\u5316\u6570\u636e\u7ea7\u5e76\u884c\u6027\u3002", "result": "\u5728GEMM\u5ef6\u8fdf\u548cGEMV\u541e\u5410\u91cf\u4e0a\u5206\u522b\u5b9e\u73b05.6\u201324.5\u500d\u548c1.1\u201386.2\u500d\u7684\u63d0\u5347\uff0cSIMD\u5355\u5143\u4ec5\u589e\u52a03.2%\u529f\u8017\u548c1.4%\u9762\u79ef\u5f00\u9500\uff1b\u76f8\u6bd4NVIDIA Jetson AGX Orin\uff0c\u80fd\u6548\u63d0\u5347\u8fbe2.5\u20134.9\u500d\u3002", "conclusion": "T-SAR\u4e3a\u8fb9\u7f18\u5e73\u53f0\u4e0a\u7684\u9ad8\u6548LLM\u63a8\u7406\u63d0\u4f9b\u4e86\u4e00\u79cd\u5b9e\u7528\u4e14\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2511.12993", "categories": ["cs.SE", "cs.CR"], "pdf": "https://arxiv.org/pdf/2511.12993", "abs": "https://arxiv.org/abs/2511.12993", "authors": ["Longfei Chen", "Ruibin Yan", "Taiyu Wong", "Yiyang Chen", "Chao Zhang"], "title": "SmartPoC: Generating Executable and Validated PoCs for Smart Contract Bug Reports", "comment": null, "summary": "Smart contracts are prone to vulnerabilities and are analyzed by experts as well as automated systems, such as static analysis and AI-assisted solutions. However, audit artifacts are heterogeneous and often lack reproducible, executable PoC tests suitable for automated validation, leading to costly, ad hoc manual verification. Large language models (LLMs) can be leveraged to turn audit reports into PoC test cases, but have three major challenges: noisy inputs, hallucinations, and missing runtime oracles. In this paper, we present SmartPoC, an automated framework that converts textual audit reports into executable, validated test cases. First, the input audit report is processed to reduce noise, and only bug-related functions are extracted and fed to LLMs as context. To curb hallucinations and ensure compile-and-run readiness, we leverage LLMs to synthesize PoC test cases with specially-designed pre-/post-execution repair. We further utilize differential verification as oracles to confirm exploitability of the PoC test cases. On the SmartBugs-Vul and FORGE-Vul benchmarks, SmartPoC generates executable, validated Foundry test cases for 85.61% and 86.45% of targets, respectively. Applied to the latest Etherscan verified-source corpus, SmartPoC confirms 236 real bugs out of 545 audit findings at a cost of only $0.03 per finding.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86SmartPoC\u6846\u67b6\uff0c\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\u5c06\u667a\u80fd\u5408\u7ea6\u5ba1\u8ba1\u62a5\u544a\u81ea\u52a8\u8f6c\u5316\u4e3a\u53ef\u6267\u884c\u4e14\u7ecf\u8fc7\u9a8c\u8bc1\u7684PoC\u6d4b\u8bd5\u7528\u4f8b\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u8f93\u5165\u566a\u58f0\u3001\u6a21\u578b\u5e7b\u89c9\u548c\u7f3a\u5c11\u8fd0\u884c\u65f6\u9884\u8a00\u673a\u4e09\u5927\u6311\u6218\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u4e0a\u5b9e\u73b0\u4e86\u9ad8\u6210\u529f\u7387\u548c\u4f4e\u6210\u672c\u9a8c\u8bc1\u3002", "motivation": "\u667a\u80fd\u5408\u7ea6\u5ba1\u8ba1\u62a5\u544a\u901a\u5e38\u7f3a\u4e4f\u53ef\u590d\u73b0\u3001\u53ef\u6267\u884c\u7684PoC\u6d4b\u8bd5\u7528\u4f8b\uff0c\u5bfc\u81f4\u81ea\u52a8\u5316\u9a8c\u8bc1\u56f0\u96be\uff0c\u4f9d\u8d56\u6602\u8d35\u4e14\u4e34\u65f6\u7684\u624b\u52a8\u9a8c\u8bc1\uff1b\u73b0\u6709\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u65b9\u6cd5\u9762\u4e34\u566a\u58f0\u8f93\u5165\u3001\u5e7b\u89c9\u548c\u7f3a\u5c11\u8fd0\u884c\u65f6\u9884\u8a00\u673a\u7b49\u95ee\u9898\u3002", "method": "SmartPoC\u9996\u5148\u5bf9\u5ba1\u8ba1\u62a5\u544a\u8fdb\u884c\u9884\u5904\u7406\u4ee5\u51cf\u5c11\u566a\u58f0\u5e76\u63d0\u53d6\u4e0e\u6f0f\u6d1e\u76f8\u5173\u7684\u51fd\u6570\u4f5c\u4e3a\u4e0a\u4e0b\u6587\uff1b\u7136\u540e\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\u751f\u6210PoC\u6d4b\u8bd5\u7528\u4f8b\uff0c\u5e76\u901a\u8fc7\u4e13\u95e8\u8bbe\u8ba1\u7684\u6267\u884c\u524d\u540e\u4fee\u590d\u673a\u5236\u786e\u4fdd\u5176\u53ef\u7f16\u8bd1\u548c\u53ef\u8fd0\u884c\uff1b\u6700\u540e\u91c7\u7528\u5dee\u5206\u9a8c\u8bc1\u4f5c\u4e3a\u9884\u8a00\u673a\u6765\u786e\u8ba4\u6f0f\u6d1e\u7684\u53ef\u5229\u7528\u6027\u3002", "result": "\u5728SmartBugs-Vul\u548cFORGE-Vul\u57fa\u51c6\u4e0a\uff0cSmartPoC\u5206\u522b\u5bf985.61%\u548c86.45%\u7684\u76ee\u6807\u751f\u6210\u4e86\u53ef\u6267\u884c\u4e14\u5df2\u9a8c\u8bc1\u7684Foundry\u6d4b\u8bd5\u7528\u4f8b\uff1b\u5728Etherscan\u6700\u65b0\u9a8c\u8bc1\u6e90\u4ee3\u7801\u8bed\u6599\u5e93\u4e2d\uff0c\u4ee5\u6bcf\u9879\u53d1\u73b0\u4ec50.03\u7f8e\u5143\u7684\u6210\u672c\u786e\u8ba4\u4e86545\u9879\u5ba1\u8ba1\u53d1\u73b0\u4e2d\u7684236\u4e2a\u771f\u5b9e\u6f0f\u6d1e\u3002", "conclusion": "SmartPoC\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u3001\u4f4e\u6210\u672c\u7684\u81ea\u52a8\u5316\u65b9\u6cd5\uff0c\u80fd\u5c06\u975e\u7ed3\u6784\u5316\u5ba1\u8ba1\u62a5\u544a\u8f6c\u5316\u4e3a\u53ef\u9a8c\u8bc1\u7684PoC\u6d4b\u8bd5\uff0c\u663e\u8457\u63d0\u5347\u4e86\u667a\u80fd\u5408\u7ea6\u6f0f\u6d1e\u9a8c\u8bc1\u7684\u53ef\u6269\u5c55\u6027\u548c\u53ef\u9760\u6027\u3002"}}
{"id": "2511.13679", "categories": ["cs.AR", "cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.13679", "abs": "https://arxiv.org/abs/2511.13679", "authors": ["Hyunwoo Oh", "Hanning Chen", "Sanggeon Yun", "Yang Ni", "Wenjun Huang", "Tamoghno Das", "Suyeon Jang", "Mohsen Imani"], "title": "QUILL: An Algorithm-Architecture Co-Design for Cache-Local Deformable Attention", "comment": "Accepted to DATE 2026", "summary": "Deformable transformers deliver state-of-the-art detection but map poorly to hardware due to irregular memory access and low arithmetic intensity. We introduce QUILL, a schedule-aware accelerator that turns deformable attention into cache-friendly, single-pass work. At its core, Distance-based Out-of-Order Querying (DOOQ) orders queries by spatial proximity; the look-ahead drives a region prefetch into an alternate buffer--forming a schedule-aware prefetch loop that overlaps memory and compute. A fused MSDeformAttn engine executes interpolation, Softmax, aggregation, and the final projection (W''m) in one pass without spilling intermediates, while small tensors are kept on-chip and surrounding dense layers run on integrated GEMMs. Implemented as RTL and evaluated end-to-end, QUILL achieves up to 7.29x higher throughput and 47.3x better energy efficiency than an RTX 4090, and exceeds prior accelerators by 3.26-9.82x in throughput and 2.01-6.07x in energy efficiency. With mixed-precision quantization, accuracy tracks FP32 within <=0.9 AP across Deformable and Sparse DETR variants. By converting sparsity into locality--and locality into utilization--QUILL delivers consistent, end-to-end speedups.", "AI": {"tldr": "QUILL\u662f\u4e00\u79cd\u9762\u5411\u53ef\u53d8\u5f62\u6ce8\u610f\u529b\u7684\u8c03\u5ea6\u611f\u77e5\u52a0\u901f\u5668\uff0c\u901a\u8fc7\u57fa\u4e8e\u8ddd\u79bb\u7684\u4e71\u5e8f\u67e5\u8be2\uff08DOOQ\uff09\u548c\u878d\u5408\u8ba1\u7b97\u5f15\u64ce\uff0c\u663e\u8457\u63d0\u5347\u541e\u5410\u91cf\u4e0e\u80fd\u6548\uff0c\u540c\u65f6\u4fdd\u6301\u9ad8\u7cbe\u5ea6\u3002", "motivation": "\u53ef\u53d8\u5f62Transformer\u867d\u7136\u5728\u68c0\u6d4b\u4efb\u52a1\u4e2d\u6027\u80fd\u4f18\u8d8a\uff0c\u4f46\u5176\u4e0d\u89c4\u5219\u5185\u5b58\u8bbf\u95ee\u548c\u4f4e\u7b97\u672f\u5f3a\u5ea6\u5bfc\u81f4\u96be\u4ee5\u9ad8\u6548\u6620\u5c04\u5230\u786c\u4ef6\u4e0a\u3002", "method": "\u63d0\u51faQUILL\u52a0\u901f\u5668\uff0c\u91c7\u7528DOOQ\u7b56\u7565\u6309\u7a7a\u95f4\u90bb\u8fd1\u6027\u6392\u5e8f\u67e5\u8be2\uff0c\u5e76\u7ed3\u5408\u9884\u53d6\u673a\u5236\u5f62\u6210\u8c03\u5ea6\u611f\u77e5\u7684\u9884\u53d6\u5faa\u73af\uff1b\u540c\u65f6\u8bbe\u8ba1\u878d\u5408\u7684MSDeformAttn\u5f15\u64ce\uff0c\u5728\u5355\u6b21\u904d\u5386\u4e2d\u5b8c\u6210\u63d2\u503c\u3001Softmax\u3001\u805a\u5408\u548c\u6295\u5f71\uff0c\u907f\u514d\u4e2d\u95f4\u6570\u636e\u6ea2\u51fa\uff0c\u5e76\u5229\u7528\u7247\u4e0a\u5b58\u50a8\u548c\u96c6\u6210GEMM\u5904\u7406\u5bc6\u96c6\u5c42\u3002", "result": "QUILL\u76f8\u6bd4RTX 4090\u6700\u9ad8\u5b9e\u73b07.29\u500d\u541e\u5410\u91cf\u63d0\u5347\u548c47.3\u500d\u80fd\u6548\u63d0\u5347\uff0c\u4f18\u4e8e\u5148\u524d\u52a0\u901f\u56683.26\u20139.82\u500d\u541e\u5410\u91cf\u548c2.01\u20136.07\u500d\u80fd\u6548\uff1b\u5728\u6df7\u5408\u7cbe\u5ea6\u91cf\u5316\u4e0b\uff0c\u7cbe\u5ea6\u635f\u5931\u4e0d\u8d85\u8fc70.9 AP\u3002", "conclusion": "QUILL\u901a\u8fc7\u5c06\u7a00\u758f\u6027\u8f6c\u5316\u4e3a\u5c40\u90e8\u6027\u3001\u518d\u5c06\u5c40\u90e8\u6027\u8f6c\u5316\u4e3a\u786c\u4ef6\u5229\u7528\u7387\uff0c\u5b9e\u73b0\u4e86\u7aef\u5230\u7aef\u4e00\u81f4\u7684\u52a0\u901f\u6548\u679c\uff0c\u5728\u4fdd\u6301\u7cbe\u5ea6\u7684\u540c\u65f6\u5927\u5e45\u63d0\u5347\u6027\u80fd\u4e0e\u80fd\u6548\u3002"}}
{"id": "2511.13069", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2511.13069", "abs": "https://arxiv.org/abs/2511.13069", "authors": ["Zhenyu Mao", "Jacky Keung", "Yicheng Sun", "Yifei Wang", "Shuo Liu", "Jialong Li"], "title": "Towards Requirements Engineering for GenAI-Enabled Software: Bridging Responsibility Gaps through Human Oversight Requirements", "comment": null, "summary": "Context: Responsibility gaps, long-recognized challenges in socio-technical systems where accountability becomes diffuse or ambiguous, have become increasingly pronounced in GenAI-enabled software. The generative and adaptive nature complicates how human oversight and responsibility are specified, delegated, and traced. Existing requirements engineering (RE) approaches remain limited in addressing these phenomena, revealing conceptual, methodological, and artifact-level research gaps.. Objective: This study aims to analyze these research gaps in the context of GenAI-enabled software systems. It seeks to establish a coherent perspective for a systematic analysis of responsibility gaps from a human oversight requirements standpoint, encompassing how these responsibility gaps should be conceptualized, identified, and represented throughout the RE process. Methods: The proposed design methodology is structured across three analytical layers. At the conceptualization layer, it establishes a conceptual framing that defines the key elements of responsibility across the human and system dimensions and explains how potential responsibility gaps emerge from their interactions. At the methodological layer, it introduces a deductive pipeline for identifying responsibility gaps by analyzing interactions between these dimensions and deriving corresponding oversight requirements within established RE frameworks. At the artifact layer, it formalizes the results in a Deductive Backbone Table, a reusable representation that traces the reasoning path from responsibility gaps identification to human oversight requirements derivation. Results: A user study compared the proposed methodology with a baseline goal-oriented RE across two scenarios. Evaluation across six dimensions indicated clear improvements of the proposed methodology, confirming its effectiveness in addressing three research gaps.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u4e09\u5c42\u8bbe\u8ba1\u65b9\u6cd5\u8bba\uff0c\u7528\u4e8e\u5728\u751f\u6210\u5f0f\u4eba\u5de5\u667a\u80fd\uff08GenAI\uff09\u8f6f\u4ef6\u7cfb\u7edf\u4e2d\u7cfb\u7edf\u5316\u5730\u8bc6\u522b\u3001\u6982\u5ff5\u5316\u548c\u8868\u793a\u8d23\u4efb\u7f3a\u53e3\uff0c\u5e76\u901a\u8fc7\u7528\u6237\u7814\u7a76\u9a8c\u8bc1\u4e86\u5176\u76f8\u8f83\u4e8e\u4f20\u7edf\u76ee\u6807\u5bfc\u5411\u9700\u6c42\u5de5\u7a0b\u65b9\u6cd5\u7684\u4f18\u8d8a\u6027\u3002", "motivation": "\u73b0\u6709\u9700\u6c42\u5de5\u7a0b\u65b9\u6cd5\u5728\u5e94\u5bf9GenAI\u7cfb\u7edf\u4e2d\u65e5\u76ca\u7a81\u51fa\u7684\u8d23\u4efb\u7f3a\u53e3\u95ee\u9898\u65f6\u5b58\u5728\u6982\u5ff5\u3001\u65b9\u6cd5\u548c\u5de5\u4ef6\u5c42\u9762\u7684\u4e0d\u8db3\uff0c\u4e9f\u9700\u5efa\u7acb\u4ece\u4eba\u7c7b\u76d1\u7763\u9700\u6c42\u89d2\u5ea6\u7cfb\u7edf\u5206\u6790\u8d23\u4efb\u7f3a\u53e3\u7684\u65b0\u89c6\u89d2\u3002", "method": "\u8be5\u65b9\u6cd5\u8bba\u5305\u542b\u4e09\u4e2a\u5206\u6790\u5c42\uff1a\u6982\u5ff5\u5c42\u5b9a\u4e49\u4eba\u4e0e\u7cfb\u7edf\u7ef4\u5ea6\u4e2d\u7684\u8d23\u4efb\u8981\u7d20\u53ca\u5176\u4ea4\u4e92\u4ea7\u751f\u7684\u8d23\u4efb\u7f3a\u53e3\uff1b\u65b9\u6cd5\u5c42\u901a\u8fc7\u6f14\u7ece\u6d41\u7a0b\u8bc6\u522b\u8d23\u4efb\u7f3a\u53e3\u5e76\u5bfc\u51fa\u76d1\u7763\u9700\u6c42\uff1b\u5de5\u4ef6\u5c42\u4ee5\u201c\u6f14\u7ece\u4e3b\u5e72\u8868\u201d\u5f62\u5f0f\u5f62\u5f0f\u5316\u8868\u8fbe\u4ece\u7f3a\u53e3\u8bc6\u522b\u5230\u9700\u6c42\u63a8\u5bfc\u7684\u8def\u5f84\u3002", "result": "\u7528\u6237\u7814\u7a76\u5728\u4e24\u4e2a\u573a\u666f\u4e0b\u6bd4\u8f83\u4e86\u6240\u63d0\u65b9\u6cd5\u4e0e\u57fa\u7ebf\u76ee\u6807\u5bfc\u5411RE\u65b9\u6cd5\uff0c\u5728\u516d\u4e2a\u7ef4\u5ea6\u4e0a\u5747\u663e\u793a\u51fa\u660e\u663e\u6539\u8fdb\uff0c\u8bc1\u5b9e\u4e86\u8be5\u65b9\u6cd5\u5728\u5f25\u8865\u4e09\u7c7b\u7814\u7a76\u7f3a\u53e3\u65b9\u9762\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3aGenAI\u8d4b\u80fd\u8f6f\u4ef6\u7cfb\u7edf\u4e2d\u7684\u8d23\u4efb\u7f3a\u53e3\u95ee\u9898\u63d0\u4f9b\u4e86\u7cfb\u7edf\u5316\u7684\u9700\u6c42\u5de5\u7a0b\u89e3\u51b3\u65b9\u6848\uff0c\u589e\u5f3a\u4e86\u4eba\u7c7b\u76d1\u7763\u9700\u6c42\u7684\u53ef\u8ffd\u6eaf\u6027\u4e0e\u53ef\u64cd\u4f5c\u6027\u3002"}}
{"id": "2511.11660", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2511.11660", "abs": "https://arxiv.org/abs/2511.11660", "authors": ["Zizheng Guo", "Haichuan Liu", "Xizhe Shi", "Shenglu Hua", "Zuodong Zhang", "Chunyuan Zhao", "Runsheng Wang", "Yibo Lin"], "title": "HeteroSTA: A CPU-GPU Heterogeneous Static Timing Analysis Engine with Holistic Industrial Design Support", "comment": "7 pages, 3 figures, to be published in ASP-DAC 2026", "summary": "We introduce in this paper, HeteroSTA, the first CPU-GPU heterogeneous timing analysis engine that efficiently supports: (1) a set of delay calculation models providing versatile accuracy-speed choices without relying on an external golden tool, (2) robust support for industry formats, including especially the .sdc constraints containing all common timing exceptions, clock domains, and case analysis modes, and (3) end-to-end GPU-acceleration for both graph-based and path-based timing queries, all exposed as a zero-overhead flattened heterogeneous application programming interface (API). HeteroSTA is publicly available with both a standalone binary executable and an embeddable shared library targeting ubiquitous academic and industry applications. Example use cases as a standalone tool, a timing-driven DREAMPlace 4.0 integration, and a timing-driven global routing integration have all demonstrated remarkable runtime speed-up and comparable quality.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86HeteroSTA\uff0c\u9996\u4e2a\u652f\u6301CPU-GPU\u5f02\u6784\u67b6\u6784\u7684\u9759\u6001\u65f6\u5e8f\u5206\u6790\u5f15\u64ce\uff0c\u63d0\u4f9b\u591a\u79cd\u7cbe\u5ea6-\u901f\u5ea6\u53ef\u9009\u7684\u5ef6\u8fdf\u8ba1\u7b97\u6a21\u578b\u3001\u5b8c\u6574\u7684\u5de5\u4e1a\u6807\u51c6\u683c\u5f0f\uff08\u5982.sdc\uff09\u652f\u6301\uff0c\u5e76\u5b9e\u73b0\u7aef\u5230\u7aefGPU\u52a0\u901f\uff0c\u540c\u65f6\u63d0\u4f9b\u96f6\u5f00\u9500\u7684\u7edf\u4e00\u5f02\u6784API\u3002\u8be5\u5de5\u5177\u5df2\u5f00\u6e90\uff0c\u53ef\u4f5c\u4e3a\u72ec\u7acb\u7a0b\u5e8f\u6216\u5d4c\u5165\u5f0f\u5e93\u4f7f\u7528\uff0c\u5728\u591a\u4e2a\u5e94\u7528\u573a\u666f\u4e2d\u5c55\u73b0\u51fa\u663e\u8457\u52a0\u901f\u6548\u679c\u548c\u826f\u597d\u8d28\u91cf\u3002", "motivation": "\u73b0\u6709\u9759\u6001\u65f6\u5e8f\u5206\u6790\u5de5\u5177\u5728\u7cbe\u5ea6\u3001\u901f\u5ea6\u3001\u5bf9\u5de5\u4e1a\u6807\u51c6\u683c\u5f0f\u7684\u652f\u6301\u4ee5\u53ca\u786c\u4ef6\u52a0\u901f\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\uff0c\u7f3a\u4e4f\u4e00\u4e2a\u9ad8\u6548\u3001\u7075\u6d3b\u4e14\u6613\u4e8e\u96c6\u6210\u7684\u5f02\u6784\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u5f00\u53d1\u4e86HeteroSTA\u5f02\u6784\u65f6\u5e8f\u5206\u6790\u5f15\u64ce\uff0c\u96c6\u6210\u4e86\u591a\u79cd\u5ef6\u8fdf\u8ba1\u7b97\u6a21\u578b\uff0c\u5168\u9762\u652f\u6301.sdc\u7b49\u5de5\u4e1a\u7ea6\u675f\u683c\u5f0f\uff0c\u5e76\u5bf9\u56fe\u57fa\u548c\u8def\u5f84\u57fa\u65f6\u5e8f\u67e5\u8be2\u5b9e\u73b0\u7aef\u5230\u7aefGPU\u52a0\u901f\uff0c\u901a\u8fc7\u7edf\u4e00\u7684\u5f02\u6784API\u5bf9\u5916\u63d0\u4f9b\u670d\u52a1\u3002", "result": "HeteroSTA\u5728\u4f5c\u4e3a\u72ec\u7acb\u5de5\u5177\u3001\u4e0eDREAMPlace 4.0\u96c6\u6210\u4ee5\u53ca\u65f6\u5e8f\u9a71\u52a8\u5168\u5c40\u5e03\u7ebf\u7b49\u7528\u4f8b\u4e2d\u5747\u5b9e\u73b0\u4e86\u663e\u8457\u7684\u8fd0\u884c\u65f6\u52a0\u901f\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u53ef\u6bd4\u7684\u5206\u6790\u8d28\u91cf\u3002", "conclusion": "HeteroSTA\u662f\u4e00\u4e2a\u9ad8\u6548\u3001\u7075\u6d3b\u4e14\u5b9e\u7528\u7684\u5f00\u6e90\u5f02\u6784\u9759\u6001\u65f6\u5e8f\u5206\u6790\u89e3\u51b3\u65b9\u6848\uff0c\u80fd\u591f\u6709\u6548\u6ee1\u8db3\u5b66\u672f\u754c\u548c\u5de5\u4e1a\u754c\u5bf9\u9ad8\u6027\u80fd\u65f6\u5e8f\u5206\u6790\u7684\u9700\u6c42\u3002"}}
{"id": "2511.13271", "categories": ["cs.SE", "cs.AI", "cs.IR"], "pdf": "https://arxiv.org/pdf/2511.13271", "abs": "https://arxiv.org/abs/2511.13271", "authors": ["Rufeng Chen", "Shuaishuai Jiang", "Jiyun Shen", "AJung Moon", "Lili Wei"], "title": "Examining the Usage of Generative AI Models in Student Learning Activities for Software Programming", "comment": "9 pages, 4 figures, accepted at AIWARE 2025", "summary": "The rise of Generative AI (GenAI) tools like ChatGPT has created new opportunities and challenges for computing education. Existing research has primarily focused on GenAI's ability to complete educational tasks and its impact on student performance, often overlooking its effects on knowledge gains. In this study, we investigate how GenAI assistance compares to conventional online resources in supporting knowledge gains across different proficiency levels. We conducted a controlled user experiment with 24 undergraduate students of two different levels of programming experience (beginner, intermediate) to examine how students interact with ChatGPT while solving programming tasks. We analyzed task performance, conceptual understanding, and interaction behaviors. Our findings reveal that generating complete solutions with GenAI significantly improves task performance, especially for beginners, but does not consistently result in knowledge gains. Importantly, usage strategies differ by experience: beginners tend to rely heavily on GenAI toward task completion often without knowledge gain in the process, while intermediates adopt more selective approaches. We find that both over-reliance and minimal use result in weaker knowledge gains overall. Based on our results, we call on students and educators to adopt GenAI as a learning rather than a problem solving tool. Our study highlights the urgent need for guidance when integrating GenAI into programming education to foster deeper understanding.", "AI": {"tldr": "\u672c\u7814\u7a76\u53d1\u73b0\uff0c\u5c3d\u7ba1\u751f\u6210\u5f0fAI\uff08\u5982ChatGPT\uff09\u80fd\u663e\u8457\u63d0\u5347\u7f16\u7a0b\u4efb\u52a1\u8868\u73b0\uff08\u5c24\u5176\u5bf9\u521d\u5b66\u8005\uff09\uff0c\u4f46\u5e76\u4e0d\u603b\u80fd\u5e26\u6765\u77e5\u8bc6\u589e\u957f\uff1b\u8fc7\u5ea6\u4f9d\u8d56\u6216\u6781\u5c11\u4f7f\u7528\u5747\u4e0d\u5229\u4e8e\u5b66\u4e60\uff0c\u5efa\u8bae\u5c06\u5176\u4f5c\u4e3a\u5b66\u4e60\u5de5\u5177\u800c\u975e\u89e3\u9898\u5de5\u5177\uff0c\u5e76\u547c\u5401\u6559\u80b2\u8005\u63d0\u4f9b\u4f7f\u7528\u6307\u5bfc\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u591a\u5173\u6ce8\u751f\u6210\u5f0fAI\u5728\u5b8c\u6210\u6559\u80b2\u4efb\u52a1\u548c\u5f71\u54cd\u5b66\u751f\u8868\u73b0\u65b9\u9762\u7684\u80fd\u529b\uff0c\u5374\u5ffd\u89c6\u4e86\u5176\u5bf9\u77e5\u8bc6\u83b7\u53d6\u7684\u5f71\u54cd\u3002\u56e0\u6b64\uff0c\u672c\u6587\u65e8\u5728\u63a2\u7a76\u751f\u6210\u5f0fAI\u4e0e\u4f20\u7edf\u5728\u7ebf\u8d44\u6e90\u5728\u652f\u6301\u4e0d\u540c\u7f16\u7a0b\u6c34\u5e73\u5b66\u751f\u77e5\u8bc6\u589e\u957f\u65b9\u9762\u7684\u5dee\u5f02\u3002", "method": "\u7814\u7a76\u901a\u8fc7\u4e00\u9879\u5bf9\u7167\u7528\u6237\u5b9e\u9a8c\uff0c\u62db\u52df24\u540d\u5177\u6709\u4e0d\u540c\u7f16\u7a0b\u7ecf\u9a8c\uff08\u521d\u7ea7\u3001\u4e2d\u7ea7\uff09\u7684\u672c\u79d1\u751f\uff0c\u5728\u89e3\u51b3\u7f16\u7a0b\u4efb\u52a1\u65f6\u5206\u6790\u4ed6\u4eec\u4e0eChatGPT\u7684\u4e92\u52a8\u884c\u4e3a\uff0c\u5e76\u8bc4\u4f30\u5176\u4efb\u52a1\u8868\u73b0\u3001\u6982\u5ff5\u7406\u89e3\u53ca\u4ea4\u4e92\u6a21\u5f0f\u3002", "result": "\u7ed3\u679c\u8868\u660e\uff0c\u4f7f\u7528\u751f\u6210\u5f0fAI\u751f\u6210\u5b8c\u6574\u89e3\u51b3\u65b9\u6848\u53ef\u663e\u8457\u63d0\u9ad8\u4efb\u52a1\u8868\u73b0\uff08\u5c24\u5176\u5bf9\u521d\u5b66\u8005\uff09\uff0c\u4f46\u5e76\u672a\u6301\u7eed\u4fc3\u8fdb\u77e5\u8bc6\u589e\u957f\u3002\u521d\u5b66\u8005\u503e\u5411\u4e8e\u4f9d\u8d56AI\u5b8c\u6210\u4efb\u52a1\u800c\u7f3a\u4e4f\u77e5\u8bc6\u5185\u5316\uff0c\u4e2d\u7ea7\u5b66\u751f\u5219\u91c7\u53d6\u66f4\u9009\u62e9\u6027\u7684\u7b56\u7565\u3002\u8fc7\u5ea6\u4f9d\u8d56\u6216\u6781\u5c11\u4f7f\u7528AI\u5747\u5bfc\u81f4\u8f83\u5f31\u7684\u77e5\u8bc6\u83b7\u53d6\u6548\u679c\u3002", "conclusion": "\u5e94\u5c06\u751f\u6210\u5f0fAI\u89c6\u4e3a\u5b66\u4e60\u5de5\u5177\u800c\u975e\u89e3\u9898\u5de5\u5177\uff0c\u6559\u80b2\u8005\u9700\u63d0\u4f9b\u660e\u786e\u6307\u5bfc\uff0c\u4ee5\u5e2e\u52a9\u5b66\u751f\u5728\u7f16\u7a0b\u6559\u80b2\u4e2d\u6709\u6548\u5229\u7528AI\uff0c\u4fc3\u8fdb\u6df1\u5c42\u6b21\u7406\u89e3\u3002"}}
{"id": "2511.13305", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2511.13305", "abs": "https://arxiv.org/abs/2511.13305", "authors": ["Rangeet Pan", "Raju Pavuluri", "Ruikai Huang", "Rahul Krishna", "Tyler Stennett", "Alessandro Orso", "Saurabh SInha"], "title": "SAINT: Service-level Integration Test Generation with Program Analysis and LLM-based Agents", "comment": "Accepted at ICSE'26", "summary": "Enterprise applications are typically tested at multiple levels, with service-level testing playing an important role in validating application functionality. Existing service-level testing tools, especially for RESTful APIs, often employ fuzzing and/or depend on OpenAPI specifications which are not readily available in real-world enterprise codebases. Moreover, these tools are limited in their ability to generate functional tests that effectively exercise meaningful scenarios. In this work, we present SAINT, a novel white-box testing approach for service-level testing of enterprise Java applications. SAINT combines static analysis, large language models (LLMs), and LLM-based agents to automatically generate endpoint and scenario-based tests. The approach builds two key models: an endpoint model, capturing syntactic and semantic information about service endpoints, and an operation dependency graph, capturing inter-endpoint ordering constraints. SAINT then employs LLM-based agents to generate tests. Endpoint-focused tests aim to maximize code and database interaction coverage. Scenario-based tests are synthesized by extracting application use cases from code and refining them into executable tests via planning, action, and reflection phases of the agentic loop. We evaluated SAINT on eight Java applications, including a proprietary enterprise application. Our results illustrate the effectiveness of SAINT in coverage, fault detection, and scenario generation. Moreover, a developer survey provides strong endorsement of the scenario-based tests generated by SAINT. Overall, our work shows that combining static analysis with agentic LLM workflows enables more effective, functional, and developer-aligned service-level test generation.", "AI": {"tldr": "SAINT \u662f\u4e00\u79cd\u7ed3\u5408\u9759\u6001\u5206\u6790\u4e0e\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u667a\u80fd\u4f53\u7684\u767d\u76d2\u6d4b\u8bd5\u65b9\u6cd5\uff0c\u7528\u4e8e\u81ea\u52a8\u751f\u6210\u4f01\u4e1a\u7ea7 Java \u5e94\u7528\u7684\u670d\u52a1\u7aef\u70b9\u548c\u573a\u666f\u5316\u6d4b\u8bd5\u7528\u4f8b\uff0c\u663e\u8457\u63d0\u5347\u8986\u76d6\u7387\u3001\u7f3a\u9677\u68c0\u6d4b\u80fd\u529b\u53ca\u5f00\u53d1\u8005\u8ba4\u53ef\u5ea6\u3002", "motivation": "\u73b0\u6709\u670d\u52a1\u7ea7\u6d4b\u8bd5\u5de5\u5177\uff08\u5c24\u5176\u9488\u5bf9 RESTful API\uff09\u4f9d\u8d56 OpenAPI \u89c4\u8303\u6216\u6a21\u7cca\u6d4b\u8bd5\uff0c\u5728\u771f\u5b9e\u4f01\u4e1a\u4ee3\u7801\u5e93\u4e2d\u96be\u4ee5\u9002\u7528\uff0c\u4e14\u96be\u4ee5\u751f\u6210\u6709\u6548\u8986\u76d6\u6709\u610f\u4e49\u4e1a\u52a1\u573a\u666f\u7684\u529f\u80fd\u6027\u6d4b\u8bd5\u3002", "method": "SAINT \u901a\u8fc7\u9759\u6001\u5206\u6790\u6784\u5efa\u7aef\u70b9\u6a21\u578b\u548c\u64cd\u4f5c\u4f9d\u8d56\u56fe\uff0c\u5e76\u5229\u7528 LLM \u667a\u80fd\u4f53\u5728\u89c4\u5212\u3001\u6267\u884c\u4e0e\u53cd\u601d\u5faa\u73af\u4e2d\u751f\u6210\u4e24\u7c7b\u6d4b\u8bd5\uff1a\u4ee5\u8986\u76d6\u4ee3\u7801\u548c\u6570\u636e\u5e93\u4ea4\u4e92\u4e3a\u76ee\u6807\u7684\u7aef\u70b9\u6d4b\u8bd5\uff0c\u4ee5\u53ca\u4ece\u4ee3\u7801\u4e2d\u63d0\u53d6\u7528\u4f8b\u5e76\u7cbe\u70bc\u6210\u53ef\u6267\u884c\u6d4b\u8bd5\u7684\u573a\u666f\u5316\u6d4b\u8bd5\u3002", "result": "\u5728\u516b\u4e2a Java \u5e94\u7528\uff08\u542b\u4e00\u4e2a\u4e13\u6709\u4f01\u4e1a\u5e94\u7528\uff09\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cSAINT \u5728\u6d4b\u8bd5\u8986\u76d6\u7387\u3001\u7f3a\u9677\u53d1\u73b0\u80fd\u529b\u548c\u573a\u666f\u751f\u6210\u65b9\u9762\u8868\u73b0\u4f18\u5f02\uff1b\u5f00\u53d1\u8005\u8c03\u67e5\u4e5f\u9ad8\u5ea6\u8ba4\u53ef\u5176\u751f\u6210\u7684\u573a\u666f\u5316\u6d4b\u8bd5\u3002", "conclusion": "\u5c06\u9759\u6001\u5206\u6790\u4e0e\u57fa\u4e8e\u667a\u80fd\u4f53\u7684 LLM \u5de5\u4f5c\u6d41\u76f8\u7ed3\u5408\uff0c\u80fd\u591f\u5b9e\u73b0\u66f4\u6709\u6548\u3001\u529f\u80fd\u6027\u66f4\u5f3a\u4e14\u66f4\u8d34\u8fd1\u5f00\u53d1\u8005\u9700\u6c42\u7684\u670d\u52a1\u7ea7\u6d4b\u8bd5\u751f\u6210\u3002"}}
{"id": "2511.11672", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2511.11672", "abs": "https://arxiv.org/abs/2511.11672", "authors": ["Zengyi Qin", "Jinyuan Chen", "Yunze Man", "Shengcao Cao", "Ziqi Pang", "Zhuoyuan Wang", "Xin Sun", "Gen Lin", "Han Fang", "Ling Zhu", "Zixin Xie", "Zibu Wei", "Tianshu Ran", "Haoran Geng", "Xander Wu", "Zachary Bright", "Qizhen Sun", "Rui Wang", "Yuyang Cai", "Song Wang", "Jiace Zhao", "Han Cao", "Yeyang Zhou", "Tianrui Liu", "Ray Pan", "Chongye Yang", "Xiang Ren", "Bo Zhang", "Yutong Ban", "Jitendra Malik", "Brian Anthony", "Pieter Abbeel"], "title": "OSGym: Super-Scalable Distributed Data Engine for Generalizable Computer Agents", "comment": null, "summary": "We introduce OSGym, a super-scalable distributed data engine for training agents across diverse computer-related tasks. OSGym efficiently scales to over a thousand operating system (OS) replicas at an academia-affordable cost, serving as dynamic runtime environments for intelligent agents. It offers three key advantages. (1) Scalability: Despite the intensive resource requirements of running multiple OS replicas, OSGym parallelizes over a thousand instances while maintaining operational efficiency under constrained resources, generating up to 1420 multi-turn trajectories per minute. (2) Generality and Customizability: OSGym supports a broad spectrum of tasks that run on OS platforms, including tool use, browser interactions, software engineering, and office applications, with flexible support for diverse model training algorithms. (3) Economic Viability: OSGym operates at only 0.2-0.3 USD per day per OS replica using accessible on-demand compute providers. It is fully open-source and freely available for both research and commercial use. Experiments show that OSGym enables comprehensive data collection, supervised fine-tuning, and reinforcement learning pipelines for computer agents. Models trained with OSGym outperform state-of-the-art baselines, demonstrating its potential to advance scalability and universality in future agent research.", "AI": {"tldr": "OSGym \u662f\u4e00\u4e2a\u9ad8\u53ef\u6269\u5c55\u7684\u5206\u5e03\u5f0f\u6570\u636e\u5f15\u64ce\uff0c\u652f\u6301\u5728\u4e0a\u5343\u4e2a\u64cd\u4f5c\u7cfb\u7edf\u526f\u672c\u4e0a\u9ad8\u6548\u8bad\u7ec3\u667a\u80fd\u4f53\uff0c\u5177\u5907\u9ad8\u6269\u5c55\u6027\u3001\u901a\u7528\u6027\u548c\u7ecf\u6d4e\u6027\uff0c\u9002\u7528\u4e8e\u591a\u79cd\u8ba1\u7b97\u673a\u4efb\u52a1\uff0c\u5e76\u663e\u8457\u63d0\u5347\u6a21\u578b\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u667a\u80fd\u4f53\u8bad\u7ec3\u5e73\u53f0\u5728\u5904\u7406\u591a\u6837\u5316\u7684\u8ba1\u7b97\u673a\u76f8\u5173\u4efb\u52a1\u65f6\u9762\u4e34\u53ef\u6269\u5c55\u6027\u5dee\u3001\u6210\u672c\u9ad8\u548c\u901a\u7528\u6027\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u4e9f\u9700\u4e00\u79cd\u9ad8\u6548\u3001\u7ecf\u6d4e\u4e14\u7075\u6d3b\u7684\u8bad\u7ec3\u57fa\u7840\u8bbe\u65bd\u3002", "method": "OSGym \u901a\u8fc7\u5e76\u884c\u5316\u7ba1\u7406\u4e0a\u5343\u4e2a\u64cd\u4f5c\u7cfb\u7edf\u526f\u672c\uff0c\u6784\u5efa\u52a8\u6001\u8fd0\u884c\u73af\u5883\uff0c\u652f\u6301\u591a\u7c7b\u4efb\u52a1\uff08\u5982\u5de5\u5177\u8c03\u7528\u3001\u6d4f\u89c8\u5668\u4ea4\u4e92\u3001\u8f6f\u4ef6\u5de5\u7a0b\u7b49\uff09\uff0c\u5e76\u517c\u5bb9\u591a\u79cd\u6a21\u578b\u8bad\u7ec3\u7b97\u6cd5\uff0c\u540c\u65f6\u5229\u7528\u4f4e\u6210\u672c\u6309\u9700\u8ba1\u7b97\u8d44\u6e90\u5b9e\u73b0\u9ad8\u6548\u6570\u636e\u751f\u6210\u3002", "result": "OSGym \u6bcf\u5206\u949f\u53ef\u751f\u6210\u591a\u8fbe1420\u6761\u591a\u8f6e\u8f68\u8ff9\uff0c\u5355\u4e2aOS\u526f\u672c\u65e5\u5747\u6210\u672c\u4ec50.2\u20130.3\u7f8e\u5143\uff1b\u57fa\u4e8e\u5176\u8bad\u7ec3\u7684\u6a21\u578b\u5728\u591a\u9879\u4efb\u52a1\u4e0a\u4f18\u4e8e\u5f53\u524d\u6700\u5148\u8fdb\u7684\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "OSGym \u4e3a\u667a\u80fd\u4f53\u7814\u7a76\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5f00\u6e90\u3001\u7ecf\u6d4e\u3001\u9ad8\u6269\u5c55\u4e14\u901a\u7528\u7684\u8bad\u7ec3\u5e73\u53f0\uff0c\u6709\u671b\u63a8\u52a8\u672a\u6765\u667a\u80fd\u4f53\u5728\u53ef\u6269\u5c55\u6027\u548c\u666e\u9002\u6027\u65b9\u9762\u7684\u8fdb\u6b65\u3002"}}
{"id": "2511.13318", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2511.13318", "abs": "https://arxiv.org/abs/2511.13318", "authors": ["Peihao Li"], "title": "LinkXplore: A Framework for Affordable High-Quality Blockchain Data", "comment": null, "summary": "Blockchain technologies are rapidly transforming both academia and industry. However, large-scale blockchain data collection remains prohibitively expensive, as many RPC providers only offer enhanced APIs with high pricing tiers that are unsuitable for budget-constrained research or industrial-scale applications, which has significantly slowed down academic studies and product development. Moreover, there is a clear lack of a systematic framework that allows flexible integration of new modules for analyzing on-chain data.\n  To address these challenges, we introduce LinkXplore, the first open framework for collecting and managing on-chain data. LinkXplore enables users to bypass costly blockchain data providers by directly analyzing raw data from RPC queries or streams, thereby offering high-quality blockchain data at a fraction of the cost. Through a simple API and backend processing logic, any type of chain data can be integrated into the framework. This makes it a practical alternative for both researchers and developers with limited budgets. Code and dataset used in this project are publicly available at https://github.com/Linkis-Project/LinkXplore", "AI": {"tldr": "LinkXplore \u662f\u4e00\u4e2a\u5f00\u6e90\u6846\u67b6\uff0c\u7528\u4e8e\u4f4e\u6210\u672c\u5730\u6536\u96c6\u548c\u7ba1\u7406\u94fe\u4e0a\u6570\u636e\uff0c\u7ed5\u8fc7\u6602\u8d35\u7684\u533a\u5757\u94fe\u6570\u636e\u63d0\u4f9b\u5546\uff0c\u652f\u6301\u7075\u6d3b\u96c6\u6210\u5404\u7c7b\u94fe\u6570\u636e\u6a21\u5757\u3002", "motivation": "\u5927\u89c4\u6a21\u533a\u5757\u94fe\u6570\u636e\u91c7\u96c6\u6210\u672c\u9ad8\u6602\uff0c\u591a\u6570 RPC \u63d0\u4f9b\u5546\u7684\u9ad8\u7ea7 API \u5b9a\u4ef7\u8fc7\u9ad8\uff0c\u9650\u5236\u4e86\u9884\u7b97\u6709\u9650\u7684\u7814\u7a76\u4e0e\u5de5\u4e1a\u5e94\u7528\uff1b\u540c\u65f6\u7f3a\u4e4f\u53ef\u7075\u6d3b\u6269\u5c55\u7684\u7cfb\u7edf\u6027\u5206\u6790\u6846\u67b6\u3002", "method": "\u63d0\u51fa LinkXplore \u6846\u67b6\uff0c\u901a\u8fc7\u76f4\u63a5\u89e3\u6790 RPC \u67e5\u8be2\u6216\u6570\u636e\u6d41\u4e2d\u7684\u539f\u59cb\u6570\u636e\uff0c\u7ed3\u5408\u7b80\u6613 API \u4e0e\u540e\u7aef\u5904\u7406\u903b\u8f91\uff0c\u5b9e\u73b0\u4f4e\u6210\u672c\u3001\u9ad8\u8d28\u91cf\u7684\u94fe\u4e0a\u6570\u636e\u83b7\u53d6\u4e0e\u6a21\u5757\u5316\u96c6\u6210\u3002", "result": "LinkXplore \u80fd\u4ee5\u8fdc\u4f4e\u4e8e\u5546\u4e1a API \u7684\u6210\u672c\u63d0\u4f9b\u9ad8\u8d28\u91cf\u94fe\u4e0a\u6570\u636e\uff0c\u5e76\u652f\u6301\u4efb\u610f\u7c7b\u578b\u94fe\u6570\u636e\u7684\u7075\u6d3b\u63a5\u5165\uff0c\u9002\u7528\u4e8e\u8d44\u6e90\u53d7\u9650\u7684\u7814\u7a76\u8005\u4e0e\u5f00\u53d1\u8005\u3002", "conclusion": "LinkXplore \u4e3a\u533a\u5757\u94fe\u6570\u636e\u91c7\u96c6\u63d0\u4f9b\u4e86\u4e00\u4e2a\u7ecf\u6d4e\u3001\u5f00\u653e\u4e14\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u6709\u52a9\u4e8e\u52a0\u901f\u5b66\u672f\u7814\u7a76\u4e0e\u4ea7\u54c1\u5f00\u53d1\u3002"}}
{"id": "2511.13341", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.13341", "abs": "https://arxiv.org/abs/2511.13341", "authors": ["Zihe Yan", "Kai Luo", "Haoyu Yang", "Yang Yu", "Zhuosheng Zhang", "Guancheng Li"], "title": "An LLM-based Quantitative Framework for Evaluating High-Stealthy Backdoor Risks in OSS Supply Chains", "comment": "7 figures, 4 tables, conference", "summary": "In modern software development workflows, the open-source software supply chain contributes significantly to efficient and convenient engineering practices. With increasing system complexity, using open-source software as third-party dependencies has become a common practice. However, the lack of maintenance for underlying dependencies and insufficient community auditing create challenges in ensuring source code security and the legitimacy of repository maintainers, especially under high-stealthy backdoor attacks exemplified by the XZ-Util incident. To address these problems, we propose a fine-grained project evaluation framework for backdoor risk assessment in open-source software. The framework models stealthy backdoor attacks from the viewpoint of the attacker and defines targeted metrics for each attack stage. In addition, to overcome the limitations of static analysis in assessing the reliability of repository maintenance activities such as irregular committer privilege escalation and limited participation in reviews, the framework uses large language models (LLMs) to conduct semantic evaluation of code repositories without relying on manually crafted patterns. The framework is evaluated on sixty six high-priority packages in the Debian ecosystem. The experimental results indicate that the current open-source software supply chain is exposed to various security risks.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ec6\u7c92\u5ea6\u7684\u5f00\u6e90\u8f6f\u4ef6\u9879\u76ee\u8bc4\u4f30\u6846\u67b6\uff0c\u7528\u4e8e\u68c0\u6d4b\u9ad8\u9690\u853d\u6027\u540e\u95e8\u653b\u51fb\u98ce\u9669\uff0c\u7ed3\u5408\u5927\u8bed\u8a00\u6a21\u578b\u5bf9\u4ee3\u7801\u4ed3\u5e93\u8fdb\u884c\u8bed\u4e49\u8bc4\u4f30\uff0c\u5e76\u5728 Debian \u751f\u6001\u7cfb\u7edf\u4e2d\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u73b0\u4ee3\u8f6f\u4ef6\u5f00\u53d1\u5e7f\u6cdb\u4f9d\u8d56\u5f00\u6e90\u7ec4\u4ef6\uff0c\u4f46\u5e95\u5c42\u4f9d\u8d56\u7ef4\u62a4\u4e0d\u8db3\u548c\u793e\u533a\u5ba1\u8ba1\u7f3a\u5931\u5bfc\u81f4\u6e90\u7801\u5b89\u5168\u6027\u548c\u7ef4\u62a4\u8005\u5408\u6cd5\u6027\u96be\u4ee5\u4fdd\u969c\uff0c\u5c24\u5176\u5728\u7c7b\u4f3c XZ-Util \u4e8b\u4ef6\u7684\u9ad8\u9690\u853d\u6027\u540e\u95e8\u653b\u51fb\u4e0b\u98ce\u9669\u52a0\u5267\u3002", "method": "\u6784\u5efa\u4e00\u4e2a\u4ece\u653b\u51fb\u8005\u89c6\u89d2\u5efa\u6a21\u540e\u95e8\u653b\u51fb\u9636\u6bb5\u7684\u8bc4\u4f30\u6846\u67b6\uff0c\u5b9a\u4e49\u5404\u9636\u6bb5\u9488\u5bf9\u6027\u6307\u6807\uff0c\u5e76\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\u5bf9\u4ee3\u7801\u4ed3\u5e93\u8fdb\u884c\u65e0\u9700\u4eba\u5de5\u89c4\u5219\u7684\u8bed\u4e49\u8bc4\u4f30\uff0c\u4ee5\u514b\u670d\u9759\u6001\u5206\u6790\u5728\u8bc4\u4f30\u7ef4\u62a4\u6d3b\u52a8\uff08\u5982\u63d0\u4ea4\u8005\u6743\u9650\u5f02\u5e38\u63d0\u5347\u3001\u8bc4\u5ba1\u53c2\u4e0e\u5ea6\u4f4e\uff09\u65b9\u9762\u7684\u5c40\u9650\u3002", "result": "\u5728 Debian \u751f\u6001\u7cfb\u7edf\u4e2d\u5bf9 66 \u4e2a\u9ad8\u4f18\u5148\u7ea7\u8f6f\u4ef6\u5305\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u5f53\u524d\u5f00\u6e90\u8f6f\u4ef6\u4f9b\u5e94\u94fe\u9762\u4e34\u591a\u79cd\u5b89\u5168\u98ce\u9669\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u7ec6\u7c92\u5ea6\u8bc4\u4f30\u6846\u67b6\u80fd\u6709\u6548\u8bc6\u522b\u5f00\u6e90\u9879\u76ee\u4e2d\u7684\u540e\u95e8\u98ce\u9669\uff0c\u63ed\u793a\u4e86\u73b0\u6709\u5f00\u6e90\u4f9b\u5e94\u94fe\u5728\u5b89\u5168\u6027\u65b9\u9762\u7684\u4e25\u91cd\u9690\u60a3\u3002"}}
{"id": "2511.11719", "categories": ["cs.DC", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.11719", "abs": "https://arxiv.org/abs/2511.11719", "authors": ["Mohammad Mahdi Kamani", "Zhongwei Cheng", "Lin Chen"], "title": "ECCENTRIC: Edge-Cloud Collaboration Framework for Distributed Inference Using Knowledge Adaptation", "comment": null, "summary": "The massive growth in the utilization of edge AI has made the applications of machine learning models ubiquitous in different domains. Despite the computation and communication efficiency of these systems, due to limited computation resources on edge devices, relying on more computationally rich systems on the cloud side is inevitable in most cases. Cloud inference systems can achieve the best performance while the computation and communication cost is dramatically increasing by the expansion of a number of edge devices relying on these systems. Hence, there is a trade-off between the computation, communication, and performance of these systems. In this paper, we propose a novel framework, dubbed as Eccentric that learns models with different levels of trade-offs between these conflicting objectives. This framework, based on an adaptation of knowledge from the edge model to the cloud one, reduces the computation and communication costs of the system during inference while achieving the best performance possible. The Eccentric framework can be considered as a new form of compression method suited for edge-cloud inference systems to reduce both computation and communication costs. Empirical studies on classification and object detection tasks corroborate the efficacy of this framework.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aEccentric\u7684\u65b0\u6846\u67b6\uff0c\u901a\u8fc7\u5728\u8fb9\u7f18\u4e0e\u4e91\u7aef\u6a21\u578b\u4e4b\u95f4\u8fdb\u884c\u77e5\u8bc6\u8fc1\u79fb\uff0c\u5728\u8fb9\u7f18-\u4e91\u63a8\u7406\u7cfb\u7edf\u4e2d\u5b9e\u73b0\u8ba1\u7b97\u3001\u901a\u4fe1\u5f00\u9500\u4e0e\u6027\u80fd\u4e4b\u95f4\u7684\u6709\u6548\u6743\u8861\u3002", "motivation": "\u8fb9\u7f18AI\u7684\u5e7f\u6cdb\u5e94\u7528\u53d7\u9650\u4e8e\u8fb9\u7f18\u8bbe\u5907\u8ba1\u7b97\u8d44\u6e90\u6709\u9650\uff0c\u901a\u5e38\u9700\u4f9d\u8d56\u4e91\u7aef\u8fdb\u884c\u63a8\u7406\uff0c\u4f46\u968f\u7740\u8fb9\u7f18\u8bbe\u5907\u6570\u91cf\u589e\u52a0\uff0c\u4e91\u7aef\u63a8\u7406\u5e26\u6765\u9ad8\u6602\u7684\u8ba1\u7b97\u4e0e\u901a\u4fe1\u6210\u672c\uff0c\u56e0\u6b64\u9700\u8981\u5728\u6027\u80fd\u4e0e\u8d44\u6e90\u6d88\u8017\u4e4b\u95f4\u53d6\u5f97\u5e73\u8861\u3002", "method": "\u63d0\u51faEccentric\u6846\u67b6\uff0c\u901a\u8fc7\u5c06\u8fb9\u7f18\u6a21\u578b\u7684\u77e5\u8bc6\u8fc1\u79fb\u5230\u4e91\u7aef\u6a21\u578b\uff0c\u5b9e\u73b0\u4e0d\u540c\u5c42\u7ea7\u7684\u8ba1\u7b97\u3001\u901a\u4fe1\u4e0e\u6027\u80fd\u6743\u8861\uff0c\u4f5c\u4e3a\u4e00\u79cd\u9002\u7528\u4e8e\u8fb9\u7f18-\u4e91\u63a8\u7406\u7cfb\u7edf\u7684\u65b0\u578b\u538b\u7f29\u65b9\u6cd5\u3002", "result": "\u5728\u5206\u7c7b\u548c\u76ee\u6807\u68c0\u6d4b\u4efb\u52a1\u4e0a\u7684\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u8be5\u6846\u67b6\u5728\u964d\u4f4e\u8ba1\u7b97\u4e0e\u901a\u4fe1\u6210\u672c\u7684\u540c\u65f6\u4ecd\u80fd\u4fdd\u6301\u4f18\u5f02\u6027\u80fd\u7684\u6709\u6548\u6027\u3002", "conclusion": "Eccentric\u6846\u67b6\u4e3a\u8fb9\u7f18-\u4e91\u534f\u540c\u63a8\u7406\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u80fd\u591f\u5728\u8d44\u6e90\u53d7\u9650\u6761\u4ef6\u4e0b\u4f18\u5316\u7cfb\u7edf\u6574\u4f53\u6548\u7387\u4e0e\u6027\u80fd\u3002"}}
{"id": "2511.13357", "categories": ["cs.SE", "cs.IR"], "pdf": "https://arxiv.org/pdf/2511.13357", "abs": "https://arxiv.org/abs/2511.13357", "authors": ["Dmitry Moskalev"], "title": "FLOWER: Flow-Oriented Entity-Relationship Tool", "comment": "12 pages, 8 figures", "summary": "Exploring relationships across data sources is a crucial optimization for entities recognition. Since databases can store big amount of information with synthetic and organic data, serving all quantity of objects correctly is an important task to deal with. However, the decision of how to construct entity relationship model is associated with human factor. In this paper, we present flow-oriented entity-relationship tool. This is first and unique end-to-end solution that eliminates routine and resource-intensive problems of processing, creating and visualizing both of explicit and implicit dependencies for prominent SQL dialects on-the-fly. Once launched, FLOWER automatically detects built-in constraints and starting to create own correct and necessary one using dynamic sampling and robust data analysis techniques. This approach applies to improve entity-relationship model and data storytelling to better understand the foundation of data and get unseen insights from DB sources using SQL or natural language. Evaluated on state-of-the-art STATS benchmark, experiments show that FLOWER is superior to reservoir sampling by 2.4x for distribution representation and 2.6x for constraint learning with 2.15x acceleration. For data storytelling, our tool archives 1.19x for accuracy enhance with 1.86x context decrease compare to LLM. Presented tool is also support 23 languages and compatible with both of CPU and GPU. Those results show that FLOWER can manage with real-world data a way better to ensure with quality, scalability and applicability for different use-cases.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86FLOWER\uff0c\u4e00\u79cd\u7aef\u5230\u7aef\u7684\u9762\u5411\u6d41\u7a0b\u7684\u5b9e\u4f53\u5173\u7cfb\u5efa\u6a21\u5de5\u5177\uff0c\u80fd\u81ea\u52a8\u68c0\u6d4b\u6570\u636e\u5e93\u7ea6\u675f\u5e76\u52a8\u6001\u6784\u5efa\u663e\u5f0f\u4e0e\u9690\u5f0f\u4f9d\u8d56\u5173\u7cfb\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5728\u5206\u5e03\u8868\u793a\u3001\u7ea6\u675f\u5b66\u4e60\u548c\u6570\u636e\u53d9\u4e8b\u65b9\u9762\u5747\u53d6\u5f97\u66f4\u4f18\u6027\u80fd\u3002", "motivation": "\u4f20\u7edf\u5b9e\u4f53\u5173\u7cfb\u6a21\u578b\u6784\u5efa\u9ad8\u5ea6\u4f9d\u8d56\u4eba\u5de5\uff0c\u96be\u4ee5\u9ad8\u6548\u5904\u7406\u5927\u89c4\u6a21\u5408\u6210\u4e0e\u6709\u673a\u6570\u636e\u4e2d\u7684\u663e\u5f0f\u548c\u9690\u5f0f\u4f9d\u8d56\u5173\u7cfb\uff0c\u4e9f\u9700\u81ea\u52a8\u5316\u3001\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\u4ee5\u63d0\u5347\u6570\u636e\u7406\u89e3\u548c\u6d1e\u5bdf\u529b\u3002", "method": "FLOWER\u91c7\u7528\u52a8\u6001\u91c7\u6837\u4e0e\u9c81\u68d2\u6570\u636e\u5206\u6790\u6280\u672f\uff0c\u81ea\u52a8\u8bc6\u522bSQL\u6570\u636e\u5e93\u4e2d\u7684\u5185\u7f6e\u7ea6\u675f\uff0c\u5e76\u5b9e\u65f6\u6784\u5efa\u6b63\u786e\u4e14\u5fc5\u8981\u7684\u5b9e\u4f53\u5173\u7cfb\u6a21\u578b\uff0c\u652f\u6301SQL\u548c\u81ea\u7136\u8bed\u8a00\u8fdb\u884c\u6570\u636e\u53d9\u4e8b\uff0c\u517c\u5bb9CPU/GPU\u5e76\u652f\u630123\u79cd\u8bed\u8a00\u3002", "result": "\u5728STATS\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cFLOWER\u5728\u5206\u5e03\u8868\u793a\u4e0a\u6bd4\u84c4\u6c34\u6c60\u91c7\u6837\u5feb2.4\u500d\uff0c\u7ea6\u675f\u5b66\u4e60\u5feb2.6\u500d\u4e14\u52a0\u901f2.15\u500d\uff1b\u5728\u6570\u636e\u53d9\u4e8b\u65b9\u9762\uff0c\u51c6\u786e\u7387\u63d0\u53471.19\u500d\uff0c\u4e0a\u4e0b\u6587\u957f\u5ea6\u51cf\u5c111.86\u500d\uff0c\u4f18\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u3002", "conclusion": "FLOWER\u662f\u4e00\u79cd\u9ad8\u6548\u3001\u53ef\u6269\u5c55\u4e14\u5b9e\u7528\u7684\u5b9e\u4f53\u5173\u7cfb\u5efa\u6a21\u5de5\u5177\uff0c\u80fd\u591f\u663e\u8457\u63d0\u5347\u771f\u5b9e\u4e16\u754c\u6570\u636e\u5904\u7406\u7684\u8d28\u91cf\u4e0e\u6548\u7387\uff0c\u9002\u7528\u4e8e\u591a\u79cd\u5e94\u7528\u573a\u666f\u3002"}}
{"id": "2511.11721", "categories": ["cs.DC", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.11721", "abs": "https://arxiv.org/abs/2511.11721", "authors": ["Leszek Sliwko", "Vladimir Getov"], "title": "A Meta-Heuristic Load Balancer for Cloud Computing Systems", "comment": null, "summary": "This paper presents a strategy to allocate services on a Cloud system without overloading nodes and maintaining the system stability with minimum cost. We specify an abstract model of cloud resources utilization, including multiple types of resources as well as considerations for the service migration costs. A prototype meta-heuristic load balancer is demonstrated and experimental results are presented and discussed. We also propose a novel genetic algorithm, where population is seeded with the outputs of other meta-heuristic algorithms.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u4e91\u7cfb\u7edf\u670d\u52a1\u5206\u914d\u7b56\u7565\uff0c\u901a\u8fc7\u65b0\u9896\u7684\u9057\u4f20\u7b97\u6cd5\u5b9e\u73b0\u8d1f\u8f7d\u5747\u8861\uff0c\u5728\u907f\u514d\u8282\u70b9\u8fc7\u8f7d\u548c\u4fdd\u8bc1\u7cfb\u7edf\u7a33\u5b9a\u6027\u7684\u540c\u65f6\u6700\u5c0f\u5316\u6210\u672c\u3002", "motivation": "\u5728\u4e91\u8ba1\u7b97\u73af\u5883\u4e2d\uff0c\u9700\u8981\u6709\u6548\u5206\u914d\u670d\u52a1\u4ee5\u907f\u514d\u8282\u70b9\u8fc7\u8f7d\u3001\u7ef4\u6301\u7cfb\u7edf\u7a33\u5b9a\u6027\uff0c\u5e76\u5c3d\u53ef\u80fd\u964d\u4f4e\u8d44\u6e90\u4f7f\u7528\u548c\u8fc1\u79fb\u6210\u672c\u3002", "method": "\u6784\u5efa\u4e86\u4e00\u4e2a\u5305\u542b\u591a\u79cd\u8d44\u6e90\u7c7b\u578b\u548c\u670d\u52a1\u8fc1\u79fb\u6210\u672c\u7684\u4e91\u8d44\u6e90\u5229\u7528\u62bd\u8c61\u6a21\u578b\uff0c\u5e76\u8bbe\u8ba1\u4e86\u4e00\u79cd\u65b0\u578b\u9057\u4f20\u7b97\u6cd5\uff0c\u5176\u521d\u59cb\u79cd\u7fa4\u7531\u5176\u4ed6\u5143\u542f\u53d1\u5f0f\u7b97\u6cd5\u7684\u8f93\u51fa\u7ed3\u679c\u6784\u6210\uff1b\u540c\u65f6\u5b9e\u73b0\u4e86\u4e00\u4e2a\u539f\u578b\u5143\u542f\u53d1\u5f0f\u8d1f\u8f7d\u5747\u8861\u5668\u8fdb\u884c\u9a8c\u8bc1\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\u6240\u63d0\u51fa\u7684\u7b97\u6cd5\u5728\u5b9e\u73b0\u8d1f\u8f7d\u5747\u8861\u548c\u63a7\u5236\u6210\u672c\u65b9\u9762\u5177\u6709\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u4f9b\u4e86\u4e00\u79cd\u517c\u987e\u7cfb\u7edf\u7a33\u5b9a\u6027\u548c\u6210\u672c\u6548\u76ca\u7684\u4e91\u670d\u52a1\u5206\u914d\u65b9\u6cd5\uff0c\u901a\u8fc7\u6df7\u5408\u5143\u542f\u53d1\u5f0f\u7b56\u7565\u63d0\u5347\u4e86\u8d1f\u8f7d\u5747\u8861\u6027\u80fd\u3002"}}
{"id": "2511.13611", "categories": ["cs.SE", "q-bio.QM"], "pdf": "https://arxiv.org/pdf/2511.13611", "abs": "https://arxiv.org/abs/2511.13611", "authors": ["Torec T. Luik", "Joost de Folter", "Rodrigo Rosas-Bertolini", "Eric A. J. Reits", "Ron A. Hoebe", "Przemek M. Krawczyk"], "title": "BIOMERO 2.0: end-to-end FAIR infrastructure for bioimaging data import, analysis, and provenance", "comment": "16 pages, 2 figures, 25 pages supplemental information; for software, see https://github.com/Cellular-Imaging-Amsterdam-UMC/NL-BIOMERO", "summary": "We present BIOMERO 2.0, a major evolution of the BIOMERO framework that transforms OMERO into a FAIR-compliant (findable, accessible, interoperable, and reusable), provenance-aware bioimaging platform. BIOMERO 2.0 integrates data import, preprocessing, analysis, and workflow monitoring through an OMERO.web plugin and containerized components. The importer subsystem facilitates in-place import using containerized preprocessing and metadata enrichment via forms, while the analyzer subsystem coordinates and tracks containerized analyses on high-performance computing systems via the BIOMERO Python library. All imports and analyses are recorded with parameters, versions, and results, ensuring real-time provenance accessible through integrated dashboards. This dual approach places OMERO at the heart of the bioimaging analysis process: the importer ensures provenance from image acquisition through preprocessing and import into OMERO, while the analyzer records it for downstream processing. These integrated layers enhance OMEROs FAIRification, supporting traceable, reusable workflows for image analysis that bridge the gap between data import, analysis, and sharing.", "AI": {"tldr": "BIOMERO 2.0 enhances OMERO into a FAIR-compliant, provenance-aware bioimaging platform by integrating containerized import, preprocessing, analysis, and real-time workflow tracking via an OMERO.web plugin.", "motivation": "To transform OMERO into a FAIR-compliant and provenance-aware platform that supports traceable, reusable bioimaging workflows from data acquisition through analysis and sharing.", "method": "BIOMERO 2.0 uses an OMERO.web plugin with containerized components: an importer subsystem for in-place data import and metadata enrichment, and an analyzer subsystem that coordinates and tracks containerized analyses on HPC systems using the BIOMERO Python library, while recording full provenance.", "result": "The system enables end-to-end provenance tracking, integrates preprocessing and analysis into OMERO, and enhances FAIR compliance by making workflows traceable, interoperable, and reusable.", "conclusion": "BIOMERO 2.0 successfully positions OMERO at the core of the bioimaging analysis lifecycle, bridging data import, processing, and sharing with robust provenance and FAIR support."}}
{"id": "2511.11729", "categories": ["cs.DC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.11729", "abs": "https://arxiv.org/abs/2511.11729", "authors": ["Ao Xu", "Han Zhao", "Weihao Cui", "Quan Chen", "Yukang Chen", "Shulai Zhang", "Shuang Chen", "Jiemin Jiang", "Zhibin Yu", "Minyi Guo"], "title": "Harli: Harvest Underutilized Resources in LLM Serving with Finetuning Tasks", "comment": null, "summary": "Large language models (LLMs) are increasingly deployed under the Model-as-a-Service (MaaS) paradigm. To meet stringent quality-of-service (QoS) requirements, existing LLM serving systems disaggregate the prefill and decode phases of inference. However, decode instances often experience low GPU utilization due to their memory-bound nature and insufficient batching in dynamic workloads, leaving compute resources underutilized.\n  We introduce Harli, a serving system that improves GPU utilization by co-locating parameter-efficient finetuning (PEFT) tasks with LLM decode instances. PEFT tasks are compute-bound and memory-efficient, making them ideal candidates for safe co-location. Specifically, Harli addresses key challenges--limited memory and unpredictable interference--using three components: a unified memory allocator for runtime memory reuse, a two-stage latency predictor for decode latency modeling, and a QoS-guaranteed throughput-maximizing scheduler for throughput maximization. Experimental results show that Harli improves the finetune throughput by 46.2% on average (up to 92.0%) over state-of-the-art serving systems, while maintaining strict QoS guarantees for inference decode.", "AI": {"tldr": "Harli \u662f\u4e00\u4e2a\u65b0\u578b\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u670d\u52a1\u7cfb\u7edf\uff0c\u901a\u8fc7\u5c06\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\uff08PEFT\uff09\u4efb\u52a1\u4e0e LLM \u89e3\u7801\u5b9e\u4f8b\u5171\u7f6e\uff0c\u663e\u8457\u63d0\u5347 GPU \u5229\u7528\u7387\uff0c\u5728\u4fdd\u8bc1\u63a8\u7406\u670d\u52a1\u8d28\u91cf\uff08QoS\uff09\u7684\u540c\u65f6\uff0c\u5e73\u5747\u63d0\u5347\u5fae\u8c03\u541e\u5410\u91cf 46.2%\u3002", "motivation": "\u73b0\u6709 LLM \u670d\u52a1\u7cfb\u7edf\u5728\u89e3\u8026\u9884\u586b\u5145\u548c\u89e3\u7801\u9636\u6bb5\u540e\uff0c\u89e3\u7801\u5b9e\u4f8b\u56e0\u5185\u5b58\u53d7\u9650\u548c\u52a8\u6001\u8d1f\u8f7d\u4e0b\u6279\u5904\u7406\u4e0d\u8db3\uff0c\u5bfc\u81f4 GPU \u5229\u7528\u7387\u4f4e\u4e0b\uff0c\u8ba1\u7b97\u8d44\u6e90\u6d6a\u8d39\u3002", "method": "Harli \u901a\u8fc7\u4e09\u4e2a\u6838\u5fc3\u7ec4\u4ef6\u5b9e\u73b0\u9ad8\u6548\u5171\u7f6e\uff1a\u7edf\u4e00\u5185\u5b58\u5206\u914d\u5668\u5b9e\u73b0\u8fd0\u884c\u65f6\u5185\u5b58\u590d\u7528\u3001\u4e24\u9636\u6bb5\u5ef6\u8fdf\u9884\u6d4b\u5668\u5efa\u6a21\u89e3\u7801\u5ef6\u8fdf\u3001\u4ee5\u53ca\u517c\u987e QoS \u4fdd\u969c\u4e0e\u541e\u5410\u91cf\u6700\u5927\u5316\u7684\u8c03\u5ea6\u5668\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cHarli \u76f8\u6bd4\u5f53\u524d\u6700\u5148\u8fdb\u7684\u670d\u52a1\u7cfb\u7edf\uff0c\u5e73\u5747\u63d0\u5347\u5fae\u8c03\u541e\u5410\u91cf 46.2%\uff08\u6700\u9ad8\u8fbe 92.0%\uff09\uff0c\u540c\u65f6\u4e25\u683c\u6ee1\u8db3\u63a8\u7406\u89e3\u7801\u7684 QoS \u8981\u6c42\u3002", "conclusion": "\u5c06\u8ba1\u7b97\u5bc6\u96c6\u578b\u4e14\u5185\u5b58\u9ad8\u6548\u7684 PEFT \u4efb\u52a1\u4e0e LLM \u89e3\u7801\u5b9e\u4f8b\u5b89\u5168\u5171\u7f6e\uff0c\u662f\u63d0\u5347 GPU \u5229\u7528\u7387\u5e76\u517c\u987e\u63a8\u7406\u4e0e\u8bad\u7ec3\u6548\u7387\u7684\u6709\u6548\u7b56\u7565\uff0cHarli \u7cfb\u7edf\u4e3a\u6b64\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848\u3002"}}
{"id": "2511.13646", "categories": ["cs.SE", "cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.13646", "abs": "https://arxiv.org/abs/2511.13646", "authors": ["Chunqiu Steven Xia", "Zhe Wang", "Yan Yang", "Yuxiang Wei", "Lingming Zhang"], "title": "Live-SWE-agent: Can Software Engineering Agents Self-Evolve on the Fly?", "comment": null, "summary": "Large Language Models (LLMs) are reshaping almost all industries, including software engineering. In recent years, a number of LLM agents have been proposed to solve real-world software problems. Such software agents are typically equipped with a suite of coding tools and can autonomously decide the next actions to form complete trajectories to solve end-to-end software tasks. While promising, they typically require dedicated design and may still be suboptimal, since it can be extremely challenging and costly to exhaust the entire agent scaffold design space. Recognizing that software agents are inherently software themselves that can be further refined/modified, researchers have proposed a number of self-improving software agents recently, including the Darwin-G\u00f6del Machine (DGM). Meanwhile, such self-improving agents require costly offline training on specific benchmarks and may not generalize well across different LLMs or benchmarks. In this paper, we propose Live-SWE-agent, the first live software agent that can autonomously and continuously evolve itself on-the-fly during runtime when solving real-world software problems. More specifically, Live-SWE-agent starts with the most basic agent scaffold with only access to bash tools (e.g., mini-SWE-agent), and autonomously evolves its own scaffold implementation while solving real-world software problems. Our evaluation on the widely studied SWE-bench Verified benchmark shows that Live-SWE-agent can achieve an impressive solve rate of 75.4% without test-time scaling, outperforming all existing open-source software agents and approaching the performance of the best proprietary solution. Moreover, Live-SWE-agent outperforms state-of-the-art manually crafted software agents on the recent SWE-Bench Pro benchmark, achieving the best-known solve rate of 45.8%.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86Live-SWE-agent\uff0c\u8fd9\u662f\u9996\u4e2a\u80fd\u5728\u8fd0\u884c\u65f6\u81ea\u4e3b\u6301\u7eed\u8fdb\u5316\u7684\u8f6f\u4ef6\u667a\u80fd\u4f53\u3002\u5b83\u4ece\u4ec5\u5177\u5907\u57fa\u7840bash\u5de5\u5177\u7684\u7b80\u5355\u7ed3\u6784\u51fa\u53d1\uff0c\u5728\u89e3\u51b3\u771f\u5b9e\u8f6f\u4ef6\u95ee\u9898\u7684\u8fc7\u7a0b\u4e2d\u4e0d\u65ad\u81ea\u6211\u6f14\u5316\uff0c\u65e0\u9700\u79bb\u7ebf\u8bad\u7ec3\u5373\u53ef\u5728SWE-bench Verified\u548cSWE-Bench Pro\u57fa\u51c6\u4e0a\u53d6\u5f97\u9886\u5148\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u8f6f\u4ef6\u667a\u80fd\u4f53\u901a\u5e38\u9700\u8981\u4e13\u95e8\u8bbe\u8ba1\u4e14\u96be\u4ee5\u7a77\u5c3d\u6240\u6709\u67b6\u6784\u53ef\u80fd\u6027\uff0c\u800c\u81ea\u6539\u8fdb\u667a\u80fd\u4f53\u53c8\u4f9d\u8d56\u6602\u8d35\u7684\u79bb\u7ebf\u8bad\u7ec3\u4e14\u6cdb\u5316\u80fd\u529b\u6709\u9650\u3002\u56e0\u6b64\uff0c\u4e9f\u9700\u4e00\u79cd\u80fd\u5728\u8fd0\u884c\u65f6\u81ea\u4e3b\u6f14\u5316\u3001\u65e0\u9700\u9884\u8bad\u7ec3\u4e14\u5177\u6709\u826f\u597d\u6cdb\u5316\u80fd\u529b\u7684\u8f6f\u4ef6\u667a\u80fd\u4f53\u3002", "method": "Live-SWE-agent\u4ece\u6700\u7b80\u667a\u80fd\u4f53\u7ed3\u6784\uff08\u5982mini-SWE-agent\uff09\u5f00\u59cb\uff0c\u4ec5\u63d0\u4f9bbash\u5de5\u5177\u8bbf\u95ee\u6743\u9650\uff0c\u5728\u89e3\u51b3\u771f\u5b9e\u8f6f\u4ef6\u4efb\u52a1\u7684\u8fc7\u7a0b\u4e2d\u52a8\u6001\u5730\u81ea\u6211\u4fee\u6539\u548c\u4f18\u5316\u5176\u667a\u80fd\u4f53\u67b6\u6784\uff0c\u5b9e\u73b0\u5728\u7ebf\u6301\u7eed\u8fdb\u5316\u3002", "result": "\u5728SWE-bench Verified\u57fa\u51c6\u4e0a\uff0cLive-SWE-agent\u5728\u65e0\u6d4b\u8bd5\u65f6\u6269\u5c55\u7684\u60c5\u51b5\u4e0b\u8fbe\u523075.4%\u7684\u89e3\u51b3\u7387\uff0c\u8d85\u8d8a\u6240\u6709\u5f00\u6e90\u8f6f\u4ef6\u667a\u80fd\u4f53\u5e76\u63a5\u8fd1\u6700\u4f73\u95ed\u6e90\u65b9\u6848\uff1b\u5728SWE-Bench Pro\u57fa\u51c6\u4e0a\u4ee545.8%\u7684\u89e3\u51b3\u7387\u4f18\u4e8e\u5f53\u524d\u6700\u5148\u8fdb\u7684\u624b\u5de5\u8bbe\u8ba1\u667a\u80fd\u4f53\u3002", "conclusion": "Live-SWE-agent\u5c55\u793a\u4e86\u8fd0\u884c\u65f6\u81ea\u4e3b\u6f14\u5316\u7684\u53ef\u884c\u6027\u4e0e\u4f18\u8d8a\u6027\uff0c\u4e3a\u6784\u5efa\u9ad8\u6548\u3001\u901a\u7528\u4e14\u65e0\u9700\u5927\u91cf\u79bb\u7ebf\u8bad\u7ec3\u7684\u8f6f\u4ef6\u667a\u80fd\u4f53\u63d0\u4f9b\u4e86\u65b0\u8303\u5f0f\u3002"}}
{"id": "2511.11739", "categories": ["cs.DC", "cond-mat.mtrl-sci", "cs.LG", "math.OC", "stat.CO"], "pdf": "https://arxiv.org/pdf/2511.11739", "abs": "https://arxiv.org/abs/2511.11739", "authors": ["Christina Schenk", "Miguel Hern\u00e1ndez-del-Valle", "Luis Calero-Lumbreras", "Marcus Noack", "Maciej Haranczyk"], "title": "Noise-Aware Optimization in Nominally Identical Manufacturing and Measuring Systems for High-Throughput Parallel Workflows", "comment": "17 pages, 4 figures, 2 tables", "summary": "Device-to-device variability in experimental noise critically impacts reproducibility, especially in automated, high-throughput systems like additive manufacturing farms. While manageable in small labs, such variability can escalate into serious risks at larger scales, such as architectural 3D printing, where noise may cause structural or economic failures. This contribution presents a noise-aware decision-making algorithm that quantifies and models device-specific noise profiles to manage variability adaptively. It uses distributional analysis and pairwise divergence metrics with clustering to choose between single-device and robust multi-device Bayesian optimization strategies. Unlike conventional methods that assume homogeneous devices or generic robustness, this framework explicitly leverages inter-device differences to enhance performance, reproducibility, and efficiency. An experimental case study involving three nominally identical 3D printers (same brand, model, and close serial numbers) demonstrates reduced redundancy, lower resource usage, and improved reliability. Overall, this framework establishes a paradigm for precision- and resource-aware optimization in scalable, automated experimental platforms.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u566a\u58f0\u611f\u77e5\u7684\u51b3\u7b56\u7b97\u6cd5\uff0c\u901a\u8fc7\u5efa\u6a21\u8bbe\u5907\u7279\u5f02\u6027\u566a\u58f0\u5e76\u5229\u7528\u8bbe\u5907\u95f4\u5dee\u5f02\uff0c\u5728\u591a\u8bbe\u5907\u81ea\u52a8\u5316\u7cfb\u7edf\uff08\u59823D\u6253\u5370\u519c\u573a\uff09\u4e2d\u5b9e\u73b0\u66f4\u9ad8\u6548\u3001\u53ef\u9760\u548c\u53ef\u590d\u73b0\u7684\u8d1d\u53f6\u65af\u4f18\u5316\u3002", "motivation": "\u5728\u9ad8\u901a\u91cf\u81ea\u52a8\u5316\u7cfb\u7edf\uff08\u5982\u589e\u6750\u5236\u9020\u96c6\u7fa4\uff09\u4e2d\uff0c\u8bbe\u5907\u95f4\u7684\u5b9e\u9a8c\u566a\u58f0\u5dee\u5f02\u4e25\u91cd\u5f71\u54cd\u7ed3\u679c\u7684\u53ef\u590d\u73b0\u6027\uff0c\u5e76\u53ef\u80fd\u5728\u5927\u89c4\u6a21\u5e94\u7528\uff08\u5982\u5efa\u7b513D\u6253\u5370\uff09\u4e2d\u5f15\u53d1\u7ed3\u6784\u6216\u7ecf\u6d4e\u98ce\u9669\u3002\u4f20\u7edf\u65b9\u6cd5\u5047\u8bbe\u8bbe\u5907\u540c\u8d28\u6216\u91c7\u7528\u901a\u7528\u9c81\u68d2\u7b56\u7565\uff0c\u65e0\u6cd5\u6709\u6548\u5e94\u5bf9\u8bbe\u5907\u7279\u5f02\u6027\u566a\u58f0\u3002", "method": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u5206\u5e03\u5206\u6790\u548c\u6210\u5bf9\u6563\u5ea6\u5ea6\u91cf\u7ed3\u5408\u805a\u7c7b\uff0c\u91cf\u5316\u5e76\u5efa\u6a21\u5404\u8bbe\u5907\u7684\u566a\u58f0\u7279\u5f81\uff0c\u636e\u6b64\u81ea\u9002\u5e94\u9009\u62e9\u5355\u8bbe\u5907\u6216\u9c81\u68d2\u591a\u8bbe\u5907\u8d1d\u53f6\u65af\u4f18\u5316\u7b56\u7565\uff0c\u663e\u5f0f\u5229\u7528\u8bbe\u5907\u95f4\u5dee\u5f02\u63d0\u5347\u6027\u80fd\u3002", "result": "\u5728\u4e09\u53f0\u540d\u4e49\u4e0a\u76f8\u540c\u76843D\u6253\u5370\u673a\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u6846\u67b6\u51cf\u5c11\u4e86\u5197\u4f59\u3001\u964d\u4f4e\u4e86\u8d44\u6e90\u6d88\u8017\uff0c\u5e76\u63d0\u9ad8\u4e86\u53ef\u9760\u6027\u3002", "conclusion": "\u8be5\u7814\u7a76\u5efa\u7acb\u4e86\u4e00\u79cd\u9762\u5411\u7cbe\u5ea6\u4e0e\u8d44\u6e90\u611f\u77e5\u7684\u4f18\u5316\u8303\u5f0f\uff0c\u9002\u7528\u4e8e\u53ef\u6269\u5c55\u7684\u81ea\u52a8\u5316\u5b9e\u9a8c\u5e73\u53f0\uff0c\u663e\u8457\u63d0\u5347\u4e86\u591a\u8bbe\u5907\u7cfb\u7edf\u7684\u53ef\u590d\u73b0\u6027\u4e0e\u6548\u7387\u3002"}}
{"id": "2511.11749", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2511.11749", "abs": "https://arxiv.org/abs/2511.11749", "authors": ["Almond Kiruthu Murimi"], "title": "How Machine Learning-Data Driven Replication Strategies Enhance Fault Tolerance in Large-Scale Distributed Systems", "comment": "9 pages, 4 tables, seminar paper", "summary": "This research paper investigates how machine learning-driven data replication strategies can enhance fault tolerance in large-scale distributed systems. Traditional replication methods, which rely on static configurations, often struggle to adapt to dynamic workloads and unexpected failures, leading to inefficient resource utilization and prolonged downtime. By integrating machine learning techniques-specifically predictive analytics and reinforcement learning. The study proposes adaptive replication mechanisms capable of forecasting system failures and optimizing data placement in real time. Through an extensive literature review, qualitative analysis, and comparative evaluations with traditional approaches, the paper identifies key limitations in existing replication strategies and highlights the transformative potential of machine learning in creating more resilient, self-optimizing systems. The findings underscore both the promise and the challenges of implementing ML-driven solutions in real-world environments, offering recommendations for future research and practical deployment in cloud-based and enterprise systems.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u57fa\u4e8e\u673a\u5668\u5b66\u4e60\u7684\u6570\u636e\u590d\u5236\u7b56\u7565\u5982\u4f55\u63d0\u5347\u5927\u89c4\u6a21\u5206\u5e03\u5f0f\u7cfb\u7edf\u7684\u5bb9\u9519\u80fd\u529b\uff0c\u63d0\u51fa\u5229\u7528\u9884\u6d4b\u5206\u6790\u548c\u5f3a\u5316\u5b66\u4e60\u5b9e\u73b0\u81ea\u9002\u5e94\u590d\u5236\u673a\u5236\uff0c\u5e76\u901a\u8fc7\u6587\u732e\u7efc\u8ff0\u4e0e\u5bf9\u6bd4\u5206\u6790\u9a8c\u8bc1\u5176\u4f18\u52bf\u4e0e\u6311\u6218\u3002", "motivation": "\u4f20\u7edf\u9759\u6001\u6570\u636e\u590d\u5236\u65b9\u6cd5\u96be\u4ee5\u5e94\u5bf9\u52a8\u6001\u8d1f\u8f7d\u548c\u7a81\u53d1\u6545\u969c\uff0c\u5bfc\u81f4\u8d44\u6e90\u5229\u7528\u6548\u7387\u4f4e\u4e0b\u548c\u7cfb\u7edf\u505c\u673a\u65f6\u95f4\u5ef6\u957f\uff0c\u4e9f\u9700\u66f4\u667a\u80fd\u3001\u81ea\u9002\u5e94\u7684\u5bb9\u9519\u673a\u5236\u3002", "method": "\u7ed3\u5408\u9884\u6d4b\u5206\u6790\u4e0e\u5f3a\u5316\u5b66\u4e60\uff0c\u8bbe\u8ba1\u80fd\u5b9e\u65f6\u9884\u6d4b\u7cfb\u7edf\u6545\u969c\u5e76\u4f18\u5316\u6570\u636e\u653e\u7f6e\u7684\u81ea\u9002\u5e94\u590d\u5236\u673a\u5236\uff1b\u901a\u8fc7\u6587\u732e\u7efc\u8ff0\u3001\u5b9a\u6027\u5206\u6790\u53ca\u4e0e\u4f20\u7edf\u65b9\u6cd5\u7684\u5bf9\u6bd4\u8bc4\u4f30\u8fdb\u884c\u7814\u7a76\u3002", "result": "\u8bc6\u522b\u51fa\u73b0\u6709\u590d\u5236\u7b56\u7565\u7684\u5173\u952e\u5c40\u9650\u6027\uff0c\u9a8c\u8bc1\u4e86\u673a\u5668\u5b66\u4e60\u5728\u6784\u5efa\u66f4\u5177\u5f39\u6027\u3001\u81ea\u6211\u4f18\u5316\u7cfb\u7edf\u4e2d\u7684\u53d8\u9769\u6f5c\u529b\uff0c\u540c\u65f6\u63ed\u793a\u4e86\u5b9e\u9645\u90e8\u7f72\u4e2d\u7684\u6311\u6218\u3002", "conclusion": "\u673a\u5668\u5b66\u4e60\u9a71\u52a8\u7684\u590d\u5236\u7b56\u7565\u5728\u63d0\u5347\u7cfb\u7edf\u5bb9\u9519\u6027\u548c\u8d44\u6e90\u6548\u7387\u65b9\u9762\u524d\u666f\u5e7f\u9614\uff0c\u4f46\u9700\u8fdb\u4e00\u6b65\u7814\u7a76\u4ee5\u89e3\u51b3\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u590d\u6742\u6027\uff0c\u4e3a\u4e91\u548c\u4f01\u4e1a\u7cfb\u7edf\u63d0\u4f9b\u53ef\u884c\u8def\u5f84\u3002"}}
{"id": "2511.11843", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2511.11843", "abs": "https://arxiv.org/abs/2511.11843", "authors": ["Yiwei Zhao", "Qiushi Lin", "Hongbo Kang", "Guy E. Blelloch", "Laxman Dhulipala", "Charles McGuffey", "Phillip B. Gibbons"], "title": "TD-Orch: Scalable Load-Balancing for Distributed Systems with Applications to Graph Processing", "comment": null, "summary": "In this paper, we highlight a task-data orchestration abstraction that supports a range of distributed applications, including graph processing and key-value stores. Given a batch of tasks each requesting one or more data items, where both tasks and data are distributed across multiple machines, each task must get co-located with its target data (by moving tasks and/or data) and executed. We present TD-Orch, an efficient and scalable orchestration framework featuring a simple application developer interface. TD-Orch employs a distributed push-pull technique, leveraging the bidirectional f low of both tasks and data to achieve scalable load balance across machines even under highly skewed data request (data hot spots), with minimal communication overhead. Experimental results show that TD-Orch achieves up to 2.7x speedup over existing distributed scheduling baselines. Building on TD-Orch, we present TDO-GP, a distributed graph processing system for general graph problems, demonstrating the effectiveness of the underlying framework. We design three families of implementation techniques to fully leverage the execution flow provided by TD-Orch. Experimental results show that TDO-GP achieves an average speedup of 4.1x over the best prior open-source distributed graph systems for general graph processing.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u4efb\u52a1-\u6570\u636e\u534f\u540c\u8c03\u5ea6\u6846\u67b6TD-Orch\uff0c\u901a\u8fc7\u5206\u5e03\u5f0f\u63a8\u62c9\u673a\u5236\u5b9e\u73b0\u9ad8\u6548\u8d1f\u8f7d\u5747\u8861\uff0c\u5e76\u5728\u6b64\u57fa\u7840\u4e0a\u6784\u5efa\u4e86\u56fe\u5904\u7406\u7cfb\u7edfTDO-GP\uff0c\u5728\u901a\u7528\u56fe\u5904\u7406\u4efb\u52a1\u4e2d\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u7cfb\u7edf\u3002", "motivation": "\u5728\u5206\u5e03\u5f0f\u5e94\u7528\uff08\u5982\u56fe\u5904\u7406\u548c\u952e\u503c\u5b58\u50a8\uff09\u4e2d\uff0c\u4efb\u52a1\u4e0e\u6240\u9700\u6570\u636e\u901a\u5e38\u5206\u5e03\u5728\u4e0d\u540c\u673a\u5668\u4e0a\uff0c\u9700\u5c06\u4efb\u52a1\u4e0e\u76ee\u6807\u6570\u636e\u534f\u540c\u8c03\u5ea6\u81f3\u540c\u4e00\u8282\u70b9\u6267\u884c\u3002\u73b0\u6709\u8c03\u5ea6\u65b9\u6cd5\u5728\u9762\u5bf9\u6570\u636e\u70ed\u70b9\u65f6\u96be\u4ee5\u9ad8\u6548\u6269\u5c55\u4e14\u901a\u4fe1\u5f00\u9500\u5927\u3002", "method": "\u63d0\u51faTD-Orch\u6846\u67b6\uff0c\u91c7\u7528\u5206\u5e03\u5f0f\u63a8\u62c9\u6280\u672f\uff0c\u5141\u8bb8\u4efb\u52a1\u548c\u6570\u636e\u53cc\u5411\u6d41\u52a8\uff0c\u4ee5\u5b9e\u73b0\u53ef\u6269\u5c55\u7684\u8d1f\u8f7d\u5747\u8861\uff1b\u5e76\u57fa\u4e8e\u6b64\u6784\u5efaTDO-GP\u56fe\u5904\u7406\u7cfb\u7edf\uff0c\u8bbe\u8ba1\u4e09\u7c7b\u5b9e\u73b0\u6280\u672f\u4ee5\u5145\u5206\u5229\u7528TD-Orch\u7684\u6267\u884c\u6d41\u7a0b\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cTD-Orch\u76f8\u6bd4\u73b0\u6709\u8c03\u5ea6\u57fa\u7ebf\u6700\u9ad8\u63d0\u901f2.7\u500d\uff1bTDO-GP\u5728\u901a\u7528\u56fe\u5904\u7406\u4efb\u52a1\u4e2d\u5e73\u5747\u6bd4\u73b0\u6709\u5f00\u6e90\u7cfb\u7edf\u5feb4.1\u500d\u3002", "conclusion": "TD-Orch\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u3001\u53ef\u6269\u5c55\u7684\u4efb\u52a1-\u6570\u636e\u534f\u540c\u8c03\u5ea6\u62bd\u8c61\uff0c\u80fd\u6709\u6548\u5e94\u5bf9\u6570\u636e\u70ed\u70b9\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u5206\u5e03\u5f0f\u5e94\u7528\uff08\u5c24\u5176\u662f\u56fe\u5904\u7406\uff09\u7684\u6027\u80fd\u3002"}}
{"id": "2511.11885", "categories": ["cs.DC", "cs.AI", "cs.DB"], "pdf": "https://arxiv.org/pdf/2511.11885", "abs": "https://arxiv.org/abs/2511.11885", "authors": ["Kausar Patherya", "Ashutosh Dhekne", "Francisco Romero"], "title": "Flash-Fusion: Enabling Expressive, Low-Latency Queries on IoT Sensor Streams with LLMs", "comment": "12 pages, 5 figures. Under review", "summary": "Smart cities and pervasive IoT deployments have generated interest in IoT data analysis across transportation and urban planning. At the same time, Large Language Models offer a new interface for exploring IoT data - particularly through natural language. Users today face two key challenges when working with IoT data using LLMs: (1) data collection infrastructure is expensive, producing terabytes of low-level sensor readings that are too granular for direct use, and (2) data analysis is slow, requiring iterative effort and technical expertise. Directly feeding all IoT telemetry to LLMs is impractical due to finite context windows, prohibitive token costs at scale, and non-interactive latencies. What is missing is a system that first parses a user's query to identify the analytical task, then selects the relevant data slices, and finally chooses the right representation before invoking an LLM.\n  We present Flash-Fusion, an end-to-end edge-cloud system that reduces the IoT data collection and analysis burden on users. Two principles guide its design: (1) edge-based statistical summarization (achieving 73.5% data reduction) to address data volume, and (2) cloud-based query planning that clusters behavioral data and assembles context-rich prompts to address data interpretation. We deploy Flash-Fusion on a university bus fleet and evaluate it against a baseline that feeds raw data to a state-of-the-art LLM. Flash-Fusion achieves a 95% latency reduction and 98% decrease in token usage and cost while maintaining high-quality responses. It enables personas across disciplines - safety officers, urban planners, fleet managers, and data scientists - to efficiently iterate over IoT data without the burden of manual query authoring or preprocessing.", "AI": {"tldr": "Flash-Fusion \u662f\u4e00\u4e2a\u7aef\u5230\u7aef\u7684\u8fb9\u7f18-\u4e91\u7cfb\u7edf\uff0c\u901a\u8fc7\u8fb9\u7f18\u7aef\u7edf\u8ba1\u6458\u8981\u548c\u4e91\u7aef\u67e5\u8be2\u89c4\u5212\uff0c\u663e\u8457\u964d\u4f4e IoT \u6570\u636e\u5206\u6790\u7684\u5ef6\u8fdf\u3001\u6210\u672c\u4e0e\u7528\u6237\u8d1f\u62c5\uff0c\u540c\u65f6\u4fdd\u6301\u9ad8\u8d28\u91cf\u7684\u81ea\u7136\u8bed\u8a00\u4ea4\u4e92\u4f53\u9a8c\u3002", "motivation": "\u5f53\u524d\u5728\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5206\u6790\u7269\u8054\u7f51\uff08IoT\uff09\u6570\u636e\u65f6\u9762\u4e34\u4e24\u5927\u6311\u6218\uff1a\u4e00\u662f\u539f\u59cb\u4f20\u611f\u5668\u6570\u636e\u91cf\u5e9e\u5927\u4e14\u8fc7\u4e8e\u7ec6\u7c92\u5ea6\uff0c\u76f4\u63a5\u4f7f\u7528\u6210\u672c\u9ad8\u6602\uff1b\u4e8c\u662f\u6570\u636e\u5206\u6790\u8fc7\u7a0b\u7f13\u6162\uff0c\u9700\u8981\u5927\u91cf\u8fed\u4ee3\u548c\u6280\u672f\u4e13\u4e1a\u77e5\u8bc6\u3002\u6b64\u5916\uff0c\u5c06\u5168\u90e8\u9065\u6d4b\u6570\u636e\u76f4\u63a5\u8f93\u5165 LLM \u4e0d\u73b0\u5b9e\uff0c\u53d7\u9650\u4e8e\u4e0a\u4e0b\u6587\u7a97\u53e3\u3001\u9ad8\u6602\u7684 token \u6210\u672c\u548c\u9ad8\u5ef6\u8fdf\u3002", "method": "Flash-Fusion \u7cfb\u7edf\u91c7\u7528\u4e24\u4e2a\u6838\u5fc3\u8bbe\u8ba1\u539f\u5219\uff1a(1) \u5728\u8fb9\u7f18\u7aef\u8fdb\u884c\u7edf\u8ba1\u6458\u8981\uff0c\u51cf\u5c11 73.5% \u7684\u539f\u59cb\u6570\u636e\u91cf\uff1b(2) \u5728\u4e91\u7aef\u8fdb\u884c\u67e5\u8be2\u89c4\u5212\uff0c\u5bf9\u884c\u4e3a\u6570\u636e\u805a\u7c7b\u5e76\u6784\u5efa\u5bcc\u542b\u4e0a\u4e0b\u6587\u7684\u63d0\u793a\uff0c\u518d\u8c03\u7528 LLM \u8fdb\u884c\u5206\u6790\u3002", "result": "\u5728\u5927\u5b66\u6821\u8f66\u8f66\u961f\u4e0a\u7684\u90e8\u7f72\u5b9e\u9a8c\u8868\u660e\uff0c\u76f8\u6bd4\u76f4\u63a5\u5c06\u539f\u59cb\u6570\u636e\u8f93\u5165\u6700\u5148\u8fdb LLM \u7684\u57fa\u7ebf\u65b9\u6cd5\uff0cFlash-Fusion \u5b9e\u73b0\u4e86 95% \u7684\u5ef6\u8fdf\u964d\u4f4e\u548c 98% \u7684 token \u4f7f\u7528\u91cf\u53ca\u6210\u672c\u4e0b\u964d\uff0c\u540c\u65f6\u4fdd\u6301\u9ad8\u8d28\u91cf\u7684\u56de\u7b54\u3002", "conclusion": "Flash-Fusion \u6709\u6548\u89e3\u51b3\u4e86 IoT \u6570\u636e\u5206\u6790\u4e2d\u6570\u636e\u91cf\u5927\u3001\u5904\u7406\u6162\u3001\u4f7f\u7528\u95e8\u69db\u9ad8\u7684\u95ee\u9898\uff0c\u4f7f\u4e0d\u540c\u9886\u57df\u7684\u7528\u6237\uff08\u5982\u5b89\u5168\u5458\u3001\u57ce\u5e02\u89c4\u5212\u5e08\u3001\u8f66\u961f\u7ecf\u7406\u548c\u6570\u636e\u79d1\u5b66\u5bb6\uff09\u80fd\u591f\u9ad8\u6548\u5730\u901a\u8fc7\u81ea\u7136\u8bed\u8a00\u4e0e IoT \u6570\u636e\u4ea4\u4e92\uff0c\u65e0\u9700\u624b\u52a8\u7f16\u5199\u67e5\u8be2\u6216\u9884\u5904\u7406\u6570\u636e\u3002"}}
{"id": "2511.11907", "categories": ["cs.DC", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.11907", "abs": "https://arxiv.org/abs/2511.11907", "authors": ["Huawei Zhang", "Chunwei Xia", "Zheng Wang"], "title": "KVSwap: Disk-aware KV Cache Offloading for Long-Context On-device Inference", "comment": null, "summary": "Language models (LMs) underpin emerging mobile and embedded AI applications like meeting and video summarization and document analysis, which often require processing multiple long-context inputs. Running an LM locally on-device improves privacy, enables offline use, and reduces cost, but long-context inference quickly hits a \\emph{memory capacity wall} as the key-value (KV) cache grows linearly with context length and batch size.\n  We present KVSwap, a software framework to break this memory wall by offloading the KV cache to non-volatile secondary storage (disk). KVSwap leverages the observation that only a small, dynamically changing subset of KV entries is critical for generation. It stores the full cache on disk, uses a compact in-memory metadata to predict which entries to preload, overlaps computation with hardware-aware disk access, and orchestrates read patterns to match storage device characteristics. Our evaluation shows that across representative LMs and storage types, KVSwap delivers higher throughput under tight memory budgets while maintaining the generation quality when compared with existing KV cache offloading schemes.", "AI": {"tldr": "KVSwap \u662f\u4e00\u79cd\u901a\u8fc7\u5c06\u8bed\u8a00\u6a21\u578b\u7684\u952e\u503c\uff08KV\uff09\u7f13\u5b58\u5378\u8f7d\u5230\u78c1\u76d8\u6765\u7a81\u7834\u8bbe\u5907\u5185\u5b58\u9650\u5236\u7684\u8f6f\u4ef6\u6846\u67b6\uff0c\u5728\u6709\u9650\u5185\u5b58\u4e0b\u5b9e\u73b0\u66f4\u9ad8\u541e\u5410\u91cf\u5e76\u4fdd\u6301\u751f\u6210\u8d28\u91cf\u3002", "motivation": "\u5728\u79fb\u52a8\u548c\u5d4c\u5165\u5f0f\u8bbe\u5907\u4e0a\u8fd0\u884c\u5927\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u957f\u4e0a\u4e0b\u6587\u63a8\u7406\u65f6\uff0cKV \u7f13\u5b58\u968f\u4e0a\u4e0b\u6587\u957f\u5ea6\u548c\u6279\u5927\u5c0f\u7ebf\u6027\u589e\u957f\uff0c\u8fc5\u901f\u8d85\u51fa\u8bbe\u5907\u5185\u5b58\u5bb9\u91cf\uff0c\u9650\u5236\u4e86\u672c\u5730\u90e8\u7f72\u7684\u53ef\u884c\u6027\u3002", "method": "KVSwap \u5c06\u5b8c\u6574\u7684 KV \u7f13\u5b58\u5b58\u50a8\u5728\u78c1\u76d8\u4e0a\uff0c\u5229\u7528\u7d27\u51d1\u7684\u5185\u5b58\u5143\u6570\u636e\u9884\u6d4b\u5173\u952e\u6761\u76ee\u8fdb\u884c\u9884\u52a0\u8f7d\uff0c\u7ed3\u5408\u8ba1\u7b97\u4e0e\u786c\u4ef6\u611f\u77e5\u7684\u78c1\u76d8\u8bbf\u95ee\uff0c\u5e76\u4f18\u5316\u8bfb\u53d6\u6a21\u5f0f\u4ee5\u5339\u914d\u5b58\u50a8\u8bbe\u5907\u7279\u6027\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cKVSwap \u5728\u591a\u79cd\u8bed\u8a00\u6a21\u578b\u548c\u5b58\u50a8\u7c7b\u578b\u4e0b\uff0c\u76f8\u6bd4\u73b0\u6709 KV \u7f13\u5b58\u5378\u8f7d\u65b9\u6848\uff0c\u5728\u4e25\u683c\u5185\u5b58\u9650\u5236\u4e0b\u5b9e\u73b0\u4e86\u66f4\u9ad8\u7684\u541e\u5410\u91cf\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u751f\u6210\u8d28\u91cf\u3002", "conclusion": "KVSwap \u6709\u6548\u7f13\u89e3\u4e86\u957f\u4e0a\u4e0b\u6587\u8bed\u8a00\u6a21\u578b\u5728\u8bbe\u5907\u7aef\u63a8\u7406\u4e2d\u7684\u5185\u5b58\u74f6\u9888\u95ee\u9898\uff0c\u4e3a\u9690\u79c1\u4fdd\u62a4\u3001\u79bb\u7ebf\u4f7f\u7528\u548c\u4f4e\u6210\u672c\u90e8\u7f72\u63d0\u4f9b\u4e86\u53ef\u884c\u8def\u5f84\u3002"}}
{"id": "2511.12009", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2511.12009", "abs": "https://arxiv.org/abs/2511.12009", "authors": ["Guangchao Yao", "Yali Li"], "title": "High-Performance N-Queens Solver on GPU: Iterative DFS with Zero Bank Conflicts", "comment": null, "summary": "The counting of solutions to the N-Queens problem is a classic NP-complete problem with extremely high computational complexity. As of now, the academic community has rigorously verified the number of solutions only up to N <= 26. In 2016, the research team led by PreuBer solved the 27-Queens problem using FPGA hardware, which took approximately one year, though the result remains unverified independently. Recent studies on GPU parallel computing suggest that verifying the 27-Queens solution would still require about 17 months, indicating excessively high time and computational resource costs. To address this challenge, we propose an innovative parallel computing method on NVIDIA GPU platform, with the following core contributions: (1) An iterative depth-first search (DFS) algorithm for solving the N-Queens problem; (2) Complete mapping of the required stack structure to GPU shared memory; (3) Effective avoidance of bank conflicts through meticulously designed memory access patterns; (4) Various optimization techniques are employed to achieve optimal performance. Under the proposed optimization framework, we successfully verified the 27-Queens problem in just 28.4 days using eight RTX 5090 GPUs, thereby confirming the correctness of PreuBer's computational results. Moreover, we have reduced the projected solving time for the next open case-the 28-Queens problem-to approximately 11 months, making its resolution computationally feasible. Compared to the state-of-the-art GPU methods, our method achieves over 10x speedup on identical hardware configurations (8 A100), while delivering over 26x acceleration when utilizing 8 RTX 5090 GPUs, and brings fresh perspectives to this long-stagnant problem.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5728NVIDIA GPU\u5e73\u53f0\u4e0a\u9ad8\u6548\u6c42\u89e3N\u7687\u540e\u95ee\u9898\u7684\u5e76\u884c\u8ba1\u7b97\u65b9\u6cd5\uff0c\u901a\u8fc7\u8fed\u4ee3\u6df1\u5ea6\u4f18\u5148\u641c\u7d22\u3001\u5171\u4eab\u5185\u5b58\u4f18\u5316\u548c\u907f\u514dbank\u51b2\u7a81\u7b49\u6280\u672f\uff0c\u57288\u5757RTX 5090 GPU\u4e0a\u4ec5\u752828.4\u5929\u9a8c\u8bc1\u4e8627\u7687\u540e\u95ee\u9898\u7684\u89e3\uff0c\u5e76\u5c0628\u7687\u540e\u95ee\u9898\u7684\u9884\u8ba1\u6c42\u89e3\u65f6\u95f4\u7f29\u77ed\u81f3\u7ea611\u4e2a\u6708\uff0c\u76f8\u6bd4\u73b0\u6709GPU\u65b9\u6cd5\u5b9e\u73b010\u500d\u4ee5\u4e0a\u52a0\u901f\u3002", "motivation": "N\u7687\u540e\u95ee\u9898\u7684\u89e3\u8ba1\u6570\u662f\u7ecf\u5178\u7684NP\u5b8c\u5168\u95ee\u9898\uff0c\u8ba1\u7b97\u590d\u6742\u5ea6\u6781\u9ad8\u3002\u76ee\u524d\u5b66\u672f\u754c\u4ec5\u4e25\u683c\u9a8c\u8bc1\u5230N\u226426\uff1b2016\u5e74PreuBer\u56e2\u961f\u4f7f\u7528FPGA\u8017\u65f6\u7ea6\u4e00\u5e74\u6c42\u89e327\u7687\u540e\u95ee\u9898\u4f46\u672a\u88ab\u72ec\u7acb\u9a8c\u8bc1\uff0c\u800c\u73b0\u6709GPU\u65b9\u6cd5\u4ecd\u9700\u7ea617\u4e2a\u6708\uff0c\u65f6\u95f4\u548c\u8d44\u6e90\u6210\u672c\u8fc7\u9ad8\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8eNVIDIA GPU\u7684\u5e76\u884c\u8ba1\u7b97\u65b9\u6cd5\uff0c\u6838\u5fc3\u5305\u62ec\uff1a(1) \u8fed\u4ee3\u5f0f\u6df1\u5ea6\u4f18\u5148\u641c\u7d22\u7b97\u6cd5\uff1b(2) \u5c06\u6240\u9700\u6808\u7ed3\u6784\u5b8c\u6574\u6620\u5c04\u81f3GPU\u5171\u4eab\u5185\u5b58\uff1b(3) \u901a\u8fc7\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u5185\u5b58\u8bbf\u95ee\u6a21\u5f0f\u6709\u6548\u907f\u514dbank\u51b2\u7a81\uff1b(4) \u5e94\u7528\u591a\u79cd\u4f18\u5316\u6280\u672f\u4ee5\u5b9e\u73b0\u6700\u4f73\u6027\u80fd\u3002", "result": "\u57288\u5757RTX 5090 GPU\u4e0a\u4ec5\u752828.4\u5929\u6210\u529f\u9a8c\u8bc127\u7687\u540e\u95ee\u9898\uff0c\u786e\u8ba4\u4e86PreuBer\u56e2\u961f\u7ed3\u679c\u7684\u6b63\u786e\u6027\uff1b\u5e76\u5c0628\u7687\u540e\u95ee\u9898\u7684\u9884\u8ba1\u6c42\u89e3\u65f6\u95f4\u7f29\u77ed\u81f3\u7ea611\u4e2a\u6708\u3002\u76f8\u6bd4\u5f53\u524d\u6700\u5148\u8fdb\u7684GPU\u65b9\u6cd5\uff0c\u5728\u76f8\u540c\u786c\u4ef6\uff088 A100\uff09\u4e0a\u5b9e\u73b0\u8d8510\u500d\u52a0\u901f\uff0c\u4f7f\u75288 RTX 5090\u65f6\u52a0\u901f\u6bd4\u8d85\u8fc726\u500d\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684GPU\u5e76\u884c\u4f18\u5316\u6846\u67b6\u663e\u8457\u63d0\u5347\u4e86N\u7687\u540e\u95ee\u9898\u7684\u6c42\u89e3\u6548\u7387\uff0c\u4e0d\u4ec5\u9a8c\u8bc1\u4e86\u5df2\u6709\u7ed3\u679c\uff0c\u8fd8\u4f7f\u66f4\u9ad8\u89c4\u6a21\u95ee\u9898\u7684\u6c42\u89e3\u53d8\u5f97\u53ef\u884c\uff0c\u4e3a\u8fd9\u4e00\u957f\u671f\u505c\u6ede\u7684\u95ee\u9898\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2511.12025", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2511.12025", "abs": "https://arxiv.org/abs/2511.12025", "authors": ["Ivan Cao", "Jaromir J. Saloni", "David A. G. Harrison"], "title": "A Quick and Exact Method for Distributed Quantile Computation", "comment": "10 pages, 2 figures. Draft version for testing and feedback", "summary": "Quantile computation is a core primitive in large-scale data analytics. In Spark, practitioners typically rely on the Greenwald-Khanna (GK) Sketch, an approximate method. When exact quantiles are required, the default option is an expensive global sort. We present GK Select, an exact Spark algorithm that avoids full-data shuffles and completes in a constant number of actions. GK Select leverages GK Sketch to identify a near-target pivot, extracts all values within the error bound around this pivot in each partition in linear time, and then tree-reduces the resulting candidate sets. We show analytically that GK Select matches the executor-side time complexity of GK Sketch while returning the exact quantile. Empirically, GK Select achieves sketch-level latency and outperforms Spark's full sort by approximately 10.5x on 10^9 values across 120 partitions on a 30-core AWS EMR cluster.", "AI": {"tldr": "GK Select \u662f\u4e00\u79cd\u5728 Spark \u4e2d\u9ad8\u6548\u8ba1\u7b97\u7cbe\u786e\u5206\u4f4d\u6570\u7684\u65b0\u7b97\u6cd5\uff0c\u5b83\u7ed3\u5408\u4e86 GK Sketch \u7684\u8fd1\u4f3c\u80fd\u529b\u4e0e\u5c40\u90e8\u7b5b\u9009\u548c\u6811\u5f52\u7ea6\u7b56\u7565\uff0c\u5728\u907f\u514d\u5168\u5c40\u6392\u5e8f\u7684\u540c\u65f6\u5b9e\u73b0\u4e0e\u8fd1\u4f3c\u65b9\u6cd5\u76f8\u5f53\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709 Spark \u7cfb\u7edf\u4e2d\uff0c\u82e5\u9700\u7cbe\u786e\u5206\u4f4d\u6570\u5219\u4f9d\u8d56\u4ee3\u4ef7\u9ad8\u6602\u7684\u5168\u5c40\u6392\u5e8f\uff0c\u800c\u8fd1\u4f3c\u65b9\u6cd5\uff08\u5982 GK Sketch\uff09\u867d\u5feb\u4f46\u4e0d\u7cbe\u786e\u3002\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u65e2\u80fd\u4fdd\u8bc1\u7cbe\u786e\u6027\u53c8\u5177\u5907\u9ad8\u6548\u7387\u7684\u5206\u4f4d\u6570\u8ba1\u7b97\u65b9\u6cd5\u3002", "method": "GK Select \u5229\u7528 GK Sketch \u627e\u5230\u63a5\u8fd1\u76ee\u6807\u5206\u4f4d\u6570\u7684\u67a2\u8f74\u503c\uff0c\u5728\u6bcf\u4e2a\u5206\u533a\u7ebf\u6027\u65f6\u95f4\u5185\u63d0\u53d6\u8be5\u67a2\u8f74\u8bef\u5dee\u8303\u56f4\u5185\u7684\u6240\u6709\u5019\u9009\u503c\uff0c\u518d\u901a\u8fc7\u6811\u5f52\u7ea6\u5408\u5e76\u5019\u9009\u96c6\u4ee5\u83b7\u5f97\u7cbe\u786e\u7ed3\u679c\u3002", "result": "\u7406\u8bba\u5206\u6790\u8868\u660e GK Select \u5728\u6267\u884c\u5668\u7aef\u7684\u65f6\u95f4\u590d\u6742\u5ea6\u4e0e GK Sketch \u76f8\u5f53\uff1b\u5b9e\u9a8c\u663e\u793a\u5176\u5728 10^9 \u6570\u636e\u91cf\u3001120 \u5206\u533a\u300130 \u6838 AWS EMR \u96c6\u7fa4\u4e0a\u6bd4 Spark \u5168\u5c40\u6392\u5e8f\u5feb\u7ea6 10.5 \u500d\uff0c\u4e14\u8fbe\u5230\u8fd1\u4f3c\u7b97\u6cd5\u7684\u5ef6\u8fdf\u6c34\u5e73\u3002", "conclusion": "GK Select \u6210\u529f\u5728\u4fdd\u6301\u7cbe\u786e\u6027\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u4e86 Spark \u4e2d\u5206\u4f4d\u6570\u8ba1\u7b97\u7684\u6548\u7387\uff0c\u4e3a\u5927\u89c4\u6a21\u6570\u636e\u5206\u6790\u63d0\u4f9b\u4e86\u4e00\u79cd\u5b9e\u7528\u4e14\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2511.12031", "categories": ["cs.DC", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.12031", "abs": "https://arxiv.org/abs/2511.12031", "authors": ["Arun Ramachandran", "Ramaswamy Govindarajan", "Murali Annavaram", "Prakash Raghavendra", "Hossein Entezari Zarch", "Lei Gao", "Chaoyi Jiang"], "title": "Striking the Right Balance between Compute and Copy: Improving LLM Inferencing Under Speculative Decoding", "comment": null, "summary": "With the skyrocketing costs of GPUs and their virtual instances in the cloud, there is a significant desire to use CPUs for large language model (LLM) inference. KV cache update, often implemented as allocation, copying, and in-place strided update for each generated token, incurs significant overhead. As the sequence length increases, the allocation and copy overheads dominate the performance. Alternate approaches may allocate large KV tensors upfront to enable in-place updates, but these matrices (with zero-padded rows) cause redundant computations. In this work, we propose a new KV cache allocation mechanism called Balancing Memory and Compute (BMC). BMC allocates, once every r iterations, KV tensors with r redundant rows, allowing in-place update without copy overhead for those iterations, but at the expense of a small amount of redundant computation. Second, we make an interesting observation that the extra rows allocated in the KV tensors and the resulting redundant computation can be repurposed for Speculative Decoding (SD) that improves token generation efficiency. Last, BMC represents a spectrum of design points with different values of r. To identify the best-performing design point(s), we derive a simple analytical model for BMC. The proposed BMC method achieves an average throughput acceleration of up to 3.2x over baseline HuggingFace (without SD). Importantly when we apply BMC with SD, it results in an additional speedup of up to 1.39x, over and above the speedup offered by SD. Further, BMC achieves a throughput acceleration of up to 1.36x and 2.29x over state-of-the-art inference servers vLLM and DeepSpeed, respectively. Although the BMC technique is evaluated extensively across different classes of CPUs (desktop and server class), we also evaluate the scheme with GPUs and demonstrate that it works well for GPUs.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aBMC\uff08Balancing Memory and Compute\uff09\u7684\u65b0\u578bKV\u7f13\u5b58\u5206\u914d\u673a\u5236\uff0c\u901a\u8fc7\u6bcfr\u6b21\u8fed\u4ee3\u4e00\u6b21\u6027\u5206\u914d\u5e26\u6709r\u4e2a\u5197\u4f59\u884c\u7684KV\u5f20\u91cf\uff0c\u5728\u907f\u514d\u9891\u7e41\u5185\u5b58\u5206\u914d\u4e0e\u62f7\u8d1d\u5f00\u9500\u7684\u540c\u65f6\u5f15\u5165\u5c11\u91cf\u5197\u4f59\u8ba1\u7b97\uff0c\u5e76\u8fdb\u4e00\u6b65\u5c06\u8fd9\u4e9b\u5197\u4f59\u8ba1\u7b97\u7528\u4e8e\u63a8\u6d4b\u89e3\u7801\uff08Speculative Decoding\uff09\uff0c\u4ece\u800c\u663e\u8457\u63d0\u5347LLM\u63a8\u7406\u541e\u5410\u91cf\u3002\u5b9e\u9a8c\u8868\u660e\uff0cBMC\u5728CPU\u4e0a\u76f8\u8f83HuggingFace\u57fa\u7ebf\u6700\u9ad8\u63d0\u901f3.2\u500d\uff0c\u7ed3\u5408SD\u540e\u989d\u5916\u63d0\u901f1.39\u500d\uff0c\u5e76\u4f18\u4e8evLLM\u548cDeepSpeed\u7b49\u5148\u8fdb\u63a8\u7406\u7cfb\u7edf\uff0c\u4e14\u5728GPU\u4e0a\u540c\u6837\u6709\u6548\u3002", "motivation": "\u968f\u7740GPU\u53ca\u5176\u4e91\u865a\u62df\u5b9e\u4f8b\u6210\u672c\u98d9\u5347\uff0c\u4e1a\u754c\u5e0c\u671b\u5229\u7528CPU\u8fdb\u884c\u5927\u8bed\u8a00\u6a21\u578b\u63a8\u7406\u3002\u7136\u800c\uff0c\u4f20\u7edfKV\u7f13\u5b58\u66f4\u65b0\u65b9\u5f0f\uff08\u6bcf\u6b21\u751f\u6210token\u65f6\u8fdb\u884c\u5206\u914d\u3001\u62f7\u8d1d\u548c\u539f\u5730\u8de8\u6b65\u66f4\u65b0\uff09\u5728\u957f\u5e8f\u5217\u4e0b\u5e26\u6765\u4e25\u91cd\u6027\u80fd\u74f6\u9888\uff1b\u800c\u9884\u5148\u5206\u914d\u5927KV\u5f20\u91cf\u867d\u53ef\u907f\u514d\u62f7\u8d1d\uff0c\u5374\u56e0\u96f6\u586b\u5145\u884c\u5bfc\u81f4\u5197\u4f59\u8ba1\u7b97\u3002\u56e0\u6b64\uff0c\u4e9f\u9700\u4e00\u79cd\u517c\u987e\u5185\u5b58\u6548\u7387\u4e0e\u8ba1\u7b97\u5f00\u9500\u7684\u65b0\u673a\u5236\u3002", "method": "\u63d0\u51faBMC\u673a\u5236\uff1a\u6bcfr\u6b21\u8fed\u4ee3\u4e00\u6b21\u6027\u5206\u914d\u5305\u542br\u4e2a\u5197\u4f59\u884c\u7684KV\u5f20\u91cf\uff0c\u5b9e\u73b0r\u6b21\u8fed\u4ee3\u5185\u65e0\u62f7\u8d1d\u7684\u539f\u5730\u66f4\u65b0\uff1b\u540c\u65f6\u89c2\u5bdf\u5230\u8fd9\u4e9b\u5197\u4f59\u884c\u53ef\u7528\u4e8e\u63a8\u6d4b\u89e3\u7801\uff08SD\uff09\u4ee5\u63d0\u5347\u751f\u6210\u6548\u7387\uff1b\u5e76\u6784\u5efa\u5206\u6790\u6a21\u578b\u4ee5\u9009\u62e9\u6700\u4f18r\u503c\u3002", "result": "BMC\u5728CPU\u4e0a\u76f8\u6bd4HuggingFace\u57fa\u7ebf\u5e73\u5747\u541e\u5410\u91cf\u6700\u9ad8\u63d0\u53473.2\u500d\uff1b\u7ed3\u5408SD\u540e\u989d\u5916\u83b7\u5f97\u6700\u9ad81.39\u500d\u52a0\u901f\uff1b\u76f8\u6bd4vLLM\u548cDeepSpeed\u5206\u522b\u6700\u9ad8\u63d0\u901f1.36\u500d\u548c2.29\u500d\uff1b\u4e14\u5728GPU\u4e0a\u4e5f\u8868\u73b0\u826f\u597d\u3002", "conclusion": "BMC\u901a\u8fc7\u5e73\u8861\u5185\u5b58\u5206\u914d\u4e0e\u8ba1\u7b97\u5197\u4f59\uff0c\u6709\u6548\u89e3\u51b3\u4e86LLM\u63a8\u7406\u4e2dKV\u7f13\u5b58\u66f4\u65b0\u7684\u6027\u80fd\u74f6\u9888\uff0c\u4e0d\u4ec5\u663e\u8457\u63d0\u5347CPU\u63a8\u7406\u6548\u7387\uff0c\u8fd8\u80fd\u4e0e\u63a8\u6d4b\u89e3\u7801\u534f\u540c\u589e\u6548\uff0c\u5e76\u5728\u591a\u79cd\u786c\u4ef6\u5e73\u53f0\u4e0a\u5177\u6709\u901a\u7528\u6027\u3002"}}
{"id": "2511.12185", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2511.12185", "abs": "https://arxiv.org/abs/2511.12185", "authors": ["Mills Staylor", "Arup Kumar Sarker", "Gregor von Laszewski", "Geoffrey Fox", "Yue Cheng", "Judy Fox"], "title": "Combining Serverless and High-Performance Computing Paradigms to support ML Data-Intensive Applications", "comment": "12 pages, 9 figures, 3 tables", "summary": "Data is found everywhere, from health and human infrastructure to the surge of sensors and the proliferation of internet-connected devices. To meet this challenge, the data engineering field has expanded significantly in recent years in both research and industry. Traditionally, data engineering, Machine Learning, and AI workloads have been run on large clusters within data center environments, requiring substantial investment in hardware and maintenance. With the rise of the public cloud, it is now possible to run large applications across nodes without owning or maintaining hardware. Serverless functions such as AWS Lambda provide horizontal scaling and precise billing without the hassle of managing traditional cloud infrastructure. However, when processing large datasets, users often rely on external storage options that are significantly slower than direct communication typical of HPC clusters. We introduce Cylon, a high-performance distributed data frame solution that has shown promising results for data processing using Python. We describe how we took inspiration from the FMI library and designed a serverless communicator to tackle communication and performance issues associated with serverless functions. With our design, we demonstrate that the performance of AWS Lambda falls below one percent of strong scaling experiments compared to serverful AWS (EC2) and HPCs based on implementing direct communication via NAT Traversal TCP Hole Punching.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aCylon\u7684\u9ad8\u6027\u80fd\u5206\u5e03\u5f0f\u6570\u636e\u5e27\u89e3\u51b3\u65b9\u6848\uff0c\u901a\u8fc7\u53d7FMI\u5e93\u542f\u53d1\u8bbe\u8ba1\u7684\u65e0\u670d\u52a1\u5668\u901a\u4fe1\u5668\uff0c\u89e3\u51b3\u4e86\u5728AWS Lambda\u7b49\u65e0\u670d\u52a1\u5668\u73af\u5883\u4e2d\u5904\u7406\u5927\u6570\u636e\u65f6\u56e0\u4f9d\u8d56\u5916\u90e8\u5b58\u50a8\u800c\u5bfc\u81f4\u7684\u6027\u80fd\u74f6\u9888\u95ee\u9898\uff0c\u5e76\u5c55\u793a\u4e86\u5176\u5728\u76f4\u63a5\u901a\u4fe1\uff08\u57fa\u4e8eNAT\u7a7f\u900fTCP\u6253\u6d1e\uff09\u4e0b\u7684\u6269\u5c55\u6027\u80fd\u8fdc\u4f18\u4e8e\u4f20\u7edf\u65e0\u670d\u52a1\u5668\u67b6\u6784\u3002", "motivation": "\u968f\u7740\u6570\u636e\u91cf\u6fc0\u589e\u548c\u65e0\u670d\u52a1\u5668\u8ba1\u7b97\uff08\u5982AWS Lambda\uff09\u7684\u666e\u53ca\uff0c\u4f20\u7edf\u4f9d\u8d56\u5916\u90e8\u5b58\u50a8\u7684\u6570\u636e\u5904\u7406\u65b9\u5f0f\u5728\u65e0\u670d\u52a1\u5668\u73af\u5883\u4e2d\u9762\u4e34\u4e25\u91cd\u7684\u6027\u80fd\u74f6\u9888\u3002\u4f5c\u8005\u65e8\u5728\u901a\u8fc7\u6539\u8fdb\u901a\u4fe1\u673a\u5236\u63d0\u5347\u65e0\u670d\u52a1\u5668\u5e73\u53f0\u5728\u5927\u6570\u636e\u5904\u7406\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\u8868\u73b0\u3002", "method": "\u4f5c\u8005\u57fa\u4e8eFMI\u5e93\u7684\u8bbe\u8ba1\u7406\u5ff5\uff0c\u5f00\u53d1\u4e86\u4e00\u4e2a\u4e13\u7528\u4e8e\u65e0\u670d\u52a1\u5668\u73af\u5883\u7684\u901a\u4fe1\u5668\uff0c\u5229\u7528NAT\u7a7f\u900fTCP\u6253\u6d1e\u6280\u672f\u5b9e\u73b0Lambda\u51fd\u6570\u95f4\u7684\u76f4\u63a5\u901a\u4fe1\uff0c\u4ece\u800c\u7ed5\u8fc7\u6162\u901f\u5916\u90e8\u5b58\u50a8\uff0c\u63d0\u5347\u6570\u636e\u5904\u7406\u6548\u7387\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u5728\u91c7\u7528\u76f4\u63a5\u901a\u4fe1\u673a\u5236\u540e\uff0cAWS Lambda\u7684\u5f3a\u6269\u5c55\u6027\u80fd\u4e0d\u8db3\u4f20\u7edf\u670d\u52a1\u5668\u578bAWS\uff08EC2\uff09\u548c\u9ad8\u6027\u80fd\u8ba1\u7b97\u96c6\u7fa4\uff08HPC\uff09\u76841%\u3002", "conclusion": "\u5c3d\u7ba1\u65e0\u670d\u52a1\u5668\u8ba1\u7b97\u63d0\u4f9b\u4e86\u4fbf\u6377\u7684\u6269\u5c55\u6027\u548c\u8ba1\u8d39\u6a21\u5f0f\uff0c\u4f46\u5728\u5f53\u524d\u67b6\u6784\u4e0b\u5176\u6570\u636e\u5904\u7406\u6027\u80fd\u8fdc\u843d\u540e\u4e8e\u4f20\u7edf\u670d\u52a1\u5668\u6216HPC\u7cfb\u7edf\uff1b\u901a\u8fc7\u5f15\u5165\u9ad8\u6548\u7684\u76f4\u63a5\u901a\u4fe1\u673a\u5236\uff08\u5982Cylon\u6240\u5b9e\u73b0\uff09\uff0c\u53ef\u663e\u8457\u7f13\u89e3\u8fd9\u4e00\u95ee\u9898\uff0c\u4f46\u4ecd\u5b58\u5728\u5de8\u5927\u6027\u80fd\u5dee\u8ddd\u3002"}}
{"id": "2511.12216", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2511.12216", "abs": "https://arxiv.org/abs/2511.12216", "authors": ["Van Ho-Long", "Nguyen Ho", "Anh-Vu Dinh-Duc", "Ha Manh Tran", "Ky Trung Nguyen", "Tran Dung Pham", "Quoc Viet Hung Nguyen"], "title": "Distributed Seasonal Temporal Pattern Mining", "comment": null, "summary": "The explosive growth of IoT-enabled sensors is producing enormous amounts of time series data across many domains, offering valuable opportunities to extract insights through temporal pattern mining. Among these patterns, an important class exhibits periodic occurrences, referred to as \\textit{seasonal temporal patterns} (STPs). However, mining STPs poses challenges, as traditional measures such as support and confidence cannot capture seasonality, and the lack of the anti-monotonicity property results in an exponentially large search space. Existing STP mining methods operate sequentially and therefore do not scale to large datasets. In this paper, we propose the Distributed Seasonal Temporal Pattern Mining (DSTPM), the first distributed framework for mining seasonal temporal patterns from time series. DSTPM leverages efficient data structures, specifically distributed hierarchical lookup hash structures, to enable efficient computation. Extensive experimental evaluations demonstrate that DSTPM significantly outperforms sequential baselines in runtime and memory usage, while scaling effectively to very large datasets.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u9996\u4e2a\u7528\u4e8e\u6316\u6398\u65f6\u95f4\u5e8f\u5217\u4e2d\u5b63\u8282\u6027\u65f6\u5e8f\u6a21\u5f0f\uff08STP\uff09\u7684\u5206\u5e03\u5f0f\u6846\u67b6DSTPM\uff0c\u901a\u8fc7\u5206\u5e03\u5f0f\u5206\u5c42\u54c8\u5e0c\u7ed3\u6784\u663e\u8457\u63d0\u5347\u4e86\u8fd0\u884c\u6548\u7387\u548c\u53ef\u6269\u5c55\u6027\u3002", "motivation": "\u4f20\u7edf\u65f6\u5e8f\u6a21\u5f0f\u6316\u6398\u65b9\u6cd5\u65e0\u6cd5\u6709\u6548\u6355\u6349\u5b63\u8282\u6027\u7279\u5f81\uff0c\u4e14\u7f3a\u4e4f\u53cd\u5355\u8c03\u6027\u5bfc\u81f4\u641c\u7d22\u7a7a\u95f4\u5e9e\u5927\uff1b\u73b0\u6709STP\u6316\u6398\u65b9\u6cd5\u4e3a\u4e32\u884c\u5904\u7406\uff0c\u96be\u4ee5\u5e94\u5bf9\u5927\u89c4\u6a21\u6570\u636e\u3002", "method": "\u63d0\u51fa\u5206\u5e03\u5f0f\u5b63\u8282\u6027\u65f6\u5e8f\u6a21\u5f0f\u6316\u6398\u6846\u67b6DSTPM\uff0c\u5229\u7528\u5206\u5e03\u5f0f\u5206\u5c42\u67e5\u627e\u54c8\u5e0c\u7ed3\u6784\u5b9e\u73b0\u9ad8\u6548\u8ba1\u7b97\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cDSTPM\u5728\u8fd0\u884c\u65f6\u95f4\u548c\u5185\u5b58\u4f7f\u7528\u65b9\u9762\u663e\u8457\u4f18\u4e8e\u4e32\u884c\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5e76\u80fd\u6709\u6548\u6269\u5c55\u5230\u8d85\u5927\u89c4\u6a21\u6570\u636e\u96c6\u3002", "conclusion": "DSTPM\u662f\u9996\u4e2a\u652f\u6301\u5927\u89c4\u6a21\u65f6\u95f4\u5e8f\u5217\u6570\u636e\u4e2d\u5b63\u8282\u6027\u65f6\u5e8f\u6a21\u5f0f\u9ad8\u6548\u6316\u6398\u7684\u5206\u5e03\u5f0f\u89e3\u51b3\u65b9\u6848\uff0c\u5177\u6709\u826f\u597d\u7684\u6027\u80fd\u4e0e\u53ef\u6269\u5c55\u6027\u3002"}}
{"id": "2511.12461", "categories": ["cs.DC", "eess.SP"], "pdf": "https://arxiv.org/pdf/2511.12461", "abs": "https://arxiv.org/abs/2511.12461", "authors": ["Fangqiang Du", "Sixuan Chong", "Zixuan Huang", "Rui Qin", "Fengnan Mi", "Caibao Hu", "Jiangang Chen"], "title": "Design of A Low-Latency and Parallelizable SVD Dataflow Architecture on FPGA", "comment": null, "summary": "Singular value decomposition (SVD) is widely used for dimensionality reduction and noise suppression, and it plays a pivotal role in numerous scientific and engineering applications. As the dimensions of the matrix grow rapidly, the computational cost increases significantly, posing a serious challenge to the efficiency of data analysis and signal processing systems,especially in time-sensitive scenarios with large-scale datasets. Although various dedicated hardware architectures have been proposed to accelerate the computation of intensive SVD, many of these designs suffer from limited scalability and high consumption of on-chip memory resources. Moreover, they typically overlook the computational and data transfer challenges associated with SVD, enabling them unsuitable for real-time processing of large-scale data stream matrices in embedded systems. In this express, we propose a Data Stream-Based SVD processing algorithm (DSB Jacobi), which significantly reduces on-chip BRAM usage while improving computational speed, offering a practical solution for real-time SVD computation of large-scale data streams. Compared with previous works, our experimental results indicate that the proposed method reduces on-chip RAM consumption by 41.5 percent and improves computational efficiency by 23 times.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6570\u636e\u6d41\u7684SVD\u5904\u7406\u7b97\u6cd5\uff08DSB Jacobi\uff09\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u7247\u4e0aBRAM\u4f7f\u7528\u91cf\u5e76\u63d0\u5347\u4e86\u8ba1\u7b97\u901f\u5ea6\uff0c\u9002\u7528\u4e8e\u5927\u89c4\u6a21\u6570\u636e\u6d41\u7684\u5b9e\u65f6SVD\u8ba1\u7b97\u3002", "motivation": "\u968f\u7740\u77e9\u9635\u7ef4\u5ea6\u5feb\u901f\u589e\u957f\uff0c\u4f20\u7edfSVD\u8ba1\u7b97\u6210\u672c\u9ad8\u6602\uff0c\u73b0\u6709\u786c\u4ef6\u52a0\u901f\u65b9\u6848\u5b58\u5728\u53ef\u6269\u5c55\u6027\u5dee\u3001\u7247\u4e0a\u5185\u5b58\u6d88\u8017\u9ad8\u4ee5\u53ca\u96be\u4ee5\u5e94\u5bf9\u5b9e\u65f6\u5927\u89c4\u6a21\u6570\u636e\u6d41\u5904\u7406\u7b49\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u540d\u4e3aDSB Jacobi\u7684\u6570\u636e\u6d41\u5f0fSVD\u5904\u7406\u7b97\u6cd5\uff0c\u4f18\u5316\u7247\u4e0a\u5185\u5b58\u4f7f\u7528\u5e76\u63d0\u5347\u8ba1\u7b97\u6548\u7387\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u4e0e\u5148\u524d\u5de5\u4f5c\u76f8\u6bd4\uff0c\u8be5\u65b9\u6cd5\u51cf\u5c11\u4e8641.5%\u7684\u7247\u4e0aRAM\u6d88\u8017\uff0c\u5e76\u5c06\u8ba1\u7b97\u6548\u7387\u63d0\u9ad8\u4e8623\u500d\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684DSB Jacobi\u7b97\u6cd5\u4e3a\u5d4c\u5165\u5f0f\u7cfb\u7edf\u4e2d\u5927\u89c4\u6a21\u6570\u636e\u6d41\u7684\u5b9e\u65f6SVD\u8ba1\u7b97\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u5b9e\u7528\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2511.12486", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2511.12486", "abs": "https://arxiv.org/abs/2511.12486", "authors": ["Duneesha Fernando", "Maria A. Rodriguez", "Rajkumar Buyya"], "title": "A Decentralized Root Cause Localization Approach for Edge Computing Environments", "comment": null, "summary": "Edge computing environments host increasingly complex microservice-based IoT applications, which are prone to performance anomalies that can propagate across dependent services. Identifying the true source of such anomalies, known as Root Cause Localization (RCL), is essential for timely mitigation. However, existing RCL approaches are designed for cloud environments and rely on centralized analysis, which increases latency and communication overhead when applied at the edge. This paper proposes a decentralized RCL approach that executes localization directly at the edge device level using the Personalized PageRank (PPR) algorithm. The proposed method first groups microservices into communication- and colocation-aware clusters, thereby confining most anomaly propagation within cluster boundaries. Within each cluster, PPR is executed locally to identify the root cause, significantly reducing localization time. For the rare cases where anomalies propagate across clusters, we introduce an inter-cluster peer-to-peer approximation process, enabling lightweight coordination among clusters with minimal communication overhead. To enhance the accuracy of localization in heterogeneous edge environments, we also propose a novel anomaly scoring mechanism tailored to the diverse anomaly triggers that arise across microservice, device, and network layers. Evaluation results on the publicly available edge dataset, MicroCERCL, demonstrate that the proposed decentralized approach achieves comparable or higher localization accuracy than its centralized counterpart while reducing localization time by up to 34%. These findings highlight that decentralized graph-based RCL can provide a practical and efficient solution for anomaly diagnosis in resource-constrained edge environments.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9762\u5411\u8fb9\u7f18\u8ba1\u7b97\u73af\u5883\u7684\u53bb\u4e2d\u5fc3\u5316\u6839\u56e0\u5b9a\u4f4d\uff08RCL\uff09\u65b9\u6cd5\uff0c\u5229\u7528\u4e2a\u6027\u5316PageRank\uff08PPR\uff09\u7b97\u6cd5\u5728\u672c\u5730\u5fae\u670d\u52a1\u96c6\u7fa4\u5185\u8fdb\u884c\u9ad8\u6548\u6839\u56e0\u8bc6\u522b\uff0c\u5e76\u901a\u8fc7\u8f7b\u91cf\u7ea7\u8de8\u96c6\u7fa4\u534f\u8c03\u673a\u5236\u5904\u7406\u8de8\u96c6\u7fa4\u5f02\u5e38\u4f20\u64ad\uff0c\u663e\u8457\u964d\u4f4e\u5b9a\u4f4d\u5ef6\u8fdf\uff0c\u540c\u65f6\u4fdd\u6301\u751a\u81f3\u63d0\u5347\u5b9a\u4f4d\u51c6\u786e\u6027\u3002", "motivation": "\u73b0\u6709RCL\u65b9\u6cd5\u4e3b\u8981\u9488\u5bf9\u4e91\u73af\u5883\u8bbe\u8ba1\uff0c\u4f9d\u8d56\u96c6\u4e2d\u5f0f\u5206\u6790\uff0c\u5728\u8fb9\u7f18\u8ba1\u7b97\u573a\u666f\u4e0b\u4f1a\u5e26\u6765\u9ad8\u5ef6\u8fdf\u548c\u901a\u4fe1\u5f00\u9500\uff1b\u800c\u8fb9\u7f18\u73af\u5883\u4e2d\u5fae\u670d\u52a1\u5e94\u7528\u590d\u6742\u3001\u8d44\u6e90\u53d7\u9650\uff0c\u4e9f\u9700\u4f4e\u5ef6\u8fdf\u3001\u4f4e\u5f00\u9500\u7684\u53bb\u4e2d\u5fc3\u5316RCL\u65b9\u6848\u3002", "method": "\u5c06\u5fae\u670d\u52a1\u6309\u901a\u4fe1\u4e0e\u5171\u7f6e\u5173\u7cfb\u805a\u7c7b\uff0c\u5728\u6bcf\u4e2a\u96c6\u7fa4\u5185\u672c\u5730\u8fd0\u884cPPR\u7b97\u6cd5\u8fdb\u884c\u6839\u56e0\u5b9a\u4f4d\uff1b\u5bf9\u8de8\u96c6\u7fa4\u5f02\u5e38\u4f20\u64ad\uff0c\u91c7\u7528\u70b9\u5bf9\u70b9\u8fd1\u4f3c\u534f\u8c03\u673a\u5236\uff1b\u540c\u65f6\u5f15\u5165\u9488\u5bf9\u8fb9\u7f18\u5f02\u6784\u73af\u5883\u7684\u65b0\u578b\u5f02\u5e38\u8bc4\u5206\u673a\u5236\u4ee5\u63d0\u5347\u5b9a\u4f4d\u51c6\u786e\u6027\u3002", "result": "\u5728MicroCERCL\u516c\u5f00\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u6240\u63d0\u65b9\u6cd5\u76f8\u6bd4\u96c6\u4e2d\u5f0f\u65b9\u6cd5\u53ef\u51cf\u5c11\u6700\u591a34%\u7684\u5b9a\u4f4d\u65f6\u95f4\uff0c\u540c\u65f6\u8fbe\u5230\u76f8\u5f53\u6216\u66f4\u9ad8\u7684\u5b9a\u4f4d\u51c6\u786e\u7387\u3002", "conclusion": "\u53bb\u4e2d\u5fc3\u5316\u7684\u56fe\u9a71\u52a8RCL\u65b9\u6cd5\u80fd\u6709\u6548\u5e94\u5bf9\u8fb9\u7f18\u73af\u5883\u4e2d\u8d44\u6e90\u53d7\u9650\u548c\u4f4e\u5ef6\u8fdf\u9700\u6c42\u7684\u6311\u6218\uff0c\u4e3a\u5fae\u670d\u52a1\u5f02\u5e38\u8bca\u65ad\u63d0\u4f9b\u5b9e\u7528\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2511.12500", "categories": ["cs.DC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.12500", "abs": "https://arxiv.org/abs/2511.12500", "authors": ["Muhammad Awad", "Muhammad Osama", "Brandon Potter"], "title": "Iris: First-Class Multi-GPU Programming Experience in Triton", "comment": null, "summary": "Multi-GPU programming traditionally requires developers to navigate complex trade-offs between performance and programmability. High-performance implementations typically rely on low-level HIP/CUDA communication libraries that demand substantial engineering effort for even basic overlap patterns, while simpler abstractions often sacrifice performance. We present Iris, a multi-GPU communication library implemented entirely in Python and Triton that eliminates this trade-off. Iris provides tile-based symmetric memory abstractions that naturally align with Triton's programming model, enabling developers to write single-source kernels that seamlessly interleave computation and communication. We demonstrate a taxonomy of compute-communication overlap patterns--from bulk-synchronous to fine-grained workgroup specialization--that can be implemented with minimal code changes in Iris, often requiring just a few additional lines within the same Triton kernel. Our evaluation shows that Iris achieves near-optimal bandwidth utilization in microbenchmarks and delivers up to 1.79x speedup over PyTorch and RCCL for GEMM+All-Scatter workloads, demonstrating that high-level implementations can match or exceed heavily-optimized libraries while dramatically simplifying multi-GPU programming.", "AI": {"tldr": "Iris \u662f\u4e00\u4e2a\u57fa\u4e8e Python \u548c Triton \u5b9e\u73b0\u7684\u591a GPU \u901a\u4fe1\u5e93\uff0c\u901a\u8fc7\u63d0\u4f9b\u4e0e Triton \u7f16\u7a0b\u6a21\u578b\u5bf9\u9f50\u7684 tile-based \u5bf9\u79f0\u5185\u5b58\u62bd\u8c61\uff0c\u4f7f\u5f00\u53d1\u8005\u80fd\u7528\u5355\u6e90\u4ee3\u7801\u5b9e\u73b0\u8ba1\u7b97\u4e0e\u901a\u4fe1\u7684\u9ad8\u6548\u91cd\u53e0\uff0c\u5728\u7b80\u5316\u7f16\u7a0b\u7684\u540c\u65f6\u8fbe\u5230\u751a\u81f3\u8d85\u8d8a\u73b0\u6709\u4f18\u5316\u5e93\uff08\u5982 PyTorch \u548c RCCL\uff09\u7684\u6027\u80fd\u3002", "motivation": "\u4f20\u7edf\u591a GPU \u7f16\u7a0b\u5728\u6027\u80fd\u4e0e\u53ef\u7f16\u7a0b\u6027\u4e4b\u95f4\u5b58\u5728\u6743\u8861\uff1a\u4f4e\u5c42 HIP/CUDA \u5e93\u867d\u9ad8\u6027\u80fd\u4f46\u5f00\u53d1\u590d\u6742\uff0c\u9ad8\u5c42\u62bd\u8c61\u5219\u5e38\u727a\u7272\u6027\u80fd\u3002\u4f5c\u8005\u65e8\u5728\u6d88\u9664\u8fd9\u4e00\u6743\u8861\u3002", "method": "\u63d0\u51fa Iris \u5e93\uff0c\u5b8c\u5168\u7528 Python \u548c Triton \u5b9e\u73b0\uff0c\u91c7\u7528 tile-based \u5bf9\u79f0\u5185\u5b58\u62bd\u8c61\uff0c\u652f\u6301\u5728\u5355\u4e00 Triton kernel \u4e2d\u65e0\u7f1d\u4ea4\u7ec7\u8ba1\u7b97\u4e0e\u901a\u4fe1\uff0c\u5e76\u6db5\u76d6\u4ece\u6279\u91cf\u540c\u6b65\u5230\u7ec6\u7c92\u5ea6\u5de5\u4f5c\u7ec4\u7279\u5316\u7684\u591a\u79cd\u91cd\u53e0\u6a21\u5f0f\u3002", "result": "\u5fae\u57fa\u51c6\u6d4b\u8bd5\u4e2d Iris \u5b9e\u73b0\u63a5\u8fd1\u6700\u4f18\u5e26\u5bbd\u5229\u7528\u7387\uff1b\u5728 GEMM+All-Scatter \u5de5\u4f5c\u8d1f\u8f7d\u4e0a\uff0c\u76f8\u6bd4 PyTorch \u548c RCCL \u6700\u9ad8\u63d0\u901f\u8fbe 1.79 \u500d\u3002", "conclusion": "Iris \u8bc1\u660e\u4e86\u9ad8\u5c42\u62bd\u8c61\u4e5f\u80fd\u5b9e\u73b0\u9ad8\u6027\u80fd\uff0c\u540c\u65f6\u5927\u5e45\u7b80\u5316\u591a GPU \u7f16\u7a0b\uff0c\u6709\u6548\u5f25\u5408\u4e86\u6027\u80fd\u4e0e\u53ef\u7f16\u7a0b\u6027\u4e4b\u95f4\u7684\u9e3f\u6c9f\u3002"}}
{"id": "2511.12667", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2511.12667", "abs": "https://arxiv.org/abs/2511.12667", "authors": ["Sepideh Masoudi", "Mark Edward Michael Daly", "Jannis Kiesel"], "title": "Artifact for A Non-Intrusive Framework for Deferred Integration of Cloud Patterns in Energy-Efficient Data-Sharing Pipelines", "comment": null, "summary": "As data mesh architectures grow, organizations increasingly build consumer-specific data-sharing pipelines from modular, cloud-based transformation services. While reusable transformation services can improve cost and energy efficiency, applying traditional cloud design patterns can reduce reusability of services in different pipelines. We present a Kubernetes-based tool that enables non-intrusive, deferred application of design patterns without modifying services code. The tool automates pattern injection and collects energy metrics, supporting energy-aware decisions while preserving reusability of transformation services in various pipeline structures.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e Kubernetes \u7684\u5de5\u5177\uff0c\u53ef\u5728\u4e0d\u4fee\u6539\u670d\u52a1\u4ee3\u7801\u7684\u524d\u63d0\u4e0b\u975e\u4fb5\u5165\u5f0f\u5730\u5ef6\u8fdf\u5e94\u7528\u4e91\u8bbe\u8ba1\u6a21\u5f0f\uff0c\u4ece\u800c\u5728\u4fdd\u6301\u6570\u636e\u8f6c\u6362\u670d\u52a1\u53ef\u91cd\u7528\u6027\u7684\u540c\u65f6\u652f\u6301\u80fd\u8017\u611f\u77e5\u51b3\u7b56\u3002", "motivation": "\u968f\u7740\u6570\u636e\u7f51\u683c\u67b6\u6784\u7684\u53d1\u5c55\uff0c\u7ec4\u7ec7\u5e7f\u6cdb\u4f7f\u7528\u6a21\u5757\u5316\u3001\u57fa\u4e8e\u4e91\u7684\u8f6c\u6362\u670d\u52a1\u6784\u5efa\u9762\u5411\u6d88\u8d39\u8005\u7684\u5171\u4eab\u7ba1\u9053\uff1b\u7136\u800c\uff0c\u4f20\u7edf\u4e91\u8bbe\u8ba1\u6a21\u5f0f\u7684\u5e94\u7528\u4f1a\u964d\u4f4e\u8fd9\u4e9b\u670d\u52a1\u5728\u4e0d\u540c\u7ba1\u9053\u4e2d\u7684\u53ef\u91cd\u7528\u6027\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u57fa\u4e8e Kubernetes \u7684\u5de5\u5177\uff0c\u5b9e\u73b0\u8bbe\u8ba1\u6a21\u5f0f\u7684\u975e\u4fb5\u5165\u5f0f\u3001\u5ef6\u8fdf\u6ce8\u5165\uff0c\u5e76\u81ea\u52a8\u6536\u96c6\u80fd\u8017\u6307\u6807\u3002", "result": "\u8be5\u5de5\u5177\u5728\u4e0d\u4fee\u6539\u670d\u52a1\u4ee3\u7801\u7684\u60c5\u51b5\u4e0b\u63d0\u5347\u4e86\u670d\u52a1\u5728\u591a\u79cd\u7ba1\u9053\u7ed3\u6784\u4e2d\u7684\u53ef\u91cd\u7528\u6027\uff0c\u5e76\u652f\u6301\u57fa\u4e8e\u80fd\u8017\u7684\u4f18\u5316\u51b3\u7b56\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u5de5\u5177\u6709\u6548\u5e73\u8861\u4e86\u4e91\u8bbe\u8ba1\u6a21\u5f0f\u5e94\u7528\u4e0e\u670d\u52a1\u53ef\u91cd\u7528\u6027\u4e4b\u95f4\u7684\u77db\u76fe\uff0c\u540c\u65f6\u4e3a\u6784\u5efa\u80fd\u6548\u654f\u611f\u7684\u6570\u636e\u7ba1\u9053\u63d0\u4f9b\u4e86\u652f\u6301\u3002"}}
{"id": "2511.12687", "categories": ["cs.DC", "math.PR"], "pdf": "https://arxiv.org/pdf/2511.12687", "abs": "https://arxiv.org/abs/2511.12687", "authors": ["Partha S. Dey", "Aditya S. Gopalan", "Vijay G. Subramanian"], "title": "The Time to Consensus in a Blockchain: Insights into Bitcoin's \"6 Blocks Rule''", "comment": null, "summary": "We investigate the time to consensus in Nakamoto blockchains. Specifically, we consider two competing growth processes, labeled \\emph{honest} and \\emph{adversarial}, and determine the time after which the honest process permananetly exceeds the adversarial process. This is done via queueing techniques. The predominant difficulty is that the honest growth process is subject to \\emph{random delays}. In a stylized Bitcoin model, we compute the Laplace transform for the time to consensus and verify it via simulation.", "AI": {"tldr": "\u672c\u6587\u5229\u7528\u6392\u961f\u8bba\u6280\u672f\u7814\u7a76\u4e86Nakamoto\u533a\u5757\u94fe\u4e2d\u8fbe\u6210\u5171\u8bc6\u6240\u9700\u7684\u65f6\u95f4\uff0c\u91cd\u70b9\u5206\u6790\u8bda\u5b9e\u4e0e\u5bf9\u6297\u4e24\u79cd\u589e\u957f\u8fc7\u7a0b\uff0c\u5e76\u5728\u8003\u8651\u8bda\u5b9e\u8fc7\u7a0b\u5b58\u5728\u968f\u673a\u5ef6\u8fdf\u7684\u60c5\u51b5\u4e0b\uff0c\u8ba1\u7b97\u5e76\u9a8c\u8bc1\u4e86\u5171\u8bc6\u65f6\u95f4\u7684\u62c9\u666e\u62c9\u65af\u53d8\u6362\u3002", "motivation": "\u5728Nakamoto\u533a\u5757\u94fe\u4e2d\uff0c\u7406\u89e3\u8bda\u5b9e\u8282\u70b9\u4e0e\u5bf9\u6297\u8282\u70b9\u4e4b\u95f4\u7684\u7ade\u4e89\u52a8\u6001\u5bf9\u4e8e\u8bc4\u4f30\u7cfb\u7edf\u5b89\u5168\u6027\u81f3\u5173\u91cd\u8981\u3002\u73b0\u6709\u7814\u7a76\u7f3a\u4e4f\u5bf9\u8bda\u5b9e\u589e\u957f\u8fc7\u7a0b\u4e2d\u968f\u673a\u5ef6\u8fdf\u5f71\u54cd\u7684\u7cbe\u786e\u5efa\u6a21\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u65b0\u65b9\u6cd5\u6765\u91cf\u5316\u8fbe\u6210\u5171\u8bc6\u6240\u9700\u7684\u65f6\u95f4\u3002", "method": "\u91c7\u7528\u6392\u961f\u8bba\u65b9\u6cd5\u5bf9\u8bda\u5b9e\u548c\u5bf9\u6297\u4e24\u79cd\u589e\u957f\u8fc7\u7a0b\u8fdb\u884c\u5efa\u6a21\uff0c\u5e76\u5728\u7b80\u5316\u7684\u6bd4\u7279\u5e01\u6a21\u578b\u4e2d\u63a8\u5bfc\u5171\u8bc6\u65f6\u95f4\u7684\u62c9\u666e\u62c9\u65af\u53d8\u6362\uff0c\u540c\u65f6\u901a\u8fc7\u4eff\u771f\u9a8c\u8bc1\u7406\u8bba\u7ed3\u679c\u3002", "result": "\u6210\u529f\u8ba1\u7b97\u51fa\u8003\u8651\u968f\u673a\u5ef6\u8fdf\u4e0b\u5171\u8bc6\u65f6\u95f4\u7684\u62c9\u666e\u62c9\u65af\u53d8\u6362\uff0c\u5e76\u901a\u8fc7\u4eff\u771f\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u8be5\u89e3\u6790\u7ed3\u679c\u7684\u51c6\u786e\u6027\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u7406\u89e3Nakamoto\u5171\u8bc6\u673a\u5236\u4e2d\u968f\u673a\u5ef6\u8fdf\u5bf9\u5b89\u5168\u6027\u7684\u5f71\u54cd\u63d0\u4f9b\u4e86\u7406\u8bba\u57fa\u7840\uff0c\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u53ef\u7528\u4e8e\u8fdb\u4e00\u6b65\u5206\u6790\u533a\u5757\u94fe\u7cfb\u7edf\u7684\u7a33\u5065\u6027\u3002"}}
{"id": "2511.13155", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2511.13155", "abs": "https://arxiv.org/abs/2511.13155", "authors": ["Jonathan Bader", "Julius Irion", "Jannis Kappel", "Joel Witzke", "Niklas Fomin", "Diellza Sherifi", "Odej Kao"], "title": "Learning Process Energy Profiles from Node-Level Power Data", "comment": null, "summary": "The growing demand for data center capacity, driven by the growth of high-performance computing, cloud computing, and especially artificial intelligence, has led to a sharp increase in data center energy consumption. To improve energy efficiency, gaining process-level insights into energy consumption is essential. While node-level energy consumption data can be directly measured with hardware such as power meters, existing mechanisms for estimating per-process energy usage, such as Intel RAPL, are limited to specific hardware and provide only coarse-grained, domain-level measurements. Our proposed approach models per-process energy profiles by leveraging fine-grained process-level resource metrics collected via eBPF and perf, which are synchronized with node-level energy measurements obtained from an attached power distribution unit. By statistically learning the relationship between process-level resource usage and node-level energy consumption through a regression-based model, our approach enables more fine-grained per-process energy predictions.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8eeBPF\u548cperf\u6536\u96c6\u7684\u7ec6\u7c92\u5ea6\u8fdb\u7a0b\u8d44\u6e90\u6307\u6807\uff0c\u7ed3\u5408\u8282\u70b9\u7ea7\u80fd\u8017\u6d4b\u91cf\uff0c\u901a\u8fc7\u56de\u5f52\u6a21\u578b\u5b9e\u73b0\u66f4\u7cbe\u786e\u7684\u8fdb\u7a0b\u7ea7\u80fd\u8017\u9884\u6d4b\u3002", "motivation": "\u968f\u7740\u9ad8\u6027\u80fd\u8ba1\u7b97\u3001\u4e91\u8ba1\u7b97\u548c\u4eba\u5de5\u667a\u80fd\u7684\u53d1\u5c55\uff0c\u6570\u636e\u4e2d\u5fc3\u80fd\u8017\u6025\u5267\u4e0a\u5347\uff0c\u4e9f\u9700\u5728\u8fdb\u7a0b\u7ea7\u522b\u6df1\u5165\u7406\u89e3\u80fd\u8017\u60c5\u51b5\u4ee5\u63d0\u5347\u80fd\u6548\u3002\u73b0\u6709\u65b9\u6cd5\uff08\u5982Intel RAPL\uff09\u53d7\u9650\u4e8e\u7279\u5b9a\u786c\u4ef6\u4e14\u4ec5\u63d0\u4f9b\u7c97\u7c92\u5ea6\u7684\u80fd\u8017\u4f30\u7b97\u3002", "method": "\u5229\u7528eBPF\u548cperf\u91c7\u96c6\u7ec6\u7c92\u5ea6\u7684\u8fdb\u7a0b\u7ea7\u8d44\u6e90\u4f7f\u7528\u6570\u636e\uff0c\u5e76\u4e0e\u901a\u8fc7\u7535\u6e90\u5206\u914d\u5355\u5143\u83b7\u53d6\u7684\u8282\u70b9\u7ea7\u80fd\u8017\u6570\u636e\u540c\u6b65\uff0c\u901a\u8fc7\u56de\u5f52\u6a21\u578b\u5b66\u4e60\u8fdb\u7a0b\u8d44\u6e90\u4f7f\u7528\u4e0e\u6574\u4f53\u80fd\u8017\u4e4b\u95f4\u7684\u5173\u7cfb\uff0c\u4ece\u800c\u9884\u6d4b\u6bcf\u4e2a\u8fdb\u7a0b\u7684\u80fd\u8017\u3002", "result": "\u8be5\u65b9\u6cd5\u80fd\u591f\u5b9e\u73b0\u6bd4\u73b0\u6709\u673a\u5236\u66f4\u7ec6\u7c92\u5ea6\u3001\u66f4\u901a\u7528\u7684\u8fdb\u7a0b\u7ea7\u80fd\u8017\u9884\u6d4b\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u57fa\u4e8e\u7edf\u8ba1\u5b66\u4e60\u548c\u7ec6\u7c92\u5ea6\u76d1\u63a7\u7684\u65b9\u6cd5\u6709\u6548\u63d0\u5347\u4e86\u8fdb\u7a0b\u7ea7\u80fd\u8017\u5efa\u6a21\u7684\u7cbe\u5ea6\u548c\u9002\u7528\u6027\uff0c\u6709\u52a9\u4e8e\u6570\u636e\u4e2d\u5fc3\u80fd\u6548\u4f18\u5316\u3002"}}
