<div id=toc></div>

# Table of Contents

- [cs.MA](#cs.MA) [Total: 1]
- [cs.SE](#cs.SE) [Total: 38]
- [cs.AR](#cs.AR) [Total: 4]
- [cs.DC](#cs.DC) [Total: 12]
- [cs.NI](#cs.NI) [Total: 11]


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [1] [An Agent-Based Simulation of Ageing Societies: Accessibility and Care Dynamics in Remote Areas](https://arxiv.org/abs/2509.26496)
*Roberto garrone*

Main category: cs.MA

TL;DR: 本文通过基于代理的模拟，研究意大利偏远老龄化社区Premeno在医疗设施搬迁前后老年人及其照护者的可达性与照护动态变化，发现搬迁虽提升局部步行性，却因绕行和距离增加导致未满足照护需求上升，且家庭收入是照护负担的主要驱动因素。


<details>
  <summary>Details</summary>
Motivation: 老龄化社会中，偏远地区医疗与照护资源的可达性面临挑战，需理解服务布局变化对老年人及其照护者的影响，以制定因地制宜的干预策略。

Method: 构建基于代理的模型，整合人口普查、市政数据、无人机高程模型、GIS路网和问卷照护信息，生成具有社会经济与移动属性的老年-照护者配对群体，模拟微尺度可达性与中尺度照护结果，并比较基准与医疗设施搬迁两种情景。

Result: 医疗设施搬迁虽局部改善步行性，但导致未满足照护时长增加；家庭收入是照护负担的主因，可达性受经济与移动资源交互影响。

Conclusion: 在偏远老龄化社区，干预措施需考虑本地具体约束，平衡服务可达性与照护负担，避免单纯优化空间布局而忽视社会经济因素。

Abstract: This paper presents an agent-based simulation of accessibility and care
dynamics in ageing societies, applied to the Italian inner area of Premeno
(VB). The model integrates census and municipal data, drone-derived elevation
models, GIS road networks, and survey-based caregiving information to generate
synthetic populations of older adults and their caregivers. Agents are
organized into dyads with socio-economic and mobility attributes, enabling the
simulation of both micro-scale accessibility and meso-scale caregiving
outcomes. Two scenarios are compared: a baseline and an alternative involving
the relocation of healthcare services. Key indicators include caregiver effort,
overwhelmed caregivers, walkability, and unmet hours of care. Findings show
that while relocation improves walkability locally, it increases unmet care
hours due to detours and reduced proximity. Household income emerges as the
primary driver of caregiver burden, with accessibility shaped by interactions
between financial and mobility resources. Results highlight the need for
interventions tailored to context-specific constraints in remote ageing
communities.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [2] [WARP -- Web-Augmented Real-time Program Repairer: A Real-Time Compilation Error Resolution using LLMs and Web-Augmented Synthesis](https://arxiv.org/abs/2509.25192)
*Anderson de Lima Luiz*

Main category: cs.SE

TL;DR: WARP 是一个结合大语言模型与实时网络增强合成技术的系统，用于自动修复编译错误，在 CGP 基准测试中表现出优于传统方法的修复率和语义正确性。


<details>
  <summary>Details</summary>
Motivation: 编译错误严重影响软件开发效率，现有 IDE 快速修复和纯 LLM 方法在准确性和语义正确性方面存在不足。

Method: WARP 系统通过监控开发者终端，检测编译错误，并结合微调后的 Code-LLM 与从开发者论坛、官方文档等网络来源检索到的最新解决方案、解释和代码片段，进行动态合成修复。

Result: 在包含 C/C++、Python 和 Go 的 CGP 基准测试中，WARP 实现了 72.5% 的正确编译修复率，并在语义正确性上优于基线方法。

Conclusion: 结合大语言模型与实时网络增强信息能有效提升编译错误自动修复的性能，但需应对从噪声网络数据中高精度合成代码的技术挑战。

Abstract: Compilation errors represent a significant bottleneck in software development
productivity. This paper introduces WARP (Web-Augmented Real-time Program
Repairer), a novel system that leverages Large Language Models (LLMs) and
dynamic web-augmented synthesis for real-time resolution of these errors. WARP
actively monitors developer terminals, intelligently detects compilation
errors, and synergistically combines the understanding of a fine-tuned Code-LLM
with relevant solutions, explanations, and code snippets retrieved from
up-to-date web sources like developer forums and official documentation.
Experimental results on our curated benchmark, CGP (featuring C/C++, Python,
and Go errors), demonstrate WARP achieves a superior fix rate (72.5 % Compiles
correctly) and higher semantic correctness compared to baseline LLM-only
approaches and traditional IDE quick-fixes. Key technical challenges in
achieving high-accuracy synthesis from noisy web data.

</details>


### [3] [Devstral: Fine-tuning Language Models for Coding Agent Applications](https://arxiv.org/abs/2509.25193)
*Abhinav Rastogi,Adam Yang,Albert Q. Jiang,Alexander H. Liu,Alexandre Sablayrolles,Amélie Héliou,Amélie Martin,Anmol Agarwal,Andy Ehrenberg,Andy Lo,Antoine Roux,Arthur Darcet,Arthur Mensch,Baptiste Bout,Baptiste Rozière,Baudouin De Monicault,Chris Bamford,Christian Wallenwein,Christophe Renaudin,Clémence Lanfranchi,Clément Denoix,Corentin Barreau,Darius Dabert Devon Mizelle,Diego de las Casas,Elliot Chane-Sane,Emilien Fugier,Emma Bou Hanna,Gabrielle Berrada,Gauthier Delerce,Gauthier Guinet,Georgii Novikov,Graham Neubig,Guillaume Lample,Guillaume Martin,Himanshu Jaju,Jan Ludziejewski,Jason Rute,Jean-Malo Delignon,JeanHadrien Chabran,Joachim Studnia,Joep Barmentlo,Jonas Amar,Josselin Somerville Roberts,Julien Denize,Karan Saxena,Karmesh Yadav,Kartik Khandelwal,Khyathi Raghavi Chandu,Kush Jain,Lélio Renard Lavaud,Léonard Blier,Lingxiao Zhao,Louis Martin,Lucile Saulnier,Luyu Gao,Marie Pellat,Mathilde Guillaumin,Mathis Felardos,Matthieu Dinot,Maxime Darrin,Maximilian Augustin,Mickaël Seznec,Neha Gupta,Nikhil Raghuraman,Olivier Duchenne,Patricia Wang,Patrick von Platen,Patryk Saffer,Paul Jacob,Paul Wambergue,Paula Kurylowicz,Philomène Chagniot,Pierre Stock,Pravesh Agrawal,Rémi Delacourt,Roman Soletskyi,Romain Sauvestre,Sagar Vaze,Sanchit Gandhi,Sandeep Subramanian,Shashwat Dalal,Siddharth Gandhi,Soham Ghosh,Srijan Mishra,Sumukh Aithal,Szymon Antoniak,Teven Le Scao,Thibaut Lavril,Thibault Schueller,Thomas Foubert,Thomas Robert,Thomas Wang,Timothée Lacroix,Tom Bewley,Valeriia Nemychnikova,Victor Paltz,Virgile Richard,Wen-Ding Li,William Marshall,Xingyao Wang,Xuanyu Zhang,Yihan Wan,Yunhao Tang*

Main category: cs.SE

TL;DR: 提出了 Devstral-Small，一个24B参数的轻量级开源代码智能体模型，在小于100B的模型中性能最佳，兼具高效部署与强大性能。


<details>
  <summary>Details</summary>
Motivation: 开发一个轻量、高效且性能优越的开源代码智能体模型，以满足在资源受限环境下对高性能代码生成模型的需求。

Method: 设计并开发了一个24B参数的小型模型 Devstral-Small，并在智能体软件开发任务中进行了专门优化。

Result: Devstral-Small 在小于100B参数的模型中实现了最佳性能，且性能可与大一个数量级以上的大模型相媲美。

Conclusion: Devstral-Small 证明了小型模型通过针对性设计和优化，也能在代码智能体任务中实现与大型模型相当的竞争力，同时具备部署便捷和推理高效的优势。

Abstract: We introduce Devstral-Small, a lightweight open source model for code agents
with the best performance among models below 100B size. In this technical
report, we give an overview of how we design and develop a model and craft
specializations in agentic software development. The resulting model,
Devstral-Small is a small 24B model, fast and easy to serve. Despite its size,
Devstral-Small still attains competitive performance compared to models more
than an order of magnitude larger.

</details>


### [4] [Automated Code Development for PDE Solvers Using Large Language Models](https://arxiv.org/abs/2509.25194)
*Haoyang Wu,Xinxin Zhang,Lailai Zhu*

Main category: cs.SE

TL;DR: 本文提出LLM-PDEveloper，一个零样本、多智能体的大语言模型框架，用于自动化生成和修改偏微分方程（PDE）数值求解库的代码，实现从数学描述到源代码的端到端转换，支持库的自我增强。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注为终端用户自动化PDE求解的案例设置与执行，缺乏面向二次开发者的代码开发自动化工具；而大语言模型具备跨领域知识和推理能力，有望用于PDE数值库的开发。

Method: 提出LLM-PDEveloper框架，利用多智能体大语言模型，将数学和算法描述直接翻译为源代码，实现新求解器/模块的生成及现有模块的修改，构建端到端的“数学到代码”自增强流水线。

Result: 在三项任务上验证了该框架：1）为新PDE构建求解器，2）为已有PDE实现新边界条件，3）修改现有求解器以加入新项，取得了中等成功率；同时分析了LLM产生的语法和语义错误，并提出有效修复策略。

Conclusion: LLM-PDEveloper展示了大语言模型在科学计算库开发中的潜力，为PDE数值库的持续扩展和自动化开发提供了可行路径，并为未来改进LLM在代码生成中的语义准确性指明方向。

Abstract: Foundation models -- large language models (LLMs) in particular -- have
become ubiquitous, shaping daily life and driving breakthroughs across science,
engineering, and technology. Harnessing their broad cross-domain knowledge,
text-processing, and reasoning abilities for software development, e.g.,
numerical libraries for solving partial differential equations (PDEs), is
therefore attracting growing interest. Yet existing studies mainly automate
case setup and execution for end users. We introduce LLM-PDEveloper, a
zero-shot, multi-agent LLM framework that automates code development for PDE
libraries, specifically targeting secondary developers. By translating
mathematical and algorithmic descriptions directly into source code,
LLM-PDEveloper generates new solvers/modules and adapts existing ones. This
end-to-end math-to-code approach enables a self-augmenting pipeline that
continuously expands the codebase of a library, extends its capacities, and
broadens its scope. We demonstrate LLM-PDEveloper on three tasks: 1) build a
solver for a new PDE, 2) implement new BCs for a given PDE, and 3) modify an
existing solver to incorporate additional terms, achieving moderate success
rates. Failures due to syntactic errors made by LLMs are analyzed and we
propose effective fixes. We also identify the mechanisms underlying certain
semantic errors, guiding future research.

</details>


### [5] [Understanding Practitioners Perspectives on Monitoring Machine Learning Systems](https://arxiv.org/abs/2509.25195)
*Hira Naveed,John Grundy,Chetan Arora,Hourieh Khalajzadeh,Omar Haggag*

Main category: cs.SE

TL;DR: 本文通过一项涵盖91名机器学习从业者的全球调查，研究了ML系统监控的现状、挑战与改进方向，发现从业者常面临模型性能下降、延迟超标和安全违规等问题，并呼吁更自动化、易用且支持公平性监控的工具。


<details>
  <summary>Details</summary>
Motivation: 由于机器学习系统具有内在的不确定性，在生产环境中可能导致意外甚至危险后果，因此亟需有效监控以及时发现异常行为，避免组织遭受财务和声誉损失。

Method: 作者通过全球问卷调查收集了91位ML从业者的定性与定量数据，分析当前ML系统监控实践中常见的运行时问题、工业界做法、主要挑战及对未来监控工具的期望。

Result: 调查结果显示，从业者普遍遭遇模型性能下降、延迟过高和安全违规等运行时问题；尽管偏好自动化监控，但因复杂性或缺乏合适工具仍依赖手动方法；监控工具的初始配置困难、增加工作负担、引发告警疲劳；从业者希望实现监控的自动部署、增强对性能与公平性的支持，并提供问题修复建议。

Conclusion: 研究为未来ML监控工具的开发提供了实践导向的指导，强调需提升自动化程度、简化配置流程、加强性能与公平性监控，并减少运维负担，以更好满足从业者实际需求。

Abstract: Given the inherent non-deterministic nature of machine learning (ML) systems,
their behavior in production environments can lead to unforeseen and
potentially dangerous outcomes. For a timely detection of unwanted behavior and
to prevent organizations from financial and reputational damage, monitoring
these systems is essential. This paper explores the strategies, challenges, and
improvement opportunities for monitoring ML systems from the practitioners
perspective. We conducted a global survey of 91 ML practitioners to collect
diverse insights into current monitoring practices for ML systems. We aim to
complement existing research through our qualitative and quantitative analyses,
focusing on prevalent runtime issues, industrial monitoring and mitigation
practices, key challenges, and desired enhancements in future monitoring tools.
Our findings reveal that practitioners frequently struggle with runtime issues
related to declining model performance, exceeding latency, and security
violations. While most prefer automated monitoring for its increased
efficiency, many still rely on manual approaches due to the complexity or lack
of appropriate automation solutions. Practitioners report that the initial
setup and configuration of monitoring tools is often complicated and
challenging, particularly when integrating with ML systems and setting alert
thresholds. Moreover, practitioners find that monitoring adds extra workload,
strains resources, and causes alert fatigue. The desired improvements from the
practitioners perspective are: automated generation and deployment of monitors,
improved support for performance and fairness monitoring, and recommendations
for resolving runtime issues. These insights offer valuable guidance for the
future development of ML monitoring tools that are better aligned with
practitioners needs.

</details>


### [6] [APRIL: API Synthesis with Automatic Prompt Optimization and Reinforcement Learning](https://arxiv.org/abs/2509.25196)
*Hua Zhong,Shan Jiang,Sarfraz Khurshid*

Main category: cs.SE

TL;DR: 本文提出APRIL方法，结合大语言模型（LLM）、自动提示优化（APO）和基于可验证奖励的强化学习（RLVR），显著提升在大型库中合成API的准确性和效率。


<details>
  <summary>Details</summary>
Motivation: 在大型库中组合新API面临搜索空间指数级增长的问题，传统基于组件的合成方法依赖昂贵的探索和手工编写规范；而大语言模型虽能根据自然语言生成代码，却常因幻觉和缺乏最新上下文信息而产生错误。

Method: APRIL结合LLM、自动提示优化（APO）和基于可验证奖励的强化学习（RLVR）：APO对冻结模型的提示进行迭代优化，RLVR则微调策略以提升功能正确性，形成高效合成流程。

Result: 在81个真实科学Python库API上的评估表明，相比由专家提示引导但未经微调的指令调优LLM，APRIL取得了显著性能提升。

Conclusion: 整合APO与RLVR为大型库中的组件化API合成提供了一条稳健且可扩展的路径。

Abstract: APIs are central to modern software development, yet composing new APIs from
large libraries is difficult due to the exponential search space; traditional
component-based synthesis relies on costly exploration and hand-crafted
specifications. While large language models (LLMs) can generate implementations
from natural language, hallucinations and limited access to up-to-date
contextual information often yield incorrect code. In this paper, we present
APRIL, an approach that combines LLM-based synthesis with Automatic Prompt
Optimization (APO) and Reinforcement Learning from Verifiable Rewards (RLVR):
APO iteratively refines prompts for a frozen model, while RLVR fine-tunes the
policy toward functional correctness, producing an efficient synthesis
pipeline. Evaluated on 81 real-world APIs from widely used scientific Python
libraries and benchmarked against instruction-tuned but unfine-tuned LLMs
guided by expert prompts, APRIL achieves substantial improvements. These
results indicate that integrating APO and RLVR provides a robust, scalable path
for component-based API synthesis in large libraries.

</details>


### [7] [Towards Repository-Level Program Verification with Large Language Models](https://arxiv.org/abs/2509.25197)
*Si Cheng Zhong,Xujie Si*

Main category: cs.SE

TL;DR: 本文提出了首个面向仓库级验证的基准 RVBench 和一个结合检索增强与上下文感知提示的可扩展框架 RagVerus，显著提升了大语言模型在多模块代码库中自动化形式化验证的能力。


<details>
  <summary>Details</summary>
Motivation: 现有基于大语言模型的形式化验证方法主要聚焦于孤立的函数级任务，忽略了真实项目中跨模块依赖和全局上下文等关键挑战，难以扩展到整个软件仓库。

Method: 作者构建了 RVBench 基准，并提出了 RagVerus 框架，该框架结合检索增强生成（RAG）与上下文感知提示，用于自动化合成多模块仓库的证明。

Result: RagVerus 在现有基准上将证明通过率提高了三倍，在更具挑战性的 RVBench 上实现了 27% 的相对提升。

Conclusion: RagVerus 展示了一种可扩展且样本高效的仓库级形式化验证解决方案，有效应对了跨模块依赖和全局上下文带来的挑战。

Abstract: Recent advancements in large language models (LLMs) suggest great promises in
code and proof generations. However, scaling automated formal verification to
real-world projects requires resolving cross-module dependencies and global
contexts, which are crucial challenges overlooked by existing LLM-based methods
with a special focus on targeting isolated, function-level verification tasks.
To systematically explore and address the significant challenges of verifying
entire software repositories, we introduce RVBench, the first verification
benchmark explicitly designed for repository-level evaluation, constructed from
four diverse and complex open-source Verus projects.
  We further introduce RagVerus, an extensible framework that synergizes
retrieval-augmented generation with context-aware prompting to automate proof
synthesis for multi-module repositories. RagVerus triples proof pass rates on
existing benchmarks under constrained model inference budgets, and achieves a
27% relative improvement on the more challenging RVBench benchmark,
demonstrating a scalable and sample-efficient verification solution.

</details>


### [8] [CircInspect: Integrating Visual Circuit Analysis, Abstraction, and Real-Time Development in Quantum Debugging](https://arxiv.org/abs/2509.25199)
*Mushahid Khan,Prashant J. Nair,Olivia Di Matteo*

Main category: cs.SE

TL;DR: 本文提出了 CircInspect，一个用于调试 Python 和 PennyLane 中量子程序的交互式工具，支持断点设置、实时分析、电路可视化等功能，以应对量子软件开发中的独特挑战。


<details>
  <summary>Details</summary>
Motivation: 量子计算的兴起带来了传统软件工程方法难以应对的新挑战，如量子计算的概率性、独特的算法原语和硬件噪声，因此需要专门的调试工具来支持量子程序开发。

Method: 开发了名为 CircInspect 的交互式调试工具，集成断点、实时监控、电路结构可视化和信息抽象等功能，用于分析和调试量子程序。

Result: CircInspect 能够有效帮助用户分析量子电路组件、监控输出、可视化结构变化并提升对量子程序的理解。

Conclusion: CircInspect 为量子软件调试提供了一种实用且直观的解决方案，有助于应对量子编程中的复杂性和独特性。

Abstract: Software bugs typically result from errors in specifications or code
translation. While classical software engineering has evolved with various
tools and methodologies to tackle such bugs, the emergence of quantum computing
presents unique challenges. Quantum software development introduces
complexities due to the probabilistic nature of quantum computing, distinct
algorithmic primitives, and potential hardware noise. In this paper, we
introduce CircInspect, an interactive tool tailored for debugging quantum
programs in Python and PennyLane. By leveraging breakpoints and real-time
software development features, \toolname~empowers users to analyze isolated
quantum circuit components, monitor program output, visualize structural
changes, and abstract information to enhance comprehension.

</details>


### [9] [Generating High-Quality Datasets for Code Editing via Open-Source Language Models](https://arxiv.org/abs/2509.25203)
*Zekai Zhang,Mingwei Liu,Zhenxi Chen,Linxi Liang,Yuxuan Chen,Guangsheng Ou,Yanlin Wang,Dan Li,Xin Peng,Zibin Zheng*

Main category: cs.SE

TL;DR: 本文提出CanItEdit开源流水线，利用多个大语言模型合成高质量、多样化的代码编辑三元组，构建了包含2万样本的数据集OCEDataFT，并在该数据集上微调模型，显著提升了代码编辑性能，接近闭源系统如GPT-4的表现。


<details>
  <summary>Details</summary>
Motivation: 现有基于提交的代码编辑数据集存在噪声大、多样性不足、无法反映真实编辑指令风格等问题，限制了模型性能。

Method: 提出CanItEdit流水线，利用多个LLM生成“简洁”和“描述性”两类代码编辑指令，并基于diff和主题进行过滤，构建高质量数据集OCEDataFT；在该数据集上微调三个先进基础模型。

Result: 微调后模型在CanItEdit基准上pass@1相对提升4.50%至20.79%，性能接近闭源系统，与GPT-4的差距缩小至3.54%。

Conclusion: 通过自动生成高质量、多样化的代码编辑数据，CanItEdit有效提升了开源模型的代码编辑能力，显著缩小了与顶尖闭源模型的差距。

Abstract: Code editing plays a vital role in software engineering, requiring developers
to adjust existing code according to natural language instructions while
keeping functionality intact and avoiding unnecessary modifications. However,
commit-based datasets commonly used for this task are often noisy, lack
diversity, and fail to reflect the style of real-world edit instructions. To
address this, we introduce CanItEdit, an open-source pipeline that leverages
multiple LLMs to synthesize realistic code-edit triplets. The pipeline produces
both concise "lazy" instructions and more detailed "descriptive" ones, and
applies filtering based on diffs and topics to guarantee data quality and
variety. Using this process, we construct OCEDataFT, a curated dataset of 20K
samples. Fine-tuning three advanced base models on OCEDataFT leads to
significant performance boosts on the CanItEdit benchmark, with relative pass@1
improvements ranging from 4.50% to 20.79%. Notably, the resulting models
achieve performance close to closed-source systems, narrowing the gap to GPT-4
to just 3.54%, without relying on proprietary resources or manual annotation.

</details>


### [10] [A Benchmark for Localizing Code and Non-Code Issues in Software Projects](https://arxiv.org/abs/2509.25242)
*Zejun Zhang,Jian Wang,Qingyun Yang,Yifan Pan,Yi Tang,Yi Li,Zhenchang Xing,Tian Zhang,Xuandong Li,Guoan Zhang*

Main category: cs.SE

TL;DR: 本文提出了MULocBench，一个包含1100个来自46个热门GitHub Python项目的多样化问题的数据集，用于更全面地评估问题定位方法。相比现有基准，它涵盖更多类型的问题、根本原因、定位范围和文件类型（包括非代码文件）。实验表明，当前最先进的定位方法和LLM提示策略在该基准上表现不佳（Acc@5和F1均低于40%），揭示了现实场景中问题定位的挑战。


<details>
  <summary>Details</summary>
Motivation: 现有问题定位基准（如SWE-Bench和LocBench）主要关注拉取请求相关的问题和代码文件，忽略了提交记录、评论、配置文件和文档等其他证据和非代码文件，无法全面反映真实软件维护中的问题定位场景。

Method: 构建了一个名为MULocBench的新基准数据集，包含来自46个热门GitHub Python项目的1100个问题，覆盖多种问题类型、根本原因、定位范围和文件类型；并在该数据集上评估了当前最先进的定位方法和五种基于大语言模型（LLM）的提示策略。

Result: 实验结果显示，即使在文件级别，现有方法的性能指标（Acc@5、F1）仍低于40%，表明当前技术在处理真实、多维度的问题定位任务时存在显著局限。

Conclusion: MULocBench提供了一个更真实、全面的问题定位评估基准，揭示了现有方法的不足，并为未来研究提供了公开资源。

Abstract: Accurate project localization (e.g., files and functions) for issue
resolution is a critical first step in software maintenance. However, existing
benchmarks for issue localization, such as SWE-Bench and LocBench, are limited.
They focus predominantly on pull-request issues and code locations, ignoring
other evidence and non-code files such as commits, comments, configurations,
and documentation. To address this gap, we introduce MULocBench, a
comprehensive dataset of 1,100 issues from 46 popular GitHub Python projects.
Comparing with existing benchmarks, MULocBench offers greater diversity in
issue types, root causes, location scopes, and file types, providing a more
realistic testbed for evaluation. Using this benchmark, we assess the
performance of state-of-the-art localization methods and five LLM-based
prompting strategies. Our results reveal significant limitations in current
techniques: even at the file level, performance metrics (Acc@5, F1) remain
below 40%. This underscores the challenge of generalizing to realistic,
multi-faceted issue resolution. To enable future research on project
localization for issue resolution, we publicly release MULocBench at
https://huggingface.co/datasets/somethingone/MULocBench.

</details>


### [11] [Reinforcement Learning-Guided Chain-of-Draft for Token-Efficient Code Generation](https://arxiv.org/abs/2509.25243)
*Xunzhu Tang,Iyiola Emmanuel Olatunji,Tiezhu Sun,Jacques Klein,Tegawende F. Bissyande*

Main category: cs.SE

TL;DR: 本文提出MultiCoD框架，结合Chain-of-Draft提示与强化学习，从多个候选解中选择最优代码方案，在保证正确性、效率和清晰度的同时显著降低用户成本。


<details>
  <summary>Details</summary>
Motivation: 大语言模型（LLMs）在代码生成中虽具表面流畅性，但在需要正确性和语义对齐的结构化推理任务中表现不佳；现有方法如CoT过于冗长，CoD虽简洁但输出质量不稳定，难以选择最优解。

Method: 提出MultiCoD框架，利用策略引导提示生成多样化的Chain-of-Draft候选解，并将解的选择建模为上下文赌博机问题，通过奖励函数优化代码复杂度、推理结构和策略元数据等可解释特征。

Result: 在MBPP、BigCodeBench、SWE-bench Verified和Defects4J等基准上，MultiCoD优于或媲美标准提示、CoT和CoD基线，同时通过仅对选定输出计费，使用户成本降低超50%，提升响应质量。

Conclusion: MultiCoD通过多候选设计与强化学习选择机制，在提升代码生成质量的同时显著降低成本，更具可持续性和实际部署价值。

Abstract: LLMs demonstrate surface-level fluency in code generation but struggle with
structured reasoning tasks requiring correctness and semantic alignment. While
Chain-of-Thought (CoT) prompting enhances reasoning through intermediate steps,
it suffers from verbosity and inefficiency. Chain-of-Draft (CoD) prompting
offers more concise reasoning, but the stochastic nature of LLMs produces
varying solution quality, making optimal selection challenging. We propose
\multicod, a reinforcement learning framework that learns to select the most
promising candidate from CoD-generated solutions. Our approach uses
strategy-guided prompting to encourage diverse reasoning styles and models
solution selection as a contextual bandit problem. The framework optimizes
interpretable features including code complexity, reasoning structure, and
strategic metadata through a reward function balancing correctness, efficiency,
and clarity. Experiments on MBPP, BigCodeBench, SWE-bench Verified, and
Defects4J show \multicod~outperforms and in some cases, on par with standard
prompting, CoT, and CoD baselines while achieving cost and token efficiency
from the user's perspective through a multi-candidate design that charges only
for the selected output, reducing user billing by over 50\% and improving LLM
response quality, making \multicod~more sustainable and scalable for real-world
deployment. Our code is available: https://anonymous.4open.science/r/MultiCoD.

</details>


### [12] [Protocode: Prototype-Driven Interpretability for Code Generation in LLMs](https://arxiv.org/abs/2509.25247)
*Krishna Vamshi Bodla,Haizhao Yang*

Main category: cs.SE

TL;DR: 本文研究了大语言模型（LLM）在代码生成任务中通过自动采样高质量的上下文学习（ICL）示例，以提升模型性能和生成代码的可解释性。


<details>
  <summary>Details</summary>
Motivation: 随着LLM在代码生成中的广泛应用，自动生成的代码可能存在次优或不安全的问题，因此需要提升其性能与可解释性。

Method: 作者基于抽象语法树（AST）分析MBPP测试集输出，识别受ICL示例影响最大的代码区域，并自动采样高质量的ICL演示。

Result: 高质量的ICL示例不仅提升了代码可解释性，还在pass@10指标上带来性能提升；而低质量示例则会降低性能。

Conclusion: 高效的ICL示例采样策略对LLM在代码生成任务中的表现具有显著影响，值得进一步研究和优化。

Abstract: Since the introduction of Large Language Models (LLMs), they have been widely
adopted for various tasks such as text summarization, question answering,
speech-to-text translation, and more. In recent times, the use of LLMs for code
generation has gained significant attention, with tools such as Cursor and
Windsurf demonstrating the ability to analyze massive code repositories and
recommend relevant changes. Big tech companies have also acknowledged the
growing reliance on LLMs for code generation within their codebases. Although
these advances significantly improve developer productivity, increasing
reliance on automated code generation can proportionally increase the risk of
suboptimal solutions and insecure code. Our work focuses on automatically
sampling In-Context Learning (ICL) demonstrations which can improve model
performance and enhance the interpretability of the generated code. Using
AST-based analysis on outputs from the MBPP test set, we identify regions of
code most influenced by the chosen demonstrations. In our experiments, we show
that high-quality ICL demonstrations not only make outputs easier to interpret
but also yield a positive performance improvement on the pass@10 metric.
Conversely, poorly chosen ICL demonstrations affected the LLM performance on
the pass@10 metric negatively compared to the base model. Overall, our approach
highlights the importance of efficient sampling strategies for ICL, which can
affect the performance of the model on any given task.

</details>


### [13] [BuildBench: Benchmarking LLM Agents on Compiling Real-World Open-Source Software](https://arxiv.org/abs/2509.25248)
*Zehua Zhang,Ati Priya Bajaj,Divij Handa,Siyu Liu,Arvind S Raj,Hongkai Chen,Hulin Wang,Yibo Liu,Zion Leonahenahe Basque,Souradip Nath,Vishal Juneja,Nikhil Chapre,Yan Shoshitaishvili,Adam Doupé,Chitta Baral,Ruoyu Wang*

Main category: cs.SE

TL;DR: 本文提出了一个更具挑战性和现实性的开源软件（OSS）自动编译基准 BUILD-BENCH，以及一个基于大语言模型的强基线代理 OSS-BUILD-AGENT，用于应对 OSS 编译中常见的缺失说明、未记录依赖和需修改源码等复杂问题。


<details>
  <summary>Details</summary>
Motivation: 现有 OSS 自动编译方法依赖人工规则，难以适应需要定制配置或环境的项目；而近期基于 LLM 的方法仅在高质量子集上评估，低估了真实编译挑战。

Method: 提出 BUILD-BENCH 基准，涵盖更多样化、更具代表性的 OSS 项目，并设计 OSS-BUILD-AGENT，包含增强的构建指令检索模块，以适应异构 OSS 特性。

Result: OSS-BUILD-AGENT 在 BUILD-BENCH 上达到当前最优性能，并通过详细分析不同编译方法设计对整体任务的影响，为未来研究提供指导。

Conclusion: BUILD-BENCH 能真实反映智能体处理复杂软件工程任务的能力，有望推动软件开发与安全领域的下游应用创新。

Abstract: Automatically compiling open-source software (OSS) projects is a vital,
labor-intensive, and complex task, which makes it a good challenge for LLM
Agents. Existing methods rely on manually curated rules and workflows, which
cannot adapt to OSS that requires customized configuration or environment
setup. Recent attempts using Large Language Models (LLMs) used selective
evaluation on a subset of highly rated OSS, a practice that underestimates the
realistic challenges of OSS compilation. In practice, compilation instructions
are often absent, dependencies are undocumented, and successful builds may even
require patching source files or modifying build scripts. We propose a more
challenging and realistic benchmark, BUILD-BENCH, comprising OSS that are more
diverse in quality, scale, and characteristics. Furthermore, we propose a
strong baseline LLM-based agent, OSS-BUILD-AGENT, an effective system with
enhanced build instruction retrieval module that achieves state-of-the-art
performance on BUILD-BENCH and is adaptable to heterogeneous OSS
characteristics. We also provide detailed analysis regarding different
compilation method design choices and their influence to the whole task,
offering insights to guide future advances. We believe performance on
BUILD-BENCH can faithfully reflect an agent's ability to tackle compilation as
a complex software engineering tasks, and, as such, our benchmark will spur
innovation with a significant impact on downstream applications in the fields
of software development and software security.

</details>


### [14] [RANGER -- Repository-Level Agent for Graph-Enhanced Retrieval](https://arxiv.org/abs/2509.25257)
*Pratik Shah,Rajat Ghosh,Aryan Singhal,Debojyoti Dutta*

Main category: cs.SE

TL;DR: 本文提出了RANGER，一个面向仓库级代码检索的智能体，能够同时处理代码实体查询和自然语言查询。它通过构建包含代码层级与跨文件依赖关系的知识图谱，并结合Cypher查询与MCTS引导的图探索，显著提升了多种自动软件工程任务的检索性能。


<details>
  <summary>Details</summary>
Motivation: 现有代码检索方法主要关注代码实体查询，缺乏对自然语言查询的有效支持，难以满足通用自动软件工程任务（如代码搜索、问答、依赖检索和代码补全）的多样化需求。

Method: RANGER首先构建一个涵盖整个代码仓库的细粒度知识图谱，包含变量级依赖关系，并为图节点添加文本描述和嵌入；然后采用双阶段检索机制：对代码实体查询使用Cypher快速查找，对自然语言查询则通过MCTS引导在图上进行探索。

Result: 在CodeSearchNet和RepoQA上优于基于强模型（如Qwen3-8B）嵌入的检索基线；在RepoBench上实现更优的跨文件依赖检索；在CrossCodeEval中，与BM25结合后在代码补全任务中取得最高精确匹配率。

Conclusion: RANGER通过统一处理代码实体与自然语言查询，在多个ASE任务中展现出卓越性能，填补了当前代码检索系统在通用性方面的空白。

Abstract: General-purpose automated software engineering (ASE) includes tasks such as
code completion, retrieval, repair, QA, and summarization. These tasks require
a code retrieval system that can handle specific queries about code entities,
or code entity queries (for example, locating a specific class or retrieving
the dependencies of a function), as well as general queries without explicit
code entities, or natural language queries (for example, describing a task and
retrieving the corresponding code). We present RANGER, a repository-level code
retrieval agent designed to address both query types, filling a gap in recent
works that have focused primarily on code-entity queries. We first present a
tool that constructs a comprehensive knowledge graph of the entire repository,
capturing hierarchical and cross-file dependencies down to the variable level,
and augments graph nodes with textual descriptions and embeddings to bridge the
gap between code and natural language. RANGER then operates on this graph
through a dual-stage retrieval pipeline. Entity-based queries are answered
through fast Cypher lookups, while natural language queries are handled by
MCTS-guided graph exploration. We evaluate RANGER across four diverse
benchmarks that represent core ASE tasks including code search, question
answering, cross-file dependency retrieval, and repository-level code
completion. On CodeSearchNet and RepoQA it outperforms retrieval baselines that
use embeddings from strong models such as Qwen3-8B. On RepoBench, it achieves
superior cross-file dependency retrieval over baselines, and on CrossCodeEval,
pairing RANGER with BM25 delivers the highest exact match rate in code
completion compared to other RAG methods.

</details>


### [15] [Automatically Generating Web Applications from Requirements Via Multi-Agent Test-Driven Development](https://arxiv.org/abs/2509.25297)
*Yuxuan Wan,Tingshuo Liang,Jiakai Xu,Jingyu Xiao,Yintong Huo,Michael R. Lyu*

Main category: cs.SE

TL;DR: TDDev 是首个支持测试驱动开发（TDD）的 LLM 智能体框架，可从自然语言或设计图自动生成端到端的全栈 Web 应用，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前基于多模态大语言模型的网页生成方法仅限于前端，无法生成完整可用的全栈应用；开发全栈应用本身复杂且耗时，需跨技术栈能力。

Method: TDDev 框架根据自然语言或设计图像自动生成可执行测试用例，同步生成前后端代码，模拟用户交互，并迭代优化实现直至满足所有需求。

Result: 在多样化的应用场景实验中，TDDev 相比当前最优基线方法整体准确率提升 14.4%，能自动生成可靠、高质量的全栈 Web 应用。

Conclusion: TDDev 有效解决了全栈应用自动化中的关键挑战，如需求模糊、多文件依赖、功能正确性与视觉保真度兼顾，实现了无需人工干预的端到端开发。

Abstract: Developing full-stack web applications is complex and time-intensive,
demanding proficiency across diverse technologies and frameworks. Although
recent advances in multimodal large language models (MLLMs) enable automated
webpage generation from visual inputs, current solutions remain limited to
front-end tasks and fail to deliver fully functional applications. In this
work, we introduce TDDev, the first test-driven development (TDD)-enabled
LLM-agent framework for end-to-end full-stack web application generation. Given
a natural language description or design image, TDDev automatically derives
executable test cases, generates front-end and back-end code, simulates user
interactions, and iteratively refines the implementation until all requirements
are satisfied. Our framework addresses key challenges in full-stack automation,
including underspecified user requirements, complex interdependencies among
multiple files, and the need for both functional correctness and visual
fidelity. Through extensive experiments on diverse application scenarios, TDDev
achieves a 14.4% improvement on overall accuracy compared to state-of-the-art
baselines, demonstrating its effectiveness in producing reliable, high-quality
web applications without requiring manual intervention.

</details>


### [16] [Detecting and Fixing API Misuses of Data Science Libraries Using Large Language Models](https://arxiv.org/abs/2509.25378)
*Akalanka Galappaththi,Francisco Ribeiro,Sarah Nadi*

Main category: cs.SE

TL;DR: 本文提出 DSCHECKER，一种基于大语言模型（LLM）的方法，用于检测和修复数据科学库中的 API 误用。通过引入 API 指令和数据信息，DSCHECKER 在实验中实现了最高 61.18% 的检测 F1 分数和 51.28% 的修复率；其改进版 DSCHECKER Agent 在更贴近现实的设定下仍达到 48.65% 的检测 F1 分数和 39.47% 的修复率。


<details>
  <summary>Details</summary>
Motivation: 数据科学库（如 scikit-learn 和 pandas）因其数据密集型特性，使得 API 误用的检测更具挑战性，亟需有效方法自动识别和修复此类问题。

Method: 提出 DSCHECKER，利用大语言模型结合 API 指令和数据信息进行 API 误用检测与修复，并进一步开发 DSCHECKER Agent，引入自适应函数调用机制以在信息未知的真实场景中按需获取上下文。

Result: 在五个数据科学库的误用数据集上，DSCHECKER 最佳模型达到 61.18% 的检测 F1 分数和 51.28% 的修复率；DSCHECKER Agent 在模拟真实场景下仍取得 48.65% 的检测 F1 分数和 39.47% 的修复率。

Conclusion: 结合 API 指令与数据信息的大语言模型方法在检测和修复数据科学库 API 误用方面具有显著潜力，尤其在贴近实际应用的动态信息获取场景中仍表现良好。

Abstract: Data science libraries, such as scikit-learn and pandas, specialize in
processing and manipulating data. The data-centric nature of these libraries
makes the detection of API misuse in them more challenging. This paper
introduces DSCHECKER, an LLM-based approach designed for detecting and fixing
API misuses of data science libraries. We identify two key pieces of
information, API directives and data information, that may be beneficial for
API misuse detection and fixing. Using three LLMs and misuses from five data
science libraries, we experiment with various prompts. We find that
incorporating API directives and data-specific details enhances Dschecker's
ability to detect and fix API misuses, with the best-performing model achieving
a detection F1-score of 61.18 percent and fixing 51.28 percent of the misuses.
Building on these results, we implement Dschecker agent which includes an
adaptive function calling mechanism to access information on demand, simulating
a real-world setting where information about the misuse is unknown in advance.
We find that Dschecker agent achieves 48.65 percent detection F1-score and
fixes 39.47 percent of the misuses, demonstrating the promise of LLM-based API
misuse detection and fixing in real-world scenarios.

</details>


### [17] [A Cartography of Open Collaboration in Open Source AI: Mapping Practices, Motivations, and Governance in 14 Open Large Language Model Projects](https://arxiv.org/abs/2509.25397)
*Johan Linåker,Cailean Osborne,Jennifer Ding,Ben Burtenshaw*

Main category: cs.SE

TL;DR: 本文通过访谈14个开源大语言模型（LLM）项目的开发者，探索了开源LLM开发与复用全周期中的协作模式，揭示了协作范围、开发者动机和组织模型的多样性，并提出支持开源AI生态发展的实践建议。


<details>
  <summary>Details</summary>
Motivation: 当前对开源大语言模型在发布前后所采用的协作方式缺乏系统研究，限制了我们对其项目启动、组织与治理机制的理解，也阻碍了进一步促进该生态系统发展的机会识别。

Method: 采用探索性分析方法，对来自北美、欧洲、非洲和亚洲的草根项目、研究机构、初创公司和大型科技公司的14个开源LLM项目的开发者进行半结构化访谈。

Result: 研究发现：（1）开源LLM项目的协作远超模型本身，涵盖数据集、基准、开源框架等多个方面；（2）开发者具有社会、经济和技术等多重动机；（3）项目呈现出五种不同的组织模型，其控制集中度和社区参与策略各不相同。

Conclusion: 为希望支持全球开源AI社区发展的利益相关者提供了实践建议，以推动更开放的AI未来。

Abstract: The proliferation of open large language models (LLMs) is fostering a vibrant
ecosystem of research and innovation in artificial intelligence (AI). However,
the methods of collaboration used to develop open LLMs both before and after
their public release have not yet been comprehensively studied, limiting our
understanding of how open LLM projects are initiated, organized, and governed
as well as what opportunities there are to foster this ecosystem even further.
We address this gap through an exploratory analysis of open collaboration
throughout the development and reuse lifecycle of open LLMs, drawing on
semi-structured interviews with the developers of 14 open LLMs from grassroots
projects, research institutes, startups, and Big Tech companies in North
America, Europe, Africa, and Asia. We make three key contributions to research
and practice. First, collaboration in open LLM projects extends far beyond the
LLMs themselves, encompassing datasets, benchmarks, open source frameworks,
leaderboards, knowledge sharing and discussion forums, and compute
partnerships, among others. Second, open LLM developers have a variety of
social, economic, and technological motivations, from democratizing AI access
and promoting open science to building regional ecosystems and expanding
language representation. Third, the sampled open LLM projects exhibit five
distinct organizational models, ranging from single company projects to
non-profit-sponsored grassroots projects, which vary in their centralization of
control and community engagement strategies used throughout the open LLM
lifecycle. We conclude with practical recommendations for stakeholders seeking
to support the global community building a more open future for AI.

</details>


### [18] [PIPer: On-Device Environment Setup via Online Reinforcement Learning](https://arxiv.org/abs/2509.25455)
*Alexander Kovrigin,Aleksandra Eliseeva,Konstantin Grotov,Egor Bogomolov,Yaroslav Zharov*

Main category: cs.SE

TL;DR: 本文提出了一种结合监督微调和基于可验证奖励的强化学习（RLVR）的方法，用于微调专门的模型以自动化软件项目的环境配置任务。该方法使Qwen3-8B在EnvBench-Python基准上达到了与更大模型（如Qwen3-32B和GPT-4o）相当的性能。


<details>
  <summary>Details</summary>
Motivation: 环境配置是软件工程中的一个长期挑战，尽管大语言模型（LLMs）已有进展，但在自动化该任务上表现仍有限。因此，作者希望开发一个专门针对环境配置任务优化的模型，以提升自动化配置的成功率，并支持开发者和研究人员更高效地开展工作。

Method: 作者结合监督微调（用于生成正确的Bash脚本）和强化学习与可验证奖励（RLVR）来微调一个专用模型，使其适应环境配置任务。

Result: 在EnvBench-Python基准测试中，经该方法微调后的Qwen3-8B模型性能与更大的Qwen3-32B和GPT-4o模型相当，且可在消费级硬件上运行。

Conclusion: 通过结合监督微调与RLVR，可以有效提升较小规模语言模型在环境配置任务上的表现，使其达到与大型模型相当的水平，为自动化软件工程任务提供了高效可行的解决方案。

Abstract: Environment setup-the process of configuring the system to work with a
specific software project-represents a persistent challenge in Software
Engineering (SE). Automated environment setup methods could assist developers
by providing fully configured environments for arbitrary repositories without
manual effort. This also helps SE researchers to scale execution-based
benchmarks. However, recent studies reveal that even state-of-the-art Large
Language Models (LLMs) achieve limited success in automating this task. To
address this limitation, we tune a specialized model for environment setup. We
combine supervised fine-tuning for generating correct Bash scripts and
Reinforcement Learning with Verifiable Rewards (RLVR) to adapt it to the task
of environment setup. On EnvBench-Python, our method enables Qwen3-8B (a model
runnable on consumer hardware) to perform on par with larger models-Qwen3-32B
and GPT-4o. The training code and model checkpoints are available online:
https://github.com/JetBrains-Research/PIPer.

</details>


### [19] [BloomAPR: A Bloom's Taxonomy-based Framework for Assessing the Capabilities of LLM-Powered APR Solutions](https://arxiv.org/abs/2509.25465)
*Yinghang Ma,Jiho Shin,Leuson Da Silva,Zhen Ming,Jiang,Song Wang,Foutse Khomh,Shin Hwei Tan*

Main category: cs.SE

TL;DR: 本文提出BloomAPR，一种基于布鲁姆分类法的动态评估框架，用于更全面地评估大语言模型（LLM）驱动的自动程序修复（APR）系统在不同认知层次上的能力，并揭示现有APR方法在复杂和真实场景中的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有LLM驱动的APR系统通常在静态基准（如Defects4J和SWE-bench）上评估，存在训练数据污染风险且难以评估其在动态多样环境中的修复能力。

Method: 提出BloomAPR动态评估框架，基于布鲁姆分类法构建不同认知层次的测试场景，并以Defects4J为案例，评估ChatRepair和CigaR在GPT-3.5-Turbo、Llama-3.1和StarCoder-2三种LLM上的表现。

Result: LLM驱动的APR系统在记忆层（Remember）表现最佳（最高修复81.57%的bug），在理解层（Understand）面对合成bug时性能提升显著（最多提升60.66%），但在应用层（Apply）对微小语法变化修复能力下降（仅43.32%），在分析层（Analyze）面对真实项目中注入的相似bug时表现最差（仅修复13.46%至41.34%）。

Conclusion: 当前LLM驱动的APR系统在复杂和真实场景中能力有限，亟需发展更动态、多维的评估基准，BloomAPR为此提供了可信评估的基础。

Abstract: Recent advances in large language models (LLMs) have accelerated the
development of AI-driven automated program repair (APR) solutions. However,
these solutions are typically evaluated using static benchmarks such as
Defects4J and SWE-bench, which suffer from two key limitations: (1) the risk of
data contamination, potentially inflating evaluation results due to overlap
with LLM training data, and (2) limited ability to assess the APR capabilities
in dynamic and diverse contexts. In this paper, we introduced BloomAPR, a novel
dynamic evaluation framework grounded in Bloom's Taxonomy. Our framework offers
a structured approach to assess the cognitive capabilities of LLM-powered APR
solutions across progressively complex reasoning levels. Using Defects4J as a
case study, we evaluated two state-of-the-art LLM-powered APR solutions,
ChatRepair and CigaR, under three different LLMs: GPT-3.5-Turbo, Llama-3.1, and
StarCoder-2. Our findings show that while these solutions exhibit basic
reasoning skills and effectively memorize bug-fixing patterns (fixing up to
81.57% of bugs at the Remember layer), their performance increases with
synthetically generated bugs (up to 60.66% increase at the Understand layer).
However, they perform worse on minor syntactic changes (fixing up to 43.32% at
the Apply layer), and they struggle to repair similar bugs when injected into
real-world projects (solving only 13.46% to 41.34% bugs at the Analyze layer).
These results underscore the urgent need for evolving benchmarks and provide a
foundation for more trustworthy evaluation of LLM-powered software engineering
solutions.

</details>


### [20] [AGNOMIN -- Architecture Agnostic Multi-Label Function Name Prediction](https://arxiv.org/abs/2509.25514)
*Yonatan Gizachew Achamyeleh,Tongtao Zhang,Joshua Hyunki Kim,Gabriel Garcia,Shih-Yuan Yu,Anton Kocheturov,Mohammad Abdullah Al Faruque*

Main category: cs.SE

TL;DR: AGNOMIN是一种新颖的、与架构无关的多标签函数名预测方法，通过构建特征增强的层次图（FEHG）并结合层次化图神经网络与改进的解码器，在多种架构的剥离二进制文件中显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有函数名预测方法受限于架构特定性、数据稀缺性和命名规范多样性，难以支持跨架构的可扩展安全评估，尤其在漏洞分析与修补等逆向工程任务中表现不足。

Method: AGNOMIN构建融合控制流图、函数调用图和动态学习的PCode特征的特征增强层次图（FEHG），并通过层次化图神经网络生成跨架构一致的函数表示，再利用改进的Ren\'ee启发式解码器（含注意力头层和算法优化）进行多标签函数名预测。

Result: 在涵盖三种架构的9,000个ELF二进制文件数据集上，AGNOMIN相较最先进方法在测试集上最高提升27.17%的精确率和55.86%的召回率；在未见架构上召回率高出基线5.89%，并在安全黑客松中成功辅助跨架构漏洞分析与修补。

Conclusion: AGNOMIN有效解决了跨架构函数名预测的挑战，显著提升了剥离二进制文件中函数语义理解的准确性与泛化能力，为可扩展的二进制安全分析提供了实用工具。

Abstract: Function name prediction is crucial for understanding stripped binaries in
software reverse engineering, a key step for \textbf{enabling subsequent
vulnerability analysis and patching}. However, existing approaches often
struggle with architecture-specific limitations, data scarcity, and diverse
naming conventions. We present AGNOMIN, a novel architecture-agnostic approach
for multi-label function name prediction in stripped binaries. AGNOMIN builds
Feature-Enriched Hierarchical Graphs (FEHGs), combining Control Flow Graphs,
Function Call Graphs, and dynamically learned \texttt{PCode} features. A
hierarchical graph neural network processes this enriched structure to generate
consistent function representations across architectures, vital for
\textbf{scalable security assessments}. For function name prediction, AGNOMIN
employs a Ren\'ee-inspired decoder, enhanced with an attention-based head layer
and algorithmic improvements.
  We evaluate AGNOMIN on a comprehensive dataset of 9,000 ELF executable
binaries across three architectures, demonstrating its superior performance
compared to state-of-the-art approaches, with improvements of up to 27.17\% in
precision and 55.86\% in recall across the testing dataset. Moreover, AGNOMIN
generalizes well to unseen architectures, achieving 5.89\% higher recall than
the closest baseline. AGNOMIN's practical utility has been validated through
security hackathons, where it successfully aided reverse engineers in analyzing
and patching vulnerable binaries across different architectures.

</details>


### [21] [M&SCheck: Towards a Checklist to Support Software Engineering Newcomers to the Modeling and Simulation Area](https://arxiv.org/abs/2509.25625)
*Luiza Martins de Freitas Cintra,Philipp Zech,Mohamad Kassab,Eliomar Araújo Lima,Sofia Larissa da Costa Paiva,Valdemar Vicente Graciano Neto*

Main category: cs.SE

TL;DR: 本文提出一个初步检查清单，帮助软件工程初学者在建模与仿真（M&S）中选择合适的仿真范式（DEVS、系统动力学、基于代理的仿真），并通过试点研究和专家反馈验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 随着数字孪生、智慧城市和工业4.0/5.0等复杂动态系统的兴起，软件开发中越来越需要集成建模与仿真（M&S）。然而，初学者在面对多种仿真形式化方法时常常难以抉择，因此需要一个指导工具来辅助选择合适的范式。

Method: 作者基于三种主流仿真形式化方法（DEVS、系统动力学和基于代理的仿真）构建了一个初步的决策检查清单，并通过试点研究和专家咨询对清单进行评估。

Result: 试点研究结果显示：(i) 检查清单推荐的范式与原始研究中实际选用的范式一致；(ii) 专家对清单给予积极反馈。

Conclusion: 该检查清单能有效辅助M&S初学者选择合适的仿真范式，为将M&S更顺畅地集成到软件开发生命周期中提供了实用工具。

Abstract: The advent of increasingly complex and dynamic ecosystems, such as digital
twins (DT), smart cities and Industry 4.0 and 5.0, has made evident the need to
include modeling and simulation (M&S) in the software development life cycle.
Such disruptive systems include simulation models in their own architecture
(such as DT) or require the use of simulation models to represent the high
degree of movement and the multiplicity of interactions that occur between the
involved systems. However, when software engineers (particularly the newcomers)
need to use M&S in their projects, they often pose themselves an important
question: which formalism should I use? In this direction, the main
contribution of this paper is the establishment of a preliminary checklist with
questions to assist beginners in M&S in choosing the most appropriate paradigm
to solve their problems. The checklist is based on three main formalisms: DEVS,
System Dynamics and Agent-Based Simulation. A pilot study was carried out and
an expert was consulted. The preliminary results show (i) conformance between
the suggestion given by the checklist and the formalism selected in the
original studies used as input for evaluating the checklist, and (ii) a
positive feedback from the expert.

</details>


### [22] [Explainable Fault Localization for Programming Assignments via LLM-Guided Annotation](https://arxiv.org/abs/2509.25676)
*Fang Liu,Tianze Wang,Li Zhang,Zheyu Yang,Jing Jiang,Zian Sun*

Main category: cs.SE

TL;DR: 本文提出FLAME，一种面向编程作业的细粒度、可解释的错误定位方法，通过LLM引导标注与多模型集成，在教育场景和通用代码库中均显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有自动错误定位（FL）技术在教育场景中存在粒度太粗或依赖不适合大语言模型（LLM）的数值预测方式，难以提供学生所需的可操作反馈。

Method: FLAME利用编程作业的上下文信息引导LLM对错误代码行进行标注并提供解释，而非直接预测行号；同时采用加权多模型投票策略融合多个LLM的结果，以提升定位可靠性。

Result: 在编程作业数据集上，FLAME比最强基线在Top-1定位中多成功定位207个错误；在Defects4J通用软件缺陷基准上也全面优于现有方法。

Conclusion: FLAME不仅在教育场景中有效提升错误定位的准确性和教学价值，还能很好地泛化到通用软件代码库，具有广泛适用性。

Abstract: Providing timely and personalized guidance for students' programming
assignments, offers significant practical value for helping students complete
assignments and enhance their learning. In recent years, various automated
Fault Localization (FL) techniques have demonstrated promising results in
identifying errors in programs. However, existing FL techniques face challenges
when applied to educational contexts. Most approaches operate at the method
level without explanatory feedback, resulting in granularity too coarse for
students who need actionable insights to identify and fix their errors. While
some approaches attempt line-level fault localization, they often depend on
predicting line numbers directly in numerical form, which is ill-suited to
LLMs. To address these challenges, we propose FLAME, a fine-grained,
explainable Fault Localization method tailored for programming assignments via
LLM-guided Annotation and Model Ensemble. FLAME leverages rich contextual
information specific to programming assignments to guide LLMs in identifying
faulty code lines. Instead of directly predicting line numbers, we prompt the
LLM to annotate faulty code lines with detailed explanations, enhancing both
localization accuracy and educational value. To further improve reliability, we
introduce a weighted multi-model voting strategy that aggregates results from
multiple LLMs to determine the suspiciousness of each code line. Extensive
experimental results demonstrate that FLAME outperforms state-of-the-art fault
localization baselines on programming assignments, successfully localizing 207
more faults at top-1 over the best-performing baseline. Beyond educational
contexts, FLAME also generalizes effectively to general-purpose software
codebases, outperforming all baselines on the Defects4J benchmark.

</details>


### [23] [DeepCodeSeek: Real-Time API Retrieval for Context-Aware Code Generation](https://arxiv.org/abs/2509.25716)
*Esakkivel Esakkiraja,Denis Akhiyarov,Aditya Shanmugham,Chitra Ganapathy*

Main category: cs.SE

TL;DR: 本文提出一种新方法，通过扩展代码索引以预测所需API，从而提升端到端代码生成质量，并构建了一个基于真实ServiceNow脚本的新数据集以解决现有基准中API泄露问题。该方法在检索准确率上达87.86%，并通过轻量级0.6B重排模型在性能上超越8B大模型，同时降低2.5倍延迟。


<details>
  <summary>Details</summary>
Motivation: 现有检索技术局限于标准RAG查询-文档应用，难以支持高质量端到端代码生成；同时当前代码到代码的基准数据集存在API泄露问题，无法真实反映API使用意图模糊的现实挑战。

Method: 构建基于真实ServiceNow Script Includes的新数据集，设计端到端代码生成流程，开发包含合成数据生成、监督微调和强化学习的后训练流水线，优化一个紧凑的0.6B重排模型。

Result: 该方法在top-40检索准确率上达到87.86%，其0.6B重排模型在性能上优于8B大模型，同时推理延迟降低2.5倍。

Conclusion: 所提方法有效解决了企业级代码中API意图模糊的问题，在保证低计算开销的同时显著提升代码生成所需的上下文检索质量。

Abstract: Current search techniques are limited to standard RAG query-document
applications. In this paper, we propose a novel technique to expand the code
and index for predicting the required APIs, directly enabling high-quality,
end-to-end code generation for auto-completion and agentic AI applications. We
address the problem of API leaks in current code-to-code benchmark datasets by
introducing a new dataset built from real-world ServiceNow Script Includes that
capture the challenge of unclear API usage intent in the code. Our evaluation
metrics show that this method achieves 87.86% top-40 retrieval accuracy,
allowing the critical context with APIs needed for successful downstream code
generation. To enable real-time predictions, we develop a comprehensive
post-training pipeline that optimizes a compact 0.6B reranker through synthetic
dataset generation, supervised fine-tuning, and reinforcement learning. This
approach enables our compact reranker to outperform a much larger 8B model
while maintaining 2.5x reduced latency, effectively addressing the nuances of
enterprise-specific code without the computational overhead of larger models.

</details>


### [24] [Are Classical Clone Detectors Good Enough For the AI Era?](https://arxiv.org/abs/2509.25754)
*Ajmain Inqiad Alam,Palash Roy,Farouq Al-omari,Chanchal Roy,Banani Roy,Kevin Schneider*

Main category: cs.SE

TL;DR: 本文系统评估了九种经典代码克隆检测工具在AI生成代码（特别是GPT-3生成克隆）上的有效性，发现部分工具在结合归一化技术后仍具较强检测能力，但也存在性能差异，并提供了可扩展性与执行时间分析以指导工具选择。


<details>
  <summary>Details</summary>
Motivation: 随着AI生成代码的普及，其引入的系统性语法模式和复杂语义差异对传统主要针对人工编写代码设计的代码克隆检测工具提出了新挑战，亟需评估这些工具在AI生成克隆场景下的有效性。

Method: 作者构建了包含GPT-3生成克隆的基准数据集GPTCloneBench，并在该数据集及两个人工编写的基准（BigCloneBench和SemanticCloneBench）上系统评估了九种广泛使用的经典代码克隆检测工具的性能。

Result: 实验表明，采用有效归一化技术的经典代码克隆检测工具对AI生成克隆仍保持相当的检测效果，但部分工具在AI生成克隆与传统克隆上的性能表现存在显著差异；研究还提供了详细的可扩展性和执行时间分析。

Conclusion: 经典代码克隆检测工具在AI生成代码场景下仍有一定适用性，尤其在结合归一化技术时效果更佳，但其局限性也凸显了适应新范式的需求；本研究为工具选择和未来改进提供了实证依据。

Abstract: The increasing adoption of AI-generated code has reshaped modern software
development, introducing syntactic and semantic variations in cloned code.
Unlike traditional human-written clones, AI-generated clones exhibit systematic
syntactic patterns and semantic differences learned from large-scale training
data. This shift presents new challenges for classical code clone detection
(CCD) tools, which have historically been validated primarily on human-authored
codebases and optimized to detect syntactic (Type 1-3) and limited semantic
clones. Given that AI-generated code can produce both syntactic and complex
semantic clones, it is essential to evaluate the effectiveness of classical CCD
tools within this new paradigm. In this paper, we systematically evaluate nine
widely used CCD tools using GPTCloneBench, a benchmark containing
GPT-3-generated clones. To contextualize and validate our results, we further
test these detectors on established human-authored benchmarks, BigCloneBench
and SemanticCloneBench, to measure differences in performance between
traditional and AI-generated clones. Our analysis demonstrates that classical
CCD tools, particularly those enhanced by effective normalization techniques,
retain considerable effectiveness against AI-generated clones, while some
exhibit notable performance variation compared to traditional benchmarks. This
paper contributes by (1) evaluating classical CCD tools against AI-generated
clones, providing critical insights into their current strengths and
limitations; (2) highlighting the role of normalization techniques in improving
detection accuracy; and (3) delivering detailed scalability and execution-time
analyses to support practical CCD tool selection.

</details>


### [25] [LogPilot: Intent-aware and Scalable Alert Diagnosis for Large-scale Online Service Systems](https://arxiv.org/abs/2509.25874)
*Zhihan Jiang,Jinyang Liu,Yichen Li,Haiyu Huang,Xiao He,Tieying Zhang,Jianjun Chen,Yi Li,Rui Shi,Michael R. Lyu*

Main category: cs.SE

TL;DR: LogPilot 是一个基于大语言模型（LLM）的自动化日志诊断框架，通过理解告警定义的意图并构建时空日志链，显著提升了根因定位的准确性和诊断效率。


<details>
  <summary>Details</summary>
Motivation: 现有自动化工具在告警诊断中存在日志范围选择与告警无关、难以组织复杂数据进行推理的问题，导致实际效果不佳，亟需一种更精准、可扩展的诊断方法。

Method: LogPilot 采用意图感知策略，解析告警定义（如 PromQL）以识别因果相关的日志和请求；通过构建请求的时空日志链、聚类相似链以提取代表性样本，再交由 LLM 进行诊断，兼顾信息丰富性与上下文长度限制。

Result: 在火山引擎云的真实告警数据上，LogPilot 相比现有最优方法，根因总结有用性提升 50.34%，精确定位准确率提升 54.79%，单次诊断耗时不到 1 分钟，成本仅 $0.074，并已投入生产使用。

Conclusion: LogPilot 提供了一种高效、低成本且实用的自动化告警诊断方案，显著优于现有方法，适用于大规模在线服务系统的可靠性保障。

Abstract: Effective alert diagnosis is essential for ensuring the reliability of
large-scale online service systems. However, on-call engineers are often
burdened with manually inspecting massive volumes of logs to identify root
causes. While various automated tools have been proposed, they struggle in
practice due to alert-agnostic log scoping and the inability to organize
complex data effectively for reasoning. To overcome these limitations, we
introduce LogPilot, an intent-aware and scalable framework powered by Large
Language Models (LLMs) for automated log-based alert diagnosis. LogPilot
introduces an intent-aware approach, interpreting the logic in alert
definitions (e.g., PromQL) to precisely identify causally related logs and
requests. To achieve scalability, it reconstructs each request's execution into
a spatiotemporal log chain, clusters similar chains to identify recurring
execution patterns, and provides representative samples to the LLMs for
diagnosis. This clustering-based approach ensures the input is both rich in
diagnostic detail and compact enough to fit within the LLM's context window.
Evaluated on real-world alerts from Volcano Engine Cloud, LogPilot improves the
usefulness of root cause summarization by 50.34% and exact localization
accuracy by 54.79% over state-of-the-art methods. With a diagnosis time under
one minute and a cost of only $0.074 per alert, LogPilot has been successfully
deployed in production, offering an automated and practical solution for
service alert diagnosis.

</details>


### [26] [Red Teaming Program Repair Agents: When Correct Patches can Hide Vulnerabilities](https://arxiv.org/abs/2509.25894)
*Simin Chen,Yixin He,Suman Jana,Baishakhi Ray*

Main category: cs.SE

TL;DR: 本文提出SWExploit，一种生成对抗性GitHub issue的方法，可诱使基于大语言模型（LLM）的自动程序修复（APR）代理生成功能正确但存在安全漏洞的补丁，揭示当前APR评估范式忽视安全风险的问题。


<details>
  <summary>Details</summary>
Motivation: 现有APR研究主要关注补丁的功能正确性，忽略了其潜在的安全风险；鉴于GitHub等平台的开放性，恶意用户可能提交看似合法的issue，诱导APR代理生成含漏洞的补丁。

Method: SWExploit包含三个步骤：(1) 程序分析以识别潜在漏洞注入点；(2) 生成保留原始语义但提供误导性复现和错误信息的对抗性issue；(3) 根据APR代理输出迭代优化对抗性issue。

Result: 在3个代理流程和5个LLM后端上的实验表明，SWExploit生成的补丁攻击成功率最高达0.91，远高于基线（均低于0.20）。

Conclusion: 本文首次挑战“通过所有测试的补丁即安全可靠”的传统假设，指出当前APR评估范式在安全性方面的严重不足。

Abstract: LLM-based agents are increasingly deployed for software maintenance tasks
such as automated program repair (APR). APR agents automatically fetch GitHub
issues and use backend LLMs to generate patches that fix the reported bugs.
However, existing work primarily focuses on the functional correctness of
APR-generated patches, whether they pass hidden or regression tests, while
largely ignoring potential security risks. Given the openness of platforms like
GitHub, where any user can raise issues and participate in discussions, an
important question arises: Can an adversarial user submit a valid issue on
GitHub that misleads an LLM-based agent into generating a functionally correct
but vulnerable patch? To answer this question, we propose SWExploit, which
generates adversarial issue statements designed to make APR agents produce
patches that are functionally correct yet vulnerable. SWExploit operates in
three main steps: (1) program analysis to identify potential injection points
for vulnerable payloads; (2) adversarial issue generation to provide misleading
reproduction and error information while preserving the original issue
semantics; and (3) iterative refinement of the adversarial issue statements
based on the outputs of the APR agents. Empirical evaluation on three agent
pipelines and five backend LLMs shows that SWExploit can produce patches that
are both functionally correct and vulnerable (the attack success rate on the
correct patch could reach 0.91, whereas the baseline ASRs are all below 0.20).
Based on our evaluation, we are the first to challenge the traditional
assumption that a patch passing all tests is inherently reliable and secure,
highlighting critical limitations in the current evaluation paradigm for APR
agents.

</details>


### [27] [R-Log: Incentivizing Log Analysis Capability in LLMs via Reasoning-based Reinforcement Learning](https://arxiv.org/abs/2509.25987)
*Yilun Liu,Ziang Chen,Song Xu,Minggui He,Shimin Tao,Weibin Meng,Yuming Xie,Tao Han,Chunguang Zhao,Jingzhou Du,Daimeng Wei,Shenglin Zhang,Yongqian Sun*

Main category: cs.SE

TL;DR: 本文提出R-Log，一种基于推理的大型语言模型日志分析新范式，通过模仿人类工程师的结构化分析流程并结合强化学习优化，在真实日志数据上显著优于现有方法，尤其在未见过的场景中表现突出，并推出了提速5倍的R-Log-fast版本。


<details>
  <summary>Details</summary>
Motivation: 现有基于监督微调（SFT）的日志分析方法存在领域差异大、过拟合和因上下文过长而忽略关键细节导致幻觉等问题，难以泛化到新场景。

Method: 提出R-Log方法：首先在包含2000多条推理轨迹的数据集上冷启动模型，该数据集基于13种人工运维策略；然后在模拟运维环境中利用强化学习和联合奖励函数进一步优化模型推理能力。

Result: 在真实日志上的实验表明，R-Log在五项日志分析任务中均优于现有方法，在未见场景中性能提升达228.05%；R-Log-fast版本在保持93%效能的同时实现5倍加速。

Conclusion: R-Log通过引入人类工程师式的推理机制和强化学习优化，有效提升了LLM在日志分析任务中的泛化能力和准确性，显著减少幻觉问题，具有实际应用价值。

Abstract: The growing complexity of log data in modern software systems has prompted
the use of Large Language Models (LLMs) for automated log analysis. Current
approaches typically rely on direct supervised fine-tuning (SFT) on log-label
pairs. However, this exacerbates the domain discrepancy between general-purpose
LLMs and specialized log data, causing overfitting. Furthermore, SFT's
imbalanced loss computation often allows lengthy contexts to overwhelm
critical, concise details in model answers, leading to hallucinations. To
address these limitations, we propose R-Log, a novel reasoning-based paradigm
that mirrors the structured, step-by-step analytical process of human
engineers. This approach enhances generalizability by learning the underlying
rules behind conclusions. We further employ Reinforcement Learning (RL) to
optimize the model within a simulated O&M environment, thereby reducing
hallucinations by directly rewarding correct outcomes. R-Log is first
cold-started on a curated dataset of 2k+ reasoning trajectories, guided by 13
strategies from manual O&M practices, to establish an initial reasoning
capability. This ability is then refined via RL using a joint reward function.
Empirical evaluations on real-world logs show that R-Log outperforms existing
methods across five log analysis tasks, particularly in unseen scenarios (by
228.05%). We also designed R-Log-fast with 5x speedup while keeping 93% of the
efficacy.

</details>


### [28] [Using GPT to build a Project Management assistant for Jira environments](https://arxiv.org/abs/2509.26014)
*Joel Garcia-Escribano,Arkaitz Carbajo,Mikel Egaña Aranguren,Unai Lopez-Novoa*

Main category: cs.SE

TL;DR: 本文提出了JiraGPT Next，一个基于GPT大语言模型的Jira插件，通过自然语言界面帮助项目经理更轻松地处理大量项目数据。


<details>
  <summary>Details</summary>
Motivation: 项目管理中数据量庞大且复杂，现有工具学习曲线陡峭、依赖复杂编程语言，难以高效获取所需信息，因此需要更直观、易用的数据交互方式。

Method: 开发JiraGPT Next插件，将其集成到Jira中，利用GPT大语言模型提供自然语言查询接口，并评估不同提示词对任务完成准确率的影响。

Result: 论文展示了JiraGPT Next的设计方案，并评估了GPT在该场景下的准确性，分析了不同提示策略对任务执行效果的影响。

Conclusion: JiraGPT Next通过自然语言交互显著降低了项目经理处理复杂数据的门槛，提升了信息检索效率，验证了大语言模型在项目管理工具中的实用潜力。

Abstract: In the domain of Project Management, the sheer volume of data is a challenge
that project managers continually have to deal with. Effectively steering
projects from inception to completion requires handling of diverse information
streams, including timelines, budgetary considerations, and task dependencies.
To navigate this data-driven landscape with precision and agility, project
managers must rely on efficient and sophisticated tools. These tools have
become essential, as they enable project managers to streamline communication,
optimize resource allocation, and make informed decisions in real-time.
However, many of these tools have steep learning curves and require using
complex programming languages to retrieve the exact data that project managers
need. In this work we present JiraGPT Next, a software that uses the GPT Large
Language Model to ease the process by which project managers deal with large
amounts of data. It is conceived as an add-on for Jira, one of the most popular
Project Management tools, and provides a natural language interface to retrieve
information. This work presents the design decisions behind JiraGPT Next and an
evaluation of the accuracy of GPT in this context, including the effects of
providing different prompts to complete a particular task.

</details>


### [29] [Evaluating the impact of code smell refactoring on the energy consumption of Android applications](https://arxiv.org/abs/2509.26031)
*Hina Anwar,Dietmar Pfahl,Satish N. Srirama*

Main category: cs.SE

TL;DR: 本文研究了Android应用中常见代码重构对性能和能耗的影响，发现“重复代码”和“类型检查”重构最多可降低10.8%的能耗，但能耗变化与执行时间无直接关联，且不同重构顺序对能耗影响各异，需进一步研究软件特征与代码坏味道及重构效果之间的关系。


<details>
  <summary>Details</summary>
Motivation: 移动应用的能耗问题日益受到关注，研究表明通过提升应用代码质量（如频繁重构）可改善设备能耗，但尚不清楚具体哪些重构对能耗有积极影响。

Method: 对Android应用中若干常见代码坏味道进行重构实验，测量并分析重构前后应用的能耗与性能变化，同时考察不同重构顺序对能耗的影响。

Result: “重复代码”和“类型检查”重构最多可降低10.8%的能耗；能耗显著降低与执行时间变化无直接关系；不同重构顺序对能耗影响较小且方向不一。

Conclusion: 某些代码重构确实能降低Android应用能耗，但其效果复杂且受多种因素影响，未来需深入研究应用规模、年龄、开发者经验等因素与代码坏味道数量及重构能耗影响之间的关联。

Abstract: Energy consumption of mobile apps is a domain that is receiving a lot of
attention from researchers. Recent studies indicate that the energy consumption
of mobile devices could be improved by improving the quality of mobile apps.
Frequent refactoring is one way of achieving this goal. In this paper, we
explore the performance and energy impact of several common code refactorings
in Android apps. Experimental results indicate that some code smell
refactorings positively impact the energy consumption of Android apps.
Refactoring of the code smells "Duplicated code" and "Type checking" reduce
energy consumption by up to 10.8%. Significant reduction in energy consumption,
however, does not seem to be directly related to the increase or decrease of
execution time. In addition, the energy impact over permutations of code smell
refactorings in the selected Android apps was small. When analyzing the order
in which refactorings were made across code smell types, it turned out that
some permutations resulted in a reduction and some in an increase of energy
consumption for the analyzed apps. More research needs to be done to
investigate how factors like size and age of software apps, experience, and
number of contributors to app development correlate with (a) the number and
type of code smells found and (b) the impact of energy consumption and
performance after refactoring.

</details>


### [30] [Agent-based code generation for the Gammapy framework](https://arxiv.org/abs/2509.26110)
*Dmitriy Kostunin,Vladimir Sotnikov,Sergo Golovachev,Abhay Mehta,Tim Lukas Holch,Elisa Jones*

Main category: cs.SE

TL;DR: 本文针对科学计算库Gammapy缺乏文档和社区支持的问题，开发了一个能在受控环境中编写、执行和验证代码的智能体，并提供了网页演示和基准测试套件。


<details>
  <summary>Details</summary>
Motivation: 通用大语言模型在流行框架上表现良好，但在缺乏文档、示例和社区支持的专用科学库（如Gammapy）上效果不佳，且这些库的API常不稳定，训练数据有限或过时。

Method: 开发一个能在受控环境中自动生成、执行和验证Gammapy代码的智能体，并构建配套的网页演示和基准测试套件。

Result: 实现了针对Gammapy的代码生成智能体，提供了最小可行的网页演示和评估基准，展示了当前进展。

Conclusion: 该方法为解决专用科学库中代码生成难题提供了可行路径，后续将在此基础上进一步优化和扩展。

Abstract: Software code generation using Large Language Models (LLMs) is one of the
most successful applications of modern artificial intelligence. Foundational
models are very effective for popular frameworks that benefit from
documentation, examples, and strong community support. In contrast, specialized
scientific libraries often lack these resources and may expose unstable APIs
under active development, making it difficult for models trained on limited or
outdated data. We address these issues for the Gammapy library by developing an
agent capable of writing, executing, and validating code in a controlled
environment. We present a minimal web demo and an accompanying benchmarking
suite. This contribution summarizes the design, reports our current status, and
outlines next steps.

</details>


### [31] [A Multi-Language Object-Oriented Programming Benchmark for Large Language Models](https://arxiv.org/abs/2509.26111)
*Shuai Wang,Liang Ding,Li Shen,Yong Luo,Han Hu,Lefei Zhang,Fu Lin*

Main category: cs.SE

TL;DR: 本文提出了MultiOOP，一个覆盖六种主流语言的多语言面向对象编程基准，揭示了现有代码生成基准在语言多样性、任务粒度和测试用例数量上的严重不足，并通过评估14个主流大语言模型，发现其在多语言OOP任务中存在性能显著下降、跨语言泛化能力弱及对核心OOP概念理解不足等问题。


<details>
  <summary>Details</summary>
Motivation: 现有35个代码生成基准存在三大不平衡：85.7%仅关注单一语言，94.3%仅评估函数或语句级别任务，超80%平均测试用例少于10个，难以全面评估大语言模型在多语言面向对象编程中的能力。

Method: 构建MultiOOP基准，涵盖Python、PHP、C++、C#、Java、JavaScript六种语言，每种语言267个任务；设计翻译器将单语言OOP基准扩展至多语言，并引入pass@o指标；开发自动化测试用例增强框架以提升评估可靠性。

Result: 评估14个主流LLM发现：1）相比函数级任务（如HumanEval），MultiOOP上pass@1分数最多下降65.6个百分点；2）模型在不同语言间表现差异巨大（如GPT-4o mini在Python达48.06%，其他语言仅0.12%-15.26%）；3）pass@o显著低于pass@k（差1.1–19.2分），表明模型常生成可执行但未正确体现OOP概念的代码。

Conclusion: MultiOOP填补了多语言面向对象代码生成评估的空白，揭示了当前LLM在该领域的关键局限，所提出的基准、指标和工具将公开发布，以推动更公平、全面的代码生成能力评估。

Abstract: Establishing fair and robust benchmarks is essential for evaluating
intelligent code generation by large language models (LLMs). Our survey of 35
existing benchmarks uncovers three major imbalances: 85.7% focus on a single
programming language; 94.3% target only function-level or statement-level
tasks; and over 80% include fewer than ten test cases on average. To address
these gaps, we propose MultiOOP, a multi-language object-oriented programming
benchmark covering six popular languages (Python, PHP, C++, C#, Java,
JavaScript) with 267 tasks per language. We design a translator that extends an
existing single-language OOP benchmark and the pass@o metric to a multilingual
setting. Moreover, we propose an automated framework for augmenting test cases
to ensure the reliability of the evaluation results. We evaluate 14 mainstream
LLMs under zero-shot prompting and report three key findings: 1) Substantial
performance degradation: pass@1 scores on MultiOOP drop by up to 65.6
percentage points compared to function-level tasks (e.g., HumanEval). 2)
Cross-language variability: GPT-4o mini achieves pass@1 of 48.06% in Python but
only 0.12%-15.26% in other languages, indicating limited multilingual
generalization. 3) Conceptual gaps: pass@o scores are consistently 1.1-19.2
points lower than pass@k, demonstrating that LLMs often generate executable
code without fully capturing core OOP concepts. Our benchmark, metric
extensions, and evaluation scripts will be publicly released to foster a more
balanced and comprehensive assessment of LLMs in object-oriented code
generation. Our code and data will be released at
https://github.com/alphadl/OOP-eval and
https://huggingface.co/datasets/codeai-dteam/MultiOOP respectively.

</details>


### [32] [Understanding Collective Social Behavior in OSS Communities: A Co-editing Network Analysis of Activity Cascades](https://arxiv.org/abs/2509.26173)
*Lisi Qarkaxhija,Maximilian Carparo,Stefan Menzel,Bernhard Sendhoff,Ingo Scholtes*

Main category: cs.SE

TL;DR: 该论文研究开源软件（OSS）社区中开发者的集体社会行为，发现其提交活动具有“突发性”，并通过构建基于协作编辑网络的模型揭示了活动级联现象。研究发现活动级联在超过一半的项目中具有统计显著性，并据此提出了一种预测开发者流失的实用方法。


<details>
  <summary>Details</summary>
Motivation: 理解开源软件社区中开发者的集体社会行为对于建模和预测社区的长期动态与可持续性至关重要。

Method: 采用基于网络的建模框架，通过协作编辑网络捕捉开发者互动，并开发识别活动级联（即开发者活动在网络中传播）的方法。

Result: 在50个主要OSS社区的数据集中，超过一半的项目中活动级联是统计显著的现象；该发现可用于构建有效的开发者流失预测方法。

Conclusion: 活动级联是理解开源社区中开发者流失与留存的关键机制，揭示了协作软件项目中涌现的集体社会动态。

Abstract: Understanding the collective social behavior of software developers is
crucial to model and predict the long-term dynamics and sustainability of Open
Source Software (OSS) communities. To this end, we analyze temporal activity
patterns of developers, revealing an inherently ``bursty'' nature of commit
contributions. To investigate the social mechanisms behind this phenomenon, we
adopt a network-based modelling framework that captures developer interactions
through co-editing networks. Our framework models social interactions, where a
developer editing the code of other developers triggers accelerated activity
among collaborators. Using a large data set on 50 major OSS communities, we
further develop a method that identifies activity cascades, i.e. the
propagation of developer activity in the underlying co-editing network. Our
results suggest that activity cascades are a statistically significant
phenomenon in more than half of the studied projects. We further show that our
insights can be used to develop a simple yet practical churn prediction method
that forecasts which developers are likely to leave a project. Our work sheds
light on the emergent collective social dynamics in OSS communities and
highlights the importance of activity cascades to understand developer churn
and retention in collaborative software projects.

</details>


### [33] [Hamster: A Large-Scale Study and Characterization of Developer-Written Tests](https://arxiv.org/abs/2509.26204)
*Rangeet Pan,Tyler Stennett,Raju Pavuluri,Nate Levin,Alessandro Orso,Saurabh Sinha*

Main category: cs.SE

TL;DR: 本文通过对170万条Java开发者编写的测试用例进行实证研究，揭示了当前自动化测试生成（ATG）工具在生成真实、具有代表性的测试方面与开发者实践之间存在显著差距，并据此提出了未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 尽管自动化测试生成（ATG）已有大量研究，但学界对开发者编写测试的特征理解不足，导致难以评估现有ATG工具生成测试的真实性和代表性。

Method: 对来自开源仓库的170万条Java开发者测试用例进行大规模实证分析，考察测试范围、测试夹具、断言、输入类型和模拟使用等被忽视的维度，并与两种前沿ATG工具生成的测试进行对比。

Result: 大多数开发者编写的测试具备当前ATG工具无法生成的特性，表明现有工具在模拟真实测试实践方面存在明显不足。

Conclusion: 基于研究发现，作者提出了若干有前景的研究方向，旨在缩小ATG工具能力与开发者实际测试需求之间的差距，推动该领域发展。

Abstract: Automated test generation (ATG), which aims to reduce the cost of manual test
suite development, has been investigated for decades and has produced countless
techniques based on a variety of approaches: symbolic analysis, search-based,
random and adaptive-random, learning-based, and, most recently,
large-language-model-based approaches. However, despite this large body of
research, there is still a gap in our understanding of the characteristics of
developer-written tests and, consequently, in our assessment of how well ATG
techniques and tools can generate realistic and representative tests. To bridge
this gap, we conducted an extensive empirical study of developer-written tests
for Java applications, covering 1.7 million test cases from open-source
repositories. Our study is the first of its kind in studying aspects of
developer-written tests that are mostly neglected in the existing literature,
such as test scope, test fixtures and assertions, types of inputs, and use of
mocking. Based on the characterization, we then compare existing tests with
those generated by two state-of-the-art ATG tools. Our results highlight that a
vast majority of developer-written tests exhibit characteristics that are
beyond the capabilities of current ATG tools. Finally, based on the insights
gained from the study, we identify promising research directions that can help
bridge the gap between current tool capabilities and more effective tool
support for developer testing practices. We hope that this work can set the
stage for new advances in the field and bring ATG tools closer to generating
the types of tests developers write.

</details>


### [34] [UniSage: A Unified and Post-Analysis-Aware Sampling for Microservices](https://arxiv.org/abs/2509.26336)
*Zhouruixing Zhu,Zhihan Jiang,Tianyi Yang,Pinjia He*

Main category: cs.SE

TL;DR: UniSage 是一个统一的 traces 和 logs 采样框架，采用“分析后采样”范式，在完整数据流上先进行轻量级多模态异常检测与根因分析（RCA），再基于分析结果指导双支柱采样策略，显著提升关键信号捕获率与下游 RCA 准确性，同时具备高效性。


<details>
  <summary>Details</summary>
Motivation: 现有采样方法多采用“先采样后分析”范式，即使基于启发式策略，仍会丢弃与故障相关的信息，影响系统行为诊断的透明性和准确性。

Method: UniSage 首先对完整数据流执行轻量级、多模态的异常检测与 RCA，获得细粒度服务级诊断信息；随后采用双支柱采样策略：一是分析引导采样器，优先保留 RCA 指出的关键数据；二是边缘案例采样器，确保捕获罕见但关键的行为。

Result: 在 2.5% 采样率下，UniSage 捕获了 56.5% 的关键 traces 和 96.25% 的相关 logs，并将下游 RCA 的准确率（AC@1）提升了 42.45%；其流水线可在 5 秒内处理 10 分钟遥测数据，适用于生产环境。

Conclusion: UniSage 通过后分析感知的统一采样框架，在显著降低存储开销的同时，有效保留了诊断所需的关键信息，提升了故障诊断的准确性和效率。

Abstract: Traces and logs are essential for observability and fault diagnosis in modern
distributed systems. However, their ever-growing volume introduces substantial
storage overhead and complicates troubleshooting. Existing approaches typically
adopt a sample-before-analysis paradigm: even when guided by data heuristics,
they inevitably discard failure-related information and hinder transparency in
diagnosing system behavior. To address this, we introduce UniSage, the first
unified framework to sample both traces and logs using a post-analysis-aware
paradigm. Instead of discarding data upfront, UniSagefirst performs lightweight
and multi-modal anomaly detection and root cause analysis (RCA) on the complete
data stream. This process yields fine-grained, service-level diagnostic
insights that guide a dual-pillar sampling strategy for handling both normal
and anomalous scenarios: an analysis-guided sampler prioritizes data implicated
by RCA, while an edge-case-based sampler ensures rare but critical behaviors
are captured. Together, these pillars ensure comprehensive coverage of critical
signals without excessive redundancy. Extensive experiments demonstrate that
UniSage significantly outperforms state-of-the-art baselines. At a 2.5%
sampling rate, it captures 56.5% of critical traces and 96.25% of relevant
logs, while improving the accuracy (AC@1) of downstream root cause analysis by
42.45%. Furthermore, its efficient pipeline processes 10 minutes of telemetry
data in under 5 seconds, demonstrating its practicality for production
environments.

</details>


### [35] [Institutional Policy Pathways for Supporting Research Software: Global Trends and Local Practices](https://arxiv.org/abs/2509.26422)
*Michelle Barker,Jeremy Cohen,Pedro Hernández Serrano,Daniel S. Katz,Kim Martin,Dan Rudmann,Hugh Shanahan*

Main category: cs.SE

TL;DR: 本文介绍了PRO4RS工作组的工作，该工作组由ReSA和RDA联合发起，旨在推动全球研究机构制定和完善研究软件相关政策，特别强调在科研评估改革中应重视研究软件人员的角色。


<details>
  <summary>Details</summary>
Motivation: 研究软件在现代科研中日益重要，但研究机构在软件人员管理、培训、认可与激励机制方面发展滞后，缺乏系统性政策支持，阻碍了软件可持续性和科研质量提升。

Method: 通过PRO4RS工作组对全球研究机构的研究软件政策进行调研与分析，识别政策空白，特别是科研评估改革中对研究软件人员的忽视问题。

Result: 揭示了当前研究机构在研究软件政策方面的不足，特别是在人员认可与长期支持方面，并提出了制定健全政策的必要性。

Conclusion: 研究机构亟需制定全面的研究软件政策，将软件人员纳入科研评估体系，以支持软件可持续发展并提升整体科研能力与声誉。

Abstract: As research software becomes increasingly central to modern science,
research-performing organisations (RPOs) need to ensure that their investment
in people, skills and infrastructure around research software produces
sustainable and maintainable software that improves the research they perform,
which in turn improves the overall institution and its reputation and funding,
for example, by competing with peers who lack this approach. However, research
institution management and recognition of research software and its personnel
has mostly often developed in an ad hoc manner. RPO training infrastructures,
recognition and reward structures, have not developed at a sufficient rate to
support and encourage both the widespread use of research software best
practices and the long-term support for technical roles that is required. To
begin to address this fundamental problem for modern research environments,
RPOs must implement and adopt robust policies to support research software
development, use, and sustainability. Despite growing momentum from funders and
publishers around FAIR and open science principles, research
institutional-level policies specifically addressing research software remain
limited or lacking in breadth.
  This article outlines the work of the Policies in Research Organisations for
Research Software (PRO4RS) Working Group (WG), a joint initiative of the
Research Software Alliance (ReSA) and the Research Data Alliance (RDA), which
examined and advanced research software policy development across institutions
worldwide. After consideration of the rationale for institutional policies on
research software, the PRO4RS WG outputs and analysis are utilised to highlight
critical policy gaps, particularly related to consideration of research
software personnel in policy work focused on reform of research assessment.

</details>


### [36] [EQ-Robin: Generating Multiple Minimal Unique-Cause MC/DC Test Suites](https://arxiv.org/abs/2509.26458)
*Robin Lee,Youngho Nam*

Main category: cs.SE

TL;DR: 本文提出EQ-Robin方法，通过生成语义等价但结构不同的布尔表达式变体，并对每个变体应用Robin's Rule算法，从而系统性地构建多个满足Unique-Cause MC/DC最小测试用例数（N+1）的测试套件，以应对因系统约束导致某些测试用例非法而无法达成100%覆盖的问题。


<details>
  <summary>Details</summary>
Motivation: 现有Robin's Rule算法虽能为单一布尔表达式生成最小测试套件，但仅产生一个测试套件，若其中关键测试用例因系统约束而非法，则无法实现100% MC/DC覆盖。因此需要一种能生成多个最小测试套件的方法以提升鲁棒性。

Method: 将布尔表达式转换为抽象语法树（AST），通过代数重排生成多个语义等价但结构不同的表达式变体，再对每个变体应用Robin's Rule算法，从而系统性地生成一族最小Unique-Cause MC/DC测试套件。

Result: 该方法能够生成多样化的最小测试套件集合，在保持N+1最小测试用例数量的同时，提高在存在系统约束条件下找到合法且满足100% MC/DC覆盖的测试套件的可能性。

Conclusion: EQ-Robin为安全关键系统中实现鲁棒且最小的MC/DC覆盖提供了一种实用且轻量级的解决方案，尤其适用于存在输入约束的实际场景。

Abstract: Modified Condition/Decision Coverage (MC/DC), particularly its strict
Unique-Cause form, is a cornerstone of safety-critical software verification. A
recent algorithm, "Robin's Rule," introduced a deterministic method to
construct the theoretical minimum of N+1 test cases for Singular Boolean
Expressions (SBEs). However, this approach yields only a single test suite,
introducing a critical risk: if a test case forming a required 'independence
pair' is an illegal input forbidden by system constraints, the suite fails to
achieve 100% coverage. This paper proposes EQ-Robin, a lightweight pipeline
that systematically generates a family of minimal Unique-Cause MC/DC suites to
mitigate this risk. We introduce a method for systematically generating
semantically equivalent SBEs by applying algebraic rearrangements to an
Abstract Syntax Tree (AST) representation of the expression. By applying
Robin's Rule to each structural variant, a diverse set of test suites can be
produced. This provides a resilient path to discovering a valid test suite that
preserves the N+1 minimality guarantee while navigating real-world constraints.
We outline an evaluation plan on TCAS-II-derived SBEs to demonstrate how
EQ-Robin offers a practical solution for ensuring robust MC/DC coverage.

</details>


### [37] [ErrorPrism: Reconstructing Error Propagation Paths in Cloud Service Systems](https://arxiv.org/abs/2509.26463)
*Junsong Pu,Yichen Li,Zhuangbin Chen,Jinyang Liu,Zhihan Jiang,Jianjun Chen,Rui Shi,Zibin Zheng,Tieying Zhang*

Main category: cs.SE

TL;DR: 本文提出了ErrorPrism，一种用于在微服务系统中自动重构错误传播路径的工具，结合静态分析与大语言模型（LLM）代理，实现了高精度的根因分析。


<details>
  <summary>Details</summary>
Motivation: 现代微服务系统中错误包装（error wrapping）虽然增强了错误上下文信息，但也导致从最终日志回溯完整错误传播路径变得困难，现有方法难以有效解决这一可追溯性问题。

Method: ErrorPrism首先对服务代码库进行静态分析，构建函数调用图并将日志字符串映射到候选函数，缩小搜索空间；然后利用LLM代理进行迭代式反向搜索，重构多跳错误传播路径。

Result: 在字节跳动67个生产微服务上评估，ErrorPrism对102个真实错误的路径重构准确率达97.0%，优于现有静态分析和基于LLM的方法。

Conclusion: ErrorPrism为工业级微服务系统提供了一种高效、实用的错误传播路径重构与根因分析工具。

Abstract: Reliability management in cloud service systems is challenging due to the
cascading effect of failures. Error wrapping, a practice prevalent in modern
microservice development, enriches errors with context at each layer of the
function call stack, constructing an error chain that describes a failure from
its technical origin to its business impact. However, this also presents a
significant traceability problem when recovering the complete error propagation
path from the final log message back to its source. Existing approaches are
ineffective at addressing this problem. To fill this gap, we present ErrorPrism
in this work for automated reconstruction of error propagation paths in
production microservice systems. ErrorPrism first performs static analysis on
service code repositories to build a function call graph and map log strings to
relevant candidate functions. This significantly reduces the path search space
for subsequent analysis. Then, ErrorPrism employs an LLM agent to perform an
iterative backward search to accurately reconstruct the complete, multi-hop
error path. Evaluated on 67 production microservices at ByteDance, ErrorPrism
achieves 97.0% accuracy in reconstructing paths for 102 real-world errors,
outperforming existing static analysis and LLM-based approaches. ErrorPrism
provides an effective and practical tool for root cause analysis in industrial
microservice systems.

</details>


### [38] [Towards Verified Code Reasoning by LLMs](https://arxiv.org/abs/2509.26546)
*Meghana Sistla,Gogul Balakrishnan,Pat Rondon,José Cambronero,Michele Tufano,Satish Chandra*

Main category: cs.SE

TL;DR: 本文提出一种通过形式化验证自动校验大语言模型代码推理答案正确性的方法，在两类任务上验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在代码推理任务中虽能力强，但答案不一定正确，导致在高精度需求场景（如代码理解、代码审查、自动生成代码验证）中难以信任，需人工验证，影响开发效率。

Method: 提取代码推理代理回答的形式化表示，并利用形式化验证与程序分析工具自动验证其推理步骤。

Result: 在20个未初始化变量错误中验证了13个推理正确性，在20个程序等价性查询中成功识别出8个错误判断中的6个。

Conclusion: 通过形式化验证可有效提升代码推理代理答案的可信度，减少人工验证负担，增强其在实际开发中的辅助价值。

Abstract: While LLM-based agents are able to tackle a wide variety of code reasoning
questions, the answers are not always correct. This prevents the agent from
being useful in situations where high precision is desired: (1) helping a
software engineer understand a new code base, (2) helping a software engineer
during code review sessions, and (3) ensuring that the code generated by an
automated code generation system meets certain requirements (e.g. fixes a bug,
improves readability, implements a feature).
  As a result of this lack of trustworthiness, the agent's answers need to be
manually verified before they can be trusted. Manually confirming responses
from a code reasoning agent requires human effort and can result in slower
developer productivity, which weakens the assistance benefits of the agent. In
this paper, we describe a method to automatically validate the answers provided
by a code reasoning agent by verifying its reasoning steps. At a very high
level, the method consists of extracting a formal representation of the agent's
response and, subsequently, using formal verification and program analysis
tools to verify the agent's reasoning steps.
  We applied this approach to a benchmark set of 20 uninitialized variable
errors detected by sanitizers and 20 program equivalence queries. For the
uninitialized variable errors, the formal verification step was able to
validate the agent's reasoning on 13/20 examples, and for the program
equivalence queries, the formal verification step successfully caught 6/8
incorrect judgments made by the agent.

</details>


### [39] [Black-box Context-free Grammar Inference for Readable & Natural Grammars](https://arxiv.org/abs/2509.26616)
*Mohammad Rifat Arefin,Shanto Rahman,Christoph Csallner*

Main category: cs.SE

TL;DR: NatGI 是一种新颖的 LLM 引导的上下文无关文法推断框架，通过括号引导的 bubble 探索、LLM 驱动的 bubble 生成与非终结符命名，以及层次化 delta 调试（HDD）显著提升了文法推断的准确性与可解释性，在多个语言基准上 F1 分数平均领先最强基线 25 个百分点。


<details>
  <summary>Details</summary>
Motivation: 现有文法推断工具（如 Arvada、TreeVada 和 Kedavra）在处理大型复杂语言时面临可扩展性、可读性和准确性不足的问题，亟需一种更高效、可解释且准确的黑盒文法推断方法。

Method: NatGI 在 TreeVada 的解析树恢复基础上引入三项创新：1）利用括号等语法线索进行结构良好的文法片段探索；2）借助大语言模型生成 bubble 并为非终结符赋予语义化名称；3）采用层次化 delta 调试（HDD）逐步简化文法规则以提升紧凑性与可读性。

Result: 在包括 Lua、C 和 MySQL 在内的多种语言基准测试中，NatGI 平均 F1 分数达 0.57，比最佳基线 TreeVada 高出 25 个百分点，同时生成的文法在可解释性方面显著优于现有方法。

Conclusion: NatGI 通过结合 LLM 与结构化探索策略，有效解决了现有文法推断工具在准确性与可解释性之间的权衡问题，为程序分析、逆向工程和安全研究提供了更可靠、易理解的文法推断方案。

Abstract: Black-box context-free grammar inference is crucial for program analysis,
reverse engineering, and security, yet existing tools such as Arvada, TreeVada,
and Kedavra struggle with scalability, readability, and accuracy on large,
complex languages. We present NatGI, a novel LLM-guided grammar inference
framework that extends TreeVada's parse tree recovery with three key
innovations: bracket-guided bubble exploration, LLM-driven bubble generation
and non-terminal labeling, and hierarchical delta debugging (HDD) for
systematic tree simplification. Bracket-guided exploration leverages syntactic
cues such as parentheses to propose well-structured grammar fragments, while
LLM guidance produces meaningful non-terminal names and selects more promising
merges. Finally, HDD incrementally reduces unnecessary rules, which makes the
grammars both compact and interpretable. In our experiments, we evaluate NatGI
on a comprehensive benchmark suite ranging from small languages to larger ones
such as lua, c, and mysql. Our results show that NatGI consistently outperforms
strong baselines in terms of F1 score. On average, NatGI achieves an F1 score
of 0.57, which is 25pp (percentage points) higher than the best-performing
baseline, TreeVada. In the case of interpretability, our generated grammars
perform significantly better than those produced by existing approaches.
Leveraging LLM-based node renaming and bubble exploration, NatGI produces rules
with meaningful non-terminal names and compact structures that align more
closely with human intuition. As a result, developers and researchers can
achieve higher accuracy while still being able to easily inspect, verify, and
reason about the structure and semantics of the induced grammars.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [40] [smallNet: Implementation of a convolutional layer in tiny FPGAs](https://arxiv.org/abs/2509.25391)
*Fernanda Zapata Bascuñán,Alan Ezequiel Fuster*

Main category: cs.AR

TL;DR: 本文提出了一种完全用Verilog实现的卷积层设计（smallNet），无需依赖IP核，适用于低成本FPGA、SoM、SoC和ASIC，并在Cora Z7单核平台上验证了其在资源受限嵌入式系统中实现实时应用的可行性。


<details>
  <summary>Details</summary>
Motivation: 当前Xilinx和VLSI中的神经网络开发系统需与Python库协同开发，限制了在资源受限硬件上的部署灵活性，因此需要一种不依赖IP核、可直接部署的硬件原生实现方案。

Method: 通过手写Verilog代码实现一个基于滤波器多项式结构的卷积层（smallNet），不使用任何IP核，并在Cora Z7单核平台上进行部署与验证。

Result: 该设计实现了5.1倍的速度提升，分类准确率超过81%，总功耗仅为1.5W，验证了其在嵌入式实时应用中的高效性与可行性。

Conclusion: 所提出的Verilog原生卷积层设计为资源受限的嵌入式平台提供了一种高效、低功耗且无需依赖软件库的神经网络部署方案。

Abstract: Since current neural network development systems in Xilinx and VLSI require
codevelopment with Python libraries, the first stage of a convolutional network
has been implemented by developing a convolutional layer entirely in Verilog.
This handcoded design, free of IP cores and based on a filter polynomial like
structure, enables straightforward deployment not only on low cost FPGAs but
also on SoMs, SoCs, and ASICs. We analyze the limitations of numerical
representations and compare our implemented architecture, smallNet, with its
computer based counterpart, demonstrating a 5.1x speedup, over 81%
classification accuracy, and a total power consumption of just 1.5 W. The
algorithm is validated on a single-core Cora Z7, demonstrating its feasibility
for real time, resource-constrained embedded applications.

</details>


### [41] [LLM-Powered Code Analysis and Optimization for Gaussian Splatting Kernels](https://arxiv.org/abs/2509.25626)
*Yi Hu,Huiyang Zhou*

Main category: cs.AR

TL;DR: 本文首次利用大语言模型（LLMs）优化3D高斯泼溅（3DGS）的GPU内核代码，在多个数据集上实现最高42%的性能提升，并展示了LLMs与领域专家协作优化的潜力。


<details>
  <summary>Details</summary>
Motivation: 3D高斯泼溅（3DGS）在新视角合成和实时渲染中具有重要意义，但其性能优化面临GPU架构复杂性和调参空间庞大的挑战。手动优化虽有效，但耗时、易错且依赖专业知识，因此作者探索使用大语言模型自动优化3DGS内核。

Method: 作者利用大语言模型（如Deepseek和GPT-5）分析并优化3DGS的GPU内核代码，结合性能分析工具提供的额外信息，探索LLM在真实世界高性能计算任务中的优化能力，并提出与LLM协作优化的策略。

Result: 在MipNeRF360数据集上，LLM优化使原始3DGS代码提速19%（Deepseek）至24%（GPT-5）；结合性能分析信息后，平均提速达38%，最高达42%。即使在已优化的新框架Seele上，LLM仍能带来6%的额外提升。手动优化最高可达48%，表明当前LLM尚不能覆盖全部优化空间。

Conclusion: 大语言模型在优化高度专业化的GPU内核方面展现出显著潜力，虽尚未超越顶尖人工优化，但能发现专家遗漏的优化机会，凸显了人机协作在高性能计算优化中的价值。

Abstract: 3D Gaussian splatting (3DGS) is a transformative technique with profound
implications on novel view synthesis and real-time rendering. Given its
importance, there have been many attempts to improve its performance. However,
with the increasing complexity of GPU architectures and the vast search space
of performance-tuning parameters, it is a challenging task. Although manual
optimizations have achieved remarkable speedups, they require domain expertise
and the optimization process can be highly time consuming and error prone. In
this paper, we propose to exploit large language models (LLMs) to analyze and
optimize Gaussian splatting kernels. To our knowledge, this is the first work
to use LLMs to optimize highly specialized real-world GPU kernels. We reveal
the intricacies of using LLMs for code optimization and analyze the code
optimization techniques from the LLMs. We also propose ways to collaborate with
LLMs to further leverage their capabilities. For the original 3DGS code on the
MipNeRF360 datasets, LLMs achieve significant speedups, 19% with Deepseek and
24% with GPT-5, demonstrating the different capabilities of different LLMs. By
feeding additional information from performance profilers, the performance
improvement from LLM-optimized code is enhanced to up to 42% and 38% on
average. In comparison, our best-effort manually optimized version can achieve
a performance improvement up to 48% and 39% on average, showing that there are
still optimizations beyond the capabilities of current LLMs. On the other hand,
even upon a newly proposed 3DGS framework with algorithmic optimizations,
Seele, LLMs can still further enhance its performance by 6%, showing that there
are optimization opportunities missed by domain experts. This highlights the
potential of collaboration between domain experts and LLMs.

</details>


### [42] [SAIL: SRAM-Accelerated LLM Inference System with Lookup-Table-based GEMV](https://arxiv.org/abs/2509.25853)
*Jingyao Zhang,Jaewoo Park,Jongeun Lee,Elaheh Sadredini*

Main category: cs.AR

TL;DR: 本文提出SAIL，一种基于CPU的高效大语言模型推理方案，通过SRAM存内计算支持任意比特精度，显著提升推理速度与成本效益。


<details>
  <summary>Details</summary>
Motivation: CPU在大语言模型推理中具有普及性优势，但面临低精度计算支持不足和生成阶段内存瓶颈两大挑战。

Method: SAIL结合三项创新：基于SRAM的查表法通用矩阵-向量乘（LUT-GEMV）、感知输入激活模式冗余的LUT优化，以及利用存内计算并行性的类型转换算法。

Result: 实验表明，SAIL相比ARM Neoverse-N1 CPU基线最高实现10.7倍加速和19.9倍每美元生成token数提升，成本效率优于NVIDIA V100 GPU达7.04倍。

Conclusion: SAIL以极低硬件开销（仅2%）和单一新指令，为高效CPU大语言模型推理提供了可行路径。

Abstract: Large Language Model (LLM) inference requires substantial computational
resources, yet CPU-based inference remains essential for democratizing AI due
to the widespread availability of CPUs compared to specialized accelerators.
However, efficient LLM inference on CPUs faces two fundamental challenges: (1)
existing CPU architectures struggle with low-precision arithmetic required by
quantized models, where optimal bit precision varies across models and layers;
and (2) the memory-bound nature of the token generation phase creates severe
performance bottlenecks. To address these challenges, we propose SAIL
(SRAM-Accelerated Inference of LLMs), a CPU-based inference solution that
efficiently supports arbitrary bit precisions with minimal overhead. SAIL
integrates three key innovations: First, we introduce Batched LUT-based General
Matrix-Vector Multiplication (LUT-GEMV) with SRAM-based processing-in-memory,
enabling high data reuse through lookup tables and reducing memory movement.
Second, our Pattern-Aware LUT optimization identifies and exploits redundancy
in input activation patterns, reducing computation cycles by 13.8\%. Third, we
develop an in-memory type conversion algorithm that leverages PIM's parallelism
for efficient de-/quantization operations, alleviating pressure on CPU's vector
units. Our architecture requires only 2\% hardware overhead and a single new
instruction, while maintaining dual functionality as both compute and storage
units. Experimental evaluations using a modified gem5 simulator demonstrate
that SAIL achieves up to 10.7x speedup and 19.9x higher tokens per dollar
compared to ARM Neoverse-N1 CPU baselines, and up to 7.04x better cost
efficiency than NVIDIA V100 GPUs, establishing a practical path for efficient
CPU-based LLM inference.

</details>


### [43] [Runtime Energy Monitoring for RISC-V Soft-Cores](https://arxiv.org/abs/2509.26065)
*Alberto Scionti,Paolo Savio,Francesco Lubrano,Olivier Terzo,Marco Ferretti,Florin Apopei,Juri Bellucci,Ennio Spano,Luca Carriere*

Main category: cs.AR

TL;DR: 本文提出了一种无需复杂架构模型即可在运行时监控计算系统能耗的整体方法，该方法基于测量板与FPGA系统模块结合，可扩展至多节点集群，并计划用于航空设计中RISC-V软核上浅层神经网络的能效优化。


<details>
  <summary>Details</summary>
Motivation: 在计算系统设计中，能耗监控至关重要，但传统依赖软件工具和详细架构模型的方法在设计空间探索时需频繁调整模型，效率低下。

Method: 采用一个测量板与FPGA系统模块集成，实时采集电流和电压（最多数十个测量点），并通过特定内存区域暴露数据；运行中的服务读取并计算能耗统计，不占用FPGA额外资源。

Result: 该方法实现了无需复杂模型的运行时能耗监控，且具备良好的可扩展性，适用于多节点集群。

Conclusion: 所提出的框架有效解决了传统能耗建模的局限性，为在RISC-V软核上优化浅层神经网络的性能与能效提供了可行实验平台。

Abstract: Energy efficiency is one of the major concern in designing advanced computing
infrastructures. From single nodes to large-scale systems (data centers),
monitoring the energy consumption of the computing system when applications run
is a critical task. Designers and application developers often rely on software
tools and detailed architectural models to extract meaningful information and
determine the system energy consumption. However, when a design space
exploration is required, designers may incur in continuous tuning of the models
to match with the system under evaluation. To overcome such limitations, we
propose a holistic approach to monitor energy consumption at runtime without
the need of running complex (micro-)architectural models. Our approach is based
on a measurement board coupled with a FPGA-based System-on-Module. The
measuring board captures currents and voltages (up to tens measuring points)
driving the FPGA and exposes such values through a specific memory region. A
running service reads and computes energy consumption statistics without
consuming extra resources on the FPGA device. Our approach is also scalable to
monitoring of multi-nodes infrastructures (clusters). We aim to leverage this
framework to perform experiments in the context of an aeronautical design
application; specifically, we will look at optimizing performance and energy
consumption of a shallow artificial neural network on RISC-V based soft-cores.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [44] [Permuting Transactions in Ethereum Blocks: An Empirical Study](https://arxiv.org/abs/2509.25415)
*Jan Droll*

Main category: cs.DC

TL;DR: 本文通过重排并执行33.5万多个以太坊主网区块的交易，评估随机交易排序在技术上的可行性，发现约22%的区块重排因协议违规而无效，但大多数区块的Gas消耗变化不超过10%，表明在谨慎选择交易的前提下，随机排序是可行的。


<details>
  <summary>Details</summary>
Motivation: 探讨随机化区块内交易顺序是否可在不违反协议规则和Gas限制的前提下部署，以缓解以太坊生态系统中的中心化问题并提升公平性。

Method: 对335,000多个以太坊主网区块的交易进行多次随机重排与执行，统计协议违规、执行错误及Gas消耗偏差的情况。

Result: 约22%的区块重排因协议违规无效；几乎所有在重排中出错但在原始顺序中正常的交易均为私挖交易；仅6%的交易出现Gas消耗偏差，98%的区块重排Gas消耗偏差不超过10%。

Conclusion: 从技术角度看，只要谨慎处理交易选择，随机交易排序在以太坊中是可行的。

Abstract: Several recent proposals implicitly or explicitly suggest making use of
randomized transaction ordering within a block to mitigate centralization
effects and to improve fairness in the Ethereum ecosystem. However,
transactions and blocks are subject to gas limits and protocol rules. In a
randomized transaction order, the behavior of transactions may change depending
on other transactions in the same block, leading to invalid blocks and varying
gas consumptions. In this paper, we quantify and characterize protocol
violations, execution errors and deviations in gas consumption of blocks and
transactions to examine technical deployability. For that, we permute and
execute the transactions of over 335,000 Ethereum Mainnet blocks multiple
times. About 22% of block permutations are invalid due to protocol violations
caused by privately mined transactions or blocks close to their gas limit.
Also, almost all transactions which show execution errors under permutation but
not in the original order are privately mined transactions. Only 6% of
transactions show deviations in gas consumption and 98% of block permutations
deviate at most 10% from their original gas consumption. From a technical
perspective, these results suggest that randomized transaction ordering may be
feasible if transaction selection is handled carefully.

</details>


### [45] [Enhancing Split Learning with Sharded and Blockchain-Enabled SplitFed Approaches](https://arxiv.org/abs/2509.25555)
*Amirreza Sokhankhosh,Khalid Hassan,Sara Rouhani*

Main category: cs.DC

TL;DR: 本文提出两种新框架：Sharded SplitFed Learning（SSFL）和Blockchain-enabled SplitFed Learning（BSFL），分别通过分片和区块链技术解决SplitFed Learning在可扩展性、性能和安全性方面的不足。实验表明SSFL显著提升性能与可扩展性，BSFL则有效增强对数据投毒攻击的鲁棒性，并首次实现端到端去中心化的SplitFed系统。


<details>
  <summary>Details</summary>
Motivation: SplitFed Learning（SFL）虽结合了联邦学习（FL）与拆分学习（SL）的优点，但仍继承了SL在可扩展性、性能和安全性方面的缺陷，亟需改进。

Method: 提出SSFL框架，通过将SL服务器的负载和通信开销分布到多个并行分片上，提升可扩展性与性能；在此基础上构建BSFL框架，用基于区块链的架构替代中心化服务器，采用委员会共识机制增强公平性与安全性，并引入评估机制剔除被污染或篡改的模型更新。

Result: 实验显示，相比基线SL和SFL方法，SSFL在性能和可扩展性上分别提升31.2%和85.2%；BSFL在正常条件下保持优越性能，同时对数据投毒攻击的抵御能力提升62.7%。

Conclusion: 所提出的SSFL和BSFL框架有效克服了SplitFed Learning的关键局限，其中BSFL是首个实现端到端去中心化SplitFed学习的区块链赋能框架，在性能、可扩展性和安全性方面均取得显著进展。

Abstract: Collaborative and distributed learning techniques, such as Federated Learning
(FL) and Split Learning (SL), hold significant promise for leveraging sensitive
data in privacy-critical domains. However, FL and SL suffer from key
limitations -- FL imposes substantial computational demands on clients, while
SL leads to prolonged training times. To overcome these challenges, SplitFed
Learning (SFL) was introduced as a hybrid approach that combines the strengths
of FL and SL. Despite its advantages, SFL inherits scalability, performance,
and security issues from SL. In this paper, we propose two novel frameworks:
Sharded SplitFed Learning (SSFL) and Blockchain-enabled SplitFed Learning
(BSFL). SSFL addresses the scalability and performance constraints of SFL by
distributing the workload and communication overhead of the SL server across
multiple parallel shards. Building upon SSFL, BSFL replaces the centralized
server with a blockchain-based architecture that employs a committee-driven
consensus mechanism to enhance fairness and security. BSFL incorporates an
evaluation mechanism to exclude poisoned or tampered model updates, thereby
mitigating data poisoning and model integrity attacks. Experimental evaluations
against baseline SL and SFL approaches show that SSFL improves performance and
scalability by 31.2% and 85.2%, respectively. Furthermore, BSFL increases
resilience to data poisoning attacks by 62.7% while maintaining superior
performance under normal operating conditions. To the best of our knowledge,
BSFL is the first blockchain-enabled framework to implement an end-to-end
decentralized SplitFed Learning system.

</details>


### [46] [LAPIS: A Performance Portable, High Productivity Compiler Framework](https://arxiv.org/abs/2509.25605)
*Brian Kelley,Sivasankaran Rajamanickam*

Main category: cs.DC

TL;DR: LAPIS 是一个基于 MLIR 的编译器，旨在同时提升编程模型在可移植性、性能和生产力三方面的表现，支持自动降低稀疏与稠密线性代数核，并实现 PyTorch 与 Kokkos 代码的集成，同时具备良好的架构可扩展性。


<details>
  <summary>Details</summary>
Motivation: 当前编程模型在可移植性、性能和生产力三者之间难以兼顾：科学计算模型偏重性能与可移植性，而机器学习模型偏重可移植性与生产力；此外，科学计算与机器学习框架之间缺乏互操作性，且多数框架难以适应新硬件架构。

Method: 提出 LAPIS 编译器，基于 MLIR 构建，设计融合 Kokkos 生态原则的新方言，实现对稀疏/稠密线性代数核的自动降级，并支持 PyTorch 与 Kokkos 代码的自动集成。

Result: LAPIS 在多种架构上展现出良好的可移植性，性能优于默认 MLIR 实现，并成功实现跨框架（PyTorch/Kokkos）代码集成，同时支持通过新方言扩展至新硬件架构。

Conclusion: LAPIS 有效解决了编程模型在可移植性、性能和生产力三者难以兼顾的问题，为科学计算与机器学习融合场景提供了统一、可扩展且高效的编译基础设施。

Abstract: Portability, performance, and productivity are three critical dimensions for
evaluating a programming model or compiler infrastructure. Several modern
programming models for computational science focus on performance and
portability. On the other end, several machine learning focused programming
models focus on portability and productivity. A clear solution that is strong
in all three dimensions has yet to emerge. A second related problem arises when
use cases from computational science converge with machine learning. The
disparate popular frameworks of these fields require programmers to manually
integrate codes written in different frameworks. Finally, several programming
frameworks lack easy options for extensibility as any new computer architecture
change require complex changes to the programming models. We present LAPIS, an
MLIR-based compiler that addresses all three of these challenges. We
demonstrate that LAPIS can automatically lower sparse and dense linear algebra
kernels from computational science and artificial intelligence use cases. We
also show how LAPIS facilitates the integration of codes between PyTorch and
Kokkos. We compare kernel performance with the default MLIR implementations on
diverse architectures to demonstrate portability. By developing a dialect that
is built on the principles of the Kokkos ecosystem, LAPIS also allows
extensibility of the framework to new architectures.

</details>


### [47] [PAST: Pilot and Adaptive Orchestration for Timely and Resilient Service Delivery in Edge-Assisted UAV Networks under Spatio-Temporal Dynamics](https://arxiv.org/abs/2509.25700)
*Houyi Qi,Minghui Liwang,Liqun Fu,Sai Zou,Xinlei Yi,Wei Ni,Huaiyu Dai*

Main category: cs.DC

TL;DR: 本文提出PAST框架，结合PilotAO和AdaptAO两种机制，在无人机与边缘网络间实现兼顾稳定性与灵活性的激励型资源交易，显著提升任务完成效率、资源利用率和社会福利。


<details>
  <summary>Details</summary>
Motivation: 传统现货交易存在协商延迟和高能耗问题，而传统期货交易难以适应无人机-边缘环境中动态不确定的特性，亟需一种兼顾稳定性与适应性的资源交易机制。

Method: 提出PAST框架，包含两个互补机制：PilotAO通过带超订的试点交易协议进行风险感知的早期决策，建立长期互惠协议；AdaptAO则根据无人机移动性、供需变化和协议表现动态调整协议与超订率。

Result: 在真实数据集上的实验表明，PAST在决策开销、任务完成延迟、资源利用率和社会福利方面均优于现有基准方法。

Conclusion: PAST通过结合预测性规划与实时调整，为提升低空任务性能提供了一种鲁棒且自适应的资源交易实践范式。

Abstract: Incentive-driven resource trading is essential for UAV applications with
intensive, time-sensitive computing demands. Traditional spot trading suffers
from negotiation delays and high energy costs, while conventional futures
trading struggles to adapt to the dynamic, uncertain UAV-edge environment. To
address these challenges, we propose PAST (pilot-and-adaptive stable trading),
a novel framework for edge-assisted UAV networks with spatio-temporal dynamism.
PAST integrates two complementary mechanisms: PilotAO (pilot trading agreements
with overbooking), a risk-aware, overbooking-enabled early-stage
decision-making module that establishes long-term, mutually beneficial
agreements and boosts resource utilization; and AdaptAO (adaptive trading
agreements with overbooking rate update), an intelligent adaptation module that
dynamically updates agreements and overbooking rates based on UAV mobility,
supply-demand variations, and agreement performance. Together, these mechanisms
enable both stability and flexibility, guaranteeing individual rationality,
strong stability, competitive equilibrium, and weak Pareto optimality.
Extensive experiments on real-world datasets show that PAST consistently
outperforms benchmark methods in decision-making overhead, task completion
latency, resource utilization, and social welfare. By combining predictive
planning with real-time adjustments, PAST offers a valuable reference on robust
and adaptive practice for improving low-altitude mission performance.

</details>


### [48] [AGOCS -- Accurate Google Cloud Simulator Framework](https://arxiv.org/abs/2509.26120)
*Leszek Sliwko,Vladimir Getov*

Main category: cs.DC

TL;DR: 本文提出了AGOCS，一个基于真实Google集群工作负载轨迹的高保真云工作负载模拟器，可在桌面环境中用于日常研究，并开源发布。


<details>
  <summary>Details</summary>
Motivation: 现有云模拟工具缺乏对真实大规模工作负载的高保真模拟能力，难以在普通桌面环境中高效运行，因此需要一个准确、易用且可扩展的模拟框架。

Method: 基于Google集群12.5K节点一个月的真实工作负载轨迹，使用Scala语言开发了一个支持并行执行、易于扩展的高保真模拟器AGOCS。

Result: AGOCS能够精确还原作业、任务和节点的详细参数，并提供真实的资源使用统计信息，且已在GitHub上开源。

Conclusion: AGOCS为云工作负载研究提供了一个高保真、易用且可扩展的桌面级模拟平台，未来可通过替代设计进一步提升性能。

Abstract: This paper presents the Accurate Google Cloud Simulator (AGOCS) - a novel
high-fidelity Cloud workload simulator based on parsing real workload traces,
which can be conveniently used on a desktop machine for day-to-day research.
Our simulation is based on real-world workload traces from a Google Cluster
with 12.5K nodes, over a period of a calendar month. The framework is able to
reveal very precise and detailed parameters of the executed jobs, tasks and
nodes as well as to provide actual resource usage statistics. The system has
been implemented in Scala language with focus on parallel execution and an
easy-to-extend design concept. The paper presents the detailed structural
framework for AGOCS and discusses our main design decisions, whilst also
suggesting alternative and possibly performance enhancing future approaches.
The framework is available via the Open Source GitHub repository.

</details>


### [49] [Accelerating LLM Inference with Precomputed Query Storage](https://arxiv.org/abs/2509.25919)
*Jay H. Park,Youngju Cho,Choungsol Lee,Moonwook Oh,Euiseong Seo*

Main category: cs.DC

TL;DR: StorInfer 是一种新型存储辅助的 LLM 推理系统，通过离线预计算并存储可预测的查询-响应对，在运行时通过语义匹配快速返回结果，从而显著降低延迟和计算开销。


<details>
  <summary>Details</summary>
Motivation: LLM 推理在资源受限环境（如端侧或边缘部署）中常面临高延迟问题，亟需高效、低开销的推理加速方案。

Method: StorInfer 利用 LLM 驱动生成器，结合自适应查询掩码和自适应采样技术，从知识库中生成多样且去重的查询；将生成的查询-响应对嵌入并存入磁盘支持的向量数据库，实现运行时的快速语义检索。

Result: 在多个问答数据集上，StorInfer 生成了 15 万条唯一预计算对（占用最多 830 MB 存储），在不损失响应质量的前提下，最高实现 17.3% 的延迟降低。

Conclusion: StorInfer 展示了利用存储作为核心手段实现高效、低延迟 LLM 部署的可行性，尤其适用于查询分布可预测的场景。

Abstract: Large language model (LLM) inference often suffers from high latency,
particularly in resource-constrained environments such as on-device or edge
deployments. To address this challenge, we present StorInfer, a novel
storage-assisted LLM inference system that accelerates response time by
precomputing and storing predictable query-response pairs offline. When a user
query semantically matches a precomputed query, StorInfer bypasses expensive
GPU inference and instantly returns the stored response, significantly reducing
latency and compute costs. To maximize coverage and effectiveness, StorInfer
employs an LLM-driven generator that adaptively produces diverse and
deduplicated queries based on a given knowledge base. This is achieved via two
techniques: adaptive query masking, which prevents regeneration of similar
queries, and adaptive sampling, which dynamically tunes generation parameters
to promote semantic diversity. The resulting query-response pairs are embedded
and indexed using a disk-backed vector database to enable fast,
similarity-based retrieval at runtime. Using this approach, we generated 150K
unique precomputed pairs (taking up to 830 MB of storage space), achieving up
to 17.3% latency reduction with no loss in response quality. Our evaluation
across multiple QA datasets demonstrates the practicality and scalability of
storage-assisted inference, especially in scenarios with predictable query
distributions. StorInfer highlights a promising direction in leveraging storage
as a primary enabler for efficient, low-latency LLM deployment.

</details>


### [50] [Efficient Construction of Large Search Spaces for Auto-Tuning](https://arxiv.org/abs/2509.26253)
*Floris-Jan Willemsen,Rob V. van Nieuwpoort,Ben van Werkhoven*

Main category: cs.DC

TL;DR: 本文将自动调优中的搜索空间构建问题形式化为约束满足问题（CSP），并基于CSP求解器开发了一套高效、灵活且开源的运行时解析与优化工具，显著提升了构建速度，消除了自动调优的可扩展性瓶颈。


<details>
  <summary>Details</summary>
Motivation: 当前自动调优中搜索空间构建耗时严重（可达数分钟至数天），而现有方法（如chain-of-trees）缺乏形式化基础且适用范围有限，难以应对复杂约束和大规模组合空间。

Method: 将搜索空间构建问题建模为约束满足问题（CSP），设计运行时解析器将用户定义的约束函数转换为求解器友好的表达式，优化CSP求解器以利用自动调优约束中的常见结构，并集成到开源工具中。

Result: 在多样化基准测试中，优化后的求解器相比暴力枚举提速四个数量级，相比未优化CSP求解器提速三个数量级，相比主流chain-of-trees框架提速一到两个数量级。

Conclusion: 该方法有效解决了自动调优中搜索空间构建的性能瓶颈，提供了一种即插即用的解决方案，使探索此前无法处理的大规模问题成为可能。

Abstract: Automatic performance tuning, or auto-tuning, accelerates high-performance
codes by exploring vast spaces of code variants. However, due to the large
number of possible combinations and complex constraints, constructing these
search spaces can be a major bottleneck. Real-world applications have been
encountered where the search space construction takes minutes to hours or even
days. Current state-of-the-art techniques for search space construction, such
as chain-of-trees, lack a formal foundation and only perform adequately on a
specific subset of search spaces.
  We show that search space construction for constraint-based auto-tuning can
be reformulated as a Constraint Satisfaction Problem (CSP). Building on this
insight with a CSP solver, we develop a runtime parser that translates
user-defined constraint functions into solver-optimal expressions, optimize the
solver to exploit common structures in auto-tuning constraints, and integrate
these and other advances in open-source tools. These contributions
substantially improve performance and accessibility while preserving
flexibility.
  We evaluate our approach using a diverse set of benchmarks, demonstrating
that our optimized solver reduces construction time by four orders of magnitude
versus brute-force enumeration, three orders of magnitude versus an unoptimized
CSP solver, and one to two orders of magnitude versus leading auto-tuning
frameworks built on chain-of-trees. We thus eliminate a critical scalability
barrier for auto-tuning and provide a drop-in solution that enables the
exploration of previously unattainable problem scales in auto-tuning and
related domains.

</details>


### [51] [Enabling Time-Aware Priority Traffic Management over Distributed FPGA Nodes](https://arxiv.org/abs/2509.26043)
*Alberto Scionti,Paolo Savio,Francesco Lubrano,Federico Stirano,Antonino Nespola,Olivier Terzo,Corrado De Sio,Luca Sterpone*

Main category: cs.DC

TL;DR: 本文通过在开源NIC实现Corundum中引入硬件级时间感知流量管理功能，实现了对不同优先级队列传输带宽的精确控制，并通过Linux QDISC机制支持多类流量的带宽预留。


<details>
  <summary>Details</summary>
Motivation: 随着NIC和FPGA向异构智能系统演进，有必要在硬件层面实现更精细的流量控制机制，以支持不同流量类别的服务质量（QoS）需求。

Method: 在Corundum开源NIC基础上，通过AXI总线添加专用控制寄存器，为每个传输队列配置其在传输窗口中占用输出端口的时间比例，结合Linux QDISC机制对队列进行优先级划分和流量分类。

Result: 实验验证了该方法能有效管理分配给不同传输流的带宽，实现对多类流量的精确带宽控制。

Conclusion: 通过在硬件中实现时间感知的流量调度机制，可以高效支持多类流量的带宽预留，提升Smart-NIC在复杂网络环境中的服务质量保障能力。

Abstract: Network Interface Cards (NICs) greatly evolved from simple basic devices
moving traffic in and out of the network to complex heterogeneous systems
offloading host CPUs from performing complex tasks on in-transit packets. These
latter comprise different types of devices, ranging from NICs accelerating
fixed specific functions (e.g., on-the-fly data compression/decompression,
checksum computation, data encryption, etc.) to complex Systems-on-Chip (SoC)
equipped with both general purpose processors and specialized engines
(Smart-NICs). Similarly, Field Programmable Gate Arrays (FPGAs) moved from pure
reprogrammable devices to modern heterogeneous systems comprising
general-purpose processors, real-time cores and even AI-oriented engines.
Furthermore, the availability of high-speed network interfaces (e.g., SFPs)
makes modern FPGAs a good choice for implementing Smart-NICs. In this work, we
extended the functionalities offered by an open-source NIC implementation
(Corundum) by enabling time-aware traffic management in hardware, and using
this feature to control the bandwidth associated with different traffic
classes. By exposing dedicated control registers on the AXI bus, the driver of
the NIC can easily configure the transmission bandwidth of different
prioritized queues. Basically, each control register is associated with a
specific transmission queue (Corundum can expose up to thousands of
transmission and receiving queues), and sets up the fraction of time in a
transmission window which the queue is supposed to get access the output port
and transmit the packets. Queues are then prioritized and associated to
different traffic classes through the Linux QDISC mechanism. Experimental
evaluation demonstrates that the approach allows to properly manage the
bandwidth reserved to the different transmission flows.

</details>


### [52] [Efficient Distributed Training via Dual Batch Sizes and Cyclic Progressive Learning](https://arxiv.org/abs/2509.26092)
*Kuan-Wei Lu,Ding-Yong Hong,Pangfeng Liu,Jan-Jan Wu*

Main category: cs.DC

TL;DR: 本文提出了一种结合双批量学习和循环渐进学习的混合分布式训练方法，在提升模型泛化能力的同时显著减少训练时间。


<details>
  <summary>Details</summary>
Motivation: 大批次训练虽能加速分布式深度学习，但会因泛化能力差而降低准确率；现有方法缺乏在保持训练效率的同时提升泛化性能的有效策略。

Method: 提出双批量学习方案（同时使用大、小批量）以兼顾训练效率与泛化能力，并结合循环渐进学习（逐步提高图像分辨率）以进一步减少训练开销。

Result: 在CIFAR-100上准确率提升3.3%、训练时间减少10.6%；在ImageNet上准确率提升0.1%、训练时间减少35.7%。

Conclusion: 所提出的混合训练方法在不显著增加训练时间的前提下有效提升了模型的泛化能力和训练效率。

Abstract: Distributed machine learning is critical for training deep learning models on
large datasets and with numerous parameters. Current research primarily focuses
on leveraging additional hardware resources and powerful computing units to
accelerate the training process. As a result, larger batch sizes are often
employed to speed up training. However, training with large batch sizes can
lead to lower accuracy due to poor generalization. To address this issue, we
propose the dual batch size learning scheme, a distributed training method
built on the parameter server framework. This approach maximizes training
efficiency by utilizing the largest batch size that the hardware can support
while incorporating a smaller batch size to enhance model generalization. By
using two different batch sizes simultaneously, this method reduces testing
loss and enhances generalization, with minimal extra training time.
Additionally, to mitigate the time overhead caused by dual batch size learning,
we propose the cyclic progressive learning scheme. This technique gradually
adjusts image resolution from low to high during training, significantly
boosting training speed. By combining cyclic progressive learning with dual
batch size learning, our hybrid approach improves both model generalization and
training efficiency. Experimental results using ResNet-18 show that, compared
to conventional training methods, our method can improve accuracy by 3.3% while
reducing training time by 10.6% on CIFAR-100, and improve accuracy by 0.1%
while reducing training time by 35.7% on ImageNet.

</details>


### [53] [Parallax: Efficient LLM Inference Service over Decentralized Environment](https://arxiv.org/abs/2509.26182)
*Chris Tong,Youhe Jiang,Gufeng Chen,Tianyi Zhao,Sibian Lu,Wenjie Qu,Eric Yang,Lynn Ai,Binhang Yuan*

Main category: cs.DC

TL;DR: Parallax 是一个去中心化的 LLM 推理系统，通过两阶段调度器在异构 GPU 池上高效执行推理，显著降低延迟并提升吞吐量。


<details>
  <summary>Details</summary>
Motivation: 集中式 LLM 推理依赖昂贵的专用 GPU 集群和高带宽互连，而利用去中心化的协作 GPU 池虽具吸引力，却面临 GPU 异构性、网络带宽受限和节点动态可用性等调度挑战。

Method: Parallax 采用两阶段调度机制：(i) 模型分配阶段，将模型各层分配到不同 GPU 上，在内存和带宽约束下联合优化延迟与吞吐；(ii) 请求时 GPU 流水线选择阶段，动态组合不同副本的层形成端到端执行链，实现负载均衡并适应实时条件。

Result: 在真实志愿者节点上对开源 LLM 的实验表明，Parallax 相比去中心化基线方法显著降低了延迟并提高了吞吐量。

Conclusion: 通过合理的调度策略，志愿者计算可以成为一种实用且经济的 LLM 推理基础设施。

Abstract: Deploying a large language model (LLM) inference service remains costly
because centralized serving depends on specialized GPU clusters and
high-bandwidth interconnects in datacenters. An appealing alternative is to
leverage collaborative decentralized GPU pools. However, heterogeneity in GPU
and limited interconnected network bandwidth, along with potentially dynamic
availability, make efficient scheduling the central challenge in this scenario.
In this paper, we present Parallax, a decentralized LLM serving system that
turns a pool of heterogeneous GPUs into an efficient inference platform via a
two-phase scheduler. Parallax decomposes planning into (i) model allocation,
which places layers of each replica across diverse GPUs to jointly optimize
latency and throughput under memory and link-bandwidth constraints, and (ii)
request-time GPU pipeline selection, which stitches layers from different
replicas into end-to-end execution chains that balance load and adapt to
current conditions. We implement Parallax and evaluate it on open-source LLMs
deployed over real volunteer nodes. Parallax consistently reduces latency and
increases throughput relative to decentralized baselines, demonstrating that
principled scheduling can make volunteer compute a practical, affordable
substrate for LLM inference.
  Github Repo at: https://github.com/GradientHQ/parallax.

</details>


### [54] [I Like To Move It -- Computation Instead of Data in the Brain](https://arxiv.org/abs/2509.26193)
*Fabian Czappa,Marvin Kaster,Felix Wolf*

Main category: cs.DC

TL;DR: 本文提出了一种新算法，通过移动计算而非数据，显著减少了脑模拟中突触更新和神经元间脉冲交换的通信开销。


<details>
  <summary>Details</summary>
Motivation: 人脑功能尚未完全理解，脑模拟虽是实验研究的重要补充，但面临神经元和突触数量庞大带来的计算挑战，尤其是结构可塑性（突触形成与删除）对记忆和学习至关重要，而现有方法在通信开销上限制了可扩展性。

Method: 采用一种受Barnes-Hut启发的近似方法降低计算复杂度，并提出新算法通过移动计算而非数据来减少通信开销。

Result: 新算法将连接性更新时间缩短为原来的六分之一，脉冲交换时间减少两个数量级以上。

Conclusion: 该算法显著提升了大规模脑模拟的可扩展性，为研究结构可塑性和脑功能提供了更高效的计算手段。

Abstract: The detailed functioning of the human brain is still poorly understood. Brain
simulations are a well-established way to complement experimental research, but
must contend with the computational demands of the approximately $10^{11}$
neurons and the $10^{14}$ synapses connecting them, the network of the latter
referred to as the connectome. Studies suggest that changes in the connectome
(i.e., the formation and deletion of synapses, also known as structural
plasticity) are essential for critical tasks such as memory formation and
learning. The connectivity update can be efficiently computed using a
Barnes-Hut-inspired approximation that lowers the computational complexity from
$O(n^2)$ to $O(n log n)$, where n is the number of neurons. However, updating
synapses, which relies heavily on RMA, and the spike exchange between neurons,
which requires all-to-all communication at every time step, still hinder
scalability. We present a new algorithm that significantly reduces the
communication overhead by moving computation instead of data. This shrinks the
time it takes to update connectivity by a factor of six and the time it takes
to exchange spikes by more than two orders of magnitude.

</details>


### [55] [CSnake: Detecting Self-Sustaining Cascading Failure via Causal Stitching of Fault Propagations](https://arxiv.org/abs/2509.26529)
*Shangshu Qian,Lin Tan,Yongle Zhang*

Main category: cs.DC

TL;DR: 本文提出了CSnake，一种用于暴露分布式系统中自维持级联故障的故障注入框架，通过因果缝合和故障因果分析（FCA）模拟复杂故障传播链，并在五个系统中发现了15个相关漏洞。


<details>
  <summary>Details</summary>
Motivation: 现有故障检测技术难以在部署前暴露自维持级联故障，因为这类故障通常需要特定条件组合才能触发，且涉及一系列由不同条件激活的故障传播。

Method: CSnake采用“因果缝合”方法，将多个单故障注入在不同测试中因果连接以模拟复杂传播链；通过故障因果分析（FCA）比较注入与非注入执行轨迹识别因果关系；使用三阶段测试预算分配策略优先探索具有独特因果后果的故障，并引入局部兼容性检查避免错误连接不兼容的传播路径。

Result: CSnake在五个分布式系统中检测到15个导致自维持级联故障的漏洞，其中5个已被确认，2个已修复。

Conclusion: CSnake通过新颖的因果分析与高效搜索策略，有效暴露了传统方法难以发现的自维持级联故障，为提升分布式系统可靠性提供了新思路。

Abstract: Recent studies have revealed that self-sustaining cascading failures in
distributed systems frequently lead to widespread outages, which are
challenging to contain and recover from. Existing failure detection techniques
struggle to expose such failures prior to deployment, as they typically require
a complex combination of specific conditions to be triggered. This challenge
stems from the inherent nature of cascading failures, as they typically involve
a sequence of fault propagations, each activated by distinct conditions.
  This paper presents CSnake, a fault injection framework to expose
self-sustaining cascading failures in distributed systems. CSnake uses the
novel idea of causal stitching, which causally links multiple single-fault
injections in different tests to simulate complex fault propagation chains. To
identify these chains, CSnake designs a counterfactual causality analysis of
fault propagations - fault causality analysis (FCA): FCA compares the execution
trace of a fault injection run with its corresponding profile run (i.e., same
test w/o the injection) and identifies any additional faults triggered, which
are considered to have a causal relationship with the injected fault.
  To address the large search space of fault and workload combinations, CSnake
employs a three-phase allocation protocol of test budget that prioritizes
faults with unique and diverse causal consequences, increasing the likelihood
of uncovering conditional fault propagations. Furthermore, to avoid incorrectly
connecting fault propagations from workloads with incompatible conditions,
CSnake performs a local compatibility check that approximately checks the
compatibility of the path constraints associated with connected fault
propagations with low overhead.
  CSnake detected 15 bugs that cause self-sustaining cascading failures in five
systems, five of which have been confirmed with two fixed.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [56] [Oh-Trust: Overbooking and Hybrid Trading-empowered Resource Scheduling with Smart Reputation Update over Dynamic Edge Networks](https://arxiv.org/abs/2509.25683)
*Houyi Qi,Minghui Liwang,Liqun Fu,Xianbin Wang,Huaiyu Dai,Xiaoyu Xia*

Main category: cs.NI

TL;DR: 本文提出了一种结合超售与混合交易的边缘计算资源调度机制Oh-Trust，通过激励固定买家签订长期合约、吸引临时买家参与现货交易，并引入动态信誉更新机制，以提升资源利用率和交易效率。


<details>
  <summary>Details</summary>
Motivation: 现有现货交易因需实时交互带来过高开销，而传统期货交易依赖历史数据，难以应对边缘网络动态变化带来的风险，因此需要一种兼顾效率、适应性和激励机制的新型资源调度方法。

Method: 提出Oh-Trust机制，结合超售策略与混合交易模式（长期期货+现货），并集成动态信誉更新机制，以优化长期合约的续签与资源分配。

Result: 基于真实数据集的仿真实验表明，Oh-Trust在多个评估指标上表现优异，有效提升了资源利用率和交易双方的收益。

Conclusion: Oh-Trust通过融合期货与现货交易、引入信誉机制，有效应对边缘网络的动态性和不确定性，为激励驱动的计算资源共享提供了高效解决方案。

Abstract: Incentive-driven computing resource sharing is crucial for meeting the
ever-growing demands of emerging mobile applications. Although conventional
spot trading offers a solution, it frequently leads to excessive overhead due
to the need for real-time trading related interactions. Likewise, traditional
futures trading, which depends on historical data, is susceptible to risks from
network dynamics. This paper explores a dynamic and uncertain edge network
comprising a computing platform, e.g., an edge server, that offers computing
services as resource seller, and various types of mobile users with diverse
resource demands as buyers, including fixed buyers (FBs) and uncertain
occasional buyers (OBs) with fluctuating needs. To facilitate efficient and
timely computing services, we propose an overbooking- and hybrid
trading-empowered resource scheduling mechanism with reputation update, termed
Oh-Trust. Particularly, our Oh-Trust incentivizes FBs to enter futures trading
by signing long-term contracts with the seller, while simultaneously attracting
OBs to spot trading, enhancing resource utilization and profitability for both
parties. Crucially, to adapt to market fluctuations, a smart reputation
updating mechanism is integrated, allowing for the timely renewal of long-term
contracts to optimize trading performance. Extensive simulations using
real-world datasets demonstrate the effectiveness of Oh-Trust across multiple
evaluation metrics.

</details>


### [57] [From Literature to Insights: Methodological Guidelines for Survey Writing in Communications Research](https://arxiv.org/abs/2509.25828)
*Dusit Niyato,Octavia A. Dobre,Trung Q. Duong,George K. Karagiannidis,Robert Schober*

Main category: cs.NI

TL;DR: 本文为通信与网络领域的研究人员提供撰写高质量综述与教程论文的系统性指南，涵盖选题、文献收集、结构组织、批判性评述、案例教学、图示设计和未来方向等七个关键方面。


<details>
  <summary>Details</summary>
Motivation: 通信与网络研究的快速发展催生了对高质量综述与教程论文的迫切需求，但撰写此类论文面临诸多挑战，远超传统研究论文的写作复杂度。

Method: 基于IEEE Communications Surveys & Tutorials等顶级期刊的丰富编辑经验，提出一套结构化、实用的综述论文撰写路线图。

Result: 为初级研究人员提供清晰可行的七步指南，帮助其撰写具有影响力、能促进领域理解与创新的综述与教程文章。

Conclusion: 通过系统化的方法和实践建议，本文旨在提升通信领域综述论文的质量，从而推动整个研究生态的理解深化与技术进步。

Abstract: The rapid growth of communications and networking research has created an
unprecedented demand for high-quality survey and tutorial papers that can
synthesize vast bodies of literature into coherent understandings and
actionable insights. However, writing impactful survey papers presents
multifaceted challenges that demand substantial effort beyond traditional
research article composition. This article provides a systematic, practical
roadmap for prospective authors in the communications research community,
drawing upon extensive editorial experience from premier venues such as the
IEEE Communications Surveys & Tutorials. We present structured guidelines
covering seven essential aspects: strategic topic selection with novelty and
importance, systematic literature collection, effective structural
organization, critical review writing, tutorial content development with
emphasis on case studies, comprehensive illustration design that enhances
comprehension, and identification of future directions. Our goal is to enable
junior researchers to craft exceptional survey and tutorial articles that
enhance understanding and accelerate innovation within the communications and
networking research ecosystem.

</details>


### [58] [OpenID Connect for Agents (OIDC-A) 1.0: A Standard Extension for LLM-Based Agent Identity and Authorization](https://arxiv.org/abs/2509.25974)
*Subramanya Nagabhushanaradhya*

Main category: cs.NI

TL;DR: OIDC-A 1.0 扩展了 OpenID Connect，为 LLM 代理在 OAuth 2.0 生态中提供身份表示、认证与细粒度授权的标准框架。


<details>
  <summary>Details</summary>
Motivation: 随着自主 AI 代理在数字系统中日益普及，亟需标准化协议来建立代理身份、验证代理证明、表示委托链，并基于代理属性实现细粒度授权。

Method: 定义标准声明、端点和协议，引入代理身份表示、委托链验证、证明验证和基于能力的授权机制，同时兼容现有 OAuth 2.0 与 OpenID Connect 基础设施。

Result: 提出了一个支持安全可信的代理-服务交互的标准化框架。

Conclusion: OIDC-A 1.0 为现代分布式系统中 LLM 代理的身份与授权管理提供了兼容且可扩展的基础。

Abstract: OpenID Connect for Agents (OIDC-A) 1.0 is an extension to OpenID Connect Core
1.0 that provides a comprehensive framework for representing, authenticating,
and authorizing LLM-based agents within the OAuth 2.0 ecosystem. As autonomous
AI agents become increasingly prevalent in digital systems, there is a critical
need for standardized protocols to establish agent identity, verify agent
attestation, represent delegation chains, and enable fine-grained authorization
based on agent attributes. This specification defines standard claims,
endpoints, and protocols that address these requirements while maintaining
compatibility with existing OAuth 2.0 and OpenID Connect infrastructure. The
proposed framework introduces mechanisms for agent identity representation,
delegation chain validation, attestation verification, and capability-based
authorization, providing a foundation for secure and trustworthy
agent-to-service interactions in modern distributed systems.

</details>


### [59] [Intelligent Multi-link EDCA Optimization for Delay-Bounded QoS in Wi-Fi 7](https://arxiv.org/abs/2509.25855)
*Peini Yi,Wenchi Cheng,Jingqing Wang,Jinzhe Pan,Yuehui Ouyang,Wei Zhang*

Main category: cs.NI

TL;DR: 本文针对Wi-Fi 7多链路操作（MLO）中静态EDCA参数和跨链路流量管理难以满足严格时延和多样化QoS需求的问题，提出一种基于遗传算法的MLO EDCA参数与接入类别（AC）流量分配联合优化方法，以最小化总丢包率并满足各AC的时延违规概率约束，实验表明该方法能有效提升高负载下MLO的QoS鲁棒性和资源利用效率。


<details>
  <summary>Details</summary>
Motivation: Wi-Fi 7的MLO虽提供并行传输能力，但现有静态EDCA参数和复杂的跨链路流量管理难以在多链路异构流量场景下保障严格的时延界限并优化QoS。

Method: 分析MLO整体QoS指标与EDCA参数及AC流量分配之间的关联，构建以最小化所有AC总丢包率为目标、满足各自时延违规概率约束的优化问题，并设计基于遗传算法（GA）的MLO EDCA QoS优化算法，在复杂的AC分配与EDCA参数配置空间中高效搜索最优解。

Result: 实验结果表明，所提方法能生成适应不同业务需求的自适应MLO配置策略，显著改善时延分布特性，提升高负载环境下MLO的QoS鲁棒性和资源利用效率。

Conclusion: 通过联合优化EDCA参数与AC流量在多链路上的分配，所提出的GA-based方法有效克服了静态EDCA在MLO场景下的局限性，为Wi-Fi 7实现高可靠、低时延的QoS保障提供了可行方案。

Abstract: IEEE 802.11be (Wi-Fi 7) introduces Multi-Link Operation (MLO) as a While MLO
offers significant parallelism and capacity, realizing its full potential in
guaranteeing strict delay bounds and optimizing Quality of Service (QoS) for
diverse, heterogeneous traffic streams in complex multi-link scenarios remain a
significant challenge. This is largely due to the limitations of static
Enhanced Distributed Channel Access (EDCA) parameters and the complexity
inherent in cross-link traffic management. To address this, this paper
investigates the correlation between overall MLO QoS indicators and the
configuration of EDCA parameters and Acess Catagory (AC) traffic allocation
among links. Based on this analysis, we formulate a constrained optimization
problem aiming to minimize the sum of overall packet loss rates for all access
categories while satisfying their respective overall delay violation
probability constraints. A Genetic Algorithm (GA)-based MLO EDCA QoS
optimization algorithm is designed to efficiently search the complex
configuration space of AC assignments and EDCA parameters. Experimental results
demonstrate that the proposed approach's efficacy in generating adaptive MLO
configuration strategies that align with diverse service requirements. The
proposed solution significantly improves delay distribution characteristics,
and enhance QoS robustness and resource utilization efficiency in high-load MLO
environments.

</details>


### [60] [User-Centric Communication Service Provision for Edge-Assisted Mobile Augmented Reality](https://arxiv.org/abs/2509.25905)
*Conghao Zhou,Jie Gao,Shisheng Hu,Nan Cheng,Weihua Zhuang,Xuemin Shen*

Main category: cs.NI

TL;DR: 本文提出了一种基于数字孪生（DT）的用户中心通信服务方案，用于6G网络中支持边缘辅助的移动增强现实（MAR），通过定制化的数据模型和自适应切换机制，有效应对MAR设备上行数据流量的用户特异性和非平稳性，从而提升帧上传的及时性和系统鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 为满足6G网络中移动增强现实（MAR）应用对沉浸式体验的需求，需确保MAR设备能及时将摄像头帧上传至边缘服务器以进行SLAM定位。然而，用户特定且非平稳的上行数据流量给通信服务带来挑战，现有5G切片方法难以有效应对。

Method: 提出一种基于数字孪生（DT）的用户中心通信服务方法：1）为每个MAR设备构建定制化数据模型，刻画SLAM帧上传机制对用户特定流量模式的影响；2）设计两个DT操作函数，协同实现不同数据驱动模型间的自适应切换；3）基于DT的用户导向数据管理，设计网络资源管理算法，保障帧上传时效性并应对建模不准确性。

Result: 基于真实轨迹的仿真实验表明，所提方法相比5G中广泛使用的切片式通信服务，在满足摄像头帧上传延迟要求方面提升了14.2%。

Conclusion: 本文提出的数字孪生驱动的用户中心通信服务能更有效地支持6G边缘辅助MAR应用，显著提升帧上传的时效性与系统鲁棒性，优于传统切片方法。

Abstract: Future 6G networks are envisioned to facilitate edge-assisted mobile
augmented reality (MAR) via strengthening the collaboration between MAR devices
and edge servers. In order to provide immersive user experiences, MAR devices
must timely upload camera frames to an edge server for simultaneous
localization and mapping (SLAM)-based device pose tracking. In this paper, to
cope with user-specific and non-stationary uplink data traffic, we develop a
digital twin (DT)-based approach for user-centric communication service
provision for MAR. Specifically, to establish DTs for individual MAR devices,
we first construct a data model customized for MAR that captures the intricate
impact of the SLAM-based frame uploading mechanism on the user-specific data
traffic pattern. We then define two DT operation functions that cooperatively
enable adaptive switching between different data-driven models for capturing
non-stationary data traffic. Leveraging the user-oriented data management
introduced by DTs, we propose an algorithm for network resource management that
ensures the timeliness of frame uploading and the robustness against inherent
inaccuracies in data traffic modeling for individual MAR devices. Trace-driven
simulation results demonstrate that the user-centric communication service
provision achieves a 14.2% increase in meeting the camera frame uploading delay
requirement in comparison with the slicing-based communication service
provision widely used for 5G.

</details>


### [61] [User-Centric Comparison of 5G NTN and DVB-S2/RCS2 Using OpenAirInterface and OpenSAND](https://arxiv.org/abs/2509.26013)
*Sumit Kumar,Juan Carlos Estrada-Jimenez,Ion Turcanu*

Main category: cs.NI

TL;DR: 本文通过6G Sandbox平台对5G NTN和DVB-S2/RCS2在GEO卫星场景下进行端到端用户中心性能评估，比较其在真实应用（如网页浏览、文件下载、视频流）中的表现，揭示各自优劣及适用部署场景。


<details>
  <summary>Details</summary>
Motivation: 随着5G非地面网络（5G-NTN）的发展，将卫星网络整合进下一代移动通信系统成为趋势；鉴于DVB-S2/RCS2已在卫星宽带中广泛应用，有必要对这两种技术进行详细比较，以指导实际部署。

Method: 利用6G Sandbox平台，采用OpenAirInterface模拟5G NTN，OpenSAND模拟DVB-S2/RCS2，在统一的透明载荷GEO卫星下行链路条件下，对多种真实应用场景进行端到端用户中心性能测试与分析。

Result: 仿真结果揭示了5G NTN与DVB-S2/RCS2在架构和协议差异下对应用层性能的不同影响，明确了各自的关键优势与局限性。

Conclusion: 5G NTN和DVB-S2/RCS2各有适用场景，应根据具体需求选择部署方案；尽管仿真缺乏实时性，但为未来卫星通信系统选型提供了重要参考。

Abstract: The integration of satellite networks into next-generation mobile
communication systems has gained considerable momentum with the advent of 5G
Non-Terrestrial Networks (5G-NTN). Since established technologies like
DVB-S2/RCS2 are already widely used for satellite broadband, a detailed
comparison with emerging 5G NTN solutions is necessary to understand their
relative merits and guide deployment decisions. This paper presents a
user-centric, end-to-end evaluation of these technologies under realistic
traffic conditions, showing how differences in architecture and protocols
impact application-layer performance. Utilizing the 6G Sandbox platform, we
employ OpenAirInterface to emulate 5G NTN and OpenSAND for DVB-S2/RCS2,
replicating transparent payload GEO satellite scenarios under uniform downlink
conditions. A range of real-world applications, such as web browsing, file
downloads, and video streaming, are tested across both systems and
systematically analyzed. While the emulation lacks real-time capability, it
reveals key strengths and limitations of each approach, helping identify
suitable deployment scenarios for 5G NTN and DVB-S2/RCS2.

</details>


### [62] [Knowledge Defined Networking for 6G: A Reinforcement Learning Example for Resource Management](https://arxiv.org/abs/2509.26075)
*Erol Koçoğlu,Mehmet Ozdem,Tuğçe Bilen*

Main category: cs.NI

TL;DR: 本文探讨了将强化学习应用于知识定义网络（KDN）以优化6G网络中的资源管理。


<details>
  <summary>Details</summary>
Motivation: 6G网络对速度、连接设备数量、能效、响应时间和移动宽带提出了更高要求，传统网络架构难以满足，因此需要引入AI技术如KDN来应对这些挑战。

Method: 采用强化学习方法在知识定义网络（KDN）框架下进行资源管理优化。

Result: 论文尚未提供具体实验结果，但指出KDN在资源管理、路由、调度、聚类和移动性预测等方面具有潜力。

Conclusion: 强化学习在KDN中可用于有效优化6G网络的资源管理，从而满足其严苛性能需求。

Abstract: 6G networks are expected to revolutionize connectivity, offering significant
improvements in speed, capacity, and smart automation. However, existing
network designs will struggle to handle the demands of 6G, which include much
faster speeds, a huge increase in connected devices, lower energy consumption,
extremely quick response times, and better mobile broadband. To solve this
problem, incorporating the artificial intelligence (AI) technologies has been
proposed. This idea led to the concept of Knowledge-Defined Networking (KDN).
KDN promises many improvements, such as resource management, routing,
scheduling, clustering, and mobility prediction. The main goal of this study is
to optimize resource management using Reinforcement Learning.

</details>


### [63] [Flexible-Sector 6DMA Base Station: Modeling and Design](https://arxiv.org/abs/2509.26086)
*Yunli Li,Xiaoming Shi,Xiaodan Shao,Jie Xu,Rui Zhang*

Main category: cs.NI

TL;DR: 本文提出了一种新型低成本的六维可移动天线（6DMA）基站架构——“灵活扇区”基站，通过联合优化扇区旋转与天线分配，显著提升上行链路平均和速率，尤其在用户空间分布不均时表现更优。


<details>
  <summary>Details</summary>
Motivation: 传统基站扇区固定，难以适应用户在空间上的动态分布（如热点区域），导致频谱效率受限。为提升系统性能，需一种能灵活调整天线位置与方向的架构。

Method: 提出“灵活扇区”BS架构，允许天线沿圆形轨道移动并旋转；建立角度域用户分布模型；在零迫（ZF）接收机下推导平均和速率表达式；设计两步算法联合优化扇区公共旋转角度与各扇区天线分配。

Result: 理论分析表明：最优天线数与扇区内用户数呈线性关系；在天线数趋于无穷时，和速率增益以$\log_2(B)$增长（$B$为扇区数）；仿真验证所提方案优于传统固定扇区BS及仅优化旋转或天线分配的方案，且扇区数越多增益越显著。

Conclusion: 灵活扇区6DMA基站能有效适配用户空间分布，通过联合优化扇区旋转与天线分配显著提升系统和速率，为未来无线网络提供了一种高效且成本可控的新架构。

Abstract: Six-dimensional movable antenna (6DMA) has emerged as a promising new
technology for future wireless networks, which can adaptively adjust the
three-dimensional (3D) positions and 3D rotations of antennas/antenna arrays
for performance enhancement. This paper proposes a novel cost-effective
6DMA-based base station (BS) architecture, termed the \textit{flexible-sector}
BS, which allows the deployed antennas to flexibly rotate and move along a
circular track, thus enabling common sector rotation and flexible antenna
allocation across sectors to adapt to the spatial user distribution
efficiently. In particular, we focus on the uplink transmission in a
single-cell system, where the flexible-sector BS receives independent messages
from multiple users. We introduce an angular-domain user distribution model,
which captures the users' spatial clustering or hot-spot distribution
effectively. Assuming the zero-forcing (ZF) based receiver applied at the BS to
decode multiuser signals, we derive the average sum rate achievable for the
users as a function of the common rotation of sectors and the antenna
allocation over them. Moreover, we develop a two-step algorithm to jointly
optimize the common sector rotation and antenna allocation to maximize the
average sum rate of all users. It is shown that the optimal antenna number in
each sector linearly increases with the number of users in it. It is also
revealed that under the most favorable user distribution, the achievable sum
rate gain increases in the order of $\log_{2}(B)$ in the regime of
asymptotically large number of antennas, where $B$ denotes the number of
sectors. Numerically results also show that as $B$ increases, the proposed
flexible-sector BS achieves higher sum rate, and it outperforms other benchmark
schemes, such as the traditional fixed-sector BS as well as the BS with sector
rotation or antenna allocation optimization only.

</details>


### [64] [Toward an Unbiased Collective Memory for Efficient LLM-Based Agentic 6G Cross-Domain Management](https://arxiv.org/abs/2509.26200)
*Hatim Chergui,Miguel Catalan Cid,Pouria Sayyad Khodashenas,Daniel Camps Mur,Christos Verikoukis*

Main category: cs.NI

TL;DR: 本文提出了一种基于大语言模型（LLM）的新型6G RAN-Edge网络跨域资源编排框架，通过引入无偏记忆机制，使RAN与Edge智能体在协商中有效克服认知偏差，显著减少未解决的协商并完全消除SLA违规。


<details>
  <summary>Details</summary>
Motivation: 现有基于记忆的智能体在跨域资源协商中易受认知偏差（如首因、近因、确认和可得性偏差）影响，导致策略学习不全面，影响6G网络中能效与延迟的协同优化。

Method: 设计了一个无偏集体记忆机制，包括：(i) 基于Jaccard相似度的语义检索；(ii) 通过加权SLA违规和强制包含失败案例来学习失败经验；(iii) 强制多样性以减少可得性偏差；(iv) 对近因与首因效应采用缓慢衰减加权以缓解时间偏差。智能体在数字孪生环境中测试策略，并迭代协商。

Result: 相比无记忆和普通记忆基线，该方法分别将未解决协商减少4.5倍和3.5倍，完全消除SLA违规，并改善了延迟与节能性能分布。

Conclusion: 引入无偏记忆机制能有效克服智能体在资源编排中的认知偏差，显著提升6G RAN-Edge网络的协同决策能力与服务质量。

Abstract: This paper introduces a novel framework for proactive cross-domain resource
orchestration in 6G RAN-Edge networks, featuring large language model
(LLM)-augmented agents. The system comprises specialized RAN (energy
efficiency) and Edge (latency assurance) agents that engage in iterative
negotiation, supported by advanced reasoning and planning capabilities. Agents
dynamically interact with a digital twin (DT) to test their proposals and
leverage a long-term collective memory where their joint successful and failed
agreements along with the related network contexts are distilled into
strategies to either follow or avoid and subsequently stored. Given that agents
are subject to a plethora of cognitive distortions when retrieving those past
experiences -- such as primacy, recency, confirmation and availability biases
-- we propose in this work a novel unbiased memory design (A reusable mockup
version of the unbiased memory source code is available for non-commercial use
at https://github.com/HatimChergui/unbiased-collective-memory). featuring (i)
semantic retrieval of past strategies via Jaccard similarity; (ii) learning
from failures through amplified weighting of SLA violations and mandatory
inclusion of failed negotiation cases to mitigate confirmation bias; (iii)
diversity enforcement to minimize availability bias and (iv) recency and
primacy weighting with slow decay to counteract temporal biases. Evaluation
results showcase the impact of existing biases and how the unbiased memory
allows to tackle them by learning from both successful and failed strategies,
either present or old, resulting in $\times 4.5$ and $\times 3.5$ reductions of
unresolved negotiations compared to non-memory and vanilla memory baselines,
respectively, while totally mitigating SLA violations as well as improving
latency and energy saving distributions.

</details>


### [65] [Target Wake Time Scheduling for Time-sensitive and Energy-efficient Wi-Fi Networks](https://arxiv.org/abs/2509.26245)
*Fabio Busacca,Corrado Puligheddu,Francesco Raviglione,Riccardo Rusca,Claudio Casetti,Carla Fabiana Chiasserini,Sergio Palazzo*

Main category: cs.NI

TL;DR: 本文提出TASP问题及其高效启发式求解器TASPER，利用Wi-Fi的TWT机制实现面向工业物联网的时间敏感网络，在满足信息新鲜度约束的同时提升吞吐量与能效。


<details>
  <summary>Details</summary>
Motivation: 传统Wi-Fi因信道竞争和冲突被认为不适合时间敏感网络（TSN），但借助Target Wake Time（TWT）机制可实现确定性传输；然而如何高效调度TWT服务周期以兼顾吞吐量、能效和信息新鲜度（AoI）仍是一个挑战。

Method: 作者形式化定义了TWT接纳与调度问题（TASP），并提出启发式算法TASPER以高效求解该NP难问题；通过基于ns-3的仿真器和包含10个商用TWT设备的IIoT测试平台对方案进行评估。

Result: 在大规模场景下，TASPER相比最优基线ShortestFirst降低平均传输拒绝成本达24.97%，节能14.86%；相比WirelessHART方案HSA，节能34%、拒绝成本降低26%；实测验证其在不违反AoI约束下接纳更多传输。

Conclusion: TASPER有效证明了Wi-Fi结合TWT机制可用于工业物联网中的时间敏感网络，在满足严格时效性要求的同时显著提升性能与能效。

Abstract: Time Sensitive Networking (TSN) is fundamental for the reliable, low-latency
networks that will enable the Industrial Internet of Things (IIoT). Wi-Fi has
historically been considered unfit for TSN, as channel contention and
collisions prevent deterministic transmission delays. However, this issue can
be overcome by using Target Wake Time (TWT), which enables the access point to
instruct Wi-Fi stations to wake up and transmit in non-overlapping TWT Service
Periods (SPs), and sleep in the remaining time. In this paper, we first
formulate the TWT Acceptance and Scheduling Problem (TASP), with the objective
to schedule TWT SPs that maximize traffic throughput and energy efficiency
while respecting Age of Information (AoI) constraints. Then, due to TASP being
NP-hard, we propose the TASP Efficient Resolver (TASPER), a heuristic strategy
to find near-optimal solutions efficiently. Using a TWT simulator based on
ns-3, we compare TASPER to several baselines, including HSA, a state-of-the-art
solution originally designed for WirelessHART networks. We demonstrate that
TASPER obtains up to 24.97% lower mean transmission rejection cost and saves up
to 14.86% more energy compared to the leading baseline, ShortestFirst, in a
challenging, large-scale scenario. Additionally, when compared to HSA, TASPER
also reduces the energy consumption by 34% and reduces the mean rejection cost
by 26%. Furthermore, we validate TASPER on our IIoT testbed, which comprises 10
commercial TWT-compatible stations, observing that our solution admits more
transmissions than the best baseline strategy, without violating any AoI
deadline.

</details>


### [66] [Introducing Large Language Models in the Design Flow of Time Sensitive Networking](https://arxiv.org/abs/2509.26368)
*Rubi Debnath,Luxi Zhao,Mohammadreza Barzegaran,Sebastian Steinhorst*

Main category: cs.NI

TL;DR: 本文首次探索利用大语言模型（LLMs）实现时间敏感网络（TSN）的端到端编排，提出一个LLM辅助的TSN自动化部署框架，并指出未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 随着实时、安全关键系统需求的增长，TSN的部署变得日益复杂，亟需自动化手段。LLM在复杂任务中的表现使其成为实现TSN编排的潜在工具，但目前尚无针对TSN的LLM编排研究。

Method: 作者首先梳理TSN编排的步骤与挑战，开展多模型参与的TSN配置概念验证实验，进而提出一个LLM辅助的TSN编排框架，并描述其构建模块与流程。

Result: 实验初步验证了现有LLM在TSN配置任务中的能力，识别出其在真实部署中的机会与局限，并提出了实现LLM辅助TSN编排的关键组件。

Conclusion: 本文为LLM辅助TSN编排的可行性评估提供了首个路线图，呼吁构建TSN专用数据集、标准化基准测试，并整合网络演算引擎等外部工具。

Abstract: The growing demand for real-time, safety-critical systems has significantly
increased both the adoption and complexity of Time Sensitive Networking (TSN).
Configuring an optimized TSN network is highly challenging, requiring careful
planning, design, verification, validation, and deployment. Large Language
Models (LLMs) have recently demonstrated strong capabilities in solving complex
tasks, positioning them as promising candidates for automating end-to-end TSN
deployment, referred to as TSN orchestration. This paper outlines the steps
involved in TSN orchestration and the associated challenges. To assess the
capabilities of existing LLM models, we conduct an initial proof-of-concept
case study focused on TSN configuration across multiple models. Building on
these insights, we propose an LLM-assisted orchestration framework. Unlike
prior research on LLMs in computer networks, which has concentrated on general
configuration and management, TSN-specific orchestration has not yet been
investigated. We present the building blocks for automating TSN using LLMs,
describe the proposed pipeline, and analyze opportunities and limitations for
real-world deployment. Finally, we highlight key challenges and research
directions, including the development of TSN-focused datasets, standardized
benchmark suites, and the integration of external tools such as Network
Calculus (NC) engines and simulators. This work provides the first roadmap
toward assessing the feasibility of LLM-assisted TSN orchestration.

</details>
