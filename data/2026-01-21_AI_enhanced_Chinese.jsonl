{"id": "2601.11584", "categories": ["cs.DC", "cs.PF"], "pdf": "https://arxiv.org/pdf/2601.11584", "abs": "https://arxiv.org/abs/2601.11584", "authors": ["Jody Almaida Putra"], "title": "Cost-Aware Logging: Measuring the Financial Impact of Excessive Log Retention in Small-Scale Cloud Deployments", "comment": "8 pages, 3 tables. Simulation-based study on cost-aware log retention", "summary": "Log data plays a critical role in observability, debugging, and performance monitoring in modern cloud-native systems. In small and early-stage cloud deployments, however, log retention policies are frequently configured far beyond operational requirements, often defaulting to 90 days or more, without explicit consideration of their financial and performance implications. As a result, excessive log retention becomes a hidden and recurring cost.\n  This study examines the financial and operational impact of log retention window selection from a cost-aware perspective. Using synthetic log datasets designed to reflect real-world variability in log volume and access patterns, we evaluate retention windows of 7, 14, 30, and 90 days. The analysis focuses on three metrics: storage cost, operationally useful log ratio, and cost per useful log. Operational usefulness is defined as log data accessed during simulated debugging and incident analysis tasks.\n  The results show that reducing log retention from 90 days to 14 days can lower log storage costs by up to 78 percent while preserving more than 97 percent of operationally useful logs. Longer retention windows provide diminishing operational returns while disproportionately increasing storage cost and query overhead. These findings suggest that modest configuration changes can yield significant cost savings without compromising system reliability.\n  Rather than proposing new logging mechanisms, this work offers a lightweight and accessible framework to help small engineering teams reason about log retention policies through a cost-effectiveness lens. The study aims to encourage more deliberate observability configurations, particularly in resource-constrained cloud environments.", "AI": {"tldr": "\u901a\u8fc7\u5206\u6790\u4e0d\u540c\u65e5\u5fd7\u4fdd\u7559\u7a97\u53e3\u7684\u6210\u672c\u4e0e\u6548\u7528\uff0c\u672c\u7814\u7a76\u53d1\u73b0\u5c06\u65e5\u5fd7\u4fdd\u7559\u671f\u4ece90\u5929\u7f29\u77ed\u81f314\u5929\u53ef\u8282\u7701\u9ad8\u8fbe78%\u7684\u5b58\u50a8\u6210\u672c\uff0c\u540c\u65f6\u4fdd\u7559\u8d85\u8fc797%\u7684\u8fd0\u7ef4\u6709\u7528\u65e5\u5fd7\u3002", "motivation": "\u5c0f\u578b\u4e91\u73af\u5883\u5e38\u9ed8\u8ba4\u8bbe\u7f6e\u8fc7\u957f\u7684\u65e5\u5fd7\u4fdd\u7559\u671f\uff0c\u5bfc\u81f4\u4e0d\u5fc5\u8981\u7684\u5b58\u50a8\u5f00\u9500\uff0c\u4e9f\u9700\u6210\u672c\u5bfc\u5411\u7684\u4f18\u5316\u65b9\u6848\u3002", "method": "\u4f7f\u7528\u6a21\u62df\u771f\u5b9e\u65e5\u5fd7\u91cf\u4e0e\u8bbf\u95ee\u6a21\u5f0f\u7684\u6570\u636e\u96c6\uff0c\u8bc4\u4f307\u300114\u300130\u548c90\u5929\u4fdd\u7559\u7a97\u53e3\u5728\u5b58\u50a8\u6210\u672c\u3001\u6709\u7528\u65e5\u5fd7\u6bd4\u4f8b\u53ca\u5355\u4f4d\u6210\u672c\u4e09\u4e2a\u6307\u6807\u4e0a\u7684\u8868\u73b0\u3002", "result": "14\u5929\u4fdd\u7559\u7a97\u53e3\u5728\u663e\u8457\u964d\u4f4e\u6210\u672c\u7684\u540c\u65f6\u51e0\u4e4e\u4e0d\u5f71\u54cd\u8fd0\u7ef4\u6548\u7528\uff1b\u66f4\u957f\u4fdd\u7559\u671f\u5e26\u6765\u8fb9\u9645\u6548\u76ca\u9012\u51cf\u4e14\u67e5\u8be2\u5f00\u9500\u589e\u52a0\u3002", "conclusion": "\u65e0\u9700\u65b0\u5de5\u5177\uff0c\u4ec5\u901a\u8fc7\u8f7b\u91cf\u7ea7\u7b56\u7565\u8c03\u6574\u5373\u53ef\u5b9e\u73b0\u53ef\u89c2\u6210\u672c\u8282\u7ea6\uff0c\u5efa\u8bae\u8d44\u6e90\u53d7\u9650\u56e2\u961f\u91c7\u7528\u6210\u672c\u6548\u76ca\u89c6\u89d2\u914d\u7f6e\u65e5\u5fd7\u4fdd\u7559\u7b56\u7565\u3002"}}
{"id": "2601.12266", "categories": ["cs.DC", "cs.NI", "cs.PF", "math.OC"], "pdf": "https://arxiv.org/pdf/2601.12266", "abs": "https://arxiv.org/abs/2601.12266", "authors": ["Neelkamal Bhuyan", "Randeep Bhatia", "Murali Kodialam", "TV Lakshman"], "title": "Opportunistic Scheduling for Optimal Spot Instance Savings in the Cloud", "comment": "Accepted for publication in the 45th IEEE International Conference on Computer Communications (INFOCOM 2026). Copyright 2026 IEEE", "summary": "We study the problem of scheduling delay-sensitive jobs over spot and on-demand cloud instances to minimize average cost while meeting an average delay constraint. Jobs arrive as a general stochastic process, and incur different costs based on the instance type. This work provides the first analytical treatment of this problem using tools from queuing theory, stochastic processes, and optimization. We derive cost expressions for general policies, prove queue length one is optimal for low target delays, and characterize the optimal wait-time distribution. For high target delays, we identify a knapsack structure and design a scheduling policy that exploits it. An adaptive algorithm is proposed to fully utilize the allowed delay, and empirical results confirm its near-optimality.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u5982\u4f55\u5728\u6ee1\u8db3\u5e73\u5747\u5ef6\u8fdf\u7ea6\u675f\u4e0b\uff0c\u901a\u8fc7\u8c03\u5ea6\u5ef6\u8fdf\u654f\u611f\u4efb\u52a1\u4ee5\u6700\u5c0f\u5316\u5e73\u5747\u6210\u672c\u3002", "motivation": "\u4f18\u5316\u4e91\u5b9e\u4f8b\u4e0a\u7684\u4efb\u52a1\u8c03\u5ea6\uff0c\u5e73\u8861\u6210\u672c\u4e0e\u5ef6\u8fdf\u9700\u6c42\u3002", "method": "\u7ed3\u5408\u6392\u961f\u8bba\u3001\u968f\u673a\u8fc7\u7a0b\u548c\u4f18\u5316\u65b9\u6cd5\uff0c\u63a8\u5bfc\u901a\u7528\u7b56\u7565\u6210\u672c\u8868\u8fbe\u5f0f\u5e76\u8bbe\u8ba1\u81ea\u9002\u5e94\u7b97\u6cd5\u3002", "result": "\u8bc1\u660e\u4f4e\u5ef6\u8fdf\u76ee\u6807\u4e0b\u961f\u5217\u957f\u5ea6\u4e3a1\u6700\u4f18\uff0c\u9ad8\u5ef6\u8fdf\u76ee\u6807\u4e0b\u5229\u7528\u80cc\u5305\u7ed3\u6784\u8bbe\u8ba1\u7b56\u7565\uff0c\u81ea\u9002\u5e94\u7b97\u6cd5\u63a5\u8fd1\u6700\u4f18\u3002", "conclusion": "\u6240\u63d0\u65b9\u6cd5\u80fd\u6709\u6548\u964d\u4f4e\u4e91\u4efb\u52a1\u8c03\u5ea6\u6210\u672c\uff0c\u540c\u65f6\u6ee1\u8db3\u5ef6\u8fdf\u7ea6\u675f\u3002"}}
{"id": "2601.13345", "categories": ["cs.SE", "cs.PF"], "pdf": "https://arxiv.org/pdf/2601.13345", "abs": "https://arxiv.org/abs/2601.13345", "authors": ["Saurabhsingh Rajput", "Alexander Brandt", "Vadim Elisseev", "Tushar Sharma"], "title": "FlipFlop: A Static Analysis-based Energy Optimization Framework for GPU Kernels", "comment": null, "summary": "Artificial Intelligence (AI) applications, such as Large Language Models, are primarily driven and executed by Graphics Processing Units (GPUs). These GPU programs (kernels) consume substantial amounts of energy, yet software developers often lack the hardware expertise and ad hoc knowledge required to optimize for power efficiency. We propose FlipFlop, a framework using static code analysis to predict energy consumption and recommend Pareto-optimal thread block configurations considering both power consumption and execution time. Our framework requires no runtime execution and analyzes PTX code, a low-level instruction set for CUDA-enabled GPUs. It is validated across a diverse set of GPUs and kernels, including multi-head attention, convolution, and matrix multiplication. FlipFlop achieves 83% accuracy in identifying locally optimal energy-efficient configurations, while also minimizing developer effort by reducing the optimization search space by 93.4%. For multi-head attention kernels, it yields up to 79% energy savings and 106% throughput gains relative to NVIDIA's occupancy heuristic. By integrating static analysis with real-time monitoring and providing explainable optimization guidance, FlipFlop empowers developers to create sustainable, high-performance GPU software which minimizes environmental and computational costs.", "AI": {"tldr": "FlipFlop\u662f\u4e00\u4e2a\u57fa\u4e8e\u9759\u6001\u4ee3\u7801\u5206\u6790\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u9884\u6d4bGPU\u5185\u6838\u80fd\u8017\u5e76\u63a8\u8350\u517c\u987e\u80fd\u8017\u4e0e\u6267\u884c\u65f6\u95f4\u7684\u5e15\u7d2f\u6258\u6700\u4f18\u7ebf\u7a0b\u5757\u914d\u7f6e\uff0c\u65e0\u9700\u8fd0\u884c\u65f6\u6267\u884c\uff0c\u51c6\u786e\u7387\u8fbe83%\uff0c\u53ef\u51cf\u5c1193.4%\u4f18\u5316\u641c\u7d22\u7a7a\u95f4\u3002", "motivation": "\u5f00\u53d1\u8005\u7f3a\u4e4f\u786c\u4ef6\u4e13\u4e1a\u77e5\u8bc6\uff0c\u96be\u4ee5\u9ad8\u6548\u4f18\u5316GPU\u7a0b\u5e8f\u80fd\u8017\uff0c\u4e9f\u9700\u81ea\u52a8\u5316\u3001\u4f4e\u95e8\u69db\u7684\u8282\u80fd\u4f18\u5316\u65b9\u6848\u3002", "method": "\u901a\u8fc7\u9759\u6001\u5206\u6790PTX\u4ee3\u7801\uff0c\u7ed3\u5408\u5b9e\u65f6\u76d1\u63a7\u4e0e\u53ef\u89e3\u91ca\u6027\u6307\u5bfc\uff0c\u9884\u6d4b\u80fd\u8017\u5e76\u63a8\u8350\u5e15\u7d2f\u6258\u6700\u4f18\u914d\u7f6e\u3002", "result": "\u5728\u591a\u5934\u6ce8\u610f\u529b\u7b49\u5185\u6838\u4e0a\u5b9e\u73b0\u6700\u9ad879%\u80fd\u8017\u8282\u7701\u548c106%\u541e\u5410\u91cf\u63d0\u5347\uff0c\u76f8\u6bd4NVIDIA\u542f\u53d1\u5f0f\u65b9\u6cd5\u663e\u8457\u4f18\u5316\u3002", "conclusion": "FlipFlop\u6709\u6548\u964d\u4f4e\u5f00\u53d1\u8005\u4f18\u5316\u8d1f\u62c5\uff0c\u52a9\u529b\u6784\u5efa\u9ad8\u6027\u80fd\u4e14\u73af\u5883\u53cb\u597d\u7684GPU\u8f6f\u4ef6\u3002"}}
{"id": "2601.11743", "categories": ["cs.OS", "cs.DC"], "pdf": "https://arxiv.org/pdf/2601.11743", "abs": "https://arxiv.org/abs/2601.11743", "authors": ["Yechen Xu", "Yifei Wang", "Nathanael Ren", "Yiran Chen", "Danyang Zhuo"], "title": "Nixie: Efficient, Transparent Temporal Multiplexing for Consumer GPUs", "comment": null, "summary": "Consumer machines are increasingly running large ML workloads such as large language models (LLMs), text-to-image generation, and interactive image editing. Unlike datacenter GPUs, consumer GPUs serve single-user, rapidly changing workloads, and each model's working set often nearly fills the GPU memory. As a result, existing sharing mechanisms (e.g., NVIDIA Unified Virtual Memory) perform poorly due to memory thrashing and excessive use of CPU pinned memory when multiple applications are active.\n  We design and implement Nixie, a system that enables efficient and transparent temporal multiplexing on consumer GPUs without requiring any application or driver changes. Nixie is a system service that coordinates GPU memory allocation and kernel launch behavior to efficiently utilize the CPU-GPU bi-directional bandwidth and CPU pinned memory. A lightweight scheduler in Nixie further improves responsiveness by automatically prioritizing latency-sensitive interactive jobs using MLFQ-inspired techniques. Our evaluations show that Nixie improves latency of real interactive code-completion tasks by up to $3.8\\times$ and saves up to 66.8% CPU pinned memory usage given the same latency requirement.", "AI": {"tldr": "Nixie \u662f\u4e00\u79cd\u65e0\u9700\u4fee\u6539\u5e94\u7528\u6216\u9a71\u52a8\u5373\u53ef\u5728\u6d88\u8d39\u7ea7 GPU \u4e0a\u5b9e\u73b0\u9ad8\u6548\u900f\u660e\u65f6\u95f4\u590d\u7528\u7684\u7cfb\u7edf\uff0c\u663e\u8457\u63d0\u5347\u4ea4\u4e92\u4efb\u52a1\u54cd\u5e94\u901f\u5ea6\u5e76\u964d\u4f4e CPU \u56fa\u5b9a\u5185\u5b58\u5360\u7528\u3002", "motivation": "\u6d88\u8d39\u7ea7 GPU \u5728\u8fd0\u884c\u591a\u4efb\u52a1\u5927\u6a21\u578b\u65f6\u56e0\u5185\u5b58\u4e0d\u8db3\u548c\u73b0\u6709\u5171\u4eab\u673a\u5236\u4f4e\u6548\u5bfc\u81f4\u6027\u80fd\u4e0b\u964d\uff0c\u4e9f\u9700\u4f18\u5316\u65b9\u6848\u3002", "method": "\u8bbe\u8ba1 Nixie \u7cfb\u7edf\u670d\u52a1\uff0c\u534f\u8c03 GPU \u5185\u5b58\u5206\u914d\u4e0e\u5185\u6838\u542f\u52a8\u884c\u4e3a\uff0c\u7ed3\u5408\u8f7b\u91cf\u7ea7 MLFQ \u542f\u53d1\u5f0f\u8c03\u5ea6\u5668\u4f18\u5148\u5904\u7406\u5ef6\u8fdf\u654f\u611f\u4efb\u52a1\u3002", "result": "\u5b9e\u9a8c\u8868\u660e Nixie \u6700\u9ad8\u53ef\u5c06\u4ea4\u4e92\u5f0f\u4ee3\u7801\u8865\u5168\u4efb\u52a1\u5ef6\u8fdf\u964d\u4f4e 3.8 \u500d\uff0c\u76f8\u540c\u5ef6\u8fdf\u8981\u6c42\u4e0b\u8282\u7701\u6700\u591a 66.8% CPU \u56fa\u5b9a\u5185\u5b58\u3002", "conclusion": "Nixie \u6709\u6548\u89e3\u51b3\u4e86\u6d88\u8d39\u7ea7 GPU \u591a\u4efb\u52a1\u573a\u666f\u4e0b\u7684\u5185\u5b58\u4e0e\u8c03\u5ea6\u74f6\u9888\uff0c\u63d0\u5347\u4e86\u7528\u6237\u4f53\u9a8c\u548c\u8d44\u6e90\u5229\u7528\u7387\u3002"}}
{"id": "2601.11770", "categories": ["cs.AR", "cs.CR"], "pdf": "https://arxiv.org/pdf/2601.11770", "abs": "https://arxiv.org/abs/2601.11770", "authors": ["Voktho Das", "Kimia Azar", "Hadi Kamali"], "title": "NuRedact: Non-Uniform eFPGA Architecture for Low-Overhead and Secure IP Redaction", "comment": "Accepted at Design, Automation, and Test 2026", "summary": "While logic locking has been extensively studied as a countermeasure against integrated circuit (IC) supply chain threats, recent research has shifted toward reconfigurable-based redaction techniques, e.g., LUT- and eFPGA-based schemes. While these approaches raise the bar against attacks, they incur substantial overhead, much of which arises not from genuine functional reconfigurability need, but from artificial complexity intended solely to frustrate reverse engineering (RE). As a result, fabrics are often underutilized, and security is achieved at disproportionate cost. This paper introduces NuRedact, the first full-custom eFPGA redaction framework that embraces architectural non-uniformity to balance security and efficiency. Built as an extension of the widely adopted OpenFPGA infrastructure, NuRedact introduces a three-stage methodology: (i) custom fabric generation with pin-mapping irregularity, (ii) VPR-level modifications to enable non-uniform placement guided by an automated Python-based optimizer, and (iii) redaction-aware reconfiguration and mapping of target IP modules. Experimental results show up to 9x area reduction compared to conventional uniform fabrics, achieving competitive efficiency with LUT-based and even transistor-level redaction techniques while retaining strong resilience. From a security perspective, NuRedact fabrics are evaluated against state-of-the-art attack models, including SAT-based, cyclic, and sequential variants, and show enhanced resilience while maintaining practical design overheads.", "AI": {"tldr": "NuRedact \u662f\u9996\u4e2a\u57fa\u4e8e\u975e\u5747\u5300\u67b6\u6784\u7684\u5168\u5b9a\u5236 eFPGA \u7535\u8def\u4fdd\u62a4\u6846\u67b6\uff0c\u5728\u4fdd\u969c\u5b89\u5168\u7684\u540c\u65f6\u663e\u8457\u964d\u4f4e\u9762\u79ef\u5f00\u9500\u3002", "motivation": "\u73b0\u6709\u53ef\u91cd\u6784\u7ea2\u5316\u65b9\u6848\u56e0\u4eba\u4e3a\u590d\u6742\u5316\u5bfc\u81f4\u8d44\u6e90\u5229\u7528\u7387\u4f4e\u3001\u6210\u672c\u8fc7\u9ad8\uff0c\u9700\u517c\u987e\u5b89\u5168\u4e0e\u6548\u7387\u7684\u4f18\u5316\u65b9\u6848\u3002", "method": "\u63d0\u51fa\u4e09\u9636\u6bb5\u65b9\u6cd5\uff1a\u5b9a\u5236\u975e\u5747\u5300\u7ed3\u6784\u5e03\u7ebf\u3001VPR \u5c42\u7ea7\u975e\u5747\u5300\u5e03\u5c40\u4f18\u5316\u3001\u652f\u6301\u7ea2\u5316\u7684 IP \u6620\u5c04\u4e0e\u91cd\u914d\u7f6e\u3002", "result": "\u76f8\u6bd4\u4f20\u7edf\u5747\u5300\u7ed3\u6784\uff0c\u9762\u79ef\u51cf\u5c11\u6700\u9ad8\u8fbe 9 \u500d\uff0c\u5b89\u5168\u5bf9\u6297 SAT\u3001\u5faa\u73af\u4e0e\u5e8f\u5217\u653b\u51fb\u4ecd\u5177\u5f3a\u97e7\u6027\uff0c\u4e14\u8bbe\u8ba1\u5f00\u9500\u53ef\u63a7\u3002", "conclusion": "NuRedact \u5728 OpenFPGA \u57fa\u7840\u4e0a\u5b9e\u73b0\u5b89\u5168\u4e0e\u6548\u7387\u7684\u826f\u597d\u5e73\u8861\uff0c\u4e3a\u7535\u8def\u4f9b\u5e94\u94fe\u9632\u62a4\u63d0\u4f9b\u5b9e\u7528\u65b0\u65b9\u6848\u3002"}}
{"id": "2601.11553", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2601.11553", "abs": "https://arxiv.org/abs/2601.11553", "authors": ["Kaiwei Liu", "Liekang Zeng", "Lilin Xu", "Bufang Yang", "Zhenyu Yan"], "title": "PerCache: Predictive Hierarchical Cache for RAG Applications on Mobile Devices", "comment": null, "summary": "Retrieval-augmented generation (RAG) has been extensively used as a de facto paradigm in various large language model (LLM)-driven applications on mobile devices, such as mobile assistants leveraging personal emails or meeting records. However, due to the lengthy prompts and the resource constraints, mobile RAG systems exhibit significantly high response latency. On this issue, one promising approach is to reuse intermediate computational results across different queries to eliminate redundant computation. But most existing approaches, such as KV cache reuse and semantic cache reuse, are designed for cloud settings and perform poorly, overlooking the distinctive characteristics of mobile RAG.\n  We propose PerCache, a novel hierarchical cache solution designed for reducing end-to-end latency of personalized RAG applications on mobile platforms. PerCache adopts a hierarchical architecture that progressively matches similar queries and QKV cache to maximize the reuse of intermediate results at different computing stages. To improve cache hit rate, PerCache applies a predictive method to populate cache with queries that are likely to be raised in the future. In addition, PerCache can adapt its configurations to dynamic system loads, aiming at maximizing the caching utility with minimal resource consumption. We implement PerCache on top of an existing mobile LLM inference engine with commodity mobile phones. Extensive evaluations show that PerCache can surpass the best-performing baseline by 34.4% latency reduction across various applications and maintain optimal latency performance under dynamic resource changes.", "AI": {"tldr": "PerCache\u662f\u4e00\u79cd\u4e3a\u79fb\u52a8\u8bbe\u5907\u4e2a\u6027\u5316RAG\u5e94\u7528\u8bbe\u8ba1\u7684\u5206\u5c42\u7f13\u5b58\u65b9\u6848\uff0c\u901a\u8fc7\u9884\u6d4b\u6027\u586b\u5145\u548c\u52a8\u6001\u8d44\u6e90\u914d\u7f6e\u663e\u8457\u964d\u4f4e\u7aef\u5230\u7aef\u5ef6\u8fdf\u3002", "motivation": "\u79fb\u52a8\u8bbe\u5907\u4e0a\u7684RAG\u7cfb\u7edf\u56e0\u8d44\u6e90\u53d7\u9650\u548c\u957f\u63d0\u793a\u5bfc\u81f4\u9ad8\u5ef6\u8fdf\uff0c\u73b0\u6709\u4e91\u73af\u5883\u7f13\u5b58\u65b9\u6cd5\u4e0d\u9002\u7528\u3002", "method": "\u91c7\u7528\u5206\u5c42\u67b6\u6784\u5339\u914d\u76f8\u4f3c\u67e5\u8be2\u4e0eQKV\u7f13\u5b58\uff0c\u7ed3\u5408\u9884\u6d4b\u6027\u7f13\u5b58\u586b\u5145\u548c\u52a8\u6001\u8d1f\u8f7d\u9002\u914d\u673a\u5236\u3002", "result": "\u5728\u591a\u79cd\u5e94\u7528\u4e2d\u6bd4\u6700\u4f73\u57fa\u7ebf\u964d\u4f4e34.4%\u5ef6\u8fdf\uff0c\u5e76\u5728\u8d44\u6e90\u52a8\u6001\u53d8\u5316\u4e0b\u4fdd\u6301\u6700\u4f18\u6027\u80fd\u3002", "conclusion": "PerCache\u6709\u6548\u4f18\u5316\u4e86\u79fb\u52a8\u7aefRAG\u7cfb\u7edf\u7684\u8ba1\u7b97\u590d\u7528\u4e0e\u8d44\u6e90\u6548\u7387\uff0c\u662f\u9762\u5411\u79fb\u52a8\u573a\u666f\u7684\u9ad8\u6548\u7f13\u5b58\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2601.13631", "categories": ["cs.OS", "cs.DC"], "pdf": "https://arxiv.org/pdf/2601.13631", "abs": "https://arxiv.org/abs/2601.13631", "authors": ["Jing Zou", "Shangyu Wu", "Hancong Duan", "Qiao Li", "Chun Jason Xue"], "title": "ContiguousKV: Accelerating LLM Prefill with Granularity-Aligned KV Cache Management", "comment": null, "summary": "Efficiently serving Large Language Models (LLMs) with persistent Prefix Key-Value (KV) Cache is critical for applications like conversational search and multi-turn dialogue. Serving a request requires loading the pre-computed prefix KV cache and generating the first token, defined as the Re-Prefill Phase. Offloading this shared prefix cache to secondary storage is essential for memory scalability. Re-Prefill with offloading suffers from severe I/O bottlenecks in two aspects. First, semantic-aware KV cache pruning algorithms select important tokens in fine granularity, while systems manage I/O in coarse, fixed-size blocks, causing severe read amplification. Second, the sequential dependency between identifying important tokens and loading KV cache creates idle I/O and compute bubbles, under-utilizing system resources.\n  This paper proposes \\textit{ContiguousKV}, a high-performance prefix KV cache offloading system that bridges algorithmic semantics with I/O efficiency to accelerate the Re-Prefill phase. We first introduce \\textit{ContiguousChunk}, a unified data management granularity that aligns KV cache pruning with I/O operations. All the mechanisms critical for I/O performance are performed at the granularity of ContiguousChunk, thereby eliminating read amplification. By exploiting the high similarity in important ContiguousChunk indices across layers, we propose intra- and inter-period asynchronous prefetching to break the sequential dependency between I/O and compute, effectively eliminating idle bubbles. Finally, we propose attention-guided cache management to retain semantically critical prefix data in memory. Evaluations on Qwen2.5 series models show that ContiguousKV achieves a 3.85x speedup in the Re-Prefill phase over the state-of-the-art offloading system IMPRESS, while maintaining high output quality.", "AI": {"tldr": "ContiguousKV \u662f\u4e00\u79cd\u9ad8\u6548\u7684\u524d\u7f00 KV \u7f13\u5b58\u5378\u8f7d\u7cfb\u7edf\uff0c\u901a\u8fc7\u4f18\u5316 I/O \u7c92\u5ea6\u548c\u5f02\u6b65\u9884\u53d6\u673a\u5236\uff0c\u663e\u8457\u52a0\u901f Re-Prefill \u9636\u6bb5\u5e76\u4fdd\u6301\u9ad8\u8d28\u91cf\u8f93\u51fa\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u7cfb\u7edf\u5728\u5378\u8f7d\u524d\u7f00 KV \u7f13\u5b58\u65f6\u5b58\u5728\u7684 I/O \u74f6\u9888\u4e0e\u8d44\u6e90\u5229\u7528\u7387\u4f4e\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51fa ContiguousChunk \u6570\u636e\u7ba1\u7406\u7c92\u5ea6\u3001\u5c42\u5185/\u5c42\u95f4\u5f02\u6b65\u9884\u53d6\u673a\u5236\u53ca\u6ce8\u610f\u529b\u5f15\u5bfc\u7f13\u5b58\u7ba1\u7406\u7b56\u7565\u3002", "result": "\u5728 Qwen2.5 \u6a21\u578b\u4e0a\uff0cRe-Prefill \u9636\u6bb5\u901f\u5ea6\u63d0\u5347 3.85 \u500d\uff0c\u540c\u65f6\u4fdd\u6301\u9ad8\u8f93\u51fa\u8d28\u91cf\u3002", "conclusion": "ContiguousKV \u6709\u6548\u5f25\u5408\u7b97\u6cd5\u8bed\u4e49\u4e0e I/O \u6548\u7387\u4e4b\u95f4\u7684\u9e3f\u6c9f\uff0c\u4e3a\u5927\u89c4\u6a21\u8bed\u8a00\u6a21\u578b\u670d\u52a1\u63d0\u4f9b\u9ad8\u6027\u80fd\u652f\u6301\u3002"}}
{"id": "2601.12348", "categories": ["cs.MA"], "pdf": "https://arxiv.org/pdf/2601.12348", "abs": "https://arxiv.org/abs/2601.12348", "authors": ["Haris Khan", "Sadia Asif"], "title": "Generative AI Agents for Controllable and Protected Content Creation", "comment": "Accepted GenProCC NeurIPS 2025, Paper # 33", "summary": "The proliferation of generative AI has transformed creative workflows, yet current systems face critical challenges in controllability and content protection. We propose a novel multi-agent framework that addresses both limitations through specialized agent roles and integrated watermarking mechanisms. Unlike existing multi-agent systems focused solely on generation quality, our approach uniquely combines controllable content synthesis with provenance protection during the generation process itself. The framework orchestrates Director/Planner, Generator, Reviewer, Integration, and Protection agents with human-in-the-loop feedback to ensure alignment with user intent while embedding imperceptible digital watermarks. We formalize the pipeline as a joint optimization objective unifying controllability, semantic alignment, and protection robustness. This work contributes to responsible generative AI by positioning multi-agent architectures as a solution for trustworthy creative workflows with built-in ownership tracking and content traceability.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u7ed3\u5408\u53ef\u63a7\u751f\u6210\u4e0e\u5185\u5bb9\u4fdd\u62a4\u7684\u591a\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u5b9e\u73b0\u53ef\u8ffd\u6eaf\u7684\u8d1f\u8d23\u4efb\u751f\u6210\u5f0fAI\u3002", "motivation": "\u89e3\u51b3\u5f53\u524d\u751f\u6210\u5f0fAI\u5728\u53ef\u63a7\u6027\u548c\u5185\u5bb9\u4fdd\u62a4\u65b9\u9762\u7684\u5173\u952e\u6311\u6218\u3002", "method": "\u901a\u8fc7\u5bfc\u6f14/\u89c4\u5212\u3001\u751f\u6210\u3001\u8bc4\u5ba1\u3001\u6574\u5408\u548c\u4fdd\u62a4\u7b49\u4e13\u7528\u667a\u80fd\u4f53\u89d2\u8272\uff0c\u7ed3\u5408\u4eba\u673a\u53cd\u9988\u4e0e\u6c34\u5370\u673a\u5236\uff0c\u6784\u5efa\u8054\u5408\u4f18\u5316\u6d41\u7a0b\u3002", "result": "\u5b9e\u73b0\u4e86\u7528\u6237\u610f\u56fe\u5bf9\u9f50\u3001\u8bed\u4e49\u4e00\u81f4\u6027\u53ca\u9c81\u68d2\u6c34\u5370\u5d4c\u5165\u7684\u7edf\u4e00\u76ee\u6807\u3002", "conclusion": "\u591a\u667a\u80fd\u4f53\u67b6\u6784\u53ef\u4f5c\u4e3a\u6784\u5efa\u53ef\u4fe1\u521b\u610f\u5de5\u4f5c\u6d41\u4e0e\u5185\u5d4c\u6240\u6709\u6743\u8ffd\u8e2a\u7684\u6709\u6548\u65b9\u6848\u3002"}}
{"id": "2601.12089", "categories": ["cs.AR", "cs.RO"], "pdf": "https://arxiv.org/pdf/2601.12089", "abs": "https://arxiv.org/abs/2601.12089", "authors": ["Erwan Tanguy-Legac", "Tommaso Belvedere", "Gianluca Corsini", "Marco Tognon", "Marcello Traiola"], "title": "Domain-specific Hardware Acceleration for Model Predictive Path Integral Control", "comment": "7 pages, 11 figures", "summary": "Accurately controlling a robotic system in real time is a challenging problem. To address this, the robotics community has adopted various algorithms, such as Model Predictive Control (MPC) and Model Predictive Path Integral (MPPI) control. The first is difficult to implement on non-linear systems such as unmanned aerial vehicles, whilst the second requires a heavy computational load. GPUs have been successfully used to accelerate MPPI implementations; however, their power consumption is often excessive for autonomous or unmanned targets, especially when battery-powered. On the other hand, custom designs, often implemented on FPGAs, have been proposed to accelerate robotic algorithms while consuming considerably less energy than their GPU (or CPU) implementation. However, no MPPI custom accelerator has been proposed so far. In this work, we present a hardware accelerator for MPPI control and simulate its execution. Results show that the MPPI custom accelerator allows more accurate trajectories than GPU-based MPPI implementations.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8eMPPI\u63a7\u5236\u7684\u786c\u4ef6\u52a0\u901f\u5668\uff0c\u76f8\u6bd4GPU\u5b9e\u73b0\u80fd\u63d0\u4f9b\u66f4\u7cbe\u786e\u7684\u8f68\u8ff9\u4e14\u529f\u8017\u66f4\u4f4e\u3002", "motivation": "\u89e3\u51b3\u673a\u5668\u4eba\u7cfb\u7edf\u5b9e\u65f6\u63a7\u5236\u4e2dMPPI\u7b97\u6cd5\u8ba1\u7b97\u8d1f\u8f7d\u9ad8\u3001GPU\u529f\u8017\u5927\u7684\u95ee\u9898\u3002", "method": "\u8bbe\u8ba1\u5e76\u4eff\u771f\u4e00\u79cd\u57fa\u4e8eFPGA\u7684MPPI\u5b9a\u5236\u786c\u4ef6\u52a0\u901f\u5668\u3002", "result": "\u8be5\u52a0\u901f\u5668\u5728\u8f68\u8ff9\u7cbe\u5ea6\u4e0a\u4f18\u4e8eGPU\u5b9e\u73b0\u3002", "conclusion": "\u5b9a\u5236\u786c\u4ef6\u52a0\u901f\u5668\u662f\u63d0\u5347MPPI\u63a7\u5236\u6548\u7387\u4e0e\u80fd\u6548\u7684\u6709\u6548\u65b9\u6848\u3002"}}
{"id": "2601.11577", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2601.11577", "abs": "https://arxiv.org/abs/2601.11577", "authors": ["Yuankai Fan", "Qizhen Weng", "Xuelong Li"], "title": "Computation-Bandwidth-Memory Trade-offs: A Unified Paradigm for AI Infrastructure", "comment": null, "summary": "Large-scale artificial intelligence models are transforming industries and redefining human machine collaboration. However, continued scaling exposes critical limitations in hardware, including constraints on computation, bandwidth, and memory. These dimensions are tightly interconnected, so improvements in one often create bottlenecks in others, making isolated optimizations less effective. Balancing them to maximize system efficiency remains a central challenge in scalable AI design. To address this challenge, we introduce {Computation-Bandwidth-Memory Trade-offs}, termed the {AI Trinity}, a unified paradigm that positions {computation}, {bandwidth}, and {memory} as coequal pillars for next-generation AI infrastructure. AI Trinity enables dynamic allocation of resources across these pillars, alleviating single-resource bottlenecks and adapting to diverse scenarios to optimize system performance. Within this framework, AI Trinity identifies three fundamental trade-offs: (1) {More Computation$\\rightarrow$Less Bandwidth}, wherein computational resources are exploited to reduce data transmission under limited bandwidth conditions, (2) {More Bandwidth$\\rightarrow$Less Memory}, which exploits abundant communication capacity to populate or refresh memory when local storage resources are constrained, and (3) {More Memory$\\rightarrow$Less Computation}, whereby storage capacity are utilized to mitigate redundant computation when computational costs are prohibitive. We illustrate the effectiveness of AI Trinity through representative system designs spanning edge-cloud communication, large-scale distributed training, and model inference. The innovations embodied in AI Trinity advance a new paradigm for scalable AI infrastructure, providing both a conceptual foundation and practical guidance for a broad range of application scenarios.", "AI": {"tldr": "\u63d0\u51faAI Trinity\u6846\u67b6\uff0c\u901a\u8fc7\u8ba1\u7b97\u3001\u5e26\u5bbd\u548c\u5185\u5b58\u4e09\u8005\u52a8\u6001\u6743\u8861\u4f18\u5316\u5927\u89c4\u6a21AI\u7cfb\u7edf\u6027\u80fd\u3002", "motivation": "\u5927\u89c4\u6a21AI\u6a21\u578b\u53d1\u5c55\u53d7\u9650\u4e8e\u786c\u4ef6\u74f6\u9888\uff0c\u5355\u4e00\u7ef4\u5ea6\u4f18\u5316\u6548\u679c\u6709\u9650\uff0c\u9700\u534f\u540c\u5e73\u8861\u8ba1\u7b97\u3001\u5e26\u5bbd\u4e0e\u5185\u5b58\u8d44\u6e90\u3002", "method": "\u6784\u5efaAI Trinity\u7edf\u4e00\u8303\u5f0f\uff0c\u5b9a\u4e49\u4e09\u9879\u6838\u5fc3\u6743\u8861\u673a\u5236\uff1a\u8ba1\u7b97\u6362\u5e26\u5bbd\u3001\u5e26\u5bbd\u6362\u5185\u5b58\u3001\u5185\u5b58\u6362\u8ba1\u7b97\uff0c\u5e76\u5728\u591a\u79cd\u7cfb\u7edf\u573a\u666f\u4e2d\u5b9e\u73b0\u52a8\u6001\u8d44\u6e90\u914d\u7f6e\u3002", "result": "\u5728\u8fb9\u7f18\u4e91\u901a\u4fe1\u3001\u5206\u5e03\u5f0f\u8bad\u7ec3\u4e0e\u6a21\u578b\u63a8\u7406\u7b49\u573a\u666f\u9a8c\u8bc1\u4e86AI Trinity\u7684\u6709\u6548\u6027\uff0c\u663e\u8457\u7f13\u89e3\u5355\u70b9\u74f6\u9888\u5e76\u63d0\u5347\u7cfb\u7edf\u6574\u4f53\u6548\u7387\u3002", "conclusion": "AI Trinity\u4e3a\u4e0b\u4e00\u4ee3AI\u57fa\u7840\u8bbe\u65bd\u63d0\u4f9b\u7406\u8bba\u57fa\u7840\u4e0e\u5b9e\u8df5\u6307\u5bfc\uff0c\u63a8\u52a8\u53ef\u6269\u5c55AI\u7cfb\u7edf\u7684\u534f\u540c\u53d1\u5c55\u3002"}}
{"id": "2601.14129", "categories": ["cs.OS", "cs.DC"], "pdf": "https://arxiv.org/pdf/2601.14129", "abs": "https://arxiv.org/abs/2601.14129", "authors": ["Haoru Zhao", "Mingkai Dong", "Erci Xu", "Zhongyu Wang", "Haibo Chen"], "title": "\"Range as a Key\" is the Key! Fast and Compact Cloud Block Store Index with RASK", "comment": null, "summary": "In cloud block store, indexing is on the critical path of I/O operations and typically resides in memory. With the scaling of users and the emergence of denser storage media, the index has become a primary memory consumer, causing memory strain. Our extensive analysis of production traces reveals that write requests exhibit a strong tendency to target continuous block ranges in cloud storage systems. Thus, compared to current per-block indexing, our insight is that we should directly index block ranges (i.e., range-as-a-key) to save memory.\n  In this paper, we propose RASK, a memory-efficient and high-performance tree-structured index that natively indexes ranges. While range-as-a-key offers the potential to save memory and improve performance, realizing this idea is challenging due to the range overlap and range fragmentation issues. To handle range overlap efficiently, RASK introduces the log-structured leaf, combined with range-tailored search and garbage collection. To reduce range fragmentation, RASK employs range-aware split and merge mechanisms. Our evaluations on four production traces show that RASK reduces memory footprint by up to 98.9% and increases throughput by up to 31.0x compared to ten state-of-the-art indexes.", "AI": {"tldr": "RASK\u662f\u4e00\u79cd\u57fa\u4e8e\u8303\u56f4\u7d22\u5f15\u7684\u9ad8\u6548\u5185\u5b58\u6811\u7ed3\u6784\uff0c\u663e\u8457\u964d\u4f4e\u5185\u5b58\u5360\u7528\u5e76\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u4e91\u5b58\u50a8\u4e2d\u7d22\u5f15\u5185\u5b58\u6d88\u8017\u8fc7\u5927\uff0c\u800c\u5199\u8bf7\u6c42\u5e38\u9488\u5bf9\u8fde\u7eed\u5757\u8303\u56f4\uff0c\u9002\u5408\u8303\u56f4\u7d22\u5f15\u4f18\u5316\u3002", "method": "\u63d0\u51faRASK\uff0c\u91c7\u7528\u65e5\u5fd7\u7ed3\u6784\u53f6\u8282\u70b9\u3001\u8303\u56f4\u5b9a\u5236\u641c\u7d22\u4e0e\u5783\u573e\u56de\u6536\u3001\u8303\u56f4\u611f\u77e5\u5206\u88c2\u5408\u5e76\u673a\u5236\u3002", "result": "\u5728\u56db\u4e2a\u751f\u4ea7\u8f68\u8ff9\u4e0a\uff0c\u5185\u5b58\u5360\u7528\u6700\u591a\u51cf\u5c1198.9%\uff0c\u541e\u5410\u91cf\u6700\u9ad8\u63d0\u534731\u500d\u3002", "conclusion": "RASK\u6709\u6548\u89e3\u51b3\u8303\u56f4\u91cd\u53e0\u4e0e\u788e\u7247\u95ee\u9898\uff0c\u5b9e\u73b0\u5185\u5b58\u9ad8\u6548\u4e0e\u9ad8\u6027\u80fd\u7d22\u5f15\u3002"}}
{"id": "2601.12580", "categories": ["cs.MA"], "pdf": "https://arxiv.org/pdf/2601.12580", "abs": "https://arxiv.org/abs/2601.12580", "authors": ["Sofiya Zaichyk"], "title": "Semantic Fusion: Verifiable Alignment in Decentralized Multi-Agent Systems", "comment": "29 pages", "summary": "We present Semantic Fusion (SF), a formal framework for decentralized semantic coordination in multi-agent systems. SF allows agents to operate over scoped views of shared memory, propose structured updates, and maintain global coherence through local ontology-based validation and refresh without centralized control or explicit message passing. The central theoretical result is a bisimulation theorem showing that each agent's local execution is behaviorally equivalent to its projection of the global semantics, in both deterministic and probabilistic settings. This enables safety, liveness, and temporal properties to be verified locally and soundly lifted to the full system. SF supports agents whose update proposals vary across invocations, including those generated by learned or heuristic components, provided updates pass semantic validation before integration. We establish deterministic and probabilistic guarantees ensuring semantic alignment under asynchronous or degraded communication. To validate the model operationally, we implement a lightweight reference architecture that instantiates its core mechanisms. A 250-agent simulation evaluates these properties across over 11,000 validated updates, demonstrating convergence under probabilistic refresh, bounded communication, and resilience to agent failure. Together, these results show that Semantic Fusion can provide a formal and scalable basis for verifiable autonomy in decentralized systems.", "AI": {"tldr": "Semantic Fusion\u63d0\u4f9b\u4e86\u4e00\u79cd\u53bb\u4e2d\u5fc3\u5316\u591a\u667a\u80fd\u4f53\u8bed\u4e49\u534f\u8c03\u6846\u67b6\uff0c\u652f\u6301\u5c40\u90e8\u9a8c\u8bc1\u4e0e\u5168\u5c40\u4e00\u81f4\u6027\uff0c\u5177\u5907\u53ef\u9a8c\u8bc1\u7684\u786e\u5b9a\u6027\u548c\u6982\u7387\u6027\u4fdd\u969c\u3002", "motivation": "\u89e3\u51b3\u53bb\u4e2d\u5fc3\u5316\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u4e2d\u7f3a\u4e4f\u5f62\u5f0f\u5316\u8bed\u4e49\u534f\u8c03\u673a\u5236\u7684\u95ee\u9898\uff0c\u5b9e\u73b0\u65e0\u9700\u4e2d\u592e\u63a7\u5236\u7684\u5b89\u5168\u81ea\u6cbb\u3002", "method": "\u63d0\u51fa\u8bed\u4e49\u878d\u5408\u6846\u67b6\uff0c\u7ed3\u5408\u672c\u4f53\u9a8c\u8bc1\u3001\u5c40\u90e8\u6295\u5f71\u4e0e\u6982\u7387\u5237\u65b0\u673a\u5236\uff0c\u5efa\u7acb\u884c\u4e3a\u7b49\u4ef7\u6027\u5b9a\u7406\u5e76\u5b9e\u73b0\u8f7b\u91cf\u67b6\u6784\u4eff\u771f\u3002", "result": "250\u667a\u80fd\u4f53\u4eff\u771f\u9a8c\u8bc1\u4e86\u6536\u655b\u6027\u3001\u901a\u4fe1\u6709\u754c\u6027\u4e0e\u5bb9\u9519\u6027\uff0c11,000+\u66f4\u65b0\u5747\u901a\u8fc7\u8bed\u4e49\u9a8c\u8bc1\u3002", "conclusion": "\u8bed\u4e49\u878d\u5408\u4e3a\u53bb\u4e2d\u5fc3\u5316\u7cfb\u7edf\u7684\u53ef\u9a8c\u8bc1\u81ea\u4e3b\u6027\u63d0\u4f9b\u4e86\u5f62\u5f0f\u5316\u4e14\u53ef\u6269\u5c55\u7684\u57fa\u7840\u3002"}}
{"id": "2601.11659", "categories": ["cs.SE", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.11659", "abs": "https://arxiv.org/abs/2601.11659", "authors": ["Aaron Adcock", "Aayushi Srivastava", "Abhimanyu Dubey", "Abhinav Jauhri", "Abhinav Pande", "Abhinav Pandey", "Abhinav Sharma", "Abhishek Kadian", "Abhishek Kumawat", "Adam Kelsey", "Adam Stelle", "Adeel Cheema", "Adela Kabiljo", "Adina Katz", "Adithya Gangidi", "Aditya Tayade", "Adolfo Victoria", "Adrian Samatan Alastuey", "Adrien Conrath", "Afroz Mohiuddin", "Ahmed Sharif", "Ahnaf Siddiqui", "Ahuva Goldstand", "Aijung Li", "Aidan Boyd", "Aidin Kazemi Daliri", "Aisha Iqbal", "Ajay Menon", "Ajit Mathews", "Akhil Mathur", "Akshat Agarwal", "Alan Schelten", "Alana Shine", "Alejandro Castillejo Mu\u00f1oz", "Aleksei Guliaev", "Alex Radovic", "Alex Song", "Alex Vaughan", "Alexander Simeonov", "Alexandre Rezende", "Alexandre Rezende", "Alexei Baevski", "Alexey Roubaud", "Allen Ma", "Alvin Lee", "Alyssa Pereira", "Aman Ahmed", "Aman Shankar", "Amanda Kallet", "Amar Budhiraja", "Ameya Khandekar", "Amine Benhalloum", "Amir Gershman", "Amit Nagpal", "Amit Zohar", "Amr Sharaf", "Anant Desai", "Anastasia Razdaibiedina", "Anca Agape", "Andranik Kurghinyan", "Andre Perunicic", "Andrea Madotto", "Andrei Darabanov", "Andr\u00e9s Alvarado", "Andrew Brown", "Andrew Cohen", "Andrew Fang", "Andrew Freeman", "Andrew Gallagher", "Andrew Gu", "Andrew Prasetyo Jo", "Andrew Ryan", "Andrew Steffen", "Andrew Wei", "Andrey Rusakov", "Andrii Golovei", "Andy Shang", "Angela Fan", "Angela Fan", "Angela Flewellen", "Animesh Pathak", "Anirudh Goyal", "Ankit Ramchandani", "Ankur Pai", "Ankur Singh", "Ankush Garg", "Anlu Xing", "Anna Cai", "Anna Grosul", "Anna Prochowska", "Anna Sun", "Annie Dong", "Annie Franco", "Anqi Hu", "Anshul Chawla", "Anthony Hartshorn", "Antonia Sheng", "Antony Thomas", "Anuj Goyal", "Anusha De", "Anvit Bodiwala", "Anvit Bodiwala", "Aobo Yang", "Aparajita Saraf", "Apurva Samudra", "Aran Mun", "Arash Rahnama", "Archi Mitra", "Archie Sravankumar", "Archit Gupta", "Aria Haghighi", "Ariel Stolerman", "Arkabandhu Chowdhury", "Arnab Choudhury", "Artem Korenev", "Arthur Guo", "Arthur Hinsvark", "Arun Mallya", "Arvind Neelakantan", "Arya Talebzadeh", "Ashish Shah", "Ashmitha Jeevaraj Shetty", "Ashwin Bharambe", "Asif Islam", "Aston Zhang", "Austen Gregerson", "Avi Lewis", "Aya Ibrahim", "Ayaz Minhas", "Ayelet Dahan", "Ayelet Regev Dabah", "Bangsheng Tang", "Bar Ulman", "Bardiya Sadeghi", "Bartosz Jedrzejewski", "Barys Skarabahaty", "Beibei Zhu", "Beibin Li", "Ben Bharier", "Benjamin Leonhardi", "Benjamin Muller", "Bennett Plessala", "Bernie Huang", "Beth Loyd", "Bhargavi Paranjape", "Bhavik Sheth", "Bill Bonner", "Bill Holland", "Bill Wang", "Bingzhe Liu", "Binh Tang", "Bo Liu", "Bo Wu", "Boduo Li", "Bokai Yu", "Bor-Chun Chen", "Boris Araya", "Boris Vidolov", "Botao Chen", "Boya Peng", "Boyu Ni", "Bradley Davis", "Bram Wasti", "Brandon Adams", "Brandon Taylor", "Brandon Wu", "Brant Swidler", "Brian Chiang", "Brian Clerkin", "Brian Fuller", "Brooks Cutter", "Bruno Novais", "Bryan Gmyrek", "Bysshe Easton", "Cait Campos", "Canaan Case", "Carl Chengyan Fu", "Carly Burton", "Caro Diaz", "Catherine Cole", "Ce Liu", "Cedric Fougerat", "Cen Peng", "Cen Peng", "Cen Zhao", "Changhan Wang", "Changkyu Kim", "Chantal Shaib", "Chao Zhou", "Charlotte Caucheteux", "Chau Nguyen", "Chawin Sitawarin", "Chaya Nayak", "Chelsea Asher", "Chen Fan", "Chen Zhu", "Cheng Cheng", "Cheng Zhang", "Chenguang Zhu", "Chengxiong Ruan", "Chengzhu Yu", "Chenheli Hua", "Chenxi Whitehouse", "Cheryl Holloway", "Ching-Hsiang Chu", "Ching-Yao Chuang", "Chinmay Karande", "Chirag Nagpal", "Chlo\u00e9 Bakalar", "Chloe Bi", "Chris Cai", "Chris Marra", "Chris McConnell", "Chris Thi", "Chris Tindal", "Chris Waterson", "Christian Deverall", "Christian Fuegen", "Christian Keller", "Christine Cheng", "Christine Jou", "Christine Smith", "Christine Wang", "Christoph Feichtenhofer", "Christophe Touret", "Christopher Luc", "Christy Sauper", "Chuanhao Zhuge", "Chun-Yi Sung", "Chunqiang Tang", "Chunyang Wu", "Clara Siegel", "Cody Heale", "Cody Wilbourn", "Colin White", "Congying Xia", "Corinne Wong", "Cornel Rat", "Cristian Canton Ferrer", "Cyrille Habis", "Cyrus Nikolaidis", "D Lohachov", "Da Ju", "Dalton Flanagan", "Damien Allonsius", "Damon Civin", "Dan Johnson", "Daniel Bolya", "Daniel Francisco", "Daniel Fried", "Daniel Hawthorne", "Daniel Haziza", "Daniel Ho", "Daniel Kreymer", "Daniel Li", "Daniel Machlab", "Daniel McKinnon", "Daniel Obenshain", "Daniel Rodriguez", "Daniel Song", "Daniel Tse", "Danielle Pintz", "Danny Livshits", "Daryl James Rodrigo", "Dat Huynh", "Daulet Askarov", "David Brandfonbrener", "David Esiobu", "David Kant", "David Levin", "David Renardy", "David Soofian", "David Stevens", "David Xu", "David Zhang", "Deep Shah", "Delia David", "Demi Douglas", "Denis Boyda", "Desh Raj", "Devamanyu Hazarika", "Dheeraj Mekala", "Dhruv Choudhary", "Dhruv Mahajan", "Di Jin", "Didac Suris Coll-Vinent", "Didem Foss", "Diego Garcia-Olano", "Diego Perino", "Dieuwke Hupkes", "DiJia Su", "Dilip Madathil", "Dinesh Govindasamy", "Dinesh Yeduguru", "Dmitry Vengertsev", "Dong He", "Dong Li", "Dong Wang", "Dongzhuo Li", "Duc Le", "Dunant Hin", "Dustin Holland", "Duy Nguyen", "Duy Nguyen", "Ed Dowling", "Eden Litt", "Egor Lakomkin", "Ehab AlBadawy", "Ehsan K. Ardestani", "Elad Eckstein", "Elahe Dabir", "Elaine Montgomery", "Elina Lobanova", "Elior Abramoviz", "Eliot Hedeman", "Elissa Li", "Elizabeth Hilbert", "Ellen Xiaoqing Tan", "Elliot Yun", "Elodie Stener", "Emilian Stoimenov", "Emilien Garreau", "Emily Dinan", "Emily Hahn", "Emily Wood", "Emma Li", "Emmanuel Ademuwagun", "Emrah Seker", "Eric Alamillo", "Eric Gan", "Eric Han", "Eric Huang", "Eric Michael Smith", "Eric-Tuan Le", "Ernie Chang", "Eryk Helenowski", "Eslam Elnikety", "Esteban Arcaute", "Ethan Myers", "Eugene Nho", "Eugene Poliukhovych", "Evan Dunbar", "Evgeniy Litvinenko", "Evrim Alt\u0131nta\u015f", "Eyal Hochman", "Eyal Shtrauch", "Fabian Mastenbroek", "Faiza Zeb", "Faizan Ahmad", "Farhad Farahbakhshian", "Fei Kou", "Fei Sun", "Feiyu Chen", "Felix Chung", "Feng Tian", "Feng Xu", "Filip Radenovic", "Filippos Kokkinos", "Francesco Barbieri", "Francesco Caggioni", "Francisco Esparza", "Francisco Guzm\u00e1n", "Frank Kanayet", "Frank Seide", "Frank Zhang", "Fred Lewis", "Freda Huang", "Fulton Wang", "Gabriel Synnaeve", "Gabriela Jacques-Silva", "Gabriella Schwarz", "Gaganjit Ghardhora", "Gal Elfer", "Garrett Dickson", "Gaurav Chaurasia", "Gautam Sewani", "Geet Shingi", "Gefei Zuo", "Geonhwa Jeong", "George Puthanpurackal", "Georgia Swee", "Gerard Moreno-Torres Bertran", "Gil Keren", "Gina Ling", "Gjergji Stasa", "Gobinda Saha", "Gor Safran", "Gordy French", "Goutham Rajendran", "Govind Thattai", "Grace Cineas", "Graeme Nail", "Greg Fletcher", "Gr\u00e9goire Mialon", "Griffin Adams", "Grigory Sizov", "Guan Pang", "Hady Elsahar", "Hai Dang Tran", "Hailey Nguyen", "Haiping Wu", "Hakan Inan", "Hamid Eghbalzadeh", "Han Fang", "Han Zou", "Hannah Doyle", "Hannah Korevaar", "Hannah Wang", "Hannah Werbel", "Hanwen Zha", "Hany Morsy", "Hao Ma", "Haoci Zhang", "Haonan Sun", "Haozhu Wang", "Hardik Shah", "Haroun Habeeb", "Harrison Rudolph", "Harsh Gupta", "Harsh Poddar", "Harshil Parikh", "Hejia Zhang", "Heming Wang", "Hengduo Li", "Himanshu Sharma", "Hoang Phi Nguyen", "Hongbo Zhang", "Honghao Qiu", "Hongjiang Lv", "Hongli Xu", "Hongyuan Zhan", "Hossein Hamooni", "Howard Huang", "Hu Xu", "Hugo Lauren\u00e7on", "Hugo Touvron", "Hung Dinh", "Hunter Goldman", "Hussein Mehanna", "Huy Nguyen", "Hweimi Tsuo", "Ian Graves", "Ian Yu", "Ibrahim Damlaj", "Idan Cohen", "Igor Tufanov", "Ilan Goldenstein", "Ilias Leontiadis", "Iliyan Zarov", "Imad Ahmed", "Innocent Djiofack", "Iosif Spulber", "Irina-Elena Veliche", "Isabella Ramos", "Ishan Misra", "Itai Gal", "Ivan Evtimov", "Ivan Evtimov", "Ivan Obraztsov", "Jack Wu", "Jacqueline Romero Vertino", "Jaemo Koo", "Jaewon Lee", "Jake Jung", "Jake Weissman", "James Beldock", "James Crnkovich", "James Grinage", "James Hongyi Zeng", "James Kohli", "James Tian", "Jamie Cahill", "Jan Geffert", "Jan Seidel", "Jan Seidel", "Janey Tracey", "Jang Hyun Cho", "Janice Wei", "Jarrod Kahn", "Jasmyn Howell", "Jason Long Vu", "Jason Park", "Jason Yan", "Jason Yip", "Jay Li", "Jay Mahadeokar", "Jaya Bharath R Goluguri", "Jayasi Mehar", "Jean-Baptiste Gaya", "Jeet Shah", "Jeff Hanson", "Jeff Marcus", "Jeff Walsh", "Jeff Yang", "Jelmer van der Linde", "Jemma Fan", "Jennifer Chan", "Jenny Zhen", "Jenya Lee", "Jeremy Fu", "Jeremy Reizenstein", "Jeremy Teboul", "Jesse He", "Jessica Zhong", "Ji Hou", "Ji Yang", "Jia Ding", "Jiabo Hu", "Jiacheng Zhu", "Jiadong Guo", "Jialiang Wang", "Jialin Ouyang", "Jianfeng Chi", "Jianyu Huang", "Jianyun Zhao", "Jiaowen Yang", "Jiatong Zhou", "Jiawei Zhao", "Jiawen Liu", "Jie Wang", "Jie You", "Jiecao Yu", "Jillian Schwiep", "Jilong Wu", "Jing Huang", "Jing Li", "Jing Yu Koh", "Jing Zhang", "Jingxiang Chen", "Jingyi Yang", "Jingyue Shen", "Jinho Hwang", "Jinxi Guo", "Jiwan Khatiwada", "Joanna Bitton", "Joe Li", "Joe Quanaim", "Joel Beales", "Johan Schuijt", "John Chang", "John Quan", "Johnnie Chan", "Jon Shepard", "Jona Harris", "Jonah Rubin", "Jonathan Janzen", "Jonathan Kaldor", "Jorge Lopez Silva", "Jose Leitao", "Joseph Greer", "Joseph Moon", "Joseph Rocca", "Joseph Tighe", "Josh Fromm", "Joshua Deng", "Joshua Fernandes", "Joshua Saxe", "Joyce Zheng", "Juan Pino", "Julien Prigent", "Jun Chen", "Junjiao Tian", "Junjie Qi", "Junjie Wang", "Junteng Jia", "Kade Baker", "Kai Londenberg", "Kai Wang", "Kainan Peng", "Kaiyan Peng", "Kaiyue Yang", "Kalyan Vasudev Alwala", "Kam Hou Yu", "Kanika Narang", "Karan Chadha", "Karan Sikka", "Karen Zhang", "Karina Schuberts", "Karishma Mandyam", "Karthik Abinav Sankararaman", "Karthik Padthe", "Karthik Prasad", "Karthik Sivakumar", "Kartikeya Upasani", "Kate Plawiak", "Kate Saenko", "Kate\u0159ina \u017dmol\u00edkov\u00e1", "Kathryn Stadler", "Kathy Matosich", "Katie Doulgass", "Kaveh Hassani", "Kay Ji", "Ke Li", "Kenneth Heafield", "Kenny Yu", "Keqian Li", "Kevin Chih-Yao Ma", "Kevin Hannan", "Keyu Man", "Kezhen Chen", "Khalid El-Arini", "Khrystyna Hutsulyak", "Kieran Nash", "Kiran Jagadeesh", "Kody Bartelt", "Konstantin Topaloglou-Mundy", "Konstantinos Chatziioannou", "Konstantinos Karanasos", "Konstantinos Vougioukas", "Kostas Tsiampouris", "Kristen Hamill", "Kristy Choi", "Krithika Iyer", "Kshitiz Malik", "Kuenley Chiu", "Kun Huang", "Kunal Bhalla", "Kunal Chawla", "Kunpeng Li", "Kushal Lakhotia", "Kyle Monk", "Lakshya Garg", "Lalit Chourey", "Lars Hamre", "Laura Gustafson", "Lauren Deason", "Laurence Rouesnel", "Laurens van der Maaten", "Lavender A", "Lawrence Chen", "Lawrence Jang", "Leandro Silva", "Leda Sari", "Lee Hetherington", "Lei Zhang", "Leiyu Zhao", "Lele Chen", "Leo Chenghui Li", "Leon Yang", "Leon Zhan", "Levi Corallo", "Liang Tan", "Licheng Yu", "Lijuan Liu", "Lilach Mor", "Lincoln Lin", "Linfeng Li", "Lisa Titus", "Liz Jenkins", "Lovish Madaan", "Lu Fang", "Lu Yuan", "Lucas Nava", "Lucas Pasqualin", "Lucas Switzer", "Lucia Fang", "Lucy Sun", "Luka Tadic", "Lukas Blecher", "Lukas Landzaat", "Luxin Zhang", "Madhavi Rao", "Madian Khabsa", "Mahalia Miller", "Mahendra Kariya", "Mahesh Pasupuleti", "Mahi Luthra", "Manaal Faruqui", "Manav Avlani", "Manchen Wang", "Mannat Singh", "Manohar Paluri", "Manoj Chakkaravarthy", "Manoj Nair", "Maquelle Tiffany", "Marcin Pawlowski", "Marcus Wu", "Maria Lomeli", "Mario Consuegra", "Marion Boiteux", "Marios Andreas Galanis", "Marshall Chen", "Martin Gleize", "Maryam Fazel-Zarandi", "Matan Hasson", "Mathew Oldham", "Mathieu Rita", "Matt Dordal", "Matt Setzler", "Matt Staats", "Matt Staats", "Matt Wilde", "Matthew Clark", "Matthew Grange", "Matthew Lennie", "Matthew Schmohl", "Max Raphael", "Maxim Naumov", "Maxim Samoylov", "Maxime Lecanu", "Maya Pavlova", "Md Taha Bin Jawaid", "Meghan Keneally", "Melanie Kambadur", "Meng Zhang", "Mengchen Liu", "Mengdi Lin", "Mengjiao Wang", "Mervyn Abraham", "Miao Liu", "Michael Au-Yeung", "Michael Feldergraf", "Michael Man", "Michael Matheny", "Michael Suo", "Michael Tontchev", "Michel Meyer", "Michelle Ma", "Mihir Patel", "Mihir Sanjay Kale", "Mik Vyatskov", "Mikayla Alexander", "Mike Andersland", "Mike Clark", "Mike Lewis", "Mike Li", "Mike Macey", "Mike Macey", "Mike Seltzer", "Mikel Jimenez Fernandez", "Mikhail Antonov", "Mikhail Plekhanov", "Milan Zhou", "Min Si", "Ming Qiao", "Mingbo Ma", "Mingjun Zhang", "Mingyi Liang", "Miquel Jubert Hermoso", "Mirac Suzgun", "Mirjam Skarica", "Mitesh Kumar Singh", "Mohammad Kabbani", "Mohammad Rastegari", "Mona Sarantakos", "Monica Sim", "Monika Gangapuram", "Mor Moshe", "Morrie Doulaty", "Morvarid Metanat", "Moya Chen", "Mrinal Kumar", "Munish Bansal", "Murali Ramarao", "Na Li", "Nadav Azaria", "Nahiyan Malik", "Naman Goyal", "Nancy Vargas Balderas", "Nanshu Wang", "Naoyuki Kanda", "Natalia Gimelshein", "Natalia Neverova", "Nathan Aclander", "Natt Sithiviraporn", "Navneet Madhu Kumar", "Ned Newton", "Neeraj Bahl", "Negar Ghorbani", "Neil Patel", "Neta-lee Golan", "Nicholas Longenbaugh", "Nick Egebo", "Nikhil Johri", "Nikhil Mehta", "Nikhil Naik", "Niko Moritz", "Nikolay Bashlykov", "Nikolay Bogoychev", "Nikolay Pavlovich Laptev", "Niladri Chatterji", "Nile Jones", "Nimish Shah", "Ning Dong", "Ning Li", "Ning Li", "Ning Zhang", "Nishant Yadav", "Noam Paz", "Norman Cheng", "Norman Cheng", "Olaoluwa Adesanya", "Oleg Repin", "Oleksandr Maksymets", "Omkar Salpekar", "Omri Harosh", "Onkar Pednekar", "Onur \u00c7elebi", "Oran Gafni", "Oren Edinger", "Osama Hanna", "Owais Khan Mohammed", "Ozlem Kalinli", "Paden Tomasello", "Pankaj Singh", "Paola Quevedo", "Parag Jain", "Paria Rashidinejad", "Parker Tooley", "Parth Parekh", "Parth Thakkar", "Parvin Taheri", "Pasan Hapuarachchi", "Pascal Kesseli", "Patrick Alrassy", "Paulo de Rezende Pinatti", "Pavan Balaji", "Pawan Sisodiya", "Pedro Jose Ferreira Moreira", "Pedro Rittner", "Pedro Valenzuela", "Peize Sun", "Peizhao Zhang", "Peng-Jen Chen", "Pengchao Wang", "Pengchuan Zhang", "Pengwei Li", "Petar Vasic", "Peter Carras", "Peter Ney", "Peter Weng", "Petru Dumea", "Phil Hayes", "Philip Woods", "Pierre Andrews", "Pierre M\u00e9nard", "Ping-Hao Wu", "Pingchuan Liu", "Piotr Dollar", "Plamen Dzhelepov", "Polina Zvyagina", "Posten A", "Prabhav Agrawal", "Pradhapan Rajendran", "Pradyot Prakash", "Prajjwal Bhargava", "Pramono", "Pranay Shah", "Pranshu Dave", "Prash Jain", "Pratik Dubal", "Praveen Gollakota", "Praveen Krishnan", "Pritish Yuvraj", "Projjal Ghosh", "Punit Singh Koura", "Puxin Xu", "Qi Qi", "Qi Zhou", "Qian Guan", "Qian Sun", "Qiang Liu", "Qing He", "Qinqing Zheng", "Qirui Yang", "Qizhen Guo", "Quanzeng You", "Quentin Carbonneaux", "Quentin Carbonneaux", "Quentin Duval", "Quintin Fettes", "Rachad Alao", "Rachel Batish", "Rachel Guo", "Rachel Rodriguez", "Radhika Bhargava", "Rafael Asuncion", "Raghotham Murthy", "Rahul Dutta", "Rahul Jha", "Rahul Kindi", "Rahul Mitra", "Raj Ganapathy", "Raj Shah", "Rajarshi Das", "Rajat Shrivastava", "Rajesh Nishtala", "Ramakant Shankar", "Raman Shukhau", "Ramon Calderer", "Rangaprabhu Parthasarathy", "Ranjan Subramanian", "Raphael Bensadoun", "Rares Bostan", "Rashnil Chaturvedi", "Ravi Agrawal", "Ray Gao", "Raymond Li", "Rebecca Kogen", "Ricardo Juan Palma Duran", "Ricardo Silveira Cabral", "Richard Lee", "Richard Yuanzhe Pang", "Riddhish Bhalodia", "Riham Mansour", "Rishabh Singh", "Rishi Godugu", "Ritun Patney", "Rob Boyle", "Robbie Goldfarb", "Robert Caldwell", "Robert Kuo", "Roberta Raileanu", "Robin Battey", "Robin Sharma", "Rochit Sapra", "Rocky Wang", "Rodolfo Granata", "Rodrigo De Castro", "Rodrigo Paim", "Rohan Maheshwari", "Rohan Varma", "Rohit Girdhar", "Rohit Patel", "Roshan Sumbaly", "Roy Sheaffer", "Ruan Silva", "Ruben Rodriguez Buchillon", "Rui Hou", "Ruiming Xie", "Ruslan Mavlyutov", "Ruslan Semenov", "Rustam Dinov", "Ruxiao Bao", "Ryan Fox", "Ryan Kilpatrick", "Ryan Kwan", "Ryan Lim", "Ryan Smith", "Saaketh Narayan", "Sabrina Qiao", "Sachin Mehta", "Sachin Siby", "Sagar Jain", "Saghar Hosseini", "Sagie Gur-Ari", "Sahana Chennabasappa", "Sahin Geyik", "Sai Jayesh Bondu", "Sai Mounika Chowdhary Nekkalapudi", "Saif Hasan", "Saisuke Okabayashi", "Saketh Rambhatla", "Salil Sawhney", "Sam Dunster", "Sam Zhao", "Saman Keon", "Samaneh Azadi", "Sameet Sapra", "Samuel Dooley", "Samyak Datta", "Sandeep Parab", "Sang Michael Xie", "Sanjay Singh", "Sanyuan Chen", "Sara Behn", "Sara Khodeir", "Sarah Shirazyan", "Sargun Dhillon", "Sarunya Pumma", "Sasha Sidorov", "Saskia Adaime", "Saurabh Khanna", "Sayem Wani", "Scott Brenton", "Sean Bell", "Sean Kelly", "Sean Koger", "Sean Nunley", "Sean Perry", "Sebastian Caicedo", "Sebastian Dahlgren", "Sebastian Ruder", "Seiji Yamamoto", "Selam Mehretu", "Selvan Sunitha Ravi", "Sen Lyu", "Senthil Chellapan", "Serafeim Mellos", "Sergey Edunov", "Sergey Royt", "Shaina Cohen", "Shangfu Peng", "Shannon Adams", "Shaoliang Nie", "Sharadh Ramaswamy", "Sharan Narang", "Shashank Pisupati", "Shashi Gandham", "Shaun Lim", "Shaun Lindsay", "Sheena Artrip", "Shelly Sheynin", "Shen Yan", "Sheng Feng", "Sheng Shen", "Shengbao Zheng", "Shenghao Lin", "Shengjie Bi", "Shengxin Cindy Zha", "Shengye Wan", "Shengyi Qian", "Shengyong Cai", "Shengzhi Shao", "Shervin Shahidi", "Shikai Li", "Shimon Bernholtz", "Shiqi Wang", "Shishir G. Patil", "Shiv Verma", "Shiva Shankar P", "Shiyang Chen", "Sho Yaida", "Shoubhik Debnath", "Shreyas Siravara", "Shruti Bhosale", "Shuang Ma", "Shun Zhang", "Shuo Tang", "Shuqiang Zhang", "Shuyan Zhou", "Sicong Che", "Sidd Srinivisan", "Siddharth Bhattacharya", "Siddharth Patki", "Sijia Chen", "Sili Chen", "Simon Vandenhende", "Simone Merello", "Sinong Wang", "Sivan Barzily", "Sixian Yi", "Siyu Lin", "SK Bong", "Sky Yin", "Sneha Agarwal", "Sneha Agarwal", "Soerian Lieve", "Soji Sajuyigbe", "Song Jiang", "Songlin Li", "Sonia Kim", "Sopan Khosla", "Soumi Maiti", "Spencer Whitman", "Sravya Popuri", "Sreen Tallam", "Srinivas Vaidyanathan", "Srinivas Vaidyanathan", "Sten Sootla", "Stephane Collot", "Stephanie Ding", "Stephen Chen", "Steven Cai", "Suchin Gururangan", "Sudarshan Govindaprasad", "Sue Young", "Suganthi Dewakar", "Sujan Kumar Gonugondla", "Sujeet Bhandari", "Suman Gumudavelli", "Suman Gumudavelli", "Sumit Gupta", "Summer Deng", "Sungmin Cho", "Suresh Ganapathy", "Surjyendu Dhal", "Susan Fedynak", "Susana Contrera", "Suyoun Kim", "Sylvestre Rebuffi", "Takshak Chahande", "Tamar Herman", "Tan Li", "Tao Xu", "Tara Fowler", "Tarek Sheasha", "Tarun Anand", "Tarun Kalluri", "Tarun Singh", "Tatiana Shavrina", "Ted Li", "Teja Rao", "Tejas Patil", "Teng Li", "Thach Bui", "Thai Quach", "Thamer Alharbash", "Thanh Vinh Vo", "Thawan Kooburat", "Thilo Koehler", "Thomas Georgiou", "Thomas Scialom", "Tian Ye", "Tianhe Li", "Tianjun Zhang", "Tianyu Li", "Tijmen Blankevoort", "Timon Willi", "Timothy Chou", "Timothy Leung", "TJ Lee", "Todor Mihaylov", "Tom Heatwole", "Tong Xiao", "Tony Cao", "Tony Lee", "Trang Le", "Tristan Rice", "Tsz Kei Serena Chan", "Tuan Tran", "Tudor Tiplea", "Tyler Baumgartner", "Uday Savagaonkar", "Ujjwal Karn", "Ulises Martinez Araiza", "Umar Farooq", "Uriel Cohen", "Usman Sharif", "Utkarsh Murarka", "Van Phung", "Varun Joginpalli", "Varun Saravagi", "Vasu Sharma", "Vasudha Viswamurthy", "Vedanuj Goswami", "Vedika Seth", "Venkat Ramesh", "Venkat Ramesh", "Vibhor Gupta", "Victoria Montanez", "Vidhya Natarajan", "Vidya Sarma", "Vignesh Ramanathan", "Viktor Kerkez", "Vinay Rao", "Vincent Gonguet", "Vincent Mauge", "Virginie Do", "Vish Vogeti", "Vishrav Chaudhary", "Viswesh Sankaran", "V\u00edtor Albiero", "Vivek Miglani", "Vivek Pai", "Vlad Cojanu", "Vlad Shubin", "Vlad Tiberiu Mihailescu", "Vladan Petrovic", "Vladimir Ivanov", "Vladislav Vorotilov", "Vrushali Bhutada", "Wai I Ng", "Wei Cheng", "Wei Sun", "Wei Tu", "Wei Wei", "Wei Zhou", "Wei-Ning Hsu", "Weiwei Chu", "Weizhe Yuan", "Wenchen Wang", "Wenjun Zhao", "Wenwen Jiang", "Wenyin Fu", "Wenzhe Jiang", "Whitney Meers", "Will Constable", "Will Wang", "William R. Wong", "Xavier Martinet", "Xi Victoria Lin", "Xi Yan", "Xi Yin", "Xian Li", "Xianfeng Rui", "Xianjun Yang", "Xiaocheng Tang", "Xiaodong Wang", "Xiaofang Wang", "Xiaolan Wang", "Xiaoliang Dai", "Xiaoliang Peng", "Xiaopeng Li", "Xiaozhu Meng", "Xibei Zhang", "Xide Xia", "Xin Jin", "xinbo Gao", "Xinfeng Xie", "Xingyi Zhou", "Xu Ma", "Xuan Ju", "Xuanyi Zhao", "Xubo Liu", "Xuchao Jia", "Xuedong Zhang", "Xuefei Cao", "Xuewei Wang", "Xuewei Wu", "Xunnan Xu", "Xutai Ma", "Xuyang Wang", "Yan Cui", "Yang Chen", "Yang Li", "Yang Shu", "Yang Xia", "Yanjun Chen", "Yanjun Zhou", "Yash Mehta", "Yash Patel", "Yash Tekena", "Yashesh Gaur", "Yasmine Babaei", "Yaxuan Zhou", "Ye Hu", "Ye Qi", "Yejin Lee", "Yeming Wen", "Yen-Cheng Liu", "Yexin Bruce Wu", "Yi Pan", "Yi Yang", "Yi-Hui Lin", "Yifan Wang", "Yifan Wu", "Yifan Yang", "Yifei Huang", "Yiftah Ben Aharon", "Yilin Yang", "Yiling You", "Ying Xu", "Ying Zhang", "Yingquan Yuan", "Yingru Liu", "Yingyi Ma", "Yining Yang", "Yiting Lu", "Yonatan Komornik", "Yongjie Lin", "Yoni Goyhman", "Yossi Moran Mamo", "Youngjin Nam", "Yu Wang", "Yu Lu", "Yu Zhao", "Yu-Ho Hsieh", "Yu-Jung Lo", "Yuandong Tian", "Yuanhan Zhang", "Yuanhao Xiong", "Yuanshun Yao", "Yuchen Hao", "Yuchen Zhang", "Yuchuan Li", "Yue Cao", "Yue Yu", "Yue Zhao", "Yuhan Guo", "Yuhao Wang", "Yuheng Huang", "Yujie Lu", "Yujun Shi", "Yulun Wang", "Yun He", "Yun Wang", "Yundi Qian", "Yunfan Wang", "Yunhao Tang", "Yuning Mao", "Yunlu Li", "Yuqi Dai", "Yuriy Hulovatyy", "Yushi Hu", "Yuxuan Sun", "Zach Rait", "Zach Wentz", "Zacharie Delpierre Coudert", "Zachary Collins", "Zahra Hankir", "Zecheng He", "Zeeshan Ahmed", "Zeeshan Ahmed", "Zef RosnBrick", "Zhan Shu", "Zhanna Rohalska", "Zhaoduo Wen", "Zhe Liu", "Zhe Liu", "Zhen Qiao", "Zhenggang Xu", "Zhengwen Zhou", "Zhengxing Chen", "Zhenyu Tang", "Zhichen Wu", "Zhicheng Ouyang", "Zhihong Lei", "Zhipeng Hong", "Zhiping Xiu", "Zhiwei Zhao", "Zhong Meng", "Zhou Jin", "Zhouhao Zeng", "Zichang Liu", "Zihang Meng", "Zihuan Qiao", "Zinnia Zheng", "Zixi Qi", "Ziyi Luo", "Zoe Foulkes Birkhead", "Zoey Sun", "Zohar Achdut"], "title": "The Llama 4 Herd: Architecture, Training, Evaluation, and Deployment Notes", "comment": "15 pages", "summary": "This document consolidates publicly reported technical details about Metas Llama 4 model family. It summarizes (i) released variants (Scout and Maverick) and the broader herd context including the previewed Behemoth teacher model, (ii) architectural characteristics beyond a high-level MoE description covering routed/shared-expert structure, early-fusion multimodality, and long-context design elements reported for Scout (iRoPE and length generalization strategies), (iii) training disclosures spanning pre-training, mid-training for long-context extension, and post-training methodology (lightweight SFT, online RL, and lightweight DPO) as described in release materials, (iv) developer-reported benchmark results for both base and instruction-tuned checkpoints, and (v) practical deployment constraints observed across major serving environments, including provider-specific context limits and quantization packaging. The manuscript also summarizes licensing obligations relevant to redistribution and derivative naming, and reviews publicly described safeguards and evaluation practices. The goal is to provide a compact technical reference for researchers and practitioners who need precise, source-backed facts about Llama 4.", "AI": {"tldr": "\u672c\u6587\u6863\u6c47\u603b\u4e86Meta Llama 4\u6a21\u578b\u5bb6\u65cf\u7684\u516c\u5f00\u6280\u672f\u7ec6\u8282\uff0c\u6db5\u76d6\u67b6\u6784\u3001\u8bad\u7ec3\u65b9\u6cd5\u3001\u57fa\u51c6\u6d4b\u8bd5\u7ed3\u679c\u53ca\u90e8\u7f72\u9650\u5236\uff0c\u65e8\u5728\u4e3a\u7814\u7a76\u4eba\u5458\u63d0\u4f9b\u7cbe\u51c6\u6280\u672f\u53c2\u8003\u3002", "motivation": "\u4e3a\u7814\u7a76\u4eba\u5458\u548c\u4ece\u4e1a\u8005\u63d0\u4f9b\u5173\u4e8eLlama 4\u6a21\u578b\u5bb6\u65cf\u7684\u7cbe\u786e\u3001\u6709\u636e\u53ef\u67e5\u7684\u6280\u672f\u4fe1\u606f\u3002", "method": "\u6574\u7406\u5e76\u5206\u6790\u516c\u5f00\u53d1\u5e03\u7684Llama 4\u6280\u672f\u6587\u6863\uff0c\u5305\u62ec\u6a21\u578b\u53d8\u4f53\u3001\u67b6\u6784\u8bbe\u8ba1\u3001\u8bad\u7ec3\u6d41\u7a0b\u3001\u57fa\u51c6\u7ed3\u679c\u53ca\u90e8\u7f72\u7ea6\u675f\u3002", "result": "\u7cfb\u7edf\u5f52\u7eb3\u4e86Llama 4\u7684\u6a21\u578b\u7ed3\u6784\u3001\u8bad\u7ec3\u7b56\u7565\u3001\u6027\u80fd\u8868\u73b0\u4e0e\u5b9e\u9645\u90e8\u7f72\u9650\u5236\uff0c\u5e76\u603b\u7ed3\u4e86\u76f8\u5173\u8bb8\u53ef\u4e0e\u5b89\u5168\u8bc4\u4f30\u5b9e\u8df5\u3002", "conclusion": "\u672c\u6587\u4e3aLlama 4\u63d0\u4f9b\u4e86\u7d27\u51d1\u4e14\u6743\u5a01\u7684\u6280\u672f\u53c2\u8003\uff0c\u4fbf\u4e8e\u7814\u7a76\u8005\u5feb\u901f\u638c\u63e1\u5176\u6838\u5fc3\u7279\u6027\u4e0e\u4f7f\u7528\u6761\u4ef6\u3002"}}
{"id": "2601.12886", "categories": ["cs.MA", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.12886", "abs": "https://arxiv.org/abs/2601.12886", "authors": ["Christoph Wittner"], "title": "Communication Methods in Multi-Agent Reinforcement Learning", "comment": "12 pages, 2 figures", "summary": "Multi-agent reinforcement learning is a promising research area that extends established reinforcement learning approaches to problems formulated as multi-agent systems. Recently, a multitude of communication methods have been introduced to this field to address problems such as partially observable environments, non-stationarity, and exponentially growing action spaces. Communication further enables efficient cooperation among all agents interacting in an environment. This work aims at providing an overview of communication techniques in multi-agent reinforcement learning. By an in-depth analysis of 29 publications on this topic, the strengths and weaknesses of explicit, implicit, attention-based, graph-based, and hierarchical/role-based communication are evaluated. The results of this comparison show that there is no general, optimal communication framework for every problem. On the contrary, the choice of communication depends heavily on the problem at hand. The comparison also highlights the importance of communication methods with low computational overhead to enable scalability to environments where many agents interact. Finally, the paper discusses current research gaps, emphasizing the need for standardized benchmarking of system-level metrics and improved robustness under realistic communication conditions to enhance the real-world applicability of these approaches.", "AI": {"tldr": "\u672c\u6587\u7efc\u8ff0\u4e86\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u901a\u4fe1\u6280\u672f\uff0c\u5206\u679029\u7bc7\u6587\u732e\u540e\u6307\u51fa\u6ca1\u6709\u901a\u7528\u6700\u4f18\u6846\u67b6\uff0c\u9700\u4f9d\u95ee\u9898\u9009\u62e9\u65b9\u6cd5\uff0c\u5e76\u5f3a\u8c03\u4f4e\u8ba1\u7b97\u5f00\u9500\u4e0e\u73b0\u5b9e\u9c81\u68d2\u6027\u7684\u91cd\u8981\u6027\u3002", "motivation": "\u4e3a\u89e3\u51b3\u90e8\u5206\u53ef\u89c2\u6d4b\u3001\u975e\u5e73\u7a33\u6027\u53ca\u52a8\u4f5c\u7a7a\u95f4\u7206\u70b8\u7b49\u95ee\u9898\uff0c\u63a8\u52a8\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u4e2d\u9ad8\u6548\u534f\u4f5c\uff0c\u9700\u7cfb\u7edf\u68b3\u7406\u73b0\u6709\u901a\u4fe1\u65b9\u6cd5\u4f18\u52a3\u3002", "method": "\u5bf929\u7bc7\u76f8\u5173\u8bba\u6587\u8fdb\u884c\u6df1\u5ea6\u5206\u6790\uff0c\u6bd4\u8f83\u663e\u5f0f\u3001\u9690\u5f0f\u3001\u6ce8\u610f\u529b\u3001\u56fe\u7ed3\u6784\u53ca\u5206\u5c42/\u89d2\u8272\u901a\u4fe1\u7b49\u4e94\u7c7b\u65b9\u6cd5\u3002", "result": "\u53d1\u73b0\u901a\u4fe1\u65b9\u6cd5\u9009\u62e9\u9ad8\u5ea6\u4f9d\u8d56\u5177\u4f53\u95ee\u9898\uff0c\u4e14\u4f4e\u8ba1\u7b97\u5f00\u9500\u5bf9\u591a\u667a\u80fd\u4f53\u6269\u5c55\u6027\u81f3\u5173\u91cd\u8981\uff1b\u5f53\u524d\u7814\u7a76\u7f3a\u4e4f\u6807\u51c6\u5316\u8bc4\u6d4b\u548c\u73b0\u5b9e\u9c81\u68d2\u6027\u3002", "conclusion": "\u672a\u6765\u5e94\u805a\u7126\u7cfb\u7edf\u7ea7\u6307\u6807\u6807\u51c6\u5316\u4e0e\u73b0\u5b9e\u901a\u4fe1\u6761\u4ef6\u4e0b\u9c81\u68d2\u6027\u63d0\u5347\uff0c\u4ee5\u589e\u5f3a\u65b9\u6cd5\u7684\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2601.12298", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2601.12298", "abs": "https://arxiv.org/abs/2601.12298", "authors": ["Ye Lin", "Chao Fang", "Xiaoyong Song", "Qi Wu", "Anying Jiang", "Yichuan Bai", "Li Du"], "title": "CD-PIM: A High-Bandwidth and Compute-Efficient LPDDR5-Based PIM for Low-Batch LLM Acceleration on Edge-Device", "comment": "To appear in 2026 Design, Automation and Test in Europe Conference (DATE 2026)", "summary": "Edge deployment of low-batch large language models (LLMs) faces critical memory bandwidth bottlenecks when executing memory-intensive general matrix-vector multiplications (GEMV) operations. While digital processing-in-memory (PIM) architectures promise to accelerate GEMV operations, existing PIM-equipped edge devices still suffer from three key limitations: limited bandwidth improvement, component under-utilization in mixed workloads, and low compute capacity of computing units (CUs). In this paper, we propose CD-PIM to address these challenges through three key innovations. First, we introduce a high-bandwidth compute-efficient mode (HBCEM) that enhances bandwidth by dividing each bank into four pseudo-banks through segmented global bitlines. Second, we propose a low-batch interleaving mode (LBIM) to improve component utilization by overlapping GEMV operations with GEMM operations. Third, we design a compute-efficient CU that performs enhanced GEMV operations in a pipelined manner by serially feeding weight data into the computing core. Forth, we adopt a column-wise mapping for the key-cache matrix and row-wise mapping for the value-cache matrix, which fully utilizes CU resources. Our evaluation shows that compared to a GPU-only baseline and state-of-the-art PIM designs, our CD-PIM achieves 11.42x and 4.25x speedup on average within a single batch in HBCEM mode, respectively. Moreover, for low-batch sizes, the CD-PIM achieves an average speedup of 1.12x in LBIM compared to HBCEM.", "AI": {"tldr": "CD-PIM\u662f\u4e00\u79cd\u9762\u5411\u8fb9\u7f18\u8bbe\u5907\u7684\u65b0\u578bPIM\u67b6\u6784\uff0c\u901a\u8fc7\u9ad8\u5e26\u5bbd\u8ba1\u7b97\u6a21\u5f0f\u3001\u4f4e\u6279\u6b21\u4ea4\u9519\u6a21\u5f0f\u548c\u9ad8\u6548\u8ba1\u7b97\u5355\u5143\u8bbe\u8ba1\uff0c\u663e\u8457\u63d0\u5347LLM\u63a8\u7406\u4e2dGEMV\u64cd\u4f5c\u7684\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u8fb9\u7f18\u8bbe\u5907\u90e8\u7f72LLM\u65f6\u56e0\u5185\u5b58\u5e26\u5bbd\u74f6\u9888\u5bfc\u81f4\u7684GEMV\u8fd0\u7b97\u6548\u7387\u4f4e\u4e0b\u95ee\u9898\uff0c\u540c\u65f6\u514b\u670d\u73b0\u6709PIM\u67b6\u6784\u5728\u5e26\u5bbd\u3001\u8d44\u6e90\u5229\u7528\u7387\u548c\u8ba1\u7b97\u80fd\u529b\u65b9\u9762\u7684\u4e0d\u8db3\u3002", "method": "\u63d0\u51faHBCEM\u6a21\u5f0f\uff08\u5206\u6bb5\u5168\u5c40\u4f4d\u7ebf\u5b9e\u73b0\u4f2a\u94f6\u884c\uff09\u3001LBIM\u6a21\u5f0f\uff08GEMV\u4e0eGEMM\u64cd\u4f5c\u91cd\u53e0\uff09\u3001\u6d41\u6c34\u7ebf\u5f0f\u8ba1\u7b97\u5355\u5143\u53ca\u952e\u503c\u7f13\u5b58\u77e9\u9635\u884c\u5217\u6620\u5c04\u7b56\u7565\uff0c\u4f18\u5316\u5e26\u5bbd\u4e0e\u8d44\u6e90\u5229\u7528\u3002", "result": "\u76f8\u6bd4\u7eafGPU\u57fa\u7ebf\u548c\u5148\u8fdbPIM\u8bbe\u8ba1\uff0cCD-PIM\u5728\u5355\u6279\u6b21\u4e0b\u5206\u522b\u5b9e\u73b011.42\u500d\u548c4.25\u500d\u5e73\u5747\u52a0\u901f\uff1b\u4f4e\u6279\u6b21\u573a\u666f\u4e0bLBIM\u76f8\u8f83HBCEM\u518d\u83b71.12\u500d\u63d0\u901f\u3002", "conclusion": "CD-PIM\u6709\u6548\u7f13\u89e3\u8fb9\u7f18LLM\u63a8\u7406\u4e2d\u7684\u5185\u5b58\u74f6\u9888\uff0c\u5728\u5e26\u5bbd\u3001\u5229\u7528\u7387\u548c\u8ba1\u7b97\u6548\u7387\u4e09\u65b9\u9762\u53d6\u5f97\u7a81\u7834\uff0c\u4e3a\u8fb9\u7f18\u667a\u80fd\u63d0\u4f9b\u9ad8\u6027\u80fd\u4f4e\u529f\u8017\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2601.11589", "categories": ["cs.DC", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.11589", "abs": "https://arxiv.org/abs/2601.11589", "authors": ["Jianshu She", "Zonghang Li", "Hongchao Du", "Shangyu Wu", "Wenhao Zheng", "Eric Xing", "Zhengzhong Liu", "Huaxiu Yao", "Jason Xue", "Qirong Ho"], "title": "PLA-Serve: A Prefill-Length-Aware LLM Serving System", "comment": "12 pages", "summary": "PLA-Serve identifies and disaggregates requests with different prompt lengths in LLM serving to reduce TTFT latency. While recent systems have decoupled the prefill and decode stages to improve throughput, they still rely on unified scheduling policies that fail to adapt to heterogeneous workload characteristics. We observe that prompt-length variations lead to distinct performance bottlenecks, motivating an adaptive scheduling strategy. PLA-Serve disaggregates multi-turn long-prefill requests from short-prefill ones and introduces a length-aware smart batching mechanism for short-prefill workloads. It adopts a dual-queue design that supports temporal disaggregation on a single prefill instance or spatial disaggregation across multiple instances. For short-prefill batches, a batch waiting window and CUDA Graph-based clustering mitigate interference from heterogeneous computation, reducing batching delay and lowering average latency. In real multi-turn workloads, PLA-Serve reduces prefill latency by over 30% compared to vanilla SGLang under prefill**--**decode disaggregation, and further decreases SLO violations by 28% in multi-instance deployments with vanilla data-parallel configuration. Compared to the SGLang router with load balancing, it further lowers SLO violations by 12% in multi-GPU settings. Under high concurrency and mixed-request scenarios, PLA-Serve improves request throughput by 35% serving Qwen2.5-32B model for prefill instance, demonstrating its effectiveness in optimizing heterogeneous LLM serving workloads.", "AI": {"tldr": "PLA-Serve\u901a\u8fc7\u6309\u63d0\u793a\u957f\u5ea6\u89e3\u8026\u8bf7\u6c42\u5e76\u91c7\u7528\u667a\u80fd\u6279\u5904\u7406\uff0c\u663e\u8457\u964d\u4f4eLLM\u670d\u52a1\u4e2d\u7684TTFT\u5ef6\u8fdf\u548cSLO\u8fdd\u89c4\u7387\uff0c\u63d0\u5347\u541e\u5410\u91cf\u3002", "motivation": "\u7edf\u4e00\u8c03\u5ea6\u7b56\u7565\u65e0\u6cd5\u9002\u5e94\u4e0d\u540c\u63d0\u793a\u957f\u5ea6\u5bfc\u81f4\u7684\u6027\u80fd\u74f6\u9888\uff0c\u9700\u9488\u5bf9\u6027\u4f18\u5316\u3002", "method": "\u91c7\u7528\u53cc\u961f\u5217\u8bbe\u8ba1\u5b9e\u73b0\u65f6\u7a7a\u89e3\u8026\uff0c\u5bf9\u77ed\u63d0\u793a\u5f15\u5165\u957f\u5ea6\u611f\u77e5\u6279\u5904\u7406\u4e0eCUDA\u56fe\u805a\u7c7b\u51cf\u5c11\u5e72\u6270\u3002", "result": "\u76f8\u6bd4SGLang\uff0c\u9884\u586b\u5145\u5ef6\u8fdf\u964d\u4f4e30%\uff0c\u591a\u5b9e\u4f8b\u4e0bSLO\u8fdd\u89c4\u51cf\u5c1128%\uff0c\u9ad8\u5e76\u53d1\u6df7\u5408\u573a\u666f\u541e\u5410\u91cf\u63d0\u534735%\u3002", "conclusion": "PLA-Serve\u6709\u6548\u4f18\u5316\u5f02\u6784LLM\u670d\u52a1\u8d1f\u8f7d\uff0c\u663e\u8457\u6539\u5584\u5ef6\u8fdf\u3001SLO\u5408\u89c4\u6027\u4e0e\u7cfb\u7edf\u541e\u5410\u80fd\u529b\u3002"}}
{"id": "2601.11672", "categories": ["cs.SE", "eess.SY"], "pdf": "https://arxiv.org/pdf/2601.11672", "abs": "https://arxiv.org/abs/2601.11672", "authors": ["Deepak Babu Piskala"], "title": "From Everything-is-a-File to Files-Are-All-You-Need: How Unix Philosophy Informs the Design of Agentic AI Systems", "comment": null, "summary": "A core abstraction in early Unix systems was the principle that 'everything is a file', enabling heterogeneous devices and kernel resources to be manipulated via uniform read/write interfaces. This paper explores how an analogous unification is emerging in contemporary agentic AI. We trace the evolution from Unix to DevOps, Infrastructure-as-Code, and finally autonomous software agents, highlighting how file-like abstractions and code-based specifications collapse diverse resources into consistent, composable interfaces. The resulting perspective suggests that adopting file- and code-centric interaction models may enable agentic systems that are more maintainable, auditable, and operationally robust.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u4e86\u5f53\u4ee3\u667a\u80fd\u4f53AI\u4e2d\u7c7b\u4f3cUnix\u2018\u4e00\u5207\u7686\u6587\u4ef6\u2019\u7684\u62bd\u8c61\u7edf\u4e00\u8d8b\u52bf\uff0c\u63d0\u51fa\u4ee5\u6587\u4ef6\u548c\u4ee3\u7801\u4e3a\u4e2d\u5fc3\u7684\u4ea4\u4e92\u6a21\u578b\u53ef\u63d0\u5347\u7cfb\u7edf\u7684\u53ef\u7ef4\u62a4\u6027\u4e0e\u9c81\u68d2\u6027\u3002", "motivation": "\u53d7Unix\u7cfb\u7edf\u2018\u4e00\u5207\u7686\u6587\u4ef6\u2019\u7406\u5ff5\u542f\u53d1\uff0c\u63a2\u7d22\u5982\u4f55\u5728\u667a\u80fd\u4f53AI\u4e2d\u5b9e\u73b0\u8d44\u6e90\u63a5\u53e3\u7684\u7edf\u4e00\u4e0e\u7b80\u5316\u3002", "method": "\u901a\u8fc7\u8ffd\u6eaf\u4eceUnix\u5230DevOps\u3001\u57fa\u7840\u8bbe\u65bd\u5373\u4ee3\u7801\u518d\u5230\u81ea\u4e3b\u8f6f\u4ef6\u667a\u80fd\u4f53\u7684\u6f14\u8fdb\u8def\u5f84\uff0c\u5206\u6790\u6587\u4ef6\u5f0f\u62bd\u8c61\u4e0e\u4ee3\u7801\u89c4\u8303\u5982\u4f55\u6574\u5408\u5f02\u6784\u8d44\u6e90\u3002", "result": "\u53d1\u73b0\u91c7\u7528\u6587\u4ef6\u4e0e\u4ee3\u7801\u4e3a\u4e2d\u5fc3\u7684\u4ea4\u4e92\u6a21\u578b\u6709\u52a9\u4e8e\u6784\u5efa\u66f4\u6613\u7ef4\u62a4\u3001\u53ef\u5ba1\u8ba1\u4e14\u8fd0\u884c\u7a33\u5065\u7684\u667a\u80fd\u4f53\u7cfb\u7edf\u3002", "conclusion": "\u6587\u4ef6\u4e0e\u4ee3\u7801\u62bd\u8c61\u5728\u667a\u80fd\u4f53AI\u4e2d\u7684\u63a8\u5e7f\uff0c\u6709\u671b\u5e26\u6765\u7c7b\u4f3cUnix\u65f6\u4ee3\u63a5\u53e3\u7edf\u4e00\u7684\u64cd\u4f5c\u4f18\u52bf\u3002"}}
{"id": "2601.12996", "categories": ["cs.MA", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.12996", "abs": "https://arxiv.org/abs/2601.12996", "authors": ["Shiyuan Li", "Yixin Liu", "Yu Zheng", "Mei Li", "Quoc Viet Hung Nguyen", "Shirui Pan"], "title": "OFA-MAS: One-for-All Multi-Agent System Topology Design based on Mixture-of-Experts Graph Generative Models", "comment": "Accepted by WWW 2026", "summary": "Multi-Agent Systems (MAS) offer a powerful paradigm for solving complex problems, yet their performance is critically dependent on the design of their underlying collaboration topology. As MAS become increasingly deployed in web services (e.g., search engines), designing adaptive topologies for diverse cross-domain user queries becomes essential. Current graph learning-based design methodologies often adhere to a \"one-for-one\" paradigm, where a specialized model is trained for each specific task domain. This approach suffers from poor generalization to unseen domains and fails to leverage shared structural knowledge across different tasks. To address this, we propose OFA-TAD, a one-for-all framework that generates adaptive collaboration graphs for any task described in natural language through a single universal model. Our approach integrates a Task-Aware Graph State Encoder (TAGSE) that filters task-relevant node information via sparse gating, and a Mixture-of-Experts (MoE) architecture that dynamically selects specialized sub-networks to drive node and edge prediction. We employ a three-stage training strategy: unconditional pre-training on canonical topologies for structural priors, large-scale conditional pre-training on LLM-generated datasets for task-topology mappings, and supervised fine-tuning on empirically validated graphs. Experiments across six diverse benchmarks show that OFA-TAD significantly outperforms specialized one-for-one models, generating highly adaptive MAS topologies. Code: https://github.com/Shiy-Li/OFA-MAS.", "AI": {"tldr": "OFA-TAD\u662f\u4e00\u4e2a\u901a\u7528\u6846\u67b6\uff0c\u901a\u8fc7\u5355\u4e00\u6a21\u578b\u4e3a\u4efb\u610f\u81ea\u7136\u8bed\u8a00\u63cf\u8ff0\u7684\u4efb\u52a1\u751f\u6210\u81ea\u9002\u5e94\u534f\u4f5c\u56fe\uff0c\u663e\u8457\u4f18\u4e8e\u4f20\u7edf\u4e13\u7528\u6a21\u578b\u3002", "motivation": "\u5f53\u524d\u57fa\u4e8e\u56fe\u5b66\u4e60\u7684\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u534f\u4f5c\u62d3\u6251\u8bbe\u8ba1\u65b9\u6cd5\u6cdb\u5316\u80fd\u529b\u5dee\uff0c\u65e0\u6cd5\u8de8\u57df\u5171\u4eab\u7ed3\u6784\u77e5\u8bc6\u3002", "method": "\u63d0\u51faTAGSE\u7f16\u7801\u5668\u4e0eMoE\u67b6\u6784\uff0c\u7ed3\u5408\u4e09\u9636\u6bb5\u8bad\u7ec3\u7b56\u7565\uff1a\u65e0\u6761\u4ef6\u9884\u8bad\u7ec3\u3001\u5927\u89c4\u6a21\u6761\u4ef6\u9884\u8bad\u7ec3\u548c\u76d1\u7763\u5fae\u8c03\u3002", "result": "\u5728\u516d\u4e2a\u591a\u6837\u5316\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cOFA-TAD\u663e\u8457\u8d85\u8d8a\u4e13\u7528\u6a21\u578b\uff0c\u80fd\u9ad8\u6548\u751f\u6210\u9ad8\u5ea6\u81ea\u9002\u5e94\u7684MAS\u62d3\u6251\u3002", "conclusion": "OFA-TAD\u6210\u529f\u5b9e\u73b0\u2018\u4e00\u901a\u767e\u901a\u2019\u7684\u534f\u4f5c\u62d3\u6251\u751f\u6210\uff0c\u63d0\u5347\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u5728\u8de8\u57df\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\u4e0e\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2601.12686", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2601.12686", "abs": "https://arxiv.org/abs/2601.12686", "authors": ["Rafi Zahedi", "Amin Zamani", "Rahul Anilkumar"], "title": "Best Practices for Large Load Interconnections: A North American Perspective on Data Centers", "comment": "Presented at CIGRE United States, and published by CIGRE", "summary": "Large loads are expanding rapidly across North America, led by data centers, cryptocurrency mining, hydrogen production facilities, and heavy-duty charging stations. Each class presents distinct electrical characteristics, but data centers are drawing particular attention as AI deployment drives unprecedented capacity growth. Their scale, duty cycles, and converter-dominated interfaces introduce new challenges for transmission interconnections, especially regarding disturbance behavior, steady-state performance, and operational visibility. This paper reviews best practices for large-load interconnections across North America, synthesizing utility and system operator guidelines into a coherent set of technical requirements. The approach combines handbook and manual analysis with cross-utility comparisons and an outlook on European directions. The review highlights requirements on power quality, telemetry, commissioning tests, and protection coordination, while noting gaps in ride-through specifications, load-variation management, and post-disturbance recovery targets. Building on these findings, the paper proposes practical guidance for developers and utilities.", "AI": {"tldr": "\u672c\u6587\u7efc\u8ff0\u5317\u7f8e\u5927\u578b\u8d1f\u8f7d\u5e76\u7f51\u6700\u4f73\u5b9e\u8df5\uff0c\u805a\u7126\u6570\u636e\u4e2d\u5fc3\u7b49\u65b0\u5174\u8d1f\u8f7d\u5e26\u6765\u7684\u6280\u672f\u6311\u6218\uff0c\u5e76\u63d0\u51fa\u5b9e\u7528\u6307\u5bfc\u5efa\u8bae\u3002", "motivation": "AI\u9a71\u52a8\u7684\u6570\u636e\u4e2d\u5fc3\u7b49\u5927\u578b\u8d1f\u8f7d\u5feb\u901f\u589e\u957f\uff0c\u5bf9\u7535\u7f51\u4e92\u8054\u5e26\u6765\u65b0\u6311\u6218\uff0c\u4e9f\u9700\u7edf\u4e00\u6280\u672f\u89c4\u8303\u3002", "method": "\u7efc\u5408\u5206\u6790\u624b\u518c\u3001\u8fd0\u8425\u5546\u6307\u5357\u53ca\u8de8\u7535\u529b\u516c\u53f8\u6bd4\u8f83\uff0c\u7ed3\u5408\u6b27\u6d32\u7ecf\u9a8c\u8fdb\u884c\u5f52\u7eb3\u3002", "result": "\u8bc6\u522b\u51fa\u73b0\u6709\u89c4\u8303\u5728\u7a7f\u8d8a\u80fd\u529b\u3001\u8d1f\u8f7d\u53d8\u5316\u7ba1\u7406\u53ca\u6270\u52a8\u540e\u6062\u590d\u76ee\u6807\u65b9\u9762\u7684\u4e0d\u8db3\u3002", "conclusion": "\u57fa\u4e8e\u5206\u6790\u7ed3\u679c\uff0c\u4e3a\u5f00\u53d1\u5546\u4e0e\u7535\u529b\u516c\u53f8\u63d0\u4f9b\u5b9e\u7528\u7684\u5e76\u7f51\u6280\u672f\u6307\u5bfc\u3002"}}
{"id": "2601.11590", "categories": ["cs.DC", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.11590", "abs": "https://arxiv.org/abs/2601.11590", "authors": ["Fan Bai", "Pai Peng", "Zhengzhi Tang", "Zhe Wang", "Gong Chen", "Xiang Lu", "Yinuo Li", "Huan Lin", "Weizhe Lin", "Yaoyuan Wang", "Xiaosong Li"], "title": "EPD-Serve: A Flexible Multimodal EPD Disaggregation Inference Serving System On Ascend", "comment": null, "summary": "With the widespread adoption of large multimodal models, efficient inference across text, image, audio, and video modalities has become critical. However, existing multimodal inference systems typically employ monolithic architectures that tightly couple the Encode, Prefill, and Decode stages on homogeneous hardware, neglecting the heterogeneous computational characteristics of each stage. This design leads to inefficient resource utilization and limited system throughput. To address these issues, we propose EPD-Serve, a stage-level disaggregated inference serving system for multimodal models. EPD-Serve decouples the inference pipeline into independent Encode, Prefill, and Decode stages, enabling logical isolation and flexible co-located deployment through dynamic orchestration. Leveraging the Ascend interconnect topology, EPD-Serve introduces asynchronous feature prefetching between Encode and Prefill stages and a hierarchical grouped KV cache transmission mechanism between Prefill and Decode stages to improve cross-node communication efficiency. In addition, EPD-Serve incorporates multi-route scheduling, instance-level load balancing, and multi-stage hardware co-location with spatial multiplexing to better support diverse multimodal workloads. Comprehensive experiments on multimodal understanding models demonstrate that, under high-concurrency scenarios, EPD-Serve improves end-to-end throughput by 57.37-69.48% compared to PD-disaggregated deployment, while satisfying strict SLO constraints, including TTFT below 2000 ms and TPOT below 50 ms. These results highlight the effectiveness of stage-level disaggregation for optimizing multimodal large model inference systems.", "AI": {"tldr": "EPD-Serve \u63d0\u51fa\u4e86\u4e00\u79cd\u9762\u5411\u591a\u6a21\u6001\u5927\u6a21\u578b\u7684\u9636\u6bb5\u7ea7\u89e3\u8026\u63a8\u7406\u7cfb\u7edf\uff0c\u901a\u8fc7\u5f02\u6784\u786c\u4ef6\u4e0a\u7684\u52a8\u6001\u7f16\u6392\u4e0e\u901a\u4fe1\u4f18\u5316\uff0c\u663e\u8457\u63d0\u5347\u541e\u5410\u91cf\u5e76\u6ee1\u8db3\u4e25\u683c SLO\u3002", "motivation": "\u73b0\u6709\u5355\u4f53\u67b6\u6784\u5728\u591a\u6a21\u6001\u63a8\u7406\u4e2d\u8d44\u6e90\u5229\u7528\u7387\u4f4e\u3001\u541e\u5410\u53d7\u9650\uff0c\u9700\u9488\u5bf9\u5404\u9636\u6bb5\u8ba1\u7b97\u7279\u6027\u8fdb\u884c\u89e3\u8026\u4f18\u5316\u3002", "method": "\u5c06\u63a8\u7406\u6d41\u7a0b\u62c6\u5206\u4e3a\u72ec\u7acb\u7684 Encode\u3001Prefill\u3001Decode \u9636\u6bb5\uff0c\u7ed3\u5408\u5f02\u6b65\u7279\u5f81\u9884\u53d6\u3001\u5206\u5c42 KV \u7f13\u5b58\u4f20\u8f93\u3001\u591a\u8def\u5f84\u8c03\u5ea6\u4e0e\u8d1f\u8f7d\u5747\u8861\u7b49\u673a\u5236\u5b9e\u73b0\u9ad8\u6548\u534f\u540c\u3002", "result": "\u5728\u9ad8\u5e76\u53d1\u573a\u666f\u4e0b\uff0c\u76f8\u6bd4 PD \u89e3\u8026\u90e8\u7f72\uff0c\u7aef\u5230\u7aef\u541e\u5410\u63d0\u5347 57.37-69.48%\uff0c\u540c\u65f6\u6ee1\u8db3 TTFT < 2000ms \u4e0e TPOT < 50ms \u7684 SLO \u8981\u6c42\u3002", "conclusion": "\u9636\u6bb5\u7ea7\u89e3\u8026\u80fd\u6709\u6548\u4f18\u5316\u591a\u6a21\u6001\u5927\u6a21\u578b\u63a8\u7406\u7cfb\u7edf\u7684\u6027\u80fd\u4e0e\u8d44\u6e90\u6548\u7387\u3002"}}
{"id": "2601.11687", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.11687", "abs": "https://arxiv.org/abs/2601.11687", "authors": ["Harmohit Singh"], "title": "Semantic Caching and Intent-Driven Context Optimization for Multi-Agent Natural Language to Code Systems", "comment": null, "summary": "We present a production-optimized multi-agent system designed to translate natural language queries into executable Python code for structured data analytics. Unlike systems that rely on expensive frontier models, our approach achieves high accuracy and cost efficiency through three key innovations: (1) a semantic caching system with LLM-based equivalence detection and structured adaptation hints that provides cache hit rates of 67% on production queries; (2) a dual-threshold decision mechanism that separates exact-match retrieval from reference-guided generation; and (3) an intent-driven dynamic prompt assembly system that reduces token consumption by 40-60% through table-aware context filtering. The system has been deployed in production for enterprise inventory management, processing over 10,000 queries with an average latency of 8.2 seconds and 94.3% semantic accuracy. We describe the architecture, present empirical results from production deployment, and discuss practical considerations for deploying LLM-based analytics systems at scale.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u751f\u4ea7\u4f18\u5316\u7684\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\uff0c\u7528\u4e8e\u5c06\u81ea\u7136\u8bed\u8a00\u67e5\u8be2\u9ad8\u6548\u8f6c\u6362\u4e3a\u7ed3\u6784\u5316\u6570\u636e\u5206\u6790\u7684Python\u4ee3\u7801\u3002", "motivation": "\u964d\u4f4e\u4f9d\u8d56\u6602\u8d35\u524d\u6cbf\u6a21\u578b\u7684\u6210\u672c\uff0c\u540c\u65f6\u4fdd\u6301\u9ad8\u51c6\u786e\u7387\u548c\u6548\u7387\uff0c\u4ee5\u652f\u6301\u4f01\u4e1a\u7ea7\u5927\u89c4\u6a21\u90e8\u7f72\u3002", "method": "\u91c7\u7528\u8bed\u4e49\u7f13\u5b58\u3001\u53cc\u9608\u503c\u51b3\u7b56\u673a\u5236\u548c\u610f\u56fe\u9a71\u52a8\u7684\u52a8\u6001\u63d0\u793a\u7ec4\u88c5\u4e09\u9879\u5173\u952e\u6280\u672f\u3002", "result": "\u5728\u4f01\u4e1a\u5e93\u5b58\u7ba1\u7406\u4e2d\u90e8\u7f72\uff0c\u5904\u7406\u8d851\u4e07\u6b21\u67e5\u8be2\uff0c\u5e73\u5747\u5ef6\u8fdf8.2\u79d2\uff0c\u8bed\u4e49\u51c6\u786e\u7387\u8fbe94.3%\uff0c\u7f13\u5b58\u547d\u4e2d\u738767%\uff0c\u8282\u770140-60%\u4ee4\u724c\u6d88\u8017\u3002", "conclusion": "\u8be5\u7cfb\u7edf\u5728\u6210\u672c\u3001\u6027\u80fd\u4e0e\u51c6\u786e\u6027\u4e4b\u95f4\u53d6\u5f97\u826f\u597d\u5e73\u8861\uff0c\u9002\u5408\u5927\u89c4\u6a21LLM\u5206\u6790\u7cfb\u7edf\u5b9e\u9645\u90e8\u7f72\u3002"}}
{"id": "2601.13452", "categories": ["cs.MA"], "pdf": "https://arxiv.org/pdf/2601.13452", "abs": "https://arxiv.org/abs/2601.13452", "authors": ["Edgar Gonzalez Fernandez"], "title": "A simulation of urban incidents involving pedestrians and vehicles based on Weighted A*", "comment": null, "summary": "This document presents a comprehensive simulation framework designed to model urban incidents involving pedestrians and vehicles. Using a multiagent systems approach, two types of agents (pedestrians and vehicles) are introduced within a 2D grid based urban environment. The environment encodes streets, sidewalks, buildings, zebra crossings, and obstacles such as potholes and infrastructure elements. Each agent employs a weighted A* algorithm for pathfinding, allowing for variation in decision making behavior such as reckless movement or strict rule-following. The model aims to simulate interactions, assess risk of collisions, and evaluate efficiency under varying environmental and behavioral conditions. Experimental results explore how factors like obstacle density, presence of traffic control mechanisms, and behavioral deviations affect safety and travel efficiency.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u591a\u667a\u80fd\u4f53\u7684\u4e8c\u7ef4\u57ce\u5e02\u73af\u5883\u4eff\u771f\u6846\u67b6\uff0c\u7528\u4e8e\u6a21\u62df\u884c\u4eba\u4e0e\u8f66\u8f86\u7684\u4ea4\u4e92\u884c\u4e3a\u5e76\u8bc4\u4f30\u78b0\u649e\u98ce\u9669\u4e0e\u901a\u884c\u6548\u7387\u3002", "motivation": "\u7814\u7a76\u65e8\u5728\u901a\u8fc7\u5efa\u6a21\u4e0d\u540c\u884c\u4e3a\u7b56\u7565\u548c\u73af\u5883\u56e0\u7d20\uff0c\u7406\u89e3\u57ce\u5e02\u4ea4\u901a\u4e2d\u884c\u4eba\u4e0e\u8f66\u8f86\u7684\u4e92\u52a8\u673a\u5236\u53ca\u5176\u5bf9\u5b89\u5168\u4e0e\u6548\u7387\u7684\u5f71\u54cd\u3002", "method": "\u91c7\u7528\u52a0\u6743A*\u7b97\u6cd5\u9a71\u52a8\u884c\u4eba\u4e0e\u8f66\u8f86\u667a\u80fd\u4f53\uff0c\u5728\u5305\u542b\u4eba\u884c\u9053\u3001\u6591\u9a6c\u7ebf\u3001\u969c\u788d\u7269\u7b49\u8981\u7d20\u7684\u7f51\u683c\u5316\u57ce\u5e02\u73af\u5883\u4e2d\u8fdb\u884c\u8def\u5f84\u89c4\u5212\u4e0e\u884c\u4e3a\u6a21\u62df\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\u969c\u788d\u5bc6\u5ea6\u3001\u4ea4\u901a\u63a7\u5236\u8bbe\u65bd\u53ca\u884c\u4e3a\u504f\u5dee\u663e\u8457\u5f71\u54cd\u4ea4\u901a\u5b89\u5168\u6027\u548c\u901a\u884c\u6548\u7387\u3002", "conclusion": "\u8be5\u6846\u67b6\u80fd\u6709\u6548\u652f\u6301\u57ce\u5e02\u4ea4\u901a\u98ce\u9669\u8bc4\u4f30\u4e0e\u4f18\u5316\u7b56\u7565\u7814\u7a76\uff0c\u4e3a\u667a\u6167\u57ce\u5e02\u5efa\u8bbe\u63d0\u4f9b\u4eff\u771f\u5de5\u5177\u57fa\u7840\u3002"}}
{"id": "2601.11595", "categories": ["cs.DC", "cs.LG", "cs.SE"], "pdf": "https://arxiv.org/pdf/2601.11595", "abs": "https://arxiv.org/abs/2601.11595", "authors": ["Meenakshi Amulya Jayanti", "X. Y. Han"], "title": "Enhancing Model Context Protocol (MCP) with Context-Aware Server Collaboration", "comment": null, "summary": "The Model Context Protocol (MCP) has emerged as a widely used framework for enabling LLM-based agents to communicate with external tools and services. The most common implementation of MCP, proposed by Anthropic, heavily relies on a Large Language Model (LLM) to decompose tasks and issue instructions to servers, which act as stateless executors. In particular, the agents, models, and servers are stateless and do not have access to a global context. However, in tasks involving LLM-driven coordination, it is natural that a Shared Context Store (SCS) could improve the efficiency and coherence of multi-agent workflows by reducing redundancy and enabling knowledge transfer between servers. Thus, in this work, we design and assess the performance of a Context-Aware MCP (CA-MCP) that offloads execution logic to specialized MCP servers that read from and write to a shared context memory, allowing them to coordinate more autonomously in real time. In this design, context management serves as the central mechanism that maintains continuity across task executions by tracking intermediate states and shared variables, thereby enabling persistent collaboration among agents without repeated prompting. We present experiments showing that the CA-MCP can outperform the traditional MCP by reducing the number of LLM calls required for complex tasks and decreasing the frequency of response failures when task conditions are not satisfied, thereby improving overall efficiency and responsiveness. In particular, we conducted experiments on the TravelPlanner and REALM-Bench benchmark datasets and observed statistically significant results indicating the potential advantages of incorporating a shared context store via CA-MCP in LLM-driven multi-agent systems.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u4e0a\u4e0b\u6587\u611f\u77e5\u7684MCP\u6846\u67b6\uff08CA-MCP\uff09\uff0c\u901a\u8fc7\u5171\u4eab\u4e0a\u4e0b\u6587\u5b58\u50a8\u63d0\u5347\u591a\u667a\u80fd\u4f53\u534f\u4f5c\u6548\u7387\uff0c\u51cf\u5c11LLM\u8c03\u7528\u6b21\u6570\u4e0e\u54cd\u5e94\u5931\u8d25\u7387\u3002", "motivation": "\u4f20\u7edfMCP\u6846\u67b6\u7f3a\u4e4f\u5168\u5c40\u4e0a\u4e0b\u6587\uff0c\u5bfc\u81f4\u4efb\u52a1\u5197\u4f59\u4e0e\u534f\u8c03\u4f4e\u6548\uff0c\u4e9f\u9700\u5f15\u5165\u5171\u4eab\u4e0a\u4e0b\u6587\u673a\u5236\u4f18\u5316\u591a\u667a\u80fd\u4f53\u5de5\u4f5c\u6d41\u3002", "method": "\u8bbe\u8ba1CA-MCP\u67b6\u6784\uff0c\u4f7fMCP\u670d\u52a1\u5668\u80fd\u8bfb\u5199\u5171\u4eab\u4e0a\u4e0b\u6587\u5185\u5b58\uff0c\u4ee5\u72b6\u6001\u8ddf\u8e2a\u548c\u53d8\u91cf\u5171\u4eab\u5b9e\u73b0\u81ea\u4e3b\u5b9e\u65f6\u534f\u4f5c\u3002", "result": "\u5728TravelPlanner\u4e0eREALM-Bench\u6570\u636e\u96c6\u4e0a\u5b9e\u9a8c\u8868\u660e\uff0cCA-MCP\u663e\u8457\u51cf\u5c11LLM\u8c03\u7528\u6b21\u6570\u5e76\u964d\u4f4e\u4efb\u52a1\u5931\u8d25\u7387\uff0c\u63d0\u5347\u7cfb\u7edf\u6548\u7387\u4e0e\u54cd\u5e94\u6027\u3002", "conclusion": "\u5f15\u5165\u5171\u4eab\u4e0a\u4e0b\u6587\u5b58\u50a8\u7684CA-MCP\u80fd\u6709\u6548\u589e\u5f3aLLM\u9a71\u52a8\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u7684\u534f\u4f5c\u8fde\u8d2f\u6027\u4e0e\u6267\u884c\u6548\u7387\u3002"}}
{"id": "2601.11688", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.11688", "abs": "https://arxiv.org/abs/2601.11688", "authors": ["Vedant Nipane", "Pulkit Agrawal", "Amit Singh"], "title": "SpecMap: Hierarchical LLM Agent for Datasheet-to-Code Traceability Link Recovery in Systems Engineering", "comment": null, "summary": "Establishing precise traceability between embedded systems datasheets and their corresponding code implementations remains a fundamental challenge in systems engineering, particularly for low-level software where manual mapping between specification documents and large code repositories is infeasible. Existing Traceability Link Recovery approaches primarily rely on lexical similarity and information retrieval techniques, which struggle to capture the semantic, structural, and symbol level relationships prevalent in embedded systems software. We present a hierarchical datasheet-to-code mapping methodology that employs large language models for semantic analysis while explicitly structuring the traceability process across multiple abstraction levels. Rather than performing direct specification-to-code matching, the proposed approach progressively narrows the search space through repository-level structure inference, file-level relevance estimation, and fine-grained symbollevel alignment. The method extends beyond function-centric mapping by explicitly covering macros, structs, constants, configuration parameters, and register definitions commonly found in systems-level C/C++ codebases. We evaluate the approach on multiple open-source embedded systems repositories using manually curated datasheet-to-code ground truth. Experimental results show substantial improvements over traditional information-retrieval-based baselines, achieving up to 73.3% file mapping accuracy. We significantly reduce computational overhead, lowering total LLM token consumption by 84% and end-to-end runtime by approximately 80%. This methodology supports automated analysis of large embedded software systems and enables downstream applications such as training data generation for systems-aware machine learning models, standards compliance verification, and large-scale specification coverage analysis.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u5206\u5c42\u7684\u5d4c\u5165\u5f0f\u7cfb\u7edf\u6570\u636e\u624b\u518c\u5230\u4ee3\u7801\u6620\u5c04\u65b9\u6cd5\uff0c\u7ed3\u5408\u5927\u8bed\u8a00\u6a21\u578b\u8bed\u4e49\u5206\u6790\u4e0e\u591a\u7ea7\u62bd\u8c61\u7ed3\u6784\uff0c\u663e\u8457\u63d0\u5347\u6620\u5c04\u51c6\u786e\u7387\u5e76\u964d\u4f4e\u8ba1\u7b97\u5f00\u9500\u3002", "motivation": "\u89e3\u51b3\u4f20\u7edf\u57fa\u4e8e\u8bcd\u6c47\u76f8\u4f3c\u6027\u7684\u8ffd\u6eaf\u65b9\u6cd5\u5728\u5d4c\u5165\u5f0f\u7cfb\u7edf\u4e2d\u96be\u4ee5\u6355\u6349\u8bed\u4e49\u3001\u7ed3\u6784\u548c\u7b26\u53f7\u7ea7\u5173\u7cfb\u7684\u95ee\u9898\u3002", "method": "\u91c7\u7528\u5206\u5c42\u7b56\u7565\uff1a\u5148\u63a8\u65ad\u4ed3\u5e93\u7ed3\u6784\uff0c\u518d\u4f30\u8ba1\u6587\u4ef6\u76f8\u5173\u6027\uff0c\u6700\u540e\u8fdb\u884c\u7ec6\u7c92\u5ea6\u7b26\u53f7\u5bf9\u9f50\uff0c\u8986\u76d6\u51fd\u6570\u3001\u5b8f\u3001\u7ed3\u6784\u4f53\u7b49\u7cfb\u7edf\u7ea7\u5143\u7d20\u3002", "result": "\u5728\u591a\u4e2a\u5f00\u6e90\u9879\u76ee\u4e0a\u5b9e\u73b0\u6700\u9ad873.3%\u7684\u6587\u4ef6\u6620\u5c04\u51c6\u786e\u7387\uff0cLLM\u4ee4\u724c\u6d88\u8017\u51cf\u5c1184%\uff0c\u7aef\u5230\u7aef\u8fd0\u884c\u65f6\u95f4\u7f29\u77ed\u7ea680%\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u652f\u6301\u5927\u89c4\u6a21\u5d4c\u5165\u5f0f\u7cfb\u7edf\u7684\u81ea\u52a8\u5316\u5206\u6790\uff0c\u9002\u7528\u4e8e\u8bad\u7ec3\u6570\u636e\u751f\u6210\u3001\u5408\u89c4\u9a8c\u8bc1\u548c\u89c4\u8303\u8986\u76d6\u7387\u5206\u6790\u7b49\u4e0b\u6e38\u4efb\u52a1\u3002"}}
{"id": "2601.13671", "categories": ["cs.MA", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.13671", "abs": "https://arxiv.org/abs/2601.13671", "authors": ["Apoorva Adimulam", "Rajesh Gupta", "Sumit Kumar"], "title": "The Orchestration of Multi-Agent Systems: Architectures, Protocols, and Enterprise Adoption", "comment": null, "summary": "Orchestrated multi-agent systems represent the next stage in the evolution of artificial intelligence, where autonomous agents collaborate through structured coordination and communication to achieve complex, shared objectives. This paper consolidates and formalizes the technical composition of such systems, presenting a unified architectural framework that integrates planning, policy enforcement, state management, and quality operations into a coherent orchestration layer. Another primary contribution of this work is the in-depth technical delineation of two complementary communication protocols - the Model Context Protocol, which standardizes how agents access external tools and contextual data, and the Agent2Agent protocol, which governs peer coordination, negotiation, and delegation. Together, these protocols establish an interoperable communication substrate that enables scalable, auditable, and policy-compliant reasoning across distributed agent collectives. Beyond protocol design, the paper details how orchestration logic, governance frameworks, and observability mechanisms collectively sustain system coherence, transparency, and accountability. By synthesizing these elements into a cohesive technical blueprint, this paper provides comprehensive treatments of orchestrated multi-agent systems - bridging conceptual architectures with implementation-ready design principles for enterprise-scale AI ecosystems.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7edf\u4e00\u7684\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u67b6\u6784\uff0c\u6574\u5408\u89c4\u5212\u3001\u7b56\u7565\u6267\u884c\u3001\u72b6\u6001\u7ba1\u7406\u4e0e\u8d28\u91cf\u64cd\u4f5c\uff0c\u5e76\u8bbe\u8ba1\u4e24\u79cd\u901a\u4fe1\u534f\u8bae\u4ee5\u5b9e\u73b0\u53ef\u6269\u5c55\u3001\u53ef\u5ba1\u8ba1\u3001\u5408\u89c4\u7684\u5206\u5e03\u5f0f\u534f\u4f5c\u3002", "motivation": "\u63a8\u52a8\u4eba\u5de5\u667a\u80fd\u5411\u7ed3\u6784\u5316\u534f\u4f5c\u7684\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u6f14\u8fdb\uff0c\u89e3\u51b3\u590d\u6742\u5171\u4eab\u76ee\u6807\u4e0b\u7684\u534f\u8c03\u4e0e\u901a\u4fe1\u95ee\u9898\u3002", "method": "\u6784\u5efa\u7edf\u4e00\u67b6\u6784\u6846\u67b6\uff0c\u8bbe\u8ba1Model Context Protocol\u4e0eAgent2Agen\u534f\u8bae\uff0c\u7ed3\u5408\u6cbb\u7406\u4e0e\u53ef\u89c2\u6d4b\u673a\u5236\u3002", "result": "\u5b9e\u73b0\u4e86\u8de8\u5206\u5e03\u5f0f\u667a\u80fd\u4f53\u7684\u4e92\u64cd\u4f5c\u901a\u4fe1\u57fa\u5e95\uff0c\u4fdd\u969c\u7cfb\u7edf\u4e00\u81f4\u6027\u3001\u900f\u660e\u6027\u4e0e\u95ee\u8d23\u6027\u3002", "conclusion": "\u672c\u7814\u7a76\u4e3a\u9762\u5411\u4f01\u4e1a\u7ea7AI\u751f\u6001\u7684\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u63d0\u4f9b\u4e86\u4ece\u6982\u5ff5\u5230\u843d\u5730\u7684\u5b8c\u6574\u6280\u672f\u84dd\u56fe\u3002"}}
{"id": "2601.13804", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2601.13804", "abs": "https://arxiv.org/abs/2601.13804", "authors": ["Ioannis Constantinou", "Arthur Perais", "Yiannakis Sazeides"], "title": "The Non-Predictability of Mispredicted Branches using Timing Information", "comment": null, "summary": "Branch misprediction latency is one of the most important contributors to performance degradation and wasted energy consumption in a modern core. State-of-the-art predictors generally perform very well but occasionally suffer from high Misprediction Per Kilo Instruction due to hard-to-predict branches. In this work, we investigate if predicting branches using microarchitectural information, in addition to traditional branch history, can improve prediction accuracy. Our approach considers branch timing information (resolution cycle) both for older branches in the Reorder Buffer (ROB) and recently committed, and for younger branches relative to the branch we re-predict. We propose Speculative Branch Resolution (SBR) in which, N cycles after a branch allocates in the ROB, various timing information is collected and used to re-predict. Using the gem5 simulator we implement and perform a limit-study of SBR using a TAGE-Like predictor. Our experiments show that the post-alloc timing information we used was not able to yield performance gains over an unbounded TAGE-SC. However, we find two hard to predict branches where timing information did provide an advantage and thoroughly analysed one of them to understand why. This finding suggests that predictors may benefit from specific microarchitectural information to increase accuracy on specific hard to predict branches and that overriding predictions in the backend may yet yield performance benefits, but that further research is needed to determine such information vectors.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u5229\u7528\u5fae\u67b6\u6784\u4fe1\u606f\u63d0\u5347\u5206\u652f\u9884\u6d4b\u51c6\u786e\u7387\uff0c\u867d\u6574\u4f53\u672a\u8d85\u8d8aTAGE-SC\uff0c\u4f46\u53d1\u73b0\u7279\u5b9a\u96be\u9884\u6d4b\u5206\u652f\u53ef\u53d7\u76ca\u4e8e\u65f6\u5e8f\u4fe1\u606f\u3002", "motivation": "\u964d\u4f4e\u73b0\u4ee3\u5904\u7406\u5668\u4e2d\u56e0\u5206\u652f\u8bef\u9884\u6d4b\u5bfc\u81f4\u7684\u6027\u80fd\u4e0b\u964d\u4e0e\u80fd\u8017\u6d6a\u8d39\u3002", "method": "\u63d0\u51faSBR\u65b9\u6cd5\uff0c\u5728ROB\u5206\u914d\u540eN\u5468\u671f\u6536\u96c6\u65f6\u5e8f\u4fe1\u606f\u91cd\u65b0\u9884\u6d4b\uff0c\u5e76\u5728gem5\u6a21\u62df\u5668\u4e2d\u57fa\u4e8eTAGE-Like\u9884\u6d4b\u5668\u8fdb\u884c\u6781\u9650\u7814\u7a76\u3002", "result": "\u603b\u4f53\u672a\u4f18\u4e8e\u65e0\u754cTAGE-SC\uff0c\u4f46\u5728\u4e24\u4e2a\u96be\u9884\u6d4b\u5206\u652f\u4e0a\u89c2\u5bdf\u5230\u65f6\u5e8f\u4fe1\u606f\u5e26\u6765\u7684\u4f18\u52bf\u3002", "conclusion": "\u7279\u5b9a\u5fae\u67b6\u6784\u4fe1\u606f\u53ef\u80fd\u63d0\u5347\u96be\u9884\u6d4b\u5206\u652f\u7684\u51c6\u786e\u7387\uff0c\u540e\u7aef\u8986\u76d6\u9884\u6d4b\u6216\u6709\u6f5c\u529b\uff0c\u9700\u8fdb\u4e00\u6b65\u63a2\u7d22\u6709\u6548\u4fe1\u606f\u5411\u91cf\u3002"}}
{"id": "2601.11608", "categories": ["cs.DC", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.11608", "abs": "https://arxiv.org/abs/2601.11608", "authors": ["Ganesh Bikshandi"], "title": "Hardware-Aware Reformulation of Convolutions for Efficient Execution on Specialized AI Hardware: A Case Study on NVIDIA Tensor Cores", "comment": null, "summary": "Convolutional Neural Networks (CNNs) are central to modern AI, but their performance is often limited by hardware constraints. NVIDIA Tensor Cores, for instance, require input channels to be multiples of 8 and sometimes 512 for efficient execution. {\\em oneDNN} framework for CPU imposes such a requirement for the blocked format. Traditional approaches address such alignment issue using zero-padding, which can be inefficient. In this work, we present a first-step, hardware-aware reformulation of CNN computations using rewrite rules, restructuring the underlying math to satisfy hardware alignment entirely {\\bf post-training} without modifying network weights. While our current implementation focuses on a single transformation for Tensor Cores, this approach is generalizable, laying the foundation to explore additional transformations for CPU and accelerators. This study represents an initial step toward {\\em semantic tuning}, a systematic, hardware-aware optimization strategy for efficient deployment of CNN models on specialized AI hardware.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u786c\u4ef6\u611f\u77e5\u7684CNN\u8ba1\u7b97\u91cd\u5199\u65b9\u6cd5\uff0c\u901a\u8fc7\u6570\u5b66\u91cd\u6784\u6ee1\u8db3\u786c\u4ef6\u5bf9\u9f50\u8981\u6c42\uff0c\u65e0\u9700\u4fee\u6539\u6743\u91cd\uff0c\u4e3a\u8bed\u4e49\u8c03\u4f18\u5960\u5b9a\u57fa\u7840\u3002", "motivation": "\u4f20\u7edf\u96f6\u586b\u5145\u65b9\u6cd5\u6548\u7387\u4f4e\uff0c\u9700\u786c\u4ef6\u611f\u77e5\u4f18\u5316\u4ee5\u63d0\u5347CNN\u5728\u4e13\u7528AI\u786c\u4ef6\u4e0a\u7684\u6027\u80fd\u3002", "method": "\u4f7f\u7528\u91cd\u5199\u89c4\u5219\u5bf9CNN\u8ba1\u7b97\u8fdb\u884c\u6570\u5b66\u91cd\u6784\uff0c\u4f7f\u5176\u5728\u8bad\u7ec3\u540e\u6ee1\u8db3\u786c\u4ef6\u5bf9\u9f50\u8981\u6c42\u3002", "result": "\u5f53\u524d\u5b9e\u73b0\u805a\u7126Tensor Core\u5355\u53d8\u6362\uff0c\u65b9\u6cd5\u53ef\u63a8\u5e7f\u81f3CPU\u53ca\u5176\u4ed6\u52a0\u901f\u5668\u3002", "conclusion": "\u672c\u7814\u7a76\u662f\u8bed\u4e49\u8c03\u4f18\u7b56\u7565\u7684\u521d\u6b65\u63a2\u7d22\uff0c\u65e8\u5728\u7cfb\u7edf\u5316\u4f18\u5316CNN\u6a21\u578b\u5728\u4e13\u7528\u786c\u4ef6\u4e0a\u7684\u90e8\u7f72\u6548\u7387\u3002"}}
{"id": "2601.11693", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2601.11693", "abs": "https://arxiv.org/abs/2601.11693", "authors": ["Shane K. Panter", "Nasir U. Eisty"], "title": "Technical Lag as Latent Technical Debt: A Rapid Review", "comment": "Accepted to: TechDebt 2026 - International Conference on Technical Debt April 12--15, 2026 Rio de Janeiro, Brazil", "summary": "Context: Technical lag accumulates when software systems fail to keep pace with technological advancements, leading to a deterioration in software quality. Objective: This paper aims to consolidate existing research on technical lag, clarify definitions, explore its detection and quantification methods, examine underlying causes and consequences, review current management practices, and lay out a vision as an indicator of passively accumulated technical debt. Method: We conducted a Rapid Review with snowballing to select the appropriate peer-reviewed studies. We leveraged the ACM Digital Library, IEEE Xplore, Scopus, and Springer as our primary source databases. Results: Technical lag accumulates passively, often unnoticed due to inadequate detection metrics and tools. It negatively impacts software quality through outdated dependencies, obsolete APIs, unsupported platforms, and aging infrastructure. Strategies to manage technical lag primarily involve automated dependency updates, continuous integration processes, and regular auditing. Conclusions: Enhancing and extending the current standardized metrics, detection methods, and empirical studies to use technical lag as an indication of accumulated latent debt can greatly improve the process of maintaining large codebases that are heavily dependent on external packages. We have identified the research gaps and outlined a future vision for researchers and practitioners to explore.", "AI": {"tldr": "\u672c\u6587\u7efc\u8ff0\u6280\u672f\u6ede\u540e\u7814\u7a76\uff0c\u63d0\u51fa\u5176\u4f5c\u4e3a\u9690\u6027\u6280\u672f\u503a\u52a1\u6307\u6807\u7684\u6f5c\u529b\uff0c\u5e76\u547c\u5401\u6539\u8fdb\u68c0\u6d4b\u65b9\u6cd5\u4e0e\u6807\u51c6\u5316\u5ea6\u91cf\u3002", "motivation": "\u5398\u6e05\u6280\u672f\u6ede\u540e\u7684\u5b9a\u4e49\u3001\u6210\u56e0\u4e0e\u5f71\u54cd\uff0c\u63a8\u52a8\u5176\u4f5c\u4e3a\u88ab\u52a8\u79ef\u7d2f\u6280\u672f\u503a\u52a1\u7684\u6709\u6548\u6307\u6807\u3002", "method": "\u91c7\u7528\u5feb\u901f\u56de\u987e\u6cd5\u7ed3\u5408\u6eda\u96ea\u7403\u7b56\u7565\uff0c\u4eceACM\u3001IEEE\u3001Scopus\u548cSpringer\u7b49\u6570\u636e\u5e93\u7b5b\u9009\u76f8\u5173\u7814\u7a76\u3002", "result": "\u6280\u672f\u6ede\u540e\u5e38\u56e0\u7f3a\u4e4f\u6709\u6548\u68c0\u6d4b\u5de5\u5177\u800c\u88ab\u5ffd\u89c6\uff0c\u5bfc\u81f4\u4f9d\u8d56\u8fc7\u65f6\u3001API\u9648\u65e7\u7b49\u95ee\u9898\uff1b\u7ba1\u7406\u7b56\u7565\u5305\u62ec\u81ea\u52a8\u5316\u66f4\u65b0\u4e0e\u6301\u7eed\u96c6\u6210\u3002", "conclusion": "\u5b8c\u5584\u5ea6\u91cf\u6807\u51c6\u4e0e\u5b9e\u8bc1\u7814\u7a76\uff0c\u53ef\u663e\u8457\u63d0\u5347\u5bf9\u5916\u90e8\u4f9d\u8d56\u4ee3\u7801\u5e93\u7684\u7ef4\u62a4\u80fd\u529b\uff0c\u5e76\u4e3a\u672a\u6765\u7814\u7a76\u6307\u660e\u65b9\u5411\u3002"}}
{"id": "2601.12522", "categories": ["cs.SE", "cs.AI", "cs.IR", "cs.LG", "cs.MA"], "pdf": "https://arxiv.org/pdf/2601.12522", "abs": "https://arxiv.org/abs/2601.12522", "authors": ["Asif Mohammed Samir", "Mohammad Masudur Rahman"], "title": "Improved Bug Localization with AI Agents Leveraging Hypothesis and Dynamic Cognition", "comment": "13 pages, 7 tables, 5 figures", "summary": "Software bugs cost technology providers (e.g., AT&T) billions annually and cause developers to spend roughly 50% of their time on bug resolution. Traditional methods for bug localization often analyze the suspiciousness of code components (e.g., methods, documents) in isolation, overlooking their connections with other components in the codebase. Recent advances in Large Language Models (LLMs) and agentic AI techniques have shown strong potential for code understanding, but still lack causal reasoning during code exploration and struggle to manage growing context effectively, limiting their capability. In this paper, we present a novel agentic technique for bug localization -- CogniGent -- that overcomes the limitations above by leveraging multiple AI agents capable of causal reasoning, call-graph-based root cause analysis and context engineering. It emulates developers-inspired debugging practices (a.k.a., dynamic cognitive debugging) and conducts hypothesis testing to support bug localization. We evaluate CogniGent on a curated dataset of 591 bug reports using three widely adopted performance metrics and compare it against six established baselines from the literature. Experimental results show that our technique consistently outperformed existing traditional and LLM-based techniques, achieving MAP improvements of 23.33-38.57% at the document and method levels. Similar gains were observed in MRR, with increases of 25.14-53.74% at both granularity levels. Statistical significance tests also confirm the superiority of our technique. By addressing the reasoning, dependency, and context limitations, CogniGent advances the state of bug localization, bridging human-like cognition with agentic automation for improved performance.", "AI": {"tldr": "CogniGent\u662f\u4e00\u79cd\u65b0\u578b\u591a\u667a\u80fd\u4f53\u6280\u672f\uff0c\u901a\u8fc7\u56e0\u679c\u63a8\u7406\u4e0e\u8c03\u7528\u56fe\u5206\u6790\u663e\u8457\u63d0\u5347\u8f6f\u4ef6\u7f3a\u9677\u5b9a\u4f4d\u6548\u679c\u3002", "motivation": "\u4f20\u7edf\u7f3a\u9677\u5b9a\u4f4d\u65b9\u6cd5\u5ffd\u89c6\u4ee3\u7801\u7ec4\u4ef6\u95f4\u5173\u8054\uff0c\u73b0\u6709LLM\u7f3a\u4e4f\u56e0\u679c\u63a8\u7406\u80fd\u529b\u4e14\u4e0a\u4e0b\u6587\u7ba1\u7406\u4e0d\u8db3\u3002", "method": "\u5f15\u5165\u591aAI\u667a\u80fd\u4f53\uff0c\u7ed3\u5408\u56e0\u679c\u63a8\u7406\u3001\u8c03\u7528\u56fe\u6839\u56e0\u5206\u6790\u4e0e\u4e0a\u4e0b\u6587\u5de5\u7a0b\uff0c\u6a21\u62df\u5f00\u53d1\u8005\u52a8\u6001\u8ba4\u77e5\u8c03\u8bd5\u8fc7\u7a0b\u3002", "result": "\u5728591\u4e2a\u7f3a\u9677\u62a5\u544a\u4e0a\u8bc4\u4f30\uff0cMAP\u63d0\u534723.33%-38.57%\uff0cMRR\u63d0\u534725.14%-53.74%\uff0c\u7edf\u8ba1\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "CogniGent\u6709\u6548\u514b\u670d\u73b0\u6709\u65b9\u6cd5\u5c40\u9650\uff0c\u5c06\u7c7b\u4eba\u8ba4\u77e5\u4e0e\u667a\u80fd\u4f53\u81ea\u52a8\u5316\u7ed3\u5408\uff0c\u63a8\u52a8\u7f3a\u9677\u5b9a\u4f4d\u6280\u672f\u53d1\u5c55\u3002"}}
{"id": "2601.11783", "categories": ["cs.SE", "cs.CL"], "pdf": "https://arxiv.org/pdf/2601.11783", "abs": "https://arxiv.org/abs/2601.11783", "authors": ["Murtuza N. Shergadwala"], "title": "The Stability Trap: Evaluating the Reliability of LLM-Based Instruction Adherence Auditing", "comment": null, "summary": "The enterprise governance of Generative AI (GenAI) in regulated sectors, such as Human Resources (HR), demands scalable yet reproducible auditing mechanisms. While Large Language Model (LLM)-as-a-Judge approaches offer scalability, their reliability in evaluating adherence of different types of system instructions remains unverified. This study asks: To what extent does the instruction type of an Application Under Test (AUT) influence the stability of judge evaluations? To address this, we introduce the Scoped Instruction Decomposition Framework to classify AUT instructions into Objective and Subjective types, isolating the factors that drive judge instability. We applied this framework to two representative HR GenAI applications, evaluating the stability of four judge architectures over variable runs. Our results reveal a ``Stability Trap'' characterized by a divergence between Verdict Stability and Reasoning Stability. While judges achieved near-perfect verdict agreement ($>99\\%$) for both objective and subjective evaluations, their accompanying justification traces diverged significantly. Objective instructions requiring quantitative analysis, such as word counting, exhibited reasoning stability as low as $\\approx19\\%$, driven by variances in numeric justifications. Similarly, reasoning stability for subjective instructions varied widely ($35\\%$--$83\\%$) based on evidence granularity, with feature-specific checks failing to reproduce consistent rationale. Conversely, objective instructions focusing on discrete entity extraction achieved high reasoning stability ($>90\\%$). These findings demonstrate that high verdict stability can mask fragile reasoning. Thus, we suggest that auditors scope automated evaluation protocols strictly: delegate all deterministically verifiable logic to code, while reserving LLM judges for complex semantic evaluation.", "AI": {"tldr": "\u7814\u7a76\u63ed\u793aLLM\u4f5c\u4e3a\u8bc4\u5224\u8005\u5728\u8bc4\u4f30\u751f\u6210\u5f0fAI\u7cfb\u7edf\u6307\u4ee4\u65f6\u5b58\u5728\u2018\u7a33\u5b9a\u6027\u9677\u9631\u2019\uff0c\u5373\u9ad8\u5224\u51b3\u4e00\u81f4\u6027\u63a9\u76d6\u4e86\u4f4e\u63a8\u7406\u4e00\u81f4\u6027\uff0c\u5efa\u8bae\u5c06\u53ef\u9a8c\u8bc1\u903b\u8f91\u4ea4\u7531\u4ee3\u7801\u5904\u7406\uff0c\u4ec5\u8ba9LLM\u8d1f\u8d23\u8bed\u4e49\u590d\u6742\u4efb\u52a1\u3002", "motivation": "\u63a2\u7d22LLM\u4f5c\u4e3a\u8bc4\u5224\u8005\u5728\u4f01\u4e1a\u6cbb\u7406\u4e2d\u8bc4\u4f30\u4e0d\u540c\u7c7b\u578b\u7cfb\u7edf\u6307\u4ee4\u7684\u7a33\u5b9a\u6027\u95ee\u9898\uff0c\u4ee5\u63d0\u5347\u5ba1\u8ba1\u673a\u5236\u7684\u53ef\u9760\u6027\u3002", "method": "\u63d0\u51faScoped Instruction Decomposition Framework\u6846\u67b6\uff0c\u5c06\u6307\u4ee4\u5206\u7c7b\u4e3a\u5ba2\u89c2\u4e0e\u4e3b\u89c2\u7c7b\u578b\uff0c\u5e76\u5728HR\u573a\u666f\u4e0b\u6d4b\u8bd5\u56db\u79cd\u8bc4\u5224\u67b6\u6784\u7684\u7a33\u5b9a\u6027\u3002", "result": "\u53d1\u73b0\u8bc4\u5224\u8005\u5728\u5224\u51b3\u4e0a\u9ad8\u5ea6\u4e00\u81f4\uff08>99%\uff09\uff0c\u4f46\u63a8\u7406\u4e00\u81f4\u6027\u6781\u4f4e\uff08\u6700\u4f4e\u224819%\uff09\uff0c\u5c24\u5176\u5728\u6570\u503c\u5206\u6790\u548c\u7279\u5f81\u68c0\u67e5\u4e2d\u8868\u73b0\u4e0d\u7a33\u5b9a\u3002", "conclusion": "\u9ad8\u5224\u51b3\u7a33\u5b9a\u6027\u53ef\u80fd\u63a9\u76d6\u8106\u5f31\u7684\u63a8\u7406\u8fc7\u7a0b\uff0c\u5efa\u8bae\u81ea\u52a8\u5316\u5ba1\u8ba1\u534f\u8bae\u5e94\u9650\u5b9aLLM\u4ec5\u7528\u4e8e\u590d\u6742\u8bed\u4e49\u8bc4\u4f30\uff0c\u5176\u4f59\u4ea4\u7531\u786e\u5b9a\u6027\u4ee3\u7801\u6267\u884c\u3002"}}
{"id": "2601.11646", "categories": ["cs.DC", "cs.FL"], "pdf": "https://arxiv.org/pdf/2601.11646", "abs": "https://arxiv.org/abs/2601.11646", "authors": ["Chao Wang", "Ruijia Li", "Yang Zhou", "Peng Wu", "Yi Lv", "Jianwei Liao", "Jim Woodcock", "Zhiming Liu"], "title": "A Forward Simulation-Based Hierarchy of Linearizable Concurrent Objects", "comment": null, "summary": "In this paper, we systematically investigate the connection between linearizable objects and forward simulation. We prove that the sets of linearizable objects satisfying wait-freedom (resp., lock-freedom or obstruction-freedom) form a bounded join-semilattice under the forward simulation relation, and that the sets of linearizable objects without liveness constraints form a bounded lattice under the same relation. As part of our lattice result, we propose an equivalent characterization of linearizability by reducing checking linearizability w.r.t. sequential specification $Spec$ into checking forward simulation by an object $\\mathcal{U}_{Spec}$. To demonstrate the forward simulation relation between linearizable objects, we prove that the objects that are strongly linearizable w.r.t. the same sequential specification and are wait-free (resp., lock-free, obstruction-free) simulate each other, and we prove that the time-stamped queue simulates the Herlihy-Wing queue. We also prove that the Herlihy-Wing queue is simulated by $\\mathcal{U}_{Spec}$, and thus, our equivalent characterization of linearizability can be used in the verification of linearizability.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u7ebf\u6027\u5316\u5bf9\u8c61\u4e0e\u524d\u5411\u6a21\u62df\u4e4b\u95f4\u7684\u5173\u7cfb\uff0c\u63d0\u51fa\u7ebf\u6027\u5316\u7684\u65b0\u7b49\u4ef7\u523b\u753b\uff0c\u5e76\u9a8c\u8bc1\u5176\u5728Herlihy-Wing\u961f\u5217\u7b49\u5bf9\u8c61\u4e0a\u7684\u9002\u7528\u6027\u3002", "motivation": "\u5398\u6e05\u7ebf\u6027\u5316\u5bf9\u8c61\u4e0e\u524d\u5411\u6a21\u62df\u7684\u7406\u8bba\u8054\u7cfb\uff0c\u4e3a\u5e76\u53d1\u5bf9\u8c61\u7684\u6b63\u786e\u6027\u9a8c\u8bc1\u63d0\u4f9b\u65b0\u5de5\u5177\u3002", "method": "\u901a\u8fc7\u6784\u5efa\u6709\u754c\u683c\u7ed3\u6784\u5e76\u5229\u7528\u524d\u5411\u6a21\u62df\u5173\u7cfb\uff0c\u5c06\u7ebf\u6027\u5316\u9a8c\u8bc1\u8f6c\u5316\u4e3a\u5bf9\u7279\u5b9a\u5bf9\u8c61\u7684\u6a21\u62df\u68c0\u67e5\u3002", "result": "\u8bc1\u660e\u4e86\u6ee1\u8db3\u4e0d\u540c\u6d3b\u6027\u7ea6\u675f\u7684\u7ebf\u6027\u5316\u5bf9\u8c61\u6784\u6210\u534a\u683c\u6216\u683c\u7ed3\u6784\uff0c\u5e76\u9a8c\u8bc1\u4e86\u65f6\u95f4\u6233\u961f\u5217\u4e0eHerlihy-Wing\u961f\u5217\u95f4\u7684\u6a21\u62df\u5173\u7cfb\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u7ebf\u6027\u5316\u7b49\u4ef7\u523b\u753b\u53ef\u6709\u6548\u7528\u4e8e\u5e76\u53d1\u5bf9\u8c61\u7684\u7ebf\u6027\u5316\u9a8c\u8bc1\u3002"}}
{"id": "2601.11835", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2601.11835", "abs": "https://arxiv.org/abs/2601.11835", "authors": ["Yufan Zhang", "Jaromir Savelka", "Seth Goldstein", "Michael Conway"], "title": "Changes in Coding Behavior and Performance Since the Introduction of LLMs", "comment": null, "summary": "The widespread availability of large language models (LLMs) has changed how students engage with coding and problem-solving. While these tools may increase student productivity, they also make it more difficult for instructors to assess students' learning and effort. In this quasi-longitudinal study, we analyze five years of student source code submissions in a graduate-level cloud computing course, focusing on an assignment that remained unchanged and examining students' behavior during the period spanning five semesters before the release of ChatGPT and five semesters after.\n  Student coding behavior has changed significantly since Fall 2022. The length of their final submissions increased. Between consecutive submissions, average edit distances increased while average score improvement decreased, suggesting that both student productivity and learning have decreased after ChatGPT's release. Additionally, there are statistically significant correlations between these behavioral changes and their overall performance. Although we cannot definitively attribute them to LLM misuse, they are consistent with our hypothesis that some students are over-reliant on LLMs, which is negatively affecting their learning outcomes. Our findings raise an alarm around the first generation of graduates in the age of LLMs, calling upon both educators and employers to reflect on their evaluation methods for genuine expertise and productivity.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0ChatGPT\u53d1\u5e03\u540e\u5b66\u751f\u7f16\u7a0b\u884c\u4e3a\u663e\u8457\u53d8\u5316\uff0c\u63d0\u4ea4\u4ee3\u7801\u53d8\u957f\u3001\u4fee\u6539\u5e45\u5ea6\u589e\u5927\u4f46\u6210\u7ee9\u63d0\u5347\u51cf\u5c11\uff0c\u6697\u793a\u8fc7\u5ea6\u4f9d\u8d56LLM\u53ef\u80fd\u635f\u5bb3\u5b66\u4e60\u6548\u679c\u3002", "motivation": "\u63a2\u8ba8\u5927\u8bed\u8a00\u6a21\u578b\u666e\u53ca\u5bf9\u5b66\u751f\u7f16\u7a0b\u5b66\u4e60\u548c\u6559\u5e08\u8bc4\u4f30\u65b9\u5f0f\u7684\u5f71\u54cd\u3002", "method": "\u5206\u6790\u67d0\u7814\u7a76\u751f\u4e91\u8ba1\u7b97\u8bfe\u7a0b\u4e94\u5e74\u5185\u5b66\u751f\u4ee3\u7801\u63d0\u4ea4\u6570\u636e\uff0c\u6bd4\u8f83ChatGPT\u53d1\u5e03\u524d\u540e\u4e94\u4e2a\u5b66\u671f\u7684\u884c\u4e3a\u5dee\u5f02\u3002", "result": "\u5b66\u751f\u6700\u7ec8\u63d0\u4ea4\u4ee3\u7801\u957f\u5ea6\u589e\u52a0\uff0c\u8fde\u7eed\u63d0\u4ea4\u95f4\u7f16\u8f91\u8ddd\u79bb\u4e0a\u5347\u800c\u5f97\u5206\u63d0\u5347\u4e0b\u964d\uff0c\u4e14\u8fd9\u4e9b\u53d8\u5316\u4e0e\u603b\u4f53\u8868\u73b0\u663e\u8457\u76f8\u5173\u3002", "conclusion": "\u5b66\u751f\u53ef\u80fd\u56e0\u8fc7\u5ea6\u4f9d\u8d56LLM\u5bfc\u81f4\u5b66\u4e60\u6210\u6548\u4e0b\u964d\uff0c\u547c\u5401\u6559\u80b2\u8005\u4e0e\u96c7\u4e3b\u91cd\u65b0\u601d\u8003\u8bc4\u4f30\u771f\u5b9e\u80fd\u529b\u7684\u65b9\u6cd5\u3002"}}
{"id": "2601.14140", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2601.14140", "abs": "https://arxiv.org/abs/2601.14140", "authors": ["Tong Xie", "Yijiahao Qi", "Jinqi Wen", "Zishen Wan", "Yanchi Dong", "Zihao Wang", "Shaofei Cai", "Yitao Liang", "Tianyu Jia", "Yuan Wang", "Runsheng Wang", "Meng Li"], "title": "CREATE: Cross-Layer Resilience Characterization and Optimization for Efficient yet Reliable Embodied AI Systems", "comment": "18 pages, 21 figures. Accepted by ASPLOS 2026", "summary": "Embodied Artificial Intelligence (AI) has recently attracted significant attention as it bridges AI with the physical world. Modern embodied AI systems often combine a Large Language Model (LLM)-based planner for high-level task planning and a reinforcement learning (RL)-based controller for low-level action generation, enabling embodied agents to tackle complex tasks in real-world environments. However, deploying embodied agents remains challenging due to their high computation requirements, especially for battery-powered local devices. Although techniques like lowering operating voltage can improve energy efficiency, they can introduce bit errors and result in task failures. In this work, we propose CREATE, a general design principle that leverages heterogeneous resilience at different layers for synergistic energy-reliability co-optimization. For the first time, we conduct a comprehensive error injection study on modern embodied AI systems and observe an inherent but heterogeneous fault tolerance. Building upon these insights, we develop an anomaly detection and clearance mechanism at the circuit level to eliminate outlier errors. At the model level, we propose a weight-rotation-enhanced planning algorithm to improve the fault tolerance of the LLM-based planner. Furthermore, we introduce an application-level technique, autonomy-adaptive voltage scaling, to dynamically adjust the operating voltage of the controllers. The voltage scaling circuit is co-designed to enable online voltage adjustment. Extensive experiments demonstrate that without compromising task quality, CREATE achieves 40.6% computational energy savings on average over nominal-voltage baselines and 35.0% over prior-art techniques. This further leads to 29.5% to 37.3% chip-level energy savings and approximately a 15% to 30% improvement in battery life.", "AI": {"tldr": "CREATE\u662f\u4e00\u79cd\u9488\u5bf9\u5177\u8eabAI\u7cfb\u7edf\u7684\u80fd\u6548\u4e0e\u53ef\u9760\u6027\u534f\u540c\u4f18\u5316\u8bbe\u8ba1\u539f\u5219\uff0c\u901a\u8fc7\u7535\u8def\u3001\u6a21\u578b\u548c\u5e94\u7528\u5c42\u7684\u5f02\u6784\u5bb9\u9519\u673a\u5236\uff0c\u5728\u4e0d\u964d\u4f4e\u4efb\u52a1\u8d28\u91cf\u7684\u524d\u63d0\u4e0b\u663e\u8457\u8282\u7701\u80fd\u8017\u5e76\u5ef6\u957f\u7535\u6c60\u5bff\u547d\u3002", "motivation": "\u5177\u8eabAI\u7cfb\u7edf\u8ba1\u7b97\u9700\u6c42\u9ad8\uff0c\u96be\u4ee5\u5728\u7535\u6c60\u4f9b\u7535\u8bbe\u5907\u4e0a\u90e8\u7f72\uff0c\u4e14\u4f4e\u7535\u538b\u8282\u80fd\u65b9\u6cd5\u6613\u5f15\u53d1\u4f4d\u9519\u8bef\u5bfc\u81f4\u4efb\u52a1\u5931\u8d25\u3002", "method": "\u63d0\u51faCREATE\u8bbe\u8ba1\u539f\u5219\uff0c\u5305\u542b\u7535\u8def\u5c42\u5f02\u5e38\u68c0\u6d4b\u6e05\u9664\u673a\u5236\u3001\u6a21\u578b\u5c42\u6743\u91cd\u65cb\u8f6c\u589e\u5f3a\u89c4\u5212\u7b97\u6cd5\u3001\u5e94\u7528\u5c42\u81ea\u4e3b\u9002\u5e94\u7535\u538b\u8c03\u8282\u6280\u672f\uff0c\u5e76\u5b9e\u73b0\u7535\u538b\u8c03\u8282\u7535\u8def\u7684\u534f\u540c\u8bbe\u8ba1\u3002", "result": "\u5b9e\u9a8c\u8868\u660eCREATE\u5e73\u5747\u8282\u770140.6%\u8ba1\u7b97\u80fd\u8017\uff0c\u82af\u7247\u7ea7\u8282\u80fd29.5%-37.3%\uff0c\u7535\u6c60\u5bff\u547d\u63d0\u534715%-30%\u3002", "conclusion": "CREATE\u6709\u6548\u5e73\u8861\u4e86\u5177\u8eabAI\u7cfb\u7edf\u7684\u80fd\u6548\u4e0e\u53ef\u9760\u6027\uff0c\u4e3a\u8fb9\u7f18\u90e8\u7f72\u63d0\u4f9b\u4e86\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2601.12980", "categories": ["cs.NI"], "pdf": "https://arxiv.org/pdf/2601.12980", "abs": "https://arxiv.org/abs/2601.12980", "authors": ["Hongbo Wang", "Xin Li", "Yinghui He", "Jingzhi Hu", "Mingming Xu", "Zhe Chen", "Fu Xiao", "Jun Luo"], "title": "Path to Diversity: A Primer on ISAC-izing Commodity Wi-Fi for Practical Deployments", "comment": null, "summary": "Integrated Sensing and Communication (ISAC) has emerged as a key paradigm in next-generation wireless networks. While the ubiquity and low cost of commodity Wi-Fi make it an ideal platform for wide-scale sensing, it is the continuous evolution of Wi-Fi standards-towards higher frequency bands, wider bandwidths, and larger antenna arrays-that fundamentally unlocks the physical resources required for high-performance ISAC. To structure this rapidly expanding field, numerous surveys have appeared. However, prevailing literature predominantly adopts a top-down perspective, emphasizing upper-layer applications or deep learning models while treating the physical layer as an opaque abstraction. Consequently, these works often fail to touch the bottom layer of signal formation and lack technical guidance on overcoming the physical barriers that constrain sensing performance. To bridge this gap, this tutorial takes a bottom-up approach, systematically analyzing the sensing gains brought by Wi-Fi advancements through the lens of physical-layer diversity. We organize the framework around four orthogonal dimensions: i) Temporal Diversity addresses synchronization gaps to enable absolute ranging; ii) Frequency Diversity expands the effective bandwidth to sharpen range resolution; iii) Link Diversity leverages distributed topologies and digital feedback to achieve ubiquitous observability; and iv) Spatial Diversity utilizes multi-antenna arrays to combine passive angular discrimination with active directional control. Collectively, these orthogonal dimensions resolve fundamental ambiguities in time, range, and space, bridging physical capabilities with challenging sensing diversities. By synthesizing these dimensions, this tutorial provides a comprehensive guide for \"ISAC-izing\" commodity Wi-Fi, paving the way for future standardization and robust deployment.", "AI": {"tldr": "\u672c\u6587\u4ece\u7269\u7406\u5c42\u591a\u6837\u6027\u89d2\u5ea6\u7cfb\u7edf\u5206\u6790Wi-Fi\u6280\u672f\u6f14\u8fdb\u5982\u4f55\u63d0\u5347\u611f\u77e5\u6027\u80fd\uff0c\u63d0\u51fa\u65f6\u95f4\u3001\u9891\u7387\u3001\u94fe\u8def\u4e0e\u7a7a\u95f4\u56db\u4e2a\u6b63\u4ea4\u7ef4\u5ea6\uff0c\u4e3a\u5b9e\u73b0\u5546\u54c1\u5316Wi-Fi\u7684\u901a\u611f\u4e00\u4f53\u5316\u63d0\u4f9b\u5e95\u5c42\u6280\u672f\u6307\u5357\u3002", "motivation": "\u73b0\u6709\u6587\u732e\u591a\u805a\u7126\u4e0a\u5c42\u5e94\u7528\u6216\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\uff0c\u5ffd\u89c6\u7269\u7406\u5c42\u4fe1\u53f7\u5f62\u6210\u673a\u5236\uff0c\u7f3a\u4e4f\u7a81\u7834\u611f\u77e5\u6027\u80fd\u7269\u7406\u74f6\u9888\u7684\u6280\u672f\u6307\u5bfc\u3002", "method": "\u91c7\u7528\u81ea\u4e0b\u800c\u4e0a\u65b9\u6cd5\uff0c\u56f4\u7ed5\u65f6\u95f4\u3001\u9891\u7387\u3001\u94fe\u8def\u4e0e\u7a7a\u95f4\u56db\u7ef4\u7269\u7406\u5c42\u591a\u6837\u6027\u6784\u5efa\u5206\u6790\u6846\u67b6\u3002", "result": "\u56db\u7ef4\u591a\u6837\u6027\u534f\u540c\u89e3\u51b3\u65f6\u57df\u3001\u8ddd\u79bb\u4e0e\u7a7a\u95f4\u7684\u6839\u672c\u6a21\u7cca\u6027\uff0c\u6253\u901a\u7269\u7406\u80fd\u529b\u4e0e\u590d\u6742\u611f\u77e5\u9700\u6c42\u95f4\u7684\u9e3f\u6c9f\u3002", "conclusion": "\u672c\u6559\u7a0b\u4e3a\u2018\u901a\u611f\u4e00\u4f53\u5316\u2019\u5546\u54c1Wi-Fi\u63d0\u4f9b\u7cfb\u7edf\u6027\u5b9e\u73b0\u8def\u5f84\uff0c\u52a9\u529b\u672a\u6765\u6807\u51c6\u5316\u4e0e\u7a33\u5065\u90e8\u7f72\u3002"}}
{"id": "2601.11652", "categories": ["cs.DC", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.11652", "abs": "https://arxiv.org/abs/2601.11652", "authors": ["Xiangchen Li", "Jiakun Fan", "Qingyuan Wang", "Dimitrios Spatharakis", "Saeid Ghafouri", "Hans Vandierendonck", "Deepu John", "Bo Ji", "Ali R. Butt", "Dimitrios S. Nikolopoulos"], "title": "WISP: Waste- and Interference-Suppressed Distributed Speculative LLM Serving at the Edge via Dynamic Drafting and SLO-Aware Batching", "comment": "28 Pages, 11 Figures, 12 Tables", "summary": "As Large Language Models (LLMs) become increasingly accessible to end users, an ever-growing number of inference requests are initiated from edge devices and computed on centralized GPU clusters. However, the resulting exponential growth in computation workload is placing significant strain on data centers, while edge devices remain largely underutilized, leading to imbalanced workloads and resource inefficiency across the network. Integrating edge devices into the LLM inference process via speculative decoding helps balance the workload between the edge and the cloud, while maintaining lossless prediction accuracy. In this paper, we identify and formalize two critical bottlenecks that limit the efficiency and scalability of distributed speculative LLM serving: Wasted Drafting Time and Verification Interference. To address these challenges, we propose WISP, an efficient and SLO-aware distributed LLM inference system that consists of an intelligent speculation controller, a verification time estimator, and a verification batch scheduler. These components collaboratively enhance drafting efficiency and optimize verification request scheduling on the server. Extensive numerical results show that WISP improves system capacity by up to 2.1x and 4.1x, and increases system goodput by up to 1.94x and 3.7x, compared to centralized serving and SLED, respectively.", "AI": {"tldr": "WISP\u901a\u8fc7\u667a\u80fd\u63a8\u6d4b\u63a7\u5236\u3001\u9a8c\u8bc1\u65f6\u95f4\u4f30\u8ba1\u548c\u6279\u5904\u7406\u8c03\u5ea6\uff0c\u4f18\u5316\u5206\u5e03\u5f0f\u5927\u8bed\u8a00\u6a21\u578b\u63a8\u7406\u6548\u7387\u3002", "motivation": "\u8fb9\u7f18\u8bbe\u5907\u672a\u88ab\u5145\u5206\u5229\u7528\uff0c\u5bfc\u81f4\u4e91\u7aef\u8ba1\u7b97\u8d1f\u8f7d\u8fc7\u91cd\u548c\u8d44\u6e90\u6d6a\u8d39\u3002", "method": "\u63d0\u51faWISP\u7cfb\u7edf\uff0c\u5305\u542b\u667a\u80fd\u63a8\u6d4b\u63a7\u5236\u5668\u3001\u9a8c\u8bc1\u65f6\u95f4\u4f30\u8ba1\u5668\u548c\u9a8c\u8bc1\u6279\u8c03\u5ea6\u5668\u3002", "result": "\u76f8\u6bd4\u96c6\u4e2d\u5f0f\u670d\u52a1\u548cSLED\uff0c\u7cfb\u7edf\u5bb9\u91cf\u63d0\u5347\u6700\u9ad8\u8fbe2.1\u500d\u548c4.1\u500d\uff0c\u7cfb\u7edf\u541e\u5410\u63d0\u5347\u6700\u9ad8\u8fbe1.94\u500d\u548c3.7\u500d\u3002", "conclusion": "WISP\u6709\u6548\u7f13\u89e3\u4e86\u5206\u5e03\u5f0f\u63a8\u6d4b\u89e3\u7801\u4e2d\u7684\u74f6\u9888\uff0c\u663e\u8457\u63d0\u5347\u63a8\u7406\u6548\u7387\u4e0e\u8d44\u6e90\u5229\u7528\u7387\u3002"}}
{"id": "2601.11836", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2601.11836", "abs": "https://arxiv.org/abs/2601.11836", "authors": ["Finn Hackett", "Evan Wrench", "Peter Macko", "A. Jesse Jiryu Davis", "Yuanhao Wei", "Ivan Beschastnikh"], "title": "Trace Validation of Unmodified Concurrent Systems with OmniLink", "comment": null, "summary": "Concurrent systems are notoriously difficult to validate: subtle bugs may only manifest under rare thread interleavings, and existing tools often require intrusive instrumentation or unrealistic execution models. We present OmniLink, a new methodology for validating concurrent implementations against high-level specifications in TLA+. Unlike prior TLA+ based approaches which use a technique called trace validation, OmniLink treats system events as black boxes with a timebox in which they occurred and a meaning in TLA+, solving for a logical total order of actions. Unlike prior approaches based on linearizability checking, which already solves for total orders of actions with timeboxes, OmniLink uses a flexible specification language, and offers a different linearizability checking method based on off-the-shelf model checking. OmniLink offers different features compared existing linearizability checking tools, and we show that it outperforms the state of the art on large scale validation tasks.\n  Our evaluation validates WiredTiger, a state-of-the-art industrial database storage layer, as well as Balanced Augmented Tree (BAT), a state-of-the art lock-free data structure from the research community, and ConcurrentQueue, a popular lock-free queue featuring aggressive performance optimizations. We use OmniLink to improve WiredTiger's existing TLA+ model, as well as develop new TLA+ models that closely match the behavior of the modeled systems, including non-linearizable behaviors. OmniLink is able to find known bugs injected into the systems under test, as well as help discover two previously unknown bugs (1 in BAT, 1 in ConcurrentQueue), which we have confirmed with the authors of those systems.", "AI": {"tldr": "OmniLink\u662f\u4e00\u79cd\u65b0\u7684\u5e76\u53d1\u7cfb\u7edf\u9a8c\u8bc1\u65b9\u6cd5\uff0c\u57fa\u4e8eTLA+\u89c4\u8303\uff0c\u901a\u8fc7\u9ed1\u76d2\u4e8b\u4ef6\u548c\u65f6\u95f4\u7a97\u53e3\u6c42\u89e3\u903b\u8f91\u5168\u5e8f\uff0c\u4f18\u4e8e\u73b0\u6709\u7ebf\u6027\u5316\u68c0\u67e5\u5de5\u5177\u3002", "motivation": "\u5e76\u53d1\u7cfb\u7edf\u96be\u4ee5\u9a8c\u8bc1\uff0c\u73b0\u6709\u5de5\u5177\u4f9d\u8d56\u4fb5\u5165\u5f0f\u63d2\u6869\u6216\u4e0d\u73b0\u5b9e\u7684\u6267\u884c\u6a21\u578b\uff0c\u4e9f\u9700\u66f4\u9ad8\u6548\u7075\u6d3b\u7684\u9a8c\u8bc1\u65b9\u6848\u3002", "method": "\u5c06\u7cfb\u7edf\u4e8b\u4ef6\u89c6\u4e3a\u5e26\u65f6\u95f4\u7a97\u53e3\u7684\u9ed1\u76d2\uff0c\u5728TLA+\u4e2d\u8d4b\u4e88\u8bed\u4e49\uff0c\u5229\u7528\u73b0\u6210\u6a21\u578b\u68c0\u67e5\u5668\u6c42\u89e3\u52a8\u4f5c\u5168\u5e8f\uff0c\u652f\u6301\u975e\u7ebf\u6027\u5316\u884c\u4e3a\u5efa\u6a21\u3002", "result": "\u6210\u529f\u9a8c\u8bc1WiredTiger\u3001BAT\u548cConcurrentQueue\u7cfb\u7edf\uff0c\u6539\u8fdb\u5df2\u6709\u6a21\u578b\uff0c\u53d1\u73b0\u4e24\u4e2a\u6b64\u524d\u672a\u77e5\u7684bug\u5e76\u83b7\u4f5c\u8005\u786e\u8ba4\u3002", "conclusion": "OmniLink\u5728\u5927\u89c4\u6a21\u9a8c\u8bc1\u4efb\u52a1\u4e2d\u6027\u80fd\u8d85\u8d8a\u73b0\u6709\u5de5\u5177\uff0c\u4e3a\u5de5\u4e1a\u7ea7\u4e0e\u7814\u7a76\u7ea7\u5e76\u53d1\u7cfb\u7edf\u63d0\u4f9b\u5f3a\u5927\u800c\u7075\u6d3b\u7684\u5f62\u5f0f\u5316\u9a8c\u8bc1\u652f\u6301\u3002"}}
{"id": "2601.14148", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2601.14148", "abs": "https://arxiv.org/abs/2601.14148", "authors": ["Meng Li", "Tong Xie", "Zuodong Zhang", "Runsheng Wang"], "title": "The Quest for Reliable AI Accelerators: Cross-Layer Evaluation and Design Optimization", "comment": "4 pages, 9 figures. Invited paper at ASICON 2025", "summary": "As the CMOS technology pushes to the nanoscale, aging effects and process variations have become increasingly pronounced, posing significant reliability challenges for AI accelerators. Traditional guardband-based design approaches, which rely on pessimistic timing margin, sacrifice significant performance and computational efficiency, rendering them inadequate for high-performance AI computing demands. Current reliability-aware AI accelerator design faces two core challenges: (1) the lack of systematic cross-layer analysis tools to capture coupling reliability effects across device, circuit, architecture, and application layers; and (2) the fundamental trade-off between conventional reliability optimization and computational efficiency. To address these challenges, this paper systematically presents a series of reliability-aware accelerator designs, encompassing (1) aging and variation-aware dynamic timing analyzer, (2) accelerator dataflow optimization using critical input pattern reduction, and (3) resilience characterization and novel architecture design for large language models (LLMs). By tightly integrating cross-layer reliability modeling and AI workload characteristics, these co-optimization approaches effectively achieve reliable and efficient AI acceleration.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u7cfb\u5217\u8de8\u5c42\u53ef\u9760\u6027\u611f\u77e5\u7684AI\u52a0\u901f\u5668\u8bbe\u8ba1\u65b9\u6cd5\uff0c\u4ee5\u5e94\u5bf9\u7eb3\u7c73\u7ea7CMOS\u6280\u672f\u4e0b\u7684\u8001\u5316\u4e0e\u5de5\u827a\u53d8\u5f02\u95ee\u9898\u3002", "motivation": "\u4f20\u7edf\u57fa\u4e8e\u4fdd\u62a4\u5e26\u7684\u8bbe\u8ba1\u727a\u7272\u6027\u80fd\u6548\u7387\uff0c\u65e0\u6cd5\u6ee1\u8db3\u9ad8\u6027\u80fdAI\u8ba1\u7b97\u9700\u6c42\uff0c\u4e14\u7f3a\u4e4f\u7cfb\u7edf\u6027\u8de8\u5c42\u5206\u6790\u5de5\u5177\u3002", "method": "\u63d0\u51fa\u8001\u5316\u4e0e\u53d8\u5f02\u611f\u77e5\u7684\u52a8\u6001\u65f6\u5e8f\u5206\u6790\u5668\u3001\u5173\u952e\u8f93\u5165\u6a21\u5f0f\u51cf\u5c11\u7684\u6570\u636e\u6d41\u4f18\u5316\u3001\u4ee5\u53ca\u9762\u5411\u5927\u8bed\u8a00\u6a21\u578b\u7684\u5f39\u6027\u67b6\u6784\u8bbe\u8ba1\u3002", "result": "\u901a\u8fc7\u6574\u5408\u8de8\u5c42\u53ef\u9760\u6027\u5efa\u6a21\u4e0eAI\u8d1f\u8f7d\u7279\u5f81\uff0c\u5b9e\u73b0\u53ef\u9760\u4e14\u9ad8\u6548\u7684AI\u52a0\u901f\u3002", "conclusion": "\u6240\u63d0\u65b9\u6cd5\u6709\u6548\u5e73\u8861\u53ef\u9760\u6027\u4e0e\u8ba1\u7b97\u6548\u7387\uff0c\u4e3a\u4e0b\u4e00\u4ee3AI\u52a0\u901f\u5668\u63d0\u4f9b\u53ef\u884c\u8bbe\u8ba1\u8def\u5f84\u3002"}}
{"id": "2601.13087", "categories": ["cs.NI"], "pdf": "https://arxiv.org/pdf/2601.13087", "abs": "https://arxiv.org/abs/2601.13087", "authors": ["Max Ilsen", "Daniel Otten", "Nils Aschenbruck", "Markus Chimani"], "title": "No Traffic to Cry: Traffic-Oblivious Link Deactivation for Green Traffic Engineering", "comment": "Appears in the Proceedings of the IEEE International Conference on Computer Communications 2026 (INFOCOM 2026)", "summary": "As internet traffic grows, the underlying infrastructure consumes increasing amounts of energy. During off-peak hours, large parts of the networks remain underutilized, presenting significant potential for energy savings. Existing Green Traffic Engineering approaches attempt to leverage this potential by switching off those parts of the networks that are not required for the routing of specific traffic matrices. When traffic changes, the approaches need to adapt rapidly, which is hard to achieve given the complexity of the problem. We take a fundamentally different approach: instead of considering a specific traffic matrix, we rely on a traffic-oblivious routing scheme. We discuss the NP-hard problem of activating as few connections as possible while still guaranteeing that any down-scaled traffic matrix $\\varrho\\cdot T$ can be routed, where $\\varrho \\in (0,1)$ and $T$ is any traffic matrix routable in the original network. We present a $\\max(\\frac{1}{\\varrho\\cdot\u03bb_{\\text{min}}},2)$-approximation algorithm for this problem, with $\u03bb_{\\text{min}}$ denoting the minimum number of connections between any two connected routers. Additionally, we propose two post-processing heuristics to further improve solution quality. Our evaluation shows that we can quickly generate near-optimal solutions. By design, our method avoids the need for frequent reconfigurations and offers a promising direction to achieve practical energy savings in backbone networks.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u6d41\u91cf\u65e0\u5173\u7684\u8def\u7531\u65b9\u6848\uff0c\u901a\u8fc7\u6700\u5c0f\u5316\u6fc0\u6d3b\u8fde\u63a5\u6570\u5b9e\u73b0\u9aa8\u5e72\u7f51\u8282\u80fd\uff0c\u907f\u514d\u9891\u7e41\u91cd\u914d\u7f6e\u3002", "motivation": "\u73b0\u6709\u7eff\u8272\u6d41\u91cf\u5de5\u7a0b\u65b9\u6cd5\u4f9d\u8d56\u7279\u5b9a\u6d41\u91cf\u77e9\u9635\uff0c\u96be\u4ee5\u5feb\u901f\u9002\u5e94\u6d41\u91cf\u53d8\u5316\uff0c\u4e14\u590d\u6742\u5ea6\u9ad8\u3002", "method": "\u8bbe\u8ba1\u4e00\u4e2aNP\u96be\u95ee\u9898\u7684\u8fd1\u4f3c\u7b97\u6cd5\uff0c\u4fdd\u8bc1\u4efb\u610f\u7f29\u653e\u6d41\u91cf\u77e9\u9635\u53ef\u8def\u7531\uff0c\u5e76\u5f15\u5165\u4e24\u79cd\u540e\u5904\u7406\u542f\u53d1\u5f0f\u4f18\u5316\u89e3\u8d28\u91cf\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\u80fd\u5feb\u901f\u751f\u6210\u8fd1\u4f18\u89e3\uff0c\u65e0\u9700\u9891\u7e41\u91cd\u914d\u7f6e\uff0c\u663e\u8457\u63d0\u5347\u9aa8\u5e72\u7f51\u80fd\u6548\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u9aa8\u5e72\u7f51\u8282\u80fd\u63d0\u4f9b\u5b9e\u7528\u3001\u7a33\u5b9a\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5177\u6709\u5b9e\u9645\u90e8\u7f72\u6f5c\u529b\u3002"}}
{"id": "2601.11868", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.11868", "abs": "https://arxiv.org/abs/2601.11868", "authors": ["Mike A. Merrill", "Alexander G. Shaw", "Nicholas Carlini", "Boxuan Li", "Harsh Raj", "Ivan Bercovich", "Lin Shi", "Jeong Yeon Shin", "Thomas Walshe", "E. Kelly Buchanan", "Junhong Shen", "Guanghao Ye", "Haowei Lin", "Jason Poulos", "Maoyu Wang", "Marianna Nezhurina", "Jenia Jitsev", "Di Lu", "Orfeas Menis Mastromichalakis", "Zhiwei Xu", "Zizhao Chen", "Yue Liu", "Robert Zhang", "Leon Liangyu Chen", "Anurag Kashyap", "Jan-Lucas Uslu", "Jeffrey Li", "Jianbo Wu", "Minghao Yan", "Song Bian", "Vedang Sharma", "Ke Sun", "Steven Dillmann", "Akshay Anand", "Andrew Lanpouthakoun", "Bardia Koopah", "Changran Hu", "Etash Guha", "Gabriel H. S. Dreiman", "Jiacheng Zhu", "Karl Krauth", "Li Zhong", "Niklas Muennighoff", "Robert Amanfu", "Shangyin Tan", "Shreyas Pimpalgaonkar", "Tushar Aggarwal", "Xiangning Lin", "Xin Lan", "Xuandong Zhao", "Yiqing Liang", "Yuanli Wang", "Zilong Wang", "Changzhi Zhou", "David Heineman", "Hange Liu", "Harsh Trivedi", "John Yang", "Junhong Lin", "Manish Shetty", "Michael Yang", "Nabil Omi", "Negin Raoof", "Shanda Li", "Terry Yue Zhuo", "Wuwei Lin", "Yiwei Dai", "Yuxin Wang", "Wenhao Chai", "Shang Zhou", "Dariush Wahdany", "Ziyu She", "Jiaming Hu", "Zhikang Dong", "Yuxuan Zhu", "Sasha Cui", "Ahson Saiyed", "Arinbj\u00f6rn Kolbeinsson", "Jesse Hu", "Christopher Michael Rytting", "Ryan Marten", "Yixin Wang", "Alex Dimakis", "Andy Konwinski", "Ludwig Schmidt"], "title": "Terminal-Bench: Benchmarking Agents on Hard, Realistic Tasks in Command Line Interfaces", "comment": null, "summary": "AI agents may soon become capable of autonomously completing valuable, long-horizon tasks in diverse domains. Current benchmarks either do not measure real-world tasks, or are not sufficiently difficult to meaningfully measure frontier models. To this end, we present Terminal-Bench 2.0: a carefully curated hard benchmark composed of 89 tasks in computer terminal environments inspired by problems from real workflows. Each task features a unique environment, human-written solution, and comprehensive tests for verification. We show that frontier models and agents score less than 65\\% on the benchmark and conduct an error analysis to identify areas for model and agent improvement. We publish the dataset and evaluation harness to assist developers and researchers in future work at https://www.tbench.ai/ .", "AI": {"tldr": "Terminal-Bench 2.0\u662f\u4e00\u4e2a\u5305\u542b89\u4e2a\u771f\u5b9e\u5de5\u4f5c\u6d41\u542f\u53d1\u7684\u7ec8\u7aef\u4efb\u52a1\u7684\u9ad8\u96be\u5ea6\u57fa\u51c6\uff0c\u7528\u4e8e\u8bc4\u4f30\u524d\u6cbfAI\u6a21\u578b\u548c\u667a\u80fd\u4f53\u5728\u957f\u7a0b\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u3002", "motivation": "\u5f53\u524d\u57fa\u51c6\u65e0\u6cd5\u6709\u6548\u8861\u91cfAI\u667a\u80fd\u4f53\u5728\u771f\u5b9e\u590d\u6742\u4efb\u52a1\u4e2d\u7684\u80fd\u529b\uff0c\u56e0\u6b64\u9700\u8981\u66f4\u96be\u3001\u66f4\u8d34\u8fd1\u73b0\u5b9e\u7684\u8bc4\u6d4b\u6807\u51c6\u3002", "method": "\u6784\u5efa\u5305\u542b\u72ec\u7acb\u73af\u5883\u3001\u4eba\u5de5\u89e3\u51b3\u65b9\u6848\u548c\u5b8c\u6574\u9a8c\u8bc1\u6d4b\u8bd5\u7684\u7ec8\u7aef\u4efb\u52a1\u96c6\uff0c\u5e76\u5bf9\u524d\u6cbf\u6a21\u578b\u8fdb\u884c\u8bc4\u5206\u4e0e\u9519\u8bef\u5206\u6790\u3002", "result": "\u524d\u6cbf\u6a21\u578b\u548c\u667a\u80fd\u4f53\u5f97\u5206\u4f4e\u4e8e65%\uff0c\u63ed\u793a\u4e86\u5f53\u524d\u7cfb\u7edf\u5728\u590d\u6742\u4efb\u52a1\u4e2d\u7684\u4e0d\u8db3\u3002", "conclusion": "\u8be5\u57fa\u51c6\u4e3a\u672a\u6765AI\u667a\u80fd\u4f53\u7814\u53d1\u63d0\u4f9b\u4e86\u53ef\u9760\u8bc4\u4f30\u5de5\u5177\uff0c\u5e76\u516c\u5f00\u6570\u636e\u96c6\u548c\u8bc4\u6d4b\u6846\u67b6\u4ee5\u4fc3\u8fdb\u7814\u7a76\u3002"}}
{"id": "2601.13040", "categories": ["cs.DC", "cs.AR", "cs.PL"], "pdf": "https://arxiv.org/pdf/2601.13040", "abs": "https://arxiv.org/abs/2601.13040", "authors": ["Harry Fitchett", "Charles Fox"], "title": "CPU-less parallel execution of lambda calculus in digital logic", "comment": null, "summary": "While transistor density is still increasing, clock speeds are not, motivating the search for new parallel architectures. One approach is to completely abandon the concept of CPU -- and thus serial imperative programming -- and instead to specify and execute tasks in parallel, compiling from programming languages to data flow digital logic. It is well-known that pure functional languages are inherently parallel, due to the Church-Rosser theorem, and CPU-based parallel compilers exist for many functional languages. However, these still rely on conventional CPUs and their von Neumann bottlenecks. An alternative is to compile functional languages directly into digital logic to maximize available parallelism. It is difficult to work with complete modern functional languages due to their many features, so we demonstrate a proof-of-concept system using lambda calculus as the source language and compiling to digital logic. We show how functional hardware can be tailored to a simplistic functional language, forming the ground for a new model of CPU-less functional computation. At the algorithmic level, we use a tree-based representation, with data localized within nodes and communicated data passed between them. This is implemented by physical digital logic blocks corresponding to nodes, and buses enabling message passing. Node types and behaviors correspond to lambda grammar forms, and beta-reductions are performed in parallel allowing branches independent from one another to perform transformations simultaneously. As evidence for this approach, we present an implementation, along with simulation results, showcasing successful execution of lambda expressions. This suggests that the approach could be scaled to larger functional languages. Successful execution of a test suite of lambda expressions suggests that the approach could be scaled to larger functional languages.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u5c06\u51fd\u6570\u5f0f\u8bed\u8a00\u76f4\u63a5\u7f16\u8bd1\u4e3a\u6570\u5b57\u903b\u8f91\u7684\u5e76\u884c\u8ba1\u7b97\u67b6\u6784\uff0c\u4ee5\u6446\u8131\u4f20\u7edfCPU\u9650\u5236\uff0c\u901a\u8fc7\u03bb\u6f14\u7b97\u5b9e\u73b0\u5e76\u884c\u5316\u786c\u4ef6\u6267\u884c\u3002", "motivation": "\u6676\u4f53\u7ba1\u5bc6\u5ea6\u6301\u7eed\u589e\u957f\u4f46\u65f6\u949f\u9891\u7387\u505c\u6ede\uff0c\u4fc3\u4f7f\u63a2\u7d22\u65e0CPU\u7684\u7eaf\u5e76\u884c\u67b6\u6784\uff0c\u5229\u7528\u51fd\u6570\u5f0f\u8bed\u8a00\u56fa\u6709\u5e76\u884c\u6027\u7a81\u7834\u51af\u00b7\u8bfa\u4f9d\u66fc\u74f6\u9888\u3002", "method": "\u91c7\u7528\u6811\u72b6\u7ed3\u6784\u8868\u793a\u7b97\u6cd5\uff0c\u8282\u70b9\u672c\u5730\u5316\u6570\u636e\u5e76\u901a\u8fc7\u603b\u7ebf\u901a\u4fe1\uff1b\u8282\u70b9\u7c7b\u578b\u5bf9\u5e94\u03bb\u8bed\u6cd5\u5f62\u5f0f\uff0c\u5e76\u884c\u6267\u884c\u03b2\u5f52\u7ea6\uff1b\u4ee5\u03bb\u6f14\u7b97\u4e3a\u6e90\u8bed\u8a00\u7f16\u8bd1\u81f3\u6570\u5b57\u903b\u8f91\u5757\u3002", "result": "\u6210\u529f\u5b9e\u73b0\u5e76\u4eff\u771f\u8fd0\u884c\u03bb\u8868\u8fbe\u5f0f\uff0c\u9a8c\u8bc1\u8be5\u67b6\u6784\u53ef\u884c\u6027\uff0c\u8868\u660e\u53ef\u6269\u5c55\u81f3\u66f4\u590d\u6742\u7684\u51fd\u6570\u5f0f\u8bed\u8a00\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u65e0CPU\u7684\u51fd\u6570\u5f0f\u8ba1\u7b97\u63d0\u4f9b\u4e86\u65b0\u6a21\u578b\u57fa\u7840\uff0c\u6709\u671b\u652f\u6301\u66f4\u5927\u89c4\u6a21\u51fd\u6570\u5f0f\u8bed\u8a00\u7684\u9ad8\u6548\u5e76\u884c\u786c\u4ef6\u5b9e\u73b0\u3002"}}
{"id": "2601.11822", "categories": ["cs.DC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.11822", "abs": "https://arxiv.org/abs/2601.11822", "authors": ["Amna Masood", "Pratishtha Gaur", "Nuwan Jayasena"], "title": "RAPID-Serve: Resource-efficient and Accelerated P/D Intra-GPU Disaggregation", "comment": null, "summary": "Two widely adopted techniques for LLM inference serving systems today are hybrid batching and disaggregated serving. A hybrid batch combines prefill and decode tokens of different requests in the same batch to improve resource utilization and throughput at the cost of increased latency per token. In contrast, disaggregated serving decouples compute-bound prefill and bandwidth-bound decode phases to optimize for service level objectives (SLOs) at the cost of resource under-utilization and KV-cache transfer overheads. To address the limitations of these techniques, we propose RAPID-Serve: a technique to concurrently execute prefill and decode on the same GPU(s) to meet latency SLOs while maintaining high throughput and efficient resource utilization. Furthermore, we propose Adaptive Resource Management for runtime compute resource allocation, optionally leveraging CU masking (a fine-grained Compute Unit partitioning feature on AMD Instinct\\textsuperscript{TM} GPUs). RAPID-Serve provides up to 4.1x (average 1.7x) unconstrained throughput improvement and 32x and higher (average 4.9x) throughput improvement under SLO constraints, showing it as an effective strategy compared to the state-of-the-art approaches, particularly in resource-constrained environments.", "AI": {"tldr": "RAPID-Serve \u63d0\u51fa\u5728\u5355 GPU \u4e0a\u5e76\u53d1\u6267\u884c\u9884\u586b\u5145\u4e0e\u89e3\u7801\uff0c\u7ed3\u5408\u81ea\u9002\u5e94\u8d44\u6e90\u7ba1\u7406\uff0c\u5728\u6ee1\u8db3\u5ef6\u8fdf SLO \u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u541e\u5410\u91cf\u548c\u8d44\u6e90\u5229\u7528\u7387\u3002", "motivation": "\u73b0\u6709\u6df7\u5408\u6279\u5904\u7406\u589e\u52a0\u5ef6\u8fdf\uff0c\u89e3\u8026\u670d\u52a1\u5bfc\u81f4\u8d44\u6e90\u6d6a\u8d39\uff0c\u9700\u517c\u987e\u4f4e\u5ef6\u8fdf\u4e0e\u9ad8\u541e\u5410\u7684\u65b9\u6848\u3002", "method": "\u5728\u540c GPU \u5e76\u53d1\u6267\u884c\u9884\u586b\u5145\u4e0e\u89e3\u7801\uff0c\u8f85\u4ee5\u81ea\u9002\u5e94\u8d44\u6e90\u5206\u914d\u53ca AMD CU \u63a9\u7801\u6280\u672f\u3002", "result": "\u65e0\u7ea6\u675f\u4e0b\u541e\u5410\u63d0\u5347\u6700\u9ad8 4.1 \u500d\uff08\u5e73\u5747 1.7 \u500d\uff09\uff0cSLO \u7ea6\u675f\u4e0b\u6700\u9ad8 32 \u500d\uff08\u5e73\u5747 4.9 \u500d\uff09\u3002", "conclusion": "RAPID-Serve \u5728\u8d44\u6e90\u53d7\u9650\u573a\u666f\u4e2d\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u517c\u987e\u5ef6\u8fdf\u3001\u541e\u5410\u4e0e\u6548\u7387\u3002"}}
{"id": "2601.11926", "categories": ["cs.SE", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.11926", "abs": "https://arxiv.org/abs/2601.11926", "authors": ["Ananya Halgatti", "Shaunak Biswas", "Hiya Bhatt", "Srinivasan Rakhunathan", "Karthik Vaidhyanathan"], "title": "Harmonica: A Self-Adaptation Exemplar for Sustainable MLOps", "comment": "This paper has been accepted to SEAMS 2026 Artifact Track", "summary": "Machine learning enabled systems (MLS) often operate in settings where they regularly encounter uncertainties arising from changes in their surrounding environment. Without structured oversight, such changes can degrade model behavior, increase operational cost, and reduce the usefulness of deployed systems. Although Machine Learning Operations (MLOps) streamlines the lifecycle of ML models, it provides limited support for addressing runtime uncertainties that influence the longer term sustainability of MLS. To support continued viability, these systems need a mechanism that detects when execution drifts outside acceptable bounds and adjusts system behavior in response. Despite the growing interest in sustainable and self-adaptive MLS, there has been limited work towards exemplars that allow researchers to study these challenges in MLOps pipelines. This paper presents Harmonica, a self-adaptation exemplar built on the HarmonE approach, designed to enable the sustainable operation of such pipelines. Harmonica introduces structured adaptive control through MAPE-K loop, separating high-level adaptation policy from low-level tactic execution. It continuously monitors sustainability metrics, evaluates them against dynamic adaptation boundaries, and automatically triggers architectural tactics when thresholds are violated. We demonstrate the tool through case studies in time series regression and computer vision, examining its ability to improve system stability and reduce manual intervention. The results show that Harmonica offers a practical and reusable foundation for enabling adaptive behavior in MLS that rely on MLOps pipelines for sustained operation.", "AI": {"tldr": "Harmonica\u662f\u4e00\u4e2a\u57fa\u4e8eHarmonE\u65b9\u6cd5\u7684\u81ea\u9002\u5e94\u793a\u4f8b\uff0c\u901a\u8fc7MAPE-K\u5faa\u73af\u5b9e\u73b0\u7ed3\u6784\u5316\u81ea\u9002\u5e94\u63a7\u5236\uff0c\u63d0\u5347MLOps\u7ba1\u9053\u4e2d\u673a\u5668\u5b66\u4e60\u7cfb\u7edf\u7684\u53ef\u6301\u7eed\u6027\u3002", "motivation": "\u73b0\u6709MLOps\u5bf9\u8fd0\u884c\u65f6\u4e0d\u786e\u5b9a\u6027\u652f\u6301\u6709\u9650\uff0c\u5f71\u54cd\u673a\u5668\u5b66\u4e60\u7cfb\u7edf\u957f\u671f\u53ef\u6301\u7eed\u6027\uff0c\u4e9f\u9700\u81ea\u9002\u5e94\u673a\u5236\u5e94\u5bf9\u6267\u884c\u6f02\u79fb\u3002", "method": "\u5f15\u5165MAPE-K\u5faa\u73af\uff0c\u5206\u79bb\u9ad8\u5c42\u81ea\u9002\u5e94\u7b56\u7565\u4e0e\u5e95\u5c42\u6218\u672f\u6267\u884c\uff0c\u6301\u7eed\u76d1\u63a7\u53ef\u6301\u7eed\u6027\u6307\u6807\u5e76\u89e6\u53d1\u67b6\u6784\u8c03\u6574\u3002", "result": "\u5728\u65f6\u95f4\u5e8f\u5217\u56de\u5f52\u548c\u8ba1\u7b97\u673a\u89c6\u89c9\u6848\u4f8b\u4e2d\u9a8c\u8bc1\u4e86\u7cfb\u7edf\u7a33\u5b9a\u6027\u63d0\u5347\u4e0e\u4eba\u5de5\u5e72\u9884\u51cf\u5c11\u3002", "conclusion": "Harmonica\u4e3a\u4f9d\u8d56MLOps\u7ba1\u9053\u7684\u673a\u5668\u5b66\u4e60\u7cfb\u7edf\u63d0\u4f9b\u4e86\u5b9e\u7528\u3001\u53ef\u590d\u7528\u7684\u81ea\u9002\u5e94\u884c\u4e3a\u57fa\u7840\u3002"}}
{"id": "2601.14159", "categories": ["cs.DC", "cs.AR"], "pdf": "https://arxiv.org/pdf/2601.14159", "abs": "https://arxiv.org/abs/2601.14159", "authors": ["Panagiotis-Eleftherios Eleftherakis", "George Anagnostopoulos", "Anastassis Kapetanakis", "Mohammad Umair", "Jean-Yves Vet", "Konstantinos Iliakis", "Jonathan Vincent", "Jing Gong", "Akshay Patil", "Clara Garc\u00eda-S\u00e1nchez", "Gerardo Zampino", "Ricardo Vinuesa", "Sotirios Xydis"], "title": "Multi-Partner Project: Multi-GPU Performance Portability Analysis for CFD Simulations at Scale", "comment": "DATE 26 conference Multi-Partner Project Paper", "summary": "As heterogeneous supercomputing architectures leveraging GPUs become increasingly central to high-performance computing (HPC), it is crucial for computational fluid dynamics (CFD) simulations, a de-facto HPC workload, to efficiently utilize such hardware. One of the key challenges of HPC codes is performance portability, i.e. the ability to maintain near-optimal performance across different accelerators. In the context of the \\textbf{REFMAP} project, which targets scalable, GPU-enabled multi-fidelity CFD for urban airflow prediction, this paper analyzes the performance portability of SOD2D, a state-of-the-art Spectral Elements simulation framework across AMD and NVIDIA GPU architectures. We first discuss the physical and numerical models underlying SOD2D, highlighting its computational hotspots. Then, we examine its performance and scalability in a multi-level manner, i.e. defining and characterizing an extensive full-stack design space spanning across application, software and hardware infrastructure related parameters. Single-GPU performance characterization across server-grade NVIDIA and AMD GPU architectures and vendor-specific compiler stacks, show the potential as well as the diverse effect of memory access optimizations, i.e. 0.69$\\times$ - 3.91$\\times$ deviations in acceleration speedup. Performance variability of SOD2D at scale is further examined on the LUMI multi-GPU cluster, where profiling reveals similar throughput variations, highlighting the limits of performance projections and the need for multi-level, informed tuning.", "AI": {"tldr": "\u672c\u6587\u5206\u6790\u4e86SOD2D\u8c31\u5143\u6cd5CFD\u6846\u67b6\u5728AMD\u4e0eNVIDIA GPU\u4e0a\u7684\u6027\u80fd\u53ef\u79fb\u690d\u6027\uff0c\u63ed\u793a\u5185\u5b58\u4f18\u5316\u6548\u679c\u5dee\u5f02\u53ca\u591a\u7ea7\u8c03\u4f18\u5fc5\u8981\u6027\u3002", "motivation": "\u5f02\u6784\u8d85\u7b97\u67b6\u6784\u666e\u53ca\u4e0b\uff0cCFD\u6a21\u62df\u9700\u9ad8\u6548\u9002\u914d\u4e0d\u540cGPU\u786c\u4ef6\uff0c\u5b9e\u73b0\u6027\u80fd\u53ef\u79fb\u690d\u6027\u4ee5\u652f\u6491\u57ce\u5e02\u6c14\u6d41\u9884\u6d4b\u7b49HPC\u5e94\u7528\u3002", "method": "\u901a\u8fc7\u6784\u5efa\u6db5\u76d6\u5e94\u7528\u3001\u8f6f\u4ef6\u4e0e\u786c\u4ef6\u53c2\u6570\u7684\u5168\u6808\u8bbe\u8ba1\u7a7a\u95f4\uff0c\u5728\u5355\u5361\u4e0eLUMI\u591a\u5361\u96c6\u7fa4\u4e0a\u5bf9SOD2D\u8fdb\u884c\u591a\u5c42\u6b21\u6027\u80fd\u8868\u5f81\u4e0e\u5256\u6790\u3002", "result": "\u5355GPU\u6d4b\u8bd5\u663e\u793a\u5185\u5b58\u8bbf\u95ee\u4f18\u5316\u5e26\u67650.69\u00d7\u81f33.91\u00d7\u52a0\u901f\u5dee\u5f02\uff1b\u591aGPU\u96c6\u7fa4\u4e2d\u541e\u5410\u6ce2\u52a8\u8868\u660e\u6027\u80fd\u9884\u6d4b\u5b58\u5728\u5c40\u9650\uff0c\u9700\u4f9d\u8d56\u591a\u5c42\u6b21\u8c03\u4f18\u3002", "conclusion": "SOD2D\u5728\u8de8\u5382\u5546GPU\u67b6\u6784\u4e0a\u5177\u5907\u6027\u80fd\u6f5c\u529b\uff0c\u4f46\u9700\u7ed3\u5408\u591a\u5c42\u6b21\u53c2\u6570\u8c03\u4f18\u624d\u80fd\u5b9e\u73b0\u7a33\u5b9a\u9ad8\u6548\u7684\u6027\u80fd\u53ef\u79fb\u690d\u6027\u3002"}}
{"id": "2601.11935", "categories": ["cs.DC", "cs.AI", "cs.SE"], "pdf": "https://arxiv.org/pdf/2601.11935", "abs": "https://arxiv.org/abs/2601.11935", "authors": ["Milan Parikh", "Aniket Abhishek Soni", "Sneja Mitinbhai Shah", "Ayush Raj Jha"], "title": "Big Data Workload Profiling for Energy-Aware Cloud Resource Management", "comment": "10 pages, 3 figures. Accepted and presented at the 2026 International Conference on Data Analytics for Sustainability and Engineering Technology (DASET 2026), Track: Big Data and Machine Learning Applications", "summary": "Cloud data centers face increasing pressure to reduce operational energy consumption as big data workloads continue to grow in scale and complexity. This paper presents a workload aware and energy efficient scheduling framework that profiles CPU utilization, memory demand, and storage IO behavior to guide virtual machine placement decisions. By combining historical execution logs with real time telemetry, the proposed system predicts the energy and performance impact of candidate placements and enables adaptive consolidation while preserving service level agreement compliance. The framework is evaluated using representative Hadoop MapReduce, Spark MLlib, and ETL workloads deployed on a multi node cloud testbed. Experimental results demonstrate consistent energy savings of 15 to 20 percent compared to a baseline scheduler, with negligible performance degradation. These findings highlight workload profiling as a practical and scalable strategy for improving the sustainability of cloud based big data processing environments.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5de5\u4f5c\u8d1f\u8f7d\u611f\u77e5\u7684\u8282\u80fd\u8c03\u5ea6\u6846\u67b6\uff0c\u901a\u8fc7\u5206\u6790CPU\u3001\u5185\u5b58\u548c\u5b58\u50a8IO\u884c\u4e3a\u4f18\u5316\u865a\u62df\u673a\u90e8\u7f72\uff0c\u5728\u4fdd\u8bc1\u6027\u80fd\u7684\u540c\u65f6\u5b9e\u73b015-20%\u7684\u80fd\u8017\u964d\u4f4e\u3002", "motivation": "\u5e94\u5bf9\u5927\u6570\u636e\u5de5\u4f5c\u8d1f\u8f7d\u589e\u957f\u5e26\u6765\u7684\u4e91\u6570\u636e\u4e2d\u5fc3\u80fd\u8017\u538b\u529b\uff0c\u63d0\u5347\u53ef\u6301\u7eed\u6027\u3002", "method": "\u7ed3\u5408\u5386\u53f2\u65e5\u5fd7\u4e0e\u5b9e\u65f6\u9065\u6d4b\u6570\u636e\uff0c\u9884\u6d4b\u5019\u9009\u90e8\u7f72\u65b9\u6848\u7684\u80fd\u8017\u4e0e\u6027\u80fd\u5f71\u54cd\uff0c\u5b9e\u73b0\u81ea\u9002\u5e94\u6574\u5408\u3002", "result": "\u5728\u591a\u8282\u70b9\u4e91\u6d4b\u8bd5\u5e73\u53f0\u4e0a\uff0c\u76f8\u6bd4\u57fa\u7ebf\u8c03\u5ea6\u5668\u5b9e\u73b015%-20%\u7684\u7a33\u5b9a\u8282\u80fd\uff0c\u6027\u80fd\u635f\u5931\u53ef\u5ffd\u7565\u3002", "conclusion": "\u5de5\u4f5c\u8d1f\u8f7d\u753b\u50cf\u662f\u4e00\u79cd\u5b9e\u7528\u4e14\u53ef\u6269\u5c55\u7684\u7b56\u7565\uff0c\u6709\u52a9\u4e8e\u63d0\u5347\u4e91\u73af\u5883\u5927\u6570\u636e\u5904\u7406\u7684\u80fd\u6548\u4e0e\u53ef\u6301\u7eed\u6027\u3002"}}
{"id": "2601.11972", "categories": ["cs.SE", "cs.CR"], "pdf": "https://arxiv.org/pdf/2601.11972", "abs": "https://arxiv.org/abs/2601.11972", "authors": ["Chi Thien Tran"], "title": "Enhancing Fuzz Testing Efficiency through Automated Fuzz Target Generation", "comment": "4 tables, 4 figures, 7 pages", "summary": "Fuzzing continues to be the most effective method for identifying security vulnerabilities in software. In the context of fuzz testing, the fuzzer supplies varied inputs to fuzz targets, which are designed to comprehensively exercise critical sections of the client code. Various studies have focused on optimizing and developing advanced fuzzers, such as AFL++, libFuzzer, Honggfuzz, syzkaller, ISP-Fuzzer, which have substantially enhanced vulnerability detection in widely used software and libraries. Nevertheless, achieving greater coverage necessitates improvements in both the quality and quantity of fuzz targets. In large-scale software projects and libraries -- characterized by numerous user defined functions and data types -- manual creation of fuzz targets is both labor-intensive and time-consuming. This challenge underscores the need for automated techniques not only to generate fuzz targets but also to streamline the execution and analysis of their results. In this paper, we introduce an approach to improving fuzz target generation through static analysis of library source code. The proposed method encompasses several key aspects: it analyzes source code structures to accurately construct function calls and generate fuzz targets; it maps fuzzer input data to the corresponding function parameters; it synthesizes compilation information for the fuzz targets; and it automatically collects and analyzes execution results. Our findings are demonstrated through the application of this approach to the generation of fuzz targets for C/C++ libraries.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u9759\u6001\u5206\u6790\u7684\u81ea\u52a8\u5316\u6a21\u7cca\u6d4b\u8bd5\u76ee\u6807\u751f\u6210\u65b9\u6cd5\uff0c\u63d0\u5347C/C++\u5e93\u7684\u8986\u76d6\u7387\u4e0e\u6f0f\u6d1e\u68c0\u6d4b\u6548\u7387\u3002", "motivation": "\u5927\u578b\u8f6f\u4ef6\u9879\u76ee\u4e2d\u624b\u52a8\u521b\u5efa\u6a21\u7cca\u6d4b\u8bd5\u76ee\u6807\u8017\u65f6\u8d39\u529b\uff0c\u9700\u81ea\u52a8\u5316\u6280\u672f\u63d0\u5347\u6548\u7387\u4e0e\u8d28\u91cf\u3002", "method": "\u901a\u8fc7\u9759\u6001\u5206\u6790\u6e90\u7801\u7ed3\u6784\uff0c\u6784\u5efa\u51fd\u6570\u8c03\u7528\u3001\u6620\u5c04\u8f93\u5165\u53c2\u6570\u3001\u5408\u6210\u7f16\u8bd1\u4fe1\u606f\u5e76\u81ea\u52a8\u6536\u96c6\u5206\u6790\u6267\u884c\u7ed3\u679c\u3002", "result": "\u6210\u529f\u5e94\u7528\u4e8eC/C++\u5e93\uff0c\u6709\u6548\u751f\u6210\u9ad8\u8d28\u91cf\u6a21\u7cca\u6d4b\u8bd5\u76ee\u6807\uff0c\u63d0\u5347\u6d4b\u8bd5\u8986\u76d6\u4e0e\u6267\u884c\u5206\u6790\u81ea\u52a8\u5316\u6c34\u5e73\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u663e\u8457\u964d\u4f4e\u4eba\u5de5\u6210\u672c\uff0c\u589e\u5f3a\u6a21\u7cca\u6d4b\u8bd5\u5728\u590d\u6742\u9879\u76ee\u4e2d\u7684\u5b9e\u7528\u6027\u4e0e\u53ef\u6269\u5c55\u6027\u3002"}}
{"id": "2601.12209", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2601.12209", "abs": "https://arxiv.org/abs/2601.12209", "authors": ["Sana Taghipour Anvari", "Julian Samaroo", "Matin Raayai Ardakani", "David Kaeli"], "title": "DaggerFFT: A Distributed FFT Framework Using Task Scheduling in Julia", "comment": null, "summary": "The Fast Fourier Transform (FFT) is a fundamental numerical technique with widespread application in a range of scientific problems. As scientific simulations attempt to exploit exascale systems, there has been a growing demand for distributed FFT algorithms that can effectively utilize modern heterogeneous high-performance computing (HPC) systems. Conventional FFT algorithms commonly encounter performance bottlenecks, especially when run on heterogeneous platforms. Most distributed FFT approaches rely on static task distribution and require synchronization barriers, limiting scalability and impacting overall resource utilization. In this paper we present DaggerFFT, a distributed FFT framework, developed in Julia, that treats highly parallel FFT computations as a dynamically scheduled task graph. Each FFT stage operates on a separately defined distributed array. FFT operations are expressed as DTasks operating on pencil or slab partitioned DArrays. Each FFT stage owns its own DArray, and the runtime assigns DTasks across devices using Dagger's dynamic scheduler that uses work stealing. We demonstrate how DaggerFFT's dynamic scheduler can outperform state-of-the-art distributed FFT libraries on both CPU and GPU backends, achieving up to a 2.6x speedup on CPU clusters and up to a 1.35x speedup on GPU clusters. We have integrated DaggerFFT into Oceananigans.jl, a geophysical fluid dynamics framework, demonstrating that high-level, task-based runtimes can deliver both superior performance and modularity in large-scale, real-world simulations.", "AI": {"tldr": "DaggerFFT\u662f\u4e00\u4e2a\u57fa\u4e8eJulia\u7684\u5206\u5e03\u5f0fFFT\u6846\u67b6\uff0c\u901a\u8fc7\u52a8\u6001\u4efb\u52a1\u8c03\u5ea6\u5728CPU\u548cGPU\u96c6\u7fa4\u4e0a\u5b9e\u73b0\u4f18\u4e8e\u73b0\u6709\u5e93\u7684\u6027\u80fd\u3002", "motivation": "\u4f20\u7edfFFT\u7b97\u6cd5\u5728\u5f02\u6784\u9ad8\u6027\u80fd\u8ba1\u7b97\u5e73\u53f0\u4e0a\u5b58\u5728\u6027\u80fd\u74f6\u9888\uff0c\u9700\u8981\u66f4\u9ad8\u6548\u7684\u5206\u5e03\u5f0f\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u5c06FFT\u8ba1\u7b97\u5efa\u6a21\u4e3a\u52a8\u6001\u8c03\u5ea6\u7684\u4efb\u52a1\u56fe\uff0c\u5229\u7528Dagger\u7684\u52a8\u6001\u8c03\u5ea6\u5668\u8de8\u8bbe\u5907\u5206\u914d\u4efb\u52a1\uff0c\u907f\u514d\u9759\u6001\u5206\u914d\u4e0e\u540c\u6b65\u5c4f\u969c\u9650\u5236\u3002", "result": "\u5728CPU\u96c6\u7fa4\u4e0a\u6700\u9ad8\u63d0\u901f2.6\u500d\uff0cGPU\u96c6\u7fa4\u4e0a\u63d0\u901f1.35\u500d\uff0c\u5e76\u6210\u529f\u96c6\u6210\u5230Oceananigans.jl\u7528\u4e8e\u771f\u5b9e\u5927\u89c4\u6a21\u6a21\u62df\u3002", "conclusion": "\u57fa\u4e8e\u4efb\u52a1\u7684\u9ad8\u5c42\u8fd0\u884c\u65f6\u7cfb\u7edf\u53ef\u5728\u4fdd\u6301\u6a21\u5757\u5316\u7684\u540c\u65f6\uff0c\u5728\u5f02\u6784HPC\u7cfb\u7edf\u4e0a\u5b9e\u73b0\u5353\u8d8aFFT\u6027\u80fd\u3002"}}
{"id": "2601.12241", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2601.12241", "abs": "https://arxiv.org/abs/2601.12241", "authors": ["Yiwei Jiang", "Sangeeta Chowdhary", "Nathaniel Morris", "Rutwik Jain", "Srilatha Manne", "Sam Bayliss"], "title": "Power Aware Dynamic Reallocation For Inference", "comment": null, "summary": "Disaggregation has emerged as a powerful strategy for optimizing large language model (LLM) inference by separating compute-intensive prefill and memory-bound decode phases across specialized GPUs. This separation improves utilization and throughput under fixed hardware capacity. However, as model and cluster scales grow, power, rather than compute, has become the dominant limiter of overall performance and cost efficiency. In this paper, we propose RAPID, a power-aware disaggregated inference framework that jointly manages GPU roles and power budgets to sustain goodput within strict power caps. RAPID utilizes static and dynamic power reallocation in addition to GPU reallocation to improve performance under fixed power bounds. RAPID improves overall performance and application consistency beyond what is achievable in current disaggregation solutions, resulting in up to a 2x improvement in SLO attainment at peak load when compared to a static assignment without an increase in complexity or cost.", "AI": {"tldr": "RAPID\u662f\u4e00\u79cd\u9762\u5411\u529f\u7387\u611f\u77e5\u7684\u89e3\u8026\u63a8\u7406\u6846\u67b6\uff0c\u901a\u8fc7\u52a8\u6001\u5206\u914dGPU\u89d2\u8272\u4e0e\u529f\u7387\u9884\u7b97\uff0c\u5728\u56fa\u5b9a\u529f\u8017\u9650\u5236\u4e0b\u63d0\u5347LLM\u63a8\u7406\u6027\u80fd\u4e0e\u4e00\u81f4\u6027\u3002", "motivation": "\u968f\u7740\u6a21\u578b\u548c\u96c6\u7fa4\u89c4\u6a21\u6269\u5927\uff0c\u529f\u8017\u800c\u975e\u7b97\u529b\u6210\u4e3a\u6027\u80fd\u4e0e\u6210\u672c\u6548\u7387\u7684\u4e3b\u8981\u74f6\u9888\uff0c\u9700\u4f18\u5316\u73b0\u6709\u89e3\u8026\u63a8\u7406\u65b9\u6848\u3002", "method": "RAPID\u7ed3\u5408\u9759\u6001\u4e0e\u52a8\u6001\u529f\u8017\u91cd\u5206\u914d\u53caGPU\u91cd\u5206\u914d\u7b56\u7565\uff0c\u5728\u4e25\u683c\u529f\u8017\u7ea6\u675f\u4e0b\u8054\u5408\u7ba1\u7406\u8d44\u6e90\u3002", "result": "\u76f8\u6bd4\u9759\u6001\u5206\u914d\u65b9\u6848\uff0cRAPID\u5728\u5cf0\u503c\u8d1f\u8f7d\u4e0bSLO\u8fbe\u6210\u7387\u6700\u9ad8\u63d0\u53472\u500d\uff0c\u4e14\u4e0d\u589e\u52a0\u590d\u6742\u5ea6\u6216\u6210\u672c\u3002", "conclusion": "RAPID\u6709\u6548\u7a81\u7834\u5f53\u524d\u89e3\u8026\u63a8\u7406\u65b9\u6848\u7684\u6027\u80fd\u74f6\u9888\uff0c\u663e\u8457\u63d0\u5347\u5927\u8bed\u8a00\u6a21\u578b\u63a8\u7406\u5728\u529f\u8017\u53d7\u9650\u73af\u5883\u4e0b\u7684\u6548\u7387\u4e0e\u7a33\u5b9a\u6027\u3002"}}
{"id": "2601.12148", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2601.12148", "abs": "https://arxiv.org/abs/2601.12148", "authors": ["Muhammad Umar Zeshan", "Motunrayo Ibiyo", "Claudio Di Sipio", "Phuong T. Nguyen", "Davide Di Ruscio"], "title": "Many Hands Make Light Work: An LLM-based Multi-Agent System for Detecting Malicious PyPI Packages", "comment": "The paper has been peer-reviewed and accepted for publication to the Journal of Systems and Software (https://www.sciencedirect.com/journal/journal-of-systems-and-software)", "summary": "Malicious code in open-source repositories such as PyPI poses a growing threat to software supply chains. Traditional rule-based tools often overlook the semantic patterns in source code that are crucial for identifying adversarial components. Large language models (LLMs) show promise for software analysis, yet their use in interpretable and modular security pipelines remains limited. This paper presents LAMPS, a multi-agent system that employs collaborative LLMs to detect malicious PyPI packages. The system consists of four role-specific agents for package retrieval, file extraction, classification, and verdict aggregation, coordinated through the CrewAI framework. A prototype combines a fine-tuned CodeBERT model for classification with LLaMA-3 agents for contextual reasoning. LAMPS has been evaluated on two complementary datasets: D1, a balanced collection of 6,000 setup.py files, and D2, a realistic multi-file dataset with 1,296 files and natural class imbalance. On D1, LAMPS achieves 97.7% accuracy, surpassing MPHunter--one of the state-of-the-art approaches. On D2, it reaches 99.5% accuracy and 99.5% balanced accuracy, outperforming RAG-based approaches and fine-tuned single-agent baselines. McNemar's test confirmed these improvements as highly significant. The results demonstrate the feasibility of distributed LLM reasoning for malicious code detection and highlight the benefits of modular multi-agent designs in software supply chain security.", "AI": {"tldr": "LAMPS\u662f\u4e00\u4e2a\u57fa\u4e8e\u591a\u667a\u80fd\u4f53LLM\u7684\u7cfb\u7edf\uff0c\u7528\u4e8e\u68c0\u6d4bPyPI\u4e2d\u7684\u6076\u610f\u5305\uff0c\u5728\u4e24\u4e2a\u6570\u636e\u96c6\u4e0a\u5747\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u4f20\u7edf\u89c4\u5219\u5de5\u5177\u96be\u4ee5\u6355\u6349\u4ee3\u7801\u8bed\u4e49\u6a21\u5f0f\uff0c\u800c\u73b0\u6709LLM\u5e94\u7528\u7f3a\u4e4f\u53ef\u89e3\u91ca\u6027\u548c\u6a21\u5757\u5316\uff0c\u6545\u9700\u6784\u5efa\u65b0\u578b\u5b89\u5168\u5206\u6790\u6846\u67b6\u3002", "method": "\u91c7\u7528CrewAI\u6846\u67b6\u534f\u8c03\u56db\u4e2a\u89d2\u8272\u667a\u80fd\u4f53\uff0c\u7ed3\u5408\u5fae\u8c03CodeBERT\u4e0eLLaMA-3\u8fdb\u884c\u5206\u7c7b\u4e0e\u4e0a\u4e0b\u6587\u63a8\u7406\u3002", "result": "\u5728D1\u6570\u636e\u96c6\u51c6\u786e\u738797.7%\uff0cD2\u8fbe99.5%\uff0c\u663e\u8457\u4f18\u4e8eMPHunter\u4e0eRAG\u7b49\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "\u5206\u5e03\u5f0fLLM\u63a8\u7406\u53ef\u884c\uff0c\u6a21\u5757\u5316\u591a\u667a\u80fd\u4f53\u8bbe\u8ba1\u80fd\u6709\u6548\u63d0\u5347\u8f6f\u4ef6\u4f9b\u5e94\u94fe\u5b89\u5168\u6027\u3002"}}
{"id": "2601.12186", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.12186", "abs": "https://arxiv.org/abs/2601.12186", "authors": ["Vatsal Venkatkrishna", "Indraneil Paul", "Iryna Gurevych"], "title": "Aletheia: What Makes RLVR For Code Verifiers Tick?", "comment": "8 pages, 6 figures", "summary": "Multi-domain thinking verifiers trained via Reinforcement Learning from Verifiable Rewards (RLVR) are a prominent fixture of the Large Language Model (LLM) post-training pipeline, owing to their ability to robustly rate and rerank model outputs. However, the adoption of such verifiers towards code generation has been comparatively sparse, with execution feedback constituting the dominant signal. Nonetheless, code verifiers remain valuable toward judging model outputs in scenarios where execution feedback is hard to obtain and are a potentially powerful addition to the code generation post-training toolbox. To this end, we create and open-source Aletheia, a controlled testbed that enables execution-grounded evaluation of code verifiers' robustness across disparate policy models and covariate shifts. We examine components of the RLVR-based verifier training recipe widely credited for its success: (1) intermediate thinking traces, (2) learning from negative samples, and (3) on-policy training. While experiments show the optimality of RLVR, we uncover important opportunities to simplify the recipe. Particularly, despite code verification exhibiting positive training- and inference-time scaling, on-policy learning stands out as the key component at small verifier sizes, and thinking-based training emerges as the most important component at larger scales.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faAletheia\u6d4b\u8bd5\u5e73\u53f0\uff0c\u8bc4\u4f30\u57fa\u4e8eRLVR\u8bad\u7ec3\u7684\u4ee3\u7801\u9a8c\u8bc1\u5668\u5728\u4e0d\u540c\u6a21\u578b\u548c\u5206\u5e03\u504f\u79fb\u4e0b\u7684\u9c81\u68d2\u6027\uff0c\u5e76\u53d1\u73b0\u7b80\u5316\u8bad\u7ec3\u65b9\u6848\u7684\u6f5c\u529b\u3002", "motivation": "\u5f53\u524d\u4ee3\u7801\u751f\u6210\u9886\u57df\u8f83\u5c11\u91c7\u7528\u591a\u57df\u601d\u7ef4\u9a8c\u8bc1\u5668\uff0c\u800c\u6267\u884c\u53cd\u9988\u96be\u4ee5\u83b7\u53d6\u65f6\uff0c\u6b64\u7c7b\u9a8c\u8bc1\u5668\u5177\u6709\u91cd\u8981\u4ef7\u503c\u3002", "method": "\u6784\u5efaAletheia\u6d4b\u8bd5\u5e73\u53f0\uff0c\u7cfb\u7edf\u7814\u7a76RLVR\u8bad\u7ec3\u4e2d\u4e2d\u95f4\u601d\u7ef4\u8f68\u8ff9\u3001\u8d1f\u6837\u672c\u5b66\u4e60\u548c\u5728\u7ebf\u7b56\u7565\u8bad\u7ec3\u4e09\u4e2a\u5173\u952e\u7ec4\u4ef6\u7684\u6548\u679c\u3002", "result": "\u5b9e\u9a8c\u8868\u660eRLVR\u6700\u4f18\uff0c\u4f46\u5c0f\u89c4\u6a21\u9a8c\u8bc1\u5668\u4e2d\u5728\u7ebf\u7b56\u7565\u5b66\u4e60\u6700\u5173\u952e\uff0c\u5927\u89c4\u6a21\u4e0b\u601d\u7ef4\u8f68\u8ff9\u8bad\u7ec3\u6700\u91cd\u8981\u3002", "conclusion": "\u4ee3\u7801\u9a8c\u8bc1\u5668\u8bad\u7ec3\u53ef\u9488\u5bf9\u6027\u7b80\u5316\uff0c\u4e3a\u4ee3\u7801\u751f\u6210\u540e\u8bad\u7ec3\u5de5\u5177\u7bb1\u63d0\u4f9b\u9ad8\u6548\u65b0\u65b9\u6848\u3002"}}
{"id": "2601.12347", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2601.12347", "abs": "https://arxiv.org/abs/2601.12347", "authors": ["Pranjal Naman", "Parv Agarwal", "Hrishikesh Haritas", "Yogesh Simmhan"], "title": "RIPPLE++: An Incremental Framework for Efficient GNN Inference on Evolving Graphs", "comment": "Extended full-length version of paper that appeared at ICDCS 2025: \"RIPPLE: Scalable Incremental GNN Inferencing on Large Streaming Graphs\", Pranjal Naman and Yogesh Simmhan, in International Conference on Distributed Computing Systems (ICDCS), 2025. DOI: https://doi.org/10.1109/icdcs63083.2025.00088", "summary": "Real-world graphs are dynamic, with frequent updates to their structure and features due to evolving vertex and edge properties. These continual changes pose significant challenges for efficient inference in graph neural networks (GNNs). Existing vertex-wise and layer-wise inference approaches are ill-suited for dynamic graphs, as they incur redundant computations, large neighborhood traversals, and high communication costs, especially in distributed settings. Additionally, while sampling-based approaches can be adopted to approximate final layer embeddings, these are often not preferred in critical applications due to their non-determinism. These limitations hinder low-latency inference required in real-time applications. To address this, we propose RIPPLE++, a framework for streaming GNN inference that efficiently and accurately updates embeddings in response to changes in the graph structure or features. RIPPLE++ introduces a generalized incremental programming model that captures the semantics of GNN aggregation functions and incrementally propagates updates to affected neighborhoods. RIPPLE++ accommodates all common graph updates, including vertex/edge addition/deletions and vertex feature updates. RIPPLE++ supports both single-machine and distributed deployments. On a single machine, it achieves up to $56$K updates/sec on sparse graphs like Arxiv ($169$K vertices, $1.2$M edges), and about $7.6$K updates/sec on denser graphs like Products ($2.5$M vertices, $123.7$M edges), with latencies of $0.06$--$960$ms, and outperforming state-of-the-art baselines by $2.2$--$24\\times$ on throughput. In distributed settings, RIPPLE++ offers up to $\\approx25\\times$ higher throughput and $20\\times$ lower communication costs compared to recomputing baselines.", "AI": {"tldr": "RIPPLE++ \u662f\u4e00\u79cd\u7528\u4e8e\u52a8\u6001\u56fe\u7684\u6d41\u5f0f GNN \u63a8\u7406\u6846\u67b6\uff0c\u652f\u6301\u9ad8\u6548\u51c6\u786e\u5730\u66f4\u65b0\u5d4c\u5165\uff0c\u9002\u7528\u4e8e\u5355\u673a\u548c\u5206\u5e03\u5f0f\u73af\u5883\uff0c\u663e\u8457\u63d0\u5347\u541e\u5410\u91cf\u5e76\u964d\u4f4e\u901a\u4fe1\u5f00\u9500\u3002", "motivation": "\u73b0\u6709 GNN \u63a8\u7406\u65b9\u6cd5\u96be\u4ee5\u5e94\u5bf9\u52a8\u6001\u56fe\u9891\u7e41\u66f4\u65b0\u5e26\u6765\u7684\u5197\u4f59\u8ba1\u7b97\u3001\u9ad8\u901a\u4fe1\u6210\u672c\u7b49\u95ee\u9898\uff0c\u5c24\u5176\u5728\u5b9e\u65f6\u5e94\u7528\u4e2d\u5ef6\u8fdf\u8fc7\u9ad8\u3002", "method": "\u63d0\u51fa\u5e7f\u4e49\u589e\u91cf\u7f16\u7a0b\u6a21\u578b\uff0c\u6355\u6349 GNN \u805a\u5408\u8bed\u4e49\uff0c\u589e\u91cf\u4f20\u64ad\u66f4\u65b0\u81f3\u53d7\u5f71\u54cd\u90bb\u57df\uff0c\u652f\u6301\u9876\u70b9/\u8fb9\u589e\u5220\u53ca\u7279\u5f81\u66f4\u65b0\u3002", "result": "\u5355\u673a\u4e0b\u6700\u9ad8\u8fbe 56K \u66f4\u65b0/\u79d2\uff08Arxiv\uff09\uff0c\u5206\u5e03\u5f0f\u4e0b\u541e\u5410\u63d0\u5347\u7ea6 25 \u500d\uff0c\u901a\u4fe1\u6210\u672c\u964d\u4f4e 20 \u500d\uff0c\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf 2.2\u201324 \u500d\u3002", "conclusion": "RIPPLE++ \u663e\u8457\u63d0\u5347\u52a8\u6001\u56fe GNN \u63a8\u7406\u6548\u7387\u4e0e\u53ef\u6269\u5c55\u6027\uff0c\u6ee1\u8db3\u4f4e\u5ef6\u8fdf\u5b9e\u65f6\u5e94\u7528\u9700\u6c42\u3002"}}
{"id": "2601.12262", "categories": ["cs.SE", "cs.CL"], "pdf": "https://arxiv.org/pdf/2601.12262", "abs": "https://arxiv.org/abs/2601.12262", "authors": ["Tongtong Wu", "Rongyi Chen", "Wenjie Du", "Suyu Ma", "Guilin Qi", "Zhenchang Xing", "Shahram Khadivi", "Ramesh Periyathambi", "Gholamreza Haffari"], "title": "Environment-Aware Code Generation: How far are We?", "comment": "ICSE 2026", "summary": "Recent progress in large language models (LLMs) has improved code generation, but most evaluations still test isolated, small-scale code (e.g., a single function) under default or unspecified software environments. As a result, it is unclear whether LLMs can reliably generate executable code tailored to a user's specific environment. We present the first systematic study of Environment-Aware Code Generation (EACG), where generated code must be functionally correct and directly executable under arbitrary software configurations. To enable realistic evaluation, we introduce VersiBCB, a benchmark that is multi-package, execution-verified, and deprecation-aware, capturing complex and evolving environments that prior datasets often overlook. Using VersiBCB, we investigate three complementary adaptation axes: data, parameters, and cache, and develop representative strategies for each. Our results show that current LLMs struggle with environment-specific code generation, while our adaptations improve environment compatibility and executability. These findings highlight key challenges and opportunities for deploying LLMs in practical software engineering workflows.", "AI": {"tldr": "\u672c\u6587\u9996\u6b21\u7cfb\u7edf\u7814\u7a76\u73af\u5883\u611f\u77e5\u4ee3\u7801\u751f\u6210\uff08EACG\uff09\uff0c\u63d0\u51fa\u65b0\u57fa\u51c6VersiBCB\uff0c\u5e76\u4ece\u6570\u636e\u3001\u53c2\u6570\u3001\u7f13\u5b58\u4e09\u65b9\u9762\u6539\u8fdbLLM\u5728\u4e0d\u540c\u8f6f\u4ef6\u73af\u5883\u4e0b\u7684\u4ee3\u7801\u53ef\u6267\u884c\u6027\u3002", "motivation": "\u5f53\u524d\u5927\u8bed\u8a00\u6a21\u578b\u5728\u7279\u5b9a\u8f6f\u4ef6\u73af\u5883\u4e0b\u751f\u6210\u53ef\u76f4\u63a5\u6267\u884c\u4ee3\u7801\u7684\u80fd\u529b\u5c1a\u4e0d\u660e\u786e\uff0c\u7f3a\u4e4f\u771f\u5b9e\u8bc4\u4f30\u57fa\u51c6\u3002", "method": "\u6784\u5efa\u591a\u5305\u3001\u6267\u884c\u9a8c\u8bc1\u3001\u5173\u6ce8\u5f03\u7528\u7684\u57fa\u51c6VersiBCB\uff0c\u4ece\u6570\u636e\u3001\u53c2\u6570\u3001\u7f13\u5b58\u4e09\u4e2a\u7ef4\u5ea6\u8bbe\u8ba1\u9002\u914d\u7b56\u7565\u8fdb\u884c\u5b9e\u9a8c\u3002", "result": "\u73b0\u6709LLM\u5728\u73af\u5883\u76f8\u5173\u4ee3\u7801\u751f\u6210\u4e0a\u8868\u73b0\u4e0d\u4f73\uff0c\u4f46\u6240\u63d0\u9002\u914d\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u73af\u5883\u517c\u5bb9\u6027\u4e0e\u53ef\u6267\u884c\u6027\u3002", "conclusion": "\u73af\u5883\u611f\u77e5\u4ee3\u7801\u751f\u6210\u4ecd\u5177\u6311\u6218\uff0c\u4f46\u901a\u8fc7\u9488\u5bf9\u6027\u4f18\u5316\u6709\u671b\u63a8\u52a8LLM\u5728\u5b9e\u9645\u8f6f\u4ef6\u5de5\u7a0b\u4e2d\u7684\u843d\u5730\u5e94\u7528\u3002"}}
{"id": "2601.12434", "categories": ["cs.DC", "cs.CR"], "pdf": "https://arxiv.org/pdf/2601.12434", "abs": "https://arxiv.org/abs/2601.12434", "authors": ["Shengwei You", "Aditya Joshi", "Andrey Kuehlkamp", "Jarek Nabrzyski"], "title": "ASAS-BridgeAMM: Trust-Minimized Cross-Chain Bridge AMM with Failure Containment", "comment": null, "summary": "Cross-chain bridges constitute the single largest vector of systemic risk in Decentralized Finance (DeFi), accounting for over \\$2.8 billion in losses since 2021. The fundamental vulnerability lies in the binary nature of existing bridge security models: a bridge is either fully operational or catastrophically compromised, with no intermediate state to contain partial failures. We present ASAS-BridgeAMM, a bridge-coupled automated market maker that introduces Contained Degradation: a formally specified operational state where the system gracefully degrades functionality in response to adversarial signals. By treating cross-chain message latency as a quantifiable execution risk, the protocol dynamically adjusts collateral haircuts, slippage bounds, and withdrawal limits. Across 18 months of historical replay on Ethereum and two auxiliary chains, ASAS-BridgeAMM reduces worst-case bridge-induced insolvency by 73% relative to baseline mint-and-burn architectures, while preserving 104.5% of transaction volume during stress periods. In rigorous adversarial simulations involving delayed finality, oracle manipulation, and liquidity griefing, the protocol maintains solvency with probability $>0.9999$ and bounds per-epoch bad debt to $<0.2%$ of total collateral. We provide a reference implementation in Solidity and formally prove safety (bounded debt), liveness (settlement completion), and manipulation resistance under a Byzantine relayer model.", "AI": {"tldr": "ASAS-BridgeAMM \u901a\u8fc7\u5f15\u5165\u2018\u53d7\u63a7\u964d\u7ea7\u2019\u673a\u5236\uff0c\u663e\u8457\u964d\u4f4e\u8de8\u94fe\u6865\u7cfb\u7edf\u6027\u98ce\u9669\uff0c\u5728\u4fdd\u6301\u9ad8\u4ea4\u6613\u91cf\u7684\u540c\u65f6\u63d0\u5347\u5b89\u5168\u6027\u548c\u6297\u653b\u51fb\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u8de8\u94fe\u6865\u56e0\u5b89\u5168\u6a21\u578b\u8fc7\u4e8e\u4e8c\u5143\u5316\uff08\u5b8c\u5168\u6b63\u5e38\u6216\u5f7b\u5e95\u5d29\u6e83\uff09\uff0c\u7f3a\u4e4f\u4e2d\u95f4\u5bb9\u9519\u72b6\u6001\uff0c\u5bfc\u81f4\u5de8\u989d\u8d44\u91d1\u635f\u5931\u3002", "method": "\u5c06\u8de8\u94fe\u6d88\u606f\u5ef6\u8fdf\u89c6\u4e3a\u53ef\u91cf\u5316\u6267\u884c\u98ce\u9669\uff0c\u52a8\u6001\u8c03\u6574\u62b5\u62bc\u6298\u6263\u3001\u6ed1\u70b9\u8fb9\u754c\u548c\u63d0\u6b3e\u9650\u989d\uff0c\u5e76\u5728 Solidity \u4e2d\u5b9e\u73b0\u53c2\u8003\u534f\u8bae\u3002", "result": "\u5386\u53f2\u56de\u6d4b\u663e\u793a\u6700\u574f\u60c5\u51b5\u507f\u4ed8\u80fd\u529b\u635f\u5931\u51cf\u5c1173%\uff0c\u538b\u529b\u671f\u4ea4\u6613\u91cf\u4fdd\u7559104.5%\uff1b\u5bf9\u6297\u6a21\u62df\u4e2d\u507f\u4ed8\u6982\u7387>0.9999\uff0c\u6bcf\u5468\u671f\u574f\u8d26<0.2%\u3002", "conclusion": "\u8be5\u534f\u8bae\u5f62\u5f0f\u5316\u8bc1\u660e\u4e86\u5b89\u5168\u6027\u3001\u6d3b\u6027\u4e0e\u6297\u64cd\u63a7\u6027\uff0c\u4e3a DeFi \u8de8\u94fe\u6865\u63d0\u4f9b\u66f4\u7a33\u5065\u7684\u67b6\u6784\u8303\u5f0f\u3002"}}
{"id": "2601.12273", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2601.12273", "abs": "https://arxiv.org/abs/2601.12273", "authors": ["Chihiro Yoshida", "Yuta Ishimoto", "Olivier Nourry", "Masanari Kondo", "Makoto Matsushita", "Yasutaka Kamei", "Yoshiki Higo"], "title": "Leveraging Mutation Analysis for LLM-based Repair of Quantum Programs", "comment": "6 pages, Accepted at SANER-ERA 2026", "summary": "In recent years, Automated Program Repair (APR) techniques specifically designed for quantum programs have been proposed. However, existing approaches often suffer from low repair success rates or poor understandability of the generated patches. In this study, we construct a framework in which a large language model (LLM) generates code repairs along with a natural language explanation of the applied repairs. To investigate how the contextual information included in prompts influences APR performance for quantum programs, we design four prompt configurations with different combinations of static information, dynamic information, and mutation analysis results. Mutation analysis evaluates how small changes to specific parts of a program affect its execution results and provides more detailed dynamic information than simple execution outputs such as stack traces. Our experimental results show that mutation analysis can provide valuable contextual information for LLM-based APR of quantum programs, improving repair success rates (achieving 94.4% in our experiment) and in some cases also improving the quality of generated explanations. Our findings point toward new directions for developing APR techniques for quantum programs that enhance both reliability and explainability.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u5927\u8bed\u8a00\u6a21\u578b\u4e0e\u53d8\u5f02\u5206\u6790\u7684\u91cf\u5b50\u7a0b\u5e8f\u81ea\u52a8\u4fee\u590d\u6846\u67b6\uff0c\u663e\u8457\u63d0\u5347\u4fee\u590d\u6210\u529f\u7387\u4e0e\u89e3\u91ca\u8d28\u91cf\u3002", "motivation": "\u73b0\u6709\u91cf\u5b50\u7a0b\u5e8f\u81ea\u52a8\u4fee\u590d\u65b9\u6cd5\u5b58\u5728\u4fee\u590d\u6210\u529f\u7387\u4f4e\u6216\u751f\u6210\u8865\u4e01\u53ef\u8bfb\u6027\u5dee\u7684\u95ee\u9898\u3002", "method": "\u6784\u5efa\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u4fee\u590d\u6846\u67b6\uff0c\u5e76\u8bbe\u8ba1\u56db\u79cd\u5305\u542b\u9759\u6001\u4fe1\u606f\u3001\u52a8\u6001\u4fe1\u606f\u548c\u53d8\u5f02\u5206\u6790\u7ed3\u679c\u7684\u4e0d\u540c\u63d0\u793a\u914d\u7f6e\uff0c\u4ee5\u7814\u7a76\u4e0a\u4e0b\u6587\u4fe1\u606f\u5bf9\u4fee\u590d\u6027\u80fd\u7684\u5f71\u54cd\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u5f15\u5165\u53d8\u5f02\u5206\u6790\u53ef\u5c06\u4fee\u590d\u6210\u529f\u7387\u63d0\u5347\u81f394.4%\uff0c\u5e76\u5728\u67d0\u4e9b\u60c5\u51b5\u4e0b\u6539\u5584\u751f\u6210\u89e3\u91ca\u7684\u8d28\u91cf\u3002", "conclusion": "\u53d8\u5f02\u5206\u6790\u80fd\u4e3a\u91cf\u5b50\u7a0b\u5e8f\u81ea\u52a8\u4fee\u590d\u63d0\u4f9b\u6709\u4ef7\u503c\u7684\u4e0a\u4e0b\u6587\u4fe1\u606f\uff0c\u6709\u52a9\u4e8e\u5f00\u53d1\u66f4\u53ef\u9760\u4e14\u53ef\u89e3\u91ca\u7684\u4fee\u590d\u6280\u672f\u3002"}}
{"id": "2601.13769", "categories": ["cs.NI"], "pdf": "https://arxiv.org/pdf/2601.13769", "abs": "https://arxiv.org/abs/2601.13769", "authors": ["Anastasios Giannopoulos", "Sotirios Spantideas", "Maria Lamprini Bartsioka", "Panagiotis Trakadas"], "title": "Interoperable rApp/xApp Control over O-RAN for Mobility-aware Dynamic Spectrum Allocation", "comment": "9 pages, 6 figures", "summary": "Open Radio Access Networks (O-RAN) enable the disaggregation of radio access functions and the deployment of control applications across different timescales. However, designing interoperable control schemes that jointly exploit long-term traffic awareness and near-real-time radio resource optimization remains a challenging problem, particularly under dense multi-cell interference and heterogeneous service demands. This paper proposes an interoperable rApp/xApp-driven dynamic spectrum allocation (DSA) framework for O-RAN, based on a graph-theoretic formulation of physical resource block (PRB) assignment. The proposed architecture leverages a non-real-time radio intelligent controller (Non-RT RIC) rApp to predict aggregated traffic evolution and generate high-level spectrum policies at the minutes timescale, while a near-real-time RIC (Near-RT RIC) xApp constructs a user-centric conflict graph and performs fairness-aware PRB allocation at sub-second timescales. To mitigate persistent user starvation, a conflict-aware modified proportional fair (MPF) scheduling mechanism is applied, enabling controlled interference-free PRB time-sharing. Extensive simulation results demonstrate that the proposed framework significantly improves the PRB assignment success rate (above 90%) and service-share fairness (above 85%) across different channel configurations and user demands, while maintaining architectural separation and rApp/xApp interoperability in accordance with O-RAN principles.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eO-RAN\u67b6\u6784\u7684rApp/xApp\u9a71\u52a8\u52a8\u6001\u9891\u8c31\u5206\u914d\u6846\u67b6\uff0c\u7ed3\u5408\u56fe\u8bba\u5efa\u6a21\u4e0e\u5206\u5c42\u63a7\u5236\uff0c\u5728\u4fdd\u969c\u4e92\u64cd\u4f5c\u6027\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u8d44\u6e90\u5206\u914d\u6210\u529f\u7387\u4e0e\u516c\u5e73\u6027\u3002", "motivation": "\u89e3\u51b3\u5728\u5bc6\u96c6\u591a\u5c0f\u533a\u5e72\u6270\u548c\u5f02\u6784\u4e1a\u52a1\u9700\u6c42\u4e0b\uff0c\u5982\u4f55\u534f\u540c\u5229\u7528\u957f\u671f\u6d41\u91cf\u611f\u77e5\u4e0e\u8fd1\u5b9e\u65f6\u65e0\u7ebf\u8d44\u6e90\u4f18\u5316\u8bbe\u8ba1\u53ef\u4e92\u64cd\u4f5c\u63a7\u5236\u65b9\u6848\u7684\u6311\u6218\u3002", "method": "\u6784\u5efa\u57fa\u4e8e\u56fe\u8bba\u7684PRB\u5206\u914d\u6a21\u578b\uff0c\u7531\u975e\u5b9e\u65f6rApp\u9884\u6d4b\u6d41\u91cf\u5e76\u751f\u6210\u9891\u8c31\u7b56\u7565\uff0c\u8fd1\u5b9e\u65f6xApp\u6784\u5efa\u7528\u6237\u4e2d\u5fc3\u51b2\u7a81\u56fe\u5e76\u6267\u884c\u516c\u5e73\u611f\u77e5PRB\u5206\u914d\uff0c\u8f85\u4ee5\u51b2\u7a81\u611f\u77e5\u6539\u8fdb\u6bd4\u4f8b\u516c\u5e73\u8c03\u5ea6\u673a\u5236\u3002", "result": "\u4eff\u771f\u8868\u660e\u8be5\u6846\u67b6\u5728\u4e0d\u540c\u4fe1\u9053\u914d\u7f6e\u548c\u7528\u6237\u9700\u6c42\u4e0b\uff0cPRB\u5206\u914d\u6210\u529f\u7387\u8d8590%\uff0c\u670d\u52a1\u4efd\u989d\u516c\u5e73\u6027\u8d8585%\uff0c\u540c\u65f6\u4fdd\u6301O-RAN\u67b6\u6784\u5206\u79bb\u4e0erApp/xApp\u4e92\u64cd\u4f5c\u6027\u3002", "conclusion": "\u6240\u63d0\u6846\u67b6\u6709\u6548\u5b9e\u73b0\u4e86O-RAN\u73af\u5883\u4e0b\u8de8\u65f6\u95f4\u5c3a\u5ea6\u7684\u52a8\u6001\u9891\u8c31\u5206\u914d\uff0c\u5728\u6027\u80fd\u4e0e\u67b6\u6784\u5408\u89c4\u6027\u4e4b\u95f4\u53d6\u5f97\u826f\u597d\u5e73\u8861\u3002"}}
{"id": "2601.12524", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2601.12524", "abs": "https://arxiv.org/abs/2601.12524", "authors": ["Zechuan Gong", "Hui Zhang", "Yuquan Yang", "Wenyu Lu"], "title": "SGCP: A Self-Organized Game-Theoretic Framework For Collaborative Perception", "comment": null, "summary": "Collaborative perception holds great promise for improving safety in autonomous driving, particularly in dense traffic where vehicles can share sensory information to overcome individual blind spots and extend awareness. However, deploying such collaboration at scale remains difficult when communication bandwidth is limited and no roadside infrastructure is available. To overcome these limitations, we introduce a fully decentralized framework that enables vehicles to self organize into cooperative groups using only vehicle to vehicle communication. The approach decomposes the problem into two sequential game theoretic stages. In the first stage, vehicles form stable clusters by evaluating mutual sensing complementarity and motion coherence, and each cluster elects a coordinator. In the second stage, the coordinator guides its members to selectively transmit point cloud segments from perceptually salient regions through a non cooperative potential game, enabling efficient local fusion. Global scene understanding is then achieved by exchanging compact detection messages across clusters rather than raw sensor data. We design distributed algorithms for both stages that guarantee monotonic improvement of the system wide potential function. Comprehensive experiments on the CARLA-OpenCDA-NS3 co-simulation platform show that our method reduces communication overhead while delivering higher perception accuracy and wider effective coverage compared to existing baselines.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u53bb\u4e2d\u5fc3\u5316\u534f\u4f5c\u611f\u77e5\u6846\u67b6\uff0c\u901a\u8fc7\u8f66\u8f86\u81ea\u7ec4\u7ec7\u5206\u7ec4\u4e0e\u9009\u62e9\u6027\u70b9\u4e91\u4f20\u8f93\uff0c\u5728\u6709\u9650\u5e26\u5bbd\u4e0b\u63d0\u5347\u81ea\u52a8\u9a7e\u9a76\u611f\u77e5\u7cbe\u5ea6\u4e0e\u8986\u76d6\u8303\u56f4\u3002", "motivation": "\u89e3\u51b3\u5927\u89c4\u6a21\u534f\u4f5c\u611f\u77e5\u4e2d\u901a\u4fe1\u5e26\u5bbd\u53d7\u9650\u4e14\u65e0\u8def\u4fa7\u8bbe\u65bd\u652f\u6301\u7684\u90e8\u7f72\u96be\u9898\u3002", "method": "\u91c7\u7528\u4e24\u9636\u6bb5\u535a\u5f08\u8bba\u65b9\u6cd5\uff1a\u7b2c\u4e00\u9636\u6bb5\u57fa\u4e8e\u611f\u77e5\u4e92\u8865\u4e0e\u8fd0\u52a8\u4e00\u81f4\u6027\u5f62\u6210\u7a33\u5b9a\u7c07\u5e76\u9009\u4e3e\u534f\u8c03\u8005\uff1b\u7b2c\u4e8c\u9636\u6bb5\u7531\u534f\u8c03\u8005\u5f15\u5bfc\u6210\u5458\u901a\u8fc7\u975e\u5408\u4f5c\u52bf\u535a\u5f08\u9009\u62e9\u6027\u4f20\u8f93\u663e\u8457\u533a\u57df\u70b9\u4e91\uff0c\u5e76\u8de8\u7c07\u4ea4\u6362\u7d27\u51d1\u68c0\u6d4b\u6d88\u606f\u5b9e\u73b0\u5168\u5c40\u573a\u666f\u7406\u89e3\u3002", "result": "\u5728CARLA-OpenCDA-NS3\u5e73\u53f0\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u76f8\u6bd4\u57fa\u7ebf\u964d\u4f4e\u901a\u4fe1\u5f00\u9500\uff0c\u540c\u65f6\u63d0\u9ad8\u611f\u77e5\u7cbe\u5ea6\u4e0e\u6709\u6548\u8986\u76d6\u8303\u56f4\u3002", "conclusion": "\u6240\u63d0\u6846\u67b6\u5728\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4e0b\u5b9e\u73b0\u4e86\u9ad8\u6548\u3001\u51c6\u786e\u7684\u534f\u4f5c\u611f\u77e5\uff0c\u5177\u5907\u5b9e\u9645\u90e8\u7f72\u6f5c\u529b\u3002"}}
{"id": "2601.12274", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2601.12274", "abs": "https://arxiv.org/abs/2601.12274", "authors": ["Mahdi Eslamimehr"], "title": "Hybrid Concolic Testing with Large Language Models for Guided Path Exploration", "comment": "12 pages, 2 Figures, 2 Tables", "summary": "Concolic testing, a powerful hybrid software testing technique, has historically been plagued by fundamental limitations such as path explosion and the high cost of constraint solving, which hinder its practical application in large-scale, real-world software systems. This paper introduces a novel algorithmic framework that synergistically integrates concolic execution with Large Language Models (LLMs) to overcome these challenges. Our hybrid approach leverages the semantic reasoning capabilities of LLMs to guide path exploration, prioritize interesting execution paths, and assist in constraint solving. We formally define the system architecture and algorithms that constitute this new paradigm. Through a series of experiments on both synthetic and real-world Fintech applications, we demonstrate that our approach significantly outperforms traditional concolic testing, random testing, and genetic algorithm-based methods in terms of branch coverage, path coverage, and time-to-coverage. The results indicate that by combining the strengths of both concolic execution and LLMs, our method achieves a more efficient and effective exploration of the program state space, leading to improved bug detection capabilities.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u5927\u8bed\u8a00\u6a21\u578b\u4e0e\u7b26\u53f7\u6267\u884c\u7684\u65b0\u578b\u6df7\u5408\u6d4b\u8bd5\u6846\u67b6\uff0c\u663e\u8457\u63d0\u5347\u8def\u5f84\u8986\u76d6\u7387\u4e0e\u7f3a\u9677\u68c0\u6d4b\u6548\u7387\u3002", "motivation": "\u4f20\u7edf\u7b26\u53f7\u6267\u884c\u9762\u4e34\u8def\u5f84\u7206\u70b8\u4e0e\u7ea6\u675f\u6c42\u89e3\u6210\u672c\u9ad8\u7684\u95ee\u9898\uff0c\u9650\u5236\u4e86\u5176\u5728\u5927\u89c4\u6a21\u7cfb\u7edf\u4e2d\u7684\u5e94\u7528\u3002", "method": "\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\u7684\u8bed\u4e49\u63a8\u7406\u80fd\u529b\u5f15\u5bfc\u8def\u5f84\u63a2\u7d22\u3001\u4f18\u5148\u9009\u62e9\u5173\u952e\u8def\u5f84\u5e76\u8f85\u52a9\u7ea6\u675f\u6c42\u89e3\uff0c\u6784\u5efa\u65b0\u7684\u7b97\u6cd5\u6846\u67b6\u3002", "result": "\u5728\u5408\u6210\u4e0e\u771f\u5b9e\u91d1\u878d\u79d1\u6280\u5e94\u7528\u4e2d\uff0c\u8be5\u65b9\u6cd5\u5728\u5206\u652f\u8986\u76d6\u7387\u3001\u8def\u5f84\u8986\u76d6\u7387\u548c\u8986\u76d6\u65f6\u95f4\u4e0a\u4f18\u4e8e\u4f20\u7edf\u7b26\u53f7\u6267\u884c\u3001\u968f\u673a\u6d4b\u8bd5\u548c\u9057\u4f20\u7b97\u6cd5\u3002", "conclusion": "\u7ed3\u5408\u7b26\u53f7\u6267\u884c\u4e0e\u5927\u8bed\u8a00\u6a21\u578b\u53ef\u66f4\u9ad8\u6548\u63a2\u7d22\u7a0b\u5e8f\u72b6\u6001\u7a7a\u95f4\uff0c\u63d0\u5347\u8f6f\u4ef6\u6d4b\u8bd5\u6548\u679c\u3002"}}
{"id": "2601.12713", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2601.12713", "abs": "https://arxiv.org/abs/2601.12713", "authors": ["Luke Marzen", "Junhyung Shim", "Ali Jannesari"], "title": "Dynamic Detection of Inefficient Data Mapping Patterns in Heterogeneous OpenMP Applications", "comment": "Accepted to The 31st ACM SIGPLAN Annual Symposium on Principles and Practice of Parallel Programming (PPoPP '26)", "summary": "With the growing prevalence of heterogeneous computing, CPUs are increasingly being paired with accelerators to achieve new levels of performance and energy efficiency. However, data movement between devices remains a significant bottleneck, complicating application development. Existing performance tools require considerable programmer intervention to diagnose and locate data transfer inefficiencies. To address this, we propose dynamic analysis techniques to detect and profile inefficient data transfer and allocation patterns in heterogeneous applications. We implemented these techniques into OMPDataPerf, which provides detailed traces of problematic data mappings, source code attribution, and assessments of optimization potential in heterogeneous OpenMP applications. OMPDataPerf uses the OpenMP Tools Interface (OMPT) and incurs only a 5 % geometric-mean runtime overhead.", "AI": {"tldr": "OMPDataPerf \u662f\u4e00\u79cd\u52a8\u6001\u5206\u6790\u5de5\u5177\uff0c\u7528\u4e8e\u68c0\u6d4b\u5f02\u6784\u8ba1\u7b97\u4e2d\u4f4e\u6548\u7684\u6570\u636e\u4f20\u8f93\u4e0e\u5206\u914d\u6a21\u5f0f\uff0c\u901a\u8fc7 OpenMP \u5de5\u5177\u63a5\u53e3\u5b9e\u73b0\uff0c\u4ec5\u5e26\u6765 5% \u7684\u8fd0\u884c\u65f6\u5f00\u9500\u3002", "motivation": "\u5f02\u6784\u8ba1\u7b97\u4e2d\u8bbe\u5907\u95f4\u6570\u636e\u79fb\u52a8\u662f\u6027\u80fd\u74f6\u9888\uff0c\u73b0\u6709\u5de5\u5177\u9700\u5927\u91cf\u4eba\u5de5\u5e72\u9884\u624d\u80fd\u8bca\u65ad\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u52a8\u6001\u5206\u6790\u6280\u672f\uff0c\u7ed3\u5408 OMPT \u63a5\u53e3\u5b9e\u73b0\u81ea\u52a8\u5316\u68c0\u6d4b\u4e0e\u6e90\u7801\u5f52\u56e0\u3002", "result": "\u5de5\u5177\u53ef\u751f\u6210\u8be6\u7ec6\u7684\u95ee\u9898\u6570\u636e\u6620\u5c04\u8f68\u8ff9\u3001\u6e90\u7801\u5b9a\u4f4d\u53ca\u4f18\u5316\u6f5c\u529b\u8bc4\u4f30\uff0c\u5e73\u5747\u8fd0\u884c\u65f6\u5f00\u9500\u4ec5 5%\u3002", "conclusion": "OMPDataPerf \u6709\u6548\u964d\u4f4e\u5f02\u6784\u5e94\u7528\u5f00\u53d1\u590d\u6742\u5ea6\uff0c\u63d0\u5347\u6570\u636e\u4f20\u8f93\u6548\u7387\u8bca\u65ad\u80fd\u529b\u3002"}}
{"id": "2601.12327", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.12327", "abs": "https://arxiv.org/abs/2601.12327", "authors": ["Lucas Gren", "Felix Dobslaw"], "title": "The Expert Validation Framework (EVF): Enabling Domain Expert Control in AI Engineering", "comment": null, "summary": "Generative AI (GenAI) systems promise to transform knowledge work by automating a range of tasks, yet their deployment in enterprise settings remains hindered by the lack of systematic quality assurance mechanisms. We present an Expert Validation Framework that places domain experts at the center of building software with GenAI components, enabling them to maintain authoritative control over system behavior through structured specification, testing, validation, and continuous monitoring processes. Our framework addresses the critical gap between AI capabilities and organizational trust by establishing a rigorous, expert-driven methodology for ensuring quality across diverse GenAI applications. Through a four-stage implementation process encompassing specification, system creation, validation, and production monitoring, the framework enables organizations to leverage GenAI capabilities while maintaining expert oversight and quality standards.", "AI": {"tldr": "\u63d0\u51fa\u4e13\u5bb6\u9a8c\u8bc1\u6846\u67b6\u4ee5\u786e\u4fdd\u751f\u6210\u5f0fAI\u5728\u4f01\u4e1a\u4e2d\u7684\u8d28\u91cf\u4e0e\u53ef\u4fe1\u5ea6", "motivation": "\u5f53\u524d\u751f\u6210\u5f0fAI\u5728\u4f01\u4e1a\u90e8\u7f72\u4e2d\u7f3a\u4e4f\u7cfb\u7edf\u5316\u7684\u8d28\u91cf\u4fdd\u969c\u673a\u5236\uff0c\u5f71\u54cd\u7ec4\u7ec7\u4fe1\u4efb", "method": "\u6784\u5efa\u4ee5\u9886\u57df\u4e13\u5bb6\u4e3a\u6838\u5fc3\u7684\u56db\u9636\u6bb5\u6846\u67b6\uff1a\u89c4\u8303\u5236\u5b9a\u3001\u7cfb\u7edf\u521b\u5efa\u3001\u9a8c\u8bc1\u548c\u751f\u4ea7\u76d1\u63a7", "result": "\u4f7f\u7ec4\u7ec7\u80fd\u5728\u4fdd\u6301\u4e13\u5bb6\u76d1\u7763\u548c\u8d28\u91cf\u6807\u51c6\u7684\u524d\u63d0\u4e0b\u6709\u6548\u5229\u7528\u751f\u6210\u5f0fAI\u80fd\u529b", "conclusion": "\u8be5\u6846\u67b6\u5f25\u5408\u4e86AI\u80fd\u529b\u4e0e\u7ec4\u7ec7\u4fe1\u4efb\u4e4b\u95f4\u7684\u5173\u952e\u9e3f\u6c9f\uff0c\u652f\u6301\u591a\u6837\u5316GenAI\u5e94\u7528\u7684\u7a33\u5065\u843d\u5730"}}
{"id": "2601.12749", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2601.12749", "abs": "https://arxiv.org/abs/2601.12749", "authors": ["Hui Zhang", "Yuquan Yang", "Zechuan Gong", "Xiaohua Xu", "Dan Keun Sung"], "title": "Efficient Local-to-Global Collaborative Perception via Joint Communication and Computation Optimization", "comment": null, "summary": "Autonomous driving relies on accurate perception to ensure safe driving. Collaborative perception improves accuracy by mitigating the sensing limitations of individual vehicles, such as limited perception range and occlusion-induced blind spots. However, collaborative perception often suffers from high communication overhead due to redundant data transmission, as well as increasing computation latency caused by excessive load with growing connected and autonomous vehicles (CAVs) participation. To address these challenges, we propose a novel local-to-global collaborative perception framework (LGCP) to achieve collaboration in a communication- and computation-efficient manner. The road of interest is partitioned into non-overlapping areas, each of which is assigned a dedicated CAV group to perform localized perception. A designated leader in each group collects and fuses perception data from its members, and uploads the perception result to the roadside unit (RSU), establishing a link between local perception and global awareness. The RSU aggregates perception results from all groups and broadcasts a global view to all CAVs. LGCP employs a centralized scheduling strategy via the RSU, which assigns CAV groups to each area, schedules their transmissions, aggregates area-level local perception results, and propagates the global view to all CAVs. Experimental results demonstrate that the proposed LGCP framework achieves an average 44 times reduction in the amount of data transmission, while maintaining or even improving the overall collaborative performance.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5c40\u90e8\u5230\u5168\u5c40\u7684\u534f\u4f5c\u611f\u77e5\u6846\u67b6\uff08LGCP\uff09\uff0c\u901a\u8fc7\u533a\u57df\u5212\u5206\u4e0e\u96c6\u4e2d\u8c03\u5ea6\u663e\u8457\u964d\u4f4e\u901a\u4fe1\u5f00\u9500\u5e76\u63d0\u5347\u611f\u77e5\u6548\u7387\u3002", "motivation": "\u89e3\u51b3\u534f\u4f5c\u611f\u77e5\u4e2d\u56e0\u6570\u636e\u5197\u4f59\u548c\u8ba1\u7b97\u8d1f\u8f7d\u589e\u52a0\u5bfc\u81f4\u7684\u9ad8\u901a\u4fe1\u5f00\u9500\u4e0e\u5ef6\u8fdf\u95ee\u9898\u3002", "method": "\u5c06\u9053\u8def\u5212\u5206\u4e3a\u975e\u91cd\u53e0\u533a\u57df\uff0c\u6bcf\u533a\u7531\u4e13\u7528\u8f66\u8f86\u7ec4\u8fdb\u884c\u5c40\u90e8\u611f\u77e5\uff0c\u7ec4\u957f\u878d\u5408\u6570\u636e\u4e0a\u4f20\u8def\u4fa7\u5355\u5143\uff08RSU\uff09\uff0cRSU\u805a\u5408\u540e\u5e7f\u64ad\u5168\u5c40\u89c6\u56fe\uff1b\u91c7\u7528RSU\u96c6\u4e2d\u8c03\u5ea6\u7b56\u7565\u7ba1\u7406\u5206\u7ec4\u3001\u4f20\u8f93\u4e0e\u7ed3\u679c\u805a\u5408\u3002", "result": "\u5b9e\u9a8c\u8868\u660eLGCP\u5e73\u5747\u51cf\u5c1144\u500d\u6570\u636e\u4f20\u8f93\u91cf\uff0c\u540c\u65f6\u4fdd\u6301\u6216\u63d0\u5347\u6574\u4f53\u534f\u4f5c\u611f\u77e5\u6027\u80fd\u3002", "conclusion": "LGCP\u5728\u4fdd\u8bc1\u611f\u77e5\u7cbe\u5ea6\u7684\u540c\u65f6\uff0c\u6709\u6548\u5b9e\u73b0\u4e86\u901a\u4fe1\u4e0e\u8ba1\u7b97\u8d44\u6e90\u7684\u9ad8\u6548\u5229\u7528\uff0c\u9002\u7528\u4e8e\u5927\u89c4\u6a21\u8f66\u8054\u7f51\u73af\u5883\u3002"}}
{"id": "2601.12360", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2601.12360", "abs": "https://arxiv.org/abs/2601.12360", "authors": ["Xinabang He", "Yuanwei Chen", "Hao Wu", "Jikang Zhang", "Zicheng Wang", "Ligeng Chen", "Junjie Peng", "Haiyang Wei", "Yi Qian", "Tiantai Zhang", "Linzhang Wang", "Bing Mao"], "title": "Discovering 100+ Compiler Defects in 72 Hours via LLM-Driven Semantic Logic Recomposition", "comment": null, "summary": "Compilers constitute the foundational root-of-trust in software supply chains; however, their immense complexity inevitably conceals critical defects. Recent research has attempted to leverage historical bugs to design new mutation operators or fine-tune models to increase program diversity for compiler fuzzing.We observe, however, that bugs manifest primarily based on the semantics of input programs rather than their syntax. Unfortunately, current approaches, whether relying on syntactic mutation or general Large Language Model (LLM) fine-tuning, struggle to preserve the specific semantics found in the logic of bug-triggering programs. Consequently, these critical semantic triggers are often lost, resulting in a limitation of the diversity of generated programs.\n  To explicitly reuse such semantics, we propose FeatureFuzz, a compiler fuzzer that combines features to generate programs. We define a feature as a decoupled primitive that encapsulates a natural language description of a bug-prone invariant, such as an out-of-bounds array access, alongside a concrete code witness of its realization. FeatureFuzz operates via a three-stage workflow: it first extracts features from historical bug reports, synthesizes coherent groups of features, and finally instantiates these groups into valid programs for compiler fuzzing.\n  We evaluated FeatureFuzz on GCC and LLVM. Over 24-hour campaigns, FeatureFuzz uncovered 167 unique crashes, which is 2.78x more than the second-best fuzzer. Furthermore, through a 72-hour fuzzing campaign, FeatureFuzz identified 106 bugs in GCC and LLVM, 76 of which have already been confirmed by compiler developers, validating the approach's ability to stress-test modern compilers effectively.", "AI": {"tldr": "FeatureFuzz \u662f\u4e00\u79cd\u57fa\u4e8e\u8bed\u4e49\u7279\u5f81\u7ec4\u5408\u7684\u7f16\u8bd1\u5668\u6a21\u7cca\u6d4b\u8bd5\u5de5\u5177\uff0c\u663e\u8457\u63d0\u5347\u7f3a\u9677\u53d1\u73b0\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u6a21\u7cca\u6d4b\u8bd5\u65b9\u6cd5\u96be\u4ee5\u4fdd\u7559\u89e6\u53d1\u7f16\u8bd1\u5668\u7f3a\u9677\u7684\u5173\u952e\u8bed\u4e49\uff0c\u9650\u5236\u4e86\u7a0b\u5e8f\u591a\u6837\u6027\u4e0e\u7f3a\u9677\u68c0\u51fa\u7387\u3002", "method": "\u4ece\u5386\u53f2\u7f3a\u9677\u62a5\u544a\u4e2d\u63d0\u53d6\u8bed\u4e49\u7279\u5f81\uff08\u542b\u81ea\u7136\u8bed\u8a00\u63cf\u8ff0\u4e0e\u4ee3\u7801\u5b9e\u4f8b\uff09\uff0c\u7ec4\u5408\u751f\u6210\u7b26\u5408\u8bed\u6cd5\u7684\u6709\u6548\u6d4b\u8bd5\u7a0b\u5e8f\u3002", "result": "\u5728 GCC \u4e0e LLVM \u4e0a 24 \u5c0f\u65f6\u5185\u53d1\u73b0 167 \u4e2a\u5d29\u6e83\uff08\u9886\u5148\u5de5\u5177 2.78 \u500d\uff09\uff0c72 \u5c0f\u65f6\u786e\u8ba4 76 \u4e2a\u65b0\u7f3a\u9677\u3002", "conclusion": "FeatureFuzz \u80fd\u6709\u6548\u590d\u7528\u8bed\u4e49\u7279\u5f81\uff0c\u663e\u8457\u589e\u5f3a\u5bf9\u73b0\u4ee3\u7f16\u8bd1\u5668\u7684\u538b\u529b\u6d4b\u8bd5\u80fd\u529b\u3002"}}
{"id": "2601.12784", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2601.12784", "abs": "https://arxiv.org/abs/2601.12784", "authors": ["Haoyang Li", "Sheng Lin", "Fangcheng Fu", "Yuming Zhou", "Xiaodong Ji", "Yanfeng Zhao", "Lefeng Wang", "Jie Jiang", "Bin Cui"], "title": "Unleashing Efficient Asynchronous RL Post-Training via Staleness-Constrained Rollout Coordination", "comment": null, "summary": "Reinforcement learning (RL) post-training has become pivotal for enhancing the capabilities of modern large models. A recent trend is to develop RL systems with a fully disaggregated architecture, which decouples the three RL phases (rollout, reward, and training) onto separate resources and executes them asynchronously. However, two critical data-level concerns arise: (1) asynchronous execution leads to data staleness in trajectories (the data generated by rollout) as the model parameters used in rollout may not be up to date, which impairs RL convergence; and (2) the length variation of trajectories introduces severe data skewness, leading to workload imbalance and degraded system performance.\n  Existing systems fail to address these two concerns in a unified manner. Techniques that tightly control data staleness often constrain effective data skewness mitigation, while aggressive data skewness mitigation tends to exacerbate data staleness. As a result, systems are forced to trade off convergence for performance, or vice versa. To address this, we propose StaleFlow, an RL post-training system that jointly tackles data staleness and skewness. First, to control staleness, StaleFlow introduces a global consistency protocol that tracks the full lifecycle of each trajectory and constrains staleness. Second, to mitigate skewness, StaleFlow re-designs the RL system architecture by constructing data servers for trajectories and parameters to achieve flexible rollout coordination. Subsequently, we develop a suite of staleness-aware, throughput-oriented strategies to enhance system performance. Evaluations show that StaleFlow achieves up to 1.42-2.68$\\times$ (1.17-2.01$\\times$ on average) higher throughput than state-of-the-art systems, without compromising convergence.", "AI": {"tldr": "StaleFlow\u662f\u4e00\u4e2a\u65b0\u578b\u5f3a\u5316\u5b66\u4e60\u540e\u8bad\u7ec3\u7cfb\u7edf\uff0c\u901a\u8fc7\u5168\u5c40\u4e00\u81f4\u6027\u534f\u8bae\u548c\u91cd\u65b0\u8bbe\u8ba1\u7684\u67b6\u6784\uff0c\u540c\u65f6\u89e3\u51b3\u6570\u636e\u9648\u65e7\u6027\u548c\u6570\u636e\u503e\u659c\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u541e\u5410\u91cf\u4e14\u4e0d\u635f\u5bb3\u6536\u655b\u6027\u3002", "motivation": "\u73b0\u6709\u7cfb\u7edf\u65e0\u6cd5\u7edf\u4e00\u5904\u7406\u6570\u636e\u9648\u65e7\u6027\u548c\u6570\u636e\u503e\u659c\u95ee\u9898\uff0c\u5bfc\u81f4\u6027\u80fd\u4e0e\u6536\u655b\u6027\u4e4b\u95f4\u7684\u6743\u8861\u3002", "method": "\u5f15\u5165\u5168\u5c40\u4e00\u81f4\u6027\u534f\u8bae\u8ddf\u8e2a\u8f68\u8ff9\u751f\u547d\u5468\u671f\u4ee5\u63a7\u5236\u9648\u65e7\u6027\uff1b\u6784\u5efa\u6570\u636e\u670d\u52a1\u5668\u5b9e\u73b0\u7075\u6d3b\u7684rollout\u534f\u8c03\uff0c\u5e76\u8bbe\u8ba1\u4e00\u7cfb\u5217\u611f\u77e5\u9648\u65e7\u6027\u3001\u9762\u5411\u541e\u5410\u91cf\u7684\u7b56\u7565\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cStaleFlow\u76f8\u6bd4\u73b0\u6709\u6700\u5148\u8fdb\u7cfb\u7edf\uff0c\u541e\u5410\u91cf\u63d0\u53471.42-2.68\u500d\uff08\u5e73\u57471.17-2.01\u500d\uff09\uff0c\u4e14\u4e0d\u5f71\u54cd\u6536\u655b\u6027\u3002", "conclusion": "StaleFlow\u6210\u529f\u5728\u4e0d\u727a\u7272\u6536\u655b\u6027\u7684\u524d\u63d0\u4e0b\uff0c\u9ad8\u6548\u89e3\u51b3\u4e86\u5f3a\u5316\u5b66\u4e60\u540e\u8bad\u7ec3\u4e2d\u7684\u6570\u636e\u9648\u65e7\u6027\u548c\u503e\u659c\u95ee\u9898\u3002"}}
{"id": "2601.12448", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2601.12448", "abs": "https://arxiv.org/abs/2601.12448", "authors": ["Yang Liu", "Yixing Luo", "Xiaofeng Li", "Xiaogang Dong", "Bin Gu", "Zhi Jin"], "title": "Evaluating Large Language Models for Time Series Anomaly Detection in Aerospace Software", "comment": "This paper has been accepted by ASE 2025", "summary": "Time series anomaly detection (TSAD) is essential for ensuring the safety and reliability of aerospace software systems. Although large language models (LLMs) provide a promising training-free alternative to unsupervised approaches, their effectiveness in aerospace settings remains under-examined because of complex telemetry, misaligned evaluation metrics, and the absence of domain knowledge. To address this gap, we introduce ATSADBench, the first benchmark for aerospace TSAD. ATSADBench comprises nine tasks that combine three pattern-wise anomaly types, univariate and multivariate signals, and both in-loop and out-of-loop feedback scenarios, yielding 108,000 data points. Using this benchmark, we systematically evaluate state-of-the-art open-source LLMs under two paradigms: Direct, which labels anomalies within sliding windows, and Prediction-Based, which detects anomalies from prediction errors. To reflect operational needs, we reformulate evaluation at the window level and propose three user-oriented metrics: Alarm Accuracy (AA), Alarm Latency (AL), and Alarm Contiguity (AC), which quantify alarm correctness, timeliness, and credibility. We further examine two enhancement strategies, few-shot learning and retrieval-augmented generation (RAG), to inject domain knowledge. The evaluation results show that (1) LLMs perform well on univariate tasks but struggle with multivariate telemetry, (2) their AA and AC on multivariate tasks approach random guessing, (3) few-shot learning provides modest gains whereas RAG offers no significant improvement, and (4) in practice LLMs can detect true anomaly onsets yet sometimes raise false alarms, which few-shot prompting mitigates but RAG exacerbates. These findings offer guidance for future LLM-based TSAD in aerospace software.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faATSADBench\u57fa\u51c6\uff0c\u9996\u6b21\u7cfb\u7edf\u8bc4\u4f30\u5927\u8bed\u8a00\u6a21\u578b\u5728\u822a\u7a7a\u822a\u5929\u65f6\u5e8f\u5f02\u5e38\u68c0\u6d4b\u4e2d\u7684\u8868\u73b0\uff0c\u5e76\u5f15\u5165\u7528\u6237\u5bfc\u5411\u6307\u6807\u4e0e\u589e\u5f3a\u7b56\u7565\u3002", "motivation": "\u586b\u8865\u5927\u8bed\u8a00\u6a21\u578b\u5728\u590d\u6742\u822a\u7a7a\u822a\u5929\u9065\u6d4b\u573a\u666f\u4e0b\u5f02\u5e38\u68c0\u6d4b\u6709\u6548\u6027\u7814\u7a76\u7684\u7a7a\u767d\u3002", "method": "\u6784\u5efa\u5305\u542b\u4e5d\u9879\u4efb\u52a1\u7684\u57fa\u51c6\u6570\u636e\u96c6\uff0c\u91c7\u7528Direct\u548cPrediction-Based\u4e24\u79cd\u8303\u5f0f\u8bc4\u4f30\u5f00\u6e90LLM\uff0c\u5e76\u7ed3\u5408\u5c11\u6837\u672c\u5b66\u4e60\u4e0eRAG\u589e\u5f3a\u7b56\u7565\u3002", "result": "LLM\u5728\u5355\u53d8\u91cf\u4efb\u52a1\u4e2d\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728\u591a\u53d8\u91cf\u4efb\u52a1\u4e2d\u63a5\u8fd1\u968f\u673a\u731c\u6d4b\uff1b\u5c11\u6837\u672c\u5b66\u4e60\u7565\u6709\u63d0\u5347\uff0cRAG\u65e0\u663e\u8457\u6539\u5584\u751a\u81f3\u52a0\u5267\u8bef\u62a5\u3002", "conclusion": "\u4e3a\u672a\u6765\u822a\u7a7a\u822a\u5929\u8f6f\u4ef6\u4e2d\u57fa\u4e8eLLM\u7684\u65f6\u5e8f\u5f02\u5e38\u68c0\u6d4b\u63d0\u4f9b\u5b9e\u8df5\u6307\u5bfc\u4e0e\u6539\u8fdb\u65b9\u5411\u3002"}}
{"id": "2601.14009", "categories": ["cs.NI"], "pdf": "https://arxiv.org/pdf/2601.14009", "abs": "https://arxiv.org/abs/2601.14009", "authors": ["Sofia Montebugnoli", "Leonardo Bonati", "Andrea Sabbioni", "Luca Foschini", "Paolo Bellavista", "Salvatore D'Oro", "Michele Polese", "Tommaso Melodia"], "title": "MANATEE: A DevOps Platform for xApp Lifecycle Management and Testing in Open RAN", "comment": "15 pages, 16 figures", "summary": "The shift to disaggregated 5G architectures introduces unprecedented flexibility but also significant complexity in Beyond 5G Radio Access Networks (RANs). Open RAN enables programmability through xApps, yet deploying and validating these applications is critical given the nature of the systems they aim to control. Current Open RAN ecosystems lack robust lifecycle management of xApps that enable automated testing, seamless migration, and production-grade observability, resulting in slow, error-prone xApp delivery. To address these issues, DevOps practices can streamline the xApp lifecycle by integrating Continuous Integration/Continuous Deployment (CI/CD) pipelines with advanced traffic management and monitoring, such as leveraging service mesh technologies to enable progressive deployment strategies (e.g., canary releases and A/B testing) to ensure fine-grained observability and resilience. The solution presented in this article, MANATEE (Mesh Architecture for Radio Access Network Automation and TEsting Ecosystems), is the first platform that combines these principles to simplify xApp delivery into production, accelerate innovation, and guarantee performance across heterogeneous O-RAN environments. We prototyped MANATEE on a Kubernetes cluster integrated with the O-RAN Software Community Near-Real Time RAN Intelligent Controller (RIC), as well as with service mesh technologies, to facilitate testing of xApps across simulated, emulated, and real testbed environments. Our experimental results demonstrate that service mesh integration introduces minimal overhead (below 1 ms latency), while enabling reliable canary deployments with fine-grained traffic control and conflict-free A/B testing through circuit-breaking mechanisms.", "AI": {"tldr": "MANATEE\u5e73\u53f0\u7ed3\u5408DevOps\u4e0e\u670d\u52a1\u7f51\u683c\u6280\u672f\uff0c\u4f18\u5316xApp\u5728O-RAN\u73af\u5883\u4e2d\u7684\u90e8\u7f72\u3001\u6d4b\u8bd5\u4e0e\u89c2\u6d4b\uff0c\u5b9e\u73b0\u4f4e\u5ef6\u8fdf\u3001\u9ad8\u53ef\u9760\u6027\u7684\u6e10\u8fdb\u5f0f\u53d1\u5e03\u3002", "motivation": "\u5f53\u524dOpen RAN\u7f3a\u4e4f\u5bf9xApp\u7684\u5168\u751f\u547d\u5468\u671f\u7ba1\u7406\uff0c\u5bfc\u81f4\u90e8\u7f72\u7f13\u6162\u4e14\u6613\u51fa\u9519\uff0c\u4e9f\u9700\u81ea\u52a8\u5316\u4e0e\u53ef\u89c2\u6d4b\u6027\u652f\u6301\u3002", "method": "\u6784\u5efaMANATEE\u5e73\u53f0\uff0c\u96c6\u6210CI/CD\u6d41\u6c34\u7ebf\u4e0e\u670d\u52a1\u7f51\u683c\uff08\u5982\u91d1\u4e1d\u96c0\u53d1\u5e03\u3001A/B\u6d4b\u8bd5\uff09\uff0c\u5e76\u5728Kubernetes+O-RAN RIC\u73af\u5883\u4e2d\u539f\u578b\u9a8c\u8bc1\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\u670d\u52a1\u7f51\u683c\u5f15\u5165\u5ef6\u8fdf\u4f4e\u4e8e1ms\uff0c\u652f\u6301\u7cbe\u51c6\u6d41\u91cf\u63a7\u5236\u4e0e\u65e0\u51b2\u7a81A/B\u6d4b\u8bd5\uff0c\u63d0\u5347xApp\u4ea4\u4ed8\u6548\u7387\u4e0e\u7cfb\u7edf\u97e7\u6027\u3002", "conclusion": "MANATEE\u662f\u9996\u4e2a\u878d\u5408DevOps\u4e0e\u670d\u52a1\u7f51\u683c\u7684O-RAN xApp\u7ba1\u7406\u5e73\u53f0\uff0c\u6709\u6548\u52a0\u901f\u521b\u65b0\u5e76\u4fdd\u969c\u5f02\u6784\u73af\u5883\u6027\u80fd\u3002"}}
{"id": "2601.12830", "categories": ["cs.DC", "cs.RO"], "pdf": "https://arxiv.org/pdf/2601.12830", "abs": "https://arxiv.org/abs/2601.12830", "authors": ["Om Mishra", "Jayesh Patil", "Sathwik Narkedimilli", "G Srikantha Sharma", "Ananda S", "Manjunath K Vanahalli"], "title": "From Design to Deorbit: A Solar-Electric Autonomous Module for Multi-Debris Remediation", "comment": "6 pages, 13 Figures, 2 tables", "summary": "The escalating accumulation of orbital debris threatens the sustainability of space operations, necessitating active removal solutions that overcome the limitations of current fuel-dependent methods. To address this, this study introduces a novel remediation architecture that integrates a mechanical clamping system for secure capture with a high-efficiency, solar-powered NASA Evolutionary Xenon Thruster (NEXT) and autonomous navigation protocols. High-fidelity simulations validate the architecture's capabilities, demonstrating a successful retrograde deorbit from 800 km to 100 km, <10m position Root Mean Square Errors (RMSE) via radar-based Extended Kalman Filter (EKF) navigation, and a 93\\% data delivery efficiency within 1 second using Delay/Disruption Tolerant Network (DTN) protocols. This approach significantly advances orbital management by establishing a benchmark for renewable solar propulsion that minimizes reliance on conventional fuels and extends mission longevity for multi-target removal.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u7ed3\u5408\u673a\u68b0\u5939\u6301\u3001\u592a\u9633\u80fd\u63a8\u8fdb\u548c\u81ea\u4e3b\u5bfc\u822a\u7684\u65b0\u578b\u8f68\u9053\u788e\u7247\u6e05\u9664\u67b6\u6784\uff0c\u5b9e\u73b0\u9ad8\u6548\u591a\u76ee\u6807\u79fb\u9664\u3002", "motivation": "\u5f53\u524d\u71c3\u6599\u4f9d\u8d56\u7684\u6e05\u9664\u65b9\u6cd5\u5b58\u5728\u5c40\u9650\uff0c\u9700\u53d1\u5c55\u53ef\u6301\u7eed\u7684\u8f68\u9053\u788e\u7247\u4e3b\u52a8\u79fb\u9664\u65b9\u6848\u3002", "method": "\u96c6\u6210\u673a\u68b0\u5939\u6301\u7cfb\u7edf\u3001\u592a\u9633\u80fdNEXT\u63a8\u8fdb\u5668\u4e0e\u96f7\u8fbe-EKF\u5bfc\u822a\u53caDTN\u901a\u4fe1\u534f\u8bae\uff0c\u901a\u8fc7\u9ad8\u4fdd\u771f\u4eff\u771f\u9a8c\u8bc1\u6027\u80fd\u3002", "result": "\u6210\u529f\u5b9e\u73b0800km\u81f3100km\u9006\u884c\u8131\u8f68\uff0c\u4f4d\u7f6eRMSE<10m\uff0c1\u79d2\u5185\u6570\u636e\u4f20\u8f93\u6548\u7387\u8fbe93%\u3002", "conclusion": "\u8be5\u67b6\u6784\u4e3a\u53ef\u518d\u751f\u80fd\u6e90\u9a71\u52a8\u7684\u957f\u671f\u591a\u76ee\u6807\u788e\u7247\u6e05\u9664\u4efb\u52a1\u6811\u7acb\u4e86\u65b0\u57fa\u51c6\uff0c\u663e\u8457\u63d0\u5347\u8f68\u9053\u7ba1\u7406\u53ef\u6301\u7eed\u6027\u3002"}}
{"id": "2601.12559", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2601.12559", "abs": "https://arxiv.org/abs/2601.12559", "authors": ["Yvan Labiche"], "title": "Automated Tool Support for Category-Partition Testing: Design Decisions, UI and Examples of Use", "comment": null, "summary": "Category-Partition is a functional testing technique that is based on the idea that the input domain of the system under test can be divided into sub-domains, with the assumption that inputs that belong to the same sub-domain trigger a similar behaviour and that therefore it is sufficient to select one input from each sub-domain. Category-Partition proceeds in several steps, from the identification of so-called categories and choices, possibly constrained, which are subsequently used to form test frames, i.e., combinations of choices, and eventually test cases. This paper reports on an ongoing attempt to automate as many of those steps as possible, with graphical-user interface tool support. Specifically, the user interface allows the user to specify parameters as well as so-called environment variables, further specify categories and choices with optional constraints. Choices are provided with precise specifications with operations specific to their types (e.g., Boolean, Integer, Real, String). Then, the tool automates the construction of test frames, which are combinations of choices, according to alternative selection criteria, and the identification of input values for parameters and environment variables for these test frames, thereby producing test cases. The paper illustrates the capabilities of the tool with the use of nine different case studies.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u4e00\u79cd\u81ea\u52a8\u5316Category-Partition\u529f\u80fd\u6d4b\u8bd5\u6280\u672f\u7684\u5de5\u5177\uff0c\u652f\u6301\u56fe\u5f62\u754c\u9762\u4e0e\u591a\u79cd\u6570\u636e\u7c7b\u578b\uff0c\u901a\u8fc7\u6848\u4f8b\u7814\u7a76\u9a8c\u8bc1\u5176\u80fd\u529b\u3002", "motivation": "\u63d0\u5347Category-Partition\u6d4b\u8bd5\u6280\u672f\u7684\u81ea\u52a8\u5316\u7a0b\u5ea6\uff0c\u51cf\u5c11\u4eba\u5de5\u64cd\u4f5c\uff0c\u63d0\u9ad8\u6d4b\u8bd5\u6548\u7387\u3002", "method": "\u5f00\u53d1\u56fe\u5f62\u754c\u9762\u5de5\u5177\uff0c\u652f\u6301\u7528\u6237\u5b9a\u4e49\u53c2\u6570\u3001\u73af\u5883\u53d8\u91cf\u3001\u7c7b\u522b\u4e0e\u9009\u62e9\u9879\uff0c\u5e76\u81ea\u52a8\u7ec4\u5408\u751f\u6210\u6d4b\u8bd5\u6846\u67b6\u4e0e\u6d4b\u8bd5\u7528\u4f8b\u3002", "result": "\u5de5\u5177\u6210\u529f\u5e94\u7528\u4e8e\u4e5d\u4e2a\u6848\u4f8b\u7814\u7a76\uff0c\u5c55\u793a\u4e86\u5176\u5728\u4e0d\u540c\u573a\u666f\u4e0b\u7684\u5b9e\u7528\u6027\u4e0e\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u5de5\u5177\u663e\u8457\u63d0\u5347\u4e86Category-Partition\u65b9\u6cd5\u7684\u81ea\u52a8\u5316\u6c34\u5e73\uff0c\u4e3a\u529f\u80fd\u6d4b\u8bd5\u63d0\u4f9b\u4e86\u5b9e\u7528\u652f\u6301\u3002"}}
{"id": "2601.12735", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2601.12735", "abs": "https://arxiv.org/abs/2601.12735", "authors": ["Hao Chen", "Yunchun Li", "Chen Chen", "Fengxu Lin", "Wei Li"], "title": "OpenAI for OpenAPI: Automated generation of REST API specification via LLMs", "comment": null, "summary": "REST APIs, based on the REpresentational State Transfer (REST) architecture, are the primary type of Web API. The OpenAPI Specification (OAS) serves as the de facto standard for describing REST APIs and is crucial for multiple software engineering tasks. However, developers face challenges in writing and maintaining OAS. Although static analysis shows potential for OAS generation, it is limited to specific programming languages and development frameworks. The powerful code understanding capabilities of LLMs offer new opportunities for OAS generation, yet they are constrained by context limitations and hallucinations. To address these challenges, we propose the OpenAI OpenAPI Project Scanner (OOPS), the first technology-agnostic LLM-based static analysis method for OAS generation, requiring fewer technology-specific rules and less human expert intervention. OOPS is implemented as an LLM agent workflow comprising two key steps: endpoint method extraction and OAS generation. By constructing an API dependency graph, it establishes necessary file associations to address LLMs' context limitations. Through multi-stage generation and self-refine, it mitigates both syntactic and semantic hallucinations during OAS generation. We evaluated OOPS on 12 real-world REST APIs spanning 5 programming languages and 8 development frameworks. Experimental results demonstrate that OOPS accurately generates high-quality OAS for REST APIs implemented with diverse technologies, achieving an average F1-score exceeding 98% for endpoint method inference, 97% for both request parameter and response inference, and 92% for parameter constraint inference. The input tokens average below 5.6K with a maximum of 16.2K, while the output tokens average below 0.9K with a maximum of 7.7K.", "AI": {"tldr": "OOPS\u662f\u4e00\u79cd\u57fa\u4e8eLLM\u7684\u9759\u6001\u5206\u6790\u65b9\u6cd5\uff0c\u7528\u4e8e\u8de8\u6280\u672f\u6808\u81ea\u52a8\u751f\u6210\u9ad8\u8d28\u91cfOpenAPI\u89c4\u8303\uff0c\u663e\u8457\u51cf\u5c11\u4eba\u5de5\u5e72\u9884\u5e76\u514b\u670d\u4e0a\u4e0b\u6587\u9650\u5236\u4e0e\u5e7b\u89c9\u95ee\u9898\u3002", "motivation": "\u5f00\u53d1\u8005\u5728\u7f16\u5199\u548c\u7ef4\u62a4OpenAPI\u89c4\u8303\u65f6\u9762\u4e34\u56f0\u96be\uff0c\u73b0\u6709\u9759\u6001\u5206\u6790\u65b9\u6cd5\u53d7\u9650\u4e8e\u7279\u5b9a\u8bed\u8a00\u548c\u6846\u67b6\uff0c\u800cLLM\u867d\u5177\u6f5c\u529b\u4f46\u53d7\u5236\u4e8e\u4e0a\u4e0b\u6587\u957f\u5ea6\u548c\u5e7b\u89c9\u3002", "method": "\u63d0\u51faOOPS\u65b9\u6cd5\uff0c\u901a\u8fc7\u6784\u5efaAPI\u4f9d\u8d56\u56fe\u89e3\u51b3\u4e0a\u4e0b\u6587\u9650\u5236\uff0c\u5e76\u91c7\u7528\u591a\u9636\u6bb5\u751f\u6210\u4e0e\u81ea\u4f18\u5316\u673a\u5236\u7f13\u89e3\u8bed\u6cd5\u4e0e\u8bed\u4e49\u5e7b\u89c9\uff0c\u5b9e\u73b0\u6280\u672f\u65e0\u5173\u7684OAS\u81ea\u52a8\u751f\u6210\u3002", "result": "\u572812\u4e2a\u771f\u5b9eREST API\u9879\u76ee\u4e0a\u6d4b\u8bd5\uff0cOOPS\u5728\u7aef\u70b9\u63a8\u65ad\u3001\u8bf7\u6c42/\u54cd\u5e94\u53c2\u6570\u53ca\u7ea6\u675f\u63a8\u65ad\u4e0aF1\u503c\u5206\u522b\u8fbe98%\u300197%\u548c92%\uff0c\u8f93\u5165\u8f93\u51fatoken\u5747\u4fdd\u6301\u8f83\u4f4e\u6c34\u5e73\u3002", "conclusion": "OOPS\u662f\u9996\u4e2a\u6280\u672f\u65e0\u5173\u7684LLM\u9a71\u52a8OAS\u751f\u6210\u65b9\u6848\uff0c\u5728\u591a\u8bed\u8a00\u591a\u6846\u67b6\u573a\u666f\u4e0b\u9ad8\u6548\u51c6\u786e\uff0c\u5927\u5e45\u964d\u4f4e\u4eba\u5de5\u6210\u672c\u4e0e\u6280\u672f\u4f9d\u8d56\u3002"}}
{"id": "2601.12762", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.12762", "abs": "https://arxiv.org/abs/2601.12762", "authors": ["Xingjie Gao", "Pengcheng Huang", "Zhenghao Liu", "Yukun Yan", "Shuo Wang", "Zulong Chen", "Chen Qian", "Ge Yu", "Yu Gu"], "title": "Teaching LLMs to Learn Tool Trialing and Execution through Environment Interaction", "comment": null, "summary": "Equipping Large Language Models (LLMs) with external tools enables them to solve complex real-world problems. However, the robustness of existing methods remains a critical challenge when confronting novel or evolving tools. Existing trajectory-centric paradigms primarily rely on memorizing static solution paths during training, which limits the ability of LLMs to generalize tool usage to newly introduced or previously unseen tools. In this paper, we propose ToolMaster, a framework that shifts tool use from imitating golden tool-calling trajectories to actively learning tool usage through interaction with the environment. To optimize LLMs for tool planning and invocation, ToolMaster adopts a trial-and-execution paradigm, which trains LLMs to first imitate teacher-generated trajectories containing explicit tool trials and self-correction, followed by reinforcement learning to coordinate the trial and execution phases jointly. This process enables agents to autonomously explore correct tool usage by actively interacting with environments and forming experiential knowledge that benefits tool execution. Experimental results demonstrate that ToolMaster significantly outperforms existing baselines in terms of generalization and robustness across unseen or unfamiliar tools. All code and data are available at https://github.com/NEUIR/ToolMaster.", "AI": {"tldr": "ToolMaster\u901a\u8fc7\u4ea4\u4e92\u5f0f\u5b66\u4e60\u63d0\u5347\u5927\u8bed\u8a00\u6a21\u578b\u5728\u65b0\u5de5\u5177\u4e0a\u7684\u6cdb\u5316\u4e0e\u9c81\u68d2\u6027\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u9759\u6001\u8f68\u8ff9\u8bb0\u5fc6\uff0c\u96be\u4ee5\u5e94\u5bf9\u65b0\u5de5\u5177\u6216\u52a8\u6001\u73af\u5883\u3002", "method": "\u91c7\u7528\u8bd5\u9519\u6267\u884c\u8303\u5f0f\uff0c\u7ed3\u5408\u6a21\u4eff\u5b66\u4e60\u4e0e\u5f3a\u5316\u5b66\u4e60\u4f18\u5316\u5de5\u5177\u89c4\u5212\u4e0e\u8c03\u7528\u3002", "result": "\u5b9e\u9a8c\u8868\u660eToolMaster\u5728\u672a\u89c1\u5de5\u5177\u4e0a\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "ToolMaster\u80fd\u6709\u6548\u63d0\u5347LLM\u5bf9\u65b0\u5de5\u5177\u7684\u81ea\u4e3b\u63a2\u7d22\u4e0e\u6267\u884c\u80fd\u529b\u3002"}}
{"id": "2601.12811", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2601.12811", "abs": "https://arxiv.org/abs/2601.12811", "authors": ["Julien Malka", "Stefano Zacchiroli", "Th\u00e9o Zimmermann"], "title": "Docker Does Not Guarantee Reproducibility", "comment": null, "summary": "The reproducibility of software environments is a critical concern in modern software engineering, with ramifications ranging from the effectiveness of collaboration workflows to software supply chain security and scientific reproducibility. Containerization technologies like Docker address this problem by encapsulating software environments into shareable filesystem snapshots known as images. While Docker is frequently cited in the literature as a tool that enables reproducibility in theory, the extent of its guarantees and limitations in practice remains under-explored.\n  In this work, we address this gap through two complementary approaches. First, we conduct a systematic literature review to examine how Docker is framed in scientific discourse on reproducibility and to identify documented best practices for writing Dockerfiles enabling reproducible image building. Then, we perform a large-scale empirical study of 5298 Docker builds collected from GitHub workflows. By rebuilding these images and comparing the results with their historical counterparts, we assess the real reproducibility of Docker images and evaluate the effectiveness of the best practices identified in the literature.", "AI": {"tldr": "\u672c\u6587\u901a\u8fc7\u6587\u732e\u7efc\u8ff0\u548c\u5927\u89c4\u6a21\u5b9e\u8bc1\u7814\u7a76\uff0c\u8bc4\u4f30Docker\u5728\u5b9e\u9645\u4e2d\u5b9e\u73b0\u73af\u5883\u53ef\u91cd\u73b0\u6027\u7684\u80fd\u529b\u53ca\u5176\u6700\u4f73\u5b9e\u8df5\u7684\u6709\u6548\u6027\u3002", "motivation": "\u5c3d\u7ba1Docker\u5e38\u88ab\u7406\u8bba\u8ba4\u4e3a\u652f\u6301\u73af\u5883\u53ef\u91cd\u73b0\u6027\uff0c\u4f46\u5176\u5b9e\u9645\u4fdd\u969c\u7a0b\u5ea6\u548c\u5c40\u9650\u6027\u5c1a\u672a\u5145\u5206\u63a2\u7d22\u3002", "method": "\u9996\u5148\u8fdb\u884c\u7cfb\u7edf\u6587\u732e\u7efc\u8ff0\uff0c\u8bc6\u522bDockerfile\u6700\u4f73\u5b9e\u8df5\uff1b\u5176\u6b21\u5bf95298\u4e2aGitHub\u5de5\u4f5c\u6d41\u4e2d\u7684Docker\u6784\u5efa\u8fdb\u884c\u5b9e\u8bc1\u5206\u6790\uff0c\u91cd\u5efa\u955c\u50cf\u5e76\u6bd4\u5bf9\u5386\u53f2\u7248\u672c\u3002", "result": "\u5b9e\u8bc1\u7ed3\u679c\u663e\u793aDocker\u955c\u50cf\u7684\u5b9e\u9645\u53ef\u91cd\u73b0\u6027\u6709\u9650\uff0c\u90e8\u5206\u6587\u732e\u63a8\u8350\u7684\u6700\u4f73\u5b9e\u8df5\u6548\u679c\u4e0d\u4f73\u3002", "conclusion": "Docker\u867d\u6709\u52a9\u4e8e\u73af\u5883\u5c01\u88c5\uff0c\u4f46\u9700\u66f4\u4e25\u683c\u7684\u6784\u5efa\u89c4\u8303\u548c\u5de5\u5177\u652f\u6301\u624d\u80fd\u771f\u6b63\u5b9e\u73b0\u53ef\u9760\u53ef\u91cd\u73b0\u6027\u3002"}}
{"id": "2601.12845", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2601.12845", "abs": "https://arxiv.org/abs/2601.12845", "authors": ["Jo\u00e3o Pascoal Faria", "Emanuel Trigo", "Vinicius Honorato", "Rui Abreu"], "title": "Automatic Generation of Formal Specification and Verification Annotations Using LLMs and Test Oracles", "comment": null, "summary": "Recent verification tools aim to make formal verification more accessible to software engineers by automating most of the verification process. However, annotating conventional programs with the formal specification and verification constructs (preconditions, postconditions, loop invariants, auxiliary predicates and functions and proof helpers) required to prove their correctness still demands significant manual effort and expertise. This paper investigates how LLMs can automatically generate such annotations for programs written in Dafny, a verification-aware programming language, starting from conventional code accompanied by natural language specifications (in comments) and test code. In experiments on 110 Dafny programs, a multimodel approach combining Claude Opus 4.5 and GPT-5.2 generated correct annotations for 98.2% of the programs within at most 8 repair iterations, using verifier feedback. A logistic regression analysis shows that proof-helper annotations contribute disproportionately to problem difficulty for current LLMs. Assertions in the test cases served as static oracles to automatically validate the generated pre/postconditions. We also compare generated and manual solutions and present an extension for Visual Studio Code to incorporate automatic generation into the IDE, with encouraging usability feedback.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u5982\u4f55\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u81ea\u52a8\u751f\u6210Dafny\u7a0b\u5e8f\u7684\u9a8c\u8bc1\u6ce8\u89e3\uff0c\u663e\u8457\u51cf\u5c11\u4eba\u5de5\u6807\u6ce8\u8d1f\u62c5\u3002", "motivation": "\u4f20\u7edf\u7a0b\u5e8f\u9a8c\u8bc1\u9700\u5927\u91cf\u624b\u52a8\u6807\u6ce8\u524d\u7f6e\u6761\u4ef6\u3001\u540e\u7f6e\u6761\u4ef6\u7b49\uff0c\u8d39\u65f6\u4e14\u4f9d\u8d56\u4e13\u5bb6\u7ecf\u9a8c\uff0c\u4e9f\u9700\u81ea\u52a8\u5316\u65b9\u6848\u3002", "method": "\u7ed3\u5408Claude Opus 4.5\u4e0eGPT-5.2\u7684\u591a\u6a21\u578b\u65b9\u6cd5\uff0c\u57fa\u4e8e\u81ea\u7136\u8bed\u8a00\u6ce8\u91ca\u548c\u6d4b\u8bd5\u4ee3\u7801\u751f\u6210Dafny\u9a8c\u8bc1\u6ce8\u89e3\uff0c\u5e76\u901a\u8fc7\u9a8c\u8bc1\u5668\u53cd\u9988\u8fed\u4ee3\u4fee\u590d\u3002", "result": "\u5728110\u4e2aDafny\u7a0b\u5e8f\u4e2d\uff0c98.2%\u5728\u6700\u591a8\u6b21\u4fee\u590d\u8fed\u4ee3\u5185\u751f\u6210\u6b63\u786e\u6ce8\u89e3\uff1b\u6d4b\u8bd5\u65ad\u8a00\u53ef\u4f5c\u4e3a\u9759\u6001\u9884\u8a00\u673a\u81ea\u52a8\u9a8c\u8bc1\u524d\u7f6e/\u540e\u7f6e\u6761\u4ef6\u3002", "conclusion": "LLM\u80fd\u9ad8\u6548\u8f85\u52a9\u7a0b\u5e8f\u5f62\u5f0f\u5316\u9a8c\u8bc1\uff0c\u8bc1\u660e\u52a9\u624b\u6ce8\u89e3\u662f\u5f53\u524d\u4e3b\u8981\u96be\u70b9\uff0c\u96c6\u6210IDE\u63d2\u4ef6\u83b7\u5f97\u826f\u597d\u53ef\u7528\u6027\u53cd\u9988\u3002"}}
{"id": "2601.12890", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2601.12890", "abs": "https://arxiv.org/abs/2601.12890", "authors": ["Hang Gao", "Tao Peng", "Baoquan Cui", "Hong Huang", "Fengge Wu", "Junsuo Zhao", "Jian Zhang"], "title": "Efficient Code Analysis via Graph-Guided Large Language Models", "comment": null, "summary": "Malicious behavior is often hidden in small, easily overlooked code fragments, especially within large and complex codebases. The cross-file dependencies of these fragments make it difficult for even powerful large language models (LLMs) to detect them reliably. We propose a graph-centric attention acquisition pipeline that enhances LLMs' ability to localize malicious behavior. The approach parses a project into a code graph, uses an LLM to encode nodes with semantic and structural signals, and trains a Graph Neural Network (GNN) under sparse supervision. The GNN performs an initial detection, and through backtracking of its predictions, identifies key code sections that are most likely to contain malicious behavior. These influential regions are then used to guide the LLM's attention for in-depth analysis. This strategy significantly reduces interference from irrelevant context while maintaining low annotation costs. Extensive experiments show that the method consistently outperforms existing methods on multiple public and self-built datasets, highlighting its potential for practical deployment in software security scenarios.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u56fe\u7684\u6ce8\u610f\u529b\u83b7\u53d6\u7ba1\u9053\uff0c\u589e\u5f3a\u5927\u8bed\u8a00\u6a21\u578b\u5728\u590d\u6742\u4ee3\u7801\u5e93\u4e2d\u5b9a\u4f4d\u6076\u610f\u884c\u4e3a\u7684\u80fd\u529b\u3002", "motivation": "\u6076\u610f\u884c\u4e3a\u5e38\u9690\u85cf\u4e8e\u5c0f\u6bb5\u4ee3\u7801\u4e2d\uff0c\u4e14\u8de8\u6587\u4ef6\u4f9d\u8d56\u4f7f\u5927\u8bed\u8a00\u6a21\u578b\u96be\u4ee5\u53ef\u9760\u68c0\u6d4b\u3002", "method": "\u5c06\u9879\u76ee\u89e3\u6790\u4e3a\u4ee3\u7801\u56fe\uff0c\u7528LLM\u7f16\u7801\u8282\u70b9\u8bed\u4e49\u4e0e\u7ed3\u6784\u4fe1\u606f\uff0c\u8bad\u7ec3\u7a00\u758f\u76d1\u7763\u7684GNN\u8fdb\u884c\u521d\u6b65\u68c0\u6d4b\u5e76\u56de\u6eaf\u5173\u952e\u4ee3\u7801\u533a\u57df\uff0c\u5f15\u5bfcLLM\u6df1\u5165\u5206\u6790\u3002", "result": "\u5728\u591a\u4e2a\u516c\u5f00\u548c\u81ea\u5efa\u6570\u636e\u96c6\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u663e\u8457\u964d\u4f4e\u65e0\u5173\u4e0a\u4e0b\u6587\u5e72\u6270\u4e14\u6807\u6ce8\u6210\u672c\u4f4e\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5177\u5907\u5b9e\u9645\u90e8\u7f72\u4e8e\u8f6f\u4ef6\u5b89\u5168\u573a\u666f\u7684\u6f5c\u529b\u3002"}}
{"id": "2601.12927", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2601.12927", "abs": "https://arxiv.org/abs/2601.12927", "authors": ["Weilin Jin", "Chenyu Zhao", "Zeshun Huang", "Chaoyun Zhang", "Qingwei Lin", "Chetan Bansal", "Saravan Rajmohan", "Shenglin Zhang", "Yongqian Sun", "Dan Pei", "Yifan Wu", "Tong Jia", "Ying Li", "Zhonghai Wu", "Minghua Ma"], "title": "A Benchmark for Language Models in Real-World System Building", "comment": null, "summary": "During migration across instruction set architectures (ISAs), software package build repair is a critical task for ensuring the reliability of software deployment and the stability of modern operating systems. While Large Language Models (LLMs) have shown promise in tackling this challenge, prior work has primarily focused on single instruction set architecture (ISA) and homogeneous programming languages. To address this limitation, we introduce a new benchmark designed for software package build repair across diverse architectures and languages. Comprising 268 real-world software package build failures, the benchmark provides a standardized evaluation pipeline. We evaluate six state-of-the-art LLMs on the benchmark, and the results show that cross-ISA software package repair remains difficult and requires further advances. By systematically exposing this challenge, the benchmark establishes a foundation for advancing future methods aimed at improving software portability and bridging architectural gaps.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u4e2a\u65b0\u57fa\u51c6\uff0c\u7528\u4e8e\u8bc4\u4f30\u8de8\u6307\u4ee4\u96c6\u67b6\u6784\u548c\u591a\u8bed\u8a00\u7684\u8f6f\u4ef6\u5305\u6784\u5efa\u4fee\u590d\u80fd\u529b\uff0c\u63ed\u793a\u5f53\u524d\u5927\u6a21\u578b\u5728\u6b64\u4efb\u52a1\u4e0a\u7684\u4e0d\u8db3\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u4e3b\u8981\u5173\u6ce8\u5355\u4e00\u6307\u4ee4\u96c6\u548c\u540c\u8d28\u8bed\u8a00\uff0c\u7f3a\u4e4f\u5bf9\u8de8\u67b6\u6784\u3001\u591a\u8bed\u8a00\u573a\u666f\u4e0b\u8f6f\u4ef6\u6784\u5efa\u4fee\u590d\u7684\u7cfb\u7edf\u8bc4\u4f30\u3002", "method": "\u6784\u5efa\u5305\u542b268\u4e2a\u771f\u5b9e\u6784\u5efa\u5931\u8d25\u6848\u4f8b\u7684\u57fa\u51c6\u6570\u636e\u96c6\uff0c\u5e76\u8bbe\u8ba1\u6807\u51c6\u5316\u8bc4\u4f30\u6d41\u7a0b\uff0c\u6d4b\u8bd5\u516d\u79cd\u524d\u6cbf\u5927\u6a21\u578b\u6027\u80fd\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u5f53\u524d\u5927\u6a21\u578b\u5728\u8de8\u6307\u4ee4\u96c6\u67b6\u6784\u7684\u8f6f\u4ef6\u5305\u4fee\u590d\u4efb\u52a1\u4e0a\u8868\u73b0\u4e0d\u4f73\uff0c\u4ecd\u9700\u8fdb\u4e00\u6b65\u6280\u672f\u7a81\u7834\u3002", "conclusion": "\u8be5\u57fa\u51c6\u4e3a\u672a\u6765\u63d0\u5347\u8f6f\u4ef6\u53ef\u79fb\u690d\u6027\u4e0e\u5f25\u5408\u67b6\u6784\u5dee\u5f02\u7684\u7814\u7a76\u5960\u5b9a\u57fa\u7840\u3002"}}
{"id": "2601.13424", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2601.13424", "abs": "https://arxiv.org/abs/2601.13424", "authors": ["Alexander Martinez Mendez", "Antonio J. Rubio-Montero", "Carlos J. Barrios H.", "Hern\u00e1n Asorey", "Rafael Mayo-Garc\u00eda", "Luis A. N\u00fa\u00f1ez"], "title": "Driving Computational Efficiency in Large-Scale Platforms using HPC Technologies", "comment": "Accepted and presented at CARLA 2025. To appear in Springer LNCS proceedings", "summary": "The Latin American Giant Observatory (LAGO) project utilizes extensive High-Performance Computing (HPC) resources for complex astroparticle physics simulations, making resource efficiency critical for scientific productivity and sustainability. This article presents a detailed analysis focused on quantifying and improving HPC resource utilization efficiency specifically within the LAGO computational environment. The core objective is to understand how LAGO's distinct computational workloads-characterized by a prevalent coarse-grained, task-parallel execution model-consume resources in practice. To achieve this, we analyze historical job accounting data from the EGI FedCloud platform, identifying primary workload categories (Monte Carlo simulations, data processing, user analysis/testing) and evaluating their performance using key efficiency metrics (CPU utilization, walltime utilization, and I/O patterns). Our analysis reveals significant patterns, including high CPU efficiency within individual simulation tasks contrasted with the distorting impact of short test jobs on aggregate metrics. This work pinpoints specific inefficiencies and provides data-driven insights into LAGO's HPC usage. The findings directly inform recommendations for optimizing resource requests, refining workflow management strategies, and guiding future efforts to enhance computational throughput, ultimately maximizing the scientific return from LAGO's HPC investments.", "AI": {"tldr": "\u8be5\u7814\u7a76\u5206\u6790\u4e86LAGO\u9879\u76ee\u5728EGI FedCloud\u5e73\u53f0\u4e0a\u7684HPC\u8d44\u6e90\u4f7f\u7528\u6548\u7387\uff0c\u8bc6\u522b\u5de5\u4f5c\u8d1f\u8f7d\u7c7b\u578b\u5e76\u63d0\u51fa\u4f18\u5316\u5efa\u8bae\u4ee5\u63d0\u5347\u79d1\u5b66\u4ea7\u51fa\u3002", "motivation": "\u63d0\u9ad8LAGO\u9879\u76ee\u5728\u590d\u6742\u5929\u4f53\u7c92\u5b50\u7269\u7406\u6a21\u62df\u4e2d\u5bf9HPC\u8d44\u6e90\u7684\u5229\u7528\u6548\u7387\uff0c\u4ee5\u589e\u5f3a\u79d1\u7814\u751f\u4ea7\u529b\u4e0e\u53ef\u6301\u7eed\u6027\u3002", "method": "\u901a\u8fc7\u5206\u6790EGI FedCloud\u5e73\u53f0\u7684\u5386\u53f2\u4f5c\u4e1a\u6570\u636e\uff0c\u5212\u5206\u4e3b\u8981\u5de5\u4f5c\u8d1f\u8f7d\u7c7b\u522b\uff0c\u5e76\u91c7\u7528CPU\u5229\u7528\u7387\u3001\u8fd0\u884c\u65f6\u95f4\u5229\u7528\u7387\u548cI/O\u6a21\u5f0f\u7b49\u5173\u952e\u6307\u6807\u8bc4\u4f30\u6027\u80fd\u3002", "result": "\u53d1\u73b0\u5355\u4e2a\u6a21\u62df\u4efb\u52a1CPU\u6548\u7387\u9ad8\uff0c\u4f46\u77ed\u65f6\u6d4b\u8bd5\u4efb\u52a1\u4f1a\u626d\u66f2\u6574\u4f53\u6307\u6807\uff0c\u63ed\u793a\u4e86\u5177\u4f53\u4f4e\u6548\u73af\u8282\u5e76\u63d0\u4f9b\u6570\u636e\u9a71\u52a8\u7684\u4f18\u5316\u65b9\u5411\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u4e3a\u4f18\u5316\u8d44\u6e90\u7533\u8bf7\u3001\u6539\u8fdb\u5de5\u4f5c\u6d41\u7ba1\u7406\u53ca\u63d0\u5347\u8ba1\u7b97\u541e\u5410\u91cf\u63d0\u4f9b\u4e86\u4f9d\u636e\uff0c\u6709\u52a9\u4e8e\u6700\u5927\u5316LAGO\u9879\u76eeHPC\u6295\u8d44\u7684\u79d1\u7814\u56de\u62a5\u3002"}}
{"id": "2601.12951", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.12951", "abs": "https://arxiv.org/abs/2601.12951", "authors": ["Felix M\u00e4chtle", "Jan-Niclas Serr", "Nils Loose", "Thomas Eisenbarth"], "title": "Beyond Accuracy: Characterizing Code Comprehension Capabilities in (Large) Language Models", "comment": "Published in the Proceedings of DeepTest 2026", "summary": "Large Language Models (LLMs) are increasingly integrated into software engineering workflows, yet current benchmarks provide only coarse performance summaries that obscure the diverse capabilities and limitations of these models. This paper investigates whether LLMs' code-comprehension performance aligns with traditional human-centric software metrics or instead reflects distinct, non-human regularities. We introduce a diagnostic framework that reframes code understanding as a binary input-output consistency task, enabling the evaluation of classification and generative models. Using a large-scale dataset, we correlate model performance with traditional, human-centric complexity metrics, such as lexical size, control-flow complexity, and abstract syntax tree structure. Our analyses reveal minimal correlation between human-defined metrics and LLM success (AUROC 0.63), while shadow models achieve substantially higher predictive performance (AUROC 0.86), capturing complex, partially predictable patterns beyond traditional software measures. These findings suggest that LLM comprehension reflects model-specific regularities only partially accessible through either human-designed or learned features, emphasizing the need for benchmark methodologies that move beyond aggregate accuracy and toward instance-level diagnostics, while acknowledging fundamental limits in predicting correct outcomes.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u8bca\u65ad\u6846\u67b6\uff0c\u63ed\u793a\u5927\u8bed\u8a00\u6a21\u578b\u4ee3\u7801\u7406\u89e3\u80fd\u529b\u4e0e\u4eba\u7c7b\u8f6f\u4ef6\u5ea6\u91cf\u76f8\u5173\u6027\u4f4e\uff0c\u5f3a\u8c03\u9700\u5b9e\u4f8b\u7ea7\u8bc4\u4f30\u65b9\u6cd5\u3002", "motivation": "\u5f53\u524d\u57fa\u51c6\u4ec5\u63d0\u4f9b\u7c97\u7c92\u5ea6\u6027\u80fd\u603b\u7ed3\uff0c\u96be\u4ee5\u63ed\u793a\u5927\u8bed\u8a00\u6a21\u578b\u5728\u4ee3\u7801\u7406\u89e3\u4e2d\u7684\u771f\u5b9e\u80fd\u529b\u4e0e\u5c40\u9650\u3002", "method": "\u5c06\u4ee3\u7801\u7406\u89e3\u91cd\u6784\u4e3a\u8f93\u5165\u8f93\u51fa\u4e00\u81f4\u6027\u4efb\u52a1\uff0c\u7ed3\u5408\u4f20\u7edf\u590d\u6742\u5ea6\u6307\u6807\u4e0e\u5f71\u5b50\u6a21\u578b\u8fdb\u884c\u5927\u89c4\u6a21\u6570\u636e\u76f8\u5173\u6027\u5206\u6790\u3002", "result": "\u4eba\u7c7b\u6307\u6807\u4e0eLLM\u8868\u73b0\u76f8\u5173\u6027\u5f31\uff08AUROC 0.63\uff09\uff0c\u5f71\u5b50\u6a21\u578b\u9884\u6d4b\u66f4\u5f3a\uff08AUROC 0.86\uff09\uff0c\u8868\u660eLLM\u9075\u5faa\u81ea\u8eab\u89c4\u5f8b\u3002", "conclusion": "\u9700\u8d85\u8d8a\u603b\u4f53\u51c6\u786e\u7387\u7684\u57fa\u51c6\u65b9\u6cd5\uff0c\u91c7\u7528\u5b9e\u4f8b\u7ea7\u8bca\u65ad\uff0c\u5e76\u627f\u8ba4\u9884\u6d4b\u6b63\u786e\u7ed3\u679c\u5b58\u5728\u6839\u672c\u6027\u9650\u5236\u3002"}}
{"id": "2601.13496", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2601.13496", "abs": "https://arxiv.org/abs/2601.13496", "authors": ["Anna Karanika", "Kai-Siang Wang", "Han-Ting Liang", "Shalni Sundram", "Indranil Gupta"], "title": "RASC: Enhancing Observability & Programmability in Smart Spaces", "comment": "16 pages, 19 figures. This paper is a preprint version of our upcoming paper of the same name in the USENIX Symposium on Networked Systems Design and Implementation (NSDI), 2026", "summary": "While RPCs form the bedrock of systems stacks, we posit that IoT device collections in smart spaces like homes, warehouses, and office buildings--which are all \"user-facing\"--require a more expressive abstraction. Orthogonal to prior work, which improved the reliability of IoT communication, our work focuses on improving the observability and programmability of IoT actions. We present the RASC (Request-Acknowledge-Start-Complete) abstraction, which provides acknowledgments at critical points after an IoT device action is initiated. RASC is a better fit for IoT actions, which naturally vary in length spatially (across devices) and temporally (across time, for a given device). RASC also enables the design of several new features: predicting action completion times accurately, detecting failures of actions faster, allowing fine-grained dependencies in programming, and scheduling. RASC is intended to be implemented atop today's available RPC mechanisms, rather than as a replacement. We integrated RASC into a popular and open-source IoT framework called Home Assistant. Our trace-driven evaluation finds that RASC meets latency SLOs, especially for long actions that last O(mins), which are common in smart spaces. Our scheduling policies for home automations (e.g., routines) outperform state-of-the-art counterparts by 10%-55%.", "AI": {"tldr": "RASC\u62bd\u8c61\u901a\u8fc7\u5728IoT\u8bbe\u5907\u52a8\u4f5c\u7684\u5173\u952e\u8282\u70b9\u63d0\u4f9b\u786e\u8ba4\u673a\u5236\uff0c\u63d0\u5347\u4e86\u53ef\u89c2\u5bdf\u6027\u548c\u53ef\u7f16\u7a0b\u6027\uff0c\u5e76\u652f\u6301\u66f4\u4f18\u7684\u8c03\u5ea6\u7b56\u7565\u3002", "motivation": "\u73b0\u6709RPC\u673a\u5236\u96be\u4ee5\u6ee1\u8db3\u9762\u5411\u7528\u6237\u7684\u667a\u80fd\u7a7a\u95f4\u4e2dIoT\u8bbe\u5907\u52a8\u4f5c\u5728\u65f6\u7a7a\u7ef4\u5ea6\u4e0a\u7684\u591a\u6837\u6027\u9700\u6c42\u3002", "method": "\u63d0\u51faRASC\uff08\u8bf7\u6c42-\u786e\u8ba4-\u5f00\u59cb-\u5b8c\u6210\uff09\u62bd\u8c61\uff0c\u6784\u5efa\u4e8e\u73b0\u6709RPC\u673a\u5236\u4e4b\u4e0a\uff0c\u96c6\u6210\u5230Home Assistant\u6846\u67b6\u4e2d\u3002", "result": "\u5b9e\u9a8c\u8868\u660eRASC\u80fd\u6ee1\u8db3\u5ef6\u8fdfSLO\uff0c\u5c24\u5176\u5bf9\u6301\u7eed\u6570\u5206\u949f\u7684\u957f\u52a8\u4f5c\u6548\u679c\u663e\u8457\uff0c\u8c03\u5ea6\u7b56\u7565\u6027\u80fd\u63d0\u534710%-55%\u3002", "conclusion": "RASC\u4e3aIoT\u7cfb\u7edf\u63d0\u4f9b\u4e86\u66f4\u9002\u5408\u7528\u6237\u573a\u666f\u7684\u52a8\u4f5c\u7ba1\u7406\u62bd\u8c61\uff0c\u589e\u5f3a\u4e86\u529f\u80fd\u6027\u548c\u6548\u7387\u3002"}}
{"id": "2601.13015", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2601.13015", "abs": "https://arxiv.org/abs/2601.13015", "authors": ["Nowfel Mashnoor", "Mohammad Akyash", "Hadi Kamali", "Kimia Azar"], "title": "MeltRTL: Multi-Expert LLMs with Inference-time Intervention for RTL Code Generation", "comment": null, "summary": "The automated generation of hardware register-transfer level (RTL) code with large language models (LLMs) shows promise, yet current solutions struggle to produce syntactically and functionally correct code for complex digital designs. This paper introduces MeltRTL, a novel framework that integrates multi-expert attention with inference-time intervention (ITI) to significantly improve LLM-based RTL code generation accuracy without retraining the base model. MeltRTL introduces three key innovations: (1) A multi-expert attention architecture that dynamically routes design specifications to specialized expert networks, enabling targeted reasoning across various hardware categories; (2) An inference-time intervention mechanism that employs non-linear probes to detect and correct hardware-specific inaccuracies during generation; and (3) An efficient intervention framework that selectively operates on expert-specific attention heads with minimal computational overhead. We evaluate MeltRTL on the VerilogEval benchmark, achieving 96% synthesizability and 60% functional correctness, compared to the base LLM's 85.3% and 45.3%, respectively. These improvements are obtained entirely at inference time, with only 27% computational overhead and no model fine-tuning, making MeltRTL immediately deployable on existing pre-trained LLMs. Ablation studies further show the complementary benefits of multi-expert architecture and ITI, highlighting their synergistic effects when combined.", "AI": {"tldr": "MeltRTL\u901a\u8fc7\u591a\u4e13\u5bb6\u6ce8\u610f\u529b\u548c\u63a8\u7406\u65f6\u5e72\u9884\u673a\u5236\uff0c\u5728\u4e0d\u91cd\u65b0\u8bad\u7ec3\u6a21\u578b\u7684\u60c5\u51b5\u4e0b\u663e\u8457\u63d0\u5347LLM\u751f\u6210\u786c\u4ef6RTL\u4ee3\u7801\u7684\u51c6\u786e\u7387\u3002", "motivation": "\u5f53\u524dLLM\u5728\u751f\u6210\u590d\u6742\u6570\u5b57\u8bbe\u8ba1\u7684RTL\u4ee3\u7801\u65f6\u5b58\u5728\u8bed\u6cd5\u548c\u529f\u80fd\u9519\u8bef\uff0c\u4e9f\u9700\u6539\u8fdb\u3002", "method": "\u5f15\u5165\u591a\u4e13\u5bb6\u6ce8\u610f\u529b\u67b6\u6784\u3001\u63a8\u7406\u65f6\u5e72\u9884\u673a\u5236\u548c\u9ad8\u6548\u5e72\u9884\u6846\u67b6\uff0c\u52a8\u6001\u8def\u7531\u8bbe\u8ba1\u89c4\u8303\u5e76\u7ea0\u6b63\u786c\u4ef6\u7279\u5b9a\u9519\u8bef\u3002", "result": "\u5728VerilogEval\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u7efc\u5408\u6027\u548c\u529f\u80fd\u6027\u6b63\u786e\u7387\u5206\u522b\u63d0\u5347\u81f396%\u548c60%\uff0c\u4ec5\u589e\u52a027%\u8ba1\u7b97\u5f00\u9500\u3002", "conclusion": "MeltRTL\u65e0\u9700\u5fae\u8c03\u5373\u53ef\u90e8\u7f72\u4e8e\u73b0\u6709\u9884\u8bad\u7ec3LLM\uff0c\u591a\u4e13\u5bb6\u67b6\u6784\u4e0eITI\u534f\u540c\u4f5c\u7528\u663e\u8457\u3002"}}
{"id": "2601.13097", "categories": ["cs.SE", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.13097", "abs": "https://arxiv.org/abs/2601.13097", "authors": ["Elena Bruches", "Daniil Grebenkin", "Mikhail Klementev", "Vadim Alperovich", "Roman Derunets", "Dari Baturova", "Georgy Mkrtchyan", "Oleg Sedukhin", "Ivan Bondarenko", "Nikolay Bushkov", "Stanislav Moiseev"], "title": "RM -RF: Reward Model for Run-Free Unit Test Evaluation", "comment": "This paper has been accepted for publication at the 33rd IEEE International Conference on Software Analysis, Evolution and Reengineering (SANER 2026)", "summary": "We present RM-RF, a lightweight reward model for run-free evaluation of automatically generated unit tests. Instead of repeatedly compiling and executing candidate tests, RM-RF predicts - from source and test code alone - three execution-derived signals: (1) whether the augmented test suite compiles and runs successfully, (2) whether the generated test cases increase code coverage, and (3) whether the generated test cases improve the mutation kill rate. To train and evaluate RM-RF we assemble a multilingual dataset (Java, Python, Go) of focal files, test files, and candidate test additions labeled by an execution-based pipeline, and we release an associated dataset and methodology for comparative evaluation. We tested multiple model families and tuning regimes (zero-shot, full fine-tuning, and PEFT via LoRA), achieving an average F1 of 0.69 across the three targets. Compared to conventional compile-and-run instruments, RM-RF provides substantially lower latency and infrastructure cost while delivering competitive predictive fidelity, enabling fast, scalable feedback for large-scale test generation and RL-based code optimization.", "AI": {"tldr": "RM-RF\u662f\u4e00\u79cd\u8f7b\u91cf\u7ea7\u5956\u52b1\u6a21\u578b\uff0c\u65e0\u9700\u8fd0\u884c\u5373\u53ef\u8bc4\u4f30\u81ea\u52a8\u751f\u6210\u7684\u5355\u5143\u6d4b\u8bd5\uff0c\u9884\u6d4b\u7f16\u8bd1\u6210\u529f\u3001\u8986\u76d6\u7387\u63d0\u5347\u548c\u53d8\u5f02\u6740\u6b7b\u7387\u4e09\u9879\u6307\u6807\uff0c\u663e\u8457\u964d\u4f4e\u5ef6\u8fdf\u4e0e\u6210\u672c\u3002", "motivation": "\u4f20\u7edf\u6d4b\u8bd5\u8bc4\u4f30\u4f9d\u8d56\u53cd\u590d\u7f16\u8bd1\u6267\u884c\uff0c\u6210\u672c\u9ad8\u3001\u5ef6\u8fdf\u5927\uff0c\u4e9f\u9700\u9ad8\u6548\u66ff\u4ee3\u65b9\u6848\u3002", "method": "\u57fa\u4e8e\u6e90\u7801\u548c\u6d4b\u8bd5\u4ee3\u7801\u8bad\u7ec3\u6a21\u578b\uff0c\u9884\u6d4b\u4e09\u9879\u6267\u884c\u4fe1\u53f7\uff1b\u91c7\u7528\u591a\u8bed\u8a00\u6570\u636e\u96c6\uff08Java/Python/Go\uff09\uff0c\u5bf9\u6bd4\u591a\u79cd\u6a21\u578b\u5bb6\u65cf\u4e0e\u8c03\u53c2\u7b56\u7565\uff08\u96f6\u6837\u672c\u3001\u5168\u5fae\u8c03\u3001LoRA\uff09\u3002", "result": "\u5728\u4e09\u9879\u9884\u6d4b\u76ee\u6807\u4e0a\u5e73\u5747F1\u8fbe0.69\uff0c\u76f8\u6bd4\u4f20\u7edf\u65b9\u6cd5\u663e\u8457\u964d\u4f4e\u5ef6\u8fdf\u4e0e\u57fa\u7840\u8bbe\u65bd\u6210\u672c\uff0c\u540c\u65f6\u4fdd\u6301\u9884\u6d4b\u51c6\u786e\u6027\u3002", "conclusion": "RM-RF\u9002\u7528\u4e8e\u5927\u89c4\u6a21\u6d4b\u8bd5\u751f\u6210\u4e0e\u5f3a\u5316\u5b66\u4e60\u4ee3\u7801\u4f18\u5316\u573a\u666f\uff0c\u63d0\u4f9b\u5feb\u901f\u53ef\u6269\u5c55\u53cd\u9988\u3002"}}
{"id": "2601.13994", "categories": ["cs.DC", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.13994", "abs": "https://arxiv.org/abs/2601.13994", "authors": ["Mingyuan Chi"], "title": "torch-sla: Differentiable Sparse Linear Algebra with Adjoint Solvers and Sparse Tensor Parallelism for PyTorch", "comment": null, "summary": "Industrial scientific computing predominantly uses sparse matrices to represent unstructured data -- finite element meshes, graphs, point clouds. We present \\torchsla{}, an open-source PyTorch library that enables GPU-accelerated, scalable, and differentiable sparse linear algebra. The library addresses three fundamental challenges: (1) GPU acceleration for sparse linear solves, nonlinear solves (Newton, Picard, Anderson), and eigenvalue computation; (2) Multi-GPU scaling via domain decomposition with halo exchange, reaching \\textbf{400 million DOF linear solve on 3 GPUs}; and (3) Adjoint-based differentiation} achieving $\\mathcal{O}(1)$ computational graph nodes (for autograd) and $\\mathcal{O}(\\text{nnz})$ memory -- independent of solver iterations. \\torchsla{} supports multiple backends (SciPy, cuDSS, PyTorch-native) and seamlessly integrates with PyTorch autograd for end-to-end differentiable simulations. Code is available at https://github.com/walkerchi/torch-sla.", "AI": {"tldr": "torchsla\u662f\u4e00\u4e2a\u652f\u6301GPU\u52a0\u901f\u3001\u591aGPU\u6269\u5c55\u548c\u81ea\u52a8\u5fae\u5206\u7684PyTorch\u7a00\u758f\u7ebf\u6027\u4ee3\u6570\u5e93\u3002", "motivation": "\u5de5\u4e1a\u79d1\u5b66\u8ba1\u7b97\u4e2d\u5e7f\u6cdb\u4f7f\u7528\u7a00\u758f\u77e9\u9635\uff0c\u4f46\u7f3a\u4e4f\u9ad8\u6548\u3001\u53ef\u5fae\u5206\u4e14\u652f\u6301\u591aGPU\u6269\u5c55\u7684\u5de5\u5177\u3002", "method": "\u901a\u8fc7GPU\u52a0\u901f\u6c42\u89e3\u5668\u3001\u57df\u5206\u89e3\u4e0e\u5149\u73af\u4ea4\u6362\u5b9e\u73b0\u591aGPU\u6269\u5c55\uff0c\u5e76\u91c7\u7528\u4f34\u968f\u6cd5\u5b9e\u73b0\u9ad8\u6548\u81ea\u52a8\u5fae\u5206\u3002", "result": "\u57283\u4e2aGPU\u4e0a\u5b9e\u73b0\u4e864\u4ebf\u81ea\u7531\u5ea6\u7684\u7ebf\u6027\u6c42\u89e3\uff0c\u8ba1\u7b97\u56fe\u8282\u70b9\u4e3aO(1)\uff0c\u5185\u5b58\u5360\u7528\u4e3aO(nnz)\u3002", "conclusion": "torchsla\u4e3a\u7aef\u5230\u7aef\u53ef\u5fae\u6a21\u62df\u63d0\u4f9b\u4e86\u9ad8\u6548\u3001\u7075\u6d3b\u4e14\u6613\u7528\u7684\u7a00\u758f\u7ebf\u6027\u4ee3\u6570\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2601.13118", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2601.13118", "abs": "https://arxiv.org/abs/2601.13118", "authors": ["Alessandro Midolo", "Alessandro Giagnorio", "Fiorella Zampetti", "Rosalia Tufano", "Gabriele Bavota", "Massimiliano Di Penta"], "title": "Guidelines to Prompt Large Language Models for Code Generation: An Empirical Characterization", "comment": null, "summary": "Large Language Models (LLMs) are nowadays extensively used for various types of software engineering tasks, primarily code generation. Previous research has shown how suitable prompt engineering could help developers in improving their code generation prompts. However, so far, there do not exist specific guidelines driving developers towards writing suitable prompts for code generation. In this work, we derive and evaluate development-specific prompt optimization guidelines. First, we use an iterative, test-driven approach to automatically refine code generation prompts, and we analyze the outcome of this process to identify prompt improvement items that lead to test passes. We use such elements to elicit 10 guidelines for prompt improvement, related to better specifying I/O, pre-post conditions, providing examples, various types of details, or clarifying ambiguities. We conduct an assessment with 50 practitioners, who report their usage of the elicited prompt improvement patterns, as well as their perceived usefulness, which does not always correspond to the actual usage before knowing our guidelines. Our results lead to implications not only for practitioners and educators, but also for those aimed at creating better LLM-aided software development tools.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u5e76\u8bc4\u4f30\u4e86\u9488\u5bf9\u4ee3\u7801\u751f\u6210\u4efb\u52a1\u768410\u6761\u63d0\u793a\u4f18\u5316\u6307\u5357\uff0c\u5e2e\u52a9\u5f00\u53d1\u8005\u66f4\u6709\u6548\u5730\u4f7f\u7528\u5927\u8bed\u8a00\u6a21\u578b\u3002", "motivation": "\u5f53\u524d\u7f3a\u4e4f\u4e13\u95e8\u6307\u5bfc\u5f00\u53d1\u8005\u7f16\u5199\u9002\u5408\u4ee3\u7801\u751f\u6210\u63d0\u793a\u7684\u5177\u4f53\u51c6\u5219\u3002", "method": "\u901a\u8fc7\u8fed\u4ee3\u3001\u6d4b\u8bd5\u9a71\u52a8\u7684\u65b9\u6cd5\u81ea\u52a8\u4f18\u5316\u63d0\u793a\uff0c\u5e76\u5206\u6790\u7ed3\u679c\u63d0\u70bc\u51fa\u6539\u8fdb\u9879\uff0c\u518d\u7ecf50\u540d\u4ece\u4e1a\u8005\u8bc4\u4f30\u5176\u5b9e\u9645\u4f7f\u7528\u4e0e\u611f\u77e5\u6548\u7528\u3002", "result": "\u63d0\u70bc\u51fa10\u6761\u63d0\u793a\u4f18\u5316\u6307\u5357\uff0c\u6db5\u76d6\u8f93\u5165\u8f93\u51fa\u8bf4\u660e\u3001\u524d\u540e\u6761\u4ef6\u3001\u793a\u4f8b\u63d0\u4f9b\u7b49\uff1b\u4ece\u4e1a\u8005\u53cd\u9988\u663e\u793a\u611f\u77e5\u6548\u7528\u4e0e\u5b9e\u9645\u4f7f\u7528\u5b58\u5728\u5dee\u5f02\u3002", "conclusion": "\u8be5\u7814\u7a76\u5bf9\u5f00\u53d1\u8005\u3001\u6559\u80b2\u8005\u53caLLM\u8f85\u52a9\u5f00\u53d1\u5de5\u5177\u8bbe\u8ba1\u8005\u5747\u6709\u5b9e\u8df5\u610f\u4e49\u3002"}}
{"id": "2601.13134", "categories": ["cs.SE", "cs.CV"], "pdf": "https://arxiv.org/pdf/2601.13134", "abs": "https://arxiv.org/abs/2601.13134", "authors": ["Heng Fang", "Adam J. Stewart", "Isaac Corley", "Xiao Xiang Zhu", "Hossein Azizpour"], "title": "Earth Embeddings as Products: Taxonomy, Ecosystem, and Standardized Access", "comment": null, "summary": "Geospatial Foundation Models (GFMs) provide powerful representations, but high compute costs hinder their widespread use. Pre-computed embedding data products offer a practical \"frozen\" alternative, yet they currently exist in a fragmented ecosystem of incompatible formats and resolutions. This lack of standardization creates an engineering bottleneck that prevents meaningful model comparison and reproducibility. We formalize this landscape through a three-layer taxonomy: Data, Tools, and Value. We survey existing products to identify interoperability barriers. To bridge this gap, we extend TorchGeo with a unified API that standardizes the loading and querying of diverse embedding products. By treating embeddings as first-class geospatial datasets, we decouple downstream analysis from model-specific engineering, providing a roadmap for more transparent and accessible Earth observation workflows.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u901a\u8fc7\u7edf\u4e00API\u6807\u51c6\u5316\u5730\u7406\u7a7a\u95f4\u5d4c\u5165\u6570\u636e\u4ea7\u54c1\uff0c\u4ee5\u89e3\u51b3\u5f53\u524d\u788e\u7247\u5316\u751f\u6001\u7cfb\u7edf\u7684\u4e92\u64cd\u4f5c\u6027\u95ee\u9898\uff0c\u63d0\u5347\u5730\u7403\u89c2\u6d4b\u5de5\u4f5c\u6d41\u7684\u900f\u660e\u6027\u548c\u53ef\u8bbf\u95ee\u6027\u3002", "motivation": "\u73b0\u6709\u5730\u7406\u7a7a\u95f4\u57fa\u7840\u6a21\u578b\u7684\u9884\u8ba1\u7b97\u5d4c\u5165\u6570\u636e\u4ea7\u54c1\u56e0\u683c\u5f0f\u548c\u5206\u8fa8\u7387\u4e0d\u517c\u5bb9\uff0c\u5bfc\u81f4\u5de5\u7a0b\u74f6\u9888\uff0c\u963b\u788d\u6a21\u578b\u6bd4\u8f83\u4e0e\u53ef\u590d\u73b0\u6027\u3002", "method": "\u6784\u5efa\u4e09\u5c42\u5206\u7c7b\u4f53\u7cfb\uff08\u6570\u636e\u3001\u5de5\u5177\u3001\u4ef7\u503c\uff09\uff0c\u5e76\u6269\u5c55TorchGeo\u6846\u67b6\u63d0\u4f9b\u7edf\u4e00API\u52a0\u8f7d\u548c\u67e5\u8be2\u591a\u6837\u5316\u5d4c\u5165\u4ea7\u54c1\u3002", "result": "\u6210\u529f\u5c06\u5d4c\u5165\u89c6\u4e3a\u9996\u8981\u5730\u7406\u7a7a\u95f4\u6570\u636e\u96c6\uff0c\u5b9e\u73b0\u4e0b\u6e38\u5206\u6790\u4e0e\u6a21\u578b\u7279\u5b9a\u5de5\u7a0b\u7684\u89e3\u8026\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u66f4\u900f\u660e\u3001\u6613\u7528\u7684\u5730\u7403\u89c2\u6d4b\u5de5\u4f5c\u6d41\u7a0b\u63d0\u4f9b\u4e86\u53ef\u884c\u8def\u5f84\u3002"}}
{"id": "2601.13139", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2601.13139", "abs": "https://arxiv.org/abs/2601.13139", "authors": ["Alessandro Midolo", "Emiliano Tramontana", "Massimiliano Di Penta"], "title": "From Human to Machine Refactoring: Assessing GPT-4's Impact on Python Class Quality and Readability", "comment": null, "summary": "Refactoring is a software engineering practice that aims to improve code quality without altering program behavior. Although automated refactoring tools have been extensively studied, their practical applicability remains limited. Recent advances in Large Language Models (LLMs) have introduced new opportunities for automated code refactoring. The evaluation of such an LLM-driven approach, however, leaves unanswered questions about its effects on code quality. In this paper, we present a comprehensive empirical study on LLM-driven refactoring using GPT-4o, applied to 100 Python classes from the ClassEval benchmark. Unlike prior work, our study explores a wide range of class-level refactorings inspired by Fowler's catalog and evaluates their effects from three complementary perspectives: (i) behavioral correctness, verified through unit tests; (ii) code quality, assessed via Pylint, Flake8, and SonarCloud; and (iii) readability, measured using a state-of-the-art readability tool. Our findings show that GPT-4o generally produces behavior-preserving refactorings that reduce code smells and improve quality metrics, albeit at the cost of decreased readability. Our results provide new evidence on the capabilities and limitations of LLMs in automated software refactoring, highlighting directions for integrating LLMs into practical refactoring workflows.", "AI": {"tldr": "\u672c\u7814\u7a76\u8bc4\u4f30\u4e86GPT-4o\u5728Python\u7c7b\u91cd\u6784\u4e2d\u7684\u8868\u73b0\uff0c\u53d1\u73b0\u5176\u80fd\u6709\u6548\u4fdd\u6301\u884c\u4e3a\u6b63\u786e\u6027\u5e76\u63d0\u5347\u4ee3\u7801\u8d28\u91cf\uff0c\u4f46\u4f1a\u964d\u4f4e\u53ef\u8bfb\u6027\u3002", "motivation": "\u73b0\u6709LLM\u9a71\u52a8\u7684\u91cd\u6784\u5de5\u5177\u7f3a\u4e4f\u5bf9\u4ee3\u7801\u8d28\u91cf\u591a\u7ef4\u5ea6\u5f71\u54cd\u7684\u7cfb\u7edf\u8bc4\u4f30\u3002", "method": "\u57fa\u4e8eClassEval\u57fa\u51c6\u4e2d\u7684100\u4e2aPython\u7c7b\uff0c\u91c7\u7528Fowler\u91cd\u6784\u76ee\u5f55\uff0c\u4ece\u884c\u4e3a\u6b63\u786e\u6027\u3001\u4ee3\u7801\u8d28\u91cf\u548c\u53ef\u8bfb\u6027\u4e09\u65b9\u9762\u8bc4\u4f30GPT-4o\u91cd\u6784\u6548\u679c\u3002", "result": "GPT-4o\u80fd\u751f\u6210\u884c\u4e3a\u4fdd\u6301\u7684\u91cd\u6784\uff0c\u51cf\u5c11\u4ee3\u7801\u5f02\u5473\u5e76\u6539\u5584\u8d28\u91cf\u6307\u6807\uff0c\u4f46\u5bfc\u81f4\u53ef\u8bfb\u6027\u4e0b\u964d\u3002", "conclusion": "LLM\u5728\u81ea\u52a8\u5316\u91cd\u6784\u4e2d\u5177\u6709\u6f5c\u529b\uff0c\u4f46\u9700\u6743\u8861\u8d28\u91cf\u4e0e\u53ef\u8bfb\u6027\uff0c\u672a\u6765\u5e94\u4f18\u5316\u5176\u5728\u5b9e\u9645\u5de5\u4f5c\u6d41\u4e2d\u7684\u96c6\u6210\u3002"}}
{"id": "2601.13240", "categories": ["cs.SE", "cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.13240", "abs": "https://arxiv.org/abs/2601.13240", "authors": ["Xue Jiang", "Jiaru Qian", "Xianjie Shi", "Chenjie Li", "Hao Zhu", "Ziyu Wang", "Jielun Zhang", "Zheyu Zhao", "Kechi Zhang", "Jia Li", "Wenpin Jiao", "Zhi Jin", "Ge Li", "Yihong Dong"], "title": "KOCO-BENCH: Can Large Language Models Leverage Domain Knowledge in Software Development?", "comment": null, "summary": "Large language models (LLMs) excel at general programming but struggle with domain-specific software development, necessitating domain specialization methods for LLMs to learn and utilize domain knowledge and data. However, existing domain-specific code benchmarks cannot evaluate the effectiveness of domain specialization methods, which focus on assessing what knowledge LLMs possess rather than how they acquire and apply new knowledge, lacking explicit knowledge corpora for developing domain specialization methods. To this end, we present KOCO-BENCH, a novel benchmark designed for evaluating domain specialization methods in real-world software development. KOCO-BENCH contains 6 emerging domains with 11 software frameworks and 25 projects, featuring curated knowledge corpora alongside multi-granularity evaluation tasks including domain code generation (from function-level to project-level with rigorous test suites) and domain knowledge understanding (via multiple-choice Q&A). Unlike previous benchmarks that only provide test sets for direct evaluation, KOCO-BENCH requires acquiring and applying diverse domain knowledge (APIs, rules, constraints, etc.) from knowledge corpora to solve evaluation tasks. Our evaluations reveal that KOCO-BENCH poses significant challenges to state-of-the-art LLMs. Even with domain specialization methods (e.g., SFT, RAG, kNN-LM) applied, improvements remain marginal. Best-performing coding agent, Claude Code, achieves only 34.2%, highlighting the urgent need for more effective domain specialization methods. We release KOCO-BENCH, evaluation code, and baselines to advance further research at https://github.com/jiangxxxue/KOCO-bench.", "AI": {"tldr": "KOCO-BENCH\u662f\u4e00\u4e2a\u4e13\u4e3a\u8bc4\u4f30\u5927\u8bed\u8a00\u6a21\u578b\u9886\u57df\u7279\u5316\u65b9\u6cd5\u800c\u8bbe\u8ba1\u7684\u65b0\u57fa\u51c6\uff0c\u6db5\u76d66\u4e2a\u65b0\u5174\u9886\u57df\u300111\u4e2a\u6846\u67b6\u548c25\u4e2a\u9879\u76ee\uff0c\u5f3a\u8c03\u4ece\u77e5\u8bc6\u5e93\u4e2d\u83b7\u53d6\u5e76\u5e94\u7528\u9886\u57df\u77e5\u8bc6\u89e3\u51b3\u591a\u7c92\u5ea6\u7f16\u7a0b\u4efb\u52a1\u3002", "motivation": "\u73b0\u6709\u4ee3\u7801\u8bc4\u6d4b\u57fa\u51c6\u65e0\u6cd5\u6709\u6548\u8bc4\u4f30LLM\u9886\u57df\u7279\u5316\u65b9\u6cd5\uff0c\u56e0\u5176\u4ec5\u5173\u6ce8\u6a21\u578b\u5df2\u6709\u77e5\u8bc6\u800c\u975e\u5b66\u4e60\u4e0e\u5e94\u7528\u65b0\u77e5\u8bc6\u7684\u80fd\u529b\uff0c\u4e14\u7f3a\u4e4f\u914d\u5957\u77e5\u8bc6\u5e93\u3002", "method": "\u6784\u5efa\u5305\u542b\u7cbe\u9009\u77e5\u8bc6\u5e93\u7684KOCO-BENCH\u57fa\u51c6\uff0c\u8bbe\u7f6e\u4ece\u51fd\u6570\u7ea7\u5230\u9879\u76ee\u7ea7\u7684\u4ee3\u7801\u751f\u6210\u4efb\u52a1\u53ca\u591a\u9009\u95ee\u7b54\u5f62\u5f0f\u7684\u77e5\u8bc6\u7406\u89e3\u4efb\u52a1\uff0c\u5f3a\u5236\u6a21\u578b\u4ece\u77e5\u8bc6\u5e93\u4e2d\u63d0\u53d6API\u3001\u89c4\u5219\u7b49\u4fe1\u606f\u5b8c\u6210\u8bc4\u6d4b\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\u5f53\u524d\u6700\u5148\u8fdb\u6a21\u578b\uff08\u5982Claude Code\uff09\u5728\u8be5\u57fa\u51c6\u4e0a\u8868\u73b0\u4e0d\u4f73\uff0c\u5373\u4f7f\u7ed3\u5408SFT\u3001RAG\u7b49\u7279\u5316\u65b9\u6cd5\uff0c\u6700\u9ad8\u51c6\u786e\u7387\u4ec534.2%\uff0c\u51f8\u663e\u73b0\u6709\u65b9\u6cd5\u6548\u679c\u6709\u9650\u3002", "conclusion": "KOCO-BENCH\u63ed\u793a\u4e86\u5f53\u524d\u9886\u57df\u7279\u5316\u65b9\u6cd5\u7684\u4e0d\u8db3\uff0c\u4e3a\u540e\u7eed\u7814\u7a76\u63d0\u4f9b\u4e86\u516c\u5f00\u8bc4\u6d4b\u5e73\u53f0\u4e0e\u57fa\u7ebf\uff0c\u4e9f\u9700\u66f4\u6709\u6548\u7684\u9886\u57df\u77e5\u8bc6\u6ce8\u5165\u4e0e\u5229\u7528\u673a\u5236\u3002"}}
{"id": "2601.13655", "categories": ["cs.SE", "cs.AI", "cs.DC"], "pdf": "https://arxiv.org/pdf/2601.13655", "abs": "https://arxiv.org/abs/2601.13655", "authors": ["Guangba Yu", "Zirui Wang", "Yujie Huang", "Renyi Zhong", "Yuedong Zhong", "Yilun Wang", "Michael R. Lyu"], "title": "Why Does the LLM Stop Computing: An Empirical Study of User-Reported Failures in Open-Source LLMs", "comment": null, "summary": "The democratization of open-source Large Language Models (LLMs) allows users to fine-tune and deploy models on local infrastructure but exposes them to a First Mile deployment landscape. Unlike black-box API consumption, the reliability of user-managed orchestration remains a critical blind spot. To bridge this gap, we conduct the first large-scale empirical study of 705 real-world failures from the open-source DeepSeek, Llama, and Qwen ecosystems.\n  Our analysis reveals a paradigm shift: white-box orchestration relocates the reliability bottleneck from model algorithmic defects to the systemic fragility of the deployment stack. We identify three key phenomena: (1) Diagnostic Divergence: runtime crashes distinctively signal infrastructure friction, whereas incorrect functionality serves as a signature for internal tokenizer defects. (2) Systemic Homogeneity: Root causes converge across divergent series, confirming reliability barriers are inherent to the shared ecosystem rather than specific architectures. (3) Lifecycle Escalation: Barriers escalate from intrinsic configuration struggles during fine-tuning to compounded environmental incompatibilities during inference. Supported by our publicly available dataset, these insights provide actionable guidance for enhancing the reliability of the LLM landscape.", "AI": {"tldr": "\u672c\u6587\u9996\u6b21\u5927\u89c4\u6a21\u5b9e\u8bc1\u7814\u7a76\u5f00\u6e90\u5927\u8bed\u8a00\u6a21\u578b\u90e8\u7f72\u4e2d\u7684\u53ef\u9760\u6027\u95ee\u9898\uff0c\u63ed\u793a\u767d\u76d2\u7f16\u6392\u5c06\u74f6\u9888\u4ece\u7b97\u6cd5\u7f3a\u9677\u8f6c\u79fb\u81f3\u7cfb\u7edf\u8106\u5f31\u6027\uff0c\u5e76\u63d0\u51fa\u4e09\u9879\u5173\u952e\u53d1\u73b0\u4ee5\u6307\u5bfc\u6539\u8fdb\u3002", "motivation": "\u5f00\u6e90\u5927\u8bed\u8a00\u6a21\u578b\u7684\u672c\u5730\u90e8\u7f72\u5e26\u6765\u53ef\u9760\u6027\u76f2\u70b9\uff0c\u9700\u586b\u8865\u7528\u6237\u7ba1\u7406\u7f16\u6392\u53ef\u9760\u6027\u7684\u7814\u7a76\u7a7a\u767d\u3002", "method": "\u5206\u6790\u6765\u81eaDeepSeek\u3001Llama\u548cQwen\u751f\u6001\u7cfb\u7edf\u7684705\u4e2a\u771f\u5b9e\u6545\u969c\u6848\u4f8b\uff0c\u8fdb\u884c\u5927\u89c4\u6a21\u5b9e\u8bc1\u7814\u7a76\u3002", "result": "\u53d1\u73b0\u8bca\u65ad\u5206\u6b67\u3001\u7cfb\u7edf\u540c\u8d28\u6027\u548c\u751f\u547d\u5468\u671f\u5347\u7ea7\u4e09\u5927\u73b0\u8c61\uff0c\u8868\u660e\u53ef\u9760\u6027\u969c\u788d\u6e90\u4e8e\u5171\u4eab\u751f\u6001\u800c\u975e\u7279\u5b9a\u67b6\u6784\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u4e3a\u63d0\u5347\u5927\u8bed\u8a00\u6a21\u578b\u90e8\u7f72\u53ef\u9760\u6027\u63d0\u4f9b\u53ef\u64cd\u4f5c\u6307\u5bfc\uff0c\u5e76\u516c\u5f00\u6570\u636e\u96c6\u652f\u6301\u540e\u7eed\u7814\u7a76\u3002"}}
{"id": "2601.13772", "categories": ["cs.SE", "cs.DC", "cs.SI", "eess.SY"], "pdf": "https://arxiv.org/pdf/2601.13772", "abs": "https://arxiv.org/abs/2601.13772", "authors": ["Matteo Vaccargiu", "Azmat Ullah", "Pierluigi Gallo"], "title": "A Blockchain-Oriented Software Engineering Architecture for Carbon Credit Certification Systems", "comment": "2026 IEEE International Conference on Software Analysis, Evolution and Reengineering - Companion (SANER-C) 9th International Workshop on Blockchain Oriented Software Engineering March 17-20, 2026 Limassol, Cyprus", "summary": "Carbon credit systems have emerged as a policy tool to incentivize emission reductions and support the transition to clean energy. Reliable carbon-credit certification depends on mechanisms that connect actual, measured renewable-energy production to verifiable emission-reduction records. Although blockchain and IoT technologies have been applied to emission monitoring and trading, existing work offers limited support for certification processes, particularly for small and medium-scale renewable installations. This paper introduces a blockchain-based carbon-credit certification architecture, demonstrated through a 100 kWp photovoltaic case study, that integrates real-time IoT data collection, edge-level aggregation, and secure on-chain storage on a permissioned blockchain with smart contracts. Unlike approaches focused on trading mechanisms, the proposed system aligns with European legislation and voluntary carbon-market standards, clarifying the practical requirements and constraints that apply to photovoltaic operators. The resulting architecture provides a structured pathway for generating verifiable carbon-credit records and supporting third-party verification.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u533a\u5757\u94fe\u7684\u78b3\u4fe1\u7528\u8ba4\u8bc1\u67b6\u6784\uff0c\u7ed3\u5408\u7269\u8054\u7f51\u5b9e\u65f6\u6570\u636e\u4e0e\u667a\u80fd\u5408\u7ea6\uff0c\u652f\u6301\u4e2d\u5c0f\u578b\u5149\u4f0f\u9879\u76ee\u751f\u6210\u53ef\u9a8c\u8bc1\u78b3\u4fe1\u7528\u8bb0\u5f55\u3002", "motivation": "\u73b0\u6709\u6280\u672f\u5bf9\u4e2d\u5c0f\u578b\u53ef\u518d\u751f\u80fd\u6e90\u9879\u76ee\u7684\u78b3\u4fe1\u7528\u8ba4\u8bc1\u652f\u6301\u4e0d\u8db3\uff0c\u9700\u7b26\u5408\u6b27\u6d32\u6cd5\u89c4\u4e0e\u81ea\u613f\u78b3\u5e02\u573a\u6807\u51c6\u3002", "method": "\u6574\u5408\u7269\u8054\u7f51\u8fb9\u7f18\u6570\u636e\u91c7\u96c6\u3001\u8bb8\u53ef\u94fe\u4e0a\u5b58\u50a8\u4e0e\u667a\u80fd\u5408\u7ea6\uff0c\u4ee5100 kWp\u5149\u4f0f\u6848\u4f8b\u9a8c\u8bc1\u67b6\u6784\u53ef\u884c\u6027\u3002", "result": "\u7cfb\u7edf\u80fd\u751f\u6210\u7ed3\u6784\u5316\u3001\u53ef\u5ba1\u8ba1\u7684\u78b3\u4fe1\u7528\u8bb0\u5f55\uff0c\u652f\u6301\u7b2c\u4e09\u65b9\u9a8c\u8bc1\u5e76\u6ee1\u8db3\u5408\u89c4\u8981\u6c42\u3002", "conclusion": "\u8be5\u67b6\u6784\u4e3a\u4e2d\u5c0f\u578b\u6e05\u6d01\u80fd\u6e90\u9879\u76ee\u63d0\u4f9b\u4e86\u5b9e\u7528\u3001\u5408\u89c4\u7684\u78b3\u4fe1\u7528\u8ba4\u8bc1\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2601.13384", "categories": ["cs.SE", "cs.CL"], "pdf": "https://arxiv.org/pdf/2601.13384", "abs": "https://arxiv.org/abs/2601.13384", "authors": ["Jiajun Zhang", "Zeyu Cui", "Jiaxi Yang", "Lei Zhang", "Yuheng Jing", "Zeyao Ma", "Tianyi Bai", "Zilei Wang", "Qiang Liu", "Liang Wang", "Binyuan Hui", "Junyang Lin"], "title": "From Completion to Editing: Unlocking Context-Aware Code Infilling via Search-and-Replace Instruction Tuning", "comment": null, "summary": "The dominant Fill-in-the-Middle (FIM) paradigm for code completion is constrained by its rigid inability to correct contextual errors and reliance on unaligned, insecure Base models. While Chat LLMs offer safety and Agentic workflows provide flexibility, they suffer from performance degradation and prohibitive latency, respectively. To resolve this dilemma, we propose Search-and-Replace Infilling (SRI), a framework that internalizes the agentic verification-and-editing mechanism into a unified, single-pass inference process. By structurally grounding edits via an explicit search phase, SRI harmonizes completion tasks with the instruction-following priors of Chat LLMs, extending the paradigm from static infilling to dynamic context-aware editing. We synthesize a high-quality dataset, SRI-200K, and fine-tune the SRI-Coder series. Extensive evaluations demonstrate that with minimal data (20k samples), SRI-Coder enables Chat models to surpass the completion performance of their Base counterparts. Crucially, unlike FIM-style tuning, SRI preserves general coding competencies and maintains inference latency comparable to standard FIM. We empower the entire Qwen3-Coder series with SRI, encouraging the developer community to leverage this framework for advanced auto-completion and assisted development.", "AI": {"tldr": "\u63d0\u51faSRI\u6846\u67b6\uff0c\u901a\u8fc7\u5355\u6b21\u63a8\u7406\u5b9e\u73b0\u52a8\u6001\u4e0a\u4e0b\u6587\u7f16\u8f91\uff0c\u63d0\u5347\u4ee3\u7801\u8865\u5168\u6027\u80fd\u5e76\u4fdd\u6301\u4f4e\u5ef6\u8fdf\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709FIM\u8303\u5f0f\u65e0\u6cd5\u7ea0\u9519\u3001\u4f9d\u8d56\u4e0d\u5b89\u5168\u57fa\u7840\u6a21\u578b\uff0c\u4ee5\u53caChat LLM\u4e0eAgentic\u5de5\u4f5c\u6d41\u5b58\u5728\u6027\u80fd\u6216\u5ef6\u8fdf\u95ee\u9898\u3002", "method": "\u8bbe\u8ba1Search-and-Replace Infilling\u6846\u67b6\uff0c\u7ed3\u5408\u663e\u5f0f\u641c\u7d22\u9636\u6bb5\u4e0e\u6307\u4ee4\u8ddf\u968f\u5148\u9a8c\uff0c\u6784\u5efaSRI-200K\u6570\u636e\u96c6\u5e76\u5fae\u8c03SRI-Coder\u7cfb\u5217\u6a21\u578b\u3002", "result": "\u4ec5\u752820k\u6837\u672c\uff0cSRI-Coder\u4f7fChat\u6a21\u578b\u8d85\u8d8aBase\u6a21\u578b\u8865\u5168\u6027\u80fd\uff0c\u540c\u65f6\u4fdd\u7559\u901a\u7528\u7f16\u7801\u80fd\u529b\u4e14\u5ef6\u8fdf\u63a5\u8fd1\u6807\u51c6FIM\u3002", "conclusion": "SRI\u6846\u67b6\u6709\u6548\u7edf\u4e00\u4e86\u5b89\u5168\u6027\u3001\u7075\u6d3b\u6027\u4e0e\u9ad8\u6548\u6027\uff0c\u9002\u7528\u4e8eQwen3-Coder\u7cfb\u5217\uff0c\u63a8\u52a8\u9ad8\u7ea7\u81ea\u52a8\u8865\u5168\u4e0e\u8f85\u52a9\u5f00\u53d1\u3002"}}
{"id": "2601.13460", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2601.13460", "abs": "https://arxiv.org/abs/2601.13460", "authors": ["Alexandra Gonz\u00e1lez", "Oscar Cerezo", "Xavier Franch", "Silverio Mart\u00ednez-Fern\u00e1ndez"], "title": "A Tool for Automatically Cataloguing and Selecting Pre-Trained Models and Datasets for Software Engineering", "comment": null, "summary": "The rapid growth of machine learning assets has made it increasingly difficult for software engineers to identify models and datasets that match their specific needs. Browsing large registries, such as Hugging Face, is time-consuming, error-prone, and rarely tailored to Software Engineering (SE) tasks. We present MLAssetSelection, a web application that automatically extracts SE assets and supports four key functionalities: (i) a configurable leaderboard for ranking models across multiple benchmarks and metrics; (ii) requirements-based selection of models and datasets; (iii) real-time automated updates through scheduled jobs that keep asset information current; and (iv) user-centric features including login, personalized asset lists, and configurable alert notifications. A demonstration video is available at https://youtu.be/t6CJ6P9asV4.", "AI": {"tldr": "MLAssetSelection\u662f\u4e00\u4e2a\u5e2e\u52a9\u8f6f\u4ef6\u5de5\u7a0b\u5e08\u81ea\u52a8\u7b5b\u9009\u548c\u7ba1\u7406\u673a\u5668\u5b66\u4e60\u8d44\u4ea7\u7684Web\u5e94\u7528\uff0c\u652f\u6301\u6392\u884c\u699c\u3001\u9700\u6c42\u5339\u914d\u3001\u5b9e\u65f6\u66f4\u65b0\u4e0e\u4e2a\u6027\u5316\u529f\u80fd\u3002", "motivation": "\u89e3\u51b3\u5728\u5927\u578b\u6ce8\u518c\u8868\u4e2d\u624b\u52a8\u67e5\u627e\u9002\u5408\u8f6f\u4ef6\u5de5\u7a0b\u4efb\u52a1\u7684\u6a21\u578b\u548c\u6570\u636e\u96c6\u6548\u7387\u4f4e\u3001\u6613\u51fa\u9519\u7684\u95ee\u9898\u3002", "method": "\u5f00\u53d1\u5177\u5907\u6392\u884c\u699c\u3001\u9700\u6c42\u7b5b\u9009\u3001\u5b9a\u65f6\u66f4\u65b0\u548c\u7528\u6237\u4e2a\u6027\u5316\u529f\u80fd\u7684Web\u5e94\u7528\u7a0b\u5e8f\u3002", "result": "\u63d0\u4f9b\u9ad8\u6548\u3001\u7cbe\u51c6\u3001\u81ea\u52a8\u5316\u7684\u673a\u5668\u5b66\u4e60\u8d44\u4ea7\u7b5b\u9009\u4e0e\u7ba1\u7406\u65b9\u6848\uff0c\u5e76\u9644\u6f14\u793a\u89c6\u9891\u3002", "conclusion": "MLAssetSelection\u6709\u6548\u63d0\u5347\u8f6f\u4ef6\u5de5\u7a0b\u5e08\u5728\u6d77\u91cf\u673a\u5668\u5b66\u4e60\u8d44\u6e90\u4e2d\u5b9a\u4f4d\u5408\u9002\u8d44\u4ea7\u7684\u6548\u7387\u4e0e\u51c6\u786e\u6027\u3002"}}
{"id": "2601.13466", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2601.13466", "abs": "https://arxiv.org/abs/2601.13466", "authors": ["Pedro Oliveira", "Doris Amoakohene", "Toby Hocking", "Marco Gerosa", "Igor Steinmacher"], "title": "Governance Matters: Lessons from Restructuring the data.table OSS Project", "comment": "ICSME 2025", "summary": "Open source software (OSS) forms the backbone of industrial data workflows and enterprise systems. However, many OSS projects face operational risks due to informal or centralized governance. This paper presents a practical case study of data.table, a high-performance R package widely adopted in production analytics pipelines, which underwent a community-led governance reform to address scalability and sustainability concerns. Before the reform, data.table faced a growing backlog of unresolved issues and open pull requests, unclear contributor pathways, and bottlenecks caused by reliance on a single core maintainer. In response, the community initiated a redesign of its governance structure. In this paper, we evaluated the impact of this transition through a mixed-methods approach, combining a contributor survey (n=17) with mining project repository data. Our results show that following the reform, the project experienced a 200% increase in new contributor recruitment, a drop in pull request resolution time from over 700 days to under a week, and a 3x increase in contributor retention. Community sentiment improved around transparency, onboarding, and project momentum, though concerns around fairness and conflict resolution remain. This case study provides practical guidance for maintainers, companies, and foundations seeking to enhance OSS governance.", "AI": {"tldr": "\u672c\u7814\u7a76\u901a\u8fc7\u6df7\u5408\u65b9\u6cd5\u8bc4\u4f30\u4e86data.table\u9879\u76ee\u6cbb\u7406\u6539\u9769\u7684\u5f71\u54cd\uff0c\u53d1\u73b0\u6539\u9769\u540e\u65b0\u8d21\u732e\u8005\u589e\u957f200%\uff0cPR\u89e3\u51b3\u65f6\u95f4\u4ece700\u5929\u7f29\u77ed\u81f3\u4e00\u5468\u5185\uff0c\u8d21\u732e\u8005\u7559\u5b58\u7387\u63d0\u53473\u500d\u3002", "motivation": "\u5f00\u6e90\u9879\u76ee\u5e38\u56e0\u975e\u6b63\u5f0f\u6216\u96c6\u4e2d\u5f0f\u6cbb\u7406\u9762\u4e34\u8fd0\u8425\u98ce\u9669\uff0cdata.table\u9879\u76ee\u4e3a\u89e3\u51b3\u53ef\u6269\u5c55\u6027\u4e0e\u53ef\u6301\u7eed\u6027\u95ee\u9898\u542f\u52a8\u6cbb\u7406\u6539\u9769\u3002", "method": "\u7ed3\u540817\u540d\u8d21\u732e\u8005\u7684\u95ee\u5377\u8c03\u67e5\u4e0e\u9879\u76ee\u4ed3\u5e93\u6570\u636e\u6316\u6398\u8fdb\u884c\u6df7\u5408\u65b9\u6cd5\u8bc4\u4f30\u3002", "result": "\u6539\u9769\u540e\u65b0\u8d21\u732e\u8005\u589e\u52a0200%\uff0cPR\u89e3\u51b3\u65f6\u95f4\u5927\u5e45\u7f29\u77ed\uff0c\u8d21\u732e\u8005\u7559\u5b58\u7387\u63d0\u9ad83\u500d\uff0c\u793e\u533a\u900f\u660e\u5ea6\u548c\u58eb\u6c14\u6539\u5584\u3002", "conclusion": "\u8be5\u6848\u4f8b\u4e3a\u5f00\u6e90\u9879\u76ee\u7ef4\u62a4\u8005\u3001\u4f01\u4e1a\u53ca\u57fa\u91d1\u4f1a\u63d0\u4f9b\u4e86\u589e\u5f3a\u6cbb\u7406\u7ed3\u6784\u7684\u5b9e\u7528\u6307\u5bfc\uff0c\u5c3d\u7ba1\u516c\u5e73\u6027\u4e0e\u51b2\u7a81\u89e3\u51b3\u4ecd\u9700\u6539\u8fdb\u3002"}}
{"id": "2601.13682", "categories": ["cs.SE", "cs.PL"], "pdf": "https://arxiv.org/pdf/2601.13682", "abs": "https://arxiv.org/abs/2601.13682", "authors": ["Jianfeng Cai", "Jinhua Zhu", "Ruopei Sun", "Kangwen Zhao", "Dongyun Xue", "Mingxiao Feng", "Wengang Zhou", "Houqiang Li"], "title": "CodeContests-O: Powering LLMs via Feedback-Driven Iterative Test Case Generation", "comment": null, "summary": "The rise of reasoning models necessitates large-scale verifiable data, for which programming tasks serve as an ideal source. However, while competitive programming platforms provide abundant problems and solutions, high-quality test cases for verification remain scarce. Existing approaches attempt to synthesize test cases using Large Language Models (LLMs), but rely solely on the model's intrinsic generation capabilities without external feedback, frequently resulting in insufficiently diverse cases. To address this limitation, we propose a $\\textbf{Feedback-Driven Iterative Framework}$ for comprehensive test case construction. Specifically, our method leverages the LLM to generate initial test cases, executes them against known correct and incorrect solutions, and utilizes the failed results as feedback to guide the LLM in refining the test cases toward high fidelity and discriminability. We then apply this method to the CodeContests dataset to construct an optimized high-quality derivative, $\\textbf{CodeContests-O}$. Evaluating against the entire pool of solutions ($1.1 \\times 10^7$ in total), our dataset achieves an average True Positive Rate (TPR) of $89.37\\%$ and True Negative Rate (TNR) of $90.89\\%$, significantly outperforming the CodeContests and CodeContests+ by margins of $4.32\\%$ and $9.37\\%$, respectively. Furthermore, fine-tuning the Qwen2.5-7B model on CodeContests-O results in a $9.52\\%$ improvement on LiveCodeBench (Pass@1). Experiments demonstrate the effectiveness of our framework and the quality of CodeContests-O. To support reproducibility and facilitate future research, we release the $\\href{https://github.com/cai-jianfeng/CodeContests-O}{code}$ and $\\href{https://huggingface.co/datasets/caijanfeng/CodeContests-O}{dataset}$.", "AI": {"tldr": "\u63d0\u51fa\u53cd\u9988\u9a71\u52a8\u7684\u8fed\u4ee3\u6846\u67b6\u751f\u6210\u9ad8\u8d28\u91cf\u6d4b\u8bd5\u7528\u4f8b\uff0c\u6784\u5efaCodeContests-O\u6570\u636e\u96c6\uff0c\u663e\u8457\u63d0\u5347\u6a21\u578b\u8bc4\u4f30\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7f16\u7a0b\u9898\u6d4b\u8bd5\u7528\u4f8b\u7a00\u7f3a\u4e14LLM\u751f\u6210\u591a\u6837\u6027\u4e0d\u8db3\uff0c\u9700\u5f15\u5165\u5916\u90e8\u53cd\u9988\u63d0\u5347\u8d28\u91cf\u3002", "method": "\u5229\u7528LLM\u751f\u6210\u521d\u59cb\u7528\u4f8b\uff0c\u901a\u8fc7\u6267\u884c\u6b63\u8bef\u89e3\u6cd5\u83b7\u5f97\u53cd\u9988\uff0c\u8fed\u4ee3\u4f18\u5316\u751f\u6210\u9ad8\u533a\u5206\u5ea6\u6d4b\u8bd5\u7528\u4f8b\u3002", "result": "CodeContests-O\u5728\u5343\u4e07\u7ea7\u89e3\u6cd5\u6c60\u4e0aTPR\u8fbe89.37%\uff0cTNR\u8fbe90.89%\uff0c\u4f18\u4e8e\u57fa\u7ebf4.32%\u548c9.37%\uff1b\u5fae\u8c03Qwen2.5-7B\u5728LiveCodeBench\u63d0\u53479.52% Pass@1\u3002", "conclusion": "\u8be5\u6846\u67b6\u6709\u6548\u63d0\u5347\u6d4b\u8bd5\u7528\u4f8b\u8d28\u91cf\uff0c\u6240\u6784\u5efa\u6570\u636e\u96c6\u663e\u8457\u589e\u5f3a\u6a21\u578b\u8bad\u7ec3\u4e0e\u8bc4\u4f30\u6548\u679c\uff0c\u5e76\u5f00\u6e90\u4ee3\u7801\u4e0e\u6570\u636e\u4fc3\u8fdb\u540e\u7eed\u7814\u7a76\u3002"}}
{"id": "2601.13713", "categories": ["cs.SE", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.13713", "abs": "https://arxiv.org/abs/2601.13713", "authors": ["Aditya Bharat Soni", "Rajat Ghosh", "Vaishnavi Bhargava", "Valerie Chen", "Debojyoti Dutta"], "title": "SWE-Tester: Training Open-Source LLMs for Issue Reproduction in Real-World Repositories", "comment": null, "summary": "Software testing is crucial for ensuring the correctness and reliability of software systems. Automated generation of issue reproduction tests from natural language issue descriptions enhances developer productivity by simplifying root cause analysis, promotes test-driven development -- \"test first, write code later\", and can be used for improving the effectiveness of automated issue resolution systems like coding agents. Existing methods proposed for this task predominantly rely on closed-source LLMs, with limited exploration of open models. To address this, we propose SWE-Tester -- a novel pipeline for training open-source LLMs to generate issue reproduction tests. First, we curate a high-quality training dataset of 41K instances from 2.6K open-source GitHub repositories and use it to train LLMs of varying sizes and families. The fine-tuned models achieve absolute improvements of up to 10\\% in success rate and 21\\% in change coverage on SWT-Bench Verified. Further analysis shows consistent improvements with increased inference-time compute, more data, and larger models. These results highlight the effectiveness of our framework for advancing open-source LLMs in this domain.", "AI": {"tldr": "SWE-Tester\u662f\u4e00\u4e2a\u7528\u4e8e\u8bad\u7ec3\u5f00\u6e90\u5927\u8bed\u8a00\u6a21\u578b\u81ea\u52a8\u751f\u6210\u95ee\u9898\u590d\u73b0\u6d4b\u8bd5\u7684\u65b0\u6846\u67b6\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6210\u529f\u7387\u4e0e\u53d8\u66f4\u8986\u76d6\u7387\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u591a\u4f9d\u8d56\u95ed\u6e90\u5927\u6a21\u578b\uff0c\u7f3a\u4e4f\u5bf9\u5f00\u6e90\u6a21\u578b\u7684\u63a2\u7d22\uff0c\u963b\u788d\u4e86\u5f00\u53d1\u8005\u6548\u7387\u548c\u81ea\u52a8\u5316\u4fee\u590d\u7cfb\u7edf\u7684\u8fdb\u6b65\u3002", "method": "\u6784\u5efa\u5305\u542b4.1\u4e07\u6837\u672c\u7684\u9ad8\u8d28\u91cf\u6570\u636e\u96c6\uff0c\u57fa\u4e8e\u591a\u4e2a\u5f00\u6e90GitHub\u4ed3\u5e93\u8bad\u7ec3\u4e0d\u540c\u89c4\u6a21\u548c\u67b6\u6784\u7684LLM\u3002", "result": "\u5fae\u8c03\u6a21\u578b\u5728SWT-Bench Verified\u4e0a\u6210\u529f\u7387\u63d0\u5347\u6700\u9ad8\u8fbe10%\uff0c\u53d8\u66f4\u8986\u76d6\u7387\u63d0\u534721%\uff0c\u4e14\u968f\u8ba1\u7b97\u8d44\u6e90\u3001\u6570\u636e\u91cf\u548c\u6a21\u578b\u89c4\u6a21\u589e\u52a0\u6301\u7eed\u6539\u5584\u3002", "conclusion": "SWE-Tester\u6709\u6548\u63a8\u52a8\u4e86\u5f00\u6e90\u5927\u6a21\u578b\u5728\u81ea\u52a8\u751f\u6210\u590d\u73b0\u6d4b\u8bd5\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\uff0c\u5177\u5907\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2601.13743", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2601.13743", "abs": "https://arxiv.org/abs/2601.13743", "authors": ["Zhenya Zhang", "Parv Kapoor", "Jie An", "Eunsuk Kang"], "title": "Counterexample Classification against Signal Temporal Logic Specifications", "comment": null, "summary": "Signal Temporal Logic (STL) has been widely adopted as a specification language for specifying desirable behaviors of hybrid systems. By monitoring a given STL specification, we can detect the executions that violate it, which are often referred to as counterexamples. In practice, these counterexamples may arise from different causes and thus are relevant to different system defects. To effectively address this, we need a proper criterion for classifying these counterexamples, by which we can comprehend the possible violation patterns and the distributions of these counterexamples with respect to the patterns. In this paper, we propose a classification criterion by using parametric signal temporal logic (PSTL) to represent each class. Due to this formalism, identifying the classes of a counterexample requires finding proper parameter values of PSTL that enable a class to include the counterexample. To improve the efficiency of class identification, we further derive an inclusion relation between different classes, and then propose a binary search-like approach over it that significantly prunes the classes needed to query. We implement a prototype tool and experimentally evaluate its effectiveness on two widely-studied systems.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u53c2\u6570\u5316\u4fe1\u53f7\u65f6\u5e8f\u903b\u8f91\uff08PSTL\uff09\u7684\u53cd\u4f8b\u5206\u7c7b\u65b9\u6cd5\uff0c\u901a\u8fc7\u5efa\u7acb\u7c7b\u95f4\u5305\u542b\u5173\u7cfb\u5e76\u91c7\u7528\u7c7b\u4e8c\u5206\u641c\u7d22\u7b56\u7565\uff0c\u663e\u8457\u63d0\u5347\u5206\u7c7b\u6548\u7387\u3002", "motivation": "\u4e3a\u6709\u6548\u7406\u89e3\u6df7\u5408\u7cfb\u7edf\u4e2d\u8fdd\u53cdSTL\u89c4\u8303\u7684\u53cd\u4f8b\u6a21\u5f0f\u53ca\u5176\u5206\u5e03\uff0c\u9700\u5efa\u7acb\u5408\u7406\u7684\u5206\u7c7b\u6807\u51c6\u4ee5\u8f85\u52a9\u7f3a\u9677\u5b9a\u4f4d\u3002", "method": "\u5229\u7528PSTL\u5f62\u5f0f\u5316\u8868\u793a\u5404\u7c7b\u522b\uff0c\u901a\u8fc7\u53c2\u6570\u5339\u914d\u8bc6\u522b\u53cd\u4f8b\u6240\u5c5e\u7c7b\u522b\uff0c\u5e76\u6784\u5efa\u7c7b\u95f4\u5305\u542b\u5173\u7cfb\u4ee5\u652f\u6301\u9ad8\u6548\u4e8c\u5206\u641c\u7d22\u5f0f\u67e5\u8be2\u526a\u679d\u3002", "result": "\u539f\u578b\u5de5\u5177\u5728\u4e24\u4e2a\u5178\u578b\u7cfb\u7edf\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u6240\u63d0\u65b9\u6cd5\u80fd\u6709\u6548\u63d0\u5347\u53cd\u4f8b\u5206\u7c7b\u6548\u7387\u3002", "conclusion": "\u57fa\u4e8ePSTL\u7684\u5206\u7c7b\u65b9\u6cd5\u7ed3\u5408\u7c7b\u95f4\u5305\u542b\u5173\u7cfb\u4e0e\u641c\u7d22\u4f18\u5316\uff0c\u4e3a\u53cd\u4f8b\u5206\u6790\u63d0\u4f9b\u4e86\u9ad8\u6548\u4e14\u53ef\u89e3\u91ca\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2601.13754", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2601.13754", "abs": "https://arxiv.org/abs/2601.13754", "authors": ["Haoyu Gao", "Peerachai Banyongrakkul", "Hao Guan", "Mansooreh Zahedi", "Christoph Treude"], "title": "On Autopilot? An Empirical Study of Human-AI Teaming and Review Practices in Open Source", "comment": "accepted as MSR short paper", "summary": "Large Language Models (LLMs) increasingly automate software engineering tasks. While recent studies highlight the accelerated adoption of ``AI as a teammate'' in Open Source Software (OSS), developer interaction patterns remain under-explored. In this work, we investigated project-level guidelines and developers' interactions with AI-assisted pull requests (PRs) by expanding the AIDev dataset to include finer-grained contributor code ownership and a comparative baseline of human-created PRs. We found that over 67.5\\% of AI-co-authored PRs originate from contributors without prior code ownership. Despite this, the majority of repositories lack guidelines for AI-coding agent usage. Notably, we observed a distinct interaction pattern: AI-co-authored PRs are merged significantly faster with minimal feedback. In contrast to human-created PRs where non-owner developers receive the most feedback, AI-co-authored PRs from non-owners receive the least, with approximately 80\\% merged without any explicit review. Finally, we discuss implications for developers and researchers.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0AI\u534f\u4f5c\u7684PR\u7531\u65e0\u4ee3\u7801\u6240\u6709\u6743\u7684\u8d21\u732e\u8005\u53d1\u8d77\u5c45\u591a\uff0c\u4e14\u5408\u5e76\u901f\u5ea6\u66f4\u5feb\u3001\u53cd\u9988\u66f4\u5c11\uff0c\u4f46\u591a\u6570\u9879\u76ee\u7f3a\u4e4f\u76f8\u5173\u6307\u5357\u3002", "motivation": "\u63a2\u7d22\u5f00\u53d1\u8005\u4e0eAI\u534f\u4f5cPR\u7684\u4ea4\u4e92\u6a21\u5f0f\u53ca\u9879\u76ee\u7ea7\u6307\u5bfc\u7f3a\u5931\u95ee\u9898\u3002", "method": "\u6269\u5c55AIDev\u6570\u636e\u96c6\uff0c\u5206\u6790AI\u534f\u4f5cPR\u4e0e\u4eba\u5de5PR\u7684\u5bf9\u6bd4\u6570\u636e\u3002", "result": "67.5%\u7684AI\u534f\u4f5cPR\u6765\u81ea\u65e0\u4ee3\u7801\u6240\u6709\u6743\u8d21\u732e\u8005\uff1b80%\u65e0\u5ba1\u67e5\u76f4\u63a5\u5408\u5e76\uff1b\u9879\u76ee\u666e\u904d\u7f3a\u4e4fAI\u4f7f\u7528\u6307\u5357\u3002", "conclusion": "\u9700\u4e3a\u5f00\u53d1\u8005\u548c\u7814\u7a76\u4eba\u5458\u5236\u5b9aAI\u534f\u4f5c\u5f00\u53d1\u7684\u89c4\u8303\u4e0e\u6700\u4f73\u5b9e\u8df5\u3002"}}
{"id": "2601.13943", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2601.13943", "abs": "https://arxiv.org/abs/2601.13943", "authors": ["Zhiyuan Peng", "Xin Yin", "Pu Zhao", "Fangkai Yang", "Lu Wang", "Ran Jia", "Xu Chen", "Qingwei Lin", "Saravan Rajmohan", "Dongmei Zhang"], "title": "RepoGenesis: Benchmarking End-to-End Microservice Generation from Readme to Repository", "comment": null, "summary": "Large language models and agents have achieved remarkable progress in code generation. However, existing benchmarks focus on isolated function/class-level generation (e.g., ClassEval) or modifications to existing codebases (e.g., SWE-Bench), neglecting complete microservice repository generation that reflects real-world 0-to-1 development workflows. To bridge this gap, we introduce RepoGenesis, the first multilingual benchmark for repository-level end-to-end web microservice generation, comprising 106 repositories (60 Python, 46 Java) across 18 domains and 11 frameworks, with 1,258 API endpoints and 2,335 test cases verified through a \"review-rebuttal\" quality assurance process. We evaluate open-source agents (e.g., DeepCode) and commercial IDEs (e.g., Cursor) using Pass@1, API Coverage (AC), and Deployment Success Rate (DSR). Results reveal that despite high AC (up to 73.91%) and DSR (up to 100%), the best-performing system achieves only 23.67% Pass@1 on Python and 21.45% on Java, exposing deficiencies in architectural coherence, dependency management, and cross-file consistency. Notably, GenesisAgent-8B, fine-tuned on RepoGenesis (train), achieves performance comparable to GPT-5 mini, demonstrating the quality of RepoGenesis for advancing microservice generation. We release our benchmark at https://github.com/pzy2000/RepoGenesis.", "AI": {"tldr": "\u63d0\u51fa\u9996\u4e2a\u9762\u5411\u5fae\u670d\u52a1\u4ed3\u5e93\u7ea7\u751f\u6210\u7684\u591a\u8bed\u8a00\u57fa\u51c6RepoGenesis\uff0c\u8bc4\u4f30\u73b0\u6709\u6a21\u578b\u5728\u771f\u5b9e\u5f00\u53d1\u573a\u666f\u4e2d\u7684\u4e0d\u8db3\uff0c\u5e76\u53d1\u5e03\u9ad8\u8d28\u91cf\u6570\u636e\u96c6\u63a8\u52a8\u7814\u7a76\u3002", "motivation": "\u73b0\u6709\u4ee3\u7801\u751f\u6210\u8bc4\u6d4b\u96c6\u4e2d\u4e8e\u51fd\u6570/\u7c7b\u7ea7\u522b\u6216\u4fee\u6539\u73b0\u6709\u4ee3\u7801\uff0c\u7f3a\u4e4f\u53cd\u6620\u771f\u5b9e0\u52301\u5f00\u53d1\u6d41\u7a0b\u7684\u5b8c\u6574\u5fae\u670d\u52a1\u4ed3\u5e93\u751f\u6210\u8bc4\u6d4b\u3002", "method": "\u6784\u5efa\u5305\u542b106\u4e2a\u4ed3\u5e93\u3001\u8986\u76d618\u9886\u57df\u548c11\u6846\u67b6\u7684\u591a\u8bed\u8a00\u57fa\u51c6RepoGenesis\uff0c\u901a\u8fc7\u201c\u8bc4\u5ba1-\u53cd\u9a73\u201d\u6d41\u7a0b\u786e\u4fdd\u8d28\u91cf\uff0c\u5e76\u91c7\u7528Pass@1\u3001API\u8986\u76d6\u7387\u548c\u90e8\u7f72\u6210\u529f\u7387\u8bc4\u4f30\u6a21\u578b\u6027\u80fd\u3002", "result": "\u6700\u4f73\u7cfb\u7edf\u5728Python\u548cJava\u4e0a\u7684Pass@1\u4ec5\u5206\u522b\u4e3a23.67%\u548c21.45%\uff0c\u66b4\u9732\u67b6\u6784\u4e00\u81f4\u6027\u3001\u4f9d\u8d56\u7ba1\u7406\u7b49\u95ee\u9898\uff1b\u5fae\u8c03\u540e\u7684GenesisAgent-8B\u8868\u73b0\u63a5\u8fd1GPT-5 mini\u3002", "conclusion": "RepoGenesis\u586b\u8865\u4e86\u4ed3\u5e93\u7ea7\u5fae\u670d\u52a1\u751f\u6210\u8bc4\u6d4b\u7a7a\u767d\uff0c\u63ed\u793a\u5f53\u524d\u6a21\u578b\u77ed\u677f\uff0c\u5176\u9ad8\u8d28\u91cf\u6570\u636e\u6709\u52a9\u4e8e\u63a8\u52a8\u8be5\u9886\u57df\u53d1\u5c55\u3002"}}
{"id": "2601.13996", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2601.13996", "abs": "https://arxiv.org/abs/2601.13996", "authors": ["Rui Abreu", "Shaukat Ali", "Paolo Arcaini", "Jose Campos", "Michael Felderer", "Claude Gravel", "Fuyuki Ishikawa", "Stefan Klikovits", "Andriy Miranskyy", "Mohammad Mousavi", "Masaomi Yamaguchi", "Lei Zhang", "Jianjun Zhao", "Anila Mjeda"], "title": "Software Testing in the Quantum World", "comment": null, "summary": "Quantum computing offers significant speedups for simulating physical, chemical, and biological systems, and for optimization and machine learning. As quantum software grows in complexity, the classical simulation of quantum computers, which has long been essential for quality assurance, becomes infeasible. This shift requires new quality-assurance methods that operate directly on real quantum computers. This paper presents the key challenges in testing large-scale quantum software and offers software engineering perspectives for addressing them.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u4e86\u5728\u91cf\u5b50\u8f6f\u4ef6\u590d\u6742\u6027\u589e\u52a0\u7684\u80cc\u666f\u4e0b\uff0c\u5982\u4f55\u5728\u771f\u5b9e\u91cf\u5b50\u8ba1\u7b97\u673a\u4e0a\u8fdb\u884c\u8d28\u91cf\u4fdd\u8bc1\u7684\u65b0\u65b9\u6cd5\u3002", "motivation": "\u968f\u7740\u91cf\u5b50\u8f6f\u4ef6\u590d\u6742\u6027\u7684\u589e\u957f\uff0c\u4f20\u7edf\u7684\u7ecf\u5178\u6a21\u62df\u65b9\u6cd5\u5df2\u65e0\u6cd5\u6ee1\u8db3\u8d28\u91cf\u4fdd\u8bc1\u9700\u6c42\uff0c\u4e9f\u9700\u65b0\u7684\u6d4b\u8bd5\u65b9\u6cd5\u3002", "method": "\u4ece\u8f6f\u4ef6\u5de5\u7a0b\u7684\u89d2\u5ea6\u5206\u6790\u5927\u89c4\u6a21\u91cf\u5b50\u8f6f\u4ef6\u6d4b\u8bd5\u7684\u5173\u952e\u6311\u6218\u5e76\u63d0\u51fa\u5e94\u5bf9\u7b56\u7565\u3002", "result": "\u660e\u786e\u4e86\u5f53\u524d\u91cf\u5b50\u8f6f\u4ef6\u6d4b\u8bd5\u7684\u4e3b\u8981\u96be\u70b9\uff0c\u5e76\u63d0\u4f9b\u4e86\u53ef\u884c\u7684\u5de5\u7a0b\u5316\u89e3\u51b3\u601d\u8def\u3002", "conclusion": "\u4e3a\u672a\u6765\u5728\u771f\u5b9e\u91cf\u5b50\u786c\u4ef6\u4e0a\u5b9e\u73b0\u9ad8\u6548\u3001\u53ef\u9760\u7684\u91cf\u5b50\u8f6f\u4ef6\u8d28\u91cf\u4fdd\u969c\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2601.14034", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2601.14034", "abs": "https://arxiv.org/abs/2601.14034", "authors": ["Alexandros Tsakpinis", "Alexander Pretschner"], "title": "Analyzing the Availability of E-Mail Addresses for PyPI Libraries", "comment": "6 pages, 4 figures", "summary": "Open Source Software (OSS) libraries form the backbone of modern software systems, yet their long-term sustainability often depends on maintainers being reachable for support, coordination, and security reporting. In this paper, we empirically analyze the availability of contact information - specifically e-mail addresses - across 686,034 Python libraries on the Python Package Index (PyPI) and their associated GitHub repositories. We examine how and where maintainers provide this information, assess its validity, and explore coverage across individual libraries and their dependency chains. Our findings show that 81.6% of libraries include at least one valid e-mail address, with PyPI serving as the primary source (79.5%). When analyzing dependency chains, we observe that up to 97.8% of direct and 97.7% of transitive dependencies provide valid contact information. At the same time, we identify over 698,000 invalid entries, primarily due to missing fields. These results demonstrate strong maintainer reachability across the ecosystem, while highlighting opportunities for improvement - such as offering clearer guidance to maintainers during the packaging process and introducing opt-in validation mechanisms for existing e-mail addresses.", "AI": {"tldr": "\u8be5\u7814\u7a76\u5206\u6790\u4e86PyPI\u4e0a686,034\u4e2aPython\u5e93\u7684\u8054\u7cfb\u4fe1\u606f\u53ef\u7528\u6027\uff0c\u53d1\u73b081.6%\u63d0\u4f9b\u6709\u6548\u90ae\u7bb1\uff0c\u4f9d\u8d56\u94fe\u8986\u76d6\u7387\u9ad8\u8fbe97.7%\uff0c\u4f46\u5b58\u5728\u5927\u91cf\u65e0\u6548\u6761\u76ee\uff0c\u5efa\u8bae\u6539\u8fdb\u6253\u5305\u6307\u5f15\u548c\u90ae\u7bb1\u9a8c\u8bc1\u673a\u5236\u3002", "motivation": "\u5f00\u6e90\u8f6f\u4ef6\u957f\u671f\u53ef\u6301\u7eed\u6027\u4f9d\u8d56\u7ef4\u62a4\u8005\u53ef\u8054\u7cfb\u6027\uff0c\u9700\u8bc4\u4f30\u5176\u8054\u7cfb\u65b9\u5f0f\u7684\u6709\u6548\u6027\u4e0e\u8986\u76d6\u8303\u56f4\u3002", "method": "\u5bf9PyPI\u53ca\u5176GitHub\u4ed3\u5e93\u4e2d\u7684Python\u5e93\u8fdb\u884c\u5b9e\u8bc1\u5206\u6790\uff0c\u68c0\u67e5\u90ae\u7bb1\u5730\u5740\u7684\u5b58\u5728\u3001\u6709\u6548\u6027\u53ca\u5728\u4f9d\u8d56\u94fe\u4e2d\u7684\u8986\u76d6\u60c5\u51b5\u3002", "result": "81.6%\u5e93\u542b\u6709\u6548\u90ae\u7bb1\uff0cPyPI\u4e3a\u4e3b\u8981\u6765\u6e90\uff0879.5%\uff09\uff0c\u4f9d\u8d56\u94fe\u8986\u76d6\u7387\u8fbe97.7%\uff0c\u540c\u65f6\u53d1\u73b0\u8d8569.8\u4e07\u6761\u65e0\u6548\u8bb0\u5f55\u3002", "conclusion": "\u751f\u6001\u7cfb\u7edf\u6574\u4f53\u53ef\u8fbe\u6027\u826f\u597d\uff0c\u4f46\u53ef\u901a\u8fc7\u4f18\u5316\u6253\u5305\u6d41\u7a0b\u6307\u5f15\u548c\u5f15\u5165\u90ae\u7bb1\u9a8c\u8bc1\u673a\u5236\u8fdb\u4e00\u6b65\u63d0\u5347\u3002"}}
{"id": "2601.14081", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2601.14081", "abs": "https://arxiv.org/abs/2601.14081", "authors": ["Xingcheng Chen", "Oliver Weissl", "Andrea Stocco"], "title": "Feature-Aware Test Generation for Deep Learning Models", "comment": null, "summary": "As deep learning models are widely used in software systems, test generation plays a crucial role in assessing the quality of such models before deployment. To date, the most advanced test generators rely on generative AI to synthesize inputs; however, these approaches remain limited in providing semantic insight into the causes of misbehaviours and in offering fine-grained semantic controllability over the generated inputs. In this paper, we introduce Detect, a feature-aware test generation framework for vision-based deep learning (DL) models that systematically generates inputs by perturbing disentangled semantic attributes within the latent space. Detect perturbs individual latent features in a controlled way and observes how these changes affect the model's output. Through this process, it identifies which features lead to behavior shifts and uses a vision-language model for semantic attribution. By distinguishing between task-relevant and irrelevant features, Detect applies feature-aware perturbations targeted for both generalization and robustness. Empirical results across image classification and detection tasks show that Detect generates high-quality test cases with fine-grained control, reveals distinct shortcut behaviors across model architectures (convolutional and transformer-based), and bugs that are not captured by accuracy metrics. Specifically, Detect outperforms a state-of-the-art test generator in decision boundary discovery and a leading spurious feature localization method in identifying robustness failures. Our findings show that fully fine-tuned convolutional models are prone to overfitting on localized cues, such as co-occurring visual traits, while weakly supervised transformers tend to rely on global features, such as environmental variances. These findings highlight the value of interpretable and feature-aware testing in improving DL model reliability.", "AI": {"tldr": "Detect\u662f\u4e00\u4e2a\u57fa\u4e8e\u7279\u5f81\u611f\u77e5\u7684\u6d4b\u8bd5\u751f\u6210\u6846\u67b6\uff0c\u901a\u8fc7\u5728\u6f5c\u5728\u7a7a\u95f4\u4e2d\u6270\u52a8\u89e3\u8026\u8bed\u4e49\u5c5e\u6027\uff0c\u5b9e\u73b0\u5bf9\u89c6\u89c9\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u7684\u7ec6\u7c92\u5ea6\u53ef\u63a7\u6d4b\u8bd5\uff0c\u63d0\u5347\u6a21\u578b\u53ef\u9760\u6027\u3002", "motivation": "\u73b0\u6709\u751f\u6210\u5f0fAI\u6d4b\u8bd5\u65b9\u6cd5\u7f3a\u4e4f\u8bed\u4e49\u6d1e\u5bdf\u529b\u548c\u7ec6\u7c92\u5ea6\u63a7\u5236\u80fd\u529b\uff0c\u96be\u4ee5\u63ed\u793a\u6a21\u578b\u5931\u6548\u7684\u6839\u672c\u539f\u56e0\u3002", "method": "Detect\u901a\u8fc7\u53ef\u63a7\u6270\u52a8\u6f5c\u5728\u7279\u5f81\uff0c\u89c2\u5bdf\u8f93\u51fa\u53d8\u5316\uff0c\u5e76\u7ed3\u5408\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u8bed\u4e49\u5f52\u56e0\uff0c\u533a\u5206\u4efb\u52a1\u76f8\u5173\u4e0e\u65e0\u5173\u7279\u5f81\uff0c\u5b9e\u65bd\u9488\u5bf9\u6027\u6270\u52a8\u3002", "result": "Detect\u5728\u56fe\u50cf\u5206\u7c7b\u4e0e\u68c0\u6d4b\u4efb\u52a1\u4e2d\u751f\u6210\u9ad8\u8d28\u91cf\u6d4b\u8bd5\u7528\u4f8b\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u53d1\u73b0\u51b3\u7b56\u8fb9\u754c\u548c\u5b9a\u4f4d\u865a\u5047\u7279\u5f81\uff0c\u63ed\u793a\u4e86\u4e0d\u540c\u67b6\u6784\u6a21\u578b\u7684\u6377\u5f84\u884c\u4e3a\u3002", "conclusion": "\u53ef\u89e3\u91ca\u3001\u7279\u5f81\u611f\u77e5\u7684\u6d4b\u8bd5\u6709\u52a9\u4e8e\u63d0\u5347\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u7684\u53ef\u9760\u6027\uff0c\u5c24\u5176\u80fd\u66b4\u9732\u51c6\u786e\u7387\u6307\u6807\u65e0\u6cd5\u6355\u6349\u7684\u7f3a\u9677\u3002"}}
{"id": "2601.14131", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2601.14131", "abs": "https://arxiv.org/abs/2601.14131", "authors": ["Amila Indika", "Rick Kazman", "Anthony Peruma"], "title": "Practitioner Views on Mobile App Accessibility: Practices and Challenges", "comment": "The 48th IEEE/ACM International Conference on Software Engineering - Research Track", "summary": "As mobile applications (apps) become ubiquitous in everyday life, it is crucial for developers to prioritize accessibility for users with diverse abilities. While previous research has identified widespread accessibility issues and raised awareness of developer challenges, there remains a lack of cross-platform, globally representative insights into how practitioners approach accessibility in practice. This paper presents findings from a mixed-methods survey of 110 mobile app developers across 43 countries, examining how platform ecosystems (iOS vs. Android) and developer experience shape accessibility practices. Results show that while developers recognize the importance of accessibility, they often rely on platform-specific guidelines and typically perform compliance testing late in the development process. Developers primarily implement text-focused features while struggling with API limitations and organizational constraints. Through systematic cross-platform comparison, we identify novel platform-specific barriers and demonstrate how accessibility practices differ across developer experience levels. Our findings offer new insights into the challenges of achieving accessibility in practice and provide actionable steps for various stakeholders to promote more consistent and inclusive app development.", "AI": {"tldr": "\u672c\u7814\u7a76\u901a\u8fc7\u8c03\u67e5110\u540d\u6765\u81ea43\u4e2a\u56fd\u5bb6\u7684\u79fb\u52a8\u5e94\u7528\u5f00\u53d1\u8005\uff0c\u63ed\u793a\u4e86iOS\u4e0eAndroid\u5e73\u53f0\u53ca\u5f00\u53d1\u8005\u7ecf\u9a8c\u5bf9\u65e0\u969c\u788d\u5b9e\u8df5\u7684\u5f71\u54cd\uff0c\u63d0\u51fa\u4fc3\u8fdb\u5305\u5bb9\u6027\u5f00\u53d1\u7684\u53ef\u884c\u5efa\u8bae\u3002", "motivation": "\u5f53\u524d\u7f3a\u4e4f\u8de8\u5e73\u53f0\u3001\u5168\u7403\u4ee3\u8868\u6027\u89c6\u89d2\u4e0b\u5f00\u53d1\u8005\u5b9e\u9645\u5904\u7406\u65e0\u969c\u788d\u95ee\u9898\u7684\u7814\u7a76\uff0c\u672c\u6587\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u91c7\u7528\u6df7\u5408\u65b9\u6cd5\u8c03\u67e5110\u540d\u8de843\u56fd\u7684\u79fb\u52a8\u5e94\u7528\u5f00\u53d1\u8005\uff0c\u7cfb\u7edf\u6bd4\u8f83iOS\u4e0eAndroid\u5e73\u53f0\u5dee\u5f02\u53ca\u4e0d\u540c\u7ecf\u9a8c\u6c34\u5e73\u5f00\u53d1\u8005\u7684\u884c\u4e3a\u6a21\u5f0f\u3002", "result": "\u5f00\u53d1\u8005\u867d\u91cd\u89c6\u65e0\u969c\u788d\uff0c\u4f46\u591a\u4f9d\u8d56\u5e73\u53f0\u7279\u5b9a\u6307\u5357\u3001\u665a\u671f\u6d4b\u8bd5\uff0c\u4e14\u96c6\u4e2d\u4e8e\u6587\u672c\u529f\u80fd\uff1bAPI\u9650\u5236\u4e0e\u7ec4\u7ec7\u7ea6\u675f\u662f\u4e3b\u8981\u969c\u788d\uff0c\u4e0d\u540c\u5e73\u53f0\u548c\u7ecf\u9a8c\u6c34\u5e73\u5b58\u5728\u663e\u8457\u5b9e\u8df5\u5dee\u5f02\u3002", "conclusion": "\u7814\u7a76\u63ed\u793a\u4e86\u5b9e\u73b0\u65e0\u969c\u788d\u7684\u5b9e\u9645\u6311\u6218\uff0c\u4e3a\u5404\u5229\u76ca\u76f8\u5173\u65b9\u63d0\u4f9b\u4e86\u63a8\u52a8\u66f4\u4e00\u81f4\u3001\u5305\u5bb9\u6027\u5e94\u7528\u5f00\u53d1\u7684\u5177\u4f53\u884c\u52a8\u65b9\u5411\u3002"}}
{"id": "2601.14163", "categories": ["cs.SE", "cs.CR"], "pdf": "https://arxiv.org/pdf/2601.14163", "abs": "https://arxiv.org/abs/2601.14163", "authors": ["Mohammed Latif Siddiq", "Tanzim Hossain Romel", "Natalie Sekerak", "Beatrice Casey", "Joanna C. S. Santos"], "title": "An Empirical Study on Remote Code Execution in Machine Learning Model Hosting Ecosystems", "comment": null, "summary": "Model-sharing platforms, such as Hugging Face, ModelScope, and OpenCSG, have become central to modern machine learning development, enabling developers to share, load, and fine-tune pre-trained models with minimal effort. However, the flexibility of these ecosystems introduces a critical security concern: the execution of untrusted code during model loading (i.e., via trust_remote_code or trust_repo). In this work, we conduct the first large-scale empirical study of custom model loading practices across five major model-sharing platforms to assess their prevalence, associated risks, and developer perceptions. We first quantify the frequency with which models require custom code to function and identify those that execute arbitrary Python files during loading. We then apply three complementary static analysis tools: Bandit, CodeQL, and Semgrep, to detect security smells and potential vulnerabilities, categorizing our findings by CWE identifiers to provide a standardized risk taxonomy. We also use YARA to identify malicious patterns and payload signatures. In parallel, we systematically analyze the documentation, API design, and safety mechanisms of each platform to understand their mitigation strategies and enforcement levels. Finally, we conduct a qualitative analysis of over 600 developer discussions from GitHub, Hugging Face, and PyTorch Hub forums, as well as Stack Overflow, to capture community concerns and misconceptions regarding security and usability. Our findings reveal widespread reliance on unsafe defaults, uneven security enforcement across platforms, and persistent confusion among developers about the implications of executing remote code. We conclude with actionable recommendations for designing safer model-sharing infrastructures and striking a balance between usability and security in future AI ecosystems.", "AI": {"tldr": "\u672c\u6587\u9996\u6b21\u5927\u89c4\u6a21\u5b9e\u8bc1\u7814\u7a76\u6a21\u578b\u5171\u4eab\u5e73\u53f0\u4e2d\u81ea\u5b9a\u4e49\u6a21\u578b\u52a0\u8f7d\u7684\u5b89\u5168\u98ce\u9669\uff0c\u63ed\u793a\u666e\u904d\u4f9d\u8d56\u4e0d\u5b89\u5168\u9ed8\u8ba4\u8bbe\u7f6e\u53ca\u5f00\u53d1\u8005\u8ba4\u77e5\u8bef\u533a\uff0c\u5e76\u63d0\u51fa\u517c\u987e\u5b89\u5168\u4e0e\u6613\u7528\u6027\u7684\u6539\u8fdb\u5efa\u8bae\u3002", "motivation": "\u6a21\u578b\u5171\u4eab\u5e73\u53f0\u5e7f\u6cdb\u4f7f\u7528\u8fdc\u7a0b\u4ee3\u7801\u6267\u884c\u529f\u80fd\uff0c\u4f46\u7f3a\u4e4f\u5bf9\u6f5c\u5728\u5b89\u5168\u98ce\u9669\u7684\u7cfb\u7edf\u8bc4\u4f30\uff0c\u4e9f\u9700\u5398\u6e05\u73b0\u72b6\u5e76\u63d0\u51fa\u6539\u8fdb\u65b9\u6848\u3002", "method": "\u7ed3\u5408\u9759\u6001\u5206\u6790\u5de5\u5177\uff08Bandit\u3001CodeQL\u3001Semgrep\u3001YARA\uff09\u68c0\u6d4b\u6f0f\u6d1e\uff0c\u5206\u6790\u4e94\u5927\u5e73\u53f0\u6587\u6863\u4e0eAPI\u8bbe\u8ba1\uff0c\u5e76\u5b9a\u6027\u7814\u7a76600\u4f59\u6761\u5f00\u53d1\u8005\u8ba8\u8bba\u3002", "result": "\u53d1\u73b0\u5e73\u53f0\u666e\u904d\u5b58\u5728\u5b89\u5168\u673a\u5236\u6267\u884c\u4e0d\u5747\u3001\u5f00\u53d1\u8005\u5bf9\u8fdc\u7a0b\u4ee3\u7801\u98ce\u9669\u8ba4\u77e5\u6a21\u7cca\uff0c\u4e14\u5927\u91cf\u6a21\u578b\u4f9d\u8d56\u4e0d\u5b89\u5168\u9ed8\u8ba4\u914d\u7f6e\u3002", "conclusion": "\u5efa\u8bae\u91cd\u6784\u6a21\u578b\u5171\u4eab\u57fa\u7840\u8bbe\u65bd\u5b89\u5168\u8bbe\u8ba1\uff0c\u5728\u4fdd\u969c\u6613\u7528\u6027\u7684\u540c\u65f6\u5f3a\u5316\u9ed8\u8ba4\u5b89\u5168\u7b56\u7565\u4e0e\u5f00\u53d1\u8005\u6559\u80b2\u3002"}}
