<div id=toc></div>

# Table of Contents

- [cs.SE](#cs.SE) [Total: 22]
- [cs.AR](#cs.AR) [Total: 3]
- [cs.DC](#cs.DC) [Total: 7]
- [cs.NI](#cs.NI) [Total: 6]
- [cs.OS](#cs.OS) [Total: 1]


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [1] [CWM: An Open-Weights LLM for Research on Code Generation with World Models](https://arxiv.org/abs/2510.02387)
*FAIR CodeGen team,Quentin Carbonneaux,Gal Cohen,Jonas Gehring,Jacob Kahn,Jannik Kossen,Felix Kreuk,Emily McMilin,Michel Meyer,Yuxiang Wei,David Zhang,Kunhao Zheng,Jordi Armengol-Estapé,Pedram Bashiri,Maximilian Beck,Pierre Chambon,Abhishek Charnalia,Chris Cummins,Juliette Decugis,Zacharias V. Fisches,François Fleuret,Fabian Gloeckle,Alex Gu,Michael Hassid,Daniel Haziza,Badr Youbi Idrissi,Christian Keller,Rahul Kindi,Hugh Leather,Gallil Maimon,Aram Markosyan,Francisco Massa,Pierre-Emmanuel Mazaré,Vegard Mella,Naila Murray,Keyur Muzumdar,Peter O'Hearn,Matteo Pagliardini,Dmitrii Pedchenko,Tal Remez,Volker Seeker,Marco Selvi,Oren Sultan,Sida Wang,Luca Wehrstedt,Ori Yoran,Lingming Zhang,Taco Cohen,Yossi Adi,Gabriel Synnaeve*

Main category: cs.SE

TL;DR: Code World Model (CWM)是一个320亿参数的开源LLM，通过在Python解释器和Docker环境中训练观察-行动轨迹，提升代码理解和生成能力，在多项编程和数学任务上表现优异。


<details>
  <summary>Details</summary>
Motivation: 为了提升代码生成能力，超越仅从静态代码训练所能学到的内容，需要构建能够理解和模拟计算环境的世界模型。

Method: 在大量Python解释器和Docker环境的观察-行动轨迹上进行中期训练，并进行多任务推理强化学习，包括可验证编程、数学和多轮软件工程环境。

Result: CWM在多项基准测试中表现优异：SWE-bench Verified 65.8%、LiveCodeBench 68.6%、Math-500 96.6%、AIME 2024 76.0%。

Conclusion: CWM为研究代码世界建模提供了强大平台，展示了世界模型如何通过推理和规划提升代码生成能力，并支持逐步模拟Python代码执行。

Abstract: We release Code World Model (CWM), a 32-billion-parameter open-weights LLM,
to advance research on code generation with world models. To improve code
understanding beyond what can be learned from training on static code alone, we
mid-train CWM on a large amount of observation-action trajectories from Python
interpreter and agentic Docker environments, and perform extensive multi-task
reasoning RL in verifiable coding, math, and multi-turn software engineering
environments. With CWM, we provide a strong testbed for researchers to explore
the opportunities world modeling affords for improving code generation with
reasoning and planning in computational environments. We present first steps of
how world models can benefit agentic coding, enable step-by-step simulation of
Python code execution, and show early results of how reasoning can benefit from
the latter. CWM is a dense, decoder-only LLM trained with a context size of up
to 131k tokens. Independent of its world modeling capabilities, CWM offers
strong performance on general coding and math tasks: it reaches pass@1 scores
of 65.8% on SWE-bench Verified (with test-time scaling), 68.6% on
LiveCodeBench, 96.6% on Math-500, and 76.0% on AIME 2024. To support further
research on code world modeling, we release model checkpoints after
mid-training, SFT, and RL.

</details>


### [2] [From Trace to Line: LLM Agent for Real-World OSS Vulnerability Localization](https://arxiv.org/abs/2510.02389)
*Haoran Xi,Minghao Shao,Brendan Dolan-Gavitt,Muhammad Shafique,Ramesh Karri*

Main category: cs.SE

TL;DR: T2L-Agent是一个项目级别的端到端漏洞发现框架，通过多轮反馈和运行时证据融合，实现了从模块到精确漏洞行的渐进式定位，在T2L-ARVO基准测试中显著优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 现有漏洞检测方法在孤立代码分析、长上下文处理和粗粒度检测方面存在局限，无法为工程师提供精确的行级定位和针对性修复指导。

Method: T2L-Agent结合多轮反馈和代理追踪分析器(ATA)，融合运行时证据(崩溃点、堆栈跟踪、覆盖率差异)与基于AST的代码分块，实现迭代精炼的精确行级诊断。

Result: 在T2L-ARVO基准测试中，T2L-Agent达到58.0%的检测率和54.8%的行级定位率，显著优于基线方法。

Conclusion: 该框架和基准测试将基于LLM的漏洞检测从粗粒度识别推向可部署的、鲁棒的精确诊断，减少了噪声并加速了开源软件工作流中的补丁过程。

Abstract: Large language models show promise for vulnerability discovery, yet
prevailing methods inspect code in isolation, struggle with long contexts, and
focus on coarse function- or file-level detections - offering limited
actionable guidance to engineers who need precise line-level localization and
targeted patches in real-world software development. We present T2L-Agent
(Trace-to-Line Agent), a project-level, end-to-end framework that plans its own
analysis and progressively narrows scope from modules to exact vulnerable
lines. T2L-Agent couples multi-round feedback with an Agentic Trace Analyzer
(ATA) that fuses runtime evidence - crash points, stack traces, and coverage
deltas - with AST-based code chunking, enabling iterative refinement beyond
single pass predictions and translating symptoms into actionable, line-level
diagnoses. To benchmark line-level vulnerability discovery, we introduce
T2L-ARVO, a diverse, expert-verified 50-case benchmark spanning five crash
families and real-world projects. T2L-ARVO is specifically designed to support
both coarse-grained detection and fine-grained localization, enabling rigorous
evaluation of systems that aim to move beyond file-level predictions. On
T2L-ARVO, T2L-Agent achieves up to 58.0% detection and 54.8% line-level
localization, substantially outperforming baselines. Together, the framework
and benchmark push LLM-based vulnerability detection from coarse identification
toward deployable, robust, precision diagnostics that reduce noise and
accelerate patching in open-source software workflows.

</details>


### [3] [AP2O: Correcting LLM-Generated Code Errors Type by Type Like Humans via Adaptive Progressive Preference Optimization](https://arxiv.org/abs/2510.02393)
*Jianqing Zhang,Wei Xia,Hande Dong,Qiang Lin,Jian Cao*

Main category: cs.SE

TL;DR: 提出AP2O-Coder方法，通过渐进式错误类型优化提升LLM代码生成能力，减少编译和运行时错误


<details>
  <summary>Details</summary>
Motivation: 现有离线偏好优化方法主要关注通过通过/失败信号提升LLM编程能力，忽视了失败代码中的深层错误类型

Method: 构建错误笔记本来自失败代码，逐步优化LLM按错误类型进行纠正，并自适应重放错误类型以适应训练过程中LLM的弱点变化

Result: 在0.5B到34B参数的代码和通用LLM上实验显示，AP2O-Coder将代码生成性能提升高达3%的pass@k，同时使用更少的偏好数据

Conclusion: AP2O-Coder通过自适应渐进式错误类型优化有效提升了LLM的代码生成质量

Abstract: LLMs' code generation capabilities have yielded substantial improvements in
the effectiveness of programming tasks. However, LLM-generated code still
suffers from compilation and runtime errors. Existing offline preference
optimization methods primarily focus on enhancing LLMs' coding abilities using
pass/fail signals in the preference data, overlooking the deep-level error
types in the failed codes. To address this, we propose Adaptively Progressive
Preference Optimization (AP2O) for coding (i.e., AP2O-Coder), a method that
guides LLMs adaptively and methodically to reduce code errors for code
generation. Specifically, we construct an error notebook from failed codes and
progressively optimize the LLM to correct errors type by type. Furthermore, we
adaptively replay error types to tailor to the LLM's changing weaknesses
throughout the training process. Through extensive experiments on both code and
general LLMs (Llama, Qwen, and DeepSeek series) with parameters ranging from
0.5B to 34B, our AP2O-Coder improves code generation performance by up to 3% in
pass@k while using less preference data. Code: https://github.com/TsingZ0/AP2O

</details>


### [4] [Dynamic Function Configuration and its Management in Serverless Computing: A Taxonomy and Future Directions](https://arxiv.org/abs/2510.02404)
*Siddharth Agarwal,Maria A. Rodriguez,Rajkumar Buyya*

Main category: cs.SE

TL;DR: 本文对FaaS环境中的资源配置技术进行了系统分析，提出了影响函数设计、配置、运行成本和性能保证的因素分类法，并指出了现有研究空白和未来方向。


<details>
  <summary>Details</summary>
Motivation: serverless计算模型中，开发者缺乏平台透明度，只能依赖经验进行资源配置决策，这导致在满足性能约束的同时实现最优资源配置成为非平凡任务。

Method: 通过分析现有文献，提出资源配置影响因素分类法，并进行全面的函数配置研究综述。

Result: 建立了系统的资源配置技术分析框架，识别了影响函数性能和质量的关键因素。

Conclusion: 需要进一步研究来增强函数配置能力，推动serverless计算环境的更广泛采用。

Abstract: The serverless cloud computing model offers a framework where the service
provider abstracts the underlying infrastructure management from developers. In
this serverless model, FaaS provides an event-driven, function-oriented
computing service characterised by fine-grained, usage-based pricing that
eliminates cost for idle resources. Platforms like AWS Lambda, Azure Functions,
and Cloud Run Functions require developers to configure their function(s) with
minimum operational resources for its successful execution. This resource
allocation influences both the operational expense and the performance quality
of these functions. However, a noticeable lack of platform transparency forces
developers to rely on expert knowledge or experience-based ad-hoc decisions to
request desired function resources. This makes optimal resource configuration a
non-trivial task while adhering to performance constraints. Furthermore, while
commercial platforms often scale resources like CPU and network bandwidth
proportional to memory, open-source frameworks permit independent configuration
of function resources, introducing additional complexity for developers aiming
to optimise their functions. These complexities have directed researchers to
resolve developer challenges and advance towards an efficient server-less
execution model. In this article, we identify different aspects of resource
configuration techniques in FaaS settings and propose a taxonomy of factors
that influence function design, configuration, run-time cost, and performance
guarantees. We conduct an analysis of existing literature on resource
configuration to present a comprehensive review of current studies on function
configuration. We also identify existing research gaps and suggest future
research directions to enhance function configuration and strengthen the
capabilities of serverless computing environments to drive its broader
adoption.

</details>


### [5] [Product Manager Practices for Delegating Work to Generative AI: "Accountability must not be delegated to non-human actors"](https://arxiv.org/abs/2510.02504)
*Mara Ulloa,Jenna L. Butler,Sankeerti Haniyur,Courtney Miller,Barrett Amos,Advait Sarkar,Margaret-Anne Storey*

Main category: cs.SE

TL;DR: 研究微软产品经理采用生成式AI的现状、使用案例、收益障碍，以及任务委托框架和角色适应实践。


<details>
  <summary>Details</summary>
Motivation: 生成式AI正在改变知识工作性质，但现有研究主要关注开发者，对产品经理如何适应GenAI的了解不足。

Method: 混合方法研究：调查885名产品经理，分析731人的遥测数据，访谈15名产品经理。

Result: 提出了产品经理GenAI采用率、使用案例、收益障碍的现状分析，以及任务委托评估框架和角色整合实践。

Conclusion: 研究揭示了产品经理角色在GenAI影响下的演变，对更广泛的GenAI工作流采用和软件开发角色具有重要启示。

Abstract: Generative AI (GenAI) is changing the nature of knowledge work, particularly
for Product Managers (PMs) in software development teams. While much software
engineering research has focused on developers' interactions with GenAI, there
is less understanding of how the work of PMs is evolving due to GenAI. To
address this gap, we conducted a mixed-methods study at Microsoft, a large,
multinational software company: surveying 885 PMs, analyzing telemetry data for
a subset of PMs (N=731), and interviewing a subset of 15 PMs. We contribute:
(1) PMs' current GenAI adoption rates, uses cases, and perceived benefits and
barriers and; (2) a framework capturing how PMs assess which tasks to delegate
to GenAI; (3) PMs adaptation practices for integrating GenAI into their roles
and perceptions of how their role is evolving. We end by discussing
implications on the broader GenAI workflow adoption process and software
development roles.

</details>


### [6] [ZeroFalse: Improving Precision in Static Analysis with LLMs](https://arxiv.org/abs/2510.02534)
*Mohsen Iranmanesh,Sina Moradi Sabet,Sina Marefat,Ali Javidi Ghasr,Allison Wilson,Iman Sharafaldin,Mohammad A. Tayebi*

Main category: cs.SE

TL;DR: ZeroFalse是一个结合静态分析和大型语言模型的框架，通过结构化处理分析器输出、添加流敏感追踪和上下文证据，显著减少SAST工具的误报率，同时保持高覆盖率。


<details>
  <summary>Details</summary>
Motivation: 静态应用安全测试(SAST)工具存在大量误报，削弱开发者信任并需要昂贵的人工审查，需要一种方法来减少误报同时保持覆盖范围。

Method: 将静态分析器输出作为结构化合约处理，添加流敏感追踪、上下文证据和CWE特定知识，然后由LLM进行裁决，保持静态分析的系统性覆盖范围同时利用LLM的推理能力。

Result: 在OWASP Java基准测试中F1分数达到0.912，OpenVuln数据集上达到0.955，召回率和精确度均超过90%，CWE专用提示始终优于通用提示。

Conclusion: ZeroFalse为增强SAST可靠性提供了实用且可扩展的方法，支持其集成到真实的CI/CD流水线中。

Abstract: Static Application Security Testing (SAST) tools are integral to modern
software development, yet their adoption is undermined by excessive false
positives that weaken developer trust and demand costly manual triage. We
present ZeroFalse, a framework that integrates static analysis with large
language models (LLMs) to reduce false positives while preserving coverage.
ZeroFalse treats static analyzer outputs as structured contracts, enriching
them with flow-sensitive traces, contextual evidence, and CWE-specific
knowledge before adjudication by an LLM. This design preserves the systematic
reach of static analysis while leveraging the reasoning capabilities of LLMs.
We evaluate ZeroFalse across both benchmarks and real-world projects using ten
state-of-the-art LLMs. Our best-performing models achieve F1-scores of 0.912 on
the OWASP Java Benchmark and 0.955 on the OpenVuln dataset, maintaining recall
and precision above 90%. Results further show that CWE-specialized prompting
consistently outperforms generic prompts, and reasoning-oriented LLMs provide
the most reliable precision-recall balance. These findings position ZeroFalse
as a practical and scalable approach for enhancing the reliability of SAST and
supporting its integration into real-world CI/CD pipelines.

</details>


### [7] [Key Considerations for Auto-Scaling: Lessons from Benchmark Microservices](https://arxiv.org/abs/2510.02585)
*Majid Dashtbani,Ladan Tahvildari*

Main category: cs.SE

TL;DR: 本文分析了微服务系统中自动扩缩容的实际挑战，提出了基于软件生命周期（架构、实现、部署）的分类方法，并通过实验验证了忽视这些关键因素会降低扩缩容性能。


<details>
  <summary>Details</summary>
Motivation: 微服务已成为云原生系统的主流架构范式，但现有基准测试往往忽视设计、实现和部署等基础方面，难以在真实条件下评估自动扩缩容方法。

Method: 通过将多种最先进的自动扩缩容方法应用于广泛使用的微服务基准测试，识别实践考虑因素，并按软件生命周期阶段分类：架构、实现和部署。使用Sock-Shop基准验证这些考虑因素，并评估多种扩缩容策略。

Result: 研究结果表明，忽视关键生命周期问题会降低自动扩缩容器性能，而解决这些问题可以实现更稳定和高效的扩缩容。

Conclusion: 这些结果强调了生命周期感知工程对于释放微服务系统中自动扩缩容全部潜力的重要性。

Abstract: Microservices have become the dominant architectural paradigm for building
scalable and modular cloud-native systems. However, achieving effective
auto-scaling in such systems remains a non-trivial challenge, as it depends not
only on advanced scaling techniques but also on sound design, implementation,
and deployment practices. Yet, these foundational aspects are often overlooked
in existing benchmarks, making it difficult to evaluate autoscaling methods
under realistic conditions. In this paper, we identify a set of practical
auto-scaling considerations by applying several state-of-the-art autoscaling
methods to widely used microservice benchmarks. To structure these findings, we
classify the issues based on when they arise during the software lifecycle:
Architecture, Implementation, and Deployment. The Architecture phase covers
high-level decisions such as service decomposition and inter-service
dependencies. The Implementation phase includes aspects like initialization
overhead, metrics instrumentation, and error propagation. The Deployment phase
focuses on runtime configurations such as resource limits and health checks. We
validate these considerations using the Sock-Shop benchmark and evaluate
diverse auto-scaling strategies, including threshold-based, control-theoretic,
learning-based, black-box optimization, and dependency-aware approaches. Our
findings show that overlooking key lifecycle concerns can degrade autoscaler
performance, while addressing them leads to more stable and efficient scaling.
These results underscore the importance of lifecycle-aware engineering for
unlocking the full potential of auto-scaling in microservice-based systems.

</details>


### [8] [RedCodeAgent: Automatic Red-teaming Agent against Diverse Code Agents](https://arxiv.org/abs/2510.02609)
*Chengquan Guo,Chulin Xie,Yu Yang,Zhaorun Chen,Zinan Lin,Xander Davies,Yarin Gal,Dawn Song,Bo Li*

Main category: cs.SE

TL;DR: RedCodeAgent是首个自动化红队代理，通过自适应记忆模块和定制化工具箱，系统性地发现代码代理的安全漏洞，在模拟沙盒环境中评估执行结果，显著优于现有红队方法。


<details>
  <summary>Details</summary>
Motivation: 当前代码代理的广泛使用带来了安全和风险问题，现有的静态安全基准和红队工具无法有效识别新兴的现实世界风险场景，特别是不同越狱工具的组合效应。

Method: 开发RedCodeAgent自动化红队代理，包含自适应记忆模块来利用现有越狱知识，动态选择最有效的红队工具组合，并在模拟沙盒环境中评估代码代理的执行结果。

Result: RedCodeAgent在多个最先进代码代理、多样化风险场景和各种编程语言的评估中，始终优于现有红队方法，实现了更高的攻击成功率和更低的拒绝率。

Conclusion: RedCodeAgent通过自动化和优化红队流程，实现了对代码代理的可扩展、自适应和有效的安全评估，暴露了先前未识别的安全风险。

Abstract: Code agents have gained widespread adoption due to their strong code
generation capabilities and integration with code interpreters, enabling
dynamic execution, debugging, and interactive programming capabilities. While
these advancements have streamlined complex workflows, they have also
introduced critical safety and security risks. Current static safety benchmarks
and red-teaming tools are inadequate for identifying emerging real-world risky
scenarios, as they fail to cover certain boundary conditions, such as the
combined effects of different jailbreak tools. In this work, we propose
RedCodeAgent, the first automated red-teaming agent designed to systematically
uncover vulnerabilities in diverse code agents. With an adaptive memory module,
RedCodeAgent can leverage existing jailbreak knowledge, dynamically select the
most effective red-teaming tools and tool combinations in a tailored toolbox
for a given input query, thus identifying vulnerabilities that might otherwise
be overlooked. For reliable evaluation, we develop simulated sandbox
environments to additionally evaluate the execution results of code agents,
mitigating potential biases of LLM-based judges that only rely on static code.
Through extensive evaluations across multiple state-of-the-art code agents,
diverse risky scenarios, and various programming languages, RedCodeAgent
consistently outperforms existing red-teaming methods, achieving higher attack
success rates and lower rejection rates with high efficiency. We further
validate RedCodeAgent on real-world code assistants, e.g., Cursor and Codeium,
exposing previously unidentified security risks. By automating and optimizing
red-teaming processes, RedCodeAgent enables scalable, adaptive, and effective
safety assessments of code agents.

</details>


### [9] [Automatic Building Code Review: A Case Study](https://arxiv.org/abs/2510.02634)
*Hanlong Wan,Weili Xu,Michael Rosenberg,Jian Zhang,Aysha Siddika*

Main category: cs.SE

TL;DR: 提出了一种基于BIM和LLM的自动化建筑规范审查框架，整合了RAG和MCP代理管道，通过COMcheck API和规则推理实现高效准确的代码审查


<details>
  <summary>Details</summary>
Motivation: 建筑官员在资源受限或农村地区面临劳动密集型、易出错且成本高昂的设计文档手动审查，随着项目规模和复杂性的增加，需要自动化解决方案

Method: 开发了代理驱动框架，集成BIM数据提取与自动化验证，使用RAG和MCP代理管道，通过COMcheck API调用和规则条款推理进行建筑规范检查

Result: 案例演示验证了框架有效性，GPT-4o在效率和稳定性方面表现最佳，MCP代理管道在严谨性和可靠性方面优于RAG推理管道

Conclusion: 该工作推进了自动化代码审查研究，展示了可扩展、可互操作且生产就绪的方法，将BIM与权威代码审查工具连接起来

Abstract: Building officials, particularly those in resource-constrained or rural
jurisdictions, face labor-intensive, error-prone, and costly manual reviews of
design documents as projects increase in size and complexity. The growing
adoption of Building Information Modeling (BIM) and Large Language Models
(LLMs) presents opportunities for automated code review (ACR) solutions. This
study introduces a novel agent-driven framework that integrates BIM-based data
extraction with automated verification using both retrieval-augmented
generation (RAG) and Model Context Protocol (MCP) agent pipelines. The
framework employs LLM-enabled agents to extract geometry, schedules, and system
attributes from heterogeneous file types, which are then processed for building
code checking through two complementary mechanisms: (1) direct API calls to the
US Department of Energy COMcheck engine, providing deterministic and
audit-ready outputs, and (2) RAG-based reasoning over rule provisions, enabling
flexible interpretation where coverage is incomplete or ambiguous.
  The framework was evaluated through case demonstrations, including automated
extraction of geometric attributes (such as surface area, tilt, and insulation
values), parsing of operational schedules, and validation of lighting
allowances under ASHRAE Standard 90.1-2022. Comparative performance tests
across multiple LLMs showed that GPT-4o achieved the best balance of efficiency
and stability, while smaller models exhibited inconsistencies or failures.
Results confirm that MCP agent pipelines outperform RAG reasoning pipelines in
rigor and reliability. This work advances ACR research by demonstrating a
scalable, interoperable, and production-ready approach that bridges BIM with
authoritative code review tools.

</details>


### [10] [Using Fourier Analysis and Mutant Clustering to Accelerate DNN Mutation Testing](https://arxiv.org/abs/2510.02718)
*Ali Ghanbari,Sasan Tavakkol*

Main category: cs.SE

TL;DR: DM#使用傅里叶分析加速DNN突变测试，通过量化突变体行为进行聚类，只需测试代表性突变体即可推断整个组的测试结果，平均加速28.38%，误差仅0.72%。


<details>
  <summary>Details</summary>
Motivation: DNN突变分析成本高昂，因为需要测试大量突变体和大数据集，需要一种加速方法。

Method: 利用傅里叶分析量化DNN突变体行为，进行聚类分组，每组选一个代表性突变体测试，结果复用给同组其他突变体。

Result: 在14个DNN模型上评估，平均加速28.38%，平均突变分数误差仅0.72%，相比基线方法误差减少11.78-114.36倍。

Conclusion: DM#能有效加速DNN突变测试，在保持高准确性的同时显著降低计算成本。

Abstract: Deep neural network (DNN) mutation analysis is a promising approach to
evaluating test set adequacy. Due to the large number of generated mutants that
must be tested on large datasets, mutation analysis is costly. In this paper,
we present a technique, named DM#, for accelerating DNN mutation testing using
Fourier analysis. The key insight is that DNN outputs are real-valued functions
suitable for Fourier analysis that can be leveraged to quantify mutant behavior
using only a few data points. DM# uses the quantified mutant behavior to
cluster the mutants so that the ones with similar behavior fall into the same
group. A representative from each group is then selected for testing, and the
result of the test, e.g., whether the mutant is killed or survived, is reused
for all other mutants represented by the selected mutant, obviating the need
for testing other mutants. 14 DNN models of sizes ranging from thousands to
millions of parameters, trained on different datasets, are used to evaluate DM#
and compare it to several baseline techniques. Our results provide empirical
evidence on the effectiveness of DM# in accelerating mutation testing by
28.38%, on average, at the average cost of only 0.72% error in mutation score.
Moreover, on average, DM# incurs 11.78, 15.16, and 114.36 times less mutation
score error compared to random mutant selection, boundary sample selection, and
random sample selection techniques, respectively, while generally offering
comparable speed-up.

</details>


### [11] [Automated Repair of OpenID Connect Programs (Extended Version)](https://arxiv.org/abs/2510.02773)
*Tamjid Al Rahat,Yanju Chen,Yu Feng,Yuan Tian*

Main category: cs.SE

TL;DR: AuthFix是一个基于LLM的自动化OpenID Connect漏洞修复引擎，通过故障定位、补丁合成和验证三个组件，成功修复了74%的测试漏洞。


<details>
  <summary>Details</summary>
Motivation: OpenID Connect虽然广泛采用，但存在严重安全漏洞导致重大经济损失和安全漏洞，需要有效的自动化修复方案。

Method: AuthFix采用反例引导的修复方法，整合故障定位、补丁合成和补丁验证三个关键组件，并使用新颖的Petri网模型检查器确保补丁正确性。

Result: 在23个OpenID漏洞测试中，AuthFix成功生成了17个正确补丁(74%)，其中大部分补丁与开发者编写的修复方案语义等价。

Conclusion: AuthFix证明了LLM在自动化程序修复中的有效性，特别是在OpenID Connect这样的复杂领域特定系统中，能够高效生成高质量的修复补丁。

Abstract: OpenID Connect has revolutionized online authentication based on single
sign-on (SSO) by providing a secure and convenient method for accessing
multiple services with a single set of credentials. Despite its widespread
adoption, critical security bugs in OpenID Connect have resulted in significant
financial losses and security breaches, highlighting the need for robust
mitigation strategies. Automated program repair presents a promising solution
for generating candidate patches for OpenID implementations. However,
challenges such as domain-specific complexities and the necessity for precise
fault localization and patch verification must be addressed. We propose
AuthFix, a counterexample-guided repair engine leveraging LLMs for automated
OpenID bug fixing. AuthFix integrates three key components: fault localization,
patch synthesis, and patch verification. By employing a novel Petri-net-based
model checker, AuthFix ensures the correctness of patches by effectively
modeling interactions. Our evaluation on a dataset of OpenID bugs demonstrates
that AuthFix successfully generated correct patches for 17 out of 23 bugs
(74%), with a high proportion of patches semantically equivalent to
developer-written fixes.

</details>


### [12] [C2|Q>: A Robust Framework for Bridging Classical and Quantum Software Development](https://arxiv.org/abs/2510.02854)
*Boshuai Ye,Arif Ali Khan,Teemu Pihkakoski,Peng Liang,Muhammad Azeem Akbar,Matti Silveri,Lauri Malmi*

Main category: cs.SE

TL;DR: C2|Q>是一个硬件无关的量子软件开发框架，通过将经典代码转换为量子可执行程序，显著降低了量子编程的门槛，使传统软件工程师能够更容易地进行量子计算开发。


<details>
  <summary>Details</summary>
Motivation: 当前量子开发环境需要开发者处理软件栈的低层细节，包括问题编码、电路构建、算法配置等，这使得传统软件工程师难以使用量子计算。

Method: 框架采用模块化软件工程原则，分为三个核心模块：编码器（分类问题、生成量子兼容格式、构建量子电路）、部署模块（生成电路并基于保真度、运行时间和成本推荐硬件）、解码器（将量子输出解释为经典解决方案）。

Result: 编码器模块完成率达到93.8%，硬件推荐模块能正确选择最多56量子比特的设备，完整工作流处理434个Python代码片段和100个JSON输入的完成率分别为93.8%和100%。在NISQ硬件上的案例研究中，相比手动实现减少了近40倍的工作量。

Conclusion: C2|Q>框架成功地将软件工程原则应用于量子计算，显著降低了量子编程的复杂性，使传统开发者能够更容易地利用量子计算能力。

Abstract: Quantum Software Engineering (QSE) is emerging as a critical discipline to
make quantum computing accessible to a broader developer community; however,
most quantum development environments still require developers to engage with
low-level details across the software stack - including problem encoding,
circuit construction, algorithm configuration, hardware selection, and result
interpretation - making them difficult for classical software engineers to use.
To bridge this gap, we present C2|Q>: a hardware-agnostic quantum software
development framework that translates classical specifications (code) into
quantum-executable programs while preserving methodological rigor. The
framework applies modular software engineering principles by classifying the
workflow into three core modules: an encoder that classifies problems, produces
Quantum-Compatible Formats (QCFs), and constructs quantum circuits, a
deployment module that generates circuits and recommends hardware based on
fidelity, runtime, and cost, and a decoder that interprets quantum outputs into
classical solutions. In evaluation, the encoder module achieved a 93.8%
completion rate, the hardware recommendation module consistently selected the
appropriate quantum devices for workloads scaling up to 56 qubits, and the full
C2|Q>: workflow successfully processed classical specifications (434 Python
snippets and 100 JSON inputs) with completion rates of 93.8% and 100%,
respectively. For case study problems executed on publicly available NISQ
hardware, C2|Q>: reduced the required implementation effort by nearly 40X
compared to manual implementations using low-level quantum software development
kits (SDKs), with empirical runs limited to small- and medium-sized instances
consistent with current NISQ capabilities. The open-source implementation of
C2|Q>: is available at https://github.com/C2-Q/C2Q

</details>


### [13] [GramTrans: A Better Code Representation Approach in Code Generation](https://arxiv.org/abs/2510.02887)
*Zhao Zhang,Qingyuan Liang,Zeyu Sun,Yizhou Chen,Guoqing Wang,Yican Sun,Lu Zhang,Ge Li,Yingfei Xiong*

Main category: cs.SE

TL;DR: 论文提出代码表示形式的解析难度与模型性能相关的猜想：解析越容易的表示形式，模型性能越好。通过GramTrans方法将上下文无关语言转换为LL(1)类表示，实验证明该方法能显著提升代码生成模型的性能。


<details>
  <summary>Details</summary>
Motivation: 现有研究使用多种代码表示形式（纯文本、语法规则序列、语法树序列等），但缺乏对解析难度与模型效果关系的系统性理解。需要探索代码表示形式选择对模型性能的影响机制。

Method: 提出GramTrans方法，使用分层冲突消除算法将上下文无关语言转换为LL(1)类表示。在Python和Java上使用StarCoder、DeepSeek-Coder、Qwen2.5等模型进行评估，通过控制实验验证解析难度与模型性能的相关性。

Result: GramTrans在多个基准测试中 consistently 显著优于基线表示形式。解析难度与模型性能呈现强相关性，支持了论文的猜想。

Conclusion: 代码表示形式的解析难度是影响模型性能的关键因素，选择易于解析的表示形式可以显著提升代码生成模型的性能。GramTrans提供了一种通用的方法来实现这种优化。

Abstract: Code generation has shown great promise in assisting software development. A
fundamental yet underexplored question is how the choice of code representation
affects model performance. While existing studies employ various
representations, such as treating code as plain text, grammar rule sequences,
or syntax tree sequences, they lack a principled understanding of the
relationship between parsing difficulty and model effectiveness. This paper
proposes a conjecture: the easier a representation is to parse, the better
performance the model achieves. We formalize this idea using grammar classes,
where representations in simpler classes (e.g., LL(1)) are easier to parse.
Through a controlled experiment on a Python-based DSL, we show that parsing
difficulty strongly correlates with model performance. Motivated by this
finding, we present GramTrans, a general approach that automatically transforms
a context-free language into a representation within the LL(1) class. GramTrans
introduces a novel hierarchical conflict elimination algorithm, enabling a
flexible trade-off between syntactic simplicity and token efficiency. We
evaluate GramTrans on both Python and Java using three code generation models:
StarCoder 1B, DeepSeek-Coder 1.3B, and Qwen2.5 1.5B. Across multiple
benchmarks, GramTrans consistently delivers significant improvements over
baseline representations. Furthermore, our analysis of existing representations
reconfirms the strong alignment between parsing difficulty and model
performance, providing additional support for the conjecture.

</details>


### [14] [Mechanistic Interpretability of Code Correctness in LLMs via Sparse Autoencoders](https://arxiv.org/abs/2510.02917)
*Kriz Tahimic,Charibeth Cheng*

Main category: cs.SE

TL;DR: 使用稀疏自编码器分解LLM表示，识别代码正确性方向，发现注意力应集中在测试用例而非问题描述上，预测方向可作为错误警报，指导选择性干预


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型在软件开发中广泛应用，理解其内部正确性机制对于安全部署至关重要

Method: 应用稀疏自编码器分解LLM表示，使用t统计量选择预测方向，通过分离分数确定转向方向，并进行转向、注意力分析和权重正交化分析

Result: 代码正确性方向能可靠预测错误代码，修正能力虽有统计显著性但需要在修复错误和保持正确代码间权衡，指令微调后基础模型中的方向仍保持有效性

Conclusion: 代码正确性机制在预训练期间学习并在微调中被重新利用，提出了三个实际应用：提示策略应优先测试用例、预测方向可作为错误警报、预测器可指导选择性转向

Abstract: As Large Language Models become integral to software development, with
substantial portions of AI-suggested code entering production, understanding
their internal correctness mechanisms becomes critical for safe deployment. We
apply sparse autoencoders to decompose LLM representations, identifying
directions that correspond to code correctness. We select predictor directions
using t-statistics and steering directions through separation scores from base
model representations, then analyze their mechanistic properties through
steering, attention analysis, and weight orthogonalization. We find that code
correctness directions in LLMs reliably predict incorrect code, while
correction capabilities, though statistically significant, involve tradeoffs
between fixing errors and preserving correct code. Mechanistically, successful
code generation depends on attending to test cases rather than problem
descriptions. Moreover, directions identified in base models retain their
effectiveness after instruction-tuning, suggesting code correctness mechanisms
learned during pre-training are repurposed during fine-tuning. Our mechanistic
insights suggest three practical applications: prompting strategies should
prioritize test examples over elaborate problem descriptions, predictor
directions can serve as error alarms for developer review, and these same
predictors can guide selective steering, intervening only when errors are
anticipated to prevent the code corruption from constant steering.

</details>


### [15] [Model-Agnostic Correctness Assessment for LLM-Generated Code via Dynamic Internal Representation Selection](https://arxiv.org/abs/2510.02934)
*Thanh Trong Vu,Tuan-Dung Bui,Thu-Trang Nguyen,Son Nguyen,Hieu Dinh Vo*

Main category: cs.SE

TL;DR: AUTOPROBE是一种模型无关的方法，通过动态选择LLM内部最有信息量的表征来评估代码正确性，在编译性、功能性和安全性评估方面均优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖预选固定层和token位置的表征，限制了在不同模型架构和任务间的泛化能力，需要更灵活的动态选择机制。

Method: 采用基于注意力的机制学习隐藏状态的重要性分数，聚焦最相关特征，加权聚合后通过探测分类器预测代码正确性。

Result: 在多个基准测试和代码LLM上的实验显示，AUTOPROBE始终优于基线方法：安全性评估超越最先进白盒方法18%；编译性和功能性评估分别比其他方法高19%和111%。

Conclusion: 动态选择重要内部信号使AUTOPROBE成为评估各种LLM生成代码正确性的鲁棒且可泛化的解决方案。

Abstract: Large Language Models (LLMs) have demonstrated impressive capabilities in
code generation and are increasingly integrated into the software development
process. However, ensuring the correctness of LLM-generated code remains a
critical concern. Prior work has shown that the internal representations of
LLMs encode meaningful signals for assessing code correctness. Nevertheless,
the existing methods rely on representations from pre-selected/fixed layers and
token positions, which could limit its generalizability across diverse model
architectures and tasks. In this work, we introduce AUTOPROBE, a novel
model-agnostic approach that dynamically selects the most informative internal
representations for code correctness assessment. AUTOPROBE employs an
attention-based mechanism to learn importance scores for hidden states,
enabling it to focus on the most relevant features. These weighted
representations are then aggregated and passed to a probing classifier to
predict code correctness across multiple dimensions, including compilability,
functionality, and security. To evaluate the performance of AUTOPROBE, we
conduct extensive experiments across multiple benchmarks and code LLMs. Our
experimental results show that AUTOPROBE consistently outperforms the
baselines. For security assessment, AUTOPROBE surpasses the state-of-the-art
white-box approach by 18%. For compilability and functionality assessment,
AUTOPROBE demonstrates its highest robustness to code complexity, with the
performance higher than the other approaches by up to 19% and 111%,
respectively. These findings highlight that dynamically selecting important
internal signals enables AUTOPROBE to serve as a robust and generalizable
solution for assessing the correctness of code generated by various LLMs.

</details>


### [16] [Tracing and Metrics Design Patterns for Monitoring Cloud-native Applications](https://arxiv.org/abs/2510.02991)
*Carlos Albuquerque,Filipe F. Correia*

Main category: cs.SE

TL;DR: 本文提出了三种云原生应用监控设计模式：分布式追踪、应用指标和基础设施指标，以解决分布式系统可观测性挑战


<details>
  <summary>Details</summary>
Motivation: 随着云原生架构日益分布式和易变，系统问题诊断变得更具挑战性，需要应对碎片化可观测性和更困难的根因分析

Method: 基于先前工作，引入三种设计模式：分布式追踪（跨服务请求流可视化）、应用指标（结构化应用性能监控）、基础设施指标（运行环境监控）

Result: 这些模式源自行业实践和可观测性框架，为软件从业者提供指导，帮助改善延迟分析、根因检测、实时监控和异常检测

Conclusion: 三种设计模式共同构成了完整的云原生应用可观测性解决方案，能够有效提升系统可靠性和可维护性

Abstract: Observability helps ensure the reliability and maintainability of
cloud-native applications. As software architectures become increasingly
distributed and subject to change, it becomes a greater challenge to diagnose
system issues effectively, often having to deal with fragmented observability
and more difficult root cause analysis. This paper builds upon our previous
work and introduces three design patterns that address key challenges in
monitoring cloud-native applications. Distributed Tracing improves visibility
into request flows across services, aiding in latency analysis and root cause
detection, Application Metrics provides a structured approach to instrumenting
applications with meaningful performance indicators, enabling real-time
monitoring and anomaly detection, and Infrastructure Metrics focuses on
monitoring the environment in which the system is operated, helping teams
assess resource utilization, scalability, and operational health. These
patterns are derived from industry practices and observability frameworks and
aim to offer guidance for software practitioners.

</details>


### [17] [Patterns for Teaching Agile with Student Projects -- Team and Project Setup](https://arxiv.org/abs/2510.03005)
*Daniel Pinho,Petr Pícha,Filipe Correia,Přemek Brada*

Main category: cs.SE

TL;DR: 提出一个用于高等教育中敏捷软件开发教学的初步模式语言，重点关注团队和项目设置阶段的五个具体模式


<details>
  <summary>Details</summary>
Motivation: 现有的敏捷软件开发教学文献缺乏可操作的建议，大多关注框架或泛化的敏捷教学方式，而非具体的软件开发实践教学

Method: 基于高等教育背景下的教学经验，开发了一个模式语言，提出了五个针对团队和项目设置阶段的模式：限制团队规模、缩小项目范围、非关键业务项目、自组织团队和团队选择主题

Result: 提出了五个具体的教学实践模式，为开发完整的敏捷软件开发教学模式语言奠定了基础

Conclusion: 这些模式为高等教育机构提供了实用的、可操作的敏捷软件开发教学指导，有助于改进相关课程的教学效果

Abstract: Higher education courses teaching about agile software development (ASD) have
increased in commonality as the ideas behind the Agile Manifesto became more
commonplace in the industry. However, a lot of the literature on how ASD is
applied in the classroom does not provide much actionable advice, focusing on
frameworks or even moving beyond the software development area into teaching in
an agile way. We, therefore, showcase early work on a pattern language that
focuses on teaching ASD practices to university students, which stems from our
own experiences as educators in higher education contexts. We present five
patterns, specifically focused on team and project setup phase: Capping Team
Size, Smaller Project Scope, Business Non-Critical Project, Self-assembling
Teams, and Team Chooses Topic as a starting point for developing the overall
pattern language.

</details>


### [18] [Investigating The Smells of LLM Generated Code](https://arxiv.org/abs/2510.03029)
*Debalina Ghosh Paul,Hong Zhu,Ian Bayley*

Main category: cs.SE

TL;DR: LLM生成的代码比专业编写的参考代码存在更多的代码异味，平均异味增加63.34%，其中Falcon表现最好(42.28%)，Codex最差(84.97%)。代码复杂度越高，异味增加越明显。


<details>
  <summary>Details</summary>
Motivation: 评估LLM生成代码的质量而非功能性正确性，识别代码质量最差的场景以指导改进方向

Method: 基于场景的评估方法，通过测量代码异味并与专业编写的参考解决方案对比，将测试数据集按代码主题和任务复杂度划分不同场景，使用自动化测试系统对四个先进LLM生成的Java程序进行评估

Result: LLM生成代码的异味发生率显著高于参考代码，平均异味增加63.34%(实现异味73.35%，设计异味21.42%)。Falcon表现最佳(42.28%)，Codex最差(84.97%)。复杂任务和面向对象主题的异味增加更明显

Conclusion: LLM在不同编码任务复杂度和主题上的表现与对应场景下人工编写代码的质量高度相关，但LLM生成代码的质量明显低于人工编写代码

Abstract: Context: Large Language Models (LLMs) are increasingly being used to generate
program code. Much research has been reported on the functional correctness of
generated code, but there is far less on code quality.
  Objectives: In this study, we propose a scenario-based method of evaluating
the quality of LLM-generated code to identify the weakest scenarios in which
the quality of LLM generated code should be improved.
  Methods: The method measures code smells, an important indicator of code
quality, and compares them with a baseline formed from reference solutions of
professionally written code. The test dataset is divided into various subsets
according to the topics of the code and complexity of the coding tasks to
represent different scenarios of using LLMs for code generation. We will also
present an automated test system for this purpose and report experiments with
the Java programs generated in response to prompts given to four
state-of-the-art LLMs: Gemini Pro, ChatGPT, Codex, and Falcon.
  Results: We find that LLM-generated code has a higher incidence of code
smells compared to reference solutions. Falcon performed the least badly, with
a smell increase of 42.28%, followed by Gemini Pro (62.07%), ChatGPT (65.05%)
and finally Codex (84.97%). The average smell increase across all LLMs was
63.34%, comprising 73.35% for implementation smells and 21.42% for design
smells. We also found that the increase in code smells is greater for more
complex coding tasks and for more advanced topics, such as those involving
object-orientated concepts.
  Conclusion: In terms of code smells, LLM's performances on various coding
task complexities and topics are highly correlated to the quality of human
written code in the corresponding scenarios. However, the quality of LLM
generated code is noticeably poorer than human written code.

</details>


### [19] [Refactoring Towards Microservices: Preparing the Ground for Service Extraction](https://arxiv.org/abs/2510.03050)
*Rita Peixoto,Filipe F. Correia,Thatiane Rosa,Eduardo Guerra,Alfredo Goldman*

Main category: cs.SE

TL;DR: 本文提出了一个包含7种重构技术的目录，专门用于支持向微服务架构的迁移，重点关注代码级依赖关系的处理。


<details>
  <summary>Details</summary>
Motivation: 随着组织从单体系统向微服务过渡，迁移过程仍然主要依赖手动且劳动密集型工作。现有文献主要提供架构级指导，忽略了代码级挑战和依赖关系。

Method: 开发了一个包含七种重构技术的系统化目录，这些重构技术从文献中识别并整合而成，提供了结构化的逐步迁移方法。

Result: 该目录为开发者提供了系统化指南，简化了迁移过程，并为潜在的自动化奠定了基础。

Conclusion: 这项工作通过提供代码级的系统化重构方法，使开发者能够更高效有效地实施微服务迁移。

Abstract: As organizations increasingly transition from monolithic systems to
microservices, they aim to achieve higher availability, automatic scaling,
simplified infrastructure management, enhanced collaboration, and streamlined
deployments. However, this migration process remains largely manual and
labour-intensive. While existing literature offers various strategies for
decomposing monoliths, these approaches primarily focus on architecture-level
guidance, often overlooking the code-level challenges and dependencies that
developers must address during the migration. This article introduces a
catalogue of seven refactorings specifically designed to support the transition
to a microservices architecture with a focus on handling dependencies. The
catalogue provides developers with a systematic guide that consolidates
refactorings identified in the literature and addresses the critical gap in
systematizing the process at the code level. By offering a structured,
step-by-step approach, this work simplifies the migration process and lays the
groundwork for its potential automation, empowering developers to implement
these changes efficiently and effectively.

</details>


### [20] [State Field Coverage: A Metric for Oracle Quality](https://arxiv.org/abs/2510.03071)
*Facundo Molina,Nazareno Aguirre,Alessandra Gorla*

Main category: cs.SE

TL;DR: 提出了一种新的测试预言质量评估指标——状态字段覆盖率，该指标通过静态分析测量测试预言在执行期间可能访问的对象状态字段比例，能够有效指导预言改进并与故障检测能力强相关。


<details>
  <summary>Details</summary>
Motivation: 现有测试预言质量评估指标要么无法全面指导预言改进，要么局限于特定预言类型，缺乏通用性。需要一种能够全面评估预言质量并指导改进的通用指标。

Method: 提出状态字段覆盖率指标，通过静态计算测试预言在执行期间可能访问的对象状态字段比例。实现静态计算机制，高效识别未检查的状态字段。

Result: 在包含273个表示不变量和249,027个测试断言的实验中，状态字段覆盖率与基于变异评分的故障检测能力强相关，证明其适合评估预言质量。

Conclusion: 状态字段覆盖率是一种有效的测试预言质量评估指标，能够指导预言改进并提高整体测试效果。

Abstract: The effectiveness of testing in uncovering software defects depends not only
on the characteristics of the test inputs and how thoroughly they exercise the
software, but also on the quality of the oracles used to determine whether the
software behaves as expected. Therefore, assessing the quality of oracles is
crucial to improve the overall effectiveness of the testing process. Existing
metrics have been used for this purpose, but they either fail to provide a
comprehensive basis for guiding oracle improvement, or they are tailored to
specific types of oracles, thus limiting their generality.
  In this paper, we introduce state field coverage, a novel metric for
assessing oracle quality. This metric measures the proportion of an object's
state, as statically defined by its class fields, that an oracle may access
during test execution. The main intuition of our metric is that oracles with a
higher state field coverage are more likely to detect faults in the software
under analysis, as they inspect a larger portion of the object states to
determine whether tests pass or not.
  We implement a mechanism to statically compute the state field coverage
metric. Being statically computed, the metric is efficient and provides direct
guidance for improving test oracles by identifying state fields that remain
unexamined. We evaluate state field coverage through experiments involving 273
representation invariants and 249,027 test assertions. The results show that
state field coverage is a well-suited metric for assessing oracle quality, as
it strongly correlates with the oracles' fault-detection ability, measured by
mutation score.

</details>


### [21] [When Names Disappear: Revealing What LLMs Actually Understand About Code](https://arxiv.org/abs/2510.03178)
*Cuong Chi Le,Minh V. T. Pham,Cuong Duc Van,Hoang N. Phan,Huy N. Phan,Tien N. Nguyen*

Main category: cs.SE

TL;DR: 论文发现LLMs在代码任务中过度依赖命名模式而非真正的语义理解，通过语义保留的混淆技术揭示了标识符泄漏问题，并提出了ClassEval-Obf基准来更可靠地评估代码理解能力


<details>
  <summary>Details</summary>
Motivation: 探究大型语言模型如何理解代码含义，发现现有基准测试可能奖励命名模式的记忆而非真正的语义推理，需要更可靠的评估方法

Method: 引入语义保留的混淆技术来消除命名线索，创建ClassEval-Obf基准系统性地抑制命名提示同时保持代码行为

Result: 移除命名通道严重降低了意图级任务性能，在执行任务中也观察到一致性能下降，混淆技术暴露了标识符泄漏问题

Conclusion: ClassEval-Obf基准减少了性能膨胀差距，削弱了记忆捷径，为评估LLMs的代码理解和泛化能力提供了更可靠的基础

Abstract: Large Language Models (LLMs) achieve strong results on code tasks, but how
they derive program meaning remains unclear. We argue that code communicates
through two channels: structural semantics, which define formal behavior, and
human-interpretable naming, which conveys intent. Removing the naming channel
severely degrades intent-level tasks such as summarization, where models
regress to line-by-line descriptions. Surprisingly, we also observe consistent
reductions on execution tasks that should depend only on structure, revealing
that current benchmarks reward memorization of naming patterns rather than
genuine semantic reasoning. To disentangle these effects, we introduce a suite
of semantics-preserving obfuscations and show that they expose identifier
leakage across both summarization and execution. Building on these insights, we
release ClassEval-Obf, an obfuscation-enhanced benchmark that systematically
suppresses naming cues while preserving behavior. Our results demonstrate that
ClassEval-Obf reduces inflated performance gaps, weakens memorization
shortcuts, and provides a more reliable basis for assessing LLMs' code
understanding and generalization.

</details>


### [22] [Abstain and Validate: A Dual-LLM Policy for Reducing Noise in Agentic Program Repair](https://arxiv.org/abs/2510.03217)
*José Cambronero,Michele Tufano,Sherry Shi,Renyao Wei,Grant Uy,Runxiang Cheng,Chin-Jung Liu,Shiying Pan,Satish Chandra,Pat Rondon*

Main category: cs.SE

TL;DR: 论文提出了两种LLM策略（bug abstention和patch validation）来减少自动化程序修复系统中的噪声，通过排除难以修复的bug和验证补丁质量，显著提高了修复成功率


<details>
  <summary>Details</summary>
Motivation: 自动化程序修复系统在处理复杂bug时会产生大量需要人工审核的低质量补丁，这浪费开发者时间并降低对自动化代码变更的信任

Method: 引入两种互补的LLM策略：bug abstention策略排除系统难以修复的bug，patch validation策略拒绝不太可能是良好修复的补丁

Result: 在Google代码库的174个人工报告bug上，两种策略组合使用可将成功率提高39个百分点；在空指针异常和sanitizer报告bug上也提高了平均单样本成功率

Conclusion: 这种双策略方法为agentic APR系统在工业规模上的可靠部署提供了实用路径

Abstract: Agentic Automated Program Repair (APR) is increasingly tackling complex,
repository-level bugs in industry, but ultimately agent-generated patches still
need to be reviewed by a human before committing them to ensure they address
the bug. Showing unlikely patches to developers can lead to substantial noise,
wasting valuable developer time and eroding trust in automated code changes. We
introduce two complementary LLM-based policies to reduce such noise: bug
abstention and patch validation policies. Bug abstention excludes bugs that the
agentic APR system is unlikely to fix. Patch validation rejects patches that
are unlikely to be a good fix for the given bug. We evaluate both policies on
three sets of bugs from Google's codebase, and their candidate patches
generated by an internal agentic APR system. On a set of 174 human-reported
bugs, removing bugs and patch trajectories rejected by our policies can raise
success rates by up to 13 percentage points and 15 percentage points,
respectively, and by up to 39 percentage points in combination. On null pointer
exceptions and sanitizer-reported bugs with machine-generated bug reports,
patch validation also improves average single-sample success rates. This
two-policy approach provides a practical path to the reliable, industrial-scale
deployment of agentic APR systems.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [23] [HALO: Memory-Centric Heterogeneous Accelerator with 2.5D Integration for Low-Batch LLM Inference](https://arxiv.org/abs/2510.02675)
*Shubham Negi,Kaushik Roy*

Main category: cs.AR

TL;DR: HALO是一种异构内存中心加速器，专门针对LLM推理中的prefill和decode阶段设计，结合HBM CiD和片上模拟CiM，在低批次长上下文场景下实现显著性能提升


<details>
  <summary>Details</summary>
Motivation: LLM推理在交互式应用中需要低延迟，但现有工作主要针对高批次推理或短上下文，低批次长上下文场景研究不足，prefill和decode阶段的异构计算内存需求给加速器设计带来挑战

Method: 采用2.5D集成将HBM CiD与片上模拟CiM结合，提出阶段感知映射策略：prefill阶段计算密集型操作映射到CiM，decode阶段内存密集型操作映射到CiD，减少数据移动

Result: 在LLaMA-2 7B和Qwen3 8B模型上测试，相比AttAcc实现18倍几何平均加速，相比全CiD设计的CENT实现2.5倍加速

Conclusion: 异构内存架构HALO能有效应对LLM推理中prefill和decode阶段的差异化需求，在低批次长上下文场景下显著提升性能

Abstract: The rapid adoption of Large Language Models (LLMs) has driven a growing
demand for efficient inference, particularly in latency-sensitive applications
such as chatbots and personalized assistants. Unlike traditional deep neural
networks, LLM inference proceeds in two distinct phases: the prefill phase,
which processes the full input sequence in parallel, and the decode phase,
which generates tokens sequentially. These phases exhibit highly diverse
compute and memory requirements, which makes accelerator design particularly
challenging. Prior works have primarily been optimized for high-batch inference
or evaluated only short input context lengths, leaving the low-batch and long
context regime, which is critical for interactive applications, largely
underexplored.
  We propose HALO, a heterogeneous memory centric accelerator designed for
these unique challenges of prefill and decode phases in low-batch LLM
inference. HALO integrates HBM based Compute-in-DRAM (CiD) with an on-chip
analog Compute-in-Memory (CiM), co-packaged using 2.5D integration. To further
improve the hardware utilization, we introduce a phase-aware mapping strategy
that adapts to the distinct demands of the prefill and decode phases. Compute
bound operations in the prefill phase are mapped to CiM to exploit its high
throughput matrix multiplication capability, while memory-bound operations in
the decode phase are executed on CiD to benefit from reduced data movement
within DRAM. Additionally, we present an analysis of the performance tradeoffs
of LLMs under two architectural extremes: a fully CiD and a fully on-chip
analog CiM design to highlight the need for a heterogeneous design. We evaluate
HALO on LLaMA-2 7B and Qwen3 8B models. Our experimental results show that LLMs
mapped to HALO achieve up to 18x geometric mean speedup over AttAcc, an
attention-optimized mapping and 2.5x over CENT, a fully CiD based mapping.

</details>


### [24] [A Hardware Accelerator for the Goemans-Williamson Algorithm](https://arxiv.org/abs/2510.02863)
*D. A. Herrera-Martí,E. Guthmuller,J. Fereyre*

Main category: cs.AR

TL;DR: 本文探讨了在凸优化中引入扩展浮点精度来加速大规模Max-Cut问题的求解，特别是在共轭梯度法等间接矩阵求逆算法中的应用。


<details>
  <summary>Details</summary>
Motivation: Max-Cut问题已成为量子与经典优化器局部搜索启发式算法的基准测试。与仅提供平均性能保证的局部搜索不同，Goemans-Williamson的凸半定松弛方法提供最坏情况保证，适用于基准构建和性能关键场景。

Method: 研究如何在凸优化的代数子程序（特别是共轭梯度法等间接矩阵求逆方法）中整合扩展浮点精度，这些方法在内点法中用于处理超大规模问题。

Result: 使用扩展精度可减少求解时间，该加速因子随系统规模增大而增加。当使用复杂度低于直接方法的间接矩阵求逆方法时，提高内部工作精度能显著提升大规模问题的求解效率。

Conclusion: 扩展浮点精度在大规模凸优化问题中具有实际价值，特别是在使用共轭梯度法等间接方法时，能够提供可扩展的性能加速。

Abstract: The combinatorial problem Max-Cut has become a benchmark in the evaluation of
local search heuristics for both quantum and classical optimisers. In contrast
to local search, which only provides average-case performance guarantees, the
convex semidefinite relaxation of Max-Cut by Goemans and Williamson, provides
worst-case guarantees and is therefore suited to both the construction of
benchmarks and in applications to performance-critic scenarios.
  We show how extended floating point precision can be incorporated in
algebraic subroutines in convex optimisation, namely in indirect matrix
inversion methods like Conjugate Gradient, which are used in Interior Point
Methods in the case of very large problem sizes. Also, an estimate is provided
of the expected acceleration of the time to solution for a hardware
architecture that runs natively on extended precision. Specifically, when using
indirect matrix inversion methods like Conjugate Gradient, which have lower
complexity than direct methods and are therefore used in very large problems,
we see that increasing the internal working precision reduces the time to
solution by a factor that increases with the system size.

</details>


### [25] [A Resource-Driven Approach for Implementing CNNs on FPGAs Using Adaptive IPs](https://arxiv.org/abs/2510.02990)
*Philippe Magalhães,Virginie Fresse,Benoît Suffran,Olivier Alata*

Main category: cs.AR

TL;DR: 提出了一种基于FPGA的资源高效卷积IP库，可自动适配可用资源，采用参数化设计和定点运算，在Zynq UltraScale+ FPGA上验证了性能与资源使用的权衡。


<details>
  <summary>Details</summary>
Motivation: 随着实时低延迟AI应用需求的增长，FPGA在CNN实现中相比GPU具有可重构性、能效和性能优势，特别适合边缘设备和嵌入式系统。

Method: 开发了用VHDL编写的参数化卷积IP库，采用定点运算，包含四个针对不同资源约束定制的IP核，提供DSP使用、逻辑消耗和精度的灵活性。

Result: 在Zynq UltraScale+ FPGA上的实验结果显示性能与资源使用的权衡，与现有FPGA加速技术相比展现出更好的通用性和架构独立性。

Conclusion: 该方法为FPGA上的CNN加速提供了灵活高效的解决方案，未来将扩展库功能以支持池化和激活函数，增强在CNN框架中的集成能力。

Abstract: The increasing demand for real-time, low-latency artificial intelligence
applications has propelled the use of Field-Programmable Gate Arrays (FPGAs)
for Convolutional Neural Network (CNN) implementations. FPGAs offer
reconfigurability, energy efficiency, and performance advantages over GPUs,
making them suitable for edge devices and embedded systems. This work presents
a novel library of resource-efficient convolution IPs designed to automatically
adapt to the available FPGA resources. Developed in VHDL, these IPs are
parameterizable and utilize fixed-point arithmetic for optimal performance.
Four IPs are introduced, each tailored to specific resource constraints,
offering flexibility in DSP usage, logic consumption, and precision.
Experimental results on a Zynq UltraScale+ FPGA highlight the trade-offs
between performance and resource usage. The comparison with recent FPGA-based
CNN acceleration techniques emphasizes the versatility and independence of this
approach from specific FPGA architectures or technological advancements. Future
work will expand the library to include pooling and activation functions,
enabling broader applicability and integration into CNN frameworks.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [26] [On the energy efficiency of sparse matrix computations on multi-GPU clusters](https://arxiv.org/abs/2510.02878)
*Massimo Bernaschi,Alessandro Celestini,Pasqua D'Ambra,Giorgio Richelli*

Main category: cs.DC

TL;DR: 研究稀疏矩阵并行计算库的能效表现，该库利用GPU加速器实现大规模科学计算，在解决超出单节点内存容量的稀疏线性系统时优化多GPU使用算法，并提供了精确的能耗测量方法和结果。


<details>
  <summary>Details</summary>
Motivation: 满足现代高性能计算平台日益增长的可持续性需求，扩展之前在大规模GPU系统上已证明的性能效率研究，提供能耗分析以全面评估计算库的能效表现。

Method: 开发了精确的运行时能耗测量方法和工具，对库的核心组件进行能耗分析，优化GPU计算并最小化内存和计算节点间的数据移动。

Result: 确认优化GPU计算和减少数据移动既能缩短求解时间又能降低能耗，相比同类软件框架在标准基准测试中展现出显著优势。

Conclusion: 该稀疏矩阵并行计算库在保持高性能的同时实现了优异的能效表现，为大规模科学计算应用提供了可持续的高性能解决方案。

Abstract: We investigate the energy efficiency of a library designed for parallel
computations with sparse matrices. The library leverages high-performance,
energy-efficient Graphics Processing Unit (GPU) accelerators to enable
large-scale scientific applications. Our primary development objective was to
maximize parallel performance and scalability in solving sparse linear systems
whose dimensions far exceed the memory capacity of a single node. To this end,
we devised methods that expose a high degree of parallelism while optimizing
algorithmic implementations for efficient multi-GPU usage. Previous work has
already demonstrated the library's performance efficiency on large-scale
systems comprising thousands of NVIDIA GPUs, achieving improvements over
state-of-the-art solutions. In this paper, we extend those results by providing
energy profiles that address the growing sustainability requirements of modern
HPC platforms. We present our methodology and tools for accurate runtime energy
measurements of the library's core components and discuss the findings. Our
results confirm that optimizing GPU computations and minimizing data movement
across memory and computing nodes reduces both time-to-solution and energy
consumption. Moreover, we show that the library delivers substantial advantages
over comparable software frameworks on standard benchmarks.

</details>


### [27] [ElasticMoE: An Efficient Auto Scaling Method for Mixture-of-Experts Models](https://arxiv.org/abs/2510.02613)
*Gursimran Singh,Timothy Yu,Haley Li,Cheng Chen,Hanieh Sadri,Qintao Zhang,Yu Zhang,Ying Xiong,Yong Zhang,Zhenan Fan*

Main category: cs.DC

TL;DR: ElasticMoE是一个用于混合专家模型(MoE)的弹性扩展框架，通过解耦推理执行和内存操作，实现细粒度、低延迟、零停机扩展


<details>
  <summary>Details</summary>
Motivation: 现有MoE模型的扩展策略存在粒度粗、配置延迟长、成本高和需要停机重启等问题，无法适应云部署中常见的突发性短时流量模式

Method: 采用HBM管理模块重用权重和KV缓存，通过零拷贝重映射；使用高带宽点对点传输添加新加速器；基于虚拟内存的专家重分配机制迁移专家

Result: 在Ascend NPU上的评估显示，ElasticMoE相比基线实现了9倍更低的扩展延迟、2倍更好的吞吐量，显著改善了SLO达成率

Conclusion: ElasticMoE通过最小干扰的细粒度并发扩展，提升了大规模MoE LLM在动态云环境中部署的实用性

Abstract: Mixture-of-Experts (MoE) models promise efficient scaling of large language
models (LLMs) by activating only a small subset of experts per token, but their
parallelized inference pipelines make elastic serving challenging. Existing
strategies fall short: horizontal scaling provisions entire replicas of the
current configuration, often tens to hundreds of accelerators, leading to
coarse granularity, long provisioning delays, and costly overprovisioning.
Vertical scaling offers finer adjustments but typically requires instance
restarts, incurring downtime. These limitations make current approaches
ill-suited for the bursty, short-lived traffic patterns common in cloud
deployments.
  We present ElasticMoE, an elastic scaling framework for MoE LLMs that
achieves fine-grained, low-latency, and zero-downtime scaling. ElasticMoE
decouples inference execution from memory operations, enabling scaling steps to
proceed concurrently with serving. An HBM Management Module (HMM) reuses
weights and KV caches via zero-copy remapping, while high-bandwidth
peer-to-peer transfers bring newly added accelerators online without
interrupting service. A virtual memory based expert redistribution mechanism
migrates MoE experts without costly buffer reallocations, reducing peak memory
usage during expert parallelism reconfiguration.
  Our evaluation on Ascend NPUs with three popular MoE LLMs shows that
ElasticMoE achieves up to 9x lower scale-up latency, up to 2x better throughput
during scaling, and significantly improves SLO attainment compared to
baselines. By enabling fine-grained, concurrent scaling with minimal
disruption, ElasticMoE advances the practicality of deploying massive MoE LLMs
in dynamic cloud environments.

</details>


### [28] [GRNND: A GPU-Parallel Relative NN-Descent Algorithm for Efficient Approximate Nearest Neighbor Graph Construction](https://arxiv.org/abs/2510.02774)
*Xiang Li,Qiong Chang,Yun Li,Jun Miyazaki*

Main category: cs.DC

TL;DR: GRNND是首个针对RNN-Descent算法的GPU并行实现，通过无序邻居传播策略和GPU架构优化，显著提升了大规模高维数据下的近似最近邻图构建效率。


<details>
  <summary>Details</summary>
Motivation: 随着数据量和维度的增加，RNN-Descent算法在图构建阶段的复杂度急剧上升，变得非常耗时，甚至阻碍后续查询处理。

Method: 提出GRNND算法，采用无序邻居传播策略缓解同步更新陷阱，利用warp级协作操作和双缓冲邻居池实现高效内存访问和无冲突的并行邻居更新。

Result: GRNND相比现有GPU方法获得2.4-51.7倍加速，相比CPU方法获得17.8-49.8倍加速，在多个实验中均优于现有方法。

Conclusion: GRNND成功实现了RNN-Descent算法的GPU并行化，通过创新的并行策略和内存优化，显著提升了大规模近似最近邻图构建的性能。

Abstract: Relative Nearest Neighbor Descent (RNN-Descent) is a state-of-the-art
algorithm for constructing sparse approximate nearest neighbor (ANN) graphs by
combining the iterative refinement of NN-Descent with the edge-pruning rules of
the Relative Neighborhood Graph (RNG). It has demonstrated strong effectiveness
in large-scale search tasks such as information retrieval and related tasks.
However, as the amount and dimensionality of data increase, the complexity of
graph construction in RNN-Descent rises sharply, making this stage increasingly
time-consuming and even prohibitive for subsequent query processing. In this
paper, we propose GRNND, the first GPU-parallel algorithm of RNN-Descent
designed to fully exploit GPU architecture. GRNND introduces a disordered
neighbor propagation strategy to mitigate synchronized update traps, enhancing
structural diversity, and avoiding premature convergence during parallel
execution. It also leverages warp-level cooperative operations and a
double-buffered neighbor pool with fixed capacity for efficient memory access,
eliminate contention, and enable highly parallelized neighbor updates.
Extensive experiments demonstrate that GRNND consistently outperforms existing
CPU- and GPU-based methods. GRNND achieves 2.4 to 51.7x speedup over existing
GPU methods, and 17.8 to 49.8x speedup over CPU methods.

</details>


### [29] [TridentServe: A Stage-level Serving System for Diffusion Pipelines](https://arxiv.org/abs/2510.02838)
*Yifei Xia,Fangcheng Fu,Hao Yuan,Hanke Zhang,Xupeng Miao,Yijun Liu,Suhan Ling,Jie Jiang,Bin Cui*

Main category: cs.DC

TL;DR: TridentServe是一个动态阶段级扩散模型服务系统，通过自动优化模型部署和请求路由，显著提升服务效率。


<details>
  <summary>Details</summary>
Motivation: 当前扩散模型服务系统采用静态、手动、管道级的资源分配方式，无法适应不同阶段和请求的资源需求差异，导致效率低下。

Method: 提出动态阶段级服务范式，自动推导管道部署的放置计划和请求处理的路由计划，协同优化模型和请求的资源分配。

Result: 实验表明TridentServe在各种工作负载下，相比现有工作将SLO达成率提升2.5倍，平均/P95延迟降低3.6倍/4.1倍。

Conclusion: 动态阶段级服务范式能有效解决扩散管道服务中的资源分配效率问题，显著提升系统性能。

Abstract: Diffusion pipelines, renowned for their powerful visual generation
capabilities, have seen widespread adoption in generative vision tasks (e.g.,
text-to-image/video). These pipelines typically follow an
encode--diffuse--decode three-stage architecture. Current serving systems
deploy diffusion pipelines within a static, manual, and pipeline-level
paradigm, allocating the same resources to every request and stage. However,
through an in-depth analysis, we find that such a paradigm is inefficient due
to the discrepancy in resource needs across the three stages of each request,
as well as across different requests. Following the analysis, we propose the
dynamic stage-level serving paradigm and develop TridentServe, a brand new
diffusion serving system. TridentServe automatically, dynamically derives the
placement plan (i.e., how each stage resides) for pipeline deployment and the
dispatch plan (i.e., how the requests are routed) for request processing,
co-optimizing the resource allocation for both model and requests. Extensive
experiments show that TridentServe consistently improves SLO attainment and
reduces average/P95 latencies by up to 2.5x and 3.6x/4.1x over existing works
across a variety of workloads.

</details>


### [30] [Energy Efficiency in Cloud-Based Big Data Processing for Earth Observation: Gap Analysis and Future Directions](https://arxiv.org/abs/2510.02882)
*Adhitya Bhawiyuga,Serkan Girgin,Rolf A. de By,Raul Zurita-Milla*

Main category: cs.DC

TL;DR: 论文分析了云处理地球观测大数据(EOBD)中的能源效率问题，指出了现有平台在能源监控、数据管理、资源分配和任务调度方面的不足，并提出了能源感知的性能监控框架、基础设施优化和节能任务调度等研究方向。


<details>
  <summary>Details</summary>
Motivation: 随着地球观测数据量快速增长和云计算广泛应用，能源效率和碳足迹问题在大数据处理中日益重要，但云处理EOBD的能源效率方面研究不足，需要填补这一空白。

Method: 通过分析当前EOBD处理现状、云处理需求以及现有解决方案，借鉴其他大数据领域的成功能源效率策略，识别EOBD处理平台的关键能源效率差距。

Result: 识别出四个关键能源效率差距：能源监控机制不足、数据管理缺乏能源意识、能源感知资源分配实施不足、任务调度缺乏能源效率标准。

Conclusion: 提出了开发能源感知性能监控框架、基础设施优化技术和节能任务调度方法的研究方向，旨在提高EOBD处理的能源意识，减少能耗和环境影响，同时保持处理性能。

Abstract: Earth observation (EO) data volumes are rapidly increasing. While cloud
computing are now used for processing large EO datasets, the energy efficiency
aspects of such a processing have received much less attention. This issue is
notable given the increasing awareness of energy costs and carbon footprint in
big data processing, particularly with increased attention on compute-intensive
foundation models. In this paper we identify gaps in energy efficiency
practices within cloud-based EO big data (EOBD) processing and propose several
research directions for improvement. We first examine the current EOBD
landscape, focus on the requirements that necessitate cloud-based processing
and analyze existing cloud-based EOBD solutions. We then investigate energy
efficiency strategies that have been successfully employed in well-studied big
data domains. Through this analysis, we identify several critical gaps in
existing EOBD processing platforms, which primarily focus on data accessibility
and computational feasibility, instead of energy efficiency. These gaps include
insufficient energy monitoring mechanisms, lack of energy awareness in data
management, inadequate implementation of energy-aware resource allocation and
lack of energy efficiency criteria on task scheduling. Based on these findings,
we propose the development of energy-aware performance monitoring and
benchmarking frameworks, the use of optimization techniques for infrastructure
orchestration, and of energy-efficient task scheduling approaches for
distributed cloud-based EOBD processing frameworks. These proposed approaches
aim to foster more energy awareness in EOBD processing , potentially reducing
power consumption and environmental impact while maintaining or minimally
impacting processing performance.

</details>


### [31] [PyRadiomics-cuda: a GPU-accelerated 3D features extraction from medical images within PyRadiomics](https://arxiv.org/abs/2510.02894)
*Jakub Lisowski,Piotr Tyrakowski,Szymon Zyguła,Krzysztof Kaczmarski*

Main category: cs.DC

TL;DR: PyRadiomics-cuda是一个GPU加速的医学影像特征提取工具，通过CUDA实现显著提升处理速度，保持与原有API兼容性


<details>
  <summary>Details</summary>
Motivation: 解决医学影像三维形状特征提取的计算挑战，特别是处理大型体数据集时的性能瓶颈问题

Method: 通过将关键几何计算卸载到GPU硬件，使用Python和C/CUDA实现GPU加速，保持与PyRadiomics API的完全兼容

Result: 在各种计算设备上（计算集群、预算设备和家用设备）都显著减少了处理时间，支持高效可扩展的影像组学分析

Conclusion: PyRadiomics-cuda为AI工作流程提供了透明的加速解决方案，支持高通量AI管道所需的快速特征提取，代码开源且易于集成

Abstract: PyRadiomics-cuda is a GPU-accelerated extension of the PyRadiomics library,
designed to address the computational challenges of extracting
three-dimensional shape features from medical images. By offloading key
geometric computations to GPU hardware it dramatically reduces processing times
for large volumetric datasets. The system maintains full compatibility with the
original PyRadiomics API, enabling seamless integration into existing AI
workflows without code modifications. This transparent acceleration facilitates
efficient, scalable radiomics analysis, supporting rapid feature extraction
essential for high-throughput AI pipeline. Tests performed on a typical
computational cluster, budget and home devices prove usefulness in all
scenarios. PyRadiomics-cuda is implemented in Python and C/CUDA and is freely
available under the BSD license at https://github.com/mis-wut/pyradiomics-CUDA
Additionally PyRadiomics-cuda test suite is available at
https://github.com/mis-wut/pyradiomics-cuda-data-gen. It provides detailed
handbook and sample scripts suited for different kinds of workflows plus
detailed installation instructions. The dataset used for testing is available
at Kaggle
https://www.kaggle.com/datasets/sabahesaraki/kidney-tumor-segmentation-challengekits-19

</details>


### [32] [iDDS: Intelligent Distributed Dispatch and Scheduling for Workflow Orchestration](https://arxiv.org/abs/2510.02930)
*Wen Guan,Tadashi Maeno,Aleksandr Alekseev,Fernando Harald Barreiro Megino,Kaushik De,Edward Karavakis,Alexei Klimentov,Tatiana Korchuganova,FaHui Lin,Paul Nilsson,Torre Wenaus,Zhaoyu Yang,Xin Zhao*

Main category: cs.DC

TL;DR: iDDS是一个智能分布式调度和工作流编排系统，专为大规模分布式科学计算设计，集成了数据感知执行、条件逻辑和可编程工作流，支持复杂动态处理管道的自动化。


<details>
  <summary>Details</summary>
Motivation: 传统的工作负载和数据管理方法无法满足大规模分布式科学计算中复杂动态处理管道的需求，需要开发一个能够统一调度、数据移动和自适应决策的系统来降低操作开销并实现可重复的高吞吐量工作流。

Method: 采用模块化消息驱动架构，集成PanDA和Rucio等系统，支持模板驱动工作流和基于Python的Function-as-a-Task模型，实现数据感知执行和条件逻辑。

Result: 成功应用于多个真实场景：ATLAS实验的磁带资源优化、Rubin天文台的大规模DAG工作流编排、机器学习分布式超参数优化、物理分析主动学习以及EIC的AI辅助探测器设计。

Conclusion: iDDS通过统一工作负载调度、数据移动和自适应决策，显著减少了操作开销，支持跨异构基础设施的可重复高吞吐量工作流，未来将扩展到交互式、云原生和无服务器工作流支持。

Abstract: The intelligent Distributed Dispatch and Scheduling (iDDS) service is a
versatile workflow orchestration system designed for large-scale, distributed
scientific computing. iDDS extends traditional workload and data management by
integrating data-aware execution, conditional logic, and programmable
workflows, enabling automation of complex and dynamic processing pipelines.
Originally developed for the ATLAS experiment at the Large Hadron Collider,
iDDS has evolved into an experiment-agnostic platform that supports both
template-driven workflows and a Function-as-a-Task model for Python-based
orchestration.
  This paper presents the architecture and core components of iDDS,
highlighting its scalability, modular message-driven design, and integration
with systems such as PanDA and Rucio. We demonstrate its versatility through
real-world use cases: fine-grained tape resource optimization for ATLAS,
orchestration of large Directed Acyclic Graph (DAG) workflows for the Rubin
Observatory, distributed hyperparameter optimization for machine learning
applications, active learning for physics analyses, and AI-assisted detector
design at the Electron-Ion Collider.
  By unifying workload scheduling, data movement, and adaptive decision-making,
iDDS reduces operational overhead and enables reproducible, high-throughput
workflows across heterogeneous infrastructures. We conclude with current
challenges and future directions, including interactive, cloud-native, and
serverless workflow support.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [33] [Interplay between Security, Privacy and Trust in 6G-enabled Intelligent Transportation Systems](https://arxiv.org/abs/2510.02487)
*Ahmed Danladi Abdullahi,Erfan Bahrami,Tooska Dargahi,Mohammed Al-Khalidi,Mohammad Hammoudeh*

Main category: cs.NI

TL;DR: 本文综述了6G智能交通系统的机遇与挑战，重点关注信任、安全和隐私问题，特别探讨了量子技术带来的双重影响——通过量子密钥分发增强安全性，同时也引入新的漏洞。


<details>
  <summary>Details</summary>
Motivation: 6G技术有望彻底改变交通行业，但必须解决各种安全和隐私挑战，以确保6G-ITS的安全部署和用户信任。

Method: 提出了6G-ITS中不同攻击模型的分类法，比较了5G-ITS和6G-ITS的安全威胁，并提供了潜在的缓解解决方案。

Result: 研究发现需要构建一个全面的多层安全框架，涵盖物理基础设施保护、网络协议安全、数据管理保障、应用安全措施和信任管理系统。

Conclusion: 为确保未来交通生态系统的完整性和韧性，迫切需要采用综合性的安全方法来应对新兴的安全和隐私风险。

Abstract: The advancement of 6G technology has the potential to revolutionize the
transportation sector and significantly improve how we travel. 6G-enabled
Intelligent Transportation Systems (ITS) promise to offer high-speed,
low-latency communication and advanced data analytics capabilities, supporting
the development of safer, more efficient, and more sustainable transportation
solutions. However, various security and privacy challenges were identified in
the literature that must be addressed to enable the safe and secure deployment
of 6G-ITS and ensure people's trust in using these technologies. This paper
reviews the opportunities and challenges of 6G-ITS, particularly focusing on
trust, security, and privacy, with special attention to quantum technologies
that both enhance security through quantum key distribution and introduce new
vulnerabilities. It discusses the potential benefits of 6G technology in the
transportation sector, including improved communication, device
interoperability support, data analytic capabilities, and increased automation
for different components, such as transportation management and communication
systems. A taxonomy of different attack models in 6G-ITS is proposed, and a
comparison of the security threats in 5G-ITS and 6G-ITS is provided, along with
potential mitigating solutions. This research highlights the urgent need for a
comprehensive, multi-layered security framework spanning physical
infrastructure protection, network protocol security, data management
safeguards, application security measures, and trust management systems to
effectively mitigate emerging security and privacy risks and ensure the
integrity and resilience of future transportation ecosystems.

</details>


### [34] [L4Span: Spanning Congestion Signaling over NextG Networks for Interactive Applications](https://arxiv.org/abs/2510.02682)
*Haoran Wan,Kyle Jamieson*

Main category: cs.NI

TL;DR: L4Span是一种新型RAN设计，通过简化RAN队列状态的抽象接口，将RAN队列状态与端到端低延迟信令连接，显著降低延迟同时保持高吞吐量。


<details>
  <summary>Details</summary>
Motivation: 有线宽带ISP已部署先进的队列占用信令机制，但蜂窝无线接入网络（RAN）在这方面落后，需要一种能够增量部署且兼容现有标准的低延迟解决方案。

Method: L4Span在毫秒级时间尺度预测RAN队列占用情况，为低延迟和传统流量执行ECN标记，采用轻量级设计，最小化RAN修改，保持3GPP和O-RAN兼容性。

Result: 评估显示L4Span将低延迟和传统流量的单向延迟降低高达98%，同时维持接近线速的吞吐量。

Conclusion: L4Span为RAN提供了一种有效的低延迟解决方案，易于部署且性能显著，有助于推动交互式应用的发展。

Abstract: Design for low latency networking is essential for tomorrow's interactive
applications, but it is essential to deploy incrementally and universally at
the network's last mile. While wired broadband ISPs are rolling out the leading
queue occupancy signaling mechanisms, the cellular Radio Access Network (RAN),
another important last mile to many users, lags behind these efforts. This
paper proposes a new RAN design, L4Span, that abstracts the complexities of RAN
queueing in a simple interface, thus tying the queue state of the RAN to
end-to-end low-latency signaling all the way back to the content server. At
millisecond-level timescales, L4Span predicts the RAN's queuing occupancy and
performs ECN marking for both low-latency and classic flows. L4Span is
lightweight, requiring minimal RAN modifications, and remains 3GPP and O-RAN
compliant for maximum ease of deployment. We implement a prototype on the
srsRAN open-source software in C++. Our evaluation compares the performance of
low-latency as well as classic flows with or without the deployment of L4Span
in various wireless channel conditions. Results show that L4Span reduces the
one-way delay of both low-latency and classic flows by up to 98 %, while
simultaneously maintaining near line-rate throughput. The code is available at
https://github.com/PrincetonUniversity/L4Span.

</details>


### [35] [FSMA: Scalable and Reliable LoRa for Non-Terrestrial Networks with Mobile Gateways](https://arxiv.org/abs/2510.02800)
*Rohith Reddy Vennam,Maiyun Zhang,Raghav Subbaraman,Deepak Vashist,Dinesh Bharadia*

Main category: cs.NI

TL;DR: FSMA协议通过FreeChirp信号实现免同步的链路感知接入，在移动网关的非地面网络中显著提升吞吐量、包接收率和能效


<details>
  <summary>Details</summary>
Motivation: LEO卫星和无人机等非地面网络面临大覆盖范围导致的频繁碰撞和移动网关带来的动态链路挑战，现有随机接入协议无法有效应对

Method: 提出Free Signal Multiple Access (FSMA)协议，使用轻量级FreeChirp信号确保节点仅在信道空闲且链路可靠时传输

Result: 实验显示相比基线方法，吞吐量提升2倍，包接收率提升2-5倍，能效提升5倍；大规模仿真支持5000+设备/卫星过境

Conclusion: FSMA为实现可扩展、高能效、可靠的非地面网络物联网提供了实用解决方案

Abstract: The proliferation of Low Earth Orbit (LEO) satellites for universal IoT
applications and the growing use of drones in emergency services, agriculture,
and military operations highlight the transformative potential of
non-terrestrial networks (NTN). However, these networks face two key
challenges: (1) large coverage footprints that create frequent collisions and
(2) moving gateways that cause dynamic links and demand synchronization-free,
link-aware transmissions. Existing random access schemes such as ALOHA, CSMA,
and BSMA fail in this setting, suffering from high collision rates, hidden
terminals, or excessive gateway energy overhead. We propose Free Signal
Multiple Access (FSMA), a gateway-controlled protocol that introduces a
lightweight free signal chirp (FreeChirp). FreeChirp ensures that nodes
transmit only when the channel is idle and when links are reliable, thereby
reducing collisions and enabling link-aware access without the need for
synchronization or complex scheduling. We evaluate FSMA using 25 commercial
LoRa devices with a drone-mounted moving gateway and demonstrate up to 2x
higher throughput, 2x to 5x better packet reception ratio, and 5x improved
energy efficiency compared to the baselines. Large-scale simulations with a
custom Satellite IoT Simulator further show that FSMA scales to 5000+ devices
per satellite pass. These results establish FSMA as a practical step toward
scalable, energy-efficient, and reliable NTN IoT networks.

</details>


### [36] [DH-EAC: Design of a Dynamic, Hierarchical Entanglement Access Control Protocol](https://arxiv.org/abs/2510.02895)
*Akihisa Takahashi,Yoshito Tobe*

Main category: cs.NI

TL;DR: DH-EAC是一种纯量子协议，用于在由多个量子局域网组成的广域量子网络中公平匿名地分配稀缺纠缠资源，采用双层量子抽签机制实现无经典信号争用解决。


<details>
  <summary>Details</summary>
Motivation: 现有基于Dicke态的纯量子MAC协议主要针对静态单量子局域网场景，扩展到广域动态环境且避免后选择协调仍是一个开放问题。需要设计能够同时满足匿名性和公平性的多量子网络纠缠访问控制协议。

Method: 采用双层纯量子抽签机制：外层选择获胜量子局域网，内层选择每个获胜局域网内的获胜节点。关键设计原则是通过测量单独确定获胜集合和每个局域网的配额，无需经典往返通信。

Result: 在标准i.i.d.损耗模型下分析了成功概率和延迟性能，与单层Dicke协议和经典GO驱动分配器对比评估。结果显示DH-EAC在成功概率、端到端延迟、吞吐量和Jain公平性指数等指标上表现良好。

Conclusion: DH-EAC为纠缠访问控制提供了一个可实现的平衡点，在纯量子争用解决、匿名性和多量子网络可扩展性之间取得了良好平衡。

Abstract: We propose Dynamic, Hierarchical Entanglement Access Control (DH-EAC), a
pure-quantum protocol for fair and anonymous allocation of scarce entanglement
across wide-area quantum networks composed of many quantum LANs (QLANs). Prior
Dicke-state-based pure-quantum MACs resolve contention by local measurements
without classical signaling, but they mainly target a single QLAN under static
conditions; extending them to wide-area, dynamic settings while avoiding
post-selection reconciliation remains open. DH-EAC adopts a two-layer
pure-quantum lottery: the outer layer selects winning QLANs and the inner layer
selects winning nodes within each winning QLAN. A key design principle is that
both the winning set and the per-QLAN quota are fixed by measurements alone, so
the contention loop requires no classical round trip. The protocol thus aims to
jointly satisfy anonymity (no node IDs revealed until decisions are fixed) and
fairness (bias suppression under heterogeneous QLAN sizes). We also provide
analytical models for success probability and latency under a standard i.i.d.
loss model, and we evaluate DH-EAC against two baselines - single-layer Dicke
within one QLAN and a classical GO-driven allocator - using a minimal,
reproducible set of scenarios. Metrics include success probability, end-to-end
latency, throughput, and Jain's fairness index. The results indicate that
DH-EAC offers an implementable design point in the space of entanglement access
control, balancing pure-quantum contention resolution, anonymity, and
scalability for multi-QLAN networks.

</details>


### [37] [Sequence-Based Deep Learning for Handover Optimization in Dense Urban Cellular Network](https://arxiv.org/abs/2510.02958)
*Muhammad Kabeer,Rosdiadee Nordin,Mehran Behjati,Lau Sian Lun*

Main category: cs.NI

TL;DR: 基于真实世界多运营商数据集，使用GRU、LSTM和Transformer等序列深度学习模型进行切换预测，GRU模型在密集城市蜂窝网络中显著减少了乒乓切换和不必要切换，提升了连接稳定性。


<details>
  <summary>Details</summary>
Motivation: 密集城市蜂窝网络中高小区密度、用户移动性和多样化服务需求增加了不必要切换和乒乓效应的可能性，需要高效的切换管理解决方案。

Method: 利用30,925条标记记录的真实数据集，将切换预测建模为序列问题，评估GRU、LSTM和Transformer架构在RSRP-only和全特征设置下的性能。

Result: GRU模型实现了98%的乒乓切换减少和46.25%的不必要切换减少，相比基线RSRP-only方法的22.19%有显著提升，同时时间停留改善46%，推理时间仅0.91秒。

Conclusion: 所提出的基于GRU的模型在移动鲁棒性和用户体验质量方面显示出显著改进，适合实时边缘部署，数据集已发布以促进5G及以后智能移动性管理的进一步研究。

Abstract: Efficient handover management remains a critical challenge in dense urban
cellular networks, where high cell density, user mobility, and diverse service
demands increase the likelihood of unnecessary handovers and ping-pong effects.
This paper leverages a real-world, multi-operator drive-test dataset of 30,925
labelled records collected within a 2 km area around Sunway City to investigate
sequence-based deep learning approaches for handover detection and avoidance.
We formulate handover prediction as a sequence problem and evaluate Gated
Recurrent Unit (GRU), Long Short-Term Memory (LSTM), and Transformer
architectures under Reference Signal Received Power (RSRP)-only and all-feature
settings. The integration of multi-dimensional features significantly enhanced
handover performance in dense urban cellular networks. The proposed GRU-based
model achieved a remarkable 98% reduction in ping-pong handovers, alongside a
46.25% decrease in unnecessary handovers, outperforming the baseline RSRP-only
approach which yielded a 22.19% reduction. Furthermore, the model demonstrated
a 46% improvement in Time of Stay (ToS), indicating more stable user
connections. With an inference time of just 0.91 seconds, the solution proves
highly efficient and well-suited for real-time edge deployment scenarios.
Compared to the conventional 3GPP A3 algorithm, these improvements demonstrate
significant gains in mobility robustness and user Quality of Experience (QoE)
improvement. The dataset is released to foster reproducibility and further
research in intelligent mobility management for 5G and beyond.

</details>


### [38] [Automatic Generation of Digital Twins for Network Testing](https://arxiv.org/abs/2510.03205)
*Shenjia Ding,David Flynn,Paul Harvey*

Main category: cs.NI

TL;DR: 自动生成数字孪生用于电信网络软件测试验证，减少人工配置时间和成本


<details>
  <summary>Details</summary>
Motivation: 电信网络软件化趋势增加了测试验证需求，传统仿真和硬件方法耗时耗力，数字孪生需要自动化配置

Method: 基于ITU-T自治网络架构实验子系统，开发自动生成数字孪生的方法

Result: 初步实验证明该方法可行，能自动创建高效且足够精确的数字孪生

Conclusion: 自动生成的数字孪生可作为现有验证流程的有效补充工具

Abstract: The increased use of software in the operation and management of
telecommunication networks has moved the industry one step closer to realizing
autonomous network operation. One consequence of this shift is the
significantly increased need for testing and validation before such software
can be deployed. Complementing existing simulation or hardware-based
approaches, digital twins present an environment to achieve this testing;
however, they require significant time and human effort to configure and
execute. This paper explores the automatic generation of digital twins to
provide efficient and accurate validation tools, aligned to the ITU-T
autonomous network architecture's experimentation subsystem. We present
experimental results for an initial use case, demonstrating that the approach
is feasible in automatically creating efficient digital twins with sufficient
accuracy to be included as part of existing validation pipelines.

</details>


<div id='cs.OS'></div>

# cs.OS [[Back]](#toc)

### [39] [NetCAS: Dynamic Cache and Backend Device Management in Networked Environments](https://arxiv.org/abs/2510.02323)
*Joon Yong Hwang,Chanseo Park,Ikjun Yeom,Younghoon Kim*

Main category: cs.OS

TL;DR: NetCAS是一个动态I/O分割框架，通过实时网络反馈和预计算性能配置文件，在缓存和后端设备间智能分配I/O请求，显著提升远程存储环境性能。


<details>
  <summary>Details</summary>
Motivation: 现代存储系统通常结合快速缓存和慢速后端设备来加速I/O。随着性能差距缩小，同时访问两个设备而非仅依赖缓存命中可以提高吞吐量。但在数据中心环境中，通过网络访问的远程后端存储面临不可预测的竞争，使得这种分割变得复杂。

Method: NetCAS采用基于实时网络反馈和预计算性能配置文件的方法动态分割I/O。使用低开销的批量轮询调度器来执行分割，避免每个请求的开销。

Result: NetCAS在远程存储环境中比传统缓存性能提升高达174%，在网络条件波动时比Orthus等收敛方案性能高出3.5倍。

Conclusion: NetCAS通过动态适应工作负载配置和网络性能的分割策略，有效解决了远程存储环境中I/O分割的挑战，显著提升了系统性能。

Abstract: Modern storage systems often combine fast cache with slower backend devices
to accelerate I/O. As performance gaps narrow, concurrently accessing both
devices, rather than relying solely on cache hits, can improve throughput.
However, in data centers, remote backend storage accessed over networks suffers
from unpredictable contention, complicating this split. We present NetCAS, a
framework that dynamically splits I/O between cache and backend devices based
on real-time network feedback and a precomputed Perf Profile. Unlike
traditional hit-rate-based policies, NetCAS adapts split ratios to workload
configuration and networking performance. NetCAS employs a low-overhead batched
round-robin scheduler to enforce splits, avoiding per-request costs. It
achieves up to 174% higher performance than traditional caching in remote
storage environments and outperforms converging schemes like Orthus by up to
3.5X under fluctuating network conditions.

</details>
