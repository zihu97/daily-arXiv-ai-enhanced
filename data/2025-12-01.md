<div id=toc></div>

# Table of Contents

- [cs.DC](#cs.DC) [Total: 9]
- [cs.AR](#cs.AR) [Total: 6]
- [cs.PF](#cs.PF) [Total: 3]
- [cs.SE](#cs.SE) [Total: 19]
- [cs.NI](#cs.NI) [Total: 3]
- [cs.MA](#cs.MA) [Total: 1]


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [1] [A Sustainable and Reward Incentivized High-Performance Cluster Computing for Artificial Intelligence: A Novel Bayesian-Time-Decay Trust Mechanism in Blockchain](https://arxiv.org/abs/2511.21844)
*Murat Yaslioglu*

Main category: cs.DC

TL;DR: 本文提出了一种融合高性能集群计算、智能算法与区块链的新框架，通过改进的工作量证明机制、动态信任评分和统计抽签系统，实现高效、公平且环保的智能算法部署。


<details>
  <summary>Details</summary>
Motivation: 高性能计算和智能算法通常需要大量计算资源，导致高能耗并排斥低性能设备，因此亟需一种更具包容性、可扩展性和环保性的新方法。

Method: 设计了一个基于区块链的框架，整合改进的PoW共识机制（将计算投入与区块奖励挂钩）、动态“信任评分”（根据节点验证准确性调整其被选中概率）以及统计“抽签”机制（使低算力节点也有机会参与出块）。

Result: 该框架提升了资源利用效率，支持广泛计算能力的设备参与，并通过信任机制和抽签机制保障了系统的公平性与可靠性。

Conclusion: 所提出的框架为构建高效、包容且可持续的智能算法生态系统提供了可行路径，兼顾性能、公平与环保目标。

Abstract: In an age where sustainability is of paramount importance, the significance of both high-performance computing and intelligent algorithms cannot be understated. Yet, these domains often demand hefty computational power, translating to substantial energy usage and potentially sidelining less robust computing systems. It's evident that we need an approach that is more encompassing, scalable, and eco-friendly for intelligent algorithm development and implementation. The strategy we present in this paper offers a compelling answer to these issues. We unveil a fresh framework that seamlessly melds high-performance cluster computing with intelligent algorithms, all within a blockchain infrastructure. This promotes both efficiency and a broad-based participation. At its core, our design integrates an evolved proof-of-work consensus process, which links computational efforts directly to rewards for producing blocks. This ensures both optimal resource use and participation from a wide spectrum of computational capacities. Additionally, our approach incorporates a dynamic 'trust rating' that evolves based on a track record of accurate block validations. This rating determines the likelihood of a node being chosen for block generation, creating a merit-based system that recognizes and rewards genuine and precise contributions. To level the playing field further, we suggest a statistical 'draw' system, allowing even less powerful nodes a chance to be part of the block creation process.

</details>


### [2] [Equivalence and Separation between Heard-Of and Asynchronous Message-Passing Models](https://arxiv.org/abs/2511.21859)
*Hagit Attiya,Armando Castañeda,Dhrubajyoti Ghosh,Thomas Nowak*

Main category: cs.DC

TL;DR: 本文研究了异步消息传递模型（AMP_f）与Heard-Of模型（HO_f）在容错分布式计算中的等价性，发现对于无色任务在 n > 2f 时两者等价，而对于有色任务仅在 f = 1 且 n > 2 时等价。


<details>
  <summary>Details</summary>
Motivation: 厘清两种主流分布式计算模型（AMP_f 和 HO_f）在可解性方面的关系，特别是它们在处理不同任务类型（无色与有色）和故障数量下的表达能力差异。

Method: 通过构建双向模拟，并引入一个能刻画“静默进程”现象的中间模型，在 AMP_f 与 HO_f 之间建立联系；同时将分析扩展至非自适应敌手下的随机协议。

Result: 证明了在 n > 2f 条件下，AMP_f 与 HO_f 对无色任务等价；对有色任务，仅当 f = 1 且 n > 2 时等价。更大的 f 值会导致 HO_f 中因静默进程而产生不兼容决策，破坏等价性。

Conclusion: 轮次抽象（round-based abstractions）是否能准确刻画异步计算具有结构性限制：对无色任务在多数派条件下成立，但对有色任务则仅在极低故障数下成立，且该限制与是否使用随机化无关。

Abstract: We revisit the relationship between two fundamental models of distributed computation: the asynchronous message-passing model with up to $f$ crash failures ($\operatorname{AMP}_f$) and the Heard-Of model with up to $f$ message omissions ($\operatorname{HO}_f$). We show that for $n > 2f$, the two models are equivalent with respect to the solvability of colorless tasks, and that for colored tasks the equivalence holds only when $f = 1$ (and $n > 2$). The separation for larger $f$ arises from the presence of silenced processes in $\operatorname{HO}_f$, which may lead to incompatible decisions. The proofs proceed through bidirectional simulations between $\operatorname{AMP}_f$ and $\operatorname{HO}_f$ via an intermediate model that captures this notion of silencing. The results extend to randomized protocols against a non-adaptive adversary, indicating that the expressive limits of canonical rounds are structural rather than probabilistic. Together, these results delineate precisely where round-based abstractions capture asynchronous computation, and where they do not.

</details>


### [3] [OOCO: Latency-disaggregated Architecture for Online-Offline Co-locate LLM Serving](https://arxiv.org/abs/2511.21862)
*Siyu Wu,Zihan Tang,Yuting Zeng,Hui Chen,Guiguang Ding,Tongxuan Liu,Ke Zhang,Hailong Yang*

Main category: cs.DC

TL;DR: 提出一种基于延迟约束的解耦架构，通过分离延迟敏感与非敏感任务池，并结合瓶颈感知调度器和快速抢占机制，在保障在线请求SLO的同时将离线吞吐提升最高3倍。


<details>
  <summary>Details</summary>
Motivation: 在Prefill/Decode解耦系统中直接共置延迟敏感的在线服务与成本敏感的离线任务会导致严重的负载不均衡，现有动态调整方法难以应对在线流量突发性。

Method: 设计延迟约束解耦架构，将集群资源划分为延迟严格与宽松两类池；引入基于Roofline模型的瓶颈感知调度器，并实现快速抢占机制以保障在线SLO。

Result: 在真实轨迹实验中，相比现有离线系统方法，该方法在满足在线请求SLO的前提下，离线吞吐量最高提升3倍。

Conclusion: 所提架构有效缓解了P/D负载失衡问题，在保障在线服务质量的同时显著提升了离线任务的资源利用效率。

Abstract: Large Language Models (LLMs) are increasingly deployed in both latency-sensitive online services and cost-sensitive offline workloads. Co-locating these workloads on shared serving instances can improve resource utilization, but directly applying this approach to Prefill/Decode (P/D) disaggregated systems introduces severe load imbalance, as fluctuating request mixes alter the intrinsic P/D ratio. Existing dynamic adjustment techniques cannot keep up with the bursty traffic patterns of online services.
  We propose a latency-constraint disaggregated architecture, which separates cluster resources into latency-strict and latency-relaxed pools based on task latency requirements. This design enables flexible placement of offline decode tasks, mitigating P/D imbalance while preserving online performance. To fully exploit this flexibility, we propose (1) a bottleneck-based scheduler guided by a Roofline-based performance model for performance bottleneck based scheduling, and (2) a fast preemption mechanism that strictly enforces Service Level Objectives (SLOs) for online requests.
  Experiments on real-world traces show that compared to existing offline system approaches, our method improves offline throughput by up to 3x, while maintaining online request SLOs.

</details>


### [4] [Clock2Q+: A Simple and Efficient Replacement Algorithm for Metadata Cache in VMware vSAN](https://arxiv.org/abs/2511.21958)
*Yiyan Zhai,Bintang Dwi Marthen,Sarath Balivada,Vamsi Sudhakar Bojji,Eric Knauft,Jitender Rohilla,Jiaqi Zuo,Quanxing Liu,Maxime Austruy,Wenguang Wang,Juncheng Yang*

Main category: cs.DC

TL;DR: 本文提出了一种专为元数据缓存设计的新型缓存替换算法Clock2Q+，通过在Small FIFO队列中引入相关性窗口来有效处理元数据访问中的相关引用问题，在元数据和数据轨迹上均优于现有先进算法。


<details>
  <summary>Details</summary>
Motivation: 元数据缓存中存在固有的相关引用，即使对应的数据访问没有相关性；这些相关引用常被误判为热块，从而降低传统缓存替换算法的效果。

Method: Clock2Q+基于S3-FIFO的三队列结构，但在Small FIFO队列中引入一个“相关性窗口”，该窗口内的块不会设置引用位，以此区分真正热块与相关引用。

Result: 在元数据轨迹上，Clock2Q+相比次优算法S3-FIFO最多降低28.5%的缺失率；同时在数据轨迹上也优于当前最先进的缓存替换算法，并具备低CPU开销、低内存开销、良好的多核扩展性及易于调优和实现等优点。

Conclusion: Clock2Q+是一种高效、实用且适用于大规模存储系统的缓存替换算法，特别适合处理元数据缓存中的相关引用问题，并已在VMware by Broadcom的vSAN和VDFS产品中部署。

Abstract: Cache replacement algorithms are critical building blocks of storage systems. This paper examines the characteristics of metadata caches and argues that they inherently exhibit correlated references, even when the corresponding data accesses do not contain correlated references. The presence of correlated references reduces the effectiveness of cache replacement algorithms because these references are often mistakenly categorized as hot blocks. Clock2Q+ is specifically designed for metadata caches and has been implemented in vSAN and VDFS, two flagship storage products of VMware by Broadcom. Similar to S3-FIFO, Clock2Q+ uses three queues; however, Clock2Q+ introduces a correlation window in the Small FIFO queue, where blocks in this window do not set the reference bit. This simple enhancement allows Clock2Q+ to outperform state-of-the-art replacement algorithms. Compared to S3-FIFO, the second-best performing algorithm, Clock2Q+ achieves up to a 28.5% lower miss ratio on metadata traces. Clock2Q+ possesses the essential properties required for large-scale storage systems: it has low CPU overhead on cache hits, low memory overhead, scales efficiently to multiple CPUs, and is both easy to tune and implement. Additionally, Clock2Q+ outperforms state-of-the-art cache replacement algorithms on data traces as well.

</details>


### [5] [An Empirical Study of Cross-Language Interoperability in Replicated Data Systems](https://arxiv.org/abs/2511.22010)
*Provakar Mondal,Eli Tilevich*

Main category: cs.DC

TL;DR: 本文通过实证研究比较了在多语言复制数据系统中集成复制数据库（RDL）的两种策略——外部函数接口（FFI）和通用数据格式（CDF），发现CDF在软件质量、延迟、内存消耗和吞吐量方面更具优势，并据此设计了一个支持多语言且具有插件扩展能力的RDL。


<details>
  <summary>Details</summary>
Motivation: 现代分布式系统常在多个执行站点间复制数据，且因业务和资源限制需混合使用多种编程语言。现有RDL通常仅支持单一语言或提供特定语言绑定，在多语言环境中集成需额外代码，但其软件质量和性能特征尚不明确。

Method: 开展实证研究，对比FFI和CDF两种RDL集成策略，测量并比较它们的软件指标与性能表现；进一步构建基于CDF的RDL以支持编译型、解释型和托管语言，并加入插件扩展机制。

Result: CDF在跨语言交互中展现出更优的软件质量、更低的延迟、更少的内存占用和更高的吞吐量；所构建的CDF-based RDL成功支持多语言混合，并通过插件机制实现单语言功能扩展而不破坏多语言集成。

Conclusion: 研究为多语言复制数据系统中RDL的设计提供了新见解，表明采用通用数据格式是提升软件质量与系统性能的有效途径。

Abstract: BACKGROUND: Modern distributed systems replicate data across multiple execution sites. Business requirements and resource constraints often necessitate mixing different languages across replica sites. To facilitate the management of replicated data, modern software engineering practices integrate special-purpose replicated data libraries (RDLs) that provide read-write access to the data and ensure its synchronization. Irrespective of the implementation languages, an RDL typically uses a single language or offers bindings to a designated one. Hence, integrating existing RDLs in multilingual environments requires special-purpose code, whose software quality and performance characteristics are poorly understood.
  AIMS: We aim to bridge this knowledge gap to understand the software quality and performance characteristics of RDL integration in multilingual environments.
  METHOD: We conduct an empirical study of two key strategies for integrating RDLs in the context of multilingual replicated data systems: foreign-function interface (FFI) and a common data format (CDF); we measure and compare their respective software metrics and performance to understand their suitability for the task at hand.
  RESULTS: Our results reveal that adopting CDF for cross-language interaction offers software quality, latency, memory consumption, and throughput advantages. We further validate our findings by (1) creating a CDF-based RDL for mixing compiled, interpreted, and managed languages; and (2) enhancing our RDL with plug-in extensibility that enables adding functionality in a single language while maintaining integration within a multilingual environment.
  CONCLUSIONS: With modern distributed systems utilizing multiple languages, our findings provide novel insights for designing RDLs in multilingual replicated data systems.

</details>


### [6] [PAT: Accelerating LLM Decoding via Prefix-Aware Attention with Resource Efficient Multi-Tile Kernel](https://arxiv.org/abs/2511.22333)
*Jinjun Yi,Zhixin Zhao,Yitao Hu,Ke Yan,Weiwei Sun,Hao Wang,Laiping Zhao,Yuhao Zhang,Wenxin Li,Keqiu Li*

Main category: cs.DC

TL;DR: 本文提出PAT，一种面向LLM解码的前缀感知注意力核，通过打包共享前缀查询、定制多分块核及多流转发等技术，显著降低注意力延迟和TPOT。


<details>
  <summary>Details</summary>
Motivation: 现有注意力实现未能充分利用真实负载中大量存在的分层共享前缀（如系统提示、工具模板、RAG），导致重复加载KV缓存、片上资源闲置及内存带宽压力增大，从而拖慢内存受限的解码注意力操作。

Method: 提出PAT方法，采用“打包-前向-合并”范式：按共享前缀打包查询以减少重复内存访问；运行定制化多分块核提升资源利用率；结合多流前向与KV切分减少资源气泡；最后通过在线softmax完成合并，开销可忽略。该方法以即插即用插件形式集成到vLLM中。

Result: 在真实与合成负载上的评估表明，相比当前最先进的注意力核，PAT在相同配置下平均降低注意力延迟67.4%，TPOT降低13.6%–83.4%。

Conclusion: PAT通过高效利用请求间的共享前缀信息，在不改变模型结构的前提下显著提升了LLM解码阶段的注意力计算效率，有效缓解了内存瓶颈问题。

Abstract: LLM serving is increasingly dominated by decode attention, which is a memory-bound operation due to massive KV cache loading from global memory. Meanwhile, real-world workloads exhibit substantial, hierarchical shared prefixes across requests (e.g., system prompts, tools/templates, RAG). Existing attention implementations fail to fully exploit prefix sharing: *one-query-per-CTA* execution repeatedly loads shared prefix KV cache, while *one-size-fits-all* tiling leaves on-chip resources idle and exacerbates bubbles for uneven KV lengths. These choices amplify memory bandwidth pressure and stall memory-bound decode attention.
  This paper introduces PAT, a prefix-aware attention kernel implementation for LLM decoding that organizes execution with a pack-forward-merge paradigm. PAT packs queries by shared prefix to reduce repeated memory accesses, runs a customized multi-tile kernel to achieve high resource efficiency. It further applies practical multi-stream forwarding and KV splitting to reduce resource bubbles. The final merge performs online softmax with negligible overhead. We implement PAT as an off-the-shelf plugin for vLLM. Evaluation on both real-world and synthetic workloads shows that PAT reduces attention latency by 67.4% on average and TPOT by 13.6-83.4% under the same configurations against state-of-the-art attention kernels.

</details>


### [7] [OmniInfer: System-Wide Acceleration Techniques for Optimizing LLM Serving Throughput and Latency](https://arxiv.org/abs/2511.22481)
*Jun Wang,Yunxiang Yao,Wenwei Kuang,Runze Mao,Zhenhao Sun,Zhuang Tao,Ziyang Zhang,Dengyu Li,Jiajun Chen,Zhili Wang,Kai Cui,Congzhi Cai,Longwen Lan,Ken Zhang*

Main category: cs.DC

TL;DR: OmniInfer 是一个面向大语言模型推理的统一系统级加速框架，通过专家放置、缓存压缩和调度的细粒度优化，显著提升端到端推理效率。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在实际部署中面临计算密集、延迟敏感和吞吐瓶颈等挑战，亟需高效的系统级优化方案以提升推理服务性能。

Method: OmniInfer 构建于 vLLM 之上，整合三个核心组件：OmniPlacement（负载感知的 MoE 调度）、OmniAttn（稀疏注意力加速）和 OmniProxy（面向资源解耦的请求调度），通过自适应资源解耦、高效利用稀疏性和全局协调预填充与解码阶段来优化整体性能。

Result: 在 10 节点 Ascend 910C 集群上使用 DeepSeek-R1 模型评估，OmniInfer 达到 616 QPM，整体框架降低 TPOT 36%，叠加 OmniProxy 后进一步减少 TTFT 38%。

Conclusion: OmniInfer 有效提升了大语言模型推理系统的端到端效率，展示了系统级协同优化在实际部署中的巨大潜力，并已开源。

Abstract: Large Language Models drive a wide range of modern AI applications but impose substantial challenges on large-scale serving systems due to intensive computation, strict latency constraints, and throughput bottlenecks. We introduce OmniInfer, a unified system-level acceleration framework designed to maximize end-to-end serving efficiency through fine-grained optimization of expert placement, cache compression, and scheduling. OmniInfer integrates three complementary components: OmniPlacement for load-aware Mixture-of-Experts scheduling, OmniAttn for sparse attention acceleration, and OmniProxy for disaggregation-aware request scheduling. Built atop vLLM, OmniInfer delivers system-wide performance gains through adaptive resource disaggregation, efficient sparsity exploitation, and global coordination across prefill and decode phases. Evaluated on DeepSeek-R1 within a 10-node Ascend 910C cluster, OmniInfer achieves 616 QPM, where the unified framework reduces TPOT by 36\%, and the superimposition of OmniProxy further slashes TTFT by 38\%. The project is open-sourced at [this https URL](https://gitee.com/omniai/omniinfer).

</details>


### [8] [Accelerating mesh-based Monte Carlo simulations using contemporary graphics ray-tracing hardware](https://arxiv.org/abs/2511.22779)
*Shijie Yan,Douglas Dwyer,David R. Kaeli,Qianqian Fang*

Main category: cs.DC

TL;DR: 提出了一种基于GPU光线追踪核心（RT-core）的加速版网格蒙特卡洛算法RT-MMC，通过硬件加速显著提升模拟光在复杂组织中传播的速度，同时简化工作流程并保持高精度。


<details>
  <summary>Details</summary>
Motivation: 传统网格蒙特卡洛（MMC）方法虽精度高，但受限于频繁的光线-边界相交计算，即使在GPU上运行仍效率不足，亟需更高效的加速策略。

Method: 利用NVIDIA OptiX平台，将图形学中的光线追踪管线扩展至浑浊介质中的体光线追踪，借助现代GPU的RT-core实现硬件加速，避免复杂的四面体网格生成，并天然支持宽场光源。

Result: RT-MMC与传统软件光线追踪MMC结果高度一致，在多种GPU架构上实现1.5倍至4.5倍的速度提升，显著增强MMC在常规模拟中的实用性。

Conclusion: 从软件转向硬件光线追踪不仅大幅简化MMC模拟流程，还带来显著加速效果，随着光线追踪硬件普及，该方法将推动生物光子学领域广泛应用。

Abstract: Significance: Monte Carlo (MC) methods are the gold-standard for modeling light-tissue interactions due to their accuracy. Mesh-based MC (MMC) offers enhanced precision for complex tissue structures using tetrahedral mesh models. Despite significant speedups achieved on graphics processing units (GPUs), MMC performance remains hindered by the computational cost of frequent ray-boundary intersection tests.
  Aim: We propose a highly accelerated MMC algorithm, RT-MMC, that leverages the hardware-accelerated ray traversal and intersection capabilities of ray-tracing cores (RT-cores) on modern GPUs.
  Approach: Implemented using NVIDIA's OptiX platform, RT-MMC extends graphics ray-tracing pipelines towards volumetric ray-tracing in turbid media, eliminating the need for challenging tetrahedral mesh generation while delivering significant speed improvements through hardware acceleration. It also intrinsically supports wide-field sources without complex mesh retesselation.
  Results: RT-MMC demonstrates excellent agreement with traditional software-ray-tracing MMC algorithms while achieving 1.5x to 4.5x speedups across multiple GPU architectures. These performance gains significantly enhance the practicality of MMC for routine simulations.
  Conclusion: Migration from software- to hardware-based ray-tracing not only greatly simplifies MMC simulation workflows, but also results in significant speedups that are expected to increase further as ray-tracing hardware rapidly gains adoption. Adoption of graphics ray-tracing pipelines in quantitative MMC simulations enables leveraging of emerging hardware resources and benefits a wide range of biophotonics applications.

</details>


### [9] [Serving Heterogeneous LoRA Adapters in Distributed LLM Inference Systems](https://arxiv.org/abs/2511.22880)
*Shashwat Jaiswal,Shrikara Arun,Anjaly Parayil,Ankur Mallick,Spyros Mastorakis,Alind Khare,Chloi Alverti,Renee St Amant,Chetan Bansal,Victor Rühle,Josep Torrellas*

Main category: cs.DC

TL;DR: LoRAServe 是一种面向 LoRA 服务的动态适配器放置与路由框架，通过感知工作负载并利用 GPU Direct RDMA 技术，在真实生产环境中显著提升吞吐量、降低首 token 延迟，并减少所需 GPU 数量。


<details>
  <summary>Details</summary>
Motivation: 现有 LoRA 服务系统在多租户环境下对不同秩（rank）的适配器进行共批处理时未考虑其大小差异，导致性能严重不均衡，进而需增加 GPU 资源以满足服务等级目标（SLO），而当前优化手段忽略了这种异构性，造成 GPU 资源利用率低下。

Method: 提出 LoRAServe 框架，通过动态地在 GPU 间重新平衡适配器分布，并结合 GPU Direct RDMA 实现远程访问，从而在面对真实工作负载变化时最大化吞吐量并最小化尾部延迟。

Result: 在 Company X 的生产轨迹评估中，LoRAServe 相比现有最先进系统，在满足 SLO 的前提下，吞吐量最高提升 2 倍，首 token 延迟（TTFT）最多降低 9 倍，并最多节省 50% 的 GPU 资源。

Conclusion: LoRAServe 有效解决了 LoRA 多租户服务中因适配器秩异构带来的资源利用与性能问题，显著提升了系统效率和可扩展性。

Abstract: Low-Rank Adaptation (LoRA) has become the de facto method for parameter-efficient fine-tuning of large language models (LLMs), enabling rapid adaptation to diverse domains. In production, LoRA-based models are served at scale, creating multi-tenant environments with hundreds of adapters sharing a base model. However, state-of-the-art serving systems co-batch heterogeneous adapters without accounting for rank (size) variability, leading to severe performance skew, which ultimately requires adding more GPUs to satisfy service-level objectives (SLOs). Existing optimizations, focused on loading, caching, and kernel execution, ignore this heterogeneity, leaving GPU resources underutilized. We present LoRAServe, a workload-aware dynamic adapter placement and routing framework designed to tame rank diversity in LoRA serving. By dynamically rebalancing adapters across GPUs and leveraging GPU Direct RDMA for remote access, LoRAServe maximizes throughput and minimizes tail latency under real-world workload drift. Evaluations on production traces from Company X show that LoRAServe elicits up to 2$\times$ higher throughput, up to 9$\times$ lower TTFT, while using up to 50% fewer GPUs under SLO constraints compared to state-of-the-art systems.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [10] [3RSeT: Read Disturbance Rate Reduction in STT-MRAM Caches by Selective Tag Comparison](https://arxiv.org/abs/2511.22551)
*Elham Cheshmikhani,Hamed Farbeh,Hossein Asad*

Main category: cs.AR

TL;DR: 本文提出了一种名为3RSeT的低成本方案，通过选择性标签比较显著降低STT-MRAM缓存中的读干扰错误率，从而提升可靠性、降低能耗，并几乎不影响性能和面积。


<details>
  <summary>Details</summary>
Motivation: STT-MRAM作为SRAM的潜在替代品虽具多项优势，但其在缓存中并行标签读取操作会引发严重的读干扰错误，而现有研究尚未解决此问题。

Method: 提出3RSeT方法，在每次访问请求时利用标签低位信息预先禁用不可能命中的标签，从而减少不必要的标签读取。

Result: 实验表明，3RSeT将标签阵列的读干扰率降低71.8%，平均故障间隔时间（MTTF）提升3.6倍，能耗降低62.1%，性能无损且面积开销低于0.4%。

Conclusion: 3RSeT是一种高效、低开销的方案，能显著缓解STT-MRAM缓存中的读干扰问题，提升系统可靠性和能效。

Abstract: Recent development in memory technologies has introduced Spin-Transfer Torque Magnetic RAM (STT-MRAM) as the most promising replacement for SRAMs in on-chip cache memories. Besides its lower leakage power, higher density, immunity to radiation-induced particles, and non-volatility, an unintentional bit flip during read operation, referred to as read disturbance error, is a severe reliability challenge in STT-MRAM caches. One major source of read disturbance error in STT-MRAM caches is simultaneous accesses to all tags for parallel comparison operation in a cache set, which has not been addressed in previous work. This paper first demonstrates that high read accesses to tag array extremely increase the read disturbance rate and then proposes a low-cost scheme, so-called Read Disturbance Rate Reduction in STT-MRAM Caches by Selective Tag Comparison (3RSeT), to reduce the error rate by eliminating a significant portion of tag reads. 3RSeT proactively disables the tags that have no chance for hit, using low significant bits of the tags on each access request. Our evaluations using gem5 full-system cycle-accurate simulator show that 3RSeT reduces the read disturbance rate in the tag array by 71.8%, which results in 3.6x improvement in Mean Time To Failure (MTTF). In addition, the energy consumption is reduced by 62.1% without compromising performance and with less than 0.4% area overhead.

</details>


### [11] [Aquas: Enhancing Domain Specialization through Holistic Hardware-Software Co-Optimization based on MLIR](https://arxiv.org/abs/2511.22267)
*Yuyang Zou,Youwei Xiao,Yansong Xu,Chenyun Yin,Yuhao Luo,Yitian Sun,Ruifan Xu,Renze Chen,Yun Liang*

Main category: cs.AR

TL;DR: Aquas 是一个基于 MLIR 的软硬件协同设计框架，通过增强的硬件合成（如突发 DMA 引擎和高级 HLS 优化）与基于 e-graph 的可重定向编译器方法，显著提升 RISC-V ASIP 的性能，在真实工作负载上实现最高 9.27 倍加速。


<details>
  <summary>Details</summary>
Motivation: 现有开源 RISC-V 生态中的框架因硬件综合能力受限和编译器支持僵化，难以充分发挥 ASIP 在特定应用中的性能潜力。

Method: 提出 Aquas 框架：硬件方面引入突发 DMA 引擎和高级 HLS 优化以加快内存访问；软件方面采用基于 e-graph 的可重定向编译方法，并设计新型匹配引擎以高效完成指令匹配。

Result: 在点云处理和大语言模型推理等真实工作负载上，Aquas 实现了最高 9.27 倍的性能加速。

Conclusion: Aquas 有效解决了现有 RISC-V ASIP 框架在性能和编译灵活性方面的瓶颈，展示了软硬件协同设计在专用处理器开发中的巨大潜力。

Abstract: Application-Specific Instruction-Set Processors (ASIPs) built on the RISC-V architecture offer specialization opportunities for various applications. However, existing frameworks from the open-source RISC-V ecosystem suffer from limited performance due to restricted hardware synthesis and rigid compiler support. To address these challenges, we introduce Aquas, a holistic hardware-software co-design framework built upon MLIR. Aquas enhances ASIP synthesis with fast memory access capability via a burst DMA engine and advanced high-level synthesis (HLS) optimizations. On the compiler side, we propose an e-graph based retargetable approach with a novel matching engine for efficient instruction matching. Evaluation demonstrates up to 9.27x speedup on real-world workloads, including point cloud processing and LLM inference.

</details>


### [12] [FADiff: Fusion-Aware Differentiable Optimization for DNN Scheduling on Tensor Accelerators](https://arxiv.org/abs/2511.22348)
*Shuao Jia,Zichao Ling,Chen Bai,Kang Zhao,Jianwang Zhai*

Main category: cs.AR

TL;DR: FADiff is a gradient-based optimization framework that jointly optimizes intra-layer mapping and inter-layer fusion for efficient DNN inference on tensor accelerators, outperforming existing methods in energy and latency.


<details>
  <summary>Details</summary>
Motivation: Efficient deployment of DNNs like LLMs on tensor accelerators is hindered by the complex design space arising from the interaction between intra-layer mapping and inter-layer fusion.

Method: FADiff constructs a unified differentiable analytical cost model to predict energy and latency, then uses gradient-based optimization with discrete constraints encoded in the loss function to explore the joint mapping-fusion design space.

Result: Experiments show FADiff achieves superior energy and latency optimization compared to existing approaches.

Conclusion: FADiff effectively automates the discovery of high-quality mapping and fusion strategies, significantly improving DNN inference efficiency on tensor accelerators.

Abstract: Efficient deployment of Deep Neural Networks (DNNs), such as Large Language Models (LLMs), on tensor accelerators is essential for maximizing computational efficiency in modern AI systems. However, achieving this is challenging due to the enormous and complex design space created by the interaction of intra-layer mapping and inter-layer fusion. In this work, we present FADiff, a gradient-based optimization framework capable of automatically identifying high-quality intra-layer mapping and inter-layer fusion strategies to accelerate inference for DNN workloads. We first construct a unified and differentiable analytical cost model, which accurately predicts the energy and latency of both single-layer mappings and various layer fusion strategies. Then, by encoding discrete constraints into the loss function, we employ a gradient-based approach to efficiently explore the vast design space, determining the optimal joint strategy for mapping and fusion. Experimental results demonstrate the superiority of FADiff, achieving better optimization in terms of energy and latency compared to existing methods.

</details>


### [13] [The Immutable Tensor Architecture: A Pure Dataflow Approach for Secure, Energy-Efficient AI Inference](https://arxiv.org/abs/2511.22889)
*Fang Li*

Main category: cs.AR

TL;DR: 提出了一种名为“不可变张量架构”（ITA）的新方法，通过将大语言模型权重固化到ASIC的物理电路中，消除内存墙问题，从而在边缘设备上高效部署大语言模型。


<details>
  <summary>Details</summary>
Motivation: 当前在消费级边缘设备上部署大语言模型受限于“内存墙”问题，即每次生成token都需要从DRAM中读取大量模型权重，带来高昂的带宽和能耗开销；现有硬件架构将权重视为可变数据，为保持通用可编程性而付出巨大能耗代价。

Method: 提出不可变张量架构（ITA），将模型权重直接编码进成熟工艺节点（如28nm/40nm）ASIC的金属互连与逻辑电路中，使权重成为物理电路拓扑的一部分；同时采用“Split-Brain”系统设计，由主机CPU处理动态KV缓存，ITA ASIC作为无状态、内嵌ROM的数据流引擎。

Result: 该方法完全消除了传统内存层次结构，显著降低能耗与带宽需求，为在边缘设备上高效运行大语言模型提供了新路径。

Conclusion: 将模型权重固化为硬件拓扑的ITA架构是一种有前景的范式转变，能有效突破内存墙限制，推动大语言模型在资源受限边缘设备上的实际部署。

Abstract: The deployment of Large Language Models (LLMs) on consumer edge devices is throttled by the "Memory Wall" -- the prohibitive bandwidth and energy cost of fetching gigabytes of model weights from DRAM for every token generated. Current architectures (GPUs, NPUs) treat model weights as mutable software data, incurring massive energy penalties to maintain general-purpose programmability. We propose The Immutable Tensor Architecture (ITA), a paradigm shift that treats model weights not as data, but as physical circuit topology. By encoding parameters directly into the metal interconnects and logic of mature-node ASICs (28nm/40nm), ITA eliminates the memory hierarchy entirely. We present a "Split-Brain" system design where a host CPU manages dynamic KV-cache operations while the ITA ASIC acts as a stateless, ROM-embedded dataflow engine.

</details>


### [14] [Cohet: A CXL-Driven Coherent Heterogeneous Computing Framework with Hardware-Calibrated Full-System Simulation](https://arxiv.org/abs/2511.23011)
*Yanjing Wang,Lizhou Wu,Sunfeng Gao,Yibo Tang,Junhui Luo,Zicong Wang,Yang Ou,Dezun Dong,Nong Xiao,Mingche Lai*

Main category: cs.AR

TL;DR: 本文提出了Cohet——首个基于CXL的缓存一致性异构计算框架，通过解耦计算与内存资源构建统一一致内存池，并提供标准内存接口；同时开发了高精度全系统模拟器SimCXL，实验表明CXL相比PCIe在细粒度数据传输、远程原子操作和远程过程调用等场景中性能显著提升。


<details>
  <summary>Details</summary>
Motivation: 传统基于PCIe的异构计算系统存在细粒度主机-设备交互效率低和编程模型复杂的问题；尽管CXL等缓存一致互连标准兴起，但受限于硬件平台稀缺、软硬件生态不成熟及应用场景不明，相关研究进展缓慢。

Method: 提出Cohet框架，将CPU与XPU的计算和内存资源解耦，构建共享的统一缓存一致内存池，并向计算线程暴露标准malloc/mmap接口；同时开发支持所有CXL子协议和设备类型的周期级全系统模拟器SimCXL用于研究验证。

Result: 评估显示，相比DMA传输，CXL.cache在缓存行粒度下延迟降低68%，带宽提升14.4倍；在远程原子操作（RAO）和远程过程调用（RPC）两个典型应用中，CXL-NIC相比PCIe-NIC分别实现5.5–40.2倍和平均1.86倍的加速。

Conclusion: CXL驱动的缓存一致异构计算能显著提升细粒度协同效率，Cohet框架与SimCXL模拟器为该方向提供了可行的系统架构与研究工具，验证了CXL在新型异构计算中的巨大潜力。

Abstract: Conventional heterogeneous computing systems built on PCIe interconnects suffer from inefficient fine-grained host-device interactions and complex programming models. In recent years, many proprietary and open cache-coherent interconnect standards have emerged, among which compute express link (CXL) prevails in the open-standard domain after acquiring several competing solutions. Although CXL-based coherent heterogeneous computing holds the potential to fundamentally transform the collaborative computing mode of CPUs and XPUs, research in this direction remains hampered by the scarcity of available CXL-supported platforms, immature software/hardware ecosystems, and unclear application prospects. This paper presents Cohet, the first CXL-driven coherent heterogeneous computing framework. Cohet decouples the compute and memory resources to form unbiased CPU and XPU pools which share a single unified and coherent memory pool. It exposes a standard malloc/mmap interface to both CPU and XPU compute threads, leaving the OS dealing with smart memory allocation and management of heterogeneous resources. To facilitate Cohet research, we also present a full-system cycle-level simulator named SimCXL, which is capable of modeling all CXL sub-protocols and device types. SimCXL has been rigorously calibrated against a real CXL testbed with various CXL memory and accelerators, showing an average simulation error of 3%. Our evaluation reveals that CXL.cache reduces latency by 68% and increases bandwidth by 14.4x compared to DMA transfers at cacheline granularity. Building upon these insights, we demonstrate the benefits of Cohet with two killer apps, which are remote atomic operation (RAO) and remote procedure call (RPC). Compared to PCIe-NIC design, CXL-NIC achieves a 5.5 to 40.2x speedup for RAO offloading and an average speedup of 1.86x for RPC (de)serialization offloading.

</details>


### [15] [GAVINA: flexible aggressive undervolting for bit-serial mixed-precision DNN acceleration](https://arxiv.org/abs/2511.23203)
*Jordi Fornt,Pau Fontova-Musté,Adrian Gras,Omar Lahyani,Martí Caro,Jaume Abella,Francesc Moll,Josep Altet*

Main category: cs.AR

TL;DR: 本文提出了一种名为GAV（Guarded Aggressive underVolting）的新技术，结合电压下缩（undervolting）与位串行计算，在保证精度几乎无损的前提下显著提升能效，并基于此构建了支持任意混合精度的加速器GAVINA，最高能效达89 TOP/sW。


<details>
  <summary>Details</summary>
Motivation: 电压下缩虽能显著降低功耗，但因错误率过高而难以广泛应用；同时现有下缩加速器多基于8位运算，无法与当前主流的低精度（<8位）架构竞争。

Method: 提出GAV方法，通过在选定的最低有效位组合上激进地降低供电电压，结合位串行计算实现灵活近似；并据此设计支持任意混合精度和灵活电压下缩的GAVINA加速器。

Result: GAVINA在最激进配置下能效高达89 TOP/sW；通过误差建模表明，在ResNet-18上使用GAV可实现20%的能效提升，且精度损失可忽略。

Conclusion: GAV通过结合电压下缩与位串行计算，在维持模型精度的同时显著提升能效，为低功耗DNN加速提供了一种可行且高效的新方案。

Abstract: Voltage overscaling, or undervolting, is an enticing approximate technique in the context of energy-efficient Deep Neural Network (DNN) acceleration, given the quadratic relationship between power and voltage. Nevertheless, its very high error rate has thwarted its general adoption. Moreover, recent undervolting accelerators rely on 8-bit arithmetic and cannot compete with state-of-the-art low-precision (<8b) architectures. To overcome these issues, we propose a new technique called Guarded Aggressive underVolting (GAV), which combines the ideas of undervolting and bit-serial computation to create a flexible approximation method based on aggressively lowering the supply voltage on a select number of least significant bit combinations. Based on this idea, we implement GAVINA (GAV mIxed-precisioN Accelerator), a novel architecture that supports arbitrary mixed precision and flexible undervolting, with an energy efficiency of up to 89 TOP/sW in its most aggressive configuration. By developing an error model of GAVINA, we show that GAV can achieve an energy efficiency boost of 20% via undervolting, with negligible accuracy degradation on ResNet-18.

</details>


<div id='cs.PF'></div>

# cs.PF [[Back]](#toc)

### [16] [Edge Deployment of Small Language Models, a comprehensive comparison of CPU, GPU and NPU backends](https://arxiv.org/abs/2511.22334)
*Pablo Prieto,Pablo Abad*

Main category: cs.PF

TL;DR: 该研究评估了在边缘设备上运行小型语言模型（SLMs）时，不同硬件平台（包括CPU、GPU和NPU）的推理性能与能效表现，发现专用硬件（尤其是NPU）在性能和能效综合指标上显著优于通用CPU。


<details>
  <summary>Details</summary>
Motivation: 边缘设备资源受限，难以支持大语言模型，而小型语言模型（SLMs）虽适合部署，但如何在有限资源下选择最优硬件平台仍具挑战。

Method: 在统一执行框架下，使用多种先进SLMs，在商用CPU（Intel、ARM）、GPU（NVIDIA）和NPU（RaiderChip）平台上评估其推理性能与能效，并采用带宽归一化进行公平跨架构比较。

Result: NPU在性能上大幅领先；低功耗ARM处理器在能效方面表现良好，但综合性能与功耗的指标（如EDP）仍显示NPU最具优势。

Conclusion: 针对效率与性能共同优化的专用硬件（如NPU）在边缘计算场景中具有明显优势，更适合部署SLMs。

Abstract: Edge computing processes data where it is generated, enabling faster decisions, lower bandwidth usage, and improved privacy. However, edge devices typically operate under strict constraints on processing power, memory, and energy consumption, making them unsuitable for large language models (LLMs). Fortunately, Small Language Models (SLMs) offer lightweight alternatives that bring AI inference to resource-constrained environments by significantly reducing computational cost while remaining suitable for specialization and customization. In this scenario, selecting the hardware platform that best balances performance and efficiency for SLM inference is challenging due to strict resource limitations. To address this issue, this study evaluates the inference performance and energy efficiency of commercial CPUs (Intel and ARM), GPUs (NVIDIA), and NPUs (RaiderChip) for running SLMs. GPUs, the usual platform of choice, are compared against commercial NPUs and recent multi-core CPUs. While NPUs leverage custom hardware designs optimized for computation, modern CPUs increasingly incorporate dedicated features targeting language-model workloads. Using a common execution framework and a suite of state-of-the-art SLMs, we analyze both maximum achievable performance and processing and energy efficiency across commercial solutions available for each platform. The results indicate that specialized backends outperform general-purpose CPUs, with NPUs achieving the highest performance by a wide margin. Bandwidth normalization proves essential for fair cross-architecture comparisons. Although low-power ARM processors deliver competitive results when energy usage is considered, metrics that combine performance and power (such as EDP) again highlight NPUs as the dominant architecture. These findings show that designs optimized for both efficiency and performance offer a clear advantage for edge workloads.

</details>


### [17] [What Is the Optimal Ranking Score Between Precision and Recall? We Can Always Find It and It Is Rarely $F_1$](https://arxiv.org/abs/2511.22442)
*Sébastien Piérard,Adrien Deliège,Marc Van Droogenbroeck*

Main category: cs.PF

TL;DR: 本文重新审视了广泛使用的Fβ分数在排序模型性能时的有效性，指出其虽常用于调和精确率与召回率之间的矛盾排序，但并不总能提供最优折中。作者通过Kendall秩相关构建优化框架，证明F1等常用指标并非理想选择，并提出一种通用方法以闭式解确定对任意性能分布最优的β值。


<details>
  <summary>Details</summary>
Motivation: 在分类任务中，精确率和召回率是两个互补但常相互冲突的重要指标，实践中常使用Fβ分数（如F1）对其加权调和平均以获得单一排序。然而，这种平均是否能产生有意义且平衡的排序尚缺乏理论保证，亟需澄清Fβ在排序中的作用及其最优参数选择。

Method: 作者将寻找两个指标间折中排序的问题形式化为基于Kendall秩相关的优化问题；通过理论分析建立Fβ诱导排序与精确率、召回率排序之间的关系；并推导出针对任意性能分布可计算最优β值的闭式表达式。

Result: 研究发现F1及其对偏斜不敏感的变体在折中排序方面远非最优；所提出的理论工具能在六个案例研究中有效确定最优β，从而生成更合理的全局排序。

Conclusion: Fβ分数虽被广泛使用，但其默认参数（如β=1）未必能提供精确率与召回率之间的最佳权衡；通过本文提出的优化框架和闭式解，可为具体应用场景定制最优β，从而获得更有意义的模型排序。

Abstract: Ranking methods or models based on their performance is of prime importance but is tricky because performance is fundamentally multidimensional. In the case of classification, precision and recall are scores with probabilistic interpretations that are both important to consider and complementary. The rankings induced by these two scores are often in partial contradiction. In practice, therefore, it is extremely useful to establish a compromise between the two views to obtain a single, global ranking. Over the last fifty years or so,it has been proposed to take a weighted harmonic mean, known as the F-score, F-measure, or $F_β$. Generally speaking, by averaging basic scores, we obtain a score that is intermediate in terms of values. However, there is no guarantee that these scores lead to meaningful rankings and no guarantee that the rankings are good tradeoffs between these base scores. Given the ubiquity of $F_β$ scores in the literature, some clarification is in order. Concretely: (1) We establish that $F_β$-induced rankings are meaningful and define a shortest path between precision- and recall-induced rankings. (2) We frame the problem of finding a tradeoff between two scores as an optimization problem expressed with Kendall rank correlations. We show that $F_1$ and its skew-insensitive version are far from being optimal in that regard. (3) We provide theoretical tools and a closed-form expression to find the optimal value for $β$ for any distribution or set of performances, and we illustrate their use on six case studies.

</details>


### [18] [Motion-to-Motion Latency Measurement Framework for Connected and Autonomous Vehicle Teleoperation](https://arxiv.org/abs/2511.22467)
*François Provost,Faisal Hawlader,Mehdi Testouri,Raphaël Frank*

Main category: cs.PF

TL;DR: 本文提出了一种用于测量远程操控自动驾驶车辆中“动作到动作”（M2M）延迟的框架，利用霍尔效应传感器和两个同步的树莓派5设备，实现了10–15毫秒的测量精度，并发现执行器延迟是M2M延迟的主要来源，中位数超过750毫秒。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注视频传输端到端（G2G）延迟，缺乏对远程操作中物理动作到车辆响应（M2M）延迟的标准测量方法，而该延迟直接影响操作响应性能。

Method: 采用霍尔效应传感器与两个时间同步的树莓派5设备，在操作端和车辆端分别记录基于中断的时间戳，从而独立于具体远程操控架构地估算M2M延迟。

Result: 系统精度测试达到10–15毫秒；实地测试表明执行器延迟主导了M2M延迟，中位延迟超过750毫秒。

Conclusion: 所提出的M2M延迟测量框架有效且具有高精度，揭示了当前远程操控系统中执行器延迟是性能瓶颈，为未来系统优化提供了依据。

Abstract: Latency is a key performance factor for the teleoperation of Connected and Autonomous Vehicles (CAVs). It affects how quickly an operator can perceive changes in the driving environment and apply corrective actions. Most existing work focuses on Glass-to-Glass (G2G) latency, which captures delays only in the video pipeline. However, there is no standard method for measuring Motion-to-Motion (M2M) latency, defined as the delay between the physical steering movement of the remote operator and the corresponding steering motion in the vehicle. This paper presents an M2M latency measurement framework that uses Hall-effect sensors and two synchronized Raspberry Pi~5 devices. The system records interrupt-based timestamps on both sides to estimate M2M latency, independently of the underlying teleoperation architecture. Precision tests show an accuracy of 10--15~ms, while field results indicate that actuator delays dominate M2M latency, with median values above 750~ms.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [19] [Technical knowledge and soft skills in software startups within the Colombian entrepreneurial ecosystem](https://arxiv.org/abs/2511.21769)
*Royer David Estrada-Esponda,Gerardo Matturro,Jose Reinaldo Sabogal-Pinilla*

Main category: cs.SE

TL;DR: 该研究调查了哥伦比亚软件初创企业创始团队最重视的技术知识和软技能，并探讨了这些需求如何随企业发展而变化。


<details>
  <summary>Details</summary>
Motivation: 初创企业的成败很大程度上取决于创始团队成员的质量，因此有必要明确在软件初创企业早期阶段哪些技术知识和软技能最为关键。

Method: 通过对哥伦比亚软件初创企业代表进行问卷调查，识别最受重视的技术知识和软技能。

Result: 最受重视的技术知识包括需求工程、软件测试、项目规划与管理、敏捷方法、市场营销、商业模式定义和预算编制；最受重视的软技能是沟通、领导力和团队合作。

Conclusion: 研究结果对软件创业者、孵化器和研究人员具有参考价值，有助于理解初创企业在不同成长阶段对人才能力的需求变化。

Abstract: The technical knowledge and soft skills of entrepreneurial team members significantly impact the early stages of software startups. It is widely recognized that the success or failure of a startup is determined by the quality of the individuals who constitute the founding team. This article presents the findings of a study conducted within the Colombian entrepreneurial ecosystem, focusing on which technical knowledge and soft skills are the most valued by founding teams of software startups, and how the needs for knowledge and skills evolve as the startup grows. A survey of software startup representatives revealed that the most valued knowledge includes requirements engineering, software testing, project planning and management, agile methodologies, marketing, business model definition, and budgeting. The most valued soft skills are typically communication, leadership, and teamwork. The outcomes of this work are relevant to software entrepreneurs, incubators, and researchers.

</details>


### [20] [Code Refactoring with LLM: A Comprehensive Evaluation With Few-Shot Settings](https://arxiv.org/abs/2511.21788)
*Md. Raihan Tapader,Md. Mostafizer Rahman,Ariful Islam Shiplu,Md Faizul Ibne Amin,Yutaka Watanobe*

Main category: cs.SE

TL;DR: 本文提出了一种基于大语言模型（LLM）的多语言代码重构框架，结合提示工程与少样本学习，在C、C++、C#、Python和Java上实现高效重构，并通过多种指标评估重构质量。实验表明，Java在正确性和可编译性方面表现最佳，Python则展现出最小的结构改动。


<details>
  <summary>Details</summary>
Motivation: 现有代码重构方法依赖人工编写的转换规则，难以跨编程语言和编码风格泛化。随着对简洁、清晰、高效和可持续代码的需求增加，亟需一种通用且高效的自动化重构方案。

Method: 提出一个结合指令微调、提示工程（如Temperature控制和不同shot算法）与少样本学习的LLM框架，用于多语言代码重构，并通过编译性、正确性、结构距离、相似度等多个指标进行评估。

Result: Java在10-shot设置下达到99.99%的正确率和94.78%的平均可编译性，同时保持约53-54%的相似度；Python在所有shot设置中结构距离最低（约277-294），相似度为44-48%，表明其重构一致性高且干扰小。

Conclusion: 基于LLM的提示工程与少样本学习方法能有效实现多语言代码重构，在保持语义一致的同时提升代码质量，尤其在Java和Python中表现突出，验证了该方法的可行性与优势。

Abstract: In today's world, the focus of programmers has shifted from writing complex, error-prone code to prioritizing simple, clear, efficient, and sustainable code that makes programs easier to understand. Code refactoring plays a critical role in this transition by improving structural organization and optimizing performance. However, existing refactoring methods are limited in their ability to generalize across multiple programming languages and coding styles, as they often rely on manually crafted transformation rules. The objectives of this study are to (i) develop an Large Language Models (LLMs)-based framework capable of performing accurate and efficient code refactoring across multiple languages (C, C++, C#, Python, Java), (ii) investigate the impact of prompt engineering (Temperature, Different shot algorithm) and instruction fine-tuning on refactoring effectiveness, and (iii) evaluate the quality improvements (Compilability, Correctness, Distance, Similarity, Number of Lines, Token, Character, Cyclomatic Complexity) in refactored code through empirical metrics and human assessment. To accomplish these goals, we propose a fine-tuned prompt-engineering-based model combined with few-shot learning for multilingual code refactoring. Experimental results indicate that Java achieves the highest overall correctness up to 99.99% the 10-shot setting, records the highest average compilability of 94.78% compared to the original source code and maintains high similarity (Approx. 53-54%) and thus demonstrates a strong balance between structural modifications and semantic preservation. Python exhibits the lowest structural distance across all shots (Approx. 277-294) while achieving moderate similarity ( Approx. 44-48%) that indicates consistent and minimally disruptive refactoring.

</details>


### [21] [LLM-Empowered Event-Chain Driven Code Generation for ADAS in SDV systems](https://arxiv.org/abs/2511.21877)
*Nenad Petrovic,Norbert Kroth,Axel Torschmied,Yinglei Song,Fengjunjie Pan,Vahid Zolfaghari,Nils Purschke,Sven Kirchner,Chengdong Wu,Andre Schamschurko,Yi Zhang,Alois Knoll*

Main category: cs.SE

TL;DR: 本文提出了一种基于事件链驱动、大语言模型（LLM）增强的工作流，用于从自然语言需求生成经过验证的汽车代码。


<details>
  <summary>Details</summary>
Motivation: 为了解决从自然语言需求直接生成汽车代码时可能出现的幻觉问题和架构不一致性，确保生成代码的行为正确性和实时可行性。

Method: 采用检索增强生成（RAG）层从不断演化的车辆信号规范（VSS）目录中检索相关信号作为提示上下文；将检索到的信号映射并验证后转换为编码因果与时序约束的事件链；利用这些事件链引导并约束LLM进行代码合成。

Result: 在紧急制动案例研究中，该方法实现了有效的信号使用和一致的代码生成，且无需对LLM进行重新训练。

Conclusion: 所提出的方法能够有效提升从自然语言需求生成汽车代码的准确性与可靠性，同时避免了对大语言模型的再训练需求。

Abstract: This paper presents an event-chain-driven, LLM-empowered workflow for generating validated, automotive code from natural-language requirements. A Retrieval-Augmented Generation (RAG) layer retrieves relevant signals from large and evolving Vehicle Signal Specification (VSS) catalogs as code generation prompt context, reducing hallucinations and ensuring architectural correctness. Retrieved signals are mapped and validated before being transformed into event chains that encode causal and timing constraints. These event chains guide and constrain LLM-based code synthesis, ensuring behavioral consistency and real-time feasibility. Based on our initial findings from the emergency braking case study, with the proposed approach, we managed to achieve valid signal usage and consistent code generation without LLM retraining.

</details>


### [22] [Advancing Automated In-Isolation Validation in Repository-Level Code Translation](https://arxiv.org/abs/2511.21878)
*Kaiyao Ke,Ali Reza Ibrahimzada,Rangeet Pan,Saurabh Sinha,Reyhaneh Jabbarvand*

Main category: cs.SE

TL;DR: TRAM 是一种用于仓库级代码翻译的新方法，通过结合上下文感知的类型解析与基于模拟对象的隔离验证，实现了高质量的跨语言翻译，尤其在 Java 到 Python 的翻译中表现优异。


<details>
  <summary>Details</summary>
Motivation: 当前仓库级代码翻译缺乏高效、自动化的验证机制，现有方法要么依赖高成本的智能体验证，要么需要大量人工干预，因此亟需一种更可靠且自动化的验证方案。

Method: TRAM 在翻译前检索源语言中各变量类型的 API 文档和上下文代码信息，并利用这些信息引导大语言模型（LLM）进行精准的跨语言类型映射；随后，通过自定义的序列化/反序列化流程，在目标语言中自动生成等效的模拟对象，从而实现方法片段的隔离验证。

Result: TRAM 在 Java 到 Python 的仓库级代码翻译任务中达到当前最优性能，验证了其结合 RAG 型类型解析与隔离验证机制的有效性。

Conclusion: TRAM 通过上下文感知的类型解析与自动化隔离验证，显著提升了仓库级代码翻译的质量与可验证性，为跨语言迁移提供了一种高效、低人工干预的解决方案。

Abstract: Repository-level code translation aims to migrate entire repositories across programming languages while preserving functionality automatically. Despite advancements in repository-level code translation, validating the translations remains challenging. This paper proposes TRAM, which combines context-aware type resolution with mock-based in-isolation validation to achieve high-quality translations between programming languages. Prior to translation, TRAM retrieves API documentation and contextual code information for each variable type in the source language. It then prompts a large language model (LLM) with retrieved contextual information to resolve type mappings across languages with precise semantic interpretations. Using the automatically constructed type mapping, TRAM employs a custom serialization/deserialization workflow that automatically constructs equivalent mock objects in the target language. This enables each method fragment to be validated in isolation, without the high cost of using agents for translation validation, or the heavy manual effort required by existing approaches that rely on language interoperability. TRAM demonstrates state-of-the-art performance in Java-to-Python translation, underscoring the effectiveness of its integration of RAG-based type resolution with reliable in-isolation validation.

</details>


### [23] [Toward Automated and Trustworthy Scientific Analysis and Visualization with LLM-Generated Code](https://arxiv.org/abs/2511.21920)
*Apu Kumar Chakroborti,Yi Ding,Lipeng Wan*

Main category: cs.SE

TL;DR: 本文评估开源大语言模型（LLMs）在自动生成科学数据分析与可视化Python脚本方面的可靠性，发现其在无干预情况下存在执行失败问题，并提出三种改进策略以提升生成代码的可执行性与正确性。


<details>
  <summary>Details</summary>
Motivation: 现代科学研究日益依赖大规模复杂数据，但许多领域科学家缺乏编程能力，难以高效构建分析流程；大语言模型虽可通过自然语言生成代码，但其在科学场景下的可信度尚不明确。

Method: 构建反映真实科研任务的提示基准集，系统评估LLM生成代码的可执行性与正确性；并设计三种策略：数据感知的提示消歧、检索增强的提示优化和迭代错误修复。

Result: 未经人工干预时，LLM生成的代码可靠性有限，常因提示模糊或缺乏领域理解而失败；所提三种策略显著提升了代码执行成功率和输出质量。

Conclusion: LLM在科学工作流自动化中具有潜力，但仍存在局限；本文提出的策略和可复用基准为构建更可信、易用的AI辅助科研工具提供了可行路径。

Abstract: As modern science becomes increasingly data-intensive, the ability to analyze and visualize large-scale, complex datasets is critical to accelerating discovery. However, many domain scientists lack the programming expertise required to develop custom data analysis workflows, creating barriers to timely and effective insight. Large language models (LLMs) offer a promising solution by generating executable code from natural language descriptions. In this paper, we investigate the trustworthiness of open-source LLMs in autonomously producing Python scripts for scientific data analysis and visualization. We construct a benchmark suite of domain-inspired prompts that reflect real-world research tasks and systematically evaluate the executability and correctness of the generated code. Our findings show that, without human intervention, the reliability of LLM-generated code is limited, with frequent failures caused by ambiguous prompts and the models' insufficient understanding of domain-specific contexts. To address these challenges, we design and assess three complementary strategies: data-aware prompt disambiguation, retrieval-augmented prompt enhancement, and iterative error repair. While these methods significantly improve execution success rates and output quality, further refinement is needed. This work highlights both the promise and current limitations of LLM-driven automation in scientific workflows and introduces actionable techniques and a reusable benchmark for building more inclusive, accessible, and trustworthy AI-assisted research tools.

</details>


### [24] [Beyond Like-for-Like: A User-centered Approach to Modernizing Legacy Applications](https://arxiv.org/abs/2511.21956)
*M. Polzin,M. Guzman*

Main category: cs.SE

TL;DR: 在现代化遗留应用程序时，不应仅做表面翻新，而应通过用户参与优化用户体验，兼顾新老用户需求。


<details>
  <summary>Details</summary>
Motivation: 避免在现代化过程中简单复制旧系统，从而延续其低效流程和痛点，导致新系统无法真正满足用户需求。

Method: 通过让用户（包括新用户和专家用户）积极参与，结合现有遗留系统的洞察，指导新应用的设计与开发。

Result: 能够弥合“逐字复制”与引入全新GUI设计之间的差距，打造更直观、高效且被用户接受的应用程序。

Conclusion: 遗留应用本身可作为新开发的宝贵参考，通过用户参与可有效提升现代化成果的实用性和可用性。

Abstract: When modernizing a legacy application, it is easy to fall back on a like-for-like replica with new tools and updated design stylings, but this is an opportunity to explore making a more intuitive application that supports user tasks and efficiency. Rather than having a blank canvas-unburdened by legacy tech debt-to create a new application, you are working with an existing application that is integral to accelerator operations and one that expert users are already familiar with. Due to this, you might assume people will prefer the like-for-like, but you could be carrying forward the pain points, processes that are inefficient, and ultimately wind up with an application that no one wants to use because it doesn't solve existing problems. Getting users involved can make all the difference in your approach to modernizing a legacy application that caters to both newer and expert users. It also can bridge the gap between like-for-like and introducing new GUI design. Having a legacy application doesn't have to make the modernized one difficult to develop, as the existing application is a tool in how you move forward with the new application. It provides insight into areas that an application with a clean slate doesn't give you.

</details>


### [25] [DRS-OSS: LLM-Driven Diff Risk Scoring Tool for PR Risk Prediction](https://arxiv.org/abs/2511.21964)
*Ali Sayedsalehi,Peter C. Rigby,Audris Mockus*

Main category: cs.SE

TL;DR: 本文提出了DRS-OSS，一个开源的差异风险评分系统，利用微调后的Llama 3.1 8B模型对代码提交进行缺陷风险预测，在ApacheJIT数据集上达到SOTA性能，并通过GitHub插件等工具集成到开发者工作流中。


<details>
  <summary>Details</summary>
Motivation: 在大规模开源项目中，每天有大量拉取请求合并，可能引入回归缺陷。需要一种机制来评估每个代码变更引入缺陷的可能性，以优化代码审查、测试规划和CI/CD流程。

Method: 基于ApacheJIT数据集，使用参数高效微调（4-bit QLoRA）、DeepSpeed ZeRO-3 CPU卸载等技术，在单张20GB GPU上训练支持22k token上下文的Llama 3.1 8B序列分类器；输入包括提交信息、结构化diff和变更指标；系统提供API、Web界面和GitHub插件。

Result: 在ApacheJIT基准测试中，DRS-OSS取得F1=0.64、ROC-AUC=0.89的SOTA结果；模拟显示，仅对风险最高的30%提交进行拦截，可防止高达86.4%的缺陷引入。

Conclusion: DRS-OSS有效实现了高精度的代码变更风险预测，具备良好的工程实用性与开源生态集成能力，已公开发布完整复现包和部署资源。

Abstract: In large-scale open-source projects, hundreds of pull requests land daily, each a potential source of regressions. Diff Risk Scoring (DRS) estimates the likelihood that a diff will introduce a defect, enabling better review prioritization, test planning, and CI/CD gating. We present DRS-OSS, an open-source DRS system equipped with a public API, web UI, and GitHub plugin. DRS-OSS uses a fine-tuned Llama 3.1 8B sequence classifier trained on the ApacheJIT dataset, consuming long-context representations that combine commit messages, structured diffs, and change metrics. Through parameter-efficient adaptation, 4-bit QLoRA, and DeepSpeed ZeRO-3 CPU offloading, we train 22k-token contexts on a single 20 GB GPU. On the ApacheJIT benchmark, DRS-OSS achieves state-of-the-art performance (F1 = 0.64, ROC-AUC = 0.89). Simulations show that gating only the riskiest 30% of commits can prevent up to 86.4% of defect-inducing changes. The system integrates with developer workflows through an API gateway, a React dashboard, and a GitHub App that posts risk labels on pull requests. We release the full replication package, fine-tuning scripts, deployment artifacts, code, demo video, and public website.

</details>


### [26] [Statistical Independence Aware Caching for LLM Workflows](https://arxiv.org/abs/2511.22118)
*Yihan Dai,Dimitrios Stamatios Bouras,Haoxiang Jia,Sergey Mechtaev*

Main category: cs.SE

TL;DR: 本文提出Mnimi，一种新型缓存设计模式，在保证大语言模型（LLM）推理中统计独立性的同时，提升可复现性、调试便利性及成本效率。


<details>
  <summary>Details</summary>
Motivation: 现有LLM缓存方法在重用响应时会破坏统计独立性，影响基于概率的工作流（如Pass@k、不确定性估计和程序修复等）的正确性，亟需一种兼顾缓存效益与统计完整性的解决方案。

Method: 作者提出Mnimi缓存设计模式，通过将统计约束封装在LLM引用类型中，并结合Python装饰器与无限序列迭代器实现，使用户能按算法需求管理与转换这些类型。

Result: 在SpecFix自动程序规范修复系统上的案例研究表明，Mnimi在保持统计正确性的同时，显著提升了可复现性、调试便捷性以及时间和成本效率。

Conclusion: Mnimi有效解决了LLM缓存中统计独立性缺失的问题，为模块化LLM工作流提供了一种兼顾效率与统计完整性的实用方案。

Abstract: Large language models (LLMs) inference is both expensive and slow. Local caching of responses offers a practical solution to reduce the cost and latency of LLM queries. In research contexts, caching also enhances reproducibility and provides flexibility for experimentation. However, naive reuse of cached responses compromises statistical independence, a critical property for probabilistic workflows. In applications of LLM for code, it underpins performance metrics such as Pass@k and uncertainty estimation, as well as algorithms like program repair loops and retries. Existing LLM caching systems lack ways to enforce statistical independence constraints. To address this, we introduce Mnimi, a cache design pattern that supports modular LLM workflows while ensuring statistical integrity at the component level. Its core innovation lies in encapsulating statistical constraints within the type of LLM references, allowing users to manage and transform these types according to the scope and requirements of their algorithm. We implemented this design pattern in Python using a combination of decorators and iterators over infinite sequences. A case study on SpecFix, an recent automated program specification repair system, highlights how Mnimi improves reproducibility, ease of debugging, time and cost efficiency while preserving statistical correctness.

</details>


### [27] [Exploring the SECURITY.md in the Dependency Chain: Preliminary Analysis of the PyPI Ecosystem](https://arxiv.org/abs/2511.22186)
*Chayanid Termphaiboon,Raula Gaikovina Kula,Youmei Fan,Morakot Choetkiertikul,Chaiyong Ragkhitwetsagul,Thanwadee Sunetnanta,Kenichi Matsumoto*

Main category: cs.SE

TL;DR: 该研究发现，拥有 SECURITY.md 安全策略的 PyPI 项目倾向于使用更多直接依赖项，并更频繁地更新依赖，表明安全策略与更主动的依赖管理相关。


<details>
  <summary>Details</summary>
Motivation: 尽管开源项目中 SECURITY.md 等安全策略日益普及，但其对软件依赖结构和演化的影响尚不明确。本研究旨在揭示安全策略与依赖管理之间的关系。

Method: 分析包含与不包含 SECURITY.md 文件的 PyPI 项目的依赖树，并追踪其随时间变化的依赖更新行为。

Result: 有安全策略的项目使用更广泛的直接依赖，但整体依赖深度和传递依赖相似；后期采用 SECURITY.md 的项目表现出更频繁的依赖更新。

Conclusion: SECURITY.md 与更模块化、功能丰富的项目相关，并在推动主动依赖管理、降低软件供应链风险方面发挥积极作用。

Abstract: Security policies, such as SECURITY.md files, are now common in open-source projects. They help guide responsible vulnerability reporting and build trust among users and contributors. Despite their growing use, it is still unclear how these policies influence the structure and evolution of software dependencies. Software dependencies are external packages or libraries that a project relies on, and their interconnected nature affects both functionality and security. This study explores the relationship between security policies and dependency management in PyPI projects. We analyzed projects with and without a SECURITY.md file by examining their dependency trees and tracking how dependencies change over time. The analysis shows that projects with a security policy tend to rely on a broader set of direct dependencies, while overall depth and transitive dependencies remain similar. Historically, projects created after the introduction of SECURITY.md, particularly later adopters, show more frequent dependency updates. These results suggest that security policies are linked to more modular and feature-rich projects, and highlight the role of SECURITY.md in promoting proactive dependency management and reducing risks in the software supply chain.

</details>


### [28] [NOMAD: A Multi-Agent LLM System for UML Class Diagram Generation from Natural Language Requirements](https://arxiv.org/abs/2511.22409)
*Polydoros Giannouris,Sophia Ananiadou*

Main category: cs.SE

TL;DR: 本文提出了NOMAD，一个受认知启发的多智能体框架，用于生成UML类图。该框架将建模任务分解为多个角色专用子任务，提升了可解释性与验证能力，在实验中优于现有基线，并首次系统地对LLM生成UML中的错误进行了分类。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）在软件工程中的应用日益广泛，但其生成如UML图等结构化产物的能力尚未得到充分探索。现有方法缺乏对建模过程的分解、可解释性和有效验证机制。

Method: 提出NOMAD框架，采用模块化多智能体架构，将UML生成任务分解为实体抽取、关系分类和图表合成等子任务，每个智能体专精于特定建模活动；通过混合评估设计（包括Northwind案例研究和人工编写的UML练习）进行验证，并构建了LLM生成UML错误的系统分类法。

Result: NOMAD在实验中优于所有选定基线，揭示了细粒度属性提取方面的持续挑战，并首次提出了LLM生成UML图的错误分类体系（结构、关系、语义/逻辑三类）；同时探讨了验证机制的设计作用及其适应性策略。

Conclusion: NOMAD不仅是一个高效的UML类图生成框架，也为语言到模型工作流的可靠性研究提供了新的视角和基础。

Abstract: Large Language Models (LLMs) are increasingly utilised in software engineering, yet their ability to generate structured artefacts such as UML diagrams remains underexplored. In this work we present NOMAD, a cognitively inspired, modular multi-agent framework that decomposes UML generation into a series of role-specialised subtasks. Each agent handles a distinct modelling activity, such as entity extraction, relationship classification, and diagram synthesis, mirroring the goal-directed reasoning processes of an engineer. This decomposition improves interpretability and allows for targeted verification strategies. We evaluate NOMAD through a mixed design: a large case study (Northwind) for in-depth probing and error analysis, and human-authored UML exercises for breadth and realism. NOMAD outperforms all selected baselines, while revealing persistent challenges in fine-grained attribute extraction. Building on these observations, we introduce the first systematic taxonomy of errors in LLM-generated UML diagrams, categorising structural, relationship, and semantic/logical. Finally, we examine verification as a design probe, showing its mixed effects and outlining adaptive strategies as promising directions. Together, these contributions position NOMAD as both an effective framework for UML class diagram generation and a lens onto the broader research challenges of reliable language-to-model workflows.

</details>


### [29] [Declarative Policy Control for Data Spaces: A DSL-Based Approach for Manufacturing-X](https://arxiv.org/abs/2511.22513)
*Jérôme Pfeiffer,Nicolai Maisch,Sebastian Friedl,Matthias Milan Strljic,Armin Lechler,Oliver Riedel,Andreas Wortmann*

Main category: cs.SE

TL;DR: 本文提出一种基于领域特定语言（DSL）的方法，使非软件工程背景的领域专家能够以声明式、可读且可机器执行的方式定义上下文相关的数据使用策略，用于主权数据共享。


<details>
  <summary>Details</summary>
Motivation: 当前在工业4.0中广泛采用的联邦数据空间（如GAIA-X和IDS）缺乏对上下文相关数据使用策略的实用描述与执行机制，尤其难以被非软件工程背景的领域专家使用。

Method: 利用领域特定语言（DSL）构建一种声明式、人类可读且机器可执行的策略定义方法，通过数据空间连接器实现细粒度的数据治理规则设定。

Result: 该方法允许领域专家无需编写命令式代码即可指定如限制特定生产批次数据访问或设置自动删除保留期等精细数据治理要求。

Conclusion: 所提出的DSL方法有效弥合了现有数据空间技术与实际业务需求之间的鸿沟，提升了主权数据共享的可用性与可控性。

Abstract: The growing adoption of federated data spaces, such as in the GAIA-X and the International Data Spaces (IDS) initiative, promises secure and sovereign data sharing across organizational boundaries in Industry 4.0. In manufacturing ecosystems, this enables use cases, such as cross-factory process optimization, predictive maintenance, and supplier integration. Frameworks and standards, such as the Asset Administration Shell (AAS), Eclipse Dataspace Connector (EDC), ID-Link and Open Platform Communications Unified Architecture (OPC UA) provide a strong foundation to realize this ecosystem. However, a major open challenge is the practical description and enforcement of context-dependent data usage policies using these base technologies - especially by domain experts without software engineering backgrounds. Therefore, this article proposes a method for leveraging domain-specific languages (DSLs) to enable declarative, human-readable, and machine-executable policy definitions for sovereign data sharing via data space connectors. The DSL empowers domain experts to specify fine-grained data governance requirements - such as restricting access to data from specific production batches or enforcing automatic deletion after a defined retention period - without writing imperative code.

</details>


### [30] [The Repeat Offenders: Characterizing and Predicting Extremely Bug-Prone Source Methods](https://arxiv.org/abs/2511.22726)
*Ethan Friesen,Sasha Morton-Salmon,Md Nahidul Islam Opu,Shahidul Islam,Shaiful Chowdhury*

Main category: cs.SE

TL;DR: 该论文对98个开源Java项目中超过125万方法进行了大规模研究，发现“极缺陷方法”（即多次引发bug修复的方法）虽占比极小，却导致了大量bug；这些方法在初始阶段就表现出更大、更复杂、可读性和可维护性更差等特征，但因数据不平衡、项目异质性及缺陷随演化产生等原因，早期预测仍不可靠；通过主题分析揭示了其常见视觉问题、上下文角色和缺陷模式，为高风险方法的早期识别提供了实践指导。


<details>
  <summary>Details</summary>
Motivation: 识别反复引发缺陷的少量源代码子集对于降低长期维护成本至关重要，但目前缺乏对此类“极缺陷方法”的系统性研究。

Method: 结合大规模定量分析（基于1.25M+ Java方法）与定性主题分析（265个极缺陷方法），评估五种机器学习模型的预测能力，并归纳其结构性、上下文和缺陷模式特征。

Result: 极缺陷方法占比极小但贡献大量bug；初始阶段已具显著不良特征；现有ML模型难以可靠预测；主题分析揭示了三类共性：混乱控制流等视觉问题、核心逻辑等角色、条件错误等缺陷模式。

Conclusion: 需发展能捕捉代码演化信息的更丰富表示方法，当前静态特征不足以支撑可靠早期预测；研究结果可帮助开发者优先审查高风险方法。

Abstract: Identifying the small subset of source code that repeatedly attracts bugs is critical for reducing long-term maintenance effort. We define ExtremelyBuggy methods as those involved in more than one bug fix and present the first large-scale study of their prevalence, characteristics, and predictability. Using a dataset of over 1.25 million methods from 98 open-source Java projects, we find that ExtremelyBuggy methods constitute only a tiny fraction of all methods, yet frequently account for a disproportionately large share of bugs. At their inception, these methods are significantly larger, more complex, less readable, and less maintainable than both singly-buggy and non-buggy methods. However, despite these measurable differences, a comprehensive evaluation of five machine learning models shows that early prediction of ExtremelyBuggy methods remains highly unreliable due to data imbalance, project heterogeneity, and the fact that many bugs emerge through subsequent evolution rather than initial implementation. To complement these quantitative findings, we conduct a thematic analysis of 265 ExtremelyBuggy methods, revealing recurring visual issues (e.g., confusing control flow, poor readability), contextual roles (e.g., core logic, data transformation, external resource handling), and common defect patterns (e.g., faulty conditionals, fragile error handling, misuse of variables). These results highlight the need for richer, evolution-aware representations of code and provide actionable insights for practitioners seeking to prioritize high-risk methods early in the development lifecycle.

</details>


### [31] [MBFL-DKMR: Improving Mutation-based Fault Localization through Denoising-based Kill Matrix Refinement](https://arxiv.org/abs/2511.22921)
*Hengyuan Liu,Xia Song,Yong Liu,Zheng Li*

Main category: cs.SE

TL;DR: 本文提出一种基于信号去噪思想的新方法DKMR，用于优化变异体-测试关系矩阵（kill matrix），从而提升基于变异的故障定位（MBFL）的准确性。该方法通过混合矩阵构建增强信号，并在频域中滤除高频噪声，在Defects4J数据集上显著优于现有技术。


<details>
  <summary>Details</summary>
Motivation: 现有MBFL方法受“虚假杀死”（false kill）噪声影响，导致定位效果下降；当前研究多聚焦于修正最终结果，而非从源头净化核心数据结构——kill matrix。

Method: 将kill matrix视为含噪声信号，提出DKMR框架：首先通过混合矩阵构造增强信噪比，再利用频域滤波进行去噪；在此基础上构建MBFL-DKMR框架，使用去噪后的模糊值矩阵计算可疑度。

Result: 在Defects4J v2.0.0上，MBFL-DKMR在Top-1定位129个故障，优于BLMu（85个）和Delta4Ms（103个），且额外开销极低（仅0.11秒，占总时间0.001%）。

Conclusion: 将信号处理中的去噪思想引入MBFL可有效提升故障定位精度，验证了对kill matrix进行源头净化的有效性与可行性。

Abstract: Software debugging is a critical and time-consuming aspect of software development, with fault localization being a fundamental step that significantly impacts debugging efficiency. Mutation-Based Fault Localization (MBFL) has gained prominence due to its robust theoretical foundations and fine-grained analysis capabilities. However, recent studies have identified a critical challenge: noise phenomena, specifically the false kill relationships between mutants and tests, which significantly degrade localization effectiveness. While several approaches have been proposed to rectify the final localization results, they do not directly address the underlying noise. In this paper, we propose a novel approach to refine the kill matrix, a core data structure capturing mutant-test relationships in MBFL, by treating it as a signal that contains both meaningful fault-related patterns and high-frequency noise. Inspired by signal processing theory, we introduce DKMR (Denoising-based Kill Matrix Refinement), which employs two key stages: (1) signal enhancement through hybrid matrix construction to improve the signal-to-noise ratio for better denoising, and (2) signal denoising via frequency domain filtering to suppress noise while preserving fault-related patterns. Building on this foundation, we develop MBFL-DKMR, a fault localization framework that utilizes the refined matrix with fuzzy values for suspiciousness calculation. Our evaluation on Defects4J v2.0.0 demonstrates that MBFL-DKMR effectively mitigates the noise and outperforms the state-of-the-art MBFL techniques. Specifically, MBFL-DKMR achieves 129 faults localized at Top-1 compared to 85 for BLMu and 103 for Delta4Ms, with negligible additional computational overhead (0.11 seconds, 0.001\% of total time). This work highlights the potential of signal processing techniques to enhance the effectiveness of MBFL by refining the kill matrix.

</details>


### [32] [Software for Studying CASCADE Error Correction Protocols in Quantum Communications](https://arxiv.org/abs/2511.23050)
*Nikita Repnkiov,Vladimir Faerman*

Main category: cs.SE

TL;DR: 本文针对量子计算威胁下的量子通信系统，研究了CASCADE协议并开发了一个用于科研与教学的软件原型，通过基于Actor模型的并行纠错算法提升了密钥协调效率，并提出了若干改进方向。


<details>
  <summary>Details</summary>
Motivation: 随着量子计算的发展，传统加密方法面临威胁，因此需要发展安全的量子通信技术；而密钥协调作为其中关键环节，亟需高效且可验证的实现方案。

Method: 设计并实现了一个基于Actor模型的并行错误校正算法软件原型，用于模拟和研究CASCADE密钥协调协议。

Result: 原型验证了CASCADE核心算法的正确性，提高了密钥协调效率并减少了通信数据量，但也暴露出消息传递开销大、错误处理复杂及代码冗余等问题。

Conclusion: 该研究为量子密钥协调提供了可行的软件实现基础，未来可通过架构重构、接口扩展和系统化验证工具进一步优化。

Abstract: This article addresses the development of quantum communication methods in the context of emerging quantum computing threats and emphasizes the importance of key reconciliation in quantum communication systems. The study focuses on the CASCADE protocol and the design of a software prototype intended for research and educational purposes. A parallel error-correction algorithm based on the actor model was implemented, improving the efficiency of key reconciliation and reducing the amount of exchanged data. Evaluation of the prototype revealed limitations, including the computational cost of message passing, complexity of error handling, and code redundancy due to iterative development. Experimental results confirmed the correct implementation of the core CASCADE algorithms and informed the design of future improvements. Proposed enhancements include redesigning the system architecture, developing interfaces for exporting intermediate data, defining the communication channel as a separate component, and expanding tools for systematic verification and comparative analysis of blind key-reconciliation methods.

</details>


### [33] [Amplifiers or Equalizers? A Longitudinal Study of LLM Evolution in Software Engineering Project-Based Learning](https://arxiv.org/abs/2511.23157)
*Hana Kataoka,Jialong Li,Yutaka Matsuno*

Main category: cs.SE

TL;DR: 本文通过两年纵向研究，探讨了最新大语言模型（LLM）在软件工程项目式学习中的双重作用：既提升整体学生表现（“均衡器”效应），又扩大成绩差距（“放大器”效应）。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型（LLM）重塑软件开发，将其整合进软件工程（SE）教育变得迫切。现有研究多聚焦于入门编程或孤立的SE任务，缺乏对更开放的项目式学习（PBL）环境中LLM影响的探索。

Method: 开展为期两年的纵向研究，比较2024年（使用早期免费LLM，n=48）与2025年（使用最新付费LLM，n=46）两个学生群体在SE项目式学习中的表现差异。

Result: 最新强大的LLM具有双重作用：一方面作为“均衡器”，提升包括编程能力较弱学生在内的整体平均表现，促进更真实的SE实践；另一方面作为“放大器”，显著拉大学生之间的绝对成绩差距。

Conclusion: 尽管先进LLM有助于提升软件工程教育的整体水平，但其加剧教育不平等的现象也带来新的教学挑战，需在教学设计中加以应对。

Abstract: As LLMs reshape software development, integrating LLM-augmented practices into SE education has become imperative. While existing studies explore LLMs' educational use in introductory programming or isolated SE tasks, their impact in more open-ended Project-Based Learning (PBL) remains unexplored. This paper introduces a two-year longitudinal study comparing a 2024 (using early free LLMs, $n$=48) and 2025 (using the latest paid LLMs, $n$=46) cohort. Our findings suggest the latest powerful LLMs' dual role: they act as "equalizers," boosting average performance even for programming-weak students, providing opportunities for more authentic SE practices; yet also as "amplifiers," dramatically widening absolute performance gaps, creating new pedagogical challenges for addressing educational inequities.

</details>


### [34] [AI for software engineering: from probable to provable](https://arxiv.org/abs/2511.23159)
*Bertrand Meyer*

Main category: cs.SE

TL;DR: Vibe coding 面临目标设定困难和幻觉问题，解决方法是结合人工智能的创造力、形式化规约方法的严谨性以及现代证明工具支持的形式化程序验证。


<details>
  <summary>Details</summary>
Motivation: 当前基于 AI 的编程方法（如 vibe coding）在实际应用中受限于两个主要问题：一是难以准确表达编程目标（即提示工程本质上属于需求工程，而需求工程本身是软件工程中最困难的领域之一）；二是 AI 生成代码时容易产生“幻觉”，导致程序不正确或不可靠。

Method: 将人工智能的创造性能力与形式化规约方法的严谨性相结合，并利用现代形式化验证工具对生成的程序进行严格验证。

Result: 该方法有望克服 vibe coding 中的目标模糊性和幻觉问题，从而生成既具创造性又高度可靠的程序。

Conclusion: 要使 AI 编程真正实用，必须融合形式化方法与 AI 技术，以确保程序的正确性与可靠性。

Abstract: Vibe coding, the much-touted use of AI techniques for programming, faces two overwhelming obstacles: the difficulty of specifying goals ("prompt engineering" is a form of requirements engineering, one of the toughest disciplines of software engineering); and the hallucination phenomenon. Programs are only useful if they are correct or very close to correct.
  The solution? Combine the creativity of artificial intelligence with the rigor of formal specification methods and the power of formal program verification, supported by modern proof tools.

</details>


### [35] [GAPS: Guiding Dynamic Android Analysis with Static Path Synthesis](https://arxiv.org/abs/2511.23213)
*Samuele Doria,Eleonora Losiouk*

Main category: cs.SE

TL;DR: GAPS 是首个结合静态方法导向调用图分析与动态交互驱动执行的系统，显著优于现有工具在 Android 应用中动态触达目标方法的能力。


<details>
  <summary>Details</summary>
Motivation: 当前 Android 应用中动态解析方法可达性仍是一个关键且未充分解决的问题，尤其对于非 GUI 组件（如库方法）的目标方法，现有 GUI 测试和静态调用图构建工具难以可靠驱动执行至这些方法，而这对漏洞验证、调试和行为分析至关重要。

Method: GAPS 采用轻量级的、由数据流分析引导的调用图反向遍历，重构通往目标方法的路径，并将这些路径转化为运行时应用探索的指令，从而整合静态分析与动态执行。

Result: 在 AndroTest 基准测试中，GAPS 静态识别 88.24% 目标方法路径（每应用仅需 4.27 秒），动态触达 57.44%；远超 APE（12.82%）、GoalExplorer（9.69%）和 Guardian（17.12%）等动态工具，也优于 FlowDroid（58.81%）和 DroidReach（9.48%）等静态工具。在 50 个热门真实应用中，GAPS 平均静态分析耗时 278.9 秒，静态路径覆盖率达 62.03%，动态触达率达 59.86%。

Conclusion: GAPS 在静态路径重建和动态方法触达方面均显著优于现有技术，展现出在真实场景下分析安全关键代码的强大实用性。

Abstract: Dynamically resolving method reachability in Android applications remains a critical and largely unsolved problem. Despite notable advancements in GUI testing and static call graph construction, current tools are insufficient for reliably driving execution toward specific target methods, especially those not embedded in a graphical component (e.g., libraries' methods), a capability essential for tasks such as vulnerability validation, debugging, and behavioral analysis.
  We present GAPS (Graph-based Automated Path Synthesizer), the first system that integrates static, method-guided call graph analysis with dynamic, interaction-driven execution. GAPS performs a lightweight backward traversal of the call graph, guided by data-flow analysis, to reconstruct paths reaching the target methods. These paths are then translated into instructions that guide runtime app exploration.
  On the AndroTest benchmark, GAPS statically identifies paths to reach 88.24\% of the target methods in just 4.27 seconds per app and dynamically reaches 57.44\% of them. In contrast, state-of-the-art dynamic interaction tools show significantly lower reachability over three runs: APE, one of the best model-based GUI testers, achieves 12.82\%, while GoalExplorer, a hybrid analysis tool, reaches 9.69\%, and Guardian, an LLM-based UI automator, reaches 17.12\%. Static analysis tools also fall short: FlowDroid and DroidReach identify paths to reach 58.81\% and 9.48\% of the targets, requiring 35.06 seconds and 23.46 seconds per app, respectively.
  Finally, an evaluation on the 50 most downloaded real-world apps demonstrates GAPS's practical utility in analyzing security-critical code under a realistic scenario. With an average static analysis time of 278.9 seconds, GAPS statically reconstructs paths to 62.03\% of the target methods and dynamically reaches 59.86\% of them.

</details>


### [36] [FLIMs: Fault Localization Interference Mutants, Definition, Recognition and Mitigation](https://arxiv.org/abs/2511.23302)
*Hengyuan Liu,Zheng Li,Donghua Wang,Yankai Wu,Xiang Chen,Yong Liu*

Main category: cs.SE

TL;DR: 本文提出MBFL-FLIM框架，通过引入故障定位干扰变异体（FLIMs）的概念，并利用基于大语言模型（LLM）的语义分析识别和缓解这些干扰变异体，从而提升基于变异的故障定位（MBFL）的准确性。


<details>
  <summary>Details</summary>
Motivation: 传统MBFL方法因非故障代码生成的干扰变异体（FLIMs）而效果受限，这些变异体能被失败测试杀死，模仿真实故障行为，误导定位结果。

Method: 基于RIPR模型理论分析FLIMs成因，结合微调与置信度估计策略，利用LLM进行语义分析以识别FLIMs，并在MBFL流程中调整可疑度分数以减轻其干扰。

Result: 在Defects4J基准上对395个程序版本的实验表明，MBFL-FLIM在Top-1指标上平均多定位44个故障，优于传统SBFL、MBFL、动态特征方法及现有LLM故障定位技术，且在多故障场景下表现稳健。

Conclusion: 通过语义识别与缓解FLIMs，MBFL-FLIM显著提升了MBFL的故障定位精度和鲁棒性，验证了所提方法的有效性。

Abstract: Mutation-based Fault Localization (MBFL) has been widely explored for automated software debugging, leveraging artificial mutants to identify faulty code entities. However, MBFL faces significant challenges due to interference mutants generated from non-faulty code entities but can be killed by failing tests. These mutants mimic the test sensitivity behaviors of real faulty code entities and weaken the effectiveness of fault localization. To address this challenge, we introduce the concept of Fault Localization Interference Mutants (FLIMs) and conduct a theoretical analysis based on the Reachability, Infection, Propagation, and Revealability (RIPR) model, identifying four distinct interference causes. Building on this, we propose a novel approach to semantically recognize and mitigate FLIMs using LLM-based semantic analysis, enhanced by fine-tuning techniques and confidence estimation strategies to address LLM output instability. The recognized FLIMs are then mitigated by refining the suspiciousness scores calculated from MBFL techniques. We integrate FLIM recognition and mitigation into the MBFL workflow, developing MBFL-FLIM, a fault localization framework that enhances MBFL's effectiveness by reducing misleading interference while preserving real fault-revealing information. Our empirical experiments on the Defects4J benchmark with 395 program versions using eight LLMs demonstrate MBFL-FLIM's superiority over traditional SBFL and MBFL methods, advanced dynamic feature-based approaches, and recent LLM-based fault localization techniques. Specifically, MBFL-FLIM achieves an average improvement of 44 faults in the Top-1 metric, representing a significant enhancement over baseline methods. Further evaluation confirms MBFL-FLIM's robust performance in multi-fault scenarios, with ablation experiments validating the contributions of the fine-tuning and confidence estimation components.

</details>


### [37] [Chart2Code-MoLA: Efficient Multi-Modal Code Generation via Adaptive Expert Routing](https://arxiv.org/abs/2511.23321)
*Yifei Wang,Jacky Keung,Zhenyu Mao,Jingyu Zhang,Yuchen Cao*

Main category: cs.SE

TL;DR: 本文提出C2C-MoLA框架，结合混合专家（MoE）与低秩自适应（LoRA），在图表到代码生成任务中显著提升准确率、降低显存消耗并加快收敛速度。


<details>
  <summary>Details</summary>
Motivation: 现有方法在图表到代码生成任务中难以兼顾跨类型泛化能力、内存效率和模块化设计，亟需更高效且可扩展的多模态模型架构。

Method: 提出C2C-MoLA框架，融合Mixture of Experts（MoE）与Low-Rank Adaptation（LoRA）。MoE采用基于图表复杂度的路由机制，包含领域专用专家和负载均衡稀疏门控；LoRA用于参数高效微调，并配合稳定路由与语义对齐的训练策略。

Result: 在Chart2Code-160k数据集上，相比标准微调和仅LoRA基线，该模型生成准确率最高提升17%，峰值GPU内存减少18%，收敛速度加快20%，尤其在复杂图表上表现突出。消融实验验证了8个专家和rank-8 LoRA为最优配置。

Conclusion: C2C-MoLA通过MoE与LoRA的协同设计，在图表到代码生成任务中实现了高准确性、低资源消耗与良好可扩展性，为多模态程序生成提供了有效解决方案。

Abstract: Chart-to-code generation is a critical task in automated data visualization, translating complex chart structures into executable programs. While recent Multi-modal Large Language Models (MLLMs) improve chart representation, existing approaches still struggle to achieve cross-type generalization, memory efficiency, and modular design. To address these challenges, this paper proposes C2C-MoLA, a multimodal framework that synergizes Mixture of Experts (MoE) with Low-Rank Adaptation (LoRA). The MoE component uses a complexity-aware routing mechanism with domain-specialized experts and load-balanced sparse gating, dynamically allocating inputs based on learnable structural metrics like element count and chart complexity. LoRA enables parameter-efficient updates for resource-conscious tuning, further supported by a tailored training strategy that aligns routing stability with semantic accuracy. Experiments on Chart2Code-160k show that the proposed model improves generation accuracy by up to 17%, reduces peak GPU memory by 18%, and accelerates convergence by 20%, when compared to standard fine-tuning and LoRA-only baselines, particularly on complex charts. Ablation studies validate optimal designs, such as 8 experts and rank-8 LoRA, and confirm scalability for real-world multimodal code generation.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [38] [Secure Command, Control and Communications Systems (C3) for Army UxVs](https://arxiv.org/abs/2511.21936)
*T. Rebolo,A. Grilo,C. Ribeiro*

Main category: cs.NI

TL;DR: 本文提出并实现了一种名为NC2S的安全无人载具指挥控制系统，采用零信任模型和基于证书的分层权限机制，通过mTLS、ECDSA、ECDH和HMAC保障通信的机密性、完整性和认证性，并在Wi-Fi与战术无线电上验证了其可行性。


<details>
  <summary>Details</summary>
Motivation: 当前军用无人载具广泛使用如MAVLink等缺乏认证与加密机制的不安全协议，存在安全风险；同时需支持地面控制站之间的实时控制权移交，因此亟需一种兼顾安全性与实时性的新型指挥控制架构。

Method: 设计并实现了NC2S系统，采用零信任架构，结合分层凭证权限管理，利用mTLS配合ECDSA证书和ECDH密钥交换保障通信安全，并通过HMAC确保消息完整性；同时开发了轻量级协议用于凭证管理、密钥更新和控制权交接，并在Wi-Fi与HR-5000H战术无线电上进行原型测试。

Result: 实验表明，HR-5000H战术无线电链路的延迟比宽带技术（如Wi-Fi或5G）高约两个数量级，但仍能维持稳定通信且消息丢失极少，适用于TC终端与GCS之间的NC2S链路。

Conclusion: 所提出的NC2S架构有效解决了现有无人载具指挥控制系统中的安全缺陷，在保障CIA三要素的同时支持实时控制权移交，且在战术通信环境下具备实用可行性。

Abstract: Unmanned Vehicles (UxVs) are increasingly used in modern military operations for reconnaissance, surveillance, and strike missions, enhancing situational awareness while reducing risk to personnel. Their affordability and rapid deployment have encouraged the adoption of commercial solutions. However, many rely on insecure protocols such as MAVLink, which lack authentication and encryption mechanisms. This paper designed, implemented, and evaluated a new secure command-and-control architecture that ensures confidentiality, integrity, and authentication (CIA) while supporting real-time control delegation between Ground Control Stations (GCSs). The proposed solution, named New Command and Control System (NC2S), enforces a zero-trust model integrating hierarchical credential-based privileges to regulate access and control among Tactical Commanders (TC), GCSs, and UxVs. It employs mutual Transport Layer Security (mTLS) with Elliptic Curve Digital Signature Algorithm (ECDSA) certificates and Elliptic Curve Diffie-Hellman (ECDH) key exchange, while message integrity is ensured through Hash-based Message Authentication Codes (HMAC). Multiple lightweight protocols were developed for credential management, key renewal, and control handover. The NC2S prototype was experimentally validated over Wi-Fi and Rohde&Schwarz HR-5000H tactical radios. Results showed that HR-5000H links introduce latencies roughly two orders of magnitude higher than broadband technologies (e.g., Wi-Fi or 5G&Beyond technologies) but are still able to maintain stable communication with minimal message loss, making them suitable for the NC2S links among TC terminals and GCSs.

</details>


### [39] [Semantic-Aware Caching for Efficient Image Generation in Edge Computing](https://arxiv.org/abs/2511.22421)
*Hanshuai Cui,Zhiqing Tang,Zhi Yao,Weijia Ji,Wei Zhao*

Main category: cs.NI

TL;DR: CacheGenius 是一种面向边缘计算的混合图像生成系统，通过结合文本到图像与图像到图像流程，并利用语义对齐的缓存参考图像，显著减少扩散模型在移动端的生成延迟和计算开销。


<details>
  <summary>Details</summary>
Motivation: 扩散模型虽能生成高质量图像，但其多步去噪过程在资源受限的移动和边缘设备上效率低下，亟需加速方案。

Method: 提出 CacheGenius 系统，采用语义感知的分类存储策略、请求调度算法以确保参考图像与目标语义对齐，并引入基于相关性分析的缓存维护策略主动淘汰过时条目。

Result: 在分布式边缘计算环境中，相比基线方法，CacheGenius 将生成延迟降低 41%，计算成本减少 48%，同时保持有竞争力的评估指标。

Conclusion: CacheGenius 有效提升了扩散模型在边缘环境中的推理效率，为资源受限场景下的高效文本到图像生成提供了可行方案。

Abstract: Text-to-image generation employing diffusion models has attained significant popularity due to its capability to produce high-quality images that adhere to textual prompts. However, the integration of diffusion models faces critical challenges into resource-constrained mobile and edge environments because it requires multiple denoising steps from the original random noise. A practical way to speed up denoising is to initialize the process with a noised reference image that is similar to the target, since both images share similar layouts, structures, and details, allowing for fewer denoising steps. Based on this idea, we present CacheGenius, a hybrid image generation system in edge computing that accelerates generation by combining text-toimage and image-to-image workflows. It generates images from user text prompts using cached reference images. CacheGenius introduces a semantic-aware classified storage scheme and a request-scheduling algorithm that ensures semantic alignment between references and targets. To ensure sustained performance, it employs a cache maintenance policy that proactively evicts obsolete entries via correlation analysis. Evaluated in a distributed edge computing system, CacheGenius reduces generation latency by 41% and computational costs by 48% relative to baselines, while maintaining competitive evaluation metrics.

</details>


### [40] [RetryGuard: Preventing Self-Inflicted Retry Storms in Cloud Microservices Applications](https://arxiv.org/abs/2511.23278)
*Jhonatan Tavori,Anat Bremler-Barr,Hanoch Levy,Ofek Lavi*

Main category: cs.NI

TL;DR: RetryGuard 是一种分布式框架，用于控制微服务间的重试行为，防止重试风暴、资源争用和成本激增，在 AWS 和 Kubernetes+Istio 环境中均显著降低资源使用与运营成本。


<details>
  <summary>Details</summary>
Motivation: 现代云应用依赖多样化的微服务和自动扩缩容机制，但服务间默认的重试策略在流量波动下易引发重试风暴，导致资源浪费和“钱包拒绝服务”（Denial-of-Wallet）问题。

Method: 提出 RetryGuard 框架，基于分析模型对每个服务独立管理重试策略，通过并行决策协调微服务间的重试行为，综合考虑重试、吞吐量、延迟与成本之间的关系。

Result: 实验表明，RetryGuard 相比 AWS 标准和高级重试策略大幅降低资源使用和成本，并在 Kubernetes 与 Istio 服务网格的复杂部署中展现出良好的可扩展性与性能优势。

Conclusion: RetryGuard 能有效缓解微服务架构中由重试风暴引发的成本与性能问题，为云原生系统提供了一种实用且高效的重试控制方案。

Abstract: Modern cloud applications are built on independent, diverse microservices, offering scalability, flexibility, and usage-based billing. However, the structural design of these varied services, along with their reliance on auto-scalers for dynamic internet traffic, introduces significant coordination challenges. As we demonstrate in this paper, common default retry patterns used between misaligned services can turn into retry storms which drive up resource usage and costs, leading to self-inflicted Denial-of-Wallet (DoW) scenarios. To overcome these problems we introduce RetryGuard, a distributed framework for productive control of retry patterns across interdependent microservices. By managing retry policy on a per-service basis and making parallel decisions, RetryGuard prevents retry storms, curbs resource contention, and mitigates escalating operational costs. RetryGuard makes its decisions based on an analytic model that captures the relationships among retries, throughput (rejections), delays, and costs. Experimental results show that RetryGuard significantly reduces resource usage and costs compared to AWS standard and advanced retry policies. We further demonstrate its scalability and superior performance in a more complex Kubernetes deployment with the Istio service mesh, where it achieves substantial improvements.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [41] [AgentShield: Make MAS more secure and efficient](https://arxiv.org/abs/2511.22924)
*Kaixiang Wang,Zhaojiacheng Zhou,Bunyod Suvonov,Jiong Lou,Jie LI*

Main category: cs.MA

TL;DR: AgentShield 是一种用于多智能体系统的高效去中心化审计框架，通过三层防御机制在保障系统鲁棒性的同时显著降低审计开销。


<details>
  <summary>Details</summary>
Motivation: 现有针对基于大语言模型的多智能体系统的防御方法存在单点故障或效率低下问题，亟需一种兼顾鲁棒性与效率的去中心化审计方案。

Method: 提出 AgentShield 框架，包含三层防御机制：(i) 基于拓扑分析的关键节点审计；(ii) 使用轻量哨兵模型的级联式轻量令牌审计；(iii) 在不确定性时触发重型仲裁器的两轮共识审计。

Result: 实验表明，AgentShield 达到 92.5% 的恢复率，审计开销比现有方法降低 70% 以上，并在多种多智能体拓扑结构和对抗场景下保持高协作准确率。

Conclusion: AgentShield 有效平衡了多智能体系统中鲁棒性与效率之间的矛盾，为抵御对抗攻击提供了实用且可扩展的解决方案。

Abstract: Large Language Model (LLM)-based Multi-Agent Systems (MAS) offer powerful cooperative reasoning but remain vulnerable to adversarial attacks, where compromised agents can undermine the system's overall performance. Existing defenses either depend on single trusted auditors, creating single points of failure, or sacrifice efficiency for robustness. To resolve this tension, we propose \textbf{AgentShield}, a distributed framework for efficient, decentralized auditing. AgentShield introduces a novel three-layer defense: \textbf{(i) Critical Node Auditing} prioritizes high-influence agents via topological analysis; \textbf{(ii) Light Token Auditing} implements a cascade protocol using lightweight sentry models for rapid discriminative verification; and \textbf{(iii) Two-Round Consensus Auditing} triggers heavyweight arbiters only upon uncertainty to ensure global agreement. This principled design optimizes the robustness-efficiency trade-off. Experiments demonstrate that AgentShield achieves a 92.5\% recovery rate and reduces auditing overhead by over 70\% compared to existing methods, maintaining high collaborative accuracy across diverse MAS topologies and adversarial scenarios.

</details>
