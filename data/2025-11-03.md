<div id=toc></div>

# Table of Contents

- [cs.NI](#cs.NI) [Total: 10]
- [cs.DC](#cs.DC) [Total: 7]
- [cs.SE](#cs.SE) [Total: 9]
- [cs.AR](#cs.AR) [Total: 3]
- [cs.PF](#cs.PF) [Total: 1]
- [cs.MA](#cs.MA) [Total: 1]


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [1] [Trace-driven Path Emulation of Satellite Networks using Hypatia](https://arxiv.org/abs/2510.27027)
*Martin Ottens,Kai-Steffen Hielscher,Reinhard German*

Main category: cs.NI

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: The increasing prevalence LEO satellite mega-constellations for global
Internet coverage requires new approaches to evaluate the behavior of existing
Internet protocols and applications. Traditional discrete event simulators like
Hypatia allow for modeling these environments but fall short in evaluating real
applications. This paper builds upon our previous work, in which we proposed a
system design for trace-driven emulation of such satellite networks, bridging
the gab between simulations and real-time testbeds. By extending the Hypatia
framework, we record network path characteristics, e.g., delay and bandwidth,
between two endpoints in the network during non-real-time simulations. Path
characteristics are exported to Trace Files, which are replayed in real-time
emulation environments on real systems, enabling evaluations with real software
and human interaction. An advantage of our approach is its easy adaptability to
existing simulation models. Our extensive evaluation involves multiple
scenarios with different satellite constellations, illustrating the approach's
accuracy in reproducing the behavior of satellite networks. Between full
simulation, which serves as a baseline for our evaluation, and emulation runs,
we observe high correlation metrics of up to 0.96, validating the approach's
effectiveness. Challenges such as the lack of emulation-to-simulation feedback
and synchronization issues are discussed.

</details>


### [2] [TheaterQ: A Qdisc for Dynamic Network Emulation](https://arxiv.org/abs/2510.27057)
*Martin Ottens,Kai-Steffen Hielscher,Reinhard German*

Main category: cs.NI

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: TheaterQ is a Linux qdisc designed for dynamic network emulation, addressing
the limitations of static parameters in traditional tools like NetEm. By
utilizing Trace Files containing timelines with network characteristics,
TheaterQ achieves high-accuracy emulation of dynamic networks without involving
the userspace and allows for resolutions of characteristic updates of up to 1
microsecond. Features include synchronization across mutliple qdisc instances
and handling of delays, bandwidth, packet loss, duplication, and reordering.
Evaluations show TheaterQ's accuracy and its comparable performance to existing
tools, offering a flexible solution for modern communication protocol
development. TheaterQ is available as open-source software under the GPLv2
license.

</details>


### [3] [Analytical Model of NR-V2X Mode 2 with Re-Evaluation Mechanism](https://arxiv.org/abs/2510.27108)
*Shuo Zhu,Siyu Lin*

Main category: cs.NI

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Massive message transmissions, unpredictable aperiodic messages, and
high-speed moving vehicles contribute to the complex wireless environment,
resulting in inefficient resource collisions in Vehicle to Everything (V2X). In
order to achieve better medium access control (MAC) layer performance, 3GPP
introduced several new features in NR-V2X. One of the most important is the
re-evaluation mechanism. It allows the vehicle to continuously sense resources
before message transmission to avoid resource collisions. So far, only a few
articles have studied the re-evaluation mechanism of NR-V2X, and they mainly
focus on network simulator that do not consider variable traffic, which makes
analysis and comparison difficult. In this paper, an analytical model of NR-V2X
Mode 2 is established, and a message generator is constructed by using discrete
time Markov chain (DTMC) to simulate the traffic pattern recommended by 3GPP
advanced V2X services. Our study shows that the re-evaluation mechanism
improves the reliability of NR-V2X transmission, but there are still local
improvements needed to reduce latency.

</details>


### [4] [Stochastic Geometry of Cylinders: Characterizing Inter-Nodal Distances for 3D UAV Networks](https://arxiv.org/abs/2510.27111)
*Yunfeng Jiang,Zhiming Huang,Jianping Pan*

Main category: cs.NI

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: The analytical characterization of coverage probability in finite
three-dimensional wireless networks has long remained an open problem, hindered
by the loss of spatial independence in finite-node settings and the coupling
between link distances and interference in bounded geometries. This paper
closes this gap by presenting the first exact analytical framework for coverage
probability in finite 3D networks modeled by a binomial point process within a
cylindrical region. To bypass the intractability that has long hindered such
analyses, we leverage the independence structure, convolution geometry, and
derivative properties of Laplace transforms, yielding a formulation that is
both mathematically exact and computationally efficient. Extensive Monte Carlo
simulations verify the analysis and demonstrate significant accuracy gains over
conventional Poisson-based models. The results generalize to any confined 3D
wireless system, including aerial, underwater, and robotic networks.

</details>


### [5] [Study of Cluster-Based Routing Based on Machine Learning for UAV Networks in 6G](https://arxiv.org/abs/2510.27121)
*Luis Antonio L. F. da Costa,Rodrigo C. de Lamare,Rafael Kunst,Edison Pignaton de Freitas*

Main category: cs.NI

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: The sixth generation (6G) wireless networks are envisioned to deliver
ultra-low latency, massive connectivity, and high data rates, enabling advanced
applications such as autonomous {unmaned aerial vehicles (UAV)} swarms and
aerial edge computing. However, realizing this vision in Flying Ad Hoc Networks
(FANETs) requires intelligent and adaptive clustering mechanisms to ensure
efficient routing and resource utilization. This paper proposes a novel machine
learning-driven framework for dynamic cluster formation and cluster head
selection in 6G-enabled FANETs. The system leverages mobility prediction using
{Extreme Gradient Boosting (XGBoost)} and a composite optimization strategy
based on signal strength and spatial proximity to identify optimal cluster
heads. To evaluate the proposed method, comprehensive simulations were
conducted in both centralized (5G) and decentralized (6G) topologies using
realistic video traffic patterns. Results show that the proposed model achieves
significant improvements in delay, jitter, and throughput in decentralized
scenarios. These findings demonstrate the potential of combining machine
learning with clustering techniques to enhance scalability, stability, and
performance in next-generation aerial networks.

</details>


### [6] [Effective Delayed Patching for Transient Malware Control on Networks](https://arxiv.org/abs/2510.27137)
*Minh Phu Vuong,Chul-Ho Lee,Do Young Eun*

Main category: cs.NI

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Patching nodes is an effective network defense strategy for malware control
at early stages, and its performance is primarily dependent on how accurately
the infection propagation is characterized. In this paper, we aim to design a
novel patching policy based on the susceptible-infected epidemic network model
by incorporating the influence of patching delay--the type of delay that has
been largely overlooked in designing patching policies in the literature, while
being prevalent in practice. We first identify 'critical edges' that form a
boundary to separate the most likely infected nodes from the nodes which would
still remain healthy after the patching delay. We next leverage the critical
edges to determine which nodes to be patched in light of limited patching
resources at early stages. To this end, we formulate a constrained graph
partitioning problem and use its solution to identify a set of nodes to patch
or vaccinate under the limited resources, to effectively prevent malware
propagation from getting through the healthy region. We numerically validate
that our patching policy significantly outperforms other baseline policies in
protecting the healthy nodes under limited patching resources and in the
presence of patching delay.

</details>


### [7] [Selected Results from the REDMARS2 Project: Recursive Delay-Tolerant Networking using Bundle-in-Bundle Encapsulation](https://arxiv.org/abs/2510.27325)
*Marius Feldmann,Tobias Nöthlich,Felix Walter,Maximilian Nitsch,Juan A. Fraire,Georg A. Murzik,Fiona Fuchs*

Main category: cs.NI

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: This whitepaper presents parts of the results of the REDMARS2 project
conducted in 2021-2022, exploring the integration of Recursive Internetwork
Architecture (RINA) concepts into Delay- and Disruption-Tolerant Networking
(DTN) protocols. Using Bundle-in-Bundle Encapsulation (BIBE), we implemented
scope-based separation mechanisms resulting in scalable DTNs. A key
contribution of this work is the demonstration of practical BIBE-based use
cases, including a realistic Solar System Internet communication scenario
involving unmanned aerial vehicles (UAVs) and satellite relays. The evaluation,
supported by field tests in collaboration with the European Space Agency (ESA),
confirmed the viability of BIBE as a foundation for scalable, recursive, and
interoperable DTN architectures.

</details>


### [8] [Challenging Tribal Knowledge -- Large Scale Measurement Campaign on Decentralized NAT Traversal](https://arxiv.org/abs/2510.27500)
*Dennis Trautwein,Cornelius Ihle,Moritz Schubotz,Bela Gipp*

Main category: cs.NI

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: The promise of decentralized peer-to-peer (P2P) systems is fundamentally
gated by the challenge of Network Address Translation (NAT) traversal, with
existing solutions often reintroducing the very centralization they seek to
avoid. This paper presents the first large-scale, longitudinal measurement
study of a fully decentralized NAT traversal protocol, Direct Connection
Upgrade through Relay (DCUtR), within the production libp2p-based IPFS network.
Drawing on over 4.4 million traversal attempts from 85,000+ distinct networks
across 167 countries, we provide a definitive empirical analysis of modern P2P
connectivity. We establish a contemporary baseline success rate of $70\% \pm
7.1\%$ for the hole-punching stage, providing a crucial new benchmark for the
field. Critically, we empirically refute the long-held 'tribal knowledge' of
UDP's superiority for NAT traversal, demonstrating that DCUtR's high-precision,
RTT-based synchronization yields statistically indistinguishable success rates
for both TCP and QUIC ($\sim70\%$). Our analysis further validates the
protocol's design for permissionless environments by showing that success is
independent of relay characteristics and that the mechanism is highly
efficient, with $97.6\%$ of successful connections established on the first
attempt. Building on this analysis, we propose a concrete roadmap of protocol
enhancements aimed at achieving universal connectivity and contribute our
complete dataset to foster further research in this domain.

</details>


### [9] [Asynchronous Risk-Aware Multi-Agent Packet Routing for Ultra-Dense LEO Satellite Networks](https://arxiv.org/abs/2510.27506)
*Ke He,Thang X. Vu,Le He,Lisheng Fan,Symeon Chatzinotas,Bjorn Ottersten*

Main category: cs.NI

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: The rise of ultra-dense LEO constellations creates a complex and asynchronous
network environment, driven by their massive scale, dynamic topologies, and
significant delays. This unique complexity demands an adaptive packet routing
algorithm that is asynchronous, risk-aware, and capable of balancing diverse
and often conflicting QoS objectives in a decentralized manner. However,
existing methods fail to address this need, as they typically rely on
impractical synchronous decision-making and/or risk-oblivious approaches. To
tackle this gap, we introduce PRIMAL, an event-driven multi-agent routing
framework designed specifically to allow each satellite to act independently on
its own event-driven timeline, while managing the risk of worst-case
performance degradation via a principled primal-dual approach. This is achieved
by enabling agents to learn the full cost distribution of the targeted QoS
objectives and constrain tail-end risks. Extensive simulations on a LEO
constellation with 1584 satellites validate its superiority in effectively
optimizing latency and balancing load. Compared to a recent risk-oblivious
baseline, it reduces queuing delay by over 70%, and achieves a nearly 12 ms
end-to-end delay reduction in loaded scenarios. This is accomplished by
resolving the core conflict between naive shortest-path finding and congestion
avoidance, highlighting such autonomous risk-awareness as a key to robust
routing.

</details>


### [10] [Rethinking Telemetry Design for Fine-Grained Anomaly Detection in 5G User Planes](https://arxiv.org/abs/2510.27664)
*Niloy Saha,Noura Limam,Yang Xiao,Raouf Boutaba*

Main category: cs.NI

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Detecting QoS anomalies in 5G user planes requires fine-grained per-flow
visibility, but existing telemetry approaches face a fundamental trade-off.
Coarse per-class counters are lightweight but mask transient and per-flow
anomalies, while per-packet telemetry postcards provide full visibility at
prohibitive cost that grows linearly with line rate. Selective postcard schemes
reduce overhead but miss anomalies that fall below configured thresholds or
occur during brief intervals. We present Kestrel, a sketch-based telemetry
system for 5G user planes that provides fine-grained visibility into key metric
distributions such as latency tails and inter-arrival times at a fraction of
the cost of per-packet postcards. Kestrel extends Count-Min Sketch with
histogram-augmented buckets and per-queue partitioning, which compress
per-packet measurements into compact summaries while preserving
anomaly-relevant signals. We develop formal detectability guarantees that
account for sketch collisions, yielding principled sizing rules and binning
strategies that maximize anomaly separability. Our evaluations on a 5G testbed
with Intel Tofino switches show that Kestrel achieves 10% better detection
accuracy than existing selective postcard schemes while reducing export
bandwidth by 10x.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [11] [FlowMesh: A Service Fabric for Composable LLM Workflows](https://arxiv.org/abs/2510.26913)
*Junyi Shen,Noppanat Wadlom,Lingfeng Zhou,Dequan Wang,Xu Miao,Lei Fang,Yao Lu*

Main category: cs.DC

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: AI deployment increasingly resembles a pipeline of data transformation,
fine-tuning, and agent interactions rather than a monolithic LLM job; recent
examples include RLHF/RLAIF training and agentic workflows. To cope with this
shift, we propose FlowMesh, a multi-tenant service fabric that executes and
optimizes these workloads as one shared service instead of isolated pipelines.
It decomposes workflows into fine-grained operators with recorded lineage,
enabling de-duplication of work across users and batching requests on the same
hardware while preserving per-workflow provenance. A global control plane
maintains a cluster-wide pool of ready operators and uses a single utility
function to pick both the batch and the worker, balancing throughput, cost, and
data locality on heterogeneous GPUs. The data plane is an elastic fleet of
stateless workers backed by a content-addressable store, enabling rapid,
automatic scale-out, safe retry after preemption, and portability across
managed clusters such as Kubernetes and geo-distributed GPU marketplaces such
as Vast.ai. Compared with baseline solutions, FlowMesh achieves up to 3.8x cost
reduction and 2.0x lower energy usage, provides a similar or better latency
profile, and remains efficient under dynamic and failure-prone conditions.

</details>


### [12] [A Cloud-Based Spatio-Temporal GNN-Transformer Hybrid Model for Traffic Flow Forecasting with External Feature Integration](https://arxiv.org/abs/2510.27039)
*Zhuo Zheng,Lingran Meng,Ziyu Lin*

Main category: cs.DC

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Accurate traffic flow forecasting is essential for the development of
intelligent transportation systems (ITS), supporting tasks such as traffic
signal optimization, congestion management, and route planning. Traditional
models often fail to effectively capture complex spatial-temporal dependencies
in large-scale road networks, especially under the influence of external
factors such as weather, holidays, and traffic accidents. To address this
challenge, this paper proposes a cloud-based hybrid model that integrates
Spatio-Temporal Graph Neural Networks (ST-GNN) with a Transformer architecture
for traffic flow prediction. The model leverages the strengths of GNNs in
modeling spatial correlations across road networks and the Transformers'
ability to capture long-term temporal dependencies. External contextual
features are incorporated via feature fusion to enhance predictive accuracy.
The proposed model is deployed on a cloud computing platform to achieve
scalability and real-time adaptability. Experimental evaluation of the dataset
shows that our model outperforms baseline methods (LSTM, TCN, GCN, pure
Transformer) with an RMSE of only 17.92 and a MAE of only 10.53. These findings
suggest that the hybrid GNN-Transformer approach provides an effective and
scalable solution for cloud-based ITS applications, offering methodological
advancements for traffic flow forecasting and practical implications for
congestion mitigation.

</details>


### [13] [Synergistic Tensor and Pipeline Parallelism](https://arxiv.org/abs/2510.27257)
*Mengshi Qi,Jiaxuan Peng,Jie Zhang,Juan Zhu,Yong Li,Huadong Ma*

Main category: cs.DC

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: In the machine learning system, the hybrid model parallelism combining tensor
parallelism (TP) and pipeline parallelism (PP) has become the dominant solution
for distributed training of Large Language Models~(LLMs) and Multimodal LLMs
(MLLMs). However, TP introduces significant collective communication overheads,
while PP suffers from synchronization inefficiencies such as pipeline bubbles.
Existing works primarily address these challenges from isolated perspectives,
focusing either on overlapping TP communication or on flexible PP scheduling to
mitigate pipeline bubbles. In this paper, we propose a new synergistic tensor
and pipeline parallelism schedule that simultaneously reduces both types of
bubbles. Our proposed schedule decouples the forward and backward passes in PP
into fine-grained computation units, which are then braided to form a composite
computation sequence. This compositional structure enables near-complete
elimination of TP-related bubbles. Building upon this structure, we further
design the PP schedule to minimize PP bubbles. Experimental results demonstrate
that our approach improves training throughput by up to 12% for LLMs and 16%
for MLLMs compared to existing scheduling methods. Our source code is avaiable
at https://github.com/MICLAB-BUPT/STP.

</details>


### [14] [A Digital Twin-based Multi-Agent Reinforcement Learning Framework for Vehicle-to-Grid Coordination](https://arxiv.org/abs/2510.27289)
*Zhengchang Hua,Panagiotis Oikonomou,Karim Djemame,Nikos Tziritas,Georgios Theodoropoulos*

Main category: cs.DC

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: The coordination of large-scale, decentralised systems, such as a fleet of
Electric Vehicles (EVs) in a Vehicle-to-Grid (V2G) network, presents a
significant challenge for modern control systems. While collaborative Digital
Twins have been proposed as a solution to manage such systems without
compromising the privacy of individual agents, deriving globally optimal
control policies from the high-level information they share remains an open
problem. This paper introduces Digital Twin Assisted Multi-Agent Deep
Deterministic Policy Gradient (DT-MADDPG) algorithm, a novel hybrid
architecture that integrates a multi-agent reinforcement learning framework
with a collaborative DT network. Our core contribution is a simulation-assisted
learning algorithm where the centralised critic is enhanced by a predictive
global model that is collaboratively built from the privacy-preserving data
shared by individual DTs. This approach removes the need for collecting
sensitive raw data at a centralised entity, a requirement of traditional
multi-agent learning algorithms. Experimental results in a simulated V2G
environment demonstrate that DT-MADDPG can achieve coordination performance
comparable to the standard MADDPG algorithm while offering significant
advantages in terms of data privacy and architectural decentralisation. This
work presents a practical and robust framework for deploying intelligent,
learning-based coordination in complex, real-world cyber-physical systems.

</details>


### [15] [Dynamic Service Scheduling and Resource Management in Energy-Harvesting Multi-access Edge Computing](https://arxiv.org/abs/2510.27317)
*Shuyi Chen,Panagiotis Oikonomou,Zhengchang Hua,Nikos Tziritas,Karim Djemame,Nan Zhang,Georgios Theodoropoulos*

Main category: cs.DC

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Multi-access Edge Computing (MEC) delivers low-latency services by hosting
applications near end-users. To promote sustainability, these systems are
increasingly integrated with renewable Energy Harvesting (EH) technologies,
enabling operation where grid electricity is unavailable. However, balancing
the intermittent nature of harvested energy with dynamic user demand presents a
significant resource allocation challenge. This work proposes an online
strategy for an MEC system powered exclusively by EH to address this trade-off.
Our strategy dynamically schedules computational tasks with dependencies and
governs energy consumption through real-time decisions on server frequency
scaling and service module migration. Experiments using real-world datasets
demonstrate our algorithm's effectiveness in efficiently utilizing harvested
energy while maintaining low service latency.

</details>


### [16] [ML-Based Optimum Sub-system Size Heuristic for the GPU Implementation of the Tridiagonal Partition Method](https://arxiv.org/abs/2510.27351)
*Milena Veneva*

Main category: cs.DC

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: This paper presents a machine learning (ML)-based heuristic for finding the
optimum sub-system size for the CUDA implementation of the parallel partition
algorithm. Computational experiments for different system of linear algebraic
equation (SLAE) sizes are conducted, and the optimum sub-system size for each
of them is found empirically. To estimate a model for the sub-system size, we
perform the k-nearest neighbors (kNN) classification method. Statistical
analysis of the results is done. By comparing the predicted values with the
actual data, the algorithm is deemed to be acceptably good. Next, the heuristic
is expanded to work for the recursive parallel partition algorithm as well. An
algorithm for determining the optimum sub-system size for each recursive step
is formulated. A kNN model for predicting the optimum number of recursive steps
for a particular SLAE size is built.

</details>


### [17] [RDMA Point-to-Point Communication for LLM Systems](https://arxiv.org/abs/2510.27656)
*Nandor Licker,Kevin Hu,Vladimir Zaytsev,Lequn Chen*

Main category: cs.DC

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Emerging Large Language Model (LLM) system patterns, such as disaggregated
inference, Mixture-of-Experts (MoE) routing, and asynchronous reinforcement
fine-tuning, require flexible point-to-point communication beyond simple
collectives. Existing implementations are locked to specific Network Interface
Controllers (NICs), hindering integration into inference engines and
portability across hardware providers. We present TransferEngine, which bridges
the functionality of common NICs to expose a uniform interface. TransferEngine
exposes one-sided WriteImm operations with a ImmCounter primitive for
completion notification, without ordering assumptions of network transport,
transparently managing multiple NICs per GPU. We demonstrate peak throughput of
400 Gbps on both NVIDIA ConnectX-7 and AWS Elastic Fabric Adapter (EFA). We
showcase TransferEngine through three production systems: (1) KvCache transfer
for disaggregated inference with dynamic scaling, (2) RL weight updates
achieving 1.3 seconds for trillion-parameter models, and (3) MoE
dispatch/combine implementation exceeding DeepEP decode latency on ConnectX-7,
with the first viable latencies on EFA. We demonstrate that our portable
point-to-point communication complements collectives while avoiding lock-in.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [18] [Empirical Studies on Quantum Optimization for Software Engineering: A Systematic Analysis](https://arxiv.org/abs/2510.27113)
*Man Zhang,Yuechen Li,Tao Yue,Kai-Yuan Cai*

Main category: cs.SE

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: In recent years, quantum, quantum-inspired, and hybrid algorithms are
increasingly showing promise for solving software engineering optimization
problems. However, best-intended practices for conducting empirical studies
have not yet well established. In this paper, based on the primary studies
identified from the latest systematic literature review on quantum optimization
for software engineering problems, we conducted a systematic analysis on these
studies from various aspects including experimental designs, hyperparameter
settings, case studies, baselines, tooling, and metrics. We identify key gaps
in the current practices such as limited reporting of the number of
repetitions, number of shots, and inadequate consideration of noise handling,
as well as a lack of standardized evaluation protocols such as the adoption of
quality metrics, especially quantum-specific metrics. Based on our analysis, we
provide insights for designing empirical studies and highlight the need for
more real-world and open case studies to assess cost-effectiveness and
practical utility of the three types of approaches: quantum-inspired, quantum,
and hybrid. This study is intended to offer an overview of current practices
and serve as an initial reference for designing and conducting empirical
studies on evaluating and comparing quantum, quantum-inspired, and hybrid
algorithms in solving optimization problems in software engineering.

</details>


### [19] [MARIA: A Framework for Marginal Risk Assessment without Ground Truth in AI Systems](https://arxiv.org/abs/2510.27163)
*Jieshan Chen,Suyu Ma,Qinghua Lu,Sung Une Lee,Liming Zhu*

Main category: cs.SE

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Before deploying an AI system to replace an existing process, it must be
compared with the incumbent to ensure improvement without added risk.
Traditional evaluation relies on ground truth for both systems, but this is
often unavailable due to delayed or unknowable outcomes, high costs, or
incomplete data, especially for long-standing systems deemed safe by
convention. The more practical solution is not to compute absolute risk but the
difference between systems. We therefore propose a marginal risk assessment
framework, that avoids dependence on ground truth or absolute risk. It
emphasizes three kinds of relative evaluation methodology, including
predictability, capability and interaction dominance. By shifting focus from
absolute to relative evaluation, our approach equips software teams with
actionable guidance: identifying where AI enhances outcomes, where it
introduces new risks, and how to adopt such systems responsibly.

</details>


### [20] [On the Marriage of Theory and Practice in Data-Aware Business Processes via Low-Code](https://arxiv.org/abs/2510.27229)
*Ali Nour Eldin,Benjamin Dalmas,Walid Gaaloul*

Main category: cs.SE

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: In recent years, there has been a growing interest in the verification of
business process models. Despite their lack of formal characterization, these
models are widely adopted in both industry and academia. To address this issue,
formalizing the execution semantics of business process modeling languages is
essential. Since data and process are two facets of the same coin, and data are
critical elements in the execution of process models, this work introduces
Proving an eXecutable BPMN injected with data, BPMN-ProX. BPMN-ProX is a
low-code testing framework that significantly enhances the verification of
data-aware BPMN. This low-code platform helps bridge the gap between
non-technical experts and professionals by proposing a tool that integrates
advanced data handling and employs a robust verification mechanism through
state-of-the-art model checkers. This innovative approach combines theoretical
verification with practical modeling, fostering more agile, reliable, and
user-centric business process management.

</details>


### [21] [Vintage Code, Modern Judges: Meta-Validation in Low Data Regimes](https://arxiv.org/abs/2510.27244)
*Ora Nova Fandina,Gal Amram,Eitan Farchi,Shmulik Froimovich,Raviv Gal,Wesam Ibraheem,Rami Katan,Alice Podolsky,Orna Raz*

Main category: cs.SE

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Application modernization in legacy languages such as COBOL, PL/I, and REXX
faces an acute shortage of resources, both in expert availability and in
high-quality human evaluation data. While Large Language Models as a Judge
(LaaJ) offer a scalable alternative to expert review, their reliability must be
validated before being trusted in high-stakes workflows. Without principled
validation, organizations risk a circular evaluation loop, where unverified
LaaJs are used to assess model outputs, potentially reinforcing unreliable
judgments and compromising downstream deployment decisions. Although various
automated approaches to validating LaaJs have been proposed, alignment with
human judgment remains a widely used and conceptually grounded validation
strategy. In many real-world domains, the availability of human-labeled
evaluation data is severely limited, making it difficult to assess how well a
LaaJ aligns with human judgment. We introduce SparseAlign, a formal framework
for assessing LaaJ alignment with sparse human-labeled data. SparseAlign
combines a novel pairwise-confidence concept with a score-sensitive alignment
metric that jointly capture ranking consistency and score proximity, enabling
reliable evaluator selection even when traditional statistical methods are
ineffective due to limited annotated examples. SparseAlign was applied
internally to select LaaJs for COBOL code explanation. The top-aligned
evaluators were integrated into assessment workflows, guiding model release
decisions. We present a case study of four LaaJs to demonstrate SparseAlign's
utility in real-world evaluation scenarios.

</details>


### [22] [Efficient Integration of cross platform functions onto service-oriented architectures](https://arxiv.org/abs/2510.27344)
*Thomas Schulik,Viswanatha Reddy Batchu,Ramesh Kumar Dharmapuri,Saran Gundlapalli,Parthasarathy Nadarajan,Philipp Pelcz*

Main category: cs.SE

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: The automotive industry is currently undergoing a major transformation with
respect to the Electric/Electronic (E/E) and software architecture, driven by a
significant increase in the complexity of the technological stack within a
vehicle. This complexity acts as a driving force for Software-Defined Vehicles
(SDVs) leading to the evolution of the automotive E/E architectures from
decentralized configuration comprising multiple Electronic Control Units (ECUs)
towards a more integrated configuration comprising a smaller number of ECUs,
domain controllers, gateways, and High-Performance Computers (HPCs) [2]. This
transition along with several other reasons have resulted in heterogeneous
software platforms such as AUTOSAR Classic, AUTOSAR Adaptive, and prototypical
frameworks like ROS 2. It is therefore essential to develop applications that
are both hardware- and platform/middleware-agnostic to attain development and
integration efficiency. This work presents an application development and
integration concept to facilitate developing applications as Software as a
Product (SaaP), while simultaneously ensuring efficient integration onto
multiple software architecture platforms. The concept involves designing
applications in a hardware- and software platform-agnostic manner and
standardizing application interfaces [6]. It also includes describing the
relevant aspects of the application and corresponding middleware in a
machine-readable format to aid the integration of developed applications.
Additionally, tools are developed to facilitate semi-automation of the
development and integration processes. An example application has been
developed and integrated onto AUTOSAR Adaptive and ROS 2, demonstrating the
applicability of the approach. Finally, metrics are presented to show the
efficiency of the overall concept.

</details>


### [23] [Agentic LLMs for REST API Test Amplification: A Comparative Study Across Cloud Applications](https://arxiv.org/abs/2510.27417)
*Jarne Besjes,Robbe Nooyens,Tolgahan Bardakci,Mutlu Beyazit,Serge Demeyer*

Main category: cs.SE

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Representational State Transfer (REST) APIs are a cornerstone of modern cloud
native systems. Ensuring their reliability demands automated test suites that
exercise diverse and boundary level behaviors. Nevertheless, designing such
test cases remains a challenging and resource intensive endeavor. This study
extends prior work on Large Language Model (LLM) based test amplification by
evaluating single agent and multi agent configurations across four additional
cloud applications. The amplified test suites maintain semantic validity with
minimal human intervention. The results demonstrate that agentic LLM systems
can effectively generalize across heterogeneous API architectures, increasing
endpoint and parameter coverage while revealing defects. Moreover, a detailed
analysis of computational cost, runtime, and energy consumption highlights
trade-offs between accuracy, scalability, and efficiency. These findings
underscore the potential of LLM driven test amplification to advance the
automation and sustainability of REST API testing in complex cloud
environments.

</details>


### [24] [CodeAlignBench: Assessing Code Generation Models on Developer-Preferred Code Adjustments](https://arxiv.org/abs/2510.27565)
*Forough Mehralian,Ryan Shar,James R. Rae,Alireza Hashemi*

Main category: cs.SE

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: As large language models become increasingly capable of generating code,
evaluating their performance remains a complex and evolving challenge. Existing
benchmarks primarily focus on functional correctness, overlooking the diversity
of real-world coding tasks and developer expectations. To this end, we
introduce a multi-language benchmark that evaluates LLM instruction-following
capabilities and is extensible to operate on any set of standalone coding
problems. Our benchmark evaluates instruction following in two key settings:
adherence to pre-defined constraints specified with the initial problem, and
the ability to perform refinements based on follow-up instructions. For this
paper's analysis, we empirically evaluated our benchmarking pipeline with
programming tasks from LiveBench, that are also automatically translated from
Python into Java and JavaScript. Our automated benchmark reveals that models
exhibit differing levels of performance across multiple dimensions of
instruction-following. Our benchmarking pipeline provides a more comprehensive
evaluation of code generation models, highlighting their strengths and
limitations across languages and generation goals.

</details>


### [25] [Enhancing software product lines with machine learning components](https://arxiv.org/abs/2510.27640)
*Luz-Viviana Cobaleda,Julián Carvajal,Paola Vallejo,Andrés López,Raúl Mazo*

Main category: cs.SE

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Modern software systems increasingly integrate machine learning (ML) due to
its advancements and ability to enhance data-driven decision-making. However,
this integration introduces significant challenges for software engineering,
especially in software product lines (SPLs), where managing variability and
reuse becomes more complex with the inclusion of ML components. Although
existing approaches have addressed variability management in SPLs and the
integration of ML components in isolated systems, few have explored the
intersection of both domains. Specifically, there is limited support for
modeling and managing variability in SPLs that incorporate ML components. To
bridge this gap, this article proposes a structured framework designed to
extend Software Product Line engineering, facilitating the integration of ML
components. It facilitates the design of SPLs with ML capabilities by enabling
systematic modeling of variability and reuse. The proposal has been partially
implemented with the VariaMos tool.

</details>


### [26] [On Selecting Few-Shot Examples for LLM-based Code Vulnerability Detection](https://arxiv.org/abs/2510.27675)
*Md Abdul Hannan,Ronghao Ni,Chi Zhang,Limin Jia,Ravi Mangal,Corina S. Pasareanu*

Main category: cs.SE

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Large language models (LLMs) have demonstrated impressive capabilities for
many coding tasks, including summarization, translation, completion, and code
generation. However, detecting code vulnerabilities remains a challenging task
for LLMs. An effective way to improve LLM performance is in-context learning
(ICL) - providing few-shot examples similar to the query, along with correct
answers, can improve an LLM's ability to generate correct solutions. However,
choosing the few-shot examples appropriately is crucial to improving model
performance. In this paper, we explore two criteria for choosing few-shot
examples for ICL used in the code vulnerability detection task. The first
criterion considers if the LLM (consistently) makes a mistake or not on a
sample with the intuition that LLM performance on a sample is informative about
its usefulness as a few-shot example. The other criterion considers similarity
of the examples with the program under query and chooses few-shot examples
based on the $k$-nearest neighbors to the given sample. We perform evaluations
to determine the benefits of these criteria individually as well as under
various combinations, using open-source models on multiple datasets.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [27] [Choreographer: A Full-System Framework for Fine-Grained Tasks in Cache Hierarchies](https://arxiv.org/abs/2510.26944)
*Hoa Nguyen,Pongstorn Maidee,Jason Lowe-Power,Alireza Kaviani*

Main category: cs.AR

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: In this paper, we introduce Choreographer, a simulation framework that
enables a holistic system-level evaluation of fine-grained accelerators
designed for latency-sensitive tasks. Unlike existing frameworks, Choreographer
captures all hardware and software overheads in core-accelerator and
cache-accelerator interactions, integrating a detailed gem5-based hardware
stack featuring an AMBA coherent hub interface (CHI) mesh network and a
complete Linux-based software stack. To facilitate rapid prototyping, it offers
a C++ application programming interface and modular configuration options. Our
detailed cache model provides accurate insights into performance variations
caused by cache configurations, which are not captured by other frameworks. The
framework is demonstrated through two case studies: a data-aware prefetcher for
graph analytics workloads, and a quicksort accelerator. Our evaluation shows
that the prefetcher achieves speedups between 1.08x and 1.88x by reducing
memory access latency, while the quicksort accelerator delivers more than 2x
speedup with minimal address translation overhead. These findings underscore
the ability of Choreographer to model complex hardware-software interactions
and optimize performance in small task offloading scenarios.

</details>


### [28] [Practical Timing Closure in FPGA and ASIC Designs: Methods, Challenges, and Case Studies](https://arxiv.org/abs/2510.26985)
*Mostafa Darvishi*

Main category: cs.AR

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: This paper presents an in-depth analysis of timing closure challenges and
constraints in Field Programmable Gate Arrays (FPGAs) and Application Specific
Integrated Circuits (ASICs). We examine core timing principles, architectural
distinctions, and design methodologies influencing timing behavior in both
technologies. A case study comparing the Xilinx Kintex UltraScale+ FPGA
(XCKU040) with a 7nm ASIC highlights practical timing analysis and performance
trade-offs. Experimental results show ASICs achieve superior timing of 45ps
setup and 35ps hold, while modern FPGAs remain competitive with 180ps setup and
120ps hold times, validating their suitability for high-performance designs.

</details>


### [29] [Descriptor-Based Object-Aware Memory Systems: A Comprehensive Review](https://arxiv.org/abs/2510.27070)
*Dong Tong*

Main category: cs.AR

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: The security and efficiency of modern computing systems are fundamentally
undermined by the absence of a native architectural mechanism to propagate
high-level program semantics, such as object identity, bounds, and lifetime,
across the hardware/software interface. This paper presents a comprehensive
survey of the architectural paradigm designed to bridge this semantic gap:
descriptor-based, object-aware memory systems. By elevating the descriptor to a
first-class architectural abstraction, this paradigm enables hardware to
dynamically acquire and enforce the rich semantics of software-defined objects.
This survey systematically charts the evolution and current landscape of this
approach. We establish the foundational concepts of memory objects and
descriptors and introduce a novel taxonomy of descriptor addressing modes,
providing a structured framework for analyzing and comparing diverse
implementations. Our unified analysis reveals how this paradigm holistically
addresses the intertwined challenges of memory protection, management, and
processing. As a culminating case study, we re-examine the CentroID model,
demonstrating how its hybrid tagged-pointer encoding and descriptor processing
mechanisms embody the path toward practical and efficient object-aware designs.
Finally, we outline how the explicit cross-layer communication of object
semantics provides a foundational research direction for next-generation cache
hierarchies, unified virtual memory, and even 128-bit architectures.

</details>


<div id='cs.PF'></div>

# cs.PF [[Back]](#toc)

### [30] [AMD MI300X GPU Performance Analysis](https://arxiv.org/abs/2510.27583)
*Chandrish Ambati,Trung Diep*

Main category: cs.PF

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: The rapid growth of large language models (LLMs) has driven the need for
high-performance, scalable GPU hardware capable of efficiently serving models
with hundreds of billions of parameters. While NVIDIA GPUs have traditionally
dominated LLM deployments due to their mature CUDA software stack and state-of
the-art accelerators, AMD's latest MI300X GPUs offer a compelling alternative,
featuring high HBM capacity, matrix cores, and their proprietary interconnect.
In this paper, we present a comprehensive evaluation of the AMD MI300X GPUs
across key performance domains critical to LLM inference including compute
throughput, memory bandwidth, and interconnect communication.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [31] [FinPos: A Position-Aware Trading Agent System for Real Financial Markets](https://arxiv.org/abs/2510.27251)
*Bijia Liu,Ronghao Dang*

Main category: cs.MA

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: The exceptional potential of large language models (LLMs) in handling text
information has garnered significant attention in the field of financial
trading. However, current trading agents primarily focus on single-step trading
tasks and lack awareness of continuous position management. Therefore, we
propose a position-aware trading task designed to simulate a more realistic
market. To address this task, we develop a trading agent system, FinPos,
optimized for position management. FinPos is able to interpret various types of
market information from a professional perspective, providing a reliable basis
for positioning decisions. To mitigate the substantial market risks arising
from position fluctuations, FinPos employs dual decision agents. Furthermore,
the continuous nature of position management necessitates our adoption of
multi-timescale rewards, which in turn empowers FinPos to effectively balance
short-term fluctuations against long-term trends. Extensive experiments
demonstrate that FinPos surpasses state-of-the-art trading agents in the
position-aware trading task, which closely mirrors real market conditions. More
importantly, our findings reveal that LLM-centered agent systems exhibit a
vast, largely unexplored potential in long-term market decision-making.

</details>
