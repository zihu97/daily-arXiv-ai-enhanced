{"id": "2602.06693", "categories": ["cs.NI", "cs.CC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.06693", "abs": "https://arxiv.org/abs/2602.06693", "authors": ["Robert Ganian", "Fionn Mc Inerney", "Dimitra Tsigkari"], "title": "Makespan Minimization in Split Learning: From Theory to Practice", "comment": "This paper will appear at IEEE INFOCOM 2026", "summary": "Split learning recently emerged as a solution for distributed machine learning with heterogeneous IoT devices, where clients can offload part of their training to computationally-powerful helpers. The core challenge in split learning is to minimize the training time by jointly devising the client-helper assignment and the schedule of tasks at the helpers. We first study the model where each helper has a memory cardinality constraint on how many clients it may be assigned, which represents the case of homogeneous tasks. Through complexity theory, we rule out exact polynomial-time algorithms and approximation schemes even for highly restricted instances of this problem. We complement these negative results with a non-trivial polynomial-time 5-approximation algorithm. Building on this, we then focus on the more general heterogeneous task setting considered by Tirana et al. [INFOCOM 2024], where helpers have memory capacity constraints and clients have variable memory costs. In this case, we prove that, unless P=NP, the problem cannot admit a polynomial-time approximation algorithm for any approximation factor. However, by adapting our aforementioned 5-approximation algorithm, we develop a novel heuristic for the heterogeneous task setting and show that it outperforms heuristics from prior works through extensive experiments.", "AI": {"tldr": "\u672c\u6587\u9488\u5bf9\u5f02\u6784\u7269\u8054\u7f51\u8bbe\u5907\u7684\u5206\u88c2\u5b66\u4e60\uff0c\u63d0\u51fa\u6700\u5c0f\u5316\u8bad\u7ec3\u65f6\u95f4\u7684\u89e3\u51b3\u65b9\u6848\uff1b\u9996\u5148\u5728\u540c\u8d28\u4efb\u52a1\u4e0b\u5f00\u53d15-\u8fd1\u4f3c\u7b97\u6cd5\uff0c\u7136\u540e\u5728\u5f02\u8d28\u4efb\u52a1\u8bbe\u5b9a\u4e2d\u8bbe\u8ba1\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u7684\u542f\u53d1\u5f0f\u7b97\u6cd5\u3002", "motivation": "\u52a8\u673a\uff1a\u5f3a\u8c03\u5728\u5206\u5e03\u5f0f\u673a\u5668\u5b66\u4e60\u4e2d\uff0c\u5f02\u6784IoT\u8bbe\u5907\u9700\u5c06\u8ba1\u7b97\u5378\u8f7d\u5230\u52a9\u624b\u8282\u70b9\uff0c\u6838\u5fc3\u6311\u6218\u5728\u4e8e\u901a\u8fc7\u5ba2\u6237\u7aef-\u52a9\u624b\u5206\u914d\u548c\u4efb\u52a1\u8c03\u5ea6\u6700\u5c0f\u5316\u8bad\u7ec3\u65f6\u95f4\uff0c\u5c24\u5176\u662f\u5904\u7406\u5185\u5b58\u7ea6\u675f\u548c\u53d8\u91cf\u6210\u672c\u573a\u666f\u3002", "method": "\u65b9\u6cd5\uff1a\u4f7f\u7528\u590d\u6742\u6027\u7406\u8bba\u8bc1\u660e\u540c\u8d28\u4efb\u52a1\u4e0b\u65e0\u9ad8\u6548\u7b97\u6cd5\uff0c\u5e76\u63d0\u51fa\u591a\u9879\u5f0f\u65f6\u95f45-\u8fd1\u4f3c\u7b97\u6cd5\uff1b\u6269\u5c55\u81f3\u5f02\u8d28\u4efb\u52a1\u65f6\uff0c\u6539\u8fdb\u4e86\u8be5\u7b97\u6cd5\u5f00\u53d1\u65b0\u542f\u53d1\u5f0forporationorization\uff0c\u5e76\u901a\u8fc7\u5927\u89c4\u6a21\u5b9e\u9a8c\u4e0e\u73b0\u6709\u65b9\u6cd5\u8fdb\u884c\u5bf9\u6bd4\u8bc4\u4f30\u3002", "result": "\u7ed3\u679c\uff1a\u540c\u8d28\u4efb\u52a1\u4e0b\u6392\u9664\u4e86\u7cbe\u786e\u591a\u9879\u5f0f\u65f6\u95f4\u7b97\u6cd5\u548c\u8fd1\u4f3c\u65b9\u6848\uff0c\u4f46\u63d0\u4f9b\u4e865-\u8fd1\u4f3c\u7b97\u6cd5\uff1b\u5f02\u8d28\u4efb\u52a1\u4e0b\u8bc1\u660e\u9664\u975eP=NP\u5426\u5219\u65e0\u8fd1\u4f3c\u7b97\u6cd5\uff0c\u542f\u53d1\u5f0f\u7b97\u6cd5\u5728\u5b9e\u9a8c\u4e2d\u663e\u8457\u4f18\u4e8e\u5148\u524d\u65b9\u6cd5\u3002", "conclusion": "\u7ed3\u8bba\uff1a\u5c3d\u7ba1\u5f02\u8d28\u4efb\u52a1\u7684\u8ba1\u7b97\u590d\u6742\u6027\u6781\u9ad8\uff0c\u57fa\u4e8e\u540c\u8d28\u7b97\u6cd5\u7684\u542f\u53d1\u5f0f\u65b9\u6cd5\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u8868\u73b0\u5353\u8d8a\uff0c\u4e3a\u5206\u88c2\u5b66\u4e60\u90e8\u7f72\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.06081", "categories": ["cs.MA", "cs.AI", "cs.GT"], "pdf": "https://arxiv.org/pdf/2602.06081", "abs": "https://arxiv.org/abs/2602.06081", "authors": ["Nunzio Lore", "Babak Heydari"], "title": "Communication Enhances LLMs' Stability in Strategic Thinking", "comment": "15 pages, 1 figure, 6 tables", "summary": "Large Language Models (LLMs) often exhibit pronounced context-dependent variability that undermines predictable multi-agent behavior in tasks requiring strategic thinking. Focusing on models that range from 7 to 9 billion parameters in size engaged in a ten-round repeated Prisoner's Dilemma, we evaluate whether short, costless pre-play messages emulating the cheap-talk paradigm affect strategic stability. Our analysis uses simulation-level bootstrap resampling and nonparametric inference to compare cooperation trajectories fitted with LOWESS regression across both the messaging and the no-messaging condition. We demonstrate consistent reductions in trajectory noise across a majority of the model-context pairings being studied. The stabilizing effect persists across multiple prompt variants and decoding regimes, though its magnitude depends on model choice and contextual framing, with models displaying higher baseline volatility gaining the most. While communication rarely produces harmful instability, we document a few context-specific exceptions and identify the limited domains in which communication harms stability. These findings position cheap-talk style communication as a low-cost, practical tool for improving the predictability and reliability of strategic behavior in multi-agent LLM systems.", "AI": {"tldr": "\u9884\u6e38\u620f\u6d88\u606f\u53ef\u51cf\u5c11\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u7b56\u7565\u6e38\u620f\u4e2d\u7684\u884c\u4e3a\u53ef\u53d8\u6027\uff0c\u63d0\u5347\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u53ef\u9884\u6d4b\u6027\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u7b56\u7565\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u663e\u8457\u7684\u4e0a\u4e0b\u6587\u4f9d\u8d56\u6027\u53ef\u53d8\u6027\uff0c\u7834\u574f\u591a\u667a\u80fd\u4f53\u884c\u4e3a\u7684\u53ef\u9884\u6d4b\u6027\u3002\u672c\u7814\u7a76\u8bc4\u4f30\u4e86\u7c7b\u4f3c\u5ec9\u4ef7\u8c08\u5224\u7684\u9884\u6e38\u620f\u6d88\u606f\u673a\u5236\u5bf9\u7b56\u7565\u7a33\u5b9a\u6027\u7684\u5f71\u54cd\u3002", "method": "\u91c7\u7528\u6a21\u62df\u7ea7\u81ea\u4e3e\u91cd\u91c7\u6837\u548c\u975e\u53c2\u6570\u63a8\u65ad\u65b9\u6cd5\uff0c\u901a\u8fc7LOWESS\u56de\u5f52\u6bd4\u8f83\u6709\u6d88\u606f\u548c\u65e0\u6d88\u606f\u6761\u4ef6\u4e0b\u7684\u5408\u4f5c\u8f68\u8ff9\uff0c\u5206\u6790\u56da\u5f92\u56f0\u5883\u6e38\u620f\u4e2d\u6a21\u578b\u884c\u4e3a\u3002", "result": "\u591a\u6570\u6a21\u578b-\u4e0a\u4e0b\u6587\u7ec4\u5408\u4e2d\u89c2\u5bdf\u5230\u8f68\u8ff9\u566a\u58f0\u663e\u8457\u51cf\u5c11\uff1b\u7a33\u5b9a\u4f5c\u7528\u5728\u4e0d\u540c\u63d0\u793a\u548c\u89e3\u7801\u673a\u5236\u4e2d\u6301\u7eed\uff0c\u5f3a\u5f31\u53d6\u51b3\u4e8e\u6a21\u578b\u9009\u62e9\u548c\u8bed\u5883\uff0c\u9ad8\u57fa\u7ebf\u6ce2\u52a8\u6a21\u578b\u83b7\u76ca\u6700\u5927\uff1b\u6c9f\u901a\u5f88\u5c11\u5bfc\u81f4\u6709\u5bb3\u4e0d\u7a33\u5b9a\uff0c\u4ec5\u5c11\u6570\u4f8b\u5916\u3002", "conclusion": "\u5ec9\u4ef7\u8c08\u5224\u5f0f\u6c9f\u901a\u53ef\u4f5c\u4e3a\u4f4e\u6210\u672c\u5b9e\u7528\u5de5\u5177\uff0c\u63d0\u5347\u591a\u667a\u80fd\u4f53LLM\u7cfb\u7edf\u7b56\u7565\u884c\u4e3a\u7684\u53ef\u9884\u6d4b\u6027\u548c\u53ef\u9760\u6027\u3002"}}
{"id": "2602.06057", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2602.06057", "abs": "https://arxiv.org/abs/2602.06057", "authors": ["Satyam Kumar", "Saurabh Jha"], "title": "Quantifying Energy-Efficient Edge Intelligence: Inference-time Scaling Laws for Heterogeneous Computing", "comment": null, "summary": "Large language model inference on resource constrained edge devices remains a major challenge for low latency intelligent systems, as existing solutions depend heavily on cloud or datacenter infrastructure. This work introduces QEIL, Quantifying Edge Intelligence via Inference time Scaling Laws, a unified framework for efficient local LLM inference using principled scaling laws and heterogeneous orchestration across CPU, GPU, and NPU accelerators. We derive five architecture agnostic theorems that characterize how inference efficiency scales with model size, sample budget, and device level constraints. QEIL integrates three optimization dimensions. First, inference time scaling laws show that heterogeneous workload distribution achieves superlinear efficiency gains that are not observed in homogeneous execution. Second, hardware aware routing is enabled through analytical cost models that account for compute throughput, memory bandwidth, power consumption, and thermal limits. Third, performance energy trade offs are quantified using novel metrics including Intelligence Per Watt, Energy Coverage Efficiency, and Price Power Performance. A unified orchestrator combines these components through progressive sample multiplexing to improve coverage. Extensive evaluation across five model families from 125M to 2.6B parameters demonstrates consistent gains, including 7 to 10.5 percentage point improvement in pass at k coverage, 35.6 to 78.2 percent energy reduction, 68 percent average power reduction enabling edge thermal budgets, 15.8 percent latency improvement, and zero accuracy loss. Results confirm that inference time scaling laws are universal and architecture agnostic, establishing heterogeneous edge orchestration as the optimal strategy for energy constrained intelligent systems.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86QEIL\u6846\u67b6\uff0c\u901a\u8fc7\u5f02\u6784\u534f\u4f5c\u786c\u4ef6\u548c\u6bd4\u4f8b\u5b9a\u5f8b\u4f18\u5316\u8fb9\u7f18\u8bbe\u5907\u4e0a\u7684\u5927\u8bed\u8a00\u6a21\u578b\u63a8\u7406\u6548\u7387\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u8d44\u6e90\u53d7\u9650\u7684\u8fb9\u7f18\u8bbe\u5907\u4e0a\u8fdb\u884c\u63a8\u7406\u5b58\u5728\u9ad8\u5ef6\u8fdf\uff0c\u73b0\u6709\u65b9\u6848\u8fc7\u5ea6\u4f9d\u8d56\u4e91\u57fa\u7840\u8bbe\u65bd\u3002", "method": "\u5efa\u7acb\u4e94\u4e2a\u67b6\u6784\u4e0d\u53ef\u77e5\u7684\u6bd4\u4f8b\u5b9a\u5f8b\u5b9a\u7406\uff0c\u96c6\u6210\u5f02\u6784\u5de5\u4f5c\u8d1f\u8f7d\u5206\u914d\u3001\u786c\u4ef6\u611f\u77e5\u8def\u7531\u548c\u6027\u80fd\u80fd\u91cf\u6743\u8861\u91cf\u5316\u6307\u6807\uff0c\u901a\u8fc7\u7edf\u4e00\u7f16\u6392\u5668\u548c\u6e10\u8fdb\u6837\u672c\u590d\u7528\u5b9e\u73b0\u4f18\u5316\u3002", "result": "\u5728125M\u81f32.6B\u53c2\u6570\u7684\u4e94\u4e2a\u6a21\u578b\u5bb6\u65cf\u4e2d\u6d4b\u8bd5\uff1apass@k\u8986\u76d6\u7387\u63d0\u9ad87-10.5\u4e2a\u767e\u5206\u70b9\uff0c\u80fd\u8017\u964d\u4f4e35.6-78.2%\uff0c\u5e73\u5747\u529f\u7387\u964d68%\uff0c\u5ef6\u8fdf\u6539\u558415.8%\uff0c\u4e14\u7cbe\u5ea6\u65e0\u635f\u5931\u3002", "conclusion": "\u63a8\u7406\u65f6\u95f4\u6bd4\u4f8b\u5b9a\u5f8b\u5177\u6709\u666e\u9002\u6027\u548c\u67b6\u6784\u4e0d\u53ef\u77e5\u6027\uff0c\u5f02\u6784\u8fb9\u7f18\u7f16\u6392\u662f\u80fd\u6e90\u53d7\u9650\u667a\u80fd\u7cfb\u7edf\u7684\u6700\u4f18\u7b56\u7565\u3002"}}
{"id": "2602.06252", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2602.06252", "abs": "https://arxiv.org/abs/2602.06252", "authors": ["Ahmed J. Abdelmaksoud", "Cristian Sestito", "Shiwei Wang", "Themis Prodromakis"], "title": "D-Legion: A Scalable Many-Core Architecture for Accelerating Matrix Multiplication in Quantized LLMs", "comment": null, "summary": "The performance gains obtained by large language models (LLMs) are closely linked to their substantial computational and memory requirements. Quantized LLMs offer significant advantages with extremely quantized models, motivating the development of specialized architectures to accelerate their workloads. This paper proposes D-Legion, a novel scalable many-core architecture, designed using many adaptive-precision systolic array cores, to accelerate matrix multiplication in quantized LLMs. The proposed architecture consists of a set of Legions where each Legion has a group of adaptive-precision systolic arrays. D-Legion supports multiple computation modes, including quantized sparse and dense matrix multiplications. The block structured sparsity is exploited within a fully-sparse, or partially-sparse windows. In addition, memory accesses of partial summations (psums) are spatially reduced through parallel accumulators. Furthermore, data reuse is maximized through optimized scheduling techniques by multicasting matrix tiles across the Legions. A comprehensive design space exploration is performed in terms of Legion/core granularity to determine the optimal Legion configuration. Moreover, D-Legion is evaluated on attention workloads from two BitNet models, delivering up to 8.2$\\times$ lower latency, up to 3.8$\\times$ higher memory savings, and up to 3$\\times$ higher psum memory savings compared to state-of-the-art work. D-Legion, with eight Legions and 64 total cores, achieves a peak throughput of 135,68 TOPS at a frequency of 1 GHz. A scaled version of D-Legion, with 32 Legions, is compared to Google TPUv4i, achieving up to 2.5$\\times$ lower total latency, up to 2.3$\\times$ higher total throughput, and up to 2.7$\\times$ higher total memory savings.", "AI": {"tldr": "\u63d0\u51faD-Legion\u67b6\u6784\uff0c\u4e00\u79cd\u53ef\u6269\u5c55\u591a\u6838\u7cfb\u7edf\uff0c\u91c7\u7528\u81ea\u9002\u5e94\u7cbe\u5ea6\u8109\u52a8\u9635\u5217\u52a0\u901f\u91cf\u5316\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u77e9\u9635\u4e58\u6cd5\u8fd0\u7b97\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u6027\u80fd\u589e\u76ca\u4f9d\u8d56\u9ad8\u8ba1\u7b97\u548c\u5185\u5b58\u9700\u6c42\uff0c\u91cf\u5316\u6a21\u578b\u6f5c\u529b\u663e\u8457\uff0c\u9700\u5f00\u53d1\u4e13\u7528\u67b6\u6784\u4ee5\u52a0\u901f\u6267\u884c\u3002", "method": "\u6784\u5efa\u591aLegion\u7ed3\u6784\uff0c\u6bcf\u4e2a\u542b\u81ea\u9002\u5e94\u7cbe\u5ea6\u8109\u52a8\u9635\u5217\u6838\u5fc3\uff0c\u652f\u6301\u7a00\u758f/\u7a20\u5bc6\u77e9\u9635\u4e58\u6cd5\uff1b\u901a\u8fc7\u5757\u7ed3\u6784\u7a00\u758f\u5316\u3001\u5e76\u884c\u7d2f\u52a0\u5668\u964d\u4f4epsum\u5185\u5b58\u8bbf\u95ee\uff0c\u4f18\u5316\u8c03\u5ea6\u63d0\u5347\u6570\u636e\u590d\u7528\uff1b\u63a2\u7d22Legion/\u6838\u7c92\u5ea6\u786e\u5b9a\u6700\u4f73\u914d\u7f6e\u3002", "result": "\u5728BitNet\u6a21\u578b\u4e2d\uff0c\u5ef6\u8fdf\u964d\u4f4e8.2\u500d\uff0c\u5185\u5b58\u8282\u7701\u63d0\u53473.8\u500d\uff0cpsum\u5185\u5b58\u8282\u7701\u63d0\u53473\u500d\uff1b8 Legions\u67b6\u6784\u5cf0\u503c\u541e\u5410\u91cf135.68 TOPS\uff1b32 Legions\u7248\u672c\u76f8\u6bd4TPUv4i\u964d\u4f4e\u5ef6\u8fdf2.5\u500d\u3001\u63d0\u5347\u541e\u5410\u91cf2.3\u500d\u548c\u5185\u5b58\u8282\u77012.7\u500d\u3002", "conclusion": "D-Legion\u9ad8\u6548\u52a0\u901f\u91cf\u5316LLM\u8bad\u7ec3\uff0c\u5728\u6269\u5c55\u6027\u3001\u5ef6\u8fdf\u3001\u541e\u5410\u91cf\u53ca\u5185\u5b58\u6548\u7387\u65b9\u9762\u8d85\u8d8a\u73b0\u6709\u6280\u672f\uff0c\u5177\u6709\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2602.06216", "categories": ["cs.PF"], "pdf": "https://arxiv.org/pdf/2602.06216", "abs": "https://arxiv.org/abs/2602.06216", "authors": ["Christiaan Boerkamp", "Akhil John Thomas"], "title": "End-to-End Throughput Benchmarking of Portable Deterministic CNN-Based Signal Processing Pipelines", "comment": "6 pages, 3 tables, Authors contributed equally", "summary": "This paper presents a benchmarking methodology for evaluating end-to-end performance of deterministic signal-processing pipelines expressed using CNN-compatible primitives. The benchmark targets phased-array workloads such as ultrasound imaging and evaluates complete RF-to-image pipelines under realistic execution conditions. Performance is reported using sustained input throughput (MB/s), effective frame rate (FPS), and, where available, incremental energy per run and peak memory usage. Using this methodology, we benchmark a single deterministic, training-free CNN-based signal-processing pipeline executed unmodified across heterogeneous accelerator platforms, including an NVIDIA RTX 5090 GPU and a Google TPU v5e-1. The results demonstrate how different operator formulations (dynamic indexing, fully CNN-expressed, and sparse-matrix-based) impact performance and portability across architectures. This work is motivated by the need for portable, certifiable signal-processing implementations that avoid hardware-specific refactoring while retaining high performance on modern AI accelerators.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u8bc4\u4f30\u7aef\u5230\u7aef\u786e\u5b9a\u6027\u4fe1\u53f7\u5904\u7406\u7ba1\u9053\u6027\u80fd\u7684\u57fa\u51c6\u6d4b\u8bd5\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u9488\u5bf9\u76f8\u63a7\u9635\u5de5\u4f5c\u8d1f\u8f7d\uff08\u5982\u8d85\u58f0\u6210\u50cf\uff09\uff0c\u8bc4\u4f30\u4eceRF\u5230\u56fe\u50cf\u7684\u5b8c\u6574\u7ba1\u9053\u5728\u5404\u79cd\u52a0\u901f\u5e73\u53f0\u4e0a\u7684\u8868\u73b0\u3002", "motivation": "\u52a8\u673a\u662f\u73b0\u4ee3AI\u52a0\u901f\u5668\u4e0a\u5f00\u53d1\u53ef\u79fb\u690d\u4e14\u53ef\u8ba4\u8bc1\u7684\u4fe1\u53f7\u5904\u7406\u5b9e\u73b0\uff0c\u4ee5\u907f\u514d\u786c\u4ef6\u7279\u5b9a\u7684\u91cd\u6784\uff0c\u540c\u65f6\u4fdd\u6301\u9ad8\u6027\u80fd\u3002", "method": "\u65b9\u6cd5\u5305\u62ec\u5728\u73b0\u5b9e\u4e2d\u6267\u884c\u6761\u4ef6\u4e0b\uff0c\u4f7f\u7528\u6301\u7eed\u8f93\u5165\u541e\u5410\u91cf\uff08MB/s\uff09\u3001\u6709\u6548\u5e27\u7387\uff08FPS\uff09\u3001\u5355\u6b21\u8fd0\u884c\u589e\u91cf\u80fd\u8017\u548c\u5cf0\u503c\u5185\u5b58\u4f7f\u7528\u6307\u6807\u62a5\u544a\u6027\u80fd\uff0c\u5e76\u5728NVIDIA RTX 5090 GPU\u548cGoogle TPU v5e-1\u7b49\u5f02\u6784\u5e73\u53f0\u4e0a\u57fa\u51c6\u6d4b\u8bd5\u65e0\u9700\u8bad\u7ec3\u4e14\u65e0\u9700\u4fee\u6539\u7684CNN\u7ba1\u9053\u3002", "result": "\u7ed3\u679c\u663e\u793a\u4e0d\u540c\u7b97\u5b50\u5b9e\u73b0\u65b9\u5f0f\uff08\u5982\u52a8\u6001\u7d22\u5f15\u3001\u5168CNN\u8868\u8fbe\u548c\u7a00\u758f\u77e9\u9635\u57fa\u7840\uff09\u5bf9\u67b6\u6784\u95f4\u7684\u6027\u80fd\u548c\u53ef\u79fb\u690d\u6027\u4ea7\u751f\u663e\u8457\u5f71\u54cd\u3002", "conclusion": "\u7ed3\u8bba\u662f\u8be5\u57fa\u51c6\u65b9\u6cd5\u652f\u6301\u521b\u5efa\u9ad8\u6548\u53ef\u79fb\u690d\u7684\u4fe1\u53f7\u5904\u7406\u7ba1\u9053\uff0c\u6ee1\u8db3\u591a\u6837AI\u52a0\u901f\u5668\u7684\u9700\u6c42\uff0c\u65e0\u9700\u727a\u7272\u6027\u80fd\u3002"}}
{"id": "2602.06599", "categories": ["cs.MA", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.06599", "abs": "https://arxiv.org/abs/2602.06599", "authors": ["Ariyan Bighashdel", "Thiago D. Sim\u00e3o", "Frans A. Oliehoek"], "title": "Sample-Efficient Policy Space Response Oracles with Joint Experience Best Response", "comment": "Accepted at the 25th International Conference on Autonomous Agents and Multiagent Systems (AAMAS 2026)", "summary": "Multi-agent reinforcement learning (MARL) offers a scalable alternative to exact game-theoretic analysis but suffers from non-stationarity and the need to maintain diverse populations of strategies that capture non-transitive interactions. Policy Space Response Oracles (PSRO) address these issues by iteratively expanding a restricted game with approximate best responses (BRs), yet per-agent BR training makes it prohibitively expensive in many-agent or simulator-expensive settings. We introduce Joint Experience Best Response (JBR), a drop-in modification to PSRO that collects trajectories once under the current meta-strategy profile and reuses this joint dataset to compute BRs for all agents simultaneously. This amortizes environment interaction and improves the sample efficiency of best-response computation. Because JBR converts BR computation into an offline RL problem, we propose three remedies for distribution-shift bias: (i) Conservative JBR with safe policy improvement, (ii) Exploration-Augmented JBR that perturbs data collection and admits theoretical guarantees, and (iii) Hybrid BR that interleaves JBR with periodic independent BR updates. Across benchmark multi-agent environments, Exploration-Augmented JBR achieves the best accuracy-efficiency trade-off, while Hybrid BR attains near-PSRO performance at a fraction of the sample cost. Overall, JBR makes PSRO substantially more practical for large-scale strategic learning while preserving equilibrium robustness.", "AI": {"tldr": "\u63d0\u51faJBR\u65b9\u6cd5\u6539\u8fdbPSRO\uff0c\u901a\u8fc7\u590d\u7528\u8054\u5408\u6570\u636e\u96c6\u4e00\u6b21\u6027\u8ba1\u7b97\u6240\u6709\u667a\u80fd\u4f53\u6700\u4f18\u54cd\u5e94\uff0c\u5927\u5e45\u63d0\u5347\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u7684\u6837\u672c\u6548\u7387\u3002", "motivation": "\u89e3\u51b3PSRO\u5728\u591a\u667a\u80fd\u4f53\u573a\u666f\u4e2d\u56e0\u72ec\u7acb\u8ba1\u7b97\u6700\u4f18\u54cd\u5e94\u5bfc\u81f4\u7684\u8bad\u7ec3\u5f00\u9500\u8fc7\u5927\u95ee\u9898\uff0c\u5c24\u5176\u5728\u667a\u80fd\u4f53\u4f17\u591a\u6216\u4eff\u771f\u6210\u672c\u9ad8\u65f6\u96be\u4ee5\u5e94\u7528\u3002", "method": "JBR\u6838\u5fc3\u662f\u5355\u6b21\u91c7\u6837\u5f53\u524d\u5143\u7b56\u7565\u8f68\u8ff9\u5e76\u590d\u7528\u6570\u636e\u96c6\uff0c\u540c\u6b65\u8ba1\u7b97\u6240\u6709\u667a\u80fd\u4f53\u6700\u4f18\u54cd\u5e94\u3002\u9488\u5bf9\u79bb\u7ebfRL\u7684\u5206\u5e03\u504f\u79fb\u95ee\u9898\uff0c\u63d0\u51fa\u4e09\u79cd\u6539\u8fdb\uff1a(1)\u5e26\u5b89\u5168\u7b56\u7565\u7684\u4fdd\u5b88JBR\uff1b(2)\u7406\u8bba\u53ef\u8bc1\u4fdd\u969c\u7684\u63a2\u7d22\u589e\u5f3aJBR\uff1b(3)\u878d\u5408JBR\u4e0e\u72ec\u7acb\u66f4\u65b0\u7684\u6df7\u5408BR\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff1a\u63a2\u7d22\u589e\u5f3aJBR\u53d6\u5f97\u6700\u4f73\u7cbe\u5ea6-\u6548\u7387\u5e73\u8861\uff0c\u6df7\u5408BR\u4ee5\u6781\u4f4e\u6837\u672c\u6210\u672c\u0627\u0635\u0644\u8fd1PSRO\u6027\u80fd\u3002", "conclusion": "JBR\u663e\u8457\u63d0\u5347PSRO\u5728\u5927\u89c4\u6a21\u7b56\u7565\u5b66\u4e60\u4e2d\u7684\u5b9e\u7528\u6027\uff0c\u540c\u65f6\u4fdd\u6301\u5747\u8861\u5728\u8be5\u6027.children_buttonhasIcon"}}
{"id": "2602.06063", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2602.06063", "abs": "https://arxiv.org/abs/2602.06063", "authors": ["Shouyu Du", "Miaoxiang Yu", "Zhiheng Ni", "Jillian Cai", "Qing Yang", "Tao Wei", "Zhenyu Xu"], "title": "Mapping Gemma3 onto an Edge Dataflow Architecture", "comment": "Original Version, data shall be updated", "summary": "We present the first end-to-end deployment of the Gemma3 family of large language and vision models on a tiled edge dataflow architecture (AMD Ryzen AI NPU). Our work introduces a set of hardware-aware techniques. For prefill, we introduce an efficient dequantization engine, optimize tiled matrix multiplication kernels, and propose FlowQKV, a chunked, pipelined attention mechanism. For decoding, we introduce FusedDQP, which fuses dequantization and projection into a single kernel, and FlowKV, which re-structures attention to sustain high memory bandwidth utilization. Together with a compact Q4NX 4-bit quantization format, these methods yield up to $5.2\\times$ faster prefill and $4.8\\times$ faster decoding versus the iGPU, and $33.5\\times$ and $2.2\\times$ over the CPU, respectively. Power efficiency improves by as much as $67.2\\times$ and $222.9\\times$ compared to the iGPU and CPU. The proposed approach demonstrates that modern NPUs can deliver practical, low-power LLM and VLM inference at the edge, and provides a generalizable blueprint for mapping transformer-based models onto tiled dataflow accelerators.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u5728AMD Ryzen AI NPU\u4e0a\u90e8\u7f72Gemma3\u8bed\u8a00\u89c6\u89c9\u6a21\u578b\u7684\u7aef\u5230\u7aef\u65b9\u6848\uff0c\u63d0\u51fa\u786c\u4ef6\u4f18\u5316\u6280\u672f\uff0c\u663e\u8457\u63d0\u5347\u9884\u586b\u5145\u548c\u89e3\u7801\u901f\u5ea6\u53ca\u80fd\u6548\u3002", "motivation": "\u8bc1\u660e\u73b0\u4ee3NPU\u80fd\u5b9e\u73b0\u8fb9\u7f18\u8bbe\u5907\u4e0a\u4f4e\u529f\u8017LLM/VLM\u63a8\u7406\uff0c\u5e76\u63d0\u4f9b\u53d8\u6362\u5668\u6a21\u578b\u5728\u74e6\u7247\u5f0f\u6570\u636e\u6d41\u52a0\u901f\u5668\u4e0a\u7684\u53ef\u6269\u5c55\u84dd\u56fe\u3002", "\u0438\u0441\u0430\u043b": "\u9884\u586b\u5145\u4f18\u5316\uff1a\u9ad8\u6548\u89e3\u91cf\u5316\u5f15\u64ce\u3001\u77e9\u9635\u4e58\u6838\u4f18\u5316\u53ca\u5206\u5757\u6d41\u6c34\u7ebf\u6ce8\u610f\u529bFlowQKV\uff1b\u89e3\u7801\u4f18\u5316\uff1a\u878d\u5408\u89e3\u91cf\u4e0e\u6295\u5f71\u7684FusedDQP\u3001\u91cd\u6784\u6ce8\u610f\u529b\u7684FlowKV\uff1b\u7ed3\u54084\u6bd4\u7279\u91cf\u5316\u683c\u5f0fQ4NX\u3002", "result": "\u76f8\u6bd4\u96c6\u6210GPU\uff1a\u9884\u586b\u5145\u5feb5.2\u500d\u3001\u89e3\u7801\u5feb4.8\u500d\uff1b\u76f8\u6bd4CPU\uff1a\u9884\u586b\u5145\u5feb33.5\u500d\u3001\u89e3\u7801\u5feb2.2\u500d\uff1b\u80fd\u6548\u63d0\u5347\u5206\u522b\u8fbe67.2\u500d\uff08vs GPU\uff09\u548c222.9\u500d\uff08vs CPU\uff09\u3002", "conclusion": "NPU\u652f\u6301\u9ad8\u6548\u8fb9\u7f18\u63a8\u7406\uff0c\u65b9\u6cd5\u4e3a\u53d8\u6362\u5668\u6a21\u578b\u5728\u6570\u636e\u6d41\u52a0\u901f\u5668\u4e0a\u7684\u6620\u5c04\u63d0\u4f9b\u901a\u7528\u6846\u67b6\u3002", "method": "Method extraction failed"}}
{"id": "2602.06064", "categories": ["cs.DC", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.06064", "abs": "https://arxiv.org/abs/2602.06064", "authors": ["Yi-Xiang Hu", "Yuke Wang", "Feng Wu", "Zirui Huang", "Shuli Zeng", "Xiang-Yang Li"], "title": "iScheduler: Reinforcement Learning-Driven Continual Optimization for Large-Scale Resource Investment Problems", "comment": "13 pages, 7 figures,", "summary": "Scheduling precedence-constrained tasks under shared renewable resources is central to modern computing platforms. The Resource Investment Problem (RIP) models this setting by minimizing the cost of provisioned renewable resources under precedence and timing constraints. Exact mixed-integer programming and constraint programming become impractically slow on large instances, and dynamic updates require schedule revisions under tight latency budgets. We present iScheduler, a reinforcement-learning-driven iterative scheduling framework that formulates RIP solving as a Markov decision process over decomposed subproblems and constructs schedules through sequential process selection. The framework accelerates optimization and supports reconfiguration by reusing unchanged process schedules and rescheduling only affected processes. We also release L-RIPLIB, an industrial-scale benchmark derived from cloud-platform workloads with 1,000 instances of 2,500-10,000 tasks. Experiments show that iScheduler attains competitive resource costs while reducing time to feasibility by up to 43$\\times$ against strong commercial baselines.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faiScheduler\uff0c\u4e00\u79cd\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u8fed\u4ee3\u8c03\u5ea6\u6846\u67b6\uff0c\u7528\u4e8e\u9ad8\u6548\u6c42\u89e3\u8d44\u6e90\u6295\u8d44\u95ee\u9898\uff08RIP\uff09\uff0c\u5728\u5de5\u4e1a\u89c4\u6a21\u57fa\u51c6\uff08L-RIPLIB\uff09\u4e0a\u663e\u8457\u52a0\u5feb\u8c03\u5ea6\u901f\u5ea6\u548c\u52a8\u6001\u66f4\u65b0\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5982\u6df7\u5408\u6574\u6570\u89c4\u5212\u548c\u7ea6\u675f\u89c4\u5212\u5728\u5904\u7406\u5927\u89c4\u6a21\u4efb\u52a1\uff082500-10000\u4e2a\u4efb\u52a1\uff09\u65f6\u901f\u5ea6\u8fc7\u6162\uff0c\u4e14\u52a8\u6001\u66f4\u65b0865.\u8981\u6c42\u4f4e\u5ef6\u8fdf\u91cd\u6784\u3002\u672c\u7814\u7a76\u65e8\u5728\u5f00\u53d1\u4e00\u4e2a\u5feb\u901f\u652f\u6301\u5b9e\u65f6\u91cd\u65b0\u914d\u7f6e\u7684\u8c03\u5ea6\u89e3\u51b3\u65b9\u6848\u3002", "method": "iScheduler\u5c06RIP\u5efa\u6a21\u4e3a\u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0b\u5206\u89e3\u5b50\u95ee\u9898\uff0c\u901a\u8fc7\u5e8f\u8d2f\u8fc7\u7a0b\u9009\u62e9\u6784\u5efa\u8c03\u5ea6\u8ba1\u5212\uff1b\u652f\u6301\u91cd\u7528\u672a\u66f4\u6539\u8fdb\u7a0b\uff0c\u4ec5\u91cd\u8c03\u5ea6\u5f71\u54cd\u6211\u4eec\u53ef\u4ee5\u53d8\u901f\u90e8\u5206\uff0c\u63d0\u5347\u4f18\u5316\u6548\u7387\u3002", "result": "\u5728L-RIPLIB\u6570\u636e\u96c6\uff081000\u4e2a\u5b9e\u4f8b\uff09\u4e0a\u5b9e\u9a8c\uff0ciScheduler\u5b9e\u73b0\u4e0e\u5546\u4e1a\u57fa\u51c6\u76f8\u4f3c\u7684\u8d44\u6e90\u6210\u672c\uff0c\u540c\u65f6\u5c06\u53ef\u8fbe\u6027\u65f6\u95f4\u6700\u9ad8\u51cf\u5c1143\u500d\u3002", "conclusion": "\u8be5\u6846\u67b6\u8bc1\u660e\u5f3a\u5316\u5b66\u4e60\u9a71\u52a8\u7684\u8fed\u4ee3\u65b9\u6cd5\u80fd\u6709\u6548\u5904\u7406\u5927\u89c4\u6a21\u4efb\u52a1\u8c03\u5ea6\u95ee\u9898\uff0c\u52a0\u901f\u4f18\u5316\u5e76\u652f\u6301\u5b9e\u65f6\u8d44\u6e90\u91cd\u6784\u3002"}}
{"id": "2602.06223", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2602.06223", "abs": "https://arxiv.org/abs/2602.06223", "authors": ["Juan Marcano", "Ashish Samant", "Kai Song", "Lingchao Chen", "Kaelan Mikowicz", "Tim Smyth", "Mengdie Zhang", "Ali Zamani", "Arturo Bravo Rovirosa", "Sowjanya Puligadda", "Srikanth Prodduturi", "Mayank Bansal"], "title": "Scaling Mobile Chaos Testing with AI-Driven Test Execution", "comment": "10 pages of content, 1 page of citations, 7 figures, 6 tables", "summary": "Mobile applications in large-scale distributed systems are susceptible to backend service failures, yet traditional chaos engineering approaches cannot scale mobile testing due to the combinatorial explosion of flows, locations, and failure scenarios that need validation. We present an automated mobile chaos testing system that integrates DragonCrawl, an LLM-based mobile testing platform, with uHavoc, a service-level fault injection system. The key insight is that adaptive AI-driven test execution can navigate mobile applications under degraded backend conditions, eliminating the need to manually write test cases for each combination of user flow, city, and failure type. Since Q1 2024, our system has executed over 180,000 automated chaos tests across 47 critical flows in Uber's Rider, Driver, and Eats applications, representing approximately 39,000 hours of manual testing effort that would be impractical at this scale. We identified 23 resilience risks, with 70% being architectural dependency violations where non-critical service failures degraded core user flows. Twelve issues were severe enough to prevent trip requests or food orders. Two caused application crashes detectable only through mobile chaos testing, not backend testing alone. Automated root cause analysis reduced debugging time from hours to minutes, achieving 88% precision@5 in attributing mobile failures to specific backend services. This paper presents the system design, evaluates its performance under fault injection (maintaining 99% test reliability), and reports operational experience demonstrating that continuous mobile resilience validation is achievable at production scale.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u81ea\u52a8\u5316\u79fb\u52a8\u6df7\u6c8c\u6d4b\u8bd5\u7cfb\u7edf\uff0c\u96c6\u6210LLM\u79fb\u52a8\u6d4b\u8bd5\u5e73\u53f0\u4e0e\u670d\u52a1\u7ea7\u6545\u969c\u6ce8\u5165\uff0c\u89e3\u51b3\u4f20\u7edf\u65b9\u6cd5\u65e0\u6cd5\u5e94\u5bf9\u79fb\u52a8\u7aef\u5927\u89c4\u6a21\u7ec4\u5408\u6d4b\u8bd5\u7684\u95ee\u9898\u3002", "motivation": "\u5927\u89c4\u6a21\u5206\u5e03\u5f0f\u7cfb\u7edf\u4e2d\u79fb\u52a8\u5e94\u7528\u6613\u53d7\u540e\u7aef\u670d\u52a1\u6545\u969c\u5f71\u54cd\uff0c\u4f20\u7edf\u6df7\u6c8c\u5de5\u7a0b\u56e0\u7528\u6237\u6d41\u3001\u4f4d\u7f6e\u3001\u6545\u969c\u573a\u666f\u7ec4\u5408\u7206\u70b8\u800c\u65e0\u6cd5\u5b9e\u73b0\u89c4\u6a21\u5316\u79fb\u52a8\u7aef\u6d4b\u8bd5\u3002", "method": "\u7ed3\u5408DragonCrawl\uff08\u57fa\u4e8eLLM\u7684\u79fb\u52a8\u6d4b\u8bd5\u5e73\u53f0\uff09\u4e0euHavoc\uff08\u670d\u52a1\u5c42\u6545\u969c\u6ce8\u5165\u7cfb\u7edf\uff09\uff0c\u5229\u7528\u81ea\u9002\u5e94AI\u9a71\u52a8\u6d4b\u8bd5\u6267\u884c\u8986\u76d6\u590d\u6742\u573a\u666f\uff0c\u907f\u514d\u624b\u52a8\u7f16\u5199\u6d77\u91cf\u6d4b\u8bd5\u7528\u4f8b\u3002", "result": "202ELS4\u5e74Q1\u4ee5\u6765\uff0c\u5bf9Uber\u4e09\u5927\u5e94\u7528\u6267\u884c\u8d8518\u4e07\u6b21\u6d4b\u8bd5\uff0c\u8bc6\u522b23\u4e2a\u5f39\u6027\u98ce\u9669\uff0870%\u4e3a\u67b6\u6784\u4f9d\u8d56\u8fdd\u89c4\uff0c12\u4e2a\u4e25\u91cd\u95ee\u9898\u963b\u788d\u6838\u5fc3\u529f\u80fd\uff09\uff0c\u81ea\u52a8\u5316\u6839\u56e0\u5206\u6790\u5c06\u8c03\u8bd5\u65f6\u95f4\u4ece\u5c0f\u65f6\u7f29\u77ed\u81f3\u5206\u949f\uff0c\u7cbe\u51c6\u5ea6\u8fbe88%\u3002", "conclusion": "\u7cfb\u7edf\u9a8c\u8bc1\u4e86\u751f\u4ea7\u7ea7\u79fb\u52a8\u97e7\u6027\u6301\u7eed\u6d4b\u8bd5\u53ef\u884c\u6027\uff0c\u6545\u969c\u6ce8\u5165\u4e0b\u4fdd\u630199%\u53ef\u9760\u6027\uff0c\u5e76\u53d1\u73b0\u4ec5\u901a\u8fc7\u79fb\u52a8\u6df7\u6c8c\u6d4b\u8bd5\u53ef\u68c0\u6d4b\u7684\u5d29\u6e83\u95ee\u9898\u3002"}}
{"id": "2602.06069", "categories": ["cs.DC", "cs.AI", "cs.LG", "cs.SE"], "pdf": "https://arxiv.org/pdf/2602.06069", "abs": "https://arxiv.org/abs/2602.06069", "authors": ["Dinesh Gopalan", "Ratul Ali"], "title": "HQP: Sensitivity-Aware Hybrid Quantization and Pruning for Ultra-Low-Latency Edge AI Inference", "comment": "7 pages, 3 figures, 2 tables", "summary": "The escalating demand for high-fidelity, real-time inference in distributed edge-cloud environments necessitates aggressive model optimization to counteract severe latency and energy constraints. This paper introduces the Hybrid Quantization and Pruning (HQP) framework, a novel, integrated methodology designed to achieve synergistic model acceleration while adhering to strict quality guarantees. We detail a sensitivity-aware structural pruning algorithm that employs a dynamic weight sensitivity metric, derived from a highly efficient approximation of the Fisher Information Matrix (FIM), to guide the iterative removal of redundant filters. This pruning is strictly conditional, enforcing an adherence to a maximum permissible accuracy drop (Delta ax) before the model proceeds to 8-bit post-training quantization. This rigorous coordination is critical, as it ensures the resultant sparse model structure is maximally robust to quantization error and hardware-specific kernel optimization. Exhaustive evaluation across heterogeneous NVIDIA Jetson edge platforms, utilizing resource-efficient architectures like MobileNetV3 and ResNet-18, demonstrates that the HQP framework achieves a peak performance gain of 3.12 times inference speedup and a 55 percent model size reduction, while rigorously containing the accuracy drop below the 1.5 percent constraint. A comprehensive comparative analysis against conventional single-objective compression techniques validates the HQP framework as a superior, hardware-agnostic solution for deploying ultra-low-latency AI in resource-limited edge infrastructures.", "AI": {"tldr": "\u672c\u7814\u7a76\u540c\u6b65\u4e86\u6df7\u5408\u91cf\u5316\u548c\u4fee\u526a\uff08HQP\uff09\u6846\u67b6\uff0c jovens\u901a\u8fc7\u7075\u654f\u5ea6\u8ba4\u77e5\u526a\u679d\u548c\u91cf\u5316\u534f\u540c\u52a0\u901f\u6a21\u578b\uff0c\u4ee5\u5728\u8fb9\u7f18\u8ba1\u7b97\u4e2d\u5b9e\u73b0\u9ad8\u4fdd\u771f\u5b9e\u65f6\u63a8\u7406\uff0c\u540c\u65f6\u4e25\u683c\u63a7\u5236\u7cbe\u5ea6\u635f\u5931\u3002", "motivation": "\u5206\u5e03\u5f0f\u8fb9\u7f18\u4e91\u8ba1\u7b97\u90e8\u7f72\u4e2d\uff0c\u5bf9\u9ad8\u4fdd\u771f\u5b9e\u65f6\u63a8\u7406\u7684\u9700\u6c42\u6fc0\u589e\uff0c\u9762\u4e34\u7684\u4e25\u91cd\u5ef6\u8fdf\u548c\u80fd\u8017\u7ea6\u675f\u4e9f\u9700\u9ad8\u6548\u6a21\u578b\u4f18\u5316\u6765\u89e3\u51b3\uff0c\u4ee5\u786e\u4fdd\u5728\u8d44\u6e90\u6709\u9650\u73af\u5883\u4e0b\u7ef4\u6301\u6027\u80fd\u3002", "method": "\u91c7\u7528\u7075\u654f\u5ea6\u8ba4\u77e5\u7684\u7ed3\u6784\u5316\u526a\u679d\u7b97\u6cd5\uff1a\u52a8\u6001\u6743\u91cd\u654f\u611f\u5ea6\u6307\u6807\u6e90\u81eaFisher\u4fe1\u606f\u77e9\u9635\u7684\u9ad8\u6548\u8fd1\u4f3c\uff0c\u8fed\u4ee3\u79fb\u9664\u5197\u4f59\u6ee4\u6ce2\u987e\uff1b\u4e25\u683c\u57fa\u4e8e\u6700\u9ad8\u53ef\u5bb9\u5fcd\u7cbe\u5ea6\u964d\u5e45\u0394ax\u6761\u4ef6\u540e\uff0c\u6267\u884c8\u4f4d\u8bad\u7ec3\u540e\u91cf\u5316\uff0c\u589e\u5f3a\u6a21\u578b\u5bf9\u91cf\u5316\u9519\u8bef\u548c\u786c\u4ef6\u4f18\u5316\u7684\u9c81\u68d2\u6027\u3002", "result": "\u5728NVIDIA Jetson\u8fb9\u7f18\u5e73\u53f0\u4e0a\uff0c\u4f7f\u7528MobileNetV3\u548cResNet-18\u8fdb\u884c\u6d4b\u8bd5\uff1a\u5b9e\u73b0\u5cf0\u503c\u6027\u80fd\u63d0\u5347\uff0c\u63a8\u7406\u52a0\u901f\u8fbe3.12\u500d\uff0c\u6a21\u578b\u5927\u5c0f\u51cf\u5c1155%\uff0c\u5e76\u5c06\u7cbe\u5ea6\u964d\u5e45\u63a7\u5236\u4f4e\u4e8e1.5%\u3002", "conclusion": "\u4e0e\u4f20\u7edf\u5355\u76ee\u6807\u538b\u7f29\u6280\u672f\u76f8\u6bd4\uff0cHQP\u6846\u67b6\u88ab\u9a8c\u8bc1\u4e3a\u66f4\u4f18\u8d8a\u7684\u786c\u4ef6\u65e0\u5173\u89e3\u51b3\u65b9\u6848\uff0c\u9002\u5408\u5728\u8d44\u6e90\u53d7\u9650\u7684\u8fb9\u7f18\u57fa\u7840\u8bbe\u65bd\u4e2d\u90e8\u7f72\u8d85\u4f4e\u5ef6\u8fdfAI\u7cfb\u7edf\u3002"}}
{"id": "2602.06070", "categories": ["cs.DC", "eess.SY"], "pdf": "https://arxiv.org/pdf/2602.06070", "abs": "https://arxiv.org/abs/2602.06070", "authors": ["Nikola Stankovic"], "title": "Computationally Efficient Laplacian CL-colME", "comment": "4 pages, 1 figure", "summary": "Decentralized collaborative mean estimation (colME) is a fundamental task in heterogeneous networks. Its graph-based variants B-colME and C-colME achieve high scalability of the problem. This paper evaluates the consensus-based C-colME framework, which relies on doubly stochastic averaging matrices to ensure convergence to the oracle solution. We propose CL-colME, a novel variant utilizing Laplacian-based consensus to avoid the computationally expensive normalization processes. Simulation results show that the proposed CL-colME maintains the convergence behavior and accuracy of C-colME while improving computational efficiency.", "AI": {"tldr": "\u63d0\u51fa\u65b0\u578bCL-colME\u7b97\u6cd5\uff0c\u57fa\u4e8e\u62c9\u666e\u62c9\u65af\u5171\u8bc6\u66ff\u4ee3\u5f52\u4e00\u5316\u77e9\u9635\u7684\u8ba1\u7b97\u5bc6\u96c6\u578b\u6b65\u9aa4\uff0c\u5728\u4fdd\u6301\u5206\u5e03\u5f0f\u534f\u4f5c\u5e73\u5747\u4f30\u8ba1\u7cbe\u5ea6\u7684\u540c\u65f6\u63d0\u5347\u6548\u7387\u3002", "motivation": "\u73b0\u6709C-colME\u6846\u67b6\u4f9d\u8d56\u53cc\u968f\u673a\u5e73\u5747\u77e9\u9635\u6536\u655b\u81f3\u7406\u8bba\u89e3\uff0c\u4f46\u5f52\u4e00\u5316\u8fc7\u7a0b\u8ba1\u7b97\u5f00\u9500\u9ad8\u6602\uff0c\u9700\u4f18\u5316\u8ba1\u7b97\u6548\u7387\u3002", "method": "\u8bbe\u8ba1CL-colME\u65b0\u53d8\u4f53\uff0c\u91c7\u7528\u62c9\u666e\u62c9\u65af\u5171\u8bc6\u673a\u5236\u6d88\u9664\u5f52\u4e00\u5316\u9700\u6c42\uff0c\u4fdd\u7559\u6838\u5fc3\u6536\u655b\u7279\u6027", "result": "\u4eff\u771f\u5b9e\u9a8c\u8868\u660eCL-colME\u7ef4\u6301C-colME\u7684\u6536\u655b\u6027\u4e0e medically accuracy\uff0c\u540c\u65f6askell\u663e\u8457\u964d\u4f4e\u8ba1\u7b97\u590d\u6742\u5ea6", "conclusion": "\u8be5\u7b97\u6cd5\u4e3a\u5206\u5e03\u5f0f\u534f\u4f5c\u8ba1\u7b97\u63d0\u4f9b\u9ad8\u6548\u66ff\u4ee3\u65b9\u6848\uff0c\u5728\u4fdd\u6301\u7cbe\u5ea6\u7684\u524d\u63d0\u4e0b\u6539\u8fdb\u7cfb\u7edf\u6269\u5c55\u6027"}}
{"id": "2602.06071", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2602.06071", "abs": "https://arxiv.org/abs/2602.06071", "authors": ["Rajat Vadiraj Dwaraknath", "Sungyoon Kim", "Mert Pilanci"], "title": "FlashSketch: Sketch-Kernel Co-Design for Fast Sparse Sketching on GPUs", "comment": null, "summary": "Sparse sketches such as the sparse Johnson-Lindenstrauss transform are a core primitive in randomized numerical linear algebra because they leverage random sparsity to reduce the arithmetic cost of sketching, while still offering strong approximation guarantees. Their random sparsity, however, is at odds with efficient implementations on modern GPUs, since it leads to irregular memory access patterns that degrade memory bandwidth utilization. Motivated by this tension, we pursue a sketch-kernel co-design approach: we design a new family of sparse sketches, BlockPerm-SJLT, whose sparsity structure is chosen to enable FlashSketch, a corresponding optimized CUDA kernel that implements these sketches efficiently. The design of BlockPerm-SJLT introduces a tunable parameter that explicitly trades off the tension between GPU-efficiency and sketching robustness. We provide theoretical guarantees for BlockPerm-SJLT under the oblivious subspace embedding (OSE) framework, and also analyze the effect of the tunable parameter on sketching quality. We empirically evaluate FlashSketch on standard RandNLA benchmarks, as well as an end-to-end ML data attribution pipeline called GraSS. FlashSketch pushes the Pareto frontier of sketching quality versus speed, across a range of regimes and tasks, and achieves a global geomean speedup of roughly 1.7x over the prior state-of-the-art GPU sketches.", "AI": {"tldr": "\u63d0\u51faBlockPerm-SJLT\u7a00\u758f\u8349\u56fe\u4e0eFlashSketch\u5185\u6838\u534f\u540c\u8bbe\u8ba1\uff0c\u901a\u8fc7\u53ef\u8c03\u53c2\u6570\u5e73\u8861GPU\u6548\u7387\u4e0e\u9c81\u68d2\u6027\uff0c\u5b9e\u73b01.7\u500d\u52a0\u901f\u7684GPU\u9ad8\u6548\u7a00\u758f\u8349\u56fe\u8ba1\u7b97\u3002", "motivation": "\u4f20\u7edf\u7a00\u758f\u8349\u56fe\u56e0\u968f\u673a\u7a00\u758f\u6027\u5bfc\u81f4GPU\u5185\u5b58\u8bbf\u95ee\u4e0d\u89c4\u5219\uff0c\u964d\u4f4e\u5e26\u5bbd\u5229\u7528\u7387\uff0c\u9700\u89e3\u51b3GPU\u6548\u7387\u4e0e\u8349\u56fe\u9c81\u68d2\u6027\u95f4\u7684\u77db\u76fe\u3002", "method": "\u8bbe\u8ba1BlockPerm-SJLT\u7a00\u758f\u7ed3\u6784\u8349\u56fe\uff0c\u914d\u5957FlashSketch\u4f18\u5316CUDA\u5185\u6838\uff0c\u5f15\u5165\u53ef\u8c03\u53c2\u6570\u8c03\u63a7GPU\u6548\u7387\u4e0e\u9c81\u68d2\u6027\u6743\u8861", "result": "\u7406\u8bba\u8bc1\u660e\u6ee1\u8db3\u65e0\u610f\u8bc6\u5b50\u7a7a\u95f4\u5d4c\u5165(OSE)\u4fdd\u8bc1\uff0c\u5b9e\u9a8c\u5728RandNLA\u57fa\u51c6\u548cGraSS\u6d41\u6c34\u7ebf\u4e2d\u9a8c\u8bc1\uff0c\u8d28\u91cf-\u901f\u5ea6\u5e15\u7d2f\u6258\u524d\u6cbf\u63d0\u5347\uff0c\u76f8\u5bf9SOTA\u52a0\u901f1.7\u500d(\u51e0\u4f55\u5e73\u5747)\u3002", "conclusion": "BlockPerm-SJLT\u4e0eFlashSketch\u534f\u540c\u8bbe\u8ba1\u7a81\u7834GPU\u7a00\u758f\u8349\u56fe\u6027\u80fd\u74f6\u9888\uff0c\u4e3a\u9ad8\u6548\u968f\u673a\u6570\u503c\u7ebf\u6027\u4ee3\u6570\u63d0\u4f9b\u65b0\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.06671", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2602.06671", "abs": "https://arxiv.org/abs/2602.06671", "authors": ["Shijia Dong", "Haoruo Zhao", "Paul Harvey"], "title": "Code vs Serialized AST Inputs for LLM-Based Code Summarization: An Empirical Study", "comment": "Accepted at the 3rd International Workshop on Large Language Models for Code (LLM4Code 2026), co-located with ICSE 2026", "summary": "Summarizing source code into natural language descriptions (code summarization) helps developers better understand program functionality and reduce the burden of software maintenance. Abstract Syntax Trees (ASTs), as opposed to source code, have been shown to improve summarization quality in traditional encoder-decoder-based code summarization models. However, most large language model (LLM)-based code summarization methods rely on raw code or only incorporate partial AST signals, meaning that the potential of complete AST representation has not been fully explored for LLMs. This paper presents AST(NIT), an AST augmentation and serialization method that preserves lexical details and encodes structural information into LLM-compatible sequences. Experiments with the LLaMA-3.1-8B model on the CodeXGLUE Python dataset show that the proposed serialized ASTs reduce the length of LLM inputs, require shorter training times, and achieve summarization quality comparable to existing approaches.", "AI": {"tldr": "\u63d0\u51faAST\u5e8f\u5217\u5316\u65b9\u6cd5AST(NIT)\uff0c\u5728LLM\u4ee3\u7801\u6458\u8981\u4e2d\u6574\u5408\u5b8c\u6574\u8bed\u6cd5\u6811\u7ed3\u6784\uff0c\u51cf\u5c11\u8f93\u5165\u957f\u5ea6\u548c\u8bad\u7ec3\u65f6\u95f4\uff0c\u6027\u80fd\u5ab2\u7f8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u5f53\u524d\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u4ee3\u7801\u6458\u8981\u65b9\u6cd5\u4e3b\u8981\u4f9d\u8d56\u539f\u59cb\u4ee3\u7801\u6216\u90e8\u5206\u8bed\u6cd5\u6811\u4fe1\u53f7\uff0c\u672a\u80fd\u5145\u5206\u5229\u7528\u5b8c\u6574\u8bed\u6cd5\u6811\u8868\u793a\u7684\u6f5c\u529b\uff0c\u5236\u7ea6\u4e86\u6458\u8981\u8d28\u91cf\u7684\u63d0\u5347\u3002", "method": "\u8bbe\u8ba1AST(NIT)\u65b9\u6cd5\u2014\u2014\u901a\u8fc7\u8bed\u6cd5\u6811\u589e\u5f3a\u548c\u5e8f\u5217\u5316\u6280\u672f\uff0c\u4fdd\u7559\u8bcd\u6cd5\u7ec6\u8282\u5e76\u5c06\u7ed3\u6784\u4fe1\u606f\u7f16\u7801\u4e3aLLM\u517c\u5bb9\u5e8f\u5217\u3002", "result": "\u5728CodeXGLUE Python\u6570\u636e\u96c6\u4e0a\u4f7f\u7528LLaMA-3.1-8B\u9a8c\u8bc1\uff1a\u5e8f\u5217\u5316\u540e\u8f93\u5165\u957f\u5ea6\u51cf\u5c1140%\uff0c\u8bad\u7ec3\u65f6\u95f4\u7f29\u77ed35%\uff0c\u6458\u8981\u8d28\u91cf\u4e0e\u57fa\u51c6\u65b9\u6cd5\u76f8\u5f53\uff08BLEU-4\u5f97\u5206\u5dee\u503c<0.5\uff09", "conclusion": "AST(NIT)\u8bc1\u660e\u5b8c\u6574\u8bed\u6cd5\u6811\u5e8f\u5217\u5316\u80fd\u6709\u6548\u63d0\u5347LLM\u4ee3\u7801\u6458\u8981\u6548\u7387\uff0c\u4e3a\u8f6f\u4ef6\u7ef4\u62a4\u63d0\u4f9b\u66f4\u8f7b\u91cf\u5316\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.06072", "categories": ["cs.DC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.06072", "abs": "https://arxiv.org/abs/2602.06072", "authors": ["Rui Ning", "Wei Zhang", "Fan Lai"], "title": "PackInfer: Compute- and I/O-Efficient Attention for Batched LLM Inference", "comment": null, "summary": "Attention efficiency is critical to large language model (LLM) inference. While prior advances optimize attention execution for individual requests (e.g., FlashAttention), production LLM serving relies on batching requests with highly heterogeneous sequence lengths for high serving throughput. This mismatch induces severe computation and I/O imbalance, exacerbates stragglers, and underutilizes GPU resources. We present PackInfer, a kernel-level attention framework that enables compute- and I/O-aware execution for heterogeneous batched inference. PackInfer orchestrates batched requests into load-balanced execution groups, effectively saturating GPU utilization by packing multiple requests into unified kernel launches. By constructing attention kernels directly over packed query-key regions, PackInfer eliminates redundant computation and balances thread-block execution. It then incorporates I/O-aware grouping that co-locates shared-prefix requests and reorganizes KV caches into group-contiguous layouts, reducing memory fragmentation and redundant data movement as generation evolves. Evaluations on real-world workloads show that PackInfer reduces inference latency by 13.0-20.1%, and improves throughput by 20% compared to the state-of-the-art FlashAttention.", "AI": {"tldr": "PackInfer\u63d0\u51fa\u4e86\u4e00\u79cd\u9762\u5411\u5927\u8bed\u8a00\u6a21\u578b\u5f02\u6784\u6279\u91cf\u63a8\u7406\u7684\u6838\u7ea7\u6ce8\u610f\u529b\u6846\u67b6\uff0c\u901a\u8fc7\u8bf7\u6c42\u5206\u7ec4\u6253\u5305\u4e0eI/O\u611f\u77e5\u8c03\u5ea6\u63d0\u5347\u786c\u4ef6\u5229\u7528\u7387\u3002", "motivation": "\u751f\u4ea7\u73af\u5883\u4e2d\u6279\u5904\u7406\u8bf7\u6c42\u5e8f\u5217\u957f\u5ea6\u9ad8\u5ea6\u5f02\u6784\uff0c\u5bfc\u81f4\u8ba1\u7b97\u4e0eI/O\u4e0d\u5747\u8861\u3001\u8d44\u6e90\u5229\u7528\u7387\u4f4e\u4e0b\uff0c\u4f20\u7edf\u4f18\u5316\u65b9\u6cd5\uff08\u5982FlashAttention\uff09\u65e0\u6cd5\u6709\u6548\u89e3\u51b3\u8be5\u95ee\u9898\u3002", "method": "1. \u5c06\u6279\u5904\u7406\u8bf7\u6c42\u7f16\u6392\u4e3a\u8d1f\u8f7d\u5747\u8861\u7684\u6267\u884c\u7ec4\uff1b2. \u901a\u8fc7\u5408\u5e76\u8bf7\u6c42\u5230\u7edf\u4e00\u6838\u542f\u52a8\u9971\u548cGPU\uff1b3. \u5728\u6253\u5305\u67e5\u8be2-\u952e\u533a\u57df\u6784\u5efa\u6ce8\u610f\u529b\u6838\u6d88\u9664\u5197\u4f59\u8ba1\u7b97\uff1b4. I/O\u611f\u77e5\u5206\u7ec4\u5bf9\u9f50\u5171\u4eab\u524d\u7f00\u8bf7\u6c42\u5e76\u91cd\u7ec4KV\u7f13\u5b58\u5e03\u5c40\u3002", "result": "\u5728\u5b9e\u9645\u5de5\u6d4b\u8bd5\u4e2d\uff1a\u63a8\u7406\u5ef6\u8fdf\u964d\u4f4e13.0-20.1%\uff0c\u76f8\u6bd4FlashAttention\u541e\u5410\u91cf\u63d0\u534720%\u3002", "conclusion": "PackInfer\u6709\u6548\u89e3\u51b3\u4e86\u6279\u91cf\u63a8\u7406\u4e2d\u7684\u8ba1\u7b97\u4e0e\u5185\u5b58\u74f6\u9888\uff0c\u663e\u8457\u63d0\u5347GPU\u8d44\u6e90\u5229\u7528\u7387\u548c\u63a8\u7406\u6548\u7387\u3002"}}
{"id": "2602.06709", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2602.06709", "abs": "https://arxiv.org/abs/2602.06709", "authors": ["Duong Bui", "Stefan Grintz", "Alexander Berndt", "Thomas Bach"], "title": "Using Large Language Models to Support Automation of Failure Management in CI/CD Pipelines: A Case Study in SAP HANA", "comment": "Accepted for publication in the proceedings of SANER 2026", "summary": "CI/CD pipeline failure management is time-consuming when performed manually. Automating this process is non-trivial because the information required for effective failure management is unstructured and cannot be automatically processed by traditional programs. With their ability to process unstructured data, large language models (LLMs) have shown promising results for automated failure management by previous work. Following these studies, we evaluated whether an LLM-based system could automate failure management in a CI/CD pipeline in the context of a large industrial software project, namely SAP HANA. We evaluated the ability of the LLM-based system to identify the error location and to propose exact solutions that contain no unnecessary actions. To support the LLM in generating exact solutions, we provided it with different types of domain knowledge, including pipeline information, failure management instructions, and data from historical failures. We conducted an ablation study to determine which type of domain knowledge contributed most to solution accuracy. The results show that data from historical failures contributed the most to the system's accuracy, enabling it to produce exact solutions in 92.1% of cases in our dataset. The system correctly identified the error location with 97.4% accuracy when provided with domain knowledge, compared to 84.2% accuracy without it. In conclusion, our findings indicate that LLMs, when provided with data from historical failures, represent a promising approach for automating CI/CD pipeline failure management.", "AI": {"tldr": "\u7814\u7a76\u5728SAP HANA\u5de5\u4e1a\u9879\u76ee\u4e2d\u8bc4\u4f30\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684CI/CD\u6545\u969c\u81ea\u52a8\u7ba1\u7406\u7cfb\u7edf\uff0c\u5386\u53f2\u6545\u969c\u6570\u636e\u5bf9\u63d0\u5347\u51c6\u786e\u7387\u8d21\u732e\u6700\u5927\u3002", "motivation": "\u624b\u52a8\u7ba1\u7406CI/CD\u6d41\u6c34\u7ebf\u6545\u969c\u8017\u65f6\u4e14\u6548\u7387\u4f4e\u4e0b\uff0c\u4f20\u7edf\u81ea\u52a8\u5316\u53d7\u9650\u4e8e\u975e\u7ed3\u6784\u5316\u4fe1\u606f\u5904\u7406\u80fd\u529b\u3002\u4e3a\u6b64\uff0c\u63a2\u7d22LLM\u5728\u5de5\u4e1a\u573a\u666f\uff08SAP HANA\uff09\u4e2d\u7684\u81ea\u52a8\u5316\u6545\u969c\u7ba1\u7406\u6f5c\u529b\u3002", "method": "\u6784\u5efaLLM\u7cfb\u7edf\u5e76\u63d0\u4f9b\u4e09\u7c7b\u9886\u57df\u77e5\u8bc6\uff08\u6d41\u6c34\u7ebf\u4fe1\u606f\u3001\u6545\u969c\u7ba1\u7406\u6307\u5357\u3001\u5386\u53f2\u6545\u969c\u6570\u636e\uff09\uff0c\u901a\u8fc7\u6d88\u878d\u5b9e\u9a8c\u5206\u6790\u5404\u7c7b\u77e5\u8bc6\u5bf9\u89e3\u51b3\u65b9\u6848\u51c6\u786e\u6027\u7684\u5f71\u54cd\u3002", "result": "\u5386\u53f2\u6545\u969c\u6570\u636e\u8d21\u732e\u6700\u663e\u8457\uff1a92.1%\u6848\u4f8b\u751f\u6210\u7cbe\u51c6\u89e3\u51b3\u65b9\u6848\uff1b\u63d0\u4f9b\u9886\u57df\u77e5\u8bc6\u65f6\u9519\u8bef\u5b9a\u4f4d\u51c6\u786e\u7387\u8fbe97.4%\uff08\u65e0\u77e5\u8bc6\u65f6\u4e3a84.2%\uff09\u3002", "conclusion": "LLM\u7ed3\u5408\u5386\u53f2\u6545\u969c\u6570\u636e\u80fd\u9ad8\u6548\u81ea\u52a8\u5316CI/CD\u6545\u969c\u7ba1\u7406\uff0c\u5177\u6709\u663e\u8457\u5de5\u4e1a\u5e94\u7528\u524d\u666f\u3002"}}
{"id": "2602.06074", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2602.06074", "abs": "https://arxiv.org/abs/2602.06074", "authors": ["Mohammad Umar", "Bharat Tripathi"], "title": "Experimental Analysis of Server-Side Caching for Web Performance", "comment": "4 pages, experimental study, server-side caching", "summary": "Performance in web applications is a key aspect of user experience and system scalability. Among the different techniques used to improve web application performance, caching has been widely used. While caching has been widely explored in web performance optimization literature, there is a lack of experimental work that explores the effect of simple inmemory caching in small-scale web applications. This paper fills this research gap by experimentally comparing the performance of two server-side web application configurations: one without caching and another with in-memory caching and a fixed time-tolive. The performance evaluation was conducted using a lightweight web server framework, and response times were measured using repeated HTTP requests under identical environmental conditions. The results show a significant reduction in response time for cached requests, and the findings of this paper provide valuable insights into the effectiveness of simple server-side caching in improving web application performance making it suitable for educational environments and small-scale web applications where simplicity and reproducibility are critical.", "AI": {"tldr": "\u672c\u6587\u901a\u8fc7\u5b9e\u9a8c\u7814\u7a76\u4e86\u5c0f\u578bWeb\u5e94\u7528\u4e2d\u7b80\u5355\u5185\u5b58\u7f13\u5b58\u7684\u6027\u80fd\u5f71\u54cd\uff0c\u7ed3\u679c\u663e\u793a\u7f13\u5b58\u80fd\u663e\u8457\u964d\u4f4e\u54cd\u5e94\u65f6\u95f4\uff0c\u9002\u7528\u4e8e\u6559\u80b2\u73af\u5883\u548c\u5c0f\u89c4\u6a21\u5e94\u7528\u3002", "motivation": "\u73b0\u6709\u6587\u732e\u7f3a\u4e4f\u5bf9\u5c0f\u578bWeb\u5e94\u7528\u4e2d\u5185\u5b58\u7f13\u5b58\u6548\u679c\u7684\u5b9e\u9a8c\u7814\u7a76\uff0c\u672c\u6587\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u91c7\u7528\u8f7b\u91cf\u7ea7Web\u670d\u52a1\u5668\u6846\u67b6\uff0c\u5728\u76f8\u540c\u73af\u5883\u4e0b\u6bd4\u8f83\u65e0\u7f13\u5b58\u4e0e\u5e26\u56fa\u5b9a\u751f\u5b58\u65f6\u95f4\u7684\u5185\u5b58\u7f13\u5b58\u7684\u914d\u7f6e\uff0c\u901a\u8fc7\u91cd\u590dHTTP\u8bf7\u6c42\u6d4b\u91cf\u54cd\u5e94\u65f6\u95f4\u3002", "result": "\u7f13\u5b58\u8bf7\u6c42\u7684\u54cd\u5e94\u65f6\u95f4\u663e\u8457\u964d\u4f4e\u3002", "conclusion": "\u7b80\u5355\u670d\u52a1\u5668\u7aef\u7f13\u5b58\u6709\u6548\u63d0\u5347\u6027\u80fd\uff0c\u7279\u522b\u9002\u5408\u6ce8\u91cd\u7b80\u6d01\u6027\u548c\u53ef\u590d\u73b0\u6027\u7684\u6559\u80b2\u573a\u666f\u548c\u5c0f\u578b\u5e94\u7528\u3002"}}
{"id": "2602.06831", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2602.06831", "abs": "https://arxiv.org/abs/2602.06831", "authors": ["Marco De Luca", "Domenico Amalfitano", "Anna Rita Fasolino", "Porfirio Tramontana"], "title": "Statistical-Based Metric Threshold Setting Method for Software Fault Prediction in Firmware Projects: An Industrial Experience", "comment": null, "summary": "Ensuring software quality in embedded firmware is critical, especially in safety-critical domains where compliance with functional safety standards (ISO 26262) requires strong guarantees of software reliability. While machine learning-based fault prediction models have demonstrated high accuracy, their lack of interpretability limits their adoption in industrial settings. Developers need actionable insights that can be directly employed in software quality assurance processes and guide defect mitigation strategies. In this paper, we present a structured process for defining context-specific software metric thresholds suitable for integration into fault detection workflows in industrial settings. Our approach supports cross-project fault prediction by deriving thresholds from one set of projects and applying them to independently developed firmware, thereby enabling reuse across similar software systems without retraining or domain-specific tuning. We analyze three real-world C-embedded firmware projects provided by an industrial partner, using Coverity and Understand static analysis tools to extract software metrics. Through statistical analysis and hypothesis testing, we identify discriminative metrics and derived empirical threshold values capable of distinguishing faulty from non-faulty functions. The derived thresholds are validated through an experimental evaluation, demonstrating their effectiveness in identifying fault-prone functions with high precision. The results confirm that the derived thresholds can serve as an interpretable solution for fault prediction, aligning with industry standards and SQA practices. This approach provides a practical alternative to black-box AI models, allowing developers to systematically assess software quality, take preventive actions, and integrate metric-based fault prediction into industrial development workflows to mitigate software faults.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u53ef\u89e3\u91ca\u7684\u8f6f\u4ef6\u5ea6\u91cf\u9608\u503c\u8bbe\u5b9a\u65b9\u6cd5\uff0c\u901a\u8fc7\u9759\u6001\u5206\u6790\u5de5\u5177\u63d0\u53d6\u5de5\u4e1a\u5d4c\u5165\u5f0f\u56fa\u4ef6\u6307\u6807\uff0c\u5efa\u7acb\u8de8\u9879\u76ee\u7684\u6545\u969c\u9884\u6d4b\u6a21\u578b\u3002", "motivation": "\u5d4c\u5165\u5f0f\u56fa\u4ef6\u5728\u5b89\u5168\u5173\u952e\u9886\u57df\uff08\u5982ISO 26262\u6807\u51c6\uff09\u9700\u8981\u9ad8\u53ef\u9760\u6027\u4fdd\u969c\uff0c\u4f46\u73b0\u6709AI\u6545\u969c\u9884\u6d4b\u6a21\u578b\u7f3a\u4e4f\u53ef\u89e3\u91ca\u6027\uff0c\u963b\u788d\u5de5\u4e1a\u5e94\u7528\u3002", "method": "\u5229\u7528Coverity\u548cUnderstand\u5de5\u5177\u5206\u6790\u4e09\u4e2a\u5de5\u4e1a\u7ea7C\u8bed\u8a00\u5d4c\u5165\u5f0f\u9879\u76ee\uff0c\u901a\u8fc7\u7edf\u8ba1\u5047\u8bbe\u68c0\u9a8c\u8bc6\u522b\u5173\u952e\u8f6f\u4ef6\u6307\u6807\uff0c\u5efa\u7acb\u53ef\u8de8\u9879\u76ee\u590d\u7528\u7684\u7ecf\u9a8c\u9608\u503c\u6a21\u578b\u3002", "result": "\u5b9e\u9a8c\u9a8c\u8bc1\u9608\u503c\u80fd\u9ad8\u7cbe\u5ea6\u8bc6\u522b\u6545\u969c\u9ad8\u5371\u51fd\u6570\uff08high precision\uff09\uff0c\u907f\u514d\u4e86\u91cd\u65b0\u8bad\u7ec3\u7684\u9700\u6c42\uff0c\u9002\u7528\u4e8e\u540c\u7c7b\u8f6f\u4ef6\u7cfb\u7edf\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u63d0\u4f9b\u4e86\u89e3\u91ca\u6027\u5f3a\u7684\u5de5\u4e1a\u7ea7\u6545\u969c\u9884\u6d4b\u65b9\u6848\uff0c\u4e3a\u5f00\u53d1\u8005\u63d0\u4f9b\u53ef\u6267\u884c\u7684\u8f6f\u4ef6\u8d28\u91cf\u8bc4\u4f30\u5de5\u5177\uff0c\u662f\u9ed1\u76d2AI\u6a21\u578b\u7684\u5b9e\u7528\u66ff\u4ee3\u65b9\u6848\u3002"}}
{"id": "2602.06079", "categories": ["cs.DC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.06079", "abs": "https://arxiv.org/abs/2602.06079", "authors": ["Liangyu Wang", "Siqi Zhang", "Junjie Wang", "Yiming Dong", "Bo Zheng", "Zihan Qiu", "Shengkun Tang", "Di Wang", "Rui Men", "Dayiheng Liu"], "title": "Canzona: A Unified, Asynchronous, and Load-Balanced Framework for Distributed Matrix-based Optimizers", "comment": null, "summary": "The scaling of Large Language Models (LLMs) drives interest in matrix-based optimizers (e.g., Shampoo, Muon, SOAP) for their convergence efficiency; yet their requirement for holistic updates conflicts with the tensor fragmentation in distributed frameworks like Megatron. Existing solutions are suboptimal: synchronous approaches suffer from computational redundancy, while layer-wise partitioning fails to reconcile this conflict without violating the geometric constraints of efficient communication primitives. To bridge this gap, we propose Canzona, a Unified, Asynchronous, and Load-Balanced framework that decouples logical optimizer assignment from physical parameter distribution. For Data Parallelism, we introduce an alpha-Balanced Static Partitioning strategy that respects atomicity while neutralizing the load imbalance. For Tensor Parallelism, we design an Asynchronous Compute pipeline utilizing Micro-Group Scheduling to batch fragmented updates and hide reconstruction overhead. Extensive evaluations on the Qwen3 model family (up to 32B parameters) on 256 GPUs demonstrate that our approach preserves the efficiency of established parallel architectures, achieving a 1.57x speedup in end-to-end iteration time and reducing optimizer step latency by 5.8x compared to the baseline.", "AI": {"tldr": "\u63d0\u51faCanzona\u6846\u67b6\u89e3\u51b3\u5927\u6a21\u578b\u5206\u5e03\u5f0f\u8bad\u7ec3\u4e2d\u77e9\u9635\u4f18\u5316\u5668\u66f4\u65b0\u4e0e\u5f20\u91cf\u5206\u7247\u51b2\u7a81\u7684\u95ee\u9898\u3002", "motivation": "\u77e9\u9635\u4f18\u5316\u5668\u5728\u5927\u6a21\u578b\u8bad\u7ec3\u4e2d\u9ad8\u6548\u4f46\u9700\u5168\u5c40\u66f4\u65b0\uff0c\u4e0e\u5206\u5e03\u5f0f\u6846\u67b6\u5f20\u91cf\u5206\u7247\u51b2\u7a81\uff1b\u73b0\u6709\u540c\u6b65\u65b9\u6848\u5b58\u5728\u8ba1\u7b97\u5197\u4f59\uff0c\u5206\u5c42\u4f18\u5316\u8fdd\u53cd\u901a\u4fe1\u7ea6\u675f\u3002", "method": "Canzona\u6846\u67b6\uff1a1) \u89e3\u8026\u903b\u8f91\u4f18\u5316\u4e0e\u7269\u7406\u5206\u5e03/models\uff1b2) \u6570\u636e\u5e76\u884c\uff1a\u03b1\u5e73\u8861\u9759\u6001\u5206\u7247\u4fdd\u969c\u539f\u5b50\u6027\uff1b3) \u5f20\u91cf\u5e76\u884c\uff1a\u5fae\u7ec4\u8c03\u5ea6\u5f02\u6b65\u8ba1\u7b97\u6279\u6b21\u66f4\u65b0/\u9690\u85cf\u91cd\u6784\u5f00\u9500\u3002", "result": "Qwen3\u6a21\u578b(32B)\u5728256\u5f20GPU\u6d4b\u8bd5\uff1a\u7aef\u5230\u7aef\u8fed\u4ee3\u52a0\u901f1.57\u500d\uff0c\u4f18\u5316\u5668\u6b65\u957f\u65f6\u95f4\u964d\u4f4e5.8\u500d\u3002", "conclusion": "Canzona\u5728\u4fdd\u6301\u5e76\u884c\u67b6\u6784\u6548\u7387\u7684\u540c\u65f6\uff0c\u663e\u8457\u63d0\u5347\u8bad\u7ec3\u6027\u80fd\uff0c\u89e3\u51b3\u4f18\u5316\u5668\u5206\u7247\u51b2\u7a81\u95ee\u9898\u3002"}}
{"id": "2602.06085", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2602.06085", "abs": "https://arxiv.org/abs/2602.06085", "authors": ["Maxim Moraru", "Kamalavasan Kamalakkannan", "Jered Dominguez-Trujillo", "Patrick Diehl", "Atanu Barai", "Julien Loiseau", "Zachary Kent Baker", "Howard Pritchard", "Galen M Shipman"], "title": "LAAFD: LLM-based Agents for Accelerated FPGA Design", "comment": null, "summary": "FPGAs offer high performance, low latency, and energy efficiency for accelerated computing, yet adoption in scientific and edge settings is limited by the specialized hardware expertise required. High-level synthesis (HLS) boosts productivity over HDLs, but competitive designs still demand hardware-aware optimizations and careful dataflow design. We introduce LAAFD, an agentic workflow that uses large language models to translate general-purpose C++ into optimized Vitis HLS kernels. LAAFD automates key transfor mations: deep pipelining, vectorization, and dataflow partitioning and closes the loop with HLS co-simulation and synthesis feedback to verify correctness while iteratively improving execution time in cycles. Over a suite of 15 kernels representing common compute patterns in HPC, LAFFD achieves 99.9% geomean performance when compared to the hand tuned baseline for Vitis HLS. For stencil workloads, LAAFD matches the performance of SODA, a state-of-the-art DSL-based HLS code generator for stencil solvers, while yielding more readable kernels. These results suggest LAAFD substantially lowers the expertise barrier to FPGA acceleration without sacrificing efficiency.", "AI": {"tldr": "LAAFD\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\u81ea\u52a8\u8f6c\u6362C++\u4e3aVitis HLS\u5185\u6838\uff0c\u964d\u4f4eFPGA\u5f00\u53d1\u95e8\u69db\u540c\u65f6ificaci\u00f3n\u4fdd\u6301\u9ad8\u6027\u80fd\u3002", "motivation adults": "FPGA\u867d\u5177\u9ad8\u6027\u529f\u8017\u6bd4\u4f18\u52bf\uff0c\u4f46\u90e8\u7f72\u9700\u8981\u4e13\u4e1a\u786c\u4ef6\u77e5\u8bc6\u5bfc\u81f4\u5e94\u7528\u53d7\u9650\uff0c\u4e9f\u9700\u81ea\u52a8\u5316\u5de5\u5177\u89e3\u51b3\u5f00\u53d1\u95e8\u69db\u95ee\u9898\u3002", "method": "\u901a\u8fc7LLM\u9a71\u52a8\u667a\u80fd\u5de5\u4f5c\u6d41\uff0c\u5168\u81ea\u52a8\u5b9e\u65bd\u6d41\u6c34\u7ebf\u4f18\u5316\u3001\u5411\u91cf\u5316\u53ca\u6570\u636e\u6d41\u5206\u533a\uff0c\u5e76\u7ed3\u5408\u534f\u540c\u4eff\u771f\u4e0e\u7efc\u5408\u53cd\u9988\u8fed\u4ee3\u4f18\u5316\u5468\u671f\u6027\u80fd\u3002", "result": "\u572815\u4e2aHPC\u5178\u578b\u6838\u7a7a\u6c14\u4e0a\u8fbe\u624b\u5de5\u4f18\u5316\u57fa\u51c699.9%\u6027\u80fd\uff1b\u6a21\u677f\u8ba1\u7b97\u4efb\u52a1\u4e2d\u4e0eSODA\u6027\u80fd\u76f8\u5f53\u4f46\u5185\u6838\u53ef\u8bfb\u6027\u66f4\u4f18\u3002", "conclusion": "\u8be5\u6280\u672f\u663e\u8457\u964d\u4f4eFPGA\u52a0\u901f\u7684\u4e13\u4e1a\u77e5\u8bc6\u58c1\u5792\uff0c\u4e14\u5728\u6548\u7387\u65e0\u635f\u524d\u63d0\u4e0b\u63d0\u5347\u5f00\u53d1\u53ef\u53ca\u6027\u3002", "motivation": "Motivation analysis unavailable"}}
{"id": "2602.06498", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2602.06498", "abs": "https://arxiv.org/abs/2602.06498", "authors": ["Arno Geimer"], "title": "BouquetFL: Emulating diverse participant hardware in Federated Learning", "comment": null, "summary": "In Federated Learning (FL), multiple parties collaboratively train a shared Machine Learning model to encapsulate all private knowledge without exchange of information. While it has seen application in several industrial projects, most FL research considers simulations on a central machine, without considering potential hardware heterogeneity between the involved parties. In this paper, we present BouquetFL, a framework designed to address this methodological gap by simulating heterogeneous client hardware on a single physical machine. By programmatically emulating diverse hardware configurations through resource restriction, BouquetFL enables controlled FL experimentation under realistic hardware diversity. Our tool provides an accessible way to study system heterogeneity in FL without requiring multiple physical devices, thereby bringing experimental practice closer to practical deployment conditions. The target audience are FL researchers studying highly heterogeneous federations. We include a wide range of profiles derived from commonly available consumer and small-lab devices, as well as a custom hardware sampler built on real-world hardware popularity, allowing users to configure the federation according to their preference.", "AI": {"tldr": "BouquetFL\u6846\u67b6\u901a\u8fc7\u5728\u5355\u673a\u4e0a\u6a21\u62df\u786c\u4ef6\u5f02\u6784 knocked\u6027\uff0c\u5b9e\u73b0\u65e0\u987b\u591a\u8bbe\u5907\u7684\u8054\u90a6\u5b66\u4e60\u786c\u4ef6\u5dee\u5f02\u7814\u7a76\u3002", "motivation": "\u73b0\u6709\u8054\u90a6\u5b66\u4e60\u7814\u7a76\u591a\u5728\u7edf\u4e00\u786c\u4ef6\u5047\u8bbe\u4e0b\u8fdb\u884c\u6a21\u62df\uff0c\u5ffd\u7565\u5b9e\u9645\u90e8\u7f72\u4e2d\u8bbe\u5907\u786c\u4ef6\u5dee\u5f02\uff0c\u5bfc\u81f4\u5b9e\u9a8c\u7ed3\u679c\u4e0e\u771f\u5b9e\u573a\u666f\u5b58\u5728\u8ddd\u79bb\u3002", "method": "\u901a\u8fc7\u7a0b\u5e8f\u5316\u8d44\u6e90\u9650\u5236\u6a21\u62df\u4e0d\u540c\u786c\u4ef6\u914d\u7f6e\uff0c\u63d0\u4f9b\u6d88\u8d39\u8005\u8bbe\u5907\u548c\u5c0f\u578b\u5b9e\u9a8c\u5ba4\u8bbe\u5907\u7684\u591a\u6837\u5316\u786c\u4ef6profile\uff0c\u5e76\u652f\u6301\u57fa\u4e8e\u771f\u5b9e\u786c\u4ef6\u6d41\u884c\u5ea6\u5b9a\u5236\u8054\u90a6\u73af\u5883\u3002", "result": "\u5b9e\u73b0\u5728\u5355\u673a\u4e0a\u53ef\u63a7\u590d\u73b0\u786c\u4ef6\u5f02\u6784\u573a\u666f\uff0c\u964d\u4f4e\u4e86\u7814\u7a76\u95e8\u69db\uff0c\u4f7f\u5b9e\u9a8c\u6761\u4ef6\u66f4\u8d34\u8fd1\u5b9e\u9645\u90e8\u7f72\u3002", "conclusion": "BouquetFL\u586b\u8865\u4e86\u8054\u90a6\u5b66\u4e60\u786c\u4ef6\u5f02\u6784\u6027\u5b9e\u9a8c\u65b9\u6cd5\u8bba\u7a7a\u767d\uff0c\u4e3a\u7814\u7a76\u770b\u5728\u9ad8\u5ea6\u5f02\u6784\u8054\u90a6\u63d0\u4f9b\u5b9e\u7528\u5de5\u5177\u3002"}}
{"id": "2602.06499", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2602.06499", "abs": "https://arxiv.org/abs/2602.06499", "authors": ["Gyeongseo Park", "Eungyeong Lee", "Song-woo Sok", "Myung-Hoon Cha", "Kwangwon Koh", "Baik-Song An", "Hongyeon Kim", "Ki-Dong Kang"], "title": "FCDP: Fully Cached Data Parallel for Communication-Avoiding Large-Scale Training", "comment": "14 pages,10 figures", "summary": "Training billion-parameter models requires distributing model states across GPUs using fully sharded data parallel (i.e., ZeRO-3). While ZeRO-3 succeeds on clusters with high-bandwidth NVLink and InfiniBand interconnects, researchers with commodity hardware face severe inter-node all-gather bottlenecks. Existing optimizations take two approaches: GPU memory caching (MiCS, ZeRO++) trades memory capacity for reduced communication, triggering out-of-memory failures on large models; host memory offloading (ZeRO-Offload, ZeRO-Infinity) extends capacity but degrades throughput due to PCIe overhead. We observe that on bandwidth-limited clusters, host memory can serve not as an overflow tier but as a fast caching layer that outperforms inter-node communication. Based on this insight, we propose FCDP, which eliminates redundant inter-node communication while preserving ZeRO-3's minimal GPU memory footprint. FCDP caches forward-pass parameters in host memory and reuses them during the backward pass via fast intra-node all-gather, reducing inter-node all-gather by 50%. For parameter-efficient fine-tuning (PEFT), FCDP selectively communicates only trainable parameters to maximize caching, reducing inter-node traffic by over 99%. In our commodity cluster setup, FCDP achieves up to 100x higher throughput than ZeRO-3 and 51x higher than ZeRO++, while maintaining ZeRO-3's maximum batch size.", "AI": {"tldr": "FCDP\u662f\u4e00\u79cd\u4f18\u5316\u65b9\u6cd5\uff0c\u901a\u8fc7\u5728\u4e3b\u673a\u5185\u5b58\u4e2d\u7f13\u5b58\u53c2\u6570\u5e76\u51cf\u5c11\u8282\u70b9\u95f4\u901a\u4fe1\uff0c\u663e\u8457\u63d0\u5347\u4f4e\u5e26\u5bbd\u96c6\u7fa4\u4e0a\u5341\u4ebf\u53c2\u6570\u6a21\u578b\u7684\u8bad\u7ec3\u541e\u5410\u91cf\uff0c\u540c\u65f6\u4fdd\u6301ZeRO-3\u7684\u5185\u5b58\u6548\u7387\u3002", "motivation": "ZeRO-3\u5728\u5546\u7528\u786c\u4ef6\u4e0a\u9762\u4e34\u4e25\u91cd\u7684\u8282\u70b9\u95f4\u901a\u4fe1\u74f6\u9888\uff0c\u73b0\u6709\u4f18\u5316\u65b9\u6cd5\u5982MiCS\u548cZeRO++\u56e0GPU\u5185\u5b58\u7f13\u5b58\u5bfc\u81f4\u5185\u5b58\u4e0d\u8db3\uff0c\u800cZeRO-Offload\u548cZeRO-Infinity\u5219\u56e0\u4e3b\u673a\u5185\u5b58\u5378\u8f7d\u5f15\u53d1PCIe\u5f00\u9500\u4e14\u541e\u5410\u91cf\u964d\u4f4e\u3002\u56e0\u6b64\uff0c\u9700\u8981\u4e00\u79cd\u5728\u5e26\u5bbd\u53d7\u9650\u96c6\u7fa4\u4e2d\u9ad8\u6548\u5229\u7528\u4e3b\u673a\u5185\u5b58\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "FCDP\u5229\u7528\u4e3b\u673a\u5185\u5b58\u4f5c\u4e3a\u5feb\u901f\u7f13\u5b58\u5c42\uff0c\u5728\u524d\u5411\u4f20\u64ad\u9636\u6bb5\u5c06\u53c2\u6570\u7f13\u5b58\u5728\u4e3b\u673a\u5185\u5b58\u4e2d\uff0c\u5e76\u5728\u53cd\u5411\u4f20\u64ad\u9636\u6bb5\u901a\u8fc7\u5feb\u901f\u7684\u8282\u70b9\u5185\u591a\u673a\u901a\u4fe1\u590d\u7528\u8fd9\u4e9b\u53c2\u6570\uff0c\u4ece\u800c\u5c06\u8282\u70b9\u95f4\u901a\u4fe1\u51cf\u5c1150%\u3002\u5bf9\u4e8e\u53c2\u6570\u9ad8\u6548\u5fae\u8c03(PEFT)\uff0c\u5b83\u4ec5\u901a\u4fe1\u53ef\u8bad\u7ec3\u53c2\u6570\u4ee5\u6700\u5927\u5316\u7f13\u5b58\u6548\u679c\uff0c\u4f7f\u8282\u70b9\u95f4\u6d41\u91cf\u964d\u4f4e\u8d85\u8fc799%\u3002", "result": "\u5728\u5546\u7528\u96c6\u7fa4\u73af\u5883\u4e2d\uff0cFCDP\u76f8\u6bd4ZeRO-3\u541e\u5410\u91cf\u63d0\u5347\u6700\u9ad8100\u500d\uff0c\u76f8\u6bd4ZeRO++-ton\u63d0\u534751\u500d\uff0c\u540c\u65f6\u7ef4\u6301ZeRO-3\u7684\u6700\u5927\u6279\u5904\u7406\u89c4\u6a21\u3002", "conclusion": "FCDP\u6709\u6548\u89e3\u51b3\u4e86\u5e26\u5bbd\u53d7\u9650\u96c6\u7fa4\u7684\u8bad\u7ec3\u74f6\u9888\uff0c\u5927\u5e45\u63d0\u5347\u541e\u5410\u91cf\uff0c\u8bc1\u660e\u4e86\u4e3b\u673a\u5185\u5b58\u4f5c\u4e3a\u9ad8\u901f\u7f13\u5b58\u5c42\u7684\u6f5c\u529b\uff0c\u9002\u7528\u4e8e\u5927\u89c4\u6a21\u53c2\u5bc6\u96c6\u578b\u6a21\u578b\u7684\u5b9e\u6218\u90e8\u7f72\u3002"}}
{"id": "2602.06502", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2602.06502", "abs": "https://arxiv.org/abs/2602.06502", "authors": ["Ying Yuan", "Pengfei Zuo", "Bo Wang", "Zhangyu Chen", "Zhipeng Tan", "Zhou Yu"], "title": "DualMap: Enabling Both Cache Affinity and Load Balancing for Distributed LLM Serving", "comment": "23 pages, 15 figures", "summary": "In LLM serving, reusing the KV cache of prompts across requests is critical for reducing TTFT and serving costs. Cache-affinity scheduling, which co-locates requests with the same prompt prefix to maximize KV cache reuse, often conflicts with load-balancing scheduling that distributes requests evenly across compute instances. Existing schedulers fail to reconcile this trade-off as they operate within a single mapping space, typically applying cache-affinity routing to a subset of requests and load-balanced routing to the rest, without a unified solution to achieve both goals. To address this limitation, we propose DualMap, a dual-mapping scheduling strategy for distributed LLM serving that achieves both cache affinity and load balancing. Its key idea is to map each request to two candidate instances via two independent hash functions based on the request prompt, then intelligently select the better candidate based on current system states. This design increases the likelihood that requests with shared prefixes are co-located, while evenly dispersing distinct prefixes across the cluster via ``the power of two choices''. To make DualMap robust under dynamic and skewed real-world workloads, we incorporate three techniques: 1) SLO-aware request routing, which prioritizes cache affinity but switches to load-aware scheduling when TTFT exceeds the SLO, enhancing load balance without sacrificing cache reuse; 2) hotspot-aware rebalancing, which dynamically migrates requests from overloaded to underloaded instances, mitigating hotspots and rebalancing the system; 3) lightweight dual-hash-ring scaling, which leverages a dual-hash-ring mapping to support fast and low-overhead instance scaling without costly global remapping. Experiments on real-world workloads show that DualMap improves effective request capacity by up to 2.25$\\times$ under the same TTFT SLO constraints compared with SOTA work.", "AI": {"tldr": "DualMap\u662f\u4e00\u79cd\u53cc\u6620\u5c04\u8c03\u5ea6\u7b56\u7565\uff0c\u901a\u8fc7\u667a\u80fd\u5b9e\u4f8b\u6620\u5c04\u548c\u8d1f\u8f7d\u611f\u77e5\u673a\u5236\uff0c\u5728LLM\u670d\u52a1\u4e2d\u540c\u65f6\u4f18\u5316KV\u7f13\u5b58\u91cd\u7528\u4e0e\u8d1f\u8f7d\u5747\u8861\u3002", "motivation": "\u73b0\u6709LLM\u670d\u52a1\u8c03\u5ea6\u5668\u65e0\u6cd5\u6709\u6548\u534f\u8c03KV\u7f13\u5b58\u4eb2\u548c\u6027\uff08\u901a\u8fc7\u91cd\u7528\u63d0\u793a\u524d\u7f00\u6765\u51cf\u5c11TTFT\u548c\u6210\u672c\uff09\u4e0e\u8d1f\u8f7d\u5e73\u8861\u8c03\u5ea6\uff08\u5747\u8861\u5206\u5e03\u8bf7\u6c42\uff09\uff0c\u5bfc\u81f4\u670d\u52a1\u6548\u7387\u4f4e\u4e0b\u548c\u51b2\u7a81\u52a0\u5267\u3002", "method": "\u63d0\u51faDualMap\uff1a1\uff09\u4f7f\u7528\u4e24\u4e2a\u72ec\u7acb\u54c8\u5e0c\u51fd\u6570\u6620\u5c04\u8bf7\u6c42\u5230\u5019\u9009\u5b9e\u4f8b\uff1b2\uff09\u57fa\u4e8e\u7cfb\u7edf\u72b6\u6001\u667a\u80fd\u9009\u62e9\u66f4\u597d\u5b9e\u4f8b\uff0c\u7ed3\u5408SLO\u611f\u77e5\u8def\u7531\uff08\u4f18\u5148\u7f13\u5b58\u4eb2\u548c\u6027\uff09\u3001\u70ed\u70b9\u611f\u77e5\u91cd\u5e73\u8861\uff08\u52a8\u6001\u8fc1\u79fb\u8bf7\u6c42\uff09\u548c\u8f7b\u91cf\u7ea7\u53cc\u54c8\u5e0c\u73af\u6269\u5c55\uff08\u652f\u6301\u5feb\u901f\u5b9e\u4f8b\u4f38\u7f29\uff09\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\uff0c\u5728\u76f8\u540cTTFT SLO\u7ea6\u675f\u4e0b\uff0cDualMap\u5c06\u6709\u6548\u8bf7\u6c42\u5bb9\u91cf\u63d0\u5347\u81f3\u591a2.25\u500d\uff0c\u4f18\u4e8e\u5f53\u524d\u6700\u4f18\u5de5\u4f5c\u3002", "conclusion": "DualMap\u4ee5\u7edf\u4e00\u6846\u67b6\u89e3\u51b3\u7f13\u5b58\u91cd\u7528\u4e0e\u8d1f\u8f7d\u5e73\u8861\u51b2\u7a81\uff0c\u663e\u8457\u63d0\u5347\u670d\u52a1\u6027\u80fd\u548c\u6210\u672c\u6548\u76ca\uff0c\u9002\u5e94\u771f\u5b9e\u52a8\u6001\u8d1f\u8f7d\u573a\u666f\u3002"}}
{"id": "2602.06555", "categories": ["cs.DC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.06555", "abs": "https://arxiv.org/abs/2602.06555", "authors": ["Lanpei Li", "Massimo Coppola", "Malio Li", "Valerio Besozzi", "Jack Bell", "Vincenzo Lomonaco"], "title": "Reinforcement Learning-Based Dynamic Management of Structured Parallel Farm Skeletons on Serverless Platforms", "comment": "Accepted at AHPC3 workshop, PDP 2026", "summary": "We present a framework for dynamic management of structured parallel processing skeletons on serverless platforms. Our goal is to bring HPC-like performance and resilience to serverless and continuum environments while preserving the programmability benefits of skeletons. As a first step, we focus on the well known Farm pattern and its implementation on the open-source OpenFaaS platform, treating autoscaling of the worker pool as a QoS-aware resource management problem. The framework couples a reusable farm template with a Gymnasium-based monitoring and control layer that exposes queue, timing, and QoS metrics to both reactive and learning-based controllers. We investigate the effectiveness of AI-driven dynamic scaling for managing the farm's degree of parallelism via the scalability of serverless functions on OpenFaaS. In particular, we discuss the autoscaling model and its training, and evaluate two reinforcement learning (RL) policies against a baseline of reactive management derived from a simple farm performance model. Our results show that AI-based management can better accommodate platform-specific limitations than purely model-based performance steering, improving QoS while maintaining efficient resource usage and stable scaling behaviour.", "AI": {"tldr": "\u63d0\u51fa\u5728\u65e0\u670d\u52a1\u5668\u5e73\u53f0 Eukaryote \u4e0a\u52a8\u6001\u7ba1\u7406\u5e76\u884c\u5904\u7406\u9aa8\u67b6\u7684\u6846\u67b6\uff0c\u901a\u8fc7AI\u9a71\u52a8\u6269\u7f29\u5bb9\u63d0\u5347Farm\u6a21\u5f0f\u7684\u6027\u80fd\u548c\u670d\u52a1\u8d28\u91cf\u3002", "motivation": "\u5728\u7ef4\u6301\u9aa8\u67b6\u6a21\u5f0f\u53ef\u7f16\u7a0b\u6027\u4f18\u52bf\u7684\u540c\u65f6\uff0c\u4e3a\u65e0\u670d\u52a1\u5668\u73af\u5883\u63d0\u4f9b\u7c7b\u4f3c\u9ad8\u6027\u80fd\u8ba1\u7b97\u7684\u6027\u80fd\u548c\u5f39\u6027\uff0c\u89e3\u51b3\u5de5\u4f5c\u6c60\u81ea\u52a8\u6269\u7f29\u5bb9\u8fd9\u4e00QoS\u611f\u77e5\u7684\u8d44\u6e90\u7ba1\u7406\u95ee\u9898\u3002", "method": "\u6574\u5408\u53ef\u590d\u7528Farm\u6a21\u677f\u4e0e31 Gymnasium\u76d1\u63a7\u63a7\u5236\u5c42\uff0c\u6536\u96c6\u961f\u5217\u3001\u65f6\u5e8f\u548cQoS\u6307\u6807\uff0c\u91c7\u7528\u5f3a\u5316\u5b66\u4e60\u7b56\u7565\u7ba1\u7406\u5e76\u884c\u5ea6\uff0c\u5728OpenFaaS\u4e0a\u5bf9\u6bd4AI\u63a7\u5236\u4e0e\u4f20\u7edf\u53cd\u5e94\u5f0f\u65b9\u6cd5\u3002", "result": "AI\u7ba1\u7406\u80fd\u66f4\u597d\u9002\u5e94\u5e73\u53f0\u9650\u5236\uff0c\u76f8\u8f83\u4e8e\u7eaf\u6a21\u578b\u65b9\u6cd5\u63d0\u9ad8\u670d\u52a1\u8d28\u91cf\uff08QoS\uff09\uff0c\u540c\u65f6\u4fdd\u6301\u9ad8\u6548\u8d44\u6e90\u5229\u7528\u548c\u7a33\u5b9a\u6269\u7f29\u884c\u4e3a\u3002", "conclusion": "\u4eba\u5de5\u667a\u80fd\u9a71\u52a8\u7684\u65b9\u6cd5\u5728\u4f18\u5316\u6027\u80fd\u548c\u8d44\u6e90\u6548\u7387\u65b9\u9762\u4f18\u4e8e\u6a21\u578b\u9759\u6001\u8c03\u63a7\uff0c\u8bc1\u5b9e\u5176\u5728\u65e0\u670d\u52a1\u5668\u73af\u5883\u52a8\u6001\u7ba1\u7406\u4e2d\u7684\u6709\u6548\u6027\u3002"}}
