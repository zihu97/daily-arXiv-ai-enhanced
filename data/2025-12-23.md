<div id=toc></div>

# Table of Contents

- [cs.AR](#cs.AR) [Total: 5]
- [cs.NI](#cs.NI) [Total: 4]
- [cs.MA](#cs.MA) [Total: 1]
- [cs.SE](#cs.SE) [Total: 20]
- [cs.DC](#cs.DC) [Total: 12]


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [1] [Making Strong Error-Correcting Codes Work Effectively for HBM in AI Inference](https://arxiv.org/abs/2512.18152)
*Rui Xie,Yunhua Fang,Asad Ul Haq,Linsen Ma,Sanchari Sen,Swagath Venkataramani,Liu Liu,Tong Zhang*

Main category: cs.AR

TL;DR: REACH是一种控制器管理的ECC设计，通过两层Reed-Solomon编码在保持HBM接口不变的前提下，显著提升可容忍原始误码率并降低系统成本。


<details>
  <summary>Details</summary>
Motivation: 当前HBM因短距片上ECC导致成本高、可靠性策略固化，需探索更高原始误码率下维持端到端正确性与吞吐量的方案。

Method: 采用两层Reed-Solomon编码：内层局部纠错，外层仅修复标记块；支持差分奇偶校验更新和重要性自适应位平面保护。

Result: 在8K上下文LLM测试中，REACH在原始BER达1e-3时仍保持近79%吞吐量，控制器面积减少11.6倍，功耗降低约60%。

Conclusion: REACH将强ECC移至控制器，使长码可靠性成为系统级选择，有望在标准接口下实现更低成本HBM。

Abstract: LLM inference is increasingly memory bound, and HBM cost per GB dominates system cost. Current HBM stacks include short on-die ECC that tightens binning, raises price, and fixes reliability policy inside the device. This paper asks whether a system can tolerate a much higher raw HBM bit error rate and still keep end-to-end correctness and throughput, without changing the HBM PHY or the fixed 32 B transaction size. We propose REACH, a controller managed ECC design that keeps the HBM link and 32 B transfers unchanged. REACH uses a two level Reed-Solomon scheme: each 32 B chunk uses an inner code to check and correct most faults locally, while chunks that cannot be fixed are marked as erasures. An outer code spans kilobytes and runs in erasure only mode, repairing only flagged chunks and avoiding the expensive locator step. For small random writes, REACH updates outer parity with differential parity to avoid recomputing parity over the whole span, and an optional importance adaptive bit plane policy can protect only critical fields such as BF16 exponents to reduce ECC work and traffic. On three LLMs at 8K context, REACH keeps about 79 percent of on-die ECC throughput at zero BER and remains qualified up to a raw BER of 1e-3, extending tolerable device error rates by about three orders of magnitude while keeping tokens per second nearly flat. In ASAP7, a full REACH controller occupies 15.2 mm2 and consumes 17.5 W at 3.56 TB/s, and it reduces ECC area by 11.6x and power by about 60 percent compared to a naive long Reed-Solomon baseline. By moving strong ECC into the controller, REACH turns long code reliability into a system choice that can enable lower cost HBM under the same standard interface.

</details>


### [2] [PIM-FW: Hardware-Software Co-Design of All-pairs Shortest Paths in DRAM](https://arxiv.org/abs/2512.18158)
*Tsung-Han Lu,Zheyu Li,Minxuan Zhou,Tajana Rosing*

Main category: cs.AR

TL;DR: PIM-FW是一种结合近内存与内存内计算的新型硬件架构与数据流，专为加速Floyd-Warshall算法而设计，在HBM3上实现显著性能提升与能耗降低。


<details>
  <summary>Details</summary>
Motivation: 传统CPU/GPU因高时间复杂度与数据搬运开销，难以高效扩展APSP算法，亟需新架构突破瓶颈。

Method: 提出PIM-FW架构，包含专用位串行PE阵列、交错映射策略及混合内存计算模型，支持细粒度并行与内存内距离更新。

Result: 在8192x8192图上，相较先进GPU方案，PIM-FW实现18.7倍速度提升与3200倍DRAM能耗降低。

Conclusion: PIM-FW通过软硬件协同设计有效消除数据搬运瓶颈，为大规模图计算提供高能效解决方案。

Abstract: All-pairs shortest paths (APSP) is a fundamental algorithm used for routing, logistics, and network analysis, but the cubic time complexity and heavy data movement of the canonical Floyd-Warshall (FW) algorithm severely limits its scalability on conventional CPUs or GPUs. In this paper, we propose PIM-FW, a novel co-designed hardware architecture and dataflow that leverages processing in and near memory architecture designed to accelerate blocked FW algorithm on an HBM3 stack. To enable fine-grained parallelism, we propose a massively parallel array of specialized bit-serial bank PE and channel PE designed to accelerate the core min-plus operations. Our novel dataflow complements this hardware, employing an interleaved mapping policy for superior load balancing and hybrid in and near memory computing model for efficient computation and reduction. The novel in-bank computing approach allows all distance updates to be performed and stored in memory bank, a key contribution is that eliminates the data movement bottleneck inherent in GPU-based approaches. We implement a full software and hardware co-design using a cycle-accurate simulator to simulate an 8-channel, 4-Hi HBM3 PIM stack on real road-network traces. Experimental results show that, for a 8192 x 8192 graph, PIM-FW achieves a 18.7x speedup in end-to-end execution, and consumes 3200x less DRAM energy compared to a state-of-the-art GPU-only Floyd-Warshall.

</details>


### [3] [BARD: Reducing Write Latency of DDR5 Memory by Exploiting Bank-Parallelism](https://arxiv.org/abs/2512.18300)
*Suhas Vittal,Moinuddin Qureshi*

Main category: cs.AR

TL;DR: BARD通过优化缓存替换策略，提升DDR5系统中DRAM写入的银行并行性，从而降低延迟、提高性能。


<details>
  <summary>Details</summary>
Motivation: DDR5中写入延迟因银行冲突差异巨大，现有缓存策略未考虑此问题，导致性能损失。

Method: 提出BARD框架，包含基于驱逐(BARD-E)、主动清理(BARD-C)和混合策略(BARD-H)三种变体，引导写入至低延迟银行。

Result: 在多种基准测试中，BARD-H平均提升性能4.3%，最高达8.5%，且仅需每LLC切片8字节SRAM开销。

Conclusion: BARD以极小硬件开销有效缓解DDR5写入延迟问题，显著提升系统性能。

Abstract: This paper studies the impact of DRAM writes on DDR5-based system. To efficiently perform DRAM writes, modern systems buffer write requests and try to complete multiple write operations whenever the DRAM mode is switched from read to write. When the DRAM system is performing writes, it is not available to service read requests, thus increasing read latency and reducing performance. We observe that, given the presence of on-die ECC in DDR5 devices, the time to perform a write operation varies significantly: from 1x (for writes to banks of different bankgroups) to 6x (for writes to banks within the same bankgroup) to 24x (for conflicting requests to the same bank). If we can orchestrate the write stream to favor write requests that incur lower latency, then we can reduce the stall time from DRAM writes and improve performance. However, for current systems, the write stream is dictated by the cache replacement policy, which makes eviction decisions without being aware of the variable latency of DRAM writes. The key insight of our work is to improve performance by modifying the cache replacement policy to increase bank-parallelism of DRAM writes.
  Our paper proposes {\em BARD (Bank-Aware Replacement Decisions)}, which modifies the cache replacement policy to favor dirty lines that belong to banks without pending writes. We analyze two variants of BARD: BARD-E (Eviction-based), which changes the eviction policy to evict low-cost dirty lines, and BARD-C (Cleansing-Based), which proactively cleans low-cost dirty lines without modifying the eviction decisions. We develop a hybrid policy (BARD-H), which uses a selective combination of both eviction and writeback. Our evaluations across workloads from SPEC2017, LIGRA, STREAM, and Google server traces show that BARD-H improves performance by 4.3\% on average and up-to 8.5\%. BARD requires only 8 bytes of SRAM per LLC slice.

</details>


### [4] [Weight Transformations in Bit-Sliced Crossbar Arrays for Fault Tolerant Computing-in-Memory: Design Techniques and Evaluation Framework](https://arxiv.org/abs/2512.18459)
*Akul Malhotra,Sumeet Kumar Gupta*

Main category: cs.AR

TL;DR: 提出无需训练的sign-flip和bit-flip方法，提升CiM加速器上多比特DNN对SAF的容错能力，恢复精度损失且硬件开销极低。


<details>
  <summary>Details</summary>
Motivation: 解决CiM加速器因存储单元卡死故障导致DNN推理精度下降的问题，现有CVM方法在高故障率或复杂任务下效果不足。

Method: 提出两种无需重训练的权重变换技术：列级符号翻转（sign-flip）与位片级比特翻转（bit-flip），结合查找表框架实现高效优化映射。

Result: 在ResNet与ViT模型上验证，显著恢复CIFAR-100与ImageNet数据集下的精度损失；硬件开销极小，bit-flip提供更高容错性伴随适度开销。

Conclusion: sign-flip与bit-flip是实用、可扩展的SAF缓解方案，适用于多比特DNN在CiM架构上的可靠部署。

Abstract: The deployment of deep neural networks (DNNs) on compute-in-memory (CiM) accelerators offers significant energy savings and speed-up by reducing data movement during inference. However, the reliability of CiM-based systems is challenged by stuck-at faults (SAFs) in memory cells, which corrupt stored weights and lead to accuracy degradation. While closest value mapping (CVM) has been shown to partially mitigate these effects for multibit DNNs deployed on bit-sliced crossbars, its fault tolerance is often insufficient under high SAF rates or for complex tasks. In this work, we propose two training-free weight transformation techniques, sign-flip and bit-flip, that enhance SAF tolerance in multi-bit DNNs deployed on bit-sliced crossbar arrays. Sign-flip operates at the weight-column level by selecting between a weight and its negation, whereas bit-flip provides finer granularity by selectively inverting individual bit slices. Both methods expand the search space for fault-aware mappings, operate synergistically with CVM, and require no retraining or additional memory. To enable scalability, we introduce a look-up-table (LUT)-based framework that accelerates the computation of optimal transformations and supports rapid evaluation across models and fault rates. Extensive experiments on ResNet-18, ResNet-50, and ViT models with CIFAR-100 and ImageNet demonstrate that the proposed techniques recover most of the accuracy lost under SAF injection. Hardware analysis shows that these methods incur negligible overhead, with sign-flip leading to negligible energy, latency, and area cost, and bit-flip providing higher fault resilience with modest overheads. These results establish sign-flip and bit-flip as practical and scalable SAF-mitigation strategies for CiM-based DNN accelerators.

</details>


### [5] [Sensitivity-Aware Mixed-Precision Quantization for ReRAM-based Computing-in-Memory](https://arxiv.org/abs/2512.19445)
*Guan-Cheng Chen,Chieh-Lin Tsai,Pei-Hsuan Tsai,Yuan-Hao Chang*

Main category: cs.AR

TL;DR: 提出一种结合敏感性分析与混合精度策略的结构化量化方法，优化ReRAM CIM系统中的权重存储与计算性能。


<details>
  <summary>Details</summary>
Motivation: 传统量化与压缩技术在ReRAM CIM架构中未能充分优化能效与性能。

Method: 采用敏感性分析指导的混合精度量化策略，提升交叉阵列利用率。

Result: 实现70%压缩率下86.33%准确率，功耗降低40%。

Conclusion: 该方法显著提升能效与计算效率，适用于功耗受限场景。

Abstract: Compute-In-Memory (CIM) systems, particularly those utilizing ReRAM and memristive technologies, offer a promising path toward energy-efficient neural network computation. However, conventional quantization and compression techniques often fail to fully optimize performance and efficiency in these architectures. In this work, we present a structured quantization method that combines sensitivity analysis with mixed-precision strategies to enhance weight storage and computational performance on ReRAM-based CIM systems. Our approach improves ReRAM Crossbar utilization, significantly reducing power consumption, latency, and computational load, while maintaining high accuracy. Experimental results show 86.33% accuracy at 70% compression, alongside a 40% reduction in power consumption, demonstrating the method's effectiveness for power-constrained applications.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [6] [Performance Guarantees for Data Freshness in Resource-Constrained Adversarial IoT Systems](https://arxiv.org/abs/2512.18155)
*Aresh Dadlani,Muthukrishnan Senthil Kumar,Omid Ardakanian,Ioanis Nikolaidis*

Main category: cs.NI

TL;DR: 本文研究了在资源受限的对抗攻击下，物联网系统中信息年龄（AoI）的性能变化，提出基于G-队列框架的分析模型并推导闭式解。


<details>
  <summary>Details</summary>
Motivation: 现有对抗性AoI模型常忽略排队动态和信道非平稳性，而实际IoT系统易受恶意干扰影响数据时效性。

Method: 构建双源M/G/1/1系统模型，引入负到达泊松过程与服务延迟，利用矩生成函数推导平均与峰值AoI闭式表达式，并采用随机占优法建立最坏情况约束攻击下的AoI界。

Result: 数值结果验证了理论分析，表明在一般服务时间分布下，资源受限的对抗干扰显著劣化AoI性能。

Conclusion: 该模型为评估对抗环境下实时监控系统的时效性提供了理论工具，强调需考虑攻击速率与时长约束对系统性能的影响。

Abstract: Timely updates are critical for real-time monitoring and control applications powered by the Internet of Things (IoT). As these systems scale, they become increasingly vulnerable to adversarial attacks, where malicious agents interfere with legitimate transmissions to reduce data rates, thereby inflating the age of information (AoI). Existing adversarial AoI models often assume stationary channels and overlook queueing dynamics arising from compromised sensing sources operating under resource constraints. Motivated by the G-queue framework, this paper investigates a two-source M/G/1/1 system in which one source is adversarial and disrupts the update process by injecting negative arrivals according to a Poisson process and inducing i.i.d. service slowdowns, bounded in attack rate and duration. Using moment generating functions, we then derive closed-form expressions for average and peak AoI for an arbitrary number of sources. Moreover, we introduce a worst-case constrained attack model and employ stochastic dominance arguments to establish analytical AoI bounds. Numerical results validate the analysis and highlight the impact of resource-limited adversarial interference under general service time distributions.

</details>


### [7] [How Many Pinching Antennas Are Enough?](https://arxiv.org/abs/2512.18761)
*Dimitrios Tyrovolas,Sotiris A. Tegos,Yue Xiao,Panagiotis D. Diamantoulakis,Sotiris Ioannidis,Christos K. Liaskos,George K. Karagiannidis,Stylianos D. Asimonis*

Main category: cs.NI

TL;DR: 该论文研究了在可编程无线环境中，固定位置的双态夹持天线系统的性能，并推导出中断概率和遍历可达数据率的闭式表达式，同时提出夹持离散化效率以评估离散与连续配置的性能差距。


<details>
  <summary>Details</summary>
Motivation: 现有夹持天线系统研究多假设天线位置可连续调节，而实际仅支持有限离散位置，本文旨在填补这一理论与现实的差距。

Method: 通过建模空间离散性，推导中断概率与遍历数据率的解析表达式，并引入夹持离散化效率指标进行性能评估。

Result: 数值结果验证了分析框架的有效性，表明使用有限数量的夹持天线即可逼近连续配置的性能。

Conclusion: 该研究为可编程无线环境中夹持天线系统的实际部署提供了理论依据和设计指导。

Abstract: Programmable wireless environments (PWEs) have emerged as a key paradigm for next-generation communication networks, aiming to transform wireless propagation from an uncontrollable phenomenon into a reconfigurable process that can adapt to diverse service requirements. In this framework, pinching-antenna systems (PASs) have recently been proposed as a promising enabling technology, as they allow the radiation location and effective propagation distance to be adjusted by selectively exciting radiating points along a dielectric waveguide. However, most existing studies on PASs rely on the idealized assumption that pinching-antenna (PA) positions can be continuously adjusted along the waveguide, while realistically only a finite set of pinching locations is available. Motivated by this, this paper analyzes the performance of two-state PASs, where the PA positions are fixed and only their activation state can be controlled. By explicitly accounting for the spatial discreteness of the available pinching points, closed-form analytical expressions for the outage probability and the ergodic achievable data rate are derived. In addition, we introduce the pinching discretization efficiency to quantify the performance gap between discrete and continuous pinching configurations, enabling a direct assessment of the number of PAs required to approximate the ideal continuous case. Finally, numerical results validate the analytical framework and show that near-continuous performance can be achieved with a limited number of PAs, offering useful insights for the design and deployment of PASs in PWEs.

</details>


### [8] [QoS-Aware Load Balancing in the Computing Continuum via Multi-Player Bandits](https://arxiv.org/abs/2512.18915)
*Ivan Čilić,Ivana Podnar Žarko,Pantelis Frangoudis,Schahram Dustdar*

Main category: cs.NI

TL;DR: QEdgeProxy是一种去中心化的QoS感知负载均衡器，通过MP-MAB与KDE方法优化边缘计算中每个客户端的服务质量。


<details>
  <summary>Details</summary>
Motivation: 现有负载均衡方法忽视单个客户端QoS需求，难以满足延迟敏感型应用在动态边缘环境中的服务质量要求。

Method: 采用多玩家多臂老虎机模型结合核密度估计，自主选择最优服务实例，并引入自适应探索机制应对环境变化。

Result: 在K3s集群仿真测试中，QEdgeProxy显著优于基于邻近性和强化学习的基线方法，能有效应对负载突增和实例变动。

Conclusion: QEdgeProxy为边缘计算连续体提供了一种高效、自适应且以客户端为中心的负载均衡解决方案。

Abstract: As computation shifts from the cloud to the edge to reduce processing latency and network traffic, the resulting Computing Continuum (CC) creates a dynamic environment where it is challenging to meet strict Quality of Service (QoS) requirements and avoid service instance overload. Existing methods often prioritize global metrics, overlooking per-client QoS, which is crucial for latency-sensitive and reliability-critical applications. We propose QEdgeProxy, a decentralized QoS-aware load balancer that acts as a proxy between IoT devices and service instances in CC. We formulate the load balancing problem as a Multi-Player Multi-Armed Bandit (MP-MAB) with heterogeneous rewards, where each load balancer autonomously selects service instances that maximize the probability of meeting its clients' QoS targets by using Kernel Density Estimation (KDE) to estimate QoS success probabilities. It also incorporates an adaptive exploration mechanism to recover rapidly from performance shifts and non-stationary conditions. We present a Kubernetes-native QEdgeProxy implementation and evaluate it on an emulated CC testbed deployed on a K3s cluster with realistic network conditions and a latency-sensitive edge-AI workload. Results show that QEdgeProxy significantly outperforms proximity-based and reinforcement-learning baselines in per-client QoS satisfaction, while adapting effectively to load surges and instance availability changes.

</details>


### [9] [BEVCooper: Accurate and Communication-Efficient Bird's-Eye-View Perception in Vehicular Networks](https://arxiv.org/abs/2512.19082)
*Jiawei Hou,Peng Yang,Xiangxiang Dai,Mingliu Liu,Conghao Zhou*

Main category: cs.NI

TL;DR: BEVCooper是一种新颖的协作感知框架，旨在提升联网自动驾驶车辆在带宽受限条件下的鸟瞰图（BEV）构建精度与效率。


<details>
  <summary>Details</summary>
Motivation: 原始感知数据在遮挡或远距离区域质量下降，影响BEV地图保真度，需协作优化。

Method: 提出BEV特征效用评估指标、在线学习式协作车辆选择策略及自适应融合机制，兼顾传输延迟与地图精度。

Result: 实验表明，相比现有方法，BEVCooper提升感知精度最高63.18%，降低端到端延迟67.9%，仅增加1.8%计算开销。

Conclusion: BEVCooper在动态拓扑和信道条件下实现渐近最优的车辆选择与特征融合，显著提升协作感知性能。

Abstract: Bird's-Eye-View (BEV) is critical to connected and automated vehicles (CAVs) as it can provide unified and precise representation of vehicular surroundings. However, quality of the raw sensing data may degrade in occluded or distant regions, undermining the fidelity of constructed BEV map. In this paper, we propose BEVCooper, a novel collaborative perception framework that can guarantee accurate and low-latency BEV map construction. We first define an effective metric to evaluate the utility of BEV features from neighboring CAVs. Then, based on this, we develop an online learning-based collaborative CAV selection strategy that captures the ever-changing BEV feature utility of neighboring vehicles, enabling the ego CAV to prioritize the most valuable sources under bandwidth-constrained vehicle-to-vehicle (V2V) links. Furthermore, we design an adaptive fusion mechanism that optimizes BEV feature compression based on the environment dynamics and real-time V2V channel quality, effectively balancing feature transmission latency and accuracy of the constructed BEV map. Theoretical analysis demonstrates that, BEVCooper achieves asymptotically optimal CAV selection and adaptive feature fusion under dynamic vehicular topology and V2V channel conditions. Extensive experiments on real-world testbed show that, compared with state-of-the-art benchmarks, the proposed BEVCooper enhances BEV perception accuracy by up to $63.18\%$ and reduces end-to-end latency by $67.9\%$, with only $1.8\%$ additional computational overhead.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [10] [ShibuyaSocial: Multi-scale Model of Pedestrian Flows in Scramble Crossing](https://arxiv.org/abs/2512.18550)
*Akihiro Sakurai,Naoya Kajio,Ko Yamamoto*

Main category: cs.MA

TL;DR: 本文提出了一种基于学习的行人流模型，结合全局路径选择与局部避障行为，利用注意力机制提升预测一致性，并在涩谷十字路口数据上验证了有效性。


<details>
  <summary>Details</summary>
Motivation: 为防止因行人过度拥挤引发事故，需对行人行为进行建模与预测，以营造安全舒适的步行环境。

Method: 结合全局路径规划与局部避障，采用注意力机制协调行为预测，并基于涩谷十字路口视频轨迹数据训练模型。

Result: 仿真结果在定性与定量上均表明，该模型能有效预测真实行人行为。

Conclusion: 所提模型成功整合多尺度行人行为，在复杂城市环境中展现出良好的预测能力。

Abstract: This paper presents a learning-based model of pedestrian flows that integrates multi scale behaviors such as global route selection and local collision avoidance in urban spaces, particularly focusing on pedestrian movements at Shibuya scramble crossing. Since too much congestion of pedestrian flows can cause serious accidents, mathematically modeling and predicting pedestrian behaviors is important for preventing such accidents and providing a safe and comfortable environment. Although numerous studies have investigated learning-based modeling methods, most of them focus only on the local behavior of pedestrians, such as collision avoidance with neighbors and environmental objects. In an actual environment, pedestrian behavior involves more complicated decision making including global route selection. Moreover, a state transition from stopping to walking at a traffic light should be considered simultaneously. In this study, the proposed model integrates local behaviors with global route selection, using an Attention mechanism to ensure consistent global and local behavior predictions. We recorded video data of pedestrians at Shibuya scramble crossing and trained the proposed model using pedestrian walking trajectory data obtained from the video. Simulations of pedestrian behaviors based on the trained model qualitatively and quantitatively validated that the proposed model can appropriately predict pedestrian behaviors.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [11] [Victor Calibration (VC): Multi-Pass Confidence Calibration and CP4.3 Governance Stress Test under Round-Table Orchestration](https://arxiv.org/abs/2512.17956)
*Victor Stasiuc,Round Table Collaboration*

Main category: cs.SE

TL;DR: 提出轻量级工具包提升大模型协作性，包含校准协议、行为审计和治理压力测试，确保安全前提下减少过度保守。


<details>
  <summary>Details</summary>
Motivation: 解决前沿大语言模型因安全对齐导致的过度保守问题，如回避或虚假拒绝，影响协作效率。

Method: 构建三部分工具包：Victor校准协议、FD-Lite行为审计、CP4.3治理压力测试，并在Claude系列模型上验证。

Result: 观察到校准轨迹单调且不违反安全约束，CP4.3表现稳定，支持独立复现与扩展。

Conclusion: 该工具包为改进模型协作性提供初步方案，鼓励社区复现、批评与拓展。

Abstract: Safety alignment can make frontier LMs overly conservative, degrading collaboration via hedging or false refusals. We present a lightweight toolkit with three parts: (1) Victor Calibration (VC), a multi-pass protocol that elicits a scalar confidence proxy T (T0<T1<T2) through iterative evidence re-evaluation; (2) FD-Lite, a behavior-only phenomenology audit with a fixed anchor phrase and a meta-prefix trap to avoid anthropomorphic claims; and (3) CP4.3, a governance stress test for rank invariance and allocation monotonicity (M6). Across Claude 4.5 models (Haiku, Sonnet no-thinking, Sonnet thinking) and Opus, we observe monotonic VC trajectories without violating safety invariants, and stable CP4.3 behavior. ("Opus" here refers to a single Claude Opus 4.1 session accessed via a standard UI account, as reported in Table 1.) This work was conducted by a single operator (n=1) and is intended as hypothesis-generating; we explicitly invite replication, critique, and extension by the research community. We include prompt templates and an artifact plan to facilitate independent verification.

</details>


### [12] [Specification and Detection of LLM Code Smells](https://arxiv.org/abs/2512.18020)
*Brahim Mahmoudi,Zacharie Chenail-Larcher,Naouel Moha,Quentin Stievenert,Florent Avellaneda*

Main category: cs.SE

TL;DR: 本文提出LLM代码异味概念并验证其在开源系统中的普遍存在。


<details>
  <summary>Details</summary>
Motivation: 为解决LLM集成不当导致的软件质量问题，填补相关代码异味分类空白。

Method: 基于文献定义五种LLM推理相关代码异味，扩展检测工具SpecDetect4AI并在200个开源系统中验证。

Result: 60.50%系统存在LLM代码异味，检测精度达86.06%。

Conclusion: LLM代码异味普遍存在，需重视其对软件质量的影响并改进开发实践。

Abstract: Large Language Models (LLMs) have gained massive popularity in recent years and are increasingly integrated into software systems for diverse purposes. However, poorly integrating them in source code may undermine software system quality. Yet, to our knowledge, there is no formal catalog of code smells specific to coding practices for LLM inference. In this paper, we introduce the concept of LLM code smells and formalize five recurrent problematic coding practices related to LLM inference in software systems, based on relevant literature. We extend the detection tool SpecDetect4AI to cover the newly defined LLM code smells and use it to validate their prevalence in a dataset of 200 open-source LLM systems. Our results show that LLM code smells affect 60.50% of the analyzed systems, with a detection precision of 86.06%.

</details>


### [13] [Detecting Flaky Tests in Quantum Software: A Dynamic Approach](https://arxiv.org/abs/2512.18088)
*Dongchan Kim,Hamidreza Khoramrokh,Lei Zhang,Andriy Miranskyy*

Main category: cs.SE

TL;DR: 首次大规模动态分析量子软件中的不稳定测试，发现其发生率低但检测困难，需大量重运行才能可靠识别。


<details>
  <summary>Details</summary>
Motivation: 量子软件中缺乏对不稳定测试的系统性研究，现有工作多依赖静态分析或少量手动报告，难以全面理解其普遍性与可检测性。

Method: 在23个Qiskit Terra版本上各执行1万次测试套件，统计结果变异、估算失败概率、分析跨版本重现性，并用Wilson置信区间量化检测所需重运行次数。

Result: 在27,026个测试用例中发现290个不稳定测试，总体发生率0-0.4%，多为单版本偶发，少数持续重现；多数失败概率极低（约10^{-4}），需数万次执行才可可靠检测；'transpiler'和'quantum_info'子组件最易受影响。

Conclusion: 量子软件中不稳定测试虽罕见，但在典型CI预算下难以检测，需专门策略应对；作者公开了测试执行数据集以支持后续研究。

Abstract: Flaky tests, tests that pass or fail nondeterministically without changes to code or environment, pose a serious threat to software reliability. While classical software engineering has developed a rich body of dynamic and static techniques to study flakiness, corresponding evidence for quantum software remains limited. Prior work relies primarily on static analysis or small sets of manually reported incidents, leaving open questions about the prevalence, characteristics, and detectability of flaky tests.
  This paper presents the first large-scale dynamic characterization of flaky tests in quantum software. We executed the Qiskit Terra test suite 10,000 times across 23 releases in controlled environments. For each release, we measured test-outcome variability, identified flaky tests, estimated empirical failure probabilities, analyzed recurrence across versions, and used Wilson confidence intervals to quantify rerun budgets for reliable detection. We further mapped flaky tests to Terra subcomponents to assess component-level susceptibility.
  Across 27,026 test cases, we identified 290 distinct flaky tests. Although overall flakiness rates were low (0-0.4%), flakiness was highly episodic: nearly two-thirds of flaky tests appeared in only one release, while a small subset recurred intermittently or persistently. Many flaky tests failed with very small empirical probabilities ($\hat{p} \approx 10^{-4}$), implying that tens of thousands of executions may be required for confident detection. Flakiness was unevenly distributed across subcomponents, with 'transpiler' and 'quantum_info' accounting for the largest share.
  These results show that quantum test flakiness is rare but difficult to detect under typical continuous integration budgets. To support future research, we release a public dataset of per-test execution outcomes.

</details>


### [14] [Holistic Evaluation of State-of-the-Art LLMs for Code Generation](https://arxiv.org/abs/2512.18131)
*Le Zhang,Suresh Kothari*

Main category: cs.SE

TL;DR: 评估六种大语言模型在代码生成中的表现，发现DeepSeek-R1和GPT-4.1最优，并提出部署建议。


<details>
  <summary>Details</summary>
Motivation: 探究当前主流大语言模型在真实编程任务中的实际性能与局限性，为开发者提供选型与使用指导。

Method: 基于944道LeetCode题目，跨五种语言评估六种LLM，采用编译错误、运行错误、功能失败和算法次优四项指标，并辅以案例分析。

Result: DeepSeek-R1与GPT-4.1在正确性、效率和鲁棒性上显著领先；常见失败包括语法错误、逻辑缺陷和算法低效。

Conclusion: 成功部署LLM需结合模型选择、提示工程与人工监督，方能保障代码生成在真实开发中的可靠性。

Abstract: This study presents a comprehensive empirical evaluation of six state-of-the-art large language models (LLMs) for code generation, including both general-purpose and code-specialized models. Using a dataset of 944 real-world LeetCode problems across five programming languages, we assess model performance using rigorous metrics: compile-time errors, runtime errors, functional failures, and algorithmic suboptimalities. The results reveal significant performance variations, with DeepSeek-R1 and GPT-4.1 consistently outperform others in terms of correctness, efficiency, and robustness. Through detailed case studies, we identify common failure scenarios such as syntax errors, logical flaws, and suboptimal algorithms, highlighting the critical role of prompt engineering and human oversight in improving results. Based on these findings, we provide actionable recommendations for developers and practitioners, emphasizing that successful LLM deployment depends on careful model selection, effective prompt design, and context-aware usage to ensure reliable code generation in real-world software development tasks.

</details>


### [15] [Understanding Typing-Related Bugs in Solidity Compiler](https://arxiv.org/abs/2512.18182)
*Lantian Li,Yue Pan,Dan Wang,Jingwen Wu,Zhongxing Yu*

Main category: cs.SE

TL;DR: 首次系统性实证研究Solidity编译器中的类型相关缺陷，分析146个官方确认的bug并总结12项核心发现。


<details>
  <summary>Details</summary>
Motivation: Solidity编译器类型系统的实现复杂性常引入隐蔽缺陷，影响智能合约安全性，亟需系统性研究。

Method: 从GitHub收集146个已修复的类型相关bug，从症状、根本原因、触发条件和修复策略四个维度进行分类与深度分析。

Result: 揭示了此类bug的独特分布模式与关键特征，总结出12项核心发现，并提出对检测与修复工作的实际启示。

Conclusion: 研究深化了对Solidity编译器固有弱点的理解，为未来改进类型系统和提升智能合约安全性提供新思路。

Abstract: The correctness of the Solidity compiler is crucial for ensuring the security of smart contracts. However, the implementation complexity of its type system often introduces elusive defects. This paper presents the first systematic empirical study on typing-related bugs in the Solidity compiler. To systematically analyze these bugs, we collected 146 officially confirmed and fixed typing-related bugs from the official GitHub repository of Solidity compiler. For each bug, we conducted an in-depth analysis and classification from four dimensions: symptoms, root causes, exposure conditions, and fix strategies. Through this study, we reveal unique distribution patterns and key characteristics of such bugs, and summarize 12 core findings. We additionally give the implications of our findings, and these implications not only deepen the understanding of inherent weaknesses in the Solidity compiler but also provide new insights for detecting and fixing typing-related bugs in the Solidity compiler.

</details>


### [16] [Software Vulnerability Management in the Era of Artificial Intelligence: An Industry Perspective](https://arxiv.org/abs/2512.18261)
*M. Mehdi Kholoosi,Triet Huynh Minh Le,M. Ali Babar*

Main category: cs.SE

TL;DR: 本研究调查了AI工具在软件漏洞管理中的行业应用现状，发现69%用户满意其速度与覆盖能力，但存在误报、上下文缺失和信任问题，建议增强可解释性与工作流集成。


<details>
  <summary>Details</summary>
Motivation: 当前AI在软件开发中广泛应用，但在软件漏洞管理领域的实际工业应用尚缺乏系统研究，亟需了解采纳程度、障碍与改进方向。

Method: 对来自27个国家、多个行业的60名从业者进行混合式问卷调查，结合定量与定性分析评估AI工具在SVM生命周期中的使用情况。

Result: AI工具已在SVM全周期部署，用户认可其效率与易用性，但普遍存在误报和信任问题；采纳模式呈现人机协同与组织治理特征。

Conclusion: 为实现AI在SVM中的安全有效应用，需提升工具的可解释性、上下文感知能力、集成流程及验证机制，为从业者与开发者提供实践指导。

Abstract: Artificial Intelligence (AI) has revolutionized software development, particularly by automating repetitive tasks and improving developer productivity. While these advancements are well-documented, the use of AI-powered tools for Software Vulnerability Management (SVM), such as vulnerability detection and repair, remains underexplored in industry settings. To bridge this gap, our study aims to determine the extent of the adoption of AI-powered tools for SVM, identify barriers and facilitators to the use, and gather insights to help improve the tools to meet industry needs better. We conducted a survey study involving 60 practitioners from diverse industry sectors across 27 countries. The survey incorporates both quantitative and qualitative questions to analyze the adoption trends, assess tool strengths, identify practical challenges, and uncover opportunities for improvement. Our findings indicate that AI-powered tools are used throughout the SVM life cycle, with 69\% of users reporting satisfaction with their current use. Practitioners value these tools for their speed, coverage, and accessibility. However, concerns about false positives, missing context, and trust issues remain prevalent. We observe a socio-technical adoption pattern in which AI outputs are filtered through human oversight and organizational governance. To support safe and effective use of AI for SVM, we recommend improvements in explainability, contextual awareness, integration workflows, and validation practices. We assert that these findings can offer practical guidance for practitioners, tool developers, and researchers seeking to enhance secure software development through the use of AI.

</details>


### [17] [Toward Training Superintelligent Software Agents through Self-Play SWE-RL](https://arxiv.org/abs/2512.18552)
*Yuxiang Wei,Zhiqing Sun,Emily McMilin,Jonas Gehring,David Zhang,Gabriel Synnaeve,Daniel Fried,Lingming Zhang,Sida Wang*

Main category: cs.SE

TL;DR: 提出Self-play SWE-RL方法，仅依赖代码库自主生成与修复漏洞，实现无须人工标注的智能体自提升。


<details>
  <summary>Details</summary>
Motivation: 现有智能体依赖人工标注数据和测试环境，限制了向超智能发展的潜力。

Method: 通过强化学习让单一LLM智能体在自对弈中注入并修复复杂度递增的软件漏洞，漏洞由测试补丁形式定义。

Result: 在SWE-bench基准上分别提升10.4和7.8分，全程优于依赖人工数据的基线方法。

Conclusion: 该方法为构建能自主从真实代码库学习、超越人类能力的超智能系统提供了可行路径。

Abstract: While current software agents powered by large language models (LLMs) and agentic reinforcement learning (RL) can boost programmer productivity, their training data (e.g., GitHub issues and pull requests) and environments (e.g., pass-to-pass and fail-to-pass tests) heavily depend on human knowledge or curation, posing a fundamental barrier to superintelligence. In this paper, we present Self-play SWE-RL (SSR), a first step toward training paradigms for superintelligent software agents. Our approach takes minimal data assumptions, only requiring access to sandboxed repositories with source code and installed dependencies, with no need for human-labeled issues or tests. Grounded in these real-world codebases, a single LLM agent is trained via reinforcement learning in a self-play setting to iteratively inject and repair software bugs of increasing complexity, with each bug formally specified by a test patch rather than a natural language issue description. On the SWE-bench Verified and SWE-Bench Pro benchmarks, SSR achieves notable self-improvement (+10.4 and +7.8 points, respectively) and consistently outperforms the human-data baseline over the entire training trajectory, despite being evaluated on natural language issues absent from self-play. Our results, albeit early, suggest a path where agents autonomously gather extensive learning experiences from real-world software repositories, ultimately enabling superintelligent systems that exceed human capabilities in understanding how systems are constructed, solving novel challenges, and autonomously creating new software from scratch.

</details>


### [18] [AI Code in the Wild: Measuring Security Risks and Ecosystem Shifts of AI-Generated Code in Modern Software](https://arxiv.org/abs/2512.18567)
*Bin Wang,Wenjie Yu,Yilu Zhong,Hao Yu,Keke Lian,Chaohua Lu,Hongfang Zheng,Dong Zhang,Hui Li*

Main category: cs.SE

TL;DR: 首次大规模实证研究揭示AI生成代码在开源项目中的分布模式及其安全影响，发现AI代码集中于非核心模块，但易引入跨项目传播的安全漏洞。


<details>
  <summary>Details</summary>
Motivation: 当前对大语言模型生成代码在实际开发中的普及程度与安全影响缺乏系统性实证分析。

Method: 构建高精度检测管道与代表性基准数据集，分析GitHub热门仓库与CVE相关代码变更，沿人机轴标注代码单元并追踪其演化路径。

Result: AI生成代码已占新增代码显著比例，集中于胶水代码与文档等非核心部分；特定CWE漏洞在AI代码中过量出现，且存在跨项目模板式传播；人机协作中若审查不足，AI引入缺陷更易长期留存并扩散。

Conclusion: AI生成代码虽提升开发效率，但需加强人工审查机制以遏制其引发的系统性安全风险。

Abstract: Large language models (LLMs) for code generation are becoming integral to modern software development, but their real-world prevalence and security impact remain poorly understood.
  We present the first large-scale empirical study of AI-generated code (AIGCode) in the wild. We build a high-precision detection pipeline and a representative benchmark to distinguish AIGCode from human-written code, and apply them to (i) development commits from the top 1,000 GitHub repositories (2022-2025) and (ii) 7,000+ recent CVE-linked code changes. This lets us label commits, files, and functions along a human/AI axis and trace how AIGCode moves through projects and vulnerability life cycles.
  Our measurements show three ecological patterns. First, AIGCode is already a substantial fraction of new code, but adoption is structured: AI concentrates in glue code, tests, refactoring, documentation, and other boilerplate, while core logic and security-critical configurations remain mostly human-written. Second, adoption has security consequences: some CWE families are overrepresented in AI-tagged code, and near-identical insecure templates recur across unrelated projects, suggesting "AI-induced vulnerabilities" propagated by shared models rather than shared maintainers. Third, in human-AI edit chains, AI introduces high-throughput changes while humans act as security gatekeepers; when review is shallow, AI-introduced defects persist longer, remain exposed on network-accessible surfaces, and spread to more files and repositories.
  We will open-source the complete dataset and release analysis artifacts and fine-grained documentation of our methodology and findings.

</details>


### [19] [Code2Doc: A Quality-First Curated Dataset for Code Documentation](https://arxiv.org/abs/2512.18748)
*Recep Kaan Karaman,Meftun Akarsu*

Main category: cs.SE

TL;DR: Code2Doc是一个高质量、经过严格筛选的函数级代码文档数据集，旨在提升自动代码文档生成模型的训练效果。


<details>
  <summary>Details</summary>
Motivation: 现有代码文档数据集普遍存在噪声大、重复多、AI生成内容污染等问题，削弱了监督学习的效果并干扰评估。

Method: 通过四阶段筛选流程构建数据集：确保文档完整性与清晰度、按结构与复杂度筛选函数、去重、识别AI生成内容。

Result: 最终数据集包含13,358个高质量样本，平均文档评分为6.93/10，86.9%含显式类型注解，仅2.9%疑似AI生成；微调模型在BLEU和ROUGE-L上分别提升29.47%和24.04%。

Conclusion: Code2Doc显著提升模型性能，其数据与筛选流程已开源以支持可复现研究。

Abstract: The performance of automatic code documentation generation models depends critically on the quality of the training data used for supervision. However, most existing code documentation datasets are constructed through large scale scraping of public repositories with limited quality control. As a result, they often contain noisy documentation, extensive duplication, and increasing contamination from AI generated content. These issues weaken the supervision signal available to learning-based models and complicate evaluation.
  We introduce \textbf{Code2Doc}, a quality-first curated dataset for function-level code documentation generation. Code2Doc consists of 13,358 high-quality function-documentation pairs extracted from widely used open-source repositories spanning five programming languages: Python, Java, TypeScript, JavaScript, and C++. The dataset is constructed using a four-stage curation pipeline that enforces documentation completeness and clarity, filters functions based on structural and complexity criteria, removes exact and near-duplicate code, and identifies documentation likely to be AI generated. Starting from 52,069 extracted candidates, only 25.6 percent satisfy all quality constraints.
  We provide a detailed analysis of the resulting dataset, which achieves a mean documentation quality score of 6.93 out of 10. Overall, 86.9% of samples contain explicit type annotations, and only 2.9\% are flagged as potentially AI generated. Baseline experiments show that fine-tuning a large language model on Code2Doc yields relative improvements of 29.47% in BLEU and 24.04% in ROUGE-L over zero shot performance, despite the modest dataset size. We release both the dataset and the full curation pipeline to support reproducible research on automatic code documentation generation.

</details>


### [20] [Misbehavior Forecasting for Focused Autonomous Driving Systems Testing](https://arxiv.org/abs/2512.18823)
*M M Abid Naziri,Stefano Carlo Lambertenghi,Andrea Stocco,Marcelo d'Amorim*

Main category: cs.SE

TL;DR: Foresee是一种基于模拟测试的自动驾驶软件可靠性评估技术，通过预测潜在失误并进行局部模糊测试，显著提升故障检测效率和效果。


<details>
  <summary>Details</summary>
Motivation: 现有自动驾驶软件的故障检测技术不可靠或成本高昂，需要更高效的方法来识别潜在失败场景。

Method: 利用行为预测器识别模拟中的近失误场景，并在这些场景附近进行局部模糊测试以发现未知故障。

Result: 相比随机方法和现有最先进故障预测器，Foresee分别多发现128.70%和38.09%的故障，速度提升2.49倍和1.42倍；与DriveFuzz结合使用时，故障检测能力最多提升93.94%。

Conclusion: Foresee在效率和效果上优于基线方法，且能有效补充现有模糊测试工具，显著提升自动驾驶系统的安全性验证能力。

Abstract: Simulation-based testing is the standard practice for assessing the reliability of self-driving cars' software before deployment. Existing bug-finding techniques are either unreliable or expensive. We build on the insight that near misses observed during simulations may point to potential failures. We propose Foresee, a technique that identifies near misses using a misbehavior forecaster that computes possible future states of the ego-vehicle under test. Foresee performs local fuzzing in the neighborhood of each candidate near miss to surface previously unknown failures. In our empirical study, we evaluate the effectiveness of different configurations of Foresee using several scenarios provided in the CARLA simulator on both end-to-end and modular self-driving systems and examine its complementarity with the state-of-the-art fuzzer DriveFuzz. Our results show that Foresee is both more effective and more efficient than the baselines. Foresee exposes 128.70% and 38.09% more failures than a random approach and a state-of-the-art failure predictor while being 2.49x and 1.42x faster, respectively. Moreover, when used in combination with DriveFuzz, Foresee enhances failure detection by up to 93.94%.

</details>


### [21] [What Drives Issue Resolution Speed? An Empirical Study of Scientific Workflow Systems on GitHub](https://arxiv.org/abs/2512.18852)
*Khairul Alam,Banani Roy*

Main category: cs.SE

TL;DR: 本研究分析了GitHub上科学工作流系统（SWS）的21,116个问题，探讨影响问题解决速度的因素，并提出优化建议。


<details>
  <summary>Details</summary>
Motivation: 科学工作流系统依赖社区维护，但目前缺乏对问题解决速度驱动因素的了解，影响软件质量和社区信任。

Method: 通过实证研究，分析项目特征、问题元数据和贡献者互动如何影响问题关闭时间。

Result: 68.91%的问题被关闭，半数在18.09天内解决；标注和分配问题与更快解决相关。

Conclusion: 建议开发者优化问题管理实践，以提升SWS项目的质量和可持续性。

Abstract: Scientific Workflow Systems (SWSs) play a vital role in enabling reproducible, scalable, and automated scientific analysis. Like other open-source software, these systems depend on active maintenance and community engagement to remain reliable and sustainable. However, despite the importance of timely issue resolution for software quality and community trust, little is known about what drives issue resolution speed within SWSs. This paper presents an empirical study of issue management and resolution across a collection of GitHub-hosted SWS projects. We analyze 21,116 issues to investigate how project characteristics, issue metadata, and contributor interactions affect time-to-close. Specifically, we address two research questions: (1) how issues are managed and addressed in SWSs, and (2) how issue and contributor features relate to issue resolution speed. We find that 68.91% of issues are closed, with half of them resolved within 18.09 days. Our results show that although SWS projects follow structured issue management practices, the issue resolution speed varies considerably across systems. Factors such as labeling and assigning issues are associated with faster issue resolution. Based on our findings, we make recommendations for developers to better manage SWS repository issues and improve their quality.

</details>


### [22] [An Empirical Study of Developer-Provided Context for AI Coding Assistants in Open-Source Projects](https://arxiv.org/abs/2512.18925)
*Shaokang Jiang,Daye Nam*

Main category: cs.SE

TL;DR: 本文通过大规模实证研究，分析了开发者为AI编码助手提供的项目上下文规则，构建了包含五类主题的分类体系，为下一代上下文感知开发工具提供指导。


<details>
  <summary>Details</summary>
Motivation: 当前AI编码助手依赖开发者提供的持久化、机器可读指令来提升响应质量，但这些指令内容尚未被系统研究。

Method: 对401个开源仓库中的cursor规则进行定性分析，归纳出开发者认为关键的项目上下文分类。

Result: 构建了涵盖‘约定、指南、项目信息、LLM指令、示例’五大主题的分类体系，并揭示其在不同项目类型与编程语言中的分布差异。

Conclusion: 该研究为设计更智能、更贴合项目需求的上下文感知AI开发工具提供了实证基础和方向指引。

Abstract: While Large Language Models (LLMs) have demonstrated remarkable capabilities, research shows that their effectiveness depends not only on explicit prompts but also on the broader context provided. This requirement is especially pronounced in software engineering, where the goals, architecture, and collaborative conventions of an existing project play critical roles in response quality. To support this, many AI coding assistants have introduced ways for developers to author persistent, machine-readable directives that encode a project's unique constraints. Although this practice is growing, the content of these directives remains unstudied.
  This paper presents a large-scale empirical study to characterize this emerging form of developer-provided context. Through a qualitative analysis of 401 open-source repositories containing cursor rules, we developed a comprehensive taxonomy of project context that developers consider essential, organized into five high-level themes: Conventions, Guidelines, Project Information, LLM Directives, and Examples. Our study also explores how this context varies across different project types and programming languages, offering implications for the next generation of context-aware AI developer tools.

</details>


### [23] [Scrum Sprint Planning: LLM-based and algorithmic solutions](https://arxiv.org/abs/2512.18966)
*Yuwon Yoon,Kevin Iwan,Madeleine Zwart,Xiaohan Qin,Hina Lee,Maria Spichkova*

Main category: cs.SE

TL;DR: 本文探索了大型语言模型（LLM）在Scrum冲刺规划中的适用性，实验表明当前模型输出质量尚不足以直接用于实际项目。


<details>
  <summary>Details</summary>
Motivation: 提升Scrum冲刺规划效率与准确性，探索AI辅助的可能性。

Method: 使用人工数据集对GPT-3.5 Turbo、GPT-4.0 Turbo和Val三个OpenAI模型进行案例研究。

Result: 模型生成结果质量未达可直接用于Scrum项目的标准。

Conclusion: 当前LLM在冲刺规划任务中仍需改进，暂不适合直接部署于实际Scrum流程。

Abstract: Planning for an upcoming project iteration (sprint) is one of the key activities in Scrum planning. In this paper, we present our work in progress on exploring the applicability of Large Language Models (LLMs) for solving this problem. We conducted case studies with manually created data sets to investigate the applicability of OpenAI models for supporting the sprint planning activities. In our experiments, we applied three models provided OpenAI: GPT-3.5 Turbo, GPT-4.0 Turbo, and Val. The experiments demonstrated that the results produced by the models aren't of acceptable quality for direct use in Scrum projects.

</details>


### [24] [PEAK: A Performance Engineering AI-Assistant for GPU Kernels Powered by Natural Language Transformations](https://arxiv.org/abs/2512.19018)
*Muhammad Usman Tariq,Abhinav Jangda,Angelica Moreira,Madan Musuvathi,Tyler Sorensen*

Main category: cs.SE

TL;DR: PEAK 是一个基于自然语言转换的 GPU 内核性能工程 AI 助手，支持 CUDA、HIP 和 HLSL 后端，能生成媲美厂商库的高性能代码。


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型在低层 GPU 内核优化中因硬件快速演进和样本稀少导致的困难。

Method: 利用自然语言描述迭代代码变换，由 LLM 执行，并辅以模块化基础设施进行验证与性能评估。

Result: 在三种后端上实现 16 种矩阵乘法优化，性能媲美厂商库；HLSL 实现达到硬件标称 FLOPS。

Conclusion: PEAK 提升性能工程师效率，支持全自动驱动，具备随 AI 进步持续优化的前瞻性设计。

Abstract: Advancements in large language models (LLMs) are showing promising impact in software development and programming assistance. However, these models struggle when operating on low-level backend code. This challenge is exacerbated in the domain of GPU kernels, where performance-critical details are coupled to rapidly evolving hardware characteristics and available code examples are sparse.
  In this work, we introduce PEAK, a Performance Engineering AI-Assistant for GPU Kernels powered by natural language transformations. PEAK utilizes the key insight that iterative code transformations (optimizations) can straightforwardly be written in natural language, and then carried out by LLMs. Thus, these transformations can be rapidly developed, encoding general portable optimizations, but also easily specialized to specific GPU devices and even kernels. These natural transformations are supported by a modular and extensible infrastructure that additionally performs validation and performance evaluation. We demonstrate the flexibility of PEAK by instantiating it for three backends, CUDA, HIP, and HLSL, and create 16 natural transformations for optimizing matrix multiplication kernels. We show that our resulting implementations are competitive with vendor libraries when available, and for HLSL (without a library) our implementations match the hardware documented FLOPS. PEAK allows the fine-grained exploration of several research questions around how LLMs behave in this domain, including characterizing transformations and their errors; and how performance evolves along optimization sequences. PEAK provides an interface that can either be utilized by performance engineers to improve productivity, or driven completely autonomously (e.g., by an AI agent), providing a forward-compatible design that can continue to improve with advances in AI capabilities.

</details>


### [25] [BanglaForge: LLM Collaboration with Self-Refinement for Bangla Code Generation](https://arxiv.org/abs/2512.19122)
*Mahir Labib Dihan,Sadif Ahmed,Md Nafiu Rahman*

Main category: cs.SE

TL;DR: BanglaForge是一个用于从孟加拉语描述生成代码的框架，结合检索增强、双模型协作与自优化机制，在BLP-2025基准上达到84%的Pass@1准确率。


<details>
  <summary>Details</summary>
Motivation: 孟加拉语是低资源语言，缺乏大规模标注数据集和自然语言转代码工具，亟需创新方案解决代码生成难题。

Method: 采用检索增强的双模型协作范式，结合上下文学习、LLM翻译、系统提示工程和基于执行反馈的迭代自优化机制。

Result: 在BLP-2025基准测试中，BanglaForge实现84.00%的Pass@1准确率。

Conclusion: 检索增强、模型协作与自优化机制对低资源孟加拉语代码生成任务有效。

Abstract: Bangla is a low-resource language for code generation, lacking large-scale annotated datasets and tools to transform natural language specifications into executable programs. This makes Bangla-to-code generation a challenging task requiring innovative solutions. To address this, we introduce BanglaForge, a novel framework for generating code from Bangla function descriptions. BanglaForge leverages a retrieval-augmented dual-model collaboration paradigm with self-refinement, combining in-context learning, llm-based translation, systematic prompt engineering, and iterative self-refinement based on execution feedback, where a coder generates initial solutions and a reviewer enhances them for robustness. On the BLP-2025 Bangla Code Generation benchmark, BanglaForge achieves a competitive Pass@1 accuracy of 84.00%, demonstrating the effectiveness of retrieval, model collaboration, and self-refinement for low-resource Bangla code generation.

</details>


### [26] [University Rents Enabling Corporate Innovation: Mapping Academic Researcher Coding and Discursive Labour in the R Language Ecosystem](https://arxiv.org/abs/2512.19153)
*Xiaolan Cai,Mathieu O'Neil,Stefano Zacchiroli*

Main category: cs.SE

TL;DR: 该文研究了研究人员在R语言生态系统中未被认可的劳动，揭示其对企业和开源社区的重要贡献。


<details>
  <summary>Details</summary>
Motivation: 探讨数字平台中未被承认的研究人员劳动如何支撑企业创新系统，并连接其专业雇佣背景。

Method: 通过分析GitHub上8924个R包仓库的提交与交流数据，结合定量与定性方法。

Result: 发现研究人员是R包的主要创建者和维护者，常无偿支持产业从业者；FLOSS意识形态使大科技公司合法利用高校资源。

Conclusion: 在知名学者之外，大量无报酬研究人员默默构建并维护关键统计基础设施，其劳动亟需被正视与制度化认可。

Abstract: This article explores the role of unrecognised labour in corporate innovation systems via an analysis of researcher coding and discursive contributions to R, one of the largest statistical software ecosystems. Studies of online platforms typically focus on how platform affordances constrain participants' actions, and profit from their labour. We innovate by connecting the labour performed inside digital platforms to the professional employment of participants. Our case study analyses 8,924 R package repositories on GitHub, examining commits and communications. Our quantitative findings show that researchers, alongside non-affiliated contributors, are the most frequent owners of R package repositories and their most active contributors. Researchers are more likely to hold official roles compared to the average, and to engage in collaborative problem-solving and support work during package development. This means there is, underneath the 'recognised' category of star researchers who transition between academia and industry and secure generous funding, an 'unrecognised' category of researchers who not only create and maintain key statistical infrastructure, but also provide support to industry employees, for no remuneration. Our qualitative findings show how this unrecognised labour affects practitioners. Finally, our analysis of the ideology and practice of free, libre and open source software (FLOSS) shows how this ideology and practice legitimate the use of 'university rents' by Big Tech.

</details>


### [27] [Semantically-Equivalent Transformations-Based Backdoor Attacks against Neural Code Models: Characterization and Mitigation](https://arxiv.org/abs/2512.19215)
*Junyao Ye,Zhen Li,Xi Tang,Shouhuai Xu,Deqing Zou,Zhongsheng Yuan*

Main category: cs.SE

TL;DR: 本文提出了一种新型后门攻击方法SET，利用语义等价的低频代码变换生成隐蔽触发器，在多任务多语言模型上实现高成功率且难以被现有防御检测。


<details>
  <summary>Details</summary>
Motivation: 当前对神经代码模型后门攻击的理解局限于注入式攻击，易被常规清理技术中和，可能导致安全误判，因此需研究更隐蔽的攻击方式。

Method: 提出SET框架，通过语义保持的低频代码变换生成触发器，并在CodeBERT、CodeT5、StarCoder等模型上进行跨任务跨语言实验。

Result: SET攻击成功率常超90%，模型效用不受损，且比注入式攻击平均降低25.13%的检测率；现有归一化防御仅部分缓解。

Conclusion: SET攻击具有高度隐蔽性与鲁棒性，亟需开发针对性的可扩展防御机制。

Abstract: Neural code models have been increasingly incorporated into software development processes. However, their susceptibility to backdoor attacks presents a significant security risk. The state-of-the-art understanding focuses on injection-based attacks, which insert anomalous patterns into software code. These attacks can be neutralized by standard sanitization techniques. This status quo may lead to a false sense of security regarding backdoor attacks. In this paper, we introduce a new kind of backdoor attacks, dubbed Semantically-Equivalent Transformation (SET)-based backdoor attacks, which use semantics-preserving low-prevalence code transformations to generate stealthy triggers. We propose a framework to guide the generation of such triggers. Our experiments across five tasks, six languages, and models like CodeBERT, CodeT5, and StarCoder show that SET-based attacks achieve high success rates (often >90%) while preserving model utility. The attack proves highly stealthy, evading state-of-the-art defenses with detection rates on average over 25.13% lower than injection-based counterparts. We evaluate normalization-based countermeasures and find they offer only partial mitigation, confirming the attack's robustness. These results motivate further investigation into scalable defenses tailored to SET-based attacks.

</details>


### [28] [A Dataset and Preliminary Study of Using GPT-5 for Code-change Impact Analysis](https://arxiv.org/abs/2512.19481)
*Katharina Stengg,Christian Macho,Martin Pinzger*

Main category: cs.SE

TL;DR: 研究评估了GPT-5和GPT-5-mini在预测代码变更影响方面的能力，发现其表现不佳，但提供diff信息略有提升效果。


<details>
  <summary>Details</summary>
Motivation: 手动分析代码变更及其影响耗时费力，需探索大语言模型在此任务中的潜力。

Method: 构建包含种子变更、变更对和变更类型的数据集，并在两种配置下测试LLM性能。

Result: 两个模型表现均不理想，GPT-5优于GPT-5-mini，提供diff信息可小幅提升性能。

Conclusion: 当前LLM在理解代码变更影响方面仍有较大改进空间，需进一步优化或结合其他技术。

Abstract: Understanding source code changes and their impact on other code entities is a crucial skill in software development. However, the analysis of code changes and their impact is often performed manually and therefore is time-consuming. Recent advancements in AI, and in particular large language models (LLMs) show promises to help developers in various code analysis tasks. However, the extent to which this potential can be utilized for understanding code changes and their impact is underexplored. To address this gap, we study the capabilities of GPT-5 and GPT-5-mini to predict the code entities impacted by given source code changes. We construct a dataset containing information about seed-changes, change pairs, and change types for each commit. Existing datasets lack crucial information about seed changes and impacted code entities. Our experiments evaluate the LLMs in two configurations: (1) seed-change information and the parent commit tree and (2) seed-change information, the parent commit tree, and the diff hunk of each seed change. We found that both LLMs perform poorly in the two experiments, whereas GPT-5 outperforms GPT-5-mini. Furthermore, the provision of the diff hunks helps both models to slightly improve their performance.

</details>


### [29] [Beyond Language Boundaries: Uncovering Programming Language Families for Code Language Models](https://arxiv.org/abs/2512.19509)
*Shangbo Yun,Xiaodong Gu,Jianghong Huang,Beijun Shen*

Main category: cs.SE

TL;DR: 本文提出一种基于嵌入的框架，揭示编程语言间的深层关系，并据此优化多语言代码大模型的训练与推理。


<details>
  <summary>Details</summary>
Motivation: 现有方法多简单聚合多语言代码数据，未深入挖掘语言间关系对模型性能的潜在提升价值。

Method: 定义21种语言特征，用LLM生成语义对齐代码样本，构建相似矩阵并聚类分析语言关系，进而设计三种训练策略。

Result: 在4项代码智能任务中显著提升模型性能，发现如C/C++/Java/Swift等形成清晰簇群，Go语言具最高跨语言相似性。

Conclusion: 本研究为理解编程语言家族结构提供新视角，并推动更高效的多语言代码大模型训练方法。

Abstract: The rapid proliferation of diverse programming languages presents both opportunities and challenges for developing multilingual code LLMs. While existing techniques often train code LLMs by simply aggregating multilingual code data, few explore the deeper relationships between programming languages(PLs) and how such relationships can be utilized to optimize the training and inference of code LLMs. In this work, we investigate 2 fundamental questions: 1) What are the deep linguistic relationships among PLs? and 2) How can these relationships be leveraged to improve multilingual code LLMs? We propose an embedding-based framework to uncover the latent families of PLs. Our approach begins by defining 21 primary linguistic features of programming languages, such as variable definition, control structures, and method declarations, and then employs LLMs to generate feature-aligned code samples across multiple languages. By embedding these semantically parallel code snippets from 19 languages, we construct a similarity matrix and perform hierarchical clustering to uncover inherent language relationships. Our analysis reveals clear hierarchical structures among programming languages. Closely related languages form well-defined clusters (e.g., C, C++, Java, and Swift group together), while Go exhibits as a central language with the highest cross-language similarity. Building on the uncovered language families, we propose three strategies to enhance multilingual LLM training: transfer learning across linguistically related languages, linguistic proximity-guided curriculum learning, and centroid-based intermediary code translation. Experiments on 4 code intelligence tasks demonstrate that our methods significantly improve multilingual LLM performance. This work offers a universal perspective on programming languages and advances more effective strategies for multilingual code LLM training.

</details>


### [30] [More code, less validation: Risk factors for over-reliance on AI coding tools among scientists](https://arxiv.org/abs/2512.19644)
*Gabrielle O'Brien,Alexis Parker,Nasir Eisty,Jeffrey Carver*

Main category: cs.SE

TL;DR: 研究调查了868名科研编程人员对生成式AI工具的使用情况，发现经验较少者更依赖AI生成代码，并倾向于高估生产力，可能影响科研代码质量。


<details>
  <summary>Details</summary>
Motivation: 科研人员普遍缺乏编程训练，而生成式AI虽可辅助编程，但存在过度依赖风险，需了解其实际使用模式与影响。

Method: 通过问卷调查868名科研编程人员，分析其工具使用偏好、采纳模式及与感知生产力相关的因素。

Result: 学生和经验较少者采纳率最高；偏好通用对话工具如ChatGPT；接受生成代码行数越多，感知生产力越高；开发实践使用不足者更易高估生产力。

Conclusion: 科研人员可能以代码生成量而非验证质量衡量生产力，引发对科研代码完整性的担忧，建议加强开发实践培训。

Abstract: Programming is essential to modern scientific research, yet most scientists report inadequate training for the software development their work demands. Generative AI tools capable of code generation may support scientific programmers, but user studies indicate risks of over-reliance, particularly among inexperienced users. We surveyed 868 scientists who program, examining adoption patterns, tool preferences, and factors associated with perceived productivity. Adoption is highest among students and less experienced programmers, with variation across fields. Scientific programmers overwhelmingly prefer general-purpose conversational interfaces like ChatGPT over developer-specific tools. Both inexperience and limited use of development practices (like testing, code review, and version control) are associated with greater perceived productivity-but these factors interact, suggesting formal practices may partially compensate for inexperience. The strongest predictor of perceived productivity is the number of lines of generated code typically accepted at once. These findings suggest scientific programmers using generative AI may gauge productivity by code generation rather than validation, raising concerns about research code integrity.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [31] [Efficient Multi-Adapter LLM Serving via Cross-Model KV-Cache Reuse with Activated LoRA](https://arxiv.org/abs/2512.17910)
*Allison Li,Kristjan Greenewald,Thomas Parnell,Navid Azizan*

Main category: cs.DC

TL;DR: 提出首个支持跨模型前缀缓存复用的LLM服务引擎，通过aLoRA显著降低多适配器推理延迟。


<details>
  <summary>Details</summary>
Motivation: 现有框架在切换任务专用适配器时存在大量重复计算，效率低下。

Method: 扩展vLLM框架，引入基模型对齐块哈希与激活感知掩码，实现跨模型KV缓存复用。

Result: 相比标准LoRA，端到端延迟最多降低58倍，首Token时间提升超100倍，效果随模型规模和序列长度增长而增强。

Conclusion: 本工作首次在现代LLM推理引擎中完整实现跨模型KV缓存复用，兼顾高效性与兼容性。

Abstract: Modern large language model (LLM) systems increasingly rely on multi-turn pipelines that are composed of multiple task-specific adapters, yet existing serving frameworks remain inefficient, incurring substantial recomputation overhead when switching between adapters. We present the first LLM serving engine that supports cross-model prefix cache reuse between base and adapted models via Activated LoRA (aLoRA), enabling efficient and fine-grained adapter switching during inference. Our design extends the vLLM framework by introducing base-aligned block hashing and activation-aware masking within the model execution path, permitting cache reuse across models while preserving compatibility with existing serving engine optimizations. Integrated into a production-grade inference stack, this approach supports dynamic adapter activation without excessive key-value tensor recomputation. Evaluation across representative multi-turn, multi-adapter pipelines demonstrates up to 58x end-to-end latency reduction and over 100x time-to-first-token improvement relative to standard LoRA baselines, with benefits that scale with model size and sequence length and manifest across all stages of the request lifecycle. This work bridges parameter-efficient model adaptation with high-performance serving, providing the first complete realization of cross-model KV-cache reuse in modern LLM inference engines.

</details>


### [32] [Byzantine Fault-Tolerant Multi-Agent System for Healthcare: A Gossip Protocol Approach to Secure Medical Message Propagation](https://arxiv.org/abs/2512.17913)
*Nihir Chadderwala*

Main category: cs.DC

TL;DR: 提出一种拜占庭容错的医疗多智能体系统，结合八卦协议与密码学验证，实现安全协作决策。


<details>
  <summary>Details</summary>
Motivation: 解决分布式医疗AI系统在不可信环境下的消息完整性与容错性挑战。

Method: 采用八卦传播协议与密码学验证机制，协调诊断、治疗、应急与数据分析智能体，通过拜占庭共识协议容忍最多f个故障节点（n=3f+1）。

Result: 实验显示系统能100%准确达成共识，支持防重放攻击、实时可视化监控，最多容忍33%拜占庭节点。

Conclusion: 为构建安全、弹性、适用于不可信环境的医疗多智能体协作系统提供实用框架。

Abstract: Recent advances in generative AI have enabled sophisticated multi-agent architectures for healthcare, where large language models power collaborative clinical decision-making. However, these distributed systems face critical challenges in ensuring message integrity and fault tolerance when operating in adversarial or untrusted environments.This paper presents a novel Byzantine fault-tolerant multi-agent system specifically designed for healthcare applications, integrating gossip-based message propagation with cryptographic validation mechanisms. Our system employs specialized AI agents for diagnosis, treatment planning, emergency response, and data analysis, coordinated through a Byzantine consensus protocol that tolerates up to f faulty nodes among n = 3f + 1 total nodes. We implement a gossip protocol for decentralized message dissemination, achieving consensus with 2f + 1 votes while maintaining system operation even under Byzantine failures. Experimental results demonstrate that our approach successfully validates medical messages with cryptographic signatures, prevents replay attacks through timestamp validation, and maintains consensus accuracy of 100% with up to 33% Byzantine nodes. The system provides real-time visualization of consensus rounds, vote tallies, and network topology, enabling transparent monitoring of fault-tolerant operations. This work contributes a practical framework for building secure, resilient healthcare multi-agent systems capable of collaborative medical decision-making in untrusted environments.

</details>


### [33] [QAISim: A Toolkit for Modeling and Simulation of AI in Quantum Cloud Computing Environments](https://arxiv.org/abs/2512.17918)
*Irwindeep Singh,Sukhpal Singh Gill,Jinzhao Sun,Jan Mol*

Main category: cs.DC

TL;DR: 本文提出基于量子强化学习的QAISim工具包，用于优化量子云环境中的资源分配，支持大规模物联网应用。


<details>
  <summary>Details</summary>
Motivation: 为应对物联网应用对量子计算资源日益增长的需求，需高效分配量子硬件资源。

Method: 采用参数化量子电路实现量子强化学习，开发Python工具包QAISim模拟策略梯度与深度Q学习算法。

Result: QAISim相比经典方法显著降低模型复杂度，减少可训练变量数量。

Conclusion: 量子强化学习在量子云资源管理中展现出高效性与简洁性，具备实际应用潜力。

Abstract: Quantum computing offers new ways to explore the theory of computation via the laws of quantum mechanics. Due to the rising demand for quantum computing resources, there is growing interest in developing cloud-based quantum resource sharing platforms that enable researchers to test and execute their algorithms on real quantum hardware. These cloud-based systems face a fundamental challenge in efficiently allocating quantum hardware resources to fulfill the growing computational demand of modern Internet of Things (IoT) applications. So far, attempts have been made in order to make efficient resource allocation, ranging from heuristic-based solutions to machine learning. In this work, we employ quantum reinforcement learning based on parameterized quantum circuits to address the resource allocation problem to support large IoT networks. We propose a python-based toolkit called QAISim for the simulation and modeling of Quantum Artificial Intelligence (QAI) models for designing resource management policies in quantum cloud environments. We have simulated policy gradient and Deep Q-Learning algorithms for reinforcement learning. QAISim exhibits a substantial reduction in model complexity compared to its classical counterparts with fewer trainable variables.

</details>


### [34] [Accelerated Digital Twin Learning for Edge AI: A Comparison of FPGA and Mobile GPU](https://arxiv.org/abs/2512.17941)
*Bin Xu,Ayan Banerjee,Midhat Urooj,Sandeep K. S. Gupta*

Main category: cs.DC

TL;DR: 本文提出了一种适用于FPGA加速的数字孪生学习框架，显著提升能效与速度，优于移动GPU和云端GPU方案，并应用于糖尿病与冠心病预测。


<details>
  <summary>Details</summary>
Motivation: 现有模型恢复技术计算与内存开销大，难以满足医疗数字孪生对高效实时学习的需求。

Method: 构建适配FPGA硬件加速的通用数字孪生学习框架，并与移动端GPU及云端GPU进行性能对比。

Result: FPGA实现在每瓦性能提升8.8倍、DRAM占用减少28.5倍、运行速度提升1.67倍；移动GPU虽能效略优，但延迟与内存占用更高。

Conclusion: FPGA加速方案在资源受限边缘医疗场景中具有显著优势，可有效支持糖尿病与冠心病等疾病的数字孪生应用。

Abstract: Digital twins (DTs) can enable precision healthcare by continually learning a mathematical representation of patient-specific dynamics. However, mission critical healthcare applications require fast, resource-efficient DT learning, which is often infeasible with existing model recovery (MR) techniques due to their reliance on iterative solvers and high compute/memory demands. In this paper, we present a general DT learning framework that is amenable to acceleration on reconfigurable hardware such as FPGAs, enabling substantial speedup and energy efficiency. We compare our FPGA-based implementation with a multi-processing implementation in mobile GPU, which is a popular choice for AI in edge devices. Further, we compare both edge AI implementations with cloud GPU baseline. Specifically, our FPGA implementation achieves an 8.8x improvement in \text{performance-per-watt} for the MR task, a 28.5x reduction in DRAM footprint, and a 1.67x runtime speedup compared to cloud GPU baselines. On the other hand, mobile GPU achieves 2x better performance per watts but has 2x increase in runtime and 10x more DRAM footprint than FPGA. We show the usage of this technique in DT guided synthetic data generation for Type 1 Diabetes and proactive coronary artery disease detection.

</details>


### [35] [Fast Online Digital Twinning on FPGA for Mission Critical Applications](https://arxiv.org/abs/2512.17942)
*Bin Xu,Ayan Banerjee,Sandeep K. S. Gupta*

Main category: cs.DC

TL;DR: 本文提出了一种基于FPGA加速的数字孪生框架，用于在边缘设备上实现低延迟实时建模，适用于安全关键场景。


<details>
  <summary>Details</summary>
Motivation: 在如空中防撞等任务关键型应用中，数字孪生需在边缘设备上以极低延迟运行，但受限于计算与内存带宽。

Method: 将GRU和全连接层等神经组件卸载至FPGA可重构硬件，实现高效并行执行。

Result: 系统响应速度达到人类反应时间的五倍，实现实时性能。

Conclusion: 该框架验证了在边缘平台上部署数字孪生用于时间敏感、安全关键环境的可行性。

Abstract: Digital twinning enables real-time simulation and predictive modeling by maintaining a continuously updated virtual representation of a physical system. In mission-critical applications, such as mid-air collision avoidance, these models must operate online with extremely low latency to ensure safety. However, executing complex Model Recovery (MR) pipelines on edge devices is limited by computational and memory bandwidth constraints. This paper introduces a fast, FPGA-accelerated digital twinning framework that offloads key neural components, including gated recurrent units (GRU) and dense layers, to reconfigurable hardware for efficient parallel execution. Our system achieves real-time responsiveness, operating five times faster than typical human reaction time, and demonstrates the practical viability of deploying digital twins on edge platforms for time-sensitive, safety-critical environments.

</details>


### [36] [ACE-Sync: An Adaptive Cloud-Edge Synchronization Framework for Communication-Efficient Large-Scale Distributed Model Training](https://arxiv.org/abs/2512.18127)
*Yi Yang,Ziyu Lin,Liesheng Wei*

Main category: cs.DC

TL;DR: ACE-Sync是一个自适应云边同步框架，通过注意力梯度预测、差异化压缩和分层协同机制，在降低60%通信开销的同时保持接近全同步的模型精度。


<details>
  <summary>Details</summary>
Motivation: 大规模深度学习模型在分布式训练中面临通信开销大、收敛不稳定与精度下降的挑战，尤其在带宽受限或异构云边环境中更为突出。

Method: 提出ACE-Sync框架，包含基于注意力的梯度重要性预测器、差异化参数压缩策略和分层云边协同机制，结合背包优化与残差误差补偿实现高效通信与稳定收敛。

Result: 相比FullSync，通信量从112.5GB降至44.7GB（减少60%），收敛周期从41缩短至39轮，Top-1准确率仅下降0.3%至82.1%。

Conclusion: ACE-Sync为大规模云边分布式训练提供了一种可扩展、通信高效且精度保持的解决方案。

Abstract: Large-scale deep learning models impose substantial communication overh ead in distributed training, particularly in bandwidth-constrained or heterogeneous clo ud-edge environments. Conventional synchronous or fixed-compression techniques o ften struggle to balance communication cost, convergence stability, and model accura cy. To address these challenges, we propose ACE-Sync, an Adaptive Cloud-Edge Sy nchronization Framework that integrates (1) an attention-based gradient importance p redictor, (2) a differentiated parameter compression strategy, and (3) a hierarchical cl oud-edge coordination mechanism. ACE-Sync dynamically selects which parameter groups to synchronize and determines appropriate compression levels under per-devic e bandwidth budgets. A knapsack-based optimization strategy is adopted to maximize important gradient preservation while reducing redundant communication. Furthermo re, residual-based error compensation and device clustering ensure long-term converg ence and cross-device personalization. Experiments show that ACE-Sync substantiall y reduces communication overhead while maintaining competitive accuracy. Compar ed with FullSync, ACE-Sync lowers communication cost from 112.5 GB to 44.7 GB (a 60% reduction) and shortens convergence from 41 to 39 epochs. Despite aggressiv e communication reduction, ACE-Sync preserves high model quality, achieving 82. 1% Top-1 accuracy-only 0.3% below the full-synchronization baseline-demonstrating its efficiency and scalability for large-scale distributed training. These results indicate that ACE-Sync provides a scalable, communication-efficient, and accuracy-preservin g solution for large-scale cloud-edge distributed model training.

</details>


### [37] [Faster Vertex Cover Algorithms on GPUs with Component-Aware Parallel Branching](https://arxiv.org/abs/2512.18334)
*Hussein Amro,Basel Fakhri,Amer E. Mouawad,Izzat El Hajj*

Main category: cs.DC

TL;DR: 提出了一种新的GPU算法，通过检测图分裂并独立处理各组件，显著提升了顶点覆盖问题的求解效率。


<details>
  <summary>Details</summary>
Motivation: 现有GPU方案因无法感知图分裂及内存占用高，难以扩展到大规模复杂图。

Method: 检测图分裂后独立分支处理组件，将后处理委托给分支末端，并通过缩减图降低内存占用。

Result: 相比现有方案，新方法在数秒内完成计算，而原方案超过6小时。

Conclusion: 首次在GPU上以负载均衡方式并行化非尾递归分支模式，显著提升性能。

Abstract: Algorithms for finding minimum or bounded vertex covers in graphs use a branch-and-reduce strategy, which involves exploring a highly imbalanced search tree. Prior GPU solutions assign different thread blocks to different sub-trees, while using a shared worklist to balance the load. However, these prior solutions do not scale to large and complex graphs because their unawareness of when the graph splits into components causes them to solve these components redundantly. Moreover, their high memory footprint limits the number of workers that can execute concurrently. We propose a novel GPU solution for vertex cover problems that detects when a graph splits into components and branches on the components independently. Although the need to aggregate the solutions of different components introduces non-tail-recursive branches which interfere with load balancing, we overcome this challenge by delegating the post-processing to the last descendant of each branch. We also reduce the memory footprint by reducing the graph and inducing a subgraph before exploring the search tree. Our solution substantially outperforms the state-of-the-art GPU solution, finishing in seconds when the state-of-the-art solution exceeds 6 hours. To the best of our knowledge, our work is the first to parallelize non-tail-recursive branching patterns on GPUs in a load balanced manner.

</details>


### [38] [Remoe: Towards Efficient and Low-Cost MoE Inference in Serverless Computing](https://arxiv.org/abs/2512.18674)
*Wentao Liu,Yuhao Hu,Ruiting Zhou,Baochun Li,Ne Wang*

Main category: cs.DC

TL;DR: Remoe 是一种为无服务器计算环境设计的异构 MoE 推理系统，通过将专家模块卸载到 CPU 和无服务器函数，结合语义预测与资源优化算法，显著降低推理成本和冷启动延迟。


<details>
  <summary>Details</summary>
Motivation: MoE 模型因专家数量庞大导致内存缓存开销高，难以用简单分区解决；而无服务器计算适合突发负载，需专门优化以降低成本。

Method: Remoe 将非专家模块放 GPU、专家模块放 CPU，并将低频专家卸载至无服务器函数；结合 SPS 语义预测、MMP 内存预分配、LPT 联合优化框架。

Result: 在 Kubernetes 上实现，实验表明相比最先进基线，推理成本最高降低 57%，冷启动延迟减少 47%。

Conclusion: Remoe 有效适配无服务器架构，在保证服务质量的同时显著提升 MoE 模型部署的经济性和响应效率。

Abstract: Mixture-of-Experts (MoE) has become a dominant architecture in large language models (LLMs) due to its ability to scale model capacity via sparse expert activation. Meanwhile, serverless computing, with its elasticity and pay-per-use billing, is well-suited for deploying MoEs with bursty workloads. However, the large number of experts in MoE models incurs high inference costs due to memory-intensive parameter caching. These costs are difficult to mitigate via simple model partitioning due to input-dependent expert activation. To address these issues, we propose Remoe, a heterogeneous MoE inference system tailored for serverless computing. Remoe assigns non-expert modules to GPUs and expert modules to CPUs, and further offloads infrequently activated experts to separate serverless functions to reduce memory overhead and enable parallel execution. We incorporate three key techniques: (1) a Similar Prompts Searching (SPS) algorithm to predict expert activation patterns based on semantic similarity of inputs; (2) a Main Model Pre-allocation (MMP) algorithm to ensure service-level objectives (SLOs) via worst-case memory estimation; and (3) a joint memory and replica optimization framework leveraging Lagrangian duality and the Longest Processing Time (LPT) algorithm. We implement Remoe on Kubernetes and evaluate it across multiple LLM benchmarks. Experimental results show that Remoe reduces inference cost by up to 57% and cold start latency by 47% compared to state-of-the-art baselines.

</details>


### [39] [A Real-Time Digital Twin for Adaptive Scheduling](https://arxiv.org/abs/2512.18894)
*Yihe Zhang,Yash Kurkure,Yiheng Tao,Michael E. Papka,Zhiling Lan*

Main category: cs.DC

TL;DR: SchedTwin是一个实时数字孪生系统，通过预测仿真动态优化HPC调度策略，显著优于静态策略且开销低。


<details>
  <summary>Details</summary>
Motivation: 传统HPC调度依赖静态启发式策略，难以应对日益多样化的作业特征。

Method: 周期性采集运行时事件，利用高保真离散事件模拟器快速评估多种策略，并动态选择最优方案。

Result: 在PBS调度器上实现并验证，SchedTwin持续优于常用静态策略，每调度周期仅需数秒开销。

Conclusion: 实时数字孪生为自适应HPC调度提供了实用有效的解决方案。

Abstract: High-performance computing (HPC) workloads are becoming increasingly diverse, exhibiting wide variability in job characteristics, yet cluster scheduling has long relied on static, heuristic-based policies. In this work we present SchedTwin, a real-time digital twin designed to adaptively guide scheduling decisions using predictive simulation. SchedTwin periodically ingests runtime events from the physical scheduler, performs rapid what-if evaluations of multiple policies using a high-fidelity discrete-event simulator, and dynamically selects the one satisfying the administrator configured optimization goal. We implement SchedTwin as an open-source software and integrate it with the production PBS scheduler. Preliminary results show that SchedTwin consistently outperforms widely used static scheduling policies, while maintaining low overhead (a few seconds per scheduling cycle). These results demonstrate that real-time digital twins offer a practical and effective path toward adaptive HPC scheduling.

</details>


### [40] [Evidential Trust-Aware Model Personalization in Decentralized Federated Learning for Wearable IoT](https://arxiv.org/abs/2512.19131)
*Murtaza Rangwala,Richard O. Sinnott,Rajkumar Buyya*

Main category: cs.DC

TL;DR: Murmura利用证据深度学习实现去中心化联邦学习中的信任感知模型个性化，显著提升非独立同分布数据下的性能与收敛速度。


<details>
  <summary>Details</summary>
Motivation: 解决去中心化联邦学习中因数据异构性导致的节点间协作兼容性问题，避免全局模型拟合不佳或启发式选择机制失效。

Method: 基于Dirichlet证据模型计算认知不确定性，通过本地验证样本交叉评估生成同伴兼容性分数，并采用自适应阈值的信任感知聚合机制实现个性化模型更新。

Result: 在三个可穿戴IoT数据集上，Murmura相比基线方法将IID到non-IID性能下降从19.3%降至0.9%，收敛速度提升7.4倍，且对超参数选择鲁棒。

Conclusion: 证据不确定性为去中心化异构环境中的兼容性感知个性化提供了理论基础和实用框架。

Abstract: Decentralized federated learning (DFL) enables collaborative model training across edge devices without centralized coordination, offering resilience against single points of failure. However, statistical heterogeneity arising from non-identically distributed local data creates a fundamental challenge: nodes must learn personalized models adapted to their local distributions while selectively collaborating with compatible peers. Existing approaches either enforce a single global model that fits no one well, or rely on heuristic peer selection mechanisms that cannot distinguish between peers with genuinely incompatible data distributions and those with valuable complementary knowledge. We present Murmura, a framework that leverages evidential deep learning to enable trust-aware model personalization in DFL. Our key insight is that epistemic uncertainty from Dirichlet-based evidential models directly indicates peer compatibility: high epistemic uncertainty when a peer's model evaluates local data reveals distributional mismatch, enabling nodes to exclude incompatible influence while maintaining personalized models through selective collaboration. Murmura introduces a trust-aware aggregation mechanism that computes peer compatibility scores through cross-evaluation on local validation samples and personalizes model aggregation based on evidential trust with adaptive thresholds. Evaluation on three wearable IoT datasets (UCI HAR, PAMAP2, PPG-DaLiA) demonstrates that Murmura reduces performance degradation from IID to non-IID conditions compared to baseline (0.9% vs. 19.3%), achieves 7.4$\times$ faster convergence, and maintains stable accuracy across hyperparameter choices. These results establish evidential uncertainty as a principled foundation for compatibility-aware personalization in decentralized heterogeneous environments.

</details>


### [41] [L4: Low-Latency and Load-Balanced LLM Serving via Length-Aware Scheduling](https://arxiv.org/abs/2512.19179)
*Yitao Yuan,Chenqi Zhao,Bohan Zhao,Zane Cao,Yongchao He,Wenfei Wu*

Main category: cs.DC

TL;DR: L4通过动态重调度请求和长度分组优化GPU利用率，显著降低延迟并提升吞吐量。


<details>
  <summary>Details</summary>
Motivation: 当前LLM推理调度器忽略批内请求长度差异，导致GPU利用率低和延迟增加。

Method: L4将实例按长度分组，采用动态规划算法优化QoE，并结合运行时范围细化与负载均衡。

Result: 相比现有系统，L4端到端延迟降低67%，尾部延迟降低69%，吞吐量提升2.89倍。

Conclusion: L4有效缓解了请求长度异构性带来的性能瓶颈，显著提升LLM服务效率。

Abstract: Efficiently harnessing GPU compute is critical to improving user experience and reducing operational costs in large language model (LLM) services. However, current inference engine schedulers overlook the attention backend's sensitivity to request-length heterogeneity within a batch. As state-of-the-art models now support context windows exceeding 128K tokens, this once-tolerable inefficiency has escalated into a primary system bottleneck, causing severe performance degradation through GPU underutilization and increased latency. We present L4, a runtime system that dynamically reschedules requests across multiple instances serving the same LLM to mitigate per-instance length heterogeneity. L4 partitions these instances into length-specialized groups, each handling requests within a designated length range, naturally forming a pipeline as requests flow through them. L4 devises a dynamic programming algorithm to efficiently find the stage partition with the best QoE, employs runtime range refinement together with decentralized load (re)balance both across and within groups, achieving a balanced and efficient multi-instance service. Our evaluation shows that, under the same configuration, L4 reduces end-to-end latency by up to 67% and tail latency by up to 69%, while improving overall system throughput by up to 2.89 times compared to the state-of-the-art multi-instance scheduling systems.

</details>


### [42] [Faster Distributed Inference-Only Recommender Systems via Bounded Lag Synchronous Collectives](https://arxiv.org/abs/2512.19342)
*Kiril Dichev,Filip Pawlowski,Albert-Jan Yzelman*

Main category: cs.DC

TL;DR: 提出一种新的有界滞后同步alltoallv操作，优化分布式DLRM推理性能。


<details>
  <summary>Details</summary>
Motivation: 解决当前分布式推荐系统中因同步通信导致的延迟瓶颈问题。

Method: 设计并实现有界滞后同步（BLS）版alltoallv，集成到PyTorch分布式后端，并在参考DLRM代码上评估。

Result: 在访问不均衡或进程延迟场景下，BLS显著提升推理延迟与吞吐量，最佳情况下可完全掩盖进程间延迟。

Conclusion: BLS技术在异构负载下有效优化DLRM推理效率，且不损失精度。

Abstract: Recommender systems are enablers of personalized content delivery, and therefore revenue, for many large companies. In the last decade, deep learning recommender models (DLRMs) are the de-facto standard in this field. The main bottleneck in DLRM inference is the lookup of sparse features across huge embedding tables, which are usually partitioned across the aggregate RAM of many nodes. In state-of-the-art recommender systems, the distributed lookup is implemented via irregular all-to-all (alltoallv) communication, and often presents the main bottleneck. Today, most related work sees this operation as a given; in addition, every collective is synchronous in nature. In this work, we propose a novel bounded lag synchronous (BLS) version of the alltoallv operation. The bound can be a parameter allowing slower processes to lag behind entire iterations before the fastest processes block. In special applications such as inference-only DLRM, the accuracy of the application is fully preserved. We implement BLS alltoallv in a new PyTorch Distributed backend and evaluate it with a BLS version of the reference DLRM code. We show that for well balanced, homogeneous-access DLRM runs our BLS technique does not offer notable advantages. But for unbalanced runs, e.g. runs with strongly irregular embedding table accesses or with delays across different processes, our BLS technique improves both the latency and throughput of inference-only DLRM. In the best-case scenario, the proposed reduced synchronisation can mask the delays across processes altogether.

</details>
