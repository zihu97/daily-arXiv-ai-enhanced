<div id=toc></div>

# Table of Contents

- [cs.DC](#cs.DC) [Total: 6]
- [cs.MA](#cs.MA) [Total: 2]
- [cs.SE](#cs.SE) [Total: 10]
- [cs.NI](#cs.NI) [Total: 1]
- [cs.AR](#cs.AR) [Total: 4]


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [1] [An Adaptive Distributed Stencil Abstraction for GPUs](https://arxiv.org/abs/2512.19851)
*Aditya Bhosale,Laxmikant Kale*

Main category: cs.DC

TL;DR: 提出一种基于CharmTyles的自适应分布式抽象，支持多节点GPU上的Stencil计算，具备NumPy语法并实现资源弹性与性能提升。


<details>
  <summary>Details</summary>
Motivation: 解决Python科学计算生态在现代超算上缺乏资源适应性与高性能执行能力的问题。

Method: 利用基于Charm++运行时的CharmTyles框架构建自适应分布式抽象，提供类NumPy接口，并支持动态节点扩展。

Result: 相比专用Stencil DSL和通用NumPy替代方案，显著提升性能，并验证了运行时资源伸缩的开销可控。

Conclusion: 该抽象弥合了原型开发与高性能计算之间的鸿沟，为科学计算提供了灵活高效的多节点GPU执行方案。

Abstract: The scientific computing ecosystem in Python is largely confined to single-node parallelism, creating a gap between high-level prototyping in NumPy and high-performance execution on modern supercomputers. The increasing prevalence of hardware accelerators and the need for energy efficiency have made resource adaptivity a critical requirement, yet traditional HPC abstractions remain rigid. To address these challenges, we present an adaptive, distributed abstraction for stencil computations on multi-node GPUs. This abstraction is built using CharmTyles, a framework based on the adaptive Charm++ runtime, and features a familiar NumPy-like syntax to minimize the porting effort from prototype to production code. We showcase the resource elasticity of our abstraction by dynamically rescaling a running application across a different number of nodes and present a performance analysis of the associated overheads. Furthermore, we demonstrate that our abstraction achieves significant performance improvements over both a specialized, high-performance stencil DSL and a generalized NumPy replacement.

</details>


### [2] [Scaling Point-based Differentiable Rendering for Large-scale Reconstruction](https://arxiv.org/abs/2512.20017)
*Hexu Zhao,Xiaoteng Liu,Xiwen Min,Jianhao Huang,Youming Deng,Yanfei Li,Ang Li,Jinyang Li,Aurojit Panda*

Main category: cs.DC

TL;DR: Gaian 是一个通用的分布式训练系统，专为点基可微渲染（PBDR）设计，通过优化数据局部性显著降低通信开销并提升训练吞吐量。


<details>
  <summary>Details</summary>
Motivation: 现有 PBDR 分布式系统耦合性强、通信开销大，亟需通用且高效的解决方案。

Method: 提出 Gaian 系统，提供统一 API 并利用数据访问信息优化局部性与通信效率。

Result: 在 4 种 PBDR 算法和 6 个数据集上验证，通信减少达 91%，训练吞吐量提升 1.50x-3.71x。

Conclusion: Gaian 在保持通用性的同时显著提升了 PBDR 的分布式训练效率。

Abstract: Point-based Differentiable Rendering (PBDR) enables high-fidelity 3D scene reconstruction, but scaling PBDR to high-resolution and large scenes requires efficient distributed training systems. Existing systems are tightly coupled to a specific PBDR method. And they suffer from severe communication overhead due to poor data locality. In this paper, we present Gaian, a general distributed training system for PBDR. Gaian provides a unified API expressive enough to support existing PBDR methods, while exposing rich data-access information, which Gaian leverages to optimize locality and reduce communication. We evaluated Gaian by implementing 4 PBDR algorithms. Our implementations achieve high performance and resource efficiency: across six datasets and up to 128 GPUs, it reduces communication by up to 91% and improves training throughput by 1.50x-3.71x.

</details>


### [3] [Population Protocols Revisited: Parity and Beyond](https://arxiv.org/abs/2512.20163)
*Leszek Gąsieniec,Tytus Grodzicki,Tomasz Jurdziński,Jakub Kowalski,Grzegorz Stachowiak*

Main category: cs.DC

TL;DR: 提出一种基于权重和时钟机制的新范式，实现高效、鲁棒的群体协议，首次在时间和空间上同时高效解决奇偶性和模运算问题。


<details>
  <summary>Details</summary>
Motivation: 填补群体协议中奇偶性及同余谓词缺乏时空高效解法的研究空白。

Method: 引入权重系统、鲁棒时钟与异常检测切换机制，构建多阶段稳定协议框架。

Result: 首个实现O(log³n)状态数与O(log³n)静默稳定时间的奇偶性与模m同余协议。

Conclusion: 该范式支持隐式一进制-二进制转换，可推广至子群体规模计算等其他问题，提升协议通用性与实用性。

Abstract: For nearly two decades, population protocols have been extensively studied, yielding efficient solutions for central problems in distributed computing, including leader election, and majority computation, a predicate type in Presburger Arithmetic closely tied to population protocols. Surprisingly, no protocols have achieved both time- and space-efficiency for congruency predicates, such as parity computation, which are complementary in this arithmetic framework. This gap highlights a significant challenge in the field. To address this gap, we explore the parity problem, where agents are tasked with computing the parity of the given sub-population size. Then we extend the solution for parity to compute congruences modulo an arbitrary $m$.
  Previous research on efficient population protocols has focused on protocols that minimise both stabilisation time and state utilisation for specific problems. In contrast, this work slightly relaxes this expectation, permitting protocols to place less emphasis on full optimisation and more on universality, robustness, and probabilistic guarantees. This allows us to propose a novel computing paradigm that integrates population weights (or simply weights), a robust clocking mechanism, and efficient anomaly detection coupled with a switching mechanism (which ensures slow but always correct solutions). This paradigm facilitates universal design of efficient multistage stable population protocols. Specifically, the first efficient parity and congruence protocols introduced here use both $O(\log^3 n)$ states and achieve silent stabilisation in $O(\log^3 n)$ time. We conclude by discussing the impact of implicit conversion between unary and binary representations enabled by the weight system, with applications to other problems, including the computation and representation of (sub-)population sizes.

</details>


### [4] [Reaching Agreement Among Reasoning LLM Agents](https://arxiv.org/abs/2512.20184)
*Chaoyi Ruan,Yiliang Wang,Ziji Shi,Jialin Li*

Main category: cs.DC

TL;DR: Aegean协议通过共识机制优化多智能体推理，显著降低延迟并保证正确性。


<details>
  <summary>Details</summary>
Motivation: 现有静态启发式工作流浪费资源、延迟高且易受瞬态协议影响，需形式化基础提升可靠性。

Method: 提出多智能体精炼问题的形式模型，并设计适用于随机推理智能体的Aegean共识协议及配套引擎Aegean-Serve。

Result: 在四个数学推理基准上，Aegean将延迟降低1.2-20倍，答案质量损失不超过2.5%，且在本地GPU和商业API中均有效。

Conclusion: 基于共识的编排能消除拖尾延迟，同时不牺牲正确性，为多智能体系统提供可靠形式化基础。

Abstract: Multi-agent systems have extended the capability of agentic AI. Instead of single inference passes, multiple agents perform collective reasoning to derive high quality answers. However, existing multi-agent orchestration relies on static heuristic workflows such as fixed loop limits and barrier synchronization. These ad-hoc approaches waste computational resources, incur high latency due to stragglers, and risk finalizing transient agreements. We argue that reliable multi-agent reasoning requires a formal foundation analogous to classical distributed consensus problem.
  To that end, we propose a formal model of the multi-agent refinement problem. The model includes definitions of the correctness guarantees and formal semantics of agent reasoning. We then introduce Aegean, a consensus protocol designed for stochastic reasoning agents that solves multi-agent refinement. We implement the protocol in Aegean-Serve, a consensus-aware serving engine that performs incremental quorum detection across concurrent agent executions, enabling early termination when sufficient agents converge. Evaluation using four mathematical reasoning benchmarks shows that Aegean provides provable safety and liveness guarantees while reducing latency by 1.2--20$\times$ compared to state-of-the-art baselines, maintaining answer quality within 2.5%. Consistent gains across both local GPU deployments and commercial API providers validate that consensus-based orchestration eliminates straggler delays without sacrificing correctness.

</details>


### [5] [Predictive-LoRA: A Proactive and Fragmentation-Aware Serverless Inference System for LLMs](https://arxiv.org/abs/2512.20210)
*Yinan Ni,Xiao Yang,Yuqi Tang,Zhimin Qiu,Chen Wang,Tingzhou Yuan*

Main category: cs.DC

TL;DR: P-LoRA 是一种面向 LoRA 微调模型的无服务器推理系统，通过预测预加载和分页内存管理显著降低冷启动延迟并提升 GPU 利用率。


<details>
  <summary>Details</summary>
Motivation: 解决无服务器环境下多 LoRA 模型推理时的冷启动延迟与显存碎片问题。

Method: 引入 LSTM 流量预测器主动预加载适配器，并采用类操作系统的分页机制管理显存。

Result: 在 Azure 工作负载下，吞吐量提升 1.52 倍，TTFT 降低 35%，GPU 利用率维持在 87% 以上。

Conclusion: P-LoRA 有效优化了无服务器架构中 LoRA 模型的推理效率与资源利用率。

Abstract: The serverless computing paradigm offers compelling advantages for deploying Large Language Model (LLM) inference services, including elastic scaling and pay-per-use billing. However, serving multiple fine-tuned LLMs via Low-Rank Adaptation (LoRA) in serverless environments faces critical challenges: reactive adapter loading causes significant cold start latency, and frequent adapter swapping leads to severe GPU memory fragmentation. In this paper, we present Predictive-LoRA (P-LoRA), a proactive and fragmentation-aware serverless inference system for LoRA-based LLMs. P-LoRA introduces two key innovations: (1) a lightweight LSTM-based traffic predictor that forecasts adapter demand and proactively prefetches hot adapters from host memory to GPU, reducing cold start latency by up to 68%; and (2) a page-based adapter memory management mechanism inspired by operating system virtual memory, which keeps GPU memory utilization above 87% even under heterogeneous adapter ranks. We evaluate P-LoRA using production-like workloads derived from the Azure Functions trace. Experimental results demonstrate that P-LoRA achieves 1.52x higher throughput than S-LoRA while reducing the average Time-To-First-Token (TTFT) by 35% under high concurrency scenarios.

</details>


### [6] [WOC: Dual-Path Weighted Object Consensus Made Efficient](https://arxiv.org/abs/2512.20485)
*Tanisha Fonseca,Gengrui Zhang*

Main category: cs.DC

TL;DR: WOC是一种双路径共识协议，通过动态路由操作提升吞吐量，兼顾节点异构性和工作负载独立性。


<details>
  <summary>Details</summary>
Motivation: 现有共识协议无法同时优化节点异构性和工作负载独立性，导致性能瓶颈。

Method: 设计双路径机制：独立操作走快速路径（对象加权法定人数），冲突操作走慢速路径（节点加权共识）。

Result: 在>70%独立对象的工作负载下，WOC吞吐量比Cabinet高4倍，高争用场景性能相当。

Conclusion: WOC有效平衡了并行性和节点异构性，显著提升分布式系统性能。

Abstract: Modern distributed systems face a critical challenge: existing consensus protocols optimize for either node heterogeneity or workload independence, but not both. For example, Cabinet leverages weighted quorums to handle node heterogeneity but serializes all operations through a global leader, limiting parallelism. EPaxos enables parallel execution for independent operations but treats all nodes uniformly, ignoring performance differences. To tackle this problem, we present WOC, a dual-path consensus protocol that dynamically routes operations into two paths based on their access patterns. Independent operations execute through a fast path that uses object-specific weighted quorums and completes in one network round-trip. Conflicting or shared objects route through a leader-coordinated slow path employing node-weighted consensus. Our evaluation demonstrates that WOC achieves up to 4X higher throughput than Cabinet for workloads with >70% independent objects, while maintaining equivalent performance under high contention.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [7] [A Multi-Agent Retrieval-Augmented Framework for Work-in-Progress Predictio](https://arxiv.org/abs/2512.19841)
*Yousef Mehrdad Bibalan,Behrouz Far,Mohammad Moshirpour,Bahareh Ghiyasian*

Main category: cs.MA

TL;DR: 本文提出一种结合检索增强生成与多智能体推理的框架，用于提升工作进展预测的准确性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 为更精准预判工作负载波动并优化运营规划，需改进现有预测方法的上下文利用与推理能力。

Method: 通过将事件日志转为自然语言故事并嵌入语义向量记忆库，结合多个预测智能体与决策辅助智能体，最终由融合智能体以ReAct推理整合结果。

Result: 在两个真实数据集上评估，MAPE达1.50%，优于TCN、LSTM和持续基线模型，展现更强鲁棒性。

Conclusion: 集成检索机制与多智能体推理能有效提升工作进展预测性能。

Abstract: Work-in-Progress (WiP) prediction is critical for predictive process monitoring, enabling accurate anticipation of workload fluctuations and optimized operational planning. This paper proposes a retrieval-augmented, multi-agent framework that combines retrieval-augmented generation (RAG) and collaborative multi-agent reasoning for WiP prediction. The narrative generation component transforms structured event logs into semantically rich natural language stories, which are embedded into a semantic vector-based process memory to facilitate dynamic retrieval of historical context during inference. The framework includes predictor agents that independently leverage retrieved historical contexts and a decision-making assistant agent that extracts high-level descriptive signals from recent events. A fusion agent then synthesizes predictions using ReAct-style reasoning over agent outputs and retrieved narratives. We evaluate our framework on two real-world benchmark datasets. Results show that the proposed retrieval-augmented multi-agent approach achieves competitive prediction accuracy, obtaining a Mean Absolute Percentage Error (MAPE) of 1.50\% on one dataset, and surpassing Temporal Convolutional Networks (TCN), Long Short-Term Memory (LSTM), and persistence baselines. The results highlight improved robustness, demonstrating the effectiveness of integrating retrieval mechanisms and multi-agent reasoning in WiP prediction.

</details>


### [8] [When Natural Strategies Meet Fuzziness and Resource-Bounded Actions (Extended Version)](https://arxiv.org/abs/2512.20457)
*Marco Aruta,Francesco Improta,Vadim Malvone,Aniello Murano*

Main category: cs.MA

TL;DR: 本文提出HumanATLF逻辑，结合模糊语义与资源约束动作，用于更贴近人类决策的多智能体系统形式化建模与验证。


<details>
  <summary>Details</summary>
Motivation: 现有MAS形式化推理假设过于理想化，忽略人类决策中的策略简洁性、行动成本及感知不确定性。

Method: 扩展自然策略框架，引入模糊语义和资源预算约束，定义HumanATLF逻辑并实现模型检测算法。

Result: 证明不同参数下模型检测复杂度分别为P、NP完全和Delta^P_2完全；基于记忆的策略可在PSPACE内判定，并通过VITAMIN工具在无人机救援场景验证有效性。

Conclusion: HumanATLF能更真实刻画人类决策行为，为资源受限与模糊环境下的MAS提供实用的形式化分析工具。

Abstract: In formal strategic reasoning for Multi-Agent Systems (MAS), agents are typically assumed to (i) employ arbitrarily complex strategies, (ii) execute each move at zero cost, and (iii) operate over fully crisp game structures. These idealized assumptions stand in stark contrast with human decision making in real world environments. The natural strategies framework along with some of its recent variants, partially addresses this gap by restricting strategies to concise rules guarded by regular expressions. Yet, it still overlook both the cost of each action and the uncertainty that often characterizes human perception of facts over the time. In this work, we introduce HumanATLF, a logic that builds upon natural strategies employing both fuzzy semantics and resource bound actions: each action carries a real valued cost drawn from a non refillable budget, and atomic conditions and goals have degrees in [0,1]. We give a formal syntax and semantics, and prove that model checking is in P when both the strategy complexity k and resource budget b are fixed, NP complete if just one strategic operator over Boolean objectives is allowed, and Delta^P_2 complete when k and b vary. Moreover, we show that recall based strategies can be decided in PSPACE. We implement our algorithms in VITAMIN, an open source model checking tool for MAS and validate them on an adversarial resource aware drone rescue scenario.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [9] [Attention Distance: A Novel Metric for Directed Fuzzing with Large Language Models](https://arxiv.org/abs/2512.19758)
*Wang Bin,Ao Yang,Kedan Li,Aofan Liu,Hui Li,Guibo Luo,Weixiang Huang,Yan Zhuang*

Main category: cs.SE

TL;DR: 提出注意力距离度量方法，利用大语言模型提升定向灰盒模糊测试效率。


<details>
  <summary>Details</summary>
Motivation: 现有定向灰盒模糊测试仅依赖物理路径距离，忽略代码逻辑关联，导致复杂二进制中引导冗余或误导。

Method: 引入基于大语言模型上下文分析的‘注意力距离’，计算代码元素间内在关联得分，替代传统物理距离。

Result: 在38个真实漏洞复现实验中，相较AFLGo提升3.43倍效率，优于DAFL和WindRanger达2.89倍和7.13倍；集成至后两者亦持续增强性能。

Conclusion: 注意力距离显著提升定向模糊测试效果，具备良好泛化能力，相关代码已开源。

Abstract: In the domain of software security testing, Directed Grey-Box Fuzzing (DGF) has garnered widespread attention for its efficient target localization and excellent detection performance. However, existing approaches measure only the physical distance between seed execution paths and target locations, overlooking logical relationships among code segments. This omission can yield redundant or misleading guidance in complex binaries, weakening DGF's real-world effectiveness. To address this, we introduce \textbf{attention distance}, a novel metric that leverages a large language model's contextual analysis to compute attention scores between code elements and reveal their intrinsic connections. Under the same AFLGo configuration -- without altering any fuzzing components other than the distance metric -- replacing physical distances with attention distances across 38 real vulnerability reproduction experiments delivers a \textbf{3.43$\times$} average increase in testing efficiency over the traditional method. Compared to state-of-the-art directed fuzzers DAFL and WindRanger, our approach achieves \textbf{2.89$\times$} and \textbf{7.13$\times$} improvements, respectively. To further validate the generalizability of attention distance, we integrate it into DAFL and WindRanger, where it also consistently enhances their original performance. All related code and datasets are publicly available at https://github.com/TheBinKing/Attention\_Distance.git.

</details>


### [10] [Larger Is Not Always Better: Leveraging Structured Code Diffs for Comment Inconsistency Detection](https://arxiv.org/abs/2512.19883)
*Phong Nguyen,Anh M. T. Bui,Phuong T. Nguyen*

Main category: cs.SE

TL;DR: 提出基于CodeT5+的即时检测方法，通过分解代码变更活动提升注释一致性检测效果。


<details>
  <summary>Details</summary>
Motivation: 现有方法忽略代码演化的结构复杂性且存在隐私与资源问题，需更高效准确的检测方案。

Method: 将代码变更分解为替换、删除、添加等有序活动序列，利用CodeT5+建模其与注释间的语义关联。

Result: 在JITDATA和CCIBENCH数据集上F1分数最高提升13.54%，优于多个微调LLM模型4.18%-10.94%。

Conclusion: 该方法能更有效捕捉代码变更与注释间关系，显著提升注释不一致检测性能。

Abstract: Ensuring semantic consistency between source code and its accompanying comments is crucial for program comprehension, effective debugging, and long-term maintainability. Comment inconsistency arises when developers modify code but neglect to update the corresponding comments, potentially misleading future maintainers and introducing errors. Recent approaches to code-comment inconsistency (CCI) detection leverage Large Language Models (LLMs) and rely on capturing the semantic relationship between code changes and outdated comments. However, they often ignore the structural complexity of code evolution, including historical change activities, and introduce privacy and resource challenges. In this paper, we propose a Just-In-Time CCI detection approach built upon the CodeT5+ backbone. Our method decomposes code changes into ordered sequences of modification activities such as replacing, deleting, and adding to more effectively capture the correlation between these changes and the corresponding outdated comments. Extensive experiments conducted on publicly available benchmark datasets-JITDATA and CCIBENCH--demonstrate that our proposed approach outperforms recent state-of-the-art models by up to 13.54% in F1-Score and achieves an improvement ranging from 4.18% to 10.94% over fine-tuned LLMs including DeepSeek-Coder, CodeLlama and Qwen2.5-Coder.

</details>


### [11] [Neuron-Guided Interpretation of Code LLMs: Where, Why, and How?](https://arxiv.org/abs/2512.19980)
*Zhe Yin,Xiaodong Gu,Beijun Shen*

Main category: cs.SE

TL;DR: 本文研究代码语言模型的神经元可解释性，发现语言特定神经元和跨语言概念层，并在多语言任务中验证其效用。


<details>
  <summary>Details</summary>
Motivation: 现有自然语言处理中的神经元可解释性方法不适用于编程语言，需针对代码特性进行专门分析。

Method: 分析Llama-3.1-8B与Qwen2.5-Coder-32B在C++、Java、Python、Go和JavaScript上的神经元选择性及层次贡献。

Result: 发现语言专用神经元与通用神经元并存；低层编码语法，中层形成跨语言语义抽象的概念层，并在三项任务中取得稳定提升。

Conclusion: 代码语言模型内部存在结构化可解释机制，概念层可用于指导微调、克隆检测与摘要迁移，提升多语言性能。

Abstract: Code language models excel on code intelligence tasks, yet their internal interpretability is underexplored. Existing neuron interpretability techniques from NLP are suboptimal for source code due to programming languages formal, hierarchical, and executable nature. We empirically investigate code LLMs at the neuron level, localizing language-specific neurons (selectively responsive to one language) and concept layers (feed-forward layers encoding language-agnostic code representations). We analyze Llama-3.1-8B and Qwen2.5-Coder-32B on multilingual inputs in C++, Java, Python, Go, and JavaScript, measuring neuron selectivity and layerwise contributions during generation. We find (1) neurons specialized for individual languages alongside a universal subset supporting general-purpose generation; and (2) lower layers mainly encode language-specific syntax, while middle layers capture semantic abstractions shared across languages, emerging as concept layers. We demonstrate utility on three tasks: neuron-guided fine-tuning for code generation, clone detection via concept-layer embeddings, and concept-layer-guided transfer for code summarization, each yielding consistent gains in multilingual settings.

</details>


### [12] [Detecting Non-Optimal Decisions of Embodied Agents via Diversity-Guided Metamorphic Testing](https://arxiv.org/abs/2512.20083)
*Wenzhao Wu,Yahui Tang,Mingfei Cheng,Wenbing Tang,Yuan Zhou,Yang Liu*

Main category: cs.SE

TL;DR: 提出NoD-DGMT框架，通过多样性引导的蜕变测试检测具身智能体任务规划中的非最优决策，显著提升检测率与覆盖多样性。


<details>
  <summary>Details</summary>
Motivation: 当前评估方法忽视规划方案的非功能性最优性，导致资源浪费和性能下降，亟需系统化检测非最优决策问题。

Method: 设计四类新颖蜕变关系捕捉最优性属性，并结合多样性引导策略主动选择测试用例，提升检测效率与覆盖率。

Result: 在AI2-THOR仿真器上实验表明，NoD-DGMT平均检测率达31.9%，较最佳基线提升16.8%，多样性评分提高3.3。

Conclusion: NoD-DGMT能有效识别具身智能体规划中的低效行为，为资源受限场景提供可靠的质量保障机制。

Abstract: As embodied agents advance toward real-world deployment, ensuring optimal decisions becomes critical for resource-constrained applications. Current evaluation methods focus primarily on functional correctness, overlooking the non-functional optimality of generated plans. This gap can lead to significant performance degradation and resource waste. We identify and formalize the problem of Non-optimal Decisions (NoDs), where agents complete tasks successfully but inefficiently. We present NoD-DGMT, a systematic framework for detecting NoDs in embodied agent task planning via diversity-guided metamorphic testing. Our key insight is that optimal planners should exhibit invariant behavioral properties under specific transformations. We design four novel metamorphic relations capturing fundamental optimality properties: position detour suboptimality, action optimality completeness, condition refinement monotonicity, and scene perturbation invariance. To maximize detection efficiency, we introduce a diversity-guided selection strategy that actively selects test cases exploring different violation categories, avoiding redundant evaluations while ensuring comprehensive diversity coverage. Extensive experiments on the AI2-THOR simulator with four state-of-the-art planning models demonstrate that NoD-DGMT achieves violation detection rates of 31.9% on average, with our diversity-guided filter improving rates by 4.3% and diversity scores by 3.3 on average. NoD-DGMT significantly outperforms six baseline methods, with 16.8% relative improvement over the best baseline, and demonstrates consistent superiority across different model architectures and task complexities.

</details>


### [13] [AXIOM: Benchmarking LLM-as-a-Judge for Code via Rule-Based Perturbation and Multisource Quality Calibration](https://arxiv.org/abs/2512.20159)
*Ruiqi Wang,Xinchen Wang,Cuiyun Gao,Chun Yong Chong,Xin Xia,Qing Liao*

Main category: cs.SE

TL;DR: 提出AXIOM框架，通过扰动和多源校准构建高质量、平衡分布的代码评估基准。


<details>
  <summary>Details</summary>
Motivation: 现有代码评估基准存在标签粗糙、主观性强、数据分布不平衡等问题，难以可靠评估LLM生成代码的质量。

Method: 采用规则引导扰动与多源质量校准两阶段方法，利用LLM对高质量程序施加可控扰动生成不同质量等级样本，并校准评分一致性。

Result: 成功构建了覆盖多种质量层级、分布均衡且标注可靠的代码评估基准，提升评估指标的有效性。

Conclusion: AXIOM为大规模合成高质量代码评估基准提供了系统化解决方案，推动LLM代码评估研究迈向更可靠、实用的方向。

Abstract: Large language models (LLMs) have been increasingly deployed in real-world software engineering, fostering the development of code evaluation metrics to study the quality of LLM-generated code. Conventional rule-based metrics merely score programs based on their surface-level similarities with reference programs instead of analyzing functionality and code quality in depth. To address this limitation, researchers have developed LLM-as-a-judge metrics, prompting LLMs to evaluate and score code, and curated various code evaluation benchmarks to validate their effectiveness. However, these benchmarks suffer from critical limitations, hindering reliable assessments of evaluation capability: Some feature coarse-grained binary labels, which reduce rich code behavior to a single bit of information, obscuring subtle errors. Others propose fine-grained but subjective, vaguely-defined evaluation criteria, introducing unreliability in manually-annotated scores, which is the ground-truth they rely on. Furthermore, they often use uncontrolled data synthesis methods, leading to unbalanced score distributions that poorly represent real-world code generation scenarios.
  To curate a diverse benchmark with programs of well-balanced distributions across various quality levels and streamline the manual annotation procedure, we propose AXIOM, a novel perturbation-based framework for synthesizing code evaluation benchmarks at scale. It reframes program scores as the refinement effort needed for deployment, consisting of two stages: (1) Rule-guided perturbation, which prompts LLMs to apply sequences of predefined perturbation rules to existing high-quality programs to modify their functionality and code quality, enabling us to precisely control each program's target score to achieve balanced score distributions. (2) Multisource quality calibration, which first selects a subset of...

</details>


### [14] [Well Begun is Half Done: Location-Aware and Trace-Guided Iterative Automated Vulnerability Repair](https://arxiv.org/abs/2512.20203)
*Zhenlei Ye,Xiaobing Sun,Sicong Cao,Lili Bo,Bin Li*

Main category: cs.SE

TL;DR: 提出一种基于大语言模型的漏洞修复方法，通过定位修复位置并评估补丁质量提升修复效果。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型漏洞修复方法忽视修复位置定位和补丁质量评估。

Method: 引入补丁位置提示与双维度质量评估（新漏洞引入风险、污点语句覆盖率），迭代优化修复过程。

Result: 在VulnLoc+数据集上生成27个合理补丁，比基线多修复8至13个漏洞。

Conclusion: 该方法显著优于现有翻译型、程序分析型及LLM型漏洞修复技术。

Abstract: The advances of large language models (LLMs) have paved the way for automated software vulnerability repair approaches, which iteratively refine the patch until it becomes plausible. Nevertheless, existing LLM-based vulnerability repair approaches face notable limitations: 1) they ignore the concern of locations that need to be patched and focus solely on the repair content. 2) they lack quality assessment for generated candidate patches in the iterative process.
  To tackle the two limitations, we propose \sysname, an LLM-based approach that provides information about where should be patched first. Furthermore, \sysname improves the iterative repair strategy by assessing the quality of test-failing patches and selecting the best patch for the next iteration. We introduce two dimensions to assess the quality of patches: whether they introduce new vulnerabilities and the taint statement coverage. We evaluated \sysname on a real-world C/C++ vulnerability repair dataset VulnLoc+, which contains 40 vulnerabilities and their Proofs-of-Vulnerability. The experimental results demonstrate that \sysname exhibits substantial improvements compared with the Neural Machine Translation-based, Program Analysis-based, and LLM-based state-of-the-art vulnerability repair approaches. Specifically, \sysname is able to generate 27 plausible patches, which is comparable to or even 8 to 22 more plausible patches than the baselines. In terms of correct patch generation, \sysname repairs 8 to 13 additional vulnerabilities compared with existing approaches.

</details>


### [15] [Toward Explaining Large Language Models in Software Engineering Tasks](https://arxiv.org/abs/2512.20328)
*Antonio Vitale,Khai-Nguyen Nguyen,Denys Poshyvanyk,Rocco Oliveto,Simone Scalabrino,Antonio Mastropaolo*

Main category: cs.SE

TL;DR: FeatureSHAP 是首个面向软件工程任务的自动化、模型无关的可解释性框架，基于 Shapley 值提供高保真解释，提升开发者对 LLM 输出的理解与决策能力。


<details>
  <summary>Details</summary>
Motivation: LLM 在软件工程中广泛应用，但其黑盒特性阻碍了在关键领域的可信部署，亟需契合开发者思维的领域专属解释方法。

Method: 基于 Shapley 值，通过系统性输入扰动和任务特定相似度比较，将模型输出归因于高层输入特征，兼容开源与闭源 LLM。

Result: 在代码生成与摘要任务中，FeatureSHAP 能降低无关特征权重，解释保真度优于基线；37 名从业者调查表明其有效辅助理解与决策。

Conclusion: FeatureSHAP 为软件工程中的实用化可解释 AI 提供了重要进展，推动 LLM 在高风险场景下的可信应用。

Abstract: Recent progress in Large Language Models (LLMs) has substantially advanced the automation of software engineering (SE) tasks, enabling complex activities such as code generation and code summarization. However, the black-box nature of LLMs remains a major barrier to their adoption in high-stakes and safety-critical domains, where explainability and transparency are vital for trust, accountability, and effective human supervision. Despite increasing interest in explainable AI for software engineering, existing methods lack domain-specific explanations aligned with how practitioners reason about SE artifacts. To address this gap, we introduce FeatureSHAP, the first fully automated, model-agnostic explainability framework tailored to software engineering tasks. Based on Shapley values, FeatureSHAP attributes model outputs to high-level input features through systematic input perturbation and task-specific similarity comparisons, while remaining compatible with both open-source and proprietary LLMs. We evaluate FeatureSHAP on two bi-modal SE tasks: code generation and code summarization. The results show that FeatureSHAP assigns less importance to irrelevant input features and produces explanations with higher fidelity than baseline methods. A practitioner survey involving 37 participants shows that FeatureSHAP helps practitioners better interpret model outputs and make more informed decisions. Collectively, FeatureSHAP represents a meaningful step toward practical explainable AI in software engineering. FeatureSHAP is available at https://github.com/deviserlab/FeatureSHAP.

</details>


### [16] [Comment Traps: How Defective Commented-out Code Augment Defects in AI-Assisted Code Generation](https://arxiv.org/abs/2512.20334)
*Yuan Huang,Yukang Zhou,Xiangping Chen,Zibin Zheng*

Main category: cs.SE

TL;DR: 缺陷注释代码显著影响AI编程助手生成代码的质量，即使明确指示忽略，缺陷率仍高达58.17%，凸显提升其鲁棒性与安全性的必要性。


<details>
  <summary>Details</summary>
Motivation: 研究旨在揭示注释代码中的缺陷如何影响AI编程助手的代码生成行为，弥补现有研究对注释缺陷影响的忽视。

Method: 通过实验评估GitHub Copilot和Cursor在包含缺陷注释代码上下文中的表现，分析其生成代码的缺陷率及行为模式。

Result: 缺陷注释代码使AI助手生成缺陷代码的比例高达58.17%；即使给予忽略指令，缺陷减少不超过21.84%；工具会主动推理补全缺陷模式。

Conclusion: 当前AI编程助手对注释缺陷缺乏足够鲁棒性，亟需改进其安全性与抗干扰能力以保障代码质量。

Abstract: With the rapid development of large language models in code generation, AI-powered editors such as GitHub Copilot and Cursor are revolutionizing software development practices. At the same time, studies have identified potential defects in the generated code. Previous research has predominantly examined how code context influences the generation of defective code, often overlooking the impact of defects within commented-out code (CO code). AI coding assistants' interpretation of CO code in prompts affects the code they generate.
  This study evaluates how AI coding assistants, GitHub Copilot and Cursor, are influenced by defective CO code. The experimental results show that defective CO code in the context causes AI coding assistants to generate more defective code, reaching up to 58.17 percent. Our findings further demonstrate that the tools do not simply copy the defective code from the context. Instead, they actively reason to complete incomplete defect patterns and continue to produce defective code despite distractions such as incorrect indentation or tags. Even with explicit instructions to ignore the defective CO code, the reduction in defects does not exceed 21.84 percent. These findings underscore the need for improved robustness and security measures in AI coding assistants.

</details>


### [17] [A Comprehensive Study of Bugs in Modern Distributed Deep Learning Systems](https://arxiv.org/abs/2512.20345)
*Xiaoxue Ma,Wanwei Zhan,Jiale Chen,Yishu Li,Jacky Keung,Federica Sarro*

Main category: cs.SE

TL;DR: 本文首次对专用分布式深度学习框架中的实践者挑战进行了大规模实证分析，构建了包含34种症状、28个根本原因和6种修复模式的分类体系，并提出针对性建议。


<details>
  <summary>Details</summary>
Motivation: 通用框架的分布式功能多为附加特性，手动实现高级并行计算成本高，亟需深入理解专用分布式框架中的问题与解决方案。

Method: 分析来自DeepSpeed、Megatron-LM和Colossal-AI的849个真实问题，构建症状-原因-修复映射关系，并按训练阶段系统归类。

Result: 45.1%的症状为分布式特有，通信设置阶段95%的问题仅出现在分布式环境；超60%问题可通过版本依赖管理及通信调优解决。

Conclusion: 研究揭示了分布式深度学习框架的核心痛点，为开发者优化工具链、提升稳定性与易用性提供了实证依据和行动指南。

Abstract: In today's data-driven era, deep learning is vital for processing massive datasets, yet single-device training is constrained by computational and memory limits. Distributed deep learning overcomes these challenges by leveraging multiple GPUs or machines in parallel. While general-purpose frameworks (e.g., TensorFlow and PyTorch) provide distributed capabilities, these are often add-on features that demand significant manual effort for advanced parallelism, underscoring the need for specialized frameworks. This study conducts the first large-scale empirical analysis of practitioner challenges in dedicated distributed frameworks. We examine 849 real-world issues from DeepSpeed, Megatron-LM, and Colossal-AI and construct a taxonomy of 34 bug symptoms, 28 root causes, and 6 fix patterns. Crucially, we establish explicit mappings between symptoms, causes, and fixes across distributed training stages, enabling a systematic understanding of how issues emerge and are resolved. Our results show that 45.1\% of bug symptoms are unique to distributed frameworks, with setup failures, memory issues, and performance anomalies being the most prevalent. Moreover, 95\% of issues in the communication setup stage occur exclusively in distributed contexts. We also find over 60\% of cases can be resolved through version and dependency management, and distributed feature, API, and communication tuning. Based on these findings, we provide actionable implications.

</details>


### [18] [Identifying Appropriately-Sized Services with Deep Reinforcement Learning](https://arxiv.org/abs/2512.20381)
*Syeda Tasnim Fabiha,Saad Shafiq,Wesley Klewerton Guez Assunção,Nenad Medvidović*

Main category: cs.SE

TL;DR: Rake是一种基于深度强化学习的服务分解方法，能从源代码和文档中自动识别合适大小的服务，无需依赖项目人员或特定文档，且支持自定义目标函数平衡模块化质量和业务能力对齐。


<details>
  <summary>Details</summary>
Motivation: 现有服务划分方法常依赖文档、人员知识或预设服务数量，在实际场景中难以适用，亟需自动化、语言无关的解决方案。

Method: 提出Rake方法，利用强化学习从实现构件（如源码）出发，在方法级别引导服务分解，并支持定制目标函数以兼顾模块质量与业务能力匹配。

Result: 在四个开源遗留项目上验证，Rake相比现有方法平均提升模块化质量7-14%，业务能力对齐度18-22%；同时发现仅优化业务上下文会损害紧耦合系统的分解质量。

Conclusion: Rake提供了一种实用、自动化、可定制的服务划分方案，有效应对现实约束，强调平衡模块化与业务目标的重要性。

Abstract: Service-based architecture (SBA) has gained attention in industry and academia as a means to modernize legacy systems. It refers to a design style that enables systems to be developed as suites of small, loosely coupled, and autonomous components (services) that encapsulate functionality and communicate via language-agnostic APIs. However, defining appropriately sized services that capture cohesive subsets of system functionality remains challenging. Existing work often relies on the availability of documentation, access to project personnel, or a priori knowledge of the target number of services, assumptions that do not hold in many real-world scenarios. Our work addresses these limitations using a deep reinforcement learning-based approach to identify appropriately sized services directly from implementation artifacts. We present Rake, a reinforcement learning-based technique that leverages available system documentation and source code to guide service decomposition at the level of implementation methods. Rake does not require specific documentation or access to project personnel and is language-agnostic. It also supports a customizable objective function that balances modularization quality and business capability alignment, i.e., the degree to which a service covers the targeted business capability. We applied Rake to four open-source legacy projects and compared it with two state-of-the-art techniques. On average, Rake achieved 7-14 percent higher modularization quality and 18-22 percent stronger business capability alignment. Our results further show that optimizing solely for business context can degrade decomposition quality in tightly coupled systems, highlighting the need for balanced objectives.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [19] [Smoothing Rough Edges of IPv6 in VPNs](https://arxiv.org/abs/2512.19698)
*Yejin Cho,John Heidemann*

Main category: cs.NI

TL;DR: 该论文揭示了商业VPN在处理IPv6时的两大问题：IPv4-only VPN泄露IPv6流量，以及双栈VPN用户常被迫使用IPv4。作者提出新IPv6地址范围方案以解决优先级问题。


<details>
  <summary>Details</summary>
Motivation: 提升用户隐私保护，解决商业VPN在IPv6支持中的实际缺陷。

Method: 基于12.9万真实用户访问数据进行实证分析，并在Android与Linux平台测试验证。

Result: 发现12款所谓安全的VPN仍存在至少5%用户IPv6泄露；57% IPv4-only用户暴露原生IPv6地址；多数双栈VPN因地址选择规则导致IPv6被降级。

Conclusion: 通过定义专用IPv6地址范围并原型实现，可有效解决VPN中IPv6被系统降级的问题，推动更完善的IPv6支持。

Abstract: How do commercial VPNs interact with IPv6? We show two "rough edges" in how commercial VPNs handle IPv6. First, we show that many IPv4-only VPNs leak IPv6 traffic to the ISP. Individual use VPNs in part to conceal their local IP addresses, so such leaks reduce user privacy. While prior work has studied VPNs in testbeds, we use a new dataset of 129k VPN-using daily visitors to WhatIsMyIPAddress.com that quantifies these leaks and show 12 VPNs previously considered safe still leak for at least 5% of their users. We show native IPv6 addresses leak most commonly in VPNs that claim only IPv4 support, with 5% to 57% of visitors of v4-only VPNs having their native IPv6 address exposed. Second, we show that most dual-stack VPNs users actually select IPv4 instead of IPv6. We observe this problem in our visitor data, and we identify the root cause arises because when user's computer follows standard address-selection rules, VPN-assigned addresses are often de-preferenced. Testing six VPNs on Android, we show that five consistently de-prioritize IPv6. Finally, we suggest a solution to IPv6 de-preferencing: we define a new IPv6 address range for VPNs that is not de-preferenced by address selection. We prototype this solution on Linux. Our findings help identify and address rough edges in the addition of IPv6 support to VPNs.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [20] [3D Stack In-Sensor-Computing (3DS-ISC): Accelerating Time-Surface Construction for Neuromorphic Event Cameras](https://arxiv.org/abs/2512.20073)
*Hongyang Shang,Shuai Dong,Ye Ke,Arindam Basu*

Main category: cs.AR

TL;DR: 提出3D堆叠感内计算架构，实现高效事件视觉处理，显著降低功耗与面积并保持高精度。


<details>
  <summary>Details</summary>
Motivation: 解决传统2D架构内存墙问题，提升事件相机实时处理效率与能效。

Method: 利用DRAM漏电特性进行时间戳归一化，结合定制电容与低漏开关，构建3D感内计算架构。

Result: 相比2D方案，功耗、延迟、面积分别降低69倍、2.2倍、1.9倍；在多个数据集上分类准确率接近SOTA，图像重建SSIM达0.62。

Conclusion: 3D-ISC为实时低功耗事件视觉处理奠定基础，未来可扩展至更广泛计算任务。

Abstract: This work proposes a 3D Stack In-Sensor-Computing (3DS-ISC) architecture for efficient event-based vision processing. A real-time normalization method using an exponential decay function is introduced to construct the time-surface, reducing hardware usage while preserving temporal information. The circuit design utilizes the leakage characterization of Dynamic Random Access Memory(DRAM) for timestamp normalization. Custom interdigitated metal-oxide-metal capacitor (MOMCAP) is used to store the charge and low leakage switch (LL switch) is used to extend the effective charge storage time. The 3DS-ISC architecture integrates sensing, memory, and computation to overcome the memory wall problem, reducing power, latency, and reducing area by 69x, 2.2x and 1.9x, respectively, compared with its 2D counterpart. Moreover, compared to works using a 16-bit SRAM to store timestamps, the ISC analog array can reduce power consumption by three orders of magnitude. In real computer vision (CV) tasks, we applied the spatial-temporal correlation filter (STCF) for denoise, and 3D-ISC achieved almost equivalent accuracy compared to the digital implementation using high precision timestamps. As for the image classification, time-surface constructed by 3D-ISC is used as the input of GoogleNet, achieving 99% on N-MNIST, 85% on N-Caltech101, 78% on CIFAR10-DVS, and 97% on DVS128 Gesture, comparable with state-of-the-art results on each dataset. Additionally, the 3D-ISC method is also applied to image reconstruction using the DAVIS240C dataset, achieving the highest average SSIM (0.62) among three methods. This work establishes a foundation for real-time, resource-efficient event-based processing and points to future integration of advanced computational circuits for broader applications.

</details>


### [21] [Designing Spatial Architectures for Sparse Attention: STAR Accelerator via Cross-Stage Tiling](https://arxiv.org/abs/2512.20198)
*Huizheng Wang,Taiquan Wei,Hongbin Wang,Zichuan Wang,Xinru Tang,Zhiheng Yue,Shaojun Wei,Yang Hu,Shouyi Yin*

Main category: cs.AR

TL;DR: STAR是一种面向大语言模型推理的跨阶段算法-硬件协同设计，通过零开销稀疏预测与排序更新机制显著提升能效与吞吐。


<details>
  <summary>Details</summary>
Motivation: 现有动态稀疏加速器在大规模令牌并行场景下效率低下，缺乏跨阶段协同优化。

Method: 提出基于对数域加法的前导零稀疏预测、分布式排序与排序更新FlashAttention机制，并配合专用硬件架构与空间多核数据流优化。

Result: 相比A100实现最高9.2倍加速与71.2倍能效提升，超越SOTA加速器达16.1倍能效与27.1倍面积效率，Spatial-STAR吞吐提升20.1倍。

Conclusion: STAR有效解决LLM推理中跨阶段冗余计算与访存问题，为高效Transformer推理提供新范式。

Abstract: Large language models (LLMs) rely on self-attention for contextual understanding, demanding high-throughput inference and large-scale token parallelism (LTPP). Existing dynamic sparsity accelerators falter under LTPP scenarios due to stage-isolated optimizations. Revisiting the end-to-end sparsity acceleration flow, we identify an overlooked opportunity: cross-stage coordination can substantially reduce redundant computation and memory access. We propose STAR, a cross-stage compute- and memory-efficient algorithm-hardware co-design tailored for Transformer inference under LTPP. STAR introduces a leading-zero-based sparsity prediction using log-domain add-only operations to minimize prediction overhead. It further employs distributed sorting and a sorted updating FlashAttention mechanism, guided by a coordinated tiling strategy that enables fine-grained stage interaction for improved memory efficiency and latency. These optimizations are supported by a dedicated STAR accelerator architecture, achieving up to 9.2$\times$ speedup and 71.2$\times$ energy efficiency over A100, and surpassing SOTA accelerators by up to 16.1$\times$ energy and 27.1$\times$ area efficiency gains. Further, we deploy STAR onto a multi-core spatial architecture, optimizing dataflow and execution orchestration for ultra-long sequence processing. Architectural evaluation shows that, compared to the baseline design, Spatial-STAR achieves a 20.1$\times$ throughput improvement.

</details>


### [22] [Nebula: Enable City-Scale 3D Gaussian Splatting in Virtual Reality via Collaborative Rendering and Accelerated Stereo Rasterization](https://arxiv.org/abs/2512.20495)
*He Zhu,Zheng Liu,Xingyang Li,Anbang Wu,Jieru Zhao,Fangxin Liu,Yiming Gan,Jingwen Leng,Yu Feng*

Main category: cs.AR

TL;DR: Nebula是一个针对大规模3D高斯点绘的协同渲染加速框架，通过优化云-端数据传输与计算共享，显著降低带宽并提升VR体验。


<details>
  <summary>Details</summary>
Motivation: 当前3D高斯点绘在建筑领域缺乏可扩展性，且云端VR内容传输受限于带宽，难以实现高保真流畅体验。

Method: 提出Nebula框架，包括云端时序感知LOD搜索减少冗余访问，客户端立体光栅化共享双目计算，并以中间结果流替代视频流。

Result: 相比有损视频流，带宽降低1925%，运动到光子延迟提速2.7倍，且画质无损。

Conclusion: Nebula有效解决了大规模3DGS在VR场景下的带宽与延迟瓶颈，为云协同渲染提供高效可行方案。

Abstract: 3D Gaussian splatting (3DGS) has drawn significant attention in the architectural community recently. However, current architectural designs often overlook the 3DGS scalability, making them fragile for extremely large-scale 3DGS. Meanwhile, the VR bandwidth requirement makes it impossible to deliver high-fidelity and smooth VR content from the cloud.
  We present Nebula, a coherent acceleration framework for large-scale 3DGS collaborative rendering. Instead of streaming videos, Nebula streams intermediate results after the LoD search, reducing 1925% data communication between the cloud and the client. To further enhance the motion-to-photon experience, we introduce a temporal-aware LoD search in the cloud that tames the irregular memory access and reduces redundant data access by exploiting temporal coherence across frames. On the client side, we propose a novel stereo rasterization that enables two eyes to share most computations during the stereo rendering with bit-accurate quality. With minimal hardware augmentations, Nebula achieves 2.7$\times$ motion-to-photon speedup and reduces 1925% bandwidth over lossy video streaming.

</details>


### [23] [Composing Mini Oscilloscope on Embedded Systems](https://arxiv.org/abs/2512.20571)
*Brennan Romero,D. G. Perera*

Main category: cs.AR

TL;DR: 本文利用NUC-140嵌入式平台实现类示波器功能，具备常用调试特性。


<details>
  <summary>Details</summary>
Motivation: 开发低成本、便携的嵌入式示波器替代方案。

Method: 通过自制子板连接探头与按键，以NUC-140液晶屏显示波形。

Result: 系统实现90%常用示波器功能，如自动/边沿触发、缩放、校准等。

Conclusion: 该系统可作为高效实用的嵌入式调试工具。

Abstract: In this paper, our goal is to reproduce the basic functionalities of a regular oscilloscope, using the Nuvoton NUC-140 embedded systems development platform as the front-end and display method. A custom-built daughter board connects the NUC-140 to a variety of peripherals, including two BNC scope-probe connections, an external nine-button keypad, and a calibration signal. The LCD of the NUC-140 development board serves as the waveform display. From the experimental results, it is demonstrated that our proposed system became a very competent debugging tool. It implements 90% of the features we typically use on original oscilloscopes, including: automatic, edge-triggered, and single modes; waveform visualization using vertical and horizontal scaling; probe calibration.

</details>
