<div id=toc></div>

# Table of Contents

- [cs.MA](#cs.MA) [Total: 3]
- [cs.DC](#cs.DC) [Total: 23]
- [cs.AR](#cs.AR) [Total: 3]
- [cs.SE](#cs.SE) [Total: 23]
- [cs.NI](#cs.NI) [Total: 2]


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [1] [ReCollab: Retrieval-Augmented LLMs for Cooperative Ad-hoc Teammate Modeling](https://arxiv.org/abs/2512.22129)
*Conor Wallace,Umer Siddique,Yongcan Cao*

Main category: cs.MA

TL;DR: 本文提出基于大语言模型的协作框架Collab及ReCollab，通过行为特征分类与检索增强生成，在Overcooked环境中实现对未知队友行为的有效推断与自适应协作。


<details>
  <summary>Details</summary>
Motivation: 传统方法在部分可观测和交互有限场景下表现脆弱，需更灵活的行为建模方式。

Method: 利用LLM将行为轨迹映射为高层假设，Collab通过行为评分表分类队友类型，ReCollab引入RAG机制稳定推理。

Result: 在Overcooked环境中，Collab能有效区分队友类型，ReCollab在各类布局中持续提升适应性，实现分类准确率与回合收益的帕累托最优。

Conclusion: LLM可作为有效的行为世界模型用于即兴协作，检索增强对复杂协作场景至关重要。

Abstract: Ad-hoc teamwork (AHT) requires agents to infer the behavior of previously unseen teammates and adapt their policy accordingly. Conventional approaches often rely on fixed probabilistic models or classifiers, which can be brittle under partial observability and limited interaction. Large language models (LLMs) offer a flexible alternative: by mapping short behavioral traces into high-level hypotheses, they can serve as world models over teammate behavior. We introduce \Collab, a language-based framework that classifies partner types using a behavior rubric derived from trajectory features, and extend it to \ReCollab, which incorporates retrieval-augmented generation (RAG) to stabilize inference with exemplar trajectories. In the cooperative Overcooked environment, \Collab effectively distinguishes teammate types, while \ReCollab consistently improves adaptation across layouts, achieving Pareto-optimal trade-offs between classification accuracy and episodic return. These findings demonstrate the potential of LLMs as behavioral world models for AHT and highlight the importance of retrieval grounding in challenging coordination settings.

</details>


### [2] [Solving Multi-Agent Multi-Goal Path Finding Problems in Polynomial Time](https://arxiv.org/abs/2512.22171)
*Stefan Edelkamp*

Main category: cs.MA

TL;DR: 本文提出了一种在无向图中为多智能体规划任务的方法，能够自主分配目标并解决冲突，实现近似最优路径规划。


<details>
  <summary>Details</summary>
Motivation: 解决传统多智能体路径规划中目标分配与冲突处理效率低的问题。

Method: 通过全局分配策略减少冲突，并结合局部调整、路径交错及目标重定位等方法解决剩余冲突。

Result: 在离散情况下该问题可在多项式时间内求解，且实现了无冲突的优化路径规划器。

Conclusion: 该方法相比传统车辆路径规划更高效，适用于网格等无向图结构中的多智能体任务规划。

Abstract: In this paper, we plan missions for a fleet of agents in undirected graphs, such as grids, with multiple goals. In contrast to regular multi-agent path-finding, the solver finds and updates the assignment of goals to the agents on its own. In the continuous case for a point agent with motions in the Euclidean plane, the problem can be solved arbitrarily close to optimal. For discrete variants that incur node and edge conflicts, we show that it can be solved in polynomial time, which is unexpected, since traditional vehicle routing on general graphs is NP-hard. We implement a corresponding planner that finds conflict-free optimized routes for the agents. Global assignment strategies greatly reduce the number of conflicts, with the remaining ones resolved by elaborating on the concept of ants-on-the-stick, by solving local assignment problems, by interleaving agent paths, and by kicking agents that have already arrived out of their destinations

</details>


### [3] [MARPO: A Reflective Policy Optimization for Multi Agent Reinforcement Learning](https://arxiv.org/abs/2512.22832)
*Cuiling Wu,Yaozhong Gan,Junliang Xing,Ying Fu*

Main category: cs.MA

TL;DR: MARPO通过反思机制和非对称裁剪机制提升多智能体强化学习的样本效率与训练稳定性。


<details>
  <summary>Details</summary>
Motivation: 解决多智能体强化学习中的样本效率低下问题。

Method: 引入利用后续轨迹的反思机制和基于KL散度动态调整裁剪范围的非对称裁剪机制。

Result: 在经典多智能体环境中，MARPO始终优于其他方法。

Conclusion: MARPO有效提升了多智能体强化学习的性能与稳定性。

Abstract: We propose Multi Agent Reflective Policy Optimization (MARPO) to alleviate the issue of sample inefficiency in multi agent reinforcement learning. MARPO consists of two key components: a reflection mechanism that leverages subsequent trajectories to enhance sample efficiency, and an asymmetric clipping mechanism that is derived from the KL divergence and dynamically adjusts the clipping range to improve training stability. We evaluate MARPO in classic multi agent environments, where it consistently outperforms other methods.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [4] [GPU Kernel Optimization Beyond Full Builds: An LLM Framework with Minimal Executable Programs](https://arxiv.org/abs/2512.22147)
*Ruifan Chu,Anbang Wang,Xiuxiu Bai,Shuai Liu,Xiaoshe Dong*

Main category: cs.DC

TL;DR: 提出一种端到端LLM框架，通过性能反馈在不构建完整应用的情况下优化GPU热点内核，实现跨平台、低成本加速。


<details>
  <summary>Details</summary>
Motivation: 专家手动调优成本高且难移植，现有LLM方法依赖完整编译运行，在大型应用中不可行。

Method: 从热点内核自动构建最小可执行程序（MEP），结合自动错误修复与性能模式继承，进行多轮迭代优化与评估。

Result: 在NVIDIA和DCU平台测试中，平均加速比达5.05x至7.77x，优于直接LLM优化，且无需完整源码依赖。

Conclusion: 该框架实现了实用、低成本、跨平台的GPU内核优化，显著提升高性能计算效率。

Abstract: In high-performance computing, hotspot GPU kernels are primary bottlenecks, and expert manual tuning is costly and hard to port. Large language model methods often assume kernels can be compiled and executed cheaply, which fails in large applications where full builds and runs are expensive. We present an end-to-end LLM framework with performance feedback that optimizes kernels without building the full application. From independently extracted hotspot kernels, it automatically completes code into a Minimal Executable Program (MEP), then performs multi-round iterative optimization and evaluation outside the full application. The framework integrates Automatic Error Repair and Performance Pattern Inheritance to fix faults, preserve correctness, reuse effective tiling/memory/synchronization strategies, and reduce search cost. Optimized variants are reintegrated into the original application for validation. We evaluate on NVIDIA GPUs and the Haiguang Deep Computing Unit (DCU) platform (AMD-licensed architecture) using PolyBench, the AMD APP SDK, and hotspot kernels from large-scale supercomputing applications. The method achieves average speedups of 5.05x (PolyBench on NVIDIA), 7.77x (PolyBench on DCU), 1.77x (AMD APP SDK), and 1.25x on three hotspot kernels, surpassing direct LLM optimization. The approach requires no full-source dependencies, offers cross-platform portability, and enables practical, low-cost GPU kernel optimization.

</details>


### [5] [GPU-Virt-Bench: A Comprehensive Benchmarking Framework for Software-Based GPU Virtualization Systems](https://arxiv.org/abs/2512.22125)
*Jithin VG,Ditto PS*

Main category: cs.DC

TL;DR: GPU-Virt-Bench是一个用于评估GPU虚拟化系统的综合基准框架，涵盖56项性能指标，支持在多租户环境中对比软件方案与MIG硬件隔离的表现。


<details>
  <summary>Details</summary>
Motivation: 当前AI和LLM推理负载激增，亟需高效共享GPU资源，但现有软件虚拟化方案缺乏统一评估标准。

Method: 构建包含10类56项指标的基准框架，涵盖开销、隔离性、LLM性能、内存带宽、缓存行为、PCIe吞吐、多GPU通信、调度效率、内存碎片和错误恢复等方面。

Result: 通过评估HAMi-core、BUD-FCSP及模拟MIG基线，揭示了各方案的关键性能特征，为生产部署提供决策依据。

Conclusion: GPU-Virt-Bench可系统化比较不同虚拟化方案，帮助用户在非MIG设备上做出更优的资源部署选择。

Abstract: The proliferation of GPU-accelerated workloads, particularly in artificial intelligence and large language model (LLM) inference, has created unprecedented demand for efficient GPU resource sharing in cloud and container environments. While NVIDIA's Multi-Instance GPU (MIG) technology provides hardware-level isolation, its availability is limited to high-end datacenter GPUs. Software-based virtualization solutions such as HAMi-core and BUD-FCSP offer alternatives for broader GPU families but lack standardized evaluation methodologies. We present GPU-Virt-Bench, a comprehensive benchmarking framework that evaluates GPU virtualization systems across 56 performance metrics organized into 10 categories. Our framework measures overhead, isolation quality, LLM-specific performance, memory bandwidth, cache behavior, PCIe throughput, multi-GPU communication, scheduling efficiency, memory fragmentation, and error recovery. GPU-Virt-Bench enables systematic comparison between software virtualization approaches and ideal MIG behavior, providing actionable insights for practitioners deploying GPU resources in multi-tenant environments. We demonstrate the framework's utility through evaluation of HAMi-core, BUD-FCSP, and simulated MIG baselines, revealing performance characteristics critical for production deployment decisions.

</details>


### [6] [Local Rendezvous Hashing: Bounded Loads and Minimal Churn via Cache-Local Candidates](https://arxiv.org/abs/2512.23434)
*Yongjie Guan*

Main category: cs.DC

TL;DR: Local Rendezvous Hashing (LRH) 提出了一种兼顾负载均衡与高效查找的新型一致性哈希方法，在减少内存访问开销的同时显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 传统环形一致性哈希存在负载不均问题，而多探针方法虽改善平衡性却带来高内存访问开销。

Method: LRH 在令牌环基础上，限制 HRW 选择至邻近 C 个物理节点窗口，通过预计算偏移实现快速候选枚举与加权选择。

Result: 在 N=5000、C=8 的测试中，LRH 将最大/平均负载比从 1.2785 降至 1.0947，吞吐达 60.05 Mkeys/s，约为 8 探针方法的 6.8 倍。

Conclusion: LRH 在保持低重映射开销的同时，实现了接近多探针方法的负载均衡性与显著更高的执行效率。

Abstract: Consistent hashing is fundamental to distributed systems, but ring-based schemes can exhibit high peak-to-average load ratios unless they use many virtual nodes, while multi-probe methods improve balance at the cost of scattered memory accesses. This paper introduces Local Rendezvous Hashing (LRH), which preserves a token ring but restricts Highest Random Weight (HRW) selection to a cache-local window of C distinct neighboring physical nodes. LRH locates a key by one binary search, enumerates exactly C distinct candidates using precomputed next-distinct offsets, and chooses the HRW winner (optionally weighted). Lookup cost is O(log|R| + C). Under fixed-topology liveness changes, fixed-candidate filtering remaps only keys whose original winner is down, yielding zero excess churn. In a benchmark with N=5000, V=256 (|R|=1.28M), K=50M and C=8, LRH reduces Max/Avg load from 1.2785 to 1.0947 and achieves 60.05 Mkeys/s, about 6.8x faster than multi-probe consistent hashing with 8 probes (8.80 Mkeys/s) while approaching its balance (Max/Avg 1.0697). A microbenchmark indicates multi-probe assignment is dominated by repeated ring searches and memory traffic rather than probe-generation arithmetic.

</details>


### [7] [Optimal Configuration of API Resources in Cloud Native Computing](https://arxiv.org/abs/2512.23494)
*Eddy Truyen,Wouter Joosen*

Main category: cs.DC

TL;DR: 本文提出在DevOps发布阶段应用性能优化框架，以调整微服务的CPU和内存资源配置，弥补现有研究集中于运维阶段自动伸缩的不足。


<details>
  <summary>Details</summary>
Motivation: 当前多数研究聚焦于运维阶段的智能调度与自动伸缩，忽略了发布阶段资源配置的前期调优，可能导致容器资源分配不当。

Method: 采用TeaStore微服务应用评估优化框架，比较多种优化算法，并结合因子筛选降低搜索空间复杂度。

Result: 因子筛选适用于预算有限时寻找最优配置或算法对比；若目标为近优解，则直接使用贝叶斯优化更佳。

Conclusion: 在发布阶段进行资源预调优可有效提升微服务性能，不同优化目标应选择相应策略以平衡采样成本与优化效果。

Abstract: This paper presents how an existing framework for offline performance optimization can be applied to microservice applications during the Release phase of the DevOps life cycle. Optimization of resource allocation configuration parameters for CPU and memory during the Release phase remains a largely unexplored problem as most research has focused on intelligent scheduling and autoscaling of microservices during the Ops stage of the DevOps cycle. Yet horizontal auto-scaling of containers, based on CPU usage for instance, may still leave these containers with an inappropriately allocated amount of memory, if no upfront fine-tuning of both resources is applied before the Deployment phase. We evaluate the performance optimization framework using the TeaStore microservice application and statistically compare different optimization algorithms, supporting informed decisions about their trade-offs between sampling cost and distance to the optimal resource configuration. This shows that upfront factor screening, for reducing the search space, is helpful when the goal is to find the optimal resource configuration with an affordable sampling budget. When the goal is to statistically compare different algorithms, screening must also be applied to make data collection of all data points in the search space feasible.  If the goal is to find a near-optimal configuration, however, it is better to run bayesian optimization without screening.

</details>


### [8] [HLS4PC: A Parametrizable Framework For Accelerating Point-Based 3D Point Cloud Models on FPGA](https://arxiv.org/abs/2512.22139)
*Amur Saqib Pal,Muhammad Mohsin Ghaffar,Faisal Shafait,Christian Weis,Norbert Wehn*

Main category: cs.DC

TL;DR: HLS4PC框架通过FPGA加速和硬件感知压缩技术，显著提升3D点云模型的推理效率，同时保持高准确率。


<details>
  <summary>Details</summary>
Motivation: 解决3D点云数据在GPU上计算与内存需求高、利用率低的问题，以满足安全关键场景的实时性需求。

Method: 提出HLS4PC框架，结合FPGA并行化、定点数实现、算法优化及模型压缩技术（如URS替代FPS、参数量化、层融合、输入点剪枝）。

Result: PointMLP-Lite模型复杂度降低4倍，ModelNet40准确率仅下降2%；FPGA实现相较前人工作吞吐量提升3.56倍，较GPU和CPU分别提升2.3倍和22倍。

Conclusion: HLS4PC为3D点云任务提供高效低耗的FPGA加速方案，在性能与精度间取得良好平衡。

Abstract: Point-based 3D point cloud models employ computation and memory intensive mapping functions alongside NN layers for classification/segmentation, and are executed on server-grade GPUs. The sparse, and unstructured nature of 3D point cloud data leads to high memory and computational demand, hindering real-time performance in safety critical applications due to GPU under-utilization. To address this challenge, we present HLS4PC, a parameterizable HLS framework for FPGA acceleration. Our approach leverages FPGA parallelization and algorithmic optimizations to enable efficient fixed-point implementations of both mapping and NN functions. We explore several hardware-aware compression techniques on a state-of-the-art PointMLP-Elite model, including replacing FPS with URS, parameter quantization, layer fusion, and input-points pruning, yielding PointMLP-Lite, a 4x less complex variant with only 2% accuracy drop on ModelNet40. Secondly, we demonstrate that the FPGA acceleration of the PointMLP-Lite results in 3.56x higher throughput than previous works. Furthermore, our implementation achieves 2.3x and 22x higher throughput compared to the GPU and CPU implementations, respectively.

</details>


### [9] [HybridFlow: Adaptive Task Scheduling for Fast and Token-Efficient LLM Inference in Edge-Cloud Collaboration](https://arxiv.org/abs/2512.22137)
*Jiangwen Dong,Jiayu Li,Wanyu Lin*

Main category: cs.DC

TL;DR: HybridFlow 是一种自适应资源的推理框架，通过边缘与云端大语言模型协作，实现快速且节省令牌的推理。


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型在边缘设备部署时推理延迟高、令牌消耗大的问题。

Method: 采用任务分解与并行执行、资源感知子任务路由两阶段方法，动态分配子任务至边缘或云端。

Result: 在多个基准测试中显著降低推理时间和令牌使用量，同时保持准确率。

Conclusion: HybridFlow 有效提升边缘-云协作推理效率，兼顾性能与资源约束。

Abstract: Large language models (LLMs) exhibit impressive reasoning and problem-solving abilities, yet their substantial inference latency and token consumption pose major challenges for real-time deployment on resource-limited edge devices. Recent efforts toward edge-cloud collaboration have attempted to mitigate this issue, but most existing methods adopt coarse-grained task allocation strategies-assigning entire queries either to the edge or the cloud. Such rigid partitioning fails to exploit fine-grained reasoning parallelism and often leads to redundant computation and inefficient resource utilization. To this end, we propose HybridFlow, a resource-adaptive inference framework that enables fast and token-efficient collaborative reasoning between edge and cloud LLMs. HybridFlow operates in two stages: (1) task decomposition and parallel execution, which dynamically splits a complex query into interdependent subtasks that can execute as soon as their dependencies are resolved; and (2) resource-aware subtask routing, where a learned router adaptively assigns each subtask to the edge or cloud model according to predicted utility gains and real-time budget states. Comprehensive evaluations on GPQA, MMLU-Pro, AIME, and LiveBench-Reasoning demonstrate that HybridFlow effectively reduces end-to-end inference time and overall token usage while maintaining competitive accuracy.

</details>


### [10] [On Harnessing Idle Compute at the Edge for Foundation Model Training](https://arxiv.org/abs/2512.22142)
*Leyang Xue,Meghana Madhyastha,Myungjin Lee,Amos Storkey,Randal Burns,Mahesh K. Marina*

Main category: cs.DC

TL;DR: Cleave提出了一种新的去中心化大模型训练范式，通过混合张量并行与参数服务器框架，在边缘设备上实现媲美云端的高效训练。


<details>
  <summary>Details</summary>
Motivation: 当前大模型训练高度集中于云数据中心，成本高昂；Cleave旨在利用边缘设备闲置算力，实现训练民主化。

Method: 采用选择性混合张量并行细粒度划分训练操作，结合参数服务器框架，并引入成本优化模型指导设备选择与负载分配。

Result: Cleave可高效扩展至数千设备，支持比基线方法多8倍设备，单批次训练时间提升10倍，故障恢复速度提升百倍以上。

Conclusion: Cleave有效解决了边缘训练中的内存、通信、异构性和动态性问题，实现了与云端相当的大模型训练性能。

Abstract: The ecosystem behind foundation model development today is highly centralized and limited to large-scale cloud data center operators: training foundation models is costly, needing immense compute resources. Decentralized foundation model training across edge devices, leveraging their spare compute, promises a democratized alternative. However, existing edge-training approaches fall short: they struggle to match cloud-based training performance, exhibit limited scalability with model size, exceed device memory capacity, and have prohibitive communication overhead. They also fail to satisfactorily handle device heterogeneity and dynamism.
  We introduce a new paradigm, Cleave, which finely partitions training operations through a novel selective hybrid tensor parallelism method. Together with a parameter server centric training framework, Cleave copes with device memory limits and avoids communication bottlenecks, thereby enabling efficient training of large models on par with the cloud. Further, with a cost optimization model to guide device selection and training workload distribution, Cleave effectively accounts for device heterogeneity and churn.
  Our evaluations show that Cleave matches cloud-based GPU training by scaling efficiently to larger models and thousands of devices, supporting up to 8x more devices than baseline edge-training approaches. It outperforms state-of-the-art edge training methods by up to a factor of 10 in per-batch training time and efficiently handles device failures, achieving at least 100x faster recovery than prior methods.

</details>


### [11] [OptiNIC: A Resilient and Tail-Optimal RDMA NIC for Distributed ML Workloads](https://arxiv.org/abs/2512.22743)
*Ertza Warraich,Ali Imran,Annus Zulfiqar,Shay Vargaftik,Sonia Fahmy,Muhammad Shahbaz*

Main category: cs.DC

TL;DR: OptiNIC是一种专为分布式机器学习设计的RDMA传输方案，通过放弃重传和保序机制，显著降低尾延迟并提升训练与推理效率。


<details>
  <summary>Details</summary>
Motivation: 现有RDMA传输因强制可靠性和保序机制，在ML场景中导致尾延迟成为瓶颈，难以扩展。

Method: OptiNIC采用尽力而为、无序传输模型，结合自适应超时触发前向进展，并将丢包恢复交由ML层处理（如Hadamard变换和纠删码）。

Result: 在Hyperstack和CloudLab上，OptiNIC使训练时间缩短2倍、推理吞吐提升1.6倍，99%延迟降低3.5倍，BRAM使用减少2.7倍，NIC容错能力近翻倍。

Conclusion: OptiNIC为分布式ML提供了一种高效、低尾延迟、高容错的专用RDMA传输方案。

Abstract: As distributed machine learning (ML) workloads scale to thousands of GPUs connected by high-speed interconnects, tail latency in collective communication has become a major bottleneck. Existing RDMA transports, such as RoCE, IRN, SRNIC, and Falcon, enforce strict reliability and in-order delivery, relying on retransmissions and packet sequencing to ensure correctness. While these approaches work well for general-purpose workloads, they introduce complexity and latency that scale poorly in ML, where even rare packet delays can stall entire model pipelines.
  We present OptiNIC, a domain-specific RDMA transport that revisits traditional reliability guarantees based on ML's tolerance for partial or missing data. OptiNIC eliminates retransmissions and in-order delivery from the NIC, enabling a best-effort, out-of-order transport model for RDMA. Unlike traditional RDMA, which signals completion only after complete data delivery, OptiNIC introduces adaptive timeouts to trigger forward progress when data may be lost or delayed. OptiNIC retains standard congestion control mechanisms (e.g., DCQCN, EQDS, or Swift) while shifting loss recovery to the ML pipeline itself (e.g., via the Hadamard Transform and Erasure Coding).
  Our evaluation shows that OptiNIC improves time-to-accuracy (TTA) by 2x and increases throughput by 1.6x for training and inference, respectively, across two public clouds (i.e., Hyperstack and CloudLab). OptiNIC also lowers 99th-percentile latency by 3.5x, cuts BRAM usage by 2.7x, and nearly doubles NIC resilience to faults-delivering a resilient, tail-optimized RDMA transport purpose-built for distributed ML workloads.

</details>


### [12] [Adaptive GPU Resource Allocation for Multi-Agent Collaborative Reasoning in Serverless Environments](https://arxiv.org/abs/2512.22149)
*Guilin Zhang,Wulan Guo,Ziqi Tan*

Main category: cs.DC

TL;DR: 提出一种自适应GPU资源分配框架，用于在无服务器GPU平台上高效部署多智能体系统，显著降低延迟并优化资源利用率。


<details>
  <summary>Details</summary>
Motivation: 解决多智能体系统在无服务器GPU平台部署时因异构负载和动态需求导致的资源分配效率低下与成本高昂问题。

Method: 设计复杂度为O(N)的实时自适应算法，依据工作负载特征、智能体优先级和最低资源需求动态分配GPU资源。

Result: 相比轮询调度降低85%延迟，吞吐量媲美静态分配；在仿真中全面优于静态均分和轮询策略，提升延迟、成本与GPU利用率表现。

Conclusion: 该框架为在无服务器GPU基础设施上经济高效地部署多智能体AI系统提供了实用解决方案。

Abstract: Multi-agent systems powered by large language models have emerged as a promising paradigm for solving complex reasoning tasks through collaborative intelligence. However, efficiently deploying these systems on serverless GPU platforms presents significant resource allocation challenges due to heterogeneous agent workloads, varying computational demands, and the need for cost-effective scaling. This paper presents an adaptive GPU resource allocation framework that achieves 85\% latency reduction compared to round-robin scheduling while maintaining comparable throughput to static allocation, using an $O(N)$ complexity algorithm for real-time adaptation. Our approach dynamically allocates GPU resources based on workload characteristics, agent priorities, and minimum resource requirements, enabling efficient utilization while maintaining quality of service. The framework addresses three key challenges: (1) heterogeneous computational demands across lightweight coordinators and heavyweight specialists, (2) dynamic workload fluctuations requiring millisecond-scale reallocation, and (3) capacity constraints in serverless environments. Through comprehensive simulations modeling realistic multi-agent workflows with four heterogeneous agents, we demonstrate that adaptive allocation outperforms static equal and round-robin strategies across latency, cost, and GPU utilization metrics. The framework provides a practical solution for deploying cost-efficient multi-agent AI systems on serverless GPU infrastructure.

</details>


### [13] [TL: Automatic End-to-End Compiler of Tile-Based Languages for Spatial Dataflow Architectures](https://arxiv.org/abs/2512.22168)
*Wei Li,Zhenyu Bai,Heru Wang,Pranav Dangi,Zhiqiang Zhang,Cheng Tan,Huiying Lan,Weng-Fai Wong,Tulika Mitra*

Main category: cs.DC

TL;DR: TL是一个端到端编译框架，用于将基于分块的程序映射到空间数据流架构，提升数据复用并减少通信开销。


<details>
  <summary>Details</summary>
Motivation: 现有空间数据流加速器因编程性差限制了广泛应用，需解决工作负载映射问题以释放其高效能潜力。

Method: 提出硬件抽象表示捕捉互连拓扑与存储层次，基于MLIR生态支持多前端输入与多后端输出，优化跨核分块实例分布。

Result: 实现跨分布式核心的数据复用增强与通信降低，支持多样化空间数据流架构的专用优化。

Conclusion: TL有效提升了空间数据流架构的可编程性，为高效能计算提供通用编译支持。

Abstract: Spatial dataflow accelerators are a promising direction for next-generation computer systems because they can reduce the memory bottlenecks of traditional von Neumann machines such as CPUs and GPUs. They do so by organizing computation around explicit, compiler-managed data movement over the on-chip network, allowing operands to be directly forwarded between processing elements and reducing reliance on high-latency, bandwidth-limited global shared memory. Such localized communications can provide higher throughput and efficiency compared to repeated off-chip memory accesses. However, their end-to-end performance depends strongly on how workloads are mapped to the hardware. Naive mappings can perform very poorly, and most users rely on hand-tuned vendor libraries. In practice, although existing spatial-dataflow accelerators have strong potential for high performance, energy- and cost-efficiency, their limited programmability remains a major barrier to their wider adoption. This paper presents TL, an end-to-end framework that compiles tile-based programs (such as Triton kernels) onto spatial dataflow architectures. Unlike most existing compiler frameworks that focus on optimizing code generation within a single tile, TL addresses the central challenge of distributing tile instances across spatially distributed cores and exploiting the on-chip network and distributed memories to increase data reuse and reduce communications. TL proposes a hardware representation that captures interconnect topology, memory hierarchy, and compute capabilities, enabling both specialized architecture-specific optimizations and support for diverse spatial dataflow targets. TL is built on the MLIR ecosystem and defines a generic entry point for different front-ends and an end point for different back-ends.

</details>


### [14] [AiiDAlab: on the route to accelerate science](https://arxiv.org/abs/2512.22173)
*Aliaksandr V. Yakutovich,Jusong Yu,Daniel Hollas,Edan Bainglass,Corsin Battaglia,Miki Bonacci,Lucas Fernandez Vilanova,Stephan Henne,Anders Kaestner,Michel Kenzelmann,Graham Kimbell,Jakob Lass,Fabio Lopes,Daniel G. Mazzone,Andres Ortega-Guerrero,Xing Wang,Nicola Marzari,Carlo A. Pignedoli,Giovanni Pizzi*

Main category: cs.DC

TL;DR: AiiDAlab平台通过直观的网页界面简化复杂计算流程，支持多学科科研与教育，提升可重复性并促进开放研究数据生成。


<details>
  <summary>Details</summary>
Motivation: 解决科研人员在超算环境下执行大量关联模拟时面临的技术门槛与操作复杂性问题。

Method: 构建基于浏览器的图形化平台AiiDAlab，集成AiiDA引擎自动追踪模拟溯源，并逐步扩展至多学科应用。

Result: 平台成功应用于量子化学、大气建模、电池研究及实验数据分析，同时支持教学使用，并与电子实验记录本整合以符合FAIR原则。

Conclusion: AiiDAlab显著降低计算科研门槛，使研究人员专注科学探索，同时保障数据可追溯与可复现，推动开放科研发展。

Abstract: With the availability of ever-increasing computational capabilities, robust and automated research workflows are essential to enable and facilitate the execution and orchestration of large numbers of interdependent simulations in supercomputer facilities. However, the execution of these workflows still typically requires technical expertise in setting up calculation inputs, interpreting outputs, and handling the complexity of parallel code execution on remote machines. To address these challenges, the AiiDAlab platform was developed, making complex computational workflows accessible through an intuitive user interface that runs in a web browser. Here, we discuss how AiiDAlab has matured over the past few years, shifting its focus from computational materials science to become a powerful platform that accelerates scientific discovery across multiple disciplines. Thanks to its design, AiiDAlab allows scientists to focus on their research rather than on computational details and challenges, while keeping automatically track of the full simulation provenance via the underlying AiiDA engine and thus ensuring reproducibility. In particular, we discuss its adoption into quantum chemistry, atmospheric modeling, battery research, and even experimental data analysis at large-scale facilities, while also being actively used in educational settings. Driven by user feedback, significant effort has been made to simplify user onboarding, streamline access to computational resources, and provide robust mechanisms to work with large datasets. Furthermore, AiiDAlab is being integrated with electronic laboratory notebooks (ELNs), reinforcing adherence to the FAIR principles and supporting researchers in data-centric scientific disciplines in easily generating reproducible Open Research Data (ORD).

</details>


### [15] [BitFlipScope: Scalable Fault Localization and Recovery for Bit-Flip Corruptions in LLMs](https://arxiv.org/abs/2512.22174)
*Muhammad Zeeshan Karamat,Sadman Saif,Christiana Chamon Garcia*

Main category: cs.DC

TL;DR: BitFlipScope是一个用于定位和修复大语言模型中位翻转故障的软件框架，支持有无参考模型两种场景，实现高效诊断与轻量恢复。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在关键场景中易受硬件故障或攻击导致的位翻转影响，需有效定位并修复以保障安全可靠运行。

Method: 提出BitFlipScope框架：有参考模型时通过差分分析定位异常；无参考模型时利用残差扰动与损失敏感度推断故障区域。

Result: 该框架能准确识别故障区域，支持无需微调的轻量级性能恢复，提升模型在易错或对抗环境中的鲁棒性。

Conclusion: BitFlipScope为构建可信、抗故障的大语言模型部署提供了实用解决方案，是迈向硬件容错LLM的重要一步。

Abstract: Large Language Models (LLMs) deployed in practical and safety-critical settings are increasingly susceptible to bit-flip faults caused by hardware degradation, cosmic radiation, or deliberate fault-injection attacks such as Rowhammer. These faults silently corrupt internal parameters and can lead to unpredictable or dangerous model behavior. Localizing these corruptions is essential: without identifying the affected region, it is impossible to diagnose the source of degradation, apply targeted corrective measures, or restore model functionality without resorting to costly fine-tuning or full retraining. This work introduces BitFlipScope, a scalable, software-based framework for identifying fault-affected regions within transformer architectures under two deployment scenarios. When a clean reference model is available, BitFlipScope performs differential analysis of outputs, hidden states, and internal activations for detecting anomalous behavior indicative of corruption to pinpoint or localize faults. When no reference model exists, it uses residual-path perturbation and loss-sensitivity profiling to infer the fault-impacted region directly from the corrupted model. In both settings, the framework not only enables effective fault diagnosis but also supports lightweight performance recovery without fine-tuning, offering a practical path to restoring corrupted models. Together, these capabilities make BitFlipScope an important step toward trustworthy, fault-resilient LLM deployment in hardware-prone and adversarial environments.

</details>


### [16] [iOS as Acceleration](https://arxiv.org/abs/2512.22180)
*Alexander K. Chen*

Main category: cs.DC

TL;DR: 本文探讨利用iOS设备的闲置计算能力，通过分布式流水线并行加速本地机器学习任务，以零成本提升弱算力环境性能。


<details>
  <summary>Details</summary>
Motivation: 解决本地弱算力环境下运行大规模机器学习的难题，同时规避云方案在隐私、成本或物理限制方面的不足。

Method: 构建原型系统，利用iOS设备强大处理器，采用分布式流水线并行方法，克服内存、散热和沙箱限制。

Result: 成功加速了小型模型训练、批量推理及智能体工具调用，在受限环境中获得显著性能提升。

Conclusion: 常见移动设备具备潜力为机器学习提供更大贡献，未来可进一步优化与扩展应用场景。

Abstract: Practical utilization of large-scale machine learning requires a powerful compute setup, a necessity which poses a significant barrier to engagement with such artificial intelligence in more restricted system environments. While cloud computing offers a solution to weaker local environments, certain situations like training involving private or sensitive data, physical environments not available through the cloud, or higher anticipated usage costs, necessitate computing locally. We explore the potential to improve weaker local compute systems at zero additional cost by taking advantage of ubiquitous yet underutilized resources: mobile phones. Specifically, recent iOS phones are equipped with surprisingly powerful processors, but they also face limitations like memory constraints, thermal throttling, and OS sandboxing. We present a proof-of-concept system demonstrating a novel approach to harness an iOS device via distributed pipeline parallelism, achieving significant benefits in a lesser compute environment by accelerating modest model training, batch inference, and agentic LRM tool-usage. We discuss practical use-cases, limitations, and directions for future work. The findings of this paper highlight the potential for the improving commonplace mobile devices to provide greater contributions to machine learning.

</details>


### [17] [MatKV: Trading Compute for Flash Storage in LLM Inference](https://arxiv.org/abs/2512.22195)
*Kun-Woo Shin,Jay H. Park,Moonwook Oh,Yohan Jo,Jaeyoung Do,Sang-Won Lee*

Main category: cs.DC

TL;DR: MatKV通过预计算并存储RAG对象的键值向量，显著降低推理时间和功耗，同时保持准确率，提升生成式AI的效率与可及性。


<details>
  <summary>Details</summary>
Motivation: RAG推理中长输入的prefill阶段能耗高、耗时长，需优化以降低成本和功耗。

Method: 提出MatKV方案，预计算键值向量并存储于高速低功耗闪存，在推理时复用而非GPU重算。

Result: 实验表明，MatKV使RAG推理时间和功耗减半，支持流水线加载与低端GPU解码，不影响问答任务精度。

Conclusion: MatKV能有效提升生成式AI在多种硬件环境下的成本效益与能效，推动其广泛应用。

Abstract: We observe two major trends in LLM-based generative AI: (1) inference is becoming the dominant factor in terms of cost and power consumption, surpassing training, and (2) retrieval augmented generation (RAG) is becoming prevalent. When processing long inputs in RAG, the prefill phase of computing the key-value vectors of input text is energy-intensive and time-consuming even with high-end GPUs. Thus, it is crucial to make the prefill phase in RAG inference efficient. To address this issue, we propose MatKV, a scheme that precomputes the key-value vectors (KVs) of RAG objects (e.g., documents), materializes them in inexpensive but fast and power-efficient flash storage, and reuses them at inference time instead of recomputing the KVs using costly and power-inefficient GPU. Experimental results using Hugging Face's Transformers library across state-of-the-art GPUs and flash memory SSDs confirm that, compared to full KV computation on GPUs, MatKV reduces both inference time and power consumption by half for RAG workloads, without severely impacting accuracy in the question-answering task. Furthermore, we demonstrate that MatKV enables additional optimizations in two ways. First, a GPU can decode text while simultaneously loading the materialized KVs for the next instance, reducing load latency. Second, since decoding speed is less sensitive to GPU performance than KV computation, low-end GPUs can be leveraged for decoding without significantly compromising speed once the materialized KVs are loaded into GPU memory. These findings underscore MatKV's potential to make large-scale generative AI applications more cost-effective, power-efficient, and accessible across a wider range of tasks and hardware environments.

</details>


### [18] [SPUMA: a minimally invasive approach to the GPU porting of OPENFOAM](https://arxiv.org/abs/2512.22215)
*Simone Bnà,Giuseppe Giaquinto,Ettore Fadiga,Tommaso Zanelli,Francesco Bottau*

Main category: cs.DC

TL;DR: 本文介绍了SPUMA，一个针对NVIDIA和AMD GPU的OPENFOAM全GPU移植方案，通过可移植编程模型与内存池管理，在LUMI和Leonardo集群上实现了高性能与高能效。


<details>
  <summary>Details</summary>
Motivation: 解决开源CFD在混合集群中利用现代加速器时面临的编程性挑战。

Method: 采用可移植编程模型及基于统一内存特性的内存池管理策略，将OPENFOAM完整移植至GPU平台。

Result: 在LUMI与Leonardo集群上实现强扩展效率65%，弱扩展效率75%-85%，单个A100 GPU性能等效200-300个Sapphire Rapids CPU核心，能耗降低高达82%。

Conclusion: SPUMA有效提升OPENFOAM在异构GPU集群上的性能与能效，为开源CFD在预exascale系统中的应用提供可行路径。

Abstract: High Performance Computing (HPC) on hybrid clusters represents a significant opportunity for Computational Fluid Dynamics (CFD), especially when modern accelerators are utilized effectively. However, despite the widespread adoption of GPUs, programmability remains a challenge, particularly in open-source contexts. In this paper, we present SPUMA, a full GPU porting of OPENFOAM targeting NVIDIA and AMD GPUs. The implementation strategy is based on a portable programming model and the adoption of a memory pool manager that leverages the unified memory feature of modern GPUs. This approach is discussed alongside several numerical tests conducted on two pre-exascale clusters in Europe, LUMI and Leonardo, which host AMD MI250X and NVIDIA A100 GPUs, respectively. In the performance analysis section, we present results related to memory usage profiling and kernel wall-time, the impact of the memory pool, and energy consumption obtained by simulating the well-known DrivAer industrial test case. GPU utilization strongly affects strong scalability results, reaching 65% efficiency on both LUMI and Leonardo when approaching a load of 8 million cells per GPU. Weak scalability results, obtained on 20 GPUs with the OpenFOAM native multigrid solver, range from 75% on Leonardo to 85% on LUMI. Notably, efficiency is no lower than 90% when switching to the NVIDIA AmgX linear algebra solver. Our tests also reveal that one A100 GPU on Leonardo is equivalent 200-300 Intel Sapphire Rapids cores, provided the GPUs are sufficiently oversubscribed (more than 10 million of cells per GPU). Finally, energy consumption is reduced by up to 82% compared to analogous simulations executed on CPUs.

</details>


### [19] [Scalable Cloud-Native Architectures for Intelligent PMU Data Processing](https://arxiv.org/abs/2512.22231)
*Nachiappan Chockalingam,Akshay Deshpande,Lokesh Butra,Ram Sekhar Bodala,Nitin Saksena,Adithya Parthasarathy,Balakrishna Pothineni,Akash Kumar Agarwal*

Main category: cs.DC

TL;DR: 提出一种基于云原生架构的智能PMU数据处理框架，结合AI与边缘云计算，实现低延迟、高扩展性与可靠性的电网实时监测。


<details>
  <summary>Details</summary>
Motivation: 传统集中式处理架构难以应对大规模PMU数据带来的延迟、扩展性与可靠性挑战。

Method: 采用分布式流处理、容器化微服务、弹性资源编排及时间序列机器学习模型。

Result: 系统可实现亚秒级响应，支持大规模PMU部署，并内建安全隐私机制。

Conclusion: 该架构为下一代智能电网分析提供了稳健灵活的基础。

Abstract: Phasor Measurement Units (PMUs) generate high-frequency, time-synchronized data essential for real-time power grid monitoring, yet the growing scale of PMU deployments creates significant challenges in latency, scalability, and reliability. Conventional centralized processing architectures are increasingly unable to handle the volume and velocity of PMU data, particularly in modern grids with dynamic operating conditions. This paper presents a scalable cloud-native architecture for intelligent PMU data processing that integrates artificial intelligence with edge and cloud computing. The proposed framework employs distributed stream processing, containerized microservices, and elastic resource orchestration to enable low-latency ingestion, real-time anomaly detection, and advanced analytics. Machine learning models for time-series analysis are incorporated to enhance grid observability and predictive capabilities. Analytical models are developed to evaluate system latency, throughput, and reliability, showing that the architecture can achieve sub-second response times while scaling to large PMU deployments. Security and privacy mechanisms are embedded to support deployment in critical infrastructure environments. The proposed approach provides a robust and flexible foundation for next-generation smart grid analytics.

</details>


### [20] [Efficient Multi-Model Orchestration for Self-Hosted Large Language Models](https://arxiv.org/abs/2512.22402)
*Bhanu Prakash Vangala,Tanu Malik*

Main category: cs.DC

TL;DR: Pick and Spin 是一个基于 Kubernetes 的实用框架，旨在提升自托管大语言模型的可扩展性与经济性，通过动态资源调度和混合路由策略显著优化成功率、延迟与 GPU 成本。


<details>
  <summary>Details</summary>
Motivation: 组织希望在保障隐私、控制成本和实现定制化的前提下自托管大语言模型，但面临 GPU 利用率低、负载路由复杂和系统可靠性差等挑战。

Method: 提出 Pick and Spin 框架，集成 Helm 部署系统、自适应缩放至零机制及结合关键词启发式与轻量级 DistilBERT 分类器的混合路由模块。

Result: 在四个主流大模型和八个公开数据集上测试，Pick and Spin 相比静态部署最高提升 21.6% 成功率、降低 30% 延迟、节省 33% 单次查询 GPU 成本。

Conclusion: Pick and Spin 有效解决了自托管 LLM 的部署与运维难题，在性能与成本间取得良好平衡，具备实际落地价值。

Abstract: Self-hosting large language models (LLMs) is increasingly appealing for organizations seeking privacy, cost control, and customization. Yet deploying and maintaining in-house models poses challenges in GPU utilization, workload routing, and reliability. We introduce Pick and Spin, a practical framework that makes self-hosted LLM orchestration scalable and economical. Built on Kubernetes, it integrates a unified Helm-based deployment system, adaptive scale-to-zero automation, and a hybrid routing module that balances cost, latency, and accuracy using both keyword heuristics and a lightweight DistilBERT classifier. We evaluate four models, Llama-3 (90B), Gemma-3 (27B), Qwen-3 (235B), and DeepSeek-R1 (685B) across eight public benchmark datasets, with five inference strategies, and two routing variants encompassing 31,019 prompts and 163,720 inference runs. Pick and Spin achieves up to 21.6% higher success rates, 30% lower latency, and 33% lower GPU cost per query compared with static deployments of the same models.

</details>


### [21] [Nightjar: Dynamic Adaptive Speculative Decoding for Large Language Models Serving](https://arxiv.org/abs/2512.22420)
*Rui Li,Zhaoning Zhang,Libo Zhang,Huaimin Wang,Xiang Fu,Zhiquan Lai*

Main category: cs.DC

TL;DR: Nightjar是一种基于学习的自适应推测推理算法，能根据请求负载动态调整推测长度，从而提升LLM推理效率。


<details>
  <summary>Details</summary>
Motivation: 现有推测解码方法因固定推测长度无法适应动态请求率，在高负载场景下性能下降。

Method: 提出Nightjar算法，通过动态选择最优推测长度甚至关闭推测解码以适配不同批次大小和负载情况。

Result: 实验表明，相比标准推测解码，Nightjar最高提升14.8%吞吐量并降低20.2%延迟。

Conclusion: Nightjar在实时服务场景中展现出更强的鲁棒性和效率。

Abstract: Speculative decoding (SD) accelerates LLM inference by verifying draft tokens in parallel. However, this method presents a critical trade-off: it improves throughput in low-load, memory-bound systems but degrades performance in high-load, compute-bound environments due to verification overhead. Current SD implementations use a fixed speculative length, failing to adapt to dynamic request rates and creating a significant performance bottleneck in real-world serving scenarios. To overcome this, we propose Nightjar, a novel learning-based algorithm for adaptive speculative inference that adjusts to request load by dynamically selecting the optimal speculative length for different batch sizes and even disabling speculative decoding when it provides no benefit. Experiments show that Nightjar achieves up to 14.8% higher throughput and 20.2% lower latency compared to standard speculative decoding, demonstrating robust efficiency for real-time serving.

</details>


### [22] [Role-Based Fault Tolerance System for LLM RL Post-Training](https://arxiv.org/abs/2512.22492)
*Zhenqian Chen,Baoquan Zhong,Xiang Li,Qing Dai,Xinkui Zhao,Miao Ye,Ren Cheng,Lufei Zhang,Jianwei Yin*

Main category: cs.DC

TL;DR: RobustRL 是首个针对 RL 后训练的容错系统，通过角色隔离、动态通信和非中断恢复机制，在高故障率下显著提升训练效率。


<details>
  <summary>Details</summary>
Motivation: 现有框架无法兼顾 RL 训练与推理混合负载下的容错需求，导致重启开销大、效率低。

Method: 提出角色感知监控、非中断角色恢复及基于 UCX 的动态点对点通信机制，实现故障隔离与快速重连。

Result: 在 256-GPU 集群上，相比 ByteRobust，ETTR 提升至 80%，端到端训练速度加快 8.4%-17.4%。

Conclusion: RobustRL 有效降低 RL 后训练因硬件故障导致的性能损失，为大规模强化学习提供高可用系统支持。

Abstract: RL post-training for LLMs has been widely scaled to enhance reasoning and tool-using capabilities. However, RL post-training interleaves training and inference workloads, exposing the system to faults from both sides. Existing fault tolerance frameworks for LLMs target either training or inference, leaving the optimization potential in the asynchronous execution unexplored for RL. Our key insight is role-based fault isolation so the failure in one machine does not affect the others. We treat trainer, rollout, and other management roles in RL training as distinct distributed sub-tasks. Instead of restarting the entire RL task in ByteRobust, we recover only the failed role and reconnect it to living ones, thereby eliminating the full-restart overhead including rollout replay and initialization delay.
  We present RobustRL, the first comprehensive robust system to handle GPU machine errors for RL post-training Effective Training Time Ratio improvement. (1) \textit{Detect}. We implement role-aware monitoring to distinguish actual failures from role-specific behaviors to avoid the false positive and delayed detection. (2) \textit{Restart}. For trainers, we implement a non-disruptive recovery where rollouts persist state and continue trajectory generation, while the trainer is rapidly restored via rollout warm standbys. For rollout, we perform isolated machine replacement without interrupting the RL task. (3) \textit{Reconnect}. We replace static collective communication with dynamic, UCX-based (Unified Communication X) point-to-point communication, enabling immediate weight synchronization between recovered roles. In an RL training task on a 256-GPU cluster with Qwen3-8B-Math workload under 10\% failure injection frequency, RobustRL can achieve an ETTR of over 80\% compared with the 60\% in ByteRobust and achieves 8.4\%-17.4\% faster in end-to-end training time.

</details>


### [23] [RollArt: Scaling Agentic RL Training via Disaggregated Infrastructure](https://arxiv.org/abs/2512.22560)
*Wei Gao,Yuheng Zhao,Tianyuan Wu,Shaopan Xiong,Weixun Wang,Dakai An,Lunxi Cao,Dilxat Muhtar,Zichen Liu,Haizhou Zhao,Ju Huang,Siran Yang,Yongbin Li,Wenbo Su,Jiamang Wang,Lin Qu,Bo Zheng,Wei Wang*

Main category: cs.DC

TL;DR: RollArc是一个专为异构基础设施设计的分布式系统，旨在提升多任务智能体强化学习训练效率。


<details>
  <summary>Details</summary>
Motivation: 智能体强化学习工作负载高度异构，传统集中式架构难以高效利用资源，亟需专用系统优化性能。

Method: 通过硬件亲和性任务映射、细粒度异步执行和有状态感知计算三大核心机制实现高效调度与弹性扩展。

Result: 相比单体和同步基线，端到端训练时间减少1.35-2.05倍，并在超3000 GPU集群上验证了系统可扩展性与鲁棒性。

Conclusion: RollArc显著提升了大规模智能体强化学习训练吞吐量，是面向异构硬件高效训练的有效解决方案。

Abstract: Agentic Reinforcement Learning (RL) enables Large Language Models (LLMs) to perform autonomous decision-making and long-term planning. Unlike standard LLM post-training, agentic RL workloads are highly heterogeneous, combining compute-intensive prefill phases, bandwidth-bound decoding, and stateful, CPU-heavy environment simulations. We argue that efficient agentic RL training requires disaggregated infrastructure to leverage specialized, best-fit hardware. However, naive disaggregation introduces substantial synchronization overhead and resource underutilization due to the complex dependencies between stages.
  We present RollArc, a distributed system designed to maximize throughput for multi-task agentic RL on disaggregated infrastructure. RollArc is built on three core principles: (1) hardware-affinity workload mapping, which routes compute-bound and bandwidth-bound tasks to bestfit GPU devices, (2) fine-grained asynchrony, which manages execution at the trajectory level to mitigate resource bubbles, and (3) statefulness-aware computation, which offloads stateless components (e.g., reward models) to serverless infrastructure for elastic scaling. Our results demonstrate that RollArc effectively improves training throughput and achieves 1.35-2.05\(\times\) end-to-end training time reduction compared to monolithic and synchronous baselines. We also evaluate RollArc by training a hundreds-of-billions-parameter MoE model for Qoder product on an Alibaba cluster with more than 3,000 GPUs, further demonstrating RollArc scalability and robustness. The code is available at https://github.com/alibaba/ROLL.

</details>


### [24] [Decoupling Adaptive Control in TeaStore](https://arxiv.org/abs/2512.23495)
*Eddy Truyen*

Main category: cs.DC

TL;DR: 本文探讨了通过控制循环实现自适应的TeaStore规范，并分析了不同架构方法在微服务自适应中的权衡与结合潜力。


<details>
  <summary>Details</summary>
Motivation: 为实现自适应系统，需考虑系统一致性、规划性和模块性等关键属性。

Method: 对比软件架构方法、云原生Operator模式及传统编程语言技术在解耦自适应逻辑方面的应用。

Result: 不同方法在细粒度表达与系统级控制间存在权衡，但可结合构建多层架构。

Conclusion: 多种方法非互斥，可融合用于构建自适应微服务的多层架构。

Abstract: The Adaptable TeaStore specification provides a microservice-based case study for implementing self-adaptation through a control loop.  We argue that implementations of this specification should be informed by key properties of self-adaptation: system-wide consistency (coordinated adaptations across replicas), planning (executing an adaptation until appropriate conditions are met),  and modularity (clean integration of adaptation logic).  In this implementation discussion paper, we examine how software architectural methods, the cloud-native Operator pattern, and legacy programming language techniques can decouple self-adaptive control logic from the TeaStore application. We analyze the trade-offs that these different approaches make between fine-grained expressive adaptation and system-wide control, and highlight when reuse of adaptation strategies is most effective. Our analysis suggests that these approaches are not mutually exclusive but can be combined into a multi-tiered architecture for self-adaptive microservices.

</details>


### [25] [Modality Inflation: Energy Characterization and Optimization Opportunities for MLLM Inference](https://arxiv.org/abs/2512.22695)
*Mona Moghadampanah,Adib Rezaei Shahmirzadi,Farhana Amin,Dimitrios S. Nikolopoulos*

Main category: cs.DC

TL;DR: 本文首次对多模态大语言模型（MLLM）推理过程中的能耗进行分阶段分析，揭示了模态膨胀带来的能效瓶颈，并提出动态电压频率调节优化方案。


<details>
  <summary>Details</summary>
Motivation: 现有研究多聚焦纯文本模型，忽视多模态输入引入的能耗权衡问题，亟需系统性分析以指导高效系统设计。

Method: 将MLLM推理拆分为视觉编码、预填充和解码三阶段，在NVIDIA A100 GPU上评估四类代表性模型，结合GPU功耗追踪与动态电压频率调节实验。

Result: 多模态推理相较纯文本基线增加17%-94%能耗，瓶颈因架构而异：或源于计算密集型视觉编码器，或来自视觉token序列扩增；同时发现GPU利用率低下及输入复杂度引发的能耗缩放差异。

Conclusion: 通过分阶段动态电压频率调节可有效节能且性能损失小，为构建高能效MLLM服务系统提供实践指导。

Abstract: Multimodal large language models (MLLMs) are built on text-only LLMs by incorporating additional modalities, enabling multimodal understanding and a broader range of applications. However, these additions introduce a previously unexplored energy trade-off across modalities that remains poorly understood, as most prior work focuses on text-only models. In this paper, we examine modality inflation, a key source of inefficiency in which multimodal inputs increase inference workloads through extra encoding stages and expanded token sequences. We provide the first detailed, stage-level analysis of energy consumption in MLLM inference by breaking the pipeline into vision encoding, prefill, and decoding stages. Using four representative MLLMs evaluated on NVIDIA A100 GPU, we quantify the additional energy required for multimodal inference compared to text-only baselines, observing overheads ranging from 17% to 94% across models for identical inputs. Our results show that energy bottlenecks differ widely across model architectures, stemming either from compute-heavy vision encoders or from the downstream impact of large visual token sequences during prefill. By examining GPU power traces, we further uncover substantial GPU underutilization during multimodal execution and show that input complexity leads to markedly different energy scaling behaviors across models. Finally, we demonstrate that stage-wise dynamic voltage and frequency scaling (DVFS) is an effective optimization, allowing energy savings with only modest performance impact. Together, these findings offer practical insights and concrete guidance for designing more energy-efficient multimodal LLM serving systems.

</details>


### [26] [Argus: Token Aware Distributed LLM Inference Optimization](https://arxiv.org/abs/2512.22925)
*Panlong Wu,Yifei Zhong,Danyang Chen,Ting Wang,Fangxin Wang*

Main category: cs.DC

TL;DR: Argus 是首个支持边缘云协同的 LLM 推理框架，通过预测输出长度与优化任务卸载提升动态环境下的推理效率。


<details>
  <summary>Details</summary>
Motivation: 现有方案忽视边缘云异构环境中输出长度变化和设备多样性对推理时延的影响。

Method: 提出 LAS 模块预测输出长度，LOO 模块优化长期体验质量，并设计 IODCC 算法求解非线性整数规划问题。

Result: 实验证明 Argus 在动态异构环境下具备鲁棒性和高效性。

Conclusion: Argus 为边缘云部署 LLM 提供了实用且高效的推理解决方案。

Abstract: Large Language Models (LLMs) are rapidly being integrated into real-world applications, yet their autoregressive architectures introduce significant inference time variability, especially when deployed across heterogeneous edge-cloud systems. Existing solutions largely neglect the dynamic, stochastic, and heterogeneous nature of such environments, often ignoring the impact of variable output token lengths and device diversity. In this work, we present Argus, the first token-aware distributed edge-cloud LLM inference framework that conducts efficient task offloading. Argus features a Length-Aware Semantics (LAS) module, which predicts output token lengths for incoming prompts using a fine-tuned language model with token-length-sensitive feature modulation, enabling precise estimation. Building on this, our Lyapunov-guided Offloading Optimization (LOO) module formulates long-term Quality-of-Experience optimization that explicitly considers both LLM prefilling and decoding costs. We introduce a novel Iterative Offloading Algorithm with Damping and Congestion Control (IODCC) to effectively solve the resulting integer nonlinear programming problem under time-varying constraints. Extensive theoretical and empirical evaluations demonstrate that Argus achieves robust performance and superior efficiency in highly dynamic, heterogeneous settings.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [27] [An Energy-Efficient RFET-Based Stochastic Computing Neural Network Accelerator](https://arxiv.org/abs/2512.22131)
*Sheng Lu,Qianhou Qu,Sungyong Jung,Qilian Liang,Chenyun Pan*

Main category: cs.AR

TL;DR: 提出基于RFET的SCNN架构，显著降低面积、延迟和能耗。


<details>
  <summary>Details</summary>
Motivation: 传统SCNN因SNG和APC等组件资源消耗高，限制性能。

Method: 利用RFET器件级可重构性设计高效紧凑的核心模块，并构建专用加速器架构。

Result: 实验表明，相比FinFET设计，RFET方案在相同工艺节点下大幅减少面积、延迟与能耗。

Conclusion: RFET为基础的SCNN架构在硬件效率上具有显著优势。

Abstract: Stochastic computing (SC) offers significant reductions in hardware complexity for traditional convolutional neural networks (CNNs), but stochastic computing neural networks (SCNNs) still suffer from high resource usage due to components such as stochastic number generators (SNGs) and accumulative parallel counters (APCs), which limit performance. This paper introduces a novel SCNN architecture based on reconfigurable field-effect transistors (RFETs), whose device-level reconfigurability enables the design of highly efficient and compact SNGs, APCs, and other core modules. A dedicated SCNN accelerator architecture is also developed for system-level simulation. Using publicly available open-source standard cell libraries, experimental results show that the proposed RFET-based SCNN accelerator achieves substantial reductions in area, latency, and energy consumption compared to a FinFET-based design at the same technology node.

</details>


### [28] [AnalogSAGE: Self-evolving Analog Design Multi-Agents with Stratified Memory and Grounded Experience](https://arxiv.org/abs/2512.22435)
*Zining Wang,Jian Gao,Weimin Fu,Xiaolong Guo,Xuan Zhang*

Main category: cs.AR

TL;DR: AnalogSAGE是一个开源、自演化的多智能体框架，通过分层记忆与仿真反馈显著提升模拟电路设计的自动化能力。


<details>
  <summary>Details</summary>
Motivation: 现有LLM方法在模拟电路设计中依赖提示或预设模板，难以满足复杂规格需求，亟需更自主、可靠的设计自动化方案。

Method: 构建三阶段智能体协同探索机制，结合四层分层记忆结构，实现基于仿真的迭代优化，并开源代码以支持复现与泛化。

Result: 在SKY130 PDK与ngspice环境下，整体通过率提升10倍，Pass@1提升48倍，参数搜索空间缩减4倍。

Conclusion: 分层记忆与接地推理机制可显著增强模拟电路设计自动化的可靠性与自主性。

Abstract: Analog circuit design remains a knowledge- and experience-intensive process that relies heavily on human intuition for topology generation and device parameter tuning. Existing LLM-based approaches typically depend on prompt-driven netlist generation or predefined topology templates, limiting their ability to satisfy complex specification requirements. We propose AnalogSAGE, an open-source self-evolving multi-agent framework that coordinates three-stage agent explorations through four stratified memory layers, enabling iterative refinement with simulation-grounded feedback. To support reproducibility and generality, we release the source code. Our benchmark spans ten specification-driven operational amplifier design problems of varying difficulty, enabling quantitative and cross-task comparison under identical conditions. Evaluated under the open-source SKY130 PDK with ngspice, AnalogSAGE achieves a 10$\times$ overall pass rate, a 48$\times$ Pass@1, and a 4$\times$ reduction in parameter search space compared with existing frameworks, demonstrating that stratified memory and grounded reasoning substantially enhance the reliability and autonomy of analog design automation in practice.

</details>


### [29] [TYTAN: Taylor-series based Non-Linear Activation Engine for Deep Learning Accelerators](https://arxiv.org/abs/2512.23062)
*Soham Pramanik,Vimal William,Arnab Raha,Debayan Das,Amitava Mukherjee,Janet L. Paluh*

Main category: cs.AR

TL;DR: TYTAN是一种基于泰勒级数的非线性激活引擎，旨在加速边缘AI推理并降低功耗，相比NVDLA实现性能提升约2倍、功耗降低56%、面积减少35倍。


<details>
  <summary>Details</summary>
Motivation: 边缘AI部署面临计算成本与能耗限制，需优化高功耗操作如GEMM与激活函数以提升能效。

Method: 提出可重构硬件结合动态近似算法的通用非线性近似引擎（G-NAE），针对每层激活函数自适应调整近似精度。

Result: 在FreePDK45工艺下系统仿真显示，TYTAN工作频率>950MHz，相较NVDLA性能提升约2倍、功耗降56%、面积缩减35倍。

Conclusion: TYTAN有效支持高效能、低功耗边缘AI推理，为领域专用架构提供可行方案。

Abstract: The rapid advancement in AI architectures and the proliferation of AI-enabled systems have intensified the need for domain-specific architectures that enhance both the acceleration and energy efficiency of AI inference, particularly at the edge. This need arises from the significant resource constraints-such as computational cost and energy consumption-associated with deploying AI algorithms, which involve intensive mathematical operations across multiple layers. High-power-consuming operations, including General Matrix Multiplications (GEMMs) and activation functions, can be optimized to address these challenges. Optimization strategies for AI at the edge include algorithmic approaches like quantization and pruning, as well as hardware methodologies such as domain-specific accelerators. This paper proposes TYTAN: TaYlor-series based non-linear acTivAtion eNgine, which explores the development of a Generalized Non-linear Approximation Engine (G-NAE). TYTAN targets the acceleration of non-linear activation functions while minimizing power consumption. The TYTAN integrates a re-configurable hardware design with a specialized algorithm that dynamically estimates the necessary approximation for each activation function, aimed at achieving minimal deviation from baseline accuracy. The proposed system is validated through performance evaluations with state-of-the-art AI architectures, including Convolutional Neural Networks (CNNs) and Transformers. Results from system-level simulations using Silvaco's FreePDK45 process node demonstrate TYTAN's capability to operate at a clock frequency >950 MHz, showcasing its effectiveness in supporting accelerated, energy-efficient AI inference at the edge, which is ~2 times performance improvement, with ~56% power reduction and ~35 times lower area compared to the baseline open-source NVIDIA Deep Learning Accelerator (NVDLA) implementation.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [30] [Syntax Is Not Enough: An Empirical Study of Small Transformer Models for Neural Code Repair](https://arxiv.org/abs/2512.22216)
*Shaunak Samant*

Main category: cs.SE

TL;DR: 小规模Transformer模型在修复真实Java缺陷时表现不佳，尽管语法正确率高，但语义修复准确率为零。


<details>
  <summary>Details</summary>
Motivation: 探索小规模神经模型能否有效修复现实Java缺陷，并检验语法正确性是否足以代表语义正确性。

Method: 基于CodeT5-small模型，在CodeXGLUE数据集上微调，使用AST解析评估语法有效性，以精确匹配衡量修复准确性。

Result: 模型语法正确率达94%，但精确匹配修复成功率为0%，约80%输出直接复制原始错误代码。

Conclusion: 仅依赖语法正确性不足以实现有效程序修复，小模型在语义层面修复能力有限。

Abstract: Automated program repair using neural models has shown promising results on benchmark datasets, yet practical deployment remains limited. In this study, we examine whether a small transformer model can meaningfully repair real-world Java bugs and whether syntactic correctness is a reliable proxy for semantic correctness.
  We fine-tune CodeT5-small (60.5M parameters) on 52,364 Java bug-fix pairs from CodeXGLUE and evaluate both token-level performance and syntactic validity using AST parsing. While the model converges cleanly and achieves high grammatical correctness, producing syntactically valid Java code in approximately ninety-four percent of cases, it fails to generate correct repairs under exact-match evaluation, achieving zero exact matches. In approximately eighty percent of cases, the model reproduces the buggy input verbatim.

</details>


### [31] [Failure Analysis of Safety Controllers in Autonomous Vehicles Under Object-Based LiDAR Attacks](https://arxiv.org/abs/2512.22244)
*Daniyal Ganiuly,Nurzhau Bolatbek,Assel Smaiyl*

Main category: cs.SE

TL;DR: 本文系统分析了在高速公路场景下，基于物体的LiDAR攻击如何导致纵向安全控制器失效，揭示感知鲁棒性与控制层安全保证之间的关键差距。


<details>
  <summary>Details</summary>
Motivation: 当前对LiDAR感知攻击的研究尚未充分理解其对车辆安全控制器的实际影响，亟需系统性失效分析以提升自动驾驶安全性。

Method: 构建高保真仿真框架，集成LiDAR感知、目标跟踪与闭环车辆控制，模拟虚假和位移物体检测在感知-规划-控制流程中的传播效应。

Result: 即使短暂的LiDAR幻象也会引发不安全制动、对真实危险反应延迟及控制不稳定；切入场景中不安全减速和碰撞时间违规显著增加；控制器失效更受欺骗物体时序一致性影响而非单纯空间误差。

Conclusion: 该研究为设计攻击感知的安全机制和增强LiDAR依赖型自动驾驶车辆的控制策略提供了实用指导，填补了感知与控制安全间的理论实践鸿沟。

Abstract: Autonomous vehicles rely on LiDAR based perception to support safety critical control functions such as adaptive cruise control and automatic emergency braking. While previous research has shown that LiDAR perception can be manipulated through object based spoofing and injection attacks, the impact of such attacks on vehicle safety controllers is still not well understood. This paper presents a systematic failure analysis of longitudinal safety controllers under object based LiDAR attacks in highway driving scenarios. The study focuses on realistic cut in and car following situations in which adversarial objects introduce persistent perception errors without directly modifying vehicle control software. A high fidelity simulation framework integrating LiDAR perception, object tracking, and closed loop vehicle control is used to evaluate how false and displaced object detections propagate through the perception planning and control pipeline. The results demonstrate that even short duration LiDAR induced object hallucinations can trigger unsafe braking, delayed responses to real hazards, and unstable control behavior. In cut in scenarios, a clear increase in unsafe deceleration events and time to collision violations is observed when compared to benign conditions, despite identical controller parameters. The analysis further shows that controller failures are more strongly influenced by the temporal consistency of spoofed objects than by spatial inaccuracies alone. These findings reveal a critical gap between perception robustness and control level safety guarantees in autonomous driving systems. By explicitly characterizing safety controller failure modes under adversarial perception, this work provides practical insights for the design of attack aware safety mechanisms and more resilient control strategies for LiDAR dependent autonomous vehicles.

</details>


### [32] [Hallucination Detection for LLM-based Text-to-SQL Generation via Two-Stage Metamorphic Testing](https://arxiv.org/abs/2512.22250)
*Bo Yang,Yinfen Xia,Weisong Sun,Yang Liu*

Main category: cs.SE

TL;DR: 提出SQLHD方法，基于蜕变测试检测Text-to-SQL任务中的LLM幻觉，无需标准答案且F1分数达69.36%-82.76%。


<details>
  <summary>Details</summary>
Motivation: 现有错误检测方法难以应对LLM生成的幻觉问题，且缺乏真实标签数据支持。

Method: 分两阶段使用结构感知和逻辑感知的蜕变关系扰动输入并交叉验证输出，从而检测幻觉。

Result: 在多个指标上优于现有方法，尤其F1分数表现优异，且超越LLM自评估方法。

Conclusion: SQLHD有效解决无监督场景下LLM幻觉检测难题，提升Text-to-SQL可靠性。

Abstract: In Text-to-SQL generation, large language models (LLMs) have shown strong generalization and adaptability. However, LLMs sometimes generate hallucinations, i.e.,unrealistic or illogical content, which leads to incorrect SQL queries and negatively impacts downstream applications. Detecting these hallucinations is particularly challenging. Existing Text-to-SQL error detection methods, which are tailored for traditional deep learning models, face significant limitations when applied to LLMs. This is primarily due to the scarcity of ground-truth data. To address this challenge, we propose SQLHD, a novel hallucination detection method based on metamorphic testing (MT) that does not require standard answers. SQLHD splits the detection task into two sequentiial stages: schema-linking hallucination detection via eight structure-aware Metamorphic Relations (MRs) that perturb comparative words, entities, sentence structure or database schema, and logical-synthesis hallucination detection via nine logic-aware MRs that mutate prefix words, extremum expressions, comparison ranges or the entire database. In each stage the LLM is invoked separately to generate schema mappings or SQL artefacts; the follow-up outputs are cross-checked against their source counterparts through the corresponding MRs, and any violation is flagged as a hallucination without requiring ground-truth SQL. The experimental results demonstrate our method's superior performance in terms of the F1-score, which ranges from 69.36\% to 82.76\%. Additionally, SQLHD demonstrates superior performance over LLM Self-Evaluation methods, effectively identifying hallucinations in Text-to-SQL tasks.

</details>


### [33] [Agentic Software Issue Resolution with Large Language Models: A Survey](https://arxiv.org/abs/2512.22256)
*Zhonghao Jiang,David Lo,Zhongxin Liu*

Main category: cs.SE

TL;DR: 本文系统综述了基于大语言模型的智能体在软件问题解决中的最新研究，涵盖基准、技术与实证三个维度，并探讨了智能体强化学习带来的范式转变及未来方向。


<details>
  <summary>Details</summary>
Motivation: 现实软件问题解决复杂且需长程推理与反馈驱动决策，传统单步方法难以胜任，亟需具备智能体能力的新方案。

Method: 对126项前沿研究进行系统性综述，构建任务流程框架并建立三维度分类体系：基准、技术、实证研究。

Result: 揭示了智能体强化学习正推动软件工程智能体系统设计与训练的范式变革，同时识别出当前关键挑战与未来研究方向。

Conclusion: 智能体化方法显著提升软件维护效率与质量，并为验证AI系统的推理、规划与执行能力提供真实场景，促进人工智能与软件工程融合。

Abstract: Software issue resolution aims to address real-world issues in software repositories (e.g., bug fixing and efficiency optimization) based on natural language descriptions provided by users, representing a key aspect of software maintenance. With the rapid development of large language models (LLMs) in reasoning and generative capabilities, LLM-based approaches have made significant progress in automated software issue resolution. However, real-world software issue resolution is inherently complex and requires long-horizon reasoning, iterative exploration, and feedback-driven decision making, which demand agentic capabilities beyond conventional single-step approaches. Recently, LLM-based agentic systems have become mainstream for software issue resolution. Advancements in agentic software issue resolution not only greatly enhance software maintenance efficiency and quality but also provide a realistic environment for validating agentic systems' reasoning, planning, and execution capabilities, bridging artificial intelligence and software engineering.
  This work presents a systematic survey of 126 recent studies at the forefront of LLM-based agentic software issue resolution research. It outlines the general workflow of the task and establishes a taxonomy across three dimensions: benchmarks, techniques, and empirical studies. Furthermore, it highlights how the emergence of agentic reinforcement learning has brought a paradigm shift in the design and training of agentic systems for software engineering. Finally, it summarizes key challenges and outlines promising directions for future research.

</details>


### [34] [AI-Generated Code Is Not Reproducible (Yet): An Empirical Study of Dependency Gaps in LLM-Based Coding Agents](https://arxiv.org/abs/2512.22387)
*Bhanu Prakash Vangala,Ali Adibifar,Tanu Malik,Ashish Gehani*

Main category: cs.SE

TL;DR: 该研究评估了大型语言模型生成代码的可复现性，发现仅有68.3%项目能直接运行，且存在显著隐藏依赖。


<details>
  <summary>Details</summary>
Motivation: 探索LLM生成代码在干净环境中的执行可复现性问题，填补当前研究空白。

Method: 通过三层次依赖框架（声明、工作、运行时依赖），对三个主流LLM在300个项目中进行实证分析。

Result: Python项目成功率最高（89.2%），Java最低（44.0%）；运行时依赖平均膨胀13.5倍。

Conclusion: LLM生成代码普遍存在依赖缺失问题，需改进依赖声明机制以提升可复现性。

Abstract: The rise of Large Language Models (LLMs) as coding agents promises to accelerate software development, but their impact on generated code reproducibility remains largely unexplored. This paper presents an empirical study investigating whether LLM-generated code can be executed successfully in a clean environment with only OS packages and using only the dependencies that the model specifies. We evaluate three state-of-the-art LLM coding agents (Claude Code, OpenAI Codex, and Gemini) across 300 projects generated from 100 standardized prompts in Python, JavaScript, and Java. We introduce a three-layer dependency framework (distinguishing between claimed, working, and runtime dependencies) to quantify execution reproducibility. Our results show that only 68.3% of projects execute out-of-the-box, with substantial variation across languages (Python 89.2%, Java 44.0%). We also find a 13.5 times average expansion from declared to actual runtime dependencies, revealing significant hidden dependencies.

</details>


### [35] [Building Software by Rolling the Dice: A Qualitative Study of Vibe Coding](https://arxiv.org/abs/2512.22418)
*Yi-Hung Chou,Boyuan Jiang,Yi Wen Chen,Mingyue Weng,Victoria Jackson,Thomas Zimmermann,James A. Jones*

Main category: cs.SE

TL;DR: 研究探讨了‘氛围编程’现象，即开发者主要通过提示而非编写代码来构建软件，揭示了不同实践者的行为差异及对AI生成结果的依赖与调试挑战。


<details>
  <summary>Details</summary>
Motivation: 了解实践中‘氛围编程’的具体定义和参与方式，填补当前对该现象认知的空白。

Method: 通过对20个‘氛围编程’视频（含7场直播编码和13个观点视频）进行扎根理论研究，并辅以活动时长与提示意图分析。

Result: 发现实践者行为呈光谱分布：部分完全依赖AI不检查代码，部分则审查并调整输出；所有人均需应对生成的随机性，调试常被形容为‘掷骰子’；不同心智模型影响提示策略、评估习惯与信任程度。

Conclusion: 该研究为软件工程未来研究指明新方向，并为工具设计与教育提供实践启示。

Abstract: Large language models (LLMs) are reshaping software engineering by enabling "vibe coding," in which developers build software primarily through prompts rather than writing code. Although widely publicized as a productivity breakthrough, little is known about how practitioners actually define and engage in these practices. To shed light on this emerging phenomenon, we conducted a grounded theory study of 20 vibe-coding videos, including 7 live-streamed coding sessions (about 16 hours, 254 prompts) and 13 opinion videos (about 5 hours), supported by additional analysis of activity durations and prompt intents. Our findings reveal a spectrum of behaviors: some vibe coders rely almost entirely on AI without inspecting code, while others examine and adapt generated outputs. Across approaches, all must contend with the stochastic nature of generation, with debugging and refinement often described as "rolling the dice." Further, divergent mental models, shaped by vibe coders' expertise and reliance on AI, influence prompting strategies, evaluation practices, and levels of trust. These findings open new directions for research on the future of software engineering and point to practical opportunities for tool design and education.

</details>


### [36] [GraphLocator: Graph-guided Causal Reasoning for Issue Localization](https://arxiv.org/abs/2512.22469)
*Wei Liu,Chao Peng,Pengfei Gao,Aofan Liu,Wei Zhang,Haiyan Zhao,Zhi Jin*

Main category: cs.SE

TL;DR: GraphLocator通过构建因果问题图（CIG）解决软件工程中问题定位的语义鸿沟，显著提升定位准确率与下游任务表现。


<details>
  <summary>Details</summary>
Motivation: 自然语言问题描述与代码实现间存在症状-原因不匹配及一对多不匹配，导致自动化问题定位困难。

Method: 提出GraphLocator方法，利用因果结构发现缓解症状-原因不匹配，通过动态问题解耦处理一对多不匹配，构建并扩展因果问题图（CIG）。

Result: 在三个真实数据集上，GraphLocator在函数级召回率和精确率分别平均提升19.49%和11.89%，并在两类不匹配场景下均显著优于基线方法，CIG使下游任务性能提升28.74%。

Conclusion: GraphLocator有效弥合语义鸿沟，显著提升问题定位准确性及对复杂依赖场景的适应能力。

Abstract: The issue localization task aims to identify the locations in a software repository that requires modification given a natural language issue description. This task is fundamental yet challenging in automated software engineering due to the semantic gap between issue description and source code implementation. This gap manifests as two mismatches:(1) symptom-to-cause mismatches, where descriptions do not explicitly reveal underlying root causes; (2) one-to-many mismatches, where a single issue corresponds to multiple interdependent code entities. To address these two mismatches, we propose GraphLocator, an approach that mitigates symptom-to-cause mismatches through causal structure discovering and resolves one-to-many mismatches via dynamic issue disentangling. The key artifact is the causal issue graph (CIG), in which vertices represent discovered sub-issues along with their associated code entities, and edges encode the causal dependencies between them. The workflow of GraphLocator consists of two phases: symptom vertices locating and dynamic CIG discovering; it first identifies symptom locations on the repository graph, then dynamically expands the CIG by iteratively reasoning over neighboring vertices. Experiments on three real-world datasets demonstrates the effectiveness of GraphLocator: (1) Compared with baselines, GraphLocator achieves more accurate localization with average improvements of +19.49% in function-level recall and +11.89% in precision. (2) GraphLocator outperforms baselines on both symptom-to-cause and one-to-many mismatch scenarios, achieving recall improvement of +16.44% and +19.18%, precision improvement of +7.78% and +13.23%, respectively. (3) The CIG generated by GraphLocator yields the highest relative improvement, resulting in a 28.74% increase in performance on downstream resolving task.

</details>


### [37] [Isolating Compiler Faults via Multiple Pairs of Adversarial Compilation Configurations](https://arxiv.org/abs/2512.22538)
*Qingyang Li,Yibiao Yang,Maolin Sun,Jiangchang Wu,Qingkai Shi,Yuming Zhou*

Main category: cs.SE

TL;DR: MultiConf是一种通过构建多对对抗性编译配置并结合频谱故障定位技术，显著提升编译器故障文件定位准确率的新方法。


<details>
  <summary>Details</summary>
Motivation: 现代编译器结构复杂，故障定位困难，亟需高效自动化方法精准定位引发故障的源文件。

Method: 构建多组仅含少量细粒度选项差异的对抗编译配置对，利用SBFL公式独立排序可疑文件，再以加权投票聚合生成最终排名。

Result: 在60个真实GCC缺陷测试中，Top-1定位成功率达45%（27/60），较Odfl和Basic分别提升35.0%与28.6%。

Conclusion: MultiConf在效率与准确性上均优于现有技术，为编译器故障定位提供了更可靠、高效的解决方案。

Abstract: Compilers are fundamental to modern software development, making the effective identification and resolution of compiler faults essential. However, localizing these faults to specific source files remains highly challenging due to the complexity and scale of modern compiler infrastructures. In this study, we propose MultiConf, a novel approach that automatically isolates compiler faults by constructing multiple pairs of adversarial compilation configurations. Each adversarial compilation configuration pair consists of a failing configuration and its corresponding passing configuration, which differ in only a small number of fine-grained options. MultiConf generates failing configurations through a lightweight construction process and derives the corresponding passing configurations by selectively disabling bug-related fine-grained options. We then employ a Spectrum-Based Fault Localization (SBFL) formula to rank the suspiciousness of compiler source files. Each adversarial configuration pair independently produces a ranking, which is subsequently aggregated using a weighted voting scheme to derive a final suspiciousness ranking, enabling more accurate and robust fault localization. We evaluate MultiConf on a benchmark of 60 real-world GCC compiler bugs. The results demonstrate that MultiConf significantly outperforms existing compiler fault localization techniques in both effectiveness and efficiency. In particular, MultiConf successfully localizes 27 out of 60 bugs at the Top-1 file level, representing improvements of 35.0% and 28.6% over the two state-of-the-art approaches, Odfl(20) and Basic(21), respectively.

</details>


### [38] [Rethinking the Capability of Fine-Tuned Language Models for Automated Vulnerability Repair](https://arxiv.org/abs/2512.22633)
*Woorim Han,Yeongjun Kwak,Miseon Yu,Kyeongmin Kim,Younghan Lee,Hyungon Moon,Yunheung Paek*

Main category: cs.SE

TL;DR: 本文评估了基于学习的自动漏洞修复模型及其评估指标的有效性，提出了改进方法以增强泛化能力和修复真实性。


<details>
  <summary>Details</summary>
Motivation: 现有模型存在过拟合问题，且评估方式未能充分反映真实修复能力。

Method: 通过语义保持变换、重新划分数据集和构建新基准L-AVRBench进行评估。

Result: 发现当前模型依赖虚假特征，泛化能力有限；新基准能更准确衡量修复效果。

Conclusion: 需改进模型训练策略与评估体系，以提升对未知漏洞的实际修复能力。

Abstract: Learning-based automated vulnerability repair (AVR) techniques that utilize fine-tuned language models have shown promise in generating vulnerability patches. However, questions remain about their ability to repair unseen vulnerabilities. Our empirical study reveals that state-of-the-art models often overfit to the training set and are evaluated using training, validation, and test sets that are not mutually exclusive. Furthermore, relying on match-based metrics that compare generated patches to reference fixes at the token level has some limitations, failing to account for the possibility of various valid ways to patch the vulnerability. In this paper, we examine the capabilities of state-of-the-art fine-tuned AVR models and the adequacy of match-based evaluation metrics in three ways. First, we apply semantic-preserving transformations to test sets in order to determine whether models truly learn robust vulnerability-repair patterns or simply rely on spurious features. Second, we re-split the training, validation, and test sets to be mutually exclusive and evaluate the models on the revised test set to assess their generalization capabilities. Third, we introduce L-AVRBench, a test-based benchmark tailored for learning-based AVR, to overcome the limitations of match-based metrics and examine the AVR models' true repair capabilities.

</details>


### [39] [CFIghter: Automated Control-Flow Integrity Enablement and Evaluation for Legacy C/C++ Systems](https://arxiv.org/abs/2512.22701)
*Sabine Houy,Bruno Kreyssig,Alexandre Bartel*

Main category: cs.SE

TL;DR: CFIghter自动化修复CFI兼容性问题，使严格编译器CFI可在大型C/C++项目中实际部署。


<details>
  <summary>Details</summary>
Motivation: 解决大型C/C++软件中因可见性错配、类型不一致和意外行为导致的CFI部署难题。

Method: 结合全程序分析与运行时监控，迭代应用最小必要调整，仅在测试失败处放宽CFI策略。

Result: 在四个GNU项目中修复95.8%的非预期CFI违规，89%间接控制流点保持严格CFI，无需手动修改源码。

Conclusion: 自动化兼容性修复使严格编译器CFI可实际部署于成熟模块化C软件。

Abstract: Compiler-based Control-Flow Integrity (CFI) offers strong forward-edge protection but remains challenging to deploy in large C/C++ software due to visibility mismatches, type inconsistencies, and unintended behavioral failures. We present CFIghter, the first fully automated system that enables strict, type-based CFI in real-world projects by detecting, classifying, and repairing unintended policy violations exposed by the test suite. CFIghter integrates whole-program analysis with guided runtime monitoring and iteratively applies the minimal necessary adjustments to CFI enforcement only where required, stopping once all tests pass or remaining failures are deemed unresolvable. We evaluate CFIghter on four GNU projects. It resolves all visibility-related build errors and automatically repairs 95.8% of unintended CFI violations in the large, multi-library util-linux codebase, while retaining strict enforcement at over 89% of indirect control-flow sites. Across all subjects, CFIghter preserves strict type-based CFI for the majority of the codebase without requiring manual source-code changes, relying only on automatically generated visibility adjustments and localized enforcement scopes where necessary. These results show that automated compatibility repair makes strict compiler CFI practically deployable in mature, modular C software.

</details>


### [40] [From Rookie to Expert: Manipulating LLMs for Automated Vulnerability Exploitation in Enterprise Software](https://arxiv.org/abs/2512.22753)
*Moustapha Awwalou Diouf,Maimouna Tamah Diao,Iyiola Emmanuel Olatunji,Abdoul Kader Kaboré,Jordan Samhi,Gervais Mendy,Samuel Ouya,Jacques Klein,Tegawendé F. Bissyandé*

Main category: cs.SE

TL;DR: 本文提出RSA策略，通过角色分配、场景预设和行动诱导，成功让主流LLM生成可运行漏洞利用代码，实现非技术人员零门槛攻击，颠覆传统软件安全假设。


<details>
  <summary>Details</summary>
Motivation: 探索LLM如何被社会工程化操纵，使无编程经验者也能生成有效攻击代码，从而挑战‘攻击需技术专长’的传统安全前提。

Method: 设计RSA三步提示策略（角色分配、场景预设、行动诱导），在Odoo平台测试五款主流LLM，评估其绕过安全机制生成CVE漏洞利用的能力。

Result: 五款LLM均100%成功，在3-4轮提示内生成可用漏洞利用代码，证明无需人工干预即可完成攻击自动化。

Conclusion: 软件工程安全范式需重构：技术与非技术人员界限失效，漏洞复杂性不再构成防护，开发工具本身可能成为攻击媒介，未来安全应聚焦提示工程防御。

Abstract: LLMs democratize software engineering by enabling non-programmers to create applications, but this same accessibility fundamentally undermines security assumptions that have guided software engineering for decades. We show in this work how publicly available LLMs can be socially engineered to transform novices into capable attackers, challenging the foundational principle that exploitation requires technical expertise. To that end, we propose RSA (Role-assignment, Scenario-pretexting, and Action-solicitation), a pretexting strategy that manipulates LLMs into generating functional exploits despite their safety mechanisms. Testing against Odoo -- a widely used ERP platform, we evaluated five mainstream LLMs (GPT-4o, Gemini, Claude, Microsoft Copilot, and DeepSeek) and achieved a 100% success rate: tested CVE yielded at least one working exploit within 3-4 prompting rounds. While prior work [13] found LLM-assisted attacks difficult and requiring manual effort, we demonstrate that this overhead can be eliminated entirely.
  Our findings invalidate core software engineering security principles: the distinction between technical and non-technical actors no longer provides valid threat models; technical complexity of vulnerability descriptions offers no protection when LLMs can abstract it away; and traditional security boundaries dissolve when the same tools that build software can be manipulated to break it. This represents a paradigm shift in software engineering -- we must redesign security practices for an era where exploitation requires only the ability to craft prompts, not understand code.
  Artifacts available at: https://anonymous.4open.science/r/From-Rookie-to-Attacker-D8B3.

</details>


### [41] [FasterPy: An LLM-based Code Execution Efficiency Optimization Framework](https://arxiv.org/abs/2512.22827)
*Yue Wu,Minghao Han,Ruiyin Li,Peng Liang,Amjed Tahir,Zengyang Li,Qiong Feng,Mojtaba Shahin*

Main category: cs.SE

TL;DR: FasterPy是一个结合RAG与LoRA的低成本高效框架，利用大语言模型优化Python代码执行效率，在PIE基准上表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 传统规则方法劳动密集且适用性有限，机器学习方法依赖特定表示和昂贵数据集，难以扩展，因此需要更低成本高效的自动化优化方案。

Method: 提出FasterPy框架，结合检索增强生成（RAG）与低秩适配（LoRA），利用性能改进代码对构建知识库，引导大语言模型优化Python代码。

Result: 在PIE基准测试中，FasterPy在多个指标上优于现有模型，验证了其有效性与优越性。

Conclusion: FasterPy为自动化代码性能优化提供了一种低成本、易扩展且高效的新途径，具备实际应用价值。

Abstract: Code often suffers from performance bugs. These bugs necessitate the research and practice of code optimization. Traditional rule-based methods rely on manually designing and maintaining rules for specific performance bugs (e.g., redundant loops, repeated computations), making them labor-intensive and limited in applicability. In recent years, machine learning and deep learning-based methods have emerged as promising alternatives by learning optimization heuristics from annotated code corpora and performance measurements. However, these approaches usually depend on specific program representations and meticulously crafted training datasets, making them costly to develop and difficult to scale. With the booming of Large Language Models (LLMs), their remarkable capabilities in code generation have opened new avenues for automated code optimization. In this work, we proposed FasterPy, a low-cost and efficient framework that adapts LLMs to optimize the execution efficiency of Python code. FasterPy combines Retrieval-Augmented Generation (RAG), supported by a knowledge base constructed from existing performance-improving code pairs and corresponding performance measurements, with Low-Rank Adaptation (LoRA) to enhance code optimization performance. Our experimental results on the Performance Improving Code Edits (PIE) benchmark demonstrate that our method outperforms existing models on multiple metrics. The FasterPy tool and the experimental results are available at https://github.com/WuYue22/fasterpy.

</details>


### [42] [Towards the analysis of team members well-being](https://arxiv.org/abs/2512.22845)
*Zan Xu,Sari Nurfauziyyah,Anastasia Romanova,Kaamesh G S,Yiqun Gao,Maria Spichkova*

Main category: cs.SE

TL;DR: 该论文研究了软件开发团队成员的幸福感，并开发了一个相关原型。


<details>
  <summary>Details</summary>
Motivation: 团队成员的幸福感对工作效率、身体健康及个人生活至关重要，而被认可是影响幸福感的重要因素。

Method: 开展项目分析团队幸福感并构建原型系统。

Result: 成功完成团队幸福感分析并开发出相应原型。

Conclusion: 认可与赞赏对提升团队成员幸福感具有关键作用，值得在实践中重视。

Abstract: Many recent research studies have focused on the well-being of software development team members, as this aspect may be critical not only for productivity and performance at work but also for the physical health and personal life of employees. Many studies agree that an important factor of team member well-being is whether team members feel appreciated and acknowledged for their contributions. This paper presents the results of a project on the team well-being analysis as well as the prototype developed within the project.

</details>


### [43] [Interpretable Gallbladder Ultrasound Diagnosis: A Lightweight Web-Mobile Software Platform with Real-Time XAI](https://arxiv.org/abs/2512.23033)
*Fuyad Hasan Bhoyan,Prashanta Sarker,Parsia Noor Ethila,Md. Emon Hossain,Md Kaviul Hossain,Md Humaion Kabir Mehedi*

Main category: cs.SE

TL;DR: 该研究提出了一种基于混合深度学习模型MobResTaNet的AI诊断软件，可从超声图像中高精度分类胆囊疾病，并通过XAI提供可解释性，支持临床决策。


<details>
  <summary>Details</summary>
Motivation: 解决胆囊疾病超声诊断困难的问题，实现早期准确检测。

Method: 采用MobResTaNet混合深度学习模型，结合XAI可视化技术，开发了Web与移动端应用。

Result: 模型准确率达99.85%，仅含2.24M参数，具备高效、可及和可信的床旁诊断能力。

Conclusion: 该系统为胆囊疾病的实时、透明化AI辅助诊断提供了实用解决方案。

Abstract: Early and accurate detection of gallbladder diseases is crucial, yet ultrasound interpretation is challenging. To address this, an AI-driven diagnostic software integrates our hybrid deep learning model MobResTaNet to classify ten categories, nine gallbladder disease types and normal directly from ultrasound images. The system delivers interpretable, real-time predictions via Explainable AI (XAI) visualizations, supporting transparent clinical decision-making. It achieves up to 99.85% accuracy with only 2.24M parameters. Deployed as web and mobile applications using HTML, CSS, JavaScript, Bootstrap, and Flutter, the software provides efficient, accessible, and trustworthy diagnostic support at the point of care

</details>


### [44] [An Automated Grey Literature Extraction Tool for Software Engineering](https://arxiv.org/abs/2512.23066)
*Houcine Abdelkader Cherief,Brahim Mahmoudi,Zacharie Chenail-Larcher,Naouel Moha,Quentin Sti'evenart,Florent Avellaneda*

Main category: cs.SE

TL;DR: GLiSE是一个基于提示驱动的工具，用于自动化收集和评估软件工程领域的灰色文献，提升研究可复现性与效率。


<details>
  <summary>Details</summary>
Motivation: 灰色文献对软件工程研究至关重要，但其来源异构、格式多样，难以大规模、可复现地收集与评估。

Method: GLiSE将研究主题提示转化为平台特定查询，从GitHub、Stack Overflow和Google等源获取结果，并利用语义嵌入分类器按相关性过滤和排序。

Result: 提供了GLiSE工具、一个经语义相关性标注的灰色文献搜索结果数据集，并完成了工具可用性的实证研究。

Conclusion: GLiSE有效支持了灰色文献的大规模、可配置、可复现检索，有助于提升软件工程实证研究的质量与效率。

Abstract: Grey literature is essential to software engineering research as it captures practices and decisions that rarely appear in academic venues. However, collecting and assessing it at scale remains difficult because of their heterogeneous sources, formats, and APIs that impede reproducible, large-scale synthesis. To address this issue, we present GLiSE, a prompt-driven tool that turns a research topic prompt into platform-specific queries, gathers results from common software-engineering web sources (GitHub, Stack Overflow) and Google Search, and uses embedding-based semantic classifiers to filter and rank results according to their relevance. GLiSE is designed for reproducibility with all settings being configuration-based, and every generated query being accessible. In this paper, (i) we present the GLiSE tool, (ii) provide a curated dataset of software engineering grey-literature search results classified by semantic relevance to their originating search intent, and (iii) conduct an empirical study on the usability of our tool.

</details>


### [45] [An Empirical Study of Generative AI Adoption in Software Engineering](https://arxiv.org/abs/2512.23327)
*Görkem Giray,Onur Demirörs,Marcos Kalinowski,Daniel Mendez*

Main category: cs.SE

TL;DR: 本文研究了GenAI在软件工程中的实际应用现状、益处、挑战及长远影响，发现其已广泛融入日常工作并带来效率提升，但仍面临输出可靠性、安全隐私等挑战。


<details>
  <summary>Details</summary>
Motivation: 缺乏对GenAI在软件工程实践中使用情况、效益与挑战的实证研究，需系统梳理其应用现状与长期影响。

Method: 通过调研与分析从业者实际使用GenAI的情况，归纳其应用场景、效益、挑战及组织制度化程度。

Result: GenAI被广泛用于实现、验证、维护等任务，显著提升效率与质量，但存在输出不可靠、提示工程难、安全风险等问题；工具制度化程度不一，培训与治理不足；从业者预期角色将被重塑而非取代。

Conclusion: GenAI已在软件工程中深度应用并带来实质效益，但需解决技术可靠性与制度化短板，以实现可持续、负责任的部署。

Abstract: Context. GenAI tools are being increasingly adopted by practitioners in SE, promising support for several SE activities. Despite increasing adoption, we still lack empirical evidence on how GenAI is used in practice, the benefits it provides, the challenges it introduces, and its broader organizational and societal implications. Objective. This study aims to provide an overview of the status of GenAI adoption in SE. It investigates the status of GenAI adoption, associated benefits and challenges, institutionalization of tools and techniques, and anticipated long term impacts on SE professionals and the community. Results. The results indicate a wide adoption of GenAI tools and how they are deeply integrated into daily SE work, particularly for implementation, verification and validation, personal assistance, and maintenance-related tasks. Practitioners report substantial benefits, most notably reduction in cycle time, quality improvements, enhanced support in knowledge work, and productivity gains. However, objective measurement of productivity and quality remains limited in practice. Significant challenges persist, including incorrect or unreliable outputs, prompt engineering difficulties, validation overhead, security and privacy concerns, and risks of overreliance. Institutionalization of tools and techniques seems to be common, but it varies considerably, with a strong focus on tool access and less emphasis on training and governance. Practitioners expect GenAI to redefine rather than replace their roles, while expressing moderate concern about job market contraction and skill shifts.

</details>


### [46] [Securing the AI Supply Chain: What Can We Learn From Developer-Reported Security Issues and Solutions of AI Projects?](https://arxiv.org/abs/2512.23385)
*The Anh Nguyen,Triet Huynh Minh Le,M. Ali Babar*

Main category: cs.SE

TL;DR: 本文通过分析Hugging Face和GitHub上的开发者讨论，构建了一个包含312,868条安全相关讨论的数据集，并归纳出32类安全问题与24类解决方案，覆盖系统、工具生态、模型与数据四大主题，为AI供应链安全提供实证指导。


<details>
  <summary>Details</summary>
Motivation: 当前对AI项目中常见的安全问题及其实际解决方案缺乏系统了解，阻碍了针对AI供应链各环节的有效防护措施开发。

Method: 结合关键词匹配与优化后的distilBERT分类器构建识别管道，筛选安全相关讨论；并对753个样本进行主题分析，建立细粒度分类体系。

Result: 发现多数安全问题源于AI组件的复杂依赖与黑盒特性，尤其在模型与数据方面常缺乏明确解决方案。

Conclusion: 研究结果可为开发者与研究人员提供基于实证的安全应对策略，提升AI供应链整体安全性。

Abstract: The rapid growth of Artificial Intelligence (AI) models and applications has led to an increasingly complex security landscape. Developers of AI projects must contend not only with traditional software supply chain issues but also with novel, AI-specific security threats. However, little is known about what security issues are commonly encountered and how they are resolved in practice. This gap hinders the development of effective security measures for each component of the AI supply chain. We bridge this gap by conducting an empirical investigation of developer-reported issues and solutions, based on discussions from Hugging Face and GitHub. To identify security-related discussions, we develop a pipeline that combines keyword matching with an optimal fine-tuned distilBERT classifier, which achieved the best performance in our extensive comparison of various deep learning and large language models. This pipeline produces a dataset of 312,868 security discussions, providing insights into the security reporting practices of AI applications and projects. We conduct a thematic analysis of 753 posts sampled from our dataset and uncover a fine-grained taxonomy of 32 security issues and 24 solutions across four themes: (1) System and Software, (2) External Tools and Ecosystem, (3) Model, and (4) Data. We reveal that many security issues arise from the complex dependencies and black-box nature of AI components. Notably, challenges related to Models and Data often lack concrete solutions. Our insights can offer evidence-based guidance for developers and researchers to address real-world security threats across the AI supply chain.

</details>


### [47] [An SLO Driven and Cost-Aware Autoscaling Framework for Kubernetes](https://arxiv.org/abs/2512.23415)
*Vinoth Punniyamoorthy,Bikesh Kumar,Sumit Saha,Lokesh Butra,Mayilsamy Palanigounder,Akash Kumar Agarwal,Kabilan Kannan*

Main category: cs.SE

TL;DR: 本文提出了一种基于AIOps的Kubernetes自动扩缩容框架，显著提升SLO达标率并降低成本。


<details>
  <summary>Details</summary>
Motivation: 现有Kubernetes扩缩容机制因响应滞后、缺乏应用层信号和控制不透明，常导致SLO违规与成本浪费。

Method: 提出安全可解释的多信号扩缩容框架，结合SLO感知、成本意识控制与轻量级需求预测。

Result: 实验表明新方法相比基线减少31% SLO违规时长、提升24%响应速度、降低18%基础设施成本。

Conclusion: AIOps驱动的SLO优先扩缩容能显著增强Kubernetes云平台的可靠性、效率与运维可信度。

Abstract: Kubernetes provides native autoscaling mechanisms, including the Horizontal Pod Autoscaler, Vertical Pod Autoscaler, and node-level autoscalers, to enable elastic resource management for cloud-native applications. However, production environments frequently experience Service Level Objective violations and cost inefficiencies due to reactive scaling behavior, limited use of application-level signals, and opaque control logic. This paper investigates how Kubernetes autoscaling can be enhanced using AIOps principles to jointly satisfy SLO and cost constraints under diverse workload patterns without compromising safety or operational transparency. We present a gap-driven analysis of existing autoscaling approaches and propose a safe and explainable multi-signal autoscaling framework that integrates SLO-aware and cost-conscious control with lightweight demand forecasting. Experimental evaluation using representative microservice and event-driven workloads shows that the proposed approach reduces SLO violation duration by up to 31 percent, improves scaling response time by 24 percent, and lowers infrastructure cost by 18 percent compared to default and tuned Kubernetes autoscaling baselines, while maintaining stable and auditable control behavior. These results demonstrate that AIOps-driven, SLO-first autoscaling can significantly improve the reliability, efficiency, and operational trustworthiness of Kubernetes-based cloud platforms.

</details>


### [48] [Embedding Quality Assurance in project-based learning](https://arxiv.org/abs/2512.23488)
*Maria Spichkova*

Main category: cs.SE

TL;DR: 本文总结了在敏捷/Scrum环境下教授软件质量十余年的经验，并提出在项目制学习中融入质量保证的建议。


<details>
  <summary>Details</summary>
Motivation: 提升敏捷/Scrum项目教学中对软件质量的关注与实践效果。

Method: 基于多年教学实践进行经验总结与反思。

Result: 提炼出若干在敏捷项目制学习中有效嵌入质量保证主题的推荐方案。

Conclusion: 将质量保证融入敏捷教学可显著提升学生工程实践能力与产品质量意识。

Abstract: In this paper, we share our lessons learned from more than a decade of teaching software quality aspects within Software Engineering (SE) courses, where the focus is on Agile/Scrum settings: final year software development projects and the course on SE Project Management. Based on the lessons learned, we also provide a number of recommendations on embedding quality assurance topics in the project-based learning with Agile/Scrum context.

</details>


### [49] [Adaptable Teastore with Energy Consumption Awareness: A Case Study](https://arxiv.org/abs/2512.23498)
*Henrique De Medeiros,Denisse Muñante,Sophie Chabridon,César Perdigão Batista,Denis Conan*

Main category: cs.SE

TL;DR: EnCoMSAS工具能有效监测自适应系统运行时能耗，助力动态节能优化，且自身能耗影响较小。


<details>
  <summary>Details</summary>
Motivation: 数据中心能耗激增，需为自适应系统配备运行时能耗监测能力以实现节能目标。

Method: 开发EnCoMSAS工具，在Adaptable TeaStore案例中部署并实测不同负载下推荐服务的能耗表现。

Result: EnCoMSAS可准确采集能耗数据，揭示算法复杂度与部署环境共同影响能耗，且其自身开销较低。

Conclusion: EnCoMSAS为构建能耗感知型自适应系统提供了实用、低开销的监测方案。

Abstract: [Context and Motivation] Global energy consumption has been steadily increasing in recent years, with data centers emerging as major contributors. This growth is largely driven by the widespread migration of applications to the Cloud, alongside a rising number of users consuming digital content. Dynamic adaptation (or self-adaptive) approaches appear as a way to reduce, at runtime and under certain constraints, the energy consumption of software applications.
  [Question/Problem] Despite efforts to make energy-efficiency a primary goal in the dynamic adaptation of software applications, there is still a gap in understanding how to equip these self-adaptive software systems (SAS), which are dynamically adapted at runtime, with effective energy consumption monitoring tools that enable energy-awareness. Furthermore, the extent to which such an energy consumption monitoring tool impacts the overall energy consumption of the SAS ecosystem has not yet been thoroughly explored.
  [Methodology] To address this gap, we introduce the EnCoMSAS (Energy Consumption Monitoring for Self-Adaptive Systems) tool that allows to gather the energy consumed by distributed software applications deployed, for instance, in the Cloud. EnCoMSAS enables the evaluation of energy consumption of SAS variants at runtime. It allows to integrate energy-efficiency as a main goal in the analysis and execution of new adaptation plans for the SAS. In order to evaluate the effectiveness of EnCoMSAS and investigate its impact on the overall energy consumption of the SAS ecosystem, we conduct an empirical study by using the Adaptable TeaStore case study. Adaptable TeaStore is a self-adaptive extension of the TeaStore application, a microservice benchmarking application. For this study, we focus on the recommender service of Adaptable TeaStore. Regarding the experiments, we first equip Adaptable TeaStore with EnCoMSAS. Next, we execute Adaptable TeaStore by varying workload conditions that simulate users interactions. Finally, we use EnCoMSAS for gathering and assessing the energy consumption of the recommender algorithms of Adaptable TeaStore. To run these experiments, we use nodes of the Grid5000 testbed.
  [Results] The results show that EnCoMSAS is effective in collecting energy consumption of software applications for enabling dynamic adaptation at runtime. The observed correlation between CPU usage and energy consumption collected by EnCoMSAS provides evidence supporting the validity of the collected energy measurements. Moreover, we point out, through EnCoMSAS, that energy consumption is influenced not only by the algorithmic complexity but also by the characteristics of the deployment environment. Finally, the results show that the impact of EnCoMSAS on the overall energy consumption of the SAS ecosystem is comparatively modest with respect to the entire set of the TeaStore applications microservices.

</details>


### [50] [Beyond Correctness: Exposing LLM-generated Logical Flaws in Reasoning via Multi-step Automated Theorem Proving](https://arxiv.org/abs/2512.23511)
*Xinyi Zheng,Ningke Li,Xiaokun Luan,Kailong Wang,Ling Shi,Meng Sun,Haoyu Wang*

Main category: cs.SE

TL;DR: MATP是一个通过多步自动定理证明系统验证大语言模型推理逻辑的框架，显著提升推理步骤验证准确率。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在高风险领域应用时存在隐藏逻辑错误，现有方法难以检测复杂多步推理缺陷。

Method: 将自然语言推理转为一阶逻辑，用自动定理证明器逐步验证逻辑有效性并分类推理正确性。

Result: 在10830个实例上超越提示基线42个百分点，揭示不同模型间逻辑连贯性差异。

Conclusion: MATP能有效增强大语言模型推理的可信度，适用于关键应用场景。

Abstract: Large Language Models (LLMs) have demonstrated impressive reasoning capabilities, leading to their adoption in high-stakes domains such as healthcare, law, and scientific research. However, their reasoning often contains subtle logical errors masked by fluent language, posing significant risks for critical applications. While existing approaches like fact-checking, self-consistency methods, and rule-based validation provide partial solutions, they fail to detect complex logical flaws in multi-step reasoning.
  To overcome these challenges, we present MATP, an evaluation framework for systematically verifying LLM reasoning via Multi-step Automatic Theorem Proving. MATP translates natural language reasoning into First-Order Logic (FOL) and applies automated theorem provers to assess step-by-step logical validity. This approach identifies hidden logical errors and provides fine-grained classifications of reasoning correctness. Evaluations on a benchmark comprising 10,830 reasoning instances generated by 10 LLMs across tasks from PrOntoQA-OOD, ProofWriter, and FOLIO show that MATP surpasses prompting-based baselines by over 42 percentage points in reasoning step verification. It further reveals model-level disparities, with reasoning models generating more logically coherent outputs than general models. These results demonstrate MATP's potential to enhance the trustworthiness of LLM-generated reasoning.

</details>


### [51] [Model-based Development for Autonomous Driving Software Considering Parallelization](https://arxiv.org/abs/2512.23575)
*Kenshin Obi,Takumi Onozawa,Hiroshi Fujimoto,Takuya Azumi*

Main category: cs.SE

TL;DR: 本文提出了一种基于模型开发的自动驾驶软件并行化方法，有效缩短执行时间并满足实时性需求。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶软件需处理复杂功能与环境，对实时性要求高，现有方法难以应对复杂处理实现。

Method: 扩展Model-Based Parallelizer（MBP）方法，结合模型驱动开发流程进行并行化设计。

Result: 实验表明该方法显著降低执行时间，适用于自动驾驶软件开发。

Conclusion: 所提方法能有效支持自动驾驶软件的实时性能开发需求。

Abstract: In recent years, autonomous vehicles have attracted attention as one of the solutions to various social problems. However, autonomous driving software requires real-time performance as it considers a variety of functions and complex environments. Therefore, this paper proposes a parallelization method for autonomous driving software using the Model-Based Development (MBD) process. The proposed method extends the existing Model-Based Parallelizer (MBP) method to facilitate the implementation of complex processing. As a result, execution time was reduced. The evaluation results demonstrate that the proposed method is suitable for the development of autonomous driving software, particularly in achieving real-time performance.

</details>


### [52] [Parallelized Code Generation from Simulink Models for Event-driven and Timer-driven ROS 2 Nodes](https://arxiv.org/abs/2512.23605)
*Kenshin Obi,Ryo Yoshinaka,Hiroshi Fujimoto,Takuya Azumi*

Main category: cs.SE

TL;DR: 本文提出一种基于模型驱动开发的框架，用于支持ROS 2多输入模型的并行代码生成，有效缩短执行时间。


<details>
  <summary>Details</summary>
Motivation: 传统手动并行化在复杂嵌入式系统中面临数据一致性和并发问题，而现有MBD方法难以适配ROS 2多输入场景。

Method: 将ROS 2兼容的Simulink模型分为事件驱动与定时驱动两类，实施针对性并行化策略，扩展MBD并行化能力。

Result: 实验表明，应用该框架后所有测试模式的执行时间均减少，验证了并行化的有效性。

Conclusion: 所提框架成功支持ROS 2多输入模型的自动化并行代码生成，提升了嵌入式系统开发效率与性能。

Abstract: In recent years, the complexity and scale of embedded systems, especially in the rapidly developing field of autonomous driving systems, have increased significantly. This has led to the adoption of software and hardware approaches such as Robot Operating System (ROS) 2 and multi-core processors. Traditional manual program parallelization faces challenges, including maintaining data integrity and avoiding concurrency issues such as deadlocks. While model-based development (MBD) automates this process, it encounters difficulties with the integration of modern frameworks such as ROS 2 in multi-input scenarios. This paper proposes an MBD framework to overcome these issues, categorizing ROS 2-compatible Simulink models into event-driven and timer-driven types for targeted parallelization. As a result, it extends the conventional parallelization by MBD and supports parallelized code generation for ROS 2-based models with multiple inputs. The evaluation results show that after applying parallelization with the proposed framework, all patterns show a reduction in execution time, confirming the effectiveness of parallelization.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [53] [A Novel Approach for a Smart IoMT-Based BAN for an Old Home Healthcare Monitoring System Using Starlink](https://arxiv.org/abs/2512.22553)
*Shermin Sultana Setu,Mst. Amena Akter Pinky,Md. Abdul Awal,Sheekar Banerjee,Ishtiak Al Mamoon*

Main category: cs.NI

TL;DR: 本文提出了一种基于Starlink辅助的IoMT老年医疗模型，以提升偏远地区远程监护与通信可靠性。


<details>
  <summary>Details</summary>
Motivation: 当前老年照护体系面临长期护理资源不足与医护沟通不畅的问题，亟需高效可靠的远程医疗方案。

Method: 构建无线监测系统采集心电、体温、跌倒等生理数据，经本地枢纽通过LEO卫星链路传输，结合FQ-CoDel与DSCP实现QoS优先级调度。

Result: NS-3仿真表明，该系统在吞吐量、延迟和可靠性方面优于现有方案，验证了Starlink在远程医疗中的高性能潜力。

Conclusion: 本研究提供了一个可扩展、可复用的卫星辅助医疗框架，有助于提升老年患者照护质量并推动QoS导向的远程医疗发展。

Abstract: The rapid evolution of the Internet of Medical Things (IoMT) technology has become a transformative force in modern healthcare, particularly in elderly patient management. The current elderly care system faces significant challenges, including insufficient long-term care resources and poor communication between healthcare providers. To address this limitation, this study introduces a novel Starlink-assisted IOMT-based elderly healthcare model designed to improve remote patient monitoring and communication reliability. This proposal system focused on a monitoring system of key biomedical parameters such as electrocardiogram (ECG), body temperature, heart attack indicators, and a fall detection alert system. Performance is evaluated using the network simulator (NS-3) to assess its effectiveness in remote and underserved regions. Physiological data collected from patients are transmitted through a local communication hub and forwarded over a Low Earth Orbit (LEO) satellite link to a medical center. Based on Quality of Service (QoS) technology that combines Flow Queuing (FQ) with Controlled Delay (CoDel) with Differentiated Services Code Point (DSCP) marking. This approach prioritizes critical health data for faster transmission while allocating lower priority to non-urgent information. This architecture is entirely wireless, allowing continuous monitoring, real-time alerts, and secure data storage for medical analysis. The simulation results demonstrate that the proposed Starlink-enabled IOMT system outperforms existing solutions in terms of throughput, latency, and reliability. The findings highlight Starlink's potential as a robust and high-performance telehealth communication tool. In addition, this study provides a scalable and reproducible framework for future satellite-assisted healthcare systems that prioritize quality of service (QoS) performance and improved elderly patient care.

</details>


### [54] [Distributed Accountability in Democracy: Using MANETs and DTNs in the Face of Acts of Questionable Legality](https://arxiv.org/abs/2512.23653)
*Mathew Schmidheiser,Milena Radenkovic*

Main category: cs.NI

TL;DR: 本文比较了Epidemic和Wave DTN路由协议在涉及敏感行为通信场景中的表现，分析了各自适用情境并提出未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 探讨在现实场景中，当个体因涉及可疑非法行为而需通信时，如何选择更优的DTN路由协议。

Method: 对比分析Epidemic与Wave两种DTN路由协议在特定情境下的行为表现。

Result: 发现Epidemic协议在某些情境下更具优势，而Wave协议在其他情境下表现更佳。

Conclusion: 不同协议适用于不同情境，需根据实际需求选择，并建议开展进一步研究。

Abstract: In this paper, we explore the behavior of the Epidemic and Wave DTN routing protocols in a realistic setting where individuals may wish to communicate with others for support regarding an act of questionable legality. We identify situations where using the Epidemic routing protocol may be more advantageous in such a scenario, and situations where using the Wave routing protocol may be more advantageous instead. We discuss other aspects of our findings in detail and suggest multiple approaches to future works.

</details>
