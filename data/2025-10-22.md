<div id=toc></div>

# Table of Contents

- [cs.MA](#cs.MA) [Total: 5]
- [cs.SE](#cs.SE) [Total: 31]
- [cs.DC](#cs.DC) [Total: 7]
- [cs.PF](#cs.PF) [Total: 1]
- [cs.AR](#cs.AR) [Total: 1]


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [1] [TACLA: An LLM-Based Multi-Agent Tool for Transactional Analysis Training in Education](https://arxiv.org/abs/2510.17913)
*Monika Zamojska,Jarosław A. Chudziak*

Main category: cs.MA

TL;DR: 本文提出TACLA，一种基于交互分析理论的多智能体架构，通过模拟Parent、Adult和Child三种自我状态，实现具有心理深度和一致人格行为的高保真社会动态模拟。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型在模拟人类复杂社会动态时缺乏心理深度和一致的人格行为，难以满足高保真训练工具的需求。

Method: TACLA架构将每个智能体建模为由Parent、Adult和Child三种自我状态组成的系统，各状态拥有独立的记忆模式，并由一个协调器智能体根据上下文触发因素和生命脚本动态激活相应状态。

Result: 在教育场景验证中，TACLA成功模拟了学生智能体的自我状态转换，能根据教师干预策略展现冲突升级或降级行为，评估显示其对话可信度高。

Conclusion: TACLA能够创建动态且具有心理学基础的社会模拟，为教育等领域开发高效AI工具提供了新路径。

Abstract: Simulating nuanced human social dynamics with Large Language Models (LLMs)
remains a significant challenge, particularly in achieving psychological depth
and consistent persona behavior crucial for high-fidelity training tools. This
paper introduces TACLA (Transactional Analysis Contextual LLM-based Agents), a
novel Multi-Agent architecture designed to overcome these limitations. TACLA
integrates core principles of Transactional Analysis (TA) by modeling agents as
an orchestrated system of distinct Parent, Adult, and Child ego states, each
with its own pattern memory. An Orchestrator Agent prioritizes ego state
activation based on contextual triggers and an agent's life script, ensuring
psychologically authentic responses. Validated in an educational scenario,
TACLA demonstrates realistic ego state shifts in Student Agents, effectively
modeling conflict de-escalation and escalation based on different teacher
intervention strategies. Evaluation shows high conversational credibility and
confirms TACLA's capacity to create dynamic, psychologically-grounded social
simulations, advancing the development of effective AI tools for education and
beyond.

</details>


### [2] [From Agent Simulation to Social Simulator: A Comprehensive Review (Part 1)](https://arxiv.org/abs/2510.18271)
*Xiao Xue,Deyu Zhou,Ming Zhang,Fei-Yue Wang*

Main category: cs.MA

TL;DR: 本文综述了基于智能体建模（ABM）的历史发展及其在社会系统模拟中的经典案例，介绍了其设计原理、基础模型类型及三类典型应用。


<details>
  <summary>Details</summary>
Motivation: 传统物理模拟方法在社会领域面临重大挑战，因此需要引入ABM以更好地理解和模拟复杂社会系统。

Method: 回顾ABM的发展历程与设计原则，介绍个体模型、环境模型和基于规则的模型等基础建模方法，并分析三类经典社会模拟案例。

Result: 系统梳理了ABM在社会模拟中的理论基础与典型应用，展示了其在思想实验、机制探索和平行优化方面的有效性。

Conclusion: ABM为社会系统建模提供了有力工具，能够有效应对传统方法难以处理的复杂性与异质性问题。

Abstract: This is the first part of the comprehensive review, focusing on the
historical development of Agent-Based Modeling (ABM) and its classic cases. It
begins by discussing the development history and design principles of
Agent-Based Modeling (ABM), helping readers understand the significant
challenges that traditional physical simulation methods face in the social
domain. Then, it provides a detailed introduction to foundational models for
simulating social systems, including individual models, environmental models,
and rule-based models. Finally, it presents classic cases of social simulation,
covering three types: thought experiments, mechanism exploration, and parallel
optimization.

</details>


### [3] [Socialized Learning and Emergent Behaviors in Multi-Agent Systems based on Multimodal Large Language Models](https://arxiv.org/abs/2510.18515)
*Sureyya Akin,Shruti T. Tiwari,Ram Bhattacharya,Sagar A. Raman,Kiran Mohanty,Sita Krishnan*

Main category: cs.MA

TL;DR: 本文提出了多模态社会化学习框架（M-S2L），通过结合多模态大语言模型与社会学习机制，使AI智能体在协作环境中展现出类人的社会智能，显著优于仅文本或无社会学习的基线方法。


<details>
  <summary>Details</summary>
Motivation: 为在多智能体系统中实现类人协作智能，需赋予AI智能体多模态感知能力与社会学习机制，以支持具身交互、共享意识与动态协作。

Method: M-S2L框架整合了多模态感知（视觉与文本）、结构化动作能力、强化学习以及两种新型社会学习路径：多模态观察学习和基于反馈的沟通驱动学习，并辅以情景记忆系统以维持长期社会上下文。

Result: 在协作组装环境（CAE）中，M-S2L智能体在任务完成率和完成时间上均显著优于基线模型；消融实验证明多模态和社会学习均不可或缺；智能体展现出高效沟通协议、角色分工及动态重规划能力。

Conclusion: 将多模态感知与显式社会学习相结合，是发展多智能体系统中类人协作智能的关键路径，初步实现了机器社会认知的雏形。

Abstract: This search introduces the Multimodal Socialized Learning Framework (M-S2L),
designed to foster emergent social intelligence in AI agents by integrating
Multimodal Large Language Models (M-LLMs) with social learning mechanisms. The
framework equips agents with multimodal perception (vision and text) and
structured action capabilities, enabling physical manipulation and grounded
multimodal communication (e.g., text with visual pointers). M-S2L combines
direct reinforcement learning with two novel social learning pathways:
multimodal observational learning and communication-driven learning from
feedback, augmented by an episodic memory system for long-term social context.
  We evaluate M-S2L in a Collaborative Assembly Environment (CAE), where agent
teams must construct complex devices from ambiguous blueprints under
informational asymmetry. Across tasks of increasing complexity, M-S2L agents
consistently outperform Text-Only and No-Social-Learning baselines in Task
Completion Rate and Time to Completion, particularly in dynamic problem-solving
scenarios. Ablation studies confirm the necessity of both multimodality and
socialized learning. Our analysis reveals the emergence of efficient
communication protocols integrating visual pointers with concise text,
alongside rapid role specialization leading to stable labor division.
Qualitative case studies demonstrate agents' abilities for shared awareness,
dynamic re-planning, and adaptive problem-solving, suggesting a nascent form of
machine social cognition. These findings indicate that integrating multimodal
perception with explicit social learning is critical for developing human-like
collaborative intelligence in multi-agent systems.

</details>


### [4] [Fetch.ai: An Architecture for Modern Multi-Agent Systems](https://arxiv.org/abs/2510.18699)
*Michael J. Wooldridge,Attila Bagoly,Jonathan J. Ward,Emanuele La Malfa,Gabriel Paludo Licks*

Main category: cs.MA

TL;DR: 该论文提出了 Fetch.ai 架构，通过融合经典多智能体系统（MAS）原则与现代 AI 能力，构建了一个去中心化、安全且可互操作的多智能体平台，以克服当前 LLM 驱动系统在中心化和信任机制等方面的局限。


<details>
  <summary>Details</summary>
Motivation: 当前基于大语言模型（LLM）的智能系统忽视了多智能体系统（MAS）的长期研究成果，导致现有框架存在中心化、缺乏可信通信与交互机制等关键缺陷。

Method: Fetch.ai 架构采用多层设计：底层基于区块链提供可验证身份、发现与交易服务；中间层提供安全、可互操作的智能体开发框架与云部署平台；顶层引入智能编排层，利用面向智能体的 LLM 将人类高层目标转化为复杂的多智能体工作流。

Result: 通过一个去中心化物流用例验证了该系统的可行性，其中自主智能体能够动态发现、协商并安全地进行交易。

Conclusion: Fetch.ai 架构为构建开放、协作且经济可持续的多智能体生态系统提供了系统性解决方案，推动了当前智能体实现范式的演进。

Abstract: Recent surges in LLM-driven intelligent systems largely overlook decades of
foundational multi-agent systems (MAS) research, resulting in frameworks with
critical limitations such as centralization and inadequate trust and
communication protocols. This paper introduces the Fetch.ai architecture, an
industrial-strength platform designed to bridge this gap by facilitating the
integration of classical MAS principles with modern AI capabilities. We present
a novel, multi-layered solution built on a decentralized foundation of on-chain
blockchain services for verifiable identity, discovery, and transactions. This
is complemented by a comprehensive development framework for creating secure,
interoperable agents, a cloud-based platform for deployment, and an intelligent
orchestration layer where an agent-native LLM translates high-level human goals
into complex, multi-agent workflows. We demonstrate the deployed nature of this
system through a decentralized logistics use case where autonomous agents
dynamically discover, negotiate, and transact with one another securely.
Ultimately, the Fetch.ai stack provides a principled architecture for moving
beyond current agent implementations towards open, collaborative, and
economically sustainable multi-agent ecosystems.

</details>


### [5] [Computational Foundations for Strategic Coopetition: Formalizing Interdependence and Complementarity](https://arxiv.org/abs/2510.18802)
*Vik Pant,Eric Yu*

Main category: cs.MA

TL;DR: 该技术报告通过结合i*建模语言与博弈论，提出了一个用于量化分析战略竞合中相互依赖与互补性的计算框架，并在三星-索尼S-LCD合资案例中验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有方法在分析战略竞合时存在不足：i*等概念建模语言缺乏定量分析能力，而经典博弈论则忽略了情境丰富性。因此，亟需一种能融合结构依赖与博弈分析的综合方法。

Method: 将i*中的依赖关系转化为定量相互依赖系数，并基于Brandenburger和Nalebuff的“附加价值”概念形式化互补性；结合结构依赖与议价能力，构建纳入结构相互依赖的纳什均衡博弈模型；通过幂函数与对数函数两种价值函数形式进行实验验证，并应用于三星-索尼S-LCD合资案例。

Result: 实验表明模型在不同函数形式下具有稳健性，其中对数函数在经验拟合上表现更优（验证得分45/60），而幂函数更具理论可处理性。

Conclusion: 该报告为战略竞合的计算建模奠定了基础，支持后续在需求工程与多智能体系统中对信任、团队生产与互惠机制的研究。

Abstract: Modern socio-technical systems are characterized by strategic coopetition
where actors simultaneously cooperate to create value and compete to capture
it. While conceptual modeling languages like i* provide rich qualitative
representations of strategic dependencies, they lack mechanisms for
quantitative analysis of dynamic trade-offs. Conversely, classical game theory
offers mathematical rigor but strips away contextual richness. This technical
report bridges this gap by developing computational foundations that formalize
two critical dimensions of coopetition: interdependence and complementarity. We
ground interdependence in i* structural dependency analysis, translating
depender-dependee-dependum relationships into quantitative interdependence
coefficients through a structured translation framework. We formalize
complementarity following Brandenburger and Nalebuff's Added Value concept,
modeling synergistic value creation with validated parameterization. We
integrate structural dependencies with bargaining power in value appropriation
and introduce a game-theoretic formulation where Nash Equilibrium incorporates
structural interdependence. Validation combines comprehensive experimental
testing across power and logarithmic value function specifications,
demonstrating functional form robustness, with empirical application to the
Samsung-Sony S-LCD joint venture (2004-2011), where logarithmic specifications
achieve superior empirical fit (validation score 45/60) while power functions
provide theoretical tractability. This technical report serves as the
foundational reference for a coordinated research program examining strategic
coopetition in requirements engineering and multi-agent systems, with companion
work addressing trust dynamics, team production, and reciprocity mechanisms.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [6] [AI Exchange Platforms](https://arxiv.org/abs/2510.17839)
*Johannes Schneider,Rene Abraham*

Main category: cs.SE

TL;DR: 本文提出了一种用于分类AI模型交换平台的分类框架，揭示了平台的关键维度、特征及其在公共研究机构与组织间的互动模式，为实践者和学者提供理解和研究AI平台的参考。


<details>
  <summary>Details</summary>
Motivation: 随着基础模型的兴起，组织对结构化AI模型交换平台的需求日益增长，但目前缺乏一个系统性的分类框架来理解这些平台的特性与作用。

Method: 通过构建一个分类法（taxonomy），对AI交换平台的关键维度和特征进行系统性分析，并考察其在公共研究机构与组织之间的互动模式。

Result: 识别出平台在同行评审、在线测试、部署和模型定制等方面的机制，并揭示了不同平台在促进组织效能与适应性方面的差异与共性。

Conclusion: 该分类框架不仅有助于实践者把握AI交换平台带来的挑战与机遇，也为学术界进一步研究AI模型共享的演化、影响及最佳实践奠定了基础。

Abstract: The rapid integration of Artificial Intelligence (AI) into organizational
technology frameworks has transformed how organizations engage with AI-driven
models, influencing both operational performance and strategic innovation. With
the advent of foundation models, the importance of structured platforms for AI
model exchange has become paramount for organizational efficacy and
adaptability. However, a comprehensive framework to categorize and understand
these platforms remains underexplored. To address this gap, our taxonomy
provides a structured approach to categorize AI exchange platforms, examining
key dimensions and characteristics, as well as revealing interesting
interaction patterns between public research institutions and organizations:
Some platforms leverage peer review as a mechanism for quality control, and
provide mechanisms for online testing, deploying, and customization of models.
Our paper is beneficial to practitioners seeking to understand challenges and
opportunities that arise from AI exchange platforms. For academics, the
taxonomy serves as a foundation for further research into the evolution,
impact, and best practices associated with AI model sharing and utilization in
different contexts. Additionally, our study provides insights into the evolving
role of AI in various industries, highlighting the importance of adaptability
and innovation in platform design. This paper serves as a critical resource for
understanding the dynamic interplay between technology, business models, and
user engagement in the rapidly growing domain of AI model exchanges pointing
also towards possible future evolution.

</details>


### [7] [Smart Contracts Formal Verification: A Systematic Literature Review](https://arxiv.org/abs/2510.17865)
*Rene Davila,Everardo Barcenas,Rocio Aldeco-Perez*

Main category: cs.SE

TL;DR: 本文综述了智能合约的形式化验证研究，指出其常见错误，并提出一种基于描述逻辑的替代验证方法。


<details>
  <summary>Details</summary>
Motivation: 智能合约作为软件模型，常在运行或规范中存在显著错误，因此有必要系统研究现有形式化验证方法并探索更有效的替代方案。

Method: 对各类文献中关于智能合约的规范、验证工具及实验进行综述，并在此基础上提出一种基于描述逻辑的形式化验证方法。

Result: 梳理了现有智能合约形式化验证的研究成果，并构建了一种基于描述逻辑的新验证框架。

Conclusion: 基于描述逻辑的形式化验证方法有望更有效地发现和纠正智能合约中的规范与实现错误。

Abstract: Formal verification entails testing software to ensure it operates as
specified. Smart contracts are self-executing contracts with the terms of the
agreement directly written into lines of code. They run on blockchain platforms
and automatically enforce and execute the terms of an agreement when meeting
predefined conditions. However, Smart Contracts, as software models, often
contain notable errors in their operation or specifications. This observation
prompts us to conduct a focused study examining related works published across
various sources. These publications detail specifications, verification tools,
and relevant experiments. Subsequently, this survey proposes an alternative
formal verification based on description logic.

</details>


### [8] [UniCode: A Framework for Generating High Quality Competitive Coding Problems](https://arxiv.org/abs/2510.17868)
*Xinyue Zheng,Haowei Lin,Shaofei Cai,Zilong Zheng,Yitao Liang*

Main category: cs.SE

TL;DR: 本文提出UniCode框架，通过大语言模型自动生成高质量算法题及抗污染测试用例，显著提升编程评测的可扩展性与挑战性。


<details>
  <summary>Details</summary>
Motivation: 现有编程竞赛评测依赖静态人工题目，存在数据污染和可扩展性差的问题。

Method: UniCode利用大语言模型，通过单题扩展、同类型融合和跨类型融合三种策略生成多样化题目，并采用无需标准答案的压力驱动测试用例合成流程，结合暴力验证与共识机制确保测试质量。

Result: 构建了包含492道题目的新基准，评估19个主流大语言模型，最强模型o4-mini仅达70.3%通过率，表明该基准具有高区分度和挑战性。

Conclusion: UniCode为编程领域提供了一种可扩展、可靠且动态的评测数据生成方案。

Abstract: The reliance of competitive coding benchmarks on static, human-authored
problems creates significant challenges, including data contamination and
limited scalability. To address these issues, we introduce UniCode, a novel
framework that automatically generates high-quality algorithmic problems
alongside robust, contamination-resistant test cases. Inspired by biological
evolution that creates better and diverse offspring, our framework leverages
Large Language Models (LLMs) to systematically diversify problems through three
strategies: single problem extension, same-type fusion, and cross-type fusion.
A key innovation is our stress-driven test case synthesis pipeline, which
generates reliable test suites without requiring a canonical ground-truth
solution. This pipeline combines brute-force grounding for small-scale inputs
with a consensus-based validation mechanism for large-scale inputs to ensure
high correctness and coverage. We demonstrate effectiveness of our framework by
curating a benchmark of 492 problems and evaluating 19 state-of-the-art LLMs.
The results reveal that UniCode is highly challenging and discriminative, with
the top-performing model, o4-mini, achieving a pass rate of only 70.3%. Our
framework provides a scalable and reliable solution for generating dynamic
evaluation datasets in coding domain.

</details>


### [9] [Repairing Tool Calls Using Post-tool Execution Reflection and RAG](https://arxiv.org/abs/2510.17874)
*Jason Tsay,Zidane Wright,Gaodan Fang,Kiran Kate,Saurabh Jha,Yara Rizk*

Main category: cs.SE

TL;DR: 本文提出了一种结合大语言模型（LLM）反思与领域特定检索增强生成（RAG）的工具调用后修复机制，专注于修复 Kubernetes 的 kubectl 命令错误。实验表明，该方法在55%的模型中提高了命令执行成功率，并平均提升36%的用户查询正确回答率；相比官方文档，故障排除文档平均提升10%的通过率。


<details>
  <summary>Details</summary>
Motivation: 工具调用常因语法或语义错误失败，其中一些语义错误需在分析工具响应后才能识别和修复，因此需要一种能在工具执行后进行有效错误修复的机制。

Method: 开发了一个结合LLM反思与领域特定RAG的后执行反思组件，利用工具说明文档和故障排除文档对kubectl命令进行修复。

Result: 在55%的评估模型中提升了kubectl命令执行成功率，平均提升36%的用户查询正确回答率；使用故障排除文档比仅用官方文档平均提升10%的通过率。

Conclusion: 结合LLM与领域特定RAG的后执行反思机制能有效修复工具调用中的语义错误，显著提升命令执行成功率和任务完成准确性，尤其在使用故障排除文档时效果更佳。

Abstract: Agentic systems interact with external systems by calling tools such as
Python functions, REST API endpoints, or command line tools such as kubectl in
Kubernetes. These tool calls often fail for various syntactic and semantic
reasons. Some less obvious semantic errors can only be identified and resolved
after analyzing the tool's response. To repair these errors, we develop a
post-tool execution reflection component that combines large language model
(LLM)-based reflection with domain-specific retrieval-augmented generation
(RAG) using documents describing both the specific tool being called and
troubleshooting documents related to the tool. For this paper, we focus on the
use case of the kubectl command line tool to manage Kubernetes, a platform for
orchestrating cluster applications. Through a larger empirical study and a
smaller manual evaluation, we find that our RAG-based reflection will repair
kubectl commands such that they are both more likely to successfully execute
(pass rate) for 55% of our models evaluated and 36% more likely to correctly
answer the user query on average. We find that troubleshooting documents
improve pass rate compared to official documentation by an average of 10%.

</details>


### [10] [TritonRL: Training LLMs to Think and Code Triton Without Cheating](https://arxiv.org/abs/2510.17891)
*Jiin Woo,Shaowei Zhu,Allen Nie,Zhen Jia,Yida Wang,Youngsuk Park*

Main category: cs.SE

TL;DR: 本文提出了TritonRL，一种专用于生成Triton高性能计算内核的领域专用大语言模型，通过结合监督微调与一种新颖的强化学习框架，在KernelBench基准上实现了当前最优的正确性和加速性能。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型的快速发展，自动生成高性能系统内核（如Triton内核）成为加速开发与部署的关键需求。然而，Triton内核生成面临数据稀缺、评估标准不完整以及易受奖励黑客攻击等独特挑战。

Method: TritonRL采用端到端训练框架：首先在精选数据集上进行监督微调以蒸馏Triton领域知识，然后通过强化学习进一步提升代码质量；该RL框架引入可验证的稳健奖励机制和分层奖励分配，能细粒度验证推理轨迹与代码token，有效检测并防止奖励黑客行为。

Result: 在KernelBench上的实验表明，TritonRL在正确性和运行加速方面均达到当前最优水平，优于所有其他专用Triton模型。

Conclusion: 本文提出的基于强化学习的训练范式能有效生成高质量、可替代现有模块的Triton内核，验证了其在领域专用代码生成任务中的强大潜力。

Abstract: With the rapid evolution of large language models (LLMs), the demand for
automated, high-performance system kernels has emerged as a key enabler for
accelerating development and deployment. We introduce TritonRL, a
domain-specialized LLM for Triton kernel generation, trained with a novel
training framework that enables robust and automated kernel synthesis. Unlike
general-purpose programming languages, Triton kernel generation faces unique
challenges due to data scarcity and incomplete evaluation criteria, vulnerable
to reward hacking. Our approach addresses these challenges end-to-end by
distilling Triton-specific knowledge through supervised fine-tuning on curated
datasets, and further improving code quality via reinforcement learning (RL)
with robust, verifiable rewards and hierarchical reward assignment. Our RL
framework robustly detects reward hacking and guides both reasoning traces and
code tokens through fine-grained verification and hierarchical reward
decomposition, enabling the model to generate high-quality Triton kernels that
can truly replace existing modules. With robust and fine-grained evaluation,
our experiments on KernelBench demonstrate that TritonRL achieves
state-of-the-art correctness and speedup, surpassing all other Triton-specific
models and underscoring the effectiveness of our RL-based training paradigm.

</details>


### [11] [A Systematic Literature Review of the Use of GenAI Assistants for Code Comprehension: Implications for Computing Education Research and Practice](https://arxiv.org/abs/2510.17894)
*Yunhan Qiao,Md Istiak Hossain Shihab,Christopher Hundhausen*

Main category: cs.SE

TL;DR: 本文通过系统性文献综述，分析了2022至2024年间31项利用生成式人工智能（GenAI）提升代码理解能力的研究，总结了相关方法、工具、评估方式及其在计算教育中的影响，并指出了未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 随着程序员越来越多地依赖生成式人工智能（GenAI）辅助编写代码，理解和验证这些AI生成代码的能力变得至关重要。同时，GenAI也被用于为程序员提供定制化的代码解释，这对计算教育中的学习者既带来挑战也带来机遇。因此，有必要系统梳理GenAI在促进代码理解方面的应用现状，为教育者提供循证指导。

Method: 采用系统性文献综述（SLR）方法，聚焦2022至2024年间发表的31项相关研究，对基于GenAI的代码理解方法和工具进行分类，分析其研究方法与实证评估结果。

Result: 研究发现，尽管GenAI在代码解释方面具有潜力，但其解释常存在不准确或不清晰的问题，且初学者往往难以编写有效提示，限制了其在代码理解中的应用效果。

Conclusion: 本文为计算教育研究与实践提供了实证依据，强调需改进GenAI工具的解释质量与用户提示能力，并提出了未来研究的关键方向。

Abstract: The ability to comprehend code has long been recognized as an essential skill
in software engineering. As programmers lean more heavily on generative
artificial intelligence (GenAI) assistants to develop code solutions, it is
becoming increasingly important for programmers to comprehend GenAI solutions
so that they can verify their appropriateness and properly integrate them into
existing code. At the same time, GenAI tools are increasingly being enlisted to
provide programmers with tailored explanations of code written both by GenAI
and humans. Thus, in computing education, GenAI presents new challenges and
opportunities for learners who are trying to comprehend computer programs. To
provide computing educators with evidence-based guidance on the use of GenAI to
facilitate code comprehension and to identify directions for future research,
we present a systematic literature review (SLR) of state-of-the-art approaches
and tools that leverage GenAI to enhance code comprehension. Our SLR focuses on
31 studies published between 2022 and 2024. Despite their potential, GenAI
assistants often yield inaccurate or unclear explanations, and novice
programmers frequently struggle to craft effective prompts, thereby impeding
their ability to leverage GenAI to aid code comprehension. Our review
classifies GenAI-based approaches and tools, identifies methods used to study
them, and summarizes the empirical evaluations of their effectiveness. We
consider the implications of our findings for computing education research and
practice, and identify directions for future research.

</details>


### [12] [SpecAgent: A Speculative Retrieval and Forecasting Agent for Code Completion](https://arxiv.org/abs/2510.17925)
*George Ma,Anurag Koul,Qi Chen,Yawen Wu,Sachit Kuhar,Yu Yu,Aritra Sengupta,Varun Kumar,Murali Krishna Ramanathan*

Main category: cs.SE

TL;DR: SpecAgent 是一种在索引阶段主动探索代码库并构建推测性上下文的智能体，旨在提升代码生成质量和降低推理延迟，同时引入无未来信息泄露的合成基准以更真实地评估性能。


<details>
  <summary>Details</summary>
Motivation: 现有检索增强方法在真实软件仓库中受限于推理时的低延迟预算，导致检索质量或用户体验受损；同时现有基准存在未来上下文泄露问题，高估模型性能。

Method: SpecAgent 在索引阶段异步地预探索仓库文件，构建推测性上下文以预测未来编辑；同时构建无泄露的合成基准用于公平评估。

Result: 实验表明，SpecAgent 相比最佳基线在代码生成质量上绝对提升 9–11%（相对提升 48–58%），并显著降低推理延迟。

Conclusion: 通过在索引阶段构建推测性上下文，SpecAgent 有效兼顾了代码生成质量与推理效率，并提出了更可靠的评估基准。

Abstract: Large Language Models (LLMs) excel at code-related tasks but often struggle
in realistic software repositories, where project-specific APIs and cross-file
dependencies are crucial. Retrieval-augmented methods mitigate this by
injecting repository context at inference time. The low inference-time latency
budget affects either retrieval quality or the added latency adversely impacts
user experience. We address this limitation with SpecAgent, an agent that
improves both latency and code-generation quality by proactively exploring
repository files during indexing and constructing speculative context that
anticipates future edits in each file. This indexing-time asynchrony allows
thorough context computation, masking latency, and the speculative nature of
the context improves code-generation quality. Additionally, we identify the
problem of future context leakage in existing benchmarks, which can inflate
reported performance. To address this, we construct a synthetic, leakage-free
benchmark that enables a more realistic evaluation of our agent against
baselines. Experiments show that SpecAgent consistently achieves absolute gains
of 9-11% (48-58% relative) compared to the best-performing baselines, while
significantly reducing inference latency.

</details>


### [13] [From Charts to Code: A Hierarchical Benchmark for Multimodal Models](https://arxiv.org/abs/2510.17932)
*Jiahao Tang,Henry Hengyuan Zhao,Lijian Wu,Yifei Tao,Dongxing Mao,Yang Wan,Jingru Tan,Min Zeng,Min Li,Alex Jinpeng Wang*

Main category: cs.SE

TL;DR: 本文提出了Chart2Code，一个用于评估大语言多模态模型（LMMs）图表理解与代码生成能力的新基准，包含三个难度递增的任务层级，并对25个前沿LMM进行了评测，结果表明现有模型在复杂图表任务上表现仍有限。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏从用户实际使用角度出发、系统性评估LMM图表理解与代码生成能力的基准，尤其在处理复杂图表编辑和长表可视化任务时，现有模型表现不足。

Method: 构建了一个包含三个层级的基准数据集Chart2Code：Level 1（图表复现）、Level 2（图表编辑）和Level 3（长表生成图表），涵盖22种图表类型共2,023个任务，并设计了代码正确性与图表视觉保真度的多层级评估指标；在此基础上评测了25个最新的LMM。

Result: 实验显示，即使是最先进的GPT-5模型，在编辑任务中的代码评估平均得分仅为0.57，图表质量评估仅为0.22，表明Chart2Code具有较高挑战性。

Conclusion: Chart2Code是首个反映实际图表生成场景并系统提升任务复杂度的分层基准，有望推动多模态推理研究，促进更鲁棒、通用LMM的发展。

Abstract: We introduce Chart2Code, a new benchmark for evaluating the chart
understanding and code generation capabilities of large multimodal models
(LMMs). Chart2Code is explicitly designed from a user-driven perspective,
capturing diverse real-world scenarios and progressively increasing task
difficulty. It consists of three levels: Level 1 (Chart Reproduction)
reproduces charts from a reference figure and user query; Level 2 (Chart
Editing) involves complex modifications such as changing chart types or adding
elements; and Level 3 (Long-Table to Chart Generation) requires models to
transform long, information-dense tables into faithful charts following user
instructions. To our knowledge, this is the first hierarchical benchmark that
reflects practical chart2code usage while systematically scaling task
complexity. In total, Chart2Code contains 2,023 tasks across 22 chart types,
paired with multi-level evaluation metrics that assess both code correctness
and the visual fidelity of rendered charts. We benchmark 25 state-of-the-art
(SoTA) LMMs, including both proprietary and the latest open-source models such
as GPT-5, Qwen2.5-VL, InternVL3/3.5, MiMo-VL, and Seed-1.6-VL. Experimental
results demonstrate that even the SoTA model GPT-5 averages only 0.57 on
code-based evaluation and 0.22 on chart-quality assessment across the editing
tasks, underscoring the difficulty of Chart2Code. We anticipate this benchmark
will drive advances in multimodal reasoning and foster the development of more
robust and general-purpose LMMs. Our code and data are available on Chart2Code.

</details>


### [14] [JunoBench: A Benchmark Dataset of Crashes in Python Machine Learning Jupyter Notebooks](https://arxiv.org/abs/2510.18013)
*Yiran Wang,José Antonio Hernández López,Ulf Nilsson,Dániel Varró*

Main category: cs.SE

TL;DR: 本文提出了JunoBench，这是首个面向Python机器学习Jupyter Notebook中真实崩溃场景的基准数据集，包含111个可复现的崩溃案例及其验证修复方案，覆盖主流ML库及Notebook特有的执行顺序问题，并提供统一执行环境以支持可复现性。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏针对Jupyter Notebook中机器学习代码的调试工具，部分原因在于缺少真实崩溃场景的基准数据集。

Method: 构建并发布JunoBench数据集，收集来自Kaggle的111个真实、可复现的崩溃案例，每个案例均配有可验证的修复方案，并提供统一的执行环境以确保复现性。

Result: JunoBench覆盖TensorFlow/Keras、PyTorch、Scikit-learn、Pandas、NumPy等主流库以及Notebook特有的乱序执行问题，为面向Notebook的ML开发中的错误检测、定位与修复提供了现实基础。

Conclusion: JunoBench填补了ML Notebook调试领域基准数据的空白，有助于推动针对交互式、迭代式ML开发环境的调试技术发展。

Abstract: Jupyter notebooks are widely used for machine learning (ML) prototyping. Yet
few debugging tools are designed for ML code in notebooks, potentially due to
the lack of benchmarks. We introduce JunoBench, the first benchmark dataset of
real-world crashes in Python-based ML notebooks. JunoBench has 111 curated and
reproducible crashes from public Kaggle notebooks, each paired with a
verifiable fix, ranging over popular ML libraries, including TensorFlow/Keras,
PyTorch, Scikit-learn, Pandas, and NumPy, as well as notebook-specific
out-of-order execution issue. To support reproducibility and ease of use,
JunoBench offers a unified execution environment where crashes and fixes can be
reliably reproduced. By providing realistic crashes and their resolutions,
JunoBench facilitates bug detection, localization, and repair tailored to the
interactive and iterative nature of notebook-based ML development.

</details>


### [15] [DIP-AI: A Discovery Framework for AI Innovation Projects](https://arxiv.org/abs/2510.18017)
*Mariana Crisostomo Martins,Lucas Elias Cardoso Rocha,Lucas Cordeiro Romao,Taciana Novo Kudo,Marcos Kalinowski,Renato de Freitas Bulcao-Neto*

Main category: cs.SE

TL;DR: 本文提出并评估了一个名为DIP-AI的发现框架，用于支持人工智能（AI）创新项目早期阶段的问题发现，结合了ISO标准与设计思维方法，在产学研合作案例中验证了其有效性和实用性。


<details>
  <summary>Details</summary>
Motivation: 当前人工智能系统的发展对需求工程（RE）提出了新挑战，尤其在AI创新项目中缺乏对问题发现阶段的支持。

Method: 基于文献综述，整合ISO 12207、ISO 5338和设计思维（Design Thinking）元素，构建DIP-AI框架，并通过一个产学研合作的AI创新项目案例进行实证评估。

Result: 评估结果表明，DIP-AI在促进AI项目中的问题发现方面具有相关性和实用性，参与者对其问题发现能力、接受度等方面给予了积极反馈。

Conclusion: DIP-AI为学术界提供了一个用于AI问题发现的框架，同时为工业界在真实产学研项目中应用该框架提供了实践参考。

Abstract: Despite the increasing development of Artificial Intelligence (AI) systems,
Requirements Engineering (RE) activities face challenges in this new
data-intensive paradigm. We identified a lack of support for problem discovery
within AI innovation projects. To address this, we propose and evaluate DIP-AI,
a discovery framework tailored to guide early-stage exploration in such
initiatives. Based on a literature review, our solution proposal combines
elements of ISO 12207, 5338, and Design Thinking to support the discovery of AI
innovation projects, aiming at promoting higher quality deliveries and
stakeholder satisfaction. We evaluated DIP-AI in an industry-academia
collaboration (IAC) case study of an AI innovation project, in which
participants applied DIP-AI to the discovery phase in practice and provided
their perceptions about the approach's problem discovery capability,
acceptance, and suggestions. The results indicate that DIP-AI is relevant and
useful, particularly in facilitating problem discovery in AI projects. This
research contributes to academia by sharing DIP-AI as a framework for AI
problem discovery. For industry, we discuss the use of this framework in a real
IAC program that develops AI innovation projects.

</details>


### [16] [A Benchmark Dataset And LLMs Comparison For NFR Classification With Explainable AI](https://arxiv.org/abs/2510.18096)
*Esrat Ebtida Sakib,MD Ahnaf Akib,Md Muktadir Mazumder,Maliha Noushin Raida,Md. Mohsinul Kabir*

Main category: cs.SE

TL;DR: 本文通过扩充现有数据集并利用多种大语言模型（如Gemma-2、Phi-3等）对非功能性需求（NFRs）进行自动分类，实验表明Gemma-2在多项指标上表现最佳。


<details>
  <summary>Details</summary>
Motivation: 手动从文档中识别非功能性需求（NFRs）耗时且易错，因此需要自动化方法；而构建高质量、全面的数据集是实现自动化的前提。

Method: 从项目章程和开源软件文档中收集NFRs以扩充现有数据集，将其细分为子类，并使用RoBERTa、CodeBERT、Gemma-2、Phi-3、Mistral-8B和Llama-3.1-8B等大语言模型进行分类，通过精确率、召回率、F1分数和lime分数评估模型性能。

Result: Gemma-2表现最优（精确率0.87、召回率0.89、F1分数0.88，lime命中78/80），Phi-3紧随其后（F1分数0.86，lime命中79/80）。

Conclusion: 通过增强数据集的上下文和技术深度，大语言模型对NFRs的理解和分类能力显著提升，其中Gemma-2和Phi-3展现出最佳性能，为NFR自动化识别提供了有效方案。

Abstract: Non-Functional Requirements (NFRs) play a critical role in determining the
overall quality and user satisfaction of software systems. Accurately
identifying and classifying NFRs is essential to ensure that software meets
performance, usability, and reliability expectations. However, manual
identification of NFRs from documentation is time-consuming and prone to
errors, necessitating automated solutions. Before implementing any automated
solution, a robust and comprehensive dataset is essential. To build such a
dataset, we collected NFRs from various Project Charters and Open Source
Software Documentation. This enhanced the technical depth and usability of an
already existing NFR dataset. We categorized NFRs into sub-classes and
identified needs using widely used Large Language Models to facilitate
automation. After classifying the NFRs, we compared the classification results
of the selected LLMs: RoBERTa, CodeBERT, Gemma-2, Phi-3, Mistral-8B, and
Llama-3.1-8B using various evaluation metrics, including precision, recall,
F1-score, and lime scores. Among these models, Gemma-2 achieved the best
results with a precision of 0.87, recall of 0.89, and F1-score of 0.88,
alongside a lime hit score of 78 out of 80. Phi-3 closely followed with a
precision of 0.85, recall of 0.87, F1-score of 0.86, and the highest lime hit
score of 79. By improving the contextual foundation, this integration enhanced
the model's comprehension of technical aspects and user requirements.

</details>


### [17] [When Old Meets New: Evaluating the Impact of Regression Tests on SWE Issue Resolution](https://arxiv.org/abs/2510.18270)
*Yang Chen,Toufique Ahmed,Reyhaneh Jabbarvand,Martin Hirzel*

Main category: cs.SE

TL;DR: TestPrune 是一种自动化技术，通过从问题追踪报告中提取信息，智能地复用并精简回归测试套件，以提升大语言模型（LLM）在自动调试中的问题复现率和修复成功率，同时显著降低上下文长度和推理成本。


<details>
  <summary>Details</summary>
Motivation: 尽管现实项目中的测试套件规模大且覆盖率高，但仍无法捕获所有缺陷；同时，大型测试套件在 LLM 调试中带来上下文限制、噪声和高成本问题，因此需要一种方法自动筛选出高度相关的回归测试子集用于调试。

Method: TestPrune 利用开源项目的问题追踪报告，自动识别并最小化回归测试套件，仅保留与当前问题高度相关的测试用例，从而支持问题复现和补丁验证，并可无缝集成到现有基于智能体的自动修复流程中。

Result: 在 SWE-Bench Lite 和 SWE-Bench Verified 基准上，TestPrune 在 Otter 框架中提升了 6.2%-9.0% 的问题复现率，在 Agentless 框架中提升了 9.4%-12.9% 的问题解决率，且每实例成本仅增加 \$0.02–\$0.05。

Conclusion: TestPrune 能有效提升 LLM 驱动的自动调试系统的性能，同时成本极低，是一种实用且可扩展的回归测试复用策略。

Abstract: Test suites in real-world projects are often large and achieve high code
coverage, yet they remain insufficient for detecting all bugs. The abundance of
unresolved issues in open-source project trackers highlights this gap. While
regression tests are typically designed to ensure past functionality is
preserved in the new version, they can also serve a complementary purpose:
debugging the current version. Specifically, regression tests can (1) enhance
the generation of reproduction tests for newly reported issues, and (2)
validate that patches do not regress existing functionality. We present
TestPrune, a fully automated technique that leverages issue tracker reports and
strategically reuses regression tests for both bug reproduction and patch
validation.
  A key contribution of TestPrune is its ability to automatically minimize the
regression suite to a small, highly relevant subset of tests. Due to the
predominance of LLM-based debugging techniques, this minimization is essential
as large test suites exceed context limits, introduce noise, and inflate
inference costs. TestPrune can be plugged into any agentic bug repair pipeline
and orthogonally improve overall performance. As a proof of concept, we show
that TestPrune leads to a 6.2%-9.0% relative increase in issue reproduction
rate within the Otter framework and a 9.4% - 12.9% relative increase in issue
resolution rate within the Agentless framework on SWE-Bench Lite and SWE-Bench
Verified benchmarks, capturing fixes that were correctly produced by agents but
not submitted as final patches. Compared to the benefits, the cost overhead of
using TestPrune is minimal, i.e., \$0.02 and \$0.05 per SWE-Bench instance,
using GPT-4o and Claude-3.7-Sonnet models, respectively.

</details>


### [18] [Ensuring Robustness in ML-enabled Software Systems: A User Survey](https://arxiv.org/abs/2510.18292)
*Hala Abdelkader,Mohamed Abdelrazek,Priya Rani,Rajesh Vasa,Jean-Guy Schneider*

Main category: cs.SE

TL;DR: 本文提出ML-On-Rails协议，通过集成OOD检测、对抗攻击检测、输入验证和可解释性等机制，提升机器学习系统在生产环境中的鲁棒性与可信度，并通过HTTP状态码增强模型与软件间的透明通信。


<details>
  <summary>Details</summary>
Motivation: 传统软件工程方法难以应对机器学习组件因数据依赖和概率决策带来的静默故障、分布外数据和对抗攻击等鲁棒性挑战。

Method: 设计并实现ML-On-Rails协议，整合多种防护机制，并采用基于HTTP状态码的模型-软件通信框架；同时通过从业者调研验证现实需求与协议价值。

Result: 调研揭示了当前ML系统在鲁棒性方面的关键问题与解决方案缺口，表明标准化协议如ML-On-Rails能有效提升系统可靠性，并为工程师提供必要支持。

Conclusion: ML-On-Rails协议为提升ML系统鲁棒性提供了可行路径，未来需结合实践反馈持续优化该协议。

Abstract: Ensuring robustness in ML-enabled software systems requires addressing
critical challenges, such as silent failures, out-of-distribution (OOD) data,
and adversarial attacks. Traditional software engineering practices, which rely
on predefined logic, are insufficient for ML components that depend on data and
probabilistic decision-making. To address these challenges, we propose the
ML-On-Rails protocol, a unified framework designed to enhance the robustness
and trustworthiness of ML-enabled systems in production. This protocol
integrates key safeguards such as OOD detection, adversarial attack detection,
input validation, and explainability. It also includes a model-to-software
communication framework using HTTP status codes to enhance transparency in
reporting model outcomes and errors. To align our approach with real-world
challenges, we conducted a practitioner survey, which revealed major robustness
issues, gaps in current solutions, and highlighted how a standardised protocol
such as ML-On-Rails can improve system robustness. Our findings highlight the
need for more support and resources for engineers working with ML systems.
Finally, we outline future directions for refining the proposed protocol,
leveraging insights from the survey and real-world applications to continually
enhance its effectiveness.

</details>


### [19] [InspectCoder: Dynamic Analysis-Enabled Self Repair through interactive LLM-Debugger Collaboration](https://arxiv.org/abs/2510.18327)
*Yunkun Wang,Yue Zhang,Guochang Li,Chen Zhi,Binhua Li,Fei Huang,Yongbin Li,Shuiguang Deng*

Main category: cs.SE

TL;DR: InspectCoder 是首个基于智能体的程序修复系统，通过让大语言模型（LLM）主动控制交互式调试器进行动态分析，显著提升了代码修复的准确性和效率。


<details>
  <summary>Details</summary>
Motivation: 现有基于LLM的自修复方法依赖静态语义分析或表面的执行日志，缺乏对运行时行为的深入探查，难以有效定位复杂逻辑错误的根本原因。

Method: 提出 InspectCoder，一个双智能体框架，使LLM能够策略性地设置断点、检查状态并在有状态的调试会话中进行增量式运行时实验；同时开发了开源中间件 InspectWare 以简化调试器集成。

Result: 在 BigCodeBench-R 和 LiveCodeBench-R 两个基准上，InspectCoder 相比最强基线在修复准确率上提升 5.10%–60.37%，修复效率提高 1.67–2.24 倍。

Conclusion: LLM驱动的交互式动态分析能显著提升自动化软件工程中的调试与修复能力，为未来LLM-调试器协同系统提供了可行路径。

Abstract: Large Language Models (LLMs) frequently generate buggy code with complex
logic errors that are challenging to diagnose. While existing LLM-based
self-repair approaches conduct intensive static semantic analysis or reply on
superficial execution logs, they miss the in-depth runtime behaviors that often
expose bug root causes-lacking the interactive dynamic analysis capabilities
that make human debugging effective. We present InspectCoder, the first agentic
program repair system that empowers LLMs to actively conduct dynamic analysis
via interactive debugger control. Our dual-agent framework enables strategic
breakpoint placement, targeted state inspection, and incremental runtime
experimentation within stateful debugger sessions. Unlike existing methods that
follow fixed log collection procedures, InspectCoder adaptively inspects and
perturbs relevant intermediate states at runtime, and leverages immediate
process rewards from debugger feedback to guide multi-step reasoning,
transforming LLM debugging paradigm from blind trial-and-error into systematic
root cause diagnosis. We conduct comprehensive experiments on two challenging
self-repair benchmarks: BigCodeBench-R and LiveCodeBench-R. InspectCoder
achieves 5.10%-60.37% relative improvements in repair accuracy over the
strongest baseline, while delivering 1.67x-2.24x superior bug-fix efficiency
respectively. We also contribute InspectWare, an open-source middleware that
abstracts debugger complexities and maintains stateful debugging sessions
across mainstream Python testing frameworks. Our work provides actionable
insight into the interactive LLM-debugger systems, demonstrating the
significant potential of LLM-driven dynamic analysis for automated software
engineering.

</details>


### [20] [Human to Document, AI to Code: Three Case Studies of Comparing GenAI for Notebook Competitions](https://arxiv.org/abs/2510.18430)
*Tasha Settewong,Youmei Fan,Raula Gaikovina Kula,Kenichi Matsumoto*

Main category: cs.SE

TL;DR: 本文通过三个案例研究比较了人类与生成式AI（GenAI）在计算笔记本中的编码与文档撰写特点，发现人类笔记本在结构多样性、复杂性和创新性方面更优，而GenAI笔记本在代码质量指标上表现更好，并据此提出四项未来研究议程以促进人机协作。


<details>
  <summary>Details</summary>
Motivation: 随着生成式AI（GenAI）技术的兴起，在竞争性环境中区分人类编写与GenAI生成的计算笔记本特征变得愈发重要，尤其是在数据科学实践中。

Method: 研究通过三个案例分析，首先对25个Kaggle金牌获奖人类编写的笔记本的代码与文档特征进行刻画，然后对比人类编写与GenAI生成笔记本在结构、复杂性和代码质量等方面的差异。

Result: 研究发现，金牌人类笔记本以更长、更详细的文档为特征；GenAI笔记本在代码质量（如代码异味、技术债务等指标）上更优，而人类笔记本则展现出更高的结构多样性、复杂性和创新性。

Conclusion: 本研究为未来探索如何在计算笔记本中最大化人类与GenAI协作潜力提供了基础，并提出了四项研究议程。

Abstract: Computational notebooks have become the preferred tool of choice for data
scientists and practitioners to perform analyses and share results. Notebooks
uniquely combine scripts with documentation. With the emergence of generative
AI (GenAI) technologies, it is increasingly important, especially in
competitive settings, to distinguish the characteristics of human-written
versus GenAI.
  In this study, we present three case studies to explore potential strengths
of both humans and GenAI through the coding and documenting activities in
notebooks. We first characterize differences between 25 code and documentation
features in human-written, medal-winning Kaggle notebooks. We find that gold
medalists are primarily distinguished by longer and more detailed
documentation. Second, we analyze the distinctions between human-written and
GenAI notebooks. Our results show that while GenAI notebooks tend to achieve
higher code quality (as measured by metrics like code smells and technical
debt), human-written notebooks display greater structural diversity,
complexity, and innovative approaches to problem-solving. Based on these
results, we envision the work as groundwork that highlight four agendas to
further investigate how GenAI could be utilized in notebooks that maximizes the
potential collaboration between human and AI.

</details>


### [21] [Real-World Usability of Vulnerability Proof-of-Concepts: A Comprehensive Study](https://arxiv.org/abs/2510.18448)
*Wenjing Dang,Kaixuan Li,Sen Chen,Zhenwei Zhuo,Lyuye Zhang,Zheli Liu*

Main category: cs.SE

TL;DR: 本文首次对现实世界中的漏洞概念验证（PoC）进行了大规模研究，涵盖其报告的可用性、完整性与可复现性，发现大多数CVE漏洞缺乏PoC，且现有PoC报告普遍缺失关键组件，进而提出提升PoC可用性的策略。


<details>
  <summary>Details</summary>
Motivation: 当前关于漏洞PoC的研究远落后于漏洞数据本身的研究，主要受限于PoC分散在多个平台、写作风格多样以及复现困难等挑战，亟需系统性分析以提升其在安全实践中的价值。

Method: 1）从13个平台收集470,921个PoC及其报告，评估CVE漏洞的PoC可用性；2）结合模式匹配与微调的BERT-NER模型，提取PoC报告中的9个关键组件以评估完整性；3）招募8名参与者手动复现150个涵盖32种漏洞类型的样本，分析PoC的可复现性及其影响因素。

Result: 研究发现78.9%的CVE漏洞没有可用PoC；现有PoC报告平均缺失约30%的关键组件；复现失败的原因多样，包括信息不全、环境依赖等。

Conclusion: 为提升PoC在软件安全中的实用性，本文为相关方提出了具体可行的改进策略，以增强PoC的完整性、可用性与可复现性。

Abstract: The Proof-of-Concept (PoC) for a vulnerability is crucial in validating its
existence, mitigating false positives, and illustrating the severity of the
security threat it poses. However, research on PoCs significantly lags behind
studies focusing on vulnerability data. This discrepancy can be directly
attributed to several challenges, including the dispersion of real-world PoCs
across multiple platforms, the diversity in writing styles, and the difficulty
associated with PoC reproduction. To fill this gap, we conduct the first
large-scale study on PoCs in the wild, assessing their report availability,
completeness, reproducibility. Specifically, 1) to investigate PoC reports
availability for CVE vulnerability, we collected an extensive dataset of
470,921 PoCs and their reports from 13 platforms, representing the broadest
collection of publicly available PoCs to date. 2) To assess the completeness of
PoC report at a fine-grained level, we proposed a component extraction method,
which combines pattern-matching techniques with a fine-tuned BERT-NER model to
extract 9 key components from PoC reports. 3) To evaluate the effectiveness of
PoCs, we recruited 8 participants to manually reproduce 150 sampled
vulnerabilities with 32 vulnerability types based on PoC reports, enabling an
in-depth analysis of PoC reproducibility and the factors influencing it. Our
findings reveal that 78.9% of CVE vulnerabilities lack available PoCs, and
existing PoC reports typically miss about 30% of the essential components
required for effective vulnerability understanding and reproduction, with
various reasons identified for the failure to reproduce vulnerabilities using
available PoC reports. Finally, we proposed actionable strategies for
stakeholders to enhance the overall usability of vulnerability PoCs in
strengthening software security.

</details>


### [22] [Large Language Models in Thematic Analysis: Prompt Engineering, Evaluation, and Guidelines for Qualitative Software Engineering Research](https://arxiv.org/abs/2510.18456)
*Cristina Martinez Montes,Robert Feldt,Cristina Miguel Martos,Sofia Ouhbi,Shweta Premanandan,Daniel Graziotin*

Main category: cs.SE

TL;DR: 本文提出了一种可复现的方法，将大语言模型（LLMs）整合进Braun和Clarke的反思性主题分析流程中，并通过专家盲评比较LLM与人类研究者生成的编码与主题，发现LLM在多数情况下更具分析价值，但也存在数据碎片化、忽略潜在含义等问题，最终提供了在保持方法严谨性前提下使用LLM辅助质性分析的指南。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型（LLMs）正逐步进入质性研究流程，但缺乏可复现的方法将其整合到如主题分析（TA）等成熟方法中，且现有研究未系统评估LLM生成结果是否符合质性研究的质量标准。

Method: 作者针对Braun和Clarke反思性TA的第2–5阶段设计并迭代优化提示词，利用15份关于软件工程师幸福感的访谈数据，通过四位专家依据Braun和Clarke质量标准进行盲评，比较多个LLM与人类研究者生成的编码和主题。

Result: 专家在61%的情况下更偏好LLM生成的编码，认为其对回答研究问题具有分析价值；但也指出LLM存在不必要地碎片化数据、遗漏潜在解释、主题边界模糊等局限。

Conclusion: 本文贡献包括：（1）一套可复现的LLM整合方法与评估框架；（2）LLM与人类在软件工程质性数据上的实证对比；（3）在保持方法严谨性前提下有效使用LLM辅助质性分析的实践指南。

Abstract: As artificial intelligence advances, large language models (LLMs) are
entering qualitative research workflows, yet no reproducible methods exist for
integrating them into established approaches like thematic analysis (TA), one
of the most common qualitative methods in software engineering research.
Moreover, existing studies lack systematic evaluation of LLM-generated
qualitative outputs against established quality criteria. We designed and
iteratively refined prompts for Phases 2-5 of Braun and Clarke's reflexive TA,
then tested outputs from multiple LLMs against codes and themes produced by
experienced researchers. Using 15 interviews on software engineers' well-being,
we conducted blind evaluations with four expert evaluators who applied rubrics
derived directly from Braun and Clarke's quality criteria. Evaluators preferred
LLM-generated codes 61% of the time, finding them analytically useful for
answering the research question. However, evaluators also identified
limitations: LLMs fragmented data unnecessarily, missed latent interpretations,
and sometimes produced themes with unclear boundaries. Our contributions are
threefold. First, a reproducible approach integrating refined, documented
prompts with an evaluation framework to operationalize Braun and Clarke's
reflexive TA. Second, an empirical comparison of LLM- and human-generated codes
and themes in software engineering data. Third, guidelines for integrating LLMs
into qualitative analysis while preserving methodological rigour, clarifying
when and how LLMs can assist effectively and when human interpretation remains
essential.

</details>


### [23] [CodeRL+: Improving Code Generation via Reinforcement with Execution Semantics Alignment](https://arxiv.org/abs/2510.18471)
*Xue Jiang,Yihong Dong,Mengyang Liu,Hongyi Deng,Tian Wang,Yongding Tao,Rongyu Cao,Binhua Li,Zhi Jin,Wenpin Jiao,Fei Huang,Yongbin Li,Ge Li*

Main category: cs.SE

TL;DR: 本文提出CodeRL+，一种在强化学习训练中融合执行语义对齐的新方法，通过变量级执行轨迹提升代码生成模型的功能正确性，在多个任务和模型上均取得显著性能提升。


<details>
  <summary>Details</summary>
Motivation: 现有基于强化学习与可验证奖励（RLVR）的方法仅依赖测试用例的二元通过/失败信号，难以有效弥合代码文本表示与其执行语义之间的语义鸿沟，尤其对细微逻辑错误效果有限。

Method: CodeRL+在RLVR训练流程中引入执行语义对齐机制，使模型能够推断变量级执行轨迹，从而获得直接的执行语义学习信号；该方法可直接利用现有on-policy rollout，并兼容多种强化学习算法。

Result: 实验表明，CodeRL+在pass@1指标上平均相对提升4.6%，在代码推理和测试输出生成任务上分别提升15.5%和4.4%准确率，且适用于多种LLM和RL算法。

Conclusion: CodeRL+有效增强了代码文本表示与其执行语义之间的对齐，显著提升了代码生成的功能正确性与泛化能力。

Abstract: While Large Language Models (LLMs) excel at code generation by learning from
vast code corpora, a fundamental semantic gap remains between their training on
textual patterns and the goal of functional correctness, which is governed by
formal execution semantics. Reinforcement Learning with Verifiable Rewards
(RLVR) approaches attempt to bridge this gap using outcome rewards from
executing test cases. However, solely relying on binary pass/fail signals is
inefficient for establishing a well-aligned connection between the textual
representation of code and its execution semantics, especially for subtle
logical errors within the code. In this paper, we propose CodeRL+, a novel
approach that integrates execution semantics alignment into the RLVR training
pipeline for code generation. CodeRL+ enables the model to infer variable-level
execution trajectory, providing a direct learning signal of execution
semantics. CodeRL+ can construct execution semantics alignment directly using
existing on-policy rollouts and integrates seamlessly with various RL
algorithms. Extensive experiments demonstrate that CodeRL+ outperforms
post-training baselines (including RLVR and Distillation), achieving a 4.6%
average relative improvement in pass@1. CodeRL+ generalizes effectively to
other coding tasks, yielding 15.5% and 4.4% higher accuracy on code-reasoning
and test-output-generation benchmarks, respectively. CodeRL+ shows strong
applicability across diverse RL algorithms and LLMs. Furthermore, probe
analyses provide compelling evidence that CodeRL+ strengthens the alignment
between code's textual representations and its underlying execution semantics.

</details>


### [24] [VAPU: System for Autonomous Legacy Code Modernization](https://arxiv.org/abs/2510.18509)
*Valtteri Ala-Salmi,Zeeshan Rasheed,Abdul Malik Sami,Muhammad Waseem,Kai-Kristian Kemell,Jussi Rasku,Mika Saari,Pekka Abrahamsson*

Main category: cs.SE

TL;DR: 本文提出了一种基于大语言模型（LLM）的多智能体系统VAPU，用于自动更新遗留Web应用，通过模拟软件开发团队角色分阶段更新代码，在满足需求方面优于Zero-Shot和One-Shot提示方法。


<details>
  <summary>Details</summary>
Motivation: 遗留应用程序常包含过时组件，带来兼容性、安全性和可靠性风险，但高昂的更新成本使企业犹豫不决，因此需要一种低成本、自动化的更新方案。

Method: 设计并实现名为VAPU（Verifying Agent Pipeline Updater）的多智能体系统，利用多个LLM在不同温度参数设置下分阶段更新代码，并在20个开源Python项目上进行评估，与Zero-Shot和One-Shot学习提示进行对比。

Result: 在低温设置下，VAPU的错误数量与ZSL/OSL相当，但满足的需求更高；相比ZSL/OSL提示，VAPU在成功更新Python文件需求方面最多提升22.5%。

Conclusion: 基于LLM的多智能体系统（如VAPU）是一种有效且可行的自动化更新遗留应用组件的解决方案。

Abstract: In this study, we present a solution for the modernization of legacy
applications, an area of code generation where LLM-based multi-agent systems
are proving essential for complex multi-phased tasks. Legacy applications often
contain deprecated components that create compatibility, security, and
reliability risks, but high resource costs make companies hesitate to update.
We take a step forward to integrate an LLM-based multi-agent system as part of
a legacy web application update to provide a cost-effective solution to update
legacy applications autonomously. We propose a multi-agent system named a
Verifying Agent Pipeline Updater (VAPU), which is designed to update code files
in phases while simulating different roles in a software development team. In
our previous study, we evaluated the system for legacy version updates by using
six legacy web application view files by resulting errors and accomplished
requirements. This study extends the previous evaluation of a multi-agent
pipeline system by extending the evaluation of VAPU from a single LLM to five
LLMs and using the temperature parameter in both 0 to 1 settings. Additionally,
we tested the system with 20 open-source Python GitHub projects. The results of
the evaluation were compared to Zero-Shot Learning (ZSL) and One-Shot Learning
(OSL) prompts. The extended evaluation of VAPU showed that particularly in a
low-temperature VAPU can get similar level of error count compared to the
ZSL/OSL prompts but with a higher level of fulfilled requirements, depending on
the LLM. VAPU showed up to 22.5% increase in the succeeding Python file update
requirements compared to ZSL/OSL prompts. The study indicates that an LLM-based
multi-agent system is a capable solution to update components of a legacy
application autonomously.

</details>


### [25] [Mining Service Behavior for Stateful Service Emulation](https://arxiv.org/abs/2510.18519)
*Md Arafat Hossain,Jun Han,Muhammad Ashad Kabir,Steve Versteeg,Jean-Guy Schneider,Jiaojiao Jiang*

Main category: cs.SE

TL;DR: 本文提出一种考虑服务状态的建模方法，通过分析交互消息间的上下文依赖和数据值关系，提升服务虚拟化中响应生成的准确性和效率。


<details>
  <summary>Details</summary>
Motivation: 现有服务虚拟化方法大多忽略服务状态，导致对有状态服务的模拟不够准确，影响测试环境的真实性。

Method: 从服务交互记录中挖掘消息间的上下文依赖关系，并分析消息数据值之间的关联，以构建包含状态信息的服务模型。

Result: 在有状态和无状态服务的交互轨迹上评估表明，该方法在响应生成的准确性和效率方面显著优于现有方法。

Conclusion: 将服务状态纳入建模过程能有效提升服务虚拟化的保真度和测试效果，尤其适用于有状态服务场景。

Abstract: Enterprise software systems are increasingly integrating with diverse
services to meet expanding business demands. Testing these highly
interconnected systems presents a challenge due to the need for access to the
connected services. Service virtualization has emerged as a widely used
technique to derive service models from recorded interactions, for service
response generation during system testing. Various methods have been proposed
to emulate actual service behavior based on these interactions, but most fail
to account for the service's state, which reduces the accuracy of service
emulation and the realism of the testing environment, especially when dealing
with stateful services. This paper proposes an approach to deriving service
models from service interactions, which enhance the accuracy of response
generation by considering service state. This is achieved by uncovering
contextual dependencies among interaction messages and analyzing the
relationships between message data values. The approach is evaluated using
interaction traces collected from both stateful and stateless services, and the
results reveal notable enhancements in accuracy and efficiency over existing
approaches in service response generation.

</details>


### [26] [Demonstrators for Industrial Cyber-Physical System Research: A Requirements Hierarchy Driven by Software-Intensive Design](https://arxiv.org/abs/2510.18534)
*Uraz Odyurt,Richard Loendersloot,Tiedo Tinga*

Main category: cs.SE

TL;DR: 本文提出了一种演示需求细化框架，用于在科研项目中更清晰地定义和评估演示目标的可行性，避免因目标模糊导致的进展受阻。


<details>
  <summary>Details</summary>
Motivation: 科研项目中对演示内容的界定常被忽视，导致目标与实际可实现成果之间存在偏差，影响项目进展；同时，现有TRL量表作为粗略描述工具效果有限。

Method: 作者构建了一个包含五个层级的演示需求细化框架，将演示目标与工作包协作及工业用例明确关联，适用于软件密集型系统和工业信息物理系统领域。

Result: 该框架在两个研究项目中进行了应用验证（一个处于初期阶段，一个处于末期阶段），初步显示出其有效性。

Conclusion: 所提出的框架有助于科研项目在早期更现实地设定和调整演示目标，提升项目执行效率，尽管完整验证需多年周期，但初步应用已证明其潜力。

Abstract: One of the challenges apparent in the organisation of research projects is
the uncertainties around the subject of demonstrators. A precise and detailed
elicitation of the coverage for project demonstrators is often an afterthought
and not sufficiently detailed during proposal writing. This practice leads to
continuous confusion and a mismatch between targeted and achievable
demonstration of results, hindering progress. The reliance on the TRL scale as
a loose descriptor does not help either. We propose a demonstrator requirements
elaboration framework aiming to evaluate the feasibility of targeted
demonstrations, making realistic adjustments, and assist in describing
requirements. In doing so, we define 5 hierarchical levels of demonstration,
clearly connected to expectations, e.g., work package interaction, and also
connected to the project's industrial use-cases. The considered application
scope in this paper is the domain of software-intensive systems and industrial
cyber-physical systems. A complete validation is not accessible, as it would
require application of our framework at the start of a project and observing
the results at the end, taking 4-5 years. Nonetheless, we have applied it to
two research projects from our portfolio, one at the early and another at the
final stages, revealing its effectiveness.

</details>


### [27] [When Abstraction Breaks Physics: Rethinking Modular Design in Quantum Software](https://arxiv.org/abs/2510.18557)
*Jianjun Zhao*

Main category: cs.SE

TL;DR: 本文指出经典软件工程中的抽象机制在量子程序中可能违反量子物理约束，识别了三类失败案例，并提出了面向量子语义的安全抽象设计原则与未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 量子程序具有幺正性、纠缠、不可克隆定理和测量的破坏性等独特语义，使得经典抽象机制在量子软件中可能不安全甚至无效，亟需重新思考适用于量子计算的抽象方法。

Method: 分析经典抽象机制在量子语境下的失效情况，归纳三类典型失败案例，并基于量子语义提出安全抽象的设计原则及研究方向，如量子类型系统、效应标注和基于契约的模块设计。

Result: 识别出经典抽象在量子程序中违反物理约束的具体情形，提出了保障量子语义一致性的抽象设计原则，并指明了若干可行的未来研究路径。

Conclusion: 为实现可扩展且符合量子物理规律的量子软件工程，必须基于量子语义系统性地重构抽象机制，本文为此奠定了初步理论基础并指明了发展方向。

Abstract: Abstraction is a fundamental principle in classical software engineering,
which enables modularity, reusability, and scalability. However, quantum
programs adhere to fundamentally different semantics, such as unitarity,
entanglement, the no-cloning theorem, and the destructive nature of
measurement, which introduce challenges to the safe use of classical
abstraction mechanisms. This paper identifies a fundamental conflict in quantum
software engineering: abstraction practices that are syntactically valid may
violate the physical constraints of quantum computation. We present three
classes of failure cases where naive abstraction breaks quantum semantics and
propose a set of design principles for physically sound abstraction mechanisms.
We further propose research directions, including quantum-specific type
systems, effect annotations, and contract-based module design. Our goal is to
initiate a systematic rethinking of abstraction in quantum software
engineering, based on quantum semantics and considering engineering
scalability.

</details>


### [28] [WebDevJudge: Evaluating (M)LLMs as Critiques for Web Development Quality](https://arxiv.org/abs/2510.18560)
*Chunyang Li,Yilun Zheng,Xinting Huang,Tianqing Fang,Jiahao Xu,Yangqiu Song,Lihui Chen,Han Hu*

Main category: cs.SE

TL;DR: 本文提出了WebDevJudge，一个用于评估大语言模型（LLM）作为评判者在网页开发任务中表现的系统性基准，发现当前LLM评判者与人类专家之间存在显著差距，主要源于模型在功能等价性识别、任务可行性验证和偏见缓解等方面的固有局限。


<details>
  <summary>Details</summary>
Motivation: 现有LLM-as-a-judge范式在定义明确的任务中表现良好，但在动态环境和复杂交互的开放式任务（如网页开发）中的可靠性尚未被充分探索，因此需要一个系统性基准来评估其在该类任务中的表现。

Method: 作者构建了WebDevJudge基准，包含基于静态观察的非交互式评估和基于动态网页环境的连续交互式评估，并提供带结构化、查询锚定评分标准的人类偏好标签。在此基础上，对多种评估器（包括LLM、多模态LLM和智能体工作流）及其不同范式和引导机制进行了系统评估。

Result: 实验揭示了LLM评判者与人类专家之间存在显著性能差距，深入分析表明该差距源于模型在识别功能等价性、验证任务可行性以及缓解偏见方面的根本性局限。

Conclusion: WebDevJudge为LLM-as-a-judge在复杂场景中的应用提出了重大挑战，并为未来开发更可靠、更强大的自动化评估器提供了研究方向和洞见。

Abstract: The paradigm of LLM-as-a-judge is emerging as a scalable and efficient
alternative to human evaluation, demonstrating strong performance on
well-defined tasks. However, its reliability in open-ended tasks with dynamic
environments and complex interactions remains unexplored. To bridge the gap, we
introduce WebDevJudge, a systematic benchmark for assessing LLM-as-a-judge
performance in web development, with support for both non-interactive
evaluation based on static observations and continuous interactive evaluation
with a dynamic web environment. WebDevJudge comprises human preference labels
over paired web implementations, annotated with structured and query-grounded
rubrics to ensure high-quality ground truth. Using this benchmark, we
comprehensively evaluate various evaluators, including LLMs, MLLMs, and agentic
workflows. We systematically investigate the impact of different paradigms and
guidance mechanisms. Our experiments reveal a significant gap between LLM
judges and human experts. In-depth analysis indicates this gap stems from
fundamental model limitations, including failures in recognizing functional
equivalence, verifying task feasibility, and mitigating bias. Overall,
WebDevJudge presents a significant challenge to LLM-as-a-judge, offering
insights to guide future research toward developing more reliable and capable
automated evaluators for complicated scenarios. Code and data are available at
https://github.com/lcy2723/WebDevJudge.

</details>


### [29] [A Structured Evaluation Framework for Low-Code Platform Selection: A Multi-Criteria Decision Model for Enterprise Digital Transformation](https://arxiv.org/abs/2510.18590)
*Antonio Lamanna*

Main category: cs.SE

TL;DR: 本文提出了一种基于五大关键标准的低代码开发平台（LCDP）评估框架，并通过加权评分模型帮助组织根据自身需求进行定量比较，从而提升平台选型决策质量。


<details>
  <summary>Details</summary>
Motivation: 当前低代码平台的选型多依赖营销宣传，缺乏系统化、情境化的评估方法，导致企业面临平台锁定或选择不当的风险，因此亟需一种严谨的评估框架。

Method: 提出一个包含业务流程编排、UI/UX定制、集成与互操作性、治理与安全、AI增强自动化五个维度的加权评分评估模型，并在企业环境中进行实证验证。

Result: 实证表明该框架能有效支持企业在具体业务背景下对低代码平台进行量化评估，显著改善决策结果并降低选型风险。

Conclusion: 该评估框架填补了现有低代码平台比较方法的空白，为组织提供了一种结构化、可定制且实证有效的平台选型工具。

Abstract: The rapid adoption of Low-Code Development Platforms (LCDPs) has created a
critical need for systematic evaluation methodologies that enable organizations
to make informed platform selection decisions. This paper presents a
comprehensive evaluation framework based on five key criteria: Business Process
Orchestration, UI/UX Customization, Integration and Interoperability,
Governance and Security, and AI-Enhanced Automation. We propose a weighted
scoring model that allows organizations to quantitatively assess and compare
different low-code platforms based on their specific requirements and strategic
priorities. The framework addresses the gap between marketing-driven platform
comparisons and rigorous, context-specific evaluation methodologies. Through
empirical validation in enterprise environments, we demonstrate how this
structured approach can significantly improve decision-making outcomes and
reduce the risk of platform lock-in or inadequate solution selection.

</details>


### [30] [CUARewardBench: A Benchmark for Evaluating Reward Models on Computer-using Agent](https://arxiv.org/abs/2510.18596)
*Haojia Lin,Xiaoyu Tan,Yulei Qin,Zihan Xu,Yuchen Shi,Zongyi Li,Gang Li,Shaofei Cai,Siqi Cai,Chaoyou Fu,Ke Li,Xing Sun*

Main category: cs.SE

TL;DR: 本文提出了CUARewardBench，首个面向计算机使用智能体（CUA）的奖励模型评测基准，包含多维度轨迹数据、全面分析及一种名为UPE的新集成方法，显著提升了奖励模型在任务结果和步骤层面的评估可靠性。


<details>
  <summary>Details</summary>
Motivation: 现有基于脚本的CUA评估方法扩展性差且无法进行步骤级评估，而奖励模型在此领域的有效性尚未充分探索，因此亟需一个系统性评测框架。

Method: 构建CUARewardBench基准，涵盖10类软件和7种智能体架构的轨迹数据，并通过专家标注确保质量；在7个视觉语言模型和3种提示模板上进行实验分析；提出Unanimous Prompt Ensemble（UPE）集成方法，利用严格一致投票和提示模板策略提升评估性能。

Result: 实验揭示当前CUA奖励模型存在视觉推理能力不足和知识缺陷等问题；UPE方法在ORM上达到89.8%精确率和93.3%阴性预测值，在PRM上达81.7%精确率和85.1%阴性预测值，显著优于单模型和传统集成方法。

Conclusion: CUARewardBench为CUA奖励模型提供了首个系统性评测平台，UPE方法有效提升了评估可靠性，同时指出通用视觉语言模型在CUA评估中优于专用模型，为未来研究指明方向。

Abstract: Computer-using agents (CUAs) enable task completion through natural
interaction with operating systems and software interfaces. While script-based
verifiers are widely adopted for evaluation, they suffer from limited
scalability and inability to provide step-wise assessment. Reward models offer
promising alternatives, but their effectiveness on CUA evaluation remains
largely underexplored. To address this gap, we present CUARewardBench,
comprising four key contributions: (1) First-ever Comprehensive CUA Reward
Benchmark: We introduce the first benchmark for evaluating both outcome reward
models (ORM) and process reward models (PRM) on CUA tasks, enabling systematic
assessment across trajectory-level and step-level evaluation. (2) Diverse,
Practical and Reliable Dataset: CUARewardBench encompasses trajectories from 10
software categories and 7 agent architectures with varying performance levels
(25.9%-50.8% success rates). All trajectories are expertly annotated through
carefully designed protocols, with rigorous quality control to ensure
reliability and practical applicability. (3) Comprehensive Analysis and
Insights: Through extensive experiments across 7 vision-language models and 3
prompt templates, we reveal critical limitations of current CUA RMs, including
insufficient visual reasoning capabilities, knowledge deficiencies, and the
superiority of general VLMs over specialized CUA models for reward evaluation.
(4) Unanimous Prompt Ensemble (UPE): Based on the insights from our
comprehensive analysis, we propose UPE, a novel ensemble method that
significantly enhances reward model reliability through strict unanimous voting
and strategic prompt-template configurations. UPE achieves 89.8% precision and
93.3% NPV for ORM, and 81.7% precision and 85.1% NPV for PRM, substantially
outperforming single VLMs and traditional ensemble approaches.

</details>


### [31] [An overview of the use of alternative funding and contracting approaches relevant for agile software development: A systematic review of real-life experiences](https://arxiv.org/abs/2510.18711)
*Bertha Ngereja,Magne Jørgensen*

Main category: cs.SE

TL;DR: 本文综述了与敏捷开发原则更契合的替代性资助与合同方法，分析其动因、益处与挑战，并建议组织采用混合方式逐步过渡。


<details>
  <summary>Details</summary>
Motivation: 传统资助与合同方法过于僵化，与敏捷原则冲突，阻碍客户与承包商协作并限制盈利，促使组织寻求更灵活的替代方案。

Method: 通过在SCOPUS、Web of Science和Google Scholar中进行系统性文献综述，筛选出38篇来自公私部门的实证研究。

Result: 识别出四种替代性资助与四种合同方法；其优势包括提升客户满意度、降低承包商风险和更高效利用资源，但也带来范围蔓延、分析瘫痪等挑战。

Conclusion: 组织应根据自身情境（如领导力、人员与系统准备度）选择合适方法，建议从兼顾灵活性与控制的混合模式起步，逐步过渡到完全灵活的定制化方案。

Abstract: Agile software development emphasizes flexibility and iterative processes,
which may conflict with the more linear, rigid, and time-consuming traditional
funding and contracting approaches. This review synthesizes real-life
experiences of using alternative (non-traditional) contracting and funding
approaches. The focus is on identifying approaches that align better with agile
principles and understanding the motivations, benefits, and challenges these
alternatives present. A systematic literature review was conducted in SCOPUS,
Web of Science, and Google Scholar, where we identified 38 relevant
peer-reviewed empirical studies from private and public sector contexts. Four
alternative funding and four alternative contracting approaches were
identified. Organizations were motivated to adopt these alternative approaches
because traditional approaches often proved too rigid, conflicted with agile
principles, hindered effective client-contractor collaboration, and limited
profitability. The benefits of these alternatives included higher client
satisfaction, reduced contractor risk, and more efficient resource utilization.
Adopting alternative funding and contracting approaches may promote flexibility
and efficiency in agile projects but also presents cultural and structural
challenges, increases the risk of scope creep and analysis paralysis, and
requires additional effort in terms of time and resources. The context of the
organization matters highly in selecting a suitable approach, such as the
organizational readiness in terms of its leaders, people, and systems. Thus,
instead of wholly adopting alternative approaches and introducing changes
abruptly, organizations may benefit from starting with hybrid approaches that
balance flexibility and control and progressively transition to fully flexible
approaches tailored to their needs

</details>


### [32] [Causally Perturbed Fairness Testing](https://arxiv.org/abs/2510.18719)
*Chengwen Du,Tao Chen*

Main category: cs.SE

TL;DR: 本文提出了一种名为CausalFT的通用公平性测试框架，通过因果推断识别与敏感特征最具因果关联的非敏感特征，并将其融入扰动生成过程，从而显著提升现有测试样本生成器在发现公平性漏洞方面的效果。


<details>
  <summary>Details</summary>
Motivation: 现有公平性测试方法多聚焦于设计测试样本生成器，忽视了利用数据特征（尤其是因果关系）来指导扰动，限制了其发现公平性漏洞的能力。

Method: CausalFT利用因果推断提取与敏感特征最直接且具有因果关系的非敏感特征，并将这种因果关系注入扰动过程，以指导测试样本生成；该框架可与多种基础生成器结合使用。

Result: 在1296个实验案例中，CausalFT在93%以上的案例中显著提升了基础生成器发现公平性漏洞的能力，且运行开销可控；相比仅基于相关性的现有方法，CausalFT在64%的案例中表现更优且效率更高，同时几乎在所有案例中都增强了模型对偏见的鲁棒性。

Conclusion: CausalFT作为一种高层框架，通过引入因果信息有效提升了公平性测试的性能，具有通用性、高效性和更强的偏见缓解能力。

Abstract: To mitigate unfair and unethical discrimination over sensitive features
(e.g., gender, age, or race), fairness testing plays an integral role in
engineering systems that leverage AI models to handle tabular data. A key
challenge therein is how to effectively reveal fairness bugs under an
intractable sample size using perturbation. Much current work has been focusing
on designing the test sample generators, ignoring the valuable knowledge about
data characteristics that can help guide the perturbation and hence limiting
their full potential. In this paper, we seek to bridge such a gap by proposing
a generic framework of causally perturbed fairness testing, dubbed CausalFT.
Through causal inference, the key idea of CausalFT is to extract the most
directly and causally relevant non-sensitive feature to its sensitive
counterpart, which can jointly influence the prediction of the label. Such a
causal relationship is then seamlessly injected into the perturbation to guide
a test sample generator. Unlike existing generator-level work, CausalFT serves
as a higher-level framework that can be paired with diverse base generators.
Extensive experiments on 1296 cases confirm that CausalFT can considerably
improve arbitrary base generators in revealing fairness bugs over 93% of the
cases with acceptable extra runtime overhead. Compared with a state-of-the-art
approach that ranks the non-sensitive features solely based on correlation,
CausalFT performs significantly better on 64% cases while being much more
efficient. Further, CausalFT can better improve bias resilience in nearly all
cases.

</details>


### [33] [ShaRE your Data! Characterizing Datasets for LLM-based Requirements Engineering](https://arxiv.org/abs/2510.18787)
*Quim Motger,Carlota Catot,Xavier Franch*

Main category: cs.SE

TL;DR: 本文通过系统映射研究，识别并分析了用于大语言模型在需求工程（LLM4RE）任务中的62个公开数据集，提出了一个结构化描述框架和公开目录，以促进数据集的复用与比较，并揭示了当前数据集在需求获取、管理任务及多语言支持等方面的不足。


<details>
  <summary>Details</summary>
Motivation: 需求工程（RE）领域中用于大语言模型（LLM）的数据集存在稀缺、碎片化和缺乏系统描述的问题，限制了其复用性和研究间的可比性。

Method: 开展系统映射研究，识别LLM4RE相关研究中使用的公开数据集，并基于工件类型、粒度、RE阶段、任务、领域和语言等维度对数据集进行结构化描述。

Result: 共识别出62个公开数据集，涵盖43项主要研究；发现当前数据集在需求获取任务、除可追溯性外的管理活动以及多语言支持方面覆盖不足。

Conclusion: 本研究提供了一个公开的数据集目录和结构化描述方案，有助于LLM4RE研究中的数据集选择、比较与复用，并计划未来纳入灰色文献及与开放数据集平台整合。

Abstract: [Context] Large Language Models (LLMs) rely on domain-specific datasets to
achieve robust performance across training and inference stages. However, in
Requirements Engineering (RE), data scarcity remains a persistent limitation
reported in surveys and mapping studies. [Question/Problem] Although there are
multiple datasets supporting LLM-based RE tasks (LLM4RE), they are fragmented
and poorly characterized, limiting reuse and comparability. This research
addresses the limited visibility and characterization of datasets used in
LLM4RE. We investigate which public datasets are employed, how they can be
systematically characterized, and which RE tasks and dataset descriptors remain
under-represented. [Ideas/Results] To address this, we conduct a systematic
mapping study to identify and analyse datasets used in LLM4RE research. A total
of 62 publicly available datasets are referenced across 43 primary studies.
Each dataset is characterized along descriptors such as artifact type,
granularity, RE stage, task, domain, and language. Preliminary findings show
multiple research gaps, including limited coverage for elicitation tasks,
scarce datasets for management activities beyond traceability, and limited
multilingual availability. [Contribution] This research preview offers a public
catalogue and structured characterization scheme to support dataset selection,
comparison, and reuse in LLM4RE research. Future work will extend the scope to
grey literature, as well as integration with open dataset and benchmark
repositories.

</details>


### [34] [FeClustRE: Hierarchical Clustering and Semantic Tagging of App Features from User Reviews](https://arxiv.org/abs/2510.18799)
*Max Tiessler,Quim Motger*

Main category: cs.SE

TL;DR: FeClustRE 是一个结合句法解析与大语言模型（LLM）增强的混合框架，通过自动调优的层次聚类和语义标签生成，将移动应用评论中的嘈杂反馈转化为结构化、可解释的特征分类体系。


<details>
  <summary>Details</summary>
Motivation: 现有方法难以从嘈杂、模糊的移动应用评论中提取具有语义深度且结构清晰的特征，导致需求分析、优先级排序和跨应用比较等任务受限。

Method: 提出 FeClustRE 框架，融合句法解析与 LLM 增强进行特征提取，采用自动调优的层次聚类组织特征，并利用 LLM 自动生成语义标签，形成可解释的特征分类体系。

Result: 在公开基准上验证了特征提取的准确性，并在生成式 AI 助手应用评论样本中评估了聚类质量、语义连贯性与可解释性，结果表明 FeClustRE 能有效生成结构化、有意义的特征表示。

Conclusion: FeClustRE 提供了一个开源、可复现的混合框架，显著提升了从用户评论中理解应用功能与需求的能力，为需求工程任务提供了更深入、结构化的支持。

Abstract: [Context and motivation.] Extracting features from mobile app reviews is
increasingly important for multiple requirements engineering (RE) tasks.
However, existing methods struggle to turn noisy, ambiguous feedback into
interpretable insights. [Question/problem.] Syntactic approaches lack semantic
depth, while large language models (LLMs) often miss fine-grained features or
fail to structure them coherently. In addition, existing methods output flat
lists of features without semantic organization, limiting interpretation and
comparability. Consequently, current feature extraction approaches do not
provide structured, meaningful representations of app features. As a result,
practitioners face fragmented information that hinder requirement analysis,
prioritization, and cross-app comparison, among other use cases. [Principal
ideas/results.] In this context, we propose FeClustRE, a framework integrating
hybrid feature extraction, hierarchical clustering with auto-tuning and
LLM-based semantic labelling. FeClustRE combines syntactic parsing with LLM
enrichment, organizes features into clusters, and automatically generates
meaningful taxonomy labels. We evaluate FeClustRE on public benchmarks for
extraction correctness and on a sample study of generative AI assistant app
reviews for clustering quality, semantic coherence, and interpretability.
[Contribution.] Overall, FeClustRE delivers (1) a hybrid framework for feature
extraction and taxonomy generation, (2) an auto-tuning mechanism with a
comprehensive evaluation methodology, and (3) open-source and replicable
implementation. These contributions bridge user feedback and feature
understanding, enabling deeper insights into current and emerging requirements.

</details>


### [35] [Streamlining Acceptance Test Generation for Mobile Applications Through Large Language Models: An Industrial Case Study](https://arxiv.org/abs/2510.18861)
*Pedro Luís Fonseca,Bruno Lima,João Pascoal Faria*

Main category: cs.SE

TL;DR: AToMIC 是一个基于大语言模型的自动化框架，能从需求文档和代码变更中生成高质量的移动端验收测试工件（如 Gherkin 场景、Page Objects 和 UI 测试脚本），显著减少人工成本，并在 BMW 的 MyBMW 应用中验证了其高效性与实用性。


<details>
  <summary>Details</summary>
Motivation: 移动端验收测试在跨平台开发（如 Flutter）中仍是瓶颈，自动化测试工具虽普及，但测试工件的创建与维护仍需大量人工投入。

Method: 提出 AToMIC 框架，利用专用大语言模型，从 JIRA 需求和近期代码变更自动生成 Gherkin 场景、Page Objects 和可执行 UI 测试脚本。

Result: 在 BMW 的 MyBMW 应用中，AToMIC 在标准硬件上每项功能平均不到 5 分钟生成可执行测试；93.3% 的 Gherkin 场景语法正确，78.8% 的 PageObjects 无需修改即可运行，100% 的 UI 测试成功执行；开发者普遍反馈节省大量时间。

Conclusion: AToMIC 是一种可扩展且实用的工业级解决方案，能有效简化移动端验收测试的创建与维护。

Abstract: Mobile acceptance testing remains a bottleneck in modern software
development, particularly for cross-platform mobile development using
frameworks like Flutter. While developers increasingly rely on automated
testing tools, creating and maintaining acceptance test artifacts still demands
significant manual effort. To help tackle this issue, we introduce AToMIC, an
automated framework leveraging specialized Large Language Models to generate
Gherkin scenarios, Page Objects, and executable UI test scripts directly from
requirements (JIRA tickets) and recent code changes. Applied to BMW's MyBMW
app, covering 13 real-world issues in a 170+ screen codebase, AToMIC produced
executable test artifacts in under five minutes per feature on standard
hardware. The generated artifacts were of high quality: 93.3% of Gherkin
scenarios were syntactically correct upon generation, 78.8% of PageObjects ran
without manual edits, and 100% of generated UI tests executed successfully. In
a survey, all practitioners reported time savings (often a full developer-day
per feature) and strong confidence in adopting the approach. These results
confirm AToMIC as a scalable, practical solution for streamlining acceptance
test creation and maintenance in industrial mobile projects.

</details>


### [36] [EffiReasonTrans: RL-Optimized Reasoning for Code Translation](https://arxiv.org/abs/2510.18863)
*Yanlin Wang,Rongyi Ou,Yanli Wang,Mingwei Liu,Jiachi Chen,Ensheng Shi,Xilin Liu,Yuchi Ma,Zibin Zheng*

Main category: cs.SE

TL;DR: 本文提出EffiReasonTrans，一种兼顾代码翻译准确率与推理延迟的训练框架，通过构建高质量的推理增强数据集并采用两阶段训练策略（监督微调+强化学习），在多个翻译任务上显著提升准确率并降低延迟。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型在代码翻译任务中虽提升了准确率，但推理延迟较高，影响了人机协同开发流程的效率，亟需在准确率与延迟之间取得平衡。

Method: 构建由强模型DeepSeek-R1生成的包含中间推理步骤的高质量三元组数据集（源代码、推理、目标代码），并通过语法与功能检查确保质量；在此基础上采用两阶段训练：先进行推理增强样本的监督微调，再通过强化学习进一步优化准确率与推理延迟的平衡。

Result: 在六个翻译任务上，EffiReasonTrans相比基线模型最高提升49.2%的CA和27.8%的CodeBLEU，同时最多减少19.3%的生成token数和29.0%的推理延迟；消融实验验证了两阶段训练的有效性，并在智能体框架中也表现出更强的翻译能力。

Conclusion: EffiReasonTrans有效解决了代码翻译中准确率与推理延迟之间的权衡问题，为实际开发场景中的人机协同提供了高效可靠的解决方案。

Abstract: Code translation is a crucial task in software development and maintenance.
While recent advancements in large language models (LLMs) have improved
automated code translation accuracy, these gains often come at the cost of
increased inference latency, hindering real-world development workflows that
involve human-in-the-loop inspection. To address this trade-off, we propose
EffiReasonTrans, a training framework designed to improve translation accuracy
while balancing inference latency. We first construct a high-quality
reasoning-augmented dataset by prompting a stronger language model,
DeepSeek-R1, to generate intermediate reasoning and target translations. Each
(source code, reasoning, target code) triplet undergoes automated syntax and
functionality checks to ensure reliability. Based on this dataset, we employ a
two-stage training strategy: supervised fine-tuning on reasoning-augmented
samples, followed by reinforcement learning to further enhance accuracy and
balance inference latency. We evaluate EffiReasonTrans on six translation
pairs. Experimental results show that it consistently improves translation
accuracy (up to +49.2% CA and +27.8% CodeBLEU compared to the base model) while
reducing the number of generated tokens (up to -19.3%) and lowering inference
latency in most cases (up to -29.0%). Ablation studies further confirm the
complementary benefits of the two-stage training framework. Additionally,
EffiReasonTrans demonstrates improved translation accuracy when integrated into
agent-based frameworks. Our code and data are available at
https://github.com/DeepSoftwareAnalytics/EffiReasonTrans.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [37] [Deploying Atmospheric and Oceanic AI Models on Chinese Hardware and Framework: Migration Strategies, Performance Optimization and Analysis](https://arxiv.org/abs/2510.17852)
*Yuze Sun,Wentao Luo,Yanfei Xiang,Jiancheng Pan,Jiahao Li,Quan Zhang,Xiaomeng Huang*

Main category: cs.DC

TL;DR: 本文提出了一种将大规模大气与海洋AI模型从PyTorch迁移至MindSpore并针对国产芯片优化的框架，在保持模型精度的同时显著提升在国产硬件上的运行效率，减少对GPU的依赖。


<details>
  <summary>Details</summary>
Motivation: 当前气候与天气研究中的人工智能模型（如FourCastNet和AI-GOMS）严重依赖GPU，限制了在国产硬件和框架上的部署能力，亟需提升硬件独立性。

Method: 构建一个面向国产芯片的迁移与优化框架，重点包括软硬件适配、内存优化和并行策略，将PyTorch模型迁移至MindSpore平台。

Result: 实验表明，迁移后的模型在训练速度、推理速度、精度和能效等方面表现良好，与GPU实现相当，同时显著降低系统依赖。

Conclusion: 该工作为在国产芯片和框架上开发大气与海洋AI模型提供了实践路径，有助于实现科学计算领域的技术自主。

Abstract: With the growing role of artificial intelligence in climate and weather
research, efficient model training and inference are in high demand. Current
models like FourCastNet and AI-GOMS depend heavily on GPUs, limiting hardware
independence, especially for Chinese domestic hardware and frameworks. To
address this issue, we present a framework for migrating large-scale
atmospheric and oceanic models from PyTorch to MindSpore and optimizing for
Chinese chips, and evaluating their performance against GPUs. The framework
focuses on software-hardware adaptation, memory optimization, and parallelism.
Furthermore, the model's performance is evaluated across multiple metrics,
including training speed, inference speed, model accuracy, and energy
efficiency, with comparisons against GPU-based implementations. Experimental
results demonstrate that the migration and optimization process preserves the
models' original accuracy while significantly reducing system dependencies and
improving operational efficiency by leveraging Chinese chips as a viable
alternative for scientific computing. This work provides valuable insights and
practical guidance for leveraging Chinese domestic chips and frameworks in
atmospheric and oceanic AI model development, offering a pathway toward greater
technological independence.

</details>


### [38] [A Distributed Framework for Causal Modeling of Performance Variability in GPU Traces](https://arxiv.org/abs/2510.18300)
*Ankur Lahiry,Ayush Pokharel,Banooqa Banday,Seth Ockerman,Amal Gueroudji,Mohammad Zaeed,Tanzima Z. Islam,Line Pouchard*

Main category: cs.DC

TL;DR: 本文提出了一种端到端的并行性能分析框架，用于高效处理多个大规模GPU追踪数据，通过并发分区处理、因果图方法和平行协调图，显著提升可扩展性。


<details>
  <summary>Details</summary>
Motivation: 大规模GPU追踪数据在异构高性能计算（HPC）架构中对识别性能瓶颈至关重要，但其庞大的数据量和复杂性使得性能分析计算开销大且耗时。

Method: 提出一个端到端的并行性能分析框架，对追踪数据进行并发分区处理，并结合因果图方法和平行协调图，以揭示执行流之间的性能差异与依赖关系。

Result: 实验结果表明，该框架在可扩展性方面提升了67%，能高效独立地分析多个追踪数据。

Conclusion: 所提出的并行分析框架有效解决了大规模GPU追踪数据处理中的效率问题，显著提升了多追踪分析的可扩展性。

Abstract: Large-scale GPU traces play a critical role in identifying performance
bottlenecks within heterogeneous High-Performance Computing (HPC)
architectures. However, the sheer volume and complexity of a single trace of
data make performance analysis both computationally expensive and
time-consuming. To address this challenge, we present an end-to-end parallel
performance analysis framework designed to handle multiple large-scale GPU
traces efficiently. Our proposed framework partitions and processes trace data
concurrently and employs causal graph methods and parallel coordinating chart
to expose performance variability and dependencies across execution flows.
Experimental results demonstrate a 67% improvement in terms of scalability,
highlighting the effectiveness of our pipeline for analyzing multiple traces
independently.

</details>


### [39] [SLICE: SLO-Driven Scheduling for LLM Inference on Edge Computing Devices](https://arxiv.org/abs/2510.18544)
*Pan Zhou,Yiming Lei,Ling Liu,Xiaoqiong Xu,Ying Cai,Daji Ergu,Hongfang Yu,Yueyue Dai*

Main category: cs.DC

TL;DR: 本文提出SLICE，一种面向边缘计算场景、支持差异化服务等级目标（SLO）的LLM调度方案，通过效用最大化调度算法与动态生成速率控制机制，显著提升SLO达成率和任务完成效率。


<details>
  <summary>Details</summary>
Motivation: 现有LLM调度系统仅以最大化输出吞吐量为目标，无法满足边缘设备上多样化且严苛的SLO需求（如TTFT、TPOT和端到端延迟），导致高SLO违规率。

Method: SLICE结合效用最大化的请求调度算法与生成速率的动态迭代控制机制，以适配边缘场景中不同任务对延迟的差异化要求。

Result: 实验表明，相比Orca和FastServe，SLICE的SLO达成率最高提升35倍，任务完成时间提速达3.4倍。

Conclusion: SLICE有效解决了边缘LLM服务中SLO多样性与实时性需求之间的矛盾，显著提升了服务质量与系统效率。

Abstract: Large Language Models (LLMs), as the foundational architecture for
next-generation interactive AI applications, not only power intelligent
dialogue systems but also drive the evolution of embodied intelligence on edge
devices, including humanoid robots, smart vehicles, and other scenarios. The
applications running on these edge devices impose differentiated Service Level
Objectives (SLO) requirements on LLM services, specifically manifested as
distinct constraints on Time to First Token (TTFT) and Time Per Output Token
(TPOT) as well as end-to-end latency. Notably, edge devices typically handle
real-time tasks that are extremely sensitive to latency, such as machine
control and navigation planning. However, existing scheduling service systems
still prioritize maximizing output token throughput as the sole optimization
objective, failing to adequately address the diversity of SLO requirements.
This ultimately results in persistently high violation rates for end-to-end
latency or TPOT related SLOs.
  This paper proposes SLICE, an innovative scheduling solution designed for
edge computing scenarios with differentiated SLO requirements. By combining a
utility-maximizing request scheduling algorithm with a dynamic iterative
control mechanism for generation rates, SLICE significantly improves LLM
inference service SLO attainment. Experimental results demonstrate that
compared to state-of-the-art solutions Orca and FastServe, SLICE achieves up to
35x higher SLO attainment and 3.4x advantage in task completion time than the
other two solutions.

</details>


### [40] [Tokencake: A KV-Cache-centric Serving Framework for LLM-based Multi-Agent Applications](https://arxiv.org/abs/2510.18586)
*Zhuohang Bian,Feiyang Wu,Teng Ma,Youwei Zhuo*

Main category: cs.DC

TL;DR: Tokencake 是一种面向多智能体应用的 KV 缓存优化框架，通过空间和时间调度机制显著降低延迟并提升 GPU 内存利用率。


<details>
  <summary>Details</summary>
Motivation: 多智能体 LLM 应用中外调函数引发 KV 缓存空间竞争与时间利用率低下，导致关键智能体缓存被驱逐或 GPU 内存闲置。

Method: Tokencake 采用智能体感知设计，通过动态内存分区的空间调度器避免缓存竞争，并利用主动卸载与预测上传的时间调度器在工具调用等待期间重用 GPU 内存。

Result: 在典型多智能体基准测试中，相比 vLLM，Tokencake 降低端到端延迟超过 47.06%，GPU 内存有效利用率提升最高达 16.9%。

Conclusion: Tokencake 有效解决了多智能体 LLM 应用中 KV 缓存管理的性能瓶颈，显著提升了系统整体效率。

Abstract: Large Language Models (LLMs) are increasingly deployed in complex multi-agent
applications that use external function calls. This workload creates severe
performance challenges for the KV Cache: space contention leads to the eviction
of critical agents' caches and time underutilization leaves the cache of agents
stalled on long-running tool calls idling in GPU memory. We present Tokencake,
a KV-Cache-centric serving framework that co-optimizes scheduling and memory
management with an agent-aware design. Tokencake's Space Scheduler uses dynamic
memory partitioning to shield critical agents from contention, while its Time
Scheduler employs a proactive offload and predictive upload mechanism to
repurpose GPU memory during function call stalls. Our evaluation on
representative multi-agent benchmarks shows that Tokencake can reduce
end-to-end latency by over 47.06%, improve effective GPU memory utilization by
up to 16.9% compared to vLLM.

</details>


### [41] [Distributed Interactive Proofs for Planarity with Log-Star Communication](https://arxiv.org/abs/2510.18592)
*Yuval Gil,Merav Parter*

Main category: cs.DC

TL;DR: 本文提出了通信高效的分布式交互证明（DIP）协议，用于判断图的平面性，实现了轮数为 $O(\log^* n)$、证明大小为常数或与 $\log \Delta / \log^* n$ 相关的高效方案，并可推广至任意 $1 \leq r \leq \log^* n$ 的轮数与证明大小权衡。


<details>
  <summary>Details</summary>
Motivation: 分布式交互证明（DIP）旨在通过一个中心化的证明者向分布式验证者证明图的某种性质，关键挑战在于减少通信开销（包括轮数和每轮的证明大小）。平面性是图论中的基本性质，设计通信高效的DIP协议具有理论和实际意义。

Method: 作者设计了针对嵌入平面性和一般平面性的DIP协议，通过控制交互轮数 $r$ 与迭代对数函数 $\log^{(r)} n$ 的关系，在轮数和证明大小之间取得平衡。

Result: 对于嵌入平面性和平面性，分别实现了 $O(\log^* n)$ 轮、证明大小为 $O(1)$ 和 $O(\lceil \log \Delta / \log^* n \rceil)$ 的协议；更一般地，对任意 $1 \leq r \leq \log^* n$，存在 $O(r)$ 轮协议，其证明大小分别为 $O(\log^{(r)} n)$ 和 $O(\log^{(r)} n + \log \Delta / r)$。

Conclusion: 该工作显著改进了平面性判定的分布式交互证明的通信效率，提供了灵活的轮数与证明大小之间的权衡方案，推动了分布式验证系统在图性质验证中的应用。

Abstract: We provide new communication-efficient distributed interactive proofs for
planarity. The notion of a \emph{distributed interactive proof (DIP)} was
introduced by Kol, Oshman, and Saxena (PODC 2018). In a DIP, the \emph{prover}
is a single centralized entity whose goal is to prove a certain claim regarding
an input graph $G$. To do so, the prover communicates with a distributed
\emph{verifier} that operates concurrently on all $n$ nodes of $G$. A DIP is
measured by the amount of prover-verifier communication it requires. Namely,
the goal is to design a DIP with a small number of interaction rounds and a
small \emph{proof size}, i.e., a small amount of communication per round. Our
main result is an $O(\log ^{*}n)$-round DIP protocol for embedded planarity and
planarity with a proof size of $O(1)$ and $O(\lceil\log \Delta/\log
^{*}n\rceil)$, respectively. In fact, this result can be generalized as
follows. For any $1\leq r\leq \log^{*}n$, there exists an $O(r)$-round protocol
for embedded planarity and planarity with a proof size of $O(\log ^{(r)}n)$ and
$O(\log ^{(r)}n+\log \Delta /r)$, respectively.

</details>


### [42] [Towards an Optimized Benchmarking Platform for CI/CD Pipelines](https://arxiv.org/abs/2510.18640)
*Nils Japke,Sebastian Koch,Helmut Lukasczyk,David Bermbach*

Main category: cs.DC

TL;DR: 本文指出当前基准测试优化技术在CI/CD中的集成仍存在关键挑战，提出需解决策略可组合性、结果自动评估及实际可用性三大问题，并构想了一个透明处理这些问题的云原生基准测试框架。


<details>
  <summary>Details</summary>
Motivation: 大规模软件系统中的性能退化会导致资源浪费，频繁基准测试对保障SLA至关重要，但现有基准测试耗时耗资源，难以高效集成到CI/CD流程中。

Method: 作为一篇愿景论文，作者通过分析现有基准测试优化方法的局限性，识别出阻碍其在CI/CD中广泛应用的三大核心挑战，并提出一个概念性的云原生基准测试框架以应对这些挑战。

Result: 识别出基准测试优化在CI/CD中落地的三个关键障碍：优化策略的可组合性、基准结果的自动评估、以及实际部署中的可用性与复杂性。

Conclusion: 为使性能退化检测在CI/CD中更实用有效，需围绕可组合性、自动化评估与易用性开展进一步研究，所提出的云原生框架为未来工作提供方向。

Abstract: Performance regressions in large-scale software systems can lead to
substantial resource inefficiencies, making their early detection critical.
Frequent benchmarking is essential for identifying these regressions and
maintaining service-level agreements (SLAs). Performance benchmarks, however,
are resource-intensive and time-consuming, which is a major challenge for
integration into Continuous Integration / Continuous Deployment (CI/CD)
pipelines. Although numerous benchmark optimization techniques have been
proposed to accelerate benchmark execution, there is currently no practical
system that integrates these optimizations seamlessly into real-world CI/CD
pipelines. In this vision paper, we argue that the field of benchmark
optimization remains under-explored in key areas that hinder its broader
adoption. We identify three central challenges to enabling frequent and
efficient benchmarking: (a) the composability of benchmark optimization
strategies, (b) automated evaluation of benchmarking results, and (c) the
usability and complexity of applying these strategies as part of CI/CD systems
in practice. We also introduce a conceptual cloud-based benchmarking framework
handling these challenges transparently. By presenting these open problems, we
aim to stimulate research toward making performance regression detection in
CI/CD systems more practical and effective.

</details>


### [43] [PCMS: Parallel Coupler For Multimodel Simulations](https://arxiv.org/abs/2510.18838)
*Jacob S. Merson,Cameron W. Smith,Mark S. Shephard,Fuad Hasan,Abhiyan Paudel,Angel Castillo-Crooke,Joyal Mathew,Mohammad Elahi*

Main category: cs.DC

TL;DR: 本文介绍了用于多模型模拟的并行耦合器（PCMS），这是一种新的GPU加速通用耦合框架，支持在超算上耦合模拟代码，并在Frontier超算上展示了良好的弱扩展性。


<details>
  <summary>Details</summary>
Motivation: 为了解决在领导级超级计算机上高效耦合多个物理模拟代码的问题，需要一个支持高维场映射、物理约束并能利用GPU加速的通用耦合框架。

Method: PCMS采用分布式控制和最多五维的场映射方法，利用离散化和场信息满足物理约束，并在GPU上实现加速。

Result: 通过将XGC与DEGAS2耦合以及GNET与GTC的5D分布函数耦合验证了PCMS的有效性，在Frontier超算上使用多达2,080个GPU实现了85%的弱扩展效率。

Conclusion: PCMS为多物理场模拟提供了一个高效、可扩展且支持物理约束的GPU加速耦合框架，适用于领导级超级计算机。

Abstract: This paper presents the Parallel Coupler for Multimodel Simulations (PCMS), a
new GPU accelerated generalized coupling framework for coupling simulation
codes on leadership class supercomputers. PCMS includes distributed control and
field mapping methods for up to five dimensions. For field mapping PCMS can
utilize discretization and field information to accommodate physics
constraints. PCMS is demonstrated with a coupling of the gyrokinetic
microturbulence code XGC with a Monte Carlo neutral transport code DEGAS2 and
with a 5D distribution function coupling of an energetic particle transport
code (GNET) to a gyrokinetic microturbulence code (GTC). Weak scaling is also
demonstrated on up to 2,080 GPUs of Frontier with a weak scaling efficiency of
85%.

</details>


<div id='cs.PF'></div>

# cs.PF [[Back]](#toc)

### [44] [Metrics and evaluations for computational and sustainable AI efficiency](https://arxiv.org/abs/2510.17885)
*Hongyuan Liu,Xinyang Liu,Guosheng Hu*

Main category: cs.PF

TL;DR: 本文提出了一种统一且可复现的AI模型推理评估方法，综合考虑计算性能与环境影响，在多种硬件和软件平台上系统衡量延迟、吞吐量、能耗及碳排放，并在保证精度一致的前提下生成用于可持续AI部署的帕累托前沿。


<details>
  <summary>Details</summary>
Motivation: 当前AI模型在部署后的性能、效率和环境影响评估方法碎片化，缺乏整体视角，难以在异构硬件、软件栈和数值精度之间进行有效比较与优化。

Method: 构建一个统一的评估框架，在真实服务条件下系统测量延迟与吞吐量分布、能耗、地理位置调整后的碳排放，并在匹配精度约束下进行多精度模型在多种硬件（如GH200、RTX 4090）和主流软件栈（如PyTorch、TensorRT、ONNX Runtime）上的测试。

Result: 该方法成功生成了精度、延迟、能耗与碳排放之间的帕累托前沿，提供了可操作的权衡分析，并通过开源代码支持独立验证与广泛应用。

Conclusion: 本研究建立了一个严谨、碳感知的AI推理评估基准框架，有助于推动可持续AI系统的证据驱动型部署决策。

Abstract: The rapid advancement of Artificial Intelligence (AI) has created
unprecedented demands for computational power, yet methods for evaluating the
performance, efficiency, and environmental impact of deployed models remain
fragmented. Current approaches often fail to provide a holistic view, making it
difficult to compare and optimise systems across heterogeneous hardware,
software stacks, and numeric precisions. To address this gap, we propose a
unified and reproducible methodology for AI model inference that integrates
computational and environmental metrics under realistic serving conditions. Our
framework provides a pragmatic, carbon-aware evaluation by systematically
measuring latency and throughput distributions, energy consumption, and
location-adjusted carbon emissions, all while maintaining matched accuracy
constraints for valid comparisons. We apply this methodology to multi-precision
models across diverse hardware platforms, from data-centre accelerators like
the GH200 to consumer-level GPUs such as the RTX 4090, running on mainstream
software stacks including PyTorch, TensorRT, and ONNX Runtime. By
systematically categorising these factors, our work establishes a rigorous
benchmarking framework that produces decision-ready Pareto frontiers,
clarifying the trade-offs between accuracy, latency, energy, and carbon. The
accompanying open-source code enables independent verification and facilitates
adoption, empowering researchers and practitioners to make evidence-based
decisions for sustainable AI deployment.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [45] [From Quarter to All: Accelerating Speculative LLM Decoding via Floating-Point Exponent Remapping and Parameter Sharing](https://arxiv.org/abs/2510.18525)
*Yushu Zhao,Yubin Qin,Yang Wang,Xiaolong Yang,Huiming Han,Shaojun Wei,Yang Hu,Shouyi Yin*

Main category: cs.AR

TL;DR: SPEQ是一种算法与硬件协同设计的推测解码方法，利用全精度模型的部分权重位构建量化草稿模型，在无需额外训练或存储开销的情况下显著提升大语言模型推理速度。


<details>
  <summary>Details</summary>
Motivation: 大语言模型因参数量庞大导致推理延迟高；现有量化方法会降低性能，而传统推测解码虽无损但带来额外开销。

Method: 提出SPEQ方法，通过复用全模型的部分权重位构建量化草稿模型，并设计可重构处理单元阵列高效执行草稿生成与验证过程。

Result: 在15个大语言模型和任务上的实验表明，SPEQ相比FP16、Olive和Tender分别实现2.07倍、1.53倍和1.45倍的加速。

Conclusion: SPEQ在不牺牲模型性能的前提下有效降低推理延迟，且无需额外训练或存储成本，展现出算法-硬件协同设计在大模型加速中的优势。

Abstract: Large language models achieve impressive performance across diverse tasks but
exhibit high inference latency due to their large parameter sizes. While
quantization reduces model size, it often leads to performance degradation
compared to the full model. Speculative decoding remains lossless but typically
incurs extra overheads. We propose SPEQ, an algorithm-hardware co-designed
speculative decoding method that uses part of the full-model weight bits to
form a quantized draft model, thereby eliminating additional training or
storage overhead. A reconfigurable processing element array enables efficient
execution of both the draft and verification passes. Experimental results
across 15 LLMs and tasks demonstrate that SPEQ achieves speedups of 2.07x,
1.53x, and 1.45x compared over FP16, Olive, and Tender, respectively.

</details>
