{"id": "2509.20412", "categories": ["cs.MA", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.20412", "abs": "https://arxiv.org/abs/2509.20412", "authors": ["Kevin Bradley Dsouza", "Graham Alexander Watt", "Yuri Leonenko", "Juan Moreno-Cruz"], "title": "Structuring Collective Action with LLM-Guided Evolution: From Ill-Structured Problems to Executable Heuristics", "comment": null, "summary": "Collective action problems, which require aligning individual incentives with\ncollective goals, are classic examples of Ill-Structured Problems (ISPs). For\nan individual agent, the causal links between local actions and global outcomes\nare unclear, stakeholder objectives often conflict, and no single, clear\nalgorithm can bridge micro-level choices with macro-level welfare. We present\nECHO-MIMIC, a computational framework that converts this global complexity into\na tractable, Well-Structured Problem (WSP) for each agent by discovering\ncompact, executable heuristics and persuasive rationales. The framework\noperates in two stages: ECHO (Evolutionary Crafting of Heuristics from\nOutcomes) evolves snippets of Python code that encode candidate behavioral\npolicies, while MIMIC (Mechanism Inference & Messaging for\nIndividual-to-Collective Alignment) evolves companion natural language messages\nthat motivate agents to adopt those policies. Both phases employ a\nlarge-language-model-driven evolutionary search: the LLM proposes diverse and\ncontext-aware code or text variants, while population-level selection retains\nthose that maximize collective performance in a simulated environment. We\ndemonstrate this framework on a canonical ISP in agricultural landscape\nmanagement, where local farming decisions impact global ecological\nconnectivity. Results show that ECHO-MIMIC discovers high-performing heuristics\ncompared to baselines and crafts tailored messages that successfully align\nsimulated farmer behavior with landscape-level ecological goals. By coupling\nalgorithmic rule discovery with tailored communication, ECHO-MIMIC transforms\nthe cognitive burden of collective action into a simple set of agent-level\ninstructions, making previously ill-structured problems solvable in practice\nand opening a new path toward scalable, adaptive policy design."}
{"id": "2509.20490", "categories": ["cs.MA", "cs.CL", "cs.CV"], "pdf": "https://arxiv.org/pdf/2509.20490", "abs": "https://arxiv.org/abs/2509.20490", "authors": ["Kai Zhang", "Corey D Barrett", "Jangwon Kim", "Lichao Sun", "Tara Taghavi", "Krishnaram Kenthapadi"], "title": "RadAgents: Multimodal Agentic Reasoning for Chest X-ray Interpretation with Radiologist-like Workflows", "comment": "In progress", "summary": "Agentic systems offer a potential path to solve complex clinical tasks\nthrough collaboration among specialized agents, augmented by tool use and\nexternal knowledge bases. Nevertheless, for chest X-ray (CXR) interpretation,\nprevailing methods remain limited: (i) reasoning is frequently neither\nclinically interpretable nor aligned with guidelines, reflecting mere\naggregation of tool outputs; (ii) multimodal evidence is insufficiently fused,\nyielding text-only rationales that are not visually grounded; and (iii) systems\nrarely detect or resolve cross-tool inconsistencies and provide no principled\nverification mechanisms. To bridge the above gaps, we present RadAgents, a\nmulti-agent framework for CXR interpretation that couples clinical priors with\ntask-aware multimodal reasoning. In addition, we integrate grounding and\nmultimodal retrieval-augmentation to verify and resolve context conflicts,\nresulting in outputs that are more reliable, transparent, and consistent with\nclinical practice."}
{"id": "2509.20563", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2509.20563", "abs": "https://arxiv.org/abs/2509.20563", "authors": ["Skyler Ruiter", "Jiannan Tian", "Fengguang Song"], "title": "FZModules: A Heterogeneous Computing Framework for Customizable Scientific Data Compression Pipelines", "comment": null, "summary": "Modern scientific simulations and instruments generate data volumes that\noverwhelm memory and storage, throttling scalability. Lossy compression\nmitigates this by trading controlled error for reduced footprint and throughput\ngains, yet optimal pipelines are highly data and objective specific, demanding\ncompression expertise. GPU compressors supply raw throughput but often\nhard-code fused kernels that hinder rapid experimentation, and underperform in\nrate-distortion. We present FZModules, a heterogeneous framework for assembling\nerror-bounded custom compression pipelines from high-performance modules\nthrough a concise extensible interface. We further utilize an asynchronous\ntask-backed execution library that infers data dependencies, manages memory\nmovement, and exposes branch and stage level concurrency for powerful\nasynchronous compression pipelines. Evaluating three pipelines built with\nFZModules on four representative scientific datasets, we show they can compare\nend-to-end speedup of fused-kernel GPU compressors while achieving similar\nrate-distortion to higher fidelity CPU or hybrid compressors, enabling rapid,\ndomain-tailored design."}
{"id": "2509.20380", "categories": ["cs.SE", "cs.AI", "cs.PL"], "pdf": "https://arxiv.org/pdf/2509.20380", "abs": "https://arxiv.org/abs/2509.20380", "authors": ["Samyak Jhaveri", "Vanessa Klotzmann", "Crista Lopes"], "title": "ACCeLLiuM: Supervised Fine-Tuning for Automated OpenACC Pragma Generation", "comment": null, "summary": "The increasing ubiquity of GPUs is accompanied by the increasing complexity\nof their hardware and parallel programming frameworks. Directive-based parallel\nprogramming standards like OpenACC simplify GPU programming to some extent by\nabstracting away low-level complexities, but a fair amount of expertise is\nstill required in order to use those directives effectively.\n  We introduce ACCeLLiuM, two open weights Large Language Models specifically\nfine-tuned for generating expert OpenACC directives for data-parallel loops,\nalong with the supervised fine-tuning dataset that was used to train them. The\nACCeLLiuM SFT dataset contains 4,033 OpenACC pragma-loop pairs mined from\npublic GitHub C/C++ repositories, with 3,223 pairs for training and 810 for\ntesting. Experimental evaluations show a pronounced performance gap in\ngenerating correct OpenACC pragmas between base LLMs and our fine-tuned\nversions. On the held-out test set, base LLMs fail to consistently generate\nvalid pragmas, whereas LLMs fine-tuned on the ACCeLLiuM dataset generate valid\npragmas with the correct directive type for $87\\%$ of the data-parallel loops,\nand exact pragmas--including directives, clauses, clause order, and clause\nvariables--for $50\\%$ of the cases. Even when not exact, generated pragmas\nfrequently incorporate the correct clauses in a different order than the\nground-truth label, or include additional clauses that enable finer control\nover parallel execution, data movement, and concurrency, offering practical\nvalue beyond strict string-matching. By publicly releasing the code, models,\nand dataset as ACCeLLiuM we hope to establish a reproducible benchmark for\nLLM-powered OpenACC pragma generation, and lower the barrier to automated GPU\noffloading of serially written programs."}
{"id": "2509.20603", "categories": ["cs.DC", "cs.AI", "cs.AR", "cs.ET", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.20603", "abs": "https://arxiv.org/abs/2509.20603", "authors": ["Angel M. Beltre", "Jeff Ogden", "Kevin Pedretti"], "title": "Experience Deploying Containerized GenAI Services at an HPC Center", "comment": "10 pages, 12 figures", "summary": "Generative Artificial Intelligence (GenAI) applications are built from\nspecialized components -- inference servers, object storage, vector and graph\ndatabases, and user interfaces -- interconnected via web-based APIs. While\nthese components are often containerized and deployed in cloud environments,\nsuch capabilities are still emerging at High-Performance Computing (HPC)\ncenters. In this paper, we share our experience deploying GenAI workloads\nwithin an established HPC center, discussing the integration of HPC and cloud\ncomputing environments. We describe our converged computing architecture that\nintegrates HPC and Kubernetes platforms running containerized GenAI workloads,\nhelping with reproducibility. A case study illustrates the deployment of the\nLlama Large Language Model (LLM) using a containerized inference server (vLLM)\nacross both Kubernetes and HPC platforms using multiple container runtimes. Our\nexperience highlights practical considerations and opportunities for the HPC\ncontainer community, guiding future research and tool development."}
{"id": "2509.20385", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.20385", "abs": "https://arxiv.org/abs/2509.20385", "authors": ["Ishara Devendra", "Chaman Wijesiriwardana", "Prasad Wimalaratne"], "title": "State-of-the-Art in Software Security Visualization: A Systematic Review", "comment": null, "summary": "Software security visualization is an interdisciplinary field that combines\nthe technical complexity of cybersecurity, including threat intelligence and\ncompliance monitoring, with visual analytics, transforming complex security\ndata into easily digestible visual formats. As software systems get more\ncomplex and the threat landscape evolves, traditional text-based and numerical\nmethods for analyzing and interpreting security concerns become increasingly\nineffective. The purpose of this paper is to systematically review existing\nresearch and create a comprehensive taxonomy of software security visualization\ntechniques through literature, categorizing these techniques into four types:\ngraph-based, notation-based, matrix-based, and metaphor-based visualization.\nThis systematic review explores over 60 recent key research papers in software\nsecurity visualization, highlighting its key issues, recent advancements, and\nprospective future research directions. From the comprehensive analysis, the\ntwo main areas were distinctly highlighted as extensive software development\nvisualization, focusing on advanced methods for depicting software\narchitecture: operational security visualization and cybersecurity\nvisualization. The findings highlight the necessity for innovative\nvisualization techniques that adapt to the evolving security landscape, with\npractical implications for enhancing threat detection, improving security\nresponse strategies, and guiding future research."}
{"id": "2509.20776", "categories": ["cs.DC", "cs.MS", "G.4"], "pdf": "https://arxiv.org/pdf/2509.20776", "abs": "https://arxiv.org/abs/2509.20776", "authors": ["Elaheh Hassani", "Md Taufique Hussain", "Ariful Azad"], "title": "Distributed-memory Algorithms for Sparse Matrix Permutation, Extraction, and Assignment", "comment": "32 pages", "summary": "We present scalable distributed-memory algorithms for sparse matrix\npermutation, extraction, and assignment. Our methods follow an\nIdentify-Exchange-Build (IEB) strategy where each process identifies the local\nnonzeros to be sent, exchanges the required data, and then builds its local\nsubmatrix from the received elements. This approach reduces communication\ncompared to SpGEMM-based methods in distributed memory. By employing\nsynchronization-free multithreaded algorithms, we further accelerate local\ncomputations, achieving substantially better performance than existing\nlibraries such as CombBLAS and PETSc. We design efficient software for these\noperations and evaluate their performance on two university clusters and the\nPerlmutter supercomputer. Our experiments span a variety of application\nscenarios, including matrix permutation for load balancing, matrix reordering,\nsubgraph extraction, and streaming graph applications. In all cases, we compare\nour algorithms against CombBLAS, the most comprehensive distributed library for\nthese operations, and, in some scenarios, against PETSc. Overall, this work\nprovides a comprehensive study of algorithms, software implementations,\nexperimental evaluations, and applications for sparse matrix permutation,\nextraction, and assignment."}
{"id": "2509.20386", "categories": ["cs.SE", "cs.AI", "cs.IR"], "pdf": "https://arxiv.org/pdf/2509.20386", "abs": "https://arxiv.org/abs/2509.20386", "authors": ["Nishant Gaurav", "Adit Akarsh", "Ankit Ranjan", "Manoj Bajaj"], "title": "Dynamic ReAct: Scalable Tool Selection for Large-Scale MCP Environments", "comment": null, "summary": "We present Dynamic ReAct, a novel approach for enabling ReAct agents to ef-\nficiently operate with extensive Model Control Protocol (MCP) tool sets that\nexceed the contextual memory limitations of large language models. Our approach\naddresses the fundamental challenge of tool selection in environments\ncontaining hundreds or thousands of available tools, where loading all tools\nsimultaneously is computationally infeasible. We propose and evaluate five\ndistinct architectures that progressively refine the tool selection process,\nculminating in a search-and-load mechanism that achieves intelligent tool\nselection with minimal computational overhead. Our experimental results\ndemonstrate that the proposed approach reduces tool loading by up to 50% while\nmaintaining task completion accuracy, advancing the path towards truly\ngeneral-purpose AI agents capable of dynamically adapting to diverse task\nenvironments."}
{"id": "2509.20819", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2509.20819", "abs": "https://arxiv.org/abs/2509.20819", "authors": ["Andre Merzky", "Mikhail Titov", "Matteo Turilli", "Shantenu Jha"], "title": "Integrating and Characterizing HPC Task Runtime Systems for hybrid AI-HPC workloads", "comment": "12 pages, 1 table, 8 figures", "summary": "Scientific workflows increasingly involve both HPC and machine-learning\ntasks, combining MPI-based simulations, training, and inference in a single\nexecution. Launchers such as Slurm's srun constrain concurrency and throughput,\nmaking them unsuitable for dynamic and heterogeneous workloads. We present a\nperformance study of RADICAL-Pilot (RP) integrated with Flux and Dragon, two\ncomplementary runtime systems that enable hierarchical resource management and\nhigh-throughput function execution. Using synthetic and production-scale\nworkloads on Frontier, we characterize the task execution properties of RP\nacross runtime configurations. RP+Flux sustains up to 930 tasks/s, and\nRP+Flux+Dragon exceeds 1,500 tasks/s with over 99.6% utilization. In contrast,\nsrun peaks at 152 tasks/s and degrades with scale, with utilization below 50%.\nFor IMPECCABLE.v2 drug discovery campaign, RP+Flux reduces makespan by 30-60%\nrelative to srun/Slurm and increases throughput more than four times on up to\n1,024. These results demonstrate hybrid runtime integration in RP as a scalable\napproach for hybrid AI-HPC workloads."}
{"id": "2509.20387", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.20387", "abs": "https://arxiv.org/abs/2509.20387", "authors": ["Qusai Ramadan", "Jukka Ruohonen", "Abhishek Tiwari", "Adam Alami", "Zeyd Boukhers"], "title": "Towards Systematic Specification and Verification of Fairness Requirements: A Position Paper", "comment": "Accepted at the 2025 IEEE 33rd International Requirements Engineering\n  Conference Workshops", "summary": "Decisions suggested by improperly designed software systems might be prone to\ndiscriminate against people based on protected characteristics, such as gender\nand ethnicity. Previous studies attribute such undesired behavior to flaws in\nalgorithmic design or biased data. However, these studies ignore that\ndiscrimination is often the result of a lack of well-specified fairness\nrequirements and their verification. The fact that experts' knowledge about\nfairness is often implicit makes the task of specifying precise and verifiable\nfairness requirements difficult. In related domains, such as security\nengineering, knowledge graphs have been proven to be effective in formalizing\nknowledge to assist requirements specification and verification. To address the\nlack of formal mechanisms for specifying and verifying fairness requirements,\nwe propose the development of a knowledge graph-based framework for fairness.\nIn this paper, we discuss the challenges, research questions, and a road map\ntowards addressing the research questions."}
{"id": "2509.21009", "categories": ["cs.DC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.21009", "abs": "https://arxiv.org/abs/2509.21009", "authors": ["Wei Gao", "Yuheng Zhao", "Dakai An", "Tianyuan Wu", "Lunxi Cao", "Shaopan Xiong", "Ju Huang", "Weixun Wang", "Siran Yang", "Wenbo Su", "Jiamang Wang", "Lin Qu", "Bo Zheng", "Wei Wang"], "title": "RollPacker: Mitigating Long-Tail Rollouts for Fast, Synchronous RL Post-Training", "comment": "16pages,14 figures", "summary": "Reinforcement Learning (RL) is a pivotal post-training technique for\nenhancing the reasoning capabilities of Large Language Models (LLMs). However,\nsynchronous RL post-training often suffers from significant GPU\nunderutilization, referred to as bubbles, caused by imbalanced response lengths\nwithin rollout steps. Many RL systems attempt to alleviate this problem by\nrelaxing synchronization, but this can compromise training accuracy. In this\npaper, we introduce tail batching, a novel rollout scheduling strategy for\nsynchronous RL that systematically consolidates prompts leading to long-tail\nresponses into a small subset of rollout steps (long rounds), while ensuring\nthat the majority of steps (short rounds) involve only balanced, short\nrollouts. By excluding long responses from short rounds and rescheduling them\ninto a few designated long rounds, tail batching effectively reduces GPU idle\ntime during rollouts and significantly accelerates RL training without\nsacrificing accuracy. We present RollPacker, a system that fully harnesses the\nbenefits of tail batching through holistic optimizations across all three RL\nstages: elastic parallelism adaptation for rollout, dynamic resource allocation\nand scheduling for reward, and stream-based training. Empirical results show\nthat RollPacker achieves a 2.03x-2.56x end-to-end training time reduction\ncompared to veRL and up to 2.24x speedup compared to RLHFuse for the Qwen2.5\nfamily of LLMs on up to 128 H800 GPUs."}
{"id": "2509.20415", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.20415", "abs": "https://arxiv.org/abs/2509.20415", "authors": ["Yu Pan", "Xiaocheng Li", "Hanzhao Wang"], "title": "Online-Optimized RAG for Tool Use and Function Calling", "comment": null, "summary": "In many applications, retrieval-augmented generation (RAG) drives tool use\nand function calling by embedding the (user) queries and matching them to\npre-specified tool/function descriptions. In this paper, we address an\nembedding misalignment issue that often arises in practical applications due to\nimperfect embedding models or noisy descriptions; such misalignment may lead to\nincorrect retrieval and task failure. We introduce Online-Optimized RAG, a\ndeployment-time framework that continually adapts retrieval embeddings from\nlive interactions using minimal feedback (e.g., task success). Online-Optimized\nRAG applies lightweight online gradient updates with negligible per-query\nlatency and requires no changes to the underlying LLM. The method is\nplug-and-play: it supports both single- and multi-hop tool use, dynamic tool\ninventories, and $K$-retrieval with re-ranking. We provide a problem-dependent\ntheoretical analysis that quantifies how the method's performance depends on\nthe initialization quality of the embeddings and other related quantities.\nAcross diverse tool-use and document-retrieval scenarios, our Online-Optimized\nRAG consistently improves tool selection accuracy and end-task success, thus\nproviding a simple, practical path to robust, self-improving RAG systems."}
{"id": "2509.21037", "categories": ["cs.DC", "cs.MS", "D.1.3; G.1.3; G.4"], "pdf": "https://arxiv.org/pdf/2509.21037", "abs": "https://arxiv.org/abs/2509.21037", "authors": ["Jakub Homola", "Ondřej Meca", "Lubomír Říha", "Tomáš Brzobohatý"], "title": "Utilizing Sparsity in the GPU-accelerated Assembly of Schur Complement Matrices in Domain Decomposition Methods", "comment": "12 pages (originally 10 pages without references), 10 figures,\n  submitted to SC25 conference", "summary": "Schur complement matrices emerge in many domain decomposition methods that\ncan solve complex engineering problems using supercomputers. Today, as most of\nthe high-performance clusters' performance lies in GPUs, these methods should\nalso be accelerated.\n  Typically, the offloaded components are the explicitly assembled dense Schur\ncomplement matrices used later in the iterative solver for multiplication with\na vector. As the explicit assembly is expensive, it represents a significant\noverhead associated with this approach to acceleration. It has already been\nshown that the overhead can be minimized by assembling the Schur complements\ndirectly on the GPU.\n  This paper shows that the GPU assembly can be further improved by wisely\nutilizing the sparsity of the input matrices. In the context of FETI methods,\nwe achieved a speedup of 5.1 in the GPU section of the code and 3.3 for the\nwhole assembly, making the acceleration beneficial from as few as 10\niterations."}
{"id": "2509.20421", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.20421", "abs": "https://arxiv.org/abs/2509.20421", "authors": ["Reiner Hähnle", "Cosimo Laneve", "Adele Veschetti"], "title": "Formal Verification of Legal Contracts: A Translation-based Approach", "comment": null, "summary": "Stipula is a domain-specific programming language designed to model legal\ncontracts with enforceable properties, especially those involving asset\ntransfers and obligations. This paper presents a methodology to formally verify\nthe correctness of Stipula contracts through translation into Java code\nannotated with Java Modeling Language specifications. As a verification\nbackend, the deductive verification tool KeY is used. Both, the translation and\nthe verification of partial and total correctness for a large subset of Stipula\ncontracts, those with disjoint cycles, is fully automatic. Our work\ndemonstrates that a general-purpose deductive verification tool can be used\nsuccessfully in a translation approach."}
{"id": "2509.21039", "categories": ["cs.DC", "cs.CE", "cs.ET", "cs.PL"], "pdf": "https://arxiv.org/pdf/2509.21039", "abs": "https://arxiv.org/abs/2509.21039", "authors": ["William F. Godoy", "Tatiana Melnichenko", "Pedro Valero-Lara", "Wael Elwasif", "Philip Fackler", "Rafael Ferreira Da Silva", "Keita Teranishi", "Jeffrey S. Vetter"], "title": "Mojo: MLIR-Based Performance-Portable HPC Science Kernels on GPUs for the Python Ecosystem", "comment": "Accepted at the IEEE/ACM SC25 Conference WACCPD Workshop. The\n  International Conference for High Performance Computing, Networking, Storage,\n  and Analysis, St. Louis, MO, Nov 16-21, 2025. 15 pages, 7 figures. WFG and TM\n  contributed equally", "summary": "We explore the performance and portability of the novel Mojo language for\nscientific computing workloads on GPUs. As the first language based on the\nLLVM's Multi-Level Intermediate Representation (MLIR) compiler infrastructure,\nMojo aims to close performance and productivity gaps by combining Python's\ninteroperability and CUDA-like syntax for compile-time portable GPU\nprogramming. We target four scientific workloads: a seven-point stencil\n(memory-bound), BabelStream (memory-bound), miniBUDE (compute-bound), and\nHartree-Fock (compute-bound with atomic operations); and compare their\nperformance against vendor baselines on NVIDIA H100 and AMD MI300A GPUs. We\nshow that Mojo's performance is competitive with CUDA and HIP for memory-bound\nkernels, whereas gaps exist on AMD GPUs for atomic operations and for fast-math\ncompute-bound kernels on both AMD and NVIDIA GPUs. Although the learning curve\nand programming requirements are still fairly low-level, Mojo can close\nsignificant gaps in the fragmented Python ecosystem in the convergence of\nscientific computing and AI."}
{"id": "2509.20491", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.20491", "abs": "https://arxiv.org/abs/2509.20491", "authors": ["Brahim Mahmoudi", "Naouel Moha", "Quentin Stievenert", "Florent Avellaneda"], "title": "AI-Specific Code Smells: From Specification to Detection", "comment": null, "summary": "The rise of Artificial Intelligence (AI) is reshaping how software systems\nare developed and maintained. However, AI-based systems give rise to new\nsoftware issues that existing detection tools often miss. Among these, we focus\non AI-specific code smells, recurring patterns in the code that may indicate\ndeeper problems such as unreproducibility, silent failures, or poor model\ngeneralization. We introduce SpecDetect4AI, a tool-based approach for the\nspecification and detection of these code smells at scale. This approach\ncombines a high-level declarative Domain-Specific Language (DSL) for rule\nspecification with an extensible static analysis tool that interprets and\ndetects these rules for AI-based systems. We specified 22 AI-specific code\nsmells and evaluated SpecDetect4AI on 826 AI-based systems (20M lines of code),\nachieving a precision of 88.66% and a recall of 88.89%, outperforming other\nexisting detection tools. Our results show that SpecDetect4AI supports the\nspecification and detection of AI-specific code smells through dedicated rules\nand can effectively analyze large AI-based systems, demonstrating both\nefficiency and extensibility (SUS 81.7/100)."}
{"id": "2509.21137", "categories": ["cs.DC", "cs.AR", "cs.ET"], "pdf": "https://arxiv.org/pdf/2509.21137", "abs": "https://arxiv.org/abs/2509.21137", "authors": ["Huynh Q. N. Vo", "Md Tawsif Rahman Chowdhury", "Paritosh Ramanan", "Gozde Tutuncuoglu", "Junchi Yang", "Feng Qiu", "Murat Yildirim"], "title": "From GPUs to RRAMs: Distributed In-Memory Primal-Dual Hybrid Gradient Method for Solving Large-Scale Linear Optimization Problem", "comment": "Main Article (12 Pages, 3 Figures), Appendix (4 Pages)", "summary": "The exponential growth of computational workloads is surpassing the\ncapabilities of conventional architectures, which are constrained by\nfundamental limits. In-memory computing (IMC) with RRAM provides a promising\nalternative by providing analog computations with significant gains in latency\nand energy use. However, existing algorithms developed for conventional\narchitectures do not translate to IMC, particularly for constrained\noptimization problems where frequent matrix reprogramming remains\ncost-prohibitive for IMC applications. Here we present a distributed in-memory\nprimal-dual hybrid gradient (PDHG) method, specifically co-designed for arrays\nof RRAM devices. Our approach minimizes costly write cycles, incorporates\nrobustness against device non-idealities, and leverages a symmetric\nblock-matrix formulation to unify operations across distributed crossbars. We\nintegrate a physics-based simulation framework called MELISO+ to evaluate\nperformance under realistic device conditions. Benchmarking against\nGPU-accelerated solvers on large-scale linear programs demonstrates that our\nRRAM-based solver achieves comparable accuracy with up to three orders of\nmagnitude reductions in energy consumption and latency. These results\ndemonstrate the first PDHG-based LP solver implemented on RRAMs, showcasing the\ntransformative potential of algorithm-hardware co-design for solving\nlarge-scale optimization through distributed in-memory computing."}
{"id": "2509.20497", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.20497", "abs": "https://arxiv.org/abs/2509.20497", "authors": ["Ahmed Aljohani", "Hyunsook Do"], "title": "PromptDebt: A Comprehensive Study of Technical Debt Across LLM Projects", "comment": "Accepted at Proceedings of the 2025 Evaluation and Assessment in\n  Software Engineering (EASE '25)", "summary": "Large Language Models (LLMs) are increasingly embedded in software via APIs\nlike OpenAI, offering powerful AI features without heavy infrastructure. Yet\nthese integrations bring their own form of self-admitted technical debt (SATD).\nIn this paper, we present the first large-scale empirical study of LLM-specific\nSATD: its origins, prevalence, and mitigation strategies. By analyzing 93,142\nPython files across major LLM APIs, we found that 54.49% of SATD instances stem\nfrom OpenAI integrations and 12.35% from LangChain use. Prompt design emerged\nas the primary source of LLM-specific SATD, with 6.61% of debt related to\nprompt configuration and optimization issues, followed by hyperparameter tuning\nand LLM-framework integration. We further explored which prompt techniques\nattract the most debt, revealing that instruction-based prompts (38.60%) and\nfew-shot prompts (18.13%) are particularly vulnerable due to their dependence\non instruction clarity and example quality. Finally, we release a comprehensive\nSATD dataset to support reproducibility and offer practical guidance for\nmanaging technical debt in LLM-powered systems."}
{"id": "2509.21275", "categories": ["cs.DC", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.21275", "abs": "https://arxiv.org/abs/2509.21275", "authors": ["Shiju Wang", "Yujie Wang", "Ao Sun", "Fangcheng Fu", "Zijian Zhu", "Bin Cui", "Xu Han", "Kaisheng Ma"], "title": "Data-Centric Elastic Pipeline Parallelism for Efficient Long-Context LLM Training", "comment": null, "summary": "Long context training is crucial for LLM's context extension. Existing\nschemes, such as sequence parallelism, incur substantial communication\noverhead. Pipeline parallelism (PP) reduces this cost, but its effectiveness\nhinges on partitioning granularity. Batch-level PP dividing input samples\nexhibits high memory consumption in long-context scenario, whereas token-level\nPP splitting sequences into slices alleviates memory overhead but may incur\nhardware under-utilization. This trade-off motivates adaptively selecting PP\ngranularity to match resource and workload characteristics. Moreover, sequence\nlength distribution of the real-world dataset exhibits skewness, posing a\nchallenge on PP's workload balance and efficient scheduling. Current static PP\nscheduling methods overlook the variance of sequence length, leading to\nsuboptimal performance. In this paper, we propose Elastic Pipeline Parallelism\n(EPP) that orchestrates token-level PP and batch-level PP to adapt to resource\nand workload heterogeneity. We build InfiniPipe, a distributed training system\nthat unleashes the potential of EPP via (1) a resource-aware and\nworkload-balanced sequence processor that splits long sequences and packs short\nones; and (2) a co-optimization methodology that jointly optimizes pipeline\nschedule and gradient checkpointing via a mechanism named stage-aware\nchunk-level adaptive checkpointing. Comprehensive experiments demonstrate that\nInfiniPipe achieves a 1.69x speedup over state-of-the-art systems."}
{"id": "2509.20518", "categories": ["cs.SE", "cs.PL", "D.2.3"], "pdf": "https://arxiv.org/pdf/2509.20518", "abs": "https://arxiv.org/abs/2509.20518", "authors": ["Sayed Mahbub Hasan Amiri", "Md Mainul Islam"], "title": "Enhancing Python Programming Education with an AI-Powered Code Helper: Design, Implementation, and Impact", "comment": "20 pages, 16 figures", "summary": "This is the study that presents an AI-Python-based chatbot that helps\nstudents to learn programming by demonstrating solutions to such problems as\ndebugging errors, solving syntax problems or converting abstract theoretical\nconcepts to practical implementations. Traditional coding tools like Integrated\nDevelopment Environments (IDEs) and static analyzers do not give robotic help\nwhile AI-driven code assistants such as GitHub Copilot focus on getting things\ndone. To close this gap, our chatbot combines static code analysis, dynamic\nexecution tracing, and large language models (LLMs) to provide the students\nwith relevant and practical advice, hence promoting the learning process. The\nchatbots hybrid architecture employs CodeLlama for code embedding, GPT-4 for\nnatural language interactions, and Docker-based sandboxing for secure\nexecution. Evaluated through a mixed-methods approach involving 1,500 student\nsubmissions, the system demonstrated an 85% error resolution success rate,\noutperforming standalone tools like pylint (62%) and GPT-4 (73%). Quantitative\nresults revealed a 59.3% reduction in debugging time among users, with pre- and\npost-test assessments showing a 34% improvement in coding proficiency,\nparticularly in recursion and exception handling. Qualitative feedback from 120\nstudents highlighted the chatbots clarity, accessibility, and\nconfidence-building impact, though critiques included occasional latency and\nrestrictive code sanitization. By balancing technical innovation with\npedagogical empathy, this research provides a blueprint for AI tools that\nprioritize educational equity and long-term skill retention over mere code\ncompletion. The chatbot exemplifies how AI can augment human instruction,\nfostering deeper conceptual understanding in programming education."}
{"id": "2509.20552", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.20552", "abs": "https://arxiv.org/abs/2509.20552", "authors": ["Xinyu Shi", "Zhenhao Li", "An Ran Chen"], "title": "Enhancing LLM-based Fault Localization with a Functionality-Aware Retrieval-Augmented Generation Framework", "comment": null, "summary": "Fault localization (FL) is a critical but time-consuming task in software\ndebugging, aiming to identify faulty code elements. While recent advances in\nlarge language models (LLMs) have shown promise for FL, they often struggle\nwith complex systems due to the lack of project-specific knowledge and the\ndifficulty of navigating large projects. To address these limitations, we\npropose FaR-Loc, a novel framework that enhances method-level FL by integrating\nLLMs with retrieval-augmented generation (RAG). FaR-Loc consists of three key\ncomponents: LLM Functionality Extraction, Semantic Dense Retrieval, and LLM\nRe-ranking. First, given a failed test and its associated stack trace, the LLM\nFunctionality Extraction module generates a concise natural language\ndescription that captures the failing behavior. Next, the Semantic Dense\nRetrieval component leverages a pre-trained code-understanding encoder to embed\nboth the functionality description (natural language) and the covered methods\n(code) into a shared semantic space, enabling the retrieval of methods with\nsimilar functional behavior. Finally, the LLM Re-ranking module reorders the\nretrieved methods based on their contextual relevance. Our experiments on the\nwidely used Defects4J benchmark show that FaR-Loc outperforms state-of-the-art\nLLM-based baselines SoapFL and AutoFL, by 14.6% and 9.1% in Top-1 accuracy, by\n19.2% and 22.1% in Top-5 accuracy, respectively. It also surpasses all\nlearning-based and spectrum-based baselines across all Top-N metrics without\nrequiring re-training. Furthermore, we find that pre-trained code embedding\nmodels that incorporate code structure, such as UniXcoder, can significantly\nimprove fault localization performance by up to 49.0% in Top-1 accuracy.\nFinally, we conduct a case study to illustrate the effectiveness of FaR-Loc and\nto provide insights for its practical application."}
{"id": "2509.20631", "categories": ["cs.SE", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.20631", "abs": "https://arxiv.org/abs/2509.20631", "authors": ["Michael Zhang", "Yuan Tian", "Mariam Guizani"], "title": "Design, Implementation and Evaluation of a Novel Programming Language Topic Classification Workflow", "comment": null, "summary": "As software systems grow in scale and complexity, understanding the\ndistribution of programming language topics within source code becomes\nincreasingly important for guiding technical decisions, improving onboarding,\nand informing tooling and education. This paper presents the design,\nimplementation, and evaluation of a novel programming language topic\nclassification workflow. Our approach combines a multi-label Support Vector\nMachine (SVM) with a sliding window and voting strategy to enable fine-grained\nlocalization of core language concepts such as operator overloading, virtual\nfunctions, inheritance, and templates. Trained on the IBM Project CodeNet\ndataset, our model achieves an average F1 score of 0.90 across topics and 0.75\nin code-topic highlight. Our findings contribute empirical insights and a\nreusable pipeline for researchers and practitioners interested in code analysis\nand data-driven software engineering."}
{"id": "2509.20780", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.20780", "abs": "https://arxiv.org/abs/2509.20780", "authors": ["Daniela Grassi", "Fabio Calefato", "Darja Smite", "Nicole Novielli", "Filippo Lanubile"], "title": "Exploring Engagement in Hybrid Meetings", "comment": null, "summary": "Background. The widespread adoption of hybrid work following the COVID-19\npandemic has fundamentally transformed software development practices,\nintroducing new challenges in communication and collaboration as organizations\ntransition from traditional office-based structures to flexible working\narrangements. This shift has established a new organizational norm where even\ntraditionally office-first companies now embrace hybrid team structures. While\nremote participation in meetings has become commonplace in this new\nenvironment, it may lead to isolation, alienation, and decreased engagement\namong remote team members. Aims. This study aims to identify and characterize\nengagement patterns in hybrid meetings through objective measurements, focusing\non the differences between co-located and remote participants. Method. We\nstudied professionals from three software companies over several weeks,\nemploying a multimodal approach to measure engagement. Data were collected\nthrough self-reported questionnaires and physiological measurements using\nbiometric devices during hybrid meetings to understand engagement dynamics.\nResults. The regression analyses revealed comparable engagement levels between\nonsite and remote participants, though remote participants show lower\nengagement in long meetings regardless of participation mode. Active roles\npositively correlate with higher engagement, while larger meetings and\nafternoon sessions are associated with lower engagement. Conclusions. Our\nresults offer insights into factors associated with engagement and\ndisengagement in hybrid meetings, as well as potential meeting improvement\nrecommendations. These insights are potentially relevant not only for software\nteams but also for knowledge-intensive organizations across various sectors\nfacing similar hybrid collaboration challenges."}
{"id": "2509.20837", "categories": ["cs.SE", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2509.20837", "abs": "https://arxiv.org/abs/2509.20837", "authors": ["Srishti Gureja", "Elena Tommasone", "Jingyi He", "Sara Hooker", "Matthias Gallé", "Marzieh Fadaee"], "title": "Verification Limits Code LLM Training", "comment": null, "summary": "Large language models for code generation increasingly rely on synthetic\ndata, where both problem solutions and verification tests are generated by\nmodels. While this enables scalable data creation, it introduces a previously\nunexplored bottleneck: the verification ceiling, in which the quality and\ndiversity of training data are fundamentally constrained by the capabilities of\nsynthetic verifiers. In this work, we systematically study how verification\ndesign and strategies influence model performance. We investigate (i) what we\nverify by analyzing the impact of test complexity and quantity: richer test\nsuites improve code generation capabilities (on average +3 pass@1), while\nquantity alone yields diminishing returns, (ii) how we verify by exploring\nrelaxed pass thresholds: rigid 100% pass criteria can be overly restrictive. By\nallowing for relaxed thresholds or incorporating LLM-based soft verification,\nwe can recover valuable training data, leading to a 2-4 point improvement in\npass@1 performance. However, this benefit is contingent upon the strength and\ndiversity of the test cases used, and (iii) why verification remains necessary\nthrough controlled comparisons of formally correct versus incorrect solutions\nand human evaluation: retaining diverse correct solutions per problem yields\nconsistent generalization gains. Our results show that Verification as\ncurrently practiced is too rigid, filtering out valuable diversity. But it\ncannot be discarded, only recalibrated. By combining calibrated verification\nwith diverse, challenging problem-solution pairs, we outline a path to break\nthe verification ceiling and unlock stronger code generation models."}
{"id": "2509.20881", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.20881", "abs": "https://arxiv.org/abs/2509.20881", "authors": ["Yixuan Li", "Xinyi Liu", "Weidong Yang", "Ben Fei", "Shuhao Li", "Mingjie Zhou", "Lipeng Ma"], "title": "PseudoBridge: Pseudo Code as the Bridge for Better Semantic and Logic Alignment in Code Retrieval", "comment": null, "summary": "Code search aims to precisely find relevant code snippets that match natural\nlanguage queries within massive codebases, playing a vital role in software\ndevelopment. Recent advances leverage pre-trained language models (PLMs) to\nbridge the semantic gap between unstructured natural language (NL) and\nstructured programming languages (PL), yielding significant improvements over\ntraditional information retrieval and early deep learning approaches. However,\nexisting PLM-based methods still encounter key challenges, including a\nfundamental semantic gap between human intent and machine execution logic, as\nwell as limited robustness to diverse code styles. To address these issues, we\npropose PseudoBridge, a novel code retrieval framework that introduces\npseudo-code as an intermediate, semi-structured modality to better align NL\nsemantics with PL logic. Specifically, PseudoBridge consists of two stages.\nFirst, we employ an advanced large language model (LLM) to synthesize\npseudo-code, enabling explicit alignment between NL queries and pseudo-code.\nSecond, we introduce a logic-invariant code style augmentation strategy and\nemploy the LLM to generate stylistically diverse yet logically equivalent code\nimplementations with pseudo-code, then align the code snippets of different\nstyles with pseudo-code, enhancing model robustness to code style variation. We\nbuild PseudoBridge across 10 different PLMs and evaluate it on 6 mainstream\nprogramming languages. Extensive experiments demonstrate that PseudoBridge\nconsistently outperforms baselines, achieving significant gains in retrieval\naccuracy and generalization, particularly under zero-shot domain transfer\nscenarios such as Solidity and XLCoST datasets. These results demonstrate the\neffectiveness of explicit logical alignment via pseudo-code and highlight\nPseudoBridge's potential as a robust, generalizable solution for code\nretrieval."}
{"id": "2509.21067", "categories": ["cs.SE", "cs.CY"], "pdf": "https://arxiv.org/pdf/2509.21067", "abs": "https://arxiv.org/abs/2509.21067", "authors": ["Oka Kurniawan", "Erick Chandra", "Christopher M. Poskitt", "Yannic Noller", "Kenny Tsu Wei Choo", "Cyrille Jegourel"], "title": "Designing for Novice Debuggers: A Pilot Study on an AI-Assisted Debugging Tool", "comment": "Accepted by the 25th Koli Calling International Conference on\n  Computing Education Research (Koli Calling 2025)", "summary": "Debugging is a fundamental skill that novice programmers must develop.\nNumerous tools have been created to assist novice programmers in this process.\nRecently, large language models (LLMs) have been integrated with automated\nprogram repair techniques to generate fixes for students' buggy code. However,\nmany of these tools foster an over-reliance on AI and do not actively engage\nstudents in the debugging process. In this work, we aim to design an intuitive\ndebugging assistant, CodeHinter, that combines traditional debugging tools with\nLLM-based techniques to help novice debuggers fix semantic errors while\npromoting active engagement in the debugging process. We present findings from\nour second design iteration, which we tested with a group of undergraduate\nstudents. Our results indicate that the students found the tool highly\neffective in resolving semantic errors and significantly easier to use than the\nfirst version. Consistent with our previous study, error localization was the\nmost valuable feature. Finally, we conclude that any AI-assisted debugging tool\nshould be personalized based on user profiles to optimize their interactions\nwith students."}
{"id": "2509.21068", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.21068", "abs": "https://arxiv.org/abs/2509.21068", "authors": ["Nek Dil Khan", "Javed Ali Khan", "Mobashir Husain", "Muhammad Sohail Khan", "Arif Ali Khan", "Muhammad Azeem Akbar", "Shahid Hussain"], "title": "An Improved Quantum Software Challenges Classification Approach using Transfer Learning and Explainable AI", "comment": null, "summary": "Quantum Software Engineering (QSE) is a research area practiced by tech\nfirms. Quantum developers face challenges in optimizing quantum computing and\nQSE concepts. They use Stack Overflow (SO) to discuss challenges and label\nposts with specialized quantum tags, which often refer to technical aspects\nrather than developer posts. Categorizing questions based on quantum concepts\ncan help identify frequent QSE challenges. We conducted studies to classify\nquestions into various challenges. We extracted 2829 questions from Q&A\nplatforms using quantum-related tags. Posts were analyzed to identify frequent\nchallenges and develop a novel grounded theory. Challenges include Tooling,\nTheoretical, Learning, Conceptual, Errors, and API Usage. Through content\nanalysis and grounded theory, discussions were annotated with common challenges\nto develop a ground truth dataset. ChatGPT validated human annotations and\nresolved disagreements. Fine-tuned transformer algorithms, including BERT,\nDistilBERT, and RoBERTa, classified discussions into common challenges. We\nachieved an average accuracy of 95% with BERT DistilBERT, compared to\nfine-tuned Deep and Machine Learning (D&ML) classifiers, including Feedforward\nNeural Networks (FNN), Convolutional Neural Networks (CNN), and Long Short-Term\nMemory networks (LSTM), which achieved accuracies of 89%, 86%, and 84%,\nrespectively. The Transformer-based approach outperforms the D&ML-based\napproach with a 6\\% increase in accuracy by processing actual discussions,\ni.e., without data augmentation. We applied SHAP (SHapley Additive\nexPlanations) for model interpretability, revealing how linguistic features\ndrive predictions and enhancing transparency in classification. These findings\ncan help quantum vendors and forums better organize discussions for improved\naccess and readability. However,empirical evaluation studies with actual\ndevelopers and vendors are needed."}
{"id": "2509.21170", "categories": ["cs.SE", "cs.AI", "D.2.3; I.2.7"], "pdf": "https://arxiv.org/pdf/2509.21170", "abs": "https://arxiv.org/abs/2509.21170", "authors": ["Yongda Yu", "Guohao Shi", "Xianwei Wu", "Haochuan He", "XueMing Gu", "Qianqian Zhao", "Kui Liu", "Qiushi Wang", "Zhao Tian", "Haifeng Shen", "Guoping Rong"], "title": "Fine-Tuning LLMs to Analyze Multiple Dimensions of Code Review: A Maximum Entropy Regulated Long Chain-of-Thought Approach", "comment": "22 pages", "summary": "Large Language Models (LLMs) have shown great potential in supporting\nautomated code review due to their impressive capabilities in context\nunderstanding and reasoning. However, these capabilities are still limited\ncompared to human-level cognition because they are heavily influenced by the\ntraining data. Recent research has demonstrated significantly improved\nperformance through fine-tuning LLMs with code review data. However, compared\nto human reviewers who often simultaneously analyze multiple dimensions of code\nreview to better identify issues, the full potential of these methods is\nhampered by the limited or vague information used to fine-tune the models. This\npaper contributes MelcotCR, a chain-of-thought (COT) fine-tuning approach that\ntrains LLMs with an impressive reasoning ability to analyze multiple dimensions\nof code review by harnessing long COT techniques to provide rich structured\ninformation. To address context loss and reasoning logic loss issues that\nfrequently occur when LLMs process long COT prompts, we propose a solution that\ncombines the Maximum Entropy (ME) modeling principle with pre-defined reasoning\npathways in MelcotCR to enable more effective utilization of in-context\nknowledge within long COT prompts while strengthening the logical tightness of\nthe reasoning process. Empirical evaluations on our curated MelcotCR dataset\nand the public CodeReviewer dataset reveal that a low-parameter base model,\nsuch as 14B Qwen2.5, fine-tuned with MelcotCR can surpass state-of-the-art\nmethods in terms of the accuracy of detecting and describing code issues, with\nits performance remarkably on par with that of the 671B DeepSeek-R1 model."}
{"id": "2509.21292", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.21292", "abs": "https://arxiv.org/abs/2509.21292", "authors": ["Ronivaldo Ferreira", "Guilherme da Silva", "Carla Rocha", "Gustavo Pinto"], "title": "Semantic Clustering of Civic Proposals: A Case Study on Brazil's National Participation Platform", "comment": "12 pages, in Portuguese language", "summary": "Promoting participation on digital platforms such as Brasil Participativo has\nemerged as a top priority for governments worldwide. However, due to the sheer\nvolume of contributions, much of this engagement goes underutilized, as\norganizing it presents significant challenges: (1) manual classification is\nunfeasible at scale; (2) expert involvement is required; and (3) alignment with\nofficial taxonomies is necessary. In this paper, we introduce an approach that\ncombines BERTopic with seed words and automatic validation by large language\nmodels. Initial results indicate that the generated topics are coherent and\ninstitutionally aligned, with minimal human effort. This methodology enables\ngovernments to transform large volumes of citizen input into actionable data\nfor public policy."}
