<div id=toc></div>

# Table of Contents

- [cs.SE](#cs.SE) [Total: 18]
- [cs.DC](#cs.DC) [Total: 8]
- [cs.AR](#cs.AR) [Total: 16]
- [cs.PF](#cs.PF) [Total: 2]
- [cs.MA](#cs.MA) [Total: 2]
- [cs.NI](#cs.NI) [Total: 2]


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [1] [Achieving Productivity Gains with AI-based IDE features: A Journey at Google](https://arxiv.org/abs/2601.19964)
*Maxim Tabachnyk,Xu Shu,Alexander Frömmgen,Pavel Sychev,Vahid Meimand,Ilia Krets,Stanislav Pyatykh,Abner Araujo,Kristóf Molnár,Satish Chandra*

Main category: cs.SE

TL;DR: 本文讨论谷歌开发IDE中的AI代码补全和自然语言驱动代码转换功能的过程，通过实验解决延迟、用户体验与建议质量挑战，展示企业生产力提升的案例。


<details>
  <summary>Details</summary>
Motivation: 动机是解决AI开发工具在延迟、用户体验和建议质量方面的挑战，以提高企业环境下的开发者生产力。

Method: 方法包括在用户界面、后端和模型层面对功能进行细化，并通过严谨实验进行验证。

Result: 结果带来了延迟改进、用户体验优化与建议质量提升，实现了显著的企业生产力增长。

Conclusion: 结论是本文明确了跨层优化AI工具可推动生产力进步，为企业提供了实用范例。

Abstract: We discuss Google's journey in developing and refining two internal AI-based IDE features: code completion and natural-language-driven code transformation (Transform Code). We address challenges in latency, user experience and suggestion quality, all backed by rigorous experimentation. The article serves as an example of how to refine AI developer tools across the user interface, backend, and model layers, to deliver tangible productivity improvements in an enterprise setting.

</details>


### [2] [Benchmarking Reward Hack Detection in Code Environments via Contrastive Analysis](https://arxiv.org/abs/2601.20103)
*Darshan Deshpande,Anand Kannappan,Rebecca Qian*

Main category: cs.SE

TL;DR: 本文提出涵盖54类奖励攻击的分类法，并创建人机验证基准TRACE（517条测试轨迹）；实验发现对比式异常检测中模型捕获奖励攻击能力显著优于孤立分类场景，GPT-5.2推理模式检出率从45%提升至63%，且模型处理语义上下文攻击弱于句法攻击。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型在代码生成强化学习中充当评估者，其检测奖励攻击的能力仍缺乏研究，亟需构建可靠环境以防止奖励欺骗问题。

Method: 引入奖励攻击分类法和TRACE基准（含517条合成人验轨迹），采用对比异常检测框架代替传统孤立分类评估，通过定性分析和消融实验探究模型行为及轨迹比例、聚类规模的影响。

Result: 对比检测模式下模型性能显著提升：GPT-5.2最强推理模式检出率达63%（孤立场景仅45%）；顶尖模型处理语义上下文化攻击的能力弱于句法攻击；良性/攻击轨迹比例及分析聚类规模对检测效果有显著影响。

Conclusion: 对比式检测更有效揭示奖励攻击，同时凸显模型语义理解短板；发布TRACE基准与评估工具供社区拓展研究，推动代码生成强化学习环境的安全性提升。

Abstract: Recent advances in reinforcement learning for code generation have made robust environments essential to prevent reward hacking. As LLMs increasingly serve as evaluators in code-based RL, their ability to detect reward hacking remains understudied. In this paper, we propose a novel taxonomy of reward exploits spanning across 54 categories and introduce TRACE (Testing Reward Anomalies in Code Environments), a synthetically curated and human-verified benchmark containing 517 testing trajectories. Unlike prior work that evaluates reward hack detection in isolated classification scenarios, we contrast these evaluations with a more realistic, contrastive anomaly detection setup on TRACE. Our experiments reveal that models capture reward hacks more effectively in contrastive settings than in isolated classification settings, with GPT-5.2 with highest reasoning mode achieving the best detection rate at 63%, up from 45% in isolated settings on TRACE. Building on this insight, we demonstrate that state-of-the-art models struggle significantly more with semantically contextualized reward hacks compared to syntactically contextualized ones. We further conduct qualitative analyses of model behaviors, as well as ablation studies showing that the ratio of benign to hacked trajectories and analysis cluster sizes substantially impact detection performance. We release the benchmark and evaluation harness to enable the community to expand TRACE and evaluate their models.

</details>


### [3] [Are We All Using Agents the Same Way? An Empirical Study of Core and Peripheral Developers Use of Coding Agents](https://arxiv.org/abs/2601.20106)
*Shamse Tasnim Cynthia,Joy Krishan Das,Banani Roy*

Main category: cs.SE

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Autonomous AI agents are transforming software development and redefining how developers collaborate with AI. Prior research shows that the adoption and use of AI-powered tools differ between core and peripheral developers. However, it remains unclear how this dynamic unfolds in the emerging era of autonomous coding agents. In this paper, we present the first empirical study of 9,427 agentic PRs, examining how core and peripheral developers use, review, modify, and verify agent-generated contributions prior to acceptance. Through a mix of qualitative and quantitative analysis, we make four key contributions. First, a subset of peripheral developers use agents more often, delegating tasks evenly across bug fixing, feature addition, documentation, and testing. In contrast, core developers focus more on documentation and testing, yet their agentic PRs are frequently merged into the main/master branch. Second, core developers engage slightly more in review discussions than peripheral developers, and both groups focus on evolvability issues. Third, agentic PRs are less likely to be modified, but when they are, both groups commonly perform refactoring. Finally, peripheral developers are more likely to merge without running CI checks, whereas core developers more consistently require passing verification before acceptance. Our analysis offers a comprehensive view of how developer experience shapes integration offer insights for both peripheral and core developers on how to effectively collaborate with coding agents.

</details>


### [4] [Beyond Bug Fixes: An Empirical Investigation of Post-Merge Code Quality Issues in Agent-Generated Pull Requests](https://arxiv.org/abs/2601.20109)
*Shamse Tasnim Cynthia,Al Muttakin,Banani Roy*

Main category: cs.SE

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: The increasing adoption of AI coding agents has increased the number of agent-generated pull requests (PRs) merged with little or no human intervention. Although such PRs promise productivity gains, their post-merge code quality remains underexplored, as prior work has largely relied on benchmarks and controlled tasks rather than large-scale post-merge analyses. To address this gap, we analyze 1,210 merged agent-generated bug-fix PRs from Python repositories in the AIDev dataset. Using SonarQube, we perform a differential analysis between base and merged commits to identify code quality issues newly introduced by PR changes. We examine issue frequency, density, severity, and rule-level prevalence across five agents. Our results show that apparent differences in raw issue counts across agents largely disappear after normalizing by code churn, indicating that higher issue counts are primarily driven by larger PRs. Across all agents, code smells dominate, particularly at critical and major severities, while bugs are less frequent but often severe. Overall, our findings show that merge success does not reliably reflect post-merge code quality, highlighting the need for systematic quality checks for agent-generated bug-fix PRs.

</details>


### [5] [Usage, Effects and Requirements for AI Coding Assistants in the Enterprise: An Empirical Study](https://arxiv.org/abs/2601.20112)
*Maja Vukovic,Rangeet Pan,Tin Kam Ho,Rahul Krishna,Raju Pavuluri,Michele Merler*

Main category: cs.SE

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: The rise of large language models (LLMs) has accelerated the development of automated techniques and tools for supporting various software engineering tasks, e.g., program understanding, code generation, software testing, and program repair. As CodeLLMs are being employed toward automating these tasks, one question that arises, especially in enterprise settings, is whether these coding assistants and the code LLMs that power them are ready for real-world projects and enterprise use cases, and how do they impact the existing software engineering process and user experience. In this paper we survey 57 developers from different domains and with varying software engineering skill about their experience with AI coding assistants and CodeLLMs. We also reviewed 35 user surveys on the usage, experience and expectations of professionals and students using AI coding assistants and CodeLLMs. Based on our study findings and analysis of existing surveys, we discuss the requirements for AI-powered coding assistants.

</details>


### [6] [Not All Tokens Matter: Data-Centric Optimization for Efficient Code Summarization](https://arxiv.org/abs/2601.20147)
*Saima Afrin,Zaiyu Cheng,Tushar Sharma,Alexander Serebrenik,Massimiliano Di Penta,Antonio Mastropaolo*

Main category: cs.SE

TL;DR: 该研究探讨系统提示对指令调优语言模型在代码生成任务中表现的影响，结合模型规模、提示策略和编程语言变量分析其效应。


<details>
  <summary>Details</summary>
Motivation: 尽管指令调优模型在代码生成领域表现优异，但系统提示对通用ILMs和专用CLMs的影响尚未充分研究，该研究旨在填补这一空白。

Method: 通过系统评估120个模型配置，测试不同详细程度的系统抱歉、模型规模、提示策略及编程语言（如Python vs Java）在代码生成任务中的影响。

Result: 结果包括：系统提示的影响随模型规模扩大而增强；少样本提示策略比零样本降低此类影响；Java编程语言比对提示变化的敏感性高于Python。

Conclusion: 系统提示在代码生成中是关键因素，其效果受模型规模和编程语言驱动，需针对性设计以优化模型性能。

Abstract: Instruction-tuned Language Models ILMs have become essential components of modern AI systems, demonstrating exceptional versatility across a wide range of natural language and reasoning tasks. Among their most impactful applications is code generation, where ILMs--commonly referred to as Code Language Models CLMs--have demonstrated remarkable capability. This strength stems from their defining feature: the use of explicit task instructions during fine-tuning, which enables them to bridge natural language and code by translating human intent into executable code. While much of their progress has been driven by advances in scaling laws and training methodologies, one critical aspect remains underexplored--the impact of system prompts on the performance of both general-purpose ILMs and specialized CLMs when instantiated to assist users with code generation activities. In this study, we take a first step toward bridging this gap by systematically evaluating how system prompts of varying instructional detail, along with model scale, prompting strategy, and programming language, affect ILMs and CLMs in code generation tasks. Our evaluation framework, spanning 120 model configurations, reveals that (1) the influence of system prompts increases with model scale; (2) few-shot prompting reduces this effect compared to zero-shot; and (3) programming language matters, with Java showing greater sensitivity to system prompt variations than Python.

</details>


### [7] [LogSieve: Task-Aware CI Log Reduction for Sustainable LLM-Based Analysis](https://arxiv.org/abs/2601.20148)
*Marcus Emmanuel Barnes,Taher A. Ghaleb,Safwat Hassan*

Main category: cs.SE

TL;DR: 提出LogSieve方法，用于持续集成日志的语义保留压缩，减少42%日志量和40%计算量，提升分析效率与环保性。


<details>
  <summary>Details</summary>
Motivation: 持续immel集成日志日益庞大且非结构化，现有方法主要针对结构化日志，导致人工与自动化分析成本高昂、能耗大。

Method: 开发轻量级LogSieve技术：利用基于嵌入的分类器自动过滤低价值日志行，保留根因分析与下游推理所需语义内容。

Result: 在20个Android项目的GitHub-Action日志测试中，语义保留指标优异(Cosine=0.93，GPTScore=0.93)，分类shima器准确率达97%，较LogZip及随机删减基线显著优化。

Conclusion: LogSieve通过日志管理与LLM推理结合，为可持续发展和可解释的CI自动化提供有效路径，降低计算能耗40%。

Abstract: Logs are essential for understanding Continuous Integration (CI) behavior, particularly for diagnosing build failures and performance regressions. Yet their growing volume and verbosity make both manual inspection and automated analysis increasingly costly, time-consuming, and environmentally costly. While prior work has explored log compression, anomaly detection, and LLM-based log analysis, most efforts target structured system logs rather than the unstructured, noisy, and verbose logs typical of CI workflows.
  We present LogSieve, a lightweight, RCA-aware and semantics-preserving log reduction technique that filters low-information lines while retaining content relevant to downstream reasoning. Evaluated on CI logs from 20 open-source Android projects using GitHub Actions, LogSieve achieves an average 42% reduction in lines and 40% reduction in tokens with minimal semantic loss. This pre-inference reduction lowers computational cost and can proportionally reduce energy use (and associated emissions) by decreasing the volume of data processed during LLM inference.
  Compared with structure-first baselines (LogZip and random-line removal), LogSieve preserves much higher semantic and categorical fidelity (Cosine = 0.93, GPTScore = 0.93, 80% exact-match accuracy). Embedding-based classifiers automate relevance detection with near-human accuracy (97%), enabling scalable and sustainable integration of semantics-aware filtering into CI workflows. LogSieve thus bridges log management and LLM reasoning, offering a practical path toward greener and more interpretable CI automation.

</details>


### [8] [Cascaded Vulnerability Attacks in Software Supply Chains](https://arxiv.org/abs/2601.20158)
*Laura Baird,Armin Moin*

Main category: cs.SE

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Most of the current software security analysis tools assess vulnerabilities in isolation. However, sophisticated software supply chain security threats often stem from cascaded vulnerability and security weakness chains that span dependent components. Moreover, although the adoption of Software Bills of Materials (SBOMs) has been accelerating, downstream vulnerability findings vary substantially across SBOM generators and analysis tools. We propose a novel approach to SBOM-driven security analysis methods and tools. We model vulnerability relationships over dependency structure rather than treating scanner outputs as independent records. We represent enriched SBOMs as heterogeneous graphs with nodes being the SBOM components and dependencies, the known software vulnerabilities, and the known software security weaknesses. We then train a Heterogeneous Graph Attention Network (HGAT) to predict whether a component is associated with at least one known vulnerability. Since documented multi-vulnerability chains are scarce, we model cascade discovery as a link prediction problem over CVE pairs using a multi-layer perceptron neural network. This way, we produce ranked candidate links that can be composed into multi-step paths. The HGAT component classifier achieves an Accuracy of 91.03% and an F1-score of 74.02%.

</details>


### [9] [Control Models for In-IDE Code Completion](https://arxiv.org/abs/2601.20223)
*Aral de Moor,Yana Hrynevich,Hleb Badzeika,Vladyslav Furda,Marko Kojic,Artem Savelev,Kostadin Cvejoski,Darya Rovdo,Ekaterina Garanina*

Main category: cs.SE

TL;DR: 本文提出用于JetBrains IDE中LLM代码补全的ML控制模型，通过分类器触发推断并过滤建议以优化用户体验，经离线评估和A/B测试证实提升了效率和质量。


<details>
  <summary>Details</summary>
Motivation: 旨在更好地对齐用户需求，减少不必要的LLM请求，增强IDE环境中LLM功能的智能集成。

Method: 在包含98名用户的真实代码补全离线数据集上评估Boosting和Transformer架构；为语法多样化的语言测试Boosting模型分类性能；并在生产环境中进行A/B实验。

Result: A/B测试显示代码补全效率和质量指标显著提升，离线分类性能在多语言验证中表现良好。

Conclusion: 证实了辅助模型在IDE集成LLM功能中的潜力，为未来研究方向（如改进模型适用性）和开放问题提供了线索。

Abstract: We introduce control models for LLM-powered code completion in JetBrains IDEs: ML classifiers which trigger inference and filter the generated suggestions to better align them with users and reduce unnecessary requests. To this end, we evaluate boosting- and transformer-based architectures on an offline dataset of real code completions with n=98 users. We further evaluate the offline classification performance of our boosting-based approach on a range of syntactically diverse languages; and perform an A/B study in a production environment where they improve completion efficiency and quality metrics. With this study, we hope to demonstrate the potential in using auxiliary models for smarter in-IDE integration of LLM-driven features, highlight fruitful future directions, and open problems.

</details>


### [10] [Understanding npm Developers' Practices, Challenges, and Recommendations for Secure Package Development](https://arxiv.org/abs/2601.20240)
*Anthony Peruma,Truman Choy,Gerald Lee,Italo De Oliveira Santos*

Main category: cs.SE

TL;DR: 研究调查了npm包开发者如何处理安全问题，发现开发者重视安全但面临供应链攻击和依赖漏洞等挑战，工具满意度仅40%，因高误报和时间限制，并提出改进工具和文档等需求。


<details>
  <summary>Details</summary>
Motivation: 近年npm第三方包漏洞导致严重安全事件，需探究开发者对安全风险的认知、实践及障碍，以提升生态系统安全性。

Method: 通过在线调查收集75名npm包开发者的反馈，并采用混合方法分析回答。

Result: 开发者认为包安全性中等，担忧供应链攻击；仅40%满意现有工具；偏爱自动化方法（如双因素认证），但受限于时间和高误报；通过快速补丁响应漏洞，建议加强检测工具、文档和教育。

Conclusion: Conclusion extraction failed

Abstract: Background: The Node Package Manager (npm) ecosystem plays a vital role in modern software development by providing a vast repository of packages and tools that developers can use to implement their software systems. However, recent vulnerabilities in third-party packages have led to serious security breaches, compromising the integrity of applications that depend on them. Objective: This study investigates how npm package developers perceive and handle security in their work. We examined developers' understanding of security risks, the practices and tools they use, the barriers to stronger security measures, and their suggestions for improving the npm ecosystem's security. Method: We conducted an online survey with 75 npm package developers and undertook a mixed-methods approach to analyzing their responses. Results: While developers prioritize security, they perceive their packages as only moderately secure, with concerns about supply chain attacks, dependency vulnerabilities, and malicious code. Only 40% are satisfied with the current npm security tools due to issues such as alert fatigue. Automated methods such as two-factor authentication and npm audit are favored over code reviews. Many drop dependencies due to abandonment or vulnerabilities, and typically respond to vulnerabilities in their packages by quickly releasing patches. Key barriers include time constraints and high false-positive rates. To improve npm security, developers seek better detection tools, clearer documentation, stronger account protections, and more education initiatives. Conclusion: Our findings will benefit npm package contributors and maintainers by highlighting prevalent security challenges and promoting discussions on best practices to strengthen security and trustworthiness within the npm landscape.

</details>


### [11] [How Software Engineering Research Overlooks Local Industry: A Smaller Economy Perspective](https://arxiv.org/abs/2601.20382)
*Klara Borowa,Andrzej Zalewski,Lech Madeyski*

Main category: cs.SE

TL;DR: 波兰研究人员使用反思性 индивидуализ主题分析ICSE FOSE调查，关注小型经济体研究与产业差距，提出改进建议


<details>
  <summary>Details</summary>
Motivation: 代表小型非英语经济体揭示软件工程社区中日益扩大的研究与产业脱节问题

Method: 采用反思性主题分析法分析 multinational ICSE FOSE社区调查数据

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: The software engineering researchers from countries with smaller economies, particularly non-English speaking ones, represent valuable minorities within the software engineering community. As researchers from Poland, we represent such a country. We analyzed the ICSE FOSE (Future of Software Engineering) community survey through reflexive thematic analysis to show our viewpoint on key software community issues. We believe that the main problem is the growing research-industry gap, which particularly impacts smaller communities and small local companies. Based on this analysis and our experiences, we present a set of recommendations for improvements that would enhance software engineering research and industrial collaborations in smaller economies.

</details>


### [12] [Comprehension vs. Adoption: Evaluating a Language Workbench Through a Family of Experiments](https://arxiv.org/abs/2601.20394)
*Giovanna Broccia,Maurice H. ter Beek,Walter Cazzola,Luca Favalli,Francesco Bertolotti,Alessio Ferrari*

Main category: cs.SE

TL;DR: 论文评估语言工作台Neverlang的可理解性和用户接受度，发现用户理解其元语言并认可其有用性，但易用性不足，且可理解性与接受度无直接关联。


<details>
  <summary>Details</summary>
Motivation: 现有文献忽略语言工作台评估中以用户为中心的可理解性和接受度因素，需填补此研究空白。

Method: 采用改进方法评估模型(MEM)，通过三轮学术参与者实验，分析元语言可理解性、感知易用性、感知有用性及使用意图间的关联。

Result: 用户对元语言（尤其语法）理解充分，认可有用性并有使用意向；但易用性不足，且易用性/有用性显著影响使用意愿；元语言理解度与用户接受度无显著相关性。

Conclusion: 用户中心评估揭示语言工作台采纳的复杂性：易用性是关键障碍，高可理解性不必然提升接受度，需独立优化理解与采纳因素。

Abstract: Language workbenches are tools that enable the definition, reuse, and composition of programming languages and their ecosystems, aiming to streamline language development. To facilitate their adoption by language designers, the comprehensibility of the language used to define other languages is an important aspect to evaluate. Moreover, considering that language workbenches are relatively new tools, user acceptance emerges as a crucial factor to be accounted for during their assessment. Current literature often neglects user-centred aspects like comprehensibility and acceptance in the assessment of this breed of tools. This paper addresses this gap through a family of experiments assessing Neverlang, a modular language workbench. The study adopts a tailored version of the Method Evaluation Model (MEM) to evaluate the comprehensibility of Neverlang's meta-language and programs, as well as user acceptance in terms of perceived ease of use, perceived usefulness, and intention to use. It also investigates the relationships among these dimensions. The experiments were conducted in three iterations involving participants from academia. The results reveal that users demonstrate sufficient comprehension of Neverlang's meta-language, particularly concerning its syntax, express a favourable perception of its usefulness, and indicate their intention to use it. However, the results also indicate that Neverlang's ease of use remains a challenge. Additionally, variations in the perceived ease of use and perceived usefulness, whether low or high, influence the users' intention to use the tool. Surprisingly, no significant correlation is found between comprehensibility and user acceptance. Notably, higher comprehensibility of the meta-language does not necessarily translate into greater acceptance, underscoring the complex interplay between comprehension and adoption.

</details>


### [13] [An Empirical Evaluation of Modern MLOps Frameworks](https://arxiv.org/abs/2601.20415)
*Jon Marcos-Mercadé,Unai Lopez-Novoa,Mikel Egaña Aranguren*

Main category: cs.SE

TL;DR: 该论文经验性地评估了MLOps工具MLflow、Metaflow、Apache Airflow和Kubeflow Pipelines，基于安装便捷性等标准，比较其在数字和情感分类任务中的表现，提出实用推荐。


<details>
  <summary>Details</summary>
Motivation: AI解决方案在专业环境应用日益广泛，为使开发者能对MLOps工具做出明智选择，实施了本次评估，以支持ML模型生命周期的管理决策。

Method: 使用MNIST数据集的手写数字分类和IMDB数据集的BERT情感分类两种常见场景，评估工具在安装简易度、配置灵活性、互操作性、代码复杂度、结果可解释性、文档支持等指标，并提供加权结果。

Result: 评估得出加权结果，显示了不同工具在多种场景下的优势和不足，具体推荐了最适合各类需求的工具组合。

Conclusion: 该研究为开发者提供了基于场景选择的MLOps工具实用指南，有助于优化ML模型生命周期管理。

Abstract: Given the increasing adoption of AI solutions in professional environments, it is necessary for developers to be able to make informed decisions about the current tool landscape. This work empirically evaluates various MLOps (Machine Learning Operations) tools to facilitate the management of the ML model lifecycle: MLflow, Metaflow, Apache Airflow, and Kubeflow Pipelines. The tools are evaluated by assessing the criteria of Ease of installation, Configuration flexibility, Interoperability, Code instrumentation complexity, result interpretability, and Documentation when implementing two common ML scenarios: Digit classifier with MNIST and Sentiment classifier with IMDB and BERT. The evaluation is completed by providing weighted results that lead to practical conclusions on which tools are best suited for different scenarios.

</details>


### [14] [Challenges in Android Data Disclosure: An Empirical Study](https://arxiv.org/abs/2601.20459)
*Mugdha Khedkar,Michael Schlichtig,Mohamed Soliman,Eric Bodden*

Main category: cs.SE

TL;DR: 本文实证研究Android开发者填报Google Play数据安全表单的经验，揭示分类困难与信心不足问题，需改进指南和工具。


<details>
  <summary>Details</summary>
Motivation: 当前法律要求Android开发者准确报告应用数据收集，但因大型代码库困难大，该研究旨在理解开发者在Google Play Data Safety Section表单填报中的实际体验。

Method: 结合调查41名Android开发者与分析172个在线讨论（含642名额外开发者），总覆盖683名开发者观点。

Result: 开发者常手动分类或省略隐私数据，过度依赖在线资源；虽有信心识别收集数据，但在转向合规披露时信心不足，挑战包括识别隐私相关信息、表单理解和顾虑应用被拒。

Conclusion: 结果呼吁提供更清晰指导与易用工具，辅助开发者满足隐私报告义务。

Abstract: Current legal frameworks enforce that Android developers accurately report the data their apps collect. However, large codebases can make this reporting challenging. This paper employs an empirical approach to understand developers' experience with Google Play Store's Data Safety Section (DSS) form.
  We first survey 41 Android developers to understand how they categorize privacy-related data into DSS categories and how confident they feel when completing the DSS form. To gain a broader and more detailed view of the challenges developers encounter during the process, we complement the survey with an analysis of 172 online developer discussions, capturing the perspectives of 642 additional developers. Together, these two data sources represent insights from 683 developers.
  Our findings reveal that developers often manually classify the privacy-related data their apps collect into the data categories defined by Google-or, in some cases, omit classification entirely-and rely heavily on existing online resources when completing the form. Moreover, developers are generally confident in recognizing the data their apps collect, yet they lack confidence in translating this knowledge into DSS-compliant disclosures. Key challenges include issues in identifying privacy-relevant data to complete the form, limited understanding of the form, and concerns about app rejection due to discrepancies with Google's privacy requirements.
  These results underscore the need for clearer guidance and more accessible tooling to support developers in meeting privacy-aware reporting obligations.

</details>


### [15] [DRAINCODE: Stealthy Energy Consumption Attacks on Retrieval-Augmented Code Generation via Context Poisoning](https://arxiv.org/abs/2601.20615)
*Yanlin Wang,Jiadong Wu,Tianyue Jiang,Mingwei Liu,Jiachi Chen,Chong Wang,Ensheng Shi,Xilin Liu,Yuchi Ma,Zibin Zheng*

Main category: cs.SE

TL;DR: 本文提出DrainCode攻击方法，首次针对基于RAG的代码生成系统进行对抗性攻击，通过污染检索上下文强制LLM生成长文本，显著增加计算开销。


<details>
  <summary>Details</summary>
Motivation: 现有研究忽略大型语言模型在代码生成中的计算成本问题（如延迟和能耗），尤其在安全领域的潜在风险未被充分探索。

Method: 采用突变策略污染检索上下文，诱导LLM生成冗余输出，从而提升GPU延迟和能耗，并在多模型和提示策略下验证攻击的普适性。

Result: 实验表明：最大延迟增加85%，能耗增加49%，输出长度增长超3倍；该攻击对多种防御机制有效，且具备跨策略迁移能力。

Conclusion: DrainCode通过增加LLM计算开销，为资源受限环境中的模型安全评估提供新手段，突显了对抗性攻击对系统效能的威胁。

Abstract: Large language models (LLMs) have demonstrated impressive capabilities in code generation by leveraging retrieval-augmented generation (RAG) methods. However, the computational costs associated with LLM inference, particularly in terms of latency and energy consumption, have received limited attention in the security context. This paper introduces DrainCode, the first adversarial attack targeting the computational efficiency of RAG-based code generation systems. By strategically poisoning retrieval contexts through a mutation-based approach, DrainCode forces LLMs to produce significantly longer outputs, thereby increasing GPU latency and energy consumption. We evaluate the effectiveness of DrainCode across multiple models. Our experiments show that DrainCode achieves up to an 85% increase in latency, a 49% increase in energy consumption, and more than a 3x increase in output length compared to the baseline. Furthermore, we demonstrate the generalizability of the attack across different prompting strategies and its effectiveness compared to different defenses. The results highlight DrainCode as a potential method for increasing the computational overhead of LLMs, making it useful for evaluating LLM security in resource-constrained environments. We provide code and data at https://github.com/DeepSoftwareAnalytics/DrainCode.

</details>


### [16] [Lila: Decentralized Build Reproducibility Monitoring for the Functional Package Management Model](https://arxiv.org/abs/2601.20662)
*Julien Malka,Arnout Engelen*

Main category: cs.SE

TL;DR: 文章讨论了软件构建产物完整性的重要性，针对大规模可重现构建在监控基础设施方面的挑战，提出了Lila系统来解决。


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: 引入Lila，一个针对功能包管理模型设计的去中心化系统，用于可重现性评估，支持分布式构建结果报告和数据聚合为可重现性数据库。 remediation: :文字列としてはこのように記載せよ。

Result: Lila实现了对构建结果的可重现性监控和数据库聚合，为实践者和未来实证研究提供了支持，有助于流动作业改善监控能力。

Conclusion: Lila系统为软件分发中的透明度和信任问题提供了一种可扩展的解决方案，提升了大规模环境下可重现构建的可行性。

Abstract: Ensuring the integrity of software build artifacts is an increasingly important concern for modern software engineering, driven by increasingly sophisticated attacks on build systems, distribution channels, and development infrastructures. Reproducible builds $\unicode{x2013}$ where binaries built independently from the same source code can be verified to be bit-for-bit identical to the distributed artifacts $\unicode{x2013}$ provide a principled foundation for transparency and trust in software distribution.
  Despite their potential, the large-scale adoption of reproducible builds faces two significant challenges: achieving high reproducibility rates across vast software collections and establishing reproducibility monitoring infrastructure that can operate at very large scale. While recent studies have shown that high reproducibility rates are achievable at scale $\unicode{x2013}$ demonstrated by the Nix ecosystem achieving over 90% reproducibility on more than 80,000 packages $\unicode{x2013}$ the problem of effective reproducibility monitoring remains largely unsolved.
  In this work, we address the reproducibility monitoring challenge by introducing Lila, a decentralized system for reproducibility assessment tailored to the functional package management model. Lila enables distributed reporting of build results and aggregation into a reproducibility database, benefiting both practitioners and future empirical build reproducibility studies.

</details>


### [17] [ProfInfer: An eBPF-based Fine-Grained LLM Inference Profiler](https://arxiv.org/abs/2601.20755)
*Bohua Zou,Debayan Roy,Dhimankumar Yogesh Airao,Weihao Xu,Binqi Sun,Yutao Liu,Haibo Chen*

Main category: cs.SE

TL;DR: 开发基于eBPF的非侵入式分析框架，实时剖析LLM推理引擎运行细节，实现低开销高性能监控


<details>
  <summary>Details</summary>
Motivation: 当前LLM推理系统缺乏算子级的运行时速率和资源消耗可见性，导致开发人员无法诊断性能瓶颈（如内存或计算瓶颈）

Method: 利用扩展型柏克莱封包过滤器（eBPF）技术，在不修改源码前提下动态挂载探针至运行时函数层，将追踪数据转化为算子/图/时间线/硬件指标等多维度可视化分析

Result: 框架实现<4%运行时开销下的高精度剖析，成功揭示稠密推理、混合专家路由及算子卸载等行为的实际运行特征

Conclusion: 该工具使LLM推理过程具备可诊断性与透明性，为优化、调度及资源感知部署提供实用分析手段

Abstract: As large language models (LLMs) move from research to production, understanding how inference engines behave in real time has become both essential and elusive. Unlike general-purpose engines such as ONNX Runtime, today's LLM inference systems offer little operator-level visibility, leaving developers blind to where time and resources go. Even basic questions -- is this workload memory-bound or compute-bound? -- often remain unanswered. To close this gap, we develop a fine-grained, non-intrusive profiling framework for modern LLM inference engines, exemplified by llama.cpp but applicable to similar runtime architectures. Built on extended Berkeley Packet Filter (eBPF) technology, our system dynamically attaches probes to runtime functions across multiple layers -- without modifying or recompiling the source. It transforms collected traces into rich visualizations of operators, graphs, timelines, and hardware counter trends, exposing how dense inference, Mixture-of-Experts routing, and operator offloading behave in practice. With less than 4% runtime overhead and high profiling fidelity, our framework makes LLM inference both transparent and diagnosable, turning performance profiling into a practical tool for optimization, scheduling, and resource-aware deployment.

</details>


### [18] [Context-Augmented Code Generation Using Programming Knowledge Graphs](https://arxiv.org/abs/2601.20810)
*Shahd Seddik,Fahd Seddik,Iman Saberi,Fatemeh Fard,Minh Hieu Huynh,Patanamon Thongtanunam*

Main category: cs.SE

TL;DR: 提出程式知識圖譜(PKG)架構，改善LLM生成代碼時檢索失效和幻覺問題


<details>
  <summary>Details</summary>
Motivation: LLM在處理複雜程式問題時檢索精度不足，生成模型易產生無關幻覺

Method: 建構程式知識圖譜實現語義表示與細粒度檢索，結合樹剪枝技術和重新排序機制

Result: HumanEval/MBPP基準測試顯示pass@1準確率提升20%，MBPP任務超越基線34%

Conclusion: PKG架構顯著提升複雜問題處理能力，對非RAG正確方案負面影響最小

Abstract: Large Language Models (LLMs) excel at code generation but struggle with complex problems. Retrieval-Augmented Generation (RAG) mitigates this issue by integrating external knowledge, yet retrieval models often miss relevant context, and generation models hallucinate with irrelevant data. We propose Programming Knowledge Graph (PKG) for semantic representation and fine-grained retrieval of code and text. Our approach enhances retrieval precision through tree pruning and mitigates hallucinations via a re-ranking mechanism that integrates non-RAG solutions. Structuring external data into finer-grained nodes improves retrieval granularity. Evaluations on HumanEval and MBPP show up to 20% pass@1 accuracy gains and a 34% improvement over baselines on MBPP. Our findings demonstrate that our proposed PKG approach along with re-ranker effectively address complex problems while maintaining minimal negative impact on solutions that are already correct without RAG. The replication package is published at https://github.com/iamshahd/ProgrammingKnowledgeGraph

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [19] [A Data-Informed Local Subspaces Method for Error-Bounded Lossy Compression of Large-Scale Scientific Datasets](https://arxiv.org/abs/2601.20113)
*Arshan Khan,Rohit Deshmukh,Ben O'Neill*

Main category: cs.DC

TL;DR: 介绍一种名为 Discontinuous DLS 的数据驱动压缩器，通过局部时空子空间提升科学数据压缩效率，同时确保误差有界。


<details>
  <summary>Details</summary>
Motivation: 解决科学模拟数据量暴增带来的存储与传输挑战，需高效压缩技术以保证分析有效性。

Method: 利用数据驱动的局部空间和时间子空间，在分布式计算环境中基于 MPI 实现，并评估压缩比和重建准确性。

Result: 显著降低存储需求，不妥协数据保真，在压缩比和精度上超越先进压缩方法。

Conclusion: Discontinuous DLS 是一种前景广阔的方法，适用于高性能计算环境下大规模科学数据管理，满足日益增长的需求。

Abstract: The growing volume of scientific simulation data presents a significant challenge for storage and transfer. Error-bounded lossy compression has emerged as a critical solution for mitigating these challenges, providing a means to reduce data size while ensuring that reconstructed data remains valid for scientific analysis. In this paper, we present a data-driven scientific data compressor, called Discontinuous Data-informed Local Subspaces (Discontinuous DLS), to improve compression-to-error ratios over data-agnostic compressors. This error-bounded compressor leverages localized spatial and temporal subspaces, informed by the underlying data structure, to enhance compression efficiency and preserve key features. The presented technique is flexible and applicable to a wide range of scientific data, including fluid dynamics, environmental simulations, and other high-dimensional, time-dependent datasets. We describe the core principles of the method and demonstrate its ability to significantly reduce storage requirements without compromising critical data fidelity. The technique is implemented in a distributed computing environment using MPI, and its performance is evaluated against state-of-the-art error-bounded compression methods in terms of compression ratio and reconstruction accuracy. This study highlights discontinuous DLS as a promising approach for large-scale scientific data compression in high-performance computing environments, providing a robust solution for managing the growing data demands of modern scientific simulations.

</details>


### [20] [StreamFusion: Scalable Sequence Parallelism for Distributed Inference of Diffusion Transformers on GPUs](https://arxiv.org/abs/2601.20273)
*Jiacheng Yang,Jun Wu,Yaoyao Ding,Zhiying Xu,Yida Wang,Gennady Pekhimenko*

Main category: cs.DC

TL;DR: StreamFusion是一种拓扑感知型高效DiT服务引擎，解决序列并行技术局限性，平均性能提升1.35倍。


<details>
  <summary>Details</summary>
Motivation: 现有序列并行技术在高分辨率图像和长视频生成中存在通信模式不适现代GPU拓扑、机器间全对全操作导致的延迟 onGPUsender-receiver同步及计算开销大的问题，导致单GPU推理效率低下。

Method: 引入了三项创新：拓扑感知序列平行技术，考虑机器内外带宽差异；Torus Attention重叠机器间全对全操作与计算；单边通信实现减少同步和计算开销。

Result: 实验显示StreamFusion性能平均高出先进方法1.35倍，最高达1.77倍。

Conclusion: StreamFusion有效提升Diffusion Transformers推理效率，解决了现有框架的瓶颈，支持更高效的高质量图像和视频生成。

Abstract: Diffusion Transformers (DiTs) have gained increasing adoption in high-quality image and video generation. As demand for higher-resolution images and longer videos increases, single-GPU inference becomes inefficient due to increased latency and large activation sizes. Current frameworks employ sequence parallelism (SP) techniques such as Ulysses Attention and Ring Attention to scale inference. However, these implementations have three primary limitations: (1) suboptimal communication patterns for network topologies on modern GPU machines, (2) latency bottlenecks from all-to-all operations in inter-machine communication, and (3) GPU sender-receiver synchronization and computation overheads from using two-sided communication libraries. To address these issues, we present StreamFusion, a topology-aware efficient DiT serving engine. StreamFusion incorporates three key innovations: (1) a topology-aware sequence parallelism technique that accounts for inter- and intra-machine bandwidth differences, (2) Torus Attention, a novel SP technique enabling overlapping of inter-machine all-to-all operations with computation, and (3) a one-sided communication implementation that minimizes GPU sender-receiver synchronization and computation overheads. Our experiments demonstrate that StreamFusion outperforms the state-of-the-art approach by an average of $1.35\times$ (up to $1.77\times$).

</details>


### [21] [Agentic Fog: A Policy-driven Framework for Distributed Intelligence in Fog Computing](https://arxiv.org/abs/2601.20764)
*Saeed Akbar,Muhammad Waqas,Rahmat Ullah*

Main category: cs.DC

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Fog and edge computing require adaptive control schemes that can handle partial observability, severe latency requirements, and dynamically changing workloads. Recent research on Agentic AI (AAI) increasingly integrates reasoning systems powered by Large Language Models; however, these tools are not applicable to infrastructure-level systems due to their high computational cost, stochastic nature, and poor formal analyzability. In this paper, a generic model, Agentic Fog (AF), is presented, in which fog nodes are represented as policy-driven autonomous agents that communicate via p2p interactions based on shared memory and localized coordination. The suggested architecture decomposes a system's goals into abstract policy guidance and formalizes decentralized fog coordination as an exact potential game. The framework is guaranteed to converge and remain stable under asynchronous updates, bounded-rational best-response dynamics, and node failures. Simulations demonstrate that the AF system achieves lower average latency and adapts more efficiently to varying demand than greedy heuristics and integer linear programming under dynamic conditions. The sensitivity analysis also demonstrates the capability to perform optimally under different memory and coordination conditions.

</details>


### [22] [Graph-Structured Deep Learning Framework for Multi-task Contention Identification with High-dimensional Metrics](https://arxiv.org/abs/2601.20389)
*Xiao Yang,Yinan Ni,Yuqi Tang,Zhimin Qiu,Chen Wang,Tingzhou Yuan*

Main category: cs.DC

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: This study addresses the challenge of accurately identifying multi-task contention types in high-dimensional system environments and proposes a unified contention classification framework that integrates representation transformation, structural modeling, and a task decoupling mechanism. The method first constructs system state representations from high-dimensional metric sequences, applies nonlinear transformations to extract cross-dimensional dynamic features, and integrates multiple source information such as resource utilization, scheduling behavior, and task load variations within a shared representation space. It then introduces a graph-based modeling mechanism to capture latent dependencies among metrics, allowing the model to learn competitive propagation patterns and structural interference across resource links. On this basis, task-specific mapping structures are designed to model the differences among contention types and enhance the classifier's ability to distinguish multiple contention patterns. To achieve stable performance, the method employs an adaptive multi-task loss weighting strategy that balances shared feature learning with task-specific feature extraction and generates final contention predictions through a standardized inference process. Experiments conducted on a public system trace dataset demonstrate advantages in accuracy, recall, precision, and F1, and sensitivity analyses on batch size, training sample scale, and metric dimensionality further confirm the model's stability and applicability. The study shows that structured representations and multi-task classification based on high-dimensional metrics can significantly improve contention pattern recognition and offer a reliable technical approach for performance management in complex computing environments.

</details>


### [23] [Meeting SLOs, Slashing Hours: Automated Enterprise LLM Optimization with OptiKIT](https://arxiv.org/abs/2601.20408)
*Nicholas Santavas,Kareem Eissa,Patrycja Cieplicka,Piotr Florek,Matteo Nulli,Stefan Vasilev,Seyyed Hadi Hashemi,Antonios Gasteratos,Shahram Khadivi*

Main category: cs.DC

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Enterprise LLM deployment faces a critical scalability challenge: organizations must optimize models systematically to scale AI initiatives within constrained compute budgets, yet the specialized expertise required for manual optimization remains a niche and scarce skillset. This challenge is particularly evident in managing GPU utilization across heterogeneous infrastructure while enabling teams with diverse workloads and limited LLM optimization experience to deploy models efficiently.
  We present OptiKIT, a distributed LLM optimization framework that democratizes model compression and tuning by automating complex optimization workflows for non-expert teams. OptiKIT provides dynamic resource allocation, staged pipeline execution with automatic cleanup, and seamless enterprise integration.
  In production, it delivers more than 2x GPU throughput improvement while empowering application teams to achieve consistent performance improvements without deep LLM optimization expertise. We share both the platform design and key engineering insights into resource allocation algorithms, pipeline orchestration, and integration patterns that enable large-scale, production-grade democratization of model optimization. Finally, we open-source the system to enable external contributions and broader reproducibility.

</details>


### [24] [Rethinking Thread Scheduling under Oversubscription: A User-Space Framework for Coordinating Multi-runtime and Multi-process Workloads](https://arxiv.org/abs/2601.20435)
*Aleix Roca,Vicenç Beltran*

Main category: cs.DC

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: The convergence of high-performance computing (HPC) and artificial intelligence (AI) is driving the emergence of increasingly complex parallel applications and workloads. These workloads often combine multiple parallel runtimes within the same application or across co-located jobs, creating scheduling demands that place significant stress on traditional OS schedulers. When oversubscribed (there are more ready threads than cores), OS schedulers rely on periodic preemptions to multiplex cores, often introducing interference that may degrade performance. In this paper, we present: (1) The User-space Scheduling Framework (USF), a novel seamless process scheduling framework completely implemented in user-space. USF enables users to implement their own process scheduling algorithms without requiring special permissions. We evaluate USF with its default cooperative policy, (2) SCHED_COOP, designed to reduce interference by switching threads only upon blocking. This approach mitigates well-known issues such as Lock-Holder Preemption (LHP), Lock-Waiter Preemption (LWP), and scalability collapse. We implement USF and SCHED_COOP by extending the GNU C library with the nOS-V runtime, enabling seamless coordination across multiple runtimes (e.g., OpenMP) without requiring invasive application changes. Evaluations show gains up to 2.4x in oversubscribed multi-process scenarios, including nested BLAS workloads, multi-process PyTorch inference with LLaMA-3, and Molecular Dynamics (MD) simulations.

</details>


### [25] [AutoOverlap: Enabling Fine-Grained Overlap of Computation and Communication with Chunk-Based Scheduling](https://arxiv.org/abs/2601.20595)
*Xinwei Qiang,Yue Guan,Zhengding Hu,Yufei Ding,Adnan Aziz*

Main category: cs.DC

TL;DR: 论文提出AutoOverlap编译器框架，通过在单一融合内核中实现细粒度通信重叠，解决大规模GPU工作负载通信瓶颈问题。


<details>
  <summary>Details</summary>
Motivation: 现有分布式编译器使用粗粒度流级重叠导致额外内核启动、设备级同步及尾部空闲等问题，因无法细粒度协调计算与通信而影响性能。

Method: 引入通信块抽象解耦通信粒度与内核结构，支持从现有编译器移植块级计划、用户自定义或模板实例化，通过编译转换实现计算与块级通信的细粒度重叠。

Result: 在Triton上实现源码到源码编译，多GPU工作负载平均加速1.3倍，最高达4.7倍。

Conclusion: AutoOverlap验证了细粒度重叠在降低通信开销中的有效性，为分布式GPU优化提供了新方向。

Abstract: Communication has become a first-order bottleneck in large-cale GPU workloads, and existing distributed compilers address it mainly by overlapping whole compute and communication kernels at the stream level. This coarse granularity incurs extra kernel launches, forces device-wide synchronizations at kernel boundaries, and leaves substantial slack when the slowest tile or kernel stretches the communication tail. We present AutoOverlap, a compiler and runtime that enables automatic fine-grained overlap inside a single fused kernel. AutoOverlap introduces a communication chunk abstraction that decouples communication granularity from kernel structure and backend mechanisms, allowing chunk-level plans to be ported from existing distributed compilers, written directly by users, or instantiated from reusable templates. Given a local Triton kernel and a chunk schedule, AutoOverlap performs transformations to align computation with chunk availability. Implemented as a source-to-source compiler on Triton, AutoOverlap delivers an average end-to-end speedup of 1.3$\times$ and up to 4.7$\times$ on multi-GPU workloads.

</details>


### [26] [OnePiece: A Large-Scale Distributed Inference System with RDMA for Complex AI-Generated Content (AIGC) Workflows](https://arxiv.org/abs/2601.20655)
*June Chen,Neal Xu,Gragas Huang,Bok Zhou,Stephen Liu*

Main category: cs.DC

TL;DR: 本文介绍了针对AI生成内容（AIGC）系统效率问题设计的分布式推理框架OnePiece，通过RDMA优化和动态资源管理显著提升性能与资源利用率。


<details>
  <summary>Details</summary>
Motivation: 现有AIGC系统存在吞吐量低、资源利用率不足及并发可扩展性差的关键缺陷，亟需高效解决方案。

Method: 将工作流拆分为微服务级流水线；采用单边RDMA通信降低节点延迟与CPU负载；创新双环缓冲区避免RDMA死锁；动态节点管理器实现实时弹性资源分配。

Result: 在Wan2.1图像转视频任务中，GPU资源消耗较传统方案减少16倍，同时系统具备高可扩展性与容错能力。

Conclusion: OnePiece为生产级AIGC环境提供了高效、可扩展且稳健的分布式推理解决方案。

Abstract: The rapid growth of AI-generated content (AIGC) has enabled high-quality creative production across diverse domains, yet existing systems face critical inefficiencies in throughput, resource utilization, and scalability under concurrent workloads. This paper introduces OnePiece, a large-scale distributed inference system with RDMA optimized for multi-stage AIGC workflows. By decomposing pipelines into fine-grained microservices and leveraging one-sided RDMA communication, OnePiece significantly reduces inter-node latency and CPU overhead while improving GPU utilization. The system incorporates a novel double-ring buffer design to resolve deadlocks in RDMA-aware memory access without CPU involvement. Additionally, a dynamic Node Manager allocates resources elastically across workflow stages in response to real-time load. Experimental results demonstrate that OnePiece reduces GPU resource consumption by 16x in Wan2.1 image-to-video generation compared to monolithic inference pipelines, offering a scalable, fault-tolerant, and efficient solution for production AIGC environments.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [27] [Flexible Bit-Truncation Memory for Approximate Applications on the Edge](https://arxiv.org/abs/2601.19900)
*William Oswald,Mario Renteria-Pinon,Md. Sajjad Hossain,Kyle Mooney,Md. Bipul Hossain,Destinie Diggs,Yiwen Xu,Mohamed Shaban,Jinhui Wang,Na Gong*

Main category: cs.AR

TL;DR: 开发全自适应位截断存储器，通过在运行动态截断任意数据位满足多类近似应用的质量-功耗权衡需求


<details>
  <summary>Details</summary>
Motivation: 现有解决方案需根据特定应用定制设计，缺乏跨场景的灵活适应性，阻碍边缘设备部署

Method: 提出弹性硬件架构实现运行时任意位数截断，并应用于视频处理（亮度/内容/兴趣区域感知）及轻量深度学习模型

Result: 视频应用节能最高47.02%，轻量/裁剪深度学习模型节能达51.69%，仅增加2.89%芯片面积开销

Conclusion: 该设计以低成本实现全域适应性，为边缘环境下近似应用提供高效能存储解决方案

Abstract: Bit truncation has demonstrated great potential to enable run-time quality-power adaptive data storage, thereby optimizing the power/energy efficiency of approximate applications and supporting their deployment in edge environments. However, existing bit-truncation memories require custom designs for a specific application. In this paper, we present a novel bit-truncation memory with full adaptation flexibility, which can truncate any number of data bits at run time to meet different quality and power trade-off requirements for various approximate applications. The developed bit-truncation memory has been applied to two representative data-intensive approximate applications: video processing and deep learning. Our experiments show that the proposed memory can support three different video applications (including luminance-aware, content-aware, and region-of-interest-aware) with enhanced power efficiency (up to 47.02% power savings) as compared to state-of-the-art. In addition, the proposed memory achieves significant (up to 51.69%) power savings for both baseline and pruned lightweight deep learning models, respectively, with a low implementation cost (2.89% silicon area overhead).

</details>


### [28] [A Flower-Inspired Solution for Computer Memory Wear-Leveling](https://arxiv.org/abs/2601.19902)
*Elizabeth Shen,Huiyang Zhou*

Main category: cs.AR

TL;DR: 摘要：研究提出双环磨损均衡方法，基于黄金比例启发，无需硬件改动或性能损失，有效延长内存寿命。


<details>
  <summary>Details</summary>
Motivation: 动机：计算机内存寿命延长对减缓电子废物和支持可持续性至关重要。不同磨损是主要挑战，新兴内存如相变内存寿命更短，问题加剧。现有方案需复杂硬件或仅适用于特定程序结构（如循环）。

Method: 方法：灵感来自自然黄金比例（如花瓣均匀受光），内存建模为两个环，结合现有内存管理和垃圾回收技术，实现双环磨损均衡。

Result: 结果：有效减少内存磨损，延长使用寿命；确定性系统，自适应内存大小，无需硬件更改，程序执行无减速。

Conclusion: 结论：该方案为内存寿命延伸提供高效实用途径，具备应用潜力。

Abstract: Lengthening a computer memory's lifespan is important for e-waste and sustainability. Uneven wear of memory is a major barrier. The problem is becoming even more urgent as emerging memory such as phase-change memory is subject to even shorter lifespan. Various solutions have been proposed, but they either require complicated hardware extensions or apply only to certain program constructs such as loops. This research proposes a new method, dual-ring wear leveling. It takes inspiration from the natural law known as the ``golden ratio" and how it helps flower petals evenly receive sun lights. By modeling memory as two rings and combines the idea with existing memory management, garbage collection, the new solution offers an effective way to reduce memory wear and hence lengthen memory lifespan. It is deterministic, able to automatically adapt to memory size, requiring no hardware changes, and adding no slowdown to program executions.

</details>


### [29] [STELLAR: Structure-guided LLM Assertion Retrieval and Generation for Formal Verification](https://arxiv.org/abs/2601.19903)
*Saeid Rajabi,Chengmo Yang,Satwik Patnaik*

Main category: cs.AR

TL;DR: 本文提出STELLAR框架，利用结构相似性引导LLM生成高质量SystemVerilog断言，提升形式化验证效率。


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: 将RTL模块表示为AST结构指纹，从知识库检索结构相似的(RTL,SVA)对，构建结构引导提示。

Result: 实验证明在语法正确性、风格一致性和功能正确性上 adoptive优越性能。

Conclusion: Conclusion extraction failed

Abstract: Formal Verification (FV) relies on high-quality SystemVerilog Assertions (SVAs), but the manual writing process is slow and error-prone. Existing LLM-based approaches either generate assertions from scratch or ignore structural patterns in hardware designs and expert-crafted assertions. This paper presents STELLAR, the first framework that guides LLM-based SVA generation with structural similarity. STELLAR represents RTL blocks as AST structural fingerprints, retrieves structurally relevant (RTL, SVA) pairs from a knowledge base, and integrates them into structure-guided prompts. Experiments show that STELLAR achieves superior syntax correctness, stylistic alignment, and functional correctness, highlighting structure-aware retrieval as a promising direction for industrial FV.

</details>


### [30] [DABench-LLM: Standardized and In-Depth Benchmarking of Post-Moore Dataflow AI Accelerators for LLMs](https://arxiv.org/abs/2601.19904)
*Ziyu Hu,Zhiqing Zhong,Weijian Zheng,Zhijing Ye,Xuwei Tan,Xueru Zhang,Zheng Xie,Rajkumar Kettimuthu,Xiaodong Yu*

Main category: cs.AR

TL;DR: 介绍DABench-LLM基准框架，用于评估数据流加速器上的大语言模型训练性能。


<details>
  <summary>Details</summary>
Motivation: 摩尔定律放缓导致传统CPU/GPU无法承受大语言模型增长的速度，而数据流加速器缺乏标准基准方法，需开发工具以提高性能分析效率。

Method: Method extraction failed

Result: 在Cerebras WSE-2、SambaNova RDU和Graphcore IPU加速器上验证，揭示性能瓶颈并提供优化策略，证明框架通用有效。

Conclusion: DABench-LLM适用于多种数据流AI硬件平台，辅助研究人员优化性能，推动技术发展。

Abstract: The exponential growth of large language models has outpaced the capabilities of traditional CPU and GPU architectures due to the slowdown of Moore's Law. Dataflow AI accelerators present a promising alternative; however, there remains a lack of in-depth performance analysis and standardized benchmarking methodologies for LLM training. We introduce DABench-LLM, the first benchmarking framework designed for evaluating LLM workloads on dataflow-based accelerators. By combining intra-chip performance profiling and inter-chip scalability analysis, DABench-LLM enables comprehensive evaluation across key metrics such as resource allocation, load balance, and resource efficiency. The framework helps researchers rapidly gain insights into underlying hardware and system behaviors, and provides guidance for performance optimizations. We validate DABench-LLM on three commodity dataflow accelerators, Cerebras WSE-2, SambaNova RDU, and Graphcore IPU. Our framework reveals performance bottlenecks and provides specific optimization strategies, demonstrating its generality and effectiveness across a diverse range of dataflow-based AI hardware platforms.

</details>


### [31] [Hardware-Aware Model Design and Training of Silicon-based Analog Neural Networks](https://arxiv.org/abs/2601.19905)
*Giulio Filippeschi,Mirko Brazzini,Cristhopher Mosquera,Marco Lanuzza,Alessandro Catania,Sebastiano Strangio,Giuseppe Iannaccone*

Main category: cs.AR

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Silicon-based analog neural networks physically embody the ideal neural network model in an approximate way. We show that by retraining the neural network using a physics-informed hardware-aware model one can fully recover the inference accuracy of the ideal network model even in the presence of significant non-idealities. This is way more promising for scalability and integration density than the default option of improving the fidelity of the analog neural network at the cost of significant energy, area, and design overhead, through extensive calibration and conservative analog design.
  We first present a physics-informed hardware-aware model for a time-domain vector-matrix multiplier implemented with single-transistor floating-gate memory cells that explicitly accounts for two dominant non-idealities of the physical implementation - capacitive crosstalk and bit-line voltage drop - and integrates seamlessly with modern deep-learning workflows. The model discretizes each operation into adaptive time slots, processes activation patterns in parallel, and accumulates their contributions to predict effective multiplier outputs. Using measurements from a 16x16 silicon array, we calibrate the model, show that crosstalk is layout-dependent and often dominant, and introduce an improved weight-extraction procedure that doubles signal-to-error ratio versus an ideal vector-matrix multiplier model. Finally, we show that by training silicon-based analog neural networks using an hardware-aware model in the forward pass we can recover the accuracy of the ideal software networks across three architectures -- custom MLP on low-resolution MNIST, LeNet-5 on MNIST, and a VGG-style CNN on CIFAR-10 - establishing a complete design-to-deployment workflow for time-domain analog neuromorphic chips.

</details>


### [32] [GTAC: A Generative Transformer for Approximate Circuits](https://arxiv.org/abs/2601.19906)
*Jingxin Wang,Shitong Guo,Ruicheng Dai,Wenhui Liang,Ruogu Ding,Xin Ning,Weikang Qian*

Main category: cs.AR

TL;DR: GTAC是一种基于Transformer的生成模型，用于设计近似电路，优化性能、功耗和面积（PPA）。


<details>
  <summary>Details</summary>
Motivation: 针对容忍错误的应用程序，引入受控错误以提升电路PPA，并利用AI驱动EDA与传统近似计算相结合的方法。

Method: 采用生成Transformer架构，将误差阈值整合到设计流程中，基于AI驱动的EDA原理实现端到端优化。

Result: 实验结果显示，GTAC在误差率约束下比现有先进方法减少6.4%面积，速度提升4.3倍。

Conclusion: GTAC在PPA优化方面表现优异，为近似计算提供了高效、快速的解决方案。

Abstract: Targeting error-tolerant applications, approximate circuits introduce controlled errors to significantly improve performance, power, and area (PPA) of circuits. In this work, we introduce GTAC, a novel generative Transformer-based model for producing approximate circuits. By leveraging principles of approximate computing and AI-driven EDA, our model innovatively integrates error thresholds into the design process. Experimental results show that compared with a state-of-the-art method, GTAC further reduces 6.4% area under the error rate constraint, while being 4.3x faster.

</details>


### [33] [RAPID-Graph: Recursive All-Pairs Shortest Paths Using Processing-in-Memory for Dynamic Programming on Graphs](https://arxiv.org/abs/2601.19907)
*Yanru Chen,Zheyu Li,Keming Fan,Runyang Tian,John Hsu,Weihong Xu,Minxuan Zhou,Tajana Rosing*

Main category: cs.AR

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: All-pairs shortest paths (APSP) remains a major bottleneck for large-scale graph analytics, as data movement with cubic complexity overwhelms the bandwidth of conventional memory hierarchies. In this work, we propose RAPID-Graph to address this challenge through a co-designed processing-in-memory (PIM) system that integrates algorithm, architecture, and device-level optimizations. At the algorithm level, we introduce a recursion-aware partitioner that enables an exact APSP computation by decomposing graphs into vertex tiles to reduce data dependency, such that both Floyd-Warshall and Min-Plus kernels execute fully in-place within digital PIM arrays. At the architecture and device levels, we design a 2.5D PIM stack integrating two phase-change memory compute dies, a logic die, and high-bandwidth scratchpad memory within a unified advanced package. An external non-volatile storage stack stores large APSP results persistently. The design achieves both tile-level and unit-level parallel processing to sustain high throughput. On the 2.45M-node OGBN-Products dataset, RAPID-Graph is 5.8x faster and 1,186x more energy efficient than state-of-the-art GPU clusters, while exceeding prior PIM accelerators by 8.3x in speed and 104x in efficiency. It further delivers up to 42.8x speedup and 392x energy savings over an NVIDIA H100 GPU.

</details>


### [34] [CHIME: Chiplet-based Heterogeneous Near-Memory Acceleration for Edge Multimodal LLM Inference](https://arxiv.org/abs/2601.19908)
*Yanru Chen,Runyang Tian,Yue Pan,Zheyu Li,Weihong Xu,Tajana Rosing*

Main category: cs.AR

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: The proliferation of large language models (LLMs) is accelerating the integration of multimodal assistants into edge devices, where inference is executed under stringent latency and energy constraints, often exacerbated by intermittent connectivity. These challenges become particularly acute in the context of multimodal LLMs (MLLMs), as high-dimensional visual inputs are transformed into extensive token sequences, thereby inflating the key-value (KV) cache and imposing substantial data movement overheads to the LLM backbone. To address these issues, we present CHIME, a chiplet-based heterogeneous near-memory acceleration for edge MLLMs inference. CHIME leverages the complementary strengths of integrated monolithic 3D (M3D) DRAM and RRAM chiplets: DRAM supplies low-latency bandwidth for attention, while RRAM offers dense, non-volatile storage for weights. This heterogeneous hardware is orchestrated by a co-designed mapping framework that executes fused kernels near data, minimizing cross-chiplet traffic to maximize effective bandwidth. On FastVLM (0.6B/1.7B) and MobileVLM (1.7B/3B), CHIME achieves up to 54x speedup and up to 246x better energy efficiency per inference as compared to the edge GPU NVIDIA Jetson Orin NX. It sustains 116.5-266.5 token/J compared to Jetson's 0.7-1.1 token/J. Furthermore, it delivers up to 69.2x higher throughput than the state-of-the-art PIM accelerator FACIL. Compared to the M3D DRAM-only design, CHIME's heterogeneous memory further improves energy efficiency by 7% and performance by 2.4x.

</details>


### [35] [Understanding Bottlenecks for Efficiently Serving LLM Inference With KV Offloading](https://arxiv.org/abs/2601.19910)
*William Meng,Benjamin Lee,Hong Wang*

Main category: cs.AR

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: KV cache offloading enables long-context LLM inference by storing caches in CPU DRAM, but PCIe bandwidth limitations create severe bottlenecks. In this paper, we develops an analytical framework that derives $κ_{\text{crit}}$, the critical cached-to-prefill token ratio where execution becomes memory-bound and show typical workloads exceed this threshold by orders of magnitude. Empirical characterization reveals 99\% of latency spent on transfers and serving offloaded requests results in GPU's consuming only 28\% of their rated TDP, motivating our proposed optimizations for hardware interconnects, model architectures, and scheduling algorithms.

</details>


### [36] [Analysis of LLM Vulnerability to GPU Soft Errors: An Instruction-Level Fault Injection Study](https://arxiv.org/abs/2601.19912)
*Duo Chai,Zizhen Liu,Shuhuai Wang,Songwei Pei,Cheng Liu,Huawei Li,Shangguang Wang*

Main category: cs.AR

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Large language models (LLMs) are highly compute- and memory-intensive, posing significant demands on high-performance GPUs. At the same time, advances in GPU technology driven by shrinking transistor sizes and lower operating voltages have made these devices increasingly susceptible to soft errors. While prior work has examined GPU reliability, most studies have focused on general-purpose applications or conventional neural networks mostly used for vision tasks such as classification and detection. In contrast, systematic analysis of modern large-scale LLMs remains limited, despite their rapid adoption in diverse application scenarios. Given the unique characteristics of LLMs, their resilience to soft errors may differ substantially from earlier models. To bridge this gap, we conduct the first instruction-level fault injection study of LLM inference. Our approach reveals reliability characteristics from multiple perspectives, highlighting the effects of model architecture, parameter scale, and task complexity. These findings provide new insights into LLM reliability and inform the design of more effective fault tolerance mechanisms.

</details>


### [37] [Bench4HLS: End-to-End Evaluation of LLMs in High-Level Synthesis Code Generation](https://arxiv.org/abs/2601.19941)
*M Zafir Sadik Khan,Kimia Azar,Hadi Kamali*

Main category: cs.AR

TL;DR: 概述：介绍Bench4HLS框架，用于评估LLM生成的高级综合设计，填补LLMs在HLS应用中的评估空白。


<details>
  <summary>Details</summary>
Motivation: 动机：LLMs在高层设计入口兴趣增长，研究比例从RTL转向HLS，需专用基准测试框架以支持优化。

Method: 方法：构建含170个案例的数据集，自动化编译检查、功能仿真和合成可行性评估，集成可扩展PPA分析API，适配多工具链。

Result: 结果：Bench4HLS提供结构化和即插即用的测试平台，成功演示于Vitis和Catapult HLS。

Conclusion: 结论：确立LLM在HLS工作流中的基础评测方法论。

Abstract: In last two years, large language models (LLMs) have shown strong capabilities in code generation, including hardware design at register-transfer level (RTL). While their use in high-level synthesis (HLS) remains comparatively less mature, the ratio of HLS- to RTL-focused studies has shifted from 1:10 to 2:10 in the past six months, indicating growing interest in leveraging LLMs for high-level design entry while relying on downstream synthesis for optimization. This growing trend highlights the need for a comprehensive benchmarking and evaluation framework dedicated to LLM-based HLS. To address this, We present Bench4HLS for evaluating LLM-generated HLS designs. Bench4HLS comprises 170 manually drafted and validated case studies, spanning small kernels to complex accelerators, curated from widely used public repositories. The framework supports fully automated assessment of compilation success, functional correctness via simulation, and synthesis feasibility/optimization. Crucially, Bench4HLS integrates a pluggable API for power, performance, and area (PPA) analysis across various HLS toolchains and architectures, demonstrated here with Xilinx Vitis HLS and validated on Catapult HLS. By providing a structured, extensible, and plug-and-play testbed, Bench4HLS establishes a foundational methodology for benchmarking LLMs in HLS workflows.

</details>


### [38] [A Paradigm for Generalized Multi-Level Priority Encoders](https://arxiv.org/abs/2601.20067)
*Maxwell Phillips,Firas Hassan,Ahmed Ammar*

Main category: cs.AR

TL;DR: 论文提出了通过级联和组合技术将两级优先编码器扩展到三到四级的新方法，并分析其在FPGA和ASIC上的复杂度与延迟表现，发现两级结构能平衡复杂度降低与延迟增加，并提供最优设计工具包。


<details>
  <summary>Details</summary>
Motivation: 高比特精度（如512位以上）优先编码器的复杂度高，限制了其在高速整数运算和内容寻址存储}\)等应用中的性能；通过减少复杂度，可加速硬件系统，提升效率。

Method: 推广已有的两级优先编码器结构，使用级联和组合技术发展为三到四级架构，分析FPGA和ASIC实现的复杂度与延迟随输入长度的变化，并与单级、树状、递归设计进行比较。

Result: 两级架构可将复杂度减少约50%，但延迟相应增加；更高级别收益递减。树状和递归设计速度更快但复杂度较高，输入长度影响设计优化方向。

Conclusion: 基于复杂度或延迟优先级，为不同输入长度和技术提供架构推荐，并提出一个工具包帮助设计师实现最优硬件实现。

Abstract: Priority encoders are typically considered expensive hardware components in terms of complexity, especially at high bit precisions or input lengths (e.g., above 512 bits). However, if the complexity can be reduced, priority encoders can feasibly accelerate a variety of key applications, such as high-precision integer arithmetic and content-addressable memory. We propose a new paradigm for constructing priority encoders by generalizing the previously proposed two-level priority encoder structure. We extend this concept to three and four levels using two techniques -- cascading and composition -- and discuss further generalization. We then analyze the complexity and delay of new and existing priority encoder designs as a function of input length, for both FPGA and ASIC implementation technologies. In particular, we compare the multi-level structure to the traditional single-level priority encoder structure, a tree-based design, a recursive design, and the two-level structure. We find that the two-level architecture provides balanced performance -- reducing complexity by around half, but at the cost of a corresponding increase in delay. Additional levels have diminishing returns, highlighting a tradeoff between complexity and delay. Meanwhile, the tree and recursive designs are generally faster, but are more complex than the two-level and multi-level structures. We explore several characteristics and patterns of the designs across a wide range of input lengths. We then provide recommendations on which architecture to use for a given input length and implementation technology, based on which design factors -- such as complexity or delay -- are most important to the hardware designer. With this overview and analysis of various priority encoder architectures, we provide a priority encoder toolkit to assist hardware designers in creating the most optimal design.

</details>


### [39] [How Much Progress Has There Been in NVIDIA Datacenter GPUs?](https://arxiv.org/abs/2601.20115)
*Emanuele Del Sozzo,Martin Fleming,Kenneth Flamm,Neil Thompson*

Main category: cs.AR

TL;DR: 论文分析2000年代中期至今NVIDIA数据中心GPU的技术演进趋势，量化计算性能、内存带宽等指标倍增周期，并评估美国出口管制对AI芯片性能差距的影响


<details>
  <summary>Details</summary>
Motivation: 研究GPU技术发展对AI领域至关重要，尤其在美国实施高端AI芯片出口管制背景下，评估过往进步可为未来科研约束提供依据

Method: 构建NVIDIA数据中心GPU综合数据集，涵盖计算性能/价格等特征，通过趋势分析计算FP运算、内存带宽、价格及功耗等指标的倍增时间

Result: FP16/FP32运算倍增周期为1.44-1.69年；FP64为2.06-3.79年；内存带宽每3.32-3.53年翻倍；价格倍增周期5.1年，功耗16年；出口管制将使性能差距从23.6倍缩小至3.54倍

Conclusion: GPU算力增长速度远超内存和能耗改进，当前出口管制政策将显著压缩国际性能差距，凸显地缘政治对技术发展路径的影响

Abstract: Graphics Processing Units (GPUs) are the state-of-the-art architecture for essential tasks, ranging from rendering 2D/3D graphics to accelerating workloads in supercomputing centers and, of course, Artificial Intelligence (AI). As GPUs continue improving to satisfy ever-increasing performance demands, analyzing past and current progress becomes paramount in determining future constraints on scientific research. This is particularly compelling in the AI domain, where rapid technological advancements and fierce global competition have led the United States to recently implement export control regulations limiting international access to advanced AI chips. For this reason, this paper studies technical progress in NVIDIA datacenter GPUs released from the mid-2000s until today. Specifically, we compile a comprehensive dataset of datacenter NVIDIA GPUs comprising several features, ranging from computational performance to release price. Then, we examine trends in main GPU features and estimate progress indicators for per-memory bandwidth, per-dollar, and per-watt increase rates. Our main results identify doubling times of 1.44 and 1.69 years for FP16 and FP32 operations (without accounting for sparsity benefits), while FP64 doubling times range from 2.06 to 3.79 years. Off-chip memory size and bandwidth grew at slower rates than computing performance, doubling every 3.32 to 3.53 years. The release prices of datacenter GPUs have roughly doubled every 5.1 years, while their power consumption has approximately doubled every 16 years. Finally, we quantify the potential implications of current U.S. export control regulations in terms of the potential performance gaps that would result if implementation were assumed to be complete and successful. We find that recently proposed changes to export controls would shrink the potential performance gap from 23.6x to 3.54x.

</details>


### [40] [SATA: Sparsity-Aware Scheduling for Selective Token Attention](https://arxiv.org/abs/2601.20267)
*Zhenkun Fan,Zishen Wan,Che-Kai Liu,Ashwin Sanjay Lele,Win-San Khwa,Bo Zhang,Meng-Fan Chang,Arijit Raychowdhury*

Main category: cs.AR

TL;DR: 论文提出SATA动态调度方案，利用选择性令牌注意力和数据局部性优化，提升Transformer硬件的计算效率与能效。


<details>
  <summary>Details</summary>
Motivation: Transformer的注意力机制存在二次计算复杂度问题，导致硬件实现效率低下。选择性令牌注意力可缩小关注范围并降噪，但需解决稀疏访问模式的管理挑战。

Method: 采用局部性驱动的动态调度机制SATA，通过重排操作数流并利用数据局部性，提前获取和处理中间查询/键向量，提高系统利用率。在选择性注意模型的控制计算系统中实现评估。

Result: 实验显示系统吞吐量最高提升1.76倍，能效提升2.94倍，且调度开销极低。

Conclusion: SATA方案高效管理稀疏访问模式，显著提升硬件性能与节能效果，为Transformer优化提供实用解决方案。

Abstract: Transformers have become the foundation of numerous state-of-the-art AI models across diverse domains, thanks to their powerful attention mechanism for modeling long-range dependencies. However, the quadratic scaling complexity of attention poses significant challenges for efficient hardware implementation. While techniques such as quantization and pruning help mitigate this issue, selective token attention offers a promising alternative by narrowing the attention scope to only the most relevant tokens, reducing computation and filtering out noise.
  In this work, we propose SATA, a locality-centric dynamic scheduling scheme that proactively manages sparsely distributed access patterns from selective Query-Key operations. By reordering operand flow and exploiting data locality, our approach enables early fetch and retirement of intermediate Query/Key vectors, improving system utilization. We implement and evaluate our token management strategy in a control and compute system, using runtime traces from selective-attention-based models. Experimental results show that our method improves system throughput by up to 1.76x and boosts energy efficiency by 2.94x, while incurring minimal scheduling overhead.

</details>


### [41] [VersaQ-3D: A Reconfigurable Accelerator Enabling Feed-Forward and Generalizable 3D Reconstruction via Versatile Quantization](https://arxiv.org/abs/2601.20317)
*Yipu Zhang,Jintao Cheng,Xingyu Liu,Zeyu Li,Carol Jingyi Li,Jin Wu,Lin Jiang,Yuan Xie,Jiang Xu,Wei Zhang*

Main category: cs.AR

TL;DR: 提出VersaQ-3D框架解决VGGT模型参数量大、部署难的问题，通过算法-架构协同设计实现高效4比特量化及硬件加速


<details>
  <summary>Details</summary>
Motivation: 现有LLM量化方法因激活饱和和3D语义复杂性无法适配VGGT，且模型存在硬件 للسفر算需求高、注意力机制内存占用大的问题，阻碍在端侧部署

Method: 算法端采用正交变换实现免校准的4位场景无关量化；架构端设计支持多精度（BF16/INT8/INT4）的可重构加速器，统一脉动数据路径处理线性和非线性算子，通过两段式分块技术优化内存

Result: W4A8下保持98-99%精度；W4A4性能比先前方法高1.61-高清.39倍；加速器较边缘GPU快5.2-10.8倍且功耗低

Conclusion: VersaQ-3D首次实现VGGT的高效端侧部署，量化与硬件协同设计为资源受限设备的实时3D重建提供可行方案

Abstract: The Visual Geometry Grounded Transformer (VGGT) enables strong feed-forward 3D reconstruction without per-scene optimization. However, its billion-parameter scale creates high memory and compute demands, hindering on-device deployment. Existing LLM quantization methods fail on VGGT due to saturated activation channels and diverse 3D semantics, which cause unreliable calibration. Furthermore, VGGT presents hardware challenges regarding precision-sensitive nonlinear operators and memory-intensive global attention. To address this, we propose VersaQ-3D, an algorithm-architecture co-design framework. Algorithmically, we introduce the first calibration-free, scene-agnostic quantization for VGGT down to 4-bit, leveraging orthogonal transforms to decorrelate features and suppress outliers. Architecturally, we design a reconfigurable accelerator supporting BF16, INT8, and INT4. A unified systolic datapath handles both linear and nonlinear operators, reducing latency by 60%, while two-stage recomputation-based tiling alleviates memory pressure for long-sequence attention. Evaluations show VersaQ-3D preserves 98-99% accuracy at W4A8. At W4A4, it outperforms prior methods by 1.61x-2.39x across diverse scenes. The accelerator delivers 5.2x-10.8x speedup over edge GPUs with low power, enabling efficient instant 3D reconstruction.

</details>


### [42] [Beyond GEMM-Centric NPUs: Enabling Efficient Diffusion LLM Sampling](https://arxiv.org/abs/2601.20706)
*Binglei Lou,Haoran Wu,Yao Lai,Jiayi Nie,Can Xiao,Xuan Guo,Rika Antonova,Robert Mullins,Aaron Zhao*

Main category: cs.AR

TL;DR: 论文分析了扩散大语言模型(dLLM)采样阶段的性能瓶颈，提出针对NPU架构的优化方案，实现GPU最高2.53倍加速。


<details>
  <summary>Details</summary>
Motivation: dLLM采样阶段占总推理延迟70%，因内存读写、词汇级logit操作和迭代掩盖更新导致不规则内存访问，现有NPU难以高效处理。

Method: 设计轻量级非通用矩阵乘法(non-GEMM)向量算子、原位内存复用策略和分离式混合精度内存体系，并开源了周期精确仿真及RTL验证代码。

Result: 在相同nm工艺节点下，优化后的NPU架构比NVIDIA RTX A6000 GPU实现最高2.53倍加速，并验证与PyTorch实现功能等价。

Conclusion: 定制化NPU架构能显著解决dLLM采样效率问题，为未来大模型推理硬件设计提供有效方向。

Abstract: Diffusion Large Language Models (dLLMs) introduce iterative denoising to enable parallel token generation, but their sampling phase displays fundamentally different characteristics compared to GEMM-centric transformer layers. Profiling on modern GPUs reveals that sampling can account for up to 70% of total model inference latency-primarily due to substantial memory loads and writes from vocabulary-wide logits, reduction-based token selection, and iterative masked updates. These processes demand large on-chip SRAM and involve irregular memory accesses that conventional NPUs struggle to handle efficiently. To address this, we identify a set of critical instructions that an NPU architecture must specifically optimize for dLLM sampling. Our design employs lightweight non-GEMM vector primitives, in-place memory reuse strategies, and a decoupled mixed-precision memory hierarchy. Together, these optimizations deliver up to a 2.53x speedup over the NVIDIA RTX A6000 GPU under an equivalent nm technology node. We also open-source our cycle-accurate simulation and post-synthesis RTL verification code, confirming functional equivalence with current dLLM PyTorch implementations.

</details>


<div id='cs.PF'></div>

# cs.PF [[Back]](#toc)

### [43] [Colored Markov Modulated Fluid Queues](https://arxiv.org/abs/2601.20537)
*Benny Van Houdt*

Main category: cs.PF

TL;DR: 本文提出了带颜色马尔科夫调制流体队列（colored MMFQs）及其含跳跃扩展的新框架，增强了建模灵活性以解决传统MMFQs无法处理的事件记忆问题。


<details>
  <summary>Details</summary>
Motivation: 传统MMFQs无法记录事件时序信息，导致高维系统建模困难，且不支持流体跳跃过程，限制了复杂系统性能分析。

Method: 引入colored MMFQs框架，其中流体颜色跟踪事件发生时的液位状态，并扩展支持流体跳跃功能。

Result: 新框架突破维度诅咒和状态爆炸问题，支持更广泛的排队系统建模，如工作负载动态和能量变化分析。

Conclusion: colored MMFQs显著提升马尔科夫流体队列的建模能力，为复杂通信系统提供高效分析工具。

Abstract: Markov-modulated fluid queues (MMFQs) are a powerful modeling framework for analyzing the performance of computer and communication systems. Their distinguishing feature is that the underlying Markov process evolves on a continuous state space, making them well suited to capture the dynamics of workloads, energy levels, and other performance-related quantities. Although classical MMFQs do not permit jumps in the fluid level, they can still be applied to analyze a wide range of jump processes.
  In this paper, we generalize the MMFQ framework in a new direction by introducing {\bf colored MMFQs} and {\bf colored MMFQs with fluid jumps}. This enriched framework provides an additional form of memory: the color of incoming fluid can be used to keep track of the fluid level when certain events took place. This capability greatly enhances modeling flexibility and enables the analysis of queueing systems that would otherwise be intractable due to the curse of dimensionality or state-space explosion.

</details>


### [44] [The Multiserver-Job Stochastic Recurrence Equation for Cloud Computing Performance Evaluation](https://arxiv.org/abs/2601.20653)
*Francois Baccelli,Diletta Olliaro,Marco Ajmone Marsan,Andrea Marin*

Main category: cs.PF

TL;DR: 本文针对 disminuciónFCFS调度下的多服务器作业排队模型(MJQM)，研究了一般独立到达和服务时间的性能，使用随机回归方程和遍历理论进行分析。


<details>
  <summary>Details</summary>
Motivation: 分析MJQM系统的稳定性条件与性能瓶颈，旨在提升系统效率并确定作业输入流对稳定性的影响。

Method: 采用随机回归方程(SREs)和遍历理论证明MJQM SRE的单调性和可分性性质，据此引入两个算法：子完美样本(SPS)绘制算法用于工作负载模拟，GPU并行化加速；另一个算法基于作业统计估计稳定性条件。

Result: 证明了MJQM SRE的可应用Loynes定理扩展，正式定义了稳定性条件；SPS算法通过GPU并行化显著提升评估效率；方法可扩展到具资源类型的复杂MJQM系统。

Conclusion: 该方法提供高效稳定的性能评估工具，且在复杂多资源排队系统中具有良好的扩展性。

Abstract: We study the Multiserver-Job Queuing Model (MJQM) with general independent arrivals and service times under FCFS scheduling, using stochastic recurrence equations (SREs) and ergodic theory. We prove the monotonicity and separability properties of the MJQM SRE, enabling the application of the monotone-separable extension of Loynes' theorem and the formal definition of the MJQM stability condition. Based on these results, we introduce and implement two algorithms: one for drawing sub-perfect samples (SPS) of the system's workload and the second one to estimate the system's stability condition given the statistics of the jobs' input stream. The SPS algorithm allows for a massive GPU parallelization, greatly improving the efficiency of performance metrics evaluation. We also show that this approach extends to more complex systems, including MJQMs with typed resources.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [45] [Game-Theoretic Autonomous Driving: A Graphs of Convex Sets Approach](https://arxiv.org/abs/2601.20054)
*Nikolaj Käfer,Ahmed Khalil,Edward Huynh,Efstathios Bakolas,David Fridovich-Keil*

Main category: cs.MA

TL;DR: IBR-GCS是一种多车自动驾驶框架，整合战略博弈与轨迹规划，利用凸优化求解安全约束下的交互问题。


<details>
  <summary>Details</summary>
Motivation: 多车高速公路驾驶需处理离散连续混合机动和策略交互，现有方法难以高效集成安全约束和全局游戏理论决策。

Method: 基于凸集图（GCS）的迭代最优响应（IBR）；每车根据他车当前策略构建车辆专属图，顶点为车道和时间变化的凸避撞区域，边表动态可行转移；转化为最短路径问题，用凸优化工具求解，无需穷举搜索；顺序更新轨迹收敛至近似广义纳什均衡。

Result: 模拟多车道多车辆场景显示，IBR-GCS生成的轨迹安全可靠，行为策略一致性强。

Conclusion: 该方法结合游戏理论与凸优化，高效解决自动驾驶中的交互规划问题，提升决策可靠性和安全性。

Abstract: Multi-vehicle autonomous driving couples strategic interaction with hybrid (discrete-continuous) maneuver planning under shared safety constraints. We introduce IBR-GCS, an Iterative Best Response (IBR) planning approach based on the Graphs of Convex Sets (GCS) framework that models highway driving as a generalized noncooperative game. IBR-GCS integrates combinatorial maneuver reasoning, trajectory planning, and game-theoretic interaction within a unified framework. The key novelty is a vehicle-specific, strategy-dependent GCS construction. Specifically, at each best-response update, each vehicle builds its own graph conditioned on the current strategies of the other vehicles, with vertices representing lane-specific, time-varying, convex, collision-free regions and edges encoding dynamically feasible transitions. This yields a shortest-path problem in GCS for each best-response step, which admits an efficient convex relaxation that can be solved using convex optimization tools without exhaustive discrete tree search. We then apply an iterative best-response scheme in which vehicles update their trajectories sequentially and provide conditions under which the resulting inexact updates converge to an approximate generalized Nash equilibrium. Simulation results across multi-lane, multi-vehicle scenarios demonstrate that IBR-GCS produces safe trajectories and strategically consistent interactive behaviors.

</details>


### [46] [Interpreting Emergent Extreme Events in Multi-Agent Systems](https://arxiv.org/abs/2601.20538)
*Ling Tang,Jilin Mei,Dongrui Liu,Chen Qian,Dawei Cheng,Jing Shao,Xia Hu*

Main category: cs.MA

TL;DR: 该论文提出首个框架，用于解释大型语言模型驱动的多智能体系统中涌现的极端事件，通过Shapley值Anyway归因行动贡献，并在多场景实验中验证有效性。


<details>
  <summary>Details</summary>
Motivation: iscono理解涌现极端事件的起源对于确保多智能体系统的安全至关重要，因为事件常因黑箱式涌现而被遮蔽。

Method: 改编Shapley值，为不同时间步的智能体行动分配归因分数以量化影响，然后聚合分数以评估时间、智能体和行为维度的风险贡献，并设计特征指标。

Result: 在经济、金融和社会等多样场景的实验中，框架成功解释极端事件，并提供关于涌现现象的通用见解。

Conclusion: 该框架能可靠分析极端事件的特征，增强系统安全性与可预测性，并为未来研究奠定基础。

Abstract: Large language model-powered multi-agent systems have emerged as powerful tools for simulating complex human-like systems. The interactions within these systems often lead to extreme events whose origins remain obscured by the black box of emergence. Interpreting these events is critical for system safety. This paper proposes the first framework for explaining emergent extreme events in multi-agent systems, aiming to answer three fundamental questions: When does the event originate? Who drives it? And what behaviors contribute to it? Specifically, we adapt the Shapley value to faithfully attribute the occurrence of extreme events to each action taken by agents at different time steps, i.e., assigning an attribution score to the action to measure its influence on the event. We then aggregate the attribution scores along the dimensions of time, agent, and behavior to quantify the risk contribution of each dimension. Finally, we design a set of metrics based on these contribution scores to characterize the features of extreme events. Experiments across diverse multi-agent system scenarios (economic, financial, and social) demonstrate the effectiveness of our framework and provide general insights into the emergence of extreme phenomena.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [47] [Proactive SFC Provisioning with Forecast-Driven DRL in Data Centers](https://arxiv.org/abs/2601.20229)
*Parisa Fard Moshiri,Poonam Lohan,Burak Kantarci,Emil Janulewicz*

Main category: cs.NI

TL;DR: 提案混合预测驱动DRL框架优化数据中心VNF放置，提升资源ческими利用率和服务接受率，并降低延迟


<details>
  <summary>Details</summary>
Motivation: 传统静态资源分配因流量需求动态变化导致过载或不足，需改善SFC在数据中心的资源效率和QoS

Method: 采用DRL生成资源利用率数据集，训练深度学习预测模型(ST-GNN, T-GNN, LSTM)，通过Opt UAE优化参数组成集成模型，实现主动决策顾及当前与未来资源

Result: 实验表明：Cloud Gaming和VoIP保持高接受率；延迟敏感服务如AR接受率从30%升至50%, Industry 4.0从30%到45%；E2E延迟降低：VoIP减20.5%、视频流减23.8%、Cloud Gaming减34.8%

Conclusion: 该方法实现更均衡资源分配，减少争用，显著提升数据中心性能与服务质量

Abstract: Service Function Chaining (SFC) requires efficient placement of Virtual Network Functions (VNFs) to satisfy diverse service requirements while maintaining high resource utilization in Data Centers (DCs). Conventional static resource allocation often leads to overprovisioning or underprovisioning due to the dynamic nature of traffic loads and application demands. To address this challenge, we propose a hybrid forecast-driven Deep reinforcement learning (DRL) framework that combines predictive intelligence with SFC provisioning. Specifically, we leverage DRL to generate datasets capturing DC resource utilization and service demands, which are then used to train deep learning forecasting models. Using Optuna-based hyperparameter optimization, the best-performing models, Spatio-Temporal Graph Neural Network, Temporal Graph Neural Network, and Long Short-Term Memory, are combined into an ensemble to enhance stability and accuracy. The ensemble predictions are integrated into the DC selection process, enabling proactive placement decisions that consider both current and future resource availability. Experimental results demonstrate that the proposed method not only sustains high acceptance ratios for resource-intensive services such as Cloud Gaming and VoIP but also significantly improves acceptance ratios for latency-critical categories such as Augmented Reality increases from 30% to 50%, while Industry 4.0 improves from 30% to 45%. Consequently, the prediction-based model achieves significantly lower E2E latencies of 20.5%, 23.8%, and 34.8% reductions for VoIP, Video Streaming, and Cloud Gaming, respectively. This strategy ensures more balanced resource allocation, and reduces contention.

</details>


### [48] [Immersive Volumetric Video Playback: Near-RT Resource Allocation and O-RAN-based Implementation](https://arxiv.org/abs/2601.20625)
*Yao Wen,Luping Xiang,Kun Yang*

Main category: cs.NI

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Immersive volumetric video streaming in extended reality (XR) demands ultra-low motion-to-photon (MTP) latency, which conventional edge-centric architectures struggle to meet due to per-frame computationally intensive rendering tightly coupled with user motion. To address this challenge, we propose an Open Radio Access Network (O-RAN)-integrated playback framework that jointly orchestrates radio, compute, and content resources in near real time (Near-RT) control loop. The system formulates the rendered-pixel ratio as a continuous control variable and jointly optimizes it over the Open Cloud (O-Cloud) compute, gNB transmit power, and bandwidth under a Weber-Fechner quality of experience (QoE) model, explicitly balancing resolution, computation, and latency. A Soft Actor-Critic (SAC) agent with structured action decomposition and QoE-aware reward shaping resolves the resulting high-dimensional control problem. Experiments on a 5G O-RAN testbed and system simulations show that SAC reduces median MTP latency by above $11\%$ and improves both mean QoE and fairness, demonstrating the feasibility of RIC-driven joint radio-compute-content control for scalable, latency-aware immersive streaming.

</details>
