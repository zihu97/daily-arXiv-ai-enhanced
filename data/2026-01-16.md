<div id=toc></div>

# Table of Contents

- [cs.DC](#cs.DC) [Total: 1]
- [cs.AR](#cs.AR) [Total: 2]
- [cs.PF](#cs.PF) [Total: 2]
- [cs.MA](#cs.MA) [Total: 5]
- [cs.SE](#cs.SE) [Total: 15]


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [1] [Distributed Linearly Separable Computation with Arbitrary Heterogeneous Data Assignment](https://arxiv.org/abs/2601.10177)
*Ziting Zhang,Kai Wan,Minquan Cheng,Shuo Shao,Giuseppe Caire*

Main category: cs.DC

TL;DR: 本文研究异构分布式线性可分计算问题，提出通用计算方案与对偶界，刻画任务维度与通信成本的权衡。


<details>
  <summary>Details</summary>
Motivation: 现有研究多假设同构数据分配，本文旨在处理更实际的异构任意数据分配场景。

Method: 通过分析数据分配结构，设计通用计算方案与对偶界，并推广至分数通信成本情形。

Result: 在整数通信成本下方案与对偶界部分参数区间吻合，扩展后亦适用于分数成本。

Conclusion: 揭示了异构环境下线性可分计算任务维度与通信开销的基本权衡关系。

Abstract: Distributed linearly separable computation is a fundamental problem in large-scale distributed systems, requiring the computation of linearly separable functions over different datasets across distributed workers. This paper studies a heterogeneous distributed linearly separable computation problem, including one master and N distributed workers. The linearly separable task function involves Kc linear combinations of K messages, where each message is a function of one dataset. Distinguished from the existing homogeneous settings that assume each worker holds the same number of datasets, where the data assignment is carefully designed and controlled by the data center (e.g., the cyclic assignment), we consider a more general setting with arbitrary heterogeneous data assignment across workers, where `arbitrary' means that the data assignment is given in advance and `heterogeneous' means that the workers may hold different numbers of datasets. Our objective is to characterize the fundamental tradeoff between the computable dimension of the task function and the communication cost under arbitrary heterogeneous data assignment. Under the constraint of integer communication costs, for arbitrary heterogeneous data assignment, we propose a universal computing scheme and a universal converse bound by characterizing the structure of data assignment, where they coincide under some parameter regimes. We then extend the proposed computing scheme and converse bound to the case of fractional communication costs.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [2] [Enhancing LUT-based Deep Neural Networks Inference through Architecture and Connectivity Optimization](https://arxiv.org/abs/2601.09773)
*Binglei Lou,Ruilin Wu,Philip Leong*

Main category: cs.AR

TL;DR: SparseLUT框架通过架构改进与非贪心训练算法，显著降低LUT消耗与推理延迟，同时提升模型精度。


<details>
  <summary>Details</summary>
Motivation: 解决现有LUT-DNN在资源受限设备上面临的LUT规模指数增长与稀疏连接效率低下的问题。

Method: 提出基于加法器聚合子神经元的架构优化，并引入非贪心训练算法动态剪枝与再生神经元连接。

Result: LUT消耗减少2.0x-13.9x，推理延迟降低1.2x-1.6x，MNIST与Jet数据集精度分别提升2.13%与0.94%。

Conclusion: SparseLUT在不增加硬件开销前提下，有效平衡了边缘设备部署DNN时的资源、延迟与精度需求。

Abstract: Deploying deep neural networks (DNNs) on resource-constrained edge devices such as FPGAs requires a careful balance among latency, power, and hardware resource usage, while maintaining high accuracy. Existing Lookup Table (LUT)-based DNNs -- such as LogicNets, PolyLUT, and NeuraLUT -- face two critical challenges: the exponential growth of LUT size and inefficient random sparse connectivity. This paper presents SparseLUT, a comprehensive framework that addresses these challenges through two orthogonal optimizations. First, we propose an architectural enhancement that aggregates multiple PolyLUT sub-neurons via an adder, significantly reducing LUT consumption by 2.0x-13.9x and lowering inference latency by 1.2x-1.6x, all while maintaining comparable accuracy. Building upon this foundation, we further introduce a non-greedy training algorithm that optimizes neuron connectivity by selectively pruning less significant inputs and strategically regrowing more effective ones. This training optimization, which incurs no additional area and latency overhead, delivers consistent accuracy improvements across benchmarks -- achieving up to a 2.13% gain on MNIST and 0.94% on Jet Substructure Classification compared to existing LUT-DNN approaches.

</details>


### [3] [Architectural Classification of XR Workloads: Cross-Layer Archetypes and Implications](https://arxiv.org/abs/2601.10463)
*Xinyu Shi,Simei Yang,Francky Catthoor*

Main category: cs.AR

TL;DR: 本文提出了一种跨层方法对扩展现实（XR）工作负载进行架构分类，提炼出关键架构特征并提供下一代XR系统芯片的设计指南。


<details>
  <summary>Details</summary>
Motivation: XR平台需在严格功耗和面积限制下实现超低延迟性能，但传统以CNN为中心的加速器架构难以应对日益多样化的XR工作负载。

Method: 结合基于模型的高层设计空间探索与商用GPU/CPU硬件上的实证分析，对12个典型XR内核进行跨层架构分类。

Result: 识别出容量受限和开销敏感等跨层工作负载原型，并据此提出相位感知调度与弹性资源分配等关键设计原则。

Conclusion: 未来XR架构设计应从通用资源扩展转向相位感知调度和弹性资源分配，以提升能效与性能。

Abstract: Edge and mobile platforms for augmented and virtual reality, collectively referred to as extended reality (XR) must deliver deterministic ultra-low-latency performance under stringent power and area constraints. However, the diversity of XR workloads is rapidly increasing, characterized by heterogeneous operator types and complex dataflow structures. This trend poses significant challenges to conventional accelerator architectures centered around convolutional neural networks (CNNs), resulting in diminishing returns for traditional compute-centric optimization strategies. Despite the importance of this problem, a systematic architectural understanding of the full XR pipeline remains lacking. In this paper, we present an architectural classification of XR workloads using a cross-layer methodology that integrates model-based high-level design space exploration (DSE) with empirical profiling on commercial GPU and CPU hardware. By analyzing a representative set of workloads spanning 12 distinct XR kernels, we distill their complex architectural characteristics into a small set of cross-layer workload archetypes (e.g., capacity-limited and overhead-sensitive). Building on these archetypes, we further extract key architectural insights and provide actionable design guidelines for next-generation XR SoCs. Our study highlights that XR architecture design must shift from generic resource scaling toward phase-aware scheduling and elastic resource allocation in order to achieve greater energy efficiency and high performance in future XR systems.

</details>


<div id='cs.PF'></div>

# cs.PF [[Back]](#toc)

### [4] [Emergency Department Patient Flow Optimization with an Alternative Care Threshold Policy](https://arxiv.org/abs/2601.10041)
*Sahba Baniasadi,Paul M. Griffin,Prakash Chakraborty*

Main category: cs.PF

TL;DR: 提出基于阈值的急诊科分流策略，通过数学建模优化非紧急患者转诊，显著提升城乡急诊运营效率。


<details>
  <summary>Details</summary>
Motivation: 解决急诊科过度拥挤和患者滞留问题，以提升医疗服务质量与系统效率。

Method: 构建双类别M/M/c抢占优先排队模型，结合状态依赖转诊与QBD过程分析，优化长期平均收益函数。

Result: 农村急诊最优阈值较低，城市急诊在中等阈值表现最佳；政策分别带来最高4.84%和5.90%的性能提升。

Conclusion: 提供兼顾临床优先级与运营效率的数学框架，适用于不同急诊环境的动态决策支持。

Abstract: Emergency department (ED) overcrowding and patient boarding represent critical systemic challenges that compromise care quality. We propose a threshold-based admission policy that redirects non-urgent patients to alternative care pathways, such as telemedicine, during peak congestion. The ED is modeled as a two-class $M/M/c$ preemptive-priority queuing system, where high-acuity patients are prioritized and low-acuity patients are subject to state-dependent redirection. Analyzed via a level-dependent Quasi-Birth-Death (QBD) process, the model determines the optimal threshold by maximizing a long-run time-averaged objective function comprising redirection-affected revenue and costs associated with patient balking and system occupancy. Numerical analysis using national healthcare data reveals that optimal policies are highly context-dependent. While rural EDs generally optimize at lower redirection thresholds, urban EDs exhibit performance peaks at moderate thresholds. Results indicate that our optimal policy yields significant performance gains of up to $4.84\%$ in rural settings and $5.90\%$ in urban environments. This research provides a mathematically rigorous framework for balancing clinical priority with operational efficiency across diverse ED settings.

</details>


### [5] [Long-term Monitoring of Kernel and Hardware Events to Understand Latency Variance](https://arxiv.org/abs/2601.10572)
*Fang Zhou,Yuyang Huang,Miao Yu,Sixiang Ma,Tongping Liu,Yang Wang*

Main category: cs.PF

TL;DR: VarMRI工具链通过长期监控内核和硬件事件，有效分析并降低应用延迟方差，优化后尾部延迟最多减少31%。


<details>
  <summary>Details</summary>
Motivation: 解决应用层难以观测的内核与硬件事件引发的延迟波动问题。

Method: 构建VarMRI工具链，选择性记录影响请求的事件，并采用高效、鲁棒的数据分析方法。

Result: 在CloudLab上完成3000小时实验，识别多种延迟诱因，如中断抢占、Java GC等，优化后尾部延迟显著下降。

Conclusion: 长期监控对准确评估和优化系统延迟至关重要。

Abstract: This paper presents our experience to understand latency variance caused by kernel and hardware events, which are often invisible at the application level. For this purpose, we have built VarMRI, a tool chain to monitor and analyze those events in the long term. To mitigate the "big data" problem caused by long-term monitoring, VarMRI selectively records a subset of events following two principles: it only records events that are affecting the requests recorded by the application; it records coarse-grained information first and records additional information only when necessary. Furthermore, VarMRI introduces an analysis method that is efficient on large amount of data, robust on different data set and against missing data, and informative to the user.
  VarMRI has helped us to carry out a 3,000-hour study of six applications and benchmarks on CloudLab. It reveals a wide variety of events causing latency variance, including interrupt preemption, Java GC, pipeline stall, NUMA balancing etc.; simple optimization or tuning can reduce tail latencies by up to 31%. Furthermore, the impacts of some of these events vary significantly across different experiments, which confirms the necessity of long-term monitoring.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [6] [Multi-Agent Cooperative Learning for Robust Vision-Language Alignment under OOD Concepts](https://arxiv.org/abs/2601.09746)
*Philip Xu,Isabel Wagner,Eerke Boiten*

Main category: cs.MA

TL;DR: 提出多智能体协作学习框架MACL，解决视觉-语言模型在处理分布外概念时的跨模态对齐崩溃问题。


<details>
  <summary>Details</summary>
Motivation: 解决视觉-语言模型在面对OOD概念时因模态不平衡导致的跨模态对齐失效问题。

Method: 构建包含图像、文本、命名与协调四个核心智能体的协作框架，通过结构化消息传递、上下文交换增强的小样本学习算法和自适应动态平衡机制实现特征空间协同优化。

Result: 在VISTA-Beyond数据集上，MACL在小样本和零样本设置下均显著提升性能，多个视觉领域精度提高1-5%。

Conclusion: MACL有效缓解模态失衡，提升模型对OOD概念的跨模态泛化能力。

Abstract: This paper introduces a novel Multi-Agent Cooperative Learning (MACL) framework to address cross-modal alignment collapse in vision-language models when handling out-of-distribution (OOD) concepts. Four core agents, including image, text, name, and coordination agents, collaboratively mitigate modality imbalance through structured message passing. The proposed framework enables multi-agent feature space name learning, incorporates a context exchange enhanced few-shot learning algorithm, and adopts an adaptive dynamic balancing mechanism to regulate inter-agent contributions. Experiments on the VISTA-Beyond dataset demonstrate that MACL significantly improves performance in both few-shot and zero-shot settings, achieving 1-5% precision gains across diverse visual domains.

</details>


### [7] [When Personas Override Payoffs: Role Identity Bias in Multi-Agent LLM Decision-Making](https://arxiv.org/abs/2601.10102)
*Viswonathan Manoranjan,Snehalkumar `Neil' S. Gaikwad*

Main category: cs.MA

TL;DR: 研究揭示角色身份和收益可见性如何影响多智能体系统中的策略推理，发现设计选择会显著改变均衡结果。


<details>
  <summary>Details</summary>
Motivation: 探索多智能体系统中角色设定与收益透明度对策略推理能力的影响机制。

Method: 在四种LLM架构上通过环境决策博弈实验，以纳什均衡达成率评估策略推理能力。

Result: 移除角色设定并提供明确收益可使Qwen模型实现高均衡率；角色存在时均衡偏向社会偏好结果；不同模型对设计参数敏感性差异显著。

Conclusion: 表征设计是实质性的治理决策，决定系统表现为策略推理者或身份驱动者，对实际部署具重要影响。

Abstract: Large language models are increasingly deployed in multi-agent systems for strategic tasks, yet how design choices such as role-based personas and payoff visibility affect reasoning remains poorly understood. We investigate whether multi-agent systems function as strategic reasoners capable of payoff optimization or as identity-driven actors that prioritize role alignment over explicit incentives. Using Nash equilibrium achievement as a diagnostic for strategic reasoning, we conduct systematic experiments across four LLM architectures (Qwen-7B, Qwen-32B, Llama-8B, Mistral-7B) in complex environmental decision-making games involving four agents. We show that role identity bias fundamentally alters strategic reasoning even when payoff-optimal equilibria exist and complete payoff information is available. Removing personas and providing explicit payoffs enables Qwen models to achieve high Nash equilibrium rates, indicating that both conditions are necessary for strategic reasoning. In contrast, personas systematically bias equilibrium selection toward socially preferred outcomes: with personas present, all of the achieved equilibria correspond to Green Transition, while models entirely fail to reach equilibrium when Tragedy of the Commons is payoff-optimal. The effect of explicit payoffs depends entirely on persona presence, revealing strong interactions between representational design choices. We also observe clear model-dependent patterns. Qwen architectures are highly sensitive to both personas and payoff visibility, whereas Llama and Mistral exhibit rigid reasoning behavior across conditions. These findings demonstrate that representational choices are substantive governance decisions that determine whether multi-agent systems act as strategic reasoners or identity-driven actors, with important implications for real-world deployment.

</details>


### [8] [TopoDIM: One-shot Topology Generation of Diverse Interaction Modes for Multi-Agent Systems](https://arxiv.org/abs/2601.10120)
*Rui Sun,Jie Ding,Chenghua Gong,Tianjun Gu,Yihang Jiang,Juyuan Zhang,Liming Pan,Linyuan Lü*

Main category: cs.MA

TL;DR: TopoDIM框架通过一次性生成多样化交互模式的通信拓扑，显著降低多智能体系统中的延迟与计算开销，同时提升任务性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖时空交互范式，导致高延迟和计算成本，而评估与辩论机制可提升问题解决能力。

Method: 提出TopoDIM框架，支持去中心化执行，使智能体自主构建异构通信拓扑，无需迭代协调。

Result: 实验表明，TopoDIM减少46.41%的总令牌消耗，平均性能提升1.50%，并具备强适应性。

Conclusion: TopoDIM在保证隐私与适应性的同时，实现了高效通信与性能提升，适用于异构多智能体系统。

Abstract: Optimizing communication topology in LLM-based multi-agent system is critical for enabling collective intelligence. Existing methods mainly rely on spatio-temporal interaction paradigms, where the sequential execution of multi-round dialogues incurs high latency and computation. Motivated by the recent insights that evaluation and debate mechanisms can improve problem-solving in multi-agent systems, we propose TopoDIM, a framework for one-shot Topology generation with Diverse Interaction Modes. Designed for decentralized execution to enhance adaptability and privacy, TopoDIM enables agents to autonomously construct heterogeneous communication without iterative coordination, achieving token efficiency and improved task performance. Experiments demonstrate that TopoDIM reduces total token consumption by 46.41% while improving average performance by 1.50% over state-of-the-art methods. Moreover, the framework exhibits strong adaptability in organizing communication among heterogeneous agents. Code is available at: https://anonymous.4open.science/r/TopoDIM-8D35/

</details>


### [9] [Fairness Driven Multi-Agent Path Finding Problem](https://arxiv.org/abs/2601.10123)
*Aditi Anand,Dildar Ali,Suman Banerjee*

Main category: cs.MA

TL;DR: 本文研究多智能体路径规划问题，提出针对非理性智能体的启发式解法及针对理性智能体的激励相容机制。


<details>
  <summary>Details</summary>
Motivation: 解决现实场景中多智能体路径冲突问题，并兼顾计算效率与公平性。

Method: 对非理性智能体采用启发式算法；对理性智能体设计满足主导策略、激励相容和个体理性的机制。

Result: 所提方法在多种实验设置下表现出高效性和有效性。

Conclusion: 该研究为多智能体路径规划提供了兼顾公平与效率的解决方案。

Abstract: The Multi-Agent Path Finding (MAPF) problem aims at finding non-conflicting paths for multiple agents from their respective sources to destinations. This problem arises in multiple real-life situations, including robot motion planning and airspace assignment for unmanned aerial vehicle movement. The problem is computationally expensive, and adding to it, the agents are rational and can misreport their private information. In this paper, we study both variants of the problem under the realm of fairness. For the non-rational agents, we propose a heuristic solution for this problem. Considering the agents are rational, we develop a mechanism and demonstrate that it is a dominant strategy, incentive compatible, and individually rational. We employ various solution methodologies to highlight the effectiveness and efficiency of the proposed solution approaches.

</details>


### [10] [Learning Latency-Aware Orchestration for Parallel Multi-Agent Systems](https://arxiv.org/abs/2601.10560)
*Xi Shi,Mengxin Zheng,Qian Lou*

Main category: cs.MA

TL;DR: LAMaS框架通过并行执行与显式优化关键路径，显著降低多智能体系统推理延迟，同时保持任务性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法多假设串行执行，难以在并行环境下有效控制延迟，限制了多智能体系统在实时场景中的扩展性与可用性。

Method: 提出Latency-Aware Multi-agent System (LAMaS)，构建带延迟监督的并行执行拓扑图，优化关键路径长度。

Result: 在多个基准测试中，相比现有最优基线，关键路径长度减少38-46%，且任务性能维持或提升。

Conclusion: 设计高效多智能体系统时，必须显式优化并行执行下的延迟，LAMaS为此提供了有效解决方案。

Abstract: Multi-agent systems (MAS) enable complex reasoning by coordinating multiple agents, but often incur high inference latency due to multi-step execution and repeated model invocations, severely limiting their scalability and usability in time-sensitive scenarios. Most existing approaches primarily optimize task performance and inference cost, and explicitly or implicitly assume sequential execution, making them less optimal for controlling latency under parallel execution. In this work, we investigate learning-based orchestration of multi-agent systems with explicit latency supervision under parallel execution. We propose Latency-Aware Multi-agent System (LAMaS), a latency-aware multi-agent orchestration framework that enables parallel execution and explicitly optimizes the critical execution path, allowing the controller to construct execution topology graphs with lower latency under parallel execution. Our experiments show that our approach reduces critical path length by 38-46% compared to the state-of-the-art baseline for multi-agent architecture search across multiple benchmarks, while maintaining or even improving task performance. These results highlight the importance of explicitly optimizing latency under parallel execution when designing efficient multi-agent systems. The code is available at https://github.com/xishi404/LAMaS

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [11] [Putting green software principles into practice](https://arxiv.org/abs/2601.09741)
*James Uther*

Main category: cs.SE

TL;DR: 本文探讨了在公有云上运行的实际产品中实现绿色软件的实践方法，重点利用无服务器架构的成本效益推动能效提升，并总结了项目中有效的绿色软件原则。


<details>
  <summary>Details</summary>
Motivation: 尽管减少计算系统二氧化碳排放的理论方法已被广泛理解，但实际案例仍较少，因此需要探索可行的实践方案。

Method: 通过分析公有云环境中无服务器系统的成本影响，驱动软件效率优化，从而降低碳排放。

Result: 成功在真实产品中实施绿色软件策略，并提炼出若干行之有效的绿色软件原则。

Conclusion: 结合成本与效率优化的无服务器架构可有效支持绿色软件实践，为行业提供实用参考。

Abstract: The need and theoretical methods for measuring and reducing CO2 emitted by computing systems are well understood, but real-world examples are still limited. We describe a journey towards green software for a live product running on a public cloud. We discuss practical solutions found, in particular using the cost implications of serverless systems to drive efficiency. We end with some `green software' principles that worked well in this project.

</details>


### [12] [A Governance Model for IoT Data in Global Manufacturing](https://arxiv.org/abs/2601.09744)
*Vignesh Alagappan*

Main category: cs.SE

TL;DR: 提出一种适用于全球制造环境中工业物联网数据治理的联邦模型，强调契约驱动互操作性与策略即代码。


<details>
  <summary>Details</summary>
Motivation: 当前企业面临物联网数据规模化治理挑战，源于缺乏适配分布式运营、异构系统和边缘持续变化的治理模型。

Method: 构建以契约驱动互操作、策略即代码执行和资产为中心问责的联邦治理架构。

Result: 实现跨架构边界的语义一致性、质量保障与合规性，无需集中控制运营技术系统。

Conclusion: 提出基于制造物联网需求的系统架构框架，实证验证留待未来工作。

Abstract: Industrial IoT platforms in global manufacturing environments generate continuous operational data across production assets, utilities, and connected products. While data ingestion and storage capabilities have matured significantly, enterprises continue to face systemic challenges in governing IoT data at scale. These challenges are not rooted in tooling limitations but in the absence of a governance model that aligns with the realities of distributed operational ownership, heterogeneous source systems, and continuous change at the edge. This paper presents a federated governance model that emphasizes contract-driven interoperability, policy-as-code enforcement, and asset-centric accountability across global manufacturing organizations. The model addresses governance enforcement at architectural boundaries, enabling semantic consistency, quality assurance, and regulatory compliance without requiring centralized control of operational technology systems. This work contributes a systems architecture and design framework grounded in analysis of manufacturing IoT requirements and constraints; empirical validation remains future work

</details>


### [13] [Enhancing Formal Software Specification with Artificial Intelligence](https://arxiv.org/abs/2601.09745)
*Antonio Abu Nassar,Eitan Farchi*

Main category: cs.SE

TL;DR: 通过结合自然语言与轻量数学符号，并利用AI辅助审查和代码生成，本文展示了一种降低形式化规范成本同时保留其优势的新方法。


<details>
  <summary>Details</summary>
Motivation: 传统形式化语言因高学习成本和符号负担限制了工业应用，亟需更易用的替代方案。

Method: 采用自然语言加轻量数学符号作为中间规范语言，由AI审查优化后生成代码。

Result: 在组织知识增长模拟案例中实现首次正确实施，显著减少开发工作量并确保设计正确性。

Conclusion: AI辅助的形式化规范方法可有效平衡严谨性与实用性，推动形式化方法在工业界落地。

Abstract: Formal software specification is known to enable early error detection and explicit invariants, yet it has seen limited industrial adoption due to its high notation overhead and the expertise required to use traditional formal languages. This paper presents a case study showing that recent advances in artificial intelligence make it possible to retain many of the benefits of formal specification while substantially reducing these costs. The necessity of a clear distinction between what is controlled by the system analyst and can highly benefits from the rigor of formal specification and what need not be controlled is demonstrated. We use natural language augmented with lightweight mathematical notation and written in \LaTeX\ as an intermediate specification language, which is reviewed and refined by AI prior to code generation. Applied to a nontrivial simulation of organizational knowledge growth, this approach enables early validation, explicit invariants, and correctness by design, while significantly reducing development effort and producing a correct implementation on the first attempt.

</details>


### [14] [Investigating Tool-Memory Conflicts in Tool-Augmented LLMs](https://arxiv.org/abs/2601.09760)
*Jiali Cheng,Rui Pan,Hadi Amiri*

Main category: cs.SE

TL;DR: 本文提出工具-记忆冲突（TMC）问题，发现现有大语言模型在STEM任务中易受其影响，且现有解决方法效果不佳。


<details>
  <summary>Details</summary>
Motivation: 解决工具增强型大语言模型中内部参数知识与外部工具知识之间的冲突问题。

Method: 分析工具-记忆冲突现象，评估基于提示和RAG的现有冲突解决技术。

Result: 现有方法无法有效解决工具-记忆冲突，尤其在STEM任务中表现明显。

Conclusion: 需开发更有效的策略以缓解工具-记忆冲突对模型性能的影响。

Abstract: Tool-augmented large language models (LLMs) have powered many applications. However, they are likely to suffer from knowledge conflict. In this paper, we propose a new type of knowledge conflict -- Tool-Memory Conflict (TMC), where the internal parametric knowledge contradicts with the external tool knowledge for tool-augmented LLMs. We find that existing LLMs, though powerful, suffer from TMC, especially on STEM-related tasks. We also uncover that under different conditions, tool knowledge and parametric knowledge may be prioritized differently. We then evaluate existing conflict resolving techniques, including prompting-based and RAG-based methods. Results show that none of these approaches can effectively resolve tool-memory conflicts.

</details>


### [15] [Explicating Tacit Regulatory Knowledge from LLMs to Auto-Formalize Requirements for Compliance Test Case Generation](https://arxiv.org/abs/2601.09762)
*Zhiyi Xue,Xiaohong Chen,Min Zhang*

Main category: cs.SE

TL;DR: RAFT框架通过多LLM提取隐性监管知识，实现需求自动形式化与合规测试生成，显著提升效率与精度。


<details>
  <summary>Details</summary>
Motivation: 解决高度监管领域中合规测试依赖人工、LLM易产生幻觉及现有方法需昂贵手动建模的问题。

Method: 提出RAFT框架，采用自适应净化-聚合策略从多个LLM中提取隐性知识，生成领域元模型、形式化需求与可测性约束，并动态注入提示引导高精度形式化与自动化测试生成。

Result: 在金融、汽车与电力领域实验表明，RAFT达到专家级性能，超越现有最优方法并大幅减少生成与审核时间。

Conclusion: RAFT有效实现合规测试自动化，兼具高精度与高效率，适用于多行业监管场景。

Abstract: Compliance testing in highly regulated domains is crucial but largely manual, requiring domain experts to translate complex regulations into executable test cases. While large language models (LLMs) show promise for automation, their susceptibility to hallucinations limits reliable application. Existing hybrid approaches mitigate this issue by constraining LLMs with formal models, but still rely on costly manual modeling. To solve this problem, this paper proposes RAFT, a framework for requirements auto-formalization and compliance test generation via explicating tacit regulatory knowledge from multiple LLMs. RAFT employs an Adaptive Purification-Aggregation strategy to explicate tacit regulatory knowledge from multiple LLMs and integrate it into three artifacts: a domain meta-model, a formal requirements representation, and testability constraints. These artifacts are then dynamically injected into prompts to guide high-precision requirement formalization and automated test generation. Experiments across financial, automotive, and power domains show that RAFT achieves expert-level performance, substantially outperforms state-of-the-art (SOTA) methods while reducing overall generation and review time.

</details>


### [16] [LLM-Based Agentic Systems for Software Engineering: Challenges and Opportunities](https://arxiv.org/abs/2601.09822)
*Yongjian Tang,Thomas Runkler*

Main category: cs.SE

TL;DR: 本文系统回顾了基于大语言模型的多智能体系统在软件工程中的应用，涵盖软件开发生命周期各阶段，并探讨了未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 复杂软件工程任务需要更协作和专业的方法，以应对大语言模型的局限性。

Method: 系统性文献综述，分析多智能体系统的框架、协议及评估基准。

Result: 识别出多智能体编排、人机协作、成本优化和数据收集等关键挑战与机遇。

Conclusion: 为研究人员和从业者提供当前多智能体系统在软件工程领域的前沿洞察。

Abstract: Despite recent advancements in Large Language Models (LLMs), complex Software Engineering (SE) tasks require more collaborative and specialized approaches. This concept paper systematically reviews the emerging paradigm of LLM-based multi-agent systems, examining their applications across the Software Development Life Cycle (SDLC), from requirements engineering and code generation to static code checking, testing, and debugging. We delve into a wide range of topics such as language model selection, SE evaluation benchmarks, state-of-the-art agentic frameworks and communication protocols. Furthermore, we identify key challenges and outline future research opportunities, with a focus on multi-agent orchestration, human-agent coordination, computational cost optimization, and effective data collection. This work aims to provide researchers and practitioners with valuable insights into the current forefront landscape of agentic systems within the software engineering domain.

</details>


### [17] [Adoption and Evolution of Code Style and Best Programming Practices in Open-Source Projects](https://arxiv.org/abs/2601.09832)
*Alvari Kupari,Nasser Giacaman,Valerio Terragni*

Main category: cs.SE

TL;DR: 该论文分析了1036个开源JAVA项目，研究代码风格和编程实践的采用情况及演变趋势，发现Javadoc和命名违规最常见，并提出改进方向。


<details>
  <summary>Details</summary>
Motivation: 提升代码质量、可维护性和减少错误，通过研究开源项目中代码风格和最佳实践的遵循情况，找出改进空间。

Method: 对GitHub上1036个流行JAVA项目进行静态分析，按月跟踪部分活跃仓库的代码规范遵守变化。

Result: 普遍存在违反代码规范的情况，尤其是Javadoc和命名；声称遵循规范的项目表现略优；Google Java Style Guide相关违规常被工具忽略。

Conclusion: 研究揭示了开源社区在代码风格实践中的主要问题，为未来提升JAVA项目代码质量提供了洞见与建议。

Abstract: Following code style conventions in software projects is essential for maintaining overall code quality. Adhering to these conventions improves maintainability, understandability, and extensibility. Additionally, following best practices during software development enhances performance and reduces the likelihood of errors. This paper analyzes 1,036 popular open-source JAVA projects on GITHUB to study how code style and programming practices are adopted and evolve over time, examining their prevalence and the most common violations. Additionally, we study a subset of active repositories on a monthly basis to track changes in adherence to coding standards over time. We found widespread violations across repositories, with Javadoc and Naming violations being the most common. We also found a significant number of violations of the GOOGLE Java Style Guide in categories often missed by modern static analysis tools. Furthermore, repositories claiming to follow code-style practices exhibited slightly higher overall adherence to code-style and best-practices. The results provide valuable insights into the adoption of code style and programming practices, highlighting key areas for improvement in the open-source development community. Furthermore, the paper identifies important lessons learned and suggests future directions for improving code quality in JAVA projects.

</details>


### [18] [On Fun for Teaching Large Programming Courses](https://arxiv.org/abs/2601.09842)
*Walid Maalej*

Main category: cs.SE

TL;DR: 本文提出了一套包含十个趣味物理活动的教学方法，旨在帮助大规模课堂中的学生理解基础编程与软件开发概念，并通过三年实践与访谈验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 解决大规模授课中学生易分心、互动少、教学效果差的问题，同时提升学生对抽象概念的记忆与反思能力。

Method: 设计并实施十种物理趣味活动（如人浪算法、纸飞机模拟消息传递、教室空间模拟树结构等），并在500+学生课程中连续三年应用，辅以15名学生和14名教育者的访谈研究。

Result: 活动能有效维持学生专注度、促进关键概念记忆与课后反思，但需保持简洁并与教学内容紧密关联才能发挥最佳效果。

Conclusion: 物理趣味活动是大规模编程教学中提升参与度与学习效果的有效辅助手段，其设计需注重概念关联性与时间控制。

Abstract: Teaching software development basics to hundreds of students in a frontal setting is cost-efficient and thus still common in universities. However, in a large lecture hall, students can easily get bored, distracted, and disengaged. The frontal setting can also frustrate lecturers since interaction opportunities are limited and hard to scale. Fun activities can activate students and, if well designed, can also help remember and reflect on abstract software development concepts. We present a novel catalogue of ten physical fun activities, developed over years to reflect on basic programming and software development concepts. The catalogue includes the execution of a LA-OLA algorithm as in stadiums, using paper planes to simulate object messages and pointers, and traversing a lecture hall as a tree or a recursive structure. We report our experience of using the activities in a large course with 500+ students three years in a row. We also conducted an interview study with 15 former students of the course and 14 experienced educators from around the globe. The results suggest that the fun activities can enable students to stay focused, remember key concepts, and reflect afterwards. However, keeping the activities concise and clearly linked to the concepts taught seems to be key to their acceptance and effectiveness.

</details>


### [19] [Beyond Strict Rules: Assessing the Effectiveness of Large Language Models for Code Smell Detection](https://arxiv.org/abs/2601.09873)
*Saymon Souza,Amanda Santana,Eduardo Figueiredo,Igor Muzetti,João Eduardo Montandon,Lionel Briand*

Main category: cs.SE

TL;DR: 本文评估了四种大语言模型在检测代码异味方面的效果，并提出结合静态分析工具的策略，发现其在部分异味检测中表现更优，但对复杂异味会产生更多误报。


<details>
  <summary>Details</summary>
Motivation: 探索大语言模型在代码异味检测中的潜力，弥补传统静态分析工具规则僵化的问题。

Method: 通过人工标注构建真实数据集，在30个Java项目中评估四种LLM对九种代码异味的检测能力，并提出融合LLM与静态工具的混合策略。

Result: LLM对结构简单的异味（如Large Class、Long Method）表现良好；混合策略在五种异味上F1分数优于单一方法，但对复杂异味误报率上升。

Conclusion: 代码异味检测的最佳策略应根据优先关注召回率还是精确率进行选择。

Abstract: Code smells are symptoms of potential code quality problems that may affect software maintainability, thus increasing development costs and impacting software reliability. Large language models (LLMs) have shown remarkable capabilities for supporting various software engineering activities, but their use for detecting code smells remains underexplored. However, unlike the rigid rules of static analysis tools, LLMs can support flexible and adaptable detection strategies tailored to the unique properties of code smells. This paper evaluates the effectiveness of four LLMs -- DeepSeek-R1, GPT-5 mini, Llama-3.3, and Qwen2.5-Code -- for detecting nine code smells across 30 Java projects. For the empirical evaluation, we created a ground-truth dataset by asking 76 developers to manually inspect 268 code-smell candidates. Our results indicate that LLMs perform strongly for structurally straightforward smells, such as Large Class and Long Method. However, we also observed that different LLMs and tools fare better for distinct code smells. We then propose and evaluate a detection strategy that combines LLMs and static analysis tools. The proposed strategy outperforms LLMs and tools in five out of nine code smells in terms of F1-Score. However, it also generates more false positives for complex smells. Therefore, we conclude that the optimal strategy depends on whether Recall or Precision is the main priority for code smell detection.

</details>


### [20] [Self-reflection in Automated Qualitative Coding: Improving Text Annotation through Secondary LLM Critique](https://arxiv.org/abs/2601.09905)
*Zackary Okun Dunivin,Mobina Noori,Seth Frey,Curtis Atkinson*

Main category: cs.SE

TL;DR: 通过两阶段LLM工作流提升定性编码准确性，第一阶段标注后由第二阶段自我反思修正错误，显著降低误报率并提高F1分数。


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型在零样本和少样本分类中错误率过高的问题，提升定性编码的可靠性。

Method: 设计两阶段流程：第一阶段LLM应用人工设计的编码本进行标注，第二阶段由另一LLM对阳性标签进行自我反思并最终决策。

Result: 在3000封邮件数据上测试，第二阶段使F1分数提升0.04至0.25，两个表现较差的代码分别从0.52和0.55提升至0.69和0.79。

Conclusion: 结合人工指导与验证，自我反思机制可有效嵌入现有LLM辅助标注流程，降低噪声并挽救不可用分类器。

Abstract: Large language models (LLMs) allow for sophisticated qualitative coding of large datasets, but zero- and few-shot classifiers can produce an intolerable number of errors, even with careful, validated prompting. We present a simple, generalizable two-stage workflow: an LLM applies a human-designed, LLM-adapted codebook; a secondary LLM critic performs self-reflection on each positive label by re-reading the source text alongside the first model's rationale and issuing a final decision. We evaluate this approach on six qualitative codes over 3,000 high-content emails from Apache Software Foundation project evaluation discussions. Our human-derived audit of 360 positive annotations (60 passages by six codes) found that the first-line LLM had a false-positive rate of 8% to 54%, despite F1 scores of 0.74 and 1.00 in testing. Subsequent recoding of all stage-one annotations via a second self-reflection stage improved F1 by 0.04 to 0.25, bringing two especially poor performing codes up to 0.69 and 0.79 from 0.52 and 0.55 respectively. Our manual evaluation identified two recurrent error classes: misinterpretation (violations of code definitions) and meta-discussion (debate about a project evaluation criterion mistaken for its use as a decision justification). Code-specific critic clauses addressing observed failure modes were especially effective with testing and refinement, replicating the codebook-adaption process for LLM interpretation in stage-one. We explain how favoring recall in first-line LLM annotation combined with secondary critique delivers precision-first, compute-light control. With human guidance and validation, self-reflection slots into existing LLM-assisted annotation pipelines to reduce noise and potentially salvage unusable classifiers.

</details>


### [21] [S$^2$F: Principled Hybrid Testing With Fuzzing, Symbolic Execution, and Sampling](https://arxiv.org/abs/2601.10068)
*Lianjing Wang,Yufeng Zhang,Kenli Li,Zhenbang Chen,Xu Zhou,Pengfei Wang,Guangning Song,Ji Wang*

Main category: cs.SE

TL;DR: 提出新型混合测试工具S$^2$F，结合模糊测试、符号执行与采样，显著提升覆盖率与漏洞发现能力。


<details>
  <summary>Details</summary>
Motivation: 现有混合测试工具未能充分发挥符号执行与采样的潜力，存在分支过度剪枝和采样策略不当的问题。

Method: 设计新架构融合传统与定制化符号执行引擎，并制定三者协同原则，实现更高效测试。

Result: 在15个真实程序上实验，S$^2$F平均提升6.14%边覆盖率和32.6%崩溃发现率，并发现3个未知崩溃。

Conclusion: S$^2$F有效克服现有工具局限，验证了所提架构与协同原则的优越性。

Abstract: Hybrid testing that integrates fuzzing, symbolic execution, and sampling has demonstrated superior testing efficiency compared to individual techniques. However, the state-of-the-art (SOTA) hybrid testing tools do not fully exploit the capabilities of symbolic execution and sampling in two key aspects. First, the SOTA hybrid testing tools employ tailored symbolic execution engines that tend to over-prune branches, leading to considerable time wasted waiting for seeds from the fuzzer and missing opportunities to discover crashes. Second, existing methods do not apply sampling to the appropriate branches and therefore cannot utilize the full capability of sampling. To address these two limitations, we propose a novel hybrid testing architecture that combines the precision of conventional symbolic execution with the scalability of tailored symbolic execution engines. Based on this architecture, we propose several principles for combining fuzzing, symbolic execution, and sampling. We implement our method in a hybrid testing tool S$^2$F. To evaluate its effectiveness, we conduct extensive experiments on 15 real-world programs. Experimental results demonstrate that S$^2$F outperforms the SOTA tool, achieving an average improvement of 6.14% in edge coverage and 32.6% in discovered crashes. Notably, our tool uncovers three previously unknown crashes in real-world programs.

</details>


### [22] [Mark My Works Autograder for Programming Courses](https://arxiv.org/abs/2601.10093)
*Yiding Qiu,Seyed Mahdi Azimi,Artem Lensky*

Main category: cs.SE

TL;DR: Mark My Works系统结合单元测试与LLM生成反馈，为编程课提供详细评语，虽评分与人工无显著相关，但分布相似且反馈更细致。


<details>
  <summary>Details</summary>
Motivation: 解决大规模编程课程中学生代码反馈不及时、不详细的问题。

Method: 开发本地自动评分系统，结合单元测试和基于角色提示的LLM生成反馈。

Result: 在191人课程中测试79份作业，AI评分均值59.95（人工80.53），无显著线性相关但分布相似，AI反馈更详尽。

Conclusion: 该系统能有效补充人工评分，提供高质量教学反馈，适合辅助大规模编程教学。

Abstract: Large programming courses struggle to provide timely, detailed feedback on student code. We developed Mark My Works, a local autograding system that combines traditional unit testing with LLM-generated explanations. The system uses role-based prompts to analyze submissions, critique code quality, and generate pedagogical feedback while maintaining transparency in its reasoning process.
  We piloted the system in a 191-student engineering course, comparing AI-generated assessments with human grading on 79 submissions. While AI scores showed no linear correlation with human scores (r = -0.177, p = 0.124), both systems exhibited similar left-skewed distributions, suggesting they recognize comparable quality hierarchies despite different scoring philosophies. The AI system demonstrated more conservative scoring (mean: 59.95 vs 80.53 human) but generated significantly more detailed technical feedback.

</details>


### [23] [Towards Online Malware Detection using Process Resource Utilization Metrics](https://arxiv.org/abs/2601.10164)
*Themistoklis Diamantopoulos,Dimosthenis Natsos,Andreas L. Symeonidis*

Main category: cs.SE

TL;DR: 本文提出一种基于在线学习的动态恶意软件检测方法，利用进程资源使用特征持续更新模型，有效应对零日攻击和数据稀缺场景。


<details>
  <summary>Details</summary>
Motivation: 现有恶意软件检测方法依赖大量标注数据且无法适应恶意软件的持续演化，导致对新型攻击检测能力不足。

Method: 采用在线学习框架，结合时间信息与行为特征（如进程资源利用率），实现模型的增量式更新。

Result: 相比传统批量算法，该方法在零日恶意软件检测和小样本场景中表现更优。

Conclusion: 所提方法能有效适应恶意软件演化，在资源受限和数据稀疏环境下仍保持高检测性能。

Abstract: The rapid growth of Cloud Computing and Internet of Things (IoT) has significantly increased the interconnection of computational resources, creating an environment where malicious software (malware) can spread rapidly. To address this challenge, researchers are increasingly utilizing Machine Learning approaches to identify malware through behavioral (i.e. dynamic) cues. However, current approaches are limited by their reliance on large labeled datasets, fixed model training, and the assumption that a trained model remains effective over time-disregarding the ever-evolving sophistication of malware. As a result, they often fail to detect evolving malware attacks that adapt over time. This paper proposes an online learning approach for dynamic malware detection, that overcomes these limitations by incorporating temporal information to continuously update its models using behavioral features, specifically process resource utilization metrics. By doing so, the proposed models can incrementally adapt to emerging threats and detect zero-day malware effectively. Upon evaluating our approach against traditional batch algorithms, we find it effective in detecting zero-day malware. Moreover, we demonstrate its efficacy in scenarios with limited data availability, where traditional batch-based approaches often struggle to perform reliably.

</details>


### [24] [Evolving with AI: A Longitudinal Analysis of Developer Logs](https://arxiv.org/abs/2601.10258)
*Agnia Sergeyuk,Eric Huang,Dariia Karaeva,Anastasiia Serova,Yaroslav Golubev,Iftekhar Ahmed*

Main category: cs.SE

TL;DR: 研究通过两年的开发者数据和调查，揭示AI编程助手如何长期影响日常开发工作流。


<details>
  <summary>Details</summary>
Motivation: 现有研究多关注短期使用或主观感知，缺乏对AI助手长期实际影响的实证分析。

Method: 结合800名开发者的两年遥测数据与62名专业人士的问卷调查，从五个维度分析工作流变化。

Result: AI用户编写更多代码但也删除更多；受访者自报生产力提升，其他维度变化感知较小。

Conclusion: AI助手正悄然重构软件开发流程，为未来工具设计提供实证依据。

Abstract: AI-powered coding assistants are rapidly becoming fixtures in professional IDEs, yet their sustained influence on everyday development remains poorly understood. Prior research has focused on short-term use or self-reported perceptions, leaving open questions about how sustained AI use reshapes actual daily coding practices in the long term. We address this gap with a mixed-method study of AI adoption in IDEs, combining longitudinal two-year fine-grained telemetry from 800 developers with a survey of 62 professionals. We analyze five dimensions of workflow change: productivity, code quality, code editing, code reuse, and context switching. Telemetry reveals that AI users produce substantially more code but also delete significantly more. Meanwhile, survey respondents report productivity gains and perceive minimal changes in other dimensions. Our results offer empirical insights into the silent restructuring of software workflows and provide implications for designing future AI-augmented tooling.

</details>


### [25] [Model See, Model Do? Exposure-Aware Evaluation of Bug-vs-Fix Preference in Code LLMs](https://arxiv.org/abs/2601.10496)
*Ali Al-Kaswan,Claudio Spiess,Prem Devanbu,Arie van Deursen,Maliheh Izadi*

Main category: cs.SE

TL;DR: 本文提出一种暴露感知评估框架，研究大语言模型在代码生成中对错误与修复代码的偏好如何受训练数据暴露影响。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在代码生成中可能因训练数据中的错误而输出有缺陷代码，需评估其对正确与错误代码的偏好机制。

Method: 利用ManySStuBs4J基准和Data Portraits成员测试，分析Stack-V2语料中bug与fix变体的暴露情况，并用多种概率评分指标评估模型偏好。

Result: 67%样本未在训练中出现；若仅一版本出现，修复版更常见；模型生成更倾向复现bug，尤其当暴露于bug时；最小/最大token概率稳定偏好修复版，Gini系数在仅见bug时反转偏好。

Conclusion: 训练数据暴露会扭曲模型对bug-fix的评估，存在传播记忆性错误的风险。

Abstract: Large language models are increasingly used for code generation and debugging, but their outputs can still contain bugs, that originate from training data. Distinguishing whether an LLM prefers correct code, or a familiar incorrect version might be influenced by what it's been exposed to during training. We introduce an exposure-aware evaluation framework that quantifies how prior exposure to buggy versus fixed code influences a model's preference. Using the ManySStuBs4J benchmark, we apply Data Portraits for membership testing on the Stack-V2 corpus to estimate whether each buggy and fixed variant was seen during training. We then stratify examples by exposure and compare model preference using code completion as well as multiple likelihood-based scoring metrics We find that most examples (67%) have neither variant in the training data, and when only one is present, fixes are more frequently present than bugs. In model generations, models reproduce buggy lines far more often than fixes, with bug-exposed examples amplifying this tendency and fix-exposed examples showing only marginal improvement. In likelihood scoring, minimum and maximum token-probability metrics consistently prefer the fixed code across all conditions, indicating a stable bias toward correct fixes. In contrast, metrics like the Gini coefficient reverse preference when only the buggy variant was seen. Our results indicate that exposure can skew bug-fix evaluations and highlight the risk that LLMs may propagate memorised errors in practice.

</details>
