<div id=toc></div>

# Table of Contents

- [cs.DC](#cs.DC) [Total: 1]
- [cs.PF](#cs.PF) [Total: 1]
- [cs.SE](#cs.SE) [Total: 7]
- [cs.MA](#cs.MA) [Total: 1]
- [cs.NI](#cs.NI) [Total: 1]
- [cs.AR](#cs.AR) [Total: 1]


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [1] [Advancing RT Core-Accelerated Fixed-Radius Nearest Neighbor Search](https://arxiv.org/abs/2601.15633)
*Enzo Meneses,Hugo Bec,Cristóbal A. Navarroa,Benoît Crespin,Felipe A. Quezada,Nancy Hitschfeld,Heinich Porro,Maxime Maria*

Main category: cs.DC

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: In this work we introduce three ideas that can further improve particle FRNN physics simulations running on RT Cores; i) a real-time update/rebuild ratio optimizer for the bounding volume hierarchy (BVH) structure, ii) a new RT core use, with two variants, that eliminates the need of a neighbor list and iii) a technique that enables RT cores for FRNN with periodic boundary conditions (BC). Experimental evaluation using the Lennard-Jones FRNN interaction model as a case study shows that the proposed update/rebuild ratio optimizer is capable of adapting to the different dynamics that emerge during a simulation, leading to a RT core pipeline up to $\sim 3.4\times$ faster than with other known approaches to manage the BVH. In terms of simulation step performance, the proposed variants can significantly improve the speedup and EE of the base RT core idea; from $\sim1.3\times$ at small radius to $\sim2.0\times$ for log normal radius distributions. Furthermore, the proposed variants manage to simulate cases that would otherwise not fit in memory because of the use of neighbor lists, such as clusters of particles with log normal radius distribution. The proposed RT Core technique to support periodic BC is indeed effective as it does not introduce any significant penalty in performance. In terms of scaling, the proposed methods scale both their performance and EE across GPU generations. Throughout the experimental evaluation, we also identify the simulation cases were regular GPU computation should still be preferred, contributing to the understanding of the strengths and limitations of RT cores.

</details>


<div id='cs.PF'></div>

# cs.PF [[Back]](#toc)

### [2] [Sawtooth Wavefront Reordering: Enhanced CuTile FlashAttention on NVIDIA GB10](https://arxiv.org/abs/2601.16032)
*Yifan Zhu,Yekai Pan,Chen Ding*

Main category: cs.PF

TL;DR: 本文分析基于CuTile的Flash Attention在NVIDIA GB10上的缓存问题，提出锯齿波前重排序技术以减少L2缓存缺失并提升吞吐量。


<details>
  <summary>Details</summary>
Motivation: 高性能注意力核对大语言模型至关重要，需优化缓存性能以提高效率。

Method: 分析GB10显卡L2缓存缺失主要原因，设计锯齿波前重排序技术以改善缓存表现。

Result: 实施后L2缓存缺失率降低超50%，吞吐量提升达60%，适用于CUDA和CuTile环境。

Conclusion: 该技术验证有效，显著加速了大语言模型的注意力核性能。

Abstract: High-performance attention kernels are essential for Large Language Models. This paper presents analysis of CuTile-based Flash Attention memory behavior and a technique to improve its cache performance. In particular, our analysis on the NVIDIA GB10 (Grace Blackwell) identifies the main cause of L2 cache miss. Leveraging this insight, we introduce a new programming technique called Sawtooth Wavefront Reordering that reduces L2 misses. We validate it in both CUDA and CuTile, observing 50\% or greater reduction in L2 misses and up to 60\% increase in throughput on GB10.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [3] [ToolCaching: Towards Efficient Caching for LLM Tool-calling](https://arxiv.org/abs/2601.15335)
*Yi Zhai,Dian Shen,Junzhou Luo,Bin Yang*

Main category: cs.SE

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Recent advances in Large Language Models (LLMs) have revolutionized web applications, enabling intelligent search, recommendation, and assistant services with natural language interfaces. Tool-calling extends LLMs with the ability to interact with external APIs, greatly enhancing their practical utility. While prior research has improved tool-calling performance by adopting traditional computer systems techniques, such as parallel and asynchronous execution, the challenge of redundant or repeated tool-calling requests remains largely unaddressed. Caching is a classic solution to this problem, but applying it to LLM tool-calling introduces new difficulties due to heterogeneous request semantics, dynamic workloads, and varying freshness requirements, which render conventional cache policies ineffective. To address these issues, we propose ToolCaching, an efficient feature-driven and adaptive caching framework for LLM tool-calling systems. ToolCaching systematically integrates semantic and system-level features to evaluate request cacheability and estimate caching value. At its core, the VAAC algorithm integrates bandit-based admission with value-driven, multi-factor eviction, jointly accounting for request frequency, recency, and caching value. Extensive experiments on synthetic and public tool-calling workloads demonstrate that ToolCaching with VAAC achieves up to 11% higher cache hit ratios and 34% lower latency compared to standard policies, effectively accelerating LLM tool-calling in practical applications.

</details>


### [4] [Lost in Transcription: How Speech-to-Text Errors Derail Code Understanding](https://arxiv.org/abs/2601.15339)
*Jayant Havare,Ashish Mittal,Srikanth Tamilselvam,Ganesh Ramakrishnan*

Main category: cs.SE

TL;DR: 本研究开发了一个多语言语音驱动框架，支持用户用母语口头查询代码，通过ASroma和LLM优化转录后执行代码问答和检索任务，解决了非英语场景下的编程语音交互问题。


<details>
  <summary>Details</summary>
Motivation: 现有编程工具主要服务于英文键盘用户，限制了多语言和语音优先场景（如印度）的可访问性。语音查询代码存在方言、术语和混合表达等挑战，亟需包容性解决方案。

Method: 构建框架包含：1) 支持母语语音输入；2) 用ASR转录语音；3) 基于LLM进行代码感知的转录修正；4) 连接代码模型处理CodeSearchNet等基准任务，重点测试四种印度语言和英语。

Result: 系统化分析转录错误对任务的影响，识别ASR关键失败模式；实验证明LLM引导的优化显著提升转录准确率和代码理解性能，具体改进体现在多项下游任务中。

Conclusion: 成果强调语音界面需适配代码特性，为构建鲁棒的多语言语音编程工具提供实用方案，推动编程工具包容性发展。

Abstract: Code understanding is a foundational capability in software engineering tools and developer workflows. However, most existing systems are designed for English-speaking users interacting via keyboards, which limits accessibility in multilingual and voice-first settings, particularly in regions like India. Voice-based interfaces offer a more inclusive modality, but spoken queries involving code present unique challenges due to the presence of non-standard English usage, domain-specific vocabulary, and custom identifiers such as variable and function names, often combined with code-mixed expressions. In this work, we develop a multilingual speech-driven framework for code understanding that accepts spoken queries in a user native language, transcribes them using Automatic Speech Recognition (ASR), applies code-aware ASR output refinement using Large Language Models (LLMs), and interfaces with code models to perform tasks such as code question answering and code retrieval through benchmarks such as CodeSearchNet, CoRNStack, and CodeQA. Focusing on four widely spoken Indic languages and English, we systematically characterize how transcription errors impact downstream task performance. We also identified key failure modes in ASR for code and demonstrated that LLM-guided refinement significantly improves performance across both transcription and code understanding stages. Our findings underscore the need for code-sensitive adaptations in speech interfaces and offer a practical solution for building robust, multilingual voice-driven programming tools.

</details>


### [5] [A Prompt-Based Framework for Loop Vulnerability Detection Using Local LLMs](https://arxiv.org/abs/2601.15352)
*Adeyemi Adeseye,Aisvarya Adeseye*

Main category: cs.SE

TL;DR: 本研究提出了基于本地LLMs的提示框架，用于检测Python 3.7+代码中的循环漏洞，并通过实验验证了框架效果。


<details>
  <summary>Details</summary>
Motivation: 传统静态分析器难以检测ritical语义级循环漏洞，而本地LLMs能兼顾代码上下文理解且解决隐私、延迟等云服务限制。

Method: 设计结构化提示框架（含语言认知、版本敏感等防护特性），指导本地部署的LLaMA 3B和Phi-4B模型通过迭代提示检测三类循环漏洞。

Result: 基于人工基准验证显示，Phi模型在精确率、召回率和F1分数上全面优于LLaMA模型。

Conclusion: 有效的提示工程是本地LLMs实现精准代码漏洞分析的关键，该方法为安全高效的离线检测提供了新思路。

Abstract: Loop vulnerabilities are one major risky construct in software development. They can easily lead to infinite loops or executions, exhaust resources, or introduce logical errors that degrade performance and compromise security. The problem are often undetected by traditional static analyzers because such tools rely on syntactic patterns, which makes them struggle to detect semantic flaws. Consequently, Large Language Models (LLMs) offer new potential for vulnerability detection because of their ability to understand code contextually. Moreover, local LLMs unlike commercial ones like ChatGPT or Gemini addresses issues such as privacy, latency, and dependency concerns by facilitating efficient offline analysis. Consequently, this study proposes a prompt-based framework that utilize local LLMs for the detection of loop vulnerabilities within Python 3.7+ code. The framework targets three categories of loop-related issues, such as control and logic errors, security risks inside loops, and resource management inefficiencies. A generalized and structured prompt-based framework was designed and tested with two locally deployed LLMs (LLaMA 3.2; 3B and Phi 3.5; 4B) by guiding their behavior via iterative prompting. The designed prompt-based framework included key safeguarding features such as language-specific awareness, code-aware grounding, version sensitivity, and hallucination prevention. The LLM results were validated against a manually established baseline truth, and the results indicate that Phi outperforms LLaMA in precision, recall, and F1-score. The findings emphasize the importance of designing effective prompts for local LLMs to perform secure and accurate code vulnerability analysis.

</details>


### [6] [FARM: Field-Aware Resolution Model for Intelligent Trigger-Action Automation](https://arxiv.org/abs/2601.15687)
*Khusrav Badalov,Young Yoon*

Main category: cs.SE

TL;DR: 本文提出FARM框架解决TAP小程序功能级配置问题，通过双阶段模型实现高精度自动化小程序生成与绑定匹配。


<details>
  <summary>Details</summary>
Motivation: 现有TAP平台多侧重于服务级自然语言预测，常生成需手动配置的非可执行小程序。需解决功能级配置问题（包括触发器-动作绑定），实现完整可执行自动化。

Method: FARM采用双阶段架构：1) 使用模式增强表示训练对比双编码器，从220万种触发-动作对中候选检索；2) 基于LLM的多智能体流水线执行意图分析、跨模式评分的选择机制及配置验证。

Result: 功能级联合准确率达81%（噪声数据下62%，单样本下70%），服务级准确率81%，较TARGE提升23个百分点；成功生成可执行的成分-字段绑定配置。

Conclusion: FARM首次实现自动化生成完整可执行的TAP小程序，显著降低了手动配置需求，为物联网自动化提供了可靠解决方案。

Abstract: Trigger-Action Programming (TAP) platforms such as IFTTT and Zapier enable Web of Things (WoT) automation by composing event-driven rules across heterogeneous services. A TAP applet links a trigger to an action and must bind trigger outputs (ingredients) to action inputs (fields) to be executable. Prior work largely treats TAP as service-level prediction from natural language, which often yields non-executable applets that still require manual configuration. We study the function-level configuration problem: generating complete applets with correct ingredient-to-field bindings. We propose FARM (Field-Aware Resolution Model), a two-stage architecture for automated applet generation with full configuration. Stage 1 trains contrastive dual encoders with selective layer freezing over schema-enriched representations, retrieving candidates from 1,724 trigger functions and 1,287 action functions (2.2M possible trigger-action pairs). Stage 2 performs selection and configuration using an LLM-based multi-agent pipeline. It includes intent analysis, trigger selection, action selection via cross-schema scoring, and configuration verification. Agents coordinate through shared state and agreement-based selection. FARM achieves 81% joint accuracy on Gold (62% Noisy, 70% One-shot) at the function level, where both trigger and action functions must match the ground truth. For comparison with service-level baselines, we map functions to their parent services and evaluate at the service level. FARM reaches 81% joint accuracy and improves over TARGE by 23 percentage points. FARM also generates ingredient-to-field bindings, producing executable automation configurations.

</details>


### [7] [Evaluating and Achieving Controllable Code Completion in Code LLM](https://arxiv.org/abs/2601.15879)
*Jiajun Zhang,Zeyu Cui,Lei Zhang,Jian Yang,Jiaxi Yang,Qiang Liu,Zilei Wang,Binyuan Hui,Liang Wang,Junyang Lin*

Main category: cs.SE

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Code completion has become a central task, gaining significant attention with the rise of large language model (LLM)-based tools in software engineering. Although recent advances have greatly improved LLMs' code completion abilities, evaluation methods have not advanced equally. Most current benchmarks focus solely on functional correctness of code completions based on given context, overlooking models' ability to follow user instructions during completion-a common scenario in LLM-assisted programming. To address this limitation, we present the first instruction-guided code completion benchmark, Controllable Code Completion Benchmark (C3-Bench), comprising 2,195 carefully designed completion tasks. Through comprehensive evaluation of over 40 mainstream LLMs across C3-Bench and conventional benchmarks, we reveal substantial gaps in instruction-following capabilities between open-source and advanced proprietary models during code completion tasks. Moreover, we develop a straightforward data synthesis pipeline that leverages Qwen2.5-Coder to generate high-quality instruction-completion pairs for supervised fine-tuning (SFT). The resulting model, Qwen2.5-Coder-C3, achieves state-of-the-art performance on C3-Bench. Our findings provide valuable insights for enhancing LLMs' code completion and instruction-following capabilities, establishing new directions for future research in code LLMs. To facilitate reproducibility and foster further research in code LLMs, we open-source all code, datasets, and models.

</details>


### [8] [The Role of Cognitive Abilities in Requirements Inspection: Comparing UML and Textual Representations](https://arxiv.org/abs/2601.16009)
*Giovanna Broccia,Sira Vegas,Alessio Ferrari*

Main category: cs.SE

TL;DR: 研究评估UML序列图如何提升需求检查准确性及认知能力的影响，发现UML支持效果因人而异。


<details>
  <summary>Details</summary>
Motivation: 探讨UML图是否比纯文本更能提高需求检查准确性，并分析认知能力（工作记忆和心理旋转）是否在文本与UML处理间造成绩效差异。

Method: 采用交叉实验设计，招募38名参与者比较纯文本与文本加UML的检查效果，使用线性混合效应模型分析处理方法、周期、序列及认知能力。

Result: UML支持效果取决于认知能力组合：高工作记忆和心理旋转能力者使用UML时违规检测准确率下降，但论证准确率提高，表明高认知能力在多模态信息（图文结合）下支持深度推理。

Conclusion: Conclusion extraction failed

Abstract: The representation of requirements plays a critical role in the accuracy of requirements inspection. While visual representations, such as UML diagrams, are widely used alongside text-based requirements, their effectiveness in supporting inspection is still debated. Cognitive abilities, such as working memory and mental rotation skills, may also influence inspection accuracy. This study aims to evaluate whether the use of UML sequence diagrams alongside text-based requirements improves the accuracy of requirements inspection compared to text-based requirements alone and to explore whether cognitive abilities are associated with differences in performance across the two treatments (text vs text with UML support). We conducted a crossover experiment with 38 participants to assess the accuracy of requirements inspection under the two treatments in terms of issues found and justifications provided. Linear mixed-effects and generalized linear models were used to analyse the effects of treatment, period, sequence, and cognitive abilities. The results indicate a significant three-way interaction between representation type, working memory capacity, and mental rotation ability. This finding suggests that the effectiveness of UML support is not uniform across individuals: participants with high scores in both cognitive abilities experienced reduced performance when using UML for violation detection. Conversely, the same cognitive profile was associated with improved justification accuracy under UML-aided inspection, indicating that higher cognitive abilities may support deeper reasoning processes when dealing with multi-modal information, i.e., diagrams and text.

</details>


### [9] [Towards a Goal-Centric Assessment of Requirements Engineering Methods for Privacy by Design](https://arxiv.org/abs/2601.16080)
*Oleksandr Kosenkov,Ehsan Zabardast,Jannik Fischbach,Tony Gorschek,Daniel Mendez*

Main category: cs.SE

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Implementing privacy by design (PbD) according to the General Data Protection Regulation (GDPR) is met with a growing number of requirements engineering (RE) approaches. However, the question of which RE method for PbD fits best the goals of organisations remains a challenge. We report our endeavor to close this gap by synthesizing a goal-centric approach for PbD methods assessment. We used literature review, interviews, and validation with practitioners to achieve the goal of our study. As practitioners do not approach PbD systematically, we suggest that RE methods for PbD should be assessed against organisational goals, rather than process characteristics only. We hope that, when further developed, the goal-centric approach could support the development, selection, and tailoring of RE practices for PbD.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [10] [Delayed Assignments in Online Non-Centroid Clustering with Stochastic Arrivals](https://arxiv.org/abs/2601.16091)
*Saar Cohen*

Main category: cs.MA

TL;DR: 本文提出一种在线非中心聚类框架，支持延迟决策，在随机到达模型下实现常数竞争比。


<details>
  <summary>Details</summary>
Motivation: 解决最坏情况到达模型下性能差的问题，允许延迟分配以联合优化聚类距离成本和延迟成本。

Method: 假设点位置从未知分布独立抽样，设计在线算法延迟分配决策；点到达时可直接聚类或延后决策产生成本。

Result: 算法在点数增多时，预期总成本与最优离线聚类的比率有常数上界，竞争性能超越最坏情形。

Conclusion: 在随机环境中有效克服不可能性，证明在线聚类可获恒定竞争比，提升实用性与扩展性。

Abstract: Clustering is a fundamental problem, aiming to partition a set of elements, like agents or data points, into clusters such that elements in the same cluster are closer to each other than to those in other clusters. In this paper, we present a new framework for studying online non-centroid clustering with delays, where elements, that arrive one at a time as points in a finite metric space, should be assigned to clusters, but assignments need not be immediate. Specifically, upon arrival, each point's location is revealed, and an online algorithm has to irrevocably assign it to an existing cluster or create a new one containing, at this moment, only this point. However, we allow decisions to be postponed at a delay cost, instead of following the more common assumption of immediate decisions upon arrival. This poses a critical challenge: the goal is to minimize both the total distance costs between points in each cluster and the overall delay costs incurred by postponing assignments. In the classic worst-case arrival model, where points arrive in an arbitrary order, no algorithm has a competitive ratio better than sublogarithmic in the number of points. To overcome this strong impossibility, we focus on a stochastic arrival model, where points' locations are drawn independently across time from an unknown and fixed probability distribution over the finite metric space. We offer hope for beyond worst-case adversaries: we devise an algorithm that is constant competitive in the sense that, as the number of points grows, the ratio between the expected overall costs of the output clustering and an optimal offline clustering is bounded by a constant.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [11] [MapViT: A Two-Stage ViT-Based Framework for Real-Time Radio Quality Map Prediction in Dynamic Environments](https://arxiv.org/abs/2601.15578)
*Cyril Shih-Huan Hsu,Xi Li,Lanfranco Zanzi,Zhiheng Yang,Chrysa Papagianni,Xavier Costa Pérez*

Main category: cs.NI

TL;DR: 研究者提出MapViT框架，基于视觉Transformer预测环境变化和无线电信号质量，以实现移动机器人的可靠导航与高效运行。


<details>
  <summary>Details</summary>
Motivation: 解决动态环境中机器人对周围环境和无线电信号理解不足的问题，这是自动驾驶领域的核心挑战。

Method: 采用两阶段Vision Transformer架构，借鉴大型语言模型的预训练精调范式hungarian notation预测环境变化与信号质量，使用ML模型评估框架性能。

Result: 实验证明该框架实现实时预测，ViT在精度和计算效率间保持平衡，提升数据效率和迁移能力，适合资源受限平台。

Conclusion: 此工作为下一代数字孪生生态系统奠定基础，引导面向6G系统的多模态智能ML基础模型发展。

Abstract: Recent advancements in mobile and wireless networks are unlocking the full potential of robotic autonomy, enabling robots to take advantage of ultra-low latency, high data throughput, and ubiquitous connectivity. However, for robots to navigate and operate seamlessly, efficiently and reliably, they must have an accurate understanding of both their surrounding environment and the quality of radio signals. Achieving this in highly dynamic and ever-changing environments remains a challenging and largely unsolved problem. In this paper, we introduce MapViT, a two-stage Vision Transformer (ViT)-based framework inspired by the success of pre-train and fine-tune paradigm for Large Language Models (LLMs). MapViT is designed to predict both environmental changes and expected radio signal quality. We evaluate the framework using a set of representative Machine Learning (ML) models, analyzing their respective strengths and limitations across different scenarios. Experimental results demonstrate that the proposed two-stage pipeline enables real-time prediction, with the ViT-based implementation achieving a strong balance between accuracy and computational efficiency. This makes MapViT a promising solution for energy- and resource-constrained platforms such as mobile robots. Moreover, the geometry foundation model derived from the self-supervised pre-training stage improves data efficiency and transferability, enabling effective downstream predictions even with limited labeled data. Overall, this work lays the foundation for next-generation digital twin ecosystems, and it paves the way for a new class of ML foundation models driving multi-modal intelligence in future 6G-enabled systems.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [12] [FlexLLM: Composable HLS Library for Flexible Hybrid LLM Accelerator Design](https://arxiv.org/abs/2601.15710)
*Jiahao Zhang,Zifan He,Nicholas Fraser,Michaela Blott,Yizhou Sun,Jason Cong*

Main category: cs.AR

TL;DR: FlexLLM是一种可组合的高级综合库，用于快速开发领域专用大语言模型加速器，支持阶段定制推理和高效量化部署。


<details>
  <summary>Details</summary>
Motivation: 解决现有硬件难以高效适配大语言模型推理中预填充与解码阶段的差异化需求，以及长上下文处理瓶颈问题。

Method: 提供架构设计自由度实现阶段定制混合设计，集成量化套件，并开发Hierarchical Memory Transformer插件优化长上下文处理。

Result: 在AMD U280 FPGA上相较NVIDIA A100 GPU：端到端加速1.29倍，解码吞吐量提升1.64倍能效提高3.14倍；使用HMT插件后预填充延迟降低23.23倍，上下文窗口扩大64倍。

Conclusion: 该框架以最少人工实现LLM算法创新与高性能硬件加速的结合，显著提升FPGA平台上的推理效率和能效。

Abstract: We present FlexLLM, a composable High-Level Synthesis (HLS) library for rapid development of domain-specific LLM accelerators. FlexLLM exposes key architectural degrees of freedom for stage-customized inference, enabling hybrid designs that tailor temporal reuse and spatial dataflow differently for prefill and decode, and provides a comprehensive quantization suite to support accurate low-bit deployment. Using FlexLLM, we build a complete inference system for the Llama-3.2 1B model in under two months with only 1K lines of code. The system includes: (1) a stage-customized accelerator with hardware-efficient quantization (12.68 WikiText-2 PPL) surpassing SpinQuant baseline, and (2) a Hierarchical Memory Transformer (HMT) plug-in for efficient long-context processing. On the AMD U280 FPGA at 16nm, the accelerator achieves 1.29$\times$ end-to-end speedup, 1.64$\times$ higher decode throughput, and 3.14$\times$ better energy efficiency than an NVIDIA A100 GPU (7nm) running BF16 inference; projected results on the V80 FPGA at 7nm reach 4.71$\times$, 6.55$\times$, and 4.13$\times$, respectively. In long-context scenarios, integrating the HMT plug-in reduces prefill latency by 23.23$\times$ and extends the context window by 64$\times$, delivering 1.10$\times$/4.86$\times$ lower end-to-end latency and 5.21$\times$/6.27$\times$ higher energy efficiency on the U280/V80 compared to the A100 baseline. FlexLLM thus bridges algorithmic innovation in LLM inference and high-performance accelerators with minimal manual effort.

</details>
