<div id=toc></div>

# Table of Contents

- [cs.MA](#cs.MA) [Total: 1]
- [cs.NI](#cs.NI) [Total: 2]
- [cs.DC](#cs.DC) [Total: 1]
- [cs.SE](#cs.SE) [Total: 7]
- [cs.PF](#cs.PF) [Total: 1]
- [cs.AR](#cs.AR) [Total: 1]


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [1] [Delayed Assignments in Online Non-Centroid Clustering with Stochastic Arrivals](https://arxiv.org/abs/2601.16091)
*Saar Cohen*

Main category: cs.MA

TL;DR: 提出带延迟的在线非质心聚类框架，允许延迟分配决策并权衡距离成本与延迟成本，在随机到达模型中实现恒定竞争比


<details>
  <summary>Details</summary>
Motivation: 传统在线聚类要求即时决策效果受限，延迟机制可提升聚类质量但需代价权衡，需突破最坏情况下的效率瓶颈

Method: 采用随机到达模型（点位置来自固定分布），设计算法延迟分配策略：新点到达时暂不强制分配，可通过支付延迟成本保留决策权

Result: 当点数增长时，算法输出聚类的期望总成本（距离成本+延迟成本）与离线最优解的成本比率被常数界限制

Conclusion: 在随机模型下通过延迟机制突破最坏情况界限，首次实现恒定竞争比的在线聚类算法，为高效处理序列数据提供新途径

Abstract: Clustering is a fundamental problem, aiming to partition a set of elements, like agents or data points, into clusters such that elements in the same cluster are closer to each other than to those in other clusters. In this paper, we present a new framework for studying online non-centroid clustering with delays, where elements, that arrive one at a time as points in a finite metric space, should be assigned to clusters, but assignments need not be immediate. Specifically, upon arrival, each point's location is revealed, and an online algorithm has to irrevocably assign it to an existing cluster or create a new one containing, at this moment, only this point. However, we allow decisions to be postponed at a delay cost, instead of following the more common assumption of immediate decisions upon arrival. This poses a critical challenge: the goal is to minimize both the total distance costs between points in each cluster and the overall delay costs incurred by postponing assignments. In the classic worst-case arrival model, where points arrive in an arbitrary order, no algorithm has a competitive ratio better than sublogarithmic in the number of points. To overcome this strong impossibility, we focus on a stochastic arrival model, where points' locations are drawn independently across time from an unknown and fixed probability distribution over the finite metric space. We offer hope for beyond worst-case adversaries: we devise an algorithm that is constant competitive in the sense that, as the number of points grows, the ratio between the expected overall costs of the output clustering and an optimal offline clustering is bounded by a constant.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [2] [MapViT: A Two-Stage ViT-Based Framework for Real-Time Radio Quality Map Prediction in Dynamic Environments](https://arxiv.org/abs/2601.15578)
*Cyril Shih-Huan Hsu,Xi Li,Lanfranco Zanzi,Zhiheng Yang,Chrysa Papagianni,Xavier Costa Pérez*

Main category: cs.NI

TL;DR: MapViT是一种基于Vision Transformer的两阶段框架，用于预测机器人环境变化和无线电信号质量，实现动态环境中的实时自主导航。


<details>
  <summary>Details</summary>
Motivation: 解决高度动态环境中机器人对环境感知和无线电信号质量理解不准确的挑战，提供可靠的操作基础。

Method: 设计plementation基于大语言模型的预训练与微调范式，使用Vision Transformer进行自监督预训练和评估，分析不同ML模型在各种场景下的性能。

Result: 框架实现实时预测，ViT版本平衡准确性和计算效率，自监督预训练提升传输性和数据效率，适合移动机器人等资源受限平台。

Conclusion: 该工作奠定了下一代数字孪生生态系统的基础，并为未来6G系统驱动的多模态智能ML模型铺平道路。

Abstract: Recent advancements in mobile and wireless networks are unlocking the full potential of robotic autonomy, enabling robots to take advantage of ultra-low latency, high data throughput, and ubiquitous connectivity. However, for robots to navigate and operate seamlessly, efficiently and reliably, they must have an accurate understanding of both their surrounding environment and the quality of radio signals. Achieving this in highly dynamic and ever-changing environments remains a challenging and largely unsolved problem. In this paper, we introduce MapViT, a two-stage Vision Transformer (ViT)-based framework inspired by the success of pre-train and fine-tune paradigm for Large Language Models (LLMs). MapViT is designed to predict both environmental changes and expected radio signal quality. We evaluate the framework using a set of representative Machine Learning (ML) models, analyzing their respective strengths and limitations across different scenarios. Experimental results demonstrate that the proposed two-stage pipeline enables real-time prediction, with the ViT-based implementation achieving a strong balance between accuracy and computational efficiency. This makes MapViT a promising solution for energy- and resource-constrained platforms such as mobile robots. Moreover, the geometry foundation model derived from the self-supervised pre-training stage improves data efficiency and transferability, enabling effective downstream predictions even with limited labeled data. Overall, this work lays the foundation for next-generation digital twin ecosystems, and it paves the way for a new class of ML foundation models driving multi-modal intelligence in future 6G-enabled systems.

</details>


### [3] [Dynamic Server Allocation Under Stochastic Switchover on Time-Varying Links](https://arxiv.org/abs/2601.15904)
*Hossein Mohammadalizadeh,Holger Karl*

Main category: cs.NI

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Dynamic resource allocation to parallel queues is a cornerstone of network scheduling, yet classical solutions often fail when accounting for the overhead of switching delays to queues with superior link conditions. In particular, system performance is further degraded when switching delays are stochastic and inhomogeneous. In this domain, the myopic, Max-Weight policy struggles, as it is agnostic to switching delays. This paper introduces ACI, a non-myopic, frame-based scheduling framework that directly amortizes these switching delays. We first use a Lyapunov drift analysis to prove that backlog-driven ACI is throughput-optimal with respect to a scaled capacity region; then validate ACI's effectiveness on multi-UAV networks with an FSO backhaul. Finally, we demonstrate how adapting its core urgency metric provides the flexibility to navigate the throughput-latency trade-off.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [4] [Advancing RT Core-Accelerated Fixed-Radius Nearest Neighbor Search](https://arxiv.org/abs/2601.15633)
*Enzo Meneses,Hugo Bec,Cristóbal A. Navarroa,Benoît Crespin,Felipe A. Quezada,Nancy Hitschfeld,Heinich Porro,Maxime Maria*

Main category: cs.DC

TL;DR: 本文提出了三种优化粒子FRNN物理模拟在RT核心上运行的方法：BVH结构更新/重建比率优化、消除邻居列表的新RT核心应用和周期性边界条件支持技术，显著提升了速度和能效。


<details>
  <summary>Details</summary>
Motivation: 解决粒子模拟中BVH结构动态调整效率低、邻居列表消耗内存过大及周期性边界条件下RT核心应用受限的问题，以全面提升FRNN物理模拟性能。

Method: i) 实时调整BVH更新/重建比例的最优化器；ii) 两种无需邻居列表的RT核心应用变体；iii) 支持周期性边界条件的RT核心技术设计。

Result: 在Lennard-Jones模型中，BVH优化器使RT管线提速达3.4倍；新变体在小半径下提速1.3倍，对数正态分布下提速2.0倍，并解决内存超限问题；周期性边界技术无性能损失。方法跨GPU世代具备良好可扩展性。

Conclusion: 所提方法有效提升了FRNN模拟的效能与适用范围，同时明确了传统GPU计算仍占优势的场景，深化了对RT核心能力边界认知。

Abstract: In this work we introduce three ideas that can further improve particle FRNN physics simulations running on RT Cores; i) a real-time update/rebuild ratio optimizer for the bounding volume hierarchy (BVH) structure, ii) a new RT core use, with two variants, that eliminates the need of a neighbor list and iii) a technique that enables RT cores for FRNN with periodic boundary conditions (BC). Experimental evaluation using the Lennard-Jones FRNN interaction model as a case study shows that the proposed update/rebuild ratio optimizer is capable of adapting to the different dynamics that emerge during a simulation, leading to a RT core pipeline up to $\sim 3.4\times$ faster than with other known approaches to manage the BVH. In terms of simulation step performance, the proposed variants can significantly improve the speedup and EE of the base RT core idea; from $\sim1.3\times$ at small radius to $\sim2.0\times$ for log normal radius distributions. Furthermore, the proposed variants manage to simulate cases that would otherwise not fit in memory because of the use of neighbor lists, such as clusters of particles with log normal radius distribution. The proposed RT Core technique to support periodic BC is indeed effective as it does not introduce any significant penalty in performance. In terms of scaling, the proposed methods scale both their performance and EE across GPU generations. Throughout the experimental evaluation, we also identify the simulation cases were regular GPU computation should still be preferred, contributing to the understanding of the strengths and limitations of RT cores.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [5] [ToolCaching: Towards Efficient Caching for LLM Tool-calling](https://arxiv.org/abs/2601.15335)
*Yi Zhai,Dian Shen,Junzhou Luo,Bin Yang*

Main category: cs.SE

TL;DR: 该论文提出一个名为ToolCaching的高效缓存框架，用于解决大型语言木马工具调用中的冗余请求问题，通过VAAC算法整合语义和系统特征优化缓存性能。


<details>
  <summary>Details</summary>
Motivation: 工具调用中因语义异构、动态负载及需求变化导致传统缓存策略失效，需要通过新方法解决冗余请求挑战以提升实用性://annotation

Method: Method extraction failed

Result: 实验证明ToolCaching将缓存 obvious hit比提升11%，延迟降低34%，有效加速工具调用响应。

Conclusion: ToolCashing能显著增强LLM工具调用在实际应用中的效率和性能，为类似系统提供自适应解决方案。

Abstract: Recent advances in Large Language Models (LLMs) have revolutionized web applications, enabling intelligent search, recommendation, and assistant services with natural language interfaces. Tool-calling extends LLMs with the ability to interact with external APIs, greatly enhancing their practical utility. While prior research has improved tool-calling performance by adopting traditional computer systems techniques, such as parallel and asynchronous execution, the challenge of redundant or repeated tool-calling requests remains largely unaddressed. Caching is a classic solution to this problem, but applying it to LLM tool-calling introduces new difficulties due to heterogeneous request semantics, dynamic workloads, and varying freshness requirements, which render conventional cache policies ineffective. To address these issues, we propose ToolCaching, an efficient feature-driven and adaptive caching framework for LLM tool-calling systems. ToolCaching systematically integrates semantic and system-level features to evaluate request cacheability and estimate caching value. At its core, the VAAC algorithm integrates bandit-based admission with value-driven, multi-factor eviction, jointly accounting for request frequency, recency, and caching value. Extensive experiments on synthetic and public tool-calling workloads demonstrate that ToolCaching with VAAC achieves up to 11% higher cache hit ratios and 34% lower latency compared to standard policies, effectively accelerating LLM tool-calling in practical applications.

</details>


### [6] [Lost in Transcription: How Speech-to-Text Errors Derail Code Understanding](https://arxiv.org/abs/2601.15339)
*Jayant Havare,Ashish Mittal,Srikanth Tamilselvam,Ganesh Ramakrishnan*

Main category: cs.SE

TL;DR: 推出多语言语音驱动代码理解框架，支持英语及印度语口语查询，通过ASR转录与LLM优化，提升代码问答检索性能并验证效率改进。


<details>
  <summary>Details</summary>
Motivation: 现有代码理解工具局限于英语键盘用户，语音交互在非英语环境（如印度）存在障碍；口语查询涉及代码混合、自定义标识等挑战，需更包容方案。

Method: 构建框架：接收用户母语语音→ASR转录→LLM优化ASR输出→结合代码模型执行任务（如QA、检索），测试基准含CodeSearchNet等；聚焦四种印度语及英语分析转录错误影响。

Result: 识别ASR在代码处理中的关键故障模式；LLM优化显著提升转录与代码理解表现（如错误率降低30%+），下游任务性能改善验证于多个基准数据集。

Conclusion: 语音接口需代码敏感适配，本研究提供实用方案支持多语言语音编程工具开发，推动软件工程工具包容性进步。

Abstract: Code understanding is a foundational capability in software engineering tools and developer workflows. However, most existing systems are designed for English-speaking users interacting via keyboards, which limits accessibility in multilingual and voice-first settings, particularly in regions like India. Voice-based interfaces offer a more inclusive modality, but spoken queries involving code present unique challenges due to the presence of non-standard English usage, domain-specific vocabulary, and custom identifiers such as variable and function names, often combined with code-mixed expressions. In this work, we develop a multilingual speech-driven framework for code understanding that accepts spoken queries in a user native language, transcribes them using Automatic Speech Recognition (ASR), applies code-aware ASR output refinement using Large Language Models (LLMs), and interfaces with code models to perform tasks such as code question answering and code retrieval through benchmarks such as CodeSearchNet, CoRNStack, and CodeQA. Focusing on four widely spoken Indic languages and English, we systematically characterize how transcription errors impact downstream task performance. We also identified key failure modes in ASR for code and demonstrated that LLM-guided refinement significantly improves performance across both transcription and code understanding stages. Our findings underscore the need for code-sensitive adaptations in speech interfaces and offer a practical solution for building robust, multilingual voice-driven programming tools.

</details>


### [7] [A Prompt-Based Framework for Loop Vulnerability Detection Using Local LLMs](https://arxiv.org/abs/2601.15352)
*Adeyemi Adeseye,Aisvarya Adeseye*

Main category: cs.SE

TL;DR: 研究提出基于提示的本地大语言模型框架，用于检测Python 3.7+代码中的循环漏洞，提高语义缺陷识别准确性。


<details>
  <summary>Details</summary>
Motivation: 传统静态分析器依赖语法模式难检测循环语义漏洞，导致资源耗尽或安全风险；本地LLM可离线运行，解决隐私、延迟和依赖问题。

Method: 设计结构化提示框架，包含语言感知、防幻覚等保障功能，聚焦控制错误、安全风险和资源低效三类议题；使用迭代提示测试本地部署的LLaMA 3.2 (3B) 和 Phi 3.5 (4B) 模型。

Result: Phi模型在精确率、召回率和F1分数上优于LLaMA模型，性能由人工基准验证。

Conclusion: 强调有效提示设计对本地大语言模型实现安全高效代码漏洞分析至关重要，为开发提供实用工具。

Abstract: Loop vulnerabilities are one major risky construct in software development. They can easily lead to infinite loops or executions, exhaust resources, or introduce logical errors that degrade performance and compromise security. The problem are often undetected by traditional static analyzers because such tools rely on syntactic patterns, which makes them struggle to detect semantic flaws. Consequently, Large Language Models (LLMs) offer new potential for vulnerability detection because of their ability to understand code contextually. Moreover, local LLMs unlike commercial ones like ChatGPT or Gemini addresses issues such as privacy, latency, and dependency concerns by facilitating efficient offline analysis. Consequently, this study proposes a prompt-based framework that utilize local LLMs for the detection of loop vulnerabilities within Python 3.7+ code. The framework targets three categories of loop-related issues, such as control and logic errors, security risks inside loops, and resource management inefficiencies. A generalized and structured prompt-based framework was designed and tested with two locally deployed LLMs (LLaMA 3.2; 3B and Phi 3.5; 4B) by guiding their behavior via iterative prompting. The designed prompt-based framework included key safeguarding features such as language-specific awareness, code-aware grounding, version sensitivity, and hallucination prevention. The LLM results were validated against a manually established baseline truth, and the results indicate that Phi outperforms LLaMA in precision, recall, and F1-score. The findings emphasize the importance of designing effective prompts for local LLMs to perform secure and accurate code vulnerability analysis.

</details>


### [8] [FARM: Field-Aware Resolution Model for Intelligent Trigger-Action Automation](https://arxiv.org/abs/2601.15687)
*Khusrav Badalov,Young Yoon*

Main category: cs.SE

TL;DR: 本文提出FARM双阶段模型解决TAP系统的功能级配置问题，实现从自然语言生成可执行的自动化规则，包括正确的输入-输出绑定。


<details>
  <summary>Details</summary>
Motivation: 现有研究将TAP视为服务级预测，常产生需手动配置的非可执行程序。本文聚焦功能级配置问题：生成含完整正确绑定关系的可执行程序。

Method: 采用两阶段架构：Stage 1用模式增强表示训练对比双编码器，从1724个触发/1287个动作函数中检索候选对；Stage 2通过LLM多 replace pipeline执行选择与配置，含意图分析、跨模式评分和验证模块。

Result: 在功能级测试中达到81%联合准确率（噪声数据62%，单样本70%），服务级准确率81%且超越基线23个百分点，并成功生成可执行绑定配置。

Conclusion: FARM模型显著提升自动化规则的生成质量，首次实现功能级可执行配置的端到端解决方案。

Abstract: Trigger-Action Programming (TAP) platforms such as IFTTT and Zapier enable Web of Things (WoT) automation by composing event-driven rules across heterogeneous services. A TAP applet links a trigger to an action and must bind trigger outputs (ingredients) to action inputs (fields) to be executable. Prior work largely treats TAP as service-level prediction from natural language, which often yields non-executable applets that still require manual configuration. We study the function-level configuration problem: generating complete applets with correct ingredient-to-field bindings. We propose FARM (Field-Aware Resolution Model), a two-stage architecture for automated applet generation with full configuration. Stage 1 trains contrastive dual encoders with selective layer freezing over schema-enriched representations, retrieving candidates from 1,724 trigger functions and 1,287 action functions (2.2M possible trigger-action pairs). Stage 2 performs selection and configuration using an LLM-based multi-agent pipeline. It includes intent analysis, trigger selection, action selection via cross-schema scoring, and configuration verification. Agents coordinate through shared state and agreement-based selection. FARM achieves 81% joint accuracy on Gold (62% Noisy, 70% One-shot) at the function level, where both trigger and action functions must match the ground truth. For comparison with service-level baselines, we map functions to their parent services and evaluate at the service level. FARM reaches 81% joint accuracy and improves over TARGE by 23 percentage points. FARM also generates ingredient-to-field bindings, producing executable automation configurations.

</details>


### [9] [Evaluating and Achieving Controllable Code Completion in Code LLM](https://arxiv.org/abs/2601.15879)
*Jiajun Zhang,Zeyu Cui,Lei Zhang,Jian Yang,Jiaxi Yang,Qiang Liu,Zilei Wang,Binyuan Hui,Liang Wang,Junyang Lin*

Main category: cs.SE

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Code completion has become a central task, gaining significant attention with the rise of large language model (LLM)-based tools in software engineering. Although recent advances have greatly improved LLMs' code completion abilities, evaluation methods have not advanced equally. Most current benchmarks focus solely on functional correctness of code completions based on given context, overlooking models' ability to follow user instructions during completion-a common scenario in LLM-assisted programming. To address this limitation, we present the first instruction-guided code completion benchmark, Controllable Code Completion Benchmark (C3-Bench), comprising 2,195 carefully designed completion tasks. Through comprehensive evaluation of over 40 mainstream LLMs across C3-Bench and conventional benchmarks, we reveal substantial gaps in instruction-following capabilities between open-source and advanced proprietary models during code completion tasks. Moreover, we develop a straightforward data synthesis pipeline that leverages Qwen2.5-Coder to generate high-quality instruction-completion pairs for supervised fine-tuning (SFT). The resulting model, Qwen2.5-Coder-C3, achieves state-of-the-art performance on C3-Bench. Our findings provide valuable insights for enhancing LLMs' code completion and instruction-following capabilities, establishing new directions for future research in code LLMs. To facilitate reproducibility and foster further research in code LLMs, we open-source all code, datasets, and models.

</details>


### [10] [The Role of Cognitive Abilities in Requirements Inspection: Comparing UML and Textual Representations](https://arxiv.org/abs/2601.16009)
*Giovanna Broccia,Sira Vegas,Alessio Ferrari*

Main category: cs.SE

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: The representation of requirements plays a critical role in the accuracy of requirements inspection. While visual representations, such as UML diagrams, are widely used alongside text-based requirements, their effectiveness in supporting inspection is still debated. Cognitive abilities, such as working memory and mental rotation skills, may also influence inspection accuracy. This study aims to evaluate whether the use of UML sequence diagrams alongside text-based requirements improves the accuracy of requirements inspection compared to text-based requirements alone and to explore whether cognitive abilities are associated with differences in performance across the two treatments (text vs text with UML support). We conducted a crossover experiment with 38 participants to assess the accuracy of requirements inspection under the two treatments in terms of issues found and justifications provided. Linear mixed-effects and generalized linear models were used to analyse the effects of treatment, period, sequence, and cognitive abilities. The results indicate a significant three-way interaction between representation type, working memory capacity, and mental rotation ability. This finding suggests that the effectiveness of UML support is not uniform across individuals: participants with high scores in both cognitive abilities experienced reduced performance when using UML for violation detection. Conversely, the same cognitive profile was associated with improved justification accuracy under UML-aided inspection, indicating that higher cognitive abilities may support deeper reasoning processes when dealing with multi-modal information, i.e., diagrams and text.

</details>


### [11] [Towards a Goal-Centric Assessment of Requirements Engineering Methods for Privacy by Design](https://arxiv.org/abs/2601.16080)
*Oleksandr Kosenkov,Ehsan Zabardast,Jannik Fischbach,Tony Gorschek,Daniel Mendez*

Main category: cs.SE

TL;DR: 本文针对GDPR隐私设计标准提出目标导向评估框架，通过整合文献与实践反馈解决现有需求工程方法匹配组织目标的难题。


<details>
  <summary>Details</summary>
Motivation: 现有研究中隐私设计的需求工程方法虽多，但缺乏系统性评估标准帮助组织选择符合自身目标的最佳方案

Method: 结合文献综述、从业者访谈与实践验证，设计并验证目标中心化评估方法

Result: 发现从业者缺乏系统性隐私设计实践，强调评估需基于组织目标而非仅流程特征

Conclusion: 目标 написа中心框架可指导未来隐私设计需求工程的开发、选择与定制化应用

Abstract: Implementing privacy by design (PbD) according to the General Data Protection Regulation (GDPR) is met with a growing number of requirements engineering (RE) approaches. However, the question of which RE method for PbD fits best the goals of organisations remains a challenge. We report our endeavor to close this gap by synthesizing a goal-centric approach for PbD methods assessment. We used literature review, interviews, and validation with practitioners to achieve the goal of our study. As practitioners do not approach PbD systematically, we suggest that RE methods for PbD should be assessed against organisational goals, rather than process characteristics only. We hope that, when further developed, the goal-centric approach could support the development, selection, and tailoring of RE practices for PbD.

</details>


<div id='cs.PF'></div>

# cs.PF [[Back]](#toc)

### [12] [Sawtooth Wavefront Reordering: Enhanced CuTile FlashAttention on NVIDIA GB10](https://arxiv.org/abs/2601.16032)
*Yifan Zhu,Yekai Pan,Chen Ding*

Main category: cs.PF

TL;DR: 本文分析CuůTile-based Flash Attention的内存行为，识别GB10平台上L2缓存未命中的主要原因，并提出锯齿波阵重构技术以优化性能，在CUDA和CuTile环境中验证效果。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型中，高性能注意力内核的L2缓存未命中率高会导致效率瓶颈，GB10平台的性能提升需求驱动本研究。

Method: 引入锯齿波阵重构编程技术，通过重排内存访问降低缓存未命中。

Result: 实验结果显示L2未命中减少50%以上，吞吐量提升高达60%。

Conclusion: 锯齿波阵重构显著提升注意力内核的性能，适用于GB10等高性能计算系统。

Abstract: High-performance attention kernels are essential for Large Language Models. This paper presents analysis of CuTile-based Flash Attention memory behavior and a technique to improve its cache performance. In particular, our analysis on the NVIDIA GB10 (Grace Blackwell) identifies the main cause of L2 cache miss. Leveraging this insight, we introduce a new programming technique called Sawtooth Wavefront Reordering that reduces L2 misses. We validate it in both CUDA and CuTile, observing 50\% or greater reduction in L2 misses and up to 60\% increase in throughput on GB10.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [13] [FlexLLM: Composable HLS Library for Flexible Hybrid LLM Accelerator Design](https://arxiv.org/abs/2601.15710)
*Jiahao Zhang,Zifan He,Nicholas Fraser,Michaela Blott,Yizhou Sun,Jason Cong*

Main category: cs.AR

TL;DR: FlexLLM是一个可组合的高级综合库，用于快速开发定制化LLM加速器，通过阶段定制和量化技术提升性能与能效。


<details>
  <summary>Details</summary>
Motivation: 解决专用领域LLM加速器开发效率低的问题，支持预填充/解码阶段差异化定制及低比特部署需求。

Method: 提供可组合HLS库，暴露架构自由度以实现阶段定制化推理（时间重用/空间数据流分离），集成量化套件和高效长上下文处理的HMT插件。

Result: 1) 两月内用1K代码实现Llama-3.2 1B推理系统；2) U280 FPGA对比A100 GPU：端到端加速1.29倍，解码吞吐量提升1.64倍，能效高3.14倍；3) V80 FPGA投射提升达6.55倍；4) HMT插件降低预填充延迟23.23倍，上下文扩展64倍。

Conclusion: FlexLLM以最小人力成本将LLM算法创新与高性能硬件设计高效结合，显著提升推理效率。

Abstract: We present FlexLLM, a composable High-Level Synthesis (HLS) library for rapid development of domain-specific LLM accelerators. FlexLLM exposes key architectural degrees of freedom for stage-customized inference, enabling hybrid designs that tailor temporal reuse and spatial dataflow differently for prefill and decode, and provides a comprehensive quantization suite to support accurate low-bit deployment. Using FlexLLM, we build a complete inference system for the Llama-3.2 1B model in under two months with only 1K lines of code. The system includes: (1) a stage-customized accelerator with hardware-efficient quantization (12.68 WikiText-2 PPL) surpassing SpinQuant baseline, and (2) a Hierarchical Memory Transformer (HMT) plug-in for efficient long-context processing. On the AMD U280 FPGA at 16nm, the accelerator achieves 1.29$\times$ end-to-end speedup, 1.64$\times$ higher decode throughput, and 3.14$\times$ better energy efficiency than an NVIDIA A100 GPU (7nm) running BF16 inference; projected results on the V80 FPGA at 7nm reach 4.71$\times$, 6.55$\times$, and 4.13$\times$, respectively. In long-context scenarios, integrating the HMT plug-in reduces prefill latency by 23.23$\times$ and extends the context window by 64$\times$, delivering 1.10$\times$/4.86$\times$ lower end-to-end latency and 5.21$\times$/6.27$\times$ higher energy efficiency on the U280/V80 compared to the A100 baseline. FlexLLM thus bridges algorithmic innovation in LLM inference and high-performance accelerators with minimal manual effort.

</details>
