<div id=toc></div>

# Table of Contents

- [cs.MA](#cs.MA) [Total: 7]
- [cs.NI](#cs.NI) [Total: 7]
- [cs.PF](#cs.PF) [Total: 2]
- [cs.SE](#cs.SE) [Total: 16]
- [cs.DC](#cs.DC) [Total: 5]
- [cs.AR](#cs.AR) [Total: 3]


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [1] [Visual Multi-Agent System: Mitigating Hallucination Snowballing via Visual Flow](https://arxiv.org/abs/2509.21789)
*Xinlei Yu,Chengming Xu,Guibin Zhang,Yongbo He,Zhangquan Chen,Zhucun Xue,Jiangning Zhang,Yue Liao,Xiaobin Hu,Yu-Gang Jiang,Shuicheng Yan*

Main category: cs.MA

TL;DR: 多智能体视觉语言模型存在视觉幻觉雪球效应，本文提出ViF方法通过视觉流传递和注意力重分配来缓解该问题，在多个基准测试中显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 多智能体系统中存在视觉幻觉雪球效应，即单个智能体的幻觉会通过文本流在后续智能体中被放大，导致视觉注意力分配减少。

Method: 提出ViF方法：通过选择视觉中继令牌构建视觉流传递智能体间消息，并应用注意力重分配机制来增强视觉模式。

Result: 在基于4种常见MAS结构和10个基础模型的8个基准测试中，该方法显著减少了幻觉雪球效应，一致提升了性能。

Conclusion: ViF是一个轻量级即插即用的缓解范式，通过视觉流传递和注意力重分配有效解决了多智能体视觉幻觉雪球问题。

Abstract: Multi-Agent System (MAS) powered by Visual Language Models (VLMs) enables
challenging tasks but suffers from a novel failure term, multi-agent visual
hallucination snowballing, where hallucinations are seeded in a single agent
and amplified by following ones due to the over-reliance on textual flow to
relay visual information. Through turn-, layer-, and token-wise attention
analyses, we provide detailed insights into the essence of hallucination
snowballing regarding the reduction of visual attention allocation. It leads us
to identify a subset of vision tokens with a unimodal attention peak in middle
layers that best preserve visual evidence but gradually diminish in deeper
agent turns, resulting in the visual hallucination snowballing in MAS. Thus, we
propose ViF, a lightweight, plug-and-play mitigation paradigm that relays
inter-agent messages with Visual Flow powered by the selected visual relay
tokens and applies attention reallocation to amplify this pattern. The
experiment results demonstrate that our method markedly reduces hallucination
snowballing, consistently improving the performance across eight benchmarks
based on four common MAS structures and ten base models. The source code will
be available at: https://github.com/YU-deep/ViF.git.

</details>


### [2] [RobustFlow: Towards Robust Agentic Workflow Generation](https://arxiv.org/abs/2509.21834)
*Shengxiang Xu,Jiayi Zhang,Shimin Di,Yuyu Luo,Liang Yao,Hanmo Liu,Jia Zhu,Fan Liu,Min-Ling Zhang*

Main category: cs.MA

TL;DR: 本文提出RobustFlow训练框架，通过偏好优化提升LLM生成代理工作流的鲁棒性，解决语义相同但表述不同的指令导致工作流不一致的问题。


<details>
  <summary>Details</summary>
Motivation: 当前LLM生成的代理工作流在面对语义相同但表述不同的指令时会产生不一致的结果，这种脆弱性严重影响了其在现实应用中的可靠性和可信度。

Method: 提出基于节点和拓扑相似度的评估指标，并开发RobustFlow训练框架，利用偏好优化技术训练模型对指令变化的鲁棒性。

Result: RobustFlow将工作流鲁棒性评分提升至70%-90%，相比现有方法有显著改进。

Conclusion: 该研究为提升LLM生成代理工作流的稳定性提供了有效的解决方案，代码已开源。

Abstract: The automated generation of agentic workflows is a promising frontier for
enabling large language models (LLMs) to solve complex tasks. However, our
investigation reveals that the robustness of agentic workflow remains a
critical, unaddressed challenge. Current methods often generate wildly
inconsistent workflows when provided with instructions that are semantically
identical but differently phrased. This brittleness severely undermines their
reliability and trustworthiness for real-world applications. To quantitatively
diagnose this instability, we propose metrics based on nodal and topological
similarity to evaluate workflow consistency against common semantic variations
such as paraphrasing and noise injection. Subsequently, we further propose a
novel training framework, RobustFlow, that leverages preference optimization to
teach models invariance to instruction variations. By training on sets of
synonymous task descriptions, RobustFlow boosts workflow robustness scores to
70\% - 90\%, which is a substantial improvement over existing approaches. The
code is publicly available at https://github.com/DEFENSE-SEU/RobustFlow.

</details>


### [3] [Multi-Agent Path Finding via Offline RL and LLM Collaboration](https://arxiv.org/abs/2509.22130)
*Merve Atasever,Matthew Hong,Mihir Nitin Kulkarni,Qingpei Li,Jyotirmoy V. Deshmukh*

Main category: cs.MA

TL;DR: 提出基于决策变换器的高效分散式路径规划框架，结合离线强化学习将训练时间从数周缩短至数小时，并集成GPT-4o提升动态环境适应性


<details>
  <summary>Details</summary>
Motivation: 解决多智能体路径规划中的组合复杂性和部分可观测性问题，克服传统分散式强化学习方法存在的自我中心行为导致频繁碰撞以及复杂通信模块导致训练时间过长的问题

Method: 基于决策变换器的分散式规划框架，利用离线强化学习加速训练，集成GPT-4o动态指导智能体策略以适应环境变化

Result: 在静态和动态变化环境中的大量实验表明，该方法显著提升了适应性和性能，训练时间大幅缩短

Conclusion: 提出的DT框架结合GPT-4o的方法有效解决了MAPF问题的核心挑战，在训练效率和环境适应性方面取得了显著改进

Abstract: Multi-Agent Path Finding (MAPF) poses a significant and challenging problem
critical for applications in robotics and logistics, particularly due to its
combinatorial complexity and the partial observability inherent in realistic
environments. Decentralized reinforcement learning methods commonly encounter
two substantial difficulties: first, they often yield self-centered behaviors
among agents, resulting in frequent collisions, and second, their reliance on
complex communication modules leads to prolonged training times, sometimes
spanning weeks. To address these challenges, we propose an efficient
decentralized planning framework based on the Decision Transformer (DT),
uniquely leveraging offline reinforcement learning to substantially reduce
training durations from weeks to mere hours. Crucially, our approach
effectively handles long-horizon credit assignment and significantly improves
performance in scenarios with sparse and delayed rewards. Furthermore, to
overcome adaptability limitations inherent in standard RL methods under dynamic
environmental changes, we integrate a large language model (GPT-4o) to
dynamically guide agent policies. Extensive experiments in both static and
dynamically changing environments demonstrate that our DT-based approach,
augmented briefly by GPT-4o, significantly enhances adaptability and
performance.

</details>


### [4] [Impact of Collective Behaviors of Autonomous Vehicles on Urban Traffic Dynamics: A Multi-Agent Reinforcement Learning Approach](https://arxiv.org/abs/2509.22216)
*Ahmet Onur Akman,Anastasia Psarou,Zoltán György Varga,Grzegorz Jamróz,Rafał Kucharski*

Main category: cs.MA

TL;DR: 研究探讨了强化学习自动驾驶车辆在混合交通环境中对城市交通流的影响，发现不同行为模式的AV对交通效率和人车交互产生显著差异


<details>
  <summary>Details</summary>
Motivation: 研究自动驾驶车辆在混合交通环境中的行为模式对城市交通流的影响，特别是不同优化目标（自私、协作、竞争等行为）对整体交通效率的影响

Method: 在多智能体环境中使用深度Q学习算法，将1/3人口转换为RL驱动的AV，通过奖励函数实施6种不同行为模式，使用自主研发的PARCOUR框架进行模拟

Result: AV可优化自身行程时间达5%，但对人类驾驶员的影响因行为模式而异；自利行为的AV总能获得比人类更短的行程时间；不同行为模式的学习复杂度差异显著

Conclusion: 多智能体强化学习适用于交通网络集体路由，但其对共存方的影响很大程度上取决于所采用的行为模式，需要谨慎设计AV行为以实现整体交通优化

Abstract: This study examines the potential impact of reinforcement learning
(RL)-enabled autonomous vehicles (AV) on urban traffic flow in a mixed traffic
environment. We focus on a simplified day-to-day route choice problem in a
multi-agent setting. We consider a city network where human drivers travel
through their chosen routes to reach their destinations in minimum travel time.
Then, we convert one-third of the population into AVs, which are RL agents
employing Deep Q-learning algorithm. We define a set of optimization targets,
or as we call them behaviors, namely selfish, collaborative, competitive,
social, altruistic, and malicious. We impose a selected behavior on AVs through
their rewards. We run our simulations using our in-house developed RL framework
PARCOUR. Our simulations reveal that AVs optimize their travel times by up to
5\%, with varying impacts on human drivers' travel times depending on the AV
behavior. In all cases where AVs adopt a self-serving behavior, they achieve
shorter travel times than human drivers. Our findings highlight the complexity
differences in learning tasks of each target behavior. We demonstrate that the
multi-agent RL setting is applicable for collective routing on traffic
networks, though their impact on coexisting parties greatly varies with the
behaviors adopted.

</details>


### [5] [VizGen: Data Exploration and Visualization from Natural Language via a Multi-Agent AI Architecture](https://arxiv.org/abs/2509.22218)
*Sandaru Fernando,Imasha Jayarathne,Sithumini Abeysekara,Shanuja Sithamparanthan,Thushari Silva,Deshan Jayawardana*

Main category: cs.MA

TL;DR: VizGen是一个AI辅助的图表生成系统，使用自然语言让用户创建有意义的数据可视化，无需技术专业知识。


<details>
  <summary>Details</summary>
Motivation: 传统数据可视化工具需要技术专业知识，限制了普通用户的可访问性，需要更直观的解决方案。

Method: 采用多智能体架构，利用先进的NLP和LLM（Claude 3.7 Sonnet和Gemini 2.0 Flash）将用户查询转换为SQL并推荐合适的图表类型，支持实时SQL数据库交互和对话式图表优化。

Result: 系统能够生成可视化图表，分析数据模式、异常和相关性，并通过互联网获取的上下文信息提供丰富的解释。

Conclusion: VizGen通过弥合技术复杂性和用户友好设计之间的差距，实现了数据可视化的民主化，使数据分析变得直观易用。

Abstract: Data visualization is essential for interpreting complex datasets, yet
traditional tools often require technical expertise, limiting accessibility.
VizGen is an AI-assisted graph generation system that empowers users to create
meaningful visualizations using natural language. Leveraging advanced NLP and
LLMs like Claude 3.7 Sonnet and Gemini 2.0 Flash, it translates user queries
into SQL and recommends suitable graph types. Built on a multi-agent
architecture, VizGen handles SQL generation, graph creation, customization, and
insight extraction. Beyond visualization, it analyzes data for patterns,
anomalies, and correlations, and enhances user understanding by providing
explanations enriched with contextual information gathered from the internet.
The system supports real-time interaction with SQL databases and allows
conversational graph refinement, making data analysis intuitive and accessible.
VizGen democratizes data visualization by bridging the gap between technical
complexity and user-friendly design.

</details>


### [6] [Effective Policy Learning for Multi-Agent Online Coordination Beyond Submodular Objectives](https://arxiv.org/abs/2509.22596)
*Qixin Zhang,Yan Sun,Can Jin,Xikun Zhang,Yao Shu,Puning Zhao,Li Shen,Dacheng Tao*

Main category: cs.MA

TL;DR: 提出了两种有效的多智能体在线协调策略学习算法：MA-SPL和MA-MPL。MA-SPL能够处理子模、α-弱DR子模和(γ,β)-弱子模目标函数，MA-MPL是完全参数无关的算法且保持相同近似比。核心创新是提出了基于策略的连续扩展技术。


<details>
  <summary>Details</summary>
Motivation: 解决多智能体在线协调问题中传统方法对未知参数的依赖问题，以及处理弱子模目标函数时的挑战。

Method: 提出了两种算法：MA-SPL（需要参数α,γ,β）和MA-MPL（参数无关）。核心是基于策略的连续扩展技术，相比传统的多线性扩展能提供无损舍入方案。

Result: MA-SPL算法在子模目标下达到最优(1-c/e)近似保证，并能处理弱子模场景。MA-MPL在无需参数的情况下保持相同性能。通过大量仿真验证了有效性。

Conclusion: 提出的基于策略的连续扩展技术为处理弱子模目标函数提供了有效解决方案，两种算法在多智能体在线协调问题上表现出色，MA-MPL尤其具有实用性优势。

Abstract: In this paper, we present two effective policy learning algorithms for
multi-agent online coordination(MA-OC) problem. The first one, \texttt{MA-SPL},
not only can achieve the optimal $(1-\frac{c}{e})$-approximation guarantee for
the MA-OC problem with submodular objectives but also can handle the unexplored
$\alpha$-weakly DR-submodular and $(\gamma,\beta)$-weakly submodular scenarios,
where $c$ is the curvature of the investigated submodular functions, $\alpha$
denotes the diminishing-return(DR) ratio and the tuple $(\gamma,\beta)$
represents the submodularity ratios. Subsequently, in order to reduce the
reliance on the unknown parameters $\alpha,\gamma,\beta$ inherent in the
\texttt{MA-SPL} algorithm, we further introduce the second online algorithm
named \texttt{MA-MPL}. This \texttt{MA-MPL} algorithm is entirely
\emph{parameter-free} and simultaneously can maintain the same approximation
ratio as the first \texttt{MA-SPL} algorithm. The core of our \texttt{MA-SPL}
and \texttt{MA-MPL} algorithms is a novel continuous-relaxation technique
termed as \emph{policy-based continuous extension}. Compared with the
well-established \emph{multi-linear extension}, a notable advantage of this new
\emph{policy-based continuous extension} is its ability to provide a lossless
rounding scheme for any set function, thereby enabling us to tackle the
challenging weakly submodular objectives. Finally, extensive simulations are
conducted to validate the effectiveness of our proposed algorithms.

</details>


### [7] [Voting-Bloc Entropy: A New Metric for DAO Decentralization](https://arxiv.org/abs/2509.22620)
*Andrés Fábrega,Amy Zhao,Jay Yu,James Austgen,Sarah Allen,Kushal Babel,Mahimna Kelkar,Ari Juels*

Main category: cs.MA

TL;DR: 提出了一个基于投票集团熵（VBE）的新框架来衡量DAO的去中心化程度，该框架通过测量参与者在多轮投票中效用函数的相似性来量化去中心化


<details>
  <summary>Details</summary>
Motivation: 现有的去中心化定义无法捕捉DAO中多元化和公平参与的关键特性，需要一个新的理论框架来准确衡量DAO的去中心化程度

Method: 基于强化学习的投票概念模型，推导出VBE测量方法；通过理论证明和实证研究验证VBE的有效性；开发开源工具进行实际测量和治理实验

Result: 证明了投票委托、提案捆绑、贿赂等因素对（去）中心化的影响；提出了增强DAO去中心化的实用建议；通过测量研究和治理实验展示了VBE的实证应用价值

Conclusion: VBE提供了一个从第一性原理出发的理论框架，能够更准确地衡量DAO的去中心化程度，并为改善DAO治理提供了实用工具和建议

Abstract: Decentralized Autonomous Organizations (DAOs) use smart contracts to foster
communities working toward common goals. Existing definitions of
decentralization, however -- the 'D' in DAO -- fall short of capturing the key
properties characteristic of diverse and equitable participation. This work
proposes a new framework for measuring DAO decentralization called Voting-Bloc
Entropy (VBE, pronounced ''vibe''). VBE is based on the idea that voters with
closely aligned interests act as a centralizing force and should be modeled as
such. VBE formalizes this notion by measuring the similarity of participants'
utility functions across a set of voting rounds. Unlike prior, ad hoc
definitions of decentralization, VBE derives from first principles: We
introduce a simple (yet powerful) reinforcement learning-based conceptual model
for voting, that in turn implies VBE. We first show VBE's utility as a
theoretical tool. We prove a number of results about the (de)centralizing
effects of vote delegation, proposal bundling, bribery, etc. that are
overlooked in previous notions of DAO decentralization. Our results lead to
practical suggestions for enhancing DAO decentralization. We also show how VBE
can be used empirically by presenting measurement studies and VBE-based
governance experiments. We make the tools we developed for these results
available to the community in the form of open-source artifacts in order to
facilitate future study of DAO decentralization.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [8] [A Target-Agnostic Protocol-Independent Interface for the Transport Layer](https://arxiv.org/abs/2509.21550)
*Pedro Mizuno,Kimiya Mohammadtaheri,Linfan Qian,Joshua Johnson,Danny Akbarzadeh,Chris Neely,Mario Baldi,Nacihket Kapre,Mina Tahmasbi Arashloo*

Main category: cs.NI

TL;DR: 提出TINF框架，用高级编程抽象实现目标无关的传输协议开发，支持多种后端部署和协议验证


<details>
  <summary>Details</summary>
Motivation: 传输协议需要适应不同的应用、工作负载和网络架构，但现有开发方式缺乏统一的高层抽象，导致开发效率低且难以验证

Method: 设计TINF编程框架，使用类C语言构造来定义传输协议，将协议描述为事件驱动的状态机，生成目标无关的指令

Result: 成功开发了多个传输协议，实现了DPDK和Linux eXpress DataPath两个后端，并在两个后端上部署了TINF程序

Conclusion: 目标无关的传输编程可以显著减少传输协议开发工作量，支持自动化分析和形式化验证，推动传输层可编程目标的研究

Abstract: Transport protocols are fundamental to network communications, continuously
evolving to meet the demands of new applications, workloads, and network
architectures while running in a wide range of execution environments (a.k.a
targets). We argue that this diversity across protocols and targets calls for a
high-level, target-agnostic programming abstraction for the transport layer.
Specifically, we propose to specify transport protocols as high-level programs
that take an event and flow state as input, and using constrained C-like
constructs, produce the updated state along with target-agnostic instructions
for key transport operations such as data reassembly, packet generation and
scheduling, and timer manipulations.
  We show the benefits of our high-level transport programs by developing
multiple transport protocols in our programming framework called TINF,
developing two TINF- compliant backends, one in DPDK and one in Linux eXpress
DataPath, and deploying TINF programs for multiple protocols across both
backends. Inspired by the benefits unlocked by L2/L3 packet-processing
languages like P4, we believe target-agnostic transport programs can reduce the
development effort for transport protocols, enable automated analysis and
formal verification of the transport layer, and further research in
programmable targets for transport protocols.

</details>


### [9] [Context-Aware Hybrid Routing in Bluetooth Mesh Networks Using Multi-Model Machine Learning and AODV Fallback](https://arxiv.org/abs/2509.21490)
*Md Sajid Islam,Tanvir Hasan*

Main category: cs.NI

TL;DR: 提出了一种基于监督机器学习的混合智能路由框架，在蓝牙mesh网络中通过集成多个预测模型来改进AODV路由协议的下一跳选择，显著提高了数据包传输成功率。


<details>
  <summary>Details</summary>
Motivation: 传统AODV路由策略在拥塞和动态拓扑变化下性能下降，需要一种更可靠的蓝牙mesh网络路由方案来应对紧急和资源受限场景中的离线通信需求。

Method: 开发了一个混合智能路由框架，将四个预测模型（投递成功分类器、TTL回归器、延迟回归器和转发器适用性分类器）集成到统一的评分机制中，动态评估邻居节点在多跳消息传输中的优先级。

Result: 在10个场景的模拟测试中，完整的混合ML模型（ABCD）实现了约99.97%的数据包投递成功率，显著优于基线AODV和部分混合模型。

Conclusion: 轻量级、可解释的机器学习模型能够有效提升蓝牙mesh网络的路由可靠性和适应性，特别是在无基础设施环境中，投递成功率比延迟约束更重要。

Abstract: Bluetooth-based mesh networks offer a promising infrastructure for offline
communication in emergency and resource constrained scenarios. However,
traditional routing strategies such as Ad hoc On-Demand Distance Vector (AODV)
often degrade under congestion and dynamic topological changes. This study
proposes a hybrid intelligent routing framework that augments AODV with
supervised machine learning to improve next-hop selection under varied network
constraints. The framework integrates four predictive models: a delivery
success classifier, a TTL regressor, a delay regressor, and a forwarder
suitability classifier, into a unified scoring mechanism that dynamically ranks
neighbors during multi-hop message transmission. A simulation environment with
stationary node deployments was developed, incorporating buffer constraints and
device heterogeneity to evaluate three strategies: baseline AODV, a partial
hybrid ML model (ABC), and the full hybrid ML model (ABCD). Across ten
scenarios, the Hybrid ABCD model achieves approximately 99.97 percent packet
delivery under these controlled conditions, significantly outperforming both
the baseline and intermediate approaches. The results demonstrate that
lightweight, explainable machine learning models can enhance routing
reliability and adaptability in Bluetooth mesh networks, particularly in
infrastructure-less environments where delivery success is prioritized over
latency constraints.

</details>


### [10] [eXplainable Artificial Intelligence for RL-based Networking Solutions](https://arxiv.org/abs/2509.21649)
*Yeison Stiven Murcia,Oscar Mauricio Caicedo,Daniela Maria Casas,Nelson Luis Saldanha da Fonseca*

Main category: cs.NI

TL;DR: eXplaNet是一个基于可解释AI的管道，旨在帮助网络研究人员理解RL智能体的决策过程，并通过改进奖励函数来优化基于Q学习的路由解决方案。


<details>
  <summary>Details</summary>
Motivation: 虽然强化学习代理在网络任务中广泛应用，但理解其决策过程对于在网络和网络管理中更广泛采用至关重要。

Method: 引入eXplaNet管道，基于可解释人工智能技术，用于分析RL智能体的决策过程，并通过改进奖励函数来优化Q学习路由解决方案。

Result: 展示了eXplaNet如何应用于改进基于Q学习代理的路由解决方案，特别是通过优化奖励函数来提升性能。

Conclusion: 讨论了将可解释性融入RL以更好优化网络性能的机会和挑战，强调了可解释AI在网络RL应用中的重要性。

Abstract: Reinforcement Learning (RL) agents have been widely used to improve
networking tasks. However, understanding the decisions made by these agents is
essential for their broader adoption in networking and network management. To
address this, we introduce eXplaNet - a pipeline grounded in explainable
artificial intelligence - designed to help networking researchers and
practitioners gain deeper insights into the decision-making processes of
RL-based solutions. We demonstrate how eXplaNet can be applied to refine a
routing solution powered by a Q-learning agent, specifically by improving its
reward function. In addition, we discuss the opportunities and challenges of
incorporating explainability into RL to better optimize network performance.

</details>


### [11] [XenoFlow: How Fast Can a SmartNIC-Based DNS Load Balancer Run?](https://arxiv.org/abs/2509.21656)
*Max Schrötter,Sten Heimbrodt,Bettina Schnor*

Main category: cs.NI

TL;DR: XenoFlow是基于Bluefield-3可编程NIC的负载均衡器，相比基于eBPF的软件方案延迟降低44%，但在Flow Pipe条目较少时无法达到线速


<details>
  <summary>Details</summary>
Motivation: 随着可编程网络硬件的发展，越来越多的功能可以从通用CPU软件迁移到NIC上。早期NIC只能卸载固定功能，而现代NIC如Bluefield-3支持完全可编程数据平面

Method: 开发了名为XenoFlow的负载均衡器，在Bluefield-3 eSwitch上运行，并测试其能力和限制

Result: Bluefield-3在Flow Pipe只有2个条目时无法达到线速，但硬件卸载和靠近网络的优势明显。XenoFlow相比基于eBPF的负载均衡器延迟降低44%，且在高负载下仍能保持低延迟

Conclusion: 可编程NIC硬件卸载具有显著优势，能够大幅降低延迟，但需要注意硬件限制（如Flow Pipe条目数量）对性能的影响

Abstract: With the advent of programmable network hardware, more and more
  functionality can be moved from software running on general purpose CPUs to
  the NIC. Early NICs only allowed offloading fixed functions like checksum
  computation. Recent NICs like the Nvidia Bluefield-3 allow a fully
  programmable dataplane. In this paper, we present our first steps towards a
  load balancer named XenoFlow running on the Bluefield-3. Furthermore, we
  show the capabilities and limitations of the Bluefield-3 eSwitch. Our
  results show that the Bluefield-3 will not achieve line rate with only 2
  entries in a Flow Pipe. However, we also show the adventages of hardware
  offloading on the NIC and being closer to the network. With XenoFlow, we
  achieve an 44% lower latency compared to a comparable eBPF-based load
  balancer running on the host. Furthermore, XenoFlow achieves this low
  latency even under high load.

</details>


### [12] [Evaluating Open-Source Large Language Models for Technical Telecom Question Answering](https://arxiv.org/abs/2509.21949)
*Arina Caraus,Alessio Buscemi,Sumit Kumar,Ion Turcanu*

Main category: cs.NI

TL;DR: 评估Gemma 3 27B和DeepSeek R1 32B两个开源大语言模型在无线通信技术领域的表现，发现Gemma在语义保真度和正确性评分方面更优，DeepSeek在词汇一致性方面略胜一筹


<details>
  <summary>Details</summary>
Motivation: 大语言模型在技术领域（特别是电信领域）的性能尚未充分探索，需要评估其在专业领域的实际表现

Method: 构建105个问答对的基准测试，使用词汇指标、语义相似度和LLM作为评判者进行评分，并通过来源归因和分数方差分析一致性和幻觉问题

Result: Gemma在语义保真度和LLM评分正确性方面表现更好，DeepSeek在词汇一致性方面略有优势，同时揭示了当前模型在电信应用中的局限性

Conclusion: 需要领域适配的模型来支持工程领域可信赖的AI助手，当前开源LLM在专业技术领域仍有改进空间

Abstract: Large Language Models (LLMs) have shown remarkable capabilities across
various fields. However, their performance in technical domains such as
telecommunications remains underexplored. This paper evaluates two open-source
LLMs, Gemma 3 27B and DeepSeek R1 32B, on factual and reasoning-based questions
derived from advanced wireless communications material. We construct a
benchmark of 105 question-answer pairs and assess performance using lexical
metrics, semantic similarity, and LLM-as-a-judge scoring. We also analyze
consistency, judgment reliability, and hallucination through source attribution
and score variance. Results show that Gemma excels in semantic fidelity and
LLM-rated correctness, while DeepSeek demonstrates slightly higher lexical
consistency. Additional findings highlight current limitations in telecom
applications and the need for domain-adapted models to support trustworthy
Artificial Intelligence (AI) assistants in engineering.

</details>


### [13] [Extreme Value Theory-enhanced Radio Maps for Handovers in Ultra-reliable Communications](https://arxiv.org/abs/2509.22547)
*Dian Echevarría Pérez,Onel L. Alcaraz López,Hirley Alves*

Main category: cs.NI

TL;DR: 提出了一种基于物理层视角的新型切换框架，利用极值理论和统计无线电地图预测信号行为，优化切换时机和位置决策，提高超可靠通信系统的服务可用性和能效。


<details>
  <summary>Details</summary>
Motivation: 超可靠通信系统对切换策略有严格性能要求，需要确保无缝切换和系统性能，同时减少乒乓效应。

Method: 采用极值理论和统计无线电地图来预测信号行为，确定最佳切换时间和位置，实现高效的资源分配和空间位置协调。

Result: 相比传统切换机制，该策略提供了更优的服务可用性和能源效率，在超可靠通信环境中表现出色。

Conclusion: 该物理层视角的切换框架能有效提升超可靠通信系统的切换性能，通过预测性资源分配和协调机制实现无缝切换和能效优化。

Abstract: Efficient handover (HO) strategies are essential for maintaining the
stringent performance requirements of ultra-reliable communication (URC)
systems. This work introduces a novel HO framework designed from a
physical-layer perspective, where the decision-making process focuses on
determining the optimal time and location for performing HOs. Leveraging
extreme value theory (EVT) and statistical radio maps, the proposed method
predicts signal behaviour and enables efficient resource allocation. The
framework ensures seamless HOs and improved system performance by facilitating
effective resource transitions and coordination across spatial locations while
incorporating mechanisms to mitigate the ping-pong effect. Comparative
evaluations demonstrate that this strategy provides superior service
availability and energy efficiency than traditional HO mechanisms, highlighting
its effectiveness in URC environments.

</details>


### [14] [Bridging Technical Capability and User Accessibility: Off-grid Civilian Emergency Communication](https://arxiv.org/abs/2509.22568)
*Karim Khamaisi,Oliver Kamer,Bruno Rodrigues,Jan von der Assen,Burkhard Stiller*

Main category: cs.NI

TL;DR: 提出了一种集成低功耗长距离网络和危机专用智能手机应用的统一应急通信系统，在基础设施中断时实现去中心化的离线民用通信


<details>
  <summary>Details</summary>
Motivation: 大规模危机期间蜂窝和互联网基础设施中断时，平民缺乏可靠的通信、援助协调和获取可信信息的方法

Method: 将868MHz频段的长距离网络（LongFast配置）与专用移动应用结合，支持点对点消息、身份验证和社区审核功能

Result: 在苏黎世城市环境中实现1.2公里通信范围，92%的数据包传输率；移动应用通过需求分析验证了功能性

Conclusion: 该系统成功将物理层韧性和用户层可用性整合为统一的危机应对框架，在现实基础设施退化条件下验证了网络鲁棒性

Abstract: During large-scale crises disrupting cellular and Internet infrastructure,
civilians lack reliable methods for communication, aid coordination, and access
to trustworthy information. This paper presents a unified emergency
communication system integrating a low-power, long-range network with a
crisis-oriented smartphone application, enabling decentralized and off-grid
civilian communication. Unlike previous solutions separating physical layer
resilience from user layer usability, our design merges these aspects into a
cohesive crisis-tailored framework.
  The system is evaluated in two dimensions: communication performance and
application functionality. Field experiments in urban Z\"urich demonstrate that
the 868 MHz band, using the LongFast configuration, achieves a communication
range of up to 1.2 km with 92% Packet Delivery Ratio, validating network
robustness under real-world infrastructure degraded conditions. In parallel, a
purpose-built mobile application featuring peer-to-peer messaging, identity
verification, and community moderation was evaluated through a
requirements-based analysis.

</details>


<div id='cs.PF'></div>

# cs.PF [[Back]](#toc)

### [15] [Size-Aware Dispatching to Fluid Queues](https://arxiv.org/abs/2509.21693)
*Runhan Xie,Esa Hyytiä,Rhonda Righter*

Main category: cs.PF

TL;DR: 提出了一种用于路由问题的流体流模型，将离散作业调度问题转化为n维空间中的最优路径问题，使用变分法分析最优策略结构


<details>
  <summary>Details</summary>
Motivation: 解决并行服务器系统中基于作业大小信息的路由问题，以最小化平均延迟，将离散作业调度问题转化为连续的流体流模型

Method: 开发流体流模型，将不同大小的作业视为流体颗粒，使用变分法在n维空间中寻找最优路径来清空系统

Result: 通过变分法刻画了最优策略的结构特征，数值示例进一步阐明了流体路由问题和大型分布式服务系统的最优控制

Conclusion: 流体流模型为路由问题提供了有效的分析框架，变分法能够有效刻画最优控制策略的结构，对大型分布式服务系统的优化控制具有指导意义

Abstract: We develop a fluid-flow model for routing problems, where fluid consists of
different size particles and the task is to route the incoming fluid to $n$
parallel servers using the size information in order to minimize the mean
latency. The problem corresponds to the dispatching problem of (discrete) jobs
arriving according to a stochastic process. In the fluid model the problem
reduces to finding an optimal path to empty the system in $n$-dimensional
space. We use the calculus of variation to characterize the structure of
optimal policies. Numerical examples shed further light on the fluid routing
problem and the optimal control of large distributed service systems.

</details>


### [16] [SAHM: State-Aware Heterogeneous Multicore for Single-Thread Performance](https://arxiv.org/abs/2509.22405)
*Shayne Wadle,Karthikeyan Sankaralingam*

Main category: cs.PF

TL;DR: SAHM是一种新型异构多核架构，通过识别单线程工作负载的16种不同行为状态，使用专门优化的核心并在运行时基于状态检测进行线程迁移，实现了17%的性能提升。


<details>
  <summary>Details</summary>
Motivation: 传统单线程性能提升方法（如深度推测、宽流水线、复杂乱序执行）面临收益递减的挑战，需要新的架构设计思路。

Method: 通过性能计数器数据分析定义16种行为状态，设计专门针对特定状态优化的核心集合，在运行时基于检测到的行为状态进行线程迁移调度。

Result: 实验结果显示在现实场景中可实现17%的速度提升，即使在高成本迁移情况下性能下降也小于1%，在单线程和多程序场景中都表现出色。

Conclusion: 状态感知的核心专门化是提升单线程性能的新途径，能够在不增加过大面积、功耗或复杂度成本的情况下实现可组合的微架构增强。

Abstract: Improving single-thread performance remains a critical challenge in modern
processor design, as conventional approaches such as deeper speculation, wider
pipelines, and complex out-of-order execution face diminishing returns. This
work introduces SAHM-State-Aware Heterogeneous Multicore-a novel architecture
that targets performance gains by exploiting fine-grained, time-varying
behavioral diversity in single-threaded workloads. Through empirical
characterization of performance counter data, we define 16 distinct behavioral
states representing different microarchitectural demands. Rather than
over-provisioning a monolithic core with all optimizations, SAHM uses a set of
specialized cores tailored to specific states and migrates threads at runtime
based on detected behavior. This design enables composable microarchitectural
enhancements without incurring prohibitive area, power, or complexity costs.
  We evaluate SAHM in both single-threaded and multiprogrammed scenarios,
demonstrating its ability to maintain core utilization while improving overall
performance through intelligent state-driven scheduling. Experimental results
show opportunity for 17% speed up in realistic scenarios. These speed ups are
robust against high-cost migration, decreasing by less than 1%. Overall,
state-aware core specialization is a new path forward for enhancing
single-thread performance.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [17] [Extracting Conceptual Knowledge to Locate Software Issues](https://arxiv.org/abs/2509.21427)
*Ying Wang,Wenjun Mao,Chong Wang,Zhenhao Zhou,Yicheng Zhou,Wenyun Zhao,Yiling Lou,Xin Peng*

Main category: cs.SE

TL;DR: RepoLens通过提取代码库的概念知识，将细粒度功能重组为高层次关注点，有效解决了大型代码库中的关注点混合和分散问题，显著提升了问题定位的准确性。


<details>
  <summary>Details</summary>
Motivation: 现有LLM方法在大规模代码库中面临关注点混合（相关逻辑被埋没在大函数中）和关注点分散（相关逻辑分散在不同文件中）的问题，导致问题定位效果不佳。

Method: RepoLens采用两阶段方法：离线阶段提取和丰富概念知识构建知识库；在线阶段检索问题相关术语，聚类和排序关注点，通过最小侵入式提示增强集成到定位流程中。

Result: 在SWE-Lancer-Loc基准测试中，RepoLens显著提升了三种先进工具的性能，平均Hit@k提升22%以上，Recall@k提升46%。在不同模型上Hit@1和Recall@10最高提升504%和376%。

Conclusion: RepoLens通过概念知识抽象和关注点重组有效解决了代码库问题定位的挑战，具有很好的泛化性和可靠性。

Abstract: Issue localization, which identifies faulty code elements such as files or
functions, is critical for effective bug fixing. While recent LLM-based and
LLM-agent-based approaches improve accuracy, they struggle in large-scale
repositories due to concern mixing, where relevant logic is buried in large
functions, and concern scattering, where related logic is dispersed across
files.
  To address these challenges, we propose RepoLens, a novel approach that
abstracts and leverages conceptual knowledge from code repositories. RepoLens
decomposes fine-grained functionalities and recomposes them into high-level
concerns, semantically coherent clusters of functionalities that guide LLMs. It
operates in two stages: an offline stage that extracts and enriches conceptual
knowledge into a repository-wide knowledge base, and an online stage that
retrieves issue-specific terms, clusters and ranks concerns by relevance, and
integrates them into localization workflows via minimally intrusive prompt
enhancements. We evaluate RepoLens on SWE-Lancer-Loc, a benchmark of 216 tasks
derived from SWE-Lancer. RepoLens consistently improves three state-of-the-art
tools, namely AgentLess, OpenHands, and mini-SWE-agent, achieving average gains
of over 22% in Hit@k and 46% in Recall@k for file- and function-level
localization. It generalizes across models (GPT-4o, GPT-4o-mini, GPT-4.1) with
Hit@1 and Recall@10 gains up to 504% and 376%, respectively. Ablation studies
and manual evaluation confirm the effectiveness and reliability of the
constructed concerns.

</details>


### [18] [Lost in Transition: The Struggle of Women Returning to Software Engineering Research after Career Breaks](https://arxiv.org/abs/2509.21533)
*Shalini Chakraborty,Sebastian Baltes*

Main category: cs.SE

TL;DR: 该研究探讨女性软件工程师在职业中断后重返学术界面临的挑战，比较不同国家的政策差异，旨在为学术机构提供支持女性回归的透明招聘实践建议。


<details>
  <summary>Details</summary>
Motivation: IT行业为女性重返职场提供了多种支持途径，但学术界缺乏类似的激励措施。女性因怀孕、移民身份或缺乏灵活工作选择等原因的职业中断会严重影响其职业发展，造成重返学术界的障碍。

Method: 采用多元文化研究项目，在多个国家和大学开展研究，通过比较分析不同国家的现有政策和机会，探索女性重返学术角色时遇到的具体挑战。

Result: 研究发现学术界相比行业在性别多样性政策方面不够突出且常被低估，识别了女性重返学术界面临的具体障碍和制度性挑战。

Conclusion: 研究将为学术机构提供支持女性回归的建议，推动透明招聘实践，帮助消除女性重返学术界的障碍。

Abstract: The IT industry provides supportive pathways such as returnship programs,
coding boot camps, and buddy systems for women re-entering their job after a
career break. Academia, however, offers limited opportunities to motivate women
to return. We propose a diverse multicultural research project investigating
the challenges faced by women with software engineering (SE) backgrounds
re-entering academia or related research roles after a career break. Career
disruptions due to pregnancy, immigration status, or lack of flexible work
options can significantly impact women's career progress, creating barriers for
returning as lecturers, professors, or senior researchers. Although many
companies promote gender diversity policies, such measures are less prominent
and often under-recognized within academic institutions. Our goal is to explore
the specific challenges women encounter when re-entering academic roles
compared to industry roles; to understand the institutional perspective,
including a comparative analysis of existing policies and opportunities in
different countries for women to return to the field; and finally, to provide
recommendations that support transparent hiring practices. The research project
will be carried out in multiple universities and in multiple countries to
capture the diverse challenges and policies that vary by location.

</details>


### [19] [No More Manual Guides: Automatic and Scalable Generation of High-Quality Excel Tutorials](https://arxiv.org/abs/2509.21816)
*Yuhang Xie,Jian Mu,Xiaojun Ma,Chaoyun Zhang,Lu Wang,Mengyu Zhou,Mugeng Liu,Si Qin,Qingwei Lin,Saravan Rajmohan,Shi Han,Dongmei Zhang*

Main category: cs.SE

TL;DR: 首个从自然语言任务描述自动生成Excel教程的框架，通过执行代理规划执行解决方案，生成结构化文档和视频演示，显著提升任务执行成功率并大幅降低人工成本。


<details>
  <summary>Details</summary>
Motivation: Excel功能复杂导致用户需要大量教程，但现有教程依赖专家手动编写，更新成本高且无法自动化，需要解决自动化生成高质量教程的问题。

Method: 提出包含执行代理的框架，首先实例化任务，然后规划执行Excel解决方案，收集中间产物并转换为结构化文档和视频教程。

Result: 在1559个真实场景任务上测试，任务执行成功率比现有最佳方法提高8.5%，生成教程可读性和教学效果接近或超越专家编写材料，时间成本降至专家编写的1/20。

Conclusion: 该框架首次实现了高质量Excel教程的自动化生成，消除了人工劳动，使大规模高质量教程生成变得可行。

Abstract: Excel is one of the most widely used productivity tools across domains,
offering rich functionality but also overwhelming users with its complexity.
This creates a persistent demand for tutorials to support effective usage.
However, existing tutorials are manually authored by experts, require frequent
updates after each software release, and incur substantial labor costs. Prior
work has not achieved fully automated tutorial generation, since existing
methods still depend on handcrafted operation sequences or example materials.
In this paper, we present the first framework for automatically generating
Excel tutorials directly from natural language task descriptions. Our framework
first instantiates the task. Then a central component of this framework,
Execution Agent, plans and executes the solution in Excel, and collects the
intermediate artifacts required for tutorial construction. These artifacts are
then transformed into both structured Excel documents and video demonstrations.
To build a comprehensive tutorial corpus, we collected 1,559 task descriptions
from real-world scenarios. In addition, we designed a systematic evaluation
framework that integrates assessments from both large language models (LLMs)
and human reviewers. Experimental results show that our framework improves task
execution success rates by 8.5% over state-of-the-art baselines. Moreover, the
generated tutorials demonstrate superior readability and instructional
effectiveness, often approaching or surpassing expert-authored materials.
Importantly, the automated pipeline eliminates manual labor and reduces time
costs to 1/20 of expert authoring, making scalable and high-quality tutorial
generation practical for the first time.

</details>


### [20] [Software Engineering Data Analytics: A Framework Based on a Multi-Layered Abstraction Mechanism](https://arxiv.org/abs/2509.21881)
*Chaman Wijesiriwardana,Prasad Wimalaratne*

Main category: cs.SE

TL;DR: 提出了一个针对软件分析领域的特定领域框架，通过查询、建模和集成异构软件仓库来实现软件分析。


<details>
  <summary>Details</summary>
Motivation: 为了解决异构软件仓库的数据集成和分析问题，提供一个统一的框架来支持软件分析任务。

Method: 采用多层抽象机制，包含特定领域操作符，并通过案例研究展示方法的可行性。

Result: 框架能够有效支持软件仓库的查询、建模和集成，案例研究证明了该方法的潜力。

Conclusion: 该特定领域框架为软件分析提供了一个有效的解决方案，通过多层抽象和领域特定操作符实现了异构软件仓库的集成和分析。

Abstract: This paper presents a concept of a domain-specific framework for software
analytics by enabling querying, modeling, and integration of heterogeneous
software repositories. The framework adheres to a multi-layered abstraction
mechanism that consists of domain-specific operators. We showcased the
potential of this approach by employing a case study.

</details>


### [21] [AgentPack: A Dataset of Code Changes, Co-Authored by Agents and Humans](https://arxiv.org/abs/2509.21891)
*Yangtian Zi,Zixuan Wu,Aleksander Boruch-Gruszecki,Jonathan Bell,Arjun Guha*

Main category: cs.SE

TL;DR: AgentPack是一个包含130万代码编辑的数据集，来自AI编程助手与人类合作的提交，相比传统人工提交数据质量更高，可用于训练更优秀的代码编辑模型。


<details>
  <summary>Details</summary>
Motivation: 传统基于人工提交数据的代码编辑模型训练存在噪音问题：提交信息简略、多个无关编辑混杂、包含机器人提交。而AI编程助手与人类合作的提交更加专注、目标明确，且经过人类维护者筛选，质量更高。

Method: 收集了截至2025年8月中的公开GitHub项目中由Claude Code、OpenAI Codex和Cursor Agent共同创作的130万代码编辑，构建了AgentPack数据集，并开发了识别和筛选流程。

Result: 分析了这些AI助手的采用趋势和编辑结构特性，证明在AgentPack上微调的模型性能优于基于传统人工提交数据训练的模型。

Conclusion: 利用软件工程AI助手产生的公开数据训练未来的代码编辑模型具有巨大潜力，AgentPack为这一方向提供了高质量的数据资源。

Abstract: Fine-tuning large language models for code editing has typically relied on
mining commits and pull requests. The working hypothesis has been that commit
messages describe human intent in natural language, and patches to code
describe the changes that implement that intent. However, much of the
previously collected data is noisy: commit messages are terse, human-written
commits commingle several unrelated edits, and many commits come from simple,
rule-based bots.
  The recent adoption of software engineering agents changes this landscape.
Code changes co-authored by humans and agents tend to be more narrowly scoped
and focused on clearer goals. Their commit messages, generated by LLMs,
articulate intent and rationale in much greater detail. Moreover, when these
changes land in public repositories, they are implicitly filtered by humans:
maintainers discard low-quality commits to their projects.
  We present AgentPack, a corpus of 1.3M code edits co-authored by Claude Code,
OpenAI Codex, and Cursor Agent across public GitHub projects up to mid-August
2025. We describe the identification and curation pipeline, quantify adoption
trends of these agents, and analyze the structural properties of the edits.
Finally, we show that models fine-tuned on AgentPack can outperform models
trained on prior human-only commit corpora, highlighting the potential of using
public data from software engineering agents to train future code-editing
models.

</details>


### [22] [Unveiling Many Faces of Surrogate Models for Configuration Tuning: A Fitness Landscape Analysis Perspective](https://arxiv.org/abs/2509.21945)
*Pengzhou Chen,Hongyuan Liang,Tao Chen*

Main category: cs.SE

TL;DR: 本文通过适应性景观分析的新视角，系统探讨了配置调优中代理模型的作用，提出了基于模型有用性而非准确性的评估理论，并开发了Model4Tune工具来自动预测最佳模型-调优器组合。


<details>
  <summary>Details</summary>
Motivation: 传统观点认为配置调优需要更准确的代理模型，但先前研究发现准确性可能具有欺骗性，这引发了对代理模型在配置调优中真正作用的疑问。

Method: 采用适应性景观分析视角，提出基于模型有用性而非准确性的评估理论，进行了涉及27,000个案例的广泛实证研究，并开发了Model4Tune自动化预测工具。

Result: Model4Tune在79%-82%的情况下显著优于随机猜测，能够有效预测未见系统的最佳模型-调优器组合。

Conclusion: 研究不仅揭示了配置调优中代理模型的多种作用，提供了实用的评估工具，还为未来研究方向提供了启示。

Abstract: To efficiently tune configuration for better system performance (e.g.,
latency), many tuners have leveraged a surrogate model to expedite the process
instead of solely relying on the profoundly expensive system measurement. As
such, it is naturally believed that we need more accurate models. However, the
fact of accuracy can lie-a somewhat surprising finding from prior work-has left
us many unanswered questions regarding what role the surrogate model plays in
configuration tuning. This paper provides the very first systematic exploration
and discussion, together with a resolution proposal, to disclose the many faces
of surrogate models for configuration tuning, through the novel perspective of
fitness landscape analysis. We present a theory as an alternative to accuracy
for assessing the model usefulness in tuning, based on which we conduct an
extensive empirical study involving up to 27,000 cases. Drawing on the above,
we propose Model4Tune, an automated predictive tool that estimates which
model-tuner pairs are the best for an unforeseen system without expensive tuner
profiling. Our results suggest that Moldel4Tune, as one of the first of its
kind, performs significantly better than random guessing in 79%-82% of the
cases. Our results not only shed light on the possible future research
directions but also offer a practical resolution that can assist practitioners
in evaluating the most useful model for configuration tuning.

</details>


### [23] [SecureAgentBench: Benchmarking Secure Code Generation under Realistic Vulnerability Scenarios](https://arxiv.org/abs/2509.22097)
*Junkai Chen,Huihui Huang,Yunbo Lyu,Junwen An,Jieke Shi,Chengran Yang,Ting Zhang,Haoye Tian,Yikun Li,Zhenhao Li,Xin Zhou,Xing Hu,David Lo*

Main category: cs.SE

TL;DR: SecureAgentBench是一个包含105个编码任务的基准测试，用于严格评估代码代理在安全代码生成方面的能力，发现当前代理在生成安全代码方面表现不佳，最佳代理仅能产生15.2%正确且安全的解决方案。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型驱动的代码代理在软件工程自动化方面发展迅速，但其生成代码的安全风险成为关键问题。现有基准测试存在不足，往往忽略漏洞引入的真实上下文或采用狭窄的评估协议。

Method: 设计了105个编码任务，每个任务包含：(i)需要在大仓库中进行多文件编辑的现实任务设置；(ii)基于真实开源漏洞的上下文环境；(iii)结合功能测试、漏洞检查和静态分析的综合评估方法。评估了三个代表性代理和三个最先进的LLM。

Result: 结果显示：(i)当前代理难以生成安全代码，最佳代理仅达到15.2%的正确且安全解决方案；(ii)一些代理产生功能正确但仍有漏洞的代码，包括之前未记录的新漏洞；(iii)添加显式安全指令对改善安全编码效果不显著。

Conclusion: SecureAgentBench为安全代码生成建立了严格的基准测试，是迈向更可靠LLM软件开发的重要一步，突显了进一步研究的必要性。

Abstract: Large language model (LLM) powered code agents are rapidly transforming
software engineering by automating tasks such as testing, debugging, and
repairing, yet the security risks of their generated code have become a
critical concern. Existing benchmarks have offered valuable insights but remain
insufficient: they often overlook the genuine context in which vulnerabilities
were introduced or adopt narrow evaluation protocols that fail to capture
either functional correctness or newly introduced vulnerabilities. We therefore
introduce SecureAgentBench, a benchmark of 105 coding tasks designed to
rigorously evaluate code agents' capabilities in secure code generation. Each
task includes (i) realistic task settings that require multi-file edits in
large repositories, (ii) aligned contexts based on real-world open-source
vulnerabilities with precisely identified introduction points, and (iii)
comprehensive evaluation that combines functionality testing, vulnerability
checking through proof-of-concept exploits, and detection of newly introduced
vulnerabilities using static analysis. We evaluate three representative agents
(SWE-agent, OpenHands, and Aider) with three state-of-the-art LLMs (Claude 3.7
Sonnet, GPT-4.1, and DeepSeek-V3.1). Results show that (i) current agents
struggle to produce secure code, as even the best-performing one, SWE-agent
supported by DeepSeek-V3.1, achieves merely 15.2% correct-and-secure solutions,
(ii) some agents produce functionally correct code but still introduce
vulnerabilities, including new ones not previously recorded, and (iii) adding
explicit security instructions for agents does not significantly improve secure
coding, underscoring the need for further research. These findings establish
SecureAgentBench as a rigorous benchmark for secure code generation and a step
toward more reliable software development with LLMs.

</details>


### [24] [SK2Decompile: LLM-based Two-Phase Binary Decompilation from Skeleton to Skin](https://arxiv.org/abs/2509.22114)
*Hanzhuo Tan,Weihao Li,Xiaolong Tian,Siyi Wang,Jiaming Liu,Jing Li,Yuqun Zhang*

Main category: cs.SE

TL;DR: SK2Decompile是一个两阶段反编译方法，先恢复程序结构骨架，再生成语义标识符，显著提升反编译正确性和可读性


<details>
  <summary>Details</summary>
Motivation: 现有基于LLM的反编译器在有效呈现程序源代码结构和原始标识符方面存在局限性，需要改进程序语义结构的恢复和标识符命名

Method: 采用两阶段方法：1)结构恢复模型将二进制代码转换为中间表示，保留控制流和数据结构；2)标识符命名模型生成反映程序语义的有意义标识符。两个阶段都使用强化学习进行优化

Result: 在HumanEval数据集上比GPT-5-mini平均重执行率提升21.6%，在GitHub2025基准测试上比Idioms平均R2I改进29.4%

Conclusion: SK2Decompile通过分离结构恢复和标识符命名两个阶段，独立推进反编译的正确性和可读性，显著优于现有最先进方法

Abstract: Large Language Models (LLMs) have emerged as a promising approach for binary
decompilation. However, the existing LLM-based decompilers still are somewhat
limited in effectively presenting a program's source-level structure with its
original identifiers. To mitigate this, we introduce SK2Decompile, a novel
two-phase approach to decompile from the skeleton (semantic structure) to the
skin (identifier) of programs. Specifically, we first apply a Structure
Recovery model to translate a program's binary code to an Intermediate
Representation (IR) as deriving the program's "skeleton", i.e., preserving
control flow and data structures while obfuscating all identifiers with generic
placeholders. We also apply reinforcement learning to reward the model for
producing program structures that adhere to the syntactic and semantic rules
expected by compilers. Second, we apply an Identifier Naming model to produce
meaningful identifiers which reflect actual program semantics as deriving the
program's "skin". We train the Identifier Naming model with a separate
reinforcement learning objective that rewards the semantic similarity between
its predictions and the reference code. Such a two-phase decompilation process
facilitates advancing the correctness and readability of decompilation
independently. Our evaluations indicate that SK2Decompile, significantly
outperforms the SOTA baselines, achieving 21.6% average re-executability rate
gain over GPT-5-mini on the HumanEval dataset and 29.4% average R2I improvement
over Idioms on the GitHub2025 benchmark.

</details>


### [25] [Leveraging LLM Agents for Automated Video Game Testing](https://arxiv.org/abs/2509.22170)
*Chengjia Wang,Lanling Tang,Ming Yuan,Jiongchi Yu,Xiaofei Xie,Jiajun Bu*

Main category: cs.SE

TL;DR: TITAN是一个基于LLM的智能MMORPG测试框架，通过状态感知、动作优化、长程推理和LLM预言机四个核心组件，显著提升了游戏测试的任务完成率和bug检测性能。


<details>
  <summary>Details</summary>
Motivation: MMORPG测试复杂且劳动密集，传统自动化方法难以实现高状态覆盖和效率，现有LLM方法在复杂游戏状态-动作空间和长复杂任务中的推理能力有限。

Method: TITAN框架包含四个关键组件：(1)感知和抽象高维游戏状态；(2)主动优化和优先处理可用动作；(3)通过动作轨迹记忆和反思自校正实现长程推理；(4)使用LLM预言机检测功能性和逻辑性bug并生成诊断报告。

Result: 在两个大型商业MMORPG上的实验显示，TITAN达到了95%的任务完成率，bug检测性能显著优于现有方法，发现了4个先前未知的bug，并已部署到8个真实游戏QA流水线中。

Conclusion: TITAN证明了LLM驱动框架在游戏测试中的有效性，为推进智能通用测试系统提供了新方向，具有重要的实际应用价值。

Abstract: Testing MMORPGs (Massively Multiplayer Online Role-Playing Games) is a
critical yet labor-intensive task in game development due to their complexity
and frequent updating nature. Traditional automated game testing approaches
struggle to achieve high state coverage and efficiency in these rich,
open-ended environments, while existing LLM-based game-playing approaches are
limited to shallow reasoning ability in understanding complex game state-action
spaces and long-complex tasks. To address these challenges, we propose TITAN,
an effective LLM-driven agent framework for intelligent MMORPG testing. TITAN
incorporates four key components to: (1) perceive and abstract high-dimensional
game states, (2) proactively optimize and prioritize available actions, (3)
enable long-horizon reasoning with action trace memory and reflective
self-correction, and (4) employ LLM-based oracles to detect potential
functional and logic bugs with diagnostic reports.
  We implement the prototype of TITAN and evaluate it on two large-scale
commercial MMORPGs spanning both PC and mobile platforms. In our experiments,
TITAN achieves significantly higher task completion rates (95%) and bug
detection performance compared to existing automated game testing approaches.
An ablation study further demonstrates that each core component of TITAN
contributes substantially to its overall performance. Notably, TITAN detects
four previously unknown bugs that prior testing approaches fail to identify. We
provide an in-depth discussion of these results, which offer guidance for new
avenues of advancing intelligent, general-purpose testing systems. Moreover,
TITAN has been deployed in eight real-world game QA pipelines, underscoring its
practical impact as an LLM-driven game testing framework.

</details>


### [26] [Library Hallucinations in LLMs: Risk Analysis Grounded in Developer Queries](https://arxiv.org/abs/2509.22202)
*Lukas Twist,Jie M. Zhang,Mark Harman,Helen Yannakoudakis*

Main category: cs.SE

TL;DR: 本文首次系统研究了用户提示变化对LLM代码生成中库幻觉的影响，发现即使单字符拼写错误也会导致高达26%的幻觉率，假库名接受率高达99%，时间相关提示幻觉率高达84%。


<details>
  <summary>Details</summary>
Motivation: LLM在代码生成中经常产生库幻觉（发明不存在的库），这些幻觉不仅是错误，还可能误导开发者、破坏构建过程并带来供应链安全威胁。目前缺乏对真实世界提示变化如何影响幻觉率的系统研究。

Method: 评估6个不同的LLM，研究两种幻觉类型：库名幻觉（无效导入）和库成员幻觉（有效库中的无效调用）。使用从开发者论坛提取的真实用户语言和不同程度的用户错误（单字符/多字符拼写错误和完全假名/成员）来测试幻觉率。

Result: 发现系统性漏洞：库名单字符拼写错误在高达26%的任务中触发幻觉，假库名在高达99%的任务中被接受，时间相关提示在高达84%的任务中导致幻觉。提示工程有缓解作用但不一致且依赖LLM。

Conclusion: LLM对自然提示变化极其脆弱，迫切需要针对库相关幻觉及其潜在利用的安全防护措施。

Abstract: Large language models (LLMs) are increasingly used to generate code, yet they
continue to hallucinate, often inventing non-existent libraries. Such library
hallucinations are not just benign errors: they can mislead developers, break
builds, and expose systems to supply chain threats such as slopsquatting.
Despite increasing awareness of these risks, little is known about how
real-world prompt variations affect hallucination rates. Therefore, we present
the first systematic study of how user-level prompt variations impact library
hallucinations in LLM-generated code. We evaluate six diverse LLMs across two
hallucination types: library name hallucinations (invalid imports) and library
member hallucinations (invalid calls from valid libraries). We investigate how
realistic user language extracted from developer forums and how user errors of
varying degrees (one- or multi-character misspellings and completely fake
names/members) affect LLM hallucination rates. Our findings reveal systemic
vulnerabilities: one-character misspellings in library names trigger
hallucinations in up to 26% of tasks, fake library names are accepted in up to
99% of tasks, and time-related prompts lead to hallucinations in up to 84% of
tasks. Prompt engineering shows promise for mitigating hallucinations, but
remains inconsistent and LLM-dependent. Our results underscore the fragility of
LLMs to natural prompt variation and highlight the urgent need for safeguards
against library-related hallucinations and their potential exploitation.

</details>


### [27] [Green Prompt Engineering: Investigating the Energy Impact of Prompt Design in Software Engineering](https://arxiv.org/abs/2509.22320)
*Vincenzo De Martino,Mohammad Amin Zadenoori,Xavier Franch,Alessio Ferrari*

Main category: cs.SE

TL;DR: 语言模型的推理过程存在环境问题，本文提出绿色提示工程，通过调整提示语言复杂度来平衡能耗和性能。研究发现简化提示可降低能耗且不影响F1分数。


<details>
  <summary>Details</summary>
Motivation: 语言模型在软件工程中应用日益广泛，但其推理过程带来环境问题。现有研究关注硬件和提示长度，但忽略了语言复杂度这一可持续性因素。

Method: 使用开源小语言模型进行需求分类实证研究，通过改变提示的可读性来测试其对能耗和性能的影响。

Result: 可读性影响环境可持续性和性能，存在权衡关系。简化提示可在不显著损失F1分数的情况下降低能耗成本。

Conclusion: 为从业者提供了降低能耗成本的实用方法，为研究者开辟了在绿色AI议程下可持续提示设计指南和研究的新路径。

Abstract: Language Models are increasingly applied in software engineering, yet their
inference raises growing environmental concerns. Prior work has examined
hardware choices and prompt length, but little attention has been paid to
linguistic complexity as a sustainability factor. This paper introduces Green
Prompt Engineering, framing linguistic complexity as a design dimension that
can influence energy consumption and performance. We conduct an empirical study
on requirement classification using open-source Small Language Models, varying
the readability of prompts. Our results reveal that readability affects
environmental sustainability and performance, exposing trade-offs between them.
For practitioners, simpler prompts can reduce energy costs without a
significant F1-score loss; for researchers, it opens a path toward guidelines
and studies on sustainable prompt design within the Green AI agenda.

</details>


### [28] [GPU-Accelerated Loopy Belief Propagation for Program Analysis](https://arxiv.org/abs/2509.22337)
*Haoyu Feng,Xin Zhang*

Main category: cs.SE

TL;DR: 本文提出了一种GPU加速的Loopy Belief Propagation算法，用于程序分析中的大规模推理任务，通过统一更新策略表示和消息分组优化，在8个真实Java程序的数据竞争分析中取得了显著的速度提升。


<details>
  <summary>Details</summary>
Motivation: Loopy Belief Propagation在程序分析等大规模应用中面临计算挑战，现有的GPU加速方法缺乏灵活的更新策略支持，且未有效整合逻辑约束与GPU加速，导致实际性能不佳。

Method: 提出了统一的更新策略表示方法，支持任意用户定义策略；开发了依赖分析算法；基于Horn子句局部结构简化消息传递，通过消息分组减少warp发散，优化GPU资源利用率。

Result: 在8个真实Java程序的数据竞争分析实验中，相比最先进的串行方法平均加速2.14倍，相比最先进的GPU方法平均加速5.56倍，同时保持高准确率。

Conclusion: 该方法成功解决了大规模程序分析中LBP算法的计算效率问题，通过GPU并行化和策略优化实现了显著的性能提升，为复杂程序分析任务提供了有效的解决方案。

Abstract: Loopy Belief Propagation (LBP) is a widely used approximate inference
algorithm in probabilistic graphical models, with applications in computer
vision, error correction codes, protein folding, program analysis, etc.
However, LBP faces significant computational challenges when applied to
large-scale program analysis. While GPU (Graphics Processing Unit) parallel
computing provides a promising solution, existing approaches lack support for
flexible update strategies and have yet to integrate logical constraints with
GPU acceleration, leading to suboptimal practical performance.
  This paper presents a GPU-accelerated LBP algorithm for program analysis. To
support the diverse update strategies required by users, we propose a unified
representation for specifying arbitrary user-defined update strategies, along
with a dependency analysis algorithm. Furthermore, building on previous work
that leverages the local structure of Horn clauses to simplify message passing,
we group messages to minimize warp divergence and better utilize GPU resources.
Experimental results on datarace analysis over eight real-world Java programs
show that our approach achieves an average speedup of $2.14\times$ over the
state-of-the-art sequential approach and $5.56\times$ over the state-of-the-art
GPU-based approach, while maintaining high accuracy.

</details>


### [29] [A Multi-Modality Evaluation of the Reality Gap in Autonomous Driving Systems](https://arxiv.org/abs/2509.22379)
*Stefano Carlo Lambertenghi,Mirena Flores Valdez,Andrea Stocco*

Main category: cs.SE

TL;DR: 本文通过实证研究比较了四种自动驾驶测试方法（SiL、ViL、MR和实车测试），分析了现实差距的三个维度（执行、感知和行为保真度），发现MR测试在提升感知真实性的同时保持安全性和控制能力。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶系统开发中模拟测试与现实行为之间存在差距（现实差距），这影响了测试结果向实际部署系统的可转移性。需要系统评估不同测试方法的有效性。

Method: 使用配备真实传感器的小型物理车辆及其数字孪生，实现四种测试设置（SiL、ViL、MR和实车测试），在包含真实障碍物、道路拓扑和室内环境的多样化场景中评估两种ADS架构（模块化和端到端）。

Result: 研究发现SiL和ViL设置简化了现实世界动力学和感知的关键方面，而MR测试提高了感知真实性且不损害安全性或控制能力。识别了故障在不同测试方法间不转移的条件，并分离了导致这些差异的现实差距维度。

Conclusion: 研究结果提供了关于每种测试方法优势和局限性的可行见解，为自动驾驶系统更稳健和可转移的验证指明了路径。

Abstract: Simulation-based testing is a cornerstone of Autonomous Driving System (ADS)
development, offering safe and scalable evaluation across diverse driving
scenarios. However, discrepancies between simulated and real-world behavior,
known as the reality gap, challenge the transferability of test results to
deployed systems. In this paper, we present a comprehensive empirical study
comparing four representative testing modalities: Software-in-the-Loop (SiL),
Vehicle-in-the-Loop (ViL), Mixed-Reality (MR), and full real-world testing.
Using a small-scale physical vehicle equipped with real sensors (camera and
LiDAR) and its digital twin, we implement each setup and evaluate two ADS
architectures (modular and end-to-end) across diverse indoor driving scenarios
involving real obstacles, road topologies, and indoor environments. We
systematically assess the impact of each testing modality along three
dimensions of the reality gap: actuation, perception, and behavioral fidelity.
Our results show that while SiL and ViL setups simplify critical aspects of
real-world dynamics and sensing, MR testing improves perceptual realism without
compromising safety or control. Importantly, we identify the conditions under
which failures do not transfer across testing modalities and isolate the
underlying dimensions of the gap responsible for these discrepancies. Our
findings offer actionable insights into the respective strengths and
limitations of each modality and outline a path toward more robust and
transferable validation of autonomous driving systems.

</details>


### [30] [Context-Specific Instruction: A Longitudinal Study on Debugging Skill Acquisition and Retention for Novice Programmers](https://arxiv.org/abs/2509.22420)
*Ziyi Zhang,Devjeet Roy,Venera Arnaoudova*

Main category: cs.SE

TL;DR: 上下文特定的教学（将具体步骤与问题特定细节结合）比抽象指导或通用步骤在错误定位技能培养上效果更好，能实现更快的学习速度和更强的技能保持


<details>
  <summary>Details</summary>
Motivation: 新手在错误定位时缺乏系统方法，现有研究测试了抽象指导和通用步骤，但上下文特定教学的影响尚不清楚

Method: 进行了为期八周的纵向研究，设置了四个组：无指导、抽象指导、具体步骤、上下文特定指导。44名本科生参与，完成5个会话的调试任务，测量正确率、完成时间、自我感知评分

Result: 上下文特定指导组（G4）正确率更高（第一会话后达80% vs 其他组20-44%）、完成时间更短（稳定在13-15分钟 vs 其他组22-27分钟），压力更低、满意度更高

Conclusion: 上下文特定教学能实现更快的技能获取和更强的保持，即使是1-2个会话也能产生显著提升。将上下文示例与抽象原则结合可以弥合错误定位教育中的理论与实践差距

Abstract: Bug localization is a critical skill, yet novices often lack systematic
approaches. Prior work tested abstract guidelines and general concrete steps;
the impact of context-specific instruction is unclear. We ran an eight-week
longitudinal study with four conditions: no instruction (G1), abstract
guidelines (G2), concrete steps (G3), and our context-specific instruction that
pairs concrete bug-localization steps with problem-specific details (G4).
Forty-four undergraduates participated; 41 completed all five sessions (S1-S5).
Each session included 2-3 debugging tasks to identify the minimal code element
containing a seeded logical fault. We measured correctness (binary), time to
completion, self-perceived scores (stress, difficulty, satisfaction, and
strategy adherence). G4 achieved higher correctness and shorter time to
completion: it reached 80% correctness after one session (vs. 20-44% for other
groups) and maintained 80% after three weeks, outperforming all groups (p <
0.05); its time to completion stabilized at 13-15 minutes in S1, whereas other
groups took 2-3 sessions to stabilize at 22-27 minutes. Qualitative responses
showed lower stress and higher satisfaction in G4, with participants
internalizing strategies via contextual examples. We conclude that
context-specific instruction yields faster skill acquisition and stronger
retention than abstract guidelines or context-agnostic steps. Even 1-2 sessions
produced significant gains, while extended practice optimized and stabilized
performance. Integrating contextual examples with abstract principles may
bridge theory-practice gaps in bug-localization education and provide a more
equitable path for novices.

</details>


### [31] [TreeMind: Automatically Reproducing Android Bug Reports via LLM-empowered Monte Carlo Tree Search](https://arxiv.org/abs/2509.22431)
*Zhengyu Chen,Zhaoyi Meng,Wenxiang Zhao,Wansen Wang,Haoyang Zhao,Jiahao Zhan,Jie Cui,Hong Zhong*

Main category: cs.SE

TL;DR: TreeMind是一种结合大型语言模型和蒙特卡洛树搜索的新技术，用于从文本bug报告中自动复现Android应用崩溃，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有的基于强化学习或大型语言模型的方法在复现不完整的bug报告时存在局限，难以推断未观察到的步骤和重建用户操作序列来导航复杂的UI交互空间。

Method: 将LLM与定制化的MCTS算法集成，使用两个LLM引导的代理：Expander基于当前UI状态生成有前景的操作，Simulator估计每个操作导致成功复现的可能性。

Result: 在93个真实Android bug报告数据集上评估，TreeMind在复现成功率上显著优于四个最先进的基线方法。

Conclusion: 将LLM推理与基于MCTS的规划相结合是自动化bug复现的一个有前景的方向。

Abstract: Automatically reproducing Android app crashes from textual bug reports is
challenging, particularly when the reports are incomplete and the modern UI
exhibits high combinatorial complexity. Existing approaches based on
reinforcement learning or large language models (LLMs) exhibit limitations in
such scenarios. They struggle to infer unobserved steps and reconstruct the
underlying user action sequences to navigate the vast UI interaction space,
primarily due to limited goal-directed reasoning and planning. We present
TreeMind, a novel technique that integrates LLMs with a customized Monte Carlo
Tree Search (MCTS) algorithm to achieve strategic UI exploration in bug
reproduction. To the best of our knowledge, this is the first work to combine
external decision-making with LLM semantic reasoning for reliable bug
reproduction. We formulate the reproduction task as a target-driven search
problem, leveraging MCTS as the core planning mechanism to iteratively refine
action sequences. To enhance MCTS with semantic reasoning, we introduce two
LLM-guided agents with distinct roles: Expander generates top-k promising
actions based on the current UI state and exploration history, while Simulator
estimates the likelihood that each action leads toward successful reproduction.
By incorporating multi-modal UI inputs and advanced prompting techniques,
TreeMind conducts feedback-aware navigation that identifies missing but
essential user actions and incrementally reconstructs the reproduction paths.
We evaluate TreeMind on a dataset of 93 real-world Android bug reports from
three widely-used benchmarks. Experimental results show that it significantly
outperforms four state-of-the-art baselines in reproduction success rate. A
real-world case study indicates that integrating LLM reasoning with MCTS-based
planning is a compelling direction for automated bug reproduction.

</details>


### [32] [Boosting Pointer Analysis With Large Language Model-Enhanced Allocation Function Detection](https://arxiv.org/abs/2509.22530)
*Baijun Cheng,Kailong Wang,Ling Shi,Haoyu Wang,Peng Di,Yao Guo,Ding Li,Xiangqun Chen*

Main category: cs.SE

TL;DR: AFD是一种通过自动识别和建模自定义分配函数来增强指针分析的新技术，使用值流分析和LLM来处理简单包装器和复杂分配模式，显著提高堆对象建模精度和别名分析效果。


<details>
  <summary>Details</summary>
Motivation: C/C++程序中普遍存在用户自定义分配函数，现有指针分析方法往往忽略这些自定义分配器，导致别名分析精度降低和堆对象建模不精确。

Method: AFD采用混合方法：使用值流分析检测简单包装器，利用大型语言模型(LLM)推理具有副作用的复杂分配模式，实现精确的堆对象建模。

Result: 在15个真实C项目中识别了600多个自定义分配函数，使建模的堆对象数量增加26倍，别名集大小减少39%，运行时开销仅为1.4倍，并发现了17个之前未检测到的内存错误。

Conclusion: 精确建模自定义分配函数为提高大型软件系统中指针分析的精度提供了可扩展且实用的途径。

Abstract: Pointer analysis is foundational for many static analysis tasks, yet its
effectiveness is often hindered by imprecise modeling of heap allocations,
particularly in C/C++ programs where user-defined allocation functions (AFs)
are pervasive. Existing approaches largely overlook these custom allocators,
leading to coarse aliasing and reduced analysis precision. In this paper, we
present AFD, a novel technique that enhances pointer analysis by automatically
identifying and modeling custom allocation functions. AFD employs a hybrid
approach: it uses value-flow analysis to detect straightforward wrappers and
leverages Large Language Models (LLMs) to reason about more complex allocation
patterns with side effects. This targeted enhancement enables precise modeling
of heap objects at each call site, achieving context-sensitivity-like benefits
without the associated overhead. We evaluate AFD on 15 real-world C projects,
identifying over 600 custom AFs. Integrating AFD into a baseline pointer
analysis yields a 26x increase in modeled heap objects and a 39% reduction in
alias set sizes, with only 1.4x runtime overhead. Furthermore, our enhanced
analysis improves indirect call resolution and uncovers 17 previously
undetected memory bugs. These results demonstrate that precise modeling of
custom allocation functions offers a scalable and practical path to improving
pointer analysis in large software systems.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [33] [Redesigning GROMACS Halo Exchange: Improving Strong Scaling with GPU-initiated NVSHMEM](https://arxiv.org/abs/2509.21527)
*Mahesh Doijade,Andrey Alekseenko,Ania Brown,Alan Gray,Szilárd Páll*

Main category: cs.DC

TL;DR: 基于NVSHMEM的GPU内核启动通信优化GROMACS分子动力学模拟，通过GPU内核融合数据打包和通信，利用硬件延迟隐藏实现细粒度重叠，显著提升强扩展性能


<details>
  <summary>Details</summary>
Motivation: GROMACS分子动力学模拟对延迟高度敏感，MPI的CPU中心特性在GPU应用中引入额外延迟，限制了GPU利用率和可扩展性

Method: 使用NVSHMEM重新设计GROMACS域分解halo交换算法，GPU内核融合数据打包和通信，利用异步复制引擎和NVLink优化延迟和带宽

Result: 在NVLink上强扩展性能提升1.5倍（节点内）和2倍（多节点），在NVLink+InfiniBand多节点环境下提升1.3倍

Conclusion: GPU启动通信方法对延迟敏感应用的强扩展具有显著优势

Abstract: Improving time-to-solution in molecular dynamics simulations often requires
strong scaling due to fixed-sized problems. GROMACS is highly
latency-sensitive, with peak iteration rates in the sub-millisecond, making
scalability on heterogeneous supercomputers challenging. MPI's CPU-centric
nature introduces additional latencies on GPU-resident applications' critical
path, hindering GPU utilization and scalability. To address these limitations,
we present an NVSHMEM-based GPU kernel-initiated redesign of the GROMACS domain
decomposition halo-exchange algorithm. Highly tuned GPU kernels fuse data
packing and communication, leveraging hardware latency-hiding for fine-grained
overlap. We employ kernel fusion across overlapped data forwarding
communication phases and utilize the asynchronous copy engine over NVLink to
optimize latency and bandwidth. Our GPU-resident formulation greatly increases
communication-computation overlap, improving GROMACS strong scaling performance
across NVLink by up to 1.5x (intra-node) and 2x (multi-node), and up to 1.3x
multi-node over NVLink+InfiniBand. This demonstrates the profound benefits of
GPU-initiated communication for strong-scaling a broad range of
latency-sensitive applications.

</details>


### [34] [Zeppelin: Balancing Variable-length Workloads in Data Parallel Large Model Training](https://arxiv.org/abs/2509.21841)
*Chang Chen,Tiancheng Chen,Jiangfei Duan,Qianchao Zhu,Zerui Wang,Qinghao Hu,Peng Sun,Xiuhong Li,Chao Yang,Torsten Hoefler*

Main category: cs.DC

TL;DR: Zeppelin是一个针对长序列LLM训练的系统，通过分层序列分区、路由层和重映射层技术，解决了计算通信不平衡问题，相比现有方法平均加速2.8倍。


<details>
  <summary>Details</summary>
Motivation: 大规模数据并行训练中，不同长度序列导致严重的负载不平衡问题，现有方法忽视了计算和通信成本随序列长度的变化规律。

Method: 采用三种关键技术：1）分层序列分区方法减少通信开销并平衡计算；2）路由层协调节点间传输以充分利用NIC带宽；3）重映射层在注意力和线性模块间转换序列布局。

Result: 在各种配置下的综合评估显示，Zeppelin相比最先进方法平均实现2.80倍加速。

Conclusion: Zeppelin有效解决了长序列LLM训练中的负载不平衡问题，通过创新的分层分区和动态调度机制显著提升了训练效率。

Abstract: Training large language models (LLMs) with increasingly long and varying
sequence lengths introduces severe load imbalance challenges in large-scale
data-parallel training. Recent frameworks attempt to mitigate these issues
through data reorganization or hybrid parallel strategies. However, they often
overlook how computational and communication costs scale with sequence length,
resulting in suboptimal performance. We identify three critical challenges: (1)
varying computation-to-communication ratios across sequences of different
lengths in distributed attention, (2) mismatch between static NIC-GPU affinity
and dynamic parallel workloads, and (3) distinct optimal partitioning
strategies required for quadratic attention versus linear components. To
address these challenges, we present Zeppelin, a novel training system that
integrates three key techniques: (1) a hierarchical sequence partitioning
method for the attention module that reduces communication overhead and
balances computation, supported by an efficient attention engine that applies
divergent parallel strategies; (2) a routing layer that orchestrates inter-node
transfers to fully utilize NIC bandwidth; and (3) a remapping layer that
transforms sequence layouts between attention and linear modules, ensuring high
computational efficiency across both. Comprehensive evaluations across diverse
configurations show that Zeppelin delivers an average 2.80x speedup over
state-of-the-art methods.

</details>


### [35] [Code once, Run Green: Automated Green Code Translation in Serverless Computing](https://arxiv.org/abs/2509.22068)
*Sebastian Werner,Mathis Kähler,Alireza Hakamian*

Main category: cs.DC

TL;DR: 本文探讨了利用LLMs将serverless函数翻译成更节能的编程语言来减少能源债务的潜力，初步结果显示翻译后的函数可减少高达70%的调用能耗。


<details>
  <summary>Details</summary>
Motivation: 计算基础设施的数字化和AI模型使用导致碳排放增加，现有节能策略依赖利益相关者干预，难以在已部署系统中实施，形成能源债务问题。

Method: 设计并实现ReFaaS系统，集成到Fission serverless框架中，评估多种LLMs进行代码翻译的能力及其对能耗的影响。

Result: 翻译后的函数调用能耗可降低高达70%，在约3000-5000次调用后实现净节能，但并非所有函数都适合翻译。

Conclusion: 虽然面临挑战，但识别出四个关键研究挑战，解决这些挑战可能实现serverless计算中长期自动化的能源债务缓解。

Abstract: The rapid digitization and the increasing use of emerging technologies such
as AI models have significantly contributed to the emissions of computing
infrastructure. Efforts to mitigate this impact typically focus on the
infrastructure level such as powering data centers with renewable energy, or
through the specific design of energy-efficient software. However, both
strategies rely on stakeholder intervention, making their adoption in legacy
and already-deployed systems unlikely. As a result, past architectural and
implementation decisions continue to incur additional energy usage - a
phenomenon we refer to as energy debt.
  Hence, in this paper, we investigate the potential of serverless computing
platforms to automatically reduce energy debt by leveraging the unique access
to function source code. Specifically, we explore whether large language models
(LLMs) can translate serverless functions into more energy-efficient
programming languages while preserving functional correctness. To this end, we
design and implement ReFaaS and integrate it into the Fission serverless
framework. We evaluate multiple LLMs on their ability to perform such code
translations and analyze their impact on energy consumption.
  Our preliminary results indicate that translated functions can reduce
invocation energy by up to 70%, achieving net energy savings after
approximately 3,000 to 5,000 invocations, depending on the LLM used.
Nonetheless, the approach faces several challenges: not all functions are
suitable for translation, and for some, the amortization threshold is
significantly higher or unreachable. Despite these limitations, we identify
four key research challenges whose resolution could unlock long-term, automated
mitigation of energy debt in serverless computing.

</details>


### [36] [The AI_INFN Platform: Artificial Intelligence Development in the Cloud](https://arxiv.org/abs/2509.22117)
*Lucio Anderlini,Giulio Bianchini,Diego Ciangottini,Stefano Dal Pra,Diego Michelotto,Rosa Petrini,Daniele Spiga*

Main category: cs.DC

TL;DR: INFN AI项目通过Kubernetes平台整合GPU加速器资源，支持机器学习工作流在异构分布式计算环境中的开发和扩展，包括网格计算和超级计算机资源。


<details>
  <summary>Details</summary>
Motivation: 机器学习在科研软件中的应用日益广泛，但对计算基础设施提出了新的挑战，特别是在硬件加速器的配置和协调方面。AI_INFN项目旨在促进INFN内部ML技术的采用。

Method: 利用云原生解决方案和Kubernetes平台，通过Virtual Kubelet和InterLink API实现资源卸载机制，管理跨不同资源提供商的工作流。

Result: 初步测试结果显示该平台能够有效管理异构分布式计算资源，包括LHC计算网格和超级计算机资源，为需要专用基础设施的工作负载提供了可行模型。

Conclusion: 该Kubernetes平台为ML工作流提供了有效的资源管理和扩展解决方案，支持INFN研究活动的多样性，展示了在异构计算环境中部署AI应用的可行性。

Abstract: Machine Learning (ML) is driving a revolution in the way scientists design,
develop, and deploy data-intensive software. However, the adoption of ML
presents new challenges for the computing infrastructure, particularly in terms
of provisioning and orchestrating access to hardware accelerators for
development, testing, and production. The INFN-funded project AI_INFN
(Artificial Intelligence at INFN) aims at fostering the adoption of ML
techniques within INFN use cases by providing support on multiple aspects,
including the provisioning of AI-tailored computing resources. It leverages
cloud-native solutions in the context of INFN Cloud, to share hardware
accelerators as effectively as possible, ensuring the diversity of the
Institute's research activities is not compromised. In this contribution, we
provide an update on the commissioning of a Kubernetes platform designed to
ease the development of GPU-powered data analysis workflows and their
scalability on heterogeneous distributed computing resources, also using the
offloading mechanism with Virtual Kubelet and InterLink API. This setup can
manage workflows across different resource providers, including sites of the
Worldwide LHC Computing Grid and supercomputers such as CINECA Leonardo,
providing a model for use cases requiring dedicated infrastructures for
different parts of the workload. Initial test results, emerging case studies,
and integration scenarios will be presented with functional tests and
benchmarks.

</details>


### [37] [Orientation does not help with 3-coloring a grid in online-LOCAL](https://arxiv.org/abs/2509.22233)
*Thomas Boudier,Filippo Casagrande,Avinandan Das,Massimo Equi,Henrik Lievonen,Augusto Modanese,Ronja Stimpert*

Main category: cs.DC

TL;DR: 本文证明了在确定性或随机性online-LOCAL模型中，即使算法明确获得网格的全局一致方向，3着色网格问题仍需要Ω(log n)的局部性。


<details>
  <summary>Details</summary>
Motivation: 先前的研究表明3着色二分图在online-LOCAL模型中需要全局内存优势，但证明都依赖于算法无法访问底层网格方向的假设。本文旨在移除这一假设限制。

Method: 通过构建新的证明技术，展示了即使在算法明确获得网格全局方向信息的情况下，3着色网格问题仍然需要对数级别的局部性。

Result: 成功证明了在确定性online-LOCAL和随机性online-LOCAL模型中，即使提供全局一致的网格方向，3着色网格问题仍然需要Ω(log n)的局部性。

Conclusion: 本文扩展了先前的结果，表明网格方向的可用性并不改变3着色问题的计算复杂性下界，这为理解online-LOCAL模型的计算能力提供了更深刻的见解。

Abstract: The online-LOCAL and SLOCAL models are extensions of the LOCAL model where
nodes are processed in a sequential but potentially adversarial order. So far,
the only problem we know of where the global memory of the online-LOCAL model
has an advantage over SLOCAL is 3-coloring bipartite graphs. Recently, Chang et
al. [PODC 2024] showed that even in grids, 3-coloring requires $\Omega(\log n)$
locality in deterministic online-LOCAL. This result was subsequently extended
by Akbari et al. [STOC 2025] to also hold in randomized online-LOCAL. However,
both proofs heavily rely on the assumption that the algorithm does not have
access to the orientation of the underlying grid. In this paper, we show how to
lift this requirement and obtain the same lower bound (against either model)
even when the algorithm is explicitly given a globally consistent orientation
of the grid.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [38] [Privacy-Preserving Performance Profiling of In-The-Wild GPUs](https://arxiv.org/abs/2509.21762)
*Ian McDougall,Michael Davies,Rahul Chatterjee,Somesh Jha,Karthikeyan Sankaralingam*

Main category: cs.AR

TL;DR: 开发了一个行星级实时GPU性能分析系统，解决大规模GPU部署中的性能数据收集问题，同时保证零性能影响和用户隐私保护


<details>
  <summary>Details</summary>
Motivation: GPU已成为深度学习、加速计算和科学模拟等关键应用的主导平台，但现有工具无法在舰队规模上收集全面的性能数据，限制了芯片设计和应用优化的能力

Method: 设计了一个系统来解决三个核心问题：零性能影响的用户体验、保护用户隐私的第三方无法知晓应用信息、在数千GPU上有效收集数据并关联到具体应用

Result: 在模拟10万GPU规模部署中运行Torchbench套件应用，证明系统成功解决了所有三个核心问题

Conclusion: 该系统实现了行星级实时GPU性能分析，为芯片制造商提供了宝贵的舰队级性能数据，同时确保了用户隐私和零性能影响

Abstract: GPUs are the dominant platform for many important applications today
including deep learning, accelerated computing, and scientific simulation.
However, as the complexity of both applications and hardware increases, GPU
chip manufacturers face a significant challenge: how to gather comprehensive
performance characteristics and value profiles from GPUs deployed in real-world
scenarios. Such data, encompassing the types of kernels executed and the time
spent in each, is crucial for optimizing chip design and enhancing application
performance. Unfortunately, despite the availability of low-level tools like
NSYS and NCU, current methodologies fall short, offering data collection
capabilities only on an individual user basis rather than a broader, more
informative fleet-wide scale. This paper takes on the problem of realizing a
system that allows planet-scale real-time GPU performance profiling of
low-level hardware characteristics. The three fundamental problems we solve
are: i) user experience of achieving this with no slowdown; ii) preserving user
privacy, so that no 3rd party is aware of what applications any user runs; iii)
efficacy in showing we are able to collect data and assign it applications even
when run on 1000s of GPUs. Our results simulate a 100,000 size GPU deployment,
running applications from the Torchbench suite, showing our system addresses
all 3 problems.

</details>


### [39] [NeuroScalar: A Deep Learning Framework for Fast, Accurate, and In-the-Wild Cycle-Level Performance Prediction](https://arxiv.org/abs/2509.22410)
*Shayne Wadle,Yanxin Zhang,Vikas Singh,Karthikeyan Sankaralingam*

Main category: cs.AR

TL;DR: 提出基于深度学习的高性能微处理器仿真框架，可在现有硬件上预测未来设计的性能，实现5 MIPS仿真速度且仅带来0.1%性能开销


<details>
  <summary>Details</summary>
Motivation: 传统周期精确模拟器速度慢且依赖不具代表性的基准测试，限制了新微处理器设计的评估效率

Method: 使用深度学习模型基于微架构无关特征预测周期级性能，包含轻量级硬件追踪收集器和系统化采样策略，并设计了Neutrino片上加速器

Result: 在商用GPU上实现5 MIPS仿真速度，仅0.1%性能开销；Neutrino加速器比GPU性能提升85倍；支持大规模硬件A/B测试和准确性能分析

Conclusion: 该框架能够在生产硬件上进行高保真仿真，为新处理器设计评估提供了高效准确的解决方案

Abstract: The evaluation of new microprocessor designs is constrained by slow,
cycle-accurate simulators that rely on unrepresentative benchmark traces. This
paper introduces a novel deep learning framework for high-fidelity,
``in-the-wild'' simulation on production hardware. Our core contribution is a
DL model trained on microarchitecture-independent features to predict
cycle-level performance for hypothetical processor designs. This unique
approach allows the model to be deployed on existing silicon to evaluate future
hardware. We propose a complete system featuring a lightweight hardware trace
collector and a principled sampling strategy to minimize user impact. This
system achieves a simulation speed of 5 MIPS on a commodity GPU, imposing a
mere 0.1% performance overhead. Furthermore, our co-designed Neutrino on-chip
accelerator improves performance by 85x over the GPU. We demonstrate that this
framework enables accurate performance analysis and large-scale hardware A/B
testing on a massive scale using real-world applications.

</details>


### [40] [AxLLM: accelerator architecture for large language models with computation reuse capability](https://arxiv.org/abs/2509.22512)
*Soroush Ahadi,Mehdi Modarressi,Masoud Daneshtalab*

Main category: cs.AR

TL;DR: AxLLM是一种针对量化LLM的硬件加速器架构，通过缓存和重用重复权重值的乘法结果来减少冗余计算，实现高达90%的计算减少、28%的能耗降低和1.7倍的加速。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型需要巨大的计算能力和内存资源，部署效率面临挑战。量化虽然能减少模型大小和计算量，但本文发现量化还能增加参数局部性，为计算重用创造机会。

Method: 提出AxLLM硬件加速器架构，采用新颖的冗余消除技术，通过双乘法和重用流水线来缓存和重用重复权重值的乘法结果，支持基础模型和LoRA微调模型而无需修改参数、重新训练或离线预处理。

Result: 实验结果显示，AxLLM实现了高达90%的计算减少，能耗降低28%，速度提升1.7倍。

Conclusion: AxLLM是专门硬件上加速LLM的可扩展高效解决方案，通过利用量化带来的参数局部性优势，显著提升了计算效率。

Abstract: Large language models demand massive computational power and memory
resources, posing significant challenges for efficient deployment. While
quantization has been widely explored to reduce model size and computation,
this paper demonstrates an additional benefit: quantization increases parameter
locality, creating opportunities for computation reuse. Building on this
insight, we propose AxLLM, a hardware accelerator architecture designed for
quantized models. Axllm introduces a novel redundancy elimination technique
that caches and reuses multiplication results for repeated weight values,
substantially reducing redundant operations. The architecture features dual
multiply and reuse pipelines, efficiently supporting both base models and LoRA
fine-tuned models without altering parameters, retraining, or requiring offline
preprocessing. Experimental results show that AxLLM achieves up to 90%
reduction in computations, delivering 28% lower energy consumption and a 1.7x
speedup over baseline execution. These results highlight Axllm as a scalable
and efficient solution for accelerating LLMs on specialized hardware.

</details>
