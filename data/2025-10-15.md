<div id=toc></div>

# Table of Contents

- [cs.AR](#cs.AR) [Total: 5]
- [cs.MA](#cs.MA) [Total: 5]
- [cs.NI](#cs.NI) [Total: 5]
- [cs.DC](#cs.DC) [Total: 10]
- [cs.SE](#cs.SE) [Total: 27]


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [1] [ISAAC: Intelligent, Scalable, Agile, and Accelerated CPU Verification via LLM-aided FPGA Parallelism](https://arxiv.org/abs/2510.10225)
*Jialin Sun,Yuchen Hu,Dean You,Yushu Du,Hui Wang,Xinwei Fang,Weiwei Shan,Nan Guan,Zhe Jiang*

Main category: cs.AR

TL;DR: 本文提出了ISAAC，一个结合大语言模型（LLM）与FPGA并行加速的全栈CPU验证框架，通过引入具备微架构知识的多智能体激励生成引擎和轻量级快照机制，显著提升验证效率与覆盖率，在实际CPU验证中实现了最高17,536倍的仿真加速，并发现了多个未知缺陷。


<details>
  <summary>Details</summary>
Motivation: CPU功能验证在工业实践中高度依赖差分测试，但现有流程在激励生成和仿真基础设施两方面均存在瓶颈：前端缺乏微架构感知，生成低质量冗余测试；后端即使使用FPGA加速，仍受限于长运行测试和调试反馈延迟。

Method: ISAAC框架在前端采用融合微架构知识与历史缺陷模式的多智能体激励引擎，生成高针对性测试；后端引入轻量级前向快照机制与ISS/DUT解耦的协同仿真架构，支持单个ISS驱动多个DUT并行执行，并利用FPGA并行性提升吞吐量。

Result: 在已多次成功流片的成熟CPU上验证ISAAC，结果显示相比软件RTL仿真最高加速17,536倍，并发现多个此前未知的bug，其中两个在文中报告。

Conclusion: ISAAC通过LLM驱动的智能激励生成与高效并行仿真架构，有效突破了传统CPU验证中的关键瓶颈，显著提升验证速度、覆盖率与缺陷检出能力，为工业级CPU验证提供了可行的新范式。

Abstract: Functional verification is a critical bottleneck in integrated circuit
development, with CPU verification being especially time-intensive and
labour-consuming. Industrial practice relies on differential testing for CPU
verification, yet faces bottlenecks at nearly each stage of the framework
pipeline: front-end stimulus generation lacks micro-architectural awareness,
yielding low-quality and redundant tests that impede coverage closure and miss
corner cases. Meanwhile, back-end simulation infrastructure, even with FPGA
acceleration, often stalls on long-running tests and offers limited visibility,
delaying feedback and prolonging the debugging cycle. Here, we present ISAAC, a
full-stack, Large Language Model (LLM)-aided CPU verification framework with
FPGA parallelism, from bug categorisation and stimulus generation to simulation
infrastructure. To do so, we presented a multi-agent stimulus engine in ISAAC's
front-end, infused with micro-architectural knowledge and historical bug
patterns, generating highly targeted tests that rapidly achieve coverage goals
and capture elusive corner cases. In ISAAC's back-end, we introduce a
lightweight forward-snapshot mechanism and a decoupled co-simulation
architecture between the Instruction Set Simulator (ISS) and the Design Under
Test (DUT), enabling a single ISS to drive multiple DUTs in parallel. By
eliminating long-tail test bottlenecks and exploiting FPGA parallelism, the
simulation throughput is significantly improved. As a demonstration, we used
ISAAC to verify a mature CPU that has undergone multiple successful tape-outs.
Results show up to 17,536x speed-up over software RTL simulation, while
detecting several previously unknown bugs, two of which are reported in this
paper.

</details>


### [2] [ADiP: Adaptive Precision Systolic Array for Matrix Multiplication Acceleration](https://arxiv.org/abs/2510.10623)
*Ahmed J. Abdelmaksoud,Cristian Sestito,Shiwei Wang,Themis Prodromakis*

Main category: cs.AR

TL;DR: 本文提出了一种名为ADiP的自适应精度脉动阵列架构，用于高效加速Transformer模型中的矩阵乘法。该架构通过动态调整计算精度（如8bit×8bit、8bit×4bit、8bit×2bit）提升计算密度和能效，在22nm工艺下实现最高4倍吞吐量提升，并在GPT-2、BERT和BitNet等模型上验证了其在延迟和能耗方面的显著优势。


<details>
  <summary>Details</summary>
Motivation: Transformer模型依赖大量矩阵乘法，计算和内存开销巨大，亟需高效硬件加速。量化可降低内存占用，若结合可重构架构动态调整精度，则可进一步优化矩阵乘法效率。

Method: 提出ADiP架构，包含N×N个自适应精度处理单元（PE）和共享累加器，支持对称单矩阵与非对称多矩阵乘法模式，提升数据复用与PE利用率；建立延迟与吞吐量分析模型，并在22nm工艺下进行硬件设计空间探索。

Result: 在64×64规模（4096个PE）下，ADiP在8bit×8bit、8bit×4bit和8bit×2bit运算中分别达到8.192、16.384和32.768 TOPS的峰值吞吐量；在BitNet-1.58B的MHA任务中，延迟降低53.6%，能耗降低24.4%，整体吞吐量最高提升4倍。

Conclusion: ADiP通过自适应精度设计显著提升了矩阵乘法的计算效率与能效，为Transformer模型提供了高效的硬件加速方案，具有良好的可扩展性和实际应用价值。

Abstract: Transformers are at the core of modern AI nowadays. They rely heavily on
matrix multiplication and require efficient acceleration due to their
substantial memory and computational requirements. Quantization plays a vital
role in reducing memory usage, and can be exploited for computations by
designing reconfigurable architectures that enhance matrix multiplication by
dynamically adjusting the precision. This paper proposes ADiP, a novel
adaptive-precision systolic array architecture designed for efficient matrix
multiplication acceleration.The proposed architecture consists of NxN
adaptive-precision processing elements (PEs) and shared accumulators. ADiP
supports multiple computation modes, including symmetric single-matrix
multiplication as well as asymmetric multi-matrix multiplication with a shared
input matrix, thereby improving data-reuse and PE utilization. In addition,
ADiP maximizes the computational density by adapting to different precisions,
such as 8bitx8bit, 8bitx4bit, and 8bitx2bit. Analytical models are developed
for ADiP architecture, including latency and throughput for versatile
architecture configurations. A comprehensive hardware design space exploration
is demonstrated using 22nm commercial technology, achieving up to a 4x higher
computational throughput. Furthermore, ADiP is evaluated on different
transformer workloads from GPT-2 Medium, BERT Large, and BitNet-1.58B models,
delivering latency improvement up to 53.6%, and energy improvement up to 24.4%
for BitNet-1.58B MHA workloads. At a 64x64 size with 4096 PEs, ADiP achieves a
peak throughput of 8.192 TOPS, 16.384 TOPS, and 32.768 TOPS for 8bitx8bit,
8bitx4bit, and 8bitx2bit operations, respectively.

</details>


### [3] [Bhasha-Rupantarika: Algorithm-Hardware Co-design approach for Multilingual Neural Machine Translation](https://arxiv.org/abs/2510.10676)
*Mukul Lokhande,Tanushree Dewangan,Mohd Sharik Mansoori,Tejas Chaudhari,Akarsh J.,Damayanti Lokhande,Adam Teman,Santosh Kumar Vishvakarma*

Main category: cs.AR

TL;DR: 本文提出了Bhasha-Rupantarika，一种通过算法与硬件协同设计实现的轻量高效多语言翻译系统，专为资源受限环境优化。通过在FP8、INT8、INT4和FP4等亚字节精度下部署模型，实现了模型体积缩小4.1倍、推理速度提升4.2倍（吞吐达66 tokens/s），并在FPGA上显著降低资源占用，适用于物联网设备的实时多语言翻译。


<details>
  <summary>Details</summary>
Motivation: 在资源受限的环境中（如物联网设备），部署高效、低延迟的多语言翻译系统面临计算资源和能耗限制。现有方法难以兼顾模型性能与硬件效率，因此亟需一种结合超低精度量化与硬件协同优化的解决方案。

Method: 该研究采用算法-硬件协同设计方法，探索在FP8、INT8、INT4和FP4等亚字节精度下进行模型量化，并在FPGA加速器上部署多语言翻译模型。系统支持印度语言与国际语言之间的双向翻译，并通过量化感知训练确保翻译质量。

Result: 实验表明，FP4量化使模型体积减少4.1倍，推理速度提升4.2倍（吞吐量达66 tokens/s，提升4.8倍）。FPGA部署相比OPU和HPTA分别实现2.2倍和4.6倍的吞吐提升，同时LUTs减少1.96倍、FFs减少1.65倍，验证了系统在低资源环境下的高效性与实用性。

Conclusion: Bhasha-Rupantarika通过超低精度量化与FPGA硬件协同设计，为资源受限场景下的多语言AI翻译系统提供了一种高效、可部署的解决方案，兼具模型压缩、推理加速与硬件资源优化优势，代码与数据集已开源以促进后续研究。

Abstract: This paper introduces Bhasha-Rupantarika, a light and efficient multilingual
translation system tailored through algorithm-hardware codesign for
resource-limited settings. The method investigates model deployment at
sub-octet precision levels (FP8, INT8, INT4, and FP4), with experimental
results indicating a 4.1x reduction in model size (FP4) and a 4.2x speedup in
inference speed, which correlates with an increased throughput of 66 tokens/s
(improvement by 4.8x). This underscores the importance of ultra-low precision
quantization for real-time deployment in IoT devices using FPGA accelerators,
achieving performance on par with expectations. Our evaluation covers
bidirectional translation between Indian and international languages,
showcasing its adaptability in low-resource linguistic contexts. The FPGA
deployment demonstrated a 1.96x reduction in LUTs and a 1.65x decrease in FFs,
resulting in a 2.2x enhancement in throughput compared to OPU and a 4.6x
enhancement compared to HPTA. Overall, the evaluation provides a viable
solution based on quantisation-aware translation along with hardware efficiency
suitable for deployable multilingual AI systems. The entire codes
[https://github.com/mukullokhande99/Bhasha-Rupantarika/] and dataset for
reproducibility are publicly available, facilitating rapid integration and
further development by researchers.

</details>


### [4] [FeNOMS: Enhancing Open Modification Spectral Library Search with In-Storage Processing on Ferroelectric NAND (FeNAND) Flash](https://arxiv.org/abs/2510.10872)
*Sumukh Pinge,Ashkan Moradifirouzabadi,Keming Fan,Prasanna Venkatesan Ravindran,Tanvir H. Pantha,Po-Kai Hsu,Zheyu Li,Weihong Xu,Zihan Xia,Flavio Ponzina,Winston Chern,Taeyoung Song,Priyankka Ravikumar,Mengkun Tian,Lance Fernandes,Huy Tran,Hari Jayasankar,Hang Chen,Chinsung Park,Amrit Garlapati,Kijoon Kim,Jongho Woo,Suhwan Lim,Kwangsoo Kim,Wanki Kim,Daewon Ha,Duygu Kuzum,Shimeng Yu,Sourav Dutta,Asif Khan,Tajana Rosing,Mingu Kang*

Main category: cs.AR

TL;DR: 本文提出一种基于3D铁电NAND（FeNAND）的存内计算架构，结合超维计算（HDC）与双边界近似匹配（D-BAM）距离度量，显著加速质谱数据的谱库搜索，相比现有3D NAND方法实现43倍加速和21倍能效提升，同时保持相当的准确率。


<details>
  <summary>Details</summary>
Motivation: 质谱（MS）数据规模迅速增长（超数百TB），传统处理器难以高效处理大规模谱库搜索任务，而现有存内计算（ISP）方案受限于NAND闪存串行读取导致的吞吐量瓶颈。

Method: 提出一种基于3D FeNAND结构的ISP架构，利用其高密度、高速度和低电压优势，并引入超维计算（HDC）实现高度并行化处理，同时设计适配FeNAND结构的双边界近似匹配（D-BAM）距离度量，以并行化向量计算。

Result: 在质谱谱库搜索任务中，相比最先进的3D NAND方法，实现了43倍的速度提升和21倍的能效提升，同时保持了相当的准确率。

Conclusion: 结合FeNAND、HDC和D-BAM的存内计算架构能有效应对大规模质谱数据搜索的效率与能耗挑战，为药物发现中的高性能计算提供了一种可行的新路径。

Abstract: The rapid expansion of mass spectrometry (MS) data, now exceeding hundreds of
terabytes, poses significant challenges for efficient, large-scale library
search - a critical component for drug discovery. Traditional processors
struggle to handle this data volume efficiently, making in-storage computing
(ISP) a promising alternative. This work introduces an ISP architecture
leveraging a 3D Ferroelectric NAND (FeNAND) structure, providing significantly
higher density, faster speeds, and lower voltage requirements compared to
traditional NAND flash. Despite its superior density, the NAND structure has
not been widely utilized in ISP applications due to limited throughput
associated with row-by-row reads from serially connected cells. To overcome
these limitations, we integrate hyperdimensional computing (HDC), a
brain-inspired paradigm that enables highly parallel processing with simple
operations and strong error tolerance. By combining HDC with the proposed
dual-bound approximate matching (D-BAM) distance metric, tailored to the FeNAND
structure, we parallelize vector computations to enable efficient MS spectral
library search, achieving 43x speedup and 21x higher energy efficiency over
state-of-the-art 3D NAND methods, while maintaining comparable accuracy.

</details>


### [5] [Efficient In-Memory Acceleration of Sparse Block Diagonal LLMs](https://arxiv.org/abs/2510.11192)
*João Paulo Cardoso de Lima,Marc Dietrich,Jeronimo Castrillon,Asif Ali Khan*

Main category: cs.AR

TL;DR: 本文提出了一种自动化框架，通过新颖的映射与调度策略，在存内计算（CIM）加速器上高效加速稀疏大语言模型（LLM）推理，显著提升CIM阵列利用率并减少内存占用与计算量。


<details>
  <summary>Details</summary>
Motivation: 尽管结构化稀疏可大幅压缩LLM模型规模，但其推理（尤其是解码阶段）在传统冯·诺依曼架构上仍受内存带宽限制；而将稀疏模型直接映射到存内计算（CIM）架构时，又因映射效率低下导致阵列利用率不足。

Method: 提出一种自动化框架，结合针对块对角稀疏结构的新型映射与调度策略，优化稀疏LLM在CIM加速器上的部署。

Result: 该方法将CIM阵列利用率提升超过50%，同时实现内存占用和浮点运算次数均减少4倍以上。

Conclusion: 通过高效利用结构化稀疏性，所提框架显著提升了CIM架构上稀疏LLM推理的效率与资源利用率，为在资源受限系统上部署大模型提供了可行路径。

Abstract: Structured sparsity enables deploying large language models (LLMs) on
resource-constrained systems. Approaches like dense-to-sparse fine-tuning are
particularly compelling, achieving remarkable structured sparsity by reducing
the model size by over 6.7x, while still maintaining acceptable accuracy.
Despite this reduction, LLM inference, especially the decode stage being
inherently memory-bound, is extremely expensive on conventional Von-Neumann
architectures. Compute-in-memory (CIM) architectures mitigate this by
performing computations directly in memory, and when paired with sparse LLMs,
enable storing and computing the entire model in memory, eliminating the data
movement on the off-chip bus and improving efficiency. Nonetheless, naively
mapping sparse matrices onto CIM arrays leads to poor array utilization and
diminished computational efficiency. In this paper, we present an automated
framework with novel mapping and scheduling strategies to accelerate sparse LLM
inference on CIM accelerators. By exploiting block-diagonal sparsity, our
approach improves CIM array utilization by over 50%, achieving more than 4x
reduction in both memory footprint and the number of required floating-point
operations.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [6] [A Hybrid Agent-Based and System Dynamics Framework for Modelling Project Execution and Technology Maturity in Early-Stage R&D](https://arxiv.org/abs/2510.09688)
*R. W. S. Pessoa,M. H. Næss,J. C. Bijos,C. M. Rebello,D. Colombo,L. Schnitman,I. B. R. Nogueira*

Main category: cs.MA

TL;DR: 本文提出一种结合系统动力学（SD）与基于智能体建模（ABM）的混合方法，用于预测研发项目中技术成熟度的演进，以油气行业为例，通过多层级框架捕捉工作量、团队规模和项目周期等不确定性因素对技术进展的影响。


<details>
  <summary>Details</summary>
Motivation: 现有研究中ABM与SD混合模型在其他领域已有应用，但在研发（R&D）领域仍较少，因此本文旨在填补这一空白，构建能更真实反映研发项目动态的模型。

Method: 构建一个融合系统层面反馈结构（如工作阶段、返工循环、周期）与去中心化智能体（如团队成员、任务、控制器）的多层级混合模型，通过不同任务结构（并行/串行）和团队规模的情景模拟进行分析。

Result: 在并行任务结构中，团队规模增至4–5人时项目周期最短、任务完成效率最高；更大团队则因沟通复杂性和管理延迟导致绩效下降。相比纯串行情景，并行结构可减少88%的返工时间。

Conclusion: 该混合模型与专家认知一致，验证了其作为分析资源分配、调度效率和技术成熟度演进的定量工具的有效性。

Abstract: This paper presents a hybrid approach to predict the evolution of
technological maturity in R and D projects, using the oil and gas sector as an
example. Integrating System Dynamics (SD) and Agent Based Modelling (ABM)
allows the proposed multi level framework to capture uncertainties in work
effort, team size, and project duration, which influence technological
progress. While AB SD hybrid models are established in other fields, their use
in R and D remains limited. The model combines system level feedback structures
governing work phases, rework cycles, and duration with decentralised agents
such as team members, tasks, and controllers, whose interactions generate
emergent project dynamics. A base case scenario analysed early stage innovation
projects with 15 parallel tasks over 156 weeks. A comparative sequential
scenario showed an 88 percent reduction in rework duration. A second scenario
assessed mixed parallel sequential task structures with varying team sizes. In
parallel configurations, increasing team size reduced project duration and
improved task completion, with optimal results for teams of four to five
members. These findings align with empirical evidence showing that moderate
team expansion enhances coordination efficiency without excessive communication
overhead. However, larger teams may decrease performance due to communication
complexity and management delays. Overall, the model outputs and framework
align with expert understanding, supporting their validity as quantitative
tools for analysing resource allocation, scheduling efficiency, and technology
maturity progression.

</details>


### [7] [KG-MAS: Knowledge Graph-Enhanced Multi-Agent Infrastructure for coupling physical and digital robotic environments](https://arxiv.org/abs/2510.10325)
*Walid Abdela*

Main category: cs.MA

TL;DR: 本文提出了一种名为KG-MAS的知识图谱增强多智能体基础设施，通过将知识图谱作为共享语义世界模型，实现异构物理与数字环境在工业4.0中的智能协同。


<details>
  <summary>Details</summary>
Motivation: 现有面向信息物理系统（CPS）的方法（如共仿真框架或点对点中间件）缺乏语义表达能力和灵活性，难以支持智能、自主的协同，尤其在工业4.0中面对系统异构性和复杂性时表现不足。

Method: KG-MAS架构利用一个中心化的知识图谱作为动态共享世界模型，为多智能体系统提供统一语义基础；智能体基于该图谱进行查询与状态更新，并通过模型驱动方式从语义描述自动生成智能体。

Result: 该架构实现了对底层通信协议的抽象，提供了统一、智能的协调机制，显著提升了系统在异构物理与数字机器人环境中的可扩展性、鲁棒性与灵活性。

Conclusion: KG-MAS为工业4.0中信息物理系统的语义化、智能化集成提供了一种有效且可维护的解决方案。

Abstract: The seamless integration of physical and digital environments in
Cyber-Physical Systems(CPS), particularly within Industry 4.0, presents
significant challenges stemming from system heterogeneity and complexity.
Traditional approaches often rely on rigid, data-centric solutions like
co-simulation frameworks or brittle point-to-point middleware bridges, which
lack the semantic richness and flexibility required for intelligent, autonomous
coordination. This report introduces the Knowledge Graph-Enhanced Multi-Agent
Infrastructure(KG-MAS), as resolution in addressing such limitations. KG-MAS
leverages a centralized Knowledge Graph (KG) as a dynamic, shared world model,
providing a common semantic foundation for a Multi-Agent System(MAS).
Autonomous agents, representing both physical and digital components, query
this KG for decision-making and update it with real-time state information. The
infrastructure features a model-driven architecture which facilitates the
automatic generation of agents from semantic descriptions, thereby simplifying
system extension and maintenance. By abstracting away underlying communication
protocols and providing a unified, intelligent coordination mechanism, KG-MAS
offers a robust, scalable, and flexible solution for coupling heterogeneous
physical and digital robotic environments.

</details>


### [8] [The Social Cost of Intelligence: Emergence, Propagation, and Amplification of Stereotypical Bias in Multi-Agent Systems](https://arxiv.org/abs/2510.10943)
*Thi-Nhung Nguyen,Linhao Luo,Thuy-Trang Vu,Dinh Phung*

Main category: cs.MA

TL;DR: 该研究探讨了多智能体系统（MAS）中刻板偏见的产生与传播机制，发现MAS通常比单智能体系统更易受偏见影响，但通过合作与辩论式通信以及更稳健的基础大语言模型可缓解偏见放大。


<details>
  <summary>Details</summary>
Motivation: 现有研究多聚焦于单个大语言模型的偏见问题，而多智能体系统中多个模型协作可能带来新的偏见动态，亟需系统性分析。

Method: 通过模拟代表不同社会群体的智能体，在多种交互与对抗场景下，评估内部专业化、基础LLM和通信协议对偏见稳健性、传播与放大效应的影响，并在三个偏见基准上进行实验。

Result: MAS通常比单智能体系统更不稳健，偏见常因内群体偏好早期出现；但合作与辩论式通信能减轻偏见放大，更稳健的底层LLM可提升系统整体稳定性。

Conclusion: 多智能体LLM系统中的公平性与鲁棒性受多种因素影响，包括通信方式和基础模型质量，需在系统设计中予以重视。

Abstract: Bias in large language models (LLMs) remains a persistent challenge,
manifesting in stereotyping and unfair treatment across social groups. While
prior research has primarily focused on individual models, the rise of
multi-agent systems (MAS), where multiple LLMs collaborate and communicate,
introduces new and largely unexplored dynamics in bias emergence and
propagation. In this work, we present a comprehensive study of stereotypical
bias in MAS, examining how internal specialization, underlying LLMs and
inter-agent communication protocols influence bias robustness, propagation, and
amplification. We simulate social contexts where agents represent different
social groups and evaluate system behavior under various interaction and
adversarial scenarios. Experiments on three bias benchmarks reveal that MAS are
generally less robust than single-agent systems, with bias often emerging early
through in-group favoritism. However, cooperative and debate-based
communication can mitigate bias amplification, while more robust underlying
LLMs improve overall system stability. Our findings highlight critical factors
shaping fairness and resilience in multi-agent LLM systems.

</details>


### [9] [Automating Structural Engineering Workflows with Large Language Model Agents](https://arxiv.org/abs/2510.11004)
*Haoran Liang,Yufa Zhou,Mohammad Talebi Kalaleh,Qipei Mei*

Main category: cs.MA

TL;DR: MASSE 是首个面向结构工程的多智能体系统，利用无需训练的 LLM 智能体实现工作流自动化，显著提升效率与准确性。


<details>
  <summary>Details</summary>
Motivation: 结构工程领域长期缺乏创新，核心工作流程数十年未变，尽管其具有巨大的经济影响；而近期 LLM 在复杂推理、长期规划和工具使用方面的能力进步，使其非常适合用于结构工程任务。

Method: 构建一个无需训练的基于大语言模型（LLM）的多智能体系统 MASSE，将其集成到真实世界的结构工程工作流程中，实现自动化执行设计规范解读、荷载计算和结构承载力验证等任务。

Result: 在真实案例中的全面验证表明，MASSE 能将专家工作量从约两小时减少到几分钟，同时提高实际工程场景中的可靠性与准确性。

Conclusion: MASSE 证明了 LLM 驱动的多智能体系统可有效自动化结构工程工作流，具备即插即用的专业部署能力，为传统工程领域带来显著效率与质量提升。

Abstract: We introduce $\textbf{MASSE}$, the first Multi-Agent System for Structural
Engineering, effectively integrating large language model (LLM)-based agents
with real-world engineering workflows. Structural engineering is a fundamental
yet traditionally stagnant domain, with core workflows remaining largely
unchanged for decades despite its substantial economic impact and global market
size. Recent advancements in LLMs have significantly enhanced their ability to
perform complex reasoning, long-horizon planning, and precise tool utilization
-- capabilities well aligned with structural engineering tasks such as
interpreting design codes, executing load calculations, and verifying
structural capacities. We present a proof-of-concept showing that most
real-world structural engineering workflows can be fully automated through a
training-free LLM-based multi-agent system. MASSE enables immediate deployment
in professional environments, and our comprehensive validation on real-world
case studies demonstrates that it can reduce expert workload from approximately
two hours to mere minutes, while enhancing both reliability and accuracy in
practical engineering scenarios.

</details>


### [10] [A Vision for Access Control in LLM-based Agent Systems](https://arxiv.org/abs/2510.11108)
*Xinfeng Li,Dong Huang,Jie Li,Hongyi Cai,Zhenhong Zhou,Wei Dong,XiaoFeng Wang,Yang Liu*

Main category: cs.MA

TL;DR: 本文提出了一种名为“智能体访问控制”（AAC）的新框架，将传统静态的访问控制机制转变为动态、上下文感知的信息流治理模型，以应对大语言模型智能体在复杂环境中动态交互所带来的安全挑战。


<details>
  <summary>Details</summary>
Motivation: 传统基于规则的静态访问控制机制无法应对大语言模型智能体在动态、复杂上下文中产生的信息流，因此需要一种更精细、灵活的信息治理方法。

Method: 提出Agent Access Control（AAC）框架，包含两个核心模块：（1）多维上下文评估，综合考量身份、关系、场景与规范；（2）自适应响应生成，通过删减、摘要和改写等方式动态调整信息输出，而非简单允许或拒绝。

Result: 该框架通过专用的访问控制推理引擎，实现了类人级别的细粒度判断与可扩展的AI安全之间的平衡，为可信智能体设计提供了新思路。

Conclusion: 未来可信智能体的安全机制应从二元访问控制转向动态信息流治理，AAC为此提供了一个有前景的研究方向。

Abstract: The autonomy and contextual complexity of LLM-based agents render traditional
access control (AC) mechanisms insufficient. Static, rule-based systems
designed for predictable environments are fundamentally ill-equipped to manage
the dynamic information flows inherent in agentic interactions. This position
paper argues for a paradigm shift from binary access control to a more
sophisticated model of information governance, positing that the core challenge
is not merely about permission, but about governing the flow of information. We
introduce Agent Access Control (AAC), a novel framework that reframes AC as a
dynamic, context-aware process of information flow governance. AAC operates on
two core modules: (1) multi-dimensional contextual evaluation, which assesses
not just identity but also relationships, scenarios, and norms; and (2)
adaptive response formulation, which moves beyond simple allow/deny decisions
to shape information through redaction, summarization, and paraphrasing. This
vision, powered by a dedicated AC reasoning engine, aims to bridge the gap
between human-like nuanced judgment and scalable Al safety, proposing a new
conceptual lens for future research in trustworthy agent design.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [11] [Fine-grained CDN Delegation](https://arxiv.org/abs/2510.09983)
*Ethan Thompson,Ali Sadeghi Jahromi,AbdelRahman Abdou*

Main category: cs.NI

TL;DR: 本文提出了一种名为Delegation Certificates（DeCerts）的新机制，通过扩展X.509证书标准，使域名所有者能够细粒度地控制CDN的委派权限，包括指定可委派/不可委派的子域名、控制委派深度，并实现无需依赖证书颁发机构（CA）的自主签发与灵活吊销机制，从而提升CDN委派的安全性、可扩展性与可管理性。


<details>
  <summary>Details</summary>
Motivation: 现有CDN委派方案（如RFC 9345中的Delegated Credentials）缺乏对委派链长度、吊销机制、允许操作及委派范围等关键方面的细粒度定义，限制了域名所有者对委派策略的控制能力。

Method: 提出DeCerts机制，修改X.509证书标准并引入新扩展字段，使域名所有者可自主签发委派证书，明确控制子域名委派范围与深度，并设计多种兼顾安全性、性能和控制权的吊销机制；同时在Firefox浏览器中实现验证逻辑作为概念验证。

Result: 成功在Firefox中实现DeCerts支持，验证了其在浏览器和TLS/HTTPS协议中的可行性与兼容性，证明DeCerts能有效提升CDN委派的灵活性、安全性和可管理性。

Conclusion: DeCerts为CDN委派提供了一种实用、安全且可扩展的解决方案，通过赋予域名所有者完全自主权，显著优于依赖CA的传统委派机制。

Abstract: The use of Content Delivery Networks (CDNs) has significantly increased over
the past decade, with approximately 55 million websites currently relying on
CDN services. Emerging solutions, such as Delegated Credentials (RFC 9345),
lack fine-grained definitions of many critical aspects of delegation, such as
the length of delegation chains, revocation mechanism, permitted operations,
and a well-defined scope for said delegation. We present Delegation
Certificates (DeCerts), which modify X.509 certificate standard and add new
extensions to enable fine-grained CDN delegation. DeCerts allow domain owners
to specify delegated and non-delegated subdomains, and control the depth of
delegation extended by CDNs, which provides flexibility in delegation
management. But more importantly, DeCerts are built on a new principle which
provides full autonomy to domain owners-domain owners can issue DeCerts fully
independent of Certificate Authorities (CAs), and thus have greater flexibility
in policy control, including revocation methods. Such level of flexibility
would be hard to match if CAs where to issue such certificates. Revoking a
DeCert revokes delegation. We discuss multiple revocation mechanisms for a
DeCerts balancing security, performance, and delegator control. We modify
Firefox to support DeCert (i.e., proper validation) as a proof-of-concept, and
test it to demonstrate the feasibility, compatibility of DeCerts with browsers
and TLS/HTTPS protocols. DeCerts enhance the security, scalability, and
manageability of CDN delegation, offering a practical solution for Internet
services.

</details>


### [12] [Pushing the Boundaries in CBRS Band: Robust Radar Detection within High 5G Interference](https://arxiv.org/abs/2510.10040)
*Shafi Ullah Khan,Michel Kulhandjian,Debashri Roy*

Main category: cs.NI

TL;DR: 本文研究了基于机器学习的方法在CBRS频段中提升频谱共享能力的潜力，特别是在5G商业信号与军用雷达系统共存的场景下。通过使用IQ数据和频谱图，所提出的模型在高干扰环境下（SINR低至-5 dB）仍能达到FCC推荐的99%雷达检测准确率，并能以93%的准确率识别六种雷达波形类型。


<details>
  <summary>Details</summary>
Motivation: 随着商业无线服务用户需求的不断增长，频谱共享成为关键策略，但其有效监管和技术实现，尤其是在与现有系统（如军用雷达）共存方面，仍面临重大挑战。因此，亟需提升在高干扰环境下的雷达检测与波形识别能力，以支持更高效的频谱共享。

Method: 采用机器学习方法，利用I/Q数据和频谱图作为输入，构建模型以实现雷达信号检测和波形分类。通过合成信号和真实信号对模型进行严格评估，测试其在不同SINR条件下的性能。

Result: 所提ML模型在SINR低至-5 dB的高干扰环境下仍能实现99%的雷达检测准确率，显著优于现有技术（此前约为12 dB）；同时，模型能以93%的准确率对六种雷达波形进行分类。

Conclusion: 机器学习方法可显著扩展频谱共享中雷达检测的SINR边界，提升在5G等强干扰环境下的共存能力，为CBRS等频谱共享机制提供有效技术支持。

Abstract: Spectrum sharing is a critical strategy for meeting escalating user demands
via commercial wireless services, yet its effective regulation and
technological enablement, particularly concerning coexistence with incumbent
systems, remain significant challenges. Federal organizations have established
regulatory frameworks to manage shared commercial use alongside
mission-critical operations, such as military communications. This paper
investigates the potential of machine learning (ML)-based approaches to enhance
spectrum sharing capabilities within the Citizens Broadband Radio Service
(CBRS) band, specifically focusing on the coexistence of commercial signals
(e.g., 5G) and military radar systems. We demonstrate that ML techniques can
potentially extend the Federal Communications Commission (FCC)-recommended
signal-to-interference-plus-noise ratio (SINR) boundaries by improving radar
detection and waveform identification in high-interference environments.
Through rigorous evaluation using both synthetic and real-world signals, our
findings indicate that proposed ML models, utilizing In-phase/Quadrature (IQ)
data and spectrograms, can achieve the FCC-recommended $99\%$ radar detection
accuracy even when subjected to high interference from 5G signals upto -5dB
SINR, exceeding the required limits of $20$ SINR. Our experimental studies
distinguish this work from the state-of-the-art by significantly extending the
SINR limit for $99\%$ radar detection accuracy from approximately $12$ dB down
to $-5$ dB. Subsequent to detection, we further apply ML to analyze and
identify radar waveforms. The proposed models also demonstrate the capability
to classify six distinct radar waveform types with $93\%$ accuracy.

</details>


### [13] [Waves of Imagination: Unconditional Spectrogram Generation using Diffusion Architectures](https://arxiv.org/abs/2510.10044)
*Rahul Vanukuri,Shafi Ullah Khan,Talip Tolga Sarı,Gokhan Secinti,Diego Patiño,Debashri Roy*

Main category: cs.NI

TL;DR: 本文提出一种基于扩散模型的生成方法，用于合成包含LTE、5G和雷达信号的CBRS频段频谱图，以解决真实雷达数据稀缺导致的数据集不平衡问题；生成数据在结构和统计特性上与真实数据高度相似，并能显著提升雷达检测模型的训练效率。


<details>
  <summary>Details</summary>
Motivation: 在CBRS等共享频段中，为保护军用通信免受商用无线信号干扰，需依赖大量高质量、多样且标注良好的频谱图数据训练雷达检测算法，但真实雷达信号稀少且出现频率低，导致难以构建平衡数据集，限制了AI模型的性能与泛化能力。

Method: 采用基于扩散的生成模型，合成CBRS频段内包含LTE、5G和雷达信号的五类逼真且多样的频谱图；使用SSIM和PSNR等指标对生成频谱图进行结构与统计保真度分析，并在真实雷达检测任务中验证其预训练效果。

Result: 生成的频谱图在SSIM和PSNR指标上表现出与训练数据高度相似性；在真实雷达检测任务中，使用生成数据预训练可使模型收敛速度提升51.5%。

Conclusion: 所提出的扩散生成模型能有效缓解真实雷达数据稀缺问题，提升频谱图数据集的多样性与平衡性，并显著增强雷达检测模型的训练效率与泛化能力。

Abstract: The growing demand for effective spectrum management and interference
mitigation in shared bands, such as the Citizens Broadband Radio Service
(CBRS), requires robust radar detection algorithms to protect the military
transmission from interference due to commercial wireless transmission. These
algorithms, in turn, depend on large, diverse, and carefully labeled
spectrogram datasets. However, collecting and annotating real-world radio
frequency (RF) spectrogram data remains a significant challenge, as radar
signals are rare, and their occurrences are infrequent. This challenge makes
the creation of balanced datasets difficult, limiting the performance and
generalizability of AI models in this domain.
  To address this critical issue, we propose a diffusion-based generative model
for synthesizing realistic and diverse spectrograms of five distinct categories
that integrate LTE, 5G, and radar signals within the CBRS band. We conduct a
structural and statistical fidelity analysis of the generated spectrograms
using widely accepted evaluation metrics Structural Similarity Index Measure
(SSIM) and Peak Signal-to-Noise Ratio (PSNR), to quantify their divergence from
the training data. Furthermore, we demonstrate that pre-training on the
generated spectrograms significantly improves training efficiency on a
real-world radar detection task by enabling $51.5\%$ faster convergence.

</details>


### [14] [Zephyrus: Scaling Gateways Beyond the Petabit-Era with DPU-Augmented Hierarchical Co-Offloading](https://arxiv.org/abs/2510.11043)
*Yuemeng Xu,Haoran Chen,Jiarui Guo,Mingwei Cui,Qiuheng Yin,Cheng Dong,Daxiang Kang,Xian Wu,Chenmin Sun,Peng He,Yang Gao,Lirong Lai,Kai Wang,Hongyu Wu,Tong Yang,Xiyun Xu*

Main category: cs.NI

TL;DR: ByteDance提出Zephyrus，一种结合Tofino与DPU的高性能云网关系统，通过统一P4流水线和分层协同卸载架构（HLCO），实现高吞吐、低功耗、低成本，并已在生产环境中部署。


<details>
  <summary>Details</summary>
Motivation: 面对不断增长的云网流量，上一代云网关在资源压力下难以为继，亟需一种既能保持低延迟高吞吐，又具备更大表容量和更强可编程性的新架构。

Method: 构建基于统一P4流水线的异构网关Zephyrus，整合Tofino交换芯片与DPU，并设计分层协同卸载架构（HLCO）以高效调度流量，实现硬件高比例卸载与软件回退路径的结合。

Result: Zephyrus相比LuoShen（NSDI '24）吞吐量提升33%，功耗降低21%，硬件成本降低14%；相比FPGA方案Albatross（SIGCOMM '25），吞吐翻倍且总体拥有成本（TCO）显著更低。

Conclusion: Zephyrus验证了DPU与Tofino融合架构在大规模云网关中的可行性与优越性，其设计经验为未来高性能云网关研发提供了重要参考。

Abstract: Operating at petabit-scale, ByteDance's cloud gateways are deployed at
critical aggregation points to orchestrate a wide array of business traffic.
However, this massive scale imposes significant resource pressure on our
previous-generation cloud gateways, rendering them unsustainable in the face of
ever-growing cloud-network traffic. As the DPU market rapidly expands, we see a
promising path to meet our escalating business traffic demands by integrating
DPUs with our established Tofino-based gateways. DPUs augment these gateways
with substantially larger table capacities and richer programmability without
compromising previously low-latency and high-throughput forwarding. Despite
compelling advantages, the practical integration of DPUs into cloud gateways
remains unexplored, primarily due to underlying challenges. In this paper, we
present Zephyrus, a production-scale gateway built upon a unified P4 pipeline
spanning high-performance Tofino and feature-rich DPUs, which successfully
overcomes these challenges. We further introduce a hierarchical co-offloading
architecture (HLCO) to orchestrate traffic flow within this heterogeneous
gateway, achieving > 99% hardware offloading while retaining software fallback
paths for complex operations. Zephyrus outperforms LuoShen (NSDI '24) with 33%
higher throughput and our evaluation further indicates 21% lower power
consumption and 14% lower hardware cost. Against FPGA-based systems, Albatross
(SIGCOMM '25), it doubles the throughput at a substantially lower Total Cost of
Ownership (TCO), showcasing its superior performance-per-dollar. Beyond these
performance gains, we also share key lessons from several years of developing
and operating Zephyrus at production scale. We believe these insights provide
valuable references for researchers and practitioners designing performant
cloud gateways.

</details>


### [15] [Visible Light Communication for Vehicular Networks: A Tutorial](https://arxiv.org/abs/2510.11123)
*Pedro E. Gória Silva,Eduardo S. Lima,Jules M. Moualeu,Mohamed Korium,Pedro H. J. Nardelli*

Main category: cs.NI

TL;DR: 本文综述了基于可见光通信（VLC）的车联网系统实现方法，涵盖系统结构、关键技术挑战（如光照干扰、闪烁、上行安全等）及未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 为实现车联网和智能交通系统对实时性与安全性的需求，探索具备高可靠性和高数据速率的短距离通信技术，可见光通信（VLC）成为有前景的解决方案，但其实际部署仍面临诸多挑战。

Method: 通过系统性综述方法，分析VLC车联网的实现特性，包括发射器、信道、接收器结构、标准化进展、拓扑类型，并探讨太阳光与人工光源干扰、闪烁、调光、吞吐量提升、上行安全及移动性等实际问题。

Result: 系统梳理了VLC在车联网中的实现架构与关键影响因素，识别出若干技术挑战并提出潜在解决方案。

Conclusion: VLC在车联网中具有应用潜力，但仍需解决光照干扰、移动性支持、标准化等关键问题；未来研究应聚焦于提升系统鲁棒性与实用性，推动商业化进程。

Abstract: The advent of the fifth-generation technology promises to bring about more
vertical applications and emerging services that include vehicular networks and
intelligent transportation systems (ITSs). To achieve their vision of real-time
and safetyapplications, vehicular networks rely on short-range to medium-range
communications. One emerging technology that aims to provide reliability and
high-data rate in short-range communications is the visible light
communications (VLC). Due to its remarkable advantages, some studies have
recently investigated the integration of VLC in vehicular networks and ITSs.
Despite their attractive features, such networks also face several
implementation issues. This paper provides an extended tutorial on the
implementation of VLC-based vehicular networks. To begin with, we present the
implementation characteristics of these systems and discuss some related
issues. The underlying system considers a general structure with transmitters,
channels, and receivers based on photodetectors and cameras, as well as
standardization efforts and types of topologies. In addition, we discuss the
impact of the sun and artificial light sources, flickering, dimming, throughput
enhancement, uplink security, and mobility on practical implementation.
Finally, we highlight some key challenges and potential solutions and provide
some directions for future research investigations that could constitute an
advancement toward the development of commercial VLC-based vehicular systems.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [16] [CPU-Limits kill Performance: Time to rethink Resource Control](https://arxiv.org/abs/2510.10747)
*Chirag Shetty,Sarthak Chakraborty,Hubertus Franke,Larisa Shwartz,Chandra Narayanaswami,Indranil Gupta,Saurabh Jha*

Main category: cs.DC

TL;DR: 该论文质疑在云原生应用中普遍使用CPU限制（CPU-limits）的做法，指出其对延迟敏感型应用的性能和成本有负面影响，并主张完全摒弃此类限制，从而推动自动扩缩容和计费范式的根本性重构。


<details>
  <summary>Details</summary>
Motivation: 当前学术界和工业界普遍认为CPU限制对于容器资源管理和系统安全至关重要，但实际云用户经验表明，CPU限制常常损害应用性能并增加成本，这与传统认知相矛盾。

Method: 通过实证分析和对实际云用户经验的观察，评估CPU限制在不同应用场景下的影响，并对比其在延迟敏感型应用与后台任务中的适用性。

Result: 研究发现，对于延迟敏感型应用，CPU限制弊大于利；但在特定场景（如后台任务）中，若合理使用，仍可带来益处。

Conclusion: 应重新审视CPU限制的必要性，尤其在延迟敏感型应用中应考虑完全弃用，同时推动自动扩缩容与计费机制的革新。

Abstract: Research in compute resource management for cloud-native applications is
dominated by the problem of setting optimal CPU limits -- a fundamental OS
mechanism that strictly restricts a container's CPU usage to its specified
CPU-limits . Rightsizing and autoscaling works have innovated on
allocation/scaling policies assuming the ubiquity and necessity of CPU-limits .
We question this. Practical experiences of cloud users indicate that CPU-limits
harms application performance and costs more than it helps. These observations
are in contradiction to the conventional wisdom presented in both academic
research and industry best practices. We argue that this indiscriminate
adoption of CPU-limits is driven by erroneous beliefs that CPU-limits is
essential for operational and safety purposes. We provide empirical evidence
making a case for eschewing CPU-limits completely from latency-sensitive
applications. This prompts a fundamental rethinking of auto-scaling and billing
paradigms and opens new research avenues. Finally, we highlight specific
scenarios where CPU-limits can be beneficial if used in a well-reasoned way
(e.g. background jobs).

</details>


### [17] [THEAS: Efficient Power Management in Multi-Core CPUs via Cache-Aware Resource Scheduling](https://arxiv.org/abs/2510.09847)
*Said Muhammad,Lahlou Laaziz,Nadjia Kara,Phat Tan Nguyen,Timothy Murphy*

Main category: cs.DC

TL;DR: 本文提出了一种名为THEAS的动态资源调度算法，在异构系统中通过自适应调整资源水平，在保障性能的同时提升能效，尤其适用于负载波动大、任务非绑定的实时应用场景。


<details>
  <summary>Details</summary>
Motivation: 在负载随时间显著波动的异构系统中，传统调度策略难以兼顾性能与能效，尤其对于非绑定任务，亟需一种能动态适应负载变化的调度机制。

Method: 提出并部署THEAS算法，该算法通过动态调整计算资源水平，实现性能与功耗之间的平衡，并在核心选择、性能扩展、缓存感知等方面进行优化。

Result: 通过与CFS、EAS、HeteroSched和基于效用的调度等经典算法对比，THEAS在适应性、核心选择、性能扩展、缓存感知、开销和实时适用性等方面表现更优。

Conclusion: THEAS算法在异构系统中有效兼顾了实时性能与能效，适用于广泛的实际应用场景，展现出优于现有调度策略的综合能力。

Abstract: The dynamic adaptation of resource levels enables the system to enhance
energy efficiency while maintaining the necessary computational resources,
particularly in scenarios where workloads fluctuate significantly over time.
The proposed approach can play a crucial role in heterogeneous systems where
workload characteristics are not uniformly distributed, such as non-pinning
tasks. The deployed THEAS algorithm in this research work ensures a balance
between performance and power consumption, making it suitable for a wide range
of real-time applications. A comparative analysis of the proposed THEAS
algorithm with well-known scheduling techniques such as Completely Fair
Scheduler (CFS), Energy-Aware Scheduling (EAS), Heterogeneous Scheduling
(HeteroSched), and Utility-Based Scheduling is presented in Table III. Each
scheme is compared based on adaptability, core selection criteria, performance
scaling, cache awareness, overhead, and real-time suitability.

</details>


### [18] [Proactive and Reactive Autoscaling Techniques for Edge Computing](https://arxiv.org/abs/2510.10166)
*Suhrid Gupta,Muhammed Tawfiqul Islam,Rajkumar Buyya*

Main category: cs.DC

TL;DR: 本文综述了边缘计算架构及其在满足服务等级协议（SLA）方面的资源扩展挑战，介绍了现有自动扩展算法的优缺点。


<details>
  <summary>Details</summary>
Motivation: 边缘计算需通过微服务架构实现低延迟以满足SLA要求，但现有自动扩展算法存在性能问题和配置复杂性，因此需系统梳理相关研究。

Method: 对边缘计算架构、SLA定义及现有边缘环境中用于满足SLA的自动扩展算法进行综述与分析。

Result: 总结了当前边缘计算中资源扩展算法的优势与不足，为后续研究提供参考。

Conclusion: 需进一步研究更高效、易配置的自动扩展机制，以在混合云边环境中有效保障SLA合规。

Abstract: Edge computing allows for the decentralization of computing resources. This
decentralization is achieved through implementing microservice architectures,
which require low latencies to meet stringent service level agreements (SLA)
such as performance, reliability, and availability metrics. While cloud
computing offers the large data storage and computation resources necessary to
handle peak demands, a hybrid cloud and edge environment is required to ensure
SLA compliance. Several auto-scaling algorithms have been proposed to try to
achieve these compliance challenges, but they suffer from performance issues
and configuration complexity. This chapter provides a brief overview of edge
computing architecture, its uses, benefits, and challenges for resource
scaling. We then introduce Service Level Agreements, and existing research on
devising algorithms used in edge computing environments to meet these
agreements, along with their benefits and drawbacks.

</details>


### [19] [SP-MoE: Speculative Decoding and Prefetching for Accelerating MoE-based Model Inference](https://arxiv.org/abs/2510.10302)
*Liangkun Chen,Zijian Wen,Tian Wu,Xiaoxi Zhang,Chuan Wu*

Main category: cs.DC

TL;DR: SP-MoE 是首个面向推测解码（SD）的专家卸载与计算通信流水线框架，通过推测性专家预取、截断层策略和流水线运行时，显著加速 MoE 模型推理。


<details>
  <summary>Details</summary>
Motivation: 将 Mixture-of-Experts（MoE）与推测解码（SD）结合虽能提升推理速度，但会加剧 GPU 内存压力和 CPU-GPU 带宽竞争，而现有 MoE 卸载系统未考虑 SD 特性，无法解决该瓶颈。

Method: SP-MoE 提出三项关键技术：(1) 利用草稿模型与目标模型结构对应关系进行推测性专家预取；(2) 基于实证分析和延迟模型设定每层预取深度上限的截断层策略；(3) 采用异步预取线程与批处理 I/O 的流水线运行时以隐藏加载延迟。

Result: 在多种数据集、环境和 MoE 模型上，SP-MoE 相比当前最优方法实现了 1.07 至 3.5 倍的每输出 token 时间（TPOT）加速。

Conclusion: SP-MoE 有效解决了 MoE 与推测解码结合时的内存与带宽瓶颈，显著提升了推理效率，为高效 MoE 推理提供了新范式。

Abstract: The Mixture-of-Experts (MoE) architecture has been widely adopted in large
language models (LLMs) to reduce computation cost through model sparsity.
Employing speculative decoding (SD) can further accelerate MoE inference by
drafting multiple tokens per step and verifying them in parallel. However,
combining MoE with SD inflates GPU memory and aggravates CPU-GPU bandwidth
contention during multi-token verification. Existing MoE offloading systems are
SD-agnostic and do not address this bottleneck. We present SP-MoE, the first
SD-aware expert-offloading and compute-communication pipelining framework.
SP-MoE introduces: (1) speculative expert prefetching that exploits structural
correspondence between the draft and target models to prefetch likely experts
ahead of verification; (2) a cutoff-layer policy that bounds per-layer prefetch
depth based on empirical profiles and an analytical latency model, guaranteeing
just-in-time availability without overfetch; and (3) a pipelined runtime with
asynchronous prefetch threads and batched I/O to hide loading latency.
Extensive experiments demonstrate that SP-MoE achieves a 1.07-3.5 times TPOT
speedup over state-of-the-art methods across diverse datasets, environments,
and MoE-based models.

</details>


### [20] [FLAMMABLE: A Multi-Model Federated Learning Framework with Multi-Model Engagement and Adaptive Batch Sizes](https://arxiv.org/abs/2510.10380)
*Shouxu Lin,Zimeng Pan,Yuhang Yao,Haeyoung Noh,Pei Zhang,Carlee Joe-Wong*

Main category: cs.DC

TL;DR: FLAMMABLE 是一个面向多模型联邦学习（MMFL）的训练框架，通过智能调整客户端批处理大小并根据其系统能力分配多个模型进行训练，显著提升了训练速度和模型精度。


<details>
  <summary>Details</summary>
Motivation: 多模型联邦学习（MMFL）面临数据、系统和模型间的多重异构性挑战，现有单模型联邦学习方法及其简单扩展无法有效应对，亟需专门优化的训练框架。

Method: 提出 FLAMMABLE 框架，在每轮训练中根据客户端的系统能力动态调整批处理大小，并智能选择多个模型进行训练；同时构建首个 MMFL 基准平台以支持可复现研究。

Result: 在多个数据集和模型上的实验表明，FLAMMABLE 相比基线方法将时间-精度性能提升 1.1～10.0 倍，最终模型精度提高 1.3～5.4%。

Conclusion: FLAMMABLE 有效解决了 MMFL 中的异构性问题，显著提升了训练效率与模型准确性，为未来 MMFL 研究提供了实用框架和基准平台。

Abstract: Multi-Model Federated Learning (MMFL) is an emerging direction in Federated
Learning (FL) where multiple models are trained in parallel, generally on
various datasets. Optimizing the models' accuracies and training times in the
MMFL setting requires adapting to data and system heterogeneity across clients
as in single-model FL; these challenges are amplified in the MMFL setting due
to additional heterogeneity across models. Neither existing solutions nor
na\"ive extensions of single-model FL frameworks efficiently address these
challenges. To bridge this gap, we propose FLAMMABLE, a comprehensive MMFL
training framework. FLAMMABLE optimizes model training by intelligently
adapting client batch sizes while engaging them to train multiple carefully
chosen models, depending on their system capabilities, in each training round.
To evaluate FLAMMABLE, we develop the first benchmark platform for the MMFL
setting, which may enable future reproducible MMFL research. Extensive
evaluations on multiple datasets and models show that FLAMMABLE boosts the MMFL
time-to-accuracy performance by 1.1$\sim$10.0$\times$ while improving the final
model accuracy by 1.3$\sim$5.4\% compared to several known baselines.

</details>


### [21] [DCP: Addressing Input Dynamism In Long-Context Training via Dynamic Context Parallelism](https://arxiv.org/abs/2510.10620)
*Chenyu Jiang,Zhenkun Cai,Ye Tian,Zhen Jia,Yida Wang,Chuan Wu*

Main category: cs.DC

TL;DR: 本文提出了DCP，一种动态上下文并行训练框架，通过细粒度的分块策略自适应处理不同序列长度和注意力模式，从而减少通信开销并提升计算与内存均衡性。


<details>
  <summary>Details</summary>
Motivation: 现有上下文并行方法采用静态并行配置，无法适应训练数据中序列长度和注意力模式的动态变化，导致通信开销大和计算负载不均衡。

Method: 提出DCP框架，对数据和计算进行细粒度的块级划分，并灵活地将这些块映射到设备上，以适应不同样本的序列特性。

Result: 微基准测试显示，在因果掩码下注意力计算加速1.19x~2.45x，在稀疏注意力模式下加速2.15x~3.77x；端到端训练速度在因果掩码下提升0.94x~1.16x，在稀疏掩码下提升1.00x~1.46x。

Conclusion: DCP通过动态适配序列特征显著提升了长上下文训练的效率和资源利用率，优于现有静态上下文并行方法。

Abstract: Context parallelism has emerged as a key technique to support long-context
training, a growing trend in generative AI for modern large models. However,
existing context parallel methods rely on static parallelization configurations
that overlook the dynamic nature of training data, specifically, the
variability in sequence lengths and token relationships (i.e., attention
patterns) across samples. As a result, these methods often suffer from
unnecessary communication overhead and imbalanced computation. In this paper,
we present DCP, a dynamic context parallel training framework that introduces
fine-grained blockwise partitioning of both data and computation. By enabling
flexible mapping of data and computation blocks to devices, DCP can adapt to
varying sequence characteristics, effectively reducing communication and
improving memory and computation balance. Micro-benchmarks demonstrate that DCP
accelerates attention by 1.19x~2.45x under causal masks and 2.15x~3.77x under
sparse attention patterns. Additionally, we observe up to 0.94x~1.16x
end-to-end training speed-up for causal masks, and 1.00x~1.46x for sparse
masks.

</details>


### [22] [Fair Kernel-Lock-Free Claim/Release Protocol for Shared Object Access in Cooperatively Scheduled Runtimes](https://arxiv.org/abs/2510.10818)
*Kevin Chalmers,Jan Bækgaard Pedersen*

Main category: cs.DC

TL;DR: 本文提出了一种无自旋、无内核锁的互斥机制，支持用户态调度器，并通过CSP/FDR形式化验证其具备FIFO公平性和线性一致性。


<details>
  <summary>Details</summary>
Motivation: 为进程导向语言（如ProcessJ）设计一种能与用户态调度器协作、保证公平性和正确性的互斥机制，用于管理对共享通信通道的访问竞争。

Method: 采用无锁队列停放等待进程，基于先入先出顺序处理资源请求；利用CSP建模和FDR验证协议的互斥、公平性与线性一致性，并提出可复用的公平性验证方法。

Result: 成功构建了形式化验证的互斥协议，确保对共享资源（如通道）的访问具有FIFO公平性和线性一致性，并验证其行为符合互斥锁规范。

Conclusion: 所提出的互斥机制不仅适用于ProcessJ语言，其公平性验证方法和稳定性证明策略也可复用于其他协程运行时系统设计。

Abstract: We present the first spin-free, kernel-lock-free mutex that cooperates with
user-mode schedulers and is formally proven FIFO-fair and linearizable using
CSP/FDR. Our fairness oracle and stability-based proof method are reusable
across coroutine runtime designs. We designed the claim/release protocol for a
process-oriented language -- ProcessJ -- to manage the race for claiming shared
inter-process communication channels. Internally, we use a lock-free queue to
park waiting processes for gaining access to a shared object, such as exclusive
access to a shared channel to read from or write to. The queue ensures control
and fairness for processes wishing to access a shared resource, as the protocol
handles claim requests in the order they are inserted into the queue. We
produce CSP models of our protocol and a mutex specification, demonstrating
with FDR that our protocol behaves as a locking mutex.

</details>


### [23] [FIDRS: A Novel Framework for Integrated Distributed Reliable Systems](https://arxiv.org/abs/2510.10833)
*Mehdi Zekriyapanah Gashti*

Main category: cs.DC

TL;DR: 本文提出了一种名为FIDRS的新框架，通过异构分布式数据库和RMSD算法提升集成系统的性能、效率与可靠性。


<details>
  <summary>Details</summary>
Motivation: 现有集成系统框架在性能、响应速度和可靠性方面存在不足，亟需一种更高效可靠的解决方案。

Method: 提出FIDRS框架，采用异构分布式数据库技术提升响应速度与可靠性，并在提取阶段引入RMSD算法以减少大数据库中的响应时间。

Result: 仿真结果表明，新框架在效率、性能和可靠性方面优于以往框架，并解决了部分原有问题。

Conclusion: FIDRS框架有效提升了集成分布式系统的整体表现，为构建高性能、高可靠系统提供了可行路径。

Abstract: In this paper we represent a new framework for integrated distributed and
reliable systems. In the proposed framework we have used three parts to
increase Satisfaction and Performance of this framework. At first we analyze
previous frameworks related to integrated systems, then represent new proposed
framework in order to improving previous framework, and we discuss its
different phases. Finally we compare the results of simulation of the new
framework with previous ones. In FIDRS framework, the technique of
heterogeneous distributed data base is used to improve Performance and speed in
responding to users and in this way we can improve dependability and
reliability of framework simultaneously. In extraction phase of the new
framework we have used RMSD algorithm that decreases responding time in big
database. Finally by using FDIRS framework we succeeded to increase Efficiency,
Performance and reliability of integrated systems and remove some of previous
frameworks problems.

</details>


### [24] [An Explorative Study on Distributed Computing Techniques in Training and Inference of Large Language Models](https://arxiv.org/abs/2510.11211)
*Sheikh Azizul Hakim,Saem Hasan*

Main category: cs.DC

TL;DR: 本文从两个角度探讨了分布式计算技术在大语言模型（LLM）中的应用：一是研究如何在消费级计算机上运行大模型，并提出一种基于元启发式的系统改进方法；二是对三种最先进的LLM服务技术进行比较研究。


<details>
  <summary>Details</summary>
Motivation: 由于当前的大语言模型参数规模庞大，单个计算节点难以承担其训练、微调或推理任务，因此需要借助分布式计算技术来有效利用这些模型。本文旨在探索如何通过分布式方法降低LLM使用门槛并评估现有服务技术的性能。

Method: 一方面，研究使LLM能在消费级硬件上运行的分布式技术，并对现有系统引入一种新颖的基于元启发式的改进方法；另一方面，对三种前沿的LLM服务技术进行对比分析。

Result: 实现了适用于消费级设备的LLM运行方案，并通过元启发式方法优化了现有系统；同时，对三种主流LLM服务技术进行了系统性比较，揭示了它们在不同场景下的优劣。

Conclusion: 分布式计算技术对于推动大语言模型的普及和高效部署至关重要。通过算法优化和系统比较，本文为在资源受限环境下使用LLM提供了可行路径，并为服务系统选型提供了参考依据。

Abstract: Large language models (LLM) are advanced AI systems trained on extensive
textual data, leveraging deep learning techniques to understand and generate
human-like language. Today's LLMs with billions of parameters are so huge that
hardly any single computing node can train, fine-tune, or infer from them.
Therefore, several distributed computing techniques are being introduced in the
literature to properly utilize LLMs. We have explored the application of
distributed computing techniques in LLMs from two angles.
  \begin{itemize}
  \item We study the techniques that democratize the LLM, that is, how large
models can be run on consumer-grade computers. Here, we also implement a novel
metaheuristics-based modification to an existing system.
  \item We perform a comparative study on three state-of-the-art LLM serving
techniques. \end{itemize}

</details>


### [25] [An Asynchronous Many-Task Algorithm for Unstructured $S_{N}$ Transport on Shared Memory Systems](https://arxiv.org/abs/2510.11513)
*Alex Elwood,Tom Deakin,Justin Lovegrove,Chris Nelson*

Main category: cs.DC

TL;DR: 本文分析了非结构网格上离散纵标（$S_N$）输运求解器在现代多核架构中的性能瓶颈，提出了一种新的异步多任务（AMT）共享内存并行算法，显著提升了计算性能。


<details>
  <summary>Details</summary>
Motivation: 非结构网格上的离散纵标输运求解器由于复杂的数据依赖、内存访问模式和高维计算域，在现代多核架构上难以有效扩展，亟需优化其共享内存并行策略。

Method: 作者首先分析现有求解器在共享内存并行化中的性能瓶颈，并在多种计算硬件上评估其性能；随后提出一种新的异步多任务（AMT）算法用于共享内存并行。

Result: 新提出的AMT算法相比现有方法在计算性能上有显著提升，并通过分析揭示了性能提升的原因。

Conclusion: 异步多任务（AMT）方法能有效缓解非结构网格$S_N$输运求解器在多核架构上的性能瓶颈，为高性能计算中的输运问题提供了更高效的并行解决方案。

Abstract: Discrete ordinates $S_N$ transport solvers on unstructured meshes pose a
challenge to scale due to complex data dependencies, memory access patterns and
a high-dimensional domain. In this paper, we review the performance bottlenecks
within the shared memory parallelization scheme of an existing transport solver
on modern many-core architectures with high core counts. With this analysis, we
then survey the performance of this solver across a variety of compute
hardware. We then present a new Asynchronous Many-Task (AMT) algorithm for
shared memory parallelism, present results showing an increase in computational
performance over the existing method, and evaluate why performance is improved.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [26] [A Comprehensive Survey on Benchmarks and Solutions in Software Engineering of LLM-Empowered Agentic System](https://arxiv.org/abs/2510.09721)
*Jiale Guo,Suizhi Huang,Mei Li,Dong Huang,Xingsheng Chen,Regina Zhang,Zhijiang Guo,Han Yu,Siu-Ming Yiu,Christian Jensen,Pietro Lio,Kwok-Yan Lam*

Main category: cs.SE

TL;DR: 本文对大语言模型（LLM）在软件工程中的应用进行了首次全面综述，构建了一个涵盖150多篇论文的分类体系，从解决方案（提示、微调、智能体）和基准任务（代码生成、翻译、修复等）两个维度系统梳理了该领域的发展，并提出了统一的工作流框架与未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 当前LLM驱动的软件工程研究缺乏对基准任务与解决方案之间关联的系统性理解，阻碍了该领域的评估与进步。

Method: 作者分析了150余篇近期论文，构建了一个二维分类体系：一是按解决方案分为提示工程、微调和智能体范式；二是按基准任务涵盖代码生成、翻译、修复等。同时提出统一工作流管道，连接50多个基准与其对应解决方案。

Result: 研究揭示了该领域从简单提示工程向复杂智能体系统的演进趋势，包括规划与分解、推理与自优化、记忆机制和工具增强等关键技术，并系统关联了各类基准与适用的解决方案。

Conclusion: 本综述为研究人员和从业者提供了理解、评估和推进LLM赋能软件工程系统的全面资源，并指出了多智能体协作、自进化代码生成和形式化验证融合等未来方向。

Abstract: The integration of LLMs into software engineering has catalyzed a paradigm
shift from traditional rule-based systems to sophisticated agentic systems
capable of autonomous problem-solving. Despite this transformation, the field
lacks a comprehensive understanding of how benchmarks and solutions
interconnect, hindering systematic progress and evaluation. This survey
presents the first holistic analysis of LLM-empowered software engineering,
bridging the critical gap between evaluation and solution approaches. We
analyze 150+ recent papers and organize them into a comprehensive taxonomy
spanning two major dimensions: (1) Solutions, categorized into prompt-based,
fine-tuning-based, and agent-based paradigms, and (2) Benchmarks, covering code
generation, translation, repair, and other tasks. Our analysis reveals how the
field has evolved from simple prompt engineering to complex agentic systems
incorporating planning and decomposition, reasoning and self-refinement, memory
mechanisms, and tool augmentation. We present a unified pipeline that
illustrates the complete workflow from task specification to final
deliverables, demonstrating how different solution paradigms address varying
complexity levels across software engineering tasks. Unlike existing surveys
that focus on isolated aspects, we provide full-spectrum coverage connecting
50+ benchmarks with their corresponding solution strategies, enabling
researchers to identify optimal approaches for specific evaluation criteria.
Furthermore, we identify critical research gaps and propose actionable future
directions, including multi-agent collaboration frameworks, self-evolving code
generation systems, and integration of formal verification with LLM-based
methods. This survey serves as a foundational resource for researchers and
practitioners seeking to understand, evaluate, and advance LLM-empowered
software engineering systems.

</details>


### [27] [InteractScience: Programmatic and Visually-Grounded Evaluation of Interactive Scientific Demonstration Code Generation](https://arxiv.org/abs/2510.09724)
*Qiaosheng Chen,Yang Liu,Lei Li,Kai Chen,Qipeng Guo,Gong Cheng,Fei Yuan*

Main category: cs.SE

TL;DR: 本文提出了InteractScience，首个用于评估大语言模型在科学领域中结合领域知识与交互式前端代码生成能力的基准测试，并通过程序化功能测试与视觉定性评估相结合的混合框架进行评测。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试无法全面评估大语言模型在科学交互式演示生成方面的能力，因其通常仅关注知识问答或静态网页代码生成，缺乏对科学知识与交互逻辑结合的综合评估。

Method: 设计了一个混合评估框架，结合程序化功能测试（验证交互逻辑）和基于视觉的定性测试（比对渲染输出与参考快照），并构建了包含五个科学领域、配有单元测试、参考快照和检查清单的InteractScience基准。

Result: 对30个主流开源和闭源大语言模型的评估显示，它们在整合科学知识与交互式前端编码方面仍存在明显不足。

Conclusion: InteractScience是首个能自动评估模型在真实交互操作中融合科学知识与前端交互代码生成能力的基准，为提升可靠且具教育价值的科学演示生成奠定了基础。

Abstract: Large Language Models (LLMs) are increasingly capable of generating complete
applications from natural language instructions, creating new opportunities in
science and education. In these domains, interactive scientific demonstrations
are particularly valuable for explaining concepts, supporting new teaching
methods, and presenting research findings. Generating such demonstrations
requires models to combine accurate scientific knowledge with the ability to
implement interactive front-end code that behaves correctly and responds to
user actions. This capability goes beyond the scope of existing benchmarks,
which typically evaluate either knowledge question answering without grounding
in code or static web code generation without scientific interactivity. To
evaluate this integrated ability, we design a hybrid framework that combines
programmatic functional testing to rigorously verify interaction logic with
visually-grounded qualitative testing to assess rendered outputs against
reference snapshots. Building on this framework, we present InteractScience, a
benchmark consisting of a substantial set of carefully designed questions
across five scientific domains, each paired with unit tests, reference
snapshots, and checklists. We evaluate 30 leading open- and closed-source LLMs
and report results that highlight ongoing weaknesses in integrating domain
knowledge with interactive front-end coding. Our work positions InteractScience
as the first benchmark to automatically measure this combined capability with
realistic interactive operations, providing a foundation for advancing reliable
and educationally useful scientific demonstration code generation. All code and
data are publicly available at https://github.com/open-compass/InteractScience.

</details>


### [28] [OFP-Repair: Repairing Floating-point Errors via Original-Precision Arithmetic](https://arxiv.org/abs/2510.09938)
*Youshuai Tan,Zishuo Ding,Jinfu Chen,Weiyi Shang*

Main category: cs.SE

TL;DR: 本文提出了一种名为OFP-Repair的新方法，用于修复浮点程序中的错误，能够有效区分并修复仅需原始精度即可修复的错误，避免不必要的高精度计算开销。实验表明，该方法在多个指标上显著优于现有工具，并在真实案例（如GNU Scientific Library的长期未修复bug）中展现出良好的实用性和修复能力。


<details>
  <summary>Details</summary>
Motivation: 浮点程序中的错误在军事、航天和金融等关键领域可能导致严重后果。现有修复工具要么依赖耗时且需要专业知识的高精度实现，要么只能修复有限类型的错误，且开发者难以判断哪些错误可用原始精度修复。因此，亟需一种能自动区分并高效修复这两类错误的方法。

Method: 提出OFP-Repair方法，无需依赖高精度程序实现，通过分析判断错误是否可用原始精度修复，并生成相应补丁。

Result: 在ACESO数据集上，OFP-Repair在四个精度指标上分别提升了3、7、3和8个数量级；在真实案例中成功检测出全部5个可原始精度修复的错误并修复其中3个（ACESO仅修复1个）；在GNU Scientific Library的15个长期bug中成功修复5个，获得开发者认可。

Conclusion: OFP-Repair在浮点程序错误修复方面具有显著优势和实际应用价值，能有效减少对高精度计算的依赖，提升修复效率与适用性，具备良好的工程落地前景。

Abstract: Errors in floating-point programs can lead to severe consequences,
particularly in critical domains such as military, aerospace, and financial
systems, making their repair a crucial research problem. In practice, some
errors can be fixed using original-precision arithmetic, while others require
high-precision computation. Developers often avoid addressing the latter due to
excessive computational resources required. However, they sometimes struggle to
distinguish between these two types of errors, and existing repair tools fail
to assist in this differentiation. Most current repair tools rely on
high-precision implementations, which are time-consuming to develop and demand
specialized expertise. Although a few tools do not require high-precision
programs, they can only fix a limited subset of errors or produce suboptimal
results.
  To address these challenges, we propose a novel method, named OFP-Repair.On
ACESO's dataset, our patches achieve improvements of three, seven, three, and
eight orders of magnitude across four accuracy metrics. In real-world cases,
our method successfully detects all five original-precision-repairable errors
and fixes three, whereas ACESO only repairs one. Notably, these results are
based on verified data and do not fully capture the potential of OFP-Repair. To
further validate our method, we deploy it on a decade-old open bug report from
GNU Scientific Library (GSL), successfully repairing five out of 15 bugs. The
developers have expressed interest in our method and are considering
integrating our tool into their development workflow. We are currently working
on applying our patches to GSL. The results are highly encouraging,
demonstrating the practical applicability of our technique.

</details>


### [29] [Operationalizing AI: Empirical Evidence on MLOps Practices, User Satisfaction, and Organizational Context](https://arxiv.org/abs/2510.09968)
*Stefan Pasch*

Main category: cs.SE

TL;DR: 本研究通过分析8000多条AI开发平台用户评论，发现大多数MLOps实践与用户满意度显著正相关，且其益处普遍适用于不同规模的组织。


<details>
  <summary>Details</summary>
Motivation: 尽管MLOps被提出作为应对AI开发与运维挑战的最佳实践，但缺乏实证证据证明这些实践是否以及如何支持用户开发和部署AI应用。

Method: 研究从G2.com收集了8000多条用户评论，采用零样本分类方法评估用户对九项MLOps实践的情感倾向，并分析组织规模对实践使用频率及满意度影响。

Result: 九项MLOps实践中，有七项与用户满意度显著正相关；小型企业用户提及某些实践的频率较低，但组织规模并不调节MLOps与满意度之间的关系。

Conclusion: MLOps实践一旦实施，无论组织规模如何，均被用户视为具有普遍价值，能有效提升AI开发与运营体验。

Abstract: Organizational efforts to utilize and operationalize artificial intelligence
(AI) are often accompanied by substantial challenges, including scalability,
maintenance, and coordination across teams. In response, the concept of Machine
Learning Operations (MLOps) has emerged as a set of best practices that
integrate software engineering principles with the unique demands of managing
the ML lifecycle. Yet, empirical evidence on whether and how these practices
support users in developing and operationalizing AI applications remains
limited. To address this gap, this study analyzes over 8,000 user reviews of AI
development platforms from G2.com. Using zero-shot classification, we measure
review sentiment toward nine established MLOps practices, including continuous
integration and delivery (CI/CD), workflow orchestration, reproducibility,
versioning, collaboration, and monitoring. Seven of the nine practices show a
significant positive relationship with user satisfaction, suggesting that
effective MLOps implementation contributes tangible value to AI development.
However, organizational context also matters: reviewers from small firms
discuss certain MLOps practices less frequently, suggesting that organizational
context influences the prevalence and salience of MLOps, though firm size does
not moderate the MLOps-satisfaction link. This indicates that once applied,
MLOps practices are perceived as universally beneficial across organizational
settings.

</details>


### [30] [SLEAN: Simple Lightweight Ensemble Analysis Network for Multi-Provider LLM Coordination: Design, Implementation, and Vibe Coding Bug Investigation Case Study](https://arxiv.org/abs/2510.10010)
*Matheus J. T. Vargas*

Main category: cs.SE

TL;DR: SLEAN 是一个轻量级、基于文本提示模板的确定性框架，通过三阶段协议（独立分析、交叉批评、仲裁）协调多个大语言模型（LLM）生成的代码修复建议，有效过滤有害修改，提升修复质量和部署安全性。


<details>
  <summary>Details</summary>
Motivation: 当前 AI 辅助调试常生成引入不必要复杂性、破坏现有功能或偏离问题本质的代码修改，亟需一种简单、可审计且无需复杂基础设施的方法来协调多个 LLM 提供商，以过滤有害建议并保留高质量修复。

Method: SLEAN 采用基于 .txt 模板的提示编排机制，通过三阶段流程：各 LLM 独立分析、相互交叉批评、最终仲裁，筛选出最小因果改动的修复方案；该框架无需深度技术知识，支持多提供商且文件驱动。

Result: 在 15 个软件缺陷上评估 69 个 AI 修复建议，SLEAN 接受 22 个（31.9%），拒绝 47 个有害建议；仲裁使代码变更范围减少 83–90%；使用简洁输入（Type 2）比详细输入（Type 1）效率高约 20%；AI 间高一致性与修复质量弱相关。

Conclusion: SLEAN 提供了一种轻量、可审计、无需专业编码知识的多 LLM 协同框架，适用于代码审查、安全审计等需可靠多源合成的场景，有效提升 AI 生成内容的安全性与实用性。

Abstract: We present SLEAN (Simple Lightweight Ensemble Analysis Network), a
deterministic framework for coordinating multiple LLM providers through
text-based prompt orchestration. Unlike complex multi-agent systems requiring
specialized infrastructure, SLEAN operates as a simple prompt bridge between
LLMs using .txt templates, requiring no deep technical knowledge for
deployment. The three-phase protocol formed by independent analysis,
cross-critique, and arbitration, filters harmful AI-generated code suggestions
before production deployment, addressing how AI-assisted debugging increasingly
produces modifications that introduce unnecessary complexity, break existing
functionality, or address problems. Evaluating 15 software bugs, we analyzed 69
AI-generated fix propositions. SLEAN's filtering accepted 22 fixes (31.9%, 95%
CI 20.9-42.9%) while rejecting 47 that would have been harmful if applied
verbatim. The arbitration process reduced code change surface by 83-90%
relative to raw AI outputs, enforcing minimal causal edits over scope-expanding
modifications. Minimal Type 2 inputs proved more efficient than detailed Type 1
inputs, requiring 2.85 versus 3.56 propositions per accepted fix (35.1% versus
28.1% acceptance, about a 20% efficiency gain). Agreement between AI systems
showed weak correlation with fix quality: high convergence (at least 80%)
occurred in 4 of 15 cases and improved acceptance by only 2.4% points;
arbitration appeared only at exactly 10% convergence in 2 of 15 cases, although
low convergence alone did not necessitate arbitration. The file-driven,
provider-agnostic architecture enables deployment without specialized coding
expertise, making it applicable to security auditing, code review, document
verification, and other domains requiring reliable multi-provider synthesis
with end-to-end auditability.

</details>


### [31] [OBsmith: Testing JavaScript Obfuscator using LLM-powered sketching](https://arxiv.org/abs/2510.10066)
*Shan Jiang,Chenguang Zhu,Sarfraz Khurshid*

Main category: cs.SE

TL;DR: 本文提出了OBsmith，一个利用大语言模型（LLMs）和真实程序提取技术，系统性测试JavaScript混淆器语义正确性的新框架，并发现了11个此前未知的正确性缺陷。


<details>
  <summary>Details</summary>
Motivation: 现有JavaScript混淆器评估主要关注抗反混淆能力，忽视了混淆是否保持程序语义正确性；错误的混淆可能破坏功能、可靠性和安全性，因此亟需系统性测试方法。

Method: OBsmith结合LLM生成涵盖多样语言特性和边界情况的程序模板（sketches），并从真实项目中自动提取模板，实例化后在不同混淆配置下执行，通过语义一致性检查发现错误。

Result: OBsmith发现了11个新正确性漏洞，而五种主流JavaScript模糊测试工具在相同测试预算下均未能发现这些问题；消融实验表明各组件对不同漏洞类别均有贡献。

Conclusion: OBsmith为混淆器及其他语义保持型工具链提供了有效的自动化测试与质量保障手段，强调需开发针对混淆器的特定蜕变关系，并引发对混淆预设与性能代价平衡的讨论。

Abstract: JavaScript obfuscators are widely deployed to protect intellectual property
and resist reverse engineering, yet their correctness has been largely
overlooked compared to performance and resilience. Existing evaluations
typically measure resistance to deobfuscation, leaving the critical question of
whether obfuscators preserve program semantics unanswered. Incorrect
transformations can silently alter functionality, compromise reliability, and
erode security-undermining the very purpose of obfuscation. To address this
gap, we present OBsmith, a novel framework to systematically test JavaScript
obfuscators using large language models (LLMs). OBsmith leverages LLMs to
generate program sketches abstract templates capturing diverse language
constructs, idioms, and corner cases-which are instantiated into executable
programs and subjected to obfuscation under different configurations. Besides
LLM-powered sketching, OBsmith also employs a second source: automatic
extraction of sketches from real programs. This extraction path enables more
focused testing of project specific features and lets developers inject domain
knowledge into the resulting test cases. OBsmith uncovers 11 previously unknown
correctness bugs. Under an equal program budget, five general purpose
state-of-the-art JavaScript fuzzers (FuzzJIT, Jsfunfuzz, Superion, DIE,
Fuzzilli) failed to detect these issues, highlighting OBsmith's complementary
focus on obfuscation induced misbehavior. An ablation shows that all components
except our generic MRs contribute to at least one bug class; the negative MR
result suggests the need for obfuscator-specific metamorphic relations. Our
results also seed discussion on how to balance obfuscation presets and
performance cost. We envision OBsmith as an important step towards automated
testing and quality assurance of obfuscators and other semantic-preserving
toolchains.

</details>


### [32] [A Mathematics-Guided Approach to Floating-Point Error Detection](https://arxiv.org/abs/2510.10081)
*Youshuai Tan,Zhanwei Zhang,Zishuo Ding,Lianyu Zheng,Jinfu Chen,Weiyi Shang*

Main category: cs.SE

TL;DR: 本文提出了一种名为MGDE的新方法，利用牛顿-拉弗森法的数学引导机制，高效检测浮点程序中的错误输入。相比现有最先进方法FPCC，MGDE在单输入程序中发现更多错误（89 vs. 48），耗时更少（仅为FPCC的约1/6.4），并在多输入程序中成功检测出FPCC无法发现的错误。


<details>
  <summary>Details</summary>
Motivation: 现有浮点错误检测方法存在计算成本高和长距离收敛能力弱两大局限，难以高效识别引发严重浮点误差的输入。

Method: 提出MGDE方法，基于牛顿-拉弗森法的二次收敛特性，通过数学引导高效搜索错误诱导输入。

Result: 在88个单输入程序中，MGDE检测到89个错误（覆盖44个程序），而FPCC仅检测到48个（覆盖29个程序）；MGDE运行时间仅为FPCC的约15.6%。在多输入程序中，MGDE平均0.6443秒检测出9个错误，而FPCC在平均100秒内未能发现任何错误。

Conclusion: MGDE显著优于现有方法，在检测效果和效率上均取得突破，验证了数学引导策略在浮点错误检测中的有效性。

Abstract: Floating-point program errors can lead to severe consequences, particularly
in critical domains such as military applications. Only a small subset of
inputs may induce substantial floating-point errors, prompting researchers to
develop methods for identifying these error-inducing inputs. Although existing
approaches have achieved some success, they still suffer from two major
limitations: (1) High computational cost: The evaluation of error magnitude for
candidate inputs relies on high-precision programs, which are prohibitively
time-consuming. (2) Limited long-range convergence capability: Current methods
exhibit inefficiency in search, making the process akin to finding a needle in
a haystack.
  To address these two limitations, we propose a novel method, named MGDE, to
detect error-inducing inputs based on mathematical guidance. By employing the
Newton-Raphson method, which exhibits quadratic convergence properties, we
achieve highly effective and efficient results. Since the goal of identifying
error-inducing inputs is to uncover the underlying bugs, we use the number of
bugs detected in floating-point programs as the primary evaluation metric in
our experiments. As FPCC represents the most effective state-of-the-art
approach to date, we use it as the baseline for comparison. The dataset of FPCC
consists of 88 single-input floating-point programs. FPCC is able to detect 48
bugs across 29 programs, whereas our method successfully identifies 89 bugs
across 44 programs. Moreover, FPCC takes 6.4096 times as long as our proposed
method. We also deploy our method to multi-input programs, identifying a total
of nine bugs with an average detection time of 0.6443 seconds per program. In
contrast, FPCC fails to detect any bugs while requiring an average computation
time of 100 seconds per program.

</details>


### [33] [IntrinTrans: LLM-based Intrinsic Code Translator for RISC-V Vector](https://arxiv.org/abs/2510.10119)
*Liutong Han,Zhiyuan Tan,Hongbin Zhang,Pengcheng Wang,Chu Kang,Mingjie Xing,Yanjun Wu*

Main category: cs.SE

TL;DR: 本文提出IntrinTrans，一种基于大语言模型（LLM）的多智能体方法，用于自动将Arm Neon等架构的向量化内建函数代码翻译为RISC-V Vector（RVV）内建函数，并结合编译测试反馈与活跃性分析优化寄存器使用，显著提升翻译正确性与性能。


<details>
  <summary>Details</summary>
Motivation: 随着RISC-V生态的快速发展，亟需将现有基于Arm或x86 SIMD内建函数的高性能库代码迁移到RVV架构。当前跨架构翻译依赖人工重写，效率低且易错；而基于规则的方法受限于规则覆盖不全和语法约束，难以充分利用RVV特性，导致性能不佳。

Method: 提出IntrinTrans框架，利用多智能体LLM系统自动翻译内建函数代码，并通过编译-测试反馈循环进行修正；进一步结合活跃性分析提取寄存器使用信息，对生成的RVV代码进行性能优化。

Result: 在34个来自开源库的向量化算法案例上评估，IntrinTrans在有限迭代内生成语义正确的RVV代码，部分案例性能达到社区原生实现的5.93倍。

Conclusion: 基于LLM的多智能体方法结合编译反馈与程序分析，能有效实现跨架构内建函数自动翻译与优化，为RISC-V生态的高性能库移植提供高效可行的解决方案。

Abstract: The use of intrinsic functions to exploit hardware-specific capabilities is
an important approach for optimizing library performance. Many mainstream
libraries implement a large number of vectorized algorithms on Arm or x86 SIMD
intrinsic functions. With the rapid expansion of the RISC-V hardware-software
ecosystem, there is a growing demand for support of the RISC-V Vector (RVV)
extension. Translating existing vectorized intrinsic code onto RVV intrinsics
is a practical and effective approach. However, current cross-architecture
translation largely relies on manual rewriting, which is time-consuming and
error-prone. Furthermore, while some rule-based methods can reduce the need for
manual intervention, their translation success rate is limited by incomplete
rule coverage and syntactic constraints, and the performance suffers from
inadequate utilization of RVV-specific features. We present IntrinTrans, a
LLM-based multi-agent approach that utilizes compile-and-test feedback to
translate intrinsic code across architectures automatically, and further
optimizes the generated RVV intrinsics using register-usage information derived
from liveness analysis. To evaluate the effectiveness of our approach, we
collected 34 vectorized algorithm cases from open-source libraries. Each case
includes an Arm Neon intrinsics implementation and a RVV intrinsics
implementation contributed by the open-source community, together with
correctness and performance tests. Our experiments show that advanced LLMs
produce semantically correct RISC-V Vector intrinsics in most cases within a
limited number of iterations, and in some cases achieve up to 5.93x the
performance of the native implementation from the open-source community.

</details>


### [34] [A Systematic Study on Generating Web Vulnerability Proof-of-Concepts Using Large Language Models](https://arxiv.org/abs/2510.10148)
*Mengyao Zhao,Kaixuan Li,Lyuye Zhang,Wenjing Dang,Chenggong Ding,Sen Chen,Zheli Liu*

Main category: cs.SE

TL;DR: 本文首次实证研究了大语言模型（LLM）在利用公开披露信息自动生成Web应用漏洞的PoC（概念验证）方面的可行性，发现LLM在仅使用公开数据时可实现8%-34%的成功率，结合代码上下文和自适应推理策略后成功率可提升至68%-72%。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型在代码理解和推理方面的进步，研究其是否能有效利用公开披露的漏洞信息（如CVE）自动生成有效的PoC，对漏洞复现、理解和缓解具有重要意义。

Method: 作者评估了GPT-4o和DeepSeek-R1在100个真实可复现的CVE上的表现，涵盖漏洞披露的三个阶段：仅有描述的新披露漏洞、有补丁的1-day漏洞、以及具有完整上下文代码的N-day漏洞；并进一步测试了补充代码上下文和自适应推理策略对生成效果的影响。

Result: LLM在仅使用公开数据时可生成有效PoC的比例为8%-34%，其中DeepSeek-R1表现优于GPT-4o；补充代码上下文可提升成功率17%-20%，函数级上下文比文件级提升9%-13%；结合自适应推理策略后成功率可达68%-72%；已有23个新生成的PoC被NVD和Exploit DB收录。

Conclusion: 大语言模型在利用公开信息自动生成漏洞PoC方面展现出显著潜力，可能重塑漏洞利用的动态格局，并对软件安全带来新的机遇与挑战。

Abstract: Recent advances in Large Language Models (LLMs) have brought remarkable
progress in code understanding and reasoning, creating new opportunities and
raising new concerns for software security. Among many downstream tasks,
generating Proof-of-Concept (PoC) exploits plays a central role in
vulnerability reproduction, comprehension, and mitigation. While previous
research has focused primarily on zero-day exploitation, the growing
availability of rich public information accompanying disclosed CVEs leads to a
natural question: can LLMs effectively use this information to automatically
generate valid PoCs? In this paper, we present the first empirical study of
LLM-based PoC generation for web application vulnerabilities, focusing on the
practical feasibility of leveraging publicly disclosed information. We evaluate
GPT-4o and DeepSeek-R1 on 100 real-world and reproducible CVEs across three
stages of vulnerability disclosure: (1) newly disclosed vulnerabilities with
only descriptions, (2) 1-day vulnerabilities with patches, and (3) N-day
vulnerabilities with full contextual code. Our results show that LLMs can
automatically generate working PoCs in 8%-34% of cases using only public data,
with DeepSeek-R1 consistently outperforming GPT-4o. Further analysis shows that
supplementing code context improves success rates by 17%-20%, with
function-level providing 9%-13% improvement than file-level ones. Further
integrating adaptive reasoning strategies to prompt refinement significantly
improves success rates to 68%-72%. Our findings suggest that LLMs could reshape
vulnerability exploitation dynamics. To date, 23 newly generated PoCs have been
accepted by NVD and Exploit DB.

</details>


### [35] [LLMs are All You Need? Improving Fuzz Testing for MOJO with Large Language Models](https://arxiv.org/abs/2510.10179)
*Linghan Huang,Peizhou Zhao,Huaming Chen*

Main category: cs.SE

TL;DR: 本文提出了MOJOFuzzer，首个面向新兴编程语言（如MOJO）的自适应LLM驱动模糊测试框架，通过多阶段过滤和动态提示调整显著提升测试有效性、API覆盖率和漏洞发现能力。


<details>
  <summary>Details</summary>
Motivation: 新兴编程语言MOJO缺乏完善的测试框架和LLM训练语料，导致LLM在生成测试用例时容易产生语义错误（幻觉），降低模糊测试效果。

Method: MOJOFuzzer采用多阶段框架，在执行前系统性过滤低质量输入，并基于运行时反馈动态调整LLM提示，实现测试用例的迭代优化。

Result: 实验表明MOJOFuzzer在测试有效性、API覆盖率和漏洞检测方面优于传统和现有LLM模糊测试方法，并在MOJO中首次大规模测试中发现13个未知漏洞。

Conclusion: 该研究推进了LLM驱动的软件测试，并为新兴编程语言的测试提供了基础方法论。

Abstract: The rapid development of large language models (LLMs) has revolutionized
software testing, particularly fuzz testing, by automating the generation of
diverse and effective test inputs. This advancement holds great promise for
improving software reliability. Meanwhile, the introduction of MOJO, a
high-performance AI programming language blending Python's usability with the
efficiency of C and C++, presents new opportunities to enhance AI model
scalability and programmability. However, as a new language, MOJO lacks
comprehensive testing frameworks and a sufficient corpus for LLM-based testing,
which exacerbates model hallucination. In this case, LLMs will generate
syntactically valid but semantically incorrect code, significantly reducing the
effectiveness of fuzz testing. To address this challenge, we propose
MOJOFuzzer, the first adaptive LLM-based fuzzing framework designed for
zero-shot learning environments of emerging programming languages. MOJOFuzzer
integrates a mutil-phase framework that systematically eliminates low-quality
generated inputs before execution, significantly improving test case validity.
Furthermore, MOJOFuzzer dynamically adapts LLM prompts based on runtime
feedback for test case mutation, enabling an iterative learning process that
continuously enhances fuzzing efficiency and bug detection performance. Our
experimental results demonstrate that MOJOFuzzer significantly enhances test
validity, API coverage, and bug detection performance, outperforming
traditional fuzz testing and state-of-the-art LLM-based fuzzing approaches.
Using MOJOFuzzer, we have conducted a first large-scale fuzz testing evaluation
of MOJO, uncorvering 13 previous unknown bugs. This study not only advances the
field of LLM-driven software testing but also establishes a foundational
methodology for leveraging LLMs in the testing of emerging programming
languages.

</details>


### [36] [Grounded AI for Code Review: Resource-Efficient Large-Model Serving in Enterprise Pipelines](https://arxiv.org/abs/2510.10290)
*Sayan Mandal,Hua Jiang*

Main category: cs.SE

TL;DR: 本文提出了一种适用于合规要求严格环境的自动化代码审查系统，结合静态分析、AST引导的上下文提取和轻量级单GPU按需服务架构，在保证低延迟和低成本的同时提供可靠、可解释的审查反馈。


<details>
  <summary>Details</summary>
Motivation: 在合规要求高的环境中，现有静态分析工具输出量大但缺乏解释，而直接使用大语言模型（LLM）存在幻觉风险和高成本问题，阻碍了自动化代码审查的落地。

Method: 构建一个生产级系统，将静态分析结果与AST引导的上下文提取相结合，并采用量化开源模型与多级缓存的单GPU按需服务架构，实现精准、高效的代码审查反馈。

Result: 在面向安全的C/C++标准上评估，系统中位首次反馈时间低于1分钟（59.8秒），违规减少效果与大型闭源模型相当甚至更优，内部小规模调研（n=8）显示人工审查轮次减少、分类负担降低。

Conclusion: 该系统架构解耦，支持模块化采用，强调可复现性、可审计性，并为支持更广泛标准和辅助修复提供可行路径。

Abstract: Automated code review adoption lags in compliance-heavy settings, where
static analyzers produce high-volume, low-rationale outputs, and naive LLM use
risks hallucination and incurring cost overhead. We present a production system
for grounded, PR-native review that pairs static-analysis findings with
AST-guided context extraction and a single-GPU, on-demand serving stack
(quantized open-weight model, multi-tier caching) to deliver concise
explanations and remediation guidance. Evaluated on safety-oriented C/C++
standards, the approach achieves sub-minute median first-feedback (offline p50
build+LLM 59.8s) while maintaining competitive violation reduction and lower
violation rates versus larger proprietary models. The architecture is
decoupled: teams can adopt the grounding/prompting layer or the serving layer
independently. A small internal survey (n=8) provides directional signals of
reduced triage effort and moderate perceived grounding, with participants
reporting fewer human review iterations. We outline operational lessons and
limitations, emphasizing reproducibility, auditability, and pathways to broader
standards and assisted patching.

</details>


### [37] [Prepared for the Unknown: Adapting AIOps Capacity Forecasting Models to Data Changes](https://arxiv.org/abs/2510.10320)
*Lorena Poenaru-Olaru,Wouter van 't Hof,Adrian Stando,Arkadiusz P. Trawinski,Eileen Kapel,Jan S. Rellermeyer,Luis Cruz,Arie van Deursen*

Main category: cs.SE

TL;DR: 本文研究了基于数据变化触发的重训练策略（漂移驱动重训练）与定期重训练在容量预测模型中的效果对比，发现前者在多数情况下能以更低的计算成本实现相当的预测精度，但在数据快速变化时仍需依赖定期重训练以保证准确性。


<details>
  <summary>Details</summary>
Motivation: 软件组织在容量管理中需高效分配资源，而依赖机器学习的预测模型需频繁重训练以应对数据演化。然而，持续重训练成本高、难扩展，因此需要探索更高效的重训练策略。

Method: 对比基于数据漂移检测触发的重训练与定期重训练两种策略，在时间序列容量预测任务中评估其预测准确性和计算效率。

Result: 漂移驱动的重训练在大多数情况下能达到与定期重训练相当的预测精度，显著降低计算开销；但在数据快速变化的场景下，定期重训练仍更优。

Conclusion: 对于软件团队而言，采用基于数据变化的重训练策略是一种兼顾效率与准确性的可行方案，可在多数场景中减少重训练负担，同时保持预测性能。

Abstract: Capacity management is critical for software organizations to allocate
resources effectively and meet operational demands. An important step in
capacity management is predicting future resource needs often relies on
data-driven analytics and machine learning (ML) forecasting models, which
require frequent retraining to stay relevant as data evolves. Continuously
retraining the forecasting models can be expensive and difficult to scale,
posing a challenge for engineering teams tasked with balancing accuracy and
efficiency. Retraining only when the data changes appears to be a more
computationally efficient alternative, but its impact on accuracy requires
further investigation. In this work, we investigate the effects of retraining
capacity forecasting models for time series based on detected changes in the
data compared to periodic retraining. Our results show that drift-based
retraining achieves comparable forecasting accuracy to periodic retraining in
most cases, making it a cost-effective strategy. However, in cases where data
is changing rapidly, periodic retraining is still preferred to maximize the
forecasting accuracy. These findings offer actionable insights for software
teams to enhance forecasting systems, reducing retraining overhead while
maintaining robust performance.

</details>


### [38] [Testing and Enhancing Multi-Agent Systems for Robust Code Generation](https://arxiv.org/abs/2510.10460)
*Zongyi Lyu,Songqiang Chen,Zhenlan Ji,Liwen Wang,Shuai Wang,Daoyuan Wu,Wenxuan Wang,Shing-Chi Cheung*

Main category: cs.SE

TL;DR: 本文首次系统研究了多智能体系统（MAS）在代码生成任务中的鲁棒性问题，发现主流MAS在语义保持的模糊测试下表现显著下降，并揭示其根本原因在于规划与编码智能体之间的沟通不足；作者提出一种包含多提示生成和监控智能体的修复方法，显著提升了MAS的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 尽管多智能体系统在代码生成任务中表现出色，但其鲁棒性尚未得到充分研究，这对其在现实场景中的部署构成潜在风险，因此亟需系统性评估与改进。

Method: 设计了一种基于模糊测试的评估流程，包含语义保持的变异算子和新颖的适应度函数，对多个主流MAS在不同数据集和大语言模型上进行测试；并提出一种修复方法，结合多提示生成与新增的监控智能体以改善智能体间通信。

Result: 实验发现，现有MAS在模糊测试后对原本已解决的问题失败率达7.9%–83.3%；所提修复方法能有效解决40.0%–88.9%的失败案例，显著提升系统鲁棒性。

Conclusion: 多智能体代码生成系统存在严重的鲁棒性缺陷，主要源于规划与编码智能体之间的信息传递不充分；通过引入监控机制和多提示策略可有效缓解该问题，为构建更可靠的MAS提供了关键见解。

Abstract: Multi-agent systems (MASs) have emerged as a promising paradigm for automated
code generation, demonstrating impressive performance on established benchmarks
by decomposing complex coding tasks across specialized agents with different
roles. Despite their prosperous development and adoption, their robustness
remains pressingly under-explored, raising critical concerns for real-world
deployment. This paper presents the first comprehensive study examining the
robustness of MASs for code generation through a fuzzing-based testing
approach. By designing a fuzzing pipeline incorporating semantic-preserving
mutation operators and a novel fitness function, we assess mainstream MASs
across multiple datasets and LLMs. Our findings reveal substantial robustness
flaws of various popular MASs: they fail to solve 7.9%-83.3% of problems they
initially resolved successfully after applying the semantic-preserving
mutations. Through comprehensive failure analysis, we identify a common yet
largely overlooked cause of the robustness issue: miscommunications between
planning and coding agents, where plans lack sufficient detail and coding
agents misinterpret intricate logic, aligning with the challenges inherent in a
multi-stage information transformation process. Accordingly, we also propose a
repairing method that encompasses multi-prompt generation and introduces a new
monitor agent to address this issue. Evaluation shows that our repairing method
effectively enhances the robustness of MASs by solving 40.0%-88.9% of
identified failures. Our work uncovers critical robustness flaws in MASs and
provides effective mitigation strategies, contributing essential insights for
developing more reliable MASs for code generation.

</details>


### [39] [How Students Use Generative AI for Software Testing: An Observational Study](https://arxiv.org/abs/2510.10551)
*Baris Ardic,Quentin Le Dilavrec,Andy Zaidman*

Main category: cs.SE

TL;DR: 本研究探讨了初级软件开发者在使用生成式AI（如ChatGPT）进行单元测试时的交互策略、依赖程度及感知的优缺点，发现虽然AI能节省时间并降低认知负担，但也带来信任度下降和代码所有权缺失等问题，且不同策略对测试效果和代码质量影响不大。


<details>
  <summary>Details</summary>
Motivation: 随着生成式AI工具在软件工程中的应用日益广泛，其对开发者角色、控制感、输出质量及学习过程的影响引发关注，尤其对初学者而言。因此，有必要研究新手开发者如何与生成式AI互动以进行单元测试。

Method: 研究通过观察性实验，招募12名具备基础软件测试知识的本科生，让他们使用生成式AI完成单元测试任务，分析其交互策略、提示风格以及对AI辅助测试的主观体验。

Result: 研究识别出四种交互策略（基于测试想法与实现来源），以及一次性提示与迭代提示两种风格；参与者普遍认为AI有助于节省时间、减轻认知负担和激发测试思路，但也担忧测试质量、信任度和缺乏代码所有权；不同策略对测试有效性（突变分数）和代码质量（测试异味）无显著影响。

Conclusion: 生成式AI在单元测试中为新手开发者带来效率提升，但也引发对质量控制和学习效果的担忧；交互策略虽影响工作流程，但未显著改变测试产出质量，提示需在教育和实践中平衡AI辅助与开发者自主性。

Abstract: The integration of generative AI tools like ChatGPT into software engineering
workflows opens up new opportunities to boost productivity in tasks such as
unit test engineering. However, these AI-assisted workflows can also
significantly alter the developer's role, raising concerns about control,
output quality, and learning, particularly for novice developers. This study
investigates how novice software developers with foundational knowledge in
software testing interact with generative AI for engineering unit tests. Our
goal is to examine the strategies they use, how heavily they rely on generative
AI, and the benefits and challenges they perceive when using generative
AI-assisted approaches for test engineering. We conducted an observational
study involving 12 undergraduate students who worked with generative AI for
unit testing tasks. We identified four interaction strategies, defined by
whether the test idea or the test implementation originated from generative AI
or the participant. Additionally, we singled out prompting styles that focused
on one-shot or iterative test generation, which often aligned with the broader
interaction strategy. Students reported benefits including time-saving, reduced
cognitive load, and support for test ideation, but also noted drawbacks such as
diminished trust, test quality concerns, and lack of ownership. While strategy
and prompting styles influenced workflow dynamics, they did not significantly
affect test effectiveness or test code quality as measured by mutation score or
test smells.

</details>


### [40] [Generative AI and the Transformation of Software Development Practices](https://arxiv.org/abs/2510.10819)
*Vivek Acharya*

Main category: cs.SE

TL;DR: 本文探讨生成式AI（特别是大语言模型）如何改变软件工程实践，涵盖聊天式编程、多智能体系统等新范式，分析其带来的效率提升与可及性扩展，同时讨论模型可靠性、成本、信任与技能转型等挑战，并提出负责任使用AI的新角色与最佳实践。


<details>
  <summary>Details</summary>
Motivation: 随着生成式AI和大语言模型的快速发展，软件开发方式正在经历深刻变革。作者旨在系统分析AI辅助编程对软件工程实践的影响，厘清其中的机遇与风险，为开发者和组织提供指导。

Method: 通过综述迭代式聊天开发、多智能体系统、动态提示编排及模型上下文协议（MCP）等技术，并结合案例研究与行业数据进行分析。

Result: 揭示了生成式AI在加速开发周期和降低编程门槛方面的显著优势，同时也识别出模型可靠性、使用成本、信任机制和技能需求转变等关键挑战。

Conclusion: 生成式AI正深刻重塑软件工程，需通过建立新角色、培养新技能和采纳最佳实践，在提升效率的同时确保其负责任和有效应用。

Abstract: Generative AI is reshaping how software is designed, written, and maintained.
Advances in large language models (LLMs) are enabling new development styles -
from chat-oriented programming and 'vibe coding' to agentic programming - that
can accelerate productivity and broaden access. This paper examines how
AI-assisted techniques are changing software engineering practice, and the
related issues of trust, accountability, and shifting skills. We survey
iterative chat-based development, multi-agent systems, dynamic prompt
orchestration, and integration via the Model Context Protocol (MCP). Using case
studies and industry data, we outline both the opportunities (faster cycles,
democratized coding) and the challenges (model reliability and cost) of
applying generative AI to coding. We describe new roles, skills, and best
practices for using AI in a responsible and effective way.

</details>


### [41] [Agentic RAG for Software Testing with Hybrid Vector-Graph and Multi-Agent Orchestration](https://arxiv.org/abs/2510.10824)
*Mohanakrishnan Hariharan,Satish Arvapalli,Seshu Barma,Evangeline Sheela*

Main category: cs.SE

TL;DR: 本文提出了一种基于智能体增强检索生成（Agentic RAG）的软件测试自动化方法，通过结合自主AI智能体与向量-图混合知识系统，显著提升了测试工件生成的准确性与效率。


<details>
  <summary>Details</summary>
Motivation: 传统软件测试存在效率低、成本高和上下文理解不足等问题，作者旨在利用大语言模型与多智能体系统提升质量工程中测试计划、用例和指标生成的自动化水平。

Method: 采用Agentic RAG架构，整合Gemini和Mistral等大语言模型、多智能体协同机制以及向量-图混合知识库，实现测试工件的自动生成与全生命周期追溯。

Result: 系统将测试准确性从65%提升至94.8%，在企业级系统工程和SAP迁移项目中实现测试周期缩短85%、测试套件效率提升85%、预计节省35%成本，并使上线时间提前2个月。

Conclusion: 该方法有效克服了传统测试的局限性，显著提升了软件测试的自动化水平、准确性与经济效益，具有良好的工程应用前景。

Abstract: We present an approach to software testing automation using Agentic
Retrieval-Augmented Generation (RAG) systems for Quality Engineering (QE)
artifact creation. We combine autonomous AI agents with hybrid vector-graph
knowledge systems to automate test plan, case, and QE metric generation. Our
approach addresses traditional software testing limitations by leveraging LLMs
such as Gemini and Mistral, multi-agent orchestration, and enhanced
contextualization. The system achieves remarkable accuracy improvements from
65% to 94.8% while ensuring comprehensive document traceability throughout the
quality engineering lifecycle. Experimental validation of enterprise Corporate
Systems Engineering and SAP migration projects demonstrates an 85% reduction in
testing timeline, an 85% improvement in test suite efficiency, and projected
35% cost savings, resulting in a 2-month acceleration of go-live.

</details>


### [42] [Software Defect Prediction using Autoencoder Transformer Model](https://arxiv.org/abs/2510.10840)
*Seshu Barma,Mohanakrishnan Hariharan,Satish Arvapalli*

Main category: cs.SE

TL;DR: 本文提出了一种名为ADE-QVAET的新模型，结合自适应差分进化（ADE）与量子变分自编码器-Transformer（QVAET），以提升软件缺陷预测的准确性。


<details>
  <summary>Details</summary>
Motivation: 现有机器学习模型在处理噪声数据、类别不平衡、模式识别、特征提取和泛化能力方面存在不足，亟需更有效的AI-ML方法来提升软件质量工程中的缺陷预测能力。

Method: 提出ADE-QVAET模型，将自适应差分进化算法与量子变分自编码器-Transformer结合，用于提取高维潜在特征、保持序列依赖性，并通过超参数调优优化模型收敛性与预测性能。

Result: 在90%训练数据比例下，ADE-QVAET在准确率、精确率、召回率和F1分数上分别达到98.08%、92.45%、94.67%和98.12%，优于传统差分进化（DE）模型。

Conclusion: ADE-QVAET模型显著提升了软件缺陷预测的性能，展示了AI-ML驱动的质量工程方法在实际应用中的潜力。

Abstract: An AI-ML-powered quality engineering approach uses AI-ML to enhance software
quality assessments by predicting defects. Existing ML models struggle with
noisy data types, imbalances, pattern recognition, feature extraction, and
generalization. To address these challenges, we develop a new model, Adaptive
Differential Evolution (ADE) based Quantum Variational Autoencoder-Transformer
(QVAET) Model (ADE-QVAET). ADE combines with QVAET to obtain high-dimensional
latent features and maintain sequential dependencies, resulting in enhanced
defect prediction accuracy. ADE optimization enhances model convergence and
predictive performance. ADE-QVAET integrates AI-ML techniques such as tuning
hyperparameters for scalable and accurate software defect prediction,
representing an AI-ML-driven technology for quality engineering. During
training with a 90% training percentage, ADE-QVAET achieves high accuracy,
precision, recall, and F1-score of 98.08%, 92.45%, 94.67%, and 98.12%,
respectively, when compared to the Differential Evolution (DE) ML model.

</details>


### [43] [Generative AI for Software Project Management: Insights from a Review of Software Practitioner Literature](https://arxiv.org/abs/2510.10887)
*Lakshana Iruni Assalaarachchi,Zainab Masood,Rashina Hoda,John Grundy*

Main category: cs.SE

TL;DR: 本文通过灰文献综述分析了47份从业者资料，探讨生成式人工智能（GenAI）在软件项目管理中的角色、影响与挑战，并提出面向项目管理人才三角的技能提升建议。


<details>
  <summary>Details</summary>
Motivation: 了解软件从业者如何看待和应用生成式人工智能（GenAI）在软件项目管理中的转型，识别其价值、挑战及对从业者能力的新要求。

Method: 采用灰文献综述方法，分析47份公开的从业者资料（如博客、文章和行业报告），归纳GenAI在软件项目管理中的应用现状与观点。

Result: 从业者普遍将GenAI视为“助手”、“副驾驶”或“朋友”，而非项目经理的替代者；GenAI在自动化常规任务、预测分析、沟通协作及敏捷实践中助力项目成功，但也存在幻觉、伦理隐私、缺乏情感智能等问题；研究提出了与PMI人才三角对应的技能提升方向。

Conclusion: GenAI正在重塑软件项目管理实践，需在发挥其辅助价值的同时关注伦理与能力适配，项目管理者应主动提升相关技能，研究者与从业者应协同推进负责任的GenAI应用。

Abstract: Software practitioners are discussing GenAI transformations in software
project management openly and widely. To understand the state of affairs, we
performed a grey literature review using 47 publicly available practitioner
sources including blogs, articles, and industry reports. We found that software
project managers primarily perceive GenAI as an "assistant", "copilot", or
"friend" rather than as a "PM replacement", with support of GenAI in automating
routine tasks, predictive analytics, communication and collaboration, and in
agile practices leading to project success. Practitioners emphasize responsible
GenAI usage given concerns such as hallucinations, ethics and privacy, and lack
of emotional intelligence and human judgment. We present upskilling
requirements for software project managers in the GenAI era mapped to the
Project Management Institute's talent triangle. We share key recommendations
for both practitioners and researchers.

</details>


### [44] [Project-Level C-to-Rust Translation via Synergistic Integration of Knowledge Graphs and Large Language Models](https://arxiv.org/abs/2510.10956)
*Zhiqiang Yuan,Wenjun Mao,Zhuo Chen,Xiyue Shang,Chong Wang,Yiling Lou,Xin Peng*

Main category: cs.SE

TL;DR: 本文提出了一种结合指针知识图谱（KG）与大语言模型（LLM）的项目级C到Rust翻译方法（\ourtool{}），通过在代码依赖图中引入全局指针语义（如指向关系、所有权、可变性等），显著减少生成Rust代码中的unsafe用法，并提升功能正确性。


<details>
  <summary>Details</summary>
Motivation: 现有基于LLM的C到Rust翻译方法在项目级别上难以正确处理指针，因其采用自底向上的单元翻译策略，缺乏对指针全局使用情况的把握，导致生成的Rust代码仍包含大量unsafe块。

Method: 作者构建了一个C-Rust指针知识图谱（KG），在传统调用图基础上增加两类指针语义信息：（i）记录全局指针行为（如指向流、结构体使用映射）；（ii）标注Rust相关属性（如所有权、可变性、可空性、生命周期）。该KG与LLM结合，用于指导项目级C代码翻译为安全且符合Rust惯用法的代码。

Result: 实验表明，\ourtool{}相比基于规则和传统LLM的方法，将翻译后Rust代码中的unsafe使用减少了99.9%，并在功能正确性上平均高出29.3%（对比结合模糊测试的LLM方法）。

Conclusion: 通过引入全局指针语义知识图谱，\ourtool{}有效解决了项目级C到Rust翻译中指针处理难题，显著提升了生成代码的安全性与正确性，为大规模遗留C代码迁移至Rust提供了可行方案。

Abstract: Translating C code into safe Rust is an effective way to ensure its memory
safety. Compared to rule-based translation which produces Rust code that
remains largely unsafe, LLM-based methods can generate more idiomatic and safer
Rust code because LLMs have been trained on vast amount of human-written
idiomatic code. Although promising, existing LLM-based methods still struggle
with project-level C-to-Rust translation. They typically partition a C project
into smaller units (\eg{} functions) based on call graphs and translate them
bottom-up to resolve program dependencies. However, this bottom-up,
unit-by-unit paradigm often fails to translate pointers due to the lack of a
global perspective on their usage. To address this problem, we propose a novel
C-Rust Pointer Knowledge Graph (KG) that enriches a code-dependency graph with
two types of pointer semantics: (i) pointer-usage information which record
global behaviors such as points-to flows and map lower-level struct usage to
higher-level units; and (ii) Rust-oriented annotations which encode ownership,
mutability, nullability, and lifetime. Synthesizing the \kg{} with LLMs, we
further propose \ourtool{}, which implements a project-level C-to-Rust
translation technique. In \ourtool{}, the \kg{} provides LLMs with
comprehensive pointer semantics from a global perspective, thus guiding LLMs
towards generating safe and idiomatic Rust code from a given C project. Our
experiments show that \ourtool{} reduces unsafe usages in translated Rust by
99.9\% compared to both rule-based translation and traditional LLM-based
rewriting, while achieving an average 29.3\% higher functional correctness than
those fuzzing-enhanced LLM methods.

</details>


### [45] [RepoSummary: Feature-Oriented Summarization and Documentation Generation for Code Repositories](https://arxiv.org/abs/2510.11039)
*Yifeng Zhu,Xianlin Zhao,Xutian Li,Yanzhen Zou,Haizhuo Yuan,Yue Wang,Bing Xie*

Main category: cs.SE

TL;DR: RepoSummary 是一种面向功能的代码仓库摘要方法，能自动生成文档并建立从功能到代码元素的准确追溯链接，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有仓库摘要技术主要基于目录树对代码进行摘要，难以将高层功能追溯到协同实现这些功能的具体方法。

Method: 提出 RepoSummary 方法，面向功能进行代码仓库摘要，同时自动生成仓库文档，并建立从功能到代码元素的准确追溯链接。

Result: 与当前最优基线 HGEN 相比，RepoSummary 在功能覆盖率和追溯准确性方面表现更优：手动文档中完全覆盖的功能比例从 61.2% 提升至 71.1%，文件级追溯召回率从 29.9% 提升至 53.0%，生成的文档在概念一致性、可读性和格式方面也更优。

Conclusion: RepoSummary 能有效提升代码仓库摘要的功能覆盖和追溯能力，有助于开发者在代码理解和维护过程中快速定位相关代码。

Abstract: Repository summarization is a crucial research question in development and
maintenance for software engineering. Existing repository summarization
techniques primarily focus on summarizing code according to the directory tree,
which is insufficient for tracing high-level features to the methods that
collaboratively implement them. To address these limitations, we propose
RepoSummary, a feature-oriented code repository summarization approach that
simultaneously generates repository documentation automatically. Furthermore,
it establishes more accurate traceability links from functional features to the
corresponding code elements, enabling developers to rapidly locate relevant
methods and files during code comprehension and maintenance. Comprehensive
experiments against the state-of-the-art baseline (HGEN) demonstrate that
RepoSummary achieves higher feature coverage and more accurate traceability. On
average, it increases the rate of completely covered features in manual
documentation from 61.2% to 71.1%, improves file-level traceability recall from
29.9% to 53.0%, and generates documentation that is more conceptually
consistent, easier to understand, and better formatted than that produced by
existing approaches.

</details>


### [46] [Defects4C: Benchmarking Large Language Model Repair Capability with C/C++ Bugs](https://arxiv.org/abs/2510.11059)
*Jian Wang,Xiaofei Xie,Qiang Hu,Shangqing Liu,Jiongchi Yu,Jiaolong Klong,Yi Li*

Main category: cs.SE

TL;DR: 本文提出了 Defects4C——一个专为 C/C++ 程序修复设计的高质量可执行基准数据集，并基于该数据集对 24 个主流大语言模型在 C/C++ 自动程序修复任务中的表现进行了系统评估。


<details>
  <summary>Details</summary>
Motivation: 尽管 C/C++ 被广泛使用且相关漏洞频发，但其自动程序修复（APR）研究远落后于 Java，主要原因是缺乏高质量、开源的 C/C++ 修复基准。

Method: 构建 Defects4C 基准，包含来自真实 C/C++ 项目的 900 万 bug 相关提交、248 个高质量缺陷函数和 102 个漏洞函数，并配备可复现的测试用例；随后利用该基准对 24 个先进大语言模型的修复能力进行实证研究。

Result: 实验揭示了当前大语言模型在 C/C++ 程序修复中的优势与局限，验证了 Defects4C 对评估和推动 APR 技术发展的有效性。

Conclusion: Defects4C 填补了 C/C++ 自动程序修复领域基准缺失的空白，为未来 APR 方法的开发与评估提供了关键基础设施，并指出了提升模型鲁棒性的必要性。

Abstract: Automated Program Repair (APR) plays a critical role in enhancing the quality
and reliability of software systems. While substantial progress has been made
in Java-based APR, largely facilitated by benchmarks like Defects4J, there
remains a significant gap in research on C/C++ program repair, despite the
widespread use of C/C++ and the prevalence of associated vulnerabilities. This
gap is primarily due to the lack of high-quality, open-source benchmarks
tailored for C/C++.
  To address this issue, we introduce Defects4C, a comprehensive and executable
benchmark specifically designed for C/C++ program repair. Our dataset is
constructed from real-world C/C++ repositories and includes a large collection
of bug-relevant commits (9M in total), 248 high-quality buggy functions, and
102 vulnerable functions, all paired with test cases for reproduction. These
resources enable rigorous evaluation of repair techniques and support the
retraining of learning-based approaches for enhanced performance.
  Using Defects4C, we conduct a comprehensive empirical study evaluating the
effectiveness of 24 state-of-the-art large language models (LLMs) in repairing
C/C++ faults. Our findings offer valuable insights into the strengths and
limitations of current LLM-based APR techniques in this domain, highlighting
both the need for more robust methods and the critical role of Defects4C in
advancing future research

</details>


### [47] [DebugTA: An LLM-Based Agent for Simplifying Debugging and Teaching in Programming Education](https://arxiv.org/abs/2510.11076)
*Lingyue Fu,Haowei Yuan,Datong Chen,Xinyi Dai,Qingyao Li,Weinan Zhang,Weiwen Liu,Yong Yu*

Main category: cs.SE

TL;DR: 本文提出DebugTA，一种基于大语言模型（LLM）的调试与教学智能体，通过专用工具（如标准代码检索、变量替换和外部编译器）简化复杂推理过程，有效提升编程教育中错误代码修改建议的准确性和教学效率。


<details>
  <summary>Details</summary>
Motivation: 现有方法在处理编程教育中的调试与教学（DT）任务时面临两大挑战：一是DT任务输入复杂且异构，增加了LLM的推理难度；二是未能充分利用标准代码，导致模型依赖复杂的多步推理，限制了LLM的潜力。

Method: 提出DebugTA框架，该框架基于明确的教学与调试原则，将复杂任务分解为多个子任务，通过依次调用专用工具（包括标准代码检索、变量替换对齐参考代码、外部编译器实时分析）进行分步推理，并引入学生模拟器-教师交互范式评估修改建议质量。

Result: 在三个真实代码数据集上的实验表明，DebugTA在提升教学效果的同时显著降低了计算成本。

Conclusion: DebugTA通过工具调用与任务分解有效缓解了LLM在DT任务中的推理负担，提高了修改建议的准确性与教学效率，验证了其在编程教育中的实用价值。

Abstract: In programming education, Debugging and Teaching (DT) task is a common
scenario where students receive assistance in correcting their erroneous code.
The task involves multiple inputs, including erroneous code, error messages,
reference solutions, and the question description, with the goal of generating
modification suggestions to the erroneous code. However, two key challenges
hinder the effectiveness of existing approaches. Firstly, the complexity and
heterogeneity of inputs inherent in DT tasks significantly elevate the
reasoning challenges faced by LLMs. Second, existing approaches often fail to
fully leverage the availability of standard code in DT tasks, forcing models to
rely solely on complex multi-step reasoning, which limits the potential of LLMs
in addressing DT tasks effectively. To address these challenges, we propose
DebugTA, a novel LLM-based debugging and teaching agent with specialized tools
for standard code retrieval, variable substitution to align reference code, and
an external compiler for real-time code analysis. Guided by explicit
pedagogical and debugging principles, DebugTA acts as an agent that decomposes
a complex task into sequential LLM interactions, each utilizing distinct tools
for specific subtasks, thereby simplifying the logical reasoning at each step
and reducing overall reasoning complexity. Furthermore, DebugTA utilizes tool
calls to align the standard code with the erroneous code as much as possible,
allowing the LLM to focus on logic errors within the erroneous code and
improving the accuracy of the generated suggestions. To rigorously assess the
quality of modification suggestions, we introduce a student simulator-teacher
interaction paradigm. Experimental results on three real-world code datasets
demonstrate that DebugTA consistently improves teaching effectiveness while
significantly reducing computational costs.

</details>


### [48] [What Slows Down FMware Development? An Empirical Study of Developer Challenges and Resolution Times](https://arxiv.org/abs/2510.11138)
*Zitao Wang,Zhimin Zhao,Michael W. Godfrey*

Main category: cs.SE

TL;DR: 本文首次对云平台和开源仓库中的FMware（基于基础模型构建的软件）开发进行了大规模实证分析，揭示了其主流应用领域、开发者面临的关键挑战及最耗时解决的问题。


<details>
  <summary>Details</summary>
Motivation: 基础模型（如GPT）正深刻改变软件工程实践，催生了FMware这一新型软件形态。然而，FMware在设计、实现和演进过程中带来了传统软件开发未曾面对的新挑战，尤其在云平台与本地部署环境中差异显著，亟需系统性研究以理解其开发生态与痛点。

Method: 研究通过实证分析GitHub开源仓库及HuggingFace、GPTStore、Ora和Poe等主流FMware平台的数据，聚焦三大方向：FMware的常见应用领域、开发者遇到的主要挑战，以及最耗费精力解决的问题类型。

Result: 研究发现FMware主要应用于教育、内容创作和商业策略领域；技术层面普遍存在内存管理、依赖处理和分词器配置等难题；GitHub上最常见的问题是缺陷报告和核心功能问题，而代码审查、相似性搜索和提示模板设计最耗时。

Conclusion: 本研究揭示了FMware开发者的实践模式与痛点，为改进FMware开发工具、优化工作流及加强社区支持提供了依据，并为FMware的未来发展提供了可操作的洞见。

Abstract: Foundation Models (FMs), such as OpenAI's GPT, are fundamentally transforming
the practice of software engineering by enabling the development of
\emph{FMware} -- applications and infrastructures built around these models.
FMware systems now support tasks such as code generation, natural-language
interaction, knowledge integration, and multi-modal content creation,
underscoring their disruptive impact on current software engineering workflows.
However, the design, implementation, and evolution of FMware present
significant new challenges, particularly across cloud-based and on-premise
platforms where goals, processes, and tools often diverge from those of
traditional software development.
  To our knowledge, this is the first large-scale analysis of FMware
development across both cloud-based platforms and open-source repositories. We
empirically investigate the FMware ecosystem through three focus areas: (1) the
most common application domains of FMware, (2) the key challenges developers
encounter, and (3) the types of issues that demand the greatest effort to
resolve. Our analysis draws on data from GitHub repositories and from leading
FMware platforms, including HuggingFace, GPTStore, Ora, and Poe. Our findings
reveal a strong focus on education, content creation, and business strategy,
alongside persistent technical challenges in memory management, dependency
handling, and tokenizer configuration. On GitHub, bug reports and core
functionality issues are the most frequently reported problems, while code
review, similarity search, and prompt template design are the most
time-consuming to resolve.
  By uncovering developer practices and pain points, this study points to
opportunities to improve FMware tools, workflows, and community support, and
provides actionable insights to help guide the future of FMware development.

</details>


### [49] [Interoperability From OpenTelemetry to Kieker: Demonstrated as Export from the Astronomy Shop](https://arxiv.org/abs/2510.11179)
*David Georg Reichelt,Shinhyung Yang,Wilhelm Hasselbring*

Main category: cs.SE

TL;DR: 本文提出了一种将OpenTelemetry追踪数据转换为Kieker框架格式的方法，从而扩展Kieker对C#、JavaScript等语言的支持，并通过Astronomy Shop示例验证了该方法的可行性。


<details>
  <summary>Details</summary>
Motivation: Kieker当前仅支持有限的编程语言（如Java、C、Fortran和Python），而OpenTelemetry提供了对更多语言（如C#和JavaScript）的追踪支持。为利用Kieker的分析能力处理这些语言的追踪数据，需实现OpenTelemetry到Kieker的数据转换。

Method: 设计并实现了一种将OpenTelemetry生成的追踪数据转换为Kieker框架可处理格式的方法，使得Kieker能够对这些数据进行分析（如生成调用树）。

Result: 成功将OpenTelemetry的追踪数据转换为Kieker格式，并在Astronomy Shop这一OpenTelemetry演示应用中实现了追踪数据的可视化。

Conclusion: 通过将OpenTelemetry与Kieker集成，扩展了Kieker对多语言追踪数据的支持，增强了其在异构系统中的可观测性分析能力。

Abstract: The observability framework Kieker provides a range of analysis capabilities,
but it is currently only able to instrument a smaller selection of languages
and technologies, including Java, C, Fortran, and Python. The OpenTelemetry
standard aims for providing reference implementations for most programming
languages, including C# and JavaScript, that are currently not supported by
Kieker. In this work, we describe how to transform OpenTelemetry tracing data
into the Kieker framework. Thereby, it becomes possible to create for example
call trees from OpenTelemetry instrumentations. We demonstrate the usability of
our approach by visualizing trace data of the Astronomy Shop, which is an
OpenTelemetry demo application.

</details>


### [50] [Cracking CodeWhisperer: Analyzing Developers' Interactions and Patterns During Programming Tasks](https://arxiv.org/abs/2510.11516)
*Jeena Javahar,Tanya Budhrani,Manaal Basha,Cleidson R. B. de Souza,Ivan Beschastnikh,Gema Rodriguez-Perez*

Main category: cs.SE

TL;DR: 本研究通过两项用户实验，分析开发者如何与AI代码生成工具CodeWhisperer互动，识别出四种典型行为模式。


<details>
  <summary>Details</summary>
Motivation: 随着AI代码生成工具的普及，理解开发者如何实际使用这些工具变得至关重要。

Method: 开展两轮用户研究，每组10名参与者，第一轮确定关键互动类型，第二轮通过自定义遥测插件收集低层级交互数据，并采用混合方法进行分析。

Result: 识别出四种开发者行为模式：1）渐进式代码优化，2）使用自然语言注释进行显式指令，3）利用模型建议进行基础结构搭建，4）与外部资源结合使用。

Conclusion: 研究提供了对开发者与AI代码生成工具互动方式的深入理解，揭示了四种核心使用模式，有助于未来工具设计和开发者支持策略的优化。

Abstract: The use of AI code-generation tools is becoming increasingly common, making
it important to understand how software developers are adopting these tools. In
this study, we investigate how developers engage with Amazon's CodeWhisperer,
an LLM-based code-generation tool. We conducted two user studies with two
groups of 10 participants each, interacting with CodeWhisperer - the first to
understand which interactions were critical to capture and the second to
collect low-level interaction data using a custom telemetry plugin. Our
mixed-methods analysis identified four behavioral patterns: 1) incremental code
refinement, 2) explicit instruction using natural language comments, 3)
baseline structuring with model suggestions, and 4) integrative use with
external sources. We provide a comprehensive analysis of these patterns .

</details>


### [51] [CodeWatcher: IDE Telemetry Data Extraction Tool for Understanding Coding Interactions with LLMs](https://arxiv.org/abs/2510.11536)
*Manaal Basha,Aimeê M. Ribeiro,Jeena Javahar,Cleidson R. B. de Souza,Gema Rodríguez-Pérez*

Main category: cs.SE

TL;DR: CodeWatcher 是一个轻量级、无干扰的客户端-服务器系统，用于在 VS Code 中捕获开发者与代码生成工具（CGT）交互的细粒度事件，支持对编程行为的深入研究。


<details>
  <summary>Details</summary>
Motivation: 研究开发者如何与代码生成工具互动需要详细且实时的编程行为数据，但这类数据通常难以在不干扰工作流的情况下收集。

Method: 开发了一个名为 CodeWatcher 的系统，包括 VS Code 插件、基于 Python 的 RESTful API 和 MongoDB 后端，所有组件均容器化，用于记录语义上有意义的编辑事件（如 CGT 插入、删除、复制粘贴和焦点切换），并为每个事件添加时间戳。

Result: CodeWatcher 能够在不改变用户工作流的前提下持续监控开发者活动，并支持对编码会话的事后重建和丰富的行为分析，特别是 CGT 的使用时机与方式。

Conclusion: CodeWatcher 为负责任 AI、开发者生产力以及以人为中心的 CGT 评估等研究提供了关键基础设施。

Abstract: Understanding how developers interact with code generation tools (CGTs)
requires detailed, real-time data on programming behavior which is often
difficult to collect without disrupting workflow. We present
\textit{CodeWatcher}, a lightweight, unobtrusive client-server system designed
to capture fine-grained interaction events from within the Visual Studio Code
(VS Code) editor. \textit{CodeWatcher} logs semantically meaningful events such
as insertions made by CGTs, deletions, copy-paste actions, and focus shifts,
enabling continuous monitoring of developer activity without modifying user
workflows. The system comprises a VS Code plugin, a Python-based RESTful API,
and a MongoDB backend, all containerized for scalability and ease of
deployment. By structuring and timestamping each event, \textit{CodeWatcher}
enables post-hoc reconstruction of coding sessions and facilitates rich
behavioral analyses, including how and when CGTs are used during development.
This infrastructure is crucial for supporting research on responsible AI,
developer productivity, and the human-centered evaluation of CGTs. Please find
the demo, diagrams, and tool here: https://osf.io/j2kru/overview.

</details>


### [52] [Automatically Generating Questions About Scratch Programs](https://arxiv.org/abs/2510.11658)
*Florian Obermüller,Gordon Fraser*

Main category: cs.SE

TL;DR: 本文提出了一种在Scratch编程环境中自动生成程序理解问题的方法，通过扩展LitterBox静态分析工具，基于已有的程序理解模型生成30类问题，并在大规模数据集上验证了其可行性与有效性。


<details>
  <summary>Details</summary>
Motivation: 仅通过学生是否完成编程任务来评估其学习效果无法反映其对编程概念的真实理解，而人工为每个学生程序设计理解性问题既繁琐又困难，因此需要一种自动化的解决方案。

Method: 作者基于已建立的程序理解模型，为Scratch块编程语言设计了30种问题类型，并扩展LitterBox静态分析工具，使其能针对任意Scratch程序自动生成相应的问题。

Result: 在包含600,913个项目的大型数据集上，系统自动生成了54,118,694个问题；对34名九年级学生的初步实验表明，这些问题具有实际意义，且学生回答这些问题的能力与其整体表现相关。

Conclusion: 该研究证明了在Scratch中自动生成程序理解问题的可行性，为评估学生对编程概念的理解提供了一种可扩展且有效的自动化方法。

Abstract: When learning to program, students are usually assessed based on the code
they wrote. However, the mere completion of a programming task does not
guarantee actual comprehension of the underlying concepts. Asking learners
questions about the code they wrote has therefore been proposed as a means to
assess program comprehension. As creating targeted questions for individual
student programs can be tedious and challenging, prior work has proposed to
generate such questions automatically. In this paper we generalize this idea to
the block-based programming language Scratch. We propose a set of 30 different
questions for Scratch code covering an established program comprehension model,
and extend the LitterBox static analysis tool to automatically generate
corresponding questions for a given Scratch program. On a dataset of 600,913
projects we generated 54,118,694 questions automatically. Our initial
experiments with 34 ninth graders demonstrate that this approach can indeed
generate meaningful questions for Scratch programs, and we find that the
ability of students to answer these questions on their programs relates to
their overall performance.

</details>
