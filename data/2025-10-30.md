<div id=toc></div>

# Table of Contents

- [cs.PF](#cs.PF) [Total: 1]
- [cs.DC](#cs.DC) [Total: 5]
- [cs.NI](#cs.NI) [Total: 1]
- [cs.SE](#cs.SE) [Total: 16]
- [cs.MA](#cs.MA) [Total: 3]


<div id='cs.PF'></div>

# cs.PF [[Back]](#toc)

### [1] [The influence of the random numbers quality on the results in stochastic simulations and machine learning](https://arxiv.org/abs/2510.25269)
*Benjamin A. Antunes*

Main category: cs.PF

TL;DR: 低质量伪随机数生成器（PRNG）会显著影响随机计算任务的结果，而一旦PRNG达到一定统计质量阈值，其具体类型对大多数任务影响甚微。


<details>
  <summary>Details</summary>
Motivation: 探究PRNG的统计质量差异是否会影响典型随机计算任务（如模拟、机器学习）的结果，因为这一影响尚未被充分研究。

Method: 选取7种不同质量的PRNG（从低质量LCG到高质量Mersenne Twister、PCG、Philox），在四个任务（流行病ABM模型、两个MNIST分类实现、CartPole强化学习）中进行实验，每种PRNG重复30次并使用固定种子，通过统计分析比较结果。

Result: 极低质量PRNG（如在TestU01 Crush中失败125项的LCG）显著影响ABM动态、降低MNIST准确率、严重损害RL性能；中等及以上质量PRNG在多数任务中表现与顶级PRNG相当，仅在RL任务中性能随统计质量提升而提升。

Conclusion: 在大多数工作负载中，只要PRNG达到足够的统计鲁棒性阈值，其家族或设计对结果影响可忽略，选择可基于性能和实现考量；但在敏感随机计算中使用低质量PRNG会引入显著系统误差。

Abstract: Pseudorandom number generators (PRNGs) are ubiquitous in stochastic
simulations and machine learning (ML), where they drive sampling, parameter
initialization, regularization, and data shuffling. While widely used, the
potential impact of PRNG statistical quality on computational results remains
underexplored. In this study, we investigate whether differences in PRNG
quality, as measured by standard statistical test suites, can influence
outcomes in representative stochastic applications. Seven PRNGs were evaluated,
ranging from low-quality linear congruential generators (LCGs) with known
statistical deficiencies to high-quality generators such as Mersenne Twister,
PCG, and Philox. We applied these PRNGs to four distinct tasks: an
epidemiological agent-based model (ABM), two independent from-scratch MNIST
classification implementations (Python/NumPy and C++), and a reinforcement
learning (RL) CartPole environment. Each experiment was repeated 30 times per
generator using fixed seeds to ensure reproducibility, and outputs were
compared using appropriate statistical analyses. Results show that very poor
statistical quality, as in the ''bad'' LCG failing 125 TestU01 Crush tests,
produces significant deviations in ABM epidemic dynamics, reduces MNIST
classification accuracy, and severely degrades RL performance. In contrast,
mid-and good-quality LCGs-despite failing a limited number of Crush or BigCrush
tests-performed comparably to top-tier PRNGs in most tasks, with the RL
experiment being the primary exception where performance scaled with
statistical quality. Our findings indicate that, once a generator meets a
sufficient statistical robustness threshold, its family or design has
negligible impact on outcomes for most workloads, allowing selection to be
guided by performance and implementation considerations. However, the use of
low-quality PRNGs in sensitive stochastic computations can introduce
substantial and systematic errors.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [2] [Radar DataTree: A FAIR and Cloud-Native Framework for Scalable Weather Radar Archives](https://arxiv.org/abs/2510.24943)
*Alfonso Ladino-Rincon,Stephen W. Nesbitt*

Main category: cs.DC

TL;DR: Radar DataTree 是一个基于 WMO FM-301 标准的新型数据集级框架，通过将天气雷达体扫数据组织为符合 FAIR 原则、云优化的分层结构（使用 xarray DataTree 和 Zarr 格式），显著提升大规模雷达数据分析的效率与可复现性，并支持 AI 就绪的气象基础设施。


<details>
  <summary>Details</summary>
Motivation: 天气雷达数据虽科学价值高，但现有档案普遍存在碎片化、厂商依赖性强、不符合 FAIR 原则等问题，限制了大规模研究、可复现性及云原生计算的发展。

Method: 基于 FM-301/CfRadial 2.1 标准，利用 xarray DataTree 构建分层、富含元数据的雷达数据结构，并序列化为 Zarr 格式；结合 Icechunk 实现 ACID 兼容的存储与版本控制，支持高效并行计算。

Result: 在 QVP 和降水累积等案例研究中展现出显著性能提升，并通过 Raw2Zarr 仓库开源全部工具与数据集。

Conclusion: Radar DataTree 为雷达数据管理、高性能地球科学研究和 AI 就绪的气象基础设施提供了可复现、可扩展的基础。

Abstract: We introduce Radar DataTree, the first dataset-level framework that extends
the WMO FM-301 standard from individual radar volume scans to time-resolved,
analysis-ready archives. Weather radar data are among the most scientifically
valuable yet structurally underutilized Earth observation datasets. Despite
widespread public availability, radar archives remain fragmented,
vendor-specific, and poorly aligned with FAIR (Findable, Accessible,
Interoperable, Reusable) principles, hindering large-scale research,
reproducibility, and cloud-native computation. Radar DataTree addresses these
limitations with a scalable, open-source architecture that transforms
operational radar archives into FAIR-compliant, cloud-optimized datasets. Built
on the FM-301/CfRadial 2.1 standard and implemented using xarray DataTree,
Radar DataTree organizes radar volume scans as hierarchical, metadata-rich
structures and serializes them to Zarr for scalable analysis. Coupled with
Icechunk for ACID-compliant storage and versioning, this architecture enables
efficient, parallel computation across thousands of radar scans with minimal
preprocessing. We demonstrate significant performance gains in case studies
including Quasi-Vertical Profile (QVP) and precipitation accumulation
workflows, and release all tools and datasets openly via the Raw2Zarr
repository. This work contributes a reproducible and extensible foundation for
radar data stewardship, high-performance geoscience, and AI-ready weather
infrastructure.

</details>


### [3] [A Privacy-Preserving Ecosystem for Developing Machine Learning Algorithms Using Patient Data: Insights from the TUM.ai Makeathon](https://arxiv.org/abs/2510.25277)
*Simon Süwer,Mai Khanh Mai,Christoph Klein,Nicola Götzenberger,Denis Dalić,Andreas Maier,Jan Baumbach*

Main category: cs.DC

TL;DR: 本文提出了一种多阶段安全AI训练方法，结合模拟临床知识图谱、联邦学习框架（FeatureCloud）和医院本地训练，在保护患者隐私的前提下实现医疗AI模型开发，并在TUM.ai Makeathon 2024挑战赛中成功验证。


<details>
  <summary>Details</summary>
Motivation: 由于GDPR对临床数据使用的严格限制，特别是在罕见病小队列研究中，高质量结构化数据难以获取，阻碍了预测性医疗AI的发展。因此，亟需一种既能保护隐私又能有效利用临床数据的方法。

Method: 该方法包括四个阶段：(1) 在模拟临床知识图谱（cKG）上设计模型，仅保留结构特征而不含敏感信息；(2) 将模型集成到FeatureCloud联邦学习框架中，在单客户端配置和受保护执行环境中准备；(3) 在医院本地真实cKG上进行训练，由医院人员监督或通过自动化流程控制；(4) 执行经验证的评估脚本，仅返回聚合性能指标。

Result: 在TUM.ai Makeathon 2024挑战赛中，50名学生在未接触真实数据的情况下成功开发了用于患者分类与诊断的模型，验证了该方法的可行性。

Conclusion: 通过联邦学习框架部署安全算法，结合模拟知识图谱与本地训练，是一种实现医疗领域隐私保护AI的可行路径。

Abstract: The integration of clinical data offers significant potential for the
development of personalized medicine. However, its use is severely restricted
by the General Data Protection Regulation (GDPR), especially for small cohorts
with rare diseases. High-quality, structured data is essential for the
development of predictive medical AI. In this case study, we propose a novel,
multi-stage approach to secure AI training: (1) The model is designed on a
simulated clinical knowledge graph (cKG). This graph is used exclusively to
represent the structural characteristics of the real cKG without revealing any
sensitive content. (2) The model is then integrated into the FeatureCloud (FC)
federated learning framework, where it is prepared in a single-client
configuration within a protected execution environment. (3) Training then takes
place within the hospital environment on the real cKG, either under the direct
supervision of hospital staff or via a fully automated pipeline controlled by
the hospital. (4) Finally, verified evaluation scripts are executed, which only
return aggregated performance metrics. This enables immediate performance
feedback without sensitive patient data or individual predictions, leaving the
clinic. A fundamental element of this approach involves the incorporation of a
cKG, which serves to organize multi-omics and patient data within the context
of real-world hospital environments. This approach was successfully validated
during the TUM.ai Makeathon 2024 (TUMaiM24) challenge set by the Dr. von Hauner
Children's Hospital (HCH-LMU): 50 students developed models for patient
classification and diagnosis without access to real data. Deploying secure
algorithms via federated frameworks, such as the FC framework, could be a
practical way of achieving privacy-preserving AI in healthcare.

</details>


### [4] [Scheduling Data-Intensive Workloads in Large-Scale Distributed Systems: Trends and Challenges](https://arxiv.org/abs/2510.25362)
*Georgios L. Stavrinides,Helen D. Karatza*

Main category: cs.DC

TL;DR: 本文对数据密集型工作负载进行了分类，综述了在大规模分布式系统中调度这类任务的常用方法，介绍了文献中提出的新策略，并探讨了当前面临的挑战与未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 随着大数据的爆炸式增长，数据密集型应用日益复杂且计算需求高，需在大规模分布式资源上高效调度，同时满足服务质量（如时间约束、容错性）和能效等多目标要求，这对调度技术提出了重大挑战。

Method: 提出一种数据密集型工作负载的分类方法，并综述现有调度策略，分析近年来文献中提出的新型调度方法。

Result: 系统梳理了数据密集型工作负载调度的研究现状，归纳了主流方法和创新策略，明确了该领域的开放问题。

Conclusion: 有效的调度技术对处理复杂数据密集型工作负载至关重要，未来需进一步探索兼顾性能、可靠性与能效的智能调度机制。

Abstract: With the explosive growth of big data, workloads tend to get more complex and
computationally demanding. Such applications are processed on distributed
interconnected resources that are becoming larger in scale and computational
capacity. Data-intensive applications may have different degrees of parallelism
and must effectively exploit data locality. Furthermore, they may impose
several Quality of Service requirements, such as time constraints and
resilience against failures, as well as other objectives, like energy
efficiency. These features of the workloads, as well as the inherent
characteristics of the computing resources required to process them, present
major challenges that require the employment of effective scheduling
techniques. In this chapter, a classification of data-intensive workloads is
proposed and an overview of the most commonly used approaches for their
scheduling in large-scale distributed systems is given. We present novel
strategies that have been proposed in the literature and shed light on open
challenges and future directions.

</details>


### [5] [Can Like Attract Like? A Study of Homonymous Gathering in Networks](https://arxiv.org/abs/2510.25451)
*Stéphane Devismes,Yoann Dieudonné,Arnaud Labourel*

Main category: cs.DC

TL;DR: 本文研究了在移动智能体可能共享相同标签的情况下，如何实现确定性聚集（gathering）的问题，给出了可聚集团队的完整刻画，并设计了一个高效算法，仅需极少的公共知识即可在多项式时间内完成聚集。


<details>
  <summary>Details</summary>
Motivation: 传统确定性聚集算法通常假设每个智能体拥有唯一的标签以打破对称性，但本文探讨是否必须所有标签都互异才能保证聚集，并研究在标签可重复情况下的可行性与效率。

Method: 作者对可聚集的智能体团队进行了完整刻画，并设计了一个时间复杂度为 poly$(n,\log\lambda)$ 的算法，该算法仅需智能体初始共享 $O(\log \log \log \mu)$ 位的公共知识，其中 $\mu$ 是最大标签重数。

Result: (1) 完整刻画了可聚集的智能体团队；(2) 提出一个高效聚集算法，仅需极少公共知识；(3) 证明该公共知识量几乎是最优的。此外，还得到了首个无需公共知识、适用于任意规模团队的 poly$(n,\log\lambda)$ 时间确定性聚集算法。

Conclusion: 即使智能体共享标签，只要满足特定条件，仍可实现高效确定性聚集；且所需公共知识量接近理论下限，同时解决了任意规模团队在无公共知识下的终止检测难题。

Abstract: A team of mobile agents, starting from distinct nodes of a network, have to
meet at the same node and declare that they all met. Agents execute the same
algorithm, which they start when activated by an adversary or by an agent
entering their initial node. When activated, agents traverse edges of the
network in synchronous rounds. Their perception and communication are strictly
local. This task, known as gathering, is a central problem in distributed
mobile systems. Most prior work focuses on minimizing its time complexity,
i.e., the worst-case number of rounds between the start of the earliest agent
and the task completion. To break possible symmetries, deterministic solutions
typically assume that agents have pairwise distinct IDs, called labels, known
only to themselves. But must all labels be pairwise distinct to guarantee
deterministic gathering?
  We address this question by considering agents that may share the same label.
A team L is said to be gatherable if, for every initial setting of L, there is
an algorithm that solves gathering. Our contribution is threefold. (1) We give
a full characterization of the gatherable teams. (2) We design an algorithm
that gathers all of them in poly$(n,\log\lambda)$ time, where $n$ (resp.
$\lambda$) is the graph order (resp. the smallest label in L). This algorithm
requires the agents to initially share only $O(\log \log \log \mu)$ bits of
common knowledge, where $\mu$ is the largest label multiplicity in L. (3) We
show this dependency is almost optimal to get a poly$(n,\log\lambda)$-time
complexity.
  As a by-product, we get the first deterministic poly$(n,\log\lambda)$-time
algorithm requiring no common knowledge to gather any team when all labels are
distinct. Known to be achievable for two-agent teams, extending this to any
team size faced a major challenge: termination detection. Our techniques to
address it may be of independent interest.

</details>


### [6] [Holon Streaming: Global Aggregations with Windowed CRDTs](https://arxiv.org/abs/2510.25757)
*Jonas Spenger,Kolya Krafeld,Ruben van Gemeren,Philipp Haller,Paris Carbone*

Main category: cs.DC

TL;DR: Holon Streaming 是一种支持全局聚合的精确一次（exactly-once）流处理系统，通过引入窗口化无冲突复制数据类型（Windowed CRDTs）实现可扩展性，并利用去中心化协调机制显著降低延迟、提升吞吐量，尤其在故障恢复场景下表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有精确一次流处理系统在处理全局聚合时存在可扩展性差的问题，通常依赖单任务实例或静态聚合树，易形成性能瓶颈；同时，端到端延迟受最慢路径限制，且故障与重配置时因集中式协调导致延迟激增。

Method: 提出 Holon Streaming 系统，采用确定性编程模型和新型抽象——窗口化无冲突复制数据类型（Windowed CRDTs），以支持可扩展的全局聚合，并基于其确定性和收敛性设计高效的去中心化故障恢复算法。

Result: 实验表明，相比现有系统，Holon Streaming 在全局聚合负载下延迟降低5倍、吞吐量提升2倍，在故障场景下延迟减少11倍。

Conclusion: 去中心化协调结合确定性编程模型能有效提升流处理系统在全局聚合任务中的性能和鲁棒性，而 Windowed CRDTs 是实现这一目标的关键抽象。

Abstract: Scaling global aggregations is a challenge for exactly-once stream processing
systems. Current systems implement these either by computing the aggregation in
a single task instance, or by static aggregation trees, which limits
scalability and may become a bottleneck. Moreover, the end-to-end latency is
determined by the slowest path in the tree, and failures and reconfiguration
cause large latency spikes due to the centralized coordination. Towards these
issues, we present Holon Streaming, an exactly-once stream processing system
for global aggregations. Its deterministic programming model uses windowed
conflict-free replicated data types (Windowed CRDTs), a novel abstraction for
shared replicated state. Windowed CRDTs make computing global aggregations
scalable. Furthermore, their guarantees such as determinism and convergence
enable the design of efficient failure recovery algorithms by decentralized
coordination. Our evaluation shows a 5x lower latency and 2x higher throughput
than an existing stream processing system on global aggregation workloads, with
an 11x latency reduction under failure scenarios. The paper demonstrates the
effectiveness of decentralized coordination with determinism, and the utility
of Windowed CRDTs for global aggregations.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [7] [MetaLore: Learning to Orchestrate Communication and Computation for Metaverse Synchronization](https://arxiv.org/abs/2510.25705)
*Elif Ebru Ohri,Qi Liao,Anastasios Giovanidis,Francesca Fossati,Nour-El-Houda Yellas*

Main category: cs.NI

TL;DR: 本文提出了MetaLore，一种基于深度强化学习的资源分配框架，用于元宇宙和数字孪生环境中通信与计算资源的联合优化，通过引入新型信息年龄指标（AoRI和AoSI）提升同步质量，并在小规模观测空间下实现接近穷举法的性能。


<details>
  <summary>Details</summary>
Motivation: 随着增强现实和虚拟现实的发展，实现在物理与数字世界之间的无缝同步成为关键挑战，尤其是在对延迟敏感的实时应用中。现有方法难以在动态环境下同时满足低延迟、高吞吐和资源高效利用的需求。

Method: 提出MetaLore框架，采用深度强化学习动态分配通信带宽与计算资源；设计两种新的信息年龄指标（AoRI和AoSI）并融入奖励函数；利用仅包含两个队列长度的小型任务导向观测空间进行决策。

Result: MetaLore在开源模拟器中验证，性能接近穷举搜索的最优解，同时能高效适应动态流量变化，有效保障端到端延迟并提升同步质量。

Conclusion: MetaLore通过创新的信息年龄指标和轻量级观测机制，为元宇宙和数字孪生系统提供了一种高效、自适应的资源协同分配方案，显著提升了实时同步性能。

Abstract: As augmented and virtual reality evolve, achieving seamless synchronization
between physical and digital realms remains a critical challenge, especially
for real-time applications where delays affect the user experience. This paper
presents MetaLore, a Deep Reinforcement Learning (DRL) based framework for
joint communication and computational resource allocation in Metaverse or
digital twin environments. MetaLore dynamically shares the communication
bandwidth and computational resources among sensors and mobile devices to
optimize synchronization, while offering high throughput performance. Special
treatment is given in satisfying end-to-end delay guarantees. A key
contribution is the introduction of two novel Age of Information (AoI) metrics:
Age of Request Information (AoRI) and Age of Sensor Information (AoSI),
integrated into the reward function to enhance synchronization quality. An open
source simulator has been extended to incorporate and evaluate the approach.
The DRL solution is shown to achieve the performance of full-enumeration
brute-force solutions by making use of a small, task-oriented observation space
of two queue lengths at the network side. This allows the DRL approach the
flexibility to effectively and autonomously adapt to dynamic traffic
conditions.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [8] [Beyond Function-Level Search: Repository-Aware Dual-Encoder Code Retrieval with Adversarial Verification](https://arxiv.org/abs/2510.24749)
*Aofan Liu,Shiyuan Song,Haoxuan Li,Cehao Yang,Yiyan Qi*

Main category: cs.SE

TL;DR: 本文提出了RepoAlign-Bench基准和ReflectCode模型，以提升面向代码变更请求的仓库级代码检索性能。


<details>
  <summary>Details</summary>
Motivation: 现有函数级代码检索方法难以理解跨组件的代码变更意图，缺乏对仓库级上下文的建模能力，因此亟需新的检索范式与评估基准。

Method: 作者构建了包含5.2万标注实例的仓库级代码检索基准RepoAlign-Bench，并提出ReflectCode模型，该模型采用对抗反射增强的双塔架构，通过大语言模型引导，动态融合语法模式、函数依赖和语义扩展意图。

Result: 实验表明，ReflectCode在Top-5准确率上比现有最优方法提升12.2%，召回率提升7.1%。

Conclusion: ReflectCode为上下文感知的代码检索提供了新方向，RepoAlign-Bench为未来研究提供了有效评估基准。

Abstract: The escalating complexity of modern codebases has intensified the need for
retrieval systems capable of interpreting cross-component change intents, a
capability fundamentally absent in conventional function-level search
paradigms. While recent studies have improved the alignment between natural
language queries and code snippets, retrieving contextually relevant code for
specific change requests remains largely underexplored. To address this gap, we
introduce RepoAlign-Bench, the first benchmark specifically designed to
evaluate repository-level code retrieval under change request driven scenarios,
encompassing 52k annotated instances. This benchmark shifts the retrieval
paradigm from function-centric matching to holistic repository-level reasoning.
Furthermore, we propose ReflectCode, an adversarial reflection augmented
dual-tower architecture featuring disentangled code_encoder and doc_encoder
components. ReflectCode dynamically integrates syntactic patterns, function
dependencies, and semantic expansion intents through large language model
guided reflection. Comprehensive experiments demonstrate that ReflectCode
achieves 12.2% improvement in Top-5 Accuracy and 7.1% in Recall over
state-of-the-art baselines, establishing a new direction for context-aware code
retrieval.

</details>


### [9] [Compiler.next: A Search-Based Compiler to Power the AI-Native Future of Software Engineering](https://arxiv.org/abs/2510.24799)
*Filipe R. Cogo,Gustavo A. Oliva,Ahmed E. Hassan*

Main category: cs.SE

TL;DR: Compiler.next 是一种新型的基于搜索的编译器，通过自动优化认知架构和系统参数，将人类意图直接转化为可运行软件，旨在推动软件工程3.0时代的发展并降低非专家开发门槛。


<details>
  <summary>Details</summary>
Motivation: 现有AI辅助软件工程工具受限于认知过载、工具集成效率低下以及AI协作者能力有限，难以支持AI原生软件系统的无缝演进。

Method: 提出Compiler.next，一种基于搜索的编译器，通过动态优化提示、基础模型配置和系统参数，在准确性、成本和延迟等多目标间寻找最优解，将人类意图自动转化为可运行软件。

Result: 论文提出了Compiler.next的架构，并规划了解决意图编译核心挑战的路线图，包括高质量编程构造、高效搜索启发式、可复现性及编译器间互操作性。

Conclusion: Compiler.next有望成为软件工程3.0的基石，推动全自动、搜索驱动的软件开发，加速创新并提升AI驱动系统的效率与可及性。

Abstract: The rapid advancement of AI-assisted software engineering has brought
transformative potential to the field of software engineering, but existing
tools and paradigms remain limited by cognitive overload, inefficient tool
integration, and the narrow capabilities of AI copilots. In response, we
propose Compiler.next, a novel search-based compiler designed to enable the
seamless evolution of AI-native software systems as part of the emerging
Software Engineering 3.0 era. Unlike traditional static compilers,
Compiler.next takes human-written intents and automatically generates working
software by searching for an optimal solution. This process involves dynamic
optimization of cognitive architectures and their constituents (e.g., prompts,
foundation model configurations, and system parameters) while finding the
optimal trade-off between several objectives, such as accuracy, cost, and
latency. This paper outlines the architecture of Compiler.next and positions it
as a cornerstone in democratizing software development by lowering the
technical barrier for non-experts, enabling scalable, adaptable, and reliable
AI-powered software. We present a roadmap to address the core challenges in
intent compilation, including developing quality programming constructs,
effective search heuristics, reproducibility, and interoperability between
compilers. Our vision lays the groundwork for fully automated, search-driven
software development, fostering faster innovation and more efficient AI-driven
systems.

</details>


### [10] [A Roadmap for Tamed Interactions with Large Language Models](https://arxiv.org/abs/2510.24819)
*Vincenzo Scotti,Jan Keim,Tobias Hey,Andreas Metzger,Anne Koziolek,Raffaela Mirandola*

Main category: cs.SE

TL;DR: 本文提出一种面向大语言模型（LLM）交互的领域特定语言（DSL）——LLM脚本语言（LSL），旨在通过编程方式控制LLM输出、增强结构化交互，并集成验证、验证与可解释性，以提升AI应用的可靠性与可信度。


<details>
  <summary>Details</summary>
Motivation: 大语言模型（LLM）虽应用广泛，但其不可靠性（如产生错误或幻觉内容）阻碍了在自动化工作流中的应用。现有工具缺乏统一框架来确保LLM输出的可靠性、鲁棒性和可信性。

Method: 提出一种LLM脚本语言（LSL），作为领域特定语言（DSL），用于编程化地定义与LLM的交互，控制输出、强制交互结构，并整合软件工程中的形式化验证、验证与可解释性技术。

Result: 尚未提供具体实验结果，但提出了LSL的愿景和设计目标，强调其在提升LLM交互可控性与可信度方面的潜力。

Conclusion: 通过引入LSL，有望将LLM交互从模型训练和实现中解耦，使其更加可编程、可靠，并与软件工程实践紧密结合，从而推动AI驱动软件的发展。

Abstract: We are witnessing a bloom of AI-powered software driven by Large Language
Models (LLMs). Although the applications of these LLMs are impressive and
seemingly countless, their unreliability hinders adoption. In fact, the
tendency of LLMs to produce faulty or hallucinated content makes them
unsuitable for automating workflows and pipelines. In this regard, Software
Engineering (SE) provides valuable support, offering a wide range of formal
tools to specify, verify, and validate software behaviour. Such SE tools can be
applied to define constraints over LLM outputs and, consequently, offer
stronger guarantees on the generated content. In this paper, we argue that the
development of a Domain Specific Language (DSL) for scripting interactions with
LLMs using an LLM Scripting Language (LSL) may be key to improve AI-based
applications. Currently, LLMs and LLM-based software still lack reliability,
robustness, and trustworthiness, and the tools or frameworks to cope with these
issues suffer from fragmentation. In this paper, we present our vision of LSL.
With LSL, we aim to address the limitations above by exploring ways to control
LLM outputs, enforce structure in interactions, and integrate these aspects
with verification, validation, and explainability. Our goal is to make LLM
interaction programmable and decoupled from training or implementation.

</details>


### [11] [VeriStruct: AI-assisted Automated Verification of Data-Structure Modules in Verus](https://arxiv.org/abs/2510.25015)
*Chuyue Sun,Yican Sun,Daneshvar Amrollahi,Ethan Zhang,Shuvendu Lahiri,Shan Lu,David Dill,Clark Barrett*

Main category: cs.SE

TL;DR: VeriStruct 是一个用于在 Verus 中实现复杂数据结构模块自动形式化验证的新框架，通过规划器协调生成抽象、不变式、规范和证明代码，并结合提示中的语法指导与自动修复机制，成功验证了11个Rust数据结构模块中的10个，共128/129个函数。


<details>
  <summary>Details</summary>
Motivation: 现有AI辅助验证方法主要局限于单个函数，难以扩展到更复杂的数据结构模块；同时，大语言模型（LLMs）常误解Verus的注解语法和验证语义，阻碍了自动化验证的可靠性。

Method: VeriStruct 引入一个规划器模块，系统地生成抽象、类型不变式、规范和证明代码；通过在提示中嵌入语法指导，并设置自动修复阶段以纠正注解错误，提升LLM生成结果的正确性。

Result: 在对11个Rust数据结构模块的评估中，VeriStruct 成功验证了其中10个，共128个函数（99.2%）通过验证。

Conclusion: VeriStruct 展示了将AI辅助形式化验证扩展到复杂数据结构模块的可行性，是迈向全自动AI辅助验证的重要一步。

Abstract: We introduce VeriStruct, a novel framework that extends AI-assisted automated
verification from single functions to more complex data structure modules in
Verus. VeriStruct employs a planner module to orchestrate the systematic
generation of abstractions, type invariants, specifications, and proof code. To
address the challenge that LLMs often misunderstand Verus' annotation syntax
and verification-specific semantics, VeriStruct embeds syntax guidance within
prompts and includes a repair stage to automatically correct annotation errors.
In an evaluation on eleven Rust data structure modules, VeriStruct succeeds on
ten of the eleven, successfully verifying 128 out of 129 functions (99.2%) in
total. These results represent an important step toward the goal of automatic
AI-assisted formal verification.

</details>


### [12] [Automating Benchmark Design](https://arxiv.org/abs/2510.25039)
*Amanda Dsouza,Harit Vishwakarma,Zhengyang Qi,Justin Bauer,Derek Pham,Thomas Walshe,Armin Parchami,Frederic Sala,Paroma Varma*

Main category: cs.SE

TL;DR: 本文提出 BeTaL 框架，利用大语言模型（LLM）自动设计动态基准测试，通过参数化基准模板并引导 LLM 在参数空间中搜索，以高效生成具有目标难度和真实性的评测任务。实验表明，BeTaL 生成的基准在难度控制上显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前对大语言模型（LLM）及其智能体的评估主要依赖手工构建的静态基准，这类基准容易饱和；而动态基准虽能随模型演进，但构建和维护成本高昂。因此，亟需一种高效、自动化的动态基准生成方法。

Method: 提出 BeTaL（Benchmark Tuning with an LLM-in-the-loop）框架，将基准模板中的关键设计选择参数化，并利用 LLM 在参数空间中推理，以低成本生成满足目标属性（如难度、真实性）的动态基准。

Result: 在三项任务（包括两个新基准和一个扩展的 τ-bench）上验证 BeTaL 的有效性，结果表明其生成的基准在目标难度上的平均偏差为 5.3%–13.2%，比基线方法提升 2–4 倍。

Conclusion: BeTaL 能高效、自动地生成贴近目标难度的动态基准，显著优于现有静态或手动动态基准方法，为 LLM 评估提供了可扩展的新范式。

Abstract: The rapid progress and widespread deployment of LLMs and LLM-powered agents
has outpaced our ability to evaluate them. Hand-crafted, static benchmarks are
the primary tool for assessing model capabilities, but these quickly become
saturated. In contrast, dynamic benchmarks evolve alongside the models they
evaluate, but are expensive to create and continuously update. To address these
challenges, we develop BeTaL (Benchmark Tuning with an LLM-in-the-loop), a
framework that leverages environment design principles to automate the process
of dynamic benchmark design. BeTaL works by parameterizing key design choices
in base benchmark templates and uses LLMs to reason through the resulting
parameter space to obtain target properties (such as difficulty and realism) in
a cost-efficient manner. We validate this approach on its ability to create
benchmarks with desired difficulty levels. Using BeTaL, we create two new
benchmarks and extend a popular agentic benchmark $\tau$-bench. Extensive
evaluation on these three tasks and multiple target difficulty levels shows
that BeTaL produces benchmarks much closer to the desired difficulty, with
average deviations ranging from 5.3% to 13.2% -- a 2-4x improvement over the
baselines.

</details>


### [13] [Same Same But Different: Preventing Refactoring Attacks on Software Plagiarism Detection](https://arxiv.org/abs/2510.25057)
*Robin Maisch,Larissa Schmid,Timur Sağlam,Nils Niehues*

Main category: cs.SE

TL;DR: 本文提出一种基于代码属性图和图变换的新框架，有效提升对重构类代码抄袭的检测能力。


<details>
  <summary>Details</summary>
Motivation: 现有教育场景中的代码抄袭检测系统难以应对保留程序行为但改变结构的重构式混淆攻击。

Method: 利用代码属性图和图变换技术构建可扩展框架，增强现有检测器对重构混淆的鲁棒性。

Result: 在真实学生提交代码上进行评估，涵盖算法与AI生成的混淆攻击，显著提升了抄袭检测效果。

Conclusion: 所提框架能有效应对重构式代码混淆，显著提升教育环境中代码抄袭检测的准确性与鲁棒性。

Abstract: Plagiarism detection in programming education faces growing challenges due to
increasingly sophisticated obfuscation techniques, particularly automated
refactoring-based attacks. While code plagiarism detection systems used in
education practice are resilient against basic obfuscation, they struggle
against structural modifications that preserve program behavior, especially
caused by refactoring-based obfuscation. This paper presents a novel and
extensible framework that enhances state-of-the-art detectors by leveraging
code property graphs and graph transformations to counteract refactoring-based
obfuscation. Our comprehensive evaluation of real-world student submissions,
obfuscated using both algorithmic and AI-based obfuscation attacks,
demonstrates a significant improvement in detecting plagiarized code.

</details>


### [14] [Adaptive Proof Refinement with LLM-Guided Strategy Selection](https://arxiv.org/abs/2510.25103)
*Minghai Lu,Zhe Zhou,Danning Xie,Songlin Jia,Benjamin Delaware,Tianyi Zhang*

Main category: cs.SE

TL;DR: 本文提出Adapt框架，利用大语言模型（LLM）动态选择合适的证明优化策略，显著提升定理证明性能，在两个基准上分别比现有最佳方法多证明16.63%和18.58%的定理。


<details>
  <summary>Details</summary>
Motivation: 形式化验证中的定理证明虽能严格保证软件正确性，但因依赖大量人工和专业知识而难以扩展；现有基于LLM的证明生成方法多采用固定优化策略，无法根据错误证明的具体问题动态调整，限制了其效果。

Method: 提出Adapt框架，通过LLM引导的决策模块，根据证明助手状态和错误证明上下文动态选择最合适的优化策略。

Result: 在两个基准测试中，Adapt分别比最佳基线多证明16.63%和18.58%的定理；在五种不同LLM上验证了其通用性，并通过消融实验分析了各组件贡献及不同决策器设计的权衡。

Conclusion: Adapt通过动态策略选择显著提升了LLM在定理证明中的效果和泛化能力，为形式化验证的自动化提供了有效方案。

Abstract: Formal verification via theorem proving enables the expressive specification
and rigorous proof of software correctness, but it is difficult to scale due to
the significant manual effort and expertise required. While Large Language
Models (LLMs) show potential in proof generation, they frequently produce
incorrect proofs on the first attempt and require additional strategies for
iterative refinement. However, existing approaches employ fixed refinement
strategies and cannot dynamically choose an effective strategy based on the
particular issues in a generated proof, which limits their performance. To
overcome this limitation, we introduce Adapt, a novel proof refinement
framework that leverages an LLM-guided decision-maker to dynamically select a
suitable refinement strategy according to the state of the proof assistant and
available context of an incorrect proof. We evaluate Adapt on two benchmarks
against four existing methods and find that it significantly outperforms the
best baseline on both by proving 16.63% and 18.58% more theorems, respectively.
Furthermore, we demonstrate Adapt's generalizability by evaluating it across
five different LLMs. We also conduct ablation studies to measure the
contribution of each component and compare the trade-offs of alternative
decision-maker designs.

</details>


### [15] [Automated Program Repair Based on REST API Specifications Using Large Language Models](https://arxiv.org/abs/2510.25148)
*Katsuki Yamagishi,Norihiro Yoshida,Erina Makihara,Katsuro Inoue*

Main category: cs.SE

TL;DR: dcFix 是一种结合 API 规范与大语言模型（LLM）自动检测并修复客户端程序中 REST API 误用的方法，相比基线方法效果更优。


<details>
  <summary>Details</summary>
Motivation: 开发者在使用云服务 REST API 时，常因错误信息缺乏细节而在测试阶段才发现规范违反问题，导致调试过程依赖试错。

Method: dcFix 识别不符合规范的代码片段，将其与相关 API 规范整合为提示，利用大语言模型生成修复后的代码。

Result: 实验表明，dcFix 能准确检测 API 误用，且在修复效果上优于未提供违规代码信息的基线方法。

Conclusion: 结合 API 规范与 LLM 的提示工程可有效提升 REST API 误用的自动检测与修复能力。

Abstract: Many cloud services provide REST API accessible to client applications.
However, developers often identify specification violations only during
testing, as error messages typically lack the detail necessary for effective
diagnosis. Consequently, debugging requires trial and error. This study
proposes dcFix, a method for detecting and automatically repairing REST API
misuses in client programs. In particular, dcFix identifies non-conforming code
fragments, integrates them with the relevant API specifications into prompts,
and leverages a Large Language Model (LLM) to produce the corrected code. Our
evaluation demonstrates that dcFix accurately detects misuse and outperforms
the baseline approach, in which prompts to the LLM omit any indication of code
fragments non conforming to REST API specifications.

</details>


### [16] [Optimizing Knowledge Utilization for Multi-Intent Comment Generation with Large Language Models](https://arxiv.org/abs/2510.25195)
*Shuochuan Li,Zan Wang,Xiaoning Du,Zhuo Wu,Jiuqiao Yu,Junjie Chen*

Main category: cs.SE

TL;DR: 本文提出了一种名为KUMIC的框架，用于生成面向多意图的代码注释。该框架结合上下文学习与思维链（CoT）机制，通过构建代码到意图再到注释的知识链，提升大语言模型在少量示例下生成准确、意图明确注释的能力。实验表明，KUMIC在多个自动评估指标上显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有代码注释生成方法通常只提供通用摘要，无法满足开发者对实现细节或用户对使用说明等多样化意图的需求。尽管大语言模型（LLMs）已被用于多意图注释生成，但在示例数量有限时，难以正确建立意图、代码与注释之间的关联。

Method: KUMIC框架基于上下文学习，首先设计检索机制获取代码-注释一致性高的示例，然后利用思维链（CoT）引导LLM聚焦于与特定意图相关的代码语句，构建从代码到意图再到注释的映射知识链，从而生成意图明确的注释。

Result: 在BLEU、METEOR、ROUGE-L和SBERT四个指标上，KUMIC分别比当前最优基线方法提升了14.49%、22.41%、20.72%和12.94%。

Conclusion: KUMIC通过引入思维链和知识映射机制，有效提升了大语言模型在多意图代码注释生成任务中的性能，验证了其在少量示例场景下的优越性。

Abstract: Code comment generation aims to produce a generic overview of a code snippet,
helping developers understand and maintain code. However, generic summaries
alone are insufficient to meet the diverse needs of practitioners; for example,
developers expect the implementation insights to be presented in an untangled
manner, while users seek clear usage instructions. This highlights the
necessity of multi-intent comment generation. With the widespread adoption of
Large Language Models (LLMs) for code-related tasks, these models have been
leveraged to tackle the challenge of multi-intent comment generation. Despite
their successes, state-of-the-art LLM-based approaches often struggle to
construct correct relationships among intents, code, and comments within a
smaller number of demonstration examples. To mitigate this issue, we propose a
framework named KUMIC for multi-intent comment generation. Built upon
in-context learning, KUMIC leverages Chain-of-Thought (CoT) to optimize
knowledge utilization for LLMs to generate intent-specific comments.
Specifically, KUMIC first designs a retrieval mechanism to obtain similar
demonstration examples, which exhibit high code-comment consistency. Then,
KUMIC leverages CoT to guide LLMs to focus on statements facilitating the
derivation of code comments aligned with specific intents. In this context,
KUMIC constructs a mapping knowledge chain, linking code to intent-specific
statements to comments, which enables LLMs to follow similar reasoning steps
when generating the desired comments. We conduct extensive experiments to
evaluate KUMIC, and the results demonstrate that KUMIC outperforms
state-of-the-art baselines by 14.49\%, 22.41\%, 20.72\%, and 12.94\% in terms
of BLEU, METEOR, ROUGE-L, and SBERT, respectively.

</details>


### [17] [TECS/Rust-OE: Optimizing Exclusive Control in Rust-based Component Systems for Embedded Devices](https://arxiv.org/abs/2510.25242)
*Nao Yoshimura,Hiroshi Oyama,Takuya Azumi*

Main category: cs.SE

TL;DR: 本文提出了一种名为TECS/Rust-OE的内存安全组件化开发框架，通过利用调用流和实时操作系统排他控制机制，在保证代码可重用性的同时优化了性能。


<details>
  <summary>Details</summary>
Motivation: 随着物联网的发展，嵌入式系统变得更大更复杂，对系统可靠性（尤其是安全性）的要求提高，需要选择合适的编程语言。现有框架TECS/Rust因过度使用排他控制导致性能下降。

Method: 提出TECS/Rust-OE框架，利用调用流和实时操作系统的排他控制机制，自动生成基于组件描述的Rust代码，在确保内存安全和可重用性的同时优化性能。

Result: 评估表明，该方法减少了因排他控制带来的开销，并实现了生成代码的高可重用性。

Conclusion: TECS/Rust-OE在不牺牲可重用性的前提下有效提升了系统性能，为复杂嵌入式系统提供了一种高效可靠的开发方案。

Abstract: The diversification of functionalities and the development of the IoT are
making embedded systems larger and more complex in structure. Ensuring system
reliability, especially in terms of security, necessitates selecting an
appropriate programming language. As part of existing research, TECS/Rust has
been proposed as a framework that combines Rust and component-based development
(CBD) to enable scalable system design and enhanced reliability. This framework
represents system structures using static mutable variables, but excessive
exclusive controls applied to ensure thread safety have led to performance
degradation. This paper proposes TECS/Rust-OE, a memory-safe CBD framework
utilizing call flows to address these limitations. The proposed Rust code
leverages real-time OS exclusive control mechanisms, optimizing performance
without compromising reusability. Rust code is automatically generated based on
component descriptions. Evaluations demonstrate reduced overhead due to
optimized exclusion control and high reusability of the generated code.

</details>


### [18] [TECS/Rust: Memory-safe Component Framework for Embedded Systems](https://arxiv.org/abs/2510.25270)
*Nao Yoshimura,Hiroshi Oyama,Takuya Azumi*

Main category: cs.SE

TL;DR: 本文提出了一种基于Rust的嵌入式组件框架TECS/Rust，利用Rust的内存安全特性解决C语言在组件化开发中的内存问题，在保证灵活性和实时系统集成能力的同时引入极低的运行开销。


<details>
  <summary>Details</summary>
Motivation: 随着嵌入式系统功能日益复杂，基于组件的开发（CBD）成为提升架构清晰度和功能复用的有效手段。然而，CBD常使用C语言，易引发内存安全问题。为解决这一问题，作者提出采用Rust语言构建更安全的组件框架。

Method: 设计并实现了一个名为TECS/Rust的Rust框架，用于支持嵌入式系统的组件开发。该框架利用Rust的编译时内存安全机制（如生命周期和借用检查），并提供组件代码自动生成、与实时操作系统高效集成等功能。

Result: 评估表明，该框架生成的代码占实际代码的很大比例，且与非框架开发的代码相比，执行时间差异极小，说明其运行时开销可忽略不计。

Conclusion: TECS/Rust在保障嵌入式系统组件开发中内存安全的同时，保持了CBD的灵活性和高效性，是一种可行且低开销的替代方案。

Abstract: As embedded systems grow in complexity and scale due to increased functional
diversity, component-based development (CBD) emerges as a solution to
streamline their architecture and enhance functionality reuse. CBD typically
utilizes the C programming language for its direct hardware access and
low-level operations, despite its susceptibility to memory-related issues. To
address these concerns, this paper proposes TECS/Rust, a Rust-based framework
specifically designed for TECS, which is a component framework for embedded
systems. It leverages Rust's compile-time memory-safe features, such as
lifetime and borrowing, to mitigate memory vulnerabilities common with C. The
proposed framework not only ensures memory safety but also maintains the
flexibility of CBD, automates Rust code generation for CBD components, and
supports efficient integration with real-time operating systems. An evaluation
of the amount of generated code indicates that the code generated by this paper
framework accounts for a large percentage of the actual code. Compared to code
developed without the proposed framework, the difference in execution time is
minimal, indicating that the overhead introduced by the proposed framework is
negligible.

</details>


### [19] [Dissect-and-Restore: AI-based Code Verification with Transient Refactoring](https://arxiv.org/abs/2510.25406)
*Changjie Wang,Mariano Scazzariello,Anoud Alshnaka,Roberto Guanciale,Dejan Kostić,Marco Chiesa*

Main category: cs.SE

TL;DR: Prometheus 是一个结合 AI 与模块化软件工程原则的新型辅助验证系统，通过将复杂程序逻辑分解为可验证组件并支持自然语言引导，显著提升了形式化验证的自动化效率和成功率。


<details>
  <summary>Details</summary>
Motivation: 形式化验证虽对构建可靠软件至关重要，但因需要专业知识编写规范、处理复杂证明义务而成本高昂；现有 AI 虽能理解数学证明与自然语言，但尚未有效融入验证流程。

Method: Prometheus 采用分解-重组策略：将复杂程序（如嵌套循环）拆解为小的可验证组件，利用结构化分解将复杂引理拆分为子引理，并在必要时接受用户提供的轻量级自然语言指导以引导证明搜索。

Result: 在评估中，Prometheus 在自建数据集上验证成功率达 86%（基线为 68%）；在高复杂度规范下，成功率从 30% 提升至 69%；结合复杂程序的证明大纲时，成功率从 25% 提升至 87%。

Conclusion: 结合模块化重构与 AI 辅助的形式化验证方法能显著提升验证效率与可扩展性，尤其在处理复杂规范和程序结构时效果突出。

Abstract: Formal verification is increasingly recognized as a critical foundation for
building reliable software systems. However, the need for specialized expertise
to write precise specifications, navigate complex proof obligations, and learn
annotations often makes verification an order of magnitude more expensive than
implementation. While modern AI systems can recognize patterns in mathematical
proofs and interpret natural language, effectively integrating them into the
formal verification process remains an open challenge. We present Prometheus, a
novel AI-assisted system that facilitates automated code verification with
current AI capabilities in conjunction with modular software engineering
principles (e.g., modular refactoring). Our approach begins by decomposing
complex program logic, such as nested loops, into smaller, verifiable
components. Once verified, these components are recomposed to construct a proof
of the original program. This decomposition-recomposition workflow is
non-trivial. Prometheus addresses this by guiding the proof search through
structured decomposition of complex lemmas into smaller, verifiable sub-lemmas.
When automated tools are insufficient, users can provide lightweight natural
language guidance to steer the proof process effectively. Our evaluation
demonstrates that transiently applying modular restructuring to the code
substantially improves the AI's effectiveness in verifying individual
components. This approach successfully verifies 86% of tasks in our curated
dataset, compared to 68% for the baseline. Gains are more pronounced with
increasing specification complexity, improving from 30% to 69%, and when
integrating proof outlines for complex programs, from 25% to 87%.

</details>


### [20] [What Challenges Do Developers Face in AI Agent Systems? An Empirical Study on Stack Overflow](https://arxiv.org/abs/2510.25423)
*Ali Asgari,Annibale Panichella,Pouria Derakhshanfar,Mitchell Olsthoorn*

Main category: cs.SE

TL;DR: 该论文通过分析 Stack Overflow 上开发者关于 AI Agent 的讨论，识别并分类了 77 个技术挑战，涵盖运行时集成、依赖管理、编排复杂性和评估可靠性等方面，并量化了问题的流行度与难度，为实践者、研究者和教育者提供具体指导。


<details>
  <summary>Details</summary>
Motivation: 尽管 AI Agent 在研究和工业界迅速流行，但开发者在构建、部署和维护这些系统时仍面临许多尚未被充分探索的挑战。为系统识别这些问题，作者开展了本研究。

Method: 作者从 Stack Overflow 平台收集开发者讨论，通过标签扩展与过滤构建挑战分类体系，使用 LDA-MALLET 进行主题建模，并人工验证和标注主题。此外，还量化了主题的流行度与难度，分析所用工具和编程语言，并追踪 2021 至 2025 年间的发展趋势。

Result: 研究识别出七大类共 77 个技术挑战，主要涉及运行时集成、依赖管理、编排复杂性和评估可靠性；同时揭示了最常见和最难解决的问题，并描绘了 AI Agent 开发中工具与语言的使用演变。

Conclusion: 研究结果为提升 AI Agent 的可靠性及改善开发者支持提供了具体建议，对实践者、研究人员和教育工作者具有指导意义。

Abstract: AI agents have rapidly gained popularity across research and industry as
systems that extend large language models with additional capabilities to plan,
use tools, remember, and act toward specific goals. Yet despite their promise,
developers face persistent and often underexplored challenges when building,
deploying, and maintaining these emerging systems. To identify these
challenges, we study developer discussions on Stack Overflow, the world's
largest developer-focused Q and A platform with about 60 million questions and
answers and 30 million users. We construct a taxonomy of developer challenges
through tag expansion and filtering, apply LDA-MALLET for topic modeling, and
manually validate and label the resulting themes. Our analysis reveals seven
major areas of recurring issues encompassing 77 distinct technical challenges
related to runtime integration, dependency management, orchestration
complexity, and evaluation reliability. We further quantify topic popularity
and difficulty to identify which issues are most common and hardest to resolve,
map the tools and programming languages used in agent development, and track
their evolution from 2021 to 2025 in relation to major AI model and framework
releases. Finally, we present the implications of our results, offering
concrete guidance for practitioners, researchers, and educators on agent
reliability and developer support.

</details>


### [21] [Reflections on the Reproducibility of Commercial LLM Performance in Empirical Software Engineering Studies](https://arxiv.org/abs/2510.25506)
*Florian Angermeir,Maximilian Amougou,Mark Kreitz,Andreas Bauer,Matthias Linhuber,Davide Fucci,Fabiola Moyón C.,Daniel Mendez,Tony Gorschek*

Main category: cs.SE

TL;DR: 该论文调查了ICSE 2024和ASE 2024会议上发表的86篇以大语言模型（LLM）为中心的研究，尝试复现其中18篇提供研究产物并使用OpenAI模型的论文，结果发现仅有5篇适合复现，且无一能完全复现结果，揭示了当前LLM研究在可复现性方面的严重问题。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型（LLM）在学术界和工业界的广泛应用，大量研究依赖LLM进行实验，但其可复现性面临挑战。作者旨在评估当前LLM相关研究的可复现程度，识别阻碍复现的因素，并提出改进建议。

Method: 作者系统分析了ICSE 2024和ASE 2024会议中86篇LLM相关论文，筛选出18篇提供研究产物且使用OpenAI模型的论文，并尝试复现实验结果，评估其可复现性。

Result: 在尝试复现的18项研究中，仅有5项适合复现；其中无一能完全复现，2项部分可复现，3项不可复现。

Conclusion: 当前LLM相关研究普遍存在可复现性不足的问题，亟需更严格的研究产物评审机制和更稳健的实验设计，以提升未来研究的可复现价值。

Abstract: Large Language Models have gained remarkable interest in industry and
academia. The increasing interest in LLMs in academia is also reflected in the
number of publications on this topic over the last years. For instance, alone
78 of the around 425 publications at ICSE 2024 performed experiments with LLMs.
Conducting empirical studies with LLMs remains challenging and raises questions
on how to achieve reproducible results, for both other researchers and
practitioners. One important step towards excelling in empirical research on
LLMs and their application is to first understand to what extent current
research results are eventually reproducible and what factors may impede
reproducibility. This investigation is within the scope of our work. We
contribute an analysis of the reproducibility of LLM-centric studies, provide
insights into the factors impeding reproducibility, and discuss suggestions on
how to improve the current state. In particular, we studied the 86 articles
describing LLM-centric studies, published at ICSE 2024 and ASE 2024. Of the 86
articles, 18 provided research artefacts and used OpenAI models. We attempted
to replicate those 18 studies. Of the 18 studies, only five were fit for
reproduction. For none of the five studies, we were able to fully reproduce the
results. Two studies seemed to be partially reproducible, and three studies did
not seem to be reproducible. Our results highlight not only the need for
stricter research artefact evaluations but also for more robust study designs
to ensure the reproducible value of future publications.

</details>


### [22] [Fuzz Smarter, Not Harder: Towards Greener Fuzzing with GreenAFL](https://arxiv.org/abs/2510.25665)
*Ayse Irmak Ercevik,Aidan Dakhama,Melane Navaratnarajah,Yazhuo Cao,Leo Fernandes*

Main category: cs.SE

TL;DR: GreenAFL 是一种兼顾覆盖率与能耗的灰盒模糊测试框架，通过引入能量感知语料库精简和能量引导启发式策略，在降低碳足迹的同时保持甚至提升测试覆盖率。


<details>
  <summary>Details</summary>
Motivation: 传统灰盒模糊测试（如 AFL++）仅关注覆盖率最大化，忽视了执行路径探索过程中的能耗问题，导致持续模糊测试带来高计算资源消耗和显著碳足迹。

Method: GreenAFL 在传统模糊测试流程中引入两项改进：1）在初始语料库精简阶段考虑功耗；2）在变异过程中采用能量引导启发式策略，优先选择高覆盖率、低能耗的输入。

Result: 消融实验表明，只要应用 GreenAFL 的任一改进组件，即可同时实现更高覆盖率和更低能耗。

Conclusion: 将能耗纳入模糊测试启发式设计是可行且有效的，GreenAFL 能在维持甚至提升测试效果的同时显著减少环境影响。

Abstract: Fuzzing has become a key search-based technique for software testing, but
continuous fuzzing campaigns consume substantial computational resources and
generate significant carbon footprints. Existing grey-box fuzzing approaches
like AFL++ focus primarily on coverage maximisation, without considering the
energy costs of exploring different execution paths. This paper presents
GreenAFL, an energy-aware framework that incorporates power consumption into
the fuzzing heuristics to reduce the environmental impact of automated testing
whilst maintaining coverage. GreenAFL introduces two key modifications to
traditional fuzzing workflows: energy-aware corpus minimisation considering
power consumption when reducing initial corpora, and energy-guided heuristics
that direct mutation towards high-coverage, low-energy inputs. We conduct an
ablation study comparing vanilla AFL++, energy-based corpus minimisation, and
energy-based heuristics to evaluate the individual contributions of each
component. Results show that highest coverage, and lowest energy usage is
achieved whenever at least one of our modifications is used.

</details>


### [23] [A Configuration-First Framework for Reproducible, Low-Code Localization](https://arxiv.org/abs/2510.25692)
*Tim Strnad,Blaž Bertalanič,Carolina Fortuna*

Main category: cs.SE

TL;DR: 本文提出了LOCALIZE框架，一个面向无线定位的低代码、以配置为中心的机器学习实验平台，旨在提升实验的可复现性、易用性与可扩展性。


<details>
  <summary>Details</summary>
Motivation: 当前无线定位领域的机器学习研究缺乏同时满足低编码负担、默认可复现性和内置可扩展性的实验框架，导致实验难以标准化和复现。

Method: 作者设计并实现了LOCALIZE框架，通过人类可读的配置文件声明实验，由工作流编排器执行从数据准备到报告生成的标准化流程，并对所有产物（数据、模型、指标等）进行版本管理；框架提供清晰的扩展点，支持灵活添加新组件。

Result: 与传统Jupyter Notebook基线相比，LOCALIZE显著降低了实验编写工作量，同时保持相近的运行时间和内存开销；在蓝牙低功耗数据集上验证了其在训练数据规模扩大时编排开销可控。

Conclusion: LOCALIZE框架使基于机器学习的无线定位实验变得实用、易用且可扩展，有效弥合了现有工具在可复现性、易用性和灵活性之间的差距。

Abstract: Machine learning is increasingly permeating radio-based localization
services. To keep results credible and comparable, everyday workflows should
make rigorous experiment specification and exact repeatability the default,
without blocking advanced experimentation. However, in practice, researchers
face a three-way gap that could be filled by a framework that offers (i) low
coding effort for end-to-end studies, (ii) reproducibility by default including
versioned code, data, and configurations, controlled randomness, isolated runs,
and recorded artifacts, and (iii) built-in extensibility so new models,
metrics, and stages can be added with minimal integration effort. Existing
tools rarely deliver all three for machine learning in general and localization
workflows in particular. In this paper we introduce LOCALIZE, a low-code,
configuration-first framework for radio localization in which experiments are
declared in human-readable configuration, a workflow orchestrator runs
standardized pipelines from data preparation to reporting, and all artifacts,
such as datasets, models, metrics, and reports, are versioned. The
preconfigured, versioned datasets reduce initial setup and boilerplate,
speeding up model development and evaluation. The design, with clear extension
points, allows experts to add components without reworking the infrastructure.
In a qualitative comparison and a head-to-head study against a plain Jupyter
notebook baseline, we show that the framework reduces authoring effort while
maintaining comparable runtime and memory behavior. Furthermore, using a
Bluetooth Low Energy dataset, we show that scaling across training data (1x to
10x) keeps orchestration overheads bounded as data grows. Overall, the
framework makes reproducible machine-learning-based localization
experimentation practical, accessible, and extensible.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [24] [From Narrative to Action: A Hierarchical LLM-Agent Framework for Human Mobility Generation](https://arxiv.org/abs/2510.24802)
*Qiumeng Li,Chunhou Ji,Xinyue Liu*

Main category: cs.MA

TL;DR: 本文提出了一种名为“Narrative-to-Action”的分层大语言模型智能体框架，通过整合高层叙事推理、中层反思规划与底层行为执行，生成既符合真实移动模式又具备可解释性的人类移动轨迹。


<details>
  <summary>Details</summary>
Motivation: 传统基于智能体或深度学习的模型虽能复现人类移动的统计模式，但难以捕捉行为背后的语义一致性和因果逻辑；而大语言模型在创造性推理与结构约束之间难以平衡。因此，亟需一种能融合认知层次与行为生成的新方法。

Method: 提出分层LLM智能体框架：高层由“创意写手”生成富含动机与上下文的日记式叙事，中层由“结构解析器”将其转化为机器可读计划，底层通过与环境仿真交互执行具体行动（如地点、交通方式选择），并引入职业感知指标“职业移动熵（MEO）”以反映不同职业人群的日程灵活性差异。

Result: 该框架生成的合成轨迹不仅高度贴近真实世界移动模式，还提供了对人类决策逻辑的可解释表征，实现了从数据驱动到认知驱动的移动行为模拟。

Conclusion: 本研究通过分层LLM智能体框架，为理解、预测和合成复杂城市移动行为提供了一条可扩展的认知驱动路径，推动了合成移动建模范式的转变。

Abstract: Understanding and replicating human mobility requires not only
spatial-temporal accuracy but also an awareness of the cognitive hierarchy
underlying real-world travel decisions. Traditional agent-based or deep
learning models can reproduce statistical patterns of movement but fail to
capture the semantic coherence and causal logic of human behavior. Large
language models (LLMs) show potential, but struggle to balance creative
reasoning with strict structural compliance. This study proposes a Hierarchical
LLM-Agent Framework, termed Narrative-to-Action, that integrates high-level
narrative reasoning, mid-level reflective planning, and low-level behavioral
execution within a unified cognitive hierarchy. At the macro level, one agent
is employed as a "creative writer" to produce diary-style narratives rich in
motivation and context, then uses another agent as a "structural parser" to
convert narratives into machine-readable plans. A dynamic execution module
further grounds agents in geographic environments and enables adaptive
behavioral adjustments guided by a novel occupation-aware metric, Mobility
Entropy by Occupation (MEO), which captures heterogeneous schedule flexibility
across different occupational personalities. At the micro level, the agent
executes concrete actions-selecting locations, transportation modes, and time
intervals-through interaction with an environmental simulation. By embedding
this multi-layer cognitive process, the framework produces not only synthetic
trajectories that align closely with real-world patterns but also interpretable
representations of human decision logic. This research advances synthetic
mobility generation from a data-driven paradigm to a cognition-driven
simulation, providing a scalable pathway for understanding, predicting, and
synthesizing complex urban mobility behaviors through hierarchical LLM agents.

</details>


### [25] [Collaborative Scheduling of Time-dependent UAVs,Vehicles and Workers for Crowdsensing in Disaster Response](https://arxiv.org/abs/2510.25212)
*Lei Han,Jinhao Zhang,Jinhui Liu,Zhiyong Yu,Liang Wang,Quan Wang,Zhiwen Yu*

Main category: cs.MA

TL;DR: 本文提出了一种异构多智能体在线协同调度算法HoCs-MPQ，用于高效收集灾后环境信息。该算法通过构建加权无向图建模多元素间的协作与冲突关系，并基于多优先级队列迭代求解最大权重独立集，实现在3秒内完成单次在线调度决策，在任务完成率上显著优于多种基线方法。


<details>
  <summary>Details</summary>
Motivation: 现有灾后环境信息采集技术（如移动众包感知）在极端复杂的灾后环境中存在环境适应性弱、专业感知能力不足和实用性差等问题，亟需更高效的协同感知调度方案。

Method: HoCs-MPQ算法首先基于多元素间的协作关系构建加权无向图节点并量化权重，再根据节点间冲突关系建模图结构；然后结合迭代局部搜索与多优先级队列加速求解最大权重独立集，实现对时变无人机、车辆和工作人员的协同感知调度。

Result: 实验表明，相比HoCs-GREEDY、HoCs-K-WTA、HoCs-MADL和HoCs-MARL等基线方法，HoCs-MPQ平均任务完成率分别提升54.13%、23.82%、14.12%和12.89%，且单次在线调度决策计算时间不超过3秒。

Conclusion: HoCs-MPQ能有效提升灾后环境信息采集效率与任务完成率，具备良好的实时性和实用性，为复杂灾后场景下的多智能体协同感知提供了可行解决方案。

Abstract: Frequent natural disasters cause significant losses to human society, and
timely, efficient collection of post-disaster environmental information is the
foundation for effective rescue operations. Due to the extreme complexity of
post-disaster environments, existing sensing technologies such as mobile
crowdsensing suffer from weak environmental adaptability, insufficient
professional sensing capabilities, and poor practicality of sensing solutions.
Therefore, this paper explores a heterogeneous multi-agent online collaborative
scheduling algorithm, HoCs-MPQ, to achieve efficient collection of
post-disaster environmental information. HoCs-MPQ models collaboration and
conflict relationships among multiple elements through weighted undirected
graph construction, and iteratively solves the maximum weight independent set
based on multi-priority queues, ultimately achieving collaborative sensing
scheduling of time-dependent UA Vs, vehicles, and workers. Specifically, (1)
HoCs-MPQ constructs weighted undirected graph nodes based on collaborative
relationships among multiple elements and quantifies their weights, then models
the weighted undirected graph based on conflict relationships between nodes;
(2) HoCs-MPQ solves the maximum weight independent set based on iterated local
search, and accelerates the solution process using multi-priority queues.
Finally, we conducted detailed experiments based on extensive real-world and
simulated data. The experiments show that, compared to baseline methods (e.g.,
HoCs-GREEDY, HoCs-K-WTA, HoCs-MADL, and HoCs-MARL), HoCs-MPQ improves task
completion rates by an average of 54.13%, 23.82%, 14.12%, and 12.89%
respectively, with computation time for single online autonomous scheduling
decisions not exceeding 3 seconds.

</details>


### [26] [Multi-party Agent Relation Sampling for Multi-party Ad Hoc Teamwork](https://arxiv.org/abs/2510.25340)
*Beiwen Zhang,Yongheng Liang,Hejun Wu*

Main category: cs.MA

TL;DR: 本文提出多边临时团队协作（MAHT）问题，并引入MARs方法，通过构建稀疏骨架图和关系建模来捕捉跨组动态，在MPE和StarCraft II实验中优于现有MARL与AHT方法且收敛更快。


<details>
  <summary>Details</summary>
Motivation: 现有合作多智能体强化学习（MARL）通常假设团队固定且完全可控，而临时团队协作（AHT）虽允许与未知伙伴协作，但仍假设共享惯例；本文旨在解决更复杂的场景——可控智能体需与多个互不熟悉的非可控队友群体协作。

Method: 提出MARs方法，通过构建稀疏骨架图并应用关系建模来捕捉不同非可控群体之间的交互动态。

Result: 在MPE和StarCraft II环境中的实验表明，MARs在性能上优于MARL和AHT基线方法，并且收敛速度更快。

Conclusion: MARs能有效应对多边临时团队协作挑战，为多智能体系统在更开放、动态环境中的协作提供了新思路。

Abstract: Multi-agent reinforcement learning (MARl) has achieved strong results in
cooperative tasks but typically assumes fixed, fully controlled teams. Ad hoc
teamwork (AHT) relaxes this by allowing collaboration with unknown partners,
yet existing variants still presume shared conventions. We introduce
Multil-party Ad Hoc Teamwork (MAHT), where controlled agents must coordinate
with multiple mutually unfamiliar groups of uncontrolled teammates. To address
this, we propose MARs, which builds a sparse skeleton graph and applies
relational modeling to capture cross-group dvnamics. Experiments on MPE and
starCralt ll show that MARs outperforms MARL and AHT baselines while converging
faster.

</details>
