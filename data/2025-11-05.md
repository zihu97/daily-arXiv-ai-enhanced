<div id=toc></div>

# Table of Contents

- [cs.OS](#cs.OS) [Total: 1]
- [cs.AR](#cs.AR) [Total: 6]
- [cs.SE](#cs.SE) [Total: 12]
- [cs.NI](#cs.NI) [Total: 3]
- [cs.DC](#cs.DC) [Total: 13]
- [cs.MA](#cs.MA) [Total: 2]


<div id='cs.OS'></div>

# cs.OS [[Back]](#toc)

### [1] [Continuum: Efficient and Robust Multi-Turn LLM Agent Scheduling with KV Cache Time-to-Live](https://arxiv.org/abs/2511.02230)
*Hanchen Li,Qiuyang Mang,Runyuan He,Qizheng Zhang,Huanzhi Mao,Xiaokun Chen,Alvin Cheung,Joseph Gonzalez,Ion Stoica*

Main category: cs.OS

TL;DR: Continuum 是一种面向多轮智能体工作负载的 LLM 推理服务系统，通过工具调用感知的 KV 缓存超时机制与程序级调度策略，显著减少任务完成时间并提升吞吐量。


<details>
  <summary>Details</summary>
Motivation: 现有 LLM 服务系统在处理包含工具调用的多轮智能体应用时，因工具调用造成的流程中断导致 KV 缓存频繁驱逐和请求间等待时间增加，从而严重影响延迟与效率。

Method: Continuum 结合工具调用持续时间预测，动态为 KV 缓存设置基于轮次的存活时间（TTL），并在程序级别采用先到先服务调度策略，以保持多轮连续性并避免调度空泡。

Result: 在 SWE-Bench 和 BFCL 等真实智能体工作负载上，使用 Llama-3.1 8B/70B 模型的实验表明，Continuum 显著缩短了平均任务完成时间，并在不同硬件配置和 DRAM 卸载方案下均表现优异。

Conclusion: Continuum 通过建模工具调用变异性与智能体程序连续性，有效优化了多轮智能体工作负载的服务性能，优于当前最先进的基线系统。

Abstract: Agentic LLM applications interleave LLM generation requests with tool calls.
These tool calls break the continuity of the workflow by creating pauses
between LLM requests, bringing many challenges for the serving system,
especially under multi-turn scenarios. Each pause potentially causes KV cache
eviction and extra waiting time before entering the continuous batch for the
following LLM request. Since these pauses happen for each call, this problem
becomes increasingly severe as turn number grow for agentic programs. Previous
works either fail to incorporate information from the tool call, evicting KV
cache that leads to repetitive prefill or loading, or ignore the continuity of
a multi-turn program, creating waiting time between turns that increases
per-request latency.
  We present Continuum, a serving system to optimize job completion time for
multi-turn agent workloads by combining tool-aware KV cache timeout with
program-level scheduling. By predicting tool call durations in agentic
workflows, Continuum selectively pins the KV cache in GPU memory with a
time-to-live value based on total turn number. When combined with program-level
first-come-first-serve, Continuum prevents scheduling bubbles, preserves
multi-turn continuity, and optimizes for throughput for complex agentic
workflows. By modeling the variability of tool call and agent program
continuity, Continuum outperforms state-of-the-art baselines. Our evaluation on
real-world agentic workloads (SWE-Bench and BFCL) with Llama-3.1 8B/70B models
shows that Continuum significantly improves the average job completion times,
and remains performant across different hardware setups and DRAM offloading
schemes. Preview code is available at:
https://github.com/Hanchenli/vllm-continuum

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [2] [Optimizing Attention on GPUs by Exploiting GPU Architectural NUMA Effects](https://arxiv.org/abs/2511.02132)
*Mansi Choudhary,Karthik Sangaiah,Sonali Singh,Muhammad Osama,Lisa Wu Wills,Ganesh Dasika*

Main category: cs.AR

TL;DR: 本文提出一种名为“Swizzled Head-first Mapping”的NUMA感知调度策略，通过将多头注意力机制中的注意力头与GPU的NUMA域对齐，显著提升缓存利用率和性能，在AMD MI300X上相比现有方法最高提升50%性能。


<details>
  <summary>Details</summary>
Motivation: 随着AI GPU向多芯粒架构发展，非统一内存访问（NUMA）导致传统假设统一内存访问的GPU调度策略效率下降，尤其在大规模注意力计算中成为性能瓶颈。

Method: 提出一种空间感知的调度策略——Swizzled Head-first Mapping，将多头注意力中的注意力头映射到对应的GPU NUMA域，以利用片内缓存复用。

Result: 在AMD MI300X架构上，该方法相比当前最先进的注意力算法最高实现50%的性能提升，并保持80-97%的高L2缓存命中率。

Conclusion: NUMA感知调度已成为在新一代解聚式GPU上实现高效AI训练与推理的关键技术。

Abstract: The rise of disaggregated AI GPUs has exposed a critical bottleneck in
large-scale attention workloads: non-uniform memory access (NUMA). As
multi-chiplet designs become the norm for scaling compute capabilities, memory
latency and bandwidth vary sharply across compute regions, undermining the
performance of traditional GPU kernel scheduling strategies that assume uniform
memory access. We identify how these NUMA effects distort locality in
multi-head attention (MHA) and present Swizzled Head-first Mapping, a
spatially-aware scheduling strategy that aligns attention heads with GPU NUMA
domains to exploit intra-chiplet cache reuse. On AMD's MI300X architecture, our
method achieves up to 50% higher performance over state-of-the-art attention
algorithms using conventional scheduling techniques and sustains consistently
high L2 cache hit rates of 80-97%. These results demonstrate that NUMA-aware
scheduling is now fundamental to achieving full efficiency on next-generation
disaggregated GPUs, offering a path forward for scalable AI training and
inference.

</details>


### [3] [Energy-Efficient Hardware Acceleration of Whisper ASR on a CGLA](https://arxiv.org/abs/2511.02269)
*Takuto Ando,Yu Eto,Ayumu Takeuchi,Yasuhiko Nakashima*

Main category: cs.AR

TL;DR: 本文首次在CGRA架构IMAX上实现并评估了Whisper模型的核心计算内核，通过软硬件协同设计，在FPGA原型上验证并预测28 nm ASIC性能，结果表明其在能效方面显著优于GPU和CPU，为边缘设备上的可持续自动语音识别提供了新方案。


<details>
  <summary>Details</summary>
Motivation: 生成式AI（如自动语音识别）的兴起带来了严重的能耗问题。专用集成电路（ASIC）虽高效但缺乏可编程性，难以适应算法演进。因此，亟需一种兼顾能效与灵活性的硬件平台。

Method: 在通用粗粒度线性阵列（CGLA）加速器IMAX上实现Whisper核心计算内核，采用软硬件协同设计方法，通过FPGA原型进行评估，并外推至28 nm ASIC工艺下的性能表现。

Result: 预测的ASIC实现相比NVIDIA Jetson AGX Orin能效提升1.90倍，相比RTX 4090提升9.83倍（针对Q8_0模型），展现出卓越的能效优势。

Conclusion: CGLA架构是一种有前景的平台，能够在功耗受限的边缘设备上实现高能效、可持续的自动语音识别。

Abstract: The rise of generative AI for tasks like Automatic Speech Recognition (ASR)
has created a critical energy consumption challenge. While ASICs offer high
efficiency, they lack the programmability to adapt to evolving algorithms. To
address this trade-off, we implement and evaluate Whisper's core computational
kernel on the IMAX, a general-purpose Coarse-Grained Linear Arrays (CGLAs)
accelerator. To our knowledge, this is the first work to execute a Whisper
kernel on a CGRA and compare its performance against CPUs and GPUs. Using
hardware/software co-design, we evaluate our system via an FPGA prototype and
project performance for a 28 nm ASIC. Our results demonstrate superior energy
efficiency. The projected ASIC is 1.90x more energy-efficient than the NVIDIA
Jetson AGX Orin and 9.83x more than an NVIDIA RTX 4090 for the Q8_0 model. This
work positions CGLA as a promising platform for sustainable ASR on
power-constrained edge devices.

</details>


### [4] [VFocus: Better Verilog Generation from Large Language Model via Focused Reasoning](https://arxiv.org/abs/2511.02285)
*Zhuorui Zhao,Bing Li,Grace Li Zhang,Ulf Schlichtmann*

Main category: cs.AR

TL;DR: VFocus 是一个三阶段框架，通过聚焦大语言模型（LLM）在关键决策点上的推理，提升生成 Verilog 代码的功能正确性。


<details>
  <summary>Details</summary>
Motivation: 现有方法在利用 LLM 生成 Verilog 代码时难以确保功能正确性，且未能有效引导模型关注设计中最关键的部分。

Method: VFocus 包含三个阶段：1）预排序阶段，通过提示生成多个候选代码，结合语法重试和密度引导过滤保留高潜力候选；2）排序阶段，使用自动生成的测试平台仿真候选代码，并基于自一致性聚类选出最一致结果；3）后排序优化阶段，对高排名候选进行不一致性挖掘，并调用增强推理的 LLM 提示进行优化。

Result: 在 VerilogEval-Human 基准测试中，VFocus 显著提升了多个推理型 LLM 的 pass@1 正确率。

Conclusion: VFocus 能有效增强 LLM 在复杂硬件设计任务中生成 Verilog 代码的正确性，通过聚焦关键推理点提升整体性能。

Abstract: Large Language Models (LLMs) have shown impressive potential in generating
Verilog codes, but ensuring functional correctness remains a challenge.
Existing approaches often rely on self-consistency or simulation feedback to
select the best candidate, but they miss opportunities to focus LLM reasoning
on the most informative parts of the design. We propose VFocus, a three-stage
framework that enhances Verilog generation by sharpening the focus of LLM
reasoning onto critical decision points in the code generation process. In the
\textbf{pre-ranking stage}, VFocus generates multiple code candidates through
LLM prompting, retries for syntactically valid outputs, and introduces a
\textit{Density-guided Filtering} to retain candidates that fall within the
"reasoning sweet spot" for functional correctness. In the \textbf{ranking
stage}, we simulate each code candidate using an automatically generated
testbench and apply self-consistency-based clustering to identify the most
consistent outputs. Finally, in the \textbf{post-ranking refinement stage},
VFocus performs inconsistency mining on top-ranked candidates and invokes
reasoning-augmented LLM prompts for candidate refinement. Experiments on the
VerilogEval-Human benchmark show that VFocus significantly improves the pass@1
correctness across multiple reasoning LLMs, demonstrating its effectiveness in
enhancing Verilog generation for complex hardware design tasks.

</details>


### [5] [Facial Expression Recognition System Using DNN Accelerator with Multi-threading on FPGA](https://arxiv.org/abs/2511.02408)
*Takuto Ando,Yusuke Inoue*

Main category: cs.AR

TL;DR: 本文在SoC FPGA上利用深度学习处理器单元（DPU）实现了一个多线程的独立面部表情识别系统，通过在同一个通用CNN加速器上运行DenseBox人脸检测和CNN表情识别，提高了FPGA资源利用率和系统吞吐量。


<details>
  <summary>Details</summary>
Motivation: 先前工作因FPGA资源限制而在CPU上运行Haar Cascade人脸检测器，导致对侧脸和光照变化图像检测精度较低；同时若使用专用电路加速器进行两次DNN推理，则需增加新的硬件模块，不够高效。

Method: 采用通用型脉动阵列结构的DPU，在其上同时运行基于DenseBox的人脸检测和基于CNN的表情识别，并开发多线程技术以提升DPU利用率和整体吞吐量。

Result: 系统实现了25 FPS的整体吞吐量，单位功耗吞吐量提升了2.4倍。

Conclusion: 通过在单一DPU上集成两个DNN推理任务并结合多线程调度，该方法在保持小电路规模的同时显著提升了系统效率与性能。

Abstract: In this paper, we implement a stand-alone facial expression recognition
system on an SoC FPGA with multi-threading using a Deep learning Processor Unit
(DPU). The system consists of two steps: one for face detection step and one
for facial expression recognition. In the previous work, the Haar Cascade
detector was run on a CPU in the face detection step due to FPGA resource
limitations, but this detector is less accurate for profile and variable
illumination condition images. Moreover, the previous work used a dedicated
circuit accelerator, so running a second DNN inference for face detection on
the FPGA would require the addition of a new accelerator. As an alternative to
this approach, we run the two inferences by DNN on a DPU, which is a
general-purpose CNN accelerator of the systolic array type. Our method for face
detection using DenseBox and facial expression recognition using CNN on the
same DPU enables the efficient use of FPGA resources while maintaining a small
circuit size. We also developed a multi-threading technique that improves the
overall throughput while increasing the DPU utilization efficiency. With this
approach, we achieved an overall system throughput of 25 FPS and a throughput
per power consumption of 2.4 times.

</details>


### [6] [Digit-Recurrence Posit Division](https://arxiv.org/abs/2511.02494)
*Raul Murillo,Julio Villalba-Moreno,Alberto A. Del Barrio,Guillermo Botella*

Main category: cs.AR

TL;DR: 本文提出了一种基于基数-4数字递归算法的posit除法器，通过硬件优化显著降低了能耗和迭代次数，在小面积开销下实现了高效posit除法运算。


<details>
  <summary>Details</summary>
Motivation: Posit算术虽在精度和动态范围上优于IEEE 754浮点数，但其除法运算因硬件复杂性而面临挑战，亟需高效实现方案。

Method: 采用基数-4数字递归算法，并结合冗余算术、在线商转换和操作数缩放等硬件导向优化技术设计posit除法单元。

Result: 在多种posit配置下，相比现有方法，能耗降低超过80%，迭代次数显著减少，且仅带来较小的面积开销。

Conclusion: 所提出的优化算法有效提升了posit算术单元中除法运算的效率，展示了其在实际硬件实现中的潜力。

Abstract: Posit arithmetic has emerged as a promising alternative to IEEE 754
floating-point representation, offering enhanced accuracy and dynamic range.
However, division operations in posit systems remain challenging due to their
inherent hardware complexity. In this work, we present posit division units
based on the digit-recurrence algorithm, marking the first implementation of
radix-4 digit-recurrence techniques within this context. Our approach
incorporates hardware-centric optimizations including redundant arithmetic,
on-the-fly quotient conversion, and operand scaling to streamline the division
process while mitigating latency, area, and power overheads. Comprehensive
synthesis evaluations across multiple posit configurations demonstrate
significant performance improvements, including more than 80% energy reduction
with small area overhead compared to existing methods, and a substantial
decrease in the number of iterations. These results underscore the potential of
our adapted algorithm to enhance the efficiency of posit-based arithmetic
units.

</details>


### [7] [Implementation and Evaluation of Stable Diffusion on a General-Purpose CGLA Accelerator](https://arxiv.org/abs/2511.02530)
*Takuto Ando,Yu Eto,Yasuhiko Nakashima*

Main category: cs.AR

TL;DR: 本文首次在通用粗粒度可重构阵列（CGRA）加速器IMAX3上实现了stable-diffusion.cpp图像生成框架的核心计算内核，并对其性能进行了深入评估，结果表明IMAX3在ASIC实现下具有良好的性能与能效表现。


<details>
  <summary>Details</summary>
Motivation: 评估通用CGRA架构IMAX3在执行高负载图像生成任务（如Stable Diffusion）时的性能潜力，为未来面向AI的专用加速器设计提供依据。

Method: 在IMAX3的FPGA原型上部署并运行stable-diffusion.cpp的关键计算内核，通过实测建立性能基线，并外推其在ASIC实现下的预期表现。

Result: 尽管IMAX3是通用架构，但在FPGA原型上已展现出良好性能，且在预测的ASIC版本中具有显著的性能和能效优势。

Conclusion: 本研究验证了IMAX3作为多功能计算平台的可行性，为下一代面向多模态AI的专用CGLA加速器设计提供了具体指导和基础。

Abstract: This paper presents the first implementation and in-depth evaluation of the
primary computational kernels from the stable-diffusion.cpp image generation
framework on IMAX3, a general-purpose Coarse-Grained Reconfigurable Array
(CGRA) accelerator. We designed IMAX3 as a versatile computational platform,
and this work assesses its capabilities by executing a demanding image
generation workload. We evaluate its performance on a current
Field-Programmable Gate Array (FPGA) prototype to establish a baseline and
project its potential for a future Application-Specific Integrated Circuit
(ASIC) implementation. Our results demonstrate that, despite its
general-purpose architecture, IMAX3 achieves promising performance and power
efficiency, particularly in its projected ASIC form. This work provides
concrete guidelines for future IMAX architectural designs and establishes a
foundation for developing next-generation, AI-specialized Coarse-Grained Linear
Array (CGLA) accelerators by refining this versatile platform. Ultimately, this
achievement contributes to the realization of energy-efficient, on-device,
multi-modal AI platforms.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [8] [Detecting Vulnerabilities from Issue Reports for Internet-of-Things](https://arxiv.org/abs/2511.01941)
*Sogol Masoumzadeh*

Main category: cs.SE

TL;DR: 本文首次探索了在物联网（IoT）项目中利用机器学习（ML）与大语言模型（LLMs）结合自然语言处理（NLP）技术，从问题报告中识别潜在漏洞。作者提出了两种方法：一是结合ML、LLMs与NLP分析21个Eclipse IoT项目的问题报告；二是基于11,000条GitHub问题微调BERT掩码语言模型进行漏洞分类。实验表明，基于BERT特征训练的SVM模型表现最佳（AUC为0.65），而微调后的BERT准确率仅为0.26，突显训练时数据覆盖的重要性。


<details>
  <summary>Details</summary>
Motivation: 当前针对非IoT系统的漏洞检测已有ML和LLM方法，但IoT系统因分析速度较慢且缺乏相关研究，亟需专门的方法来及时识别其问题报告中的安全漏洞。

Method: 提出两种方法：(1) 结合ML、LLMs与NLP技术分析21个Eclipse IoT项目的问题报告；(2) 在11,000条GitHub问题上微调预训练BERT MLM模型用于漏洞分类。

Result: 基于BERT NLP特征训练的SVM模型取得最佳性能（AUC=0.65）；微调后的BERT模型准确率仅为0.26，表明训练过程中暴露全部数据的重要性。

Conclusion: 本研究首次在IoT领域探索利用ML与LLMs从问题报告中检测漏洞，为未来实现与非IoT系统相当的IoT漏洞识别精度奠定了基础。

Abstract: Timely identification of issue reports reflecting software vulnerabilities is
crucial, particularly for Internet-of-Things (IoT) where analysis is slower
than non-IoT systems. While Machine Learning (ML) and Large Language Models
(LLMs) detect vulnerability-indicating issues in non-IoT systems, their IoT use
remains unexplored. We are the first to tackle this problem by proposing two
approaches: (1) combining ML and LLMs with Natural Language Processing (NLP)
techniques to detect vulnerability-indicating issues of 21 Eclipse IoT projects
and (2) fine-tuning a pre-trained BERT Masked Language Model (MLM) on 11,000
GitHub issues for classifying \vul. Our best performance belongs to a Support
Vector Machine (SVM) trained on BERT NLP features, achieving an Area Under the
receiver operator characteristic Curve (AUC) of 0.65. The fine-tuned BERT
achieves 0.26 accuracy, emphasizing the importance of exposing all data during
training. Our contributions set the stage for accurately detecting IoT
vulnerabilities from issue reports, similar to non-IoT systems.

</details>


### [9] [Metamorphic Testing of Large Language Models for Natural Language Processing](https://arxiv.org/abs/2511.02108)
*Steven Cho,Stefano Ruberto,Valerio Terragni*

Main category: cs.SE

TL;DR: 本文对大语言模型（LLM）的变形测试（MT）进行了迄今为止最全面的研究，收集了191个适用于自然语言处理任务的变形关系（MR），并选取其中36个在三个主流LLM上执行约56万次测试，系统评估了MT在检测LLM错误行为方面的潜力与局限。


<details>
  <summary>Details</summary>
Motivation: 大语言模型虽在NLP任务中表现优异，但常产生错误结果；由于缺乏标注数据作为判断正确性的“预言机”，自动识别这些错误行为面临挑战。变形测试可缓解这一预言机问题，因此有必要系统研究其在LLM中的应用。

Method: 作者通过文献综述收集191个适用于NLP任务的变形关系（MR），从中选取36个代表性MR，在三个流行的大语言模型上执行约560,000次变形测试实验。

Result: 实验揭示了变形测试在检测大语言模型错误行为方面的有效性、适用场景及其当前局限性。

Conclusion: 变形测试是一种有前景且实用的方法，可用于在无标注数据情况下评估和改进大语言模型的可靠性，但其效果受限于所选变形关系的质量与覆盖范围。

Abstract: Using large language models (LLMs) to perform natural language processing
(NLP) tasks has become increasingly pervasive in recent times. The versatile
nature of LLMs makes them applicable to a wide range of such tasks. While the
performance of recent LLMs is generally outstanding, several studies have shown
that they can often produce incorrect results. Automatically identifying these
faulty behaviors is extremely useful for improving the effectiveness of LLMs.
One obstacle to this is the limited availability of labeled datasets, which
necessitates an oracle to determine the correctness of LLM behaviors.
Metamorphic testing (MT) is a popular testing approach that alleviates this
oracle problem. At the core of MT are metamorphic relations (MRs), which define
relationships between the outputs of related inputs. MT can expose faulty
behaviors without the need for explicit oracles (e.g., labeled datasets). This
paper presents the most comprehensive study of MT for LLMs to date. We
conducted a literature review and collected 191 MRs for NLP tasks. We
implemented a representative subset (36 MRs) to conduct a series of experiments
with three popular LLMs, running approximately 560,000 metamorphic tests. The
results shed light on the capabilities and opportunities of MT for LLMs, as
well as its limitations.

</details>


### [10] [Open the Oyster: Empirical Evaluation and Improvement of Code Reasoning Confidence in LLMs](https://arxiv.org/abs/2511.02197)
*Shufan Wang,Xing Hu,Junkai Chen,Zhiyuan Pan,Xin Xia*

Main category: cs.SE

TL;DR: 本文提出了一种面向代码推理任务的大语言模型（LLM）置信度分析与增强框架，通过实证研究评估主流LLM在不同任务中的置信可靠性，并验证了提示策略优化与Platt Scaling等技术的有效性。结果表明，DeepSeek-Reasoner表现最优，而结合重评估提示与Platt Scaling的混合策略提升最显著。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型在代码智能领域的广泛应用，其在代码推理任务中输出的可靠性和可控性日益受到关注，而置信度估计是评估这些特性的有效手段。

Method: 对主流大语言模型在多种代码推理任务中的置信可靠性进行系统性实证研究，并评估提示策略优化和数学校准（如Platt Scaling）等技术对提升置信可靠性的效果。

Result: DeepSeek-Reasoner在ECE、Brier Score和Performance Score三项指标上分别领先最多0.680、0.636和13.652；混合策略相较原始模型最多提升0.541、0.628和15.084。研究表明具备推理能力的模型置信可靠性更优，且混合策略效果最佳。

Conclusion: 当前大语言模型在复杂推理任务中的置信度仍有较大提升空间；本研究为LLM辅助软件工程中的置信机制应用提供了基础和技术参考，并指明了未来优化与工程部署方向。

Abstract: With the widespread application of large language models (LLMs) in the field
of code intelligence, increasing attention has been paid to the reliability and
controllability of their outputs in code reasoning tasks. Confidence estimation
serves as an effective and convenient approach for evaluating these aspects.
This paper proposes a confidence analysis and enhancement framework for LLMs
tailored to code reasoning tasks. We conduct a comprehensive empirical study on
the confidence reliability of mainstream LLMs across different tasks, and
further evaluate the effectiveness of techniques such as prompt strategy
optimisation and mathematical calibration (e.g., Platt Scaling) in improving
confidence reliability. Our results show that DeepSeek-Reasoner achieves the
best performance across various tasks, outperforming other models by up to
$0.680$, $0.636$, and $13.652$ in terms of ECE, Brier Score, and Performance
Score, respectively. The hybrid strategy combining the reassess prompt strategy
and Platt Scaling achieves improvements of up to $0.541$, $0.628$, and $15.084$
over the original performance in the aforementioned three metrics. These
results indicate that models with reasoning capabilities demonstrate superior
confidence reliability, and that the hybrid strategy is the most effective in
enhancing the confidence reliability of various models. Meanwhile, we elucidate
the impact of different task complexities, model scales, and strategies on
confidence performance, and highlight that the confidence of current LLMs in
complex reasoning tasks still has considerable room for improvement. This study
not only provides a research foundation and technical reference for the
application of confidence in LLM-assisted software engineering, but also points
the way for future optimisation and engineering deployment of confidence
mechanisms.

</details>


### [11] [LLMs as Judges: Toward The Automatic Review of GSN-compliant Assurance Cases](https://arxiv.org/abs/2511.02203)
*Gerhard Yu,Mithila Sivakumar,Alvine B. Belle,Soude Ghari,Song Wang,Timothy C. Lethbridge*

Main category: cs.SE

TL;DR: 本文提出一种基于大语言模型（LLM）的自动化方法，用于评审保障案例（assurance cases），通过将评审标准形式化为谓词规则并结合“LLM-as-a-judge”范式，提升评审效率与准确性；实验表明DeepSeek-R1和GPT-4.1表现最佳，但人类专家仍需参与优化评审结果。


<details>
  <summary>Details</summary>
Motivation: 保障案例在关键系统中用于验证安全、可靠性和安全性等非功能性需求，但其文档冗长复杂，导致创建、评审和维护过程易出错且耗时，亟需借助自动化技术（如大语言模型）提高整个生命周期的效率与一致性。

Method: 提出一种基于“LLM-as-a-judge”范式的自动化评审方法，将已有的保障案例评审准则形式化为谓词规则，并据此设计针对性的LLM提示；在多个主流大语言模型（GPT-4o、GPT-4.1、DeepSeek-R1、Gemini 2.0 Flash）上进行实验评估。

Result: 实验结果显示大多数LLM具备较好的评审能力，其中DeepSeek-R1和GPT-4.1表现突出，DeepSeek-R1最终优于GPT-4.1；但LLM生成的评审结果仍需人工进一步优化。

Conclusion: 利用大语言模型可有效辅助保障案例的自动化评审，显著提升效率和一致性，但当前技术尚不能完全替代人类专家，人机协同仍是保障评审质量的关键。

Abstract: Assurance cases allow verifying the correct implementation of certain
non-functional requirements of mission-critical systems, including their
safety, security, and reliability. They can be used in the specification of
autonomous driving, avionics, air traffic control, and similar systems. They
aim to reduce risks of harm of all kinds including human mortality,
environmental damage, and financial loss. However, assurance cases often tend
to be organized as extensive documents spanning hundreds of pages, making their
creation, review, and maintenance error-prone, time-consuming, and tedious.
Therefore, there is a growing need to leverage (semi-)automated techniques,
such as those powered by generative AI and large language models (LLMs), to
enhance efficiency, consistency, and accuracy across the entire assurance-case
lifecycle. In this paper, we focus on assurance case review, a critical task
that ensures the quality of assurance cases and therefore fosters their
acceptance by regulatory authorities. We propose a novel approach that
leverages the \textit{LLM-as-a-judge} paradigm to automate the review process.
Specifically, we propose new predicate-based rules that formalize
well-established assurance case review criteria, allowing us to craft LLM
prompts tailored to the review task. Our experiments on several
state-of-the-art LLMs (GPT-4o, GPT-4.1, DeepSeek-R1, and Gemini 2.0 Flash) show
that, while most LLMs yield relatively good review capabilities, DeepSeek-R1
and GPT-4.1 demonstrate superior performance, with DeepSeek-R1 ultimately
outperforming GPT-4.1. However, our experimental results also suggest that
human reviewers are still needed to refine the reviews LLMs yield.

</details>


### [12] [EvoDev: An Iterative Feature-Driven Framework for End-to-End Software Development with LLM-based Agents](https://arxiv.org/abs/2511.02399)
*Junwei Liu,Chen Xu,Chong Wang,Tong Bai,Weitong Chen,Kaseng Wong,Yiling Lou,Xin Peng*

Main category: cs.SE

TL;DR: 本文提出了EvoDev，一种受特性驱动开发启发的迭代式软件开发框架，通过构建显式建模特性的依赖关系的Feature Map，并在多层级信息间传播上下文，显著提升了大语言模型在复杂Android项目中的自动化开发性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于大语言模型的自动化软件开发方法多采用线性、瀑布式的流程，无法有效应对真实世界中迭代性强、规模复杂的软件项目开发需求。

Method: EvoDev将用户需求分解为有价值的特性集合，构建一个有向无环图（Feature Map）以显式建模特性间的依赖关系；每个节点维护业务逻辑、设计和代码等多层信息，并沿依赖关系传播上下文，用于指导后续迭代开发。

Result: 在具有挑战性的Android开发任务上，EvoDev相比最佳基线Claude Code性能提升56.8%，并在不同基础大语言模型上将单智能体性能提升16.0%–76.6%。

Conclusion: 研究表明，在复杂软件项目中，依赖建模、上下文传播和工作流感知的智能体设计至关重要；该工作为设计迭代式LLM驱动开发框架提供了实践洞见，并为未来基础大模型的训练指明了方向。

Abstract: Recent advances in large language model agents offer the promise of
automating end-to-end software development from natural language requirements.
However, existing approaches largely adopt linear, waterfall-style pipelines,
which oversimplify the iterative nature of real-world development and struggle
with complex, large-scale projects. To address these limitations, we propose
EvoDev, an iterative software development framework inspired by feature-driven
development. EvoDev decomposes user requirements into a set of user-valued
features and constructs a Feature Map, a directed acyclic graph that explicitly
models dependencies between features. Each node in the feature map maintains
multi-level information, including business logic, design, and code, which is
propagated along dependencies to provide context for subsequent development
iterations. We evaluate EvoDev on challenging Android development tasks and
show that it outperforms the best-performing baseline, Claude Code, by a
substantial margin of 56.8%, while improving single-agent performance by
16.0%-76.6% across different base LLMs, highlighting the importance of
dependency modeling, context propagation, and workflow-aware agent design for
complex software projects. Our work summarizes practical insights for designing
iterative, LLM-driven development frameworks and informs future training of
base LLMs to better support iterative software development.

</details>


### [13] [Who's Who? LLM-assisted Software Traceability with Architecture Entity Recognition](https://arxiv.org/abs/2511.02434)
*Dominik Fuchß,Haoyu Liu,Sophie Corallo,Tobias Hey,Jan Keim,Johannes von Geisau,Anne Koziolek*

Main category: cs.SE

TL;DR: 本文提出两种基于大语言模型（LLM）的方法ExArch和ArTEMiS，用于自动识别软件架构文档和源代码中的架构相关实体，以支持架构模型自动生成和追踪链接恢复，效果媲美或优于现有先进方法。


<details>
  <summary>Details</summary>
Motivation: 手动创建软件架构模型（SAM）耗时且繁琐，而架构文档与源代码之间的语义鸿沟阻碍了追踪链接恢复（TLR）。利用LLM自动提取架构实体可降低人工成本并提升TLR的可行性。

Method: 提出两种LLM驱动的方法：ExArch从架构文档和源代码中提取组件名以自动生成简化SAM；ArTEMiS识别文档中的架构实体并与已有SAM（手动或自动生成）进行匹配。

Result: ExArch在无需人工SAM的情况下达到F1 0.86，接近需人工SAM的TransArC（F1 0.87）；ArTEMiS表现与SWATTR相当（F1 0.81），并与TransArC结合使用效果更佳；ExArch+ArTEMiS组合优于无需人工SAM的最佳基线ArDoCode。

Conclusion: 大语言模型能有效识别文本制品中的架构实体，支持自动化SAM构建与追踪链接恢复，显著提升架构-代码可追溯性的实用性与可及性。

Abstract: Identifying architecturally relevant entities in textual artifacts is crucial
for Traceability Link Recovery (TLR) between Software Architecture
Documentation (SAD) and source code. While Software Architecture Models (SAMs)
can bridge the semantic gap between these artifacts, their manual creation is
time-consuming. Large Language Models (LLMs) offer new capabilities for
extracting architectural entities from SAD and source code to construct SAMs
automatically or establish direct trace links. This paper presents two
LLM-based approaches: ExArch extracts component names as simple SAMs from SAD
and source code to eliminate the need for manual SAM creation, while ArTEMiS
identifies architectural entities in documentation and matches them with
(manually or automatically generated) SAM entities. Our evaluation compares
against state-of-the-art approaches SWATTR, TransArC and ArDoCode. TransArC
achieves strong performance (F1: 0.87) but requires manually created SAMs;
ExArch achieves comparable results (F1: 0.86) using only SAD and code. ArTEMiS
is on par with the traditional heuristic-based SWATTR (F1: 0.81) and can
successfully replace it when integrated with TransArC. The combination of
ArTEMiS and ExArch outperforms ArDoCode, the best baseline without manual SAMs.
Our results demonstrate that LLMs can effectively identify architectural
entities in textual artifacts, enabling automated SAM generation and TLR,
making architecture-code traceability more practical and accessible.

</details>


### [14] [When Continuous Delivery Is Not an Option: Practical Paths to Continuous Engineering in Complex Organizations](https://arxiv.org/abs/2511.02445)
*Eriks Klotins,Magnus Ahlgren,Nicolas Martin Vivaldi,Even-Andre Karlsson*

Main category: cs.SE

TL;DR: 本文通过四个工业案例研究，分析了在复杂产品、遗留系统和组织惯性等约束下持续软件工程（CSE）的实际采纳情况，并据此更新了CSE行业就绪模型，以帮助组织设定更现实的CSE目标。


<details>
  <summary>Details</summary>
Motivation: 全面采用持续软件工程（CSE）常受限于复杂产品、遗留系统、组织惯性和法规要求，因此需要理解这些约束如何影响实际采纳过程，并为实践者提供可行指导。

Method: 应用并扩展已有的CSE行业就绪模型，结合专家访谈与叙事综合方法，对自动化、汽车、零售和化工四个行业的案例进行评估与分析。

Result: 提出了一个更新版的就绪模型，增加了内外部反馈层级，区分了面向市场与组织的约束，并识别出组织准备度、跨组织依赖和客户对持续交付需求有限等关键驱动因素与障碍。

Conclusion: 尽管端到端的CSE全面采纳未必总可行，但在约束条件下仍可实现有意义的内部改进；本研究为面临部分或受限CSE转型的组织提供了实证指导。

Abstract: Purpose: Continuous Software Engineering (CSE) promises improved efficiency,
quality, and responsiveness in software-intensive organizations. However, fully
adopting CSE is often constrained by complex products, legacy systems,
organizational inertia, and regulatory requirements. In this paper, we examine
four industrial cases from the automation, automotive, retail, and chemical
sectors to explore how such constraints shape CSE adoption in practice.
Methods: We apply and extend a previously proposed CSE Industry Readiness Model
to assess the current and potential levels of adoption in each case. Through
expert interviews and narrative synthesis, we identify common driving forces
and adoption barriers, including organizational preparedness,
cross-organizational dependencies, and limited customer demand for continuous
delivery. Results: Based on our findings, we propose an updated readiness model
that introduces additional levels of internal and external feedback,
distinguishes market- and organization-facing constraints, and better guides
practitioners in setting realistic CSE adoption goals. Conclusions: Our results
highlight that while full end-to-end CSE adoption may not always be feasible,
meaningful internal improvements are still possible and beneficial. This study
provides empirically grounded guidance for organizations navigating partial or
constrained CSE transformations.

</details>


### [15] [Lost in Code Generation: Reimagining the Role of Software Models in AI-driven Software Engineering](https://arxiv.org/abs/2511.02475)
*Jürgen Cito,Dominik Bork*

Main category: cs.SE

TL;DR: 生成式AI虽加速了软件开发，但也模糊了原型与工程化软件的界限，导致系统脆弱；文章主张通过事后恢复模型来重建理解、揭示风险并引导优化，使模型成为连接人类意图、AI生成与系统演化的中介。


<details>
  <summary>Details</summary>
Motivation: 生成式AI带来的“氛围编码”虽降低了软件创建门槛，却使原型与工程化软件边界消失，造成系统缺乏鲁棒性、安全性和可维护性，亟需重新思考软件模型的角色。

Method: 提出将软件模型从AI生成的代码中事后恢复，而非仅作为前期蓝图，以此重建对系统的理解、暴露潜在风险并指导后续改进。

Result: 模型可作为人类意图、AI生成结果与系统长期演化之间的中介，为可持续的AI驱动软件工程提供路径。

Conclusion: 通过赋予软件模型新的中介角色，可在保留生成式AI效率的同时，提升AI生成软件的可靠性与可持续性。

Abstract: Generative AI enables rapid ``vibe coding," where natural language prompts
yield working software systems. While this lowers barriers to software
creation, it also collapses the boundary between prototypes and engineered
software, leading to fragile systems that lack robustness, security, and
maintainability. We argue that this shift motivates a reimagining of software
models. Rather than serving only as upfront blueprints, models can be recovered
post-hoc from AI-generated code to restore comprehension, expose risks, and
guide refinement. In this role, models serve as mediators between human intent,
AI generation, and long-term system evolution, providing a path toward
sustainable AI-driven software engineering.

</details>


### [16] [ReleaseEval: A Benchmark for Evaluating Language Models in Automated Release Note Generation](https://arxiv.org/abs/2511.02713)
*Qianru Meng,Zhaochun Ren,Joost Visser*

Main category: cs.SE

TL;DR: 本文提出了ReleaseEval，一个可复现且开源许可的基准数据集，用于系统评估语言模型在自动生成发布说明任务中的表现，涵盖三种不同粒度输入（提交信息、提交树结构和代码差异），发现大语言模型在利用结构化信息方面表现优异，但在处理细粒度代码差异时仍有挑战。


<details>
  <summary>Details</summary>
Motivation: 当前自动发布说明生成面临数据集缺乏明确许可、可复现性差以及任务设计不完整（仅依赖提交信息而忽略提交层级和代码变更等细粒度上下文）等问题。

Method: 构建ReleaseEval基准，包含来自3,369个仓库的94,987条发布说明，覆盖6种编程语言，并定义三种任务设置：commit2sum（基于提交信息）、tree2sum（结合提交树结构）和diff2sum（利用代码差异）。

Result: 自动与人工评估均表明，大语言模型在所有任务上优于传统基线方法，在tree2sum任务中提升显著，但在diff2sum任务上仍表现不佳。

Conclusion: 大语言模型擅长利用结构化信息生成发布说明，但在从长代码差异中抽象关键信息方面仍存在困难，未来需进一步改进对细粒度代码变更的理解能力。

Abstract: Automated release note generation addresses the challenge of documenting
frequent software updates, where manual efforts are time-consuming and prone to
human error. Although recent advances in language models further enhance this
process, progress remains hindered by dataset limitations, including the lack
of explicit licensing and limited reproducibility, and incomplete task design
that relies mainly on commit messages for summarization while overlooking
fine-grained contexts such as commit hierarchies and code changes. To fill this
gap, we introduce ReleaseEval, a reproducible and openly licensed benchmark
designed to systematically evaluate language models for automated release note
generation. ReleaseEval comprises 94,987 release notes from 3,369 repositories
across 6 programming languages, and supports three task settings with three
levels of input granularity: (1) commit2sum, which generates release notes from
commit messages; (2) tree2sum, which incorporates commit tree structures; and
(3) diff2sum, which leverages fine-grained code diffs. Both automated and human
evaluations show that large language models consistently outperform traditional
baselines across all tasks, achieving substantial gains on tree2sum, while
still struggling on diff2sum. These findings highlight LLMs' proficiency in
leveraging structured information while revealing challenges in abstracting
from long code diffs.

</details>


### [17] [Investigating the Experience of Autistic Individuals in Software Engineering](https://arxiv.org/abs/2511.02736)
*Madalena Sasportes,Grischa Liebel,Miguel Goulão*

Main category: cs.SE

TL;DR: 该研究通过访谈和问卷调查，发现自闭症软件工程师在逻辑思维、细节关注、编程中的高度专注等方面具有显著优势，并偏好书面沟通、远程工作及与AI系统互动。


<details>
  <summary>Details</summary>
Motivation: 现有软件工程研究多聚焦于自闭症个体面临的挑战和所需支持，而忽视了其独特优势；本研究旨在深入探索自闭症软件工程师在软件工程活动（如代码审查）中的优势体验。

Method: 结合社会技术扎根理论，对16名自闭症软件工程师进行半结构化访谈，并辅以包含49名受访者（其中5名为自闭症人士）的问卷调查，将结果与Gama等人关于神经多样性认知功能障碍对软件工程表现影响的理论进行对比分析。

Result: 自闭症软件工程师普遍展现出逻辑思维能力强、注重细节、编程时能高度专注等优势；他们乐于学习新编程语言和技术，偏好书面沟通与远程工作，并对与AI系统交互感到高度舒适。

Conclusion: 本研究通过实证进一步证实了自闭症软件工程师的优势，拓展了现有文献对神经多样性在软件工程中积极作用的理解。

Abstract: Context: Autism spectrum disorder (ASD) leads to various issues in the
everyday life of autistic individuals, often resulting in unemployment and
mental health problems. To improve the inclusion of autistic adults, existing
studies have highlighted the strengths these individuals possess in comparison
to non-autistic individuals, e.g., high attention to detail or excellent
logical reasoning skills. If fostered, these strengths could be valuable in
software engineering activities, such for identifying specific kinds of bugs in
code. However, existing work in SE has primarily studied the challenges of
autistic individuals and possible accommodations, with little attention their
strengths. Objective: Our goal is to analyse the experiences of autistic
individuals in software engineering activities, such as code reviews, with a
particular emphasis on strengths. Methods: This study combines Social-Technical
Grounded Theory through semi-structured interviews with 16 autistic software
engineers and a survey with 49 respondents, including 5 autistic participants.
We compare the emerging themes with the theory by Gama et al. on the Effect of
Neurodivergent Cognitive Dysfunctions in Software Engineering Performance.
Results: Our results suggest that autistic software engineers are often skilled
in logical thinking, attention to detail, and hyperfocus in programming; and
they enjoy learning new programming languages and programming-related
technologies. Confirming previous work, they tend to prefer written
communication and remote work. Finally, we report a high comfort level in
interacting with AI-based systems. Conclusions: Our findings extend existing
work by providing further evidence on the strengths of autistic software
engineers.

</details>


### [18] [Formalizing Regression Testing for Agile and Continuous Integration Environments](https://arxiv.org/abs/2511.02810)
*Suddhasvatta Das,Kevin Gary*

Main category: cs.SE

TL;DR: 本文提出了一种用于描述敏捷开发中连续回归测试的形式化模型，将软件构建序列建模为时间有序链，并定义了构建间的回归测试窗口；该模型在特定条件下可退化为经典回归测试场景，并通过表示现有算法验证了其正确性与完备性。


<details>
  <summary>Details</summary>
Motivation: 传统回归测试理论假设软件仅在交付或维护阶段进行一次测试，而现代敏捷开发实践要求对持续发布的版本进行连续回归测试，因此需要一种新的形式化方法来准确刻画这一现象。

Method: 作者将连续回归测试建模为一个时间有序的构建链，每个构建包含程序、需求和对应测试，并引入“回归测试窗口”概念以反映有限的测试时间预算；随后通过将两种先进的敏捷回归测试算法直接表达为构建元组操作，验证该形式化模型的合理性。

Result: 所提出的形式化模型能够准确表示现有敏捷回归测试算法，且在时间预算无限、构建链退化为两个版本时，可还原为经典的“全部重测”策略，证明了模型的语义一致性；同时完成了该形式化体系的soundness与completeness证明。

Conclusion: 该研究为连续回归测试提供了一个严谨且通用的形式化框架，既兼容经典理论，又能有效支持现代敏捷开发环境下的测试实践。

Abstract: Software developed using modern agile practices delivers a stream of software
versions that require continuous regression testing rather than testing once
close to the delivery or maintenance phase, as assumed by classical
regression-testing theory. In this work, we formalize the phenomenon of
continuous or near-continuous regression testing using successive builds as a
time-ordered chain, where each build contains the program, requirements, and
the accompanying tests. We also formalize the regression test window between
any two builds, which captures the limited time budget available for regression
testing. As the time limit is set to infinity and the chain is closed to two
builds, the model degenerates to retest-all, thereby preserving semantics for
the classical two-version case. The formalization is validated by directly
representing two state-of-the-art agile regression testing algorithms in terms
of build-tuple operations without requiring auxiliary assumptions, followed by
proof of the soundness and completeness of our formalization.

</details>


### [19] [From Code Changes to Quality Gains: An Empirical Study in Python ML Systems with PyQu](https://arxiv.org/abs/2511.02827)
*Mohamed Almukhtar,Anwar Ghammam,Marouane Kessentini,Hua Ming*

Main category: cs.SE

TL;DR: 该研究通过大规模实证分析3,340个开源Python机器学习项目，开发了名为PyQu的新工具，用于识别提升软件质量的代码变更，并归纳出61种对质量有直接影响的变更模式，其中41%为此前未被发现的新类型。


<details>
  <summary>Details</summary>
Motivation: 随着生成式AI在代码生成中的广泛应用以及Python机器学习系统的普及，软件质量成为关键问题。现有研究缺乏对代码变更如何具体影响机器学习系统质量的深入理解，且缺少有效的质量评估工具和清晰的映射关系。

Method: 研究对超过3.7百万次提交和2.7万亿行代码进行大规模实证分析，开发了基于底层软件度量的PyQu工具，结合主题分析识别并分类影响质量的代码变更。

Result: PyQu在识别质量提升型提交方面达到平均0.84的准确率、精确率和召回率，F1分数平均为0.85；共识别出61种有效代码变更，归为13类，其中41%为新发现。

Conclusion: 本研究为Python机器学习系统的自动化质量评估、最佳实践制定及工具开发提供了重要基础，填补了代码变更与软件质量关系的认知空白。

Abstract: In an era shaped by Generative Artificial Intelligence for code generation
and the rising adoption of Python-based Machine Learning systems (MLS),
software quality has emerged as a major concern. As these systems grow in
complexity and importance, a key obstacle lies in understanding exactly how
specific code changes affect overall quality-a shortfall aggravated by the lack
of quality assessment tools and a clear mapping between ML systems code changes
and their quality effects. Although prior work has explored code changes in
MLS, it mostly stops at what the changes are, leaving a gap in our knowledge of
the relationship between code changes and the MLS quality. To address this gap,
we conducted a large-scale empirical study of 3,340 open-source Python ML
projects, encompassing more than 3.7 million commits and 2.7 trillion lines of
code. We introduce PyQu, a novel tool that leverages low level software metrics
to identify quality-enhancing commits with an average accuracy, precision, and
recall of 0.84 and 0.85 of average F1 score. Using PyQu and a thematic
analysis, we identified 61 code changes, each demonstrating a direct impact on
enhancing software quality, and we classified them into 13 categories based on
contextual characteristics. 41% of the changes are newly discovered by our
study and have not been identified by state-of-the-art Python changes detection
tools. Our work offers a vital foundation for researchers, practitioners,
educators, and tool developers, advancing the quest for automated quality
assessment and best practices in Python-based ML software.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [20] [Permissioned Blockchain in Advanced Air Mobility: A Performance Analisys for UTM](https://arxiv.org/abs/2511.02171)
*Rodrigo Nunes,André Melo,Rafael Albarello,Reinaldo Gomes,Cesar Marcondes,Lourenço Pereira Jr*

Main category: cs.NI

TL;DR: 本文评估了两种符合航空监管框架的分布式无人机交通管理（UTM）架构——InterUSS平台和基于Hyperledger Fabric的私有账本，发现区块链系统需专门设计以满足航空性能约束。


<details>
  <summary>Details</summary>
Motivation: 随着无人机（UAV）的快速普及，航空管理机构提出了分布式无人机交通管理（UTM）架构；尽管区块链被视为有前景的技术，但UTM作为安全关键且高度监管的领域，必须同时满足性能、安全与合规要求。

Method: 对两种符合现行监管框架的分布式UTM架构（Linux基金会的InterUSS平台和基于Hyperledger Fabric的私有账本）进行基准测试。

Result: 研究发现，基于区块链的系统需要针对航空性能约束进行专门架构设计。

Conclusion: 在安全关键的UTM环境中，仅靠通用区块链技术不足以满足需求，必须结合航空领域的性能和合规要求进行定制化架构设计。

Abstract: The rapid adoption of Uncrewed Aerial Vehicles (UAVs) has driven aviation
authorities to propose distributed Uncrewed Traffic Management (UTM)
architectures. Several studies have advocated blockchain as a promising
technology to meet these requirements. However, since UTM is a safety-critical
and highly regulated domain, compliance with standards and regulatory
frameworks is as crucial as performance and security. This work benchmarks two
distributed architectures aligned with current regulatory frameworks: the Linux
Foundation's InterUSS platform and a Hyperledger Fabric-based private ledger.
Our findings reveal that blockchain-based systems require architectures
specifically designed for aeronautical performance constraints.

</details>


### [21] [Optimizing Multi-UAV 3D Deployment for Energy-Efficient Sensing over Uneven Terrains](https://arxiv.org/abs/2511.02368)
*Rushi Moliya,Dhaval K. Patel,Brijesh Soni,Miguel López-Benítez*

Main category: cs.NI

TL;DR: 本文提出一种结合遗传算法（GA）与粒子群优化（PSO）的分层启发式框架，用于在复杂地形中优化多无人机协同感知系统的探测概率与悬停能耗，在保证地形感知视距（LoS）连通性的同时显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 在不平坦地形中，地形遮挡会破坏无人机与目标之间的视距（LoS）连接，从而降低协同探测性能；现有方法难以同时兼顾探测概率最大化与能量效率，且缺乏高效的LoS评估机制。

Method: 引入二元LoS指示器并采用基于包围体层次结构（BVH）的自适应方案进行高效LoS评估；构建一个双目标优化问题，并设计分层启发式框架：外层使用遗传算法进行全局探索，内层对每架无人机使用粒子群优化进行精细调整，结合基于惩罚的适应度函数确保解满足空间、朝向和安全约束。

Result: 在真实地形数据上的蒙特卡洛仿真表明，相比仅使用PSO的基线方法，所提GA+PSO框架在2架和3架无人机场景下分别将探测概率提升37.02%和36.5%，平均多余悬停能耗降低45.0%和48.9%；相比非优化方案，探测概率提升59.5%和54.2%，多余能耗降低59.8%和65.9%。

Conclusion: 所提出的GA+PSO分层优化框架在少量无人机和复杂地形条件下，能有效平衡探测性能与能耗，显著优于现有方法，验证了其在地形感知协同感知任务中的实用性和有效性。

Abstract: In this work, we consider a multi-unmanned aerial vehicle (UAV) cooperative
sensing system where UAVs are deployed to sense multiple targets in
terrain-aware line of sight (LoS) conditions in uneven terrain equipped with
directional antennas. To mitigate terrain-induced LoS blockages that degrade
detection performance, we incorporate a binary LoS indicator and propose a
bounding volume hierarchy (BHV)-based adaptive scheme for efficient LoS
evaluation. We formulate a bi-objective problem that maximizes the probability
of cooperative detection with minimal hover energy constraints governing
spatial, orientational, and safety constraints. To address the problem, which
is inherently non-convex, we propose a hierarchical heuristic framework that
combines exploration through a genetic algorithm (GA) with per-UAV refinement
via particle swarm optimization (PSO), where a penalty-based fitness evaluation
guides solutions toward feasibility, bounded within constraints. The proposed
methodology is an effective trade-off method of traversing through a complex
search space and maintaining terrain-aware LoS connectivity and energy aware
deployment. Monte Carlo simulations on real-world terrain data show that the
proposed GA+PSO framework improves detection probability by 37.02% and 36.5%
for 2 and 3 UAVs, respectively, while reducing average excess hover energy by
45.0% and 48.9% compared to the PSO-only baseline. Relative to the
non-optimized scheme, it further achieves 59.5% and 54.2% higher detection
probability with 59.8% and 65.9% lower excess hover energy, thereby showing its
effectiveness with a small number of UAVs over uneven terrain.

</details>


### [22] [Janus: Leveraging Incremental Computation for Efficient DNS Verification](https://arxiv.org/abs/2511.02559)
*Yao Wang,Kexin Yu,Wenyun Xu,Kaiqiang Hu,Ziyi Wang,Lizhao You,Qiang Su,Dong Guo,Haizhou Du,Wanjian Feng,Qingyu Song,Linghe Kong,Qiao Xiang,Jiwu Shu*

Main category: cs.NI

TL;DR: Janus 是一种新型 DNS 配置验证工具，通过将域名服务器查询处理建模为匹配-动作表操作，实现了高效、支持增量验证的 DNS 配置检查，在真实数据集上显著提升了验证速度并减少了计算开销。


<details>
  <summary>Details</summary>
Motivation: 现有 DNS 配置验证工具效率低下且不支持增量验证，难以应对大规模部署中的配置错误问题。

Method: Janus 将名称服务器处理查询的过程转化为匹配-动作表上的匹配过程，采用基于行为划分查询空间的数据结构、符号执行算法以覆盖所有可能查询，并设计了支持增量验证的机制。

Result: 在包含超过 600 万条资源记录的真实数据集上，Janus 实现了最高 255.7 倍的速度提升和最多 6046 倍的 LEC 数量减少。

Conclusion: Janus 有效解决了 DNS 配置验证中的效率与可扩展性问题，为大规模 DNS 系统提供了实用且高效的验证方案。

Abstract: Existing DNS configuration verification tools face significant issues (e.g.,
inefficient and lacking support for incremental verification). Inspired by the
advancements in recent work of distributed data plane verification and the
resemblance be- tween the data plane and DNS configuration, we tackle the
challenge of DNS misconfiguration by introducing Janus, a DNS verification
tool. Our key insight is that the process of a nameserver handling queries can
be transformed into a matching process on a match-action table. With this
insight, Janus consists of (1) an efficient data structure for partition query
space based on the behaviors, (2) a symbolic execution algorithm that specifies
how a single nameserver can efficiently cover all possible queries and ensure
the accuracy of verification, (3) a mechanism to support incremental
verification with less computational effort. Extensive experiments on
real-world datasets (with over 6 million resource records) show that Janus
achieves significant speedups, with peak improvements of up to 255.7x and a
maximum 6046x reduction in the number of LECs.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [23] [A Taxonomy of Schedulers -- Operating Systems, Clusters and Big Data Frameworks](https://arxiv.org/abs/2511.01860)
*Leszek Sliwko*

Main category: cs.DC

TL;DR: 本文综述了当前部署并广泛使用的负载调度器，提出了一种基于架构与设计的分层分类法，并重点分析了影响系统吞吐量与可扩展性的关键设计因素及架构改进，特别关注了Google的Borg系统。


<details>
  <summary>Details</summary>
Motivation: 现有调度系统种类繁多，缺乏聚焦于吞吐量与可扩展性关键设计因素的系统性分类，因此需要一种新的分类方法来深入理解这些系统的架构演进。

Method: 通过分析已部署和实际使用的负载调度器，构建一个基于架构和设计的层次化分类体系，并聚焦于影响性能的关键设计要素和增量式架构改进。

Result: 提出了一个新的调度系统分类法，并对包括Google Borg在内的先进调度系统进行了深入剖析，揭示了提升吞吐量与可扩展性的设计策略。

Conclusion: 该综述为理解和设计高性能、可扩展的负载调度系统提供了结构化视角，强调了关键架构选择对系统性能的重要影响。

Abstract: This review analyzes deployed and actively used workload schedulers'
solutions and presents a taxonomy in which those systems are divided into
several hierarchical groups based on their architecture and design. While other
taxonomies do exist, this review has focused on the key design factors that
affect the throughput and scalability of a given solution, as well as the
incremental improvements which bettered such an architecture. This review gives
special attention to Google's Borg, which is one of the most advanced and
published systems of this kind.

</details>


### [24] [Conceptual Design Report for FAIR Computing](https://arxiv.org/abs/2511.01861)
*Johan Messchendorp,Mohammad Al-Turany,Volker Friese,Thorsten Kollegger,Bastian Loeher,Jochen Markert,Andrew Mistry,Thomas Neff,Adrian Oeftiger,Michael Papenbrock,Stephane Pietri,Shahab Sanjari,Tobias Stockmanns*

Main category: cs.DC

TL;DR: 该概念设计报告概述了德国达姆施塔特FAIR设施从2028年“首次科学（加）”阶段起的计算基础设施规划，旨在构建一个兼具联邦化与集中协调、可扩展且灵活的计算与存储架构，以支持多样化的科研需求和未来数据挑战。


<details>
  <summary>Details</summary>
Motivation: 为应对FAIR设施多样化的科研需求及未来数据处理挑战，需建立一个既能满足各研究团队计算与存储要求，又具备足够可扩展性与灵活性的统一基础设施。

Method: 制定涵盖开放数据、软件与服务政策的FAIR计算模型，设计联邦化但由中心协调的计算与存储基础设施架构，并明确相关政策与技术路线。

Result: 提出了面向2028年及以后阶段的FAIR计算基础设施整体方案，包括需求分析、政策框架和系统架构。

Conclusion: 通过构建联邦化与集中协调相结合的计算基础设施，可有效支撑FAIR多学科研究并应对未来的数据密集型科研挑战。

Abstract: This Conceptual Design Report (CDR) presents the plans of the computing
infrastructure for research at FAIR, Darmstadt, Germany. It presents the
computing requirements of the various research groups, the policies for the
computing and storage infrastructure, the foreseen FAIR computing model
including the open data, software and services policies and architecture for
the periods starting in 2028 with the "first science (plus)" phase to the
modularized start version of FAIR. The overall ambition is to create a
federated and centrally-orchestrated infrastructure serving the large diversity
of the research lines present with sufficient scalability and flexibility to
cope with future data challenges that will be present at FAIR.

</details>


### [25] [Possible Futures for Cloud Cost Models](https://arxiv.org/abs/2511.01862)
*Vanessa Sochat,Daniel Milroy*

Main category: cs.DC

TL;DR: 本文探讨了云计算成本模型如何从服务科学计算演变为以AI/ML为中心，并分析了这种转变对科研工作负载带来的挑战及未来可能的解决方案。


<details>
  <summary>Details</summary>
Motivation: 随着AI/ML需求主导现代计算，云资源和成本模型日益偏向单一消费者（AI/ML），导致科研用户在竞争资源时处于不利地位，可能被迫在非理想环境中运行科学工作负载。

Method: 文章通过回顾云计算成本模型的历史演变，分析当前以AI/ML为中心的资源分配机制，并探讨其对科学计算的影响，进而提出对未来云成本模型支持科研的可能路径。

Result: 指出当前云成本模型不适合科学计算需求，若不加以调整，科研工作将面临资源获取困难和运行环境不适配的问题。

Conclusion: 为持续支持科学发现，云成本模型需重新设计，以兼顾AI/ML与科研等多元用户的需求，避免资源分配过度集中于单一用途。

Abstract: Cloud is now the leading software and computing hardware innovator, and is
changing the landscape of compute to one that is optimized for artificial
intelligence and machine learning (AI/ML). Computing innovation was initially
driven to meet the needs of scientific computing. As industry and consumer
usage of computing proliferated, there was a shift to satisfy a multipolar
customer base. Demand for AI/ML now dominates modern computing and innovation
has centralized on cloud. As a result, cost and resource models designed to
serve AI/ML use cases are not currently well suited for science. If resource
contention resulting from a unipole consumer makes access to contended
resources harder for scientific users, a likely future is running scientific
workloads where they were not intended. In this article, we discuss the past,
current, and possible futures of cloud cost models for the continued support of
discovery and science.

</details>


### [26] [EdgeReasoning: Characterizing Reasoning LLM Deployment on Edge GPUs](https://arxiv.org/abs/2511.01866)
*Benjamin Kubwimana,Qijing Huang*

Main category: cs.DC

TL;DR: 本文提出了EdgeReasoning，系统研究了在边缘GPU上部署用于推理的大语言模型（LLMs）时的延迟与准确率权衡，评估了不同架构、模型规模、提示策略和测试时缩放方法，并绘制了准确率-延迟的帕累托前沿，为边缘端高效部署推理型LLM提供指导。


<details>
  <summary>Details</summary>
Motivation: 在机器人等新兴自主系统中，边缘智能日益重要。尽管边缘部署具备隐私保护、连接鲁棒性以及能效和成本优势，但将大语言模型用于边缘GPU上的推理任务仍面临严苛的延迟限制和有限计算资源的挑战。目前缺乏对如何组合多种设计因素（如模型架构选择、规模、token预算和测试时缩放策略）以实现最佳性能的系统性指导。

Method: 作者系统性地量化了不同LLM架构和模型规模下的延迟-准确率权衡；评估了基于提示和基于模型微调的技术在减少推理token长度的同时保持性能的效果；并分析了不同并行度的测试时缩放方法在严格延迟约束下提升准确率的能力。

Result: 研究绘制了在边缘GPU上部署推理型LLM时可实现的准确率与延迟之间的帕累托前沿，明确了各种设计选择对性能的影响，并提供了优化边缘部署的系统性指南。

Conclusion: EdgeReasoning为在资源受限的边缘设备上高效部署推理型大语言模型提供了全面的实证分析和实用指导，有助于开发者在延迟和准确率之间做出最优权衡。

Abstract: Edge intelligence paradigm is increasingly demanded by the emerging
autonomous systems, such as robotics. Beyond ensuring privacy-preserving
operation and resilience in connectivity-limited environments, edge deployment
offers significant energy and cost advantages over cloud-based solutions.
However, deploying large language models (LLMs) for reasoning tasks on edge
GPUs faces critical challenges from strict latency constraints and limited
computational resources. To navigate these constraints, developers must balance
multiple design factors - choosing reasoning versus non-reasoning
architectures, selecting appropriate model sizes, allocating token budgets, and
applying test-time scaling strategies - to meet target latency and optimize
accuracy. Yet guidance on optimal combinations of these variables remains
scarce. In this work, we present EdgeReasoning, a comprehensive study
characterizing the deployment of reasoning LLMs on edge GPUs. We systematically
quantify latency-accuracy tradeoffs across various LLM architectures and model
sizes. We systematically evaluate prompt-based and model-tuning-based
techniques for reducing reasoning token length while maintaining performance
quality. We further profile test-time scaling methods with varying degrees of
parallelism to maximize accuracy under strict latency budgets. Through these
analyses, EdgeReasoning maps the Pareto frontier of achievable accuracy-latency
configurations, offering systematic guidance for optimal edge deployment of
reasoning LLMs.

</details>


### [27] [Structural Analysis of Multi-Core Processor and Reliability Evaluation Model](https://arxiv.org/abs/2511.01871)
*S. Tsiramua,H. Meladze,T. Davitashvili,J. M. Sanchez,F. Criado-Aldeanueva*

Main category: cs.DC

TL;DR: 本文基于逻辑概率方法，构建了多核处理器（含多功能核心）在可靠性、容错性、可用性与灵活性等方面的结构分析与效率评估模型，并对双核与四核处理器进行了实例分析，揭示了效率指标随结构变化的趋势。


<details>
  <summary>Details</summary>
Motivation: 为提升多核处理器在复杂应用场景下的性能表现，需系统评估其可靠性、容错性、可用性和灵活性等关键效率指标，而现有方法缺乏对多功能核心及可变结构的综合建模。

Method: 采用逻辑概率方法，建立多功能核心的可靠性与容错性评估模型；构建基于最短路径和运行条件的灵活性与性能逻辑概率模型；并综合所有运行状态，估算多核处理器的整体可靠性、容错能力与寿命。

Result: 对双核与四核处理器进行了结构分析，展示了所提模型的有效性，并揭示了随着核心数量增加，多核处理器各项效率指标的变化趋势。

Conclusion: 所提出的逻辑概率模型能有效支持对可变结构多核处理器的效率评估，为设计高可靠、高容错、高灵活性的多核系统提供理论依据。

Abstract: In the present paper, the models of structural analysis and evaluation of
efficiency indicators (reliability, fault tolerance, viability, and
flexibility) of a multi core processor with variable structure, equipped with
multi functional cores, are considered. Using logical probabilistic methods,
the following has been developed: models for evaluating the reliability and
fault tolerance of processor cores as multi functional elements; logical
probabilistic models of the shortest paths, flexibility, and performance
conditions for successful operation of multi core processors based on multi
functional cores; and models for estimating the reliability, fault tolerance,
and lifetime of multi core processors considering all possible states of
performance. The results of the structural analysis of two core and four core
processors and the trends of increasing the efficiency indicators of multi core
processors are presented.

</details>


### [28] [mLR: Scalable Laminography Reconstruction based on Memoization](https://arxiv.org/abs/2511.01893)
*Bin Ma,Viktor Nikitin,Xi Wang,Tekin Bicer,Dong Li*

Main category: cs.DC

TL;DR: 本文提出mLR方法，通过记忆化技术优化ADMM-FFT算法中重复的FFT操作，并结合变量卸载策略，在有限内存下实现了对2K×2K×2K规模层析成像重建问题的高效求解，平均性能提升52.8%。


<details>
  <summary>Details</summary>
Motivation: ADMM-FFT虽在层析成像重建中具有高精度，但存在计算时间过长和内存消耗大的问题，限制了其在大规模问题中的应用。

Method: 引入记忆化（memoization）技术替代ADMM-FFT迭代过程中重复且耗时的FFT操作，并设计一系列优化策略使记忆化在性能和可扩展性上有效；同时采用变量卸载（variable offloading）技术节省CPU内存，支持跨GPU乃至跨节点扩展。

Result: mLR成功将ADMM-FFT扩展至2K×2K×2K输入规模，这是目前在有限内存条件下使用ADMM-FFT进行层析成像重建的最大问题规模，平均性能提升52.8%（最高达65.4%）。

Conclusion: 通过记忆化与变量卸载相结合，mLR显著提升了ADMM-FFT的效率和可扩展性，使其适用于更大规模的层析成像重建任务。

Abstract: ADMM-FFT is an iterative method with high reconstruction accuracy for
laminography but suffers from excessive computation time and large memory
consumption. We introduce mLR, which employs memoization to replace the
time-consuming Fast Fourier Transform (FFT) operations based on an unique
observation that similar FFT operations appear in iterations of ADMM-FFT. We
introduce a series of techniques to make the application of memoization to
ADMM-FFT performance-beneficial and scalable. We also introduce variable
offloading to save CPU memory and scale ADMM-FFT across GPUs within and across
nodes. Using mLR, we are able to scale ADMM-FFT on an input problem of
2Kx2Kx2K, which is the largest input problem laminography reconstruction has
ever worked on with the ADMM-FFT solution on limited memory; mLR brings 52.8%
performance improvement on average (up to 65.4%), compared to the original
ADMM-FFT.

</details>


### [29] [GPoS: Geospatially-aware Proof of Stake](https://arxiv.org/abs/2511.02034)
*Shashank Motepalli,Naman Garg,Gengrui Zhang,Hans-Arno Jacobsen*

Main category: cs.DC

TL;DR: 该论文提出了一种地理感知权益证明机制（GPoS），通过结合地理位置多样性与权益投票权，在主流PoS区块链中显著提升地理去中心化水平，同时对共识性能影响极小。


<details>
  <summary>Details</summary>
Motivation: 现有主流权益证明（PoS）区块链在共识投票权上高度集中于少数地理区域，导致地理去中心化程度不足，影响系统的监管韧性、鲁棒性和公平性。

Method: 作者实证分析了Aptos、Avalanche、Ethereum、Solana和Sui五个主流PoS区块链的地理分布，并提出了GPoS机制，将地理位置多样性纳入权益投票权计算中。

Result: 实验表明，GPoS平均可将基于特征向量中心性的基尼系数衡量的地理去中心化水平提升45%，且在HotStuff和CometBFT等BFT协议中仅带来极小的性能开销。

Conclusion: GPoS能有效增强区块链的地理去中心化，同时几乎不影响共识性能，为构建更具韧性与公平性的区块链系统提供了可行方案。

Abstract: Geospatial decentralization is essential for blockchains, ensuring regulatory
resilience, robustness, and fairness. We empirically analyze five major Proof
of Stake (PoS) blockchains: Aptos, Avalanche, Ethereum, Solana, and Sui,
revealing that a few geographic regions dominate consensus voting power,
resulting in limited geospatial decentralization. To address this, we propose
Geospatially aware Proof of Stake (GPoS), which integrates geospatial diversity
with stake-based voting power. Experimental evaluation demonstrates an average
45% improvement in geospatial decentralization, as measured by the Gini
coefficient of Eigenvector centrality, while incurring minimal performance
overhead in BFT protocols, including HotStuff and CometBFT. These results
demonstrate that GPoS can improve geospatial decentralization {while, in our
experiments, incurring minimal overhead} to consensus performance.

</details>


### [30] [Eliminating Multi-GPU Performance Taxes: A Systems Approach to Efficient Distributed LLMs](https://arxiv.org/abs/2511.02168)
*Octavian Alexandru Trifan,Karthik Sangaiah,Muhammad Awad,Muhammad Osama,Sumanth Gudaparthi,Alexandru Nicolau,Alexander Veidenbaum,Ganesh Dasika*

Main category: cs.DC

TL;DR: 本文提出了一种超越传统BSP模型的新方法，通过利用Triton中的Iris等库实现细粒度通信与同步，系统性消除“三大开销”，在分布式大语言模型（LLM）关键算子上实现了10-20%的端到端延迟提升。


<details>
  <summary>Details</summary>
Motivation: 传统BSP模型在分布式GPU执行中引入显著性能瓶颈，作者旨在通过新编程范式解决这些低效问题。

Method: 引入“三大开销”分析框架，并基于Iris for Triton提供的核内通信原语，设计细粒度的生产者-消费者流水线和数据流同步机制，替代全局屏障和粗粒度同步。

Result: 在All-Gather+GEMM和Flash Decode等关键算子上，相比BSP方法获得10-20%的端到端延迟加速。

Conclusion: 该方法为分布式LLM工作负载提供了一种更高效、更可编程的新范式，有效克服了传统BSP模型的局限性。

Abstract: As large language models (LLMs) continue to scale, their workloads
increasingly rely on distributed execution across multiple GPUs. However, the
conventional bulk synchronous parallel~(BSP) model used in such settings
introduces significant performance inefficiencies. To characterize these
bottlenecks, we introduce the ''Three Taxes'' (Bulk Synchronous, Inter-Kernel
Data Locality, and Kernel Launch Overhead) as an analytical framework. We
propose moving beyond the rigid BSP model to address key inefficiencies in
distributed GPU execution. By exploiting libraries like Iris for Triton, we
gain access to in-kernel communication primitives that enable the design of
novel fine-grained programming patterns, offering greater flexibility and
performance than traditional BSP-based approaches. These patterns
systematically eliminate the three taxes by creating direct, tile-level
producer-consumer pipelines and replacing global barriers with fine-grained
dataflow synchronization. Applying this methodology to critical kernels, from
the foundational All-Gather + general matrix multiplication operation to the
complex Flash Decode algorithm, we observe a 10-20% speedup in end-to-end
latency over BSP-based approaches, establishing a more programmable and
efficient paradigm for distributed LLM workloads.

</details>


### [31] [From Models to Operators: Rethinking Autoscaling Granularity for Large Generative Models](https://arxiv.org/abs/2511.02248)
*Xingqi Cui,Chieh-Jan Mike Liang,Jiarong Xing,Haoran Qiu*

Main category: cs.DC

TL;DR: 该论文提出了一种面向算子级别的自动扩缩框架，通过细粒度资源分配，在满足用户服务等级目标（SLO）的同时显著提升大生成模型推理的资源效率和能效。


<details>
  <summary>Details</summary>
Motivation: 现有大模型服务方案采用静态资源配置或模型级自动扩缩，将模型视为整体，无法适应在线推理中动态变化的流量，导致性能下降或资源利用率低下。

Method: 作者通过对生成模型内部算子的异构性进行系统分析，提出以算子为单位进行自动扩缩，根据各算子的计算、内存特征及对批大小、序列长度和流量的敏感性，优化其扩缩、批处理和部署策略。

Result: 在生产级负载下评估表明，该方法在满足SLO的前提下可减少最多40%的GPU使用量和35%的能耗；在固定资源下可实现1.6倍吞吐量并降低5%能耗。

Conclusion: 算子而非整个模型，是扩缩大规模生成模型工作负载更有效的基本单元。

Abstract: Serving large generative models such as LLMs and multi- modal transformers
requires balancing user-facing SLOs (e.g., time-to-first-token,
time-between-tokens) with provider goals of efficiency and cost reduction.
Existing solutions rely on static provisioning or model-level autoscaling, both
of which treat the model as a monolith. This coarse-grained resource management
leads to degraded performance or significant resource underutilization due to
poor adaptability to dynamic inference traffic that is common online.
  The root cause of this inefficiency lies in the internal structure of
generative models: they are executed as graphs of interconnected operators.
Through detailed characterization and systematic analysis, we find that
operators are heterogeneous in their compute and memory footprints and exhibit
diverse sensitivity to workload and resource factors such as batch size,
sequence length, and traffic rate. This heterogeneity suggests that the
operator, rather than the entire model, is the right granularity for scaling
decisions.
  We propose an operator-level autoscaling framework, which allocates resources
at finer (operator)-granularity, optimizing the scaling, batching, and
placement based on individual operator profiles. Evaluated on production-scale
traces, our approach preserves SLOs with up to 40% fewer GPUs and 35% less
energy, or under fixed resources achieves 1.6x higher throughput with 5% less
energy. These results show that the operator, rather than the model, is
fundamentally a more effective unit for scaling large generative workloads.

</details>


### [32] [Fast Algorithms for Scheduling Many-body Correlation Functions on Accelerators](https://arxiv.org/abs/2511.02257)
*Oguz Selvitopi,Emin Ozturk,Jie Chen,Ponnuswamy Sadayappan,Robert G. Edwards,Aydın Buluç*

Main category: cs.DC

TL;DR: 本文提出两种新颖的调度算法，通过优化张量重用以提升格点量子色动力学（LQCD）中关联函数计算的内存效率和计算速度。


<details>
  <summary>Details</summary>
Motivation: 在LQCD模拟中，关联函数的计算涉及大量大规模张量收缩操作，在GPU上执行时面临如何优化张量重用、减少数据传输并降低峰值内存使用的挑战。

Method: 设计两种快速调度算法，利用LQCD应用特有的二元收缩结构和收缩树中的局部性特征，对收缩操作进行重排序，以增强时间局部性并最小化峰值内存占用；并将算法集成到Redstar软件套件中。

Result: 调度器最多实现2.1倍的峰值内存降低，对应缓存驱逐减少达4.2倍、数据流量减少达1.8倍，并使关联函数计算速度最高提升1.9倍。

Conclusion: 所提出的调度算法显著提升了LQCD关联函数计算的性能和内存效率，验证了针对领域特定特征进行调度优化的有效性。

Abstract: Computation of correlation functions is a key operation in Lattice quantum
chromodynamics (LQCD) simulations to extract nuclear physics observables. These
functions involve many binary batch tensor contractions, each tensor possibly
occupying hundreds of MBs of memory. Performing these contractions on GPU
accelerators poses the challenge of scheduling them as to optimize tensor reuse
and reduce data traffic. In this work we propose two fast novel scheduling
algorithms that reorder contractions to increase temporal locality via
input/intermediate tensor reuse. Our schedulers take advantage of
application-specific features, such as contractions being binary and locality
within contraction trees, to optimize the objective of minimizing peak memory.
We integrate them into the LQCD analysis software suite Redstar and improve
time-to-solution. Our schedulers attain upto 2.1x improvement in peak memory,
which is reflected by a reduction of upto 4.2x in evictions, upto 1.8x in data
traffic, resulting in upto 1.9x faster correlation function computation time.

</details>


### [33] [Federated Attention: A Distributed Paradigm for Collaborative LLM Inference over Edge Networks](https://arxiv.org/abs/2511.02647)
*Xiumei Deng,Zehui Xiong,Binbin Chen,Dong In Kim,Merouane Debbah,H. Vincent Poor*

Main category: cs.DC

TL;DR: 本文提出了联邦注意力（FedAttn）框架，将联邦学习思想融入大语言模型的自注意力机制中，在保护用户隐私的同时提升通信与计算效率，支持边缘设备上的协作式大模型推理。


<details>
  <summary>Details</summary>
Motivation: 在边缘设备上部署大语言模型进行协作推理时，面临隐私泄露、通信开销大和计算瓶颈三大挑战，亟需一种兼顾隐私、效率与性能的新方法。

Method: 提出FedAttn框架，允许参与者在本地执行自注意力计算，并周期性地跨多个Transformer层交换和聚合Key-Value矩阵；同时揭示其与联邦学习在结构上的对偶性，从而引入联邦优化技术，并分析误差传播与效率-质量权衡。

Result: 实验验证了理论分析的正确性，表明通过稀疏注意力和自适应KV聚合可显著优化性能，FedAttn在保证响应质量的同时有效提升了通信与计算效率。

Conclusion: FedAttn为边缘环境下协作式大语言模型推理提供了一个兼顾隐私保护、通信效率和计算效率的可行框架，具有良好的可扩展性和实际部署潜力。

Abstract: Large language models (LLMs) are proliferating rapidly at the edge,
delivering intelligent capabilities across diverse application scenarios.
However, their practical deployment in collaborative scenarios confronts
fundamental challenges: privacy vulnerabilities, communication overhead, and
computational bottlenecks. To address these, we propose Federated Attention
(FedAttn), which integrates the federated paradigm into the self-attention
mechanism, creating a new distributed LLM inference framework that
simultaneously achieves privacy protection, communication efficiency, and
computational efficiency. FedAttn enables participants to perform local
self-attention over their own token representations while periodically
exchanging and aggregating Key-Value (KV) matrices across multiple Transformer
blocks, collaboratively generating LLM responses without exposing private
prompts. Further, we identify a structural duality between contextual
representation refinement in FedAttn and parameter optimization in FL across
private data, local computation, and global aggregation. This key insight
provides a principled foundation for systematically porting federated
optimization techniques to collaborative LLM inference. Building on this
framework, we theoretically analyze how local self-attention computation within
participants and heterogeneous token relevance among participants shape error
propagation dynamics across Transformer blocks. Moreover, we characterize the
fundamental trade-off between response quality and communication/computation
efficiency, which is governed by the synchronization interval and the number of
participants. Experimental results validate our theoretical analysis, and
reveal significant optimization opportunities through sparse attention and
adaptive KV aggregation, highlighting FedAttn's potential to deliver
scalability and efficiency in real-world edge deployments.

</details>


### [34] [Implementing Multi-GPU Scientific Computing Miniapps Across Performance Portable Frameworks](https://arxiv.org/abs/2511.02655)
*Johansell Villalobos,Josef Ruzicka,Silvio Rizzi*

Main category: cs.DC

TL;DR: 本文在Polaris超算单节点上使用四种性能可移植框架（Kokkos、OpenMP、RAJA和OCCA）对N体模拟和结构网格模拟进行初步性能评估，发现各框架在不同场景下表现差异显著，指出需进一步优化以提升可扩展性和效率。


<details>
  <summary>Details</summary>
Motivation: 随着异构计算架构的兴起，科学计算需要能在不同硬件平台上高效运行且代码改动最小的性能可移植框架，因此有必要评估主流框架在实际应用中的表现。

Method: 在配备四块NVIDIA A100 GPU的Polaris超算单节点上，采用分布式内存方法，利用Kokkos、OpenMP、RAJA和OCCA四种框架分别实现N体模拟和结构网格模拟，并比较其求解时间。

Result: OCCA在小规模验证问题中因JIT编译表现出更快执行时间，但其默认API缺乏优化的归约算法可能限制大规模模拟的可扩展性；OpenMP在结构网格模拟中表现较差，可能源于节点间数据同步与通信效率低下。

Conclusion: 各性能可移植框架在不同应用中表现各异，需针对归约算法、数据通信和内存管理等方面进一步优化，并开展可扩展性研究与全面统计分析以充分挖掘其潜力。

Abstract: Scientific computing in the exascale era demands increased computational
power to solve complex problems across various domains. With the rise of
heterogeneous computing architectures the need for vendor-agnostic, performance
portability frameworks has been highlighted. Libraries like Kokkos have become
essential for enabling high-performance computing applications to execute
efficiently across different hardware platforms with minimal code changes. In
this direction, this paper presents preliminary time-to-solution results for
two representative scientific computing applications: an N-body simulation and
a structured grid simulation. Both applications used a distributed memory
approach and hardware acceleration through four performance portability
frameworks: Kokkos, OpenMP, RAJA, and OCCA. Experiments conducted on a single
node of the Polaris supercomputer using four NVIDIA A100 GPUs revealed
significant performance variability among frameworks. OCCA demonstrated faster
execution times for small-scale validation problems, likely due to JIT
compilation, however its lack of optimized reduction algorithms may limit
scalability for larger simulations while using its out of the box API. OpenMP
performed poorly in the structured grid simulation most likely due to
inefficiencies in inter-node data synchronization and communication. These
findings highlight the need for further optimization to maximize each
framework's capabilities. Future work will focus on enhancing reduction
algorithms, data communication, memory management, as wells as performing
scalability studies, and a comprehensive statistical analysis to evaluate and
compare framework performance.

</details>


### [35] [Making Democracy Work: Fixing and Simplifying Egalitarian Paxos (Extended Version)](https://arxiv.org/abs/2511.02743)
*Fedor Ryabinin,Alexey Gotsman,Pierre Sutra*

Main category: cs.DC

TL;DR: 本文提出了EPaxos*，一种更简单且正确的Egalitarian Paxos变体，通过简化故障恢复算法并扩展其适用的容错阈值范围，解决了原协议复杂、模糊和存在缺陷的问题。


<details>
  <summary>Details</summary>
Motivation: Egalitarian Paxos虽避免了传统Paxos中单点领导者带来的性能瓶颈和单点故障问题，但其协议本身过于复杂、规范模糊且存在严重错误，亟需一个更清晰、正确且易于实现的替代方案。

Method: 作者设计了EPaxos*，核心贡献是一个更简单的故障恢复算法，并对该算法进行了严格的形式化正确性证明；同时将协议推广到更一般的容错参数范围（满足 n ≥ max{2e+f−1, 2f+1}）。

Result: EPaxos*在保持Egalitarian Paxos无领导者、低延迟和高吞吐优势的同时，显著降低了协议复杂性，消除了原有缺陷，并证明了所用进程数在给定容错要求下是最优的。

Conclusion: EPaxos*是对Egalitarian Paxos的有效改进，提供了一个更简洁、正确且通用的无领导者状态机复制协议，为后续相关研究和实践提供了可靠基础。

Abstract: Classical state-machine replication protocols, such as Paxos, rely on a
distinguished leader process to order commands. Unfortunately, this approach
makes the leader a single point of failure and increases the latency for
clients that are not co-located with it. As a response to these drawbacks,
Egalitarian Paxos introduced an alternative, leaderless approach, that allows
replicas to order commands collaboratively. Not relying on a single leader
allows the protocol to maintain non-zero throughput with up to $f$ crashes of
any processes out of a total of $n = 2f+1$. The protocol furthermore allows any
process to execute a command $c$ fast, in $2$ message delays, provided no more
than $e = \lceil\frac{f+1}{2}\rceil$ other processes fail, and all concurrently
submitted commands commute with $c$; the latter condition is often satisfied in
practical systems.
  Egalitarian Paxos has served as a foundation for many other replication
protocols. But unfortunately, the protocol is very complex, ambiguously
specified and suffers from nontrivial bugs. In this paper, we present EPaxos*
-- a simpler and correct variant of Egalitarian Paxos. Our key technical
contribution is a simpler failure-recovery algorithm, which we have rigorously
proved correct. Our protocol also generalizes Egalitarian Paxos to cover the
whole spectrum of failure thresholds $f$ and $e$ such that $n \ge \max\{2e+f-1,
2f+1\}$ -- the number of processes that we show to be optimal.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [36] [EvoMem: Improving Multi-Agent Planning with Dual-Evolving Memory](https://arxiv.org/abs/2511.01912)
*Wenzhe Fan,Ning Yan,Masood Mortazavi*

Main category: cs.MA

TL;DR: 本文提出了EvoMem，一种基于双演化记忆机制的多智能体规划框架，通过约束记忆（CMem）和查询反馈记忆（QMem）提升自然语言规划任务中的协调与推理能力，在多个规划任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有基于大语言模型的多智能体框架在规划任务中缺乏对类人记忆机制的探索，而记忆对于迭代推理、约束追踪和错误修正至关重要。

Method: EvoMem框架包含三个智能体（约束提取器、验证器和执行器）和两个记忆模块：跨查询演化的约束记忆（CMem）和查询内演化的反馈记忆（QMem），两者在每次查询结束后重置。

Result: 在旅行规划、会议安排和日程调度任务中，EvoMem均展现出一致的性能提升。

Conclusion: 记忆机制对提升多智能体规划能力具有关键作用，EvoMem有效模拟了人类工作记忆模型，增强了智能体间的协调与规划效果。

Abstract: Planning has been a cornerstone of artificial intelligence for solving
complex problems, and recent progress in LLM-based multi-agent frameworks have
begun to extend this capability. However, the role of human-like memory within
these frameworks remains largely unexplored. Understanding how agents
coordinate through memory is critical for natural language planning, where
iterative reasoning, constraint tracking, and error correction drive the
success. Inspired by working memory model in cognitive psychology, we present
EvoMem, a multi-agent framework built on a dual-evolving memory mechanism. The
framework consists of three agents (Constraint Extractor, Verifier, and Actor)
and two memory modules: Constraint Memory (CMem), which evolves across queries
by storing task-specific rules and constraints while remains fixed within a
query, and Query-feedback Memory (QMem), which evolves within a query by
accumulating feedback across iterations for solution refinement. Both memory
modules are reset at the end of each query session. Evaluations on trip
planning, meeting planning, and calendar scheduling show consistent performance
improvements, highlighting the effectiveness of EvoMem. This success
underscores the importance of memory in enhancing multi-agent planning.

</details>


### [37] [Automata-Conditioned Cooperative Multi-Agent Reinforcement Learning](https://arxiv.org/abs/2511.02304)
*Beyazit Yalcinkaya,Marcell Vazquez-Chanlatte,Ameesh Shah,Hanna Krasowski,Sanjit A. Seshia*

Main category: cs.MA

TL;DR: 本文提出了一种名为ACC-MARL的新框架，用于在集中训练、分散执行的多智能体强化学习中，利用自动机对复杂协作任务进行分解与条件化学习，实现了高效的多任务策略学习和测试时的任务最优分配。


<details>
  <summary>Details</summary>
Motivation: 现有方法在使用自动机表示任务时仍存在样本效率低、仅限于单任务等问题，难以应对多任务、多智能体协作场景中的复杂时序目标。

Method: 提出Automata-Conditioned Cooperative Multi-Agent Reinforcement Learning（ACC-MARL）框架，通过自动机将复杂任务分解为子任务，并设计任务条件化的去中心化团队策略；同时解决该框架在实际应用中的关键挑战，并证明其正确性。

Result: 实验表明，所学策略能实现任务感知的多步协调行为（如按按钮开门、扶门、跳过任务等），且其价值函数可用于测试阶段的任务最优分配。

Conclusion: ACC-MARL有效提升了多任务多智能体协作策略的学习效率与泛化能力，为复杂时序任务的自动分解与分配提供了可行方案。

Abstract: We study the problem of learning multi-task, multi-agent policies for
cooperative, temporal objectives, under centralized training, decentralized
execution. In this setting, using automata to represent tasks enables the
decomposition of complex tasks into simpler sub-tasks that can be assigned to
agents. However, existing approaches remain sample-inefficient and are limited
to the single-task case. In this work, we present Automata-Conditioned
Cooperative Multi-Agent Reinforcement Learning (ACC-MARL), a framework for
learning task-conditioned, decentralized team policies. We identify the main
challenges to ACC-MARL's feasibility in practice, propose solutions, and prove
the correctness of our approach. We further show that the value functions of
learned policies can be used to assign tasks optimally at test time.
Experiments show emergent task-aware, multi-step coordination among agents,
e.g., pressing a button to unlock a door, holding the door, and
short-circuiting tasks.

</details>
