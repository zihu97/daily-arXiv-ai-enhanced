{"id": "2512.23742", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.23742", "abs": "https://arxiv.org/abs/2512.23742", "authors": ["Guangxi Fan", "Tianliang Ma", "Xuguang Sun", "Xun Wang", "Kain Lu Low", "Leilai Shao"], "title": "AgenticTCAD: A LLM-based Multi-Agent Framework for Automated TCAD Code Generation and Device Optimization", "comment": "7 pages, 7 figures, 2 tables", "summary": "With the continued scaling of advanced technology nodes, the design-technology co-optimization (DTCO) paradigm has become increasingly critical, rendering efficient device design and optimization essential. In the domain of TCAD simulation, however, the scarcity of open-source resources hinders language models from generating valid TCAD code. To overcome this limitation, we construct an open-source TCAD dataset curated by experts and fine-tune a domain-specific model for TCAD code generation. Building on this foundation, we propose AgenticTCAD, a natural language - driven multi-agent framework that enables end-to-end automated device design and optimization. Validation on a 2 nm nanosheet FET (NS-FET) design shows that AgenticTCAD achieves the International Roadmap for Devices and Systems (IRDS)-2024 device specifications within 4.2 hours, whereas human experts required 7.1 days with commercial tools.", "AI": {"tldr": "\u63d0\u51faAgenticTCAD\u6846\u67b6\uff0c\u901a\u8fc7\u591a\u667a\u80fd\u4f53\u4e0e\u81ea\u7136\u8bed\u8a00\u9a71\u52a8\u5b9e\u73b02\u7eb3\u7c73\u7eb3\u7c73\u7247\u573a\u6548\u5e94\u7ba1\u7684\u81ea\u52a8\u5316\u8bbe\u8ba1\uff0c\u5927\u5e45\u7f29\u77ed\u4f18\u5316\u65f6\u95f4\u3002", "motivation": "\u89e3\u51b3TCAD\u4eff\u771f\u9886\u57df\u5f00\u6e90\u8d44\u6e90\u7a00\u7f3a\u5bfc\u81f4\u8bed\u8a00\u6a21\u578b\u96be\u4ee5\u751f\u6210\u6709\u6548\u4ee3\u7801\u7684\u95ee\u9898\uff0c\u63d0\u5347\u5668\u4ef6\u8bbe\u8ba1\u6548\u7387\u3002", "method": "\u6784\u5efa\u4e13\u5bb6\u7cbe\u9009\u5f00\u6e90TCAD\u6570\u636e\u96c6\u5e76\u5fae\u8c03\u4e13\u7528\u6a21\u578b\uff0c\u5f00\u53d1\u81ea\u7136\u8bed\u8a00\u9a71\u52a8\u7684\u591a\u667a\u80fd\u4f53\u6846\u67b6AgenticTCAD\u3002", "result": "\u57282\u7eb3\u7c73NS-FET\u8bbe\u8ba1\u4e2d\uff0c4.2\u5c0f\u65f6\u5185\u8fbe\u6210IRDS-2024\u6807\u51c6\uff0c\u8fdc\u5feb\u4e8e\u4eba\u5de57.1\u5929\u3002", "conclusion": "AgenticTCAD\u663e\u8457\u52a0\u901f\u5148\u8fdb\u8282\u70b9\u5668\u4ef6\u8bbe\u8ba1\uff0c\u9a8c\u8bc1\u4e86\u81ea\u52a8\u5316\u4e0eAI\u534f\u540c\u4f18\u5316\u5728DTCO\u4e2d\u7684\u5de8\u5927\u6f5c\u529b\u3002"}}
{"id": "2512.23743", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.23743", "abs": "https://arxiv.org/abs/2512.23743", "authors": ["Yunguo Yu"], "title": "Hybrid-Code: A Privacy-Preserving, Redundant Multi-Agent Framework for Reliable Local Clinical Coding", "comment": "18 pages, 1 figure, original research paper", "summary": "Clinical coding automation using cloud-based Large Language Models (LLMs) poses privacy risks and latency bottlenecks, rendering them unsuitable for on-premise healthcare deployment. We introduce Hybrid-Code, a hybrid neuro-symbolic multi-agent framework for local clinical coding that ensures production reliability through redundancy and verification. Our system comprises two agents: a Coder that attempts language model-based semantic reasoning using BioMistral-7B but falls back to deterministic keyword matching when model output is unreliable, ensuring pipeline completion; and an Auditor that verifies codes against a 257-code knowledge base and clinical evidence. Evaluating on 1,000 MIMIC-III discharge summaries, we demonstrate no hallucinated codes among accepted outputs within the knowledge base, 24.47% verification rate, and 34.11% coverage (95% CI: 31.2%--37.0%) with 86%+ language model utilization. The Auditor filtered invalid format codes and provided evidence-based quality control (75.53% rejection rate) while ensuring no patient data leaves the hospital firewall. The hybrid architecture -- combining language model semantic understanding (when successful), deterministic fallback (when the model fails), and symbolic verification (always active) -- ensures both reliability and privacy preservation, addressing critical barriers to AI adoption in healthcare. Our key finding is that reliability through redundancy is more valuable than pure model performance in production healthcare systems, where system failures are unacceptable.", "AI": {"tldr": "Hybrid-Code \u662f\u4e00\u79cd\u672c\u5730\u90e8\u7f72\u7684\u795e\u7ecf\u7b26\u53f7\u591a\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u901a\u8fc7\u5197\u4f59\u4e0e\u9a8c\u8bc1\u673a\u5236\u786e\u4fdd\u4e34\u5e8a\u7f16\u7801\u81ea\u52a8\u5316\u5728\u533b\u7597\u73af\u5883\u4e2d\u7684\u53ef\u9760\u6027\u4e0e\u9690\u79c1\u6027\u3002", "motivation": "\u4e91\u7aef\u5927\u8bed\u8a00\u6a21\u578b\u5b58\u5728\u9690\u79c1\u6cc4\u9732\u548c\u5ef6\u8fdf\u95ee\u9898\uff0c\u4e0d\u9002\u5408\u9662\u5185\u90e8\u7f72\uff0c\u9700\u6784\u5efa\u517c\u987e\u53ef\u9760\u6027\u4e0e\u9690\u79c1\u4fdd\u62a4\u7684\u672c\u5730\u5316\u65b9\u6848\u3002", "method": "\u7cfb\u7edf\u5305\u542b Coder \u4e0e Auditor \u4e24\u4e2a\u667a\u80fd\u4f53\uff1aCoder \u4f7f\u7528 BioMistral-7B \u8fdb\u884c\u8bed\u4e49\u63a8\u7406\uff0c\u5931\u8d25\u65f6\u56de\u9000\u81f3\u5173\u952e\u8bcd\u5339\u914d\uff1bAuditor \u57fa\u4e8e\u77e5\u8bc6\u5e93\u9a8c\u8bc1\u7f16\u7801\u5e76\u63d0\u4f9b\u8bc1\u636e\u652f\u6301\u3002", "result": "\u5728 MIMIC-III \u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u96f6\u5e7b\u89c9\u7f16\u7801\u300124.47% \u9a8c\u8bc1\u7387\u300134.11% \u8986\u76d6\u7387\uff0c86%+ \u6a21\u578b\u5229\u7528\u7387\uff0c75.53% \u65e0\u6548\u7f16\u7801\u88ab\u8fc7\u6ee4\uff0c\u4e14\u60a3\u8005\u6570\u636e\u4e0d\u51fa\u9662\u5185\u9632\u706b\u5899\u3002", "conclusion": "\u5728\u533b\u7597\u751f\u4ea7\u7cfb\u7edf\u4e2d\uff0c\u901a\u8fc7\u5197\u4f59\u673a\u5236\u4fdd\u969c\u53ef\u9760\u6027\u6bd4\u5355\u7eaf\u8ffd\u6c42\u6a21\u578b\u6027\u80fd\u66f4\u91cd\u8981\uff0c\u672c\u6846\u67b6\u6709\u6548\u514b\u670d\u4e86AI\u843d\u5730\u7684\u5173\u952e\u969c\u788d\u3002"}}
{"id": "2512.23746", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2512.23746", "abs": "https://arxiv.org/abs/2512.23746", "authors": ["Wei Li", "Yan Zou", "Yixin Liang", "Jos\u00e9 Moura", "Shawn Blanton"], "title": "DEFT: Differentiable Automatic Test Pattern Generation", "comment": null, "summary": "Modern IC complexity drives test pattern growth, with the majority of patterns targeting a small set of hard-to-detect (HTD) faults. This motivates new ATPG algorithms to improve test effectiveness specifically for HTD faults. This paper presents DEFT (Differentiable Automatic Test Pattern Generation), a new ATPG approach that reformulates the discrete ATPG problem as a continuous optimization task. DEFT introduces a mathematically grounded reparameterization that aligns the expected continuous objective with discrete fault-detection semantics, enabling reliable gradient-based pattern generation. To ensure scalability and stability on deep circuit graphs, DEFT integrates a custom CUDA kernel for efficient forward-backward propagation and applies gradient normalization to mitigate vanishing gradients. Compared to a leading commercial tool on two industrial benchmarks, DEFT improves HTD fault detection by 21.1% and 48.9% on average under the same pattern budget and comparable runtime. DEFT also supports practical ATPG settings such as partial assignment pattern generation, producing patterns with 19.3% fewer 0/1 bits while still detecting 35% more faults. These results indicate DEFT is a promising and effective ATPG engine, offering a valuable complement to existing heuristic.", "AI": {"tldr": "DEFT\u662f\u4e00\u79cd\u65b0\u7684ATPG\u65b9\u6cd5\uff0c\u901a\u8fc7\u5c06\u79bb\u6563\u95ee\u9898\u8f6c\u5316\u4e3a\u8fde\u7eed\u4f18\u5316\u4efb\u52a1\uff0c\u663e\u8457\u63d0\u5347\u5bf9\u96be\u68c0\u6d4b\u6545\u969c\u7684\u8986\u76d6\u7387\u3002", "motivation": "\u73b0\u4ee3\u96c6\u6210\u7535\u8def\u590d\u6742\u6027\u5bfc\u81f4\u6d4b\u8bd5\u6a21\u5f0f\u589e\u957f\uff0c\u591a\u6570\u6a21\u5f0f\u9488\u5bf9\u5c11\u91cf\u96be\u68c0\u6d4b\u6545\u969c\uff0c\u9700\u66f4\u6709\u6548\u7684ATPG\u7b97\u6cd5\u3002", "method": "DEFT\u91c7\u7528\u6570\u5b66\u91cd\u53c2\u6570\u5316\u5bf9\u9f50\u8fde\u7eed\u76ee\u6807\u4e0e\u79bb\u6563\u8bed\u4e49\uff0c\u7ed3\u5408CUDA\u5185\u6838\u548c\u68af\u5ea6\u5f52\u4e00\u5316\u5b9e\u73b0\u9ad8\u6548\u7a33\u5b9a\u4f18\u5316\u3002", "result": "\u5728\u4e24\u4e2a\u5de5\u4e1a\u57fa\u51c6\u4e0a\uff0cDEFT\u5728\u76f8\u540c\u9884\u7b97\u4e0b\u5e73\u5747\u63d0\u5347HTD\u6545\u969c\u68c0\u6d4b\u738721.1%\u548c48.9%\uff0c\u5e76\u652f\u6301\u90e8\u5206\u8d4b\u503c\u6a21\u5f0f\u751f\u6210\u3002", "conclusion": "DEFT\u662f\u73b0\u6709\u542f\u53d1\u5f0fATPG\u7684\u6709\u6548\u8865\u5145\uff0c\u5177\u5907\u5b9e\u7528\u6027\u548c\u6269\u5c55\u6f5c\u529b\u3002"}}
{"id": "2512.23747", "categories": ["cs.SE", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2512.23747", "abs": "https://arxiv.org/abs/2512.23747", "authors": ["Abhinav Parmar", "Abhisek Panigrahi", "Abhishek Kumar Dwivedi", "Abhishek Bhattacharya", "Adarsh Ramachandra", "Aditya Choudhary", "Aditya Garg", "Aditya Raj", "Alankrit Bhatt", "Alpesh Yadav", "Anant Vishnu", "Ananthu Pillai", "Ankush Kumar", "Aryan Patnaik", "Aswatha Narayanan S", "Avanish Raj Singh", "Bhavya Shree Gadda", "Brijesh Pankajbhai Kachhadiya", "Buggala Jahnavi", "Chidurala Nithin Krishna", "Chintan Shah", "Chunduru Akshaya", "Debarshi Banerjee", "Debrup Dey", "Deepa R.", "Deepika B G", "Faiz ur Rahman", "Gagan Gayari", "Gudhi Jagadeesh Kumar Naidu", "Gursimar Singh", "Harshal Tyagi", "Harshini K", "James Mani Vathalloor", "Jayarama Nettar", "Jayashree Gajjam", "Joe Walter Sugil George", "Kamalakara Sri Krishna Tadepalli", "Kamalkumar Rathinasamy", "Karan Chaurasia", "Karthikeyan S", "Kashish Arora", "Kaushal Desai", "Khushboo Buwade", "Kiran Manjrekar", "Malikireddy Venkata Sai Likhitha", "Manjunath A", "Mitali Mahavir Bedmutha", "Mohammed Rafee Tarafdar", "Nikhil Tiwari", "Nikitha K Gigi", "Pavan Ravikumar", "Pendyala Swarnanjali", "Piyush Anand", "Prakash Chandrasekar", "Prasanna Bhalchandra Gawade", "Prasanth Sivan", "Preeti Khurana", "Priyanshi Babbar", "Rajab Ali Mondal", "Rajesh Kumar Vissapragada", "Rajeshwari Ganesan", "Rajeswari Koppisetti", "Ramjee R.", "Ramkumar Thiruppathisamy", "Rani G. S.", "S Reka", "Samarth Gupta", "Sandeep Reddy Kothakota", "Sarathy K", "Sathyanarayana Sampath Kumar", "Saurabh Kumar", "Shashank Khasare", "Shenbaga Devi Venkatesh Kumar", "Shiva Rama Krishna Parvatham", "Shoeb Shaikh", "Shrishanmathi A", "Shubham Pathak", "Sree Samhita Koppaka", "Sreenivasa Raghavan K S", "Sreeram Venkatasubramanian", "Suprabha Desai Bojja", "Swetha R", "Syed Ahmed", "Chinmai Harshitha Thota", "Tushar Yadav", "Veeravelly Kusumitha", "V V S S Prasanth Patnaik", "Vidya Sri Sesetti", "Vijayakeerthi K", "Vikram Raj Bakshi", "Vinay K K", "Vinoth Kumar Loganathan", "Vipin Tiwari", "Vivek Kumar Shrivastav", "V Venkata Sri Datta Charan", "Wasim Akhtar Khan"], "title": "State-of-the-art Small Language Coder Model: Mify-Coder", "comment": null, "summary": "We present Mify-Coder, a 2.5B-parameter code model trained on 4.2T tokens using a compute-optimal strategy built on the Mify-2.5B foundation model. Mify-Coder achieves comparable accuracy and safety while significantly outperforming much larger baseline models on standard coding and function-calling benchmarks, demonstrating that compact models can match frontier-grade models in code generation and agent-driven workflows. Our training pipeline combines high-quality curated sources with synthetic data generated through agentically designed prompts, refined iteratively using enterprise-grade evaluation datasets. LLM-based quality filtering further enhances data density, enabling frugal yet effective training. Through disciplined exploration of CPT-SFT objectives, data mixtures, and sampling dynamics, we deliver frontier-grade code intelligence within a single continuous training trajectory. Empirical evidence shows that principled data and compute discipline allow smaller models to achieve competitive accuracy, efficiency, and safety compliance. Quantized variants of Mify-Coder enable deployment on standard desktop environments without requiring specialized hardware.", "AI": {"tldr": "Mify-Coder\u662f\u4e00\u4e2a2.5B\u53c2\u6570\u7684\u4ee3\u7801\u6a21\u578b\uff0c\u901a\u8fc7\u9ad8\u6548\u8bad\u7ec3\u7b56\u7565\u5728\u6807\u51c6\u7f16\u7801\u4efb\u52a1\u4e2d\u8d85\u8d8a\u66f4\u5927\u6a21\u578b\uff0c\u652f\u6301\u684c\u9762\u90e8\u7f72\u3002", "motivation": "\u63a2\u7d22\u5c0f\u89c4\u6a21\u6a21\u578b\u80fd\u5426\u901a\u8fc7\u9ad8\u8d28\u91cf\u6570\u636e\u4e0e\u8bad\u7ec3\u7b56\u7565\u8fbe\u5230\u524d\u6cbf\u5927\u6a21\u578b\u7684\u4ee3\u7801\u80fd\u529b\u4e0e\u5b89\u5168\u6027\u3002", "method": "\u57fa\u4e8eMify-2.5B\uff0c\u7ed3\u5408\u4eba\u5de5\u7cbe\u9009\u4e0e\u667a\u80fd\u4f53\u751f\u6210\u7684\u5408\u6210\u6570\u636e\uff0c\u8f85\u4ee5LLM\u8d28\u91cf\u8fc7\u6ee4\uff0c\u5728\u5355\u4e00\u8bad\u7ec3\u6d41\u7a0b\u4e2d\u4f18\u5316CPT-SFT\u76ee\u6807\u4e0e\u6570\u636e\u6df7\u5408\u7b56\u7565\u3002", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u4e8e\u66f4\u5927\u57fa\u7ebf\u6a21\u578b\uff0c\u540c\u65f6\u4fdd\u6301\u9ad8\u51c6\u786e\u7387\u4e0e\u5b89\u5168\u6027\uff0c\u91cf\u5316\u7248\u672c\u53ef\u5728\u666e\u901a\u684c\u9762\u8fd0\u884c\u3002", "conclusion": "\u4e25\u8c28\u7684\u6570\u636e\u4e0e\u7b97\u529b\u7ba1\u7406\u53ef\u4f7f\u5c0f\u578b\u6a21\u578b\u5728\u4ee3\u7801\u751f\u6210\u4e0e\u667a\u80fd\u4f53\u5de5\u4f5c\u6d41\u4e2d\u5ab2\u7f8e\u524d\u6cbf\u5927\u6a21\u578b\u3002"}}
{"id": "2512.24637", "categories": ["cs.OS"], "pdf": "https://arxiv.org/pdf/2512.24637", "abs": "https://arxiv.org/abs/2512.24637", "authors": ["Weihang Shen", "Yinqiu Chen", "Rong Chen", "Haibo Chen"], "title": "MSched: GPU Multitasking via Proactive Memory Scheduling", "comment": null, "summary": "The limited HBM capacity has become the primary bottleneck for hosting an increasing number of larger-scale GPU tasks. While demand paging extends capacity via host DRAM, it incurs up to 78x slowdown due to the massive working sets and poor locality of GPU workloads. We observe, however, that GPU memory access patterns are inherently predictable via kernel launch arguments and their asynchronous execution nature. Leveraging this, we propose MSched, an OS-level scheduler that extends GPU context switching to include proactive working set preparation, thereby coalescing fragmented, eventual, and expensive page faults into a single efficient migration. MSched employs a template-based approach to predict working sets with near-perfect accuracy and proposes a co-design between task scheduler and memory manager to enforce a globally optimal page placement policy. Evaluation demonstrates that MSched outperforms demand paging by up to 11.05x for scientific and deep learning workloads, and 57.88x for LLM under memory oversubscription.", "AI": {"tldr": "MSched\u901a\u8fc7\u9884\u6d4bGPU\u5de5\u4f5c\u96c6\u5e76\u4f18\u5316\u9875\u9762\u8fc1\u79fb\uff0c\u663e\u8457\u63d0\u5347\u5185\u5b58\u8fc7\u8f7d\u573a\u666f\u4e0b\u7684\u6027\u80fd\u3002", "motivation": "HBM\u5bb9\u91cf\u9650\u5236\u5bfc\u81f4GPU\u4efb\u52a1\u6027\u80fd\u4e0b\u964d\uff0c\u4f20\u7edf\u6309\u9700\u5206\u9875\u56e0\u5de5\u4f5c\u96c6\u5927\u548c\u5c40\u90e8\u6027\u5dee\u9020\u6210\u4e25\u91cd\u5ef6\u8fdf\u3002", "method": "\u63d0\u51fa\u57fa\u4e8e\u5185\u6838\u53c2\u6570\u9884\u6d4b\u5de5\u4f5c\u96c6\u7684\u6a21\u677f\u65b9\u6cd5\uff0c\u5e76\u534f\u540c\u4efb\u52a1\u8c03\u5ea6\u4e0e\u5185\u5b58\u7ba1\u7406\u5b9e\u73b0\u5168\u5c40\u6700\u4f18\u9875\u9762\u5e03\u5c40\u3002", "result": "\u5728\u79d1\u5b66\u8ba1\u7b97\u3001\u6df1\u5ea6\u5b66\u4e60\u548c\u5927\u8bed\u8a00\u6a21\u578b\u8d1f\u8f7d\u4e2d\uff0c\u6027\u80fd\u5206\u522b\u63d0\u534711.05\u500d\u548c57.88\u500d\u3002", "conclusion": "MSched\u6709\u6548\u7f13\u89e3GPU\u5185\u5b58\u74f6\u9888\uff0c\u4e3a\u5927\u89c4\u6a21\u4efb\u52a1\u63d0\u4f9b\u9ad8\u6548\u5185\u5b58\u7ba1\u7406\u65b9\u6848\u3002"}}
{"id": "2512.24530", "categories": ["cs.SE", "cs.PF"], "pdf": "https://arxiv.org/pdf/2512.24530", "abs": "https://arxiv.org/abs/2512.24530", "authors": ["Nikolaos Mavrogeorgis", "Christos Vasiladiotis", "Pei Mu", "Amir Khordadi", "Bj\u00f6rn Franke", "Antonio Barbalace"], "title": "A Magnified View into Heterogeneous-ISA Thread Migration Performance without State Transformation", "comment": null, "summary": "Heterogeneous-ISA processor designs have attracted considerable research interest. However, unlike their homogeneous-ISA counterparts, explicit software support for bridging ISA heterogeneity is required. The lack of a compilation toolchain ready to support heterogeneous-ISA targets has been a major factor hindering research in this exciting emerging area. For any such compiler, \"getting right\" the mechanics involved in state transformation upon migration and doing this efficiently is of critical importance. In particular, any runtime conversion of the current program stack from one architecture to another would be prohibitively expensive. In this paper, we design and develop Unifico, a new multi-ISA compiler that generates binaries that maintain the same stack layout during their execution on either architecture. Unifico avoids the need for runtime stack transformation, thus eliminating overheads associated with ISA migration. Additional responsibilities of the Unifico compiler backend include maintenance of a uniform ABI and virtual address space across ISAs. Unifico is implemented using the LLVM compiler infrastructure, and we are currently targeting the x86-64 and ARMv8 ISAs. We have evaluated Unifico across a range of compute-intensive NAS benchmarks and show its minimal impact on overall execution time, where less than 6% (10%) overhead is introduced on average for high-end (low-end) processors. We also analyze the performance impact of Unifico's key design features and demonstrate that they can be further optimized to mitigate this impact. When compared against the state-of-the-art Popcorn compiler, Unifico reduces binary size overhead from ~200% to ~10%, whilst eliminating the stack transformation overhead during ISA migration.", "AI": {"tldr": "Unifico\u662f\u4e00\u4e2a\u65b0\u578b\u591aISA\u7f16\u8bd1\u5668\uff0c\u901a\u8fc7\u7edf\u4e00\u6808\u5e03\u5c40\u3001ABI\u548c\u865a\u62df\u5730\u5740\u7a7a\u95f4\uff0c\u6d88\u9664\u5f02\u6784ISA\u8fc1\u79fb\u65f6\u7684\u8fd0\u884c\u65f6\u5f00\u9500\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6848\u3002", "motivation": "\u5f53\u524d\u7f3a\u4e4f\u652f\u6301\u5f02\u6784ISA\u76ee\u6807\u7684\u7f16\u8bd1\u5de5\u5177\u94fe\uff0c\u963b\u788d\u4e86\u8be5\u9886\u57df\u7684\u7814\u7a76\u8fdb\u5c55\uff0c\u5c24\u5176\u9700\u8981\u9ad8\u6548\u89e3\u51b3\u8fc1\u79fb\u65f6\u72b6\u6001\u8f6c\u6362\u95ee\u9898\u3002", "method": "\u57fa\u4e8eLLVM\u5b9e\u73b0Unifico\u7f16\u8bd1\u5668\uff0c\u9488\u5bf9x86-64\u4e0eARMv8\u67b6\u6784\uff0c\u786e\u4fdd\u6267\u884c\u671f\u95f4\u6808\u5e03\u5c40\u4e00\u81f4\uff0c\u907f\u514d\u8fd0\u884c\u65f6\u6808\u8f6c\u6362\uff0c\u5e76\u7ef4\u62a4\u7edf\u4e00ABI\u4e0e\u865a\u62df\u5730\u5740\u7a7a\u95f4\u3002", "result": "\u5728NAS\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cUnifico\u5e73\u5747\u4ec5\u5f15\u5165\u4f4e\u4e8e6%\uff08\u9ad8\u7aef\uff09\u621610%\uff08\u4f4e\u7aef\uff09\u6027\u80fd\u5f00\u9500\uff0c\u76f8\u6bd4Popcorn\u7f16\u8bd1\u5668\u5c06\u4e8c\u8fdb\u5236\u4f53\u79ef\u5f00\u9500\u4ece\u7ea6200%\u964d\u81f3\u7ea610%\uff0c\u5e76\u5b8c\u5168\u6d88\u9664\u6808\u8f6c\u6362\u5f00\u9500\u3002", "conclusion": "Unifico\u6709\u6548\u89e3\u51b3\u4e86\u5f02\u6784ISA\u8fc1\u79fb\u4e2d\u7684\u5173\u952e\u6027\u80fd\u74f6\u9888\uff0c\u5176\u8bbe\u8ba1\u5177\u5907\u8fdb\u4e00\u6b65\u4f18\u5316\u6f5c\u529b\uff0c\u4e3a\u5f02\u6784\u8ba1\u7b97\u7814\u7a76\u63d0\u4f9b\u4e86\u5b9e\u7528\u7f16\u8bd1\u57fa\u7840\u8bbe\u65bd\u3002"}}
{"id": "2512.23969", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2512.23969", "abs": "https://arxiv.org/abs/2512.23969", "authors": ["Yaoyun Zhou", "Qian Wang"], "title": "HERO-Sign: Hierarchical Tuning and Efficient Compiler-Time GPU Optimizations for SPHINCS+ Signature Generation", "comment": "accepted by HPCA 2026", "summary": "SPHINCS+ is a stateless hash-based signature scheme that provides strong post quantum security, but its signature generation is slow due to intensive hash computations. GPUs offer massive parallelism that can potentially accelerate SPHINCS+ signatures. However, existing GPU-based optimizations either fail to fully exploit the inherent parallelism of SPHINCS+'s Merkle tree structure or lack fine-grained, compiler-level customization across its diverse computational kernels. This paper proposes HERO Sign, a GPU-accelerated SPHINCS+ implementation that adopts hierarchical tuning and efficient compiler time optimizations. HERO Sign reexamines the parallelization opportunities enabled by data independence across SPHINCS+ components, including FORS, MSS, and WOTS+. It introduces a Tree Fusion strategy for FORS, which contains a large number of independent branches. The fusion strategy is guided by an automated Tree Tuning search algorithm that adapts fusion schemes to different GPU architectures. To further improve performance, HERO Sign employs an adaptive compilation strategy that accounts for the varying effectiveness of compiler optimizations across SPHINCS+ kernels such as FORS Sign, TREE Sign, and WOTS+ Sign. During compilation, the strategy automatically selects between PTX and native code paths to maximize efficiency. For batched signature generation, HERO Sign optimizes kernel-level overlapping using a task graph-based construction to reduce multi-stream idle time and kernel launch overhead. Experimental results show that, compared to state of the art GPU implementations, HERO Sign achieves throughput improvements of 1.28-3.13, 1.28-2.92, and 1.24-2.60 under the SPHINCS+ 128f, 192f, and 256f parameter sets on RTX 4090. Similar gains are observed on A100, H100, and GTX 2080, along with a two orders of magnitude reduction in kernel launch latency.", "AI": {"tldr": "HERO Sign\u901a\u8fc7\u5206\u5c42\u8c03\u4f18\u548c\u7f16\u8bd1\u5668\u4f18\u5316\u663e\u8457\u52a0\u901f\u4e86SPHINCS+\u5728GPU\u4e0a\u7684\u7b7e\u540d\u751f\u6210\u3002", "motivation": "\u73b0\u6709GPU\u4f18\u5316\u672a\u80fd\u5145\u5206\u6316\u6398SPHINCS+\u7684\u5e76\u884c\u6f5c\u529b\u6216\u7f3a\u4e4f\u7ec6\u7c92\u5ea6\u7f16\u8bd1\u5b9a\u5236\u3002", "method": "\u63d0\u51faTree Fusion\u7b56\u7565\u4e0e\u81ea\u9002\u5e94\u7f16\u8bd1\uff0c\u7ed3\u5408\u4efb\u52a1\u56fe\u51cf\u5c11\u6838\u542f\u52a8\u5f00\u9500\u3002", "result": "\u5728RTX 4090\u7b49GPU\u4e0a\u5b9e\u73b01.24-3.13\u500d\u541e\u5410\u63d0\u5347\uff0c\u6838\u542f\u52a8\u5ef6\u8fdf\u964d\u4f4e\u4e24\u4e2a\u6570\u91cf\u7ea7\u3002", "conclusion": "HERO Sign\u9ad8\u6548\u5229\u7528GPU\u67b6\u6784\u7279\u6027\uff0c\u4e3a\u540e\u91cf\u5b50\u7b7e\u540d\u63d0\u4f9b\u5b9e\u7528\u5316\u52a0\u901f\u65b9\u6848\u3002"}}
{"id": "2512.25065", "categories": ["cs.OS", "cs.AI", "cs.DC"], "pdf": "https://arxiv.org/pdf/2512.25065", "abs": "https://arxiv.org/abs/2512.25065", "authors": ["Rohit Dwivedula", "Divyanshu Saxena", "Sujay Yadalam", "Daehyeok Kim", "Aditya Akella"], "title": "Vulcan: Instance-Optimal Systems Heuristics Through LLM-Driven Search", "comment": "27 pages, 11 figures, 7 tables", "summary": "Resource-management tasks in modern operating and distributed systems continue to rely primarily on hand-designed heuristics for tasks such as scheduling, caching, or active queue management. Designing performant heuristics is an expensive, time-consuming process that we are forced to continuously go through due to the constant flux of hardware, workloads and environments.\n  We propose a new alternative: synthesizing instance-optimal heuristics -- specialized for the exact workloads and hardware where they will be deployed -- using code-generating large language models (LLMs). To make this synthesis tractable, Vulcan separates policy and mechanism through LLM-friendly, task-agnostic interfaces. With these interfaces, users specify the inputs and objectives of their desired policy, while Vulcan searches for performant policies via evolutionary search over LLM-generated code. This interface is expressive enough to capture a wide range of system policies, yet sufficiently constrained to allow even small, inexpensive LLMs to generate correct and executable code.\n  We use Vulcan to synthesize performant heuristics for cache eviction and memory tiering, and find that these heuristics outperform all human-designed state-of-the-art algorithms by upto 69% and 7.9% in performance for each of these tasks respectively.", "AI": {"tldr": "Vulcan\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\u81ea\u52a8\u751f\u6210\u9488\u5bf9\u7279\u5b9a\u5de5\u4f5c\u8d1f\u8f7d\u548c\u786c\u4ef6\u4f18\u5316\u7684\u8d44\u6e90\u7ba1\u7406\u542f\u53d1\u5f0f\u7b97\u6cd5\uff0c\u663e\u8457\u8d85\u8d8a\u4eba\u5de5\u8bbe\u8ba1\u7684\u7b97\u6cd5\u6027\u80fd\u3002", "motivation": "\u4f20\u7edf\u8d44\u6e90\u7ba1\u7406\u4f9d\u8d56\u624b\u5de5\u8bbe\u8ba1\u7684\u542f\u53d1\u5f0f\u65b9\u6cd5\uff0c\u5f00\u53d1\u6210\u672c\u9ad8\u4e14\u96be\u4ee5\u9002\u5e94\u4e0d\u65ad\u53d8\u5316\u7684\u786c\u4ef6\u548c\u5de5\u4f5c\u8d1f\u8f7d\u73af\u5883\u3002", "method": "\u901a\u8fc7\u5c06\u7b56\u7565\u4e0e\u673a\u5236\u5206\u79bb\uff0cVulcan\u4f7f\u7528\u8fdb\u5316\u641c\u7d22\u5728LLM\u751f\u6210\u7684\u4ee3\u7801\u4e2d\u5bfb\u627e\u9ad8\u6027\u80fd\u7b56\u7565\uff0c\u5e76\u91c7\u7528\u4efb\u52a1\u65e0\u5173\u63a5\u53e3\u7b80\u5316\u751f\u6210\u8fc7\u7a0b\u3002", "result": "\u5728\u7f13\u5b58\u6dd8\u6c70\u548c\u5185\u5b58\u5206\u5c42\u4efb\u52a1\u4e2d\uff0cVulcan\u751f\u6210\u7684\u542f\u53d1\u5f0f\u7b97\u6cd5\u5206\u522b\u6bd4\u73b0\u6709\u6700\u4f18\u4eba\u5de5\u7b97\u6cd5\u63d0\u534769%\u548c7.9%\u6027\u80fd\u3002", "conclusion": "Vulcan\u5c55\u793a\u4e86\u5229\u7528LLM\u81ea\u52a8\u5408\u6210\u5b9e\u4f8b\u6700\u4f18\u542f\u53d1\u5f0f\u7b97\u6cd5\u7684\u53ef\u884c\u6027\u4e0e\u4f18\u8d8a\u6027\uff0c\u4e3a\u7cfb\u7edf\u8d44\u6e90\u7ba1\u7406\u63d0\u4f9b\u65b0\u8303\u5f0f\u3002"}}
{"id": "2512.23952", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2512.23952", "abs": "https://arxiv.org/abs/2512.23952", "authors": ["Yongmin Zhang", "Pengyu Huang", "Mingyi Dong", "Jing Yao"], "title": "Squeezing Edge Performance: A Sensitivity-Aware Container Management for Heterogeneous Tasks", "comment": null, "summary": "Edge computing enables latency-critical applications to process data close to end devices, yet task heterogeneity and limited resources pose significant challenges to efficient orchestration. This paper presents a measurement-driven, container-based resource management framework for intra-node optimization on a single edge server hosting multiple heterogeneous applications. Extensive profiling experiments are conducted to derive a nonlinear fitting model that characterizes the relationship among CPU/memory allocations and processing latency across diverse workloads, enabling reliable estimation of performance under varying configurations and providing quantitative support for subsequent optimization. Using this model and a queueing-based delay formulation, we formulate a mixed-integer nonlinear programming (MINLP) problem to jointly minimize system latency and power consumption, which is shown to be NP-hard. The problem is decomposed into tractable convex subproblems and solved through a two-stage container-based resource management scheme (CRMS) combining convex optimization and greedy refinement. The proposed scheme achieves polynomial-time complexity and supports quasi-dynamic execution under global resource constraints. Simulation results demonstrate that CRMS reduces latency by over 14\\% and improves energy efficiency compared with heuristic and search-based baselines, offering a practical and scalable solution for heterogeneous edge environments with dynamic workload characteristics.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5bb9\u5668\u7684\u8d44\u6e90\u7ba1\u7406\u6846\u67b6CRMS\uff0c\u901a\u8fc7\u975e\u7ebf\u6027\u62df\u5408\u5efa\u6a21\u4e0e\u51f8\u4f18\u5316\uff0c\u5728\u5355\u8fb9\u7f18\u8282\u70b9\u4e0a\u8054\u5408\u4f18\u5316\u5ef6\u8fdf\u4e0e\u80fd\u8017\uff0c\u663e\u8457\u4f18\u4e8e\u542f\u53d1\u5f0f\u65b9\u6cd5\u3002", "motivation": "\u8fb9\u7f18\u8ba1\u7b97\u4e2d\u4efb\u52a1\u5f02\u6784\u6027\u4e0e\u8d44\u6e90\u53d7\u9650\u5bfc\u81f4\u8c03\u5ea6\u56f0\u96be\uff0c\u9700\u9ad8\u6548\u8d44\u6e90\u7ba1\u7406\u65b9\u6848\u652f\u6301\u591a\u6837\u5316\u5e94\u7528\u3002", "method": "\u901a\u8fc7\u5b9e\u9a8c\u6784\u5efaCPU/\u5185\u5b58\u5206\u914d\u4e0e\u5ef6\u8fdf\u5173\u7cfb\u7684\u975e\u7ebf\u6027\u6a21\u578b\uff0c\u5efa\u7acbMINLP\u95ee\u9898\u5e76\u5206\u89e3\u4e3a\u51f8\u5b50\u95ee\u9898\uff0c\u91c7\u7528\u4e24\u9636\u6bb5\u5bb9\u5668\u8d44\u6e90\u7ba1\u7406\u65b9\u6848\u6c42\u89e3\u3002", "result": "\u4eff\u771f\u663e\u793aCRMS\u964d\u4f4e\u5ef6\u8fdf\u8d8514%\uff0c\u63d0\u5347\u80fd\u6548\uff0c\u5177\u5907\u591a\u9879\u5f0f\u65f6\u95f4\u590d\u6742\u5ea6\u548c\u51c6\u52a8\u6001\u6267\u884c\u80fd\u529b\u3002", "conclusion": "CRMS\u4e3a\u5f02\u6784\u8fb9\u7f18\u73af\u5883\u63d0\u4f9b\u5b9e\u7528\u3001\u53ef\u6269\u5c55\u7684\u8d44\u6e90\u4f18\u5316\u65b9\u6848\uff0c\u6709\u6548\u5e94\u5bf9\u52a8\u6001\u8d1f\u8f7d\u6311\u6218\u3002"}}
{"id": "2512.23901", "categories": ["cs.NI"], "pdf": "https://arxiv.org/pdf/2512.23901", "abs": "https://arxiv.org/abs/2512.23901", "authors": ["Bradley Fang", "Michael Roger"], "title": "Road Rules for Radio: Why Your Wi-Fi Got Better", "comment": null, "summary": "WiFi allows for the connection of devices and people around the globe. It has proven to be a monumental and revolutionary tool that keeps the world connected. However, recent WiFi advancements are numerous and at times confusing. WiFi has grown significantly over the years, yet few understand the scope and scale of WiFi progression as a whole. This paper tackles that problem, providing a broad literature review on the advancements of key WiFi features to date. This paper will center on seven key areas of focus: (1) bandwidth, (2) battery life, (3) traffic collisions, (4) interference, (5) data-intensive transmissions, (6) numerous devices, and (7) peak throughput/modulation. Each section will focus on WiFi's problems, how those problems were fixed, as well as the limitations of existing solutions. Moreover, the paper explains the role of new unreleased technologies in these seven areas. This includes exploring the upcoming WiFi 8 standard based on the IEEE 802.11bn \"Ultra High Reliability\" (UHR) specification and how it builds upon current specifications. Compared to previous specifications, WiFi 8 marks a stronger and more significant shift toward prioritizing reliability over pure data rates. Beyond a sole literature review, this paper uses a novel analogy. A road/highway analogy will be integrated throughout the paper to facilitate understanding of networking mechanisms. This paper is approachable and is written such that someone with very little WiFi knowledge should come away with a strong understanding of WiFi. As is typical of literature review papers, technical claims will be grounded in prior work.", "AI": {"tldr": "\u672c\u6587\u901a\u8fc7\u6587\u732e\u7efc\u8ff0\u548c\u516c\u8def\u7c7b\u6bd4\uff0c\u7cfb\u7edf\u68b3\u7406WiFi\u5173\u952e\u6280\u672f\u6f14\u8fdb\uff0c\u805a\u7126\u4e03\u5927\u6838\u5fc3\u95ee\u9898\uff0c\u5e76\u5c55\u671bWiFi 8\u6807\u51c6\u5bf9\u53ef\u9760\u6027\u7684\u5f3a\u5316\u3002", "motivation": "\u5398\u6e05\u8fd1\u5e74\u7e41\u6742\u7684WiFi\u6280\u672f\u8fdb\u5c55\uff0c\u5e2e\u52a9\u8bfb\u8005\u5168\u9762\u7406\u89e3\u5176\u53d1\u5c55\u8109\u7edc\u4e0e\u672a\u6765\u65b9\u5411\u3002", "method": "\u56f4\u7ed5\u5e26\u5bbd\u3001\u7eed\u822a\u3001\u78b0\u649e\u3001\u5e72\u6270\u7b49\u4e03\u4e2a\u7ef4\u5ea6\u5c55\u5f00\u6587\u732e\u7efc\u8ff0\uff0c\u8f85\u4ee5\u516c\u8def\u7c7b\u6bd4\u589e\u5f3a\u53ef\u8bfb\u6027\uff0c\u5e76\u5f15\u7528\u65e2\u6709\u7814\u7a76\u652f\u6491\u6280\u672f\u8bba\u70b9\u3002", "result": "\u6e05\u6670\u5448\u73b0\u5404\u95ee\u9898\u7684\u5386\u53f2\u89e3\u51b3\u65b9\u6848\u4e0e\u73b0\u5b58\u5c40\u9650\uff0c\u540c\u65f6\u9610\u91ca\u57fa\u4e8eIEEE 802.11bn\u7684WiFi 8\u5982\u4f55\u8f6c\u5411\u53ef\u9760\u6027\u4f18\u5148\u3002", "conclusion": "WiFi 8\u6807\u5fd7\u7740\u4ece\u8ffd\u6c42\u901f\u7387\u5411\u4fdd\u969c\u53ef\u9760\u6027\u7684\u6218\u7565\u8f6c\u53d8\uff0c\u672c\u6587\u4e3a\u975e\u4e13\u4e1a\u8bfb\u8005\u63d0\u4f9b\u4e86\u7cfb\u7edf\u4e14\u6613\u61c2\u7684\u6280\u672f\u6f14\u8fdb\u56fe\u666f\u3002"}}
{"id": "2512.23782", "categories": ["cs.SE", "cs.CY"], "pdf": "https://arxiv.org/pdf/2512.23782", "abs": "https://arxiv.org/abs/2512.23782", "authors": ["Kessia Nepomuceno", "Fabio Petrillo"], "title": "A Systematic Mapping on Software Fairness: Focus, Trends and Industrial Context", "comment": null, "summary": "Context: Fairness in systems has emerged as a critical concern in software engineering, garnering increasing attention as the field has advanced in recent years. While several guidelines have been proposed to address fairness, achieving a comprehensive understanding of research solutions for ensuring fairness in software systems remains challenging. Objectives: This paper presents a systematic literature mapping to explore and categorize current advancements in fairness solutions within software engineering, focusing on three key dimensions: research trends, research focus, and viability in industrial contexts. Methods: We develop a classification framework to organize research on software fairness from a fresh perspective, applying it to 95 selected studies and analyzing their potential for industrial adoption. Results: Our findings reveal that software fairness research is expanding, yet it remains heavily focused on methods and algorithms. It primarily focuses on post-processing and group fairness, with less emphasis on early-stage interventions, individual fairness metrics, and understanding bias root causes. Additionally fairness research remains largely academic, with limited industry collaboration and low to medium Technology Readiness Level (TRL), indicating that industrial transferability remains distant. Conclusion: Our results underscore the need to incorporate fairness considerations across all stages of the software development life-cycle and to foster greater collaboration between academia and industry. This analysis provides a comprehensive overview of the field, offering a foundation to guide future research and practical applications of fairness in software systems.", "AI": {"tldr": "\u672c\u6587\u7cfb\u7edf\u68b3\u7406\u4e86\u8f6f\u4ef6\u5de5\u7a0b\u4e2d\u516c\u5e73\u6027\u7814\u7a76\u7684\u73b0\u72b6\uff0c\u6307\u51fa\u5f53\u524d\u7814\u7a76\u591a\u96c6\u4e2d\u4e8e\u7b97\u6cd5\u4e0e\u540e\u5904\u7406\u9636\u6bb5\uff0c\u7f3a\u4e4f\u65e9\u671f\u5e72\u9884\u4e0e\u4ea7\u4e1a\u843d\u5730\uff0c\u547c\u5401\u5168\u751f\u547d\u5468\u671f\u6574\u5408\u53ca\u4ea7\u5b66\u5408\u4f5c\u3002", "motivation": "\u8f6f\u4ef6\u7cfb\u7edf\u4e2d\u7684\u516c\u5e73\u6027\u65e5\u76ca\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u7814\u7a76\u5206\u6563\u4e14\u5de5\u4e1a\u9002\u7528\u6027\u4f4e\uff0c\u4e9f\u9700\u7cfb\u7edf\u6027\u7efc\u8ff0\u4ee5\u6307\u5bfc\u672a\u6765\u65b9\u5411\u3002", "method": "\u6784\u5efa\u5206\u7c7b\u6846\u67b6\uff0c\u5bf995\u7bc7\u6587\u732e\u8fdb\u884c\u7cfb\u7edf\u6620\u5c04\u5206\u6790\uff0c\u4ece\u7814\u7a76\u8d8b\u52bf\u3001\u7126\u70b9\u4e0e\u5de5\u4e1a\u53ef\u884c\u6027\u4e09\u7ef4\u5ea6\u8bc4\u4f30\u3002", "result": "\u7814\u7a76\u5448\u589e\u957f\u8d8b\u52bf\uff0c\u4f46\u504f\u91cd\u7b97\u6cd5\u4e0e\u7fa4\u4f53\u516c\u5e73\uff0c\u5ffd\u89c6\u4e2a\u4f53\u516c\u5e73\u4e0e\u6839\u6e90\u5206\u6790\uff1b\u5b66\u672f\u4e3b\u5bfc\uff0cTRL\u504f\u4f4e\uff0c\u4ea7\u4e1a\u8f6c\u5316\u9065\u8fdc\u3002", "conclusion": "\u9700\u5728\u8f6f\u4ef6\u5f00\u53d1\u751f\u547d\u5468\u671f\u5404\u9636\u6bb5\u878d\u5165\u516c\u5e73\u6027\u8003\u91cf\uff0c\u5e76\u52a0\u5f3a\u4ea7\u5b66\u534f\u4f5c\uff0c\u4ee5\u63a8\u52a8\u516c\u5e73\u6027\u7814\u7a76\u7684\u5b9e\u9645\u5e94\u7528\u3002"}}
{"id": "2512.24449", "categories": ["cs.DC", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.24449", "abs": "https://arxiv.org/abs/2512.24449", "authors": ["Bo Jiang", "Taolue Yang", "Youyuan Liu", "Xubin He", "Sheng Di", "Sian Jin"], "title": "PackKV: Reducing KV Cache Memory Footprint through LLM-Aware Lossy Compression", "comment": null, "summary": "Transformer-based large language models (LLMs) have demonstrated remarkable potential across a wide range of practical applications. However, long-context inference remains a significant challenge due to the substantial memory requirements of the key-value (KV) cache, which can scale to several gigabytes as sequence length and batch size increase. In this paper, we present \\textbf{PackKV}, a generic and efficient KV cache management framework optimized for long-context generation. %, which synergistically supports both latency-critical and throughput-critical inference scenarios. PackKV introduces novel lossy compression techniques specifically tailored to the characteristics of KV cache data, featuring a careful co-design of compression algorithms and system architecture. Our approach is compatible with the dynamically growing nature of the KV cache while preserving high computational efficiency. Experimental results show that, under the same and minimum accuracy drop as state-of-the-art quantization methods, PackKV achieves, on average, \\textbf{153.2}\\% higher memory reduction rate for the K cache and \\textbf{179.6}\\% for the V cache. Furthermore, PackKV delivers extremely high execution throughput, effectively eliminating decompression overhead and accelerating the matrix-vector multiplication operation. Specifically, PackKV achieves an average throughput improvement of \\textbf{75.7}\\% for K and \\textbf{171.7}\\% for V across A100 and RTX Pro 6000 GPUs, compared to cuBLAS matrix-vector multiplication kernels, while demanding less GPU memory bandwidth. Code available on https://github.com/BoJiang03/PackKV", "AI": {"tldr": "PackKV\u662f\u4e00\u79cd\u9ad8\u6548\u7684KV\u7f13\u5b58\u7ba1\u7406\u6846\u67b6\uff0c\u901a\u8fc7\u5b9a\u5236\u538b\u7f29\u7b97\u6cd5\u548c\u7cfb\u7edf\u67b6\u6784\u534f\u540c\u8bbe\u8ba1\uff0c\u5728\u4fdd\u6301\u9ad8\u8ba1\u7b97\u6548\u7387\u7684\u540c\u65f6\u663e\u8457\u964d\u4f4e\u5185\u5b58\u5360\u7528\u5e76\u63d0\u5347\u541e\u5410\u91cf\u3002", "motivation": "\u957f\u4e0a\u4e0b\u6587\u63a8\u7406\u4e2dKV\u7f13\u5b58\u5185\u5b58\u9700\u6c42\u5de8\u5927\uff0c\u9650\u5236\u4e86Transformer\u5927\u6a21\u578b\u7684\u5b9e\u9645\u5e94\u7528\u3002", "method": "\u63d0\u51faPackKV\u6846\u67b6\uff0c\u7ed3\u5408\u4e13\u4e3aKV\u7f13\u5b58\u6570\u636e\u8bbe\u8ba1\u7684\u6709\u635f\u538b\u7f29\u6280\u672f\u4e0e\u7cfb\u7edf\u67b6\u6784\u534f\u540c\u4f18\u5316\u3002", "result": "\u76f8\u6bd4\u73b0\u6709\u91cf\u5316\u65b9\u6cd5\uff0cK\u7f13\u5b58\u5185\u5b58\u51cf\u5c11\u7387\u63d0\u9ad8153.2%\uff0cV\u7f13\u5b58\u63d0\u9ad8179.6%\uff1b\u5728A100\u548cRTX Pro 6000\u4e0a\uff0cK/V\u541e\u5410\u91cf\u5206\u522b\u5e73\u5747\u63d0\u534775.7%\u548c171.7%\u3002", "conclusion": "PackKV\u6709\u6548\u7f13\u89e3\u957f\u5e8f\u5217\u751f\u6210\u4e2d\u7684\u5185\u5b58\u74f6\u9888\uff0c\u540c\u65f6\u5927\u5e45\u63d0\u5347\u6267\u884c\u6548\u7387\uff0c\u9002\u7528\u4e8e\u5ef6\u8fdf\u654f\u611f\u548c\u541e\u5410\u91cf\u654f\u611f\u573a\u666f\u3002"}}
{"id": "2512.24049", "categories": ["cs.NI"], "pdf": "https://arxiv.org/pdf/2512.24049", "abs": "https://arxiv.org/abs/2512.24049", "authors": ["Negin Doostar", "Mohammad Reza Heidarpour", "Amir Khorsandi"], "title": "Beyond Dedicated-Active: A General Reliability Provisioning Framework for SFC Placement in Fog Computing", "comment": null, "summary": "The explosive growth of Internet of Things (IoT) devices has strained traditional cloud infrastructures, highlighting the need for low-latency and energy-efficient alternatives. Fog computing addresses this by placing computation near the network edge. However, limited and heterogeneous fog resources pose reliability challenges, especially for mission-critical applications. On the other hand, to improve flexibility, applications are deployed as Service Function Chains (SFCs), where each function runs as a Virtual Network Function (VNF). While scalable, this approach is more failure-prone than monolithic deployments, necessitating intelligent redundancy and placement strategies. This paper addresses the reliability-aware SFC placement problem over heterogeneous fog servers through the lens of reliability theory. We explore four redundancy strategies, combining shared vs. dedicated and active vs. standby modes, and propose a general framework to minimize latency and cost while meeting reliability and deadline constraints. The problem is formulated as an Integer Non-Linear Program (INLP), and two genetic algorithm (GA)-based solutions are developed. Simulation results show that shared-standby redundancy outperforms the conventional dedicated-active approach by up to 84%.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u57fa\u4e8e\u53ef\u9760\u6027\u7406\u8bba\u7684SFC\u90e8\u7f72\u6846\u67b6\uff0c\u7ed3\u5408\u56db\u79cd\u5197\u4f59\u7b56\u7565\uff0c\u5728\u5f02\u6784\u96fe\u670d\u52a1\u5668\u4e0a\u4f18\u5316\u5ef6\u8fdf\u4e0e\u6210\u672c\uff0c\u540c\u65f6\u6ee1\u8db3\u53ef\u9760\u6027\u7ea6\u675f\u3002", "motivation": "\u7269\u8054\u7f51\u8bbe\u5907\u6fc0\u589e\u5bfc\u81f4\u4f20\u7edf\u4e91\u67b6\u6784\u538b\u529b\u589e\u5927\uff0c\u96fe\u8ba1\u7b97\u867d\u80fd\u964d\u4f4e\u5ef6\u8fdf\uff0c\u4f46\u8d44\u6e90\u5f02\u6784\u6027\u5f71\u54cd\u53ef\u9760\u6027\uff0c\u9700\u667a\u80fd\u5197\u4f59\u4e0e\u90e8\u7f72\u65b9\u6848\u3002", "method": "\u6784\u5efa\u6574\u6570\u975e\u7ebf\u6027\u89c4\u5212\u6a21\u578b\uff0c\u91c7\u7528\u9057\u4f20\u7b97\u6cd5\u6c42\u89e3\uff0c\u6bd4\u8f83\u5171\u4eab/\u4e13\u7528\u3001\u4e3b\u52a8/\u5907\u7528\u56db\u79cd\u5197\u4f59\u7b56\u7565\u3002", "result": "\u4eff\u771f\u8868\u660e\uff0c\u5171\u4eab-\u5907\u7528\u5197\u4f59\u7b56\u7565\u76f8\u6bd4\u4f20\u7edf\u4e13\u7528-\u4e3b\u52a8\u65b9\u6848\u6027\u80fd\u63d0\u5347\u6700\u9ad8\u8fbe84%\u3002", "conclusion": "\u6240\u63d0\u6846\u67b6\u80fd\u6709\u6548\u5e73\u8861\u96fe\u73af\u5883\u4e2dSFC\u90e8\u7f72\u7684\u5ef6\u8fdf\u3001\u6210\u672c\u4e0e\u53ef\u9760\u6027\u9700\u6c42\uff0c\u5171\u4eab-\u5907\u7528\u7b56\u7565\u8868\u73b0\u6700\u4f18\u3002"}}
{"id": "2512.24511", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2512.24511", "abs": "https://arxiv.org/abs/2512.24511", "authors": ["Mikaila J. Gossman", "Avinash Maurya", "Bogdan Nicolae", "Jon C. Calhoun"], "title": "Understanding LLM Checkpoint/Restore I/O Strategies and Patterns", "comment": "SCA/HPCAsia 2026 Workshops: Supercomputing Asia and International Conference on High Performance Computing in the Asia Pacific Region Workshops", "summary": "As LLMs and foundation models scale, checkpoint/restore has become a critical pattern for training and inference. With 3D parallelism (tensor, pipeline, data), checkpointing involves many processes, each managing numerous tensors of varying shapes and sizes, that must be persisted frequently to stable storage (e.g., parallel file systems). This turns checkpoint/restore into a big-data I/O problem characterized by volume, variety, and velocity. The workflow must traverse the full storage stack -- from GPU memory through host memory and local storage to external repositories -- whose tiers differ by orders of magnitude in performance, creating bottlenecks under concurrency even with asynchronous flush/prefetch. Kernel-accelerated I/O libraries such as \\texttt{liburing} may mitigate these issues versus POSIX, but their effectiveness for LLM checkpointing remains underexplored. We develop microbenchmarks to quantify trade-offs when using \\texttt{liburing}, evaluating how aggregation, alignment, and I/O coalescing interact under buffered and direct I/O. We find that uncoalesced small-buffer operations halve throughput relative to synthetic workloads, while file system-aware aggregation restores bandwidth and reduces metadata overhead. Compared to state-of-the-art LLM checkpointing engines, our approach achieves up to $3.9\\times$ higher write throughput than DataStates-LLM and $7.6\\times$ higher than TorchSnapshot. These results highlight the need for aggregation and coalescing strategies that align with modern file systems and I/O backends.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u5728\u5927\u89c4\u6a21\u8bed\u8a00\u6a21\u578b\u8bad\u7ec3\u4e2d\u4f7f\u7528liburing\u4f18\u5316\u68c0\u67e5\u70b9I/O\u6027\u80fd\uff0c\u63d0\u51fa\u805a\u5408\u4e0e\u5bf9\u9f50\u7b56\u7565\uff0c\u663e\u8457\u63d0\u5347\u541e\u5410\u91cf\u3002", "motivation": "\u968f\u7740LLM\u89c4\u6a21\u6269\u5927\uff0c\u68c0\u67e5\u70b9/\u6062\u590d\u6210\u4e3a\u5173\u952e\u64cd\u4f5c\uff0c\u4f46\u5176I/O\u6027\u80fd\u53d7\u5b58\u50a8\u6808\u74f6\u9888\u9650\u5236\uff0c\u4e9f\u9700\u4f18\u5316\u3002", "method": "\u901a\u8fc7\u5fae\u57fa\u51c6\u6d4b\u8bd5\u5206\u6790liburing\u5728\u7f13\u51b2\u4e0e\u76f4\u63a5I/O\u4e0b\u7684\u805a\u5408\u3001\u5bf9\u9f50\u4e0eI/O\u5408\u5e76\u6548\u679c\u3002", "result": "\u76f8\u6bd4\u73b0\u6709\u5f15\u64ce\uff0c\u672c\u65b9\u6cd5\u5199\u5165\u541e\u5410\u91cf\u6700\u9ad8\u63d0\u53473.9\u500d\uff08\u5bf9\u6bd4DataStates-LLM\uff09\u548c7.6\u500d\uff08\u5bf9\u6bd4TorchSnapshot\uff09\u3002", "conclusion": "\u73b0\u4ee3\u6587\u4ef6\u7cfb\u7edf\u4e0b\uff0c\u805a\u5408\u4e0eI/O\u5408\u5e76\u7b56\u7565\u5bf9\u63d0\u5347LLM\u68c0\u67e5\u70b9\u6027\u80fd\u81f3\u5173\u91cd\u8981\u3002"}}
{"id": "2512.23875", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2512.23875", "abs": "https://arxiv.org/abs/2512.23875", "authors": ["Mohsen Hesamolhokama", "Behnam Rohani", "Amirahmad Shafiee", "MohammadAmin Fazli", "Jafar Habibi"], "title": "From Illusion to Insight: Change-Aware File-Level Software Defect Prediction Using Agentic AI", "comment": null, "summary": "Much of the reported progress in file-level software defect prediction (SDP) is, in reality, nothing but an illusion of accuracy. Over the last decades, machine learning and deep learning models have reported increasing performance across software versions. However, since most files persist across releases and retain their defect labels, standard evaluation rewards label-persistence bias rather than reasoning about code changes. To address this issue, we reformulate SDP as a change-aware prediction task, in which models reason over code changes of a file within successive project versions, rather than relying on static file snapshots. Building on this formulation, we propose an LLM-driven, change-aware, multi-agent debate framework. Our experiments on multiple PROMISE projects show that traditional models achieve inflated F1, while failing on rare but critical defect-transition cases. In contrast, our change-aware reasoning and multi-agent debate framework yields more balanced performance across evolution subsets and significantly improves sensitivity to defect introductions. These results highlight fundamental flaws in current SDP evaluation practices and emphasize the need for change-aware reasoning in practical defect prediction. The source code is publicly available.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8eLLM\u7684\u591a\u667a\u80fd\u4f53\u8fa9\u8bba\u6846\u67b6\uff0c\u7528\u4e8e\u6539\u8fdb\u6587\u4ef6\u7ea7\u7f3a\u9677\u9884\u6d4b\u4e2d\u7684\u53d8\u5316\u611f\u77e5\u80fd\u529b\uff0c\u89e3\u51b3\u4f20\u7edf\u65b9\u6cd5\u56e0\u6807\u7b7e\u6301\u4e45\u6027\u504f\u5dee\u5bfc\u81f4\u7684\u8bc4\u4f30\u5931\u771f\u95ee\u9898\u3002", "motivation": "\u4f20\u7edf\u6587\u4ef6\u7ea7\u7f3a\u9677\u9884\u6d4b\u65b9\u6cd5\u4f9d\u8d56\u9759\u6001\u5feb\u7167\uff0c\u5ffd\u89c6\u4ee3\u7801\u53d8\u66f4\uff0c\u5bfc\u81f4\u8bc4\u4f30\u7ed3\u679c\u865a\u9ad8\uff0c\u65e0\u6cd5\u6709\u6548\u6355\u6349\u5173\u952e\u7f3a\u9677\u5f15\u5165\u573a\u666f\u3002", "method": "\u5c06\u7f3a\u9677\u9884\u6d4b\u91cd\u6784\u4e3a\u53d8\u5316\u611f\u77e5\u4efb\u52a1\uff0c\u5229\u7528LLM\u9a71\u52a8\u7684\u591a\u667a\u80fd\u4f53\u8fa9\u8bba\u673a\u5236\u5bf9\u8fde\u7eed\u7248\u672c\u95f4\u7684\u4ee3\u7801\u53d8\u66f4\u8fdb\u884c\u63a8\u7406\u3002", "result": "\u5728\u591a\u4e2aPROMISE\u9879\u76ee\u4e0a\u5b9e\u9a8c\u8868\u660e\uff0c\u65b0\u65b9\u6cd5\u5728\u5404\u7c7b\u6f14\u5316\u5b50\u96c6\u4e0a\u8868\u73b0\u66f4\u5747\u8861\uff0c\u663e\u8457\u63d0\u5347\u5bf9\u7f3a\u9677\u5f15\u5165\u7684\u654f\u611f\u5ea6\u3002", "conclusion": "\u5f53\u524d\u7f3a\u9677\u9884\u6d4b\u8bc4\u4f30\u5b58\u5728\u6839\u672c\u7f3a\u9677\uff0c\u5b9e\u9645\u5e94\u7528\u4e2d\u5fc5\u987b\u5f15\u5165\u53d8\u5316\u611f\u77e5\u63a8\u7406\u673a\u5236\u3002"}}
{"id": "2512.24667", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2512.24667", "abs": "https://arxiv.org/abs/2512.24667", "authors": ["Mingyi Li", "Xiao Zhang", "Ruisheng Zheng", "Hongjian Shi", "Yuan Yuan", "Xiuzhen Cheng", "Dongxiao Yu"], "title": "Distributed Bilevel Optimization with Dual Pruning for Resource-limited Clients", "comment": null, "summary": "With the development of large-scale models, traditional distributed bilevel optimization algorithms cannot be applied directly in low-resource clients. The key reason lies in the excessive computation involved in optimizing both the lower- and upper-level functions. Thus, we present the first resource-adaptive distributed bilevel optimization framework with a second-order free hypergradient estimator, which allows each client to optimize the submodels adapted to the available resources. Due to the coupled influence of partial outer parameters x and inner parameters y, it's challenging to theoretically analyze the upper bound regarding the globally averaged hypergradient for full model parameters. The error bound of inner parameter also needs to be reformulated since the local partial training. The provable theorems show that both RABO and RAFBO can achieve an asymptotically optimal convergence rate of $O(1/\\sqrt{C_x^{\\ast}Q})$, which is dominated by the minimum coverage of the outer parameter $C_x^{\\ast}$. Extensive experiments on two different tasks demonstrate the effectiveness and computation efficiency of our proposed methods.", "AI": {"tldr": "\u63d0\u51fa\u9996\u4e2a\u8d44\u6e90\u81ea\u9002\u5e94\u5206\u5e03\u5f0f\u53cc\u5c42\u4f18\u5316\u6846\u67b6\uff0c\u663e\u8457\u63d0\u5347\u4f4e\u8d44\u6e90\u5ba2\u6237\u7aef\u7684\u8ba1\u7b97\u6548\u7387\u3002", "motivation": "\u5927\u89c4\u6a21\u6a21\u578b\u53d1\u5c55\u4e0b\uff0c\u4f20\u7edf\u53cc\u5c42\u4f18\u5316\u7b97\u6cd5\u65e0\u6cd5\u76f4\u63a5\u7528\u4e8e\u4f4e\u8d44\u6e90\u5ba2\u6237\u7aef\uff0c\u56e0\u5176\u8ba1\u7b97\u5f00\u9500\u8fc7\u5927\u3002", "method": "\u8bbe\u8ba1\u65e0\u4e8c\u9636\u8d85\u68af\u5ea6\u4f30\u8ba1\u5668\u7684\u8d44\u6e90\u81ea\u9002\u5e94\u6846\u67b6\uff0c\u652f\u6301\u5ba2\u6237\u7aef\u6839\u636e\u8d44\u6e90\u4f18\u5316\u5b50\u6a21\u578b\u3002", "result": "\u7406\u8bba\u8bc1\u660eRABO\u4e0eRAFBO\u5747\u8fbe\u5230\u6700\u4f18\u6536\u655b\u901f\u7387$O(1/\\sqrt{C_x^{\\ast}Q})$\uff0c\u5b9e\u9a8c\u9a8c\u8bc1\u5176\u9ad8\u6548\u6027\u3002", "conclusion": "\u8be5\u6846\u67b6\u5728\u4fdd\u8bc1\u6536\u655b\u6027\u7684\u540c\u65f6\uff0c\u6709\u6548\u964d\u4f4e\u8d44\u6e90\u6d88\u8017\uff0c\u9002\u7528\u4e8e\u5b9e\u9645\u4f4e\u8d44\u6e90\u573a\u666f\u3002"}}
{"id": "2512.23982", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.23982", "abs": "https://arxiv.org/abs/2512.23982", "authors": ["Hung-Fu Chang", "MohammadShokrolah Shirazi", "Lizhou Cao", "Supannika Koolmanojwong Mobasser"], "title": "Coding With AI: From a Reflection on Industrial Practices to Future Computer Science and Software Engineering Education", "comment": "21 pages, 5 figures", "summary": "Recent advances in large language models (LLMs) have introduced new paradigms in software development, including vibe coding, AI-assisted coding, and agentic coding, fundamentally reshaping how software is designed, implemented, and maintained. Prior research has primarily examined AI-based coding at the individual level or in educational settings, leaving industrial practitioners' perspectives underexplored. This paper addresses this gap by investigating how LLM coding tools are used in professional practice, the associated concerns and risks, and the resulting transformations in development workflows, with particular attention to implications for computing education. We conducted a qualitative analysis of 57 curated YouTube videos published between late 2024 and 2025, capturing reflections and experiences shared by practitioners. Following a filtering and quality assessment process, the selected sources were analyzed to compare LLM-based and traditional programming, identify emerging risks, and characterize evolving workflows. Our findings reveal definitions of AI-based coding practices, notable productivity gains, and lowered barriers to entry. Practitioners also report a shift in development bottlenecks toward code review and concerns regarding code quality, maintainability, security vulnerabilities, ethical issues, erosion of foundational problem-solving skills, and insufficient preparation of entry-level engineers. Building on these insights, we discuss implications for computer science and software engineering education and argue for curricular shifts toward problem-solving, architectural thinking, code review, and early project-based learning that integrates LLM tools. This study offers an industry-grounded perspective on AI-based coding and provides guidance for aligning educational practices with rapidly evolving professional realities.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63a2\u8ba8\u4e86\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5728\u5de5\u4e1a\u754c\u7f16\u7a0b\u5b9e\u8df5\u4e2d\u7684\u5e94\u7528\u3001\u98ce\u9669\u4e0e\u5de5\u4f5c\u6d41\u53d8\u9769\uff0c\u5e76\u63d0\u51fa\u8ba1\u7b97\u673a\u6559\u80b2\u9700\u8f6c\u5411\u95ee\u9898\u89e3\u51b3\u3001\u67b6\u6784\u601d\u7ef4\u4e0e\u4ee3\u7801\u5ba1\u67e5\u7b49\u80fd\u529b\u57f9\u517b\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u591a\u805a\u7126\u4e2a\u4f53\u6216\u6559\u80b2\u573a\u666f\u4e0b\u7684AI\u8f85\u52a9\u7f16\u7a0b\uff0c\u7f3a\u4e4f\u5bf9\u5de5\u4e1a\u4ece\u4e1a\u8005\u5b9e\u9645\u4f53\u9a8c\u4e0e\u6311\u6218\u7684\u7cfb\u7edf\u5206\u6790\u3002", "method": "\u5bf92024\u5e74\u672b\u81f32025\u5e74\u95f4\u53d1\u5e03\u768457\u4e2a\u7cbe\u9009YouTube\u89c6\u9891\u8fdb\u884c\u5b9a\u6027\u5206\u6790\uff0c\u6bd4\u8f83\u4f20\u7edf\u4e0eLLM\u7f16\u7a0b\u65b9\u5f0f\uff0c\u8bc6\u522b\u98ce\u9669\u5e76\u523b\u753b\u5de5\u4f5c\u6d41\u6f14\u53d8\u3002", "result": "\u53d1\u73b0LLM\u663e\u8457\u63d0\u5347\u751f\u4ea7\u529b\u5e76\u964d\u4f4e\u5165\u95e8\u95e8\u69db\uff0c\u4f46\u4e5f\u5f15\u53d1\u4ee3\u7801\u8d28\u91cf\u3001\u5b89\u5168\u6027\u3001\u4f26\u7406\u53ca\u57fa\u7840\u6280\u80fd\u9000\u5316\u7b49\u62c5\u5fe7\uff0c\u5f00\u53d1\u74f6\u9888\u8f6c\u5411\u4ee3\u7801\u5ba1\u67e5\u73af\u8282\u3002", "conclusion": "\u5efa\u8bae\u8ba1\u7b97\u673a\u6559\u80b2\u8bfe\u7a0b\u5e94\u878d\u5165LLM\u5de5\u5177\uff0c\u5f3a\u5316\u95ee\u9898\u89e3\u51b3\u3001\u67b6\u6784\u8bbe\u8ba1\u4e0e\u9879\u76ee\u5b9e\u8df5\uff0c\u4ee5\u9002\u5e94\u884c\u4e1a\u5feb\u901f\u53d8\u9769\u3002"}}
{"id": "2512.24452", "categories": ["cs.NI", "cs.AI", "cs.CR", "cs.IT", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.24452", "abs": "https://arxiv.org/abs/2512.24452", "authors": ["Yalin E. Sagduyu", "Tugba Erpek", "Aylin Yener", "Sennur Ulukus"], "title": "Privacy-Preserving Semantic Communications via Multi-Task Learning and Adversarial Perturbations", "comment": null, "summary": "Semantic communications conveys task-relevant meaning rather than focusing solely on message reconstruction, improving bandwidth efficiency and robustness for next-generation wireless systems. However, learned semantic representations can still leak sensitive information to unintended receivers (eavesdroppers). This paper presents a deep learning-based semantic communication framework that jointly supports multiple receiver tasks while explicitly limiting semantic leakage to an eavesdropper. The legitimate link employs a learned encoder at the transmitter, while the receiver trains decoders for semantic inference and data reconstruction. The security problem is formulated via an iterative min-max optimization in which an eavesdropper is trained to improve its semantic inference, while the legitimate transmitter-receiver pair is trained to preserve task performance while reducing the eavesdropper's success. We also introduce an auxiliary layer that superimposes a cooperative, adversarially crafted perturbation on the transmitted waveform to degrade semantic leakage to an eavesdropper. Performance is evaluated over Rayleigh fading channels with additive white Gaussian noise using MNIST and CIFAR-10 datasets. Semantic accuracy and reconstruction quality improve with increasing latent dimension, while the min-max mechanism reduces the eavesdropper's inference performance significantly without degrading the legitimate receiver. The perturbation layer is successful in reducing semantic leakage even when the legitimate link is trained only for its own task. This comprehensive framework motivates semantic communication designs with tunable, end-to-end privacy against adaptive adversaries in realistic wireless settings.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u8bed\u4e49\u901a\u4fe1\u6846\u67b6\uff0c\u901a\u8fc7\u6700\u5c0f-\u6700\u5927\u4f18\u5316\u548c\u5bf9\u6297\u6270\u52a8\u5c42\uff0c\u5728\u4fdd\u969c\u5408\u6cd5\u63a5\u6536\u65b9\u6027\u80fd\u7684\u540c\u65f6\u6709\u6548\u964d\u4f4e\u7a83\u542c\u8005\u7684\u8bed\u4e49\u4fe1\u606f\u6cc4\u9732\u3002", "motivation": "\u89e3\u51b3\u8bed\u4e49\u901a\u4fe1\u4e2d\u654f\u611f\u4fe1\u606f\u53ef\u80fd\u88ab\u7a83\u542c\u8005\u83b7\u53d6\u7684\u95ee\u9898\uff0c\u63d0\u5347\u65e0\u7ebf\u7cfb\u7edf\u5b89\u5168\u6027\u3002", "method": "\u91c7\u7528\u6df1\u5ea6\u5b66\u4e60\u67b6\u6784\uff0c\u7ed3\u5408\u6700\u5c0f-\u6700\u5927\u4f18\u5316\u7b56\u7565\u4e0e\u5bf9\u6297\u6270\u52a8\u5c42\uff0c\u8bad\u7ec3\u5408\u6cd5\u6536\u53d1\u5668\u4e0e\u7a83\u542c\u8005\u6a21\u578b\uff0c\u4ee5\u5e73\u8861\u4efb\u52a1\u6027\u80fd\u4e0e\u9690\u79c1\u4fdd\u62a4\u3002", "result": "\u5728MNIST\u548cCIFAR-10\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u964d\u4f4e\u7a83\u542c\u8005\u63a8\u7406\u80fd\u529b\uff0c\u540c\u65f6\u4e0d\u635f\u5bb3\u5408\u6cd5\u63a5\u6536\u65b9\u7684\u8bed\u4e49\u51c6\u786e\u6027\u548c\u91cd\u5efa\u8d28\u91cf\u3002", "conclusion": "\u8be5\u6846\u67b6\u652f\u6301\u7aef\u5230\u7aef\u53ef\u8c03\u9690\u79c1\u4fdd\u62a4\uff0c\u9002\u7528\u4e8e\u73b0\u5b9e\u65e0\u7ebf\u73af\u5883\u4e2d\u5bf9\u6297\u81ea\u9002\u5e94\u653b\u51fb\u8005\u7684\u8bed\u4e49\u901a\u4fe1\u7cfb\u7edf\u8bbe\u8ba1\u3002"}}
{"id": "2512.24914", "categories": ["cs.DC", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.24914", "abs": "https://arxiv.org/abs/2512.24914", "authors": ["Vinoth Punniyamoorthy", "Akash Kumar Agarwal", "Bikesh Kumar", "Abhirup Mazumder", "Kabilan Kannan", "Sumit Saha"], "title": "AI-Driven Cloud Resource Optimization for Multi-Cluster Environments", "comment": null, "summary": "Modern cloud-native systems increasingly rely on multi-cluster deployments to support scalability, resilience, and geographic distribution. However, existing resource management approaches remain largely reactive and cluster-centric, limiting their ability to optimize system-wide behavior under dynamic workloads. These limitations result in inefficient resource utilization, delayed adaptation, and increased operational overhead across distributed environments. This paper presents an AI-driven framework for adaptive resource optimization in multi-cluster cloud systems. The proposed approach integrates predictive learning, policy-aware decision-making, and continuous feedback to enable proactive and coordinated resource management across clusters. By analyzing cross-cluster telemetry and historical execution patterns, the framework dynamically adjusts resource allocation to balance performance, cost, and reliability objectives. A prototype implementation demonstrates improved resource efficiency, faster stabilization during workload fluctuations, and reduced performance variability compared to conventional reactive approaches. The results highlight the effectiveness of intelligent, self-adaptive infrastructure management as a key enabler for scalable and resilient cloud platforms.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8eAI\u7684\u591a\u96c6\u7fa4\u4e91\u7cfb\u7edf\u81ea\u9002\u5e94\u8d44\u6e90\u4f18\u5316\u6846\u67b6\uff0c\u901a\u8fc7\u9884\u6d4b\u5b66\u4e60\u4e0e\u7b56\u7565\u611f\u77e5\u51b3\u7b56\u63d0\u5347\u8d44\u6e90\u6548\u7387\u548c\u7cfb\u7edf\u7a33\u5b9a\u6027\u3002", "motivation": "\u73b0\u6709\u8d44\u6e90\u7ba1\u7406\u65b9\u6cd5\u53cd\u5e94\u8fdf\u7f13\u4e14\u4ee5\u5355\u96c6\u7fa4\u4e3a\u4e2d\u5fc3\uff0c\u96be\u4ee5\u5e94\u5bf9\u52a8\u6001\u8d1f\u8f7d\u4e0b\u7684\u5168\u5c40\u4f18\u5316\u9700\u6c42\uff0c\u5bfc\u81f4\u8d44\u6e90\u5229\u7528\u7387\u4f4e\u3001\u9002\u5e94\u5ef6\u8fdf\u548c\u8fd0\u7ef4\u5f00\u9500\u5927\u3002", "method": "\u6574\u5408\u9884\u6d4b\u5b66\u4e60\u3001\u7b56\u7565\u611f\u77e5\u51b3\u7b56\u4e0e\u6301\u7eed\u53cd\u9988\u673a\u5236\uff0c\u5229\u7528\u8de8\u96c6\u7fa4\u9065\u6d4b\u6570\u636e\u548c\u5386\u53f2\u6267\u884c\u6a21\u5f0f\u52a8\u6001\u8c03\u6574\u8d44\u6e90\u5206\u914d\u3002", "result": "\u539f\u578b\u5b9e\u73b0\u8868\u660e\uff0c\u76f8\u6bd4\u4f20\u7edf\u65b9\u6cd5\uff0c\u8be5\u6846\u67b6\u63d0\u5347\u4e86\u8d44\u6e90\u6548\u7387\uff0c\u52a0\u5feb\u4e86\u8d1f\u8f7d\u6ce2\u52a8\u4e0b\u7684\u7a33\u5b9a\u901f\u5ea6\uff0c\u5e76\u964d\u4f4e\u4e86\u6027\u80fd\u6ce2\u52a8\u3002", "conclusion": "\u667a\u80fd\u81ea\u9002\u5e94\u57fa\u7840\u8bbe\u65bd\u7ba1\u7406\u662f\u6784\u5efa\u53ef\u6269\u5c55\u3001\u9ad8\u97e7\u6027\u4e91\u5e73\u53f0\u7684\u5173\u952e\u63a8\u52a8\u529b\u3002"}}
{"id": "2512.24159", "categories": ["cs.SE", "cs.AI", "cs.FL"], "pdf": "https://arxiv.org/pdf/2512.24159", "abs": "https://arxiv.org/abs/2512.24159", "authors": ["Natalia Garanina", "Vladimir Zyubin", "Igor Anureev"], "title": "Developing controlled natural language for formal specification patterns using AI assistants", "comment": null, "summary": "Using an AI assistant, we developed a method for systematically constructing controlled natural language for requirements based on formal specification patterns containing logical attributes. The method involves three stages: 1) compiling a generalized natural language requirement pattern that utilizes all attributes of the formal specification template; 2) generating, using the AI assistant, a corpus of natural language requirement patterns, reduced by partially evaluating attributes (the developed prompt utilizes the generalized template, attribute definitions, and specific formal semantics of the requirement patterns); and 3) formalizing the syntax of the controlled natural language based on an analysis of the grammatical structure of the resulting patterns. The method has been tested for event-driven temporal requirements.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8eAI\u52a9\u624b\u7cfb\u7edf\u5316\u6784\u5efa\u9700\u6c42\u63a7\u5236\u81ea\u7136\u8bed\u8a00\u7684\u65b9\u6cd5\uff0c\u9002\u7528\u4e8e\u4e8b\u4ef6\u9a71\u52a8\u7684\u65f6\u5e8f\u9700\u6c42\u3002", "motivation": "\u4e3a\u63d0\u5347\u9700\u6c42\u8868\u8fbe\u7684\u51c6\u786e\u6027\u4e0e\u5f62\u5f0f\u5316\u7a0b\u5ea6\uff0c\u89e3\u51b3\u81ea\u7136\u8bed\u8a00\u6a21\u7cca\u6027\u95ee\u9898\u3002", "method": "\u5206\u4e09\u9636\u6bb5\uff1a\u6784\u5efa\u901a\u7528\u6a21\u677f\u3001AI\u751f\u6210\u8bed\u6599\u5e93\u5e76\u7cbe\u7b80\u5c5e\u6027\u3001\u8bed\u6cd5\u7ed3\u6784\u5206\u6790\u5f62\u6210\u53d7\u63a7\u8bed\u8a00\u3002", "result": "\u65b9\u6cd5\u5728\u4e8b\u4ef6\u9a71\u52a8\u65f6\u5e8f\u9700\u6c42\u573a\u666f\u4e2d\u9a8c\u8bc1\u6709\u6548\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u80fd\u6709\u6548\u652f\u6301\u4ece\u5f62\u5f0f\u5316\u6a21\u677f\u5230\u53d7\u63a7\u81ea\u7136\u8bed\u8a00\u7684\u9700\u6c42\u6784\u9020\u3002"}}
{"id": "2512.24659", "categories": ["cs.NI"], "pdf": "https://arxiv.org/pdf/2512.24659", "abs": "https://arxiv.org/abs/2512.24659", "authors": ["Yixian Wang", "Geng Sun", "Zemin Sun", "Jiacheng Wang", "Changyuan Zhao", "Daxin Tian", "Dusit Niyato", "Shiwen Mao"], "title": "Hierarchical Online Optimization Approach for IRS-enabled Low-altitude MEC in Vehicular Networks", "comment": "18 pages, 7 figures,", "summary": "In this paper, we propose an intelligent reflecting surface (IRS)-enabled low-altitude multi-access edge computing (MEC) architecture, where an aerial MEC server cooperates with a terrestrial MEC server to provide computing services, while hybrid IRSs (i.e., building-installed and UAV-carried IRSs) are deployed to enhance the air-ground connectivity under blockage. Based on this architecture, we formulate a multi-objective optimization problem (MOOP) to minimize the task completion delay and energy consumption by jointly optimizing task offloading, UAV trajectory control, IRS phase-shift configuration, and computation resource allocation. The considered problem is NP-hard, and thus we propose a hierarchical online optimization approach (HOOA) to efficiently solve the problem. Specifically, we reformulate the MOOP as a Stackelberg game, where MEC servers collectively act as the leader to determine the system-level decisions, while the vehicles act as followers to make individual decisions. At the follower level, we present a many-to-one matching mechanism to generate feasible discrete decisions. At the leader level, we propose a generative diffusion model-enhanced twin delayed deep deterministic policy gradient (GDMTD3) algorithm integrated with a Karush-Kuhn-Tucker (KKT)-based method, which is a deep reinforcement learning (DRL)-based approach, to determine the continuous decisions. Simulation results demonstrate that the proposed HOOA achieves significant improvements, which reduces average task completion delay by 2.5% and average energy consumption by 3.1% compared with the best-performing benchmark approach and state-of-the-art DRL algorithm, respectively. Moreover, the proposed HOOA exhibits superior convergence stability while maintaining strong robustness and scalability in dynamic environments.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u667a\u80fd\u53cd\u5c04\u9762\u8f85\u52a9\u7684\u4f4e\u7a7a\u591a\u63a5\u5165\u8fb9\u7f18\u8ba1\u7b97\u67b6\u6784\uff0c\u5e76\u8bbe\u8ba1\u5206\u5c42\u5728\u7ebf\u4f18\u5316\u7b97\u6cd5\u4ee5\u964d\u4f4e\u4efb\u52a1\u5ef6\u8fdf\u4e0e\u80fd\u8017\u3002", "motivation": "\u63d0\u5347\u53d7\u906e\u6321\u73af\u5883\u4e0b\u7684\u7a7a\u5730\u901a\u4fe1\u8d28\u91cf\uff0c\u4f18\u5316\u4efb\u52a1\u5378\u8f7d\u4e0e\u8d44\u6e90\u5206\u914d\u6548\u7387\u3002", "method": "\u6784\u5efa\u591a\u76ee\u6807\u4f18\u5316\u95ee\u9898\uff0c\u91c7\u7528Stackelberg\u535a\u5f08\u5efa\u6a21\uff0c\u7ed3\u5408\u5339\u914d\u673a\u5236\u4e0eGDMTD3\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\u6c42\u89e3\u3002", "result": "\u76f8\u6bd4\u73b0\u6709\u6700\u4f18\u57fa\u51c6\u65b9\u6cd5\uff0c\u5e73\u5747\u4efb\u52a1\u5ef6\u8fdf\u964d\u4f4e2.5%\uff0c\u80fd\u8017\u964d\u4f4e3.1%\uff0c\u4e14\u5177\u5907\u826f\u597d\u6536\u655b\u6027\u4e0e\u9c81\u68d2\u6027\u3002", "conclusion": "\u6240\u63d0\u67b6\u6784\u4e0e\u7b97\u6cd5\u5728\u52a8\u6001\u73af\u5883\u4e2d\u9ad8\u6548\u7a33\u5b9a\uff0c\u663e\u8457\u63d0\u5347\u8fb9\u7f18\u8ba1\u7b97\u6027\u80fd\u3002"}}
{"id": "2512.24462", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2512.24462", "abs": "https://arxiv.org/abs/2512.24462", "authors": ["Yoonha Cha", "Victoria Jackson", "Lauren Shu", "Stacy Branham", "Andr\u00e9 van der Hoek"], "title": "\"Game Changer\" or \"Overenthusiastic Drunk Acquaintance\"? Generative AI Use by Blind and Low Vision Software Professionals in the Workplace", "comment": "13 pages", "summary": "The software development workplace poses numerous technical and collaborative accessibility challenges for blind and low vision software professionals (BLVSPs). Though Generative AI (GenAI) is increasingly adopted within the software development industry and has been a rapidly growing topic of interest in research, to date, the unique perspectives of BLVSPs have yet to be consulted. We report on a qualitative study involving 39 semi-structured interviews with BLVSPs about what the introduction of GenAI has meant for their work. We found that BLVSPs used GenAI for many software development tasks, resulting in benefits such as increased productivity and accessibility. However, significant costs were also accompanied by GenAI use as they were more vulnerable to hallucinations than their sighted colleagues. Sometimes, organizational policies prevented use. Based on our findings, we discuss the higher-risks and higher-returns that BLVSPs had to carefully weigh when deciding whether and when to use GenAI tools for work.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u4e86\u751f\u6210\u5f0fAI\u5bf9\u76f2\u4eba\u53ca\u4f4e\u89c6\u529b\u8f6f\u4ef6\u4ece\u4e1a\u8005\u7684\u5f71\u54cd\uff0c\u53d1\u73b0\u5176\u5728\u63d0\u5347\u6548\u7387\u4e0e\u53ef\u8bbf\u95ee\u6027\u7684\u540c\u65f6\u4e5f\u5e26\u6765\u66f4\u9ad8\u98ce\u9669\u3002", "motivation": "\u586b\u8865\u751f\u6210\u5f0fAI\u5728\u76f2\u4eba\u53ca\u4f4e\u89c6\u529b\u8f6f\u4ef6\u4ece\u4e1a\u8005\u7fa4\u4f53\u4e2d\u7814\u7a76\u89c6\u89d2\u7684\u7a7a\u767d\u3002", "method": "\u5bf939\u540d\u76f2\u4eba\u53ca\u4f4e\u89c6\u529b\u8f6f\u4ef6\u4ece\u4e1a\u8005\u8fdb\u884c\u534a\u7ed3\u6784\u5316\u8bbf\u8c08\u7684\u5b9a\u6027\u7814\u7a76\u3002", "result": "\u53d7\u8bbf\u8005\u5229\u7528\u751f\u6210\u5f0fAI\u63d0\u5347\u5de5\u4f5c\u6548\u7387\u548c\u53ef\u8bbf\u95ee\u6027\uff0c\u4f46\u66f4\u6613\u53d7\u5e7b\u89c9\u5f71\u54cd\u4e14\u53d7\u7ec4\u7ec7\u653f\u7b56\u9650\u5236\u3002", "conclusion": "\u76f2\u4eba\u53ca\u4f4e\u89c6\u529b\u4ece\u4e1a\u8005\u9700\u6743\u8861\u751f\u6210\u5f0fAI\u5e26\u6765\u7684\u9ad8\u56de\u62a5\u4e0e\u9ad8\u98ce\u9669\u4ee5\u51b3\u5b9a\u662f\u5426\u4f7f\u7528\u3002"}}
{"id": "2512.24750", "categories": ["cs.NI"], "pdf": "https://arxiv.org/pdf/2512.24750", "abs": "https://arxiv.org/abs/2512.24750", "authors": ["Wenxue Li", "Xiangzhou Liu", "Yuxuan Li", "Yilun Jin", "Zhenghang Ren", "Xudong Liao", "Han Tian", "Bo Ren", "Zhizhen Zhong", "Guyue Liu", "Ying Zhang", "Kai Chen"], "title": "Analyzing Communication Predictability in LLM Training", "comment": null, "summary": "Effective communication is essential in distributed training, with predictability being one of its most significant characteristics. However, existing studies primarily focus on exploiting predictability through online profiling for runtime optimization, without a systematic understanding of it. In this work, we aim to systematically formulate communication predictability in distributed training, particularly in Large Language Models (LLMs) that utilize hybrid parallelism. Our analysis focuses on both traffic patterns and communication overhead. Specifically, we investigate predictable traffic patterns in typical LLMs and evaluate how various factors influence GPU utilization and effective bandwidth (two critical variables affecting communication overhead). Furthermore, we develop an analytical formulation to estimate communication overhead in LLM training, which is validated with high accuracy against empirical data. Leveraging this formulation, we propose a configuration tuning tool, ConfigTuner, to optimize training performance. Compared to Megatron-LM, the training configurations optimized by ConfigTuner demonstrate up to a 1.36$\\times$ increase in throughput. Compared to Alpa, ConfigTuner generates the same configuration suggestion while significantly reducing the search complexity.", "AI": {"tldr": "\u672c\u6587\u7cfb\u7edf\u6027\u5730\u7814\u7a76\u4e86\u5206\u5e03\u5f0f\u8bad\u7ec3\u4e2d\u7684\u901a\u4fe1\u53ef\u9884\u6d4b\u6027\uff0c\u63d0\u51fa\u5206\u6790\u6a21\u578b\u5e76\u5f00\u53d1ConfigTuner\u5de5\u5177\u4f18\u5316LLM\u8bad\u7ec3\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u7f3a\u4e4f\u5bf9\u901a\u4fe1\u53ef\u9884\u6d4b\u6027\u7684\u7cfb\u7edf\u7406\u89e3\uff0c\u4ec5\u4f9d\u8d56\u5728\u7ebf\u5256\u6790\u8fdb\u884c\u8fd0\u884c\u65f6\u4f18\u5316\u3002", "method": "\u5206\u6790\u5178\u578bLLM\u7684\u901a\u4fe1\u6d41\u91cf\u6a21\u5f0f\u4e0e\u5f00\u9500\u5f71\u54cd\u56e0\u7d20\uff0c\u5efa\u7acb\u901a\u4fe1\u5f00\u9500\u89e3\u6790\u6a21\u578b\uff0c\u5e76\u57fa\u4e8e\u6b64\u6784\u5efa\u914d\u7f6e\u8c03\u4f18\u5de5\u5177ConfigTuner\u3002", "result": "ConfigTuner\u76f8\u8f83Megatron-LM\u63d0\u5347\u6700\u9ad81.36\u500d\u541e\u5410\u91cf\uff0c\u76f8\u8f83Alpa\u751f\u6210\u76f8\u540c\u914d\u7f6e\u4f46\u641c\u7d22\u590d\u6742\u5ea6\u663e\u8457\u964d\u4f4e\u3002", "conclusion": "\u7cfb\u7edf\u5efa\u6a21\u901a\u4fe1\u53ef\u9884\u6d4b\u6027\u53ef\u6709\u6548\u6307\u5bfc\u5206\u5e03\u5f0f\u8bad\u7ec3\u914d\u7f6e\u4f18\u5316\uff0c\u663e\u8457\u63d0\u5347\u8bad\u7ec3\u6548\u7387\u3002"}}
{"id": "2512.24560", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.24560", "abs": "https://arxiv.org/abs/2512.24560", "authors": ["David Gros", "Prem Devanbu"], "title": "Localized Calibrated Uncertainty in Code Language Models", "comment": null, "summary": "Large Language models (LLMs) can generate complicated source code from natural language prompts. However, LLMs can generate output that deviates from what the user wants, requiring supervision and editing. To support this process, we offer techniques to localize where generations might be misaligned from user intent. We first create a dataset of \"Minimal Intent Aligning Patches\" of repaired LLM generated programs. Each program uses test cases to verify correctness. After creating a dataset of programs, we measure how well various techniques can assign a well-calibrated probability to indicate which parts of code will be edited in a minimal patch (i.e., give a probability that corresponds with empirical odds it is edited). We compare white-box probing (where we propose a technique for efficient arbitrary-span querying), against black-box reflective and self-consistency based approaches. We find probes with a small supervisor model can achieve low calibration error and Brier Skill Score of approx 0.2 estimating edited lines on code generated by models many orders of magnitude larger. We discuss the generalizability of the techniques, and the connections to AI oversight and control, finding a probe trained only on code shows some signs of generalizing to natural language errors if new probability scaling is allowed.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5b9a\u4f4d\u5927\u8bed\u8a00\u6a21\u578b\u751f\u6210\u4ee3\u7801\u4e2d\u4e0e\u7528\u6237\u610f\u56fe\u504f\u5dee\u90e8\u5206\u7684\u6280\u672f\uff0c\u5e76\u901a\u8fc7\u6784\u5efa\u6570\u636e\u96c6\u8bc4\u4f30\u4e0d\u540c\u65b9\u6cd5\u7684\u6821\u51c6\u6982\u7387\u6027\u80fd\u3002", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b\u751f\u6210\u7684\u4ee3\u7801\u53ef\u80fd\u504f\u79bb\u7528\u6237\u610f\u56fe\uff0c\u9700\u76d1\u7763\u548c\u7f16\u8f91\uff0c\u56e0\u6b64\u9700\u8981\u6280\u672f\u6765\u5b9a\u4f4d\u8fd9\u4e9b\u504f\u5dee\u3002", "method": "\u6784\u5efa\u5305\u542b\u201c\u6700\u5c0f\u610f\u56fe\u5bf9\u9f50\u8865\u4e01\u201d\u7684\u6570\u636e\u96c6\uff0c\u4f7f\u7528\u6d4b\u8bd5\u7528\u4f8b\u9a8c\u8bc1\u7a0b\u5e8f\u6b63\u786e\u6027\uff0c\u5e76\u6bd4\u8f83\u767d\u76d2\u63a2\u6d4b\u3001\u9ed1\u76d2\u53cd\u601d\u53ca\u81ea\u4e00\u81f4\u6027\u65b9\u6cd5\u7684\u6821\u51c6\u6548\u679c\u3002", "result": "\u5c0f\u76d1\u7763\u6a21\u578b\u7684\u63a2\u9488\u5728\u6821\u51c6\u8bef\u5dee\u548cBrier\u6280\u80fd\u8bc4\u5206\u4e0a\u8868\u73b0\u826f\u597d\uff0c\u4e14\u5728\u4ec5\u8bad\u7ec3\u4e8e\u4ee3\u7801\u7684\u60c5\u51b5\u4e0b\u5bf9\u81ea\u7136\u8bed\u8a00\u9519\u8bef\u4e5f\u6709\u4e00\u5b9a\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "\u6240\u63d0\u6280\u672f\u5728\u6821\u51c6\u6982\u7387\u4f30\u8ba1\u548c\u8de8\u9886\u57df\u6cdb\u5316\u65b9\u9762\u5177\u6709\u6f5c\u529b\uff0c\u6709\u52a9\u4e8eAI\u76d1\u7763\u4e0e\u63a7\u5236\u3002"}}
{"id": "2512.24570", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2512.24570", "abs": "https://arxiv.org/abs/2512.24570", "authors": ["Shiqi Kuang", "Zhao Tian", "Tao Xiao", "Dong Wang", "Junjie Chen"], "title": "On the Effectiveness of Training Data Optimization for LLM-based Code Generation: An Empirical Study", "comment": null, "summary": "Large language models (LLMs) have achieved remarkable progress in code generation, largely driven by the availability of high-quality code datasets for effective training. To further improve data quality, numerous training data optimization techniques have been proposed; however, their overall effectiveness has not been systematically evaluated. To bridge this gap, we conduct the first large-scale empirical study, examining five widely-used training data optimization techniques and their pairwise combinations for LLM-based code generation across three benchmarks and four LLMs. Our results show that data synthesis is the most effective technique for improving functional correctness and reducing code smells, although it performs relatively worse on code maintainability compared to data refactoring, cleaning, and selection. Regarding combinations, we find that most combinations do not further improve functional correctness but can effectively enhance code quality (code smells and maintainability). Among all combinations, data synthesis combined with data refactoring achieves the strongest overall performance. Furthermore, our fine-grained analysis reinforces these findings and provides deeper insights into how individual techniques and their combinations influence code generation effectiveness. Overall, this work represents a first step toward a systematic understanding of training data optimization and combination strategies, offering practical guidance for future research and deployment in LLM-based code generation.", "AI": {"tldr": "\u672c\u6587\u9996\u6b21\u5927\u89c4\u6a21\u8bc4\u4f30\u4e86\u4e94\u79cd\u5e38\u7528\u8bad\u7ec3\u6570\u636e\u4f18\u5316\u6280\u672f\u53ca\u5176\u7ec4\u5408\u5728LLM\u4ee3\u7801\u751f\u6210\u4e2d\u7684\u6548\u679c\uff0c\u53d1\u73b0\u6570\u636e\u5408\u6210\u5bf9\u529f\u80fd\u6b63\u786e\u6027\u6700\u6709\u6548\uff0c\u800c\u4e0e\u91cd\u6784\u7ed3\u5408\u65f6\u6574\u4f53\u6027\u80fd\u6700\u4f73\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u7f3a\u4e4f\u5bf9\u8bad\u7ec3\u6570\u636e\u4f18\u5316\u6280\u672f\u53ca\u5176\u7ec4\u5408\u5728\u4ee3\u7801\u751f\u6210\u4e2d\u6548\u679c\u7684\u7cfb\u7edf\u6027\u8bc4\u4f30\uff0c\u672c\u6587\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u5728\u4e09\u4e2a\u57fa\u51c6\u548c\u56db\u4e2aLLM\u4e0a\uff0c\u5b9e\u8bc1\u5206\u6790\u4e94\u79cd\u4e3b\u6d41\u6570\u636e\u4f18\u5316\u6280\u672f\u53ca\u5176\u4e24\u4e24\u7ec4\u5408\u7684\u6548\u679c\uff0c\u5e76\u8fdb\u884c\u7ec6\u7c92\u5ea6\u5f52\u56e0\u5206\u6790\u3002", "result": "\u6570\u636e\u5408\u6210\u63d0\u5347\u529f\u80fd\u6b63\u786e\u6027\u548c\u51cf\u5c11\u4ee3\u7801\u5f02\u5473\u6700\u663e\u8457\uff0c\u4f46\u53ef\u7ef4\u62a4\u6027\u5f31\u4e8e\u91cd\u6784\u3001\u6e05\u6d17\u548c\u9009\u62e9\uff1b\u591a\u6570\u7ec4\u5408\u4e0d\u80fd\u8fdb\u4e00\u6b65\u63d0\u5347\u529f\u80fd\u6b63\u786e\u6027\uff0c\u4f46\u80fd\u6539\u5584\u4ee3\u7801\u8d28\u91cf\uff1b\u5408\u6210+\u91cd\u6784\u7ec4\u5408\u8868\u73b0\u6700\u4f18\u3002", "conclusion": "\u672c\u7814\u7a76\u4e3aLLM\u4ee3\u7801\u751f\u6210\u7684\u6570\u636e\u4f18\u5316\u7b56\u7565\u63d0\u4f9b\u4e86\u9996\u4e2a\u7cfb\u7edf\u6027\u5b9e\u8bc1\u57fa\u7840\u548c\u5b9e\u7528\u6307\u5bfc\uff0c\u63a8\u52a8\u672a\u6765\u7814\u7a76\u4e0e\u90e8\u7f72\u3002"}}
{"id": "2512.24594", "categories": ["cs.SE", "cs.LO"], "pdf": "https://arxiv.org/pdf/2512.24594", "abs": "https://arxiv.org/abs/2512.24594", "authors": ["Zhongyi Wang", "Tengjie Lin", "Mingshuai Chen", "Haokun Li", "Mingqi Yang", "Xiao Yi", "Shengchao Qin", "Yixing Luo", "Xiaofeng Li", "Bin Gu", "Liqiang Lu", "Jianwei Yin"], "title": "A Tale of 1001 LoC: Potential Runtime Error-Guided Specification Synthesis for Verifying Large-Scale Programs", "comment": "Accepted at OOPSLA 2026", "summary": "Fully automated verification of large-scale software and hardware systems is arguably the holy grail of formal methods. Large language models (LLMs) have recently demonstrated their potential for enhancing the degree of automation in formal verification by, e.g., generating formal specifications as essential to deductive verification, yet exhibit poor scalability due to long-context reasoning limitations and, more importantly, the difficulty of inferring complex, interprocedural specifications. This paper presents Preguss -- a modular, fine-grained framework for automating the generation and refinement of formal specifications. Preguss synergizes between static analysis and deductive verification by steering two components in a divide-and-conquer fashion: (i) potential runtime error-guided construction and prioritization of verification units, and (ii) LLM-aided synthesis of interprocedural specifications at the unit level. We show that Preguss substantially outperforms state-of-the-art LLM-based approaches and, in particular, it enables highly automated RTE-freeness verification for real-world programs with over a thousand LoC, with a reduction of 80.6%~88.9% human verification effort.", "AI": {"tldr": "Preguss\u6846\u67b6\u901a\u8fc7\u7ed3\u5408\u9759\u6001\u5206\u6790\u4e0e\u6f14\u7ece\u9a8c\u8bc1\uff0c\u663e\u8457\u63d0\u5347\u5927\u89c4\u6a21\u7a0b\u5e8f\u5f62\u5f0f\u5316\u9a8c\u8bc1\u7684\u81ea\u52a8\u5316\u7a0b\u5ea6\uff0c\u51cf\u5c11\u4eba\u5de5\u9a8c\u8bc1\u5de5\u4f5c\u91cf\u8fbe80.6%~88.9%\u3002", "motivation": "\u89e3\u51b3\u5927\u8bed\u8a00\u6a21\u578b\u5728\u5f62\u5f0f\u5316\u9a8c\u8bc1\u4e2d\u56e0\u957f\u4e0a\u4e0b\u6587\u63a8\u7406\u9650\u5236\u548c\u590d\u6742\u8de8\u8fc7\u7a0b\u89c4\u8303\u63a8\u65ad\u56f0\u96be\u5bfc\u81f4\u7684\u53ef\u6269\u5c55\u6027\u5dee\u95ee\u9898\u3002", "method": "\u63d0\u51faPreguss\u6846\u67b6\uff0c\u91c7\u7528\u5206\u6cbb\u7b56\u7565\uff0c\u7ed3\u5408\u8fd0\u884c\u65f6\u9519\u8bef\u5f15\u5bfc\u7684\u9a8c\u8bc1\u5355\u5143\u6784\u5efa\u4e0e\u4f18\u5148\u7ea7\u6392\u5e8f\uff0c\u4ee5\u53caLLM\u8f85\u52a9\u7684\u5355\u5143\u7ea7\u8de8\u8fc7\u7a0b\u89c4\u8303\u5408\u6210\u3002", "result": "\u5728\u771f\u5b9e\u5343\u884c\u7ea7\u4ee3\u7801\u7a0b\u5e8f\u4e0a\u5b9e\u73b0\u9ad8\u5ea6\u81ea\u52a8\u5316\u7684\u8fd0\u884c\u65f6\u9519\u8bef\u81ea\u7531\u9a8c\u8bc1\uff0c\u6027\u80fd\u5927\u5e45\u8d85\u8d8a\u73b0\u6709LLM\u65b9\u6cd5\u3002", "conclusion": "Preguss\u6709\u6548\u63d0\u5347\u4e86\u5f62\u5f0f\u5316\u9a8c\u8bc1\u7684\u81ea\u52a8\u5316\u6c34\u5e73\uff0c\u663e\u8457\u964d\u4f4e\u4eba\u5de5\u5e72\u9884\u9700\u6c42\uff0c\u4e3a\u5927\u89c4\u6a21\u8f6f\u786c\u4ef6\u7cfb\u7edf\u9a8c\u8bc1\u63d0\u4f9b\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2512.24635", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.24635", "abs": "https://arxiv.org/abs/2512.24635", "authors": ["Zhili Huang", "Ling Xu", "Chao Liu", "Weifeng Sun", "Xu Zhang", "Yan Lei", "Meng Yan", "Hongyu Zhang"], "title": "DynaFix: Iterative Automated Program Repair Driven by Execution-Level Dynamic Information", "comment": "22 pages, 7 figures, preprint version", "summary": "Automated Program Repair (APR) aims to automatically generate correct patches for buggy programs. Recent approaches leveraging large language models (LLMs) have shown promise but face limitations. Most rely solely on static analysis, ignoring runtime behaviors. Some attempt to incorporate dynamic signals, but these are often restricted to training or fine-tuning, or injected only once into the repair prompt, without iterative use. This fails to fully capture program execution. Current iterative repair frameworks typically rely on coarse-grained feedback, such as pass/fail results or exception types, and do not leverage fine-grained execution-level information effectively. As a result, models struggle to simulate human stepwise debugging, limiting their effectiveness in multi-step reasoning and complex bug repair.\n  To address these challenges, we propose DynaFix, an execution-level dynamic information-driven APR method that iteratively leverages runtime information to refine the repair process. In each repair round, DynaFix captures execution-level dynamic information such as variable states, control-flow paths, and call stacks, transforming them into structured prompts to guide LLMs in generating candidate patches. If a patch fails validation, DynaFix re-executes the modified program to collect new execution information for the next attempt. This iterative loop incrementally improves patches based on updated feedback, similar to the stepwise debugging practices of human developers. We evaluate DynaFix on the Defects4J v1.2 and v2.0 benchmarks. DynaFix repairs 186 single-function bugs, a 10% improvement over state-of-the-art baselines, including 38 bugs previously unrepaired. It achieves correct patches within at most 35 attempts, reducing the patch search space by 70% compared with existing methods, thereby demonstrating both effectiveness and efficiency in repairing complex bugs.", "AI": {"tldr": "DynaFix\u662f\u4e00\u79cd\u5229\u7528\u8fd0\u884c\u65f6\u52a8\u6001\u4fe1\u606f\u8fed\u4ee3\u4fee\u590d\u7a0b\u5e8f\u7f3a\u9677\u7684\u65b0\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86\u81ea\u52a8\u7a0b\u5e8f\u4fee\u590d\u7684\u6548\u679c\u548c\u6548\u7387\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u81ea\u52a8\u7a0b\u5e8f\u4fee\u590d\u65b9\u6cd5\u591a\u4f9d\u8d56\u9759\u6001\u5206\u6790\u6216\u7c97\u7c92\u5ea6\u53cd\u9988\uff0c\u96be\u4ee5\u6a21\u62df\u4eba\u7c7b\u9010\u6b65\u8c03\u8bd5\u8fc7\u7a0b\uff0c\u9650\u5236\u4e86\u590d\u6742\u7f3a\u9677\u7684\u4fee\u590d\u80fd\u529b\u3002", "method": "DynaFix\u5728\u6bcf\u8f6e\u4fee\u590d\u4e2d\u6355\u83b7\u53d8\u91cf\u72b6\u6001\u3001\u63a7\u5236\u6d41\u8def\u5f84\u548c\u8c03\u7528\u6808\u7b49\u6267\u884c\u7ea7\u52a8\u6001\u4fe1\u606f\uff0c\u5c06\u5176\u7ed3\u6784\u5316\u540e\u5f15\u5bfcLLM\u751f\u6210\u5019\u9009\u8865\u4e01\uff0c\u5e76\u901a\u8fc7\u5931\u8d25\u8865\u4e01\u7684\u91cd\u65b0\u6267\u884c\u6301\u7eed\u6536\u96c6\u65b0\u4fe1\u606f\u8fdb\u884c\u8fed\u4ee3\u4f18\u5316\u3002", "result": "\u5728Defects4J v1.2\u548cv2.0\u57fa\u51c6\u4e0a\uff0cDynaFix\u4fee\u590d\u4e86186\u4e2a\u5355\u51fd\u6570\u7f3a\u9677\uff0c\u6bd4\u73b0\u6709\u6700\u4f18\u65b9\u6cd5\u63d0\u534710%\uff0c\u5176\u4e2d38\u4e2a\u4e3a\u6b64\u524d\u672a\u4fee\u590d\u7f3a\u9677\uff1b\u6700\u591a35\u6b21\u5c1d\u8bd5\u5185\u5b8c\u6210\u4fee\u590d\uff0c\u641c\u7d22\u7a7a\u95f4\u51cf\u5c1170%\u3002", "conclusion": "DynaFix\u901a\u8fc7\u7ec6\u7c92\u5ea6\u52a8\u6001\u4fe1\u606f\u9a71\u52a8\u7684\u8fed\u4ee3\u673a\u5236\uff0c\u6709\u6548\u6a21\u62df\u4eba\u7c7b\u8c03\u8bd5\u884c\u4e3a\uff0c\u5728\u4fee\u590d\u80fd\u529b\u548c\u6548\u7387\u4e0a\u5747\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002"}}
{"id": "2512.24656", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2512.24656", "abs": "https://arxiv.org/abs/2512.24656", "authors": ["Mir Mohammad Yousuf", "Shabir Ahmad Sofi"], "title": "Characterizing Bugs and Quality Attributes in Quantum Software: A Large-Scale Empirical Study", "comment": null, "summary": "Quantum Software Engineering (QSE) is essential for ensuring the reliability and maintainability of hybrid quantum-classical systems, yet empirical evidence on how bugs emerge and affect quality in real-world quantum projects remains limited. This study presents the first ecosystem-scale longitudinal analysis of software defects across 123 open source quantum repositories from 2012 to 2024, spanning eight functional categories, including full-stack libraries, simulators, annealing, algorithms, compilers, assembly, cryptography, and experimental computing. Using a mixed method approach combining repository mining, static code analysis, issue metadata extraction, and a validated rule-based classification framework, we analyze 32,296 verified bug reports. Results show that full-stack libraries and compilers are the most defect-prone categories due to circuit, gate, and transpilation-related issues, while simulators are mainly affected by measurement and noise modeling errors. Classical bugs primarily impact usability and interoperability, whereas quantum-specific bugs disproportionately degrade performance, maintainability, and reliability. Longitudinal analysis indicates ecosystem maturation, with defect densities peaking between 2017 and 2021 and declining thereafter. High-severity defects cluster in cryptography, experimental computing, and compiler toolchains. Repositories employing automated testing detect more defects and resolve issues faster. A negative binomial regression further shows that automated testing is associated with an approximate 60 percent reduction in expected defect incidence. Overall, this work provides the first large-scale data-driven characterization of quantum software defects and offers empirical guidance for improving testing, documentation, and maintainability practices in QSE.", "AI": {"tldr": "\u9996\u6b21\u5927\u89c4\u6a21\u7eb5\u5411\u5206\u6790123\u4e2a\u5f00\u6e90\u91cf\u5b50\u9879\u76ee\u4e2d\u7684\u8f6f\u4ef6\u7f3a\u9677\uff0c\u63ed\u793a\u5168\u6808\u5e93\u548c\u7f16\u8bd1\u5668\u6700\u6613\u51fa\u9519\uff0c\u81ea\u52a8\u5316\u6d4b\u8bd5\u53ef\u964d\u4f4e\u7ea660%\u7f3a\u9677\u53d1\u751f\u7387\u3002", "motivation": "\u5f53\u524d\u7f3a\u4e4f\u5bf9\u771f\u5b9e\u91cf\u5b50\u9879\u76ee\u4e2d\u7f3a\u9677\u5982\u4f55\u4ea7\u751f\u53ca\u5f71\u54cd\u8d28\u91cf\u7684\u5b9e\u8bc1\u7814\u7a76\uff0c\u9700\u7cfb\u7edf\u6027\u5206\u6790\u4ee5\u6307\u5bfc\u91cf\u5b50\u8f6f\u4ef6\u5de5\u7a0b\u5b9e\u8df5\u3002", "method": "\u7ed3\u5408\u4ed3\u5e93\u6316\u6398\u3001\u9759\u6001\u4ee3\u7801\u5206\u6790\u3001\u95ee\u9898\u5143\u6570\u636e\u63d0\u53d6\u4e0e\u89c4\u5219\u5206\u7c7b\u6846\u67b6\uff0c\u5206\u67902012-2024\u5e74\u95f432,296\u4e2a\u5df2\u9a8c\u8bc1\u7f3a\u9677\u62a5\u544a\u3002", "result": "\u5168\u6808\u5e93\u4e0e\u7f16\u8bd1\u5668\u56e0\u7535\u8def\u4e0e\u8f6c\u8bd1\u95ee\u9898\u6700\u6613\u51fa\u9519\uff1b\u6a21\u62df\u5668\u4e3b\u8981\u53d7\u6d4b\u91cf\u4e0e\u566a\u58f0\u5efa\u6a21\u9519\u8bef\u5f71\u54cd\uff1b\u81ea\u52a8\u5316\u6d4b\u8bd5\u663e\u8457\u51cf\u5c11\u7f3a\u9677\u5e76\u52a0\u901f\u4fee\u590d\u3002", "conclusion": "\u672c\u7814\u7a76\u9996\u6b21\u63d0\u4f9b\u91cf\u5b50\u8f6f\u4ef6\u7f3a\u9677\u7684\u5927\u89c4\u6a21\u6570\u636e\u9a71\u52a8\u7279\u5f81\uff0c\u4e3a\u6539\u8fdb\u6d4b\u8bd5\u3001\u6587\u6863\u4e0e\u53ef\u7ef4\u62a4\u6027\u5b9e\u8df5\u63d0\u4f9b\u5b9e\u8bc1\u4f9d\u636e\u3002"}}
{"id": "2512.24858", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2512.24858", "abs": "https://arxiv.org/abs/2512.24858", "authors": ["Ke Ma", "Jianjun Huang", "Wei You", "Bin Liang", "Jingzheng Wu", "Yanjun Wu", "Yuanjun Gong"], "title": "Feature Slice Matching for Precise Bug Detection", "comment": "Accepted by FSE2026", "summary": "Measuring the function similarity to detect bugs is effective, but the statements unrelated to the bugs can impede the performance due to the noise interference. Suppressing the noise interference in existing works does not manage the tough job, i.e., eliminating the noise in the targets. In this paper, we propose MATUS to mitigate the target noise for precise bug detection based on similarity measurement. Feature slices are extracted from both the buggy query and the targets to represent the semantic feature of (potential) bug logics. In particular, MATUS guides the target slicing with the prior knowledge from the buggy code, in an end-to-end way to pinpoint the slicing criterion in the targets. All feature slices are embedded and compared based on the vector similarity. Buggy candidates are audited to confirm unknown bugs in the targets. Experiments show that MATUS holds advantages in bug detection for real-world projects with acceptable efficiency. In total, MATUS has spotted 31 unknown bugs in the Linux kernel. All of them have been confirmed by the kernel developers, and 11 have been assigned CVEs.", "AI": {"tldr": "MATUS\u901a\u8fc7\u7aef\u5230\u7aef\u65b9\u6cd5\u63d0\u53d6\u8bed\u4e49\u7279\u5f81\u5207\u7247\uff0c\u6709\u6548\u6291\u5236\u76ee\u6807\u566a\u58f0\u4ee5\u63d0\u5347\u76f8\u4f3c\u6027\u5ea6\u91cf\u7684\u7cbe\u51c6\u6027\uff0c\u6210\u529f\u5728Linux\u5185\u6838\u4e2d\u53d1\u73b031\u4e2a\u672a\u77e5\u6f0f\u6d1e\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u6d88\u9664\u76ee\u6807\u4ee3\u7801\u4e2d\u7684\u566a\u58f0\u5e72\u6270\uff0c\u5f71\u54cd\u57fa\u4e8e\u76f8\u4f3c\u6027\u5ea6\u91cf\u7684\u7f3a\u9677\u68c0\u6d4b\u6548\u679c\u3002", "method": "\u63d0\u51faMATUS\u6846\u67b6\uff0c\u5229\u7528\u7f3a\u9677\u4ee3\u7801\u5148\u9a8c\u77e5\u8bc6\u5f15\u5bfc\u76ee\u6807\u5207\u7247\uff0c\u63d0\u53d6\u8bed\u4e49\u7279\u5f81\u5207\u7247\u5e76\u57fa\u4e8e\u5411\u91cf\u76f8\u4f3c\u6027\u6bd4\u8f83\uff0c\u6700\u7ec8\u5ba1\u8ba1\u786e\u8ba4\u6f5c\u5728\u7f3a\u9677\u3002", "result": "\u5b9e\u9a8c\u8868\u660eMATUS\u5728\u771f\u5b9e\u9879\u76ee\u4e2d\u5177\u6709\u8f83\u9ad8\u68c0\u6d4b\u6548\u7387\uff0c\u5df2\u5728Linux\u5185\u6838\u4e2d\u53d1\u73b031\u4e2a\u672a\u77e5\u7f3a\u9677\uff0c\u5168\u90e8\u83b7\u5f00\u53d1\u8005\u786e\u8ba4\uff0c\u5176\u4e2d11\u4e2a\u88ab\u5206\u914dCVE\u7f16\u53f7\u3002", "conclusion": "MATUS\u80fd\u6709\u6548\u7f13\u89e3\u76ee\u6807\u566a\u58f0\u5e72\u6270\uff0c\u663e\u8457\u63d0\u5347\u57fa\u4e8e\u76f8\u4f3c\u6027\u7684\u7f3a\u9677\u68c0\u6d4b\u7cbe\u5ea6\u4e0e\u5b9e\u7528\u6027\u3002"}}
{"id": "2512.24941", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2512.24941", "abs": "https://arxiv.org/abs/2512.24941", "authors": ["Zhiyong Zhang", "Xiaoyan Zhang", "Xiaoqi Li"], "title": "Securing High-Concurrency Ticket Sales: A Framework Based on Microservice", "comment": null, "summary": "The railway ticketing system is one of the most important public service infrastructure. In peak periods such as holidays, it is often faced with the challenge of high concurrency scenarios because of a large number of users accessing at the same time. The traditional aggregation architecture can not meet the peak user requirements because of its insufficient fault tolerance and low ability. Therefore, the system needs to use microservice architecture for development, and add multiple security methods to ensure that the system can have good stability and data consistency under high concurrency scenarios, and can respond quickly to user requests. This paper introduces the use of B/S architecture and Spring Cloud to design and develop a railway ticket purchase system that can maintain stability and reliability under high concurrency scenarios, and formulate multiple security design methods for the system. This system integrates a range of functions, such as real-time train inquiries, dynamic seat updates, online seat selection, and ticket purchasing, effectively addressing common problems associated with offline ticket purchasing, such as long queues and delayed information. It enables a complete online process from inquiry and booking to payment and refunds. Furthermore, the \"add passenger\" function allows users to purchase tickets for others, extending the convenience of online ticketing to people with limited internet access. The system design prioritizes security and stability, while also focusing on high performance, and achieves these goals through a carefully designed architecture and the integration of multiple middleware components. After the completion of the system development, the core interface of the system is tested, and then the results are analyzed. The test data proves that the system has good ability and stability under high concurrency.", "AI": {"tldr": "\u672c\u6587\u8bbe\u8ba1\u5e76\u5f00\u53d1\u4e86\u4e00\u4e2a\u57fa\u4e8e\u5fae\u670d\u52a1\u67b6\u6784\u7684\u94c1\u8def\u8d2d\u7968\u7cfb\u7edf\uff0c\u4ee5\u5e94\u5bf9\u9ad8\u5e76\u53d1\u573a\u666f\u4e0b\u7684\u7a33\u5b9a\u6027\u548c\u6570\u636e\u4e00\u81f4\u6027\u6311\u6218\u3002", "motivation": "\u4f20\u7edf\u805a\u5408\u67b6\u6784\u5728\u8282\u5047\u65e5\u7b49\u9ad8\u5cf0\u671f\u65e0\u6cd5\u6ee1\u8db3\u5927\u91cf\u7528\u6237\u540c\u65f6\u8bbf\u95ee\u7684\u9700\u6c42\uff0c\u7cfb\u7edf\u9700\u8981\u66f4\u9ad8\u7684\u5bb9\u9519\u6027\u548c\u6027\u80fd\u3002", "method": "\u91c7\u7528B/S\u67b6\u6784\u548cSpring Cloud\u6846\u67b6\uff0c\u7ed3\u5408\u591a\u79cd\u4e2d\u95f4\u4ef6\u7ec4\u4ef6\u4e0e\u5b89\u5168\u8bbe\u8ba1\u65b9\u6cd5\uff0c\u5b9e\u73b0\u7cfb\u7edf\u529f\u80fd\u5982\u5b9e\u65f6\u67e5\u8be2\u3001\u52a8\u6001\u5ea7\u4f4d\u66f4\u65b0\u3001\u5728\u7ebf\u9009\u5ea7\u4e0e\u8d2d\u7968\u7b49\u3002", "result": "\u7cfb\u7edf\u6d4b\u8bd5\u8868\u660e\uff0c\u5728\u9ad8\u5e76\u53d1\u73af\u5883\u4e0b\u5177\u6709\u826f\u597d\u7684\u7a33\u5b9a\u6027\u548c\u54cd\u5e94\u80fd\u529b\uff0c\u80fd\u6709\u6548\u89e3\u51b3\u7ebf\u4e0b\u8d2d\u7968\u6392\u961f\u4e0e\u4fe1\u606f\u5ef6\u8fdf\u7b49\u95ee\u9898\u3002", "conclusion": "\u8be5\u7cfb\u7edf\u5b9e\u73b0\u4e86\u4ece\u67e5\u8be2\u3001\u9884\u8ba2\u5230\u652f\u4ed8\u3001\u9000\u7968\u7684\u5168\u6d41\u7a0b\u7ebf\u4e0a\u5316\uff0c\u517c\u987e\u5b89\u5168\u6027\u3001\u7a33\u5b9a\u6027\u4e0e\u9ad8\u6027\u80fd\uff0c\u63d0\u5347\u4e86\u7528\u6237\u4f53\u9a8c\u4e0e\u670d\u52a1\u6548\u7387\u3002"}}
