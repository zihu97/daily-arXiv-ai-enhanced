{"id": "2510.02323", "categories": ["cs.OS", "cs.NI", "cs.PF", "D.4.2; D.4.8; C.2.1; C.2.4"], "pdf": "https://arxiv.org/pdf/2510.02323", "abs": "https://arxiv.org/abs/2510.02323", "authors": ["Joon Yong Hwang", "Chanseo Park", "Ikjun Yeom", "Younghoon Kim"], "title": "NetCAS: Dynamic Cache and Backend Device Management in Networked Environments", "comment": "10 pages, 8 figures, submitted to USENIX FAST 2026", "summary": "Modern storage systems often combine fast cache with slower backend devices\nto accelerate I/O. As performance gaps narrow, concurrently accessing both\ndevices, rather than relying solely on cache hits, can improve throughput.\nHowever, in data centers, remote backend storage accessed over networks suffers\nfrom unpredictable contention, complicating this split. We present NetCAS, a\nframework that dynamically splits I/O between cache and backend devices based\non real-time network feedback and a precomputed Perf Profile. Unlike\ntraditional hit-rate-based policies, NetCAS adapts split ratios to workload\nconfiguration and networking performance. NetCAS employs a low-overhead batched\nround-robin scheduler to enforce splits, avoiding per-request costs. It\nachieves up to 174% higher performance than traditional caching in remote\nstorage environments and outperforms converging schemes like Orthus by up to\n3.5X under fluctuating network conditions.", "AI": {"tldr": "NetCAS\u662f\u4e00\u4e2a\u52a8\u6001I/O\u5206\u5272\u6846\u67b6\uff0c\u901a\u8fc7\u5b9e\u65f6\u7f51\u7edc\u53cd\u9988\u548c\u9884\u8ba1\u7b97\u6027\u80fd\u914d\u7f6e\u6587\u4ef6\uff0c\u5728\u7f13\u5b58\u548c\u540e\u7aef\u8bbe\u5907\u95f4\u667a\u80fd\u5206\u914dI/O\u8bf7\u6c42\uff0c\u663e\u8457\u63d0\u5347\u8fdc\u7a0b\u5b58\u50a8\u73af\u5883\u6027\u80fd\u3002", "motivation": "\u73b0\u4ee3\u5b58\u50a8\u7cfb\u7edf\u901a\u5e38\u7ed3\u5408\u5feb\u901f\u7f13\u5b58\u548c\u6162\u901f\u540e\u7aef\u8bbe\u5907\u6765\u52a0\u901fI/O\u3002\u968f\u7740\u6027\u80fd\u5dee\u8ddd\u7f29\u5c0f\uff0c\u540c\u65f6\u8bbf\u95ee\u4e24\u4e2a\u8bbe\u5907\u800c\u975e\u4ec5\u4f9d\u8d56\u7f13\u5b58\u547d\u4e2d\u53ef\u4ee5\u63d0\u9ad8\u541e\u5410\u91cf\u3002\u4f46\u5728\u6570\u636e\u4e2d\u5fc3\u73af\u5883\u4e2d\uff0c\u901a\u8fc7\u7f51\u7edc\u8bbf\u95ee\u7684\u8fdc\u7a0b\u540e\u7aef\u5b58\u50a8\u9762\u4e34\u4e0d\u53ef\u9884\u6d4b\u7684\u7ade\u4e89\uff0c\u4f7f\u5f97\u8fd9\u79cd\u5206\u5272\u53d8\u5f97\u590d\u6742\u3002", "method": "NetCAS\u91c7\u7528\u57fa\u4e8e\u5b9e\u65f6\u7f51\u7edc\u53cd\u9988\u548c\u9884\u8ba1\u7b97\u6027\u80fd\u914d\u7f6e\u6587\u4ef6\u7684\u65b9\u6cd5\u52a8\u6001\u5206\u5272I/O\u3002\u4f7f\u7528\u4f4e\u5f00\u9500\u7684\u6279\u91cf\u8f6e\u8be2\u8c03\u5ea6\u5668\u6765\u6267\u884c\u5206\u5272\uff0c\u907f\u514d\u6bcf\u4e2a\u8bf7\u6c42\u7684\u5f00\u9500\u3002", "result": "NetCAS\u5728\u8fdc\u7a0b\u5b58\u50a8\u73af\u5883\u4e2d\u6bd4\u4f20\u7edf\u7f13\u5b58\u6027\u80fd\u63d0\u5347\u9ad8\u8fbe174%\uff0c\u5728\u7f51\u7edc\u6761\u4ef6\u6ce2\u52a8\u65f6\u6bd4Orthus\u7b49\u6536\u655b\u65b9\u6848\u6027\u80fd\u9ad8\u51fa3.5\u500d\u3002", "conclusion": "NetCAS\u901a\u8fc7\u52a8\u6001\u9002\u5e94\u5de5\u4f5c\u8d1f\u8f7d\u914d\u7f6e\u548c\u7f51\u7edc\u6027\u80fd\u7684\u5206\u5272\u7b56\u7565\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u8fdc\u7a0b\u5b58\u50a8\u73af\u5883\u4e2dI/O\u5206\u5272\u7684\u6311\u6218\uff0c\u663e\u8457\u63d0\u5347\u4e86\u7cfb\u7edf\u6027\u80fd\u3002"}}
{"id": "2510.02878", "categories": ["cs.DC", "cs.MS", "cs.PF"], "pdf": "https://arxiv.org/pdf/2510.02878", "abs": "https://arxiv.org/abs/2510.02878", "authors": ["Massimo Bernaschi", "Alessandro Celestini", "Pasqua D'Ambra", "Giorgio Richelli"], "title": "On the energy efficiency of sparse matrix computations on multi-GPU clusters", "comment": null, "summary": "We investigate the energy efficiency of a library designed for parallel\ncomputations with sparse matrices. The library leverages high-performance,\nenergy-efficient Graphics Processing Unit (GPU) accelerators to enable\nlarge-scale scientific applications. Our primary development objective was to\nmaximize parallel performance and scalability in solving sparse linear systems\nwhose dimensions far exceed the memory capacity of a single node. To this end,\nwe devised methods that expose a high degree of parallelism while optimizing\nalgorithmic implementations for efficient multi-GPU usage. Previous work has\nalready demonstrated the library's performance efficiency on large-scale\nsystems comprising thousands of NVIDIA GPUs, achieving improvements over\nstate-of-the-art solutions. In this paper, we extend those results by providing\nenergy profiles that address the growing sustainability requirements of modern\nHPC platforms. We present our methodology and tools for accurate runtime energy\nmeasurements of the library's core components and discuss the findings. Our\nresults confirm that optimizing GPU computations and minimizing data movement\nacross memory and computing nodes reduces both time-to-solution and energy\nconsumption. Moreover, we show that the library delivers substantial advantages\nover comparable software frameworks on standard benchmarks.", "AI": {"tldr": "\u7814\u7a76\u7a00\u758f\u77e9\u9635\u5e76\u884c\u8ba1\u7b97\u5e93\u7684\u80fd\u6548\u8868\u73b0\uff0c\u8be5\u5e93\u5229\u7528GPU\u52a0\u901f\u5668\u5b9e\u73b0\u5927\u89c4\u6a21\u79d1\u5b66\u8ba1\u7b97\uff0c\u5728\u89e3\u51b3\u8d85\u51fa\u5355\u8282\u70b9\u5185\u5b58\u5bb9\u91cf\u7684\u7a00\u758f\u7ebf\u6027\u7cfb\u7edf\u65f6\u4f18\u5316\u591aGPU\u4f7f\u7528\u7b97\u6cd5\uff0c\u5e76\u63d0\u4f9b\u4e86\u7cbe\u786e\u7684\u80fd\u8017\u6d4b\u91cf\u65b9\u6cd5\u548c\u7ed3\u679c\u3002", "motivation": "\u6ee1\u8db3\u73b0\u4ee3\u9ad8\u6027\u80fd\u8ba1\u7b97\u5e73\u53f0\u65e5\u76ca\u589e\u957f\u7684\u53ef\u6301\u7eed\u6027\u9700\u6c42\uff0c\u6269\u5c55\u4e4b\u524d\u5728\u5927\u89c4\u6a21GPU\u7cfb\u7edf\u4e0a\u5df2\u8bc1\u660e\u7684\u6027\u80fd\u6548\u7387\u7814\u7a76\uff0c\u63d0\u4f9b\u80fd\u8017\u5206\u6790\u4ee5\u5168\u9762\u8bc4\u4f30\u8ba1\u7b97\u5e93\u7684\u80fd\u6548\u8868\u73b0\u3002", "method": "\u5f00\u53d1\u4e86\u7cbe\u786e\u7684\u8fd0\u884c\u65f6\u80fd\u8017\u6d4b\u91cf\u65b9\u6cd5\u548c\u5de5\u5177\uff0c\u5bf9\u5e93\u7684\u6838\u5fc3\u7ec4\u4ef6\u8fdb\u884c\u80fd\u8017\u5206\u6790\uff0c\u4f18\u5316GPU\u8ba1\u7b97\u5e76\u6700\u5c0f\u5316\u5185\u5b58\u548c\u8ba1\u7b97\u8282\u70b9\u95f4\u7684\u6570\u636e\u79fb\u52a8\u3002", "result": "\u786e\u8ba4\u4f18\u5316GPU\u8ba1\u7b97\u548c\u51cf\u5c11\u6570\u636e\u79fb\u52a8\u65e2\u80fd\u7f29\u77ed\u6c42\u89e3\u65f6\u95f4\u53c8\u80fd\u964d\u4f4e\u80fd\u8017\uff0c\u76f8\u6bd4\u540c\u7c7b\u8f6f\u4ef6\u6846\u67b6\u5728\u6807\u51c6\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5c55\u73b0\u51fa\u663e\u8457\u4f18\u52bf\u3002", "conclusion": "\u8be5\u7a00\u758f\u77e9\u9635\u5e76\u884c\u8ba1\u7b97\u5e93\u5728\u4fdd\u6301\u9ad8\u6027\u80fd\u7684\u540c\u65f6\u5b9e\u73b0\u4e86\u4f18\u5f02\u7684\u80fd\u6548\u8868\u73b0\uff0c\u4e3a\u5927\u89c4\u6a21\u79d1\u5b66\u8ba1\u7b97\u5e94\u7528\u63d0\u4f9b\u4e86\u53ef\u6301\u7eed\u7684\u9ad8\u6027\u80fd\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.02675", "categories": ["cs.AR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.02675", "abs": "https://arxiv.org/abs/2510.02675", "authors": ["Shubham Negi", "Kaushik Roy"], "title": "HALO: Memory-Centric Heterogeneous Accelerator with 2.5D Integration for Low-Batch LLM Inference", "comment": null, "summary": "The rapid adoption of Large Language Models (LLMs) has driven a growing\ndemand for efficient inference, particularly in latency-sensitive applications\nsuch as chatbots and personalized assistants. Unlike traditional deep neural\nnetworks, LLM inference proceeds in two distinct phases: the prefill phase,\nwhich processes the full input sequence in parallel, and the decode phase,\nwhich generates tokens sequentially. These phases exhibit highly diverse\ncompute and memory requirements, which makes accelerator design particularly\nchallenging. Prior works have primarily been optimized for high-batch inference\nor evaluated only short input context lengths, leaving the low-batch and long\ncontext regime, which is critical for interactive applications, largely\nunderexplored.\n  We propose HALO, a heterogeneous memory centric accelerator designed for\nthese unique challenges of prefill and decode phases in low-batch LLM\ninference. HALO integrates HBM based Compute-in-DRAM (CiD) with an on-chip\nanalog Compute-in-Memory (CiM), co-packaged using 2.5D integration. To further\nimprove the hardware utilization, we introduce a phase-aware mapping strategy\nthat adapts to the distinct demands of the prefill and decode phases. Compute\nbound operations in the prefill phase are mapped to CiM to exploit its high\nthroughput matrix multiplication capability, while memory-bound operations in\nthe decode phase are executed on CiD to benefit from reduced data movement\nwithin DRAM. Additionally, we present an analysis of the performance tradeoffs\nof LLMs under two architectural extremes: a fully CiD and a fully on-chip\nanalog CiM design to highlight the need for a heterogeneous design. We evaluate\nHALO on LLaMA-2 7B and Qwen3 8B models. Our experimental results show that LLMs\nmapped to HALO achieve up to 18x geometric mean speedup over AttAcc, an\nattention-optimized mapping and 2.5x over CENT, a fully CiD based mapping.", "AI": {"tldr": "HALO\u662f\u4e00\u79cd\u5f02\u6784\u5185\u5b58\u4e2d\u5fc3\u52a0\u901f\u5668\uff0c\u4e13\u95e8\u9488\u5bf9LLM\u63a8\u7406\u4e2d\u7684prefill\u548cdecode\u9636\u6bb5\u8bbe\u8ba1\uff0c\u7ed3\u5408HBM CiD\u548c\u7247\u4e0a\u6a21\u62dfCiM\uff0c\u5728\u4f4e\u6279\u6b21\u957f\u4e0a\u4e0b\u6587\u573a\u666f\u4e0b\u5b9e\u73b0\u663e\u8457\u6027\u80fd\u63d0\u5347", "motivation": "LLM\u63a8\u7406\u5728\u4ea4\u4e92\u5f0f\u5e94\u7528\u4e2d\u9700\u8981\u4f4e\u5ef6\u8fdf\uff0c\u4f46\u73b0\u6709\u5de5\u4f5c\u4e3b\u8981\u9488\u5bf9\u9ad8\u6279\u6b21\u63a8\u7406\u6216\u77ed\u4e0a\u4e0b\u6587\uff0c\u4f4e\u6279\u6b21\u957f\u4e0a\u4e0b\u6587\u573a\u666f\u7814\u7a76\u4e0d\u8db3\uff0cprefill\u548cdecode\u9636\u6bb5\u7684\u5f02\u6784\u8ba1\u7b97\u5185\u5b58\u9700\u6c42\u7ed9\u52a0\u901f\u5668\u8bbe\u8ba1\u5e26\u6765\u6311\u6218", "method": "\u91c7\u75282.5D\u96c6\u6210\u5c06HBM CiD\u4e0e\u7247\u4e0a\u6a21\u62dfCiM\u7ed3\u5408\uff0c\u63d0\u51fa\u9636\u6bb5\u611f\u77e5\u6620\u5c04\u7b56\u7565\uff1aprefill\u9636\u6bb5\u8ba1\u7b97\u5bc6\u96c6\u578b\u64cd\u4f5c\u6620\u5c04\u5230CiM\uff0cdecode\u9636\u6bb5\u5185\u5b58\u5bc6\u96c6\u578b\u64cd\u4f5c\u6620\u5c04\u5230CiD\uff0c\u51cf\u5c11\u6570\u636e\u79fb\u52a8", "result": "\u5728LLaMA-2 7B\u548cQwen3 8B\u6a21\u578b\u4e0a\u6d4b\u8bd5\uff0c\u76f8\u6bd4AttAcc\u5b9e\u73b018\u500d\u51e0\u4f55\u5e73\u5747\u52a0\u901f\uff0c\u76f8\u6bd4\u5168CiD\u8bbe\u8ba1\u7684CENT\u5b9e\u73b02.5\u500d\u52a0\u901f", "conclusion": "\u5f02\u6784\u5185\u5b58\u67b6\u6784HALO\u80fd\u6709\u6548\u5e94\u5bf9LLM\u63a8\u7406\u4e2dprefill\u548cdecode\u9636\u6bb5\u7684\u5dee\u5f02\u5316\u9700\u6c42\uff0c\u5728\u4f4e\u6279\u6b21\u957f\u4e0a\u4e0b\u6587\u573a\u666f\u4e0b\u663e\u8457\u63d0\u5347\u6027\u80fd"}}
{"id": "2510.02487", "categories": ["cs.NI", "cs.CR"], "pdf": "https://arxiv.org/pdf/2510.02487", "abs": "https://arxiv.org/abs/2510.02487", "authors": ["Ahmed Danladi Abdullahi", "Erfan Bahrami", "Tooska Dargahi", "Mohammed Al-Khalidi", "Mohammad Hammoudeh"], "title": "Interplay between Security, Privacy and Trust in 6G-enabled Intelligent Transportation Systems", "comment": "Submitted to IEEE Open Journal of Intelligent Transportation Systems\n  (32 pages, 5 figures, 7 tables)", "summary": "The advancement of 6G technology has the potential to revolutionize the\ntransportation sector and significantly improve how we travel. 6G-enabled\nIntelligent Transportation Systems (ITS) promise to offer high-speed,\nlow-latency communication and advanced data analytics capabilities, supporting\nthe development of safer, more efficient, and more sustainable transportation\nsolutions. However, various security and privacy challenges were identified in\nthe literature that must be addressed to enable the safe and secure deployment\nof 6G-ITS and ensure people's trust in using these technologies. This paper\nreviews the opportunities and challenges of 6G-ITS, particularly focusing on\ntrust, security, and privacy, with special attention to quantum technologies\nthat both enhance security through quantum key distribution and introduce new\nvulnerabilities. It discusses the potential benefits of 6G technology in the\ntransportation sector, including improved communication, device\ninteroperability support, data analytic capabilities, and increased automation\nfor different components, such as transportation management and communication\nsystems. A taxonomy of different attack models in 6G-ITS is proposed, and a\ncomparison of the security threats in 5G-ITS and 6G-ITS is provided, along with\npotential mitigating solutions. This research highlights the urgent need for a\ncomprehensive, multi-layered security framework spanning physical\ninfrastructure protection, network protocol security, data management\nsafeguards, application security measures, and trust management systems to\neffectively mitigate emerging security and privacy risks and ensure the\nintegrity and resilience of future transportation ecosystems.", "AI": {"tldr": "\u672c\u6587\u7efc\u8ff0\u4e866G\u667a\u80fd\u4ea4\u901a\u7cfb\u7edf\u7684\u673a\u9047\u4e0e\u6311\u6218\uff0c\u91cd\u70b9\u5173\u6ce8\u4fe1\u4efb\u3001\u5b89\u5168\u548c\u9690\u79c1\u95ee\u9898\uff0c\u7279\u522b\u63a2\u8ba8\u4e86\u91cf\u5b50\u6280\u672f\u5e26\u6765\u7684\u53cc\u91cd\u5f71\u54cd\u2014\u2014\u901a\u8fc7\u91cf\u5b50\u5bc6\u94a5\u5206\u53d1\u589e\u5f3a\u5b89\u5168\u6027\uff0c\u540c\u65f6\u4e5f\u5f15\u5165\u65b0\u7684\u6f0f\u6d1e\u3002", "motivation": "6G\u6280\u672f\u6709\u671b\u5f7b\u5e95\u6539\u53d8\u4ea4\u901a\u884c\u4e1a\uff0c\u4f46\u5fc5\u987b\u89e3\u51b3\u5404\u79cd\u5b89\u5168\u548c\u9690\u79c1\u6311\u6218\uff0c\u4ee5\u786e\u4fdd6G-ITS\u7684\u5b89\u5168\u90e8\u7f72\u548c\u7528\u6237\u4fe1\u4efb\u3002", "method": "\u63d0\u51fa\u4e866G-ITS\u4e2d\u4e0d\u540c\u653b\u51fb\u6a21\u578b\u7684\u5206\u7c7b\u6cd5\uff0c\u6bd4\u8f83\u4e865G-ITS\u548c6G-ITS\u7684\u5b89\u5168\u5a01\u80c1\uff0c\u5e76\u63d0\u4f9b\u4e86\u6f5c\u5728\u7684\u7f13\u89e3\u89e3\u51b3\u65b9\u6848\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u9700\u8981\u6784\u5efa\u4e00\u4e2a\u5168\u9762\u7684\u591a\u5c42\u5b89\u5168\u6846\u67b6\uff0c\u6db5\u76d6\u7269\u7406\u57fa\u7840\u8bbe\u65bd\u4fdd\u62a4\u3001\u7f51\u7edc\u534f\u8bae\u5b89\u5168\u3001\u6570\u636e\u7ba1\u7406\u4fdd\u969c\u3001\u5e94\u7528\u5b89\u5168\u63aa\u65bd\u548c\u4fe1\u4efb\u7ba1\u7406\u7cfb\u7edf\u3002", "conclusion": "\u4e3a\u786e\u4fdd\u672a\u6765\u4ea4\u901a\u751f\u6001\u7cfb\u7edf\u7684\u5b8c\u6574\u6027\u548c\u97e7\u6027\uff0c\u8feb\u5207\u9700\u8981\u91c7\u7528\u7efc\u5408\u6027\u7684\u5b89\u5168\u65b9\u6cd5\u6765\u5e94\u5bf9\u65b0\u5174\u7684\u5b89\u5168\u548c\u9690\u79c1\u98ce\u9669\u3002"}}
{"id": "2510.02613", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2510.02613", "abs": "https://arxiv.org/abs/2510.02613", "authors": ["Gursimran Singh", "Timothy Yu", "Haley Li", "Cheng Chen", "Hanieh Sadri", "Qintao Zhang", "Yu Zhang", "Ying Xiong", "Yong Zhang", "Zhenan Fan"], "title": "ElasticMoE: An Efficient Auto Scaling Method for Mixture-of-Experts Models", "comment": "19 pages, 15 figures, Under Submission", "summary": "Mixture-of-Experts (MoE) models promise efficient scaling of large language\nmodels (LLMs) by activating only a small subset of experts per token, but their\nparallelized inference pipelines make elastic serving challenging. Existing\nstrategies fall short: horizontal scaling provisions entire replicas of the\ncurrent configuration, often tens to hundreds of accelerators, leading to\ncoarse granularity, long provisioning delays, and costly overprovisioning.\nVertical scaling offers finer adjustments but typically requires instance\nrestarts, incurring downtime. These limitations make current approaches\nill-suited for the bursty, short-lived traffic patterns common in cloud\ndeployments.\n  We present ElasticMoE, an elastic scaling framework for MoE LLMs that\nachieves fine-grained, low-latency, and zero-downtime scaling. ElasticMoE\ndecouples inference execution from memory operations, enabling scaling steps to\nproceed concurrently with serving. An HBM Management Module (HMM) reuses\nweights and KV caches via zero-copy remapping, while high-bandwidth\npeer-to-peer transfers bring newly added accelerators online without\ninterrupting service. A virtual memory based expert redistribution mechanism\nmigrates MoE experts without costly buffer reallocations, reducing peak memory\nusage during expert parallelism reconfiguration.\n  Our evaluation on Ascend NPUs with three popular MoE LLMs shows that\nElasticMoE achieves up to 9x lower scale-up latency, up to 2x better throughput\nduring scaling, and significantly improves SLO attainment compared to\nbaselines. By enabling fine-grained, concurrent scaling with minimal\ndisruption, ElasticMoE advances the practicality of deploying massive MoE LLMs\nin dynamic cloud environments.", "AI": {"tldr": "ElasticMoE\u662f\u4e00\u4e2a\u7528\u4e8e\u6df7\u5408\u4e13\u5bb6\u6a21\u578b(MoE)\u7684\u5f39\u6027\u6269\u5c55\u6846\u67b6\uff0c\u901a\u8fc7\u89e3\u8026\u63a8\u7406\u6267\u884c\u548c\u5185\u5b58\u64cd\u4f5c\uff0c\u5b9e\u73b0\u7ec6\u7c92\u5ea6\u3001\u4f4e\u5ef6\u8fdf\u3001\u96f6\u505c\u673a\u6269\u5c55", "motivation": "\u73b0\u6709MoE\u6a21\u578b\u7684\u6269\u5c55\u7b56\u7565\u5b58\u5728\u7c92\u5ea6\u7c97\u3001\u914d\u7f6e\u5ef6\u8fdf\u957f\u3001\u6210\u672c\u9ad8\u548c\u9700\u8981\u505c\u673a\u91cd\u542f\u7b49\u95ee\u9898\uff0c\u65e0\u6cd5\u9002\u5e94\u4e91\u90e8\u7f72\u4e2d\u5e38\u89c1\u7684\u7a81\u53d1\u6027\u77ed\u65f6\u6d41\u91cf\u6a21\u5f0f", "method": "\u91c7\u7528HBM\u7ba1\u7406\u6a21\u5757\u91cd\u7528\u6743\u91cd\u548cKV\u7f13\u5b58\uff0c\u901a\u8fc7\u96f6\u62f7\u8d1d\u91cd\u6620\u5c04\uff1b\u4f7f\u7528\u9ad8\u5e26\u5bbd\u70b9\u5bf9\u70b9\u4f20\u8f93\u6dfb\u52a0\u65b0\u52a0\u901f\u5668\uff1b\u57fa\u4e8e\u865a\u62df\u5185\u5b58\u7684\u4e13\u5bb6\u91cd\u5206\u914d\u673a\u5236\u8fc1\u79fb\u4e13\u5bb6", "result": "\u5728Ascend NPU\u4e0a\u7684\u8bc4\u4f30\u663e\u793a\uff0cElasticMoE\u76f8\u6bd4\u57fa\u7ebf\u5b9e\u73b0\u4e869\u500d\u66f4\u4f4e\u7684\u6269\u5c55\u5ef6\u8fdf\u30012\u500d\u66f4\u597d\u7684\u541e\u5410\u91cf\uff0c\u663e\u8457\u6539\u5584\u4e86SLO\u8fbe\u6210\u7387", "conclusion": "ElasticMoE\u901a\u8fc7\u6700\u5c0f\u5e72\u6270\u7684\u7ec6\u7c92\u5ea6\u5e76\u53d1\u6269\u5c55\uff0c\u63d0\u5347\u4e86\u5927\u89c4\u6a21MoE LLM\u5728\u52a8\u6001\u4e91\u73af\u5883\u4e2d\u90e8\u7f72\u7684\u5b9e\u7528\u6027"}}
{"id": "2510.02387", "categories": ["cs.SE", "cs.AI", "cs.LG", "68T07", "I.2.7"], "pdf": "https://arxiv.org/pdf/2510.02387", "abs": "https://arxiv.org/abs/2510.02387", "authors": ["FAIR CodeGen team", "Quentin Carbonneaux", "Gal Cohen", "Jonas Gehring", "Jacob Kahn", "Jannik Kossen", "Felix Kreuk", "Emily McMilin", "Michel Meyer", "Yuxiang Wei", "David Zhang", "Kunhao Zheng", "Jordi Armengol-Estap\u00e9", "Pedram Bashiri", "Maximilian Beck", "Pierre Chambon", "Abhishek Charnalia", "Chris Cummins", "Juliette Decugis", "Zacharias V. Fisches", "Fran\u00e7ois Fleuret", "Fabian Gloeckle", "Alex Gu", "Michael Hassid", "Daniel Haziza", "Badr Youbi Idrissi", "Christian Keller", "Rahul Kindi", "Hugh Leather", "Gallil Maimon", "Aram Markosyan", "Francisco Massa", "Pierre-Emmanuel Mazar\u00e9", "Vegard Mella", "Naila Murray", "Keyur Muzumdar", "Peter O'Hearn", "Matteo Pagliardini", "Dmitrii Pedchenko", "Tal Remez", "Volker Seeker", "Marco Selvi", "Oren Sultan", "Sida Wang", "Luca Wehrstedt", "Ori Yoran", "Lingming Zhang", "Taco Cohen", "Yossi Adi", "Gabriel Synnaeve"], "title": "CWM: An Open-Weights LLM for Research on Code Generation with World Models", "comment": "58 pages", "summary": "We release Code World Model (CWM), a 32-billion-parameter open-weights LLM,\nto advance research on code generation with world models. To improve code\nunderstanding beyond what can be learned from training on static code alone, we\nmid-train CWM on a large amount of observation-action trajectories from Python\ninterpreter and agentic Docker environments, and perform extensive multi-task\nreasoning RL in verifiable coding, math, and multi-turn software engineering\nenvironments. With CWM, we provide a strong testbed for researchers to explore\nthe opportunities world modeling affords for improving code generation with\nreasoning and planning in computational environments. We present first steps of\nhow world models can benefit agentic coding, enable step-by-step simulation of\nPython code execution, and show early results of how reasoning can benefit from\nthe latter. CWM is a dense, decoder-only LLM trained with a context size of up\nto 131k tokens. Independent of its world modeling capabilities, CWM offers\nstrong performance on general coding and math tasks: it reaches pass@1 scores\nof 65.8% on SWE-bench Verified (with test-time scaling), 68.6% on\nLiveCodeBench, 96.6% on Math-500, and 76.0% on AIME 2024. To support further\nresearch on code world modeling, we release model checkpoints after\nmid-training, SFT, and RL.", "AI": {"tldr": "Code World Model (CWM)\u662f\u4e00\u4e2a320\u4ebf\u53c2\u6570\u7684\u5f00\u6e90LLM\uff0c\u901a\u8fc7\u5728Python\u89e3\u91ca\u5668\u548cDocker\u73af\u5883\u4e2d\u8bad\u7ec3\u89c2\u5bdf-\u884c\u52a8\u8f68\u8ff9\uff0c\u63d0\u5347\u4ee3\u7801\u7406\u89e3\u548c\u751f\u6210\u80fd\u529b\uff0c\u5728\u591a\u9879\u7f16\u7a0b\u548c\u6570\u5b66\u4efb\u52a1\u4e0a\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u4e3a\u4e86\u63d0\u5347\u4ee3\u7801\u751f\u6210\u80fd\u529b\uff0c\u8d85\u8d8a\u4ec5\u4ece\u9759\u6001\u4ee3\u7801\u8bad\u7ec3\u6240\u80fd\u5b66\u5230\u7684\u5185\u5bb9\uff0c\u9700\u8981\u6784\u5efa\u80fd\u591f\u7406\u89e3\u548c\u6a21\u62df\u8ba1\u7b97\u73af\u5883\u7684\u4e16\u754c\u6a21\u578b\u3002", "method": "\u5728\u5927\u91cfPython\u89e3\u91ca\u5668\u548cDocker\u73af\u5883\u7684\u89c2\u5bdf-\u884c\u52a8\u8f68\u8ff9\u4e0a\u8fdb\u884c\u4e2d\u671f\u8bad\u7ec3\uff0c\u5e76\u8fdb\u884c\u591a\u4efb\u52a1\u63a8\u7406\u5f3a\u5316\u5b66\u4e60\uff0c\u5305\u62ec\u53ef\u9a8c\u8bc1\u7f16\u7a0b\u3001\u6570\u5b66\u548c\u591a\u8f6e\u8f6f\u4ef6\u5de5\u7a0b\u73af\u5883\u3002", "result": "CWM\u5728\u591a\u9879\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\uff1aSWE-bench Verified 65.8%\u3001LiveCodeBench 68.6%\u3001Math-500 96.6%\u3001AIME 2024 76.0%\u3002", "conclusion": "CWM\u4e3a\u7814\u7a76\u4ee3\u7801\u4e16\u754c\u5efa\u6a21\u63d0\u4f9b\u4e86\u5f3a\u5927\u5e73\u53f0\uff0c\u5c55\u793a\u4e86\u4e16\u754c\u6a21\u578b\u5982\u4f55\u901a\u8fc7\u63a8\u7406\u548c\u89c4\u5212\u63d0\u5347\u4ee3\u7801\u751f\u6210\u80fd\u529b\uff0c\u5e76\u652f\u6301\u9010\u6b65\u6a21\u62dfPython\u4ee3\u7801\u6267\u884c\u3002"}}
{"id": "2510.02863", "categories": ["cs.AR", "cs.DS", "cs.NA", "math.NA", "quant-ph", "G.1.3; J.2; B.6.1"], "pdf": "https://arxiv.org/pdf/2510.02863", "abs": "https://arxiv.org/abs/2510.02863", "authors": ["D. A. Herrera-Mart\u00ed", "E. Guthmuller", "J. Fereyre"], "title": "A Hardware Accelerator for the Goemans-Williamson Algorithm", "comment": "Impact of Extended Precision Arithmetic in Interior Point Methods\n  using Conjugate Gradient. 10 pages. Hardware estimates", "summary": "The combinatorial problem Max-Cut has become a benchmark in the evaluation of\nlocal search heuristics for both quantum and classical optimisers. In contrast\nto local search, which only provides average-case performance guarantees, the\nconvex semidefinite relaxation of Max-Cut by Goemans and Williamson, provides\nworst-case guarantees and is therefore suited to both the construction of\nbenchmarks and in applications to performance-critic scenarios.\n  We show how extended floating point precision can be incorporated in\nalgebraic subroutines in convex optimisation, namely in indirect matrix\ninversion methods like Conjugate Gradient, which are used in Interior Point\nMethods in the case of very large problem sizes. Also, an estimate is provided\nof the expected acceleration of the time to solution for a hardware\narchitecture that runs natively on extended precision. Specifically, when using\nindirect matrix inversion methods like Conjugate Gradient, which have lower\ncomplexity than direct methods and are therefore used in very large problems,\nwe see that increasing the internal working precision reduces the time to\nsolution by a factor that increases with the system size.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u4e86\u5728\u51f8\u4f18\u5316\u4e2d\u5f15\u5165\u6269\u5c55\u6d6e\u70b9\u7cbe\u5ea6\u6765\u52a0\u901f\u5927\u89c4\u6a21Max-Cut\u95ee\u9898\u7684\u6c42\u89e3\uff0c\u7279\u522b\u662f\u5728\u5171\u8f6d\u68af\u5ea6\u6cd5\u7b49\u95f4\u63a5\u77e9\u9635\u6c42\u9006\u7b97\u6cd5\u4e2d\u7684\u5e94\u7528\u3002", "motivation": "Max-Cut\u95ee\u9898\u5df2\u6210\u4e3a\u91cf\u5b50\u4e0e\u7ecf\u5178\u4f18\u5316\u5668\u5c40\u90e8\u641c\u7d22\u542f\u53d1\u5f0f\u7b97\u6cd5\u7684\u57fa\u51c6\u6d4b\u8bd5\u3002\u4e0e\u4ec5\u63d0\u4f9b\u5e73\u5747\u6027\u80fd\u4fdd\u8bc1\u7684\u5c40\u90e8\u641c\u7d22\u4e0d\u540c\uff0cGoemans-Williamson\u7684\u51f8\u534a\u5b9a\u677e\u5f1b\u65b9\u6cd5\u63d0\u4f9b\u6700\u574f\u60c5\u51b5\u4fdd\u8bc1\uff0c\u9002\u7528\u4e8e\u57fa\u51c6\u6784\u5efa\u548c\u6027\u80fd\u5173\u952e\u573a\u666f\u3002", "method": "\u7814\u7a76\u5982\u4f55\u5728\u51f8\u4f18\u5316\u7684\u4ee3\u6570\u5b50\u7a0b\u5e8f\uff08\u7279\u522b\u662f\u5171\u8f6d\u68af\u5ea6\u6cd5\u7b49\u95f4\u63a5\u77e9\u9635\u6c42\u9006\u65b9\u6cd5\uff09\u4e2d\u6574\u5408\u6269\u5c55\u6d6e\u70b9\u7cbe\u5ea6\uff0c\u8fd9\u4e9b\u65b9\u6cd5\u5728\u5185\u70b9\u6cd5\u4e2d\u7528\u4e8e\u5904\u7406\u8d85\u5927\u89c4\u6a21\u95ee\u9898\u3002", "result": "\u4f7f\u7528\u6269\u5c55\u7cbe\u5ea6\u53ef\u51cf\u5c11\u6c42\u89e3\u65f6\u95f4\uff0c\u8be5\u52a0\u901f\u56e0\u5b50\u968f\u7cfb\u7edf\u89c4\u6a21\u589e\u5927\u800c\u589e\u52a0\u3002\u5f53\u4f7f\u7528\u590d\u6742\u5ea6\u4f4e\u4e8e\u76f4\u63a5\u65b9\u6cd5\u7684\u95f4\u63a5\u77e9\u9635\u6c42\u9006\u65b9\u6cd5\u65f6\uff0c\u63d0\u9ad8\u5185\u90e8\u5de5\u4f5c\u7cbe\u5ea6\u80fd\u663e\u8457\u63d0\u5347\u5927\u89c4\u6a21\u95ee\u9898\u7684\u6c42\u89e3\u6548\u7387\u3002", "conclusion": "\u6269\u5c55\u6d6e\u70b9\u7cbe\u5ea6\u5728\u5927\u89c4\u6a21\u51f8\u4f18\u5316\u95ee\u9898\u4e2d\u5177\u6709\u5b9e\u9645\u4ef7\u503c\uff0c\u7279\u522b\u662f\u5728\u4f7f\u7528\u5171\u8f6d\u68af\u5ea6\u6cd5\u7b49\u95f4\u63a5\u65b9\u6cd5\u65f6\uff0c\u80fd\u591f\u63d0\u4f9b\u53ef\u6269\u5c55\u7684\u6027\u80fd\u52a0\u901f\u3002"}}
{"id": "2510.02682", "categories": ["cs.NI"], "pdf": "https://arxiv.org/pdf/2510.02682", "abs": "https://arxiv.org/abs/2510.02682", "authors": ["Haoran Wan", "Kyle Jamieson"], "title": "L4Span: Spanning Congestion Signaling over NextG Networks for Interactive Applications", "comment": null, "summary": "Design for low latency networking is essential for tomorrow's interactive\napplications, but it is essential to deploy incrementally and universally at\nthe network's last mile. While wired broadband ISPs are rolling out the leading\nqueue occupancy signaling mechanisms, the cellular Radio Access Network (RAN),\nanother important last mile to many users, lags behind these efforts. This\npaper proposes a new RAN design, L4Span, that abstracts the complexities of RAN\nqueueing in a simple interface, thus tying the queue state of the RAN to\nend-to-end low-latency signaling all the way back to the content server. At\nmillisecond-level timescales, L4Span predicts the RAN's queuing occupancy and\nperforms ECN marking for both low-latency and classic flows. L4Span is\nlightweight, requiring minimal RAN modifications, and remains 3GPP and O-RAN\ncompliant for maximum ease of deployment. We implement a prototype on the\nsrsRAN open-source software in C++. Our evaluation compares the performance of\nlow-latency as well as classic flows with or without the deployment of L4Span\nin various wireless channel conditions. Results show that L4Span reduces the\none-way delay of both low-latency and classic flows by up to 98 %, while\nsimultaneously maintaining near line-rate throughput. The code is available at\nhttps://github.com/PrincetonUniversity/L4Span.", "AI": {"tldr": "L4Span\u662f\u4e00\u79cd\u65b0\u578bRAN\u8bbe\u8ba1\uff0c\u901a\u8fc7\u7b80\u5316RAN\u961f\u5217\u72b6\u6001\u7684\u62bd\u8c61\u63a5\u53e3\uff0c\u5c06RAN\u961f\u5217\u72b6\u6001\u4e0e\u7aef\u5230\u7aef\u4f4e\u5ef6\u8fdf\u4fe1\u4ee4\u8fde\u63a5\uff0c\u663e\u8457\u964d\u4f4e\u5ef6\u8fdf\u540c\u65f6\u4fdd\u6301\u9ad8\u541e\u5410\u91cf\u3002", "motivation": "\u6709\u7ebf\u5bbd\u5e26ISP\u5df2\u90e8\u7f72\u5148\u8fdb\u7684\u961f\u5217\u5360\u7528\u4fe1\u4ee4\u673a\u5236\uff0c\u4f46\u8702\u7a9d\u65e0\u7ebf\u63a5\u5165\u7f51\u7edc\uff08RAN\uff09\u5728\u8fd9\u65b9\u9762\u843d\u540e\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u591f\u589e\u91cf\u90e8\u7f72\u4e14\u517c\u5bb9\u73b0\u6709\u6807\u51c6\u7684\u4f4e\u5ef6\u8fdf\u89e3\u51b3\u65b9\u6848\u3002", "method": "L4Span\u5728\u6beb\u79d2\u7ea7\u65f6\u95f4\u5c3a\u5ea6\u9884\u6d4bRAN\u961f\u5217\u5360\u7528\u60c5\u51b5\uff0c\u4e3a\u4f4e\u5ef6\u8fdf\u548c\u4f20\u7edf\u6d41\u91cf\u6267\u884cECN\u6807\u8bb0\uff0c\u91c7\u7528\u8f7b\u91cf\u7ea7\u8bbe\u8ba1\uff0c\u6700\u5c0f\u5316RAN\u4fee\u6539\uff0c\u4fdd\u63013GPP\u548cO-RAN\u517c\u5bb9\u6027\u3002", "result": "\u8bc4\u4f30\u663e\u793aL4Span\u5c06\u4f4e\u5ef6\u8fdf\u548c\u4f20\u7edf\u6d41\u91cf\u7684\u5355\u5411\u5ef6\u8fdf\u964d\u4f4e\u9ad8\u8fbe98%\uff0c\u540c\u65f6\u7ef4\u6301\u63a5\u8fd1\u7ebf\u901f\u7684\u541e\u5410\u91cf\u3002", "conclusion": "L4Span\u4e3aRAN\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u4f4e\u5ef6\u8fdf\u89e3\u51b3\u65b9\u6848\uff0c\u6613\u4e8e\u90e8\u7f72\u4e14\u6027\u80fd\u663e\u8457\uff0c\u6709\u52a9\u4e8e\u63a8\u52a8\u4ea4\u4e92\u5f0f\u5e94\u7528\u7684\u53d1\u5c55\u3002"}}
{"id": "2510.02774", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2510.02774", "abs": "https://arxiv.org/abs/2510.02774", "authors": ["Xiang Li", "Qiong Chang", "Yun Li", "Jun Miyazaki"], "title": "GRNND: A GPU-Parallel Relative NN-Descent Algorithm for Efficient Approximate Nearest Neighbor Graph Construction", "comment": null, "summary": "Relative Nearest Neighbor Descent (RNN-Descent) is a state-of-the-art\nalgorithm for constructing sparse approximate nearest neighbor (ANN) graphs by\ncombining the iterative refinement of NN-Descent with the edge-pruning rules of\nthe Relative Neighborhood Graph (RNG). It has demonstrated strong effectiveness\nin large-scale search tasks such as information retrieval and related tasks.\nHowever, as the amount and dimensionality of data increase, the complexity of\ngraph construction in RNN-Descent rises sharply, making this stage increasingly\ntime-consuming and even prohibitive for subsequent query processing. In this\npaper, we propose GRNND, the first GPU-parallel algorithm of RNN-Descent\ndesigned to fully exploit GPU architecture. GRNND introduces a disordered\nneighbor propagation strategy to mitigate synchronized update traps, enhancing\nstructural diversity, and avoiding premature convergence during parallel\nexecution. It also leverages warp-level cooperative operations and a\ndouble-buffered neighbor pool with fixed capacity for efficient memory access,\neliminate contention, and enable highly parallelized neighbor updates.\nExtensive experiments demonstrate that GRNND consistently outperforms existing\nCPU- and GPU-based methods. GRNND achieves 2.4 to 51.7x speedup over existing\nGPU methods, and 17.8 to 49.8x speedup over CPU methods.", "AI": {"tldr": "GRNND\u662f\u9996\u4e2a\u9488\u5bf9RNN-Descent\u7b97\u6cd5\u7684GPU\u5e76\u884c\u5b9e\u73b0\uff0c\u901a\u8fc7\u65e0\u5e8f\u90bb\u5c45\u4f20\u64ad\u7b56\u7565\u548cGPU\u67b6\u6784\u4f18\u5316\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5927\u89c4\u6a21\u9ad8\u7ef4\u6570\u636e\u4e0b\u7684\u8fd1\u4f3c\u6700\u8fd1\u90bb\u56fe\u6784\u5efa\u6548\u7387\u3002", "motivation": "\u968f\u7740\u6570\u636e\u91cf\u548c\u7ef4\u5ea6\u7684\u589e\u52a0\uff0cRNN-Descent\u7b97\u6cd5\u5728\u56fe\u6784\u5efa\u9636\u6bb5\u7684\u590d\u6742\u5ea6\u6025\u5267\u4e0a\u5347\uff0c\u53d8\u5f97\u975e\u5e38\u8017\u65f6\uff0c\u751a\u81f3\u963b\u788d\u540e\u7eed\u67e5\u8be2\u5904\u7406\u3002", "method": "\u63d0\u51faGRNND\u7b97\u6cd5\uff0c\u91c7\u7528\u65e0\u5e8f\u90bb\u5c45\u4f20\u64ad\u7b56\u7565\u7f13\u89e3\u540c\u6b65\u66f4\u65b0\u9677\u9631\uff0c\u5229\u7528warp\u7ea7\u534f\u4f5c\u64cd\u4f5c\u548c\u53cc\u7f13\u51b2\u90bb\u5c45\u6c60\u5b9e\u73b0\u9ad8\u6548\u5185\u5b58\u8bbf\u95ee\u548c\u65e0\u51b2\u7a81\u7684\u5e76\u884c\u90bb\u5c45\u66f4\u65b0\u3002", "result": "GRNND\u76f8\u6bd4\u73b0\u6709GPU\u65b9\u6cd5\u83b7\u5f972.4-51.7\u500d\u52a0\u901f\uff0c\u76f8\u6bd4CPU\u65b9\u6cd5\u83b7\u5f9717.8-49.8\u500d\u52a0\u901f\uff0c\u5728\u591a\u4e2a\u5b9e\u9a8c\u4e2d\u5747\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "GRNND\u6210\u529f\u5b9e\u73b0\u4e86RNN-Descent\u7b97\u6cd5\u7684GPU\u5e76\u884c\u5316\uff0c\u901a\u8fc7\u521b\u65b0\u7684\u5e76\u884c\u7b56\u7565\u548c\u5185\u5b58\u4f18\u5316\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5927\u89c4\u6a21\u8fd1\u4f3c\u6700\u8fd1\u90bb\u56fe\u6784\u5efa\u7684\u6027\u80fd\u3002"}}
{"id": "2510.02389", "categories": ["cs.SE", "cs.CR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.02389", "abs": "https://arxiv.org/abs/2510.02389", "authors": ["Haoran Xi", "Minghao Shao", "Brendan Dolan-Gavitt", "Muhammad Shafique", "Ramesh Karri"], "title": "From Trace to Line: LLM Agent for Real-World OSS Vulnerability Localization", "comment": null, "summary": "Large language models show promise for vulnerability discovery, yet\nprevailing methods inspect code in isolation, struggle with long contexts, and\nfocus on coarse function- or file-level detections - offering limited\nactionable guidance to engineers who need precise line-level localization and\ntargeted patches in real-world software development. We present T2L-Agent\n(Trace-to-Line Agent), a project-level, end-to-end framework that plans its own\nanalysis and progressively narrows scope from modules to exact vulnerable\nlines. T2L-Agent couples multi-round feedback with an Agentic Trace Analyzer\n(ATA) that fuses runtime evidence - crash points, stack traces, and coverage\ndeltas - with AST-based code chunking, enabling iterative refinement beyond\nsingle pass predictions and translating symptoms into actionable, line-level\ndiagnoses. To benchmark line-level vulnerability discovery, we introduce\nT2L-ARVO, a diverse, expert-verified 50-case benchmark spanning five crash\nfamilies and real-world projects. T2L-ARVO is specifically designed to support\nboth coarse-grained detection and fine-grained localization, enabling rigorous\nevaluation of systems that aim to move beyond file-level predictions. On\nT2L-ARVO, T2L-Agent achieves up to 58.0% detection and 54.8% line-level\nlocalization, substantially outperforming baselines. Together, the framework\nand benchmark push LLM-based vulnerability detection from coarse identification\ntoward deployable, robust, precision diagnostics that reduce noise and\naccelerate patching in open-source software workflows.", "AI": {"tldr": "T2L-Agent\u662f\u4e00\u4e2a\u9879\u76ee\u7ea7\u522b\u7684\u7aef\u5230\u7aef\u6f0f\u6d1e\u53d1\u73b0\u6846\u67b6\uff0c\u901a\u8fc7\u591a\u8f6e\u53cd\u9988\u548c\u8fd0\u884c\u65f6\u8bc1\u636e\u878d\u5408\uff0c\u5b9e\u73b0\u4e86\u4ece\u6a21\u5757\u5230\u7cbe\u786e\u6f0f\u6d1e\u884c\u7684\u6e10\u8fdb\u5f0f\u5b9a\u4f4d\uff0c\u5728T2L-ARVO\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u6f0f\u6d1e\u68c0\u6d4b\u65b9\u6cd5\u5728\u5b64\u7acb\u4ee3\u7801\u5206\u6790\u3001\u957f\u4e0a\u4e0b\u6587\u5904\u7406\u548c\u7c97\u7c92\u5ea6\u68c0\u6d4b\u65b9\u9762\u5b58\u5728\u5c40\u9650\uff0c\u65e0\u6cd5\u4e3a\u5de5\u7a0b\u5e08\u63d0\u4f9b\u7cbe\u786e\u7684\u884c\u7ea7\u5b9a\u4f4d\u548c\u9488\u5bf9\u6027\u4fee\u590d\u6307\u5bfc\u3002", "method": "T2L-Agent\u7ed3\u5408\u591a\u8f6e\u53cd\u9988\u548c\u4ee3\u7406\u8ffd\u8e2a\u5206\u6790\u5668(ATA)\uff0c\u878d\u5408\u8fd0\u884c\u65f6\u8bc1\u636e(\u5d29\u6e83\u70b9\u3001\u5806\u6808\u8ddf\u8e2a\u3001\u8986\u76d6\u7387\u5dee\u5f02)\u4e0e\u57fa\u4e8eAST\u7684\u4ee3\u7801\u5206\u5757\uff0c\u5b9e\u73b0\u8fed\u4ee3\u7cbe\u70bc\u7684\u7cbe\u786e\u884c\u7ea7\u8bca\u65ad\u3002", "result": "\u5728T2L-ARVO\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cT2L-Agent\u8fbe\u523058.0%\u7684\u68c0\u6d4b\u7387\u548c54.8%\u7684\u884c\u7ea7\u5b9a\u4f4d\u7387\uff0c\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u6846\u67b6\u548c\u57fa\u51c6\u6d4b\u8bd5\u5c06\u57fa\u4e8eLLM\u7684\u6f0f\u6d1e\u68c0\u6d4b\u4ece\u7c97\u7c92\u5ea6\u8bc6\u522b\u63a8\u5411\u53ef\u90e8\u7f72\u7684\u3001\u9c81\u68d2\u7684\u7cbe\u786e\u8bca\u65ad\uff0c\u51cf\u5c11\u4e86\u566a\u58f0\u5e76\u52a0\u901f\u4e86\u5f00\u6e90\u8f6f\u4ef6\u5de5\u4f5c\u6d41\u4e2d\u7684\u8865\u4e01\u8fc7\u7a0b\u3002"}}
{"id": "2510.02990", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2510.02990", "abs": "https://arxiv.org/abs/2510.02990", "authors": ["Philippe Magalh\u00e3es", "Virginie Fresse", "Beno\u00eet Suffran", "Olivier Alata"], "title": "A Resource-Driven Approach for Implementing CNNs on FPGAs Using Adaptive IPs", "comment": "HiPEAC Workshop on Reconfigurable Computing (WRC), Jan 2025,\n  Barcelona, Spain", "summary": "The increasing demand for real-time, low-latency artificial intelligence\napplications has propelled the use of Field-Programmable Gate Arrays (FPGAs)\nfor Convolutional Neural Network (CNN) implementations. FPGAs offer\nreconfigurability, energy efficiency, and performance advantages over GPUs,\nmaking them suitable for edge devices and embedded systems. This work presents\na novel library of resource-efficient convolution IPs designed to automatically\nadapt to the available FPGA resources. Developed in VHDL, these IPs are\nparameterizable and utilize fixed-point arithmetic for optimal performance.\nFour IPs are introduced, each tailored to specific resource constraints,\noffering flexibility in DSP usage, logic consumption, and precision.\nExperimental results on a Zynq UltraScale+ FPGA highlight the trade-offs\nbetween performance and resource usage. The comparison with recent FPGA-based\nCNN acceleration techniques emphasizes the versatility and independence of this\napproach from specific FPGA architectures or technological advancements. Future\nwork will expand the library to include pooling and activation functions,\nenabling broader applicability and integration into CNN frameworks.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eFPGA\u7684\u8d44\u6e90\u9ad8\u6548\u5377\u79efIP\u5e93\uff0c\u53ef\u81ea\u52a8\u9002\u914d\u53ef\u7528\u8d44\u6e90\uff0c\u91c7\u7528\u53c2\u6570\u5316\u8bbe\u8ba1\u548c\u5b9a\u70b9\u8fd0\u7b97\uff0c\u5728Zynq UltraScale+ FPGA\u4e0a\u9a8c\u8bc1\u4e86\u6027\u80fd\u4e0e\u8d44\u6e90\u4f7f\u7528\u7684\u6743\u8861\u3002", "motivation": "\u968f\u7740\u5b9e\u65f6\u4f4e\u5ef6\u8fdfAI\u5e94\u7528\u9700\u6c42\u7684\u589e\u957f\uff0cFPGA\u5728CNN\u5b9e\u73b0\u4e2d\u76f8\u6bd4GPU\u5177\u6709\u53ef\u91cd\u6784\u6027\u3001\u80fd\u6548\u548c\u6027\u80fd\u4f18\u52bf\uff0c\u7279\u522b\u9002\u5408\u8fb9\u7f18\u8bbe\u5907\u548c\u5d4c\u5165\u5f0f\u7cfb\u7edf\u3002", "method": "\u5f00\u53d1\u4e86\u7528VHDL\u7f16\u5199\u7684\u53c2\u6570\u5316\u5377\u79efIP\u5e93\uff0c\u91c7\u7528\u5b9a\u70b9\u8fd0\u7b97\uff0c\u5305\u542b\u56db\u4e2a\u9488\u5bf9\u4e0d\u540c\u8d44\u6e90\u7ea6\u675f\u5b9a\u5236\u7684IP\u6838\uff0c\u63d0\u4f9bDSP\u4f7f\u7528\u3001\u903b\u8f91\u6d88\u8017\u548c\u7cbe\u5ea6\u7684\u7075\u6d3b\u6027\u3002", "result": "\u5728Zynq UltraScale+ FPGA\u4e0a\u7684\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\u6027\u80fd\u4e0e\u8d44\u6e90\u4f7f\u7528\u7684\u6743\u8861\uff0c\u4e0e\u73b0\u6709FPGA\u52a0\u901f\u6280\u672f\u76f8\u6bd4\u5c55\u73b0\u51fa\u66f4\u597d\u7684\u901a\u7528\u6027\u548c\u67b6\u6784\u72ec\u7acb\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3aFPGA\u4e0a\u7684CNN\u52a0\u901f\u63d0\u4f9b\u4e86\u7075\u6d3b\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u672a\u6765\u5c06\u6269\u5c55\u5e93\u529f\u80fd\u4ee5\u652f\u6301\u6c60\u5316\u548c\u6fc0\u6d3b\u51fd\u6570\uff0c\u589e\u5f3a\u5728CNN\u6846\u67b6\u4e2d\u7684\u96c6\u6210\u80fd\u529b\u3002"}}
{"id": "2510.02800", "categories": ["cs.NI", "eess.SP"], "pdf": "https://arxiv.org/pdf/2510.02800", "abs": "https://arxiv.org/abs/2510.02800", "authors": ["Rohith Reddy Vennam", "Maiyun Zhang", "Raghav Subbaraman", "Deepak Vashist", "Dinesh Bharadia"], "title": "FSMA: Scalable and Reliable LoRa for Non-Terrestrial Networks with Mobile Gateways", "comment": "14 pages, 19 figures", "summary": "The proliferation of Low Earth Orbit (LEO) satellites for universal IoT\napplications and the growing use of drones in emergency services, agriculture,\nand military operations highlight the transformative potential of\nnon-terrestrial networks (NTN). However, these networks face two key\nchallenges: (1) large coverage footprints that create frequent collisions and\n(2) moving gateways that cause dynamic links and demand synchronization-free,\nlink-aware transmissions. Existing random access schemes such as ALOHA, CSMA,\nand BSMA fail in this setting, suffering from high collision rates, hidden\nterminals, or excessive gateway energy overhead. We propose Free Signal\nMultiple Access (FSMA), a gateway-controlled protocol that introduces a\nlightweight free signal chirp (FreeChirp). FreeChirp ensures that nodes\ntransmit only when the channel is idle and when links are reliable, thereby\nreducing collisions and enabling link-aware access without the need for\nsynchronization or complex scheduling. We evaluate FSMA using 25 commercial\nLoRa devices with a drone-mounted moving gateway and demonstrate up to 2x\nhigher throughput, 2x to 5x better packet reception ratio, and 5x improved\nenergy efficiency compared to the baselines. Large-scale simulations with a\ncustom Satellite IoT Simulator further show that FSMA scales to 5000+ devices\nper satellite pass. These results establish FSMA as a practical step toward\nscalable, energy-efficient, and reliable NTN IoT networks.", "AI": {"tldr": "FSMA\u534f\u8bae\u901a\u8fc7FreeChirp\u4fe1\u53f7\u5b9e\u73b0\u514d\u540c\u6b65\u7684\u94fe\u8def\u611f\u77e5\u63a5\u5165\uff0c\u5728\u79fb\u52a8\u7f51\u5173\u7684\u975e\u5730\u9762\u7f51\u7edc\u4e2d\u663e\u8457\u63d0\u5347\u541e\u5410\u91cf\u3001\u5305\u63a5\u6536\u7387\u548c\u80fd\u6548", "motivation": "LEO\u536b\u661f\u548c\u65e0\u4eba\u673a\u7b49\u975e\u5730\u9762\u7f51\u7edc\u9762\u4e34\u5927\u8986\u76d6\u8303\u56f4\u5bfc\u81f4\u7684\u9891\u7e41\u78b0\u649e\u548c\u79fb\u52a8\u7f51\u5173\u5e26\u6765\u7684\u52a8\u6001\u94fe\u8def\u6311\u6218\uff0c\u73b0\u6709\u968f\u673a\u63a5\u5165\u534f\u8bae\u65e0\u6cd5\u6709\u6548\u5e94\u5bf9", "method": "\u63d0\u51faFree Signal Multiple Access (FSMA)\u534f\u8bae\uff0c\u4f7f\u7528\u8f7b\u91cf\u7ea7FreeChirp\u4fe1\u53f7\u786e\u4fdd\u8282\u70b9\u4ec5\u5728\u4fe1\u9053\u7a7a\u95f2\u4e14\u94fe\u8def\u53ef\u9760\u65f6\u4f20\u8f93", "result": "\u5b9e\u9a8c\u663e\u793a\u76f8\u6bd4\u57fa\u7ebf\u65b9\u6cd5\uff0c\u541e\u5410\u91cf\u63d0\u53472\u500d\uff0c\u5305\u63a5\u6536\u7387\u63d0\u53472-5\u500d\uff0c\u80fd\u6548\u63d0\u53475\u500d\uff1b\u5927\u89c4\u6a21\u4eff\u771f\u652f\u63015000+\u8bbe\u5907/\u536b\u661f\u8fc7\u5883", "conclusion": "FSMA\u4e3a\u5b9e\u73b0\u53ef\u6269\u5c55\u3001\u9ad8\u80fd\u6548\u3001\u53ef\u9760\u7684\u975e\u5730\u9762\u7f51\u7edc\u7269\u8054\u7f51\u63d0\u4f9b\u4e86\u5b9e\u7528\u89e3\u51b3\u65b9\u6848"}}
{"id": "2510.02838", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2510.02838", "abs": "https://arxiv.org/abs/2510.02838", "authors": ["Yifei Xia", "Fangcheng Fu", "Hao Yuan", "Hanke Zhang", "Xupeng Miao", "Yijun Liu", "Suhan Ling", "Jie Jiang", "Bin Cui"], "title": "TridentServe: A Stage-level Serving System for Diffusion Pipelines", "comment": null, "summary": "Diffusion pipelines, renowned for their powerful visual generation\ncapabilities, have seen widespread adoption in generative vision tasks (e.g.,\ntext-to-image/video). These pipelines typically follow an\nencode--diffuse--decode three-stage architecture. Current serving systems\ndeploy diffusion pipelines within a static, manual, and pipeline-level\nparadigm, allocating the same resources to every request and stage. However,\nthrough an in-depth analysis, we find that such a paradigm is inefficient due\nto the discrepancy in resource needs across the three stages of each request,\nas well as across different requests. Following the analysis, we propose the\ndynamic stage-level serving paradigm and develop TridentServe, a brand new\ndiffusion serving system. TridentServe automatically, dynamically derives the\nplacement plan (i.e., how each stage resides) for pipeline deployment and the\ndispatch plan (i.e., how the requests are routed) for request processing,\nco-optimizing the resource allocation for both model and requests. Extensive\nexperiments show that TridentServe consistently improves SLO attainment and\nreduces average/P95 latencies by up to 2.5x and 3.6x/4.1x over existing works\nacross a variety of workloads.", "AI": {"tldr": "TridentServe\u662f\u4e00\u4e2a\u52a8\u6001\u9636\u6bb5\u7ea7\u6269\u6563\u6a21\u578b\u670d\u52a1\u7cfb\u7edf\uff0c\u901a\u8fc7\u81ea\u52a8\u4f18\u5316\u6a21\u578b\u90e8\u7f72\u548c\u8bf7\u6c42\u8def\u7531\uff0c\u663e\u8457\u63d0\u5347\u670d\u52a1\u6548\u7387\u3002", "motivation": "\u5f53\u524d\u6269\u6563\u6a21\u578b\u670d\u52a1\u7cfb\u7edf\u91c7\u7528\u9759\u6001\u3001\u624b\u52a8\u3001\u7ba1\u9053\u7ea7\u7684\u8d44\u6e90\u5206\u914d\u65b9\u5f0f\uff0c\u65e0\u6cd5\u9002\u5e94\u4e0d\u540c\u9636\u6bb5\u548c\u8bf7\u6c42\u7684\u8d44\u6e90\u9700\u6c42\u5dee\u5f02\uff0c\u5bfc\u81f4\u6548\u7387\u4f4e\u4e0b\u3002", "method": "\u63d0\u51fa\u52a8\u6001\u9636\u6bb5\u7ea7\u670d\u52a1\u8303\u5f0f\uff0c\u81ea\u52a8\u63a8\u5bfc\u7ba1\u9053\u90e8\u7f72\u7684\u653e\u7f6e\u8ba1\u5212\u548c\u8bf7\u6c42\u5904\u7406\u7684\u8def\u7531\u8ba1\u5212\uff0c\u534f\u540c\u4f18\u5316\u6a21\u578b\u548c\u8bf7\u6c42\u7684\u8d44\u6e90\u5206\u914d\u3002", "result": "\u5b9e\u9a8c\u8868\u660eTridentServe\u5728\u5404\u79cd\u5de5\u4f5c\u8d1f\u8f7d\u4e0b\uff0c\u76f8\u6bd4\u73b0\u6709\u5de5\u4f5c\u5c06SLO\u8fbe\u6210\u7387\u63d0\u53472.5\u500d\uff0c\u5e73\u5747/P95\u5ef6\u8fdf\u964d\u4f4e3.6\u500d/4.1\u500d\u3002", "conclusion": "\u52a8\u6001\u9636\u6bb5\u7ea7\u670d\u52a1\u8303\u5f0f\u80fd\u6709\u6548\u89e3\u51b3\u6269\u6563\u7ba1\u9053\u670d\u52a1\u4e2d\u7684\u8d44\u6e90\u5206\u914d\u6548\u7387\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u7cfb\u7edf\u6027\u80fd\u3002"}}
{"id": "2510.02393", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.02393", "abs": "https://arxiv.org/abs/2510.02393", "authors": ["Jianqing Zhang", "Wei Xia", "Hande Dong", "Qiang Lin", "Jian Cao"], "title": "AP2O: Correcting LLM-Generated Code Errors Type by Type Like Humans via Adaptive Progressive Preference Optimization", "comment": null, "summary": "LLMs' code generation capabilities have yielded substantial improvements in\nthe effectiveness of programming tasks. However, LLM-generated code still\nsuffers from compilation and runtime errors. Existing offline preference\noptimization methods primarily focus on enhancing LLMs' coding abilities using\npass/fail signals in the preference data, overlooking the deep-level error\ntypes in the failed codes. To address this, we propose Adaptively Progressive\nPreference Optimization (AP2O) for coding (i.e., AP2O-Coder), a method that\nguides LLMs adaptively and methodically to reduce code errors for code\ngeneration. Specifically, we construct an error notebook from failed codes and\nprogressively optimize the LLM to correct errors type by type. Furthermore, we\nadaptively replay error types to tailor to the LLM's changing weaknesses\nthroughout the training process. Through extensive experiments on both code and\ngeneral LLMs (Llama, Qwen, and DeepSeek series) with parameters ranging from\n0.5B to 34B, our AP2O-Coder improves code generation performance by up to 3% in\npass@k while using less preference data. Code: https://github.com/TsingZ0/AP2O", "AI": {"tldr": "\u63d0\u51faAP2O-Coder\u65b9\u6cd5\uff0c\u901a\u8fc7\u6e10\u8fdb\u5f0f\u9519\u8bef\u7c7b\u578b\u4f18\u5316\u63d0\u5347LLM\u4ee3\u7801\u751f\u6210\u80fd\u529b\uff0c\u51cf\u5c11\u7f16\u8bd1\u548c\u8fd0\u884c\u65f6\u9519\u8bef", "motivation": "\u73b0\u6709\u79bb\u7ebf\u504f\u597d\u4f18\u5316\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8\u901a\u8fc7\u901a\u8fc7/\u5931\u8d25\u4fe1\u53f7\u63d0\u5347LLM\u7f16\u7a0b\u80fd\u529b\uff0c\u5ffd\u89c6\u4e86\u5931\u8d25\u4ee3\u7801\u4e2d\u7684\u6df1\u5c42\u9519\u8bef\u7c7b\u578b", "method": "\u6784\u5efa\u9519\u8bef\u7b14\u8bb0\u672c\u6765\u81ea\u5931\u8d25\u4ee3\u7801\uff0c\u9010\u6b65\u4f18\u5316LLM\u6309\u9519\u8bef\u7c7b\u578b\u8fdb\u884c\u7ea0\u6b63\uff0c\u5e76\u81ea\u9002\u5e94\u91cd\u653e\u9519\u8bef\u7c7b\u578b\u4ee5\u9002\u5e94\u8bad\u7ec3\u8fc7\u7a0b\u4e2dLLM\u7684\u5f31\u70b9\u53d8\u5316", "result": "\u57280.5B\u523034B\u53c2\u6570\u7684\u4ee3\u7801\u548c\u901a\u7528LLM\u4e0a\u5b9e\u9a8c\u663e\u793a\uff0cAP2O-Coder\u5c06\u4ee3\u7801\u751f\u6210\u6027\u80fd\u63d0\u5347\u9ad8\u8fbe3%\u7684pass@k\uff0c\u540c\u65f6\u4f7f\u7528\u66f4\u5c11\u7684\u504f\u597d\u6570\u636e", "conclusion": "AP2O-Coder\u901a\u8fc7\u81ea\u9002\u5e94\u6e10\u8fdb\u5f0f\u9519\u8bef\u7c7b\u578b\u4f18\u5316\u6709\u6548\u63d0\u5347\u4e86LLM\u7684\u4ee3\u7801\u751f\u6210\u8d28\u91cf"}}
{"id": "2510.02895", "categories": ["cs.NI"], "pdf": "https://arxiv.org/pdf/2510.02895", "abs": "https://arxiv.org/abs/2510.02895", "authors": ["Akihisa Takahashi", "Yoshito Tobe"], "title": "DH-EAC: Design of a Dynamic, Hierarchical Entanglement Access Control Protocol", "comment": null, "summary": "We propose Dynamic, Hierarchical Entanglement Access Control (DH-EAC), a\npure-quantum protocol for fair and anonymous allocation of scarce entanglement\nacross wide-area quantum networks composed of many quantum LANs (QLANs). Prior\nDicke-state-based pure-quantum MACs resolve contention by local measurements\nwithout classical signaling, but they mainly target a single QLAN under static\nconditions; extending them to wide-area, dynamic settings while avoiding\npost-selection reconciliation remains open. DH-EAC adopts a two-layer\npure-quantum lottery: the outer layer selects winning QLANs and the inner layer\nselects winning nodes within each winning QLAN. A key design principle is that\nboth the winning set and the per-QLAN quota are fixed by measurements alone, so\nthe contention loop requires no classical round trip. The protocol thus aims to\njointly satisfy anonymity (no node IDs revealed until decisions are fixed) and\nfairness (bias suppression under heterogeneous QLAN sizes). We also provide\nanalytical models for success probability and latency under a standard i.i.d.\nloss model, and we evaluate DH-EAC against two baselines - single-layer Dicke\nwithin one QLAN and a classical GO-driven allocator - using a minimal,\nreproducible set of scenarios. Metrics include success probability, end-to-end\nlatency, throughput, and Jain's fairness index. The results indicate that\nDH-EAC offers an implementable design point in the space of entanglement access\ncontrol, balancing pure-quantum contention resolution, anonymity, and\nscalability for multi-QLAN networks.", "AI": {"tldr": "DH-EAC\u662f\u4e00\u79cd\u7eaf\u91cf\u5b50\u534f\u8bae\uff0c\u7528\u4e8e\u5728\u7531\u591a\u4e2a\u91cf\u5b50\u5c40\u57df\u7f51\u7ec4\u6210\u7684\u5e7f\u57df\u91cf\u5b50\u7f51\u7edc\u4e2d\u516c\u5e73\u533f\u540d\u5730\u5206\u914d\u7a00\u7f3a\u7ea0\u7f20\u8d44\u6e90\uff0c\u91c7\u7528\u53cc\u5c42\u91cf\u5b50\u62bd\u7b7e\u673a\u5236\u5b9e\u73b0\u65e0\u7ecf\u5178\u4fe1\u53f7\u4e89\u7528\u89e3\u51b3\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8eDicke\u6001\u7684\u7eaf\u91cf\u5b50MAC\u534f\u8bae\u4e3b\u8981\u9488\u5bf9\u9759\u6001\u5355\u91cf\u5b50\u5c40\u57df\u7f51\u573a\u666f\uff0c\u6269\u5c55\u5230\u5e7f\u57df\u52a8\u6001\u73af\u5883\u4e14\u907f\u514d\u540e\u9009\u62e9\u534f\u8c03\u4ecd\u662f\u4e00\u4e2a\u5f00\u653e\u95ee\u9898\u3002\u9700\u8981\u8bbe\u8ba1\u80fd\u591f\u540c\u65f6\u6ee1\u8db3\u533f\u540d\u6027\u548c\u516c\u5e73\u6027\u7684\u591a\u91cf\u5b50\u7f51\u7edc\u7ea0\u7f20\u8bbf\u95ee\u63a7\u5236\u534f\u8bae\u3002", "method": "\u91c7\u7528\u53cc\u5c42\u7eaf\u91cf\u5b50\u62bd\u7b7e\u673a\u5236\uff1a\u5916\u5c42\u9009\u62e9\u83b7\u80dc\u91cf\u5b50\u5c40\u57df\u7f51\uff0c\u5185\u5c42\u9009\u62e9\u6bcf\u4e2a\u83b7\u80dc\u5c40\u57df\u7f51\u5185\u7684\u83b7\u80dc\u8282\u70b9\u3002\u5173\u952e\u8bbe\u8ba1\u539f\u5219\u662f\u901a\u8fc7\u6d4b\u91cf\u5355\u72ec\u786e\u5b9a\u83b7\u80dc\u96c6\u5408\u548c\u6bcf\u4e2a\u5c40\u57df\u7f51\u7684\u914d\u989d\uff0c\u65e0\u9700\u7ecf\u5178\u5f80\u8fd4\u901a\u4fe1\u3002", "result": "\u5728\u6807\u51c6i.i.d.\u635f\u8017\u6a21\u578b\u4e0b\u5206\u6790\u4e86\u6210\u529f\u6982\u7387\u548c\u5ef6\u8fdf\u6027\u80fd\uff0c\u4e0e\u5355\u5c42Dicke\u534f\u8bae\u548c\u7ecf\u5178GO\u9a71\u52a8\u5206\u914d\u5668\u5bf9\u6bd4\u8bc4\u4f30\u3002\u7ed3\u679c\u663e\u793aDH-EAC\u5728\u6210\u529f\u6982\u7387\u3001\u7aef\u5230\u7aef\u5ef6\u8fdf\u3001\u541e\u5410\u91cf\u548cJain\u516c\u5e73\u6027\u6307\u6570\u7b49\u6307\u6807\u4e0a\u8868\u73b0\u826f\u597d\u3002", "conclusion": "DH-EAC\u4e3a\u7ea0\u7f20\u8bbf\u95ee\u63a7\u5236\u63d0\u4f9b\u4e86\u4e00\u4e2a\u53ef\u5b9e\u73b0\u7684\u5e73\u8861\u70b9\uff0c\u5728\u7eaf\u91cf\u5b50\u4e89\u7528\u89e3\u51b3\u3001\u533f\u540d\u6027\u548c\u591a\u91cf\u5b50\u7f51\u7edc\u53ef\u6269\u5c55\u6027\u4e4b\u95f4\u53d6\u5f97\u4e86\u826f\u597d\u5e73\u8861\u3002"}}
{"id": "2510.02404", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.02404", "abs": "https://arxiv.org/abs/2510.02404", "authors": ["Siddharth Agarwal", "Maria A. Rodriguez", "Rajkumar Buyya"], "title": "Dynamic Function Configuration and its Management in Serverless Computing: A Taxonomy and Future Directions", "comment": "34 pages, 2 figures, 2 tables, journal", "summary": "The serverless cloud computing model offers a framework where the service\nprovider abstracts the underlying infrastructure management from developers. In\nthis serverless model, FaaS provides an event-driven, function-oriented\ncomputing service characterised by fine-grained, usage-based pricing that\neliminates cost for idle resources. Platforms like AWS Lambda, Azure Functions,\nand Cloud Run Functions require developers to configure their function(s) with\nminimum operational resources for its successful execution. This resource\nallocation influences both the operational expense and the performance quality\nof these functions. However, a noticeable lack of platform transparency forces\ndevelopers to rely on expert knowledge or experience-based ad-hoc decisions to\nrequest desired function resources. This makes optimal resource configuration a\nnon-trivial task while adhering to performance constraints. Furthermore, while\ncommercial platforms often scale resources like CPU and network bandwidth\nproportional to memory, open-source frameworks permit independent configuration\nof function resources, introducing additional complexity for developers aiming\nto optimise their functions. These complexities have directed researchers to\nresolve developer challenges and advance towards an efficient server-less\nexecution model. In this article, we identify different aspects of resource\nconfiguration techniques in FaaS settings and propose a taxonomy of factors\nthat influence function design, configuration, run-time cost, and performance\nguarantees. We conduct an analysis of existing literature on resource\nconfiguration to present a comprehensive review of current studies on function\nconfiguration. We also identify existing research gaps and suggest future\nresearch directions to enhance function configuration and strengthen the\ncapabilities of serverless computing environments to drive its broader\nadoption.", "AI": {"tldr": "\u672c\u6587\u5bf9FaaS\u73af\u5883\u4e2d\u7684\u8d44\u6e90\u914d\u7f6e\u6280\u672f\u8fdb\u884c\u4e86\u7cfb\u7edf\u5206\u6790\uff0c\u63d0\u51fa\u4e86\u5f71\u54cd\u51fd\u6570\u8bbe\u8ba1\u3001\u914d\u7f6e\u3001\u8fd0\u884c\u6210\u672c\u548c\u6027\u80fd\u4fdd\u8bc1\u7684\u56e0\u7d20\u5206\u7c7b\u6cd5\uff0c\u5e76\u6307\u51fa\u4e86\u73b0\u6709\u7814\u7a76\u7a7a\u767d\u548c\u672a\u6765\u65b9\u5411\u3002", "motivation": "serverless\u8ba1\u7b97\u6a21\u578b\u4e2d\uff0c\u5f00\u53d1\u8005\u7f3a\u4e4f\u5e73\u53f0\u900f\u660e\u5ea6\uff0c\u53ea\u80fd\u4f9d\u8d56\u7ecf\u9a8c\u8fdb\u884c\u8d44\u6e90\u914d\u7f6e\u51b3\u7b56\uff0c\u8fd9\u5bfc\u81f4\u5728\u6ee1\u8db3\u6027\u80fd\u7ea6\u675f\u7684\u540c\u65f6\u5b9e\u73b0\u6700\u4f18\u8d44\u6e90\u914d\u7f6e\u6210\u4e3a\u975e\u5e73\u51e1\u4efb\u52a1\u3002", "method": "\u901a\u8fc7\u5206\u6790\u73b0\u6709\u6587\u732e\uff0c\u63d0\u51fa\u8d44\u6e90\u914d\u7f6e\u5f71\u54cd\u56e0\u7d20\u5206\u7c7b\u6cd5\uff0c\u5e76\u8fdb\u884c\u5168\u9762\u7684\u51fd\u6570\u914d\u7f6e\u7814\u7a76\u7efc\u8ff0\u3002", "result": "\u5efa\u7acb\u4e86\u7cfb\u7edf\u7684\u8d44\u6e90\u914d\u7f6e\u6280\u672f\u5206\u6790\u6846\u67b6\uff0c\u8bc6\u522b\u4e86\u5f71\u54cd\u51fd\u6570\u6027\u80fd\u548c\u8d28\u91cf\u7684\u5173\u952e\u56e0\u7d20\u3002", "conclusion": "\u9700\u8981\u8fdb\u4e00\u6b65\u7814\u7a76\u6765\u589e\u5f3a\u51fd\u6570\u914d\u7f6e\u80fd\u529b\uff0c\u63a8\u52a8serverless\u8ba1\u7b97\u73af\u5883\u7684\u66f4\u5e7f\u6cdb\u91c7\u7528\u3002"}}
{"id": "2510.02958", "categories": ["cs.NI"], "pdf": "https://arxiv.org/pdf/2510.02958", "abs": "https://arxiv.org/abs/2510.02958", "authors": ["Muhammad Kabeer", "Rosdiadee Nordin", "Mehran Behjati", "Lau Sian Lun"], "title": "Sequence-Based Deep Learning for Handover Optimization in Dense Urban Cellular Network", "comment": "6 pages, 6 figures, conference", "summary": "Efficient handover management remains a critical challenge in dense urban\ncellular networks, where high cell density, user mobility, and diverse service\ndemands increase the likelihood of unnecessary handovers and ping-pong effects.\nThis paper leverages a real-world, multi-operator drive-test dataset of 30,925\nlabelled records collected within a 2 km area around Sunway City to investigate\nsequence-based deep learning approaches for handover detection and avoidance.\nWe formulate handover prediction as a sequence problem and evaluate Gated\nRecurrent Unit (GRU), Long Short-Term Memory (LSTM), and Transformer\narchitectures under Reference Signal Received Power (RSRP)-only and all-feature\nsettings. The integration of multi-dimensional features significantly enhanced\nhandover performance in dense urban cellular networks. The proposed GRU-based\nmodel achieved a remarkable 98% reduction in ping-pong handovers, alongside a\n46.25% decrease in unnecessary handovers, outperforming the baseline RSRP-only\napproach which yielded a 22.19% reduction. Furthermore, the model demonstrated\na 46% improvement in Time of Stay (ToS), indicating more stable user\nconnections. With an inference time of just 0.91 seconds, the solution proves\nhighly efficient and well-suited for real-time edge deployment scenarios.\nCompared to the conventional 3GPP A3 algorithm, these improvements demonstrate\nsignificant gains in mobility robustness and user Quality of Experience (QoE)\nimprovement. The dataset is released to foster reproducibility and further\nresearch in intelligent mobility management for 5G and beyond.", "AI": {"tldr": "\u57fa\u4e8e\u771f\u5b9e\u4e16\u754c\u591a\u8fd0\u8425\u5546\u6570\u636e\u96c6\uff0c\u4f7f\u7528GRU\u3001LSTM\u548cTransformer\u7b49\u5e8f\u5217\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u8fdb\u884c\u5207\u6362\u9884\u6d4b\uff0cGRU\u6a21\u578b\u5728\u5bc6\u96c6\u57ce\u5e02\u8702\u7a9d\u7f51\u7edc\u4e2d\u663e\u8457\u51cf\u5c11\u4e86\u4e52\u4e53\u5207\u6362\u548c\u4e0d\u5fc5\u8981\u5207\u6362\uff0c\u63d0\u5347\u4e86\u8fde\u63a5\u7a33\u5b9a\u6027\u3002", "motivation": "\u5bc6\u96c6\u57ce\u5e02\u8702\u7a9d\u7f51\u7edc\u4e2d\u9ad8\u5c0f\u533a\u5bc6\u5ea6\u3001\u7528\u6237\u79fb\u52a8\u6027\u548c\u591a\u6837\u5316\u670d\u52a1\u9700\u6c42\u589e\u52a0\u4e86\u4e0d\u5fc5\u8981\u5207\u6362\u548c\u4e52\u4e53\u6548\u5e94\u7684\u53ef\u80fd\u6027\uff0c\u9700\u8981\u9ad8\u6548\u7684\u5207\u6362\u7ba1\u7406\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u5229\u752830,925\u6761\u6807\u8bb0\u8bb0\u5f55\u7684\u771f\u5b9e\u6570\u636e\u96c6\uff0c\u5c06\u5207\u6362\u9884\u6d4b\u5efa\u6a21\u4e3a\u5e8f\u5217\u95ee\u9898\uff0c\u8bc4\u4f30GRU\u3001LSTM\u548cTransformer\u67b6\u6784\u5728RSRP-only\u548c\u5168\u7279\u5f81\u8bbe\u7f6e\u4e0b\u7684\u6027\u80fd\u3002", "result": "GRU\u6a21\u578b\u5b9e\u73b0\u4e8698%\u7684\u4e52\u4e53\u5207\u6362\u51cf\u5c11\u548c46.25%\u7684\u4e0d\u5fc5\u8981\u5207\u6362\u51cf\u5c11\uff0c\u76f8\u6bd4\u57fa\u7ebfRSRP-only\u65b9\u6cd5\u768422.19%\u6709\u663e\u8457\u63d0\u5347\uff0c\u540c\u65f6\u65f6\u95f4\u505c\u7559\u6539\u558446%\uff0c\u63a8\u7406\u65f6\u95f4\u4ec50.91\u79d2\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u57fa\u4e8eGRU\u7684\u6a21\u578b\u5728\u79fb\u52a8\u9c81\u68d2\u6027\u548c\u7528\u6237\u4f53\u9a8c\u8d28\u91cf\u65b9\u9762\u663e\u793a\u51fa\u663e\u8457\u6539\u8fdb\uff0c\u9002\u5408\u5b9e\u65f6\u8fb9\u7f18\u90e8\u7f72\uff0c\u6570\u636e\u96c6\u5df2\u53d1\u5e03\u4ee5\u4fc3\u8fdb5G\u53ca\u4ee5\u540e\u667a\u80fd\u79fb\u52a8\u6027\u7ba1\u7406\u7684\u8fdb\u4e00\u6b65\u7814\u7a76\u3002"}}
{"id": "2510.02882", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2510.02882", "abs": "https://arxiv.org/abs/2510.02882", "authors": ["Adhitya Bhawiyuga", "Serkan Girgin", "Rolf A. de By", "Raul Zurita-Milla"], "title": "Energy Efficiency in Cloud-Based Big Data Processing for Earth Observation: Gap Analysis and Future Directions", "comment": null, "summary": "Earth observation (EO) data volumes are rapidly increasing. While cloud\ncomputing are now used for processing large EO datasets, the energy efficiency\naspects of such a processing have received much less attention. This issue is\nnotable given the increasing awareness of energy costs and carbon footprint in\nbig data processing, particularly with increased attention on compute-intensive\nfoundation models. In this paper we identify gaps in energy efficiency\npractices within cloud-based EO big data (EOBD) processing and propose several\nresearch directions for improvement. We first examine the current EOBD\nlandscape, focus on the requirements that necessitate cloud-based processing\nand analyze existing cloud-based EOBD solutions. We then investigate energy\nefficiency strategies that have been successfully employed in well-studied big\ndata domains. Through this analysis, we identify several critical gaps in\nexisting EOBD processing platforms, which primarily focus on data accessibility\nand computational feasibility, instead of energy efficiency. These gaps include\ninsufficient energy monitoring mechanisms, lack of energy awareness in data\nmanagement, inadequate implementation of energy-aware resource allocation and\nlack of energy efficiency criteria on task scheduling. Based on these findings,\nwe propose the development of energy-aware performance monitoring and\nbenchmarking frameworks, the use of optimization techniques for infrastructure\norchestration, and of energy-efficient task scheduling approaches for\ndistributed cloud-based EOBD processing frameworks. These proposed approaches\naim to foster more energy awareness in EOBD processing , potentially reducing\npower consumption and environmental impact while maintaining or minimally\nimpacting processing performance.", "AI": {"tldr": "\u8bba\u6587\u5206\u6790\u4e86\u4e91\u5904\u7406\u5730\u7403\u89c2\u6d4b\u5927\u6570\u636e(EOBD)\u4e2d\u7684\u80fd\u6e90\u6548\u7387\u95ee\u9898\uff0c\u6307\u51fa\u4e86\u73b0\u6709\u5e73\u53f0\u5728\u80fd\u6e90\u76d1\u63a7\u3001\u6570\u636e\u7ba1\u7406\u3001\u8d44\u6e90\u5206\u914d\u548c\u4efb\u52a1\u8c03\u5ea6\u65b9\u9762\u7684\u4e0d\u8db3\uff0c\u5e76\u63d0\u51fa\u4e86\u80fd\u6e90\u611f\u77e5\u7684\u6027\u80fd\u76d1\u63a7\u6846\u67b6\u3001\u57fa\u7840\u8bbe\u65bd\u4f18\u5316\u548c\u8282\u80fd\u4efb\u52a1\u8c03\u5ea6\u7b49\u7814\u7a76\u65b9\u5411\u3002", "motivation": "\u968f\u7740\u5730\u7403\u89c2\u6d4b\u6570\u636e\u91cf\u5feb\u901f\u589e\u957f\u548c\u4e91\u8ba1\u7b97\u5e7f\u6cdb\u5e94\u7528\uff0c\u80fd\u6e90\u6548\u7387\u548c\u78b3\u8db3\u8ff9\u95ee\u9898\u5728\u5927\u6570\u636e\u5904\u7406\u4e2d\u65e5\u76ca\u91cd\u8981\uff0c\u4f46\u4e91\u5904\u7406EOBD\u7684\u80fd\u6e90\u6548\u7387\u65b9\u9762\u7814\u7a76\u4e0d\u8db3\uff0c\u9700\u8981\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u901a\u8fc7\u5206\u6790\u5f53\u524dEOBD\u5904\u7406\u73b0\u72b6\u3001\u4e91\u5904\u7406\u9700\u6c42\u4ee5\u53ca\u73b0\u6709\u89e3\u51b3\u65b9\u6848\uff0c\u501f\u9274\u5176\u4ed6\u5927\u6570\u636e\u9886\u57df\u7684\u6210\u529f\u80fd\u6e90\u6548\u7387\u7b56\u7565\uff0c\u8bc6\u522bEOBD\u5904\u7406\u5e73\u53f0\u7684\u5173\u952e\u80fd\u6e90\u6548\u7387\u5dee\u8ddd\u3002", "result": "\u8bc6\u522b\u51fa\u56db\u4e2a\u5173\u952e\u80fd\u6e90\u6548\u7387\u5dee\u8ddd\uff1a\u80fd\u6e90\u76d1\u63a7\u673a\u5236\u4e0d\u8db3\u3001\u6570\u636e\u7ba1\u7406\u7f3a\u4e4f\u80fd\u6e90\u610f\u8bc6\u3001\u80fd\u6e90\u611f\u77e5\u8d44\u6e90\u5206\u914d\u5b9e\u65bd\u4e0d\u8db3\u3001\u4efb\u52a1\u8c03\u5ea6\u7f3a\u4e4f\u80fd\u6e90\u6548\u7387\u6807\u51c6\u3002", "conclusion": "\u63d0\u51fa\u4e86\u5f00\u53d1\u80fd\u6e90\u611f\u77e5\u6027\u80fd\u76d1\u63a7\u6846\u67b6\u3001\u57fa\u7840\u8bbe\u65bd\u4f18\u5316\u6280\u672f\u548c\u8282\u80fd\u4efb\u52a1\u8c03\u5ea6\u65b9\u6cd5\u7684\u7814\u7a76\u65b9\u5411\uff0c\u65e8\u5728\u63d0\u9ad8EOBD\u5904\u7406\u7684\u80fd\u6e90\u610f\u8bc6\uff0c\u51cf\u5c11\u80fd\u8017\u548c\u73af\u5883\u5f71\u54cd\uff0c\u540c\u65f6\u4fdd\u6301\u5904\u7406\u6027\u80fd\u3002"}}
{"id": "2510.02504", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.02504", "abs": "https://arxiv.org/abs/2510.02504", "authors": ["Mara Ulloa", "Jenna L. Butler", "Sankeerti Haniyur", "Courtney Miller", "Barrett Amos", "Advait Sarkar", "Margaret-Anne Storey"], "title": "Product Manager Practices for Delegating Work to Generative AI: \"Accountability must not be delegated to non-human actors\"", "comment": "12 pages, 4 figures, 1 table", "summary": "Generative AI (GenAI) is changing the nature of knowledge work, particularly\nfor Product Managers (PMs) in software development teams. While much software\nengineering research has focused on developers' interactions with GenAI, there\nis less understanding of how the work of PMs is evolving due to GenAI. To\naddress this gap, we conducted a mixed-methods study at Microsoft, a large,\nmultinational software company: surveying 885 PMs, analyzing telemetry data for\na subset of PMs (N=731), and interviewing a subset of 15 PMs. We contribute:\n(1) PMs' current GenAI adoption rates, uses cases, and perceived benefits and\nbarriers and; (2) a framework capturing how PMs assess which tasks to delegate\nto GenAI; (3) PMs adaptation practices for integrating GenAI into their roles\nand perceptions of how their role is evolving. We end by discussing\nimplications on the broader GenAI workflow adoption process and software\ndevelopment roles.", "AI": {"tldr": "\u7814\u7a76\u5fae\u8f6f\u4ea7\u54c1\u7ecf\u7406\u91c7\u7528\u751f\u6210\u5f0fAI\u7684\u73b0\u72b6\u3001\u4f7f\u7528\u6848\u4f8b\u3001\u6536\u76ca\u969c\u788d\uff0c\u4ee5\u53ca\u4efb\u52a1\u59d4\u6258\u6846\u67b6\u548c\u89d2\u8272\u9002\u5e94\u5b9e\u8df5\u3002", "motivation": "\u751f\u6210\u5f0fAI\u6b63\u5728\u6539\u53d8\u77e5\u8bc6\u5de5\u4f5c\u6027\u8d28\uff0c\u4f46\u73b0\u6709\u7814\u7a76\u4e3b\u8981\u5173\u6ce8\u5f00\u53d1\u8005\uff0c\u5bf9\u4ea7\u54c1\u7ecf\u7406\u5982\u4f55\u9002\u5e94GenAI\u7684\u4e86\u89e3\u4e0d\u8db3\u3002", "method": "\u6df7\u5408\u65b9\u6cd5\u7814\u7a76\uff1a\u8c03\u67e5885\u540d\u4ea7\u54c1\u7ecf\u7406\uff0c\u5206\u6790731\u4eba\u7684\u9065\u6d4b\u6570\u636e\uff0c\u8bbf\u8c0815\u540d\u4ea7\u54c1\u7ecf\u7406\u3002", "result": "\u63d0\u51fa\u4e86\u4ea7\u54c1\u7ecf\u7406GenAI\u91c7\u7528\u7387\u3001\u4f7f\u7528\u6848\u4f8b\u3001\u6536\u76ca\u969c\u788d\u7684\u73b0\u72b6\u5206\u6790\uff0c\u4ee5\u53ca\u4efb\u52a1\u59d4\u6258\u8bc4\u4f30\u6846\u67b6\u548c\u89d2\u8272\u6574\u5408\u5b9e\u8df5\u3002", "conclusion": "\u7814\u7a76\u63ed\u793a\u4e86\u4ea7\u54c1\u7ecf\u7406\u89d2\u8272\u5728GenAI\u5f71\u54cd\u4e0b\u7684\u6f14\u53d8\uff0c\u5bf9\u66f4\u5e7f\u6cdb\u7684GenAI\u5de5\u4f5c\u6d41\u91c7\u7528\u548c\u8f6f\u4ef6\u5f00\u53d1\u89d2\u8272\u5177\u6709\u91cd\u8981\u542f\u793a\u3002"}}
{"id": "2510.03205", "categories": ["cs.NI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.03205", "abs": "https://arxiv.org/abs/2510.03205", "authors": ["Shenjia Ding", "David Flynn", "Paul Harvey"], "title": "Automatic Generation of Digital Twins for Network Testing", "comment": "Accepted to ANMS at ICDCS 2025", "summary": "The increased use of software in the operation and management of\ntelecommunication networks has moved the industry one step closer to realizing\nautonomous network operation. One consequence of this shift is the\nsignificantly increased need for testing and validation before such software\ncan be deployed. Complementing existing simulation or hardware-based\napproaches, digital twins present an environment to achieve this testing;\nhowever, they require significant time and human effort to configure and\nexecute. This paper explores the automatic generation of digital twins to\nprovide efficient and accurate validation tools, aligned to the ITU-T\nautonomous network architecture's experimentation subsystem. We present\nexperimental results for an initial use case, demonstrating that the approach\nis feasible in automatically creating efficient digital twins with sufficient\naccuracy to be included as part of existing validation pipelines.", "AI": {"tldr": "\u81ea\u52a8\u751f\u6210\u6570\u5b57\u5b6a\u751f\u7528\u4e8e\u7535\u4fe1\u7f51\u7edc\u8f6f\u4ef6\u6d4b\u8bd5\u9a8c\u8bc1\uff0c\u51cf\u5c11\u4eba\u5de5\u914d\u7f6e\u65f6\u95f4\u548c\u6210\u672c", "motivation": "\u7535\u4fe1\u7f51\u7edc\u8f6f\u4ef6\u5316\u8d8b\u52bf\u589e\u52a0\u4e86\u6d4b\u8bd5\u9a8c\u8bc1\u9700\u6c42\uff0c\u4f20\u7edf\u4eff\u771f\u548c\u786c\u4ef6\u65b9\u6cd5\u8017\u65f6\u8017\u529b\uff0c\u6570\u5b57\u5b6a\u751f\u9700\u8981\u81ea\u52a8\u5316\u914d\u7f6e", "method": "\u57fa\u4e8eITU-T\u81ea\u6cbb\u7f51\u7edc\u67b6\u6784\u5b9e\u9a8c\u5b50\u7cfb\u7edf\uff0c\u5f00\u53d1\u81ea\u52a8\u751f\u6210\u6570\u5b57\u5b6a\u751f\u7684\u65b9\u6cd5", "result": "\u521d\u6b65\u5b9e\u9a8c\u8bc1\u660e\u8be5\u65b9\u6cd5\u53ef\u884c\uff0c\u80fd\u81ea\u52a8\u521b\u5efa\u9ad8\u6548\u4e14\u8db3\u591f\u7cbe\u786e\u7684\u6570\u5b57\u5b6a\u751f", "conclusion": "\u81ea\u52a8\u751f\u6210\u7684\u6570\u5b57\u5b6a\u751f\u53ef\u4f5c\u4e3a\u73b0\u6709\u9a8c\u8bc1\u6d41\u7a0b\u7684\u6709\u6548\u8865\u5145\u5de5\u5177"}}
{"id": "2510.02894", "categories": ["cs.DC", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.02894", "abs": "https://arxiv.org/abs/2510.02894", "authors": ["Jakub Lisowski", "Piotr Tyrakowski", "Szymon Zygu\u0142a", "Krzysztof Kaczmarski"], "title": "PyRadiomics-cuda: a GPU-accelerated 3D features extraction from medical images within PyRadiomics", "comment": null, "summary": "PyRadiomics-cuda is a GPU-accelerated extension of the PyRadiomics library,\ndesigned to address the computational challenges of extracting\nthree-dimensional shape features from medical images. By offloading key\ngeometric computations to GPU hardware it dramatically reduces processing times\nfor large volumetric datasets. The system maintains full compatibility with the\noriginal PyRadiomics API, enabling seamless integration into existing AI\nworkflows without code modifications. This transparent acceleration facilitates\nefficient, scalable radiomics analysis, supporting rapid feature extraction\nessential for high-throughput AI pipeline. Tests performed on a typical\ncomputational cluster, budget and home devices prove usefulness in all\nscenarios. PyRadiomics-cuda is implemented in Python and C/CUDA and is freely\navailable under the BSD license at https://github.com/mis-wut/pyradiomics-CUDA\nAdditionally PyRadiomics-cuda test suite is available at\nhttps://github.com/mis-wut/pyradiomics-cuda-data-gen. It provides detailed\nhandbook and sample scripts suited for different kinds of workflows plus\ndetailed installation instructions. The dataset used for testing is available\nat Kaggle\nhttps://www.kaggle.com/datasets/sabahesaraki/kidney-tumor-segmentation-challengekits-19", "AI": {"tldr": "PyRadiomics-cuda\u662f\u4e00\u4e2aGPU\u52a0\u901f\u7684\u533b\u5b66\u5f71\u50cf\u7279\u5f81\u63d0\u53d6\u5de5\u5177\uff0c\u901a\u8fc7CUDA\u5b9e\u73b0\u663e\u8457\u63d0\u5347\u5904\u7406\u901f\u5ea6\uff0c\u4fdd\u6301\u4e0e\u539f\u6709API\u517c\u5bb9\u6027", "motivation": "\u89e3\u51b3\u533b\u5b66\u5f71\u50cf\u4e09\u7ef4\u5f62\u72b6\u7279\u5f81\u63d0\u53d6\u7684\u8ba1\u7b97\u6311\u6218\uff0c\u7279\u522b\u662f\u5904\u7406\u5927\u578b\u4f53\u6570\u636e\u96c6\u65f6\u7684\u6027\u80fd\u74f6\u9888\u95ee\u9898", "method": "\u901a\u8fc7\u5c06\u5173\u952e\u51e0\u4f55\u8ba1\u7b97\u5378\u8f7d\u5230GPU\u786c\u4ef6\uff0c\u4f7f\u7528Python\u548cC/CUDA\u5b9e\u73b0GPU\u52a0\u901f\uff0c\u4fdd\u6301\u4e0ePyRadiomics API\u7684\u5b8c\u5168\u517c\u5bb9", "result": "\u5728\u5404\u79cd\u8ba1\u7b97\u8bbe\u5907\u4e0a\uff08\u8ba1\u7b97\u96c6\u7fa4\u3001\u9884\u7b97\u8bbe\u5907\u548c\u5bb6\u7528\u8bbe\u5907\uff09\u90fd\u663e\u8457\u51cf\u5c11\u4e86\u5904\u7406\u65f6\u95f4\uff0c\u652f\u6301\u9ad8\u6548\u53ef\u6269\u5c55\u7684\u5f71\u50cf\u7ec4\u5b66\u5206\u6790", "conclusion": "PyRadiomics-cuda\u4e3aAI\u5de5\u4f5c\u6d41\u7a0b\u63d0\u4f9b\u4e86\u900f\u660e\u7684\u52a0\u901f\u89e3\u51b3\u65b9\u6848\uff0c\u652f\u6301\u9ad8\u901a\u91cfAI\u7ba1\u9053\u6240\u9700\u7684\u5feb\u901f\u7279\u5f81\u63d0\u53d6\uff0c\u4ee3\u7801\u5f00\u6e90\u4e14\u6613\u4e8e\u96c6\u6210"}}
{"id": "2510.02534", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.02534", "abs": "https://arxiv.org/abs/2510.02534", "authors": ["Mohsen Iranmanesh", "Sina Moradi Sabet", "Sina Marefat", "Ali Javidi Ghasr", "Allison Wilson", "Iman Sharafaldin", "Mohammad A. Tayebi"], "title": "ZeroFalse: Improving Precision in Static Analysis with LLMs", "comment": null, "summary": "Static Application Security Testing (SAST) tools are integral to modern\nsoftware development, yet their adoption is undermined by excessive false\npositives that weaken developer trust and demand costly manual triage. We\npresent ZeroFalse, a framework that integrates static analysis with large\nlanguage models (LLMs) to reduce false positives while preserving coverage.\nZeroFalse treats static analyzer outputs as structured contracts, enriching\nthem with flow-sensitive traces, contextual evidence, and CWE-specific\nknowledge before adjudication by an LLM. This design preserves the systematic\nreach of static analysis while leveraging the reasoning capabilities of LLMs.\nWe evaluate ZeroFalse across both benchmarks and real-world projects using ten\nstate-of-the-art LLMs. Our best-performing models achieve F1-scores of 0.912 on\nthe OWASP Java Benchmark and 0.955 on the OpenVuln dataset, maintaining recall\nand precision above 90%. Results further show that CWE-specialized prompting\nconsistently outperforms generic prompts, and reasoning-oriented LLMs provide\nthe most reliable precision-recall balance. These findings position ZeroFalse\nas a practical and scalable approach for enhancing the reliability of SAST and\nsupporting its integration into real-world CI/CD pipelines.", "AI": {"tldr": "ZeroFalse\u662f\u4e00\u4e2a\u7ed3\u5408\u9759\u6001\u5206\u6790\u548c\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u6784\u5316\u5904\u7406\u5206\u6790\u5668\u8f93\u51fa\u3001\u6dfb\u52a0\u6d41\u654f\u611f\u8ffd\u8e2a\u548c\u4e0a\u4e0b\u6587\u8bc1\u636e\uff0c\u663e\u8457\u51cf\u5c11SAST\u5de5\u5177\u7684\u8bef\u62a5\u7387\uff0c\u540c\u65f6\u4fdd\u6301\u9ad8\u8986\u76d6\u7387\u3002", "motivation": "\u9759\u6001\u5e94\u7528\u5b89\u5168\u6d4b\u8bd5(SAST)\u5de5\u5177\u5b58\u5728\u5927\u91cf\u8bef\u62a5\uff0c\u524a\u5f31\u5f00\u53d1\u8005\u4fe1\u4efb\u5e76\u9700\u8981\u6602\u8d35\u7684\u4eba\u5de5\u5ba1\u67e5\uff0c\u9700\u8981\u4e00\u79cd\u65b9\u6cd5\u6765\u51cf\u5c11\u8bef\u62a5\u540c\u65f6\u4fdd\u6301\u8986\u76d6\u8303\u56f4\u3002", "method": "\u5c06\u9759\u6001\u5206\u6790\u5668\u8f93\u51fa\u4f5c\u4e3a\u7ed3\u6784\u5316\u5408\u7ea6\u5904\u7406\uff0c\u6dfb\u52a0\u6d41\u654f\u611f\u8ffd\u8e2a\u3001\u4e0a\u4e0b\u6587\u8bc1\u636e\u548cCWE\u7279\u5b9a\u77e5\u8bc6\uff0c\u7136\u540e\u7531LLM\u8fdb\u884c\u88c1\u51b3\uff0c\u4fdd\u6301\u9759\u6001\u5206\u6790\u7684\u7cfb\u7edf\u6027\u8986\u76d6\u8303\u56f4\u540c\u65f6\u5229\u7528LLM\u7684\u63a8\u7406\u80fd\u529b\u3002", "result": "\u5728OWASP Java\u57fa\u51c6\u6d4b\u8bd5\u4e2dF1\u5206\u6570\u8fbe\u52300.912\uff0cOpenVuln\u6570\u636e\u96c6\u4e0a\u8fbe\u52300.955\uff0c\u53ec\u56de\u7387\u548c\u7cbe\u786e\u5ea6\u5747\u8d85\u8fc790%\uff0cCWE\u4e13\u7528\u63d0\u793a\u59cb\u7ec8\u4f18\u4e8e\u901a\u7528\u63d0\u793a\u3002", "conclusion": "ZeroFalse\u4e3a\u589e\u5f3aSAST\u53ef\u9760\u6027\u63d0\u4f9b\u4e86\u5b9e\u7528\u4e14\u53ef\u6269\u5c55\u7684\u65b9\u6cd5\uff0c\u652f\u6301\u5176\u96c6\u6210\u5230\u771f\u5b9e\u7684CI/CD\u6d41\u6c34\u7ebf\u4e2d\u3002"}}
{"id": "2510.02930", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2510.02930", "abs": "https://arxiv.org/abs/2510.02930", "authors": ["Wen Guan", "Tadashi Maeno", "Aleksandr Alekseev", "Fernando Harald Barreiro Megino", "Kaushik De", "Edward Karavakis", "Alexei Klimentov", "Tatiana Korchuganova", "FaHui Lin", "Paul Nilsson", "Torre Wenaus", "Zhaoyu Yang", "Xin Zhao"], "title": "iDDS: Intelligent Distributed Dispatch and Scheduling for Workflow Orchestration", "comment": null, "summary": "The intelligent Distributed Dispatch and Scheduling (iDDS) service is a\nversatile workflow orchestration system designed for large-scale, distributed\nscientific computing. iDDS extends traditional workload and data management by\nintegrating data-aware execution, conditional logic, and programmable\nworkflows, enabling automation of complex and dynamic processing pipelines.\nOriginally developed for the ATLAS experiment at the Large Hadron Collider,\niDDS has evolved into an experiment-agnostic platform that supports both\ntemplate-driven workflows and a Function-as-a-Task model for Python-based\norchestration.\n  This paper presents the architecture and core components of iDDS,\nhighlighting its scalability, modular message-driven design, and integration\nwith systems such as PanDA and Rucio. We demonstrate its versatility through\nreal-world use cases: fine-grained tape resource optimization for ATLAS,\norchestration of large Directed Acyclic Graph (DAG) workflows for the Rubin\nObservatory, distributed hyperparameter optimization for machine learning\napplications, active learning for physics analyses, and AI-assisted detector\ndesign at the Electron-Ion Collider.\n  By unifying workload scheduling, data movement, and adaptive decision-making,\niDDS reduces operational overhead and enables reproducible, high-throughput\nworkflows across heterogeneous infrastructures. We conclude with current\nchallenges and future directions, including interactive, cloud-native, and\nserverless workflow support.", "AI": {"tldr": "iDDS\u662f\u4e00\u4e2a\u667a\u80fd\u5206\u5e03\u5f0f\u8c03\u5ea6\u548c\u5de5\u4f5c\u6d41\u7f16\u6392\u7cfb\u7edf\uff0c\u4e13\u4e3a\u5927\u89c4\u6a21\u5206\u5e03\u5f0f\u79d1\u5b66\u8ba1\u7b97\u8bbe\u8ba1\uff0c\u96c6\u6210\u4e86\u6570\u636e\u611f\u77e5\u6267\u884c\u3001\u6761\u4ef6\u903b\u8f91\u548c\u53ef\u7f16\u7a0b\u5de5\u4f5c\u6d41\uff0c\u652f\u6301\u590d\u6742\u52a8\u6001\u5904\u7406\u7ba1\u9053\u7684\u81ea\u52a8\u5316\u3002", "motivation": "\u4f20\u7edf\u7684\u5de5\u4f5c\u8d1f\u8f7d\u548c\u6570\u636e\u7ba1\u7406\u65b9\u6cd5\u65e0\u6cd5\u6ee1\u8db3\u5927\u89c4\u6a21\u5206\u5e03\u5f0f\u79d1\u5b66\u8ba1\u7b97\u4e2d\u590d\u6742\u52a8\u6001\u5904\u7406\u7ba1\u9053\u7684\u9700\u6c42\uff0c\u9700\u8981\u5f00\u53d1\u4e00\u4e2a\u80fd\u591f\u7edf\u4e00\u8c03\u5ea6\u3001\u6570\u636e\u79fb\u52a8\u548c\u81ea\u9002\u5e94\u51b3\u7b56\u7684\u7cfb\u7edf\u6765\u964d\u4f4e\u64cd\u4f5c\u5f00\u9500\u5e76\u5b9e\u73b0\u53ef\u91cd\u590d\u7684\u9ad8\u541e\u5410\u91cf\u5de5\u4f5c\u6d41\u3002", "method": "\u91c7\u7528\u6a21\u5757\u5316\u6d88\u606f\u9a71\u52a8\u67b6\u6784\uff0c\u96c6\u6210PanDA\u548cRucio\u7b49\u7cfb\u7edf\uff0c\u652f\u6301\u6a21\u677f\u9a71\u52a8\u5de5\u4f5c\u6d41\u548c\u57fa\u4e8ePython\u7684Function-as-a-Task\u6a21\u578b\uff0c\u5b9e\u73b0\u6570\u636e\u611f\u77e5\u6267\u884c\u548c\u6761\u4ef6\u903b\u8f91\u3002", "result": "\u6210\u529f\u5e94\u7528\u4e8e\u591a\u4e2a\u771f\u5b9e\u573a\u666f\uff1aATLAS\u5b9e\u9a8c\u7684\u78c1\u5e26\u8d44\u6e90\u4f18\u5316\u3001Rubin\u5929\u6587\u53f0\u7684\u5927\u89c4\u6a21DAG\u5de5\u4f5c\u6d41\u7f16\u6392\u3001\u673a\u5668\u5b66\u4e60\u5206\u5e03\u5f0f\u8d85\u53c2\u6570\u4f18\u5316\u3001\u7269\u7406\u5206\u6790\u4e3b\u52a8\u5b66\u4e60\u4ee5\u53caEIC\u7684AI\u8f85\u52a9\u63a2\u6d4b\u5668\u8bbe\u8ba1\u3002", "conclusion": "iDDS\u901a\u8fc7\u7edf\u4e00\u5de5\u4f5c\u8d1f\u8f7d\u8c03\u5ea6\u3001\u6570\u636e\u79fb\u52a8\u548c\u81ea\u9002\u5e94\u51b3\u7b56\uff0c\u663e\u8457\u51cf\u5c11\u4e86\u64cd\u4f5c\u5f00\u9500\uff0c\u652f\u6301\u8de8\u5f02\u6784\u57fa\u7840\u8bbe\u65bd\u7684\u53ef\u91cd\u590d\u9ad8\u541e\u5410\u91cf\u5de5\u4f5c\u6d41\uff0c\u672a\u6765\u5c06\u6269\u5c55\u5230\u4ea4\u4e92\u5f0f\u3001\u4e91\u539f\u751f\u548c\u65e0\u670d\u52a1\u5668\u5de5\u4f5c\u6d41\u652f\u6301\u3002"}}
{"id": "2510.02585", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.02585", "abs": "https://arxiv.org/abs/2510.02585", "authors": ["Majid Dashtbani", "Ladan Tahvildari"], "title": "Key Considerations for Auto-Scaling: Lessons from Benchmark Microservices", "comment": null, "summary": "Microservices have become the dominant architectural paradigm for building\nscalable and modular cloud-native systems. However, achieving effective\nauto-scaling in such systems remains a non-trivial challenge, as it depends not\nonly on advanced scaling techniques but also on sound design, implementation,\nand deployment practices. Yet, these foundational aspects are often overlooked\nin existing benchmarks, making it difficult to evaluate autoscaling methods\nunder realistic conditions. In this paper, we identify a set of practical\nauto-scaling considerations by applying several state-of-the-art autoscaling\nmethods to widely used microservice benchmarks. To structure these findings, we\nclassify the issues based on when they arise during the software lifecycle:\nArchitecture, Implementation, and Deployment. The Architecture phase covers\nhigh-level decisions such as service decomposition and inter-service\ndependencies. The Implementation phase includes aspects like initialization\noverhead, metrics instrumentation, and error propagation. The Deployment phase\nfocuses on runtime configurations such as resource limits and health checks. We\nvalidate these considerations using the Sock-Shop benchmark and evaluate\ndiverse auto-scaling strategies, including threshold-based, control-theoretic,\nlearning-based, black-box optimization, and dependency-aware approaches. Our\nfindings show that overlooking key lifecycle concerns can degrade autoscaler\nperformance, while addressing them leads to more stable and efficient scaling.\nThese results underscore the importance of lifecycle-aware engineering for\nunlocking the full potential of auto-scaling in microservice-based systems.", "AI": {"tldr": "\u672c\u6587\u5206\u6790\u4e86\u5fae\u670d\u52a1\u7cfb\u7edf\u4e2d\u81ea\u52a8\u6269\u7f29\u5bb9\u7684\u5b9e\u9645\u6311\u6218\uff0c\u63d0\u51fa\u4e86\u57fa\u4e8e\u8f6f\u4ef6\u751f\u547d\u5468\u671f\uff08\u67b6\u6784\u3001\u5b9e\u73b0\u3001\u90e8\u7f72\uff09\u7684\u5206\u7c7b\u65b9\u6cd5\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5ffd\u89c6\u8fd9\u4e9b\u5173\u952e\u56e0\u7d20\u4f1a\u964d\u4f4e\u6269\u7f29\u5bb9\u6027\u80fd\u3002", "motivation": "\u5fae\u670d\u52a1\u5df2\u6210\u4e3a\u4e91\u539f\u751f\u7cfb\u7edf\u7684\u4e3b\u6d41\u67b6\u6784\u8303\u5f0f\uff0c\u4f46\u73b0\u6709\u57fa\u51c6\u6d4b\u8bd5\u5f80\u5f80\u5ffd\u89c6\u8bbe\u8ba1\u3001\u5b9e\u73b0\u548c\u90e8\u7f72\u7b49\u57fa\u7840\u65b9\u9762\uff0c\u96be\u4ee5\u5728\u771f\u5b9e\u6761\u4ef6\u4e0b\u8bc4\u4f30\u81ea\u52a8\u6269\u7f29\u5bb9\u65b9\u6cd5\u3002", "method": "\u901a\u8fc7\u5c06\u591a\u79cd\u6700\u5148\u8fdb\u7684\u81ea\u52a8\u6269\u7f29\u5bb9\u65b9\u6cd5\u5e94\u7528\u4e8e\u5e7f\u6cdb\u4f7f\u7528\u7684\u5fae\u670d\u52a1\u57fa\u51c6\u6d4b\u8bd5\uff0c\u8bc6\u522b\u5b9e\u8df5\u8003\u8651\u56e0\u7d20\uff0c\u5e76\u6309\u8f6f\u4ef6\u751f\u547d\u5468\u671f\u9636\u6bb5\u5206\u7c7b\uff1a\u67b6\u6784\u3001\u5b9e\u73b0\u548c\u90e8\u7f72\u3002\u4f7f\u7528Sock-Shop\u57fa\u51c6\u9a8c\u8bc1\u8fd9\u4e9b\u8003\u8651\u56e0\u7d20\uff0c\u5e76\u8bc4\u4f30\u591a\u79cd\u6269\u7f29\u5bb9\u7b56\u7565\u3002", "result": "\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u5ffd\u89c6\u5173\u952e\u751f\u547d\u5468\u671f\u95ee\u9898\u4f1a\u964d\u4f4e\u81ea\u52a8\u6269\u7f29\u5bb9\u5668\u6027\u80fd\uff0c\u800c\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u53ef\u4ee5\u5b9e\u73b0\u66f4\u7a33\u5b9a\u548c\u9ad8\u6548\u7684\u6269\u7f29\u5bb9\u3002", "conclusion": "\u8fd9\u4e9b\u7ed3\u679c\u5f3a\u8c03\u4e86\u751f\u547d\u5468\u671f\u611f\u77e5\u5de5\u7a0b\u5bf9\u4e8e\u91ca\u653e\u5fae\u670d\u52a1\u7cfb\u7edf\u4e2d\u81ea\u52a8\u6269\u7f29\u5bb9\u5168\u90e8\u6f5c\u529b\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2510.02609", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.02609", "abs": "https://arxiv.org/abs/2510.02609", "authors": ["Chengquan Guo", "Chulin Xie", "Yu Yang", "Zhaorun Chen", "Zinan Lin", "Xander Davies", "Yarin Gal", "Dawn Song", "Bo Li"], "title": "RedCodeAgent: Automatic Red-teaming Agent against Diverse Code Agents", "comment": null, "summary": "Code agents have gained widespread adoption due to their strong code\ngeneration capabilities and integration with code interpreters, enabling\ndynamic execution, debugging, and interactive programming capabilities. While\nthese advancements have streamlined complex workflows, they have also\nintroduced critical safety and security risks. Current static safety benchmarks\nand red-teaming tools are inadequate for identifying emerging real-world risky\nscenarios, as they fail to cover certain boundary conditions, such as the\ncombined effects of different jailbreak tools. In this work, we propose\nRedCodeAgent, the first automated red-teaming agent designed to systematically\nuncover vulnerabilities in diverse code agents. With an adaptive memory module,\nRedCodeAgent can leverage existing jailbreak knowledge, dynamically select the\nmost effective red-teaming tools and tool combinations in a tailored toolbox\nfor a given input query, thus identifying vulnerabilities that might otherwise\nbe overlooked. For reliable evaluation, we develop simulated sandbox\nenvironments to additionally evaluate the execution results of code agents,\nmitigating potential biases of LLM-based judges that only rely on static code.\nThrough extensive evaluations across multiple state-of-the-art code agents,\ndiverse risky scenarios, and various programming languages, RedCodeAgent\nconsistently outperforms existing red-teaming methods, achieving higher attack\nsuccess rates and lower rejection rates with high efficiency. We further\nvalidate RedCodeAgent on real-world code assistants, e.g., Cursor and Codeium,\nexposing previously unidentified security risks. By automating and optimizing\nred-teaming processes, RedCodeAgent enables scalable, adaptive, and effective\nsafety assessments of code agents.", "AI": {"tldr": "RedCodeAgent\u662f\u9996\u4e2a\u81ea\u52a8\u5316\u7ea2\u961f\u4ee3\u7406\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u8bb0\u5fc6\u6a21\u5757\u548c\u5b9a\u5236\u5316\u5de5\u5177\u7bb1\uff0c\u7cfb\u7edf\u6027\u5730\u53d1\u73b0\u4ee3\u7801\u4ee3\u7406\u7684\u5b89\u5168\u6f0f\u6d1e\uff0c\u5728\u6a21\u62df\u6c99\u76d2\u73af\u5883\u4e2d\u8bc4\u4f30\u6267\u884c\u7ed3\u679c\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u7ea2\u961f\u65b9\u6cd5\u3002", "motivation": "\u5f53\u524d\u4ee3\u7801\u4ee3\u7406\u7684\u5e7f\u6cdb\u4f7f\u7528\u5e26\u6765\u4e86\u5b89\u5168\u548c\u98ce\u9669\u95ee\u9898\uff0c\u73b0\u6709\u7684\u9759\u6001\u5b89\u5168\u57fa\u51c6\u548c\u7ea2\u961f\u5de5\u5177\u65e0\u6cd5\u6709\u6548\u8bc6\u522b\u65b0\u5174\u7684\u73b0\u5b9e\u4e16\u754c\u98ce\u9669\u573a\u666f\uff0c\u7279\u522b\u662f\u4e0d\u540c\u8d8a\u72f1\u5de5\u5177\u7684\u7ec4\u5408\u6548\u5e94\u3002", "method": "\u5f00\u53d1RedCodeAgent\u81ea\u52a8\u5316\u7ea2\u961f\u4ee3\u7406\uff0c\u5305\u542b\u81ea\u9002\u5e94\u8bb0\u5fc6\u6a21\u5757\u6765\u5229\u7528\u73b0\u6709\u8d8a\u72f1\u77e5\u8bc6\uff0c\u52a8\u6001\u9009\u62e9\u6700\u6709\u6548\u7684\u7ea2\u961f\u5de5\u5177\u7ec4\u5408\uff0c\u5e76\u5728\u6a21\u62df\u6c99\u76d2\u73af\u5883\u4e2d\u8bc4\u4f30\u4ee3\u7801\u4ee3\u7406\u7684\u6267\u884c\u7ed3\u679c\u3002", "result": "RedCodeAgent\u5728\u591a\u4e2a\u6700\u5148\u8fdb\u4ee3\u7801\u4ee3\u7406\u3001\u591a\u6837\u5316\u98ce\u9669\u573a\u666f\u548c\u5404\u79cd\u7f16\u7a0b\u8bed\u8a00\u7684\u8bc4\u4f30\u4e2d\uff0c\u59cb\u7ec8\u4f18\u4e8e\u73b0\u6709\u7ea2\u961f\u65b9\u6cd5\uff0c\u5b9e\u73b0\u4e86\u66f4\u9ad8\u7684\u653b\u51fb\u6210\u529f\u7387\u548c\u66f4\u4f4e\u7684\u62d2\u7edd\u7387\u3002", "conclusion": "RedCodeAgent\u901a\u8fc7\u81ea\u52a8\u5316\u548c\u4f18\u5316\u7ea2\u961f\u6d41\u7a0b\uff0c\u5b9e\u73b0\u4e86\u5bf9\u4ee3\u7801\u4ee3\u7406\u7684\u53ef\u6269\u5c55\u3001\u81ea\u9002\u5e94\u548c\u6709\u6548\u7684\u5b89\u5168\u8bc4\u4f30\uff0c\u66b4\u9732\u4e86\u5148\u524d\u672a\u8bc6\u522b\u7684\u5b89\u5168\u98ce\u9669\u3002"}}
{"id": "2510.02634", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.02634", "abs": "https://arxiv.org/abs/2510.02634", "authors": ["Hanlong Wan", "Weili Xu", "Michael Rosenberg", "Jian Zhang", "Aysha Siddika"], "title": "Automatic Building Code Review: A Case Study", "comment": null, "summary": "Building officials, particularly those in resource-constrained or rural\njurisdictions, face labor-intensive, error-prone, and costly manual reviews of\ndesign documents as projects increase in size and complexity. The growing\nadoption of Building Information Modeling (BIM) and Large Language Models\n(LLMs) presents opportunities for automated code review (ACR) solutions. This\nstudy introduces a novel agent-driven framework that integrates BIM-based data\nextraction with automated verification using both retrieval-augmented\ngeneration (RAG) and Model Context Protocol (MCP) agent pipelines. The\nframework employs LLM-enabled agents to extract geometry, schedules, and system\nattributes from heterogeneous file types, which are then processed for building\ncode checking through two complementary mechanisms: (1) direct API calls to the\nUS Department of Energy COMcheck engine, providing deterministic and\naudit-ready outputs, and (2) RAG-based reasoning over rule provisions, enabling\nflexible interpretation where coverage is incomplete or ambiguous.\n  The framework was evaluated through case demonstrations, including automated\nextraction of geometric attributes (such as surface area, tilt, and insulation\nvalues), parsing of operational schedules, and validation of lighting\nallowances under ASHRAE Standard 90.1-2022. Comparative performance tests\nacross multiple LLMs showed that GPT-4o achieved the best balance of efficiency\nand stability, while smaller models exhibited inconsistencies or failures.\nResults confirm that MCP agent pipelines outperform RAG reasoning pipelines in\nrigor and reliability. This work advances ACR research by demonstrating a\nscalable, interoperable, and production-ready approach that bridges BIM with\nauthoritative code review tools.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eBIM\u548cLLM\u7684\u81ea\u52a8\u5316\u5efa\u7b51\u89c4\u8303\u5ba1\u67e5\u6846\u67b6\uff0c\u6574\u5408\u4e86RAG\u548cMCP\u4ee3\u7406\u7ba1\u9053\uff0c\u901a\u8fc7COMcheck API\u548c\u89c4\u5219\u63a8\u7406\u5b9e\u73b0\u9ad8\u6548\u51c6\u786e\u7684\u4ee3\u7801\u5ba1\u67e5", "motivation": "\u5efa\u7b51\u5b98\u5458\u5728\u8d44\u6e90\u53d7\u9650\u6216\u519c\u6751\u5730\u533a\u9762\u4e34\u52b3\u52a8\u5bc6\u96c6\u578b\u3001\u6613\u51fa\u9519\u4e14\u6210\u672c\u9ad8\u6602\u7684\u8bbe\u8ba1\u6587\u6863\u624b\u52a8\u5ba1\u67e5\uff0c\u968f\u7740\u9879\u76ee\u89c4\u6a21\u548c\u590d\u6742\u6027\u7684\u589e\u52a0\uff0c\u9700\u8981\u81ea\u52a8\u5316\u89e3\u51b3\u65b9\u6848", "method": "\u5f00\u53d1\u4e86\u4ee3\u7406\u9a71\u52a8\u6846\u67b6\uff0c\u96c6\u6210BIM\u6570\u636e\u63d0\u53d6\u4e0e\u81ea\u52a8\u5316\u9a8c\u8bc1\uff0c\u4f7f\u7528RAG\u548cMCP\u4ee3\u7406\u7ba1\u9053\uff0c\u901a\u8fc7COMcheck API\u8c03\u7528\u548c\u89c4\u5219\u6761\u6b3e\u63a8\u7406\u8fdb\u884c\u5efa\u7b51\u89c4\u8303\u68c0\u67e5", "result": "\u6848\u4f8b\u6f14\u793a\u9a8c\u8bc1\u4e86\u6846\u67b6\u6709\u6548\u6027\uff0cGPT-4o\u5728\u6548\u7387\u548c\u7a33\u5b9a\u6027\u65b9\u9762\u8868\u73b0\u6700\u4f73\uff0cMCP\u4ee3\u7406\u7ba1\u9053\u5728\u4e25\u8c28\u6027\u548c\u53ef\u9760\u6027\u65b9\u9762\u4f18\u4e8eRAG\u63a8\u7406\u7ba1\u9053", "conclusion": "\u8be5\u5de5\u4f5c\u63a8\u8fdb\u4e86\u81ea\u52a8\u5316\u4ee3\u7801\u5ba1\u67e5\u7814\u7a76\uff0c\u5c55\u793a\u4e86\u53ef\u6269\u5c55\u3001\u53ef\u4e92\u64cd\u4f5c\u4e14\u751f\u4ea7\u5c31\u7eea\u7684\u65b9\u6cd5\uff0c\u5c06BIM\u4e0e\u6743\u5a01\u4ee3\u7801\u5ba1\u67e5\u5de5\u5177\u8fde\u63a5\u8d77\u6765"}}
{"id": "2510.02718", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.02718", "abs": "https://arxiv.org/abs/2510.02718", "authors": ["Ali Ghanbari", "Sasan Tavakkol"], "title": "Using Fourier Analysis and Mutant Clustering to Accelerate DNN Mutation Testing", "comment": "2025 40th IEEE/ACM International Conference on Automated Software\n  Engineering (ASE)", "summary": "Deep neural network (DNN) mutation analysis is a promising approach to\nevaluating test set adequacy. Due to the large number of generated mutants that\nmust be tested on large datasets, mutation analysis is costly. In this paper,\nwe present a technique, named DM#, for accelerating DNN mutation testing using\nFourier analysis. The key insight is that DNN outputs are real-valued functions\nsuitable for Fourier analysis that can be leveraged to quantify mutant behavior\nusing only a few data points. DM# uses the quantified mutant behavior to\ncluster the mutants so that the ones with similar behavior fall into the same\ngroup. A representative from each group is then selected for testing, and the\nresult of the test, e.g., whether the mutant is killed or survived, is reused\nfor all other mutants represented by the selected mutant, obviating the need\nfor testing other mutants. 14 DNN models of sizes ranging from thousands to\nmillions of parameters, trained on different datasets, are used to evaluate DM#\nand compare it to several baseline techniques. Our results provide empirical\nevidence on the effectiveness of DM# in accelerating mutation testing by\n28.38%, on average, at the average cost of only 0.72% error in mutation score.\nMoreover, on average, DM# incurs 11.78, 15.16, and 114.36 times less mutation\nscore error compared to random mutant selection, boundary sample selection, and\nrandom sample selection techniques, respectively, while generally offering\ncomparable speed-up.", "AI": {"tldr": "DM#\u4f7f\u7528\u5085\u91cc\u53f6\u5206\u6790\u52a0\u901fDNN\u7a81\u53d8\u6d4b\u8bd5\uff0c\u901a\u8fc7\u91cf\u5316\u7a81\u53d8\u4f53\u884c\u4e3a\u8fdb\u884c\u805a\u7c7b\uff0c\u53ea\u9700\u6d4b\u8bd5\u4ee3\u8868\u6027\u7a81\u53d8\u4f53\u5373\u53ef\u63a8\u65ad\u6574\u4e2a\u7ec4\u7684\u6d4b\u8bd5\u7ed3\u679c\uff0c\u5e73\u5747\u52a0\u901f28.38%\uff0c\u8bef\u5dee\u4ec50.72%\u3002", "motivation": "DNN\u7a81\u53d8\u5206\u6790\u6210\u672c\u9ad8\u6602\uff0c\u56e0\u4e3a\u9700\u8981\u6d4b\u8bd5\u5927\u91cf\u7a81\u53d8\u4f53\u548c\u5927\u6570\u636e\u96c6\uff0c\u9700\u8981\u4e00\u79cd\u52a0\u901f\u65b9\u6cd5\u3002", "method": "\u5229\u7528\u5085\u91cc\u53f6\u5206\u6790\u91cf\u5316DNN\u7a81\u53d8\u4f53\u884c\u4e3a\uff0c\u8fdb\u884c\u805a\u7c7b\u5206\u7ec4\uff0c\u6bcf\u7ec4\u9009\u4e00\u4e2a\u4ee3\u8868\u6027\u7a81\u53d8\u4f53\u6d4b\u8bd5\uff0c\u7ed3\u679c\u590d\u7528\u7ed9\u540c\u7ec4\u5176\u4ed6\u7a81\u53d8\u4f53\u3002", "result": "\u572814\u4e2aDNN\u6a21\u578b\u4e0a\u8bc4\u4f30\uff0c\u5e73\u5747\u52a0\u901f28.38%\uff0c\u5e73\u5747\u7a81\u53d8\u5206\u6570\u8bef\u5dee\u4ec50.72%\uff0c\u76f8\u6bd4\u57fa\u7ebf\u65b9\u6cd5\u8bef\u5dee\u51cf\u5c1111.78-114.36\u500d\u3002", "conclusion": "DM#\u80fd\u6709\u6548\u52a0\u901fDNN\u7a81\u53d8\u6d4b\u8bd5\uff0c\u5728\u4fdd\u6301\u9ad8\u51c6\u786e\u6027\u7684\u540c\u65f6\u663e\u8457\u964d\u4f4e\u8ba1\u7b97\u6210\u672c\u3002"}}
{"id": "2510.02773", "categories": ["cs.SE", "cs.CR"], "pdf": "https://arxiv.org/pdf/2510.02773", "abs": "https://arxiv.org/abs/2510.02773", "authors": ["Tamjid Al Rahat", "Yanju Chen", "Yu Feng", "Yuan Tian"], "title": "Automated Repair of OpenID Connect Programs (Extended Version)", "comment": "This is an extended version. The original paper is accepted to ASE\n  2025", "summary": "OpenID Connect has revolutionized online authentication based on single\nsign-on (SSO) by providing a secure and convenient method for accessing\nmultiple services with a single set of credentials. Despite its widespread\nadoption, critical security bugs in OpenID Connect have resulted in significant\nfinancial losses and security breaches, highlighting the need for robust\nmitigation strategies. Automated program repair presents a promising solution\nfor generating candidate patches for OpenID implementations. However,\nchallenges such as domain-specific complexities and the necessity for precise\nfault localization and patch verification must be addressed. We propose\nAuthFix, a counterexample-guided repair engine leveraging LLMs for automated\nOpenID bug fixing. AuthFix integrates three key components: fault localization,\npatch synthesis, and patch verification. By employing a novel Petri-net-based\nmodel checker, AuthFix ensures the correctness of patches by effectively\nmodeling interactions. Our evaluation on a dataset of OpenID bugs demonstrates\nthat AuthFix successfully generated correct patches for 17 out of 23 bugs\n(74%), with a high proportion of patches semantically equivalent to\ndeveloper-written fixes.", "AI": {"tldr": "AuthFix\u662f\u4e00\u4e2a\u57fa\u4e8eLLM\u7684\u81ea\u52a8\u5316OpenID Connect\u6f0f\u6d1e\u4fee\u590d\u5f15\u64ce\uff0c\u901a\u8fc7\u6545\u969c\u5b9a\u4f4d\u3001\u8865\u4e01\u5408\u6210\u548c\u9a8c\u8bc1\u4e09\u4e2a\u7ec4\u4ef6\uff0c\u6210\u529f\u4fee\u590d\u4e8674%\u7684\u6d4b\u8bd5\u6f0f\u6d1e\u3002", "motivation": "OpenID Connect\u867d\u7136\u5e7f\u6cdb\u91c7\u7528\uff0c\u4f46\u5b58\u5728\u4e25\u91cd\u5b89\u5168\u6f0f\u6d1e\u5bfc\u81f4\u91cd\u5927\u7ecf\u6d4e\u635f\u5931\u548c\u5b89\u5168\u6f0f\u6d1e\uff0c\u9700\u8981\u6709\u6548\u7684\u81ea\u52a8\u5316\u4fee\u590d\u65b9\u6848\u3002", "method": "AuthFix\u91c7\u7528\u53cd\u4f8b\u5f15\u5bfc\u7684\u4fee\u590d\u65b9\u6cd5\uff0c\u6574\u5408\u6545\u969c\u5b9a\u4f4d\u3001\u8865\u4e01\u5408\u6210\u548c\u8865\u4e01\u9a8c\u8bc1\u4e09\u4e2a\u5173\u952e\u7ec4\u4ef6\uff0c\u5e76\u4f7f\u7528\u65b0\u9896\u7684Petri\u7f51\u6a21\u578b\u68c0\u67e5\u5668\u786e\u4fdd\u8865\u4e01\u6b63\u786e\u6027\u3002", "result": "\u572823\u4e2aOpenID\u6f0f\u6d1e\u6d4b\u8bd5\u4e2d\uff0cAuthFix\u6210\u529f\u751f\u6210\u4e8617\u4e2a\u6b63\u786e\u8865\u4e01(74%)\uff0c\u5176\u4e2d\u5927\u90e8\u5206\u8865\u4e01\u4e0e\u5f00\u53d1\u8005\u7f16\u5199\u7684\u4fee\u590d\u65b9\u6848\u8bed\u4e49\u7b49\u4ef7\u3002", "conclusion": "AuthFix\u8bc1\u660e\u4e86LLM\u5728\u81ea\u52a8\u5316\u7a0b\u5e8f\u4fee\u590d\u4e2d\u7684\u6709\u6548\u6027\uff0c\u7279\u522b\u662f\u5728OpenID Connect\u8fd9\u6837\u7684\u590d\u6742\u9886\u57df\u7279\u5b9a\u7cfb\u7edf\u4e2d\uff0c\u80fd\u591f\u9ad8\u6548\u751f\u6210\u9ad8\u8d28\u91cf\u7684\u4fee\u590d\u8865\u4e01\u3002"}}
{"id": "2510.02854", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.02854", "abs": "https://arxiv.org/abs/2510.02854", "authors": ["Boshuai Ye", "Arif Ali Khan", "Teemu Pihkakoski", "Peng Liang", "Muhammad Azeem Akbar", "Matti Silveri", "Lauri Malmi"], "title": "C2|Q>: A Robust Framework for Bridging Classical and Quantum Software Development", "comment": "46 pages, 8 images, 14 tables, Manuscript submitted to a Journal\n  (2025)", "summary": "Quantum Software Engineering (QSE) is emerging as a critical discipline to\nmake quantum computing accessible to a broader developer community; however,\nmost quantum development environments still require developers to engage with\nlow-level details across the software stack - including problem encoding,\ncircuit construction, algorithm configuration, hardware selection, and result\ninterpretation - making them difficult for classical software engineers to use.\nTo bridge this gap, we present C2|Q>: a hardware-agnostic quantum software\ndevelopment framework that translates classical specifications (code) into\nquantum-executable programs while preserving methodological rigor. The\nframework applies modular software engineering principles by classifying the\nworkflow into three core modules: an encoder that classifies problems, produces\nQuantum-Compatible Formats (QCFs), and constructs quantum circuits, a\ndeployment module that generates circuits and recommends hardware based on\nfidelity, runtime, and cost, and a decoder that interprets quantum outputs into\nclassical solutions. In evaluation, the encoder module achieved a 93.8%\ncompletion rate, the hardware recommendation module consistently selected the\nappropriate quantum devices for workloads scaling up to 56 qubits, and the full\nC2|Q>: workflow successfully processed classical specifications (434 Python\nsnippets and 100 JSON inputs) with completion rates of 93.8% and 100%,\nrespectively. For case study problems executed on publicly available NISQ\nhardware, C2|Q>: reduced the required implementation effort by nearly 40X\ncompared to manual implementations using low-level quantum software development\nkits (SDKs), with empirical runs limited to small- and medium-sized instances\nconsistent with current NISQ capabilities. The open-source implementation of\nC2|Q>: is available at https://github.com/C2-Q/C2Q", "AI": {"tldr": "C2|Q>\u662f\u4e00\u4e2a\u786c\u4ef6\u65e0\u5173\u7684\u91cf\u5b50\u8f6f\u4ef6\u5f00\u53d1\u6846\u67b6\uff0c\u901a\u8fc7\u5c06\u7ecf\u5178\u4ee3\u7801\u8f6c\u6362\u4e3a\u91cf\u5b50\u53ef\u6267\u884c\u7a0b\u5e8f\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u91cf\u5b50\u7f16\u7a0b\u7684\u95e8\u69db\uff0c\u4f7f\u4f20\u7edf\u8f6f\u4ef6\u5de5\u7a0b\u5e08\u80fd\u591f\u66f4\u5bb9\u6613\u5730\u8fdb\u884c\u91cf\u5b50\u8ba1\u7b97\u5f00\u53d1\u3002", "motivation": "\u5f53\u524d\u91cf\u5b50\u5f00\u53d1\u73af\u5883\u9700\u8981\u5f00\u53d1\u8005\u5904\u7406\u8f6f\u4ef6\u6808\u7684\u4f4e\u5c42\u7ec6\u8282\uff0c\u5305\u62ec\u95ee\u9898\u7f16\u7801\u3001\u7535\u8def\u6784\u5efa\u3001\u7b97\u6cd5\u914d\u7f6e\u7b49\uff0c\u8fd9\u4f7f\u5f97\u4f20\u7edf\u8f6f\u4ef6\u5de5\u7a0b\u5e08\u96be\u4ee5\u4f7f\u7528\u91cf\u5b50\u8ba1\u7b97\u3002", "method": "\u6846\u67b6\u91c7\u7528\u6a21\u5757\u5316\u8f6f\u4ef6\u5de5\u7a0b\u539f\u5219\uff0c\u5206\u4e3a\u4e09\u4e2a\u6838\u5fc3\u6a21\u5757\uff1a\u7f16\u7801\u5668\uff08\u5206\u7c7b\u95ee\u9898\u3001\u751f\u6210\u91cf\u5b50\u517c\u5bb9\u683c\u5f0f\u3001\u6784\u5efa\u91cf\u5b50\u7535\u8def\uff09\u3001\u90e8\u7f72\u6a21\u5757\uff08\u751f\u6210\u7535\u8def\u5e76\u57fa\u4e8e\u4fdd\u771f\u5ea6\u3001\u8fd0\u884c\u65f6\u95f4\u548c\u6210\u672c\u63a8\u8350\u786c\u4ef6\uff09\u3001\u89e3\u7801\u5668\uff08\u5c06\u91cf\u5b50\u8f93\u51fa\u89e3\u91ca\u4e3a\u7ecf\u5178\u89e3\u51b3\u65b9\u6848\uff09\u3002", "result": "\u7f16\u7801\u5668\u6a21\u5757\u5b8c\u6210\u7387\u8fbe\u523093.8%\uff0c\u786c\u4ef6\u63a8\u8350\u6a21\u5757\u80fd\u6b63\u786e\u9009\u62e9\u6700\u591a56\u91cf\u5b50\u6bd4\u7279\u7684\u8bbe\u5907\uff0c\u5b8c\u6574\u5de5\u4f5c\u6d41\u5904\u7406434\u4e2aPython\u4ee3\u7801\u7247\u6bb5\u548c100\u4e2aJSON\u8f93\u5165\u7684\u5b8c\u6210\u7387\u5206\u522b\u4e3a93.8%\u548c100%\u3002\u5728NISQ\u786c\u4ef6\u4e0a\u7684\u6848\u4f8b\u7814\u7a76\u4e2d\uff0c\u76f8\u6bd4\u624b\u52a8\u5b9e\u73b0\u51cf\u5c11\u4e86\u8fd140\u500d\u7684\u5de5\u4f5c\u91cf\u3002", "conclusion": "C2|Q>\u6846\u67b6\u6210\u529f\u5730\u5c06\u8f6f\u4ef6\u5de5\u7a0b\u539f\u5219\u5e94\u7528\u4e8e\u91cf\u5b50\u8ba1\u7b97\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u91cf\u5b50\u7f16\u7a0b\u7684\u590d\u6742\u6027\uff0c\u4f7f\u4f20\u7edf\u5f00\u53d1\u8005\u80fd\u591f\u66f4\u5bb9\u6613\u5730\u5229\u7528\u91cf\u5b50\u8ba1\u7b97\u80fd\u529b\u3002"}}
{"id": "2510.02887", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.02887", "abs": "https://arxiv.org/abs/2510.02887", "authors": ["Zhao Zhang", "Qingyuan Liang", "Zeyu Sun", "Yizhou Chen", "Guoqing Wang", "Yican Sun", "Lu Zhang", "Ge Li", "Yingfei Xiong"], "title": "GramTrans: A Better Code Representation Approach in Code Generation", "comment": null, "summary": "Code generation has shown great promise in assisting software development. A\nfundamental yet underexplored question is how the choice of code representation\naffects model performance. While existing studies employ various\nrepresentations, such as treating code as plain text, grammar rule sequences,\nor syntax tree sequences, they lack a principled understanding of the\nrelationship between parsing difficulty and model effectiveness. This paper\nproposes a conjecture: the easier a representation is to parse, the better\nperformance the model achieves. We formalize this idea using grammar classes,\nwhere representations in simpler classes (e.g., LL(1)) are easier to parse.\nThrough a controlled experiment on a Python-based DSL, we show that parsing\ndifficulty strongly correlates with model performance. Motivated by this\nfinding, we present GramTrans, a general approach that automatically transforms\na context-free language into a representation within the LL(1) class. GramTrans\nintroduces a novel hierarchical conflict elimination algorithm, enabling a\nflexible trade-off between syntactic simplicity and token efficiency. We\nevaluate GramTrans on both Python and Java using three code generation models:\nStarCoder 1B, DeepSeek-Coder 1.3B, and Qwen2.5 1.5B. Across multiple\nbenchmarks, GramTrans consistently delivers significant improvements over\nbaseline representations. Furthermore, our analysis of existing representations\nreconfirms the strong alignment between parsing difficulty and model\nperformance, providing additional support for the conjecture.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4ee3\u7801\u8868\u793a\u5f62\u5f0f\u7684\u89e3\u6790\u96be\u5ea6\u4e0e\u6a21\u578b\u6027\u80fd\u76f8\u5173\u7684\u731c\u60f3\uff1a\u89e3\u6790\u8d8a\u5bb9\u6613\u7684\u8868\u793a\u5f62\u5f0f\uff0c\u6a21\u578b\u6027\u80fd\u8d8a\u597d\u3002\u901a\u8fc7GramTrans\u65b9\u6cd5\u5c06\u4e0a\u4e0b\u6587\u65e0\u5173\u8bed\u8a00\u8f6c\u6362\u4e3aLL(1)\u7c7b\u8868\u793a\uff0c\u5b9e\u9a8c\u8bc1\u660e\u8be5\u65b9\u6cd5\u80fd\u663e\u8457\u63d0\u5347\u4ee3\u7801\u751f\u6210\u6a21\u578b\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u4f7f\u7528\u591a\u79cd\u4ee3\u7801\u8868\u793a\u5f62\u5f0f\uff08\u7eaf\u6587\u672c\u3001\u8bed\u6cd5\u89c4\u5219\u5e8f\u5217\u3001\u8bed\u6cd5\u6811\u5e8f\u5217\u7b49\uff09\uff0c\u4f46\u7f3a\u4e4f\u5bf9\u89e3\u6790\u96be\u5ea6\u4e0e\u6a21\u578b\u6548\u679c\u5173\u7cfb\u7684\u7cfb\u7edf\u6027\u7406\u89e3\u3002\u9700\u8981\u63a2\u7d22\u4ee3\u7801\u8868\u793a\u5f62\u5f0f\u9009\u62e9\u5bf9\u6a21\u578b\u6027\u80fd\u7684\u5f71\u54cd\u673a\u5236\u3002", "method": "\u63d0\u51faGramTrans\u65b9\u6cd5\uff0c\u4f7f\u7528\u5206\u5c42\u51b2\u7a81\u6d88\u9664\u7b97\u6cd5\u5c06\u4e0a\u4e0b\u6587\u65e0\u5173\u8bed\u8a00\u8f6c\u6362\u4e3aLL(1)\u7c7b\u8868\u793a\u3002\u5728Python\u548cJava\u4e0a\u4f7f\u7528StarCoder\u3001DeepSeek-Coder\u3001Qwen2.5\u7b49\u6a21\u578b\u8fdb\u884c\u8bc4\u4f30\uff0c\u901a\u8fc7\u63a7\u5236\u5b9e\u9a8c\u9a8c\u8bc1\u89e3\u6790\u96be\u5ea6\u4e0e\u6a21\u578b\u6027\u80fd\u7684\u76f8\u5173\u6027\u3002", "result": "GramTrans\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d consistently \u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u8868\u793a\u5f62\u5f0f\u3002\u89e3\u6790\u96be\u5ea6\u4e0e\u6a21\u578b\u6027\u80fd\u5448\u73b0\u5f3a\u76f8\u5173\u6027\uff0c\u652f\u6301\u4e86\u8bba\u6587\u7684\u731c\u60f3\u3002", "conclusion": "\u4ee3\u7801\u8868\u793a\u5f62\u5f0f\u7684\u89e3\u6790\u96be\u5ea6\u662f\u5f71\u54cd\u6a21\u578b\u6027\u80fd\u7684\u5173\u952e\u56e0\u7d20\uff0c\u9009\u62e9\u6613\u4e8e\u89e3\u6790\u7684\u8868\u793a\u5f62\u5f0f\u53ef\u4ee5\u663e\u8457\u63d0\u5347\u4ee3\u7801\u751f\u6210\u6a21\u578b\u7684\u6027\u80fd\u3002GramTrans\u63d0\u4f9b\u4e86\u4e00\u79cd\u901a\u7528\u7684\u65b9\u6cd5\u6765\u5b9e\u73b0\u8fd9\u79cd\u4f18\u5316\u3002"}}
{"id": "2510.02917", "categories": ["cs.SE", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.02917", "abs": "https://arxiv.org/abs/2510.02917", "authors": ["Kriz Tahimic", "Charibeth Cheng"], "title": "Mechanistic Interpretability of Code Correctness in LLMs via Sparse Autoencoders", "comment": null, "summary": "As Large Language Models become integral to software development, with\nsubstantial portions of AI-suggested code entering production, understanding\ntheir internal correctness mechanisms becomes critical for safe deployment. We\napply sparse autoencoders to decompose LLM representations, identifying\ndirections that correspond to code correctness. We select predictor directions\nusing t-statistics and steering directions through separation scores from base\nmodel representations, then analyze their mechanistic properties through\nsteering, attention analysis, and weight orthogonalization. We find that code\ncorrectness directions in LLMs reliably predict incorrect code, while\ncorrection capabilities, though statistically significant, involve tradeoffs\nbetween fixing errors and preserving correct code. Mechanistically, successful\ncode generation depends on attending to test cases rather than problem\ndescriptions. Moreover, directions identified in base models retain their\neffectiveness after instruction-tuning, suggesting code correctness mechanisms\nlearned during pre-training are repurposed during fine-tuning. Our mechanistic\ninsights suggest three practical applications: prompting strategies should\nprioritize test examples over elaborate problem descriptions, predictor\ndirections can serve as error alarms for developer review, and these same\npredictors can guide selective steering, intervening only when errors are\nanticipated to prevent the code corruption from constant steering.", "AI": {"tldr": "\u4f7f\u7528\u7a00\u758f\u81ea\u7f16\u7801\u5668\u5206\u89e3LLM\u8868\u793a\uff0c\u8bc6\u522b\u4ee3\u7801\u6b63\u786e\u6027\u65b9\u5411\uff0c\u53d1\u73b0\u6ce8\u610f\u529b\u5e94\u96c6\u4e2d\u5728\u6d4b\u8bd5\u7528\u4f8b\u800c\u975e\u95ee\u9898\u63cf\u8ff0\u4e0a\uff0c\u9884\u6d4b\u65b9\u5411\u53ef\u4f5c\u4e3a\u9519\u8bef\u8b66\u62a5\uff0c\u6307\u5bfc\u9009\u62e9\u6027\u5e72\u9884", "motivation": "\u968f\u7740\u5927\u8bed\u8a00\u6a21\u578b\u5728\u8f6f\u4ef6\u5f00\u53d1\u4e2d\u5e7f\u6cdb\u5e94\u7528\uff0c\u7406\u89e3\u5176\u5185\u90e8\u6b63\u786e\u6027\u673a\u5236\u5bf9\u4e8e\u5b89\u5168\u90e8\u7f72\u81f3\u5173\u91cd\u8981", "method": "\u5e94\u7528\u7a00\u758f\u81ea\u7f16\u7801\u5668\u5206\u89e3LLM\u8868\u793a\uff0c\u4f7f\u7528t\u7edf\u8ba1\u91cf\u9009\u62e9\u9884\u6d4b\u65b9\u5411\uff0c\u901a\u8fc7\u5206\u79bb\u5206\u6570\u786e\u5b9a\u8f6c\u5411\u65b9\u5411\uff0c\u5e76\u8fdb\u884c\u8f6c\u5411\u3001\u6ce8\u610f\u529b\u5206\u6790\u548c\u6743\u91cd\u6b63\u4ea4\u5316\u5206\u6790", "result": "\u4ee3\u7801\u6b63\u786e\u6027\u65b9\u5411\u80fd\u53ef\u9760\u9884\u6d4b\u9519\u8bef\u4ee3\u7801\uff0c\u4fee\u6b63\u80fd\u529b\u867d\u6709\u7edf\u8ba1\u663e\u8457\u6027\u4f46\u9700\u8981\u5728\u4fee\u590d\u9519\u8bef\u548c\u4fdd\u6301\u6b63\u786e\u4ee3\u7801\u95f4\u6743\u8861\uff0c\u6307\u4ee4\u5fae\u8c03\u540e\u57fa\u7840\u6a21\u578b\u4e2d\u7684\u65b9\u5411\u4ecd\u4fdd\u6301\u6709\u6548\u6027", "conclusion": "\u4ee3\u7801\u6b63\u786e\u6027\u673a\u5236\u5728\u9884\u8bad\u7ec3\u671f\u95f4\u5b66\u4e60\u5e76\u5728\u5fae\u8c03\u4e2d\u88ab\u91cd\u65b0\u5229\u7528\uff0c\u63d0\u51fa\u4e86\u4e09\u4e2a\u5b9e\u9645\u5e94\u7528\uff1a\u63d0\u793a\u7b56\u7565\u5e94\u4f18\u5148\u6d4b\u8bd5\u7528\u4f8b\u3001\u9884\u6d4b\u65b9\u5411\u53ef\u4f5c\u4e3a\u9519\u8bef\u8b66\u62a5\u3001\u9884\u6d4b\u5668\u53ef\u6307\u5bfc\u9009\u62e9\u6027\u8f6c\u5411"}}
{"id": "2510.02934", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.02934", "abs": "https://arxiv.org/abs/2510.02934", "authors": ["Thanh Trong Vu", "Tuan-Dung Bui", "Thu-Trang Nguyen", "Son Nguyen", "Hieu Dinh Vo"], "title": "Model-Agnostic Correctness Assessment for LLM-Generated Code via Dynamic Internal Representation Selection", "comment": null, "summary": "Large Language Models (LLMs) have demonstrated impressive capabilities in\ncode generation and are increasingly integrated into the software development\nprocess. However, ensuring the correctness of LLM-generated code remains a\ncritical concern. Prior work has shown that the internal representations of\nLLMs encode meaningful signals for assessing code correctness. Nevertheless,\nthe existing methods rely on representations from pre-selected/fixed layers and\ntoken positions, which could limit its generalizability across diverse model\narchitectures and tasks. In this work, we introduce AUTOPROBE, a novel\nmodel-agnostic approach that dynamically selects the most informative internal\nrepresentations for code correctness assessment. AUTOPROBE employs an\nattention-based mechanism to learn importance scores for hidden states,\nenabling it to focus on the most relevant features. These weighted\nrepresentations are then aggregated and passed to a probing classifier to\npredict code correctness across multiple dimensions, including compilability,\nfunctionality, and security. To evaluate the performance of AUTOPROBE, we\nconduct extensive experiments across multiple benchmarks and code LLMs. Our\nexperimental results show that AUTOPROBE consistently outperforms the\nbaselines. For security assessment, AUTOPROBE surpasses the state-of-the-art\nwhite-box approach by 18%. For compilability and functionality assessment,\nAUTOPROBE demonstrates its highest robustness to code complexity, with the\nperformance higher than the other approaches by up to 19% and 111%,\nrespectively. These findings highlight that dynamically selecting important\ninternal signals enables AUTOPROBE to serve as a robust and generalizable\nsolution for assessing the correctness of code generated by various LLMs.", "AI": {"tldr": "AUTOPROBE\u662f\u4e00\u79cd\u6a21\u578b\u65e0\u5173\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u52a8\u6001\u9009\u62e9LLM\u5185\u90e8\u6700\u6709\u4fe1\u606f\u91cf\u7684\u8868\u5f81\u6765\u8bc4\u4f30\u4ee3\u7801\u6b63\u786e\u6027\uff0c\u5728\u7f16\u8bd1\u6027\u3001\u529f\u80fd\u6027\u548c\u5b89\u5168\u6027\u8bc4\u4f30\u65b9\u9762\u5747\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u9884\u9009\u56fa\u5b9a\u5c42\u548ctoken\u4f4d\u7f6e\u7684\u8868\u5f81\uff0c\u9650\u5236\u4e86\u5728\u4e0d\u540c\u6a21\u578b\u67b6\u6784\u548c\u4efb\u52a1\u95f4\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u9700\u8981\u66f4\u7075\u6d3b\u7684\u52a8\u6001\u9009\u62e9\u673a\u5236\u3002", "method": "\u91c7\u7528\u57fa\u4e8e\u6ce8\u610f\u529b\u7684\u673a\u5236\u5b66\u4e60\u9690\u85cf\u72b6\u6001\u7684\u91cd\u8981\u6027\u5206\u6570\uff0c\u805a\u7126\u6700\u76f8\u5173\u7279\u5f81\uff0c\u52a0\u6743\u805a\u5408\u540e\u901a\u8fc7\u63a2\u6d4b\u5206\u7c7b\u5668\u9884\u6d4b\u4ee3\u7801\u6b63\u786e\u6027\u3002", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u548c\u4ee3\u7801LLM\u4e0a\u7684\u5b9e\u9a8c\u663e\u793a\uff0cAUTOPROBE\u59cb\u7ec8\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\uff1a\u5b89\u5168\u6027\u8bc4\u4f30\u8d85\u8d8a\u6700\u5148\u8fdb\u767d\u76d2\u65b9\u6cd518%\uff1b\u7f16\u8bd1\u6027\u548c\u529f\u80fd\u6027\u8bc4\u4f30\u5206\u522b\u6bd4\u5176\u4ed6\u65b9\u6cd5\u9ad819%\u548c111%\u3002", "conclusion": "\u52a8\u6001\u9009\u62e9\u91cd\u8981\u5185\u90e8\u4fe1\u53f7\u4f7fAUTOPROBE\u6210\u4e3a\u8bc4\u4f30\u5404\u79cdLLM\u751f\u6210\u4ee3\u7801\u6b63\u786e\u6027\u7684\u9c81\u68d2\u4e14\u53ef\u6cdb\u5316\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.02991", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.02991", "abs": "https://arxiv.org/abs/2510.02991", "authors": ["Carlos Albuquerque", "Filipe F. Correia"], "title": "Tracing and Metrics Design Patterns for Monitoring Cloud-native Applications", "comment": "Accepted for publication in the EuroPLoP 2025 proceedings", "summary": "Observability helps ensure the reliability and maintainability of\ncloud-native applications. As software architectures become increasingly\ndistributed and subject to change, it becomes a greater challenge to diagnose\nsystem issues effectively, often having to deal with fragmented observability\nand more difficult root cause analysis. This paper builds upon our previous\nwork and introduces three design patterns that address key challenges in\nmonitoring cloud-native applications. Distributed Tracing improves visibility\ninto request flows across services, aiding in latency analysis and root cause\ndetection, Application Metrics provides a structured approach to instrumenting\napplications with meaningful performance indicators, enabling real-time\nmonitoring and anomaly detection, and Infrastructure Metrics focuses on\nmonitoring the environment in which the system is operated, helping teams\nassess resource utilization, scalability, and operational health. These\npatterns are derived from industry practices and observability frameworks and\naim to offer guidance for software practitioners.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e09\u79cd\u4e91\u539f\u751f\u5e94\u7528\u76d1\u63a7\u8bbe\u8ba1\u6a21\u5f0f\uff1a\u5206\u5e03\u5f0f\u8ffd\u8e2a\u3001\u5e94\u7528\u6307\u6807\u548c\u57fa\u7840\u8bbe\u65bd\u6307\u6807\uff0c\u4ee5\u89e3\u51b3\u5206\u5e03\u5f0f\u7cfb\u7edf\u53ef\u89c2\u6d4b\u6027\u6311\u6218", "motivation": "\u968f\u7740\u4e91\u539f\u751f\u67b6\u6784\u65e5\u76ca\u5206\u5e03\u5f0f\u548c\u6613\u53d8\uff0c\u7cfb\u7edf\u95ee\u9898\u8bca\u65ad\u53d8\u5f97\u66f4\u5177\u6311\u6218\u6027\uff0c\u9700\u8981\u5e94\u5bf9\u788e\u7247\u5316\u53ef\u89c2\u6d4b\u6027\u548c\u66f4\u56f0\u96be\u7684\u6839\u56e0\u5206\u6790", "method": "\u57fa\u4e8e\u5148\u524d\u5de5\u4f5c\uff0c\u5f15\u5165\u4e09\u79cd\u8bbe\u8ba1\u6a21\u5f0f\uff1a\u5206\u5e03\u5f0f\u8ffd\u8e2a\uff08\u8de8\u670d\u52a1\u8bf7\u6c42\u6d41\u53ef\u89c6\u5316\uff09\u3001\u5e94\u7528\u6307\u6807\uff08\u7ed3\u6784\u5316\u5e94\u7528\u6027\u80fd\u76d1\u63a7\uff09\u3001\u57fa\u7840\u8bbe\u65bd\u6307\u6807\uff08\u8fd0\u884c\u73af\u5883\u76d1\u63a7\uff09", "result": "\u8fd9\u4e9b\u6a21\u5f0f\u6e90\u81ea\u884c\u4e1a\u5b9e\u8df5\u548c\u53ef\u89c2\u6d4b\u6027\u6846\u67b6\uff0c\u4e3a\u8f6f\u4ef6\u4ece\u4e1a\u8005\u63d0\u4f9b\u6307\u5bfc\uff0c\u5e2e\u52a9\u6539\u5584\u5ef6\u8fdf\u5206\u6790\u3001\u6839\u56e0\u68c0\u6d4b\u3001\u5b9e\u65f6\u76d1\u63a7\u548c\u5f02\u5e38\u68c0\u6d4b", "conclusion": "\u4e09\u79cd\u8bbe\u8ba1\u6a21\u5f0f\u5171\u540c\u6784\u6210\u4e86\u5b8c\u6574\u7684\u4e91\u539f\u751f\u5e94\u7528\u53ef\u89c2\u6d4b\u6027\u89e3\u51b3\u65b9\u6848\uff0c\u80fd\u591f\u6709\u6548\u63d0\u5347\u7cfb\u7edf\u53ef\u9760\u6027\u548c\u53ef\u7ef4\u62a4\u6027"}}
{"id": "2510.03005", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.03005", "abs": "https://arxiv.org/abs/2510.03005", "authors": ["Daniel Pinho", "Petr P\u00edcha", "Filipe Correia", "P\u0159emek Brada"], "title": "Patterns for Teaching Agile with Student Projects -- Team and Project Setup", "comment": "Accepted for publication in the EuroPLoP 2025 proceedings", "summary": "Higher education courses teaching about agile software development (ASD) have\nincreased in commonality as the ideas behind the Agile Manifesto became more\ncommonplace in the industry. However, a lot of the literature on how ASD is\napplied in the classroom does not provide much actionable advice, focusing on\nframeworks or even moving beyond the software development area into teaching in\nan agile way. We, therefore, showcase early work on a pattern language that\nfocuses on teaching ASD practices to university students, which stems from our\nown experiences as educators in higher education contexts. We present five\npatterns, specifically focused on team and project setup phase: Capping Team\nSize, Smaller Project Scope, Business Non-Critical Project, Self-assembling\nTeams, and Team Chooses Topic as a starting point for developing the overall\npattern language.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u4e2a\u7528\u4e8e\u9ad8\u7b49\u6559\u80b2\u4e2d\u654f\u6377\u8f6f\u4ef6\u5f00\u53d1\u6559\u5b66\u7684\u521d\u6b65\u6a21\u5f0f\u8bed\u8a00\uff0c\u91cd\u70b9\u5173\u6ce8\u56e2\u961f\u548c\u9879\u76ee\u8bbe\u7f6e\u9636\u6bb5\u7684\u4e94\u4e2a\u5177\u4f53\u6a21\u5f0f", "motivation": "\u73b0\u6709\u7684\u654f\u6377\u8f6f\u4ef6\u5f00\u53d1\u6559\u5b66\u6587\u732e\u7f3a\u4e4f\u53ef\u64cd\u4f5c\u7684\u5efa\u8bae\uff0c\u5927\u591a\u5173\u6ce8\u6846\u67b6\u6216\u6cdb\u5316\u7684\u654f\u6377\u6559\u5b66\u65b9\u5f0f\uff0c\u800c\u975e\u5177\u4f53\u7684\u8f6f\u4ef6\u5f00\u53d1\u5b9e\u8df5\u6559\u5b66", "method": "\u57fa\u4e8e\u9ad8\u7b49\u6559\u80b2\u80cc\u666f\u4e0b\u7684\u6559\u5b66\u7ecf\u9a8c\uff0c\u5f00\u53d1\u4e86\u4e00\u4e2a\u6a21\u5f0f\u8bed\u8a00\uff0c\u63d0\u51fa\u4e86\u4e94\u4e2a\u9488\u5bf9\u56e2\u961f\u548c\u9879\u76ee\u8bbe\u7f6e\u9636\u6bb5\u7684\u6a21\u5f0f\uff1a\u9650\u5236\u56e2\u961f\u89c4\u6a21\u3001\u7f29\u5c0f\u9879\u76ee\u8303\u56f4\u3001\u975e\u5173\u952e\u4e1a\u52a1\u9879\u76ee\u3001\u81ea\u7ec4\u7ec7\u56e2\u961f\u548c\u56e2\u961f\u9009\u62e9\u4e3b\u9898", "result": "\u63d0\u51fa\u4e86\u4e94\u4e2a\u5177\u4f53\u7684\u6559\u5b66\u5b9e\u8df5\u6a21\u5f0f\uff0c\u4e3a\u5f00\u53d1\u5b8c\u6574\u7684\u654f\u6377\u8f6f\u4ef6\u5f00\u53d1\u6559\u5b66\u6a21\u5f0f\u8bed\u8a00\u5960\u5b9a\u4e86\u57fa\u7840", "conclusion": "\u8fd9\u4e9b\u6a21\u5f0f\u4e3a\u9ad8\u7b49\u6559\u80b2\u673a\u6784\u63d0\u4f9b\u4e86\u5b9e\u7528\u7684\u3001\u53ef\u64cd\u4f5c\u7684\u654f\u6377\u8f6f\u4ef6\u5f00\u53d1\u6559\u5b66\u6307\u5bfc\uff0c\u6709\u52a9\u4e8e\u6539\u8fdb\u76f8\u5173\u8bfe\u7a0b\u7684\u6559\u5b66\u6548\u679c"}}
{"id": "2510.03029", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.03029", "abs": "https://arxiv.org/abs/2510.03029", "authors": ["Debalina Ghosh Paul", "Hong Zhu", "Ian Bayley"], "title": "Investigating The Smells of LLM Generated Code", "comment": null, "summary": "Context: Large Language Models (LLMs) are increasingly being used to generate\nprogram code. Much research has been reported on the functional correctness of\ngenerated code, but there is far less on code quality.\n  Objectives: In this study, we propose a scenario-based method of evaluating\nthe quality of LLM-generated code to identify the weakest scenarios in which\nthe quality of LLM generated code should be improved.\n  Methods: The method measures code smells, an important indicator of code\nquality, and compares them with a baseline formed from reference solutions of\nprofessionally written code. The test dataset is divided into various subsets\naccording to the topics of the code and complexity of the coding tasks to\nrepresent different scenarios of using LLMs for code generation. We will also\npresent an automated test system for this purpose and report experiments with\nthe Java programs generated in response to prompts given to four\nstate-of-the-art LLMs: Gemini Pro, ChatGPT, Codex, and Falcon.\n  Results: We find that LLM-generated code has a higher incidence of code\nsmells compared to reference solutions. Falcon performed the least badly, with\na smell increase of 42.28%, followed by Gemini Pro (62.07%), ChatGPT (65.05%)\nand finally Codex (84.97%). The average smell increase across all LLMs was\n63.34%, comprising 73.35% for implementation smells and 21.42% for design\nsmells. We also found that the increase in code smells is greater for more\ncomplex coding tasks and for more advanced topics, such as those involving\nobject-orientated concepts.\n  Conclusion: In terms of code smells, LLM's performances on various coding\ntask complexities and topics are highly correlated to the quality of human\nwritten code in the corresponding scenarios. However, the quality of LLM\ngenerated code is noticeably poorer than human written code.", "AI": {"tldr": "LLM\u751f\u6210\u7684\u4ee3\u7801\u6bd4\u4e13\u4e1a\u7f16\u5199\u7684\u53c2\u8003\u4ee3\u7801\u5b58\u5728\u66f4\u591a\u7684\u4ee3\u7801\u5f02\u5473\uff0c\u5e73\u5747\u5f02\u5473\u589e\u52a063.34%\uff0c\u5176\u4e2dFalcon\u8868\u73b0\u6700\u597d(42.28%)\uff0cCodex\u6700\u5dee(84.97%)\u3002\u4ee3\u7801\u590d\u6742\u5ea6\u8d8a\u9ad8\uff0c\u5f02\u5473\u589e\u52a0\u8d8a\u660e\u663e\u3002", "motivation": "\u8bc4\u4f30LLM\u751f\u6210\u4ee3\u7801\u7684\u8d28\u91cf\u800c\u975e\u529f\u80fd\u6027\u6b63\u786e\u6027\uff0c\u8bc6\u522b\u4ee3\u7801\u8d28\u91cf\u6700\u5dee\u7684\u573a\u666f\u4ee5\u6307\u5bfc\u6539\u8fdb\u65b9\u5411", "method": "\u57fa\u4e8e\u573a\u666f\u7684\u8bc4\u4f30\u65b9\u6cd5\uff0c\u901a\u8fc7\u6d4b\u91cf\u4ee3\u7801\u5f02\u5473\u5e76\u4e0e\u4e13\u4e1a\u7f16\u5199\u7684\u53c2\u8003\u89e3\u51b3\u65b9\u6848\u5bf9\u6bd4\uff0c\u5c06\u6d4b\u8bd5\u6570\u636e\u96c6\u6309\u4ee3\u7801\u4e3b\u9898\u548c\u4efb\u52a1\u590d\u6742\u5ea6\u5212\u5206\u4e0d\u540c\u573a\u666f\uff0c\u4f7f\u7528\u81ea\u52a8\u5316\u6d4b\u8bd5\u7cfb\u7edf\u5bf9\u56db\u4e2a\u5148\u8fdbLLM\u751f\u6210\u7684Java\u7a0b\u5e8f\u8fdb\u884c\u8bc4\u4f30", "result": "LLM\u751f\u6210\u4ee3\u7801\u7684\u5f02\u5473\u53d1\u751f\u7387\u663e\u8457\u9ad8\u4e8e\u53c2\u8003\u4ee3\u7801\uff0c\u5e73\u5747\u5f02\u5473\u589e\u52a063.34%(\u5b9e\u73b0\u5f02\u547373.35%\uff0c\u8bbe\u8ba1\u5f02\u547321.42%)\u3002Falcon\u8868\u73b0\u6700\u4f73(42.28%)\uff0cCodex\u6700\u5dee(84.97%)\u3002\u590d\u6742\u4efb\u52a1\u548c\u9762\u5411\u5bf9\u8c61\u4e3b\u9898\u7684\u5f02\u5473\u589e\u52a0\u66f4\u660e\u663e", "conclusion": "LLM\u5728\u4e0d\u540c\u7f16\u7801\u4efb\u52a1\u590d\u6742\u5ea6\u548c\u4e3b\u9898\u4e0a\u7684\u8868\u73b0\u4e0e\u5bf9\u5e94\u573a\u666f\u4e0b\u4eba\u5de5\u7f16\u5199\u4ee3\u7801\u7684\u8d28\u91cf\u9ad8\u5ea6\u76f8\u5173\uff0c\u4f46LLM\u751f\u6210\u4ee3\u7801\u7684\u8d28\u91cf\u660e\u663e\u4f4e\u4e8e\u4eba\u5de5\u7f16\u5199\u4ee3\u7801"}}
{"id": "2510.03050", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.03050", "abs": "https://arxiv.org/abs/2510.03050", "authors": ["Rita Peixoto", "Filipe F. Correia", "Thatiane Rosa", "Eduardo Guerra", "Alfredo Goldman"], "title": "Refactoring Towards Microservices: Preparing the Ground for Service Extraction", "comment": "Accepted for publication in the EuroPLoP 2025 proceedings", "summary": "As organizations increasingly transition from monolithic systems to\nmicroservices, they aim to achieve higher availability, automatic scaling,\nsimplified infrastructure management, enhanced collaboration, and streamlined\ndeployments. However, this migration process remains largely manual and\nlabour-intensive. While existing literature offers various strategies for\ndecomposing monoliths, these approaches primarily focus on architecture-level\nguidance, often overlooking the code-level challenges and dependencies that\ndevelopers must address during the migration. This article introduces a\ncatalogue of seven refactorings specifically designed to support the transition\nto a microservices architecture with a focus on handling dependencies. The\ncatalogue provides developers with a systematic guide that consolidates\nrefactorings identified in the literature and addresses the critical gap in\nsystematizing the process at the code level. By offering a structured,\nstep-by-step approach, this work simplifies the migration process and lays the\ngroundwork for its potential automation, empowering developers to implement\nthese changes efficiently and effectively.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u5305\u542b7\u79cd\u91cd\u6784\u6280\u672f\u7684\u76ee\u5f55\uff0c\u4e13\u95e8\u7528\u4e8e\u652f\u6301\u5411\u5fae\u670d\u52a1\u67b6\u6784\u7684\u8fc1\u79fb\uff0c\u91cd\u70b9\u5173\u6ce8\u4ee3\u7801\u7ea7\u4f9d\u8d56\u5173\u7cfb\u7684\u5904\u7406\u3002", "motivation": "\u968f\u7740\u7ec4\u7ec7\u4ece\u5355\u4f53\u7cfb\u7edf\u5411\u5fae\u670d\u52a1\u8fc7\u6e21\uff0c\u8fc1\u79fb\u8fc7\u7a0b\u4ecd\u7136\u4e3b\u8981\u4f9d\u8d56\u624b\u52a8\u4e14\u52b3\u52a8\u5bc6\u96c6\u578b\u5de5\u4f5c\u3002\u73b0\u6709\u6587\u732e\u4e3b\u8981\u63d0\u4f9b\u67b6\u6784\u7ea7\u6307\u5bfc\uff0c\u5ffd\u7565\u4e86\u4ee3\u7801\u7ea7\u6311\u6218\u548c\u4f9d\u8d56\u5173\u7cfb\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u5305\u542b\u4e03\u79cd\u91cd\u6784\u6280\u672f\u7684\u7cfb\u7edf\u5316\u76ee\u5f55\uff0c\u8fd9\u4e9b\u91cd\u6784\u6280\u672f\u4ece\u6587\u732e\u4e2d\u8bc6\u522b\u5e76\u6574\u5408\u800c\u6210\uff0c\u63d0\u4f9b\u4e86\u7ed3\u6784\u5316\u7684\u9010\u6b65\u8fc1\u79fb\u65b9\u6cd5\u3002", "result": "\u8be5\u76ee\u5f55\u4e3a\u5f00\u53d1\u8005\u63d0\u4f9b\u4e86\u7cfb\u7edf\u5316\u6307\u5357\uff0c\u7b80\u5316\u4e86\u8fc1\u79fb\u8fc7\u7a0b\uff0c\u5e76\u4e3a\u6f5c\u5728\u7684\u81ea\u52a8\u5316\u5960\u5b9a\u4e86\u57fa\u7840\u3002", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u901a\u8fc7\u63d0\u4f9b\u4ee3\u7801\u7ea7\u7684\u7cfb\u7edf\u5316\u91cd\u6784\u65b9\u6cd5\uff0c\u4f7f\u5f00\u53d1\u8005\u80fd\u591f\u66f4\u9ad8\u6548\u6709\u6548\u5730\u5b9e\u65bd\u5fae\u670d\u52a1\u8fc1\u79fb\u3002"}}
{"id": "2510.03071", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.03071", "abs": "https://arxiv.org/abs/2510.03071", "authors": ["Facundo Molina", "Nazareno Aguirre", "Alessandra Gorla"], "title": "State Field Coverage: A Metric for Oracle Quality", "comment": null, "summary": "The effectiveness of testing in uncovering software defects depends not only\non the characteristics of the test inputs and how thoroughly they exercise the\nsoftware, but also on the quality of the oracles used to determine whether the\nsoftware behaves as expected. Therefore, assessing the quality of oracles is\ncrucial to improve the overall effectiveness of the testing process. Existing\nmetrics have been used for this purpose, but they either fail to provide a\ncomprehensive basis for guiding oracle improvement, or they are tailored to\nspecific types of oracles, thus limiting their generality.\n  In this paper, we introduce state field coverage, a novel metric for\nassessing oracle quality. This metric measures the proportion of an object's\nstate, as statically defined by its class fields, that an oracle may access\nduring test execution. The main intuition of our metric is that oracles with a\nhigher state field coverage are more likely to detect faults in the software\nunder analysis, as they inspect a larger portion of the object states to\ndetermine whether tests pass or not.\n  We implement a mechanism to statically compute the state field coverage\nmetric. Being statically computed, the metric is efficient and provides direct\nguidance for improving test oracles by identifying state fields that remain\nunexamined. We evaluate state field coverage through experiments involving 273\nrepresentation invariants and 249,027 test assertions. The results show that\nstate field coverage is a well-suited metric for assessing oracle quality, as\nit strongly correlates with the oracles' fault-detection ability, measured by\nmutation score.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u6d4b\u8bd5\u9884\u8a00\u8d28\u91cf\u8bc4\u4f30\u6307\u6807\u2014\u2014\u72b6\u6001\u5b57\u6bb5\u8986\u76d6\u7387\uff0c\u8be5\u6307\u6807\u901a\u8fc7\u9759\u6001\u5206\u6790\u6d4b\u91cf\u6d4b\u8bd5\u9884\u8a00\u5728\u6267\u884c\u671f\u95f4\u53ef\u80fd\u8bbf\u95ee\u7684\u5bf9\u8c61\u72b6\u6001\u5b57\u6bb5\u6bd4\u4f8b\uff0c\u80fd\u591f\u6709\u6548\u6307\u5bfc\u9884\u8a00\u6539\u8fdb\u5e76\u4e0e\u6545\u969c\u68c0\u6d4b\u80fd\u529b\u5f3a\u76f8\u5173\u3002", "motivation": "\u73b0\u6709\u6d4b\u8bd5\u9884\u8a00\u8d28\u91cf\u8bc4\u4f30\u6307\u6807\u8981\u4e48\u65e0\u6cd5\u5168\u9762\u6307\u5bfc\u9884\u8a00\u6539\u8fdb\uff0c\u8981\u4e48\u5c40\u9650\u4e8e\u7279\u5b9a\u9884\u8a00\u7c7b\u578b\uff0c\u7f3a\u4e4f\u901a\u7528\u6027\u3002\u9700\u8981\u4e00\u79cd\u80fd\u591f\u5168\u9762\u8bc4\u4f30\u9884\u8a00\u8d28\u91cf\u5e76\u6307\u5bfc\u6539\u8fdb\u7684\u901a\u7528\u6307\u6807\u3002", "method": "\u63d0\u51fa\u72b6\u6001\u5b57\u6bb5\u8986\u76d6\u7387\u6307\u6807\uff0c\u901a\u8fc7\u9759\u6001\u8ba1\u7b97\u6d4b\u8bd5\u9884\u8a00\u5728\u6267\u884c\u671f\u95f4\u53ef\u80fd\u8bbf\u95ee\u7684\u5bf9\u8c61\u72b6\u6001\u5b57\u6bb5\u6bd4\u4f8b\u3002\u5b9e\u73b0\u9759\u6001\u8ba1\u7b97\u673a\u5236\uff0c\u9ad8\u6548\u8bc6\u522b\u672a\u68c0\u67e5\u7684\u72b6\u6001\u5b57\u6bb5\u3002", "result": "\u5728\u5305\u542b273\u4e2a\u8868\u793a\u4e0d\u53d8\u91cf\u548c249,027\u4e2a\u6d4b\u8bd5\u65ad\u8a00\u7684\u5b9e\u9a8c\u4e2d\uff0c\u72b6\u6001\u5b57\u6bb5\u8986\u76d6\u7387\u4e0e\u57fa\u4e8e\u53d8\u5f02\u8bc4\u5206\u7684\u6545\u969c\u68c0\u6d4b\u80fd\u529b\u5f3a\u76f8\u5173\uff0c\u8bc1\u660e\u5176\u9002\u5408\u8bc4\u4f30\u9884\u8a00\u8d28\u91cf\u3002", "conclusion": "\u72b6\u6001\u5b57\u6bb5\u8986\u76d6\u7387\u662f\u4e00\u79cd\u6709\u6548\u7684\u6d4b\u8bd5\u9884\u8a00\u8d28\u91cf\u8bc4\u4f30\u6307\u6807\uff0c\u80fd\u591f\u6307\u5bfc\u9884\u8a00\u6539\u8fdb\u5e76\u63d0\u9ad8\u6574\u4f53\u6d4b\u8bd5\u6548\u679c\u3002"}}
{"id": "2510.03178", "categories": ["cs.SE", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.03178", "abs": "https://arxiv.org/abs/2510.03178", "authors": ["Cuong Chi Le", "Minh V. T. Pham", "Cuong Duc Van", "Hoang N. Phan", "Huy N. Phan", "Tien N. Nguyen"], "title": "When Names Disappear: Revealing What LLMs Actually Understand About Code", "comment": null, "summary": "Large Language Models (LLMs) achieve strong results on code tasks, but how\nthey derive program meaning remains unclear. We argue that code communicates\nthrough two channels: structural semantics, which define formal behavior, and\nhuman-interpretable naming, which conveys intent. Removing the naming channel\nseverely degrades intent-level tasks such as summarization, where models\nregress to line-by-line descriptions. Surprisingly, we also observe consistent\nreductions on execution tasks that should depend only on structure, revealing\nthat current benchmarks reward memorization of naming patterns rather than\ngenuine semantic reasoning. To disentangle these effects, we introduce a suite\nof semantics-preserving obfuscations and show that they expose identifier\nleakage across both summarization and execution. Building on these insights, we\nrelease ClassEval-Obf, an obfuscation-enhanced benchmark that systematically\nsuppresses naming cues while preserving behavior. Our results demonstrate that\nClassEval-Obf reduces inflated performance gaps, weakens memorization\nshortcuts, and provides a more reliable basis for assessing LLMs' code\nunderstanding and generalization.", "AI": {"tldr": "\u8bba\u6587\u53d1\u73b0LLMs\u5728\u4ee3\u7801\u4efb\u52a1\u4e2d\u8fc7\u5ea6\u4f9d\u8d56\u547d\u540d\u6a21\u5f0f\u800c\u975e\u771f\u6b63\u7684\u8bed\u4e49\u7406\u89e3\uff0c\u901a\u8fc7\u8bed\u4e49\u4fdd\u7559\u7684\u6df7\u6dc6\u6280\u672f\u63ed\u793a\u4e86\u6807\u8bc6\u7b26\u6cc4\u6f0f\u95ee\u9898\uff0c\u5e76\u63d0\u51fa\u4e86ClassEval-Obf\u57fa\u51c6\u6765\u66f4\u53ef\u9760\u5730\u8bc4\u4f30\u4ee3\u7801\u7406\u89e3\u80fd\u529b", "motivation": "\u63a2\u7a76\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5982\u4f55\u7406\u89e3\u4ee3\u7801\u542b\u4e49\uff0c\u53d1\u73b0\u73b0\u6709\u57fa\u51c6\u6d4b\u8bd5\u53ef\u80fd\u5956\u52b1\u547d\u540d\u6a21\u5f0f\u7684\u8bb0\u5fc6\u800c\u975e\u771f\u6b63\u7684\u8bed\u4e49\u63a8\u7406\uff0c\u9700\u8981\u66f4\u53ef\u9760\u7684\u8bc4\u4f30\u65b9\u6cd5", "method": "\u5f15\u5165\u8bed\u4e49\u4fdd\u7559\u7684\u6df7\u6dc6\u6280\u672f\u6765\u6d88\u9664\u547d\u540d\u7ebf\u7d22\uff0c\u521b\u5efaClassEval-Obf\u57fa\u51c6\u7cfb\u7edf\u6027\u5730\u6291\u5236\u547d\u540d\u63d0\u793a\u540c\u65f6\u4fdd\u6301\u4ee3\u7801\u884c\u4e3a", "result": "\u79fb\u9664\u547d\u540d\u901a\u9053\u4e25\u91cd\u964d\u4f4e\u4e86\u610f\u56fe\u7ea7\u4efb\u52a1\u6027\u80fd\uff0c\u5728\u6267\u884c\u4efb\u52a1\u4e2d\u4e5f\u89c2\u5bdf\u5230\u4e00\u81f4\u6027\u80fd\u4e0b\u964d\uff0c\u6df7\u6dc6\u6280\u672f\u66b4\u9732\u4e86\u6807\u8bc6\u7b26\u6cc4\u6f0f\u95ee\u9898", "conclusion": "ClassEval-Obf\u57fa\u51c6\u51cf\u5c11\u4e86\u6027\u80fd\u81a8\u80c0\u5dee\u8ddd\uff0c\u524a\u5f31\u4e86\u8bb0\u5fc6\u6377\u5f84\uff0c\u4e3a\u8bc4\u4f30LLMs\u7684\u4ee3\u7801\u7406\u89e3\u548c\u6cdb\u5316\u80fd\u529b\u63d0\u4f9b\u4e86\u66f4\u53ef\u9760\u7684\u57fa\u7840"}}
{"id": "2510.03217", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.03217", "abs": "https://arxiv.org/abs/2510.03217", "authors": ["Jos\u00e9 Cambronero", "Michele Tufano", "Sherry Shi", "Renyao Wei", "Grant Uy", "Runxiang Cheng", "Chin-Jung Liu", "Shiying Pan", "Satish Chandra", "Pat Rondon"], "title": "Abstain and Validate: A Dual-LLM Policy for Reducing Noise in Agentic Program Repair", "comment": null, "summary": "Agentic Automated Program Repair (APR) is increasingly tackling complex,\nrepository-level bugs in industry, but ultimately agent-generated patches still\nneed to be reviewed by a human before committing them to ensure they address\nthe bug. Showing unlikely patches to developers can lead to substantial noise,\nwasting valuable developer time and eroding trust in automated code changes. We\nintroduce two complementary LLM-based policies to reduce such noise: bug\nabstention and patch validation policies. Bug abstention excludes bugs that the\nagentic APR system is unlikely to fix. Patch validation rejects patches that\nare unlikely to be a good fix for the given bug. We evaluate both policies on\nthree sets of bugs from Google's codebase, and their candidate patches\ngenerated by an internal agentic APR system. On a set of 174 human-reported\nbugs, removing bugs and patch trajectories rejected by our policies can raise\nsuccess rates by up to 13 percentage points and 15 percentage points,\nrespectively, and by up to 39 percentage points in combination. On null pointer\nexceptions and sanitizer-reported bugs with machine-generated bug reports,\npatch validation also improves average single-sample success rates. This\ntwo-policy approach provides a practical path to the reliable, industrial-scale\ndeployment of agentic APR systems.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e24\u79cdLLM\u7b56\u7565\uff08bug abstention\u548cpatch validation\uff09\u6765\u51cf\u5c11\u81ea\u52a8\u5316\u7a0b\u5e8f\u4fee\u590d\u7cfb\u7edf\u4e2d\u7684\u566a\u58f0\uff0c\u901a\u8fc7\u6392\u9664\u96be\u4ee5\u4fee\u590d\u7684bug\u548c\u9a8c\u8bc1\u8865\u4e01\u8d28\u91cf\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u4fee\u590d\u6210\u529f\u7387", "motivation": "\u81ea\u52a8\u5316\u7a0b\u5e8f\u4fee\u590d\u7cfb\u7edf\u5728\u5904\u7406\u590d\u6742bug\u65f6\u4f1a\u4ea7\u751f\u5927\u91cf\u9700\u8981\u4eba\u5de5\u5ba1\u6838\u7684\u4f4e\u8d28\u91cf\u8865\u4e01\uff0c\u8fd9\u6d6a\u8d39\u5f00\u53d1\u8005\u65f6\u95f4\u5e76\u964d\u4f4e\u5bf9\u81ea\u52a8\u5316\u4ee3\u7801\u53d8\u66f4\u7684\u4fe1\u4efb", "method": "\u5f15\u5165\u4e24\u79cd\u4e92\u8865\u7684LLM\u7b56\u7565\uff1abug abstention\u7b56\u7565\u6392\u9664\u7cfb\u7edf\u96be\u4ee5\u4fee\u590d\u7684bug\uff0cpatch validation\u7b56\u7565\u62d2\u7edd\u4e0d\u592a\u53ef\u80fd\u662f\u826f\u597d\u4fee\u590d\u7684\u8865\u4e01", "result": "\u5728Google\u4ee3\u7801\u5e93\u7684174\u4e2a\u4eba\u5de5\u62a5\u544abug\u4e0a\uff0c\u4e24\u79cd\u7b56\u7565\u7ec4\u5408\u4f7f\u7528\u53ef\u5c06\u6210\u529f\u7387\u63d0\u9ad839\u4e2a\u767e\u5206\u70b9\uff1b\u5728\u7a7a\u6307\u9488\u5f02\u5e38\u548csanitizer\u62a5\u544abug\u4e0a\u4e5f\u63d0\u9ad8\u4e86\u5e73\u5747\u5355\u6837\u672c\u6210\u529f\u7387", "conclusion": "\u8fd9\u79cd\u53cc\u7b56\u7565\u65b9\u6cd5\u4e3aagentic APR\u7cfb\u7edf\u5728\u5de5\u4e1a\u89c4\u6a21\u4e0a\u7684\u53ef\u9760\u90e8\u7f72\u63d0\u4f9b\u4e86\u5b9e\u7528\u8def\u5f84"}}
