<div id=toc></div>

# Table of Contents

- [cs.MA](#cs.MA) [Total: 1]
- [cs.NI](#cs.NI) [Total: 1]
- [cs.DC](#cs.DC) [Total: 4]
- [cs.SE](#cs.SE) [Total: 3]
- [cs.AR](#cs.AR) [Total: 1]


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [1] [Pancake: Hierarchical Memory System for Multi-Agent LLM Serving](https://arxiv.org/abs/2602.21477)
*Zhengding Hu,Zaifeng Pan,Prabhleen Kaur,Vibha Murthy,Zhongkai Yu,Yue Guan,Zhen Wang,Steven Swanson,Yufei Ding*

Main category: cs.MA

TL;DR: 该论文介绍了Pancake，一种针对LLM服务中智能体内存管理的新型多层级内存系统，通过多级索引缓存、跨智能体协调索引管理和GPU-CPU协同加速技术，解决了大规模存储、频繁更新和多智能体共存带来的复杂近似最近邻搜索问题。


<details>
  <summary>Details</summary>
Motivation: LLM服务中的智能体内存管理面临大规模存储、频繁更新和多智能体共存带来的复杂且高成本的近似最近邻(ANN)搜索挑战。

Method: 提出Pancake多层级智能体内存系统，整合三项关键技术：单智能体的多级索引缓存、跨智能体的协调索引管理以及协同GPU-CPU加速。

Result: 在真实智能体工作负载实验中，Pancake显著超越现有框架，实现了超过4.29倍的端到端吞吐量提升。

Conclusion: Pancake为多智能体LLM服务提供了高效内存管理解决方案，易于集成到现有智能体框架中，为构建大规模智能体系统提供了技术基础。

Abstract: In this work, we identify and address the core challenges of agentic memory management in LLM serving, where large-scale storage, frequent updates, and multiple coexisting agents jointly introduce complex and high-cost approximate nearest neighbor (ANN) searching problems. We present Pancake, a multi-tier agentic memory system that unifies three key techniques: (i) multi-level index caching for single agents, (ii) coordinated index management across multiple agents, and (iii) collaborative GPU-CPU acceleration. Pancake exposes easy-to-use interface that can be integrated into memory-based agents like Mem-GPT, and is compatible with agentic frameworks such as LangChain and LlamaIndex. Experiments on realistic agent workloads show that Pancake substantially outperforms existing frameworks, achieving more than 4.29x end-to-end throughput improvement.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [2] [UnlinkableDFL: a Practical Mixnet Protocol for Churn-Tolerant Decentralized FL Model Sharing](https://arxiv.org/abs/2602.21343)
*Chao Feng,Thomas Grubl,Jan von der Assen,Sandrin Raphael Hunkeler,Linn Anna Spitz,Gerome Bovet,Burkhard Stiller*

Main category: cs.NI

TL;DR: UnlinkableDFL是一个结合了基于对等方的混合网和基于片段的模型聚合的去中心化联邦学习框架，通过将模型更新分割为加密片段并通过多跳路径发送，确保了去中心化设置中的不可链接性，同时保持与标准FedAvg相似的收敛性能。


<details>
  <summary>Details</summary>
Motivation: 去中心化联邦学习(DFL)虽然消除了对中心聚合器的需求，但会暴露通信模式，从而可能揭示参与者的身份。

Method: UnlinkableDFL框架将模型更新分割为加密片段，通过独立的多跳路径发送，并在不使用任何身份信息的情况下进行聚合，结合了基于对等方的混合网和基于片段的模型聚合技术。

Result: 理论分析表明，更大的混合集和更长的路径提高了中继和端到端的不可链接性，同时收敛性类似于标准FedAvg。原型评估显示UnlinkableDFL可靠收敛并能适应节点变化，通信延迟是主要开销，而内存和CPU使用保持适中。

Conclusion: 研究结果说明了匿名性和系统效率之间的平衡，证明了在去中心化学习工作流程中可以维持强大的不可链接性。

Abstract: Decentralized Federated Learning (DFL) eliminates the need for a central aggregator, but it can expose communication patterns that reveal participant identities. This work presents UnlinkableDFL, a DFL framework that combines a peer-based mixnet with fragment-based model aggregation to ensure unlinkability in fully decentralized settings. Model updates are divided into encrypted fragments, sent over separate multi-hop paths, and aggregated without using any identity information. A theoretical analysis indicates that relay and end-to-end unlinkability improve with larger mixing sets and longer paths, while convergence remains similar to standard FedAvg. A prototype implementation evaluates learning performance, latency, unlinkability, and resource usage. The results show that UnlinkableDFL converges reliably and adapts to node churn. Communication latency emerges as the main overhead, while memory and CPU usage stay moderate. These findings illustrate the balance between anonymity and system efficiency, demonstrating that strong unlinkability can be maintained in decentralized learning workflows.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [3] [General Convex Agreement with Near-Optimal Communication](https://arxiv.org/abs/2602.21411)
*Marc Dufay,Diana Ghinea,Anton Paramonov*

Main category: cs.DC

TL;DR: 本文研究了凸共识(Convex Agreement, CA)问题，提出了一种确定性同步协议，在一般凸性空间中实现了接近最优的通信复杂度。当输入长度L足够大时，有限凸性空间的通信复杂度为O(L·n log n)，欧几里得空间的复杂度为O(L·n^{1+o(1)})。该协议具有渐进最优的轮复杂度O(n)，并且具有良好的容错能力。


<details>
  <summary>Details</summary>
Motivation: 凸共识是对拜占庭共识(Byzantine Agreement, BA)的加强，要求协商的输出值位于诚实方输入的凸包内。这一有效性条件对于实际聚合任务(如鲁棒学习或传感器融合)很重要，其中诚实输入不需要一致但应约束决策。标准CA方法的通信复杂度为Θ(Ln²)，与BA的Ω(Ln)下界存在差距，且最新成果尚未扩展到一般凸性空间。

Method: 作者利用提取器图(extractor graphs)来实现对参与方到委员会的确定性分配，这种方法对自适应对手具有鲁棒性。通过这种方法，他们设计了适用于一般凸性空间的确定性同步CA协议，克服了通信复杂度的挑战。

Result: 论文实现了以下成果：1) 当L = Ω(n·κ)时，有限凸性空间的通信复杂度为O(L·n log n)，欧几里得空间的复杂度为O(L·n^{1+o(1)})；2) 协议具有渐进最优的轮复杂度O(n)；3) 固定L时达到接近最优的容错性t < n/(ω+ε)；4) L未知时仍能达到t < n/(ω+ε+1)的容错性；5) 可有效用于并行拜占庭广播问题。

Conclusion: 该工作填补了凸共识在一般凸性空间中的通信复杂度空白，提供了具有良好通信效率、轮复杂度和容错性的确定性协议。主要技术贡献是使用提取器图实现确定性委员会分配，对自适应对手具有鲁棒性。这些成果对于鲁棒机器学习、传感器融合等实际应用具有重要意义。

Abstract: Convex Agreement (CA) strengthens Byzantine Agreement (BA) by requiring the output agreed upon to lie in the convex hull of the honest parties' inputs. This validity condition is motivated by practical aggregation tasks (e.g., robust learning or sensor fusion) where honest inputs need not coincide but should still constrain the decision. CA inherits BA lower bounds, and optimal synchronous round complexity is easy to obtain (e.g., via Byzantine Broadcast). The main challenge is \emph{communication}: standard approaches for CA have a communication complexity of $Θ(Ln^2)$ for large $L$-bit inputs, leaving a gap in contrast to BA's lower bound of $Ω(Ln)$ bits. While recent work achieves optimal communication complexity of $O(Ln)$ for sufficiently large $L$ [GLW,PODC'25], translating this result to general convexity spaces remained an open problem.
  We investigate this gap for abstract convexity spaces, and we present deterministic synchronous CA protocols with near-optimal communication complexity: when $L = Ω(n \cdot κ)$, where $κ$ is a security parameter, we achieve $O(L\cdot n\log n)$ communication for finite convexity spaces and $O(L\cdot n^{1+o(1)})$ communication for Euclidean spaces $\mathbb{R}^d$. Our protocols have asymptotically optimal round complexity $O(n)$ and, when a bound on the inputs' lengths $L$ is fixed a priori, we achieve near-optimal resilience $t < n/(ω+\varepsilon)$ for any constant $\varepsilon>0$, where $ω$ is the Helly number of the convexity space. If $L$ is unknown, we still achieve resilience $t<n/(ω+\varepsilon+1)$ for any constant $\varepsilon > 0$. We further note that our protocols can be leveraged to efficiently solve parallel BA.
  Our main technical contribution is the use of extractor graphs to obtain a deterministic assignment of parties to committees, which is resilient against adaptive adversaries.

</details>


### [4] [Multi-Layer Scheduling for MoE-Based LLM Reasoning](https://arxiv.org/abs/2602.21626)
*Yifan Sun,Gholamreza Haffar,Minxian Xu,Rajkumar Buyya,Adel N. Toosi*

Main category: cs.DC

TL;DR: 提出针对MoE-based LLM服务的多层次调度框架，在请求、引擎和专家三个层面优化调度策略，显著降低延迟并提高吞吐量。


<details>
  <summary>Details</summary>
Motivation: 现有LLM推理框架使用简单调度策略无法充分利用系统资源，存在head-of-line阻塞和负载不均衡问题；MoE模型引入了专家并行和路由复杂性等新挑战。

Method: 构建三层调度框架：请求层采用SJF和优先级感知老化算法；引擎层设计负载感知调度策略，考虑前缀token负载、KV缓存利用率和用户粘性；专家层缓解专家热点并 strategically 放置层间专家依赖关系。

Result: 在100多次不同工作负载分布的实验中，显著优于vLLM，TTFT延迟降低最高达17.8%，TPOT延迟降低最高达13.3%。

Conclusion: 多层次调度框架能有效解决MoE-based LLM服务中的资源利用和延迟问题，为大规模LLM部署提供了高效解决方案。

Abstract: Large Language Models (LLMs) have achieved remarkable success across a wide range of tasks, but serving them efficiently at scale remains a critical challenge due to their substantial computational and latency demands. While most existing inference frameworks rely on simple scheduling strategies such as First-Come-First-Serve (FCFS) at the engine level and Round-Robin (RR) at the scheduler or coordinator level, they often fail to fully utilize system resources and may suffer from issues such as head-of-line blocking and load imbalance. Recent advances in Mixture-of-Experts (MoE) models have also introduced new challenges in scheduling arising from expert parallelism and routing complexity. This research proposes a multi-layer scheduling framework tailored for MoE-based LLM serving. It targets scheduling at three levels: request-level, enginelevel, and expert-level. At the request level, we explore algorithms such as Shortest-Job-First (SJF) and priority-aware aging to improve throughput and reduce latency. At the engine level, we design load-aware dispatching strategies that account for the current prefix token load, KV cache utilization, and user stickiness to achieve better resource matching. At the expert level, we focus on alleviating expert hotspots and strategically placing inter-layer expert dependencies to balance load and improve routing efficiency. Extensive experimental results from more than 100 experiments conducted under diverse workload distributions show that our approach consistently outperforms the state-of-theart inference framework vLLM, achieving up to 17.8% reduction in Time To First Token (TTFT) latency and 13.3% reduction in Time-Per-Output-Token (TPOT) latency.

</details>


### [5] [DHP: Efficient Scaling of MLLM Training with Dynamic Hybrid Parallelism](https://arxiv.org/abs/2602.21788)
*Yifan Niu,Han Xiao,Dongyi Liu,Wei Zhou,Jia Li*

Main category: cs.DC

TL;DR: 本文提出了一种动态混合并行策略(DHP)，能够自适应地重新配置通信组和并行度，解决多模态大模型在异构数据集训练中的负载均衡和硬件利用率问题。


<details>
  <summary>Details</summary>
Motivation: 现有静态并行策略在处理真实世界多模态数据的极端异构性时，存在严重的负载不平衡、冗余通信和次优硬件利用问题。

Method: DHP推广了非2的幂次并行度，开发了多项式时间算法生成近优并行策略，每个训练批次仅毫秒级开销，在训练过程中自适应地重新配置通信组。

Result: DHP显著优于Megatron-LM和DeepSpeed，在NPU集群上实现高达1.36倍训练吞吐量提升，同时保持近线性扩展效率。

Conclusion: DHP通过动态适应并行策略有效解决了多模态大模型在异构数据上的训练挑战，提高了硬件利用率和训练效率。

Abstract: Scaling long-context capabilities is crucial for Multimodal Large Language Models (MLLMs). However, real-world multimodal datasets are extremely heterogeneous. Existing training frameworks predominantly rely on static parallelism strategies, which suffer from severe load imbalance, redundant communication, and suboptimal hardware utilization under data heterogeneity. In this work, we propose Dynamic Hybrid Parallelism (DHP), an efficient parallelism strategy that adaptively reconfigures communication groups and parallelism degrees during MLLM training. We generalize the non-power-of-two parallelism degrees and develop a polynomial-time algorithm to generate near-optimal parallelism strategies with only millisecond-level overhead per training batch. DHP is able to maintain high hardware efficiency even under extreme data variability. Experimental results demonstrate that DHP significantly outperforms Megatron-LM and DeepSpeed, achieving up to 1.36 $\times$ speedup in training throughput while maintaining near-linear scaling efficiency across large-scale NPU clusters.

</details>


### [6] [LLMTailor: A Layer-wise Tailoring Tool for Efficient Checkpointing of Large Language Models](https://arxiv.org/abs/2602.22158)
*Minqiu Sun,Xin Huang,Luanzheng Guo,Nathan R. Tallent,Kento Sato,Dong Dai*

Main category: cs.DC

TL;DR: LLMTailor是一个检查点合并框架，通过选择性地仅检查点有显著更新的层，大幅减少大型语言模型训练中的检查点大小和时间，同时保持模型质量。


<details>
  <summary>Details</summary>
Motivation: 现有检查点方法无论I/O策略如何，都需要周期性存储整个模型和优化器状态，导致巨大的存储开销和资源竞争。研究表明LLM各层更新高度不均匀，这为选择性检查点提供了可能性。

Method: LLMTailor是一个检查点合并框架，能够过滤和组装来自不同检查点的层，形成复合检查点。它提供了对权重和优化器状态的细粒度控制，支持选择性检查点策略。

Result: 评估显示LLMTailor可与不同选择性检查点策略配合使用，有效减少检查点大小（例如Llama3.1-8B减少4.3倍）和检查点时间（例如Qwen2.5-7B加速2.8倍），同时保持模型质量。

Conclusion: LLMTailor解决了现有检查点方法在大型语言模型训练中的存储和效率问题，通过选择性检查点显著降低了开销，为更高效、更具成本效益的LLM训练提供了实用解决方案。

Abstract: Checkpointing is essential for fault tolerance in training large language models (LLMs). However, existing methods, regardless of their I/O strategies, periodically store the entire model and optimizer states, incurring substantial storage overhead and resource contention. Recent studies reveal that updates across LLM layers are highly non-uniform. Across training steps, some layers may undergo more significant changes, while others remain relatively stable or even unchanged. This suggests that selectively checkpointing only layers with significant updates could reduce overhead without harming training. Implementing such selective strategies requires fine-grained control over both weights and optimizer states, which no current tool provides. To address this gap, we propose \texttt{LLMTailor}, a checkpoint-merging framework that filters and assembles layers from different checkpoints to form a composite checkpoint. Our evaluation indicates that LLMTailor can work with different selective checkpointing strategies and effectively reduce checkpoint size (e.g., 4.3 times smaller for Llama3.1-8B) and checkpoint time (e.g., 2.8 times faster for Qwen2.5-7B) while maintaining model quality.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [7] [AgenticTyper: Automated Typing of Legacy Software Projects Using Agentic AI](https://arxiv.org/abs/2602.21251)
*Clemens Pohle*

Main category: cs.SE

TL;DR: AgenticTyper是一个基于大型语言模型的自动化系统，能够高效解决JavaScript代码库中的TypeScript类型错误，显著减少手动工作量。


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: AgenticTyper是一个基于大型语言模型的智能系统，通过迭代错误纠正和通过转译比较保持行为正确性，自动解决TypeScript类型问题。

Result: 在两个私有代码库(81K代码行)上评估显示，AgenticTyper在20分钟内解决了所有633个初始类型错误，将手动工作量从一个人工作日减少到20分钟。

Conclusion: AgenticTyper通过提供可扩展的类型安全解决方案，有效解决了自动化类型转换中的空白，显著减少了手动工作量，同时保持行为正确性。

Abstract: Legacy JavaScript systems lack type safety, making maintenance risky. While TypeScript can help, manually adding types is expensive. Previous automated typing research focuses on type inference but rarely addresses type checking setup, definition generation, bug identification, or behavioral correctness at repository scale. We present AgenticTyper, a Large Language Model (LLM)-based agentic system that addresses these gaps through iterative error correction and behavior preservation via transpilation comparison. Evaluation on two proprietary repositories (81K LOC) shows that AgenticTyper resolves all 633 initial type errors in 20 minutes, reducing manual effort from one working day.

</details>


### [8] [From Ad-Hoc Scripts to Orchestrated Pipelines: Architecting a Resilient ELT Framework for Developer Productivity Metrics](https://arxiv.org/abs/2602.21568)
*Yuvraj Agrawal,Pallav Jain*

Main category: cs.SE

TL;DR: 该论文报告了从传统调度系统迁移到基于DAG编排和Medallion架构的ELT管道的经验，解决了开发者生产力仪表板中的数据可靠性问题。


<details>
  <summary>Details</summary>
Motivation: 开发者生产力仪表板用于可视化DevOps性能指标，但经常因数据可靠性问题而效用降低。早期使用临时脚本(Cron jobs)导致

Method: 从传统调度迁移到健壮的提取-加载-转换(ELT)管道，使用有向无环图(DAG)编排和Medallion架构。解耦数据提取与转换，保持不可变的原始历史记录用于指标重新定义，并实施基于状态的依赖管理。

Result: 该迁移解决了数据可靠性问题，消除了

Conclusion: 将指标管道视为生产级分布式系统是可持续工程分析的前提条件，需要架构上的重视和系统性方法。

Abstract: Developer Productivity Dashboards are essential for visualizing DevOps performance metrics such as Deployment Frequency and Change Failure Rate (DORA). However, the utility of these dashboards is frequently undermined by data reliability issues. In early iterations of our platform, ad-hoc ingestion scripts (Cron jobs) led to "silent failures," where data gaps went undetected for days, eroding organizational trust. This paper reports on our experience migrating from legacy scheduling to a robust Extract-Load-Transform (ELT) pipeline using Directed Acyclic Graph (DAG) orchestration and Medallion Architecture. We detail the operational benefits of decoupling data extraction from transformation, the necessity of immutable raw history for metric redefinition, and the implementation of state-based dependency management. Our experience suggests that treating the metrics pipeline as a production-grade distributed system is a prerequisite for sustainable engineering analytics.

</details>


### [9] [Uncertainty Modeling for SysML v2](https://arxiv.org/abs/2602.21641)
*Man Zhang,Yunyang Li,Tao Yue*

Main category: cs.SE

TL;DR: 本文提出了一种将PSUM元模型整合到SysML v2中的系统化扩展方法，通过七个案例研究验证了该扩展在不确定感知的MBSE中的适用性，实现了不确定性源明确指定、不确定性结构化表征和系统模型中不确定性的一致传播。


<details>
  <summary>Details</summary>
Motivation: 现代工程系统在动态和部分可观察环境中运行存在固有不确定性，而新一代系统建模语言SysML v2缺乏与PSUM标准对齐的一流不确定性表示构造。

Method: 提出对SysML v2的系统化扩展，将PSUM元模型整合到其建模框架中，同时保持与SysML v2语法和语义的一致性。

Result: 通过七个案例研究验证，结果表明PSUM-SysMLv2扩展在不确定感知的MBSE中具有表达性和适用性，并可能支持不确定性及传播分析。

Conclusion: 所提出的扩展成功地将不确定性建模能力整合到SysML v2中，增强了该语言在不确定环境中系统工程的实用性。

Abstract: Uncertainty is inherent in modern engineered systems, including cyber-physical systems, autonomous systems, and large-scale software-intensive infrastructures (such as microservice-based systems) operating in dynamic and partially observable environments. The recent publication of Precise Semantics for Uncertainty Modeling (PSUM) by the Object Management Group represents the first standardized specification for uncertainty modeling within the Model-Based Systems Engineering (MBSE) community, providing formally defined semantics for representing and reasoning about uncertainty in models. In parallel, the second version of Systems Modeling Language (SysML v2) was released as the next-generation systems modeling language, offering improved semantic rigor and reusability, yet lacking native constructs aligned with PSUM for first-class uncertainty representation. This paper proposes a systematic extension of SysML v2 that incorporates the PSUM metamodel into its modeling framework. The extension enables explicit specification of indeterminacy sources, structured characterization of uncertainties, and consistent propagation of uncertainty within system models, while preserving conformance with SysML v2 syntax and semantics. We validate the approach through seven case studies. Results demonstrate that the proposed extension (PSUM-SysMLv2) is expressive and applicable for uncertainty-aware MBSE, and potentially enables uncertainty and uncertainty propagation analyses.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [10] [Heterogeneous Memory Design Exploration for AI Accelerators with a Gain Cell Memory Compiler](https://arxiv.org/abs/2602.21278)
*Xinxin Wang,Lixian Yan,Shuhan Liu,Luke Upton,Zhuoqi Cai,Yiming Tan,Shengman Li,Koustav Jana,Peijing Li,Jesse Cirimelli-Low,Thierry Tambe,Matthew Guthaus,H. -S. Philip Wong*

Main category: cs.AR

TL;DR: 本文介绍了一种名为OpenGCRAM的编译器工具，支持SRAM和GCRAM两种存储技术，能够为商用CMOS工艺生成宏级设计和布局，帮助识别针对AI任务的最优异构存储配置。


<details>
  <summary>Details</summary>
Motivation: 随着存储在系统成本和能耗中占比日益增加，结合互补特性技术的异构片上存储系统变得至关重要。GCRAM（增益单元RAM）相比传统SRAM具有更高密度、更低功耗和可调节保持时间的优势，能够扩展设计空间。

Method: 开发OpenGCRAM编译器，支持SRAM和GCRAM两种技术，能够生成商用CMOS工艺的宏级设计和布局，并针对用户定义的配置进行面积、延迟和功耗的表征。

Result: 该工具能够根据指定的性能指标，系统性地识别针对AI任务的最优异构存储配置，为存储子系统设计提供灵活选择。

Conclusion: OpenGCRAM编译器为利用GCRAM技术优化存储子系统提供了完整工具链，有助于降低系统成本和能耗，特别适用于对存储有特殊要求的AI应用。

Abstract: As memory increasingly dominates system cost and energy, heterogeneous on-chip memory systems that combine technologies with complementary characteristics are becoming essential. Gain Cell RAM (GCRAM) offers higher density, lower power, and tunable retention, expanding the design space beyond conventional SRAM. To this end, we create an OpenGCRAM compiler supporting both SRAM and GCRAM. It generates macro-level designs and layouts for commercial CMOS processes and characterizes area, delay, and power across user-defined configurations. The tool enables systematic identification of optimal heterogeneous memory configurations for AI tasks under specified performance metrics.

</details>
