<div id=toc></div>

# Table of Contents

- [cs.MA](#cs.MA) [Total: 3]
- [cs.SE](#cs.SE) [Total: 24]
- [cs.AR](#cs.AR) [Total: 7]
- [cs.PF](#cs.PF) [Total: 1]
- [cs.NI](#cs.NI) [Total: 3]
- [cs.DC](#cs.DC) [Total: 12]


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [1] [Beyond Task Completion: An Assessment Framework for Evaluating Agentic AI Systems](https://arxiv.org/abs/2512.12791)
*Sreemaee Akshathala,Bassam Adnan,Mahisha Ramesh,Karthik Vaidhyanathan,Basil Muhammed,Kannan Parthasarathy*

Main category: cs.MA

TL;DR: 提出了一种端到端的智能体评估框架，涵盖LLM、记忆、工具和环境四个维度，以应对现有评估方法忽视模型非确定性的问题。


<details>
  <summary>Details</summary>
Motivation: 当前对LLM智能体及多智能体系统的评估仍面临根本挑战，传统二元任务完成指标无法捕捉执行过程中的行为不确定性。

Method: 构建包含LLM、记忆、工具和环境四大支柱的评估框架，并在Autonomous CloudOps用例中进行验证。

Result: 实验表明该框架能有效识别传统指标忽略的运行时行为偏差，更全面地评估智能体系统。

Conclusion: 该框架为评估具有非确定性的智能体系统提供了新思路，有助于推动多智能体架构的可靠性和实用性。

Abstract: Recent advances in agentic AI have shifted the focus from standalone Large Language Models (LLMs) to integrated systems that combine LLMs with tools, memory, and other agents to perform complex tasks. These multi-agent architectures enable coordinated reasoning, planning, and execution across diverse domains, allowing agents to collaboratively automate complex workflows. Despite these advances, evaluation and assessment of LLM agents and the multi-agent systems they constitute remain a fundamental challenge. Although various approaches have been proposed in the software engineering literature for evaluating conventional software components, existing methods for AI-based systems often overlook the non-deterministic nature of models. This non-determinism introduces behavioral uncertainty during execution, yet existing evaluations rely on binary task completion metrics that fail to capture it. Evaluating agentic systems therefore requires examining additional dimensions, including the agent ability to invoke tools, ingest and retrieve memory, collaborate with other agents, and interact effectively with its environment. We propose an end-to-end Agent Assessment Framework with four evaluation pillars encompassing LLMs, Memory, Tools, and Environment. We validate the framework on a representative Autonomous CloudOps use case, where experiments reveal behavioral deviations overlooked by conventional metrics, demonstrating its effectiveness in capturing runtime uncertainties.

</details>


### [2] [Quantigence: A Multi-Agent AI Framework for Quantum Security Research](https://arxiv.org/abs/2512.12989)
*Abdulmalik Alquwayfili*

Main category: cs.MA

TL;DR: Quantigence是一个多智能体AI框架，用于加速后量子密码迁移中的风险评估与研究分析。


<details>
  <summary>Details</summary>
Motivation: 应对量子计算机对现有公钥基础设施的威胁，解决PQC迁移过程中标准不统一、部署复杂和研究效率低的问题。

Method: 采用认知并行的多智能体架构，包含密码分析师、威胁建模师、标准专家和风险评估师，结合MCP协议与QARS评分模型，在资源受限硬件上实现高效分析。

Result: 实证表明Quantigence可减少67%的研究周期，提升文献覆盖度，实现高保真量子风险评估的普及化。

Conclusion: Quantigence为组织提供结构化、自动化的量子安全分析能力，助力应对‘现在存储、未来解密’攻击模型下的紧迫迁移需求。

Abstract: Cryptographically Relevant Quantum Computers (CRQCs) pose a structural threat to the global digital economy. Algorithms like Shor's factoring and Grover's search threaten to dismantle the public-key infrastructure (PKI) securing sovereign communications and financial transactions. While the timeline for fault-tolerant CRQCs remains probabilistic, the "Store-Now, Decrypt-Later" (SNDL) model necessitates immediate migration to Post-Quantum Cryptography (PQC). This transition is hindered by the velocity of research, evolving NIST standards, and heterogeneous deployment environments. To address this, we present Quantigence, a theory-driven multi-agent AI framework for structured quantum-security analysis. Quantigence decomposes research objectives into specialized roles - Cryptographic Analyst, Threat Modeler, Standards Specialist, and Risk Assessor - coordinated by a supervisory agent. Using "cognitive parallelism," agents reason independently to maintain context purity while execution is serialized on resource-constrained hardware (e.g., NVIDIA RTX 2060). The framework integrates external knowledge via the Model Context Protocol (MCP) and prioritizes vulnerabilities using the Quantum-Adjusted Risk Score (QARS), a formal extension of Mosca's Theorem. Empirical validation shows Quantigence achieves a 67% reduction in research turnaround time and superior literature coverage compared to manual workflows, democratizing access to high-fidelity quantum risk assessment.

</details>


### [3] [The Optimal Control Algorithm of Connected and Automated Vehicles at Roundabouts with Communication Delay](https://arxiv.org/abs/2512.13056)
*Chen Huang,Ronghui Hou*

Main category: cs.MA

TL;DR: 本文提出一种考虑通信延迟的环岛控制算法，通过分布式模型预测控制优化车辆运动与系统指标，提升高密度交通下CAV的安全性和稳定性。


<details>
  <summary>Details</summary>
Motivation: 通信延迟影响联网自动驾驶车辆在环岛场景中的控制性能，需设计鲁棒控制策略应对时延带来的不确定性。

Method: 建立含时延的车辆运动模型，基于DMPC机制确定满足约束的控制方案，并通过多尺度优化整合车辆与系统指标调度入环序列。

Result: 仿真实验表明，该算法在不同渗透率和高负载场景下优于多种对比算法，有效提升控制性能与通行效率。

Conclusion: 所提算法能有效缓解通信延迟对环岛CAV控制的影响，增强系统在复杂交通环境下的稳定性和安全性。

Abstract: Connected and automated vehicles (CAVs) rely on wireless communication to exchange state information for distributed control, making communication delays a critical factor that can affect vehicle motion and degrade control performance, particularly in high-speed scenarios. To address these challenges in the complex environment of roundabout intersections, this paper proposes a roundabout control algorithm, which takes into account the uncertainty of interactive information caused by time delays. First, to maintain the required distance between the current vehicle and its preceding and following vehicles, conflicting vehicles are identified based on the time-to-collision (TTC) in the conflict zone. To fully consider communication performance, a vehicle motion model incorporating time delays is established. According to the distributed model predictive control (DMPC) mechanism, the vehicle motion control that satisfies the roundabout constraints is determined. Second, by scheduling the sequence of vehicles entering the roundabout, a multiscale optimization objective is developed by integrating vehicle motion indicators and roundabout system indicators. Traffic density and travel time are embedded into the optimization problem to guide vehicles to enter the roundabout safely and stably. Through a variety of simulation experiments, the effectiveness of the proposed control algorithm is verified by comparing its performance with that of multiple control algorithms under different autonomous vehicle penetration rates and heavy traffic load scenarios.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [4] [Evaluating Asynchronous Semantics in Trace-Discovered Resilience Models: A Case Study on the OpenTelemetry Demo](https://arxiv.org/abs/2512.12314)
*Anatoly A. Krasnovsky*

Main category: cs.SE

TL;DR: 本文提出一种基于分布式追踪数据的微服务韧性建模方法，通过蒙特卡洛模拟评估服务故障下的端点可用性，并在OpenTelemetry Demo中验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 当前微服务韧性模型多为手工定制，缺乏自动化和通用性，亟需基于实际追踪数据构建更精准的依赖与失效模型。

Method: 从原始OpenTelemetry追踪数据构建服务依赖图，附加端点成功谓词，引入异步语义（如Kafka非阻塞），并通过蒙特卡洛模拟与混沌实验验证模型预测能力。

Result: 模型准确复现了不同故障比例下的整体可用性下降趋势；异步语义对HTTP即时可用性影响极小（约10^(-5)），表明在此场景下可简化为仅连通性模型。

Conclusion: 对于以HTTP即时可用性为主的微服务系统，显式建模异步依赖收益甚微，采用简化的连通性模型已足够有效。

Abstract: While distributed tracing and chaos engineering are becoming standard for microservices, resilience models remain largely manual and bespoke. We revisit a trace-discovered connectivity model that derives a service dependency graph from traces and uses Monte Carlo simulation to estimate endpoint availability under fail-stop service failures. Compared to earlier work, we (i) derive the graph directly from raw OpenTelemetry traces, (ii) attach endpoint-specific success predicates, and (iii) add a simple asynchronous semantics that treats Kafka edges as non-blocking for immediate HTTP success. We apply this model to the OpenTelemetry Demo ("Astronomy Shop") using a GitHub Actions workflow that discovers the graph, runs simulations, and executes chaos experiments that randomly kill microservices in a Docker Compose deployment. Across the studied failure fractions, the model reproduces the overall availability degradation curve, while asynchronous semantics for Kafka edges change predicted availabilities by at most about 10^(-5) (0.001 percentage points). This null result suggests that for immediate HTTP availability in this case study, explicitly modeling asynchronous dependencies is not warranted, and a simpler connectivity-only model is sufficient.

</details>


### [5] [Vibe Coding in Practice: Flow, Technical Debt, and Guidelines for Sustainable Use](https://arxiv.org/abs/2512.11922)
*Muhammad Waseem,Aakash Ahmad,Kai-Kristian Kemell,Jussi Rasku,Sami Lahti,Kalle Mäkelä,Pekka Abrahamsson*

Main category: cs.SE

TL;DR: 本文探讨了Vibe Coding在快速原型开发中的优势及其引发的技术债务问题，并提出可持续发展的应对策略。


<details>
  <summary>Details</summary>
Motivation: 分析Vibe Coding在软件开发生命周期中因流程缺陷和模型限制导致的技术债务风险，以促进其可持续应用。

Method: 基于内部MVP开发经验与行业报告，识别模型、平台与硬件限制对技术债务的影响并提出对策。

Result: 明确了Vibe Coding引发架构不一致、安全漏洞与维护负担增加等技术债务的具体成因。

Conclusion: 需通过改进设计流程、增强模型透明度与迭代开发意识，推动Vibe Coding向更可持续方向演进。

Abstract: Vibe Coding (VC) is a form of software development assisted by generative AI, in which developers describe the intended functionality or logic via natural language prompts, and the AI system generates the corresponding source code. VC can be leveraged for rapid prototyping or developing the Minimum Viable Products (MVPs); however, it may introduce several risks throughout the software development life cycle. Based on our experience from several internally developed MVPs and a review of recent industry reports, this article analyzes the flow-debt tradeoffs associated with VC. The flow-debt trade-off arises when the seamless code generation occurs, leading to the accumulation of technical debt through architectural inconsistencies, security vulnerabilities, and increased maintenance overhead. These issues originate from process-level weaknesses, biases in model training data, a lack of explicit design rationale, and a tendency to prioritize quick code generation over human-driven iterative development. Based on our experiences, we identify and explain how current model, platform, and hardware limitations contribute to these issues, and propose countermeasures to address them, informing research and practice towards more sustainable VC approaches.

</details>


### [6] [Evidence-Driven Decision Support for AI Model Selection in Research Software Engineering](https://arxiv.org/abs/2512.11984)
*Alireza Joonbakhsh,Alireza Rostami,AmirMohammad Kamalinia,Ali Nazeri,Farshad Khunjush,Bedir Tekinerdogan,Siamak Farshidi*

Main category: cs.SE

TL;DR: 提出一个基于多准则决策的AI模型选择框架ModelSelect，以提升科研软件工程中的透明性与可复现性。


<details>
  <summary>Details</summary>
Motivation: 当前AI模型选择过程缺乏系统性，依赖碎片化元数据和个体经验，影响科研软件的可复现性和质量。

Method: 将AI模型选择建模为多准则决策问题，结合自动化数据管道、知识图谱与MCDM方法，采用设计科学研究法进行实证验证。

Result: 在50个案例中，ModelSelect推荐结果可靠、可解释、可复现，与专家推理高度一致，且在可追溯性与一致性上优于生成式AI助手。

Conclusion: 该框架为科研软件工程中的AI模型选择提供了可扩展、可解释、基于实证的决策支持基础，有助于提升研究软件的质量与稳健性。

Abstract: The rapid proliferation of artificial intelligence (AI) models and methods presents growing challenges for research software engineers and researchers who must select, integrate, and maintain appropriate models within complex research workflows. Model selection is often performed in an ad hoc manner, relying on fragmented metadata and individual expertise, which can undermine reproducibility, transparency, and overall research software quality.
  This work proposes a structured and evidence-driven approach to support AI model selection that aligns with both technical and contextual requirements. We conceptualize AI model selection as a Multi-Criteria Decision-Making (MCDM) problem and introduce an evidence-based decision-support framework that integrates automated data collection pipelines, a structured knowledge graph, and MCDM principles. Following the Design Science Research methodology, the proposed framework (ModelSelect) is empirically validated through 50 real-world case studies and comparative experiments against leading generative AI systems.
  The evaluation results show that ModelSelect produces reliable, interpretable, and reproducible recommendations that closely align with expert reasoning. Across the case studies, the framework achieved high coverage and strong rationale alignment in both model and library recommendation tasks, performing comparably to generative AI assistants while offering superior traceability and consistency.
  By framing AI model selection as an MCDM problem, this work establishes a rigorous foundation for transparent and reproducible decision support in research software engineering. The proposed framework provides a scalable and explainable pathway for integrating empirical evidence into AI model recommendation processes, ultimately improving the quality and robustness of research software decision-making.

</details>


### [7] [A Reference Architecture for Embedding Quantum Software Into Enterprise Systems](https://arxiv.org/abs/2512.12009)
*Marc Uphues,Sebastian Thöne,Herbert Kuchen*

Main category: cs.SE

TL;DR: 本文提出了一种模块化参考架构，用于将量子软件嵌入企业系统，并通过两个运筹学案例验证其可行性。


<details>
  <summary>Details</summary>
Motivation: 企业系统需与量子计算服务协作，但必须考虑特定特性和质量属性，因此需要一种稳定可复用的架构方案。

Method: 构建松耦合分布式服务模块，结合量子无关与量子相关任务，以BPMN模型定义可执行流程。

Result: 在两个运筹学组合优化案例中成功应用该架构，验证了其稳定性和可重用性。

Conclusion: 所提出的参考架构能有效支持企业系统整合量子计算能力，适用于计算密集型问题场景。

Abstract: Quantum computing promises a remarkable performance boost for certain applications, including computational intensive problems addressed by enterprise systems. However, software architectures of enterprise systems must consider specific characteristics and quality attributes when collaborating with quantum computing services. Hence, this paper presents a modular reference architecture for embedding quantum software into enterprise systems. Its building blocks consist of loosely coupled and distributed services that implement both quantum-independent and quantum-specific tasks. Although these services either depend on the business domain or the selected quantum algorithm, their orchestration forms a stable and reusable pipeline, specified as an executable BPMN model. For demonstration and evaluation purposes, the proposed reference architecture is utilized in two case studies addressing combinatorial challenges from the field of operations research.

</details>


### [8] [Hyper model checking for high-level relational models](https://arxiv.org/abs/2512.12024)
*Nuno Macedo,Hugo Pacheco*

Main category: cs.SE

TL;DR: HyperPardinus扩展了Alloy语言，支持在设计阶段自动验证超属性，并提供高层抽象的可视化功能。


<details>
  <summary>Details</summary>
Motivation: 现有工具缺乏对超属性的高层规范支持，影响早期系统设计的安全与并发验证。

Method: 通过扩展现有Pardinus后端，结合底层超属性模型检查器，实现对关系模型中超属性的自动验证。

Result: 成功建模并找到含交替量词的复杂超属性反例，适用于前沿场景。

Conclusion: 该方法使Alloy能有效支持超属性规范与验证，提升早期设计阶段的形式化分析能力。

Abstract: Many properties related to security or concurrency must be encoded as so-called hyperproperties, temporal properties that allow reasoning about multiple traces of a system. However, despite recent advances on model checking hyperproperties, there is still a lack of higher-level specification languages that can effectively support software engineering practitioners in verifying properties of this class at early stages of system design.
  Alloy is a lightweight formal method with a high-level specification language that is supported by automated analysis procedures, making it particularly well-suited for the verification of design models at early development stages. It does not natively support, however, the verification of hyperproperties.
  This work proposes HyperPardinus, a new model finding procedure that extends Pardinus -- the temporal logic backend of the Alloy language -- to automatically verify hyperproperties over relational models by relying on existing low-level model checkers for hyperproperties. It then conservatively extends Alloy to support the specification and automatic verification of hyperproperties over design models, as well as the visualization of (counter-)examples at a higher-level of abstraction. Evaluation shows that our approach enables modeling and finding (counter-)examples for complex hyperproperties with alternating quantifiers, making it feasible to address relevant scenarios from the state of the art.

</details>


### [9] [Instruction-Tuning Open-Weight Language Models for BPMN Model Generation](https://arxiv.org/abs/2512.12063)
*Gökberk Çelikmasat,Atay Özgövde,Fatma Başak Aydemir*

Main category: cs.SE

TL;DR: 通过指令微调开源大语言模型，InstruBPM能高效生成符合BPMN规范的流程图，降低建模门槛并保护隐私。


<details>
  <summary>Details</summary>
Motivation: 解决实践中因耗时和缺乏专家而跳过领域建模的问题。

Method: 使用配对文本-图表数据对开源大语言模型进行参数高效微调与量化，支持本地部署。

Result: 微调后模型在序列和结构指标上优于基线，生成结果符合BPMN最佳实践，获专家认可。

Conclusion: 指令微调显著提升结构准确性与鲁棒性，减少对复杂提示依赖，适合实际部署。

Abstract: Domain models are central to software engineering, as they enable a shared understanding, guide implementation, and support automated analyses and model-driven development. Yet, despite these benefits, practitioners often skip modeling because it is time-consuming and demands scarce expertise. We address this barrier by investigating whether open-weight large language models, adapted via instruction tuning, can generate high-quality BPMN process models directly from natural language descriptions in a cost-effective and privacy-preserving way. We introduce InstruBPM, a reproducible approach that prepares paired text-diagram data and instruction tunes an open source large language model with parameter-efficient fine-tuning and quantization for on-prem deployment. We evaluate the tuned model through complementary perspectives: (i) text/code similarity using BLEU, ROUGE-L, and METEOR, (ii) structural fidelity using Relative Graph Edit Distance, (iii) guidelines conformance using external tool checks, and (iv) a small expert review. Using a curated subset of a multi-domain BPMN dataset, we compare the tuned model with untuned open-weight baselines and strong proprietary models under consistent prompting regimes. Our compact tuned model outperforms all baselines across sequence and structural metrics while requiring substantially fewer resources; guideline analysis and expert feedback further indicate that the generated diagrams largely follow BPMN best practices and are useful starting points that reduce modeling effort. Overall, instruction tuning improves structural accuracy and robustness compared to untuned baselines and reduces reliance on heavy prompt scaffolding. We publicly share the trained models and scripts to support reproducibility and further research.

</details>


### [10] [Citation-Grounded Code Comprehension: Preventing LLM Hallucination Through Hybrid Retrieval and Graph-Augmented Context](https://arxiv.org/abs/2512.12117)
*Jahidul Arafat*

Main category: cs.SE

TL;DR: 本文提出一种结合混合检索与轻量级结构推理的方法，以实现可验证、引用接地的代码理解，显著提升引用准确性并消除幻觉。


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型在代码理解中因幻觉导致的不可靠引用问题，提高开发者查询陌生代码库时的准确性与可信度。

Method: 构建融合BM25稀疏匹配、BGE稠密嵌入和基于Neo4j导入关系图扩展的混合检索系统，并通过跨文件结构推理增强证据发现能力。

Result: 在30个Python仓库180个查询上达到92%引用准确率且零幻觉，相较单模态基线提升14-18个百分点，62%架构查询中发现纯文本相似性遗漏的跨文件证据。

Conclusion: 引用接地应作为代码理解系统的核心架构原则，结构感知的混合检索能有效克服现有纯文本方法的局限性。

Abstract: Large language models have become essential tools for code comprehension, enabling developers to query unfamiliar codebases through natural language interfaces. However, LLM hallucination, generating plausible but factually incorrect citations to source code, remains a critical barrier to reliable developer assistance. This paper addresses the challenges of achieving verifiable, citation grounded code comprehension through hybrid retrieval and lightweight structural reasoning. Our work is grounded in systematic evaluation across 30 Python repositories with 180 developer queries, comparing retrieval modalities, graph expansion strategies, and citation verification mechanisms. We find that challenges of citation accuracy arise from the interplay between sparse lexical matching, dense semantic similarity, and cross file architectural dependencies. Among these, cross file evidence discovery is the largest contributor to citation completeness, but it is largely overlooked because existing systems rely on pure textual similarity without leveraging code structure. We advocate for citation grounded generation as an architectural principle for code comprehension systems and demonstrate this need by achieving 92 percent citation accuracy with zero hallucinations. Specifically, we develop a hybrid retrieval system combining BM25 sparse matching, BGE dense embeddings, and Neo4j graph expansion via import relationships, which outperforms single mode baselines by 14 to 18 percentage points while discovering cross file evidence missed by pure text similarity in 62 percent of architectural queries.

</details>


### [11] [Training Versatile Coding Agents in Synthetic Environments](https://arxiv.org/abs/2512.12216)
*Yiqi Zhu,Apurva Gandhi,Graham Neubig*

Main category: cs.SE

TL;DR: SWE-Playground 是一种新型训练管道，通过合成生成项目与任务，摆脱对外部数据依赖，支持更广泛编码任务的智能体训练。


<details>
  <summary>Details</summary>
Motivation: 现有方法受限于 GitHub 仓库的灵活性不足及任务类型单一，难以覆盖软件工程师所需的多样化任务。

Method: 利用强语言模型和智能体从零合成生成项目与任务，构建多样化的训练环境与轨迹。

Result: 在三个基准测试中表现优异，能以更少轨迹提供密集训练信号，达到与先前工作相当的性能。

Conclusion: SWE-Playground 显著提升了编码智能体的训练效率与任务泛化能力。

Abstract: Prior works on training software engineering agents have explored utilizing existing resources such as issues on GitHub repositories to construct software engineering tasks and corresponding test suites. These approaches face two key limitations: (1) their reliance on pre-existing GitHub repositories offers limited flexibility, and (2) their primary focus on issue resolution tasks restricts their applicability to the much wider variety of tasks a software engineer must handle. To overcome these challenges, we introduce SWE-Playground, a novel pipeline for generating environments and trajectories which supports the training of versatile coding agents. Unlike prior efforts, SWE-Playground synthetically generates projects and tasks from scratch with strong language models and agents, eliminating reliance on external data sources. This allows us to tackle a much wider variety of coding tasks, such as reproducing issues by generating unit tests and implementing libraries from scratch. We demonstrate the effectiveness of this approach on three distinct benchmarks, and results indicate that SWE-Playground produces trajectories with dense training signal, enabling agents to reach comparable performance with significantly fewer trajectories than previous works.

</details>


### [12] [Cluster-guided LLM-Based Anonymization of Software Analytics Data: Studying Privacy-Utility Trade-offs in JIT Defect Prediction](https://arxiv.org/abs/2512.12224)
*Maaz Khan,Gul Sher Khan,Ahsan Raza,Pir Sami Ullah,Abdul Ali Bangash*

Main category: cs.SE

TL;DR: 提出一种基于大语言模型的聚类引导匿名化方法，用于JIT缺陷预测中的隐私保护，同时保持预测性能。


<details>
  <summary>Details</summary>
Motivation: 现有匿名化方法忽视软件度量间的上下文依赖，导致隐私与效用难以平衡。

Method: 将提交按特征聚类，利用LLM为每类生成上下文感知的参数配置，定义alpha-beta比率和变更混合分布以实现匿名化。

Result: 在六个项目上评估表明，该方法达到80%以上隐私水平，比现有图匿名方法提升18-25%，F1分数相当。

Conclusion: LLM可作为自适应匿名引擎，在保护隐私的同时不牺牲预测准确性。

Abstract: The increasing use of machine learning (ML) for Just-In-Time (JIT) defect prediction raises concerns about privacy leakage from software analytics data. Existing anonymization methods, such as tabular transformations and graph perturbations, often overlook contextual dependencies among software metrics, leading to suboptimal privacy-utility tradeoffs. Leveraging the contextual reasoning of Large Language Models (LLMs), we propose a cluster-guided anonymization technique that preserves contextual and statistical relationships within JIT datasets. Our method groups commits into feature-based clusters and employs an LLM to generate context-aware parameter configurations for each commit cluster, defining alpha-beta ratios and churn mixture distributions used for anonymization. Our evaluation on six projects (Cassandra, Flink, Groovy, Ignite, OpenStack, and Qt) shows that our LLM-based approach achieves privacy level 2 (IPR >= 80 percent), improving privacy by 18 to 25 percent over four state-of-the-art graph-based anonymization baselines while maintaining comparable F1 scores. Our results demonstrate that LLMs can act as adaptive anonymization engines when provided with cluster-specific statistical information about similar data points, enabling context-sensitive and privacy-preserving software analytics without compromising predictive accuracy.

</details>


### [13] [The Role of AI in Modern Penetration Testing](https://arxiv.org/abs/2512.12326)
*J. Alexander Curtis,Nasir U. Eisty*

Main category: cs.SE

TL;DR: AI正在重塑渗透测试，尤其在利用强化学习自动化攻击策略和漏洞识别方面取得进展，但侦察与后渗透阶段仍存挑战，LLM应用尚待探索。


<details>
  <summary>Details</summary>
Motivation: 传统渗透测试依赖人工且耗时，随着系统复杂性增加，亟需更高效、可扩展的AI驱动方法。

Method: 通过系统文献综述分析58篇同行评审论文，聚焦AI特别是强化学习在渗透测试各阶段的应用现状与趋势。

Result: 77%研究集中于强化学习，在发现与利用阶段表现突出；实际应用如PenBox已展现潜力，但在侦察、后渗透及LLM领域仍不足。

Conclusion: AI在渗透测试中具广阔前景，但需进一步提升模型灵活性并拓展LLM等新兴技术的研究，以实现全流程自动化安全评估。

Abstract: Penetration testing is a cornerstone of cybersecurity, traditionally driven by manual, time-intensive processes. As systems grow in complexity, there is a pressing need for more scalable and efficient testing methodologies. This systematic literature review examines how Artificial Intelligence (AI) is reshaping penetration testing, analyzing 58 peer-reviewed studies from major academic databases. Our findings reveal that while AI-assisted pentesting is still in its early stages, notable progress is underway, particularly through Reinforcement Learning (RL), which was the focus of 77% of the reviewed works. Most research centers on the discovery and exploitation phases of pentesting, where AI shows the greatest promise in automating repetitive tasks, optimizing attack strategies, and improving vulnerability identification. Real-world applications remain limited but encouraging, including the European Space Agency's PenBox and various open-source tools. These demonstrate AI's potential to streamline attack path analysis, analyze complex network topology, and reduce manual workload. However, challenges persist: current models often lack flexibility and are underdeveloped for the reconnaissance and post-exploitation phases of pentesting. Applications involving Large Language Models (LLMs) remain relatively under-researched, pointing to a promising direction for future exploration. This paper offers a critical overview of AI's current and potential role in penetration testing, providing valuable insights for researchers, practitioners, and organizations aiming to enhance security assessments through advanced automation or looking for gaps in existing research.

</details>


### [14] [ATLAS: Automated Tree-based Language Analysis System for C and C++ source programs](https://arxiv.org/abs/2512.12507)
*Jaid Monwar Chowdhury,Ahmad Farhan Shahriar Chowdhury,Humayra Binte Monwar,Mahmuda Naznin*

Main category: cs.SE

TL;DR: ATLAS是一个Python CLI工具，用于生成C/C++项目的多视图代码表示，以提升程序理解和软件工程任务的效果。


<details>
  <summary>Details</summary>
Motivation: 传统编程分析技术在处理C/C++复杂结构时存在不足，而现有机器学习方法未能充分捕捉代码的结构性依赖。

Method: ATLAS通过构建语句级控制流图（CFG）和类型感知数据流图（DFG），结合抽象语法树（AST），生成统一的多视图代码表示。

Result: 该工具支持跨文件、可编译或不可编译代码的分析，能有效保留语义与结构信息，为下游任务提供实用基础。

Conclusion: ATLAS为改进基于机器学习的程序理解与软件工程任务提供了强大且实用的代码表示方案。

Abstract: The growing complexity of modern software systems has highlighted the shortcomings of traditional programming analysis techniques, particularly for Software Engineering (SE) tasks. While machine learning and Large Language Models (LLMs) offer promising solutions, their effectiveness is limited by the way they interpret data. Unlike natural language, source code meaning is defined less by token adjacency and more by complex, long-range, and structural relationships and dependencies. This limitation is especially pronounced for C and C++, where flatter syntactic hierarchies, pointer aliasing, multi-level indirection, typedef-based type obfuscation, and function-pointer calls hinder accurate static analysis. To address these challenges, this paper introduces ATLAS, a Python-based Command-Line Interface (CLI) that (i) generates statement-level Control Flow Graphs (CFG) and type-aware Data Flow Graphs (DFG) that capture inter-functional dependencies for the entire program; (ii) has the ability to work on entire C and C++ projects comprising multiple files; (iii) works on both compilable and non-compilable code and (iv) produces a unified multi-view code representation using Abstract Syntax Trees (AST), CFG and DFG. By preserving essential structural and semantic information, ATLAS provides a practical foundation for improving downstream SE and machine-learning-based program understanding. Video demonstration: https://youtu.be/RACWQe5ELwY Tool repository: https://github.com/jaid-monwar/ATLAS-code-representation-tool

</details>


### [15] [Diverse LLMs vs. Vulnerabilities: Who Detects and Fixes Them Better?](https://arxiv.org/abs/2512.12536)
*Arastoo Zibaeirad,Marco Vieira*

Main category: cs.SE

TL;DR: DVDR-LLM是一个集成多种大语言模型的框架，用于提升软件漏洞检测与修复的准确率，尤其在复杂代码和多文件场景下表现更优，但需权衡误报与漏报。


<details>
  <summary>Details</summary>
Motivation: 单一LLM在复杂漏洞识别与修复上存在局限，因此探索集成多个LLM能否降低错误率并提升性能。

Method: 提出DVDR-LLM框架，聚合多个LLM的输出，并通过调整模型间共识阈值优化不同安全场景下的表现。

Result: 相比单个模型平均性能，DVDR-LLM检测准确率提升10-12%，多文件漏洞场景下召回率+18%、F1分数+11.8%。

Conclusion: 集成方法有效提升复杂漏洞检测能力，但需根据应用场景谨慎设定模型共识阈值以平衡误报与漏报。

Abstract: Large Language Models (LLMs) are increasingly being studied for Software Vulnerability Detection (SVD) and Repair (SVR). Individual LLMs have demonstrated code understanding abilities, but they frequently struggle when identifying complex vulnerabilities and generating fixes.
  This study presents DVDR-LLM, an ensemble framework that combines outputs from diverse LLMs to determine whether aggregating multiple models reduces error rates. Our evaluation reveals that DVDR-LLM achieves 10-12% higher detection accuracy compared to the average performance of individual models, with benefits increasing as code complexity grows. For multi-file vulnerabilities, the ensemble approach demonstrates significant improvements in recall (+18%) and F1 score (+11.8%) over individual models. However, the approach raises measurable trade-offs: reducing false positives in verification tasks while simultaneously increasing false negatives in detection tasks, requiring careful decision on the required level of agreement among the LLMs (threshold) for increased performance across different security contexts.
  Artifact: https://github.com/Erroristotle/DVDR_LLM

</details>


### [16] [Assessing the Capability of Android Dynamic Analysis Tools to Combat Anti-Runtime Analysis Techniques](https://arxiv.org/abs/2512.12551)
*Dewen Suo,Lei Xue,Weihao Huang,Runze Tan,Guozi Sun*

Main category: cs.SE

TL;DR: 本文评估了主流Android动态分析工具对抗反运行时分析（ARA）技术的能力，揭示了现有工具在应对ARA机制方面的严重不足。


<details>
  <summary>Details</summary>
Motivation: 恶意应用广泛采用ARA技术阻碍动态分析，威胁Android生态安全，亟需更有效的解决方案。

Method: 通过实证研究，系统评估多种常用动态分析工具绕过不同ARA技术的能力。

Result: 发现现有工具在对抗ARA方面存在显著效能差距，难以有效检测和分析恶意行为。

Conclusion: 研究指出现有工具的局限性，呼吁开发更强大的方法以应对ARA技术，推动软件安全与动态分析领域发展。

Abstract: As the dominant mobile operating system, Android continues to attract a substantial influx of new applications each year. However, this growth is accompanied by increased attention from malicious actors, resulting in a significant rise in security threats to the Android ecosystem. Among these threats, the adoption of Anti-Runtime Analysis (ARA) techniques by malicious applications poses a serious challenge, as it hinders security professionals from effectively analyzing malicious behaviors using dynamic analysis tools. ARA technologies are designed to prevent the dynamic examination of applications, thus complicating efforts to ensure platform security. This paper presents a comprehensive empirical study that assesses the ability of widely-used Android dynamic analysis tools to bypass various ARA techniques. Our findings reveal a critical gap in the effectiveness of existing dynamic analysis tools to counter ARA mechanisms, highlighting an urgent need for more robust solutions. This work provides valuable insights into the limitations of existing tools and highlights the need for improved methods to counteract ARA technologies, thus advancing the field of software security and dynamic analysis.

</details>


### [17] [A Systematic Analysis of Higher Education on Software Engineering in the Netherlands](https://arxiv.org/abs/2512.12650)
*Bastiaan Heeren,Fabiano Dalpiaz,Mazyar Seraj,Roberto Verdecchia,Vadim Zaytsev*

Main category: cs.SE

TL;DR: 本研究通过分析荷兰10所大学的207门课程，评估软件工程高等教育现状，发现核心知识领域覆盖均匀，并识别出三个紧密关联的知识集群及待加强领域如软件工程经济学。


<details>
  <summary>Details</summary>
Motivation: 帮助教育者通过基准对比优化课程设计，提升软件工程教学效果。

Method: 采用众包方式，依据SWEBOK知识领域对课程进行映射，经过标准化与一致性优化后进行数据分析。

Result: Construction和Programming在本科阶段最常覆盖；识别出三个知识集群；各校覆盖均衡但反映研究特色；软件工程经济学等较少涉及。

Conclusion: 研究揭示知识领域间的关联性及整合学习潜力，建议全球学者应用此方法进行跨区域比较，以促进课程改进。

Abstract: Software engineering educators strive to continuously improve their courses and programs. Understanding the current state of practice of software engineering higher education can empower educators to critically assess their courses, fine-tune them by benchmarking against observed practices, and ultimately enhance their curricula. In this study, we aim to provide an encompassing analysis of higher education on software engineering by considering the higher educational offering of an entire European country, namely the Netherlands. We leverage a crowd-sourced analysis process by considering 10 Dutch universities and 207 university courses. The courses are analysed via knowledge areas adopted from the SWEBOK. The mapping process is refined via homogenisation and internal consistency improvement phases, and is followed by a data analysis phase. Given its fundamental nature, Construction and Programming is the most covered knowledge area at Bachelor level. Other knowledge areas are equally covered at Bachelor and Master level (e.g., software engineering models), while more advanced ones are almost exclusively covered at Master level. We identify three clusters of tightly coupled knowledge areas: (i) requirements, architecture, and design, (ii) testing, verification, and security, and (iii) process-oriented and DevOps topics. Dutch universities generally cover all knowledge areas uniformly, with minor deviations reflecting institutional research strengths. Our results highlight correlations among key knowledge areas and their potential for enhancing integrated learning. We also identify underrepresented areas, such as software engineering economics, which educators may consider including in curricula. We invite researchers to use our research method in their own geographical region, in order to contrast software engineering education programs across the globe.

</details>


### [18] [Attributes to Support the Formulation of Practically Relevant Research Problems in Software Engineering](https://arxiv.org/abs/2512.12699)
*Anrafel Fernandes Pereira,Maria Teresa Baldassarre,Daniel Mendez,Jürgen Börstler,Nauman bin Ali,Rahul Mohanani,Darja Smite,Stefan Biffl,Rogardt Heldal,Davide Falessi,Daniel Graziotin,Marcos Kalinowski*

Main category: cs.SE

TL;DR: 该研究通过工作坊评估了软件工程中七个研究问题属性的重要性与应用方法，以提升学术与工业需求的对齐。


<details>
  <summary>Details</summary>
Motivation: 缺乏结构化指导导致软件工程研究问题制定不完善，影响实际相关性。

Method: 在ISERN 2024会议中组织42位资深研究人员参与工作坊，使用问题愿景板讨论并收集反馈与问卷数据。

Result: 七个属性被确认重要，参与者提出改进意见如加入财务指标和可行性约束。

Conclusion: 七项属性有助于反思性和情境感知的问题制定，适配具体情境可增强学术与工业实践的契合度。

Abstract: [Background] A well-formulated research problem is essential for achieving practical relevance in Software Engineering (SE), yet there is a lack of structured guidance in this early phase. [Aims] Our goal is to introduce and evaluate seven attributes identified in the SE literature as relevant for formulating research problems (practical problem, context, implications/impacts, practitioners, evidence, objective, and research questions) in terms of their perceived importance and completeness, and learn how they can be applied. [Method] We conducted a workshop with 42 senior SE researchers during the ISERN 2024 meeting. The seven attributes were presented using a Problem Vision board filled with a research example. Participants discussed attributes in groups, shared written feedback, and individually completed a survey assessing their importance, completeness, and suggestions for improvement. [Results] The findings confirm the importance of the seven attributes in the formulation of industry-oriented research problems. Qualitative feedback illustrated how they can be applied in practice and revealed suggestions to refine them, such as incorporating financial criteria (e.g., ROI) into implications/impacts and addressing feasibility and constraints under evidence. [Conclusion] The results reaffirm the importance of the seven attributes in supporting a reflective and context-aware problem formulation. Adapting their use to specific research contexts can help to improve the alignment between academic research and industry needs.

</details>


### [19] [Temporal HAL-API Dependencies as a Gateway to Formal Embedded Software Development](https://arxiv.org/abs/2512.12788)
*Manuel Bentele,Andreas Podelski,Axel Sikora,Bernd Westphal*

Main category: cs.SE

TL;DR: Temporal HAL-API依赖(THADs)为嵌入式软件开发提供了一种平衡的正确性验证方法，兼具低开销与自动化优势。


<details>
  <summary>Details</summary>
Motivation: 推动形式化方法在工业嵌入式软件中的更广泛和经济应用。

Method: 通过程序注解指定属性，并利用软件模型检查自动验证。

Result: THADs在通用静态分析与全形式化方法之间找到了实用的中间点。

Conclusion: THADs有望成为工业界采用形式化方法的重要切入点。

Abstract: Temporal HAL-API Dependencies (THADs) can be useful to capture an interesting class of correctness properties in embedded software development. They demand a moderate effort for specification (which can be done via program annotations) and verification (which can be done automatically via software model checking). In this sense, they have the potential to form an interesting sweet spot between generic properties (that demand virtually no specification effort, and that are typically addressed by static analysis) and application-specific properties as addressed by full-fledged formal methods. Thus, they may form a gateway to wider and more economic use of formal methods in industrial embedded software development.

</details>


### [20] [Challenges and Enablers: Remote Work for People with Disabilities in Software Development Teams](https://arxiv.org/abs/2512.12965)
*Thayssa Rocha,Luciano Teran,Marcelle Mota,Cleidson de Souza,Kiev Gama,Gustavo Pinto*

Main category: cs.SE

TL;DR: 研究探讨远程工作对残障人士在混合能力软件开发团队中的影响，揭示了团队成员、领导对其挑战认知不足的问题，并提出改进无障碍工具、沟通策略和管理方法的机会。


<details>
  <summary>Details</summary>
Motivation: 远程与混合办公模式的普及为残障人士融入软件开发团队带来新机遇与挑战，需深入理解其协作体验以促进包容性。

Method: 通过在线问卷（含残障员工、领导及同事）与14名自述残障的开发者深度访谈，结合定量数据与定性编码分析远程协作的社会技术层面。

Result: 残障团队成员面临诸多障碍，但其同事与领导对其日常挑战认知有限，凸显无障碍工具、沟通方式与管理策略的改进空间。

Conclusion: 提升远程协作中对残障人士需求的认知与支持，是实现真正包容性软件开发团队的关键。

Abstract: The increasing adoption of remote and hybrid work modalities in the technology sector has brought new opportunities and challenges for the inclusion of people with disabilities (PWD) in software development teams (SDT). This study investigates how remote work affects PWDs' experience in mixed-ability SDT, focusing on the unique challenges and strategies that emerge in remote environments. We conducted an online survey with \totalSurveyResponses valid responses, encompassing PWD, their leaders, and teammates, to capture sociotechnical aspects of their experiences with remote collaboration. To deepen our understanding, we carried out 14 structured interviews with software developers who self-identified as having disabilities (six autistic individuals, six with physical disabilities, and two who are d/Deaf). Our analysis combines quantitative data with qualitative coding of open-ended survey responses and interview transcripts. The results reveal that, despite the barriers faced by team members with disabilities, their teammates and leaders have a limited perception of the daily challenges involved in sustaining collaborative remote work. These findings highlight opportunities for improvement in accessibility tools, communication strategies, and adaptive management approaches.

</details>


### [21] [A Decision Support Framework for Blockchain Pattern Selection Based on Soft Goals](https://arxiv.org/abs/2512.13239)
*Eddy Kiomba Kambilo,Nicolas Herbaut,Irina Rychkova,Carine Souveyet*

Main category: cs.SE

TL;DR: 提出BC-TEAEM框架，结合本体与多准则决策方法，辅助区块链模式选择。


<details>
  <summary>Details</summary>
Motivation: 区块链模式多样且缺乏标准化框架，导致架构师难以匹配业务目标与技术设计。

Method: 构建BC-TEAEM决策支持框架，融合区块链模式本体、领域无关软目标及多准则决策方法，通过领域与技术专家协作迭代优化偏好。

Result: 开发原型工具并以制药供应链追溯案例验证，证明框架有效支持区块链模式的系统化选择。

Conclusion: BC-TEAEM能提升区块链技术选型与业务目标的一致性，增强决策可追溯性与适用性。

Abstract: Blockchain technology is gaining momentum across many sectors. Whereas blockchain solutions have important positive effects on the business domain, they also introduce constraints and may cause delayed or unforeseen negative effects, undermining business strategies. The diversity of blockchain patterns and lack of standardized frameworks linking business goals to technical design decisions make pattern selection a complex task for system architects. To address this challenge, we propose Blockchain--Technology-Aware Enterprise Modeling (BC-TEAEM), a decision support framework that combines ontologies of blockchain patterns and domain-independent soft goals with a multi-criteria decision-making approach. The framework focuses on the interplay between a domain expert and a technical expert to ensure alignment and traceability. By iteratively capturing and refining preferences, BC-TEAEM supports systematic selection of blockchain patterns. We develop a prototype decision support tool implementing our method and validate it through a case study of a pharmaceutical company's supply chain traceability system, demonstrating the framework's applicability. %a supply chain traceability case study.

</details>


### [22] [UCRBench: Benchmarking LLMs on Use Case Recovery](https://arxiv.org/abs/2512.13360)
*Shuyuan Xiao,Yiran Zhang,Weisong Sun,Xiaohong Chen,Yang Liu,Zhi Jin*

Main category: cs.SE

TL;DR: 本文提出了一种与代码对齐的用例基准，用于系统评估大语言模型在从源码生成用例时的表现，并揭示其当前能力与局限。


<details>
  <summary>Details</summary>
Motivation: 现有用例基准稀缺且易与实际系统行为脱节，难以严谨评估大语言模型生成用例的能力。

Method: 通过人工验证九个真实项目中的用户目标与子功能用例构建基准，并设计分层评估协议，衡量角色正确性、名称准确性、路径保真度和行为覆盖率。

Result: 大语言模型能部分重构系统功能，但在领域特定和多模块系统中表现较差，存在高遗漏率和抽象不一致问题。

Conclusion: 大语言模型在用例逆向工程上具备潜力，但当前仍存在显著局限，需进一步改进以适应复杂系统场景。

Abstract: Use cases are widely employed to specify functional requirements, yet existing benchmarks are scarce and face the risk of being misaligned with actual system behavior, similarly limiting the rigorous evaluation of large language models (LLMs) in generating use cases from source code. We address this gap by introducing code-aligned use case benchmarks, constructed through manual validation of both user-goal and subfunction use cases across nine real-world software projects. Using this benchmark, we conduct the first systematic study of LLMs and propose a hierarchical evaluation protocol that assesses actor correctness, name accuracy, path fidelity, and behavioral coverage. The results show that while LLMs can partially reconstruct system functionality, their performance varies significantly across projects, with particularly noticeable shortcomings in domain-specific and multi-module systems. The models also exhibit high omission rates and struggle to maintain consistent abstraction when aggregating subfunctions into user-goal use cases, highlighting both the potential and current limitations of LLM-based use case reverse engineering.

</details>


### [23] [PSALM: applying Proportional SAmpLing strategy in Metamorphic testing](https://arxiv.org/abs/2512.13414)
*Zenghui Zhou,Pak-Lok Poon,Zheng Zheng,Xiao-Yi Zhang*

Main category: cs.SE

TL;DR: PSALM是一种针对蜕变测试的改进选择策略，兼具理论保障与实践效果。


<details>
  <summary>Details</summary>
Motivation: 现有研究缺乏对源测试用例和蜕变组选择的系统方法，而传统PSS策略无法直接适用于MT场景。

Method: 提出PSALM策略，形式化证明其优于随机选择，并通过实证研究验证其在多个程序和变异体上的有效性。

Result: PSALM在八个程序和184个变异体上表现优于ART和MT-ART等现有策略，且理论与实验结果一致。

Conclusion: PSALM为蜕变测试提供了一种兼具理论基础和实际效能的选择策略。

Abstract: Metamorphic testing (MT) alleviates the oracle problem by checking metamorphic relations (MRs) across multiple test executions. The fault detection effectiveness of MT is influenced not only by the choice and quality of MRs, but also by how source test cases and metamorphic groups (MGs) are selected. While substantial research has focused on designing, generating, and validating MRs, systematic methods for source test case selection and MG selection remain largely unexplored. Although the Proportional Sampling Strategy (PSS) provides strong theoretical guarantees in traditional testing, its assumptions cannot be directly applied in MT due to differences in selection domains, test units, and failure distributions. This paper proposes PSALM, an adaptation of PSS to MT for both source test case selection and MG selection. We formally prove that PSALM is never inferior to random selection regardless of how the source test case and MG domains are partitioned. We further identify the conditions under which applying PSALM to source test case selection and MG selection yields identical effectiveness. A comprehensive empirical study on eight subject programs and 184 mutants shows that the results are consistent with our theoretical analysis and that PSALM generally performs more effectively than existing selection strategies such as ART and MT-ART. These results demonstrate that PSALM provides a theoretically grounded and practically effective selection strategy for MT.

</details>


### [24] [QMon: Monitoring the Execution of Quantum Circuits with Mid-Circuit Measurement and Reset](https://arxiv.org/abs/2512.13422)
*Ning Ma,Jianjun Zhao,Foutse Khomh,Shaukat Ali,Heng Li*

Main category: cs.SE

TL;DR: QMON是一种通过中段测量和重置操作监控量子电路内部状态的方法，能够在保持原有行为的同时检测和定位编程错误。


<details>
  <summary>Details</summary>
Motivation: 量子电路因不可克隆性和测量坍缩等特性难以直接观测，导致调试与运行时监控困难。

Method: 在开发者指定位置插入监控算子，比较预期与实际量子态概率，并利用中段测量与重置维持原电路行为。

Result: 在154个电路上的实验表明，所有电路功能无损，QMON能有效检测并定位多种编程错误，对量子态干扰极小。

Conclusion: QMON有助于提升量子软件的健壮性与可靠性，推动量子计算领域发展。

Abstract: Unlike classical software, where logging and runtime tracing can effectively reveal internal execution status, quantum circuits possess unique properties, such as the no-cloning theorem and measurement-induced collapse, that prevent direct observation or duplication of their states. These characteristics make it especially challenging to monitor the execution of quantum circuits, complicating essential tasks such as debugging and runtime monitoring. This paper presents QMON, a practical methodology that leverages mid-circuit measurements and reset operations to monitor the internal states of quantum circuits while preserving their original runtime behavior. QMON enables the instrumentation of monitoring operators at developer-specified locations within the circuit, allowing comparisons between expected and observed quantum-state probabilities at those locations. We evaluated QMON by analyzing its impact on circuit behavior, monitoring coverage, and effectiveness in bug localization. Experimental results involving 154 quantum circuits show that all circuits preserve their intended functionality after instrumentation and that QMON successfully detects and localizes various programming errors. Although monitoring coverage is limited by the need to preserve delicate quantum properties, such as entanglement, QMON effectively detects errors while introducing no or negligible disturbance to the original quantum states. QMON facilitates the development of more robust and reliable quantum software as the field continues to mature.

</details>


### [25] [A Data Annotation Requirements Representation and Specification (DARS)](https://arxiv.org/abs/2512.13444)
*Yi Peng,Hina Saeeda,Hans-Martin Heyn,Jennifer Horkoff,Eric Knauss,Fredrick Warg*

Main category: cs.SE

TL;DR: 提出DARS框架以提升数据标注需求的完整性、准确性和一致性，从而增强依赖标注数据的智能信息系统可靠性。


<details>
  <summary>Details</summary>
Motivation: 现有需求工程未充分解决数据标注特有的挑战，需专门的需求表示方法。

Method: 设计DARS框架，包含注释协商卡和基于场景的标注规范，并通过汽车感知案例与18类真实标注错误映射进行评估。

Result: DARS有效缓解了标注错误的根本原因，提升了安全关键系统的可靠性。

Conclusion: 将DARS整合进需求工程可推动面向数据依赖组件的工程框架演进。

Abstract: With the rise of AI-enabled cyber-physical systems, data annotation has become a critical yet often overlooked process in the development of these intelligent information systems. Existing work in requirements engineering (RE) has explored how requirements for AI systems and their data can be represented. However, related interviews with industry professionals show that data annotations and their related requirements introduce distinct challenges, indicating a need for annotation-specific requirement representations. We propose the Data Annotation Requirements Representation and Specification (DARS), including an Annotation Negotiation Card to align stakeholders on objectives and constraints, and a Scenario-Based Annotation Specification to express atomic and verifiable data annotation requirements. We evaluate DARS with an automotive perception case related to an ongoing project, and a mapping against 18 real-world data annotation error types. The results suggest that DARS mitigates root causes of completeness, accuracy, and consistency annotation errors. By integrating DARS into RE, this work improves the reliability of safety-critical systems using data annotations and demonstrates how engineering frameworks must evolve for data-dependent components of today's intelligent information systems.

</details>


### [26] [Mapping of the system of software-related emissions and shared responsibilities](https://arxiv.org/abs/2512.13474)
*Laura Partanen,Antti Sipila,Md Sanaul Haque,Jari Porras*

Main category: cs.SE

TL;DR: 本文探讨ICT行业对气候变化的影响，提出系统性映射以识别软件相关碳排放源及各利益相关方在软件生命周期中的责任。


<details>
  <summary>Details</summary>
Motivation: 为实现巴黎协定1.5°C温控目标，需明确ICT行业碳排放责任并采取减排措施。

Method: 通过系统映射分析ICT领域内主要碳排放与能耗来源，并界定各利益相关方责任。

Result: 成功识别软件生命周期中关键排放环节及对应责任主体，提升行业认知。

Conclusion: 系统映射有助于推动ICT行业履行环境责任，支持全球气候治理目标。

Abstract: The global climate is experiencing a rapid and unprecedented warming trend. The ICT sector is a notable contributor to global greenhouse gas emissions, with its environmental impact continuing to expand. Addressing this issue is vital for achieving the objectives of the Paris Agreement, particularly the goal of limiting global temperature rise to 1.5°C. At the European Union level, regulatory measures such as the CSRD and the CSDD impose obligations on companies, including those within the ICT sector, to recognize and mitigate their environmental footprint. This study provides a comprehensive system mapping aimed at enhancing the awareness and understanding of software-related emissions and the corresponding responsibilities borne by the ICT sector. The mapping identifies the primary sources of carbon emissions and energy consumption within the ICT domain while also outlining the key responsibilities of the stakeholders accountable throughout the software lifecycle.

</details>


### [27] [Fine-tuned LLM-based Code Migration Framework](https://arxiv.org/abs/2512.13515)
*Oleg Grynets,Vasyl Lyashkevych,Dmytro Baran,Maksym Orliansky,Taras Zelenyy,Markiian Leshchyshyn*

Main category: cs.SE

TL;DR: 该研究提出了一种结合大语言模型与传统工程方法的自动化SQL代码迁移框架，显著降低语法错误率并提升迁移效率。


<details>
  <summary>Details</summary>
Motivation: 解决从Oracle PL/SQL到PostgreSQL等数据库系统迁移中的语法映射、逻辑兼容性和效率问题。

Method: 集成微调大语言模型，结合提示工程、自动化特征检测、半监督错误分析及专家反馈，构建迭代式迁移流程。

Result: 显著降低语法错误率，提高功能对齐度，并通过数据采样实现持续优化，提升整体迁移效率。

Conclusion: 将生成式AI嵌入迁移流程可实现精准映射与半自动化纠错，为大规模数据库转型提供高效、可扩展的解决方案。

Abstract: The study presents the outcomes of research and experimental validation in the domain of automated codebase migration, with a focus on addressing challenges in transitioning SQL-based systems. The proposed method for migration essentially appears as a framework that leverages the best aspects of traditional software engineering techniques and provides an iterative, scalable, precise and efficient solution for modern database transformations. The central piece of the approach is the integration of a fine-tuned Large Language Model to address critical issues in SQL code conversion, such as syntax mapping, resolving discrepancies between Oracle PL/SQL and PostgreSQL, and optimising database elements such as stored procedures, triggers, views, and overall database logic. Thus, the method involves a trade-off between fine-tuning and prompt engineering. Special attention is given to a fine-tuning approach, which enhances the adaptability and compatibility with migration requirements across the entire database. According to the achieved results, fine-tuning plays a very important role. The study employs targeted evaluation methodologies along with computational metrics to measure the success of iterative conversion cycles. Core innovations include automated SQL feature detection, semi-supervised error analysis and integration of Subject Matter Experts feedback within a systematic migration workflow. The methodology achieves significant reductions in Syntax Error Rates, enhances feature alignment throughout migration iterations, and leverages dataset sampling to ensure continual improvement. By embedding GAI into the migration process, the framework facilitates precise feature mapping, semi-automated error resolution, and data-driven optimisation loops, improving workflow efficiency.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [28] [FSL-HDnn: A 40 nm Few-shot On-Device Learning Accelerator with Integrated Feature Extraction and Hyperdimensional Computing](https://arxiv.org/abs/2512.11826)
*Weihong Xu,Chang Eun Song,Haichao Yang,Leo Liu,Meng-Fan Chang,Carlos H. Diaz,Tajana Rosing,Mingu Kang*

Main category: cs.AR

TL;DR: FSL-HDnn是一种高效能端侧少样本学习加速器，结合权重聚类特征提取与超维计算分类器，显著降低能耗与延迟。


<details>
  <summary>Details</summary>
Motivation: 解决资源受限边缘设备上端侧学习的高计算复杂度与高延迟问题。

Method: 采用权重聚类减少特征提取计算量，利用超维计算实现单次训练分类，并引入早退机制与批量单次训练优化硬件效率。

Result: 40nm芯片实测达6mJ/图像训练能效、28图像/秒吞吐率，训练延迟较现有方案降低2至20.9倍。

Conclusion: FSL-HDnn在能效与速度方面显著优于现有端侧学习芯片，适合边缘设备部署。

Abstract: This paper introduces FSL-HDnn, an energy-efficient accelerator that implements the end-to-end pipeline of feature extraction and on-device few-shot learning (FSL). The accelerator addresses fundamental challenges of on-device learning (ODL) for resource-constrained edge applications through two synergistic modules: a parameter-efficient feature extractor employing weight clustering and an FSL classifier based on hyperdimensional computing (HDC). The feature extractor exploits the weight clustering mechanism to reduce computational complexity, while the HDC-based FSL classifier eliminates gradient-based back propagation operations, enabling single-pass training with substantially reduced latency. Additionally, FSL-HDnn enables low-latency ODL and inference via two proposed optimization strategies, including an early-exit mechanism with branch feature extraction and batched single-pass training that improves hardware utilization. Measurement results demonstrate that our chip fabricated in a 40 nm CMOS process delivers superior training energy efficiency of 6 mJ/image and end-to-end training throughput of 28 images/s on a 10-way 5-shot FSL task. The end-to-end training latency is also reduced by 2x to 20.9x compared to state-of-the-art ODL chips.

</details>


### [29] [DreamRAM: A Fine-Grained Configurable Design Space Modeling Tool for Custom 3D Die-Stacked DRAM](https://arxiv.org/abs/2512.12106)
*Victor Cai,Jennifer Zhou,Haebin Do,David Brooks,Gu-Yeon Wei*

Main category: cs.AR

TL;DR: DreamRAM是一个可配置的3D堆叠DRAM建模工具，支持带宽、容量、能耗、延迟和面积的定制化设计，以满足不同应用需求。


<details>
  <summary>Details</summary>
Motivation: 现有固定设计的DRAM无法满足多样化应用对性能、功耗和面积的不同需求，因此需要一个灵活的设计空间探索工具。

Method: DreamRAM通过在MAT、子阵列、Bank等层级提供细粒度参数配置，并结合物理布线建模（如DLOMAT方案），分析3D堆叠DRAM的性能权衡。

Result: 相比基线设计，DreamRAM可实现带宽提升66%、容量翻倍、每比特功耗降低45%。

Conclusion: DreamRAM为定制化3D堆叠DRAM架构提供了强大的设计空间探索能力，有助于优化特定应用场景下的内存系统。

Abstract: 3D die-stacked DRAM has emerged as a key technology for delivering high bandwidth and high density for applications such as high-performance computing, graphics, and machine learning. However, different applications place diverse and sometimes diverging demands on power, performance, and area that cannot be universally satisfied with fixed commodity DRAM designs. Die stacking creates the opportunity for a large DRAM design space through 3D integration and expanded total die area. To open and navigate this expansive design space of customized memory architectures that cater to application-specific needs, we introduce DreamRAM, a configurable bandwidth, capacity, energy, latency, and area modeling tool for custom 3D die-stacked DRAM designs. DreamRAM exposes fine-grained design customization parameters at the MAT, subarray, bank, and inter-bank levels, including extensions of partial page and subarray parallelism proposals found in the literature, to open a large previously-unexplored design space. DreamRAM analytically models wire pitch, width, length, capacitance, and scaling parameters to capture the performance tradeoffs of physical layout and routing design choices. Routing awareness enables DreamRAM to model a custom MAT-level routing scheme, Dataline-Over-MAT (DLOMAT), to facilitate better bandwidth tradeoffs. DreamRAM is calibrated and validated against published industry HBM3 and HBM2E designs. Within DreamRAM's rich design space, we identify designs that achieve each of 66% higher bandwidth, 100% higher capacity, and 45% lower power and energy per bit compared to the baseline design, each on an iso-bandwidth, iso-capacity, and iso-power basis.

</details>


### [30] [SliceMoE: Bit-Sliced Expert Caching under Miss-Rate Constraints for Efficient MoE Inference](https://arxiv.org/abs/2512.12990)
*Yuseon Choi,Sangjin Kim,Jungjun Oh,Gwangtae Park,Byeongcheol Kim,Hoi-Jun Yoo*

Main category: cs.AR

TL;DR: SliceMoE 是一种面向设备端部署的高效 MoE 推理框架，通过动态位切片缓存和无校准量化技术显著降低能耗与延迟，同时保持高精度。


<details>
  <summary>Details</summary>
Motivation: 解决 MoE 模型因参数量大、专家卸载成本高而难以在设备端部署的问题，同时避免现有加速方法增加能耗或降低专家多样性。

Method: 提出 SliceMoE 框架，包含动态位切片缓存（DBSC）、无校准非对称套娃量化（AMAT）和预测性缓存预热（PCW）三项核心技术。

Result: 在 DeepSeek-V2-Lite 和 Qwen1.5-MoE-A2.7B 上，解码阶段能耗分别降低 2.37 倍和 2.85 倍，延迟改善 1.81 倍和 1.64 倍，精度接近高位模型。

Conclusion: 切片级缓存机制能有效支持设备端 MoE 高效部署，兼顾性能、能耗与精度。

Abstract: MoE models offer efficient scaling through conditional computation, but their large parameter size and expensive expert offloading make on-device deployment challenging. Existing acceleration techniques such as prefetching or expert clustering often increase energy usage or reduce expert diversity. We present SliceMoE, an energy-efficient MoE inference framework for miss-rate-constrained deployment. SliceMoE introduces Dynamic Bit-Sliced Caching (DBSC), which caches experts at slice-level granularity and assigns precision on demand to expand effective expert capacity. To support mixed-precision experts without memory duplication, we propose Calibration-Free Asymmetric Matryoshka Quantization (AMAT), a truncation-based scheme that maintains compatibility between low-bit and high-bit slices. We further introduce Predictive Cache Warmup (PCW) to reduce early-decode cold misses by reshaping cache contents during prefill. Evaluated on DeepSeek-V2-Lite and Qwen1.5-MoE-A2.7B, SliceMoE reduces decode-stage energy consumption by up to 2.37x and 2.85x, respectively, and improves decode latency by up to 1.81x and 1.64x, while preserving near-high-bit accuracy. These results demonstrate that slice-level caching enables an efficient on-device MoE deployment.

</details>


### [31] [An Optimal Alignment-Driven Iterative Closed-Loop Convergence Framework for High-Performance Ultra-Large Scale Layout Pattern Clustering](https://arxiv.org/abs/2512.13133)
*Shuo Liu*

Main category: cs.AR

TL;DR: 提出一种最优对齐驱动的闭环迭代框架，高效解决VLSI布局聚类问题，在压缩率和速度上显著优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 现有布局聚类方法存在计算复杂度高、采样次优、速度与质量难以兼顾等问题，制约DFM应用如OPC的发展。

Method: 结合FFT相位相关与几何极小极大策略实现最优对齐，建模为集合覆盖问题并采用惊喜值懒贪心启发式求解，辅以多级剪枝机制加速计算。

Result: 在2025中国研究生EDA精英挑战赛基准上实现93.4%压缩率和超100倍加速，秒级处理数万图案，77支队伍中排名第一。

Conclusion: 该框架在可扩展性与精度间取得最优平衡，有效攻克NP难的布局聚类问题，具备工业实用价值。

Abstract: With the aggressive scaling of VLSI technology, the explosion of layout patterns creates a critical bottleneck for DFM applications like OPC. Pattern clustering is essential to reduce data complexity, yet existing methods struggle with computational prohibitiveness ($O(N^2)$ comparisons), sub-optimal discrete sampling for center alignment, and difficult speed-quality trade-offs. To address these, we propose an Optimal Alignment-Driven Iterative Closed-Loop Convergence Framework. First, to resolve alignment ambiguity, we introduce a hybrid suite of high-performance algorithms: an FFT-based Phase Correlation method for cosine similarity constraints, and a Robust Geometric Min-Max strategy for edge displacement constraints that analytically solves for the global optimum. Second, we model clustering as a Set Cover Problem (SCP) using a Surprisal-Based Lazy Greedy heuristic within a coarse-to-fine iterative refinement loop to ensure convergence. Additionally, a multi-stage pruning mechanism filters over 99% of redundant computations. Experimental results on the 2025 China Postgraduate EDA Elite Challenge benchmark demonstrate a 93.4% compression ratio relative to raw inputs and an over 100x speedup compared to the official baseline, effectively handling tens of thousands of patterns in seconds. Securing First Place among 77 teams, this approach proves its superiority in solving the NP-Hard layout clustering problem with an optimal balance of scalability and precision.

</details>


### [32] [Striking the Balance: GEMM Performance Optimization Across Generations of Ryzen AI NPUs](https://arxiv.org/abs/2512.13282)
*Endri Taka,Andre Roesti,Joseph Melber,Pranathi Vasireddy,Kristof Denolf,Diana Marculescu*

Main category: cs.AR

TL;DR: 本文提出了一种系统化方法，优化GEMM算法在AMD Ryzen AI XDNA和XDNA2 NPU上的性能，实现业界领先的int8和bf16精度吞吐量。


<details>
  <summary>Details</summary>
Motivation: 现代深度学习工作负载对计算和内存需求高，需针对专用硬件如AMD NPU进行优化以提升性能。

Method: 利用AMD NPU架构特性，系统性优化GEMM算法并解决系统级性能瓶颈。

Result: 在int8精度下分别达到6.76 TOPS（XDNA）和38.05 TOPS（XDNA2），在bf16精度下分别达到3.14 TOPS（XDNA）和14.71 TOPS（XDNA2）。

Conclusion: 该研究为在Ryzen AI NPU上优化GEMM工作负载提供了关键性能洞察和高效实现方法。

Abstract: The high computational and memory demands of modern deep learning (DL) workloads have led to the development of specialized hardware devices from cloud to edge, such as AMD's Ryzen AI XDNA NPUs. Optimizing general matrix multiplication (GEMM) algorithms for these architectures is critical for improving DL workload performance. To this end, this paper presents a common systematic methodology to optimize GEMM workloads across the two current NPU generations, namely XDNA and XDNA2. Our implementations exploit the unique architectural features of AMD's NPUs and address key performance bottlenecks at the system level. End-to-end performance evaluation across various GEMM sizes demonstrates state-of-the-art throughput of up to 6.76 TOPS (XDNA) and 38.05 TOPS (XDNA2) for 8-bit integer (int8) precision. Similarly, for brain floating-point (bf16) precision, our GEMM implementations attain up to 3.14 TOPS (XDNA) and 14.71 TOPS (XDNA2). This work provides significant insights into key performance aspects of optimizing GEMM workloads on Ryzen AI NPUs.

</details>


### [33] [Reproducibility and Standardization in gem5 Resources v25.0](https://arxiv.org/abs/2512.13479)
*Kunal Pai,Harshil Patel,Erin Le,Noah Krim,Mahyar Samani,Bobby R. Bruce,Jason Lowe-Power*

Main category: cs.AR

TL;DR: 本文通过改进gem5模拟器和gem5资源库，标准化磁盘镜像创建、增强客户机-主机通信、支持多工作负载并行仿真，从而提升计算机体系结构研究的可复现性。


<details>
  <summary>Details</summary>
Motivation: 现有仿真工作流不一致，研究人员在自定义镜像、仿真监控和多任务协调方面面临困难，影响研究可复现性。

Method: 使用Packer标准化x86/ARM/RISC-V镜像构建；重构退出事件系统并引入超调用；开发gem5-bridge驱动和MultiSim功能实现并行仿真管理。

Result: 新增12个镜像、6个内核、200+工作负载；实现无需外部脚本的并行全系统仿真；显著降低配置复杂度并提升标准化与可复现性。

Conclusion: 所提方案有效解决了仿真研究中的关键瓶颈，为体系结构社区提供了可扩展、已验证的标准化工具链。

Abstract: Reproducibility in simulation-based computer architecture research requires coordinating artifacts like disk images, kernels, and benchmarks, but existing workflows are inconsistent. We improve gem5, an open-source simulator with over 1600 forks, and gem5 Resources, a centralized repository of over 2000 pre-packaged artifacts, to address these issues. While gem5 Resources enables artifact sharing, researchers still face challenges. Creating custom disk images is complex and time-consuming, with no standardized process across ISAs, making it difficult to extend and share images. gem5 provides limited guest-host communication features through a set of predefined exit events that restrict researchers' ability to dynamically control and monitor simulations. Lastly, running simulations with multiple workloads requires researchers to write custom external scripts to coordinate multiple gem5 simulations which creates error-prone and hard-to-reproduce workflows. To overcome this, we introduce several features in gem5 and gem5 Resources. We standardize disk-image creation across x86, ARM, and RISC-V using Packer, and provide validated base images with pre-annotated benchmark suites (NPB, GAPBS). We provide 12 new disk images, 6 new kernels, and over 200 workloads across three ISAs. We refactor the exit event system to a class-based model and introduce hypercalls for enhanced guest-host communication that allows researchers to define custom behavior for their exit events. We also provide a utility to remotely monitor simulations and the gem5-bridge driver for user-space m5 operations. Additionally, we implemented Suites and MultiSim to enable parallel full-system simulations from gem5 configuration scripts, eliminating the need for external scripting. These features reduce setup complexity and provide extensible, validated resources that improve reproducibility and standardization.

</details>


### [34] [Lyra: A Hardware-Accelerated RISC-V Verification Framework with Generative Model-Based Processor Fuzzing](https://arxiv.org/abs/2512.13686)
*Juncheng Huo,Yunfan Gao,Xinxin Liu,Sa Wang,Yungang Bao,Xitong Gao,Kan Shi*

Main category: cs.AR

TL;DR: Lyra是一个结合硬件加速与语义感知生成模型的RISC-V验证框架，显著提升覆盖率并大幅加速验证过程。


<details>
  <summary>Details</summary>
Motivation: 传统软件仿真和随机测试在复杂处理器设计中效率低下、质量不足，导致验证成本高、收敛慢。

Method: Lyra在FPGA SoC上并行执行待测设备与参考模型，并利用专门训练的生成模型LyraGen产生高质量指令序列。

Result: 相比现有软件模糊测试工具，Lyra覆盖率最高提升1.27倍，端到端验证速度提升107至3343倍，且收敛难度更低。

Conclusion: Lyra有效解决了硬件验证中的效率与质量瓶颈，为复杂处理器验证提供了高效新方案。

Abstract: As processor designs grow more complex, verification remains bottlenecked by slow software simulation and low-quality random test stimuli. Recent research has applied software fuzzers to hardware verification, but these rely on semantically blind random mutations that may generate shallow, low-quality stimuli unable to explore complex behaviors. These limitations result in slow coverage convergence and prohibitively high verification costs. In this paper, we present Lyra, a heterogeneous RISC-V verification framework that addresses both challenges by pairing hardware-accelerated verification with an ISA-aware generative model. Lyra executes the DUT and reference model concurrently on an FPGA SoC, enabling high-throughput differential checking and hardware-level coverage collection. Instead of creating verification stimuli randomly or through simple mutations, we train a domain-specialized generative model, LyraGen, with inherent semantic awareness to generate high-quality, semantically rich instruction sequences. Empirical results show Lyra achieves up to $1.27\times$ higher coverage and accelerates end-to-end verification by up to $107\times$ to $3343\times$ compared to state-of-the-art software fuzzers, while consistently demonstrating lower convergence difficulty.

</details>


<div id='cs.PF'></div>

# cs.PF [[Back]](#toc)

### [35] [EDAN: Towards Understanding Memory Parallelism and Latency Sensitivity in HPC](https://arxiv.org/abs/2512.13176)
*Siyuan Shen,Mikhail Khalilov,Lukas Gianinazzi,Timo Schneider,Marcin Chrapek,Jai Dayal,Manisha Gajbe,Robert Wisniewski,Torsten Hoefler*

Main category: cs.PF

TL;DR: EDAN是一个基于运行时指令追踪生成执行DAG的性能分析工具，用于评估程序内存延迟敏感性和硬件配置影响。


<details>
  <summary>Details</summary>
Motivation: 资源解耦虽提升系统效率，但增加内存访问延迟，需灵活高效工具评估应用延迟敏感性。

Method: 通过程序运行时指令追踪构建执行DAG，估算延迟敏感性并分析不同硬件配置影响。

Result: 成功应用于PolyBench、HPCG和LULESH等基准，揭示其内存级并行性和延迟敏感性特征。

Conclusion: EDAN能有效提供性能理论边界，并深入理解HPC应用内在的内存级并行性。

Abstract: Resource disaggregation is a promising technique for improving the efficiency of large-scale computing systems. However, this comes at the cost of increased memory access latency due to the need to rely on the network fabric to transfer data between remote nodes. As such, it is crucial to ascertain an application's memory latency sensitivity to minimize the overall performance impact. Existing tools for measuring memory latency sensitivity often rely on custom ad-hoc hardware or cycle-accurate simulators, which can be inflexible and time-consuming. To address this, we present EDAN (Execution DAG Analyzer), a novel performance analysis tool that leverages an application's runtime instruction trace to generate its corresponding execution DAG. This approach allows us to estimate the latency sensitivity of sequential programs and investigate the impact of different hardware configurations. EDAN not only provides us with the capability of calculating the theoretical bounds for performance metrics, but it also helps us gain insight into the memory-level parallelism inherent to HPC applications. We apply EDAN to applications and benchmarks such as PolyBench, HPCG, and LULESH to unveil the characteristics of their intrinsic memory-level parallelism and latency sensitivity.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [36] [Agentic AI for 6G: A New Paradigm for Autonomous RAN Security Compliance](https://arxiv.org/abs/2512.12400)
*Sotiris Chatzimiltis,Mahdi Boloursaz Mashhadi,Mohammad Shojafar,Merouane Debbah,Rahim Tafazolli*

Main category: cs.NI

TL;DR: 本文提出了一种基于LLM智能体与RAG管道的框架，用于自动化评估和执行电信RAN安全合规，支持标准比对、可解释性说明及自动修复。


<details>
  <summary>Details</summary>
Motivation: 传统安全合规方法难以应对下一代无线接入网快速演进的标准与实时变化，亟需智能化自动化解决方案。

Method: 结合大语言模型智能体与检索增强生成技术，构建可自主分析配置文件、比对O-RAN与3GPP标准并生成修复建议的系统框架。

Result: 初步案例验证了该框架在合规评估、解释生成和自动修复方面的可行性，同时识别出模型幻觉与厂商差异等关键挑战。

Conclusion: 未来需发展面向电信领域的专用大模型及标准化评估体系，以提升智能体安全性、透明度与系统可信度。

Abstract: Agentic AI systems are emerging as powerful tools for automating complex, multi-step tasks across various industries. One such industry is telecommunications, where the growing complexity of next-generation radio access networks (RANs) opens up numerous opportunities for applying these systems. Securing the RAN is a key area, particularly through automating the security compliance process, as traditional methods often struggle to keep pace with evolving specifications and real-time changes. In this article, we propose a framework that leverages LLM-based AI agents integrated with a retrieval-augmented generation (RAG) pipeline to enable intelligent and autonomous enforcement of security compliance. An initial case study demonstrates how an agent can assess configuration files for compliance with O-RAN Alliance and 3GPP standards, generate explainable justifications, and propose automated remediation if needed. We also highlight key challenges such as model hallucinations and vendor inconsistencies, along with considerations like agent security, transparency, and system trust. Finally, we outline future directions, emphasizing the need for telecom-specific LLMs and standardized evaluation frameworks.

</details>


### [37] [Efficient Resource Allocation for Multi-User and Multi-Target MIMO-OFDM Underwater ISAC](https://arxiv.org/abs/2512.12611)
*Wei Men,Longfei Zhao,Yong Liang Guan,Xiangwang Hou,Yong Ren,Dusit Niyato*

Main category: cs.NI

TL;DR: 提出一种基于交错OFDM的MIMO水声ISAC系统，通过优化资源分配，在保证感知性能和PAPR约束下显著提升通信速率与覆盖范围乘积（PRR），并实现快速收敛。


<details>
  <summary>Details</summary>
Motivation: 解决复杂水声环境中多用户通信与全向目标感知的覆盖及性能平衡难题。

Method: 构建多目标优化框架，采用二维分组随机搜索算法优化子载波交错模式与资源分配方案。

Result: 仿真表明，算法收敛速度比穷举法快90%，PRR仅下降0.5 kbps·km，且在严苛约束下保持鲁棒性。

Conclusion: 所提系统在水声ISAC场景中兼顾通信与感知性能，具备高效性和实用性。

Abstract: Integrated sensing and communication (ISAC) technology is crucial for next-generation underwater networks. However, covering multiple users and targets and balancing sensing and communication performance in complex underwater acoustic (UWA) environments remains challenging. This paper proposes an interleaved orthogonal frequency division multiplexing-based MIMO UWA-ISAC system, which employs a horizontal array to simultaneously transmit adaptive waveforms for downlink multi-user communication and omnidirectional target sensing. A multi-objective optimization framework is formulated to maximize the product of communication rate and range (PRR) while ensuring sensing performance and peak-to-average power ratio (PAPR) constraints. To solve this mixed-integer nonconvex problem, a two-dimensional grouped random search algorithm is developed, efficiently exploring subcarrier interleaved patterns and resource allocation schemes. Numerical simulations under real-world UWA channels demonstrate the designed system's superiority and effectiveness: our algorithm achieves 90% faster convergence than conventional exhaustive search with only a marginal 0.5 kbps km PRR degradation. Furthermore, the proposed resource allocation scheme maintains robustness beyond the baseline allocation schemes under stringent PRR and PAPR constraints.

</details>


### [38] [Low-Complexity Monitoring and Compensation of Transceiver IQ Imbalance by Multi-dimensional Architecture for Dual-Polarization 16 Quadrature Amplitude Modulation](https://arxiv.org/abs/2512.13266)
*Yukun Zhang,Xiaoxue Gong,Xu Zhang,Lei Guo*

Main category: cs.NI

TL;DR: 提出一种低复杂度IQ失衡补偿架构，显著减少计算量并有效补偿收发器IQ失衡。


<details>
  <summary>Details</summary>
Motivation: 降低IQ失衡补偿的计算复杂度，同时保持高性能信号恢复能力。

Method: 采用发射端色散预补偿、Gardner相位检测估计接收端IQ偏斜、最小均衡误差法估计发射端偏斜，并结合复值与实值MIMO结构进行极化解复用与失衡补偿。

Result: 在100公里36 Gbaud DP-QAM传输仿真与实验中，接收端IQ偏斜估计使实数乘法减少超70%，低复杂度MIMO均衡器使乘法运算减少51%。

Conclusion: 该架构以低复杂度高效补偿收发器IQ失衡，适用于高速光通信系统。

Abstract: In this paper, a low-complexity IQ imbalance compensation architecture is proposed, which reduces the effects of in-phase (I) and quadrature (Q) imbalance. The architecture consists of transceiver IQ skew estimation methods and a low-complexity MIMO equalizer structure. Before the IQ skew estimation, the chromatic dispersion(CD) is pre-compensated in the transmitter(TX) by chirp filtering. The receiver(RX) IQ skew is estimated by Gardner's phase detector, and the TX skew is estimated by finding the value that yields the lowest equalizer error. The low-complexity MIMO equalizer consists of a complex-valued MIMO(CV-MIMO) and a real-valued DD-LMS MIMO(RV-MIMO), which employ a butterfly and a non-butterfly structure, respectively. The CV-MIMO is used to perform polarization demultiplexing. The RV-MIMO equalizes each of the two polarisations and simultaneously compensates for the TX IQ imbalance. The architecture first compensates for the IQ skew at low-complexity, and the other imperfections are compensated by the low-complexity MIMO equalizer. Therefore, this architecture can equalize signals impaired by the transceiver IQ imbalance with low complexity. A 100 km transmission simulation and experiment with 36 Gbaud dual-polarization quadrature amplitude modulation(DP-QAM) signals and offline DSP showed that, with the RX IQ skew estimation, the number of real multiplications is reduced by more than 70% compared with conventional cases. With the low-complexity MIMO equalizer, the number of real multiplications is reduced by 51% compared with 4x4 MIMO

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [39] [Fine-Grained Energy Prediction For Parallellized LLM Inference With PIE-P](https://arxiv.org/abs/2512.12801)
*Anurag Dutt,Young Won Choi,Avirup Sil,Anshul Gandhi,Aruna Balasubramanian,Niranjan Balasubramanian*

Main category: cs.DC

TL;DR: PIE-P是一个用于多GPU环境下大语言模型推理的细粒度能耗预测框架，显著优于现有基线方法。


<details>
  <summary>Details</summary>
Motivation: 大语言模型的能耗问题日益突出，但现有能耗测量工具难以准确评估多GPU并行推理场景下的能耗。

Method: 提出PIE-P框架，通过精确采样、细粒度建模GPU间通信和合理计算并行开销来解决多GPU能耗预测难题。

Result: 实验表明，PIE-P在不同并行策略下均能提供高精度细粒度能耗预测，性能显著优于基线方法。

Conclusion: PIE-P有效填补了多GPU并行推理能耗预测的技术空白，为优化大语言模型能耗提供了实用工具。

Abstract: With the widespread adoption of Large Language Models (LLMs), energy costs of running LLMs is quickly becoming a critical concern. However, precisely measuring the energy consumption of LLMs is often infeasible because hardware-based power monitors are not always accessible and software-based energy measurement tools are not accurate. While various prediction techniques have been developed to estimate LLM energy consumption, these approaches are limited to single-GPU environments and thus are not applicable to modern LLM inference which is typically parallelized across multiple GPUs. In this work, we remedy this gap and introduce PIE-P, a fine-grained energy prediction framework for multi-GPU inference, including tensor, pipeline, and data parallelism. Predicting the energy under parallelized inference is complicated by the non-determinism in inter-GPU communication, additional communication overheads, and difficulties in isolating energy during the communication/synchronization phase. We develop a scalable prediction framework that addresses these issues via precise sampling, fine-grained modeling of inter-GPU communication, and careful accounting of parallelization overhead. Our evaluation results show that PIE-P yields accurate and fine-grained energy predictions across parallelism strategies, significantly outperforming baselines.

</details>


### [40] [astroCAMP: A Community Benchmark and Co-Design Framework for Sustainable SKA-Scale Radio Imaging](https://arxiv.org/abs/2512.13591)
*Denisa-Andreea Constantinescu,Rubén Rodríguez Álvarez,Jacques Morin,Etienne Orliac,Mickaël Dardaillon,Sunrise Wang,Hugo Miomandre,Miguel Peón-Quirós,Jean-François Nezan,David Atienza*

Main category: cs.DC

TL;DR: astroCAMP框架旨在优化SKA项目成像管线与高性能计算架构的协同设计，提升科学产出并降低能耗与成本。


<details>
  <summary>Details</summary>
Motivation: 当前射电干涉成像管线效率低下且缺乏标准化评估指标，限制了硬件-软件协同优化与可持续性发展。

Method: 提出astroCAMP框架，包含统一指标体系、标准数据集与多目标协同设计模型，并在CPU、GPU及FPGA平台上进行评估。

Result: 成功展示框架在异构平台上的设计空间探索能力，可识别SKA规模部署下的帕累托最优运行点。

Conclusion: 呼吁SKA社区建立可量化保真度指标，以加速下一代成像管线的系统化优化。

Abstract: The Square Kilometre Array (SKA) project will operate one of the world's largest continuous scientific data systems, sustaining petascale imaging under strict power caps. Yet, current radio-interferometric pipelines utilize only a small fraction of hardware peak performance, typically 4-14%, due to memory and I/O bottlenecks, resulting in poor energy efficiency and high operational and carbon costs. Progress is further limited by the absence of standardised metrics and fidelity tolerances, preventing principled hardware-software co-design and rigorous exploration of quality-efficiency trade-offs. We introduce astroCAMP, a framework for guiding the co-design of next-generation imaging pipelines and sustainable HPC architectures that maximise scientific return within SKA's operational and environmental limits. astroCAMP provides: (1) a unified, extensible metric suite covering scientific fidelity, computational performance, sustainability, and lifecycle economics; (2) standardised SKA-representative datasets and reference outputs enabling reproducible benchmarking across CPUs, GPUs, and emerging accelerators; and (3) a multi-objective co-design formulation linking scientific-quality constraints to time-, energy-, carbon-to-solution, and total cost of ownership. We release datasets, benchmarking results, and a reproducibility kit, and evaluate co-design metrics for WSClean and IDG on an AMD EPYC 9334 processor and an NVIDIA H100 GPU. Further, we illustrate the use of astroCAMP for heterogeneous CPU-FPGA design-space exploration, and its potential to facilitate the identification of Pareto-optimal operating points for SKA-scale imaging deployments. Last, we make a call to the SKA community to define quantifiable fidelity metrics and thresholds to accelerate principled optimisation for SKA-scale imaging.

</details>


### [41] [Accelerating Sparse Matrix-Matrix Multiplication on GPUs with Processing Near HBMs](https://arxiv.org/abs/2512.12036)
*Shiju Li,Younghoon Min,Hane Yie,Hoshik Kim,Soohong Ahn,Joonseop Sim,Chul-Ho Lee,Jongryool Kim*

Main category: cs.DC

TL;DR: 本文提出基于哈希的多阶段SpGEMM与AIA近内存处理技术，显著提升GPU上稀疏矩阵乘法性能。


<details>
  <summary>Details</summary>
Motivation: 解决SpGEMM在科学计算和数据分析中因不规则内存访问导致的性能瓶颈问题。

Method: 采用硬件-软件协同设计，结合哈希多阶段算法与近内存加速技术优化GPU HBM上的SpGEMM。

Result: 在图分析任务中相较cuSPARSE最高提速76.5%，GNN训练平均提速1.43倍，大规模数据集最高达4.18倍。

Conclusion: 所提方法在多种图应用负载下显著优于现有方案，具备广泛实用价值。

Abstract: Sparse General Matrix-Matrix Multiplication (SpGEMM) is a fundamental operation in numerous scientific computing and data analytics applications, often bottlenecked by irregular memory access patterns. This paper presents Hash based Multi-phase SpGEMM on GPU and the Acceleration of Indirect Memory Access (AIA) technique, a novel custom near-memory processing approach to optimizing SpGEMM on GPU HBM. Our hardware-software co-designed framework for SpGEMM demonstrates significant performance improvements over state-of-the-art methods, particularly in handling complex, application-specific workloads. We evaluate our approach on various graph workloads, including graph contraction, Markov clustering, and Graph Neural Networks (GNNs), showcasing its practical applicability. For graph analytics applications, AIA demonstrates up to 17.3% time reduction from the software-only implementation, while achieving time reduction of 76.5% for Graph Contraction and 58.4% for Markov Clustering compared to cuSPARSE. For GNN training applications with structured global pruning, our hybrid approach delivers an average of 1.43x speedup over software-only implementation across six benchmark datasets and three architectures (GCN, GIN, GraphSAGE), and shows 1.95x speedup for GNN workloads when compared to cuSPARSE, with up to 4.18x gains on large-scale datasets.

</details>


### [42] [Near-Zero-Overhead Freshness for Recommendation Systems via Inference-Side Model Updates](https://arxiv.org/abs/2512.12295)
*Wenjun Yu,Sitian Chen,Cheng Chen,Amelie Chi Zhou*

Main category: cs.DC

TL;DR: LiveUpdate利用推理节点的闲置资源，通过低秩适配技术实现模型实时更新，显著降低同步开销并提升推荐准确率。


<details>
  <summary>Details</summary>
Motivation: 解决深度学习推荐模型因参数同步延迟导致的推荐质量下降问题。

Method: 在推理节点内嵌入低秩适配训练器，结合动态秩调整与NUMA感知调度，消除跨集群同步需求。

Result: 相比基线方法，更新成本降低2倍，1小时内准确率提升0.04%-0.24%，P99延迟影响小于20ms。

Conclusion: LiveUpdate有效将闲置推理资源转化为实时更新引擎，在保证低延迟的同时显著提升模型新鲜度与准确性。

Abstract: Deep Learning Recommendation Models (DLRMs) underpin personalized services but face a critical freshness-accuracy tradeoff due to massive parameter synchronization overheads. Production DLRMs deploy decoupled training/inference clusters, where synchronizing petabyte-scale embedding tables (EMTs) causes multi-minute staleness, degrading recommendation quality and revenue. We observe that (1) inference nodes exhibit sustained CPU underutilization (peak <= 20%), and (2) EMT gradients possess intrinsic low-rank structure, enabling compact update representation. We present LiveUpdate, a system that eliminates inter-cluster synchronization by colocating Low-Rank Adaptation (LoRA) trainers within inference nodes. LiveUpdate addresses two core challenges: (1) dynamic rank adaptation via singular value monitoring to constrain memory overhead (<2% of EMTs), and (2) NUMA-aware resource scheduling with hardware-enforced QoS to eliminate update inference contention (P99 latency impact <20ms). Evaluations show LiveUpdate reduces update costs by 2x versus delta-update baselines while achieving higher accuracy within 1-hour windows. By transforming idle inference resources into freshness engines, LiveUpdate delivers online model updates while outperforming state-of-the-art delta-update methods by 0.04% to 0.24% in accuracy.

</details>


### [43] [A Conflict-Aware Resource Management Framework for the Computing Continuum](https://arxiv.org/abs/2512.12299)
*Vlad Popescu-Vifor,Ilir Murturi,Praveen Kumar Donta,Schahram Dustdar*

Main category: cs.DC

TL;DR: 提出基于深度强化学习的自适应冲突解决框架，优化边缘-雾-云计算连续体中的资源编排。


<details>
  <summary>Details</summary>
Motivation: 设备异构性和去中心化需求导致资源编排中出现持续冲突、效率低下和服务性能下降问题。

Method: 采用深度强化学习模型，结合实时性能反馈与历史状态信息，动态调解跨部署的资源冲突。

Result: 在Kubernetes测试平台上验证了框架的可行性与弹性，实现了高效资源再分配与动态场景下的自适应学习。

Conclusion: 该框架为计算连续体中的冲突感知编排提供了可扩展且强韧的解决方案。

Abstract: The increasing device heterogeneity and decentralization requirements in the computing continuum (i.e., spanning edge, fog, and cloud) introduce new challenges in resource orchestration. In such environments, agents are often responsible for optimizing resource usage across deployed services. However, agent decisions can lead to persistent conflict loops, inefficient resource utilization, and degraded service performance. To overcome such challenges, we propose a novel framework for adaptive conflict resolution in resource-oriented orchestration using a Deep Reinforcement Learning (DRL) approach. The framework enables handling resource conflicts across deployments and integrates a DRL model trained to mediate such conflicts based on real-time performance feedback and historical state information. The framework has been prototyped and validated on a Kubernetes-based testbed, illustrating its methodological feasibility and architectural resilience. Preliminary results show that the framework achieves efficient resource reallocation and adaptive learning in dynamic scenarios, thus providing a scalable and resilient solution for conflict-aware orchestration in the computing continuum.

</details>


### [44] [Strategic Server Deployment under Uncertainty in Mobile Edge Computing](https://arxiv.org/abs/2512.12532)
*Duc A. Tran,Dung Truong,Duy Le*

Main category: cs.DC

TL;DR: 本文提出一种基于随机双层优化的边缘服务器部署方法，以应对用户负载和服务器容量的不确定性，通过子模函数近似结合贪心算法实现高效求解。


<details>
  <summary>Details</summary>
Motivation: 移动边缘计算中服务器部署需兼顾计算效率与通信效率，但实际场景中用户负载和服务器容量具有未知性和时变性，亟需鲁棒解决方案。

Method: 将问题建模为随机双层优化，并利用子模函数近似目标函数，结合贪心算法进行高效求解。

Result: 在真实数据集上验证，所提算法相较其他方法最高可提升55%性能。

Conclusion: 该方法能有效应对动态不确定环境，在计算与通信效率之间取得可持续平衡。

Abstract: Server deployment is a fundamental task in mobile edge computing: where to place the edge servers and what user cells to assign to them. To make this decision is context-specific, but common goals are 1) computing efficiency: maximize the amount of workload processed by the edge, and 2) communication efficiency: minimize the communication cost between the cells and their assigned servers. We focus on practical scenarios where the user workload in each cell is unknown and time-varying, and so are the effective capacities of the servers. Our research problem is to choose a subset of candidate servers and assign them to the user cells such that the above goals are sustainably achieved under the above uncertainties. We formulate this problem as a stochastic bilevel optimization, which is strongly NP-hard and unseen in the literature. By approximating the objective function with submodular functions, we can utilize state-of-the-art greedy algorithms for submodular maximization to effectively solve our problem. We evaluate the proposed algorithm using real-world data, showing its superiority to alternative methods; the improvement can be as high as 55%

</details>


### [45] [Ethical Risk Analysis of L2 Rollups](https://arxiv.org/abs/2512.12732)
*Georgy Ishmaev,Emmanuelle Anceaume,Davide Frey,François Taïani*

Main category: cs.DC

TL;DR: 该论文分析Layer 2 rollup架构中的伦理风险，发现多数项目存在即时升级与提款冻结等用户资金风险，并提出缓解策略。


<details>
  <summary>Details</summary>
Motivation: 探讨Layer 2 rollup中因运营商设计和治理结构引发的用户伦理风险问题。

Method: 采用伦理风险分析框架，结合L2BEAT项目快照与2022-2025年事故数据，构建角色决策权与风险暴露分类模型。

Result: 86%项目支持无退出窗口的即时升级，约50%项目允许运营商冻结提款；事故多集中于排序器活跃性与交易包含性。

Conclusion: 需通过技术组件改进与治理机制优化，缓解由控制权分配引发的伦理风险。

Abstract: Layer 2 rollups improve throughput and fees, but can reintroduce risk through operator discretion and information asymmetry. We ask which operator and governance designs produce ethically problematic user risk. We adapt Ethical Risk Analysis to rollup architectures, build a role-based taxonomy of decision authority and exposure, and pair the framework with two empirical signals, a cross sectional snapshot of 129 projects from L2BEAT and a hand curated incident set covering 2022 to 2025. We analyze mechanisms that affect risks to users funds, including upgrade timing and exit windows, proposer liveness and whitelisting, forced inclusion usability, and data availability choices. We find that ethical hazards rooted in L2 components control arrangements are widespread: instant upgrades without exit windows appear in about 86 percent of projects, and proposer controls that can freeze withdrawals in about 50 percent. Reported incidents concentrate in sequencer liveness and inclusion, consistent with these dependencies. We translate these findings into ethically grounded suggestions on mitigation strategies including technical components and governance mechanisms.

</details>


### [46] [PROSERVE: Unified Multi-Priority Request Scheduling for LLM Serving](https://arxiv.org/abs/2512.12928)
*Weizhe Huang,Tao Peng,Tongxuan Liu,Donghe Jin,Xianzhe Dong,Ke Zhang*

Main category: cs.DC

TL;DR: PROSERVE是一个两层调度框架，旨在通过动态批处理和智能路由优化多优先级LLM请求的服务收益。


<details>
  <summary>Details</summary>
Motivation: 现有LLM调度器未能兼顾SLO达成与客户优先级差异，导致高价值业务请求缺乏性能保障。

Method: 提出SlideBatching引擎层动态批处理机制与GoRouting服务层能力感知路由策略，联合优化服务收益最大化。

Result: 在四个开源数据集和工业实测中，系统增益提升最高35%，SLO达成率提高最高52%。

Conclusion: PROSERVE有效解决了多优先级LLM请求调度问题，显著提升系统整体服务质量与商业价值。

Abstract: The widespread deployment of large language models (LLMs) for interactive applications necessitates serving systems that can handle thousands of concurrent requests with diverse Service Level Objective (SLO) requirements. A critical yet often overlooked dimension in this context is the inherent priority difference among clients; for instance, business-critical functions demand higher performance guarantees, as fulfilling such requests yields significantly greater business value. However, existing LLM serving schedulers fail to jointly optimize for both SLO attainment and client-level priorities.
  To bridge this gap, we first \textit{formalize multi-priority request scheduling as a service gain maximization problem}, where satisfying latency requirements for requests of different priorities contributes varying levels of gain. We then propose PROSERVE, a unified two-tier scheduling framework designed to maximize overall service gain. At the engine level, SlideBatching dynamically adapts batch formation and request ordering under varying load conditions, employing a sliding boundary mechanism to balance deadline-first and density-first strategies. At the service level, GoRouting performs gain-oriented and capability-aware dispatching across distributed instances, proactively reserving capacity for future high-priority or long requests. Extensive evaluation across four open-source datasets and a real-world industrial trace demonstrates that \systemname{} consistently outperforms state-of-the-art baselines, improving system gain by up to 35% and boosting SLO attainment by up to 52%.

</details>


### [47] [Design in Tiles: Automating GEMM Deployment on Tile-Based Many-PE Accelerators](https://arxiv.org/abs/2512.13638)
*Aofeng Shen,Chi Zhang,Yakup Budanaz,Alexandru Calotoiu,Torsten Hoefler,Luca Benini*

Main category: cs.DC

TL;DR: DiT是一个自动化框架，用于简化基于Tile的多处理单元加速器的编程，并在GEMM任务中实现比NVIDIA GH200更高的性能。


<details>
  <summary>Details</summary>
Motivation: 当前基于Tile的加速器虽然在GEMM上性能优异，但编程困难，软件映射与硬件设计深度耦合，难以手动部署。

Method: 提出名为“Design in Tiles (DiT)”的自动化框架，结合部署工具链和可配置执行模型，实现软硬件协同优化。

Result: 在32x32 Tile、1979 TFLOPS@FP8、4TB/s带宽的大型配置下，相比GH200专家调优库，PE利用率更高，速度提升1.2-2.0倍。

Conclusion: DiT框架有效解决了Tile加速器编程复杂性问题，并在性能上超越现有商业方案。

Abstract: Tile-based many-Processing Element (PE) accelerators can achieve competitive performance on General Matrix Multiplication (GEMM), but they are extremely hard to program, as their optimal software mapping is deeply coupled with hardware design which is unwieldy to manual deployment. We propose "Design in Tiles (DiT)", an automated framework connecting a deployment toolchain with a configurable executable model for these accelerators. For evaluation, we apply our framework to GEMM targeting a large acceleration configuration (e.g., 32x32 tiles, 1979 TFLOPS@FP8, 4 TB/s Bandwidth) comparable to an NVIDIA GH200. We achieve higher PE utilization than GH200 with its expert-tuned GEMM libraries, achieving 1.2-2.0x speedup across diverse matrix shapes.

</details>


### [48] [Temporal parallelisation of continuous-time maximum-a-posteriori trajectory estimation](https://arxiv.org/abs/2512.13319)
*Hassan Razavi,Ángel F. García-Fernández,Simo Särkkä*

Main category: cs.DC

TL;DR: 本文提出了一种并行时间方法，用于加速部分观测随机微分方程的连续时间最大后验轨迹估计，并在GPU实验中验证了其计算效率和精度。


<details>
  <summary>Details</summary>
Motivation: 提升在并行架构上计算部分观测随机微分方程状态的最大后验估计的速度。

Method: 基于Onsager-Machlup泛函将MAP估计问题重构为最优控制问题，并利用并行关联扫描算法求解；在线性高斯情形下推导出并行Kalman-Bucy滤波器和Rauch-Tung-Striebel平滑器，再通过泰勒展开推广到非线性模型。

Result: GPU实验表明，该框架在保持与序列算法相同精度的同时，显著提升了计算速度。

Conclusion: 所提并行方法有效兼顾了计算效率与估计精度，适用于线性和非线性连续时间状态空间模型。

Abstract: This paper proposes a parallel-in-time method for computing continuous-time maximum-a-posteriori (MAP) trajectory estimates of the states of partially observed stochastic differential equations (SDEs), with the goal of improving computational speed on parallel architectures. The MAP estimation problem is reformulated as a continuous-time optimal control problem based on the Onsager-Machlup functional. This reformulation enables the use of a previously proposed parallel-in-time solution for optimal control problems, which we adapt to the current problem. The structure of the resulting optimal control problem admits a parallel solution based on parallel associative scan algorithms. In the linear Gaussian special case, it yields a parallel Kalman-Bucy filter and a parallel continuous-time Rauch-Tung-Striebel smoother. These linear computational methods are further extended to nonlinear continuous-time state-space models through Taylor expansions. We also present the corresponding parallel two-filter smoother. The graphics processing unit (GPU) experiments on linear and nonlinear models demonstrate that the proposed framework achieves a significant speedup in computations while maintaining the accuracy of sequential algorithms.

</details>


### [49] [SIGMA: An AI-Empowered Training Stack on Early-Life Hardware](https://arxiv.org/abs/2512.13488)
*Lei Qu,Lianhai Ren,Peng Cheng,Rui Gao,Ruizhe Wang,Tianyu Chen,Xiao Liu,Xingjian Zhang,Yeyun Gong,Yifan Xiong,Yucheng Ding,Yuting Jiang,Zhenghao Lin,Zhongxin Guo,Ziyue Yang*

Main category: cs.DC

TL;DR: SIGMA是一个开源训练栈，旨在提升早期AI加速器上大规模分布式训练的可靠性、稳定性和效率，其核心LUCIA训练平台显著提高集群利用率并降低故障恢复时间，成功支撑200B MoE模型高效稳定训练。


<details>
  <summary>Details</summary>
Motivation: 解决早期AI加速器在大规模训练中面临的系统中断频繁、数值错误、并行优化复杂等三大核心挑战。

Method: 构建LUCIA TRAINING PLATFORM（LTP）系统优化集群运行，并基于LTP开发LUCIA TRAINING FRAMEWORK（LTF）框架实现高效稳定的大模型训练。

Result: LTP实现94.45%有效加速器利用率，大幅减少节点回收与作业恢复时间；LTF在2048个加速器上以21.08% MFU训练200B MoE模型，75天仅一次稳定性事故，达到SOTA下游精度。

Conclusion: SIGMA为早期AI硬件提供高可靠、高效率、低成本的大规模训练解决方案，树立AI基础设施新标杆，推动AI能力与可扩展性显著进步。

Abstract: An increasing variety of AI accelerators is being considered for large-scale training. However, enabling large-scale training on early-life AI accelerators faces three core challenges: frequent system disruptions and undefined failure modes that undermine reliability; numerical errors and training instabilities that threaten correctness and convergence; and the complexity of parallelism optimization combined with unpredictable local noise that degrades efficiency. To address these challenges, SIGMA is an open-source training stack designed to improve the reliability, stability, and efficiency of large-scale distributed training on early-life AI hardware. The core of this initiative is the LUCIA TRAINING PLATFORM (LTP), the system optimized for clusters with early-life AI accelerators. Since its launch in March 2025, LTP has significantly enhanced training reliability and operational productivity. Over the past five months, it has achieved an impressive 94.45% effective cluster accelerator utilization, while also substantially reducing node recycling and job-recovery times. Building on the foundation of LTP, the LUCIA TRAINING FRAMEWORK (LTF) successfully trained SIGMA-MOE, a 200B MoE model, using 2,048 AI accelerators. This effort delivered remarkable stability and efficiency outcomes, achieving 21.08% MFU, state-of-the-art downstream accuracy, and encountering only one stability incident over a 75-day period. Together, these advances establish SIGMA, which not only tackles the critical challenges of large-scale training but also establishes a new benchmark for AI infrastructure and platform innovation, offering a robust, cost-effective alternative to prevailing established accelerator stacks and significantly advancing AI capabilities and scalability. The source code of SIGMA is available at https://github.com/microsoft/LuciaTrainingPlatform.

</details>


### [50] [Janus: Disaggregating Attention and Experts for Scalable MoE Inference](https://arxiv.org/abs/2512.13525)
*Zhexiang Zhang,Ye Wang,Xiangyu Wang,Yumiao Zhao,Jingzhe Jiang,Qizhen Weng,Shaohuai Shi,Yin Chen,Minchen Yu*

Main category: cs.DC

TL;DR: Janus 是一种可扩展的 MoE 推理系统，通过将注意力和专家模块分离部署在不同 GPU 子集群上，实现高效资源管理与独立扩展。


<details>
  <summary>Details</summary>
Motivation: 现有方案因统一资源配置导致 MoE 模型推理存在资源低效和扩展性差的问题。

Method: 提出自适应两阶段通信、轻量级调度器内核及细粒度资源管理三项关键技术。

Result: 实验表明 Janus 在满足延迟要求下，每 GPU 吞吐量最高提升 3.9 倍。

Conclusion: Janus 显著提升了 MoE 模型推理效率与可扩展性。

Abstract: Large Mixture-of-Experts (MoE) model inference is challenging due to high resource demands and dynamic workloads. Existing solutions often deploy the entire model as a single monolithic unit, which applies a unified resource configuration to both attention and expert modules despite their different requirements, leading to limited scalability and resource inefficiency. In this paper, we propose Janus, a scalable MoE inference system that disaggregates attention and experts on separate GPU sub-clusters, enabling each module to be managed and scaled independently. Janus incorporates three key designs for efficient, disaggregated MoE inference. First, it proposes an adaptive two-phase communication scheme that exploits intra- and inter-node bandwidth hierarchies for low-latency data exchange. Second, motivated by the memory-bound nature of MoE modules, Janus introduces a lightweight scheduler and implements it as a GPU kernel to balance the number of activated experts across GPUs at minimal overhead, thereby reducing inference latency. Third, Janus performs fine-grained resource management to dynamically adjust expert placement and independently scale attention and MoE resources to improve overall efficiency. Evaluation shows Janus achieves up to 3.9 higher perGPU throughput than state-of-the-art systems while meeting per-token latency requirements.

</details>
