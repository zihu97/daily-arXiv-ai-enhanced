<div id=toc></div>

# Table of Contents

- [cs.DC](#cs.DC) [Total: 10]
- [cs.SE](#cs.SE) [Total: 5]
- [cs.MA](#cs.MA) [Total: 1]
- [cs.NI](#cs.NI) [Total: 2]


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [1] [It's Not Just Timestamps: A Study on Docker Reproducibility](https://arxiv.org/abs/2602.17678)
*Oreofe Solarin*

Main category: cs.DC

TL;DR: 研究发现在无需配置调整时，仅2.7%的Dockerfile构建可实现比特级复现；配置优化后复现率提升18.6%，但仍有78.7%无法复现，开发者操作是主要原因。


<details>
  <summary>Details</summary>
Motivation: 验证Docker构建作为软件供应链完整性检查的可行性，探索容器镜像哈希一致性在实际项目中的实现情况。

Method: 构建Docker测量管道，对GitHub中2000个含Dockerfile的仓库进行分层抽样测试，分析 activité图像构建成功率与比特复现率。

Result: 仅56%仓库能成功构建镜像，基础环境下仅2.7%可比特复现；调整基础设施配置后复现率提高18.6%，但总失败率达78.7%。主要故障源于时间戳、缓存残留及开发者决策。

Conclusion: 提出Dockerfile编写指南，建议通过改进开发规范和集成CI工具（如linters）提升容器复现性，解决日志、版本浮动等可控制因素。

Abstract: Reproducible container builds promise a simple integrity check for software supply chains: rebuild an image from its Dockerfile and compare hashes. We build a Docker measurement pipeline and apply it to a stratified sample of 2,000 GitHub repositories that contained a Dockerfile. We found that only 56% produce any buildable image, and just 2.7% of those are bitwise reproducible without any infrastructure configurations. After modifying infrastructure configurations, we raise bitwise reproducibility by 18.6%, but 78.7% of buildable Dockerfiles remain non-reproducible. We analyze the root causes of the remaining differences, and find that beyond timestamps and metadata, developer-controlled choices such as uncleaned caches, logs, documentation, and floating versions are dominant causes of non-reproducibility. We derive concrete Dockerfile guidelines from these patterns and discuss how they can inform future linters and Continuous Integration (CI) checks for reproducible containers.

</details>


### [2] [Message-Oriented Middleware Systems: Technology Overview](https://arxiv.org/abs/2602.17774)
*Wael Al-Manasrah,Zuhair AlSader,Tim Brecht,Ahmed Alquraan,Samer Al-Kiswany*

Main category: cs.DC

TL;DR: 对10种开源消息中间件系统的研究揭示其向云应用框架的演进及开源社区整合契机。


<details>
  <summary>Details</summary>
Motivation: 旨在系统评估开源MOM系统特性，为开发者和从业者提供特性对比依据。

Method: 选取10个主流MOM系统，采用42项特征（含134种配置选项）的量化分析方法。

Result: MOM系统通过事务支持、资源管理等核心模块支持云应用，但存在开源项目分散现象。

Conclusion: 建立可验证标注数据集并开源，提出社区应集中发展核心项目以提升开发效率。

Abstract: We present a comprehensive characterization study of open-source message-oriented middleware (MOM) systems. We followed a rigorous methodology to select and study ten popular and diverse MOM systems. For each system, we examine 42 features with a total of 134 different options. We found that MOM systems have evolved to provide a framework for modern cloud applications through high flexibility and configurability and by offering core building blocks for complex applications including transaction support, active messaging, resource management, flow control, and native support for multi-tenancy. We also identify that there is an opportunity for the community to consolidate its efforts on fewer open-source projects.
  We have also created an annotated data set that makes it easy to verify our findings, which can also be used to help practitioners and developers understand and compare the features of different systems. For a wider impact, we make our data set publicly available.

</details>


### [3] [Collaborative Processing for Multi-Tenant Inference on Memory-Constrained Edge TPUs](https://arxiv.org/abs/2602.17808)
*Nathan Ng,Walid A. Hanafy,Prashanthi Kadambi,Balachandra Sunil,Ayush Gupta,David Irwin,Yogesh Simmhan,Prashant Shenoy*

Main category: cs.DC

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: IoT applications are increasingly relying on on-device AI accelerators to ensure high performance, especially in limited connectivity and safety-critical scenarios. However, the limited on-chip memory of these accelerators forces inference runtimes to swap model segments between host and accelerator memory, substantially inflating latency. While collaborative processing by partitioning the model processing between CPU and accelerator resources can reduce accelerator memory pressure and latency, naive partitioning may worsen end-to-end latency by either shifting excessive computation to the CPU or failing to sufficiently curb swapping, a problem that is further amplified in multi-tenant and dynamic environments.
  To address these issues, we present SwapLess, a system for adaptive, multi-tenant TPU-CPU collaborative inference for memory-constrained Edge TPUs. SwapLess utilizes an analytic queueing model that captures partition-dependent CPU/TPU service times as well as inter- and intra-model swapping overheads across different workload mixes and request rates. Using this model, SwapLess continuously adjusts both the partition point and CPU core allocation online to minimize end-to-end response time with low decision overhead. An implementation on Edge TPU-equipped platforms demonstrates that SwapLess reduces mean latency by up to 63.8% for single-tenant workloads and up to 77.4% for multi-tenant workloads relative to the default Edge TPU compiler.

</details>


### [4] [Faster Parallel Batch-Dynamic Algorithms for Low Out-Degree Orientation](https://arxiv.org/abs/2602.17811)
*Guy Blelloch,Andrew Brady,Laxman Dhulipala,Jeremy Fineman,Kishen Gowda,Chase Hutton*

Main category: cs.DC

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: A low out-degree orientation directs each edge of an undirected graph with the goal of minimizing the maximum out-degree of a vertex. In the parallel batch-dynamic setting, one can insert or delete batches of edges, and the goal is to process the entire batch in parallel with work per edge similar to that of a single sequential update and with span (or depth) for the entire batch that is polylogarithmic. In this paper we present faster parallel batch-dynamic algorithms for maintaining a low out-degree orientation of an undirected graph. All results herein achieve polylogarithmic depth, with high probability (whp); the focus of this paper is on minimizing the work, which varies across results.
  Our first result is the first parallel batch-dynamic algorithm to maintain an asymptotically optimal orientation with asymptotically optimal expected work bounds, in an amortized sense, improving over the prior best work bounds of Liu et al.~[SPAA~'22] by a logarithmic factor.
  Our second result is a $O(c \log n)$ orientation algorithm with expected worst-case $O(\sqrt{\log n})$ work per edge update, where $c$ is a known upper-bound on the arboricity of the graph. This matches the best-known sequential worst-case $O(c \log n)$ orientation algorithm given by Berglin and Brodal ~[Algorithmica~'18], albeit in expectation.
  Our final result is a $O(c + \log n)$-orientation algorithm with $O(\log^2 n)$ expected worst-case work per edge update. This algorithm significantly improves upon the recent result of Ghaffari and Koo~[SPAA~'25], which maintains a $O(c)$-orientation with $O(\log^9 n)$ worst-case work per edge whp.

</details>


### [5] [GPU Memory and Utilization Estimation for Training-Aware Resource Management: Opportunities and Limitations](https://arxiv.org/abs/2602.17817)
*Ehsan Yousefzadeh-Asl-Miandoab,Reza Karimzadeh,Danyal Yorulmaz,Bulat Ibragimov,Pınar Tözün*

Main category: cs.DC

TL;DR: 该论文系统评估了深度学习任务协同定位中GPU资源估计的三种范式（分析模型、CPU侧库和ML基估计器），揭示了准确性、泛化性和开销的关键权衡，并验证了现实模型的性能。


<details>
  <summary>Details</summary>
Motivation: 解决GPU任务协同定位的资源争用问题，避免内存溢出风险和性能下降，需准确的资源估计以实现鲁棒协同定位和干扰感知调度。

Method: 构建合成数据集涵盖MLPs、CNNs和Transformers架构变体；系统分析代表性估计器（Horus、PyTorch FakeTensor和轻量级ML基估计器），训练MLP和Transformer基内存预测模型，评估准确度、泛化性和实践开销。

Result: 分析模型硬件依赖性高，CPU库集成成本大且侵入性，ML基估计器跨架构泛化困难；利用率估计非累计算性且硬件敏感，实验揭示了范式间核心权衡并验证了未见过模型的预测可靠性。

Conclusion: 现有估计器面临重大泛化与集成挑战，释放的数据集和工具支持后续研究，以应对GPU异构性和资源争用问题。

Abstract: Collocating deep learning training tasks improves GPU utilization but causes drastic slowdowns due to resource contention and risks Out-of-Memory (OOM) failures. Accurate memory estimation is essential for robust collocation, while GPU utilization -- a key proxy for resource contention -- enables interference-aware scheduling to reduce slowdowns and improve throughput. Existing GPU memory estimators span three paradigms -- analytical models, CPU-side libraries, and ML-based estimators -- each with distinct limitations: dependence on detailed model specifications, intrusive integration, poor generalization, and varying latency overhead. GPU heterogeneity further complicates estimation, as identical tasks can exhibit markedly different memory footprints across hardware generations. GPU utilization remains comparatively understudied, further complicated by the non-additive nature of utilization metrics and hardware sensitivity. We conduct a systematic analysis of representative estimators from each paradigm -- Horus, PyTorch FakeTensor, and our lightweight ML-based estimator -- evaluating accuracy, generalizability, and practical overhead. We construct a synthetic dataset spanning MLPs, CNNs, and Transformers with controlled architectural variations, and train MLP- and Transformer-based estimators for memory prediction. We further experiment with utilization estimation on the same dataset. Our evaluation reveals key tradeoffs and validates estimators against real-world unseen models. Significant challenges remain: analytical models are hardware-dependent, CPU-side libraries impose intrusive integration costs, and ML-based estimators struggle with cross-architecture generalization. We release all datasets, tools, and artifacts to support further research.

</details>


### [6] [Distributed Triangle Enumeration in Hypergraphs](https://arxiv.org/abs/2602.17834)
*Duncan Adamson,Will Rosenbaum,Paul G. Spirakis*

Main category: cs.DC

TL;DR: 本文系统性地研究了在超图中分布式子超图列举的问题，引入了新计算模型，设计高效算法，并分析其最优性。


<details>
  <summary>Details</summary>
Motivation: 子图检测在分布式算法中意义重大，但因超图领域研究不足，本论文旨在填补这一空白。

Method: 引入推广图的CONGEST模型到超图的多个计算模型；设计分布式三角列举算法；定义稀疏超图类并提出通用算法设计技术。

Result: 在某些模型下证明三角列举算法的最优性；在稀疏超图中开发高效算法；提供了一套实用工具箱。

Conclusion: 本研究建立了分布式子超图枚举的基础框架，为未来高效算法设计铺平了道路。

Abstract: In the last decade, subgraph detection and enumeration have emerged as a central problem in distributed graph algorithms. This is largely due to the theoretical challenges and practical applications of these problems. In this paper, we initiate the systematic study of distributed sub-hypergraph enumeration in hypergraphs. To this end, we (1)~introduce several computational models for hypergraphs that generalize the CONGEST model for graphs and evaluate their relative computational power, (2)~devise algorithms for distributed triangle enumeration in our computational models and prove their optimality in two such models, (3)~introduce classes of sparse and ``everywhere sparse'' hypergraphs and describe efficient distributed algorithms for triangle enumeration in these classes, and (4)~describe general techniques that we believe to be useful for designing efficient algorithms in our hypergraph models.

</details>


### [7] [Joint Training on AMD and NVIDIA GPUs](https://arxiv.org/abs/2602.18007)
*Jon Hu,Thomas Jia,Jing Zhu,Zhendong Yu*

Main category: cs.DC

TL;DR: 提出跨AMD-NVIDIA异构环境的混合训练方案，实现高效数据传输和近同构系统性能


<details>
  <summary>Details</summary>
Motivation: 大模型训练对算力需求激增，单一供应商集群已无法满足需求，需解决异构硬件混合训练问题

Method: 1. 兼容性方案：基于CPU转发通信，分并行组选择通信后端与多网卡并行传输；2. 设备直通方案：采用CPU卸载的点对点机制，实现跨供应商GPU直传数据

Result: 在LLaMA-8B和Qwen2-7B测试中，设备直通方案达到NVIDIA同构系统98%吞吐量，保持训练稳定性与正确性

Conclusion: 设备直通通信方案高效解决跨供应商异构训练难题，性能逼近同构系统

Abstract: As large language models continue to scale, training demands on compute and system capacity grow rapidly, making single-vendor homogeneous clusters insufficient. This paper presents a technical solution for heterogeneous mixed training in AMD-NVIDIA environments. We first adopt a compatibility-oriented approach based on CPU-Forwarding Communication, with differentiated communication back-end selection across parallel groups and multi-NIC parallel data transfer. To achieve higher performance, we further propose another Device-Direct Communication approach, integrating a CPU-offloading P2P mechanism to enable direct cross-vendor GPU data transfer without host-memory staging. Experiments on LLaMA-8B and Qwen2-7B demonstrate that the proposed Device-Direct Communication approach achieves up to 98% of the throughput of an NVIDIA homogeneous system, while preserving training stability and correctness.

</details>


### [8] [A reliability- and latency-driven task allocation framework for workflow applications in the edge-hub-cloud continuum](https://arxiv.org/abs/2602.18158)
*Andreas Kouloumpris,Georgios L. Stavrinides,Maria K. Michael,Theocharis Theocharides*

Main category: cs.DC

TL;DR: 本文提出一种基于边缘-中心-云架构的精确任务分配框架，旨在优化关键工作流应用的可靠性和延迟性能。


<details>
  <summary>Details</summary>
Motivation: 常规边缘计算架构中，设备限制和操作条件多样化使任务分配困难，可靠性和延迟目标冲突且关键性高，现有方法无法整体解决，因此需要精确优化方案。

Method: 采用二元整数线性规划的多目标优化框架，整合时间冗余技术并考虑相关研究中常忽略的关键约束，以协同优化可靠性和延迟。

Result: 实际应用中可靠性平均提升84.19%，延迟降低49.81%；在各类合成工作欢呼上展现出高效性和可扩展性，运行时间在0.03至50.94秒之间。

Conclusion: 该方法有效、可扩展且实用，适用于不同结构的多种工作流应用，凸显其在新型架构中的重要价值。

Abstract: A growing number of critical workflow applications leverage a streamlined edge-hub-cloud architecture, which diverges from the conventional edge computing paradigm. An edge device, in collaboration with a hub device and a cloud server, often suffices for their reliable and efficient execution. However, task allocation in this streamlined architecture is challenging due to device limitations and diverse operating conditions. Given the inherent criticality of such workflow applications, where reliability and latency are vital yet conflicting objectives, an exact task allocation approach is typically required to ensure optimal solutions. As no existing method holistically addresses these issues, we propose an exact multi-objective task allocation framework to jointly optimize the overall reliability and latency of a workflow application in the specific edge-hub-cloud architecture. We present a comprehensive binary integer linear programming formulation that considers the relative importance of each objective. It incorporates time redundancy techniques, while accounting for crucial constraints often overlooked in related studies. We evaluate our approach using a relevant real-world workflow application, as well as synthetic workflows varying in structure, size, and criticality. In the real-world application, our method achieved average improvements of 84.19% in reliability and 49.81% in latency over baseline strategies, across relevant objective trade-offs. Overall, the experimental results demonstrate the effectiveness and scalability of our approach across diverse workflow applications for the considered system architecture, highlighting its practicality with runtimes averaging between 0.03 and 50.94 seconds across all examined workflows.

</details>


### [9] [It does not matter how you define locally checkable labelings](https://arxiv.org/abs/2602.18188)
*Antonio Cruciani,Avinandan Das,Alesya Raevskaya,Jukka Suomela*

Main category: cs.DC

TL;DR: 论文证明局部可检测标签问题（LCL）对形式变化具有极强稳健性：一种受限的节点-边检测形式结构与标准LCL问题可相互转换，且仅需对数轮时间开销。


<details>
  <summary>Details</summary>
Motivation: LCL问题在分布式图算法理论中占据核心地位，但现有形式对局部结构变化的敏感性未知。本研究旨在验证LCL家族在不同形式约束下是否保持计算等价性。

Method: 采用了一种强受限的节点-边检测问题形式（排除环路依赖），构建与标准LCL问题的双向局部归约机制，通过对称性破坏预言机实现转换。

Result: 两种形式间存在高效转换：在LOCAL模型中，转换过程的最大时间开销仅为加性O(log* n)轮，证明形式变化不影响核心计算复杂度。

Conclusion: LCL问题具备极强的形式稳健性，这为分布式复杂性理论的普适性提供了关键支撑，并拓展了问题建模的灵活性边界。

Abstract: Locally checkable labeling problems (LCLs) form the foundation of the modern theory of distributed graph algorithms. First introduced in the seminal paper by Naor and Stockmeyer [STOC 1993], these are graph problems that can be described by listing a finite set of valid local neighborhoods. This seemingly simple definition strikes a careful balance between two objectives: they are a family of problems that is broad enough so that it captures numerous problems that are of interest to researchers working in this field, yet restrictive enough so that it is possible to prove strong theorems that hold for all LCL problems. In particular, the distributed complexity landscape of LCL problems is now very well understood.
  In this work we show that the family of LCL problems is extremely robust to variations. We present a very restricted family of locally checkable problems (essentially, the "node-edge checkable" formalism familiar from round elimination, restricted to regular unlabeled graphs); most importantly, such problems cannot directly refer to e.g. the existence of short cycles. We show that one can translate between the two formalisms (there are local reductions in both directions that only need access to a symmetry-breaking oracle, and hence the overhead is at most an additive $O(\log^* n)$ rounds in the LOCAL model).

</details>


### [10] [Green by Design: Constraint-Based Adaptive Deployment in the Cloud Continuum](https://arxiv.org/abs/2602.18287)
*Andrea D'Iapico,Monica Vitali*

Main category: cs.DC

TL;DR: 论文提出一种基于绿色约束的自动部署计划生成方法，用于优化云边应用能耗与排放。


<details>
  <summary>Details</summary>
Motivation: 信息技术能耗与温室气体排放问题日益重要，需解决云边应用中动态部署的挑战以确保环境可持续性。

Method: 利用能耗模式、组件通信及基础设施环境特性分析，学习并更新绿色约束，指导调度器生成环保部署计划。

Result: 在实际部署场景中验证，成功降低能源使用及相关排放。

Conclusion: 该方法实现了自适应节能编排，显著提升IT部署的环境可持续性。

Abstract: The environmental sustainability of Information Technology (IT) has emerged as a critical concern, driven by the need to reduce both energy consumption and greenhouse gas (GHG) emissions. In the context of cloud-native applications deployed across the cloud-edge continuum, this challenge translates into identifying energy-efficient deployment strategies that consider not only the computational demands of application components but also the environmental impact of the nodes on which they are executed. Generating deployment plans that account for these dynamic factors is non-trivial, due to fluctuations in application behaviour and variations in the carbon intensity of infrastructure nodes. In this paper, we present an approach for the automatic generation of deployment plans guided by green constraints. These constraints are derived from a continuous analysis of energy consumption patterns, inter-component communication, and the environmental characteristics of the underlying infrastructure. This paper introduces a methodology and architecture for the generation of a set of green-aware constraints that inform the scheduler to produce environmentally friendly deployment plans. We demonstrate how these constraints can be automatically learned and updated over time using monitoring data, enabling adaptive, energy-aware orchestration. The proposed approach is validated through realistic deployment scenarios of a cloud-native application, showcasing its effectiveness in reducing energy usage and associated emissions.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [11] [Examining LLMs Ability to Summarize Code Through Mutation-Analysis](https://arxiv.org/abs/2602.17838)
*Lara Khatib,Micheal Pu,Bogdan Vasilescu,Meiyappan Nagappan*

Main category: cs.SE

TL;DR: 本研究开发了基于变异的评测方法，评估大语言模型生成的代码摘要是否准确反映程序实际行为而非意图，发现准确性随复杂性下降，新模型改进大但仍有局限。


<details>
  <summary>Details</summary>
Motivation: 开发者逐渐依赖大语言模型生成代码摘要用于文档编写、测试和审查，但摘要常误描述程序意图而非实际行为（如忽略边缘案例），因此需系统性评估其真实性。

Method: 采用变异评测：首先生成摘要，然后向代码注入目标变异（如语句、值或决策变更），检查大语言模型是否更新摘要反映新行为，进而通过624次评测（覆盖62个程序，含合成与人写数据集）进行验证。

Result: 结果显示：合成程序准确性从单函数76.5%降至多线程系统17.3%，变异类型和位置影响较小；人类编写程序准确率为49.3%，模型倾向描述意图而非行为；GPT-5.2对比GPT-4性能跃升至85.3%，但两者仍难以区分实现细节与标准算法模式。

Conclusion: 变异分析确立为系统方法，可有效评估大语言模型摘要是否反映程序实际行为而非文本模式，但模型在复杂性下存在明显不足。

Abstract: As developers increasingly rely on LLM-generated code summaries for documentation, testing, and review, it is important to study whether these summaries accurately reflect what the program actually does. LLMs often produce confident descriptions of what the code looks like it should do (intent), while missing subtle edge cases or logic changes that define what it actually does (behavior). We present a mutation-based evaluation methodology that directly tests whether a summary truly matches the code's logic. Our approach generates a summary, injects a targeted mutation into the code, and checks if the LLM updates its summary to reflect the new behavior. We validate it through three experiments totalling 624 mutation-summary evaluations across 62 programs. First, on 12 controlled synthetic programs with 324 mutations varying in type (statement, value, decision) and location (beginning, middle, end). We find that summary accuracy decreases sharply with complexity from 76.5% for single functions to 17.3% for multi-threaded systems, while mutation type and location exhibit weaker effects. Second, testing 150 mutated samples on 50 human-written programs from the Less Basic Python Problems (LBPP) dataset confirms the same failure patterns persist as models often describe algorithmic intent rather than actual mutated behavior with a summary accuracy rate of 49.3%. Furthermore, while a comparison between GPT-4 and GPT-5.2 shows a substantial performance leap (from 49.3% to 85.3%) and an improved ability to identify mutations as "bugs", both models continue to struggle with distinguishing implementation details from standard algorithmic patterns. This work establishes mutation analysis as a systematic approach for assessing whether LLM-generated summaries reflect program behavior rather than superficial textual patterns.

</details>


### [12] [DeCEAT: Decoding Carbon Emissions for AI-driven Software Testing](https://arxiv.org/abs/2602.18012)
*Pragati Kumari,Novarun Deb*

Main category: cs.SE

TL;DR: 本文引入DeCEAT框架评估小型语言模型在自动测试生成中的环境影响和性能权衡，发现可持续性是多维且高度依赖提示设计。


<details>
  <summary>Details</summary>
Motivation: 现有环境可持续性分析主要针对大型语言模型，小型语言模型在测试生成中的能源消耗和碳排放特性研究不足。

Method: 使用DeCEAT框架，结合HumanEval基准和自适应提示变体，CodeCarbon量化能源消耗和碳排放，单元测试覆盖率评估生成测试质量。

Result: 不同小型语言模型表现出独特优势，有的能效高、执行快，有的在碳约束下稳定性或准确性更强。

Conclusion: 可持续性是多个维度的综合，受提示结构和模型选择共同影响，该框架填补了SLM测试生成环保评估的空白。

Abstract: The increasing use of language models in automated software testing raises concerns about their environmental impact, yet existing sustainability analyses focus almost exclusively on large language models. As a result, the energy and carbon characteristics of small language models (SLMs) during test generation remain largely unexplored. To address this gap, this work introduces the DeCEAT framework, which systematically evaluates the environmental and performance trade-offs of SLMs using the HumanEval benchmark and adaptive prompt variants (based on the Anthropic template). The framework quantifies emission and time-aware behavior under controlled conditions, with CodeCarbon measuring energy consumption and carbon emissions, and unit test coverage assessing the quality of generated tests. Our results show that different SLMs exhibit distinct sustainability strengths: some prioritize lower energy use and faster execution, while others maintain higher stability or accuracy under carbon constraints. These findings demonstrate that sustainability in the generation of SLM-driven tests is multidimensional and strongly shaped by prompt design. This work provides a focused sustainability evaluation framework specifically tailored to automated SLM-based test generation, clarifying how prompt structure and model choice jointly influence environmental and performance outcomes.

</details>


### [13] [Role and Identity Work of Software Engineering Professionals in the Generative AI Era](https://arxiv.org/abs/2602.18190)
*Jorge Melegati*

Main category: cs.SE

TL;DR: 本文探讨Generative AI (GenAI)采纳对软件专业人员的工作身份影响，指出不同角色(如开发者和测试者)的身份工作差异未被考虑，并提出研究议程以解析角色作用和支持工具开发。


<details>
  <summary>Details</summary>
Motivation: 动机是现有研究表明GenAI采纳触发身份工作，但忽略了不同角色的差异，这可能导致对软件工程变化的影响理解不足，因此需要针对性研究。

Method: 方法包括综述不同角色的研究和GenAI采纳文献，提出研究议程来探索角色对身份工作的影响，开发新工具，并讨论实际含义。

Result: 结果是确立了角色在定义身份工作中的关键作用，并规划了未来研究，如角色导向工具的开发，以优化GenAI采纳。

Conclusion: 结论强调角色因素是身份工作的核心变量，研究议程将促进更有效的GenAI整合和实践改进。

Abstract: The adoption of Generative AI (GenAI) suggests major changes for software engineering, including technical aspects but also human aspects of the professionals involved. One of these aspects is how individuals perceive themselves regarding their work, i.e., their work identity, and the processes they perform to form, adapt and reject these identities, i.e., identity work. Existent studies provide evidence of such identity work of software professionals triggered by the adoption of GenAI, however they do not consider differences among diverse roles, such as developers and testers. In this paper, we argue the need for considering the role as a factor defining the identity work of software professionals. To support our claim, we review some studies regarding different roles and also recent studies on how to adopt GenAI in software engineering. Then, we propose a research agenda to better understand how the role influences identity work of software professionals triggered by the adoption of GenAI, and, based on that, to propose new artifacts to support this adoption. We also discuss the potential implications for practice of the results to be obtained.

</details>


### [14] [VeriSoftBench: Repository-Scale Formal Verification Benchmarks for Lean](https://arxiv.org/abs/2602.18307)
*Yutong Xin,Qiaochu Chen,Greg Durrett,Işil Dillig*

Main category: cs.SE

TL;DR: 提出VeriSoftBench基准测试，评估LLM在软件验证环境中的定理证明能力，揭示数学库模型在代码库场景下的局限性与依赖关系带来的挑战。


<details>
  <summary>Details</summary>
Motivation: 现有基于LLM的定理证明评测多聚焦Mathlib数学库，但软件验证需处理项目专属代码库的多文件依赖场景，缺乏针对性评估方案。

Method: 创建包含500个是不够4证明任务的VeriSoftBench基准，素材源于开源形式化方法项目，保留代码库上下文与文件依赖关系；测试ນ门LLM与专用证明器性能。

Result: 三大发现：1）Mathlib调优模型在代码库场景表现不佳；2）证明成功率与传递依赖深度负相关；3）限定依赖范围的上下文比全库上下文提升效果更显著，但仍存在明显差距。

Conclusion: 软件验证场景中项目依赖结构严重影响证明自动化效果，当前方法亟待改进；公开基准评测集促进相关研究。

Abstract: Large language models have achieved striking results in interactive theorem proving, particularly in Lean. However, most benchmarks for LLM-based proof automation are drawn from mathematics in the Mathlib ecosystem, whereas proofs in software verification are developed inside definition-rich codebases with substantial project-specific libraries. We introduce VeriSoftBench, a benchmark of 500 Lean 4 proof obligations drawn from open-source formal-methods developments and packaged to preserve realistic repository context and cross-file dependencies. Our evaluation of frontier LLMs and specialized provers yields three observations. First, provers tuned for Mathlib-style mathematics transfer poorly to this repository-centric setting. Second, success is strongly correlated with transitive repository dependence: tasks whose proofs draw on large, multi-hop dependency closures are less likely to be solved. Third, providing curated context restricted to a proof's dependency closure improves performance relative to exposing the full repository, but nevertheless leaves substantial room for improvement. Our benchmark and evaluation suite are released at https://github.com/utopia-group/VeriSoftBench.

</details>


### [15] [Statistical Confidence in Functional Correctness: An Approach for AI Product Functional Correctness Evaluation](https://arxiv.org/abs/2602.18357)
*Wallace Albertini,Marina Condé Araújo,Júlia Condé Araújo,Antonio Pedro Santos Alves,Marcos Kalinowski*

Main category: cs.SE

TL;DR: 该论文针对AI系统功能正确性评估缺乏实用工具的现状，提出SCFC方法，通过统计置信度连接业务需求并整合性能指标与变异性分析。


<details>
  <summary>Details</summary>
Motivation: 现有AI系统标准化质量模型缺乏统计鲁棒性的功能正确性评估方法，导致质量验证不足阻碍实际应用。

Method: 提出SCFC四步法：定义量化阈值、分层概率抽样、自助法估计置信区间、计算能力指标，并在工业案例中验证可行性。

Result: 两类真实工业AI系统案例研究显示，领域专家认可方法具备实用性、易用性和应用潜力canvas提升评估可信度。

Conclusion: SCFC通过统计置信度取代点估计，为功能正确性评估提供了可行且有效的标准化解决方案。

Abstract: The quality assessment of Artificial Intelligence (AI) systems is a fundamental challenge due to their inherently probabilistic nature. Standards such as ISO/IEC 25059 provide a quality model, but they lack practical and statistically robust methods for assessing functional correctness. This paper proposes and evaluates the Statistical Confidence in Functional Correctness (SCFC) approach, which seeks to fill this gap by connecting business requirements to a measure of statistical confidence that considers both the model's average performance and its variability. The approach consists of four steps: defining quantitative specification limits, performing stratified and probabilistic sampling, applying bootstrapping to estimate a confidence interval for the performance metric, and calculating a capability index as a final indicator. The approach was evaluated through a case study on two real-world AI systems in industry involving interviews with AI experts. Valuable insights were collected from the experts regarding the utility, ease of use, and intention to adopt the methodology in practical scenarios. We conclude that the proposed approach is a feasible and valuable way to operationalize the assessment of functional correctness, moving the evaluation from a point estimate to a statement of statistical confidence.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [16] [Mean-Field Reinforcement Learning without Synchrony](https://arxiv.org/abs/2602.18026)
*Shan Yang*

Main category: cs.MA

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Mean-field reinforcement learning (MF-RL) scales multi-agent RL to large populations by reducing each agent's dependence on others to a single summary statistic -- the mean action. However, this reduction requires every agent to act at every time step; when some agents are idle, the mean action is simply undefined. Addressing asynchrony therefore requires a different summary statistic -- one that remains defined regardless of which agents act. The population distribution $μ\in Δ(\mathcal{O})$ -- the fraction of agents at each observation -- satisfies this requirement: its dimension is independent of $N$, and under exchangeability it fully determines each agent's reward and transition. Existing MF-RL theory, however, is built on the mean action and does not extend to $μ$. We therefore construct the Temporal Mean Field (TMF) framework around the population distribution $μ$ from scratch, covering the full spectrum from fully synchronous to purely sequential decision-making within a single theory. We prove existence and uniqueness of TMF equilibria, establish an $O(1/\sqrt{N})$ finite-population approximation bound that holds regardless of how many agents act per step, and prove convergence of a policy gradient algorithm (TMF-PG) to the unique equilibrium. Experiments on a resource selection game and a dynamic queueing game confirm that TMF-PG achieves near-identical performance whether one agent or all $N$ act per step, with approximation error decaying at the predicted $O(1/\sqrt{N})$ rate.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [17] [Rethinking Beam Management: Generalization Limits Under Hardware Heterogeneity](https://arxiv.org/abs/2602.18151)
*Nikita Zeulin,Olga Galinina,Ibrahim Kilinc,Sergey Andreev,Robert W. Heath*

Main category: cs.NI

TL;DR: 本文探讨硬件异构性对5G+波束管理的挑战，强调必须将其作为机器学习算法设计的首要考量。


<details>
  <summary>Details</summary>
Motivation: 用户设备硬件多样性限制了机器学习算法在波束管理中的适用性，成为5G及未来通信的关键瓶颈。

Method: 分析了硬件异构性引发的故障模式，通过案例研究展示性能影响，并提出潜在策略以增强算法泛化能力。

Result: 案例研究证实硬件异构性会显著降低系统性能，策略改进可提升波束管理的鲁棒性。

Conclusion: 需将硬件异构性视为机器学习辅助波束管理的基础设计因素，并采用针对性策略优化系统性能。

Abstract: Hardware heterogeneity across diverse user devices poses new challenges for beam-based communication in 5G and beyond. This heterogeneity limits the applicability of machine learning (ML)-based algorithms. This article highlights the critical need to treat hardware heterogeneity as a first-class design concern in ML-aided beam management. We analyze key failure modes in the presence of heterogeneity and present case studies demonstrating their performance impact. Finally, we discuss potential strategies to improve generalization in beam management.

</details>


### [18] [Noise Mitigation Methods for Digital Visible Light Communication](https://arxiv.org/abs/2602.18187)
*Wataru Uemura,Takumi Hamano*

Main category: cs.NI

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Visible Light Communication (VLC) using Light Emitting Diodes (LEDs) has gained attention due to its low power consumption, long lifetime, and fast response. However, VLC suffers from optical noise generated by ambient light sources such as fluorescent lamps, which leads to waveform distortion and increased bit error rates (BER). In this paper, we propose two noise reduction methods for Digital Visible Light Communication (DVLC) systems. The first method exploits the periodic nature of interference caused by AC-powered-line illumination and reduces interference by subtracting sampled noise waveforms from the received signal. Second, inspired by Active Noise Control (ANC) techniques, an additional photodiode is introduced for noise reception, and subtraction circuits are employed to attenuate noise in real time. Experimental results show that both methods improve BER performance compared with conventional receivers, with the ANC-inspired approach achieving superior performance under all tested conditions.

</details>
