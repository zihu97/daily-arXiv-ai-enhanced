<div id=toc></div>

# Table of Contents

- [cs.AR](#cs.AR) [Total: 1]
- [cs.NI](#cs.NI) [Total: 2]
- [cs.PF](#cs.PF) [Total: 1]
- [cs.SE](#cs.SE) [Total: 13]
- [cs.DC](#cs.DC) [Total: 14]


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [1] [NL-DPE: An Analog In-memory Non-Linear Dot Product Engine for Efficient CNN and LLM Inference](https://arxiv.org/abs/2511.13950)
*Lei Zhao,Luca Buonanno,Archit Gajjar,John Moon,Aishwarya Natarajan,Sergey Serebryakov,Ron M. Roth,Xia Sheng,Youtao Zhang,Paolo Faraboschi,Jim Ignowski,Giacomo Pedretti*

Main category: cs.AR

TL;DR: NL-DPE是一种新型非线性点积引擎，通过结合RRAM交叉阵列与模拟内容可寻址存储器（ACAM），在模拟域中实现任意非线性函数和数据依赖的矩阵乘法，无需ADC，并采用噪声感知微调（NAF）提升精度，在能效和速度上显著优于GPU和现有IMC加速器。


<details>
  <summary>Details</summary>
Motivation: 当前基于RRAM的存内计算（IMC）加速器存在三大局限：仅支持静态点积运算、需要高功耗ADC电路、以及器件非理想性引入权重映射误差，难以高效准确地扩展到现代大语言模型。

Method: 提出NL-DPE架构，利用RRAM构建的模拟内容可寻址存储器（ACAM）将非线性函数和数据依赖乘法转化为决策树形式，在模拟域执行；同时采用无需片上校准的软件级噪声感知微调（NAF）方法缓解器件噪声影响。

Result: 实验表明，NL-DPE相比GPU基线实现28倍能效提升和249倍加速，相比现有IMC加速器实现22倍能效提升和245倍加速，同时保持高精度。

Conclusion: NL-DPE有效克服了传统RRAM IMC在非线性计算、ADC开销和器件误差方面的限制，为大模型提供了高能效、高精度的模拟存内计算解决方案。

Abstract: Resistive Random Access Memory (RRAM) based in-memory computing (IMC) accelerators offer significant performance and energy advantages for deep neural networks (DNNs), but face three major limitations: (1) they support only \textit{static} dot-product operations and cannot accelerate arbitrary non-linear functions or data-dependent multiplications essential to modern LLMs; (2) they demand large, power-hungry analog-to-digital converter (ADC) circuits; and (3) mapping model weights to device conductance introduces errors from cell nonidealities. These challenges hinder scalable and accurate IMC acceleration as models grow.
  We propose NL-DPE, a Non-Linear Dot Product Engine that overcomes these barriers. NL-DPE augments crosspoint arrays with RRAM-based Analog Content Addressable Memory (ACAM) to execute arbitrary non-linear functions and data-dependent matrix multiplications in the analog domain by transforming them into decision trees, fully eliminating ADCs. To address device noise, NL-DPE uses software-based Noise Aware Fine-tuning (NAF), requiring no in-device calibration. Experiments show that NL-DPE delivers 28X energy efficiency and 249X speedup over a GPU baseline, and 22X energy efficiency and 245X speedup over existing IMC accelerators, while maintaining high accuracy.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [2] [Benchmarking OpenWiFiSync on ESP32: Towards Cost-Effective Wireless Time Synchronization](https://arxiv.org/abs/2511.14457)
*Michael Gundall,Jan Herbst,Robin Müller,Hans D. Schotten*

Main category: cs.NI

TL;DR: 该论文提出并验证了一种基于无线广播特性的低成本、高精度时间同步协议（RBIS），在ESP32硬件平台上实现了±30微秒的同步精度，并开源了相关实现。


<details>
  <summary>Details</summary>
Motivation: 工业4.0应用需要移动设备间高精度无线时间同步，而传统有线同步协议难以直接适用于无线环境。

Method: 采用参考广播基础设施同步协议（RBIS），利用无线通信的广播特性，在基于ESP32模块和商用Wi-Fi接入点的低成本测试平台上实现并验证该协议。

Result: 在节能且价格低廉的硬件上实现了±30微秒的时间同步精度。

Conclusion: 所提出的方法具有非侵入性、符合标准、成本低且精度高，适用于多种工业应用场景，并通过开源促进后续研究。

Abstract: Wireless time synchronization of mobile devices is a key enabler for numerous Industry 4.0 applications, such as coordinated and synchronized tasks or the generation of high-precision timestamps for machine learning or artificial intelligence algorithms. Traditional wireline clock synchronization protocols, however, cannot achieve the performance in wireless environments without significant modifications. To address this challenge, we make use of the Reference Broadcast Infrastructure Synchronization protocol, which leverages the broadcast nature of wireless communications and remains both non-invasive and standard-compliant. We implement and validate this protocol on a low-cost testbed using ESP32 modules and a commercial Wi-Fi access point. To support further research and development, we release our implementation as open-source software under the GNU General Public License Version 3 license via the OpenWifiSync project on GitHub.
  Our results demonstrate that synchronization accuracies within +/-30 microseconds are achievable using energy-efficient and affordable hardware, making this approach suitable for a wide range of use cases.

</details>


### [3] [From Topology to Behavioral Semantics: Enhancing BGP Security by Understanding BGP's Language with LLMs](https://arxiv.org/abs/2511.14467)
*Heng Zhao,Ruoyu Wang,Tianhang Zheng,Qi Li,Bo Lv,Yuyi Wang,Wenliang Du*

Main category: cs.NI

TL;DR: BGPShield is a novel BGP anomaly detection framework leveraging LLM embeddings to capture semantic characteristics of Autonomous Systems beyond topology, achieving 100% detection of verified anomalies with <5% false discovery rate and strong generalizability without costly retraining.


<details>
  <summary>Details</summary>
Motivation: Existing BGP anomaly detection methods based on machine/deep learning suffer from low precision, poor generalizability, and high retraining costs because they focus only on topological structures and ignore the rich semantic behaviors and routing policy rationales of ASes.

Method: BGPShield uses LLM embeddings to represent each AS’s Behavior Portrait and Routing Policy Rationale. It employs a segment-wise aggregation scheme to preserve information from AS descriptions, a lightweight contrastive reduction network for semantic-consistent compression, and an AR-DTW algorithm to align and accumulate semantic distances for anomaly detection.

Result: Evaluated on 16 real-world datasets, BGPShield detects all verified anomalies with a false discovery rate under 5%. It generalizes well using pre-released LLMs and constructs representations for unseen ASes in under one second, vastly outperforming BEAM which requires ~65 hours of retraining.

Conclusion: By incorporating semantic insights via LLMs beyond topology, BGPShield significantly improves accuracy, generalizability, and efficiency in BGP anomaly detection, offering a practical and scalable solution for securing inter-domain routing.

Abstract: The trust-based nature of Border Gateway Protocol (BGP) makes it vulnerable to disruptions like prefix hijacking and misconfigurations, threatening routing stability. Traditional detection relies on manual inspection with limited scalability. Machine/Deep Learning (M/DL) approaches automate detection but suffer from suboptimal precision, limited generalizability, and high retraining costs. This is because existing methods focus on topological structures rather than comprehensive semantic characteristics of Autonomous Systems (ASes), often misinterpreting functionally similar but topologically distant ASes.
  To address this, we propose BGPShield, an anomaly detection framework built on LLM embeddings that captures the Behavior Portrait and Routing Policy Rationale of each AS beyond topology, such as operational scale and global role. We propose a segment-wise aggregation scheme to transform AS descriptions into LLM representations without information loss, and a lightweight contrastive reduction network to compress them into a semantic-consistent version. Using these representations, our AR-DTW algorithm aligns and accumulates semantic distances to reveal behavioral inconsistencies. Evaluated on 16 real-world datasets, BGPShield detects 100% of verified anomalies with a false discovery rate below 5%. Notably, the employed LLMs were released prior to evaluation events, verifying generalizability. Furthermore, BGPShield constructs representations for unseen ASes within one second, significantly outperforming BEAM which demands costly retraining (averaging 65 hours).

</details>


<div id='cs.PF'></div>

# cs.PF [[Back]](#toc)

### [4] [Enabling Heterogeneous Performance Analysis for Scientific Workloads](https://arxiv.org/abs/2511.13928)
*Maksymilian Graczyk,Vincent Desbiolles,Stefan Roiser,Andrea Guerrieri*

Main category: cs.PF

TL;DR: 该论文研究了在异构计算环境中，利用eBPF技术中的Uprobes和USDT两种方法进行性能分析的可行性与复杂性，旨在为CERN的Adaptyst项目提供未来集成路线图，以支持科学工作负载的架构无关性能分析。


<details>
  <summary>Details</summary>
Motivation: 异构计算系统结合了CPU、GPU、FPGA等多种处理单元，需高效性能分析手段以合理分配任务；现有工具缺乏对科学工作负载的开放、架构无关支持，因此CERN启动Adaptyst项目填补这一空白。

Method: 对比分析eBPF框架下的Uprobes和USDT两种内置性能追踪方法，在实现复杂度与性能开销方面进行评估。

Result: 研究明确了Uprobes和USDT在科学工作负载场景下的适用性、性能表现及集成难度，为Adaptyst后续开发提供了技术选型依据。

Conclusion: Uprobes和USDT具备用于异构系统性能分析的潜力，未来可集成至Adaptyst中，推动其向支持多架构性能分析能力发展。

Abstract: Heterogeneous computing integrates diverse processing elements, such as CPUs, GPUs, and FPGAs, within a single system, aiming to leverage the strengths of each architecture to optimize performance and energy consumption. In this context, efficient performance analysis plays a critical role in determining the most suitable platform for dispatching tasks, ensuring that workloads are allocated to the processing units where they can execute most effectively. Adaptyst is a novel ongoing effort at CERN, with the aim to develop an open-source, architecture-agnostic performance analysis for scientific workloads. This study explores the performance and implementation complexity of two built-in eBPF-based methods such as Uprobes and USDT, with the aim of outlining a roadmap for future integration into Adaptyst and advancing toward heterogeneous performance analysis capabilities.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [5] [Show and Tell: Prompt Strategies for Style Control in Multi-Turn LLM Code Generation](https://arxiv.org/abs/2511.13972)
*Jeremiah Bohr*

Main category: cs.SE

TL;DR: 本文研究了在代码生成任务中，不同提示策略（指令型、示例型及两者结合）对语言模型初始代码风格控制及其在后续增强过程中风格保持能力的影响。结果表明，结合型提示在初始压缩性和后续扩展纪律性方面表现最佳。


<details>
  <summary>Details</summary>
Motivation: 当前语言模型生成的代码虽然功能正确，但往往过于冗长，偏离人类编写风格。如何在模型对代码进行功能增强的同时维持所需的代码风格，是本文关注的核心问题。

Method: 作者采用配对两轮协议，在四个系统提示条件下（指令型、示例型、结合型和对照组），让模型先生成中间Python任务的解决方案，再根据通用改进指令修订代码（任务不变），共收集160对程序进行分析。

Result: 结合型提示实现了最强的初始压缩效果和最佳的扩展纪律性；指令型提示初始效果显著但扩展纪律性中等；示例型提示初始效果有限且无扩展纪律性。

Conclusion: 初始提示效果与扩展纪律性是提示设计中的两个独立维度，结合型提示在两轮工作流中提供了最稳定的风格控制。

Abstract: Language models generate functionally correct code that tends toward excessive verbosity, with elaborate documentation and defensive patterns that diverge from human baselines. Two prompting mechanisms have emerged for stylistic control: instruction based prompts that articulate abstract directives, and example based prompts that provide concrete code demonstrations. The core problem is whether stylistic constraints persist when models enhance initial implementations with additional features while maintaining high functional accuracy. Here we show that instruction-based, example-based, and combined prompts produce distinct patterns of initial control and expansion discipline over one enhancement turn. We manipulated system prompts across four conditions in a paired two-turn protocol where models first generated solutions to an intermediate Python task, then revised their code under general improvement directives, holding the user task fixed (N = 160 paired programs). Combined prompts produced the strongest initial compression and greatest expansion discipline. Instructions showed large initial effects and moderate expansion discipline. Examples showed modest initial effects with no expansion discipline. These results show that initial prompt effectiveness and expansion discipline are separate aspects of prompt design, and that combined approaches provide the most stable stylistic control in this two-turn workflow.

</details>


### [6] [Exploring the Use of ChatGPT by Computer Science Students in Software Development: Applications, Ethical Considerations, and Insights for Engineering Education](https://arxiv.org/abs/2511.13996)
*Daihan Xu,Diana Martin*

Main category: cs.SE

TL;DR: 本研究通过质性访谈探讨英国某高校计算机专业学生在软件开发项目中如何策略性且合乎伦理地使用ChatGPT，发现学生普遍将其作为辅助工具（贡献约30%），用于深化理解而非替代创造性思维，但对AI生成代码的批判性分析不足，并呼吁教师制定明确使用规范。


<details>
  <summary>Details</summary>
Motivation: 尽管ChatGPT在计算机教育中的应用日益广泛，现有研究多依赖问卷调查，缺乏对学生使用策略与伦理意识的深入分析。本文旨在填补这一空白，探究学生如何在学术和职业背景下策略性、伦理地使用ChatGPT。

Method: 采用半结构化访谈对一所英国高校的计算机科学学生进行质性研究，聚焦其在软件开发项目中使用ChatGPT的策略与伦理认知。

Result: 学生的学习模式从“独立思考—手动编码—迭代调试”转向“AI辅助构思—交互式编程—协作优化”；多数学生将ChatGPT用于对话式学习，保留高阶决策权，限制其贡献约30%，并评估输出以避免过度依赖；但仅少数深入分析AI代码，存在批判性思维减弱风险；学生普遍反对未注明出处的使用，关注隐私泄露与技能退化，并呼吁教师提供清晰指引。

Conclusion: 研究揭示了学生与AI互动的新动态，强调需通过明确的教学指导促进ChatGPT在教育中的负责任、符合教学目标的使用。

Abstract: ChatGPT has been increasingly used in computer science, offering efficient support across software development tasks. While it helps students navigate programming challenges, its use also raises concerns about academic integrity and overreliance. Despite growing interest in this topic, prior research has largely relied on surveys, emphasizing trends over in-depth analysis of students' strategies and ethical awareness. This study complements existing work through a qualitative investigation of how computer science students in one UK institution strategically and ethically engage with ChatGPT in software development projects. Drawing on semi-structured interviews, it explores two key questions: How do computer science students ethically and strategically report using ChatGPT in software development projects? How do students understand and perceive the ethical issues associated with using ChatGPT in academic and professional contexts? Findings reveal a shift in students' learning models, moving from traditional "independent thinking-manual coding-iterative debugging" to "AI-assisted ideation-interactive programming-collaborative optimization." Importantly, many use ChatGPT conversationally to deepen understanding, while consciously reserving creative and high-level decision-making tasks for themselves. Students tend to cap ChatGPT's contribution to roughly 30%, and evaluate its output to mitigate overreliance. However, only a minority thoroughly analyze AI-generated code, raising concerns about reduced critical engagement. Meanwhile, students reject uncredited use, highlight risks such as privacy breaches and skill degradation, and call for clear usage guidelines set by their teachers. This research offers novel insights into the evolving learner-AI dynamic and highlights the need for explicit guidance to support responsible and pedagogically sound use of such tools.

</details>


### [7] [LoCoBench-Agent: An Interactive Benchmark for LLM Agents in Long-Context Software Engineering](https://arxiv.org/abs/2511.13998)
*Jielin Qiu,Zuxin Liu,Zhiwei Liu,Rithesh Murthy,Jianguo Zhang,Haolin Chen,Shiyu Wang,Ming Zhu,Liangwei Yang,Juntao Tan,Roshan Ram,Akshara Prabhakar,Tulika Awalgaonkar,Zixiang Chen,Zhepeng Cen,Cheng Qian,Shelby Heinecke,Weiran Yao,Silvio Savarese,Caiming Xiong,Huan Wang*

Main category: cs.SE

TL;DR: 本文提出了LoCoBench-Agent，首个面向软件工程的长上下文大语言模型（LLM）智能体评测框架，通过将原有8000个场景扩展为多轮交互环境，系统评估智能体在长上下文编程任务中的理解能力、工具使用效率、错误恢复与架构一致性，并引入9项涵盖理解与效率维度的指标，在10K至1M token范围内对主流模型进行评测，揭示了长上下文鲁棒性、理解-效率权衡及工具使用策略等关键发现。


<details>
  <summary>Details</summary>
Motivation: 现有基准（如LoCoBench）仅支持单轮评估，无法反映真实软件开发中LLM智能体所需的多轮交互、工具调用和自适应推理能力，因此亟需一个更贴近实际工作流的综合评测框架。

Method: 构建LoCoBench-Agent框架，将LoCoBench的8000个长上下文代码理解场景转化为支持多轮对话的智能体环境，提供8种专用工具（如文件操作、搜索、代码分析），并在10K–1M token上下文长度下，通过9项指标系统评估智能体在理解力与效率方面的表现。

Result: 实验发现：(1) 智能体在长上下文中表现出强鲁棒性；(2) 理解深度与执行效率存在负相关权衡；(3) 不同模型在对话效率和工具使用策略上差异显著，高效智能体展现出更优的工具调用模式。

Conclusion: LoCoBench-Agent作为首个面向软件工程的长上下文LLM智能体基准，为评估自主编程智能体的能力、识别性能瓶颈和推动大规模自动化软件开发提供了坚实基础。

Abstract: As large language models (LLMs) evolve into sophisticated autonomous agents capable of complex software development tasks, evaluating their real-world capabilities becomes critical. While existing benchmarks like LoCoBench~\cite{qiu2025locobench} assess long-context code understanding, they focus on single-turn evaluation and cannot capture the multi-turn interactive nature, tool usage patterns, and adaptive reasoning required by real-world coding agents. We introduce \textbf{LoCoBench-Agent}, a comprehensive evaluation framework specifically designed to assess LLM agents in realistic, long-context software engineering workflows. Our framework extends LoCoBench's 8,000 scenarios into interactive agent environments, enabling systematic evaluation of multi-turn conversations, tool usage efficiency, error recovery, and architectural consistency across extended development sessions. We also introduce an evaluation methodology with 9 metrics across comprehension and efficiency dimensions. Our framework provides agents with 8 specialized tools (file operations, search, code analysis) and evaluates them across context lengths ranging from 10K to 1M tokens, enabling precise assessment of long-context performance. Through systematic evaluation of state-of-the-art models, we reveal several key findings: (1) agents exhibit remarkable long-context robustness; (2) comprehension-efficiency trade-off exists with negative correlation, where thorough exploration increases comprehension but reduces efficiency; and (3) conversation efficiency varies dramatically across models, with strategic tool usage patterns differentiating high-performing agents. As the first long-context LLM agent benchmark for software engineering, LoCoBench-Agent establishes a rigorous foundation for measuring agent capabilities, identifying performance gaps, and advancing autonomous software development at scale.

</details>


### [8] [FlakyGuard: Automatically Fixing Flaky Tests at Industry Scale](https://arxiv.org/abs/2511.14002)
*Chengpeng Li,Farnaz Behrang,August Shi,Peng Liu*

Main category: cs.SE

TL;DR: FlakyGuard improves automatic repair of flaky tests by using selective graph-based context retrieval, outperforming existing methods and providing useful root cause explanations.


<details>
  <summary>Details</summary>
Motivation: Existing LLM-based approaches for repairing flaky tests suffer from the context problem—either insufficient or excessive context—limiting their effectiveness in industrial settings.

Method: FlakyGuard models code as a graph and employs selective graph exploration to retrieve only the most relevant context for repairing flaky tests.

Result: FlakyGuard repairs 47.6% of reproducible flaky tests, with 51.8% of fixes accepted by developers, outperforming prior methods by at least 22% in success rate; 100% of surveyed developers found its explanations useful.

Conclusion: FlakyGuard effectively addresses the context problem in flaky test repair through graph-based context selection, achieving high repair and developer acceptance rates in real-world industrial scenarios.

Abstract: Flaky tests that non-deterministically pass or fail waste developer time and slow release cycles. While large language models (LLMs) show promise for automatically repairing flaky tests, existing approaches like FlakyDoctor fail in industrial settings due to the context problem: providing either too little context (missing critical production code) or too much context (overwhelming the LLM with irrelevant information). We present FlakyGuard, which addresses this problem by treating code as a graph structure and using selective graph exploration to find only the most relevant context. Evaluation on real-world flaky tests from industrial repositories shows that FlakyGuard repairs 47.6 % of reproducible flaky tests with 51.8 % of the fixes accepted by developers. Besides it outperforms state-of-the-art approaches by at least 22 % in repair success rate. Developer surveys confirm that 100 % find FlakyGuard's root cause explanations useful.

</details>


### [9] [Keeping Code-Aware LLMs Fresh: Full Refresh, In-Context Deltas, and Incremental Fine-Tuning](https://arxiv.org/abs/2511.14022)
*Pradeep Kumar Sharma,Ishaan Puri,Mantinder Jit Singh,Swapnil Shivaprasad,Hritvik Shrivastava*

Main category: cs.SE

TL;DR: 本文研究了在代码库持续演进过程中，如何在不遗忘旧知识的前提下保持代码检索模型的时效性，并系统比较了全量重训、上下文学习（ICL）和增量微调（Inc-FT）三种策略。


<details>
  <summary>Details</summary>
Motivation: 现代代码库不断演化（如文件重命名、删除、API漂移等），导致昨天训练的模型在今天可能失效。因此需要一种方法在保留对历史代码理解的同时，使模型能适应最新代码状态。

Method: 作者将模型时效性问题建模为基线快照与当前HEAD之间的领域漂移，并对比三类更新策略：(A) 全量重训；(B) 推理时通过上下文学习注入近期变更（原始diff或英文摘要）；(C) 基于变更生成的数据集进行增量微调，并通过控制新旧数据混合比例缓解灾难性遗忘。此外，提出别名感知评估协议和遗忘探针以更准确衡量性能。

Result: 在Flask、SQLAlchemy、Pandas和Poetry等仓库上的实验表明：Inc-FT配合旧数据混合在综合性能上最优；当无法训练时，使用英文摘要的ICL能最快提升对新代码的响应能力；若追求最高新代码准确率，全量重训仍是上限。此外，基于Git diff的Inc-FT在重命名/删除频繁的窗口表现更好，而基于完整文件的Inc-FT更适合行为变化为主的场景。

Conclusion: 保持代码检索模型时效性需在新旧知识间取得平衡。增量微调结合旧数据混合是兼顾二者最有效的方法，而ICL和全量重训则分别适用于资源受限或追求极致新代码精度的场景。评估方法也需考虑代码演化的特性，如重命名而非简单删除。

Abstract: Modern codebases evolve continuously: files are renamed or deleted; public APIs drift; behavior shifts within otherwise familiar modules. A model trained yesterday to map a developer's natural-language question to the exact set of repository file paths that matter will degrade tomorrow, even if the questions themselves look unchanged. In this paper we study, at system scale and across several widely used repositories, how to keep such a model fresh without surrendering retention on earlier code. We frame freshness as a form of domain drift between a base snapshot and the current HEAD, and we compare three families of update strategies: (A) Full Refresh, retraining the entire model at the new snapshot; (B) In-Context Learning (ICL) that injects recent deltas (raw git diffs or concise English summaries) at inference; and (C) Incremental Fine-Tuning (Inc-FT) on delta-derived training sets, with carefully controlled NEW:OLD mixing to mitigate catastrophic forgetting. We contribute an alias-aware evaluation protocol that credits rename while never rewarding deleted paths, and a practical Forgetting Probe that quantifies residual emissions of obsolete paths. Across Flask, SQLAlchemy, Pandas, and Poetry, Inc-FT with old-aware mixes delivers the best overall balance on mixed sets, ICL with English delta summaries delivers the fastest new-code lift when training is not feasible, and Full Refresh remains the ceiling when maximum NEW accuracy matters. We also compare Git-diff Inc-FT to full-file Inc-FT, showing that diffs excel in rename/delete-heavy windows while full-file context wins in behavior-change-heavy windows.

</details>


### [10] [LogPurge: Log Data Purification for Anomaly Detection via Rule-Enhanced Filtering](https://arxiv.org/abs/2511.14062)
*Shenglin Zhang,Ziang Chen,Zijing Que,Yilun Liu,Yongqian Sun,Sicheng Wei,Dan Pei,Hailin Li*

Main category: cs.SE

TL;DR: 本文提出了一种名为LogPurge的低成本、规则增强的日志净化框架，通过两阶段过滤算法自动从含异常的日志序列中筛选出正常子集用于训练异常检测模型，在多个数据集上显著优于现有无监督方法。


<details>
  <summary>Details</summary>
Motivation: 现有日志异常检测方法依赖于干净无异常的日志数据进行模型训练，但获取此类数据需昂贵的人工标注，而当前自动清洗方法未能充分结合日志的特性与语义。

Method: 提出LogPurge框架：第一阶段利用大语言模型（LLM）去除聚类异常模式并增强系统规则以提升LLM对日志的理解；第二阶段采用分治策略将剩余污染区域分解为更小子问题，递归应用第一阶段流程进行净化。

Result: 在两个公开数据集和一个工业数据集上的实验表明，该方法平均去除98.74%的异常，保留82.39%的正常样本；相比最新无监督日志样本选择算法，F-1分数分别提升35.7%、84.11%和149.72%。

Conclusion: LogPurge能高效自动净化日志数据，显著提升异常检测性能，验证了其在实际应用中的有效性与优越性。

Abstract: Log anomaly detection, which is critical for identifying system failures and preempting security breaches, detects irregular patterns within large volumes of log data, and impacts domains such as service reliability, performance optimization, and database log analysis. Modern log anomaly detection methods rely on training deep learning models on clean, anomaly-free log sequences. However, obtaining such clean log data requires costly and tedious human labeling, and existing automatic cleaning methods fail to fully integrate the specific characteristics and actual semantics of logs in their purification process. In this paper, we propose a cost-aware, rule-enhanced purification framework, LogPurge, that automatically selects a sufficient subset of normal log sequences from contamination log sequences to train a anomaly detection model. Our approach involves a two-stage filtering algorithm: In the first stage, we use a large language model (LLM) to remove clustered anomalous patterns and enhance system rules to improve LLM's understanding of system logs; in the second stage, we utilize a divide-and-conquer strategy that decomposes the remaining contaminated regions into smaller subproblems, allowing each to be effectively purified through the first stage procedure. Our experiments, conducted on two public datasets and one industrial dataset, show that our method significantly removes an average of 98.74% of anomalies while retaining 82.39% of normal samples. Compared to the latest unsupervised log sample selection algorithms, our method achieves F-1 score improvements of 35.7% and 84.11% on the public datasets, and an impressive 149.72% F-1 improvement on the private dataset, demonstrating the effectiveness of our approach.

</details>


### [11] [A Practical Implementation of Customized Scrum-Based Agile Framework in Aerospace Software Development Under DO-178C Constraints](https://arxiv.org/abs/2511.14215)
*Malik Muhammad Umer*

Main category: cs.SE

TL;DR: 本文提出并验证了一种专为DO-178C合规的航空航天安全关键软件定制的Scrum敏捷框架，在满足严格认证要求的同时显著提升了开发效率和质量。


<details>
  <summary>Details</summary>
Motivation: 航空航天系统日益复杂，需要在敏捷开发与严格的安全及认证要求之间取得平衡。传统敏捷方法难以直接满足DO-178C等标准的要求，因此需要一种经过调整的、既能保持敏捷性又能确保合规性的开发框架。

Method: 研究设计了一个经验验证的Scrum敏捷框架，通过调整Scrum的核心角色、工件和事件来满足DO-178C的认证、验证和独立性目标。具体增强措施包括多学科产品负责人模型、双重验收标准、独立的测试与文档团队以及专职的认证联络人。该框架通过两个可比的航空航天项目（一个采用定制敏捷流程，另一个采用传统瀑布模型）进行了评估。

Result: 与瀑布模型相比，采用定制敏捷框架的项目实现了显著改进：每个需求的总工作量减少76%，缺陷检测速度加快75%，缺陷解决速度加快78%，缺陷密度降低超过50%，同时完全符合DO-178C设计保证等级A的要求。

Conclusion: 研究表明，通过有纪律的裁剪和与认证机构的主动合作，敏捷实践与法规遵从可以有效共存。尽管存在因迭代开发导致的验证与确认（V&V）工作量增加等挑战，但通过工作流自动化、CI/CD以及自动化的文档、验证和配置管理，仍有巨大的改进空间。未来研究应将该框架的验证扩展到更广泛的航空航天领域及其他具有类似认证要求的安全关键行业。

Abstract: The increasing complexity of aerospace systems requires development processes that balance agility with stringent safety and certification demands. This study presents an empirically validated Scrum-based Agile framework tailored for DO-178C compliant, safety-critical aerospace software. The framework adapts core Scrum roles, artifacts, and events to meet certification, verification, and independence objectives. Key enhancements include a multi-disciplinary product ownership model, dual compliance-and-functionality acceptance criteria, independent testing and documentation teams, and dedicated certification liaisons. The approach was evaluated through two comparable aerospace projects-one using the customized Agile process and the other a traditional Waterfall model. Results showed significant improvements: a 76% reduction in Total Effort per Requirement, 75% faster Defect Detection, 78% faster Defect Resolution, and over 50% lower Defect Density, while maintaining full compliance with DO-178C Design Assurance Level A. These findings demonstrate that Agile practices and regulatory compliance can coexist effectively when supported by disciplined tailoring and proactive engagement with certification authorities. The study also notes challenges, including increased V&V effort due to recurring Sprint activities and refactoring inherent to iterative development. Nonetheless, it identifies substantial opportunities for further gains through workflow automation, CI/CD practices, and automated documentation, verification, and configuration management. Future research should expand validation of this framework across the aerospace domain and other safety-critical industries with similar certification requirements.

</details>


### [12] [KTester: Leveraging Domain and Testing Knowledge for More Effective LLM-based Test Generation](https://arxiv.org/abs/2511.14224)
*Anji Li,Mingwei Liu,Zhenxi Chen,Zheng Pei,Zike Li,Dekun Dai,Yanlin Wang,Zibin Zheng*

Main category: cs.SE

TL;DR: KTester is a knowledge-enhanced framework that improves LLM-based unit test generation by integrating project-specific and testing-domain knowledge, leading to more correct, readable, and maintainable tests with higher coverage and fewer failures.


<details>
  <summary>Details</summary>
Motivation: Existing LLM-based approaches to automated unit test generation often produce tests that lack correctness and maintainability in real-world software projects, due to insufficient contextual and domain-specific knowledge.

Method: KTester extracts project structure and usage patterns via static analysis, separates test design from test method generation guided by testing-domain knowledge, and uses multi-perspective prompting with structured templates to steer the LLM toward high-quality test output.

Result: KTester outperforms state-of-the-art baselines by 5.69% in execution pass rate and 8.83% in line coverage, while generating fewer tests in less time; human evaluators also rated its outputs significantly higher in correctness, readability, and maintainability.

Conclusion: Integrating both project-specific and testing-domain knowledge into LLM-based test generation yields substantial improvements in test quality and practicality, demonstrating the effectiveness of a knowledge-driven approach like KTester.

Abstract: Automated unit test generation using large language models (LLMs) holds great promise but often struggles with generating tests that are both correct and maintainable in real-world projects. This paper presents KTester, a novel framework that integrates project-specific knowledge and testing domain knowledge to enhance LLM-based test generation. Our approach first extracts project structure and usage knowledge through static analysis, which provides rich context for the model. It then employs a testing-domain-knowledge-guided separation of test case design and test method generation, combined with a multi-perspective prompting strategy that guides the LLM to consider diverse testing heuristics. The generated tests follow structured templates, improving clarity and maintainability. We evaluate KTester on multiple open-source projects, comparing it against state-of-the-art LLM-based baselines using automatic correctness and coverage metrics, as well as a human study assessing readability and maintainability. Results demonstrate that KTester significantly outperforms existing methods across six key metrics, improving execution pass rate by 5.69% and line coverage by 8.83% over the strongest baseline, while requiring less time and generating fewer test cases. Human evaluators also rate the tests produced by KTester significantly higher in terms of correctness, readability, and maintainability, confirming the practical advantages of our knowledge-driven framework.

</details>


### [13] [Watchdogs and Oracles: Runtime Verification Meets Large Language Models for Autonomous Systems](https://arxiv.org/abs/2511.14435)
*Angelo Ferrando*

Main category: cs.SE

TL;DR: 本文提出将运行时验证（RV）与大语言模型（LLMs）协同整合，以提升自主系统的可信性：RV为LLM提供安全保障，LLM则增强RV在规范获取、预测推理和不确定性处理方面的能力。


<details>
  <summary>Details</summary>
Motivation: 在包含学习组件和开放环境的自主系统中，确保其安全性与可信性极具挑战；形式化方法依赖完整模型且假设静态，而大语言模型虽擅长自然语言到形式化表达的转换但缺乏形式保证，因此需要结合两者优势。

Method: 提出一种RV与LLM的共生集成框架，其中RV作为LLM驱动自主性的“护栏”，而LLM辅助RV完成规范捕获、预期推理和不确定性应对。

Result: 阐明了该协同方法相较于现有综述与路线图的独特之处，讨论了相关挑战、认证影响，并指出了未来研究方向。

Conclusion: RV与LLM的相互强化有望推动可信赖自主系统的发展，但仍需解决技术与认证方面的关键问题。

Abstract: Assuring the safety and trustworthiness of autonomous systems is particularly difficult when learning-enabled components and open environments are involved. Formal methods provide strong guarantees but depend on complete models and static assumptions. Runtime verification (RV) complements them by monitoring executions at run time and, in its predictive variants, by anticipating potential violations. Large language models (LLMs), meanwhile, excel at translating natural language into formal artefacts and recognising patterns in data, yet they remain error-prone and lack formal guarantees. This vision paper argues for a symbiotic integration of RV and LLMs. RV can serve as a guardrail for LLM-driven autonomy, while LLMs can extend RV by assisting specification capture, supporting anticipatory reasoning, and helping to handle uncertainty. We outline how this mutual reinforcement differs from existing surveys and roadmaps, discuss challenges and certification implications, and identify future research directions towards dependable autonomy.

</details>


### [14] [LLM-Assisted Thematic Analysis: Opportunities, Limitations, and Recommendations](https://arxiv.org/abs/2511.14528)
*Tatiane Ornelas,Allysson Allex Araújo,Júlia Araújo,Marina Araújo,Bianca Trinkenreich,Marcos Kalinowski*

Main category: cs.SE

TL;DR: 本研究通过工作坊探讨软件工程领域资深研究者如何理解大语言模型（LLMs）在主题分析中的机遇与风险，发现LLMs可提升效率但不能替代人类解释，强调需保持人工监督与提示素养。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型在软件工程定性研究中的应用日益增多，其对主题分析等解释性过程的方法论影响尚不明确，亟需探讨其对研究严谨性、透明度和研究者主体性的影响。

Method: 组织25位ISERN研究人员参与反思性工作坊，通过结构化讨论LLM辅助的开放式编码、主题生成与主题审查，并使用彩色画布记录参与者对机会、局限与建议的看法。

Result: 参与者认可LLMs带来的效率与可扩展性优势，但也指出其存在偏见、语境丢失、可复现性差及模型快速演进等风险，并强调提示素养和持续人工监督的重要性。

Conclusion: LLMs可作为支持工具增强定性研究，但无法取代人类的解释性分析；研究为软件工程社区负责任地整合LLMs提供了方法论反思基础。

Abstract: [Context] Large Language Models (LLMs) are increasingly used to assist qualitative research in Software Engineering (SE), yet the methodological implications of this usage remain underexplored. Their integration into interpretive processes such as thematic analysis raises fundamental questions about rigor, transparency, and researcher agency. [Objective] This study investigates how experienced SE researchers conceptualize the opportunities, risks, and methodological implications of integrating LLMs into thematic analysis. [Method] A reflective workshop with 25 ISERN researchers guided participants through structured discussions of LLM-assisted open coding, theme generation, and theme reviewing, using color-coded canvases to document perceived opportunities, limitations, and recommendations. [Results] Participants recognized potential efficiency and scalability gains, but highlighted risks related to bias, contextual loss, reproducibility, and the rapid evolution of LLMs. They also emphasized the need for prompting literacy and continuous human oversight. [Conclusion] Findings portray LLMs as tools that can support, but not substitute, interpretive analysis. The study contributes to ongoing community reflections on how LLMs can responsibly enhance qualitative research in SE.

</details>


### [15] [FHIRconnect: Towards a seamless integration of openEHR and FHIR](https://arxiv.org/abs/2511.14618)
*Severin Kohler,Jordi Piera Jiménez,Michael Anywar,Lars Fuhrmann,Heather Leslie,Maximilian Meixner,Julian Saß,Florian Kärcher,Diego Boscá,Birger Haarbrandt,Michael Marschollek,Roland Eils*

Main category: cs.SE

TL;DR: 本文提出FHIRconnect，一种用于openEHR与HL7 FHIR之间标准化双向数据转换的领域特定语言（DSL）及开源引擎，通过三层架构实现65%的映射复用，并覆盖24个国际原型与15个FHIR配置文件。


<details>
  <summary>Details</summary>
Motivation: 由于openEHR与HL7 FHIR在数据建模方法上的根本差异以及缺乏标准化的转换机制，医疗系统间的互操作性面临挑战。

Method: 设计并实现了一种三层架构的领域特定语言（DSL）和开源转换引擎（openFHIR），结合国际原型基础与本地定制支持，完成openEHR与FHIR之间的双向映射。

Result: 成功将24个国际原型映射至15个FHIR配置文件，覆盖七个临床领域，实现65%的映射复用率，并发布首个形式化规范的openEHR-FHIR转换DSL、开源执行引擎及高影响力临床原型映射库。

Conclusion: FHIRconnect为社区驱动的映射标准化提供了技术基础，减少了对定制ETL方案的依赖，推动了基于开放标准的医疗IT系统在语法与语义层面的互操作性。

Abstract: Healthcare interoperability between openEHR and HL7 FHIR remains challenging due to fundamental differences in their data modeling approaches and the absence of standardized transformation mechanisms. This paper presents FHIRconnect, a novel domain-specific language and open-source transformation engine that enables standardized, bidirectional data exchange between openEHR and FHIR. Our approach addresses critical interoperability gaps through a triple-layered architecture that achieves 65% mapping reuse across projects by leveraging international archetype-based foundations while supporting local customizations. Using this framework, FHIRconnect successfully mapped 24 international archetypes to 15 FHIR profiles across seven clinical domains. Key contributions include the first comprehensive DSL for openEHR-FHIR transformation with a formal specification, an open-source execution engine (openFHIR), and an accessible mapping library covering high-impact clinical archetypes. Together, these components establish the technical basis for community-driven mapping standardization, reducing reliance on custom ETL solutions and advancing syntactic and semantic interoperability in healthcare IT systems built on open standards.

</details>


### [16] [Why Do We Code? A Theory on Motivations and Challenges in Software Engineering from Education to Practice](https://arxiv.org/abs/2511.14711)
*Aaliyah Chang,Mariam Guizani,Brittany Johnson*

Main category: cs.SE

TL;DR: 该研究通过15次半结构化访谈和Gioia方法论，揭示了软件工程领域中动机与挑战如何共同影响个体从教育到职业的过渡，并提出了“暴露-追求-评估”（EPE）过程模型。


<details>
  <summary>Details</summary>
Motivation: 探索软件工程中个体在教育到职业过渡阶段动机与挑战的相互作用机制，填补现有研究对此动态关系理解的空白。

Method: 采用15次半结构化访谈，结合组织行为学中的Gioia方法论（一种改编的扎根理论方法），归纳构建动机与挑战的分类体系，并建立EPE过程模型。

Result: 研究发现：有影响力的早期接触激发内在动机，缺乏影响力的接触则依赖外在推动；好奇心和规避其他选择是教育阶段的独特驱动力；归属感障碍是贯穿教育与职业的唯一持续性挑战；职业发展挑战限制外在满足，而技术培训、归属障碍和动机威胁则限制内在满足；未满足的动机与反复出现的挑战会影响个体是否坚持、转岗或离开该领域。

Conclusion: 研究提出了一个基于实证的EPE过程模型，为设计干预措施以增强软件工程教育与实践中个体的内在满足感并减少系统性障碍提供了理论基础。

Abstract: Motivations and challenges jointly shape how individuals enter, persist, and evolve within software engineering (SE), yet their interplay remains underexplored across the transition from education to professional practice. We conducted 15 semi-structured interviews and employed the Gioia Methodology, an adapted grounded theory methodology from organizational behavior, to inductively derive taxonomies of motivations and challenges, and build the Exposure-Pursuit-Evaluation (EPE) Process Model. Our findings reveal that impactful early exposure triggers intrinsic motivations, while non-impactful exposure requires an extrinsic push (e.g., career/ personal goals, external validation). We identify curiosity and avoiding alternatives as a distinct educational drivers, and barriers to belonging as the only challenge persisting across education and career. Our findings show that career progression challenges (e.g., navigating the corporate world) constrain extrinsic fulfillment while technical training challenges, barriers to belonging and threats to motivation constrain intrinsic fulfillment. The theory shows how unmet motivations and recurring challenges influence persistence, career shifts, or departure from the field. Our results provide a grounded model for designing interventions that strengthen intrinsic fulfillment and reduce systemic barriers in SE education and practice.

</details>


### [17] [From Legacy Fortran to Portable Kokkos: An Autonomous Agentic AI Workflow](https://arxiv.org/abs/2509.12443)
*Sparsh Gupta,Kamalavasan Kamalakkannan,Maxim Moraru,Galen Shipman,Patrick Diehl*

Main category: cs.SE

TL;DR: 本文提出一种基于智能体的AI工作流，利用大型语言模型（LLMs）自动将遗留Fortran代码翻译、优化并验证为性能可移植的Kokkos C++程序，在多种硬件上实现高效运行。


<details>
  <summary>Details</summary>
Motivation: 随着高性能计算向GPU异构架构演进，大量缺乏原生Fortran绑定的加速器使得传统Fortran科学代码难以迁移。手动重写为Kokkos等现代框架耗时费力，亟需自动化解决方案。

Method: 构建一个多智能体LLM协作系统，负责Fortran内核到Kokkos C++的翻译、编译、运行、测试、调试与优化，并在不同硬件平台上验证其性能可移植性。

Result: 该流程成功现代化多个基准内核；付费模型（如GPT-5）以较低成本生成优于原始Fortran的可移植Kokkos代码，而开源模型（如Llama4-Maverick）常无法产出可用代码。

Conclusion: 智能体驱动的LLM工作流在Fortran到Kokkos的自动转换中展现出可行性，为遗留科学应用在多样化超算平台上的高效可移植运行提供了新路径，并突显了LLM在科学计算领域结构化推理任务中的潜力。

Abstract: Scientific applications continue to rely on legacy Fortran codebases originally developed for homogeneous, CPU-based systems. As High-Performance Computing (HPC) shifts toward heterogeneous GPU-accelerated architectures, many accelerators lack native Fortran bindings, creating an urgent need to modernize legacy codes for portability. Frameworks like Kokkos provide performance portability and a single-source C++ abstraction, but manual Fortran-to-Kokkos porting demands significant expertise and time. Large language models (LLMs) have shown promise in source-to-source code generation, yet their use in fully autonomous workflows for translating and optimizing parallel code remains largely unexplored, especially for performance portability across diverse hardware. This paper presents an agentic AI workflow where specialized LLM "agents" collaborate to translate, validate, compile, run, test, debug, and optimize Fortran kernels into portable Kokkos C++ programs. Results show the pipeline modernizes a range of benchmark kernels, producing performance-portable Kokkos codes across hardware partitions. Paid OpenAI models such as GPT-5 and o4-mini-high executed the workflow for only a few U.S. dollars, generating optimized codes that surpassed Fortran baselines, whereas open-source models like Llama4-Maverick often failed to yield functional codes. This work demonstrates the feasibility of agentic AI for Fortran-to-Kokkos transformation and offers a pathway for autonomously modernizing legacy scientific applications to run portably and efficiently on diverse supercomputers. It further highlights the potential of LLM-driven agentic systems to perform structured, domain-specific reasoning tasks in scientific and systems-oriented applications.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [18] [Boosting performance: Gradient Clock Synchronisation with two-way measured links](https://arxiv.org/abs/2511.13727)
*Sophie Wenning*

Main category: cs.DC

TL;DR: 本文将GCS算法的理论模型从单向测量范式扩展为更贴近实际实现的双向测量范式，从而放宽了先前对链路长度等限制，并显著降低了估计误差。


<details>
  <summary>Details</summary>
Motivation: 先前关于GCS算法的形式化模型基于单向测量假设，施加了诸多不切实际的限制（如单位链路长度），难以在真实系统中部署。为提升模型的实用性，作者旨在通过引入双向测量范式来构建更贴近实现的模型。

Method: 将GCS算法的形式化模型从单向测量范式转换为双向测量范式，并在此基础上重新建模频率源、细化误差来源分析。

Result: 1. 取消了单位链路长度的限制，使模型适用于灵活部署；2. 明确形式化了先前工作中隐含的频率源假设；3. 对算法估计误差进行细粒度分解，并整体降低多个数量级；4. 将不确定度对误差的贡献降至每链路延迟的0.1%–10%，并给出了局部与全局偏斜的匹配上界。

Conclusion: 通过采用双向测量范式，本文显著提升了GCS算法模型的现实适用性，并大幅优化了其时钟同步精度，为实际部署提供了坚实的理论基础。

Abstract: This master thesis extends the formal model of the GCS algorithm as presented by (Fan and Lynch 2004, 325), (Lenzen, Locher and Wattenhofer 2008, 510) and (Függer et al. 2023) to operate under implementation-near assumptions by replacing the one-way measurement paradigm assumed in prior work by the two-way measurement paradigm. With this change of paradigm, we remove many restrictions previously enforced to allow provable performance. Most notability, while maintaining the core behaviour of GCS, we: 1. Lift the requirement for unitary link lengths and thereby create a realistic model for flexible deployment of implementations of GCS in practice. 2. Provide a formal model of frequency sources assumed in prior work. 3. Perform a fine grained distinction between the different components of the algorithm's estimation error and globally reduce its impact by multiple orders of magnitude. 4. Significantly reduce the contribution of the uncertainty to the algorithm's estimation error to be in the range of 10\% to 0,1\% of the delay per link instead of being in the oder of the delay per link as in prior work and show matching upper bounds on the local and global skew of GCS.

</details>


### [19] [Gaia: Hybrid Hardware Acceleration for Serverless AI in the 3D Compute Continuum](https://arxiv.org/abs/2511.13728)
*Maximilian Reisecker,Cynthia Marcelino,Thomas Pusztai,Stefan Nastic*

Main category: cs.DC

TL;DR: Gaia is a GPU-as-a-service architecture for serverless AI that dynamically selects between CPU and GPU backends based on workload characteristics and SLOs, reducing latency by up to 95%.


<details>
  <summary>Details</summary>
Motivation: Current serverless platforms lack effective mechanisms to manage hardware acceleration in heterogeneous environments like the Edge-Cloud-Space continuum, leading to poor SLO compliance and inefficient resource usage due to static or one-time dynamic device assignments.

Method: Gaia introduces a lightweight Execution Mode Identifier that classifies functions into four execution modes at deploy time, and a Dynamic Function Runtime that continuously monitors and adjusts hardware backend (CPU/GPU) based on user-defined SLOs.

Result: Gaia reduces end-to-end latency by up to 95% while maintaining SLO compliance and cost efficiency across heterogeneous computing environments.

Conclusion: Gaia successfully makes hardware acceleration a platform-managed concern, enabling SLO-aware and cost-efficient execution of serverless AI workloads in dynamic, heterogeneous infrastructures.

Abstract: Serverless computing offers elastic scaling and pay-per-use execution, making it well-suited for AI workloads. As these workloads run in heterogeneous environments such as the Edge-Cloud-Space 3D Continuum, they often require intensive parallel computation, which GPUs can perform far more efficiently than CPUs. However, current platforms struggle to manage hardware acceleration effectively, as static user-device assignments fail to ensure SLO compliance under varying loads or placements, and one-time dynamic selections often lead to suboptimal or cost-inefficient configurations. To address these issues, we present Gaia, a GPU-as-a-service model and architecture that makes hardware acceleration a platform concern. Gaia combines (i) a lightweight Execution Mode Identifier that inspects function code at deploy time to emit one of four execution modes, and a Dynamic Function Runtime that continuously reevaluates user-defined SLOs to promote or demote between CPU- and GPU backends. Our evaluation shows that it seamlessly selects the best hardware acceleration for the workload, reducing end-to-end latency by up to 95%. These results indicate that Gaia enables SLO-aware, cost-efficient acceleration for serverless AI across heterogeneous environments.

</details>


### [20] [TT-Edge: A Hardware-Software Co-Design for Energy-Efficient Tensor-Train Decomposition on Edge AI](https://arxiv.org/abs/2511.13738)
*Hyunseok Kwak,Kyeongwon Lee,Kyeongpil Min,Chaebin Jung,Woojoo Lee*

Main category: cs.DC

TL;DR: 本文提出TT-Edge，一种软硬件协同设计的框架，用于在边缘设备上高效执行基于张量列分解（TTD）的模型压缩，通过专用硬件加速关键计算，在保持低精度损失的同时显著提升速度并降低能耗。


<details>
  <summary>Details</summary>
Motivation: 在资源受限的边缘设备上进行分布式学习对高效设备端模型压缩提出了更高要求。尽管张量列分解（TTD）能实现高压缩率且精度损失小，但其依赖的重复奇异值分解（SVD）和矩阵乘法在低功耗处理器上带来显著的延迟和能耗开销。

Method: TT-Edge将SVD分为双对角化和对角化两个阶段，并将计算密集部分卸载至专门设计的TTD引擎。该引擎与现有的GEMM加速器紧密集成，减少频繁的矩阵-向量数据传输。整体方案在RISC-V边缘AI处理器上实现，复用GEMM资源并采用共享浮点单元以控制硬件开销。

Result: 在FPGA原型和45nm工艺下的后综合功耗分析表明，TT-Edge在对ResNet-32模型进行TTD压缩时，相比仅使用GEMM的基线方案实现了1.7倍的速度提升和40.2%的能耗降低，同时仅增加4%的总功耗和极少的硬件开销。

Conclusion: TT-Edge有效解决了TTD在边缘环境中面临的延迟和能耗瓶颈，为资源受限设备上的高效模型压缩提供了可行的软硬件协同方案。

Abstract: The growing demands of distributed learning on resource constrained edge devices underscore the importance of efficient on device model compression. Tensor Train Decomposition (TTD) offers high compression ratios with minimal accuracy loss, yet repeated singular value decompositions (SVDs) and matrix multiplications can impose significant latency and energy costs on low power processors. In this work, we present TT-Edge, a hardware software co designed framework aimed at overcoming these challenges. By splitting SVD into two phases--bidiagonalization and diagonalization--TT-Edge offloads the most compute intensive tasks to a specialized TTD Engine. This engine integrates tightly with an existing GEMM accelerator, thereby curtailing the frequent matrix vector transfers that often undermine system performance and energy efficiency. Implemented on a RISC-V-based edge AI processor, TT-Edge achieves a 1.7x speedup compared to a GEMM only baseline when compressing a ResNet 32 model via TTD, while reducing overall energy usage by 40.2 percent. These gains come with only a 4 percent increase in total power and minimal hardware overhead, enabled by a lightweight design that reuses GEMM resources and employs a shared floating point unit. Our experimental results on both FPGA prototypes and post-synthesis power analysis at 45 nm demonstrate that TT-Edge effectively addresses the latency and energy bottlenecks of TTD based compression in edge environments.

</details>


### [21] [Inside VOLT: Designing an Open-Source GPU Compiler](https://arxiv.org/abs/2511.13751)
*Shinnung Jeong,Chihyo Ahn,Huanzhi Pu,Jisheng Zhao,Hyesoon Kim,Blaise Tine*

Main category: cs.DC

TL;DR: 本文提出了Vortex优化轻量级工具链（VOLT），用于支持开源GPU架构上的SIMT代码生成与优化，通过分层设计实现对多种前端语言和硬件的兼容，并将核心SIMT分析与优化集中于中端以提升可扩展性。


<details>
  <summary>Details</summary>
Motivation: 开源GPU研究虽有进展，但缺乏高效、可复用的编译器框架来支持现有GPU程序在新型开源ISA上的执行与性能优化，而这类工具链的开发复杂且常被低估。

Method: 设计并实现了VOLT编译器工具链，采用分层架构，在中端集中处理SIMT相关的核心分析与优化，支持多前端语言和不同开源GPU硬件，并通过两个案例（ISA扩展与主机运行时API）验证其可扩展性。

Result: VOLT成功实现了跨抽象层级的SIMT代码生成与优化，展示了对新ISA扩展和运行时接口的良好支持能力，为开源GPU生态提供了可复用、易扩展的编译基础设施。

Conclusion: VOLT为开源GPU提供了一个灵活、模块化且可扩展的编译器框架，有助于降低开源GPU软件栈开发成本，并推动其生态发展。

Abstract: Recent efforts in open-source GPU research are opening new avenues in a domain that has long been tightly coupled with a few commercial vendors. Emerging open GPU architectures define SIMT functionality through their own ISAs, but executing existing GPU programs and optimizing performance on these ISAs relies on a compiler framework that is technically complex and often undercounted in open hardware development costs.
  To address this challenge, the Vortex-Optimized Lightweight Toolchain (VOLT) has been proposed. This paper presents its design principles, overall structure, and the key compiler transformations required to support SIMT execution on Vortex. VOLT enables SIMT code generation and optimization across multiple levels of abstraction through a hierarchical design that accommodates diverse front-end languages and open GPU hardware. To ensure extensibility as GPU architectures evolve, VOLT centralizes fundamental SIMT-related analyses and optimizations in the middle-end, allowing them to be reused across front-ends and easily adapted to emerging open-GPU variants. Through two case studies on ISA extensions and host-runtime API, this paper also demonstrates how VOLT can support extensions

</details>


### [22] [Guaranteed DGEMM Accuracy While Using Reduced Precision Tensor Cores Through Extensions of the Ozaki Scheme](https://arxiv.org/abs/2511.13778)
*Angelika Schwarz,Anton Anders,Cole Brower,Harun Bayraktar,John Gunnels,Kate Clark,RuQing G. Xu,Samuel Rodriguez,Sebastien Cayrols,Paweł Tabaszewski,Victor Podlozhnyuk*

Main category: cs.DC

TL;DR: 本文提出了自动动态精度（ADP）框架，利用低精度硬件高效可靠地模拟双精度矩阵乘法，通过指数跨度容量（ESC）估算器确保FP64精度，并在NVIDIA Blackwell GPU上实现显著加速。


<details>
  <summary>Details</summary>
Motivation: 现代GPU越来越侧重于低精度计算单元（如FP16、FP8、FP4），其Tensor Core性能远超传统FP64流水线。为在科学计算中兼顾高性能与高精度，需研究如何利用这些低精度单元有效模拟FP64精度。

Method: 提出Automatic Dynamic Precision (ADP)框架，核心是硬件无关的Exponent Span Capacity (ESC)估算器，用于保守确定Ozaki分解所需的切片参数；结合异常处理、运行时启发式策略和无缝回退机制，并引入无符号整数切片方案提升表示效率。

Result: 在保持FP64精度的前提下，ADP在挑战性输入上运行开销低于10%；在55位尾数设置下，相较于原生FP64 GEMM，在NVIDIA Blackwell GB200和RTX Pro 6000 Blackwell Server Edition上分别实现最高2.3倍和13.2倍加速。

Conclusion: 低精度加速器可作为高保真、高性能科学计算任务的实用且可投入生产的基础设施。

Abstract: The rapid growth of artificial intelligence (AI) has made low-precision formats such as FP16, FP8, and, most recently, block-scaled FP4 the primary focus of modern GPUs, where Tensor Cores now deliver orders-of-magnitude higher throughput than traditional FP64 pipelines. This hardware shift has sparked a new line of algorithm research: using low-precision units to emulate double-precision accuracy through schemes such as Ozaki decompositions. We advance this direction with Automatic Dynamic Precision (ADP), a fully GPU-resident framework that makes emulated FP64 matrix multiplication both efficient and reliable. At its core is the Exponent Span Capacity (ESC), a hardware-agnostic estimator that conservatively determines the decomposition parameter (also known as slices) required to achieve FP64-level accuracy. Built on ESC, ADP integrates exception handling, run time heuristics, and seamless fallback to native FP64, ensuring correctness without host-device synchronization or user intervention. Additionally, we further improve Ozaki-style decompositions with an unsigned integer slicing scheme, which increases representational efficiency and reduces computational waste. Validated against recently proposed BLAS grading tests, ADP consistently preserves FP64 fidelity on challenging inputs while incurring less than 10% run time overhead. In a 55-bit mantissa setting, our approach achieves up to 2.3x and 13.2x speedups over native FP64 GEMM on NVIDIA Blackwell GB200 and the RTX Pro 6000 Blackwell Server Edition, respectively. Our results demonstrate that low-precision accelerators can serve as a practical, production-ready foundation for high-fidelity and high-performance scientific computing workloads.

</details>


### [23] [Semantic Multiplexing](https://arxiv.org/abs/2511.13779)
*Mohammad Abdi,Francesca Meneghello,Francesco Restuccia*

Main category: cs.DC

TL;DR: 本文提出语义复用（Semantic Multiplexing）新方法，通过在语义层将多个任务的压缩表示融合为单一语义表示，在不增加天线或带宽的前提下支持比物理信道数量更多的并发任务处理，显著降低延迟、能耗和通信负载，同时保持任务准确性。


<details>
  <summary>Details</summary>
Motivation: 现有无线通信系统仅支持比特级并行传输，限制了边缘计算中可并发处理的任务数量；为突破这一瓶颈，需在更高抽象层次（语义层）实现多任务复用。

Method: 将多个任务相关的压缩表示在语义层面融合成单一语义表示进行传输，利用语义层的有效自由度扩展，在不违反香农容量限制的前提下实现超物理信道数的多任务并发处理。

Result: 实验表明，语义复用在4×4信道上将复用任务数从2增至8时，图像分类准确率下降不到4%；相比现有基线，延迟、能耗和通信负载分别最多降低8倍、25倍和54倍，同时保持相当的任务性能。

Conclusion: 语义复用是一种高效可行的新范式，能够在无线边缘支持更多并发计算任务，显著提升系统效率，并已在真实测试平台上验证其有效性，代码与数据集将公开以促进复现。

Abstract: Mobile devices increasingly require the parallel execution of several computing tasks offloaded at the wireless edge. Existing communication systems only support parallel transmissions at the bit level, which fundamentally limits the number of tasks that can be concurrently processed. To address this bottleneck, this paper introduces the new concept of Semantic Multiplexing. Our approach shifts stream multiplexing from bits to tasks by merging multiple task-related compressed representations into a single semantic representation. As such, Semantic Multiplexing can multiplex more tasks than the number of physical channels without adding antennas or widening bandwidth by extending the effective degrees of freedom at the semantic layer, without contradicting Shannon capacity rules. We have prototyped Semantic Multiplexing on an experimental testbed with Jetson Orin Nano and millimeter-wave software-defined radios and tested its performance on image classification and sentiment analysis while comparing to several existing baselines in semantic communications. Our experiments demonstrate that Semantic Multiplexing allows jointly processing multiple tasks at the semantic level while maintaining sufficient task accuracy. For example, image classification accuracy drops by less than 4% when increasing from 2 to 8 the number of tasks multiplexed over a 4$\times$4 channel. Semantic Multiplexing reduces latency, energy consumption, and communication load respectively by up to 8$\times$, 25$\times$, and 54$\times$ compared to the baselines while keeping comparable performance. We pledge to publicly share the complete software codebase and the collected datasets for reproducibility.

</details>


### [24] [Do MPI Derived Datatypes Actually Help? A Single-Node Cross-Implementation Study on Shared-Memory Communication](https://arxiv.org/abs/2511.13804)
*Temitayo Adefemi*

Main category: cs.DC

TL;DR: 该论文对MPI派生数据类型（DDTs）在多个MPI实现中的性能进行了跨平台评估，发现其性能高度依赖于具体应用、通信语义和MPI库，没有一种策略在所有场景下都占优，建议针对目标MPI实现进行实际性能测试。


<details>
  <summary>Details</summary>
Motivation: MPI派生数据类型虽承诺简化非连续数据的零拷贝通信，但其实际性能表现存在争议，且缺乏跨MPI实现的系统性评估。

Method: 作者使用三个2D应用（Jacobi CFD求解器、生命游戏、基于格点的图像重建），分别实现手动打包（BASIC）和DDT两种版本，并在四种主流MPI实现（MPICH、Open MPI、Intel MPI、MVAPICH2）上，对多种通信语义（非阻塞点对点、邻域集合通信、MPI-4持久操作）进行强弱扩展性测试，验证结果一致性并比较性能。

Result: DDT在某些组合下最快（如图像重建在Intel MPI和MPICH上），但在其他组合下最慢（如同一代码在Open MPI和MVAPICH2上）；CFD求解器中BASIC通常优于DDT，而生命游戏中性能排名随MPI库变化；还观察到特定MPI栈的异常行为（如MPICH在DDT邻域和持久模式下的减速）。

Conclusion: DDT的性能不具备跨程序、通信语义和MPI实现的可移植性，建议在目标MPI环境下对DDT和手动打包方案都进行性能剖析以选择最优策略。

Abstract: MPI's derived datatypes (DDTs) promise easier, copy-free communication of non-contiguous data, yet their practical performance remains debated and is often reported only for a single MPI stack. We present a cross-implementation assessment using three 2D applications: a Jacobi CFD solver, Conway's Game of Life, and a lattice-based image reconstruction. Each application is written in two ways: (i) a BASIC version with manual packing and unpacking of non-contiguous regions and (ii) a DDT version using MPI_Type_vector and MPI_Type_create_subarray with correct true extent via MPI_Type_create_resized. For API parity, we benchmark identical communication semantics: non-blocking point-to-point (Irecv/Isend + Waitall), neighborhood collectives (MPI_Neighbor_alltoallw), and MPI-4 persistent operations (*_init). We run strong and weak scaling on 1-4 ranks, validate bitwise-identical halos, and evaluate four widely used MPI implementations: MPICH, Open MPI, Intel MPI, and MVAPICH2 on a single ARCHER2 node. Results are mixed. DDTs can be fastest, for example for the image reconstruction code on Intel MPI and MPICH, but can also be among the slowest on other stacks, such as Open MPI and MVAPICH2 for the same code. For the CFD solver, BASIC variants generally outperform DDTs across semantics, whereas for Game of Life the ranking flips depending on the MPI library. We also observe stack-specific anomalies, for example MPICH slowdowns with DDT neighborhood and persistent modes. Overall, no strategy dominates across programs, semantics, and MPI stacks; performance portability for DDTs is not guaranteed. We therefore recommend profiling both DDT-based and manual-packing designs under the intended MPI implementation and communication mode. Our study is limited to a single node and does not analyze memory overhead; multi-node and GPU-aware paths are left for future work.

</details>


### [25] [ParallelKittens: Systematic and Practical Simplification of Multi-GPU AI Kernels](https://arxiv.org/abs/2511.13940)
*Stuart H. Sul,Simran Arora,Benjamin F. Spector,Christopher Ré*

Main category: cs.DC

TL;DR: ParallelKittens (PK) 是一个基于 ThunderKittens 的极简 CUDA 框架，通过八个核心原语和统一编程模板，系统化指导多 GPU 重叠内核设计，在 Hopper 和 Blackwell 架构上显著提升多种并行工作负载的性能。


<details>
  <summary>Details</summary>
Motivation: 随着 AI 模型规模扩大，GPU 间通信已成为性能瓶颈，现有系统虽采用计算-通信重叠策略，但在异构工作负载和新型加速器上难以达到理论峰值性能。作者希望探索是否能通过一组通用、可复用的设计原则来系统性地构建最优多 GPU 内核。

Method: 提出 ParallelKittens（PK）框架，基于对影响多 GPU 性能的关键因素（数据传输机制、资源调度、设计开销）的全面分析，提炼出八个核心原语和统一编程模板，简化重叠式多 GPU 内核开发。

Result: 在 Hopper 和 Blackwell 架构上验证表明，仅需不到 50 行设备代码，PK 在数据/张量并行任务中提速最高达 2.33 倍，序列并行任务达 4.08 倍，专家并行任务达 1.22 倍。

Conclusion: ParallelKittens 通过简洁而系统化的设计原则，有效提升了多 GPU 通信与计算重叠效率，为未来多 GPU 编程提供了高效且可扩展的框架。

Abstract: Inter-GPU communication has become a major bottleneck for modern AI workloads as models scale and improvements in hardware compute throughput outpace improvements in interconnect bandwidth. Existing systems mitigate this through compute-communication overlap but often fail to meet theoretical peak performance across heterogeneous workloads and new accelerators. Instead of operator-specific techniques, we ask whether a small set of simple, reusable principles can systematically guide the design of optimal multi-GPU kernels. We present ParallelKittens (PK), a minimal CUDA framework that drastically simplifies the development of overlapped multi-GPU kernels. PK extends the ThunderKittens framework and embodies the principles of multi-GPU kernel design through eight core primitives and a unified programming template, derived from a comprehensive analysis of the factors that govern multi-GPU performance$\unicode{x2014}$data-transfer mechanisms, resource scheduling, and design overheads. We validate PK on both Hopper and Blackwell architectures. With fewer than 50 lines of device code, PK achieves up to $2.33 \times$ speedup for data- and tensor-parallel workloads, $4.08 \times$ for sequence-parallel workloads, and $1.22 \times$ for expert-parallel workloads.

</details>


### [26] [FailSafe: High-performance Resilient Serving](https://arxiv.org/abs/2511.14116)
*Ziyi Xu,Zhiqiang Xie,Swapnil Gandhi,Christos Kozyrakis*

Main category: cs.DC

TL;DR: FailSafe 是一种容错的张量并行（TP）大语言模型（LLM）推理系统，通过循环 KVCache 放置、混合注意力机制和细粒度负载感知路由，在 GPU 故障下仍能保持高性能与资源均衡，并显著降低恢复延迟。


<details>
  <summary>Details</summary>
Motivation: 张量并行虽能提升 LLM 推理效率，但其强耦合性导致单点 GPU 故障会中断执行、引发昂贵的 KVCache 重计算，并造成长期计算与内存失衡，亟需一种在不规则 GPU 可用性下仍能维持高效稳定服务的容错机制。

Method: FailSafe 提出三项关键技术：(1) 循环 KVCache 放置以实现均匀内存利用；(2) 混合注意力机制结合张量与数据并行消除拖尾效应；(3) 细粒度负载感知路由动态平衡请求。此外，采用主动 KVCache 备份与按需权重恢复策略避免重计算和冗余传输，并集成到轻量级服务引擎中。

Result: 在 8xH100 DGX 系统上使用真实故障轨迹和典型工作负载评估表明，FailSafe 相比传统容错方法最高提升 2 倍吞吐量，恢复延迟降低两个数量级；即使在最多三块 GPU 故障情况下，仍能维持高吞吐与资源均衡。

Conclusion: FailSafe 能在动态且不可靠的硬件环境下实现鲁棒、高效的 LLM 推理服务，显著提升系统容错能力与资源利用率。

Abstract: Tensor parallelism (TP) enables large language models (LLMs) to scale inference efficiently across multiple GPUs, but its tight coupling makes systems fragile: a single GPU failure can halt execution, trigger costly KVCache recomputation, and introduce long-term compute and memory imbalance. We present FailSafe, a fault-tolerant TP serving system that sustains high performance under irregular GPU availability. FailSafe introduces three techniques to balance computation and memory across GPUs: (1) Cyclic KVCache Placement for uniform memory utilization, (2) Hybrid Attention combining tensor- and data-parallel attention to eliminate stragglers, and (3) Fine-Grained Load-Aware Routing to dynamically balance requests. It further employs proactive KVCache backup and on-demand weight recovery to avoid expensive recomputation and redundant data transfers. We implement these techniques in a lightweight serving engine compatible with existing LLM infrastructures. Evaluated on an 8xH100 DGX system with real-world fault traces and representative workloads, FailSafe achieves up to 2x higher throughput and two orders of magnitude lower recovery latency compared to standard fault handling approaches. Even with up to three GPU failures, FailSafe sustains high throughput and balanced utilization, demonstrating robust and efficient LLM serving under dynamic and unreliable hardware conditions.

</details>


### [27] [10Cache: Heterogeneous Resource-Aware Tensor Caching and Migration for LLM Training](https://arxiv.org/abs/2511.14124)
*Sabiha Afroz,Redwan Ibne Seraj Khan,Hadeel Albahar,Jingoo Han,Ali R. Butt*

Main category: cs.DC

TL;DR: 10Cache 是一种面向云环境的资源感知张量缓存与迁移系统，通过优化 GPU、CPU 和 NVMe 之间的内存协同使用，显著加速大语言模型训练并提升内存利用率。


<details>
  <summary>Details</summary>
Motivation: 云上训练大语言模型面临 GPU 内存容量有限和成本高昂的问题，现有内存卸载方法存在张量迁移延迟高和设备内存利用率低的缺陷，导致训练时间延长和云成本上升。

Method: 10Cache 通过分析张量执行顺序构建预取策略，在固定内存中按张量大小分布分配缓冲区，并重用缓冲区以减少分配开销，从而智能协调多级存储（GPU/CPU/NVMe）间的内存使用。

Result: 在多种大语言模型工作负载下，10Cache 相比现有先进卸载方法，训练速度最高提升 2 倍，GPU 缓存命中率最高提升 86.6 倍，CPU 和 GPU 内存利用率分别最高提升 2.15 倍和 1.33 倍。

Conclusion: 10Cache 是一种实用且可扩展的解决方案，能有效优化云环境中大语言模型训练的吞吐量和资源效率，降低对高端 GPU 的依赖。

Abstract: Training large language models (LLMs) in the cloud faces growing memory bottlenecks due to the limited capacity and high cost of GPUs. While GPU memory offloading to CPU and NVMe has made large-scale training more feasible, existing approaches suffer from high tensor migration latency and suboptimal device memory utilization, ultimately increasing training time and cloud costs. To address these challenges, we present 10Cache, a resource-aware tensor caching and migration system that accelerates LLM training by intelligently coordinating memory usage across GPU, CPU, and NVMe tiers. 10Cache profiles tensor execution order to construct prefetch policies, allocates memory buffers in pinned memory based on tensor size distributions, and reuses memory buffers to minimize allocation overhead.
  Designed for cloud-scale deployments, 10Cache improves memory efficiency and reduces reliance on high-end GPUs. Across diverse LLM workloads, it achieves up to 2x speedup in training time, improves GPU cache hit rate by up to 86.6x, and increases CPU/GPU memory utilization by up to 2.15x and 1.33x, respectively, compared to state-of-the-art offloading methods. These results demonstrate that 10Cache is a practical and scalable solution for optimizing LLM training throughput and resource efficiency in cloud environments.

</details>


### [28] [Analyzing the Impact of Participant Failures in Cross-Silo Federated Learning](https://arxiv.org/abs/2511.14456)
*Fabian Stricker,David Bermbach,Christian Zirpins*

Main category: cs.DC

TL;DR: 本文研究了跨机构联邦学习中参与者失败对模型质量的影响，发现数据偏斜会导致评估过于乐观，且失败发生的时机显著影响模型性能。


<details>
  <summary>Details</summary>
Motivation: 在跨机构（cross-silo）联邦学习场景中，参与组织可能因通信问题或配置错误而失败，但目前对此类失败对模型质量影响的研究较少，尤其缺乏对评估偏差和失败时机等因素的系统分析。

Method: 开展了一项广泛实验研究，分析参与者失败对跨机构联邦学习模型质量的影响，重点关注失败发生的时间、数据分布偏斜以及对模型评估结果的影响。

Result: 研究发现，在高度数据偏斜情况下，模型评估结果过于乐观，掩盖了失败带来的真实负面影响；同时，失败发生的训练阶段（时机）显著影响最终模型质量。

Conclusion: 该研究为构建可靠、鲁棒的跨机构联邦学习系统提供了重要见解，强调在设计FL系统时需考虑参与者失败对模型训练与评估的实际影响。

Abstract: Federated learning (FL) is a new paradigm for training machine learning (ML) models without sharing data. While applying FL in cross-silo scenarios, where organizations collaborate, it is necessary that the FL system is reliable; however, participants can fail due to various reasons (e.g., communication issues or misconfigurations). In order to provide a reliable system, it is necessary to analyze the impact of participant failures. While this problem received attention in cross-device FL where mobile devices with limited resources participate, there is comparatively little research in cross-silo FL.
  Therefore, we conduct an extensive study for analyzing the impact of participant failures on the model quality in the context of inter-organizational cross-silo FL with few participants. In our study, we focus on analyzing generally influential factors such as the impact of the timing and the data as well as the impact on the evaluation, which is important for deciding, if the model should be deployed. We show that under high skews the evaluation is optimistic and hides the real impact. Furthermore, we demonstrate that the timing impacts the quality of the trained model. Our results offer insights for researchers and software architects aiming to build robust FL systems.

</details>


### [29] [Hapax Locks : Value-Based Mutual Exclusion](https://arxiv.org/abs/2511.14608)
*Dave Dice,Alex Kogan*

Main category: cs.DC

TL;DR: Hapax Locks 是一种新颖的锁算法，具有常数时间的加锁/解锁路径、FIFO 排队顺序、空间高效且在争用下产生较少缓存一致性流量，性能媲美当前最优锁，同时对运行时环境依赖更少，易于集成。


<details>
  <summary>Details</summary>
Motivation: 现有高性能锁通常对运行时环境有较多约束或依赖，难以在已有系统中集成；作者旨在设计一种既高效又易于部署的锁机制。

Method: 提出 Hapax Locks 算法，其关键特性包括常数时间的到达与解锁路径、FIFO 公平性、低空间开销，并避免线程间指针转移或所有权逃逸。

Result: Hapax Locks 在延迟和可扩展性方面达到与当前先进锁相当的性能，同时显著减少对运行时环境的依赖，适用于现有 API 和系统。

Conclusion: Hapax Locks 在保持高性能的同时简化了实现与集成，是一种实用且高效的同步原语。

Abstract: We present Hapax Locks, a novel locking algorithm that is simple, enjoys constant-time arrival and unlock paths, provides FIFO admission order, and which is also space efficient and generates relatively little coherence traffic under contention in the common case. Hapax Locks offer performance (both latency and scalability) that is comparable with the best state of the art locks, while at the same time Hapax Locks impose fewer constraints and dependencies on the ambient runtime environment, making them particularly easy to integrate or retrofit into existing systems or under existing application programming interfaces Of particular note, no pointers shift or escape ownership between threads in our algorithm.

</details>


### [30] [Seer: Online Context Learning for Fast Synchronous LLM Reinforcement Learning](https://arxiv.org/abs/2511.14617)
*Ruoyu Qin,Weiran He,Weixiao Huang,Yangkun Zhang,Yikai Zhao,Bo Pang,Xinran Xu,Yingdi Shan,Yongwei Wu,Mingxing Zhang*

Main category: cs.DC

TL;DR: Seer 是一种新型在线上下文学习系统，通过利用相同提示下请求在输出长度和生成模式上的相似性，采用分阶段 rollout、上下文感知调度和自适应分组推测解码三项关键技术，显著提升强化学习中 rollout 阶段的吞吐量并降低长尾延迟。


<details>
  <summary>Details</summary>
Motivation: 现有同步强化学习系统在 rollout 阶段存在严重性能瓶颈，包括长尾延迟高和资源利用率低的问题，主要源于工作负载不平衡。

Method: 提出 Seer 系统，结合分阶段 rollout 实现动态负载均衡、上下文感知调度以及自适应分组推测解码，以优化 rollout 过程。

Result: 在生产级强化学习工作负载上的评估表明，Seer 相比当前最先进的同步 RL 系统，端到端 rollout 吞吐量提升 74%–97%，长尾延迟降低 75%–93%。

Conclusion: Seer 显著加速了强化学习训练迭代过程，有效解决了 rollout 阶段的性能瓶颈问题。

Abstract: Reinforcement Learning (RL) has become critical for advancing modern Large Language Models (LLMs), yet existing synchronous RL systems face severe performance bottlenecks. The rollout phase, which dominates end-to-end iteration time, suffers from substantial long-tail latency and poor resource utilization due to inherent workload imbalance. We present Seer, a novel online context learning system that addresses these challenges by exploiting previously overlooked similarities in output lengths and generation patterns among requests sharing the same prompt. Seer introduces three key techniques: divided rollout for dynamic load balancing, context-aware scheduling, and adaptive grouped speculative decoding. Together, these mechanisms substantially reduce long-tail latency and improve resource efficiency during rollout. Evaluations on production-grade RL workloads demonstrate that Seer improves end-to-end rollout throughput by 74% to 97% and reduces long-tail latency by 75% to 93% compared to state-of-the-art synchronous RL systems, significantly accelerating RL training iterations.

</details>


### [31] [Multi-GPU Quantum Circuit Simulation and the Impact of Network Performance](https://arxiv.org/abs/2511.14664)
*W. Michael Brown,Anurag Ramesh,Thomas Lubinski,Thien Nguyen,David E. Bernal Neira*

Main category: cs.DC

TL;DR: 本文将MPI引入QED-C应用导向基准测试中，以支持在高性能计算（HPC）系统上对多GPU量子电路模拟进行基准测试，并评估了不同互连技术（包括NVIDIA Grace Blackwell NVL72架构）对性能的影响，发现互连性能的提升对多GPU模拟的加速效果（16倍）远超GPU架构本身的改进（4.5倍）。


<details>
  <summary>Details</summary>
Motivation: 经典模拟量子算法资源消耗巨大，但对算法开发、验证和硬件设计至关重要。随着系统规模扩大，多GPU模拟成为必需，而GPU间通信可能成为性能瓶颈，因此需评估和优化多GPU互连技术。

Method: 在QED-C应用导向基准测试中集成MPI，利用多种互连路径（包括NVIDIA Grace Blackwell NVL72架构）进行多GPU量子模拟基准测试，比较不同代际GPU架构与互连技术对模拟性能的影响。

Result: 实验表明，相较于过去几代GPU架构带来的4.5倍加速，互连性能的提升对多GPU模拟的时间到解（time to solution）带来了超过16倍的性能改进。

Conclusion: 在多GPU量子算法模拟中，互连技术的进步比单个GPU架构的改进对整体性能影响更大，强调了高性能互连在大规模量子模拟中的关键作用。

Abstract: As is intrinsic to the fundamental goal of quantum computing, classical simulation of quantum algorithms is notoriously demanding in resource requirements. Nonetheless, simulation is critical to the success of the field and a requirement for algorithm development and validation, as well as hardware design. GPU-acceleration has become standard practice for simulation, and due to the exponential scaling inherent in classical methods, multi-GPU simulation can be required to achieve representative system sizes. In this case, inter-GPU communications can bottleneck performance. In this work, we present the introduction of MPI into the QED-C Application-Oriented Benchmarks to facilitate benchmarking on HPC systems. We review the advances in interconnect technology and the APIs for multi-GPU communication. We benchmark using a variety of interconnect paths, including the recent NVIDIA Grace Blackwell NVL72 architecture that represents the first product to expand high-bandwidth GPU-specialized interconnects across multiple nodes. We show that while improvements to GPU architecture have led to speedups of over 4.5X across the last few generations of GPUs, advances in interconnect performance have had a larger impact with over 16X performance improvements in time to solution for multi-GPU simulations.

</details>
