{"id": "2511.17849", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2511.17849", "abs": "https://arxiv.org/abs/2511.17849", "authors": ["Shuyuan Fan", "Zhao Zhang"], "title": "Pier: Efficient Large Language Model pretraining with Relaxed Global Communication", "comment": null, "summary": "Global communication, such as all-reduce and allgather, is the prominent performance bottleneck in large language model (LLM) pretraining. To address this issue, we present Pier, an efficient and scalable optimizer with relaxed global communication. Pier is built upon DiLoCo, which leverages an inner optimizer within groups of processors and an outer optimizer that requires global communication. To preserve the convergence and model performance, Pier incorporates two key techniques for the outer optimizer: momentum warmup and momentum decay. Pier employs an efficient and scalable system architecture to enable complex parallelization strategies in LLM pretraining. We examine the model performance and runtime reduction of Pier using the GPT model family (e.g., small, medium, XL, and 7B) and the OpenWebText dataset with a suite of thirteen downstream tasks. With data parallel strategy, Pier speeds up GPT-2 XL training by up to 2.7x-3.7x on 256 NVIDIA A100 GPUs and 1.2x-1.9x on 64 GH200 Superchips, respectively, without degradation of validation loss or downstream task performance. With data parallel and tensor parallel, Pier reduces the time cost GPT-2 7B model training by 54.5% on 128 A100s.", "AI": {"tldr": "Pier \u662f\u4e00\u79cd\u9ad8\u6548\u4e14\u53ef\u6269\u5c55\u7684\u4f18\u5316\u5668\uff0c\u901a\u8fc7\u51cf\u5c11\u5168\u5c40\u901a\u4fe1\u5f00\u9500\uff0c\u5728\u4fdd\u6301\u6a21\u578b\u6027\u80fd\u7684\u540c\u65f6\u663e\u8457\u52a0\u901f\u5927\u8bed\u8a00\u6a21\u578b\uff08\u5982 GPT \u7cfb\u5217\uff09\u7684\u9884\u8bad\u7ec3\u8fc7\u7a0b\u3002", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b\u9884\u8bad\u7ec3\u4e2d\u7684\u5168\u5c40\u901a\u4fe1\uff08\u5982 all-reduce \u548c allgather\uff09\u662f\u4e3b\u8981\u6027\u80fd\u74f6\u9888\uff0c\u4e9f\u9700\u66f4\u9ad8\u6548\u7684\u4f18\u5316\u7b56\u7565\u6765\u964d\u4f4e\u901a\u4fe1\u5f00\u9500\u5e76\u63d0\u5347\u8bad\u7ec3\u901f\u5ea6\u3002", "method": "Pier \u57fa\u4e8e DiLoCo \u6846\u67b6\uff0c\u5728\u5904\u7406\u5668\u7ec4\u5185\u4f7f\u7528\u5185\u5c42\u4f18\u5316\u5668\uff0c\u5728\u9700\u8981\u5168\u5c40\u901a\u4fe1\u65f6\u4f7f\u7528\u5916\u5c42\u4f18\u5316\u5668\uff0c\u5e76\u5f15\u5165\u52a8\u91cf\u9884\u70ed\uff08momentum warmup\uff09\u548c\u52a8\u91cf\u8870\u51cf\uff08momentum decay\uff09\u6280\u672f\u4ee5\u4fdd\u6301\u6536\u655b\u6027\u548c\u6a21\u578b\u6027\u80fd\u3002\u540c\u65f6\u91c7\u7528\u9ad8\u6548\u53ef\u6269\u5c55\u7684\u7cfb\u7edf\u67b6\u6784\u652f\u6301\u590d\u6742\u5e76\u884c\u7b56\u7565\u3002", "result": "\u5728 GPT-2 XL \u6a21\u578b\u4e0a\uff0cPier \u5728 256 \u5757 A100 GPU \u4e0a\u5b9e\u73b0 2.7x\u20133.7x \u7684\u8bad\u7ec3\u52a0\u901f\uff0c\u5728 64 \u5757 GH200 Superchips \u4e0a\u5b9e\u73b0 1.2x\u20131.9x \u52a0\u901f\uff1b\u5728 GPT-2 7B \u6a21\u578b\u4e0a\u7ed3\u5408\u6570\u636e\u5e76\u884c\u4e0e\u5f20\u91cf\u5e76\u884c\uff0c\u8bad\u7ec3\u65f6\u95f4\u51cf\u5c11 54.5%\uff0c\u4e14\u672a\u635f\u5931\u9a8c\u8bc1\u635f\u5931\u6216\u4e0b\u6e38\u4efb\u52a1\u6027\u80fd\u3002", "conclusion": "Pier \u80fd\u6709\u6548\u7f13\u89e3\u5927\u8bed\u8a00\u6a21\u578b\u9884\u8bad\u7ec3\u4e2d\u7684\u5168\u5c40\u901a\u4fe1\u74f6\u9888\uff0c\u5728\u4e0d\u727a\u7272\u6a21\u578b\u8d28\u91cf\u7684\u524d\u63d0\u4e0b\u663e\u8457\u63d0\u5347\u8bad\u7ec3\u6548\u7387\uff0c\u5177\u5907\u826f\u597d\u7684\u53ef\u6269\u5c55\u6027\u4e0e\u5b9e\u7528\u6027\u3002"}}
{"id": "2511.17882", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2511.17882", "abs": "https://arxiv.org/abs/2511.17882", "authors": ["Ruide Cao", "Zhuyun Qi", "Qinyang He", "Chenxi Ling", "Yi Wang", "Guoming Tang"], "title": "SAGkit: A Python SAG Toolkit for Response Time Analysis of Hybrid-Triggered Jobs", "comment": "6 pages, 5 figures, ICDCS 2025 Demo Paper", "summary": "For distributed control systems, modern latency-critical applications are increasingly demanding real-time guarantees and robustness. Response-time analysis (RTA) is useful for this purpose, as it helps analyze and guarantee timing bounds. However, conventional RTA methods struggle with the state-space explosion problem, especially in non-preemptive systems with release jitter and execution time variations. In this paper, we introduce SAGkit, a Python toolkit that implements the schedule-abstraction graph (SAG) framework. SAGkit novelly enables exact and sustainable RTA of hybrid-triggered jobs by allowing job absence on the SAG basis. Our experiments demonstrate that SAGkit achieves exactness with acceptable runtime and memory overhead. This lightweight toolkit empowers researchers to analyze complex distributed control systems and is open-access for further development.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86SAGkit\uff0c\u4e00\u4e2a\u57fa\u4e8e\u8c03\u5ea6\u62bd\u8c61\u56fe\uff08SAG\uff09\u6846\u67b6\u7684Python\u5de5\u5177\u5305\uff0c\u7528\u4e8e\u5bf9\u6df7\u5408\u89e6\u53d1\u4efb\u52a1\u8fdb\u884c\u7cbe\u786e\u4e14\u53ef\u6301\u7eed\u7684\u54cd\u5e94\u65f6\u95f4\u5206\u6790\uff08RTA\uff09\uff0c\u6709\u6548\u5e94\u5bf9\u4f20\u7edfRTA\u65b9\u6cd5\u5728\u975e\u62a2\u5360\u5f0f\u7cfb\u7edf\u4e2d\u56e0\u72b6\u6001\u7a7a\u95f4\u7206\u70b8\u5e26\u6765\u7684\u6311\u6218\u3002", "motivation": "\u73b0\u4ee3\u5206\u5e03\u5f0f\u63a7\u5236\u7cfb\u7edf\u4e2d\u7684\u5ef6\u8fdf\u654f\u611f\u578b\u5e94\u7528\u5bf9\u5b9e\u65f6\u6027\u4fdd\u8bc1\u548c\u9c81\u68d2\u6027\u8981\u6c42\u8d8a\u6765\u8d8a\u9ad8\uff0c\u800c\u4f20\u7edf\u54cd\u5e94\u65f6\u95f4\u5206\u6790\u65b9\u6cd5\u5728\u5904\u7406\u5177\u6709\u91ca\u653e\u6296\u52a8\u548c\u6267\u884c\u65f6\u95f4\u53d8\u5316\u7684\u975e\u62a2\u5360\u5f0f\u7cfb\u7edf\u65f6\u9762\u4e34\u72b6\u6001\u7a7a\u95f4\u7206\u70b8\u95ee\u9898\u3002", "method": "\u5f15\u5165SAGkit\u5de5\u5177\u5305\uff0c\u57fa\u4e8e\u8c03\u5ea6\u62bd\u8c61\u56fe\uff08SAG\uff09\u6846\u67b6\uff0c\u901a\u8fc7\u5141\u8bb8\u5728SAG\u57fa\u7840\u4e0a\u7f3a\u5931\u4efb\u52a1\u5b9e\u4f8b\uff0c\u5b9e\u73b0\u5bf9\u6df7\u5408\u89e6\u53d1\u4efb\u52a1\u7684\u7cbe\u786e\u54cd\u5e94\u65f6\u95f4\u5206\u6790\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cSAGkit\u5728\u4fdd\u6301\u53ef\u63a5\u53d7\u7684\u8fd0\u884c\u65f6\u95f4\u548c\u5185\u5b58\u5f00\u9500\u7684\u540c\u65f6\uff0c\u5b9e\u73b0\u4e86\u7cbe\u786e\u7684\u54cd\u5e94\u65f6\u95f4\u5206\u6790\u3002", "conclusion": "SAGkit\u662f\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u3001\u5f00\u6e90\u7684\u5de5\u5177\u5305\uff0c\u80fd\u591f\u652f\u6301\u7814\u7a76\u4eba\u5458\u5206\u6790\u590d\u6742\u7684\u5206\u5e03\u5f0f\u63a7\u5236\u7cfb\u7edf\uff0c\u5e76\u4e3a\u540e\u7eed\u5f00\u53d1\u63d0\u4f9b\u57fa\u7840\u3002"}}
{"id": "2511.18124", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2511.18124", "abs": "https://arxiv.org/abs/2511.18124", "authors": ["Sangam Ghimire", "Nigam Niraula", "Nirjal Bhurtel", "Paribartan Timalsina", "Bishal Neupane", "James Bhattarai", "Sudan Jha"], "title": "MIDAS: Adaptive Proxy Middleware for Mitigating Metadata Hotspots in HPC I/O at Scale", "comment": null, "summary": "Metadata hotspots remain one of the key obstacles to scalable Input/Output (I/O) in both High-Performance Computing (HPC) and cloud-scale storage environments. Situations such as job start-ups, checkpoint storms, or heavily skewed namespace access can trigger thousands of concurrent metadata requests against a small subset of servers. The result is long queues, inflated tail latencies, and reduced system throughput. Prior efforts including static namespace partitioning, backend-specific extensions, and kernel-level modifications address parts of the problem, but they often prove too rigid, intrusive to deploy, or unstable under shifting workloads. We present MIDAS, an adaptive middleware layer that operates transparently between clients and metadata servers, requiring no changes to kernels or storage backends. The design brings together three mechanisms: (i) a namespace-aware load balancer that enhances consistent hashing with power-of-d sampling informed by live telemetry, (ii) a cooperative caching layer that preserves backend semantics through leases, invalidations, or adaptive timeouts, and (iii) a self-stabilizing control loop that dynamically adjusts routing aggressiveness and cache lifetimes while avoiding oscillations under bursty workloads. Analysis of the model and controlled experiments show that MIDAS reduces average queue lengths by roughly 23% and mitigates worst-case hotspots by up to 80% when compared to round-robin scheduling. These findings highlight that a stability-aware, middleware-based strategy can provide backend-agnostic improvements to metadata management, enabling better scalability in bursty scenarios, more predictable tail latencies, and stronger overall system performance.", "AI": {"tldr": "MIDAS \u662f\u4e00\u79cd\u81ea\u9002\u5e94\u4e2d\u95f4\u4ef6\uff0c\u901a\u8fc7\u900f\u660e\u90e8\u7f72\u5728\u5ba2\u6237\u7aef\u4e0e\u5143\u6570\u636e\u670d\u52a1\u5668\u4e4b\u95f4\uff0c\u7ed3\u5408\u547d\u540d\u7a7a\u95f4\u611f\u77e5\u8d1f\u8f7d\u5747\u8861\u3001\u534f\u4f5c\u7f13\u5b58\u548c\u81ea\u7a33\u5b9a\u63a7\u5236\u73af\uff0c\u5728\u4e0d\u4fee\u6539\u5185\u6838\u6216\u5b58\u50a8\u540e\u7aef\u7684\u524d\u63d0\u4e0b\u663e\u8457\u7f13\u89e3\u5143\u6570\u636e\u70ed\u70b9\u95ee\u9898\u3002", "motivation": "\u5143\u6570\u636e\u70ed\u70b9\u662f\u9ad8\u6027\u80fd\u8ba1\u7b97\uff08HPC\uff09\u548c\u4e91\u5b58\u50a8\u73af\u5883\u4e2d\u53ef\u6269\u5c55 I/O \u7684\u4e3b\u8981\u969c\u788d\uff0c\u73b0\u6709\u65b9\u6cd5\u5982\u9759\u6001\u547d\u540d\u7a7a\u95f4\u5212\u5206\u3001\u540e\u7aef\u7279\u5b9a\u6269\u5c55\u6216\u5185\u6838\u7ea7\u4fee\u6539\u5f80\u5f80\u8fc7\u4e8e\u50f5\u5316\u3001\u4fb5\u5165\u6027\u5f3a\u6216\u5728\u52a8\u6001\u8d1f\u8f7d\u4e0b\u4e0d\u7a33\u5b9a\u3002", "method": "MIDAS \u91c7\u7528\u4e09\u79cd\u673a\u5236\uff1a(i) \u57fa\u4e8e\u5b9e\u65f6\u9065\u6d4b\u4fe1\u606f\u589e\u5f3a\u4e00\u81f4\u6027\u54c8\u5e0c\u7684\u547d\u540d\u7a7a\u95f4\u611f\u77e5\u8d1f\u8f7d\u5747\u8861\u5668\uff1b(ii) \u901a\u8fc7\u79df\u7ea6\u3001\u5931\u6548\u6216\u81ea\u9002\u5e94\u8d85\u65f6\u7ef4\u6301\u540e\u7aef\u8bed\u4e49\u7684\u534f\u4f5c\u7f13\u5b58\u5c42\uff1b(iii) \u52a8\u6001\u8c03\u8282\u8def\u7531\u6fc0\u8fdb\u7a0b\u5ea6\u548c\u7f13\u5b58\u5bff\u547d\u7684\u81ea\u7a33\u5b9a\u63a7\u5236\u73af\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u76f8\u6bd4\u8f6e\u8be2\u8c03\u5ea6\uff0cMIDAS \u5e73\u5747\u961f\u5217\u957f\u5ea6\u51cf\u5c11\u7ea6 23%\uff0c\u6700\u574f\u60c5\u51b5\u70ed\u70b9\u964d\u4f4e\u9ad8\u8fbe 80%\u3002", "conclusion": "MIDAS \u5c55\u793a\u4e86\u4e00\u79cd\u7a33\u5b9a\u6027\u611f\u77e5\u3001\u4e2d\u95f4\u4ef6\u9a71\u52a8\u7684\u7b56\u7565\u53ef\u5728\u4e0d\u4f9d\u8d56\u7279\u5b9a\u540e\u7aef\u7684\u60c5\u51b5\u4e0b\u63d0\u5347\u5143\u6570\u636e\u7ba1\u7406\u6548\u7387\uff0c\u4ece\u800c\u5728\u7a81\u53d1\u8d1f\u8f7d\u573a\u666f\u4e2d\u5b9e\u73b0\u66f4\u597d\u7684\u53ef\u6269\u5c55\u6027\u3001\u66f4\u53ef\u9884\u6d4b\u7684\u5c3e\u5ef6\u8fdf\u548c\u66f4\u5f3a\u7684\u6574\u4f53\u6027\u80fd\u3002"}}
{"id": "2511.18137", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2511.18137", "abs": "https://arxiv.org/abs/2511.18137", "authors": ["Christoph Goldgruber", "Benedikt Pittl", "Erich Schikuta"], "title": "Simulating Dynamic Cloud Marketspaces: Modeling Spot Instance Behavior and Scheduling with CloudSim Plus", "comment": null, "summary": "The increasing reliance on dynamic pricing models, such as spot instances, in public cloud environments presents new challenges for workload scheduling and reliability. While these models offer cost advantages, they introduce volatility and uncertainty that are not fully addressed by current allocation algorithms or simulation tools. This work contributes to the modeling and evaluation of such environments by extending the CloudSim Plus simulation framework to support realistic spot instance lifecycle management, including interruption, termination, hibernation, and reallocation. The enhanced simulator is validated using synthetic scenarios and large-scale simulations based on the Google Cluster Trace dataset. Building on this foundation, the HLEM-VMP allocation algorithm, originally proposed in earlier research, was adapted to operate under dynamic spot market conditions. Its performance was evaluated against baseline allocation strategies to assess its efficiency and resilience in volatile workload environments. The comparison demonstrated a reduction in the number of spot instance interruptions as well as a decrease in the maximum interruption duration. Overall, this work provides both a simulation framework for simulating dynamic cloud behavior and analytical insights into virtual machine allocation performance and market risk, contributing to more robust and cost-effective resource management in cloud computing.", "AI": {"tldr": "\u672c\u6587\u6269\u5c55\u4e86CloudSim Plus\u4eff\u771f\u6846\u67b6\u4ee5\u652f\u6301\u771f\u5b9e\u7684\u7ade\u4ef7\u5b9e\u4f8b\u751f\u547d\u5468\u671f\u7ba1\u7406\uff0c\u5e76\u5728\u6b64\u57fa\u7840\u4e0a\u6539\u8fdb\u4e86HLEM-VMP\u865a\u62df\u673a\u5206\u914d\u7b97\u6cd5\uff0c\u9a8c\u8bc1\u5176\u5728\u52a8\u6001\u4e91\u73af\u5883\u4e2d\u51cf\u5c11\u4e2d\u65ad\u6b21\u6570\u4e0e\u6301\u7eed\u65f6\u95f4\u7684\u6709\u6548\u6027\u3002", "motivation": "\u5f53\u524d\u516c\u6709\u4e91\u4e2d\u5e7f\u6cdb\u91c7\u7528\u7684\u52a8\u6001\u5b9a\u4ef7\u6a21\u578b\uff08\u5982\u7ade\u4ef7\u5b9e\u4f8b\uff09\u5e26\u6765\u4e86\u6210\u672c\u4f18\u52bf\uff0c\u4f46\u4e5f\u5f15\u5165\u4e86\u73b0\u6709\u8c03\u5ea6\u7b97\u6cd5\u548c\u4eff\u771f\u5de5\u5177\u672a\u80fd\u5145\u5206\u5e94\u5bf9\u7684\u4e0d\u786e\u5b9a\u6027\u548c\u6ce2\u52a8\u6027\u3002", "method": "\u6269\u5c55CloudSim Plus\u4eff\u771f\u6846\u67b6\u4ee5\u652f\u6301\u7ade\u4ef7\u5b9e\u4f8b\u7684\u4e2d\u65ad\u3001\u7ec8\u6b62\u3001\u4f11\u7720\u548c\u91cd\u65b0\u5206\u914d\u7b49\u751f\u547d\u5468\u671f\u884c\u4e3a\uff1b\u57fa\u4e8eGoogle Cluster Trace\u6570\u636e\u96c6\u8fdb\u884c\u5927\u89c4\u6a21\u4eff\u771f\u5b9e\u9a8c\uff1b\u6539\u8fdb\u5e76\u8bc4\u4f30HLEM-VMP\u7b97\u6cd5\u5728\u52a8\u6001\u7ade\u4ef7\u5e02\u573a\u6761\u4ef6\u4e0b\u7684\u6027\u80fd\u3002", "result": "\u6539\u8fdb\u540e\u7684HLEM-VMP\u7b97\u6cd5\u76f8\u6bd4\u57fa\u7ebf\u7b56\u7565\u663e\u8457\u51cf\u5c11\u4e86\u7ade\u4ef7\u5b9e\u4f8b\u4e2d\u65ad\u6b21\u6570\u548c\u6700\u5927\u4e2d\u65ad\u6301\u7eed\u65f6\u95f4\u3002", "conclusion": "\u672c\u7814\u7a76\u4e0d\u4ec5\u63d0\u4f9b\u4e86\u6a21\u62df\u52a8\u6001\u4e91\u73af\u5883\u7684\u6709\u6548\u4eff\u771f\u6846\u67b6\uff0c\u8fd8\u4e3a\u865a\u62df\u673a\u5206\u914d\u7b56\u7565\u5728\u6210\u672c\u6548\u76ca\u4e0e\u53ef\u9760\u6027\u4e4b\u95f4\u7684\u6743\u8861\u63d0\u4f9b\u4e86\u5206\u6790\u4f9d\u636e\uff0c\u6709\u52a9\u4e8e\u63d0\u5347\u4e91\u8d44\u6e90\u7ba1\u7406\u7684\u9c81\u68d2\u6027\u3002"}}
{"id": "2511.17922", "categories": ["cs.PF"], "pdf": "https://arxiv.org/pdf/2511.17922", "abs": "https://arxiv.org/abs/2511.17922", "authors": ["Robert Krahn", "Josia M\u00e4dler", "Christoph Seidl", "Christof Fetzer"], "title": "GROOT: General-Purpose Automatic Parameter Tuning Across Layers, Domains, and Use Cases", "comment": "International Conferences on Applied Computing 2025 and WWW/Internet 2025", "summary": "Modern software systems are executed on a runtime stack with layers (virtualization, storage, trusted execution, etc.) each incurring an execution and/or monetary cost, which may be mitigated by finding suitable parameter configurations. While specialized parameter tuners exist, they are tied to a particular domain or use case, fixed in type and number of optimization goals, or focused on a specific layer or technology. These limitations pose significant adoption hurdles for specialized and innovative ventures (SIVs) that address a variety of domains and use cases, operate under strict cost-performance constraints requiring tradeoffs, and rely on self-hosted servers with custom technology stacks while having little data or expertise to set up and operate specialized tuners. In this paper, we present Groot - a general-purpose configuration tuner designed to a) be explicitly agnostic of a particular domain or use case, b) balance multiple potentially competing optimization goals, c) support different custom technology setups, and d) make minimal assumptions about parameter types, ranges, or suitable values. Our evaluation on both real-world use cases and benchmarks shows that Groot reliably improves performance and reduces resource consumption in scenarios representative for SIVs.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa Groot\uff0c\u4e00\u79cd\u901a\u7528\u914d\u7f6e\u8c03\u4f18\u5de5\u5177\uff0c\u9002\u7528\u4e8e\u591a\u9886\u57df\u3001\u591a\u76ee\u6807\u4f18\u5316\uff0c\u652f\u6301\u81ea\u5b9a\u4e49\u6280\u672f\u6808\uff0c\u5e76\u5728\u771f\u5b9e\u573a\u666f\u4e2d\u6709\u6548\u63d0\u5347\u6027\u80fd\u4e0e\u964d\u4f4e\u8d44\u6e90\u6d88\u8017\u3002", "motivation": "\u73b0\u6709\u53c2\u6570\u8c03\u4f18\u5de5\u5177\u901a\u5e38\u5c40\u9650\u4e8e\u7279\u5b9a\u9886\u57df\u3001\u56fa\u5b9a\u4f18\u5316\u76ee\u6807\u6216\u7279\u5b9a\u6280\u672f\u5c42\uff0c\u96be\u4ee5\u6ee1\u8db3\u4e13\u4e1a\u521b\u65b0\u4f01\u4e1a\uff08SIVs\uff09\u5728\u591a\u53d8\u5e94\u7528\u573a\u666f\u3001\u6210\u672c-\u6027\u80fd\u6743\u8861\u53ca\u81ea\u6258\u7ba1\u5b9a\u5236\u6280\u672f\u6808\u4e0b\u7684\u8c03\u4f18\u9700\u6c42\u3002", "method": "\u8bbe\u8ba1\u5e76\u5b9e\u73b0 Groot\uff0c\u4e00\u4e2a\u901a\u7528\u914d\u7f6e\u8c03\u4f18\u5668\uff0c\u5176\u7279\u70b9\u5305\u62ec\uff1a\u9886\u57df\u65e0\u5173\u6027\u3001\u652f\u6301\u591a\u76ee\u6807\u4f18\u5316\u3001\u517c\u5bb9\u4e0d\u540c\u81ea\u5b9a\u4e49\u6280\u672f\u6808\uff0c\u4ee5\u53ca\u5bf9\u53c2\u6570\u7c7b\u578b\u548c\u53d6\u503c\u8303\u56f4\u4e0d\u505a\u5f3a\u5047\u8bbe\u3002", "result": "\u5728\u771f\u5b9e\u7528\u4f8b\u548c\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cGroot \u80fd\u7a33\u5b9a\u63d0\u5347\u7cfb\u7edf\u6027\u80fd\u5e76\u51cf\u5c11\u8d44\u6e90\u6d88\u8017\uff0c\u9002\u7528\u4e8e SIVs \u7684\u5178\u578b\u90e8\u7f72\u573a\u666f\u3002", "conclusion": "Groot \u6709\u6548\u89e3\u51b3\u4e86\u73b0\u6709\u8c03\u4f18\u5de5\u5177\u5728\u901a\u7528\u6027\u3001\u7075\u6d3b\u6027\u548c\u6613\u7528\u6027\u65b9\u9762\u7684\u4e0d\u8db3\uff0c\u4e3a\u8d44\u6e90\u53d7\u9650\u4e14\u6280\u672f\u6808\u591a\u6837\u7684 SIVs \u63d0\u4f9b\u4e86\u5b9e\u7528\u7684\u914d\u7f6e\u4f18\u5316\u65b9\u6848\u3002"}}
{"id": "2511.18323", "categories": ["cs.OS", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.18323", "abs": "https://arxiv.org/abs/2511.18323", "authors": ["Juha Jeon"], "title": "Crash-Consistent Checkpointing for AI Training on macOS/APFS", "comment": "18 pages, 6 figures. Independent mini-research report; not submitted to a conference or journal", "summary": "Deep learning training relies on periodic checkpoints to recover from failures, but unsafe checkpoint installation can leave corrupted files on disk. This paper presents an experimental study of checkpoint installation protocols and integrity validation for AI training on macOS/APFS. We implement three write modes with increasing durability guarantees: unsafe (baseline, no fsync), atomic_nodirsync (file-level durability via fsync()), and atomic_dirsync (file + directory durability). We design a format-agnostic integrity guard using SHA-256 checksums with automatic rollback. Through controlled experiments including crash injection (430 unsafe-mode trials) and corruption injection (1,600 atomic-mode trials), we demonstrate that the integrity guard detects 99.8-100% of corruptions with zero false positives. Performance overhead is 56.5-108.4% for atomic_nodirsync and 84.2-570.6% for atomic_dirsync relative to the unsafe baseline. Our findings quantify the reliability-performance trade-offs and provide deployment guidance for production AI infrastructure.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u5728 macOS/APFS \u4e0a\u8fdb\u884c AI \u8bad\u7ec3\u65f6\u7684\u68c0\u67e5\u70b9\u5b89\u88c5\u534f\u8bae\u4e0e\u5b8c\u6574\u6027\u9a8c\u8bc1\u673a\u5236\uff0c\u63d0\u51fa\u4e86\u4e09\u79cd\u5177\u6709\u4e0d\u540c\u6301\u4e45\u6027\u4fdd\u8bc1\u7684\u5199\u5165\u6a21\u5f0f\uff0c\u5e76\u8bbe\u8ba1\u4e86\u4e00\u79cd\u57fa\u4e8e SHA-256 \u7684\u683c\u5f0f\u65e0\u5173\u5b8c\u6574\u6027\u4fdd\u62a4\u673a\u5236\uff0c\u5728\u5927\u91cf\u5d29\u6e83\u4e0e\u635f\u574f\u6ce8\u5165\u5b9e\u9a8c\u4e2d\u5b9e\u73b0\u4e86\u63a5\u8fd1 100% \u7684\u635f\u574f\u68c0\u6d4b\u7387\u4e14\u65e0\u8bef\u62a5\uff0c\u540c\u65f6\u91cf\u5316\u4e86\u53ef\u9760\u6027\u4e0e\u6027\u80fd\u4e4b\u95f4\u7684\u6743\u8861\u3002", "motivation": "\u6df1\u5ea6\u5b66\u4e60\u8bad\u7ec3\u4f9d\u8d56\u5468\u671f\u6027\u68c0\u67e5\u70b9\u4ee5\u4ece\u6545\u969c\u4e2d\u6062\u590d\uff0c\u4f46\u4e0d\u5b89\u5168\u7684\u68c0\u67e5\u70b9\u5b89\u88c5\u53ef\u80fd\u5bfc\u81f4\u78c1\u76d8\u4e0a\u7559\u4e0b\u635f\u574f\u6587\u4ef6\uff0c\u5f71\u54cd\u8bad\u7ec3\u53ef\u9760\u6027\u3002\u56e0\u6b64\uff0c\u9700\u8981\u7814\u7a76\u66f4\u5b89\u5168\u3001\u53ef\u9760\u7684\u68c0\u67e5\u70b9\u5b89\u88c5\u534f\u8bae\u548c\u5b8c\u6574\u6027\u9a8c\u8bc1\u65b9\u6cd5\u3002", "method": "\u5b9e\u73b0\u4e09\u79cd\u5199\u5165\u6a21\u5f0f\uff08unsafe\u3001atomic_nodirsync\u3001atomic_dirsync\uff09\uff0c\u63d0\u4f9b\u9012\u589e\u7684\u6301\u4e45\u6027\u4fdd\u8bc1\uff1b\u8bbe\u8ba1\u4e00\u79cd\u683c\u5f0f\u65e0\u5173\u7684\u5b8c\u6574\u6027\u4fdd\u62a4\u673a\u5236\uff0c\u4f7f\u7528 SHA-256 \u6821\u9a8c\u548c\u5e76\u652f\u6301\u81ea\u52a8\u56de\u6eda\uff1b\u901a\u8fc7\u5d29\u6e83\u6ce8\u5165\uff08430 \u6b21 unsafe \u6a21\u5f0f\u8bd5\u9a8c\uff09\u548c\u635f\u574f\u6ce8\u5165\uff081600 \u6b21 atomic \u6a21\u5f0f\u8bd5\u9a8c\uff09\u8fdb\u884c\u5b9e\u9a8c\u8bc4\u4f30\u3002", "result": "\u5b8c\u6574\u6027\u4fdd\u62a4\u673a\u5236\u5728\u5b9e\u9a8c\u4e2d\u68c0\u6d4b\u5230 99.8%-100% \u7684\u635f\u574f\uff0c\u4e14\u65e0\u5047\u9633\u6027\uff1b\u76f8\u6bd4 unsafe \u57fa\u7ebf\uff0catomic_nodirsync \u6027\u80fd\u5f00\u9500\u4e3a 56.5%-108.4%\uff0catomic_dirsync \u4e3a 84.2%-570.6%\u3002", "conclusion": "\u8be5\u7814\u7a76\u91cf\u5316\u4e86 AI \u8bad\u7ec3\u4e2d\u68c0\u67e5\u70b9\u673a\u5236\u5728\u53ef\u9760\u6027\u4e0e\u6027\u80fd\u4e4b\u95f4\u7684\u6743\u8861\uff0c\u4e3a\u751f\u4ea7\u73af\u5883\u4e2d\u90e8\u7f72\u9ad8\u53ef\u9760 AI \u57fa\u7840\u8bbe\u65bd\u63d0\u4f9b\u4e86\u5b9e\u8df5\u6307\u5bfc\u3002"}}
{"id": "2511.17773", "categories": ["cs.AR", "cs.CR"], "pdf": "https://arxiv.org/pdf/2511.17773", "abs": "https://arxiv.org/abs/2511.17773", "authors": ["Shiv Kaushik", "Mahesh Madhav", "Nagi Aboulenein", "Jason Bessette", "Sandeep Brahmadathan", "Ben Chaffin", "Matthew Erler", "Stephan Jourdan", "Thomas Maciukenas", "Ramya Masti", "Jon Perry", "Massimo Sutera", "Scott Tetrick", "Bret Toll", "David Turley", "Carl Worth", "Atiq Bajwa"], "title": "Optimized Memory Tagging on AmpereOne Processors", "comment": "12 pages, 8 figures", "summary": "Memory-safety escapes continue to form the launching pad for a wide range of security attacks, especially for the substantial base of deployed software that is coded in pointer-based languages such as C/C++. Although compiler and Instruction Set Architecture (ISA) extensions have been introduced to address elements of this issue, the overhead and/or comprehensive applicability have limited broad production deployment. The Memory Tagging Extension (MTE) to the ARM AArch64 Instruction Set Architecture is a valuable tool to address memory-safety escapes; when used in synchronous tag-checking mode, MTE provides deterministic detection and prevention of sequential buffer overflow attacks, and probabilistic detection and prevention of exploits resulting from temporal use-after-free pointer programming bugs. The AmpereOne processor, launched in 2024, is the first datacenter processor to support MTE. Its optimized MTE implementation uniquely incurs no memory capacity overhead for tag storage and provides synchronous tag-checking with single-digit performance impact across a broad range of datacenter class workloads. Furthermore, this paper analyzes the complete hardware-software stack, identifying application memory management as the primary remaining source of overhead and highlighting clear opportunities for software optimization. The combination of an efficient hardware foundation and a clear path for software improvement makes the MTE implementation of the AmpereOne processor highly attractive for deployment in production cloud environments.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86AmpereOne\u5904\u7406\u5668\u5bf9ARM AArch64\u67b6\u6784\u4e2d\u5185\u5b58\u6807\u8bb0\u6269\u5c55\uff08MTE\uff09\u7684\u9ad8\u6548\u5b9e\u73b0\uff0c\u8be5\u5b9e\u73b0\u65e0\u5185\u5b58\u5bb9\u91cf\u5f00\u9500\uff0c\u5e76\u5728\u6570\u636e\u4e2d\u5fc3\u5de5\u4f5c\u8d1f\u8f7d\u4e2d\u4ec5\u5e26\u6765\u4e2a\u4f4d\u6570\u6027\u80fd\u5f71\u54cd\uff0c\u4e3a\u4e91\u73af\u5883\u4e2d\u7684\u5185\u5b58\u5b89\u5168\u63d0\u4f9b\u4e86\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\u3002", "motivation": "\u5185\u5b58\u5b89\u5168\u6f0f\u6d1e\u4ecd\u662fC/C++\u7b49\u6307\u9488\u8bed\u8a00\u7f16\u5199\u8f6f\u4ef6\u9762\u4e34\u7684\u4e3b\u8981\u5b89\u5168\u5a01\u80c1\uff0c\u73b0\u6709\u7f16\u8bd1\u5668\u548cISA\u6269\u5c55\u65b9\u6848\u56e0\u5f00\u9500\u5927\u6216\u9002\u7528\u6027\u6709\u9650\u800c\u96be\u4ee5\u5e7f\u6cdb\u90e8\u7f72\u3002", "method": "\u91c7\u7528ARM AArch64\u7684Memory Tagging Extension\uff08MTE\uff09\uff0c\u5728AmpereOne\u5904\u7406\u5668\u4e0a\u5b9e\u73b0\u540c\u6b65\u6807\u8bb0\u68c0\u67e5\uff0c\u65e0\u9700\u989d\u5916\u5185\u5b58\u5b58\u50a8\u6807\u7b7e\uff0c\u5e76\u5206\u6790\u8f6f\u786c\u4ef6\u6808\u4ee5\u8bc6\u522b\u4f18\u5316\u70b9\u3002", "result": "AmpereOne\u5904\u7406\u5668\u652f\u6301MTE\u4e14\u65e0\u5185\u5b58\u5bb9\u91cf\u5f00\u9500\uff0c\u5728\u591a\u79cd\u6570\u636e\u4e2d\u5fc3\u5de5\u4f5c\u8d1f\u8f7d\u4e0b\u540c\u6b65\u6a21\u5f0f\u4ec5\u5e26\u6765\u4e2a\u4f4d\u6570\u767e\u5206\u6bd4\u7684\u6027\u80fd\u635f\u8017\uff0c\u5e76\u6307\u51fa\u5e94\u7528\u5185\u5b58\u7ba1\u7406\u662f\u5f53\u524d\u4e3b\u8981\u5f00\u9500\u6765\u6e90\u3002", "conclusion": "AmpereOne\u7684MTE\u5b9e\u73b0\u7ed3\u5408\u9ad8\u6548\u7684\u786c\u4ef6\u57fa\u7840\u4e0e\u660e\u786e\u7684\u8f6f\u4ef6\u4f18\u5316\u8def\u5f84\uff0c\u975e\u5e38\u9002\u5408\u5728\u751f\u4ea7\u4e91\u73af\u5883\u4e2d\u90e8\u7f72\uff0c\u4ee5\u6709\u6548\u7f13\u89e3\u5185\u5b58\u5b89\u5168\u95ee\u9898\u3002"}}
{"id": "2511.17580", "categories": ["cs.MA", "cs.AI", "cs.DC", "cs.SE"], "pdf": "https://arxiv.org/pdf/2511.17580", "abs": "https://arxiv.org/abs/2511.17580", "authors": ["Leszek Sliwko", "Aleksander Zgrzywa"], "title": "A novel strategy for multi-resource load balancing in agent-based systems", "comment": null, "summary": "The paper presents a multi-resource load balancing strategy which can be utilised within an agent-based system. This approach can assist system designers in their attempts to optimise the structure for complex enterprise architectures. In this system, the social behaviour of the agent and its adaptation abilities are applied to determine an optimal setup for a given configuration. All the methods have been developed to allow the agent's self-assessment. The proposed agent system has been implemented and the experiment results are presented here.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u667a\u80fd\u4f53\u7684\u591a\u8d44\u6e90\u8d1f\u8f7d\u5747\u8861\u7b56\u7565\uff0c\u5229\u7528\u667a\u80fd\u4f53\u7684\u793e\u4f1a\u884c\u4e3a\u4e0e\u81ea\u9002\u5e94\u80fd\u529b\u4f18\u5316\u590d\u6742\u4f01\u4e1a\u67b6\u6784\u7684\u7ed3\u6784\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "motivation": "\u4e3a\u5e2e\u52a9\u7cfb\u7edf\u8bbe\u8ba1\u8005\u4f18\u5316\u590d\u6742\u4f01\u4e1a\u67b6\u6784\u7684\u7ed3\u6784\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u591f\u52a8\u6001\u8c03\u6574\u5e76\u5b9e\u73b0\u8d1f\u8f7d\u5747\u8861\u7684\u667a\u80fd\u65b9\u6cd5\u3002", "method": "\u91c7\u7528\u57fa\u4e8e\u667a\u80fd\u4f53\u7684\u7cfb\u7edf\uff0c\u7ed3\u5408\u667a\u80fd\u4f53\u7684\u793e\u4f1a\u884c\u4e3a\u548c\u81ea\u9002\u5e94\u80fd\u529b\uff0c\u4f7f\u5176\u5177\u5907\u81ea\u6211\u8bc4\u4f30\u529f\u80fd\uff0c\u4ece\u800c\u786e\u5b9a\u7ed9\u5b9a\u914d\u7f6e\u4e0b\u7684\u6700\u4f18\u8bbe\u7f6e\u3002", "result": "\u6240\u63d0\u51fa\u7684\u667a\u80fd\u4f53\u7cfb\u7edf\u5df2\u6210\u529f\u5b9e\u73b0\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u5c55\u793a\u4e86\u5176\u5728\u591a\u8d44\u6e90\u8d1f\u8f7d\u5747\u8861\u65b9\u9762\u7684\u6548\u679c\u3002", "conclusion": "\u8be5\u591a\u8d44\u6e90\u8d1f\u8f7d\u5747\u8861\u7b56\u7565\u80fd\u6709\u6548\u652f\u6301\u590d\u6742\u4f01\u4e1a\u67b6\u6784\u7684\u4f18\u5316\uff0c\u5177\u6709\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2511.17762", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2511.17762", "abs": "https://arxiv.org/abs/2511.17762", "authors": ["Henning Femmer", "Ivan Esau"], "title": "The Software Engineering Simulations Lab: Agentic AI for RE Quality Simulations", "comment": null, "summary": "Context and motivation. Quality in Requirements Engineering (RE) is still predominantly anecdotal and intuition-driven. Creating a solid requirements quality model requires broad sets of empirical evidence to evaluate quality factors and their context. Problem. However, empirical data on the detailed effects of requirements quality defects is scarce, since it is costly to obtain. Furthermore, with the advent of AI-based development, the requirements quality factors may change: Requirements are no longer only consumed by humans, but increasingly also by AI agents, which might lead to a different efficient and effective requirements style. Principal ideas. We propose to extend the RE research toolbox with Agentic AI simulations, in which software engineering (SE) processes are replicated by standardized agents in stochastic, dynamic, event-driven, qualitative simulations. We argue that their speed and simplicity makes them a valuable addition to RE research, although limitations in replicating human behavior need to be studied and understood. Contribution. This paper contributes a first concept, a research roadmap, a prototype, and a first feasibility study for RE simulations with agentic AI. Study results indicate that even a naive implementation leads to executable simulations, encouraging technical improvements along with broader application in RE research.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u5229\u7528\u57fa\u4e8e\u667a\u80fd\u4f53\u7684AI\u4eff\u771f\u6765\u8865\u5145\u9700\u6c42\u5de5\u7a0b\uff08RE\uff09\u7814\u7a76\uff0c\u901a\u8fc7\u6a21\u62df\u8f6f\u4ef6\u5de5\u7a0b\u8fc7\u7a0b\u751f\u6210\u7ecf\u9a8c\u6570\u636e\uff0c\u4ee5\u5e94\u5bf9\u9ad8\u8d28\u91cf\u9700\u6c42\u7f3a\u9677\u5b9e\u8bc1\u6570\u636e\u7a00\u7f3a\u7684\u95ee\u9898\uff0c\u5e76\u9002\u5e94AI\u53c2\u4e0e\u9700\u6c42\u6d88\u8d39\u7684\u65b0\u8d8b\u52bf\u3002", "motivation": "\u5f53\u524d\u9700\u6c42\u5de5\u7a0b\u4e2d\u7684\u8d28\u91cf\u8bc4\u4f30\u4ecd\u4e3b\u8981\u4f9d\u8d56\u76f4\u89c9\u548c\u8f76\u4e8b\u8bc1\u636e\uff0c\u7f3a\u4e4f\u5927\u91cf\u5b9e\u8bc1\u6570\u636e\u652f\u6301\uff1b\u540c\u65f6\uff0c\u968f\u7740AI\u8d8a\u6765\u8d8a\u591a\u5730\u53c2\u4e0e\u5f00\u53d1\u8fc7\u7a0b\uff0c\u4f20\u7edf\u9762\u5411\u4eba\u7c7b\u7684\u9700\u6c42\u8868\u8fbe\u65b9\u5f0f\u53ef\u80fd\u4e0d\u518d\u6700\u4f18\uff0c\u4e9f\u9700\u65b0\u7684\u7814\u7a76\u624b\u6bb5\u6765\u63a2\u7d22\u9002\u7528\u4e8e\u4eba\u673a\u5171\u8bfb\u7684\u9700\u6c42\u8d28\u91cf\u56e0\u7d20\u3002", "method": "\u4f5c\u8005\u63d0\u51fa\u5728\u9700\u6c42\u5de5\u7a0b\u7814\u7a76\u4e2d\u5f15\u5165\u57fa\u4e8e\u667a\u80fd\u4f53\u7684AI\u4eff\u771f\u65b9\u6cd5\uff0c\u901a\u8fc7\u6784\u5efa\u6807\u51c6\u5316\u667a\u80fd\u4f53\uff0c\u5728\u968f\u673a\u3001\u52a8\u6001\u3001\u4e8b\u4ef6\u9a71\u52a8\u7684\u5b9a\u6027\u4eff\u771f\u73af\u5883\u4e2d\u590d\u73b0\u8f6f\u4ef6\u5de5\u7a0b\u6d41\u7a0b\uff0c\u4ece\u800c\u9ad8\u6548\u751f\u6210\u5173\u4e8e\u9700\u6c42\u7f3a\u9677\u5f71\u54cd\u7684\u7ecf\u9a8c\u6570\u636e\u3002", "result": "\u7814\u7a76\u5f00\u53d1\u4e86\u4e00\u4e2a\u521d\u6b65\u6982\u5ff5\u3001\u7814\u7a76\u8def\u7ebf\u56fe\u3001\u539f\u578b\u7cfb\u7edf\u53ca\u53ef\u884c\u6027\u7814\u7a76\uff0c\u7ed3\u679c\u8868\u660e\u5373\u4f7f\u91c7\u7528\u7b80\u5355\u5b9e\u73b0\u4e5f\u80fd\u8fd0\u884c\u6709\u6548\u4eff\u771f\uff0c\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\u7684\u6280\u672f\u53ef\u884c\u6027\u5e76\u5c55\u793a\u4e86\u5176\u5728RE\u7814\u7a76\u4e2d\u7684\u6f5c\u529b\u3002", "conclusion": "\u57fa\u4e8e\u667a\u80fd\u4f53\u7684AI\u4eff\u771f\u662f\u5bf9\u9700\u6c42\u5de5\u7a0b\u7814\u7a76\u5de5\u5177\u7bb1\u7684\u6709\u76ca\u8865\u5145\uff0c\u5c3d\u7ba1\u5728\u6a21\u62df\u4eba\u7c7b\u884c\u4e3a\u65b9\u9762\u5b58\u5728\u5c40\u9650\uff0c\u4f46\u5176\u9ad8\u6548\u6027\u548c\u7b80\u6613\u6027\u4f7f\u5176\u503c\u5f97\u8fdb\u4e00\u6b65\u6280\u672f\u4f18\u5316\u4e0e\u5e7f\u6cdb\u5e94\u7528\u3002"}}
{"id": "2511.18151", "categories": ["cs.DC", "cs.AR", "cs.CV", "cs.LG", "cs.NI"], "pdf": "https://arxiv.org/pdf/2511.18151", "abs": "https://arxiv.org/abs/2511.18151", "authors": ["Rajat Bhattacharjya", "Sing-Yao Wu", "Hyunwoo Oh", "Chaewon Nam", "Suyeon Koo", "Mohsen Imani", "Elaheh Bozorgzadeh", "Nikil Dutt"], "title": "AVERY: Adaptive VLM Split Computing through Embodied Self-Awareness for Efficient Disaster Response Systems", "comment": "8 pages, 5 figures. Paper is currently under review. Authors' version posted for personal use and not for redistribution", "summary": "Unmanned Aerial Vehicles (UAVs) in disaster response require complex, queryable intelligence that on-board CNNs cannot provide. While Vision-Language Models (VLMs) offer this semantic reasoning, their high resource demands make on-device deployment infeasible, and naive cloud offloading fails under the low-bandwidth networks common in disaster zones. We present AVERY, a framework that enables VLM deployment through adaptive split computing. We advance the split computing paradigm beyond traditional depth-wise partitioning by introducing a functional, cognitive-inspired dual-stream split that separates the VLM into a high-frequency, low-resolution \"context stream\" for real-time awareness and a low-frequency, high-fidelity \"insight stream\" for deep analysis. A lightweight, self-aware on-board controller manages this architecture, monitoring network conditions and operator intent to dynamically select from pre-trained compression models, navigating the fundamental accuracy-throughput trade-off. Evaluated using the VLM LISA-7B across an edge-cloud scenario under fluctuating network conditions, AVERY consistently outperforms static configurations, achieving 11.2% higher accuracy than raw image compression and 93.98% lower energy consumption compared to full-edge execution, thereby enhancing mission efficiency and enabling real-time, queryable intelligence on resource-constrained platforms in dynamic environments.", "AI": {"tldr": "AVERY is an adaptive split-computing framework that enables efficient deployment of Vision-Language Models (VLMs) on UAVs in disaster response by splitting VLM processing into dual streams\u2014context and insight\u2014and dynamically managing compression based on network conditions and operator intent, achieving higher accuracy and lower energy use.", "motivation": "UAVs in disaster zones need queryable semantic intelligence beyond what on-board CNNs offer, but VLMs are too resource-intensive for on-device use and cloud offloading struggles under low-bandwidth conditions.", "method": "AVERY introduces a cognitive-inspired dual-stream split computing approach: a high-frequency, low-resolution \u201ccontext stream\u201d for real-time awareness and a low-frequency, high-fidelity \u201cinsight stream\u201d for deep analysis, managed by a lightweight on-board controller that adapts compression models based on network state and user intent.", "result": "In edge-cloud evaluations with LISA-7B under variable networks, AVERY achieved 11.2% higher accuracy than raw image compression and 93.98% lower energy consumption versus full-edge execution.", "conclusion": "AVERY effectively balances accuracy and efficiency to enable real-time, queryable VLM intelligence on resource-constrained UAVs operating in dynamic, low-bandwidth disaster environments."}}
{"id": "2511.18674", "categories": ["cs.PF", "cs.AI", "cs.DC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.18674", "abs": "https://arxiv.org/abs/2511.18674", "authors": ["Alfredo Metere"], "title": "Low-Rank GEMM: Efficient Matrix Multiplication via Low-Rank Approximation with FP8 Acceleration", "comment": null, "summary": "Large matrix multiplication is a cornerstone of modern machine learning workloads, yet traditional approaches suffer from cubic computational complexity (e.g., $\\mathcal{O}(n^3)$ for a matrix of size $n\\times n$). We present Low-Rank GEMM, a novel approach that leverages low-rank matrix approximations to achieve sub-quadratic complexity while maintaining hardware-accelerated performance through FP8 precision and intelligent kernel selection. On a NVIDIA RTX 4090, our implementation achieves up to 378 TFLOPS on matrices up to $N=20480$, providing 75\\% memory savings and $7.8\\times$ speedup over PyTorch FP32 for large matrices. The system automatically adapts to hardware capabilities, selecting optimal decomposition methods (SVD, randomized SVD) and precision levels based on matrix characteristics and available accelerators. Comprehensive benchmarking on NVIDIA RTX 4090 demonstrates that Low-Rank GEMM becomes the fastest approach for matrices $N\\geq10240$, surpassing traditional cuBLAS implementations through memory bandwidth optimization rather than computational shortcuts.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faLow-Rank GEMM\u65b9\u6cd5\uff0c\u5229\u7528\u4f4e\u79e9\u77e9\u9635\u8fd1\u4f3c\u7ed3\u5408FP8\u7cbe\u5ea6\u548c\u667a\u80fd\u6838\u9009\u62e9\uff0c\u5728NVIDIA RTX 4090\u4e0a\u5b9e\u73b0\u9ad8\u8fbe378 TFLOPS\u7684\u6027\u80fd\uff0c\u76f8\u6bd4PyTorch FP32\u5728\u5927\u77e9\u9635\u4e0a\u63d0\u901f7.8\u500d\u5e76\u8282\u770175%\u5185\u5b58\u3002", "motivation": "\u4f20\u7edf\u5927\u77e9\u9635\u4e58\u6cd5\u5177\u6709\u7acb\u65b9\u8ba1\u7b97\u590d\u6742\u5ea6\uff08\u5982$\\mathcal{O}(n^3)$\uff09\uff0c\u96be\u4ee5\u6ee1\u8db3\u73b0\u4ee3\u673a\u5668\u5b66\u4e60\u5bf9\u9ad8\u6548\u8ba1\u7b97\u7684\u9700\u6c42\uff0c\u4e9f\u9700\u4e00\u79cd\u517c\u987e\u8ba1\u7b97\u6548\u7387\u3001\u5185\u5b58\u5360\u7528\u4e0e\u786c\u4ef6\u52a0\u901f\u80fd\u529b\u7684\u65b0\u65b9\u6cd5\u3002", "method": "\u91c7\u7528\u4f4e\u79e9\u77e9\u9635\u8fd1\u4f3c\u6280\u672f\uff0c\u7ed3\u5408FP8\u7cbe\u5ea6\u8ba1\u7b97\u4e0e\u81ea\u52a8\u5316\u7684\u5185\u6838\u9009\u62e9\u7b56\u7565\uff08\u5982SVD\u6216\u968f\u673aSVD\uff09\uff0c\u6839\u636e\u77e9\u9635\u7279\u6027\u4e0e\u786c\u4ef6\u80fd\u529b\u52a8\u6001\u4f18\u5316\u5206\u89e3\u65b9\u5f0f\u4e0e\u7cbe\u5ea6\u3002", "result": "\u5728NVIDIA RTX 4090\u4e0a\uff0c\u5bf9\u5c3a\u5bf8\u8fbe$N=20480$\u7684\u77e9\u9635\u5b9e\u73b0\u6700\u9ad8378 TFLOPS\u6027\u80fd\uff1b\u76f8\u6bd4PyTorch FP32\uff0c\u5185\u5b58\u8282\u770175%\uff0c\u901f\u5ea6\u63d0\u53477.8\u500d\uff1b\u5f53$N\\geq10240$\u65f6\uff0c\u6027\u80fd\u8d85\u8d8a\u4f20\u7edfcuBLAS\u5b9e\u73b0\u3002", "conclusion": "Low-Rank GEMM\u901a\u8fc7\u5185\u5b58\u5e26\u5bbd\u4f18\u5316\u800c\u975e\u8ba1\u7b97\u6377\u5f84\uff0c\u5728\u5927\u77e9\u9635\u573a\u666f\u4e0b\u5b9e\u73b0\u4e86\u663e\u8457\u7684\u6027\u80fd\u4e0e\u6548\u7387\u63d0\u5347\uff0c\u6210\u4e3a\u9002\u7528\u4e8e\u73b0\u4ee3GPU\u786c\u4ef6\u7684\u9ad8\u6548\u77e9\u9635\u4e58\u6cd5\u65b0\u8303\u5f0f\u3002"}}
{"id": "2511.17506", "categories": ["cs.NI", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.17506", "abs": "https://arxiv.org/abs/2511.17506", "authors": ["Narjes Nourzad", "Mingyu Zong", "Bhaskar Krishnamachari"], "title": "AURA: Adaptive Unified Reasoning and Automation with LLM-Guided MARL for NextG Cellular Networks", "comment": null, "summary": "Next-generation (NextG) cellular networks are expected to manage dynamic traffic while sustaining high performance. Large language models (LLMs) provide strategic reasoning for 6G planning, but their computational cost and latency limit real-time use. Multi-agent reinforcement learning (MARL) supports localized adaptation, yet coordination at scale remains challenging. We present AURA, a framework that integrates cloud-based LLMs for high-level planning with base stations modeled as MARL agents for local decision-making. The LLM generates objectives and subgoals from its understanding of the environment and reasoning capabilities, while agents at base stations execute these objectives autonomously, guided by a trust mechanism that balances local learning with external input. To reduce latency, AURA employs batched communication so that agents update the LLM's view of the environment and receive improved feedback. In a simulated 6G scenario, AURA improves resilience, reducing dropped handoff requests by more than half under normal and high traffic and lowering system failures. Agents use LLM input in fewer than 60\\% of cases, showing that guidance augments rather than replaces local adaptability, thereby mitigating latency and hallucination risks. These results highlight the promise of combining LLM reasoning with MARL adaptability for scalable, real-time NextG network management.", "AI": {"tldr": "AURA is a hybrid framework combining cloud-based large language models (LLMs) for strategic planning and multi-agent reinforcement learning (MARL) at base stations for local execution, improving 6G network resilience and reducing latency.", "motivation": "Next-generation cellular networks need to handle dynamic traffic with high performance, but LLMs are too slow for real-time use and MARL struggles with large-scale coordination. AURA addresses these limitations by integrating both approaches.", "method": "AURA uses cloud-based LLMs to generate high-level objectives and subgoals based on environmental understanding, while base stations act as MARL agents that execute these goals locally. A trust mechanism balances external guidance with local learning, and batched communication reduces latency.", "result": "In a simulated 6G environment, AURA reduces dropped handoff requests by over 50% under normal and high traffic and decreases system failures. Agents rely on LLM input in fewer than 60% of decisions, showing effective augmentation without full dependency.", "conclusion": "Integrating LLM reasoning with MARL adaptability offers a scalable and efficient solution for real-time management of NextG cellular networks, balancing global strategy with local responsiveness while mitigating latency and hallucination risks."}}
{"id": "2511.17836", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2511.17836", "abs": "https://arxiv.org/abs/2511.17836", "authors": ["Edwin Sundberg", "Thea Ekmark", "Workneh Yilma Ayele"], "title": "Validating API Design Requirements for Interoperability: A Static Analysis Approach Using OpenAPI", "comment": "11 pages, 3 tables, 2 figures. Preprint. To appear in: PoEM2025: Companion Proceedings of the 18th IFIP Working Conference on the Practice of Enterprise Modeling: PoEM Forum, Doctoral Consortium, Business Case and Tool Forum, Workshops, December 3-5, 2025, Geneva, Switzerland", "summary": "RESTful APIs are central in developing interoperable, modular, and maintainable software systems in enterprises today. Also, it is essential to support system evolution, service interoperability, and governance across organizational boundaries to ensure good quality and consistency of these APIs. However, evaluating API design quality, which is part of non-functional requirement tasks, remains a largely manual and ad hoc process, particularly during early development. Using a Design Science Research (DSR) methodology, we elicited user needs, identified 75 API design rules using a literature review, and implemented a configurable rule engine to detect structural violations in OpenAPI specifications. The proposed tool supports organizational adaptability by allowing rules to be customized, enabled, or disabled, enabling integration of domain-specific standards. The evaluation was conducted through structured experiments and thematic analysis involving industry experts. API quality validation contributes to aligning technical designs with requirements and enterprise architecture by strengthening interoperability and governance between enterprise systems. The results show that S.E.O.R.A facilitates early validation of non-functional API requirements, provides actionable and traceable feedback, and aligns well with requirements elicitation and quality assurance processes. It improves the API design process by automating checks that would otherwise require manual inspection, thus supporting consistent and reusable conformance practices. This work contributes to requirements engineering by operationalizing design principles as verifiable constraints and embedding them into a practical validation tool. Future directions include IDE integration, expanded rule coverage, and real-world deployment to support continuous compliance in agile API development lifecycles.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aS.E.O.R.A\u7684\u53ef\u914d\u7f6e\u89c4\u5219\u5f15\u64ce\uff0c\u7528\u4e8e\u5728API\u8bbe\u8ba1\u65e9\u671f\u81ea\u52a8\u68c0\u6d4bOpenAPI\u89c4\u8303\u4e2d\u7684\u7ed3\u6784\u8fdd\u89c4\uff0c\u4ece\u800c\u652f\u6301\u4f01\u4e1a\u7ea7API\u7684\u8d28\u91cf\u3001\u4e92\u64cd\u4f5c\u6027\u548c\u6cbb\u7406\u3002", "motivation": "\u5f53\u524dAPI\u8bbe\u8ba1\u8d28\u91cf\u8bc4\u4f30\u4e3b\u8981\u4f9d\u8d56\u624b\u52a8\u4e14\u4e34\u65f6\u7684\u65b9\u5f0f\uff0c\u5c24\u5176\u5728\u5f00\u53d1\u65e9\u671f\u7f3a\u4e4f\u7cfb\u7edf\u5316\u652f\u6301\uff0c\u96be\u4ee5\u4fdd\u969cAPI\u7684\u4e00\u81f4\u6027\u3001\u4e92\u64cd\u4f5c\u6027\u53ca\u4e0e\u4f01\u4e1a\u67b6\u6784\u7684\u5bf9\u9f50\u3002", "method": "\u91c7\u7528\u8bbe\u8ba1\u79d1\u5b66\u7814\u7a76\u65b9\u6cd5\uff08DSR\uff09\uff0c\u901a\u8fc7\u6587\u732e\u7efc\u8ff0\u63d0\u70bc75\u6761API\u8bbe\u8ba1\u89c4\u5219\uff0c\u5e76\u6784\u5efa\u4e00\u4e2a\u53ef\u914d\u7f6e\u7684\u89c4\u5219\u5f15\u64ce\uff0c\u5141\u8bb8\u7ec4\u7ec7\u6839\u636e\u81ea\u8eab\u9700\u6c42\u542f\u7528\u3001\u7981\u7528\u6216\u5b9a\u5236\u89c4\u5219\uff1b\u901a\u8fc7\u7ed3\u6784\u5316\u5b9e\u9a8c\u548c\u884c\u4e1a\u4e13\u5bb6\u7684\u4e3b\u9898\u5206\u6790\u8fdb\u884c\u8bc4\u4f30\u3002", "result": "S.E.O.R.A\u80fd\u6709\u6548\u652f\u6301\u975e\u529f\u80fd\u6027API\u9700\u6c42\u7684\u65e9\u671f\u9a8c\u8bc1\uff0c\u63d0\u4f9b\u53ef\u8ffd\u6eaf\u7684\u53cd\u9988\uff0c\u81ea\u52a8\u5316\u539f\u672c\u9700\u4eba\u5de5\u68c0\u67e5\u7684\u8bbe\u8ba1\u5408\u89c4\u6027\u4efb\u52a1\uff0c\u5e76\u4e0e\u9700\u6c42\u5de5\u7a0b\u548c\u8d28\u91cf\u4fdd\u8bc1\u6d41\u7a0b\u826f\u597d\u96c6\u6210\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u5c06API\u8bbe\u8ba1\u539f\u5219\u8f6c\u5316\u4e3a\u53ef\u9a8c\u8bc1\u7ea6\u675f\u5e76\u5d4c\u5165\u5b9e\u7528\u5de5\u5177\u4e2d\uff0c\u63d0\u5347\u4e86API\u8bbe\u8ba1\u7684\u4e00\u81f4\u6027\u4e0e\u53ef\u590d\u7528\u6027\uff0c\u672a\u6765\u5c06\u63a2\u7d22IDE\u96c6\u6210\u3001\u89c4\u5219\u6269\u5c55\u53ca\u5728\u654f\u6377\u5f00\u53d1\u4e2d\u7684\u6301\u7eed\u5408\u89c4\u652f\u6301\u3002"}}
{"id": "2511.18234", "categories": ["cs.AR", "cs.DB"], "pdf": "https://arxiv.org/pdf/2511.18234", "abs": "https://arxiv.org/abs/2511.18234", "authors": ["Quanling Zhao", "Yanru Chen", "Runyang Tian", "Sumukh Pinge", "Weihong Xu", "Augusto Vega", "Steven Holmes", "Saransh Gupta", "Tajana Rosing"], "title": "HDDB: Efficient In-Storage SQL Database Search Using Hyperdimensional Computing on Ferroelectric NAND Flash", "comment": null, "summary": "Hyperdimensional Computing (HDC) encodes information and data into high-dimensional distributed vectors that can be manipulated using simple bitwise operations and similarity searches, offering parallelism, low-precision hardware friendliness, and strong robustness to noise. These properties are a natural fit for SQL database workloads dominated by predicate evaluation and scans, which demand low energy and low latency over large fact tables. Notably, HDC's noise-tolerance maps well onto emerging ferroelectric NAND (FeNAND) memories, which provide ultra-high density and in-storage compute capability but suffer from elevated raw bit-error rates. In this work, we propose HDDB, a hardware-software co-design that combines HDC with FeNAND multi-level cells (MLC) to perform in-storage SQL predicate evaluation and analytics with massive parallelism and minimal data movement. Particularly, we introduce novel HDC encoding techniques for standard SQL data tables and formulate predicate-based filtering and aggregation as highly efficient HDC operations that can happen in-storage. By exploiting the intrinsic redundancy of HDC, HDDB maintains correct predicate and decode outcomes under substantial device noise (up to 10% randomly corrupted TLC cells) without explicit error-correction overheads. Experiments on TPC-DS fact tables show that HDDB achieves up to 80.6x lower latency and 12,636x lower energy consumption compared to conventional CPU/GPU SQL database engines, suggesting that HDDB provides a practical substrate for noise-robust, memory-centric database processing.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faHDDB\uff0c\u4e00\u79cd\u7ed3\u5408\u8d85\u7ef4\u8ba1\u7b97\uff08HDC\uff09\u4e0e\u94c1\u7535NAND\uff08FeNAND\uff09\u5b58\u50a8\u5668\u7684\u8f6f\u786c\u4ef6\u534f\u540c\u8bbe\u8ba1\uff0c\u7528\u4e8e\u5728\u5b58\u50a8\u5185\u9ad8\u6548\u6267\u884cSQL\u8c13\u8bcd\u8bc4\u4f30\u4e0e\u5206\u6790\u4efb\u52a1\uff0c\u663e\u8457\u964d\u4f4e\u5ef6\u8fdf\u4e0e\u80fd\u8017\uff0c\u5e76\u5177\u5907\u5bf9\u9ad8\u566a\u58f0\u5b58\u50a8\u8bbe\u5907\u7684\u5f3a\u9c81\u68d2\u6027\u3002", "motivation": "\u4f20\u7edfSQL\u6570\u636e\u5e93\u5728\u5904\u7406\u5927\u89c4\u6a21\u4e8b\u5b9e\u8868\u65f6\u9762\u4e34\u9ad8\u80fd\u8017\u4e0e\u9ad8\u5ef6\u8fdf\u95ee\u9898\uff0c\u800c\u65b0\u5174\u7684\u9ad8\u5bc6\u5ea6FeNAND\u5b58\u50a8\u5668\u867d\u652f\u6301\u5b58\u5185\u8ba1\u7b97\u4f46\u5b58\u5728\u8f83\u9ad8\u539f\u59cb\u8bef\u7801\u7387\uff1b\u4f5c\u8005\u65e8\u5728\u5229\u7528HDC\u5bf9\u566a\u58f0\u7684\u5bb9\u5fcd\u80fd\u529b\uff0c\u6784\u5efa\u4e00\u4e2a\u4f4e\u529f\u8017\u3001\u4f4e\u5ef6\u8fdf\u4e14\u80fd\u9002\u5e94\u9ad8\u566a\u58f0\u5b58\u50a8\u73af\u5883\u7684\u6570\u636e\u5e93\u5904\u7406\u7cfb\u7edf\u3002", "method": "\u63d0\u51faHDDB\u67b6\u6784\uff0c\u5c06\u6807\u51c6SQL\u6570\u636e\u8868\u7f16\u7801\u4e3aHDC\u5411\u91cf\uff0c\u5e76\u5c06\u8c13\u8bcd\u8fc7\u6ee4\u4e0e\u805a\u5408\u64cd\u4f5c\u8f6c\u5316\u4e3a\u53ef\u5728FeNAND\u591a\u5c42\u5355\u5143\uff08MLC\uff09\u4e2d\u6267\u884c\u7684\u9ad8\u6548HDC\u8fd0\u7b97\uff0c\u5229\u7528HDC\u56fa\u6709\u7684\u5197\u4f59\u6027\u907f\u514d\u663e\u5f0f\u7ea0\u9519\u5f00\u9500\u3002", "result": "\u5728TPC-DS\u4e8b\u5b9e\u8868\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cHDDB\u76f8\u6bd4\u4f20\u7edfCPU/GPU\u6570\u636e\u5e93\u5f15\u64ce\u6700\u9ad8\u53ef\u5b9e\u73b080.6\u500d\u7684\u5ef6\u8fdf\u964d\u4f4e\u548c12,636\u500d\u7684\u80fd\u8017\u964d\u4f4e\uff0c\u5e76\u80fd\u5728\u9ad8\u8fbe10%\u968f\u673a\u635f\u574f\u7684TLC\u5355\u5143\u4e0b\u4fdd\u6301\u6b63\u786e\u7ed3\u679c\u3002", "conclusion": "HDDB\u4e3a\u9762\u5411\u9ad8\u566a\u58f0\u3001\u5185\u5b58\u4e3a\u4e2d\u5fc3\u7684\u6570\u636e\u5e93\u5904\u7406\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5b9e\u7528\u4e14\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5c55\u793a\u4e86HDC\u4e0e\u65b0\u578b\u975e\u6613\u5931\u5b58\u50a8\u6280\u672f\u7ed3\u5408\u7684\u5de8\u5927\u6f5c\u529b\u3002"}}
{"id": "2511.17621", "categories": ["cs.MA", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2511.17621", "abs": "https://arxiv.org/abs/2511.17621", "authors": ["Brendan Gho", "Suman Muppavarapu", "Afnan Shaik", "Tyson Tsay", "James Begin", "Kevin Zhu", "Archana Vaidheeswaran", "Vasu Sharma"], "title": "From Competition to Coordination: Market Making as a Scalable Framework for Safe and Aligned Multi-Agent LLM Systems", "comment": null, "summary": "As foundation models are increasingly deployed as interacting agents in multi-agent systems, their collective behavior raises new challenges for trustworthiness, transparency, and accountability. Traditional coordination mechanisms, such as centralized oversight or adversarial adjudication, struggle to scale and often obscure how decisions emerge. We introduce a market-making framework for multi-agent large language model (LLM) coordination that organizes agent interactions as structured economic exchanges. In this setup, each agent acts as a market participant, updating and trading probabilistic beliefs, to converge toward shared, truthful outcomes. By aligning local incentives with collective epistemic goals, the framework promotes self-organizing, verifiable reasoning without requiring external enforcement. Empirically, we evaluate this approach across factual reasoning, ethical judgment, and commonsense inference tasks. Market-based coordination yields accuracy gains of up to 10% over single-shot baselines while preserving interpretability and transparency of intermediate reasoning steps. Beyond these improvements, our findings demonstrate that economic coordination principles can operationalize accountability and robustness in multi-agent LLM systems, offering a scalable pathway toward self-correcting, socially responsible AI capable of maintaining trust and oversight in real world deployment scenarios.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u5e02\u573a\u673a\u5236\u7684\u591a\u667a\u80fd\u4f53\u5927\u8bed\u8a00\u6a21\u578b\u534f\u8c03\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u6784\u5316\u7ecf\u6d4e\u4ea4\u6362\u4fc3\u8fdb\u667a\u80fd\u4f53\u5728\u65e0\u9700\u5916\u90e8\u76d1\u7763\u7684\u60c5\u51b5\u4e0b\u5b9e\u73b0\u53ef\u4fe1\u3001\u900f\u660e\u4e14\u53ef\u9a8c\u8bc1\u7684\u96c6\u4f53\u63a8\u7406\uff0c\u5728\u591a\u9879\u4efb\u52a1\u4e2d\u51c6\u786e\u7387\u63d0\u5347\u6700\u9ad8\u8fbe10%\u3002", "motivation": "\u968f\u7740\u57fa\u7840\u6a21\u578b\u8d8a\u6765\u8d8a\u591a\u5730\u4f5c\u4e3a\u4ea4\u4e92\u5f0f\u667a\u80fd\u4f53\u90e8\u7f72\u4e8e\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u4e2d\uff0c\u5176\u96c6\u4f53\u884c\u4e3a\u5728\u53ef\u4fe1\u5ea6\u3001\u900f\u660e\u6027\u548c\u95ee\u8d23\u5236\u65b9\u9762\u5e26\u6765\u65b0\u6311\u6218\u3002\u4f20\u7edf\u534f\u8c03\u673a\u5236\uff08\u5982\u96c6\u4e2d\u76d1\u7ba1\u6216\u5bf9\u6297\u6027\u88c1\u51b3\uff09\u96be\u4ee5\u6269\u5c55\u4e14\u5e38\u63a9\u76d6\u51b3\u7b56\u8fc7\u7a0b\u3002", "method": "\u5f15\u5165\u4e00\u79cd\u5e02\u573a\u5236\u9020\u6846\u67b6\uff0c\u5c06\u667a\u80fd\u4f53\u4e92\u52a8\u7ec4\u7ec7\u4e3a\u7ed3\u6784\u5316\u7684\u7ecf\u6d4e\u4ea4\u6362\uff1a\u6bcf\u4e2a\u667a\u80fd\u4f53\u4f5c\u4e3a\u5e02\u573a\u53c2\u4e0e\u8005\uff0c\u66f4\u65b0\u5e76\u4ea4\u6613\u6982\u7387\u4fe1\u5ff5\uff0c\u4ee5\u6536\u655b\u81f3\u5171\u4eab\u4e14\u771f\u5b9e\u7684\u7ed3\u8bba\uff1b\u8be5\u65b9\u6cd5\u901a\u8fc7\u5c06\u5c40\u90e8\u6fc0\u52b1\u4e0e\u96c6\u4f53\u8ba4\u77e5\u76ee\u6807\u5bf9\u9f50\uff0c\u5b9e\u73b0\u81ea\u7ec4\u7ec7\u3001\u53ef\u9a8c\u8bc1\u7684\u63a8\u7406\u3002", "result": "\u5728\u4e8b\u5b9e\u63a8\u7406\u3001\u4f26\u7406\u5224\u65ad\u548c\u5e38\u8bc6\u63a8\u65ad\u4efb\u52a1\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u57fa\u4e8e\u5e02\u573a\u7684\u534f\u8c03\u65b9\u6cd5\u76f8\u6bd4\u5355\u6b21\u63a8\u7406\u57fa\u7ebf\u51c6\u786e\u7387\u6700\u9ad8\u63d0\u534710%\uff0c\u540c\u65f6\u4fdd\u7559\u4e2d\u95f4\u63a8\u7406\u6b65\u9aa4\u7684\u53ef\u89e3\u91ca\u6027\u4e0e\u900f\u660e\u6027\u3002", "conclusion": "\u7ecf\u6d4e\u534f\u8c03\u539f\u5219\u80fd\u591f\u6709\u6548\u5b9e\u73b0\u591a\u667a\u80fd\u4f53\u5927\u8bed\u8a00\u6a21\u578b\u7cfb\u7edf\u4e2d\u7684\u95ee\u8d23\u6027\u4e0e\u9c81\u68d2\u6027\uff0c\u4e3a\u6784\u5efa\u53ef\u81ea\u6211\u4fee\u6b63\u3001\u5177\u6709\u793e\u4f1a\u8d23\u4efb\u611f\u5e76\u5728\u771f\u5b9e\u573a\u666f\u4e2d\u7ef4\u6301\u4fe1\u4efb\u4e0e\u76d1\u7763\u80fd\u529b\u7684\u4eba\u5de5\u667a\u80fd\u7cfb\u7edf\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u8def\u5f84\u3002"}}
{"id": "2511.17853", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.17853", "abs": "https://arxiv.org/abs/2511.17853", "authors": ["SunMin Moon", "Jangwon Gim", "Chaerin Kim", "Yeeun Kim", "YoungJoo Kim", "Kang Choi"], "title": "A Low-Code Methodology for Developing AI Kiosks: a Case Study with the DIZEST Platform", "comment": "5 pages, 2 figures, conference, 2 tables", "summary": "This paper presents a comprehensive study on enhancing kiosk systems through a low-code architecture, with a focus on AI-based implementations. Modern kiosk systems are confronted with significant challenges, including a lack of integration, structural rigidity, performance bottlenecks, and the absence of collaborative frameworks. To overcome these limitations, we propose a DIZEST-based approach methodology, a specialized low-code platform that enables intuitive workflow design and seamless AI integration. Through a comparative analysis with existing platforms, including Jupyter Notebook, ComfyUI, and Orange3, we demonstrate that DIZEST delivers superior performance across key evaluation criteria. Our photo kiosk case study further validates the effectiveness of this approach in improving interoperability, enhancing user experience, and increasing deployment flexibility.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8eDIZEST\u4f4e\u4ee3\u7801\u5e73\u53f0\u7684AI\u96c6\u6210\u65b9\u6cd5\uff0c\u4ee5\u89e3\u51b3\u4f20\u7edf\u4fe1\u606f\u4ead\u7cfb\u7edf\u5728\u96c6\u6210\u6027\u3001\u7075\u6d3b\u6027\u548c\u534f\u4f5c\u6027\u65b9\u9762\u7684\u4e0d\u8db3\uff0c\u5e76\u901a\u8fc7\u6848\u4f8b\u7814\u7a76\u9a8c\u8bc1\u5176\u5728\u4e92\u64cd\u4f5c\u6027\u3001\u7528\u6237\u4f53\u9a8c\u548c\u90e8\u7f72\u7075\u6d3b\u6027\u65b9\u9762\u7684\u4f18\u52bf\u3002", "motivation": "\u73b0\u4ee3\u4fe1\u606f\u4ead\u7cfb\u7edf\u9762\u4e34\u7f3a\u4e4f\u96c6\u6210\u3001\u7ed3\u6784\u50f5\u5316\u3001\u6027\u80fd\u74f6\u9888\u53ca\u7f3a\u5c11\u534f\u4f5c\u6846\u67b6\u7b49\u6311\u6218\uff0c\u4e9f\u9700\u4e00\u79cd\u66f4\u7075\u6d3b\u9ad8\u6548\u7684\u67b6\u6784\u6765\u652f\u6301AI\u529f\u80fd\u7684\u5feb\u901f\u90e8\u7f72\u4e0e\u534f\u540c\u3002", "method": "\u91c7\u7528DIZEST\u4f4e\u4ee3\u7801\u5e73\u53f0\uff0c\u901a\u8fc7\u76f4\u89c2\u7684\u5de5\u4f5c\u6d41\u8bbe\u8ba1\u5b9e\u73b0AI\u6a21\u5757\u7684\u65e0\u7f1d\u96c6\u6210\uff0c\u5e76\u4e0eJupyter Notebook\u3001ComfyUI\u548cOrange3\u7b49\u73b0\u6709\u5e73\u53f0\u8fdb\u884c\u5bf9\u6bd4\u5206\u6790\u3002", "result": "DIZEST\u5728\u5173\u952e\u8bc4\u4f30\u6307\u6807\u4e0a\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u5e73\u53f0\uff1b\u7167\u7247\u4fe1\u606f\u4ead\u6848\u4f8b\u7814\u7a76\u8868\u660e\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u7cfb\u7edf\u7684\u4e92\u64cd\u4f5c\u6027\u3001\u7528\u6237\u4f53\u9a8c\u548c\u90e8\u7f72\u7075\u6d3b\u6027\u3002", "conclusion": "DIZEST\u4f4e\u4ee3\u7801\u67b6\u6784\u4e3a\u4fe1\u606f\u4ead\u7cfb\u7edf\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u3001\u7075\u6d3b\u4e14\u6613\u4e8e\u96c6\u6210AI\u80fd\u529b\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5177\u6709\u826f\u597d\u7684\u5e94\u7528\u524d\u666f\u548c\u63a8\u5e7f\u4ef7\u503c\u3002"}}
{"id": "2511.18906", "categories": ["cs.DC", "cs.NI"], "pdf": "https://arxiv.org/pdf/2511.18906", "abs": "https://arxiv.org/abs/2511.18906", "authors": ["Marco Zambianco", "Lorenzo Fasol", "Roberto Doriguzzi-Corin"], "title": "An Online Fragmentation-Aware GPU Scheduler for Multi-Tenant MIG-based Clouds", "comment": null, "summary": "The explosive growth of AI applications has created unprecedented demand for GPU resources. Cloud providers meet this demand through GPU-as-a-Service platforms that offer rentable GPU resources for running AI workloads. In this context, the sharing of GPU resources between different tenants is essential to maximize the number of scheduled workloads. Among the various GPU sharing technologies, NVIDIA's Multi-Instance GPU (MIG) stands out by partitioning GPUs at hardware level into isolated slices with dedicated compute and memory, ensuring strong tenant isolation, preventing resource contention, and enhancing security. Despite these advantages, MIG's fixed partitioning introduces scheduling rigidity, leading to severe GPU fragmentation in multi-tenant environments, where workloads are continuously deployed and terminated. Fragmentation leaves GPUs underutilized, limiting the number of workloads that can be accommodated. To overcome this challenge, we propose a novel scheduling framework for MIG-based clouds that maximizes workload acceptance while mitigating fragmentation in an online, workload-agnostic setting. We introduce a fragmentation metric to quantify resource inefficiency and guide allocation decisions. Building on this metric, our greedy scheduling algorithm selects GPUs and MIG slices that minimize fragmentation growth for each incoming workload. We evaluate our approach against multiple baseline strategies under diverse workload distributions. Results demonstrate that our method consistently achieves higher workload acceptance rates, leading to an average 10% increase in the number of scheduled workloads in heavy load conditions, while using approximately the same number of GPUs as the benchmark methods.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9762\u5411MIG\uff08\u591a\u5b9e\u4f8bGPU\uff09\u4e91\u73af\u5883\u7684\u65b0\u578b\u8c03\u5ea6\u6846\u67b6\uff0c\u901a\u8fc7\u5f15\u5165\u788e\u7247\u5ea6\u91cf\u6307\u6807\u548c\u8d2a\u5fc3\u7b97\u6cd5\uff0c\u5728\u7ebf\u4f18\u5316GPU\u8d44\u6e90\u5206\u914d\uff0c\u6709\u6548\u51cf\u5c11\u788e\u7247\u5e76\u63d0\u5347\u5de5\u4f5c\u8d1f\u8f7d\u63a5\u7eb3\u7387\u3002", "motivation": "MIG\u867d\u80fd\u63d0\u4f9b\u5f3a\u9694\u79bb\u6027\u548c\u5b89\u5168\u6027\uff0c\u4f46\u5176\u56fa\u5b9a\u5206\u533a\u65b9\u5f0f\u5728\u591a\u79df\u6237\u52a8\u6001\u8d1f\u8f7d\u573a\u666f\u4e0b\u6613\u5bfc\u81f4\u4e25\u91cd\u7684GPU\u8d44\u6e90\u788e\u7247\uff0c\u9020\u6210\u8d44\u6e90\u5229\u7528\u7387\u4f4e\u4e0b\uff0c\u9650\u5236\u53ef\u8c03\u5ea6\u5de5\u4f5c\u8d1f\u8f7d\u6570\u91cf\u3002", "method": "\u4f5c\u8005\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u788e\u7247\u5ea6\u91cf\u6307\u6807\uff0c\u5e76\u57fa\u4e8e\u8be5\u6307\u6807\u6784\u5efa\u4e86\u4e00\u4e2a\u8d2a\u5fc3\u8c03\u5ea6\u7b97\u6cd5\uff0c\u5728\u5de5\u4f5c\u8d1f\u8f7d\u5230\u8fbe\u65f6\u9009\u62e9\u4f7f\u788e\u7247\u589e\u957f\u6700\u5c0f\u7684GPU\u548cMIG\u5207\u7247\u8fdb\u884c\u5206\u914d\uff0c\u9002\u7528\u4e8e\u5728\u7ebf\u3001\u5de5\u4f5c\u8d1f\u8f7d\u4e0d\u53ef\u9884\u77e5\u7684\u573a\u666f\u3002", "result": "\u5728\u591a\u79cd\u8d1f\u8f7d\u5206\u5e03\u4e0b\uff0c\u6240\u63d0\u65b9\u6cd5\u76f8\u6bd4\u591a\u4e2a\u57fa\u7ebf\u7b56\u7565\uff0c\u5728\u91cd\u8d1f\u8f7d\u6761\u4ef6\u4e0b\u5e73\u5747\u591a\u8c03\u5ea610%\u7684\u5de5\u4f5c\u8d1f\u8f7d\uff0c\u4e14\u4f7f\u7528\u7684GPU\u6570\u91cf\u4e0e\u57fa\u51c6\u65b9\u6cd5\u76f8\u5f53\u3002", "conclusion": "\u8be5\u8c03\u5ea6\u6846\u67b6\u6709\u6548\u7f13\u89e3\u4e86MIG\u73af\u5883\u4e2d\u7684GPU\u788e\u7247\u95ee\u9898\uff0c\u5728\u4e0d\u589e\u52a0\u786c\u4ef6\u5f00\u9500\u7684\u524d\u63d0\u4e0b\u663e\u8457\u63d0\u5347\u4e86\u7cfb\u7edf\u541e\u5410\u91cf\u548c\u8d44\u6e90\u5229\u7528\u6548\u7387\u3002"}}
{"id": "2511.18687", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2511.18687", "abs": "https://arxiv.org/abs/2511.18687", "authors": ["Kasidis Arunruangsirilert", "Jiro Katto"], "title": "Evaluation of NVENC Split-Frame Encoding (SFE) for UHD Video Transcoding", "comment": "2025 Picture Coding Symposium (PCS 2025), 8-11 December 2025, Aachen, Germany", "summary": "NVIDIA Encoder (NVENC) features in modern NVIDIA GPUs, offer significant advantages over software encoders by providing comparable Rate-Distortion (RD) performance while consuming considerably less power. The increasing capability of consumer devices to capture footage in Ultra High-Definition (UHD) at 4K and 8K resolutions necessitates high-performance video transcoders for internet-based delivery. To address this demand, NVIDIA introduced Split-Frame Encoding (SFE), a technique that leverages multiple on-die NVENC chips available in high-end GPUs. SFE splits a single UHD frame for parallel encoding across these physical encoders and subsequently stitches the results, which significantly improves encoding throughput. However, this approach is known to incur an RD performance penalty. The widespread adoption of NVIDIA GPUs in data centers, driven by the rise of Generative AI, means NVENC is poised to play a critical role in transcoding UHD video. To better understand the performance-efficiency tradeoff of SFE, this paper evaluates SFE's impact on RD performance, encoding throughput, power consumption, and end-to-end latency using standardized test sequences. The results show that for real-time applications, SFE nearly doubles encoding throughput with a negligible RD performance penalty, which enables the use of higher-quality presets for 4K and makes real-time 8K encoding feasible, effectively offsetting the minor RD penalty. Moreover, SFE adds no latency at 4K and can reduce it at 8K, positioning it as a key enabler for high-throughput, real-time UHD transcoding.", "AI": {"tldr": "\u672c\u6587\u8bc4\u4f30\u4e86NVIDIA\u7684Split-Frame Encoding\uff08SFE\uff09\u6280\u672f\u5728\u8d85\u9ad8\u6e05\u89c6\u9891\u8f6c\u7801\u4e2d\u7684\u6027\u80fd\u8868\u73b0\uff0c\u53d1\u73b0\u5176\u5728\u51e0\u4e4e\u4e0d\u635f\u5931\u7387\u5931\u771f\uff08RD\uff09\u6027\u80fd\u7684\u524d\u63d0\u4e0b\uff0c\u5c06\u8fd1\u5b9e\u65f6\u7f16\u7801\u541e\u5410\u91cf\u63d0\u5347\u8fd1\u4e00\u500d\uff0c\u5e76\u652f\u63014K\u9ad8\u8d28\u91cf\u9884\u8bbe\u548c8K\u5b9e\u65f6\u7f16\u7801\uff0c\u540c\u65f6\u4e0d\u589e\u52a0\u751a\u81f3\u964d\u4f4e\u7aef\u5230\u7aef\u5ef6\u8fdf\u3002", "motivation": "\u968f\u7740\u6d88\u8d39\u8bbe\u5907\u80fd\u591f\u4ee54K/8K\u8d85\u9ad8\u6e05\u5206\u8fa8\u7387\u5f55\u5236\u89c6\u9891\uff0c\u5bf9\u9ad8\u6027\u80fd\u89c6\u9891\u8f6c\u7801\u5668\u7684\u9700\u6c42\u65e5\u76ca\u589e\u957f\u3002NVIDIA\u63a8\u51fa\u7684Split-Frame Encoding\uff08SFE\uff09\u6280\u672f\u5229\u7528\u9ad8\u7aefGPU\u4e2d\u7684\u591a\u4e2aNVENC\u7f16\u7801\u5355\u5143\u5e76\u884c\u5904\u7406\u5355\u5e27\u56fe\u50cf\uff0c\u4ee5\u63d0\u9ad8\u541e\u5410\u91cf\uff0c\u4f46\u53ef\u80fd\u5e26\u6765\u7387\u5931\u771f\u6027\u80fd\u635f\u5931\u3002\u56e0\u6b64\uff0c\u6709\u5fc5\u8981\u7cfb\u7edf\u8bc4\u4f30SFE\u5728\u541e\u5410\u91cf\u3001RD\u6027\u80fd\u3001\u529f\u8017\u4e0e\u5ef6\u8fdf\u7b49\u65b9\u9762\u7684\u6743\u8861\u3002", "method": "\u672c\u6587\u4f7f\u7528\u6807\u51c6\u6d4b\u8bd5\u5e8f\u5217\uff0c\u5728\u652f\u6301SFE\u7684NVIDIA GPU\u4e0a\u5bf9\u8d85\u9ad8\u6e05\u89c6\u9891\u8fdb\u884c\u5b9e\u9a8c\uff0c\u8bc4\u4f30SFE\u5bf9\u7387\u5931\u771f\u6027\u80fd\u3001\u7f16\u7801\u541e\u5410\u91cf\u3001\u529f\u8017\u548c\u7aef\u5230\u7aef\u5ef6\u8fdf\u7684\u5f71\u54cd\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u5728\u5b9e\u65f6\u5e94\u7528\u573a\u666f\u4e2d\uff0cSFE\u51e0\u4e4e\u5c06\u7f16\u7801\u541e\u5410\u91cf\u63d0\u5347\u4e00\u500d\uff0c\u800cRD\u6027\u80fd\u635f\u5931\u53ef\u5ffd\u7565\uff1b\u8fd9\u4f7f\u5f974K\u89c6\u9891\u53ef\u4f7f\u7528\u66f4\u9ad8\u8d28\u91cf\u7684\u7f16\u7801\u9884\u8bbe\uff0c\u5e76\u4f7f8K\u5b9e\u65f6\u7f16\u7801\u6210\u4e3a\u53ef\u80fd\u3002\u6b64\u5916\uff0cSFE\u57284K\u4e0b\u4e0d\u5f15\u5165\u989d\u5916\u5ef6\u8fdf\uff0c\u57288K\u4e0b\u751a\u81f3\u80fd\u964d\u4f4e\u5ef6\u8fdf\u3002", "conclusion": "SFE\u662f\u4e00\u79cd\u9ad8\u6548\u53ef\u884c\u7684\u6280\u672f\uff0c\u80fd\u591f\u5728\u4fdd\u6301\u826f\u597d\u89c6\u9891\u8d28\u91cf\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u8d85\u9ad8\u6e05\u89c6\u9891\u7684\u5b9e\u65f6\u7f16\u7801\u541e\u5410\u91cf\uff0c\u9002\u7528\u4e8e\u6570\u636e\u4e2d\u5fc3\u5927\u89c4\u6a21\u90e8\u7f72\uff0c\u662f\u5b9e\u73b0\u9ad8\u541e\u5410\u3001\u4f4e\u5ef6\u8fdfUHD\u89c6\u9891\u8f6c\u7801\u7684\u5173\u952e\u6280\u672f\u3002"}}
{"id": "2511.17517", "categories": ["cs.NI"], "pdf": "https://arxiv.org/pdf/2511.17517", "abs": "https://arxiv.org/abs/2511.17517", "authors": ["Marco Savarese", "Antonio De Blasi", "Carmine Zaccagnino", "Giacomo Salici", "Silvia Cascianelli", "Roberto Vezzani", "Carlo Augusto Grazia"], "title": "RI-PIENO -- Revised and Improved Petrol-Filling Itinerary Estimation aNd Optimization", "comment": "Accepted at IEEE Consumer Communications & Networking Conference 2026", "summary": "Efficient energy provisioning is a fundamental requirement for modern transportation systems, making refueling path optimization a critical challenge. Existing solutions often focus either on inter-vehicle communication or intra-vehicle monitoring, leveraging Intelligent Transportation Systems, Digital Twins, and Software-Defined Internet of Vehicles with Cloud/Fog/Edge infrastructures. However, integrated frameworks that adapt dynamically to driver mobility patterns are still underdeveloped. Building on our previous PIENO framework, we present RI-PIENO (Revised and Improved Petrol-filling Itinerary Estimation aNd Optimization), a system that combines intra-vehicle sensor data with external geospatial and fuel price information, processed via IoT-enabled Cloud/Fog services. RI-PIENO models refueling as a dynamic, time-evolving directed acyclic graph that reflects both habitual daily trips and real-time vehicular inputs, transforming the system from a static recommendation tool into a continuously adaptive decision engine. We validate RI-PIENO in a daily-commute use case through realistic multi-driver, multi-week simulations, showing that it achieves significant cost savings and more efficient routing compared to previous approaches. The framework is designed to leverage emerging roadside infrastructure and V2X communication, supporting scalable deployment within next-generation IoT and vehicular networking ecosystems.", "AI": {"tldr": "RI-PIENO \u662f\u4e00\u79cd\u878d\u5408\u8f66\u5185\u4f20\u611f\u5668\u6570\u636e\u4e0e\u5916\u90e8\u5730\u7406\u548c\u6cb9\u4ef7\u4fe1\u606f\u7684\u52a8\u6001\u52a0\u6cb9\u8def\u5f84\u4f18\u5316\u7cfb\u7edf\uff0c\u901a\u8fc7\u7269\u8054\u7f51\u4e91/\u96fe\u670d\u52a1\u6784\u5efa\u65f6\u53d8\u6709\u5411\u65e0\u73af\u56fe\uff0c\u5b9e\u73b0\u6bd4\u73b0\u6709\u65b9\u6cd5\u66f4\u4f18\u7684\u6210\u672c\u8282\u7701\u4e0e\u8def\u5f84\u6548\u7387\u3002", "motivation": "\u73b0\u6709\u52a0\u6cb9\u8def\u5f84\u4f18\u5316\u65b9\u6848\u591a\u5173\u6ce8\u8f66\u9645\u901a\u4fe1\u6216\u8f66\u5185\u76d1\u63a7\uff0c\u7f3a\u4e4f\u80fd\u52a8\u6001\u9002\u5e94\u9a7e\u9a76\u8005\u51fa\u884c\u6a21\u5f0f\u7684\u96c6\u6210\u6846\u67b6\u3002", "method": "\u57fa\u4e8e PIENO \u6846\u67b6\u63d0\u51fa RI-PIENO\uff0c\u7ed3\u5408\u8f66\u5185\u4f20\u611f\u6570\u636e\u4e0e\u5916\u90e8\u5730\u7406\u53ca\u6cb9\u4ef7\u4fe1\u606f\uff0c\u5229\u7528 IoT \u4e91/\u96fe\u670d\u52a1\u6784\u5efa\u52a8\u6001\u65f6\u53d8\u6709\u5411\u65e0\u73af\u56fe\uff0c\u5c06\u52a0\u6cb9\u51b3\u7b56\u5efa\u6a21\u4e3a\u6301\u7eed\u81ea\u9002\u5e94\u8fc7\u7a0b\u3002", "result": "\u5728\u591a\u9a7e\u9a76\u5458\u3001\u591a\u5468\u7684\u771f\u5b9e\u901a\u52e4\u6a21\u62df\u4e2d\uff0cRI-PIENO \u76f8\u8f83\u4ee5\u5f80\u65b9\u6cd5\u5b9e\u73b0\u4e86\u663e\u8457\u7684\u6210\u672c\u8282\u7ea6\u548c\u66f4\u9ad8\u7684\u8def\u5f84\u6548\u7387\u3002", "conclusion": "RI-PIENO \u80fd\u6709\u6548\u6574\u5408\u65b0\u5174\u8def\u4fa7\u57fa\u7840\u8bbe\u65bd\u4e0e V2X \u901a\u4fe1\uff0c\u9002\u7528\u4e8e\u4e0b\u4e00\u4ee3\u7269\u8054\u7f51\u4e0e\u8f66\u8054\u7f51\u751f\u6001\u7cfb\u7edf\u7684\u53ef\u6269\u5c55\u90e8\u7f72\u3002"}}
{"id": "2511.17625", "categories": ["cs.MA", "cs.GT"], "pdf": "https://arxiv.org/pdf/2511.17625", "abs": "https://arxiv.org/abs/2511.17625", "authors": ["Jaehan Im", "John-Paul Clarke", "Ufuk Topcu", "David Fridovich-Keil"], "title": "Iterative Negotiation and Oversight: A Case Study in Decentralized Air Traffic Management", "comment": null, "summary": "Achieving consensus among noncooperative agents remains challenging in decentralized multi-agent systems, where agents often have conflicting preferences. Existing coordination methods enable agents to reach consensus without a centralized coordinator, but do not provide formal guarantees on system-level objectives such as efficiency or fairness. To address this limitation, we propose an iterative negotiation and oversight framework that augments a decentralized negotiation mechanism with taxation-like oversight. The framework builds upon the trading auction for consensus, enabling noncooperative agents with conflicting preferences to negotiate through asset trading while preserving valuation privacy. We introduce an oversight mechanism, which implements a taxation-like intervention that guides decentralized negotiation toward system-efficient and equitable outcomes while also regulating how fast the framework converges. We establish theoretical guarantees of finite-time termination and derive bounds linking system efficiency and convergence rate to the level of central intervention. A case study based on the collaborative trajectory options program, a rerouting initiative in U.S. air traffic management, demonstrates that the framework can reliably achieve consensus among noncooperative airspace sector managers, and reveals how the level of intervention regulates the relationship between system efficiency and convergence speed. Taken together, the theoretical and experimental results indicate that the proposed framework provides a general mechanism for decentralized coordination in noncooperative multi-agent systems while safeguarding system-level objectives.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u7a0e\u6536\u5f0f\u76d1\u7763\u673a\u5236\u7684\u8fed\u4ee3\u534f\u5546\u6846\u67b6\uff0c\u7528\u4e8e\u5728\u65e0\u4e2d\u5fc3\u534f\u8c03\u8005\u7684\u975e\u5408\u4f5c\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u4e2d\u5b9e\u73b0\u517c\u987e\u7cfb\u7edf\u6548\u7387\u4e0e\u516c\u5e73\u6027\u7684\u5171\u8bc6\uff0c\u5e76\u5728\u6709\u9650\u65f6\u95f4\u5185\u6536\u655b\u3002", "motivation": "\u5728\u53bb\u4e2d\u5fc3\u5316\u7684\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u4e2d\uff0c\u975e\u5408\u4f5c\u667a\u80fd\u4f53\u56e0\u504f\u597d\u51b2\u7a81\u96be\u4ee5\u8fbe\u6210\u5171\u8bc6\uff0c\u73b0\u6709\u65b9\u6cd5\u7f3a\u4e4f\u5bf9\u7cfb\u7edf\u7ea7\u76ee\u6807\uff08\u5982\u6548\u7387\u548c\u516c\u5e73\uff09\u7684\u5f62\u5f0f\u5316\u4fdd\u8bc1\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u8fed\u4ee3\u534f\u5546\u4e0e\u76d1\u7763\u6846\u67b6\uff0c\u5728\u57fa\u4e8e\u8d44\u4ea7\u4ea4\u6613\u7684\u53bb\u4e2d\u5fc3\u5316\u534f\u5546\u673a\u5236\u57fa\u7840\u4e0a\u5f15\u5165\u7c7b\u7a0e\u6536\u7684\u76d1\u7763\u5e72\u9884\uff0c\u5f15\u5bfc\u534f\u5546\u7ed3\u679c\u8d8b\u5411\u9ad8\u6548\u4e0e\u516c\u5e73\uff0c\u5e76\u8c03\u63a7\u6536\u655b\u901f\u5ea6\u3002", "result": "\u7406\u8bba\u5206\u6790\u8bc1\u660e\u4e86\u8be5\u6846\u67b6\u53ef\u5728\u6709\u9650\u65f6\u95f4\u5185\u7ec8\u6b62\uff0c\u5e76\u63a8\u5bfc\u51fa\u7cfb\u7edf\u6548\u7387\u4e0e\u6536\u655b\u901f\u5ea6\u53d7\u4e2d\u592e\u5e72\u9884\u7a0b\u5ea6\u5f71\u54cd\u7684\u8fb9\u754c\uff1b\u5728\u7f8e\u56fd\u7a7a\u7ba1\u8def\u5f84\u91cd\u89c4\u5212\u6848\u4f8b\u4e2d\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "conclusion": "\u6240\u63d0\u6846\u67b6\u4e3a\u975e\u5408\u4f5c\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u63d0\u4f9b\u4e86\u4e00\u79cd\u901a\u7528\u7684\u53bb\u4e2d\u5fc3\u5316\u534f\u8c03\u673a\u5236\uff0c\u5728\u4fdd\u969c\u9690\u79c1\u7684\u540c\u65f6\u517c\u987e\u7cfb\u7edf\u7ea7\u76ee\u6807\u3002"}}
{"id": "2511.19192", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2511.19192", "abs": "https://arxiv.org/abs/2511.19192", "authors": ["Xinkui Zhao", "Qingyu Ma", "Yifan Zhang", "Hengxuan Lou", "Guanjie Cheng", "Shuiguang Deng", "Jianwei Yin"], "title": "AME: An Efficient Heterogeneous Agentic Memory Engine for Smartphones", "comment": null, "summary": "On-device agents on smartphones increasingly require continuously evolving memory to support personalized, context-aware, and long-term behaviors. To meet both privacy and responsiveness demands, user data is embedded as vectors and stored in a vector database for fast similarity search. However, most existing vector databases target server-class environments. When ported directly to smartphones, two gaps emerge: (G1) a mismatch between mobile SoC constraints and vector-database assumptions, including tight bandwidth budgets, limited on-chip memory, and stricter data type and layout constraints; and (G2) a workload mismatch, because on-device usage resembles a continuously learning memory, in which queries must coexist with frequent inserts, deletions, and ongoing index maintenance. To address these challenges, we propose AME, an on-device Agentic Memory Engine co-designed with modern smartphone SoCs. AME introduces two key techniques: (1) a hardware-aware, high-efficiency matrix pipeline that maximizes compute-unit utilization and exploits multi-level on-chip storage to sustain high throughput; and (2) a hardware- and workload-aware scheduling scheme that coordinates querying, insertion, and index rebuilding to minimize latency. We implement AME on Snapdragon 8-series SoCs and evaluate it on HotpotQA. In our experiments, AME improves query throughput by up to 1.4x at matched recall, achieves up to 7x faster index construction, and delivers up to 6x higher insertion throughput under concurrent query workloads.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86AME\uff08Agentic Memory Engine\uff09\uff0c\u4e00\u79cd\u4e13\u4e3a\u667a\u80fd\u624b\u673aSoC\u8bbe\u8ba1\u7684\u7aef\u4fa7\u5411\u91cf\u6570\u636e\u5e93\u5f15\u64ce\uff0c\u901a\u8fc7\u786c\u4ef6\u611f\u77e5\u7684\u9ad8\u6548\u77e9\u9635\u6d41\u6c34\u7ebf\u548c\u8f6f\u786c\u4ef6\u534f\u540c\u7684\u4efb\u52a1\u8c03\u5ea6\u673a\u5236\uff0c\u5728\u6ee1\u8db3\u9690\u79c1\u4e0e\u54cd\u5e94\u6027\u9700\u6c42\u7684\u540c\u65f6\uff0c\u663e\u8457\u63d0\u5347\u4e86\u67e5\u8be2\u541e\u5410\u91cf\u3001\u7d22\u5f15\u6784\u5efa\u901f\u5ea6\u548c\u63d2\u5165\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u5411\u91cf\u6570\u636e\u5e93\u4e3b\u8981\u9762\u5411\u670d\u52a1\u5668\u73af\u5883\uff0c\u76f4\u63a5\u79fb\u690d\u5230\u667a\u80fd\u624b\u673a\u4e0a\u65f6\u5b58\u5728\u4e24\u5927\u95ee\u9898\uff1a\u4e00\u662f\u4e0e\u79fb\u52a8\u7aefSoC\u8d44\u6e90\u9650\u5236\uff08\u5982\u5e26\u5bbd\u3001\u7247\u4e0a\u5185\u5b58\u3001\u6570\u636e\u7c7b\u578b\u7b49\uff09\u4e0d\u5339\u914d\uff1b\u4e8c\u662f\u65e0\u6cd5\u6709\u6548\u652f\u6301\u7aef\u4fa7\u667a\u80fd\u4f53\u6301\u7eed\u5b66\u4e60\u573a\u666f\u4e0b\u9891\u7e41\u7684\u63d2\u5165\u3001\u5220\u9664\u4e0e\u7d22\u5f15\u7ef4\u62a4\u7b49\u6df7\u5408\u8d1f\u8f7d\u3002", "method": "AME\u91c7\u7528\u4e24\u9879\u5173\u952e\u6280\u672f\uff1a(1) \u786c\u4ef6\u611f\u77e5\u7684\u9ad8\u6548\u77e9\u9635\u6d41\u6c34\u7ebf\uff0c\u5145\u5206\u5229\u7528\u8ba1\u7b97\u5355\u5143\u5e76\u5229\u7528\u591a\u7ea7\u7247\u4e0a\u5b58\u50a8\u4ee5\u7ef4\u6301\u9ad8\u541e\u5410\uff1b(2) \u9762\u5411\u786c\u4ef6\u4e0e\u5de5\u4f5c\u8d1f\u8f7d\u7684\u8c03\u5ea6\u7b56\u7565\uff0c\u534f\u8c03\u67e5\u8be2\u3001\u63d2\u5165\u548c\u7d22\u5f15\u91cd\u5efa\u4efb\u52a1\u4ee5\u964d\u4f4e\u5ef6\u8fdf\u3002", "result": "\u5728Snapdragon 8\u7cfb\u5217SoC\u4e0a\u57fa\u4e8eHotpotQA\u7684\u5b9e\u9a8c\u8868\u660e\uff0cAME\u5728\u76f8\u540c\u53ec\u56de\u7387\u4e0b\u67e5\u8be2\u541e\u5410\u63d0\u5347\u6700\u9ad81.4\u500d\uff0c\u7d22\u5f15\u6784\u5efa\u901f\u5ea6\u63d0\u5347\u6700\u9ad87\u500d\uff0c\u5e76\u53d1\u67e5\u8be2\u4e0b\u7684\u63d2\u5165\u541e\u5410\u63d0\u5347\u6700\u9ad86\u500d\u3002", "conclusion": "AME\u901a\u8fc7\u8f6f\u786c\u4ef6\u534f\u540c\u8bbe\u8ba1\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u73b0\u6709\u5411\u91cf\u6570\u636e\u5e93\u5728\u79fb\u52a8\u7aef\u90e8\u7f72\u65f6\u7684\u6027\u80fd\u74f6\u9888\uff0c\u4e3a\u7aef\u4fa7\u667a\u80fd\u4f53\u63d0\u4f9b\u4e86\u9ad8\u6548\u3001\u4f4e\u5ef6\u8fdf\u7684\u6301\u7eed\u8bb0\u5fc6\u80fd\u529b\u3002"}}
{"id": "2511.18688", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2511.18688", "abs": "https://arxiv.org/abs/2511.18688", "authors": ["Kasidis Arunruangsirilert", "Jiro Katto"], "title": "Evaluation of GPU Video Encoder for Low-Latency Real-Time 4K UHD Encoding", "comment": "2025 IEEE International Conference on Visual Communications and Image Processing (VCIP 2025), 1-4 December 2025, Klagenfurt, Austria", "summary": "The demand for high-quality, real-time video streaming has grown exponentially, with 4K Ultra High Definition (UHD) becoming the new standard for many applications such as live broadcasting, TV services, and interactive cloud gaming. This trend has driven the integration of dedicated hardware encoders into modern Graphics Processing Units (GPUs). Nowadays, these encoders support advanced codecs like HEVC and AV1 and feature specialized Low-Latency and Ultra Low-Latency tuning, targeting end-to-end latencies of < 2 seconds and < 500 ms, respectively. As the demand for such capabilities grows toward the 6G era, a clear understanding of their performance implications is essential. In this work, we evaluate the low-latency encoding modes on GPUs from NVIDIA, Intel, and AMD from both Rate-Distortion (RD) performance and latency perspectives. The results are then compared against both the normal-latency tuning of hardware encoders and leading software encoders. Results show hardware encoders achieve significantly lower E2E latency than software solutions with slightly better RD performance. While standard Low-Latency tuning yields a poor quality-latency trade-off, the Ultra Low-Latency mode reduces E2E latency to 83 ms (5 frames) without additional RD impact. Furthermore, hardware encoder latency is largely insensitive to quality presets, enabling high-quality, low-latency streams without compromise.", "AI": {"tldr": "\u672c\u6587\u8bc4\u4f30\u4e86NVIDIA\u3001Intel\u548cAMD GPU\u4e0a\u7684\u4f4e\u5ef6\u8fdf\u786c\u4ef6\u7f16\u7801\u6a21\u5f0f\uff0c\u5728\u7aef\u5230\u7aef\u5ef6\u8fdf\u548c\u7387\u5931\u771f\uff08RD\uff09\u6027\u80fd\u65b9\u9762\u8fdb\u884c\u4e86\u5206\u6790\uff0c\u53d1\u73b0\u8d85\u4f4e\u5ef6\u8fdf\u6a21\u5f0f\u53ef\u5728\u51e0\u4e4e\u4e0d\u5f71\u54cd\u753b\u8d28\u7684\u524d\u63d0\u4e0b\u5c06\u5ef6\u8fdf\u964d\u81f383\u6beb\u79d2\u3002", "motivation": "\u968f\u77404K UHD\u89c6\u9891\u6d41\u5728\u76f4\u64ad\u3001\u7535\u89c6\u670d\u52a1\u548c\u4e91\u6e38\u620f\u7b49\u573a\u666f\u4e2d\u7684\u666e\u53ca\uff0c\u5bf9\u4f4e\u5ef6\u8fdf\u9ad8\u8d28\u91cf\u89c6\u9891\u7f16\u7801\u7684\u9700\u6c42\u6fc0\u589e\u3002\u73b0\u4ee3GPU\u96c6\u6210\u4e86\u652f\u6301HEVC\u548cAV1\u7b49\u5148\u8fdb\u7f16\u89e3\u7801\u5668\u7684\u4e13\u7528\u786c\u4ef6\u7f16\u7801\u5668\uff0c\u5e76\u63d0\u4f9b\u4f4e\u5ef6\u8fdf\u548c\u8d85\u4f4e\u5ef6\u8fdf\u8c03\u4f18\u9009\u9879\u3002\u4e3a\u8fce\u63a56G\u65f6\u4ee3\uff0c\u6709\u5fc5\u8981\u6df1\u5165\u7406\u89e3\u8fd9\u4e9b\u7f16\u7801\u6a21\u5f0f\u7684\u6027\u80fd\u6743\u8861\u3002", "method": "\u4f5c\u8005\u4ece\u7387\u5931\u771f\u6027\u80fd\u548c\u7aef\u5230\u7aef\u5ef6\u8fdf\u4e24\u4e2a\u7ef4\u5ea6\uff0c\u7cfb\u7edf\u8bc4\u4f30\u4e86NVIDIA\u3001Intel\u548cAMD GPU\u4e0a\u7684\u4f4e\u5ef6\u8fdf\u4e0e\u8d85\u4f4e\u5ef6\u8fdf\u786c\u4ef6\u7f16\u7801\u6a21\u5f0f\uff0c\u5e76\u5c06\u5176\u4e0e\u5e38\u89c4\u5ef6\u8fdf\u786c\u4ef6\u7f16\u7801\u53ca\u4e3b\u6d41\u8f6f\u4ef6\u7f16\u7801\u5668\u8fdb\u884c\u5bf9\u6bd4\u3002", "result": "\u786c\u4ef6\u7f16\u7801\u5668\u76f8\u6bd4\u8f6f\u4ef6\u65b9\u6848\u663e\u8457\u964d\u4f4e\u7aef\u5230\u7aef\u5ef6\u8fdf\uff0c\u4e14RD\u6027\u80fd\u7565\u4f18\uff1b\u6807\u51c6\u4f4e\u5ef6\u8fdf\u6a21\u5f0f\u8d28\u91cf-\u5ef6\u8fdf\u6743\u8861\u8f83\u5dee\uff0c\u800c\u8d85\u4f4e\u5ef6\u8fdf\u6a21\u5f0f\u53ef\u5c06\u5ef6\u8fdf\u964d\u81f383ms\uff085\u5e27\uff09\u4e14\u65e0\u989d\u5916RD\u635f\u5931\uff1b\u6b64\u5916\uff0c\u786c\u4ef6\u7f16\u7801\u5668\u5ef6\u8fdf\u5bf9\u8d28\u91cf\u9884\u8bbe\u4e0d\u654f\u611f\uff0c\u53ef\u517c\u987e\u9ad8\u8d28\u91cf\u4e0e\u4f4e\u5ef6\u8fdf\u3002", "conclusion": "GPU\u786c\u4ef6\u7f16\u7801\u5668\uff0c\u5c24\u5176\u662f\u5176\u8d85\u4f4e\u5ef6\u8fdf\u6a21\u5f0f\uff0c\u5728\u4fdd\u6301\u9ad8\u753b\u8d28\u7684\u540c\u65f6\u5b9e\u73b0\u4e86\u6781\u4f4e\u5ef6\u8fdf\uff0c\u9002\u7528\u4e8e\u672a\u6765\u5bf9\u5b9e\u65f6\u6027\u8981\u6c42\u4e25\u82db\u7684\u5e94\u7528\u573a\u666f\uff0c\u59826G\u65f6\u4ee3\u7684\u4e92\u52a8\u5a92\u4f53\u670d\u52a1\u3002"}}
{"id": "2511.17518", "categories": ["cs.NI"], "pdf": "https://arxiv.org/pdf/2511.17518", "abs": "https://arxiv.org/abs/2511.17518", "authors": ["Siddharth Agarwal", "Maria A. Rodriguez", "Rajkumar Buyya"], "title": "Serv-Drishti: An Interactive Serverless Function Request Simulation Engine and Visualiser", "comment": "7 pages, 9 figures, conference - ITNAC 2025 (accepted)", "summary": "The rapid adoption of serverless computing necessitates a deeper understanding of its underlying operational mechanics, particularly concerning request routing, cold starts, function scaling, and resource management. This paper presents Serv-Drishti, an interactive, open-source simulation tool designed to demystify these complex behaviours. Serv-Drishti simulates and visualises the journey of a request through a representative serverless platform, from the API Gateway and intelligent Request Dispatcher to dynamic Function Instances on resource-constrained Compute Nodes. Unlike simple simulators, Serv-Drishti provides a robust framework for comparative analysis. It features configurable platform parameters, multiple request routing and function placement strategies, and a comprehensive failure simulation module. This allows users to not only observe but also rigorously analyse system responses under various loads and fault conditions. The tool generates real-time performance graphs and provides detailed data exports, establishing it as a valuable resource for research, education, and the design analysis of serverless architectures.", "AI": {"tldr": "Serv-Drishti \u662f\u4e00\u4e2a\u5f00\u6e90\u7684\u4ea4\u4e92\u5f0f\u6a21\u62df\u5de5\u5177\uff0c\u7528\u4e8e\u53ef\u89c6\u5316\u548c\u5206\u6790\u65e0\u670d\u52a1\u5668\u8ba1\u7b97\u5e73\u53f0\u4e2d\u8bf7\u6c42\u8def\u7531\u3001\u51b7\u542f\u52a8\u3001\u51fd\u6570\u6269\u5c55\u548c\u8d44\u6e90\u7ba1\u7406\u7b49\u5173\u952e\u884c\u4e3a\u3002", "motivation": "\u65e0\u670d\u52a1\u5668\u8ba1\u7b97\u7684\u5feb\u901f\u666e\u53ca\u8981\u6c42\u6df1\u5165\u7406\u89e3\u5176\u5e95\u5c42\u8fd0\u884c\u673a\u5236\uff0c\u5c24\u5176\u662f\u8bf7\u6c42\u8def\u7531\u3001\u51b7\u542f\u52a8\u3001\u51fd\u6570\u6269\u5c55\u548c\u8d44\u6e90\u7ba1\u7406\u7b49\u65b9\u9762\u7684\u590d\u6742\u6027\u3002", "method": "\u5f00\u53d1\u5e76\u5b9e\u73b0\u4e86\u4e00\u4e2a\u540d\u4e3a Serv-Drishti \u7684\u4ea4\u4e92\u5f0f\u6a21\u62df\u5668\uff0c\u652f\u6301\u53ef\u914d\u7f6e\u7684\u5e73\u53f0\u53c2\u6570\u3001\u591a\u79cd\u8bf7\u6c42\u8def\u7531\u4e0e\u51fd\u6570\u653e\u7f6e\u7b56\u7565\uff0c\u4ee5\u53ca\u6545\u969c\u6a21\u62df\u6a21\u5757\uff0c\u901a\u8fc7\u5b9e\u65f6\u56fe\u8868\u548c\u6570\u636e\u5bfc\u51fa\u8fdb\u884c\u7cfb\u7edf\u884c\u4e3a\u5206\u6790\u3002", "result": "Serv-Drishti \u80fd\u591f\u6709\u6548\u6a21\u62df\u8bf7\u6c42\u5728\u65e0\u670d\u52a1\u5668\u5e73\u53f0\u4e2d\u7684\u5b8c\u6574\u6d41\u7a0b\uff0c\u5e76\u652f\u6301\u5728\u4e0d\u540c\u8d1f\u8f7d\u548c\u6545\u969c\u6761\u4ef6\u4e0b\u5bf9\u7cfb\u7edf\u6027\u80fd\u8fdb\u884c\u6bd4\u8f83\u5206\u6790\u3002", "conclusion": "Serv-Drishti \u4e3a\u65e0\u670d\u52a1\u5668\u67b6\u6784\u7684\u7814\u7a76\u3001\u6559\u5b66\u548c\u8bbe\u8ba1\u5206\u6790\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5f3a\u5927\u4e14\u7075\u6d3b\u7684\u5de5\u5177\uff0c\u6709\u52a9\u4e8e\u6df1\u5165\u7406\u89e3\u5176\u5185\u90e8\u5de5\u4f5c\u673a\u5236\u3002"}}
{"id": "2511.18001", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2511.18001", "abs": "https://arxiv.org/abs/2511.18001", "authors": ["Jiaolong Kong", "Xiaofei Xie", "Yiheng Xiong", "Yuekun Wang", "Jian Wang"], "title": "Enhancing Automated Program Repair via Faulty Token Localization and Quality-Aware Patch Refinement", "comment": null, "summary": "Large language models (LLMs) have recently demonstrated strong potential for automated program repair (APR). However, existing LLM-based techniques primarily rely on coarse-grained external feedback (e.g.,test results) to guide iterative patch generation, while lacking fine-grained internal signals that reveal why a patch fails or which parts of the generated code are likely incorrect. This limitation often leads to inefficient refinement, error propagation, and suboptimal repair performance. In this work, we propose TokenRepair, a novel two-level refinement framework that enhances APR by integrating internal reflection for localizing potentially faulty tokens with external feedback for quality-aware patch refinement. Specifically, TokenRepair first performs internal reflection by analyzing context-aware token-level uncertainty fluctuations to identify suspicious or low-confidence tokens within a patch. It then applies Chain-of-Thought guided rewriting to refine only these localized tokens, enabling targeted and fine-grained correction. To further stabilize the iterative repair loop, TokenRepair incorporates a quality-aware external feedback mechanism that evaluates patch quality and filters out low-quality candidates before refinement. Experimental results show that TokenRepair achieves new state-of-the-art repair performance, correctly fixing 88 bugs on Defects4J 1.2 and 139 bugs on HumanEval-Java, demonstrating substantial improvements ranging from 8.2% to 34.9% across all models on Defects4J 1.2 and from 3.3% to 16.1% on HumanEval-Java.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86TokenRepair\uff0c\u4e00\u79cd\u7ed3\u5408\u5185\u90e8\u53cd\u601d\u4e0e\u5916\u90e8\u53cd\u9988\u7684\u4e24\u5c42\u7ec6\u5316\u6846\u67b6\uff0c\u901a\u8fc7\u5b9a\u4f4d\u53ef\u7591\u4ee3\u7801\u4ee4\u724c\u5e76\u8fdb\u884c\u9488\u5bf9\u6027\u4fee\u590d\uff0c\u663e\u8457\u63d0\u5347\u4e86\u81ea\u52a8\u7a0b\u5e8f\u4fee\u590d\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u81ea\u52a8\u7a0b\u5e8f\u4fee\u590d\u65b9\u6cd5\u4e3b\u8981\u4f9d\u8d56\u7c97\u7c92\u5ea6\u7684\u5916\u90e8\u53cd\u9988\uff08\u5982\u6d4b\u8bd5\u7ed3\u679c\uff09\uff0c\u7f3a\u4e4f\u7ec6\u7c92\u5ea6\u7684\u5185\u90e8\u4fe1\u53f7\u6765\u8bc6\u522b\u8865\u4e01\u5931\u8d25\u539f\u56e0\u6216\u9519\u8bef\u4ee3\u7801\u4f4d\u7f6e\uff0c\u5bfc\u81f4\u4fee\u590d\u6548\u7387\u4f4e\u3001\u9519\u8bef\u4f20\u64ad\u548c\u6027\u80fd\u4e0d\u4f73\u3002", "method": "TokenRepair\u9996\u5148\u901a\u8fc7\u5206\u6790\u4e0a\u4e0b\u6587\u611f\u77e5\u7684\u4ee4\u724c\u7ea7\u4e0d\u786e\u5b9a\u6027\u6ce2\u52a8\u8fdb\u884c\u5185\u90e8\u53cd\u601d\uff0c\u8bc6\u522b\u8865\u4e01\u4e2d\u53ef\u7591\u6216\u4f4e\u7f6e\u4fe1\u5ea6\u7684\u4ee4\u724c\uff1b\u7136\u540e\u91c7\u7528\u601d\u7ef4\u94fe\u5f15\u5bfc\u91cd\u5199\u7b56\u7565\u4ec5\u5bf9\u8fd9\u4e9b\u5c40\u90e8\u4ee4\u724c\u8fdb\u884c\u7cbe\u7ec6\u5316\u4fee\u6b63\uff1b\u540c\u65f6\u5f15\u5165\u8d28\u91cf\u611f\u77e5\u7684\u5916\u90e8\u53cd\u9988\u673a\u5236\uff0c\u5728\u8fed\u4ee3\u524d\u8fc7\u6ee4\u4f4e\u8d28\u91cf\u8865\u4e01\u3002", "result": "\u5728Defects4J 1.2\u4e0a\u6210\u529f\u4fee\u590d88\u4e2a\u7f3a\u9677\uff0c\u5728HumanEval-Java\u4e0a\u4fee\u590d139\u4e2a\uff0c\u76f8\u6bd4\u73b0\u6709\u65b9\u6cd5\u5728Defects4J 1.2\u4e0a\u63d0\u53478.2%\u81f334.9%\uff0c\u5728HumanEval-Java\u4e0a\u63d0\u53473.3%\u81f316.1%\u3002", "conclusion": "TokenRepair\u901a\u8fc7\u878d\u5408\u7ec6\u7c92\u5ea6\u5185\u90e8\u53cd\u601d\u4e0e\u8d28\u91cf\u611f\u77e5\u5916\u90e8\u53cd\u9988\uff0c\u5b9e\u73b0\u4e86\u66f4\u9ad8\u6548\u3001\u7cbe\u51c6\u7684\u81ea\u52a8\u7a0b\u5e8f\u4fee\u590d\uff0c\u8fbe\u5230\u4e86\u5f53\u524d\u6700\u4f18\u7684\u4fee\u590d\u6548\u679c\u3002"}}
{"id": "2511.19208", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2511.19208", "abs": "https://arxiv.org/abs/2511.19208", "authors": ["J\u00e9r\u00e9mie Chalopin", "Maria Kokkou"], "title": "Constant-Size Certificates for Leader Election in Chordal Graphs and Related Classes", "comment": "16 pages, 3 figures, under review", "summary": "In distributed computing a certification scheme consists of a set of states and conditions over those states that enable each node of a graph to efficiently verify the correctness of a solution to a given problem. This work focuses on two fundamental problems: leader election and spanning tree construction. For each problem, we present a constant-size (per edge), local certification scheme, where the conditions available to each node can only refer to the graph induced by its one-hop neighborhood. In particular, we provide certification schemes for leader election in chordal and $K_4$-free dismantlable graphs and for spanning tree construction in dismantlable graphs, assuming a root is given. For chordal graphs, our leader election certification scheme additionally ensures an acyclic orientation, a property that is not generally verifiable using constant-size certificates in arbitrary graphs. To the best of our knowledge, these are the first local certification results tailored to these graph classes, potentially highlighting structural properties useful for verifying additional problems. Finally, we propose an algorithm that automatically transforms any certification scheme into a silent self-stabilizing algorithm (i.e., an algorithm that automatically recovers from faults) by adding only one extra state to the set of states of the certification scheme, assuming a Gouda fair scheduler. This transformation may be of independent interest.", "AI": {"tldr": "\u672c\u6587\u9488\u5bf9\u5206\u5e03\u5f0f\u8ba1\u7b97\u4e2d\u7684\u9886\u5bfc\u8005\u9009\u4e3e\u548c\u751f\u6210\u6811\u6784\u9020\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u9002\u7528\u4e8e\u7279\u5b9a\u56fe\u7c7b\uff08\u5982\u5f26\u56fe\u3001\u65e0K4\u53ef\u62c6\u89e3\u56fe\uff09\u7684\u5e38\u6570\u5927\u5c0f\u5c40\u90e8\u8ba4\u8bc1\u65b9\u6848\uff0c\u5e76\u8fdb\u4e00\u6b65\u63d0\u51fa\u4e00\u79cd\u5c06\u4efb\u610f\u8ba4\u8bc1\u65b9\u6848\u81ea\u52a8\u8f6c\u5316\u4e3a\u9759\u9ed8\u81ea\u7a33\u5b9a\u7b97\u6cd5\u7684\u901a\u7528\u65b9\u6cd5\u3002", "motivation": "\u5728\u5206\u5e03\u5f0f\u7cfb\u7edf\u4e2d\uff0c\u5982\u4f55\u8ba9\u6bcf\u4e2a\u8282\u70b9\u4ec5\u901a\u8fc7\u5176\u5c40\u90e8\u4fe1\u606f\u9ad8\u6548\u9a8c\u8bc1\u5168\u5c40\u89e3\u7684\u6b63\u786e\u6027\u662f\u4e00\u4e2a\u6838\u5fc3\u6311\u6218\u3002\u73b0\u6709\u5de5\u4f5c\u7f3a\u4e4f\u9488\u5bf9\u7279\u5b9a\u56fe\u7ed3\u6784\uff08\u5982\u5f26\u56fe\u548c\u53ef\u62c6\u89e3\u56fe\uff09\u7684\u5c40\u90e8\u8ba4\u8bc1\u65b9\u6848\uff0c\u4e14\u7f3a\u4e4f\u5c06\u8ba4\u8bc1\u65b9\u6848\u81ea\u52a8\u8f6c\u5316\u4e3a\u5bb9\u9519\u7b97\u6cd5\u7684\u901a\u7528\u673a\u5236\u3002", "method": "\u4f5c\u8005\u4e3a\u9886\u5bfc\u8005\u9009\u4e3e\u548c\u751f\u6210\u6811\u6784\u9020\u8bbe\u8ba1\u4e86\u6bcf\u6761\u8fb9\u4ec5\u9700\u5e38\u6570\u5927\u5c0f\u8bc1\u4e66\u7684\u5c40\u90e8\u8ba4\u8bc1\u65b9\u6848\uff0c\u5176\u4e2d\u6bcf\u4e2a\u8282\u70b9\u53ea\u80fd\u8bbf\u95ee\u5176\u4e00\u8df3\u90bb\u57df\u5185\u7684\u4fe1\u606f\u3002\u6b64\u5916\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u7528\u8f6c\u6362\u7b97\u6cd5\uff0c\u5728Gouda\u516c\u5e73\u8c03\u5ea6\u5668\u5047\u8bbe\u4e0b\uff0c\u901a\u8fc7\u4ec5\u589e\u52a0\u4e00\u4e2a\u72b6\u6001\u5373\u53ef\u5c06\u4efb\u610f\u8ba4\u8bc1\u65b9\u6848\u8f6c\u5316\u4e3a\u9759\u9ed8\u81ea\u7a33\u5b9a\u7b97\u6cd5\u3002", "result": "\u6210\u529f\u4e3a\u5f26\u56fe\u548c\u65e0K4\u53ef\u62c6\u89e3\u56fe\u4e0a\u7684\u9886\u5bfc\u8005\u9009\u4e3e\u4ee5\u53ca\u7ed9\u5b9a\u6839\u7684\u53ef\u62c6\u89e3\u56fe\u4e0a\u7684\u751f\u6210\u6811\u6784\u9020\u63d0\u4f9b\u4e86\u9996\u4e2a\u5e38\u6570\u5927\u5c0f\u5c40\u90e8\u8ba4\u8bc1\u65b9\u6848\uff1b\u5176\u4e2d\u5f26\u56fe\u65b9\u6848\u8fd8\u80fd\u4fdd\u8bc1\u65e0\u73af\u5b9a\u5411\u3002\u540c\u65f6\uff0c\u63d0\u51fa\u7684\u8f6c\u6362\u7b97\u6cd5\u80fd\u6709\u6548\u5b9e\u73b0\u8ba4\u8bc1\u65b9\u6848\u5230\u81ea\u7a33\u5b9a\u7b97\u6cd5\u7684\u81ea\u52a8\u8f6c\u5316\u3002", "conclusion": "\u672c\u5de5\u4f5c\u4e0d\u4ec5\u9996\u6b21\u4e3a\u7279\u5b9a\u56fe\u7c7b\u63d0\u4f9b\u4e86\u5c40\u90e8\u8ba4\u8bc1\u65b9\u6848\uff0c\u63ed\u793a\u4e86\u5176\u7ed3\u6784\u7279\u6027\u5bf9\u9a8c\u8bc1\u95ee\u9898\u7684\u5e2e\u52a9\uff0c\u8fd8\u63d0\u51fa\u4e86\u4e00\u4e2a\u5177\u6709\u72ec\u7acb\u4ef7\u503c\u7684\u901a\u7528\u8f6c\u6362\u6280\u672f\uff0c\u8fde\u63a5\u4e86\u5c40\u90e8\u8ba4\u8bc1\u4e0e\u81ea\u7a33\u5b9a\u8ba1\u7b97\u4e24\u4e2a\u9886\u57df\u3002"}}
{"id": "2511.18755", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2511.18755", "abs": "https://arxiv.org/abs/2511.18755", "authors": ["Xiaotong Huang", "He Zhu", "Tianrui Ma", "Yuxiang Xiong", "Fangxin Liu", "Zhezhi He", "Yiming Gan", "Zihan Liu", "Jingwen Leng", "Yu Feng", "Minyi Guo"], "title": "Splatonic: Architecture Support for 3D Gaussian Splatting SLAM via Sparse Processing", "comment": null, "summary": "3D Gaussian splatting (3DGS) has emerged as a promising direction for SLAM due to its high-fidelity reconstruction and rapid convergence. However, 3DGS-SLAM algorithms remain impractical for mobile platforms due to their high computational cost, especially for their tracking process.\n  This work introduces Splatonic, a sparse and efficient real-time 3DGS-SLAM algorithm-hardware co-design for resource-constrained devices. Inspired by classical SLAMs, we propose an adaptive sparse pixel sampling algorithm that reduces the number of rendered pixels by up to 256$\\times$ while retaining accuracy. To unlock this performance potential on mobile GPUs, we design a novel pixel-based rendering pipeline that improves hardware utilization via Gaussian-parallel rendering and preemptive $\u03b1$-checking. Together, these optimizations yield up to 121.7$\\times$ speedup on the bottleneck stages and 14.6$\\times$ end-to-end speedup on off-the-shelf GPUs. To further address new bottlenecks introduced by our rendering pipeline, we propose a pipelined architecture that simplifies the overall design while addressing newly emerged bottlenecks in projection and aggregation. Evaluated across four 3DGS-SLAM algorithms, Splatonic achieves up to 274.9$\\times$ speedup and 4738.5$\\times$ energy savings over mobile GPUs and up to 25.2$\\times$ speedup and 241.1$\\times$ energy savings over state-of-the-art accelerators, all with comparable accuracy.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86Splatonic\uff0c\u4e00\u79cd\u9762\u5411\u8d44\u6e90\u53d7\u9650\u8bbe\u5907\u7684\u7a00\u758f\u9ad8\u6548\u5b9e\u65f63D\u9ad8\u65af\u6cfc\u6e85SLAM\uff083DGS-SLAM\uff09\u7b97\u6cd5\u4e0e\u786c\u4ef6\u534f\u540c\u8bbe\u8ba1\u65b9\u6848\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u7a00\u758f\u50cf\u7d20\u91c7\u6837\u548c\u65b0\u578b\u50cf\u7d20\u7ea7\u6e32\u67d3\u7ba1\u7ebf\uff0c\u5728\u4fdd\u6301\u7cbe\u5ea6\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u901f\u5ea6\u5e76\u964d\u4f4e\u80fd\u8017\u3002", "motivation": "\u73b0\u67093DGS-SLAM\u7b97\u6cd5\u56e0\u8ba1\u7b97\u5f00\u9500\u5927\uff08\u5c24\u5176\u5728\u8ddf\u8e2a\u9636\u6bb5\uff09\uff0c\u96be\u4ee5\u90e8\u7f72\u4e8e\u79fb\u52a8\u5e73\u53f0\uff1b\u4e9f\u9700\u517c\u987e\u6548\u7387\u3001\u7cbe\u5ea6\u4e0e\u80fd\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u63d0\u51fa\u81ea\u9002\u5e94\u7a00\u758f\u50cf\u7d20\u91c7\u6837\u7b97\u6cd5\u4ee5\u51cf\u5c11\u6e32\u67d3\u50cf\u7d20\u6570\u91cf\uff0c\u5e76\u8bbe\u8ba1\u57fa\u4e8e\u50cf\u7d20\u7684\u6e32\u67d3\u7ba1\u7ebf\uff08\u5305\u62ec\u9ad8\u65af\u5e76\u884c\u6e32\u67d3\u548c\u9884\u5224\u03b1\u68c0\u6d4b\uff09\u4ee5\u63d0\u5347\u79fb\u52a8GPU\u5229\u7528\u7387\uff1b\u8fdb\u4e00\u6b65\u6784\u5efa\u6d41\u6c34\u7ebf\u67b6\u6784\u4f18\u5316\u6295\u5f71\u4e0e\u805a\u5408\u9636\u6bb5\u7684\u65b0\u74f6\u9888\u3002", "result": "\u5728\u56db\u79cd3DGS-SLAM\u7b97\u6cd5\u4e0a\u8bc4\u4f30\u8868\u660e\uff0cSplatonic\u76f8\u6bd4\u5546\u7528\u79fb\u52a8GPU\u6700\u9ad8\u5b9e\u73b0274.9\u500d\u52a0\u901f\u548c4738.5\u500d\u80fd\u6548\u63d0\u5347\uff0c\u76f8\u6bd4\u5f53\u524d\u6700\u5148\u8fdb\u7684\u52a0\u901f\u5668\u4e5f\u670925.2\u500d\u52a0\u901f\u548c241.1\u500d\u80fd\u6548\u63d0\u5347\uff0c\u4e14\u7cbe\u5ea6\u76f8\u5f53\u3002", "conclusion": "Splatonic\u901a\u8fc7\u7b97\u6cd5-\u786c\u4ef6\u534f\u540c\u8bbe\u8ba1\u6709\u6548\u89e3\u51b3\u4e863DGS-SLAM\u5728\u79fb\u52a8\u7aef\u90e8\u7f72\u7684\u6027\u80fd\u4e0e\u80fd\u6548\u74f6\u9888\uff0c\u4e3a\u9ad8\u4fdd\u771f\u5b9e\u65f6SLAM\u5728\u8d44\u6e90\u53d7\u9650\u8bbe\u5907\u4e0a\u7684\u5e94\u7528\u63d0\u4f9b\u4e86\u53ef\u884c\u8def\u5f84\u3002"}}
{"id": "2511.19366", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2511.19366", "abs": "https://arxiv.org/abs/2511.19366", "authors": ["Alan Jia Bao Du", "Tarek S. Abdelrahman"], "title": "HeLEx: A Heterogeneous Layout Explorer for Spatial Elastic Coarse-Grained Reconfigurable Arrays", "comment": null, "summary": "We present HeLEx, a framework for determining the functional layout of heterogeneous spatially-configured elastic Coarse-Grained Reconfigurable Arrays (CGRAs). Given a collection of input data flow graphs (DFGs) and a target CGRA, the framework starts with a full layout in which every processing element (PE) supports every operation in the DFGs. It then employs a branch-and-bound (BB) search to eliminate operations out of PEs, ensuring that the input DFGs successfully map onto the resulting CGRAs, eventually returning an optimized heterogeneous CGRA. Experimental evaluation with 12 DFGs and 9 target CGRA sizes reveals that the framework reduces the number of operations by 68.7% on average, resulting in a reduction of CGRA area by almost 70% and of power by over 51%, all compared to the initial full layout. HeLEx generates CGRAs that are on average only within 6.2% of theoretically minimum CGRAs that support exactly the number of operations needed by the input DFGs. A comparison with functional layouts produced by two state-of-the-art frameworks indicates that HeLEx achieves better reduction in the number of operations, by up to 2.6X.", "AI": {"tldr": "HeLEx \u662f\u4e00\u4e2a\u7528\u4e8e\u4f18\u5316\u5f02\u6784\u5f39\u6027\u7c97\u7c92\u5ea6\u53ef\u91cd\u6784\u9635\u5217\uff08CGRAs\uff09\u529f\u80fd\u5e03\u5c40\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u5206\u652f\u9650\u754c\u641c\u7d22\u663e\u8457\u51cf\u5c11\u5904\u7406\u5355\u5143\u4e2d\u7684\u64cd\u4f5c\u6570\u91cf\uff0c\u5728\u4fdd\u6301\u6620\u5c04\u6210\u529f\u7684\u540c\u65f6\u5927\u5e45\u964d\u4f4e\u9762\u79ef\u548c\u529f\u8017\u3002", "motivation": "\u73b0\u6709 CGRA \u8bbe\u8ba1\u901a\u5e38\u91c7\u7528\u5168\u529f\u80fd\u5e03\u5c40\uff0c\u5bfc\u81f4\u8d44\u6e90\u5197\u4f59\u3001\u9762\u79ef\u548c\u529f\u8017\u8fc7\u9ad8\uff1b\u4e9f\u9700\u4e00\u79cd\u80fd\u6839\u636e\u5177\u4f53\u6570\u636e\u6d41\u56fe\u81ea\u52a8\u4f18\u5316\u529f\u80fd\u5e03\u5c40\u7684\u65b9\u6cd5\u3002", "method": "HeLEx \u6846\u67b6\u4ece\u5168\u529f\u80fd\u5e03\u5c40\u51fa\u53d1\uff0c\u5229\u7528\u5206\u652f\u9650\u754c\uff08branch-and-bound\uff09\u641c\u7d22\u7b56\u7565\u9010\u6b65\u79fb\u9664\u5904\u7406\u5355\u5143\uff08PE\uff09\u4e2d\u4e0d\u5fc5\u8981\u7684\u64cd\u4f5c\uff0c\u540c\u65f6\u786e\u4fdd\u8f93\u5165\u7684\u6570\u636e\u6d41\u56fe\uff08DFGs\uff09\u4ecd\u80fd\u6210\u529f\u6620\u5c04\u5230\u4f18\u5316\u540e\u7684 CGRA \u4e0a\u3002", "result": "\u5728 12 \u4e2a DFG \u548c 9 \u79cd CGRA \u5c3a\u5bf8\u7684\u5b9e\u9a8c\u4e2d\uff0cHeLEx \u5e73\u5747\u51cf\u5c11 68.7% \u7684\u64cd\u4f5c\u6570\uff0c\u9762\u79ef\u51cf\u5c11\u8fd1 70%\uff0c\u529f\u8017\u964d\u4f4e\u8d85 51%\uff1b\u5176\u7ed3\u679c\u5e73\u5747\u4ec5\u6bd4\u7406\u8bba\u6700\u5c0f CGRA \u591a\u51fa 6.2% \u7684\u64cd\u4f5c\uff0c\u5e76\u4f18\u4e8e\u73b0\u6709\u4e24\u79cd\u5148\u8fdb\u6846\u67b6\u6700\u591a\u8fbe 2.6 \u500d\u3002", "conclusion": "HeLEx \u80fd\u9ad8\u6548\u751f\u6210\u63a5\u8fd1\u7406\u8bba\u6700\u4f18\u7684\u5f02\u6784 CGRA \u529f\u80fd\u5e03\u5c40\uff0c\u5728\u663e\u8457\u8282\u7701\u786c\u4ef6\u8d44\u6e90\u7684\u540c\u65f6\u4fdd\u6301\u826f\u597d\u7684\u6620\u5c04\u80fd\u529b\uff0c\u4f18\u4e8e\u5f53\u524d\u4e3b\u6d41\u65b9\u6cd5\u3002"}}
{"id": "2511.17775", "categories": ["cs.MA", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.17775", "abs": "https://arxiv.org/abs/2511.17775", "authors": ["Sandro Rama Fiorini", "Leonardo G. Azevedo", "Raphael M. Thiago", "Valesca M. de Sousa", "Anton B. Labate", "Viviane Torres da Silva"], "title": "Episodic Memory in Agentic Frameworks: Suggesting Next Tasks", "comment": null, "summary": "Agentic frameworks powered by Large Language Models (LLMs) can be useful tools in scientific workflows by enabling human-AI co-creation. A key challenge is recommending the next steps during workflow creation without relying solely on LLMs, which risk hallucination and require fine-tuning with scarce proprietary data. We propose an episodic memory architecture that stores and retrieves past workflows to guide agents in suggesting plausible next tasks. By matching current workflows with historical sequences, agents can recommend steps based on prior patterns.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u60c5\u666f\u8bb0\u5fc6\u67b6\u6784\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u5b58\u50a8\u548c\u68c0\u7d22\u5386\u53f2\u5de5\u4f5c\u6d41\u6765\u8f85\u52a9\u5927\u8bed\u8a00\u6a21\u578b\u9a71\u52a8\u7684\u667a\u80fd\u4f53\u63a8\u8350\u79d1\u5b66\u5de5\u4f5c\u6d41\u4e2d\u7684\u4e0b\u4e00\u6b65\u64cd\u4f5c\uff0c\u4ee5\u51cf\u5c11\u6a21\u578b\u5e7b\u89c9\u5e76\u907f\u514d\u4f9d\u8d56\u7a00\u7f3a\u7684\u4e13\u6709\u6570\u636e\u8fdb\u884c\u5fae\u8c03\u3002", "motivation": "\u5728\u79d1\u5b66\u5de5\u4f5c\u6d41\u4e2d\uff0c\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u9a71\u52a8\u7684\u667a\u80fd\u4f53\u867d\u80fd\u4fc3\u8fdb\u4eba\u673a\u534f\u540c\u521b\u4f5c\uff0c\u4f46\u5176\u63a8\u8350\u4e0b\u4e00\u6b65\u64cd\u4f5c\u65f6\u6613\u4ea7\u751f\u5e7b\u89c9\uff0c\u4e14\u4f9d\u8d56\u7a00\u7f3a\u7684\u4e13\u6709\u6570\u636e\u8fdb\u884c\u5fae\u8c03\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u4e0d\u5b8c\u5168\u4f9d\u8d56LLM\u7684\u53ef\u9760\u63a8\u8350\u673a\u5236\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u60c5\u666f\u8bb0\u5fc6\u67b6\u6784\uff0c\u7528\u4e8e\u5b58\u50a8\u548c\u68c0\u7d22\u8fc7\u5f80\u7684\u5de5\u4f5c\u6d41\uff1b\u901a\u8fc7\u5c06\u5f53\u524d\u5de5\u4f5c\u6d41\u4e0e\u5386\u53f2\u5e8f\u5217\u5339\u914d\uff0c\u4f7f\u667a\u80fd\u4f53\u80fd\u591f\u57fa\u4e8e\u4ee5\u5f80\u6a21\u5f0f\u63a8\u8350\u5408\u7406\u7684\u540e\u7eed\u4efb\u52a1\u3002", "result": "\u8be5\u65b9\u6cd5\u4f7f\u667a\u80fd\u4f53\u80fd\u591f\u4f9d\u636e\u5386\u53f2\u5de5\u4f5c\u6d41\u4e2d\u7684\u5b9e\u9645\u6a21\u5f0f\u63a8\u8350\u4e0b\u4e00\u6b65\u4efb\u52a1\uff0c\u4ece\u800c\u63d0\u5347\u63a8\u8350\u7684\u5408\u7406\u6027\u548c\u53ef\u9760\u6027\u3002", "conclusion": "\u5229\u7528\u60c5\u666f\u8bb0\u5fc6\u67b6\u6784\u53ef\u6709\u6548\u8f85\u52a9LLM\u667a\u80fd\u4f53\u5728\u79d1\u5b66\u5de5\u4f5c\u6d41\u4e2d\u8fdb\u884c\u4e0b\u4e00\u6b65\u63a8\u8350\uff0c\u964d\u4f4e\u5e7b\u89c9\u98ce\u9669\u5e76\u51cf\u5c11\u5bf9\u4e13\u6709\u8bad\u7ec3\u6570\u636e\u7684\u4f9d\u8d56\u3002"}}
{"id": "2511.18092", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2511.18092", "abs": "https://arxiv.org/abs/2511.18092", "authors": ["Sebastian Dingler", "Philip Rehkop", "Florian Mayer", "Ralf Muenzenberger"], "title": "Event-Chain Analysis for Automated Driving and ADAS Systems: Ensuring Safety and Meeting Regulatory Timing Requirements", "comment": null, "summary": "Automated Driving Systems (ADS), including Advanced Driver Assistance Systems (ADAS), must fulfill not only high functional expectations but also stringent timing constraints mandated by international regulations and standards. Regulatory frameworks such as UN regulations, NCAP standards, ISO norms, and NHTSA guidelines impose strict bounds on system reaction times to ensure safe vehicle operation. This paper presents a structured, White-Box methodology based on Event-Chain Modeling to address these timing challenges. Unlike Black-Box approaches, Event-Chain Analysis offers transparent insights into the timing behavior of each functional component - from perception and planning to actuation and human interaction. This perspective is also aligned with multiple regulations, which require that homologation dossiers provide evidence that the chosen system architecture is suitable to ensure compliance with the specified requirements. Our methodology enables the derivation, modeling, and validation of end-to-end timing constraints at the architectural level and facilitates early verification through simulation. Through a detailed case study, we demonstrate how this Event-Chain-centric approach enhances regulatory compliance, optimizes system design, and supports model-based safety analysis techniques, with results showing early identification of compliance issues, systematic parameter optimization, and quantitative evidence generation through probabilistic analysis.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u4e8b\u4ef6\u94fe\u5efa\u6a21\u7684\u767d\u76d2\u65b9\u6cd5\uff0c\u7528\u4e8e\u5728\u67b6\u6784\u5c42\u9762\u63a8\u5bfc\u3001\u5efa\u6a21\u548c\u9a8c\u8bc1\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u7684\u7aef\u5230\u7aef\u65f6\u5e8f\u7ea6\u675f\uff0c\u4ee5\u6ee1\u8db3\u56fd\u9645\u6cd5\u89c4\u5bf9\u7cfb\u7edf\u54cd\u5e94\u65f6\u95f4\u7684\u4e25\u683c\u8981\u6c42\u3002", "motivation": "\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\uff08ADS\uff09\u548c\u9ad8\u7ea7\u9a7e\u9a76\u8f85\u52a9\u7cfb\u7edf\uff08ADAS\uff09\u9700\u540c\u65f6\u6ee1\u8db3\u9ad8\u529f\u80fd\u6027\u4e0e\u4e25\u683c\u7684\u65f6\u5e8f\u7ea6\u675f\uff0c\u800c\u73b0\u6709\u9ed1\u76d2\u65b9\u6cd5\u96be\u4ee5\u63d0\u4f9b\u8db3\u591f\u7684\u900f\u660e\u5ea6\u6765\u8bc1\u660e\u7cfb\u7edf\u67b6\u6784\u7b26\u5408\u56fd\u9645\u6cd5\u89c4\uff08\u5982UN\u3001NCAP\u3001ISO\u3001NHTSA\u7b49\uff09\u5bf9\u53cd\u5e94\u65f6\u95f4\u7684\u8981\u6c42\u3002", "method": "\u91c7\u7528\u57fa\u4e8e\u4e8b\u4ef6\u94fe\u5efa\u6a21\uff08Event-Chain Modeling\uff09\u7684\u767d\u76d2\u65b9\u6cd5\uff0c\u5bf9\u4ece\u611f\u77e5\u3001\u89c4\u5212\u5230\u6267\u884c\u548c\u4eba\u673a\u4ea4\u4e92\u7684\u6bcf\u4e2a\u529f\u80fd\u7ec4\u4ef6\u8fdb\u884c\u65f6\u5e8f\u884c\u4e3a\u5206\u6790\uff0c\u5e76\u5728\u67b6\u6784\u5c42\u9762\u8fdb\u884c\u7aef\u5230\u7aef\u65f6\u5e8f\u7ea6\u675f\u7684\u5efa\u6a21\u4e0e\u4eff\u771f\u9a8c\u8bc1\u3002", "result": "\u901a\u8fc7\u6848\u4f8b\u7814\u7a76\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\u80fd\u65e9\u671f\u8bc6\u522b\u5408\u89c4\u95ee\u9898\u3001\u7cfb\u7edf\u5316\u4f18\u5316\u53c2\u6570\uff0c\u5e76\u901a\u8fc7\u6982\u7387\u5206\u6790\u751f\u6210\u5b9a\u91cf\u8bc1\u636e\uff0c\u4ece\u800c\u63d0\u5347\u6cd5\u89c4\u5408\u89c4\u6027\u3001\u4f18\u5316\u7cfb\u7edf\u8bbe\u8ba1\u5e76\u652f\u6301\u57fa\u4e8e\u6a21\u578b\u7684\u5b89\u5168\u5206\u6790\u3002", "conclusion": "\u4e8b\u4ef6\u94fe\u4e2d\u5fc3\u7684\u65b9\u6cd5\u4e3a\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u63d0\u4f9b\u4e86\u4e00\u79cd\u900f\u660e\u3001\u53ef\u9a8c\u8bc1\u4e14\u7b26\u5408\u6cd5\u89c4\u8981\u6c42\u7684\u65f6\u5e8f\u5206\u6790\u6846\u67b6\uff0c\u6709\u52a9\u4e8e\u5728\u5f00\u53d1\u65e9\u671f\u786e\u4fdd\u7cfb\u7edf\u6ee1\u8db3\u5b89\u5168\u4e0e\u5408\u89c4\u6027\u6807\u51c6\u3002"}}
{"id": "2511.17915", "categories": ["cs.MA"], "pdf": "https://arxiv.org/pdf/2511.17915", "abs": "https://arxiv.org/abs/2511.17915", "authors": ["Yao Liu", "Sampad Mohanty", "Elizabeth Ondula", "Bhaskar Krishnamachari"], "title": "DISPATCH -- Decentralized Informed Spatial Planning and Assignment of Tasks for Cooperative Heterogeneous Agents", "comment": null, "summary": "Spatial task allocation in systems such as multi-robot delivery or ride-sharing requires balancing efficiency with fair service across tasks. Greedy assignment policies that match each agent to its highest-preference or lowest-cost task can maximize efficiency but often create inequities: some tasks receive disproportionately favorable service (e.g., shorter delays or better matches), while others face long waits or poor allocations.\n  We study fairness in heterogeneous multi-agent systems where tasks vary in preference alignment and urgency. Most existing approaches either assume centralized coordination or largely ignore fairness under partial observability. Distinct from this prior work, we establish a connection between the Eisenberg-Gale (EG) equilibrium convex program and decentralized, partially observable multi-agent learning. Building on this connection, we develop two equilibrium-informed algorithms that integrate fairness and efficiency: (i) a multi-agent reinforcement learning (MARL) framework, EG-MARL, whose training is guided by centralized fair assignment algorithms (EG and a preference-aware Hungarian method); and (ii) a stochastic online optimization mechanism that performs guided exploration and subset-based fair assignment as tasks are discovered.\n  We evaluate our frameworks across a range of team sizes and assignment formulations against centralized EG, Hungarian, and Min-Max Distance baselines. Both algorithms preserve the fairness-efficiency balance of the Eisenberg-Gale equilibrium under partial observability. EG-MARL achieves near-centralized coordination and reduced travel distances, while the stochastic online mechanism enables real-time allocation with competitive fairness. Together, these results demonstrate that spatially aware EG formulations can effectively guide decentralized coordination in agents with heterogeneous capabilities.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u5728\u90e8\u5206\u53ef\u89c2\u6d4b\u3001\u5f02\u6784\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u4e2d\u7684\u7a7a\u95f4\u4efb\u52a1\u5206\u914d\u95ee\u9898\uff0c\u63d0\u51fa\u4e24\u79cd\u57fa\u4e8eEisenberg-Gale\uff08EG\uff09\u5747\u8861\u7684\u65b0\u7b97\u6cd5\u2014\u2014EG-MARL\u548c\u4e00\u79cd\u968f\u673a\u5728\u7ebf\u4f18\u5316\u673a\u5236\uff0c\u5728\u4fdd\u8bc1\u6548\u7387\u7684\u540c\u65f6\u5b9e\u73b0\u516c\u5e73\u5206\u914d\u3002", "motivation": "\u73b0\u6709\u8d2a\u5a6a\u5206\u914d\u7b56\u7565\u867d\u9ad8\u6548\u4f46\u5bfc\u81f4\u4efb\u52a1\u95f4\u670d\u52a1\u4e0d\u516c\u5e73\uff1b\u800c\u591a\u6570\u516c\u5e73\u5206\u914d\u65b9\u6cd5\u4f9d\u8d56\u96c6\u4e2d\u5f0f\u534f\u8c03\u6216\u5ffd\u7565\u90e8\u5206\u53ef\u89c2\u6d4b\u4e0b\u7684\u516c\u5e73\u6027\u3002\u56e0\u6b64\uff0c\u9700\u5728\u53bb\u4e2d\u5fc3\u5316\u3001\u90e8\u5206\u53ef\u89c2\u6d4b\u573a\u666f\u4e0b\u517c\u987e\u6548\u7387\u4e0e\u516c\u5e73\u3002", "method": "\u5efa\u7acbEG\u5747\u8861\u51f8\u89c4\u5212\u4e0e\u53bb\u4e2d\u5fc3\u5316\u591a\u667a\u80fd\u4f53\u5b66\u4e60\u4e4b\u95f4\u7684\u8054\u7cfb\uff0c\u63d0\u51fa\u4e24\u79cd\u7b97\u6cd5\uff1a(i) \u57fa\u4e8e\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u7684EG-MARL\uff0c\u5229\u7528\u96c6\u4e2d\u5f0f\u516c\u5e73\u5206\u914d\u7b97\u6cd5\uff08EG\u548c\u504f\u597d\u611f\u77e5\u5308\u7259\u5229\u6cd5\uff09\u6307\u5bfc\u8bad\u7ec3\uff1b(ii) \u4e00\u79cd\u968f\u673a\u5728\u7ebf\u4f18\u5316\u673a\u5236\uff0c\u5728\u4efb\u52a1\u9010\u6b65\u51fa\u73b0\u65f6\u8fdb\u884c\u5f15\u5bfc\u63a2\u7d22\u548c\u5b50\u96c6\u516c\u5e73\u5206\u914d\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u4e24\u79cd\u7b97\u6cd5\u5728\u4e0d\u540c\u56e2\u961f\u89c4\u6a21\u548c\u4efb\u52a1\u8bbe\u5b9a\u4e0b\u5747\u80fd\u7ef4\u6301EG\u5747\u8861\u6240\u5b9a\u4e49\u7684\u516c\u5e73-\u6548\u7387\u5e73\u8861\u3002EG-MARL\u63a5\u8fd1\u96c6\u4e2d\u5f0f\u534f\u8c03\u6548\u679c\u5e76\u51cf\u5c11\u884c\u9a76\u8ddd\u79bb\uff0c\u968f\u673a\u5728\u7ebf\u673a\u5236\u5219\u5b9e\u73b0\u5b9e\u65f6\u5206\u914d\u4e14\u4fdd\u6301\u826f\u597d\u516c\u5e73\u6027\u3002", "conclusion": "\u7a7a\u95f4\u611f\u77e5\u7684EG\u516c\u5f0f\u53ef\u6709\u6548\u6307\u5bfc\u5177\u6709\u5f02\u6784\u80fd\u529b\u7684\u667a\u80fd\u4f53\u5728\u53bb\u4e2d\u5fc3\u5316\u73af\u5883\u4e0b\u7684\u534f\u8c03\uff0c\u5b9e\u73b0\u517c\u987e\u516c\u5e73\u4e0e\u6548\u7387\u7684\u4efb\u52a1\u5206\u914d\u3002"}}
{"id": "2511.18165", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.18165", "abs": "https://arxiv.org/abs/2511.18165", "authors": ["Israel Puerta-Merino", "Carlos N\u00fa\u00f1ez-Molina", "Pablo Mesejo", "Juan Fern\u00e1ndez-Olivares"], "title": "Towards a General Framework for HTN Modeling with LLMs", "comment": "10 pages, 5 figures, to be published in the Workshop on Planning in the Era of LLMs ( LM4Plan - https://llmforplanning.github.io ) and the Workshop on Hierarchical Planning ( HPlan - https://icaps25.icaps-conference.org/program/workshops/hplan/ ), both in the International Conference on Automated Planning and Scheduling (ICAPS) 2025", "summary": "The use of Large Language Models (LLMs) for generating Automated Planning (AP) models has been widely explored; however, their application to Hierarchical Planning (HP) is still far from reaching the level of sophistication observed in non-hierarchical architectures. In this work, we try to address this gap. We present two main contributions. First, we propose L2HP, an extension of L2P (a library to LLM-driven PDDL models generation) that support HP model generation and follows a design philosophy of generality and extensibility. Second, we apply our framework to perform experiments where we compare the modeling capabilities of LLMs for AP and HP. On the PlanBench dataset, results show that parsing success is limited but comparable in both settings (around 36\\%), while syntactic validity is substantially lower in the hierarchical case (1\\% vs. 20\\% of instances). These findings underscore the unique challenges HP presents for LLMs, highlighting the need for further research to improve the quality of generated HP models.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86L2HP\uff0c\u4e00\u4e2a\u652f\u6301\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u751f\u6210\u5206\u5c42\u89c4\u5212\uff08HP\uff09\u6a21\u578b\u7684\u6846\u67b6\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u53d1\u73b0LLM\u5728\u751f\u6210HP\u6a21\u578b\u65f6\u9762\u4e34\u6bd4\u975e\u5206\u5c42\u81ea\u52a8\u89c4\u5212\uff08AP\uff09\u66f4\u5927\u7684\u6311\u6218\uff0c\u5c24\u5176\u662f\u5728\u8bed\u6cd5\u6709\u6548\u6027\u65b9\u9762\u3002", "motivation": "\u5f53\u524d\u5927\u8bed\u8a00\u6a21\u578b\u5728\u81ea\u52a8\u751f\u6210\u5206\u5c42\u89c4\u5212\uff08HP\uff09\u6a21\u578b\u65b9\u9762\u7684\u80fd\u529b\u8fdc\u4e0d\u5982\u5728\u975e\u5206\u5c42\u81ea\u52a8\u89c4\u5212\uff08AP\uff09\u4e2d\u6210\u719f\uff0c\u5b58\u5728\u660e\u663e\u7684\u7814\u7a76\u7a7a\u767d\u3002", "method": "\u4f5c\u8005\u6269\u5c55\u4e86\u73b0\u6709\u7684L2P\u5e93\uff0c\u63d0\u51faL2HP\u6846\u67b6\uff0c\u4f7f\u5176\u652f\u6301HP\u6a21\u578b\u751f\u6210\uff0c\u5e76\u5728PlanBench\u6570\u636e\u96c6\u4e0a\u5bf9LLM\u5728AP\u4e0eHP\u5efa\u6a21\u80fd\u529b\u65b9\u9762\u8fdb\u884c\u4e86\u5bf9\u6bd4\u5b9e\u9a8c\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u5728PlanBench\u6570\u636e\u96c6\u4e2d\uff0cLLM\u751f\u6210\u7684HP\u548cAP\u6a21\u578b\u7684\u89e3\u6790\u6210\u529f\u7387\u76f8\u8fd1\uff08\u7ea636%\uff09\uff0c\u4f46HP\u6a21\u578b\u7684\u8bed\u6cd5\u6709\u6548\u6027\u663e\u8457\u66f4\u4f4e\uff081% vs. 20%\uff09\u3002", "conclusion": "\u5206\u5c42\u89c4\u5212\u5bf9\u5927\u8bed\u8a00\u6a21\u578b\u63d0\u51fa\u4e86\u72ec\u7279\u6311\u6218\uff0c\u9700\u8fdb\u4e00\u6b65\u7814\u7a76\u4ee5\u63d0\u5347\u5176\u751f\u6210HP\u6a21\u578b\u7684\u8d28\u91cf\u3002"}}
{"id": "2511.17524", "categories": ["cs.NI"], "pdf": "https://arxiv.org/pdf/2511.17524", "abs": "https://arxiv.org/abs/2511.17524", "authors": ["Huaizhe Liu", "Jiaqi Wu", "Zhizongkai Wang", "Bin Cao", "Lin Gao"], "title": "Joint Edge Server Deployment and Computation Offloading: A Multi-Timescale Stochastic Programming Framework", "comment": "This is the online report for the paper to be published in IEEE TMC", "summary": "Mobile Edge Computing (MEC) is a promising approach for enhancing the quality-of-service (QoS) of AI-enabled applications in the B5G/6G era, by bringing computation capability closer to end-users at the network edge. In this work, we investigate the joint optimization of edge server (ES) deployment, service placement, and computation task offloading under the stochastic information scenario. Traditional approaches often treat these decisions as equal, disregarding the differences in information realization. However, in practice, the ES deployment decision must be made in advance and remain unchanged, prior to the complete realization of information, whereas the decisions regarding service placement and computation task offloading can be made and adjusted in real-time after information is fully realized. To address such temporal coupling between decisions and information realization, we introduce the stochastic programming (SP) framework, which involves a strategic-layer for deciding ES deployment based on (incomplete) stochastic information and a tactical-layer for deciding service placement and task offloading based on complete information realization. The problem is challenging due to the different timescales of two layers' decisions. To overcome this challenge, we propose a multi-timescale SP framework, which includes a large timescale (called period) for strategic-layer decision-making and a small timescale (called slot) for tactical-layer decision making. Moreover, we design a Lyapunov-based algorithm to solve the tactical-layer problem at each time slot, and a Markov approximation algorithm to solve the strategic-layer problem in every time period.", "AI": {"tldr": "\u672c\u6587\u9488\u5bf9\u79fb\u52a8\u8fb9\u7f18\u8ba1\u7b97\u4e2d\u8fb9\u7f18\u670d\u52a1\u5668\u90e8\u7f72\u3001\u670d\u52a1\u653e\u7f6e\u4e0e\u4efb\u52a1\u5378\u8f7d\u7684\u8054\u5408\u4f18\u5316\u95ee\u9898\uff0c\u63d0\u51fa\u4e00\u79cd\u591a\u65f6\u95f4\u5c3a\u5ea6\u968f\u673a\u89c4\u5212\u6846\u67b6\uff0c\u533a\u5206\u957f\u671f\u6218\u7565\u51b3\u7b56\u4e0e\u77ed\u671f\u6218\u672f\u51b3\u7b56\uff0c\u5e76\u8bbe\u8ba1\u76f8\u5e94\u7b97\u6cd5\u6c42\u89e3\u3002", "motivation": "\u4f20\u7edf\u65b9\u6cd5\u5c06\u8fb9\u7f18\u670d\u52a1\u5668\u90e8\u7f72\u3001\u670d\u52a1\u653e\u7f6e\u548c\u4efb\u52a1\u5378\u8f7d\u89c6\u4e3a\u540c\u7b49\u51b3\u7b56\uff0c\u5ffd\u7565\u4e86\u5b83\u4eec\u5728\u4fe1\u606f\u5b9e\u73b0\u65f6\u95f4\u4e0a\u7684\u5dee\u5f02\uff1a\u8fb9\u7f18\u670d\u52a1\u5668\u90e8\u7f72\u9700\u5728\u4fe1\u606f\u5b8c\u5168\u83b7\u77e5\u524d\u505a\u51fa\u4e14\u4e0d\u53ef\u9891\u7e41\u53d8\u66f4\uff0c\u800c\u670d\u52a1\u653e\u7f6e\u4e0e\u4efb\u52a1\u5378\u8f7d\u53ef\u5728\u4fe1\u606f\u5b8c\u5168\u83b7\u77e5\u540e\u5b9e\u65f6\u8c03\u6574\u3002\u8fd9\u79cd\u65f6\u95f4\u8026\u5408\u6027\u672a\u88ab\u73b0\u6709\u7814\u7a76\u5145\u5206\u8003\u8651\u3002", "method": "\u4f5c\u8005\u5f15\u5165\u968f\u673a\u89c4\u5212\uff08SP\uff09\u6846\u67b6\uff0c\u5206\u4e3a\u6218\u7565\u5c42\uff08\u57fa\u4e8e\u4e0d\u5b8c\u6574\u968f\u673a\u4fe1\u606f\u51b3\u5b9a\u8fb9\u7f18\u670d\u52a1\u5668\u90e8\u7f72\uff09\u548c\u6218\u672f\u5c42\uff08\u57fa\u4e8e\u5b8c\u6574\u4fe1\u606f\u51b3\u5b9a\u670d\u52a1\u653e\u7f6e\u4e0e\u4efb\u52a1\u5378\u8f7d\uff09\u3002\u4e3a\u5904\u7406\u4e24\u5c42\u51b3\u7b56\u7684\u65f6\u95f4\u5c3a\u5ea6\u5dee\u5f02\uff0c\u6784\u5efa\u591a\u65f6\u95f4\u5c3a\u5ea6SP\u6846\u67b6\uff1a\u5927\u65f6\u95f4\u5c3a\u5ea6\uff08\u5468\u671f\uff09\u7528\u4e8e\u6218\u7565\u5c42\uff0c\u5c0f\u65f6\u95f4\u5c3a\u5ea6\uff08\u65f6\u9699\uff09\u7528\u4e8e\u6218\u672f\u5c42\uff0c\u5e76\u5206\u522b\u91c7\u7528\u9a6c\u5c14\u53ef\u592b\u8fd1\u4f3c\u7b97\u6cd5\u548c\u57fa\u4e8eLyapunov\u7684\u7b97\u6cd5\u8fdb\u884c\u6c42\u89e3\u3002", "result": "\u8be5\u65b9\u6cd5\u6709\u6548\u5e94\u5bf9\u4e86\u8fb9\u7f18\u8ba1\u7b97\u4e2d\u4e0d\u540c\u51b3\u7b56\u5c42\u7ea7\u95f4\u7684\u65f6\u95f4\u8026\u5408\u4e0e\u4fe1\u606f\u4e0d\u786e\u5b9a\u6027\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u5bf9\u8fb9\u7f18\u670d\u52a1\u5668\u90e8\u7f72\u3001\u670d\u52a1\u653e\u7f6e\u4e0e\u4efb\u52a1\u5378\u8f7d\u7684\u8054\u5408\u4f18\u5316\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u591a\u65f6\u95f4\u5c3a\u5ea6\u968f\u673a\u89c4\u5212\u6846\u67b6\u80fd\u66f4\u771f\u5b9e\u5730\u5efa\u6a21\u5b9e\u9645\u90e8\u7f72\u573a\u666f\u4e2d\u7684\u4fe1\u606f\u52a8\u6001\u6027\u4e0e\u51b3\u7b56\u7ea6\u675f\uff0c\u4e3aB5G/6G\u65f6\u4ee3AI\u9a71\u52a8\u5e94\u7528\u7684QoS\u4fdd\u969c\u63d0\u4f9b\u4e86\u53ef\u884c\u7684\u4f18\u5316\u65b9\u6848\u3002"}}
{"id": "2511.18187", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2511.18187", "abs": "https://arxiv.org/abs/2511.18187", "authors": ["Sristy Sumana Nath", "Banani Roy", "Munima Jahan"], "title": "Establishing Traceability Links between Release Notes & Software Artifacts: Practitioners' Perspectives", "comment": null, "summary": "Maintaining traceability links between software release notes and corresponding development artifacts, e.g., pull requests (PRs), commits, and issues, is essential for managing technical debt and ensuring maintainability. However, in open-source environments where contributors work remotely and asynchronously, establishing and maintaining these links is often error-prone, time-consuming, and frequently overlooked. Our empirical study of GitHub repositories revealed that 47% of release artifacts lacked traceability links, and 12% contained broken links. To address this gap, we first analyzed release notes to identify their What, Why, and How information and assessed how these align with PRs, commits, and issues. We curated a benchmark dataset consisting of 3,500 filtered and validated traceability link instances. Then, we implemented LLM-based approaches to automatically establish traceability links of three pairs between release note contents & PRs, release note contents & PRs and release note contents & issues. By combining the time proximity feature, the LLM-based approach, e.g., Gemini 1.5 Pro, achieved a high Precision@1 value of 0.73 for PR traceability recovery. To evaluate the usability and adoption potential of this approach, we conducted an online survey involving 33 open-source practitioners. 16% of respondents rated as very important, and 68% as somewhat important for traceability maintenance.", "AI": {"tldr": "\u672c\u6587\u9488\u5bf9\u5f00\u6e90\u9879\u76ee\u4e2d\u53d1\u5e03\u8bf4\u660e\u4e0e\u5f00\u53d1\u5de5\u4ef6\uff08\u5982PR\u3001\u63d0\u4ea4\u3001\u95ee\u9898\uff09\u4e4b\u95f4\u8ffd\u6eaf\u94fe\u63a5\u7f3a\u5931\u6216\u635f\u574f\u7684\u95ee\u9898\uff0c\u63d0\u51fa\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u65b9\u6cd5\u81ea\u52a8\u5efa\u7acb\u8ffd\u6eaf\u94fe\u63a5\uff0c\u5e76\u6784\u5efa\u4e86\u5305\u542b3500\u4e2a\u9a8c\u8bc1\u5b9e\u4f8b\u7684\u57fa\u51c6\u6570\u636e\u96c6\u3002\u7ed3\u5408\u65f6\u95f4\u90bb\u8fd1\u7279\u5f81\uff0cGemini 1.5 Pro\u5728PR\u8ffd\u6eaf\u4efb\u52a1\u4e2d\u8fbe\u52300.73\u7684Precision@1\u3002\u7528\u6237\u8c03\u67e5\u663e\u793a\uff0c84%\u7684\u5f00\u6e90\u5f00\u53d1\u8005\u8ba4\u4e3a\u7ef4\u62a4\u8ffd\u6eaf\u6027\u201c\u6709\u4e9b\u91cd\u8981\u201d\u6216\u201c\u975e\u5e38\u91cd\u8981\u201d\u3002", "motivation": "\u5728\u5f00\u6e90\u73af\u5883\u4e2d\uff0c\u7531\u4e8e\u8d21\u732e\u8005\u8fdc\u7a0b\u5f02\u6b65\u534f\u4f5c\uff0c\u53d1\u5e03\u8bf4\u660e\u4e0e\u5f00\u53d1\u5de5\u4ef6\u4e4b\u95f4\u7684\u8ffd\u6eaf\u94fe\u63a5\u5e38\u5e38\u7f3a\u5931\u6216\u51fa\u9519\uff0c\u5f71\u54cd\u6280\u672f\u503a\u52a1\u7ba1\u7406\u548c\u8f6f\u4ef6\u53ef\u7ef4\u62a4\u6027\u3002\u5b9e\u8bc1\u7814\u7a76\u53d1\u73b047%\u7684\u53d1\u5e03\u5de5\u4ef6\u7f3a\u5c11\u8ffd\u6eaf\u94fe\u63a5\uff0c12%\u5b58\u5728\u65ad\u94fe\uff0c\u4e9f\u9700\u81ea\u52a8\u5316\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u9996\u5148\u5206\u6790\u53d1\u5e03\u8bf4\u660e\u4e2d\u7684What\u3001Why\u3001How\u4fe1\u606f\uff0c\u5e76\u4e0ePR\u3001\u63d0\u4ea4\u548c\u95ee\u9898\u5bf9\u9f50\uff1b\u7136\u540e\u6784\u5efa\u5305\u542b3500\u4e2a\u7ecf\u8fc7\u7b5b\u9009\u548c\u9a8c\u8bc1\u7684\u8ffd\u6eaf\u94fe\u63a5\u5b9e\u4f8b\u7684\u57fa\u51c6\u6570\u636e\u96c6\uff1b\u63a5\u7740\u91c7\u7528\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\uff08\u5982Gemini 1.5 Pro\uff09\u7684\u65b9\u6cd5\uff0c\u7ed3\u5408\u65f6\u95f4\u90bb\u8fd1\u7279\u5f81\uff0c\u81ea\u52a8\u5efa\u7acb\u53d1\u5e03\u8bf4\u660e\u4e0ePR\u3001\u63d0\u4ea4\u3001\u95ee\u9898\u4e4b\u95f4\u7684\u4e09\u7c7b\u8ffd\u6eaf\u94fe\u63a5\u3002", "result": "\u5728PR\u8ffd\u6eaf\u4efb\u52a1\u4e2d\uff0c\u7ed3\u5408\u65f6\u95f4\u90bb\u8fd1\u7279\u5f81\u7684LLM\u65b9\u6cd5\u5b9e\u73b0\u4e860.73\u7684Precision@1\uff1b\u5bf933\u540d\u5f00\u6e90\u4ece\u4e1a\u8005\u7684\u5728\u7ebf\u8c03\u67e5\u663e\u793a\uff0c16%\u8ba4\u4e3a\u8ffd\u6eaf\u6027\u7ef4\u62a4\u201c\u975e\u5e38\u91cd\u8981\u201d\uff0c68%\u8ba4\u4e3a\u201c\u6709\u4e9b\u91cd\u8981\u201d\u3002", "conclusion": "\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u65b9\u6cd5\u80fd\u6709\u6548\u6062\u590d\u53d1\u5e03\u8bf4\u660e\u4e0e\u5f00\u53d1\u5de5\u4ef6\u95f4\u7684\u8ffd\u6eaf\u94fe\u63a5\uff0c\u5177\u6709\u8f83\u9ad8\u7684\u51c6\u786e\u7387\u548c\u826f\u597d\u7684\u5b9e\u9645\u5e94\u7528\u6f5c\u529b\uff0c\u6709\u52a9\u4e8e\u63d0\u5347\u5f00\u6e90\u9879\u76ee\u7684\u53ef\u7ef4\u62a4\u6027\u548c\u6280\u672f\u503a\u52a1\u7ba1\u7406\u80fd\u529b\u3002"}}
{"id": "2511.18249", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2511.18249", "abs": "https://arxiv.org/abs/2511.18249", "authors": ["Mostafijur Rahman Akhond", "Gias Uddin"], "title": "LLM Assisted Coding with Metamorphic Specification Mutation Agent", "comment": null, "summary": "Metamorphic Relations (MRs) serve as a foundational mechanism for generating semantically equivalent mutations. Software engineering has advanced significantly in recent years with the advent of Large Language Models (LLMs). However, the reliability of LLMs in software engineering is often compromised by ambiguities and inconsistencies due to improper user specification. To address this challenge, we present CodeMetaAgent (CMA), a metamorphic relation-driven LLM agent that systematically refines task specifications and generates semantically constrained test cases. Our proposed framework uses MRs with LLMs to improve generation consistency and reduce variability caused by specifications, unlike the traditional use of MRs as post validations. Our framework has been evaluated on the HumanEval-Pro, MBPP-Pro, and SWE-Bench_Lite datasets using the GPT-4o, Mistral Large, GPT-OSS, and Qwen3-Coder models. It improved code generation accuracy by up to 17% and achieved code coverage gains of up to 99.81%. These results show that metamorphic relations can be a simple but effective guide in assisting LLM-based software development.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faCodeMetaAgent\uff08CMA\uff09\uff0c\u4e00\u79cd\u57fa\u4e8e\u8715\u53d8\u5173\u7cfb\uff08MRs\uff09\u9a71\u52a8\u7684\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u667a\u80fd\u4f53\uff0c\u901a\u8fc7\u5728\u751f\u6210\u8fc7\u7a0b\u4e2d\u5f15\u5165MRs\u6765\u7cfb\u7edf\u5316\u5730\u4f18\u5316\u4efb\u52a1\u63cf\u8ff0\u5e76\u751f\u6210\u8bed\u4e49\u7ea6\u675f\u7684\u6d4b\u8bd5\u7528\u4f8b\uff0c\u4ece\u800c\u63d0\u5347LLM\u5728\u8f6f\u4ef6\u5de5\u7a0b\u4e2d\u7684\u4ee3\u7801\u751f\u6210\u51c6\u786e\u6027\u548c\u8986\u76d6\u7387\u3002", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b\u5728\u8f6f\u4ef6\u5de5\u7a0b\u4e2d\u7684\u53ef\u9760\u6027\u5e38\u56e0\u7528\u6237\u8f93\u5165\u89c4\u8303\u4e0d\u660e\u786e\u6216\u4e0d\u4e00\u81f4\u800c\u53d7\u635f\uff0c\u73b0\u6709\u65b9\u6cd5\u591a\u5c06\u8715\u53d8\u5173\u7cfb\u7528\u4e8e\u4e8b\u540e\u9a8c\u8bc1\uff0c\u7f3a\u4e4f\u5728\u751f\u6210\u9636\u6bb5\u5f15\u5bfcLLM\u7684\u80fd\u529b\u3002", "method": "\u5c06\u8715\u53d8\u5173\u7cfb\u4e0eLLM\u7ed3\u5408\uff0c\u5728\u4ee3\u7801\u751f\u6210\u524d\u5229\u7528MRs\u5bf9\u4efb\u52a1\u89c4\u8303\u8fdb\u884c\u7cfb\u7edf\u6027\u7cbe\u70bc\uff0c\u5e76\u751f\u6210\u8bed\u4e49\u7b49\u4ef7\u4e14\u53d7\u7ea6\u675f\u7684\u6d4b\u8bd5\u7528\u4f8b\uff0c\u4ee5\u51cf\u5c11\u7531\u6a21\u7cca\u89c4\u8303\u5f15\u8d77\u7684\u8f93\u51fa\u53d8\u5f02\u6027\u3002", "result": "\u5728HumanEval-Pro\u3001MBPP-Pro\u548cSWE-Bench_Lite\u6570\u636e\u96c6\u4e0a\u8bc4\u4f30\uff0c\u4f7f\u7528GPT-4o\u3001Mistral Large\u3001GPT-OSS\u548cQwen3-Coder\u6a21\u578b\uff0c\u4ee3\u7801\u751f\u6210\u51c6\u786e\u7387\u6700\u9ad8\u63d0\u534717%\uff0c\u4ee3\u7801\u8986\u76d6\u7387\u6700\u9ad8\u8fbe99.81%\u3002", "conclusion": "\u8715\u53d8\u5173\u7cfb\u53ef\u4f5c\u4e3a\u7b80\u5355\u800c\u6709\u6548\u7684\u5f15\u5bfc\u673a\u5236\uff0c\u663e\u8457\u63d0\u5347LLM\u5728\u8f6f\u4ef6\u5f00\u53d1\u4e2d\u7684\u751f\u6210\u4e00\u81f4\u6027\u4e0e\u53ef\u9760\u6027\u3002"}}
{"id": "2511.19146", "categories": ["cs.MA"], "pdf": "https://arxiv.org/pdf/2511.19146", "abs": "https://arxiv.org/abs/2511.19146", "authors": ["Qian Zhang", "Zhuo Sun", "Yao Zhang", "Zhiwen Yu", "Bin Guo", "Jun Zhang"], "title": "VIL2C: Value-of-Information Aware Low-Latency Communication for Multi-Agent Reinforcement Learning", "comment": null, "summary": "Inter-agent communication serves as an effective mechanism for enhancing performance in collaborative multi-agent reinforcement learning(MARL) systems. However, the inherent communication latency in practical systems induces both action decision delays and outdated information sharing, impeding MARL performance gains, particularly in time-critical applications like autonomous driving. In this work, we propose a Value-of-Information aware Low-latency Communication(VIL2C) scheme that proactively adjusts the latency distribution to mitigate its effects in MARL systems. Specifically, we define a Value of Information (VOI) metric to quantify the importance of delayed message transmission based on each delayed message's importance. Moreover, we propose a progressive message reception mechanism to adaptively adjust the reception duration based on received messages. We derive the optimized VoI aware resource allocation and theoretically prove the performance advantage of the proposed VIL2C scheme. Extensive experiments demonstrate that VIL2C outperforms existing approaches under various communication conditions. These gains are attributed to the low-latency transmission of high-VoI messages via resource allocation and the elimination of unnecessary waiting periods via adaptive reception duration.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9762\u5411\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\uff08MARL\uff09\u7cfb\u7edf\u7684\u4f4e\u5ef6\u8fdf\u901a\u4fe1\u65b9\u6848VIL2C\uff0c\u901a\u8fc7\u5f15\u5165\u4fe1\u606f\u4ef7\u503c\uff08VoI\uff09\u5ea6\u91cf\u548c\u81ea\u9002\u5e94\u63a5\u6536\u673a\u5236\uff0c\u5728\u5b58\u5728\u901a\u4fe1\u5ef6\u8fdf\u7684\u73b0\u5b9e\u573a\u666f\u4e2d\u4f18\u5316\u8d44\u6e90\u5206\u914d\uff0c\u663e\u8457\u63d0\u5347\u4e86\u7cfb\u7edf\u6027\u80fd\u3002", "motivation": "\u5b9e\u9645MARL\u7cfb\u7edf\u4e2d\u7684\u901a\u4fe1\u5ef6\u8fdf\u4f1a\u5bfc\u81f4\u52a8\u4f5c\u51b3\u7b56\u6ede\u540e\u548c\u4fe1\u606f\u8fc7\u65f6\uff0c\u5c24\u5176\u5728\u81ea\u52a8\u9a7e\u9a76\u7b49\u65f6\u95f4\u654f\u611f\u5e94\u7528\u4e2d\u4e25\u91cd\u5236\u7ea6\u6027\u80fd\u63d0\u5347\uff0c\u56e0\u6b64\u4e9f\u9700\u4e00\u79cd\u80fd\u4e3b\u52a8\u5e94\u5bf9\u5ef6\u8fdf\u5f71\u54cd\u7684\u901a\u4fe1\u673a\u5236\u3002", "method": "\u63d0\u51faVIL2C\u65b9\u6848\uff1a1\uff09\u5b9a\u4e49\u4fe1\u606f\u4ef7\u503c\uff08VoI\uff09\u6307\u6807\u4ee5\u91cf\u5316\u5ef6\u8fdf\u6d88\u606f\u7684\u91cd\u8981\u6027\uff1b2\uff09\u8bbe\u8ba1\u6e10\u8fdb\u5f0f\u6d88\u606f\u63a5\u6536\u673a\u5236\uff0c\u6839\u636e\u5df2\u63a5\u6536\u6d88\u606f\u52a8\u6001\u8c03\u6574\u63a5\u6536\u65f6\u957f\uff1b3\uff09\u63a8\u5bfc\u57fa\u4e8eVoI\u7684\u6700\u4f18\u8d44\u6e90\u5206\u914d\u7b56\u7565\u3002", "result": "\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cVIL2C\u5728\u591a\u79cd\u901a\u4fe1\u6761\u4ef6\u4e0b\u5747\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5176\u4f18\u52bf\u6e90\u4e8e\u9ad8VoI\u6d88\u606f\u7684\u4f4e\u5ef6\u8fdf\u4f20\u8f93\u548c\u901a\u8fc7\u81ea\u9002\u5e94\u63a5\u6536\u673a\u5236\u6d88\u9664\u4e0d\u5fc5\u8981\u7684\u7b49\u5f85\u65f6\u95f4\u3002", "conclusion": "VIL2C\u901a\u8fc7\u7ed3\u5408\u4fe1\u606f\u4ef7\u503c\u611f\u77e5\u4e0e\u81ea\u9002\u5e94\u901a\u4fe1\u8c03\u5ea6\uff0c\u6709\u6548\u7f13\u89e3\u4e86\u901a\u4fe1\u5ef6\u8fdf\u5bf9MARL\u6027\u80fd\u7684\u8d1f\u9762\u5f71\u54cd\uff0c\u4e3a\u65f6\u95f4\u654f\u611f\u578b\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u63d0\u4f9b\u4e86\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2511.18343", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2511.18343", "abs": "https://arxiv.org/abs/2511.18343", "authors": ["Dongming Jin", "Zhi Jin", "Xiaohong Chen", "Zheng Fang", "Linyu Li", "Yuanpeng He", "Jia Li", "Yirang Zhang", "Yingtao Fang"], "title": "A Needle in a Haystack: Intent-driven Reusable Artifacts Recommendation with LLMs", "comment": "15 pages, 7 figures", "summary": "In open source software development, the reuse of existing artifacts has been widely adopted to avoid redundant implementation work. Reusable artifacts are considered more efficient and reliable than developing software components from scratch. However, when faced with a large number of reusable artifacts, developers often struggle to find artifacts that can meet their expected needs. To reduce this burden, retrieval-based and learning-based techniques have been proposed to automate artifact recommendations. Recently, Large Language Models (LLMs) have shown the potential to understand intentions, perform semantic alignment, and recommend usable artifacts. Nevertheless, their effectiveness has not been thoroughly explored. To fill this gap, we construct an intent-driven artifact recommendation benchmark named IntentRecBench, covering three representative open source ecosystems. Using IntentRecBench, we conduct a comprehensive comparative study of five popular LLMs and six traditional approaches in terms of precision and efficiency. Our results show that although LLMs outperform traditional methods, they still suffer from low precision and high inference cost due to the large candidate space. Inspired by the ontology-based semantic organization in software engineering, we propose TreeRec, a feature tree-guided recommendation framework to mitigate these issues. TreeRec leverages LLM-based semantic abstraction to organize artifacts into a hierarchical semantic tree, enabling intent and function alignment and reducing reasoning time. Extensive experiments demonstrate that TreeRec consistently improves the performance of diverse LLMs across ecosystems, highlighting its generalizability and potential for practical deployment.", "AI": {"tldr": "\u672c\u6587\u9488\u5bf9\u5f00\u6e90\u8f6f\u4ef6\u5f00\u53d1\u4e2d\u53ef\u590d\u7528\u6784\u4ef6\u63a8\u8350\u7684\u6311\u6218\uff0c\u6784\u5efa\u4e86\u610f\u56fe\u9a71\u52a8\u7684\u57fa\u51c6\u6570\u636e\u96c6IntentRecBench\uff0c\u5e76\u5bf9\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u4e0e\u4f20\u7edf\u65b9\u6cd5\u8fdb\u884c\u4e86\u7cfb\u7edf\u8bc4\u4f30\u3002\u7814\u7a76\u53d1\u73b0LLMs\u867d\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\uff0c\u4f46\u4ecd\u5b58\u5728\u7cbe\u5ea6\u4f4e\u548c\u63a8\u7406\u6210\u672c\u9ad8\u7684\u95ee\u9898\u3002\u4e3a\u6b64\uff0c\u4f5c\u8005\u63d0\u51faTreeRec\u6846\u67b6\uff0c\u5229\u7528\u7279\u5f81\u6811\u5f15\u5bfc\u7684\u8bed\u4e49\u7ec4\u7ec7\u65b9\u5f0f\u63d0\u5347\u63a8\u8350\u6548\u679c\u548c\u6548\u7387\uff0c\u5b9e\u9a8c\u8bc1\u660e\u5176\u5728\u591a\u4e2a\u751f\u6001\u7cfb\u7edf\u4e2d\u5177\u6709\u826f\u597d\u7684\u901a\u7528\u6027\u548c\u5b9e\u7528\u6027\u3002", "motivation": "\u5728\u5f00\u6e90\u8f6f\u4ef6\u5f00\u53d1\u4e2d\uff0c\u5c3d\u7ba1\u53ef\u590d\u7528\u6784\u4ef6\u80fd\u63d0\u9ad8\u6548\u7387\u548c\u53ef\u9760\u6027\uff0c\u4f46\u9762\u5bf9\u5927\u91cf\u5019\u9009\u6784\u4ef6\uff0c\u5f00\u53d1\u8005\u96be\u4ee5\u5feb\u901f\u627e\u5230\u7b26\u5408\u9700\u6c42\u7684\u6784\u4ef6\u3002\u73b0\u6709\u57fa\u4e8e\u68c0\u7d22\u548c\u5b66\u4e60\u7684\u65b9\u6cd5\u4ee5\u53ca\u65b0\u5174\u7684\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5c1a\u672a\u88ab\u5145\u5206\u8bc4\u4f30\u5176\u5728\u8be5\u4efb\u52a1\u4e2d\u7684\u6709\u6548\u6027\uff0c\u5c24\u5176\u7f3a\u4e4f\u7edf\u4e00\u7684\u57fa\u51c6\u548c\u5bf9LLMs\u5c40\u9650\u6027\u7684\u6df1\u5165\u5206\u6790\u3002", "method": "\u4f5c\u8005\u9996\u5148\u6784\u5efa\u4e86\u4e00\u4e2a\u8986\u76d6\u4e09\u4e2a\u4e3b\u6d41\u5f00\u6e90\u751f\u6001\u7cfb\u7edf\u7684\u610f\u56fe\u9a71\u52a8\u6784\u4ef6\u63a8\u8350\u57fa\u51c6IntentRecBench\uff1b\u7136\u540e\u5bf9\u4e94\u79cd\u4e3b\u6d41LLMs\u548c\u516d\u79cd\u4f20\u7edf\u65b9\u6cd5\u8fdb\u884c\u7cbe\u5ea6\u4e0e\u6548\u7387\u7684\u7efc\u5408\u6bd4\u8f83\uff1b\u5728\u6b64\u57fa\u7840\u4e0a\uff0c\u63d0\u51faTreeRec\u6846\u67b6\uff0c\u901a\u8fc7LLM\u9a71\u52a8\u7684\u8bed\u4e49\u62bd\u8c61\u5c06\u6784\u4ef6\u7ec4\u7ec7\u6210\u5c42\u6b21\u5316\u8bed\u4e49\u6811\uff0c\u4ee5\u5b9e\u73b0\u610f\u56fe-\u529f\u80fd\u5bf9\u9f50\u5e76\u964d\u4f4e\u63a8\u7406\u5f00\u9500\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u867d\u7136LLMs\u6574\u4f53\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\uff0c\u4f46\u5728\u5927\u89c4\u6a21\u5019\u9009\u7a7a\u95f4\u4e0b\u4ecd\u9762\u4e34\u7cbe\u5ea6\u4e0d\u8db3\u548c\u63a8\u7406\u6210\u672c\u9ad8\u7684\u95ee\u9898\uff1b\u800cTreeRec\u80fd\u663e\u8457\u63d0\u5347\u591a\u79cdLLMs\u5728\u4e0d\u540c\u751f\u6001\u7cfb\u7edf\u4e2d\u7684\u63a8\u8350\u6027\u80fd\uff0c\u5c55\u73b0\u51fa\u826f\u597d\u7684\u6cdb\u5316\u80fd\u529b\u548c\u5b9e\u9645\u90e8\u7f72\u6f5c\u529b\u3002", "conclusion": "\u672c\u7814\u7a76\u63ed\u793a\u4e86\u5f53\u524dLLMs\u5728\u6784\u4ef6\u63a8\u8350\u4efb\u52a1\u4e2d\u7684\u4f18\u52bf\u4e0e\u5c40\u9650\uff0c\u5e76\u901a\u8fc7\u63d0\u51fa\u7684TreeRec\u6846\u67b6\u6709\u6548\u7f13\u89e3\u4e86\u8fd9\u4e9b\u95ee\u9898\uff0c\u4e3a\u672a\u6765\u57fa\u4e8e\u8bed\u4e49\u7ed3\u6784\u5316\u548c\u5927\u6a21\u578b\u7684\u8f6f\u4ef6\u6784\u4ef6\u63a8\u8350\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u548c\u5b9e\u7528\u5de5\u5177\u3002"}}
{"id": "2511.18488", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.18488", "abs": "https://arxiv.org/abs/2511.18488", "authors": ["Samuel Ackerman", "Wesam Ibraheem", "Orna Raz", "Marcel Zalmanovici"], "title": "Evaluating perturbation robustnessof generative systems that use COBOL code inputs", "comment": "16 pages (8 main, 8 appendix). Accepted to AI-SQE (ICSE, 2026): The 1st International Workshop on AI for Software Quality Evaluation: Judgment, Metrics, Benchmarks, and Beyond", "summary": "Systems incorporating large language models (LLMs) as a component are known to be sensitive (i.e., non-robust) to minor input variations that do not change the meaning of the input; such sensitivity may reduce the system's usefulness. Here, we present a framework to evaluate robustness of systems using COBOL code as input; our application is translation between COBOL and Java programming languages, but the approach extends to other tasks such as code generation or explanation. Targeting robustness of systems with COBOL as input is essential yet challenging. Many business-critical applications are written in COBOL, yet these are typically proprietary legacy applications and their code is unavailable to LLMs for training. We develop a library of COBOL paragraph and full-program perturbation methods, and create variant-expanded versions of a benchmark dataset of examples for a specific task. The robustness of the LLM-based system is evaluated by measuring changes in values of individual and aggregate metrics calculated on the system's outputs. Finally, we present a series of dynamic table and chart visualization dashboards that assist in debugging the system's outputs, and monitoring and understanding root causes of the system's sensitivity to input variation. These tools can be further used to improve the system by, for instance, indicating variations that should be handled by pre-processing steps.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u4e2a\u8bc4\u4f30\u4ee5COBOL\u4ee3\u7801\u4e3a\u8f93\u5165\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7cfb\u7edf\u9c81\u68d2\u6027\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u6784\u5efaCOBOL\u4ee3\u7801\u6270\u52a8\u65b9\u6cd5\u548c\u6269\u5c55\u57fa\u51c6\u6570\u636e\u96c6\uff0c\u7ed3\u5408\u53ef\u89c6\u5316\u5de5\u5177\u5206\u6790\u7cfb\u7edf\u5bf9\u8bed\u4e49\u4e0d\u53d8\u8f93\u5165\u53d8\u5316\u7684\u654f\u611f\u6027\uff0c\u5e76\u652f\u6301\u540e\u7eed\u7cfb\u7edf\u4f18\u5316\u3002", "motivation": "\u5f53\u524d\u5305\u542b\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u7cfb\u7edf\u5bf9\u4e0d\u6539\u53d8\u8bed\u4e49\u7684\u5fae\u5c0f\u8f93\u5165\u53d8\u5316\u8fc7\u4e8e\u654f\u611f\uff0c\u5f71\u54cd\u5b9e\u7528\u6027\uff1b\u800cCOBOL\u4f5c\u4e3a\u5927\u91cf\u5173\u952e\u4e1a\u52a1\u7cfb\u7edf\u7684\u8bed\u8a00\uff0c\u5176\u4ee3\u7801\u901a\u5e38\u65e0\u6cd5\u7528\u4e8e\u6a21\u578b\u8bad\u7ec3\uff0c\u4f7f\u5f97\u9488\u5bf9COBOL\u8f93\u5165\u7684\u7cfb\u7edf\u9c81\u68d2\u6027\u8bc4\u4f30\u5c24\u4e3a\u5fc5\u8981\u4e14\u5177\u6709\u6311\u6218\u6027\u3002", "method": "\u5f00\u53d1\u4e00\u5957COBOL\u6bb5\u843d\u4e0e\u5b8c\u6574\u7a0b\u5e8f\u7684\u6270\u52a8\u65b9\u6cd5\u5e93\uff0c\u6784\u5efa\u4efb\u52a1\u7279\u5b9a\u7684\u53d8\u4f53\u6269\u5c55\u57fa\u51c6\u6570\u636e\u96c6\uff1b\u901a\u8fc7\u8ba1\u7b97\u7cfb\u7edf\u8f93\u51fa\u5728\u4e2a\u4f53\u4e0e\u805a\u5408\u6307\u6807\u4e0a\u7684\u53d8\u5316\u6765\u8bc4\u4f30\u9c81\u68d2\u6027\uff0c\u5e76\u8bbe\u8ba1\u52a8\u6001\u8868\u683c\u4e0e\u56fe\u8868\u53ef\u89c6\u5316\u4eea\u8868\u677f\u8f85\u52a9\u8c03\u8bd5\u4e0e\u5f52\u56e0\u5206\u6790\u3002", "result": "\u6210\u529f\u6784\u5efa\u4e86\u53ef\u7528\u4e8e\u8bc4\u4f30COBOL\u76f8\u5173\u4efb\u52a1\uff08\u5982COBOL-Java\u7ffb\u8bd1\uff09\u4e2dLLM\u7cfb\u7edf\u9c81\u68d2\u6027\u7684\u6270\u52a8\u6570\u636e\u96c6\u4e0e\u8bc4\u4f30\u6d41\u7a0b\uff0c\u5e76\u63d0\u4f9b\u4e86\u53ef\u89c6\u5316\u5de5\u5177\u5e2e\u52a9\u8bc6\u522b\u8f93\u5165\u53d8\u5316\u5bfc\u81f4\u6027\u80fd\u4e0b\u964d\u7684\u6839\u672c\u539f\u56e0\u3002", "conclusion": "\u8be5\u6846\u67b6\u6709\u6548\u652f\u6301\u5bf9\u57fa\u4e8eLLM\u7684COBOL\u5904\u7406\u7cfb\u7edf\u7684\u9c81\u68d2\u6027\u8bc4\u4f30\u4e0e\u8c03\u8bd5\uff0c\u5176\u65b9\u6cd5\u53ef\u63a8\u5e7f\u81f3\u5176\u4ed6\u4ee3\u7801\u751f\u6210\u6216\u89e3\u91ca\u4efb\u52a1\uff0c\u5e76\u53ef\u901a\u8fc7\u9884\u5904\u7406\u7b49\u624b\u6bb5\u8fdb\u4e00\u6b65\u63d0\u5347\u7cfb\u7edf\u7a33\u5b9a\u6027\u3002"}}
{"id": "2511.18506", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2511.18506", "abs": "https://arxiv.org/abs/2511.18506", "authors": ["Michael Adjei Osei", "Sidney Shapiro"], "title": "HQPEF-Py: Metrics, Python Patterns, and Guidance for Evaluating Hybrid Quantum Programs", "comment": "17 pages", "summary": "We study how to evaluate hybrid quantum programs as end-to-end workflows rather than as isolated devices or algorithms. Building on the Hybrid Quantum Program Evaluation Framework (HQPEF), we formalize a workflow-aware Quantum Readiness Level (QRL) score; define a normalized speedup under quality constraints for the Utility of Quantumness (UQ); and provide a timing-and-drift audit for hybrid pipelines. We complement these definitions with concise Python reference implementations that illustrate how to instantiate the metrics and audit procedures with state-of-the-art classical and quantum solvers (e.g., via Qiskit or PennyLane), while preserving matched-budget discipline and reproducibility.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9762\u5411\u7aef\u5230\u7aef\u6df7\u5408\u91cf\u5b50\u5de5\u4f5c\u6d41\u7684\u8bc4\u4f30\u65b9\u6cd5\uff0c\u5305\u62ec\u5de5\u4f5c\u6d41\u611f\u77e5\u7684\u91cf\u5b50\u5c31\u7eea\u7b49\u7ea7\uff08QRL\uff09\u3001\u5e26\u8d28\u91cf\u7ea6\u675f\u7684\u5f52\u4e00\u5316\u52a0\u901f\u6bd4\uff08\u7528\u4e8e\u91cf\u5b50\u6548\u7528UQ\uff09\uff0c\u4ee5\u53ca\u6df7\u5408\u6d41\u6c34\u7ebf\u7684\u65f6\u95f4\u4e0e\u6f02\u79fb\u5ba1\u8ba1\uff0c\u5e76\u63d0\u4f9b\u4e86\u76f8\u5e94\u7684Python\u53c2\u8003\u5b9e\u73b0\u3002", "motivation": "\u73b0\u6709\u91cf\u5b50\u7a0b\u5e8f\u8bc4\u4f30\u591a\u805a\u7126\u4e8e\u5b64\u7acb\u8bbe\u5907\u6216\u7b97\u6cd5\uff0c\u7f3a\u4e4f\u5bf9\u7aef\u5230\u7aef\u6df7\u5408\u91cf\u5b50\u5de5\u4f5c\u6d41\u7684\u6574\u4f53\u8bc4\u4ef7\u4f53\u7cfb\u3002", "method": "\u57fa\u4e8eHQPEF\u6846\u67b6\uff0c\u5f62\u5f0f\u5316\u5b9a\u4e49\u4e86\u5de5\u4f5c\u6d41\u611f\u77e5\u7684QRL\u8bc4\u5206\u3001\u5728\u8d28\u91cf\u7ea6\u675f\u4e0b\u7684\u5f52\u4e00\u5316\u52a0\u901f\u6bd4\uff08UQ\uff09\u4ee5\u53ca\u6df7\u5408\u6d41\u6c34\u7ebf\u7684\u65f6\u5e8f\u4e0e\u6f02\u79fb\u5ba1\u8ba1\u65b9\u6cd5\uff0c\u5e76\u901a\u8fc7Python\u5b9e\u73b0\u793a\u4f8b\u8bf4\u660e\u5982\u4f55\u7ed3\u5408\u4e3b\u6d41\u7ecf\u5178\u4e0e\u91cf\u5b50\u6c42\u89e3\u5668\u8fdb\u884c\u53ef\u590d\u73b0\u3001\u9884\u7b97\u5339\u914d\u7684\u8bc4\u4f30\u3002", "result": "\u63d0\u51fa\u4e86\u4e09\u9879\u65b0\u6307\u6807\u4e0e\u5ba1\u8ba1\u6d41\u7a0b\uff0c\u5e76\u914d\u5957\u63d0\u4f9b\u7b80\u6d01\u7684Python\u53c2\u8003\u5b9e\u73b0\uff0c\u652f\u6301\u5728Qiskit\u6216PennyLane\u7b49\u5e73\u53f0\u4e0a\u5b9e\u4f8b\u5316\u8fd9\u4e9b\u8bc4\u4f30\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u4e3a\u6df7\u5408\u91cf\u5b50\u7a0b\u5e8f\u7684\u7aef\u5230\u7aef\u8bc4\u4f30\u63d0\u4f9b\u4e86\u7cfb\u7edf\u6027\u6846\u67b6\u548c\u5b9e\u7528\u5de5\u5177\uff0c\u6709\u52a9\u4e8e\u66f4\u5168\u9762\u5730\u8861\u91cf\u91cf\u5b50\u8ba1\u7b97\u7684\u5b9e\u9645\u6548\u7528\u4e0e\u6210\u719f\u5ea6\u3002"}}
{"id": "2511.18528", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2511.18528", "abs": "https://arxiv.org/abs/2511.18528", "authors": ["Renyi Zhong", "Yintong Huo", "Wenwei Gu", "Yichen Li", "Michael R. Lyu"], "title": "End-to-End Automated Logging via Multi-Agent Framework", "comment": null, "summary": "Software logging is critical for system observability, yet developers face a dual crisis of costly overlogging and risky underlogging. Existing automated logging tools often overlook the fundamental whether-to-log decision and struggle with the composite nature of logging. In this paper, we propose Autologger, a novel hybrid framework that addresses the complete the end-to-end logging pipeline. Autologger first employs a fine-tuned classifier, the Judger, to accurately determine if a method requires new logging statements. If logging is needed, a multi-agent system is activated. The system includes specialized agents: a Locator dedicated to determining where to log, and a Generator focused on what to log. These agents work together, utilizing our designed program analysis and retrieval tools. We evaluate Autologger on a large corpus from three mature open-source projects against state-of-the-art baselines. Our results show that Autologger achieves 96.63\\% F1-score on the crucial whether-to-log decision. In an end-to-end setting, Autologger improves the overall quality of generated logging statements by 16.13\\% over the strongest baseline, as measured by an LLM-as-a-judge score. We also demonstrate that our framework is generalizable, consistently boosting the performance of various backbone LLMs.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa Autologger\uff0c\u4e00\u4e2a\u7ed3\u5408\u5206\u7c7b\u5668\u4e0e\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u7684\u6df7\u5408\u6846\u67b6\uff0c\u7528\u4e8e\u81ea\u52a8\u5316\u5b8c\u6210\u201c\u662f\u5426\u8bb0\u5f55\u65e5\u5fd7\u201d\u3001\u201c\u5728\u54ea\u91cc\u8bb0\u5f55\u201d\u4ee5\u53ca\u201c\u8bb0\u5f55\u4ec0\u4e48\u201d\u4e09\u4e2a\u5173\u952e\u51b3\u7b56\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u5f00\u53d1\u8005\u5728\u8f6f\u4ef6\u65e5\u5fd7\u5b9e\u8df5\u4e2d\u9762\u4e34\u8fc7\u5ea6\u8bb0\u5f55\uff08\u6210\u672c\u9ad8\uff09\u548c\u8bb0\u5f55\u4e0d\u8db3\uff08\u98ce\u9669\u5927\uff09\u7684\u53cc\u91cd\u56f0\u5883\uff0c\u800c\u73b0\u6709\u81ea\u52a8\u65e5\u5fd7\u5de5\u5177\u5f80\u5f80\u5ffd\u7565\u201c\u662f\u5426\u8bb0\u5f55\u201d\u8fd9\u4e00\u6839\u672c\u51b3\u7b56\uff0c\u4e14\u96be\u4ee5\u5904\u7406\u65e5\u5fd7\u751f\u6210\u7684\u590d\u5408\u6027\u95ee\u9898\u3002", "method": "Autologger \u6846\u67b6\u9996\u5148\u4f7f\u7528\u5fae\u8c03\u540e\u7684\u5206\u7c7b\u5668\uff08Judger\uff09\u5224\u65ad\u65b9\u6cd5\u662f\u5426\u9700\u8981\u65b0\u589e\u65e5\u5fd7\uff1b\u82e5\u9700\u8981\uff0c\u5219\u6fc0\u6d3b\u5305\u542b Locator\uff08\u51b3\u5b9a\u65e5\u5fd7\u4f4d\u7f6e\uff09\u548c Generator\uff08\u51b3\u5b9a\u65e5\u5fd7\u5185\u5bb9\uff09\u7684\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\uff0c\u5e76\u7ed3\u5408\u7a0b\u5e8f\u5206\u6790\u4e0e\u68c0\u7d22\u5de5\u5177\u534f\u540c\u5de5\u4f5c\u3002", "result": "\u5728\u4e09\u4e2a\u6210\u719f\u5f00\u6e90\u9879\u76ee\u7684\u5927\u89c4\u6a21\u8bed\u6599\u4e0a\u8bc4\u4f30\u8868\u660e\uff0cAutologger \u5728\u201c\u662f\u5426\u8bb0\u5f55\u201d\u4efb\u52a1\u4e0a\u8fbe\u5230 96.63% \u7684 F1 \u5206\u6570\uff1b\u7aef\u5230\u7aef\u8bbe\u7f6e\u4e0b\uff0c\u5176\u751f\u6210\u65e5\u5fd7\u7684\u6574\u4f53\u8d28\u91cf\u6bd4\u6700\u5f3a\u57fa\u7ebf\u63d0\u5347 16.13%\uff08\u57fa\u4e8e LLM-as-a-judge \u8bc4\u5206\uff09\uff0c\u4e14\u80fd\u7a33\u5b9a\u63d0\u5347\u591a\u79cd\u9aa8\u5e72 LLM \u7684\u6027\u80fd\u3002", "conclusion": "Autologger \u6709\u6548\u89e3\u51b3\u4e86\u81ea\u52a8\u65e5\u5fd7\u751f\u6210\u4e2d\u7684\u6838\u5fc3\u6311\u6218\uff0c\u63d0\u4f9b\u4e86\u4e00\u4e2a\u901a\u7528\u3001\u9ad8\u6548\u4e14\u9ad8\u8d28\u91cf\u7684\u7aef\u5230\u7aef\u65e5\u5fd7\u63d2\u5165\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2511.18538", "categories": ["cs.SE", "cs.CL"], "pdf": "https://arxiv.org/pdf/2511.18538", "abs": "https://arxiv.org/abs/2511.18538", "authors": ["Jian Yang", "Wei Zhang", "Shark Liu", "Jiajun Wu", "Shawn Guo", "Yizhi Li"], "title": "From Code Foundation Models to Agents and Applications: A Practical Guide to Code Intelligence", "comment": null, "summary": "Large language models (LLMs) have fundamentally transformed automated software development by enabling direct translation of natural language descriptions into functional code, driving commercial adoption through tools like Github Copilot (Microsoft), Cursor (Anysphere), Trae (ByteDance), and Claude Code (Anthropic). While the field has evolved dramatically from rule-based systems to Transformer-based architectures, achieving performance improvements from single-digit to over 95\\% success rates on benchmarks like HumanEval. In this work, we provide a comprehensive synthesis and practical guide (a series of analytic and probing experiments) about code LLMs, systematically examining the complete model life cycle from data curation to post-training through advanced prompting paradigms, code pre-training, supervised fine-tuning, reinforcement learning, and autonomous coding agents. We analyze the code capability of the general LLMs (GPT-4, Claude, LLaMA) and code-specialized LLMs (StarCoder, Code LLaMA, DeepSeek-Coder, and QwenCoder), critically examining the techniques, design decisions, and trade-offs. Further, we articulate the research-practice gap between academic research (e.g., benchmarks and tasks) and real-world deployment (e.g., software-related code tasks), including code correctness, security, contextual awareness of large codebases, and integration with development workflows, and map promising research directions to practical needs. Last, we conduct a series of experiments to provide a comprehensive analysis of code pre-training, supervised fine-tuning, and reinforcement learning, covering scaling law, framework selection, hyperparameter sensitivity, model architectures, and dataset comparisons.", "AI": {"tldr": "\u672c\u6587\u7cfb\u7edf\u7efc\u8ff0\u4e86\u4ee3\u7801\u5927\u8bed\u8a00\u6a21\u578b\uff08Code LLMs\uff09\u7684\u5168\u751f\u547d\u5468\u671f\uff0c\u6db5\u76d6\u6570\u636e\u6784\u5efa\u3001\u9884\u8bad\u7ec3\u3001\u76d1\u7763\u5fae\u8c03\u3001\u5f3a\u5316\u5b66\u4e60\u53ca\u9ad8\u7ea7\u63d0\u793a\u65b9\u6cd5\uff0c\u5e76\u5bf9\u6bd4\u901a\u7528\u4e0e\u4e13\u7528\u6a21\u578b\u6027\u80fd\uff0c\u5206\u6790\u5b66\u672f\u7814\u7a76\u4e0e\u5de5\u4e1a\u5b9e\u8df5\u4e4b\u95f4\u7684\u5dee\u8ddd\uff0c\u540c\u65f6\u901a\u8fc7\u4e00\u7cfb\u5217\u5b9e\u9a8c\u6df1\u5165\u63a2\u8ba8\u8bad\u7ec3\u7b56\u7565\u4e0e\u6a21\u578b\u8bbe\u8ba1\u7684\u5173\u952e\u56e0\u7d20\u3002", "motivation": "\u968f\u7740\u5927\u8bed\u8a00\u6a21\u578b\u5728\u4ee3\u7801\u751f\u6210\u4efb\u52a1\u4e2d\u6027\u80fd\u663e\u8457\u63d0\u5347\u5e76\u88ab\u5e7f\u6cdb\u5e94\u7528\u4e8e\u5546\u4e1a\u5de5\u5177\uff0c\u4e9f\u9700\u7cfb\u7edf\u6027\u68b3\u7406\u5176\u6280\u672f\u6f14\u8fdb\u3001\u5173\u952e\u65b9\u6cd5\u4e0e\u5b9e\u9645\u90e8\u7f72\u6311\u6218\uff0c\u5f25\u5408\u5b66\u672f\u7814\u7a76\u4e0e\u771f\u5b9e\u8f6f\u4ef6\u5f00\u53d1\u573a\u666f\u4e4b\u95f4\u7684\u9e3f\u6c9f\u3002", "method": "\u4f5c\u8005\u901a\u8fc7\u7efc\u5408\u6587\u732e\u5206\u6790\u4e0e\u4e00\u7cfb\u5217\u63a2\u9488\u5b9e\u9a8c\uff0c\u7cfb\u7edf\u8003\u5bdf\u4ee3\u7801LLM\u4ece\u6570\u636e\u51c6\u5907\u5230\u540e\u8bad\u7ec3\u5404\u9636\u6bb5\u7684\u6280\u672f\u8def\u5f84\uff0c\u8bc4\u4f30\u4e3b\u6d41\u901a\u7528\u548c\u4e13\u7528\u6a21\u578b\u7684\u8868\u73b0\uff0c\u5e76\u5bf9\u9884\u8bad\u7ec3\u3001\u5fae\u8c03\u548c\u5f3a\u5316\u5b66\u4e60\u7b49\u73af\u8282\u8fdb\u884c\u6d88\u878d\u4e0e\u5bf9\u6bd4\u5b9e\u9a8c\u3002", "result": "\u7814\u7a76\u63ed\u793a\u4e86\u4e0d\u540c\u8bad\u7ec3\u9636\u6bb5\uff08\u5982\u9884\u8bad\u7ec3\u6570\u636e\u89c4\u6a21\u3001\u5fae\u8c03\u7b56\u7565\u3001RL\u5e94\u7528\uff09\u5bf9\u6a21\u578b\u6027\u80fd\u7684\u5f71\u54cd\uff0c\u660e\u786e\u4e86\u67b6\u6784\u9009\u62e9\u3001\u8d85\u53c2\u654f\u611f\u6027\u53ca\u6570\u636e\u8d28\u91cf\u7684\u5173\u952e\u4f5c\u7528\uff0c\u5e76\u6307\u51fa\u73b0\u6709\u57fa\u51c6\u6d4b\u8bd5\u5728\u4ee3\u7801\u6b63\u786e\u6027\u3001\u5b89\u5168\u6027\u53ca\u4e0a\u4e0b\u6587\u7406\u89e3\u7b49\u65b9\u9762\u4e0e\u5b9e\u9645\u9700\u6c42\u5b58\u5728\u5dee\u8ddd\u3002", "conclusion": "\u4ee3\u7801\u5927\u8bed\u8a00\u6a21\u578b\u867d\u53d6\u5f97\u663e\u8457\u8fdb\u5c55\uff0c\u4f46\u8981\u5b9e\u73b0\u53ef\u9760\u7684\u5b9e\u9645\u90e8\u7f72\u4ecd\u9700\u89e3\u51b3\u6b63\u786e\u6027\u3001\u5b89\u5168\u6027\u548c\u5927\u89c4\u6a21\u4ee3\u7801\u4e0a\u4e0b\u6587\u6574\u5408\u7b49\u95ee\u9898\uff1b\u672a\u6765\u7814\u7a76\u5e94\u805a\u7126\u4e8e\u8d34\u8fd1\u771f\u5b9e\u5f00\u53d1\u6d41\u7a0b\u7684\u4efb\u52a1\u8bbe\u8ba1\u4e0e\u8bc4\u4f30\u4f53\u7cfb\u3002"}}
{"id": "2511.18589", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.18589", "abs": "https://arxiv.org/abs/2511.18589", "authors": ["Michael Trusov", "Minha Hwang", "Zainab Jamal", "Swarup Chandra"], "title": "Strategic Decision Framework for Enterprise LLM Adoption", "comment": "14 pages, 1 key figure", "summary": "Organizations are rapidly adopting Large Language Models (LLMs) to transform their operations, yet they lack clear guidance on key decisions for adoption and implementation. While LLMs offer powerful capabilities in content generation, assisted coding, and process automation, businesses face critical challenges in data security, LLM solution development approach, infrastructure requirements, and deployment strategies. Healthcare providers must protect patient data while leveraging LLMs for medical analysis, financial institutions need to balance automated customer service with regulatory compliance, and software companies seek to enhance development productivity while maintaining code security.\n  This article presents a systematic six-step decision framework for LLM adoption, helping organizations navigate from initial application selection to final deployment. Based on extensive interviews and analysis of successful and failed implementations, our framework provides practical guidance for business leaders to align technological capabilities with business objectives. Through key decision points and real-world examples from both B2B and B2C contexts, organizations can make informed decisions about LLM adoption while ensuring secure and efficient integration across various use cases, from customer service automation to content creation and advanced analytics.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u4e2a\u516d\u6b65\u51b3\u7b56\u6846\u67b6\uff0c\u5e2e\u52a9\u4f01\u4e1a\u5728\u91c7\u7528\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u65f6\u4ece\u5e94\u7528\u573a\u666f\u9009\u62e9\u5230\u90e8\u7f72\u5168\u8fc7\u7a0b\u505a\u51fa\u660e\u667a\u51b3\u7b56\uff0c\u517c\u987e\u5b89\u5168\u6027\u3001\u5408\u89c4\u6027\u4e0e\u4e1a\u52a1\u76ee\u6807\u3002", "motivation": "\u4f01\u4e1a\u5728\u5feb\u901f\u91c7\u7528\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u8fc7\u7a0b\u4e2d\u9762\u4e34\u7f3a\u4e4f\u660e\u786e\u6307\u5bfc\u7684\u95ee\u9898\uff0c\u5c24\u5176\u5728\u6570\u636e\u5b89\u5168\u3001\u5f00\u53d1\u8def\u5f84\u3001\u57fa\u7840\u8bbe\u65bd\u548c\u90e8\u7f72\u7b56\u7565\u7b49\u5173\u952e\u51b3\u7b56\u4e0a\u5b58\u5728\u6311\u6218\u3002", "method": "\u57fa\u4e8e\u5bf9\u6210\u529f\u4e0e\u5931\u8d25\u6848\u4f8b\u7684\u6df1\u5165\u8bbf\u8c08\u548c\u5206\u6790\uff0c\u6784\u5efa\u4e86\u4e00\u4e2a\u7cfb\u7edf\u6027\u7684\u516d\u6b65\u51b3\u7b56\u6846\u67b6\uff0c\u5e76\u7ed3\u5408B2B\u548cB2C\u573a\u666f\u4e2d\u7684\u5b9e\u9645\u6848\u4f8b\u63d0\u4f9b\u5b9e\u8df5\u6307\u5bfc\u3002", "result": "\u8be5\u6846\u67b6\u5e2e\u52a9\u4e0d\u540c\u884c\u4e1a\u7684\u7ec4\u7ec7\uff08\u5982\u533b\u7597\u3001\u91d1\u878d\u3001\u8f6f\u4ef6\u516c\u53f8\uff09\u5728\u4fdd\u969c\u6570\u636e\u5b89\u5168\u4e0e\u5408\u89c4\u7684\u524d\u63d0\u4e0b\uff0c\u6709\u6548\u5c06LLM\u80fd\u529b\u4e0e\u4e1a\u52a1\u76ee\u6807\u5bf9\u9f50\uff0c\u5b9e\u73b0\u5b89\u5168\u9ad8\u6548\u7684\u96c6\u6210\u3002", "conclusion": "\u901a\u8fc7\u7ed3\u6784\u5316\u7684\u51b3\u7b56\u6d41\u7a0b\u548c\u771f\u5b9e\u6848\u4f8b\u652f\u6301\uff0c\u4f01\u4e1a\u53ef\u66f4\u81ea\u4fe1\u5730\u63a8\u8fdbLLM\u5e94\u7528\uff0c\u5728\u5ba2\u6237\u81ea\u52a8\u5316\u670d\u52a1\u3001\u5185\u5bb9\u751f\u6210\u548c\u9ad8\u7ea7\u5206\u6790\u7b49\u573a\u666f\u4e2d\u5b9e\u73b0\u4ef7\u503c\u3002"}}
{"id": "2511.18608", "categories": ["cs.SE", "cs.CR"], "pdf": "https://arxiv.org/pdf/2511.18608", "abs": "https://arxiv.org/abs/2511.18608", "authors": ["Jiangrui Zheng", "Yingming Zhou", "Ali Abdullah Ahmad", "Hanqing Yao", "Xueqing Liu"], "title": "From Reviewers' Lens: Understanding Bug Bounty Report Invalid Reasons with LLMs", "comment": "10 pages, 4 figures", "summary": "Bug bounty platforms (e.g., HackerOne, BugCrowd) leverage crowd-sourced vulnerability discovery to improve continuous coverage, reduce the cost of discovery, and serve as an integral complement to internal red teams. With the rise of AI-generated bug reports, little work exists to help bug hunters understand why these reports are labeled as invalid. To improve report quality and reduce reviewers' burden, it is critical to predict invalid reports and interpret invalid reasons.\n  In this work, we conduct an empirical study with the purpose of helping bug hunters understand the validity of reports. We collect a dataset of 9,942 disclosed bug bounty reports, including 1,400 invalid reports, and evaluate whether state-of-the-art large language models can identify invalid reports. While models such as GPT-5, DeepSeek, and a fine-tuned RoBERTa achieve strong overall accuracy, they consistently struggle to detect invalid cases, showing a tendency to over-accept reports. To improve invalidity detection, we build a taxonomy of rejection reasons for Information Disclosure vulnerabilities and incorporate it into a retrieval-augmented generation (RAG) framework. This approach substantially improves classification consistency and reduces bias. We also examine whether reviewer decisions may be influenced by factors beyond the content of the report. Our analysis shows that reporters with higher reputations tend to receive more favorable outcomes in borderline cases, suggesting that perceived expertise can influence review judgments.\n  Overall, our findings highlight the challenges of invalid report identification and show that combining LLMs with structured reviewer knowledge can support more transparent and consistent vulnerability report review.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86AI\u751f\u6210\u6f0f\u6d1e\u62a5\u544a\u5728\u4f17\u6d4b\u5e73\u53f0\u4e0a\u7684\u6709\u6548\u6027\u95ee\u9898\uff0c\u53d1\u73b0\u5927\u8bed\u8a00\u6a21\u578b\u867d\u6574\u4f53\u51c6\u786e\u7387\u9ad8\uff0c\u4f46\u5728\u8bc6\u522b\u65e0\u6548\u62a5\u544a\u65b9\u9762\u8868\u73b0\u4e0d\u4f73\uff1b\u901a\u8fc7\u6784\u5efa\u4fe1\u606f\u6cc4\u9732\u7c7b\u6f0f\u6d1e\u7684\u62d2\u7edd\u539f\u56e0\u5206\u7c7b\u4f53\u7cfb\u5e76\u7ed3\u5408\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u6846\u67b6\uff0c\u663e\u8457\u63d0\u5347\u4e86\u65e0\u6548\u62a5\u544a\u8bc6\u522b\u7684\u4e00\u81f4\u6027\u4e0e\u516c\u5e73\u6027\uff0c\u5e76\u63ed\u793a\u4e86\u62a5\u544a\u8005\u58f0\u8a89\u5bf9\u8bc4\u5ba1\u7ed3\u679c\u7684\u6f5c\u5728\u5f71\u54cd\u3002", "motivation": "\u968f\u7740AI\u751f\u6210\u6f0f\u6d1e\u62a5\u544a\u7684\u5174\u8d77\uff0c\u5927\u91cf\u62a5\u544a\u88ab\u6807\u8bb0\u4e3a\u65e0\u6548\uff0c\u4f46\u7f3a\u4e4f\u5bf9\u5176\u65e0\u6548\u539f\u56e0\u7684\u7cfb\u7edf\u7406\u89e3\u3002\u4e3a\u63d0\u5347\u62a5\u544a\u8d28\u91cf\u3001\u51cf\u8f7b\u8bc4\u5ba1\u8d1f\u62c5\uff0c\u4e9f\u9700\u6709\u6548\u9884\u6d4b\u5e76\u89e3\u91ca\u65e0\u6548\u62a5\u544a\u7684\u539f\u56e0\u3002", "method": "\u4f5c\u8005\u6536\u96c6\u4e869,942\u4efd\u516c\u5f00\u7684\u4f17\u6d4b\u62a5\u544a\uff08\u542b1,400\u4efd\u65e0\u6548\u62a5\u544a\uff09\uff0c\u8bc4\u4f30\u591a\u79cd\u5927\u8bed\u8a00\u6a21\u578b\uff08\u5982GPT-5\u3001DeepSeek\u548c\u5fae\u8c03RoBERTa\uff09\u5728\u8bc6\u522b\u65e0\u6548\u62a5\u544a\u4e0a\u7684\u8868\u73b0\uff1b\u968f\u540e\u6784\u5efa\u4fe1\u606f\u6cc4\u9732\u6f0f\u6d1e\u7684\u62d2\u7edd\u539f\u56e0\u5206\u7c7b\u4f53\u7cfb\uff0c\u5e76\u5c06\u5176\u6574\u5408\u8fdb\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u6846\u67b6\u4ee5\u6539\u8fdb\u8bc6\u522b\u6548\u679c\uff1b\u540c\u65f6\u5206\u6790\u62a5\u544a\u8005\u58f0\u8a89\u7b49\u975e\u5185\u5bb9\u56e0\u7d20\u5bf9\u8bc4\u5ba1\u51b3\u7b56\u7684\u5f71\u54cd\u3002", "result": "\u5927\u8bed\u8a00\u6a21\u578b\u5728\u6574\u4f53\u51c6\u786e\u7387\u4e0a\u8868\u73b0\u826f\u597d\uff0c\u4f46\u666e\u904d\u96be\u4ee5\u8bc6\u522b\u65e0\u6548\u62a5\u544a\uff0c\u5b58\u5728\u8fc7\u5ea6\u63a5\u53d7\u503e\u5411\uff1b\u5f15\u5165\u57fa\u4e8e\u5206\u7c7b\u4f53\u7cfb\u7684RAG\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u65e0\u6548\u62a5\u544a\u5206\u7c7b\u7684\u4e00\u81f4\u6027\u5e76\u51cf\u5c11\u4e86\u504f\u89c1\uff1b\u6b64\u5916\uff0c\u9ad8\u58f0\u8a89\u62a5\u544a\u8005\u5728\u8fb9\u754c\u6848\u4f8b\u4e2d\u66f4\u53ef\u80fd\u83b7\u5f97\u6709\u5229\u8bc4\u5ba1\u7ed3\u679c\u3002", "conclusion": "\u8bc6\u522b\u65e0\u6548\u6f0f\u6d1e\u62a5\u544a\u4ecd\u5177\u6311\u6218\u6027\uff0c\u5c06\u5927\u8bed\u8a00\u6a21\u578b\u4e0e\u7ed3\u6784\u5316\u7684\u8bc4\u5ba1\u77e5\u8bc6\u76f8\u7ed3\u5408\uff0c\u6709\u52a9\u4e8e\u5b9e\u73b0\u66f4\u900f\u660e\u3001\u4e00\u81f4\u7684\u6f0f\u6d1e\u62a5\u544a\u8bc4\u5ba1\u6d41\u7a0b\u3002"}}
{"id": "2511.18625", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2511.18625", "abs": "https://arxiv.org/abs/2511.18625", "authors": ["Wei Wang", "Hourieh Khalajzadeh", "John Grundy", "Anuradha Madugalla", "Humphrey O. Obie"], "title": "Leveraging Discrete Choice Experiments for User-Centric Requirements Prioritization in mHealth Applications", "comment": null, "summary": "Mobile health (mHealth) applications are widely used for chronic disease management, but usability and accessibility challenges persist due to the diverse needs of users. Adaptive User Interfaces (AUIs) offer a personalized solution to enhance user experience, yet barriers to adoption remain. Understanding user preferences and trade-offs is essential to ensure widespread acceptance of adaptation designs. This study identifies key factors influencing user preferences and trade-offs in mHealth adaptation design. A Discrete Choice Experiment (DCE) was conducted with 186 participants who have chronic diseases and use mHealth applications. Participants were asked to select preferred adaptation designs from choices featuring six attributes with varying levels. A mixed logit model was used to analyze preference heterogeneity and determine the factors most likely influencing adoption. Additionally, subgroup analyses were performed to explore differences by age, gender, health conditions, and coping mechanisms. Maintaining usability while ensuring controllability over adaptations, infrequent adaptations, and small-scale changes are key factors that facilitate the adoption of adaptive mHealth app designs. In contrast, frequently used functions and caregiver involvement can diminish the perceived value of such adaptations. This study employs a data-driven approach to quantify user preferences, identify key trade-offs, and reveal variations across demographic and behavioral subgroups through preference heterogeneity modeling. Furthermore, our results offer valuable guidance for developing future adaptive mHealth applications and lay the groundwork for continued exploration into requirements prioritization within the field of software engineering.", "AI": {"tldr": "\u672c\u7814\u7a76\u901a\u8fc7\u79bb\u6563\u9009\u62e9\u5b9e\u9a8c\uff08DCE\uff09\u91cf\u5316\u6162\u6027\u75c5\u60a3\u8005\u5bf9\u79fb\u52a8\u5065\u5eb7\uff08mHealth\uff09\u5e94\u7528\u81ea\u9002\u5e94\u754c\u9762\u8bbe\u8ba1\u7684\u504f\u597d\uff0c\u53d1\u73b0\u53ef\u63a7\u6027\u3001\u4f4e\u9891\u6b21\u548c\u5c0f\u5e45\u5ea6\u7684\u9002\u914d\u66f4\u6613\u88ab\u63a5\u53d7\uff0c\u800c\u5e38\u7528\u529f\u80fd\u548c\u7167\u62a4\u8005\u4ecb\u5165\u53ef\u80fd\u964d\u4f4e\u9002\u914d\u4ef7\u503c\uff0c\u5e76\u63ed\u793a\u4e86\u4e0d\u540c\u4eba\u7fa4\u95f4\u7684\u504f\u597d\u5dee\u5f02\u3002", "motivation": "\u5c3d\u7ba1\u81ea\u9002\u5e94\u7528\u6237\u754c\u9762\uff08AUIs\uff09\u53ef\u63d0\u5347mHealth\u5e94\u7528\u7684\u7528\u6237\u4f53\u9a8c\uff0c\u4f46\u5176\u91c7\u7eb3\u4ecd\u9762\u4e34\u969c\u788d\u3002\u4e3a\u63a8\u52a8\u5e7f\u6cdb\u63a5\u53d7\uff0c\u9700\u6df1\u5165\u7406\u89e3\u7528\u6237\u5728\u9002\u914d\u8bbe\u8ba1\u4e2d\u7684\u504f\u597d\u4e0e\u6743\u8861\u3002", "method": "\u7814\u7a76\u91c7\u7528\u79bb\u6563\u9009\u62e9\u5b9e\u9a8c\uff08DCE\uff09\uff0c\u62db\u52df186\u540d\u60a3\u6709\u6162\u6027\u75c5\u5e76\u4f7f\u7528mHealth\u5e94\u7528\u7684\u53c2\u4e0e\u8005\uff0c\u8bc4\u4f30\u5176\u5728\u516d\u4e2a\u5c5e\u6027\u4e0d\u540c\u6c34\u5e73\u7ec4\u5408\u4e0b\u7684\u9009\u62e9\u504f\u597d\uff1b\u4f7f\u7528\u6df7\u5408Logit\u6a21\u578b\u5206\u6790\u504f\u597d\u5f02\u8d28\u6027\uff0c\u5e76\u6309\u5e74\u9f84\u3001\u6027\u522b\u3001\u5065\u5eb7\u72b6\u51b5\u548c\u5e94\u5bf9\u673a\u5236\u8fdb\u884c\u5b50\u7fa4\u5206\u6790\u3002", "result": "\u7ed3\u679c\u8868\u660e\uff0c\u4fdd\u6301\u53ef\u7528\u6027\u7684\u540c\u65f6\u63d0\u4f9b\u5bf9\u9002\u914d\u7684\u63a7\u5236\u6743\u3001\u8f83\u4f4e\u9891\u7387\u7684\u9002\u914d\u4ee5\u53ca\u5c0f\u5e45\u6539\u52a8\u662f\u4fc3\u8fdb\u91c7\u7eb3\u7684\u5173\u952e\u56e0\u7d20\uff1b\u800c\u9ad8\u9891\u4f7f\u7528\u7684\u529f\u80fd\u548c\u7167\u62a4\u8005\u53c2\u4e0e\u4f1a\u524a\u5f31\u9002\u914d\u7684\u611f\u77e5\u4ef7\u503c\uff1b\u4e0d\u540c\u4eba\u53e3\u7edf\u8ba1\u548c\u884c\u4e3a\u5b50\u7fa4\u4f53\u95f4\u5b58\u5728\u663e\u8457\u504f\u597d\u5dee\u5f02\u3002", "conclusion": "\u8be5\u7814\u7a76\u901a\u8fc7\u6570\u636e\u9a71\u52a8\u65b9\u6cd5\u63ed\u793a\u4e86mHealth\u81ea\u9002\u5e94\u8bbe\u8ba1\u4e2d\u7684\u5173\u952e\u7528\u6237\u504f\u597d\u4e0e\u6743\u8861\uff0c\u4e3a\u672a\u6765\u81ea\u9002\u5e94\u5e94\u7528\u5f00\u53d1\u53ca\u8f6f\u4ef6\u5de5\u7a0b\u4e2d\u7684\u9700\u6c42\u4f18\u5148\u7ea7\u8bbe\u5b9a\u63d0\u4f9b\u4e86\u5b9e\u8bc1\u4f9d\u636e\u548c\u6307\u5bfc\u3002"}}
{"id": "2511.18634", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2511.18634", "abs": "https://arxiv.org/abs/2511.18634", "authors": ["Wei Wang", "Devi Karolita", "Hourieh Khalajzadeh", "John Grundy", "Anuradha Madugalla", "Humphrey O. Obie"], "title": "ChroniUXMag: A Persona-Driven Framework for Inclusive mHealth Requirements Engineering", "comment": null, "summary": "Mobile health (mHealth) applications are increasingly adopted for chronic disease management, yet they face persistent challenges related to accessibility, inclusivity, and sustained engagement. Patients' needs evolve dynamically with their health progression, adherence, and caregiver support, creating unique requirements engineering (RE) challenges that traditional approaches often overlook. This study introduces ChroniUXMag, a framework for eliciting and analysing inclusivity requirements in mHealth design. Building on InclusiveMag and GenderMag principles, the framework aims to help researchers and practitioners systematically capture and evaluate factors that influence how individuals with chronic conditions perceive, trust, and interact with mHealth systems. The framework was developed through two stages of the InclusiveMag process. In the first stage, inclusivity facets were identified through a systematic literature review, focus groups, interviews, and a large-scale survey. In the second stage, these facets were synthesised into personas representing diverse health situations, attitudes, and digital practices, and integrated into an adapted cognitive walkthrough form. Thirteen facets were identified that capture the socio-technical complexity of mHealth use, including trust, digital literacy, dependency, and cultural context. These facets support structured, persona-driven evaluations that reveal inclusivity barriers often missed by traditional usability assessments. ChroniUXMag contributes to RE by offering a reproducible, evidence-based approach for embedding inclusivity into mHealth requirements. Future work will extend the third stage Apply through practitioner-led evaluation in real-world design contexts.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86ChroniUXMag\u6846\u67b6\uff0c\u7528\u4e8e\u5728\u79fb\u52a8\u5065\u5eb7\uff08mHealth\uff09\u5e94\u7528\u7684\u9700\u6c42\u5de5\u7a0b\u4e2d\u7cfb\u7edf\u5730\u8bc6\u522b\u548c\u8bc4\u4f30\u5305\u5bb9\u6027\u9700\u6c42\uff0c\u4ee5\u5e94\u5bf9\u6162\u6027\u75c5\u60a3\u8005\u5728\u4f7f\u7528mHealth\u65f6\u56e0\u4e2a\u4f53\u5dee\u5f02\u800c\u9762\u4e34\u7684\u53ef\u53ca\u6027\u4e0e\u6301\u7eed\u53c2\u4e0e\u6311\u6218\u3002", "motivation": "\u4f20\u7edf\u9700\u6c42\u5de5\u7a0b\u65b9\u6cd5\u5ffd\u89c6\u4e86\u6162\u6027\u75c5\u60a3\u8005\u5728\u5065\u5eb7\u8fdb\u5c55\u3001\u4f9d\u4ece\u6027\u548c\u7167\u62a4\u652f\u6301\u53d8\u5316\u4e0b\u52a8\u6001\u6f14\u5316\u7684\u591a\u6837\u5316\u9700\u6c42\uff0c\u5bfc\u81f4mHealth\u5e94\u7528\u5728\u53ef\u8bbf\u95ee\u6027\u3001\u5305\u5bb9\u6027\u548c\u7528\u6237\u6301\u7eed\u53c2\u4e0e\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\u3002", "method": "\u57fa\u4e8eInclusiveMag\u548cGenderMag\u539f\u5219\uff0c\u901a\u8fc7\u4e24\u9636\u6bb5\u6d41\u7a0b\u5f00\u53d1ChroniUXMag\u6846\u67b6\uff1a\u7b2c\u4e00\u9636\u6bb5\u901a\u8fc7\u6587\u732e\u7efc\u8ff0\u3001\u7126\u70b9\u5c0f\u7ec4\u3001\u8bbf\u8c08\u548c\u5927\u89c4\u6a21\u8c03\u67e5\u8bc6\u522b\u5305\u5bb9\u6027\u7ef4\u5ea6\uff1b\u7b2c\u4e8c\u9636\u6bb5\u5c06\u8fd9\u4e9b\u7ef4\u5ea6\u6574\u5408\u4e3a\u4ee3\u8868\u4e0d\u540c\u5065\u5eb7\u72b6\u51b5\u3001\u6001\u5ea6\u548c\u6570\u5b57\u884c\u4e3a\u7684\u7528\u6237\u753b\u50cf\uff0c\u5e76\u5d4c\u5165\u6539\u826f\u7684\u8ba4\u77e5\u8d70\u67e5\u8868\u4e2d\u3002", "result": "\u7814\u7a76\u8bc6\u522b\u51fa13\u4e2a\u53cd\u6620mHealth\u4f7f\u7528\u793e\u4f1a\u6280\u672f\u590d\u6742\u6027\u7684\u5305\u5bb9\u6027\u7ef4\u5ea6\uff08\u5982\u4fe1\u4efb\u3001\u6570\u5b57\u7d20\u517b\u3001\u4f9d\u8d56\u6027\u548c\u6587\u5316\u80cc\u666f\uff09\uff0c\u5e76\u901a\u8fc7\u753b\u50cf\u9a71\u52a8\u7684\u8bc4\u4f30\u65b9\u6cd5\u63ed\u793a\u4f20\u7edf\u53ef\u7528\u6027\u6d4b\u8bd5\u5e38\u5ffd\u7565\u7684\u5305\u5bb9\u6027\u969c\u788d\u3002", "conclusion": "ChroniUXMag\u4e3amHealth\u9700\u6c42\u5de5\u7a0b\u63d0\u4f9b\u4e86\u4e00\u79cd\u53ef\u590d\u73b0\u3001\u5faa\u8bc1\u7684\u5305\u5bb9\u6027\u5d4c\u5165\u65b9\u6cd5\uff0c\u672a\u6765\u5c06\u5728\u771f\u5b9e\u8bbe\u8ba1\u573a\u666f\u4e2d\u901a\u8fc7\u5b9e\u8df5\u8005\u4e3b\u5bfc\u7684\u8bc4\u4f30\u8fdb\u4e00\u6b65\u9a8c\u8bc1\u5176\u7b2c\u4e09\u9636\u6bb5\u201c\u5e94\u7528\u201d\u3002"}}
{"id": "2511.18782", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2511.18782", "abs": "https://arxiv.org/abs/2511.18782", "authors": ["Lukas Twist"], "title": "Summary-Mediated Repair: Can LLMs use code summarisation as a tool for program repair?", "comment": "6 pages, 3 tables, 1 figure", "summary": "Large Language Models (LLMs) often produce code with subtle implementation-level bugs despite strong benchmark performance. These errors are hard for LLMs to spot and can have large behavioural effects; yet when asked to summarise code, LLMs can frequently surface high-level intent and sometimes overlook this low-level noise. Motivated by this, we propose summary-mediated repair, a prompt-only pipeline for program repair that leverages natural-language code summarisation as an explicit intermediate step, extending previous work that has already shown code summarisation to be a useful intermediary for downstream tasks. We evaluate our method across eight production-grade LLMs on two function level benchmarks (HumanEvalPack and MBPP), comparing several summary styles against a direct repair baseline. Error-aware diagnostic summaries consistently yield the largest gains - repairing up to 65% of unseen errors, on average of 5% more than the baseline - though overall improvements are modest and LLM-dependent. Our results position summaries as a cheap, human-interpretable diagnostic artefact that can be integrated into program-repair pipelines rather than a stand-alone fix-all.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u4ec5\u901a\u8fc7\u63d0\u793a\u5b9e\u73b0\u7684\u7a0b\u5e8f\u4fee\u590d\u65b9\u6cd5\u2014\u2014\u6458\u8981\u4ecb\u5bfc\u4fee\u590d\uff08summary-mediated repair\uff09\uff0c\u5229\u7528\u81ea\u7136\u8bed\u8a00\u4ee3\u7801\u6458\u8981\u4f5c\u4e3a\u4e2d\u95f4\u6b65\u9aa4\uff0c\u5c24\u5176\u91c7\u7528\u9519\u8bef\u611f\u77e5\u578b\u8bca\u65ad\u6458\u8981\uff0c\u5728\u591a\u4e2a\u5927\u8bed\u8a00\u6a21\u578b\u548c\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5e73\u5747\u6bd4\u76f4\u63a5\u4fee\u590d\u57fa\u7ebf\u591a\u4fee\u590d5%\u7684\u9519\u8bef\u3002", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5c3d\u7ba1\u5728\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u751f\u6210\u7684\u4ee3\u7801\u5e38\u5305\u542b\u96be\u4ee5\u5bdf\u89c9\u7684\u5b9e\u73b0\u7ea7\u9519\u8bef\uff1b\u800c\u5b83\u4eec\u5728\u4ee3\u7801\u6458\u8981\u4efb\u52a1\u4e2d\u80fd\u8f83\u597d\u5730\u6355\u6349\u9ad8\u5c42\u610f\u56fe\uff0c\u53ef\u80fd\u5ffd\u7565\u4f4e\u5c42\u9519\u8bef\u3002\u53d7\u6b64\u542f\u53d1\uff0c\u4f5c\u8005\u63a2\u7d22\u5c06\u4ee3\u7801\u6458\u8981\u4f5c\u4e3a\u7a0b\u5e8f\u4fee\u590d\u7684\u4e2d\u95f4\u6b65\u9aa4\u3002", "method": "\u63d0\u51fa\u201c\u6458\u8981\u4ecb\u5bfc\u4fee\u590d\u201d\u6d41\u7a0b\uff1a\u5148\u8ba9LLM\u751f\u6210\u4ee3\u7801\u7684\u81ea\u7136\u8bed\u8a00\u6458\u8981\uff08\u5c24\u5176\u662f\u9519\u8bef\u611f\u77e5\u578b\u8bca\u65ad\u6458\u8981\uff09\uff0c\u518d\u57fa\u4e8e\u8be5\u6458\u8981\u8fdb\u884c\u4fee\u590d\uff0c\u65e0\u9700\u989d\u5916\u8bad\u7ec3\uff0c\u4ec5\u4f9d\u8d56\u63d0\u793a\u5de5\u7a0b\u3002", "result": "\u57288\u4e2a\u751f\u4ea7\u7ea7LLM\u53caHumanEvalPack\u3001MBPP\u4e24\u4e2a\u51fd\u6570\u7ea7\u57fa\u51c6\u4e0a\u8bc4\u4f30\uff0c\u9519\u8bef\u611f\u77e5\u6458\u8981\u5e73\u5747\u6bd4\u76f4\u63a5\u4fee\u590d\u57fa\u7ebf\u591a\u4fee\u590d5%\u7684\u672a\u89c1\u9519\u8bef\uff0c\u6700\u9ad8\u53ef\u4fee\u590d65%\u7684\u9519\u8bef\uff0c\u4f46\u6574\u4f53\u63d0\u5347\u6709\u9650\u4e14\u4f9d\u8d56\u5177\u4f53\u6a21\u578b\u3002", "conclusion": "\u4ee3\u7801\u6458\u8981\u53ef\u4f5c\u4e3a\u5ec9\u4ef7\u3001\u4eba\u7c7b\u53ef\u89e3\u91ca\u7684\u8bca\u65ad\u5de5\u5177\uff0c\u6709\u6548\u96c6\u6210\u5230\u7a0b\u5e8f\u4fee\u590d\u6d41\u7a0b\u4e2d\uff0c\u4f46\u5e76\u975e\u4e07\u80fd\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2511.18842", "categories": ["cs.SE", "cs.AI", "cs.HC"], "pdf": "https://arxiv.org/pdf/2511.18842", "abs": "https://arxiv.org/abs/2511.18842", "authors": ["Mohammad Nour Al Awad", "Sergey Ivanov", "Olga Tikhonova"], "title": "Optimizing LLM Code Suggestions: Feedback-Driven Timing with Lightweight State Bounds", "comment": "\\c{opyright} 2025 IEEE. Personal use of this material is permitted. Permission from IEEE must be obtained for all other uses", "summary": "Large Language Models (LLMs) have transformed code auto-completion by generating context-aware suggestions. Yet, deciding when to present these suggestions remains underexplored, often leading to interruptions or wasted inference calls. We propose an adaptive timing mechanism that dynamically adjusts the delay before offering a suggestion based on real-time developer feedback. Our suggested method combines a logistic transform of recent acceptance rates with a bounded delay range, anchored by a high-level binary prediction of the developer's cognitive state. In a two-month deployment with professional developers, our system improved suggestion acceptance from 4.9% with no delay to 15.4% with static delays, and to 18.6% with adaptive timing-while reducing blind rejections (rejections without being read) from 8.3% to 0.36%. Together, these improvements increase acceptance and substantially reduce wasted inference calls by 75%, making LLM-based code assistants more efficient and cost-effective in practice.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u81ea\u9002\u5e94\u5ef6\u8fdf\u673a\u5236\uff0c\u6839\u636e\u5f00\u53d1\u8005\u5b9e\u65f6\u53cd\u9988\u52a8\u6001\u8c03\u6574\u4ee3\u7801\u8865\u5168\u5efa\u8bae\u7684\u5448\u73b0\u65f6\u673a\uff0c\u5728\u63d0\u5347\u5efa\u8bae\u63a5\u53d7\u7387\u7684\u540c\u65f6\u663e\u8457\u51cf\u5c11\u65e0\u6548\u63a8\u7406\u8c03\u7528\u3002", "motivation": "\u73b0\u6709\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u9a71\u52a8\u7684\u4ee3\u7801\u81ea\u52a8\u8865\u5168\u7cfb\u7edf\u5728\u4f55\u65f6\u5c55\u793a\u5efa\u8bae\u65b9\u9762\u7f3a\u4e4f\u6709\u6548\u7b56\u7565\uff0c\u5e38\u5bfc\u81f4\u6253\u65ad\u5f00\u53d1\u8005\u6216\u6d6a\u8d39\u8ba1\u7b97\u8d44\u6e90\u3002", "method": "\u7ed3\u5408\u8fd1\u671f\u5efa\u8bae\u63a5\u53d7\u7387\u7684\u903b\u8f91\u53d8\u6362\u3001\u6709\u754c\u5ef6\u8fdf\u8303\u56f4\u4ee5\u53ca\u5bf9\u5f00\u53d1\u8005\u8ba4\u77e5\u72b6\u6001\u7684\u9ad8\u5c42\u4e8c\u5143\u9884\u6d4b\uff0c\u6784\u5efa\u81ea\u9002\u5e94\u5ef6\u8fdf\u673a\u5236\u3002", "result": "\u5728\u4e3a\u671f\u4e24\u4e2a\u6708\u7684\u771f\u5b9e\u90e8\u7f72\u4e2d\uff0c\u5efa\u8bae\u63a5\u53d7\u7387\u4ece\u65e0\u5ef6\u8fdf\u65f6\u76844.9%\u63d0\u5347\u81f3\u81ea\u9002\u5e94\u5ef6\u8fdf\u4e0b\u768418.6%\uff0c\u540c\u65f6\u201c\u672a\u8bfb\u5373\u62d2\u201d\u6bd4\u4f8b\u4ece8.3%\u964d\u81f30.36%\uff0c\u65e0\u6548\u63a8\u7406\u8c03\u7528\u51cf\u5c1175%\u3002", "conclusion": "\u6240\u63d0\u81ea\u9002\u5e94\u65f6\u673a\u673a\u5236\u663e\u8457\u63d0\u5347\u4e86LLM\u4ee3\u7801\u52a9\u624b\u7684\u6548\u7387\u4e0e\u6210\u672c\u6548\u76ca\uff0c\u517c\u987e\u7528\u6237\u4f53\u9a8c\u4e0e\u7cfb\u7edf\u8d44\u6e90\u5229\u7528\u3002"}}
{"id": "2511.18849", "categories": ["cs.SE", "cs.AI", "cs.HC"], "pdf": "https://arxiv.org/pdf/2511.18849", "abs": "https://arxiv.org/abs/2511.18849", "authors": ["Mohammad Nour Al Awad", "Sergey Ivanov", "Olga Tikhonova"], "title": "Pre-Filtering Code Suggestions using Developer Behavioral Telemetry to Optimize LLM-Assisted Programming", "comment": "\\c{opyright} 2025 IEEE. Personal use of this material is permitted. Permission from IEEE must be obtained for all other uses", "summary": "Large Language Models (LLMs) are increasingly integrated into code editors to provide AI-powered code suggestions. Yet many of these suggestions are ignored, resulting in wasted computation, increased latency, and unnecessary interruptions. We introduce a lightweight pre-filtering model that predicts the likelihood of suggestion acceptance before invoking the LLM, using only real-time developer telemetry such as typing speed, file navigation, and editing activity. Deployed in a production-grade Visual Studio Code plugin over four months of naturalistic use, our approach nearly doubled acceptance rates (18.4% -> 34.2%) while suppressing 35% of low-value LLM calls. These findings demonstrate that behavioral signals alone can meaningfully improve both user experience and system efficiency in LLM-assisted programming, highlighting the value of timing-aware, privacy-preserving adaptation mechanisms. The filter operates solely on pre-invocation editor telemetry and never inspects code or prompts.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u8f7b\u91cf\u7ea7\u9884\u8fc7\u6ee4\u6a21\u578b\uff0c\u5229\u7528\u5f00\u53d1\u8005\u5b9e\u65f6\u884c\u4e3a\u4fe1\u53f7\uff08\u5982\u6253\u5b57\u901f\u5ea6\u3001\u6587\u4ef6\u5bfc\u822a\u548c\u7f16\u8f91\u6d3b\u52a8\uff09\u9884\u6d4b\u4ee3\u7801\u5efa\u8bae\u7684\u63a5\u53d7\u6982\u7387\uff0c\u5728\u4e0d\u67e5\u770b\u4ee3\u7801\u6216\u63d0\u793a\u5185\u5bb9\u7684\u524d\u63d0\u4e0b\uff0c\u663e\u8457\u63d0\u5347\u5927\u8bed\u8a00\u6a21\u578b\u4ee3\u7801\u5efa\u8bae\u7684\u63a5\u53d7\u7387\u5e76\u51cf\u5c11\u65e0\u6548\u8c03\u7528\u3002", "motivation": "\u5f53\u524d\u96c6\u6210\u5728\u4ee3\u7801\u7f16\u8f91\u5668\u4e2d\u7684\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u751f\u6210\u7684\u4ee3\u7801\u5efa\u8bae\u5e38\u88ab\u7528\u6237\u5ffd\u7565\uff0c\u9020\u6210\u8ba1\u7b97\u8d44\u6e90\u6d6a\u8d39\u3001\u5ef6\u8fdf\u589e\u52a0\u548c\u4e0d\u5fc5\u8981\u7684\u5e72\u6270\uff0c\u4e9f\u9700\u4e00\u79cd\u9ad8\u6548\u673a\u5236\u63d0\u524d\u5224\u65ad\u5efa\u8bae\u662f\u5426\u503c\u5f97\u751f\u6210\u3002", "method": "\u5f00\u53d1\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u9884\u8fc7\u6ee4\u6a21\u578b\uff0c\u4ec5\u57fa\u4e8e\u8c03\u7528LLM\u524d\u7684\u5b9e\u65f6\u7f16\u8f91\u5668\u9065\u6d4b\u6570\u636e\uff08\u5982\u6253\u5b57\u901f\u5ea6\u3001\u6587\u4ef6\u5bfc\u822a\u548c\u7f16\u8f91\u6d3b\u52a8\uff09\u9884\u6d4b\u5efa\u8bae\u88ab\u63a5\u53d7\u7684\u53ef\u80fd\u6027\uff0c\u5e76\u5728Visual Studio Code\u63d2\u4ef6\u4e2d\u90e8\u7f72\u9a8c\u8bc1\u3002", "result": "\u5728\u4e3a\u671f\u56db\u4e2a\u6708\u7684\u771f\u5b9e\u4f7f\u7528\u73af\u5883\u4e2d\uff0c\u8be5\u65b9\u6cd5\u5c06\u4ee3\u7801\u5efa\u8bae\u63a5\u53d7\u7387\u4ece18.4%\u63d0\u5347\u81f334.2%\uff0c\u540c\u65f6\u51cf\u5c11\u4e8635%\u7684\u4f4e\u4ef7\u503cLLM\u8c03\u7528\u3002", "conclusion": "\u4ec5\u4f9d\u9760\u5f00\u53d1\u8005\u884c\u4e3a\u4fe1\u53f7\u5373\u53ef\u6709\u6548\u63d0\u5347LLM\u8f85\u52a9\u7f16\u7a0b\u7684\u7528\u6237\u4f53\u9a8c\u4e0e\u7cfb\u7edf\u6548\u7387\uff0c\u8868\u660e\u65f6\u673a\u611f\u77e5\u4e14\u4fdd\u62a4\u9690\u79c1\u7684\u9002\u914d\u673a\u5236\u5177\u6709\u91cd\u8981\u4ef7\u503c\u3002"}}
{"id": "2511.18854", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.18854", "abs": "https://arxiv.org/abs/2511.18854", "authors": ["Yujing Wang", "Weize Hong"], "title": "Time Travel: LLM-Assisted Semantic Behavior Localization with Git Bisect", "comment": "submitted to Git Bisect SCALCOM 2025 Calgary (to be published)", "summary": "We present a novel framework that integrates Large Language Models (LLMs) into the Git bisect process for semantic fault localization. Traditional bisect assumes deterministic predicates and binary failure states assumptions often violated in modern software development due to flaky tests, nonmonotonic regressions, and semantic divergence from upstream repositories. Our system augments bisect traversal with structured chain of thought reasoning, enabling commit by commit analysis under noisy conditions. We evaluate multiple open source and proprietary LLMs for their suitability and fine tune DeepSeekCoderV2 using QLoRA on a curated dataset of semantically labeled diffs. We adopt a weak supervision workflow to reduce annotation overhead, incorporating human in the loop corrections and self consistency filtering. Experiments across multiple open source projects show a 6.4 point absolute gain in success rate from 74.2 to 80.6 percent, leading to significantly fewer failed traversals and by experiment up to 2x reduction in average bisect time. We conclude with discussions on temporal reasoning, prompt design, and finetuning strategies tailored for commit level behavior analysis.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5c06\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u878d\u5165 Git bisect \u8fc7\u7a0b\u7684\u65b0\u6846\u67b6\uff0c\u7528\u4e8e\u8bed\u4e49\u6545\u969c\u5b9a\u4f4d\uff0c\u5728\u591a\u4e2a\u5f00\u6e90\u9879\u76ee\u4e2d\u663e\u8457\u63d0\u5347\u4e86\u6210\u529f\u7387\u5e76\u51cf\u5c11\u4e86\u5e73\u5747 bisect \u65f6\u95f4\u3002", "motivation": "\u4f20\u7edf Git bisect \u65b9\u6cd5\u4f9d\u8d56\u786e\u5b9a\u6027\u8c13\u8bcd\u548c\u4e8c\u5143\u5931\u8d25\u72b6\u6001\uff0c\u4f46\u5728\u73b0\u4ee3\u8f6f\u4ef6\u5f00\u53d1\u4e2d\u5e38\u56e0\u6d4b\u8bd5\u4e0d\u7a33\u5b9a\u3001\u975e\u5355\u8c03\u56de\u5f52\u548c\u8bed\u4e49\u5206\u6b67\u800c\u5931\u6548\uff0c\u56e0\u6b64\u9700\u8981\u66f4\u9c81\u68d2\u7684\u8bed\u4e49\u6545\u969c\u5b9a\u4f4d\u65b9\u6cd5\u3002", "method": "\u901a\u8fc7\u7ed3\u6784\u5316\u601d\u7ef4\u94fe\u63a8\u7406\u589e\u5f3a bisect \u904d\u5386\u8fc7\u7a0b\uff0c\u4f7f\u7528\u5f31\u76d1\u7763\u6d41\u7a0b\u51cf\u5c11\u6807\u6ce8\u5f00\u9500\uff0c\u5e76\u57fa\u4e8e\u4eba\u5de5\u6821\u6b63\u4e0e\u81ea\u4e00\u81f4\u6027\u8fc7\u6ee4\u5bf9 DeepSeekCoderV2 \u6a21\u578b\u8fdb\u884c QLoRA \u5fae\u8c03\u3002", "result": "\u5728\u591a\u4e2a\u5f00\u6e90\u9879\u76ee\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5c06 bisect \u6210\u529f\u7387\u4ece 74.2% \u63d0\u5347\u81f3 80.6%\uff0c\u6700\u591a\u53ef\u5c06\u5e73\u5747 bisect \u65f6\u95f4\u51cf\u5c11\u4e00\u534a\u3002", "conclusion": "\u8be5\u7814\u7a76\u9a8c\u8bc1\u4e86 LLM \u5728 commit \u7ea7\u522b\u884c\u4e3a\u5206\u6790\u4e2d\u7684\u6709\u6548\u6027\uff0c\u5e76\u8ba8\u8bba\u4e86\u9002\u7528\u4e8e\u6b64\u4efb\u52a1\u7684\u65f6\u95f4\u63a8\u7406\u3001\u63d0\u793a\u8bbe\u8ba1\u548c\u5fae\u8c03\u7b56\u7565\u3002"}}
{"id": "2511.18484", "categories": ["cs.NI"], "pdf": "https://arxiv.org/pdf/2511.18484", "abs": "https://arxiv.org/abs/2511.18484", "authors": ["Weiwei Chen", "Huaxuan Xiao", "Jiefeng Zhang", "Xianjin Xia", "Shuai Wang", "Xianjun Deng", "Dan Zeng"], "title": "SFusion: Energy and Coding Fusion for Ultra-Robust Low-SNR LoRa Networks", "comment": null, "summary": "LoRa has become a cornerstone for city-wide IoT applications due to its long-range, low-power communication. It achieves extended transmission by spreading symbols over multiple samples, with redundancy controlled by the Spreading Factor (SF), and further error resilience provided by Forward Error Correction (FEC). However, practical limits on SF and the separation between signal-level demodulation and coding-level error correction in conventional LoRa PHY leave it vulnerable under extremely weak signals - common in city-scale deployments. To address this, we present SFusion, a software-based coding framework that jointly leverages signal-level aggregation and coding-level redundancy to enhance LoRa's robustness. When signals fall below the decodable threshold, SFusion encodes a quasi-SF(k +m) symbol using 2^m SFk symbols to boost processing gain through energy accumulation. Once partial decoding becomes feasible with energy aggregation, an opportunistic decoding strategy directly combines IQ signals across symbols to recover errors. Extensive evaluations show that SFusion achieves up to 15dB gain over SF12 and up to 13dB improvement over state-of-the-art solutions.", "AI": {"tldr": "SFusion \u662f\u4e00\u79cd\u65b0\u578b\u8f6f\u4ef6\u7f16\u7801\u6846\u67b6\uff0c\u901a\u8fc7\u8054\u5408\u5229\u7528\u4fe1\u53f7\u7ea7\u805a\u5408\u4e0e\u7f16\u7801\u7ea7\u5197\u4f59\uff0c\u663e\u8457\u63d0\u5347 LoRa \u5728\u6781\u5f31\u4fe1\u53f7\u4e0b\u7684\u89e3\u7801\u80fd\u529b\uff0c\u76f8\u6bd4 SF12 \u6700\u591a\u83b7\u5f97 15dB \u589e\u76ca\u3002", "motivation": "\u4f20\u7edf LoRa \u7269\u7406\u5c42\u5728\u4fe1\u53f7\u7ea7\u89e3\u8c03\u4e0e\u7f16\u7801\u7ea7\u7ea0\u9519\u4e4b\u95f4\u5b58\u5728\u5206\u79bb\uff0c\u4e14\u6269\u9891\u56e0\u5b50\uff08SF\uff09\u53d7\u9650\uff0c\u5bfc\u81f4\u5728\u57ce\u5e02\u7ea7\u90e8\u7f72\u4e2d\u9762\u5bf9\u6781\u5f31\u4fe1\u53f7\u65f6\u9c81\u68d2\u6027\u4e0d\u8db3\u3002", "method": "SFusion \u5728\u4fe1\u53f7\u4f4e\u4e8e\u53ef\u89e3\u7801\u9608\u503c\u65f6\uff0c\u4f7f\u7528 2^m \u4e2a SFk \u7b26\u53f7\u6784\u9020\u7b49\u6548\u7684\u51c6 SF(k+m) \u7b26\u53f7\u4ee5\u7d2f\u79ef\u80fd\u91cf\uff1b\u5f53\u90e8\u5206\u89e3\u7801\u53ef\u884c\u65f6\uff0c\u91c7\u7528\u673a\u4f1a\u5f0f\u89e3\u7801\u7b56\u7565\u76f4\u63a5\u8de8\u7b26\u53f7\u878d\u5408 IQ \u4fe1\u53f7\u8fdb\u884c\u7ea0\u9519\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cSFusion \u76f8\u6bd4 SF12 \u6700\u591a\u63d0\u5347 15dB\uff0c\u6bd4\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6848\u6700\u591a\u63d0\u5347 13dB\u3002", "conclusion": "SFusion \u6709\u6548\u589e\u5f3a\u4e86 LoRa \u5728\u57ce\u5e02\u7ea7\u7269\u8054\u7f51\u5e94\u7528\u4e2d\u7684\u901a\u4fe1\u9c81\u68d2\u6027\uff0c\u5c24\u5176\u9002\u7528\u4e8e\u6781\u5f31\u4fe1\u53f7\u73af\u5883\u3002"}}
{"id": "2511.18867", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2511.18867", "abs": "https://arxiv.org/abs/2511.18867", "authors": ["Liutong Han", "Chu Kang", "Mingjie Xing", "Yanjun Wu"], "title": "VecIntrinBench: Benchmarking Cross-Architecture Intrinsic Code Migration for RISC-V Vector", "comment": "5 pages, 7 figures", "summary": "Intrinsic functions are specialized functions provided by the compiler that efficiently operate on architecture-specific hardware, allowing programmers to write optimized code in a high-level language that fully exploits hardware features. Using intrinsics to vectorize core code blocks is a standard optimization method in high-performance libraries, often requiring specific vector optimization implementations for multiple mainstream architectures. The promising RISC-V software ecosystem has a significant demand for algorithm library migration and adaptation. Translating existing intrinsic functions to RISC-V Vector (RVV) intrinsic functions across architectures is currently a mainstream approach. Rule-based intrinsic mapping methods and LLM-based code generation can help developers address the code migration challenge. However, existing intrinsic code benchmarks focus on mainstream SIMD intrinsics and lack support for the emerging RISC-V architecture. There is currently no benchmark that comprehensively evaluates the intrinsic migration capabilities for the RVV extension. To fill this gap, we propose VecIntrinBench, the first intrinsic benchmark encompassing RVV extensions. It includes 50 function-level tasks from open source repositories, implemented as scalars, RVV intrinsics, Arm Neon intrinsics, and x86 intrinsics, along with comprehensive functional and performance test cases. We systematically evaluated various code migration approaches on VecIntrinBench, yielding a series of insightful findings. The results demonstrate that advanced Large Language Models (LLMs) achieve a similar effect as rule-based mapping approaches for RISC-V code migration, while also delivering superior performance. We further analyze the reasons and identify future directions for LLM development in the code migration field. The VecIntrinBench is open-sourced to benefit the broader community and developers.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86VecIntrinBench\uff0c\u8fd9\u662f\u9996\u4e2a\u652f\u6301RISC-V Vector\uff08RVV\uff09\u6269\u5c55\u7684\u5185\u5efa\u51fd\u6570\u57fa\u51c6\u6d4b\u8bd5\u96c6\uff0c\u7528\u4e8e\u8bc4\u4f30\u8de8\u67b6\u6784\u5185\u5efa\u51fd\u6570\u8fc1\u79fb\u80fd\u529b\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u6bd4\u8f83\u4e86\u57fa\u4e8e\u89c4\u5219\u548c\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u8fc1\u79fb\u65b9\u6cd5\uff0c\u53d1\u73b0\u5148\u8fdbLLM\u5728\u6548\u679c\u548c\u6027\u80fd\u4e0a\u5177\u6709\u4f18\u52bf\u3002", "motivation": "\u5f53\u524d\u7f3a\u4e4f\u9488\u5bf9RISC-V Vector\uff08RVV\uff09\u67b6\u6784\u7684\u5185\u5efa\u51fd\u6570\u8fc1\u79fb\u80fd\u529b\u8bc4\u4f30\u57fa\u51c6\uff0c\u963b\u788d\u4e86\u9ad8\u6027\u80fd\u7b97\u6cd5\u5e93\u5728RISC-V\u751f\u6001\u4e2d\u7684\u8fc1\u79fb\u4e0e\u9002\u914d\u3002", "method": "\u6784\u5efa\u5305\u542b50\u4e2a\u51fd\u6570\u7ea7\u4efb\u52a1\u7684\u57fa\u51c6\u6d4b\u8bd5\u96c6VecIntrinBench\uff0c\u6db5\u76d6\u6807\u91cf\u3001RVV\u3001Arm Neon\u548cx86\u5185\u5efa\u51fd\u6570\u5b9e\u73b0\uff0c\u5e76\u8bbe\u8ba1\u529f\u80fd\u4e0e\u6027\u80fd\u6d4b\u8bd5\u7528\u4f8b\uff1b\u5728\u6b64\u57fa\u7840\u4e0a\u7cfb\u7edf\u8bc4\u4f30\u57fa\u4e8e\u89c4\u5219\u548c\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u4ee3\u7801\u8fc1\u79fb\u65b9\u6cd5\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u5148\u8fdb\u7684\u5927\u8bed\u8a00\u6a21\u578b\u5728RISC-V\u5185\u5efa\u51fd\u6570\u8fc1\u79fb\u4efb\u52a1\u4e2d\u80fd\u8fbe\u5230\u4e0e\u57fa\u4e8e\u89c4\u5219\u65b9\u6cd5\u76f8\u5f53\u7684\u6b63\u786e\u6027\uff0c\u5e76\u5728\u6027\u80fd\u65b9\u9762\u8868\u73b0\u66f4\u4f18\uff1b\u540c\u65f6\u63ed\u793a\u4e86LLM\u5728\u8be5\u9886\u57df\u7684\u6539\u8fdb\u65b9\u5411\u3002", "conclusion": "VecIntrinBench\u586b\u8865\u4e86RVV\u5185\u5efa\u51fd\u6570\u8fc1\u79fb\u8bc4\u4f30\u7684\u7a7a\u767d\uff0c\u4e3a\u793e\u533a\u63d0\u4f9b\u4e86\u6709\u6548\u5de5\u5177\uff1b\u7814\u7a76\u7ed3\u679c\u652f\u6301LLM\u5728\u8de8\u67b6\u6784\u4ee3\u7801\u8fc1\u79fb\u4e2d\u7684\u5e94\u7528\u6f5c\u529b\uff0c\u5e76\u4e3a\u672a\u6765\u7814\u7a76\u6307\u660e\u65b9\u5411\u3002"}}
{"id": "2511.18918", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2511.18918", "abs": "https://arxiv.org/abs/2511.18918", "authors": ["Qingchao Shen", "Zan Wang", "Haoyang Ma", "Yongqiang Tian", "Lili Huang", "Zibo Xiao", "Junjie Chen", "Shing-Chi Cheung"], "title": "Optimization-Aware Test Generation for Deep Learning Compilers", "comment": "This paper has been accpected by ICSE 2026", "summary": "Deep Learning (DL) compilers have been widely utilized to optimize DL models for efficient deployment across various hardware. Due to their vital role in the DL ecosystem, ensuring their reliability and security is critical. However, existing approaches have limitations in testing optimization stages, which is the core functionality of DL compilers, due to the difficulty in generating optimization-aware tests. In this paper, we proposed OATest, a novel approach for synthesizing optimization-aware computational graphs. The approach combines patterns extracted from documented tests for optimization and incorporates them into seed computational graphs, enabling broader exploration of optimization paths. To guarantee the optimization-awareness of generated graphs, OATest introduces the edges reusing strategy to establish strong connections between patterns and contexts. Additionally, to solve the validity challenge for the generated graphs, OATest employs an auxiliary layers addition strategy to resolve broken constraints. Equipped with two distinct test oracles, OATest applies differential testing to evaluate the two widely used DL compilers (i.e., TVM and ONNXRuntime). Our experimental results show that OATest outperforms the state-of-the-art method by detecting more bugs and achieving higher code coverage in TVM and ONNXRutimes. Additionally, OATest uncovers 58 previously unknown bugs, 36 of which have been confirmed or fixed by developers.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faOATest\uff0c\u4e00\u79cd\u7528\u4e8e\u751f\u6210\u9762\u5411\u4f18\u5316\u7684\u8ba1\u7b97\u56fe\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u7ed3\u5408\u6587\u6863\u6d4b\u8bd5\u4e2d\u7684\u6a21\u5f0f\u4e0e\u79cd\u5b50\u56fe\uff0c\u5e76\u91c7\u7528\u8fb9\u91cd\u7528\u548c\u8f85\u52a9\u5c42\u6dfb\u52a0\u7b56\u7565\uff0c\u6709\u6548\u63d0\u5347\u5bf9\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\u5668\uff08\u5982TVM\u548cONNXRuntime\uff09\u7684\u6d4b\u8bd5\u8986\u76d6\u4e0e\u7f3a\u9677\u68c0\u6d4b\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u6d4b\u8bd5\u65b9\u6cd5\u96be\u4ee5\u6709\u6548\u8986\u76d6\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\u5668\u7684\u6838\u5fc3\u4f18\u5316\u9636\u6bb5\uff0c\u56e0\u5176\u7f3a\u4e4f\u751f\u6210\u201c\u4f18\u5316\u611f\u77e5\u201d\u6d4b\u8bd5\u7528\u4f8b\u7684\u80fd\u529b\uff0c\u4ece\u800c\u5f71\u54cd\u7f16\u8bd1\u5668\u7684\u53ef\u9760\u6027\u4e0e\u5b89\u5168\u6027\u9a8c\u8bc1\u3002", "method": "OATest\u4ece\u5df2\u6709\u6587\u6863\u5316\u6d4b\u8bd5\u4e2d\u63d0\u53d6\u4f18\u5316\u6a21\u5f0f\uff0c\u5c06\u5176\u878d\u5408\u5230\u79cd\u5b50\u8ba1\u7b97\u56fe\u4e2d\uff1b\u901a\u8fc7\u8fb9\u91cd\u7528\u7b56\u7565\u589e\u5f3a\u6a21\u5f0f\u4e0e\u4e0a\u4e0b\u6587\u7684\u5173\u8054\u4ee5\u786e\u4fdd\u4f18\u5316\u611f\u77e5\u6027\uff0c\u5e76\u91c7\u7528\u8f85\u52a9\u5c42\u6dfb\u52a0\u7b56\u7565\u89e3\u51b3\u751f\u6210\u56fe\u7684\u6709\u6548\u6027\u95ee\u9898\uff1b\u6700\u540e\u5229\u7528\u4e24\u4e2a\u5dee\u5f02\u6d4b\u8bd5\u9884\u8a00\u5bf9TVM\u548cONNXRuntime\u8fdb\u884c\u8bc4\u4f30\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cOATest\u5728\u4ee3\u7801\u8986\u76d6\u7387\u548c\u7f3a\u9677\u53d1\u73b0\u6570\u91cf\u4e0a\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5\uff0c\u5728TVM\u548cONNXRuntime\u4e2d\u53d1\u73b0\u4e8658\u4e2a\u672a\u77e5\u6f0f\u6d1e\uff0c\u5176\u4e2d36\u4e2a\u5df2\u88ab\u5f00\u53d1\u8005\u786e\u8ba4\u6216\u4fee\u590d\u3002", "conclusion": "OATest\u80fd\u6709\u6548\u751f\u6210\u4f18\u5316\u611f\u77e5\u7684\u8ba1\u7b97\u56fe\uff0c\u663e\u8457\u63d0\u5347\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\u5668\u6d4b\u8bd5\u7684\u5e7f\u5ea6\u4e0e\u6df1\u5ea6\uff0c\u4e3a\u4fdd\u969c\u5176\u53ef\u9760\u6027\u4e0e\u5b89\u5168\u6027\u63d0\u4f9b\u4e86\u6709\u529b\u652f\u6301\u3002"}}
{"id": "2511.18924", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.18924", "abs": "https://arxiv.org/abs/2511.18924", "authors": ["Arina Kharlamova", "Jiawen Liu", "Tianyi Zhang", "Xinrui Yang", "Humaid Alqasimi", "Youcheng Sun", "Chun Jason Xue"], "title": "LLM-Driven Kernel Evolution: Automating Driver Updates in Linux", "comment": null, "summary": "Linux kernel evolution breaks drivers through API/ABI changes, semantic shifts, and security-hardening updates. We introduce DRIVEBENCH, an executable corpus of kernel$\\rightarrow$driver co-evolution cases, and AUTODRIVER, a closed-loop, LLM-driven system for automating driver maintenance. The system integrates prompt engineering, multi-agent collaboration, static analysis, and iterative validation to ensure that generated patches are not only syntactically correct but also functionally and semantically consistent with kernel conventions. The corpus spans v5.10-v6.10 with 235 validated cases drawn from 612 candidates. In evaluation across 55 cases, AUTODRIVER achieves 56.4% compilation success; QEMU-based boot verification indicates that compiled patches preserve driver initialization in most instances. By releasing DRIVEBENCH and tooling, we enable reproducible research and a practical route to continuous, safe co-evolution of drivers with the Linux kernel.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86DRIVEBENCH\uff08\u4e00\u4e2a\u5305\u542bLinux\u5185\u6838\u4e0e\u9a71\u52a8\u534f\u540c\u6f14\u5316\u6848\u4f8b\u7684\u53ef\u6267\u884c\u8bed\u6599\u5e93\uff09\u548cAUTODRIVER\uff08\u4e00\u4e2a\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u95ed\u73af\u9a71\u52a8\u7ef4\u62a4\u81ea\u52a8\u5316\u7cfb\u7edf\uff09\uff0c\u901a\u8fc7\u63d0\u793a\u5de5\u7a0b\u3001\u591a\u667a\u80fd\u4f53\u534f\u4f5c\u3001\u9759\u6001\u5206\u6790\u548c\u8fed\u4ee3\u9a8c\u8bc1\uff0c\u5b9e\u73b0\u5bf9\u56e0\u5185\u6838\u66f4\u65b0\u800c\u5931\u6548\u7684\u9a71\u52a8\u7a0b\u5e8f\u7684\u81ea\u52a8\u4fee\u590d\u3002", "motivation": "Linux\u5185\u6838\u7684\u6301\u7eed\u6f14\u8fdb\u901a\u8fc7API/ABI\u53d8\u66f4\u3001\u8bed\u4e49\u53d8\u5316\u548c\u5b89\u5168\u5f3a\u5316\u66f4\u65b0\u5bfc\u81f4\u9a71\u52a8\u7a0b\u5e8f\u5931\u6548\uff0c\u4e9f\u9700\u81ea\u52a8\u5316\u624b\u6bb5\u652f\u6301\u9a71\u52a8\u7684\u6301\u7eed\u7ef4\u62a4\u4e0e\u534f\u540c\u6f14\u5316\u3002", "method": "\u6784\u5efa\u5305\u542b235\u4e2a\u5df2\u9a8c\u8bc1\u6848\u4f8b\u7684DRIVEBENCH\u8bed\u6599\u5e93\uff08\u8986\u76d6\u5185\u6838v5.10\u2013v6.10\uff09\uff0c\u5e76\u5f00\u53d1AUTODRIVER\u7cfb\u7edf\uff0c\u8be5\u7cfb\u7edf\u7ed3\u5408\u5927\u8bed\u8a00\u6a21\u578b\u3001\u63d0\u793a\u5de5\u7a0b\u3001\u591a\u667a\u80fd\u4f53\u534f\u4f5c\u3001\u9759\u6001\u5206\u6790\u548c\u8fed\u4ee3\u9a8c\u8bc1\uff0c\u81ea\u52a8\u751f\u6210\u7b26\u5408\u5185\u6838\u89c4\u8303\u7684\u9a71\u52a8\u8865\u4e01\u3002", "result": "\u572855\u4e2a\u6d4b\u8bd5\u6848\u4f8b\u4e2d\uff0cAUTODRIVER\u8fbe\u523056.4%\u7684\u7f16\u8bd1\u6210\u529f\u7387\uff1b\u57fa\u4e8eQEMU\u7684\u542f\u52a8\u9a8c\u8bc1\u8868\u660e\uff0c\u5927\u591a\u6570\u6210\u529f\u7f16\u8bd1\u7684\u8865\u4e01\u80fd\u6b63\u786e\u4fdd\u7559\u9a71\u52a8\u521d\u59cb\u5316\u529f\u80fd\u3002", "conclusion": "DRIVEBENCH\u548cAUTODRIVER\u4e3a\u9a71\u52a8\u7a0b\u5e8f\u7684\u81ea\u52a8\u5316\u7ef4\u62a4\u63d0\u4f9b\u4e86\u53ef\u590d\u73b0\u7684\u7814\u7a76\u57fa\u7840\u548c\u5b9e\u7528\u5de5\u5177\uff0c\u6709\u52a9\u4e8e\u5b9e\u73b0\u9a71\u52a8\u4e0eLinux\u5185\u6838\u7684\u5b89\u5168\u3001\u6301\u7eed\u534f\u540c\u6f14\u5316\u3002"}}
{"id": "2511.19059", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2511.19059", "abs": "https://arxiv.org/abs/2511.19059", "authors": ["Pei Liu", "Terry Zhuo", "Jiawei Deng", "Thong James", "Shidong Pan", "Sherry Xu", "Zhenchang Xing", "Qinghua Lu", "Xiaoning Du", "Hongyu Zhang"], "title": "LLMAID: Identifying AI Capabilities in Android Apps with LLMs", "comment": null, "summary": "Recent advancements in artificial intelligence (AI) and its widespread integration into mobile software applications have received significant attention, highlighting the growing prominence of AI capabilities in modern software systems. However, the inherent hallucination and reliability issues of AI continue to raise persistent concerns. Consequently, application users and regulators increasingly ask critical questions such as: Does the application incorporate AI capabilities? and What specific types of AI functionalities are embedded? Preliminary efforts have been made to identify AI capabilities in mobile software; however, existing approaches mainly rely on manual inspection and rule-based heuristics. These methods are not only costly and time-consuming but also struggle to adapt advanced AI techniques.\n  To address the limitations of existing methods, we propose LLMAID (Large Language Model for AI Discovery). LLMAID includes four main tasks: (1) candidate extraction, (2) knowledge base interaction, (3) AI capability analysis and detection, and (4) AI service summarization. We apply LLMAID to a dataset of 4,201 Android applications and demonstrate that it identifies 242% more real-world AI apps than state-of-the-art rule-based approaches. Our experiments show that LLM4AID achieves high precision and recall, both exceeding 90%, in detecting AI-related components. Additionally, a user study indicates that developers find the AI service summaries generated by LLMAID to be more informative and preferable to the original app descriptions. Finally, we leverage LLMAID to perform an empirical analysis of AI capabilities across Android apps. The results reveal a strong concentration of AI functionality in computer vision (54.80%), with object detection emerging as the most common task (25.19%).", "AI": {"tldr": "\u672c\u6587\u63d0\u51faLLMAID\uff0c\u4e00\u79cd\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u81ea\u52a8\u5316\u65b9\u6cd5\uff0c\u7528\u4e8e\u9ad8\u6548\u8bc6\u522b\u548c\u603b\u7ed3\u79fb\u52a8\u5e94\u7528\u4e2d\u7684AI\u529f\u80fd\uff0c\u5728\u51c6\u786e\u7387\u548c\u53ec\u56de\u7387\u4e0a\u5747\u8d85\u8fc790%\uff0c\u5e76\u53d1\u73b0\u8ba1\u7b97\u673a\u89c6\u89c9\u662f\u5f53\u524dAndroid\u5e94\u7528\u4e2d\u6700\u4e3b\u8981\u7684AI\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u8bc6\u522b\u79fb\u52a8\u5e94\u7528\u4e2dAI\u80fd\u529b\u7684\u65b9\u6cd5\u4f9d\u8d56\u4eba\u5de5\u68c0\u67e5\u548c\u57fa\u4e8e\u89c4\u5219\u7684\u542f\u53d1\u5f0f\u65b9\u6cd5\uff0c\u6210\u672c\u9ad8\u3001\u8017\u65f6\u957f\u4e14\u96be\u4ee5\u9002\u5e94\u5148\u8fdbAI\u6280\u672f\uff0c\u4e9f\u9700\u66f4\u9ad8\u6548\u3001\u81ea\u52a8\u5316\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u63d0\u51faLLMAID\u6846\u67b6\uff0c\u5305\u542b\u56db\u4e2a\u4efb\u52a1\uff1a\u5019\u9009\u63d0\u53d6\u3001\u77e5\u8bc6\u5e93\u4ea4\u4e92\u3001AI\u80fd\u529b\u5206\u6790\u4e0e\u68c0\u6d4b\u3001AI\u670d\u52a1\u6458\u8981\u751f\u6210\uff0c\u5e76\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\u5b9e\u73b0\u5bf9Android\u5e94\u7528\u4e2dAI\u7ec4\u4ef6\u7684\u81ea\u52a8\u8bc6\u522b\u4e0e\u603b\u7ed3\u3002", "result": "\u57284,201\u4e2aAndroid\u5e94\u7528\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cLLMAID\u6bd4\u73b0\u6709\u57fa\u4e8e\u89c4\u5219\u7684\u65b9\u6cd5\u591a\u8bc6\u522b242%\u7684\u771f\u5b9eAI\u5e94\u7528\uff0c\u68c0\u6d4b\u7cbe\u5ea6\u548c\u53ec\u56de\u7387\u5747\u8d8590%\uff1b\u7528\u6237\u7814\u7a76\u8868\u660e\u5f00\u53d1\u8005\u66f4\u504f\u597d\u5176\u751f\u6210\u7684AI\u670d\u52a1\u6458\u8981\uff1b\u5b9e\u8bc1\u5206\u6790\u663e\u793a54.80%\u7684AI\u529f\u80fd\u96c6\u4e2d\u5728\u8ba1\u7b97\u673a\u89c6\u89c9\uff0c\u5176\u4e2d\u76ee\u6807\u68c0\u6d4b\u6700\u5e38\u89c1\uff0825.19%\uff09\u3002", "conclusion": "LLMAID\u6709\u6548\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u4e3a\u5927\u89c4\u6a21\u8bc6\u522b\u548c\u7406\u89e3\u79fb\u52a8\u5e94\u7528\u4e2d\u7684AI\u80fd\u529b\u63d0\u4f9b\u4e86\u9ad8\u6548\u3001\u51c6\u786e\u7684\u81ea\u52a8\u5316\u5de5\u5177\uff0c\u6709\u52a9\u4e8e\u63d0\u5347\u900f\u660e\u5ea6\u5e76\u652f\u6301\u76d1\u7ba1\u4e0e\u5f00\u53d1\u5b9e\u8df5\u3002"}}
{"id": "2511.19044", "categories": ["cs.NI"], "pdf": "https://arxiv.org/pdf/2511.19044", "abs": "https://arxiv.org/abs/2511.19044", "authors": ["Nguyen Duc Minh Quang", "Chang Liu", "Shuangyang Li", "Hoai-Nam Vu", "Derrick Wing Kwan Ng", "Wei Xiang"], "title": "Diffusion Model-Enhanced Environment Reconstruction in ISAC", "comment": "6 pages, 5 figures, submitted to IEEE WCL", "summary": "Recently, environment reconstruction (ER) in integrated sensing and communication (ISAC) systems has emerged as a promising approach for achieving high-resolution environmental perception. However, the initial results obtained from ISAC systems are coarse and often unsatisfactory due to the high sparsity of the point clouds and significant noise variance. To address this problem, we propose a noise-sparsity-aware diffusion model (NSADM) post-processing framework. Leveraging the powerful data recovery capabilities of diffusion models, the proposed scheme exploits spatial features and the additive nature of noise to enhance point cloud density and denoise the initial input. Simulation results demonstrate that the proposed method significantly outperforms existing model-based and deep learning-based approaches in terms of Chamfer distance and root mean square error.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u566a\u58f0-\u7a00\u758f\u6027\u611f\u77e5\u6269\u6563\u6a21\u578b\uff08NSADM\uff09\u540e\u5904\u7406\u6846\u67b6\uff0c\u7528\u4e8e\u63d0\u5347ISAC\u7cfb\u7edf\u4e2d\u73af\u5883\u91cd\u5efa\u7684\u70b9\u4e91\u8d28\u91cf\u548c\u53bb\u566a\u6548\u679c\u3002", "motivation": "ISAC\u7cfb\u7edf\u521d\u59cb\u83b7\u5f97\u7684\u73af\u5883\u91cd\u5efa\u7ed3\u679c\u56e0\u70b9\u4e91\u9ad8\u5ea6\u7a00\u758f\u548c\u566a\u58f0\u65b9\u5dee\u5927\u800c\u8d28\u91cf\u8f83\u5dee\uff0c\u4e9f\u9700\u6709\u6548\u65b9\u6cd5\u63d0\u5347\u91cd\u5efa\u7cbe\u5ea6\u3002", "method": "\u5229\u7528\u6269\u6563\u6a21\u578b\u5f3a\u5927\u7684\u6570\u636e\u6062\u590d\u80fd\u529b\uff0c\u7ed3\u5408\u7a7a\u95f4\u7279\u5f81\u4e0e\u566a\u58f0\u7684\u53ef\u52a0\u6027\uff0c\u8bbe\u8ba1NSADM\u540e\u5904\u7406\u6846\u67b6\u4ee5\u589e\u5f3a\u70b9\u4e91\u5bc6\u5ea6\u5e76\u53bb\u9664\u566a\u58f0\u3002", "result": "\u4eff\u771f\u5b9e\u9a8c\u8868\u660e\uff0c\u6240\u63d0\u65b9\u6cd5\u5728Chamfer\u8ddd\u79bb\u548c\u5747\u65b9\u6839\u8bef\u5dee\u6307\u6807\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u7684\u57fa\u4e8e\u6a21\u578b\u548c\u6df1\u5ea6\u5b66\u4e60\u7684\u65b9\u6cd5\u3002", "conclusion": "NSADM\u80fd\u6709\u6548\u6539\u5584ISAC\u7cfb\u7edf\u4e2d\u73af\u5883\u91cd\u5efa\u7684\u70b9\u4e91\u8d28\u91cf\uff0c\u4e3a\u9ad8\u5206\u8fa8\u7387\u73af\u5883\u611f\u77e5\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848\u3002"}}
{"id": "2511.19113", "categories": ["cs.NI"], "pdf": "https://arxiv.org/pdf/2511.19113", "abs": "https://arxiv.org/abs/2511.19113", "authors": ["Shaolong Guo", "Yuntao Wang", "Zhou Su", "Yanghe Pan", "Qinnan Hu", "Tom H. Luan"], "title": "Agent Discovery in Internet of Agents: Challenges and Solutions", "comment": "This work has been submitted to the IEEE for possible publication", "summary": "Rapid advances in large language models and agentic AI are driving the emergence of the Internet of Agents (IoA), a paradigm where billions of autonomous software and embodied agents interact, coordinate, and collaborate to accomplish complex tasks. A key prerequisite for such large-scale collaboration is agent capability discovery, where agents identify, advertise, and match one another's capabilities under dynamic tasks. Agent's capability in IoA is inherently heterogeneous and context-dependent, raising challenges in capability representation, scalable discovery, and long-term performance. To address these issues, this paper introduces a novel two-stage capability discovery framework. The first stage, autonomous capability announcement, allows agents to credibly publish machine-interpretable descriptions of their abilities. The second stage, task-driven capability discovery, enables context-aware search, ranking, and composition to locate and assemble suitable agents for specific tasks. Building on this framework, we propose a novel scheme that integrates semantic capability modeling, scalable and updatable indexing, and memory-enhanced continual discovery. Simulation results demonstrate that our approach enhances discovery performance and scalability. Finally, we outline a research roadmap and highlight open problems and promising directions for future IoA.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9762\u5411\u4e92\u8054\u7f51\u667a\u80fd\u4f53\uff08IoA\uff09\u7684\u4e24\u9636\u6bb5\u80fd\u529b\u53d1\u73b0\u6846\u67b6\uff0c\u901a\u8fc7\u81ea\u4e3b\u80fd\u529b\u58f0\u660e\u4e0e\u4efb\u52a1\u9a71\u52a8\u7684\u80fd\u529b\u53d1\u73b0\u673a\u5236\uff0c\u7ed3\u5408\u8bed\u4e49\u5efa\u6a21\u3001\u53ef\u6269\u5c55\u7d22\u5f15\u548c\u8bb0\u5fc6\u589e\u5f3a\u6280\u672f\uff0c\u63d0\u5347\u4e86\u5927\u89c4\u6a21\u667a\u80fd\u4f53\u534f\u4f5c\u4e2d\u7684\u80fd\u529b\u5339\u914d\u6548\u7387\u4e0e\u53ef\u6269\u5c55\u6027\u3002", "motivation": "\u5728\u4e92\u8054\u7f51\u667a\u80fd\u4f53\uff08IoA\uff09\u8303\u5f0f\u4e2d\uff0c\u6570\u5341\u4ebf\u5f02\u6784\u4e14\u4e0a\u4e0b\u6587\u76f8\u5173\u7684\u667a\u80fd\u4f53\u9700\u9ad8\u6548\u53d1\u73b0\u5f7c\u6b64\u80fd\u529b\u4ee5\u534f\u540c\u5b8c\u6210\u590d\u6742\u4efb\u52a1\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u5728\u80fd\u529b\u8868\u793a\u3001\u53ef\u6269\u5c55\u53d1\u73b0\u53ca\u957f\u671f\u6027\u80fd\u65b9\u9762\u9762\u4e34\u6311\u6218\u3002", "method": "\u63d0\u51fa\u4e00\u4e2a\u4e24\u9636\u6bb5\u80fd\u529b\u53d1\u73b0\u6846\u67b6\uff1a\u7b2c\u4e00\u9636\u6bb5\u4e3a\u81ea\u4e3b\u80fd\u529b\u58f0\u660e\uff0c\u4f7f\u667a\u80fd\u4f53\u80fd\u53ef\u4fe1\u5730\u53d1\u5e03\u673a\u5668\u53ef\u89e3\u91ca\u7684\u80fd\u529b\u63cf\u8ff0\uff1b\u7b2c\u4e8c\u9636\u6bb5\u4e3a\u4efb\u52a1\u9a71\u52a8\u7684\u80fd\u529b\u53d1\u73b0\uff0c\u652f\u6301\u4e0a\u4e0b\u6587\u611f\u77e5\u7684\u641c\u7d22\u3001\u6392\u5e8f\u4e0e\u7ec4\u5408\u3002\u8be5\u6846\u67b6\u878d\u5408\u4e86\u8bed\u4e49\u80fd\u529b\u5efa\u6a21\u3001\u53ef\u6269\u5c55\u4e14\u53ef\u66f4\u65b0\u7684\u7d22\u5f15\u673a\u5236\u4ee5\u53ca\u8bb0\u5fc6\u589e\u5f3a\u7684\u6301\u7eed\u53d1\u73b0\u7b56\u7565\u3002", "result": "\u4eff\u771f\u5b9e\u9a8c\u8868\u660e\uff0c\u6240\u63d0\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u80fd\u529b\u53d1\u73b0\u7684\u6027\u80fd\u4e0e\u53ef\u6269\u5c55\u6027\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3aIoA\u4e2d\u7684\u5927\u89c4\u6a21\u667a\u80fd\u4f53\u534f\u4f5c\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u80fd\u529b\u53d1\u73b0\u673a\u5236\uff0c\u5e76\u63d0\u51fa\u4e86\u672a\u6765\u7814\u7a76\u8def\u7ebf\u56fe\u53ca\u82e5\u5e72\u5f00\u653e\u95ee\u9898\u4e0e\u6f5c\u5728\u65b9\u5411\u3002"}}
{"id": "2511.19132", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2511.19132", "abs": "https://arxiv.org/abs/2511.19132", "authors": ["Mohammad Abboush", "Ahmad Hatahet", "Andreas Rausch"], "title": "LLMs-Powered Real-Time Fault Injection: An Approach Toward Intelligent Fault Test Cases Generation", "comment": null, "summary": "A well-known testing method for the safety evaluation and real-time validation of automotive software systems (ASSs) is Fault Injection (FI). In accordance with the ISO 26262 standard, the faults are introduced artificially for the purpose of analyzing the safety properties and verifying the safety mechanisms during the development phase. However, the current FI method and tools have a significant limitation in that they require manual identification of FI attributes, including fault type, location and time. The more complex the system, the more expensive, time-consuming and labour-intensive the process. To address the aforementioned challenge, a novel Large Language Models (LLMs)-assisted fault test cases (TCs) generation approach for utilization during real-time FI tests is proposed in this paper. To this end, considering the representativeness and coverage criteria, the applicability of various LLMs to create fault TCs from the functional safety requirements (FSRs) has been investigated. Through the validation results of LLMs, the superiority of the proposed approach utilizing gpt-4o in comparison to other state-of-the-art models has been demonstrated. Specifically, the proposed approach exhibits high performance in terms of FSRs classification and fault TCs generation with F1-score of 88% and 97.5%, respectively. To illustrate the proposed approach, the generated fault TCs were executed in real time on a hardware-in-the-loop system, where a high-fidelity automotive system model served as a case study. This novel approach offers a means of optimizing the real-time testing process, thereby reducing costs while simultaneously enhancing the safety properties of complex safety-critical ASSs.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u6545\u969c\u6d4b\u8bd5\u7528\u4f8b\u81ea\u52a8\u751f\u6210\u65b9\u6cd5\uff0c\u7528\u4e8e\u6c7d\u8f66\u8f6f\u4ef6\u7cfb\u7edf\u7684\u5b9e\u65f6\u6545\u969c\u6ce8\u5165\u6d4b\u8bd5\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6d4b\u8bd5\u6548\u7387\u4e0e\u5b89\u5168\u6027\u3002", "motivation": "\u5f53\u524d\u6545\u969c\u6ce8\u5165\uff08FI\uff09\u6d4b\u8bd5\u65b9\u6cd5\u5728\u8bc6\u522b\u6545\u969c\u7c7b\u578b\u3001\u4f4d\u7f6e\u548c\u65f6\u95f4\u7b49\u5c5e\u6027\u65f6\u4f9d\u8d56\u4eba\u5de5\u64cd\u4f5c\uff0c\u8fc7\u7a0b\u8017\u65f6\u3001\u6602\u8d35\u4e14\u52b3\u52a8\u5bc6\u96c6\uff0c\u5c24\u5176\u5728\u590d\u6742\u7cfb\u7edf\u4e2d\u95ee\u9898\u66f4\u4e3a\u7a81\u51fa\u3002", "method": "\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\uff08\u7279\u522b\u662fgpt-4o\uff09\u4ece\u529f\u80fd\u5b89\u5168\u9700\u6c42\uff08FSRs\uff09\u4e2d\u81ea\u52a8\u751f\u6210\u5177\u6709\u4ee3\u8868\u6027\u548c\u8986\u76d6\u6027\u7684\u6545\u969c\u6d4b\u8bd5\u7528\u4f8b\uff0c\u5e76\u5728\u786c\u4ef6\u5728\u73af\u7cfb\u7edf\u4e2d\u8fdb\u884c\u5b9e\u65f6\u9a8c\u8bc1\u3002", "result": "\u6240\u63d0\u65b9\u6cd5\u5728FSR\u5206\u7c7b\u548c\u6545\u969c\u6d4b\u8bd5\u7528\u4f8b\u751f\u6210\u65b9\u9762\u5206\u522b\u8fbe\u5230\u4e8688%\u548c97.5%\u7684F1\u5206\u6570\uff0c\u5e76\u5728\u9ad8\u4fdd\u771f\u6c7d\u8f66\u7cfb\u7edf\u6a21\u578b\u4e0a\u6210\u529f\u5b9e\u65bd\u4e86\u5b9e\u65f6\u6d4b\u8bd5\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6709\u6548\u4f18\u5316\u4e86\u6c7d\u8f66\u8f6f\u4ef6\u7cfb\u7edf\u7684\u5b9e\u65f6\u6545\u969c\u6ce8\u5165\u6d4b\u8bd5\u6d41\u7a0b\uff0c\u5728\u964d\u4f4e\u6210\u672c\u7684\u540c\u65f6\u63d0\u5347\u4e86\u590d\u6742\u5b89\u5168\u5173\u952e\u7cfb\u7edf\u7684\u5b89\u5168\u6027\u3002"}}
{"id": "2511.19177", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2511.19177", "abs": "https://arxiv.org/abs/2511.19177", "authors": ["Alcino Cunha", "Nuno Macedo"], "title": "Synthesizing Test Cases for Narrowing Specification Candidates", "comment": null, "summary": "This paper proposes a technique to help choose the best formal specification candidate among a set of alternatives. Given a set of specifications, our technique generates a suite of test cases that, once classified by the user as desirable or not, narrows down the set of candidates to at most one specification. Two alternative solver-based algorithms are proposed, one that generates a minimal test suite, and another that does not ensure minimality. Both algorithms were implemented in a prototype that can be used generate test suites to help choose among alternative Alloy specifications. Our evaluation of this prototype against a large set of problems showed that the optimal algorithm is efficient enough for many practical problems, and that the non-optimal algorithm can scale up to dozens of candidate specifications while still generating reasonably sized test suites.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6d4b\u8bd5\u7528\u4f8b\u751f\u6210\u7684\u6280\u672f\uff0c\u7528\u4e8e\u4ece\u591a\u4e2a\u5019\u9009\u5f62\u5f0f\u5316\u89c4\u7ea6\u4e2d\u9009\u51fa\u6700\u4f18\u7684\u4e00\u4e2a\uff1b\u901a\u8fc7\u7528\u6237\u5bf9\u751f\u6210\u6d4b\u8bd5\u7528\u4f8b\u7684\u53cd\u9988\uff0c\u53ef\u5c06\u5019\u9009\u96c6\u7f29\u5c0f\u81f3\u6700\u591a\u4e00\u4e2a\u89c4\u7ea6\uff0c\u5e76\u5b9e\u73b0\u4e86\u4e24\u79cd\u6c42\u89e3\u5668\u9a71\u52a8\u7684\u7b97\u6cd5\uff08\u6700\u4f18\u4e0e\u975e\u6700\u4f18\uff09\uff0c\u5b9e\u9a8c\u8868\u660e\u4e24\u8005\u5728\u5b9e\u9645\u95ee\u9898\u4e2d\u5747\u5177\u5b9e\u7528\u6027\u3002", "motivation": "\u5728\u5f62\u5f0f\u5316\u65b9\u6cd5\u5b9e\u8df5\u4e2d\uff0c\u5e38\u5e38\u5b58\u5728\u591a\u4e2a\u8bed\u4e49\u76f8\u8fd1\u4f46\u884c\u4e3a\u4e0d\u540c\u7684\u89c4\u7ea6\u5019\u9009\uff0c\u5982\u4f55\u4ece\u4e2d\u9009\u62e9\u6700\u7b26\u5408\u7528\u6237\u610f\u56fe\u7684\u4e00\u4e2a\u662f\u4e00\u4e2a\u5173\u952e\u6311\u6218\u3002\u73b0\u6709\u65b9\u6cd5\u7f3a\u4e4f\u6709\u6548\u673a\u5236\u6765\u5f15\u5bfc\u7528\u6237\u5728\u591a\u4e2a\u5019\u9009\u4e4b\u95f4\u505a\u51fa\u51c6\u786e\u9009\u62e9\u3002", "method": "\u63d0\u51fa\u4e24\u79cd\u57fa\u4e8e\u6c42\u89e3\u5668\u7684\u7b97\u6cd5\uff1a\u4e00\u79cd\u751f\u6210\u6700\u5c0f\u6d4b\u8bd5\u5957\u4ef6\uff0c\u53e6\u4e00\u79cd\u4e0d\u4fdd\u8bc1\u6700\u5c0f\u6027\u4f46\u53ef\u6269\u5c55\u6027\u66f4\u5f3a\uff1b\u901a\u8fc7\u7528\u6237\u5bf9\u6d4b\u8bd5\u7528\u4f8b\u662f\u5426\u201c\u53ef\u63a5\u53d7\u201d\u7684\u5206\u7c7b\u53cd\u9988\uff0c\u9010\u6b65\u6392\u9664\u4e0d\u7b26\u5408\u9884\u671f\u7684\u89c4\u7ea6\u5019\u9009\uff0c\u6700\u7ec8\u6536\u655b\u5230\u81f3\u591a\u4e00\u4e2a\u5019\u9009\u3002", "result": "\u539f\u578b\u5de5\u5177\u5728\u5927\u91cf\u95ee\u9898\u4e0a\u7684\u8bc4\u4f30\u8868\u660e\uff0c\u6700\u4f18\u7b97\u6cd5\u5bf9\u8bb8\u591a\u5b9e\u9645\u95ee\u9898\u8db3\u591f\u9ad8\u6548\uff0c\u800c\u975e\u6700\u4f18\u7b97\u6cd5\u53ef\u5904\u7406\u591a\u8fbe\u6570\u5341\u4e2a\u5019\u9009\u89c4\u7ea6\uff0c\u540c\u65f6\u4ecd\u751f\u6210\u89c4\u6a21\u5408\u7406\u7684\u6d4b\u8bd5\u5957\u4ef6\u3002", "conclusion": "\u8be5\u6280\u672f\u4e3a\u5728\u591a\u4e2a\u5f62\u5f0f\u5316\u89c4\u7ea6\u4e4b\u95f4\u8fdb\u884c\u9009\u62e9\u63d0\u4f9b\u4e86\u4e00\u79cd\u5b9e\u7528\u4e14\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5c24\u5176\u9002\u7528\u4e8eAlloy\u7b49\u8f7b\u91cf\u7ea7\u5f62\u5f0f\u5316\u89c4\u7ea6\u8bed\u8a00\u7684\u573a\u666f\u3002"}}
{"id": "2511.19422", "categories": ["cs.SE", "cs.AI", "cs.PL"], "pdf": "https://arxiv.org/pdf/2511.19422", "abs": "https://arxiv.org/abs/2511.19422", "authors": ["David Jiahao Fu", "Aryan Gupta", "Aaron Councilman", "David Grove", "Yu-Xiong Wang", "Vikram Adve"], "title": "SLMFix: Leveraging Small Language Models for Error Fixing with Reinforcement Learning", "comment": null, "summary": "Recent advancements in large language models (LLMs) have shown very impressive capabilities in code generation across many programming languages. However, even state-of-the-art LLMs generate programs that contains syntactic errors and fail to complete the given tasks, especially for low-resource programming languages (LRPLs). In addition, high training cost makes finetuning LLMs unaffordable with constrained computational resources, further undermining the effectiveness of LLMs for code generation. In this work, we propose SLMFix, a novel code generation pipeline that leverages a small language model (SLM) finetuned using reinforcement learning (RL) techniques to fix syntactic errors in LLM-generated programs to improve the quality of LLM-generated programs for domain-specific languages (DSLs). In specific, we applied RL on the SLM for the program repair task using a reward calculated using both a static validator and a static semantic similarity metric. Our experimental results demonstrate the effectiveness and generalizability of our approach across multiple DSLs, achieving more than 95% pass rate on the static validator. Notably, SLMFix brings substantial improvement to the base model and outperforms supervised finetuning approach even for 7B models on a LRPL, showing the potential of our approach as an alternative to traditional finetuning approaches.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faSLMFix\uff0c\u4e00\u79cd\u5229\u7528\u5f3a\u5316\u5b66\u4e60\u5fae\u8c03\u7684\u5c0f\u8bed\u8a00\u6a21\u578b\uff08SLM\uff09\u6765\u4fee\u590d\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u751f\u6210\u4ee3\u7801\u4e2d\u7684\u8bed\u6cd5\u9519\u8bef\u7684\u65b0\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5728\u4f4e\u8d44\u6e90\u9886\u57df\u7279\u5b9a\u8bed\u8a00\uff08DSL\uff09\u4e0a\u7684\u4ee3\u7801\u751f\u6210\u8d28\u91cf\u3002", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5728\u4ee3\u7801\u751f\u6210\u4e2d\u5e38\u4ea7\u751f\u8bed\u6cd5\u9519\u8bef\uff0c\u5c24\u5176\u5728\u4f4e\u8d44\u6e90\u7f16\u7a0b\u8bed\u8a00\u4e0a\u8868\u73b0\u4e0d\u4f73\uff1b\u540c\u65f6\uff0c\u4f20\u7edf\u5fae\u8c03\u6210\u672c\u9ad8\u6602\uff0c\u96be\u4ee5\u5728\u8d44\u6e90\u53d7\u9650\u6761\u4ef6\u4e0b\u6709\u6548\u5e94\u7528\u3002", "method": "\u63d0\u51faSLMFix\u6846\u67b6\uff0c\u4f7f\u7528\u5f3a\u5316\u5b66\u4e60\u5bf9\u5c0f\u8bed\u8a00\u6a21\u578b\uff08SLM\uff09\u8fdb\u884c\u5fae\u8c03\uff0c\u901a\u8fc7\u7ed3\u5408\u9759\u6001\u9a8c\u8bc1\u5668\u548c\u9759\u6001\u8bed\u4e49\u76f8\u4f3c\u5ea6\u6307\u6807\u8ba1\u7b97\u5956\u52b1\u4fe1\u53f7\uff0c\u7528\u4e8e\u4fee\u590dLLM\u751f\u6210\u4ee3\u7801\u4e2d\u7684\u8bed\u6cd5\u9519\u8bef\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cSLMFix\u5728\u591a\u4e2a\u9886\u57df\u7279\u5b9a\u8bed\u8a00\uff08DSL\uff09\u4e0a\u5177\u6709\u826f\u597d\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u5728\u9759\u6001\u9a8c\u8bc1\u5668\u4e0a\u7684\u901a\u8fc7\u7387\u8d85\u8fc795%\uff0c\u4e14\u4f18\u4e8e\u76d1\u7763\u5fae\u8c03\u65b9\u6cd5\uff0c\u5373\u4f7f\u57287B\u89c4\u6a21\u6a21\u578b\u548c\u4f4e\u8d44\u6e90\u8bed\u8a00\u4e0a\u4e5f\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "SLMFix\u4e3a\u63d0\u5347LLM\u5728\u4f4e\u8d44\u6e90DSL\u4e0a\u7684\u4ee3\u7801\u751f\u6210\u8d28\u91cf\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u3001\u4f4e\u6210\u672c\u7684\u66ff\u4ee3\u65b9\u6848\uff0c\u5c55\u793a\u4e86\u5f3a\u5316\u5b66\u4e60\u5728\u7a0b\u5e8f\u4fee\u590d\u4efb\u52a1\u4e2d\u7684\u6f5c\u529b\u3002"}}
{"id": "2511.19427", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.19427", "abs": "https://arxiv.org/abs/2511.19427", "authors": ["Jayanaka L. Dantanarayana", "Savini Kashmira", "Thakee Nathees", "Zichen Zhang", "Krisztian Flautner", "Lingjia Tang", "Jason Mars"], "title": "Prompt Less, Smile More: MTP with Semantic Engineering in Lieu of Prompt Engineering", "comment": null, "summary": "AI-Integrated programming is emerging as a foundational paradigm for building intelligent systems with large language models (LLMs). Recent approaches such as Meaning Typed Programming (MTP) automate prompt generation by leveraging the semantics already present in code. However, many real-world applications depend on contextual cues, developer intent, and domain-specific reasoning that extend beyond what static code semantics alone can express. To address this limitation, we introduce Semantic Engineering, a lightweight method for enriching program semantics so that LLM-based systems can more accurately reflect developer intent without requiring full manual prompt design. We present Semantic Context Annotations (SemTexts), a language-level mechanism that allows developers to embed natural-language context directly into program constructs. Integrated into the Jac programming language, Semantic Engineering extends MTP to incorporate these enriched semantics during prompt generation. We further introduce a benchmark suite designed to reflect realistic AI-Integrated application scenarios. Our evaluation shows that Semantic Engineering substantially improves prompt fidelity, achieving performance comparable to Prompt Engineering while requiring significantly less developer effort.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u201c\u8bed\u4e49\u5de5\u7a0b\u201d\u7684\u8f7b\u91cf\u7ea7\u65b9\u6cd5\uff0c\u901a\u8fc7\u5728\u7a0b\u5e8f\u4e2d\u5d4c\u5165\u81ea\u7136\u8bed\u8a00\u4e0a\u4e0b\u6587\uff08SemTexts\uff09\u6765\u589e\u5f3a\u4ee3\u7801\u8bed\u4e49\uff0c\u4f7f\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u7cfb\u7edf\u80fd\u66f4\u51c6\u786e\u5730\u53cd\u6620\u5f00\u53d1\u8005\u610f\u56fe\uff0c\u663e\u8457\u63d0\u5347\u63d0\u793a\u751f\u6210\u8d28\u91cf\uff0c\u540c\u65f6\u51cf\u5c11\u4eba\u5de5\u63d0\u793a\u8bbe\u8ba1\u7684\u5de5\u4f5c\u91cf\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u4ee3\u7801\u9759\u6001\u8bed\u4e49\u7684\u65b9\u6cd5\uff08\u5982MTP\uff09\u65e0\u6cd5\u5145\u5206\u8868\u8fbe\u771f\u5b9e\u5e94\u7528\u573a\u666f\u4e2d\u7684\u4e0a\u4e0b\u6587\u7ebf\u7d22\u3001\u5f00\u53d1\u8005\u610f\u56fe\u548c\u9886\u57df\u7279\u5b9a\u63a8\u7406\uff0c\u9650\u5236\u4e86AI\u96c6\u6210\u7f16\u7a0b\u7684\u6548\u679c\u3002", "method": "\u5f15\u5165\u8bed\u4e49\u5de5\u7a0b\u65b9\u6cd5\uff0c\u5728Jac\u7f16\u7a0b\u8bed\u8a00\u4e2d\u5b9e\u73b0\u8bed\u4e49\u4e0a\u4e0b\u6587\u6ce8\u89e3\uff08SemTexts\uff09\uff0c\u5141\u8bb8\u5f00\u53d1\u8005\u5c06\u81ea\u7136\u8bed\u8a00\u4e0a\u4e0b\u6587\u76f4\u63a5\u5d4c\u5165\u7a0b\u5e8f\u7ed3\u6784\uff0c\u5e76\u5c06\u5176\u6574\u5408\u8fdbMTP\u6846\u67b6\u7528\u4e8e\u63d0\u793a\u751f\u6210\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8bed\u4e49\u5de5\u7a0b\u663e\u8457\u63d0\u5347\u4e86\u63d0\u793a\u4fdd\u771f\u5ea6\uff0c\u5728\u6027\u80fd\u4e0a\u63a5\u8fd1\u4f20\u7edf\u63d0\u793a\u5de5\u7a0b\uff0c\u4f46\u6240\u9700\u5f00\u53d1\u8005\u5de5\u4f5c\u91cf\u5927\u5e45\u51cf\u5c11\u3002", "conclusion": "\u8bed\u4e49\u5de5\u7a0b\u662f\u4e00\u79cd\u9ad8\u6548\u4e14\u5b9e\u7528\u7684\u65b9\u6cd5\uff0c\u80fd\u591f\u5728\u4e0d\u4f9d\u8d56\u5b8c\u6574\u624b\u52a8\u63d0\u793a\u8bbe\u8ba1\u7684\u524d\u63d0\u4e0b\uff0c\u63d0\u5347AI\u96c6\u6210\u7f16\u7a0b\u4e2dLLM\u5bf9\u5f00\u53d1\u8005\u610f\u56fe\u7684\u7406\u89e3\u4e0e\u8868\u8fbe\u80fd\u529b\u3002"}}
