{"id": "2512.06042", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.06042", "abs": "https://arxiv.org/abs/2512.06042", "authors": ["Ashish Hooda", "Mihai Christodorescu", "Chuangang Ren", "Aaron Wilson", "Kassem Fawaz", "Somesh Jha"], "title": "Auto-SPT: Automating Semantic Preserving Transformations for Code", "comment": null, "summary": "Machine learning (ML) models for code clone detection determine whether two pieces of code are semantically equivalent, which in turn is a key building block for software-engineering tasks like refactoring and security tasks like vulnerability and malware detection. While these models are predominantly trained on clean, structured code datasets, real-world code often undergoes a variety of semantic-preserving transformations, including refactoring, minification, automated formatting, and compiler optimizations. To address this critical gap between training and test data, we propose Auto-SPT, a novel framework to automatically construct synthetic-data generators for code. Auto-SPT is designed to produce Semantic Preserving Transformations (SPTs) that alter a program's syntactic structure while preserving its functionality and is instantiated on top of Large Language Models (LLMs). In particular, we use LLMs to craft a diverse set of SPTs, generate strong implementations for these SPTs, and compose them to result into strong transformations. Our formal analysis shows that the diversity of SPTs impacts the strength of their composition. We then empirically demonstrate that Auto-SPT generates more diverse SPTs than existing approaches and these SPTs significantly drop the performance of state-of-the-art code clone detectors. Further experiments show Auto-SPT can be used to enhance code datasets for training, to produce code-clone detection models that are robust to real-world, adversarial code transformations.", "AI": {"tldr": "Auto-SPT \u662f\u4e00\u79cd\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u81ea\u52a8\u751f\u6210\u8bed\u4e49\u4fdd\u6301\u53d8\u6362\uff08SPT\uff09\uff0c\u4ee5\u589e\u5f3a\u4ee3\u7801\u514b\u9686\u68c0\u6d4b\u6a21\u578b\u5bf9\u771f\u5b9e\u4e16\u754c\u4ee3\u7801\u53d8\u6362\u7684\u9c81\u68d2\u6027\u3002", "motivation": "\u73b0\u6709\u4ee3\u7801\u514b\u9686\u68c0\u6d4b\u6a21\u578b\u591a\u5728\u5e72\u51c0\u7ed3\u6784\u5316\u6570\u636e\u4e0a\u8bad\u7ec3\uff0c\u96be\u4ee5\u5e94\u5bf9\u771f\u5b9e\u4ee3\u7801\u4e2d\u5e38\u89c1\u7684\u8bed\u4e49\u4fdd\u6301\u53d8\u6362\uff0c\u5bfc\u81f4\u6027\u80fd\u4e0b\u964d\u3002", "method": "\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\u81ea\u52a8\u751f\u6210\u591a\u6837\u5316\u7684 SPT\uff0c\u5e76\u7ec4\u5408\u751f\u6210\u5f3a\u53d8\u6362\uff0c\u7528\u4e8e\u6784\u5efa\u5408\u6210\u6570\u636e\u96c6\u6216\u589e\u5f3a\u8bad\u7ec3\u6570\u636e\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e Auto-SPT \u751f\u6210\u7684 SPT \u6bd4\u73b0\u6709\u65b9\u6cd5\u66f4\u4e30\u5bcc\u591a\u6837\uff0c\u80fd\u663e\u8457\u964d\u4f4e\u5f53\u524d\u6700\u4f18\u68c0\u6d4b\u5668\u6027\u80fd\uff0c\u540c\u65f6\u63d0\u5347\u6a21\u578b\u5bf9\u5bf9\u6297\u6027\u53d8\u6362\u7684\u9c81\u68d2\u6027\u3002", "conclusion": "Auto-SPT \u6709\u6548\u5f25\u5408\u4e86\u8bad\u7ec3\u4e0e\u6d4b\u8bd5\u6570\u636e\u95f4\u7684\u8bed\u4e49\u9e3f\u6c9f\uff0c\u4e3a\u6784\u5efa\u9c81\u68d2\u4ee3\u7801\u514b\u9686\u68c0\u6d4b\u7cfb\u7edf\u63d0\u4f9b\u4e86\u65b0\u9014\u5f84\u3002"}}
{"id": "2512.06046", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.06046", "abs": "https://arxiv.org/abs/2512.06046", "authors": ["Ramprasath Ganesaraja", "Swathika N", "Saravanan AP", "Kamalkumar Rathinasamy", "Chetana Amancharla", "Rahul Das", "Sahil Dilip Panse", "Aditya Batwe", "Dileep Vijayan", "Veena Ashok", "Thanushree A P", "Kausthubh J Rao", "Alden Olivero", "Roshan", "Rajeshwar Reddy Manthena", "Asmitha Yuga Sre A", "Harsh Tripathi", "Suganya Selvaraj", "Vito Chin", "Kasthuri Rangan Bhaskar", "Kasthuri Rangan Bhaskar", "Venkatraman R", "Sajit Vijayakumar"], "title": "Beyond Prototyping: Autonomous, Enterprise-Grade Frontend Development from Pixel to Production via a Specialized Multi-Agent Framework", "comment": "17 pages, 9 figures", "summary": "We present AI4UI, a framework of autonomous front-end development agents purpose-built to meet the rigorous requirements of enterprise-grade application delivery. Unlike general-purpose code assistants designed for rapid prototyping, AI4UI focuses on production readiness delivering secure, scalable, compliant, and maintainable UI code integrated seamlessly into enterprise workflows. AI4UI operates with targeted human-in-the-loop involvement: at the design stage, developers embed a Gen-AI-friendly grammar into Figma prototypes to encode requirements for precise interpretation; and at the post processing stage, domain experts refine outputs for nuanced design adjustments, domain-specific optimizations, and compliance needs. Between these stages, AI4UI runs fully autonomously, converting designs into engineering-ready UI code. Technical contributions include a Figma grammar for autonomous interpretation, domain-aware knowledge graphs, a secure abstract/package code integration strategy, expertise driven architecture templates, and a change-oriented workflow coordinated by specialized agent roles. In large-scale benchmarks against industry baselines and leading competitor systems, AI4UI achieved 97.24% platform compatibility, 87.10% compilation success, 86.98% security compliance, 78.00% feature implementation success, 73.50% code-review quality, and 73.36% UI/UX consistency. In blind preference studies with 200 expert evaluators, AI4UI emerged as one of the leaders demonstrating strong competitive standing among leading solutions. Operating asynchronously, AI4UI generates thousands of validated UI screens in weeks rather than months, compressing delivery timeline", "AI": {"tldr": "AI4UI\u662f\u4e00\u4e2a\u9762\u5411\u4f01\u4e1a\u7ea7\u524d\u7aef\u5f00\u53d1\u7684\u81ea\u4e3b\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u901a\u8fc7\u8bbe\u8ba1\u9636\u6bb5\u5d4c\u5165\u8bed\u6cd5\u4e0e\u540e\u671f\u4e13\u5bb6\u5fae\u8c03\u5b9e\u73b0\u5168\u6d41\u7a0b\u81ea\u52a8\u5316\uff0c\u663e\u8457\u63d0\u5347\u4ea4\u4ed8\u6548\u7387\u4e0e\u4ee3\u7801\u8d28\u91cf\u3002", "motivation": "\u89e3\u51b3\u901a\u7528\u4ee3\u7801\u52a9\u624b\u5728\u4f01\u4e1a\u751f\u4ea7\u73af\u5883\u4e2d\u5b89\u5168\u6027\u3001\u5408\u89c4\u6027\u3001\u53ef\u7ef4\u62a4\u6027\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u6ee1\u8db3\u4f01\u4e1a\u5bf9\u9ad8\u8d28\u91cfUI\u4ee3\u7801\u7684\u4e25\u82db\u9700\u6c42\u3002", "method": "\u5f15\u5165Figma\u8bed\u6cd5\u89c4\u5219\u3001\u9886\u57df\u77e5\u8bc6\u56fe\u8c31\u3001\u5b89\u5168\u4ee3\u7801\u96c6\u6210\u7b56\u7565\u3001\u4e13\u5bb6\u67b6\u6784\u6a21\u677f\u53ca\u591a\u89d2\u8272\u534f\u540c\u5de5\u4f5c\u6d41\uff0c\u7ed3\u5408\u4eba\u673a\u534f\u4f5c\u673a\u5236\u5b9e\u73b0\u8bbe\u8ba1\u5230\u4ee3\u7801\u7684\u5168\u81ea\u52a8\u8f6c\u5316\u3002", "result": "\u5728\u591a\u9879\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\uff1a\u5e73\u53f0\u517c\u5bb9\u602797.24%\u3001\u7f16\u8bd1\u6210\u529f\u738787.10%\u3001\u5b89\u5168\u5408\u89c486.98%\uff0c\u5e76\u83b7200\u540d\u4e13\u5bb6\u76f2\u6d4b\u8ba4\u53ef\uff0c\u4ea4\u4ed8\u5468\u671f\u4ece\u6570\u6708\u7f29\u77ed\u81f3\u6570\u5468\u3002", "conclusion": "AI4UI\u5728\u4f01\u4e1a\u7ea7\u524d\u7aef\u81ea\u52a8\u5316\u5f00\u53d1\u4e2d\u5177\u5907\u9886\u5148\u7ade\u4e89\u529b\uff0c\u517c\u987e\u6548\u7387\u3001\u8d28\u91cf\u4e0e\u5408\u89c4\uff0c\u662f\u8fc8\u5411\u9ad8\u53ef\u9760AI\u5de5\u7a0b\u5316\u7684\u91cd\u8981\u5b9e\u8df5\u3002"}}
{"id": "2512.06060", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.06060", "abs": "https://arxiv.org/abs/2512.06060", "authors": ["Mohanakrishnan Hariharan"], "title": "Reinforcement Learning Integrated Agentic RAG for Software Test Cases Authoring", "comment": null, "summary": "This paper introduces a framework that integrates reinforcement learning (RL) with autonomous agents to enable continuous improvement in the automated process of software test cases authoring from business requirement documents within Quality Engineering (QE) workflows. Conventional systems employing Large Language Models (LLMs) generate test cases from static knowledge bases, which fundamentally limits their capacity to enhance performance over time. Our proposed Reinforcement Infused Agentic RAG (Retrieve, Augment, Generate) framework overcomes this limitation by employing AI agents that learn from QE feedback, assessments, and defect discovery outcomes to automatically improve their test case generation strategies. The system combines specialized agents with a hybrid vector-graph knowledge base that stores and retrieves software testing knowledge. Through advanced RL algorithms, specifically Proximal Policy Optimization (PPO) and Deep Q-Networks (DQN), these agents optimize their behavior based on QE-reported test effectiveness, defect detection rates, and workflow metrics. As QEs execute AI-generated test cases and provide feedback, the system learns from this expert guidance to improve future iterations. Experimental validation on enterprise Apple projects yielded substantive improvements: a 2.4% increase in test generation accuracy (from 94.8% to 97.2%), and a 10.8% improvement in defect detection rates. The framework establishes a continuous knowledge refinement loop driven by QE expertise, resulting in progressively superior test case quality that enhances, rather than replaces, human testing capabilities.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u5f3a\u5316\u5b66\u4e60\u4e0e\u81ea\u4e3b\u667a\u80fd\u4f53\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u5728\u8d28\u91cf\u5de5\u7a0b\u6d41\u7a0b\u4e2d\u6301\u7eed\u4f18\u5316\u4ece\u9700\u6c42\u6587\u6863\u81ea\u52a8\u751f\u6210\u6d4b\u8bd5\u7528\u4f8b\u7684\u80fd\u529b\u3002", "motivation": "\u4f20\u7edf\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u7cfb\u7edf\u4f9d\u8d56\u9759\u6001\u77e5\u8bc6\u5e93\uff0c\u65e0\u6cd5\u968f\u65f6\u95f4\u6301\u7eed\u63d0\u5347\u6027\u80fd\uff0c\u9650\u5236\u4e86\u81ea\u52a8\u5316\u6d4b\u8bd5\u7684\u53d1\u5c55\u3002", "method": "\u91c7\u7528\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\uff08PPO\u548cDQN\uff09\u9a71\u52a8\u7684\u667a\u80fd\u4f53\uff0c\u7ed3\u5408\u6df7\u5408\u5411\u91cf-\u56fe\u8c31\u77e5\u8bc6\u5e93\uff0c\u4f9d\u636eQE\u53cd\u9988\u3001\u7f3a\u9677\u53d1\u73b0\u7b49\u6307\u6807\u52a8\u6001\u4f18\u5316\u6d4b\u8bd5\u751f\u6210\u7b56\u7565\u3002", "result": "\u5728\u82f9\u679c\u4f01\u4e1a\u9879\u76ee\u5b9e\u9a8c\u4e2d\uff0c\u6d4b\u8bd5\u751f\u6210\u51c6\u786e\u7387\u63d0\u53472.4%\uff0c\u7f3a\u9677\u68c0\u51fa\u7387\u63d0\u9ad810.8%\u3002", "conclusion": "\u8be5\u6846\u67b6\u6784\u5efa\u4e86\u7531QE\u4e13\u5bb6\u9a71\u52a8\u7684\u77e5\u8bc6\u6301\u7eed\u4f18\u5316\u95ed\u73af\uff0c\u5728\u589e\u5f3a\u800c\u975e\u53d6\u4ee3\u4eba\u5de5\u6d4b\u8bd5\u80fd\u529b\u7684\u540c\u65f6\uff0c\u5b9e\u73b0\u6d4b\u8bd5\u7528\u4f8b\u8d28\u91cf\u7684\u6e10\u8fdb\u5f0f\u63d0\u5347\u3002"}}
{"id": "2512.06331", "categories": ["cs.OS", "eess.SY"], "pdf": "https://arxiv.org/pdf/2512.06331", "abs": "https://arxiv.org/abs/2512.06331", "authors": ["Marcus V\u00f6lp", "Mohammad Ibrahim Alkoudsi", "Azin Bayrami Asl", "Kristin Kr\u00fcger", "Julio Rodrigues Mendonca da Neto", "Gerhard Fohler"], "title": "Defending Event-Triggered Systems against Out-of-Envelope Environments", "comment": "Published at the RTAutoSec Workshop, which was co-located to ECRTS 2025", "summary": "The design of real-time systems is based on assumptions about environmental conditions in which they will operate. We call this their safe operational envelope. Violation of these assumptions, i.e., out-of-envelope environments, can jeopardize timeliness and safety of real-time systems, e.g., by overwhelming them with interrupt storms. A long-lasting debate has been going on over which design paradigm, the time- or event-triggered, is more robust against such behavior. In this work, we investigate the claim that time-triggered systems are immune against out-of-envelope behavior and how event-triggered systems can be constructed to defend against being overwhelmed by interrupt showers. We introduce importance (independently of priority and criticality) as a means to express which tasks should still be scheduled in case environmental design assumptions cease to hold, draw parallels to mixed-criticality scheduling, and demonstrate how event-triggered systems can defend against out-of-envelope behavior.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u5b9e\u65f6\u7cfb\u7edf\u5728\u8d85\u51fa\u8bbe\u8ba1\u73af\u5883\u6761\u4ef6\u65f6\u7684\u9c81\u68d2\u6027\uff0c\u6bd4\u8f83\u65f6\u95f4\u89e6\u53d1\u4e0e\u4e8b\u4ef6\u89e6\u53d1\u8303\u5f0f\uff0c\u5e76\u63d0\u51fa\u901a\u8fc7\u4efb\u52a1\u91cd\u8981\u6027\u63d0\u5347\u4e8b\u4ef6\u89e6\u53d1\u7cfb\u7edf\u7684\u9632\u5fa1\u80fd\u529b\u3002", "motivation": "\u5b9e\u65f6\u7cfb\u7edf\u5728\u8d85\u51fa\u5b89\u5168\u64cd\u4f5c\u73af\u5883\u65f6\u53ef\u80fd\u4e27\u5931\u65f6\u6548\u6027\u4e0e\u5b89\u5168\u6027\uff0c\u9700\u7814\u7a76\u5982\u4f55\u589e\u5f3a\u5176\u9c81\u68d2\u6027\u3002", "method": "\u5f15\u5165\u201c\u91cd\u8981\u6027\u201d\u6982\u5ff5\uff0c\u72ec\u7acb\u4e8e\u4f18\u5148\u7ea7\u4e0e\u5173\u952e\u6027\uff0c\u6307\u5bfc\u4efb\u52a1\u8c03\u5ea6\uff1b\u7c7b\u6bd4\u6df7\u5408\u5173\u952e\u6027\u8c03\u5ea6\u65b9\u6cd5\u3002", "result": "\u4e8b\u4ef6\u89e6\u53d1\u7cfb\u7edf\u53ef\u901a\u8fc7\u91cd\u8981\u6027\u673a\u5236\u6709\u6548\u62b5\u5fa1\u4e2d\u65ad\u98ce\u66b4\u7b49\u8d85\u51fa\u8bbe\u8ba1\u5047\u8bbe\u7684\u73af\u5883\u884c\u4e3a\u3002", "conclusion": "\u65f6\u95f4\u89e6\u53d1\u7cfb\u7edf\u5e76\u975e\u5929\u7136\u514d\u75ab\u73af\u5883\u8d8a\u754c\u884c\u4e3a\uff0c\u4e8b\u4ef6\u89e6\u53d1\u7cfb\u7edf\u7ecf\u9002\u5f53\u8bbe\u8ba1\u53ef\u5177\u5907\u540c\u7b49\u751a\u81f3\u66f4\u5f3a\u7684\u9c81\u68d2\u6027\u3002"}}
{"id": "2512.06123", "categories": ["cs.SE", "cs.AI", "cs.CR"], "pdf": "https://arxiv.org/pdf/2512.06123", "abs": "https://arxiv.org/abs/2512.06123", "authors": ["Qilin Zhou", "Zhengyuan Wei", "Haipeng Wang", "Zhuo Wang", "W. K. Chan"], "title": "Toward Patch Robustness Certification and Detection for Deep Learning Systems Beyond Consistent Samples", "comment": "accepted by IEEE Transactions on Reliability; extended technical report", "summary": "Patch robustness certification is an emerging kind of provable defense technique against adversarial patch attacks for deep learning systems. Certified detection ensures the detection of all patched harmful versions of certified samples, which mitigates the failures of empirical defense techniques that could (easily) be compromised. However, existing certified detection methods are ineffective in certifying samples that are misclassified or whose mutants are inconsistently pre icted to different labels. This paper proposes HiCert, a novel masking-based certified detection technique. By focusing on the problem of mutants predicted with a label different from the true label with our formal analysis, HiCert formulates a novel formal relation between harmful samples generated by identified loopholes and their benign counterparts. By checking the bound of the maximum confidence among these potentially harmful (i.e., inconsistent) mutants of each benign sample, HiCert ensures that each harmful sample either has the minimum confidence among mutants that are predicted the same as the harmful sample itself below this bound, or has at least one mutant predicted with a label different from the harmful sample itself, formulated after two novel insights. As such, HiCert systematically certifies those inconsistent samples and consistent samples to a large extent. To our knowledge, HiCert is the first work capable of providing such a comprehensive patch robustness certification for certified detection. Our experiments show the high effectiveness of HiCert with a new state-of the-art performance: It certifies significantly more benign samples, including those inconsistent and consistent, and achieves significantly higher accuracy on those samples without warnings and a significantly lower false silent ratio.", "AI": {"tldr": "HiCert\u662f\u4e00\u79cd\u65b0\u578b\u7684\u57fa\u4e8e\u63a9\u7801\u7684\u8ba4\u8bc1\u68c0\u6d4b\u6280\u672f\uff0c\u80fd\u5168\u9762\u8ba4\u8bc1\u5bf9\u6297\u6027\u8865\u4e01\u653b\u51fb\u4e2d\u7684\u4e0d\u4e00\u81f4\u548c\u4e00\u81f4\u6837\u672c\uff0c\u663e\u8457\u63d0\u5347\u8ba4\u8bc1\u6548\u679c\u3002", "motivation": "\u73b0\u6709\u8ba4\u8bc1\u68c0\u6d4b\u65b9\u6cd5\u5bf9\u8bef\u5206\u7c7b\u6216\u9884\u6d4b\u4e0d\u4e00\u81f4\u7684\u6837\u672c\u65e0\u6548\uff0c\u4e9f\u9700\u6539\u8fdb\u4ee5\u589e\u5f3a\u9632\u5fa1\u80fd\u529b\u3002", "method": "\u901a\u8fc7\u5206\u6790\u6709\u5bb3\u6837\u672c\u4e0e\u826f\u6027\u6837\u672c\u4e4b\u95f4\u7684\u5f62\u5f0f\u5316\u5173\u7cfb\uff0c\u8bbe\u5b9a\u6700\u5927\u7f6e\u4fe1\u5ea6\u8fb9\u754c\uff0c\u7cfb\u7edf\u6027\u5730\u8ba4\u8bc1\u4e0d\u4e00\u81f4\u548c\u4e00\u81f4\u6837\u672c\u3002", "result": "\u5b9e\u9a8c\u8868\u660eHiCert\u5728\u8ba4\u8bc1\u66f4\u591a\u826f\u6027\u6837\u672c\u3001\u63d0\u9ad8\u65e0\u8b66\u544a\u6837\u672c\u51c6\u786e\u7387\u53ca\u964d\u4f4e\u5047\u6c89\u9ed8\u7387\u65b9\u9762\u8fbe\u5230\u65b0SOTA\u6027\u80fd\u3002", "conclusion": "HiCert\u662f\u9996\u4e2a\u80fd\u63d0\u4f9b\u5168\u9762\u8865\u4e01\u9c81\u68d2\u6027\u8ba4\u8bc1\u7684\u65b9\u6848\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002"}}
{"id": "2512.05983", "categories": ["cs.MA", "cs.CL", "cs.GT"], "pdf": "https://arxiv.org/pdf/2512.05983", "abs": "https://arxiv.org/abs/2512.05983", "authors": ["Eyal Briman", "Ehud Shapiro", "Nimrod Talmon"], "title": "AI-Generated Compromises for Coalition Formation: Modeling, Simulation, and a Textual Case Study", "comment": "In Proceedings TARK 2025, arXiv:2511.20540. arXiv admin note: substantial text overlap with arXiv:2506.06837", "summary": "The challenge of finding compromises between agent proposals is fundamental to AI sub-fields such as argumentation, mediation, and negotiation. Building on this tradition, Elkind et al. (2021) introduced a process for coalition formation that seeks majority-supported proposals preferable to the status quo, using a metric space where each agent has an ideal point. The crucial step in this iterative process involves identifying compromise proposals around which agent coalitions can unite. How to effectively find such compromise proposals, however, remains an open question. We address this gap by formalizing a holistic model that encompasses agent bounded rationality and uncertainty and developing AI models to generate such compromise proposals. We focus on the domain of collaboratively writing text documents -- e.g., to enable the democratic creation of a community constitution. We apply NLP (Natural Language Processing) techniques and utilize LLMs (Large Language Models) to create a semantic metric space for text and develop algorithms to suggest suitable compromise points. To evaluate the effectiveness of our algorithms, we simulate various coalition formation processes and demonstrate the potential of AI to facilitate large-scale democratic text editing, such as collaboratively drafting a constitution, an area where traditional tools are limited.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408NLP\u4e0eLLM\u7684AI\u6a21\u578b\uff0c\u7528\u4e8e\u5728\u8bed\u4e49\u7a7a\u95f4\u4e2d\u751f\u6210\u652f\u6301\u591a\u6570\u4e14\u4f18\u4e8e\u73b0\u72b6\u7684\u59a5\u534f\u63d0\u6848\uff0c\u4ee5\u4fc3\u8fdb\u5927\u89c4\u6a21\u6c11\u4e3b\u6587\u672c\u534f\u4f5c\u7f16\u8f91\u3002", "motivation": "\u89e3\u51b3\u5728\u8054\u76df\u5f62\u6210\u8fc7\u7a0b\u4e2d\u5982\u4f55\u6709\u6548\u5bfb\u627e\u59a5\u534f\u63d0\u6848\u7684\u95ee\u9898\uff0c\u7279\u522b\u662f\u5728\u6c11\u4e3b\u5316\u6587\u672c\u534f\u4f5c\u573a\u666f\u4e2d\u4f20\u7edf\u5de5\u5177\u7684\u4e0d\u8db3\u3002", "method": "\u6784\u5efa\u5305\u542b\u6709\u9650\u7406\u6027\u4e0e\u4e0d\u786e\u5b9a\u6027\u7684\u6574\u4f53\u6a21\u578b\uff0c\u5229\u7528\u81ea\u7136\u8bed\u8a00\u5904\u7406\u4e0e\u5927\u8bed\u8a00\u6a21\u578b\u5efa\u7acb\u8bed\u4e49\u5ea6\u91cf\u7a7a\u95f4\uff0c\u5e76\u8bbe\u8ba1\u7b97\u6cd5\u63a8\u8350\u59a5\u534f\u70b9\u3002", "result": "\u901a\u8fc7\u6a21\u62df\u591a\u79cd\u8054\u76df\u5f62\u6210\u8fc7\u7a0b\uff0c\u9a8c\u8bc1\u4e86\u6240\u63d0\u7b97\u6cd5\u5728\u534f\u52a9\u6c11\u4e3b\u5316\u6587\u672c\u7f16\u8f91\uff08\u5982\u8d77\u8349\u793e\u533a\u7ae0\u7a0b\uff09\u4e2d\u7684\u6709\u6548\u6027\u3002", "conclusion": "AI\u6280\u672f\u6709\u671b\u663e\u8457\u63d0\u5347\u5927\u89c4\u6a21\u7fa4\u4f53\u5728\u6587\u672c\u534f\u4f5c\u4e2d\u7684\u534f\u5546\u6548\u7387\u4e0e\u6c11\u4e3b\u6027\uff0c\u5f25\u8865\u73b0\u6709\u5de5\u5177\u7684\u5c40\u9650\u3002"}}
{"id": "2512.06443", "categories": ["cs.DC", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.06443", "abs": "https://arxiv.org/abs/2512.06443", "authors": ["Xiangyu Li", "Chengyu Yin", "Weijun Wang", "Jianyu Wei", "Ting Cao", "Yunxin Liu"], "title": "Vec-LUT: Vector Table Lookup for Parallel Ultra-Low-Bit LLM Inference on Edge Devices", "comment": "Preprint", "summary": "Large language models (LLMs) are increasingly deployed on edge devices. To meet strict resource constraints, real-world deployment has pushed LLM quantization from 8-bit to 4-bit, 2-bit, and now 1.58-bit. Combined with lookup table (LUT)-based inference, CPUs run these ultra-low-bit LLMs even faster than NPUs, opening new opportunities for ubiquitous on-device intelligence.\n  However, this paper identifies that LUT-based inference underutilizes memory bandwidth during parallel inference, which is required for prefilling, test-time scaling, and other multi-token scenarios. The root cause is the scalar LUT paradigm, which performs repetitive and non-contiguous memory accesses for each token.\n  To solve the issue, we propose vector LUT, a new lookup paradigm that constructs a unified LUT across parallel tokens, and performs a single $1 \\rightarrow N$ lookup per index. To realize it efficiently, we further introduce (1) Vector LUT-Centric Tensor Layout, and (2) Cache-Aware Streamed Lookup techniques. Evaluations on 5 edge devices across 3 LLMs show that Vec-LUT outperforms state-of-the-art baselines by up to $4.2\\times$. Our implementation is integrated into llama.cpp. The code is available at https://github.com/Cipherxzc/vlut.cpp.", "AI": {"tldr": "\u63d0\u51fa\u5411\u91cfLUT\u65b9\u6cd5\uff0c\u4f18\u5316\u8d85\u4f4e\u4f4d\u5bbdLLM\u5728\u8fb9\u7f18\u8bbe\u5907\u4e0a\u7684\u5e76\u884c\u63a8\u7406\u6548\u7387\u3002", "motivation": "\u89e3\u51b3\u57fa\u4e8e\u6807\u91cfLUT\u7684\u63a8\u7406\u5728\u591atoken\u573a\u666f\u4e0b\u5185\u5b58\u5e26\u5bbd\u5229\u7528\u7387\u4f4e\u7684\u95ee\u9898\u3002", "method": "\u5f15\u5165\u5411\u91cfLUT\u8303\u5f0f\u3001\u4ee5\u5411\u91cfLUT\u4e3a\u4e2d\u5fc3\u7684\u5f20\u91cf\u5e03\u5c40\u548c\u7f13\u5b58\u611f\u77e5\u6d41\u5f0f\u67e5\u627e\u6280\u672f\u3002", "result": "\u57285\u79cd\u8fb9\u7f18\u8bbe\u5907\u4e0a\u5b9e\u73b0\u6700\u9ad84.2\u500d\u6027\u80fd\u63d0\u5347\uff0c\u5e76\u96c6\u6210\u8fdbllama.cpp\u3002", "conclusion": "\u5411\u91cfLUT\u663e\u8457\u63d0\u5347\u8d85\u4f4e\u4f4d\u5bbdLLM\u63a8\u7406\u6548\u7387\uff0c\u63a8\u52a8\u7aef\u4fa7\u667a\u80fd\u666e\u53ca\u3002"}}
{"id": "2512.06093", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2512.06093", "abs": "https://arxiv.org/abs/2512.06093", "authors": ["Boyu Li", "Zongwei Zhu", "Yi Xiong", "Qianyue Cao", "Jiawei Geng", "Xiaonan Zhang", "Xi Li"], "title": "Compass: Mapping Space Exploration for Multi-Chiplet Accelerators Targeting LLM Inference Serving Workloads", "comment": null, "summary": "Large Language Models (LLMs) impose massive computational demands, driving the need for scalable multi-chiplet accelerators. However, existing mapping space exploration efforts for such accelerators primarily focus on traditional CNN/Transformer workloads and fail to adequately support the dynamic behaviors of mixed request types and variable sequence lengths in real-world LLM inference serving. To bridge this gap, we first propose a computation execution graph-based mapping encoding scheme that decouples micro-batches and layers, enabling fine-grained execution control on heterogeneous chiplets and flexibly representing various parallelism strategies. Second, building upon this scheme, we develop the Compass framework, which integrates an evaluation engine and a genetic algorithm-based mapping generation engine to achieve efficient mapping search. Compared to state-of-the-art works, our solution achieves an average EDP reduction of 63.12%.", "AI": {"tldr": "\u63d0\u51faCompass\u6846\u67b6\uff0c\u901a\u8fc7\u8ba1\u7b97\u6267\u884c\u56fe\u7f16\u7801\u548c\u9057\u4f20\u7b97\u6cd5\u4f18\u5316\u6620\u5c04\uff0c\u663e\u8457\u964d\u4f4eLLM\u63a8\u7406\u80fd\u8017\u5ef6\u8fdf\u79ef\u3002", "motivation": "\u73b0\u6709\u52a0\u901f\u5668\u6620\u5c04\u65b9\u6cd5\u65e0\u6cd5\u6709\u6548\u652f\u6301LLM\u63a8\u7406\u4e2d\u6df7\u5408\u8bf7\u6c42\u7c7b\u578b\u4e0e\u53ef\u53d8\u5e8f\u5217\u957f\u5ea6\u7684\u52a8\u6001\u7279\u6027\u3002", "method": "\u91c7\u7528\u8ba1\u7b97\u6267\u884c\u56fe\u7f16\u7801\u65b9\u6848\u89e3\u8026\u5fae\u6279\u6b21\u4e0e\u5c42\uff0c\u5e76\u7ed3\u5408\u9057\u4f20\u7b97\u6cd5\u5b9e\u73b0\u9ad8\u6548\u6620\u5c04\u641c\u7d22\u3002", "result": "\u76f8\u6bd4\u73b0\u6709\u6700\u4f18\u65b9\u6848\uff0c\u5e73\u5747EDP\u964d\u4f4e63.12%\u3002", "conclusion": "\u8be5\u6846\u67b6\u80fd\u7075\u6d3b\u9002\u914d\u5f02\u6784\u82af\u7247\u5e76\u9ad8\u6548\u652f\u6301LLM\u52a8\u6001\u63a8\u7406\u9700\u6c42\u3002"}}
{"id": "2512.06699", "categories": ["cs.PF", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.06699", "abs": "https://arxiv.org/abs/2512.06699", "authors": ["Karthik Prabhakar"], "title": "Predictive Modeling of I/O Performance for Machine Learning Training Pipelines: A Data-Driven Approach to Storage Optimization", "comment": "20 pages, 10 figures", "summary": "Modern machine learning training is increasingly bottlenecked by data I/O rather than compute. GPUs often sit idle at below 50% utilization waiting for data. This paper presents a machine learning approach to predict I/O performance and recommend optimal storage configurations for ML training pipelines. We collected 141 observations through systematic benchmarking across different storage backends (NVMe SSD, network-attached storage, in-memory filesystems), data formats, and access patterns, covering both low-level I/O operations and full training pipelines. After evaluating seven regression models and three classification approaches, XGBoost achieved the best performance with R-squared of 0.991, predicting I/O throughput within 11.8% error on average. Feature importance analysis revealed that throughput metrics and batch size are the primary performance drivers. This data-driven approach can reduce configuration time from days of trial-and-error to minutes of predictive recommendation. The methodology is reproducible and extensible to other resource management problems in ML systems. Code and data are available at https://github.com/knkarthik01/gpu_storage_ml_project", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u673a\u5668\u5b66\u4e60\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7XGBoost\u6a21\u578b\u9884\u6d4bI/O\u6027\u80fd\u5e76\u63a8\u8350\u6700\u4f18\u5b58\u50a8\u914d\u7f6e\uff0c\u663e\u8457\u7f29\u77edML\u8bad\u7ec3\u7ba1\u9053\u7684\u8c03\u4f18\u65f6\u95f4\u3002", "motivation": "\u73b0\u4ee3\u673a\u5668\u5b66\u4e60\u8bad\u7ec3\u5e38\u53d7\u9650\u4e8e\u6570\u636eI/O\u74f6\u9888\uff0c\u5bfc\u81f4GPU\u5229\u7528\u7387\u4f4e\u4e0b\uff0c\u9700\u9ad8\u6548\u9884\u6d4b\u4e0e\u4f18\u5316\u5b58\u50a8\u914d\u7f6e\u3002", "method": "\u7cfb\u7edf\u6027\u57fa\u51c6\u6d4b\u8bd5\u6536\u96c6141\u7ec4\u6570\u636e\uff0c\u6db5\u76d6\u591a\u79cd\u5b58\u50a8\u540e\u7aef\u3001\u6570\u636e\u683c\u5f0f\u4e0e\u8bbf\u95ee\u6a21\u5f0f\uff0c\u5e76\u8bc4\u4f307\u79cd\u56de\u5f52\u4e0e3\u79cd\u5206\u7c7b\u6a21\u578b\uff0c\u6700\u7ec8\u9009\u7528XGBoost\u8fdb\u884c\u9884\u6d4b\u3002", "result": "XGBoost\u6a21\u578b\u8fbe\u5230R\u00b2=0.991\uff0c\u5e73\u5747\u8bef\u5dee\u4ec511.8%\uff0c\u5173\u952e\u7279\u5f81\u4e3a\u541e\u5410\u91cf\u6307\u6807\u4e0e\u6279\u5927\u5c0f\u3002", "conclusion": "\u8be5\u6570\u636e\u9a71\u52a8\u65b9\u6cd5\u53ef\u5c06\u914d\u7f6e\u65f6\u95f4\u4ece\u6570\u5929\u7f29\u77ed\u81f3\u6570\u5206\u949f\uff0c\u5177\u5907\u53ef\u590d\u73b0\u6027\u4e0e\u6269\u5c55\u6027\uff0c\u9002\u7528\u4e8e\u5176\u4ed6ML\u7cfb\u7edf\u8d44\u6e90\u7ba1\u7406\u95ee\u9898\u3002"}}
{"id": "2512.06178", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2512.06178", "abs": "https://arxiv.org/abs/2512.06178", "authors": ["Georgiana Haldeman", "Peter Ohmann", "Paul Denny"], "title": "Systematically Thinking about the Complexity of Code Structuring Exercises at Introductory Level", "comment": null, "summary": "Decomposition and abstraction is an essential component of computational thinking, yet it is not always emphasized in introductory programming courses. In addition, as generative AI further reduces the focus on syntax and increases the importance of higher-level code reasoning, there is renewed opportunity to teach DA explicitly. In this paper, we introduce a framework for systematically assessing the complexity of code structuring tasks, where students must identify and separate meaningful abstractions within existing, unstructured code. The framework defines three dimensions of task complexity, each with multiple levels: repetition, code pattern, and data dependency. To support practical use, we provide example tasks mapped to these levels and offer an interactive tool for generating and exploring DA problems. The framework is designed to support the development of educational tasks that build students' skills with DA in the procedural paradigm.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u4e2a\u8bc4\u4f30\u4ee3\u7801\u7ed3\u6784\u4efb\u52a1\u590d\u6742\u6027\u7684\u6846\u67b6\uff0c\u4ee5\u5e2e\u52a9\u5b66\u751f\u5728\u7a0b\u5e8f\u8bbe\u8ba1\u4e2d\u638c\u63e1\u5206\u89e3\u4e0e\u62bd\u8c61\u6280\u80fd\u3002", "motivation": "\u968f\u7740\u751f\u6210\u5f0fAI\u964d\u4f4e\u8bed\u6cd5\u5173\u6ce8\u3001\u63d0\u5347\u9ad8\u5c42\u6b21\u4ee3\u7801\u63a8\u7406\u7684\u91cd\u8981\u6027\uff0c\u6709\u5fc5\u8981\u5728\u7f16\u7a0b\u8bfe\u7a0b\u4e2d\u660e\u786e\u6559\u6388\u5206\u89e3\u4e0e\u62bd\u8c61\u3002", "method": "\u6784\u5efa\u5305\u542b\u91cd\u590d\u6027\u3001\u4ee3\u7801\u6a21\u5f0f\u548c\u6570\u636e\u4f9d\u8d56\u4e09\u4e2a\u7ef4\u5ea6\u7684\u590d\u6742\u5ea6\u8bc4\u4f30\u6846\u67b6\uff0c\u5e76\u63d0\u4f9b\u914d\u5957\u793a\u4f8b\u4efb\u52a1\u4e0e\u4ea4\u4e92\u5de5\u5177\u3002", "result": "\u8be5\u6846\u67b6\u80fd\u6709\u6548\u652f\u6301\u6559\u80b2\u8005\u8bbe\u8ba1\u5faa\u5e8f\u6e10\u8fdb\u7684DA\u8bad\u7ec3\u4efb\u52a1\uff0c\u63d0\u5347\u5b66\u751f\u5728\u8fc7\u7a0b\u5f0f\u7f16\u7a0b\u4e2d\u7684\u62bd\u8c61\u80fd\u529b\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u7cfb\u7edf\u5316\u6559\u5b66\u5206\u89e3\u4e0e\u62bd\u8c61\u63d0\u4f9b\u4e86\u5b9e\u7528\u57fa\u7840\uff0c\u6709\u52a9\u4e8e\u5f3a\u5316\u8ba1\u7b97\u601d\u7ef4\u6838\u5fc3\u80fd\u529b\u7684\u57f9\u517b\u3002"}}
{"id": "2512.06432", "categories": ["cs.MA"], "pdf": "https://arxiv.org/pdf/2512.06432", "abs": "https://arxiv.org/abs/2512.06432", "authors": ["Yihan Xia", "Taotao Wang", "Shengli Zhang", "Zhangyuhua Weng", "Bin Cao", "Soung Chang Liew"], "title": "HiveMind: Contribution-Guided Online Prompt Optimization of LLM Multi-Agent Systems", "comment": "This paper was accepted to AAAI2026", "summary": "Recent advances in LLM-based multi-agent systems have demonstrated remarkable capabilities in complex decision-making scenarios such as financial trading and software engineering. However, evaluating each individual agent's effectiveness and online optimization of underperforming agents remain open challenges. To address these issues, we present HiveMind, a self-adaptive framework designed to optimize LLM multi-agent collaboration through contribution analysis. At its core, HiveMind introduces Contribution-Guided Online Prompt Optimization (CG-OPO), which autonomously refines agent prompts based on their quantified contributions. We first propose the Shapley value as a grounded metric to quantify each agent's contribution, thereby identifying underperforming agents in a principled manner for automated prompt refinement. To overcome the computational complexity of the classical Shapley value, we present DAG-Shapley, a novel and efficient attribution algorithm that leverages the inherent Directed Acyclic Graph structure of the agent workflow to axiomatically prune non-viable coalitions. By hierarchically reusing intermediate outputs of agents in the DAG, our method further reduces redundant computations, and achieving substantial cost savings without compromising the theoretical guarantees of Shapley values. Evaluated in a multi-agent stock-trading scenario, HiveMind achieves superior performance compared to static baselines. Notably, DAG-Shapley reduces LLM calls by over 80\\% while maintaining attribution accuracy comparable to full Shapley values, establishing a new standard for efficient credit assignment and enabling scalable, real-world optimization of multi-agent collaboration.", "AI": {"tldr": "HiveMind\u6846\u67b6\u901a\u8fc7\u8d21\u732e\u5206\u6790\u5b9e\u73b0LLM\u591a\u667a\u80fd\u4f53\u534f\u4f5c\u7684\u81ea\u9002\u5e94\u4f18\u5316\uff0c\u63d0\u51faDAG-Shapley\u7b97\u6cd5\u9ad8\u6548\u8ba1\u7b97Shapley\u503c\uff0c\u663e\u8457\u964d\u4f4e\u8ba1\u7b97\u6210\u672c\u5e76\u4fdd\u6301\u9ad8\u7cbe\u5ea6\u3002", "motivation": "\u73b0\u6709LLM\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u7f3a\u4e4f\u5bf9\u4e2a\u4f53\u6548\u80fd\u7684\u8bc4\u4f30\u4e0e\u5728\u7ebf\u4f18\u5316\u673a\u5236\uff0c\u96be\u4ee5\u652f\u6301\u590d\u6742\u534f\u4f5c\u573a\u666f\u4e2d\u7684\u52a8\u6001\u8c03\u6574\u3002", "method": "\u5f15\u5165\u57fa\u4e8eShapley\u503c\u7684\u8d21\u732e\u5ea6\u91cf\uff0c\u5e76\u8bbe\u8ba1DAG-Shapley\u7b97\u6cd5\uff0c\u5229\u7528\u6709\u5411\u65e0\u73af\u56fe\u7ed3\u6784\u526a\u679d\u65e0\u6548\u8054\u76df\uff0c\u590d\u7528\u4e2d\u95f4\u8f93\u51fa\u4ee5\u51cf\u5c11\u5197\u4f59\u8ba1\u7b97\u3002", "result": "\u5728\u80a1\u7968\u4ea4\u6613\u573a\u666f\u4e2d\u6027\u80fd\u4f18\u4e8e\u9759\u6001\u57fa\u7ebf\uff0cDAG-Shapley\u51cf\u5c1180%\u4ee5\u4e0aLLM\u8c03\u7528\u4e14\u4fdd\u6301\u4e0e\u5b8c\u6574Shapley\u503c\u76f8\u5f53\u7684\u5f52\u56e0\u7cbe\u5ea6\u3002", "conclusion": "HiveMind\u4e3a\u591a\u667a\u80fd\u4f53\u534f\u4f5c\u63d0\u4f9b\u4e86\u9ad8\u6548\u3001\u53ef\u6269\u5c55\u7684\u5728\u7ebf\u4f18\u5316\u65b9\u6848\uff0c\u63a8\u52a8\u4e86LLM\u667a\u80fd\u4f53\u7cfb\u7edf\u5728\u73b0\u5b9e\u573a\u666f\u4e2d\u7684\u843d\u5730\u5e94\u7528\u3002"}}
{"id": "2512.06113", "categories": ["cs.AR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.06113", "abs": "https://arxiv.org/abs/2512.06113", "authors": ["Bin Xu", "Ayan Banerjee", "Sandeep Gupta"], "title": "Hardware Software Optimizations for Fast Model Recovery on Reconfigurable Architectures", "comment": null, "summary": "Model Recovery (MR) is a core primitive for physical AI and real-time digital twins, but GPUs often execute MR inefficiently due to iterative dependencies, kernel-launch overheads, underutilized memory bandwidth, and high data-movement latency. We present MERINDA, an FPGA-accelerated MR framework that restructures computation as a streaming dataflow pipeline. MERINDA exploits on-chip locality through BRAM tiling, fixed-point kernels, and the concurrent use of LUT fabric and carry-chain adders to expose fine-grained spatial parallelism while minimizing off-chip traffic. This hardware-aware formulation removes synchronization bottlenecks and sustains high throughput across the iterative updates in MR. On representative MR workloads, MERINDA delivers up to 6.3x fewer cycles than an FPGA-based LTC baseline, enabling real-time performance for time-critical physical systems.", "AI": {"tldr": "MERINDA\u662f\u4e00\u4e2a\u57fa\u4e8eFPGA\u7684\u6a21\u578b\u6062\u590d\u52a0\u901f\u6846\u67b6\uff0c\u901a\u8fc7\u6d41\u5f0f\u6570\u636e\u6d41\u6c34\u7ebf\u548c\u786c\u4ef6\u611f\u77e5\u8bbe\u8ba1\u663e\u8457\u63d0\u5347\u6548\u7387\u3002", "motivation": "\u89e3\u51b3GPU\u5728\u6a21\u578b\u6062\u590d\u4efb\u52a1\u4e2d\u56e0\u8fed\u4ee3\u4f9d\u8d56\u3001\u5185\u6838\u542f\u52a8\u5f00\u9500\u7b49\u95ee\u9898\u5bfc\u81f4\u7684\u4f4e\u6548\u6267\u884c\u3002", "method": "\u91c7\u7528BRAM\u5206\u5757\u3001\u5b9a\u70b9\u8ba1\u7b97\u6838\u3001LUT\u4e0e\u8fdb\u4f4d\u94fe\u52a0\u6cd5\u5668\u5e76\u884c\u7b49\u6280\u672f\u6784\u5efa\u6d41\u5f0f\u6570\u636e\u901a\u8def\uff0c\u51cf\u5c11\u7247\u5916\u8bbf\u5b58\u4e0e\u540c\u6b65\u74f6\u9888\u3002", "result": "\u5728\u5178\u578b\u5de5\u4f5c\u8d1f\u8f7d\u4e0a\u6bd4FPGA\u57fa\u7ebf\u65b9\u6848\u51cf\u5c11\u6700\u591a6.3\u500d\u5468\u671f\u6570\uff0c\u5b9e\u73b0\u5b9e\u65f6\u6027\u80fd\u3002", "conclusion": "MERINDA\u80fd\u9ad8\u6548\u652f\u6301\u7269\u7406AI\u4e0e\u5b9e\u65f6\u6570\u5b57\u5b6a\u751f\u4e2d\u7684\u6a21\u578b\u6062\u590d\u9700\u6c42\u3002"}}
{"id": "2512.07449", "categories": ["cs.PF"], "pdf": "https://arxiv.org/pdf/2512.07449", "abs": "https://arxiv.org/abs/2512.07449", "authors": ["Mukta Debnath", "Krishnendu Guha", "Debasri Saha", "Amlan Chakrabarti", "Susmita Sur-Kolay"], "title": "AFarePart: Accuracy-aware Fault-resilient Partitioner for DNN Edge Accelerators", "comment": "6 pages, 4 figures, 2 tables", "summary": "Deep Neural Networks (DNNs) are increasingly deployed across distributed and resource-constrained platforms, such as System-on-Chip (SoC) accelerators and edge-cloud systems. DNNs are often partitioned and executed across heterogeneous processing units to optimize latency and energy. However, the reliability of these partitioned models under hardware faults and communication errors remains a critical yet underexplored topic, especially in safety-critical applications. In this paper, we propose an accuracy-aware, fault-resilient DNN partitioning framework targeting multi-objective optimization using NSGA-II, where accuracy degradation under fault conditions is introduced as a core metric alongside energy and latency. Our framework performs runtime fault injection during optimization and utilizes a feedback loop to prioritize fault-tolerant partitioning. We evaluate our approach on benchmark CNNs including AlexNet, SqueezeNet and ResNet18 on hardware accelerators, and demonstrate up to 27.7% improvement in fault tolerance with minimal increase in performance overhead. Our results highlight the importance of incorporating resilience into DNN partitioning, and thereby paving the way for robust AI inference in error-prone environments.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u517c\u987e\u51c6\u786e\u7387\u3001\u5bb9\u9519\u6027\u7684\u591a\u76ee\u6807DNN\u5212\u5206\u6846\u67b6\uff0c\u663e\u8457\u63d0\u5347\u6a21\u578b\u5728\u6545\u969c\u73af\u5883\u4e0b\u7684\u9c81\u68d2\u6027\u3002", "motivation": "\u5206\u5e03\u5f0f\u8d44\u6e90\u53d7\u9650\u5e73\u53f0\u4e2dDNN\u5212\u5206\u7684\u53ef\u9760\u6027\u95ee\u9898\u5c1a\u672a\u5145\u5206\u7814\u7a76\uff0c\u5c24\u5176\u5728\u5b89\u5168\u5173\u952e\u573a\u666f\u4e0b\u4e9f\u9700\u89e3\u51b3\u3002", "method": "\u91c7\u7528NSGA-II\u7b97\u6cd5\u8fdb\u884c\u591a\u76ee\u6807\u4f18\u5316\uff0c\u5c06\u6545\u969c\u6761\u4ef6\u4e0b\u7684\u7cbe\u5ea6\u4e0b\u964d\u4f5c\u4e3a\u6838\u5fc3\u6307\u6807\uff0c\u5e76\u901a\u8fc7\u8fd0\u884c\u65f6\u6545\u969c\u6ce8\u5165\u4e0e\u53cd\u9988\u673a\u5236\u4f18\u5148\u9009\u62e9\u5bb9\u9519\u5212\u5206\u65b9\u6848\u3002", "result": "\u5728AlexNet\u3001SqueezeNet\u548cResNet18\u4e0a\u9a8c\u8bc1\uff0c\u5bb9\u9519\u80fd\u529b\u6700\u9ad8\u63d0\u534727.7%\uff0c\u6027\u80fd\u5f00\u9500\u589e\u52a0\u6781\u5c0f\u3002", "conclusion": "\u5c06\u5bb9\u9519\u6027\u7eb3\u5165DNN\u5212\u5206\u6846\u67b6\uff0c\u53ef\u4e3a\u6613\u9519\u73af\u5883\u4e2d\u7684\u7a33\u5065AI\u63a8\u7406\u5960\u5b9a\u57fa\u7840\u3002"}}
{"id": "2512.06800", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2512.06800", "abs": "https://arxiv.org/abs/2512.06800", "authors": ["Deepa Gurung", "S M Zia Ur Rashid", "Zain ul Abdeen", "Suman Rath"], "title": "Cloud Revolution: Tracing the Origins and Rise of Cloud Computing", "comment": null, "summary": "The history behind the development of cloud computing is more than several decades of technological progress in the fields of virtualization, distributed systems, and high-speed networking, but its current application is much broader than the underlying technologies that made it possible. This paper reexamines the historical evolution of the field, including the initial ideas of resource sharing and utility-based computing approaches and the development of hyperscale data centers and modern globally federated cloud ecosystems. We also analyze the technological and economic forces and point to the way cloud platforms altered the organizational computing habits, decreasing the entrance-level to the data-intensive and computation-heavy apps. The study also takes into account the ongoing limitations which have come with the large-scale adoption of clouds which include exposure to security due to the weaknesses in configuration, particular establishment regulations, and structural reliance on the single vendors. Lastly, we address some of the new trends that are transforming the cloud environment, including the convergence of edge and cloud infrastructure, the increased prominence of AI-optimised architectures and the initial adoption of quantum computing services. Collectively, the developments above describe an emerging but quickly changing paradigm with its future direction being determined by a strike of balancing between scalability, openness, and trust.", "AI": {"tldr": "\u672c\u6587\u56de\u987e\u4e91\u8ba1\u7b97\u53d1\u5c55\u5386\u7a0b\uff0c\u5206\u6790\u5176\u6280\u672f\u7ecf\u6d4e\u5f71\u54cd\u53ca\u5f53\u524d\u6311\u6218\uff0c\u5e76\u63a2\u8ba8\u8fb9\u7f18\u8ba1\u7b97\u3001AI\u4f18\u5316\u67b6\u6784\u548c\u91cf\u5b50\u8ba1\u7b97\u7b49\u65b0\u8d8b\u52bf\u5982\u4f55\u5851\u9020\u672a\u6765\u4e91\u73af\u5883\u3002", "motivation": "\u91cd\u65b0\u5ba1\u89c6\u4e91\u8ba1\u7b97\u7684\u5386\u53f2\u6f14\u8fdb\u53ca\u5176\u5bf9\u7ec4\u7ec7\u8ba1\u7b97\u6a21\u5f0f\u7684\u6df1\u8fdc\u5f71\u54cd\uff0c\u540c\u65f6\u63a2\u8ba8\u5f53\u524d\u5c40\u9650\u4e0e\u672a\u6765\u53d1\u5c55\u65b9\u5411\u3002", "method": "\u901a\u8fc7\u5386\u53f2\u56de\u987e\u4e0e\u8d8b\u52bf\u5206\u6790\uff0c\u7ed3\u5408\u6280\u672f\u7ecf\u6d4e\u89c6\u89d2\uff0c\u7cfb\u7edf\u68b3\u7406\u4e91\u8ba1\u7b97\u53d1\u5c55\u8109\u7edc\u4e0e\u65b0\u5174\u53d8\u9769\u529b\u91cf\u3002", "result": "\u63ed\u793a\u4e91\u8ba1\u7b97\u5df2\u4ece\u57fa\u7840\u6280\u672f\u6f14\u53d8\u4e3a\u5168\u7403\u6027\u751f\u6001\u4f53\u7cfb\uff0c\u6b63\u9762\u4e34\u5b89\u5168\u3001\u5408\u89c4\u4e0e\u4f9b\u5e94\u5546\u9501\u5b9a\u7b49\u6311\u6218\uff0c\u540c\u65f6\u53d7\u8fb9\u7f18\u878d\u5408\u3001AI\u67b6\u6784\u4e0e\u91cf\u5b50\u670d\u52a1\u7b49\u8d8b\u52bf\u9a71\u52a8\u8f6c\u578b\u3002", "conclusion": "\u4e91\u8ba1\u7b97\u6b63\u5904\u4e8e\u5feb\u901f\u6f14\u53d8\u9636\u6bb5\uff0c\u5176\u672a\u6765\u53d1\u5c55\u53d6\u51b3\u4e8e\u5728\u53ef\u6269\u5c55\u6027\u3001\u5f00\u653e\u6027\u4e0e\u4fe1\u4efb\u4e4b\u95f4\u53d6\u5f97\u5e73\u8861\u3002"}}
{"id": "2512.06177", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2512.06177", "abs": "https://arxiv.org/abs/2512.06177", "authors": ["Jiahan Xie", "Evan Williams", "Adrian Sampson"], "title": "From PyTorch to Calyx: An Open-Source Compiler Toolchain for ML Accelerators", "comment": "5 pages, 3 figures", "summary": "We present an end-to-end open-source compiler toolchain that targets synthesizable SystemVerilog from ML models written in PyTorch. Our toolchain leverages the accelerator design language Allo, the hardware intermediate representation (IR) Calyx, and the CIRCT project under LLVM. We also implement a set of compiler passes for memory partitioning, enabling effective parallelism in memory-intensive ML workloads. Experimental results demonstrate that our compiler can effectively generate optimized FPGA-implementable hardware designs that perform reasonably well against closed-source industry-grade tools such as Vitis HLS.", "AI": {"tldr": "\u5f00\u6e90\u7f16\u8bd1\u5668\u5de5\u5177\u94fe\u5c06PyTorch\u6a21\u578b\u8f6c\u4e3a\u53ef\u7efc\u5408SystemVerilog\uff0c\u652f\u6301FPGA\u9ad8\u6548\u5b9e\u73b0\u3002", "motivation": "\u63d0\u5347ML\u6a21\u578b\u5728FPGA\u4e0a\u7684\u90e8\u7f72\u6548\u7387\uff0c\u66ff\u4ee3\u95ed\u6e90\u5de5\u4e1a\u5de5\u5177\u3002", "method": "\u7ed3\u5408Allo\u3001Calyx\u4e0eCIRCT\uff0c\u5b9e\u73b0\u5185\u5b58\u5206\u533a\u4f18\u5316\u4e0e\u5e76\u884c\u52a0\u901f\u3002", "result": "\u751f\u6210\u786c\u4ef6\u8bbe\u8ba1\u6027\u80fd\u5ab2\u7f8eVitis HLS\u7b49\u95ed\u6e90\u5de5\u5177\u3002", "conclusion": "\u8be5\u5de5\u5177\u94fe\u6709\u6548\u652f\u6301\u7aef\u5230\u7aefML\u6a21\u578b\u5230FPGA\u786c\u4ef6\u7684\u81ea\u52a8\u5316\u7f16\u8bd1\u3002"}}
{"id": "2512.07622", "categories": ["cs.AR", "cs.PF"], "pdf": "https://arxiv.org/pdf/2512.07622", "abs": "https://arxiv.org/abs/2512.07622", "authors": ["Martha Semken", "Mariano Vargas", "Ignacio Tula", "Giuliana Zorzoli", "Andr\u00e9s Rojas Paredes"], "title": "An\u00e1lisis de rendimiento y eficiencia energ\u00e9tica en el cluster Raspberry Pi Cronos", "comment": "in Spanish language", "summary": "This article presents an evaluation of the computational performance and energy efficiency of the Cronos cluster, composed of Raspberry Pi4 and 3b microcomputers designed for educational purposes. Experimental tests were performed using the High Performance Linpack (HPL) benchmark, under a resource management environment configured with Slurm and parallel communication via Open MPI. The study focuses on analyzing scalability, stability, and power consumption during the execution of computationally intensive workloads, considering different node configurations. The results show that the cluster achieves a performance of up to 6.91 GFLOPS in homogeneous configurations of 6 Raspberry Pi 4 nodes, and that the use of heterogeneous nodes (including Raspberry Pi 3b) can negatively impact stability and efficiency. Additionally, the total electrical consumption of the system was measured during the runs, allowing for the estimation of the performance-to-consumption ratio (GFLOPS/W) as a comparative metric. This study constitutes a concrete contribution to the design, evaluation, and utilization of low-cost ARM clusters in educational and research contexts.", "AI": {"tldr": "\u672c\u6587\u8bc4\u4f30\u4e86\u7531\u6811\u8393\u6d3e4\u548c3b\u7ec4\u6210\u7684Cronos\u96c6\u7fa4\u5728\u6559\u80b2\u7528\u9014\u4e0b\u7684\u8ba1\u7b97\u6027\u80fd\u4e0e\u80fd\u6548\uff0c\u901a\u8fc7HPL\u57fa\u51c6\u6d4b\u8bd5\u53d1\u73b06\u8282\u70b9\u540c\u6784\u914d\u7f6e\u53ef\u8fbe6.91 GFLOPS\uff0c\u5f02\u6784\u8282\u70b9\u5219\u964d\u4f4e\u7a33\u5b9a\u6027\u548c\u6548\u7387\uff0c\u5e76\u63d0\u4f9b\u4e86GFLOPS/W\u80fd\u6548\u6bd4\u3002", "motivation": "\u63a2\u7d22\u4f4e\u6210\u672cARM\u96c6\u7fa4\u5728\u6559\u80b2\u548c\u79d1\u7814\u573a\u666f\u4e2d\u7684\u5b9e\u9645\u6027\u80fd\u4e0e\u80fd\u8017\u8868\u73b0\uff0c\u4e3a\u7c7b\u4f3c\u7cfb\u7edf\u8bbe\u8ba1\u63d0\u4f9b\u5b9e\u8bc1\u4f9d\u636e\u3002", "method": "\u4f7f\u7528HPL\u57fa\u51c6\uff0c\u5728Slurm\u8d44\u6e90\u7ba1\u7406\u548cOpen MPI\u5e76\u884c\u901a\u4fe1\u73af\u5883\u4e0b\uff0c\u6d4b\u8bd5\u4e0d\u540c\u8282\u70b9\u914d\u7f6e\u7684\u6269\u5c55\u6027\u3001\u7a33\u5b9a\u6027\u53ca\u529f\u8017\u3002", "result": "6\u4e2a\u6811\u8393\u6d3e4\u8282\u70b9\u540c\u6784\u96c6\u7fa4\u6700\u9ad8\u8fbe6.91 GFLOPS\uff1b\u6df7\u5165\u6811\u8393\u6d3e3b\u4f1a\u964d\u4f4e\u7a33\u5b9a\u6027\u548c\u6548\u7387\uff1b\u6d4b\u5f97\u6574\u673a\u529f\u8017\u5e76\u8ba1\u7b97\u4e86GFLOPS/W\u80fd\u6548\u6307\u6807\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u4f4e\u6210\u672cARM\u96c6\u7fa4\u5728\u6559\u5b66\u4e0e\u79d1\u7814\u4e2d\u7684\u8bbe\u8ba1\u4e0e\u90e8\u7f72\u63d0\u4f9b\u4e86\u5b9e\u7528\u53c2\u8003\uff0c\u5f3a\u8c03\u540c\u6784\u914d\u7f6e\u66f4\u4f18\u4e14\u9700\u5173\u6ce8\u80fd\u6548\u5e73\u8861\u3002"}}
{"id": "2512.06248", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2512.06248", "abs": "https://arxiv.org/abs/2512.06248", "authors": ["Cheng Cheng", "Jinqiu Yang"], "title": "CFCEval: Evaluating Security Aspects in Code Generated by Large Language Models", "comment": null, "summary": "Code-focused Large Language Models (LLMs), such as CodeX and Star-Coder, have demonstrated remarkable capabilities in enhancing developer productivity through context-aware code generation. However, evaluating the quality and security of LLM-generated code remains a significant challenge. Existing evaluation protocols for Code LLMs lack both methodological rigor and comprehensive scope. A key limitation is dataset bias, which arises from unintentional overlap between training and testing data. Furthermore, while CodeBLEU, a BLEU-based metric, is widely used to assess code similarity, it suffers from critical shortcomings, including imprecise tokenization, structural limitations, and low reference diversity. To address these challenges, we introduce CFCEval, a novel framework for evaluating the quality and security of code generated by LLMs. CFCEval mitigates dataset bias by creating a new benchmark, MLVBench, and incorporates ELRM, a new metric designed to assess the relevance between reference code and generated code. CFCEval evaluates generated code across four dimensions: programming quality, vulnerability-fixing capability, post-transformation fixing capability, and relevance. Our experiments show that CFCEval not only captures both quality and security aspects of generated code more effectively but also that its ELRM aligns more closely with human judgments than CodeBLEU, thus paving the way for future advancements in Code LLMs evaluation.", "AI": {"tldr": "CFCEval\u662f\u4e00\u4e2a\u7528\u4e8e\u8bc4\u4f30\u4ee3\u7801\u751f\u6210\u5927\u6a21\u578b\u8d28\u91cf\u548c\u5b89\u5168\u6027\u7684\u65b0\u6846\u67b6\uff0c\u901a\u8fc7\u5f15\u5165MLVBench\u57fa\u51c6\u548cELRM\u6307\u6807\uff0c\u66f4\u5168\u9762\u51c6\u786e\u5730\u8861\u91cf\u751f\u6210\u4ee3\u7801\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u4ee3\u7801\u5927\u6a21\u578b\u8bc4\u4f30\u65b9\u6cd5\u5b58\u5728\u6570\u636e\u96c6\u504f\u5dee\u548cCodeBLEU\u6307\u6807\u7f3a\u9677\uff0c\u96be\u4ee5\u5168\u9762\u8bc4\u4f30\u751f\u6210\u4ee3\u7801\u7684\u8d28\u91cf\u4e0e\u5b89\u5168\u6027\u3002", "method": "\u63d0\u51faCFCEval\u6846\u67b6\uff0c\u6784\u5efaMLVBench\u57fa\u51c6\u6d88\u9664\u6570\u636e\u504f\u5dee\uff0c\u8bbe\u8ba1ELRM\u6307\u6807\u8bc4\u4f30\u4ee3\u7801\u76f8\u5173\u6027\uff0c\u5e76\u4ece\u7f16\u7a0b\u8d28\u91cf\u3001\u6f0f\u6d1e\u4fee\u590d\u80fd\u529b\u3001\u8f6c\u6362\u540e\u4fee\u590d\u80fd\u529b\u548c\u76f8\u5173\u6027\u56db\u4e2a\u7ef4\u5ea6\u8fdb\u884c\u8bc4\u4f30\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660eCFCEval\u5728\u8d28\u91cf\u548c\u5b89\u5168\u6027\u8bc4\u4f30\u4e0a\u66f4\u6709\u6548\uff0c\u4e14ELRM\u6307\u6807\u6bd4CodeBLEU\u66f4\u8d34\u8fd1\u4eba\u7c7b\u5224\u65ad\u3002", "conclusion": "CFCEval\u4e3a\u672a\u6765\u4ee3\u7801\u5927\u6a21\u578b\u8bc4\u4f30\u63d0\u4f9b\u4e86\u66f4\u53ef\u9760\u7684\u65b9\u6cd5\u8bba\u57fa\u7840\u3002"}}
{"id": "2512.06645", "categories": ["cs.MA"], "pdf": "https://arxiv.org/pdf/2512.06645", "abs": "https://arxiv.org/abs/2512.06645", "authors": ["Muyang Fan"], "title": "Analyzing Collision Rates in Large-Scale Mixed Traffic Control via Multi-Agent Reinforcement Learning", "comment": null, "summary": "Vehicle collisions remain a major challenge in large-scale mixed traffic systems, especially when human-driven vehicles (HVs) and robotic vehicles (RVs) interact under dynamic and uncertain conditions. Although Multi-Agent Reinforcement Learning (MARL) offers promising capabilities for traffic signal control, ensuring safety in such environments remains difficult. As a direct indicator of traffic risk, the collision rate must be well understood and incorporated into traffic control design. This study investigates the primary factors influencing collision rates in a MARL-governed Mixed Traffic Control (MTC) network. We examine three dimensions: total vehicle count, signalized versus unsignalized intersection configurations, and turning-movement strategies. Through controlled simulation experiments, we evaluate how each factor affects collision likelihood. The results show that collision rates are sensitive to traffic density, the level of signal coordination, and turning-control design. These findings provide practical insights for improving the safety and robustness of MARL-based mixed traffic control systems, supporting the development of intelligent transportation systems in which both efficiency and safety are jointly optimized.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63a2\u8ba8\u4e86\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u5728\u6df7\u5408\u4ea4\u901a\u63a7\u5236\u4e2d\u5f71\u54cd\u78b0\u649e\u7387\u7684\u5173\u952e\u56e0\u7d20\uff0c\u4e3a\u63d0\u5347\u4ea4\u901a\u5b89\u5168\u4e0e\u6548\u7387\u63d0\u4f9b\u5b9e\u8df5\u6307\u5bfc\u3002", "motivation": "\u89e3\u51b3\u4eba\u7c7b\u9a7e\u9a76\u8f66\u8f86\u4e0e\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u5728\u52a8\u6001\u4e0d\u786e\u5b9a\u73af\u5883\u4e0b\u7684\u78b0\u649e\u98ce\u9669\u95ee\u9898\uff0c\u63d0\u9ad8\u6df7\u5408\u4ea4\u901a\u7cfb\u7edf\u7684\u5b89\u5168\u6027\u3002", "method": "\u901a\u8fc7\u53d7\u63a7\u4eff\u771f\u5b9e\u9a8c\uff0c\u5206\u6790\u8f66\u8f86\u603b\u6570\u3001\u4fe1\u53f7\u706f\u914d\u7f6e\u53ca\u8f6c\u5411\u7b56\u7565\u4e09\u4e2a\u7ef4\u5ea6\u5bf9\u78b0\u649e\u7387\u7684\u5f71\u54cd\u3002", "result": "\u78b0\u649e\u7387\u53d7\u4ea4\u901a\u5bc6\u5ea6\u3001\u4fe1\u53f7\u534f\u8c03\u7a0b\u5ea6\u548c\u8f6c\u5411\u63a7\u5236\u8bbe\u8ba1\u663e\u8457\u5f71\u54cd\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u6709\u52a9\u4e8e\u4f18\u5316\u57fa\u4e8eMARL\u7684\u6df7\u5408\u4ea4\u901a\u63a7\u5236\u7cfb\u7edf\uff0c\u5728\u4fdd\u969c\u6548\u7387\u7684\u540c\u65f6\u63d0\u5347\u5b89\u5168\u6027\u548c\u9c81\u68d2\u6027\u3002"}}
{"id": "2512.07009", "categories": ["cs.DC", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.07009", "abs": "https://arxiv.org/abs/2512.07009", "authors": ["Saeid Ghafouri", "Yuming Ding", "Katerine Diaz Chito", "Jes\u00fas Martinez del Rinc\u00f3n", "Niamh O'Connell", "Hans Vandierendonck"], "title": "Optimizing video analytics inference pipelines: a case study", "comment": "Accepted to the IEEE/ACM International Conference on Big Data Computing, Applications and Technologies (BDCAT 2025)", "summary": "Cost-effective and scalable video analytics are essential for precision livestock monitoring, where high-resolution footage and near-real-time monitoring needs from commercial farms generates substantial computational workloads. This paper presents a comprehensive case study on optimizing a poultry welfare monitoring system through system-level improvements across detection, tracking, clustering, and behavioral analysis modules. We introduce a set of optimizations, including multi-level parallelization, Optimizing code with substituting CPU code with GPU-accelerated code, vectorized clustering, and memory-efficient post-processing. Evaluated on real-world farm video footage, these changes deliver up to a 2x speedup across pipelines without compromising model accuracy. Our findings highlight practical strategies for building high-throughput, low-latency video inference systems that reduce infrastructure demands in agricultural and smart sensing deployments as well as other large-scale video analytics applications.", "AI": {"tldr": "\u901a\u8fc7\u7cfb\u7edf\u7ea7\u4f18\u5316\uff0c\u5b9e\u73b0\u5bb6\u79bd\u798f\u5229\u76d1\u6d4b\u7cfb\u7edf\u76842\u500d\u52a0\u901f\uff0c\u540c\u65f6\u4fdd\u6301\u6a21\u578b\u7cbe\u5ea6\u3002", "motivation": "\u7cbe\u51c6\u755c\u7267\u76d1\u6d4b\u9700\u8981\u9ad8\u5206\u8fa8\u7387\u89c6\u9891\u548c\u8fd1\u5b9e\u65f6\u5904\u7406\uff0c\u8ba1\u7b97\u8d1f\u8f7d\u5927\uff0c\u9700\u964d\u4f4e\u6210\u672c\u548c\u63d0\u9ad8\u53ef\u6269\u5c55\u6027\u3002", "method": "\u91c7\u7528\u591a\u7ea7\u5e76\u884c\u5316\u3001GPU\u52a0\u901f\u4ee3\u7801\u66ff\u6362CPU\u4ee3\u7801\u3001\u5411\u91cf\u5316\u805a\u7c7b\u548c\u5185\u5b58\u9ad8\u6548\u540e\u5904\u7406\u7b49\u4f18\u5316\u65b9\u6cd5\u3002", "result": "\u5728\u771f\u5b9e\u519c\u573a\u89c6\u9891\u6570\u636e\u4e0a\u6d4b\u8bd5\uff0c\u7ba1\u9053\u6027\u80fd\u63d0\u5347\u9ad8\u8fbe2\u500d\uff0c\u4e14\u4e0d\u635f\u5931\u6a21\u578b\u7cbe\u5ea6\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u519c\u4e1a\u53ca\u5176\u4ed6\u5927\u89c4\u6a21\u89c6\u9891\u5206\u6790\u5e94\u7528\u63d0\u4f9b\u4e86\u6784\u5efa\u9ad8\u541e\u5410\u3001\u4f4e\u5ef6\u8fdf\u63a8\u7406\u7cfb\u7edf\u7684\u5b9e\u7528\u7b56\u7565\u3002"}}
{"id": "2512.06208", "categories": ["cs.AR", "cs.LG", "hep-ex"], "pdf": "https://arxiv.org/pdf/2512.06208", "abs": "https://arxiv.org/abs/2512.06208", "authors": ["Ho Fung Tsoi", "Dylan Rankin", "Vladimir Loncar", "Philip Harris"], "title": "SparsePixels: Efficient Convolution for Sparse Data on FPGAs", "comment": "Under review", "summary": "Inference of standard CNNs on FPGAs often incurs high latency and a long initiation interval due to the deep nested loops required to densely convolve every input pixel regardless of its feature value, especially when the image size is large. However, in some image data, input features can be spatially sparse, and semantic information may occupy only a small fraction of the input pixels. In this case most computation would be wasted on empty regions. In this work, we introduce SparsePixels, a framework for efficient convolution for spatially sparse image data on FPGAs, targeting fast inference applications in constrained environments with latency requirements of microseconds or below. Our approach implements a special class of CNNs that selectively retain and compute on a small subset of pixels that are active while ignoring the rest. We show that, for example, in a neutrino physics dataset for identifying neutrino interactions in LArTPC images that have around 4k input pixels but are naturally very sparse, a standard CNN with a compact size of 4k parameters incurs an inference latency of 48.665 $\u03bc$s on an FPGA, whereas a sparse CNN of the same base architecture computing on less than 1% of the input pixels results in a $\\times 73$ inference speedup to 0.665 $\u03bc$s, with resource utilization well within on-chip budgets, trading only a small percent-level performance loss. At least one-order-of magnitude speedups with comparable performance are also demonstrated in similar datasets with sparse image patterns. This work aims to benefit future algorithm developments for fast and efficient data readout in modern experiments such as the trigger and data acquisition systems at the CERN Large Hadron Collider. For easy adoption, we have developed a library to support building sparse CNNs with quantization-aware training, as well as an HLS implementation for FPGA deployment.", "AI": {"tldr": "SparsePixels\u6846\u67b6\u901a\u8fc7\u9009\u62e9\u6027\u8ba1\u7b97\u7a00\u758f\u56fe\u50cf\u4e2d\u7684\u6d3b\u8dc3\u50cf\u7d20\uff0c\u663e\u8457\u52a0\u901fFPGA\u4e0a\u7684CNN\u63a8\u7406\uff0c\u9002\u7528\u4e8e\u5fae\u79d2\u7ea7\u5ef6\u8fdf\u9700\u6c42\u7684\u573a\u666f\u3002", "motivation": "\u6807\u51c6CNN\u5728FPGA\u4e0a\u5904\u7406\u7a00\u758f\u56fe\u50cf\u65f6\u56e0\u904d\u5386\u6240\u6709\u50cf\u7d20\u5bfc\u81f4\u9ad8\u5ef6\u8fdf\uff0c\u800c\u5b9e\u9645\u8bed\u4e49\u4fe1\u606f\u4ec5\u5360\u5c11\u6570\u50cf\u7d20\uff0c\u9020\u6210\u5927\u91cf\u65e0\u6548\u8ba1\u7b97\u3002", "method": "\u63d0\u51faSparsePixels\u6846\u67b6\uff0c\u6784\u5efa\u4e00\u7c7b\u7279\u6b8aCNN\uff0c\u4ec5\u4fdd\u7559\u5e76\u8ba1\u7b97\u6d3b\u8dc3\u50cf\u7d20\u5b50\u96c6\uff0c\u7ed3\u5408\u91cf\u5316\u611f\u77e5\u8bad\u7ec3\u4e0eHLS\u5b9e\u73b0FPGA\u90e8\u7f72\u3002", "result": "\u5728LArTPC\u4e2d\u5fae\u5b50\u6570\u636e\u96c6\u4e0a\u5b9e\u73b073\u500d\u63a8\u7406\u52a0\u901f\uff0848.665\u03bcs\u21920.665\u03bcs\uff09\uff0c\u8d44\u6e90\u5360\u7528\u4f4e\uff0c\u6027\u80fd\u635f\u5931\u4ec5\u767e\u5206\u6bd4\u7ea7\u522b\uff1b\u7c7b\u4f3c\u7a00\u758f\u6570\u636e\u96c6\u4ea6\u83b7\u81f3\u5c1110\u500d\u52a0\u901f\u3002", "conclusion": "\u8be5\u6846\u67b6\u53ef\u6709\u6548\u652f\u6301\u73b0\u4ee3\u5b9e\u9a8c\u5982LHC\u89e6\u53d1\u4e0e\u6570\u636e\u91c7\u96c6\u7cfb\u7edf\u7684\u9ad8\u6548\u5feb\u901f\u7b97\u6cd5\u5f00\u53d1\u3002"}}
{"id": "2512.07219", "categories": ["cs.MA", "cs.GT", "cs.RO", "eess.SY"], "pdf": "https://arxiv.org/pdf/2512.07219", "abs": "https://arxiv.org/abs/2512.07219", "authors": ["Sungyong Chung", "Alireza Talebpour", "Samer H. Hamdar"], "title": "Characterizing Lane-Changing Behavior in Mixed Traffic", "comment": null, "summary": "Characterizing and understanding lane-changing behavior in the presence of automated vehicles (AVs) is crucial to ensuring safety and efficiency in mixed traffic. Accordingly, this study aims to characterize the interactions between the lane-changing vehicle (active vehicle) and the vehicle directly impacted by the maneuver in the target lane (passive vehicle). Utilizing real-world trajectory data from the Waymo Open Motion Dataset (WOMD), this study explores patterns in lane-changing behavior and provides insight into how these behaviors evolve under different AV market penetration rates (MPRs). In particular, we propose a game-theoretic framework to analyze cooperative and defective behaviors in mixed traffic, applied to the 7,636 observed lane-changing events in the WOMD. First, we utilize k-means clustering to classify vehicles as cooperative or defective, revealing that the proportions of cooperative AVs are higher than those of HDVs in both active and passive roles. Next, we jointly estimate the utilities of active and passive vehicles to model their behaviors using the quantal response equilibrium framework. Empirical payoff tables are then constructed based on these utilities. Using these payoffs, we analyze the presence of social dilemmas and examine the evolution of cooperative behaviors using evolutionary game theory. Our results reveal the presence of social dilemmas in approximately 4% and 11% of lane-changing events for active and passive vehicles, respectively, with most classified as Stag Hunt or Prisoner's Dilemma (Chicken Game rarely observed). Moreover, the Monte Carlo simulation results show that repeated lane-changing interactions consistently lead to increased cooperative behavior over time, regardless of the AV penetration rate.", "AI": {"tldr": "\u672c\u7814\u7a76\u901a\u8fc7\u535a\u5f08\u8bba\u6846\u67b6\u5206\u6790\u6df7\u5408\u4ea4\u901a\u4e2d\u8f66\u9053\u53d8\u6362\u884c\u4e3a\uff0c\u53d1\u73b0\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u6bd4\u4eba\u5de5\u9a7e\u9a76\u8f66\u8f86\u66f4\u503e\u5411\u4e8e\u5408\u4f5c\uff0c\u5e76\u4e14\u91cd\u590d\u4ea4\u4e92\u4f1a\u4fc3\u8fdb\u5408\u4f5c\u884c\u4e3a\u589e\u52a0\u3002", "motivation": "\u4e3a\u4e86\u786e\u4fdd\u6df7\u5408\u4ea4\u901a\u4e2d\u7684\u5b89\u5168\u4e0e\u6548\u7387\uff0c\u9700\u8981\u6df1\u5165\u7406\u89e3\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u5b58\u5728\u65f6\u7684\u8f66\u9053\u53d8\u6362\u884c\u4e3a\u53ca\u5176\u4e92\u52a8\u6a21\u5f0f\u3002", "method": "\u5229\u7528Waymo\u5f00\u653e\u8fd0\u52a8\u6570\u636e\u96c6\u4e2d\u7684\u8f68\u8ff9\u6570\u636e\uff0c\u91c7\u7528k-means\u805a\u7c7b\u548c\u91cf\u5316\u54cd\u5e94\u5747\u8861\u6846\u67b6\u5efa\u6a21\uff0c\u5e76\u7ed3\u5408\u6f14\u5316\u535a\u5f08\u7406\u8bba\u5206\u6790\u5408\u4f5c\u884c\u4e3a\u6f14\u53d8\u3002", "result": "\u7ea64%\u548c11%\u7684\u8f66\u9053\u53d8\u6362\u4e8b\u4ef6\u4e2d\u5b58\u5728\u793e\u4f1a\u56f0\u5883\uff0c\u4e3b\u8981\u4e3a\u730e\u9e7f\u535a\u5f08\u6216\u56da\u5f92\u56f0\u5883\uff1b\u8499\u7279\u5361\u6d1b\u6a21\u62df\u8868\u660e\uff0c\u65e0\u8bba\u81ea\u52a8\u9a7e\u9a76\u6e17\u900f\u7387\u5982\u4f55\uff0c\u91cd\u590d\u4ea4\u4e92\u5747\u80fd\u63d0\u5347\u5408\u4f5c\u884c\u4e3a\u3002", "conclusion": "\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u5728\u8f66\u9053\u53d8\u6362\u4e2d\u8868\u73b0\u51fa\u66f4\u9ad8\u5408\u4f5c\u6027\uff0c\u4e14\u957f\u671f\u91cd\u590d\u4ea4\u4e92\u6709\u52a9\u4e8e\u4fc3\u8fdb\u6574\u4f53\u4ea4\u901a\u73af\u5883\u7684\u5408\u4f5c\u6f14\u5316\u3002"}}
{"id": "2512.06362", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2512.06362", "abs": "https://arxiv.org/abs/2512.06362", "authors": ["Junyi Yang", "Xinyu Luo", "Ye Ke", "Zheng Wang", "Hongyang Shang", "Shuai Dong", "Zhengnan Fu", "Xiaofeng Yang", "Hongjie Liu", "Arindam Basu"], "title": "A 33.6-136.2 TOPS/W Nonlinear Analog Computing-In-Memory Macro for Multi-bit LSTM Accelerator in 65 nm CMOS", "comment": null, "summary": "The energy efficiency of analog computing-in-memory (ACIM) accelerator for recurrent neural networks, particularly long short-term memory (LSTM) network, is limited by the high proportion of nonlinear (NL) operations typically executed digitally. To address this, we propose an LSTM accelerator incorporating an ACIM macro with reconfigurable (1-5 bit) nonlinear in-memory (NLIM) analog-to-digital converter (ADC) to compute NL activations directly in the analog domain using: 1) a dual 9T bitcell with decoupled read/write paths for signed inputs and ternary weight operations; 2) a read-word-line underdrive Cascode (RUDC) technique achieving 2.8X higher read-bitline dynamic range than single-transistor designs (1.4X better over conventional Cascode structure with 7X lower current variation); 3) a dual-supply 6T-SRAM array for efficient multi-bit weight operations and reducing both bitcell count (7.8X) and latency (4X) for 5-bit weight operations. We experimentally demonstrate 5-bit NLIM ADC for approximating NL activations in LSTM cells, achieving average error <1 LSB. Simulation confirms the robustness of NLIM ADC against temperature variations thanks to the replica bias strategy. Our design achieves 92.0% on-chip inference accuracy for a 12-class keyword-spotting task while demonstrating 2.2X higher system-level normalized energy efficiency and 1.6X better normalized area efficiency than state-of-the-art works. The results combine physical measurements of a macro unit-accounting for the majority of LSTM operations (99% linear and 80% nonlinear operations)-with simulations of the remaining components, including additional LSTM and fully connected layers.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u7ed3\u5408\u53ef\u91cd\u6784\u975e\u7ebf\u6027\u5b58\u5185\u6a21\u62df-\u6570\u5b57\u8f6c\u6362\u5668\u7684LSTM\u52a0\u901f\u5668\uff0c\u663e\u8457\u63d0\u5347\u80fd\u6548\u4e0e\u9762\u79ef\u6548\u7387\u3002", "motivation": "\u89e3\u51b3\u6a21\u62df\u5b58\u5185\u8ba1\u7b97\u4e2d\u975e\u7ebf\u6027\u64cd\u4f5c\u6570\u5b57\u5316\u5bfc\u81f4\u80fd\u6548\u53d7\u9650\u7684\u95ee\u9898\u3002", "method": "\u91c7\u7528\u53cc9T\u4f4d\u5355\u5143\u3001RUDC\u6280\u672f\u53ca\u53cc\u7535\u6e906T-SRAM\u9635\u5217\uff0c\u5728\u6a21\u62df\u57df\u76f4\u63a5\u8ba1\u7b97\u975e\u7ebf\u6027\u6fc0\u6d3b\u3002", "result": "\u5b9e\u73b0\u5e73\u5747\u8bef\u5dee<1 LSB\uff0c\u7cfb\u7edf\u80fd\u6548\u63d0\u53472.2\u500d\uff0c\u9762\u79ef\u6548\u7387\u63d0\u53471.6\u500d\uff0c\u5173\u952e\u8bcd\u8bc6\u522b\u51c6\u786e\u7387\u8fbe92.0%\u3002", "conclusion": "\u8be5\u8bbe\u8ba1\u6709\u6548\u63d0\u5347LSTM\u52a0\u901f\u5668\u80fd\u6548\uff0c\u517c\u5177\u9ad8\u7cbe\u5ea6\u4e0e\u9c81\u68d2\u6027\uff0c\u9002\u7528\u4e8e\u8fb9\u7f18\u667a\u80fd\u573a\u666f\u3002"}}
{"id": "2512.06448", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2512.06448", "abs": "https://arxiv.org/abs/2512.06448", "authors": ["Takaaki Tateishi", "Yasuharu Katsuno"], "title": "Translating PL/I Macro Procedures into Java Using Automatic Templatization and Large Language Models", "comment": "5 pages, 7 figures, to be published in ICSE 2026 NIER", "summary": "Modernizing legacy enterprise systems often involves translating PL/I programs into modern languages such as Java. This task becomes significantly more complex when PL/I macro procedures are involved. The PL/I macro procedures are considered string-manipulating programs that generate PL/I code, and they make automated translation more complex. Recently, large language models (LLMs) have been explored for automated code translation. However, LLM-based code translation struggles to translate the PL/I macro procedures to Java programs that reproduce the behavior of the plain PL/I code generated by the original PL/I macro procedures.\n  This paper proposes a novel method called templatization, which uses symbolic execution to generate code templates (code with named placeholders) as an intermediate representation. In this approach, symbolic values are treated as parts of macro-generated code. By symbolically executing macro procedures and generating code templates, our approach facilitates LLMs to generate readable and maintainable Java code. Our preliminary experiment on ten PL/I macro procedures shows that the LLM-based translation through templatization successfully generates Java programs that reproduce the behavior of the macro-generated PL/I programs.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u7b26\u53f7\u6267\u884c\u7684\u6a21\u677f\u5316\u65b9\u6cd5\uff0c\u63d0\u5347LLM\u5bf9PL/I\u5b8f\u7a0b\u5e8f\u5230Java\u7684\u7ffb\u8bd1\u6548\u679c\u3002", "motivation": "\u89e3\u51b3PL/I\u5b8f\u7a0b\u5e8f\u56e0\u5b57\u7b26\u4e32\u64cd\u4f5c\u7279\u6027\u5bfc\u81f4LLM\u7ffb\u8bd1\u56f0\u96be\u7684\u95ee\u9898\u3002", "method": "\u901a\u8fc7\u7b26\u53f7\u6267\u884c\u751f\u6210\u542b\u5360\u4f4d\u7b26\u7684\u4ee3\u7801\u6a21\u677f\u4f5c\u4e3a\u4e2d\u95f4\u8868\u793a\uff0c\u8f85\u52a9LLM\u751f\u6210\u53ef\u8bfb\u3001\u53ef\u7ef4\u62a4\u7684Java\u4ee3\u7801\u3002", "result": "\u572810\u4e2aPL/I\u5b8f\u7a0b\u5e8f\u4e0a\u7684\u521d\u6b65\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u80fd\u6210\u529f\u751f\u6210\u884c\u4e3a\u4e00\u81f4\u7684Java\u7a0b\u5e8f\u3002", "conclusion": "\u6a21\u677f\u5316\u65b9\u6cd5\u6709\u6548\u63d0\u5347LLM\u5728\u590d\u6742\u5b8f\u7a0b\u5e8f\u7ffb\u8bd1\u4efb\u52a1\u4e2d\u7684\u51c6\u786e\u6027\u548c\u5b9e\u7528\u6027\u3002"}}
{"id": "2512.07462", "categories": ["cs.MA", "cs.AI", "cs.GT", "cs.LG", "math.DS"], "pdf": "https://arxiv.org/pdf/2512.07462", "abs": "https://arxiv.org/abs/2512.07462", "authors": ["Trung-Kiet Huynh", "Duy-Minh Dao-Sy", "Thanh-Bang Cao", "Phong-Hao Le", "Hong-Dan Nguyen", "Phu-Quy Nguyen-Lam", "Minh-Luan Nguyen-Vo", "Hong-Phat Pham", "Phu-Hoa Pham", "Thien-Kim Than", "Chi-Nguyen Tran", "Huy Tran", "Gia-Thoai Tran-Le", "Alessio Buscemi", "Le Hong Trang", "The Anh Han"], "title": "Understanding LLM Agent Behaviours via Game Theory: Strategy Recognition, Biases and Multi-Agent Dynamics", "comment": null, "summary": "As Large Language Models (LLMs) increasingly operate as autonomous decision-makers in interactive and multi-agent systems and human societies, understanding their strategic behaviour has profound implications for safety, coordination, and the design of AI-driven social and economic infrastructures. Assessing such behaviour requires methods that capture not only what LLMs output, but the underlying intentions that guide their decisions. In this work, we extend the FAIRGAME framework to systematically evaluate LLM behaviour in repeated social dilemmas through two complementary advances: a payoff-scaled Prisoners Dilemma isolating sensitivity to incentive magnitude, and an integrated multi-agent Public Goods Game with dynamic payoffs and multi-agent histories. These environments reveal consistent behavioural signatures across models and languages, including incentive-sensitive cooperation, cross-linguistic divergence and end-game alignment toward defection. To interpret these patterns, we train traditional supervised classification models on canonical repeated-game strategies and apply them to FAIRGAME trajectories, showing that LLMs exhibit systematic, model- and language-dependent behavioural intentions, with linguistic framing at times exerting effects as strong as architectural differences. Together, these findings provide a unified methodological foundation for auditing LLMs as strategic agents and reveal systematic cooperation biases with direct implications for AI governance, collective decision-making, and the design of safe multi-agent systems.", "AI": {"tldr": "\u672c\u6587\u6269\u5c55FAIRGAME\u6846\u67b6\uff0c\u901a\u8fc7\u65b0\u8bbe\u8ba1\u7684\u535a\u5f08\u73af\u5883\u8bc4\u4f30\u5927\u8bed\u8a00\u6a21\u578b\u5728\u91cd\u590d\u793e\u4f1a\u56f0\u5883\u4e2d\u7684\u7b56\u7565\u884c\u4e3a\uff0c\u63ed\u793a\u5176\u5408\u4f5c\u503e\u5411\u4e0e\u8bed\u8a00\u3001\u67b6\u6784\u76f8\u5173\u6027\uff0c\u4e3aAI\u6cbb\u7406\u63d0\u4f9b\u65b9\u6cd5\u8bba\u57fa\u7840\u3002", "motivation": "\u7406\u89e3\u5927\u8bed\u8a00\u6a21\u578b\u5728\u4ea4\u4e92\u4e0e\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u4e2d\u7684\u7b56\u7565\u884c\u4e3a\uff0c\u5bf9AI\u5b89\u5168\u3001\u534f\u8c03\u53ca\u793e\u4f1a\u7ecf\u6d4e\u57fa\u7840\u8bbe\u65bd\u8bbe\u8ba1\u81f3\u5173\u91cd\u8981\u3002", "method": "\u6269\u5c55FAIRGAME\u6846\u67b6\uff0c\u5f15\u5165\u6536\u76ca\u7f29\u653e\u56da\u5f92\u56f0\u5883\u4e0e\u52a8\u6001\u6536\u76ca\u516c\u5171\u7269\u54c1\u535a\u5f08\uff0c\u5e76\u7528\u76d1\u7763\u5206\u7c7b\u6a21\u578b\u89e3\u6790\u884c\u4e3a\u610f\u56fe\u3002", "result": "\u53d1\u73b0LLM\u884c\u4e3a\u5177\u6709\u4e00\u81f4\u6027\u7279\u5f81\uff1a\u6fc0\u52b1\u654f\u611f\u5408\u4f5c\u3001\u8de8\u8bed\u8a00\u5dee\u5f02\u3001\u7ec8\u5c40\u8d8b\u5411\u80cc\u53db\uff1b\u8bed\u8a00\u6846\u67b6\u5f71\u54cd\u53ef\u5ab2\u7f8e\u67b6\u6784\u5dee\u5f02\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u5ba1\u8ba1LLM\u4f5c\u4e3a\u7b56\u7565\u667a\u80fd\u4f53\u63d0\u4f9b\u4e86\u7edf\u4e00\u65b9\u6cd5\u8bba\uff0c\u5e76\u63ed\u793a\u5176\u5408\u4f5c\u504f\u5dee\uff0c\u5bf9AI\u6cbb\u7406\u4e0e\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u8bbe\u8ba1\u6709\u76f4\u63a5\u610f\u4e49\u3002"}}
{"id": "2512.07280", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2512.07280", "abs": "https://arxiv.org/abs/2512.07280", "authors": ["Hendrik Reiter", "Janick Edinger", "Martin Kabierski", "Agnes Koschmider", "Olaf Landsiedel", "Arvid Lepsien", "Xixi Lu", "Andrea Marrella", "Estefania Serral", "Stefan Schulte", "Florian Tschorsch", "Matthias Weidlich", "Wilhelm Hasselbring"], "title": "ContinuumConductor : Decentralized Process Mining on the Edge-Cloud Continuum", "comment": "Accepted at COMINDS workshop ICPM2025", "summary": "Process mining traditionally assumes centralized event data collection and analysis. However, modern Industrial Internet of Things systems increasingly operate over distributed, resource-constrained edge-cloud infrastructures. This paper proposes a structured approach for decentralizing process mining by enabling event data to be mined directly within the IoT systems edge-cloud continuum. We introduce ContinuumConductor a layered decision framework that guides when to perform process mining tasks such as preprocessing, correlation, and discovery centrally or decentrally. Thus, enabling privacy, responsive and resource-efficient process mining. For each step in the process mining pipeline, we analyze the trade-offs of decentralization versus centralization across these layers and propose decision criteria. We demonstrate ContinuumConductor at a real-world use-case of process optimazition in inland ports. Our contributions lay the foundation for computing-aware process mining in cyber-physical and IIoT systems.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5728\u8fb9\u7f18-\u4e91\u8fde\u7eed\u4f53\u4e2d\u5b9e\u73b0\u53bb\u4e2d\u5fc3\u5316\u6d41\u7a0b\u6316\u6398\u7684\u6846\u67b6ContinuumConductor\uff0c\u4ee5\u652f\u6301\u9690\u79c1\u4fdd\u62a4\u3001\u54cd\u5e94\u8fc5\u901f\u4e14\u8d44\u6e90\u9ad8\u6548\u7684\u6d41\u7a0b\u5206\u6790\u3002", "motivation": "\u73b0\u4ee3\u5de5\u4e1a\u7269\u8054\u7f51\u7cfb\u7edf\u591a\u8fd0\u884c\u4e8e\u5206\u5e03\u5f0f\u3001\u8d44\u6e90\u53d7\u9650\u7684\u8fb9\u7f18-\u4e91\u8ba1\u7b97\u67b6\u6784\uff0c\u4f20\u7edf\u96c6\u4e2d\u5f0f\u6d41\u7a0b\u6316\u6398\u96be\u4ee5\u6ee1\u8db3\u5176\u9700\u6c42\u3002", "method": "\u8bbe\u8ba1\u5206\u5c42\u51b3\u7b56\u6846\u67b6ContinuumConductor\uff0c\u6307\u5bfc\u9884\u5904\u7406\u3001\u5173\u8054\u4e0e\u53d1\u73b0\u7b49\u4efb\u52a1\u5728\u8fb9\u7f18\u6216\u4e91\u7aef\u6267\u884c\uff0c\u5e76\u5206\u6790\u5404\u6b65\u9aa4\u53bb\u4e2d\u5fc3\u5316\u4e0e\u96c6\u4e2d\u5316\u7684\u6743\u8861\u3002", "result": "\u5728\u5185\u6cb3\u6e2f\u53e3\u5b9e\u9645\u7528\u4f8b\u4e2d\u9a8c\u8bc1\u4e86\u6846\u67b6\u6709\u6548\u6027\uff0c\u5b9e\u73b0\u4e86\u8ba1\u7b97\u611f\u77e5\u7684\u6d41\u7a0b\u4f18\u5316\u3002", "conclusion": "\u672c\u7814\u7a76\u4e3a\u4fe1\u606f\u7269\u7406\u7cfb\u7edf\u4e0e\u5de5\u4e1a\u7269\u8054\u7f51\u4e2d\u7684\u8ba1\u7b97\u611f\u77e5\u6d41\u7a0b\u6316\u6398\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2512.07408", "categories": ["cs.NI"], "pdf": "https://arxiv.org/pdf/2512.07408", "abs": "https://arxiv.org/abs/2512.07408", "authors": ["Minju Jeon", "Jiyun Kim", "Sewon Kim", "Seongmin Park", "Bo Zhang", "Anthony H. Smith"], "title": "WaggleNet: A LoRa and MQTT-Based Monitoring System for Internal and External Beehive Conditions", "comment": "8 pages, 7 figures, 3 tables", "summary": "Bee populations are declining globally due to habitat loss, pesticide exposure, and climate change, threatening agricultural productivity and food security. While existing smart beehive systems monitor internal conditions, they typically overlook external environmental factors that significantly influence colony health, and are constrained by high cost, limited scalability, and inadequate contextual analysis. We present WaggleNet, a novel dual-scope monitoring system that simultaneously captures both internal hive conditions and external environmental parameters using a cost-effective LoRa-MQTT architecture. Our system deploys modular worker nodes ($\\sim$\\$15 each) equipped with temperature, humidity, light, and GPS sensors both inside and around beehives. A master node functions as a LoRa-MQTT gateway, forwarding data to a cloud server with a mobile application interface. Field experiments confirmed reliable operation with 100\\% packet delivery over 110 meters in line-of-sight conditions and 95 meters in obstructed environments, including successful deployment inside wooden hive structures. Our system demonstrated stable end-to-end latency under 5 seconds and continuous operation over a two-month period across diverse environmental conditions. By bridging the gap between internal and external monitoring, WaggleNet enables contextual anomaly detection and supports data-driven precision beekeeping in resource-constrained settings.", "AI": {"tldr": "WaggleNet\u662f\u4e00\u79cd\u4f4e\u6210\u672c\u3001\u53cc\u8303\u56f4\u76d1\u6d4b\u7cfb\u7edf\uff0c\u901a\u8fc7LoRa-MQTT\u67b6\u6784\u540c\u6b65\u91c7\u96c6\u8702\u7bb1\u5185\u5916\u73af\u5883\u6570\u636e\uff0c\u652f\u6301\u7cbe\u51c6\u517b\u8702\u3002", "motivation": "\u73b0\u6709\u667a\u80fd\u8702\u7bb1\u7cfb\u7edf\u5ffd\u89c6\u5916\u90e8\u73af\u5883\u56e0\u7d20\uff0c\u4e14\u6210\u672c\u9ad8\u3001\u6269\u5c55\u6027\u5dee\uff0c\u65e0\u6cd5\u6ee1\u8db3\u5b9e\u9645\u517b\u8702\u9700\u6c42\u3002", "method": "\u90e8\u7f72\u7ea615\u7f8e\u5143\u7684\u6a21\u5757\u5316\u8282\u70b9\uff0c\u7ed3\u5408\u6e29\u6e7f\u5ea6\u3001\u5149\u7167\u3001GPS\u4f20\u611f\u5668\u4e0eLoRa-MQTT\u7f51\u5173\uff0c\u5b9e\u73b0\u5185\u5916\u6570\u636e\u91c7\u96c6\u4e0e\u4e91\u7aef\u4f20\u8f93\u3002", "result": "\u5b9e\u6d4b\u5728110\u7c73\u89c6\u8ddd\u548c95\u7c73\u906e\u6321\u73af\u5883\u4e0b\u5b9e\u73b0100%\u6570\u636e\u5305\u9001\u8fbe\uff0c\u7aef\u5230\u7aef\u5ef6\u8fdf\u4f4e\u4e8e5\u79d2\uff0c\u7a33\u5b9a\u8fd0\u884c\u4e24\u4e2a\u6708\u3002", "conclusion": "WaggleNet\u586b\u8865\u4e86\u8702\u7fa4\u5185\u5916\u73af\u5883\u534f\u540c\u76d1\u6d4b\u7a7a\u767d\uff0c\u52a9\u529b\u8d44\u6e90\u53d7\u9650\u573a\u666f\u4e0b\u7684\u6570\u636e\u9a71\u52a8\u578b\u517b\u8702\u7ba1\u7406\u3002"}}
{"id": "2512.07588", "categories": ["cs.MA"], "pdf": "https://arxiv.org/pdf/2512.07588", "abs": "https://arxiv.org/abs/2512.07588", "authors": ["James Rudd-Jones", "Mar\u00eda P\u00e9rez-Ortiz", "Mirco Musolesi"], "title": "Understanding Individual Decision-Making in Multi-Agent Reinforcement Learning: A Dynamical Systems Approach", "comment": null, "summary": "Analysing learning behaviour in Multi-Agent Reinforcement Learning (MARL) environments is challenging, in particular with respect to \\textit{individual} decision-making. Practitioners frequently tend to study or compare MARL algorithms from a qualitative perspective largely due to the inherent stochasticity in practical algorithms arising from random dithering exploration strategies, environment transition noise, and stochastic gradient updates to name a few. Traditional analytical approaches, such as replicator dynamics, often rely on mean-field approximations to remove stochastic effects, but this simplification, whilst able to provide general overall trends, might lead to dissonance between analytical predictions and actual realisations of individual trajectories. In this paper, we propose a novel perspective on MARL systems by modelling them as \\textit{coupled stochastic dynamical systems}, capturing both agent interactions and environmental characteristics. Leveraging tools from dynamical systems theory, we analyse the stability and sensitivity of agent behaviour at individual level, which are key dimensions for their practical deployments, for example, in presence of strict safety requirements. This framework allows us, for the first time, to rigorously study MARL dynamics taking into consideration their inherent stochasticity, providing a deeper understanding of system behaviour and practical insights for the design and control of multi-agent learning processes.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u65b0\u89c6\u89d2\uff0c\u5c06\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u7cfb\u7edf\u5efa\u6a21\u4e3a\u8026\u5408\u968f\u673a\u52a8\u529b\u7cfb\u7edf\uff0c\u4ee5\u66f4\u51c6\u786e\u5206\u6790\u4e2a\u4f53\u884c\u4e3a\u7684\u7a33\u5b9a\u6027\u548c\u654f\u611f\u6027\u3002", "motivation": "\u4f20\u7edf\u65b9\u6cd5\u4f9d\u8d56\u5e73\u5747\u573a\u8fd1\u4f3c\u5ffd\u7565\u968f\u673a\u6027\uff0c\u5bfc\u81f4\u5206\u6790\u9884\u6d4b\u4e0e\u5b9e\u9645\u8f68\u8ff9\u4e0d\u7b26\uff0c\u96be\u4ee5\u6ee1\u8db3\u5b9e\u9645\u90e8\u7f72\u4e2d\u7684\u5b89\u5168\u9700\u6c42\u3002", "method": "\u5229\u7528\u52a8\u529b\u7cfb\u7edf\u7406\u8bba\u5de5\u5177\uff0c\u5c06MARL\u5efa\u6a21\u4e3a\u8026\u5408\u968f\u673a\u52a8\u529b\u7cfb\u7edf\uff0c\u6355\u6349\u667a\u80fd\u4f53\u4ea4\u4e92\u4e0e\u73af\u5883\u7279\u6027\u3002", "result": "\u9996\u6b21\u80fd\u4e25\u8c28\u7814\u7a76MARL\u52a8\u529b\u5b66\u5e76\u8003\u8651\u5176\u56fa\u6709\u968f\u673a\u6027\uff0c\u52a0\u6df1\u5bf9\u7cfb\u7edf\u884c\u4e3a\u7684\u7406\u89e3\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u591a\u667a\u80fd\u4f53\u5b66\u4e60\u8fc7\u7a0b\u7684\u8bbe\u8ba1\u4e0e\u63a7\u5236\u63d0\u4f9b\u4e86\u5b9e\u7528\u89c1\u89e3\uff0c\u5c24\u5176\u9002\u7528\u4e8e\u9ad8\u5b89\u5168\u6027\u8981\u6c42\u573a\u666f\u3002"}}
{"id": "2512.07344", "categories": ["cs.DC", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.07344", "abs": "https://arxiv.org/abs/2512.07344", "authors": ["Shengyuan Ye", "Bei Ouyang", "Tianyi Qian", "Liekang Zeng", "Mu Yuan", "Xiaowen Chu", "Weijie Hong", "Xu Chen"], "title": "Venus: An Efficient Edge Memory-and-Retrieval System for VLM-based Online Video Understanding", "comment": "Accepted by IEEE International Conference on Computer Communications 2026", "summary": "Vision-language models (VLMs) have demonstrated impressive multimodal comprehension capabilities and are being deployed in an increasing number of online video understanding applications. While recent efforts extensively explore advancing VLMs' reasoning power in these cases, deployment constraints are overlooked, leading to overwhelming system overhead in real-world deployments. To address that, we propose Venus, an on-device memory-and-retrieval system for efficient online video understanding. Venus proposes an edge-cloud disaggregated architecture that sinks memory construction and keyframe retrieval from cloud to edge, operating in two stages. In the ingestion stage, Venus continuously processes streaming edge videos via scene segmentation and clustering, where the selected keyframes are embedded with a multimodal embedding model to build a hierarchical memory for efficient storage and retrieval. In the querying stage, Venus indexes incoming queries from memory, and employs a threshold-based progressive sampling algorithm for keyframe selection that enhances diversity and adaptively balances system cost and reasoning accuracy. Our extensive evaluation shows that Venus achieves a 15x-131x speedup in total response latency compared to state-of-the-art methods, enabling real-time responses within seconds while maintaining comparable or even superior reasoning accuracy.", "AI": {"tldr": "Venus \u662f\u4e00\u79cd\u9762\u5411\u8bbe\u5907\u7aef\u7684\u5185\u5b58\u4e0e\u68c0\u7d22\u7cfb\u7edf\uff0c\u65e8\u5728\u63d0\u5347\u5728\u7ebf\u89c6\u9891\u7406\u89e3\u7684\u6548\u7387\uff0c\u901a\u8fc7\u8fb9\u7f18-\u4e91\u5206\u79bb\u67b6\u6784\u5b9e\u73b0\u4f4e\u5ef6\u8fdf\u548c\u9ad8\u7cbe\u5ea6\u3002", "motivation": "\u73b0\u6709\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u90e8\u7f72\u65f6\u5ffd\u89c6\u4e86\u7cfb\u7edf\u5f00\u9500\uff0c\u5bfc\u81f4\u5b9e\u9645\u5e94\u7528\u4e2d\u6027\u80fd\u74f6\u9888\u3002", "method": "\u63d0\u51fa\u4e24\u9636\u6bb5\u67b6\u6784\uff1a\u6444\u5165\u9636\u6bb5\u901a\u8fc7\u573a\u666f\u5206\u5272\u548c\u805a\u7c7b\u6784\u5efa\u5206\u5c42\u5185\u5b58\uff1b\u67e5\u8be2\u9636\u6bb5\u91c7\u7528\u9608\u503c\u6e10\u8fdb\u91c7\u6837\u7b97\u6cd5\u4f18\u5316\u5173\u952e\u5e27\u9009\u62e9\u3002", "result": "\u76f8\u6bd4\u73b0\u6709\u65b9\u6cd5\uff0cVenus \u5b9e\u73b015\u500d\u81f3131\u500d\u7684\u54cd\u5e94\u5ef6\u8fdf\u52a0\u901f\uff0c\u5e76\u4fdd\u6301\u76f8\u5f53\u6216\u66f4\u4f18\u7684\u63a8\u7406\u7cbe\u5ea6\u3002", "conclusion": "Venus \u6709\u6548\u5e73\u8861\u4e86\u7cfb\u7edf\u6210\u672c\u4e0e\u63a8\u7406\u51c6\u786e\u6027\uff0c\u652f\u6301\u79d2\u7ea7\u5b9e\u65f6\u54cd\u5e94\uff0c\u9002\u5408\u5b9e\u9645\u90e8\u7f72\u3002"}}
{"id": "2512.06854", "categories": ["cs.AR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.06854", "abs": "https://arxiv.org/abs/2512.06854", "authors": ["Qijun Zhang", "Yao Lu", "Mengming Li", "Shang Liu", "Zhiyao Xie"], "title": "ArchPower: Dataset for Architecture-Level Power Modeling of Modern CPU Design", "comment": "Published in NeurIPS'25 Dataset and Benchmark Track", "summary": "Power is the primary design objective of large-scale integrated circuits (ICs), especially for complex modern processors (i.e., CPUs). Accurate CPU power evaluation requires designers to go through the whole time-consuming IC implementation process, easily taking months. At the early design stage (e.g., architecture-level), classical power models are notoriously inaccurate. Recently, ML-based architecture-level power models have been proposed to boost accuracy, but the data availability is a severe challenge. Currently, there is no open-source dataset for this important ML application. A typical dataset generation process involves correct CPU design implementation and repetitive execution of power simulation flows, requiring significant design expertise, engineering effort, and execution time. Even private in-house datasets often fail to reflect realistic CPU design scenarios. In this work, we propose ArchPower, the first open-source dataset for architecture-level processor power modeling. We go through complex and realistic design flows to collect the CPU architectural information as features and the ground-truth simulated power as labels. Our dataset includes 200 CPU data samples, collected from 25 different CPU configurations when executing 8 different workloads. There are more than 100 architectural features in each data sample, including both hardware and event parameters. The label of each sample provides fine-grained power information, including the total design power and the power for each of the 11 components. Each power value is further decomposed into four fine-grained power groups: combinational logic power, sequential logic power, memory power, and clock power. ArchPower is available at https://github.com/hkust-zhiyao/ArchPower.", "AI": {"tldr": "ArchPower\u662f\u9996\u4e2a\u5f00\u6e90\u7684\u67b6\u6784\u7ea7\u5904\u7406\u5668\u529f\u8017\u5efa\u6a21\u6570\u636e\u96c6\uff0c\u5305\u542b200\u4e2a\u6837\u672c\u3001100\u591a\u4e2a\u67b6\u6784\u7279\u5f81\u53ca\u7ec6\u7c92\u5ea6\u529f\u8017\u6807\u7b7e\uff0c\u65e8\u5728\u89e3\u51b3ML\u529f\u8017\u6a21\u578b\u7684\u6570\u636e\u7a00\u7f3a\u95ee\u9898\u3002", "motivation": "\u5f53\u524d\u7f3a\u4e4f\u5f00\u6e90\u4e14\u8d34\u8fd1\u771f\u5b9e\u8bbe\u8ba1\u573a\u666f\u7684\u5904\u7406\u5668\u67b6\u6784\u7ea7\u529f\u8017\u6570\u636e\u96c6\uff0c\u5236\u7ea6\u4e86ML\u529f\u8017\u6a21\u578b\u7684\u53d1\u5c55\u4e0e\u9a8c\u8bc1\u3002", "method": "\u901a\u8fc7\u590d\u6742\u771f\u5b9e\u7684\u8bbe\u8ba1\u6d41\u7a0b\uff0c\u4ece25\u79cdCPU\u914d\u7f6e\u6267\u884c8\u79cd\u5de5\u4f5c\u8d1f\u8f7d\u4e2d\u91c7\u96c6\u67b6\u6784\u7279\u5f81\u4e0e\u4eff\u771f\u529f\u8017\u6807\u7b7e\uff0c\u6784\u5efa\u5305\u542b\u7ec4\u4ef6\u4e0e\u529f\u8017\u7c7b\u578b\u5206\u89e3\u7684\u7ec6\u7c92\u5ea6\u6570\u636e\u96c6\u3002", "result": "\u6210\u529f\u6784\u5efa\u5e76\u5f00\u6e90\u5305\u542b200\u6837\u672c\u7684ArchPower\u6570\u636e\u96c6\uff0c\u6bcf\u4e2a\u6837\u672c\u542b100+\u7279\u5f81\u548c11\u7ec4\u4ef6\u00d74\u529f\u8017\u7c7b\u578b\u7684\u8be6\u7ec6\u6807\u7b7e\u3002", "conclusion": "ArchPower\u586b\u8865\u4e86\u67b6\u6784\u7ea7\u529f\u8017\u5efa\u6a21\u9886\u57df\u5f00\u6e90\u6570\u636e\u7a7a\u767d\uff0c\u6709\u52a9\u4e8e\u63a8\u52a8\u9ad8\u7cbe\u5ea6ML\u529f\u8017\u6a21\u578b\u7684\u7814\u7a76\u4e0e\u5e94\u7528\u3002"}}
{"id": "2512.06836", "categories": ["cs.SE", "cs.AI", "cs.PL"], "pdf": "https://arxiv.org/pdf/2512.06836", "abs": "https://arxiv.org/abs/2512.06836", "authors": ["Weixing Zhang", "Regina Hebig", "Daniel Str\u00fcber"], "title": "Leveraging LLMs to support co-evolution between definitions and instances of textual DSLs", "comment": null, "summary": "Software languages evolve over time for various reasons, such as the addition of new features. When the language's grammar definition evolves, textual instances that originally conformed to the grammar become outdated. For DSLs in a model-driven engineering context, there exists a plethora of techniques to co-evolve models with the evolving metamodel. However, these techniques are not geared to support DSLs with a textual syntax -- applying them to textual language definitions and instances may lead to the loss of information from the original instances, such as comments and layout information, which are valuable for software comprehension and maintenance. This study explores the potential of Large Language Model (LLM)-based solutions in achieving grammar and instance co-evolution, with attention to their ability to preserve auxiliary information when directly processing textual instances. By applying two advanced language models, Claude-3.5 and GPT-4o, and conducting experiments across seven case languages, we evaluated the feasibility and limitations of this approach. Our results indicate a good ability of the considered LLMs for migrating textual instances in small-scale cases with limited instance size, which are representative of a subset of cases encountered in practice. In addition, we observe significant challenges with the scalability of LLM-based solutions to larger instances, leading to insights that are useful for informing future research.", "AI": {"tldr": "\u672c\u7814\u7a76\u63a2\u8ba8\u4e86\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u65b9\u6848\u5728\u8bed\u6cd5\u4e0e\u5b9e\u4f8b\u534f\u540c\u6f14\u5316\u4e2d\u7684\u53ef\u884c\u6027\uff0c\u7279\u522b\u5173\u6ce8\u5176\u5728\u5904\u7406\u6587\u672c\u5b9e\u4f8b\u65f6\u4fdd\u7559\u6ce8\u91ca\u548c\u5e03\u5c40\u7b49\u8f85\u52a9\u4fe1\u606f\u7684\u80fd\u529b\u3002", "motivation": "\u6587\u672cDSL\u5728\u8bed\u6cd5\u6f14\u8fdb\u65f6\u6613\u4e22\u5931\u6ce8\u91ca\u548c\u5e03\u5c40\u7b49\u91cd\u8981\u4fe1\u606f\uff0c\u73b0\u6709\u6a21\u578b\u9a71\u52a8\u65b9\u6cd5\u4e0d\u9002\u7528\u4e8e\u6587\u672c\u8bed\u6cd5\uff0c\u56e0\u6b64\u9700\u63a2\u7d22LLM\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u4f7f\u7528Claude-3.5\u548cGPT-4o\u4e24\u4e2a\u5148\u8fdb\u8bed\u8a00\u6a21\u578b\uff0c\u5728\u4e03\u79cd\u6848\u4f8b\u8bed\u8a00\u4e0a\u8fdb\u884c\u5b9e\u9a8c\u8bc4\u4f30\u3002", "result": "LLM\u5728\u5c0f\u89c4\u6a21\u3001\u5b9e\u4f8b\u5c3a\u5bf8\u6709\u9650\u7684\u60c5\u51b5\u4e0b\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728\u5927\u89c4\u6a21\u5b9e\u4f8b\u4e2d\u9762\u4e34\u663e\u8457\u53ef\u6269\u5c55\u6027\u6311\u6218\u3002", "conclusion": "LLM\u65b9\u6848\u5bf9\u90e8\u5206\u5b9e\u9645\u573a\u666f\u53ef\u884c\uff0c\u4f46\u5176\u6269\u5c55\u6027\u9650\u5236\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u91cd\u8981\u542f\u793a\u3002"}}
{"id": "2512.07350", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2512.07350", "abs": "https://arxiv.org/abs/2512.07350", "authors": ["Zhiyuan Wu", "Shuai Wang", "Li Chen", "Kaihui Gao", "Dan Li", "Yanyu Ren", "Qiming Zhang", "Yong Wang"], "title": "Communication-Efficient Serving for Video Diffusion Models with Latent Parallelism", "comment": "19 pages", "summary": "Video diffusion models (VDMs) perform attention computation over the 3D spatio-temporal domain. Compared to large language models (LLMs) processing 1D sequences, their memory consumption scales cubically, necessitating parallel serving across multiple GPUs. Traditional parallelism strategies partition the computational graph, requiring frequent high-dimensional activation transfers that create severe communication bottlenecks. To tackle this issue, we exploit the local spatio-temporal dependencies inherent in the diffusion denoising process and propose Latent Parallelism (LP), the first parallelism strategy tailored for VDM serving. \\textcolor{black}{LP decomposes the global denoising problem into parallelizable sub-problems by dynamically rotating the partitioning dimensions (temporal, height, and width) within the compact latent space across diffusion timesteps, substantially reducing the communication overhead compared to prevailing parallelism strategies.} To ensure generation quality, we design a patch-aligned overlapping partition strategy that matches partition boundaries with visual patches and a position-aware latent reconstruction mechanism for smooth stitching. Experiments on three benchmarks demonstrate that LP reduces communication overhead by up to 97\\% over baseline methods while maintaining comparable generation quality. As a non-intrusive plug-in paradigm, LP can be seamlessly integrated with existing parallelism strategies, enabling efficient and scalable video generation services.", "AI": {"tldr": "Latent Parallelism (LP) \u662f\u9996\u4e2a\u4e13\u4e3a\u89c6\u9891\u6269\u6563\u6a21\u578b\u8bbe\u8ba1\u7684\u5e76\u884c\u7b56\u7565\uff0c\u901a\u8fc7\u5728\u6f5c\u5728\u7a7a\u95f4\u52a8\u6001\u65cb\u8f6c\u5212\u5206\u7ef4\u5ea6\u663e\u8457\u964d\u4f4e\u901a\u4fe1\u5f00\u9500\uff0c\u540c\u65f6\u4fdd\u6301\u751f\u6210\u8d28\u91cf\u3002", "motivation": "\u89c6\u9891\u6269\u6563\u6a21\u578b\u56e0\u4e09\u7ef4\u65f6\u7a7a\u6ce8\u610f\u529b\u8ba1\u7b97\u5bfc\u81f4\u5185\u5b58\u6d88\u8017\u5267\u589e\uff0c\u4f20\u7edf\u5e76\u884c\u7b56\u7565\u5b58\u5728\u4e25\u91cd\u901a\u4fe1\u74f6\u9888\uff0c\u4e9f\u9700\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u63d0\u51fa Latent Parallelism\uff0c\u52a8\u6001\u65cb\u8f6c\u6f5c\u5728\u7a7a\u95f4\u7684\u65f6\u95f4\u3001\u9ad8\u5ea6\u548c\u5bbd\u5ea6\u5212\u5206\u7ef4\u5ea6\uff0c\u914d\u5408\u8fb9\u754c\u5bf9\u9f50\u4e0e\u4f4d\u7f6e\u611f\u77e5\u91cd\u5efa\u673a\u5236\u5b9e\u73b0\u4f4e\u901a\u4fe1\u5e76\u884c\u63a8\u7406\u3002", "result": "\u5728\u4e09\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cLP \u76f8\u6bd4\u57fa\u7ebf\u65b9\u6cd5\u6700\u9ad8\u964d\u4f4e 97% \u901a\u4fe1\u5f00\u9500\uff0c\u4e14\u751f\u6210\u8d28\u91cf\u76f8\u5f53\u3002", "conclusion": "LP \u662f\u4e00\u79cd\u975e\u4fb5\u5165\u5f0f\u63d2\u4ef6\u65b9\u6848\uff0c\u53ef\u65e0\u7f1d\u96c6\u6210\u73b0\u6709\u5e76\u884c\u7b56\u7565\uff0c\u5b9e\u73b0\u9ad8\u6548\u53ef\u6269\u5c55\u7684\u89c6\u9891\u751f\u6210\u670d\u52a1\u3002"}}
{"id": "2512.07312", "categories": ["cs.AR", "cs.AI", "cs.DC"], "pdf": "https://arxiv.org/pdf/2512.07312", "abs": "https://arxiv.org/abs/2512.07312", "authors": ["Zhongchun Zhou", "Chengtao Lai", "Yuhang Gu", "Wei Zhang"], "title": "DCO: Dynamic Cache Orchestration for LLM Accelerators through Predictive Management", "comment": "\\c{opyright} 2025 IEEE. Personal use of this material is permitted. Permission from IEEE must be obtained for all other uses, in any current or future media, including reprinting/republishing this material for advertising or promotional purposes, creating new collective works, for resale or redistribution to servers or lists, or reuse of any copyrighted component of this work in other works", "summary": "The rapid adoption of large language models (LLMs) is pushing AI accelerators toward increasingly powerful and specialized designs. Instead of further complicating software development with deeply hierarchical scratchpad memories (SPMs) and their asynchronous management, we investigate the opposite point of the design spectrum: a multi-core AI accelerator equipped with a shared system-level cache and application-aware management policies, which keeps the programming effort modest. Our approach exploits dataflow information available in the software stack to guide cache replacement (including dead-block prediction), in concert with bypass decisions and mechanisms that alleviate cache thrashing.\n  We assess the proposal using a cycle-accurate simulator and observe substantial performance gains (up to 1.80x speedup) compared with conventional cache architectures. In addition, we build and validate an analytical model that takes into account the actual overlapping behaviors to extend the measurement results of our policies to real-world larger-scale workloads. Experiment results show that when functioning together, our bypassing and thrashing mitigation strategies can handle scenarios both with and without inter-core data sharing and achieve remarkable speedups.\n  Finally, we implement the design in RTL and the area of our design is $\\mathbf{0.064mm^2}$ with 15nm process, which can run at 2 GHz clock frequency. Our findings explore the potential of the shared cache design to assist the development of future AI accelerator systems.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u5171\u4eab\u7f13\u5b58\u4e0e\u5e94\u7528\u611f\u77e5\u7b56\u7565\u7684\u591a\u6838AI\u52a0\u901f\u5668\u8bbe\u8ba1\uff0c\u663e\u8457\u63d0\u5347\u6027\u80fd\u5e76\u7b80\u5316\u7f16\u7a0b\u3002", "motivation": "\u907f\u514d\u4f7f\u7528\u590d\u6742\u7684\u5206\u5c42\u6682\u5b58\u5185\u5b58\u67b6\u6784\uff0c\u964d\u4f4e\u8f6f\u4ef6\u5f00\u53d1\u590d\u6742\u5ea6\u3002", "method": "\u5229\u7528\u8f6f\u4ef6\u6808\u4e2d\u7684\u6570\u636e\u6d41\u4fe1\u606f\u6307\u5bfc\u7f13\u5b58\u66ff\u6362\u3001\u65c1\u8def\u51b3\u7b56\u548c\u7f13\u89e3\u7f13\u5b58\u98a0\u7c38\u673a\u5236\u3002", "result": "\u76f8\u6bd4\u4f20\u7edf\u7f13\u5b58\u67b6\u6784\u6700\u9ad8\u5b9e\u73b01.80\u500d\u52a0\u901f\uff0cRTL\u5b9e\u73b0\u572815nm\u5de5\u827a\u4e0b\u9762\u79ef\u4ec50.064mm\u00b2\uff0c\u652f\u63012GHz\u9891\u7387\u3002", "conclusion": "\u5171\u4eab\u7f13\u5b58\u8bbe\u8ba1\u6709\u52a9\u4e8e\u672a\u6765AI\u52a0\u901f\u5668\u7cfb\u7edf\u7684\u5f00\u53d1\u3002"}}
{"id": "2512.07401", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2512.07401", "abs": "https://arxiv.org/abs/2512.07401", "authors": ["Sadaf Ehtesabi", "Manoar Hossain", "Tobias Kenter", "Andreas Krawinkel", "Holger Nitsche", "Lukas Ostermann", "Christian Plessl", "Heinrich Riebler", "Stefan Rohde", "Robert Schade", "Michael Schwarz", "Jens Simon", "Nils Winnwa", "Alex Wiens", "Xin Wu"], "title": "Otus Supercomputer", "comment": null, "summary": "Otus is a high-performance computing cluster that was launched in 2025 and is operated by the Paderborn Center for Parallel Computing (PC2) at Paderborn University in Germany. The system is part of the National High Performance Computing (NHR) initiative. Otus complements the previous supercomputer Noctua 2, offering approximately twice the computing power while retaining the three node types that were characteristic of Noctua 2: 1) CPU compute nodes with different memory capacities, 2) high-end GPU nodes, and 3) HPC-grade FPGA nodes. On the Top500 list, which ranks the 500 most powerful supercomputers in the world, Otus is in position 164 with the CPU partition and in position 255 with the GPU partition (June 2025). On the Green500 list, ranking the 500 most energy-efficient supercomputers in the world, Otus is in position 5 with the GPU partition (June 2025).\n  This article provides a comprehensive overview of the system in terms of its hardware, software, system integration, and its overall integration into the data center building to ensure energy-efficient operation. The article aims to provide unique insights for scientists using the system and for other centers operating HPC clusters. The article will be continuously updated to reflect the latest system setup and measurements.", "AI": {"tldr": "Otus\u662f\u5fb7\u56fd\u5e15\u5fb7\u535a\u6069\u5927\u5b66\u9ad8\u6027\u80fd\u8ba1\u7b97\u96c6\u7fa4\uff0c\u6027\u80fd\u7ea6\u4e3aNoctua 2\u7684\u4e24\u500d\uff0c\u5177\u5907CPU\u3001GPU\u4e0eFPGA\u4e09\u79cd\u8282\u70b9\uff0c\u5728Top500\u548cGreen500\u699c\u5355\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u4e3a\u79d1\u5b66\u7528\u6237\u63d0\u4f9b\u7cfb\u7edf\u6d1e\u5bdf\uff0c\u5e76\u4e3a\u5176\u4ed6HPC\u4e2d\u5fc3\u63d0\u4f9b\u53c2\u8003\uff0c\u63a8\u52a8\u9ad8\u6548\u80fd\u4e0e\u8282\u80fd\u8fd0\u7b97\u53d1\u5c55\u3002", "method": "\u4ecb\u7ecdOtus\u786c\u4ef6\u67b6\u6784\u3001\u8f6f\u4ef6\u73af\u5883\u3001\u7cfb\u7edf\u96c6\u6210\u53ca\u6570\u636e\u4e2d\u5fc3\u80fd\u6548\u8bbe\u8ba1\uff0c\u5e76\u6301\u7eed\u66f4\u65b0\u6700\u65b0\u914d\u7f6e\u4e0e\u5b9e\u6d4b\u6570\u636e\u3002", "result": "Otus\u5728Top500\u4e2dCPU\u5206\u533a\u6392\u540d\u7b2c164\u3001GPU\u5206\u533a\u7b2c255\uff1b\u5728Green500\u4e2dGPU\u5206\u533a\u9ad8\u5c45\u7b2c5\uff0c\u5c55\u73b0\u5353\u8d8a\u80fd\u6548\u3002", "conclusion": "Otus\u4f5c\u4e3aNHR\u8ba1\u5212\u5173\u952e\u8bbe\u65bd\uff0c\u517c\u987e\u9ad8\u6027\u80fd\u4e0e\u7eff\u8272\u8282\u80fd\uff0c\u4e3a\u79d1\u7814\u4e0eHPC\u8fd0\u8425\u63d0\u4f9b\u91cd\u8981\u8303\u4f8b\u3002"}}
{"id": "2512.07520", "categories": ["cs.AR", "cs.CR", "cs.SC"], "pdf": "https://arxiv.org/pdf/2512.07520", "abs": "https://arxiv.org/abs/2512.07520", "authors": ["No\u00e9 Amiot", "Quentin L. Meunier", "Karine Heydemann", "Emmanuelle Encrenaz"], "title": "aLEAKator: HDL Mixed-Domain Simulation for Masked Hardware \\& Software Formal Verification", "comment": null, "summary": "Verifying the security of masked hardware and software implementations, under advanced leakage models, remains a significant challenge, especially then accounting for glitches, transitions and CPU micro-architectural specifics. Existing verification approaches are either restricted to small hardware gadgets, small programs on CPUs such as Sboxes, limited leakage models, or require hardware-specific prior knowledge. In this work, we present aLEAKator, an open-source framework for the automated formal verification of masked cryptographic accelerators and software running on CPUs from their HDL descriptions. Our method introduces mixed-domain simulation, enabling precise modeling and verification under various (including robust and relaxed) 1-probing leakage models, and supports variable signal granularity without being restricted to 1-bit wires. aLEAKator also supports verification in the presence of lookup tables, and does not require prior knowledge of the target CPU architecture. Our approach is validated against existing tools and real-world measurements while providing innovative results such as the verification of a full, first-order masked AES on various CPUs", "AI": {"tldr": "aLEAKator\u662f\u4e00\u4e2a\u5f00\u6e90\u6846\u67b6\uff0c\u7528\u4e8e\u4eceHDL\u63cf\u8ff0\u4e2d\u81ea\u52a8\u5f62\u5f0f\u5316\u9a8c\u8bc1\u63a9\u7801\u52a0\u5bc6\u52a0\u901f\u5668\u548cCPU\u4e0a\u8fd0\u884c\u7684\u8f6f\u4ef6\u3002", "motivation": "\u73b0\u6709\u9a8c\u8bc1\u65b9\u6cd5\u53d7\u9650\u4e8e\u5c0f\u578b\u786c\u4ef6\u6a21\u5757\u3001\u5c0f\u89c4\u6a21\u7a0b\u5e8f\u3001\u6709\u9650\u6cc4\u6f0f\u6a21\u578b\u6216\u9700\u786c\u4ef6\u5148\u9a8c\u77e5\u8bc6\uff0c\u96be\u4ee5\u5e94\u5bf9\u590d\u6742\u6cc4\u6f0f\u6a21\u578b\u4e0b\u7684\u5b89\u5168\u9a8c\u8bc1\u6311\u6218\u3002", "method": "\u5f15\u5165\u6df7\u5408\u57df\u4eff\u771f\uff0c\u652f\u6301\u591a\u79cd1-\u63a2\u9488\u6cc4\u6f0f\u6a21\u578b\uff0c\u5141\u8bb8\u53ef\u53d8\u4fe1\u53f7\u7c92\u5ea6\uff0c\u65e0\u9700\u9650\u4e8e1\u4f4d\u7ebf\uff0c\u5e76\u652f\u6301\u67e5\u627e\u8868\u53ca\u65e0\u9700\u76ee\u6807CPU\u67b6\u6784\u5148\u9a8c\u77e5\u8bc6\u3002", "result": "\u9a8c\u8bc1\u4e86\u5305\u62ec\u5b8c\u6574\u4e00\u9636\u63a9\u7801AES\u5728\u4e0d\u540cCPU\u4e0a\u7684\u5b9e\u73b0\uff0c\u7ed3\u679c\u4e0e\u73b0\u6709\u5de5\u5177\u548c\u5b9e\u9645\u6d4b\u91cf\u4e00\u81f4\u3002", "conclusion": "aLEAKator\u63d0\u4f9b\u4e86\u4e00\u79cd\u901a\u7528\u4e14\u7cbe\u786e\u7684\u63a9\u7801\u5b9e\u73b0\u5b89\u5168\u9a8c\u8bc1\u65b9\u6cd5\uff0c\u9002\u7528\u4e8e\u786c\u4ef6\u548c\u8f6f\u4ef6\u573a\u666f\u3002"}}
{"id": "2512.06906", "categories": ["cs.SE", "cs.CR", "cs.DB", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.06906", "abs": "https://arxiv.org/abs/2512.06906", "authors": ["Wenjie Zhang", "Yun Lin", "Chun Fung Amos Kwok", "Xiwen Teoh", "Xiaofei Xie", "Frank Liauw", "Hongyu Zhang", "Jin Song Dong"], "title": "MINES: Explainable Anomaly Detection through Web API Invariant Inference", "comment": null, "summary": "Detecting the anomalies of web applications, important infrastructures for running modern companies and governments, is crucial for providing reliable web services. Many modern web applications operate on web APIs (e.g., RESTful, SOAP, and WebSockets), their exposure invites intended attacks or unintended illegal visits, causing abnormal system behaviors. However, such anomalies can share very similar logs with normal logs, missing crucial information (which could be in database) for log discrimination. Further, log instances can be also noisy, which can further mislead the state-of-the-art log learning solutions to learn spurious correlation, resulting superficial models and rules for anomaly detection. In this work, we propose MINES which infers explainable API invariants for anomaly detection from the schema level instead of detailed raw log instances, which can (1) significantly discriminate noise in logs to identify precise normalities and (2) detect abnormal behaviors beyond the instrumented logs. Technically, MINES (1) converts API signatures into table schema to enhance the original database shema; and (2) infers the potential database constraints on the enhanced database schema to capture the potential relationships between APIs and database tables. MINES uses LLM for extracting potential relationship based on two given table structures; and use normal log instances to reject and accept LLM-generated invariants. Finally, MINES translates the inferred constraints into invariants to generate Python code for verifying the runtime logs. We extensively evaluate MINES on web-tamper attacks on the benchmarks of TrainTicket, NiceFish, Gitea, Mastodon, and NextCloud against baselines such as LogRobust, LogFormer, and WebNorm. The results show that MINES achieves high recall for the anomalies while introducing almost zero false positives, indicating a new state-of-the-art.", "AI": {"tldr": "MINES\u901a\u8fc7\u4eceAPI\u7b7e\u540d\u63a8\u65ad\u53ef\u89e3\u91ca\u7684\u4e0d\u53d8\u91cf\uff0c\u4ee5\u68c0\u6d4bWeb\u5e94\u7528\u4e2d\u7684\u5f02\u5e38\u884c\u4e3a\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u4ee3Web\u5e94\u7528\u5e38\u56e0API\u66b4\u9732\u800c\u906d\u53d7\u653b\u51fb\u6216\u975e\u6cd5\u8bbf\u95ee\uff0c\u5bfc\u81f4\u7cfb\u7edf\u5f02\u5e38\uff0c\u4f46\u73b0\u6709\u65e5\u5fd7\u5206\u6790\u65b9\u6cd5\u6613\u53d7\u566a\u58f0\u5e72\u6270\uff0c\u96be\u4ee5\u7cbe\u51c6\u8bc6\u522b\u5f02\u5e38\u3002", "method": "MINES\u5c06API\u7b7e\u540d\u8f6c\u6362\u4e3a\u8868\u7ed3\u6784\u589e\u5f3a\u6570\u636e\u5e93\u6a21\u5f0f\uff0c\u5229\u7528LLM\u63a8\u65ad\u6f5c\u5728\u7ea6\u675f\uff0c\u5e76\u7ed3\u5408\u6b63\u5e38\u65e5\u5fd7\u7b5b\u9009\u751f\u6210\u4e0d\u53d8\u91cf\uff0c\u6700\u7ec8\u8f6c\u5316\u4e3aPython\u4ee3\u7801\u9a8c\u8bc1\u8fd0\u884c\u65f6\u65e5\u5fd7\u3002", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cMINES\u5bf9\u5f02\u5e38\u7684\u53ec\u56de\u7387\u9ad8\u4e14\u51e0\u4e4e\u65e0\u8bef\u62a5\uff0c\u6027\u80fd\u8d85\u8d8aLogRobust\u3001LogFormer\u548cWebNorm\u7b49\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "MINES\u63d0\u4f9b\u4e86\u4e00\u79cd\u65b0\u9896\u4e14\u9ad8\u6548\u7684Web\u5e94\u7528\u5f02\u5e38\u68c0\u6d4b\u65b9\u6848\uff0c\u5177\u5907\u9ad8\u7cbe\u5ea6\u4e0e\u5f3a\u89e3\u91ca\u6027\uff0c\u8fbe\u5230\u5f53\u524d\u6700\u4f18\u6c34\u5e73\u3002"}}
{"id": "2512.06915", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2512.06915", "abs": "https://arxiv.org/abs/2512.06915", "authors": ["Kelin Fu", "Tianyu Liu", "Zeyu Shang", "Yingwei Ma", "Jian Yang", "Jiaheng Liu", "Kaigui Bian"], "title": "Multi-Docker-Eval: A `Shovel of the Gold Rush' Benchmark on Automatic Environment Building for Software Engineering", "comment": null, "summary": "Automated environment configuration is a critical bottleneck in scaling software engineering (SWE) automation. To provide a reliable evaluation standard for this task, we present Multi-Docker-Eval benchmark. It includes 40 real-world repositories spanning 9 programming languages and measures both success in achieving executable states and efficiency under realistic constraints. Our extensive evaluation of state-of-the-art LLMs and agent frameworks reveals key insights: (1) the overall success rate of current models is low (F2P at most 37.7%), with environment construction being the primary bottleneck; (2) model size and reasoning length are not decisive factors, and open-source models like DeepSeek-V3.1 and Kimi-K2 are competitive in both efficiency and effectiveness; (3) agent framework and programming language also have significantly influence on success rate. These findings provide actionable guidelines for building scalable, fully automated SWE pipelines.", "AI": {"tldr": "\u63d0\u51faMulti-Docker-Eval\u57fa\u51c6\uff0c\u8bc4\u4f30LLM\u5728\u81ea\u52a8\u5316\u73af\u5883\u914d\u7f6e\u4e2d\u7684\u8868\u73b0\uff0c\u53d1\u73b0\u5f53\u524d\u6a21\u578b\u6210\u529f\u7387\u4f4e\uff0c\u4e14\u6a21\u578b\u89c4\u6a21\u975e\u51b3\u5b9a\u56e0\u7d20\u3002", "motivation": "\u89e3\u51b3\u8f6f\u4ef6\u5de5\u7a0b\u81ea\u52a8\u5316\u4e2d\u73af\u5883\u914d\u7f6e\u7684\u74f6\u9888\u95ee\u9898\uff0c\u63d0\u4f9b\u53ef\u9760\u8bc4\u4f30\u6807\u51c6\u3002", "method": "\u6784\u5efa\u6db5\u76d640\u4e2a\u771f\u5b9e\u4ed3\u5e93\u30019\u79cd\u7f16\u7a0b\u8bed\u8a00\u7684Multi-Docker-Eval\u57fa\u51c6\uff0c\u8bc4\u4f30\u4e3b\u6d41LLM\u4e0e\u667a\u80fd\u4f53\u6846\u67b6\u7684\u8868\u73b0\u3002", "result": "\u5f53\u524d\u6a21\u578b\u6700\u9ad8\u6210\u529f\u7387\u4ec537.7%\uff0c\u5f00\u6e90\u6a21\u578b\u5982DeepSeek-V3.1\u548cKimi-K2\u8868\u73b0\u4f18\u5f02\uff0c\u667a\u80fd\u4f53\u6846\u67b6\u4e0e\u7f16\u7a0b\u8bed\u8a00\u663e\u8457\u5f71\u54cd\u7ed3\u679c\u3002", "conclusion": "\u4e3a\u6784\u5efa\u53ef\u6269\u5c55\u7684\u5168\u81ea\u52a8\u8f6f\u4ef6\u5de5\u7a0b\u6d41\u6c34\u7ebf\u63d0\u4f9b\u5b9e\u7528\u6307\u5bfc\u3002"}}
{"id": "2512.07750", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2512.07750", "abs": "https://arxiv.org/abs/2512.07750", "authors": ["Roozbeh Bostandoost", "Pooria Namyar", "Siva Kesava Reddy Kakarla", "Ryan Beckett", "Santiago Segarra", "Eli Cortez", "Ankur Mallick", "Kevin Hsieh", "Rodrigo Fonseca", "Mohammad Hajiesmaili", "Behnaz Arzani"], "title": "A Performance Analyzer for a Public Cloud's ML-Augmented VM Allocator", "comment": null, "summary": "Many operational cloud systems use one or more machine learning models that help them achieve better efficiency and performance. But operators do not have tools to help them understand how each model and the interaction between them affect the end-to-end system performance. SANJESH is such a tool. SANJESH supports a diverse set of performance-related queries which we answer through a bi-level optimization. We invent novel mechanisms to solve this optimization more quickly. These techniques allow us to solve an optimization which prior work failed to solve even after $24$ hours.\n  As a proof of concept, we apply SANJESH to an example production system that uses multiple ML models to optimize virtual machine (VM) placement. These models impact how many servers the operators uses to host VMs and the frequency with which it has to live-migrate them because the servers run out of resources. SANJESH finds scenarios where these models cause $~4\\times$ worse performance than what simulation-based approaches detect.", "AI": {"tldr": "SANJESH\u662f\u4e00\u79cd\u7528\u4e8e\u5206\u6790\u4e91\u7cfb\u7edf\u4e2d\u591a\u4e2a\u673a\u5668\u5b66\u4e60\u6a21\u578b\u5bf9\u7aef\u5230\u7aef\u6027\u80fd\u5f71\u54cd\u7684\u5de5\u5177\uff0c\u901a\u8fc7\u53cc\u5c42\u4f18\u5316\u5feb\u901f\u5b9a\u4f4d\u6027\u80fd\u74f6\u9888\u3002", "motivation": "\u73b0\u6709\u4e91\u7cfb\u7edf\u7f3a\u4e4f\u5de5\u5177\u6765\u7406\u89e3\u591a\u4e2a\u673a\u5668\u5b66\u4e60\u6a21\u578b\u53ca\u5176\u4ea4\u4e92\u5982\u4f55\u5f71\u54cd\u6574\u4f53\u6027\u80fd\u3002", "method": "\u63d0\u51faSANJESH\u5de5\u5177\uff0c\u91c7\u7528\u53cc\u5c42\u4f18\u5316\u65b9\u6cd5\u5e76\u5f15\u5165\u65b0\u9896\u673a\u5236\u52a0\u901f\u6c42\u89e3\u3002", "result": "\u5728VM\u653e\u7f6e\u573a\u666f\u4e2d\uff0cSANJESH\u53d1\u73b0\u67d0\u4e9b\u6a21\u578b\u5bfc\u81f4\u7684\u6027\u80fd\u6bd4\u6a21\u62df\u65b9\u6cd5\u68c0\u6d4b\u7ed3\u679c\u5dee\u7ea64\u500d\u3002", "conclusion": "SANJESH\u80fd\u6709\u6548\u63ed\u793a\u591a\u6a21\u578b\u4ea4\u4e92\u5bf9\u7cfb\u7edf\u6027\u80fd\u7684\u5f71\u54cd\uff0c\u5e2e\u52a9\u4f18\u5316\u4e91\u7cfb\u7edf\u6548\u7387\u3002"}}
{"id": "2512.07792", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2512.07792", "abs": "https://arxiv.org/abs/2512.07792", "authors": ["Animesh Dangwal", "Yufeng Jiang", "Charlie Arnold", "Jun Fan", "Mohamed Bassem", "Aish Rajagopal"], "title": "Designing Co-operation in Systems of Hierarchical, Multi-objective Schedulers for Stream Processing", "comment": null, "summary": "Stream processing is a computing paradigm that supports real-time data processing for a wide variety of applications. At Meta, it's used across the company for various tasks such as deriving product insights, providing and improving user services, and enabling AI at scale for our ever-growing user base. Meta's current stream processing framework supports processing TerraBytes(TBs) of data in mere seconds. This is enabled by our efficient schedulers and multi-layered infrastructure, which allocate workloads across various compute resources, working together in hierarchies across various parts of the infrastructure. But with the ever growing complexity of applications, and user needs, areas of the infrastructure that previously required minimal load balancing, now must be made more robust and proactive to application load. In our work we explore how to build and design such a system that focuses on load balancing over key compute resources and properties of these applications. We also showcase how to integrate new schedulers into the hierarchy of the existing ones, allowing multiple schedulers to work together and perform load balancing, at their infrastructure level, effectively.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u4e86\u5728Meta\u6784\u5efa\u548c\u8bbe\u8ba1\u4e13\u6ce8\u4e8e\u5173\u952e\u8ba1\u7b97\u8d44\u6e90\u8d1f\u8f7d\u5747\u8861\u7684\u6d41\u5904\u7406\u7cfb\u7edf\uff0c\u5e76\u5c55\u793a\u4e86\u5982\u4f55\u5c06\u65b0\u8c03\u5ea6\u5668\u96c6\u6210\u5230\u73b0\u6709\u5c42\u6b21\u7ed3\u6784\u4e2d\u4ee5\u5b9e\u73b0\u9ad8\u6548\u534f\u4f5c\u3002", "motivation": "\u968f\u7740\u5e94\u7528\u590d\u6742\u6027\u548c\u7528\u6237\u9700\u6c42\u7684\u589e\u957f\uff0c\u539f\u5148\u53ea\u9700\u6700\u5c0f\u8d1f\u8f7d\u5747\u8861\u7684\u57fa\u7840\u8bbe\u65bd\u90e8\u5206\u73b0\u5728\u9700\u8981\u66f4\u5065\u58ee\u548c\u4e3b\u52a8\u7684\u8d1f\u8f7d\u7ba1\u7406\u3002", "method": "\u901a\u8fc7\u8bbe\u8ba1\u4e13\u6ce8\u4e8e\u5173\u952e\u8ba1\u7b97\u8d44\u6e90\u548c\u5e94\u7528\u5c5e\u6027\u7684\u8d1f\u8f7d\u5747\u8861\u7cfb\u7edf\uff0c\u5e76\u5c06\u65b0\u8c03\u5ea6\u5668\u96c6\u6210\u5230\u73b0\u6709\u8c03\u5ea6\u5668\u5c42\u6b21\u7ed3\u6784\u4e2d\uff0c\u5b9e\u73b0\u591a\u5c42\u6b21\u534f\u540c\u5de5\u4f5c\u3002", "result": "\u6210\u529f\u5b9e\u73b0\u4e86\u591a\u4e2a\u8c03\u5ea6\u5668\u5728\u5404\u81ea\u57fa\u7840\u8bbe\u65bd\u5c42\u7ea7\u4e0a\u7684\u6709\u6548\u8d1f\u8f7d\u5747\u8861\uff0c\u63d0\u5347\u4e86\u7cfb\u7edf\u7684\u6574\u4f53\u6027\u80fd\u4e0e\u53ef\u6269\u5c55\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u589e\u5f3a\u4e86\u6d41\u5904\u7406\u6846\u67b6\u5e94\u5bf9\u590d\u6742\u8d1f\u8f7d\u7684\u80fd\u529b\uff0c\u4e3a\u5927\u89c4\u6a21\u5b9e\u65f6\u6570\u636e\u5904\u7406\u63d0\u4f9b\u4e86\u66f4\u7a33\u5065\u7684\u652f\u6301\u3002"}}
{"id": "2512.07122", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.07122", "abs": "https://arxiv.org/abs/2512.07122", "authors": ["Liping Han", "Tingting Nie", "Le Yu", "Mingzhe Hu", "Tao Yue"], "title": "RisConFix: LLM-based Automated Repair of Risk-Prone Drone Configurations", "comment": null, "summary": "Flight control software is typically designed with numerous configurable parameters governing multiple functionalities, enabling flexible adaptation to mission diversity and environmental uncertainty. Although developers and manufacturers usually provide recommendations for these parameters to ensure safe and stable operations, certain combinations of parameters with recommended values may still lead to unstable flight behaviors, thereby degrading the drone's robustness. To this end, we propose a Large Language Model (LLM) based approach for real-time repair of risk-prone configurations (named RisConFix) that degrade drone robustness. RisConFix continuously monitors the drone's operational state and automatically triggers a repair mechanism once abnormal flight behaviors are detected. The repair mechanism leverages an LLM to analyze relationships between configuration parameters and flight states, and then generates corrective parameter updates to restore flight stability. To ensure the validity of the updated configuration, RisConFix operates as an iterative process; it continuously monitors the drone's flight state and, if an anomaly persists after applying an update, automatically triggers the next repair cycle. We evaluated RisConFix through a case study of ArduPilot (with 1,421 groups of misconfigurations). Experimental results show that RisConFix achieved a best repair success rate of 97% and an optimal average number of repairs of 1.17, demonstrating its capability to effectively and efficiently repair risk-prone configurations in real time.", "AI": {"tldr": "RisConFix\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\u5b9e\u65f6\u4fee\u590d\u65e0\u4eba\u673a\u98ce\u9669\u914d\u7f6e\uff0c\u63d0\u5347\u98de\u884c\u7a33\u5b9a\u6027\u3002", "motivation": "\u67d0\u4e9b\u63a8\u8350\u53c2\u6570\u7ec4\u5408\u4ecd\u53ef\u80fd\u5bfc\u81f4\u98de\u884c\u4e0d\u7a33\u5b9a\uff0c\u964d\u4f4e\u65e0\u4eba\u673a\u9c81\u68d2\u6027\uff0c\u9700\u5b9e\u65f6\u4fee\u590d\u673a\u5236\u3002", "method": "\u57fa\u4e8eLLM\u5206\u6790\u53c2\u6570\u4e0e\u98de\u884c\u72b6\u6001\u5173\u7cfb\uff0c\u751f\u6210\u4fee\u6b63\u53c2\u6570\u5e76\u8fed\u4ee3\u9a8c\u8bc1\u76f4\u81f3\u7a33\u5b9a\u3002", "result": "\u5728ArduPilot\u6d4b\u8bd5\u4e2d\u4fee\u590d\u6210\u529f\u7387\u6700\u9ad8\u8fbe97%\uff0c\u5e73\u5747\u4fee\u590d\u6b21\u6570\u4ec51.17\u6b21\u3002", "conclusion": "RisConFix\u80fd\u9ad8\u6548\u5b9e\u65f6\u4fee\u590d\u98ce\u9669\u914d\u7f6e\uff0c\u663e\u8457\u589e\u5f3a\u65e0\u4eba\u673a\u8fd0\u884c\u9c81\u68d2\u6027\u3002"}}
{"id": "2512.07799", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2512.07799", "abs": "https://arxiv.org/abs/2512.07799", "authors": ["Roozbeh Bostandoost", "Adam Lechowicz", "Walid A. Hanafy", "Prashant Shenoy", "Mohammad Hajiesmaili"], "title": "Quantifying the Carbon Reduction of DAG Workloads: A Job Shop Scheduling Perspective", "comment": null, "summary": "Carbon-aware schedulers aim to reduce the operational carbon footprint of data centers by running flexible workloads during periods of low carbon intensity. Most schedulers treat workloads as single monolithic tasks, ignoring that many jobs, like video encoding or offline inference, consist of smaller tasks with specific dependencies and resource needs; however, knowledge of this structure enables opportunities for greater carbon efficiency.\n  We quantify the maximum benefit of a dependency-aware approach for batch workloads. We model the problem as a flexible job-shop scheduling variant and use an offline solver to compute upper bounds on carbon and energy savings. Results show up to $25\\%$ lower carbon emissions on average without increasing the optimal makespan (total job completion time) compared to a makespan-only baseline. Although in heterogeneous server setup, these schedules may use more energy than energy-optimal ones. Our results also show that allowing twice the optimal makespan nearly doubles the carbon savings, underscoring the tension between carbon, energy, and makespan. We also highlight key factors such as job structure and server count influence the achievable carbon reductions.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u4f9d\u8d56\u611f\u77e5\u7684\u78b3\u611f\u77e5\u8c03\u5ea6\u65b9\u6cd5\uff0c\u4ee5\u4f18\u5316\u6570\u636e\u4e2d\u5fc3\u6279\u5904\u7406\u4f5c\u4e1a\u7684\u78b3\u6392\u653e\u3002", "motivation": "\u73b0\u6709\u8c03\u5ea6\u5668\u5ffd\u7565\u4f5c\u4e1a\u5185\u90e8\u4efb\u52a1\u7ed3\u6784\uff0c\u5bfc\u81f4\u78b3\u6548\u7387\u4f4e\u4e0b\uff0c\u4f5c\u8005\u5e0c\u671b\u901a\u8fc7\u5229\u7528\u4efb\u52a1\u4f9d\u8d56\u5173\u7cfb\u63d0\u5347\u78b3\u51cf\u6392\u6548\u679c\u3002", "method": "\u5c06\u95ee\u9898\u5efa\u6a21\u4e3a\u67d4\u6027\u4f5c\u4e1a\u8f66\u95f4\u8c03\u5ea6\u53d8\u4f53\uff0c\u5e76\u4f7f\u7528\u79bb\u7ebf\u6c42\u89e3\u5668\u8ba1\u7b97\u78b3\u6392\u653e\u548c\u80fd\u8017\u8282\u7701\u7684\u4e0a\u9650\u3002", "result": "\u76f8\u6bd4\u4ec5\u4f18\u5316\u5b8c\u6210\u65f6\u95f4\u7684\u57fa\u7ebf\uff0c\u5e73\u5747\u51cf\u5c1125%\u78b3\u6392\u653e\uff1b\u5141\u8bb8\u53cc\u500d\u5b8c\u6210\u65f6\u95f4\u53ef\u8fd1\u4f3c\u7ffb\u500d\u78b3\u8282\u7701\uff0c\u4f46\u5f02\u6784\u670d\u52a1\u5668\u4e0b\u53ef\u80fd\u589e\u52a0\u80fd\u8017\u3002", "conclusion": "\u4f5c\u4e1a\u7ed3\u6784\u548c\u670d\u52a1\u5668\u6570\u91cf\u663e\u8457\u5f71\u54cd\u78b3\u51cf\u6392\u6f5c\u529b\uff0c\u78b3\u3001\u80fd\u8017\u4e0e\u5b8c\u6210\u65f6\u95f4\u4e4b\u95f4\u5b58\u5728\u6743\u8861\u5173\u7cfb\u3002"}}
{"id": "2512.07193", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2512.07193", "abs": "https://arxiv.org/abs/2512.07193", "authors": ["Manthan Shenoy", "Andreas Rausch"], "title": "Towards Benchmarking Design Pattern Detection Under Obfuscation: Reproducing and Evaluating Attention-Based Detection Method", "comment": "Pre-peer-review version of the paper submitted to the Workshop Track of the European Conference on Software Architecture (ECSA 2025), Springer LNCS 15982. Dataset: https://github.com/manthan410/Benchmark_dpd_att. Version of Record: https://doi.org/10.1007/978-3-032-04403-7_13", "summary": "This paper investigates the semantic robustness of attention-based classifiers for design pattern detection, particularly focusing on their reliance on structural and behavioral semantics. We reproduce the DPDAtt, an attention-based design pattern detection approach using learning-based classifiers, and evaluate its performance under obfuscation. To this end, we curate an obfuscated version of the DPDAtt Corpus, where the name identifiers in code such as class names, method names, etc., and string literals like print statements and comment blocks are replaced while preserving control flow, inheritance, and logic. Our findings reveal that these trained classifiers in DPDAtt depend significantly on superficial syntactic features, leading to substantial misclassification when such cues are removed through obfuscation. This work highlights the need for more robust detection tools capable of capturing deeper semantic meanings in source code. We propose our curated Obfuscated corpus (containing 34 Java source files) as a reusable proof-of-concept benchmark for evaluating state-of-the-art design pattern detectors on their true semantic generalization capabilities.", "AI": {"tldr": "\u8be5\u8bba\u6587\u8bc4\u4f30\u4e86\u57fa\u4e8e\u6ce8\u610f\u529b\u673a\u5236\u7684\u5206\u7c7b\u5668\u5728\u8bbe\u8ba1\u6a21\u5f0f\u68c0\u6d4b\u4e2d\u7684\u8bed\u4e49\u9c81\u68d2\u6027\uff0c\u53d1\u73b0\u5176\u8fc7\u5ea6\u4f9d\u8d56\u8868\u5c42\u8bed\u6cd5\u7279\u5f81\uff0c\u5728\u4ee3\u7801\u6df7\u6dc6\u540e\u6027\u80fd\u663e\u8457\u4e0b\u964d\u3002", "motivation": "\u73b0\u6709\u8bbe\u8ba1\u6a21\u5f0f\u68c0\u6d4b\u5de5\u5177\u53ef\u80fd\u4f9d\u8d56\u975e\u8bed\u4e49\u7279\u5f81\uff0c\u9700\u8bc4\u4f30\u5176\u771f\u5b9e\u8bed\u4e49\u6cdb\u5316\u80fd\u529b\u3002", "method": "\u590d\u73b0DPDAtt\u65b9\u6cd5\uff0c\u5e76\u6784\u5efa\u6df7\u6dc6\u7248\u8bed\u6599\u5e93\uff08\u66ff\u6362\u6807\u8bc6\u7b26\u548c\u5b57\u7b26\u4e32\uff0c\u4fdd\u7559\u7ed3\u6784\u903b\u8f91\uff09\uff0c\u6d4b\u8bd5\u5206\u7c7b\u5668\u6027\u80fd\u53d8\u5316\u3002", "result": "\u6df7\u6dc6\u540e\u5206\u7c7b\u5668\u6027\u80fd\u5927\u5e45\u4e0b\u964d\uff0c\u8868\u660e\u5176\u4e25\u91cd\u4f9d\u8d56\u8868\u5c42\u8bed\u6cd5\u800c\u975e\u6df1\u5c42\u8bed\u4e49\u3002", "conclusion": "\u9700\u5f00\u53d1\u66f4\u9c81\u68d2\u7684\u8bbe\u8ba1\u6a21\u5f0f\u68c0\u6d4b\u5de5\u5177\uff0c\u5e76\u63a8\u8350\u6240\u6784\u5efa\u6df7\u6dc6\u8bed\u6599\u4f5c\u4e3a\u8bc4\u4f30\u57fa\u51c6\u3002"}}
{"id": "2512.07293", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2512.07293", "abs": "https://arxiv.org/abs/2512.07293", "authors": ["Roberto Verdecchia", "Justus Bogner"], "title": "The Human Need for Storytelling: Reflections on Qualitative Software Engineering Research With a Focus Group of Experts", "comment": "Published in ACM SIGSOFT Software Engineering Notes (SEN), Volume 51, Issue 1, 2026", "summary": "From its first adoption in the late 80s, qualitative research has slowly but steadily made a name for itself in what was, and perhaps still is, the predominantly quantitative software engineering (SE) research landscape. As part of our regular column on empirical software engineering (ACM SIGSOFT SEN-ESE), we reflect on the state of qualitative SE research with a focus group of experts. Among other things, we discuss why qualitative SE research is important, how it evolved over time, common impediments faced while practicing it today, and what the future of qualitative SE research might look like. Joining the conversation are Rashina Hoda (Monash University, Australia), Carolyn Seaman (University of Maryland, United States), and Klaas Stol (University College Cork, Ireland). The content of this paper is a faithful account of our conversation from October 25, 2025, which we moderated and edited for our column.", "AI": {"tldr": "\u672c\u6587\u56de\u987e\u4e86\u5b9a\u6027\u7814\u7a76\u5728\u8f6f\u4ef6\u5de5\u7a0b\u9886\u57df\u7684\u53d1\u5c55\u5386\u7a0b\u3001\u91cd\u8981\u6027\u53ca\u672a\u6765\u5c55\u671b\u3002", "motivation": "\u63a2\u8ba8\u5b9a\u6027\u7814\u7a76\u5728\u4ee5\u5b9a\u91cf\u4e3a\u4e3b\u5bfc\u7684\u8f6f\u4ef6\u5de5\u7a0b\u7814\u7a76\u4e2d\u7684\u4ef7\u503c\u4e0e\u6311\u6218\u3002", "method": "\u901a\u8fc7\u4e13\u5bb6\u7126\u70b9\u5c0f\u7ec4\u8ba8\u8bba\u7684\u5f62\u5f0f\u8fdb\u884c\u53cd\u601d\u4e0e\u5206\u6790\u3002", "result": "\u63ed\u793a\u4e86\u5b9a\u6027\u7814\u7a76\u7684\u6f14\u8fdb\u8fc7\u7a0b\u3001\u5f53\u524d\u5b9e\u8df5\u969c\u788d\u53ca\u672a\u6765\u53ef\u80fd\u65b9\u5411\u3002", "conclusion": "\u5b9a\u6027\u7814\u7a76\u867d\u9762\u4e34\u6311\u6218\uff0c\u4f46\u5728\u8f6f\u4ef6\u5de5\u7a0b\u9886\u57df\u5177\u6709\u4e0d\u53ef\u66ff\u4ee3\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2512.07404", "categories": ["cs.SE", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.07404", "abs": "https://arxiv.org/abs/2512.07404", "authors": ["Francisco Ribeiro", "Claudio Spiess", "Prem Devanbu", "Sarah Nadi"], "title": "Do LLMs Trust the Code They Write?", "comment": null, "summary": "Despite the effectiveness of large language models (LLMs) for code generation, they often output incorrect code. One reason is that model output probabilities are often not well-correlated with correctness, and reflect only the final output of the generation process. Inspired by findings that LLMs internally encode concepts like truthfulness, this paper explores if LLMs similarly represent code correctness. Specifically, we identify a correctness representation inside LLMs by contrasting the hidden states between pairs of correct and incorrect code for the same programming tasks. By experimenting on four LLMs, we show that exploiting this extracted correctness representation outperforms standard log-likelihood ranking, as well as verbalized model confidence. Furthermore, we explore how this internal correctness signal can be used to select higher-quality code samples, without requiring test execution. Ultimately, this work demonstrates how leveraging internal representations can enhance code generation systems and make LLMs more reliable, thus improving confidence in automatically generated code.", "AI": {"tldr": "\u901a\u8fc7\u6316\u6398\u5927\u8bed\u8a00\u6a21\u578b\u5185\u90e8\u7684\u6b63\u786e\u6027\u8868\u5f81\uff0c\u63d0\u5347\u4ee3\u7801\u751f\u6210\u7684\u53ef\u9760\u6027\uff0c\u65e0\u9700\u6267\u884c\u6d4b\u8bd5\u5373\u53ef\u7b5b\u9009\u9ad8\u8d28\u91cf\u4ee3\u7801\u3002", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b\u751f\u6210\u7684\u4ee3\u7801\u5e38\u56e0\u8f93\u51fa\u6982\u7387\u4e0e\u6b63\u786e\u6027\u4e0d\u76f8\u5173\u800c\u9519\u8bef\uff0c\u9700\u6539\u8fdb\u5176\u53ef\u9760\u6027\u3002", "method": "\u5bf9\u6bd4\u540c\u4e00\u7f16\u7a0b\u4efb\u52a1\u4e2d\u6b63\u786e\u4e0e\u9519\u8bef\u4ee3\u7801\u7684\u9690\u85cf\u72b6\u6001\uff0c\u63d0\u53d6\u5185\u90e8\u6b63\u786e\u6027\u8868\u5f81\uff0c\u5e76\u5728\u56db\u4e2a\u6a21\u578b\u4e0a\u9a8c\u8bc1\u5176\u6548\u679c\u3002", "result": "\u8be5\u65b9\u6cd5\u4f18\u4e8e\u6807\u51c6\u5bf9\u6570\u4f3c\u7136\u6392\u5e8f\u548c\u6a21\u578b\u53e3\u5934\u7f6e\u4fe1\u5ea6\uff0c\u80fd\u6709\u6548\u7b5b\u9009\u66f4\u9ad8\u8d28\u91cf\u4ee3\u7801\u3002", "conclusion": "\u5229\u7528\u5185\u90e8\u8868\u5f81\u53ef\u589e\u5f3a\u4ee3\u7801\u751f\u6210\u7cfb\u7edf\uff0c\u63d0\u9ad8\u81ea\u52a8\u751f\u6210\u4ee3\u7801\u7684\u53ef\u4fe1\u5ea6\u3002"}}
{"id": "2512.07501", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.07501", "abs": "https://arxiv.org/abs/2512.07501", "authors": ["Weilin Luo", "Xueyi Liang", "Haotian Deng", "Yanan Liu", "Hai Wan"], "title": "AutoICE: Automatically Synthesizing Verifiable C Code via LLM-driven Evolution", "comment": null, "summary": "Automatically synthesizing verifiable code from natural language requirements ensures software correctness and reliability while significantly lowering the barrier to adopting the techniques of formal methods. With the rise of large language models (LLMs), long-standing efforts at autoformalization have gained new momentum. However, existing approaches suffer from severe syntactic and semantic errors due to the scarcity of domain-specific pre-training corpora and often fail to formalize implicit knowledge effectively. In this paper, we propose AutoICE, an LLM-driven evolutionary search for synthesizing verifiable C code. It introduces the diverse individual initialization and the collaborative crossover to enable diverse iterative updates, thereby mitigating error propagation inherent in single-agent iterations. Besides, it employs the self-reflective mutation to facilitate the discovery of implicit knowledge. Evaluation results demonstrate the effectiveness of AutoICE: it successfully verifies $90.36$\\% of code, outperforming the state-of-the-art (SOTA) approach. Besides, on a developer-friendly dataset variant, AutoICE achieves a $88.33$\\% verification success rate, significantly surpassing the $65$\\% success rate of the SOTA approach.", "AI": {"tldr": "AutoICE\u662f\u4e00\u79cd\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u8fdb\u5316\u641c\u7d22\u65b9\u6cd5\uff0c\u7528\u4e8e\u4ece\u81ea\u7136\u8bed\u8a00\u9700\u6c42\u81ea\u52a8\u751f\u6210\u53ef\u9a8c\u8bc1\u7684C\u4ee3\u7801\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u6280\u672f\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u56e0\u7f3a\u4e4f\u9886\u57df\u7279\u5b9a\u8bed\u6599\u548c\u96be\u4ee5\u5f62\u5f0f\u5316\u9690\u542b\u77e5\u8bc6\uff0c\u5bfc\u81f4\u8bed\u6cd5\u4e0e\u8bed\u4e49\u9519\u8bef\u9891\u53d1\u3002", "method": "\u63d0\u51fa\u591a\u6837\u5316\u4e2a\u4f53\u521d\u59cb\u5316\u3001\u534f\u540c\u4ea4\u53c9\u64cd\u4f5c\u548c\u81ea\u7701\u5f0f\u53d8\u5f02\u673a\u5236\uff0c\u4ee5\u652f\u6301\u591a\u6837\u5316\u7684\u8fed\u4ee3\u66f4\u65b0\u5e76\u6316\u6398\u9690\u542b\u77e5\u8bc6\u3002", "result": "\u5728\u6807\u51c6\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u6210\u529f\u7387\u8fbe90.36%\uff0c\u5728\u5f00\u53d1\u8005\u53cb\u597d\u6570\u636e\u96c6\u4e0a\u8fbe88.33%\uff0c\u5747\u663e\u8457\u8d85\u8d8a\u5f53\u524d\u6700\u4f18\u65b9\u6cd5\u3002", "conclusion": "AutoICE\u6709\u6548\u63d0\u5347\u4e86\u4ece\u81ea\u7136\u8bed\u8a00\u751f\u6210\u53ef\u9a8c\u8bc1\u4ee3\u7801\u7684\u51c6\u786e\u7387\u4e0e\u5b9e\u7528\u6027\uff0c\u63a8\u52a8\u4e86\u5f62\u5f0f\u5316\u65b9\u6cd5\u7684\u666e\u53ca\u3002"}}
{"id": "2512.07814", "categories": ["cs.SE", "cs.AI", "cs.CR"], "pdf": "https://arxiv.org/pdf/2512.07814", "abs": "https://arxiv.org/abs/2512.07814", "authors": ["Hua Yang", "Alejandro Velasco", "Sen Fang", "Bowen Xu", "Denys Poshyvanyk"], "title": "Understanding Privacy Risks in Code Models Through Training Dynamics: A Causal Approach", "comment": "21 pages, 8 figures", "summary": "Large language models for code (LLM4Code) have greatly improved developer productivity but also raise privacy concerns due to their reliance on open-source repositories containing abundant personally identifiable information (PII). Prior work shows that commercial models can reproduce sensitive PII, yet existing studies largely treat PII as a single category and overlook the heterogeneous risks among different types. We investigate whether distinct PII types vary in their likelihood of being learned and leaked by LLM4Code, and whether this relationship is causal. Our methodology includes building a dataset with diverse PII types, fine-tuning representative models of different scales, computing training dynamics on real PII data, and formulating a structural causal model to estimate the causal effect of learnability on leakage. Results show that leakage risks differ substantially across PII types and correlate with their training dynamics: easy-to-learn instances such as IP addresses exhibit higher leakage, while harder types such as keys and passwords leak less frequently. Ambiguous types show mixed behaviors. This work provides the first causal evidence that leakage risks are type-dependent and offers guidance for developing type-aware and learnability-aware defenses for LLM4Code.", "AI": {"tldr": "\u8be5\u7814\u7a76\u9996\u6b21\u63ed\u793a\u4ee3\u7801\u5927\u6a21\u578b\u4e2d\u4e0d\u540c\u7c7b\u578b\u7684\u4e2a\u4eba\u8eab\u4efd\u4fe1\u606f\uff08PII\uff09\u6cc4\u9732\u98ce\u9669\u5b58\u5728\u56e0\u679c\u5dee\u5f02\uff0c\u6613\u5b66\u7c7b\u578b\u5982IP\u5730\u5740\u66f4\u6613\u6cc4\u9732\uff0c\u800c\u5bc6\u7801\u7b49\u96be\u5b66\u7c7b\u578b\u6cc4\u9732\u8f83\u5c11\uff0c\u4e3a\u6784\u5efa\u7c7b\u578b\u611f\u77e5\u9632\u5fa1\u63d0\u4f9b\u4f9d\u636e\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u5c06PII\u89c6\u4e3a\u5355\u4e00\u7c7b\u522b\uff0c\u5ffd\u89c6\u4e0d\u540c\u7c7b\u578b\u95f4\u7684\u5f02\u8d28\u98ce\u9669\uff0c\u672c\u6587\u65e8\u5728\u63a2\u7a76\u4e0d\u540cPII\u7c7b\u578b\u5728\u4ee3\u7801\u5927\u6a21\u578b\u4e2d\u7684\u5b66\u4e60\u4e0e\u6cc4\u9732\u98ce\u9669\u662f\u5426\u5b58\u5728\u56e0\u679c\u5173\u7cfb\u3002", "method": "\u6784\u5efa\u591a\u7c7b\u578bPII\u6570\u636e\u96c6\uff0c\u5fae\u8c03\u4e0d\u540c\u89c4\u6a21\u6a21\u578b\uff0c\u8ba1\u7b97\u771f\u5b9ePII\u6570\u636e\u7684\u8bad\u7ec3\u52a8\u6001\uff0c\u5e76\u5efa\u7acb\u7ed3\u6784\u56e0\u679c\u6a21\u578b\u4f30\u8ba1\u53ef\u5b66\u4e60\u6027\u5bf9\u6cc4\u9732\u7684\u56e0\u679c\u6548\u5e94\u3002", "result": "\u6cc4\u9732\u98ce\u9669\u56e0PII\u7c7b\u578b\u663e\u8457\u4e0d\u540c\uff1a\u6613\u5b66\u7c7b\u578b\uff08\u5982IP\u5730\u5740\uff09\u6cc4\u9732\u7387\u9ad8\uff0c\u96be\u5b66\u7c7b\u578b\uff08\u5982\u5bc6\u94a5\u3001\u5bc6\u7801\uff09\u6cc4\u9732\u5c11\uff0c\u6a21\u7cca\u7c7b\u578b\u8868\u73b0\u6df7\u5408\u3002", "conclusion": "\u9996\u6b21\u63d0\u4f9b\u56e0\u679c\u8bc1\u636e\u8868\u660e\u6cc4\u9732\u98ce\u9669\u4f9d\u8d56\u4e8ePII\u7c7b\u578b\uff0c\u5efa\u8bae\u5f00\u53d1\u7c7b\u578b\u611f\u77e5\u548c\u53ef\u5b66\u4e60\u6027\u611f\u77e5\u7684\u9632\u5fa1\u673a\u5236\u4ee5\u63d0\u5347\u4ee3\u7801\u5927\u6a21\u578b\u9690\u79c1\u4fdd\u62a4\u3002"}}
{"id": "2512.07824", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2512.07824", "abs": "https://arxiv.org/abs/2512.07824", "authors": ["Rabe Abdalkareem"], "title": "Studying the Role of Reusing Crowdsourcing Knowledge in Software Development", "comment": null, "summary": "Crowdsourcing platforms, such as Stack Overflow, have changed and impacted the software development practice. In these platforms, developers share and reuse their software development and programming experience. Therefore, a plethora of research work focused on crowdsourcing in software engineering and showed that, among other things, crowdsourced development tends to increase developers' productivity and reduce time-to-market. However, in crowdsourcing, the empirical studies of software quality are lacking, and simple questions, such as what developers use the crowdsourcing knowledge for, are unanswered.\n  Therefore, our research focused on studying the impact of reusing crowdsourcing knowledge on software projects. To do so, we conduct several large-scale empirical studies on some of the well-known crowdsourcing platforms, including Stack Overflow and npm. Our results showed that reusing knowledge from these crowdsourcing platforms has the potential to assist software development practice, specifically in the form of reusing crowdsourced code. However, using such knowledge affects the quality of the software in several aspects, such as making the software projects suffer from dependency overhead and increasing the maintenance effort. Based on these findings, we use the gained knowledge to make sound data-driven decisions where we examine software quality assurance methods to mitigate the risk of relying on crowd sourcing knowledge in software development. We examine the use of continuous integration (CI). Our analysis showed how CI can be improved to increase developers' productivity and save their resources.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u4e86\u91cd\u7528\u4f17\u5305\u77e5\u8bc6\u5bf9\u8f6f\u4ef6\u9879\u76ee\u7684\u5f71\u54cd\uff0c\u53d1\u73b0\u5176\u867d\u80fd\u63d0\u5347\u5f00\u53d1\u6548\u7387\uff0c\u4f46\u4e5f\u5e26\u6765\u4f9d\u8d56\u8fc7\u8f7d\u548c\u7ef4\u62a4\u6210\u672c\u589e\u52a0\u7b49\u95ee\u9898\uff0c\u5e76\u63d0\u51fa\u901a\u8fc7\u6301\u7eed\u96c6\u6210\u6539\u8fdb\u6765\u7f13\u89e3\u98ce\u9669\u3002", "motivation": "\u5f53\u524d\u4f17\u5305\u5e73\u53f0\u5982Stack Overflow\u7f3a\u4e4f\u5bf9\u8f6f\u4ef6\u8d28\u91cf\u5f71\u54cd\u7684\u5b9e\u8bc1\u7814\u7a76\uff0c\u5c24\u5176\u4e0d\u6e05\u695a\u5f00\u53d1\u8005\u5982\u4f55\u4f7f\u7528\u8fd9\u4e9b\u77e5\u8bc6\u3002", "method": "\u5728Stack Overflow\u548cnpm\u7b49\u77e5\u540d\u4f17\u5305\u5e73\u53f0\u4e0a\u8fdb\u884c\u5927\u89c4\u6a21\u5b9e\u8bc1\u7814\u7a76\uff0c\u5206\u6790\u4ee3\u7801\u91cd\u7528\u884c\u4e3a\u53ca\u5176\u5bf9\u9879\u76ee\u8d28\u91cf\u7684\u5f71\u54cd\uff0c\u5e76\u63a2\u7d22\u6301\u7eed\u96c6\u6210\uff08CI\uff09\u4f5c\u4e3a\u8d28\u91cf\u4fdd\u969c\u624b\u6bb5\u7684\u6548\u679c\u3002", "result": "\u91cd\u7528\u4f17\u5305\u77e5\u8bc6\u53ef\u8f85\u52a9\u5f00\u53d1\u5b9e\u8df5\uff0c\u4f46\u4f1a\u5bfc\u81f4\u4f9d\u8d56\u8fc7\u8f7d\u548c\u7ef4\u62a4\u8d1f\u62c5\uff1b\u5f15\u5165\u5e76\u4f18\u5316CI\u6709\u52a9\u4e8e\u63d0\u9ad8\u751f\u4ea7\u529b\u5e76\u8282\u7701\u8d44\u6e90\u3002", "conclusion": "\u5e94\u57fa\u4e8e\u6570\u636e\u9a71\u52a8\u51b3\u7b56\uff0c\u5728\u5229\u7528\u4f17\u5305\u77e5\u8bc6\u7684\u540c\u65f6\uff0c\u901a\u8fc7\u6539\u8fdbCI\u7b49\u65b9\u6cd5\u964d\u4f4e\u5176\u5bf9\u8f6f\u4ef6\u8d28\u91cf\u7684\u8d1f\u9762\u5f71\u54cd\u3002"}}
