<div id=toc></div>

# Table of Contents

- [cs.SE](#cs.SE) [Total: 8]
- [cs.AR](#cs.AR) [Total: 3]
- [cs.DC](#cs.DC) [Total: 8]
- [cs.NI](#cs.NI) [Total: 1]
- [cs.MA](#cs.MA) [Total: 4]


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [1] [Search-based Software Testing Driven by Domain Knowledge: Reflections and New Perspectives](https://arxiv.org/abs/2512.10079)
*Federico Formica,Mark Lawford,Claudio Menghi*

Main category: cs.SE

TL;DR: 本文回顾了基于搜索的软件测试（SBST）中结合领域知识的最新实验结果，提出从新视角重新审视相关技术并指明未来研究方向。


<details>
  <summary>Details</summary>
Motivation: SBST虽能高效生成测试用例，但缺乏工程师的领域知识，需探索如何有效融合以提升测试效果。

Method: 通过分析近期实验中的显著和意外结果，对现有SBST结合领域知识的技术进行反思。

Result: 揭示了当前方法中存在的问题与潜力，为后续研究提供新思路。

Conclusion: 应从新视角重新评估SBST与领域知识结合的技术，并推动更有效的融合方法研究。

Abstract: Search-based Software Testing (SBST) can automatically generate test cases to search for requirements violations. Unlike manual test case development, it can generate a substantial number of test cases in a limited time. However, SBST does not possess the domain knowledge of engineers. Several techniques have been proposed to integrate engineers' domain knowledge within existing SBST frameworks. This paper will reflect on recent experimental results by highlighting bold and unexpected results. It will help re-examine SBST techniques driven by domain knowledge from a new perspective, suggesting new directions for future research.

</details>


### [2] [ATLAS: Automated Toolkit for Large-Scale Verified Code Synthesis](https://arxiv.org/abs/2512.10173)
*Mantas Baksys,Stefan Zetzsche,Olivier Bouissou,Remi Delmas,Soonho Kong*

Main category: cs.SE

TL;DR: ATLAS通过自动生成大规模验证代码，显著提升大语言模型在程序验证任务中的表现。


<details>
  <summary>Details</summary>
Motivation: 解决训练大语言模型进行程序验证时缺乏验证代码数据的问题。

Method: 构建ATLAS自动化流水线，生成完整的Dafny程序并分解为多个子任务，从中提取大量训练样本用于微调Qwen 2.5 7B Coder模型。

Result: 在DafnyBench上提升23个百分点，在DafnySynthesis上提升50个百分点。

Conclusion: 合成的验证代码能有效增强大语言模型的程序验证能力。

Abstract: Large language models have shown potential for program verification, but progress is hindered by the scarcity of verified code for training. We present ATLAS, an automated pipeline that synthesizes verified programs at scale to address this data bottleneck. ATLAS generates complete Dafny programs with specifications, implementations, and proofs, producing 2.7K verified programs from which we extract over 19K training examples--more than 7 per verified program--by decomposing the synthesis process into multiple specialized tasks. Fine-tuning Qwen 2.5 7B Coder on this dataset produces substantial gains: +23 percentage points on DafnyBench and +50 percentage points on DafnySynthesis. These results demonstrate that synthetic verified code can effectively enhance LLM capabilities for formal verification.

</details>


### [3] [Does SWE-Bench-Verified Test Agent Ability or Model Memory?](https://arxiv.org/abs/2512.10218)
*Thanosan Prathifkumar,Noble Saji Mathews,Meiyappan Nagappan*

Main category: cs.SE

TL;DR: 本文通过实验发现SWE-Bench-Verified基准可能已被模型训练数据污染，导致评估结果失真，建议转向防污染的新数据集。


<details>
  <summary>Details</summary>
Motivation: 探究SWE-Bench-Verified基准是否因训练数据污染而无法真实反映模型解决实际软件问题的能力。

Method: 在仅提供issue文本或加上文件路径的最小上下文条件下，测试Claude模型在多个基准上的文件定位能力。

Result: 模型在SWE-Bench-Verified上表现显著优于其他基准，暗示其训练数据可能包含该基准任务。

Conclusion: 依赖旧有流行基准存在风险，应采用专为防污染设计的新数据集以准确评估模型能力。

Abstract: SWE-Bench-Verified, a dataset comprising 500 issues, serves as a de facto benchmark for evaluating various large language models (LLMs) on their ability to resolve GitHub issues. But this benchmark may overlap with model training data. If that is true, scores may reflect training recall, not issue-solving skill. To study this, we test two Claude models that frequently appear in top-performing agents submitted to the benchmark. We ask them to find relevant files using only issue text, and then issue text plus file paths. We then run the same setup on BeetleBox and SWE-rebench. Despite both benchmarks involving popular open-source Python projects, models performed 3 times better on SWE-Bench-Verified. They were also 6 times better at finding edited files, without any additional context about the projects themselves. This gap suggests the models may have seen many SWE-Bench-Verified tasks during training. As a result, scores on this benchmark may not reflect an agent's ability to handle real software issues, yet it continues to be used in ways that can misrepresent progress and lead to choices that favour agents that use certain models over strong agent design. Our setup tests the localization step with minimal context to the extent that the task should be logically impossible to solve. Our results show the risk of relying on older popular benchmarks and support the shift toward newer datasets built with contamination in mind.

</details>


### [4] [Studying and Automating Issue Resolution for Software Quality](https://arxiv.org/abs/2512.10238)
*Antu Saha*

Main category: cs.SE

TL;DR: 本研究通过LLM和AI技术提升软件问题报告质量、理解开发者工作流并自动化解决复杂问题，以推动AI驱动的问题解决。


<details>
  <summary>Details</summary>
Motivation: 开发者常面临低质量的问题报告、对实际工作流理解不足及缺乏自动化支持等挑战。

Method: 结合LLM推理、应用特定信息优化报告质量；实证分析传统与AI增强的工作流；利用ML/DL/LLM自动化定位与解决问题。

Result: 提供实证洞察、实用工具与自动化方法，提升软件可维护性与质量。

Conclusion: 该研究有效推进了AI在软件问题解决中的应用，助力构建更高质量的软件系统。

Abstract: Effective issue resolution is crucial for maintaining software quality. Yet developers frequently encounter challenges such as low-quality issue reports, limited understanding of real-world workflows, and a lack of automated support. This research aims to address these challenges through three complementary directions. First, we enhance issue report quality by proposing techniques that leverage LLM reasoning and application-specific information. Second, we empirically characterize developer workflows in both traditional and AI-augmented systems. Third, we automate cognitively demanding resolution tasks, including buggy UI localization and solution identification, through ML, DL, and LLM-based approaches. Together, our work delivers empirical insights, practical tools, and automated methods to advance AI-driven issue resolution, supporting more maintainable and high-quality software systems.

</details>


### [5] [UniCoR: Modality Collaboration for Robust Cross-Language Hybrid Code Retrieval](https://arxiv.org/abs/2512.10452)
*Yang Yang,Li Kuang,Jiakun Liu,Zhongxin Liu,Yingjie Xia,David Lo*

Main category: cs.SE

TL;DR: UniCoR是一种自监督框架，通过多视角对比学习和跨语言特征分布对齐，提升混合模式代码检索的语义理解、模态融合与跨语言泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有方法在混合查询（自然语言+代码片段）及跨语言场景下效果不佳，存在语义理解不足、模态融合低效、跨语言泛化弱三大问题。

Method: 提出UniCoR框架：1）多视角监督对比学习模块，从代码-代码、自然语言-代码、自然语言-自然语言三视角对齐表征；2）表示分布一致性学习模块，显式对齐不同编程语言的特征分布。

Result: 在基准数据集上，UniCoR相较最优基线平均提升MRR 8.64%、MAP 11.54%，并在混合检索与跨语言场景中表现稳定。

Conclusion: UniCoR有效解决了混合代码检索中的语义理解、模态融合与跨语言泛化问题，显著提升检索性能。

Abstract: Effective code retrieval is indispensable and it has become an important paradigm to search code in hybrid mode using both natural language and code snippets. Nevertheless, it remains unclear whether existing approaches can effectively leverage such hybrid queries, particularly in cross-language contexts. We conduct a comprehensive empirical study of representative code models and reveal three challenges: (1) insufficient semantic understanding; (2) inefficient fusion in hybrid code retrieval; and (3) weak generalization in cross-language scenarios. To address these challenges, we propose UniCoR, a novel self-supervised framework that learns Unified Code Representations framework designed to learn unified and robust code representations. Firstly, we design a multi-perspective supervised contrastive learning module to enhance semantic understanding and modality fusion. It aligns representations from multiple perspectives, including code-to-code, natural language-to-code, and natural language-to-natural language, enforcing the model to capture a semantic essence among modalities. Secondly, we introduce a representation distribution consistency learning module to improve cross-language generalization, which explicitly aligns the feature distributions of different programming languages, enabling language-agnostic representation learning. Extensive experiments on both empirical benchmark and large-scale benchmark show that UniCoR outperforms all baseline models, achieving an average improvement of 8.64% in MRR and 11.54% in MAP over the best-performing baseline. Furthermore, UniCoR exhibits stability in hybrid code retrieval and generalization capability in cross-language scenarios.

</details>


### [6] [Decoding Human-LLM Collaboration in Coding: An Empirical Study of Multi-Turn Conversations in the Wild](https://arxiv.org/abs/2512.10493)
*Binquan Zhang,Li Zhang,Haoyuan Zhang,Fang Liu,Song Wang,Bo Shen,An Fu,Lin Shi*

Main category: cs.SE

TL;DR: 本文通过分析LMSYS-Chat-1M和WildChat数据集，研究人类与大语言模型在编程协作中的交互模式、指令遵循能力及用户满意度。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏对编程场景中人机协作机制的系统性研究，本文旨在填补该空白并提升LLM交互体验。

Method: 基于真实对话数据集进行实证分析，识别交互模式、评估指令遵循表现、测量用户满意度。

Result: 发现任务类型影响交互结构；调试与重构任务指令遵循率低；查询类任务满意度更高。

Conclusion: 研究为优化LLM编程助手界面设计与提升协作效率提供实证依据，并指明自适应对话系统的未来方向。

Abstract: Large language models (LLMs) are increasingly acting as dynamic conversational interfaces, supporting multi-turn interactions that mimic human-like conversation and facilitate complex tasks like coding. While datasets such as LMSYS-Chat-1M and WildChat capture real-world user-LLM conversations, few studies systematically explore the mechanisms of human-LLM collaboration in coding scenarios. What tortuous paths do users experience during the interaction process? How well do the LLMs follow instructions? Are users satisfied? In this paper, we conduct an empirical analysis on human-LLM coding collaboration using LMSYS-Chat-1M and WildChat datasets to explore the human-LLM collaboration mechanism, LLMs' instruction following ability, and human satisfaction. This study yields interesting findings: 1) Task types shape interaction patterns(linear, star and tree), with code quality optimization favoring linear patterns, design-driven tasks leaning toward tree structures, and queries preferring star patterns; 2) Bug fixing and code refactoring pose greater challenges to LLMs' instruction following, with non-compliance rates notably higher than in information querying; 3) Code quality optimization and requirements-driven development tasks show lower user satisfaction, whereas structured knowledge queries and algorithm designs yield higher levels. These insights offer recommendations for improving LLM interfaces and user satisfaction in coding collaborations, while highlighting avenues for future research on adaptive dialogue systems. We believe this work broadens understanding of human-LLM synergies and supports more effective AI-assisted development.

</details>


### [7] [Analyzing developer discussions on EU and US privacy legislation compliance in GitHub repositories](https://arxiv.org/abs/2512.10618)
*Georgia M. Kapitsaki,Maria Papoutsoglou,Christoph Treude,Ioanna Theophilou*

Main category: cs.SE

TL;DR: 本文通过分析GitHub上的32,820个议题，研究开源软件开发者在遵守GDPR和CCPA等隐私法规时讨论的问题，归纳出24个讨论类别并归为六大类，帮助从业者、教育界和研究界更好地应对合规挑战。


<details>
  <summary>Details</summary>
Motivation: 隐私法规如GDPR和CCPA促使开发者调整系统实现，但目前缺乏对开源社区如何应对这些法规的实证研究。

Method: 通过自动与人工结合的方式分析GitHub上32,820个议题，识别涉及的法律权利与原则，并对1,186个样本进行手动分类。

Result: 归纳出六大类共24个讨论主题，开发者最关注用户删除权、退出权、访问权及同意管理、功能实现、缺陷修复与Cookie管理。

Conclusion: 构建的分类体系有助于从业者优先处理合规问题，教育界可据此优化课程，研究界可识别改进方向以加速合规进程。

Abstract: Context: Privacy legislation has impacted the way software systems are developed, prompting practitioners to update their implementations. Specifically, the EU General Data Protection Regulation (GDPR) and the California Consumer Privacy Act (CCPA) have forced the community to focus on users' data privacy. Despite the vast amount of data on developer issues available in GitHub repositories, there is a lack of empirical evidence on the issues developers of Open Source Software discuss to comply with privacy legislation. Method: In this work, we examine such discussions by mining and analyzing 32,820 issues from GitHub repositories. We partially analyzed the dataset automatically to identify law user rights and principles indicated, and manually analyzed a sample of 1,186 issues based on the type of concern addressed. Results: We devised 24 discussion categories placed in six clusters: features/bugs, consent-related, documentation, data storing/sharing, adaptability, and general compliance. Our results show that developers mainly focus on specific user rights from the legislation (right to erasure, right to opt-out, right to access), addressing other rights less frequently, while most discussions concern user consent, user rights functionality, bugs and cookies management. Conclusion: The created taxonomy can help practitioners understand which issues are discussed for law compliance, so that they ensure they address them first in their systems. In addition, the educational community can reshape curricula to better educate future engineers on the privacy law concerns raised, and the research community can identify gaps and areas for improvement to support and accelerate data privacy law compliance.

</details>


### [8] [Zorya: Automated Concolic Execution of Single-Threaded Go Binaries](https://arxiv.org/abs/2512.10799)
*Karolina Gorna,Nicolas Iooss,Yannick Seurin,Rida Khatoun*

Main category: cs.SE

TL;DR: 本文改进Zorya框架，通过多层过滤和路径分析提升Go二进制漏洞检测效率与覆盖率。


<details>
  <summary>Details</summary>
Motivation: 现有符号执行工具难以应对Go二进制的运行时复杂性与扩展性挑战。

Method: 在Zorya基础上增加未执行路径检测与多层过滤机制，聚焦panic相关路径，并采用函数模式分析加速执行。

Result: 实验显示panic可达性门控提速1.8-3.9倍，过滤33%-70%分支，Zorya能检测全部panic，优于现有工具；函数模式分析提速约百倍。

Conclusion: 针对含运行时安全检查的语言生态，专用混合执行可实现高效实用的漏洞检测。

Abstract: Go's adoption in critical infrastructure intensifies the need for systematic vulnerability detection, yet existing symbolic execution tools struggle with Go binaries due to runtime complexity and scalability challenges. In this work, we build upon Zorya, a concolic execution framework that translates Go binaries to Ghidra's P-Code intermediate representation to address these challenges. We added the detection of bugs in concretely not taken paths and a multi-layer filtering mechanism to concentrate symbolic reasoning on panic-relevant paths. Evaluation on five Go vulnerabilities demonstrates that panic-reachability gating achieves 1.8-3.9x speedups when filtering 33-70% of branches, and that Zorya detects all panics while existing tools detect at most two. Function-mode analysis proved essential for complex programs, running roughly two orders of magnitude faster than starting from main. This work establishes that specialized concolic execution can achieve practical vulnerability detection in language ecosystems with runtime safety checks.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [9] [Algorithm-Driven On-Chip Integration for High Density and Low Cost](https://arxiv.org/abs/2512.10089)
*Jeongeun Kim,Sabrina Yarzada,Paul Chen,Christopher Torng*

Main category: cs.AR

TL;DR: 提出一种新方法，通过结构化设计空间、窄区域架构和片上电源域，实现大规模芯片设计的高效集成与成本降低。


<details>
  <summary>Details</summary>
Motivation: 传统多项目晶圆服务在项目数量增加时扩展性不足，需系统化方法解决密集集成设计的布局、连接和验证问题。

Method: 1. 结构化设计空间以支持算法驱动的自动化布局；2. 利用站点间窄区域实现片外通信等共享功能；3. 提供无需低功耗设计专业知识的片上电源域方案。

Result: 相比现有物理聚合方法，面积减少高达13倍，显著提升大规模流片环境的可扩展性和成本效益。

Conclusion: 该方法为半导体人才培养和科研提供了一种高效、低成本的大规模芯片集成解决方案。

Abstract: Growing interest in semiconductor workforce development has generated demand for platforms capable of supporting large numbers of independent hardware designs for research and training without imposing high per-project overhead. Traditional multi-project wafer (MPW) services based solely on physical co-placement have historically met this need, yet their scalability breaks down as project counts rise. Recent efforts towards scalable chip tapeouts mitigate these limitations by integrating many small designs within a shared die and attempt to amortize costly resources such as IO pads and memory macros. However, foundational principles for arranging, linking, and validating such densely integrated design sites have received limited systematic investigation. This work presents a new approach with three key techniques to address this gap. First, we establish a structured formulation of the design space that enables automated, algorithm-driven packing of many projects, replacing manual layout practices. Second, we introduce an architecture that exploits only the narrow-area regions between sites to deliver on off-chip communication and other shared needs. Third, we provide a practical approach for on-chip power domains enabling per-project power characterization at a standard laboratory bench and requiring no expertise in low-power ASIC design. Experimental results show that our approach achieves substantial area reductions of up to 13x over state-of-the-art physical-only aggregation methods, offering a scalable and cost-effective path forward for large-scale tapeout environments.

</details>


### [10] [A Vertically Integrated Framework for Templatized Chip Design](https://arxiv.org/abs/2512.10155)
*Jeongeun Kim,Christopher Torng*

Main category: cs.AR

TL;DR: 本文提出一种从高级面向对象软件规范生成芯片的方法，旨在降低软件开发者参与芯片设计的门槛。


<details>
  <summary>Details</summary>
Motivation: 软件开发者难以将定制硬件融入应用，尽管专用硅片对机器学习和AI等领域有显著优势。

Method: 每个软件对象映射为芯片上的一个区域，采用模块化构建策略和基于序列的形式类型系统确保通信模式一致。

Result: 实现了从软件到芯片设计的心理连续性，支持实际布局生成并降低入门门槛。

Conclusion: 该方法使软件开发者能更轻松参与芯片设计，推动软硬件协同创新。

Abstract: Developers who primarily engage with software often struggle to incorporate custom hardware into their applications, even though specialized silicon can provide substantial benefits to machine learning and AI, as well as to the application domains that they enable. This work investigates how a chip can be generated from a high-level object-oriented software specification, targeting introductory-level chip design learners with only very light performance requirements, while maintaining mental continuity between the chip layout and the software source program. In our approach, each software object is represented as a corresponding region on the die, producing a one-to-one structural mapping that preserves these familiar abstractions throughout the design flow. To support this mapping, we employ a modular construction strategy in which vertically composed IP blocks implement the behavioral protocols expressed in software. A direct syntactic translation, however, cannot meet hardware-level efficiency or communication constraints. For this reason, we leverage formal type systems based on sequences that check whether interactions between hardware modules adhere to the communication patterns described in the software model. We further examine hardware interconnect strategies for composing many such modules and develop layout techniques suited to this object-aligned design style. Together, these contributions preserve mental continuity from software to chip design for new learners and enables practical layout generation, ultimately reducing the expertise required for software developers to participate in chip creation.

</details>


### [11] [SemanticBBV: A Semantic Signature for Cross-Program Knowledge Reuse in Microarchitecture Simulation](https://arxiv.org/abs/2512.10231)
*Zhenguo Liu,Chengao Shi,Chen Ding,Jiang Xu*

Main category: cs.AR

TL;DR: SemanticBBV 是一种新型两阶段框架，通过语义编码与集合变换生成性能敏感的程序签名，实现跨程序模拟复用与高达7143倍加速。


<details>
  <summary>Details</summary>
Motivation: 传统 BBV 存在顺序依赖与语义缺失问题，阻碍了跨程序知识复用与性能预测能力。

Method: 采用 RWKV 编码器生成基本块嵌入，再经集变换器聚合并联合训练 CPI 回归与三元组损失，构建顺序无关、性能感知的签名。

Result: 在仅模拟14个通用点下，对10个 SPEC 基准测试平均准确率达86.3%，速度提升7143倍，且适配新微架构只需微调。

Conclusion: SemanticBBV 突破传统采样方法局限，为跨程序微架构模拟提供高效、可迁移的新范式。

Abstract: For decades, sampling-based techniques have been the de facto standard for accelerating microarchitecture simulation, with the Basic Block Vector (BBV) serving as the cornerstone program representation. Yet, the BBV's fundamental limitations: order-dependent IDs that prevent cross-program knowledge reuse and a lack of semantic content predictive of hardware performance have left a massive potential for optimization untapped.
  To address these gaps, we introduce SemanticBBV, a novel, two-stage framework that generates robust, performance-aware signatures for cross-program simulation reuse. First, a lightweight RWKV-based semantic encoder transforms assembly basic blocks into rich Basic Block Embeddings (BBEs), capturing deep functional semantics. Second, an order-invariant Set Transformer aggregates these BBEs, weighted by execution frequency, into a final signature. Crucially, this stage is co-trained with a dual objective: a triplet loss for signature distinctiveness and a Cycles Per Instruction (CPI) regression task, directly imbuing the signature with performance sensitivity. Our evaluation demonstrates that SemanticBBV not only matches traditional BBVs in single-program accuracy but also enables unprecedented cross-program analysis. By simulating just 14 universal program points, we estimated the performance of ten SPEC CPU benchmarks with 86.3% average accuracy, achieving a 7143x simulation speedup. Furthermore, the signature shows strong adaptability to new microarchitectures with minimal fine-tuning.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [12] [A study of the spectrum resource leasing method based on ERC4907 extension](https://arxiv.org/abs/2512.09942)
*Zhiming Liang,Bin Chen,Litao Ye,Chen Sun,Shuo Wang,Zhe Peng*

Main category: cs.DC

TL;DR: M-ERC4907扩展了ERC4907标准，支持多用户、多时段批量授权，显著降低链上交易与Gas消耗。


<details>
  <summary>Details</summary>
Motivation: 解决ERC4907仅支持单用户单时段授权导致的效率低下问题，适应去中心化多时段调度场景。

Method: 提出M-ERC4907方法，新增批量配置多时段与多用户同时授权功能，突破原有序列化授权限制。

Result: 在Remix平台实验表明，新方法有效减少链上交易次数与Gas总消耗，提升系统可扩展性与资源分配效率。

Conclusion: M-ERC4907显著优化了NFT租赁机制，为复杂调度场景提供高效、灵活的链上解决方案。

Abstract: The ERC4907 standard enables rentable Non-Fungible Tokens (NFTs) but is limited to single-user, single-time-slot authorization, which severely limits its applicability and efficiency in decentralized multi-slot scheduling scenarios. To address this limitation, this paper proposes Multi-slot ERC4907 (M-ERC4907) extension method. The M-ERC4907 method introduces novel functionalities to support the batch configuration of multiple time slots and simultaneous authorization of multiple users, thereby effectively eliminating the rigid sequential authorization constraint of ERC4907. The experiment was conducted on the Remix development platform. Experimental results show that the M-ERC4907 method significantly reduces on-chain transactions and overall Gas consumption, leading to enhanced scalability and resource allocation efficiency.

</details>


### [13] [ELANA: A Simple Energy and Latency Analyzer for LLMs](https://arxiv.org/abs/2512.09946)
*Hung-Yueh Chiang,Bokun Wang,Diana Marculescu*

Main category: cs.DC

TL;DR: ELANA是一个开源的轻量级分析工具，用于评估大语言模型在多GPU和边缘GPU平台上的延迟、缓存及能耗等性能指标。


<details>
  <summary>Details</summary>
Motivation: 为优化大语言模型在不同硬件平台上的部署效率及下一代模型开发提供基准测试支持。

Method: 设计并实现兼容Hugging Face API的命令行工具，支持模型大小、KV缓存、预填充与生成延迟等关键指标测量，并可选记录能耗。

Result: 工具支持所有Hugging Face公开模型，易于定制，适用于高效LLM研究与小规模验证实验。

Conclusion: ELANA为学术界提供了便捷、灵活的大语言模型性能分析方案，助力高效模型研发与部署。

Abstract: The latency and power consumption of large language models (LLMs) are major constraints when serving them across a wide spectrum of hardware platforms, from mobile edge devices to cloud GPU clusters. Benchmarking is crucial for optimizing efficiency in both model deployment and next-generation model development. To address this need, we open-source a simple profiling tool, \textbf{ELANA}, for evaluating LLMs. ELANA is designed as a lightweight, academic-friendly profiler for analyzing model size, key-value (KV) cache size, prefilling latency (Time-to-first-token, TTFT), generation latency (Time-per-output-token, TPOT), and end-to-end latency (Time-to-last-token, TTLT) of LLMs on both multi-GPU and edge GPU platforms. It supports all publicly available models on Hugging Face and offers a simple command-line interface, along with optional energy consumption logging. Moreover, ELANA is fully compatible with popular Hugging Face APIs and can be easily customized or adapted to compressed or low bit-width models, making it ideal for research on efficient LLMs or for small-scale proof-of-concept studies. We release the ELANA profiling tool at: https://github.com/enyac-group/Elana.

</details>


### [14] [GoodSpeed: Optimizing Fair Goodput with Adaptive Speculative Decoding in Distributed Edge Inference](https://arxiv.org/abs/2512.09963)
*Phuong Tran,Tzu-Hao Liu,Long Tan Le,Tung-Anh Nguyen,Van Quan La,Eason Yu,Han Shu,Choong Seon Hong,Nguyen H. Tran*

Main category: cs.DC

TL;DR: GOODSPEED 是一种新型分布式推理框架，通过自适应推测解码优化大语言模型推理的吞吐效率与公平性。


<details>
  <summary>Details</summary>
Motivation: 解决多用户环境下大语言模型推理时高计算需求与资源受限问题，同时兼顾吞吐效率与多服务器间的公平性。

Method: 采用中心验证服务器协调多个异构草稿服务器，结合梯度调度算法动态分配任务，最大化对数效用函数以实现比例公平。

Result: 经流体路径分析证明，GOODSPEED 在稳态下收敛至最优吞吐分配，动态负载下误差有界且性能接近最优。

Conclusion: GOODSPEED 为分布式大语言模型推理提供了一种可扩展、公平且高效的解决方案。

Abstract: Large language models (LLMs) have revolutionized natural language processing, yet their high computational demands pose significant challenges for real-time inference, especially in multi-user server speculative decoding and resource-constrained environments. Speculative decoding has emerged as a promising technique to accelerate LLM inference by using lightweight draft models to generate candidate tokens, which are subsequently verified by a larger, more accurate model. However, ensuring both high goodput (the effective rate of accepted tokens) and fairness across multiple draft servers cooperating with a central verification server remains an open challenge. This paper introduces GOODSPEED, a novel distributed inference framework that optimizes goodput through adaptive speculative decoding. GOODSPEED employs a central verification server that coordinates a set of heterogeneous draft servers, each running a small language model to generate speculative tokens. To manage resource allocation effectively, GOODSPEED incorporates a gradient scheduling algorithm that dynamically assigns token verification tasks, maximizing a logarithmic utility function to ensure proportional fairness across servers. By processing speculative outputs from all draft servers in parallel, the framework enables efficient collaboration between the verification server and distributed draft generators, streamlining both latency and throughput. Through rigorous fluid sample path analysis, we show that GOODSPEED converges to the optimal goodput allocation in steady-state conditions and maintains near-optimal performance with provably bounded error under dynamic workloads. These results demonstrate that GOODSPEED provides a scalable, fair and efficient solution for multi- in distributed LLM inference systems.

</details>


### [15] [Hybrid Learning and Optimization-Based Dynamic Scheduling for DL Workloads on Heterogeneous GPU Clusters](https://arxiv.org/abs/2512.10271)
*Shruti Dongare,Redwan Ibne Seraj Khan,Hadeel Albahar,Nannan Zhao,Diego Melendez Maita,Ali R. Butt*

Main category: cs.DC

TL;DR: RLTune是一个基于强化学习的调度框架，用于在异构GPU集群上动态分配深度学习任务，无需依赖任务特定配置即可提升资源利用率和降低延迟。


<details>
  <summary>Details</summary>
Motivation: 现有调度器难以应对日益异构的GPU集群和缺乏应用特征可见性的问题，限制了云平台高效运行大规模深度学习任务的能力。

Method: 结合强化学习进行任务优先级排序，并使用混合整数线性规划（MILP）实现任务到节点的映射，以优化系统整体性能指标。

Result: 在微软Philly、Helios及阿里生产数据集上验证，RLTune最高可提升20% GPU利用率、降低81%排队延迟、缩短70%任务完成时间。

Conclusion: RLTune具备跨工作负载泛化能力，无需逐任务分析，适合云服务商规模化部署，实现更高效、公平和可持续的深度学习任务管理。

Abstract: Modern cloud platforms increasingly host large-scale deep learning (DL) workloads, demanding high-throughput, low-latency GPU scheduling. However, the growing heterogeneity of GPU clusters and limited visibility into application characteristics pose major challenges for existing schedulers, which often rely on offline profiling or application-specific assumptions. We present RLTune, an application-agnostic reinforcement learning (RL)-based scheduling framework that dynamically prioritizes and allocates DL jobs on heterogeneous GPU clusters. RLTune integrates RL-driven prioritization with MILP-based job-to-node mapping to optimize system-wide objectives such as job completion time (JCT), queueing delay, and resource utilization. Trained on large-scale production traces from Microsoft Philly, Helios, and Alibaba, RLTune improves GPU utilization by up to 20%, reduces queueing delay by up to 81%, and shortens JCT by as much as 70 percent. Unlike prior approaches, RLTune generalizes across diverse workloads without requiring per-job profiling, making it practical for cloud providers to deploy at scale for more efficient, fair, and sustainable DL workload management.

</details>


### [16] [High-Dimensional Data Processing: Benchmarking Machine Learning and Deep Learning Architectures in Local and Distributed Environments](https://arxiv.org/abs/2512.10312)
*Julian Rodriguez,Piotr Lopez,Emiliano Lerma,Rafael Medrano,Jacobo Hernandez*

Main category: cs.DC

TL;DR: 本文档总结了大数据课程中实施的实践与方法，涵盖数据处理、文本分析、电影特征分析及分布式计算集群搭建。


<details>
  <summary>Details</summary>
Motivation: 为系统记录和展示大数据课程中的学习与实践过程。

Method: 采用Epsilon数据集进行分组与个人策略处理，结合RestMex进行文本分类，利用IMDb分析电影特征，并基于Linux与Scala搭建Apache Spark分布式集群。

Result: 成功完成从数据处理到分布式系统实现的全流程技术实践。

Conclusion: 该课程实践有效提升了对大数据技术栈的理解与应用能力。

Abstract: This document reports the sequence of practices and methodologies implemented during the Big Data course. It details the workflow beginning with the processing of the Epsilon dataset through group and individual strategies, followed by text analysis and classification with RestMex and movie feature analysis with IMDb. Finally, it describes the technical implementation of a distributed computing cluster with Apache Spark on Linux using Scala.

</details>


### [17] [Making Wide Stripes Practical: Cascaded Parity LRCs for Efficient Repair and High Reliability](https://arxiv.org/abs/2512.10425)
*Fan Yu,Guodong Li,Si Wu,Weijun Fang,Sihuang Hu*

Main category: cs.DC

TL;DR: CP-LRCs通过在局部和全局校验块间建立级联依赖，显著降低宽条带纠删码的修复开销。


<details>
  <summary>Details</summary>
Motivation: 现有LRC在宽条带场景下存在修复成本高、可靠性下降等问题，源于局部与全局校验块设计独立、缺乏协作。

Method: 提出CP-LRCs，将全局校验块分解嵌入所有局部校验块，构建级联校验组，并提供通用系数生成框架与修复算法。

Result: 在阿里云实测中，单节点修复时间最多减少41%，双节点修复时间减少26%。

Conclusion: CP-LRCs在保持MDS容错能力的同时，实现了低带宽的单节点与多节点高效修复。

Abstract: Erasure coding with wide stripes is increasingly adopted to reduce storage overhead in large-scale storage systems. However, existing Locally Repairable Codes (LRCs) exhibit structural limitations in this setting: inflated local groups increase single-node repair cost, multi-node failures frequently trigger expensive global repair, and reliability degrades sharply. We identify a key root cause: local and global parity blocks are designed independently, preventing them from cooperating during repair. We present Cascaded Parity LRCs (CP-LRCs), a new family of wide stripe LRCs that embed structured dependency between parity blocks by decomposing a global parity block across all local parity blocks. This creates a cascaded parity group that preserves MDS-level fault tolerance while enabling low-bandwidth single-node and multi-node repairs. We provide a general coefficient-generation framework, develop repair algorithms exploiting cascading, and instantiate the design with CP-Azure and CP-Uniform. Evaluations on Alibaba Cloud show reductions in repair time of up to 41% for single-node failures and 26% for two-node failures.

</details>


### [18] [Clustered Federated Learning with Hierarchical Knowledge Distillation](https://arxiv.org/abs/2512.10443)
*Sabtain Ahmad,Meerzhan Kanatbekova,Ivona Brandic,Atakan Aral*

Main category: cs.DC

TL;DR: CFLHKD通过分层聚类联邦学习与多教师知识蒸馏，提升模型准确率3.32-7.57%。


<details>
  <summary>Details</summary>
Motivation: 传统CFL方法因独立训练各簇模型而忽略跨簇知识共享，导致学习碎片化。

Method: 提出CFLHKD方案，结合分层聚合与多教师知识蒸馏实现跨簇知识共享并保留个性化。

Result: 在标准数据集上，CFLHKD显著优于基线方法，在簇特异性和全局模型准确率上提升3.32-7.57%。

Conclusion: CFLHKD有效平衡了个性化与全局知识共享，提升了分层联邦学习的效率与性能。

Abstract: Clustered Federated Learning (CFL) has emerged as a powerful approach for addressing data heterogeneity and ensuring privacy in large distributed IoT environments. By clustering clients and training cluster-specific models, CFL enables personalized models tailored to groups of heterogeneous clients. However, conventional CFL approaches suffer from fragmented learning for training independent global models for each cluster and fail to take advantage of collective cluster insights. This paper advocates a shift to hierarchical CFL, allowing bi-level aggregation to train cluster-specific models at the edge and a unified global model at the cloud. This shift improves training efficiency yet might introduce communication challenges. To this end, we propose CFLHKD, a novel personalization scheme for integrating hierarchical cluster knowledge into CFL. Built upon multi-teacher knowledge distillation, CFLHKD enables inter-cluster knowledge sharing while preserving cluster-specific personalization. CFLHKD adopts a bi-level aggregation to bridge the gap between local and global learning. Extensive evaluations of standard benchmark datasets demonstrate that CFLHKD outperforms representative baselines in cluster-specific and global model accuracy and achieves a performance improvement of 3.32-7.57\%.

</details>


### [19] [ESS: An Offload-Centric Latent-Cache Management Architecture for DeepSeek-V3.2-Exp](https://arxiv.org/abs/2512.10576)
*Xinhang Chen,Chao Zhang,Jiahuan He,Wei Liu,Jianming Zhang,Wenlong Zhou,Xiao Li,Pai Zeng,Shiyong Li,Yuanpan Qian,Dong Li,Zhaogeng Li*

Main category: cs.DC

TL;DR: ESS通过将Latent-Cache卸载至CPU内存，缓解GPU显存瓶颈，显著提升长上下文推理吞吐量。


<details>
  <summary>Details</summary>
Motivation: 解决DeepSeek-V3.2-Exp在长序列推理中因Latent-Cache线性增长导致的GPU显存瓶颈问题。

Method: 提出ESS系统，选择性将Latent-Cache卸载到CPU内存，保留关键组件于GPU，实现批处理规模与显存解耦。

Result: 仿真显示，在32K和128K上下文长度下，吞吐量分别提升69.4%和123%。

Conclusion: ESS是面向长上下文LLM服务的一种实用且可扩展的解决方案，能有效降低部署成本。

Abstract: DeepSeek-V3.2-Exp introduces a sparse attention mechanism that significantly reduces inference latency in long-context scenarios. Although the overall throughput has improved greatly, the Decode-stage of PD disaggregation remains to be a major bottleneck. This bottleneck primarily stems from the conflict between linear growth of Latent-Cache with sequence length and the limited GPU memory capacity, which constrains the feasible batch-size and thereby suppresses Decode-stage throughput.
  To address this challenge, we propose ESS (Extended Sparse Server), an offload-centric system design tailored for DeepSeek-V3.2-Exp. ESS selectively offloads Latent-Cache to CPU memory while preserving latency-critical components on GPU. By freeing up GPU memory, ESS effectively decoupling batch-size scaling from GPU memory constraints. This design significantly improves Decode-stage throughput, thereby reducing deployment costs in real-world settings.
  Our high-fidelity simulations show that ESS delivers 69.4\% throughput improvement at 32K context length and up to 123\% throughput improvement at 128K, demonstrating its effectiveness for large-context inference workloads. These results highlight ESS as a practical and scalable solution for long-context LLM serving.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [20] [Natural Language Interface for Firewall Configuration](https://arxiv.org/abs/2512.10789)
*F. Taghiyev,A. Aslanbayli*

Main category: cs.NI

TL;DR: 本文提出了一种用于企业防火墙配置的自然语言接口原型，通过大语言模型辅助解析用户意图并转换为设备配置，结合多层验证确保安全与合规。


<details>
  <summary>Details</summary>
Motivation: 简化防火墙策略管理，降低人为错误风险，提升配置的可审计性与人性化体验。

Method: 构建中间表示架构，利用大语言模型生成结构化策略对象，再编译为厂商命令行配置，并集成静态检查、安全门控与仿真验证三层保障机制。

Result: 原型在合成数据集上验证有效，支持Palo Alto平台并具备跨平台扩展能力，初步实现安全、可审计的人机协同配置流程。

Conclusion: 该方法为防火墙策略管理提供了一种可扩展、以人为中心且具备强验证保障的新范式。

Abstract: This paper presents the design and prototype implementation of a natural language interface for configuring enterprise firewalls. The framework allows administrators to express access control policies in plain language, which are then translated into vendor specific configurations. A compact schema bound intermediate representation separates human intent from device syntax and in the current prototype compiles to Palo Alto PAN OS command line configuration while remaining extensible to other platforms. Large language models are used only as assistive parsers that generate typed intermediate representation objects, while compilation and enforcement remain deterministic. The prototype integrates three validation layers, namely a static linter that checks structural and vendor specific constraints, a safety gate that blocks overly permissive rules such as any to any allows, and a Batfish based simulator that validates configuration syntax and referential integrity against a synthetic device model. The paper describes the architecture, implementation, and test methodology on synthetic network context datasets and discusses how this approach can evolve into a scalable auditable and human centered workflow for firewall policy management.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [21] [Norm-Governed Multi-Agent Decision-Making in Simulator-Coupled Environments:The Reinsurance Constrained Multi-Agent Simulation Process (R-CMASP)](https://arxiv.org/abs/2512.09939)
*Stella C. Dong*

Main category: cs.MA

TL;DR: 提出R-CMASP模型，以多智能体系统解决再保险决策中的复杂约束与协作问题。


<details>
  <summary>Details</summary>
Motivation: 再保险决策涉及分布式信息、部分可观测性及监管约束，传统自动化方法难以应对。

Method: 构建结合模拟器动态、角色化智能体与规范约束的多智能体框架，并采用LLM智能体进行实验。

Result: 相比确定性自动化或单体LLM，该方法显著提升稳定性、资本效率与条款解释准确性。

Conclusion: 受监管的模拟驱动决策环境最适合建模为规范约束下的多智能体系统。

Abstract: Reinsurance decision-making exhibits the core structural properties that motivate multi-agent models: distributed and asymmetric information, partial observability, heterogeneous epistemic responsibilities, simulator-driven environment dynamics, and binding prudential and regulatory constraints. Deterministic workflow automation cannot meet these requirements, as it lacks the epistemic flexibility, cooperative coordination mechanisms, and norm-sensitive behaviour required for institutional risk-transfer.
  We propose the Reinsurance Constrained Multi-Agent Simulation Process (R-CMASP), a formal model that extends stochastic games and Dec-POMDPs by adding three missing elements: (i) simulator-coupled transition dynamics grounded in catastrophe, capital, and portfolio engines; (ii) role-specialized agents with structured observability, belief updates, and typed communication; and (iii) a normative feasibility layer encoding solvency, regulatory, and organizational rules as admissibility constraints on joint actions.
  Using LLM-based agents with tool access and typed message protocols, we show in a domain-calibrated synthetic environment that governed multi-agent coordination yields more stable, coherent, and norm-adherent behaviour than deterministic automation or monolithic LLM baselines--reducing pricing variance, improving capital efficiency, and increasing clause-interpretation accuracy. Embedding prudential norms as admissibility constraints and structuring communication into typed acts measurably enhances equilibrium stability.
  Overall, the results suggest that regulated, simulator-driven decision environments are most naturally modelled as norm-governed, simulator-coupled multi-agent systems.

</details>


### [22] [Empirical Hardness in Multi-Agent Pathfinding: Research Challenges and Opportunities](https://arxiv.org/abs/2512.10078)
*Jingyao Ren,Eric Ewing,T. K. Satish Kumar,Sven Koenig,Nora Ayanian*

Main category: cs.MA

TL;DR: 本文探讨多智能体路径规划（MAPF）的实际求解难度，提出算法选择、影响因素分析与困难实例生成三大研究挑战。


<details>
  <summary>Details</summary>
Motivation: MAPF虽属NP难问题，但实际求解难度差异大，需深入理解其经验硬度现象。

Method: 通过梳理现有研究，归纳出三大关键研究方向以系统化推进MAPF经验硬度分析。

Result: 明确算法选择、结构特征识别与基准数据集构建为未来核心研究议题。

Conclusion: 本研究奠定MAPF经验硬度系统研究基础，呼吁学界深入探索此尚待开发领域。

Abstract: Multi-agent pathfinding (MAPF) is the problem of finding collision-free paths for a team of agents on a map. Although MAPF is NP-hard, the hardness of solving individual instances varies significantly, revealing a gap between theoretical complexity and actual hardness. This paper outlines three key research challenges in MAPF empirical hardness to understand such phenomena. The first challenge, known as algorithm selection, is determining the best-performing algorithms for a given instance. The second challenge is understanding the key instance features that affect MAPF empirical hardness, such as structural properties like phase transition and backbone/backdoor. The third challenge is how to leverage our knowledge of MAPF empirical hardness to effectively generate hard MAPF instances or diverse benchmark datasets. This work establishes a foundation for future empirical hardness research and encourages deeper investigation into these promising and underexplored areas.

</details>


### [23] [Emergent Collective Memory in Decentralized Multi-Agent AI Systems](https://arxiv.org/abs/2512.10166)
*Khushiyant*

Main category: cs.MA

TL;DR: 该论文研究了去中心化多智能体系统中集体记忆的形成机制，通过个体记忆与环境痕迹交互实现高效协作。


<details>
  <summary>Details</summary>
Motivation: 探索在无中心控制下，多智能体如何通过个体记忆与环境痕迹协同构建集体记忆以提升任务表现。

Method: 设计实验模拟不同规模网格与智能体密度，对比仅用个体记忆、仅用环境痕迹及两者结合的效果，并验证理论相变预测。

Result: 个体记忆可提升68.7%性能，环境痕迹需依赖记忆解读；在高密度下（rho>0.20），痕迹协调优于记忆36-41%，临界密度预测误差仅13%。

Conclusion: 集体记忆源于个体认知与环境交互，环境痕迹在足够密度下能主导协作，但必须依托个体记忆基础才能生效。

Abstract: We demonstrate how collective memory emerges in decentralized multi-agent systems through the interplay between individual agent memory and environmental trace communication. Our agents maintain internal memory states while depositing persistent environmental traces, creating a spatially distributed collective memory without centralized control. Comprehensive validation across five environmental conditions (20x20 to 50x50 grids, 5-20 agents, 50 runs per configuration) reveals a critical asymmetry: individual memory alone provides 68.7% performance improvement over no-memory baselines (1563.87 vs 927.23, p < 0.001), while environmental traces without memory fail completely. This demonstrates that memory functions independently but traces require cognitive infrastructure for interpretation. Systematic density-sweep experiments (rho in [0.049, 0.300], up to 625 agents) validate our theoretical phase transition prediction. On realistic large grids (30x30, 50x50), stigmergic coordination dominates above rho ~ 0.20, with traces outperforming memory by 36-41% on composite metrics despite lower food efficiency. The experimental crossover confirms the predicted critical density rho_c = 0.230 within 13% error.

</details>


### [24] [Thinking While Driving: A Concurrent Framework for Real-Time, LLM-Based Adaptive Routing](https://arxiv.org/abs/2512.10610)
*Xiaopei Tan,Muyang Fan*

Main category: cs.MA

TL;DR: 提出了一种名为“边驾驶边思考”的并发路由框架，将大语言模型嵌入图结构交通环境，实现实时动态路径规划。


<details>
  <summary>Details</summary>
Motivation: 解决传统方法需停车决策导致效率低的问题，提升高流量场景下智能体的实时响应能力。

Method: 采用非阻塞异步架构（Unity协程+专用请求管理器），结合带权重无向图与实时拥堵数据，支持移动中持续路径规划。

Result: 高流量下平均决策延迟仅0.75秒，智能体可动态避堵并展现超越静态寻路的行为，同时保持实时性能。

Conclusion: 提供了一个可复现的自适应路由与多智能体协作研究框架。

Abstract: We present Thinking While Driving, a concurrent routing framework that integrates LLMs into a graph-based traffic environment. Unlike approaches that require agents to stop and deliberate, our system enables LLM-based route planning while agents are moving, significantly reducing intersection wait times. Under high traffic, agents average just 0.75 seconds of decision latency. To coordinate many agents in real-time, we implement a non-blocking asynchronous architecture using Unity coroutines and a dedicated request manager. The environment is a weighted undirected graph with live congestion metrics, updated continuously by the agents to enable shared perception. Our results show LLM-driven agents can dynamically adapt to traffic, reroute around congestion, and exhibit behaviors beyond static pathfinding, all while maintaining real-time performance. This work provides a reproducible framework for future research in adaptive routing and multi-agent cooperation.

</details>
