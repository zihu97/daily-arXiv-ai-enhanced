{"id": "2510.19972", "categories": ["cs.DC", "cs.CC"], "pdf": "https://arxiv.org/pdf/2510.19972", "abs": "https://arxiv.org/abs/2510.19972", "authors": ["Alkida Balliu", "Filippo Casagrande", "Francesco d'Amore", "Dennis Olivetti"], "title": "New Hardness Results for the LOCAL Model via a Simple Self-Reduction", "comment": "21 pages, no figures", "summary": "Very recently, Khoury and Schild [FOCS 2025] showed that any randomized LOCAL\nalgorithm that solves maximal matching requires $\\Omega(\\min\\{\\log \\Delta,\n\\log_\\Delta n\\})$ rounds, where $n$ is the number of nodes in the graph and\n$\\Delta$ is the maximum degree. This result is shown through a new technique,\ncalled round elimination via self-reduction. The lower bound proof is beautiful\nand presents very nice ideas. However, it spans more than 25 pages of technical\ndetails, and hence it is hard to digest and generalize to other problems.\nHistorically, the simplification of proofs and techniques has marked an\nimportant turning point in our understanding of the complexity of graph\nproblems. Our paper makes a step forward towards this direction, and provides\nthe following contributions.\n  1. We present a short and simplified version of the round elimination via\nself-reduction technique. The simplification of this technique enables us to\nobtain the following two hardness results.\n  2. We show that any randomized LOCAL algorithm that solves the maximal\n$b$-matching problem requires $\\Omega(\\min\\{\\log_{1+b}\\Delta, \\log_\\Delta n\\})$\nand $\\Omega(\\sqrt{\\log_{1+b} n})$ rounds. We recall that the $b$-matching\nproblem is a generalization of the matching problem where each vertex can have\nup to $b$ incident edges in the matching. As a corollary, for $b=1$, we obtain\na short proof for the maximal matching lower bound shown by Khoury and Schild.\n  3. Finally, we show that any randomized LOCAL algorithm that properly colors\nthe edges of a graph with $\\Delta + k$ colors requires $\\Omega(\\min\\{\\log\n\\Delta, \\log_\\Delta n\\})$ and $\\Omega(\\sqrt{\\log n})$ rounds, for any $k\\le\n\\Delta^{1-\\varepsilon}$ and any constant $\\varepsilon > 0$."}
{"id": "2510.20111", "categories": ["cs.DC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.20111", "abs": "https://arxiv.org/abs/2510.20111", "authors": ["Huawei Bai", "Yifan Huang", "Wenqi Shi", "Ansheng You", "Feifan Shao", "Tengfei Han", "Minghui Yu"], "title": "AsyncHZP: Hierarchical ZeRO Parallelism with Asynchronous Scheduling for Scalable LLM Training", "comment": "14 pages, 5 figures, tech report", "summary": "The training efficiency and scalability of language models on massive\nclusters currently remain a critical bottleneck. Mainstream approaches like ND\nparallelism are often cumbersome and complex, while flexible alternatives such\nas the Zero Redundancy Optimizer (ZeRO) are frequently hampered by\ncommunication overhead. In this paper, we propose Asynchronous Hierarchical\nZero Parallelism (AsyncHZP), a novel asynchronous variant of ZeRO designed to\nachieve superior performance while maintaining simplicity and memory\nefficiency. Unlike traditional ZeRO, which employs over-fine-grained sharding\nthat can lead to inefficient communication, AsyncHZP adaptively reshards\nparameters, gradients, and optimizer states across different replica groups.\nThis strategy optimizes device memory utilization and significantly reduces\ncommunication overhead. In addition, we also design a multi-stream asynchronous\nscheduling method that executes parameter all-gather and gradient\nreduce-scatter operations in dedicated background threads, effectively\noverlapping communication with computation while incurring negligible memory\nfragmentation. Empirical evaluations on both Dense and Mixture-of-Experts (MoE)\nmodels confirm that AsyncHZP maintains robust stability at scale. It\nconsistently outperforms classic ND parallelism, achieving state-of-the-art\nperformance without complex strategic tuning, thereby simplifying the path to\nefficient large-scale training."}
{"id": "2510.20128", "categories": ["cs.DC", "quant-ph", "D.2.6"], "pdf": "https://arxiv.org/pdf/2510.20128", "abs": "https://arxiv.org/abs/2510.20128", "authors": ["Xin Zhan", "K. Grace Johnson", "Aniello Esposito", "Barbara Chapman", "Marco Fiorentino", "Kirk M. Bresniker", "Raymond G. Beausoleil", "Masoud Mohseni"], "title": "A Full Stack Framework for High Performance Quantum-Classical Computing", "comment": "9 pages, 8 figures, presented at Cray User Group Meeting 2025, May\n  04-09, 2025, New York, NY", "summary": "To address the growing needs for scalable High Performance Computing (HPC)\nand Quantum Computing (QC) integration, we present our HPC-QC full stack\nframework and its hybrid workload development capability with modular\nhardware/device-agnostic software integration approach. The latest development\nin extensible interfaces for quantum programming, dispatching, and compilation\nwithin existing mature HPC programming environment are demonstrated. Our HPC-QC\nfull stack enables high-level, portable invocation of quantum kernels from\ncommercial quantum SDKs within HPC meta-program in compiled languages (C/C++\nand Fortran) as well as Python through a quantum programming interface library\nextension. An adaptive circuit knitting hypervisor is being developed to\npartition large quantum circuits into sub-circuits that fit on smaller noisy\nquantum devices and classical simulators. At the lower-level, we leverage Cray\nLLVM-based compilation framework to transform and consume LLVM IR and Quantum\nIR (QIR) from commercial quantum software frontends in a retargetable fashion\nto different hardware architectures. Several hybrid HPC-QC multi-node multi-CPU\nand GPU workloads (including solving linear system of equations, quantum\noptimization, and simulating quantum phase transitions) have been demonstrated\non HPE EX supercomputers to illustrate functionality and execution viability\nfor all three components developed so far. This work provides the framework for\na unified quantum-classical programming environment built upon classical HPC\nsoftware stack (compilers, libraries, parallel runtime and process scheduling)."}
{"id": "2510.20171", "categories": ["cs.DC", "cs.AI", "cs.NI", "C.2.4; I.2"], "pdf": "https://arxiv.org/pdf/2510.20171", "abs": "https://arxiv.org/abs/2510.20171", "authors": ["Min Si", "Pavan Balaji", "Yongzhou Chen", "Ching-Hsiang Chu", "Adi Gangidi", "Saif Hasan", "Subodh Iyengar", "Dan Johnson", "Bingzhe Liu", "Jingliang Ren", "Ashmitha Jeevaraj Shetty", "Greg Steinbrecher", "Xinfeng Xie", "Yulun Wang", "Bruce Wu", "Jingyi Yang", "Mingran Yang", "Minlan Yu", "Cen Zhao", "Wes Bland", "Denis Boyda", "Suman Gumudavelli", "Cristian Lumezanu", "Rui Miao", "Zhe Qu", "Venkat Ramesh", "Maxim Samoylov", "Jan Seidel", "Feng Tian", "Qiye Tan", "Shuqiang Zhang", "Yimeng Zhao", "Shengbao Zheng", "Art Zhu", "Hongyi Zeng"], "title": "Collective Communication for 100k+ GPUs", "comment": null, "summary": "The increasing scale of large language models (LLMs) necessitates highly\nefficient collective communication frameworks, particularly as training\nworkloads extend to hundreds of thousands of GPUs. Traditional communication\nmethods face significant throughput and latency limitations at this scale,\nhindering both the development and deployment of state-of-the-art models. This\npaper presents the NCCLX collective communication framework, developed at Meta,\nengineered to optimize performance across the full LLM lifecycle, from the\nsynchronous demands of large-scale training to the low-latency requirements of\ninference. The framework is designed to support complex workloads on clusters\nexceeding 100,000 GPUs, ensuring reliable, high-throughput, and low-latency\ndata exchange. Empirical evaluation on the Llama4 model demonstrates\nsubstantial improvements in communication efficiency. This research contributes\na robust solution for enabling the next generation of LLMs to operate at\nunprecedented scales."}
{"id": "2510.20137", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2510.20137", "abs": "https://arxiv.org/abs/2510.20137", "authors": ["Hasnain A. Ziad", "Ashiq A. Sakib"], "title": "HALOC-AxA: An Area/-Energy-Efficient Approximate Adder for Image Processing Application", "comment": "5 Pages, 6 Figures, and 1 Table", "summary": "The design of approximate adders has been widely researched to advance\nenergy-efficient hardware for computation-intensive multimedia applications,\nsuch as image, audio, or video processing. The design of approximate adders has\nbeen widely researched to advance energy-efficient hardware for computation\nintensive multimedia applications, such as image/audio/video processing.\nSeveral static and dynamic approximate adders exist in the literature, each of\nwhich endeavors to balance the conflicting demands of high performance,\ncomputational accuracy, and energy efficiency. This work introduces a novel\napproximate adder that is more energy- and area-efficient than existing adders,\nwhile achieving improved or comparable accuracy, as demonstrated by simulation\nresults. The proposed adder's ability to digitally reconstruct high quality\nimages is further demonstrated by the deployment of the design for an image\nprocessing task."}
{"id": "2510.19860", "categories": ["cs.SE", "D.2.5"], "pdf": "https://arxiv.org/pdf/2510.19860", "abs": "https://arxiv.org/abs/2510.19860", "authors": ["Ketai Qiu", "Luca Di Grazia", "Leonardo Mariani", "Mauro Pezzè"], "title": "E-Test: E'er-Improving Test Suites", "comment": "Accepted at the 48th IEEE/ACM International Conference on Software\n  Engineering (ICSE 2026)", "summary": "Test suites are inherently imperfect, and testers can always enrich a suite\nwith new test cases that improve its quality and, consequently, the reliability\nof the target software system. However, finding test cases that explore\nexecution scenarios beyond the scope of an existing suite can be extremely\nchallenging and labor-intensive, particularly when managing large test suites\nover extended periods.\n  In this paper, we propose E-Test, an approach that reduces the gap between\nthe execution space explored with a test suite and the executions experienced\nafter testing by augmenting the test suite with test cases that explore\nexecution scenarios that emerge in production. E-Test (i) identifies executions\nthat have not yet been tested from large sets of scenarios, such as those\nmonitored during intensive production usage, and (ii) generates new test cases\nthat enhance the test suite. E-Test leverages Large Language Models (LLMs) to\npinpoint scenarios that the current test suite does not adequately cover, and\naugments the suite with test cases that execute these scenarios.\n  Our evaluation on a dataset of 1,975 scenarios, collected from highly-starred\nopen-source Java projects already in production and Defects4J, demonstrates\nthat E-Test retrieves not-yet-tested execution scenarios significantly better\nthan state-of-the-art approaches. While existing regression testing and field\ntesting approaches for this task achieve a maximum F1-score of 0.34, and\nvanilla LLMs achieve a maximum F1-score of 0.39, E-Test reaches 0.55. These\nresults highlight the impact of E-Test in enhancing test suites by effectively\ntargeting not-yet-tested execution scenarios and reducing manual effort\nrequired for maintaining test suites."}
{"id": "2510.19973", "categories": ["cs.NI", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.19973", "abs": "https://arxiv.org/abs/2510.19973", "authors": ["Hatim Chergui", "Farhad Rezazadeh", "Merouane Debbah", "Christos Verikoukis"], "title": "A Tutorial on Cognitive Biases in Agentic AI-Driven 6G Autonomous Networks", "comment": "19 pages, 15 figures, 1 table", "summary": "The path to higher network autonomy in 6G lies beyond the mere optimization\nof key performance indicators (KPIs). While KPIs have enabled automation gains\nunder TM Forum Levels 1--3, they remain numerical abstractions that act only as\nproxies for the real essence of communication networks: seamless connectivity,\nfairness, adaptability, and resilience. True autonomy requires perceiving and\nreasoning over the network environment as it is. Such progress can be achieved\nthrough \\emph{agentic AI}, where large language model (LLM)-powered agents\nperceive multimodal telemetry, reason with memory, negotiate across domains,\nand act via APIs to achieve multi-objective goals. However, deploying such\nagents introduces the challenge of cognitive biases inherited from human\ndesign, which can distort reasoning, negotiation, tool use, and actuation.\nBetween neuroscience and AI, this paper provides a tutorial on a selection of\nwell-known biases, including their taxonomy, definition, mathematical\nformulation, emergence in telecom systems and the commonly impacted agentic\ncomponents. The tutorial also presents various mitigation strategies tailored\nto each type of bias. The article finally provides two practical use-cases,\nwhich tackle the emergence, impact and mitigation gain of some famous biases in\n6G inter-slice and cross-domain management. In particular, anchor\nrandomization, temporal decay and inflection bonus techniques are introduced to\nspecifically address anchoring, temporal and confirmation biases. This avoids\nthat agents stick to the initial high resource allocation proposal or decisions\nthat are recent and/or confirming a prior hypothesis. By grounding decisions in\na richer and fairer set of past experiences, the quality and bravery of the\nagentic agreements in the second use-case, for instance, are leading to $\\times\n5$ lower latency and around $40\\%$ higher energy saving."}
{"id": "2510.19995", "categories": ["cs.MA", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.19995", "abs": "https://arxiv.org/abs/2510.19995", "authors": ["Yiming Lu", "Xun Wang", "Simin Ma", "Shujian Liu", "Sathish Reddy Indurthi", "Song Wang", "Haoyun Deng", "Fei Liu", "Kaiqiang Song"], "title": "Communication to Completion: Modeling Collaborative Workflows with Intelligent Multi-Agent Communication", "comment": "13 pages", "summary": "Teamwork in workspace for complex tasks requires diverse communication\nstrategies, but current multi-agent LLM systems lack systematic frameworks for\ntask oriented communication. We introduce Communication to Completion (C2C), a\nscalable framework that addresses this gap through two key innovations: (1) the\nAlignment Factor (AF), a novel metric quantifying agent task alignment that\ndirectly impacts work efficiency, and (2) a Sequential Action Framework that\nintegrates stepwise execution with intelligent communication decisions. C2C\nenables agents to make cost aware communication choices, dynamically improving\ntask understanding through targeted interactions. We evaluated C2C on realistic\ncoding workflows across three complexity tiers and team sizes from 5 to 17\nagents, comparing against no communication and fixed steps baselines. The\nresults show that C2C reduces the task completion time by about 40% with\nacceptable communication costs. The framework completes all tasks successfully\nin standard configurations and maintains effectiveness at scale. C2C\nestablishes both a theoretical foundation for measuring communication\neffectiveness in multi-agent systems and a practical framework for complex\ncollaborative tasks."}
{"id": "2510.20388", "categories": ["cs.DC", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.20388", "abs": "https://arxiv.org/abs/2510.20388", "authors": ["Víctor Rampérez", "Javier Soriano", "David Lizcano", "Juan A. Lara"], "title": "FLAS: a combination of proactive and reactive auto-scaling architecture for distributed services", "comment": null, "summary": "Cloud computing has established itself as the support for the vast majority\nof emerging technologies, mainly due to the characteristic of elasticity it\noffers. Auto-scalers are the systems that enable this elasticity by acquiring\nand releasing resources on demand to ensure an agreed service level. In this\narticle we present FLAS (Forecasted Load Auto-Scaling), an auto-scaler for\ndistributed services that combines the advantages of proactive and reactive\napproaches according to the situation to decide the optimal scaling actions in\nevery moment. The main novelties introduced by FLAS are (i) a predictive model\nof the high-level metrics trend which allows to anticipate changes in the\nrelevant SLA parameters (e.g. performance metrics such as response time or\nthroughput) and (ii) a reactive contingency system based on the estimation of\nhigh-level metrics from resource use metrics, reducing the necessary\ninstrumentation (less invasive) and allowing it to be adapted agnostically to\ndifferent applications. We provide a FLAS implementation for the use case of a\ncontent-based publish-subscribe middleware (E-SilboPS) that is the cornerstone\nof an event-driven architecture. To the best of our knowledge, this is the\nfirst auto-scaling system for content-based publish-subscribe distributed\nsystems (although it is generic enough to fit any distributed service). Through\nan evaluation based on several test cases recreating not only the expected\ncontexts of use, but also the worst possible scenarios (following the\nBoundary-Value Analysis or BVA test methodology), we have validated our\napproach and demonstrated the effectiveness of our solution by ensuring\ncompliance with performance requirements over 99% of the time."}
{"id": "2510.20269", "categories": ["cs.AR", "cs.CR", "cs.DC"], "pdf": "https://arxiv.org/pdf/2510.20269", "abs": "https://arxiv.org/abs/2510.20269", "authors": ["Ismail Emir Yuksel", "Ataberk Olgun", "F. Nisa Bostanci", "Oguzhan Canpolat", "Geraldo F. Oliveira", "Mohammad Sadrosadati", "Abdullah Giray Yaglikci", "Onur Mutlu"], "title": "In-DRAM True Random Number Generation Using Simultaneous Multiple-Row Activation: An Experimental Study of Real DRAM Chips", "comment": "Extended version of our publication at the 43rd IEEE International\n  Conference on Computer Design (ICCD-43), 2025", "summary": "In this work, we experimentally demonstrate that it is possible to generate\ntrue random numbers at high throughput and low latency in commercial\noff-the-shelf (COTS) DRAM chips by leveraging simultaneous multiple-row\nactivation (SiMRA) via an extensive characterization of 96 DDR4 DRAM chips. We\nrigorously analyze SiMRA's true random generation potential in terms of\nentropy, latency, and throughput for varying numbers of simultaneously\nactivated DRAM rows (i.e., 2, 4, 8, 16, and 32), data patterns, temperature\nlevels, and spatial variations. Among our 11 key experimental observations, we\nhighlight four key results. First, we evaluate the quality of our TRNG designs\nusing the commonly-used NIST statistical test suite for randomness and find\nthat all SiMRA-based TRNG designs successfully pass each test. Second, 2-, 8-,\n16-, and 32-row activation-based TRNG designs outperform the state-of-theart\nDRAM-based TRNG in throughput by up to 1.15x, 1.99x, 1.82x, and 1.39x,\nrespectively. Third, SiMRA's entropy tends to increase with the number of\nsimultaneously activated DRAM rows. Fourth, operational parameters and\nconditions (e.g., data pattern and temperature) significantly affect entropy.\nFor example, for most of the tested modules, the average entropy of 32-row\nactivation is 2.51x higher than that of 2-row activation. For example,\nincreasing the temperature from 50{\\deg}C to 90{\\deg}C decreases SiMRA's\nentropy by 1.53x for 32-row activation. To aid future research and development,\nwe open-source our infrastructure at https://github.com/CMU-SAFARI/SiMRA-TRNG."}
{"id": "2510.19864", "categories": ["cs.SE", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.19864", "abs": "https://arxiv.org/abs/2510.19864", "authors": ["Amila Indika", "Igor Molybog"], "title": "SODBench: A Large Language Model Approach to Documenting Spreadsheet Operations", "comment": "14 pages, 5 figures, 4 tables", "summary": "Numerous knowledge workers utilize spreadsheets in business, accounting, and\nfinance. However, a lack of systematic documentation methods for spreadsheets\nhinders automation, collaboration, and knowledge transfer, which risks the loss\nof crucial institutional knowledge. This paper introduces Spreadsheet\nOperations Documentation (SOD), an AI task that involves generating\nhuman-readable explanations from spreadsheet operations. Many previous studies\nhave utilized Large Language Models (LLMs) for generating spreadsheet\nmanipulation code; however, translating that code into natural language for SOD\nis a less-explored area. To address this, we present a benchmark of 111\nspreadsheet manipulation code snippets, each paired with a corresponding\nnatural language summary. We evaluate five LLMs, GPT-4o, GPT-4o-mini,\nLLaMA-3.3-70B, Mixtral-8x7B, and Gemma2-9B, using BLEU, GLEU, ROUGE-L, and\nMETEOR metrics. Our findings suggest that LLMs can generate accurate\nspreadsheet documentation, making SOD a feasible prerequisite step toward\nenhancing reproducibility, maintainability, and collaborative workflows in\nspreadsheets, although there are challenges that need to be addressed."}
{"id": "2510.20297", "categories": ["cs.NI"], "pdf": "https://arxiv.org/pdf/2510.20297", "abs": "https://arxiv.org/abs/2510.20297", "authors": ["Xiao Song", "John Heidemann"], "title": "Rediscovering Recurring Routing Results", "comment": null, "summary": "Routing is central to networking performance, including: (1) latency in\nanycast services and websites served from multiple locations,(2) networking\nexpenses and throughput in multi-homed enterprises, (3) the ability to keep\ntraffic domestic when considering data sovereignty. However, understanding and\nmanaging how routing affects these services is challenging. Operators use\nTraffic Engineering (TE) with BGP to optimize network performance, but what\nthey get is the result of all BGP policies throughout the Internet, not just\ntheir local choices. Our paper proposes Fenrir, a new system to rediscover\nrecurring routing results. Fenrir can discover changes in network routing, even\nwhen it happens multiple hops away from the observer. Fenrir also provides new\nmethods to quantify the degree of routing change, and to identify routing\n\"modes\" that may reappear. Second, we show that Fenrir can be applied to many\ndifferent problems: we use five instances of three different types of systems\nto illustrate the generalization: anycast catchments showing in a root DNS\nservice, route optimization for two multi-homed enterprises, and website\nselection for two of the top-10 web services. Each type requires different\ntypes of active measurements, data cleaning and weighting. We demonstrate\nFenrir's methods of detecting and quantifying change are helpful because they\nall face similar operational questions: How much effect did traffic engineering\nhave? Did a third-party change alter my routing? In either case, is the current\nrouting new, or is it like a routing mode I saw before?"}
{"id": "2510.20218", "categories": ["cs.MA", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.20218", "abs": "https://arxiv.org/abs/2510.20218", "authors": ["Qinyu Xu", "Yuanyang Zhu", "Xuefei Wu", "Chunlin Chen"], "title": "High-order Interactions Modeling for Interpretable Multi-Agent Q-Learning", "comment": "39th Conference on Neural Information Processing Systems", "summary": "The ability to model interactions among agents is crucial for effective\ncoordination and understanding their cooperation mechanisms in multi-agent\nreinforcement learning (MARL). However, previous efforts to model high-order\ninteractions have been primarily hindered by the combinatorial explosion or the\nopaque nature of their black-box network structures. In this paper, we propose\na novel value decomposition framework, called Continued Fraction Q-Learning\n(QCoFr), which can flexibly capture arbitrary-order agent interactions with\nonly linear complexity $\\mathcal{O}\\left({n}\\right)$ in the number of agents,\nthus avoiding the combinatorial explosion when modeling rich cooperation.\nFurthermore, we introduce the variational information bottleneck to extract\nlatent information for estimating credits. This latent information helps agents\nfilter out noisy interactions, thereby significantly enhancing both cooperation\nand interpretability. Extensive experiments demonstrate that QCoFr not only\nconsistently achieves better performance but also provides interpretability\nthat aligns with our theoretical analysis."}
{"id": "2510.20495", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2510.20495", "abs": "https://arxiv.org/abs/2510.20495", "authors": ["Panagiotis Giannakopoulos", "Bart van Knippenberg", "Kishor Chandra Joshi", "Nicola Calabretta", "George Exarchakos"], "title": "Accurate Performance Predictors for Edge Computing Applications", "comment": null, "summary": "Accurate prediction of application performance is critical for enabling\neffective scheduling and resource management in resource-constrained dynamic\nedge environments. However, achieving predictable performance in such\nenvironments remains challenging due to the co-location of multiple\napplications and the node heterogeneity. To address this, we propose a\nmethodology that automatically builds and assesses various performance\npredictors. This approach prioritizes both accuracy and inference time to\nidentify the most efficient model. Our predictors achieve up to 90% accuracy\nwhile maintaining an inference time of less than 1% of the Round Trip Time.\nThese predictors are trained on the historical state of the most correlated\nmonitoring metrics to application performance and evaluated across multiple\nservers in dynamic co-location scenarios. As usecase we consider electron\nmicroscopy (EM) workflows, which have stringent real-time demands and diverse\nresource requirements. Our findings emphasize the need for a systematic\nmethodology that selects server-specific predictors by jointly optimizing\naccuracy and inference latency in dynamic co-location scenarios. Integrating\nsuch predictors into edge environments can improve resource utilization and\nresult in predictable performance."}
{"id": "2510.20400", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2510.20400", "abs": "https://arxiv.org/abs/2510.20400", "authors": ["Rubén Langarita", "Jesús Alastruey-Benedé", "Pablo Ibáñez-Marín", "Santiago Marco-Sola", "Miquel Moretó", "Adrià Armejach"], "title": "Squire: A General-Purpose Accelerator to Exploit Fine-Grain Parallelism on Dependency-Bound Kernels", "comment": "11 pages, 10 figures, 5 tables, 4 algorithms, accepted on PACT25", "summary": "Multiple HPC applications are often bottlenecked by compute-intensive kernels\nimplementing complex dependency patterns (data-dependency bound). Traditional\ngeneral-purpose accelerators struggle to effectively exploit fine-grain\nparallelism due to limitations in implementing convoluted data-dependency\npatterns (like SIMD) and overheads due to synchronization and data transfers\n(like GPGPUs). In contrast, custom FPGA and ASIC designs offer improved\nperformance and energy efficiency at a high cost in hardware design and\nprogramming complexity and often lack the flexibility to process different\nworkloads. We propose Squire, a general-purpose accelerator designed to exploit\nfine-grain parallelism effectively on dependency-bound kernels. Each Squire\naccelerator has a set of general-purpose low-power in-order cores that can\nrapidly communicate among themselves and directly access data from the L2\ncache. Our proposal integrates one Squire accelerator per core in a typical\nmulticore system, allowing the acceleration of dependency-bound kernels within\nparallel tasks with minimal software changes. As a case study, we evaluate\nSquire's effectiveness by accelerating five kernels that implement complex\ndependency patterns. We use three of these kernels to build an end-to-end\nread-mapping tool that will be used to evaluate Squire. Squire obtains speedups\nup to 7.64$\\times$ in dynamic programming kernels. Overall, Squire provides an\nacceleration for an end-to-end application of 3.66$\\times$. In addition, Squire\nreduces energy consumption by up to 56% with a minimal area overhead of 10.5%\ncompared to a Neoverse-N1 baseline."}
{"id": "2510.19868", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.19868", "abs": "https://arxiv.org/abs/2510.19868", "authors": ["Qian Xiong", "Bo Yang", "Weisong Sun", "Yiran Zhang", "Tianlin Li", "Yang Liu", "Zhi Jin"], "title": "Knowledge-Guided Multi-Agent Framework for Application-Level Software Code Generation", "comment": null, "summary": "Automated code generation driven by Large Lan- guage Models (LLMs) has\nenhanced development efficiency, yet generating complex application-level\nsoftware code remains challenging. Multi-agent frameworks show potential, but\nexisting methods perform inadequately in large-scale application-level software\ncode generation, failing to ensure reasonable orga- nizational structures of\nproject code and making it difficult to maintain the code generation process.\nTo address this, this paper envisions a Knowledge-Guided Application-Level Code\nGeneration framework named KGACG, which aims to trans- form software\nrequirements specification and architectural design document into executable\ncode through a collaborative closed- loop of the Code Organization & Planning\nAgent (COPA), Coding Agent (CA), and Testing Agent (TA), combined with a\nfeedback mechanism. We demonstrate the collaborative process of the agents in\nKGACG in a Java Tank Battle game case study while facing challenges. KGACG is\ndedicated to advancing the automation of application-level software\ndevelopment."}
{"id": "2510.20440", "categories": ["cs.NI", "68-06", "C.2.3; C.2.5"], "pdf": "https://arxiv.org/pdf/2510.20440", "abs": "https://arxiv.org/abs/2510.20440", "authors": ["Heiko Geppert", "Frank Dürr", "Simon Naß", "Kurt Rothermel"], "title": "Multicast-partitioning in Time-triggered Stream Planning for Time-Sensitive Networks", "comment": null, "summary": "Multicast allows sending a message to multiple recipients without having to\ncreate and send a separate message for each recipient. This preserves network\nbandwidth, which is particularly important in time-sensitive networks. These\nnetworks are commonly used to provide latency-bounded communication for\nreal-time systems in domains like automotive, avionics, industrial internet of\nthings, automated shop floors, and smart energy grids. The preserved bandwidth\ncan be used to admit additional real-time messages with specific quality of\nservice requirements or to reduce the end-to-end latencies for messages of any\ntype. However, using multicast communication can complicate traffic planning,\nas it requires free queues or available downstream egress ports on all branches\nof the multicast tree. In this work, we present a novel multicast partitioning\ntechnique to split multicast trees into smaller multicast or unicast trees.\nThis allows for a more fine-grained trade-off between bandwidth utilization and\ntraffic scheduling difficulty. Thus, schedulability in dynamic systems can be\nimproved, in terms the number of admitted streams and the accumulated network\nthroughput. We evaluated the multicast partitioning on different network\ntopologies and with three different scheduling algorithms. With the\npartitioning, 5-15\\% fewer streams were rejected, while achieving 5-125\\% more\nnetwork throughput, depending on the scheduling algorithm."}
{"id": "2510.20469", "categories": ["cs.MA", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.20469", "abs": "https://arxiv.org/abs/2510.20469", "authors": ["Horacio Paggi", "Juan A. Lara", "Javier Soriano"], "title": "Structures generated in a multiagent system performing information fusion in peer-to-peer resource-constrained networks", "comment": null, "summary": "There has recently been a major advance with respect to how information\nfusion is performed. Information fusion has gone from being conceived as a\npurely hierarchical procedure, as is the case of traditional military\napplications, to now being regarded collaboratively, as holonic fusion, which\nis better suited for civil applications and edge organizations. The above\nparadigm shift is being boosted as information fusion gains ground in different\nnon-military areas, and human-computer and machine-machine communications,\nwhere holarchies, which are more flexible structures than ordinary, static\nhierarchies, become more widespread. This paper focuses on showing how holonic\nstructures tend to be generated when there are constraints on resources\n(energy, available messages, time, etc.) for interactions based on a set of\nfully intercommunicating elements (peers) whose components fuse information as\na means of optimizing the impact of vagueness and uncertainty present message\nexchanges. Holon formation is studied generically based on a multiagent system\nmodel, and an example of its possible operation is shown. Holonic structures\nhave a series of advantages, such as adaptability, to sudden changes in the\nenvironment or its composition, are somewhat autonomous and are capable of\ncooperating in order to achieve a common goal. This can be useful when the\nshortage of resources prevents communications or when the system components\nstart to fail."}
{"id": "2510.20506", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2510.20506", "abs": "https://arxiv.org/abs/2510.20506", "authors": ["Panagiotis Giannakopoulos", "Bart van Knippenberg", "Kishor Chandra Joshi", "Nicola Calabretta", "George Exarchakos"], "title": "Morpheus: Lightweight RTT Prediction for Performance-Aware Load Balancing", "comment": null, "summary": "Distributed applications increasingly demand low end-to-end latency,\nespecially in edge and cloud environments where co-located workloads contend\nfor limited resources. Traditional load-balancing strategies are typically\nreactive and rely on outdated or coarse-grained metrics, often leading to\nsuboptimal routing decisions and increased tail latencies. This paper\ninvestigates the use of round-trip time (RTT) predictors to enhance request\nrouting by anticipating application latency. We develop lightweight and\naccurate RTT predictors that are trained on time-series monitoring data\ncollected from a Kubernetes-managed GPU cluster. By leveraging a reduced set of\nhighly correlated monitoring metrics, our approach maintains low overhead while\nremaining adaptable to diverse co-location scenarios and heterogeneous\nhardware. The predictors achieve up to 95% accuracy while keeping the\nprediction delay within 10% of the application RTT. In addition, we identify\nthe minimum prediction accuracy threshold and key system-level factors required\nto ensure effective predictor deployment in resource-constrained clusters.\nSimulation-based evaluation demonstrates that performance-aware load balancing\ncan significantly reduce application RTT and minimize resource waste. These\nresults highlight the feasibility of integrating predictive load balancing into\nfuture production systems."}
{"id": "2510.19898", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.19898", "abs": "https://arxiv.org/abs/2510.19898", "authors": ["Atharv Sonwane", "Isadora White", "Hyunji Lee", "Matheus Pereira", "Lucas Caccia", "Minseon Kim", "Zhengyan Shi", "Chinmay Singh", "Alessandro Sordoni", "Marc-Alexandre Côté", "Xingdi Yuan"], "title": "BugPilot: Complex Bug Generation for Efficient Learning of SWE Skills", "comment": null, "summary": "High quality bugs are key to training the next generation of language model\nbased software engineering (SWE) agents. We introduce a novel method for\nsynthetic generation of difficult and diverse bugs. Our method instructs SWE\nAgents to introduce a feature into the codebase whereby they may\nunintentionally break tests, resulting in bugs. Prior approaches often induce\nan out-of-distribution effect by generating bugs intentionally (e.g. by\nintroducing local perturbation to existing code), which does not reflect\nrealistic development processes. We perform qualitative analysis to demonstrate\nthat our approach for generating bugs more closely reflects the patterns found\nin human-authored edits. Through extensive experiments, we demonstrate that our\nbugs provide more efficient training data for supervised fine-tuning,\noutperforming other bug datasets by 2% with half the training data (1.2k vs. 3k\nbugs). We train on our newly generated bugs in addition to existing bug\ndatasets to get FrogBoss a state-of-the-art 32B parameter model on SWE-bench\nVerified with a pass@1 of 54.6% and FrogMini a state-of-the-art 14B model on\nSWE-bench Verified with a pass@1 of 45.3% on SWE-bench Verified averaged over\nthree seeds."}
{"id": "2510.20703", "categories": ["cs.NI"], "pdf": "https://arxiv.org/pdf/2510.20703", "abs": "https://arxiv.org/abs/2510.20703", "authors": ["Felipe Avencourt Soares", "Muriel F. Franco", "Eder J. Scheid", "Lisandro Z. Granville"], "title": "Trust, But Verify: An Empirical Evaluation of AI-Generated Code for SDN Controllers", "comment": "Submitted to IEEE/IFIP Network Operations and Management Symposium\n  (NOMS 2025)", "summary": "Generative Artificial Intelligence (AI) tools have been used to generate\nhuman-like content across multiple domains (e.g., sound, image, text, and\nprogramming). However, their reliability in terms of correctness and\nfunctionality in novel contexts such as programmable networks remains unclear.\nHence, this paper presents an empirical evaluation of the source code of a POX\ncontroller generated by different AI tools, namely ChatGPT, Copilot, DeepSeek,\nand BlackBox.ai. To evaluate such a code, three networking tasks of increasing\ncomplexity were defined and for each task, zero-shot and few-shot prompting\ntechniques were input to the tools. Next, the output code was tested in\nemulated network topologies with Mininet and analyzed according to\nfunctionality, correctness, and the need for manual fixes. Results show that\nall evaluated models can produce functional controllers. However, ChatGPT and\nDeepSeek exhibited higher consistency and code quality, while Copilot and\nBlackBox.ai required more adjustments."}
{"id": "2510.20269", "categories": ["cs.AR", "cs.CR", "cs.DC"], "pdf": "https://arxiv.org/pdf/2510.20269", "abs": "https://arxiv.org/abs/2510.20269", "authors": ["Ismail Emir Yuksel", "Ataberk Olgun", "F. Nisa Bostanci", "Oguzhan Canpolat", "Geraldo F. Oliveira", "Mohammad Sadrosadati", "Abdullah Giray Yaglikci", "Onur Mutlu"], "title": "In-DRAM True Random Number Generation Using Simultaneous Multiple-Row Activation: An Experimental Study of Real DRAM Chips", "comment": "Extended version of our publication at the 43rd IEEE International\n  Conference on Computer Design (ICCD-43), 2025", "summary": "In this work, we experimentally demonstrate that it is possible to generate\ntrue random numbers at high throughput and low latency in commercial\noff-the-shelf (COTS) DRAM chips by leveraging simultaneous multiple-row\nactivation (SiMRA) via an extensive characterization of 96 DDR4 DRAM chips. We\nrigorously analyze SiMRA's true random generation potential in terms of\nentropy, latency, and throughput for varying numbers of simultaneously\nactivated DRAM rows (i.e., 2, 4, 8, 16, and 32), data patterns, temperature\nlevels, and spatial variations. Among our 11 key experimental observations, we\nhighlight four key results. First, we evaluate the quality of our TRNG designs\nusing the commonly-used NIST statistical test suite for randomness and find\nthat all SiMRA-based TRNG designs successfully pass each test. Second, 2-, 8-,\n16-, and 32-row activation-based TRNG designs outperform the state-of-theart\nDRAM-based TRNG in throughput by up to 1.15x, 1.99x, 1.82x, and 1.39x,\nrespectively. Third, SiMRA's entropy tends to increase with the number of\nsimultaneously activated DRAM rows. Fourth, operational parameters and\nconditions (e.g., data pattern and temperature) significantly affect entropy.\nFor example, for most of the tested modules, the average entropy of 32-row\nactivation is 2.51x higher than that of 2-row activation. For example,\nincreasing the temperature from 50{\\deg}C to 90{\\deg}C decreases SiMRA's\nentropy by 1.53x for 32-row activation. To aid future research and development,\nwe open-source our infrastructure at https://github.com/CMU-SAFARI/SiMRA-TRNG."}
{"id": "2510.19984", "categories": ["cs.SE", "D.2.5"], "pdf": "https://arxiv.org/pdf/2510.19984", "abs": "https://arxiv.org/abs/2510.19984", "authors": ["Konstantinos Kitsios", "Marcel Böhme", "Alberto Bacchelli"], "title": "On Interaction Effects in Greybox Fuzzing", "comment": "12 pages, 2 figures, Accepted for presentation at the 48th\n  International Conference on Software Engineering (ICSE '26)", "summary": "A greybox fuzzer is an automated software testing tool that generates new\ntest inputs by applying randomly chosen mutators (e.g., flipping a bit or\ndeleting a block of bytes) to a seed input in random order and adds all\ncoverage-increasing inputs to the corpus of seeds. We hypothesize that the\norder in which mutators are applied to a seed input has an impact on the\neffectiveness of greybox fuzzers. In our experiments, we fit a linear model to\na dataset that contains the effectiveness of all possible mutator pairs and\nindeed observe the conjectured interaction effect. This points us to more\nefficient fuzzing by choosing the most promising mutator sequence with a higher\nlikelihood. We propose MuoFuzz, a greybox fuzzer that learns and chooses the\nmost promising mutator sequences. MuoFuzz learns the conditional probability\nthat the next mutator will yield an interesting input, given the previously\nselected mutator. Then, it samples from the learned probability using a random\nwalk to generate mutator sequences. We compare the performance of MuoFuzz to\nAFL++, which uses a fixed selection probability, and MOPT, which optimizes the\nselection probability of each mutator in isolation. Experimental results on the\nFuzzBench and MAGMA benchmarks show that MuoFuzz achieves the highest code\ncoverage and finds four bugs missed by AFL++ and one missed by both AFL++ and\nMOPT."}
{"id": "2510.20796", "categories": ["cs.NI"], "pdf": "https://arxiv.org/pdf/2510.20796", "abs": "https://arxiv.org/abs/2510.20796", "authors": ["John Sengendo", "Fabrizio Granelli"], "title": "AI-Enabled Digital Twins for Next-Generation Networks: Forecasting Traffic and Resource Management in 5G/6G", "comment": "6 pages, 6 figures, 2 Tables, Accepted and to be presented at IEEE\n  Global Communications Conference (GLOBECOM) 2025", "summary": "As 5G and future 6G mobile networks become increasingly more sophisticated,\nthe requirements for agility, scalability, resilience, and precision in\nreal-time service provisioning cannot be met using traditional and\nheuristic-based resource management techniques, just like any advancing\ntechnology. With the aim of overcoming such limitations, network operators are\nforeseeing Digital Twins (DTs) as key enablers, which are designed as dynamic\nand virtual replicas of network infrastructure, allowing operators to model,\nanalyze, and optimize various operations without any risk of affecting the live\nnetwork. However, for Digital Twin Networks (DTNs) to meet the challenges faced\nby operators especially in line with resource management, a driving engine is\nneeded. In this paper, an AI (Artificial Intelligence)-driven approach is\npresented by integrating a Long Short-Term Memory (LSTM) neural network into\nthe DT framework, aimed at forecasting network traffic patterns and proactively\nmanaging resource allocation. Through analytical experiments, the AI-Enabled DT\nframework demonstrates superior performance benchmarked against baseline\nmethods. Our study concludes that embedding AI capabilities within DTs paves\nthe way for fully autonomous, adaptive, and high-performance network management\nin future mobile networks."}
{"id": "2510.20389", "categories": ["cs.SE", "cs.DC", "D.m"], "pdf": "https://arxiv.org/pdf/2510.20389", "abs": "https://arxiv.org/abs/2510.20389", "authors": ["Bjorn Remseth"], "title": "Symmetry in Software Platforms as an Architectural Principle", "comment": "Working paper, 11 pages", "summary": "Software platforms often act as structure preserving systems. They provide\nconsistent interfaces and behaviors that remain stable under specific\ntransformations that we denote as symmetries. This paper explores the idea that\narchitectural robustness emerges from enforcing such structural regularities"}
{"id": "2510.19997", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.19997", "abs": "https://arxiv.org/abs/2510.19997", "authors": ["Abraham Itzhak Weinberg"], "title": "A Framework for the Adoption and Integration of Generative AI in Midsize Organizations and Enterprises (FAIGMOE)", "comment": null, "summary": "Generative Artificial Intelligence (GenAI) presents transformative\nopportunities for organizations, yet both midsize organizations and larger\nenterprises face distinctive adoption challenges. Midsize organizations\nencounter resource constraints and limited AI expertise, while enterprises\nstruggle with organizational complexity and coordination challenges. Existing\ntechnology adoption frameworks, including TAM (Technology Acceptance Model),\nTOE (Technology Organization Environment), and DOI (Diffusion of Innovations)\ntheory, lack the specificity required for GenAI implementation across these\ndiverse contexts, creating a critical gap in adoption literature. This paper\nintroduces FAIGMOE (Framework for the Adoption and Integration of Generative AI\nin Midsize Organizations and Enterprises), a conceptual framework addressing\nthe unique needs of both organizational types. FAIGMOE synthesizes technology\nadoption theory, organizational change management, and innovation diffusion\nperspectives into four interconnected phases: Strategic Assessment, Planning\nand Use Case Development, Implementation and Integration, and\nOperationalization and Optimization. Each phase provides scalable guidance on\nreadiness assessment, strategic alignment, risk governance, technical\narchitecture, and change management adaptable to organizational scale and\ncomplexity. The framework incorporates GenAI specific considerations including\nprompt engineering, model orchestration, and hallucination management that\ndistinguish it from generic technology adoption frameworks. As a perspective\ncontribution, FAIGMOE provides the first comprehensive conceptual framework\nexplicitly addressing GenAI adoption across midsize and enterprise\norganizations, offering actionable implementation protocols, assessment\ninstruments, and governance templates requiring empirical validation through\nfuture research."}
{"id": "2510.20171", "categories": ["cs.DC", "cs.AI", "cs.NI", "C.2.4; I.2"], "pdf": "https://arxiv.org/pdf/2510.20171", "abs": "https://arxiv.org/abs/2510.20171", "authors": ["Min Si", "Pavan Balaji", "Yongzhou Chen", "Ching-Hsiang Chu", "Adi Gangidi", "Saif Hasan", "Subodh Iyengar", "Dan Johnson", "Bingzhe Liu", "Jingliang Ren", "Ashmitha Jeevaraj Shetty", "Greg Steinbrecher", "Xinfeng Xie", "Yulun Wang", "Bruce Wu", "Jingyi Yang", "Mingran Yang", "Minlan Yu", "Cen Zhao", "Wes Bland", "Denis Boyda", "Suman Gumudavelli", "Cristian Lumezanu", "Rui Miao", "Zhe Qu", "Venkat Ramesh", "Maxim Samoylov", "Jan Seidel", "Feng Tian", "Qiye Tan", "Shuqiang Zhang", "Yimeng Zhao", "Shengbao Zheng", "Art Zhu", "Hongyi Zeng"], "title": "Collective Communication for 100k+ GPUs", "comment": null, "summary": "The increasing scale of large language models (LLMs) necessitates highly\nefficient collective communication frameworks, particularly as training\nworkloads extend to hundreds of thousands of GPUs. Traditional communication\nmethods face significant throughput and latency limitations at this scale,\nhindering both the development and deployment of state-of-the-art models. This\npaper presents the NCCLX collective communication framework, developed at Meta,\nengineered to optimize performance across the full LLM lifecycle, from the\nsynchronous demands of large-scale training to the low-latency requirements of\ninference. The framework is designed to support complex workloads on clusters\nexceeding 100,000 GPUs, ensuring reliable, high-throughput, and low-latency\ndata exchange. Empirical evaluation on the Llama4 model demonstrates\nsubstantial improvements in communication efficiency. This research contributes\na robust solution for enabling the next generation of LLMs to operate at\nunprecedented scales."}
{"id": "2510.20041", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.20041", "abs": "https://arxiv.org/abs/2510.20041", "authors": ["Gareema Ranjan", "Mahmoud Alfadel", "Gengyi Sun", "Shane McIntosh"], "title": "The Cost of Downgrading Build Systems: A Case Study of Kubernetes", "comment": null, "summary": "Since developers invoke the build system frequently, its performance can\nimpact productivity. Modern artifact-based build tools accelerate builds, yet\nprior work shows that teams may abandon them for alternatives that are easier\nto maintain. While prior work shows why downgrades are performed, the\nimplications of downgrades remain largely unexplored. In this paper, we\ndescribe a case study of the Kubernetes project, focusing on its downgrade from\nan artifact-based build tool (Bazel) to a language-specific solution (Go\nBuild). We reproduce and analyze the full and incremental builds of change sets\nduring the downgrade period. On the one hand, we find that Bazel builds are\nfaster than Go Build, completing full builds in 23.06-38.66 up to 75.19 impose\na larger memory footprint than Go Build of 81.42-351.07 respectively. Bazel\nbuilds also impose a greater CPU load at parallelism settings above eight for\nfull builds and above one for incremental builds. We estimate that downgrading\nfrom Bazel can increase CI resource costs by up to 76 explore whether our\nobservations generalize by replicating our Kubernetes study on four other\nprojects that also downgraded from Bazel to older build tools. We observe that\nwhile build time penalties decrease, Bazel consistently consumes more memory.\nWe conclude that abandoning artifact-based build tools, despite perceived\nmaintainability benefits, tends to incur considerable performance costs for\nlarge projects. Our observations may help stakeholders to balance trade-offs in\nbuild tool adoption"}
{"id": "2510.20121", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.20121", "abs": "https://arxiv.org/abs/2510.20121", "authors": ["Carlos J. Fernandez-Candel", "Jesus Garcia-Molina", "Francisco Javier Bermudez Ruiz", "Jose Ramon Hoyos Barcelo", "Diego Sevilla Ruiz", "Benito Jose Cuesta Viera"], "title": "Developing a Model-Driven Reengineering Approach for Migrating PL/SQL Triggers to Java: A Practical Experience", "comment": "31 pages, 22 figures", "summary": "Model-driven software engineering (MDE) techniques are not only useful in\nforward engineering scenarios, but can also be successfully applied to evolve\nexisting systems. RAD (Rapid Application Development) platforms emerged in the\nnineties, but the success of modern software technologies motivated that a\nlarge number of enterprises tackled the migration of their RAD applications,\nsuch as Oracle Forms. Our research group has collaborated with a software\ncompany in developing a solution to migrate PL/SQL monolithic code on Forms\ntriggers and program units to Java code separated in several tiers.\n  Our research focused on the model-driven reengineering process applied to\ndevelop the migration tool for the conversion of PL/SQL code to Java. Legacy\ncode is represented in form of KDM (Knowledge-Discovery Metamodel) models. In\nthis paper, we propose a software process to implement a model-driven\nre-engineering. This process integrates a TDD-like approach to incrementally\ndevelop model transformations with three kinds of validations for the generated\ncode. The implementation and validation of the re-engineering approach are\nexplained in detail, as well as the evaluation of some issues related with the\napplication of MDE."}
{"id": "2510.20211", "categories": ["cs.SE", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.20211", "abs": "https://arxiv.org/abs/2510.20211", "authors": ["Zhenning Yang", "Hui Guan", "Victor Nicolet", "Brandon Paulsen", "Joey Dodds", "Daniel Kroening", "Ang Chen"], "title": "Automated Cloud Infrastructure-as-Code Reconciliation with AI Agents", "comment": null, "summary": "Cloud infrastructure is managed through a mix of interfaces -- traditionally,\ncloud consoles, command-line interfaces (CLI), and SDKs are the tools of\nchoice. Recently, Infrastructure-as-Code/IaC frameworks (e.g., Terraform) have\nquickly gained popularity. Unlike conventional tools, IaC~frameworks encode the\ninfrastructure in a \"source-of-truth\" configuration. They are capable of\nautomatically carrying out modifications to the cloud -- deploying, updating,\nor destroying resources -- to bring the actual infrastructure into alignment\nwith the IaC configuration. However, when IaC is used alongside consoles, CLIs,\nor SDKs, it loses visibility into external changes, causing infrastructure\ndrift, where the configuration becomes outdated, and later IaC operations may\nundo valid updates or trigger errors.\n  We present NSync, an automated system for IaC reconciliation that propagates\nout-of-band changes back into the IaC program. Our key insight is that\ninfrastructure changes eventually all occur via cloud API invocations -- the\nlowest layer for cloud management operations. NSync gleans insights from API\ntraces to detect drift (i.e., non-IaC changes) and reconcile it (i.e., update\nthe IaC configuration to capture the changes). It employs an agentic\narchitecture that leverages LLMs to infer high-level intents from noisy API\nsequences, synthesize targeted IaC updates using specialized tools, and\ncontinually improve through a self-evolving knowledge base of past\nreconciliations. We further introduce a novel evaluation pipeline for injecting\nrealistic drifts into cloud infrastructure and assessing reconciliation\nperformance. Experiments across five real-world Terraform projects and 372\ndrift scenarios show that NSync outperforms the baseline both in terms of\naccuracy (from 0.71 to 0.97 pass@3) and token efficiency (1.47$\\times$\nimprovement)."}
{"id": "2510.20340", "categories": ["cs.SE", "cs.CR"], "pdf": "https://arxiv.org/pdf/2510.20340", "abs": "https://arxiv.org/abs/2510.20340", "authors": ["Serena Cofano", "Daniel Williams", "Aman Sharma", "Martin Monperrus"], "title": "Classport: Designing Runtime Dependency Introspection for Java", "comment": null, "summary": "Runtime introspection of dependencies, i.e., the ability to observe which\ndependencies are currently used during program execution, is fundamental for\nSoftware Supply Chain security. Yet, Java has no support for it. We solve this\nproblem with Classport, a system that embeds dependency information into Java\nclass files, enabling the retrieval of dependency information at runtime. We\nevaluate Classport on six real-world projects, demonstrating the feasibility in\nidentifying dependencies at runtime. Runtime dependency introspection with\nClassport opens important avenues for runtime integrity checking."}
{"id": "2510.20389", "categories": ["cs.SE", "cs.DC", "D.m"], "pdf": "https://arxiv.org/pdf/2510.20389", "abs": "https://arxiv.org/abs/2510.20389", "authors": ["Bjorn Remseth"], "title": "Symmetry in Software Platforms as an Architectural Principle", "comment": "Working paper, 11 pages", "summary": "Software platforms often act as structure preserving systems. They provide\nconsistent interfaces and behaviors that remain stable under specific\ntransformations that we denote as symmetries. This paper explores the idea that\narchitectural robustness emerges from enforcing such structural regularities"}
{"id": "2510.20403", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.20403", "abs": "https://arxiv.org/abs/2510.20403", "authors": ["Santiago Gil", "Ecem E. Baş", "Christian D. Jensen", "Sebastian Engelsgaard", "Giuseppe Abbiati", "Cláudio Gomes"], "title": "FMI-Based Distributed Co-Simulation with Enhanced Security and Intellectual Property Safeguards", "comment": "6 pages, Proceedings of the 2025 Annual Modeling and Simulation\n  Conference (ANNSIM)", "summary": "Distributed co-simulation plays a key role in enabling collaborative modeling\nand simulation by different stakeholders while protecting their Intellectual\nProperty (IP). Although IP protection is provided implicitly by co-simulation,\nthere is no consensus in the guidelines to conduct distributed co-simulation of\ncontinuous-time or hybrid systems with no exposure to potential hacking\nattacks. We propose an approach for distributed co-simulation on top of UniFMU\nwith enhanced cybersecurity and IP protection mechanisms, ensuring that the\nconnection is initiated by the client and the models and binaries live on\ntrusted platforms. We showcase the functionality of this approach using two\nco-simulation demos in four different network settings and analyze the\ntrade-off between IP-protected distribution and performance efficiency in these\nsettings."}
{"id": "2510.20514", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.20514", "abs": "https://arxiv.org/abs/2510.20514", "authors": ["Lea Salome Brugger", "Xavier Denis", "Peter Müller"], "title": "Toward Practical Deductive Verification: Insights from a Qualitative Survey in Industry and Academia", "comment": null, "summary": "Deductive verification is an effective method to ensure that a given system\nexposes the intended behavior. In spite of its proven usefulness and\nfeasibility in selected projects, deductive verification is still not a\nmainstream technique. To pave the way to widespread use, we present a study\ninvestigating the factors enabling successful applications of deductive\nverification and the underlying issues preventing broader adoption. We\nconducted semi-structured interviews with 30 practitioners of verification from\nboth industry and academia and systematically analyzed the collected data\nemploying a thematic analysis approach. Beside empirically confirming familiar\nchallenges, e.g., the high level of expertise needed for conducting formal\nproofs, our data reveal several underexplored obstacles, such as proof\nmaintenance, insufficient control over automation, and usability concerns. We\nfurther use the results from our data analysis to extract enablers and barriers\nfor deductive verification and formulate concrete recommendations for\npractitioners, tool builders, and researchers, including principles for\nusability, automation, and integration with existing workflows."}
{"id": "2510.20521", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.20521", "abs": "https://arxiv.org/abs/2510.20521", "authors": ["YingJian Xiao", "RongQun Hu", "WeiWei Gong", "HongWei Li", "AnQuan Jie"], "title": "Large Language Models for Fault Localization: An Empirical Study", "comment": "in Chinese language", "summary": "Large language models (LLMs) have demonstrated remarkable capabilities in\ncode-related tasks, particularly in automated program repair. However, the\neffectiveness of such repairs is highly dependent on the performance of\nupstream fault localization, for which comprehensive evaluations are currently\nlacking. This paper presents a systematic empirical study on LLMs in the\nstatement-level code fault localization task. We evaluate representative\nopen-source models (Qwen2.5-coder-32b-instruct, DeepSeek-V3) and closed-source\nmodels (GPT-4.1 mini, Gemini-2.5-flash) to assess their fault localization\ncapabilities on the HumanEval-Java and Defects4J datasets. The study\ninvestigates the impact of different prompting strategies--including standard\nprompts, few-shot examples, and chain-of-reasoning--on model performance, with\na focus on analysis across accuracy, time efficiency, and economic cost\ndimensions. Our experimental results show that incorporating bug report context\nsignificantly enhances model performance. Few-shot learning shows potential for\nimprovement but exhibits noticeable diminishing marginal returns, while\nchain-of-thought reasoning's effectiveness is highly contingent on the model's\ninherent reasoning capabilities. This study not only highlights the performance\ncharacteristics and trade-offs of different models in fault localization tasks,\nbut also offers valuable insights into the strengths of current LLMs and\nstrategies for improving fault localization effectiveness."}
{"id": "2510.20679", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.20679", "abs": "https://arxiv.org/abs/2510.20679", "authors": ["Jonas Klauke", "Tom Ohlmer", "Stefan Schott", "Serena Elisa Ponta", "Wolfram Fischer", "Eric Bodden"], "title": "A Soundness and Precision Benchmark for Java Debloating Tools", "comment": "Preprint - accepted at the ACM Workshop on Software Supply Chain\n  Offensive Research and Ecosystem Defenses (SCORED '25)", "summary": "Modern software development reuses code by importing libraries as\ndependencies. Software projects typically include an average of 36\ndependencies, with 80% being transitive, meaning they are dependencies of\ndependencies. Recent research indicates that only 24.9% of these dependencies\nare required at runtime, and even within those, many program constructs remain\nunused, adding unnecessary code to the project. This has led to the development\nof debloating tools that remove unnecessary dependencies and program constructs\nwhile balancing precision by eliminating unused constructs and soundness by\npreserving all required constructs. To systematically evaluate this trade-off,\nwe developed Deblometer, a micro-benchmark consisting of 59 test cases designed\nto assess support for various Java language features in debloating tools. Each\ntest case includes a manually curated ground truth specifying necessary and\nbloated classes, methods, and fields, enabling precise measurement of soundness\nand precision. Using Deblometer, we evaluated three popular Java debloating\ntools: Deptrim, JShrink, and ProGuard. Our evaluation reveals that all tools\nremove required program constructs, which results in changed semantics or\nexecution crashes. In particular, the dynamic class loading feature introduces\nunsoundness in all evaluated tools. Our comparison shows that Deptrim retains\nmore bloated constructs, while ProGuard removes more required constructs.\nJShrink's soundness is significantly affected by limited support for\nannotations, which leads to corrupted debloated artifacts. These soundness\nissues highlight the need to improve debloating tools to ensure stable and\nreliable debloated software."}
{"id": "2510.20692", "categories": ["cs.SE", "cs.AI", "cs.FL", "D.4.6; D.2.4; I.2.2; I.2.7; F.3.1; F.4.3"], "pdf": "https://arxiv.org/pdf/2510.20692", "abs": "https://arxiv.org/abs/2510.20692", "authors": ["Adarsh Vatsa", "Bethel Hall", "William Eiers"], "title": "Exploring Large Language Models for Access Control Policy Synthesis and Summarization", "comment": "20 pages, 7 figures", "summary": "Cloud computing is ubiquitous, with a growing number of services being hosted\non the cloud every day. Typical cloud compute systems allow administrators to\nwrite policies implementing access control rules which specify how access to\nprivate data is governed. These policies must be manually written, and due to\ntheir complexity can often be error prone. Moreover, existing policies often\nimplement complex access control specifications and thus can be difficult to\nprecisely analyze in determining their behavior works exactly as intended.\nRecently, Large Language Models (LLMs) have shown great success in automated\ncode synthesis and summarization. Given this success, they could potentially be\nused for automatically generating access control policies or aid in\nunderstanding existing policies. In this paper, we explore the effectiveness of\nLLMs for access control policy synthesis and summarization. Specifically, we\nfirst investigate diverse LLMs for access control policy synthesis, finding\nthat: although LLMs can effectively generate syntactically correct policies,\nthey have permissiveness issues, generating policies equivalent to the given\nspecification 45.8% of the time for non-reasoning LLMs, and 93.7% of the time\nfor reasoning LLMs. We then investigate how LLMs can be used to analyze\npolicies by introducing a novel semantic-based request summarization approach\nwhich leverages LLMs to generate a precise characterization of the requests\nallowed by a policy. Our results show that while there are significant hurdles\nin leveraging LLMs for automated policy generation, LLMs show promising results\nwhen combined with symbolic approaches in analyzing existing policies."}
