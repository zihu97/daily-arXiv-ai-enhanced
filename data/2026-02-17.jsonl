{"id": "2602.13434", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2602.13434", "abs": "https://arxiv.org/abs/2602.13434", "authors": ["Maccoy Merrell", "Daniel Puckett", "Gino Chacon", "Jeffrey Stuecheli", "Stavros Kalafatis", "Paul V. Gratz"], "title": "ORAP: Optimized Row Access Prefetching for Rowhammer-mitigated Memory", "comment": "15 pages, 19 figures", "summary": "Rowhammer is a well-studied DRAM phenomenon wherein multiple activations to a given row can cause bit flips in adjacent rows. Many mitigation techniques have been introduced to address Rowhammer, with some support being incorporated into the JEDEC DDR5 standard for per-row-activation-counter (PRAC) and refresh-management (RFM) systems. Mitigation schemes built on these mechanisms claim to have various levels of area, power, and performance overheads. To date the evaluation of existing mitigation schemes typically neglects the impact of other memory system components such as hardware prefetchers. Nearly all modern systems incorporate hardware prefetching and these can significantly improve processor performance through speculative cache population. These prefetchers induce higher numbers of downstream memory requests and increase DRAM activation rates. The performance overhead of Rowhammer mitigations are tied directly to memory access patterns, exposing both hardware prefetchers and Rowhammer mitigations to cross-interaction. We find that the performance improvement provided by prior-work hardware prefetchers is often severely impacted by Rowhammer mitigations. In effect, much of the benefit of speculative memory references from prefetching lies in accelerating and reordering DRAM references in ways that trigger mitigations, significantly reducing the benefits of prefetching. This work proposes the Optimized Row Access Prefetcher (ORAP), leveraging last-level-cache (LLC) space to cache large portions of DRAM rowbuffer contents to reduce the need for future activations. Working with the state-of-the-art Berti prefetcher, ORAP reduces DRAM activation rates by 51.3% and achieves a 4.6% speedup over the prefetcher configuration of Berti and SPP-PPF when prefetching in an RFM-mitigated memory system. Under PRAC mitigations, ORAP reduces energy overheads by 11.8%."}
{"id": "2602.13825", "categories": ["cs.AR", "physics.app-ph"], "pdf": "https://arxiv.org/pdf/2602.13825", "abs": "https://arxiv.org/abs/2602.13825", "authors": ["Paras Tiwari", "Narendra Singh Dhakad", "Shalu Rani", "Sanjay Kumar", "Themis Prodromakis"], "title": "Implementation and Performance Evaluation of CMOS-integrated Memristor-driven Flip-flop Circuits", "comment": null, "summary": "In this work, we report implementation and performance evaluation of memristor-driven fundamental logic gates, including NOT, AND, NAND, OR, NOR, and XOR, and novel and optimized design of the sequential logic circuits, such as D flip-flop, T-flip-flop, JK-flip-flop, and SR-flip-flop. The design, implementation, and optimization of these logic circuits were performed in SPECTRE in Cadence Virtuoso and integrated with 90 nm CMOS technology node. Additionally, we discuss an optimized design of memristor-driven logic gates and sequential logic circuits, and draw a comparative analysis with the other reported state-of-the-art work on sequential circuits. Moreover, the utilized memristor framework was experimentally pre-validated with the experimental data of Y2O3-based memristive devices, which shows significantly low values of variability during switching in both device-to-device (D2D) and cycle-to-cycle (C2C) operation. The performance metrics were calculated in terms of area, power, and delay of these sequential circuits and were found to be reduced by more than ~24%, 60%, and 58%, respectively, as compared to the other state-of-the-art work on sequential circuits. Therefore, the implemented memristor-based design significantly improves the performance of various logic designs, which makes it more area and power-efficient and shows the potential of memristor in designing various low-power, low-cost, ultrafast, and compact circuits."}
{"id": "2602.14262", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2602.14262", "abs": "https://arxiv.org/abs/2602.14262", "authors": ["Siddhartha Raman Sundara Raman", "Jaydeep P. Kulkarni"], "title": "ABI: A tightly integrated, unified, sparsity-aware, reconfigurable, compute near-register file/cache GPU architecture with light-weight softmax for deep learning, linear algebra, and Ising compute", "comment": null, "summary": "We present a tightly integrated and unified near-memory GPU architecture that delivers 6 to 16 times speedup and 6 to 13 times energy savings across Convolutional Neural Networks, Graph Convolutional Networks, Linear Programming, Large Language Models, and Ising workloads compared to MIAOW GPU. The design includes a custom sparsity-aware near-memory circuit providing about 1.5 times energy savings, and a lightweight softmax circuit providing about 1.6 times energy savings. The architecture supports reconfigurable compute up to INT16 with dynamic resolution updates and scales efficiently across problem sizes. ABI-enabled MI300 and Blackwell systems achieve about 4.5 times speedup over baseline MI300 and Blackwell."}
{"id": "2602.14393", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2602.14393", "abs": "https://arxiv.org/abs/2602.14393", "authors": ["Zongle Huang", "Hongyang Jia", "Kaiwei Zou", "Yongpan Liu"], "title": "Scope: A Scalable Merged Pipeline Framework for Multi-Chip-Module NN Accelerators", "comment": "Accepted in ASP-DAC 2026", "summary": "Neural network (NN) accelerators with multi-chip-module (MCM) architectures enable integration of massive computation capability; however, they face challenges of computing resource underutilization and off-chip communication overheads. Traditional parallelization schemes for NN inference on MCM architectures, such as intra-layer parallelism and inter-layer pipelining, show incompetency in breaking through both challenges, limiting the scalability of MCM architectures.\n  We observed that existing works typically deploy layers separately rather than considering them jointly. This underexploited dimension leads to compromises between system computation and communication, thus hindering optimal utilization, especially as hardware/software scale. To address this limitation, we propose Scope, a merged pipeline framework incorporating this overlooked multi-layer dimension, thereby achieving improved throughput and scalability by relaxing tradeoffs between computation, communication and memory costs. This new dimension, however, adds to the complexity of design space exploration (DSE). To tackle this, we develop a series of search algorithms that achieves exponential-to-linear complexity reduction, while identifying solutions that rank in the top 0.05% of performance. Experiments show that Scope achieves up to 1.73x throughput improvement while maintaining similar energy consumption for ResNet-152 inference compared to state-of-the-art approaches."}
{"id": "2602.13668", "categories": ["cs.PF"], "pdf": "https://arxiv.org/pdf/2602.13668", "abs": "https://arxiv.org/abs/2602.13668", "authors": ["Ioannis Balatsos", "Athanasios Liakos", "Panagiotis Karakostas", "Tao Song", "Vassilios Pantazopoulos", "Christos Papalitsas"], "title": "Efficient Data-Driven Production Scheduling in Pharmaceutical Manufacturing", "comment": null, "summary": "This paper develops a data-driven, constraint-based optimization framework for a complex industrial job shop scheduling problem variant in pharmaceutical manufacturing. The formulation captures fixed routings and designated machines, explicit resource calendars with weekends and planned maintenance, and campaign sequencing through sequence-dependent cleaning times derived from site tables. The model is implemented with an open source constraint solver and evaluated on deterministic snapshots from a solid oral dosage facility under three objective formulations: makespan, makespan plus total tardiness, and makespan plus average tardiness. On three industrial instances of increasing size (10, 30, and 84 jobs) the proposed schedules dominate reference plans that solve a simplified variant without the added site rules. Makespan reductions reach \\(88.1\\%\\), \\(77.6\\%\\), and \\(54.9\\%\\) and total tardiness reductions reach \\(72.1\\%\\), \\(58.7\\%\\), and \\(18.2\\%\\), respectively. The composite objectives further decrease late job counts with negligible makespan change on the smaller instances and a modest increase on the largest instance. Optimality is proven on the small case, with relative gaps of \\(0.77\\%\\) and \\(14.92\\%\\) on the medium and large cases under a fixed time limit. The results show that a compact constraint programming formulation can deliver feasible, transparent schedules that respect site rules while improving adherence to due dates on real industrial data."}
{"id": "2602.13789", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2602.13789", "abs": "https://arxiv.org/abs/2602.13789", "authors": ["Zhengyan Chu"], "title": "TEG: Exascale Cluster Governance via Non-Equilibrium Thermodynamics and Langevin Dynamics", "comment": "7 pages", "summary": "As cloud computing scales toward the Exascale regime ($10^5+$ nodes), the prevailing \"Newtonian\" orchestration paradigm -- exemplified by Kubernetes -- approaches fundamental physical limits. The centralized, deterministic scheduling model suffers from $O(N)$ latency scaling, \"Head-of-Line\" blocking, and thermodynamic blindness, rendering it incapable of managing the stochastic chaos of next-generation AI workloads. This paper proposes a paradigm shift from orchestration to Thermodynamic Governance. We model the compute cluster not as a static state machine, but as a Dissipative Structure far from equilibrium. We introduce TEG (Thermo-Economic Governor), a decentralized architecture that establishes a rigorous topological isomorphism between cluster resource contention and many-body physics. TEG replaces the global scheduler with Langevin Agents that execute Brownian motion on a Holographic Potential Field, reducing decision complexity to $O(1)$. System stability is maintained via a macro-scale Landau Phase Transition mechanism, which modulates global damping (taxation) to physically dissolve deadlocks. Crucially, we enforce Token Evaporation to mirror entropy dissipation, preventing economic inflation and ensuring an open thermodynamic system. We provide formal theoretical analysis proving that: (1) The system converges asymptotically to a Nash Equilibrium via Dual-Number Damping; (2) OOM catastrophic failures are converted into manageable Glassy States via an OS-level Airlock Mutex; and (3) Safety is mathematically guaranteed under high inertia using High-Order Control Barrier Functions (HOCBF). TEG demonstrates that emergent order, rather than deterministic control, is the necessary condition for Exascale scalability."}
{"id": "2602.13377", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2602.13377", "abs": "https://arxiv.org/abs/2602.13377", "authors": ["Taufiqul Islam Khan", "Shaowei Wang", "Haoxiang Zhang", "Tse-Hsun Chen"], "title": "A Survey of Code Review Benchmarks and Evaluation Practices in Pre-LLM and LLM Era", "comment": null, "summary": "Code review is a critical practice in modern software engineering, helping developers detect defects early, improve code quality, and facilitate knowledge sharing. With the rapid advancement of large language models (LLMs), a growing body of work has explored automated support for code review. However, progress in this area is hindered by the lack of a systematic understanding of existing benchmarks and evaluation practices. Current code review datasets are scattered, vary widely in design, and provide limited insight into what review capabilities are actually being assessed. In this paper, we present a comprehensive survey of code review benchmarks spanning both the Pre-LLM and LLM eras (2015--2025). We analyze 99 research papers (58 Pre-LLM era and 41 LLM era) and extract key metadata, including datasets, evaluation metrics, data sources, and target tasks. Based on this analysis, we propose a multi-level taxonomy that organizes code review research into five domains and 18 fine-grained tasks. Our study reveals a clear shift toward end-to-end generative peer review, increasing multilingual coverage, and a decline in standalone change understanding tasks. We further identify limitations of current benchmarks and outline future directions, including broader task coverage, dynamic runtime evaluation, and taxonomy-guided fine-grained assessment. This survey provides a structured foundation for developing more realistic and comprehensive benchmarks for LLM-based code review."}
{"id": "2602.14107", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2602.14107", "abs": "https://arxiv.org/abs/2602.14107", "authors": ["Yuze Liu", "Shibo Chu", "Tiehua Zhang", "Hao Zhou", "Zhishu Shen", "Jinze Wang", "Jianzhong Qi", "Feng Xia"], "title": "ML-ECS: A Collaborative Multimodal Learning Framework for Edge-Cloud Synergies", "comment": null, "summary": "Edge-cloud synergies provide a promising paradigm for privacy-preserving deployment of foundation models, where lightweight on-device models adapt to domain-specific data and cloud-hosted models coordinate knowledge sharing. However, in real-world edge environments, collaborative multimodal learning is challenged by modality heterogeneity (different modality combinations across domains) and model-structure heterogeneity (different modality-specific encoders/fusion modules. To address these issues, we propose ML-ECS, a collaborative multimodal learning framework that enables joint training between a server-based model and heterogeneous edge models. This framework consists of four components: (1) cross-modal contrastive learning (CCL) to align modality representations in a shared latent space, (2) adaptive multimodal tuning (AMT) to preserve domain-specific knowledge from local datasets, (3) modality-aware model aggregation (MMA) to robustly aggregate while mitigating noise caused by missing modalities, and (4) SLM-enhanced CCL (SE-CCL) to facilitate bidirectional knowledge transfer between cloud and edge. Experimental results on various multimodal tasks show that \\pname consistently outperform state-of-the-art baselines under varying modality availability, achieving improvements of 5.44% to 12.08% in Rouge-LSum and improving both client- and server-side performance. In addition, by communicating only low-rank LoRA parameters and fused representations, ML-ECS achieves high communication efficiency, requiring only 0.65% of the total parameter volume."}
{"id": "2602.13400", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2602.13400", "abs": "https://arxiv.org/abs/2602.13400", "authors": ["Tanner Wright", "Adams Chen", "Gema Rodríguez-Pérez"], "title": "InEx-Bug: A Human Annotated Dataset of Intrinsic and Extrinsic Bugs in the NPM Ecosystem", "comment": null, "summary": "Understanding the causes of software defects is essential for reliable software maintenance and ecosystem stability. However, existing bug datasets do not distinguish between issues originating within a project from those caused by external dependencies or environmental factors. In this paper we present InEx-Bug, a manually annotated dataset of 377 GitHub issues from 103 NPM repositories, categorizing issues as Intrinsic (internal defect), Extrinsic (dependency/environment issue), Not-a-Bug, or Unknown. Beyond labels, the dataset includes rich temporal and behavioral metadata such as maintainer participation, code changes, and reopening patterns. Analyses show Intrinsic bugs resolve faster (median 8.9 vs 10.2 days), are close more often (92% vs 78%), and require code changes more frequently (57% vs 28%) compared to Extrinsic bugs. While Extrinsic bugs exhibit higher reopen rates (12% vs 4%) and delayed recurrence (median 157 vs 87 days). The dataset provides a foundation for further studying Intrinsic and Extrinsic defects in the NPM ecosystem."}
{"id": "2602.13200", "categories": ["cs.NI", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.13200", "abs": "https://arxiv.org/abs/2602.13200", "authors": ["Andrii Grekhov", "Volodymyr Kharchenko", "Vasyl Kondratiuk"], "title": "Traffic Simulation in Ad Hoc Network of Flying UAVs with Generative AI Adaptation", "comment": "15 pages, 10 figures", "summary": "The purpose of this paper is to model traffic in Ad Hoc network of Unmanned Aerial Vehicles and demonstrate a way for adapting communication channel using Artificial Intelligence. The modeling was based on the original model of Ad Hoc network including 20 Unmanned Aerial Vehicles. The dependences of packet loss on the packet size for different transmission powers, on the packet size for different frequencies, on Unmanned Aerial Vehicles flight area and on the number of Unmanned Aerial Vehicles were obtained and analyzed. The implementation of adaptive data transmission is presented in the program code. The dependences of packet loss, power and transaction size on time during Artificial Intelligence adaptation are shown."}
{"id": "2602.13309", "categories": ["cs.MA", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.13309", "abs": "https://arxiv.org/abs/2602.13309", "authors": ["Yexin Li", "Jinjin Guo", "Haoyu Zhang", "Yuhan Zhao", "Yiwen Sun", "Zihao Jiao"], "title": "Adaptive Value Decomposition: Coordinating a Varying Number of Agents in Urban Systems", "comment": null, "summary": "Multi-agent reinforcement learning (MARL) provides a promising paradigm for coordinating multi-agent systems (MAS). However, most existing methods rely on restrictive assumptions, such as a fixed number of agents and fully synchronous action execution. These assumptions are often violated in urban systems, where the number of active agents varies over time, and actions may have heterogeneous durations, resulting in a semi-MARL setting. Moreover, while sharing policy parameters among agents is commonly adopted to improve learning efficiency, it can lead to highly homogeneous actions when a subset of agents make decisions concurrently under similar observations, potentially degrading coordination quality. To address these challenges, we propose Adaptive Value Decomposition (AVD), a cooperative MARL framework that adapts to a dynamically changing agent population. AVD further incorporates a lightweight mechanism to mitigate action homogenization induced by shared policies, thereby encouraging behavioral diversity and maintaining effective cooperation among agents. In addition, we design a training-execution strategy tailored to the semi-MARL setting that accommodates asynchronous decision-making when some agents act at different times. Experiments on real-world bike-sharing redistribution tasks in two major cities, London and Washington, D.C., demonstrate that AVD outperforms state-of-the-art baselines, confirming its effectiveness and generalizability."}
{"id": "2602.14302", "categories": ["cs.DC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.14302", "abs": "https://arxiv.org/abs/2602.14302", "authors": ["Chunlin Tian", "Kahou Tam", "Yebo Wu", "Shuaihang Zhong", "Li Li", "Nicholas D. Lane", "Chengzhong Xu"], "title": "Floe: Federated Specialization for Real-Time LLM-SLM Inference", "comment": "Accepted by IEEE Transactions on Parallel and Distributed Systems", "summary": "Deploying large language models (LLMs) in real-time systems remains challenging due to their substantial computational demands and privacy concerns. We propose Floe, a hybrid federated learning framework designed for latency-sensitive, resource-constrained environments. Floe combines a cloud-based black-box LLM with lightweight small language models (SLMs) on edge devices to enable low-latency, privacy-preserving inference. Personal data and fine-tuning remain on-device, while the cloud LLM contributes general knowledge without exposing proprietary weights. A heterogeneity-aware LoRA adaptation strategy enables efficient edge deployment across diverse hardware, and a logit-level fusion mechanism enables real-time coordination between edge and cloud models. Extensive experiments demonstrate that Floe enhances user privacy and personalization. Moreover, it significantly improves model performance and reduces inference latency on edge devices under real-time constraints compared with baseline approaches."}
{"id": "2602.13574", "categories": ["cs.SE", "cs.CR"], "pdf": "https://arxiv.org/pdf/2602.13574", "abs": "https://arxiv.org/abs/2602.13574", "authors": ["Haoyu Li", "Xijia Che", "Yanhao Wang", "Xiaojing Liao", "Luyi Xing"], "title": "Execution-State-Aware LLM Reasoning for Automated Proof-of-Vulnerability Generation", "comment": "Version 1.0 (13 pages, 7 figures)", "summary": "Proof-of-Vulnerability (PoV) generation is a critical task in software security, serving as a cornerstone for vulnerability validation, false positive reduction, and patch verification. While directed fuzzing effectively drives path exploration, satisfying complex semantic constraints remains a persistent bottleneck in automated exploit generation. Large Language Models (LLMs) offer a promising alternative with their semantic reasoning capabilities; however, existing LLM-based approaches lack sufficient grounding in concrete execution behavior, limiting their ability to generate precise PoVs.\n  In this paper, we present DrillAgent, an agentic framework that reformulates PoV generation as an iterative hypothesis-verification-refinement process. To bridge the gap between static reasoning and dynamic execution, DrillAgent synergizes LLM-based semantic inference with feedback from concrete program states. The agent analyzes the target code to hypothesize inputs, observes execution behavior, and employs a novel mechanism to translate low-level execution traces into source-level constraints. This closed-loop design enables the agent to incrementally align its input generation with the precise requirements of the vulnerability. We evaluate DrillAgent on SEC-bench, a large-scale benchmark of real-world C/C++ vulnerabilities. Experimental results show that DrillAgent substantially outperforms state-of-the-art LLM agent baselines under fixed budget constraints, solving up to 52.8% more CVE tasks than the best-performing baseline. These results highlight the necessity of execution-state-aware reasoning for reliable PoV generation in complex software systems."}
{"id": "2602.13201", "categories": ["cs.NI"], "pdf": "https://arxiv.org/pdf/2602.13201", "abs": "https://arxiv.org/abs/2602.13201", "authors": ["Cunlai Pu", "Fangrui Wu", "Zhe Wang", "Xiangbo Shu"], "title": "CLF-ULP: Cross-Layer Fusion-Based Link Prediction in Dynamic Multiplex UAV Networks", "comment": "12 pages, 5 figures", "summary": "In complex Unmanned Aerial Vehicle (UAV) networks, UAVs can establish dynamic and heterogeneous links with one another for various purposes, such as communication coverage, collective sensing, and task collaboration. These interactions give rise to dynamic multiplex UAV networks, where each layer represents a distinct type of interaction among UAVs. Understanding how such links form and evolve is both of theoretical interest and of practical importance for the control and maintenance of networked UAV systems. In this paper, we first develop a dynamic multiplex network model for UAV networks to characterize their dynamic and heterogeneous link properties. We then propose a cross-layer fusion-based deep learning model, termed CLF-ULP, to predict future inter-UAV links based on historical topology data. CLF-ULP incorporates graph attention networks to extract topological features within each layer and perform a cross-layer attention fusion to capture inter-layer dependencies. Furthermore, a shared-parameter long short-term memory network is employed to model the temporal evolution of each layer. To improve embedding quality and link prediction performance, we develop a joint loss function that considers both intra-layer and inter-layer UAV adjacency. Extensive experiments on simulated UAV datasets under diverse mobility patterns demonstrate that CLF-ULP achieves state-of-the-art performance in predicting links within dynamic multiplex UAV networks."}
{"id": "2602.13312", "categories": ["cs.MA", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.13312", "abs": "https://arxiv.org/abs/2602.13312", "authors": ["Yishu Wang", "Wei Liu", "Yifan Li", "Shengxiang Xu", "Xujie Yuan", "Ran Li", "Yuyu Luo", "Jia Zhu", "Shimin Di", "Min-Ling Zhang", "Guixiang Li"], "title": "PeroMAS: A Multi-agent System of Perovskite Material Discovery", "comment": null, "summary": "As a pioneer of the third-generation photovoltaic revolution, Perovskite Solar Cells (PSCs) are renowned for their superior optoelectronic performance and cost potential. The development process of PSCs is precise and complex, involving a series of closed-loop workflows such as literature retrieval, data integration, experimental design, and synthesis. However, existing AI perovskite approaches focus predominantly on discrete models, including material design, process optimization,and property prediction. These models fail to propagate physical constraints across the workflow, hindering end-to-end optimization. In this paper, we propose a multi-agent system for perovskite material discovery, named PeroMAS. We first encapsulated a series of perovskite-specific tools into Model Context Protocols (MCPs). By planning and invoking these tools, PeroMAS can design perovskite materials under multi-objective constraints, covering the entire process from literature retrieval and data extraction to property prediction and mechanism analysis. Furthermore, we construct an evaluation benchmark by perovskite human experts to assess this multi-agent system. Results demonstrate that, compared to single Large Language Model (LLM) or traditional search strategies, our system significantly enhances discovery efficiency. It successfully identified candidate materials satisfying multi-objective constraints. Notably, we verify PeroMAS's effectiveness in the physical world through real synthesis experiments."}
{"id": "2602.14516", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2602.14516", "abs": "https://arxiv.org/abs/2602.14516", "authors": ["Wenhao He", "Youhe Jiang", "Penghao Zhao", "Quanqing Xu", "Eiko Yoneki", "Bin Cui", "Fangcheng Fu"], "title": "Efficient Multi-round LLM Inference over Disaggregated Serving", "comment": null, "summary": "With the rapid evolution of Large Language Models (LLMs), multi-round workflows, such as autonomous agents and iterative retrieval, have become increasingly prevalent. However, this raises hurdles for serving LLMs under prefill-decode (PD) disaggregation, a widely adopted paradigm that separates the compute-bound prefill phase and memory-bound decode phase onto individual resources. Specifically, existing systems overlook the interleaved prefill-decode workload pattern in multi-round inference, leading to sub-optimal handling of the incremental prefill workloads and model deployment for the two phases.\n  In this work, we present AMPD, a brand new disaggregated serving framework for multi-round LLM inference. The core of AMPD is to coordinate the prefill workloads based on real-time workloads by adaptively determining where to carry out these workloads and how they are scheduled, in order to maximize service level objective (SLO) attainment. In addition, we tailor a planning algorithm for our scenario, facilitating the deduction of optimal resource allocation and parallel strategies for the two phases. Empirical results demonstrate that AMPD substantially improves SLO attainment compared to state-of-the-art baselines."}
{"id": "2602.13611", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.13611", "abs": "https://arxiv.org/abs/2602.13611", "authors": ["Xiao He", "Ru Chen", "Jialun Cao"], "title": "From What to How: Bridging User Requirements with Software Development Using Large Language Models", "comment": null, "summary": "Recently, large language models (LLMs) are extensively utilized to enhance development efficiency, leading to numerous benchmarks for evaluating their performance. However, these benchmarks predominantly focus on implementation, overlooking the equally critical aspect of software design. This gap raises two pivotal questions: (1) Can LLMs handle software design? (2) Can LLMs write code following the specific designs? To investigate these questions, this paper proposes DesBench, a design-aware benchmark for evaluating LLMs on three software design-related tasks: design-aware code generation, object-oriented modeling, and the design of acceptance test cases. DesBench comprises 30 manually crafted Java projects that include requirement documents, design models, implementations, and acceptance tests, amounting to a total of 30 design models, 194 Java classes, and 737 test cases. We evaluated seven state-of-the-art LLMs, including three DeepSeek R1, two Qwen2.5, and two GPT models, using DesBench. The results reveal that LLMs remain significantly challenged by the intricacies of software design: (1) For code generation, LLMs struggle to produce correct implementations when provided with only high-level or no designs. (2) In object-oriented modeling, while LLMs can accurately identify objects and classes, they face challenges in defining operations and inter-class relationships. (3) Acceptance test cases generated by LLMs from functional requirements achieve code coverage quality comparable to those written by humans. Our research highlights the current limitations of LLMs in managing software design and calls for further investigation into new design methodologies and languages suitable for LLM-based development."}
{"id": "2602.13202", "categories": ["cs.NI"], "pdf": "https://arxiv.org/pdf/2602.13202", "abs": "https://arxiv.org/abs/2602.13202", "authors": ["Sumita Majhi", "G Vasantha Reddy", "Pinaki Mitra"], "title": "Enhancing NOMA Handover Performance Using Hybrid AI-Driven Modulated Deterministic Sequences", "comment": null, "summary": "Non-Orthogonal Multiple Access (NOMA) is an information-theoretical approach used in 5G networks to improve spectral efficiency, but it is prone to interference during handovers. In this work, we propose a hybrid method that combines Gold-Walsh modulated sequences with Deep Q-Networks (DQN) to intelligently manage interference during NOMA handovers. This method optimizes sequence selection and power allocation dynamically. As a result, it achieves a 95.2\\% handover success rate, which is an improvement of up to 23.1 percentage points. It also delivers up to 28\\% throughput gain and reduces interference by up to 41\\% in various mobility scenarios. All improvements are statistically significant (\\(p < 0.001\\)). The DQN trains in \\(4{,}200 \\pm 400\\) episodes with a complexity of \\(O(N \\log N + d \\cdot h + \\log B)\\) and can be deployed in real-time."}
{"id": "2602.13353", "categories": ["cs.MA", "cs.GT"], "pdf": "https://arxiv.org/pdf/2602.13353", "abs": "https://arxiv.org/abs/2602.13353", "authors": ["Bhavini Jeloka", "Yue Guan", "Panagiotis Tsiotras"], "title": "Robust Mean-Field Games with Risk Aversion and Bounded Rationality", "comment": "25 pages, 2 figures", "summary": "Recent advances in mean-field game literature enable the reduction of large-scale multi-agent problems to tractable interactions between a representative agent and a population distribution. However, existing approaches typically assume a fixed initial population distribution and fully rational agents, limiting robustness under distributional uncertainty and cognitive constraints. We address these limitations by introducing risk aversion with respect to the initial population distribution and by incorporating bounded rationality to model deviations from fully rational decision-making agents. The combination of these two elements yields a new and more general equilibrium concept, which we term the mean-field risk-averse quantal response equilibrium (MF-RQE). We establish existence results and prove convergence of fixed-point iteration and fictitious play to MF-RQE. Building on these insights, we develop a scalable reinforcement learning algorithm for scenarios with large state-action spaces. Numerical experiments demonstrate that MF-RQE policies achieve improved robustness relative to classical mean-field approaches that optimize expected cumulative rewards under a fixed initial distribution and are restricted to entropy-based regularizers."}
{"id": "2602.14704", "categories": ["cs.DC", "cs.DS"], "pdf": "https://arxiv.org/pdf/2602.14704", "abs": "https://arxiv.org/abs/2602.14704", "authors": ["Zong Yu Lee", "Xueyan Tang"], "title": "Evaluation of Dynamic Vector Bin Packing for Virtual Machine Placement", "comment": "Extended version of a paper that will appear in IEEE IPDPS 2026 conference", "summary": "Virtual machine placement is a crucial challenge in cloud computing for efficiently utilizing physical machine resources in data centers. Virtual machine placement can be formulated as a MinUsageTime Dynamic Vector Bin Packing (DVBP) problem, aiming to minimize the total usage time of the physical machines. This paper evaluates state-of-the-art MinUsageTime DVBP algorithms in non-clairvoyant, clairvoyant and learning-augmented online settings, where item durations (virtual machine lifetimes) are unknown, known and predicted, respectively. Besides the algorithms taken from the literature, we also develop several new algorithms or enhancements. Empirical experimentation is carried out with real-world datasets of Microsoft Azure. The insights from the experimental results are discussed to explore the structures of algorithms and promising design elements that work well in practice."}
{"id": "2602.13682", "categories": ["cs.SE", "cs.CR"], "pdf": "https://arxiv.org/pdf/2602.13682", "abs": "https://arxiv.org/abs/2602.13682", "authors": ["Gianpietro Castiglione", "Shahriar Ebrahimi", "Narges Khakpour"], "title": "VeriSBOM: Secure and Verifiable SBOM Sharing Via Zero-Knowledge Proofs", "comment": null, "summary": "A Software Bill of Materials (SBOM) is a key component for the transparency of software supply chain; it is a structured inventory of the components, dependencies, and associated metadata of a software artifact. However, an SBOM often contain sensitive information that organizations are unwilling to disclose in full to anyone, for two main concerns: technological risks deriving from exposing proprietary dependencies or unpatched vulnerabilities, and business risks, deriving from exposing architectural strategies. Therefore, delivering a plaintext SBOM may result in the disruption of the intellectual property of a company. To address this, we present VeriSBOM, a trustless, selectively disclosed SBOM framework that provides cryptographic verifiability of SBOMs using zero-knowledge proofs. Within VeriSBOM, third parties can validate specific statements about a delivered software. Respectively, VeriSBOM allows independent third parties to verify if a software contains authentic dependencies distributed by official package managers and that the same dependencies satisfy rigorous policy constraints such as the absence of vulnerable dependencies or the adherence with specific licenses models. VeriSBOM leverages a scalable vector commitment scheme together with folding-based proof aggregation to produce succinct zero-knowledge proofs that attest to security and compliance properties while preserving confidentiality. Crucially, the verification process requires no trust in the SBOM publisher beyond the soundness of the underlying primitives, and third parties can independently check proofs against the public cryptographic commitments. We implement VeriSBOM, analyze its security, and evaluate its performance on real-world package registries. The results show that our method enables scalable, privacy-preserving, and verifiable SBOM sharing and validation."}
{"id": "2602.13203", "categories": ["cs.NI", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.13203", "abs": "https://arxiv.org/abs/2602.13203", "authors": ["Vignesh Sriram", "Yuqiao Meng", "Luoxi Tang", "Zhaohan Xi"], "title": "Adversarial Network Imagination: Causal LLMs and Digital Twins for Proactive Telecom Mitigation", "comment": null, "summary": "Telecommunication networks experience complex failures such as fiber cuts, traffic overloads, and cascading outages. Existing monitoring and digital twin systems are largely reactive, detecting failures only after service degradation occurs. We propose Adversarial Network Imagination, a closed-loop framework that integrates a Causal Large Language Model (LLM), a Knowledge Graph, and a Digital Twin to proactively generate, simulate, and evaluate adversarial network failures. The Causal LLM produces structured failure scenarios grounded in network dependencies encoded in the Knowledge Graph. These scenarios are executed within a Digital Twin to measure performance degradation and evaluate mitigation strategies. By iteratively refining scenarios based on simulation feedback, the framework shifts network operations from reactive troubleshooting toward anticipatory resilience analysis."}
{"id": "2602.13370", "categories": ["cs.MA", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2602.13370", "abs": "https://arxiv.org/abs/2602.13370", "authors": ["Karim Ben Khaled", "Davy Monticolo"], "title": "G2CP: A Graph-Grounded Communication Protocol for Verifiable and Efficient Multi-Agent Reasoning", "comment": null, "summary": "Multi-agent systems powered by Large Language Models face a critical challenge: agents communicate through natural language, leading to semantic drift, hallucination propagation, and inefficient token consumption. We propose G2CP (Graph-Grounded Communication Protocol), a structured agent communication language where messages are graph operations rather than free text. Agents exchange explicit traversal commands, subgraph fragments, and update operations over a shared knowledge graph, enabling verifiable reasoning traces and eliminating ambiguity. We validate G2CP within an industrial knowledge management system where specialized agents (Diagnostic, Procedural, Synthesis, and Ingestion) coordinate to answer complex queries. Experimental results on 500 industrial scenarios and 21 real-world maintenance cases show that G2CP reduces inter-agent communication tokens by 73%, improves task completion accuracy by 34% over free-text baselines, eliminates cascading hallucinations, and produces fully auditable reasoning chains. G2CP represents a fundamental shift from linguistic to structural communication in multi-agent systems, with implications for any domain requiring precise agent coordination. Code, data, and evaluation scripts are publicly available."}
{"id": "2602.13277", "categories": ["cs.NI", "cs.DC"], "pdf": "https://arxiv.org/pdf/2602.13277", "abs": "https://arxiv.org/abs/2602.13277", "authors": ["Uma Mahesh Boda", "Mallikharjuna Rao Nuka"], "title": "Intent-driven Diffusion-based Path for Mobile Data Collector in IoT-enabled Dense WSNs", "comment": null, "summary": "Mobile data collection using controllable sinks is an effective approach to improve energy efficiency and data freshness in densely deployed wireless sensor networks (WSNs). However, existing path-planning methods are often heuristic-driven and lack the flexibility to adapt to high-level operational objectives under dynamic network conditions. In this paper, we propose ID2P2, a intent-driven diffusion-based path planning framework for jointly addresses rendezvous point selection and mobile data collector (MDC) tour construction in IoT-enabled dense WSNs. High-level intents, such as latency minimization, energy balancing, or coverage prioritization, are explicitly modeled and incorporated into a generative diffusion planning process that produces feasible and adaptive data collection trajectories. The proposed approach learns a trajectory prior that captures spatial node distribution and network characteristics, enabling the MDC to generate paths that align with specified intents while maintaining collision-free and energy-aware operation. Extensive simulations are conducted to evaluate the effectiveness of the proposed framework against conventional path-planning baselines. The results demonstrate that ID2P2 consistently outperforms representative baselines, achieving up to 25-30% reduction in tour completion time and travel overhead, approximately 10-30% improvement in data freshness, and 15-30% gains in energy efficiency and packet delivery performance, while maintaining higher throughput and fairness as network density increases, confirming its robustness and scalability for WSNs."}
{"id": "2602.13723", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2602.13723", "abs": "https://arxiv.org/abs/2602.13723", "authors": ["Weiyu Kong", "Yun Lin", "Xiwen Teoh", "Duc-Minh Nguyen", "Ruofei Ren", "Jiaxin Chang", "Haoxu Hu", "Haoyu Chen"], "title": "ARC: Compiling Hundreds of Requirement Scenarios into A Runnable Web System", "comment": null, "summary": "Large Language Models (LLMs) have improved programming efficiency, but their performance degrades significantly as requirements scale; when faced with multi-modal documents containing hundreds of scenarios, LLMs often produce incorrect implementations or omit constraints. We propose Agentic Requirement Compilation (ARC), a technique that moves beyond simple code generation to requirement compilation, enabling the creation of runnable web systems directly from multi-modal DSL documents. ARC generates not only source code but also modular designs for UI, API, and database layers, enriched test suites (unit, modular, and integration), and detailed traceability for software maintenance. Our approach employs a bidirectional test-driven agentic loop: a top-down architecture phase decomposes requirements into verifiable interfaces, followed by a bottom-up implementation phase where agents generate code to satisfy those tests. ARC maintains strict traceability across requirements, design, and code to facilitate intelligent asset reuse. We evaluated ARC by generating six runnable web systems from documents spanning 50-200 multi-modal scenarios. Compared to state-of-the-art baselines, ARC-generated systems pass 50.6% more GUI tests on average. A user study with 21 participants showed that novice users can successfully write DSL documents for complex systems, such as a 10K-line ticket-booking system, in an average of 5.6 hours. These results demonstrate that ARC effectively transforms non-trivial requirement specifications into maintainable, runnable software."}
{"id": "2602.13204", "categories": ["cs.NI", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.13204", "abs": "https://arxiv.org/abs/2602.13204", "authors": ["Soundes Oumaima Boufaida", "Abdemadjid Benmachiche", "Majda Maatallah", "Chaouki Chemam"], "title": "Hybrid Secure Routing in Mobile Ad-hoc Networks (MANETSs)", "comment": null, "summary": "Because wireless communication is dynamic and has inherent defects, routing algorithms are crucial in the quickly evolving field of mobile ad hoc networks, or MANETs This study looks at the many security problems that MANETs encounter. These problems, which pose major risks to network performance, include flooding, sinkholes, and black hole assaults to address these challenges. We introduce the Hybrid Secure Routing Protocol (HSRP), which enhances the security and robustness of routing operations by fusing trust-based tactics with cryptographic approaches. HSRP combines the strengths of both proactive and reactive routing strategies, enabling it to adapt dynamically to evolving network conditions while protecting against malicious activities. We use extensive simulations with Network Simulator (NS-2) and a thorough review of the literature to assess HSRP's performance under different attack scenarios. The results show that, in comparison to traditional protocols, HSRP increases throughput and decreases latency, hence improving routing efficiency while simultaneously bolstering data transfer security. With uses in vital domains including military operations and disaster response, this study provides a scalable and workable approach for safe routing in MANETs. The findings highlight how crucial it is to include cutting-edge security features in routing protocol design to guarantee the dependability and integrity of MANETs in practical situations."}
{"id": "2602.13671", "categories": ["cs.MA", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.13671", "abs": "https://arxiv.org/abs/2602.13671", "authors": ["Guangyi Liu", "Haojun Lin", "Huan Zeng", "Heng Wang", "Quanming Yao"], "title": "MAS-on-the-Fly: Dynamic Adaptation of LLM-based Multi-Agent Systems at Test Time", "comment": null, "summary": "Large Language Model (LLM)-based multi-agent systems (MAS) have emerged as a promising paradigm for solving complex tasks. However, existing works often rely on manual designs or \"one-size-fits-all\" automation, lacking dynamic adaptability after deployment. Inspired by how biological systems adapt, we introduce MASFly, a novel multi-agent framework enabling dynamic adaptation at test time. To adapt system generation, MASFly employs a retrieval-augmented SOP instantiation mechanism that leverages a self-constructed repository of successful collaboration patterns, enabling the LLM to assemble customized MASs for new queries. For adaptive execution, MASFly incorporates an experience-guided supervision mechanism, where a dedicated Watcher agent monitors system behaviors with reference to a personalized experience pool and provides real-time interventions. Extensive experiments demonstrate that MASFly achieves state-of-the-art performance, most notably a 61.7% success rate on the TravelPlanner benchmark, while exhibiting strong task adaptability and robustness."}
{"id": "2602.13542", "categories": ["cs.NI", "cs.DC"], "pdf": "https://arxiv.org/pdf/2602.13542", "abs": "https://arxiv.org/abs/2602.13542", "authors": ["George M. Gichuru", "Zoe Aiyanna M. Cayetano"], "title": "SIDSense: Database-Free TV White Space Sensing for Disaster-Resilient Connectivity", "comment": "7 pages, 1 figure", "summary": "Small Island Developing States (SIDS) are disproportionately exposed to climate-driven disasters, yet often rely on fragile terrestrial networks that fail when they are most needed. TV White Space (TVWS) links offer long-range, low-power coverage; however, current deployments depend on Protocol to Access White Spaces (PAWS) database connectivity for channel authorization, creating a single point of failure during outages.\n  We present SIDSense, an edge AI framework for database-free TVWS operation that preserves regulatory intent through a compliance-gated controller, audit logging, and graceful degradation. SIDSense couples CNN-based spectrum classification with a hybrid sensing-first, authorization-as-soon-as-possible workflow and co-locates sensing and video enhancement with a private 5G stack on a maritime vessel to sustain situational-awareness video backhaul.\n  Field experiments in Barbados demonstrate sustained connectivity during simulated PAWS outages, achieving 94.2% sensing accuracy over 470-698 MHz with 23 ms mean decision latency, while maintaining zero missed 5G Layer-1 (L1) deadlines under GPU-aware scheduling. We release an empirical Caribbean TVWS propagation and occupancy dataset and look to contribute some of the components of the SIDSense pipeline to the open source community to accelerate resilient connectivity deployments in climate-vulnerable regions."}
{"id": "2602.13766", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2602.13766", "abs": "https://arxiv.org/abs/2602.13766", "authors": ["Rafael Tomaz", "Paloma Guenes", "Allysson Allex Araújo", "Maria Teresa Baldassarre", "Marcos Kalinowski"], "title": "Impacts of Generative AI on Agile Teams' Productivity: A Multi-Case Longitudinal Study", "comment": "Preprint with the original submission accepted for publication at Forge 2026", "summary": "Context: Generative Artificial Intelligence (GenAI) tools, such as GitHub Copilot and GPT tools, represent a paradigm shift in software engineering. While their impact is clear, most studies are short-term, focused on individual experiments. The sustained, team-level effects on productivity within industrial agile environments remain largely uncharacterized. Goal: This study aims to provide a longitudinal evaluation of GenAI's impact on agile software teams. We characterize its effect on developers' productivity by applying the multi-dimensional SPACE framework. Method: We conducted a multi-case longitudinal study involving 3 agile teams at a large technology consulting firm for around 13 months. We collected and compared quantitative telemetry (Jira, SonarQube, Git) and qualitative survey data from historical (pre-adoption) and research (post-adoption) sprints. Conclusion: GenAI tools can significantly improve team performance and well-being. Our key finding is a sharp increase in Performance and perceived Efficiency concurrent with flat developer Activity. This suggests GenAI increases the value density of development work, not its volume. This finding validates the necessity of multi-dimensional frameworks like SPACE to capture the true, nuanced impact of GenAI in situ, which would be invisible to studies measuring Activity alone."}
{"id": "2602.13205", "categories": ["cs.NI"], "pdf": "https://arxiv.org/pdf/2602.13205", "abs": "https://arxiv.org/abs/2602.13205", "authors": ["Sumita Majhi", "Kishan Thakkar", "Pinaki Mitra"], "title": "Reinforcement Learning-Enabled Dynamic Code Assignment for Ultra-Dense IoT Networks: A NOMA-Based Approach to Massive Device Connectivity", "comment": null, "summary": "Ultra-dense IoT networks require an effective non-orthogonal multiple access (NOMA) scheme, yet they experience intense interference because of fixed code assignment. We suggest a reinforcement learning (RL) model of dynamic Gold code assignment in IoT-NOMA networks. Our Markov Decision Process which is IoT aware is a joint optimization of throughput, energy efficiency, and fairness. Two RL algorithms are created, including Natural Policy Gradient (NPG) to learn stable discrete actions and Deep Deterministic Policy Gradient (DDPG) with continuous code embedding. Under smart city conditions, NPG can attain throughput of 11.6% and energy efficiency of 15.8 likewise superior to its performance with a static allocation. Nonetheless, the performance is worse in organized industrial settings, and the reliability is minimal (0-2%), which points to the fact that dynamic code assignment is not a sufficient measure of ultra-reliable IoT and needs to be supplemented by power control or retransmission schemes. The work offers a basis to the RL-based resource allocation in massive IoT network."}
{"id": "2602.13878", "categories": ["cs.MA"], "pdf": "https://arxiv.org/pdf/2602.13878", "abs": "https://arxiv.org/abs/2602.13878", "authors": ["Martina Baiardi", "Samuele Burattini", "Giovanni Ciatto", "Danilo Pianini"], "title": "Testing BDI-based Multi-Agent Systems using Discrete Event Simulation", "comment": null, "summary": "Multi-agent systems are designed to deal with open, distributed systems with unpredictable dynamics, which makes them inherently hard to test. The value of using simulation for this purpose is recognized in the literature, although achieving sufficient fidelity (i.e., the degree of similarity between the simulation and the real-world system) remains a challenging task. This is exacerbated when dealing with cognitive agent models, such as the Belief Desire Intention (BDI) model, where the agent codebase is not suitable to run unchanged in simulation environments, thus increasing the reality gap between the deployed and simulated systems. We argue that BDI developers should be able to test in simulation the same specification that will be later deployed, with no surrogate representations. Thus, in this paper, we discuss how the control flow of BDI agents can be mapped onto a Discrete Event Simulation (DES), showing that such integration is possible at different degrees of granularity. We substantiate our claims by producing an open-source prototype integration between two pre-existing tools (JaKtA and Alchemist), showing that it is possible to produce a simulation-based testing environment for distributed BDI} agents, and that different granularities in mapping BDI agents over DESs may lead to different degrees of fidelity."}
{"id": "2602.13767", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2602.13767", "abs": "https://arxiv.org/abs/2602.13767", "authors": ["Paloma Guenes", "Rafael Tomaz", "Maria Teresa Baldassarre", "Alexander Serebrenik"], "title": "Impostor Phenomenon as Human Debt: A Challenge to the Future of Software Engineering", "comment": "Preprint of the paper accepted for the Future of Software Engineering (FoSE) track at ICSE 2026", "summary": "The Impostor Phenomenon (IP) impacts a significant portion of the Software Engineering workforce, yet it is often viewed primarily through an internal individual lens. In this position paper, we propose framing the prevalence of IP as a form of Human Debt and discuss the relation with the ICSE2026 Pre Survey on the Future of Software Engineering results. Similar to technical debt, which arises when short-term goals are prioritized over long-term structural integrity, Human Debt accumulates due to gaps in psychological safety and inclusive support within socio-technical ecosystems. We observe that this debt is not distributed equally, it weighs heavier on underrepresented engineers and researchers, who face compounded challenges within traditional hierarchical structures and academic environments. We propose cultural refactoring, transparency and active maintenance through allyship, suggesting that leaders and institutions must address the environmental factors that exacerbate these feelings, ensuring a sustainable ecosystem for all professionals."}
{"id": "2602.13206", "categories": ["cs.NI"], "pdf": "https://arxiv.org/pdf/2602.13206", "abs": "https://arxiv.org/abs/2602.13206", "authors": ["Peichun Li", "Liping Qian", "Dusit Niyato", "Shiwen Mao", "Yuan Wu"], "title": "Toward Resource-Efficient Collaboration of Large AI Models in Mobile Edge Networks", "comment": "Accepted by IEEE Network. 9 pages, 4 figures", "summary": "The collaboration of large artificial intelligence (AI) models in mobile edge networks has emerged as a promising paradigm to meet the growing demand for intelligent services at the network edge. By enabling multiple devices to cooperatively execute submodels or subtasks, collaborative AI enhances inference efficiency and service quality with constrained resources. However, deploying large AI models in such environments remains challenging due to the intrinsic mismatch between model complexity and the limited computation, memory, and communication resources in edge networks. This article provides a comprehensive overview of the system architecture for collaborative AI in mobile edge networks, along with representative application scenarios in transportation and healthcare. We further present recent advances in resource-efficient collaboration techniques, categorized into spatial and temporal approaches. The major spatial approaches include federated tuning, mixture of experts, patch-based diffusion, and hierarchical diffusion. Meanwhile, the important temporal approaches encompass split learning, cascading inference, speculative decoding, and routing inference. Building upon these foundations, we propose a multi-stage diffusion framework that enables elastic distribution of large generative models across heterogeneous edge resources. Experimental results demonstrate that our framework achieves performance improvement in both efficiency and adaptability for data generation."}
{"id": "2602.14471", "categories": ["cs.MA", "cs.AI", "cs.GT", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.14471", "abs": "https://arxiv.org/abs/2602.14471", "authors": ["Furkan Mumcu", "Yasin Yilmaz"], "title": "Socially-Weighted Alignment: A Game-Theoretic Framework for Multi-Agent LLM Systems", "comment": null, "summary": "Deploying large language model (LLM) agents in shared environments introduces a fundamental tension between individual alignment and collective stability: locally rational decisions can impose negative externalities that degrade system-level performance. We propose Socially-Weighted Alignment (SWA), a game-theoretic framework that modifies inference-time decision making by interpolating between an agent's private objective and an estimate of group welfare via a social weight $λ\\in[0,1]$. In a shared-resource congestion game with $n$ agents and congestion severity $β$, we show that SWA induces a critical threshold $λ^*=(n-β)/(n-1)$ above which agents no longer have marginal incentive to increase demand under overload, yielding a phase transition from persistent congestion to stable operation near capacity. We further provide an inference-time algorithmic instantiation of SWA that does not require parameter updates or multi-agent reinforcement learning, and use a multi-agent simulation to empirically validate the predicted threshold behavior."}
{"id": "2602.13774", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2602.13774", "abs": "https://arxiv.org/abs/2602.13774", "authors": ["Paloma Guenes", "Joan Leite", "Rafael Tomaz", "Allysson Allex Araujo", "Jean Natividade", "Maria Teresa Baldassarre", "Marcos Kalinowski"], "title": "A Quasi-Experimental Evaluation of Coaching to Mitigate the Impostor Phenomenon in Early-Career Software Engineers", "comment": "Preprint with the original submission accepted for publication at CHASE 2026", "summary": "Context: The Impostor Phenomenon (IP), the persistent belief of being a fraud despite evident competence, is common in Software Engineering (SE), where high expectations for expertise and innovation prevail. Although coaching and similar interventions are proposed to mitigate IP, empirical evidence in SE remains underexplored.\n  Objective: This study examines the impact of a structured group coaching intervention on reducing IP feelings among early-career software engineers.\n  Method: We conducted a quasi-experiment with 20 participants distributed across two project teams using a wait-list control design, complemented by non-participant observation. The treatment group received a three-session coaching intervention, while the control group received it after an observation phase. IP was assessed using the Clance Impostor Phenomenon Scale (CIPS), alongside evaluated measures of well-being (WHO-5), life satisfaction (SWLS), and affect (PANAS).\n  Results: The coaching resulted in modest reductions in CIPS scores, whereas the control group also improved during the observation phase, suggesting that contextual and temporal factors may have exerted a stronger influence than the formal intervention.\n  Conclusion: These results suggest that coaching may support reflection and awareness related to IP, yet other contextual aspects of team collaboration and project work might also contribute to these changes. This study offers a novel empirical step toward understanding how structured IP interventions operate within SE environments."}
{"id": "2602.13207", "categories": ["cs.NI", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.13207", "abs": "https://arxiv.org/abs/2602.13207", "authors": ["Abdikarim Mohamed Ibrahim", "Rosdiadee Nordin"], "title": "A Safety-Constrained Reinforcement Learning Framework for Reliable Wireless Autonomy", "comment": null, "summary": "Artificial intelligence (AI) and reinforcement learning (RL) have shown significant promise in wireless systems, enabling dynamic spectrum allocation, traffic management, and large-scale Internet of Things (IoT) coordination. However, their deployment in mission-critical applications introduces the risk of unsafe emergent behaviors, such as UAV collisions, denial-of-service events, or instability in vehicular networks. Existing safety mechanisms are predominantly reactive, relying on anomaly detection or fallback controllers that intervene only after unsafe actions occur, which cannot guarantee reliability in ultra-reliable low-latency communication (URLLC) settings. In this work, we propose a proactive safety-constrained RL framework that integrates proof-carrying control (PCC) with empowerment-budgeted (EB) enforcement. Each agent action is verified through lightweight mathematical certificates to ensure compliance with interference constraints, while empowerment budgets regulate the frequency of safety overrides to balance safety and autonomy. We implement this framework on a wireless uplink scheduling task using Proximal Policy Optimization (PPO). Simulation results demonstrate that the proposed PCC+EB controller eliminates unsafe transmissions while preserving system throughput and predictable autonomy. Compared with unconstrained and reactive baselines, our method achieves provable safety guarantees with minimal performance degradation. These results highlight the potential of proactive safety constrained RL to enable trustworthy wireless autonomy in future 6G networks."}
{"id": "2602.14606", "categories": ["cs.MA", "cs.AI", "cs.CE"], "pdf": "https://arxiv.org/pdf/2602.14606", "abs": "https://arxiv.org/abs/2602.14606", "authors": ["Jose Manuel de la Chica Rodriguez", "Juan Manuel Vera Díaz"], "title": "Towards Selection as Power: Bounding Decision Authority in Autonomous Agents", "comment": null, "summary": "Autonomous agentic systems are increasingly deployed in regulated, high-stakes domains where decisions may be irreversible and institutionally constrained. Existing safety approaches emphasize alignment, interpretability, or action-level filtering. We argue that these mechanisms are necessary but insufficient because they do not directly govern selection power: the authority to determine which options are generated, surfaced, and framed for decision. We propose a governance architecture that separates cognition, selection, and action into distinct domains and models autonomy as a vector of sovereignty. Cognitive autonomy remains unconstrained, while selection and action autonomy are bounded through mechanically enforced primitives operating outside the agent's optimization space. The architecture integrates external candidate generation (CEFL), a governed reducer, commit-reveal entropy isolation, rationale validation, and fail-loud circuit breakers. We evaluate the system across multiple regulated financial scenarios under adversarial stress targeting variance manipulation, threshold gaming, framing skew, ordering effects, and entropy probing. Metrics quantify selection concentration, narrative diversity, governance activation cost, and failure visibility. Results show that mechanical selection governance is implementable, auditable, and prevents deterministic outcome capture while preserving reasoning capacity. Although probabilistic concentration remains, the architecture measurably bounds selection authority relative to conventional scalar pipelines. This work reframes governance as bounded causal power rather than internal intent alignment, offering a foundation for deploying autonomous agents where silent failure is unacceptable."}
{"id": "2602.13845", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2602.13845", "abs": "https://arxiv.org/abs/2602.13845", "authors": ["Allysson Allex Araújo", "Gabriel Vasconcelos", "Marvin Wyrich", "Maria Teresa Baldassarre", "Paloma Guenes", "Marcos Kalinowski"], "title": "Constructive Patterns for Human-Centered Tech Hiring", "comment": null, "summary": "[Context] Online Recruitment and Selection (R&S) processes are often the first point of contact between early-career software engineers and the tech industry. Yet many candidates experience these processes as opaque, inefficient, or even discouraging. While prior research has extensively documented the flaws and biases in online tech hiring, little is known about the practices that create positive candidate experiences. [Objective & Method] This paper explores such practices, referred to as Constructive Patterns (CPs), from the perspective of early-career software engineers. Guided by Applicant Attribution-Reaction Theory, we conducted 22 semi-structured interviews in which participants collectively described over 470 online R&S experiences. [Results] Through thematic analysis, we identified 22 CPs that reflect positive practices such as comprehensive and transparent job advertisements (CP01), specific and developmental feedback (CP03), humanized and respectful interaction (CP06), and framing the process as a two-way street (CP18). [Conclusion] Our findings extend the conversation on tech hiring beyond diagnosing dysfunctions toward designing for human-centered and growth-oriented candidate experiences. The resulting catalog of CPs provides a concrete and empirically grounded resource for organizations seeking to attract and support early-career software engineers more effectively."}
{"id": "2602.13210", "categories": ["cs.NI", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.13210", "abs": "https://arxiv.org/abs/2602.13210", "authors": ["Jie Zheng", "Ruichen Zhang", "Dusit Niyato", "Haijun Zhang", "Jiacheng Wang", "Hongyang Du", "Jiawen Kang", "Zehui Xiong"], "title": "Large Language Model (LLM)-enabled Reinforcement Learning for Wireless Network Optimization", "comment": null, "summary": "Enhancing future wireless networks presents a significant challenge for networking systems due to diverse user demands and the emergence of 6G technology. While reinforcement learning (RL) is a powerful framework, it often encounters difficulties with high-dimensional state spaces and complex environments, leading to substantial computational demands, distributed intelligence, and potentially inconsistent outcomes. Large language models (LLMs), with their extensive pretrained knowledge and advanced reasoning capabilities, offer promising tools to enhance RL in optimizing 6G wireless networks. We explore RL models augmented by LLMs, emphasizing their roles and the potential benefits of their synergy in wireless network optimization. We then examine LLM-enabled RL across various protocol layers: physical, data link, network, transport, and application layers. Additionally, we propose an LLM-assisted state representation and semantic extraction to enhance the multi-agent reinforcement learning (MARL) framework. This approach is applied to service migration and request routing, as well as topology graph generation in unmanned aerial vehicle (UAV)-satellite networks. Through case studies, we demonstrate that our framework effectively performs optimization of wireless network. Finally, we outline prospective research directions for LLM-enabled RL in wireless network optimization."}
{"id": "2602.14681", "categories": ["cs.MA", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.14681", "abs": "https://arxiv.org/abs/2602.14681", "authors": ["Xingjian Wu", "Xvyuan Liu", "Junkai Lu", "Siyuan Wang", "Yang Shu", "Jilin Hu", "Chenjuan Guo", "Bin Yang"], "title": "ST-EVO: Towards Generative Spatio-Temporal Evolution of Multi-Agent Communication Topologies", "comment": null, "summary": "LLM-powered Multi-Agent Systems (MAS) have emerged as an effective approach towards collaborative intelligence, and have attracted wide research interests. Among them, ``self-evolving'' MAS, treated as a more flexible and powerful technical route, can construct task-adaptive workflows or communication topologies, instead of relying on a predefined static structue template. Current self-evolving MAS mainly focus on Spatial Evolving or Temporal Evolving paradigm, which only considers the single dimension of evolution and does not fully incentivize LLMs' collaborative capability. In this work, we start from a novel Spatio-Temporal perspective by proposing ST-EVO, which supports dialogue-wise communication scheduling with a compact yet powerful flow-matching based Scheduler. To make precise Spatio-Temporal scheduling, ST-EVO can also perceive the uncertainty of MAS, and possesses self-feedback ability to learn from accumulated experience. Extensive experiments on nine benchmarks demonstrate the state-of-the-art performance of ST-EVO, achieving about 5%--25% accuracy improvement."}
{"id": "2602.13851", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.13851", "abs": "https://arxiv.org/abs/2602.13851", "authors": ["Arshad Beg", "Diarmuid O'Donoghue", "Rosemary Monahan"], "title": "Evaluating LLM-Generated ACSL Annotations for Formal Verification", "comment": "12 pages. Submitted to Formal Techniques for Judicious Programming FTfJP-2026 at ECOOP. Under review", "summary": "Formal specifications are crucial for building verifiable and dependable software systems, yet generating accurate and verifiable specifications for real-world C programs remains challenging. This paper empirically evaluates the extent to which formal-analysis tools can automatically generate and verify ACSL specifications without human or learning-based assistance. We conduct a controlled study on a recently released dataset of 506 C programs, repurposing it from interactive, developer-driven workflows to an automated evaluation setting. Five ACSL generation systems are compared: a rule-based Python script, Frama-C's RTE plugin, and three large language models--DeepSeek-V3.2, GPT-5.2, and OLMo 3.1 32B Instruct. All generated specifications are verified under identical conditions using the Frama-C WP plugin powered by multiple SMT solvers, allowing a direct comparison of annotation quality, solver sensitivity, and proof stability. Our results provide new empirical evidence on the capabilities and limitations of automated ACSL generation, complementing prior survey-based work."}
{"id": "2602.13211", "categories": ["cs.NI", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.13211", "abs": "https://arxiv.org/abs/2602.13211", "authors": ["Miao Ye", "Yanye Chen", "Yong Wang", "Cheng Zhu", "Qiuxiang Jiang", "Gai Huang", "Feng Ding"], "title": "An Overlay Multicast Routing Method Based on Network Situational Aware-ness and Hierarchical Multi-Agent Reinforcement Learning", "comment": "30page, 10 figures", "summary": "Compared with IP multicast, Overlay Multicast (OM) offers better compatibility and flexible deployment in heterogeneous, cross-domain networks. However, traditional OM struggles to adapt to dynamic traffic due to unawareness of physical resource states, and existing reinforcement learning methods fail to decouple OM's tightly coupled multi-objective nature, leading to high complexity, slow convergence, and instability. To address this, we propose MA-DHRL-OM, a multi-agent deep hierarchical reinforcement learning approach. Using SDN's global view, it builds a traffic-aware model for OM path planning. The method decomposes OM tree construction into two stages via hierarchical agents, reducing action space and improving convergence stability. Multi-agent collaboration balances multi-objective optimization while enhancing scalability and adaptability. Experiments show MA-DHRL-OM outperforms existing methods in delay, bandwidth utilization, and packet loss, with more stable convergence and flexible routing."}
{"id": "2602.14780", "categories": ["cs.MA", "cs.CY", "cs.RO", "eess.SY"], "pdf": "https://arxiv.org/pdf/2602.14780", "abs": "https://arxiv.org/abs/2602.14780", "authors": ["Anna-Lena Schlamp", "Jeremias Gerner", "Klaus Bogenberger", "Werner Huber", "Stefanie Schmidtner"], "title": "ROSA: Roundabout Optimized Speed Advisory with Multi-Agent Trajectory Prediction in Multimodal Traffic", "comment": "8 pages, 1 figure, 4 tables, 2026 IEEE International Conference on Intelligent Transportation Systems (ITSC)", "summary": "We present ROSA -- Roundabout Optimized Speed Advisory -- a system that combines multi-agent trajectory prediction with coordinated speed guidance for multimodal, mixed traffic at roundabouts. Using a Transformer-based model, ROSA jointly predicts the future trajectories of vehicles and Vulnerable Road Users (VRUs) at roundabouts. Trained for single-step prediction and deployed autoregressively, it generates deterministic outputs, enabling actionable speed advisories. Incorporating motion dynamics, the model achieves high accuracy (ADE: 1.29m, FDE: 2.99m at a five-second prediction horizon), surpassing prior work. Adding route intention further improves performance (ADE: 1.10m, FDE: 2.36m), demonstrating the value of connected vehicle data. Based on predicted conflicts with VRUs and circulating vehicles, ROSA provides real-time, proactive speed advisories for approaching and entering the roundabout. Despite prediction uncertainty, ROSA significantly improves vehicle efficiency and safety, with positive effects even on perceived safety from a VRU perspective. The source code of this work is available under: github.com/urbanAIthi/ROSA."}
{"id": "2602.13962", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2602.13962", "abs": "https://arxiv.org/abs/2602.13962", "authors": ["Yunkun Wang", "Xuanhe Zhang", "Junxiao Han", "Chen Zhi", "Shuiguang Deng"], "title": "CodeGlance: Understanding Code Reasoning Challenges in LLMs through Multi-Dimensional Feature Analysis", "comment": null, "summary": "In modern software development, developers frequently need to understand code behavior at a glance -- whether reviewing pull requests, debugging issues, or navigating unfamiliar codebases. This ability to reason about dynamic program behavior is fundamental to effective software engineering and increasingly supported by Large Language Models (LLMs). However, existing studies on code reasoning focus primarily on isolated code snippets, overlooking the complexity of real-world scenarios involving external API interactions and unfamiliar functions. This gap hinders our understanding of what truly makes code reasoning challenging for LLMs across diverse programming contexts.\n  We present CodeGlance, a multi-dimensional benchmark investigating code reasoning challenges across three realistic scenarios: intrinsic logic reasoning, API interaction reasoning, and unseen function reasoning. Through systematic evaluation of 7 state-of-the-art LLMs, we reveal that unseen function reasoning poses significant challenges especially for smaller models, with Qwen2.5-3b achieving only 6.0\\% accuracy on unseen functions compared to 37.5\\% on familiar APIs. We identify critical code complexity features -- including execution trace length, API invocation count, and control flow complexity -- that significantly impact code reasoning difficulty across scenarios. We further investigate how common augmentation strategies, including CoT, document retrieval, and code search, can improve reasoning performance, finding that their effectiveness varies substantially depending on whether challenges stem from logical complexity or knowledge gaps. These findings provide actionable guidance for developing more capable code reasoning systems and deploying LLM-based programming assistants in real-world software development."}
{"id": "2602.13216", "categories": ["cs.NI"], "pdf": "https://arxiv.org/pdf/2602.13216", "abs": "https://arxiv.org/abs/2602.13216", "authors": ["Jiayi Liu", "Yilin Wang", "Michael Beyeler"], "title": "Network-Adaptive Cloud Preprocessing for Visual Neuroprostheses", "comment": null, "summary": "Cloud-based machine learning is increasingly explored as a preprocessing strategy for next-generation visual neuroprostheses, where advanced scene understanding may exceed the computational and energy constraints of battery-powered visual processing units (VPUs). Offloading computation to remote servers enables the use of state-of-the-art vision models, but also introduces sensitivity to network latency, jitter, and packet loss, which can disrupt the temporal consistency of the delivered neural stimulus. In this work, we examine the feasibility of cloud-assisted visual preprocessing for artificial vision by framing remote inference as a perceptually constrained systems problem. We present a network-adaptive cloud-assisted pipeline in which real-time round-trip-time (RTT) feedback is used to dynamically modulate image resolution, compression, and transmission rate, explicitly prioritizing temporal continuity under adverse network conditions. Using a Raspberry Pi 4 as a simulated VPU and a client-server architecture, we evaluate system performance across a range of realistic wireless network regimes. Results show that adaptive visual encoding substantially reduces end-to-end latency during network congestion, with only modest degradation of global scene structure, while boundary precision degrades more sharply. Together, these findings delineate operating regimes in which cloud-assisted preprocessing may remain viable for future visual neuroprostheses and underscore the importance of network-aware adaptation for maintaining perceptual stability."}
{"id": "2602.15006", "categories": ["cs.MA", "cs.LG", "math.DG"], "pdf": "https://arxiv.org/pdf/2602.15006", "abs": "https://arxiv.org/abs/2602.15006", "authors": ["Meet Gandhi", "George P. Kontoudis"], "title": "Distributed Quantum Gaussian Processes for Multi-Agent Systems", "comment": "9 pages, 4 figures, accepted at AAMAS 2026 (International Conference on Autonomous Agents and Multiagent Systems)", "summary": "Gaussian Processes (GPs) are a powerful tool for probabilistic modeling, but their performance is often constrained in complex, largescale real-world domains due to the limited expressivity of classical kernels. Quantum computing offers the potential to overcome this limitation by embedding data into exponentially large Hilbert spaces, capturing complex correlations that remain inaccessible to classical computing approaches. In this paper, we propose a Distributed Quantum Gaussian Process (DQGP) method in a multiagent setting to enhance modeling capabilities and scalability. To address the challenging non-Euclidean optimization problem, we develop a Distributed consensus Riemannian Alternating Direction Method of Multipliers (DR-ADMM) algorithm that aggregates local agent models into a global model. We evaluate the efficacy of our method through numerical experiments conducted on a quantum simulator in classical hardware. We use real-world, non-stationary elevation datasets of NASA's Shuttle Radar Topography Mission and synthetic datasets generated by Quantum Gaussian Processes. Beyond modeling advantages, our framework highlights potential computational speedups that quantum hardware may provide, particularly in Gaussian processes and distributed optimization."}
{"id": "2602.13987", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2602.13987", "abs": "https://arxiv.org/abs/2602.13987", "authors": ["Zhengyu Zhan", "Ye Shang", "Jiawei Liu", "Chunrong Fang", "Quanjun Zhang", "Zhenyu Chen"], "title": "ATTest: Agent-Driven Tensor Testing for Deep Learning Library Modules", "comment": "5 pages, 3 figures", "summary": "The unit testing of Deep Learning (DL) libraries is challenging due to complex numerical semantics and implicit tensor constraints. Traditional Search-Based Software Testing (SBST) often suffers from semantic blindness, failing to satisfy the constraints of high-dimensional tensors, whereas Large Language Models (LLMs) struggle with cross-file context and unstable code modifications. This paper proposes ATTest, an agent-driven tensor testing framework for module-level unit test generation. ATTest orchestrates a seven-stage pipeline, which encompasses constraint extraction and an iterative \"generation-validation-repair\" loop, to maintain testing stability and mitigate context-window saturation. An evaluation on PyTorch and TensorFlow demonstrates that ATTest significantly outperforms state-of-the-art baselines such as PynguinML, achieving an average branch coverage of 55.60% and 54.77%, respectively. The results illustrate how agent-driven workflows bridge the semantic gap in numerical libraries while ensuring auditable test synthesis. Source code: https://github.com/iSEngLab/ATTest.git"}
{"id": "2602.13227", "categories": ["cs.NI", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.13227", "abs": "https://arxiv.org/abs/2602.13227", "authors": ["Eranga Bandara", "Ross Gore", "Sachin Shetty", "Ravi Mukkamala", "Tharaka Hewa", "Abdul Rahman", "Xueping Liang", "Safdar H. Bouk", "Amin Hass", "Peter Foytik", "Ng Wee Keong", "Kasun De Zoysa"], "title": "An Agentic AI Control Plane for 6G Network Slice Orchestration, Monitoring, and Trading", "comment": null, "summary": "6G networks are expected to be AI-native, intent-driven, and economically programmable, requiring fundamentally new approaches to network slice orchestration. Existing slicing frameworks, largely designed for 5G, rely on static policies and manual workflows and are ill-suited for the dynamic, multi-domain, and service-centric nature of emerging 6G environments. In this paper, we propose an agentic AI control plane architecture for 6G network slice orchestration, monitoring, and trading that treats orchestration as a holistic control function encompassing slice planning, deployment, continuous monitoring, and economically informed decision-making. The proposed control plane is realized as a layered architecture in which multiple cooperating AI agents. To support flexible and on-demand slice utilization, the control plane incorporates market-aware orchestration capabilities, allowing slice requirements, pricing, and availability to be jointly considered during orchestration decisions. A natural language interface, implemented using the Model Context Protocol (MCP), enables users and applications to interact with control-plane functions through intent-based queries while enforcing safety and policy constraints. To ensure responsible and explainable autonomy, the control plane integrates fine-tuned large language models organized as a multi-model consortium, governed by a dedicated reasoning model. The proposed approach is evaluated using a real-world testbed with multiple mobile core instances (e.g Open5GS) integrated with Ericsson's RAN infrastructure. The results demonstrate that combining agentic autonomy, closed-loop SLA assurance, market-aware orchestration, and natural language control enables a scalable and adaptive 6G-native control plane for network slice management, highlighting the potential of agentic AI as a foundational mechanism for future 6G networks."}
{"id": "2602.13692", "categories": ["cs.OS", "cs.MA"], "pdf": "https://arxiv.org/pdf/2602.13692", "abs": "https://arxiv.org/abs/2602.13692", "authors": ["Hao Kang", "Ziyang Li", "Xinyu Yang", "Weili Xu", "Yinfang Chen", "Junxiong Wang", "Beidi Chen", "Tushar Krishna", "Chenfeng Xu", "Simran Arora"], "title": "ThunderAgent: A Simple, Fast and Program-Aware Agentic Inference System", "comment": null, "summary": "Large language models(LLMs) are now used to power complex multi-turn agentic workflows. Existing systems run agentic inference by loosely assembling isolated components: an LLM inference engine (e.g., vLLM) and a tool orchestrator (e.g., Kubernetes). Although agentic workflows involve multiple LLM and tool requests, these systems schedule and allocate resources separately on a per-request basis, without end-to-end knowledge of the workflow. This leads to sub-optimal management of KV cache and tool execution environments. To address the challenges, we propose ThunderAgent, a fast, simple, and program-aware agentic inference system. We first abstract agentic workflows as LLM Programs, enabling a unified view of heterogeneous resources, including KV caches, system states, and external tool assets such as disk memory and network ports. Built upon this abstraction, ThunderAgent introduces a program-aware scheduler and a tool resource manager designed to maximize KV cache hit rates, mitigate memory imbalances, and enable asynchronous environment preparation. Evaluations across coding, routing, and scientific discovery agents demonstrate that ThunderAgent achieves 1.5-3.6x throughput improvements in serving, 1.8-3.9x in RL rollout, and up to 4.2x disk memory savings compared to state-of-the-art inference systems. To facilitate reproducibility and support future development, we open-source the system implementations of the whole ThunderAgent at: https://github.com/Agentic-Kinetics/ThunderAgent."}
{"id": "2602.14046", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2602.14046", "abs": "https://arxiv.org/abs/2602.14046", "authors": ["Zirui Chen", "Xing Hu", "Xin Xia", "Xiaohu Yang"], "title": "Every Maintenance Has Its Exemplar: The Future of Software Maintenance through Migration", "comment": "Accepted to ACM Transactions on Software Engineering and Methodology (TOSEM)", "summary": "Maintenance is a critical stage in the software lifecycle, ensuring that post-release systems remain reliable, efficient, and adaptable. However, manual software maintenance is labor-intensive, time-consuming, and error-prone, which highlights the urgent need for automation. Learning from maintenance activities conducted on other software systems offers an effective way to improve efficiency. In particular, recent research has demonstrated that migration-based approaches transfer knowledge, artifacts, or solutions from one system to another and show strong potential in tasks such as API evolution adaptation, software testing, and migrating patches for fault correction. This makes migration-based maintenance a valuable research direction for advancing automated maintenance.\n  This paper takes a step further by presenting the first systematic research agenda on migration-based approaches to software maintenance. We characterize the migration-based maintenance lifecycle through four key stages: \\ding{182} identifying a maintenance task that can be addressed through migration, \\ding{183} selecting suitable migration sources for the target project,\\ding{184} matching relevant data across systems and adapting the migrated data to the target context, and \\ding{185} validating the correctness of the migration. We also analyze the challenges that may arise at each stage. Our goal is to encourage the community to explore migration-based approaches more thoroughly and to tackle the key challenges that must be solved to advance automated software maintenance."}
{"id": "2602.13229", "categories": ["cs.NI"], "pdf": "https://arxiv.org/pdf/2602.13229", "abs": "https://arxiv.org/abs/2602.13229", "authors": ["Dong Ho Kang", "Hyunjoon Lee", "Hyeonjeong Cha", "Minkyu Choi", "Sungsoo Lim"], "title": "Pocket RAG: On-Device RAG for First Aid Guidance in Offline Mobile Environment", "comment": null, "summary": "In disaster scenarios or remote areas, first responders often lose network connectivity when providing first aid. In such situations, server-based AI systems fail to provide critical guidance. To address this issue, we present a lightweight, mobile-based retrieval-augmented generation system for small language models (SLMs) that can run directly on Android devices. Our system integrates a mobile-friendly optimized pipeline featuring Hybrid RAG, selective compression, batched prompt decoding, and quantization caching. Despite the model's small size, our RAG-based system achieves 94.5\\% accuracy for physical first aid and 97.0\\% for psychological first aid. Additionally, we reduce response time from 14.2s to 3.7s, achieving a nearly 4x speedup. These results prove that our system is practical and can deliver reliable first aid guidance even without internet connectivity."}
{"id": "2602.14337", "categories": ["cs.SE", "cs.MA"], "pdf": "https://arxiv.org/pdf/2602.14337", "abs": "https://arxiv.org/abs/2602.14337", "authors": ["Yukang Feng", "Jianwen Sun", "Zelai Yang", "Jiaxin Ai", "Chuanhao Li", "Zizhen Li", "Fanrui Zhang", "Kang He", "Rui Ma", "Jifan Lin", "Jie Sun", "Yang Xiao", "Sizhuo Zhou", "Wenxiao Wu", "Yiming Liu", "Pengfei Liu", "Yu Qiao", "Shenglin Zhang", "Kaipeng Zhang"], "title": "LongCLI-Bench: A Preliminary Benchmark and Study for Long-horizon Agentic Programming in Command-Line Interfaces", "comment": null, "summary": "Recent advances in AI-assisted programming have empowered agents to execute complex workflows via command-line interfaces, however, existing benchmarks are limited by short task horizons, data contamination from GitHub scraping, and a lack of fine-grained evaluation metrics, fail to rigorously evaluate the long-horizon planning and execution capabilities essential for realistic software engineering. To address these gaps, we introduce LongCLI-Bench, a comprehensive benchmark designed to evaluate agentic capabilities across long-horizon, realistic tasks. We curated 20 high-quality, long-horizon tasks from over 1,000 computer science assignments and real-world workflows, covering four engineering categories: from scratch, feature addition, bug fixing, and refactoring. We propose a dual-set testing protocol for LongCLI-Bench, which measures requirement fulfillment (fail-to-pass) and regression avoidance (pass-to-pass), and incorporates step-level scoring to pinpoint execution failures. Extensive experiments reveal that even state-of-the-art agents achieve pass rates below 20% in LongCLI-Bench. Step-level analysis further indicates that the majority of tasks stall at less than 30% completion, highlighting that critical failures often occur in the early stages. Although self-correction offers marginal gains, human-agent collaboration through plan injection and interactive guidance yields significantly higher improvements. These results highlight that future research must emphasize the development of synergistic human-agent workflows alongside advances in agents' planning and execution capabilities to overcome key challenges in long-horizon task performance."}
{"id": "2602.14337", "categories": ["cs.SE", "cs.MA"], "pdf": "https://arxiv.org/pdf/2602.14337", "abs": "https://arxiv.org/abs/2602.14337", "authors": ["Yukang Feng", "Jianwen Sun", "Zelai Yang", "Jiaxin Ai", "Chuanhao Li", "Zizhen Li", "Fanrui Zhang", "Kang He", "Rui Ma", "Jifan Lin", "Jie Sun", "Yang Xiao", "Sizhuo Zhou", "Wenxiao Wu", "Yiming Liu", "Pengfei Liu", "Yu Qiao", "Shenglin Zhang", "Kaipeng Zhang"], "title": "LongCLI-Bench: A Preliminary Benchmark and Study for Long-horizon Agentic Programming in Command-Line Interfaces", "comment": null, "summary": "Recent advances in AI-assisted programming have empowered agents to execute complex workflows via command-line interfaces, however, existing benchmarks are limited by short task horizons, data contamination from GitHub scraping, and a lack of fine-grained evaluation metrics, fail to rigorously evaluate the long-horizon planning and execution capabilities essential for realistic software engineering. To address these gaps, we introduce LongCLI-Bench, a comprehensive benchmark designed to evaluate agentic capabilities across long-horizon, realistic tasks. We curated 20 high-quality, long-horizon tasks from over 1,000 computer science assignments and real-world workflows, covering four engineering categories: from scratch, feature addition, bug fixing, and refactoring. We propose a dual-set testing protocol for LongCLI-Bench, which measures requirement fulfillment (fail-to-pass) and regression avoidance (pass-to-pass), and incorporates step-level scoring to pinpoint execution failures. Extensive experiments reveal that even state-of-the-art agents achieve pass rates below 20% in LongCLI-Bench. Step-level analysis further indicates that the majority of tasks stall at less than 30% completion, highlighting that critical failures often occur in the early stages. Although self-correction offers marginal gains, human-agent collaboration through plan injection and interactive guidance yields significantly higher improvements. These results highlight that future research must emphasize the development of synergistic human-agent workflows alongside advances in agents' planning and execution capabilities to overcome key challenges in long-horizon task performance."}
{"id": "2602.13231", "categories": ["cs.NI", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.13231", "abs": "https://arxiv.org/abs/2602.13231", "authors": ["Khaleda Papry", "Francesco Spinnato", "Marco Fiore", "Mirco Nanni", "Israat Haque"], "title": "An Explainable Failure Prediction Framework for Neural Networks in Radio Access Networks", "comment": null, "summary": "As 5G networks continue to evolve to deliver high speed, low latency, and reliable communications, ensuring uninterrupted service has become increasingly critical. While millimeter wave (mmWave) frequencies enable gigabit data rates, they are highly susceptible to environmental factors, often leading to radio link failures (RLF). Predictive models leveraging radio and weather data have been proposed to address this issue; however, many operate as black boxes, offering limited transparency for operational deployment. This work bridges that gap by introducing a framework that combines explainability based feature pruning with model refinement. Our framework can be integrated into state of the art predictors such as GNN Transformer and LSTM based architectures for RLF prediction, enabling the development of accurate and explainability guided models in 5G networks. It provides insights into the contribution of input features and the decision making logic of neural networks, leading to lighter and more scalable models. When applied to RLF prediction, our framework unveils that weather data contributes minimally to the forecast in extensive real world datasets, which informs the design of a leaner model with 50 percent fewer parameters and improved F1 scores with respect to the state of the art solution. Ultimately, this work empowers network providers to evaluate and refine their neural network based prediction models for better interpretability, scalability, and performance."}
{"id": "2602.14572", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2602.14572", "abs": "https://arxiv.org/abs/2602.14572", "authors": ["Pooya Rostami Mazrae", "Alexandre Decan", "Tom Mens", "Mairieli Wessel"], "title": "An Empirical Study of the Evolution of GitHub Actions Workflows", "comment": null, "summary": "CI/CD practices play a significant role during collaborative software development by automating time-consuming and repetitive tasks such as testing, building, quality checking, dependency and security management. GitHub Actions, the CI/CD tool integrated into GitHub, allows repository maintainers to automate development workflows. We conducted a mixed methods analysis of GitHub Actions workflow changes over time. Through a preliminary qualitative analysis of 439 modified workflow files we identified seven types of conceptual changes to workflows. Next, we performed a quantitative analysis over 49K+ GitHub repositories totaling 267K+ workflow change histories and 3.4M+ workflow file versions from November 2019 to August 2025. This analysis revealed that repositories contain a median of three workflow files, and 7.3% of all workflow files are being changed every week. The changes made to workflows tend to be small, with about three-quarters containing only a single change. The large majority of the observed changes have to do with task configuration and task specification in workflow jobs. We did not find any conclusive evidence of the effect of LLM coding tools or other major technological changes on workflow creation and workflow maintenance frequency. Our findings highlight the need for improved tooling to support fine-grained maintenance tasks, such as a broader adoption of dependency management and AI-based support for ensuring and sustaining workflow security and quality."}
{"id": "2602.13238", "categories": ["cs.NI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.13238", "abs": "https://arxiv.org/abs/2602.13238", "authors": ["Le-Hung Hoang", "Quang-Trung Luu", "Dinh Thai Hoang", "Diep N. Nguyen", "Van-Dinh Nguyen"], "title": "Securing SIM-Assisted Wireless Networks via Quantum Reinforcement Learning", "comment": "13 pages. Submmited for possible publication", "summary": "Stacked intelligent metasurfaces (SIMs) have recently emerged as a powerful wave-domain technology that enables multi-stage manipulation of electromagnetic signals through multilayer programmable architectures. While SIMs offer unprecedented degrees of freedom for enhancing physical-layer security, their extremely large number of meta-atoms leads to a high-dimensional and strongly coupled optimization space, making conventional design approaches inefficient and difficult to scale. Moreover, existing deep reinforcement learning (DRL) techniques suffer from slow convergence and performance degradation in dynamic wireless environments with imperfect knowledge of passive eavesdroppers. To overcome these challenges, we propose a hybrid quantum proximal policy optimization (Q-PPO) framework for SIM-assisted secure communications, which jointly optimizes transmit power allocation and SIM phase shifts to maximize the average secrecy rate under power and quality-of-service constraints. Specifically, a parameterized quantum circuit is embedded into the actor network, forming a hybrid classical-quantum policy architecture that enhances policy representation capability and exploration efficiency in high-dimensional continuous action spaces. Extensive simulations demonstrate that the proposed Q-PPO scheme consistently outperforms DRL baselines, achieving approximately 15% higher secrecy rates and 30% faster convergence under imperfect eavesdropper channel state information. These results establish Q-PPO as a powerful optimization paradigm for SIM-enabled secure wireless networks."}
{"id": "2602.14591", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.14591", "abs": "https://arxiv.org/abs/2602.14591", "authors": ["Evgenii Kniazev"], "title": "Automated Classification of Source Code Changes Based on Metrics Clustering in the Software Development Process", "comment": "This is an English translation of the author's Ph.D. dissertation abstract, originally defended in Russian at ITMO University (2009) under the supervision of Prof. A.A. Shalyto. The original research was co-authored with D.G. Shopyrin. Original available at https://is.ifmo.ru/disser/knyazev_autorefer.pdf", "summary": "This paper presents an automated method for classifying source code changes during the software development process based on clustering of change metrics. The method consists of two steps: clustering of metric vectors computed for each code change, followed by expert mapping of the resulting clusters to predefined change classes. The distribution of changes into clusters is performed automatically, while the mapping of clusters to classes is carried out by an expert. Automation of the distribution step substantially reduces the time required for code change review. The k-means algorithm with a cosine similarity measure between metric vectors is used for clustering. Eleven source code metrics are employed, covering lines of code, cyclomatic complexity, file counts, interface changes, and structural changes. The method was validated on five software systems, including two open-source projects (Subversion and NHibernate), and demonstrated classification purity of P_C = 0.75 +/- 0.05 and entropy of E_C = 0.37 +/- 0.06 at a significance level of 0.05."}
{"id": "2602.13245", "categories": ["cs.NI"], "pdf": "https://arxiv.org/pdf/2602.13245", "abs": "https://arxiv.org/abs/2602.13245", "authors": ["Yihe Diao", "Yuhang Wu", "Hongtao Liang", "Ming Xu", "Rui Ding", "Fuhui Zhou", "Qihui Wu", "Jun Zhang"], "title": "Embodied Intelligent Spectrum Management: A New Paradigm for Dynamic Spectrum Access", "comment": null, "summary": "Wireless communication is evolving into an agent era, where numerous intelligent agents equipped with perception, reasoning, and interaction capabilities will operate in highly dynamic wireless environments. To complete diverse complex tasks, agent communication will play a critical role, which enables autonomous information exchange with external tools, services, and other agents. This trendy movement will dramatically increase spectrum demand and result in unprecedented challenges for spectrum management. However, current spectrum management paradigms, including static spectrum allocation and intelligent management, lack the flexibility and generalization to accommodate the dynamic and heterogeneous demands of agent communication. The recent advancements in embodied intelligence (EI) bring a promising solution, and this article will provide our vision of an emerging embodied intelligent spectrum management (EISM) paradigm. We start with an architecture for EISM, elaborating its key enabling technologies. Then, a prototype platform is presented to demonstrate the advantages of EISM. Finally, key challenges and open issues are outlined to facilitate future research in this emerging field."}
{"id": "2602.14595", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2602.14595", "abs": "https://arxiv.org/abs/2602.14595", "authors": ["Shirin Pirouzkhah", "Souhaila Serbout", "Alberto Bacchelli"], "title": "Consistent or Sensitive? Automated Code Revision Tools Against Semantics-Preserving Perturbations", "comment": null, "summary": "Automated Code Revision (ACR) tools aim to reduce manual effort by automatically generating code revisions based on reviewer feedback. While ACR tools have shown promising performance on historical data, their real-world utility depends on their ability to handle similar code variants expressing the same issue - a property we define as consistency. However, the probabilistic nature of ACR tools often compromises consistency, which may lead to divergent revisions even for semantically equivalent code variants. In this paper, we investigate the extent to which ACR tools maintain consistency when presented with semantically equivalent code variants. To do so, we first designed nine types of semantics-preserving perturbations (SPP) and applied them to 2032 Java methods from real-world GitHub projects, generating over 10K perturbed variants for evaluation. Then we used these perturbations to evaluate the consistency of five state-of-the-art transformer-based ACR tools. We found that the ACR tools' ability to generate correct revisions can drop by up to 45.3%, when presented with semantically equivalent code. The closer the perturbation is to this targeted region, the more likely an ACR tool is to fail to generate the correct revision. We explored potential mitigation strategies that modify the input representation, but found that these attention-guiding heuristics yielded only marginal improvements, thus leaving the solution to this problem as an open research question."}
{"id": "2602.13269", "categories": ["cs.NI"], "pdf": "https://arxiv.org/pdf/2602.13269", "abs": "https://arxiv.org/abs/2602.13269", "authors": ["Ying Liu", "Yifan Zhang", "Xinyu Wang", "Chao Yang", "Kandaraj Piamrat", "Stephan Sigg", "Zheng Changr", "Yusheng Ji"], "title": "Modality-Tailored Age of Information for Multimodal Data in Edge Computing Systems", "comment": null, "summary": "As Internet of Things (IoT) systems scale and device heterogeneity grows, multimodal data have become ubiquitous. Meanwhile, evaluating the freshness of multimodal data is essential, as stale updates would delay task execution, degrade decision accuracy, and undermine safety in latency-sensitive services. However, existing freshness metrics such as Age of Information (AoI) are not suitable for multimodal data, as they do not capture modality-specific characteristics. In this paper, we propose a metric, namely, Modality-Tailored Age of Information (MAoI), to provide a unified and decision-relevant evaluation of freshness for resource management and policy optimization for multimodal data. This metric integrates modality-specific semantic and temporal characteristics, reflecting both age evolution and content importance for multimodal data in multi-access edge computing (MEC) systems. Then, the closed-form expression of the average MAoI is derived, and an MAoI minimization problem is formulated, where sampling intervals and offloading decisions are optimized with practical energy constraints. To effectively solve this problem, a Joint Sampling Offloading Optimization (JSO) algorithm is proposed to jointly optimize the sampling intervals and offloading decisions. It is a block coordinate descent-based algorithm where an optimal sampling-interval subalgorithm is used to update the sampling intervals, and an interference-aware best-response offloading subalgorithm is proposed to update the offloading decisions alternately. Finally, a comprehensive simulation is performed, confirming that the MAoI metric effectively quantifies multimodal freshness compared to traditional AoI, and the JSO algorithm significantly minimizes the average MAoI compared to state-of-the-art algorithms."}
{"id": "2602.14611", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2602.14611", "abs": "https://arxiv.org/abs/2602.14611", "authors": ["Shirin Pirouzkhah", "Pavlína Wurzel Gonçalves", "Alberto Bacchelli"], "title": "The Value of Effective Pull Request Description", "comment": null, "summary": "In the pull-based development model, code contributions are submitted as pull requests (PRs) to undergo reviews and approval by other developers with the goal of being merged into the code base. A PR can be supported by a description, whose role has not yet been systematically investigated. To fill in this gap, we conducted a mixed-methods empirical study of PR descriptions. We conducted a grey literature review of guidelines on writing PR descriptions and derived a taxonomy of eight recommended elements. Using this taxonomy, we analyzed 80K GitHub PRs across 156 projects and five programming languages to assess associations between these elements and code review outcomes (e.g., merge decision, latency, first response time, review comments, and review iteration cycles). To complement these results, we surveyed 64 developers about the perceived importance of each element. Finally, we analyzed which submission-time factors predict whether PRs include a description and which elements they contain. We found that developers view PR descriptions as important, but their elements matter differently: purpose and code explanations are valued by developers for preserving the rationale and history of changes, while stating the desired feedback type best predicts change acceptance and reviewer engagement. PR descriptions are also more common in mature projects and complex changes, suggesting they are written when most useful rather than as a formality."}
{"id": "2602.13277", "categories": ["cs.NI", "cs.DC"], "pdf": "https://arxiv.org/pdf/2602.13277", "abs": "https://arxiv.org/abs/2602.13277", "authors": ["Uma Mahesh Boda", "Mallikharjuna Rao Nuka"], "title": "Intent-driven Diffusion-based Path for Mobile Data Collector in IoT-enabled Dense WSNs", "comment": null, "summary": "Mobile data collection using controllable sinks is an effective approach to improve energy efficiency and data freshness in densely deployed wireless sensor networks (WSNs). However, existing path-planning methods are often heuristic-driven and lack the flexibility to adapt to high-level operational objectives under dynamic network conditions. In this paper, we propose ID2P2, a intent-driven diffusion-based path planning framework for jointly addresses rendezvous point selection and mobile data collector (MDC) tour construction in IoT-enabled dense WSNs. High-level intents, such as latency minimization, energy balancing, or coverage prioritization, are explicitly modeled and incorporated into a generative diffusion planning process that produces feasible and adaptive data collection trajectories. The proposed approach learns a trajectory prior that captures spatial node distribution and network characteristics, enabling the MDC to generate paths that align with specified intents while maintaining collision-free and energy-aware operation. Extensive simulations are conducted to evaluate the effectiveness of the proposed framework against conventional path-planning baselines. The results demonstrate that ID2P2 consistently outperforms representative baselines, achieving up to 25-30% reduction in tour completion time and travel overhead, approximately 10-30% improvement in data freshness, and 15-30% gains in energy efficiency and packet delivery performance, while maintaining higher throughput and fairness as network density increases, confirming its robustness and scalability for WSNs."}
{"id": "2602.14690", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2602.14690", "abs": "https://arxiv.org/abs/2602.14690", "authors": ["Matthias Galster", "Seyedmoein Mohsenimofidi", "Jai Lal Lulla", "Muhammad Auwal Abubakar", "Christoph Treude", "Sebastian Baltes"], "title": "Configuring Agentic AI Coding Tools: An Exploratory Study", "comment": "9 pages, 7 figures, 3 tables", "summary": "Agentic AI coding tools with autonomous capabilities beyond conversational content generation increasingly automate repetitive and time-consuming software development tasks. Developers can configure these tools through versioned repository-level artifacts such as Markdown and JSON files. In this paper, we present a systematic analysis of configuration mechanisms for agentic AI coding tools, covering Claude Code, GitHub Copilot, Cursor, Gemini, and Codex. We identify eight configuration mechanisms and, in an empirical study of 2,926 GitHub repositories, examine whether and how they are adopted. We then explore Context Files, Skills, and Subagents, that is, three mechanisms available across tools, in more detail. Our findings reveal three trends. First, Context Files dominate the configuration landscape and are often the sole mechanism in a repository, with AGENTS$.$md emerging as an interoperable standard across tools. Second, advanced mechanisms such as Skills and Subagents are only shallowly adopted: most repositories define only one or two artifacts, and Skills predominantly rely on static instructions rather than executable workflows. Third, distinct configuration cultures are forming around different tools, with Claude Code users employing the broadest range of mechanisms. These findings establish an empirical baseline for longitudinal and experimental research on how configuration strategies evolve and affect agent performance as agentic AI coding tools mature."}
{"id": "2602.13282", "categories": ["cs.NI", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.13282", "abs": "https://arxiv.org/abs/2602.13282", "authors": ["Ziyi Li", "Hui Ma", "Fei Xing", "Chunjiong Zhang", "Ming Yan"], "title": "GraFSTNet: Graph-based Frequency SpatioTemporal Network for Cellular Traffic Prediction", "comment": "submitted in a conference", "summary": "With rapid expansion of cellular networks and the proliferation of mobile devices, cellular traffic data exhibits complex temporal dynamics and spatial correlations, posing challenges to accurate traffic prediction. Previous methods often focus predominantly on temporal modeling or depend on predefined spatial topologies, which limits their ability to jointly model spatio-temporal dependencies and effectively capture periodic patterns in cellular traffic. To address these issues, we propose a cellular traffic prediction framework that integrates spatio-temporal modeling with time-frequency analysis. First, we construct a spatial modeling branch to capture inter-cell dependencies through an attention mechanism, minimizing the reliance on predefined topological structures. Second, we build a time-frequency modeling branch to enhance the representation of periodic patterns. Furthermore, we introduce an adaptive-scale LogCosh loss function, which adjusts the error penalty based on traffic magnitude, preventing large errors from dominating the training process and helping the model maintain relatively stable prediction accuracy across different traffic intensities. Experiments on three open-sourced datasets demonstrate that the proposed method achieves prediction performance superior to state-of-the-art approaches."}
{"id": "2602.14878", "categories": ["cs.SE", "cs.ET"], "pdf": "https://arxiv.org/pdf/2602.14878", "abs": "https://arxiv.org/abs/2602.14878", "authors": ["Mohammed Mehedi Hasan", "Hao Li", "Gopi Krishnan Rajbahadur", "Bram Adams", "Ahmed E. Hassan"], "title": "Model Context Protocol (MCP) Tool Descriptions Are Smelly! Towards Improving AI Agent Efficiency with Augmented MCP Tool Descriptions", "comment": null, "summary": "The Model Context Protocol (MCP) standardizes how Foundation Model (FM)-based agents interact with external systems by invoking tools. However, to understand a tool's purpose and features, FMs rely on natural-language tool descriptions, making these descriptions a critical component in guiding FMs to select the optimal tool for a given (sub)task and to pass the right arguments to the tool. While defects or smells in these descriptions can misguide FM-based agents, their prevalence and consequences in the MCP ecosystem remain unclear.\n  To address this, we conduct the first large-scale empirical study of 856 tools spread across 103 MCP servers, assessing their description quality and their impact on agent performance. We identify six components of tool descriptions from the literature, develop a scoring rubric utilizing these components, then formalize tool description smells based on this rubric. By operationalizing this rubric through an FM-based scanner, we find that 97.1% of the analyzed tool descriptions contain at least one smell, with 56% failing to state their purpose clearly. While augmenting these descriptions for all components improves task success rates by a median of 5.85 percentage points and improves partial goal completion by 15.12%, it also increases the number of execution steps by 67.46% and regresses performance in 16.67% of cases. These findings highlight a trade-off between agent performance and cost, as well as the context sensitivity of the performance gain. Furthermore, component ablations show that compact variants of different component combinations often preserve behavioral reliability while reducing unnecessary token overhead, enabling more efficient use of the FM context window and lower execution costs."}
{"id": "2602.13288", "categories": ["cs.NI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.13288", "abs": "https://arxiv.org/abs/2602.13288", "authors": ["Mohammad Saiful Islam", "Andriy Miranskyy"], "title": "Benchmarking Anomaly Detection Across Heterogeneous Cloud Telemetry Datasets", "comment": null, "summary": "Anomaly detection is important for keeping cloud systems reliable and stable. Deep learning has improved time-series anomaly detection, but most models are evaluated on one dataset at a time. This raises questions about whether these models can handle different types of telemetry, especially in large-scale and high-dimensional environments.\n  In this study, we evaluate four deep learning models, GRU, TCN, Transformer, and TSMixer. We also include Isolation Forest as a classical baseline. The models are tested across four telemetry datasets: the Numenta Anomaly Benchmark, Microsoft Cloud Monitoring dataset, Exathlon dataset, and IBM Console dataset. These datasets differ in structure, dimensionality, and labelling strategy. They include univariate time series, synthetic multivariate workloads, and real-world production telemetry with over 100,000 features.\n  We use a unified training and evaluation pipeline across all datasets. The evaluation includes NAB-style metrics to capture early detection behaviour for datasets where anomalies persist over contiguous time intervals. This enables window-based scoring in settings where anomalies occur over contiguous time intervals, even when labels are recorded at the point level. The unified setup enables consistent analysis of model behaviour under shared scoring and calibration assumptions.\n  Our results demonstrate that anomaly detection performance in cloud systems is governed not only by model architecture, but critically by calibration stability and feature-space geometry. By releasing our preprocessing pipelines, benchmark configuration, and evaluation artifacts, we aim to support reproducible and deployment-aware evaluation of anomaly detection systems for cloud environments."}
{"id": "2602.13290", "categories": ["cs.NI", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.13290", "abs": "https://arxiv.org/abs/2602.13290", "authors": ["Rodrigo Moreira", "Larissa Ferreira Rodrigues Moreira", "Maycon Peixoto", "Flavio De Oliveira Silva"], "title": "AGORA: Agentic Green Orchestration Architecture for Beyond 5G Networks", "comment": null, "summary": "Effective management and operational decision-making for complex mobile network systems present significant challenges, particularly when addressing conflicting requirements such as efficiency, user satisfaction, and energy-efficient traffic steering. The literature presents various approaches aimed at enhancing network management, including the Zero-Touch Network (ZTN) and Self-Organizing Network (SON); however, these approaches often lack a practical and scalable mechanism to consider human sustainability goals as input, translate them into energy-aware operational policies, and enforce them at runtime. In this study, we address this gap by proposing the AGORA: Agentic Green Orchestration Architecture for Beyond 5G Networks. AGORA embeds a local tool-augmented Large Language Model (LLM) agent in the mobile network control loop to translate natural-language sustainability goals into telemetry-grounded actions, actuating the User Plane Function (UPF) to perform energy-aware traffic steering. The findings indicate a strong latency-energy coupling in tool-driven control loops and demonstrate that compact models can achieve a low energy footprint while still facilitating correct policy execution, including non-zero migration behavior under stressed Multi-access Edge Computing (MEC) conditions. Our approach paves the way for sustainability-first, intent-driven network operations that align human objectives with executable orchestration in Beyond-5G infrastructures."}
{"id": "2602.13307", "categories": ["cs.NI"], "pdf": "https://arxiv.org/pdf/2602.13307", "abs": "https://arxiv.org/abs/2602.13307", "authors": ["Ning Yang", "Wentao Wang", "Lingtao Ouyang", "Haijun Zhang"], "title": "Cooperative Edge Caching with Large Language Model in Wireless Networks", "comment": null, "summary": "Cooperative edge caching in overlapping zones creates intricate coupling among Base Station (BS) decisions, making content replacement highly sensitive to topology and temporal reuse. While heuristics are often myopic and Deep Reinforcement Learning lacks robustness under dynamics, this paper proposes a Large Language Model (LLM)-based multi-BS orchestrator. The LLM acts as the sole autonomous engine, interacting with the environment via a validated text-to-action interface. Each time slot, the system renders environmental states -- including cache inventories and frequency statistics -- into prompts, parsing LLM-generated decisions against strict feasibility constraints. We align the model through a two-stage paradigm: Supervised Fine-Tuning on oracle trajectories for syntax and initialization, followed by Group Relative Policy Optimization. The latter employs an ``opportunity-aware'' reward that prioritizes multi-step cooperative gains relative to a No-Operation baseline. Evaluated on identical request traces, the orchestrator approaches exhaustive-search performance (0.610 vs.\\ 0.617 in a 5-BS scenario), outperforms classical baselines (e.g., +4.1\\% over least-frequently used), and demonstrates robust zero-shot transfer across varying cache capacities, library sizes, and user densities."}
{"id": "2602.13311", "categories": ["cs.NI"], "pdf": "https://arxiv.org/pdf/2602.13311", "abs": "https://arxiv.org/abs/2602.13311", "authors": ["Shuo Zhu", "Siyu Lin", "Zijing Wang", "Qiao Ren", "Xiaoheng Deng", "Bo Ai"], "title": "Resilient and Freshness-Aware Scheduling for Industrial Multi-Hop IAB Networks: A Packet Duplication Approach", "comment": "6 pages, 6 figures, conference", "summary": "In industrial millimeter-wave (mmWave) multi-hop Integrated Access and Backhaul (IAB) networks, dynamic blockages caused by moving obstacles pose a severe threat to robust and continuous networks. While Packet Duplication (PD) enhances reliability by path diversity, it inevitably doubles the traffic load, leading to severe congestion and degraded Age of Information (AoI). To navigate this reliability-congestion trade-off, we formulated an optimization problem in a multi-hop IAB scenario that minimizes the average AOI while satisfying strict queue stability constraints. We utilize Lyapunov optimization to transform the long-term stochastic optimization problem into tractable deterministic sub-problems. To solve these sub-problems efficiently, we propose a Resilient and Freshness-Aware Scheduling (RFAS) algorithm. Simulation results show that in blockage-prone environments, RFAS significantly outperforms baselines by maintaining a Packet Delivery Ratio (PDR) above 95\\%. Crucially, it strictly guarantees queue stability under hard buffer constraints, whereas baselines suffer from buffer overflows. Furthermore, RFAS reduces the network load imbalance by 19\\% compared to the baseline in high-frequency traffic scenarios. This confirms RFAS as a robust and sustainable solution for real-time industrial control loops."}
{"id": "2602.13316", "categories": ["cs.NI", "cs.AI", "cs.ET"], "pdf": "https://arxiv.org/pdf/2602.13316", "abs": "https://arxiv.org/abs/2602.13316", "authors": ["Nour Hello", "Mohamed Amine Hamoura", "Francois Rivet", "Emilio Calvanese Strinati"], "title": "Semantic Waveforms for AI-Native 6G Networks", "comment": null, "summary": "In this paper, we propose a semantic-aware waveform design framework for AI-native 6G networks that jointly optimizes physical layer resource usage and semantic communication efficiency and robustness, while explicitly accounting for the hardware constraints of RF chains. Our approach, called Orthogonal Semantic Sequency Division Multiplexing (OSSDM), introduces a parametrizable, orthogonal-base waveform design that enables controlled degradation of the wireless transmitted signal to preserve semantically significant content while minimizing resource consumption. We demonstrate that OSSDM not only reinforces semantic robustness against channel impairments but also improves semantic spectral efficiency by encoding meaningful information directly at the waveform level. Extensive numerical evaluations show that OSSDM outperforms conventional OFDM waveforms in spectral efficiency and semantic fidelity. The proposed semantic waveform co-design opens new research frontiers for AI-native, intelligent communication systems by enabling meaning-aware physical signal construction through the direct encoding of semantics at the waveform level."}
{"id": "2602.13340", "categories": ["cs.NI", "cs.ET"], "pdf": "https://arxiv.org/pdf/2602.13340", "abs": "https://arxiv.org/abs/2602.13340", "authors": ["Mengyuan Wang", "Haopeng Wang", "Haiwei Dong", "Abdulmotaleb El Saddik"], "title": "3D Wi-Fi Signal Measurement in Realistic Digital Twin Testbed Environments Using Ray Tracing", "comment": "accepted by IEEE Transactions on Instrumentation and Measurement", "summary": "Accurate and efficient modeling of indoor wireless signal propagation is crucial for the deployment of next-generation Wi-Fi. This paper presents a digital twin-based measurement system that integrates real-world 3D environment reconstruction with deterministic ray tracing for physically grounded electromagnetic modeling. Building geometry is obtained through LiDAR scanning, followed by object segmentation and assignment of ITU-R standard material parameters. The propagation process is simulated with a GPU-accelerated ray-tracing engine that generates path-level channel attributes, including delay, power, angular dispersion, and Ricean K-factor. Under identical runtime constraints, the proposed system is evaluated against a commercial measurement simulator, demonstrating up to 21 dB higher path gain and consistently improved signal-to-interference-plus-noise ratio in line-of-sight conditions. Additionally, experiments against onsite RSSI measurements confirm a high spatial correlation of 0.98 after calibration, proving the system's fidelity in real-world settings. Furthermore, coverage analysis across 2.4 GHz, 5 GHz, and 6 GHz bands demonstrates the capability of system to model frequency-dependent material attenuation for Wi-Fi 6E/7 networks. Finally, the system offers interactive 3D visualization and on-demand data extraction, highlighting its potential for digital twin-driven wireless system design and optimization."}
{"id": "2602.13358", "categories": ["cs.NI", "cs.RO"], "pdf": "https://arxiv.org/pdf/2602.13358", "abs": "https://arxiv.org/abs/2602.13358", "authors": ["Christopher Schahn", "Jorin Kouril", "Bernd Schaeufele", "Ilja Radusch"], "title": "Location as a service with a MEC architecture", "comment": "Published and presented at 2024 International Conference on Information Networking (ICOIN)", "summary": "In recent years, automated driving has become viable, and advanced driver assistance systems (ADAS) are now part of modern cars. These systems require highly precise positioning. In this paper, a cooperative approach to localization is presented. The GPS information from several road users is collected in a Mobile Edge Computing cloud, and the characteristics of GNSS positioning are used to provide lane-precise positioning for all participants by applying probabilistic filters and HD maps."}
{"id": "2602.13369", "categories": ["cs.NI"], "pdf": "https://arxiv.org/pdf/2602.13369", "abs": "https://arxiv.org/abs/2602.13369", "authors": ["Nicolas Tacheny"], "title": "Parametric Traversal for Multi-Dimensional Cost-Aware Graph Reasoning", "comment": null, "summary": "Classical path search assumes complete graphs and scalar optimization metrics, yet real infrastructure networks are incomplete and require multi-dimensional evaluation. We introduce the concept of traversal: a generalization of paths that combines existing edges with gap transitions, missing but acceptable connections representing links that can be built. This abstraction captures how engineers actually reason about infrastructure: not just what exists, but what can be realized.\n  We present a parametric framework that treats planned connections as first-class transitions, scales to large graphs through efficient candidate filtering, and uses multi-dimensional criteria to decide whether a traversal should continue to be explored or be abandoned. We evaluate the framework through representative scenarios in datacenter circuit design and optical route construction in telecommunication networks, demonstrating conditional feasibility, non-scalarizable trade-offs, and policy calibration capabilities beyond the reach of classical formulations."}
{"id": "2602.13439", "categories": ["cs.NI"], "pdf": "https://arxiv.org/pdf/2602.13439", "abs": "https://arxiv.org/abs/2602.13439", "authors": ["Qiaomei Han", "Xianbin Wang", "Minghui Liwang", "Dusit Niyato"], "title": "Spatiotemporal Feature Alignment and Weighted Fusion in Collaborative Perception Enabled by Network Synchronization and Age of Information", "comment": null, "summary": "Collaborative perception in Internet of Vehicles (IoV) aggregates multi-vehicle observations for broader scene coverage and improved decision-making. However, fusion quality degrades under spatiotemporal heterogeneity from unsynchronized clocks, communication delays, and motion variations across vehicles. Prior work mitigates these through spatial transformations or fixed time-offset corrections, overlooking time-varying clock drifts and delays that cause persistent feature misalignment. To overcome these, we propose a spatiotemporal feature alignment and weighted fusion framework. Specifically, network synchronization is designed to continuously compensate for clock state differences between vehicles and establish a common time reference, onto which all feature timestamps can be mapped. After synchronization, to align the freshness of received features since their generation, their Age of Information (AoI) is determined by estimating network delay with given feature size and link quality. Our spatiotemporal feature alignment then projects vehicles' features into one spatial coordinate and corrects them to a synchronized fusion instant using AoIs, enabling all features to describe the scene coherently. Furthermore, due to varying synchronization and alignment quality, we estimate their uncertainties and integrate with AoI to generate feature weights for efficient fusion, prioritizing fresh, reliable feature regions. Simulations show consistent perception accuracy improvements over strong baselines under clock drifts and link delays."}
{"id": "2602.13542", "categories": ["cs.NI", "cs.DC"], "pdf": "https://arxiv.org/pdf/2602.13542", "abs": "https://arxiv.org/abs/2602.13542", "authors": ["George M. Gichuru", "Zoe Aiyanna M. Cayetano"], "title": "SIDSense: Database-Free TV White Space Sensing for Disaster-Resilient Connectivity", "comment": "7 pages, 1 figure", "summary": "Small Island Developing States (SIDS) are disproportionately exposed to climate-driven disasters, yet often rely on fragile terrestrial networks that fail when they are most needed. TV White Space (TVWS) links offer long-range, low-power coverage; however, current deployments depend on Protocol to Access White Spaces (PAWS) database connectivity for channel authorization, creating a single point of failure during outages.\n  We present SIDSense, an edge AI framework for database-free TVWS operation that preserves regulatory intent through a compliance-gated controller, audit logging, and graceful degradation. SIDSense couples CNN-based spectrum classification with a hybrid sensing-first, authorization-as-soon-as-possible workflow and co-locates sensing and video enhancement with a private 5G stack on a maritime vessel to sustain situational-awareness video backhaul.\n  Field experiments in Barbados demonstrate sustained connectivity during simulated PAWS outages, achieving 94.2% sensing accuracy over 470-698 MHz with 23 ms mean decision latency, while maintaining zero missed 5G Layer-1 (L1) deadlines under GPU-aware scheduling. We release an empirical Caribbean TVWS propagation and occupancy dataset and look to contribute some of the components of the SIDSense pipeline to the open source community to accelerate resilient connectivity deployments in climate-vulnerable regions."}
{"id": "2602.13606", "categories": ["cs.NI", "cs.AI", "cs.ET", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.13606", "abs": "https://arxiv.org/abs/2602.13606", "authors": ["Muhammad Baqer Mollah", "Honggang Wang", "Mohammad Ataul Karim", "Hua Fang"], "title": "Multi-Modal Sensing and Fusion in mmWave Beamforming for Connected Vehicles: A Transformer Based Framework", "comment": "13 Pages. arXiv admin note: text overlap with arXiv:2509.11112", "summary": "Millimeter wave (mmWave) communication, utilizing beamforming techniques to address the inherent path loss limitation, is considered as one of the key technologies to support ever increasing high throughput and low latency demands of connected vehicles. However, adopting standard defined beamforming approach in highly dynamic vehicular environments often incurs high beam training overheads and reduction in the available airtime for communications, which is mainly due to exchanging pilot signals and exhaustive beam measurements. To this end, we present a multi-modal sensing and fusion learning framework as a potential alternative solution to reduce such overheads. In this framework, we first extract the representative features from the sensing modalities by modality specific encoders, then, utilize multi-head cross-modal attention to learn dependencies and correlations between different modalities, and subsequently fuse the multimodal features to obtain predicted top-k beams so that the best line-of-sight links can be proactively established. To show the generalizability of the proposed framework, we perform a comprehensive experiment in four different vehicle-to-infrastructure (V2I) and vehicle-to-vehicle (V2V) scenarios from real world multimodal and 60 GHz mmWave wireless sensing data. The experiment reveals that the proposed framework (i) achieves up to 96.72% accuracy on predicting top-15 beams correctly, (ii) incurs roughly 0.77 dB average power loss, and (iii) improves the overall latency and beam searching space overheads by 86.81% and 76.56% respectively for top-15 beams compared to standard defined approach."}
{"id": "2602.13607", "categories": ["cs.NI", "cs.IT"], "pdf": "https://arxiv.org/pdf/2602.13607", "abs": "https://arxiv.org/abs/2602.13607", "authors": ["You Zhou", "Qunsong Zeng", "Kaibin Huang"], "title": "Parametric-Sensitivity Aware Retransmission for Efficient AI Downloading", "comment": null, "summary": "The edge artificial intelligence (AI) applications in next-generation mobile networks demand efficient AI-model downloading techniques to support real-time, on-device inference. However, transmitting high-dimensional AI models over wireless channels remains challenging due to limited communication resources. To address this issue, we propose a parametric-sensitivity-aware retransmission (PASAR) framework that manages radio-resource usage of different parameter packets according to their importance on model inference accuracy, known as parametric sensitivity. Empirical analysis reveals a highly right-skewed sensitivity distribution, indicating that only a small fraction of parameters significantly affect model performance. Leveraging this insight, we design a novel online retransmission protocol, i.e., the PASAR protocol, that adaptively terminates packet transmission based on real-time bit error rate (BER) measurements and the associated parametric sensitivity. The protocol employs an adaptive, round-wise stopping criterion, enabling heterogeneous, packet-level retransmissions that preserve overall model functionality but reduce overall latency. Extensive experiments across diverse deep neural network architectures and real-world datasets demonstrate that PASAR substantially outperforms classical hybrid automatic repeat request (HARQ) schemes in terms of communication efficiency and latency."}
{"id": "2602.13628", "categories": ["cs.NI"], "pdf": "https://arxiv.org/pdf/2602.13628", "abs": "https://arxiv.org/abs/2602.13628", "authors": ["Ruichen Zhang", "Xiaofeng Luo", "Jiayi He", "Dusit Niyato", "Jiawen Kang", "Zehui Xiong", "Yonghui Li"], "title": "Compact LLM Deployment and World Model Assisted Offloading in Mobile Edge Computing", "comment": "16 pages, 10 figures", "summary": "This paper investigates compact large language model (LLM) deployment and world-model-assisted inference offloading in mobile edge computing (MEC) networks. We first propose an edge compact LLM deployment (ECLD) framework that jointly applies structured pruning, low-bit quantization, and knowledge distillation to construct edge-deployable LLM variants, and we evaluate these models using four complementary metrics: accessibility, energy consumption, hallucination rate, and generalization accuracy. Building on the resulting compact models, we formulate an MEC offloading optimization problem that minimizes the long-term average inference latency subject to per-device energy budgets and LLM-specific quality-of-service constraints on effective accuracy and hallucination. To solve this problem under unknown and time-varying network dynamics, we develop a world model-proximal policy optimization (PPO) algorithm, which augments an on-policy PPO algorithm with a learned recurrent world model that provides improved value targets and short imagination rollouts. Extensive experiments on Llama-3.1-8B, Qwen3-8B, and Mistral-12B show that ECLD compresses base models by about 70-80% in storage (i.e., from 15.3 GB to 3.3 GB for Llama-3.1-8B) and reduces per-query energy consumption by up to 50%, while largely preserving accuracy and often lowering hallucination compared with quantization-only or pruning-only baselines. Moreover, they also show that world model-PPO speeds up convergence by about 50%, improves the final reward by 15.8% over vanilla PPO, and reduces average inference latency by 12-30% across different user populations, while satisfying the accuracy and hallucination constraints and approaching the generation quality of always-offloading with much of the efficiency of local execution."}
{"id": "2602.13672", "categories": ["cs.NI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.13672", "abs": "https://arxiv.org/abs/2602.13672", "authors": ["Md. Kamrul Hossain", "Walid Aljoby"], "title": "LEAD-Drift: Real-time and Explainable Intent Drift Detection by Learning a Data-Driven Risk Score", "comment": "Copyright 2026 IEEE. Accepted for publication in IEEE ICC 2026. This is a author version. 6 pages", "summary": "Intent-Based Networking (IBN) simplifies network management, but its reliability is challenged by \"intent drift\", where the network's state gradually deviates from its intended goal, often leading to silent failures. Conventional approaches struggle to detect the subtle, early stages of intent drift, raising alarms only when degradation is significant and failure is imminent, which limits their effectiveness for proactive assurance. To address this, we propose LEAD-Drift, a framework that detects intent drift in real time to enable proactive failure prevention. LEAD-Drift's core contribution is reformulating intent failure detection as a supervised learning problem by training a lightweight neural network on fixed-horizon labels to predict a future risk score. The model's raw output is then smoothed with an Exponential Moving Average (EMA) and passed through a statistically tuned threshold to generate robust, real-time alerts. Furthermore, we enhance the framework with two key features for operational intelligence: a multi-horizon modeling technique for dynamic time-to-failure estimation, and per-alert explainability using SHAP to identify root-cause KPIs. Our evaluation on a time-series dataset shows LEAD-Drift provides significantly earlier warnings, improving the average lead time by 7.3 minutes (+17.8\\%) compared to a distance-based baseline. It also reduces alert noise by 80.2\\% compared to a weighted-KPI heuristic, with only a minor trade-off in lead time. These results demonstrate that LEAD-Drift as a highly effective, interpretable, and operationally efficient solution for proactive network assurance in IBN."}
{"id": "2602.13795", "categories": ["cs.NI"], "pdf": "https://arxiv.org/pdf/2602.13795", "abs": "https://arxiv.org/abs/2602.13795", "authors": ["Wenxin Xu", "Taotao Wang", "Yihan Xia", "Shengli Zhang", "Soung Chang Liew"], "title": "Agent-OSI: A Layered Protocol Stack Toward a Decentralized Internet of Agents", "comment": "7 pages, 3 figures", "summary": "Large Language Models (LLMs) are accelerating the shift from an Internet of information to an Internet of Agents (IoA), where autonomous entities discover services, negotiate, execute tasks, and exchange value. Yet today's agents are still confined to platform silos and proprietary interfaces, lacking a common stack for interoperability, trust, and pay-per-use settlement. This article proposes \\textit{Agent-OSI}, a six-layer reference stack for decentralized agent networking built on top of the existing Internet. Agent-OSI combines secure connectivity and A2A messaging, decentralized identity and authorization, settlement and metering, verifiable execution and provenance, and semantic interoperability for orchestration. In particular, we treat HTTP 402 (Payment Required) as an application-level payment challenge (analogous to HTTP 401 for authentication) that triggers escrow-based settlement and verifiable receipts (instantiated via a blockchain escrow in our prototype), rather than introducing a new network-layer protocol. We implement a prototype and evaluate cost and latency. Results show that keeping negotiation and delivery off-chain while preserving verifiable settlement reduces on-chain session costs by approximately 51\\% compared with a standard Web3 baseline in our prototype setting, and that blockchain confirmation latency is often not the dominant factor for generative workloads."}
{"id": "2602.13868", "categories": ["cs.NI", "cs.IR"], "pdf": "https://arxiv.org/pdf/2602.13868", "abs": "https://arxiv.org/abs/2602.13868", "authors": ["Udhaya Srinivasan", "Weisi Guo"], "title": "Agentic Assistant for 6G: Turn-based Conversations for AI-RAN Hierarchical Co-Management", "comment": "submitted to IEEE conference", "summary": "New generations of radio access networks (RAN), especially with native AI services are increasingly difficult for human engineers to manage in real-time. Enterprise networks are often managed locally, where expertise is scarce. Existing research has focused on creating Retrieval-Augmented Generation (RAG) LLMs that can help to plan and configure RAN and core aspects only. Co-management of RAN and edge AI is the gap, which creates hierarchical and dynamic problems that require turn-based human interactions. Here, we create an agentic network manager and turn-based conversation assistant that can understand human intent-based queries that match hierarchical problems in AI-RAN. The framework constructed consists of: (a) a user interface and evaluation dashboard, (b) an intelligence layer that interfaces with the AI-RAN, and (c) a knowledge layer for providing the basis for evaluations and recommendations. These form 3 layers of capability with the following validation performances (average response time 13s): (1) design and planning a service (78\\% accuracy), (2) operating specific AI-RAN tools (89\\% accuracy), and (3) tuning AI-RAN performance (67\\%). These initial results indicate the universal challenges of hallucination but also fast response performance success that can really reduce OPEX costs for small scale enterprise users."}
{"id": "2602.14117", "categories": ["cs.NI", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.14117", "abs": "https://arxiv.org/abs/2602.14117", "authors": ["Hojjat Navidan", "Mohammad Cheraghinia", "Jaron Fontaine", "Mohamed Seif", "Eli De Poorter", "H. Vincent Poor", "Ingrid Moerman", "Adnan Shahid"], "title": "Toward Autonomous O-RAN: A Multi-Scale Agentic AI Framework for Real-Time Network Control and Management", "comment": "Submitted to the IEEE Networks Journal", "summary": "Open Radio Access Networks (O-RAN) promise flexible 6G network access through disaggregated, software-driven components and open interfaces, but this programmability also increases operational complexity. Multiple control loops coexist across the service management layer and RAN Intelligent Controller (RIC), while independently developed control applications can interact in unintended ways. In parallel, recent advances in generative Artificial Intelligence (AI) are enabling a shift from isolated AI models toward agentic AI systems that can interpret goals, coordinate multiple models and control functions, and adapt their behavior over time. This article proposes a multi-scale agentic AI framework for O-RAN that organizes RAN intelligence as a coordinated hierarchy across the Non-Real-Time (Non-RT), Near-Real-Time (Near-RT), and Real-Time (RT) control loops: (i) A Large Language Model (LLM) agent in the Non-RT RIC translates operator intent into policies and governs model lifecycles. (ii) Small Language Model (SLM) agents in the Near-RT RIC execute low-latency optimization and can activate, tune, or disable existing control applications; and (iii) Wireless Physical-layer Foundation Model (WPFM) agents near the distributed unit provide fast inference close to the air interface. We describe how these agents cooperate through standardized O-RAN interfaces and telemetry. Using a proof-of-concept implementation built on open-source models, software, and datasets, we demonstrate the proposed agentic approach in two representative scenarios: robust operation under non-stationary conditions and intent-driven slice resource control."}
{"id": "2602.14283", "categories": ["cs.NI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.14283", "abs": "https://arxiv.org/abs/2602.14283", "authors": ["Md. Kamrul Hossain", "Walid Aljoby"], "title": "MILD: Multi-Intent Learning and Disambiguation for Proactive Failure Prediction in Intent-based Networking", "comment": "Copyright 2026 IEEE. Accepted for presentation in IEEE/IFIP NOMS 2026", "summary": "In multi-intent intent-based networks, a single fault can trigger co-drift where multiple intents exhibit symptomatic KPI degradation, creating ambiguity about the true root-cause intent. We present MILD, a proactive framework that reformulates intent assurance from reactive drift detection to fixed-horizon failure prediction with intent-level disambiguation. MILD uses a teacher-augmented Mixture-of-Experts where a gated disambiguation module identifies the root-cause intent while per-intent heads output calibrated risk scores. On a benchmark with non-linear failures and co-drifts, MILD provides 3.8\\%--92.5\\% longer remediation lead time and improves intent-level root-cause disambiguation accuracy by 9.4\\%--45.8\\% over baselines. MILD also provides per-alert KPI explanations, enabling actionable diagnosis."}
{"id": "2602.14360", "categories": ["cs.NI"], "pdf": "https://arxiv.org/pdf/2602.14360", "abs": "https://arxiv.org/abs/2602.14360", "authors": ["Zuyuan Zhang", "Vaneet Aggarwal", "Tian Lan"], "title": "LiSFC-Search: Lifelong Search for Network SFC Optimization under Non-stationary Drifts", "comment": "This work has been accepted to the IEEE INFOCOM 2026 Workshop on CNC: Cloud-Network Convergence", "summary": "Edge-cloud convergence is reshaping service provisioning across 5G/6G and computing power networks (CPNs). Service function chaining (SFC) requires continuously placing and scheduling virtual network functions (VNFs) chains under compute/bandwidth and end-to-end QoS constraints. Most SFC optimizers assume static or stationary networks, and degrade under long-term topology/resource changes (failures, upgrades, expansions) that induce non-stationary graph drifts. We propose LiSFC, a Lipschitz lifelong planner that transfers MCTS statistics across drifting network configurations using an MDP-distance bound. More precisely, we formulate the problem as a sequence of MDPs indexed by the underlying network graph and constraints, and we define a \\emph{graph drift} metric that upper-bounds the LiZero MDP distance. This allows LiSFC to import theoretical guarantees on bias and sample efficiency from the LiZero framework while being tailored to cloud-network convergence. We then design \\emph{LiSFC-Search}, an SFC-aware unified MCTS (UMCTS) procedure that uses transferable adaptive UCT (aUCT) bonuses to reuse search statistics from prior CPN configurations. Preliminary results on synthetic CPN topologies and SFC workloads show that LiSFC consistently reduces SFC blocking probability and improves tail delay compared to non-transfer MCTS and purely learning-based baselines, highlighting its potential as an AI/ML building block for cloud-network convergence."}
{"id": "2602.14372", "categories": ["cs.NI"], "pdf": "https://arxiv.org/pdf/2602.14372", "abs": "https://arxiv.org/abs/2602.14372", "authors": ["Wenbin Wu", "Alexander Neumueller"], "title": "Bitcoin Under Stress: Measuring Infrastructure Resilience 2014-2025", "comment": "8 pages, 6 figures. Submitted to IEEE ICBC 2026", "summary": "Bitcoin's design promises resilience through decentralization, yet physical infrastructure creates hidden dependencies. We present the first longitudinal study of Bitcoin's resilience to infrastructure failures using 11 years of P2P network data (2014-2025), 658 submarine cables, and 68 verified cable fault events. Using a Buldyrev-style cascade model with a country-level physical layer (225 countries, 354 submarine cable edges, 325 land border edges), we find that Bitcoin's clearnet percolation threshold $p_c \\approx 0.72$-$0.92$ for random cable failures, declining 22% from $p_c \\approx 0.92$ (2014-2017) to a minimum of $p_c = 0.72$ in 2021 during peak mining concentration. Targeted attacks reduce $p_c$ to 0.05-0.20. To address the 64% of nodes using Tor with unobservable locations, we develop a 4-layer multiplex model incorporating Tor relay infrastructure. Tor relay bandwidth concentrates in well-connected European countries, increasing resilience by $Δp_c \\approx +0.02$-$+0.10$ rather than introducing fragility. Empirical validation shows 87% of cable faults caused less than 5% node impact. We contribute: (1) a multiplex percolation framework for overlay-underlay coupling with a 4-layer Tor relay model; (2) the first empirical measurement of Bitcoin's physical-layer resilience over a decade; and (3) evidence that Tor adoption amplifies resilience with distributional bounds under partial observability."}
{"id": "2602.14390", "categories": ["cs.NI"], "pdf": "https://arxiv.org/pdf/2602.14390", "abs": "https://arxiv.org/abs/2602.14390", "authors": ["Bahar Mojtabaei Ranani", "Mahmood Ahmadi", "Sajad Ahmadian"], "title": "A Q-Learning Approach for Dynamic Resource Management in Three-Tier Vehicular Fog Computing", "comment": null, "summary": "In this paper, a method for predicting the resources required for an intelligent vehicle client using a three-layer vehicular computing architecture is proposed. This method leverages Q-Learning to optimize resource allocation and enhance overall system performance. This approach employs reinforcement learning capabilities to provide a dynamic and adaptive strategy for resource management in a fog computing environment. The key findings of this study indicate that Q-learning can effectively predict the appropriate allocation of resources by learning from past experiences and making informed decisions. Through continuous training and updating of the Q-learning agent, the system can adapt to changing conditions and make resource allocation decisions based on real-time information. The experimental results demonstrate the effectiveness of the proposed method in optimizing resource allocation. The Q-learning agent predicts the optimal values for memory, bandwidth, and processor. These predictions not only minimize resource consumption but also meet the performance requirements of the fog system. Implementations show that this method improves the average task processing time in compared to other methods evaluated in this study"}
{"id": "2602.14391", "categories": ["cs.NI"], "pdf": "https://arxiv.org/pdf/2602.14391", "abs": "https://arxiv.org/abs/2602.14391", "authors": ["Ali Salimi", "Saadat Izadi", "Mahmood Ahmadi", "Hadi Tabatabaee Malazi"], "title": "ASA: Adaptive Smart Agent Federated Learning via Device-Aware Clustering for Heterogeneous IoT", "comment": null, "summary": "Federated learning (FL) has become a promising answer to facilitating privacy-preserving collaborative learning in distributed IoT devices. However, device heterogeneity is a key challenge because IoT networks include devices with very different computational powers, memory availability, and network environments. To this end, we introduce ASA (Adaptive Smart Agent). This new framework clusters devices adaptively based on real-time resource profiles and adapts customized models suited to every cluster's capability. ASA capitalizes on an intelligent agent layer that examines CPU power, available memory, and network environment to categorize devices into three levels: high-performance, mid-tier, and low-capability. Each level is provided with a model tuned to its computational power to ensure inclusive engagement across the network. Experimental evaluation on two benchmark datasets, MNIST and CIFAR-10, proves that ASA decreases communication burden by 43% to 50%, improves resource utilization by 43%, and achieves final model accuracies of 98.89% on MNIST and 85.30% on CIFAR-10. These results highlight ASA's efficacy in enhancing efficiency, scalability, and fairness in heterogeneous FL environments, rendering it a suitable answer for real-world IoT apps."}
