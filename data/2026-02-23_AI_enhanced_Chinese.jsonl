{"id": "2602.17678", "categories": ["cs.DC", "cs.CR", "cs.SE"], "pdf": "https://arxiv.org/pdf/2602.17678", "abs": "https://arxiv.org/abs/2602.17678", "authors": ["Oreofe Solarin"], "title": "It's Not Just Timestamps: A Study on Docker Reproducibility", "comment": null, "summary": "Reproducible container builds promise a simple integrity check for software supply chains: rebuild an image from its Dockerfile and compare hashes. We build a Docker measurement pipeline and apply it to a stratified sample of 2,000 GitHub repositories that contained a Dockerfile. We found that only 56% produce any buildable image, and just 2.7% of those are bitwise reproducible without any infrastructure configurations. After modifying infrastructure configurations, we raise bitwise reproducibility by 18.6%, but 78.7% of buildable Dockerfiles remain non-reproducible. We analyze the root causes of the remaining differences, and find that beyond timestamps and metadata, developer-controlled choices such as uncleaned caches, logs, documentation, and floating versions are dominant causes of non-reproducibility. We derive concrete Dockerfile guidelines from these patterns and discuss how they can inform future linters and Continuous Integration (CI) checks for reproducible containers.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\u5728\u65e0\u9700\u914d\u7f6e\u8c03\u6574\u65f6\uff0c\u4ec52.7%\u7684Dockerfile\u6784\u5efa\u53ef\u5b9e\u73b0\u6bd4\u7279\u7ea7\u590d\u73b0\uff1b\u914d\u7f6e\u4f18\u5316\u540e\u590d\u73b0\u7387\u63d0\u534718.6%\uff0c\u4f46\u4ecd\u670978.7%\u65e0\u6cd5\u590d\u73b0\uff0c\u5f00\u53d1\u8005\u64cd\u4f5c\u662f\u4e3b\u8981\u539f\u56e0\u3002", "motivation": "\u9a8c\u8bc1Docker\u6784\u5efa\u4f5c\u4e3a\u8f6f\u4ef6\u4f9b\u5e94\u94fe\u5b8c\u6574\u6027\u68c0\u67e5\u7684\u53ef\u884c\u6027\uff0c\u63a2\u7d22\u5bb9\u5668\u955c\u50cf\u54c8\u5e0c\u4e00\u81f4\u6027\u5728\u5b9e\u9645\u9879\u76ee\u4e2d\u7684\u5b9e\u73b0\u60c5\u51b5\u3002", "method": "\u6784\u5efaDocker\u6d4b\u91cf\u7ba1\u9053\uff0c\u5bf9GitHub\u4e2d2000\u4e2a\u542bDockerfile\u7684\u4ed3\u5e93\u8fdb\u884c\u5206\u5c42\u62bd\u6837\u6d4b\u8bd5\uff0c\u5206\u6790 activit\u00e9\u56fe\u50cf\u6784\u5efa\u6210\u529f\u7387\u4e0e\u6bd4\u7279\u590d\u73b0\u7387\u3002", "result": "\u4ec556%\u4ed3\u5e93\u80fd\u6210\u529f\u6784\u5efa\u955c\u50cf\uff0c\u57fa\u7840\u73af\u5883\u4e0b\u4ec52.7%\u53ef\u6bd4\u7279\u590d\u73b0\uff1b\u8c03\u6574\u57fa\u7840\u8bbe\u65bd\u914d\u7f6e\u540e\u590d\u73b0\u7387\u63d0\u9ad818.6%\uff0c\u4f46\u603b\u5931\u8d25\u7387\u8fbe78.7%\u3002\u4e3b\u8981\u6545\u969c\u6e90\u4e8e\u65f6\u95f4\u6233\u3001\u7f13\u5b58\u6b8b\u7559\u53ca\u5f00\u53d1\u8005\u51b3\u7b56\u3002", "conclusion": "\u63d0\u51faDockerfile\u7f16\u5199\u6307\u5357\uff0c\u5efa\u8bae\u901a\u8fc7\u6539\u8fdb\u5f00\u53d1\u89c4\u8303\u548c\u96c6\u6210CI\u5de5\u5177\uff08\u5982linters\uff09\u63d0\u5347\u5bb9\u5668\u590d\u73b0\u6027\uff0c\u89e3\u51b3\u65e5\u5fd7\u3001\u7248\u672c\u6d6e\u52a8\u7b49\u53ef\u63a7\u5236\u56e0\u7d20\u3002"}}
{"id": "2602.17774", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2602.17774", "abs": "https://arxiv.org/abs/2602.17774", "authors": ["Wael Al-Manasrah", "Zuhair AlSader", "Tim Brecht", "Ahmed Alquraan", "Samer Al-Kiswany"], "title": "Message-Oriented Middleware Systems: Technology Overview", "comment": null, "summary": "We present a comprehensive characterization study of open-source message-oriented middleware (MOM) systems. We followed a rigorous methodology to select and study ten popular and diverse MOM systems. For each system, we examine 42 features with a total of 134 different options. We found that MOM systems have evolved to provide a framework for modern cloud applications through high flexibility and configurability and by offering core building blocks for complex applications including transaction support, active messaging, resource management, flow control, and native support for multi-tenancy. We also identify that there is an opportunity for the community to consolidate its efforts on fewer open-source projects.\n  We have also created an annotated data set that makes it easy to verify our findings, which can also be used to help practitioners and developers understand and compare the features of different systems. For a wider impact, we make our data set publicly available.", "AI": {"tldr": "\u5bf910\u79cd\u5f00\u6e90\u6d88\u606f\u4e2d\u95f4\u4ef6\u7cfb\u7edf\u7684\u7814\u7a76\u63ed\u793a\u5176\u5411\u4e91\u5e94\u7528\u6846\u67b6\u7684\u6f14\u8fdb\u53ca\u5f00\u6e90\u793e\u533a\u6574\u5408\u5951\u673a\u3002", "motivation": "\u65e8\u5728\u7cfb\u7edf\u8bc4\u4f30\u5f00\u6e90MOM\u7cfb\u7edf\u7279\u6027\uff0c\u4e3a\u5f00\u53d1\u8005\u548c\u4ece\u4e1a\u8005\u63d0\u4f9b\u7279\u6027\u5bf9\u6bd4\u4f9d\u636e\u3002", "method": "\u9009\u53d610\u4e2a\u4e3b\u6d41MOM\u7cfb\u7edf\uff0c\u91c7\u752842\u9879\u7279\u5f81\uff08\u542b134\u79cd\u914d\u7f6e\u9009\u9879\uff09\u7684\u91cf\u5316\u5206\u6790\u65b9\u6cd5\u3002", "result": "MOM\u7cfb\u7edf\u901a\u8fc7\u4e8b\u52a1\u652f\u6301\u3001\u8d44\u6e90\u7ba1\u7406\u7b49\u6838\u5fc3\u6a21\u5757\u652f\u6301\u4e91\u5e94\u7528\uff0c\u4f46\u5b58\u5728\u5f00\u6e90\u9879\u76ee\u5206\u6563\u73b0\u8c61\u3002", "conclusion": "\u5efa\u7acb\u53ef\u9a8c\u8bc1\u6807\u6ce8\u6570\u636e\u96c6\u5e76\u5f00\u6e90\uff0c\u63d0\u51fa\u793e\u533a\u5e94\u96c6\u4e2d\u53d1\u5c55\u6838\u5fc3\u9879\u76ee\u4ee5\u63d0\u5347\u5f00\u53d1\u6548\u7387\u3002"}}
{"id": "2602.17808", "categories": ["cs.DC", "cs.PF"], "pdf": "https://arxiv.org/pdf/2602.17808", "abs": "https://arxiv.org/abs/2602.17808", "authors": ["Nathan Ng", "Walid A. Hanafy", "Prashanthi Kadambi", "Balachandra Sunil", "Ayush Gupta", "David Irwin", "Yogesh Simmhan", "Prashant Shenoy"], "title": "Collaborative Processing for Multi-Tenant Inference on Memory-Constrained Edge TPUs", "comment": null, "summary": "IoT applications are increasingly relying on on-device AI accelerators to ensure high performance, especially in limited connectivity and safety-critical scenarios. However, the limited on-chip memory of these accelerators forces inference runtimes to swap model segments between host and accelerator memory, substantially inflating latency. While collaborative processing by partitioning the model processing between CPU and accelerator resources can reduce accelerator memory pressure and latency, naive partitioning may worsen end-to-end latency by either shifting excessive computation to the CPU or failing to sufficiently curb swapping, a problem that is further amplified in multi-tenant and dynamic environments.\n  To address these issues, we present SwapLess, a system for adaptive, multi-tenant TPU-CPU collaborative inference for memory-constrained Edge TPUs. SwapLess utilizes an analytic queueing model that captures partition-dependent CPU/TPU service times as well as inter- and intra-model swapping overheads across different workload mixes and request rates. Using this model, SwapLess continuously adjusts both the partition point and CPU core allocation online to minimize end-to-end response time with low decision overhead. An implementation on Edge TPU-equipped platforms demonstrates that SwapLess reduces mean latency by up to 63.8% for single-tenant workloads and up to 77.4% for multi-tenant workloads relative to the default Edge TPU compiler.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2602.17811", "categories": ["cs.DC", "cs.DS"], "pdf": "https://arxiv.org/pdf/2602.17811", "abs": "https://arxiv.org/abs/2602.17811", "authors": ["Guy Blelloch", "Andrew Brady", "Laxman Dhulipala", "Jeremy Fineman", "Kishen Gowda", "Chase Hutton"], "title": "Faster Parallel Batch-Dynamic Algorithms for Low Out-Degree Orientation", "comment": "57 pages", "summary": "A low out-degree orientation directs each edge of an undirected graph with the goal of minimizing the maximum out-degree of a vertex. In the parallel batch-dynamic setting, one can insert or delete batches of edges, and the goal is to process the entire batch in parallel with work per edge similar to that of a single sequential update and with span (or depth) for the entire batch that is polylogarithmic. In this paper we present faster parallel batch-dynamic algorithms for maintaining a low out-degree orientation of an undirected graph. All results herein achieve polylogarithmic depth, with high probability (whp); the focus of this paper is on minimizing the work, which varies across results.\n  Our first result is the first parallel batch-dynamic algorithm to maintain an asymptotically optimal orientation with asymptotically optimal expected work bounds, in an amortized sense, improving over the prior best work bounds of Liu et al.~[SPAA~'22] by a logarithmic factor.\n  Our second result is a $O(c \\log n)$ orientation algorithm with expected worst-case $O(\\sqrt{\\log n})$ work per edge update, where $c$ is a known upper-bound on the arboricity of the graph. This matches the best-known sequential worst-case $O(c \\log n)$ orientation algorithm given by Berglin and Brodal ~[Algorithmica~'18], albeit in expectation.\n  Our final result is a $O(c + \\log n)$-orientation algorithm with $O(\\log^2 n)$ expected worst-case work per edge update. This algorithm significantly improves upon the recent result of Ghaffari and Koo~[SPAA~'25], which maintains a $O(c)$-orientation with $O(\\log^9 n)$ worst-case work per edge whp.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2602.18151", "categories": ["cs.NI", "cs.IT", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.18151", "abs": "https://arxiv.org/abs/2602.18151", "authors": ["Nikita Zeulin", "Olga Galinina", "Ibrahim Kilinc", "Sergey Andreev", "Robert W. Heath"], "title": "Rethinking Beam Management: Generalization Limits Under Hardware Heterogeneity", "comment": "This work has been submitted to the IEEE for possible publication", "summary": "Hardware heterogeneity across diverse user devices poses new challenges for beam-based communication in 5G and beyond. This heterogeneity limits the applicability of machine learning (ML)-based algorithms. This article highlights the critical need to treat hardware heterogeneity as a first-class design concern in ML-aided beam management. We analyze key failure modes in the presence of heterogeneity and present case studies demonstrating their performance impact. Finally, we discuss potential strategies to improve generalization in beam management.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u786c\u4ef6\u5f02\u6784\u6027\u5bf95G+\u6ce2\u675f\u7ba1\u7406\u7684\u6311\u6218\uff0c\u5f3a\u8c03\u5fc5\u987b\u5c06\u5176\u4f5c\u4e3a\u673a\u5668\u5b66\u4e60\u7b97\u6cd5\u8bbe\u8ba1\u7684\u9996\u8981\u8003\u91cf\u3002", "motivation": "\u7528\u6237\u8bbe\u5907\u786c\u4ef6\u591a\u6837\u6027\u9650\u5236\u4e86\u673a\u5668\u5b66\u4e60\u7b97\u6cd5\u5728\u6ce2\u675f\u7ba1\u7406\u4e2d\u7684\u9002\u7528\u6027\uff0c\u6210\u4e3a5G\u53ca\u672a\u6765\u901a\u4fe1\u7684\u5173\u952e\u74f6\u9888\u3002", "method": "\u5206\u6790\u4e86\u786c\u4ef6\u5f02\u6784\u6027\u5f15\u53d1\u7684\u6545\u969c\u6a21\u5f0f\uff0c\u901a\u8fc7\u6848\u4f8b\u7814\u7a76\u5c55\u793a\u6027\u80fd\u5f71\u54cd\uff0c\u5e76\u63d0\u51fa\u6f5c\u5728\u7b56\u7565\u4ee5\u589e\u5f3a\u7b97\u6cd5\u6cdb\u5316\u80fd\u529b\u3002", "result": "\u6848\u4f8b\u7814\u7a76\u8bc1\u5b9e\u786c\u4ef6\u5f02\u6784\u6027\u4f1a\u663e\u8457\u964d\u4f4e\u7cfb\u7edf\u6027\u80fd\uff0c\u7b56\u7565\u6539\u8fdb\u53ef\u63d0\u5347\u6ce2\u675f\u7ba1\u7406\u7684\u9c81\u68d2\u6027\u3002", "conclusion": "\u9700\u5c06\u786c\u4ef6\u5f02\u6784\u6027\u89c6\u4e3a\u673a\u5668\u5b66\u4e60\u8f85\u52a9\u6ce2\u675f\u7ba1\u7406\u7684\u57fa\u7840\u8bbe\u8ba1\u56e0\u7d20\uff0c\u5e76\u91c7\u7528\u9488\u5bf9\u6027\u7b56\u7565\u4f18\u5316\u7cfb\u7edf\u6027\u80fd\u3002"}}
{"id": "2602.17838", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2602.17838", "abs": "https://arxiv.org/abs/2602.17838", "authors": ["Lara Khatib", "Micheal Pu", "Bogdan Vasilescu", "Meiyappan Nagappan"], "title": "Examining LLMs Ability to Summarize Code Through Mutation-Analysis", "comment": null, "summary": "As developers increasingly rely on LLM-generated code summaries for documentation, testing, and review, it is important to study whether these summaries accurately reflect what the program actually does. LLMs often produce confident descriptions of what the code looks like it should do (intent), while missing subtle edge cases or logic changes that define what it actually does (behavior). We present a mutation-based evaluation methodology that directly tests whether a summary truly matches the code's logic. Our approach generates a summary, injects a targeted mutation into the code, and checks if the LLM updates its summary to reflect the new behavior. We validate it through three experiments totalling 624 mutation-summary evaluations across 62 programs. First, on 12 controlled synthetic programs with 324 mutations varying in type (statement, value, decision) and location (beginning, middle, end). We find that summary accuracy decreases sharply with complexity from 76.5% for single functions to 17.3% for multi-threaded systems, while mutation type and location exhibit weaker effects. Second, testing 150 mutated samples on 50 human-written programs from the Less Basic Python Problems (LBPP) dataset confirms the same failure patterns persist as models often describe algorithmic intent rather than actual mutated behavior with a summary accuracy rate of 49.3%. Furthermore, while a comparison between GPT-4 and GPT-5.2 shows a substantial performance leap (from 49.3% to 85.3%) and an improved ability to identify mutations as \"bugs\", both models continue to struggle with distinguishing implementation details from standard algorithmic patterns. This work establishes mutation analysis as a systematic approach for assessing whether LLM-generated summaries reflect program behavior rather than superficial textual patterns.", "AI": {"tldr": "\u672c\u7814\u7a76\u5f00\u53d1\u4e86\u57fa\u4e8e\u53d8\u5f02\u7684\u8bc4\u6d4b\u65b9\u6cd5\uff0c\u8bc4\u4f30\u5927\u8bed\u8a00\u6a21\u578b\u751f\u6210\u7684\u4ee3\u7801\u6458\u8981\u662f\u5426\u51c6\u786e\u53cd\u6620\u7a0b\u5e8f\u5b9e\u9645\u884c\u4e3a\u800c\u975e\u610f\u56fe\uff0c\u53d1\u73b0\u51c6\u786e\u6027\u968f\u590d\u6742\u6027\u4e0b\u964d\uff0c\u65b0\u6a21\u578b\u6539\u8fdb\u5927\u4f46\u4ecd\u6709\u5c40\u9650\u3002", "motivation": "\u5f00\u53d1\u8005\u9010\u6e10\u4f9d\u8d56\u5927\u8bed\u8a00\u6a21\u578b\u751f\u6210\u4ee3\u7801\u6458\u8981\u7528\u4e8e\u6587\u6863\u7f16\u5199\u3001\u6d4b\u8bd5\u548c\u5ba1\u67e5\uff0c\u4f46\u6458\u8981\u5e38\u8bef\u63cf\u8ff0\u7a0b\u5e8f\u610f\u56fe\u800c\u975e\u5b9e\u9645\u884c\u4e3a\uff08\u5982\u5ffd\u7565\u8fb9\u7f18\u6848\u4f8b\uff09\uff0c\u56e0\u6b64\u9700\u7cfb\u7edf\u6027\u8bc4\u4f30\u5176\u771f\u5b9e\u6027\u3002", "method": "\u91c7\u7528\u53d8\u5f02\u8bc4\u6d4b\uff1a\u9996\u5148\u751f\u6210\u6458\u8981\uff0c\u7136\u540e\u5411\u4ee3\u7801\u6ce8\u5165\u76ee\u6807\u53d8\u5f02\uff08\u5982\u8bed\u53e5\u3001\u503c\u6216\u51b3\u7b56\u53d8\u66f4\uff09\uff0c\u68c0\u67e5\u5927\u8bed\u8a00\u6a21\u578b\u662f\u5426\u66f4\u65b0\u6458\u8981\u53cd\u6620\u65b0\u884c\u4e3a\uff0c\u8fdb\u800c\u901a\u8fc7624\u6b21\u8bc4\u6d4b\uff08\u8986\u76d662\u4e2a\u7a0b\u5e8f\uff0c\u542b\u5408\u6210\u4e0e\u4eba\u5199\u6570\u636e\u96c6\uff09\u8fdb\u884c\u9a8c\u8bc1\u3002", "result": "\u7ed3\u679c\u663e\u793a\uff1a\u5408\u6210\u7a0b\u5e8f\u51c6\u786e\u6027\u4ece\u5355\u51fd\u657076.5%\u964d\u81f3\u591a\u7ebf\u7a0b\u7cfb\u7edf17.3%\uff0c\u53d8\u5f02\u7c7b\u578b\u548c\u4f4d\u7f6e\u5f71\u54cd\u8f83\u5c0f\uff1b\u4eba\u7c7b\u7f16\u5199\u7a0b\u5e8f\u51c6\u786e\u7387\u4e3a49.3%\uff0c\u6a21\u578b\u503e\u5411\u63cf\u8ff0\u610f\u56fe\u800c\u975e\u884c\u4e3a\uff1bGPT-5.2\u5bf9\u6bd4GPT-4\u6027\u80fd\u8dc3\u5347\u81f385.3%\uff0c\u4f46\u4e24\u8005\u4ecd\u96be\u4ee5\u533a\u5206\u5b9e\u73b0\u7ec6\u8282\u4e0e\u6807\u51c6\u7b97\u6cd5\u6a21\u5f0f\u3002", "conclusion": "\u53d8\u5f02\u5206\u6790\u786e\u7acb\u4e3a\u7cfb\u7edf\u65b9\u6cd5\uff0c\u53ef\u6709\u6548\u8bc4\u4f30\u5927\u8bed\u8a00\u6a21\u578b\u6458\u8981\u662f\u5426\u53cd\u6620\u7a0b\u5e8f\u5b9e\u9645\u884c\u4e3a\u800c\u975e\u6587\u672c\u6a21\u5f0f\uff0c\u4f46\u6a21\u578b\u5728\u590d\u6742\u6027\u4e0b\u5b58\u5728\u660e\u663e\u4e0d\u8db3\u3002"}}
{"id": "2602.17817", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2602.17817", "abs": "https://arxiv.org/abs/2602.17817", "authors": ["Ehsan Yousefzadeh-Asl-Miandoab", "Reza Karimzadeh", "Danyal Yorulmaz", "Bulat Ibragimov", "P\u0131nar T\u00f6z\u00fcn"], "title": "GPU Memory and Utilization Estimation for Training-Aware Resource Management: Opportunities and Limitations", "comment": null, "summary": "Collocating deep learning training tasks improves GPU utilization but causes drastic slowdowns due to resource contention and risks Out-of-Memory (OOM) failures. Accurate memory estimation is essential for robust collocation, while GPU utilization -- a key proxy for resource contention -- enables interference-aware scheduling to reduce slowdowns and improve throughput. Existing GPU memory estimators span three paradigms -- analytical models, CPU-side libraries, and ML-based estimators -- each with distinct limitations: dependence on detailed model specifications, intrusive integration, poor generalization, and varying latency overhead. GPU heterogeneity further complicates estimation, as identical tasks can exhibit markedly different memory footprints across hardware generations. GPU utilization remains comparatively understudied, further complicated by the non-additive nature of utilization metrics and hardware sensitivity. We conduct a systematic analysis of representative estimators from each paradigm -- Horus, PyTorch FakeTensor, and our lightweight ML-based estimator -- evaluating accuracy, generalizability, and practical overhead. We construct a synthetic dataset spanning MLPs, CNNs, and Transformers with controlled architectural variations, and train MLP- and Transformer-based estimators for memory prediction. We further experiment with utilization estimation on the same dataset. Our evaluation reveals key tradeoffs and validates estimators against real-world unseen models. Significant challenges remain: analytical models are hardware-dependent, CPU-side libraries impose intrusive integration costs, and ML-based estimators struggle with cross-architecture generalization. We release all datasets, tools, and artifacts to support further research.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7cfb\u7edf\u8bc4\u4f30\u4e86\u6df1\u5ea6\u5b66\u4e60\u4efb\u52a1\u534f\u540c\u5b9a\u4f4d\u4e2dGPU\u8d44\u6e90\u4f30\u8ba1\u7684\u4e09\u79cd\u8303\u5f0f\uff08\u5206\u6790\u6a21\u578b\u3001CPU\u4fa7\u5e93\u548cML\u57fa\u4f30\u8ba1\u5668\uff09\uff0c\u63ed\u793a\u4e86\u51c6\u786e\u6027\u3001\u6cdb\u5316\u6027\u548c\u5f00\u9500\u7684\u5173\u952e\u6743\u8861\uff0c\u5e76\u9a8c\u8bc1\u4e86\u73b0\u5b9e\u6a21\u578b\u7684\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3GPU\u4efb\u52a1\u534f\u540c\u5b9a\u4f4d\u7684\u8d44\u6e90\u4e89\u7528\u95ee\u9898\uff0c\u907f\u514d\u5185\u5b58\u6ea2\u51fa\u98ce\u9669\u548c\u6027\u80fd\u4e0b\u964d\uff0c\u9700\u51c6\u786e\u7684\u8d44\u6e90\u4f30\u8ba1\u4ee5\u5b9e\u73b0\u9c81\u68d2\u534f\u540c\u5b9a\u4f4d\u548c\u5e72\u6270\u611f\u77e5\u8c03\u5ea6\u3002", "method": "\u6784\u5efa\u5408\u6210\u6570\u636e\u96c6\u6db5\u76d6MLPs\u3001CNNs\u548cTransformers\u67b6\u6784\u53d8\u4f53\uff1b\u7cfb\u7edf\u5206\u6790\u4ee3\u8868\u6027\u4f30\u8ba1\u5668\uff08Horus\u3001PyTorch FakeTensor\u548c\u8f7b\u91cf\u7ea7ML\u57fa\u4f30\u8ba1\u5668\uff09\uff0c\u8bad\u7ec3MLP\u548cTransformer\u57fa\u5185\u5b58\u9884\u6d4b\u6a21\u578b\uff0c\u8bc4\u4f30\u51c6\u786e\u5ea6\u3001\u6cdb\u5316\u6027\u548c\u5b9e\u8df5\u5f00\u9500\u3002", "result": "\u5206\u6790\u6a21\u578b\u786c\u4ef6\u4f9d\u8d56\u6027\u9ad8\uff0cCPU\u5e93\u96c6\u6210\u6210\u672c\u5927\u4e14\u4fb5\u5165\u6027\uff0cML\u57fa\u4f30\u8ba1\u5668\u8de8\u67b6\u6784\u6cdb\u5316\u56f0\u96be\uff1b\u5229\u7528\u7387\u4f30\u8ba1\u975e\u7d2f\u8ba1\u7b97\u6027\u4e14\u786c\u4ef6\u654f\u611f\uff0c\u5b9e\u9a8c\u63ed\u793a\u4e86\u8303\u5f0f\u95f4\u6838\u5fc3\u6743\u8861\u5e76\u9a8c\u8bc1\u4e86\u672a\u89c1\u8fc7\u6a21\u578b\u7684\u9884\u6d4b\u53ef\u9760\u6027\u3002", "conclusion": "\u73b0\u6709\u4f30\u8ba1\u5668\u9762\u4e34\u91cd\u5927\u6cdb\u5316\u4e0e\u96c6\u6210\u6311\u6218\uff0c\u91ca\u653e\u7684\u6570\u636e\u96c6\u548c\u5de5\u5177\u652f\u6301\u540e\u7eed\u7814\u7a76\uff0c\u4ee5\u5e94\u5bf9GPU\u5f02\u6784\u6027\u548c\u8d44\u6e90\u4e89\u7528\u95ee\u9898\u3002"}}
{"id": "2602.18187", "categories": ["cs.NI"], "pdf": "https://arxiv.org/pdf/2602.18187", "abs": "https://arxiv.org/abs/2602.18187", "authors": ["Wataru Uemura", "Takumi Hamano"], "title": "Noise Mitigation Methods for Digital Visible Light Communication", "comment": null, "summary": "Visible Light Communication (VLC) using Light Emitting Diodes (LEDs) has gained attention due to its low power consumption, long lifetime, and fast response. However, VLC suffers from optical noise generated by ambient light sources such as fluorescent lamps, which leads to waveform distortion and increased bit error rates (BER). In this paper, we propose two noise reduction methods for Digital Visible Light Communication (DVLC) systems. The first method exploits the periodic nature of interference caused by AC-powered-line illumination and reduces interference by subtracting sampled noise waveforms from the received signal. Second, inspired by Active Noise Control (ANC) techniques, an additional photodiode is introduced for noise reception, and subtraction circuits are employed to attenuate noise in real time. Experimental results show that both methods improve BER performance compared with conventional receivers, with the ANC-inspired approach achieving superior performance under all tested conditions.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2602.18026", "categories": ["cs.MA", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.18026", "abs": "https://arxiv.org/abs/2602.18026", "authors": ["Shan Yang"], "title": "Mean-Field Reinforcement Learning without Synchrony", "comment": "21 pages, 5 figures, 1 algorithm", "summary": "Mean-field reinforcement learning (MF-RL) scales multi-agent RL to large populations by reducing each agent's dependence on others to a single summary statistic -- the mean action. However, this reduction requires every agent to act at every time step; when some agents are idle, the mean action is simply undefined. Addressing asynchrony therefore requires a different summary statistic -- one that remains defined regardless of which agents act. The population distribution $\u03bc\\in \u0394(\\mathcal{O})$ -- the fraction of agents at each observation -- satisfies this requirement: its dimension is independent of $N$, and under exchangeability it fully determines each agent's reward and transition. Existing MF-RL theory, however, is built on the mean action and does not extend to $\u03bc$. We therefore construct the Temporal Mean Field (TMF) framework around the population distribution $\u03bc$ from scratch, covering the full spectrum from fully synchronous to purely sequential decision-making within a single theory. We prove existence and uniqueness of TMF equilibria, establish an $O(1/\\sqrt{N})$ finite-population approximation bound that holds regardless of how many agents act per step, and prove convergence of a policy gradient algorithm (TMF-PG) to the unique equilibrium. Experiments on a resource selection game and a dynamic queueing game confirm that TMF-PG achieves near-identical performance whether one agent or all $N$ act per step, with approximation error decaying at the predicted $O(1/\\sqrt{N})$ rate.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2602.17834", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2602.17834", "abs": "https://arxiv.org/abs/2602.17834", "authors": ["Duncan Adamson", "Will Rosenbaum", "Paul G. Spirakis"], "title": "Distributed Triangle Enumeration in Hypergraphs", "comment": null, "summary": "In the last decade, subgraph detection and enumeration have emerged as a central problem in distributed graph algorithms. This is largely due to the theoretical challenges and practical applications of these problems. In this paper, we initiate the systematic study of distributed sub-hypergraph enumeration in hypergraphs. To this end, we (1)~introduce several computational models for hypergraphs that generalize the CONGEST model for graphs and evaluate their relative computational power, (2)~devise algorithms for distributed triangle enumeration in our computational models and prove their optimality in two such models, (3)~introduce classes of sparse and ``everywhere sparse'' hypergraphs and describe efficient distributed algorithms for triangle enumeration in these classes, and (4)~describe general techniques that we believe to be useful for designing efficient algorithms in our hypergraph models.", "AI": {"tldr": "\u672c\u6587\u7cfb\u7edf\u6027\u5730\u7814\u7a76\u4e86\u5728\u8d85\u56fe\u4e2d\u5206\u5e03\u5f0f\u5b50\u8d85\u56fe\u5217\u4e3e\u7684\u95ee\u9898\uff0c\u5f15\u5165\u4e86\u65b0\u8ba1\u7b97\u6a21\u578b\uff0c\u8bbe\u8ba1\u9ad8\u6548\u7b97\u6cd5\uff0c\u5e76\u5206\u6790\u5176\u6700\u4f18\u6027\u3002", "motivation": "\u5b50\u56fe\u68c0\u6d4b\u5728\u5206\u5e03\u5f0f\u7b97\u6cd5\u4e2d\u610f\u4e49\u91cd\u5927\uff0c\u4f46\u56e0\u8d85\u56fe\u9886\u57df\u7814\u7a76\u4e0d\u8db3\uff0c\u672c\u8bba\u6587\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u5f15\u5165\u63a8\u5e7f\u56fe\u7684CONGEST\u6a21\u578b\u5230\u8d85\u56fe\u7684\u591a\u4e2a\u8ba1\u7b97\u6a21\u578b\uff1b\u8bbe\u8ba1\u5206\u5e03\u5f0f\u4e09\u89d2\u5217\u4e3e\u7b97\u6cd5\uff1b\u5b9a\u4e49\u7a00\u758f\u8d85\u56fe\u7c7b\u5e76\u63d0\u51fa\u901a\u7528\u7b97\u6cd5\u8bbe\u8ba1\u6280\u672f\u3002", "result": "\u5728\u67d0\u4e9b\u6a21\u578b\u4e0b\u8bc1\u660e\u4e09\u89d2\u5217\u4e3e\u7b97\u6cd5\u7684\u6700\u4f18\u6027\uff1b\u5728\u7a00\u758f\u8d85\u56fe\u4e2d\u5f00\u53d1\u9ad8\u6548\u7b97\u6cd5\uff1b\u63d0\u4f9b\u4e86\u4e00\u5957\u5b9e\u7528\u5de5\u5177\u7bb1\u3002", "conclusion": "\u672c\u7814\u7a76\u5efa\u7acb\u4e86\u5206\u5e03\u5f0f\u5b50\u8d85\u56fe\u679a\u4e3e\u7684\u57fa\u7840\u6846\u67b6\uff0c\u4e3a\u672a\u6765\u9ad8\u6548\u7b97\u6cd5\u8bbe\u8ba1\u94fa\u5e73\u4e86\u9053\u8def\u3002"}}
{"id": "2602.18007", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2602.18007", "abs": "https://arxiv.org/abs/2602.18007", "authors": ["Jon Hu", "Thomas Jia", "Jing Zhu", "Zhendong Yu"], "title": "Joint Training on AMD and NVIDIA GPUs", "comment": null, "summary": "As large language models continue to scale, training demands on compute and system capacity grow rapidly, making single-vendor homogeneous clusters insufficient. This paper presents a technical solution for heterogeneous mixed training in AMD-NVIDIA environments. We first adopt a compatibility-oriented approach based on CPU-Forwarding Communication, with differentiated communication back-end selection across parallel groups and multi-NIC parallel data transfer. To achieve higher performance, we further propose another Device-Direct Communication approach, integrating a CPU-offloading P2P mechanism to enable direct cross-vendor GPU data transfer without host-memory staging. Experiments on LLaMA-8B and Qwen2-7B demonstrate that the proposed Device-Direct Communication approach achieves up to 98% of the throughput of an NVIDIA homogeneous system, while preserving training stability and correctness.", "AI": {"tldr": "\u63d0\u51fa\u8de8AMD-NVIDIA\u5f02\u6784\u73af\u5883\u7684\u6df7\u5408\u8bad\u7ec3\u65b9\u6848\uff0c\u5b9e\u73b0\u9ad8\u6548\u6570\u636e\u4f20\u8f93\u548c\u8fd1\u540c\u6784\u7cfb\u7edf\u6027\u80fd", "motivation": "\u5927\u6a21\u578b\u8bad\u7ec3\u5bf9\u7b97\u529b\u9700\u6c42\u6fc0\u589e\uff0c\u5355\u4e00\u4f9b\u5e94\u5546\u96c6\u7fa4\u5df2\u65e0\u6cd5\u6ee1\u8db3\u9700\u6c42\uff0c\u9700\u89e3\u51b3\u5f02\u6784\u786c\u4ef6\u6df7\u5408\u8bad\u7ec3\u95ee\u9898", "method": "1. \u517c\u5bb9\u6027\u65b9\u6848\uff1a\u57fa\u4e8eCPU\u8f6c\u53d1\u901a\u4fe1\uff0c\u5206\u5e76\u884c\u7ec4\u9009\u62e9\u901a\u4fe1\u540e\u7aef\u4e0e\u591a\u7f51\u5361\u5e76\u884c\u4f20\u8f93\uff1b2. \u8bbe\u5907\u76f4\u901a\u65b9\u6848\uff1a\u91c7\u7528CPU\u5378\u8f7d\u7684\u70b9\u5bf9\u70b9\u673a\u5236\uff0c\u5b9e\u73b0\u8de8\u4f9b\u5e94\u5546GPU\u76f4\u4f20\u6570\u636e", "result": "\u5728LLaMA-8B\u548cQwen2-7B\u6d4b\u8bd5\u4e2d\uff0c\u8bbe\u5907\u76f4\u901a\u65b9\u6848\u8fbe\u5230NVIDIA\u540c\u6784\u7cfb\u7edf98%\u541e\u5410\u91cf\uff0c\u4fdd\u6301\u8bad\u7ec3\u7a33\u5b9a\u6027\u4e0e\u6b63\u786e\u6027", "conclusion": "\u8bbe\u5907\u76f4\u901a\u901a\u4fe1\u65b9\u6848\u9ad8\u6548\u89e3\u51b3\u8de8\u4f9b\u5e94\u5546\u5f02\u6784\u8bad\u7ec3\u96be\u9898\uff0c\u6027\u80fd\u903c\u8fd1\u540c\u6784\u7cfb\u7edf"}}
{"id": "2602.18012", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2602.18012", "abs": "https://arxiv.org/abs/2602.18012", "authors": ["Pragati Kumari", "Novarun Deb"], "title": "DeCEAT: Decoding Carbon Emissions for AI-driven Software Testing", "comment": null, "summary": "The increasing use of language models in automated software testing raises concerns about their environmental impact, yet existing sustainability analyses focus almost exclusively on large language models. As a result, the energy and carbon characteristics of small language models (SLMs) during test generation remain largely unexplored. To address this gap, this work introduces the DeCEAT framework, which systematically evaluates the environmental and performance trade-offs of SLMs using the HumanEval benchmark and adaptive prompt variants (based on the Anthropic template). The framework quantifies emission and time-aware behavior under controlled conditions, with CodeCarbon measuring energy consumption and carbon emissions, and unit test coverage assessing the quality of generated tests. Our results show that different SLMs exhibit distinct sustainability strengths: some prioritize lower energy use and faster execution, while others maintain higher stability or accuracy under carbon constraints. These findings demonstrate that sustainability in the generation of SLM-driven tests is multidimensional and strongly shaped by prompt design. This work provides a focused sustainability evaluation framework specifically tailored to automated SLM-based test generation, clarifying how prompt structure and model choice jointly influence environmental and performance outcomes.", "AI": {"tldr": "\u672c\u6587\u5f15\u5165DeCEAT\u6846\u67b6\u8bc4\u4f30\u5c0f\u578b\u8bed\u8a00\u6a21\u578b\u5728\u81ea\u52a8\u6d4b\u8bd5\u751f\u6210\u4e2d\u7684\u73af\u5883\u5f71\u54cd\u548c\u6027\u80fd\u6743\u8861\uff0c\u53d1\u73b0\u53ef\u6301\u7eed\u6027\u662f\u591a\u7ef4\u4e14\u9ad8\u5ea6\u4f9d\u8d56\u63d0\u793a\u8bbe\u8ba1\u3002", "motivation": "\u73b0\u6709\u73af\u5883\u53ef\u6301\u7eed\u6027\u5206\u6790\u4e3b\u8981\u9488\u5bf9\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff0c\u5c0f\u578b\u8bed\u8a00\u6a21\u578b\u5728\u6d4b\u8bd5\u751f\u6210\u4e2d\u7684\u80fd\u6e90\u6d88\u8017\u548c\u78b3\u6392\u653e\u7279\u6027\u7814\u7a76\u4e0d\u8db3\u3002", "method": "\u4f7f\u7528DeCEAT\u6846\u67b6\uff0c\u7ed3\u5408HumanEval\u57fa\u51c6\u548c\u81ea\u9002\u5e94\u63d0\u793a\u53d8\u4f53\uff0cCodeCarbon\u91cf\u5316\u80fd\u6e90\u6d88\u8017\u548c\u78b3\u6392\u653e\uff0c\u5355\u5143\u6d4b\u8bd5\u8986\u76d6\u7387\u8bc4\u4f30\u751f\u6210\u6d4b\u8bd5\u8d28\u91cf\u3002", "result": "\u4e0d\u540c\u5c0f\u578b\u8bed\u8a00\u6a21\u578b\u8868\u73b0\u51fa\u72ec\u7279\u4f18\u52bf\uff0c\u6709\u7684\u80fd\u6548\u9ad8\u3001\u6267\u884c\u5feb\uff0c\u6709\u7684\u5728\u78b3\u7ea6\u675f\u4e0b\u7a33\u5b9a\u6027\u6216\u51c6\u786e\u6027\u66f4\u5f3a\u3002", "conclusion": "\u53ef\u6301\u7eed\u6027\u662f\u591a\u4e2a\u7ef4\u5ea6\u7684\u7efc\u5408\uff0c\u53d7\u63d0\u793a\u7ed3\u6784\u548c\u6a21\u578b\u9009\u62e9\u5171\u540c\u5f71\u54cd\uff0c\u8be5\u6846\u67b6\u586b\u8865\u4e86SLM\u6d4b\u8bd5\u751f\u6210\u73af\u4fdd\u8bc4\u4f30\u7684\u7a7a\u767d\u3002"}}
{"id": "2602.18158", "categories": ["cs.DC", "cs.ET"], "pdf": "https://arxiv.org/pdf/2602.18158", "abs": "https://arxiv.org/abs/2602.18158", "authors": ["Andreas Kouloumpris", "Georgios L. Stavrinides", "Maria K. Michael", "Theocharis Theocharides"], "title": "A reliability- and latency-driven task allocation framework for workflow applications in the edge-hub-cloud continuum", "comment": "This version of the manuscript has been accepted for publication in Future Generation Computer Systems after peer review (Author Accepted Manuscript). It is not the final published version (Version of Record) and does not reflect any post-acceptance improvements. The Version of Record is available online at https://doi.org/10.1016/j.future.2026.108414", "summary": "A growing number of critical workflow applications leverage a streamlined edge-hub-cloud architecture, which diverges from the conventional edge computing paradigm. An edge device, in collaboration with a hub device and a cloud server, often suffices for their reliable and efficient execution. However, task allocation in this streamlined architecture is challenging due to device limitations and diverse operating conditions. Given the inherent criticality of such workflow applications, where reliability and latency are vital yet conflicting objectives, an exact task allocation approach is typically required to ensure optimal solutions. As no existing method holistically addresses these issues, we propose an exact multi-objective task allocation framework to jointly optimize the overall reliability and latency of a workflow application in the specific edge-hub-cloud architecture. We present a comprehensive binary integer linear programming formulation that considers the relative importance of each objective. It incorporates time redundancy techniques, while accounting for crucial constraints often overlooked in related studies. We evaluate our approach using a relevant real-world workflow application, as well as synthetic workflows varying in structure, size, and criticality. In the real-world application, our method achieved average improvements of 84.19% in reliability and 49.81% in latency over baseline strategies, across relevant objective trade-offs. Overall, the experimental results demonstrate the effectiveness and scalability of our approach across diverse workflow applications for the considered system architecture, highlighting its practicality with runtimes averaging between 0.03 and 50.94 seconds across all examined workflows.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u8fb9\u7f18-\u4e2d\u5fc3-\u4e91\u67b6\u6784\u7684\u7cbe\u786e\u4efb\u52a1\u5206\u914d\u6846\u67b6\uff0c\u65e8\u5728\u4f18\u5316\u5173\u952e\u5de5\u4f5c\u6d41\u5e94\u7528\u7684\u53ef\u9760\u6027\u548c\u5ef6\u8fdf\u6027\u80fd\u3002", "motivation": "\u5e38\u89c4\u8fb9\u7f18\u8ba1\u7b97\u67b6\u6784\u4e2d\uff0c\u8bbe\u5907\u9650\u5236\u548c\u64cd\u4f5c\u6761\u4ef6\u591a\u6837\u5316\u4f7f\u4efb\u52a1\u5206\u914d\u56f0\u96be\uff0c\u53ef\u9760\u6027\u548c\u5ef6\u8fdf\u76ee\u6807\u51b2\u7a81\u4e14\u5173\u952e\u6027\u9ad8\uff0c\u73b0\u6709\u65b9\u6cd5\u65e0\u6cd5\u6574\u4f53\u89e3\u51b3\uff0c\u56e0\u6b64\u9700\u8981\u7cbe\u786e\u4f18\u5316\u65b9\u6848\u3002", "method": "\u91c7\u7528\u4e8c\u5143\u6574\u6570\u7ebf\u6027\u89c4\u5212\u7684\u591a\u76ee\u6807\u4f18\u5316\u6846\u67b6\uff0c\u6574\u5408\u65f6\u95f4\u5197\u4f59\u6280\u672f\u5e76\u8003\u8651\u76f8\u5173\u7814\u7a76\u4e2d\u5e38\u5ffd\u7565\u7684\u5173\u952e\u7ea6\u675f\uff0c\u4ee5\u534f\u540c\u4f18\u5316\u53ef\u9760\u6027\u548c\u5ef6\u8fdf\u3002", "result": "\u5b9e\u9645\u5e94\u7528\u4e2d\u53ef\u9760\u6027\u5e73\u5747\u63d0\u534784.19%\uff0c\u5ef6\u8fdf\u964d\u4f4e49.81%\uff1b\u5728\u5404\u7c7b\u5408\u6210\u5de5\u4f5c\u6b22\u547c\u4e0a\u5c55\u73b0\u51fa\u9ad8\u6548\u6027\u548c\u53ef\u6269\u5c55\u6027\uff0c\u8fd0\u884c\u65f6\u95f4\u57280.03\u81f350.94\u79d2\u4e4b\u95f4\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6709\u6548\u3001\u53ef\u6269\u5c55\u4e14\u5b9e\u7528\uff0c\u9002\u7528\u4e8e\u4e0d\u540c\u7ed3\u6784\u7684\u591a\u79cd\u5de5\u4f5c\u6d41\u5e94\u7528\uff0c\u51f8\u663e\u5176\u5728\u65b0\u578b\u67b6\u6784\u4e2d\u7684\u91cd\u8981\u4ef7\u503c\u3002"}}
{"id": "2602.18188", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2602.18188", "abs": "https://arxiv.org/abs/2602.18188", "authors": ["Antonio Cruciani", "Avinandan Das", "Alesya Raevskaya", "Jukka Suomela"], "title": "It does not matter how you define locally checkable labelings", "comment": "40 pages", "summary": "Locally checkable labeling problems (LCLs) form the foundation of the modern theory of distributed graph algorithms. First introduced in the seminal paper by Naor and Stockmeyer [STOC 1993], these are graph problems that can be described by listing a finite set of valid local neighborhoods. This seemingly simple definition strikes a careful balance between two objectives: they are a family of problems that is broad enough so that it captures numerous problems that are of interest to researchers working in this field, yet restrictive enough so that it is possible to prove strong theorems that hold for all LCL problems. In particular, the distributed complexity landscape of LCL problems is now very well understood.\n  In this work we show that the family of LCL problems is extremely robust to variations. We present a very restricted family of locally checkable problems (essentially, the \"node-edge checkable\" formalism familiar from round elimination, restricted to regular unlabeled graphs); most importantly, such problems cannot directly refer to e.g. the existence of short cycles. We show that one can translate between the two formalisms (there are local reductions in both directions that only need access to a symmetry-breaking oracle, and hence the overhead is at most an additive $O(\\log^* n)$ rounds in the LOCAL model).", "AI": {"tldr": "\u8bba\u6587\u8bc1\u660e\u5c40\u90e8\u53ef\u68c0\u6d4b\u6807\u7b7e\u95ee\u9898\uff08LCL\uff09\u5bf9\u5f62\u5f0f\u53d8\u5316\u5177\u6709\u6781\u5f3a\u7a33\u5065\u6027\uff1a\u4e00\u79cd\u53d7\u9650\u7684\u8282\u70b9-\u8fb9\u68c0\u6d4b\u5f62\u5f0f\u7ed3\u6784\u4e0e\u6807\u51c6LCL\u95ee\u9898\u53ef\u76f8\u4e92\u8f6c\u6362\uff0c\u4e14\u4ec5\u9700\u5bf9\u6570\u8f6e\u65f6\u95f4\u5f00\u9500\u3002", "motivation": "LCL\u95ee\u9898\u5728\u5206\u5e03\u5f0f\u56fe\u7b97\u6cd5\u7406\u8bba\u4e2d\u5360\u636e\u6838\u5fc3\u5730\u4f4d\uff0c\u4f46\u73b0\u6709\u5f62\u5f0f\u5bf9\u5c40\u90e8\u7ed3\u6784\u53d8\u5316\u7684\u654f\u611f\u6027\u672a\u77e5\u3002\u672c\u7814\u7a76\u65e8\u5728\u9a8c\u8bc1LCL\u5bb6\u65cf\u5728\u4e0d\u540c\u5f62\u5f0f\u7ea6\u675f\u4e0b\u662f\u5426\u4fdd\u6301\u8ba1\u7b97\u7b49\u4ef7\u6027\u3002", "method": "\u91c7\u7528\u4e86\u4e00\u79cd\u5f3a\u53d7\u9650\u7684\u8282\u70b9-\u8fb9\u68c0\u6d4b\u95ee\u9898\u5f62\u5f0f\uff08\u6392\u9664\u73af\u8def\u4f9d\u8d56\uff09\uff0c\u6784\u5efa\u4e0e\u6807\u51c6LCL\u95ee\u9898\u7684\u53cc\u5411\u5c40\u90e8\u5f52\u7ea6\u673a\u5236\uff0c\u901a\u8fc7\u5bf9\u79f0\u6027\u7834\u574f\u9884\u8a00\u673a\u5b9e\u73b0\u8f6c\u6362\u3002", "result": "\u4e24\u79cd\u5f62\u5f0f\u95f4\u5b58\u5728\u9ad8\u6548\u8f6c\u6362\uff1a\u5728LOCAL\u6a21\u578b\u4e2d\uff0c\u8f6c\u6362\u8fc7\u7a0b\u7684\u6700\u5927\u65f6\u95f4\u5f00\u9500\u4ec5\u4e3a\u52a0\u6027O(log* n)\u8f6e\uff0c\u8bc1\u660e\u5f62\u5f0f\u53d8\u5316\u4e0d\u5f71\u54cd\u6838\u5fc3\u8ba1\u7b97\u590d\u6742\u5ea6\u3002", "conclusion": "LCL\u95ee\u9898\u5177\u5907\u6781\u5f3a\u7684\u5f62\u5f0f\u7a33\u5065\u6027\uff0c\u8fd9\u4e3a\u5206\u5e03\u5f0f\u590d\u6742\u6027\u7406\u8bba\u7684\u666e\u9002\u6027\u63d0\u4f9b\u4e86\u5173\u952e\u652f\u6491\uff0c\u5e76\u62d3\u5c55\u4e86\u95ee\u9898\u5efa\u6a21\u7684\u7075\u6d3b\u6027\u8fb9\u754c\u3002"}}
{"id": "2602.18190", "categories": ["cs.SE", "cs.CY"], "pdf": "https://arxiv.org/pdf/2602.18190", "abs": "https://arxiv.org/abs/2602.18190", "authors": ["Jorge Melegati"], "title": "Role and Identity Work of Software Engineering Professionals in the Generative AI Era", "comment": "Accepted to the 19th International Conference on Cooperative and Human Aspects of Software Engineering (CHASE 2026)", "summary": "The adoption of Generative AI (GenAI) suggests major changes for software engineering, including technical aspects but also human aspects of the professionals involved. One of these aspects is how individuals perceive themselves regarding their work, i.e., their work identity, and the processes they perform to form, adapt and reject these identities, i.e., identity work. Existent studies provide evidence of such identity work of software professionals triggered by the adoption of GenAI, however they do not consider differences among diverse roles, such as developers and testers. In this paper, we argue the need for considering the role as a factor defining the identity work of software professionals. To support our claim, we review some studies regarding different roles and also recent studies on how to adopt GenAI in software engineering. Then, we propose a research agenda to better understand how the role influences identity work of software professionals triggered by the adoption of GenAI, and, based on that, to propose new artifacts to support this adoption. We also discuss the potential implications for practice of the results to be obtained.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8Generative AI (GenAI)\u91c7\u7eb3\u5bf9\u8f6f\u4ef6\u4e13\u4e1a\u4eba\u5458\u7684\u5de5\u4f5c\u8eab\u4efd\u5f71\u54cd\uff0c\u6307\u51fa\u4e0d\u540c\u89d2\u8272(\u5982\u5f00\u53d1\u8005\u548c\u6d4b\u8bd5\u8005)\u7684\u8eab\u4efd\u5de5\u4f5c\u5dee\u5f02\u672a\u88ab\u8003\u8651\uff0c\u5e76\u63d0\u51fa\u7814\u7a76\u8bae\u7a0b\u4ee5\u89e3\u6790\u89d2\u8272\u4f5c\u7528\u548c\u652f\u6301\u5de5\u5177\u5f00\u53d1\u3002", "motivation": "\u52a8\u673a\u662f\u73b0\u6709\u7814\u7a76\u8868\u660eGenAI\u91c7\u7eb3\u89e6\u53d1\u8eab\u4efd\u5de5\u4f5c\uff0c\u4f46\u5ffd\u7565\u4e86\u4e0d\u540c\u89d2\u8272\u7684\u5dee\u5f02\uff0c\u8fd9\u53ef\u80fd\u5bfc\u81f4\u5bf9\u8f6f\u4ef6\u5de5\u7a0b\u53d8\u5316\u7684\u5f71\u54cd\u7406\u89e3\u4e0d\u8db3\uff0c\u56e0\u6b64\u9700\u8981\u9488\u5bf9\u6027\u7814\u7a76\u3002", "method": "\u65b9\u6cd5\u5305\u62ec\u7efc\u8ff0\u4e0d\u540c\u89d2\u8272\u7684\u7814\u7a76\u548cGenAI\u91c7\u7eb3\u6587\u732e\uff0c\u63d0\u51fa\u7814\u7a76\u8bae\u7a0b\u6765\u63a2\u7d22\u89d2\u8272\u5bf9\u8eab\u4efd\u5de5\u4f5c\u7684\u5f71\u54cd\uff0c\u5f00\u53d1\u65b0\u5de5\u5177\uff0c\u5e76\u8ba8\u8bba\u5b9e\u9645\u542b\u4e49\u3002", "result": "\u7ed3\u679c\u662f\u786e\u7acb\u4e86\u89d2\u8272\u5728\u5b9a\u4e49\u8eab\u4efd\u5de5\u4f5c\u4e2d\u7684\u5173\u952e\u4f5c\u7528\uff0c\u5e76\u89c4\u5212\u4e86\u672a\u6765\u7814\u7a76\uff0c\u5982\u89d2\u8272\u5bfc\u5411\u5de5\u5177\u7684\u5f00\u53d1\uff0c\u4ee5\u4f18\u5316GenAI\u91c7\u7eb3\u3002", "conclusion": "\u7ed3\u8bba\u5f3a\u8c03\u89d2\u8272\u56e0\u7d20\u662f\u8eab\u4efd\u5de5\u4f5c\u7684\u6838\u5fc3\u53d8\u91cf\uff0c\u7814\u7a76\u8bae\u7a0b\u5c06\u4fc3\u8fdb\u66f4\u6709\u6548\u7684GenAI\u6574\u5408\u548c\u5b9e\u8df5\u6539\u8fdb\u3002"}}
{"id": "2602.18287", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2602.18287", "abs": "https://arxiv.org/abs/2602.18287", "authors": ["Andrea D'Iapico", "Monica Vitali"], "title": "Green by Design: Constraint-Based Adaptive Deployment in the Cloud Continuum", "comment": null, "summary": "The environmental sustainability of Information Technology (IT) has emerged as a critical concern, driven by the need to reduce both energy consumption and greenhouse gas (GHG) emissions. In the context of cloud-native applications deployed across the cloud-edge continuum, this challenge translates into identifying energy-efficient deployment strategies that consider not only the computational demands of application components but also the environmental impact of the nodes on which they are executed. Generating deployment plans that account for these dynamic factors is non-trivial, due to fluctuations in application behaviour and variations in the carbon intensity of infrastructure nodes. In this paper, we present an approach for the automatic generation of deployment plans guided by green constraints. These constraints are derived from a continuous analysis of energy consumption patterns, inter-component communication, and the environmental characteristics of the underlying infrastructure. This paper introduces a methodology and architecture for the generation of a set of green-aware constraints that inform the scheduler to produce environmentally friendly deployment plans. We demonstrate how these constraints can be automatically learned and updated over time using monitoring data, enabling adaptive, energy-aware orchestration. The proposed approach is validated through realistic deployment scenarios of a cloud-native application, showcasing its effectiveness in reducing energy usage and associated emissions.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u7eff\u8272\u7ea6\u675f\u7684\u81ea\u52a8\u90e8\u7f72\u8ba1\u5212\u751f\u6210\u65b9\u6cd5\uff0c\u7528\u4e8e\u4f18\u5316\u4e91\u8fb9\u5e94\u7528\u80fd\u8017\u4e0e\u6392\u653e\u3002", "motivation": "\u4fe1\u606f\u6280\u672f\u80fd\u8017\u4e0e\u6e29\u5ba4\u6c14\u4f53\u6392\u653e\u95ee\u9898\u65e5\u76ca\u91cd\u8981\uff0c\u9700\u89e3\u51b3\u4e91\u8fb9\u5e94\u7528\u4e2d\u52a8\u6001\u90e8\u7f72\u7684\u6311\u6218\u4ee5\u786e\u4fdd\u73af\u5883\u53ef\u6301\u7eed\u6027\u3002", "method": "\u5229\u7528\u80fd\u8017\u6a21\u5f0f\u3001\u7ec4\u4ef6\u901a\u4fe1\u53ca\u57fa\u7840\u8bbe\u65bd\u73af\u5883\u7279\u6027\u5206\u6790\uff0c\u5b66\u4e60\u5e76\u66f4\u65b0\u7eff\u8272\u7ea6\u675f\uff0c\u6307\u5bfc\u8c03\u5ea6\u5668\u751f\u6210\u73af\u4fdd\u90e8\u7f72\u8ba1\u5212\u3002", "result": "\u5728\u5b9e\u9645\u90e8\u7f72\u573a\u666f\u4e2d\u9a8c\u8bc1\uff0c\u6210\u529f\u964d\u4f4e\u80fd\u6e90\u4f7f\u7528\u53ca\u76f8\u5173\u6392\u653e\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5b9e\u73b0\u4e86\u81ea\u9002\u5e94\u8282\u80fd\u7f16\u6392\uff0c\u663e\u8457\u63d0\u5347IT\u90e8\u7f72\u7684\u73af\u5883\u53ef\u6301\u7eed\u6027\u3002"}}
{"id": "2602.18307", "categories": ["cs.SE", "cs.CL", "cs.LG", "cs.PL"], "pdf": "https://arxiv.org/pdf/2602.18307", "abs": "https://arxiv.org/abs/2602.18307", "authors": ["Yutong Xin", "Qiaochu Chen", "Greg Durrett", "I\u015fil Dillig"], "title": "VeriSoftBench: Repository-Scale Formal Verification Benchmarks for Lean", "comment": null, "summary": "Large language models have achieved striking results in interactive theorem proving, particularly in Lean. However, most benchmarks for LLM-based proof automation are drawn from mathematics in the Mathlib ecosystem, whereas proofs in software verification are developed inside definition-rich codebases with substantial project-specific libraries. We introduce VeriSoftBench, a benchmark of 500 Lean 4 proof obligations drawn from open-source formal-methods developments and packaged to preserve realistic repository context and cross-file dependencies. Our evaluation of frontier LLMs and specialized provers yields three observations. First, provers tuned for Mathlib-style mathematics transfer poorly to this repository-centric setting. Second, success is strongly correlated with transitive repository dependence: tasks whose proofs draw on large, multi-hop dependency closures are less likely to be solved. Third, providing curated context restricted to a proof's dependency closure improves performance relative to exposing the full repository, but nevertheless leaves substantial room for improvement. Our benchmark and evaluation suite are released at https://github.com/utopia-group/VeriSoftBench.", "AI": {"tldr": "\u63d0\u51faVeriSoftBench\u57fa\u51c6\u6d4b\u8bd5\uff0c\u8bc4\u4f30LLM\u5728\u8f6f\u4ef6\u9a8c\u8bc1\u73af\u5883\u4e2d\u7684\u5b9a\u7406\u8bc1\u660e\u80fd\u529b\uff0c\u63ed\u793a\u6570\u5b66\u5e93\u6a21\u578b\u5728\u4ee3\u7801\u5e93\u573a\u666f\u4e0b\u7684\u5c40\u9650\u6027\u4e0e\u4f9d\u8d56\u5173\u7cfb\u5e26\u6765\u7684\u6311\u6218\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8eLLM\u7684\u5b9a\u7406\u8bc1\u660e\u8bc4\u6d4b\u591a\u805a\u7126Mathlib\u6570\u5b66\u5e93\uff0c\u4f46\u8f6f\u4ef6\u9a8c\u8bc1\u9700\u5904\u7406\u9879\u76ee\u4e13\u5c5e\u4ee3\u7801\u5e93\u7684\u591a\u6587\u4ef6\u4f9d\u8d56\u573a\u666f\uff0c\u7f3a\u4e4f\u9488\u5bf9\u6027\u8bc4\u4f30\u65b9\u6848\u3002", "method": "\u521b\u5efa\u5305\u542b500\u4e2a\u662f\u4e0d\u591f4\u8bc1\u660e\u4efb\u52a1\u7684VeriSoftBench\u57fa\u51c6\uff0c\u7d20\u6750\u6e90\u4e8e\u5f00\u6e90\u5f62\u5f0f\u5316\u65b9\u6cd5\u9879\u76ee\uff0c\u4fdd\u7559\u4ee3\u7801\u5e93\u4e0a\u4e0b\u6587\u4e0e\u6587\u4ef6\u4f9d\u8d56\u5173\u7cfb\uff1b\u6d4b\u8bd5\u0e99\u95e8LLM\u4e0e\u4e13\u7528\u8bc1\u660e\u5668\u6027\u80fd\u3002", "result": "\u4e09\u5927\u53d1\u73b0\uff1a1\uff09Mathlib\u8c03\u4f18\u6a21\u578b\u5728\u4ee3\u7801\u5e93\u573a\u666f\u8868\u73b0\u4e0d\u4f73\uff1b2\uff09\u8bc1\u660e\u6210\u529f\u7387\u4e0e\u4f20\u9012\u4f9d\u8d56\u6df1\u5ea6\u8d1f\u76f8\u5173\uff1b3\uff09\u9650\u5b9a\u4f9d\u8d56\u8303\u56f4\u7684\u4e0a\u4e0b\u6587\u6bd4\u5168\u5e93\u4e0a\u4e0b\u6587\u63d0\u5347\u6548\u679c\u66f4\u663e\u8457\uff0c\u4f46\u4ecd\u5b58\u5728\u660e\u663e\u5dee\u8ddd\u3002", "conclusion": "\u8f6f\u4ef6\u9a8c\u8bc1\u573a\u666f\u4e2d\u9879\u76ee\u4f9d\u8d56\u7ed3\u6784\u4e25\u91cd\u5f71\u54cd\u8bc1\u660e\u81ea\u52a8\u5316\u6548\u679c\uff0c\u5f53\u524d\u65b9\u6cd5\u4e9f\u5f85\u6539\u8fdb\uff1b\u516c\u5f00\u57fa\u51c6\u8bc4\u6d4b\u96c6\u4fc3\u8fdb\u76f8\u5173\u7814\u7a76\u3002"}}
{"id": "2602.18357", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2602.18357", "abs": "https://arxiv.org/abs/2602.18357", "authors": ["Wallace Albertini", "Marina Cond\u00e9 Ara\u00fajo", "J\u00falia Cond\u00e9 Ara\u00fajo", "Antonio Pedro Santos Alves", "Marcos Kalinowski"], "title": "Statistical Confidence in Functional Correctness: An Approach for AI Product Functional Correctness Evaluation", "comment": "Author version of the paper accepted for publication at CAIN 2026", "summary": "The quality assessment of Artificial Intelligence (AI) systems is a fundamental challenge due to their inherently probabilistic nature. Standards such as ISO/IEC 25059 provide a quality model, but they lack practical and statistically robust methods for assessing functional correctness. This paper proposes and evaluates the Statistical Confidence in Functional Correctness (SCFC) approach, which seeks to fill this gap by connecting business requirements to a measure of statistical confidence that considers both the model's average performance and its variability. The approach consists of four steps: defining quantitative specification limits, performing stratified and probabilistic sampling, applying bootstrapping to estimate a confidence interval for the performance metric, and calculating a capability index as a final indicator. The approach was evaluated through a case study on two real-world AI systems in industry involving interviews with AI experts. Valuable insights were collected from the experts regarding the utility, ease of use, and intention to adopt the methodology in practical scenarios. We conclude that the proposed approach is a feasible and valuable way to operationalize the assessment of functional correctness, moving the evaluation from a point estimate to a statement of statistical confidence.", "AI": {"tldr": "\u8be5\u8bba\u6587\u9488\u5bf9AI\u7cfb\u7edf\u529f\u80fd\u6b63\u786e\u6027\u8bc4\u4f30\u7f3a\u4e4f\u5b9e\u7528\u5de5\u5177\u7684\u73b0\u72b6\uff0c\u63d0\u51faSCFC\u65b9\u6cd5\uff0c\u901a\u8fc7\u7edf\u8ba1\u7f6e\u4fe1\u5ea6\u8fde\u63a5\u4e1a\u52a1\u9700\u6c42\u5e76\u6574\u5408\u6027\u80fd\u6307\u6807\u4e0e\u53d8\u5f02\u6027\u5206\u6790\u3002", "motivation": "\u73b0\u6709AI\u7cfb\u7edf\u6807\u51c6\u5316\u8d28\u91cf\u6a21\u578b\u7f3a\u4e4f\u7edf\u8ba1\u9c81\u68d2\u6027\u7684\u529f\u80fd\u6b63\u786e\u6027\u8bc4\u4f30\u65b9\u6cd5\uff0c\u5bfc\u81f4\u8d28\u91cf\u9a8c\u8bc1\u4e0d\u8db3\u963b\u788d\u5b9e\u9645\u5e94\u7528\u3002", "method": "\u63d0\u51faSCFC\u56db\u6b65\u6cd5\uff1a\u5b9a\u4e49\u91cf\u5316\u9608\u503c\u3001\u5206\u5c42\u6982\u7387\u62bd\u6837\u3001\u81ea\u52a9\u6cd5\u4f30\u8ba1\u7f6e\u4fe1\u533a\u95f4\u3001\u8ba1\u7b97\u80fd\u529b\u6307\u6807\uff0c\u5e76\u5728\u5de5\u4e1a\u6848\u4f8b\u4e2d\u9a8c\u8bc1\u53ef\u884c\u6027\u3002", "result": "\u4e24\u7c7b\u771f\u5b9e\u5de5\u4e1aAI\u7cfb\u7edf\u6848\u4f8b\u7814\u7a76\u663e\u793a\uff0c\u9886\u57df\u4e13\u5bb6\u8ba4\u53ef\u65b9\u6cd5\u5177\u5907\u5b9e\u7528\u6027\u3001\u6613\u7528\u6027\u548c\u5e94\u7528\u6f5c\u529bcanvas\u63d0\u5347\u8bc4\u4f30\u53ef\u4fe1\u5ea6\u3002", "conclusion": "SCFC\u901a\u8fc7\u7edf\u8ba1\u7f6e\u4fe1\u5ea6\u53d6\u4ee3\u70b9\u4f30\u8ba1\uff0c\u4e3a\u529f\u80fd\u6b63\u786e\u6027\u8bc4\u4f30\u63d0\u4f9b\u4e86\u53ef\u884c\u4e14\u6709\u6548\u7684\u6807\u51c6\u5316\u89e3\u51b3\u65b9\u6848\u3002"}}
