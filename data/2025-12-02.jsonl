{"id": "2512.00233", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2512.00233", "abs": "https://arxiv.org/abs/2512.00233", "authors": ["Davide Rucci", "Sebastian Parfeniuc", "Matteo Mordacchini", "Emanuele Carlini", "Alfredo Cuzzocrea", "Patrizio Dazzi"], "title": "A Parallel and Distributed Rust Library for Core Decomposition on Large Graphs", "comment": null, "summary": "In this paper, we investigate the parallelization of $k$-core decomposition, a method used in graph analysis to identify cohesive substructures and assess node centrality. Although efficient sequential algorithms exist for this task, the scale of modern networks requires faster, multicore-ready approaches. To this end, we adapt a distributed $k$-core algorithm originally proposed by Montresor et al. to shared-memory systems and implement it in Rust, leveraging the language's strengths in concurrency and memory safety. We developed three progressively optimized versions: SequentialK as a baseline, ParallelK introducing multi-threaded message passing, and FastK further reducing synchronization overhead. Extensive experiments on real-world datasets, including road networks, web graphs, and social networks, show that FastK consistently outperforms both SequentialK and ParallelK, as well as a reference Python implementation available in the NetworkX library. Results indicate up to an 11x speedup on 16 threads and execution times up to two orders of magnitude faster than the Python implementation."}
{"id": "2512.00398", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2512.00398", "abs": "https://arxiv.org/abs/2512.00398", "authors": ["Bingzheng Xia", "Zujie Ren", "Kuang Ma", "Xiaoqian Li", "Wenda Li", "Shuibing He"], "title": "Heimdall++: Optimizing GPU Utilization and Pipeline Parallelism for Efficient Single-Pulse Detection", "comment": null, "summary": "With the increasing time and frequency resolution of modern radio telescopes and the exponential growth in observational data volumes, real-time single-pulse detection has become a critical requirement for time-domain radio astronomy. Heimdall, as a representative GPU-accelerated single-pulse search tool, offers substantial performance advantages over CPU-based approaches. However, its sequential execution model and resource contention in intermediate processing stages limit GPU utilization, leading to suboptimal throughput and increased computational latency. To address these limitations, we present Heimdall++, an optimized successor to Heimdall that incorporates fine-grained GPU parallelization, enhanced memory management, and a multi-threaded framework to decouple CPU-bound and GPU-bound processing stages. This design mitigates the GPU stall problem and improves end-to-end efficiency. We evaluated Heimdall++ on a system equipped with NVIDIA RTX 3080 Ti GPUs using both a single large-scale observational file and multiple files. Experimental results demonstrate that Heimdall++ achieves up to 2.66x speedup in single-file processing and 2.05x speedup in multi-file batch processing, while maintaining full consistency with the original Heimdall's search results."}
{"id": "2512.00595", "categories": ["cs.DC", "cs.AI", "cs.CR"], "pdf": "https://arxiv.org/pdf/2512.00595", "abs": "https://arxiv.org/abs/2512.00595", "authors": ["Bala Siva Sai Akhil Malepati"], "title": "IslandRun: Privacy-Aware Multi-Objective Orchestration for Distributed AI Inference", "comment": "15 pages, 3 figures, 2 tables", "summary": "Modern AI inference faces an irreducible tension: no single computational resource simultaneously maximizes performance, preserves privacy, minimizes cost, and maintains trust. Existing orchestration frameworks optimize single dimensions (Kubernetes prioritizes latency, federated learning preserves privacy, edge computing reduces network distance), creating solutions that struggle under real-world heterogeneity. We present IslandRun, a multi-objective orchestration system that treats computational resources as autonomous \"islands\" spanning personal devices, private edge servers, and public cloud. Our key insights: (1) request-level heterogeneity demands policy-constrained multi-objective optimization, (2) data locality enables routing compute to data rather than data to compute, and (3) typed placeholder sanitization preserves context semantics across trust boundaries. IslandRun introduces agent-based routing, tiered island groups with differential trust, and reversible anonymization. This establishes a new paradigm for privacy-aware, decentralized inference orchestration across heterogeneous personal computing ecosystems."}
{"id": "2512.00623", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2512.00623", "abs": "https://arxiv.org/abs/2512.00623", "authors": ["Basilis Mamalis", "Marios Perlitis"], "title": "Steady and Energy-Efficient Multi-Hop Clustering for Flying Ad-Hoc Networks (FANETs)", "comment": "7 pages, 5 figures, Accepted for publication in International Journal of Computer Applications (IJCA) - December 2025 Edition", "summary": "Flying Ad-hoc Networks (FANETs), formed by Unmanned Aerial Vehicles (UAVs), represent an emerging and promising communication paradigm. These networks face unique challenges due to UAVs high mobility, limited energy resources, and dynamic topology. In this work, we propose a novel multi-hop clustering algorithm aimed at creating stable, energy-efficient clusters in FANET environments. The proposed solution enhances cluster longevity and communication efficiency through mobility-aware clustering, energy-centric cluster head (CH) selection, and a ground station(GS)-assisted cluster maintenance management mechanism. First, steady multi-hop clusters are constructed, having CHs with not only high stability and high energy but also with steady and high-energy neighboring areas, and then a proper GS-assisted cluster maintenance mechanism is applied. Experimental results, based on extended simulations, demonstrate that our approach outperforms existing schemes significantly, in terms of cluster stability, communication overhead, and security resilience."}
{"id": "2512.00400", "categories": ["cs.OS", "eess.SY"], "pdf": "https://arxiv.org/pdf/2512.00400", "abs": "https://arxiv.org/abs/2512.00400", "authors": ["Xinkui Zhao", "Yifan Zhang", "Haidan Zhao", "Hao Zhang", "Qingyu Ma", "Lufei Zhang", "Guanjie Cheng", "Shuiguang Deng", "Jianwei Yin", "Zuoning Chen"], "title": "TenonOS: A Self-Generating Intelligent Embedded Operating System Framework for Edge Computing", "comment": null, "summary": "The rapid evolution of edge computing has exposed fundamental limitations in traditional operating system and hypervisor architectures, particularly in managing heterogeneous platforms and meeting the constraints of limited resources. Existing solutions often rely on monolithic or layered combinations of hypervisors and guest OSes, which are difficult to tailor for the diverse and dynamic requirements of edge scenarios. To address these challenges, we propose TenonOS, a demand-driven, self-generating, and lightweight operating system framework that fundamentally rethinks and reconstructs both the hypervisor and OS architectures. TenonOS introduces a novel LibOS-on-LibOS approach, in which both virtualization and OS functionalities are modularized into fine-grained, reusable micro-libraries. A dynamic orchestration engine composes these modules on demand to construct customized, application-specific runtime environments. At the core of TenonOS are two key components: Mortise, a minimal, modularized hypervisor, and Tenon, a real-time LibOS. Mortise provides low-overhead resource isolation, fast inter-VM communication, and manages the full lifecycle of Tenon instances - including on-demand creation, suspension, and termination - enabling TenonOS to flexibly adapt its runtime layout to workload variations. Tenon delivers deterministic scheduling and multi-process support for time-critical applications. Through this unified and modular architecture, TenonOS eliminates redundant layers, reduces system overhead, and enhances scalability, security, and maintainability. Extensive evaluations demonstrate that TenonOS achieves superior real-time scheduling (40.28% improvement), a compact memory footprint (361 KiB), and high adaptability to dynamic edge workloads, making it an ideal foundation for heterogeneous, resource-constrained edge systems."}
{"id": "2512.00023", "categories": ["cs.NI"], "pdf": "https://arxiv.org/pdf/2512.00023", "abs": "https://arxiv.org/abs/2512.00023", "authors": ["Hetvi Shastri", "Akanksha Atrey", "Andre Beck", "Nirupama Ravi"], "title": "Trust or Bust: A Survey of Threats in Decentralized Wireless Networks", "comment": null, "summary": "The recent emergence of decentralized wireless networks empowers individual entities to own, operate, and offer subscriptionless connectivity services in exchange for monetary compensation. While traditional connectivity providers have built trust over decades through widespread adoption, established practices, and regulation, entities in a decentralized wireless network, lacking this foundation, may be incentivized to exploit the service for their own advantage. For example, a dishonest hotspot operator can intentionally violate the agreed upon connection terms in an attempt to increase their profits. In this paper, we examine and develop a taxonomy of adversarial behavior patterns in decentralized wireless networks. Our case study finds that provider-driven attacks can potentially more than triple provider earnings. We conclude the paper with a discussion on the critical need to develop novel techniques to detect and mitigate adversarial behavior in decentralized wireless networks."}
{"id": "2512.00006", "categories": ["cs.AR", "cs.CL"], "pdf": "https://arxiv.org/pdf/2512.00006", "abs": "https://arxiv.org/abs/2512.00006", "authors": ["Yuqin Zhao", "Linghui Ye", "Haihang Xia", "Luke Seed", "Tiantai Deng"], "title": "VeriPy - A New Python-Based Approach for SDR Pipelined/Unrolled Hardware Accelerator Generation", "comment": "13 Pages, 16 figures, and 9 tables. Aim to submit to IEEE TCAD", "summary": "Software-defined radio (SDR) plays an important role in the communication field by providing a flexible and customized communication system for different purposes according to the needs. To enhance the performance of SDR applications, hardware accelerators have been widely deployed in recent years. In facing this obstacle, a necessity arises for a high-level synthesis (HLS) tool specifically designed for communication engineers without detailed hardware knowledge. To lower the barrier between SDR engineers and hardware development, this work proposed a Python-based HLS tool, VeriPy, which can generate both mainstream architecture for hardware accelerators in Verilog specifically for SDR designs including unrolled design and pipelined design, requiring no detailed digital hardware knowledge or Hardware Description Languages (HDL). Furthermore, VeriPy supports automatic testbench generation with random input stimulus, an extensible hardware library, performance and resource estimation, and offers strong optimisation potential at both the algorithmic and digital hardware levels. The generated hardware design by VeriPy can achieve up to 70% faster operating frequency compared to pragma-optimised Vivado HLS designs with a reasonably higher resource con-sumption while delivering comparable performance and resource consumption to hand-coded implementations. Regarding code complexity, VeriPy requires no pragmas, completely eliminating the need for low-level hardware knowledge. For straightforward algorithms, the input code length remains comparable to that of Vivado HLS."}
{"id": "2512.00106", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2512.00106", "abs": "https://arxiv.org/abs/2512.00106", "authors": ["Markus Funke", "Patricia Lago"], "title": "Injecting Sustainability in Software Architecture: A Rapid Review", "comment": "(Accepted/In press) 10th IEEE/ACM International Workshop on Green and Sustainable Software (GREENS '26): GREENS@ICSE 2026", "summary": "Sustainability has evolved from an emerging concern into a fundamental responsibility in software design, development, and operation. Research increasingly explores how sustainability can be systematically integrated into existing software engineering practices. Building on an industry-academia collaboration, we contribute to this discourse by conducting a mixed-method empirical study. We combine a rapid review of secondary studies with a focus group of practitioners. The review identifies challenges and opportunities in embedding sustainability in software architecture, while the focus group enriches and compares these findings. Based on the literature and industry synthesis, we derive five tangible takeaways to inform architects working in the field, and to guide our industry partners in the integration of sustainability concerns in architecture practices."}
{"id": "2512.00520", "categories": ["cs.MA"], "pdf": "https://arxiv.org/pdf/2512.00520", "abs": "https://arxiv.org/abs/2512.00520", "authors": ["Juan A. Wibowo", "George C. Polyzos"], "title": "Toward a Safe Internet of Agents", "comment": "Submitted to the Journal of Artificial Intelligence Research (JAIR) for peer review. 43 pages", "summary": "Background: Autonomous agents powered by Large Language Models (LLMs) are driving a paradigm shift toward an \"Internet of Agents\" (IoA). While offering immense potential, this vision also introduces novel and systemic risks to safety and security. Objectives: Unlike common threat-centric taxonomies, our survey provides a principled, architectural framework for engineering safe and reliable agentic systems. We aim to identify the architectural sources of vulnerabilities to establish a foundation for secure design. Methods: We perform a bottom-up deconstruction of agentic systems, treating each component as a dual-use interface. The analysis spans three levels of complexity: the foundational Single Agent, the collaborative Multi-Agent System (MAS), and the visionary Interoperable Multi-Agent System (IMAS). At each level, we identify core architectural components and their inherent security risks. Results & Conclusions: Our central finding is that agentic safety is an architectural principle, not an add-on. By identifying specific vulnerabilities and deriving mitigation principles at each level of the agentic stack, this survey serves as a foundational guide for building the capable, safe, and trustworthy AI needed to realize a secure Internet of Agents."}
{"id": "2512.00705", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2512.00705", "abs": "https://arxiv.org/abs/2512.00705", "authors": ["Seongyeon Park", "Jaeyong Song", "Changmin Shin", "Sukjin Kim", "Junguk Hong", "Jinho Lee"], "title": "FlexiWalker: Extensible GPU Framework for Efficient Dynamic Random Walks with Runtime Adaptation", "comment": "To appear at EuroSys 2026", "summary": "Dynamic random walks are fundamental to various graph analysis applications, offering advantages by adapting to evolving graph properties. Their runtime-dependent transition probabilities break down the pre-computation strategy that underpins most existing CPU and GPU static random walk optimizations. This leaves practitioners suffering from suboptimal frameworks and having to write hand-tuned kernels that do not adapt to workload diversity. To handle this issue, we present FlexiWalker, the first GPU framework that delivers efficient, workload-generic support for dynamic random walks. Our design-space study shows that rejection sampling and reservoir sampling are more suitable than other sampling techniques under massive parallelism. Thus, we devise (i) new high-performance kernels for them that eliminate global reductions, redundant memory accesses, and random-number generation. Given the necessity of choosing the best-fitting sampling strategy at runtime, we adopt (ii) a lightweight first-order cost model that selects the faster kernel per node at runtime. To enhance usability, we introduce (iii) a compile-time component that automatically specializes user-supplied walk logic into optimized building blocks. On various dynamic random walk workloads with real-world graphs, FlexiWalker outperforms the best published CPU/GPU baselines by geometric means of 73.44x and 5.91x, respectively, while successfully executing workloads that prior systems cannot support. We open-source FlexiWalker in https://github.com/AIS-SNU/FlexiWalker."}
{"id": "2512.01381", "categories": ["cs.OS", "cs.DS", "cs.RO"], "pdf": "https://arxiv.org/pdf/2512.01381", "abs": "https://arxiv.org/abs/2512.01381", "authors": ["Hiroto Takahashi", "Atsushi Yano", "Takuya Azumi"], "title": "Accelerating Probabilistic Response-Time Analysis: Revised Critical Instant and Optimized Convolution", "comment": "8 pages, 5 figures. Proceedings of APRIS2025", "summary": "Accurate estimation of the Worst-Case Deadline Failure Probability (WCDFP) has attracted growing attention as a means to provide safety assurances in complex systems such as robotic platforms and autonomous vehicles. WCDFP quantifies the likelihood of deadline misses under the most pessimistic operating conditions, and safe estimation is essential for dependable real-time applications. However, achieving high accuracy in WCDFP estimation often incurs significant computational cost. Recent studies have revealed that the classical assumption of the critical instant, the activation pattern traditionally considered to trigger the worst-case behavior, can lead to underestimation of WCDFP in probabilistic settings. This observation motivates the use of a revised critical instant formulation that more faithfully captures the true worst-case scenario. This paper investigates convolution-based methods for WCDFP estimation under this revised setting and proposes an optimization technique that accelerates convolution by improving the merge order. Extensive experiments with diverse execution-time distributions demonstrate that the proposed optimized Aggregate Convolution reduces computation time by up to an order of magnitude compared to Sequential Convolution, while retaining accurate and safe-sided WCDFP estimates. These results highlight the potential of the approach to provide both efficiency and reliability in probabilistic timing analysis for safety-critical real-time applications."}
{"id": "2512.00025", "categories": ["cs.NI"], "pdf": "https://arxiv.org/pdf/2512.00025", "abs": "https://arxiv.org/abs/2512.00025", "authors": ["Yun Ji", "Zeyu Chen", "Xiaoxiong Zhong", "Yanan Ma", "Sheng Zhang", "Yuguang Fang"], "title": "Multi-Server FL with Overlapping Clients: A Latency-Aware Relay Framework", "comment": null, "summary": "Multi-server Federated Learning (FL) has emerged as a promising solution to mitigate communication bottlenecks of single-server FL. In a typical multi-server FL architecture, the regions covered by different edge servers (ESs) may overlap. Under this architecture, clients located in the overlapping areas can access edge models from multiple ESs. Building on this observation, we propose a cloud-free multi-server FL framework that leverages Overlapping Clients (OCs) as relays for inter-server model exchange while uploading the local updated model to ESs. This enables ES models to be relayed across multiple hops through neighboring ESs by OCs without introducing new communication links. We derive a new convergence upper bound for non-convex objectives under non-IID data and an arbitrary number of cells, which explicitly quantifies the impact of inter-server propagation depth on convergence error. Guided by this theoretical result, we formulate an optimization problem that aims to maximize dissemination range of each ES model among all ESs within a limited latency. To solve this problem, we develop a conflict-graph-based local search algorithm optimizing the routing strategy and scheduling the transmission times of individual ESs to its neighboring ESs. This enables ES models to be relayed across multiple hops through neighboring ESs by OCs, achieving the widest possible transmission coverage for each model without introducing new communication links. Extensive experimental results show remarkable performance gains of our scheme compared to existing methods."}
{"id": "2512.00016", "categories": ["cs.AR", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.00016", "abs": "https://arxiv.org/abs/2512.00016", "authors": ["Mubarek Mohammed"], "title": "Architect in the Loop Agentic Hardware Design and Verification", "comment": null, "summary": "The ever increasing complexity of the hardware design process demands improved hardware design and verification methodologies. With the advent of generative AI various attempts have been made to automate parts of the design and verification process. Large language models (LLMs) as well as specialized models generate hdl and testbenches for small components, having a few leaf level components. However, there are only a few attempts to automate the entire processor design process. Hardware design demands hierarchical and modular design processes. We utilized this best practice systematically and effectively. We propose agentic automated processor design and verification with engineers in the loop. The agent with optional specification tries to break down the design into sub-components, generate HDL and cocotb tests, and verifies the components involving engineer guidance, especially during debugging and synthesis. We designed various digital systems using this approach. However, we selected two simple processors for demonstration purposes in this work. The first one is a LEGv8 like a simple processor verified, synthesized and programmed for the DE-10 Lite FPGA. The second one is a RISC-V like 32-bit processor designed and verified in similar manner and synthesized. However, it is not programmed into the DE-10 Lite. This process is accomplished usually using around a million inference tokens per processor, using a combination of reasoning (e.g gemini-pro) and non-reasoning models (eg. gpt-5-mini) based on the complexity of the task. This indicates that hardware design and verification experimentation can be done cost effectively without using any specialized hardware. The approach is scalable, we even attempted system-on-chip, which we want to experiment in our future work."}
{"id": "2512.00127", "categories": ["cs.SE", "cs.AI", "cs.PL"], "pdf": "https://arxiv.org/pdf/2512.00127", "abs": "https://arxiv.org/abs/2512.00127", "authors": ["Shailja Thakur", "Vaibhav Saxena", "Rohan Kulkarni", "Shivdeep Singh", "Parameswaran Selvam", "Hima Patel", "Hiroshi Kanayama"], "title": "Generating Verifiable CoT from Execution-Traces", "comment": null, "summary": "Teaching language models to reason about code execution remains a fundamental challenge. While Chain-of-Thought (CoT) prompting has shown promise, current synthetic training data suffers from a critical weakness: the reasoning steps are often plausible-sounding explanations generated by teacher models, not verifiable accounts of what the code actually does. This creates a troubling failure mode where models learn to mimic superficially convincing but logically flawed reasoning patterns.\n  We address this by grounding CoT generation directly in program execution traces. Our pipeline instruments code to capture its dynamic behavior, then narrates these verified execution traces into natural language rationales that are correct by construction. This execution-grounded approach ensures every reasoning step reflects what the program genuinely computes, eliminating logical hallucinations at the source. We evaluate our method on code reasoning tasks (forward reasoning on CruxEval and LiveCodeBench-Exec, backward reasoning on CruxEval-Input), as well as code generation and explanation tasks from HumanEval. Models trained on our bi-directional trace-grounded data achieve substantial improvements, with gains of up to 30 points on output prediction and 28 points on input prediction over base models, alongside improved explanation and code generation, demonstrating that verifiable reasoning fundamentally enhances model capabilities. https://github.ibm.com/IBM-Research-AI/Verified-Code-CoT"}
{"id": "2512.00602", "categories": ["cs.MA", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.00602", "abs": "https://arxiv.org/abs/2512.00602", "authors": ["Wanle Zhong", "Keman Huang", "Xiaoyong Du"], "title": "AgentODRL: A Large Language Model-based Multi-agent System for ODRL Generation", "comment": "Accepted by AAAI 2026. 9 pages, 1 figure", "summary": "The Open Digital Rights Language (ODRL) is a pivotal standard for automating data rights management. However, the inherent logical complexity of authorization policies, combined with the scarcity of high-quality \"Natural Language-to-ODRL\" training datasets, impedes the ability of current methods to efficiently and accurately translate complex rules from natural language into the ODRL format. To address this challenge, this research leverages the potent comprehension and generation capabilities of Large Language Models (LLMs) to achieve both automation and high fidelity in this translation process. We introduce AgentODRL, a multi-agent system based on an Orchestrator-Workers architecture. The architecture consists of specialized Workers, including a Generator for ODRL policy creation, a Decomposer for breaking down complex use cases, and a Rewriter for simplifying nested logical relationships. The Orchestrator agent dynamically coordinates these Workers, assembling an optimal pathway based on the complexity of the input use case. Specifically, we enhance the ODRL Generator by incorporating a validator-based syntax strategy and a semantic reflection mechanism powered by a LoRA-finetuned model, significantly elevating the quality of the generated policies. Extensive experiments were conducted on a newly constructed dataset comprising 770 use cases of varying complexity, all situated within the context of data spaces. The results, evaluated using ODRL syntax and semantic scores, demonstrate that our proposed Orchestrator-Workers system, enhanced with these strategies, achieves superior performance on the ODRL generation task."}
{"id": "2512.00719", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2512.00719", "abs": "https://arxiv.org/abs/2512.00719", "authors": ["Bohan Zhao", "Zane Cao", "Yongchao He"], "title": "SIMPLE: Disaggregating Sampling from GPU Inference into a Decision Plane for Faster Distributed LLM Serving", "comment": null, "summary": "As large language models (LLMs) scale out with tensor parallelism (TP) and pipeline parallelism (PP) and production stacks have aggressively optimized the data plane (attention/GEMM and KV cache), sampling, the decision plane that turns logits into tokens, becomes a new bottleneck. This creates a structural holdout: sampling neither expands with TP nor balances across PP stages, so its share of iteration time grows as GPUs get faster and it caps pipeline frequency at the last stage. We present SIMPLE, a stage-agnostic, sequence-parallel, overlappable decision plane that disaggregates sampling into a CPU-side service and shrinks its runtime footprint back to a minor, hidden role. SIMPLE combines: (1) sequence-parallel sampling, which shards work along the batch dimension and removes vocabulary-axis collectives; (2) a CPU-based algorithm with column-wise penalties and truncation-first filtering to realize single-pass, linear-time kernels; and (3) speculative hot-vocab sampling (SHVS), which samples on a small hot set with rejection-correctness and uses a simple sizing model to choose the hot-vocab size that maximizes throughput. In evaluation, SIMPLE improves end-to-end throughput by up to 96% and reduces P95 latency by 20-65%. Crucially, SIMPLE requires no user-side code changes and composes with existing data-plane optimizations, unlocking scaling benefits that compound with future GPU generations."}
{"id": "2512.00029", "categories": ["cs.NI", "cs.DC"], "pdf": "https://arxiv.org/pdf/2512.00029", "abs": "https://arxiv.org/abs/2512.00029", "authors": ["Andreas Kouloumpris", "Georgios L. Stavrinides", "Maria K. Michael", "Theocharis Theocharides"], "title": "An optimization framework for task allocation in the edge/hub/cloud paradigm", "comment": "This version of the manuscript has been accepted for publication in Future Generation Computer Systems after peer review (Author Accepted Manuscript). It is not the final published version (Version of Record) and does not reflect any post-acceptance improvements. The Version of Record is available online at https://doi.org/10.1016/j.future.2024.02.005", "summary": "With the advent of the Internet of Things (IoT), novel critical applications have emerged that leverage the edge/hub/cloud paradigm, which diverges from the conventional edge computing perspective. A growing number of such applications require a streamlined architecture for their effective execution, often comprising a single edge device with sensing capabilities, a single hub device (e.g., a laptop or smartphone) for managing and assisting the edge device, and a more computationally capable cloud server. Typical examples include the utilization of an unmanned aerial vehicle (UAV) for critical infrastructure inspection or a wearable biomedical device (e.g., a smartwatch) for remote patient monitoring. Task allocation in this streamlined architecture is particularly challenging, due to the computational, communication, and energy limitations of the devices at the network edge. Consequently, there is a need for a comprehensive framework that can address the specific task allocation problem optimally and efficiently. To this end, we propose a complete, binary integer linear programming (BILP) based formulation for an application-driven design-time approach, capable of providing an optimal task allocation in the targeted edge/hub/cloud environment. The proposed method minimizes the desired objective, either the overall latency or overall energy consumption, while considering several crucial parameters and constraints often overlooked in related literature. We evaluate our framework using a real-world use-case scenario, as well as appropriate synthetic benchmarks. Our extensive experimentation reveals that the proposed approach yields optimal and scalable results, enabling efficient design space exploration for different applications and computational devices."}
{"id": "2512.00017", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2512.00017", "abs": "https://arxiv.org/abs/2512.00017", "authors": ["Kunlong Zhang", "Guiying Li", "Ning Lu", "Peng Yang", "Ke Tang"], "title": "Hardware-Aware DNN Compression for Homogeneous Edge Devices", "comment": "International Conference on Data-driven Optimization of Complex Systems 2025 Camera Ready", "summary": "Deploying deep neural networks (DNNs) across homogeneous edge devices (the devices with the same SKU labeled by the manufacturer) often assumes identical performance among them. However, once a device model is widely deployed, the performance of each device becomes different after a period of running. This is caused by the differences in user configurations, environmental conditions, manufacturing variances, battery degradation, etc. Existing DNN compression methods have not taken this scenario into consideration and can not guarantee good compression results in all homogeneous edge devices. To address this, we propose Homogeneous-Device Aware Pruning (HDAP), a hardware-aware DNN compression framework explicitly designed for homogeneous edge devices, aiming to achieve optimal average performance of the compressed model across all devices. To deal with the difficulty of time-consuming hardware-aware evaluations for thousands or millions of homogeneous edge devices, HDAP partitions all the devices into several device clusters, which can dramatically reduce the number of devices to evaluate and use the surrogate-based evaluation instead of hardware evaluation in real-time. Extensive experiments on multiple device types (Jetson Xavier NX and Jetson Nano) and task types (image classification with ResNet50, MobileNetV1, ResNet56, VGG16; object detection with YOLOv8n) demonstrate that HDAP consistently achieves lower average latency and competitive accuracy compared to state-of-the-art methods, with significant speedups (e.g., 2.86$\\times$ on ResNet50 at 1.0G FLOPs). HDAP offers an effective solution for scalable, high-performance DNN deployment methods for homogeneous edge devices."}
{"id": "2512.00134", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.00134", "abs": "https://arxiv.org/abs/2512.00134", "authors": ["Parisa Hamedi", "Hamed Jelodar", "Samita Bai", "Mohammad Meymani", "Roozbeh Razavi-Far", "Ali A. Ghorbani"], "title": "Asm2SrcEval: Evaluating Large Language Models for Assembly-to-Source Code Translation", "comment": null, "summary": "Assembly-to-source code translation is a critical task in reverse engineering, cybersecurity, and software maintenance, yet systematic benchmarks for evaluating large language models on this problem remain scarce. In this work, we present the first comprehensive evaluation of five state-of-the-art large language models on assembly-to-source translation. We assess model performance using a diverse set of metrics capturing lexical similarity (BLEU, ROUGE, and METEOR), semantic alignment (BERTScore), fluency (Perplexity), and efficiency (time prediction). Our results reveal clear trade-offs: while certain models excel in text similarity metrics, others demonstrate lower perplexity or faster inference times. We further provide qualitative analyses of typical model successes and failure cases, highlighting challenges such as control flow recovery and identifier reconstruction. Taken together, our benchmark offers actionable insights into the strengths and limitations of current large language models for program translation, establishing a foundation for future research in combining accuracy with efficiency for real-world applications."}
{"id": "2512.00614", "categories": ["cs.MA", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.00614", "abs": "https://arxiv.org/abs/2512.00614", "authors": ["Goutham Nalagatla"], "title": "Hierarchical Decentralized Multi-Agent Coordination with Privacy-Preserving Knowledge Sharing: Extending AgentNet for Scalable Autonomous Systems", "comment": "6 pages, 4 figures", "summary": "Decentralized multi-agent systems have shown promise in enabling autonomous collaboration among LLM-based agents. While AgentNet demonstrated the feasibility of fully decentralized coordination through dynamic DAG topologies, several limitations remain: scalability challenges with large agent populations, communication overhead, lack of privacy guarantees, and suboptimal resource allocation. We propose AgentNet++, a hierarchical decentralized framework that extends AgentNet with multilevel agent organization, privacy-preserving knowledge sharing via differential privacy and secure aggregation, adaptive resource management, and theoretical convergence guarantees. Our approach introduces cluster-based hierarchies where agents self-organize into specialized groups, enabling efficient task routing and knowledge distillation while maintaining full decentralization. We provide formal analysis of convergence properties and privacy bounds, and demonstrate through extensive experiments on complex multi-agent tasks that AgentNet++ achieves 23% higher task completion rates, 40% reduction in communication overhead, and maintains strong privacy guarantees compared to AgentNet and other baselines. Our framework scales effectively to 1000+ agents while preserving the emergent intelligence properties of the original AgentNet."}
{"id": "2512.00902", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2512.00902", "abs": "https://arxiv.org/abs/2512.00902", "authors": ["Yebo Wu", "Jingguang Li", "Zhijiang Guo", "Li Li"], "title": "Elastic Mixture of Rank-Wise Experts for Knowledge Reuse in Federated Fine-Tuning", "comment": null, "summary": "Federated fine-tuning offers a promising solution for adapting Large Language Models (LLMs) to downstream tasks while safeguarding data privacy. However, its high computational and communication demands hinder its deployment on resource-constrained devices. In this paper, we propose SmartFed, a resource-efficient federated fine-tuning framework. SmartFed intelligently reuses knowledge embedded in existing LoRA modules, eliminating the need for expensive training from scratch when adapting LLMs to new tasks. To effectively exploit this knowledge and ensure scalability, we introduce the Mixture of Rank-Wise Experts (MoRE). MoRE decomposes LoRA modules into fine-grained rank-level experts. These experts are selectively activated and combined based on input semantics and resource budgets. Moreover, to optimize resource utilization, we present the Elastic Expert Quota Allocation (EEQA). EEQA adaptively allocates expert capacity across parameter matrices based on their contribution to model performance, focusing computing resources on the critical experts. Extensive evaluations across multiple benchmarks demonstrate that SmartFed significantly outperforms existing methods in model performance and training efficiency."}
{"id": "2512.00036", "categories": ["cs.NI", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.00036", "abs": "https://arxiv.org/abs/2512.00036", "authors": ["Parth Ashokbhai Shiroya", "Amod Ashtekar", "Swarnagowri Shashidhar", "Mohammed E. Eltayeb"], "title": "Refined Bayesian Optimization for Efficient Beam Alignment in Intelligent Indoor Wireless Environments", "comment": null, "summary": "Future intelligent indoor wireless environments re- quire fast and reliable beam alignment to sustain high-throughput links under mobility and blockage. Exhaustive beam training achieves optimal performance but is prohibitively costly. In indoor settings, dense scatterers and transceiver hardware imperfections introduce multipath and sidelobe leakage, producing measurable power across multiple angles and reducing the effectiveness of outdoor-oriented alignment algorithms. This paper presents a Refined Bayesian Optimization (R-BO) framework that exploits the inherent structure of mmWave transceiver patterns, where received power gradually increases as the transmit and receive beams converge toward the optimum. R-BO integrates a Gaussian Process (GP) surrogate with a Matern kernel and an Expected Improvement (EI) acquisition function, followed by a localized refinement around the predicted optimum. The GP hyperparam- eters are re-optimized online to adapt to irregular variations in the measured angular power field caused by reflections and sidelobe leakage. Experiments across 43 receiver positions in an indoor laboratory demonstrate 97.7% beam-alignment accuracy within 10 degrees, less than 0.3 dB average loss, and an 88% reduction in probing overhead compared to exhaustive search. These results establish R-BO as an efficient and adaptive beam-alignment solution for real-time intelligent indoor wireless environments."}
{"id": "2512.00020", "categories": ["cs.AR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.00020", "abs": "https://arxiv.org/abs/2512.00020", "authors": ["Guang Yang", "Wei Zheng", "Xiang Chen", "Dong Liang", "Peng Hu", "Yukui Yang", "Shaohang Peng", "Zhenghan Li", "Jiahui Feng", "Xiao Wei", "Kexin Sun", "Deyuan Ma", "Haotian Cheng", "Yiheng Shen", "Xing Hu", "Terry Yue Zhuo", "David Lo"], "title": "Large Language Model for Verilog Code Generation: Literature Review and the Road Ahead", "comment": "WIP", "summary": "Code generation has emerged as a critical research area at the intersection of Software Engineering (SE) and Artificial Intelligence (AI), attracting significant attention from both academia and industry. Within this broader landscape, Verilog, as a representative hardware description language (HDL), plays a fundamental role in digital circuit design and verification, making its automated generation particularly significant for Electronic Design Automation (EDA). Consequently, recent research has increasingly focused on applying Large Language Models (LLMs) to Verilog code generation, particularly at the Register Transfer Level (RTL), exploring how these AI-driven techniques can be effectively integrated into hardware design workflows. Despite substantial research efforts have explored LLM applications in this domain, a comprehensive survey synthesizing these developments remains absent from the literature. This review fill addresses this gap by providing a systematic literature review of LLM-based methods for Verilog code generation, examining their effectiveness, limitations, and potential for advancing automated hardware design. The review encompasses research work from conferences and journals in the fields of SE, AI, and EDA, encompassing 70 papers published on venues, along with 32 high-quality preprint papers, bringing the total to 102 papers. By answering four key research questions, we aim to (1) identify the LLMs used for Verilog generation, (2) examine the datasets and metrics employed in evaluation, (3) categorize the techniques proposed for Verilog generation, and (4) analyze LLM alignment approaches for Verilog generation. Based on our findings, we have identified a series of limitations of existing studies. Finally, we have outlined a roadmap highlighting potential opportunities for future research endeavors in LLM-assisted hardware design."}
{"id": "2512.00215", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2512.00215", "abs": "https://arxiv.org/abs/2512.00215", "authors": ["Mohammad Abdollahi", "Khandaker Rifah Tasnia", "Soumit Kanti Saha", "Jinqiu Yang", "Song Wang", "Hadi Hemmati"], "title": "Demystifying Errors in LLM Reasoning Traces: An Empirical Study of Code Execution Simulation", "comment": null, "summary": "Understanding a program's runtime reasoning behavior, meaning how intermediate states and control flows lead to final execution results, is essential for reliable code generation, debugging, and automated reasoning. Although large language models (LLMs) can accurately predict program outputs, most prior work has focused on output accuracy and performance, treating reasoning as a black box. As a result, little is known about the structure or failure modes of their reasoning traces. To address this gap, we conduct the first empirical study on runtime behavior inference with reasoning LLMs, aiming to uncover and characterize errors in their reasoning traces. We curate a benchmark from HumanEval Plus and LiveCodeBench, containing 427 code snippets. For each snippet, we test three input types: regular, edge, and invalid. Twelve input values are selected per snippet, each paired with its ground-truth execution result. We evaluate four state-of-the-art reasoning LLMs. Our results show that these models reach accuracies between 85 percent and 98 percent across input types. We also analyze the produced reasoning traces and develop a taxonomy with nine categories of inference errors. Finally, we explore tool-augmented reasoning. Using failures in the Computation Errors category as a case study, our experiments show that this approach corrects 58 percent of such errors, demonstrating the potential of tool support for improving LLM reasoning."}
{"id": "2512.00740", "categories": ["cs.MA"], "pdf": "https://arxiv.org/pdf/2512.00740", "abs": "https://arxiv.org/abs/2512.00740", "authors": ["Qingwen Yang", "Feiyu Qu", "Tiezheng Guo", "Yanyi Liu", "Yingyou Wen"], "title": "Augmented Runtime Collaboration for Self-Organizing Multi-Agent Systems: A Hybrid Bi-Criteria Routing Approach", "comment": "Accepted at The 40th Annual AAAI Conference on Artificial Intelligence", "summary": "LLM-based multi-agent systems have demonstrated significant capabilities across diverse domains. However, the task performance and efficiency are fundamentally constrained by their collaboration strategies. Prevailing approaches rely on static topologies and centralized global planning, a paradigm that limits their scalability and adaptability in open, decentralized networks. Effective collaboration planning in distributed systems using only local information thus remains a formidable challenge. To address this, we propose BiRouter, a novel dual-criteria routing method for Self-Organizing Multi-Agent Systems (SO-MAS). This method enables each agent to autonomously execute ``next-hop'' task routing at runtime, relying solely on local information. Its core decision-making mechanism is predicated on balancing two metrics: (1) the ImpScore, which evaluates a candidate agent's long-term importance to the overall goal, and (2) the GapScore, which assesses its contextual continuity for the current task state. Furthermore, we introduce a dynamically updated reputation mechanism to bolster system robustness in untrustworthy environments and have developed a large-scale, cross-domain dataset, comprising thousands of annotated task-routing paths, to enhance the model's generalization. Extensive experiments demonstrate that BiRouter achieves superior performance and token efficiency over existing baselines, while maintaining strong robustness and effectiveness in information-limited, decentralized, and untrustworthy settings."}
{"id": "2512.01039", "categories": ["cs.DC", "cs.LG", "cs.NI"], "pdf": "https://arxiv.org/pdf/2512.01039", "abs": "https://arxiv.org/abs/2512.01039", "authors": ["Aladin Djuhera", "Fernando Koch", "Alecio Binotto"], "title": "Joint Partitioning and Placement of Foundation Models for Real-Time Edge AI", "comment": null, "summary": "Inference over large-scale foundation models within heterogeneous edge environments necessitates a fundamentally reconfigurable orchestration substrate. Static partitioning of model layers presumes temporal stability across compute and network resources, which is misaligned with the volatility of real-world deployments. We introduce a framework in which both the spatial placement and internal segmentation of foundation models are elevated to runtime-resolved constructs. The orchestration problem is formalized as a constrained optimization over layer-wise assignments, subject to evolving latency, utilization, and privacy gradients. The framework implements reactive inference composition responsive to infrastructural fluctuations by integrating model-aware capacity profiling with dynamic graph re-partitioning and reallocation. We introduce architectural and algorithmic components, along with a representative use case in 6G multi-access edge computing."}
{"id": "2512.00039", "categories": ["cs.NI", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2512.00039", "abs": "https://arxiv.org/abs/2512.00039", "authors": ["Tasnim Ahmed", "Siana Rizwan", "Naveed Ejaz", "Salimur Choudhury"], "title": "LM4Opt-RA: A Multi-Candidate LLM Framework with Structured Ranking for Automating Network Resource Allocation", "comment": null, "summary": "Building on advancements in Large Language Models (LLMs), we can tackle complex analytical and mathematical reasoning tasks requiring nuanced contextual understanding. A prime example of such complex tasks is modelling resource allocation optimization in networks, which extends beyond translating natural language inputs into mathematical equations or Linear Programming (LP), Integer Linear Programming (ILP), and Mixed-Integer Linear Programming (MILP) models. However, existing benchmarks and datasets cannot address the complexities of such problems with dynamic environments, interdependent variables, and heterogeneous constraints. To address this gap, we introduce NL4RA, a curated dataset comprising 50 resource allocation optimization problems formulated as LP, ILP, and MILP. We then evaluate the performance of well-known open-source LLMs with varying parameter counts. To enhance existing LLM based methods, we introduce LM4Opt RA, a multi candidate framework that applies diverse prompting strategies such as direct, few shot, and chain of thought, combined with a structured ranking mechanism to improve accuracy. We identified discrepancies between human judgments and automated scoring such as ROUGE, BLEU, or BERT scores. However, human evaluation is time-consuming and requires specialized expertise, making it impractical for a fully automated end-to-end framework. To quantify the difference between LLM-generated responses and ground truth, we introduce LLM-Assisted Mathematical Evaluation (LAME), an automated metric designed for mathematical formulations. Using LM4Opt-RA, Llama-3.1-70B achieved a LAME score of 0.8007, outperforming other models by a significant margin, followed closely by Llama-3.1-8B. While baseline LLMs demonstrate considerable promise, they still lag behind human expertise; our proposed method surpasses these baselines regarding LAME and other metrics."}
{"id": "2512.00026", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2512.00026", "abs": "https://arxiv.org/abs/2512.00026", "authors": ["Mahek Desai", "Rowena Quinn", "Marjan Asadinia"], "title": "ML-PCM : Machine Learning Technique for Write Optimization in Phase Change Memory (PCM)", "comment": null, "summary": "As transistor-based memory technologies like dynamic random access memory (DRAM) approach their scalability limits, the need to explore alternative storage solutions becomes increasingly urgent. Phase-change memory (PCM) has gained attention as a promising option due to its scalability, fast access speeds, and zero leakage power compared to conventional memory systems. However, despite these advantages, PCM faces several challenges that impede its broader adoption, particularly its limited lifespan due to material degradation during write operations, as well as the high energy demands of these processes. For PCM to become a viable storage alternative, enhancing its endurance and reducing the energy required for write operations are essential. This paper proposes the use of a neural network (NN) model to predict critical parameters such as write latency, energy consumption, and endurance by monitoring real-time operating conditions and device characteristics. These predictions are key to improving PCM performance and identifying optimal write settings, making PCM a more practical and efficient option for data storage in applications with frequent write operations. Our approach leads to significant improvements, with NN predictions achieving a Mean Absolute Percentage Error (MAPE) of 0.0073% for endurance, 0.23% for total write latency, and 4.92% for total write energy."}
{"id": "2512.00231", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.00231", "abs": "https://arxiv.org/abs/2512.00231", "authors": ["Monique Louise Monteiro", "George G. Cabral", "Adriano L. I. OLiveira"], "title": "CodeFlowLM: Incremental Just-In-Time Defect Prediction with Pretrained Language Models and Exploratory Insights into Defect Localization", "comment": null, "summary": "This work introduces CodeFlowLM, an incremental learning framework for Just-In-Time Software Defect Prediction (JIT-SDP) that leverages pre-trained language models (PLMs). Unlike traditional online learners, CodeFlowLM employs continual fine-tuning to address concept drift, class imbalance, and verification latency without retraining from scratch. We evaluated encoder-only and encoder-decoder PLMs (notably CodeT5+ and UniXCoder) in JIT-SDP scenarios within and between projects, comparing them with the incremental baseline BORB. The results show that CodeFlowLM achieves up to 68% G-Mean gains, confirming its superior adaptability and robustness in evolving software environments. We further extend the analysis to Just-in-Time Defect Localization (JIT-DL), benchmarking Large Language Models (LLMs) such as GPT-5, Claude Sonnet 4.5, and Gemini 2.5 Pro against attention-based models. GPT-5 delivers comparable performance for Recall@20% and Effort@20% with higher stability, although attention-based methods retain an advantage in fine-grained ranking metrics (Top-k, IFA). A qualitative error analysis reveals that most false positives arise from (1) human-like conservative bias, (2) insufficient contextual information in diff-based prompts, and (3) potential dataset mislabeling in JIT-Defects4J. These findings highlight both the promise and the current limitations of LLM reasoning in defect localization. False negatives occur in smaller proportions. Overall, CodeFlowLM significantly advances the state of the art in incremental JIT-SDP, demonstrating superior adaptability and robustness in evolving software environments. Furthermore, our exploratory analysis of LLMs in JIT-DL not only benchmarks their performance against established attention-based models but also provides critical insights into the current limitations of prompt-based defect reasoning."}
{"id": "2512.01010", "categories": ["cs.MA", "cs.AI", "cs.LG", "cs.SE", "physics.comp-ph", "physics.flu-dyn"], "pdf": "https://arxiv.org/pdf/2512.01010", "abs": "https://arxiv.org/abs/2512.01010", "authors": ["Vansh Sharma", "Venkat Raman"], "title": "Chain of Unit-Physics: A Primitive-Centric Approach to Scientific Code Synthesis", "comment": null, "summary": "Agentic large language models are proposed as autonomous code generators for scientific computing, yet their reliability in high-stakes problems remains unclear. Developing computational scientific software from natural-language queries remains challenging broadly due to (a) sparse representation of domain codes during training and (b) the limited feasibility of RLHF with a small expert community. To address these limitations, this work conceptualizes an inverse approach to code design, embodied in the Chain of Unit-Physics framework: a first-principles (or primitives)-centric, multi-agent system in which human expert knowledge is encoded as unit-physics tests that explicitly constrain code generation. The framework is evaluated on a nontrivial combustion task, used here as a representative benchmark for scientific problem with realistic physical constraints. Closed-weight systems and code-focused agentic variants fail to produce correct end-to-end solvers, despite tool and web access, exhibiting four recurrent error classes: interface (syntax/API) hallucinations, overconfident assumptions, numerical/physical incoherence, and configuration fragility. Open-weight models with chain-of-thought (CoT) decoding reduce interface errors but still yield incorrect solutions. On the benchmark task, the proposed framework converges within 5-6 iterations, matches the human-expert implementation (mean error of $3.1\\times10^{-3}$ %), with a $\\sim$33.4 % faster runtime and a $\\sim$30 % efficient memory usage at a cost comparable to mid-sized commercial APIs, yielding a practical template for physics-grounded scientific code generation. As datasets and models evolve, zero-shot code accuracy will improve; however, the Chain of Unit-Physics framework goes further by embedding first-principles analysis that is foundational to scientific codes."}
{"id": "2512.01357", "categories": ["cs.DC", "cs.AI", "cs.AR"], "pdf": "https://arxiv.org/pdf/2512.01357", "abs": "https://arxiv.org/abs/2512.01357", "authors": ["Wenbin Zhu", "Zhaoyan Shen", "Zili Shao", "Hongjun Dai", "Feng Chen"], "title": "Tangram: Accelerating Serverless LLM Loading through GPU Memory Reuse and Affinity", "comment": null, "summary": "Serverless Large Language Models (LLMs) have emerged as a cost-effective solution for deploying AI services by enabling a 'pay-as-you-go' pricing model through GPU resource sharing. However, cold-start latency, especially the model loading phase, has become a critical performance bottleneck, as it scales linearly with model size and severely limits the practical deployment of large-scale LLM services. This paper presents Tangram, a novel system that accelerates Serverless LLM loading through efficient GPU memory reuse. By leveraging the unused GPU memory to retain model parameters, Tangram significantly reduces model transfer time and cold-start latency. Its design includes three key components: unified GPU memory pool for tensor-level parameter sharing across models, on-demand KV cache allocation for dynamic memory management, and GPU-affinity-aware scheduling for maximizing resource utilization. These techniques collectively address the critical challenges of inefficient memory usage and the cold-start problem in Serverless LLM platforms. We have implemented a fully functional prototype, and experiments show that Tangram achieves up to 6.2 times faster loading and reduces Time-To-First-Token (TTFT) during cold-start by 23--55% over state-of-the-art methods."}
{"id": "2512.00040", "categories": ["cs.NI", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.00040", "abs": "https://arxiv.org/abs/2512.00040", "authors": ["Sagar Sudhakara", "Pankaj Rajak"], "title": "Constrained Network Slice Assignment via Large Language Models", "comment": "Accepted at NeurIPS 2025 Workshop on AI and ML for Next-Generation Wireless Communications and Networking (AI4NextG), San Diego, CA", "summary": "Modern networks support network slicing, which partitions physical infrastructure into virtual slices tailored to different service requirements (for example, high bandwidth or low latency). Optimally allocating users to slices is a constrained optimization problem that traditionally requires complex algorithms. In this paper, we explore the use of Large Language Models (LLMs) to tackle radio resource allocation for network slicing. We focus on two approaches: (1) using an LLM in a zero-shot setting to directly assign user service requests to slices, and (2) formulating an integer programming model where the LLM provides semantic insight by estimating similarity between requests. Our experiments show that an LLM, even with zero-shot prompting, can produce a reasonable first draft of slice assignments, although it may violate some capacity or latency constraints. We then incorporate the LLM's understanding of service requirements into an optimization solver to generate an improved allocation. The results demonstrate that LLM-guided grouping of requests, based on minimal textual input, achieves performance comparable to traditional methods that use detailed numerical data, in terms of resource utilization and slice isolation. While the LLM alone does not perfectly satisfy all constraints, it significantly reduces the search space and, when combined with exact solvers, provides a promising approach for efficient 5G network slicing resource allocation."}
{"id": "2512.00028", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2512.00028", "abs": "https://arxiv.org/abs/2512.00028", "authors": ["Nan Jonckers", "Toon Vinck", "Peter Karsmakers", "Jeffrey Prinzie"], "title": "Analysis of Single Event Induced Bit Faults in a Deep Neural Network Accelerator Pipeline", "comment": "Submitted to JINST in context of TWEPP 2025 proceedings", "summary": "In recent years, the increased interest and the growth in application domains of Artificial Intelligence (AI), and more specifically Deep Neural Networks (DNNs), has led to an extensive usage of domain specific DNN accelerator processors to improve the computational efficiency of DNN inference. However, like any digital circuit, these processors are prone to faults induced by radiation particles such as heavy ions, protons, etc., making their use in harsh radiation environments a challenge. This work presents an in-depth analysis of the impact of such faults on the computational pipeline of a Systolic Array based Deep Neural Network accelerator (SA-DNN accelerator) by means of a Register Transfer Level (RTL) Fault Injection (FI) simulation in order to improve the observability of each hardware block. From this analysis, we present the sensitivity to single bit faults of register groups in the pipeline for three different DNN workloads utilising two datasets, namely MNIST and CIFAR-10. These sensitivity figures are presented in terms of Fault Propagation Probability ($P(f_{non-crit})$) and False Classification Probability ($P(f_{crit})$) which respectively show the probability that an injected fault causes a non-critical error (numerical offset) or a critical error (classification fault). From these results, we devise a fault mitigation strategy to harden the SA-DNN accelerator in an efficient way, both in terms of area and power overhead."}
{"id": "2512.00250", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2512.00250", "abs": "https://arxiv.org/abs/2512.00250", "authors": ["Shrinivass Arunachalam Balasubramanian"], "title": "ng-reactive-lint: Smarter Linting for Angular Apps", "comment": null, "summary": "Reactivity is central to Angular applications, yet subtle misuse of Observables, Signals, and change-detection often leads to performance regressions that are difficult to diagnose. Although Angular 17 introduced a unified, signal-first model, most enterprise codebases still rely heavily on legacy RxJS patterns that create unpredictable update flows, memory leaks, and excessive change cycles. To address these issues, we developed ng-reactive-lint, a deterministic static analysis tool that understands Angular's component semantics, lifecycle hooks, template bindings, and reactivity patterns. Unlike generic ESLint or RxJS plugins, ng-reactive-lint performs framework-aware analysis to detect high-impact anti-patterns and provide actionable, context-specific fixes. Evaluation across five large real-world projects showed reductions of up to threefold in unnecessary change detection cycles and up to 75% lower peak memory usage. The tool offers a practical, automated path to adopting modern Angular reactivity at scale."}
{"id": "2512.01363", "categories": ["cs.MA", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.01363", "abs": "https://arxiv.org/abs/2512.01363", "authors": ["Jiaguo Tian", "Zhengbang Zhu", "Shenyu Zhang", "Li Xu", "Bo Zheng", "Xu Liu", "Weiji Peng", "Shizeng Yao", "Weinan Zhang"], "title": "SocialDriveGen: Generating Diverse Traffic Scenarios with Controllable Social Interactions", "comment": null, "summary": "The generation of realistic and diverse traffic scenarios in simulation is essential for developing and evaluating autonomous driving systems. However, most simulation frameworks rely on rule-based or simplified models for scene generation, which lack the fidelity and diversity needed to represent real-world driving. While recent advances in generative modeling produce more realistic and context-aware traffic interactions, they often overlook how social preferences influence driving behavior. SocialDriveGen addresses this gap through a hierarchical framework that integrates semantic reasoning and social preference modeling with generative trajectory synthesis. By modeling egoism and altruism as complementary social dimensions, our framework enables controllable diversity in driver personalities and interaction styles. Experiments on the Argoverse 2 dataset show that SocialDriveGen generates diverse, high-fidelity traffic scenarios spanning cooperative to adversarial behaviors, significantly enhancing policy robustness and generalization to rare or high-risk situations."}
{"id": "2512.01549", "categories": ["cs.DC", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.01549", "abs": "https://arxiv.org/abs/2512.01549", "authors": ["Tom Goethals", "Merlijn Sebrechts", "Stijn De Schrijver", "Filip De Turck", "Bruno Volckaert"], "title": "Delta Sum Learning: an approach for fast and global convergence in Gossip Learning", "comment": null, "summary": "Federated Learning is a popular approach for distributed learning due to its security and computational benefits. With the advent of powerful devices in the network edge, Gossip Learning further decentralizes Federated Learning by removing centralized integration and relying fully on peer to peer updates. However, the averaging methods generally used in both Federated and Gossip Learning are not ideal for model accuracy and global convergence. Additionally, there are few options to deploy Learning workloads in the edge as part of a larger application using a declarative approach such as Kubernetes manifests. This paper proposes Delta Sum Learning as a method to improve the basic aggregation operation in Gossip Learning, and implements it in a decentralized orchestration framework based on Open Application Model, which allows for dynamic node discovery and intent-driven deployment of multi-workload applications. Evaluation results show that Delta Sum performance is on par with alternative integration methods for 10 node topologies, but results in a 58% lower global accuracy drop when scaling to 50 nodes. Overall, it shows strong global convergence and a logarithmic loss of accuracy with increasing topology size compared to a linear loss for alternatives under limited connectivity."}
{"id": "2512.00161", "categories": ["cs.NI"], "pdf": "https://arxiv.org/pdf/2512.00161", "abs": "https://arxiv.org/abs/2512.00161", "authors": ["Ram Ramanathan", "Dmitrii Dugaev", "Liang Tan", "Warren Ramanathan"], "title": "Mesh Augmentation of LoRaWAN-based IoT Networks", "comment": null, "summary": "LoRaWAN is a leading standard and technology for low-power, long-range Internet-of-Things (IoT) communications. However, its single-hop architecture results in limited effective range and excessive power consumption for end devices, especially when deployed in large, remote and RF-challenged environments. Existing solutions are either incompatible with LoRaWAN, or limit relaying to a single hop. We present LIMA, a protocol for augmenting an existing or new LoRaWAN deployment with a mesh network of LIMA Routers. LIMA increases the effective coverage range well beyond the maximum LoRa range via multi-hopping, and significantly reduces the energy consumed by end-devices. LIMA requires no changes to the end-device, the servers or the LoRaWAN standard. LIMA builds routes using reverse path forwarding, tunnels LoRaWAN messages over LIMA, provides transparent extension of the existing Adaptive Data Rate (ADR), and suppresses duplicate forwarding if the device is directly reachable from the Gateway. Simulations using Network Simulator 3 (ns-3) show that LIMA increases the delivery rate, scalability, ED energy consumption by up to 5x, 8x and 12.6x respectively, and reduces latency by up to 2.3x. Table-top and outdoor testing with a prototype constructed using a commercial gateway as a starting point confirm that LIMA can be successfully deployed within an existing LoRaWAN system, and can provide range and energy gains transparently."}
{"id": "2512.00031", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2512.00031", "abs": "https://arxiv.org/abs/2512.00031", "authors": ["Ravindra Ganti", "Steve Xu"], "title": "Hardware-Aware Neural Network Compilation with Learned Optimization: A RISC-V Accelerator Approach", "comment": "18 pages, 7 figures, 6 tables", "summary": "We present XgenSilicon ML Compiler, a fully automated end-to-end compilation framework that transforms high-level machine learning models into optimized RISC-V assembly code for custom ASIC accelerators. By unifying the system's cost model across software and hardware, the compiler achieves significant improvements in Power, Performance, and Area (PPA) metrics compared to standard off-the-shelf components and hand-designed chips through five key innovations: (1) a multi-algorithm auto-tuning framework with five search strategies (Bayesian Optimization, Genetic Algorithm, Simulated Annealing, Random Search, Grid Search) combined with a learned cost model, (2) an integrated quantization framework supporting extreme precisions from FP32 to Binary with full KL divergence calibration (2048-bin histogram optimization) and momentum-based QAT gradient updates, (3) hardware-aware validation ensuring 100 percent ISA compliance and memory constraint satisfaction, (4) dynamic shape support with multi-configuration specialization, and (5) advanced cache-aware cost modeling with multi-level cache hierarchy analysis. Our evaluation demonstrates that ASICs produced by this compiler achieve 2.5-4.5x better performance, 3-6x lower power consumption, and 40-60 percent area reduction compared to baseline implementations. The compiler supports more than 100 ONNX operators across 12 categories, implements advanced RISC-V Vector optimizations, and generates hardware-validated assembly code suitable for direct ASIC synthesis. All compilation steps are fully automated, requiring zero manual intervention from model input to ASIC-ready output."}
{"id": "2512.00325", "categories": ["cs.SE", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2512.00325", "abs": "https://arxiv.org/abs/2512.00325", "authors": ["Shaira Sadia Karim", "Abrar Mahmud Rahim", "Lamia Alam", "Ishmam Tashdeed", "Lutfun Nahar Lota", "Md. Abu Raihan M. Kamal", "Md. Azam Hossain"], "title": "Progressive Code Integration for Abstractive Bug Report Summarization", "comment": null, "summary": "Bug reports are often unstructured and verbose, making it challenging for developers to efficiently comprehend software issues. Existing summarization approaches typically rely on surface-level textual cues, resulting in incomplete or redundant summaries, and they frequently ignore associated code snippets, which are essential for accurate defect diagnosis. To address these limitations, we propose a progressive code-integration framework for LLM-based abstractive bug report summarization. Our approach incrementally incorporates long code snippets alongside textual content, overcoming standard LLM context window constraints and producing semantically rich summaries. Evaluated on four benchmark datasets using eight LLMs, our pipeline outperforms extractive baselines by 7.5%-58.2% and achieves performance comparable to state-of-the-art abstractive methods, highlighting the benefits of jointly leveraging textual and code information for enhanced bug comprehension."}
{"id": "2512.01610", "categories": ["cs.MA"], "pdf": "https://arxiv.org/pdf/2512.01610", "abs": "https://arxiv.org/abs/2512.01610", "authors": ["Yuren Mao", "Peigen Liu", "Xinjian Wang", "Rui Ding", "Jing Miao", "Hui Zou", "Mingjie Qi", "Wanxiang Luo", "Longbin Lai", "Kai Wang", "Zhengping Qian", "Peilun Yang", "Yunjun Gao", "Ying Zhang"], "title": "Agent-Kernel: A MicroKernel Multi-Agent System Framework for Adaptive Social Simulation Powered by LLMs", "comment": null, "summary": "Multi-Agent System (MAS) developing frameworks serve as the foundational infrastructure for social simulations powered by Large Language Models (LLMs). However, existing frameworks fail to adequately support large-scale simulation development due to inherent limitations in adaptability, configurability, reliability, and code reusability. For example, they cannot simulate a society where the agent population and profiles change over time. To fill this gap, we propose Agent-Kernel, a framework built upon a novel society-centric modular microkernel architecture. It decouples core system functions from simulation logic and separates cognitive processes from physical environments and action execution. Consequently, Agent-Kernel achieves superior adaptability, configurability, reliability, and reusability. We validate the framework's superiority through two distinct applications: a simulation of the Universe 25 (Mouse Utopia) experiment, which demonstrates the handling of rapid population dynamics from birth to death; and a large-scale simulation of the Zhejiang University Campus Life, successfully coordinating 10,000 heterogeneous agents, including students and faculty."}
{"id": "2512.01646", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2512.01646", "abs": "https://arxiv.org/abs/2512.01646", "authors": ["Barenya Kumar Nandy", "Rupesh Nasre"], "title": "StarDist: A Code Generator for Distributed Graph Algorithms", "comment": null, "summary": "Relational data, occurring in the real world, are often structured as graphs, which provide the logical abstraction required to make analytical derivations simpler. As graphs get larger, the irregular access patterns exhibited in most graph algorithms, hamper performance. This, along with NUMA and physical memory limits, results in scaling complexities with sequential/shared memory frameworks. StarPlat's MPI backend abstracts away the programmatic complexity involved in designing optimal distributed graph algorithms. It provides an instrument for coding graph algorithms that scale over distributed memory. In this work, we provide an analysis-transformation framework that leverages general semantics associated with iterations involving nodes and their neighbors, within StarPlat, to aggregate communication. The framework scans for patterns that warrant re-ordering in neighborhood access patterns, aggregate communication, and avoid communication altogether with opportunistic caching in reduction constructs. We also architect an optimized bulk-reduction substrate using Open MPI's passive Remote Memory Access (RMA) constructs. We applied our optimization logic to StarPlat's distributed backend and outperformed d-Galois by 2.05 and DRONE by 1.44 times in Single Source Shortest Paths across several big data graphs."}
{"id": "2512.00211", "categories": ["cs.NI", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.00211", "abs": "https://arxiv.org/abs/2512.00211", "authors": ["Gabriele Formis", "Amanda Ericson", "Stefan Forsstrom", "Kyi Thar", "Gianluca Cena", "Stefano Scanzio"], "title": "On the Prediction of Wi-Fi Performance through Deep Learning", "comment": "preprint accepted, 4 pages, 2025", "summary": "Ensuring reliable and predictable communications is one of the main goals in modern industrial systems that rely on Wi-Fi networks, especially in scenarios where continuity of operation and low latency are required. In these contexts, the ability to predict changes in wireless channel quality can enable adaptive strategies and significantly improve system robustness. This contribution focuses on the prediction of the Frame Delivery Ratio (FDR), a key metric that represents the percentage of successful transmissions, starting from time sequences of binary outcomes (success/failure) collected in a real scenario. The analysis focuses on two models of deep learning: a Convolutional Neural Network (CNN) and a Long Short-Term Memory network (LSTM), both selected for their ability to predict the outcome of time sequences. Models are compared in terms of prediction accuracy and computational complexity, with the aim of evaluating their applicability to systems with limited resources. Preliminary results show that both models are able to predict the evolution of the FDR with good accuracy, even from minimal information (a single binary sequence). In particular, CNN shows a significantly lower inference latency, with a marginal loss in accuracy compared to LSTM."}
{"id": "2512.00032", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2512.00032", "abs": "https://arxiv.org/abs/2512.00032", "authors": ["Giuseppe M. Sarda", "Nimish Shah", "Abubakr Nada", "Debjyoti Bhattacharjee", "Marian Verhelst"], "title": "Decoupled Control Flow and Data Access in RISC-V GPGPUs", "comment": null, "summary": "Vortex, a newly proposed open-source GPGPU platform based on the RISC-V ISA, offers a valid alternative for GPGPU research over the broadly-used modeling platforms based on commercial GPUs. Similarly to the push originating from the RISC-V movement for CPUs, Vortex can enable a myriad of fresh research directions for GPUs. However, as a young hardware platform, it currently lacks the performance competitiveness of commercial GPUs, which is crucial for widespread adoption. State-of-the-art GPUs, in fact, rely on complex architectural features, still unavailable in Vortex, to hide the micro-code overheads linked to control flow (CF) management and memory orchestration for data access. In particular, these components account for the majority of the dynamic instruction count in regular, memory-intensive kernels, such as linear algebra routines, which form the basis of many applications, including Machine Learning. To address these challenges with simple yet powerful micro-architecture modifications, this paper introduces decoupled CF and data access through 1.) a hardware CF manager to accelerate branching and predication in regular loop execution and 2.) decoupled memory streaming lanes to further hide memory latency with useful computation. The evaluation results for different kernels show 8$\\times$ faster execution, 10$\\times$ reduction in dynamic instruction count, and overall performance improvement from 0.35 to 1.63 $\\mathrm{GFLOP/s/mm^2}$. Thanks to these enhancements, Vortex can become an ideal playground to enable GPGPU research for the next generation of Machine Learning."}
{"id": "2512.00380", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2512.00380", "abs": "https://arxiv.org/abs/2512.00380", "authors": ["Mingwei Liu", "Zheng Pei", "Yanlin Wang", "Zihao Wang", "Zikang Li", "Enci Lin", "Xin Peng", "Zibin Zheng"], "title": "Framework-Aware Code Generation with API Knowledge Graph-Constructed Data: A Study on HarmonyOS", "comment": null, "summary": "In the context of software frameworks with limited resources (such as HarmonyOS), large language models (LLMs) often exhibit poor code generation performance because they lack sufficient exposure to such environments during pre-training. Although LLMs can usually maintain correct logical structures across programming languages, they frequently struggle when dealing with framework-specific APIs or syntax, resulting in errors. This indicates that while pre-training equips LLMs with general algorithmic capabilities, they remain unfamiliar with the distinctive syntax and API usage of underrepresented frameworks. As a result, even advanced commercial models like GPT-4o cannot reliably generate correct code without prior adaptation. To address this issue, we propose APIKG4SYN, a framework designed to exploit API knowledge graphs for the construction of API-oriented question-code pairs, specifically tailored for low-resource frameworks without requiring executable code. APIKG4SYN integrates both single-API and multi-API knowledge, where the latter is derived through uncertainty estimation (UE)-driven Monte Carlo Tree Search (MCTS), enabling the creation of a diverse and informative dataset for fine-tuning LLMs. Using HarmonyOS as a case study, we build the first benchmark for HarmonyOS code generation. Experimental results show that fine-tuning Qwen with APIKG4SYN raises pass@1 accuracy to 25.00%, compared with 17.59% for the baseline GPT model. These results confirm that API-oriented data significantly enhance LLM performance in low-resource software development scenarios."}
{"id": "2512.01764", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2512.01764", "abs": "https://arxiv.org/abs/2512.01764", "authors": ["Kingshuk Haldar"], "title": "Trace-based, time-resolved analysis of MPI application performance using standard metrics", "comment": "Presented at and submitted to the International Parallel Tools Workshop 2025", "summary": "Detailed trace analysis of MPI applications is essential for performance engineering, but growing trace sizes and complex communication behaviour often render comprehensive visual inspection impractical. This work presents a trace-based calculation of time-resolved values of standard MPI performance metrics, load balance, serialisation, and transfer efficiency, by discretising execution traces into fixed or adaptive time segments. The implementation processes Paraver traces postmortem, reconstructing critical execution paths and handling common event anomalies, such as clock inconsistencies and unmatched MPI events, to robustly calculate metrics for each segment. The calculated per-window metric values expose transient performance bottlenecks that the timeaggregated metrics from existing tools may conceal. Evaluations on a synthetic benchmark and real-world applications (LaMEM and ls1-MarDyn) demonstrate how time-resolved metrics reveal localised performance bottlenecks obscured by global aggregates, offering a lightweight and scalable alternative even when trace visualisation is impractical."}
{"id": "2512.00221", "categories": ["cs.NI"], "pdf": "https://arxiv.org/pdf/2512.00221", "abs": "https://arxiv.org/abs/2512.00221", "authors": ["Stefano Scanzio", "Pietro Chiavassa", "Gianluca Cena"], "title": "Analysis of the operation of a TSN switch and other devices using executable QR codes", "comment": "preprint accepted, 2 pages, 2025", "summary": "Executable QR codes, also known as sQRy, are a technology aimed at inserting executable programs in a QR code. Through a concrete example, in this paper, we demonstrate their usage in the context of industrial networks in order to assess the operation of a TSN switch by analyzing its status LEDs even in the absence of an internet connection. The entire generation chain that is used to create the sQRy, as well as the corresponding execution chain that, starting from the sQRy, runs it on a mobile device, has been detailed through examples."}
{"id": "2512.00035", "categories": ["cs.AR", "cs.OS"], "pdf": "https://arxiv.org/pdf/2512.00035", "abs": "https://arxiv.org/abs/2512.00035", "authors": ["Mislav Has", "Tao Xiong", "Fehmi Ben Abdesslem", "Mario Kuek"], "title": "WebAssembly on Resource-Constrained IoT Devices: Performance, Efficiency, and Portability", "comment": null, "summary": "The increasing heterogeneity of hardware and software in the Internet of Things (IoT) poses a major challenge for the portability, maintainability and deployment of software on devices with limited resources. WebAssembly (WASM), originally designed for the web, is increasingly recognized as a portable, secure and efficient runtime environment that can overcome these challenges. This paper explores the feasibility of using WASM in embedded IoT systems by evaluating its performance, memory footprint and energy consumption on three representative microcontrollers: the Raspberry Pi Pico, the ESP32 C6 and the nRF5340. Two lightweight WASM runtimes, WAMR and wasm3, are compared with the native C execution. The results show that while the native execution remains superior in terms of speed and energy efficiency, WASM offers acceptable trade-offs in return for cross-platform compatibility and sandbox execution. The results highlight that WASM is a viable option for embedded IoT applications when portability and security outweigh strict performance constraints, and that further runtime optimization could extend its practicality in this area."}
{"id": "2512.00556", "categories": ["cs.SE", "cs.CL"], "pdf": "https://arxiv.org/pdf/2512.00556", "abs": "https://arxiv.org/abs/2512.00556", "authors": ["Sina Salimian", "Gias Uddin", "Sumon Biswas", "Henry Leung"], "title": "Bias Testing and Mitigation in Black Box LLMs using Metamorphic Relations", "comment": null, "summary": "The widespread deployment of Large Language Models (LLMs) has intensified concerns about subtle social biases embedded in their outputs. Existing guardrails often fail when faced with indirect or contextually complex bias-inducing prompts. To address these limitations, we propose a unified framework for both systematic bias evaluation and targeted mitigation. Our approach introduces six novel Metamorphic Relations (MRs) that, based on metamorphic testing principles, transform direct bias-inducing inputs into semantically equivalent yet adversarially challenging variants. These transformations enable an automated method for exposing hidden model biases: when an LLM responds inconsistently or unfairly across MR-generated variants, the underlying bias becomes detectable. We further show that the same MRs can be used to generate diverse bias-inducing samples for fine-tuning, directly linking the testing process to mitigation. Using six state-of-the-art LLMs - spanning open-source and proprietary models - and a representative subset of 385 questions from the 8,978-item BiasAsker benchmark covering seven protected groups, our MRs reveal up to 14% more hidden biases compared to existing tools. Moreover, fine-tuning with both original and MR-mutated samples significantly enhances bias resiliency, increasing safe response rates from 54.7% to over 88.9% across models. These results highlight metamorphic relations as a practical mechanism for improving fairness in conversational AI."}
{"id": "2512.00029", "categories": ["cs.NI", "cs.DC"], "pdf": "https://arxiv.org/pdf/2512.00029", "abs": "https://arxiv.org/abs/2512.00029", "authors": ["Andreas Kouloumpris", "Georgios L. Stavrinides", "Maria K. Michael", "Theocharis Theocharides"], "title": "An optimization framework for task allocation in the edge/hub/cloud paradigm", "comment": "This version of the manuscript has been accepted for publication in Future Generation Computer Systems after peer review (Author Accepted Manuscript). It is not the final published version (Version of Record) and does not reflect any post-acceptance improvements. The Version of Record is available online at https://doi.org/10.1016/j.future.2024.02.005", "summary": "With the advent of the Internet of Things (IoT), novel critical applications have emerged that leverage the edge/hub/cloud paradigm, which diverges from the conventional edge computing perspective. A growing number of such applications require a streamlined architecture for their effective execution, often comprising a single edge device with sensing capabilities, a single hub device (e.g., a laptop or smartphone) for managing and assisting the edge device, and a more computationally capable cloud server. Typical examples include the utilization of an unmanned aerial vehicle (UAV) for critical infrastructure inspection or a wearable biomedical device (e.g., a smartwatch) for remote patient monitoring. Task allocation in this streamlined architecture is particularly challenging, due to the computational, communication, and energy limitations of the devices at the network edge. Consequently, there is a need for a comprehensive framework that can address the specific task allocation problem optimally and efficiently. To this end, we propose a complete, binary integer linear programming (BILP) based formulation for an application-driven design-time approach, capable of providing an optimal task allocation in the targeted edge/hub/cloud environment. The proposed method minimizes the desired objective, either the overall latency or overall energy consumption, while considering several crucial parameters and constraints often overlooked in related literature. We evaluate our framework using a real-world use-case scenario, as well as appropriate synthetic benchmarks. Our extensive experimentation reveals that the proposed approach yields optimal and scalable results, enabling efficient design space exploration for different applications and computational devices."}
{"id": "2512.00259", "categories": ["cs.NI"], "pdf": "https://arxiv.org/pdf/2512.00259", "abs": "https://arxiv.org/abs/2512.00259", "authors": ["Diogo Ferreira", "Pedro Ribeiro", "Andr Coelho", "Rui Campos"], "title": "Design and Evaluation of a Multi-Agent Perception System for Autonomous Flying Networks", "comment": null, "summary": "Autonomous Flying Networks (FNs) are emerging as a key enabler of on-demand connectivity in dynamic and infrastructure-limited environments. However, current approaches mainly focus on UAV placement, routing, and resource management, neglecting the autonomous perception of users and their service demands - a critical capability for zero-touch network operation.\n  This paper presents the Multi-Agent Perception System (MAPS), a modular and scalable system that leverages multi-modal large language models (MM-LLMs) and agentic Artificial Intelligence (AI) to interpret visual and audio data collected by UAVs and generate Service Level Specifications (SLSs) describing user count, spatial distribution, and traffic demand. MAPS is evaluated using a synthetic multimodal emergency dataset, achieving user detection accuracies above 70% and SLS generation under 130 seconds in 90% of cases. Results demonstrate that combining audio and visual modalities enhances user detection and show that MAPS provides the perception layer required for autonomous, zero-touch FNs."}
{"id": "2512.00038", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2512.00038", "abs": "https://arxiv.org/abs/2512.00038", "authors": ["He Jiang", "Yi Guo", "Shikai Guo", "Huijiang Liu", "Xiaochen Li", "Ning Wang", "Zhixiong Di"], "title": "Critical Path Aware Timing-Driven Global Placement for Large-Scale Heterogeneous FPGAs", "comment": null, "summary": "Timing optimization during global placement is critical for achieving optimal circuit performance and remains a key challenge in modern Field Programmable Gate Array (FPGA) design. As FPGA designs scale and heterogeneous resources increase, dense interconnects introduce significant resistive and capacitive effects, making timing closure increasingly difficult. Existing methods face challenges in constructing accurate timing models due to multi-factor nonlinear constraints as well as load and crosstalk coupling effects arising in multi-pin driving scenarios. To address these challenges, we propose TD-Placer, a critical path aware, timing-driven global placement framework. It leverages graph-based representations to capture global net interactions and employs a nonlinear model to integrate diverse timing-related features for precise delay prediction, thereby improving the overall placement quality for FPGAs. TD-Placer adopts a quadratic placement objective that minimizes wirelength while incorporating a timing term constructed by a lightweight algorithm, enabling efficient and high-quality timing optimization. Regarding net-level timing contention, it also employs a finer-grained weighting scheme to facilitate smooth reduction of the Critical Path Delay (CPD). Extensive experiments were carried out on seven real-world open-source FPGA projects with LUT counts ranging from 60K to 400K. The results demonstrate that TD-Placer achieves an average 10% improvement in Worst Negative Slack (WNS) and a 5% reduction in CPD compared to the state-of-the-art method, with an average CPD comparable (*1.01) to the commercial AMD Vivado across five versions (2020.2-2024.2). Its code and dataset are publicly available."}
{"id": "2512.00560", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2512.00560", "abs": "https://arxiv.org/abs/2512.00560", "authors": ["Jinyu Cai", "Jialong Li", "Nianyu Li", "Zhenyu Mao", "Mingyue Zhang", "Kenji Tei"], "title": "SAGE: Semantic-Aware Gray-Box Game Regression Testing with Large Language Models", "comment": "This paper has been submitted to the Automated Software Engineering journal", "summary": "The rapid iteration cycles of modern live-service games make regression testing indispensable for maintaining quality and stability. However, existing regression testing approaches face critical limitations, especially in common gray-box settings where full source code access is unavailable: they heavily rely on manual effort for test case construction, struggle to maintain growing suites plagued by redundancy, and lack efficient mechanisms for prioritizing relevant tests. These challenges result in excessive testing costs, limited automation, and insufficient bug detection. To address these issues, we propose SAGE, a semanticaware regression testing framework for gray-box game environments. SAGE systematically addresses the core challenges of test generation, maintenance, and selection. It employs LLM-guided reinforcement learning for efficient, goal-oriented exploration to automatically generate a diverse foundational test suite. Subsequently, it applies a semantic-based multi-objective optimization to refine this suite into a compact, high-value subset by balancing cost, coverage, and rarity. Finally, it leverages LLM-based semantic analysis of update logs to prioritize test cases most relevant to version changes, enabling efficient adaptation across iterations. We evaluate SAGE on two representative environments, Overcooked Plus and Minecraft, comparing against both automated baselines and human-recorded test cases. Across all environments, SAGE achieves superior bug detection with significantly lower execution cost, while demonstrating strong adaptability to version updates."}
{"id": "2512.00083", "categories": ["cs.AR", "cs.DC"], "pdf": "https://arxiv.org/pdf/2512.00083", "abs": "https://arxiv.org/abs/2512.00083", "authors": ["Zhongchun Zhou", "Chengtao Lai", "Wei Zhang"], "title": "LLaMCAT: Optimizing Large Language Model Inference with Cache Arbitration and Throttling", "comment": "Accepted to ICPP 2025", "summary": "Large Language Models (LLMs) have achieved unprecedented success across various applications, but their substantial memory requirements pose significant challenges to current memory system designs, especially during inference. Our work targets last-level cache (LLC) based architectures, including GPUs (e.g., NVIDIA GPUs) and AI accelerators. We introduce LLaMCAT, a novel approach to optimize the LLC for LLM inference. LLaMCAT combines Miss Status Holding Register (MSHR)- and load balance-aware cache arbitration with thread throttling to address stringent bandwidth demands and minimize cache stalls in KV Cache access. We also propose a hybrid simulation framework integrating analytical models with cycle-level simulators via memory traces, balancing architecture detail and efficiency.\n  Experiments demonstrate that LLaMCAT achieves an average speedup of 1.26x when the system is mainly bottlenecked by miss handling throughput, while baselines mostly show negative improvements since they are not optimized for this scenario. When the cache size is also limited, our policy achieves a speedup of 1.58x over the unoptimized version, and a 1.26x improvement over the best baseline (dyncta). Overall, LLaMCAT is the first to target LLM decoding-specific MSHR contention, a gap in previous work. It presents a practical solution for accelerating LLM inference on future hardware platforms."}
{"id": "2512.00491", "categories": ["cs.NI"], "pdf": "https://arxiv.org/pdf/2512.00491", "abs": "https://arxiv.org/abs/2512.00491", "authors": ["Yule Han", "Kezhi Wang", "Kun Yang"], "title": "Smart-TCP: An Agentic AI-based Autonomous and Adaptive TCP Protocol", "comment": "Submitted for possible journal publication", "summary": "The Transmission Control Protocol (TCP) relies on a state machine and deterministic arithmetic to ensure reliable connections. However, traditional protocol logic driven by hard-coded state machines struggles to meet the demands of intelligent and autonomous network architectures. Here, we adopt the agentic AI-based paradigm, driven by Large Language Models (LLMs), characterized by context perception, autonomous reasoning, and tool use. Based on this, we propose Smart-TCP, which re-imagines TCP's core control logic as an autonomous agent. Specifically, the proposed architecture employs a context aggregation mechanism to synthesize the protocol context, utilizes the LLM for autonomous logical reasoning, and invokes an Arithmetic Logic Unit (ALU) as a tool for computation. Furthermore, we establish a dual-agent interaction framework based on this architecture and implement TCP protocol interactions. Experiments demonstrate that the Smart-TCP agent excels in static prediction and error detection, achieving a 93.33% success rate in end-to-end sessions. These results strongly validate the technical feasibility of an agentic AI-based TCP protocol."}
{"id": "2512.00044", "categories": ["cs.AR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.00044", "abs": "https://arxiv.org/abs/2512.00044", "authors": ["Junzhuo Zhou", "Ziwen Wang", "Haoxuan Xia", "Yuxin Yan", "Chengyu Zhu", "Ting-Jung Lin", "Wei Xing", "Lei He"], "title": "SetupKit: Efficient Multi-Corner Setup/Hold Time Characterization Using Bias-Enhanced Interpolation and Active Learning", "comment": null, "summary": "Accurate setup/hold time characterization is crucial for modern chip timing closure, but its reliance on potentially millions of SPICE simulations across diverse process-voltagetemperature (PVT) corners creates a major bottleneck, often lasting weeks or months. Existing methods suffer from slow search convergence and inefficient exploration, especially in the multi-corner setting. We introduce SetupKit, a novel framework designed to break this bottleneck using statistical intelligence, circuit analysis and active learning (AL). SetupKit integrates three key innovations: BEIRA, a bias-enhanced interpolation search derived from statistical error modeling to accelerate convergence by overcoming stagnation issues, initial search interval estimation by circuit analysis and AL strategy using Gaussian Process. This AL component intelligently learns PVT-timing correlations, actively guiding the expensive simulations to the most informative corners, thus minimizing redundancy in multicorner characterization. Evaluated on industrial 22nm standard cells across 16 PVT corners, SetupKit demonstrates a significant 2.4x overall CPU time reduction (from 720 to 290 days on a single core) compared to standard practices, drastically cutting characterization time. SetupKit offers a principled, learningbased approach to library characterization, addressing a critical EDA challenge and paving the way for more intelligent simulation management."}
{"id": "2512.00571", "categories": ["cs.SE", "cs.AI", "cs.LG", "cs.NE"], "pdf": "https://arxiv.org/pdf/2512.00571", "abs": "https://arxiv.org/abs/2512.00571", "authors": ["Tarun Chintada", "Uday Kiran Cheera"], "title": "Enhancing Analogy-Based Software Effort Estimation with Firefly Algorithm Optimization", "comment": "12 pages, 3 figures, 2 tables. Research conducted in June 2024", "summary": "Analogy-Based Estimation (ABE) is a popular method for non-algorithmic estimation due to its simplicity and effectiveness. The Analogy-Based Estimation (ABE) model was proposed by researchers, however, no optimal approach for reliable estimation was developed. Achieving high accuracy in the ABE might be challenging for new software projects that differ from previous initiatives. This study (conducted in June 2024) proposes a Firefly Algorithm-guided Analogy-Based Estimation (FAABE) model that combines FA with ABE to improve estimation accuracy. The FAABE model was tested on five publicly accessible datasets: Cocomo81, Desharnais, China, Albrecht, Kemerer and Maxwell. To improve prediction efficiency, feature selection was used. The results were measured using a variety of evaluation metrics; various error measures include MMRE, MAE, MSE, and RMSE. Compared to conventional models, the experimental results show notable increases in prediction precision, demonstrating the efficacy of the Firefly-Analogy ensemble."}
{"id": "2512.00844", "categories": ["cs.SE", "cs.DC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.00844", "abs": "https://arxiv.org/abs/2512.00844", "authors": ["Giles Winchester", "George Parisis", "Luc Berthouze"], "title": "FC-ADL: Efficient Microservice Anomaly Detection and Localisation Through Functional Connectivity", "comment": "13 pages, 6 figures, 2 tables", "summary": "Microservices have transformed software architecture through the creation of modular and independent services. However, they introduce operational complexities in service integration and system management that makes swift and accurate anomaly detection and localisation challenging. Despite the complex, dynamic, and interconnected nature of microservice architectures, prior works that investigate metrics for anomaly detection rarely include explicit information about time-varying interdependencies. And whilst prior works on fault localisation typically do incorporate information about dependencies between microservices, they scale poorly to real world large-scale deployments due to their reliance on computationally expensive causal inference. To address these challenges we propose FC-ADL, an end-to-end scalable approach for detecting and localising anomalous changes from microservice metrics based on the neuroscientific concept of functional connectivity. We show that by efficiently characterising time-varying changes in dependencies between microservice metrics we can both detect anomalies and provide root cause candidates without incurring the significant overheads of causal and multivariate approaches. We demonstrate that our approach can achieve top detection and localisation performance across a wide degree of different fault scenarios when compared to state-of-the-art approaches. Furthermore, we illustrate the scalability of our approach by applying it to Alibaba's extremely large real-world microservice deployment."}
{"id": "2512.00509", "categories": ["cs.NI"], "pdf": "https://arxiv.org/pdf/2512.00509", "abs": "https://arxiv.org/abs/2512.00509", "authors": ["Sumita Majhi", "Kaushal Shelke", "Pinaki Mitra", "Ujjwal Biswas"], "title": "Improving Channel Estimation Through Gold Sequences", "comment": null, "summary": "This study evaluates Non-Orthogonal Multiple Access (NOMA) systems using Gold coding and Conventional-V-BLAST (C-V-BLAST). Superimposed signals on shared subcarriers make NOMA user separation difficult, unlike MIMO. Gold sequences' orthogonal features may enhance user separation and channel estimation. A novel channel estimation approach uses fractional power allocation and partially decoded data symbols. A realistic simulation environment was created using AWGN, Rayleigh fading, and shadowing. Using pilot signals, power allocation, and data symbols, our Channel Prediction Function (CPF) surpasses pilot-based techniques."}
{"id": "2512.00045", "categories": ["cs.AR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.00045", "abs": "https://arxiv.org/abs/2512.00045", "authors": ["Hung-Ming Huang", "Yu-Hsin Yang", "Fu-Chieh Chang", "Yun-Chia Hsu", "Yin-Yu Lin", "Ming-Fang Tsai", "Chun-Chih Yang", "Pei-Yuan Wu"], "title": "Assessing Large Language Models in Generating RTL Design Specifications", "comment": null, "summary": "As IC design grows more complex, automating comprehension and documentation of RTL code has become increasingly important. Engineers currently should manually interpret existing RTL code and write specifications, a slow and error-prone process. Although LLMs have been studied for generating RTL from specifications, automated specification generation remains underexplored, largely due to the lack of reliable evaluation methods. To address this gap, we investigate how prompting strategies affect RTL-to-specification quality and introduce metrics for faithfully evaluating generated specs. We also benchmark open-source and commercial LLMs, providing a foundation for more automated and efficient specification workflows in IC design."}
{"id": "2512.00651", "categories": ["cs.SE", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.00651", "abs": "https://arxiv.org/abs/2512.00651", "authors": ["Mohammed Latif Siddiq", "Arvin Islam-Gomes", "Natalie Sekerak", "Joanna C. S. Santos"], "title": "Large Language Models for Software Engineering: A Reproducibility Crisis", "comment": "Submitted to Empirical Software Engineering (EMSE) journal; 112 pages (81 pages of references)", "summary": "Reproducibility is a cornerstone of scientific progress, yet its state in large language model (LLM)-based software engineering (SE) research remains poorly understood. This paper presents the first large-scale, empirical study of reproducibility practices in LLM-for-SE research. We systematically mined and analyzed 640 papers published between 2017 and 2025 across premier software engineering, machine learning, and natural language processing venues, extracting structured metadata from publications, repositories, and documentation. Guided by four research questions, we examine (i) the prevalence of reproducibility smells, (ii) how reproducibility has evolved over time, (iii) whether artifact evaluation badges reliably reflect reproducibility quality, and (iv) how publication venues influence transparency practices. Using a taxonomy of seven smell categories: Code and Execution, Data, Documentation, Environment and Tooling, Versioning, Model, and Access and Legal, we manually annotated all papers and associated artifacts. Our analysis reveals persistent gaps in artifact availability, environment specification, versioning rigor, and documentation clarity, despite modest improvements in recent years and increased adoption of artifact evaluation processes at top SE venues. Notably, we find that badges often signal artifact presence but do not consistently guarantee execution fidelity or long-term reproducibility. Motivated by these findings, we provide actionable recommendations to mitigate reproducibility smells and introduce a Reproducibility Maturity Model (RMM) to move beyond binary artifact certification toward multi-dimensional, progressive evaluation of reproducibility rigor."}
{"id": "2512.00998", "categories": ["cs.NI", "eess.SY"], "pdf": "https://arxiv.org/pdf/2512.00998", "abs": "https://arxiv.org/abs/2512.00998", "authors": ["Christof Rhrig", "Benz Cramer"], "title": "LPWAN based IoT Architecture for Distributed Energy Monitoring in Deep Indoor Environments", "comment": null, "summary": "Continuous energy monitoring is essential for identifying potential savings and predicting the energy requirements of buildings. Energy meters are often located in underground spaces that are difficult to reach with wireless technology. This paper presents an experimental study comparing different Low Power Wide Area Networks (LPWAN) technologies in terms of building penetration and radio coverage. The technologies Low Power Long Range Wide Area Networks (LoRaWAN), Narrow Band Internet of Things (NB-IoT), Sigfox 0G and Wireless Smart Ubiquitous Networks (Wi-SUN) are evaluated experimentally. It also proposes a distributed hybrid IoT architecture that combines multiple LPWAN technologies using an abstraction layer to optimize cost and coverage. Communication is message-based using the publish-subscribe messaging pattern. It is implemented using the MQTT protocol. The abstraction layer decodes the proprietary binary data and converts it to a normalized JSON format."}
{"id": "2512.00053", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2512.00053", "abs": "https://arxiv.org/abs/2512.00053", "authors": ["Nikhil Rout", "Blaise Tine"], "title": "A Configurable Mixed-Precision Fused Dot Product Unit for GPGPU Tensor Computation", "comment": "3 pages, 2 figures", "summary": "Efficient mixed-precision MMA operations are critical for accelerating Deep Learning workloads on GPGPUs. However, existing open-source RTL implementations of inner dot products rely on discrete arithmetic units, leading to suboptimal throughput and poor resource utilization. To address these challenges, we propose a scalable mixed-precision dot product unit that integrates floating-point and integer arithmetic pipelines within a singular fused architecture, implemented as part of the open-source RISC-V based Vortex GPGPU's Tensor Core Unit extension. Our design supports low-precision multiplication in (FP16/BF16/FP8/BF8/INT8/UINT4) formats and higher-precision accumulation in (FP32/INT32), with an extensible framework for adding and evaluating other custom representations in the future. Experimental results demonstrate 4-cycle operation latency at 306.6 MHz clock frequency on the AMD Xilinx Alveo U55C FPGA, delivering an ideal filled pipeline throughput of 9.812 GFLOPS in a 4-thread per warp configuration."}
{"id": "2512.00766", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2512.00766", "abs": "https://arxiv.org/abs/2512.00766", "authors": ["Zenghui Zhou", "Yuechen Li", "Yi Cai", "Jinlong Wen", "Xiaohan Yu", "Zheng Zheng", "Beibei Yin"], "title": "Code Comments for Quantum Software Development Kits: An Empirical Study on Qiskit", "comment": "Zenghui Zhou and Yuechen Li contributed equally to this work. Corresponding author is Zheng Zheng", "summary": "Quantum computing is gaining attention from academia and industry. With the quantum Software Development Kits (SDKs), programmers can develop quantum software to explore the power of quantum computing. However, programmers may face challenges in understanding quantum software due to the non-intuitive quantum mechanics. To facilitate software development and maintenance, code comments offered in quantum SDKs serve as a natural language explanation of program functionalities and logical flows. Despite their importance, scarce research systematically reports their value and provides constructive guidelines for programmers. To address this gap, our paper focuses on Qiskit, one of the most popular quantum SDKs, and presents CC4Q, the first dataset of code comments for quantum computing. CC4Q incorporates 9677 code comment pairs and 21970 sentence-level code comment units, the latter of which involve heavy human annotation. Regarding the annotation, we validate the applicability of the developer-intent taxonomy used in classical programs, and also propose a new taxonomy considering quantum-specific knowledge. We conduct an empirical study comprehensively interpreting code comments from three perspectives: comment structure and coverage, developers' intentions, and associated quantum topics. Our findings uncover key differences in code comments between classical and quantum software, and also outline quantum-specific knowledge relevant to quantum software development."}
{"id": "2512.01035", "categories": ["cs.NI", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.01035", "abs": "https://arxiv.org/abs/2512.01035", "authors": ["Shutong Chen", "Qi Liao", "Adnan Aijaz", "Yansha Deng"], "title": "Goal-Oriented Multi-Agent Semantic Networking: Unifying Intents, Semantics, and Intelligence", "comment": "Submitting to IEEE for potential publications", "summary": "6G services are evolving toward goal-oriented and AI-native communication, which are expected to deliver transformative societal benefits across various industries and promote energy sustainability. Yet today's networking architectures, built on complete decoupling of the applications and the network, cannot expose or exploit high-level goals, limiting their ability to adapt intelligently to service needs. This work introduces Goal-Oriented Multi-Agent Semantic Networking (GoAgentNet), a new architecture that elevates communication from data exchange to goal fulfilment. GoAgentNet enables applications and the network to collaborate by abstracting their functions into multiple collaborative agents, and jointly orchestrates multi-agent sensing, networking, computation, and control through semantic computation and cross-layer semantic networking, allowing the entire architecture to pursue unified application goals. We first outline the limitations of legacy network designs in supporting 6G services, based on which we highlight key enablers of our GoAgentNet design. Then, through three representative 6G usage scenarios, we demonstrate how GoAgentNet can unlock more efficient and intelligent services. We further identify unique challenges faced by GoAgentNet deployment and corresponding potential solutions. A case study on robotic fault detection and recovery shows that our GoAgentNet architecture improves energy efficiency by up to 99% and increases the task success rate by up to 72%, compared with the existing networking architectures without GoAgentNet, which underscores its potential to support scalable and sustainable 6G systems."}
{"id": "2512.00055", "categories": ["cs.AR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.00055", "abs": "https://arxiv.org/abs/2512.00055", "authors": ["Sohaib Errabii", "Olivier Sentieys", "Marcello Traiola"], "title": "KAN-SAs: Efficient Acceleration of Kolmogorov-Arnold Networks on Systolic Arrays", "comment": null, "summary": "Kolmogorov-Arnold Networks (KANs) have garnered significant attention for their promise of improved parameter efficiency and explainability compared to traditional Deep Neural Networks (DNNs). KANs' key innovation lies in the use of learnable non-linear activation functions, which are parametrized as splines. Splines are expressed as a linear combination of basis functions (B-splines). B-splines prove particularly challenging to accelerate due to their recursive definition. Systolic Array (SA)based architectures have shown great promise as DNN accelerators thanks to their energy efficiency and low latency. However, their suitability and efficiency in accelerating KANs have never been assessed. Thus, in this work, we explore the use of SA architecture to accelerate the KAN inference. We show that, while SAs can be used to accelerate part of the KAN inference, their utilization can be reduced to 30%. Hence, we propose KAN-SAs, a novel SA-based accelerator that leverages intrinsic properties of B-splines to enable efficient KAN inference. By including a nonrecursive B-spline implementation and leveraging the intrinsic KAN sparsity, KAN-SAs enhances conventional SAs, enabling efficient KAN inference, in addition to conventional DNNs. KAN-SAs achieves up to 100% SA utilization and up to 50% clock cycles reduction compared to conventional SAs of equivalent area, as shown by hardware synthesis results on a 28nm FD-SOI technology. We also evaluate different configurations of the accelerator on various KAN applications, confirming the improved efficiency of KAN inference provided by KAN-SAs."}
{"id": "2512.00844", "categories": ["cs.SE", "cs.DC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.00844", "abs": "https://arxiv.org/abs/2512.00844", "authors": ["Giles Winchester", "George Parisis", "Luc Berthouze"], "title": "FC-ADL: Efficient Microservice Anomaly Detection and Localisation Through Functional Connectivity", "comment": "13 pages, 6 figures, 2 tables", "summary": "Microservices have transformed software architecture through the creation of modular and independent services. However, they introduce operational complexities in service integration and system management that makes swift and accurate anomaly detection and localisation challenging. Despite the complex, dynamic, and interconnected nature of microservice architectures, prior works that investigate metrics for anomaly detection rarely include explicit information about time-varying interdependencies. And whilst prior works on fault localisation typically do incorporate information about dependencies between microservices, they scale poorly to real world large-scale deployments due to their reliance on computationally expensive causal inference. To address these challenges we propose FC-ADL, an end-to-end scalable approach for detecting and localising anomalous changes from microservice metrics based on the neuroscientific concept of functional connectivity. We show that by efficiently characterising time-varying changes in dependencies between microservice metrics we can both detect anomalies and provide root cause candidates without incurring the significant overheads of causal and multivariate approaches. We demonstrate that our approach can achieve top detection and localisation performance across a wide degree of different fault scenarios when compared to state-of-the-art approaches. Furthermore, we illustrate the scalability of our approach by applying it to Alibaba's extremely large real-world microservice deployment."}
{"id": "2512.01083", "categories": ["cs.NI", "cs.ET"], "pdf": "https://arxiv.org/pdf/2512.01083", "abs": "https://arxiv.org/abs/2512.01083", "authors": ["Fatih E. Bilgen", "A. Sila Okcu", "O. Tansel Baydas", "Ozgur B. Akan"], "title": "Internet of Intelligent Reflecting Surfaces (IoIRS)", "comment": "8 pages, 3 figures", "summary": "Intelligent Reflecting Surfaces (IRS) are anticipated to serve as a key cornerstone of future wireless networks, providing an unmatched capability to deterministically shape electromagnetic wave propagation. Despite this potential, most existing research still considers the IRS merely as a standalone physical-layer component, controlled by transmitters. However, as networks grow to encompass a massive number of these surfaces and a massive number of transmitters wishing to use them, this transmitter-centric design encounters substantial challenges. To overcome this challenge, we propose the Internet of IRS (IoIRS), an architecture that reconceives the IRS not just as a passive reflecting surface, but as a connected, hybrid entity functioning across both the physical layer and upper network layers. We present the conceptual framework and a preliminary protocol suite necessary to integrate these surfaces into the higher network layers. We conclude by examining how IoIRS architectures could be applied in practice, as their deployment will be essential for fully realizing the capabilities of future wireless networks."}
{"id": "2512.00059", "categories": ["cs.AR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.00059", "abs": "https://arxiv.org/abs/2512.00059", "authors": ["Swastik Bhattacharya", "Sanjay Das", "Anand Menon", "Shamik Kundu", "Arnab Raha", "Kanad Basu"], "title": "SafeCiM: Investigating Resilience of Hybrid Floating-Point Compute-in-Memory Deep Learning Accelerators", "comment": null, "summary": "Deep Neural Networks (DNNs) continue to grow in complexity with Large Language Models (LLMs) incorporating vast numbers of parameters. Handling these parameters efficiently in traditional accelerators is limited by data-transmission bottlenecks, motivating Compute-in-Memory (CiM) architectures that integrate computation within or near memory to reduce data movement. Recent work has explored CiM designs using Floating-Point (FP) and Integer (INT) operations. FP computations typically deliver higher output quality due to their wider dynamic range and precision, benefiting precision-sensitive Generative AI applications. These include models such as LLMs, thus driving advancements in FP-CiM accelerators. However, the vulnerability of FP-CiM to hardware faults remains underexplored, posing a major reliability concern in mission-critical settings. To address this gap, we systematically analyze hardware fault effects in FP-CiM by introducing bit-flip faults at key computational stages, including digital multipliers, CiM memory cells, and digital adder trees. Experiments with Convolutional Neural Networks (CNNs) such as AlexNet and state-of-the-art LLMs including LLaMA-3.2-1B and Qwen-0.3B-Base reveal how faults at each stage affect inference accuracy. Notably, a single adder fault can reduce LLM accuracy to 0%. Based on these insights, we propose a fault-resilient design, SafeCiM, that mitigates fault impact far better than a naive FP-CiM with a pre-alignment stage. For example, with 4096 MAC units, SafeCiM reduces accuracy degradation by up to 49x for a single adder fault compared to the baseline FP-CiM architecture."}
{"id": "2512.00855", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2512.00855", "abs": "https://arxiv.org/abs/2512.00855", "authors": ["Miikka Kuutila", "Paul Ralph", "Huilian Sophie Qiu", "Ronnie de Souza Santos", "Morakot Choetkiertikul", "Amin Milani Fard", "Rana Alkadhi", "Xavier Devroey", "Gregorio Robles", "Hideaki Hata", "Sebastian Baltes", "Vladimir Kovalenko", "Shalini Chakraborty", "Eray Tuzun", "Hera Arif", "Gianisa Adisaputri", "Kelly Garcs", "Anielle S. L. Andrade", "Eyram Amedzor", "Bimpe Ayoola", "Keisha Gaspard-Chickoree", "Arazoo Hoseyni"], "title": "The Software Infrastructure Attitude Scale (SIAS): A Questionnaire Instrument for Measuring Professionals' Attitudes Toward Technical and Sociotechnical Infrastructure", "comment": "Accepted to ICSE 2026, 11 pages + 2 for references, 1 figure, 7 tables", "summary": "Context: Recent software engineering (SE) research has highlighted the need for sociotechnical research, implying a demand for customized psychometric scales. Objective: We define the concepts of technical and sociotechnical infrastructure in software engineering, and develop and validate a psychometric scale that measures attitudes toward them. Method: Grounded in theories of infrastructure, attitudes, and prior work on psychometric measurement, we defined the target constructs and generated scale items. The scale was administered to 225 software professionals and evaluated using a split sample. We conducted an exploratory factor analysis (EFA) on one half of the sample to uncover the underlying factor structure and performed a confirmatory factor analysis (CFA) on the other half to validate the structure. Further analyses with the whole sample assessed face, criterion-related, and discriminant validity. Results: EFA supported a two-factor structure (technical and sociotechnical infrastructure), accounting for 65% of the total variance with strong loadings. CFA confirmed excellent model fit. Face and content validity were supported by the item content reflecting cognitive, affective, and behavioral components. Both subscales were correlated with job satisfaction, perceived autonomy, and feedback from the job itself, supporting convergent validity. Regression analysis supported criterion-related validity, while the Heterotrait-Monotrait ratio of correlations (HTMT), the Fornell-Larcker criterion, and model comparison all supported discriminant validity. Discussion: The resulting scale is a valid instrument for measuring attitudes toward technical and sociotechnical infrastructure in software engineering research. Our work contributes to ongoing efforts to integrate psychological measurement rigor into empirical and behavioral software engineering research."}
{"id": "2512.01088", "categories": ["cs.NI"], "pdf": "https://arxiv.org/pdf/2512.01088", "abs": "https://arxiv.org/abs/2512.01088", "authors": ["Jingxiang Huang", "Samer Lahoud"], "title": "Physical-Layer Analysis of LoRa Robustness in the Presence of Narrowband Interference", "comment": null, "summary": "With the rapid development of Internet of Things (IoT) technologies, the sub-GHz unlicensed spectrum is increasingly being shared by protocols such as Long Range (LoRa), Sigfox, and Long-Range Frequency-Hopping Spread Spectrum (LR-FHSS). These protocols must coexist within the same frequency bands, leading to mutual interference. This paper investigates the physical-layer impact of two types of narrowband signals (BPSK and GMSK) on LoRa demodulation. We employ symbol-level Monte Carlo simulations to analyse how the interference-to-noise ratio (INR) affects the symbol error rate (SER) at a given signal-to-noise ratio (SNR) and noise floor, and then compare the results with those for additive white Gaussian noise (AWGN) of equal power. We demonstrate that modelling narrowband interference as additive white Gaussian noise (AWGN) systematically overestimates the SER of Chirp Spread Spectrum (CSS) demodulation. We also clarify the distinct impairment levels induced by AWGN and two types of narrowband interferers, and provide physical insight into the underlying mechanisms. Finally, we fit a two-segment function for the maximum INR that ensures correct demodulation across SNRs, with one segment for low SNR and the other for high SNR."}
{"id": "2512.00070", "categories": ["cs.AR", "cs.AI", "cs.LG", "eess.IV"], "pdf": "https://arxiv.org/pdf/2512.00070", "abs": "https://arxiv.org/abs/2512.00070", "authors": ["Sungyu Jeong", "Minsu Kim", "Byungsub Kim"], "title": "A CNN-Based Technique to Assist Layout-to-Generator Conversion for Analog Circuits", "comment": null, "summary": "We propose a technique to assist in converting a reference layout of an analog circuit into the procedural layout generator by efficiently reusing available generators for sub-cell creation. The proposed convolutional neural network (CNN) model automatically detects sub-cells that can be generated by available generator scripts in the library, and suggests using them in the hierarchically correct places of the generator software. In experiments, the CNN model examined sub-cells of a high-speed wireline receiver that has a total of 4,885 sub-cell instances including different 145 sub-cell designs. The CNN model classified the sub-cell instances into 51 generatable and one not-generatable classes. One not-generatable class indicates that no available generator can generate the classified sub-cell. The CNN model achieved 99.3% precision in examining the 145 different sub-cell designs. The CNN model greatly reduced the examination time to 18 seconds from 88 minutes required in manual examination. Also, the proposed CNN model could correctly classify unfamiliar sub-cells that are very different from the training dataset."}
{"id": "2512.00867", "categories": ["cs.SE", "cs.HC"], "pdf": "https://arxiv.org/pdf/2512.00867", "abs": "https://arxiv.org/abs/2512.00867", "authors": ["Obada Kraishan"], "title": "The AI Attribution Paradox: Transparency as Social Strategy in Open-Source Software Development", "comment": "23 pages, 7 figures, 9 tables", "summary": "AI coding assistants have transformed software development, raising questions about transparency and attribution practices. We examine the \"AI attribution paradox\": how developers strategically balance acknowledging AI assistance with managing community scrutiny. Analyzing 14,300 GitHub commits across 7,393 repositories from 2023-2025, we investigated attribution strategies and community responses across eight major AI tools. Results reveal widespread AI usage (95.2% of commits) but strategic attribution: only 29.5% employ explicit disclosure, with dramatic tool variation (Claude 80.5% versus Copilot 9.0%). Explicit attribution triggers modest scrutiny (23% more questions and 21% more comments) but tool choice matters 20-30 times more for predicting reception. Community sentiment remains neutral regardless of attribution type, suggesting curiosity rather than hostility. Temporal analyses show rapid norm evolution: explicit attribution increased from near-zero in early 2024 to 40% by late 2025, indicating community adaptation. These findings illuminate attribution as strategic communication rather than simple transparency, advancing understanding of algorithmic accountability and norm formation during technological transitions. We discuss implications for developers navigating disclosure decisions, platforms designing attribution mechanisms, and researchers studying emergent practices in AI-augmented collaborative work."}
{"id": "2512.01477", "categories": ["cs.NI"], "pdf": "https://arxiv.org/pdf/2512.01477", "abs": "https://arxiv.org/abs/2512.01477", "authors": ["Saso Nikolovski", "Pece Mitrevski"], "title": "Modeling and Simulation of Data Protection Systems for Business Continuity and Disaster Recovery", "comment": "17 pages, 11 figures, 9 tables", "summary": "In today's corporate landscape, particularly where operations rely heavily on information technologies, establishing a robust business continuity plan, including a disaster recovery strategy, is essential for ensuring swift recuperation following outages. This study presents a comparative analysis of recovery solutions, focusing on systems that operate partially or entirely within cloud environments and assessing their reliability in fulfilling organizational roles securely and dependably. Two such systems were deployed and evaluated in a real-world production setting. Key performance and reliability metrics were identified using simulation software to enhance these systems, alongside a System Dynamics analysis conducted for each. This work proposes a comprehensive framework for selecting and maintaining data protection and recovery solutions within organizational structures, outlining criteria for aligning chosen approaches with operational needs while adhering to predetermined timelines specified in business continuity and disaster recovery plans. The resulting analysis and findings offer actionable insights to guide decision-making when selecting appropriate recovery concepts."}
{"id": "2512.00079", "categories": ["cs.AR", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.00079", "abs": "https://arxiv.org/abs/2512.00079", "authors": ["Bin Sun", "Rengang Zhang", "Zhiteng Chao", "Zizhen Liu", "Jianan Mu", "Jing Ye", "Huawei Li"], "title": "InF-ATPG: Intelligent FFR-Driven ATPG with Advanced Circuit Representation Guided Reinforcement Learning", "comment": "9 pages,6 figures", "summary": "Automatic test pattern generation (ATPG) is a crucial process in integrated circuit (IC) design and testing, responsible for efficiently generating test patterns. As semiconductor technology progresses, traditional ATPG struggles with long execution times to achieve the expected fault coverage, which impacts the time-to-market of chips. Recent machine learning techniques, like reinforcement learning (RL) and graph neural networks (GNNs), show promise but face issues such as reward delay in RL models and inadequate circuit representation in GNN-based methods. In this paper, we propose InF-ATPG, an intelligent FFR-driven ATPG framework that overcomes these challenges by using advanced circuit representation to guide RL. By partitioning circuits into fanout-free regions (FFRs) and incorporating ATPG-specific features into a novel QGNN architecture, InF-ATPG enhances test pattern generation efficiency. Experimental results show InF-ATPG reduces backtracks by 55.06\\% on average compared to traditional methods and 38.31\\% compared to the machine learning approach, while also improving fault coverage."}
{"id": "2512.00869", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2512.00869", "abs": "https://arxiv.org/abs/2512.00869", "authors": ["Miikka Kuutila", "Paul Ralph", "Huilian Sophie Qiu", "Ronnie de Souza Santos", "Morakot Choetkiertikul", "Rana Alkadhi", "Xavier Devroey", "Gregorio Robles", "Hideaki Hata", "Sebastian Baltes", "Hera Arif", "Vladimir Kovalenko", "Shalini Chakraborty", "Eray Tuzun", "Gianisa Adisaputri"], "title": "Staying or Leaving? How Job Satisfaction, Embeddedness and Antecedents Predict Turnover Intentions of Software Professionals", "comment": "11 pages, 1 figure, 7 tables. Accepted to ICSE 2026 research track", "summary": "Context: Voluntary turnover is common in the software industry, increasing recruitment and onboarding costs and the risk of losing organizational and tacit knowledge. Objective: This study investigates how job satisfaction, work-life balance, job embeddedness, and their antecedents, including job quality, personality traits, attitudes toward technical and sociotechnical infrastructure, and perceptions of organizational justice, relate to software professionals' turnover intentions. Method: We conducted a geographically diverse cross-sectional survey of software professionals (N = 224) and analyzed the data using partial least squares structural equation modeling (PLS-SEM). Our model includes both reflective and formative constructs and tests 15 hypotheses grounded in occupational psychology and software engineering literature. Results: Job satisfaction and embeddedness were significantly negatively associated with software professionals' turnover intentions, while work-life balance showed no direct effect. The strongest antecedents for job satisfaction were work-life balance and job quality, while organizational justice was the strongest predictor of job embeddedness. Discussion: The resulting PLS-SEM model has considerably higher explanatory power for key outcome variables than previous work conducted in the software development context, highlighting the importance of both psychological (e.g., job satisfaction, job embeddedness) and organizational (e.g., organizational justice, job quality) factors in understanding turnover intentions of software professionals. Our results imply that improving job satisfaction and job embeddedness is the key to retaining software professionals. In turn, enhancing job quality, supporting work-life balance, and ensuring high organizational justice can improve job satisfaction and embeddedness, indirectly reducing turnover intentions."}
{"id": "2512.01571", "categories": ["cs.NI"], "pdf": "https://arxiv.org/pdf/2512.01571", "abs": "https://arxiv.org/abs/2512.01571", "authors": ["Xiao Xu", "Qiong Wu", "Pingyi Fan", "Kezhi Wang", "Nan Cheng", "Wen Chen", "Khaled B. Letaief"], "title": "Velocity-Adaptive Access Scheme for Semantic-Aware Vehicular Networks: Joint Fairness and AoI Optimization", "comment": "This paper has been submitted to IEEE transactions on moblie computing", "summary": "In this paper, we address the problem of fair access and Age of Information (AoI) optimization in 5G New Radio (NR) Vehicle to Everything (V2X) Mode 2. Specifically, vehicles need to exchange information with the road side unit (RSU). However, due to the varying vehicle speeds leading to different communication durations, the amount of data exchanged between different vehicles and the RSU may vary. This may poses significant safety risks in high-speed environments. To address this, we define a fairness index through tuning the selection window of different vehicles and consider the image semantic communication system to reduce latency. However, adjusting the selection window may affect the communication time, thereby impacting the AoI. Moreover, considering the re-evaluation mechanism in 5G NR, which helps reduce resource collisions, it may lead to an increase in AoI. We analyze the AoI using Stochastic Hybrid System (SHS) and construct a multi-objective optimization problem to achieve fair access and AoI optimization. Sequential Convex Approximation (SCA) is employed to transform the non-convex problem into a convex one, and solve it using convex optimization. We also provide a large language model (LLM) based algorithm. The scheme's effectiveness is validated through numerical simulations."}
{"id": "2512.00083", "categories": ["cs.AR", "cs.DC"], "pdf": "https://arxiv.org/pdf/2512.00083", "abs": "https://arxiv.org/abs/2512.00083", "authors": ["Zhongchun Zhou", "Chengtao Lai", "Wei Zhang"], "title": "LLaMCAT: Optimizing Large Language Model Inference with Cache Arbitration and Throttling", "comment": "Accepted to ICPP 2025", "summary": "Large Language Models (LLMs) have achieved unprecedented success across various applications, but their substantial memory requirements pose significant challenges to current memory system designs, especially during inference. Our work targets last-level cache (LLC) based architectures, including GPUs (e.g., NVIDIA GPUs) and AI accelerators. We introduce LLaMCAT, a novel approach to optimize the LLC for LLM inference. LLaMCAT combines Miss Status Holding Register (MSHR)- and load balance-aware cache arbitration with thread throttling to address stringent bandwidth demands and minimize cache stalls in KV Cache access. We also propose a hybrid simulation framework integrating analytical models with cycle-level simulators via memory traces, balancing architecture detail and efficiency.\n  Experiments demonstrate that LLaMCAT achieves an average speedup of 1.26x when the system is mainly bottlenecked by miss handling throughput, while baselines mostly show negative improvements since they are not optimized for this scenario. When the cache size is also limited, our policy achieves a speedup of 1.58x over the unoptimized version, and a 1.26x improvement over the best baseline (dyncta). Overall, LLaMCAT is the first to target LLM decoding-specific MSHR contention, a gap in previous work. It presents a practical solution for accelerating LLM inference on future hardware platforms."}
{"id": "2512.01141", "categories": ["cs.SE", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.01141", "abs": "https://arxiv.org/abs/2512.01141", "authors": ["Muhammad Yousuf", "Akshat Bagade", "Chhittebbayi Penugonda", "Maanas Baraya"], "title": "Neural Variable Name Repair: Learning to Rename Identifiers for Readability", "comment": null, "summary": "Developers routinely work with source files whose variable names are generic or misleading, and with teams moving quickly, many functions are left undocumented. This slows comprehension, increases the risk of subtle bugs, and makes it harder for both humans and large language models (LLMs) to reason about code. We study variable name repair: given a real C++ function where all occurrences of one local or parameter name have been replaced by a placeholder (e.g. ID 1), the goal is to generate a natural, descriptive replacement name. We automatically construct this task from the C++ portion of BigCode's The Stack by parsing functions with Tree-sitter, masking a single identifier, and treating the original name as supervision. On top of Llama 3.1-8B, we build a pipeline with (i) warmup and dropout schedules for more stable fine-tuning, (ii) LoRA adapters for efficient specialization on identifier repair, and (iii) a dual-encoder reranker over top-k generator candidates. We evaluate using exact match, Top-5 Hit, and an embedding-based partial similarity score (0-100) that gives credit for near synonyms and format variants (e.g., jsonValue vs. json). On a held-out set of 200 C++ functions, a zero-shot Llama 3.1 baseline reaches 6.1 percent exact match. Our best LoRA-tuned model (with warmup and dropout) achieves 43.1 percent exact match, 50.2 percent Top-5 Hit, and an 82.03 partial-match score. A dual encoder reranker further improves selection quality without modifying the underlying generator, suggesting that task-specific fine-tuning plus reranking is a promising approach for practical identifier repair tools."}
{"id": "2512.01824", "categories": ["cs.NI", "cs.NE"], "pdf": "https://arxiv.org/pdf/2512.01824", "abs": "https://arxiv.org/abs/2512.01824", "authors": ["Jssica Conscincia", "Antnio Grilo"], "title": "HERMES: Heterogeneous Application-Enabled Routing Middleware for Edge-IoT Systems", "comment": "14 pages", "summary": "The growth of the Internet of Things has enabled a new generation of applications, pushing computation and intelligence toward the network edge. This trend, however, exposes challenges, as the heterogeneity of devices and the complex requirements of applications are often misaligned with the assumptions of traditional routing protocols, which lack the flexibility to accommodate application-layer metrics and policies. This work addresses this gap by proposing a software framework that enhances routing flexibility by dynamically incorporating application-aware decisions. The core of the work establishes a multi-hop Wi-Fi network of heterogeneous devices, specifically ESP8266, ESP32, and Raspberry Pi 3B. The routing layer follows a proactive approach, while the network is fault-tolerant, maintaining operation despite both node loss and message loss. On top of this, a middleware layer introduces three strategies for influencing routing behavior: two adapt the path a message traverses until arriving at the destination, while the third allows applications to shape the network topology. This layer offers a flexible interface for diverse applications. The framework was validated on a physical testbed through edge intelligence use cases, including distributing neural network inference computations across multiple devices and offloading the entire workload to the most capable node. Distributed inference is useful in scenarios requiring low latency, energy efficiency, privacy, and autonomy. Experimental results indicated that device heterogeneity significantly impacts network performance. Throughput and inference duration analysis showed the influence of the strategies on application behaviour, revealed that topology critically affects decentralized performance, and demonstrated the suitability of the framework for complex tasks."}
{"id": "2512.00096", "categories": ["cs.AR", "cs.ET"], "pdf": "https://arxiv.org/pdf/2512.00096", "abs": "https://arxiv.org/abs/2512.00096", "authors": ["Mahdi Aghaei", "Saba Ebrahimi", "Mohammad Saleh Arafati", "Elham Cheshmikhani", "Dara Rahmati", "Saeid Gorgin", "Jungrae Kim"], "title": "Modeling and Simulation Frameworks for Processing-in-Memory Architectures", "comment": null, "summary": "Processing-in-Memory (PIM) has emerged as a promising computing paradigm to address the memory wall and the fundamental bottleneck of the von Neumann architecture by reducing costly data movement between memory and processing units. As with any engineering challenge, identifying the most effective solutions requires thorough exploration of diverse architectural proposals, device technologies, and application domains. In this context, simulation plays a critical role in enabling researchers to evaluate, compare, and refine PIM designs prior to fabrication. Over the past decade, a variety of PIM simulators have been introduced, spanning low-level device models, architectural frameworks, and application-oriented environments. These tools differ significantly in fidelity, scalability, supported memory/compute technologies, and benchmark compatibility. Understanding these trade-offs is essential for researchers to select appropriate simulators that accurately map and validate their research efforts. This chapter provides a comprehensive overview of PIM simulation methodologies and tools. We categorize simulators according to abstraction levels, design objectives, and evaluation metrics, highlighting representative examples. To improve accessibility, some content may appear in multiple contexts to guide readers with different backgrounds. We also survey benchmark suites commonly employed in PIM studies and discuss open challenges in simulation methodology, paving the way for more reliable, scalable, and efficient PIM modeling."}
{"id": "2512.01155", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.01155", "abs": "https://arxiv.org/abs/2512.01155", "authors": ["Krishna Kumaar Sharma"], "title": "Beyond Greenfield: AI-Driven Productivity in Documentation and Brownfield Engineering", "comment": "53 pages, 7 figures", "summary": "Brownfield engineering work involving legacy systems, incomplete documentation, and fragmented architectural knowledge poses unique challenges for the effective use of large language models (LLMs). Prior research has largely focused on greenfield or synthetic tasks, leaving a gap in structured workflows for complex, context-heavy environments. This paper introduces the Discover-Define-Deliver (D3) Framework, a disciplined LLM-assisted workflow that combines role-separated prompting strategies with applied best practices for navigating ambiguity in brownfield systems. The framework incorporates a dual-agent prompting architecture in which a Builder model generates candidate outputs and a Reviewer model provides structured critique to improve reliability. I conducted an exploratory survey study with 52 software practitioners who applied the D3 workflow to real-world engineering tasks such as legacy system exploration, documentation reconstruction, and architectural refactoring. Respondents reported perceived improvements in task clarity, documentation quality, and cognitive load, along with self-estimated productivity gains. In this exploratory study, participants reported a weighted average productivity improvement of 26.9%, reduced cognitive load for approximately 77% of participants, and reduced rework for 83% during the Define phase. As these findings are self-reported and not derived from controlled experiments, they should be interpreted as preliminary evidence of practitioner sentiment rather than causal effects. The results highlight both the potential and limitations of structured LLM workflows for legacy engineering systems and motivate future controlled evaluations."}
{"id": "2512.01829", "categories": ["cs.NI", "eess.SP"], "pdf": "https://arxiv.org/pdf/2512.01829", "abs": "https://arxiv.org/abs/2512.01829", "authors": ["Salah Abdeljabar", "Marco Zennaro", "Mohamed-Slim Alouini"], "title": "Delay Tolerant Networking to Extend Connectivity in Rural Areas Using Public Transport Systems: Design And Analysis", "comment": null, "summary": "In today's digital age, access to the Internet is essential, yet a significant digital divide exists, particularly in rural areas of developing nations. This paper presents a Delay Tolerant Networking (DTN) framework that utilizes informal public transportation systems, such as minibus taxis, as mobile data mules to enhance connectivity in these underserved regions. We develop a probabilistic model to capture the randomness in vehicle mobility, including travel times and contact durations at bus stops. Key performance metrics are analyzed, including average data transmission rate and Peak Age of Information (PAoI), to assess the effectiveness of the proposed system. An analytical approximation for the Mean PAoI (MPAoI) is derived and validated through simulations. Case studies from real-world datasets in Nouakchott, Accra, and Addis Ababa demonstrate the practical applicability and scalability of our framework. The findings indicate that leveraging existing transportation networks can significantly bridge the digital divide by providing reliable internet-like connectivity to remote areas."}
{"id": "2512.00112", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2512.00112", "abs": "https://arxiv.org/abs/2512.00112", "authors": ["Elham Cheshmikhani", "Hamed Farbeh"], "title": "An Analytical and Empirical Investigation of Tag Partitioning for Energy-Efficient Reliable Cache", "comment": null, "summary": "Associative cache memory significantly influences processor performance and energy consumption. Because it occupies over half of the chip area, cache memory is highly susceptible to transient and permanent faults, posing reliability challenges. As the only hardware-managed memory module, the cache tag array is the most active and critical component, dominating both energy usage and error rate. Tag partitioning is a widely used technique to reduce tag-access energy and enhance reliability. It divides tag comparison into two phases: first comparing the k lower bits, and then activating only the matching tag entries to compare the remaining higher bits. The key design parameter is the selection of the tag-splitting point k, which determines how many reads are eliminated. However, prior studies have chosen k intuitively, randomly, or empirically, without justification. Even experimentally determined values are ad-hoc and do not generalize across cache configurations due to high sensitivity to architectural parameters.\n  In this paper, we analytically show that choosing k too large or too small substantially reduces the effectiveness of tag partitioning. We then derive a formulation that determines the optimal splitting point based on cache configuration parameters. The formulation is convex, differentiable, and capable of precisely quantifying tag-partitioning efficiency for any k and configuration. To validate our model, we experimentally evaluate tag-partitioning efficiency and optimal k across a broad set of cache designs and demonstrate close agreement between analytical and experimental results. The proposed formulation enables designers and researchers to instantly compute the optimal tag-splitting point and accurately estimate tag-read reduction."}
{"id": "2512.01232", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.01232", "abs": "https://arxiv.org/abs/2512.01232", "authors": ["Donghao Huang", "Shila Chew", "Anna Dutkiewicz", "Zhaoxia Wang"], "title": "LLM-as-a-Judge for Scalable Test Coverage Evaluation: Accuracy, Operational Reliability, and Cost", "comment": "7 pages, accepted by the AAAI 2026 Workshop on Next Gen Code Development with Collaborative AI Agents", "summary": "Assessing software test coverage at scale remains a bottleneck in QA pipelines. We present LLM-as-a-Judge (LAJ), a production-ready, rubric-driven framework for evaluating Gherkin acceptance tests with structured JSON outputs. Across 20 model configurations (GPT-4, GPT-5 with varying reasoning effort, and open-weight models) on 100 expert-annotated scripts over 5 runs (500 evaluations), we provide the first comprehensive analysis spanning accuracy, operational reliability, and cost. We introduce the Evaluation Completion Rate (ECR@1) to quantify first-attempt success, revealing reliability from 85.4% to 100.0% with material cost implications via retries. Results show that smaller models can outperform larger ones: GPT-4o Mini attains the best accuracy (6.07 MAAE), high reliability (96.6% ECR@1), and low cost ($1.01 per 1K), yielding a 78x cost reduction vs. GPT-5 (high reasoning) while improving accuracy. Reasoning effort is model-family dependent: GPT-5 benefits from increased reasoning (with predictable accuracy-cost tradeoffs), whereas open-weight models degrade across all dimensions as reasoning increases. Overall, cost spans 175x ($0.45-$78.96 per 1K). We release the dataset, framework, and code to support reproducibility and deployment."}
{"id": "2512.01039", "categories": ["cs.DC", "cs.LG", "cs.NI"], "pdf": "https://arxiv.org/pdf/2512.01039", "abs": "https://arxiv.org/abs/2512.01039", "authors": ["Aladin Djuhera", "Fernando Koch", "Alecio Binotto"], "title": "Joint Partitioning and Placement of Foundation Models for Real-Time Edge AI", "comment": null, "summary": "Inference over large-scale foundation models within heterogeneous edge environments necessitates a fundamentally reconfigurable orchestration substrate. Static partitioning of model layers presumes temporal stability across compute and network resources, which is misaligned with the volatility of real-world deployments. We introduce a framework in which both the spatial placement and internal segmentation of foundation models are elevated to runtime-resolved constructs. The orchestration problem is formalized as a constrained optimization over layer-wise assignments, subject to evolving latency, utilization, and privacy gradients. The framework implements reactive inference composition responsive to infrastructural fluctuations by integrating model-aware capacity profiling with dynamic graph re-partitioning and reallocation. We introduce architectural and algorithmic components, along with a representative use case in 6G multi-access edge computing."}
{"id": "2512.00113", "categories": ["cs.AR", "cs.AI", "eess.SP"], "pdf": "https://arxiv.org/pdf/2512.00113", "abs": "https://arxiv.org/abs/2512.00113", "authors": ["Amirreza Yousefzadeh"], "title": "From RISC-V Cores to Neuromorphic Arrays: A Tutorial on Building Scalable Digital Neuromorphic Processors", "comment": null, "summary": "Digital neuromorphic processors are emerging as a promising computing substrate for low-power, always-on EdgeAI applications. In this tutorial paper, we outline the main architectural design principles behind fully digital neuromorphic processors and illustrate them using the SENECA platform as a running example. Starting from a flexible array of tiny RISC-V processing cores connected by a simple Network-on-Chip (NoC), we show how to progressively evolve the architecture: from a baseline event-driven implementation of fully connected networks, to versions with dedicated Neural Processing Elements (NPEs) and a loop controller that offloads fine-grained control from the general-purpose cores. Along the way, we discuss software and mapping techniques such as spike grouping, event-driven depth-first convolution for convolutional networks, and hard-attention style processing for high-resolution event-based vision. The focus is on architectural trade-offs, performance and energy bottlenecks, and on leveraging flexibility to incrementally add domain-specific acceleration. This paper assumes familiarity with basic neuromorphic concepts (spikes, event-driven computation, sparse activation) and deep neural network workloads. It does not present new experimental results; instead, it synthesizes and contextualizes findings previously reported in our SENECA publications to provide a coherent, step-by-step architectural perspective for students and practitioners who wish to design their own digital neuromorphic processors."}
{"id": "2512.01356", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2512.01356", "abs": "https://arxiv.org/abs/2512.01356", "authors": ["Yuxin Zhang", "Yuxia Zhang", "Zeyu Sun", "Yanjie Jiang", "Hui Liu"], "title": "LAURA: Enhancing Code Review Generation with Context-Enriched Retrieval-Augmented LLM", "comment": "Accepted by the 2025 40th IEEE/ACM International Conference on Automated Software Engineering (ASE). Copyright 2025 IEEE. This is the author's accepted manuscript. The final published version may differ and will be available from IEEE Xplore", "summary": "Code review is critical for ensuring software quality and maintainability. With the rapid growth in software scale and complexity, code review has become a bottleneck in the development process because of its time-consuming and knowledge-intensive nature and the shortage of experienced developers willing to review code. Several approaches have been proposed for automatically generating code reviews based on retrieval, neural machine translation, pre-trained models, or large language models (LLMs). These approaches mainly leverage historical code changes and review comments. However, a large amount of crucial information for code review, such as the context of code changes and prior review knowledge, has been overlooked. This paper proposes an LLM-based review knowledge-augmented, context-aware framework for code review generation, named LAURA. The framework integrates review exemplar retrieval, context augmentation, and systematic guidance to enhance the performance of ChatGPT-4o and DeepSeek v3 in generating code review comments. Besides, given the extensive low-quality reviews in existing datasets, we also constructed a high-quality dataset. Experimental results show that for both models, LAURA generates review comments that are either completely correct or at least helpful to developers in 42.2% and 40.4% of cases, respectively, significantly outperforming SOTA baselines. Furthermore, our ablation studies demonstrate that all components of LAURA contribute positively to improving comment quality."}
{"id": "2512.00138", "categories": ["cs.AR", "cs.CV", "eess.IV"], "pdf": "https://arxiv.org/pdf/2512.00138", "abs": "https://arxiv.org/abs/2512.00138", "authors": ["Yuyang Li", "Swasthik Muloor", "Jack Laudati", "Nickolas Dematteis", "Yidam Park", "Hana Kim", "Nathan Chang", "Inhee Lee"], "title": "Ternary-Input Binary-Weight CNN Accelerator Design for Miniature Object Classification System with Query-Driven Spatial DVS", "comment": "6 pages.12 figures & 2 table", "summary": "Miniature imaging systems are essential for space-constrained applications but are limited by memory and power constraints. While machine learning can reduce data size by extracting key features, its high energy demands often exceed the capacity of small batteries. This paper presents a CNN hardware accelerator optimized for object classification in miniature imaging systems. It processes data from a spatial Dynamic Vision Sensor (DVS), reconfigurable to a temporal DVS via pixel sharing, minimizing sensor area. By using ternary DVS outputs and a ternary-input, binary-weight neural network, the design reduces computation and memory needs. Fabricated in 28 nm CMOS, the accelerator cuts data size by 81% and MAC operations by 27%. It achieves 440 ms inference time at just 1.6 mW power consumption, improving the Figure-of-Merit (FoM) by 7.3x over prior CNN accelerators for miniature systems."}
{"id": "2512.01396", "categories": ["cs.SE", "cs.CL", "cs.CR"], "pdf": "https://arxiv.org/pdf/2512.01396", "abs": "https://arxiv.org/abs/2512.01396", "authors": ["Zhiqing Zhong", "Jiaming Huang", "Pinjia He"], "title": "BackportBench: A Multilingual Benchmark for Automated Backporting of Patches", "comment": "Under review", "summary": "Many modern software projects evolve rapidly to incorporate new features and security patches. It is important for users to update their dependencies to safer versions, but many still use older, vulnerable package versions because upgrading can be difficult and may break their existing codebase. Software developers can mitigate this problem by backporting security patches to older releases. However, manually backporting is time-consuming and error-prone. The effectiveness of existing automated backporting techniques on general software remains unclear since they typically target only code-hunk or function-level patch porting scenarios and are evaluated with imperfect metrics.\n  To facilitate the development and evaluation of automated backporting techniques, we introduce BackportBench, the first comprehensive benchmark suite for patch backporting problem. BackportBench is a multilingual benchmark that contains 202 patch backporting problems from PyPI, Maven, and npm, each with executable Docker environments and relevant test cases. We evaluated existing patch porting methods and LLM-based techniques that have the potential to adapt to this task using BackportBench. The results show that the agentic method has outperformed traditional patch porting methods, especially on cases that require logical and structural changes. However, the performance varies across different programming languages. Based on the findings, we draw several implications for researchers and software practitioners in future work on automated backporting."}
{"id": "2512.00186", "categories": ["cs.AR", "eess.SP"], "pdf": "https://arxiv.org/pdf/2512.00186", "abs": "https://arxiv.org/abs/2512.00186", "authors": ["Seyed Hadi Mirfarshbafan", "Nicolas Filliol", "Oscar Castaeda", "Christoph Studer"], "title": "Variable Point: A Number Format for Area- and Energy-Efficient Multiplication of High-Dynamic-Range Numbers", "comment": "Presented at the 59th Asilomar Conference on Signals, Systems, and Computers", "summary": "Fixed-point number representation is commonly employed in digital VLSI designs that have stringent hardware efficiency constraints. However, fixed-point numbers cover a relatively small dynamic range for a given bitwidth. In contrast, floating-point numbers offer a larger dynamic range at the cost of increased hardware complexity. In this paper, we propose a novel number format called variable-point (VP). VP numbers cover a larger dynamic range than fixed-point numbers with similar bitwidth, without notably increasing hardware complexity -- this allows for a more efficient representation of signals with high dynamic range. To demonstrate the efficacy of the proposed VP number format, we consider a matrix-vector multiplication engine for spatial equalization in multi-antenna wireless communication systems involving high-dynamic-range signals. Through post-layout VLSI implementation results, we demonstrate that the proposed VP-based design achieves 20% and 10% area and power savings, respectively, compared to a fully optimized fixed-point design, without incurring any noticeable performance degradation."}
{"id": "2512.01523", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.01523", "abs": "https://arxiv.org/abs/2512.01523", "authors": ["Pankaj Jalote", "Y. Raghu Reddy", "Vasudeva Varma"], "title": "Teaching an Online Multi-Institutional Research Level Software Engineering Course with Industry - an Experience Report", "comment": "7 pages", "summary": "Covid has made online teaching and learning acceptable and students, faculty, and industry professionals are all comfortable with this mode. This comfort can be leveraged to offer an online multi-institutional research-level course in an area where individual institutions may not have the requisite faculty to teach and/or research students to enroll. If the subject is of interest to industry, online offering also allows industry experts to contribute and participate with ease. Advanced topics in Software Engineering are ideally suited for experimenting with this approach as industry, which is often looking to incorporate advances in software engineering in their practices, is likely to agree to contribute and participate. In this paper we describe an experiment in teaching a course titled \"AI in Software Engineering\" jointly between two institutions with active industry participation, and share our and student's experience. We believe this collaborative teaching approach can be used for offering research level courses in any applied area of computer science by institutions who are small and find it difficult to offer research level courses on their own."}
{"id": "2512.00335", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2512.00335", "abs": "https://arxiv.org/abs/2512.00335", "authors": ["Takuto Ando", "Yu Eto", "Ayumu Takeuchi", "Yasuhiko Nakashima"], "title": "Efficient Kernel Mapping and Comprehensive System Evaluation of LLM Acceleration on a CGLA", "comment": "This paper is published at IEEE Access", "summary": "Large Language Models (LLMs) demand substantial computational resources, resulting in high energy consumption on GPUs. To address this challenge, we focus on Coarse-Grained Reconfigurable Arrays (CGRAs) as an effective alternative that provides a trade-off between energy efficiency and programmability. This paper presents the first comprehensive, end-to-end evaluation of a non-AI-specialized Coarse-Grained Linear Array (CGLA) accelerator for the state-of-the-art Qwen LLM family. The architecture has a general-purpose, task-agnostic design, yet its flexible instruction set allows for domain-specific adaptations. This flexibility enables the architecture to achieve high efficiency for sustainable LLM inference. We assess the performance of our architecture on an FPGA prototype using the widely adopted llama.cpp framework. We then project its potential as a 28nm ASIC and compare it against a high-performance GPU (NVIDIA RTX 4090) and an edge AI device (NVIDIA Jetson AGX Orin). While GPUs exhibit lower latency, our non-AI-specific accelerator achieves higher energy efficiency, improving the Power-Delay Product (PDP) by up to 44.4x and 13.6x compared with the RTX 4090 and Jetson, respectively. Similarly, it reduces the Energy-Delay Product (EDP) by up to 11.5x compared to the high-performance GPU, demonstrating a favorable performance-energy trade-off. Critically, our system-level analysis identifies host-accelerator data transfer as the primary performance bottleneck, a factor often overlooked in kernel-level studies. These findings provide design guidance for next-generation LLM accelerators. This work validates CGRAs as a suitable platform for LLM inference in power-constrained environments, without being confined to specific algorithms."}
{"id": "2512.01570", "categories": ["cs.SE", "cs.DL"], "pdf": "https://arxiv.org/pdf/2512.01570", "abs": "https://arxiv.org/abs/2512.01570", "authors": ["Stephan Druskat", "Lars Grunske"], "title": "OpenDORS: A dataset of openly referenced open research software", "comment": "5 pages, 3 figures, 1 table", "summary": "In many academic disciplines, software is created during the research process or for a research purpose. The crucial role of software for research is increasingly acknowledged. The application of software engineering to research software has been formalized as research software engineering, to create better software that enables better research. Despite this, large-scale studies of research software and its development are still lacking. To enable such studies, we present a dataset of 134,352 unique open research software projects and 134,154 source code repositories referenced in open access literature. Each dataset record identifies the referencing publication and lists source code repositories of the software project. For 122,425 source code repositories, the dataset provides metadata on latest versions, license information, programming languages and descriptive metadata files. We summarize the distributions of these features in the dataset and describe additional software metadata that extends the dataset in future work. Finally, we suggest examples of research that could use the dataset to develop a better understanding of research software practice in RSE research."}
{"id": "2512.00441", "categories": ["cs.AR", "cs.ET"], "pdf": "https://arxiv.org/pdf/2512.00441", "abs": "https://arxiv.org/abs/2512.00441", "authors": ["Amogh K M", "Sunita M S"], "title": "A Novel 8T SRAM-Based In-Memory Computing Architecture for MAC-Derived Logical Functions", "comment": "6 pages, 6 figures, Accepted at 39th VLSID 2026 conference", "summary": "This paper presents an in-memory computing (IMC) architecture developed on an 8x8 array of 8T SRAM cells. This architecture enables both multi-bit parallel Multiply-Accumulate (MAC) operations and standard memory processing through charge-sharing on dedicated read bit-lines. By leveraging the maturity of SRAM technology, this work introduces an 8T SRAM-based IMC architecture that decouples read and write paths, thereby overcoming the reliability limitations of prior 6T SRAM designs. A novel analog-to-digital decoding scheme converts the MAC voltage output into digital counts, which are subsequently interpreted to realize fundamental logic functions including AND/NAND, NOR/OR, XOR/XNOR, and 1-bit addition within the same array. Simulated in a 90 nm CMOS process at 1.8 V supply voltage, the proposed design achieves 8-bit MAC and logical operations at a frequency of 142.85 MHz, with a latency of 0.7 ns and energy consumption of 56.56 fJ/bit per MAC operation and throughput of 15.8 M operations/s."}
{"id": "2512.01609", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2512.01609", "abs": "https://arxiv.org/abs/2512.01609", "authors": ["Patrick Herter", "Vincent Ahlrichs", "Ridvan Ailan", "Julian Horsch"], "title": "GPTrace: Effective Crash Deduplication Using LLM Embeddings", "comment": "Original publication in 2026 IEEE/ACM 48th International Conference on Software Engineering (ICSE '26), April 12-18, 2026, Rio de Janeiro, Brazil. ACM, New York, NY, USA, 12 pages", "summary": "Fuzzing is a highly effective method for uncovering software vulnerabilities, but analyzing the resulting data typically requires substantial manual effort. This is amplified by the fact that fuzzing campaigns often find a large number of crashing inputs, many of which share the same underlying bug. Crash deduplication is the task of finding such duplicate crashing inputs and thereby reducing the data that needs to be examined. Many existing deduplication approaches rely on comparing stack traces or other information that is collected when a program crashes. Although various metrics for measuring the similarity of such pieces of information have been proposed, many do not yield satisfactory deduplication results. In this work, we present GPTrace, a deduplication workflow that leverages a large language model to evaluate the similarity of various data sources associated with crashes by computing embedding vectors and supplying those as input to a clustering algorithm. We evaluate our approach on over 300 000 crashing inputs belonging to 50 ground truth labels from 14 different targets. The deduplication results produced by GPTrace show a noticeable improvement over hand-crafted stack trace comparison methods and even more complex state-of-the-art approaches that are less flexible."}
{"id": "2512.00487", "categories": ["cs.AR", "cs.PL"], "pdf": "https://arxiv.org/pdf/2512.00487", "abs": "https://arxiv.org/abs/2512.00487", "authors": ["Yuhao Gu", "Zhongchun Zheng", "Nong Xiao", "Yutong Lu", "Xianwei Zhang"], "title": "Partial Cross-Compilation and Mixed Execution for Accelerating Dynamic Binary Translation", "comment": null, "summary": "With the growing diversity of instruction set architectures (ISAs), cross-ISA program execution has become common. Dynamic binary translation (DBT) is the main solution but suffers from poor performance. Cross-compilation avoids emulation costs but is constrained by an \"all-or-nothing\" model-programs are either fully cross-compiled or entirely emulated. Complete cross-compilation is often unfeasible due to ISA-specific code or missing dependencies, leaving programs with high emulation overhead.\n  We propose a hybrid execution system that combines compilation and emulation, featuring a selective function offloading mechanism. This mechanism establishes cross-environment calling channels, offloading eligible functions to the host for native execution to reduce DBT overhead. Key optimizations address offloading costs, enabling efficient hybrid operation. Built on LLVM and QEMU, the system works automatically for both applications and libraries. Evaluations show it achieves up to 13x speedups over existing DBT, with strong practical value."}
{"id": "2512.01617", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2512.01617", "abs": "https://arxiv.org/abs/2512.01617", "authors": ["Pierciro Caliandro", "Matteo Ciccaglione", "Alessandro Pellegrini"], "title": "When High-Performance Computing Meets Software Testing: Distributed Fuzzing using MPI", "comment": null, "summary": "This paper explores the integration of MPI-based synchronization techniques into distributed fuzzing frameworks, highlighting possible substantial performance improvements compared to traditional filesystem-based synchronization methods. By employing lightweight MPI primitives, reductions in communication latency are achieved, facilitating more efficient data exchanges across distributed fuzzing nodes. Experimental results obtained over standard benchmarks demonstrate enhanced coverage progression from the early stages of the fuzzing process, which could be beneficial if fuzzing is employed in CI/CD pipelines at any stage of software development. Furthermore, the coordinated exchange of input corpora among clusters of fuzzers effectively addresses coverage stagnation, enabling a sustained exploration of complex and deep execution paths. Overall, the adoption of MPI-based synchronization approaches shows promising potential for significantly enhancing the scalability and efficacy of distributed fuzz testing."}
{"id": "2512.00974", "categories": ["cs.AR", "cs.ET"], "pdf": "https://arxiv.org/pdf/2512.00974", "abs": "https://arxiv.org/abs/2512.00974", "authors": ["Aradhya Chakrabarti"], "title": "A WASM-Subset Stack Architecture for Low-cost FPGAs using Open-Source EDA Flows", "comment": "6 pages, 5 figures. Source code available at https://github.com/TimeATronics/wasm_cpu", "summary": "Soft-core processors on resource-constrained FPGAs often suffer from low code density and reliance on proprietary toolchains. This paper details the design, implementation, and evaluation of a 32-bit dual-stack microprocessor architecture optimized for low-cost, resource-constrained Field-Programmable Gate Arrays (FPGAs). Implemented on the Gowin GW1NR-9 (Tang Nano 9K), the processor utilizes an instruction set architecture (ISA) inspired from a subset of the WebAssembly (WASM) specification to achieve high code density. Unlike traditional soft-cores that often rely on proprietary vendor toolchains and opaque IP blocks, this design is synthesized and routed utilizing an open-source flow, providing transparency and portability. The architecture features a dual-stack model (Data and Return), executing directly from SPI Flash via an Execute-in-Place (XIP) mechanism to conserve scarce Block RAM on the intended target device. An analysis of the trade-offs involved in stack depth parametrization is presented, demonstrating that an 8-entry distributed RAM implementation provides a balance between logic resource utilization ($\\sim 80\\%$) and routing congestion. Furthermore, timing hazards in single-cycle stack operations are identified and resolved through a refined Finite State Machine (FSM) design. The system achieves a stable operating frequency of 27 MHz, limited by Flash latency, and successfully executes simple applications including a single and multi-digit infix calculator."}
{"id": "2512.01630", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2512.01630", "abs": "https://arxiv.org/abs/2512.01630", "authors": ["Ziheng Liu", "Runzhi He", "Minghui Zhou"], "title": "Package Dashboard: A Cross-Ecosystem Framework for Dual-Perspective Analysis of Software Packages", "comment": null, "summary": "Software supply chain attacks have revealed blind spots in existing SCA tools, which are often limited to a single ecosystem and assess either software artifacts or community activity in isolation. This fragmentation across tools and ecosystems forces developers to manually reconcile scattered data, undermining risk assessments. We present Package Dashboard, a cross-ecosystem framework that provides a unified platform for supply chain analysis, enabling a holistic, dual-perspective risk assessment by integrating package metadata, vulnerability information, and upstream community health metrics. By combining dependency resolution with repository analysis, it reduces cognitive load and improves traceability. Demonstrating the framework's versatility, a large-scale study of 374,000 packages across five Linux distributions shows its ability to uncover not only conventional vulnerabilities and license conflicts but also overlooked risks such as archived or inaccessible repositories. Ultimately, Package Dashboard provides a unified view of risk, equipping developers and DevSecOps engineers with actionable insights to strengthen the transparency, trustworthiness, and traceability of open-source ecosystems. Package Dashboard is publicly available at https://github.com/n19htfall/PackageDashboard, and a demonstration video can be found at https://youtu.be/y9ncftP8KPQ. Besides, the online version is available at https://pkgdash.osslab-pku.org."}
{"id": "2512.01193", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2512.01193", "abs": "https://arxiv.org/abs/2512.01193", "authors": ["Masoud Rahimi", "Sbastien Le Beux"], "title": "Leveraging Recurrent Patterns in Graph Accelerators", "comment": "Accepted at DATE 2026", "summary": "Graph accelerators have emerged as a promising solution for processing large-scale sparse graphs, leveraging the in-situ compu-tation of ReRAM-based crossbars to maximize computational efficiency. However, existing designs suffer from memristor access overhead due to the large number of graph partitions. This leads to increased execution time, higher energy consumption, and re-duced circuit lifetime. This paper proposes a graph processing method that minimizes memristor write operations by identifying frequent subgraph patterns and assigning them to graph engines, referred to as static, allowing most subgraphs to be processed without a need for crossbar reconfiguration. Experimental results show speed up to 2.38x speedup and 7.23x energy savings com-pared to state-of-the-art accelerators. Furthermore, our method extends the circuit lifetime by 2x compared to state-of-the-art ReRAM graph accelerators."}
{"id": "2512.01649", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2512.01649", "abs": "https://arxiv.org/abs/2512.01649", "authors": ["Daniel Strassler", "Gabe Elkin", "Curran Schiefelbein", "Daniel Herring", "Ian Jessen", "David Johnson", "Santiago A. Paredes", "Tod Shannon", "Jim Flavin"], "title": "MIT Lincoln Laboratory: A Case Study on Improving Software Support for Research Projects", "comment": "Strassler, D., et al. MIT Lincoln Laboratory: A Case Study on Improving Software Support for Research Projects. Zenodo, 2025, https://doi.org/10.5281/zenodo.16878161", "summary": "Software plays an ever increasing role in complex system development and prototyping, and in recent years, MIT Lincoln Laboratory has sought to improve both the effectiveness and culture surrounding software engineering in execution of its mission. The Homeland Protection and Air Traffic Control Division conducted an internal study to examine challenges to effective and efficient research software development, and to identify ways to strengthen both the culture and execution for greater impact on our mission. Key findings of this study fell into three main categories: project attributes that influence how software development activities must be conducted and managed, potential efficiencies from centralization, opportunities to improve staffing and culture with respect to software practitioners. The study delivered actionable recommendations, including centralizing and standardizing software support tooling, developing a common database to help match the right software talent and needs to projects, and creating a software stakeholder panel to assist with continued improvement."}
{"id": "2512.01463", "categories": ["cs.AR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.01463", "abs": "https://arxiv.org/abs/2512.01463", "authors": ["Jan-Frederik Schulte", "Benjamin Ramhorst", "Chang Sun", "Jovan Mitrevski", "Nicol Ghielmetti", "Enrico Lupi", "Dimitrios Danopoulos", "Vladimir Loncar", "Javier Duarte", "David Burnette", "Lauri Laatu", "Stylianos Tzelepis", "Konstantinos Axiotis", "Quentin Berthet", "Haoyan Wang", "Paul White", "Suleyman Demirsoy", "Marco Colombo", "Thea Aarrestad", "Sioni Summers", "Maurizio Pierini", "Giuseppe Di Guglielmo", "Jennifer Ngadiuba", "Javier Campos", "Ben Hawks", "Abhijith Gandrakota", "Farah Fahim", "Nhan Tran", "George Constantinides", "Zhiqiang Que", "Wayne Luk", "Alexander Tapper", "Duc Hoang", "Noah Paladino", "Philip Harris", "Bo-Cheng Lai", "Manuel Valentin", "Ryan Forelli", "Seda Ogrenci", "Lino Gerlach", "Rian Flynn", "Mia Liu", "Daniel Diaz", "Elham Khoda", "Melissa Quinnan", "Russell Solares", "Santosh Parajuli", "Mark Neubauer", "Christian Herwig", "Ho Fung Tsoi", "Dylan Rankin", "Shih-Chieh Hsu", "Scott Hauck"], "title": "hls4ml: A Flexible, Open-Source Platform for Deep Learning Acceleration on Reconfigurable Hardware", "comment": null, "summary": "We present hls4ml, a free and open-source platform that translates machine learning (ML) models from modern deep learning frameworks into high-level synthesis (HLS) code that can be integrated into full designs for field-programmable gate arrays (FPGAs) or application-specific integrated circuits (ASICs). With its flexible and modular design, hls4ml supports a large number of deep learning frameworks and can target HLS compilers from several vendors, including Vitis HLS, Intel oneAPI and Catapult HLS. Together with a wider eco-system for software-hardware co-design, hls4ml has enabled the acceleration of ML inference in a wide range of commercial and scientific applications where low latency, resource usage, and power consumption are critical. In this paper, we describe the structure and functionality of the hls4ml platform. The overarching design considerations for the generated HLS code are discussed, together with selected performance results."}
{"id": "2512.01690", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2512.01690", "abs": "https://arxiv.org/abs/2512.01690", "authors": ["Philip Garrett", "Juan P. Galeotti", "Andrea Arcuri", "Alexander Poth", "Olsi Rrjolli"], "title": "Generating REST API Tests With Descriptive Names", "comment": null, "summary": "Automated test generation has become a key technique for ensuring software quality, particularly in modern API-based architectures. However, automatically generated test cases are typically assigned non-descriptive names (e.g., test0, test1), which reduces their readability and hinders their usefulness during comprehension and maintenance. In this work, we present three novel deterministic techniques to generate REST API test names. We then compare eight techniques in total for generating descriptive names for REST API tests automatically produced by the fuzzer EvoMaster, using 10 test cases generated for 9 different open-source APIs. The eight techniques include rule-based heuristics and large language model (LLM)-based approaches. Their effectiveness was empirically evaluated through two surveys (involving up to 39 people recruited via LinkedIn). Our results show that a rule-based approach achieves the highest clarity ratings among deterministic methods, performs on par with state-of-the-art LLM-based models such as Gemini and GPT-4o, and significantly outperforms GPT-3.5.\n  To further evaluate the practical impact of our results, an industrial case study was carried out with practitioners who actively use EvoMaster at Volkswagen AG. A developer questionnaire was then carried out based on the use of EvoMaster on four different APIs by four different users, for a total of 74 evaluated test cases. Feedback from practitioners further confirms that descriptive names produced by this approach improve test suite readability.\n  These findings highlight that lightweight, deterministic techniques can serve as effective alternatives to computationally expensive and security-sensitive LLM-based approaches for automated system-level test naming, providing a practical step toward more developer-friendly API test generation."}
{"id": "2512.01541", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2512.01541", "abs": "https://arxiv.org/abs/2512.01541", "authors": ["Hwayong Nam", "Seungmin Baek", "Jumin Kim", "Michael Jaemin Kim", "Jung Ho Ahn"], "title": "RoMe: Row Granularity Access Memory System for Large Language Models", "comment": "15 pages, 14 figures, accepted at HPCA 2026", "summary": "Modern HBM-based memory systems have evolved over generations while retaining cache line granularity accesses. Preserving this fine granularity necessitated the introduction of bank groups and pseudo channels. These structures expand timing parameters and control overhead, significantly increasing memory controller scheduling complexity. Large language models (LLMs) now dominate deep learning workloads, streaming contiguous data blocks ranging from several kilobytes to megabytes per operation. In a conventional HBM-based memory system, these transfers are fragmented into hundreds of 32B cache line transactions. This forces the memory controller to employ unnecessarily intricate scheduling, leading to growing inefficiency.\n  To address this problem, we propose RoMe. RoMe accesses DRAM at row granularity and removes columns, bank groups, and pseudo channels from the memory interface. This design simplifies memory scheduling, thereby requiring fewer pins per channel. The freed pins are aggregated to form additional channels, increasing overall bandwidth by 12.5% with minimal extra pins. RoMe demonstrates how memory scheduling logic can be significantly simplified for representative LLM workloads, and presents an alternative approach for next-generation HBM-based memory systems achieving increased bandwidth with minimal hardware overhead."}
{"id": "2512.01939", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.01939", "abs": "https://arxiv.org/abs/2512.01939", "authors": ["Yanlin Wang", "Xinyi Xu", "Jiachi Chen", "Tingting Bi", "Wenchao Gu", "Zibin Zheng"], "title": "An Empirical Study of Agent Developer Practices in AI Agent Frameworks", "comment": null, "summary": "The rise of large language models (LLMs) has sparked a surge of interest in agents, leading to the rapid growth of agent frameworks. Agent frameworks are software toolkits and libraries that provide standardized components, abstractions, and orchestration mechanisms to simplify agent development. Despite widespread use of agent frameworks, their practical applications and how they influence the agent development process remain underexplored. Different agent frameworks encounter similar problems during use, indicating that these recurring issues deserve greater attention and call for further improvements in agent framework design. Meanwhile, as the number of agent frameworks continues to grow and evolve, more than 80% of developers report difficulties in identifying the frameworks that best meet their specific development requirements. In this paper, we conduct the first empirical study of LLM-based agent frameworks, exploring real-world experiences of developers in building AI agents. To compare how well the agent frameworks meet developer needs, we further collect developer discussions for the ten previously identified agent frameworks, resulting in a total of 11,910 discussions. Finally, by analyzing these discussions, we compare the frameworks across five dimensions: development efficiency, functional abstraction, learning cost, performance optimization, and maintainability, which refers to how easily developers can update and extend both the framework itself and the agents built upon it over time. Our comparative analysis reveals significant differences among frameworks in how they meet the needs of agent developers. Overall, we provide a set of findings and implications for the LLM-driven AI agent framework ecosystem and offer insights for the design of future LLM-based agent frameworks and agent developers."}
{"id": "2512.01644", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2512.01644", "abs": "https://arxiv.org/abs/2512.01644", "authors": ["Haonan Wang", "Xuxin Xiao", "Mingyu Yan", "Zhuoyuan Zhu", "Dengke Han", "Duo Wang", "Wenming Li", "Xiaochun Ye", "Cunchen Hu", "Hongyang Chen", "Guangyu Sun"], "title": "A Systematic Characterization of LLM Inference on GPUs", "comment": null, "summary": "This work presents a systematic characterization of Large Language Model (LLM) inference to address fragmented understanding. Through comprehensive experiments, we establish a four-dimensional analytical framework: (1) Two-Phase Heterogeneity Observation; (2) Microarchitectural Root Cause Analysis; (3) System Scaling Principles; and (4) Emerging Paradigm Boundaries. Our investigation progresses systematically from observation to foresight: identifying performance phenomena, revealing hardware causes, validating system behavior, and exploring new paradigms. This study not only consolidates a reliable empirical foundation for existing research but also provides new discoveries and practical optimization guidance for LLM inference."}
{"id": "2512.01010", "categories": ["cs.MA", "cs.AI", "cs.LG", "cs.SE", "physics.comp-ph", "physics.flu-dyn"], "pdf": "https://arxiv.org/pdf/2512.01010", "abs": "https://arxiv.org/abs/2512.01010", "authors": ["Vansh Sharma", "Venkat Raman"], "title": "Chain of Unit-Physics: A Primitive-Centric Approach to Scientific Code Synthesis", "comment": null, "summary": "Agentic large language models are proposed as autonomous code generators for scientific computing, yet their reliability in high-stakes problems remains unclear. Developing computational scientific software from natural-language queries remains challenging broadly due to (a) sparse representation of domain codes during training and (b) the limited feasibility of RLHF with a small expert community. To address these limitations, this work conceptualizes an inverse approach to code design, embodied in the Chain of Unit-Physics framework: a first-principles (or primitives)-centric, multi-agent system in which human expert knowledge is encoded as unit-physics tests that explicitly constrain code generation. The framework is evaluated on a nontrivial combustion task, used here as a representative benchmark for scientific problem with realistic physical constraints. Closed-weight systems and code-focused agentic variants fail to produce correct end-to-end solvers, despite tool and web access, exhibiting four recurrent error classes: interface (syntax/API) hallucinations, overconfident assumptions, numerical/physical incoherence, and configuration fragility. Open-weight models with chain-of-thought (CoT) decoding reduce interface errors but still yield incorrect solutions. On the benchmark task, the proposed framework converges within 5-6 iterations, matches the human-expert implementation (mean error of $3.1\\times10^{-3}$ %), with a $\\sim$33.4 % faster runtime and a $\\sim$30 % efficient memory usage at a cost comparable to mid-sized commercial APIs, yielding a practical template for physics-grounded scientific code generation. As datasets and models evolve, zero-shot code accuracy will improve; however, the Chain of Unit-Physics framework goes further by embedding first-principles analysis that is foundational to scientific codes."}
{"id": "2512.01357", "categories": ["cs.DC", "cs.AI", "cs.AR"], "pdf": "https://arxiv.org/pdf/2512.01357", "abs": "https://arxiv.org/abs/2512.01357", "authors": ["Wenbin Zhu", "Zhaoyan Shen", "Zili Shao", "Hongjun Dai", "Feng Chen"], "title": "Tangram: Accelerating Serverless LLM Loading through GPU Memory Reuse and Affinity", "comment": null, "summary": "Serverless Large Language Models (LLMs) have emerged as a cost-effective solution for deploying AI services by enabling a 'pay-as-you-go' pricing model through GPU resource sharing. However, cold-start latency, especially the model loading phase, has become a critical performance bottleneck, as it scales linearly with model size and severely limits the practical deployment of large-scale LLM services. This paper presents Tangram, a novel system that accelerates Serverless LLM loading through efficient GPU memory reuse. By leveraging the unused GPU memory to retain model parameters, Tangram significantly reduces model transfer time and cold-start latency. Its design includes three key components: unified GPU memory pool for tensor-level parameter sharing across models, on-demand KV cache allocation for dynamic memory management, and GPU-affinity-aware scheduling for maximizing resource utilization. These techniques collectively address the critical challenges of inefficient memory usage and the cold-start problem in Serverless LLM platforms. We have implemented a fully functional prototype, and experiments show that Tangram achieves up to 6.2 times faster loading and reduces Time-To-First-Token (TTFT) during cold-start by 23--55% over state-of-the-art methods."}
