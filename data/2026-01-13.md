<div id=toc></div>

# Table of Contents

- [cs.MA](#cs.MA) [Total: 6]
- [cs.OS](#cs.OS) [Total: 1]
- [cs.AR](#cs.AR) [Total: 1]
- [cs.DC](#cs.DC) [Total: 6]
- [cs.SE](#cs.SE) [Total: 17]


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [1] [Dynamic Incentivized Cooperation under Changing Rewards](https://arxiv.org/abs/2601.06382)
*Philipp Altmann,Thomy Phan,Maximilian Zorn,Claudia Linnhoff-Popien,Sven Koenig*

Main category: cs.MA

TL;DR: DRIVE是一种自适应的多智能体强化学习方法，通过动态奖励激励实现社会困境中的合作。


<details>
  <summary>Details</summary>
Motivation: 现有PI方法依赖固定激励值，难以应对环境奖励变化，导致合作失效。

Method: DRIVE通过智能体间交换奖励差异，在完全去中心化方式下激励互惠合作。

Result: 在一般囚徒困境和复杂序列社会困境中，DRIVE能有效实现并维持合作，优于现有PI方法。

Conclusion: DRIVE为动态奖励环境下的多智能体合作提供了鲁棒且可扩展的解决方案。

Abstract: Peer incentivization (PI) is a popular multi-agent reinforcement learning approach where all agents can reward or penalize each other to achieve cooperation in social dilemmas. Despite their potential for scalable cooperation, current PI methods heavily depend on fixed incentive values that need to be appropriately chosen with respect to the environmental rewards and thus are highly sensitive to their changes. Therefore, they fail to maintain cooperation under changing rewards in the environment, e.g., caused by modified specifications, varying supply and demand, or sensory flaws - even when the conditions for mutual cooperation remain the same. In this paper, we propose Dynamic Reward Incentives for Variable Exchange (DRIVE), an adaptive PI approach to cooperation in social dilemmas with changing rewards. DRIVE agents reciprocally exchange reward differences to incentivize mutual cooperation in a completely decentralized way. We show how DRIVE achieves mutual cooperation in the general Prisoner's Dilemma and empirically evaluate DRIVE in more complex sequential social dilemmas with changing rewards, demonstrating its ability to achieve and maintain cooperation, in contrast to current state-of-the-art PI methods.

</details>


### [2] [Bi-Mem: Bidirectional Construction of Hierarchical Memory for Personalized LLMs via Inductive-Reflective Agents](https://arxiv.org/abs/2601.06490)
*Wenyu Mao,Haosong Tan,Shuchang Liu,Haoyang Liu,Yifan Xu,Huaxiang Ji,Xiang Wang*

Main category: cs.MA

TL;DR: Bi-Mem是一种通过双向构建确保分层记忆保真度的智能体框架，显著提升长期个性化对话任务中的问答性能。


<details>
  <summary>Details</summary>
Motivation: 解决对话噪声和记忆幻觉在聚类过程中被放大，导致局部聚合记忆与用户全局人设不一致的问题。

Method: 部署归纳智能体构建分层记忆（事实级、场景级、人设级），同时设计反思智能体利用人设级约束校准场景级记忆；并提出关联检索机制实现连贯记忆召回。

Result: 实证评估表明，Bi-Mem在长期个性化对话任务的问答性能上取得显著提升。

Conclusion: Bi-Mem有效缓解了分层记忆中的噪声与幻觉问题，实现了全局与局部记忆的一致性，提升了个性化交互质量。

Abstract: Constructing memory from users' long-term conversations overcomes LLMs' contextual limitations and enables personalized interactions. Recent studies focus on hierarchical memory to model users' multi-granular behavioral patterns via clustering and aggregating historical conversations. However, conversational noise and memory hallucinations can be amplified during clustering, causing locally aggregated memories to misalign with the user's global persona. To mitigate this issue, we propose Bi-Mem, an agentic framework ensuring hierarchical memory fidelity through bidirectional construction. Specifically, we deploy an inductive agent to form the hierarchical memory: it extracts factual information from raw conversations to form fact-level memory, aggregates them into thematic scenes (i.e., local scene-level memory) using graph clustering, and infers users' profiles as global persona-level memory. Simultaneously, a reflective agent is designed to calibrate local scene-level memories using global constraints derived from the persona-level memory, thereby enforcing global-local alignment. For coherent memory recall, we propose an associative retrieval mechanism: beyond initial hierarchical search, a spreading activation process allows facts to evoke contextual scenes, while scene-level matches retrieve salient supporting factual information. Empirical evaluations demonstrate that Bi-Mem achieves significant improvements in question answering performance on long-term personalized conversational tasks.

</details>


### [3] [The Axiom of Consent: Friction Dynamics in Multi-Agent Coordination](https://arxiv.org/abs/2601.06692)
*Murad Farzulla*

Main category: cs.MA

TL;DR: 本文提出一个基于同意公理的多智能体协调摩擦形式化框架，通过核三元组（α, σ, ε）量化协调难度，并推导出摩擦方程 F = σ(1+ε)/(1+α)，揭示低摩擦配置在演化中具有稳定性。


<details>
  <summary>Details</summary>
Motivation: 解决多智能体系统中因偏好异质、权益不对称和信息不完全导致的协调摩擦问题，提供可量化分析与预测的统一理论框架。

Method: 从‘行动需按权益比例获得授权’这一同意公理出发，构建核三元组（对齐度α、权益σ、熵ε），推导摩擦方程，并引入复制优化机制（ROM）模拟策略演化过程。

Result: 验证了高奖励对齐度加速MARL收敛、考虑权益不对称降低协调失败率、可解释性缺陷与人机对齐差距成比例产生摩擦等预测；框架成功应用于加密治理与政治系统。

Conclusion: 协调摩擦是跨领域普遍存在的复杂性现象，基于同意的形式化框架不仅具预测力，亦揭示尊重权益的配置在动力学上具吸引子特性，为多智能体系统设计提供科学依据。

Abstract: Multi-agent systems face a fundamental coordination problem: agents must coordinate despite heterogeneous preferences, asymmetric stakes, and imperfect information. When coordination fails, friction emerges: measurable resistance manifesting as deadlock, thrashing, communication overhead, or outright conflict. This paper derives a formal framework for analyzing coordination friction from a single axiom: actions affecting agents require authorization from those agents in proportion to stakes.
  From this axiom of consent, we establish the kernel triple $(α, σ, ε)$ (alignment, stake, and entropy) characterizing any resource allocation configuration. The friction equation $F = σ (1 + ε)/(1 + α)$ predicts coordination difficulty as a function of preference alignment $α$, stake magnitude $σ$, and communication entropy $ε$. The Replicator-Optimization Mechanism (ROM) governs evolutionary selection over coordination strategies: configurations generating less friction persist longer, establishing consent-respecting arrangements as dynamical attractors rather than normative ideals.
  We develop formal definitions for resource consent, coordination legitimacy, and friction-aware allocation in multi-agent systems. The framework yields testable predictions: MARL systems with higher reward alignment exhibit faster convergence; distributed allocations accounting for stake asymmetry generate lower coordination failure; AI systems with interpretability deficits produce friction proportional to the human-AI alignment gap. Applications to cryptocurrency governance and political systems demonstrate that the same equations govern friction dynamics across domains, providing a complexity science perspective on coordination under preference heterogeneity.

</details>


### [4] [Logic-Driven Semantic Communication for Resilient Multi-Agent Systems](https://arxiv.org/abs/2601.06733)
*Tamara Alshammari,Mehdi Bennis*

Main category: cs.MA

TL;DR: 本文提出了一种基于认知与行动双维度的多智能体系统韧性形式化定义，并设计了相应的去中心化算法与验证机制，以支持6G环境下动态扰动中的持续适应与恢复能力。


<details>
  <summary>Details</summary>
Motivation: 现有去中心化多智能体系统韧性研究缺乏统一的形式化定义，难以支撑复杂动态环境下的持续感知、适应与恢复能力设计。

Method: 通过时态认知逻辑形式化定义韧性，提出认知韧性与行动韧性两个互补维度，并设计对应智能体架构与去中心化算法，结合可恢复时间与持续时间指标进行量化评估。

Result: 案例研究表明所提方法优于基线方法，形式化验证与仿真结果表明该框架能实现知识驱动的韧性决策与持续运行。

Conclusion: 本研究为下一代通信系统中构建韧性去中心化多智能体系统提供了理论基础与实用架构。

Abstract: The advent of 6G networks is accelerating autonomy and intelligence in large-scale, decentralized multi-agent systems (MAS). While this evolution enables adaptive behavior, it also heightens vulnerability to stressors such as environmental changes and adversarial behavior. Existing literature on resilience in decentralized MAS largely focuses on isolated aspects, such as fault tolerance, without offering a principled unified definition of multi-agent resilience. This gap limits the ability to design systems that can continuously sense, adapt, and recover under dynamic conditions. This article proposes a formal definition of MAS resilience grounded in two complementary dimensions: epistemic resilience, wherein agents recover and sustain accurate knowledge of the environment, and action resilience, wherein agents leverage that knowledge to coordinate and sustain goals under disruptions. We formalize resilience via temporal epistemic logic and quantify it using recoverability time (how quickly desired properties are re-established after a disturbance) and durability time (how long accurate beliefs and goal-directed behavior are sustained after recovery). We design an agent architecture and develop decentralized algorithms to achieve both epistemic and action resilience. We provide formal verification guarantees, showing that our specifications are sound with respect to the metric bounds and admit finite-horizon verification, enabling design-time certification and lightweight runtime monitoring. Through a case study on distributed multi-agent decision-making under stressors, we show that our approach outperforms baseline methods. Our formal verification analysis and simulation results highlight that the proposed framework enables resilient, knowledge-driven decision-making and sustained operation, laying the groundwork for resilient decentralized MAS in next-generation communication systems.

</details>


### [5] [DarwinTOD: LLM Driven Lifelong Self Evolution for Task Oriented Dialog Systems](https://arxiv.org/abs/2601.07248)
*Shuyu Zhang,Yujie Liu,Xinru Wang,Cheng Zhang,Yanmin Zhu,Bin Li*

Main category: cs.MA

TL;DR: DarwinTOD是一个结合进化计算与LLM自优化的终身自我演化对话框架，无需人工干预或任务微调即可持续提升性能。


<details>
  <summary>Details</summary>
Motivation: 传统任务型对话系统无法在部署后持续适应新领域或从交互中自主进化，现有持续学习方法依赖人工数据重训练，缺乏统一框架实现策略迭代优化。

Method: 提出双循环架构：在线多智能体对话执行与同伴互评，离线结构化进化操作更新可演化策略库，形成闭环自主优化机制。

Result: 实验表明DarwinTOD超越现有最优方法，并在演化过程中持续提升性能。

Conclusion: 该框架为构建具备终身自我演化能力的对话系统提供了新范式。

Abstract: Traditional task-oriented dialog systems are unable to evolve from ongoing interactions or adapt to new domains after deployment, that is a critical limitation in real-world dynamic environments. Continual learning approaches depend on episodic retraining with human curated data, failing to achieve autonomy lifelong improvement. While evolutionary computation and LLM driven self improvement offer promising mechanisms for dialog optimization, they lack a unified framework for holistic, iterative strategy refinement. To bridge this gap, we propose DarwinTOD, a lifelong self evolving dialog framework that systematically integrates these two paradigms, enabling continuous strategy optimization from a zero-shot base without task specific fine-tuning. DarwinTOD maintains an Evolvable Strategy Bank and operates through a dual-loop process: online multi-agent dialog execution with peer critique, and offline structured evolutionary operations that refine the strategy bank using accumulated feedback. This closed-loop design enables autonomous continuous improvement without human intervention. Extensive experiments show that DarwinTOD surpasses previous state-of-the-art methods and exhibits continuous performance gains throughout evolution. Our work provides a novel framework for building dialog systems with lifelong self evolution capabilities.

</details>


### [6] [Self-Creating Random Walks for Decentralized Learning under Pac-Man Attacks](https://arxiv.org/abs/2601.07674)
*Xingran Chen,Parimal Parag,Rohit Bhagat,Salim El Rouayheb*

Main category: cs.MA

TL;DR: 提出CREATE-IF-LATE算法以抵御Pac-Man攻击，确保随机游走在分布式学习中的持续性和收敛性。


<details>
  <summary>Details</summary>
Motivation: 随机游走算法易受恶意节点终止访问的隐蔽攻击，需设计抗攻击机制保障学习过程稳定。

Method: 设计完全去中心化的CREATE-IF-LATE算法，支持随机游走自我再生并理论证明其抗灭绝、有界性和收敛性。

Result: 理论与实验均验证CIL算法能有效抵御Pac-Man攻击，学习过程最多仅线性延迟，且在真实和合成数据集上表现稳健。

Conclusion: CIL算法为分布式随机游走学习提供了可靠的安全保障，具备实际部署价值。

Abstract: Random walk (RW)-based algorithms have long been popular in distributed systems due to low overheads and scalability, with recent growing applications in decentralized learning. However, their reliance on local interactions makes them inherently vulnerable to malicious behavior. In this work, we investigate an adversarial threat that we term the ``Pac-Man'' attack, in which a malicious node probabilistically terminates any RW that visits it. This stealthy behavior gradually eliminates active RWs from the network, effectively halting the learning process without triggering failure alarms. To counter this threat, we propose the CREATE-IF-LATE (CIL) algorithm, which is a fully decentralized, resilient mechanism that enables self-creating RWs and prevents RW extinction in the presence of Pac-Man. Our theoretical analysis shows that the CIL algorithm guarantees several desirable properties, such as (i) non-extinction of the RW population, (ii) almost sure boundedness of the RW population, and (iii) convergence of RW-based stochastic gradient descent even in the presence of Pac-Man with a quantifiable deviation from the true optimum. Moreover, the learning process experiences at most a linear time delay due to Pac-Man interruptions and RW regeneration. Our extensive empirical results on both synthetic and public benchmark datasets validate our theoretical findings.

</details>


<div id='cs.OS'></div>

# cs.OS [[Back]](#toc)

### [7] [Rethinking Inter-Process Communication with Memory Operation Offloading](https://arxiv.org/abs/2601.06331)
*Misun Park,Richi Dubey,Yifan Yuan,Nam Sung Kim,Ada Gavrilovska*

Main category: cs.OS

TL;DR: 本文提出统一的IPC运行时套件，通过软硬件协同内存卸载优化现代数据密集型系统的通信效率。


<details>
  <summary>Details</summary>
Motivation: 现有IPC运行时在处理多模态和AI驱动服务时耗费大量CPU周期于内存拷贝，缺乏统一模型协调软硬件卸载机制。

Method: 集成软硬件内存卸载技术，引入异步流水线、选择性缓存注入和混合协调机制，支持多种IPC模式平衡吞吐、延迟与CPU效率。

Result: 实测指令数减少最多22%，吞吐提升达2.1倍，延迟降低高达72%。

Conclusion: 协调式IPC卸载可显著提升现代数据密集系统端到端效率。

Abstract: As multimodal and AI-driven services exchange hundreds of megabytes per request, existing IPC runtimes spend a growing share of CPU cycles on memory copies. Although both hardware and software mechanisms are exploring memory offloading, current IPC stacks lack a unified runtime model to coordinate them effectively.
  This paper presents a unified IPC runtime suite that integrates both hardware- and software-based memory offloading into shared-memory communication. The system characterizes the interaction between offload strategies and IPC execution, including synchronization, cache visibility, and concurrency, and introduces multiple IPC modes that balance throughput, latency, and CPU efficiency.
  Through asynchronous pipelining, selective cache injection, and hybrid coordination, the system turns offloading from a device-specific feature into a general system capability. Evaluations on real-world workloads show instruction count reductions of up to 22%, throughput improvements of up to 2.1x, and latency reductions of up to 72%, demonstrating that coordinated IPC offloading can deliver tangible end-to-end efficiency gains in modern data-intensive systems.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [8] [DS-CIM: Digital Stochastic Computing-In-Memory Featuring Accurate OR-Accumulation via Sample Region Remapping for Edge AI Models](https://arxiv.org/abs/2601.06724)
*Kunming Shao,Liang Zhao,Jiangnan Yu,Zhipeng Liao,Xiaomeng Wang,Yi Zou,Tim Kwang-Ting Cheng,Chi-Ying Tsui*

Main category: cs.AR

TL;DR: 本文提出一种数字随机计算内存架构（DS-CIM），兼顾高精度与高能效，解决传统随机计算吞吐低与数字存内计算加法器开销大的问题。


<details>
  <summary>Details</summary>
Motivation: 克服随机计算吞吐率低与数字存内计算加法逻辑成本高的矛盾。

Method: 采用改进数据表示的无符号OR电路实现有符号乘累加，复制64倍电路提升吞吐，共享PRNG与2D分区避免冲突，并通过随机过程分析与数据重映射解决1饱和问题。

Result: DS-CIM1在CIFAR-10上实现94.45%准确率（RMSE 0.74%）；DS-CIM2达3566.1 TOPS/W能效与363.7 TOPS/mm²面积效率（RMSE 3.81%），并成功验证于ResNet50与LLaMA-7B模型。

Conclusion: DS-CIM架构有效平衡精度与效率，适用于大模型部署，具备高扩展性与实用性。

Abstract: Stochastic computing (SC) offers hardware simplicity but suffers from low throughput, while high-throughput Digital Computing-in-Memory (DCIM) is bottlenecked by costly adder logic for matrix-vector multiplication (MVM). To address this trade-off, this paper introduces a digital stochastic CIM (DS-CIM) architecture that achieves both high accuracy and efficiency. We implement signed multiply-accumulation (MAC) in a compact, unsigned OR-based circuit by modifying the data representation. Throughput is enhanced by replicating this low-cost circuit 64 times with only a 1x area increase. Our core strategy, a shared Pseudo Random Number Generator (PRNG) with 2D partitioning, enables single-cycle mutually exclusive activation to eliminate OR-gate collisions. We also resolve the 1s saturation issue via stochastic process analysis and data remapping, significantly improving accuracy and resilience to input sparsity. Our high-accuracy DS-CIM1 variant achieves 94.45% accuracy for INT8 ResNet18 on CIFAR-10 with a root-mean-squared error (RMSE) of just 0.74%. Meanwhile, our high-efficiency DS-CIM2 variant attains an energy efficiency of 3566.1 TOPS/W and an area efficiency of 363.7 TOPS/mm^2, while maintaining a low RMSE of 3.81%. The DS-CIM capability with larger models is further demonstrated through experiments with INT8 ResNet50 on ImageNet and the FP8 LLaMA-7B model.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [9] [HiDVFS: A Hierarchical Multi-Agent DVFS Scheduler for OpenMP DAG Workloads](https://arxiv.org/abs/2601.06425)
*Mohammad Pivezhandi,Abusayeed Saifullah,Ali Jannesari*

Main category: cs.DC

TL;DR: HiDVFS是一种基于多智能体的性能优先DVFS调度器，通过任务剖析、温度感知和资源竞争管理，在NVIDIA Jetson TX2上实现显著性能提升与能耗降低。


<details>
  <summary>Details</summary>
Motivation: 现有DVFS方法缺乏核心频率监控与执行模式分析，导致过热与能效低下，尤其在OpenMP DAG负载下难以平衡性能、能耗与温度。

Method: 提出三层智能体架构：第一层基于剖析数据选择核心与频率，第二层通过温度传感器管理核心组合，第三层在资源竞争时设定任务优先级，并采用以makespan为主、能量与温度为正则项的奖励函数。

Result: 在BOTS套件9个基准测试中，HiDVFS平均实现3.95倍加速与47.1%能耗降低；最优调优结果达4.16±0.58s（L10），比GearDVFS快3.44倍且节能50.4%。

Conclusion: HiDVFS有效解决了并行系统中因核心活动不均导致的过热问题，在保障性能的同时显著优化了能效，适用于嵌入式多核平台上的DAG型工作负载。

Abstract: With advancements in multicore embedded systems, leakage power, exponentially tied to chip temperature, has surpassed dynamic power consumption. Energy-aware solutions use dynamic voltage and frequency scaling (DVFS) to mitigate overheating in performance-intensive scenarios, while software approaches allocate high-utilization tasks across core configurations in parallel systems to reduce power. However, existing heuristics lack per-core frequency monitoring, failing to address overheating from uneven core activity, and task assignments without detailed profiling overlook irregular execution patterns. We target OpenMP DAG workloads. Because makespan, energy, and thermal goals often conflict within a single benchmark, this work prioritizes performance (makespan) while reporting energy and thermal as secondary outcomes. To overcome these issues, we propose HiDVFS (a hierarchical multi-agent, performance-aware DVFS scheduler) for parallel systems that optimizes task allocation based on profiling data, core temperatures, and makespan-first objectives. It employs three agents: one selects cores and frequencies using profiler data, another manages core combinations via temperature sensors, and a third sets task priorities during resource contention. A makespan-focused reward with energy and temperature regularizers estimates future states and enhances sample efficiency. Experiments on the NVIDIA Jetson TX2 using the BOTS suite (9 benchmarks) compare HiDVFS against state-of-the-art approaches. With multi-seed validation (seeds 42, 123, 456), HiDVFS achieves the best finetuned performance with 4.16 plus/minus 0.58s average makespan (L10), representing a 3.44x speedup over GearDVFS (14.32 plus/minus 2.61s) and 50.4% energy reduction (63.7 kJ vs 128.4 kJ). Across all BOTS benchmarks, HiDVFS achieves an average 3.95x speedup and 47.1% energy reduction.

</details>


### [10] [SkyNomad: On Using Multi-Region Spot Instances to Minimize AI Batch Job Cost](https://arxiv.org/abs/2601.06520)
*Zhifei Li,Tian Xia,Ziming Mao,Zihan Zhou,Ethan J. Jackson,Jamison Kerney,Zhanghao Wu,Pratik Mishra,Yi Xu,Yifan Qiao,Scott Shenker,Ion Stoica*

Main category: cs.DC

TL;DR: SkyNomad 是一种多区域调度系统，通过利用现货实例的时空异构性，在保证截止时间的前提下最大化使用低成本现货资源，从而显著降低 AI 批处理作业成本。


<details>
  <summary>Details</summary>
Motivation: 现有系统在使用云现货实例时难以兼顾成本与截止时间要求，且未充分利用跨区域的现货资源异构性。

Method: SkyNomad 采用轻量探测预估可用性、预测现货生命周期、考虑迁移成本，并将区域特征与截止压力统一建模为货币成本以指导调度决策。

Result: 实测显示 SkyNomad 在真实云环境中节省 1.25-3.96 倍成本，模拟中成本仅比最优策略高 10% 以内，且始终满足截止时间。

Conclusion: 通过智能调度跨区域现货资源，SkyNomad 能在保障时效的同时大幅降低成本，是高效经济的云资源调度方案。

Abstract: AI batch jobs such as model training, inference pipelines, and data analytics require substantial GPU resources and often need to finish before a deadline. Spot instances offer 3-10x lower cost than on-demand instances, but their unpredictable availability makes meeting deadlines difficult. Existing systems either rely solely on spot instances and risk deadline violations, or operate in simplified single-region settings. These approaches overlook substantial spatial and temporal heterogeneity in spot availability, lifetimes, and prices. We show that exploiting such heterogeneity to access more spot capacity is the key to reduce the job execution cost. We present SkyNomad, a multi-region scheduling system that maximizes spot usage and minimizes cost while guaranteeing deadlines. SkyNomad uses lightweight probing to estimate availability, predicts spot lifetimes, accounts for migration cost, and unifies regional characteristics and deadline pressure into a monetary cost model that guides scheduling decisions. Our evaluation shows that SkyNomad achieves 1.25-3.96x cost savings in real cloud deployments and performs within 10% cost differences of an optimal policy in simulation, while consistently meeting deadlines.

</details>


### [11] [Learning-Augmented Performance Model for Tensor Product Factorization in High-Order FEM](https://arxiv.org/abs/2601.06886)
*Xuanzhengbo Ren,Yuta Kawai,Tetsuya Hoshino,Hirofumi Tomita,Takahiro Katagiri,Daichi Mukunoki,Seiya Nishizawa*

Main category: cs.DC

TL;DR: 本文提出了一种结合依赖链分析与XGBoost学习的性能预测模型，用于优化高算术强度核函数在现代HPC架构上的执行效率。


<details>
  <summary>Details</summary>
Motivation: 传统以内存带宽为中心的性能模型无法准确预测高算术强度核函数（如张量n模积）在高SIMD延迟处理器上的表现，需建立反映指令级效率的新模型。

Method: 构建基于依赖链的解析模型，并利用XGBoost估计难以显式建模的关键参数，从而实现对循环分块策略的精准性能预测。

Result: 在Fujitsu A64FX和Intel Xeon Gold 6230处理器上，该学习增强模型相比Roofline和ECM模型显著降低预测误差，MAPE最低达1%。

Conclusion: 融合机器学习的指令级分析模型能更准确预测高算术强度核函数性能，为科学计算应用在现代HPC平台上的优化提供有效工具。

Abstract: Accurate performance prediction is essential for optimizing scientific applications on modern high-performance computing (HPC) architectures. Widely used performance models primarily focus on cache and memory bandwidth, which is suitable for many memory-bound workloads. However, it is unsuitable for highly arithmetic intensive cases such as the sum-factorization with tensor $n$-mode product kernels, which are an optimization technique for high-order finite element methods (FEM). On processors with relatively high single instruction multiple data (SIMD) instruction latency, such as the Fujitsu A64FX, the performance of these kernels is strongly influenced by loop-body splitting strategies. Memory-bandwidth-oriented models are therefore not appropriate for evaluating these splitting configurations, and a model that directly reflects instruction-level efficiency is required. To address this need, we develop a dependency-chain-based analytical formulation that links loop-splitting configurations to instruction dependencies in the tensor $n$-mode product kernel. We further use XGBoost to estimate key parameters in the analytical model that are difficult to model explicitly. Evaluations show that the learning-augmented model outperforms the widely used standard Roofline and Execution-Cache-Memory (ECM) models. On the Fujitsu A64FX processor, the learning-augmented model achieves mean absolute percentage errors (MAPE) between 1% and 24% for polynomial orders ($P$) from 1 to 15. In comparison, the standard Roofline and ECM models yield errors of 42%-256% and 5%-117%, respectively. On the Intel Xeon Gold 6230 processor, the learning-augmented model achieves MAPE values from 1% to 13% for $P$=1 to $P$=14, and 24% at $P$=15. In contrast, the standard Roofline and ECM models produce errors of 1%-73% and 8%-112% for $P$=1 to $P$=15, respectively.

</details>


### [12] [Divergence-Based Adaptive Aggregation for Byzantine Robust Federated Learning](https://arxiv.org/abs/2601.06903)
*Bingnan Xiao,Feng Zhu,Jingjing Zhang,Wei Ni,Xin Wang*

Main category: cs.DC

TL;DR: 本文提出DRAG和BR-DRAG框架，以缓解联邦学习中的客户端漂移并抵御拜占庭攻击，同时加速训练过程。


<details>
  <summary>Details</summary>
Motivation: 联邦学习中因数据异构性和拜占庭攻击导致的客户端漂移阻碍了模型的有效训练与收敛。

Method: DRAG通过设计参考方向和偏差度量对本地更新进行线性校准；BR-DRAG则在服务器维护可信根数据集，生成可信参考方向以应对恶意攻击。

Result: 理论证明DRAG与BR-DRAG在部分参与、数据异构和拜占庭攻击下对非凸模型实现快速收敛；实验表明其优于现有方法。

Conclusion: DRAG有效缓解客户端漂移，BR-DRAG在多种攻击下保持鲁棒性，二者显著提升联邦学习性能。

Abstract: Inherent client drifts caused by data heterogeneity, as well as vulnerability to Byzantine attacks within the system, hinder effective model training and convergence in federated learning (FL). This paper presents two new frameworks, named DiveRgence-based Adaptive aGgregation (DRAG) and Byzantine-Resilient DRAG (BR-DRAG), to mitigate client drifts and resist attacks while expediting training. DRAG designs a reference direction and a metric named divergence of degree to quantify the deviation of local updates. Accordingly, each worker can align its local update via linear calibration without extra communication cost. BR-DRAG refines DRAG under Byzantine attacks by maintaining a vetted root dataset at the server to produce trusted reference directions. The workers' updates can be then calibrated to mitigate divergence caused by malicious attacks. We analytically prove that DRAG and BR-DRAG achieve fast convergence for non-convex models under partial worker participation, data heterogeneity, and Byzantine attacks. Experiments validate the effectiveness of DRAG and its superior performance over state-of-the-art methods in handling client drifts, and highlight the robustness of BR-DRAG in maintaining resilience against data heterogeneity and diverse Byzantine attacks.

</details>


### [13] [SC-MII: Infrastructure LiDAR-based 3D Object Detection on Edge Devices for Split Computing with Multiple Intermediate Outputs Integration](https://arxiv.org/abs/2601.07119)
*Taisuke Noguchi,Takayuki Nishio,Takuya Azumi*

Main category: cs.DC

TL;DR: SC-MII通过在边缘设备上进行部分推理并上传中间特征至服务器整合，实现高效低耗的多雷达3D目标检测。


<details>
  <summary>Details</summary>
Motivation: 解决单雷达盲区问题及边缘设备部署高算力模型时的延迟与能耗挑战。

Method: 提出SC-MII架构，边缘设备运行DNN前段并上传中间输出，由服务器完成特征融合与最终推理。

Result: 实测速度提升2.19倍，边缘处理时间减少71.6%，精度损失不超过1.09%。

Conclusion: SC-MII在保障精度的同时显著降低边缘负载与延迟，适用于自动驾驶场景。

Abstract: 3D object detection using LiDAR-based point cloud data and deep neural networks is essential in autonomous driving technology. However, deploying state-of-the-art models on edge devices present challenges due to high computational demands and energy consumption. Additionally, single LiDAR setups suffer from blind spots. This paper proposes SC-MII, multiple infrastructure LiDAR-based 3D object detection on edge devices for Split Computing with Multiple Intermediate outputs Integration. In SC-MII, edge devices process local point clouds through the initial DNN layers and send intermediate outputs to an edge server. The server integrates these features and completes inference, reducing both latency and device load while improving privacy. Experimental results on a real-world dataset show a 2.19x speed-up and a 71.6% reduction in edge device processing time, with at most a 1.09% drop in accuracy.

</details>


### [14] [Bringing Computation to the data: Interoperable serverless function execution for astrophysical data analysis in the SRCNet](https://arxiv.org/abs/2601.07308)
*Manuel Parra-Royón,Julián Garrido-Sánchez,Susana Sánchez-Expósito,María Ángeles Mendoza,Rob Barnsley,Anthony Moraghan,Jesús Sánchez,Laura Darriba,Carlos Ruíz-Monje,Edgar Joao,Javier Moldón,Jesús Salgado,Lourdes Verdes-Montenegro*

Main category: cs.DC

TL;DR: 本文探讨了无服务器计算和FaaS在SKAO天文数据处理中的应用，验证其可提升效率与扩展性。


<details>
  <summary>Details</summary>
Motivation: SKAO每年生成700PB数据，需在分布式环境中就近计算以降低延迟与传输成本。

Method: 开发并部署代表性天文分析函数（如高斯卷积），集成至SRCNet生态系统中。

Result: FaaS成功嵌入现有架构，实现数据就近计算，减少传输、降低延迟、提高效率。

Conclusion: 无服务器模型为SKA时代海量数据处理提供了高效且可扩展的解决方案。

Abstract: Serverless computing is a paradigm in which the underlying infrastructure is fully managed by the provider, enabling applications and services to be executed with elastic resource provisioning and minimal operational overhead. A core model within this paradigm is Function-as-a-Service (FaaS), where lightweight functions are deployed and triggered on demand, scaling seamlessly with workload. FaaS offers flexibility, cost-effectiveness, and fine-grained scalability, qualities particularly relevant for large-scale scientific infrastructures where data volumes are too large to centralise and computation must increasingly occur close to the data. The Square Kilometre Array Observatory (SKAO) exemplifies this challenge. Once operational, it will generate about 700~PB of data products annually, distributed across the SKA Regional Centre Network (SRCNet), a federation of international centres providing storage, computing, and analysis services. In such a context, FaaS offers a mechanism to bring computation to the data. We studied the principles of serverless and FaaS computing and explored their application to radio astronomy workflows. Representative functions for astrophysical data analysis were developed and deployed, including micro-functions derived from existing libraries and wrappers around domain-specific applications. In particular, a Gaussian convolution function was implemented and integrated within the SRCNet ecosystem. The use case demonstrates that FaaS can be embedded into the existing SRCNet ecosystem of services, allowing functions to run directly at sites where data replicas are stored. This reduces latency, minimises transfers, and improves efficiency, aligning with federated, data-proximate computation. The results show that serverless models provide a scalable and efficient pathway to address the data volumes of the SKA era.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [15] [Contract2Plan: Verified Contract-Grounded Retrieval-Augmented Optimization for BOM-Aware Procurement and Multi-Echelon Inventory Planning](https://arxiv.org/abs/2601.06164)
*Sahil Agarwal*

Main category: cs.SE

TL;DR: Contract2Plan是一个结合生成式AI与优化求解器的验证型采购规划系统，通过在计划输出前插入合规性检查门，确保合同条款被正确提取、建模和执行，避免因LLM提取错误导致的不可行计划或违约风险。


<details>
  <summary>Details</summary>
Motivation: 现有仅依赖LLM提取合同条款的规划流程易受遗漏、单位错误或冲突影响，导致计划不可行或违反合同，尤其在BOM耦合场景下风险被放大。

Method: 系统采用三阶段流程：1）带溯源的条款证据检索与类型化约束提取；2）编译为BOM感知的混合整数线性规划（MILP）模型；3）通过求解器诊断验证约束的接地性、资格性、一致性和可行性，并触发修复或人工介入。

Result: 在包含MOQ提升与紧急采购的合成微基准测试中（500实例，T=5），仅提取方法表现出重尾后悔与显著MOQ违规率，验证了合同约束下规划系统中合规门控的必要性。

Conclusion: 合同驱动的规划系统必须将验证作为核心组件，Contract2Plan框架能有效保障自动化决策的安全性，并对可保守修复与需人工确认的条款类别进行了形式化区分。

Abstract: Procurement and inventory planning is governed not only by demand forecasts and bills of materials (BOMs), but also by operational terms in contracts and supplier documents (e.g., MOQs, lead times, price tiers, allocation caps, substitution approvals). LLM-based extraction can speed up structuring these terms, but extraction-only or LLM-only decision pipelines are brittle: missed clauses, unit errors, and unresolved conflicts can yield infeasible plans or silent contract violations, amplified by BOM coupling. We introduce Contract2Plan, a verified GenAI-to-optimizer pipeline that inserts a solver-based compliance gate before plans are emitted. The system retrieves clause evidence with provenance, extracts a typed constraint schema with evidence spans, compiles constraints into a BOM-aware MILP, and verifies grounding, eligibility, consistency, and feasibility using solver diagnostics, triggering targeted repair or abstention when automation is unsafe. We formalize which clause classes admit conservative repair with contract-safe feasibility guarantees and which require human confirmation. A self-contained synthetic micro-benchmark (500 instances; T=5) computed by exact enumeration under an execution model with MOQ uplift and emergency purchases shows heavy-tailed regret and nontrivial MOQ-violation incidence for extraction-only planning, motivating verification as a first-class component of contract-grounded planning systems.

</details>


### [16] [Attention Mechanism and Heuristic Approach: Context-Aware File Ranking Using Multi-Head Self-Attention](https://arxiv.org/abs/2601.06185)
*Pradeep Kumar Sharma,Shantanu Godbole,Sarada Prasad Jena,Hritvik Shrivastava*

Main category: cs.SE

TL;DR: 本文提出使用多头自注意力机制改进确定性评分，以提升变更影响分析中受影响文件的召回率。


<details>
  <summary>Details</summary>
Motivation: 现有方法因忽略特征间上下文依赖，导致召回率存在瓶颈，难以模拟专家推理模式。

Method: 在确定性评分基础上引入多头自注意力机制，动态调整各文件特征权重，生成上下文感知的修正评分。

Result: 在200个测试案例中，Top-50召回率从62-65%提升至78-82%，专家主观评分从6.5/10提升至8.6/10。

Conclusion: 该方法弥合了自动化与专家判断间的推理能力差距，显著提升召回率，适用于仓库感知的工作量预估场景。

Abstract: The identification and ranking of impacted files within software reposi-tories is a key challenge in change impact analysis. Existing deterministic approaches that combine heuristic signals, semantic similarity measures, and graph-based centrality metrics have demonstrated effectiveness in nar-rowing candidate search spaces, yet their recall plateaus. This limitation stems from the treatment of features as linearly independent contributors, ignoring contextual dependencies and relationships between metrics that characterize expert reasoning patterns. To address this limitation, we propose the application of Multi-Head Self-Attention as a post-deterministic scoring refinement mechanism. Our approach learns contextual weighting between features, dynamically adjust-ing importance levels per file based on relational behavior exhibited across candidate file sets. The attention mechanism produces context-aware adjustments that are additively combined with deterministic scores, pre-serving interpretability while enabling reasoning similar to that performed by experts when reviewing change surfaces. We focus on recall rather than precision, as false negatives (missing impacted files) are far more costly than false positives (irrelevant files that can be quickly dismissed during review). Empirical evaluation on 200 test cases demonstrates that the introduc-tion of self-attention improves Top-50 recall from approximately 62-65% to between 78-82% depending on repository complexity and structure, achiev-ing 80% recall at Top-50 files. Expert validation yields improvement from 6.5/10 to 8.6/10 in subjective accuracy alignment. This transformation bridges the reasoning capability gap between deterministic automation and expert judgment, improving recall in repository-aware effort estimation.

</details>


### [17] [RiskBridge: Turning CVEs into Business-Aligned Patch Priorities](https://arxiv.org/abs/2601.06201)
*Yelena Mujibur Sheikh,Awez Akhtar Khatik,Luoxi Tang,Yuqiao Meng,Zhaohan Xi*

Main category: cs.SE

TL;DR: RiskBridge是一个可解释、合规感知的漏洞管理框架，通过整合CVSS v4、EPSS和CISA KEV等多源情报，结合零日暴露模拟、策略即代码引擎与ROI优化器，显著提升企业漏洞修复效率与合规性。


<details>
  <summary>Details</summary>
Motivation: 传统漏洞优先级评估方法（如CVSS）仅提供静态严重性评分，无法反映实际利用概率、合规紧迫性及业务影响，导致修复低效滞后。

Method: 提出RiskBridge框架，包含零日暴露概率模拟模型、策略即代码合规引擎、以及基于投资回报率的风险优化器，实现动态、业务对齐的修复排序。

Result: 实验表明，相比现有商业方案，RiskBridge降低88%残余风险，提升18天SLA合规时效，提高35%修复效率。

Conclusion: RiskBridge为现代企业提供了一个自动化、可解释、以业务为中心的漏洞决策智能系统，融合概率建模、合规推理与优化分析。

Abstract: Enterprises are confronted with an unprecedented escalation in cybersecurity vulnerabilities, with thousands of new CVEs disclosed each month. Conventional prioritization frameworks such as CVSS offer static severity metrics that fail to account for exploit probability, compliance urgency, and operational impact, resulting in inefficient and delayed remediation. This paper introduces RiskBridge, an explainable and compliance-aware vulnerability management framework that integrates multi-source intelligence from CVSS v4, EPSS, and CISA KEV to produce dynamic, business -- aligned patch priorities. RiskBridge employs a probabilistic Zero-Day Exposure Simulation (ZDES) model to forecast near-term exploit likelihood, a Policy-as-Code Engine to translate regulatory mandates (e.g., PCI DSS, NIST SP 800-53) into automated SLA logic, and an ROI-driven Optimizer to maximize cumulative risk reduction per remediation effort. Experimental evaluations using live CVE datasets demonstrate an 88% reduction in residual risk, an 18-day improvement in SLA compliance, and a 35% increase in remediation efficiency compared to state-of-the-art commercial baselines. These findings validate RiskBridge as a practical and auditable decision-intelligence system that unifies probabilistic modeling, compliance reasoning, and optimization analytics. The framework represents a step toward automated, explainable, and business-centric vulnerability management in modern enterprise environments

</details>


### [18] [Self-Admitted Technical Debt in LLM Software: An Empirical Comparison with ML and Non-ML Software](https://arxiv.org/abs/2601.06266)
*Niruthiha Selvanayagam,Manel Abdellatif,Taher A. Ghaleb*

Main category: cs.SE

TL;DR: 本文首次实证研究LLM系统中的自认技术债（SATD），发现其积累速率与ML系统相近，但无债期更长，且识别出三种新型LLM特有债务。


<details>
  <summary>Details</summary>
Motivation: 探索LLM系统中SATD的表现与演化，弥补现有研究对LLM时代技术债认知的空白。

Method: 对比分析477个仓库（LLM/ML/非ML各159个），进行生存分析与定性编码，识别SATD引入与移除动态及新型债务类型。

Result: LLM仓库SATD积累率3.95%，无债期中位数492天（ML为204天）；发现三类新债务：模型栈权宜债、模型依赖债、性能优化债。

Conclusion: LLM系统虽架构复杂，但初期抗债能力强，后期债务增长快，需针对性治理其特有债务形态。

Abstract: Self-admitted technical debt (SATD), referring to comments flagged by developers that explicitly acknowledge suboptimal code or incomplete functionality, has received extensive attention in machine learning (ML) and traditional (Non-ML) software. However, little is known about how SATD manifests and evolves in contemporary Large Language Model (LLM)-based systems, whose architectures, workflows, and dependencies differ fundamentally from both traditional and pre-LLM ML software. In this paper, we conduct the first empirical study of SATD in the LLM era, replicating and extending prior work on ML technical debt to modern LLM-based systems. We compare SATD prevalence across LLM, ML, and non-ML repositories across a total of 477 repositories (159 per category). We perform survival analysis of SATD introduction and removal to understand the dynamics of technical debt across different development paradigms. Surprisingly, despite their architectural complexity, our results reveal that LLM repositories accumulate SATD at similar rates to ML systems (3.95% vs. 4.10%). However, we observe that LLM repositories remain debt-free 2.4x longer than ML repositories (a median of 492 days vs. 204 days), and then start to accumulate technical debt rapidly. Moreover, our qualitative analysis of 377 SATD instances reveals three new forms of technical debt unique to LLM-based development that have not been reported in prior research: Model-Stack Workaround Debt, Model Dependency Debt, and Performance Optimization Debt. Finally, by mapping SATD to stages of the LLM development pipeline, we observe that debt concentrates

</details>


### [19] [Mining Quantum Software Patterns in Open-Source Projects](https://arxiv.org/abs/2601.06281)
*Neilson Carlos Leite Ramalho,Erico A. da Silva,Higor Amario de Souza,Marcos Lordello Chaim*

Main category: cs.SE

TL;DR: 本文通过分析985个Jupyter Notebook，研究量子编程模式的实际应用，发现开发者正从基础电路工具转向高阶算法和领域专用模式，表明量子软件工程日趋成熟。


<details>
  <summary>Details</summary>
Motivation: 随着量子计算框架的发展，亟需了解实践中如何使用高阶抽象模式以推动量子软件工程进步。

Method: 构建基于Qiskit、PennyLane和Classiq的知识库，识别9种新模式，并开发语义搜索工具在80个开源项目中自动检测模式应用。

Result: 发现开发者分三个层次使用模式：基础电路工具、算法原语（如振幅放大）、领域专用应用（如金融与优化）。

Conclusion: 量子软件工程正走向成熟，开发者越来越多依赖高阶构建块解决实际问题。

Abstract: Quantum computing has become an active research field in recent years, as its applications in fields such as cryptography, optimization, and materials science are promising. Along with these developments, challenges and opportunities exist in the field of Quantum Software Engineering, as the development of frameworks and higher-level abstractions has attracted practitioners from diverse backgrounds. Unlike initial quantum frameworks based on the circuit model, recent frameworks and libraries leverage higher-level abstractions for creating quantum programs. This paper presents an empirical study of 985 Jupyter Notebooks from 80 open-source projects to investigate how quantum patterns are applied in practice. Our work involved two main stages. First, we built a knowledge base from three quantum computing frameworks (Qiskit, PennyLane, and Classiq). This process led us to identify and document 9 new patterns that refine and extend the existing quantum computing pattern catalog. Second, we developed a reusable semantic search tool to automatically detect these patterns across our large-scale dataset, providing a practitioner-focused analysis. Our results show that developers use patterns in three levels: from foundational circuit utilities, to common algorithmic primitives (e.g., Amplitude Amplification), up to domain-specific applications for finance and optimization. This indicates a maturing field where developers are increasingly using high-level building blocks to solve real-world problems.

</details>


### [20] [Foundational Analysis of Safety Engineering Requirements (SAFER)](https://arxiv.org/abs/2601.06335)
*Noga Chemo,Yaniv Mordecai,Yoram Reich*

Main category: cs.SE

TL;DR: SAFER框架利用生成式AI与模型驱动方法，提升复杂安全关键系统的需求分析效率与可靠性。


<details>
  <summary>Details</summary>
Motivation: 解决多利益相关方需求不协调导致的安全要求漏洞、重复与矛盾问题。

Method: 结合MBSE与生成式AI，对需求规范模型进行结构化分析，映射功能、识别不足、检测重复与矛盾。

Result: 在无人机系统案例中显著提升需求一致性检测能力，增强安全工程流程效率与可靠性。

Conclusion: 生成式AI需结合形式化模型并系统查询，才能有效支持早期安全需求规范与稳健架构设计。

Abstract: We introduce a framework for Foundational Analysis of Safety Engineering Requirements (SAFER), a model-driven methodology supported by Generative AI to improve the generation and analysis of safety requirements for complex safety-critical systems. Safety requirements are often specified by multiple stakeholders with uncoordinated objectives, leading to gaps, duplications, and contradictions that jeopardize system safety and compliance. Existing approaches are largely informal and insufficient for addressing these challenges. SAFER enhances Model-Based Systems Engineering (MBSE) by consuming requirement specification models and generating the following results: (1) mapping requirements to system functions, (2) identifying functions with insufficient requirement specifications, (3) detecting duplicate requirements, and (4) identifying contradictions within requirement sets. SAFER provides structured analysis, reporting, and decision support for safety engineers. We demonstrate SAFER on an autonomous drone system, significantly improving the detection of requirement inconsistencies, enhancing both efficiency and reliability of the safety engineering process. We show that Generative AI must be augmented by formal models and queried systematically, to provide meaningful early-stage safety requirement specifications and robust safety architectures.

</details>


### [21] [Coding in a Bubble? Evaluating LLMs in Resolving Context Adaptation Bugs During Code Adaptation](https://arxiv.org/abs/2601.06497)
*Tanghaoran Zhang,Xinjun Mao,Shangwen Wang,Yuxin Zhao,Yao Lu,Zezhou Tang,Wenyu Xu,Longfei Sun,Changrong Xie,Kang Yang,Yue Yu*

Main category: cs.SE

TL;DR: 提出CtxBugGen框架生成上下文适配缺陷以评估大语言模型在代码适配中的表现，发现现有模型跨上下文推理能力不足。


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型在代码适配中难以识别和修复上下文适配缺陷（CtxBugs）的问题，提升其实际应用可靠性。

Method: 设计四步流程框架CtxBugGen：任务选择、任务扰动、LLM变体生成、缺陷识别，构建评估基准并实证分析四个主流LLM性能。

Result: 最佳模型Kimi-K2仅55.93% Pass@1，仅修复52.47%缺陷；CtxBugs使适配性能下降最多30%，模型常忽略或复制缺陷。

Conclusion: 当前LLM缺乏跨上下文推理能力，亟需新方法增强其上下文感知能力以实现可靠代码适配。

Abstract: Code adaptation is a fundamental but challenging task in software development, requiring developers to modify existing code for new contexts. A key challenge is to resolve Context Adaptation Bugs (CtxBugs), which occurs when code correct in its original context violates constraints in the target environment. Unlike isolated bugs, CtxBugs cannot be resolved through local fixes and require cross-context reasoning to identify semantic mismatches. Overlooking them may lead to critical failures in adaptation. Although Large Language Models (LLMs) show great potential in automating code-related tasks, their ability to resolve CtxBugs remains a significant and unexplored obstacle to their practical use in code adaptation. To bridge this gap, we propose CtxBugGen, a novel framework for generating CtxBugs to evaluate LLMs. Its core idea is to leverage LLMs' tendency to generate plausible but context-free code when contextual constraints are absent. The framework generates CtxBugs through a four-step process to ensure their relevance and validity: (1) Adaptation Task Selection, (2) Task-specific Perturbation,(3) LLM-based Variant Generation and (4) CtxBugs Identification. Based on the benchmark constructed by CtxBugGen, we conduct an empirical study with four state-of-the-art LLMs. Our results reveal their unsatisfactory performance in CtxBug resolution. The best performing LLM, Kimi-K2, achieves 55.93% on Pass@1 and resolves just 52.47% of CtxBugs. The presence of CtxBugs degrades LLMs' adaptation performance by up to 30%. Failure analysis indicates that LLMs often overlook CtxBugs and replicate them in their outputs. Our study highlights a critical weakness in LLMs' cross-context reasoning and emphasize the need for new methods to enhance their context awareness for reliable code adaptation.

</details>


### [22] [Fixturize: Bridging the Fixture Gap in Test Generation](https://arxiv.org/abs/2601.06615)
*Pengyu Xue,Chengyi Wang,Zhen Yang,Xiapu Luo,Yuxuan Zhang,Xiran Lyu,Yifei Pei,Zonghan Jia,Yichen Sun,Linhao Wu,Kunwu Zheng*

Main category: cs.SE

TL;DR: Fixturize框架通过识别和生成测试夹具，显著提升LLM自动生成单元测试的质量与覆盖率。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型在自动生成单元测试时常忽略构建必要的测试夹具，影响测试执行与覆盖率。

Method: 提出Fixturize诊断框架，结合FixtureEval基准，通过迭代反馈机制识别夹具依赖并合成相应夹具。

Result: 在Python和Java上，夹具依赖识别准确率达88.38%-97.00%，SuitePS提升18.03%-42.86%，行/分支覆盖率分别提升最高达31.54%/119.66%。

Conclusion: 测试夹具感知是现代自动化测试流程中不可或缺的关键环节，Fixturize有效填补了这一空白。

Abstract: Current Large Language Models (LLMs) have advanced automated unit test generation but face a critical limitation: they often neglect to construct the necessary test fixtures, which are the environmental setups required for a test to run. To bridge this gap, this paper proposes Fixturize, a diagnostic framework that proactively identifies fixture-dependent functions and synthesizes test fixtures accordingly through an iterative, feedback-driven process, thereby improving the quality of auto-generated test suites of existing approaches. For rigorous evaluation, the authors introduce FixtureEval, a dedicated benchmark comprising 600 curated functions across two Programming Languages (PLs), i.e., Python and Java, with explicit fixture dependency labels, enabling both the corresponding classification and generation tasks. Empirical results demonstrate that Fixturize is highly effective, achieving 88.38%-97.00% accuracy across benchmarks in identifying the dependence of test fixtures and significantly enhancing the Suite Pass rate (SuitePS) by 18.03%-42.86% on average across both PLs with the auto-generated fixtures. Owing to the maintenance of test fixtures, Fixturize further improves line/branch coverage when integrated with existing testing tools of both LLM-based and Search-based by 16.85%/24.08% and 31.54%/119.66% on average, respectively. The findings establish fixture awareness as an essential, missing component in modern auto-testing pipelines.

</details>


### [23] [An Exploratory Pilot Survey on Technical Quality Control Practices in Agile R&D Projects](https://arxiv.org/abs/2601.06689)
*Mateus Costa Lucena*

Main category: cs.SE

TL;DR: 本研究通过问卷调查探索敏捷研发团队在Scrum环境中管理技术质量的实践与挑战，发现尽管自动化测试等方法被广泛认知，但实际应用不一致且缺乏有效度量。


<details>
  <summary>Details</summary>
Motivation: 解决高技术不确定性与实验压力下敏捷研发项目中技术质量管理的持续性难题。

Method: 对巴西马瑙斯科技机构的专业人员发放结构化问卷，结合定量数据与定性反馈进行分析。

Result: 团队虽普遍了解自动化测试、代码审查等实践，但执行不连贯；技术质量指标监控不足，技术债务评估机制缺失。

Conclusion: 研究为区域创新生态系统内敏捷研发项目的技术质量管理提供了探索性基线，而非普适性结论。

Abstract: Managing technical quality in agile Research and Development (R&D) software projects represents a persistent challenge, particularly in contexts characterized by high technical uncertainty and experimental pressure. This exploratory pilot survey explores how agile R&D software teams report the use of practices and metrics related to technical quality control within Scrum-based environments. The study employed a structured questionnaire administered to professionals from Science and Technology Institutions (STIs) located in Manaus, Brazil, aiming to capture reported practices, perceptions of quality, and recurrent challenges. Quantitative data were complemented by qualitative responses to support contextual interpretation. The results indicate that although practices such as automated testing, code review, and continuous integration are widely acknowledged, their reported application is often inconsistent across iterations. Gaps were also observed in the monitoring of technical quality metrics and in the reporting of mechanisms for assessing technical debt from a business perspective. Rather than aiming for generalization, this study offers an exploratory baseline that describes how technical quality is managed in agile R&D projects within a regional innovation ecosystem.

</details>


### [24] [Comparative Separation: Evaluating Separation on Comparative Judgment Test Data](https://arxiv.org/abs/2601.06761)
*Xiaoyin Xi,Neeku Capak,Kate Stockwell,Zhe Yu*

Main category: cs.SE

TL;DR: 本文提出了一种新颖的公平性概念——比较分离，用于在比较判断测试数据上评估机器学习软件的公平性，并证明其在二分类问题中与传统分离准则等价。


<details>
  <summary>Details</summary>
Motivation: 由于机器学习软件越来越多地用于高风险决策，确保其对不同敏感群体表现一致至关重要，但传统分离评估需要每个测试点的真实标签，因此探索基于比较判断数据的评估方法具有现实意义。

Method: 定义了比较分离概念及其度量指标，从理论和实证两方面证明其在二分类中与分离等价，并分析了达到相同统计功效所需的数据量。

Result: 比较分离在二分类任务中与传统分离等价，且使用比较判断数据进行模型评估具备可行性与认知负担更低的实践优势。

Conclusion: 本研究首次探索了在比较判断测试数据上进行公平性评估，验证了该方法的有效性和实用性，为软件工程领域提供了新工具。

Abstract: This research seeks to benefit the software engineering society by proposing comparative separation, a novel group fairness notion to evaluate the fairness of machine learning software on comparative judgment test data. Fairness issues have attracted increasing attention since machine learning software is increasingly used for high-stakes and high-risk decisions. It is the responsibility of all software developers to make their software accountable by ensuring that the machine learning software do not perform differently on different sensitive groups -- satisfying the separation criterion. However, evaluation of separation requires ground truth labels for each test data point. This motivates our work on analyzing whether separation can be evaluated on comparative judgment test data. Instead of asking humans to provide the ratings or categorical labels on each test data point, comparative judgments are made between pairs of data points such as A is better than B. According to the law of comparative judgment, providing such comparative judgments yields a lower cognitive burden for humans than providing ratings or categorical labels. This work first defines the novel fairness notion comparative separation on comparative judgment test data, and the metrics to evaluate comparative separation. Then, both theoretically and empirically, we show that in binary classification problems, comparative separation is equivalent to separation. Lastly, we analyze the number of test data points and test data pairs required to achieve the same level of statistical power in the evaluation of separation and comparative separation, respectively. This work is the first to explore fairness evaluation on comparative judgment test data. It shows the feasibility and the practical benefits of using comparative judgment test data for model evaluations.

</details>


### [25] [MicLog: Towards Accurate and Efficient LLM-based Log Parsing via Progressive Meta In-Context Learning](https://arxiv.org/abs/2601.07005)
*Jianbo Yu,Yixuan Li,Hai Xu,Kang Xu,Junjielong Xu,Zhijing Li,Pinjia He,Wanyuan Wang*

Main category: cs.SE

TL;DR: MicLog是一种结合元学习与上下文学习的新型日志解析框架，显著提升准确率并降低耗时。


<details>
  <summary>Details</summary>
Motivation: 传统日志解析器在语义变化和数据稀缺场景下表现不佳，而现有LLM方法存在示例利用不足与查询效率低的问题。

Method: 提出ProgMeta-ICL范式，结合加权DBSCAN采样与增强BM25示例选择，并引入多级预查询缓存加速解析。

Result: 在Loghub-2.0上准确率提升10.3%，解析时间减少42.4%。

Conclusion: MicLog有效提升了小规模开源LLM在日志解析任务中的性能与效率。

Abstract: Log parsing converts semi-structured logs into structured templates, forming a critical foundation for downstream analysis. Traditional syntax and semantic-based parsers often struggle with semantic variations in evolving logs and data scarcity stemming from their limited domain coverage. Recent large language model (LLM)-based parsers leverage in-context learning (ICL) to extract semantics from examples, demonstrating superior accuracy. However, LLM-based parsers face two main challenges: 1) underutilization of ICL capabilities, particularly in dynamic example selection and cross-domain generalization, leading to inconsistent performance; 2) time-consuming and costly LLM querying. To address these challenges, we present MicLog, the first progressive meta in-context learning (ProgMeta-ICL) log parsing framework that combines meta-learning with ICL on small open-source LLMs (i.e., Qwen-2.5-3B). Specifically, MicLog: i) enhances LLMs' ICL capability through a zero-shot to k-shot ProgMeta-ICL paradigm, employing weighted DBSCAN candidate sampling and enhanced BM25 demonstration selection; ii) accelerates parsing via a multi-level pre-query cache that dynamically matches and refines recently parsed templates. Evaluated on Loghub-2.0, MicLog achieves 10.3% higher parsing accuracy than the state-of-the-art parser while reducing parsing time by 42.4%.

</details>


### [26] [Between Policy and Practice: GenAI Adoption in Agile Software Development Teams](https://arxiv.org/abs/2601.07051)
*Michael Neumann,Lasse Bischof,Nic Elias Hinz,Luca Stockmann,Dennis Schrader,Ana Carolina Ahaus,Erim Can Demirci,Benjamin Gabel,Maria Rauschenberger,Philipp Diebold,Henning Fritzemeier,Adam Przybylek*

Main category: cs.SE

TL;DR: 该研究探讨了敏捷环境中生成式AI工具的实际应用，发现其在创意、文档和代码辅助方面有显著效益，但需解决数据隐私、治理缺失等障碍。


<details>
  <summary>Details</summary>
Motivation: 生成式AI正快速改变软件工程活动，但在敏捷环境中的实际采纳情况尚不明确，亟需探索其落地模式与挑战。

Method: 在三家德国企业开展多案例研究，通过17次半结构化访谈与文档分析，结合TOE框架进行跨案例主题分析。

Result: GenAI主要用于创意任务、文档与代码辅助；优势是提升效率与创造力，障碍包括数据隐私、验证成本与治理缺失；政策与实践常脱节。

Conclusion: GenAI可增强敏捷角色能力，但需在技术、组织与环境维度协同对齐，建立清晰政策、数据保护机制与用户培训以实现负责任整合。

Abstract: Context: The rapid emergence of generative AI (GenAI) tools has begun to reshape various software engineering activities. Yet, their adoption within agile environments remains underexplored. Objective: This study investigates how agile practitioners adopt GenAI tools in real-world organizational contexts, focusing on regulatory conditions, use cases, benefits, and barriers. Method: An exploratory multiple case study was conducted in three German organizations, involving 17 semi-structured interviews and document analysis. A cross-case thematic analysis was applied to identify GenAI adoption patterns. Results: Findings reveal that GenAI is primarily used for creative tasks, documentation, and code assistance. Benefits include efficiency gains and enhanced creativity, while barriers relate to data privacy, validation effort, and lack of governance. Using the Technology-Organization-Environment (TOE) framework, we find that these barriers stem from misalignments across the three dimensions. Regulatory pressures are often translated into policies without accounting for actual technological usage patterns or organizational constraints. This leads to systematic gaps between policy and practice. Conclusion: GenAI offers significant potential to augment agile roles but requires alignment across TOE dimensions, including clear policies, data protection measures, and user training to ensure responsible and effective integration.

</details>


### [27] [A Large-Scale Study on the Development and Issues of Multi-Agent AI Systems](https://arxiv.org/abs/2601.07136)
*Daniel Liu,Krishna Upadhyay,Vinaik Chhetri,A. B. Siddique,Umar Farooq*

Main category: cs.SE

TL;DR: 本研究首次对开源多智能体AI系统进行大规模实证分析，揭示其开发模式、维护重点与问题解决效率，强调需提升测试、文档与维护实践以确保长期稳定。


<details>
  <summary>Details</summary>
Motivation: 当前多智能体AI系统快速发展，但对其实际演化与维护机制缺乏系统性理解。

Method: 分析八个主流系统中超过42K次提交与4.7K个已解决问题，识别开发模式并统计变更类型与问题分布。

Result: 发现三类开发模式（持续型、稳定型、爆发型），功能增强占主导（40.8%），最常见问题是缺陷（22%）、基础设施（14%）和协调挑战（10%），中位解决时间从一天至两周不等。

Conclusion: 当前生态系统虽具发展动能，但仍显脆弱，亟需加强测试架构、文档质量与维护流程以保障可持续性。

Abstract: The rapid emergence of multi-agent AI systems (MAS), including LangChain, CrewAI, and AutoGen, has shaped how large language model (LLM) applications are developed and orchestrated. However, little is known about how these systems evolve and are maintained in practice. This paper presents the first large-scale empirical study of open-source MAS, analyzing over 42K unique commits and over 4.7K resolved issues across eight leading systems. Our analysis identifies three distinct development profiles: sustained, steady, and burst-driven. These profiles reflect substantial variation in ecosystem maturity. Perfective commits constitute 40.8% of all changes, suggesting that feature enhancement is prioritized over corrective maintenance (27.4%) and adaptive updates (24.3%). Data about issues shows that the most frequent concerns involve bugs (22%), infrastructure (14%), and agent coordination challenges (10%). Issue reporting also increased sharply across all frameworks starting in 2023. Median resolution times range from under one day to about two weeks, with distributions skewed toward fast responses but a minority of issues requiring extended attention. These results highlight both the momentum and the fragility of the current ecosystem, emphasizing the need for improved testing infrastructure, documentation quality, and maintenance practices to ensure long-term reliability and sustainability.

</details>


### [28] [Engineering Decisions in MBSE: Insights for a Decision Capture Framework Development](https://arxiv.org/abs/2601.07301)
*Nidhal Selmi,Jean-michel Bruel,Sébastien Mosser,Matthieu Crespo,Alain Kerbrat*

Main category: cs.SE

TL;DR: 本文提出了一种轻量级框架，将决策捕获集成到基于模型的系统工程（MBSE）工作流中，以提升工程团队效率。


<details>
  <summary>Details</summary>
Motivation: 传统决策捕获耗时且缺乏上下文，难以复用，亟需改进方法。

Method: 通过将决策备选方案表示为系统模型切片，将其嵌入MBSE模型中，与需求、行为和架构元素建立显式关联。

Result: 在飞机架构简化案例中验证了该框架的可行性，并提出了应对决策捕获挑战的初步解决方案。

Conclusion: 该框架可降低决策捕获负担，同时增强决策知识的可追溯性和复用性，有助于提升系统工程效率。

Abstract: Decision-making is a core engineering design activity that conveys the engineer's knowledge and translates it into courses of action. Capturing this form of knowledge can reap potential benefits for the engineering teams and enhance development efficiency. Despite its clear value, traditional decision capture often requires a significant amount of effort and still falls short of capturing the necessary context for reuse. Model-based systems engineering (MBSE) can be a promising solution to address these challenges by embedding decisions directly within system models, which can reduce the capture workload while maintaining explicit links to requirements, behaviors, and architectural elements. This article discusses a lightweight framework for integrating decision capture into MBSE workflows by representing decision alternatives as system model slices. Using a simplified industry example from aircraft architecture, we discuss the main challenges associated with decision capture and propose preliminary solutions to address these challenges.

</details>


### [29] [FairRF: Multi-Objective Search for Single and Intersectional Software Fairness](https://arxiv.org/abs/2601.07537)
*Giordano d'Alosio,Max Hort,Rebecca Moussa,Federica Sarro*

Main category: cs.SE

TL;DR: FairRF是一种基于多目标进化搜索的新方法，用于在分类任务中优化公平性和有效性，允许利益相关者根据需求选择最优解。


<details>
  <summary>Details</summary>
Motivation: AI和ML系统在敏感领域的广泛应用引发了对其公平性的严重担忧，现有方法多为黑箱，无法让利益相关者权衡公平性与有效性。

Method: FairRF以随机森林模型为基础，通过多目标进化搜索寻找最佳超参数配置和数据变异，返回帕累托最优解集。

Result: 实验表明，FairRF能显著提升基础分类器的公平性，同时保持预测有效性，并在多种公平性定义下表现优于现有方法。

Conclusion: FairRF是一种有效的偏见缓解方法，支持利益相关者根据具体需求定制公平软件系统的开发。

Abstract: Background: The wide adoption of AI- and ML-based systems in sensitive domains raises severe concerns about their fairness. Many methods have been proposed in the literature to enhance software fairness. However, the majority behave as a black-box, not allowing stakeholders to prioritise fairness or effectiveness (i.e., prediction correctness) based on their needs. Aims: In this paper, we introduce FairRF, a novel approach based on multi-objective evolutionary search to optimise fairness and effectiveness in classification tasks. FairRF uses a Random Forest (RF) model as a base classifier and searches for the best hyperparameter configurations and data mutation to maximise fairness and effectiveness. Eventually, it returns a set of Pareto optimal solutions, allowing the final stakeholders to choose the best one based on their needs. Method: We conduct an extensive empirical evaluation of FairRF against 26 different baselines in 11 different scenarios using five effectiveness and three fairness metrics. Additionally, we also include two variations of the fairness metrics for intersectional bias for a total of six definitions analysed. Result: Our results show that FairRF can significantly improve the fairness of base classifiers, while maintaining consistent prediction effectiveness. Additionally, FairRF provides a more consistent optimisation under all fairness definitions compared to state-of-the-art bias mitigation methods and overcomes the existing state-of-the-art approach for intersectional bias mitigation. Conclusions: FairRF is an effective approach for bias mitigation also allowing stakeholders to adapt the development of fair software systems based on their specific needs.

</details>


### [30] [OODEval: Evaluating Large Language Models on Object-Oriented Design](https://arxiv.org/abs/2601.07602)
*Bingxu Xiao,Yunwei Dong,Yiqi Tang,Manqing Zhang,Yifan Zhou,Chunyan Ma,Yepang Liu*

Main category: cs.SE

TL;DR: 本文评估了29个大语言模型在面向对象设计任务中的表现，提出了新基准OODEval和统一评价指标CLUE，发现LLMs虽语法准确但语义缺陷明显，性能受参数规模、代码专精与指令调优影响显著。


<details>
  <summary>Details</summary>
Motivation: 弥补当前大语言模型在软件设计能力评估上的空白，尤其是面向对象设计领域缺乏标准化评测基准与度量方法。

Method: 构建包含50个OOD任务的OODEval基准和含940份人工评分类图的OODEval-Human基准，提出CLUE统一评估指标，从五个研究问题系统分析模型表现。

Result: Qwen3-Coder-30B表现最优，接近DeepSeek-R1与GPT-4o；小模型Gemma3-4B-IT优于GPT-4o-Mini；顶级模型接近本科生平均水平，但远逊于优秀人类设计师；参数规模、代码专精与指令调优显著提升性能。

Conclusion: 当前LLMs在OOD任务中仍存在严重语义缺陷，需在方法与关系生成方面改进；未来应结合人类设计知识优化模型架构与训练策略以提升设计能力。

Abstract: Recent advances in large language models (LLMs) have driven extensive evaluations in software engineering. however, most prior work concentrates on code-level tasks, leaving software design capabilities underexplored. To fill this gap, we conduct a comprehensive empirical study evaluating 29 LLMs on object-oriented design (OOD) tasks. Owing to the lack of standardized benchmarks and metrics, we introduce OODEval, a manually constructed benchmark comprising 50 OOD tasks of varying difficulty, and OODEval-Human, the first human-rated OOD benchmark, which includes 940 undergraduate-submitted class diagrams evaluated by instructors. We further propose CLUE (Class Likeness Unified Evaluation), a unified metric set that assesses both global correctness and fine-grained design quality in class diagram generation. Using these benchmarks and metrics, we investigate five research questions: overall correctness, comparison with humans, model dimension analysis, task feature analysis, and bad case analysis. The results indicate that while LLMs achieve high syntactic accuracy, they exhibit substantial semantic deficiencies, particularly in method and relationship generation. Among the evaluated models, Qwen3-Coder-30B achieves the best overall performance, rivaling DeepSeek-R1 and GPT-4o, while Gemma3-4B-IT outperforms GPT-4o-Mini despite its smaller parameter scale. Although top-performing LLMs nearly match the average performance of undergraduates, they remain significantly below the level of the best human designers. Further analysis shows that parameter scale, code specialization, and instruction tuning strongly influence performance, whereas increased design complexity and lower requirement readability degrade it. Bad case analysis reveals common failure modes, including keyword misuse, missing classes or relationships, and omitted methods.

</details>


### [31] ["TODO: Fix the Mess Gemini Created": Towards Understanding GenAI-Induced Self-Admitted Technical Debt](https://arxiv.org/abs/2601.07786)
*Abdullah Al Mujahid,Mia Mohammad Imran*

Main category: cs.SE

TL;DR: 研究分析了6540条提及LLM的代码注释，发现81条同时承认技术债务，提出‘GIST’概念描述开发者因不确定AI生成代码行为而引入的技术债务。


<details>
  <summary>Details</summary>
Motivation: 探讨AI工具在软件开发中的影响，尤其是其如何导致技术债务的产生。

Method: 从GitHub公共仓库中收集并分析2022年11月至2025年7月期间提及LLM的Python和JavaScript代码注释。

Result: 发现开发者常推迟测试、未完全适配或不理解AI生成代码，表明AI辅助影响技术债务出现的时间与原因。

Conclusion: 提出‘GIST’作为描述AI引发自认技术债务的新概念，有助于理解AI在开发流程中的副作用。

Abstract: As large language models (LLMs) such as ChatGPT, Copilot, Claude, and Gemini become integrated into software development workflows, developers increasingly leave traces of AI involvement in their code comments. Among these, some comments explicitly acknowledge both the use of generative AI and the presence of technical shortcomings. Analyzing 6,540 LLM-referencing code comments from public Python and JavaScript-based GitHub repositories (November 2022-July 2025), we identified 81 that also self-admit technical debt(SATD). Developers most often describe postponed testing, incomplete adaptation, and limited understanding of AI-generated code, suggesting that AI assistance affects both when and why technical debt emerges. We term GenAI-Induced Self-admitted Technical debt (GIST) as a proposed conceptual lens to describe recurring cases where developers incorporate AI-generated code while explicitly expressing uncertainty about its behavior or correctness.

</details>
