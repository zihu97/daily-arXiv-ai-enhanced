<div id=toc></div>

# Table of Contents

- [cs.SE](#cs.SE) [Total: 12]
- [cs.AR](#cs.AR) [Total: 1]
- [cs.NI](#cs.NI) [Total: 3]
- [cs.DC](#cs.DC) [Total: 4]
- [cs.MA](#cs.MA) [Total: 2]


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [1] [AI-Driven Development of a Publishing Imprint: Xynapse Traces](https://arxiv.org/abs/2510.23627)
*Fred Zimmerman*

Main category: cs.SE

TL;DR: Xynapse Traces 是一个结合人类与算法方法的实验性出版品牌，通过配置驱动架构和多模型AI集成框架，实现了出版周期缩短90%、成本降低80%，并在首年出版52本书，同时保持高质量标准。


<details>
  <summary>Details</summary>
Motivation: 传统出版流程耗时长、成本高，限制了小众市场的发展。该研究旨在探索人机协作的新范式，以提升出版效率、降低成本，并拓展出版的可及性。

Method: 采用配置驱动架构与多模型AI集成框架，构建涵盖创意生成、评估、生产到发行的全流程自动化系统，结合持续创意流水线、竞赛式评估机制、新型抄本设计、出版人角色建模，并辅以人工监督的自动验证机制。

Result: 系统将出版周期从6–12个月缩短至2–4周（减少90%），成本降低80%，首年成功出版52本书，引用准确率达99%，修正后验证成功率达100%。

Conclusion: Xynapse Traces 展示了人机协同在出版领域的巨大潜力，为高质量、高效率、低成本的出版提供了新范式，并有望推动小众出版市场的繁荣。

Abstract: Xynapse Traces is an experimental publishing imprint created via a fusion of
human and algorithmic methods using a configuration-driven architecture and a
multi-model AI integration framework. The system achieved a remarkable 90%
reduction in time-to-market (from a typical 6-12 months to just 2-4 weeks),
with 80% cost reduction compared to traditional imprint development, while
publishing 52 books in its first year and maintaining exceptional quality
metrics, including 99% citation accuracy and 100% validation success after
initial corrections. Key technical innovations include a continuous ideation
pipeline with tournament-style evaluation, a novel codex design for
transcriptive meditation practice, comprehensive automation spanning from
ideation through production and distribution, and publisher personas that
define and guide the imprint's mission. The system also integrates automated
verification with human oversight, ensuring that gains in speed do not
compromise publishing standards. This effort has significant implications for
the future of book publishing, suggesting new paradigms for human-AI
collaboration that democratize access to sophisticated publishing capabilities
and make previously unviable niche markets accessible.

</details>


### [2] [Agentsway -- Software Development Methodology for AI Agents-based Teams](https://arxiv.org/abs/2510.23664)
*Eranga Bandara,Ross Gore,Xueping Liang,Sachini Rajapakse,Isurunima Kularathne,Pramoda Karunarathna,Peter Foytik,Sachin Shetty,Ravi Mukkamala,Abdul Rahman,Amin Hass,Ng Wee Keong,Kasun De Zoysa,Aruna Withanage,Nilaan Loganathan*

Main category: cs.SE

TL;DR: Agentsway 是一种面向 AI 智能体协作的新型软件开发框架，将智能体作为核心参与者，引入结构化生命周期、角色分工与回顾式学习机制，以提升开发效率、可解释性与可信度。


<details>
  <summary>Details</summary>
Motivation: 传统软件开发方法（如敏捷、看板等）为人类团队设计，难以适应 AI 智能体参与规划、编码、测试和持续学习的新场景，亟需一种专为智能体协作设计的新方法论。

Method: 提出 Agentsway 框架，定义规划、提示、编码、测试和微调等专用智能体角色，通过人类协调、隐私保护协作以及基于微调大语言模型的回顾学习机制，实现整个开发周期的自适应优化。

Result: Agentsway 实现了面向智能体的协作结构化，增强了领域推理能力与决策可解释性，并通过多模型协同嵌入负责任 AI 原则，建立了可衡量的生产力与信任指标。

Conclusion: Agentsway 是首个专为 AI 智能体软件工程团队设计的方法论，为下一代 AI 原生、自进化的软件开发范式奠定了基础。

Abstract: The emergence of Agentic AI is fundamentally transforming how software is
designed, developed, and maintained. Traditional software development
methodologies such as Agile, Kanban, ShapeUp, etc, were originally designed for
human-centric teams and are increasingly inadequate in environments where
autonomous AI agents contribute to planning, coding, testing, and continuous
learning. To address this methodological gap, we present "Agentsway" a novel
software development framework designed for ecosystems where AI agents operate
as first-class collaborators. Agentsway introduces a structured lifecycle
centered on human orchestration, and privacy-preserving collaboration among
specialized AI agents. The framework defines distinct roles for planning,
prompting, coding, testing, and fine-tuning agents, each contributing to
iterative improvement and adaptive learning throughout the development process.
By integrating fine-tuned LLMs that leverage outputs and feedback from
different agents throughout the development cycle as part of a retrospective
learning process, Agentsway enhances domain-specific reasoning, and explainable
decision-making across the entire software development lifecycle. Responsible
AI principles are further embedded across the agents through the coordinated
use of multiple fine-tuned LLMs and advanced reasoning models, ensuring
balanced, transparent, and accountable decision-making. This work advances
software engineering by formalizing agent-centric collaboration, integrating
privacy-by-design principles, and defining measurable metrics for productivity
and trust. Agentsway represents a foundational step toward the next generation
of AI-native, self-improving software development methodologies. To the best of
our knowledge, this is the first research effort to introduce a dedicated
methodology explicitly designed for AI agent-based software engineering teams.

</details>


### [3] [RefleXGen:The unexamined code is not worth using](https://arxiv.org/abs/2510.23674)
*Bin Wang,Hui Li,AoFan Liu,BoTao Yang,Ao Yang,YiLu Zhong,Weixiang Huang,Yanping Zhang,Runhuai Huang,Weimin Zeng*

Main category: cs.SE

TL;DR: RefleXGen 提出了一种结合检索增强生成（RAG）与大语言模型自反思机制的新方法，在无需微调或专用数据集的情况下，显著提升了生成代码的安全性。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型在代码生成中的安全性仍面临重大挑战，而传统方法如微调模型或构建专用安全数据集成本高昂、资源密集。

Method: RefleXGen 通过引导大语言模型进行迭代式的自评估与自反思，结合 RAG 技术，在生成过程中不断优化并积累安全知识，从而提升代码安全性。

Result: 在多个主流模型上验证有效：GPT-3.5 Turbo 提升 13.6%，GPT-4o 提升 6.7%，CodeQwen 提升 4.5%，Gemini 提升 5.8%。

Conclusion: 提升模型自反思质量是一种高效且实用的策略，可显著增强 AI 生成代码的安全性。

Abstract: Security in code generation remains a pivotal challenge when applying large
language models (LLMs). This paper introduces RefleXGen, an innovative method
that significantly enhances code security by integrating Retrieval-Augmented
Generation (RAG) techniques with guided self-reflection mechanisms inherent in
LLMs. Unlike traditional approaches that rely on fine-tuning LLMs or developing
specialized secure code datasets - processes that can be resource-intensive -
RefleXGen iteratively optimizes the code generation process through
self-assessment and reflection without the need for extensive resources. Within
this framework, the model continuously accumulates and refines its knowledge
base, thereby progressively improving the security of the generated code.
Experimental results demonstrate that RefleXGen substantially enhances code
security across multiple models, achieving a 13.6% improvement with GPT-3.5
Turbo, a 6.7% improvement with GPT-4o, a 4.5% improvement with CodeQwen, and a
5.8% improvement with Gemini. Our findings highlight that improving the quality
of model self-reflection constitutes an effective and practical strategy for
strengthening the security of AI-generated code.

</details>


### [4] [Evaluating the effectiveness of LLM-based interoperability](https://arxiv.org/abs/2510.23893)
*Rodrigo Falcão,Stefan Schweitzer,Julien Siebert,Emily Calvet,Frank Elberzhager*

Main category: cs.SE

TL;DR: 该论文评估了13个开源大语言模型（LLMs）在农业互操作性场景下实现系统自主互操作的能力，发现qwen2.5-coder:32b在多数数据集版本中表现最佳，尤其在涉及单位转换时CODEGEN策略优于DIRECT策略。


<details>
  <summary>Details</summary>
Motivation: 系统之系统日益动态和异构，对互操作性提出更高要求；传统互操作方案需大量开发投入，而大语言模型（LLMs）的进展为实现运行时无需人工干预的自主互操作提供了新可能。

Method: 选取13个开源LLM，在农业互操作用例中构建四个版本的数据集，采用DIRECT和CODEGEN两种策略，对每个模型在每种数据集版本上运行三次，评估其效果和结果一致性。

Result: qwen2.5-coder:32b在三种数据集版本中使用两种策略均取得高成功率（DIRECT ≥0.99，CODEGEN ≥0.89）；在包含单位转换的第四种数据集中，DIRECT策略全部失败，而CODEGEN策略下该模型仍以0.75的平均pass@1成功。

Conclusion: 部分大语言模型能够实现系统间的自主互操作，建议在其他领域进一步验证，并开展提升可靠性的策略研究。

Abstract: Background: Systems of systems are becoming increasingly dynamic and
heterogeneous, and this adds pressure on the long-standing challenge of
interoperability. Besides its technical aspect, interoperability has also an
economic side, as development time efforts are required to build the
interoperability artifacts. Objectives: With the recent advances in the field
of large language models (LLMs), we aim at analyzing the effectiveness of
LLM-based strategies to make systems interoperate autonomously, at runtime,
without human intervention. Method: We selected 13 open source LLMs and curated
four versions of a dataset in the agricultural interoperability use case. We
performed three runs of each model with each version of the dataset, using two
different strategies. Then we compared the effectiveness of the models and the
consistency of their results across multiple runs. Results: qwen2.5-coder:32b
was the most effective model using both strategies DIRECT (average pass@1 >=
0.99) and CODEGEN (average pass@1 >= 0.89) in three out of four dataset
versions. In the fourth dataset version, which included an unit conversion, all
models using the strategy DIRECT failed, whereas using CODEGEN
qwen2.5-coder:32b succeeded with an average pass@1 = 0.75. Conclusion: Some
LLMs can make systems interoperate autonomously. Further evaluation in
different domains is recommended, and further research on reliability
strategies should be conducted.

</details>


### [5] [Validating Alerts in Cloud-Native Observability](https://arxiv.org/abs/2510.23970)
*Maria C. Borges,Julian Legler,Lucca Di Benedetto*

Main category: cs.SE

TL;DR: 本文为可观测性实验工具 OXN 引入了一种新的告警扩展，使工程师能在开发早期对告警规则进行实验、调优和验证，从而提升告警有效性并减少运行时问题。


<details>
  <summary>Details</summary>
Motivation: 设计有效的告警机制具有挑战性，需在早期发现问题与减少误报之间取得平衡；同时，由于告警代码很少执行，缺乏系统性验证，业界呼吁像对待应用代码一样严格测试告警代码，但缺少相应工具支持。

Method: 在可观测性实验工具 OXN 中引入新的告警扩展，允许工程师在开发阶段对告警规则进行实验、调优和定期验证其触发行为。

Result: 工程师能够在设计阶段就优化告警规则，并持续验证其行为，从而避免运行时出现告警失效或误报等问题。

Conclusion: 通过将告警代码纳入系统化实验与验证流程，该方法有效提升了告警的可靠性与实用性，填补了当前工具链的空白。

Abstract: Observability and alerting form the backbone of modern reliability
engineering. Alerts help teams catch faults early before they turn into
production outages and serve as first clues for troubleshooting. However,
designing effective alerts is challenging. They need to strike a fine balance
between catching issues early and minimizing false alarms. On top of this,
alerts often cover uncommon faults, so the code is rarely executed and
therefore rarely checked. To address these challenges, several industry
practitioners advocate for testing alerting code with the same rigor as
application code. Still, there's a lack of tools that support such systematic
design and validation of alerts.
  This paper introduces a new alerting extension for the observability
experimentation tool OXN. It lets engineers experiment with alerts early during
development. With OXN, engineers can now tune rules at design time and
routinely validate the firing behavior of their alerts, avoiding future
problems at runtime.

</details>


### [6] [Lifecycle-Aware code generation: Leveraging Software Engineering Phases in LLMs](https://arxiv.org/abs/2510.24019)
*Xing Xing,Wei Wang,Lipeng Ma,Weidong Yang,Junjie Zheng*

Main category: cs.SE

TL;DR: 本文提出了一种生命周期感知的代码生成框架，通过在训练和推理阶段引入需求分析、状态机建模和伪代码等中间产物，显著提升了代码生成的正确性和结构化水平。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型在代码生成中多采用从问题描述到代码的单步直接翻译方式，忽略了结构化的软件工程实践，限制了生成代码的质量和可维护性。

Method: 作者设计了一个生命周期感知框架，在训练和推理过程中系统性地引入软件开发各阶段的中间产物（如需求分析、状态机建模、伪代码），实现多阶段结构化推理。

Result: 实验表明，该方法在相同模型上通过生命周期级微调可将代码正确率提升高达75%；微调后的开源模型性能媲美甚至略超专门预训练的代码模型；在DeepSeek-Coder-1.3B上，CodeBLEU指标显著优于多个主流模型；且在训练数据减少80%时仍保持稳健性能。

Conclusion: 引入软件开发生命周期中的中间产物能有效提升大语言模型代码生成的质量与结构化程度，状态机建模对最终性能贡献最大，该框架具有高效、鲁棒和实用性强的特点。

Abstract: Recent progress in large language models (LLMs) has advanced automatic code
generation, yet most approaches rely on direct, single-step translation from
problem descriptions to code, disregarding structured software engineering
practices. We introduce a lifecycle-aware framework that systematically
incorporates intermediate artifacts such as requirements analysis, state
machine modeling, and pseudocode into both the training and inference stages.
This design aligns code generation with standard software development phases
and enables more structured reasoning. Experiments show that lifecycle-level
fine-tuning improves code correctness by up to 75% over the same model before
fine-tuning, with performance gains compounding across intermediate stages.
Multi-step inference consistently surpasses single-step generation,
demonstrating the effectiveness of intermediate scaffolding. Notably,
open-source LLMs, once fine-tuned under our framework, match or slightly
outperform models pretrained on code. When applied to DeepSeek-Coder-1.3B, our
framework yields relative CodeBLEU improvements of 34.3%, 20.0%, 11.2%, and
22.3% over ChatGPT-3.5, ChatGPT-4o-mini, DeepSeek-R1, and LLaMA-8B,
respectively. Our pipeline also proves robust with up to 80\% less training
data, confirming its resilience. Ablation studies further reveal that each
intermediate artifact contributes distinctly to final code quality, with state
machine modeling yielding the most substantial impact. Our source code and
detailed experimental data are available at
https://anonymous.4open.science/r/Lifecycle-Aware-3CCB.

</details>


### [7] [Monitoring and Observability of Machine Learning Systems: Current Practices and Gaps](https://arxiv.org/abs/2510.24142)
*Joran Leest,Ilias Gerostathopoulos,Patricia Lago,Claudia Raibulet*

Main category: cs.SE

TL;DR: 该研究通过七个跨领域的焦点小组会议，调查了机器学习（ML）系统在实际生产中的可观测性实践，总结了从业者捕获的信息类型及其用途，并指出了当前实践中的不足和对未来工具与研究的启示。


<details>
  <summary>Details</summary>
Motivation: 生产中的机器学习系统往往不会崩溃，而是悄无声息地做出错误决策，因此需要有效的可观测性手段；然而，目前缺乏关于从业者实际如何实现ML可观测性的实证证据。

Method: 通过在多个领域组织七场焦点小组会议，收集并分析从业者在ML系统及其环境中系统性捕获的信息，以及他们如何利用这些信息进行模型验证、故障检测与诊断、性能退化解释。

Result: 研究整理了实践中常用的可观测信息类别，揭示了其在模型验证、故障排查和性能解释中的具体应用，并识别出现有实践中的关键缺口。

Conclusion: 当前ML可观测性实践尚不完善，需在工具设计和研究方向上进一步发展，以建立更系统、有效的ML可观测性方法。

Abstract: Production machine learning (ML) systems fail silently -- not with crashes,
but through wrong decisions. While observability is recognized as critical for
ML operations, there is a lack empirical evidence of what practitioners
actually capture. This study presents empirical results on ML observability in
practice through seven focus group sessions in several domains. We catalog the
information practitioners systematically capture across ML systems and their
environment and map how they use it to validate models, detect and diagnose
faults, and explain observed degradations. Finally, we identify gaps in current
practice and outline implications for tooling design and research to establish
ML observability practices.

</details>


### [8] [Investigating Software Aging in LLM-Generated Software Systems](https://arxiv.org/abs/2510.24188)
*César Santos,Ermeson Andrade,Roberto Natella*

Main category: cs.SE

TL;DR: 本文通过50小时负载测试发现，由大语言模型自动生成的服务型应用普遍存在软件老化现象，表现为内存持续增长、响应时间增加和性能不稳定，强调需重视自动生成代码的长期可靠性。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型生成代码的广泛应用，其在长期运行下的可靠性问题尚未被充分研究，尤其是软件老化现象缺乏实证分析。

Method: 基于Bolt平台和Baxbench标准提示生成四个服务型应用，进行50小时负载测试，持续监控资源使用、响应时间和吞吐量，采用统计分析识别老化趋势。

Result: 所有测试应用均表现出显著的软件老化迹象，包括内存持续增长、响应时间延长和性能不稳定，且老化严重程度因应用类型而异。

Conclusion: 自动生成软件存在不可忽视的长期可靠性风险，需将软件老化纳入考量，并为未来制定缓解策略和评估方法奠定基础。

Abstract: Automatically generated software, especially code produced by Large Language
Models (LLMs), is increasingly adopted to accelerate development and reduce
manual effort. However, little is known about the long-term reliability of such
systems under sustained execution. In this paper, we experimentally investigate
the phenomenon of software aging in applications generated by LLM-based tools.
Using the Bolt platform and standardized prompts from Baxbench, we generated
four service-oriented applications and subjected them to 50-hour load tests.
Resource usage, response time, and throughput were continuously monitored to
detect degradation patterns. The results reveal significant evidence of
software aging, including progressive memory growth, increased response time,
and performance instability across all applications. Statistical analyzes
confirm these trends and highlight variability in the severity of aging
according to the type of application. Our findings show the need to consider
aging in automatically generated software and provide a foundation for future
studies on mitigation strategies and long-term reliability evaluation.

</details>


### [9] [Developer Productivity with GenAI](https://arxiv.org/abs/2510.24265)
*Sadia Afroz,Zixuan Feng,Katie Kimura,Bianca Trinkenreich,Igor Steinmacher,Anita Sarma*

Main category: cs.SE

TL;DR: 该研究调查了415名软件从业者，发现生成式AI工具的使用虽可能提升开发速度，但对整体生产力（如软件质量、开发者满意度等）的提升有限，体现了“生产力悖论”。


<details>
  <summary>Details</summary>
Motivation: 尽管生成式AI工具在软件开发中被广泛采用，但其是否真正提升开发者生产力尚不明确。因此，作者旨在探究GenAI对开发者生产力不同维度的实际影响。

Method: 通过问卷调查415名软件从业者，基于SPACE框架（满意度与幸福感、绩效、活动、沟通协作、效率与流畅感）评估其对AI辅助开发带来的生产力变化的感知，并按AI使用频率进行分组分析。

Result: 结果显示，总体生产力变化有限；高频使用AI的开发者虽在效率上有所提升，但在软件质量、满意度等方面未见显著改善，体现出“生产力悖论”。

Conclusion: 生成式AI工具虽能加快开发速度，但未必带来全面的生产力提升，开发者并未因此产出更高质量的软件或获得更高的职业满足感。

Abstract: Generative AI (GenAI) tools are increasingly being adopted in software
development as productivity aids. However, evidence regarding where and when
these tools actually enhance productivity is unclear. In this paper, we
investigate how GenAI adoption affects different dimensions of developer
productivity. We surveyed 415 software practitioners to capture their
perceptions of productivity changes associated with AI-assisted development
using the SPACE framework - Satisfaction and well-being, Performance, Activity,
Communication and collaboration, and Efficiency and flow. Our results,
disaggregated by frequency of AI usage, reveal limited overall productivity
change, highlighting the productivity paradox in which developers become faster
but do not necessarily create better software or feel more fulfilled.

</details>


### [10] [Automatically Benchmarking LLM Code Agents through Agent-Driven Annotation and Evaluation](https://arxiv.org/abs/2510.24358)
*Lingyue Fu,Bolun Zhang,Hao Guan,Yaoming Zhu,Lin Qiu,Weiwen Liu,Xuezhi Cao,Xunliang Cai,Weinan Zhang,Yong Yu*

Main category: cs.SE

TL;DR: 本文提出了PRDBench，一个基于真实Python项目的新型基准，通过结合人类监督与Agent-as-a-Judge范式，解决了现有代码智能体评估中高标注成本和僵化指标的问题。


<details>
  <summary>Details</summary>
Motivation: 现有代码智能体评估基准存在两大局限：一是标注成本高且需要专业知识，二是评估指标过于依赖单元测试，缺乏灵活性。

Method: 提出一种由智能体驱动的基准构建流程，结合人类监督生成多样且具挑战性的项目级任务，并引入Agent-as-a-Judge范式对智能体输出进行多维度评分。

Result: 构建了包含50个真实Python项目、覆盖20个领域的PRDBench基准，具备结构化产品需求文档、综合评估标准和参考实现，并通过大量实验证明其在评估代码智能体和评估智能体方面的有效性。

Conclusion: PRDBench为代码智能体的评估提供了一个可扩展、鲁棒且灵活的框架，显著提升了项目级软件开发自动化评估的可行性与实用性。

Abstract: Recent advances in code agents have enabled automated software development at
the project level, supported by large language models (LLMs) and widely adopted
tools. However, existing benchmarks for code agent evaluation face two major
limitations: high annotation cost and expertise requirements, and rigid
evaluation metrics that rely primarily on unit tests. To address these
challenges, we propose an agent-driven benchmark construction pipeline that
leverages human supervision to efficiently generate diverse and challenging
project-level tasks. Based on this approach, we introduce PRDBench, a novel
benchmark comprising 50 real-world Python projects across 20 domains, each with
structured Product Requirement Document (PRD) requirements, comprehensive
evaluation criteria, and reference implementations. PRDBench features rich data
sources, high task complexity, and flexible metrics. We further employ an
Agent-as-a-Judge paradigm to score agent outputs, enabling the evaluation of
various test types beyond unit tests. Extensive experiments on PRDBench
demonstrate its effectiveness in assessing the capabilities of both code agents
and evaluation agents, providing a scalable and robust framework for annotation
and evaluation.

</details>


### [11] [LLM-as-a-Judge for Software Engineering: Literature Review, Vision, and the Road Ahead](https://arxiv.org/abs/2510.24367)
*Junda He,Jieke Shi,Terry Yue Zhuo,Christoph Treude,Jiamou Sun,Zhenchang Xing,Xiaoning Du,David Lo*

Main category: cs.SE

TL;DR: 本文探讨了在软件工程中利用大语言模型（LLM）作为评估者（LLM-as-a-Judge）来自动评价LLM生成的软件制品，指出当前研究的不足，并提出到2030年实现可靠、鲁棒且可扩展评估框架的路线图。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型在软件工程中的广泛应用，产生了大量代码等软件制品，但缺乏可扩展且可靠的评估方法。人工评估成本高，传统自动指标（如BLEU）无法捕捉质量细节，因此亟需更智能的自动化评估手段。

Method: 作者对现有软件工程领域中LLM-as-a-Judge相关研究进行文献综述，分析其局限性，识别关键研究空白，并提出未来发展的详细路线图。

Result: 明确了当前LLM-as-a-Judge在软件工程评估中的研究现状与挑战，为构建具备人类评估水平的自动化评估框架提供了方向。

Conclusion: LLM-as-a-Judge有望成为可靠、鲁棒和可扩展的人类评估替代方案，推动软件制品评估的自动化与规模化，作者呼吁社区加强相关研究与应用。

Abstract: The rapid integration of Large Language Models (LLMs) into software
engineering (SE) has revolutionized tasks like code generation, producing a
massive volume of software artifacts. This surge has exposed a critical
bottleneck: the lack of scalable, reliable methods to evaluate these outputs.
Human evaluation is costly and time-consuming, while traditional automated
metrics like BLEU fail to capture nuanced quality aspects. In response, the
LLM-as-a-Judge paradigm - using LLMs for automated evaluation - has emerged.
This approach leverages the advanced reasoning of LLMs, offering a path toward
human-like nuance at automated scale. However, LLM-as-a-Judge research in SE is
still in its early stages. This forward-looking SE 2030 paper aims to steer the
community toward advancing LLM-as-a-Judge for evaluating LLM-generated software
artifacts. We provide a literature review of existing SE studies, analyze their
limitations, identify key research gaps, and outline a detailed roadmap. We
envision these frameworks as reliable, robust, and scalable human surrogates
capable of consistent, multi-faceted artifact evaluation by 2030. Our work aims
to foster research and adoption of LLM-as-a-Judge frameworks, ultimately
improving the scalability of software artifact evaluation.

</details>


### [12] [The Divine Software Engineering Comedy -- Inferno: The Okinawa Files](https://arxiv.org/abs/2510.24483)
*Michele Lanza*

Main category: cs.SE

TL;DR: 本文以讽刺和悲观的视角，总结了2024年6月在日本冲绳举办的“软件工程的未来”（FUSE）研讨会中的核心讨论，提出软件工程正面临三大“噩梦”：从业者缺乏真正理解却仍交付软件、领域发展过快导致遗忘过往经验、以及技术过度快速繁衍。


<details>
  <summary>Details</summary>
Motivation: 作者参与组织FUSE研讨会后，对软件工程未来的发展方向感到忧虑，希望通过带有讽刺和批判性的反思，揭示当前领域中存在的深层问题。

Method: 基于FUSE研讨会中的观察与讨论，结合个人见解，采用讽刺与批判性散文的形式，提炼出软件工程面临的三大挑战。

Result: 识别出软件工程未来发展的三大“噩梦”：1）缺乏专业理解但能完成任务的开发者；2）发展过快而忽视历史教训的领域；3）技术无节制地快速扩张。

Conclusion: 软件工程的未来如同一场缓慢发生的车祸，虽可预见却难以回避，亟需对当前趋势进行深刻反思与调整。

Abstract: In June 2024 I co-organized the FUture of Software Engineering symposium in
Okinawa, Japan. Me, Andrian Marcus, Takashi Kobayashi and Shinpei Hayashi were
general chairs, Nicole Novielli, Kevin Moran, Yutaro Kashiwa and Masanari Kondo
were program chairs, some members of my group, Carmen Armenti, Stefano
Campanella, Roberto Minelli, were the tables, can't have a room with only
chairs, after all. We invited a crowd of people to discuss what future software
engineering has. FUSE became a 3-day marathon on whether there is actually a
future at all for SE. This essay is a slightly dark take about what I saw at
that event, very loosely based on the discussions that took place, adding some
healthy sarcasm and cynicism, the intellectual salt and pepper I never seem to
run out of. I listened to the brilliant people who gathered to talk about where
we're headed, and distilled three nightmares headed in our direction: software
makers who don't know what they're doing, but get the job done anyway, a field
moving so fast it can't remember its own lessons, and technologies multiplying
like rabbits in Spring. So, let's start. The future, eh? The future of software
engineering looks like a car crash in slow motion: you can see it coming but
you can't look away. The thing is...

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [13] [SlowPoke: Understanding and Detecting On-Chip Fail-Slow Failures in Many-Core Systems](https://arxiv.org/abs/2510.24112)
*Junchi Wu,Xinfei Wan,Zhuoran Li,Yuyang Jin,Guangyu Sun,Yun Liang,Diyu Zhou,Youwei Zhuo*

Main category: cs.AR

TL;DR: 本文提出了SlowPoke，一种轻量级、硬件感知的片上慢速故障检测框架，通过编译器插桩、实时追踪压缩和拓扑感知排序算法，在极低内存开销下实现高准确率的故障检测。


<details>
  <summary>Details</summary>
Motivation: 多核架构中普遍存在“慢速故障”（fail-slow）问题，严重影响性能；现有分布式系统中的检测方法因内存限制和无法追踪硬件拓扑结构而不适用于片上环境。

Method: SlowPoke结合了基于编译器的低开销监控、千字节级内存内运行的实时追踪压缩技术，以及一种新颖的拓扑感知根因排序算法。

Result: 实验表明，SlowPoke平均将检测追踪的存储开销降低115.9倍，平均检测准确率达86.77%，误报率为12.11%，并在多种多核架构上具有良好可扩展性。

Conclusion: SlowPoke是一种实用且可扩展的片上慢速故障检测方案，适用于大规模多核系统部署。

Abstract: Many-core architectures are essential for high-performance computing, but
their performance is undermined by widespread fail-slow failures. Detecting
such failures on-chip is challenging, as prior methods from distributed systems
are unsuitable due to strict memory limits and their inability to track
failures across the hardware topology. This paper introduces SlowPoke, a
lightweight, hardware-aware framework for practical on-chip fail-slow
detection. SlowPoke combines compiler-based instrumentation for low-overhead
monitoring, on-the-fly trace compression to operate within kilobytes of memory,
and a novel topology-aware ranking algorithm to pinpoint a failure's root
cause. We evaluate SlowPoke on a wide range of representative many-core
workloads, and the results demonstrate that SlowPoke reduces the storage
overhead of detection traces by an average of 115.9$\times$, while achieving an
average fail-slow detection accuracy of 86.77% and a false positive rate (FPR)
of 12.11%. More importantly, SlowPoke scales effectively across different
many-core architectures, making it practical for large-scale deployments.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [14] [Enabling Near-realtime Remote Sensing via Satellite-Ground Collaboration of Large Vision-Language Models](https://arxiv.org/abs/2510.24242)
*Zihan Li,Jiahao Yang,Yuxin Zhang,Zhe Chen,Yue Gao*

Main category: cs.NI

TL;DR: Grace 是一个星地协同系统，通过在卫星上部署轻量级大视觉语言模型（LVLM），在地面站部署更大模型，并结合异步检索增强生成（RAG）与任务调度算法，在保证推理精度的同时，将遥感任务的平均延迟降低76–95%。


<details>
  <summary>Details</summary>
Motivation: 现有大型视觉语言模型（LVLM）在低轨卫星遥感任务中展现出潜力，但受限于星上计算资源有限和星地通信窗口短暂，难以在真实卫星系统中部署。

Method: 提出 Grace 系统，包含两个核心机制：1）在有限星地通信期内，通过自适应更新算法将地面站 RAG 知识库同步至卫星；2）设计基于置信度的测试算法，动态决定任务是在星上处理还是卸载至地面站。

Result: 基于真实卫星轨道数据的实验表明，Grace 相比当前最优方法，在不损失推理准确率的前提下，将平均延迟降低了 76–95%。

Conclusion: Grace 有效解决了 LVLM 在低轨卫星系统中部署的延迟与资源限制问题，为近实时遥感任务提供了可行的星地协同推理框架。

Abstract: Large vision-language models (LVLMs) have recently demonstrated great
potential in remote sensing (RS) tasks (e.g., disaster monitoring) conducted by
low Earth orbit (LEO) satellites. However, their deployment in real-world LEO
satellite systems remains largely unexplored, hindered by limited onboard
computing resources and brief satellite-ground contacts. We propose Grace, a
satellite-ground collaborative system designed for near-realtime LVLM inference
in RS tasks. Accordingly, we deploy compact LVLM on satellites for realtime
inference, but larger ones on ground stations (GSs) to guarantee end-to-end
performance. Grace is comprised of two main phases that are asynchronous
satellite-GS Retrieval-Augmented Generation (RAG), and a task dispatch
algorithm. Firstly, we still the knowledge archive of GS RAG to satellite
archive with tailored adaptive update algorithm during limited satellite-ground
data exchange period. Secondly, propose a confidence-based test algorithm that
either processes the task onboard the satellite or offloads it to the GS.
Extensive experiments based on real-world satellite orbital data show that
Grace reduces the average latency by 76-95% compared to state-of-the-art
methods, without compromising inference accuracy.

</details>


### [15] [A New Hybrid Precoding Approach for Multi-user Massive MIMO over Fading Channels](https://arxiv.org/abs/2510.24595)
*Azadeh Pourkabirian,Kai Li,Photios A. Stavrou,Wei Ni*

Main category: cs.NI

TL;DR: 本文提出了一种新的混合预编码方法，通过将角度与相位建模为服从二元高斯分布的关联变量，并首次定义了联合角度-相位熵，以提升MU-MIMO系统的和速率与鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有混合预编码方法未充分考虑无线信道中角度与相位之间的内在相关性，限制了系统性能的进一步提升。

Method: 将角度与相位建模为服从二元高斯分布的关联变量，定义联合角度-相位熵以刻画其不确定性，并据此优化混合预编码设计。

Result: 仿真结果表明，所提方法相比现有先进方法，和速率提升了18.31%，鲁棒性提高了11.47%。

Conclusion: 通过建模角度与相位的相关性并引入联合熵度量，所提出的混合预编码方法能有效提升MU-MIMO系统的性能与适应性。

Abstract: Hybrid precoding is an indispensable technique to harness the full potential
of a multi-user massive multiple-input, multiple-output (MU-MMIMO) system. In
this paper, we propose a new hybrid precoding approach that combines digital
and analog precoding to optimize data transmission over multiple antennas. This
approach steers signals in specific directions, leading to maximizing sum-rate
and suppressing side-lobe interference. When dealing with complex signals,
changes in phase are naturally associated with changes in angle, and these
variations are inherently correlated. The correlation between the angle and
phase is essential for accurately determining the channel characteristics. An
important aspect of this approach is that we model the angle and phase as
correlated variables following a bivariate Gaussian distribution, and for the
first time, we define a joint angle and phase entropy to measure the
uncertainty of angle and phase variations in wireless channels. This entropy is
crucial to adapt the proposed precoding method with variations. Simulation
result validate the accuracy of our analytical findings, demonstrating 18.31%
increase in sum-rate and an 11.47% improvement in robustness compared to other
state-of-the-art methods.

</details>


### [16] [Strategic Task Offloading for Delay-Sensitive IoT Applications: A Game-Theory-Based Demand-Supply Mechanism with Participation Incentives](https://arxiv.org/abs/2510.24611)
*Azadeh Pourkabirian,Amir Masoud Rahmani,Kai Li,Wei Ni*

Main category: cs.NI

TL;DR: 本文提出了一种基于经济供需模型和VCG拍卖机制的任务卸载方法，用于在边缘计算环境中优化延迟敏感型物联网应用的资源分配，实现市场均衡、真实性和社会福利最大化。


<details>
  <summary>Details</summary>
Motivation: 由于物联网设备计算资源有限且需满足实时响应需求，传统任务卸载方案难以有效平衡资源供需，因此需要一种能兼顾用户延迟需求与边缘服务器资源供给的新机制。

Method: 将任务卸载问题建模为经济供需模型，并设计基于Vickrey-Clarke-Groves（VCG）拍卖的博弈论框架，结合激励机制协调用户与边缘服务器之间的资源分配与参与意愿。

Result: 仿真结果表明，该方法能够最大化社会福利、保证参与者的真实性、维持市场供需平衡，并为延迟敏感型物联网应用提供延迟保障。

Conclusion: 所提出的任务卸载方法通过经济模型与博弈机制有效解决了边缘计算中资源供需不匹配的问题，提升了系统整体性能与参与方收益。

Abstract: Delay-sensitive Internet of Things (IoT) applications have drawn significant
attention. Running many of these applications on IoT devices is challenging due
to the limited processing resources of these devices and the need for real-time
responses. Task offloading can minimize latency by transferring computationally
intensive tasks from IoT devices to resource-rich edge servers, ensuring delay
and performance guarantees. In this paper, we develop a task-offloading
approach for delay-sensitive IoT applications in edge computing environments.
Unlike existing schemes, we model the task offloading problem as an economic
demand and supply model to achieve market balance. The proposed model avoids
under- and over-supply, ensuring the computational resources at edge servers
(supply) are allocated in a manner that best meets the processing and
computational needs of user devices (demand). Given the multi-agent nature of
task offloading involving users and service providers with different
preferences and objectives, we design a game-theoretic framework using a
Vickrey-Clarke-Groves (VCG) auction. This framework analyzes agent interactions
and decision-making processes. Additionally, we develop an incentive mechanism
to encourage both parties to participate in the auction. The mechanism
maximizes user task offloading to edge servers and motivates edge servers to
share their computational resources, achieving profitability for both IoT users
and edge servers. Simulations demonstrate our method maximizes social welfare,
ensures truthfulness, maintains market balance, and provides latency guarantees
for delay-sensitive IoT applications.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [17] [The SAP Cloud Infrastructure Dataset: A Reality Check of Scheduling and Placement of VMs in Cloud Computing](https://arxiv.org/abs/2510.23911)
*Arno Uhlig,Iris Braun,Matthias Wählisch*

Main category: cs.DC

TL;DR: 该论文基于SAP云平台中约1800台物理机和48000个虚拟机的30天观测数据，揭示了当前虚拟机调度与放置中的多种资源利用低效问题，并提出了改进调度算法的设计需求，同时公开了该细粒度企业级数据集以支持后续研究。


<details>
  <summary>Details</summary>
Motivation: 在分布式环境中高效分配资源是一项基本挑战。现有研究缺乏来自真实企业级云平台、涵盖长期运行和内存密集型工作负载的细粒度时序遥测数据，限制了调度算法的评估与优化。

Method: 作者利用SAP云平台的可观测性工具收集了1800台hypervisor和48000个VM在30天内的资源使用与性能指标数据，分析其中的调度与放置问题，并基于发现提炼出新调度算法的设计需求。

Result: 研究发现多个次优调度现象：CPU资源争用超过40%、CPU就绪时间高达220秒、主机间CPU利用率极度不均衡（最高达99%）、超过80%的VM使用不到70%分配的CPU和内存资源。

Conclusion: 论文揭示了企业云环境中资源调度的显著优化空间，提出了面向新型调度与放置算法的关键需求，并公开了真实世界的大规模细粒度数据集，以推动未来云资源管理研究。

Abstract: Allocating resources in a distributed environment is a fundamental challenge.
In this paper, we analyze the scheduling and placement of virtual machines
(VMs) in the cloud platform of SAP, the world's largest enterprise resource
planning software vendor. Based on data from roughly 1,800 hypervisors and
48,000 VMs within a 30-day observation period, we highlight potential
improvements for workload management. The data was measured through
observability tooling that tracks resource usage and performance metrics across
the entire infrastructure. In contrast to existing datasets, ours uniquely
offers fine-grained time-series telemetry data of fully virtualized
enterprise-level workloads from both long-running and memory-intensive SAP
S/4HANA and diverse, general-purpose applications. Our key findings include
several suboptimal scheduling situations, such as CPU resource contention
exceeding 40%, CPU ready times of up to 220 seconds, significantly imbalanced
compute hosts with a maximum CPU~utilization on intra-building block hosts of
up to 99%, and overprovisioned CPU and memory resources resulting into over 80%
of VMs using less than 70% of the provided resources. Bolstered by these
findings, we derive requirements for the design and implementation of novel
placement and scheduling algorithms and provide guidance to optimize resource
allocations. We make the full dataset used in this study publicly available to
enable data-driven evaluations of scheduling approaches for large-scale cloud
infrastructures in future research.

</details>


### [18] [A GPU-based Compressible Combustion Solver for Applications Exhibiting Disparate Space and Time Scales](https://arxiv.org/abs/2510.23993)
*Anthony Carreon,Jagmohan Singh,Shivank Sharma,Shuzhi Zhang,Venkat Raman*

Main category: cs.DC

TL;DR: 本文提出了一种基于AMReX框架、面向多GPU优化的高性能可压缩反应流求解器，通过改进内存访问模式、采用批量稀疏积分策略处理化学反应，并优化多GPU负载均衡，在典型燃烧模拟中实现了2–5倍的性能提升，并展现出接近理想的弱扩展性。


<details>
  <summary>Details</summary>
Motivation: 高速化学反应流的模拟因时空尺度差异大、化学反应刚性强而计算成本高昂；现有基于GPU的可压缩燃烧求解器在内存管理、负载均衡及处理局部化化学反应方面存在瓶颈。

Method: 该求解器基于AMReX框架，针对GPU性能瓶颈进行了三项优化：采用列主存储优化内存访问、通过批量稀疏积分策略处理化学动力学以应对计算负载不均、并在自适应网格细化（AMR）应用中实现多GPU负载均衡；同时将基于矩阵的化学动力学方法适配到多重网格环境中。

Result: 在氢气-空气爆轰和超音速横向射流等典型燃烧案例中，相比初始GPU实现获得2–5倍性能提升，在1–96块NVIDIA H100 GPU上实现接近理想的弱扩展性；屋顶线分析显示对流和化学反应部分的算术强度分别提升约10倍和4倍。

Conclusion: 所提出的GPU优化策略显著提升了可压缩反应流模拟的计算效率和可扩展性，有效利用了GPU的内存带宽与计算资源，为大规模燃烧模拟提供了高效工具。

Abstract: High-speed chemically active flows present significant computational
challenges due to their disparate space and time scales, where stiff chemistry
often dominates simulation time. While modern supercomputing scientific codes
achieve exascale performance by leveraging graphics processing units (GPUs),
existing GPU-based compressible combustion solvers face critical limitations in
memory management, load balancing, and handling the highly localized nature of
chemical reactions. To this end, we present a high-performance compressible
reacting flow solver built on the AMReX framework and optimized for multi-GPU
settings. Our approach addresses three GPU performance bottlenecks: memory
access patterns through column-major storage optimization, computational
workload variability via a bulk-sparse integration strategy for chemical
kinetics, and multi-GPU load distribution for adaptive mesh refinement
applications. The solver adapts existing matrix-based chemical kinetics
formulations to multigrid contexts. Using representative combustion
applications including hydrogen-air detonations and jet in supersonic crossflow
configurations, we demonstrate $2-5\times$ performance improvements over
initial GPU implementations with near-ideal weak scaling across $1-96$ NVIDIA
H100 GPUs. Roofline analysis reveals substantial improvements in arithmetic
intensity for both convection ($\sim 10 \times$) and chemistry ($\sim 4
\times$) routines, confirming efficient utilization of GPU memory bandwidth and
computational resources.

</details>


### [19] [Towards Exascale Computing for Astrophysical Simulation Leveraging the Leonardo EuroHPC System](https://arxiv.org/abs/2510.24175)
*Nitin Shukla,Alessandro Romeo,Caterina Caravita,Michael Redenti,Radim Vavrik,Lubomir Riha,Andrea Mignone,Marco Rossazza,Stefano Truzzi,Luca Tornatore,Antonio Ragagnin,Tiago Castro,Geray S. Karademir,Klaus Dolag,Pranab J. Deka,Fabio Bacchini,Rostislav-Paul Wilhelm,Daniele Gregori,Elisabetta Boella*

Main category: cs.DC

TL;DR: 该论文介绍了SPACE-CoE在Leonardo系统上对gPLUTO、OpenGadget3和iPIC3D三个天体物理与等离子体模拟代码进行性能优化的策略与初步成果，展示了这些代码在多达1,024个GPU上达到80%的高效扩展性。


<details>
  <summary>Details</summary>
Motivation: 为应对在现有及下一代加速器上开发和重构天体物理、宇宙学和空间等离子体数值模拟代码的挑战，推动大规模模拟在exascale超算时代的实现。

Method: 通过SPACE-CoE联合科学家、代码开发者与高性能计算专家，利用性能分析工具在单节点和多节点上对三个旗舰代码（gPLUTO、OpenGadget3、iPIC3D）在CINECA的Leonardo系统上进行优化与测试。

Result: 初步测试表明，三个代码均表现出良好的扩展性，在多达1,024个GPU上可实现高达80%的并行效率。

Conclusion: 该工作验证了在exascale系统上高效运行复杂天体物理与等离子体模拟代码的可行性，为未来大规模科学模拟奠定了基础。

Abstract: Developing and redesigning astrophysical, cosmological, and space plasma
numerical codes for existing and next-generation accelerators is critical for
enabling large-scale simulations. To address these challenges, the SPACE Center
of Excellence (SPACE-CoE) fosters collaboration between scientists, code
developers, and high-performance computing experts to optimize applications for
the exascale era. This paper presents our strategy and initial results on the
Leonardo system at CINECA for three flagship codes, namely gPLUTO, OpenGadget3
and iPIC3D, using profiling tools to analyze performance on single and multiple
nodes. Preliminary tests show all three codes scale efficiently, reaching 80%
scalability up to 1,024 GPUs.

</details>


### [20] [CoMPSeT: A Framework for Comparing Multiparty Session Types](https://arxiv.org/abs/2510.24205)
*Telmo Ribeiro,José Proença,Mário Florido*

Main category: cs.DC

TL;DR: 本文提出了一个名为 CoMPSeT 的开源工具，用于分析和比较多种多参与方会话类型（MPST）的特性，支持在浏览器中直接运行，便于研究人员和教师理解和教学全局编排协议。


<details>
  <summary>Details</summary>
Motivation: 多参与方会话类型（MPST）存在众多变体，各自具有特定特性和差异，导致理解和比较困难，因此需要一个统一工具来清晰展示和对比这些特性。

Method: 作者选取了一组具有代表性的 MPST 示例，设计并实现了 CoMPSeT 工具，该工具支持组合不同特性、动画演示及语义比较，并以 JavaScript 编译，可在浏览器中运行。

Result: CoMPSeT 能有效支持对不同 MPST 特性的组合、动画化和语义比较，为研究人员和教育者提供了直观理解全局编排语言的手段。

Conclusion: CoMPSeT 作为一个开源、易访问的工具，有助于厘清 MPST 的多样化特性，促进相关研究与教学。

Abstract: Concurrent systems are often complex and difficult to design. Choreographic
languages, such as Multiparty Session Types (MPST), allow the description of
global protocols of interactions by capturing valid patterns of interactions
between participants. Many variations of MPST exist, each one with its rather
specific features and idiosyncrasies. Here we propose a tool (CoMPSeT) that
provides clearer insights over different features in existing MPST. We select a
representative set of MPST examples and provide mechanisms to combine different
features and to animate and compare the semantics of concrete examples. CoMPSeT
is open-source, compiled into JavaScript, and can be directly executed from any
browser, becoming useful both for researchers who want to better understand the
landscape of MPST and for teachers who want to explain global choreographies.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [21] [Coordinated Autonomous Drones for Human-Centered Fire Evacuation in Partially Observable Urban Environments](https://arxiv.org/abs/2510.23899)
*Maria G. Mendoza,Addison Kalanther,Daniel Bostwick,Emma Stephan,Chinmay Maheshwari,Shankar Sastry*

Main category: cs.MA

TL;DR: 本文提出一种多智能体协同框架，利用两类自主无人机（UAV）在火灾等紧急疏散场景中实时引导受困人员，通过结合基于心理学的人类行为建模与POMDP决策模型，显著缩短疏散时间。


<details>
  <summary>Details</summary>
Motivation: 现有疏散辅助模型常忽略极端压力下人类心理与情绪的复杂性，导致在真实火灾场景中，受困者因恐慌和不确定性偏离安全路径；同时，无人机在动态实时疏散支持中的应用仍有限。

Method: 将问题建模为部分可观测马尔可夫决策过程（POMDP），由高阶救援无人机（HLR）与低阶救援无人机（LLR）通过共享观测与互补能力协同工作；人类行为采用基于实证心理学的智能体模型，恐慌程度动态影响其决策；使用带循环策略的近端策略优化（PPO）算法进行鲁棒决策。

Result: 仿真结果表明，该无人机团队能快速定位并拦截受困人员，显著缩短其抵达安全区域所需时间，优于无无人机辅助的场景。

Conclusion: 所提出的多智能体无人机协同框架能有效应对火灾疏散中人类行为的不确定性和环境动态性，为提升应急响应效率提供了可行方案。

Abstract: Autonomous drone technology holds significant promise for enhancing search
and rescue operations during evacuations by guiding humans toward safety and
supporting broader emergency response efforts. However, their application in
dynamic, real-time evacuation support remains limited. Existing models often
overlook the psychological and emotional complexity of human behavior under
extreme stress. In real-world fire scenarios, evacuees frequently deviate from
designated safe routes due to panic and uncertainty. To address these
challenges, this paper presents a multi-agent coordination framework in which
autonomous Unmanned Aerial Vehicles (UAVs) assist human evacuees in real-time
by locating, intercepting, and guiding them to safety under uncertain
conditions. We model the problem as a Partially Observable Markov Decision
Process (POMDP), where two heterogeneous UAV agents, a high-level rescuer (HLR)
and a low-level rescuer (LLR), coordinate through shared observations and
complementary capabilities. Human behavior is captured using an agent-based
model grounded in empirical psychology, where panic dynamically affects
decision-making and movement in response to environmental stimuli. The
environment features stochastic fire spread, unknown evacuee locations, and
limited visibility, requiring UAVs to plan over long horizons to search for
humans and adapt in real-time. Our framework employs the Proximal Policy
Optimization (PPO) algorithm with recurrent policies to enable robust
decision-making in partially observable settings. Simulation results
demonstrate that the UAV team can rapidly locate and intercept evacuees,
significantly reducing the time required for them to reach safety compared to
scenarios without UAV assistance.

</details>


### [22] [Human Machine Social Hybrid Intelligence:A Collaborative Decision Making Framework for Large Model Agent Groups and Human Experts](https://arxiv.org/abs/2510.24030)
*Ahmet Akkaya Melih,Yamuna Singh,Kunal L. Agarwal,Priya Mukherjee,Kiran Pattnaik,Hanuman Bhatia*

Main category: cs.MA

TL;DR: 本文提出了一种名为“人机社会混合智能”（HMS-HI）的新框架，通过共享认知空间、动态角色任务分配和跨物种信任校准三大模块，显著提升复杂高风险环境中人机协作的决策质量与效率。


<details>
  <summary>Details</summary>
Motivation: 当前的人在环路（HiTL）范式未能有效整合人类专家知识，在复杂高风险环境中易导致认知过载和决策瓶颈。

Method: 构建HMS-HI框架，包含：(1) 共享认知空间（SCS）实现多模态态势感知与结构化世界建模；(2) 动态角色与任务分配（DRTA）模块根据能力与负荷自适应分配任务；(3) 跨物种信任校准（CSTC）协议通过可解释声明与结构化反馈增强透明度与互适应性。

Result: 在高保真城市应急响应模拟中，相比传统HiTL方法，HMS-HI将平民伤亡减少72%，认知负荷降低70%，并显著提升决策质量、效率与人机信任；消融实验验证了各模块的关键作用。

Conclusion: 工程化的信任机制与共享上下文是实现可扩展、协同性人机协作的基础，HMS-HI为复杂场景下的人机深度融合提供了有效架构。

Abstract: The rapid advancements in large foundation models and multi-agent systems
offer unprecedented capabilities, yet current Human-in-the-Loop (HiTL)
paradigms inadequately integrate human expertise, often leading to cognitive
overload and decision-making bottlenecks in complex, high-stakes environments.
We propose the "Human-Machine Social Hybrid Intelligence" (HMS-HI) framework, a
novel architecture designed for deep, collaborative decision-making between
groups of human experts and LLM-powered AI agents. HMS-HI is built upon three
core pillars: (1) a \textbf{Shared Cognitive Space (SCS)} for unified,
multi-modal situational awareness and structured world modeling; (2) a
\textbf{Dynamic Role and Task Allocation (DRTA)} module that adaptively assigns
tasks to the most suitable agent (human or AI) based on capabilities and
workload; and (3) a \textbf{Cross-Species Trust Calibration (CSTC)} protocol
that fosters transparency, accountability, and mutual adaptation through
explainable declarations and structured feedback. Validated in a high-fidelity
urban emergency response simulation, HMS-HI significantly reduced civilian
casualties by 72\% and cognitive load by 70\% compared to traditional HiTL
approaches, demonstrating superior decision quality, efficiency, and human-AI
trust. An ablation study confirms the critical contribution of each module,
highlighting that engineered trust and shared context are foundational for
scalable, synergistic human-AI collaboration.

</details>
