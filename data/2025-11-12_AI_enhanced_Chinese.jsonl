{"id": "2511.07421", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2511.07421", "abs": "https://arxiv.org/abs/2511.07421", "authors": ["Tong Qiao", "Ao Zhou", "Yingjie Qi", "Yiou Wang", "Han Wan", "Jianlei Yang", "Chunming Hu"], "title": "Towards Affordable, Adaptive and Automatic GNN Training on CPU-GPU Heterogeneous Platforms", "comment": "Accepted by The 43rd IEEE International Conference on Computer Design, ICCD'25", "summary": "Graph Neural Networks (GNNs) have been widely adopted due to their strong performance. However, GNN training often relies on expensive, high-performance computing platforms, limiting accessibility for many tasks. Profiling of representative GNN workloads indicates that substantial efficiency gains are possible on resource-constrained devices by fully exploiting available resources. This paper introduces A3GNN, a framework for affordable, adaptive, and automatic GNN training on heterogeneous CPU-GPU platforms. It improves resource usage through locality-aware sampling and fine-grained parallelism scheduling. Moreover, it leverages reinforcement learning to explore the design space and achieve pareto-optimal trade-offs among throughput, memory footprint, and accuracy. Experiments show that A3GNN can bridge the performance gap, allowing seven Nvidia 2080Ti GPUs to outperform two A100 GPUs by up to 1.8X in throughput with minimal accuracy loss.", "AI": {"tldr": "A3GNN \u662f\u4e00\u4e2a\u9762\u5411\u5f02\u6784 CPU-GPU \u5e73\u53f0\u7684\u9ad8\u6548 GNN \u8bad\u7ec3\u6846\u67b6\uff0c\u901a\u8fc7\u5c40\u90e8\u6027\u611f\u77e5\u91c7\u6837\u3001\u7ec6\u7c92\u5ea6\u5e76\u884c\u8c03\u5ea6\u548c\u5f3a\u5316\u5b66\u4e60\u4f18\u5316\uff0c\u5728\u8d44\u6e90\u53d7\u9650\u8bbe\u5907\u4e0a\u5b9e\u73b0\u63a5\u8fd1\u9ad8\u7aef\u786c\u4ef6\u7684\u6027\u80fd\u3002", "motivation": "GNN \u8bad\u7ec3\u901a\u5e38\u4f9d\u8d56\u6602\u8d35\u7684\u9ad8\u6027\u80fd\u8ba1\u7b97\u5e73\u53f0\uff0c\u9650\u5236\u4e86\u5176\u5728\u8d44\u6e90\u53d7\u9650\u573a\u666f\u4e0b\u7684\u5e94\u7528\uff1b\u73b0\u6709\u5de5\u4f5c\u8d1f\u8f7d\u5206\u6790\u8868\u660e\uff0c\u901a\u8fc7\u5145\u5206\u5229\u7528\u53ef\u7528\u8d44\u6e90\u53ef\u5728\u4f4e\u7aef\u8bbe\u5907\u4e0a\u663e\u8457\u63d0\u5347\u6548\u7387\u3002", "method": "\u63d0\u51fa A3GNN \u6846\u67b6\uff0c\u7ed3\u5408\u5c40\u90e8\u6027\u611f\u77e5\u91c7\u6837\u3001\u7ec6\u7c92\u5ea6\u5e76\u884c\u8c03\u5ea6\uff0c\u5e76\u5229\u7528\u5f3a\u5316\u5b66\u4e60\u5728\u541e\u5410\u91cf\u3001\u5185\u5b58\u5360\u7528\u548c\u7cbe\u5ea6\u4e4b\u95f4\u63a2\u7d22\u5e15\u7d2f\u6258\u6700\u4f18\u6743\u8861\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cA3GNN \u80fd\u4f7f 7 \u5757 Nvidia 2080Ti GPU \u5728\u541e\u5410\u91cf\u4e0a\u6bd4 2 \u5757 A100 GPU \u63d0\u5347\u6700\u591a 1.8 \u500d\uff0c\u540c\u65f6\u4fdd\u6301\u7cbe\u5ea6\u635f\u5931\u6781\u5c0f\u3002", "conclusion": "A3GNN \u6709\u6548\u7f29\u5c0f\u4e86\u8d44\u6e90\u53d7\u9650\u8bbe\u5907\u4e0e\u9ad8\u7aef\u786c\u4ef6\u4e4b\u95f4\u7684 GNN \u8bad\u7ec3\u6027\u80fd\u5dee\u8ddd\uff0c\u4e3a\u4f4e\u6210\u672c\u3001\u81ea\u9002\u5e94\u3001\u81ea\u52a8\u5316\u7684 GNN \u8bad\u7ec3\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848\u3002"}}
{"id": "2511.07422", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2511.07422", "abs": "https://arxiv.org/abs/2511.07422", "authors": ["Madabattula Rajesh Kumar", "Srinivasa Rao Aravilli", "Mustafa Saify", "Shashank Srivastava"], "title": "From Attention to Disaggregation: Tracing the Evolution of LLM Inference", "comment": null, "summary": "The evolution of Large Language Models from the Transformer architecture to models with trillions of parameters has shifted the primary bottleneck from model training to real time inference. Deploying these massive models is a complex distributed systems challenge constrained by memory bandwidth, computational throughput, and latency requirements. LLM inference fundamentally requires solving a multi objective optimization problem to minimize latency, maximize throughput, and reduce cost. This paper explores the necessary architectural shift towards disaggregated inference, which applies distributed systems principles such as service decomposition, resource disaggregation, and workload partitioning to overcome the limitations of traditional monolithic GPU clusters. By decoupling the compute intensive prefill phase from the memory intensive decode phase into independently scalable components, this paradigm mitigates resource contention and enables independent optimization of key metrics like Time to First Token and Inter Token Latency.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u4e86\u5927\u8bed\u8a00\u6a21\u578b\u63a8\u7406\u4e2d\u7684\u6027\u80fd\u74f6\u9888\uff0c\u5e76\u63d0\u51fa\u901a\u8fc7\u89e3\u8026\u63a8\u7406\u67b6\u6784\uff08\u5c06\u9884\u586b\u5145\u9636\u6bb5\u4e0e\u89e3\u7801\u9636\u6bb5\u5206\u79bb\uff09\u6765\u4f18\u5316\u5ef6\u8fdf\u3001\u541e\u5410\u91cf\u548c\u6210\u672c\u3002", "motivation": "\u968f\u7740\u5927\u8bed\u8a00\u6a21\u578b\u53c2\u6570\u89c4\u6a21\u6fc0\u589e\uff0c\u5b9e\u65f6\u63a8\u7406\u6210\u4e3a\u4e3b\u8981\u74f6\u9888\uff0c\u4f20\u7edf\u5355\u4f53GPU\u96c6\u7fa4\u5728\u5185\u5b58\u5e26\u5bbd\u3001\u8ba1\u7b97\u541e\u5410\u548c\u5ef6\u8fdf\u65b9\u9762\u9762\u4e34\u6311\u6218\uff0c\u4e9f\u9700\u65b0\u7684\u7cfb\u7edf\u67b6\u6784\u5e94\u5bf9\u591a\u76ee\u6807\u4f18\u5316\u9700\u6c42\u3002", "method": "\u5f15\u5165\u89e3\u8026\u63a8\u7406\u67b6\u6784\uff0c\u501f\u9274\u5206\u5e03\u5f0f\u7cfb\u7edf\u7406\u5ff5\uff08\u5982\u670d\u52a1\u5206\u89e3\u3001\u8d44\u6e90\u89e3\u8026\u548c\u5de5\u4f5c\u8d1f\u8f7d\u5206\u533a\uff09\uff0c\u5c06\u8ba1\u7b97\u5bc6\u96c6\u7684\u9884\u586b\u5145\u9636\u6bb5\u4e0e\u5185\u5b58\u5bc6\u96c6\u7684\u89e3\u7801\u9636\u6bb5\u62c6\u5206\u4e3a\u53ef\u72ec\u7acb\u6269\u5c55\u7684\u7ec4\u4ef6\u3002", "result": "\u8be5\u67b6\u6784\u6709\u6548\u7f13\u89e3\u4e86\u8d44\u6e90\u4e89\u7528\u95ee\u9898\uff0c\u5e76\u652f\u6301\u5bf9\u9996Token\u65f6\u95f4\uff08Time to First Token\uff09\u548cToken\u95f4\u5ef6\u8fdf\uff08Inter Token Latency\uff09\u7b49\u5173\u952e\u6307\u6807\u8fdb\u884c\u72ec\u7acb\u4f18\u5316\u3002", "conclusion": "\u89e3\u8026\u63a8\u7406\u662f\u4e00\u79cd\u6709\u524d\u666f\u7684\u67b6\u6784\u65b9\u5411\uff0c\u80fd\u591f\u663e\u8457\u63d0\u5347\u5927\u8bed\u8a00\u6a21\u578b\u63a8\u7406\u6548\u7387\uff0c\u5728\u5ef6\u8fdf\u3001\u541e\u5410\u548c\u6210\u672c\u4e4b\u95f4\u5b9e\u73b0\u66f4\u4f18\u5e73\u8861\u3002"}}
{"id": "2511.07423", "categories": ["cs.DC", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.07423", "abs": "https://arxiv.org/abs/2511.07423", "authors": ["Genglin Wang", "Liekang Zeng", "Bufang Yang", "Kaiwei Liu", "Guoliang Xing", "Chumin Sun", "Li Zhou", "Jie Sun", "Zhenyu Yan"], "title": "Synera: Synergistic LLM Serving across Device and Cloud at Scale", "comment": null, "summary": "Large Language Models (LLMs) are becoming key components in various mobile operating systems, driving smart applications like interactive chatbots and personal assistants. While bringing enhanced intelligence to mobile ends, their deployment suffers from a set of performance challenges, especially the generation quality degradation and prolonged latency. Prior works have mainly relied on solutions of cloud offloading or on-device Small Language Models (SLMs). However, the former is usually limited by the communication bottleneck, and the latter sacrifices generation quality due to resource constraints. To mitigate these limitations, this paper proposes Synera, a device-cloud synergistic LLM serving system that applies an efficient SLM-LLM synergistic mechanism. Through empirical studies on LLM's unique computing characteristics, Synera identifies a set of underexplored optimization opportunities in device-cloud synergistic LLM inference, including offloading decisions, pipeline stalls, and batching bottlenecks. To translate them into enhanced performance, Synera introduces tailored designs of communication-efficient selective offloading, stall-free parallel inference, and scalable cloud batching. Extensive evaluations with real-world testbeds show that Synera enables 1.20-5.47x better generation quality against competitive baselines with on-par latency performance. Compared with existing cloud serving, Synera achieves 8.2-16.5% lower cloud serving cost on various benchmarks.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faSynera\uff0c\u4e00\u79cd\u8bbe\u5907-\u4e91\u534f\u540c\u7684\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u670d\u52a1\u7cfb\u7edf\uff0c\u901a\u8fc7\u9ad8\u6548\u7684\u5c0f\u8bed\u8a00\u6a21\u578b\uff08SLM\uff09\u4e0eLLM\u534f\u540c\u673a\u5236\uff0c\u5728\u4fdd\u8bc1\u5ef6\u8fdf\u6027\u80fd\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u751f\u6210\u8d28\u91cf\u5e76\u964d\u4f4e\u4e91\u670d\u52a1\u6210\u672c\u3002", "motivation": "\u5f53\u524d\u5728\u79fb\u52a8\u8bbe\u5907\u4e0a\u90e8\u7f72\u5927\u8bed\u8a00\u6a21\u578b\u9762\u4e34\u751f\u6210\u8d28\u91cf\u4e0b\u964d\u548c\u5ef6\u8fdf\u589e\u52a0\u7684\u95ee\u9898\uff1b\u73b0\u6709\u65b9\u6848\u5982\u4e91\u7aef\u5378\u8f7d\u53d7\u9650\u4e8e\u901a\u4fe1\u74f6\u9888\uff0c\u800c\u672c\u5730\u5c0f\u6a21\u578b\u5219\u56e0\u8d44\u6e90\u9650\u5236\u727a\u7272\u751f\u6210\u8d28\u91cf\u3002", "method": "Synera\u901a\u8fc7\u5b9e\u8bc1\u7814\u7a76\u8bc6\u522b\u51fa\u8bbe\u5907-\u4e91\u534f\u540c\u63a8\u7406\u4e2d\u7684\u4f18\u5316\u673a\u4f1a\uff0c\u5e76\u5f15\u5165\u4e09\u9879\u5173\u952e\u6280\u672f\uff1a\u901a\u4fe1\u9ad8\u6548\u7684\u6709\u9009\u62e9\u6027\u5378\u8f7d\u3001\u65e0\u505c\u6ede\u5e76\u884c\u63a8\u7406\u548c\u53ef\u6269\u5c55\u7684\u4e91\u7aef\u6279\u5904\u7406\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cSynera\u76f8\u6bd4\u73b0\u6709\u57fa\u7ebf\u65b9\u6cd5\u5728\u5ef6\u8fdf\u76f8\u5f53\u7684\u60c5\u51b5\u4e0b\uff0c\u751f\u6210\u8d28\u91cf\u63d0\u53471.20\u20135.47\u500d\uff1b\u76f8\u6bd4\u7eaf\u4e91\u670d\u52a1\uff0c\u4e91\u670d\u52a1\u6210\u672c\u964d\u4f4e8.2%\u201316.5%\u3002", "conclusion": "Synera\u6709\u6548\u89e3\u51b3\u4e86\u79fb\u52a8\u7aefLLM\u90e8\u7f72\u4e2d\u7684\u6027\u80fd\u6311\u6218\uff0c\u5b9e\u73b0\u4e86\u9ad8\u8d28\u91cf\u3001\u4f4e\u5ef6\u8fdf\u548c\u4f4e\u6210\u672c\u7684\u534f\u540c\u63a8\u7406\u3002"}}
{"id": "2511.07424", "categories": ["cs.DC", "cs.CY"], "pdf": "https://arxiv.org/pdf/2511.07424", "abs": "https://arxiv.org/abs/2511.07424", "authors": ["Bhala Ranganathan", "Mickey Zhang", "Kai Wu"], "title": "Enhancing reliability in AI inference services: An empirical study on real production incidents", "comment": null, "summary": "Hyperscale large language model (LLM) inference places extraordinary demands on cloud systems, where even brief failures can translate into significant user and business impact. To better understand and mitigate these risks, we present one of the first provider-internal, practice-based analysis of LLM inference incidents. We developed a taxonomy and methodology grounded in a year of operational experience, validating it on 156 high-severity incidents, and conducted a focused quantitative study of Apr-Jun 2025 to ensure recency and relevance. Our approach achieves high labeling consistency (Cohen's K ~0.89), identifies dominant failure modes (in our dataset ~60% inference engine failures, within that category ~40% timeouts), and surfaces mitigation levers (~74% auto-detected; ~28% required hotfix). Beyond hotfixes, many incidents were mitigated via traffic routing, node rebalancing, or capacity increase policies, indicating further automation opportunities. We also show how the taxonomy guided targeted strategies such as connection liveness, GPU capacity-aware routing, and per-endpoint isolation and reduced incident impact and accelerated recovery. Finally, we contribute a practitioner-oriented adoption checklist that enables others to replicate our taxonomy, analysis, and automation opportunities in their own systems. This study demonstrates how systematic, empirically grounded analysis of inference operations can drive more reliable and cost-efficient LLM serving at scale.", "AI": {"tldr": "\u672c\u6587\u57fa\u4e8e\u4e00\u5e74\u7684\u8fd0\u8425\u7ecf\u9a8c\uff0c\u5bf9156\u8d77\u9ad8\u4e25\u91cd\u6027\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u63a8\u7406\u4e8b\u6545\u8fdb\u884c\u4e86\u7cfb\u7edf\u5206\u6790\uff0c\u63d0\u51fa\u4e86\u4e00\u4e2a\u5177\u6709\u9ad8\u6807\u6ce8\u4e00\u81f4\u6027\u7684\u5206\u7c7b\u6cd5\uff0c\u8bc6\u522b\u51fa\u4e3b\u8981\u6545\u969c\u6a21\u5f0f\uff08\u5982\u63a8\u7406\u5f15\u64ce\u6545\u969c\u548c\u8d85\u65f6\uff09\uff0c\u5e76\u603b\u7ed3\u4e86\u591a\u79cd\u7f13\u89e3\u7b56\u7565\uff08\u5982\u81ea\u52a8\u68c0\u6d4b\u3001\u6d41\u91cf\u8c03\u5ea6\u3001\u8282\u70b9\u518d\u5e73\u8861\u7b49\uff09\uff0c\u6700\u7ec8\u63d0\u4f9b\u4e86\u4e00\u4e2a\u4f9b\u4ece\u4e1a\u8005\u91c7\u7528\u7684\u68c0\u67e5\u6e05\u5355\uff0c\u4ee5\u63d0\u5347\u5927\u89c4\u6a21LLM\u63a8\u7406\u670d\u52a1\u7684\u53ef\u9760\u6027\u4e0e\u6210\u672c\u6548\u76ca\u3002", "motivation": "\u8d85\u5927\u89c4\u6a21\u5927\u8bed\u8a00\u6a21\u578b\u63a8\u7406\u5bf9\u4e91\u7cfb\u7edf\u63d0\u51fa\u6781\u9ad8\u8981\u6c42\uff0c\u77ed\u6682\u505c\u673a\u4e5f\u53ef\u80fd\u5e26\u6765\u91cd\u5927\u7528\u6237\u548c\u4e1a\u52a1\u5f71\u54cd\u3002\u4e3a\u7406\u89e3\u5e76\u7f13\u89e3\u8fd9\u4e9b\u98ce\u9669\uff0c\u4e9f\u9700\u57fa\u4e8e\u5b9e\u9645\u8fd0\u7ef4\u7ecf\u9a8c\u7684\u7cfb\u7edf\u6027\u5206\u6790\u3002", "method": "\u4f5c\u8005\u57fa\u4e8e\u4e00\u5e74\u7684\u5b9e\u9645\u8fd0\u7ef4\u7ecf\u9a8c\u6784\u5efa\u4e86\u4e00\u5957\u4e8b\u6545\u5206\u7c7b\u6cd5\u548c\u5206\u6790\u65b9\u6cd5\uff0c\u5728156\u8d77\u9ad8\u4e25\u91cd\u6027\u4e8b\u6545\u4e0a\u9a8c\u8bc1\u5176\u6709\u6548\u6027\uff0c\u5e76\u5bf92025\u5e744\u6708\u81f36\u6708\u7684\u6570\u636e\u8fdb\u884c\u805a\u7126\u5b9a\u91cf\u7814\u7a76\uff1b\u4f7f\u7528Cohen's Kappa\u7cfb\u6570\u8bc4\u4f30\u6807\u6ce8\u4e00\u81f4\u6027\uff0c\u5e76\u7ed3\u5408\u6848\u4f8b\u63d0\u70bc\u7f13\u89e3\u7b56\u7565\u548c\u81ea\u52a8\u5316\u673a\u4f1a\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u7ea660%\u7684\u4e8b\u6545\u6e90\u4e8e\u63a8\u7406\u5f15\u64ce\u6545\u969c\uff0c\u5176\u4e2d\u7ea640%\u4e3a\u8d85\u65f6\u95ee\u9898\uff1b\u7ea674%\u7684\u4e8b\u6545\u53ef\u88ab\u81ea\u52a8\u68c0\u6d4b\uff0c28%\u9700\u70ed\u4fee\u590d\uff1b\u591a\u6570\u4e8b\u6545\u53ef\u901a\u8fc7\u6d41\u91cf\u8def\u7531\u3001\u8282\u70b9\u518d\u5e73\u8861\u6216\u6269\u5bb9\u7b56\u7565\u7f13\u89e3\uff1b\u6240\u63d0\u5206\u7c7b\u6cd5\u6307\u5bfc\u4e86\u8fde\u63a5\u6d3b\u6027\u68c0\u6d4b\u3001GPU\u5bb9\u91cf\u611f\u77e5\u8def\u7531\u548c\u7aef\u70b9\u9694\u79bb\u7b49\u9488\u5bf9\u6027\u7b56\u7565\uff0c\u6709\u6548\u964d\u4f4e\u4e8b\u6545\u5f71\u54cd\u5e76\u52a0\u901f\u6062\u590d\u3002", "conclusion": "\u7cfb\u7edf\u5316\u3001\u57fa\u4e8e\u5b9e\u8bc1\u7684LLM\u63a8\u7406\u8fd0\u7ef4\u5206\u6790\u80fd\u591f\u663e\u8457\u63d0\u5347\u5927\u89c4\u6a21\u670d\u52a1\u7684\u53ef\u9760\u6027\u4e0e\u6210\u672c\u6548\u7387\uff0c\u6240\u63d0\u51fa\u7684\u5206\u7c7b\u6cd5\u548c\u4ece\u4e1a\u8005\u68c0\u67e5\u6e05\u5355\u6709\u52a9\u4e8e\u5728\u5176\u4ed6\u7cfb\u7edf\u4e2d\u590d\u73b0\u8be5\u6210\u679c\u3002"}}
{"id": "2511.08297", "categories": ["cs.OS", "cs.RO"], "pdf": "https://arxiv.org/pdf/2511.08297", "abs": "https://arxiv.org/abs/2511.08297", "authors": ["Takahiro Ishikawa-Aso", "Atsushi Yano", "Yutaro Kobayashi", "Takumi Jin", "Yuuki Takano", "Shinpei Kato"], "title": "Work-in-Progress: Function-as-Subtask API Replacing Publish/Subscribe for OS-Native DAG Scheduling", "comment": "4 pages, 6 figures. Accepted for IEEE RTSS 2025; this is the author-accepted manuscript", "summary": "The Directed Acyclic Graph (DAG) task model for real-time scheduling finds its primary practical target in Robot Operating System 2 (ROS 2). However, ROS 2's publish/subscribe API leaves DAG precedence constraints unenforced: a callback may publish mid-execution, and multi-input callbacks let developers choose topic-matching policies. Thus preserving DAG semantics relies on conventions; once violated, the model collapses. We propose the Function-as-Subtask (FasS) API, which expresses each subtask as a function whose arguments/return values are the subtask's incoming/outgoing edges. By minimizing description freedom, DAG semantics is guaranteed at the API rather than by programmer discipline. We implement a DAG-native scheduler using FasS on a Rust-based experimental kernel and evaluate its semantic fidelity, and we outline design guidelines for applying FasS to Linux Linux sched_ext.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aFunction-as-Subtask\uff08FasS\uff09\u7684\u65b0API\uff0c\u901a\u8fc7\u5c06\u5b50\u4efb\u52a1\u8868\u793a\u4e3a\u51fd\u6570\u5e76\u660e\u786e\u5176\u8f93\u5165\u8f93\u51fa\u4f9d\u8d56\u5173\u7cfb\uff0c\u4ece\u63a5\u53e3\u5c42\u9762\u4fdd\u969c\u6709\u5411\u65e0\u73af\u56fe\uff08DAG\uff09\u8bed\u4e49\u7684\u6b63\u786e\u6027\uff0c\u907f\u514d\u4e86ROS 2\u4e2d\u56e0\u53d1\u5e03/\u8ba2\u9605\u673a\u5236\u5bfc\u81f4\u7684DAG\u7ea6\u675f\u5931\u6548\u95ee\u9898\uff0c\u5e76\u5728Rust\u5b9e\u9a8c\u5185\u6838\u4e0a\u5b9e\u73b0\u4e86\u539f\u751fDAG\u8c03\u5ea6\u5668\u8fdb\u884c\u9a8c\u8bc1\u3002", "motivation": "ROS 2\u7684\u53d1\u5e03/\u8ba2\u9605API\u65e0\u6cd5\u5f3a\u5236\u6267\u884cDAG\u4efb\u52a1\u6a21\u578b\u4e2d\u7684\u4f18\u5148\u7ea6\u675f\uff0c\u4f9d\u8d56\u5f00\u53d1\u8005\u7ea6\u5b9a\u7ef4\u6301\u8bed\u4e49\uff0c\u4e00\u65e6\u8fdd\u53cd\u5219\u6a21\u578b\u5931\u6548\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u80fd\u4eceAPI\u5c42\u9762\u4fdd\u8bc1DAG\u8bed\u4e49\u7684\u65b9\u6cd5\u3002", "method": "\u63d0\u51faFunction-as-Subtask\uff08FasS\uff09API\uff0c\u5c06\u6bcf\u4e2a\u5b50\u4efb\u52a1\u5efa\u6a21\u4e3a\u51fd\u6570\uff0c\u5176\u53c2\u6570\u548c\u8fd4\u56de\u503c\u5206\u522b\u5bf9\u5e94\u8f93\u5165\u548c\u8f93\u51fa\u8fb9\uff0c\u4ece\u800c\u9650\u5236\u63cf\u8ff0\u81ea\u7531\u5ea6\uff1b\u5e76\u5728\u57fa\u4e8eRust\u7684\u5b9e\u9a8c\u5185\u6838\u4e0a\u5b9e\u73b0\u652f\u6301FasS\u7684DAG\u539f\u751f\u8c03\u5ea6\u5668\u3002", "result": "\u5b9e\u73b0\u4e86\u8bed\u4e49\u4fdd\u771f\u5ea6\u66f4\u9ad8\u7684DAG\u8c03\u5ea6\u7cfb\u7edf\uff0c\u5e76\u63d0\u51fa\u4e86\u5c06FasS\u5e94\u7528\u4e8eLinux sched_ext\u7684\u8bbe\u8ba1\u6307\u5357\u3002", "conclusion": "FasS API\u80fd\u591f\u6709\u6548\u4fdd\u969cDAG\u4efb\u52a1\u6a21\u578b\u7684\u8bed\u4e49\u6b63\u786e\u6027\uff0c\u51cf\u5c11\u5bf9\u7a0b\u5e8f\u5458\u7eaa\u5f8b\u7684\u4f9d\u8d56\uff0c\u4e3a\u5b9e\u65f6\u7cfb\u7edf\u4e2dDAG\u8c03\u5ea6\u7684\u53ef\u9760\u5b9e\u73b0\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2511.08568", "categories": ["cs.PF"], "pdf": "https://arxiv.org/pdf/2511.08568", "abs": "https://arxiv.org/abs/2511.08568", "authors": ["Jie Ren", "Bin Ma", "Shuangyan Yang", "Benjamin Francis", "Ehsan K. Ardestani", "Min Si", "Dong Li"], "title": "Machine Learning-Guided Memory Optimization for DLRM Inference on Tiered Memory", "comment": null, "summary": "Deep learning recommendation models (DLRMs) are widely used in industry, and their memory capacity requirements reach the terabyte scale. Tiered memory architectures provide a cost-effective solution but introduce challenges in embedding-vector placement due to complex embedding-access patterns. We propose RecMG, a machine learning (ML)-guided system for vector caching and prefetching on tiered memory. RecMG accurately predicts accesses to embedding vectors with long reuse distances or few reuses. The design of RecMG focuses on making ML feasible in the context of DLRM inference by addressing unique challenges in data labeling and navigating the search space for embedding-vector placement. By employing separate ML models for caching and prefetching, plus a novel differentiable loss function, RecMG narrows the prefetching search space and minimizes on-demand fetches. Compared to state-of-the-art temporal, spatial, and ML-based prefetchers, RecMG reduces on-demand fetches by 2.2x, 2.8x, and 1.5x, respectively. In industrial-scale DLRM inference scenarios, RecMG effectively reduces end-to-end DLRM inference time by up to 43%.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faRecMG\uff0c\u4e00\u79cd\u57fa\u4e8e\u673a\u5668\u5b66\u4e60\u7684\u7cfb\u7edf\uff0c\u7528\u4e8e\u5728\u5206\u5c42\u5185\u5b58\u67b6\u6784\u4e2d\u4f18\u5316\u6df1\u5ea6\u5b66\u4e60\u63a8\u8350\u6a21\u578b\uff08DLRM\uff09\u7684\u5d4c\u5165\u5411\u91cf\u7f13\u5b58\u4e0e\u9884\u53d6\uff0c\u663e\u8457\u51cf\u5c11\u6309\u9700\u83b7\u53d6\u6b21\u6570\u5e76\u63d0\u5347\u63a8\u7406\u6548\u7387\u3002", "motivation": "DLRM\u6a21\u578b\u5bf9\u5185\u5b58\u5bb9\u91cf\u9700\u6c42\u5de8\u5927\uff0c\u800c\u5206\u5c42\u5185\u5b58\u867d\u6210\u672c\u4f4e\uff0c\u5374\u56e0\u590d\u6742\u7684\u5d4c\u5165\u8bbf\u95ee\u6a21\u5f0f\u5e26\u6765\u5d4c\u5165\u5411\u91cf\u653e\u7f6e\u96be\u9898\uff0c\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u9ad8\u6548\u5e94\u5bf9\u957f\u91cd\u7528\u8ddd\u79bb\u6216\u4f4e\u9891\u91cd\u7528\u7684\u8bbf\u95ee\u573a\u666f\u3002", "method": "RecMG\u91c7\u7528\u4e24\u4e2a\u72ec\u7acb\u7684\u673a\u5668\u5b66\u4e60\u6a21\u578b\u5206\u522b\u5904\u7406\u7f13\u5b58\u548c\u9884\u53d6\u4efb\u52a1\uff0c\u5e76\u8bbe\u8ba1\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u53ef\u5fae\u635f\u5931\u51fd\u6570\uff0c\u4ee5\u7f29\u5c0f\u9884\u53d6\u641c\u7d22\u7a7a\u95f4\u3001\u51cf\u5c11\u6309\u9700\u83b7\u53d6\uff1b\u540c\u65f6\u89e3\u51b3DLRM\u63a8\u7406\u4e2d\u6570\u636e\u6807\u6ce8\u548c\u5d4c\u5165\u653e\u7f6e\u641c\u7d22\u7a7a\u95f4\u7684\u72ec\u7279\u6311\u6218\u3002", "result": "\u76f8\u6bd4\u5f53\u524d\u6700\u5148\u8fdb\u7684\u65f6\u5e8f\u3001\u7a7a\u95f4\u53ca\u57fa\u4e8eML\u7684\u9884\u53d6\u5668\uff0cRecMG\u5206\u522b\u5c06\u6309\u9700\u83b7\u53d6\u6b21\u6570\u51cf\u5c11\u4e862.2\u500d\u30012.8\u500d\u548c1.5\u500d\uff0c\u5728\u5de5\u4e1a\u7ea7DLRM\u63a8\u7406\u4e2d\u7aef\u5230\u7aef\u63a8\u7406\u65f6\u95f4\u6700\u591a\u51cf\u5c1143%\u3002", "conclusion": "RecMG\u901a\u8fc7\u673a\u5668\u5b66\u4e60\u5f15\u5bfc\u7684\u7f13\u5b58\u4e0e\u9884\u53d6\u7b56\u7565\uff0c\u6709\u6548\u89e3\u51b3\u4e86DLRM\u5728\u5206\u5c42\u5185\u5b58\u4e0a\u7684\u5d4c\u5165\u8bbf\u95ee\u6548\u7387\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u63a8\u7406\u6027\u80fd\u3002"}}
{"id": "2511.07506", "categories": ["cs.SE", "cs.DB"], "pdf": "https://arxiv.org/pdf/2511.07506", "abs": "https://arxiv.org/abs/2511.07506", "authors": ["Izaque Esteves", "Regina Braga", "Jos\u00e9 Maria David", "Victor Stroele"], "title": "A Service Suite for Specifying Digital Twins for Industry 5.0", "comment": "38 pages, submitted do IEEE Access. It is under review - second rebuttal", "summary": "One of the challenges of predictive maintenance is making decisions based on data in an agile and assertive way. Connected sensors and operational data favor intelligent processing techniques to enrich information and enable decision-making. Digital Twins (DTs) can be used to process information and support decision-making. DTs are a real-time representation of physical machines and generate data that predictive maintenance can use to make assertive and quick decisions. The main contribution of this work is the specification of a suite of services for specifying DTs, called DT-Create, focused on decision support in predictive maintenance. DT-Create suite is based on intelligent techniques, semantic data processing, and self-adaptation. This suite was developed using the Design Science Research (DSR) methodology through two development cycles and evaluated through case studies. The results demonstrate the feasibility of using DT-Create in specifying DTs considering the following aspects: (i) collection, storage, and intelligent processing of data generated by sensors, (ii) enrichment of information through machine learning and ontologies, (iii) use of intelligent techniques to select predictive models that adhere to the available data set, and (iv) decision support and self-adaptation.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u5957\u540d\u4e3aDT-Create\u7684\u670d\u52a1\u5957\u4ef6\uff0c\u7528\u4e8e\u5728\u9884\u6d4b\u6027\u7ef4\u62a4\u4e2d\u6784\u5efa\u6570\u5b57\u5b6a\u751f\uff08DT\uff09\uff0c\u4ee5\u652f\u6301\u57fa\u4e8e\u4f20\u611f\u5668\u6570\u636e\u7684\u667a\u80fd\u51b3\u7b56\u3002", "motivation": "\u9884\u6d4b\u6027\u7ef4\u62a4\u9762\u4e34\u5982\u4f55\u57fa\u4e8e\u6570\u636e\u654f\u6377\u4e14\u51c6\u786e\u5730\u505a\u51fa\u51b3\u7b56\u7684\u6311\u6218\uff0c\u800c\u6570\u5b57\u5b6a\u751f\u53ef\u63d0\u4f9b\u5b9e\u65f6\u7269\u7406\u8bbe\u5907\u7684\u8868\u793a\u5e76\u751f\u6210\u53ef\u7528\u4e8e\u51b3\u7b56\u7684\u6570\u636e\u3002", "method": "\u91c7\u7528\u8bbe\u8ba1\u79d1\u5b66\u7814\u7a76\uff08DSR\uff09\u65b9\u6cd5\uff0c\u901a\u8fc7\u4e24\u4e2a\u5f00\u53d1\u5468\u671f\u6784\u5efaDT-Create\u5957\u4ef6\uff0c\u5e76\u7ed3\u5408\u667a\u80fd\u6280\u672f\u3001\u8bed\u4e49\u6570\u636e\u5904\u7406\u548c\u81ea\u9002\u5e94\u673a\u5236\uff0c\u901a\u8fc7\u6848\u4f8b\u7814\u7a76\u8fdb\u884c\u8bc4\u4f30\u3002", "result": "\u7ed3\u679c\u8868\u660eDT-Create\u5728\u4ee5\u4e0b\u65b9\u9762\u5177\u6709\u53ef\u884c\u6027\uff1a(i) \u4f20\u611f\u5668\u6570\u636e\u7684\u91c7\u96c6\u3001\u5b58\u50a8\u4e0e\u667a\u80fd\u5904\u7406\uff1b(ii) \u5229\u7528\u673a\u5668\u5b66\u4e60\u548c\u672c\u4f53\u8bba\u4e30\u5bcc\u4fe1\u606f\uff1b(iii) \u667a\u80fd\u9009\u62e9\u9002\u914d\u6570\u636e\u96c6\u7684\u9884\u6d4b\u6a21\u578b\uff1b(iv) \u652f\u6301\u51b3\u7b56\u4e0e\u81ea\u9002\u5e94\u80fd\u529b\u3002", "conclusion": "DT-Create\u5957\u4ef6\u6709\u6548\u652f\u6301\u4e86\u9762\u5411\u9884\u6d4b\u6027\u7ef4\u62a4\u7684\u6570\u5b57\u5b6a\u751f\u6784\u5efa\uff0c\u63d0\u5347\u4e86\u6570\u636e\u9a71\u52a8\u51b3\u7b56\u7684\u51c6\u786e\u6027\u4e0e\u654f\u6377\u6027\u3002"}}
{"id": "2511.07425", "categories": ["cs.DC", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.07425", "abs": "https://arxiv.org/abs/2511.07425", "authors": ["Tung", "Nguyen", "Tuyen Nguyen"], "title": "An Evaluation of LLMs Inference on Popular Single-board Computers", "comment": "9 pages, 3 figures", "summary": "The growing demand for on-device large language model (LLM) inference is driving interest in deploying lightweight, cost-effective AI solutions on edge hardware. Single-board computers (SBCs) such as the Raspberry Pi and Orange Pi offer a promising platform for localized, privacy-preserving inference-but remain underexplored in the context of LLM workloads. In this work, we benchmark the performance of 25 quantized open-source LLMs across three SBCs-Raspberry Pi 4, Raspberry Pi 5, and Orange Pi 5 Pro-using two inference runtimes: Ollama and Llamafile. We evaluate generation throughput, memory usage, and power consumption under varying CPU configurations, using multiple prompt types to simulate realistic workloads. Our results show that SBCs can reliably support models up to 1.5B parameters, with Llamafile achieving up to 4x higher throughput and 30-40% lower power usage than Ollama. We identify architecture-specific bottlenecks, highlight runtime-level trade-offs, and provide practical deployment recommendations. This study offers the first broad evaluation of LLM inference on SBCs, bridging the gap between high-performance language models and affordable edge computing.", "AI": {"tldr": "\u672c\u6587\u9996\u6b21\u7cfb\u7edf\u8bc4\u4f30\u4e8625\u4e2a\u91cf\u5316\u5f00\u6e90\u5927\u8bed\u8a00\u6a21\u578b\u5728\u6811\u8393\u6d3e\u548cOrange Pi\u7b49\u5355\u677f\u8ba1\u7b97\u673a\u4e0a\u7684\u63a8\u7406\u6027\u80fd\uff0c\u6bd4\u8f83\u4e86Ollama\u4e0eLlamafile\u4e24\u79cd\u8fd0\u884c\u65f6\u5728\u541e\u5410\u91cf\u3001\u5185\u5b58\u5360\u7528\u548c\u529f\u8017\u65b9\u9762\u7684\u8868\u73b0\uff0c\u5e76\u63d0\u4f9b\u4e86\u5b9e\u7528\u7684\u90e8\u7f72\u5efa\u8bae\u3002", "motivation": "\u968f\u7740\u5bf9\u8bbe\u5907\u7aef\u5927\u8bed\u8a00\u6a21\u578b\u63a8\u7406\u9700\u6c42\u7684\u589e\u957f\uff0c\u4e9f\u9700\u5728\u8fb9\u7f18\u786c\u4ef6\u4e0a\u90e8\u7f72\u8f7b\u91cf\u3001\u4f4e\u6210\u672c\u7684AI\u89e3\u51b3\u65b9\u6848\u3002\u5355\u677f\u8ba1\u7b97\u673a\uff08\u5982\u6811\u8393\u6d3e\uff09\u867d\u5177\u6f5c\u529b\uff0c\u4f46\u5728\u5927\u8bed\u8a00\u6a21\u578b\u8d1f\u8f7d\u4e0b\u7684\u8868\u73b0\u5c1a\u4e0d\u660e\u786e\u3002", "method": "\u5728\u6811\u8393\u6d3e4\u3001\u6811\u8393\u6d3e5\u548cOrange Pi 5 Pro\u4e09\u79cd\u5355\u677f\u8ba1\u7b97\u673a\u4e0a\uff0c\u4f7f\u7528Ollama\u548cLlamafile\u4e24\u79cd\u63a8\u7406\u8fd0\u884c\u65f6\uff0c\u5bf925\u4e2a\u91cf\u5316\u5f00\u6e90\u5927\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u57fa\u51c6\u6d4b\u8bd5\uff0c\u8bc4\u4f30\u4e0d\u540cCPU\u914d\u7f6e\u4e0b\u591a\u79cd\u63d0\u793a\u7c7b\u578b\u4e0b\u7684\u751f\u6210\u541e\u5410\u91cf\u3001\u5185\u5b58\u4f7f\u7528\u548c\u529f\u8017\u3002", "result": "\u5355\u677f\u8ba1\u7b97\u673a\u53ef\u7a33\u5b9a\u652f\u6301\u6700\u591a1.5B\u53c2\u6570\u7684\u6a21\u578b\uff1bLlamafile\u76f8\u6bd4Ollama\u6700\u9ad8\u5b9e\u73b04\u500d\u541e\u5410\u91cf\u63d0\u5347\u548c30\u201340%\u7684\u529f\u8017\u964d\u4f4e\uff1b\u7814\u7a76\u8fd8\u8bc6\u522b\u51fa\u67b6\u6784\u76f8\u5173\u74f6\u9888\u5e76\u63ed\u793a\u4e86\u8fd0\u884c\u65f6\u5c42\u9762\u7684\u6743\u8861\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u5728\u4f4e\u6210\u672c\u8fb9\u7f18\u8bbe\u5907\u4e0a\u90e8\u7f72\u5927\u8bed\u8a00\u6a21\u578b\u63d0\u4f9b\u4e86\u9996\u4e2a\u5168\u9762\u6027\u80fd\u8bc4\u4f30\u548c\u5b9e\u7528\u6307\u5357\uff0c\u5f25\u5408\u4e86\u9ad8\u6027\u80fd\u8bed\u8a00\u6a21\u578b\u4e0e\u53ef\u8d1f\u62c5\u8fb9\u7f18\u8ba1\u7b97\u4e4b\u95f4\u7684\u5dee\u8ddd\u3002"}}
{"id": "2511.07985", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2511.07985", "abs": "https://arxiv.org/abs/2511.07985", "authors": ["Simei Yang", "Xinyu Shi", "Lu Zhao", "Yunyu Ling", "Quanjun Wang", "Francky Catthoor"], "title": "PIMfused: Near-Bank DRAM-PIM with Fused-layer Dataflow for CNN Data Transfer Optimization", "comment": "6 pages", "summary": "Near-bank Processing-in-Memory (PIM) architectures integrate processing cores (PIMcores) close to DRAM banks to mitigate the high cost of off-chip memory accesses. When accelerating convolutional neural network (CNN) on DRAM-PIM, performance is often constrained by cross-bank (or cross-PIMcore) data transfers, which are induced by the conventional layer-by-layer dataflow that enforces inter-bank (or inter-PIMcore) dependencies across successive CNN layers. To address this challenge, we propose PIMfused, a hardware-software co-design that enables fused-layer dataflow for end-to-end CNN execution in near-bank DRAM-PIM. By adopting fused-layer dataflow, PIMfused improves data reuse and, more importantly, breaks inter-bank data dependencies, thereby optimizing cross-bank data transfers without sacrificing bank-level parallelism. We study the impact of buffer sizes and PIMcore parallelism (1-bank vs. 4-bank) on PIMfused using end-to-end ResNet18. We present three key takeaways and show that with 4-bank PIMcores, PIMfused achieves overall PPA gains over a GDDR6-AiM-like baseline, cutting memory cycles to 30.6%, energy to 83.4%, and area to 76.5%.", "AI": {"tldr": "PIMfused \u662f\u4e00\u79cd\u8f6f\u786c\u4ef6\u534f\u540c\u8bbe\u8ba1\uff0c\u901a\u8fc7\u878d\u5408\u5c42\u6570\u636e\u6d41\u5728\u8fd1\u5b58 DRAM-PIM \u67b6\u6784\u4e0a\u9ad8\u6548\u6267\u884c CNN\uff0c\u663e\u8457\u51cf\u5c11\u8de8 Bank \u6570\u636e\u4f20\u8f93\uff0c\u63d0\u5347\u6570\u636e\u590d\u7528\u5e76\u6253\u7834 Bank \u95f4\u4f9d\u8d56\uff0c\u5728 ResNet18 \u4e0a\u5b9e\u73b0\u663e\u8457\u7684\u6027\u80fd\u3001\u529f\u8017\u4e0e\u9762\u79ef\u4f18\u52bf\u3002", "motivation": "\u5728\u8fd1\u5b58 PIM \u67b6\u6784\u4e0a\u8fd0\u884c CNN \u65f6\uff0c\u4f20\u7edf\u9010\u5c42\u6570\u636e\u6d41\u5bfc\u81f4\u8de8 Bank\uff08\u6216\u8de8 PIMcore\uff09\u7684\u6570\u636e\u4f20\u8f93\u5f00\u9500\u5927\uff0c\u9650\u5236\u4e86\u6027\u80fd\u3002", "method": "\u63d0\u51fa PIMfused\uff0c\u91c7\u7528\u878d\u5408\u5c42\u6570\u636e\u6d41\uff08fused-layer dataflow\uff09\u8fdb\u884c\u7aef\u5230\u7aef CNN \u6267\u884c\uff0c\u4ee5\u63d0\u9ad8\u6570\u636e\u590d\u7528\u5e76\u6d88\u9664 Bank \u95f4\u7684\u6570\u636e\u4f9d\u8d56\u3002", "result": "\u5728 4-Bank PIMcore \u914d\u7f6e\u4e0b\uff0c\u76f8\u6bd4 GDDR6-AiM \u7c7b\u57fa\u7ebf\uff0cPIMfused \u5c06\u5185\u5b58\u5468\u671f\u964d\u81f3 30.6%\u3001\u80fd\u8017\u964d\u81f3 83.4%\u3001\u9762\u79ef\u964d\u81f3 76.5%\u3002", "conclusion": "PIMfused \u901a\u8fc7\u878d\u5408\u5c42\u6570\u636e\u6d41\u6709\u6548\u4f18\u5316\u4e86\u8fd1\u5b58 PIM \u67b6\u6784\u4e0a\u7684 CNN \u6267\u884c\u6548\u7387\uff0c\u5728\u4fdd\u6301 Bank \u5e76\u884c\u6027\u7684\u540c\u65f6\u5927\u5e45\u964d\u4f4e\u8de8 Bank \u901a\u4fe1\u5f00\u9500\uff0c\u5e26\u6765\u663e\u8457\u7684 PPA \u6539\u8fdb\u3002"}}
{"id": "2511.07778", "categories": ["cs.MA"], "pdf": "https://arxiv.org/pdf/2511.07778", "abs": "https://arxiv.org/abs/2511.07778", "authors": ["Ao Ding", "Licheng Sun", "Yongjie Hou", "Huaqing Zhang", "Hongbin Ma"], "title": "A Historical Interaction-Enhanced Shapley Policy Gradient Algorithm for Multi-Agent Credit Assignment", "comment": null, "summary": "Multi-agent reinforcement learning (MARL) has demonstrated remarkable performance in multi-agent collaboration problems and has become a prominent topic in artificial intelligence research in recent years. However, traditional credit assignment schemes in MARL cannot reliably capture individual contributions in strongly coupled tasks while maintaining training stability, which leads to limited generalization capabilities and hinders algorithm performance. To address these challenges, we propose a Historical Interaction-Enhanced Shapley Policy Gradient Algorithm (HIS) for Multi-Agent Credit Assignment, which employs a hybrid credit assignment mechanism to balance base rewards with individual contribution incentives. By utilizing historical interaction data to calculate the Shapley value in a sample-efficient manner, HIS enhances the agent's ability to perceive its own contribution, while retaining the global reward to maintain training stability. Additionally, we provide theoretical guarantees for the hybrid credit assignment mechanism, ensuring that the assignment results it generates are both efficient and stable. We evaluate the proposed algorithm in three widely used continuous-action benchmark environments: Multi-Agent Particle Environment, Multi-Agent MuJoCo, and Bi-DexHands. Experimental results demonstrate that HIS outperforms state-of-the-art methods, particularly excelling in strongly coupled, complex collaborative tasks.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aHIS\u7684\u591a\u667a\u80fd\u4f53\u4fe1\u7528\u5206\u914d\u7b97\u6cd5\uff0c\u901a\u8fc7\u7ed3\u5408\u5386\u53f2\u4ea4\u4e92\u6570\u636e\u4e0eShapley\u503c\uff0c\u5728\u4fdd\u6301\u8bad\u7ec3\u7a33\u5b9a\u6027\u7684\u540c\u65f6\u66f4\u51c6\u786e\u5730\u8bc4\u4f30\u4e2a\u4f53\u8d21\u732e\uff0c\u5728\u591a\u4e2a\u8fde\u7eed\u52a8\u4f5c\u57fa\u51c6\u73af\u5883\u4e2d\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u4f20\u7edf\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u4fe1\u7528\u5206\u914d\u65b9\u6cd5\u96be\u4ee5\u5728\u5f3a\u8026\u5408\u4efb\u52a1\u4e2d\u53ef\u9760\u6355\u6349\u4e2a\u4f53\u8d21\u732e\uff0c\u540c\u65f6\u4fdd\u6301\u8bad\u7ec3\u7a33\u5b9a\u6027\uff0c\u9650\u5236\u4e86\u7b97\u6cd5\u7684\u6cdb\u5316\u80fd\u529b\u548c\u6027\u80fd\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u5386\u53f2\u4ea4\u4e92\u589e\u5f3a\u7684Shapley\u7b56\u7565\u68af\u5ea6\u7b97\u6cd5\uff08HIS\uff09\uff0c\u91c7\u7528\u6df7\u5408\u4fe1\u7528\u5206\u914d\u673a\u5236\uff0c\u7ed3\u5408\u57fa\u7840\u5956\u52b1\u4e0e\u4e2a\u4f53\u8d21\u732e\u6fc0\u52b1\uff0c\u5e76\u5229\u7528\u5386\u53f2\u4ea4\u4e92\u6570\u636e\u9ad8\u6548\u8ba1\u7b97Shapley\u503c\uff0c\u540c\u65f6\u4fdd\u7559\u5168\u5c40\u5956\u52b1\u4ee5\u7ef4\u6301\u8bad\u7ec3\u7a33\u5b9a\u6027\u3002", "result": "\u5728Multi-Agent Particle Environment\u3001Multi-Agent MuJoCo\u548cBi-DexHands\u4e09\u4e2a\u8fde\u7eed\u52a8\u4f5c\u57fa\u51c6\u73af\u5883\u4e2d\u7684\u5b9e\u9a8c\u8868\u660e\uff0cHIS\u5728\u5f3a\u8026\u5408\u590d\u6742\u534f\u4f5c\u4efb\u52a1\u4e2d\u663e\u8457\u4f18\u4e8e\u5f53\u524d\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\u3002", "conclusion": "HIS\u901a\u8fc7\u7406\u8bba\u4fdd\u8bc1\u7684\u6df7\u5408\u4fe1\u7528\u5206\u914d\u673a\u5236\uff0c\u6709\u6548\u63d0\u5347\u4e86\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u5728\u590d\u6742\u534f\u4f5c\u573a\u666f\u4e0b\u7684\u6027\u80fd\u4e0e\u7a33\u5b9a\u6027\uff0c\u4e3a\u591a\u667a\u80fd\u4f53\u4fe1\u7528\u5206\u914d\u95ee\u9898\u63d0\u4f9b\u4e86\u65b0\u7684\u89e3\u51b3\u601d\u8def\u3002"}}
{"id": "2511.07426", "categories": ["cs.DC", "cs.AI", "cs.CL", "cs.NI", "cs.SE"], "pdf": "https://arxiv.org/pdf/2511.07426", "abs": "https://arxiv.org/abs/2511.07426", "authors": ["Zihao Ding", "Mufeng Zhu", "Yao Liu"], "title": "Network and Systems Performance Characterization of MCP-Enabled LLM Agents", "comment": null, "summary": "Model Context Protocol (MCP) has recently gained increased attention within the AI community for providing a standardized way for large language models (LLMs) to interact with external tools and services, significantly enhancing their capabilities. However, the inclusion of extensive contextual information, including system prompts, MCP tool definitions, and context histories, in MCP-enabled LLM interactions, dramatically inflates token usage. Given that LLM providers charge based on tokens, these expanded contexts can quickly escalate monetary costs and increase the computational load on LLM services. This paper presents a comprehensive measurement-based analysis of MCP-enabled interactions with LLMs, revealing trade-offs between capability, performance, and cost. We explore how different LLM models and MCP configurations impact key performance metrics such as token efficiency, monetary cost, task completion times, and task success rates, and suggest potential optimizations, including enabling parallel tool calls and implementing robust task abort mechanisms. These findings provide useful insights for developing more efficient, robust, and cost-effective MCP-enabled workflows.", "AI": {"tldr": "\u672c\u6587\u5bf9\u542f\u7528MCP\uff08Model Context Protocol\uff09\u7684\u5927\u8bed\u8a00\u6a21\u578b\u4ea4\u4e92\u8fdb\u884c\u4e86\u7cfb\u7edf\u6027\u6d4b\u91cf\u5206\u6790\uff0c\u63ed\u793a\u4e86\u5176\u5728\u80fd\u529b\u3001\u6027\u80fd\u4e0e\u6210\u672c\u4e4b\u95f4\u7684\u6743\u8861\uff0c\u5e76\u63d0\u51fa\u4e86\u63d0\u5347\u6548\u7387\u548c\u964d\u4f4e\u6210\u672c\u7684\u4f18\u5316\u7b56\u7565\u3002", "motivation": "MCP\u867d\u80fd\u589e\u5f3a\u5927\u8bed\u8a00\u6a21\u578b\u4e0e\u5916\u90e8\u5de5\u5177\u4ea4\u4e92\u7684\u80fd\u529b\uff0c\u4f46\u5f15\u5165\u5927\u91cf\u4e0a\u4e0b\u6587\u4fe1\u606f\u663e\u8457\u589e\u52a0token\u4f7f\u7528\u91cf\uff0c\u8fdb\u800c\u63a8\u9ad8\u8d39\u7528\u548c\u8ba1\u7b97\u8d1f\u62c5\uff0c\u4e9f\u9700\u5bf9\u5176\u5f71\u54cd\u8fdb\u884c\u91cf\u5316\u5206\u6790\u3002", "method": "\u901a\u8fc7\u6d4b\u91cf\u4e0d\u540c\u5927\u8bed\u8a00\u6a21\u578b\u53caMCP\u914d\u7f6e\u4e0b\u7684\u4ea4\u4e92\u8868\u73b0\uff0c\u8bc4\u4f30\u5176\u5728token\u6548\u7387\u3001\u91d1\u94b1\u6210\u672c\u3001\u4efb\u52a1\u5b8c\u6210\u65f6\u95f4\u548c\u6210\u529f\u7387\u7b49\u5173\u952e\u6307\u6807\u4e0a\u7684\u5dee\u5f02\u3002", "result": "\u7814\u7a76\u53d1\u73b0MCP\u914d\u7f6e\u663e\u8457\u5f71\u54cd\u6210\u672c\u4e0e\u6027\u80fd\uff0c\u5e76\u9a8c\u8bc1\u4e86\u5e76\u884c\u5de5\u5177\u8c03\u7528\u548c\u4efb\u52a1\u4e2d\u6b62\u673a\u5236\u7b49\u4f18\u5316\u624b\u6bb5\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u6784\u5efa\u9ad8\u6548\u3001\u7a33\u5065\u4e14\u7ecf\u6d4e\u7684MCP\u9a71\u52a8\u5de5\u4f5c\u6d41\u63d0\u4f9b\u4e86\u5b9e\u8bc1\u4f9d\u636e\u548c\u5b9e\u7528\u5efa\u8bae\u3002"}}
{"id": "2511.08054", "categories": ["cs.AR", "cs.CV", "eess.SY"], "pdf": "https://arxiv.org/pdf/2511.08054", "abs": "https://arxiv.org/abs/2511.08054", "authors": ["Yunqi Shi", "Xi Lin", "Zhiang Wang", "Siyuan Xu", "Shixiong Kai", "Yao Lai", "Chengrui Gao", "Ke Xue", "Mingxuan Yuan", "Chao Qian", "Zhi-Hua Zhou"], "title": "Re$^{\\text{2}}$MaP: Macro Placement by Recursively Prototyping and Packing Tree-based Relocating", "comment": "IEEE Transactions on Comupter-Aided Design under review", "summary": "This work introduces the Re$^{\\text{2}}$MaP method, which generates expert-quality macro placements through recursively prototyping and packing tree-based relocating. We first perform multi-level macro grouping and PPA-aware cell clustering to produce a unified connection matrix that captures both wirelength and dataflow among macros and clusters. Next, we use DREAMPlace to build a mixed-size placement prototype and obtain reference positions for each macro and cluster. Based on this prototype, we introduce ABPlace, an angle-based analytical method that optimizes macro positions on an ellipse to distribute macros uniformly near chip periphery, while optimizing wirelength and dataflow. A packing tree-based relocating procedure is then designed to jointly adjust the locations of macro groups and the macros within each group, by optimizing an expertise-inspired cost function that captures various design constraints through evolutionary search. Re$^{\\text{2}}$MaP repeats the above process: Only a subset of macro groups are positioned in each iteration, and the remaining macros are deferred to the next iteration to improve the prototype's accuracy. Using a well-established backend flow with sufficient timing optimizations, Re$^{\\text{2}}$MaP achieves up to 22.22% (average 10.26%) improvement in worst negative slack (WNS) and up to 97.91% (average 33.97%) improvement in total negative slack (TNS) compared to the state-of-the-art academic placer Hier-RTLMP. It also ranks higher on WNS, TNS, power, design rule check (DRC) violations, and runtime than the conference version ReMaP, across seven tested cases. Our code is available at https://github.com/lamda-bbo/Re2MaP.", "AI": {"tldr": "Re\u00b2MaP\u662f\u4e00\u79cd\u901a\u8fc7\u9012\u5f52\u539f\u578b\u6784\u5efa\u4e0e\u57fa\u4e8e\u6253\u5305\u6811\u7684\u91cd\u5b9a\u4f4d\u65b9\u6cd5\uff0c\u5b9e\u73b0\u9ad8\u8d28\u91cf\u5b8f\u5355\u5143\u5e03\u5c40\u7684\u65b0\u578b\u5e03\u5c40\u7b97\u6cd5\uff0c\u5728\u65f6\u5e8f\u3001\u529f\u8017\u548c\u8bbe\u8ba1\u89c4\u5219\u7b49\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u5b66\u672f\u5e03\u5c40\u5668\u3002", "motivation": "\u73b0\u6709\u5b8f\u5355\u5143\u5e03\u5c40\u65b9\u6cd5\u5728\u5904\u7406\u590d\u6742\u8bbe\u8ba1\u7ea6\u675f\uff08\u5982\u7ebf\u957f\u3001\u6570\u636e\u6d41\u3001\u65f6\u5e8f\u7b49\uff09\u65f6\u5b58\u5728\u5c40\u9650\uff0c\u96be\u4ee5\u517c\u987e\u5e03\u5c40\u8d28\u91cf\u4e0e\u6548\u7387\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u80fd\u878d\u5408\u4e13\u5bb6\u77e5\u8bc6\u5e76\u8fed\u4ee3\u4f18\u5316\u7684\u65b0\u578b\u5e03\u5c40\u7b56\u7565\u3002", "method": "\u8be5\u65b9\u6cd5\u9996\u5148\u8fdb\u884c\u591a\u5c42\u7ea7\u5b8f\u5206\u7ec4\u4e0ePPA\u611f\u77e5\u7684\u5355\u5143\u805a\u7c7b\uff0c\u6784\u5efa\u7edf\u4e00\u8fde\u63a5\u77e9\u9635\uff1b\u7136\u540e\u5229\u7528DREAMPlace\u751f\u6210\u6df7\u5408\u5c3a\u5bf8\u5e03\u5c40\u539f\u578b\uff0c\u5e76\u63d0\u51fa\u57fa\u4e8e\u89d2\u5ea6\u7684ABPlace\u65b9\u6cd5\u5728\u692d\u5706\u4e0a\u4f18\u5316\u5b8f\u4f4d\u7f6e\uff1b\u63a5\u7740\u901a\u8fc7\u6253\u5305\u6811\u7ed3\u6784\u5bf9\u5b8f\u7ec4\u53ca\u5176\u5185\u90e8\u5b8f\u8054\u5408\u91cd\u5b9a\u4f4d\uff0c\u91c7\u7528\u8fdb\u5316\u641c\u7d22\u4f18\u5316\u878d\u5408\u591a\u79cd\u8bbe\u8ba1\u7ea6\u675f\u7684\u6210\u672c\u51fd\u6570\uff1b\u6574\u4e2a\u6d41\u7a0b\u9012\u5f52\u6267\u884c\uff0c\u6bcf\u6b21\u4ec5\u5b9a\u4f4d\u90e8\u5206\u5b8f\u7ec4\u4ee5\u63d0\u5347\u539f\u578b\u7cbe\u5ea6\u3002", "result": "\u5728\u6807\u51c6\u540e\u7aef\u6d41\u7a0b\u4e0b\uff0cRe\u00b2MaP\u76f8\u6bd4\u5f53\u524d\u6700\u5148\u8fdb\u7684\u5b66\u672f\u5e03\u5c40\u5668Hier-RTLMP\uff0c\u5728\u6700\u5dee\u8d1f\u88d5\u91cf\uff08WNS\uff09\u4e0a\u6700\u591a\u63d0\u534722.22%\uff08\u5e73\u574710.26%\uff09\uff0c\u603b\u8d1f\u88d5\u91cf\uff08TNS\uff09\u6700\u591a\u63d0\u534797.91%\uff08\u5e73\u574733.97%\uff09\uff1b\u5728\u4e03\u4e2a\u6d4b\u8bd5\u6848\u4f8b\u4e2d\uff0c\u5176\u5728WNS\u3001TNS\u3001\u529f\u8017\u3001DRC\u8fdd\u89c4\u6570\u548c\u8fd0\u884c\u65f6\u95f4\u65b9\u9762\u5747\u4f18\u4e8e\u5176\u4f1a\u8bae\u7248\u672cReMaP\u3002", "conclusion": "Re\u00b2MaP\u901a\u8fc7\u9012\u5f52\u539f\u578b\u6784\u5efa\u4e0e\u6253\u5305\u6811\u91cd\u5b9a\u4f4d\u673a\u5236\uff0c\u6709\u6548\u878d\u5408\u4e86\u4e13\u5bb6\u7ecf\u9a8c\u4e0e\u81ea\u52a8\u5316\u4f18\u5316\uff0c\u5728\u591a\u9879\u5173\u952e\u6307\u6807\u4e0a\u663e\u8457\u8d85\u8d8a\u73b0\u6709\u65b9\u6cd5\uff0c\u4e3a\u9ad8\u8d28\u91cf\u5b8f\u5355\u5143\u5e03\u5c40\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2511.07612", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2511.07612", "abs": "https://arxiv.org/abs/2511.07612", "authors": ["Samuel W. Flint", "Jigyasa Chauhan", "Niloofar Mansoor", "Bonita Sharif", "Robert Dyer"], "title": "An Exploratory Eye Tracking Study on How Developers Classify and Debug Python Code in Different Paradigms", "comment": null, "summary": "Modern programming languages, such as Python, support language features from several paradigms, such as object-oriented, procedural, and functional. Research has shown that code written in some paradigms can be harder to comprehend, but to date, no research has looked at which paradigm-specific language features impact comprehension. To this end, this study seeks to uncover which paradigm-specific features impactcomprehension and debugging of code or how multi-paradigm code might affect a developer's ability to do so. We present an exploratory empirical eye-tracking study to investigate 1) how developers classify the predominant paradigm in Python code and 2) how the paradigm affects their ability to debug Python code. The goal is to uncover if specific language features are looked at more often while classifying and debugging code with a predominant paradigm. Twenty-nine developers (primarily students) were recruited for the study and were each given four classification and four debugging tasks in Python. Eye movements were recorded during all the tasks. The results indicate confusion in labeling Functional and Procedural paradigms, but not Object-Oriented. The code with predominantly functional paradigms also took the longest to complete. Changing the predominant paradigm did not affect the ability to debug the code, though developers did rate themselves with lower confidence for Functional code. We report significant differences in reading patterns during debugging, especially in the Functional code. During classification, results show that developers do not necessarily read paradigm-relevant token types.", "AI": {"tldr": "\u8be5\u7814\u7a76\u901a\u8fc7\u773c\u52a8\u8ffd\u8e2a\u5b9e\u9a8c\u63a2\u7d22Python\u4e2d\u4e0d\u540c\u7f16\u7a0b\u8303\u5f0f\uff08\u9762\u5411\u5bf9\u8c61\u3001\u8fc7\u7a0b\u5f0f\u3001\u51fd\u6570\u5f0f\uff09\u5bf9\u5f00\u53d1\u8005\u4ee3\u7801\u5206\u7c7b\u4e0e\u8c03\u8bd5\u80fd\u529b\u7684\u5f71\u54cd\uff0c\u53d1\u73b0\u51fd\u6570\u5f0f\u4ee3\u7801\u66f4\u96be\u7406\u89e3\u4e14\u8017\u65f6\u66f4\u957f\uff0c\u4f46\u8303\u5f0f\u8f6c\u6362\u5e76\u672a\u663e\u8457\u5f71\u54cd\u8c03\u8bd5\u6b63\u786e\u7387\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u8868\u660e\u67d0\u4e9b\u7f16\u7a0b\u8303\u5f0f\u7f16\u5199\u7684\u4ee3\u7801\u66f4\u96be\u7406\u89e3\uff0c\u4f46\u5c1a\u65e0\u7814\u7a76\u63a2\u8ba8\u5177\u4f53\u662f\u54ea\u4e9b\u8303\u5f0f\u76f8\u5173\u7684\u8bed\u8a00\u7279\u6027\u5f71\u54cd\u4e86\u4ee3\u7801\u7684\u7406\u89e3\u4e0e\u8c03\u8bd5\u3002\u672c\u6587\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u5f00\u5c55\u4e86\u4e00\u9879\u63a2\u7d22\u6027\u7684\u773c\u52a8\u8ffd\u8e2a\u5b9e\u8bc1\u7814\u7a76\uff0c\u62db\u52df29\u540d\u5f00\u53d1\u8005\uff08\u4e3b\u8981\u4e3a\u5b66\u751f\uff09\uff0c\u5b8c\u62104\u4e2a\u4ee3\u7801\u8303\u5f0f\u5206\u7c7b\u4efb\u52a1\u548c4\u4e2a\u8c03\u8bd5\u4efb\u52a1\uff0c\u8bb0\u5f55\u5176\u773c\u52a8\u6570\u636e\u4ee5\u5206\u6790\u9605\u8bfb\u6a21\u5f0f\u3002", "result": "\u53c2\u4e0e\u8005\u5728\u533a\u5206\u51fd\u6570\u5f0f\u4e0e\u8fc7\u7a0b\u5f0f\u8303\u5f0f\u65f6\u5b58\u5728\u6df7\u6dc6\uff0c\u4f46\u5bf9\u9762\u5411\u5bf9\u8c61\u8303\u5f0f\u8bc6\u522b\u8f83\u6e05\u6670\uff1b\u51fd\u6570\u5f0f\u4ee3\u7801\u4efb\u52a1\u8017\u65f6\u6700\u957f\uff1b\u8303\u5f0f\u53d8\u5316\u672a\u663e\u8457\u5f71\u54cd\u8c03\u8bd5\u6b63\u786e\u7387\uff0c\u4f46\u5f00\u53d1\u8005\u5bf9\u51fd\u6570\u5f0f\u4ee3\u7801\u7684\u81ea\u8bc4\u4fe1\u5fc3\u8f83\u4f4e\uff1b\u773c\u52a8\u6570\u636e\u663e\u793a\u8c03\u8bd5\u51fd\u6570\u5f0f\u4ee3\u7801\u65f6\u9605\u8bfb\u6a21\u5f0f\u6709\u663e\u8457\u5dee\u5f02\uff0c\u4e14\u5206\u7c7b\u65f6\u5f00\u53d1\u8005\u672a\u5fc5\u5173\u6ce8\u8303\u5f0f\u76f8\u5173\u7684\u5173\u952e\u8bed\u8a00\u5143\u7d20\u3002", "conclusion": "\u5c3d\u7ba1\u7f16\u7a0b\u8303\u5f0f\u7684\u6539\u53d8\u672a\u76f4\u63a5\u5f71\u54cd\u8c03\u8bd5\u51c6\u786e\u6027\uff0c\u4f46\u51fd\u6570\u5f0f\u8303\u5f0f\u663e\u8457\u589e\u52a0\u4e86\u8ba4\u77e5\u8d1f\u62c5\u548c\u7406\u89e3\u96be\u5ea6\uff0c\u8868\u660e\u7279\u5b9a\u8bed\u8a00\u7279\u6027\u53ef\u80fd\u5f71\u54cd\u5f00\u53d1\u8005\u5bf9\u591a\u8303\u5f0f\u4ee3\u7801\u7684\u7406\u89e3\u6548\u7387\u3002"}}
{"id": "2511.07784", "categories": ["cs.MA"], "pdf": "https://arxiv.org/pdf/2511.07784", "abs": "https://arxiv.org/abs/2511.07784", "authors": ["Haolun Wu", "Zhenkun Li", "Lingyao Li"], "title": "Can LLM Agents Really Debate? A Controlled Study of Multi-Agent Debate in Logical Reasoning", "comment": "20 pages, 6 figures", "summary": "Multi-agent debate (MAD) has recently emerged as a promising framework for improving the reasoning performance of large language models (LLMs). Yet, whether LLM agents can genuinely engage in deliberative reasoning, beyond simple ensembling or majority voting, remains unclear. We address this question through a controlled study using the Knight--Knave--Spy logic puzzle, which enables precise, step-wise evaluation of debate outcomes and processes under verifiable ground truth. We systematically set up six structural and cognitive factors, including agent team size, composition, confidence visibility, debate order, debate depth, and task difficulty, to disentangle their respective effects on collective reasoning. Our results show that intrinsic reasoning strength and group diversity are the dominant drivers of debate success, while structural parameters such as order or confidence visibility offer limited gains. Beyond outcomes, process-level analyses identify key behavioral patterns: majority pressure suppresses independent correction, effective teams overturn incorrect consensus, and rational, validity-aligned reasoning most strongly predicts improvement. These findings provide valuable insights into how and why LLM debates succeed or fail, offering guidance for designing interpretable and truth-seeking multi-agent reasoning systems.", "AI": {"tldr": "\u8be5\u7814\u7a76\u901a\u8fc7\u9a91\u58eb-\u9a97\u5b50-\u95f4\u8c0d\u903b\u8f91\u8c1c\u9898\uff0c\u7cfb\u7edf\u8bc4\u4f30\u591a\u667a\u80fd\u4f53\u8fa9\u8bba\uff08MAD\uff09\u4e2d\u7ed3\u6784\u4e0e\u8ba4\u77e5\u56e0\u7d20\u5bf9\u5927\u8bed\u8a00\u6a21\u578b\u96c6\u4f53\u63a8\u7406\u6548\u679c\u7684\u5f71\u54cd\uff0c\u53d1\u73b0\u5185\u5728\u63a8\u7406\u80fd\u529b\u548c\u7fa4\u4f53\u591a\u6837\u6027\u662f\u6210\u529f\u5173\u952e\uff0c\u800c\u591a\u6570\u538b\u529b\u4f1a\u6291\u5236\u72ec\u7acb\u7ea0\u9519\u3002", "motivation": "\u6f84\u6e05\u5927\u8bed\u8a00\u6a21\u578b\u5728\u591a\u667a\u80fd\u4f53\u8fa9\u8bba\u4e2d\u662f\u5426\u771f\u6b63\u5177\u5907\u5ba1\u8bae\u5f0f\u63a8\u7406\u80fd\u529b\uff0c\u800c\u975e\u4ec5\u4f9d\u8d56\u7b80\u5355\u96c6\u6210\u6216\u591a\u6570\u6295\u7968\uff0c\u5e76\u6df1\u5165\u7406\u89e3\u5f71\u54cd\u5176\u96c6\u4f53\u63a8\u7406\u6210\u8d25\u7684\u5173\u952e\u56e0\u7d20\u3002", "method": "\u5728\u5177\u6709\u53ef\u9a8c\u8bc1\u771f\u503c\u7684Knight\u2013Knave\u2013Spy\u903b\u8f91\u8c1c\u9898\u4e0a\u8fdb\u884c\u53d7\u63a7\u5b9e\u9a8c\uff0c\u7cfb\u7edf\u64cd\u63a7\u516d\u4e2a\u7ed3\u6784\u6027\u548c\u8ba4\u77e5\u6027\u56e0\u7d20\uff08\u56e2\u961f\u89c4\u6a21\u3001\u7ec4\u6210\u3001\u7f6e\u4fe1\u5ea6\u53ef\u89c1\u6027\u3001\u8fa9\u8bba\u987a\u5e8f\u3001\u8fa9\u8bba\u6df1\u5ea6\u3001\u4efb\u52a1\u96be\u5ea6\uff09\uff0c\u5e76\u7ed3\u5408\u7ed3\u679c\u4e0e\u8fc7\u7a0b\u5c42\u9762\u7684\u5206\u6790\u3002", "result": "\u5185\u5728\u63a8\u7406\u80fd\u529b\u548c\u7fa4\u4f53\u591a\u6837\u6027\u662f\u8fa9\u8bba\u6210\u529f\u7684\u4e3b\u5bfc\u56e0\u7d20\uff1b\u7ed3\u6784\u6027\u53c2\u6570\uff08\u5982\u987a\u5e8f\u3001\u7f6e\u4fe1\u5ea6\u53ef\u89c1\u6027\uff09\u589e\u76ca\u6709\u9650\uff1b\u8fc7\u7a0b\u5206\u6790\u63ed\u793a\u591a\u6570\u538b\u529b\u6291\u5236\u72ec\u7acb\u7ea0\u9519\u3001\u6709\u6548\u56e2\u961f\u80fd\u63a8\u7ffb\u9519\u8bef\u5171\u8bc6\u3001\u57fa\u4e8e\u6709\u6548\u6027\u7684\u7406\u6027\u63a8\u7406\u6700\u80fd\u9884\u6d4b\u6539\u8fdb\u3002", "conclusion": "LLM\u591a\u667a\u80fd\u4f53\u8fa9\u8bba\u7684\u6210\u529f\u4f9d\u8d56\u4e8e\u4e2a\u4f53\u80fd\u529b\u4e0e\u591a\u6837\u6027\uff0c\u800c\u975e\u8868\u9762\u7ed3\u6784\u8bbe\u8ba1\uff1b\u7814\u7a76\u4e3a\u6784\u5efa\u53ef\u89e3\u91ca\u3001\u6c42\u771f\u7684\u591a\u667a\u80fd\u4f53\u63a8\u7406\u7cfb\u7edf\u63d0\u4f9b\u4e86\u5b9e\u8bc1\u4f9d\u636e\u548c\u8bbe\u8ba1\u6307\u5bfc\u3002"}}
{"id": "2511.07427", "categories": ["cs.DC", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.07427", "abs": "https://arxiv.org/abs/2511.07427", "authors": ["Tuowei Wang", "Minxing Huang", "Fengzu Li", "Ligeng Chen", "Jinrui Zhang", "Ju Ren"], "title": "DynaKV: Enabling Accurate and Efficient Long-Sequence LLM Decoding on Smartphones", "comment": null, "summary": "As the demand for human-like reasoning, multi-turn dialogues, and long-form responses grows, large language models (LLMs) are increasingly expected to support efficient and effective long-sequence decoding. However, due to limited DRAM capacity, long-seuqence LLM decoding on smartphones is constrained by the key-value cache (KVCache), whose memory footprint increases linearly with sequence length. Retrieval-based methods mitigate DRAM pressure by offloading KVCache to flash and retrieving query-relevant entries through cluster-based indexing. Unfortunately, as decoding progresses, KVCache distribution shifts render static or local cluster updates progressively misaligned, excluding essential entries or fetching redundant ones. These issues are further exacerbated by smartphone-specific limitations in bandwidth, IOPS, and memory capacity.\n  We propose DynaKV, the first adaptive KVCache management approach that jointly addresses accuracy and efficiency for long-sequence decoding on smartphones. DynaKV integrates three key techniques: (1) Migration-Free Cluster Adaptation, which adaptively splits clusters during retrieval without incurring additional transfers; (2) Continuity-Centric Flash Management, which co-locates correlated entries and clusters and employs a dual-head layout for efficient updates; and (3) Memory-Efficient Cache Design, which virtualizes cache space across DRAM and flash and extends replacement policies to align with cluster-level access patterns. Evaluations demonstrate that DynaKV improves retrieval accuracy and reduces end-to-end latency compared to state-of-the-art solutions, achieving average gains of $1.38\\times$ in accuracy and $1.47\\times$ speedups. Furthermore, the insights of DynaKV naturally extend to other long-context workloads and multi-tier memory hierarchies, underscoring its broader applicability.", "AI": {"tldr": "DynaKV \u662f\u4e00\u79cd\u9762\u5411\u667a\u80fd\u624b\u673a\u7684\u81ea\u9002\u5e94 KVCache \u7ba1\u7406\u65b9\u6cd5\uff0c\u901a\u8fc7\u65e0\u8fc1\u79fb\u805a\u7c7b\u9002\u914d\u3001\u4ee5\u8fde\u7eed\u6027\u4e3a\u4e2d\u5fc3\u7684\u95ea\u5b58\u7ba1\u7406\u548c\u5185\u5b58\u9ad8\u6548\u7f13\u5b58\u8bbe\u8ba1\uff0c\u5728\u957f\u5e8f\u5217\u89e3\u7801\u4e2d\u540c\u65f6\u63d0\u5347\u51c6\u786e\u7387\u4e0e\u6548\u7387\u3002", "motivation": "\u5728\u667a\u80fd\u624b\u673a\u4e0a\u8fd0\u884c\u957f\u5e8f\u5217\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u63a8\u7406\u65f6\uff0c\u53d7\u9650\u4e8e DRAM \u5bb9\u91cf\u548c\u95ea\u5b58\u5e26\u5bbd\uff0c\u4f20\u7edf\u57fa\u4e8e\u68c0\u7d22\u7684 KVCache \u7ba1\u7406\u65b9\u6cd5\u56e0\u9759\u6001\u6216\u5c40\u90e8\u805a\u7c7b\u66f4\u65b0\u65e0\u6cd5\u9002\u5e94\u89e3\u7801\u8fc7\u7a0b\u4e2d KVCache \u5206\u5e03\u7684\u53d8\u5316\uff0c\u5bfc\u81f4\u5173\u952e\u6761\u76ee\u9057\u6f0f\u6216\u5197\u4f59\u6570\u636e\u8bfb\u53d6\uff0c\u5f71\u54cd\u51c6\u786e\u6027\u548c\u5ef6\u8fdf\u3002", "method": "DynaKV \u63d0\u51fa\u4e09\u9879\u6838\u5fc3\u6280\u672f\uff1a(1) \u65e0\u8fc1\u79fb\u805a\u7c7b\u9002\u914d\uff0c\u5728\u68c0\u7d22\u8fc7\u7a0b\u4e2d\u52a8\u6001\u62c6\u5206\u805a\u7c7b\u800c\u4e0d\u5f15\u5165\u989d\u5916\u6570\u636e\u4f20\u8f93\uff1b(2) \u4ee5\u8fde\u7eed\u6027\u4e3a\u4e2d\u5fc3\u7684\u95ea\u5b58\u7ba1\u7406\uff0c\u5c06\u76f8\u5173\u6761\u76ee\u548c\u805a\u7c7b\u5171\u7f6e\u5e76\u91c7\u7528\u53cc\u5934\u5e03\u5c40\u4ee5\u652f\u6301\u9ad8\u6548\u66f4\u65b0\uff1b(3) \u5185\u5b58\u9ad8\u6548\u7f13\u5b58\u8bbe\u8ba1\uff0c\u8de8 DRAM \u4e0e\u95ea\u5b58\u865a\u62df\u5316\u7f13\u5b58\u7a7a\u95f4\uff0c\u5e76\u6269\u5c55\u66ff\u6362\u7b56\u7565\u4ee5\u5339\u914d\u805a\u7c7b\u7ea7\u8bbf\u95ee\u6a21\u5f0f\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cDynaKV \u76f8\u6bd4\u5f53\u524d\u6700\u4f18\u65b9\u6848\u5e73\u5747\u63d0\u5347 1.38 \u500d\u68c0\u7d22\u51c6\u786e\u7387\u548c 1.47 \u500d\u7aef\u5230\u7aef\u901f\u5ea6\uff0c\u5e76\u53ef\u81ea\u7136\u62d3\u5c55\u81f3\u5176\u4ed6\u957f\u4e0a\u4e0b\u6587\u4efb\u52a1\u548c\u591a\u7ea7\u5185\u5b58\u67b6\u6784\u3002", "conclusion": "DynaKV \u9996\u6b21\u5728\u667a\u80fd\u624b\u673a\u4e0a\u5b9e\u73b0\u4e86\u517c\u987e\u51c6\u786e\u6027\u548c\u6548\u7387\u7684\u81ea\u9002\u5e94 KVCache \u7ba1\u7406\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u957f\u5e8f\u5217 LLM \u63a8\u7406\u4e2d\u7684\u5185\u5b58\u74f6\u9888\u95ee\u9898\uff0c\u5177\u6709\u826f\u597d\u7684\u901a\u7528\u6027\u548c\u5b9e\u7528\u4ef7\u503c\u3002"}}
{"id": "2511.07770", "categories": ["cs.NI"], "pdf": "https://arxiv.org/pdf/2511.07770", "abs": "https://arxiv.org/abs/2511.07770", "authors": ["Zewei Guo", "Zhen Jia", "JinXiao Zhu", "Wenhao Huang", "Yin Chen"], "title": "A Large-Scale Dataset and Reproducible Framework for RF Fingerprinting on IEEE 802.11g Same-Model Devices", "comment": null, "summary": "Radio frequency (RF) fingerprinting exploits hardware imperfections for device identification, but distinguishing between same-model devices remains challenging due to their minimal hardware variations. Existing datasets for RF fingerprinting are constrained by small device scales and heterogeneous models, which hinders robust training and fair evaluation for machine learning models. To address this gap, we introduce a large-scale dataset of same-model devices along with a fully reproducible, open-source experimental framework. The dataset is built using 123 identical commercial IEEE 802.11g devices and contains 35.42 million raw I/Q samples from the preambles and corresponding 1.85 million RF features. The open-source framework further ensures full reproducibility from data collection to final evaluation. Within this framework, a Random Forest-based algorithm is proposed to achieve 89.06% identification accuracy on this dataset. Extensive experimental evaluations further confirm the relationships between the extracted features.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u5927\u89c4\u6a21\u540c\u578b\u53f7\u8bbe\u5907\u7684\u5c04\u9891\u6307\u7eb9\u6570\u636e\u96c6\u53ca\u5f00\u6e90\u53ef\u590d\u73b0\u5b9e\u9a8c\u6846\u67b6\uff0c\u5305\u542b123\u4e2a\u76f8\u540c\u5546\u7528IEEE 802.11g\u8bbe\u5907\u76843542\u4e07\u539f\u59cbI/Q\u6837\u672c\u548c185\u4e07\u5c04\u9891\u7279\u5f81\uff0c\u5e76\u57fa\u4e8e\u968f\u673a\u68ee\u6797\u7b97\u6cd5\u5b9e\u73b0\u4e8689.06%\u7684\u8bc6\u522b\u51c6\u786e\u7387\u3002", "motivation": "\u73b0\u6709\u5c04\u9891\u6307\u7eb9\u6570\u636e\u96c6\u53d7\u9650\u4e8e\u8bbe\u5907\u6570\u91cf\u5c11\u4e14\u578b\u53f7\u5f02\u6784\uff0c\u96be\u4ee5\u652f\u6301\u5bf9\u540c\u578b\u53f7\u8bbe\u5907\u7684\u6709\u6548\u533a\u5206\u548c\u673a\u5668\u5b66\u4e60\u6a21\u578b\u7684\u9c81\u68d2\u8bad\u7ec3\u4e0e\u516c\u5e73\u8bc4\u4f30\u3002", "method": "\u6784\u5efa\u5305\u542b123\u4e2a\u76f8\u540c\u578b\u53f7IEEE 802.11g\u8bbe\u5907\u7684\u5927\u89c4\u6a21\u6570\u636e\u96c6\uff0c\u91c7\u96c63542\u4e07\u539f\u59cbI/Q\u6837\u672c\u5e76\u63d0\u53d6185\u4e07\u5c04\u9891\u7279\u5f81\uff1b\u5f00\u53d1\u5b8c\u5168\u5f00\u6e90\u3001\u53ef\u590d\u73b0\u7684\u5b9e\u9a8c\u6846\u67b6\uff0c\u5e76\u91c7\u7528\u57fa\u4e8e\u968f\u673a\u68ee\u6797\u7684\u7b97\u6cd5\u8fdb\u884c\u8bbe\u5907\u8bc6\u522b\u3002", "result": "\u5728\u6240\u63d0\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e8689.06%\u7684\u8bbe\u5907\u8bc6\u522b\u51c6\u786e\u7387\uff0c\u5e76\u901a\u8fc7\u5927\u91cf\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u6240\u63d0\u53d6\u7279\u5f81\u4e4b\u95f4\u7684\u5173\u8054\u6027\u3002", "conclusion": "\u8be5\u7814\u7a76\u901a\u8fc7\u63d0\u4f9b\u5927\u89c4\u6a21\u540c\u578b\u53f7\u8bbe\u5907\u6570\u636e\u96c6\u4e0e\u5b8c\u6574\u53ef\u590d\u73b0\u6846\u67b6\uff0c\u663e\u8457\u63a8\u52a8\u4e86\u5c04\u9891\u6307\u7eb9\u8bc6\u522b\u9886\u57df\u7684\u53d1\u5c55\uff0c\u5c24\u5176\u5728\u533a\u5206\u9ad8\u5ea6\u76f8\u4f3c\u8bbe\u5907\u65b9\u9762\u53d6\u5f97\u5b9e\u8d28\u6027\u8fdb\u5c55\u3002"}}
{"id": "2511.07645", "categories": ["cs.SE", "cs.AI", "cs.CR"], "pdf": "https://arxiv.org/pdf/2511.07645", "abs": "https://arxiv.org/abs/2511.07645", "authors": ["Tyler Slater"], "title": "A Self-Improving Architecture for Dynamic Safety in Large Language Models", "comment": "Under review at the journal Information and Software Technology (Special Issue on Software Architecture for AI-Driven Systems)", "summary": "Context: The integration of Large Language Models (LLMs) into core software systems is accelerating. However, existing software architecture patterns are static, while current safety assurance methods are not scalable, leaving systems vulnerable to novel adversarial threats.\n  Objective: To design, implement, and evaluate a novel software architecture that enables an AI-driven system to autonomously and continuously adapt its own safety protocols at runtime.\n  Method: We propose the Self-Improving Safety Framework (SISF), a runtime architecture that couples an unprotected, unaligned base LLM (mistralai/Mistral-7B-v0.1) with a dynamic feedback loop. This loop consists of an AI Adjudicator (GPT-4o) for breach detection and a Policy Synthesis Module (GPT-4 Turbo) that autonomously generates new, generalized safety policies (both heuristic and semantic) in response to failures.\n  Results: We conducted a dynamic learning evaluation using the 520-prompt AdvBench dataset. The unprotected model was 100% vulnerable. Our SISF, starting from zero policies, demonstrated a clear learning curve: it detected 237 breaches, autonomously synthesized 234 new policies, and reduced the overall Attack Success Rate (ASR) to 45.58%. In a subsequent test on 520 benign prompts, the SISF achieved a 0.00% False Positive Rate (FPR), proving its ability to adapt without compromising user utility.\n  Conclusion: An architectural approach to AI safety, based on the principles of self-adaptation, is a viable and effective strategy. Our framework demonstrates a practical path towards building more robust, resilient, and scalable AI-driven systems, shifting safety assurance from a static, pre-deployment activity to an automated, runtime process.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u81ea\u6539\u8fdb\u5b89\u5168\u6846\u67b6\uff08SISF\uff09\u7684\u65b0\u578b\u8fd0\u884c\u65f6\u67b6\u6784\uff0c\u4f7fAI\u7cfb\u7edf\u80fd\u5728\u8fd0\u884c\u8fc7\u7a0b\u4e2d\u81ea\u4e3b\u68c0\u6d4b\u5b89\u5168\u6f0f\u6d1e\u5e76\u52a8\u6001\u751f\u6210\u65b0\u7684\u5b89\u5168\u7b56\u7565\uff0c\u4ece\u800c\u663e\u8457\u964d\u4f4e\u653b\u51fb\u6210\u529f\u7387\u4e14\u4e0d\u4ea7\u751f\u8bef\u62a5\u3002", "motivation": "\u73b0\u6709\u8f6f\u4ef6\u67b6\u6784\u6a21\u5f0f\u9759\u6001\u4e14\u5b89\u5168\u673a\u5236\u4e0d\u53ef\u6269\u5c55\uff0c\u96be\u4ee5\u5e94\u5bf9\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u96c6\u6210\u5e26\u6765\u7684\u65b0\u578b\u5bf9\u6297\u5a01\u80c1\uff0c\u4e9f\u9700\u4e00\u79cd\u80fd\u52a8\u6001\u9002\u5e94\u7684\u5b89\u5168\u67b6\u6784\u3002", "method": "SISF\u67b6\u6784\u7ed3\u5408\u4e00\u4e2a\u672a\u53d7\u4fdd\u62a4\u7684\u57fa\u7840LLM\uff08Mistral-7B-v0.1\uff09\u3001\u4e00\u4e2a\u7528\u4e8e\u68c0\u6d4b\u8fdd\u89c4\u7684AI\u4ef2\u88c1\u5668\uff08GPT-4o\uff09\u548c\u4e00\u4e2a\u7b56\u7565\u5408\u6210\u6a21\u5757\uff08GPT-4 Turbo\uff09\uff0c\u540e\u8005\u80fd\u6839\u636e\u5931\u8d25\u6848\u4f8b\u81ea\u52a8\u751f\u6210\u65b0\u7684\u542f\u53d1\u5f0f\u4e0e\u8bed\u4e49\u5b89\u5168\u7b56\u7565\u3002", "result": "\u5728AdvBench\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cSISF\u4ece\u96f6\u7b56\u7565\u5f00\u59cb\uff0c\u6210\u529f\u68c0\u6d4b237\u6b21\u653b\u51fb\u3001\u751f\u6210234\u6761\u65b0\u7b56\u7565\uff0c\u5c06\u653b\u51fb\u6210\u529f\u7387\u4ece100%\u964d\u81f345.58%\uff0c\u5e76\u5728\u826f\u6027\u63d0\u793a\u6d4b\u8bd5\u4e2d\u5b9e\u73b00.00%\u7684\u8bef\u62a5\u7387\u3002", "conclusion": "\u57fa\u4e8e\u81ea\u9002\u5e94\u539f\u5219\u7684\u67b6\u6784\u5316AI\u5b89\u5168\u65b9\u6cd5\u662f\u53ef\u884c\u4e14\u6709\u6548\u7684\uff0c\u80fd\u591f\u5c06\u5b89\u5168\u4fdd\u8bc1\u4ece\u9759\u6001\u9884\u90e8\u7f72\u8f6c\u53d8\u4e3a\u81ea\u52a8\u5316\u8fd0\u884c\u65f6\u8fc7\u7a0b\uff0c\u63d0\u5347AI\u7cfb\u7edf\u7684\u9c81\u68d2\u6027\u4e0e\u53ef\u6269\u5c55\u6027\u3002"}}
{"id": "2511.07574", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2511.07574", "abs": "https://arxiv.org/abs/2511.07574", "authors": ["Vasilis Bountris", "Lauritz Thamsen", "Ulf Leser"], "title": "HyProv: Hybrid Provenance Management for Scientific Workflows", "comment": "10 pages, 2 figures", "summary": "Provenance plays a crucial role in scientific workflow execution, for instance by providing data for failure analysis, real-time monitoring, or statistics on resource utilization for right-sizing allocations. The workflows themselves, however, become increasingly complex in terms of involved components. Furthermore, they are executed on distributed cluster infrastructures, which makes the real-time collection, integration, and analysis of provenance data challenging. Existing provenance systems struggle to balance scalability, real-time processing, online provenance analytics, and integration across different components and compute resources. Moreover, most provenance solutions are not workflow-aware; by focusing on arbitrary workloads, they miss opportunities for workflow systems where optimization and analysis can exploit the availability of a workflow specification that dictates, to some degree, task execution orders and provides abstractions for physical tasks at a logical level.\n  In this paper, we present HyProv, a hybrid provenance management system that combines centralized and federated paradigms to offer scalable, online, and workflow-aware queries over workflow provenance traces. HyProv uses a centralized component for efficient management of the small and stable workflow-specification-specific provenance, and complements this with federated querying over different scalable monitoring and provenance databases for the large-scale execution logs. This enables low-latency access to current execution data. Furthermore, the design supports complex provenance queries, which we exemplify for the workflow system Airflow in combination with the resource manager Kubernetes. Our experiments indicate that HyProv scales to large workflows, answers provenance queries with sub-second latencies, and adds only modest CPU and memory overhead to the cluster.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86HyProv\uff0c\u4e00\u79cd\u6df7\u5408\u5f0f\u6eaf\u6e90\u7ba1\u7406\u7cfb\u7edf\uff0c\u7ed3\u5408\u96c6\u4e2d\u5f0f\u4e0e\u8054\u90a6\u5f0f\u67b6\u6784\uff0c\u4ee5\u5b9e\u73b0\u5bf9\u79d1\u5b66\u5de5\u4f5c\u6d41\u6267\u884c\u8fc7\u7a0b\u4e2d\u4ea7\u751f\u7684\u6eaf\u6e90\u6570\u636e\u8fdb\u884c\u9ad8\u6548\u3001\u5b9e\u65f6\u4e14\u5de5\u4f5c\u6d41\u611f\u77e5\u7684\u67e5\u8be2\u3002", "motivation": "\u73b0\u6709\u6eaf\u6e90\u7cfb\u7edf\u96be\u4ee5\u5728\u53ef\u6269\u5c55\u6027\u3001\u5b9e\u65f6\u5904\u7406\u3001\u5728\u7ebf\u6eaf\u6e90\u5206\u6790\u4ee5\u53ca\u8de8\u7ec4\u4ef6\u548c\u8ba1\u7b97\u8d44\u6e90\u7684\u96c6\u6210\u4e4b\u95f4\u53d6\u5f97\u5e73\u8861\uff0c\u4e14\u5927\u591a\u6570\u7cfb\u7edf\u7f3a\u4e4f\u5bf9\u5de5\u4f5c\u6d41\u7ed3\u6784\u7684\u8ba4\u77e5\uff0c\u65e0\u6cd5\u5229\u7528\u5de5\u4f5c\u6d41\u89c4\u8303\u8fdb\u884c\u4f18\u5316\u3002", "method": "HyProv\u91c7\u7528\u6df7\u5408\u67b6\u6784\uff1a\u4f7f\u7528\u96c6\u4e2d\u5f0f\u7ec4\u4ef6\u7ba1\u7406\u5c0f\u89c4\u6a21\u4e14\u7a33\u5b9a\u7684\u5de5\u4f5c\u6d41\u89c4\u8303\u76f8\u5173\u6eaf\u6e90\u4fe1\u606f\uff0c\u5e76\u901a\u8fc7\u8054\u90a6\u67e5\u8be2\u673a\u5236\u8bbf\u95ee\u5206\u5e03\u5f0f\u7684\u3001\u53ef\u6269\u5c55\u7684\u76d1\u63a7\u4e0e\u6267\u884c\u65e5\u5fd7\u6570\u636e\u5e93\uff0c\u4ece\u800c\u652f\u6301\u4f4e\u5ef6\u8fdf\u7684\u590d\u6742\u6eaf\u6e90\u67e5\u8be2\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cHyProv\u80fd\u591f\u6269\u5c55\u81f3\u5927\u89c4\u6a21\u5de5\u4f5c\u6d41\uff0c\u4ee5\u4e9a\u79d2\u7ea7\u5ef6\u8fdf\u54cd\u5e94\u6eaf\u6e90\u67e5\u8be2\uff0c\u5e76\u4ec5\u5f15\u5165\u8f83\u4f4e\u7684CPU\u548c\u5185\u5b58\u5f00\u9500\u3002", "conclusion": "HyProv\u901a\u8fc7\u7ed3\u5408\u96c6\u4e2d\u5f0f\u4e0e\u8054\u90a6\u5f0f\u6eaf\u6e90\u7ba1\u7406\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u73b0\u6709\u7cfb\u7edf\u5728\u5b9e\u65f6\u6027\u3001\u53ef\u6269\u5c55\u6027\u548c\u5de5\u4f5c\u6d41\u611f\u77e5\u65b9\u9762\u7684\u4e0d\u8db3\uff0c\u4e3a\u590d\u6742\u5206\u5e03\u5f0f\u5de5\u4f5c\u6d41\u63d0\u4f9b\u4e86\u9ad8\u6548\u7684\u6eaf\u6e90\u652f\u6301\u3002"}}
{"id": "2511.08395", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2511.08395", "abs": "https://arxiv.org/abs/2511.08395", "authors": ["Xingyu Liu", "Jiawei Liang", "Yipu Zhang", "Linfeng Du", "Chaofang Ma", "Hui Yu", "Jiang Xu", "Wei Zhang"], "title": "DRACO: Co-design for DSP-Efficient Rigid Body Dynamics Accelerator", "comment": null, "summary": "We propose a hardware-efficient RBD accelerator based on FPGA, introducing three key innovations. First, we propose a precision-aware quantization framework that reduces DSP demand while preserving motion accuracy. This is also the first study to systematically evaluate quantization impact on robot control and motion for hardware acceleration. Second, we leverage a division deferring optimization in mass matrix inversion algorithm, which decouples reciprocal operations from the longest latency path to improve the performance. Finally, we present an inter-module DSP reuse methodology to improve DSP utilization and save DSP usage. Experiment results show that our work achieves up to 8x throughput improvement and 7.4x latency reduction over state-of-the-art RBD accelerators across various robot types, demonstrating its effectiveness and scalability for high-DOF robotic systems.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eFPGA\u7684\u786c\u4ef6\u9ad8\u6548RBD\u52a0\u901f\u5668\uff0c\u901a\u8fc7\u7cbe\u5ea6\u611f\u77e5\u91cf\u5316\u3001\u9664\u6cd5\u5ef6\u8fdf\u4f18\u5316\u548c\u6a21\u5757\u95f4DSP\u590d\u7528\u4e09\u9879\u521b\u65b0\uff0c\u5728\u591a\u79cd\u673a\u5668\u4eba\u7cfb\u7edf\u4e0a\u5b9e\u73b0\u4e86\u6700\u9ad88\u500d\u541e\u5410\u91cf\u63d0\u5347\u548c7.4\u500d\u5ef6\u8fdf\u964d\u4f4e\u3002", "motivation": "\u73b0\u6709RBD\uff08\u521a\u4f53\u52a8\u529b\u5b66\uff09\u52a0\u901f\u5668\u5728\u9ad8\u81ea\u7531\u5ea6\u673a\u5668\u4eba\u7cfb\u7edf\u4e2d\u5b58\u5728\u786c\u4ef6\u8d44\u6e90\u5229\u7528\u7387\u4f4e\u3001\u5ef6\u8fdf\u9ad8\u548c\u541e\u5410\u91cf\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u4e9f\u9700\u4e00\u79cd\u66f4\u9ad8\u6548\u7684\u786c\u4ef6\u52a0\u901f\u65b9\u6848\u3002", "method": "\u63d0\u51fa\u4e09\u9879\u5173\u952e\u6280\u672f\uff1a1\uff09\u7cbe\u5ea6\u611f\u77e5\u91cf\u5316\u6846\u67b6\uff0c\u5728\u964d\u4f4eDSP\u9700\u6c42\u7684\u540c\u65f6\u4fdd\u6301\u8fd0\u52a8\u7cbe\u5ea6\uff1b2\uff09\u5728\u8d28\u91cf\u77e9\u9635\u6c42\u9006\u7b97\u6cd5\u4e2d\u91c7\u7528\u9664\u6cd5\u5ef6\u8fdf\u4f18\u5316\uff0c\u5c06\u5012\u6570\u8fd0\u7b97\u4ece\u5173\u952e\u8def\u5f84\u4e2d\u89e3\u8026\uff1b3\uff09\u6a21\u5757\u95f4DSP\u590d\u7528\u65b9\u6cd5\uff0c\u63d0\u9ad8DSP\u5229\u7528\u7387\u5e76\u51cf\u5c11\u4f7f\u7528\u91cf\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u52a0\u901f\u5668\u76f8\u6bd4\u5f53\u524d\u6700\u5148\u8fdb\u7684RBD\u52a0\u901f\u5668\uff0c\u5728\u591a\u79cd\u673a\u5668\u4eba\u7c7b\u578b\u4e0a\u5b9e\u73b0\u4e86\u6700\u9ad88\u500d\u7684\u541e\u5410\u91cf\u63d0\u5347\u548c7.4\u500d\u7684\u5ef6\u8fdf\u964d\u4f4e\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684FPGA\u52a0\u901f\u5668\u5728\u4fdd\u8bc1\u673a\u5668\u4eba\u63a7\u5236\u7cbe\u5ea6\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\u4e0e\u8d44\u6e90\u6548\u7387\uff0c\u5177\u5907\u826f\u597d\u7684\u53ef\u6269\u5c55\u6027\uff0c\u9002\u7528\u4e8e\u9ad8\u81ea\u7531\u5ea6\u673a\u5668\u4eba\u7cfb\u7edf\u3002"}}
{"id": "2511.07698", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2511.07698", "abs": "https://arxiv.org/abs/2511.07698", "authors": ["Mohammadjavad Mehditabar", "Saurabhsingh Rajput", "Antonio Mastropaolo", "Tushar Sharma"], "title": "Smart but Costly? Benchmarking LLMs on Functional Accuracy and Energy Efficiency", "comment": null, "summary": "The rapid advancement of AI technologies and their accelerated adoption in software development necessitates a systematic evaluation of their environmental impact alongside functional correctness. While prior studies have examined sustainability in large language models, existing approaches lack systematic frameworks for evaluating accuracy-energy trade-offs in Code Language Models (CLMs). In this paper, we present a framework, BRACE, to benchmark CLMs on a unified scale of energy efficiency and functional correctness (referred to as accuracy). We benchmark 22 state-of-the-art models on code generation and summarization tasks, proposing two rating methods: Concentric Incremental Rating Circles (CIRC) and Observation to Expectation Rating (OTER). CIRC provides deterministic Euclidean-based rankings with static trade-offs that are robust to outliers, and OTER offers trend-aware evaluation with dynamic trade-offs that capture the complex correlation between energy and accuracy, each offering a distinct perspective and addressing the problem in a unique way. These rating methods enable us to rate LLMs on a 1-5 scale reflecting their combined capabilities in terms of energy efficiency and functional correctness. Our analysis reveals models generally perform better in the code summarization tasks as they are not enforced to generate a grammar-based and syntactically correct output. Also, we find that models' size does not have a significant impact on their ratings, indicating that if models utilize their parameters efficiently, they can be ranked higher on these scales. The proposed BRACE framework empowers practitioners to make evidence-based model selections that balance sustainability with task requirements, guiding rating choice -- CIRC for deterministic comparisons or OTER for trend-aware evaluation -- based on deployment priorities.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86BRACE\u6846\u67b6\uff0c\u7528\u4e8e\u5728\u7edf\u4e00\u5c3a\u5ea6\u4e0a\u8bc4\u4f30\u4ee3\u7801\u8bed\u8a00\u6a21\u578b\uff08CLM\uff09\u5728\u80fd\u8017\u4e0e\u529f\u80fd\u6b63\u786e\u6027\uff08\u51c6\u786e\u6027\uff09\u4e4b\u95f4\u7684\u6743\u8861\uff0c\u5e76\u5f15\u5165\u4e24\u79cd\u8bc4\u7ea7\u65b9\u6cd5CIRC\u548cOTER\u5bf922\u4e2a\u4e3b\u6d41\u6a21\u578b\u8fdb\u884c\u57fa\u51c6\u6d4b\u8bd5\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u7f3a\u4e4f\u7cfb\u7edf\u6027\u6846\u67b6\u6765\u8bc4\u4f30\u4ee3\u7801\u8bed\u8a00\u6a21\u578b\u5728\u51c6\u786e\u6027\u548c\u80fd\u8017\u4e4b\u95f4\u7684\u6743\u8861\uff0c\u800cAI\u6280\u672f\u5728\u8f6f\u4ef6\u5f00\u53d1\u4e2d\u7684\u5feb\u901f\u5e94\u7528\u4e9f\u9700\u5bf9\u5176\u73af\u5883\u5f71\u54cd\u8fdb\u884c\u7cfb\u7edf\u8bc4\u4f30\u3002", "method": "\u63d0\u51faBRACE\u6846\u67b6\uff0c\u5305\u542b\u4e24\u79cd\u8bc4\u7ea7\u65b9\u6cd5\uff1a\u57fa\u4e8e\u6b27\u6c0f\u8ddd\u79bb\u3001\u5bf9\u5f02\u5e38\u503c\u9c81\u68d2\u7684\u9759\u6001\u6743\u8861\u65b9\u6cd5CIRC\uff0c\u4ee5\u53ca\u80fd\u6355\u6349\u80fd\u8017\u4e0e\u51c6\u786e\u6027\u590d\u6742\u5173\u7cfb\u7684\u52a8\u6001\u8d8b\u52bf\u611f\u77e5\u65b9\u6cd5OTER\uff1b\u5728\u4ee3\u7801\u751f\u6210\u4e0e\u6458\u8981\u4efb\u52a1\u4e0a\u5bf922\u4e2a\u6a21\u578b\u8fdb\u884c\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5e76\u63091-5\u5206\u5236\u8bc4\u7ea7\u3002", "result": "\u5b9e\u9a8c\u53d1\u73b0\u6a21\u578b\u5728\u4ee3\u7801\u6458\u8981\u4efb\u52a1\u4e2d\u8868\u73b0\u66f4\u4f18\uff08\u56e0\u65e0\u9700\u751f\u6210\u8bed\u6cd5\u6b63\u786e\u7684\u4ee3\u7801\uff09\uff0c\u4e14\u6a21\u578b\u89c4\u6a21\u5bf9\u5176\u8bc4\u7ea7\u65e0\u663e\u8457\u5f71\u54cd\uff0c\u8868\u660e\u53c2\u6570\u5229\u7528\u6548\u7387\u6bd4\u6a21\u578b\u5927\u5c0f\u66f4\u91cd\u8981\u3002", "conclusion": "BRACE\u6846\u67b6\u652f\u6301\u5f00\u53d1\u8005\u6839\u636e\u90e8\u7f72\u9700\u6c42\uff08\u786e\u5b9a\u6027\u6bd4\u8f83\u6216\u8d8b\u52bf\u611f\u77e5\u8bc4\u4f30\uff09\u9009\u62e9\u5408\u9002\u6a21\u578b\uff0c\u5728\u53ef\u6301\u7eed\u6027\u4e0e\u4efb\u52a1\u6027\u80fd\u4e4b\u95f4\u53d6\u5f97\u5e73\u8861\u3002"}}
{"id": "2511.08562", "categories": ["cs.MA"], "pdf": "https://arxiv.org/pdf/2511.08562", "abs": "https://arxiv.org/abs/2511.08562", "authors": ["Shivank", "Anurag Singha", "Fakhteh Ghanbarnejad", "Ajay K Sharma"], "title": "Climate Driven Interactions Between Malaria Transmission and Diabetes Prevalence", "comment": "21 pages, 6 figures", "summary": "Climate change is intensifying infectious and chronic diseases like malaria and diabetes, respectively, especially among the vulnerable populations. Global temperatures have risen by approximately $0.6^\\circ$C since 1950, extending the window of transmission for mosquito-borne infections and worsening outcomes in diabetes due to metabolic stress caused by heat. People living with diabetes have already weakened immune defenses and, therefore, are at an alarmingly increased risk of contraction of malaria. However, most models rarely include both ways of interaction in changing climate conditions. In the paper, we introduce a new compartmental epidemiological model based on synthetic data fitted to disease patterns of India from 2019 to 2021. The framework captures temperature-dependent transmission parameters, seasonal variability, and different disease dynamics between diabetic and non-diabetic groups within the three-compartment system. Model calibration using Multi-Start optimization combined with Sequential Quadratic Programming allows us to find outstanding differences between populations. The odds of malaria infection in diabetic individuals were found to be 1.8--4.0 times higher, with peak infection levels in 35--36\\%, as compared to 20--21\\% in the non-diabetic ones. The fitted model was able to capture well the epidemiological patterns observed, while the basic reproduction number averaged around 2.3, ranging from 0.31 to 2.75 in different seasons. Given that India's diabetic population is set to rise to about 157 million people by 2050, these findings point to a pressing need for concerted efforts toward climate-informed health strategies and monitoring systems that address both malaria and diabetes jointly.", "AI": {"tldr": "\u672c\u6587\u6784\u5efa\u4e86\u4e00\u4e2a\u65b0\u7684\u533a\u5ba4\u6d41\u884c\u75c5\u5b66\u6a21\u578b\uff0c\u7ed3\u5408\u5370\u5ea62019\u20132021\u5e74\u6570\u636e\uff0c\u63ed\u793a\u5728\u6c14\u5019\u53d8\u5316\u80cc\u666f\u4e0b\u7cd6\u5c3f\u75c5\u60a3\u8005\u611f\u67d3\u759f\u75be\u7684\u98ce\u9669\u663e\u8457\u9ad8\u4e8e\u975e\u7cd6\u5c3f\u75c5\u4eba\u7fa4\uff0c\u5e76\u5f3a\u8c03\u9700\u5236\u5b9a\u517c\u987e\u759f\u75be\u4e0e\u7cd6\u5c3f\u75c5\u7684\u6c14\u5019\u9002\u5e94\u578b\u516c\u5171\u536b\u751f\u7b56\u7565\u3002", "motivation": "\u6c14\u5019\u53d8\u5316\u52a0\u5267\u4e86\u759f\u75be\u548c\u7cd6\u5c3f\u75c5\u7b49\u4f20\u67d3\u6027\u4e0e\u6162\u6027\u75be\u75c5\u7684\u8d1f\u62c5\uff0c\u5c24\u5176\u5bf9\u8106\u5f31\u4eba\u7fa4\u5f71\u54cd\u663e\u8457\u3002\u73b0\u6709\u6a21\u578b\u5f88\u5c11\u540c\u65f6\u8003\u8651\u8fd9\u4e24\u79cd\u75be\u75c5\u5728\u6c14\u5019\u53d8\u5316\u4e0b\u7684\u76f8\u4e92\u4f5c\u7528\uff0c\u56e0\u6b64\u4e9f\u9700\u4e00\u4e2a\u6574\u5408\u4e8c\u8005\u52a8\u6001\u5173\u7cfb\u7684\u65b0\u5efa\u6a21\u6846\u67b6\u3002", "method": "\u4f5c\u8005\u57fa\u4e8e\u5408\u6210\u6570\u636e\u6784\u5efa\u4e86\u4e00\u4e2a\u4e09\u533a\u5ba4\u6d41\u884c\u75c5\u5b66\u6a21\u578b\uff0c\u7eb3\u5165\u6e29\u5ea6\u4f9d\u8d56\u7684\u4f20\u64ad\u53c2\u6570\u3001\u5b63\u8282\u53d8\u5f02\u6027\u548c\u7cd6\u5c3f\u75c5\u4e0e\u975e\u7cd6\u5c3f\u75c5\u4eba\u7fa4\u7684\u75be\u75c5\u52a8\u6001\u5dee\u5f02\uff1b\u4f7f\u7528\u591a\u8d77\u70b9\u4f18\u5316\u7ed3\u5408\u5e8f\u5217\u4e8c\u6b21\u89c4\u5212\u8fdb\u884c\u6a21\u578b\u6821\u51c6\u3002", "result": "\u6a21\u578b\u663e\u793a\u7cd6\u5c3f\u75c5\u60a3\u8005\u7684\u759f\u75be\u611f\u67d3\u51e0\u7387\u662f\u975e\u7cd6\u5c3f\u75c5\u4eba\u7fa4\u76841.8\u20134.0\u500d\uff0c\u5cf0\u503c\u611f\u67d3\u7387\u5206\u522b\u4e3a35\u201336%\u4e0e20\u201321%\uff1b\u57fa\u672c\u518d\u751f\u6570\u5728\u4e0d\u540c\u5b63\u8282\u4ecb\u4e8e0.31\u81f32.75\u4e4b\u95f4\uff0c\u5e73\u5747\u7ea6\u4e3a2.3\u3002", "conclusion": "\u9274\u4e8e\u5370\u5ea6\u7cd6\u5c3f\u75c5\u4eba\u53e3\u9884\u8ba1\u52302050\u5e74\u5c06\u8fbe1.57\u4ebf\uff0c\u7814\u7a76\u7ed3\u679c\u51f8\u663e\u4e86\u53d1\u5c55\u6574\u5408\u759f\u75be\u4e0e\u7cd6\u5c3f\u75c5\u9632\u63a7\u7684\u6c14\u5019\u667a\u80fd\u578b\u5065\u5eb7\u76d1\u6d4b\u4e0e\u5e72\u9884\u4f53\u7cfb\u7684\u7d27\u8feb\u6027\u3002"}}
{"id": "2511.07885", "categories": ["cs.DC", "cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.07885", "abs": "https://arxiv.org/abs/2511.07885", "authors": ["Jon Saad-Falcon", "Avanika Narayan", "Hakki Orhun Akengin", "J. Wes Griffin", "Herumb Shandilya", "Adrian Gamarra Lafuente", "Medhya Goel", "Rebecca Joseph", "Shlok Natarajan", "Etash Kumar Guha", "Shang Zhu", "Ben Athiwaratkun", "John Hennessy", "Azalia Mirhoseini", "Christopher R\u00e9"], "title": "Intelligence per Watt: Measuring Intelligence Efficiency of Local AI", "comment": null, "summary": "Large language model (LLM) queries are predominantly processed by frontier models in centralized cloud infrastructure. Rapidly growing demand strains this paradigm, and cloud providers struggle to scale infrastructure at pace. Two advances enable us to rethink this paradigm: small LMs (<=20B active parameters) now achieve competitive performance to frontier models on many tasks, and local accelerators (e.g., Apple M4 Max) run these models at interactive latencies. This raises the question: can local inference viably redistribute demand from centralized infrastructure? Answering this requires measuring whether local LMs can accurately answer real-world queries and whether they can do so efficiently enough to be practical on power-constrained devices (i.e., laptops). We propose intelligence per watt (IPW), task accuracy divided by unit of power, as a metric for assessing capability and efficiency of local inference across model-accelerator pairs. We conduct a large-scale empirical study across 20+ state-of-the-art local LMs, 8 accelerators, and a representative subset of LLM traffic: 1M real-world single-turn chat and reasoning queries. For each query, we measure accuracy, energy, latency, and power. Our analysis reveals $3$ findings. First, local LMs can accurately answer 88.7% of single-turn chat and reasoning queries with accuracy varying by domain. Second, from 2023-2025, IPW improved 5.3x and local query coverage rose from 23.2% to 71.3%. Third, local accelerators achieve at least 1.4x lower IPW than cloud accelerators running identical models, revealing significant headroom for optimization. These findings demonstrate that local inference can meaningfully redistribute demand from centralized infrastructure, with IPW serving as the critical metric for tracking this transition. We release our IPW profiling harness for systematic intelligence-per-watt benchmarking.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u201c\u6bcf\u74e6\u667a\u80fd\u201d\uff08IPW\uff09\u4f5c\u4e3a\u8861\u91cf\u672c\u5730\u5927\u8bed\u8a00\u6a21\u578b\u63a8\u7406\u80fd\u529b\u4e0e\u80fd\u6548\u7684\u6307\u6807\uff0c\u901a\u8fc7\u5927\u89c4\u6a21\u5b9e\u8bc1\u7814\u7a76\u8868\u660e\uff1a\u5f53\u524d\u672c\u5730\u5c0f\u6a21\u578b\u5df2\u80fd\u51c6\u786e\u56de\u7b5488.7%\u7684\u771f\u5b9e\u5355\u8f6e\u5bf9\u8bdd\u4e0e\u63a8\u7406\u67e5\u8be2\uff1b2023\u81f32025\u5e74\u95f4IPW\u63d0\u53475.3\u500d\uff0c\u672c\u5730\u67e5\u8be2\u8986\u76d6\u7387\u4ece23.2%\u5347\u81f371.3%\uff1b\u4e14\u672c\u5730\u52a0\u901f\u5668\u5728\u76f8\u540c\u6a21\u578b\u4e0b\u6bd4\u4e91\u7aef\u52a0\u901f\u5668\u81f3\u5c11\u8282\u80fd1.4\u500d\u3002\u7ed3\u679c\u8868\u660e\u672c\u5730\u63a8\u7406\u53ef\u6709\u6548\u5206\u6d41\u96c6\u4e2d\u5f0f\u4e91\u57fa\u7840\u8bbe\u65bd\u8d1f\u8f7d\u3002", "motivation": "\u968f\u7740\u5927\u8bed\u8a00\u6a21\u578b\u67e5\u8be2\u9700\u6c42\u6fc0\u589e\uff0c\u96c6\u4e2d\u5f0f\u4e91\u57fa\u7840\u8bbe\u65bd\u9762\u4e34\u6269\u5c55\u538b\u529b\u3002\u540c\u65f6\uff0c\u53c2\u6570\u91cf\u226420B\u7684\u5c0f\u6a21\u578b\u6027\u80fd\u663e\u8457\u63d0\u5347\uff0c\u4e14\u672c\u5730\u8bbe\u5907\uff08\u5982Apple M4 Max\uff09\u5df2\u80fd\u4ee5\u4ea4\u4e92\u7ea7\u5ef6\u8fdf\u8fd0\u884c\u8fd9\u4e9b\u6a21\u578b\u3002\u8fd9\u4fc3\u4f7f\u7814\u7a76\u8005\u63a2\u7d22\u672c\u5730\u63a8\u7406\u662f\u5426\u53ef\u6709\u6548\u5206\u62c5\u4e91\u7aef\u8d1f\u8f7d\uff0c\u5173\u952e\u5728\u4e8e\u8bc4\u4f30\u5176\u5728\u771f\u5b9e\u67e5\u8be2\u4e2d\u7684\u51c6\u786e\u6027\u4e0e\u5728\u529f\u8017\u53d7\u9650\u8bbe\u5907\u4e0a\u7684\u5b9e\u7528\u6027\u3002", "method": "\u63d0\u51fa\u201c\u6bcf\u74e6\u667a\u80fd\u201d\uff08IPW\uff09\u6307\u6807\uff08\u4efb\u52a1\u51c6\u786e\u7387\u9664\u4ee5\u5355\u4f4d\u529f\u8017\uff09\uff0c\u5e76\u572820\u591a\u4e2a\u524d\u6cbf\u672c\u5730\u8bed\u8a00\u6a21\u578b\u30018\u79cd\u52a0\u901f\u5668\u53ca100\u4e07\u6761\u771f\u5b9e\u5355\u8f6e\u804a\u5929\u4e0e\u63a8\u7406\u67e5\u8be2\u4e0a\u8fdb\u884c\u5927\u89c4\u6a21\u5b9e\u8bc1\u7814\u7a76\uff0c\u6d4b\u91cf\u6bcf\u6761\u67e5\u8be2\u7684\u51c6\u786e\u7387\u3001\u80fd\u8017\u3001\u5ef6\u8fdf\u548c\u529f\u8017\u3002", "result": "1\uff09\u672c\u5730LM\u53ef\u51c6\u786e\u56de\u7b5488.7%\u7684\u5355\u8f6e\u67e5\u8be2\uff0c\u51c6\u786e\u7387\u56e0\u9886\u57df\u800c\u5f02\uff1b2\uff092023\u20132025\u5e74IPW\u63d0\u53475.3\u500d\uff0c\u672c\u5730\u67e5\u8be2\u8986\u76d6\u7387\u4ece23.2%\u589e\u81f371.3%\uff1b3\uff09\u672c\u5730\u52a0\u901f\u5668\u8fd0\u884c\u76f8\u540c\u6a21\u578b\u65f6IPW\u81f3\u5c11\u4f18\u4e8e\u4e91\u7aef\u52a0\u901f\u56681.4\u500d\u3002", "conclusion": "\u672c\u5730\u63a8\u7406\u5df2\u5177\u5907\u663e\u8457\u5206\u6d41\u96c6\u4e2d\u5f0f\u4e91\u57fa\u7840\u8bbe\u65bd\u8d1f\u8f7d\u7684\u6f5c\u529b\uff0cIPW\u662f\u8861\u91cf\u548c\u63a8\u52a8\u8fd9\u4e00\u8f6c\u53d8\u7684\u5173\u952e\u6307\u6807\u3002\u4f5c\u8005\u5f00\u6e90\u4e86IPW\u8bc4\u6d4b\u5de5\u5177\u4ee5\u652f\u6301\u7cfb\u7edf\u6027\u57fa\u51c6\u6d4b\u8bd5\u3002"}}
{"id": "2511.08282", "categories": ["cs.NI", "cs.CR", "cs.ET"], "pdf": "https://arxiv.org/pdf/2511.08282", "abs": "https://arxiv.org/abs/2511.08282", "authors": ["Eranga Bandara", "Safdar H. Bouk", "Sachin Shetty", "Ravi Mukkamala", "Abdul Rahman", "Peter Foytik", "Ross Gore", "Xueping Liang", "Ng Wee Keong", "Kasun De Zoysa"], "title": "SRE-Llama -- Fine-Tuned Meta's Llama LLM, Federated Learning, Blockchain and NFT Enabled Site Reliability Engineering(SRE) Platform for Communication and Networking Software Services", "comment": null, "summary": "Software services are crucial for reliable communication and networking; therefore, Site Reliability Engineering (SRE) is important to ensure these systems stay reliable and perform well in cloud-native environments. SRE leverages tools like Prometheus and Grafana to monitor system metrics, defining critical Service Level Indicators (SLIs) and Service Level Objectives (SLOs) for maintaining high service standards. However, a significant challenge arises as many developers often lack in-depth understanding of these tools and the intricacies involved in defining appropriate SLIs and SLOs. To bridge this gap, we propose a novel SRE platform, called SRE-Llama, enhanced by Generative-AI, Federated Learning, Blockchain, and Non-Fungible Tokens (NFTs). This platform aims to automate and simplify the process of monitoring, SLI/SLO generation, and alert management, offering ease in accessibility and efficy for developers. The system operates by capturing metrics from cloud-native services and storing them in a time-series database, like Prometheus and Mimir. Utilizing this stored data, our platform employs Federated Learning models to identify the most relevant and impactful SLI metrics for different services and SLOs, addressing concerns around data privacy. Subsequently, fine-tuned Meta's Llama-3 LLM is adopted to intelligently generate SLIs, SLOs, error budgets, and associated alerting mechanisms based on these identified SLI metrics. A unique aspect of our platform is the encoding of generated SLIs and SLOs as NFT objects, which are then stored on a Blockchain. This feature provides immutable record-keeping and facilitates easy verification and auditing of the SRE metrics and objectives. The automation of the proposed platform is governed by the blockchain smart contracts. The proposed SRE-Llama platform prototype has been implemented with a use case featuring a customized Open5GS 5G Core.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aSRE-Llama\u7684\u65b0\u5e73\u53f0\uff0c\u7ed3\u5408\u751f\u6210\u5f0fAI\u3001\u8054\u90a6\u5b66\u4e60\u3001\u533a\u5757\u94fe\u548cNFT\u6280\u672f\uff0c\u65e8\u5728\u81ea\u52a8\u5316\u5e76\u7b80\u5316\u4e91\u539f\u751f\u73af\u5883\u4e2dSLI/SLO\u7684\u751f\u6210\u3001\u76d1\u63a7\u4e0e\u544a\u8b66\u7ba1\u7406\uff0c\u63d0\u5347\u5f00\u53d1\u8005\u5728\u7ad9\u70b9\u53ef\u9760\u6027\u5de5\u7a0b\uff08SRE\uff09\u4e2d\u7684\u6548\u7387\u4e0e\u51c6\u786e\u6027\u3002", "motivation": "\u5f53\u524d\u8bb8\u591a\u5f00\u53d1\u8005\u5bf9Prometheus\u3001Grafana\u7b49SRE\u5de5\u5177\u4ee5\u53caSLI/SLO\u5b9a\u4e49\u7f3a\u4e4f\u6df1\u5165\u7406\u89e3\uff0c\u5bfc\u81f4\u96be\u4ee5\u6709\u6548\u4fdd\u969c\u4e91\u539f\u751f\u670d\u52a1\u7684\u53ef\u9760\u6027\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u4e2a\u66f4\u667a\u80fd\u3001\u6613\u7528\u4e14\u9690\u79c1\u5b89\u5168\u7684\u81ea\u52a8\u5316SRE\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u5e73\u53f0\u901a\u8fc7Prometheus\u548cMimir\u6536\u96c6\u5e76\u5b58\u50a8\u4e91\u539f\u751f\u670d\u52a1\u6307\u6807\uff0c\u5229\u7528\u8054\u90a6\u5b66\u4e60\u8bc6\u522b\u5173\u952eSLI\u6307\u6807\u4ee5\u4fdd\u62a4\u6570\u636e\u9690\u79c1\uff1b\u518d\u57fa\u4e8e\u5fae\u8c03\u540e\u7684Llama-3\u5927\u8bed\u8a00\u6a21\u578b\u81ea\u52a8\u751f\u6210SLI\u3001SLO\u3001\u9519\u8bef\u9884\u7b97\u53ca\u544a\u8b66\u89c4\u5219\uff1b\u6700\u540e\u5c06\u751f\u6210\u7684SLI/SLO\u7f16\u7801\u4e3aNFT\u5e76\u5b58\u5165\u533a\u5757\u94fe\uff0c\u7531\u667a\u80fd\u5408\u7ea6\u9a71\u52a8\u6574\u4e2a\u81ea\u52a8\u5316\u6d41\u7a0b\u3002", "result": "\u5b9e\u73b0\u4e86SRE-Llama\u539f\u578b\u7cfb\u7edf\uff0c\u5e76\u5728\u5b9a\u5236\u5316\u7684Open5GS 5G\u6838\u5fc3\u7f51\u7528\u4f8b\u4e2d\u9a8c\u8bc1\u4e86\u5176\u5728SLI/SLO\u81ea\u52a8\u5316\u751f\u6210\u3001\u76d1\u63a7\u4e0e\u5ba1\u8ba1\u65b9\u9762\u7684\u53ef\u884c\u6027\u4e0e\u6709\u6548\u6027\u3002", "conclusion": "SRE-Llama\u5e73\u53f0\u901a\u8fc7\u878d\u5408\u591a\u79cd\u524d\u6cbf\u6280\u672f\uff0c\u663e\u8457\u964d\u4f4e\u4e86SRE\u5b9e\u8df5\u95e8\u69db\uff0c\u63d0\u5347\u4e86SLI/SLO\u7ba1\u7406\u7684\u667a\u80fd\u5316\u3001\u53ef\u5ba1\u8ba1\u6027\u4e0e\u9690\u79c1\u5b89\u5168\u6027\uff0c\u4e3a\u4e91\u539f\u751f\u7cfb\u7edf\u7684\u53ef\u9760\u6027\u4fdd\u969c\u63d0\u4f9b\u4e86\u65b0\u8303\u5f0f\u3002"}}
{"id": "2511.07709", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2511.07709", "abs": "https://arxiv.org/abs/2511.07709", "authors": ["Lars Olt", "Luis Diego Fonseca Flores", "Ian Mckinley"], "title": "Post Processing Graphical User Interface for Heat Flow Visualization", "comment": "Presented at the 53rd International Conference on Environmental Systems (ICES 2024), Louisville, Kentucky, USA, 2024. Official link: https://hdl.handle.net/2346/98969", "summary": "Thermal Desktop (TD) is an industry-standard thermal analysis tool used to create and analyze thermal models for landers, rovers, spacecraft, and instrument payloads. Currently, limited software exists to extract and visualize metrics relevant to heat flow within TD, impeding thermal engineers from analyzing their results quickly. This paper discusses a graphical user interface (GUI) built in MATLAB and C++ which uses TDs application programming interface (API), OpenTD, and a custom parser to address this void. Specifically, we present a method for efficiently loading temperature, conductance, and submodel metrics using a side effect of TDs Compressed Solution Results (CSR) files. This approach can reduce the runtime for correlating model nodes and conductors with submodel IDs by orders of magnitude. Lastly, we reflect on the shortcomings of this method for reading data, consider the future of the GUI, and provide recommendations for subsequent OpenTD releases.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u4e00\u4e2a\u57fa\u4e8eMATLAB\u548cC++\u5f00\u53d1\u7684\u56fe\u5f62\u7528\u6237\u754c\u9762\uff08GUI\uff09\uff0c\u5229\u7528Thermal Desktop\u7684API\uff08OpenTD\uff09\u548c\u81ea\u5b9a\u4e49\u89e3\u6790\u5668\uff0c\u9ad8\u6548\u63d0\u53d6\u5e76\u53ef\u89c6\u5316\u70ed\u6a21\u578b\u4e2d\u7684\u6e29\u5ea6\u3001\u5bfc\u70ed\u7cfb\u6570\u548c\u5b50\u6a21\u578b\u6307\u6807\uff0c\u663e\u8457\u63d0\u5347\u70ed\u5de5\u7a0b\u5e08\u5206\u6790\u6548\u7387\u3002", "motivation": "\u5f53\u524d\u7f3a\u4e4f\u6709\u6548\u5de5\u5177\u4eceThermal Desktop\u4e2d\u63d0\u53d6\u548c\u53ef\u89c6\u5316\u70ed\u6d41\u76f8\u5173\u6307\u6807\uff0c\u9650\u5236\u4e86\u70ed\u5de5\u7a0b\u5e08\u5feb\u901f\u5206\u6790\u7ed3\u679c\u7684\u80fd\u529b\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u7ed3\u5408MATLAB\u4e0eC++\u7684GUI\uff0c\u901a\u8fc7\u8c03\u7528Thermal Desktop\u7684OpenTD API\uff0c\u5e76\u5229\u7528\u5176\u538b\u7f29\u6c42\u89e3\u7ed3\u679c\uff08CSR\uff09\u6587\u4ef6\u7684\u526f\u4f5c\u7528\uff0c\u9ad8\u6548\u52a0\u8f7d\u6e29\u5ea6\u3001\u5bfc\u70ed\u7cfb\u6570\u53ca\u5b50\u6a21\u578b\u6570\u636e\u3002", "result": "\u8be5\u65b9\u6cd5\u5c06\u6a21\u578b\u8282\u70b9\u3001\u5bfc\u4f53\u4e0e\u5b50\u6a21\u578bID\u5173\u8054\u7684\u8fd0\u884c\u65f6\u95f4\u51cf\u5c11\u4e86\u591a\u4e2a\u6570\u91cf\u7ea7\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6570\u636e\u5904\u7406\u6548\u7387\u3002", "conclusion": "\u5c3d\u7ba1\u8be5\u65b9\u6cd5\u5728\u6570\u636e\u8bfb\u53d6\u65b9\u9762\u5b58\u5728\u5c40\u9650\u6027\uff0c\u4f46\u4e3a\u672a\u6765GUI\u7684\u53d1\u5c55\u63d0\u4f9b\u4e86\u65b9\u5411\uff0c\u5e76\u5bf9\u540e\u7eedOpenTD\u7248\u672c\u63d0\u51fa\u4e86\u6539\u8fdb\u5efa\u8bae\u3002"}}
{"id": "2511.08135", "categories": ["cs.DC", "cs.AR"], "pdf": "https://arxiv.org/pdf/2511.08135", "abs": "https://arxiv.org/abs/2511.08135", "authors": ["Zhuoheng Ran", "Chong Wu", "Renjie Xu", "Maolin Che", "Hong Yan"], "title": "UniFormer: Unified and Efficient Transformer for Reasoning Across General and Custom Computing", "comment": "Accepted on 24 September 2025 at NeurIPS 2025 Efficient Reasoning Workshop", "summary": "The success of neural networks such as convolutional neural networks (CNNs) has been largely attributed to their effective and widespread deployment on customised computing platforms, including field-programmable gate arrays (FPGAs) and application-specific integrated circuits (ASICs). In the current era, Transformer-based architectures underpin the majority of state-of-the-art (SOTA) larger models that are also increasingly deployed on customised computing hardware for low-power and real-time applications. However, the fundamentally different parallel computation paradigms between general-purpose and customised computing often lead to compromises in model transfer and deployability, which typically come at the cost of complexity, efficiency or accuracy. Moreover, many cross-platform optimisation principles have also remained underexplored in existing studies. This paper introduces UniFormer, a unified and efficient Transformer architecture for both general-purpose and customised computing platforms. By enabling higher parallelism and compute-storage fusion, UniFormer achieves state-of-the-art (SOTA) accuracy and latency on GPUs while exhibiting strong adaptability on FPGAs. To the best of our knowledge, this paper is the first efficient Transformer work that jointly considers both general-purpose and customised computing architectures.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86UniFormer\uff0c\u4e00\u79cd\u9762\u5411\u901a\u7528\u4e0e\u5b9a\u5236\u8ba1\u7b97\u5e73\u53f0\u7684\u7edf\u4e00\u9ad8\u6548Transformer\u67b6\u6784\uff0c\u5728GPU\u4e0a\u5b9e\u73b0SOTA\u7cbe\u5ea6\u4e0e\u5ef6\u8fdf\uff0c\u5e76\u5728FPGA\u4e0a\u5c55\u73b0\u826f\u597d\u9002\u5e94\u6027\u3002", "motivation": "\u73b0\u6709Transformer\u6a21\u578b\u5728\u901a\u7528\u4e0e\u5b9a\u5236\u8ba1\u7b97\u5e73\u53f0\uff08\u5982GPU\u4e0eFPGA/ASIC\uff09\u4e4b\u95f4\u90e8\u7f72\u65f6\uff0c\u56e0\u5e76\u884c\u8ba1\u7b97\u8303\u5f0f\u5dee\u5f02\u5bfc\u81f4\u6a21\u578b\u8fc1\u79fb\u5b58\u5728\u590d\u6742\u6027\u3001\u6548\u7387\u6216\u7cbe\u5ea6\u7684\u6298\u8877\uff0c\u4e14\u8de8\u5e73\u53f0\u4f18\u5316\u539f\u5219\u7814\u7a76\u4e0d\u8db3\u3002", "method": "\u63d0\u51faUniFormer\u67b6\u6784\uff0c\u901a\u8fc7\u63d0\u5347\u5e76\u884c\u6027\u4e0e\u8ba1\u7b97-\u5b58\u50a8\u878d\u5408\uff0c\u5b9e\u73b0\u5bf9\u901a\u7528\u548c\u5b9a\u5236\u8ba1\u7b97\u5e73\u53f0\u7684\u7edf\u4e00\u652f\u6301\u3002", "result": "UniFormer\u5728GPU\u4e0a\u8fbe\u5230SOTA\u7684\u51c6\u786e\u7387\u4e0e\u5ef6\u8fdf\u8868\u73b0\uff0c\u540c\u65f6\u5728FPGA\u4e0a\u5c55\u73b0\u51fa\u5f3a\u9002\u5e94\u6027\u3002", "conclusion": "UniFormer\u662f\u9996\u4e2a\u8054\u5408\u8003\u8651\u901a\u7528\u4e0e\u5b9a\u5236\u8ba1\u7b97\u67b6\u6784\u7684\u9ad8\u6548Transformer\u5de5\u4f5c\uff0c\u6709\u6548\u5f25\u5408\u4e86\u4e24\u7c7b\u5e73\u53f0\u95f4\u7684\u90e8\u7f72\u9e3f\u6c9f\u3002"}}
{"id": "2511.08375", "categories": ["cs.NI"], "pdf": "https://arxiv.org/pdf/2511.08375", "abs": "https://arxiv.org/abs/2511.08375", "authors": ["Darius Saif", "Ashraf Matrawy"], "title": "Demystifying QUIC from the Specifications", "comment": null, "summary": "QUIC is an advanced transport layer protocol whose ubiquity on the Internet is now very apparent. Importantly, QUIC fuels the next generation of web browsing: HTTP/3. QUIC is a stateful and connection oriented protocol which offers similar features (and more) to the combination of TCP and TLS. There are several difficulties which readers may encounter when learning about QUIC: i.) its rapid evolution (particularly, differentiation between the QUIC standard and the now deprecated Google QUIC), ii.) numerous RFCs whose organization, language, and detail may be challenging to the casual reader, and iii.) the nature of QUIC's cross-layer and privacy-centric implementation, making it impossible to understand or debug by looking at packets alone. For these reasons, the aim of this paper is to present QUIC in a complete yet approachable fashion, thereby demystifying the protocol from its specifications.", "AI": {"tldr": "\u672c\u6587\u65e8\u5728\u4ee5\u5b8c\u6574\u4e14\u6613\u4e8e\u7406\u89e3\u7684\u65b9\u5f0f\u4ecb\u7ecdQUIC\u534f\u8bae\uff0c\u5e2e\u52a9\u8bfb\u8005\u514b\u670d\u5176\u5feb\u901f\u6f14\u8fdb\u3001RFC\u6587\u6863\u590d\u6742\u4ee5\u53ca\u8de8\u5c42\u9690\u79c1\u8bbe\u8ba1\u5e26\u6765\u7684\u5b66\u4e60\u969c\u788d\u3002", "motivation": "QUIC\u534f\u8bae\u56e0\u5176\u5feb\u901f\u6f14\u8fdb\u3001\u76f8\u5173RFC\u6587\u6863\u7ec4\u7ec7\u4e0e\u8bed\u8a00\u590d\u6742\uff0c\u4ee5\u53ca\u8de8\u5c42\u548c\u6ce8\u91cd\u9690\u79c1\u7684\u5b9e\u73b0\u65b9\u5f0f\uff0c\u4f7f\u521d\u5b66\u8005\u96be\u4ee5\u7406\u89e3\u548c\u8c03\u8bd5\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u7bc7\u901a\u4fd7\u6613\u61c2\u7684\u7efc\u8ff0\u6027\u6587\u7ae0\u6765\u5398\u6e05\u5176\u6838\u5fc3\u5185\u5bb9\u3002", "method": "\u901a\u8fc7\u7cfb\u7edf\u68b3\u7406QUIC\u534f\u8bae\u7684\u6807\u51c6\u6f14\u8fdb\u3001\u5173\u952e\u7279\u6027\u53ca\u5176\u4e0eHTTP/3\u7684\u5173\u7cfb\uff0c\u5e76\u5bf9\u76f8\u5173RFC\u8fdb\u884c\u6574\u5408\u89e3\u8bfb\uff0c\u4ee5\u6e05\u6670\u3001\u8fde\u8d2f\u7684\u65b9\u5f0f\u5448\u73b0\u534f\u8bae\u5168\u8c8c\u3002", "result": "\u4e3a\u8bfb\u8005\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5168\u9762\u800c\u6613\u4e8e\u7406\u89e3\u7684QUIC\u534f\u8bae\u4ecb\u7ecd\uff0c\u6709\u6548\u964d\u4f4e\u4e86\u5b66\u4e60\u95e8\u69db\uff0c\u6709\u52a9\u4e8e\u7406\u89e3\u5176\u5de5\u4f5c\u673a\u5236\u4e0e\u8bbe\u8ba1\u601d\u60f3\u3002", "conclusion": "QUIC\u4f5c\u4e3a\u4e0b\u4e00\u4ee3Web\u6d4f\u89c8\u7684\u6838\u5fc3\u4f20\u8f93\u534f\u8bae\uff0c\u5c3d\u7ba1\u590d\u6742\uff0c\u4f46\u901a\u8fc7\u7ed3\u6784\u5316\u548c\u7b80\u660e\u7684\u9610\u8ff0\u53ef\u4ee5\u88ab\u6709\u6548\u638c\u63e1\uff1b\u672c\u6587\u6210\u529f\u5b9e\u73b0\u4e86\u5bf9\u8be5\u534f\u8bae\u7684\u201c\u53bb\u795e\u79d8\u5316\u201d\u3002"}}
{"id": "2511.07742", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2511.07742", "abs": "https://arxiv.org/abs/2511.07742", "authors": ["Luan Lazzari", "Kleinner Farias"], "title": "Event-Driven Inconsistency Detection Between UML Class and Sequence Diagrams", "comment": "2 figures", "summary": "Modeling is a central and demanding activity in software engineering that requires skills such as abstraction, consistency maintenance, and precise communication. These skills are difficult to master and even harder to teach effectively. Educators and students often struggle to understand and manage inconsistencies that arise during the modeling process. To address this challenge, we present \\texttt{Harmony Validator}, a tool integrated as a plugin for the Papyrus modeling environment, designed to automatically detect and report inconsistencies in UML models, including class and sequence diagrams. The tool adopts an event-driven architecture that continuously monitors modeling actions and notifies users of emerging inconsistencies in real time. This approach enhances awareness of model integrity and supports the iterative refinement of design artifacts. The paper describes the architecture, detection mechanisms, and usage scenarios of Harmony Validator. It also includes a case study conducted with students in a software engineering course to evaluate the perceived usefulness and benefits of UML modeling in teaching and learning. Our results indicate that Harmony Validator fosters a better understanding of model consistency and promotes reflective learning practices in software modeling education.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86Harmony Validator\uff0c\u4e00\u4e2a\u96c6\u6210\u4e8ePapyrus\u5efa\u6a21\u73af\u5883\u7684\u63d2\u4ef6\u5de5\u5177\uff0c\u7528\u4e8e\u5b9e\u65f6\u68c0\u6d4bUML\u7c7b\u56fe\u548c\u5e8f\u5217\u56fe\u4e2d\u7684\u4e0d\u4e00\u81f4\u6027\uff0c\u4ee5\u652f\u6301\u8f6f\u4ef6\u5de5\u7a0b\u6559\u5b66\u4e2d\u7684\u5efa\u6a21\u5b66\u4e60\u3002", "motivation": "\u8f6f\u4ef6\u5efa\u6a21\u9700\u8981\u62bd\u8c61\u3001\u4fdd\u6301\u4e00\u81f4\u6027\u548c\u7cbe\u786e\u6c9f\u901a\u7b49\u6280\u80fd\uff0c\u8fd9\u4e9b\u6280\u80fd\u96be\u4ee5\u638c\u63e1\u4e14\u6559\u5b66\u56f0\u96be\uff1b\u5e08\u751f\u5e38\u96be\u4ee5\u7406\u89e3\u548c\u7ba1\u7406\u5efa\u6a21\u8fc7\u7a0b\u4e2d\u7684\u4e0d\u4e00\u81f4\u6027\u95ee\u9898\u3002", "method": "\u5f00\u53d1\u5e76\u96c6\u6210\u4e86\u540d\u4e3aHarmony Validator\u7684\u63d2\u4ef6\u5de5\u5177\uff0c\u91c7\u7528\u4e8b\u4ef6\u9a71\u52a8\u67b6\u6784\uff0c\u5728Papyrus\u73af\u5883\u4e2d\u5b9e\u65f6\u76d1\u63a7\u5efa\u6a21\u64cd\u4f5c\u5e76\u5373\u65f6\u62a5\u544a\u4e0d\u4e00\u81f4\u6027\uff1b\u5e76\u901a\u8fc7\u8f6f\u4ef6\u5de5\u7a0b\u8bfe\u7a0b\u4e2d\u7684\u5b66\u751f\u6848\u4f8b\u7814\u7a76\u8bc4\u4f30\u5176\u6559\u5b66\u6548\u679c\u3002", "result": "\u6848\u4f8b\u7814\u7a76\u8868\u660e\uff0cHarmony Validator\u6709\u52a9\u4e8e\u5b66\u751f\u66f4\u597d\u5730\u7406\u89e3\u6a21\u578b\u4e00\u81f4\u6027\uff0c\u5e76\u4fc3\u8fdb\u53cd\u601d\u6027\u5b66\u4e60\u5b9e\u8df5\u3002", "conclusion": "Harmony Validator\u901a\u8fc7\u5b9e\u65f6\u68c0\u6d4b\u548c\u53cd\u9988\u673a\u5236\u6709\u6548\u652f\u6301\u4e86\u8f6f\u4ef6\u5efa\u6a21\u6559\u80b2\uff0c\u63d0\u5347\u4e86\u5b66\u751f\u5bf9\u6a21\u578b\u5b8c\u6574\u6027\u7684\u8ba4\u77e5\u4e0e\u5efa\u6a21\u80fd\u529b\u3002"}}
{"id": "2511.07851", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2511.07851", "abs": "https://arxiv.org/abs/2511.07851", "authors": ["Sharif Ahmed", "Addi Malviya Thakur", "Gregory R. Watson", "Nasir U. Eisty"], "title": "Uncovering Scientific Software Sustainability through Community Engagement and Software Quality Metrics", "comment": null, "summary": "Scientific open-source software (Sci-OSS) projects are critical for advancing research, yet sustaining these projects long-term remains a major challenge. This paper explores the sustainability of Sci-OSS hosted on GitHub, focusing on two factors drawn from stewardship organizations: community engagement and software quality. We map sustainability to repository metrics from the literature and mined data from ten prominent Sci-OSS projects. A multimodal analysis of these projects led us to a novel visualization technique, providing a robust way to display both current and evolving software metrics over time, replacing multiple traditional visualizations with one. Additionally, our statistical analysis shows that even similar-domain projects sustain themselves differently. Natural language analysis supports claims from the literature, highlighting that project-specific feedback plays a key role in maintaining software quality. Our visualization and analysis methods offer researchers, funders, and developers key insights into long-term software sustainability.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86GitHub\u4e0a\u79d1\u5b66\u5f00\u6e90\u8f6f\u4ef6\uff08Sci-OSS\uff09\u9879\u76ee\u7684\u53ef\u6301\u7eed\u6027\uff0c\u805a\u7126\u793e\u533a\u53c2\u4e0e\u548c\u8f6f\u4ef6\u8d28\u91cf\u4e24\u4e2a\u7ef4\u5ea6\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u53ef\u89c6\u5316\u65b9\u6cd5\u4ee5\u7efc\u5408\u5c55\u793a\u8f6f\u4ef6\u6307\u6807\u7684\u52a8\u6001\u53d8\u5316\uff0c\u5e76\u901a\u8fc7\u7edf\u8ba1\u4e0e\u81ea\u7136\u8bed\u8a00\u5206\u6790\u63ed\u793a\u4e0d\u540c\u9879\u76ee\u5728\u53ef\u6301\u7eed\u6027\u7b56\u7565\u4e0a\u7684\u5dee\u5f02\u3002", "motivation": "\u79d1\u5b66\u5f00\u6e90\u8f6f\u4ef6\u5bf9\u79d1\u7814\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u5176\u957f\u671f\u53ef\u6301\u7eed\u6027\u9762\u4e34\u6311\u6218\u3002\u73b0\u6709\u7814\u7a76\u7f3a\u4e4f\u5bf9\u53ef\u6301\u7eed\u6027\u56e0\u7d20\u7684\u7cfb\u7edf\u6027\u5ea6\u91cf\u4e0e\u53ef\u89c6\u5316\u624b\u6bb5\uff0c\u96be\u4ee5\u652f\u6301\u9879\u76ee\u7ef4\u62a4\u8005\u3001\u8d44\u52a9\u65b9\u548c\u5f00\u53d1\u8005\u505a\u51fa\u6709\u6548\u51b3\u7b56\u3002", "method": "\u4f5c\u8005\u4ece\u6587\u732e\u4e2d\u63d0\u53d6\u4e0e\u53ef\u6301\u7eed\u6027\u76f8\u5173\u7684\u4ed3\u5e93\u6307\u6807\uff0c\u5bf9\u5341\u4e2a\u77e5\u540dSci-OSS\u9879\u76ee\u8fdb\u884c\u6570\u636e\u6316\u6398\uff0c\u5e76\u7ed3\u5408\u591a\u6a21\u6001\u5206\u6790\uff08\u5305\u62ec\u7edf\u8ba1\u5206\u6790\u548c\u81ea\u7136\u8bed\u8a00\u5206\u6790\uff09\u6765\u8bc4\u4f30\u793e\u533a\u53c2\u4e0e\u548c\u8f6f\u4ef6\u8d28\u91cf\uff1b\u540c\u65f6\u5f00\u53d1\u4e86\u4e00\u79cd\u65b0\u578b\u53ef\u89c6\u5316\u6280\u672f\uff0c\u7528\u4e8e\u7edf\u4e00\u5c55\u793a\u968f\u65f6\u95f4\u6f14\u53d8\u7684\u8f6f\u4ef6\u6307\u6807\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff0c\u5373\u4f7f\u5728\u540c\u4e00\u9886\u57df\uff0c\u4e0d\u540c\u9879\u76ee\u7684\u53ef\u6301\u7eed\u6027\u8def\u5f84\u4e5f\u5b58\u5728\u663e\u8457\u5dee\u5f02\uff1b\u81ea\u7136\u8bed\u8a00\u5206\u6790\u9a8c\u8bc1\u4e86\u9879\u76ee\u7279\u5b9a\u53cd\u9988\u5bf9\u8f6f\u4ef6\u8d28\u91cf\u7684\u5173\u952e\u4f5c\u7528\uff1b\u6240\u63d0\u51fa\u7684\u53ef\u89c6\u5316\u65b9\u6cd5\u80fd\u6709\u6548\u66ff\u4ee3\u591a\u4e2a\u4f20\u7edf\u56fe\u8868\uff0c\u76f4\u89c2\u5448\u73b0\u8f6f\u4ef6\u53ef\u6301\u7eed\u6027\u72b6\u6001\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u7406\u89e3\u548c\u63d0\u5347\u79d1\u5b66\u5f00\u6e90\u8f6f\u4ef6\u7684\u53ef\u6301\u7eed\u6027\u63d0\u4f9b\u4e86\u53ef\u64cd\u4f5c\u7684\u5206\u6790\u6846\u67b6\u4e0e\u53ef\u89c6\u5316\u5de5\u5177\uff0c\u6709\u52a9\u4e8e\u5229\u76ca\u76f8\u5173\u8005\u66f4\u597d\u5730\u652f\u6301\u548c\u7ef4\u62a4\u957f\u671f\u79d1\u7814\u8f6f\u4ef6\u9879\u76ee\u3002"}}
{"id": "2511.08147", "categories": ["cs.DC", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.08147", "abs": "https://arxiv.org/abs/2511.08147", "authors": ["Andrija Stanisic", "Stefan Nastic"], "title": "ProbSelect: Stochastic Client Selection for GPU-Accelerated Compute Devices in the 3D Continuum", "comment": null, "summary": "Integration of edge, cloud and space devices into a unified 3D continuum imposes significant challenges for client selection in federated learning systems. Traditional approaches rely on continuous monitoring and historical data collection, which becomes impractical in dynamic environments where satellites and mobile devices frequently change operational conditions. Furthermore, existing solutions primarily consider CPU-based computation, failing to capture complex characteristics of GPU-accelerated training that is prevalent across the 3D continuum. This paper introduces ProbSelect, a novel approach utilizing analytical modeling and probabilistic forecasting for client selection on GPU-accelerated devices, without requiring historical data or continuous monitoring. We model client selection within user-defined SLOs. Extensive evaluation across diverse GPU architectures and workloads demonstrates that ProbSelect improves SLO compliance by 13.77% on average while achieving 72.5% computational waste reduction compared to baseline approaches.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faProbSelect\uff0c\u4e00\u79cd\u65e0\u9700\u5386\u53f2\u6570\u636e\u6216\u6301\u7eed\u76d1\u63a7\u7684\u5ba2\u6237\u7aef\u9009\u62e9\u65b9\u6cd5\uff0c\u901a\u8fc7\u5206\u6790\u5efa\u6a21\u4e0e\u6982\u7387\u9884\u6d4b\u5728GPU\u52a0\u901f\u8bbe\u5907\u4e0a\u5b9e\u73b0\u9ad8\u6548\u8054\u90a6\u5b66\u4e60\uff0c\u5728\u6ee1\u8db3\u7528\u6237\u5b9a\u4e49SLO\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u5408\u89c4\u6027\u5e76\u51cf\u5c11\u8ba1\u7b97\u6d6a\u8d39\u3002", "motivation": "\u5728\u8fb9\u7f18\u3001\u4e91\u548c\u7a7a\u95f4\u8bbe\u5907\u6784\u6210\u76843D\u8fde\u7eed\u4f53\u4e2d\uff0c\u4f20\u7edf\u8054\u90a6\u5b66\u4e60\u5ba2\u6237\u7aef\u9009\u62e9\u65b9\u6cd5\u4f9d\u8d56\u6301\u7eed\u76d1\u63a7\u548c\u5386\u53f2\u6570\u636e\uff0c\u5728\u52a8\u6001\u73af\u5883\u4e2d\uff08\u5982\u536b\u661f\u548c\u79fb\u52a8\u8bbe\u5907\u9891\u7e41\u53d8\u5316\uff09\u96be\u4ee5\u9002\u7528\uff1b\u540c\u65f6\u73b0\u6709\u65b9\u6cd5\u4e3b\u8981\u57fa\u4e8eCPU\u8ba1\u7b97\uff0c\u65e0\u6cd5\u6709\u6548\u5904\u7406\u5e7f\u6cdb\u5b58\u5728\u7684GPU\u52a0\u901f\u8bad\u7ec3\u573a\u666f\u3002", "method": "\u63d0\u51faProbSelect\u65b9\u6cd5\uff0c\u7ed3\u5408\u89e3\u6790\u5efa\u6a21\u4e0e\u6982\u7387\u9884\u6d4b\uff0c\u5728\u65e0\u9700\u5386\u53f2\u6570\u636e\u6216\u6301\u7eed\u76d1\u63a7\u7684\u524d\u63d0\u4e0b\uff0c\u9488\u5bf9GPU\u52a0\u901f\u8bbe\u5907\u8fdb\u884c\u5ba2\u6237\u7aef\u9009\u62e9\uff0c\u5e76\u5c06\u5176\u5efa\u6a21\u4e3a\u6ee1\u8db3\u7528\u6237\u5b9a\u4e49\u670d\u52a1\u7b49\u7ea7\u76ee\u6807\uff08SLO\uff09\u7684\u95ee\u9898\u3002", "result": "\u5728\u591a\u79cdGPU\u67b6\u6784\u548c\u5de5\u4f5c\u8d1f\u8f7d\u4e0b\u7684\u5b9e\u9a8c\u8868\u660e\uff0cProbSelect\u76f8\u6bd4\u57fa\u7ebf\u65b9\u6cd5\u5e73\u5747\u63d0\u534713.77%\u7684SLO\u5408\u89c4\u7387\uff0c\u5e76\u51cf\u5c1172.5%\u7684\u8ba1\u7b97\u6d6a\u8d39\u3002", "conclusion": "ProbSelect\u6709\u6548\u89e3\u51b3\u4e863D\u8fde\u7eed\u4f53\u4e2d\u52a8\u6001\u73af\u5883\u4e0bGPU\u52a0\u901f\u8054\u90a6\u5b66\u4e60\u7684\u5ba2\u6237\u7aef\u9009\u62e9\u96be\u9898\uff0c\u5728\u4e0d\u4f9d\u8d56\u5386\u53f2\u6570\u636e\u7684\u60c5\u51b5\u4e0b\u663e\u8457\u63d0\u5347\u4e86\u7cfb\u7edf\u6548\u7387\u4e0e\u670d\u52a1\u8d28\u91cf\u3002"}}
{"id": "2511.07924", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2511.07924", "abs": "https://arxiv.org/abs/2511.07924", "authors": ["Shuang Liu", "Zhirun Zhang", "Jinhao Dong", "Zan Wang", "Qingchao Shen", "Junjie Chen", "Wei Lu", "Xiaoyong Du"], "title": "Testing Question Answering Software with Context-Driven Question Generation", "comment": null, "summary": "Question-answering software is becoming increasingly integrated into our daily lives, with prominent examples including Apple Siri and Amazon Alexa. Ensuring the quality of such systems is critical, as incorrect answers could lead to significant harm. Current state-of-the-art testing approaches apply metamorphic relations to existing test datasets, generating test questions based on these relations. However, these methods have two key limitations. First, they often produce unnatural questions that humans are unlikely to ask, reducing the effectiveness of the generated questions in identifying bugs that might occur in real-world scenarios. Second, these questions are generated from pre-existing test datasets, ignoring the broader context and thus limiting the diversity and relevance of the generated questions.\n  In this work, we introduce CQ^2A, a context-driven question generation approach for testing question-answering systems. Specifically, CQ^2A extracts entities and relationships from the context to form ground truth answers, and utilizes large language models to generate questions based on these ground truth answers and the surrounding context. We also propose the consistency verification and constraint checking to increase the reliability of LLM's outputs. Experiments conducted on three datasets demonstrate that CQ^2A outperforms state-of-the-art approaches on the bug detection capability, the naturalness of the generated questions as well as the coverage of the context. Moreover, the test cases generated by CQ^2A reduce error rate when utilized for fine-tuning the QA software under test", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aCQ\u00b2A\u7684\u4e0a\u4e0b\u6587\u9a71\u52a8\u95ee\u7b54\u7cfb\u7edf\u6d4b\u8bd5\u65b9\u6cd5\uff0c\u901a\u8fc7\u4ece\u4e0a\u4e0b\u6587\u4e2d\u63d0\u53d6\u5b9e\u4f53\u548c\u5173\u7cfb\u751f\u6210\u66f4\u81ea\u7136\u3001\u591a\u6837\u4e14\u8d34\u8fd1\u771f\u5b9e\u573a\u666f\u7684\u95ee\u9898\uff0c\u5e76\u7ed3\u5408\u5927\u8bed\u8a00\u6a21\u578b\u4e0e\u4e00\u81f4\u6027\u9a8c\u8bc1\u673a\u5236\u63d0\u5347\u95ee\u9898\u8d28\u91cf\u3002\u5b9e\u9a8c\u8868\u660e\u8be5\u65b9\u6cd5\u5728\u7f3a\u9677\u68c0\u6d4b\u80fd\u529b\u3001\u95ee\u9898\u81ea\u7136\u5ea6\u548c\u4e0a\u4e0b\u6587\u8986\u76d6\u7387\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u6280\u672f\uff0c\u4e14\u751f\u6210\u7684\u6d4b\u8bd5\u7528\u4f8b\u8fd8\u80fd\u6709\u6548\u964d\u4f4e\u88ab\u6d4b\u95ee\u7b54\u7cfb\u7edf\u7684\u9519\u8bef\u7387\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u5143\u5173\u7cfb\u7684\u95ee\u7b54\u7cfb\u7edf\u6d4b\u8bd5\u65b9\u6cd5\u5b58\u5728\u4e24\u4e2a\u4e3b\u8981\u95ee\u9898\uff1a\u4e00\u662f\u751f\u6210\u7684\u95ee\u9898\u4e0d\u591f\u81ea\u7136\uff0c\u96be\u4ee5\u53cd\u6620\u771f\u5b9e\u7528\u6237\u63d0\u95ee\uff1b\u4e8c\u662f\u4f9d\u8d56\u5df2\u6709\u6d4b\u8bd5\u6570\u636e\u96c6\uff0c\u7f3a\u4e4f\u5bf9\u66f4\u5e7f\u6cdb\u4e0a\u4e0b\u6587\u7684\u5229\u7528\uff0c\u5bfc\u81f4\u95ee\u9898\u591a\u6837\u6027\u4e0e\u76f8\u5173\u6027\u53d7\u9650\u3002", "method": "CQ\u00b2A\u65b9\u6cd5\u4ece\u4e0a\u4e0b\u6587\u4e2d\u63d0\u53d6\u5b9e\u4f53\u4e0e\u5173\u7cfb\u4ee5\u6784\u5efa\u6807\u51c6\u7b54\u6848\uff0c\u5e76\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\u7ed3\u5408\u4e0a\u4e0b\u6587\u548c\u6807\u51c6\u7b54\u6848\u751f\u6210\u95ee\u9898\u3002\u540c\u65f6\u5f15\u5165\u4e00\u81f4\u6027\u9a8c\u8bc1\u548c\u7ea6\u675f\u68c0\u67e5\u673a\u5236\uff0c\u63d0\u9ad8\u5927\u8bed\u8a00\u6a21\u578b\u8f93\u51fa\u7684\u53ef\u9760\u6027\u3002", "result": "\u5728\u4e09\u4e2a\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cCQ\u00b2A\u5728\u7f3a\u9677\u68c0\u6d4b\u80fd\u529b\u3001\u751f\u6210\u95ee\u9898\u7684\u81ea\u7136\u5ea6\u4ee5\u53ca\u4e0a\u4e0b\u6587\u8986\u76d6\u7387\u65b9\u9762\u5747\u4f18\u4e8e\u5f53\u524d\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\u3002\u6b64\u5916\uff0c\u5176\u751f\u6210\u7684\u6d4b\u8bd5\u7528\u4f8b\u7528\u4e8e\u5fae\u8c03\u95ee\u7b54\u7cfb\u7edf\u65f6\u53ef\u6709\u6548\u964d\u4f4e\u9519\u8bef\u7387\u3002", "conclusion": "CQ\u00b2A\u662f\u4e00\u79cd\u6709\u6548\u7684\u4e0a\u4e0b\u6587\u9a71\u52a8\u95ee\u7b54\u7cfb\u7edf\u6d4b\u8bd5\u65b9\u6cd5\uff0c\u4e0d\u4ec5\u80fd\u751f\u6210\u66f4\u8d34\u8fd1\u771f\u5b9e\u4f7f\u7528\u573a\u666f\u7684\u9ad8\u8d28\u91cf\u6d4b\u8bd5\u95ee\u9898\uff0c\u8fd8\u80fd\u63d0\u5347\u95ee\u7b54\u7cfb\u7edf\u7684\u6574\u4f53\u6027\u80fd\u548c\u9c81\u68d2\u6027\u3002"}}
{"id": "2511.08222", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2511.08222", "abs": "https://arxiv.org/abs/2511.08222", "authors": ["Serafino Cicerone", "Alessia Di Fonso", "Gabriele Di Stefano", "Alfredo Navarra"], "title": "Gathering in Vertex- and Edge-Transitive Graphs without Multiplicity Detection under Round Robin", "comment": "25 pages, 9 fugures, 2 tables", "summary": "In the field of swarm robotics, one of the most studied problem is Gathering. It asks for a distributed algorithm that brings the robots to a common location, not known in advance. We consider the case of robots constrained to move along the edges of a graph under the well-known OBLOT model. Gathering is then accomplished once all the robots occupy a same vertex. Differently from classical settings, we assume: i) the initial configuration may contain multiplicities, i.e. more than one robot may occupy the same vertex; ii) robots cannot detect multiplicities; iii) robots move along the edges of vertex- and edge-transitive graphs, i.e. graphs where all the vertices (and the edges, resp.) belong to a same class of equivalence. To balance somehow such a `hostile' setting, as a scheduler for the activation of the robots, we consider the round-robin, where robots are cyclically activated one at a time.\n  We provide some basic impossibility results and we design two different algorithms approaching the Gathering for robots moving on two specific topologies belonging to edge- and vertex-transitive graphs: infinite grids and hypercubes. The two algorithms are both time-optimal and heavily exploit the properties of the underlying topologies. Because of this, we conjecture that no general algorithm can exist for all the solvable cases.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u5728OBLOT\u6a21\u578b\u4e0b\u3001\u53d7\u9650\u4e8e\u9876\u70b9\u548c\u8fb9\u4f20\u9012\u56fe\u7ed3\u6784\u4e2d\u7684\u673a\u5668\u4eba\u805a\u96c6\u95ee\u9898\uff0c\u5047\u8bbe\u521d\u59cb\u914d\u7f6e\u53ef\u80fd\u5305\u542b\u591a\u91cd\u5360\u636e\u4e14\u673a\u5668\u4eba\u65e0\u6cd5\u611f\u77e5\u591a\u91cd\u6027\uff0c\u5728\u8f6e\u8be2\u8c03\u5ea6\u673a\u5236\u4e0b\u63d0\u51fa\u4e86\u9488\u5bf9\u65e0\u9650\u7f51\u683c\u548c\u8d85\u7acb\u65b9\u4f53\u4e24\u79cd\u62d3\u6251\u7684\u65f6\u95f4\u6700\u4f18\u805a\u96c6\u7b97\u6cd5\uff0c\u5e76\u6307\u51fa\u53ef\u80fd\u4e0d\u5b58\u5728\u9002\u7528\u4e8e\u6240\u6709\u53ef\u89e3\u60c5\u5f62\u7684\u901a\u7528\u7b97\u6cd5\u3002", "motivation": "\u7ecf\u5178\u805a\u96c6\u95ee\u9898\u901a\u5e38\u5047\u8bbe\u673a\u5668\u4eba\u80fd\u611f\u77e5\u591a\u91cd\u6027\u6216\u521d\u59cb\u65e0\u591a\u91cd\u5360\u636e\uff0c\u800c\u672c\u6587\u8003\u8651\u66f4\u201c\u654c\u5bf9\u201d\u7684\u8bbe\u5b9a\uff1a\u521d\u59cb\u5b58\u5728\u591a\u91cd\u5360\u636e\u3001\u673a\u5668\u4eba\u65e0\u6cd5\u68c0\u6d4b\u591a\u91cd\u6027\uff0c\u4e14\u8fd0\u52a8\u53d7\u9650\u4e8e\u9876\u70b9\u4e0e\u8fb9\u4f20\u9012\u56fe\u3002\u4e3a\u63a2\u7d22\u5728\u6b64\u9650\u5236\u4e0b\u805a\u96c6\u662f\u5426\u53ef\u884c\uff0c\u4f5c\u8005\u5f00\u5c55\u6b64\u9879\u7814\u7a76\u3002", "method": "\u5728\u8f6e\u8be2\uff08round-robin\uff09\u8c03\u5ea6\u673a\u5236\u4e0b\uff0c\u5229\u7528\u65e0\u9650\u7f51\u683c\u548c\u8d85\u7acb\u65b9\u4f53\u8fd9\u4e24\u79cd\u7279\u5b9a\u9876\u70b9-\u8fb9\u4f20\u9012\u56fe\u7684\u5bf9\u79f0\u6027\u548c\u7ed3\u6784\u6027\u8d28\uff0c\u5206\u522b\u8bbe\u8ba1\u4e24\u4e2a\u65f6\u95f4\u6700\u4f18\u7684\u5206\u5e03\u5f0f\u805a\u96c6\u7b97\u6cd5\uff0c\u5e76\u5206\u6790\u57fa\u672c\u4e0d\u53ef\u80fd\u6027\u7ed3\u679c\u3002", "result": "\u9488\u5bf9\u65e0\u9650\u7f51\u683c\u548c\u8d85\u7acb\u65b9\u4f53\uff0c\u63d0\u51fa\u4e86\u4e24\u79cd\u65f6\u95f4\u6700\u4f18\u7684\u805a\u96c6\u7b97\u6cd5\uff1b\u540c\u65f6\u8bc1\u660e\u4e86\u4e00\u4e9b\u57fa\u672c\u7684\u4e0d\u53ef\u80fd\u6027\u7ed3\u679c\uff0c\u5e76\u63a8\u6d4b\u4e0d\u5b58\u5728\u9002\u7528\u4e8e\u6240\u6709\u53ef\u89e3\u9876\u70b9-\u8fb9\u4f20\u9012\u56fe\u7684\u901a\u7528\u805a\u96c6\u7b97\u6cd5\u3002", "conclusion": "\u5728\u6240\u8bbe\u5b9a\u7684\u4e25\u82db\u6761\u4ef6\u4e0b\uff0c\u867d\u7136\u5bf9\u67d0\u4e9b\u7279\u5b9a\u62d3\u6251\uff08\u5982\u65e0\u9650\u7f51\u683c\u548c\u8d85\u7acb\u65b9\u4f53\uff09\u53ef\u4ee5\u5b9e\u73b0\u65f6\u95f4\u6700\u4f18\u7684\u805a\u96c6\uff0c\u4f46\u56e0\u7b97\u6cd5\u9ad8\u5ea6\u4f9d\u8d56\u5e95\u5c42\u56fe\u7ed3\u6784\u7279\u6027\uff0c\u5f88\u53ef\u80fd\u4e0d\u5b58\u5728\u9002\u7528\u4e8e\u6240\u6709\u53ef\u89e3\u60c5\u5f62\u7684\u901a\u7528\u89e3\u6cd5\u3002"}}
{"id": "2511.08373", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2511.08373", "abs": "https://arxiv.org/abs/2511.08373", "authors": ["Henrik Daniel Christensen", "Saverio Giallorenzo", "Jacopo Mauro"], "title": "Priority Matters: Optimising Kubernetes Clusters Usage with Constraint-Based Pod Packing", "comment": null, "summary": "Distributed applications employ Kubernetes for scalable, fault-tolerant deployments over computer clusters, where application components run in groups of containers called pods. The scheduler, at the heart of Kubernetes' architecture, determines the placement of pods given their priority and resource requirements on cluster nodes. To quickly allocate pods, the scheduler uses lightweight heuristics that can lead to suboptimal placements and resource fragmentation, preventing allocations of otherwise deployable pods on the available nodes.\n  We propose the usage of constraint programming to find the optimal allocation of pods satisfying all their priorities and resource requests. Implementation-wise, our solution comes as a plug-in to the default scheduler that operates as a fallback mechanism when some pods cannot be allocated. Using the OR-Tools constraint solver, our experiments on small-to-mid-sized clusters indicate that, within a 1-second scheduling window, our approach places more higher-priority pods than the default scheduler (possibly demonstrating allocation optimality) in over 44\\% of realisable allocation scenarios where the default scheduler fails, while certifying that the default scheduler's placement is already optimal in over 19\\% of scenarios. With a 10-second window, our approach improves placements in over 73\\% and still certifies that the default scheduler's placement is already optimal in over 19\\% of scenarios.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u7ea6\u675f\u89c4\u5212\u7684Kubernetes\u8c03\u5ea6\u63d2\u4ef6\uff0c\u5728\u9ed8\u8ba4\u8c03\u5ea6\u5668\u5931\u8d25\u65f6\u4f5c\u4e3a\u540e\u5907\u673a\u5236\uff0c\u80fd\u57281\u79d2\u5185\u4f18\u531644%\u4ee5\u4e0a\u573a\u666f\u4e2d\u7684\u9ad8\u4f18\u5148\u7ea7Pod\u5206\u914d\uff0c\u5e76\u572810\u79d2\u5185\u63d0\u5347\u81f373%\uff0c\u540c\u65f6\u9a8c\u8bc1\u9ed8\u8ba4\u8c03\u5ea6\u5668\u5728\u7ea619%\u573a\u666f\u4e2d\u5df2\u662f\u6700\u4f18\u3002", "motivation": "Kubernetes\u9ed8\u8ba4\u8c03\u5ea6\u5668\u4f7f\u7528\u8f7b\u91cf\u7ea7\u542f\u53d1\u5f0f\u7b97\u6cd5\uff0c\u53ef\u80fd\u5bfc\u81f4\u6b21\u4f18\u7684Pod\u653e\u7f6e\u548c\u8d44\u6e90\u788e\u7247\u5316\uff0c\u4ece\u800c\u65e0\u6cd5\u90e8\u7f72\u672c\u53ef\u8fd0\u884c\u7684Pod\u3002", "method": "\u91c7\u7528\u7ea6\u675f\u89c4\u5212\u65b9\u6cd5\uff0c\u5229\u7528OR-Tools\u7ea6\u675f\u6c42\u89e3\u5668\u5b9e\u73b0\u4e00\u4e2a\u8c03\u5ea6\u63d2\u4ef6\uff0c\u4f5c\u4e3a\u9ed8\u8ba4\u8c03\u5ea6\u5668\u7684\u540e\u5907\u673a\u5236\uff0c\u5728\u8c03\u5ea6\u5931\u8d25\u65f6\u5bfb\u627e\u6ee1\u8db3\u6240\u6709\u4f18\u5148\u7ea7\u548c\u8d44\u6e90\u9700\u6c42\u7684\u6700\u4f18Pod\u5206\u914d\u65b9\u6848\u3002", "result": "\u5728\u5c0f\u5230\u4e2d\u578b\u96c6\u7fa4\u5b9e\u9a8c\u4e2d\uff0c1\u79d2\u8c03\u5ea6\u7a97\u53e3\u4e0b\uff0c\u8be5\u65b9\u6cd5\u5728\u8d85\u8fc744%\u7684\u53ef\u5b9e\u73b0\u573a\u666f\u4e2d\u4f18\u4e8e\u9ed8\u8ba4\u8c03\u5ea6\u5668\uff1b10\u79d2\u7a97\u53e3\u4e0b\u63d0\u5347\u81f373%\uff1b\u540c\u65f6\u5728\u7ea619%\u7684\u573a\u666f\u4e2d\u9a8c\u8bc1\u9ed8\u8ba4\u8c03\u5ea6\u5668\u7ed3\u679c\u5df2\u662f\u6700\u4f18\u3002", "conclusion": "\u5c06\u7ea6\u675f\u89c4\u5212\u5f15\u5165Kubernetes\u8c03\u5ea6\u53ef\u663e\u8457\u63d0\u5347\u8d44\u6e90\u5229\u7528\u7387\u4e0e\u9ad8\u4f18\u5148\u7ea7\u4efb\u52a1\u7684\u8c03\u5ea6\u6210\u529f\u7387\uff0c\u4e14\u80fd\u6709\u6548\u8bc4\u4f30\u9ed8\u8ba4\u8c03\u5ea6\u5668\u7684\u6700\u4f18\u6027\u3002"}}
{"id": "2511.08127", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2511.08127", "abs": "https://arxiv.org/abs/2511.08127", "authors": ["Weiye Li", "Wenyi Tang"], "title": "A Small Leak Sinks All: Exploring the Transferable Vulnerability of Source Code Models", "comment": null, "summary": "Source Code Model learn the proper embeddings from source codes, demonstrating significant success in various software engineering or security tasks. The recent explosive development of LLM extends the family of SCMs,bringing LLMs for code that revolutionize development workflows. Investigating different kinds of SCM vulnerability is the cornerstone for the security and trustworthiness of AI-powered software ecosystems, however, the fundamental one, transferable vulnerability, remains critically underexplored. Existing studies neither offer practical ways, i.e. require access to the downstream classifier of SCMs, to produce effective adversarial samples for adversarial defense, nor give heed to the widely used LLM4Code in modern software development platforms and cloud-based integrated development environments. Therefore, this work systematically studies the intrinsic vulnerability transferability of both traditional SCMs and LLM4Code, and proposes a victim-agnostic approach to generate practical adversarial samples. We design HABITAT, consisting of a tailored perturbation-inserting mechanism and a hierarchical Reinforcement Learning framework that adaptively selects optimal perturbations without requiring any access to the downstream classifier of SCMs. Furthermore, an intrinsic transferability analysis of SCM vulnerabilities is conducted, revealing the potential vulnerability correlation between traditional SCMs and LLM4Code, together with fundamental factors that govern the success rate of victim-agnostic transfer attacks. These findings of SCM vulnerabilities underscore the critical focal points for developing robust defenses in the future. Experimental evaluation demonstrates that our constructed adversarial examples crafted based on traditional SCMs achieve up to 64% success rates against LLM4Code, surpassing the state-of-the-art by over 15%.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7cfb\u7edf\u7814\u7a76\u4e86\u4f20\u7edf\u6e90\u4ee3\u7801\u6a21\u578b\uff08SCMs\uff09\u4e0e\u5927\u8bed\u8a00\u6a21\u578b\u7528\u4e8e\u4ee3\u7801\uff08LLM4Code\uff09\u4e4b\u95f4\u7684\u53ef\u8fc1\u79fb\u8106\u5f31\u6027\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u9700\u8bbf\u95ee\u4e0b\u6e38\u5206\u7c7b\u5668\u7684\u901a\u7528\u5bf9\u6297\u6837\u672c\u751f\u6210\u65b9\u6cd5HABITAT\uff0c\u5e76\u63ed\u793a\u4e86\u5f71\u54cd\u8fc1\u79fb\u653b\u51fb\u6210\u529f\u7387\u7684\u5173\u952e\u56e0\u7d20\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u7f3a\u4e4f\u5bf9\u6e90\u4ee3\u7801\u6a21\u578b\uff08\u5305\u62ec\u4f20\u7edfSCMs\u548cLLM4Code\uff09\u4e2d\u53ef\u8fc1\u79fb\u8106\u5f31\u6027\u7684\u6df1\u5165\u63a2\u7d22\uff0c\u4e14\u591a\u6570\u65b9\u6cd5\u4f9d\u8d56\u4e8e\u5bf9\u4e0b\u6e38\u5206\u7c7b\u5668\u7684\u8bbf\u95ee\uff0c\u96be\u4ee5\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u5e94\u7528\u3002\u56e0\u6b64\uff0c\u4e9f\u9700\u4e00\u79cd\u4e0d\u4f9d\u8d56\u76ee\u6807\u6a21\u578b\u4fe1\u606f\u3001\u9002\u7528\u4e8e\u73b0\u4ee3\u5f00\u53d1\u73af\u5883\u7684\u901a\u7528\u5bf9\u6297\u653b\u51fb\u65b9\u6cd5\uff0c\u4ee5\u8bc4\u4f30\u548c\u63d0\u5347AI\u9a71\u52a8\u8f6f\u4ef6\u751f\u6001\u7684\u5b89\u5168\u6027\u3002", "method": "\u4f5c\u8005\u63d0\u51fa\u4e86HABITAT\u6846\u67b6\uff0c\u5305\u542b\u5b9a\u5236\u5316\u7684\u6270\u52a8\u63d2\u5165\u673a\u5236\u548c\u5206\u5c42\u5f3a\u5316\u5b66\u4e60\u7ed3\u6784\uff0c\u80fd\u591f\u5728\u65e0\u9700\u8bbf\u95ee\u4e0b\u6e38\u5206\u7c7b\u5668\u7684\u60c5\u51b5\u4e0b\u81ea\u9002\u5e94\u5730\u9009\u62e9\u6700\u4f18\u6270\u52a8\uff0c\u751f\u6210\u9488\u5bf9\u6e90\u4ee3\u7801\u6a21\u578b\u7684\u5bf9\u6297\u6837\u672c\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u57fa\u4e8e\u4f20\u7edfSCMs\u751f\u6210\u7684\u5bf9\u6297\u6837\u672c\u5bf9LLM4Code\u7684\u653b\u51fb\u6210\u529f\u7387\u6700\u9ad8\u53ef\u8fbe64%\uff0c\u6bd4\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5\u9ad8\u51fa15%\u4ee5\u4e0a\uff1b\u540c\u65f6\u63ed\u793a\u4e86\u4f20\u7edfSCMs\u4e0eLLM4Code\u4e4b\u95f4\u5b58\u5728\u5185\u5728\u8106\u5f31\u6027\u5173\u8054\u53ca\u5f71\u54cd\u8fc1\u79fb\u653b\u51fb\u6548\u679c\u7684\u5173\u952e\u56e0\u7d20\u3002", "conclusion": "\u8be5\u7814\u7a76\u63ed\u793a\u4e86\u6e90\u4ee3\u7801\u6a21\u578b\uff08\u5305\u62ec\u4f20\u7edf\u6a21\u578b\u4e0eLLM4Code\uff09\u4e2d\u5b58\u5728\u7684\u53ef\u8fc1\u79fb\u8106\u5f31\u6027\uff0c\u5f3a\u8c03\u4e86\u6784\u5efa\u4e0d\u4f9d\u8d56\u76ee\u6807\u6a21\u578b\u4fe1\u606f\u7684\u901a\u7528\u9632\u5fa1\u673a\u5236\u7684\u91cd\u8981\u6027\uff0c\u4e3a\u672a\u6765\u63d0\u5347AI\u8d4b\u80fd\u8f6f\u4ef6\u751f\u6001\u7cfb\u7edf\u7684\u5b89\u5168\u6027\u63d0\u4f9b\u4e86\u5173\u952e\u65b9\u5411\u3002"}}
{"id": "2511.08232", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2511.08232", "abs": "https://arxiv.org/abs/2511.08232", "authors": ["Alkid Baci", "Luke Friedrichs", "Caglar Demir", "Axel-Cyrille Ngonga Ngomo"], "title": "OWLAPY: A Pythonic Framework for OWL Ontology Engineering", "comment": null, "summary": "In this paper, we introduce OWLAPY, a comprehensive Python framework for OWL ontology engineering. OWLAPY streamlines the creation, modification, and serialization of OWL 2 ontologies. It uniquely integrates native Python-based reasoners with support for external Java reasoners, offering flexibility for users. OWLAPY facilitates multiple implementations of core ontology components and provides robust conversion capabilities between OWL class expressions and formats such as Description Logics, Manchester Syntax, and SPARQL. It also allows users to define custom workflows to leverage large language models (LLMs) in ontology generation from natural language text. OWLAPY serves as a well-tested software framework for users seeking a flexible Python library for advanced ontology engineering, including those transitioning from Java-based environments. The project is publicly available on GitHub at https://github.com/dice-group/owlapy and on the Python Package Index (PyPI) at https://pypi.org/project/owlapy/ , with over 50,000 downloads at the time of writing.", "AI": {"tldr": "OWLAPY \u662f\u4e00\u4e2a\u529f\u80fd\u5168\u9762\u7684 Python \u6846\u67b6\uff0c\u7528\u4e8e OWL \u672c\u4f53\u5de5\u7a0b\uff0c\u652f\u6301\u672c\u4f53\u7684\u521b\u5efa\u3001\u4fee\u6539\u3001\u5e8f\u5217\u5316\uff0c\u5e76\u96c6\u6210\u672c\u5730\u4e0e\u5916\u90e8\u63a8\u7406\u673a\uff0c\u540c\u65f6\u63d0\u4f9b\u591a\u79cd\u8bed\u6cd5\u683c\u5f0f\u8f6c\u6362\u53ca\u7ed3\u5408\u5927\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u81ea\u7136\u8bed\u8a00\u5230\u672c\u4f53\u751f\u6210\u7684\u80fd\u529b\u3002", "motivation": "\u4e3a\u6ee1\u8db3\u7528\u6237\u5bf9\u7075\u6d3b\u3001\u9ad8\u6548\u4e14\u6613\u4e8e\u4f7f\u7528\u7684 Python \u5de5\u5177\u5728 OWL \u672c\u4f53\u5de5\u7a0b\u4e2d\u7684\u9700\u6c42\uff0c\u7279\u522b\u662f\u5e2e\u52a9\u4ece Java \u73af\u5883\u8fc1\u79fb\u7684\u7528\u6237\uff0c\u4f5c\u8005\u5f00\u53d1\u4e86 OWLAPY\u3002", "method": "OWLAPY \u63d0\u4f9b\u4e86\u5bf9 OWL 2 \u672c\u4f53\u7684\u539f\u751f Python \u652f\u6301\uff0c\u6574\u5408\u4e86\u672c\u5730 Python \u63a8\u7406\u5668\u548c\u5916\u90e8 Java \u63a8\u7406\u5668\uff0c\u5b9e\u73b0\u4e86\u591a\u79cd\u672c\u4f53\u7ec4\u4ef6\u5b9e\u73b0\u65b9\u5f0f\uff0c\u5e76\u652f\u6301 OWL \u7c7b\u8868\u8fbe\u5f0f\u4e0e\u63cf\u8ff0\u903b\u8f91\u3001Manchester \u8bed\u6cd5\u548c SPARQL \u7b49\u683c\u5f0f\u4e4b\u95f4\u7684\u8f6c\u6362\u3002\u6b64\u5916\uff0c\u8fd8\u652f\u6301\u7528\u6237\u81ea\u5b9a\u4e49\u5de5\u4f5c\u6d41\u4ee5\u7ed3\u5408\u5927\u8bed\u8a00\u6a21\u578b\u4ece\u81ea\u7136\u8bed\u8a00\u751f\u6210\u672c\u4f53\u3002", "result": "OWLAPY \u5df2\u4f5c\u4e3a\u5f00\u6e90\u9879\u76ee\u53d1\u5e03\u4e8e GitHub \u548c PyPI\uff0c\u83b7\u5f97\u4e86\u8d85\u8fc7 5 \u4e07\u6b21\u4e0b\u8f7d\uff0c\u6210\u4e3a\u4e00\u4e2a\u7ecf\u8fc7\u5145\u5206\u6d4b\u8bd5\u3001\u9002\u7528\u4e8e\u9ad8\u7ea7\u672c\u4f53\u5de5\u7a0b\u7684 Python \u8f6f\u4ef6\u6846\u67b6\u3002", "conclusion": "OWLAPY \u4e3a\u672c\u4f53\u5de5\u7a0b\u5e08\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5f3a\u5927\u800c\u7075\u6d3b\u7684 Python \u5de5\u5177\uff0c\u663e\u8457\u964d\u4f4e\u4e86 OWL \u672c\u4f53\u5f00\u53d1\u548c\u63a8\u7406\u7684\u95e8\u69db\uff0c\u5e76\u4fc3\u8fdb\u4e86\u81ea\u7136\u8bed\u8a00\u5904\u7406\u4e0e\u672c\u4f53\u5de5\u7a0b\u7684\u878d\u5408\u3002"}}
{"id": "2511.08475", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.08475", "abs": "https://arxiv.org/abs/2511.08475", "authors": ["Yangxiao Cai", "Ruiyin Li", "Peng Liang", "Mojtaba Shahin", "Zengyang Li"], "title": "Designing LLM-based Multi-Agent Systems for Software Engineering Tasks: Quality Attributes, Design Patterns and Rationale", "comment": null, "summary": "As the complexity of Software Engineering (SE) tasks continues to escalate, Multi-Agent Systems (MASs) have emerged as a focal point of research and practice due to their autonomy and scalability. Furthermore, through leveraging the reasoning and planning capabilities of Large Language Models (LLMs), the application of LLM-based MASs in the field of SE is garnering increasing attention. However, there is no dedicated study that systematically explores the design of LLM-based MASs, including the Quality Attributes (QAs) on which the designers mainly focus, the design patterns used by the designers, and the rationale guiding the design of LLM-based MASs for SE tasks. To this end, we conducted a study to identify the QAs that LLM-based MASs for SE tasks focus on, the design patterns used in the MASs, and the design rationale for the MASs. We collected 94 papers on LLM-based MASs for SE tasks as the source. Our study shows that: (1) Code Generation is the most common SE task solved by LLM-based MASs among ten identified SE tasks, (2) Functional Suitability is the QA on which designers of LLM-based MASs pay the most attention, (3) Role-Based Cooperation is the design pattern most frequently employed among 16 patterns used to construct LLM-based MASs, and (4) Improving the Quality of Generated Code is the most common rationale behind the design of LLM-based MASs. Based on the study results, we presented the implications for the design of LLM-based MASs to support SE tasks.", "AI": {"tldr": "\u672c\u6587\u7cfb\u7edf\u7814\u7a76\u4e86\u9762\u5411\u8f6f\u4ef6\u5de5\u7a0b\u4efb\u52a1\u7684\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\uff08LLM-based MASs\uff09\u7684\u8bbe\u8ba1\uff0c\u5206\u6790\u4e86\u5176\u5173\u6ce8\u7684\u8d28\u91cf\u5c5e\u6027\u3001\u5e38\u7528\u8bbe\u8ba1\u6a21\u5f0f\u53ca\u8bbe\u8ba1\u52a8\u673a\uff0c\u5e76\u57fa\u4e8e94\u7bc7\u76f8\u5173\u8bba\u6587\u63d0\u51fa\u4e86\u8bbe\u8ba1\u542f\u793a\u3002", "motivation": "\u968f\u7740\u8f6f\u4ef6\u5de5\u7a0b\u4efb\u52a1\u65e5\u76ca\u590d\u6742\uff0c\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u56e0\u5176\u81ea\u4e3b\u6027\u548c\u53ef\u6269\u5c55\u6027\u53d7\u5230\u5173\u6ce8\uff0c\u4f46\u5c1a\u7f3a\u4e4f\u5bf9\u5176\u7cfb\u7edf\u6027\u8bbe\u8ba1\u7684\u7814\u7a76\uff0c\u5305\u62ec\u5173\u6ce8\u7684\u8d28\u91cf\u5c5e\u6027\u3001\u91c7\u7528\u7684\u8bbe\u8ba1\u6a21\u5f0f\u53ca\u8bbe\u8ba1\u4f9d\u636e\u3002", "method": "\u4f5c\u8005\u6536\u96c6\u4e8694\u7bc7\u5173\u4e8e\u9762\u5411\u8f6f\u4ef6\u5de5\u7a0b\u4efb\u52a1\u7684LLM-based MASs\u7684\u8bba\u6587\uff0c\u901a\u8fc7\u5185\u5bb9\u5206\u6790\u8bc6\u522b\u5176\u4e2d\u5173\u6ce8\u7684\u8d28\u91cf\u5c5e\u6027\u3001\u4f7f\u7528\u7684\u8bbe\u8ba1\u6a21\u5f0f\u4ee5\u53ca\u8bbe\u8ba1\u80cc\u540e\u7684\u52a8\u673a\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff1a(1) \u4ee3\u7801\u751f\u6210\u662f\u6700\u5e38\u89c1\u7684\u5e94\u7528\u4efb\u52a1\uff1b(2) \u529f\u80fd\u9002\u7528\u6027\u662f\u8bbe\u8ba1\u5e08\u6700\u5173\u6ce8\u7684\u8d28\u91cf\u5c5e\u6027\uff1b(3) \u57fa\u4e8e\u89d2\u8272\u7684\u534f\u4f5c\u662f\u6700\u5e38\u7528\u7684\u8bbe\u8ba1\u6a21\u5f0f\uff1b(4) \u63d0\u5347\u751f\u6210\u4ee3\u7801\u8d28\u91cf\u662f\u6700\u4e3b\u8981\u7684\u8bbe\u8ba1\u52a8\u673a\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u9762\u5411\u8f6f\u4ef6\u5de5\u7a0b\u4efb\u52a1\u7684LLM-based MASs\u7684\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u5b9e\u8bc1\u57fa\u7840\u548c\u5b9e\u8df5\u542f\u793a\uff0c\u6709\u52a9\u4e8e\u6307\u5bfc\u672a\u6765\u7cfb\u7edf\u7684\u8bbe\u8ba1\u4e0e\u4f18\u5316\u3002"}}
{"id": "2511.08530", "categories": ["cs.SE", "cs.PL"], "pdf": "https://arxiv.org/pdf/2511.08530", "abs": "https://arxiv.org/abs/2511.08530", "authors": ["Rong Feng", "Vanisha Gupta", "Vivek Patel", "Viroopaksh Reddy Ernampati", "Suman Saha"], "title": "Can Large Language Models Simulate Symbolic Execution Output Like KLEE?", "comment": null, "summary": "Symbolic execution helps check programs by exploring different paths based on symbolic inputs. Tools like KLEE are commonly used because they can automatically detect bugs and create test cases. But one of KLEE's biggest issues is how slow it can get when programs have lots of branching paths-it often becomes too resource-heavy to run on large or complex code. In this project, we wanted to see if a large language model like GPT-4o could simulate the kinds of outputs that KLEE generates. The idea was to explore whether LLMs could one day replace parts of symbolic execution to save time and resources.\n  One specific goal was to have GPT-4o identify the most constrained path in a program, this is the execution path with the most symbolic conditions. These paths are especially important because they often represent edge cases that are harder to test and more likely to contain deep bugs. However, figuring this out usually requires fully running KLEE, which can be expensive. So, we tested whether GPT-4o could predict the KLEE outputs and the most complex path using a dataset of 100 C programs. Our results showed about 20% accuracy in generating KLEE-like outputs and identifying the most constrained path. While not highly accurate, this early work helps show what current LLMs can and can't do when it comes to simulating symbolic execution.", "AI": {"tldr": "\u672c\u6587\u63a2\u7d22\u4e86\u4f7f\u7528\u5927\u8bed\u8a00\u6a21\u578b\uff08\u5982GPT-4o\uff09\u6a21\u62df\u7b26\u53f7\u6267\u884c\u5de5\u5177KLEE\u8f93\u51fa\u7684\u53ef\u884c\u6027\uff0c\u65e8\u5728\u901a\u8fc7LLM\u8bc6\u522b\u7a0b\u5e8f\u4e2d\u6700\u53d7\u7ea6\u675f\u7684\u8def\u5f84\u4ee5\u8282\u7701\u8d44\u6e90\uff0c\u5b9e\u9a8c\u5728100\u4e2aC\u7a0b\u5e8f\u4e0a\u8fdb\u884c\uff0c\u51c6\u786e\u7387\u7ea6\u4e3a20%\u3002", "motivation": "KLEE\u7b49\u7b26\u53f7\u6267\u884c\u5de5\u5177\u5728\u5904\u7406\u5177\u6709\u5927\u91cf\u5206\u652f\u8def\u5f84\u7684\u7a0b\u5e8f\u65f6\u6548\u7387\u4f4e\u4e0b\u3001\u8d44\u6e90\u6d88\u8017\u5927\uff0c\u56e0\u6b64\u7814\u7a76\u8005\u5e0c\u671b\u501f\u52a9\u5927\u8bed\u8a00\u6a21\u578b\u6765\u90e8\u5206\u66ff\u4ee3\u7b26\u53f7\u6267\u884c\uff0c\u4ece\u800c\u63d0\u5347\u6548\u7387\u5e76\u964d\u4f4e\u5f00\u9500\u3002", "method": "\u4f7f\u7528GPT-4o\u5bf9100\u4e2aC\u7a0b\u5e8f\u8fdb\u884c\u5206\u6790\uff0c\u5c1d\u8bd5\u9884\u6d4bKLEE\u7684\u8f93\u51fa\u5e76\u8bc6\u522b\u5176\u4e2d\u5305\u542b\u6700\u591a\u7b26\u53f7\u6761\u4ef6\u7684\u6700\u590d\u6742\uff08\u6700\u53d7\u7ea6\u675f\uff09\u6267\u884c\u8def\u5f84\u3002", "result": "GPT-4o\u5728\u751f\u6210\u7c7b\u4f3cKLEE\u8f93\u51fa\u548c\u8bc6\u522b\u6700\u590d\u6742\u8def\u5f84\u65b9\u9762\u7684\u51c6\u786e\u7387\u7ea6\u4e3a20%\uff0c\u8868\u660e\u5f53\u524dLLM\u5728\u6b64\u4efb\u52a1\u4e0a\u80fd\u529b\u6709\u9650\u4f46\u5177\u5907\u521d\u6b65\u6f5c\u529b\u3002", "conclusion": "\u5c3d\u7ba1\u51c6\u786e\u7387\u4e0d\u9ad8\uff0c\u8be5\u7814\u7a76\u4e3a\u7406\u89e3\u5927\u8bed\u8a00\u6a21\u578b\u5728\u6a21\u62df\u7b26\u53f7\u6267\u884c\u65b9\u9762\u7684\u80fd\u529b\u8fb9\u754c\u63d0\u4f9b\u4e86\u521d\u6b65\u8bc1\u636e\uff0c\u6307\u51fa\u4e86\u672a\u6765\u6539\u8fdb\u7684\u65b9\u5411\u3002"}}
