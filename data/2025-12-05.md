<div id=toc></div>

# Table of Contents

- [cs.NI](#cs.NI) [Total: 1]
- [cs.AR](#cs.AR) [Total: 2]
- [cs.DC](#cs.DC) [Total: 10]
- [cs.MA](#cs.MA) [Total: 2]
- [cs.SE](#cs.SE) [Total: 16]


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [1] [Vision and Causal Learning Based Channel Estimation for THz Communications](https://arxiv.org/abs/2512.04380)
*Kitae Kim,Yan Kyaw Tun,Md. Shirajum Munir,Chirsto Kurisummoottil Thomas,Walid Saad,Choong Seon Hong*

Main category: cs.NI

TL;DR: 本文提出一种基于视觉和因果推理的太赫兹信道估计方法，显著提升城市非视距环境下的估计精度与泛化能力。


<details>
  <summary>Details</summary>
Motivation: 太赫兹通信在6G中面临高传播损耗和复杂城市环境导致的传统信道估计方法失效问题。

Method: 结合计算机视觉与变分因果动力学（VCD），利用实时图像分析物理对象对信号传播的影响。

Result: 相比传统方法，新方法在NLoS场景下预测精度提升高达两倍，且在未见过的城市环境中表现优异。

Conclusion: 该方法在动态城市场景中比现有AI技术更准确、鲁棒，尤其擅长处理反射与衍射等间接路径。

Abstract: The use of terahertz (THz) communications with massive multiple input multiple output (MIMO) systems in 6G can potentially provide high data rates and low latency communications. However, accurate channel estimation in THz frequencies presents significant challenges due to factors such as high propagation losses, sensitivity to environmental obstructions, and strong atmospheric absorption. These challenges are par- ticularly pronounced in urban environments, where traditional channel estimation methods often fail to deliver reliable results, particularly in complex non-line-of-sight (NLoS) scenarios. This paper introduces a novel vision-based channel estimation tech- nique that integrates causal reasoning into urban THz communi- cation systems. The proposed method combines computer vision algorithms with variational causal dynamics (VCD) to analyze real-time images of the urban environment, allowing for a deeper understanding of the physical factors that influence THz signal propagation. By capturing the complex, dynamic interactions between physical objects (such as buildings, trees, and vehicles) and the transmitted signals, the model can predict the channel with up to twice the accuracy of conventional methods. This model improves estimation accuracy and demonstrates supe- rior generalization performance. Hence, it can provide reliable predictions even in previously unseen urban environments. The effectiveness of the proposed method is particularly evident in NLoS conditions, where it significantly outperforms traditional methods such as by accounting for indirect signal paths, such as reflections and diffractions. Simulation results confirm that the proposed vision-based approach surpasses conventional artificial intelligence (AI)-based estimation techniques in accuracy and robustness, showing a substantial improvement across various dynamic urban scenarios.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [2] [FLEX: Leveraging FPGA-CPU Synergy for Mixed-Cell-Height Legalization Acceleration](https://arxiv.org/abs/2512.04527)
*Xingyu Liu,Jiawei Liang,Linfeng Du,Yipu Zhang,Chaofang Ma,Hanwei Fan,Jiang Xu,Wei Zhang*

Main category: cs.AR

TL;DR: FLEX是一种用于混合单元高度合法化任务的FPGA-CPU加速器，通过优化任务分配、多粒度流水线技术和单元移位过程，显著提升速度与质量。


<details>
  <summary>Details</summary>
Motivation: 解决混合单元高度合法化任务中的计算瓶颈，提高处理速度与结果质量。

Method: 优化FPGA与CPU间任务分配，采用多粒度流水线技术加速关键步骤，并针对单元移位过程进行设计优化。

Result: 相比最先进的CPU-GPU和多线程CPU合法化工具，FLEX分别实现最高18.3倍和5.4倍加速，同时合法化质量提升4%和1%。

Conclusion: FLEX在速度和可扩展性方面表现优异，同时提升了合法化结果的质量，是高效硬件加速方案。

Abstract: In this work, we present FLEX, an FPGA-CPU accelerator for mixed-cell-height legalization tasks. We address challenges from the following perspectives. First, we optimize the task assignment strategy and perform an efficient task partition between FPGA and CPU to exploit their complementary strengths. Second, a multi-granularity pipelining technique is employed to accelerate the most time-consuming step, finding optimal placement position (FOP), in legalization. At last, we particularly target the computationally intensive cell shifting process in FOP, optimizing the design to align it seamlessly with the multi-granularity pipelining framework for further speedup. Experimental results show that FLEX achieves up to 18.3x and 5.4x speedups compared to state-of-the-art CPU-GPU and multi-threaded CPU legalizers with better scalability, while improving legalization quality by 4% and 1%.

</details>


### [3] [Declarative Synthesis and Multi-Objective Optimization of Stripboard Circuit Layouts Using Answer Set Programming](https://arxiv.org/abs/2512.04910)
*Fang Li*

Main category: cs.AR

TL;DR: 该论文提出了一种基于答案集编程（ASP）的自动化条形板电路布局方法，兼顾布局生成与多目标优化，实验表明能高效生成紧凑且可制造的布局。


<details>
  <summary>Details</summary>
Motivation: 解决条形板电路手工布局繁琐、易出错的问题，提升电子原型设计与教学效率。

Method: 利用ASP的声明式特性建模几何与电气约束，采用两阶段求解策略：先确保可行性，再优化布局质量。

Result: 针对不同复杂度电路均能生成面积小、跨条少的可制造布局。

Conclusion: 该方法显著推进了条形板自动布局技术，同时展示了声明式编程在复杂设计自动化中的强大能力。

Abstract: This paper presents a novel approach to automated stripboard circuit layout design using Answer Set Programming (ASP). The work formulates the layout problem as both a synthesis and multi-objective optimization task that simultaneously generates viable layouts while minimizing board area and component strip crossing. By leveraging ASP's declarative nature, this work expresses complex geometric and electrical constraints in a natural and concise manner. The two-phase solving methodology first ensures feasibility before optimizing layout quality. Experimental results demonstrate that this approach generates compact, manufacturable layouts for a range of circuit complexities. This work represents a significant advancement in automated stripboard layout, offering a practical tool for electronics prototyping and education while showcasing the power of declarative programming for solving complex design automation problems.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [4] [Toward Sustainability-Aware LLM Inference on Edge Clusters](https://arxiv.org/abs/2512.04088)
*Kolichala Rajashekar,Nafiseh Sharghivand,Radu Prodan,Reza Farahani*

Main category: cs.DC

TL;DR: 本文提出一种面向边缘集群的可持续LLM推理方法，通过碳感知与延迟感知的路由策略，在NVIDIA Jetson和Ada设备上平衡推理延迟与碳足迹。


<details>
  <summary>Details</summary>
Motivation: 为缓解云端LLM推理的高延迟与高能耗问题，探索在边缘设备上实现低环境成本、高性能的推理方案。

Method: 基于NVIDIA Jetson Orin NX与Ada 2000设备构建边缘集群，通过实证基准测试不同批次配置下的能耗与执行时间，设计并比较碳感知、延迟感知与贪心路由策略。

Result: 实验表明，批次大小为4时可在吞吐量与能效间取得最佳平衡，更大批次易导致GPU内存饱和。

Conclusion: 碳感知与延迟感知路由策略可有效优化边缘LLM推理的可持续性，为绿色AI部署提供可行路径。

Abstract: Large language models (LLMs) require substantial computational resources, leading to significant carbon emissions and operational costs. Although training is energy-intensive, the long-term environmental burden arises from inference, amplified by the massive global query volume. Cloud-based inference offers scalability but suffers from latency and bandwidth constraints due to centralized processing and continuous data transfer. Edge clusters instead can mitigate these limitations by enabling localized execution, yet they face trade-offs between performance, energy efficiency, and device constraints. This short paper presents a sustainability-aware LLM inference for edge clusters comprising NVIDIA Jetson Orin NX (8GB) and Nvidia Ada 2000 (16GB) devices. It aims to balance inference latency and carbon footprint through carbon- and latency-aware routing strategies, guided by empirical benchmarking of energy consumption and execution time across diverse prompts and batch (i.e., group of prompts) configurations. We compared baseline greedy strategies to carbon-aware and latency-aware strategies in prompt routing to specific hardware based on benchmarking information. Experimental evaluation shows that a batch size of four prompts achieves a trade-off between throughput, energy efficiency, while larger batches risk GPU memory saturation.

</details>


### [5] [Serverless Everywhere: A Comparative Analysis of WebAssembly Workflows Across Browser, Edge, and Cloud](https://arxiv.org/abs/2512.04089)
*Mario Colosi,Reza Farahani,Lauri Loven,Radu Prodan,Massimo Villari*

Main category: cs.DC

TL;DR: 本文评估了基于WebAssembly的无服务器工作流在浏览器、边缘和云端的一致性执行性能，发现AOT编译与实例预热显著降低启动延迟，浏览器适合小负载场景，而边缘/云节点在大负载下表现更优。


<details>
  <summary>Details</summary>
Motivation: 探索WebAssembly在异构平台（浏览器、边缘、云）上执行无服务器工作流时的性能与稳定性影响因素。

Method: 使用wasm32-wasi模块，在浏览器中通过web worker执行，边缘/云端通过HTTP shim传输帧；测量冷/热启动延迟、步骤延迟、总耗时、吞吐量及资源利用率。

Result: AOT编译与实例预热大幅减少启动延迟；小负载下浏览器性能具竞争力；大负载时边缘/云端AOT执行明显优于浏览器。

Conclusion: WebAssembly适用于跨平台无服务器工作流，但需根据负载大小选择部署环境并结合AOT优化以获得最佳性能。

Abstract: WebAssembly (Wasm) is a binary instruction format that enables portable, sandboxed, and near-native execution across heterogeneous platforms, making it well-suited for serverless workflow execution on browsers, edge nodes, and cloud servers. However, its performance and stability depend heavily on factors such as startup overhead, runtime execution model (e.g., Ahead-of-Time (AOT) and Just-in-Time (JIT) compilation), and resource variability across deployment contexts. This paper evaluates a Wasm-based serverless workflow executed consistently from the browser to edge and cloud instances. The setup uses wasm32-wasi modules: in the browser, execution occurs within a web worker, while on Edge and Cloud, an HTTP shim streams frames to the Wasm runtime. We measure cold- and warm-start latency, per-step delays, workflow makespan, throughput, and CPU/memory utilization to capture the end-to-end behavior across environments. Results show that AOT compilation and instance warming substantially reduce startup latency. For workflows with small payloads, the browser achieves competitive performance owing to fully in-memory data exchanges. In contrast, as payloads grow, the workflow transitions into a compute- and memory-intensive phase where AOT execution on edge and cloud nodes distinctly surpasses browser performance.

</details>


### [6] [Energy-Efficient Resource Management in Microservices-based Fog and Edge Computing: State-of-the-Art and Future Directions](https://arxiv.org/abs/2512.04093)
*Ali Akbar Vali,Sadoon Azizi,Mohammad Shojafar,Rajkumar Buyya*

Main category: cs.DC

TL;DR: 本文综述了2020-2024年间136多项关于雾计算与边缘计算中微服务资源管理的研究，聚焦节能方案并提出未来方向。


<details>
  <summary>Details</summary>
Motivation: 物联网设备激增带来对高效低延迟服务的需求，雾与边缘计算虽缓解了问题，但资源管理仍存挑战。

Method: 系统性回顾并分类现有研究，按优化技术、目标及优缺点划分至五大子领域，并分析文献缺口。

Result: 识别出现有方法缺乏组件协同，提出结合AI优化、量子计算与无服务器架构的未来研究路径。

Conclusion: 本综述为研究人员提供统一节能视角，推动更集成高效可持续的资源管理解决方案。

Abstract: The exponential growth of Internet of Things (IoT) devices has intensified the demand for efficient and responsive services. To address this demand, fog and edge computing have emerged as distributed paradigms that bring computational resources closer to end users, reducing latency, bandwidth limitations, and energy consumption. However, these paradigms present challenges in resource management due to resource constraints, computational heterogeneity, dynamic workloads, and diverse Quality of Service (QoS) requirements. This paper presents a comprehensive survey of state-of-the-art resource management strategies in microservices-based fog and edge computing, focusing on energy-efficient solutions. We systematically review and classify more than 136 studies (2020-2024) into five key subdomains: service placement, resource provisioning, task scheduling and offloading, resource allocation, and instance selection. Our categorization is based on optimization techniques, targeted objectives, and the strengths and limitations of each approach. In addition, we examine existing surveys and identify unresolved challenges and gaps in the literature. By highlighting the lack of synergy among fundamental resource management components, we outline promising research directions leveraging AI-driven optimization, quantum computing, and serverless computing. This survey serves as a comprehensive reference for researchers and practitioners by providing a unified and energy-aware perspective on resource management in microservices-based fog and edge computing, paving the way for more integrated, efficient, and sustainable future solutions.

</details>


### [7] [VLCs: Managing Parallelism with Virtualized Libraries](https://arxiv.org/abs/2512.04320)
*Yineng Yan,William Ruys,Hochan Lee,Ian Henriksen,Arthur Peters,Sean Stephens,Bozhi You,Henrique Fingler,Martin Burtscher,Milos Gligoric,Keshav Pingali,Mattan Erez,George Biros,Christopher J. Rossbach*

Main category: cs.DC

TL;DR: 提出虚拟库上下文（VLCs）以在不修改库代码前提下优化并行资源分配，实测最高提速2.85倍。


<details>
  <summary>Details</summary>
Motivation: 现代并行机器复杂度增加，现有库未考虑组合使用易引发资源争用与性能下降。

Method: 设计VLC作为进程子单元封装库及资源，实现资源隔离或并行加载线程不安全库。

Result: C++/Python原型在OpenMP、OpenBLAS、LibTorch等基准测试中最高获得2.85倍加速。

Conclusion: VLC有效解决库组合使用的资源争用问题，无需修改库或操作系统即可提升并行性能。

Abstract: As the complexity and scale of modern parallel machines continue to grow, programmers increasingly rely on composition of software libraries to encapsulate and exploit parallelism. However, many libraries are not designed with composition in mind and assume they have exclusive access to all resources. Using such libraries concurrently can result in contention and degraded performance. Prior solutions involve modifying the libraries or the OS, which is often infeasible.
  We propose Virtual Library Contexts (VLCs), which are process subunits that encapsulate sets of libraries and associated resource allocations. VLCs control the resource utilization of these libraries without modifying library code. This enables the user to partition resources between libraries to prevent contention, or load multiple copies of the same library to allow parallel execution of otherwise thread-unsafe code within the same process.
  In this paper, we describe and evaluate C++ and Python prototypes of VLCs. Experiments show VLCs enable a speedup up to 2.85x on benchmarks including applications using OpenMP, OpenBLAS, and LibTorch.

</details>


### [8] [Counting Without Running: Evaluating LLMs' Reasoning About Code Complexity](https://arxiv.org/abs/2512.04355)
*Gregory Bolet,Giorgis Georgakoudis,Konstantinos Parasyris,Harshitha Menon,Niranjan Hasabnis,Kirk W. Cameron,Gal Oren*

Main category: cs.DC

TL;DR: 提出gpuFLOPBench基准测试，评估LLM预测CUDA内核浮点运算次数的能力，揭示现有模型在硬件微码效应推理上的不足。


<details>
  <summary>Details</summary>
Motivation: 当前LLM缺乏对GPU性能瓶颈的前瞻性推理能力，影响代码优化与硬件选型。

Method: 构建包含577个CUDA内核的基准，标注真实性能数据与8项执行属性，测试模型预测单双精度FLOP数的准确性。

Result: 最新LLM可准确处理简单内核，但对隐式FLOP（如除法、数学函数）仍存在数量级误差。

Conclusion: 现有代码助手难以内化硬件特定微码效应，gpuFLOPBench可作为开发高性能推理工具的专用测试平台。

Abstract: Modern GPU software stacks demand developers who can anticipate performance bottlenecks before ever launching a kernel; misjudging floating-point workloads upstream can derail tuning, scheduling, and even hardware procurement. Yet despite rapid progress in code generation, today's Large Language Models (LLMs) are rarely tested on this kind of forward-looking reasoning. We close that gap with gpuFLOPBench, a benchmark that asks models to "count without running" by predicting single and double-precision FLOP counts for 577 CUDA kernels drawn from HeCBench, annotated with ground-truth profiles and eight execution attributes that distinguish trivially analyzable code from kernels whose FLOPs depend on hidden compiler or runtime behavior. Evaluating current closed-source reasoning models shows clear but uneven progress: the newest LLMs achieve perfect classification on straightforward kernels but still incur multiple order-of-magnitude errors whenever implicit FLOPs arise from division, intrinsic math functions, or common subexpressions. These results surface a core limitation of existing code assistants -- the inability to internalize hardware-specific microcode effects -- and position gpuFLOPBench as a focused testbed for developing LLM tooling that can reason about performance with the same rigor as experienced GPU developers. Sources are available at our repository: https://github.com/Scientific-Computing-Lab/gpuFLOPBench

</details>


### [9] [Formal Specification for Fast ACS: Low-Latency File-Based Ordered Message Delivery at Scale](https://arxiv.org/abs/2512.04096)
*Sushant Kumar Gupta,Anil Raghunath Iyer,Chang Yu,Neel Bagora,Olivier Pomerleau,Vivek Kumar,Prunthaban Kanthakumar*

Main category: cs.DC

TL;DR: Fast ACS 是一种基于文件的有序消息传递系统，结合了跨集群和集群内通信原语，实现低延迟、高扩展性的消息分发。


<details>
  <summary>Details</summary>
Motivation: 为应对大规模实时系统中数千消费者对低延迟、有序、至少一次投递消息的需求，设计高效可靠的消息系统。

Method: 采用两阶段通信：跨集群使用远程过程调用（RPC），集群内使用远程内存访问（RMA），并基于文件存储保证顺序性。

Result: 系统已部署于数十个生产集群，支持每集群数千消费者，峰值流量达Tbps级，全球范围内消息投递延迟为秒级或亚秒级（p99）。

Conclusion: Fast ACS 在低资源开销下实现了高吞吐、低延迟、强一致的消息投递，适用于超大规模分布式实时系统。

Abstract: Low-latency message delivery is crucial for real-time systems. Data originating from a producer must be delivered to consumers, potentially distributed in clusters across metropolitan and continental boundaries. With the growing scale of computing, there can be several thousand consumers of the data. Such systems require a robust messaging system capable of transmitting messages containing data across clusters and efficiently delivering them to consumers. The system must offer guarantees like ordering and at-least-once delivery while avoiding overload on consumers, allowing them to consume messages at their own pace.
  This paper presents the design of Fast ACS (an abbreviation for Ads Copy Service), a file-based ordered message delivery system that leverages a combination of two-sided (inter-cluster) and one-sided (intra-cluster) communication primitives - namely, Remote Procedure Call and Remote Memory Access, respectively - to deliver messages. The system has been successfully deployed to dozens of production clusters and scales to accommodate several thousand consumers within each cluster, which amounts to Tbps-scale intra-cluster consumer traffic at peak. Notably, Fast ACS delivers messages to consumers across the globe within a few seconds or even sub-seconds (p99) based on the message volume and consumer scale, at a low resource cost.

</details>


### [10] [tritonBLAS: Triton-based Analytical Approach for GEMM Kernel Parameter Selection](https://arxiv.org/abs/2512.04226)
*Ryan Swann,Muhammad Osama,Xiaohu Guo,Bryant Nelson,Lixun Zhang,Alex Brown,Yen Ong,Ali Yazdani,Sean Siddens,Ganesh Dasika,Alex Underwood*

Main category: cs.DC

TL;DR: tritonBLAS 是一个基于 Triton 的轻量级 GEMM 框架，通过分析模型预测最优配置，无需运行时自动调优，性能达自动调优方案的 95% 以上。


<details>
  <summary>Details</summary>
Motivation: 消除传统 GEMM 内核对耗时运行时自动调优的依赖，提升高性能计算和机器学习工作负载的实际部署效率。

Method: 利用缓存层次结构、代码与数据布局等架构参数构建确定性分析模型，结合矩阵形状与算法分块行为预测近似最优配置。

Result: 在现代 GPU 上多种 GEMM 问题规模测试中，性能达到自动调优方案的 95% 以上，且调优时间为零。

Conclusion: tritonBLAS 可作为生产环境中经验调优的实用替代方案，兼顾高效性与即时部署能力。

Abstract: We present tritonBLAS, a fast and deterministic analytical model that uses architectural parameters like the cache hierarchy, and relative code and data placement to generate performant GPU GEMM kernels. tritonBLAS explicitly models the relationship between architectural topology, matrix shapes, and algorithmic blocking behavior to predict near-optimal configurations without runtime autotuning. Based on this model, we developed and implemented a lightweight GEMM framework entirely within Triton. We evaluate the performance of tritonBLAS across a diverse set of GEMM problem sizes on modern GPUs. tritonBLAS achieves over 95% of the performance of autotuning solutions, while reducing autotuning time to zero. This makes tritonBLAS a practical drop-in replacement for empirical tuning in production HPC and ML workloads.

</details>


### [11] [A Structure-Aware Irregular Blocking Method for Sparse LU Factorization](https://arxiv.org/abs/2512.04389)
*Zhen Hu,Dongliang Xiong,Kai Huang,Changjun Wu,Xiaowen Jiang*

Main category: cs.DC

TL;DR: 提出一种结构感知的不规则分块方法，以优化稀疏LU分解中的非零元素分布和负载均衡。


<details>
  <summary>Details</summary>
Motivation: 稀疏矩阵在符号分解后非零元素集中在对角线和右下区域，传统2D分块易导致负载不均，现有特征无法有效指导分块。

Method: 引入基于对角块的新特征刻画局部非零分布，并据此设计不规则分块策略：密集区用细粒度块、稀疏区用粗粒度块，平衡依赖树内及跨层的非零元素分布。

Result: 单A100 GPU上相较PanguLU和SuperLU_DIST平均提速1.50x和3.32x；4块A100上提速1.40x和3.84x。

Conclusion: 该方法显著提升稀疏LU分解效率，有效解决负载不均问题。

Abstract: In sparse LU factorization, nonzero elements after symbolic factorization tend to distribute in diagonal and right-bottom region of sparse matrices. However, regular 2D blocking on this non-uniform distribution structure may lead to workload imbalance across blocks. Besides, existing matrix features fail to guide us effectively in blocking. In this paper, we propose a structure-aware irregular blocking method for numerical factorization. A novel diagonal block-based feature is introduced to effectively characterize the local nonzero distribution of sparse matrices. Based on this, we further propose an irregular blocking method that adjusts block sizes according to the local distribution of nonzeros. The strategy utilizes fine-grained blocks in dense regions and coarse-grained blocks in sparse regions, adequately balancing the nonzeros of blocks both within the same level and across levels in the dependency tree. Experiments demonstrate that, on a single NVIDIA A100 GPU, our proposed irregular blocking method achieves average speedups of 1.50x and 3.32x over PanguLU and the latest SuperLU_DIST, respectively. In addition, it achieves speedups of 1.40x and 3.84x over PanguLU and SuperLU_DIST on 4 NVIDIA A100 GPUs.

</details>


### [12] [Offloading to CXL-based Computational Memory](https://arxiv.org/abs/2512.04449)
*Suyeon Lee,Kangkyu Park,Kwangsik Shin,Ada Gavrilovska*

Main category: cs.DC

TL;DR: 提出KAI系统，通过异步回流协议优化CXL计算内存性能。


<details>
  <summary>Details</summary>
Motivation: 现有卸载机制无法有效利用不同CXL协议的权衡，影响性能与效率。

Method: 设计异步回流协议，在CXL协议上分层数据与控制传输，并构建KAI系统实现轻量流水线与异步数据移动。

Result: 端到端运行时间最多减少50.4%，CCM和主机空闲时间平均降低22.11倍和3.85倍。

Conclusion: KAI有效提升CXL计算内存系统的性能与资源利用率。

Abstract: CXL-based Computational Memory (CCM) enables near-memory processing within expanded remote memory, presenting opportunities to address data movement costs associated with disaggregated memory systems and to accelerate overall performance. However, existing operation offloading mechanisms are not capable of leveraging the trade-offs of different models based on different CXL protocols. This work first examines these tradeoffs and demonstrates their impact on end-to-end performance and system efficiency for workloads with diverse data and processing requirements. We propose a novel 'Asynchronous Back-Streaming' protocol by carefully layering data and control transfer operations on top of the underlying CXL protocols. We design KAI, a system that realizes the asynchronous back-streaming model that supports asynchronous data movement and lightweight pipelining in host-CCM interactions. Overall, KAI reduces end-to-end runtime by up to 50.4%, and CCM and host idle times by average 22.11x and 3.85x, respectively.

</details>


### [13] [Federated Learning for Terahertz Wireless Communication](https://arxiv.org/abs/2512.04984)
*O. Tansel Baydas,Ozgur B. Akan*

Main category: cs.DC

TL;DR: 本文提出一种多载波随机框架，分析太赫兹频段中宽带损伤对联邦学习收敛性的影响，并提出SNR加权聚合策略以克服频谱空洞问题。


<details>
  <summary>Details</summary>
Motivation: 现有研究未理论刻画太赫兹通信中宽带损伤对联邦学习优化动态的影响，亟需建立相应分析框架。

Method: 构建耦合本地梯度更新与频率选择性太赫兹效应（如波束偏斜、分子吸收、抖动）的多载波随机模型，推导收敛误差下界并提出SNR加权聚合方法。

Result: 发现标准无偏聚合下收敛误差由子载波信噪比调和均值主导；识别带宽扩展存在临界点，超过后因热噪声与边缘增益崩溃导致性能下降；SNR加权聚合可有效恢复高偏斜场景下的收敛性。

Conclusion: 太赫兹联邦学习系统需考虑物理层非理想特性，采用信道感知聚合策略方能保障分布式学习收敛性能。

Abstract: The convergence of Terahertz (THz) communications and Federated Learning (FL) promises ultra-fast distributed learning, yet the impact of realistic wideband impairments on optimization dynamics remains theoretically uncharacterized. This paper bridges this gap by developing a multicarrier stochastic framework that explicitly couples local gradient updates with frequency-selective THz effects, including beam squint, molecular absorption, and jitter. Our analysis uncovers a critical diversity trap: under standard unbiased aggregation, the convergence error floor is driven by the harmonic mean of subcarrier SNRs. Consequently, a single spectral hole caused by severe beam squint can render the entire bandwidth useless for reliable model updates. We further identify a fundamental bandwidth limit, revealing that expanding the spectrum beyond a critical point degrades convergence due to the integration of thermal noise and gain collapse at band edges. Finally, we demonstrate that an SNR-weighted aggregation strategy is necessary to suppress the variance singularity at these spectral holes, effectively recovering convergence in high-squint regimes where standard averaging fails. Numerical results validate the expected impact of the discussed physical layer parameters' on performance of THz-FL systems.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [14] [Semi Centralized Training Decentralized Execution Architecture for Multi Agent Deep Reinforcement Learning in Traffic Signal Control](https://arxiv.org/abs/2512.04653)
*Pouria Yazdani,Arash Rezaali,Monireh Abdoos*

Main category: cs.MA

TL;DR: 提出一种半集中式训练、分布式执行的多智能体强化学习架构，用于自适应交通信号控制，显著提升多路口协同效率。


<details>
  <summary>Details</summary>
Motivation: 现有全集中或全分布式方法存在维度灾难、局部观测不足或缺乏显式协调等问题，需改进区域化多智能体学习框架。

Method: 设计SEMI-CTDE架构，在区域内集中训练并共享参数，采用融合局部与区域信息的状态与奖励函数，支持多种策略主干和状态奖励实例化。

Result: 实验表明所提模型在不同交通密度和分布下性能稳定优越，优于规则基线和全分布式方法。

Conclusion: SEMI-CTDE架构有效平衡了集中训练与分布式执行的优势，为多路口自适应信号控制提供高效、可迁移的解决方案。

Abstract: Multi-agent reinforcement learning (MARL) has emerged as a promising paradigm for adaptive traffic signal control (ATSC) of multiple intersections. Existing approaches typically follow either a fully centralized or a fully decentralized design. Fully centralized approaches suffer from the curse of dimensionality, and reliance on a single learning server, whereas purely decentralized approaches operate under severe partial observability and lack explicit coordination resulting in suboptimal performance. These limitations motivate region-based MARL, where the network is partitioned into smaller, tightly coupled intersections that form regions, and training is organized around these regions. This paper introduces a Semi-Centralized Training, Decentralized Execution (SEMI-CTDE) architecture for multi intersection ATSC. Within each region, SEMI-CTDE performs centralized training with regional parameter sharing and employs composite state and reward formulations that jointly encode local and regional information. The architecture is highly transferable across different policy backbones and state-reward instantiations. Building on this architecture, we implement two models with distinct design objectives. A multi-perspective experimental analysis of the two implemented SEMI-CTDE-based models covering ablations of the architecture's core elements including rule based and fully decentralized baselines shows that they achieve consistently superior performance and remain effective across a wide range of traffic densities and distributions.

</details>


### [15] [Complementary Characterization of Agent-Based Models via Computational Mechanics and Diffusion Models](https://arxiv.org/abs/2512.04771)
*Roberto Garrone*

Main category: cs.MA

TL;DR: 本文结合ε-机器与扩散模型，从时序结构和分布几何两轴分析ABM输出，首次整合计算力学与生成建模方法。


<details>
  <summary>Details</summary>
Motivation: 为更全面刻画基于主体模型（ABM）的复杂输出，需同时分析其时间动态与高维分布结构。

Method: 联合使用ε-机器（分析时间序列预测结构）与扩散模型（学习截面分布与数据流形），构建双轴分析框架。

Result: 在老年照护ABM数据集上验证了该框架，形式化证明两种方法数学互补性，可协同分析时序可预测性与分布结构。

Conclusion: 该框架为ABM输出提供了融合现代机器学习密度估计与内在计算理论的系统性分析方法。

Abstract: This article extends the preprint "Characterizing Agent-Based Model Dynamics via $ε$-Machines and Kolmogorov-Style Complexity" by introducing diffusion models as orthogonal and complementary tools for characterizing the output of agent-based models (ABMs). Where $ε$-machines capture the predictive temporal structure and intrinsic computation of ABM-generated time series, diffusion models characterize high-dimensional cross-sectional distributions, learn underlying data manifolds, and enable synthetic generation of plausible population-level outcomes. We provide a formal analysis demonstrating that the two approaches operate on distinct mathematical domains -processes vs.\ distributions- and show that their combination yields a two-axis representation of ABM behavior based on temporal organization and distributional geometry. To our knowledge, this is the first framework to integrate computational mechanics with score-based generative modeling for the structural analysis of ABM outputs, thereby situating ABM characterization within the broader landscape of modern machine-learning methods for density estimation and intrinsic computation. The framework is validated using the same elder-caregiver ABM dataset introduced in the companion paper, and we provide precise definitions and propositions formalizing the mathematical complementarity between $ε$-machines and diffusion models. This establishes a principled methodology for jointly analyzing temporal predictability and high-dimensional distributional structure in complex simulation models.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [16] [Retrieval-Augmented Few-Shot Prompting Versus Fine-Tuning for Code Vulnerability Detection](https://arxiv.org/abs/2512.04106)
*Fouad Trad,Ali Chehab*

Main category: cs.SE

TL;DR: 本文提出检索增强提示方法，在代码漏洞检测任务中显著优于随机示例提示和零样本提示，且无需微调成本。


<details>
  <summary>Details</summary>
Motivation: 提升大语言模型在复杂领域（如代码漏洞检测）中少样本提示的效果，降低对高质量上下文示例的依赖。

Method: 采用三种策略：随机示例提示、语义相似检索增强提示、基于检索的标签分配，并在Gemini-1.5-Flash上系统评估。

Result: 检索增强提示在20样本下F1达74.05%，部分匹配准确率83.90%，优于零样本和微调Gemini，但低于需训练的CodeBERT。

Conclusion: 检索增强提示是高效低成本的替代方案，在性能与资源消耗间取得良好平衡，适合实际部署。

Abstract: Few-shot prompting has emerged as a practical alternative to fine-tuning for leveraging the capabilities of large language models (LLMs) in specialized tasks. However, its effectiveness depends heavily on the selection and quality of in-context examples, particularly in complex domains. In this work, we examine retrieval-augmented prompting as a strategy to improve few-shot performance in code vulnerability detection, where the goal is to identify one or more security-relevant weaknesses present in a given code snippet from a predefined set of vulnerability categories. We perform a systematic evaluation using the Gemini-1.5-Flash model across three approaches: (1) standard few-shot prompting with randomly selected examples, (2) retrieval-augmented prompting using semantically similar examples, and (3) retrieval-based labeling, which assigns labels based on retrieved examples without model inference. Our results show that retrieval-augmented prompting consistently outperforms the other prompting strategies. At 20 shots, it achieves an F1 score of 74.05% and a partial match accuracy of 83.90%. We further compare this approach against zero-shot prompting and several fine-tuned models, including Gemini-1.5-Flash and smaller open-source models such as DistilBERT, DistilGPT2, and CodeBERT. Retrieval-augmented prompting outperforms both zero-shot (F1 score: 36.35%, partial match accuracy: 20.30%) and fine-tuned Gemini (F1 score: 59.31%, partial match accuracy: 53.10%), while avoiding the training time and cost associated with model fine-tuning. On the other hand, fine-tuning CodeBERT yields higher performance (F1 score: 91.22%, partial match accuracy: 91.30%) but requires additional training, maintenance effort, and resources.

</details>


### [17] [HAI-Eval: Measuring Human-AI Synergy in Collaborative Coding](https://arxiv.org/abs/2512.04111)
*Hanjun Luo,Chiming Ni,Jiaheng Wen,Zhimu Huang,Yiran Wang,Bingduo Liao,Sylvia Chung,Yingbin Jin,Xinfeng Li,Wenyuan Xu,XiaoFeng Wang,Hanan Salam*

Main category: cs.SE

TL;DR: HAI-Eval 是一个衡量人机协作编程效能的新基准，通过45种‘必须协作’模板动态生成任务，实证显示人机协同显著提升解题成功率，并揭示双向共推理伙伴关系。


<details>
  <summary>Details</summary>
Motivation: 现有评估体系无法衡量人与LLM在编程中的协同效应，亟需面向真实协作场景的统一评测框架。

Method: 设计45种协作必要型问题模板，构建标准化IDE与可复现工具包，在45名参与者与5个主流LLM上开展对照实验，评估不同干预层级下的表现。

Result: 独立LLM与人类解题通过率仅0.67%与18.89%，而人机协作提升至31.11%，并发现突破性策略可由任一方发起，形成共推理伙伴关系。

Conclusion: HAI-Eval 为下一代编程智能体提供挑战性基准，同时建立评估AI时代开发者核心能力的可扩展框架。

Abstract: LLM-powered coding agents are reshaping the development paradigm. However, existing evaluation systems, neither traditional tests for humans nor benchmarks for LLMs, fail to capture this shift. They remain focused on well-defined algorithmic problems, which excludes problems where success depends on human-AI collaboration. Such collaborative problems not only require human reasoning to interpret complex contexts and guide solution strategies, but also demand AI efficiency for implementation. To bridge this gap, we introduce HAI-Eval, a unified benchmark designed to measure the synergy of human-AI partnership in coding. HAI-Eval's core innovation is its "Collaboration-Necessary" problem templates, which are intractable for both standalone LLMs and unaided humans, but solvable through effective collaboration. Specifically, HAI-Eval uses 45 templates to dynamically create tasks. It also provides a standardized IDE for human participants and a reproducible toolkit with 450 task instances for LLMs, ensuring an ecologically valid evaluation. We conduct a within-subject study with 45 participants and benchmark their performance against 5 state-of-the-art LLMs under 4 different levels of human intervention. Results show that standalone LLMs and unaided participants achieve poor pass rates (0.67% and 18.89%), human-AI collaboration significantly improves performance to 31.11%. Our analysis reveals an emerging co-reasoning partnership. This finding challenges the traditional human-tool hierarchy by showing that strategic breakthroughs can originate from either humans or AI. HAI-Eval establishes not only a challenging benchmark for next-generation coding agents but also a grounded, scalable framework for assessing core developer competencies in the AI era. Our benchmark and interactive demo will be openly accessible.

</details>


### [18] [Reusing Model Validation Methods for the Continuous Validation of Digital Twins of Cyber-Physical Systems](https://arxiv.org/abs/2512.04117)
*Joost Mertens,Joachim Denil*

Main category: cs.SE

TL;DR: 本文提出了一种基于模型验证技术的通用方法，用于检测数字孪生系统中的异常，并通过参数估计修正数字孪生模型，以保持其与物理系统的同步。


<details>
  <summary>Details</summary>
Motivation: 物理系统会因维护、磨损或人为错误而不断演化，因此需要确保数字孪生能随之更新，维持其代表性。

Method: 复用基于模型设计中的验证技术，结合验证指标检测异常，并利用历史数据进行参数估计来修正数字孪生模型。

Result: 在港口常见的龙门起重机案例中成功验证了该方法的有效性，实现了对数字孪生模型的动态校正。

Conclusion: 所提方法能有效检测并修正数字孪生与物理系统间的偏差，提升数字孪生的长期可信度与实用性。

Abstract: One of the challenges in twinned systems is ensuring the digital twin remains a valid representation of the system it twins. Depending on the type of twinning occurring, it is either trivial, such as in dashboarding/visualizations that mirror the system with real-time data, or challenging, in case the digital twin is a simulation model that reflects the behavior of a physical twinned system. The challenge in this latter case comes from the fact that in contrast to software systems, physical systems are not immutable once deployed, but instead they evolve through processes like maintenance, wear and tear or user error. It is therefore important to detect when changes occur in the physical system to evolve the twin alongside it. We employ and reuse validation techniques from model-based design for this goal. Model validation is one of the steps used to gain trust in the representativeness of a simulation model. In this work, we provide two contributions: (i) we provide a generic approach that, through the use of validation metrics, is able to detect anomalies in twinned systems, and (ii) we demonstrate these techniques with the help of an academic yet industrially relevant case study of a gantry crane such as found in ports. Treating anomalies also means correcting the error in the digital twin, which we do with a parameter estimation based on the historical data.

</details>


### [19] [DrP: Meta's Efficient Investigations Platform at Scale](https://arxiv.org/abs/2512.04250)
*Shubham Somani,Vanish Talwar,Madhura Parikh,Eduardo Hernandez,Jimmy Wang,Shreya Shah,Chinmay Gandhi,Sanjay Sundarajan,Neeru Sharma,Srikanth Kamath,Nitin Gupta,Benjamin Renard,Ohad Yahalom,Chris Davis*

Main category: cs.SE

TL;DR: DrP是一个自动化调查框架，通过可编程的分析器和可扩展后端系统，显著降低故障平均解决时间并减轻运维负担。


<details>
  <summary>Details</summary>
Motivation: 当前大规模系统中的故障调查流程多为手动或依赖临时脚本，效率低下且增加运维负担。

Method: 提出DrP框架，包含灵活SDK编写分析器、可扩展执行后端、与告警及事件管理工具集成插件、以及事后处理系统。

Result: 在Meta部署5年，覆盖300+团队、2000+分析器，日均执行5万次自动化分析，平均MTTR降低20%，部分团队达80%，显著提升运维效率。

Conclusion: DrP有效实现了大规模系统中故障调查的自动化，大幅缩短响应时间并优化运维生产力。

Abstract: Investigations are a significant step in the operational workflows for large scale systems across multiple domains such as services, data, AI/ML, mobile. Investigation processes followed by on-call engineers are often manual or rely on ad-hoc scripts. This leads to inefficient investigations resulting in increased time to mitigate and isolate failures/SLO violations. It also contributes to on-call toil and poor productivity leading to multiple hours/days spent in triaging/debugging incidents. In this paper, we present DrP, an end-to-end framework and system to automate investigations that reduces the mean time to resolve incidents (MTTR) and reduces on-call toil. DrP consists of an expressive and flexible SDK to author investigation playbooks in code (called analyzers), a scalable backend system to execute these automated playbooks, plug-ins to integrate playbooks into mainstream workflows such as alerts and incident management tools, and a post-processing system to take actions on investigations including mitigation steps.
  We have implemented and deployed DrP at large scale at Meta covering 300+ teams, 2000+ analyzers, across a large set of use cases across domains such as services, core infrastructure, AI/ML, hardware, mobile. DrP has been running in production for the past 5 years and executes 50K automated analyses per day. Overall, our results and experience show that DrP has been able to reduce average MTTR by 20 percent at large scale (with over 80 percent for some teams) and has significantly improved on-call productivity.

</details>


### [20] [On the Role and Impact of GenAI Tools in Software Engineering Education](https://arxiv.org/abs/2512.04256)
*Qiaolin Qin,Ronnie de Souza Santos,Rodrigo Spinola*

Main category: cs.SE

TL;DR: 本研究调查了本科生在软件工程教育中使用生成式AI工具的情况，发现其在学习支持和信心建立方面有益，但也存在输出适应困难和伦理问题，呼吁加强教学指导与政策支持。


<details>
  <summary>Details</summary>
Motivation: 探索生成式AI工具在软件工程教育中的实际应用、学生体验及潜在挑战，以推动更公平有效的教学实践。

Method: 通过对两所大学130名本科生进行混合式问卷调查（含李克特量表与开放性问题），从使用情境、感知益处、挑战、伦理与教学期望五个维度展开分析。

Result: 学生主要将GenAI用于渐进学习与高级实现，认为其有助于头脑风暴与增强自信；但常遇输出逻辑不清、适配困难等问题，并担忧公平性与学术不端，期待明确教学指引。

Conclusion: 生成式AI正以复杂方式重塑软件工程教育，需通过教学支架、伦理规范与适应性策略确保其促进公平且高效的学习体验。

Abstract: Context. The rise of generative AI (GenAI) tools like ChatGPT and GitHub Copilot has transformed how software is learned and written. In software engineering (SE) education, these tools offer new opportunities for support, but also raise concerns about over-reliance, ethical use, and impacts on learning. Objective. This study investigates how undergraduate SE students use GenAI tools, focusing on the benefits, challenges, ethical concerns, and instructional expectations that shape their experiences. Method. We conducted a survey with 130 undergraduate students from two universities. The survey combined structured Likert-scale items and open-ended questions to investigate five dimensions: usage context, perceived benefits, challenges, ethical and instructional perceptions. Results. Students most often use GenAI for incremental learning and advanced implementation, reporting benefits such as brainstorming support and confidence-building. At the same time, they face challenges including unclear rationales and difficulty adapting outputs. Students highlight ethical concerns around fairness and misconduct, and call for clearer instructional guidance. Conclusion. GenAI is reshaping SE education in nuanced ways. Our findings underscore the need for scaffolding, ethical policies, and adaptive instructional strategies to ensure that GenAI supports equitable and effective learning.

</details>


### [21] [Catching UX Flaws in Code: Leveraging LLMs to Identify Usability Flaws at the Development Stage](https://arxiv.org/abs/2512.04262)
*Nolan Platt,Ethan Luchs,Sehrish Nizamani*

Main category: cs.SE

TL;DR: 本文探讨了大语言模型（如GPT-4o）在早期开发阶段进行启发式可用性评估的可行性与局限性。


<details>
  <summary>Details</summary>
Motivation: 传统人工专家评估耗时且主观，希望借助LLM实现自动化、一致性的可用性测试。

Method: 基于Nielsen十大可用性启发式，对30个开源网站使用GPT-4o生成850+条评估，计算Cohen's Kappa与Krippendorff's Alpha衡量一致性。

Result: 问题检测一致性中等（Kappa 0.50），严重性判断差异较大（精确一致率仅56%，Alpha近零）。

Conclusion: GPT-4o可辅助识别可用性问题，但严重性评估仍需人工监督；为自动化UX评估提供基础与改进方向。

Abstract: Usability evaluations are essential for ensuring that modern interfaces meet user needs, yet traditional heuristic evaluations by human experts can be time-consuming and subjective, especially early in development. This paper investigates whether large language models (LLMs) can provide reliable and consistent heuristic assessments at the development stage. By applying Jakob Nielsen's ten usability heuristics to thirty open-source websites, we generated over 850 heuristic evaluations in three independent evaluations per site using a pipeline of OpenAI's GPT-4o. For issue detection, the model demonstrated moderate consistency, with an average pairwise Cohen's Kappa of 0.50 and an exact agreement of 84%. Severity judgments showed more variability: weighted Cohen's Kappa averaged 0.63, but exact agreement was just 56%, and Krippendorff's Alpha was near zero. These results suggest that while GPT-4o can produce internally consistent evaluations, especially for identifying the presence of usability issues, its ability to judge severity varies and requires human oversight in practice. Our findings highlight the feasibility and limitations of using LLMs for early-stage, automated usability testing, and offer a foundation for improving consistency in automated User Experience (UX) evaluation. To the best of our knowledge, our work provides one of the first quantitative inter-rater reliability analyses of automated heuristic evaluation and highlights methods for improving model consistency.

</details>


### [22] [Polynomiogram: An Integrated Framework for Root Visualization and Generative Art](https://arxiv.org/abs/2512.04263)
*Hoang Duc Nguyen,Anh Van Pham,Hien D. Nguyen*

Main category: cs.SE

TL;DR: Polynomiogram框架结合科学计算与算法艺术，通过灵活采样和双引擎架构实现多项式根系统的可视化与生成艺术。


<details>
  <summary>Details</summary>
Motivation: 探索多项式根系统在科学研究与生成艺术中的双重应用价值。

Method: 采用灵活参数映射与NumPy/MPSolve双引擎计算架构，支持高效可视化与高精度验证。

Result: 成功分析三次多项式分岔结构，生成类木槿花自然形态及AI致敬艺术作品。

Conclusion: 该框架兼具科研、教育与艺术创作潜力，拓展了多项式根系统的跨领域应用。

Abstract: This work presents the Polynomiogram framework, an integrated computational platform for exploring, visualizing, and generating art from polynomial root systems. The main innovation is a flexible sampling scheme in which two independent parameters are drawn from user defined domains and mapped to the polynomial coefficients through a generating function. This design allows the same mathematical foundation to support both scientific investigation and generative algorithmic art. The framework integrates two complementary numerical engines: NumPy companion matrix solver for fast, large scale computation and MPSolve for high precision, scientifically rigorous validation. This dual architecture enables efficient visualization for creative use and accurate computation for research and education. Numerical accuracy was verified using classical ensembles, including the Kac and Lucas polynomials. The method was applied to the cubic polynomial system to analyze its bifurcation structure, demonstrating its value as both a scientific tool for exploring root phenomena and an educational aid for visualizing fundamental concepts in algebra and dynamical systems. Beyond analysis, the Polynomiogram also demonstrated its potential as a tool for personalized generative art. Examples include the use of the platform to generate a natural form resembling a hibiscus flower and to create personalized artwork expressing gratitude toward advances in artificial intelligence and large language models through a tribute composition.

</details>


### [23] [Quantitative Analysis of Technical Debt and Pattern Violation in Large Language Model Architectures](https://arxiv.org/abs/2512.04273)
*Tyler Slater*

Main category: cs.SE

TL;DR: 该研究首次量化了大语言模型在微服务架构设计中的技术债务积累问题，发现开源模型相比闭源模型存在更高的架构违规率和代码逻辑缺失现象。


<details>
  <summary>Details</summary>
Motivation: 探索大语言模型从代码补全工具转向系统架构师角色后，对软件长期可维护性的影响，填补当前研究仅关注功能正确性的空白。

Method: 通过提示三种前沿模型（GPT-5.1、Claude 4.5 Sonnet、Llama 3 8B）在六边形架构约束下实现标准化图书借阅微服务，并利用AST解析评估架构合规性与代码复杂度。

Result: 闭源模型（如GPT-5.1）架构违规率为0%，而Llama 3高达80%；开源模型平均少生成60%逻辑代码行，存在‘实现惰性’导致结构性技术债务加速积累。

Conclusion: 若缺乏自动化架构检查机制，使用小型开源模型进行系统搭建将显著增加长期维护成本和技术债务风险。

Abstract: As Large Language Models (LLMs) transition from code completion tools to autonomous system architects, their impact on long-term software maintainability remains unquantified. While existing research benchmarks functional correctness (pass@k), this study presents the first empirical framework to measure "Architectural Erosion" and the accumulation of Technical Debt in AI-synthesized microservices. We conducted a comparative pilot study of three state-of-the-art models (GPT-5.1, Claude 4.5 Sonnet, and Llama 3 8B) by prompting them to implement a standardized Book Lending Microservice under strict Hexagonal Architecture constraints. Utilizing Abstract Syntax Tree (AST) parsing, we find that while proprietary models achieve high architectural conformance (0% violation rate for GPT-5.1), open-weights models exhibit critical divergence. Specifically, Llama 3 demonstrated an 80% Architectural Violation Rate, frequently bypassing interface adapters to create illegal circular dependencies between Domain and Infrastructure layers. Furthermore, we identified a phenomenon of "Implementation Laziness," where open-weights models generated 60% fewer Logical Lines of Code (LLOC) than their proprietary counterparts, effectively omitting complex business logic to satisfy token constraints. These findings suggest that without automated architectural linting, utilizing smaller open-weights models for system scaffolding accelerates the accumulation of structural technical debt.

</details>


### [24] [MANTRA: a Framework for Multi-stage Adaptive Noise TReAtment During Training](https://arxiv.org/abs/2512.04319)
*Zixiao Zhao,Fatemeh H. Fard,Jie JW Wu*

Main category: cs.SE

TL;DR: MANTRA是一种多阶段自适应噪声处理框架，通过在代码预训练模型和代码大语言模型微调过程中嵌入噪声诊断与缓解机制，提升模型在含噪数据下的性能。


<details>
  <summary>Details</summary>
Motivation: 软件工程领域的大规模数据集中常包含噪声或错误标签，影响模型准确性和鲁棒性，而现有噪声标签学习方法在该领域研究较少。

Method: 提出MANTRA框架，结合样本损失动态和高斯混合模型聚类，采用自适应Dropout策略剔除持续噪声样本，保留干净数据。

Result: 在代码摘要和提交意图分类任务中，MANTRA显著提升了各类模型的性能，且降低了数据清洗成本。

Conclusion: MANTRA有效缓解了训练数据噪声对模型的影响，提高了微调效率和模型鲁棒性，适用于多种代码相关任务。

Abstract: The reliable application of deep learning models to software engineering tasks hinges on high-quality training data. Yet, large-scale repositories inevitably introduce noisy or mislabeled examples that degrade both accuracy and robustness. While Noise Label Learning (NLL) has been extensively studied in other fields, there are a few works that investigate NLL in Software Engineering (SE) and Large Language Models (LLMs) for SE tasks. In this work, we propose MANTRA, a Multi-stage Adaptive Noise TReAtment framework that embeds noise diagnosis and mitigation directly into the fine-tuning process of code-Pretrained Language Models (PTM) and code-LLMs. We first investigate the effect of noise at varying levels on convergence and loss trajectories of the models. Then we apply an adaptive dropout strategy guided by per-sample loss dynamics and Gaussian Mixture Model clustering to exclude persistently noisy points while preserving clean data. Applying to code summarization and commit intent classification, our experiments reveal that some LLMs are more sensitive to noise than others. However, with MANTRA, the performance of all models in both tasks is improved. MANTRA enables researchers and practitioners to reduce the impact of errors introduced by the dataset in training, saves time in data cleaning and processing, while maximizing the effect of fine-tuning.

</details>


### [25] [Targeted Testing of Compiler Optimizations via Grammar-Level Composition Styles](https://arxiv.org/abs/2512.04344)
*Zitong Zhou,Ben Limpanukorn,Hong Jin Kang,Jiyuan Wang,Yaoxuan Wu,Akos Kiss,Renata Hodovan,Miryung Kim*

Main category: cs.SE

TL;DR: TargetFuzz 是一种基于语法的变异模糊测试工具，通过挖掘和重建优化相关的结构关系，有效提升编译器优化的测试覆盖率和触发率。


<details>
  <summary>Details</summary>
Motivation: 现有模糊测试工具难以有效测试编译器优化，因其依赖优化流水线且难以生成满足特定结构关系的输入。

Method: 提出针对单个优化的定向模糊测试方法，利用程序构造的组合样式（如相邻、嵌套等），从相关语料中挖掘样式并在通用语料中重建以测试优化逻辑。

Result: 在 LLVM 和 MLIR 上，TargetFuzz 分别提升覆盖率 8% 和 11%，优化触发次数提高 2.8 倍和 2.6 倍，并能有效测试所有 37 个采样 LLVM 优化。

Conclusion: 定向模糊测试可作为流水线测试的有效补充，尤其适用于模块化框架如 MLIR，无需手工编写生成器或语言特定变异器。

Abstract: Ensuring the correctness of compiler optimizations is critical, but existing fuzzers struggle to test optimizations effectively. First, most fuzzers use optimization pipelines (heuristics-based, fixed sequences of passes) as their harness. The phase-ordering problem can enable or preempt transformations, so pipelines inevitably miss optimization interactions; moreover, many optimizations are not scheduled, even at aggressive levels. Second, optimizations typically fire only when inputs satisfy specific structural relationships, which existing generators and mutations struggle to produce. We propose targeted fuzzing of individual optimizations to complement pipeline-based testing. Our key idea is to exploit composition styles - structural relations over program constructs (adjacency, nesting, repetition, ordering) - that optimizations look for. We build a general-purpose, grammar-based mutational fuzzer, TargetFuzz, that (i) mines composition styles from an optimization-relevant corpus, then (ii) rebuilds them inside different contexts offered by a larger, generic corpus via synthesized mutations to test variations of optimization logic. TargetFuzz is adaptable to a new programming language by lightweight, grammar-based, construct annotations - and it automatically synthesizes mutators and crossovers to rebuild composition styles. No need for hand-coded generators or language-specific mutators, which is particularly useful for modular frameworks such as MLIR, whose dialect-based, rapidly evolving ecosystem makes optimizations difficult to fuzz. Our evaluation on LLVM and MLIR shows that TargetFuzz improves coverage by 8% and 11% and triggers optimizations 2.8$\times$ and 2.6$\times$, compared to baseline fuzzers under the targeted fuzzing mode. We show that targeted fuzzing is complementary: it effectively tests all 37 sampled LLVM optimizations, while pipeline-fuzzing missed 12.

</details>


### [26] [LLM-SrcLog: Towards Proactive and Unified Log Template Extraction via Large Language Models](https://arxiv.org/abs/2512.04474)
*Jiaqi Sun,Wei Li,Heng Zhang,Chutong Ding,Shiyou Qian,Jian Cao,Guangtao Xue*

Main category: cs.SE

TL;DR: LLM-SrcLog 是一种结合源代码分析与数据驱动的日志模板解析框架，显著提升准确率并降低延迟。


<details>
  <summary>Details</summary>
Motivation: 现有日志解析方法多为被动式、仅依赖日志数据，难以适应动态结构和系统演进，且逐条使用LLM推理成本过高。

Method: 提出 LLM-SrcLog 框架，通过静态代码分析提取日志模板，结合白盒（基于LLM）与黑盒（聚类）方法处理无源码日志。

Result: 在 Hadoop、Zookeeper 和 Sunfire-Compute 数据集上，F1-score 提升 2-35%，解析速度比逐条LLM快约1000倍。

Conclusion: LLM-SrcLog 在精度与效率间取得近乎理想的平衡，并通过工业案例验证其有效性。

Abstract: Log parsing transforms raw logs into structured templates containing constants and variables. It underpins anomaly detection, failure diagnosis, and other AIOps tasks. Current parsers are mostly reactive and log-centric. They only infer templates from logs, mostly overlooking the source code. This restricts their capacity to grasp dynamic log structures or adjust to evolving systems. Moreover, per-log LLM inference is too costly for practical deployment. In this paper, we propose LLM-SrcLog, a proactive and unified framework for log template parsing. It extracts templates directly from source code prior to deployment and supplements them with data-driven parsing for logs without available code. LLM-SrcLog integrates a cross-function static code analyzer to reconstruct meaningful logging contexts, an LLM-based white-box template extractor with post-processing to distinguish constants from variables, and a black-box template extractor that incorporates data-driven clustering for remaining unmatched logs. Experiments on two public benchmarks (Hadoop and Zookeeper) and a large-scale industrial system (Sunfire-Compute) show that, compared to two LLM-based baselines, LLM-SrcLog improves average F1-score by 2-17% and 8-35%. Meanwhile, its online parsing latency is comparable to data-driven methods and about 1,000 times faster than per-log LLM parsing. LLM-SrcLog achieves a near-ideal balance between speed and accuracy. Finally, we further validate the effectiveness of LLM-SrcLog through practical case studies in a real-world production environment.

</details>


### [27] [Completion by Comprehension: Guiding Code Generation with Multi-Granularity Understanding](https://arxiv.org/abs/2512.04538)
*Xinkui Zhao,Rongkai Liu,Yifan Zhang,Chen Zhi,Lufei Zhang,Guanjie Cheng,Yueshen Xu,Shuiguang Deng,Jianwei Yin*

Main category: cs.SE

TL;DR: CoCo 是一种通过多粒度上下文理解实现代码补全的新框架，显著提升代码生成质量。


<details>
  <summary>Details</summary>
Motivation: 现有 RAG 方法忽视代码结构语义与依赖，难以捕捉控制流与意图，限制代码补全效果。

Method: CoCo 通过静态分析提取函数、文件、项目级结构化上下文，结合图基多粒度选择机制去噪，并转换为自然语言提示，辅以结构感知重排序机制。

Result: 在 CrossCodeEval 和 RepoEval 基准上，CoCo 超越 SOTA 方法，EM 最高提升 20.2%，且模型无关、易于集成。

Conclusion: CoCo 有效融合结构语义与上下文信息，显著提升大规模代码库下的代码补全性能。

Abstract: As code completion task from function-level to repository-level, leveraging contextual information from large-scale codebases becomes a core challenge. However, existing retrieval-augmented generation (RAG) methods typically treat code as plain natural language, relying primarily on shallow semantic matching while overlooking structural semantics and code-specific dependencies. This limits their ability to capture control flow and underlying intent, ultimately constraining the quality of generated code. Therefore, we propose CoCo, a novel framework that enables code Completion by Comprehension of multi-granularity context from large-scale code repositories. CoCo employs static code analysis to extract structured context at the function, file, and project levels, capturing execution logic and semantic dependencies. It then adopts an graph-based multi-granularity context selection mechanism to filter out redundant information and remove noise. Consequently, the information is converted into natural language in a consistent manner, thereby functioning as explicit contextual prompts to guide subsequent code completion. Additionally, a structure-aware code re-ranker mechanism ensures alignment at both semantic and structural levels. Extensive experiments on CrossCodeEval and RepoEval benchmarks demonstrate that CoCo consistently surpasses state-of-the-art baselines, achieving up to 20.2% gains in EM. Moreover, the framework is model-agnostic and can be seamlessly integrated into existing methods, leading to significant performance.

</details>


### [28] [Cross-Task Benchmarking and Evaluation of General-Purpose and Code-Specific Large Language Models](https://arxiv.org/abs/2512.04673)
*Gunjan Das,Paheli Bhattacharya,Rishabh Gupta*

Main category: cs.SE

TL;DR: 本文系统评估了通用和代码专用大语言模型在语言、推理和代码理解等多领域任务中的表现，发现代码优化模型在非编码任务中亦具优势。


<details>
  <summary>Details</summary>
Motivation: 现有研究多聚焦单一模型能力，缺乏跨领域的统一系统性比较。

Method: 在六个多样化基准上评估五种通用与三种代码专用LLM，并分析CoNaLa数据集上的代码解释行为。

Result: 代码专用模型（如CodeLLaMA）展现出更强的推理与语法精度，在非编码任务中性能亦优于通用模型（如Mistral-7B和Llama-3-8B）。

Conclusion: 针对特定领域优化的LLM即使在跨领域任务中仍可能具备显著性能优势，值得进一步探索其泛化能力。

Abstract: Large Language Models (LLMs) have revolutionized both general natural language processing and domain-specific applications such as code synthesis, legal reasoning, and finance. However, while prior studies have explored individual model capabilities, a systematic cross-domain comparison that unifies linguistic, reasoning, and code understanding abilities remains underexplored. In this work, we present a comprehensive evaluation of five general-purpose and three code-specific state-of-the-art LLMs across six diverse benchmarks encompassing linguistic competence, mathematical reasoning, and trustworthiness. Additionally, we analyze model behavior on the CoNaLa dataset for code explanation, comparing natural language and code-specialized LLMs. Our findings reveal that models optimized for code (e.g., CodeLLaMA variants) exhibit strong reasoning and syntactic precision, that even for non-coding tasks can show measurable performance gains, in contrast to general-purpose models like Mistral-7B and Llama-3-8B.

</details>


### [29] [Generative AI for Self-Adaptive Systems: State of the Art and Research Roadmap](https://arxiv.org/abs/2512.04680)
*Jialong Li,Mingyue Zhang,Nianyu Li,Danny Weyns,Zhi Jin,Kenji Tei*

Main category: cs.SE

TL;DR: 本文探讨了生成式人工智能（GenAI）在自适应系统（SASs）中的潜在优势与挑战，提出增强MAPE-K循环自主性及人机交互的两大方向，并制定研究路线图。


<details>
  <summary>Details</summary>
Motivation: GenAI能力契合SASs需求，但其具体效益与挑战尚不明确，需系统梳理以指导研究与实践。

Method: 跨四个研究领域收集、筛选并分析文献，归纳为两大类潜在优势，并据此构建研究路线图。

Result: 明确了GenAI在提升SASs自主性和人机协作方面的潜力，识别关键研究挑战并提出缓解当前GenAI局限性的策略。

Conclusion: GenAI有望显著增强SASs能力，但需克服技术整合与应用落地中的多项挑战，未来研究应聚焦路线图所列重点问题。

Abstract: Self-adaptive systems (SASs) are designed to handle changes and uncertainties through a feedback loop with four core functionalities: monitoring, analyzing, planning, and execution. Recently, generative artificial intelligence (GenAI), especially the area of large language models, has shown impressive performance in data comprehension and logical reasoning. These capabilities are highly aligned with the functionalities required in SASs, suggesting a strong potential to employ GenAI to enhance SASs. However, the specific benefits and challenges of employing GenAI in SASs remain unclear. Yet, providing a comprehensive understanding of these benefits and challenges is complex due to several reasons: limited publications in the SAS field, the technological and application diversity within SASs, and the rapid evolution of GenAI technologies. To that end, this paper aims to provide researchers and practitioners a comprehensive snapshot that outlines the potential benefits and challenges of employing GenAI's within SAS. Specifically, we gather, filter, and analyze literature from four distinct research fields and organize them into two main categories to potential benefits: (i) enhancements to the autonomy of SASs centered around the specific functions of the MAPE-K feedback loop, and (ii) improvements in the interaction between humans and SASs within human-on-the-loop settings. From our study, we outline a research roadmap that highlights the challenges of integrating GenAI into SASs. The roadmap starts with outlining key research challenges that need to be tackled to exploit the potential for applying GenAI in the field of SAS. The roadmap concludes with a practical reflection, elaborating on current shortcomings of GenAI and proposing possible mitigation strategies.

</details>


### [30] [POLARIS: Is Multi-Agentic Reasoning the Next Wave in Engineering Self-Adaptive Systems?](https://arxiv.org/abs/2512.04702)
*Divyansh Pandey,Vyakhya Gupta,Prakhar Singhal,Karthik Vaidhyanathan*

Main category: cs.SE

TL;DR: POLARIS是一个三层多智能体自适应框架，通过整合适配、推理与元学习层，实现系统对未知挑战的预测性、可解释性和持续进化能力。


<details>
  <summary>Details</summary>
Motivation: 现代软件生态系统的复杂性与不确定性使传统自适应方法难以应对新兴未知问题，亟需融合AI与自适应控制的新范式。

Method: 提出POLARIS框架，包含低延迟适配层、透明推理层和元学习层，支持协同规划、经验积累与策略演化。

Result: 在SWIM与SWITCH两个基准测试中，POLARIS性能优于现有最先进方法，验证其有效性。

Conclusion: POLARIS代表迈向Self-Adaptation 3.0的重要一步，使系统能自主推理并持续优化其适应机制，应对未来未知挑战。

Abstract: The growing scale, complexity, interconnectivity, and autonomy of modern software ecosystems introduce unprecedented uncertainty, challenging the foundations of traditional self-adaptation. Existing approaches, typically rule-driven controllers or isolated learning components, struggle to generalize to novel contexts or coordinate responses across distributed subsystems, leaving them ill-equipped for emergent unknown unknowns. Recent discussions on Self-Adaptation 2.0 emphasize an equal partnership between AI and adaptive systems, merging learning-driven intelligence with adaptive control for predictive and proactive behavior. Building on this foundation, we introduce POLARIS, a three-layer multi-agentic self-adaptation framework that advances beyond reactive adaptation. POLARIS integrates: (1) a low-latency Adapter layer for monitoring and safe execution, (2) a transparent Reasoning layer that generates and verifies plans using tool-aware, explainable agents, and (3) a Meta layer that records experiences and meta-learns improved adaptation policies over time. Through shared knowledge and predictive models, POLARIS handles uncertainty, learns from past actions, and evolves its strategies, enabling systems that anticipate change and maintain resilient, goal-directed behavior. Preliminary evaluation on two self-adaptive exemplars, SWIM and SWITCH, shows that POLARIS consistently outperforms state-of-the-art baselines. We argue this marks a shift toward Self-Adaptation 3.0, akin to Software 3.0: a paradigm where systems not only learn from their environment but also reason about and evolve their own adaptation processes, continuously improving to meet novel challenges.

</details>


### [31] [Configuration Defects in Kubernetes](https://arxiv.org/abs/2512.05062)
*Yue Zhang,Uchswas Paul,Marcelo d'Amorim,Akond Rahman*

Main category: cs.SE

TL;DR: 本文通过实证研究分析了Kubernetes配置缺陷，识别出15类缺陷，并开发了一个检测工具，成功发现26个未知缺陷。


<details>
  <summary>Details</summary>
Motivation: Kubernetes配置易出错且后果严重，需帮助开发者检测和预防此类缺陷。

Method: 从开源仓库提取719个缺陷，进行定性分析并评估8个静态分析工具，再开发专用linter检测未覆盖的严重缺陷类别。

Result: 现有工具仅能检测15类中的8类，新linter发现26个确认缺陷，其中19个已修复。

Conclusion: 论文提供了针对Kubernetes配置脚本的缺陷检测与修复建议，相关数据与代码已公开。

Abstract: Kubernetes is a tool that facilitates rapid deployment of software. Unfortunately, configuring Kubernetes is prone to errors. Configuration defects are not uncommon and can result in serious consequences. This paper reports an empirical study about configuration defects in Kubernetes with the goal of helping practitioners detect and prevent these defects. We study 719 defects that we extract from 2,260 Kubernetes configuration scripts using open source repositories. Using qualitative analysis, we identify 15 categories of defects. We find 8 publicly available static analysis tools to be capable of detecting 8 of the 15 defect categories. We find that the highest precision and recall of those tools are for defects related to data fields. We develop a linter to detect two categories of defects that cause serious consequences, which none of the studied tools are able to detect. Our linter revealed 26 previously-unknown defects that have been confirmed by practitioners, 19 of which have already been fixed. We conclude our paper by providing recommendations on how defect detection and repair techniques can be used for Kubernetes configuration scripts. The datasets and source code used for the paper are publicly available online.

</details>
