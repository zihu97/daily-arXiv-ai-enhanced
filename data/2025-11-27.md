<div id=toc></div>

# Table of Contents

- [cs.SE](#cs.SE) [Total: 7]
- [cs.MA](#cs.MA) [Total: 4]
- [cs.OS](#cs.OS) [Total: 1]
- [cs.AR](#cs.AR) [Total: 4]
- [cs.DC](#cs.DC) [Total: 8]


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [1] [Train While You Fight -- Technical Requirements for Advanced Distributed Learning Platforms](https://arxiv.org/abs/2511.20813)
*Simon Hacks*

Main category: cs.SE

TL;DR: 本文探讨了支持“边战边训”（TWYF）理念的先进分布式学习（ADL）平台所需满足的技术要求，并通过设计科学研究方法，将来自北约文档和实践中的挑战映射到已有的软件工程模式，提出了七项关键技术挑战及其解决方案，并以德国武装部队的国家用例加以说明。


<details>
  <summary>Details</summary>
Motivation: 为实现“边战边训”（TWYF）这一在作战过程中持续学习的理念，需要ADL平台具备特定技术能力；然而当前平台尚缺乏对这些需求的系统性分析与应对。

Method: 采用设计科学研究方法，从北约及实践文献中提炼技术挑战，明确解决目标，并系统地将挑战映射到成熟的软件工程模式。

Result: 识别出七大技术挑战：互操作性、韧性、多语言支持、数据安全与隐私、可扩展性、平台无关性和模块化，并通过德国军队的实际用例展示了相关软件模式的应用。

Conclusion: 现有软件工程模式能够有效应对TWYF对ADL平台提出的技术挑战，为构建支持实战中持续学习的系统提供了可行路径。

Abstract: "Train While You Fight" (TWYF) advocates for continuous learning that occurs during operations, not just before or after. This paper examines the technical requirements that advanced distributed learning (ADL) platforms must meet to support TWYF, and how existing software engineering patterns can fulfill these requirements. Using a Design Science Research approach, we (i) derive challenges from PfPC/NATO documentation and recent practice, (ii) define solution objectives, and (iii) conduct a systematic mapping from challenges to proven patterns. We identify seven technical challenges: interoperability, resilience, multilingual support, data security and privacy, scalability, platform independence, and modularity. We illustrate the patterns with a national use case from the German armed forces.

</details>


### [2] [Hierarchical Evaluation of Software Design Capabilities of Large Language Models of Code](https://arxiv.org/abs/2511.20933)
*Mootez Saad,Boqi Chen,José Antonio Hernández López,Dániel Varró,Tushar Sharma*

Main category: cs.SE

TL;DR: 本文评估了DeepSeek-R1系列大语言模型对软件工程中内聚性和耦合性概念的理解能力，发现在理想条件下表现良好，但在噪声干扰或缺乏引导的开放场景下推理能力显著下降，尤其在耦合性分析上更为脆弱。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型在软件工程中被广泛使用，但其对核心软件设计概念（如内聚与耦合）的理解是否稳健尚不明确，因此有必要系统评估其在不同条件下的表现。

Method: 通过程序化生成设计不良的代码片段，在不同引导程度（验证、引导、开放式生成）和不同噪声水平（注入干扰元素）下，测试DeepSeek-R1系列模型（14B、32B、70B）对内聚性和耦合性的理解能力，并分析其推理轨迹。

Result: 模型在理想条件下对两个概念有较好理解，但在噪声和开放场景中表现脆弱：耦合性分析F1分数下降超50%，而内聚性在引导任务中较稳健，但在无引导时同样失效；推理轨迹显示模型对耦合采取认知捷径，对内聚则进行更详尽但仍失败的分析。

Conclusion: 尽管大语言模型在识别设计缺陷方面可提供可靠辅助，但在噪声干扰和缺乏引导的真实场景中自主推理能力有限，亟需提升其程序理解的可扩展性与鲁棒性。

Abstract: Large language models (LLMs) are being increasingly adopted in the software engineering domain, yet the robustness of their grasp on core software design concepts remains unclear. We conduct an empirical study to systematically evaluate their understanding of cohesion (intra-module) and coupling (inter-module). We programmatically generate poorly designed code fragments and test the DeepSeek-R1 model family ($14$B, $32$B, $70$B) under varying levels of guidance, from simple \textit{Verification} to \textit{Guided} and \textit{Open-ended Generation}, while varying contextual noise by injecting distractor elements. While models exhibit a solid baseline understanding of both concepts in ideal conditions, their practical knowledge is fragile and highly asymmetrical. Reasoning about coupling proves brittle; performance collapses in noisy, open-ended scenarios, with F1 scores dropping by over $50\%$. In contrast, the models' analysis of cohesion is remarkably robust to internal noise in guided tasks, showing little performance degradation. However, this resilience also fails when all guidance is removed. Reasoning-trace analysis confirms these failure modes, revealing \textit{cognitive shortcutting} for coupling versus a more exhaustive (yet still failing) analysis for cohesion. To summarize, while LLMs can provide reliable assistance for recognizing design flaws, their ability to reason autonomously in noisy, realistic contexts is limited, highlighting the critical need for more scalable and robust program understanding capabilities.

</details>


### [3] [Lightweight Model Editing for LLMs to Correct Deprecated API Recommendations](https://arxiv.org/abs/2511.21022)
*Guancheng Lin,Xiao Yu,Jacky Keung,Xing Hu,Xin Xia,Alex X. Liu*

Main category: cs.SE

TL;DR: 本文首次系统评估了10种前沿模型编辑方法在更新大语言模型（LLMs）中已弃用API知识方面的效果，并提出了改进方法AdaLoRA-L，通过区分“通用API层”与“特定API层”显著提升了编辑的特异性，同时保持其他指标性能。


<details>
  <summary>Details</summary>
Motivation: 大语言模型因训练数据时效性限制，常生成已弃用的API；重新训练成本高，而现有轻量级模型编辑方法是否能有效更新此类知识尚不明确。

Method: 构建包含70多个已弃用API和3000多个编辑实例的基准EDA PI Bench，在Qwen2.5-Coder、StarCoder2和DeepSeek-Coder三种模型上评估10种模型编辑技术；提出AdaLoRA-L方法，仅在特定API层进行编辑以提升特异性。

Result: AdaLoRA在生成正确新API方面表现最佳但特异性不足；AdaLoRA-L显著提升特异性，同时保持其他指标相当。

Conclusion: 通过区分模型中通用与特定知识层，AdaLoRA-L有效解决了模型编辑中的特异性问题，为高效更新LLM中的过时API知识提供了可行方案。

Abstract: Pre-trained or fine-tuned on large code corpora, Large Language Models (LLMs) have demonstrated strong performance in code completion tasks. However, their embedded knowledge is constrained by the timeliness of training data, which often includes code using deprecated APIs. Consequently, LLMs frequently generate deprecated APIs that will no longer be supported in future versions of third-party libraries. While retraining LLMs on updated codebases could refresh their API knowledge, this approach is computationally expensive. Recently, lightweight model editing methods have emerged to efficiently correct specific knowledge in LLMs. However, it remains unclear whether these methods can effectively update deprecated API knowledge and enable edited models to generate up-to-date APIs. To address this gap, we conduct the first systematic study applying 10 state-of-the-art model editing techniques to update deprecated API knowledge in three LLMs: Qwen2.5-Coder, StarCoder2, and DeepSeek-Coder. We introduce EDAPIBench, a dedicated benchmark featuring over 70 deprecated APIs from 8 popular Python libraries, with more than 3,000 editing instances. Our results show that the parameter-efficient fine-tuning method AdaLoRA achieves the best performance in enabling edited models to generate correct, up-to-date APIs, but falls short in Specificity (i.e., the editing influences untargeted knowledge). To resolve this, we propose AdaLoRA-L, which defines "Common API Layers" (layers within the LLMs with high importance across all APIs, storing general knowledge and excluded from editing) and restricts edits exclusively to "Specific API Layers" (layers with high importance only for the target API, storing the API-specific knowledge). Experimental results demonstrate that AdaLoRA-L significantly improves Specificity while maintaining comparable performance across other evaluation metrics.

</details>


### [4] [Exploring Hidden Geographic Disparities in Android Apps](https://arxiv.org/abs/2511.21151)
*M. Alecci,P. Jiménez,J. Samhi,T. Bissyandé,J. Klein*

Main category: cs.SE

TL;DR: 本文揭示了Android应用在不同地理区域存在显著差异，提出了“GeoTwins”现象和App Bundle中base.apk的区域定制问题，指出这些隐藏差异对安全、隐私、公平性和研究可复现性构成挑战，并发布了包含81,963个GeoTwins的数据集。


<details>
  <summary>Details</summary>
Motivation: 尽管移动应用演化已被广泛研究，但其地理差异尚未得到充分关注。作者旨在揭示地理位置如何导致同一应用在权限、第三方库和隐私政策等方面产生不一致，进而影响安全评估、研究可复现性及用户知情权。

Method: 构建了一个跨区域的分布式应用采集系统，收集并分析数千款Android应用；识别出功能相似但包名不同的“GeoTwins”；对比不同地区下载的base.apk文件以检测隐藏的区域定制行为。

Result: 发现了两类重要现象：（1）GeoTwins在权限、库和隐私披露方面存在显著差异；（2）即使基础APK（base.apk）也因地区而异。这些差异可能导致同一应用在不同地区的安全评估结果不一致，并造成地理偏见。

Conclusion: 移动应用存在系统性的区域差异，这对安全研究、平台设计、开发者实践和政策制定均有深远影响。研究呼吁提升透明度，并强调需将地理因素纳入移动软件分析框架。

Abstract: While mobile app evolution has been widely studied, geographical variation in app behavior remains largely unexplored. This paper presents a large-scale study of location-based Android app differentiation, uncovering two important and underexamined phenomena with security and fairness implications. First, we introduce GeoTwins: apps that are functionally similar and share branding but are released under different package names across countries. Despite their similarity, GeoTwins often diverge in requested permissions, third-party libraries, and privacy disclosures. Second, we examine the Android App Bundle ecosystem and reveal unexpected regional differences in supposedly consistent base.apk files. Contrary to common assumptions, even base.apk files vary by region, exposing hidden customizations that may affect app behavior or security.
  These discrepancies have concrete consequences. Geographically distinct variants can lead the same app to be labeled benign in one malware study but suspicious in another, depending on the region of download. Such hidden variation undermines reproducibility and introduces geographic bias into assessments of security, privacy, and functionality. It also raises ethical concerns about transparency and consent: visually identical Google Play listings may mask subtle but important differences.
  To study these issues, we built a distributed app collection pipeline spanning multiple regions and analyzed thousands of apps. We also release a dataset of 81,963 GeoTwins to support future work. Our findings reveal systemic regional disparities in mobile software, with implications for researchers, developers, platform architects, and policymakers.

</details>


### [5] [Bug Detective and Quality Coach: Developers' Mental Models of AI-Assisted IDE Tools](https://arxiv.org/abs/2511.21197)
*Paolo Buono,Mary Cerullo,Stefano Cirillo,Giuseppe Desolda,Francesco Greco,Emanuela Guglielmi,Grazia Margarella,Giuseppe Polese,Simone Scalabrino,Cesare Tucci*

Main category: cs.SE

TL;DR: 该研究通过六场联合设计工作坊，探索了58名开发者对AI辅助缺陷检测与代码可读性评估工具的心理模型，并提炼出以人为核心的IDE中AI工具设计原则。


<details>
  <summary>Details</summary>
Motivation: 尽管AI辅助开发工具在技术上不断进步，但开发者如何理解这些工具（即其心理模型）以及模型不匹配如何影响信任、控制和采纳仍不清楚。

Method: 开展六场联合设计工作坊，共邀请58名开发者参与，通过讨论引出他们对AI辅助缺陷检测和可读性评估功能的心理模型。

Result: 开发者将缺陷检测工具视为“缺陷侦探”，强调关键问题预警、透明性、可操作反馈和信心提示；将可读性评估工具视为“质量教练”，期望获得情境化、个性化和渐进式指导。两类工具的信任均依赖于解释清晰度、时机和用户控制权。

Conclusion: 研究提炼出一套面向IDE中人类中心AI的设计原则，旨在平衡干扰与支持、简洁与深度、自动化与人类能动性。

Abstract: AI-assisted tools support developers in performing cognitively demanding tasks such as bug detection and code readability assessment. Despite the advancements in the technical characteristics of these tools, little is known about how developers mentally model them and how mismatches affect trust, control, and adoption. We conducted six co-design workshops with 58 developers to elicit their mental models about AI-assisted bug detection and readability features. It emerged that developers conceive bug detection tools as \textit{bug detectives}, which warn users only in case of critical issues, guaranteeing transparency, actionable feedback, and confidence cues. Readability assessment tools, on the other hand, are envisioned as \textit{quality coaches}, which provide contextual, personalized, and progressive guidance. Trust, in both tasks, depends on the clarity of explanations, timing, and user control. A set of design principles for Human-Centered AI in IDEs has been distilled, aiming to balance disruption with support, conciseness with depth, and automation with human agency.

</details>


### [6] [Multi-Agent Systems for Dataset Adaptation in Software Engineering: Capabilities, Limitations, and Future Directions](https://arxiv.org/abs/2511.21380)
*Jingyi Chen,Xiaoyan Guo,Songqiang Chen,Shing-Chi Cheung,Jiasi Shen*

Main category: cs.SE

TL;DR: 本文首次实证研究了基于大语言模型的多智能体系统（如GitHub Copilot）在软件工程研究中跨数据集适配任务中的表现，发现其虽能识别关键文件并生成部分适配代码，但很少产出功能正确的实现；通过提供执行错误信息和参考代码等提示干预，可显著提升输出与真实结果的结构相似度。


<details>
  <summary>Details</summary>
Motivation: 自动化适配软件工程研究制品以适应不同数据集对可扩展性和可复现性至关重要，但目前相关研究仍十分缺乏。随着基于大语言模型的多智能体系统的发展，探索其在此类任务中的能力具有重要意义。

Method: 作者构建了一个五阶段评估流程（文件理解、代码编辑、命令生成、验证和最终执行），在ROCODE和LogHub2.0等基准仓库上评估Copilot（基于GPT-4.1和Claude Sonnet 4）的表现，并测试了多种提示干预策略的效果。

Result: 当前多智能体系统能识别关键文件并生成部分适配，但极少产出功能正确的实现；引入执行错误信息和参考代码等提示干预后，输出与真实结果的结构相似度从7.25%显著提升至67.14%。

Conclusion: 当前多智能体LLM系统在数据集适配任务中展现出潜力但也存在明显局限，未来需设计更具上下文感知能力和反馈驱动机制的自修正智能体以提升可靠性。

Abstract: Automating the adaptation of software engineering (SE) research artifacts across datasets is essential for scalability and reproducibility, yet it remains largely unstudied. Recent advances in large language model (LLM)-based multi-agent systems, such as GitHub Copilot's agent mode, promise to automate complex development workflows through coordinated reasoning, code generation, and tool interaction. This paper presents the first empirical study on how state-of-the-art multi-agent systems perform in dataset adaptation tasks. We evaluate Copilot, backed by GPT-4.1 and Claude Sonnet 4, on adapting SE research artifacts from benchmark repositories including ROCODE and LogHub2.0. Through a five-stage evaluation pipeline (file comprehension, code editing, command generation, validation, and final execution), we measure success rates, analyze failure patterns, and assess prompt-based interventions designed to enhance agent performance. Results show that current systems can identify key files and generate partial adaptations but rarely produce functionally correct implementations. Prompt-level interventions, especially providing execution error messages and reference code, substantially improve structural similarity to ground truth (from 7.25% to 67.14%), highlighting the importance of contextual and feedback-driven guidance. Our findings reveal both the promise and limitations of today's multi-agent LLM systems for dataset adaptation, and suggest concrete directions for building more reliable, self-correcting agents in future SE research.

</details>


### [7] [Large Language Models for Unit Test Generation: Achievements, Challenges, and the Road Ahead](https://arxiv.org/abs/2511.21382)
*Bei Chu,Yang Feng,Kui Liu,Zifan Nan,Zhaoqiang Guo,Baowen Xu*

Main category: cs.SE

TL;DR: 本文对2021年5月至2025年8月间发表的115篇关于大语言模型（LLMs）在单元测试生成中应用的文献进行了系统综述，提出基于单元测试生成生命周期的统一分类法，指出提示工程是主流策略（占89%），迭代验证与修复显著提升测试通过率，但仍面临故障检测能力弱和缺乏标准评估基准等挑战，并展望了自主测试智能体与混合系统的未来方向。


<details>
  <summary>Details</summary>
Motivation: 传统自动化单元测试方法虽能有效探索程序结构，但缺乏语义信息，难以生成真实输入和断言；而大语言模型（LLMs）凭借其对代码语义和编程模式的数据驱动理解，有望弥补这一不足，因此有必要系统梳理该领域的研究现状与发展路径。

Method: 作者对2021年5月至2025年8月间发表的115篇相关文献进行了系统性综述，提出一个基于单元测试生成生命周期的统一分类框架，将LLMs视为需系统工程约束的随机生成器，并从生成策略、上下文增强到后处理质量保障等方面进行分析。

Result: 研究发现提示工程是主导性使用策略（占比89%），迭代验证与修复机制已成为提升编译和执行通过率的标准做法；但生成测试的缺陷检测能力仍较弱，且缺乏统一的评估基准。

Conclusion: 未来研究应聚焦于发展自主测试智能体及融合LLMs与传统软件工程工具的混合系统，以推动LLMs在工业级测试中的实际应用。

Abstract: Unit testing is an essential yet laborious technique for verifying software and mitigating regression risks. Although classic automated methods effectively explore program structures, they often lack the semantic information required to produce realistic inputs and assertions. Large Language Models (LLMs) address this limitation by utilizing by leveraging their data-driven knowledge of code semantics and programming patterns. To analyze the state of the art in this domain, we conducted a systematic literature review of 115 publications published between May 2021 and August 2025. We propose a unified taxonomy based on the unit test generation lifecycle that treats LLMs as stochastic generators requiring systematic engineering constraints. This framework analyzes the literature regarding core generative strategies and a set of enhancement techniques ranging from pre-generation context enrichment to post-generation quality assurance. Our analysis reveals that prompt engineering has emerged as the dominant utilization strategy and accounts for 89% of the studies due to its flexibility. We find that iterative validation and repair loops have become the standard mechanism to ensure robust usability and lead to significant improvements in compilation and execution pass rates. However, critical challenges remain regarding the weak fault detection capabilities of generated tests and the lack of standardized evaluation benchmarks. We conclude with a roadmap for future research that emphasizes the progression towards autonomous testing agents and hybrid systems combining LLMs with traditional software engineering tools. This survey provides researchers and practitioners with a comprehensive perspective on converting the potential of LLMs into industrial-grade testing solutions.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [8] [MTTR-A: Measuring Cognitive Recovery Latency in Multi-Agent Systems](https://arxiv.org/abs/2511.20663)
*Barak Or*

Main category: cs.MA

TL;DR: 本文将传统可靠性指标（如MTTR、MTBF）引入多智能体系统的认知领域，提出MTTR-A作为衡量认知恢复延迟的运行时指标，并通过仿真实验验证了不同反射模式下的恢复性能，为智能体认知的运行时可靠性提供了量化基础。


<details>
  <summary>Details</summary>
Motivation: 现有可观测性工具无法量化多智能体系统在推理一致性丧失后的恢复速度，缺乏对认知稳定性恢复过程的度量手段。

Method: 将经典可靠性指标（MTTR、MTBF等）适配到认知域，定义MTTR-A作为认知恢复延迟的运行时度量；利用AG~News语料库和LangGraph框架构建仿真环境，模拟多种反射模式下的恢复延迟。

Result: 实验结果显示：自动反射平均约6秒恢复稳定，人工干预约12秒；200次运行中，中位MTTR-A为6.21±2.14秒，MTBF为6.7±2.14秒，NRR为0.08，表明不同反射策略具有可测量的运行时韧性。

Conclusion: 通过将恢复延迟形式化为分布式推理的可量化属性，并建立恢复时间与认知可用性的可靠性边界，本研究为智能体认知的运行时可靠性奠定了基础，使认知恢复从临时性过程转变为标准化、可解释的性能指标。

Abstract: Ensuring cognitive stability in autonomous multi-agent systems (MAS) is a central challenge for large-scale, distributed AI. While existing observability tools monitor system outputs, they cannot quantify how rapidly agentic workflows recover once reasoning coherence has been lost. We adapt classical reliability metrics-Mean Time-to-Recovery (MTTR), Mean Time Between Failures (MTBF), and related ratios-into the cognitive domain, defining MTTR-A (Mean Time-to-Recovery for Agentic Systems) as a runtime measure of cognitive recovery latency. MTTR-A quantifies the time required for a MAS to detect reasoning drift and restore consistent operation, capturing the recovery of reasoning coherence rather than infrastructural repair.
  A benchmark simulation using the AG~News corpus and the LangGraph orchestration framework was conducted, modeling recovery latencies across multiple reflex modes. Automated reflexes restored stability within approximately 6s on average, while human-approval interventions required about 12s. Across 200 runs, the median simulated MTTR-A was 6.21+-2.14s, MTBF=6.7+-2.14s, and NRR=0.08, demonstrating measurable runtime resilience across reflex strategies.
  By formalizing recovery latency as a quantifiable property of distributed reasoning-and deriving reliability bounds linking recovery time and cognitive uptime-this work establishes a foundation for runtime dependability in agentic cognition, transforming cognitive recovery from an ad-hoc process into a standardized, interpretable performance

</details>


### [9] [Resilient Charging Infrastructure via Decentralized Coordination of Electric Vehicles at Scale](https://arxiv.org/abs/2511.20943)
*Chuhao Qin,Alexandru Sorici,Andrei Olaru,Evangelos Pournaras,Adina Magda Florea*

Main category: cs.MA

TL;DR: 本文提出了一种基于集体学习的去中心化电动汽车充电协调框架，在保障系统效率的同时提升个体驾驶舒适度，并在站桩故障或需求激增等极端情况下展现出更强的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有去中心化充电控制方法在面对充电桩故障或突发高需求等严重异常情况时表现不佳，易导致排队过长和用户体验下降。

Method: 提出一种集体学习驱动的协调框架，使电动汽车在个体舒适度与系统整体效率（如各站点排队情况）之间动态权衡，实现帕累托最优；通过真实数据实验验证其有效性。

Result: 所提方法显著减少了行驶与排队等待时间；研究发现，在不确定条件下，适时采取自私或利他行为的驾驶员比始终采取中庸策略者等待时间更短；在高比例桩损和对抗性用户场景下系统仍具良好韧性。

Conclusion: 该框架有效提升了去中心化电动汽车充电系统的适应性、效率与可靠性，为应对现实复杂场景提供了新思路。

Abstract: The rapid adoption of electric vehicles (EVs) introduces major challenges for decentralized charging control. Existing decentralized approaches efficiently coordinate a large number of EVs to select charging stations while reducing energy costs, preventing power peak and preserving driver privacy. However, they often struggle under severe contingencies, such as station outages or unexpected surges in charging requests. These situations create competition for limited charging slots, resulting in long queues and reduced driver comfort. To address these limitations, we propose a novel collective learning-based coordination framework that allows EVs to balance individual comfort on their selections against system-wide efficiency, i.e., the overall queues across all stations. In the framework, EVs are recommended for adaptive charging behaviors that shift priority between comfort and efficiency, achieving Pareto-optimal trade-offs under varying station capacities and dynamic spatio-temporal EV distribution. Experiments using real-world data from EVs and charging stations show that the proposed approach outperforms baseline methods, significantly reducing travel and queuing time. The results reveal that, under uncertain charging conditions, EV drivers that behave selfishly or altruistically at the right moments achieve shorter waiting time than those maintaining moderate behavior throughout. Our findings under high fractions of station outages and adversarial EVs further demonstrate improved resilience and trustworthiness of decentralized EV charging infrastructure.

</details>


### [10] [Tool-RoCo: An Agent-as-Tool Self-organization Large Language Model Benchmark in Multi-robot Cooperation](https://arxiv.org/abs/2511.21510)
*Ke Zhang,Xiaoning Zhao,Ce Zheng,Jiahong Ning,Dandan Zhu,Wenqi Zhang,Chen Sun,Toshiharu Sugawara*

Main category: cs.MA

TL;DR: 本文提出了Tool-RoCo，一个基于多机器人协作基准RoCo的新评测框架，用于评估大语言模型（LLMs）在长期多智能体合作中的自主性与协作能力。该框架将其他智能体视为工具，通过工具调用机制衡量不同合作范式下的协调表现，并在三个任务上进行了实验，揭示当前LLM在主动协作和动态激活/停用智能体方面仍存在明显不足。


<details>
  <summary>Details</summary>
Motivation: 现有基于LLM的多智能体系统研究多依赖预设编排，忽视了智能体的自主性。为系统评估LLM在多智能体环境中的自组织与协作能力，作者提出Tool-RoCo基准。

Method: Tool-RoCo将其他智能体视为“协作工具”，设计了四种LLM合作范式（集中式合作、集中式自组织、去中心化合作、自组织），并在SORT、PACK和CABINET三个多机器人任务中，通过工具调用的格式、参数准确性和协调性来评估性能。

Result: 实验表明，在所有工具调用中，协作工具仅占7.09%，说明LLM很少主动请求其他智能体协助；而激活类工具占比高达96.42%，显示当前LLM倾向于保持智能体活跃，缺乏动态停用以实现自适应协调的能力。

Conclusion: Tool-RoCo为评估LLM在多智能体任务中的自主性与协作能力提供了一个系统性基准，揭示了当前模型在真正自组织协作方面的局限性。

Abstract: This study proposes Tool-RoCo, a novel benchmark for evaluating large language models (LLMs) in long-term multi-agent cooperation based on RoCo, a multi-robot cooperative benchmark. Recent research on LLM-based multi-agent systems has relied on predefined orchestration, while ignoring agent autonomy. Tool-RoCo treats other agents as tools and introduces cooperative tools, leveraging tool usage to evaluate multi-agent cooperation and self-organization. Tool usage means that each agent (LLM) selects a tool from a candidate set based on the current state, receives feedback, and adjusts its selection in subsequent rounds. To evaluate different autonomy levels, we propose four LLM paradigms: (1) centralized cooperation, where a single LLM allocates tools to all agents; (2) centralized self-organization, where a central LLM autonomously activates agents while keeping others inactive; (3) decentralized cooperation, where each agent has its own LLM and calls tools based on local information; and (4) self-organization, where a randomly chosen initial agent can request collaboration, activating additional agents via tool calls. Tool-RoCo includes three multi-robot tasks, SORT, PACK, and CABINET, to measure format and parameter accuracy and agent coordination through tool usage. The results using several LLMs showed that cooperative tools accounted for only 7.09% of all tools, indicating that LLM-based agents rarely invoked others as assistants. Moreover, activation tools accounted for 96.42%, suggesting that current LLMs tend to maintain active agents while seldom deactivating them for adaptive coordination. Tool-RoCo provides a systematic benchmark to evaluate LLM autonomy and cooperation in multi-agent tasks. Code and Demo: https://github.com/ColaZhang22/Tool-Roco

</details>


### [11] [BAMAS: Structuring Budget-Aware Multi-Agent Systems](https://arxiv.org/abs/2511.21572)
*Liming Yang,Junyu Luo,Xuanzhe Liu,Yiling Lou,Zhenpeng Chen*

Main category: cs.MA

TL;DR: BAMAS is a budget-aware method for constructing multi-agent LLM systems that jointly optimizes model selection and collaboration topology, achieving up to 86% cost reduction with comparable performance.


<details>
  <summary>Details</summary>
Motivation: Existing multi-agent LLM systems rarely consider explicit budget constraints despite rising deployment costs as system complexity increases.

Method: BAMAS formulates an Integer Linear Programming problem to select a cost-effective set of LLMs and uses reinforcement learning to determine their optimal interaction topology.

Result: Evaluated on three tasks, BAMAS matches the performance of state-of-the-art methods while cutting costs by up to 86%.

Conclusion: BAMAS effectively enables the construction of cost-efficient multi-agent LLM systems without sacrificing performance, addressing a critical gap in practical deployment under budget constraints.

Abstract: Large language model (LLM)-based multi-agent systems have emerged as a powerful paradigm for enabling autonomous agents to solve complex tasks. As these systems scale in complexity, cost becomes an important consideration for practical deployment. However, existing work rarely addresses how to structure multi-agent systems under explicit budget constraints. In this paper, we propose BAMAS, a novel approach for building multi-agent systems with budget awareness. BAMAS first selects an optimal set of LLMs by formulating and solving an Integer Linear Programming problem that balances performance and cost. It then determines how these LLMs should collaborate by leveraging a reinforcement learning-based method to select the interaction topology. Finally, the system is instantiated and executed based on the selected agents and their collaboration topology. We evaluate BAMAS on three representative tasks and compare it with state-of-the-art agent construction methods. Results show that BAMAS achieves comparable performance while reducing cost by up to 86%.

</details>


<div id='cs.OS'></div>

# cs.OS [[Back]](#toc)

### [12] [DynamicAdaptiveClimb: Adaptive Cache Replacement with Dynamic Resizing](https://arxiv.org/abs/2511.21235)
*Daniel Berend,Shlomi Dolev,Sweta Kumari,Dhruv Mishra,Marina Kogan-Sadetsky,Archit Somani*

Main category: cs.OS

TL;DR: 本文提出了两种新型缓存替换策略 AdaptiveClimb 和 DynamicAdaptiveClimb，通过轻量级自适应机制动态调整缓存项的提升距离和缓存大小，在多种真实工作负载下显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 传统缓存替换策略（如 LRU 和 CLIMB）难以在动态变化的工作负载下高效适应，且往往需要复杂的统计信息或高开销。因此，亟需一种低开销、高适应性的缓存管理机制。

Method: AdaptiveClimb 根据最近的命中/未命中模式动态调整缓存对象的提升距离，仅需一个可调参数且无需逐项统计；DynamicAdaptiveClimb 在此基础上进一步根据工作负载自动调整缓存容量。

Result: 在涵盖6个数据集、1067条真实轨迹的评估中，DynamicAdaptiveClimb 相比 FIFO 基线最高提升29%命中率，并比次优方法（AdaptiveClimb 和 SIEVE）高出10%–15%的性能，尤其在工作集大小波动的场景下表现突出。

Conclusion: 所提方法在保持低开销的同时实现了高效的缓存性能，特别适用于现代动态缓存环境。

Abstract: Efficient cache management is critical for optimizing the system performance, and numerous caching mechanisms have been proposed, each exploring various insertion and eviction strategies. In this paper, we present AdaptiveClimb and its extension, DynamicAdaptiveClimb, two novel cache replacement policies that leverage lightweight, cache adaptation to outperform traditional approaches. Unlike classic Least Recently Used (LRU) and Incremental Rank Progress (CLIMB) policies, AdaptiveClimb dynamically adjusts the promotion distance (jump) of the cached objects based on recent hit and miss patterns, requiring only a single tunable parameter and no per-item statistics. This enables rapid adaptation to changing access distributions while maintaining low overhead. Building on this foundation, DynamicAdaptiveClimb further enhances adaptability by automatically tuning the cache size in response to workload demands. Our comprehensive evaluation across a diverse set of real-world traces, including 1067 traces from 6 different datasets, demonstrates that DynamicAdaptiveClimb consistently achieves substantial speedups and higher hit ratios compared to other state-of-the-art algorithms. In particular, our approach achieves up to a 29% improvement in hit ratio and a substantial reduction in miss penalties compared to the FIFO baseline. Furthermore, it outperforms the next-best contenders, AdaptiveClimb and SIEVE [43], by approximately 10% to 15%, especially in environments characterized by fluctuating working set sizes. These results highlight the effectiveness of our approach in delivering efficient performance, making it well-suited for modern, dynamic caching environments.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [13] [RISC-V Based TinyML Accelerator for Depthwise Separable Convolutions in Edge AI](https://arxiv.org/abs/2511.21232)
*Muhammed Yildirim,Ozcan Ozturk*

Main category: cs.AR

TL;DR: 本文提出一种新型硬件加速器架构，采用融合像素级数据流，在RISC-V处理器上实现零中间缓存的深度可分离卷积（DSC）计算，显著降低数据移动开销并提升能效。


<details>
  <summary>Details</summary>
Motivation: 边缘AI和TinyML应用对设备端智能的需求日益增长，但现有轻量级CNN（如MobileNetV2）在逐层执行时因频繁访问片上或片外存储而产生高能耗与延迟，形成“内存墙”瓶颈。

Method: 设计一种基于融合像素级数据流的定制功能单元（CFU），集成于RISC-V处理器，通过紧密耦合流水线一次性完成单个输出像素在DSC各阶段（扩展、深度卷积、投影）的计算，无需写入中间特征图。

Result: 在Xilinx Artix-7 FPGA上实现最高59.3倍于RISC-V软件基线的速度提升；ASIC综合显示在28 nm工艺下面积仅0.284 mm²、功耗910 mW（2 GHz），在40 nm下为1.20 mm²、233 mW（300 MHz），数据移动减少高达87%。

Conclusion: 该工作验证了在TinyML资源约束下实现零缓存数据流的可行性，为边缘AI加速器突破内存墙提供了高效新策略。

Abstract: The increasing demand for on-device intelligence in Edge AI and TinyML applications requires the efficient execution of modern Convolutional Neural Networks (CNNs). While lightweight architectures like MobileNetV2 employ Depthwise Separable Convolutions (DSC) to reduce computational complexity, their multi-stage design introduces a critical performance bottleneck inherent to layer-by-layer execution: the high energy and latency cost of transferring intermediate feature maps to either large on-chip buffers or off-chip DRAM. To address this memory wall, this paper introduces a novel hardware accelerator architecture that utilizes a fused pixel-wise dataflow. Implemented as a Custom Function Unit (CFU) for a RISC-V processor, our architecture eliminates the need for intermediate buffers entirely, reducing the data movement up to 87\% compared to conventional layer-by-layer execution. It computes a single output pixel to completion across all DSC stages-expansion, depthwise convolution, and projection-by streaming data through a tightly-coupled pipeline without writing to memory. Evaluated on a Xilinx Artix-7 FPGA, our design achieves a speedup of up to 59.3x over the baseline software execution on the RISC-V core. Furthermore, ASIC synthesis projects a compact 0.284 mm$^2$ footprint with 910 mW power at 2 GHz in 28 nm, and a 1.20 mm$^2$ footprint with 233 mW power at 300 MHz in 40 nm. This work confirms the feasibility of a zero-buffer dataflow within a TinyML resource envelope, offering a novel and effective strategy for overcoming the memory wall in edge AI accelerators.

</details>


### [14] [Bombyx: OpenCilk Compilation for FPGA Hardware Acceleration](https://arxiv.org/abs/2511.21346)
*Mohamed Shahawy,Julien de Castelnau,Paolo Ienne*

Main category: cs.AR

TL;DR: 本文提出了Bombyx编译器工具链，将OpenCilk程序转换为基于Cilk-1风格的中间表示，以更高效地在FPGA上实现任务级并行（TLP），并通过解耦访存-执行优化自动生成高性能处理单元。


<details>
  <summary>Details</summary>
Motivation: 现有OpenCilk的隐式任务模型在硬件中需要昂贵的上下文切换，难以高效映射到FPGA等空间架构；而Cilk-1的显式延续传递模型更适合FPGA的流式特性，因此需要一种新的编译方法来支持这种映射。

Method: Bombyx工具链将OpenCilk程序降级为Cilk-1风格的中间表示，并提供两个编译目标：一是兼容OpenCilk运行时的后端，二是面向Vitis HLS等高层次综合工具的可综合处理单元（PE）生成器；同时引入解耦访存-执行优化以提升内存与计算重叠。

Result: Bombyx能够自动生成高性能PE，在FPGA上有效支持CPU导向的TLP应用，提升吞吐量和内存-计算重叠效率。

Conclusion: 通过采用Cilk-1的显式任务模型和编译优化，Bombyx显著提升了TLP应用在FPGA上的映射效率和性能，为软硬件协同设计提供了新思路。

Abstract: Task-level parallelism (TLP) is a widely used approach in software where independent tasks are dynamically created and scheduled at runtime. Recent systems have explored architectural support for TLP on field-programmable gate arrays (FPGAs), often leveraging high-level synthesis (HLS) to create processing elements (PEs). In this paper, we present Bombyx, a compiler toolchain that lowers OpenCilk programs into a Cilk-1-inspired intermediate representation, enabling efficient mapping of CPU-oriented TLP applications to spatial architectures on FPGAs. Unlike OpenCilk's implicit task model, which requires costly context switching in hardware, Cilk-1 adopts explicit continuation-passing - a model that better aligns with the streaming nature of FPGAs. Bombyx supports multiple compilation targets: one is an OpenCilk-compatible runtime for executing Cilk-1-style code using the OpenCilk backend, and another is a synthesizable PE generator designed for HLS tools like Vitis HLS. Additionally, we introduce a decoupled access-execute optimization that enables automatic generation of high-performance PEs, improving memory-compute overlap and overall throughput.

</details>


### [15] [A Jammer-Resilient 2.87 mm$^2$ 1.28 MS/s 310 mW Multi-Antenna Synchronization ASIC in 65 nm](https://arxiv.org/abs/2511.21451)
*Flurin Arquint,Oscar Castañeda,Gian Marti,Christoph Studer*

Main category: cs.AR

TL;DR: 本文首次实现了抗干扰多天线时间同步的ASIC芯片，支持单发16收天线配置，可抵御最多两根天线的智能干扰，采用65 nm工艺，核心面积2.87 mm²，功耗310 mW，采样率达1.28 MS/s。


<details>
  <summary>Details</summary>
Motivation: 现有时间同步机制易受干扰攻击影响，尤其在关键通信场景中，亟需具备抗干扰能力的高可靠性同步方案。

Method: 采用多天线信号处理算法，在ASIC中实现对同步信号的抗干扰处理，支持单天线发射端与16天线接收端之间的同步，并能应对最多两根天线的智能干扰。

Result: 成功流片65 nm ASIC芯片，核心面积2.87 mm²，功耗310 mW，采样率1.28 MS/s，验证了所提算法的硬件可行性与有效性。

Conclusion: 该工作展示了抗干扰多天线时间同步在专用硬件上的可行性和高效性，为高安全通信系统提供了关键支撑。

Abstract: We present the first ASIC implementation of jammer-resilient multi-antenna time synchronization. The ASIC implements a recent algorithm that mitigates jamming attacks on synchronization signals using multi-antenna processing. Our design supports synchronization between a single-antenna transmitter and a 16-antenna receiver while mitigating smart jammers with up to two transmit antennas. The fabricated 65 nm ASIC has a core area of 2.87 mm$^2$, consumes a power of 310 mW, and supports a sampling rate of 1.28 mega-samples per second (MS/s).

</details>


### [16] [A 0.32 mm$^2$ 100 Mb/s 223 mW ASIC in 22FDX for Joint Jammer Mitigation, Channel Estimation, and SIMO Data Detection](https://arxiv.org/abs/2511.21461)
*Jonas Elmiger,Fabian Stuber,Oscar Castañeda,Gian Marti,Christoph Studer*

Main category: cs.AR

TL;DR: 本文提出了一款新型单输入多输出（SIMO）接收机ASIC，集成了对抗智能干扰和阻塞干扰的抑制、信道估计与数据检测功能，基于名为MAED的联合优化算法，在22nm FD-SOI工艺下实现了高吞吐量与高面积效率。


<details>
  <summary>Details</summary>
Motivation: 现有抗干扰接收机在面对智能干扰时性能受限，且通常将干扰抑制、信道估计和数据检测分步处理，难以实现最优性能。因此，亟需一种能联合处理这些任务的高效硬件方案。

Method: 采用siMultaneous mitigAtion, Estimation, and Detection（MAED）算法，通过非线性优化统一建模干扰估计与置零、信道估计和数据检测，并在ASIC中实现该算法，支持8根接收天线。

Result: 所设计的ASIC在22 nm FD-SOI工艺下核心面积为0.32 mm²，功耗223 mW，吞吐率达100 Mb/s，相比现有抗干扰检测器，每用户吞吐量提升3倍，面积效率提升4.5倍。

Conclusion: 该工作首次实现了集成干扰抑制、信道估计与数据检测于一体的SIMO接收机ASIC，显著提升了抗干扰性能与硬件效率，为未来高鲁棒性无线通信系统提供了可行方案。

Abstract: We present the first single-input multiple-output (SIMO) receiver ASIC that jointly performs jammer mitigation, channel estimation, and data detection. The ASIC implements a recent algorithm called siMultaneous mitigAtion, Estimation, and Detection (MAED). MAED mitigates smart jammers via spatial filtering using a nonlinear optimization problem that unifies jammer estimation and nulling, channel estimation, and data detection to achieve state-of-the-art error-rate performance under jamming. The design supports eight receive antennas and enables mitigation of smart jammers as well as of barrage jammers. The ASIC is fabricated in 22 nm FD-SOI, has a core area of 0.32 mm$^2$, and achieves a throughput of 100 Mb/s at 223 mW, thus delivering 3$\times$ higher per-user throughput and 4.5$\times$ higher area efficiency than the state-of-the-art jammer-resilient detector.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [17] [Assessing Redundancy Strategies to Improve Availability in Virtualized System Architectures](https://arxiv.org/abs/2511.20780)
*Alison Silva,Gustavo Callou*

Main category: cs.DC

TL;DR: 本文提出一种基于随机Petri网（SPN）的方法，用于评估在Apache CloudStack私有云中部署的Nextcloud文件服务器的可用性，并比较四种架构配置（包括主机级冗余、虚拟机冗余及其组合），结果表明同时采用主机和虚拟机冗余能显著提升系统可用性。


<details>
  <summary>Details</summary>
Motivation: 随着云存储平台在学术和商业环境中日益普及，组织对可靠性的需求不断提升，尤其在寻求公有云替代方案时。因此，评估私有云环境中文件服务器系统的可用性变得至关重要。

Method: 采用随机Petri网（SPNs）对Nextcloud文件服务器在Apache CloudStack私有云中的四种架构配置进行建模，包括基线配置、主机级冗余、虚拟机冗余以及两者结合的方式，以评估不同冗余策略对系统可用性的影响。

Result: 研究结果表明，在主机和虚拟机两个层面同时实施冗余策略能够显著提高系统可用性，并有效减少预期停机时间。

Conclusion: 所提出的基于SPN的建模方法为私有云环境中文件服务器的可用性评估提供了有效工具，可辅助基础设施架构设计决策。

Abstract: Cloud-based storage platforms are becoming more common in both academic and business settings due to their flexible access to data and support for collaborative functionalities. As reliability becomes a vital requirement, particularly for organizations looking for alternatives to public cloud services, assessing the dependability of these systems is crucial. This paper presents a methodology for analyzing the availability of a file server (Nextcloud) hosted in a private cloud environment using Apache CloudStack. The analysis is based on a modeling approach through Stochastic Petri Nets (SPNs) that allows the evaluation of different redundancy strategies to enhance the availability of such systems. Four architectural configurations were modeled, including the baseline, host-level redundancy, virtual machine (VM) redundancy, and a combination of both. The results show that redundancy at both the host and VM levels significantly improves availability and reduces expected downtime. The proposed approach provides a method to evaluate the availability of a private cloud and support infrastructure design decisions.

</details>


### [18] [Aragog: Just-in-Time Model Routing for Scalable Serving of Agentic Workflows](https://arxiv.org/abs/2511.20975)
*Yinwei Dai,Zhuofu Chen,Anand Iyer,Ravi Netravali*

Main category: cs.DC

TL;DR: Aragog 是一个动态调整工作流中各阶段大语言模型配置的系统，通过运行时自适应调度，在保持准确率的同时显著提升吞吐量并降低延迟。


<details>
  <summary>Details</summary>
Motivation: 现有工作流配置方法在请求执行前静态绑定模型选择，无法应对执行过程中系统负载的快速变化，导致成本高或性能下降。

Method: Aragog 将配置问题解耦为两个部分：一次性路由步骤识别所有保准确率的配置，以及每阶段基于实时系统状态的轻量级调度器从中选择最优配置，并引入新策略加速这两个过程。

Result: 在多种工作流和模型族上，Aragog 在峰值请求速率下将最大吞吐量提升 50.0–217.0%，中位延迟降低 32.5–78.9%，同时保持与最昂贵配置相当的准确率。

Conclusion: 通过运行时动态调整配置，Aragog 能有效应对系统负载波动，在保证准确性的前提下显著优化服务效率和响应速度。

Abstract: Agentic workflows have emerged as a powerful paradigm for solving complex, multi-stage tasks, but serving them at scale is computationally expensive given the many LLM inferences that each request must pass through. Configuration selection, or the cost-aware assignment of workflow agents to specific LLMs, can reduce these costs, but existing approaches bind configuration decisions before request execution, making them ill-suited for the heterogeneous and lengthy execution of workflows. Specifically, system loads can fluctuate rapidly and substantially during a request's lifetime, causing fixed configurations to quickly become suboptimal. We present Aragog, a system that progressively adapts a request's configuration throughout its execution to match runtime dynamics. To make this practical despite the massive space of workflow configurations, Aragog decouples the problem into two core elements -- a one-time routing step that identifies all accuracy-preserving configurations, and a cheap per-stage scheduler that selects among them using up-to-date system observations -- and introduces novel strategies to accelerate each. Across diverse workflows and model families, Aragog increases maximum serving throughput by 50.0--217.0\% and reduces median latency by 32.5--78.9\% at peak request rates, while maintaining accuracy comparable to the most expensive configurations.

</details>


### [19] [A Dynamic PD-Disaggregation Architecture for Maximizing Goodput in LLM Inference Serving](https://arxiv.org/abs/2511.20982)
*Junhan Liao,Minxian Xu,Wanyi Zheng,Yan Wang,Kejiang Ye,Rajkumar Buyya,Chengzhong Xu*

Main category: cs.DC

TL;DR: DOPD 是一种动态 LLM 推理系统，通过实时监控负载动态调整预填充（prefill）与解码（decoding）阶段的 GPU 资源分配比例，以解决异构工作负载导致的生产者-消费者失衡问题，从而显著提升系统吞吐量并降低延迟。


<details>
  <summary>Details</summary>
Motivation: 当前 LLM 推理系统将 prefill 和 decoding 阶段分离部署在不同 GPU 上以应对各自瓶颈，但因工作负载异构性，容易造成两个阶段资源利用不均衡，影响整体性能和 SLO 达成。

Method: 提出 DOPD 系统，基于实时负载监测动态调整 prefill/decoding 实例数量以维持最优 P/D 比例，并结合合适的请求调度策略；同时利用历史负载信息进行主动重配置。

Result: 相比 vLLM 和 DistServe，DOPD 最多提升 1.5 倍系统 goodput，P90 TTFT 降低最多 67.5%，P90 TPOT 降低最多 22.8%，并在使用较少额外资源的情况下实现超过 99% 的 SLO 达成率。

Conclusion: DOPD 有效解决了 LLM 推理中 disaggregated 架构下的资源失衡问题，显著提升了性能与资源效率，同时保障高 SLO 达成率。

Abstract: To meet strict Service-Level Objectives (SLOs),contemporary Large Language Models (LLMs) decouple the prefill and decoding stages and place them on separate GPUs to mitigate the distinct bottlenecks inherent to each phase. However, the heterogeneity of LLM workloads causes producerconsumer imbalance between the two instance types in such disaggregated architecture. To address this problem, we propose DOPD (Dynamic Optimal Prefill/Decoding), a dynamic LLM inference system that adjusts instance allocations to achieve an optimal prefill-to-decoding (P/D) ratio based on real-time load monitoring. Combined with an appropriate request-scheduling policy, DOPD effectively resolves imbalances between prefill and decoding instances and mitigates resource allocation mismatches due to mixed-length requests under high concurrency. Experimental evaluations show that, compared with vLLM and DistServe (representative aggregation-based and disaggregationbased approaches), DOPD improves overall system goodput by up to 1.5X, decreases P90 time-to-first-token (TTFT) by up to 67.5%, and decreases P90 time-per-output-token (TPOT) by up to 22.8%. Furthermore, our dynamic P/D adjustment technique performs proactive reconfiguration based on historical load, achieving over 99% SLOs attainment while using less additional resources.

</details>


### [20] [Automated Dynamic AI Inference Scaling on HPC-Infrastructure: Integrating Kubernetes, Slurm and vLLM](https://arxiv.org/abs/2511.21413)
*Tim Trappen,Robert Keßler,Roland Pabel,Viktor Achter,Stefan Wesner*

Main category: cs.DC

TL;DR: 本文提出了一种在超算RAMSES上结合vLLM、Slurm和Kubernetes的架构，以高效支持面向用户的动态大语言模型（LLM）推理服务，实验证明其在高并发请求下仅引入约500毫秒的端到端延迟。


<details>
  <summary>Details</summary>
Motivation: 传统高性能计算（HPC）的运行模式难以满足同步、面向用户的动态AI应用负载需求，尤其是在高等教育领域对AI推理需求不断增长的背景下，亟需利用现有基础设施构建新型解决方案。

Method: 在超算RAMSES上集成vLLM、Slurm和Kubernetes，构建支持大语言模型（LLM）推理的服务架构。

Result: 初步基准测试表明，该架构在100、500和1000个并发请求下均能高效扩展，端到端延迟仅增加约500毫秒。

Conclusion: 所提出的集成架构有效解决了传统HPC在支持动态AI推理任务时的适配问题，为在现有超算基础设施上部署用户导向的LLM服务提供了可行方案。

Abstract: Due to rising demands for Artificial Inteligence (AI) inference, especially in higher education, novel solutions utilising existing infrastructure are emerging. The utilisation of High-Performance Computing (HPC) has become a prevalent approach for the implementation of such solutions. However, the classical operating model of HPC does not adapt well to the requirements of synchronous, user-facing dynamic AI application workloads. In this paper, we propose our solution that serves LLMs by integrating vLLM, Slurm and Kubernetes on the supercomputer \textit{RAMSES}. The initial benchmark indicates that the proposed architecture scales efficiently for 100, 500 and 1000 concurrent requests, incurring only an overhead of approximately 500 ms in terms of end-to-end latency.

</details>


### [21] [Modeling the Effect of Data Redundancy on Speedup in MLFMA Near-Field Computation](https://arxiv.org/abs/2511.21535)
*Morteza Sadeghi*

Main category: cs.DC

TL;DR: 本文通过引入数据冗余改善MLFMA中近场（P2P）算子在GPU上的内存局部性，从而提升性能；提出基于局部性指标的分析模型预测加速趋势，并在两类应用中验证，获得最高7倍的核函数加速，但端到端加速受限于数据重构开销。


<details>
  <summary>Details</summary>
Motivation: MLFMA中的近场（P2P）算子在GPU上因内存局部性差而成为性能瓶颈。

Method: 引入数据冗余以减少内存访问分散性，提升空间局部性；构建结合数据量与访问分散性的局部性指标模型，用于预测加速趋势；在DBIM-MLFMA和PhotoNs-2.0两个应用中进行验证。

Result: 核函数最高加速达7倍，但因数据重构开销，端到端应用加速仅达1.04倍；所提模型虽不能精确预测绝对加速比，但能可靠反映不同问题规模和密度下的性能趋势。

Conclusion: 数据冗余可有效提升P2P算子在GPU上的性能，前提是局部性收益超过数据移动成本；该方法易于集成到现有实现中，且分析模型对性能趋势具有良好的预测能力。

Abstract: The near-field (P2P) operator in the Multilevel Fast Multipole Algorithm (MLFMA) is a performance bottleneck on GPUs due to poor memory locality. This work introduces data redundancy to improve spatial locality by reducing memory access dispersion. For validation of results, we propose an analytical model based on a Locality metric that combines data volume and access dispersion to predict speedup trends without hardware-specific profiling. The approach is validated on two MLFMA-based applications: an electromagnetic solver (DBIM-MLFMA) with regular structure, and a stellar dynamics code (PhotoNs-2.0) with irregular particle distribution. Results show up to 7X kernel speedup due to improved cache behavior. However, increased data volume raises overheads in data restructuring, limiting end-to-end application speedup to 1.04X. While the model cannot precisely predict absolute speedups, it reliably captures performance trends across different problem sizes and densities. The technique is injectable into existing implementations with minimal code changes. This work demonstrates that data redundancy can enhance GPU performance for P2P operator, provided locality gains outweigh data movement costs.

</details>


### [22] [MemFine: Memory-Aware Fine-Grained Scheduling for MoE Training](https://arxiv.org/abs/2511.21431)
*Lu Zhao,Rong Shi,Shaoqing Zhang,Yueqiang Chen,Baoguo He,Hongfeng Sun,Ziqing Yin,Shangchao Su,Zhiyan Cui,Liang Dong,Xiyuan Li,Lingbin Wang,Jianwei He,Jiesong Ma,Weikang Huang,Jianglei Tong,Dongdong Gao,Jian Zhang,Hong Tian*

Main category: cs.DC

TL;DR: MemFine is a memory-aware scheduling framework that enables efficient large-scale Mixture of Experts (MoE) model training on memory-limited GPUs by reducing activation memory and improving throughput through fine-grained chunking and dynamic recomputation.


<details>
  <summary>Details</summary>
Motivation: Training large-scale MoE models suffers from severe memory bottlenecks due to load imbalance from dynamic token routing, causing GPU memory overflow and limiting scalability—existing capacity-capping methods hurt accuracy and fail under tight memory constraints.

Method: MemFine decomposes token distribution and expert computation into chunks and applies a dynamically optimized chunked recomputation strategy guided by a theoretical memory model to balance memory usage and computational throughput.

Result: MemFine reduces activation memory by 48.03% and increases throughput by 4.42% compared to full recomputation baselines, enabling stable MoE training on GPUs with limited memory.

Conclusion: MemFine effectively addresses the memory bottleneck in large-scale MoE training, making it feasible to train such models on memory-constrained hardware without sacrificing accuracy or stability.

Abstract: The training of large-scale Mixture of Experts (MoE) models faces a critical memory bottleneck due to severe load imbalance caused by dynamic token routing. This imbalance leads to memory overflow on GPUs with limited capacity, constraining model scalability. Existing load balancing methods, which cap expert capacity, compromise model accuracy and fail on memory-constrained hardware. To address this, we propose MemFine, a memory-aware fine-grained scheduling framework for MoE training. MemFine decomposes the token distribution and expert computation into manageable chunks and employs a chunked recomputation strategy, dynamically optimized through a theoretical memory model to balance memory efficiency and throughput. Experiments demonstrate that MemFine reduces activation memory by 48.03% and improves throughput by 4.42% compared to full recomputation-based baselines, enabling stable large-scale MoE training on memory-limited GPUs.

</details>


### [23] [Diagonal Scaling: A Multi-Dimensional Resource Model and Optimization Framework for Distributed Databases](https://arxiv.org/abs/2511.21612)
*Shahir Abdullah,Syed Rohit Zaman*

Main category: cs.DC

TL;DR: 本文提出“缩放平面”（Scaling Plane）模型，将数据库扩缩容视为二维问题（节点数量与单节点资源），并设计DIAGONALSCALE算法进行联合优化，在延迟、成本和重平衡开销方面显著优于传统仅水平或仅垂直扩缩容方法。


<details>
  <summary>Details</summary>
Motivation: 现有云数据库将扩缩容简化为水平扩展（加节点）或垂直扩展（提升单节点资源）的二元选择，忽略了二者协同对性能、成本和协调开销的综合影响，导致系统响应负载变化时效率低下。

Method: 提出二维“缩放平面”模型，其中每个配置表示为(H, V)点，并在此平面上定义延迟、吞吐量、协调开销和成本的平滑近似；设计DIAGONALSCALE离散局部搜索算法，在满足SLA约束下，通过评估水平、垂直及对角线移动，选择多目标最优配置。

Result: 实验表明，相比仅水平或仅垂直扩缩容，对角线扩缩容可将p95延迟最多降低40%，每查询成本最多降低37%，并将重平衡操作减少2至5倍。

Conclusion: 多维扩缩容模型能更全面地捕捉云数据库的性能权衡，对角线扩缩容策略显著优于传统方法，为下一代云数据库自动扩缩容系统提供了新范式。

Abstract: Modern cloud databases present scaling as a binary decision: scale-out by adding nodes or scale-up by increasing per-node resources. This one-dimensional view is limiting because database performance, cost, and coordination overhead emerge from the joint interaction of horizontal elasticity and per-node CPU, memory, network bandwidth, and storage IOPS. As a result, systems often overreact to load spikes, underreact to memory pressure, or oscillate between suboptimal states. We introduce the Scaling Plane, a two-dimensional model in which each distributed database configuration is represented as a point (H, V), with H denoting node count and V a vector of resources. Over this plane, we define smooth approximations of latency, throughput, coordination overhead, and monetary cost, providing a unified view of performance trade-offs. We show analytically and empirically that optimal scaling trajectories frequently lie along diagonal paths: sequences of joint horizontal and vertical adjustments that simultaneously exploit cluster parallelism and per-node improvements. To compute such actions, we propose DIAGONALSCALE, a discrete local-search algorithm that evaluates horizontal, vertical, and diagonal moves in the Scaling Plane and selects the configuration minimizing a multi-objective function subject to SLA constraints. Using synthetic surfaces, microbenchmarks, and experiments on distributed SQL and KV systems, we demonstrate that diagonal scaling reduces p95 latency by up to 40 percent, lowers cost-per-query by up to 37 percent, and reduces rebalancing by 2 to 5 times compared to horizontal-only and vertical-only autoscaling. Our results highlight the need for multi-dimensional scaling models and provide a foundation for next-generation autoscaling in cloud database systems.

</details>


### [24] [AI/ML Model Cards in Edge AI Cyberinfrastructure: towards Agentic AI](https://arxiv.org/abs/2511.21661)
*Beth Plale,Neelesh Karthikeyan,Isuru Gamage,Joe Stubbs,Sachith Withana*

Main category: cs.DC

TL;DR: 该论文研究了将模型上下文协议（MCP）作为Patra模型卡服务器接口的优劣，对比其与REST接口的开销，并探讨MCP在支持动态模型卡中的适用性。


<details>
  <summary>Details</summary>
Motivation: 静态的一次性模型评估无法反映AI/ML模型在其生命周期中的实际使用情况，因此需要将模型卡视为动态对象进行持续追踪和更新。

Method: 通过在ICICLE AI研究所软件生态系统中嵌入Patra模型卡，采用定量方法比较MCP与REST接口的性能开销，并通过定性分析评估MCP在支持动态模型卡场景下的适用性和用户体验。

Result: 定量结果显示MCP相比REST接口存在一定开销；定性分析则表明MCP能更好地支持动态模型卡所需的活跃会话和上下文交互。

Conclusion: MCP虽有性能开销，但在支持动态模型卡的上下文感知和持续交互方面具有优势，适合用于需要长期追踪模型使用情况的场景。

Abstract: AI/ML model cards can contain a benchmarked evaluation of an AI/ML model against intended use but a one time assessment during model training does not get at how and where a model is actually used over its lifetime. Through Patra Model Cards embedded in the ICICLE AI Institute software ecosystem we study model cards as dynamic objects. The study reported here assesses the benefits and tradeoffs of adopting the Model Context Protocol (MCP) as an interface to the Patra Model Card server. Quantitative assessment shows the overhead of MCP as compared to a REST interface. The core question however is of active sessions enabled by MCP; this is a qualitative question of fit and use in the context of dynamic model cards that we address as well.

</details>
