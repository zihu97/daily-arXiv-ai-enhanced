<div id=toc></div>

# Table of Contents

- [cs.DC](#cs.DC) [Total: 9]
- [cs.SE](#cs.SE) [Total: 22]
- [cs.MA](#cs.MA) [Total: 2]
- [cs.AR](#cs.AR) [Total: 3]
- [cs.NI](#cs.NI) [Total: 3]


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [1] [A Multi-Port Concurrent Communication Model for handling Compute Intensive Tasks on Distributed Satellite System Constellations](https://arxiv.org/abs/2601.01031)
*Bharadwaj Veeravalli*

Main category: cs.DC

TL;DR: 提出首个适用于分布式卫星系统的多端口并发通信可分负载理论模型，优化任务分配与完成时间，并支持实时准入控制。


<details>
  <summary>Details</summary>
Motivation: 解决分布式卫星系统中异构计算与通信条件下任务调度与时间约束的挑战。

Method: 构建MPCC-DLT框架，推导闭式最优负载分配与截止期可行性条件，结合仿真与实时准入控制机制验证。

Result: 高度可分任务显著降低延迟，通信密集型任务因结果回传开销收益递减；系统参数决定截止期满足率与运行模式。

Conclusion: 该模型为未来卫星星座的应用感知调度与系统设计提供理论支撑与实用指导。

Abstract: We develop an integrated Multi-Port Concurrent Communication Divisible Load Theory (MPCC-DLT) framework for relay-centric distributed satellite systems (DSS), capturing concurrent data dissemination, parallel computation, and result return under heterogeneous onboard processing and inter-satellite link conditions. We propose a formulation that yields closed-form expressions for optimal load allocation and completion time that explicitly quantify the joint impact of computation speed, link bandwidth, and result-size overhead. We further derive deadline feasibility conditions that enable explicit sizing of cooperative satellite clusters to meet time-critical task requirements. Extensive simulation results demonstrate that highly distributable tasks achieve substantial latency reduction, while communication-heavy tasks exhibit diminishing returns due to result-transfer overheads. To bridge theory and practice, we extend the MPCC-DLT framework with a real-time admission control mechanism that handles stochastic task arrivals and deadline constraints, enabling blocking-aware operation. Our real-time simulations illustrate how task structure and system parameters jointly govern deadline satisfaction and operating regimes. Overall, this work provides the first analytically tractable MPCC-DLT model for distributed satellite systems and offers actionable insights for application-aware scheduling and system-level design of future satellite constellations.

</details>


### [2] [Performance and Security Aware Distributed Service Placement in Fog Computing](https://arxiv.org/abs/2601.01125)
*Mohammad Goudarzi,Arash Shaghaghi,Zhiyu Wang,Rajkumar Buyya*

Main category: cs.DC

TL;DR: 本文提出了一种兼顾安全与性能的分布式深度强化学习框架（SPA-DDRL），用于雾计算中服务放置的联合优化，在降低响应时间的同时提升安全合规性。


<details>
  <summary>Details</summary>
Motivation: 现有方案多关注性能指标，忽视部署决策中的安全影响，而物联网应用对高效、安全的服务放置需求日益增长。

Method: 构建加权多目标优化模型，引入三层安全评分机制，并采用分布式broker-learner架构结合LSTM、优先经验回放与离策略校正提升智能体性能。

Result: 实验表明SPA-DDRL在真实IoT负载下响应时间提升16.3%，收敛速度加快33%，且在所有系统规模下均保持安全合规。

Conclusion: SPA-DDRL能有效平衡雾计算中服务放置的安全性与性能，优于现有基线方法。

Abstract: The rapid proliferation of IoT applications has intensified the demand for efficient and secure service placement in Fog computing. However, heterogeneous resources, dynamic workloads, and diverse security requirements make optimal service placement highly challenging. Most solutions focus primarily on performance metrics while overlooking the security implications of deployment decisions. This paper proposes a Security and Performance-Aware Distributed Deep Reinforcement Learning (SPA-DDRL) framework for joint optimization of service response time and security compliance in Fog computing. The problem is formulated as a weighted multi-objective optimization task, minimizing latency while maximizing a security score derived from the security capabilities of Fog nodes. The security score features a new three-tier hierarchy, where configuration-level checks verify proper settings, capability-level assessments evaluate the resource security features, and control-level evaluations enforce stringent policies, thereby ensuring compliant solutions that align with performance objectives. SPA-DDRL adopts a distributed broker-learner architecture where multiple brokers perform autonomous service-placement decisions and a centralized learner coordinates global policy optimization through shared prioritized experiences. It integrates three key improvements, including Long Short-Term Memory networks, Prioritized Experience Replay, and off-policy correction mechanisms to improve the agent's performance. Experiments based on real IoT workloads show that SPA-DDRL significantly improves both service response time and placement security compared to current approaches, achieving a 16.3% improvement in response time and a 33% faster convergence rate. It also maintains consistent, feasible, security-compliant solutions across all system scales, while baseline techniques fail or show performance degradation.

</details>


### [3] [Making MoE based LLM inference resilient with Tarragon](https://arxiv.org/abs/2601.01310)
*Songyu Zhang,Aaron Tam,Myungjin Lee,Shixiong Qi,K. K. Ramakrishnan*

Main category: cs.DC

TL;DR: Tarragon是一个针对Mixture-of-Experts模型的高弹性推理框架，通过隔离故障影响、重路由请求和自愈机制显著降低故障恢复延迟。


<details>
  <summary>Details</summary>
Motivation: 现有MoE系统在大规模部署时故障恢复效率低下，严重影响延迟敏感型LLM服务。

Method: Tarragon将注意力与专家计算分离为独立故障域，采用可重构数据路径、异步KV缓存检查点和影子专家实现低开销恢复。

Result: 相比MegaScale-Infer，Tarragon将故障导致的停顿减少160-213倍（从约64秒降至0.3-0.4秒），且无故障时性能无损。

Conclusion: Tarragon有效提升了MoE模型在大规模部署中的容错能力，适合实际生产环境。

Abstract: Mixture-of-Experts (MoE) models are increasingly used to serve LLMs at scale, but failures become common as deployment scale grows. Existing systems exhibit poor failure resilience: even a single worker failure triggers a coarse-grained, service-wide restart, discarding accumulated progress and halting the entire inference pipeline during recovery--an approach clearly ill-suited for latency-sensitive, LLM services.
  We present Tarragon, a resilient MoE inference framework that confines the failures impact to individual workers while allowing the rest of the pipeline to continue making forward progress. Tarragon exploits the natural separation between the attention and expert computation in MoE-based transformers, treating attention workers (AWs) and expert workers (EWs) as distinct failure domains. Tarragon introduces a reconfigurable datapath to mask failures by rerouting requests to healthy workers. On top of this datapath, Tarragon implements a self-healing mechanism that relaxes the tightly synchronized execution of existing MoE frameworks. For stateful AWs, Tarragon performs asynchronous, incremental KV cache checkpointing with per-request restoration, and for stateless EWs, it leverages residual GPU memory to deploy shadow experts. These together keep recovery cost and recomputation overhead extremely low. Our evaluation shows that, compared to state-of-the-art MegaScale-Infer, Tarragon reduces failure-induced stalls by 160-213x (from ~64 s down to 0.3-0.4 s) while preserving performance when no failures occur.

</details>


### [4] [DiT-HC: Enabling Efficient Training of Visual Generation Model DiT on HPC-oriented CPU Cluster](https://arxiv.org/abs/2601.01500)
*Jinxiao Zhang,Yunpu Xu,Xiyong Wu,Runmin Dong,Shenggan Cheng,Yi Zhao,Mengxuan Chen,Qinrui Zheng,Jianting Liu,Haohuan Fu*

Main category: cs.DC

TL;DR: DiT-HC是首个在下一代HPC CPU集群上训练和扩展生成模型DiT的系统，通过三项关键技术实现高效能计算与AI融合。


<details>
  <summary>Details</summary>
Motivation: 推动生成式基础模型在科学计算中的大规模训练，并利用新型CPU硬件特性加速AI与高性能计算的协同设计。

Method: 提出通信无关张量并行（CFTP）、优化算子库HCOps、以及定制MPI后端以重叠计算、通信与内存移动。

Result: 相比原生或公开CPU库提速8.2至87.7倍，在256节点上实现90.6%弱扩展效率。

Conclusion: 证明了在CPU集群上训练大规模生成模型的可行性，为未来HPC-AI协同设计提供新思路。

Abstract: Generative foundation models have become an important tool for data reconstruction and simulation in scientific computing, showing a tight integration with traditional numerical simulations. At the same time, with the development of new hardware features, such as matrix acceleration units and high-bandwidth memory, CPU-based clusters offer promising opportunities to accelerate and scale such models, facilitating the unification of artificial intelligence and scientific computing. We present DiT-HC, the first system to train and scale the generative model DiT on a next-generation HPC CPU cluster. DiT-HC introduces three key techniques: (1) communication-free tensor parallelism (CFTP) with AutoMem for automated memory-aware dataflow, (2) HCOps, a suite of optimized GEMM and operator kernels leveraging vector and matrix acceleration units, and (3) a custom MPI backend that overlaps computation, communication, and memory movement. Experiments show 8.2 to 87.7 times speedups over native or public CPU libraries and 90.6% weak scaling efficiency on 256 nodes. These results demonstrate the feasibility of large-scale generative model training on CPU clusters and provide new insights for future HPC-AI co-design.

</details>


### [5] [FFCz: Fast Fourier Correction for Spectrum-Preserving Lossy Compression of Scientific Data](https://arxiv.org/abs/2601.01596)
*Congrong Ren,Robert Underwood,Sheng Di,Emrecan Kutay,Zarija Lukic,Aylin Yener,Franck Cappello,Hanqi Guo*

Main category: cs.DC

TL;DR: 提出一种基于快速傅里叶校正的新算法，在有损压缩中同时保留空间与频域特征。


<details>
  <summary>Details</summary>
Motivation: 现有压缩方法仅保证空间域精度，忽视频域保真，而许多科学应用需双域精确表示。

Method: 通过将频域误差表达为空间误差的线性组合，迭代投影至双域约束交集区域，并利用GPU加速实现高效校正。

Result: 在宇宙学、X射线衍射、燃烧模拟等数据上验证了算法在双域均有效保留关键科学信息。

Conclusion: 该方法成功弥补现有压缩器频域失真缺陷，为多领域科学数据压缩提供新方案。

Abstract: This paper introduces a novel technique to preserve spectral features in lossy compression based on a novel fast Fourier correction algorithm\added{ for regular-grid data}. Preserving both spatial and frequency representations of data is crucial for applications such as cosmology, turbulent combustion, and X-ray diffraction, where spatial and frequency views provide complementary scientific insights. In particular, many analysis tasks rely on frequency-domain representations to capture key features, including the power spectrum of cosmology simulations, the turbulent energy spectrum in combustion, and diffraction patterns in reciprocal space for ptychography. However, existing compression methods guarantee accuracy only in the spatial domain while disregarding the frequency domain. To address this limitation, we propose an algorithm that corrects the errors produced by off-the-shelf ``base'' compressors such as SZ3, ZFP, and SPERR, thereby preserving both spatial and frequency representations by bounding errors in both domains. By expressing frequency-domain errors as linear combinations of spatial-domain errors, we derive a region that jointly bounds errors in both domains. Given as input the spatial errors from a base compressor and user-defined error bounds in the spatial and frequency domains, we iteratively project the spatial error vector onto the regions defined by the spatial and frequency constraints until it lies within their intersection. We further accelerate the algorithm using GPU parallelism to achieve practical performance. We validate our approach with datasets from cosmology simulations, X-ray diffraction, combustion simulation, and electroencephalography demonstrating its effectiveness in preserving critical scientific information in both spatial and frequency domains.

</details>


### [6] [RelayGR: Scaling Long-Sequence Generative Recommendation via Cross-Stage Relay-Race Inference](https://arxiv.org/abs/2601.01712)
*Jiarui Wang,Huichao Chai,Yuanhang Zhang,Zongjin Zhou,Wei Guo,Xingkun Yang,Qiang Tang,Bo Pan,Jiawei Zhu,Ke Cheng,Yuting Yan,Shulan Wang,Yingjie Zhu,Zhengfan Yuan,Jiaqi Huang,Yuhan Zhang,Xiaosong Sun,Zhinan Zhang,Hong Zhu,Yongsheng Zhang,Tiantian Dong,Zhong Xiao,Deliang Liu,Chengzhou Lu,Yuan Sun,Zhiyuan Chen,Xinming Han,Zaizhu Liu,Yaoyuan Wang,Ziyang Zhang,Yong Liu,Jinxin Xu,Yajing Sun,Zhoujun Yu,Wenting Zhou,Qidong Zhang,Zhengyong Zhang,Zhonghai Gu,Yibo Jin,Yongxiang Feng,Pengfei Zuo*

Main category: cs.DC

TL;DR: RelayGR通过预推理用户行为前缀并缓存于HBM，显著提升生成式推荐模型在实时推荐系统中的序列长度与吞吐量。


<details>
  <summary>Details</summary>
Motivation: 生成式推荐模型虽能提升质量，但受限于线上P99延迟预算，无法处理长序列；RelayGR旨在突破此瓶颈。

Method: 结合序列感知触发器、亲和感知路由与内存感知扩展器，在Ascend NPU上实现HBM内接力推理，避免远程获取缓存。

Result: 在固定P99 SLO下，序列长度支持提升1.5倍，SLO合规吞吐量最高提升3.6倍。

Conclusion: RelayGR有效解决工业级生成式推荐系统中长序列推理的延迟与资源瓶颈问题。

Abstract: Real-time recommender systems execute multi-stage cascades (retrieval, pre-processing, fine-grained ranking) under strict tail-latency SLOs, leaving only tens of milliseconds for ranking. Generative recommendation (GR) models can improve quality by consuming long user-behavior sequences, but in production their online sequence length is tightly capped by the ranking-stage P99 budget. We observe that the majority of GR tokens encode user behaviors that are independent of the item candidates, suggesting an opportunity to pre-infer a user-behavior prefix once and reuse it during ranking rather than recomputing it on the critical path. Realizing this idea at industrial scale is non-trivial: the prefix cache must survive across multiple pipeline stages before the final ranking instance is determined, the user population implies cache footprints far beyond a single device, and indiscriminate pre-inference would overload shared resources under high QPS. We present RelayGR, a production system that enables in-HBM relay-race inference for GR. RelayGR selectively pre-infers long-term user prefixes, keeps their KV caches resident in HBM over the request lifecycle, and ensures the subsequent ranking can consume them without remote fetches. RelayGR combines three techniques: 1) a sequence-aware trigger that admits only at-risk requests under a bounded cache footprint and pre-inference load, 2) an affinity-aware router that co-locates cache production and consumption by routing both the auxiliary pre-infer signal and the ranking request to the same instance, and 3) a memory-aware expander that uses server-local DRAM to capture short-term cross-request reuse while avoiding redundant reloads. We implement RelayGR on Huawei Ascend NPUs and evaluate it with real queries. Under a fixed P99 SLO, RelayGR supports up to 1.5$\times$ longer sequences and improves SLO-compliant throughput by up to 3.6$\times$.

</details>


### [7] [pMSz: A Distributed Parallel Algorithm for Correcting Extrema and Morse Smale Segmentations in Lossy Compression](https://arxiv.org/abs/2601.01787)
*Yuxiao Li,Mingze Xia,Xin Liang,Bei Wang,Robert Underwood,Sheng Di,Hemant Sharma,Dishant Beniwal,Franck Cappello,Hanqi Guo*

Main category: cs.DC

TL;DR: 提出一种分布式并行算法，高效修正压缩后数据的拓扑特征（PLMSS），在128个GPU上实现超90%并行效率。


<details>
  <summary>Details</summary>
Motivation: 有损压缩会扭曲关键拓扑特征，影响下游科学分析准确性，现有单GPU方法无法扩展至超大规模数据。

Method: 通过保留最陡升降方向替代显式积分路径计算，降低通信开销与同步需求，实现多GPU高效并行化。

Result: 在Perlmutter超算128 GPU上对真实数据集实现超过90%并行效率，存储开销可忽略。

Conclusion: 该方法有效解决了大规模数据拓扑特征修正的扩展瓶颈，兼顾精度与高性能。

Abstract: Lossy compression, widely used by scientists to reduce data from simulations, experiments, and observations, can distort features of interest even under bounded error. Such distortions may compromise downstream analyses and lead to incorrect scientific conclusions in applications such as combustion and cosmology. This paper presents a distributed and parallel algorithm for correcting topological features, specifically, piecewise linear Morse Smale segmentations (PLMSS), which decompose the domain into monotone regions labeled by their corresponding local minima and maxima. While a single GPU algorithm (MSz) exists for PLMSS correction after compression, no methodology has been developed that scales beyond a single GPU for extreme scale data. We identify the key bottleneck in scaling PLMSS correction as the parallel computation of integral paths, a communication-intensive computation that is notoriously difficult to scale. Instead of explicitly computing and correcting integral paths, our algorithm simplifies MSz by preserving steepest ascending and descending directions across all locations, thereby minimizing interprocess communication while introducing negligible additional storage overhead. With this simplified algorithm and relaxed synchronization, our method achieves over 90% parallel efficiency on 128 GPUs on the Perlmutter supercomputer for real world datasets.

</details>


### [8] [BigSUMO: A Scalable Framework for Big Data Traffic Analytics and Parallel Simulation](https://arxiv.org/abs/2601.02286)
*Rahul Sengupta,Nooshin Yousefzadeh,Manav Sanghvi,Yash Ranjan,Anand Rangarajan,Sanjay Ranka,Yashaswi Karnati,Jeremy Dilmore,Tushar Patel,Ryan Casburn*

Main category: cs.DC

TL;DR: BigSUMO是一个开源、可扩展的端到端框架，用于交通数据分析、异常检测与并行仿真，助力智慧城市交通管理。


<details>
  <summary>Details</summary>
Motivation: 应对全球城市化带来的交通基础设施管理挑战，需高效分析海量交通数据并制定干预措施。

Method: 整合高分辨率检测器数据与稀疏轨迹数据，先做描述性分析和异常检测，再利用SUMO微仿真器进行数百种场景优化。

Result: 系统模块化设计支持灵活算法集成，基于开源工具构建，成本低、易部署、可扩展。

Conclusion: BigSUMO有望成为开发智慧城市出行解决方案的重要工具。

Abstract: With growing urbanization worldwide, efficient management of traffic infrastructure is critical for transportation agencies and city planners. It is essential to have tools that help analyze large volumes of stored traffic data and make effective interventions. To address this need, we present ``BigSUMO", an end-to-end, scalable, open-source framework for analytics, interruption detection, and parallel traffic simulation. Our system ingests high-resolution loop detector and signal state data, along with sparse probe trajectory data. It first performs descriptive analytics and detects potential interruptions. It then uses the SUMO microsimulator for prescriptive analytics, testing hundreds of what-if scenarios to optimize traffic performance. The modular design allows integration of different algorithms for data processing and outlier detection. Built using open-source software and libraries, the pipeline is cost-effective, scalable, and easy to deploy. We hope BigSUMO will be a valuable aid in developing smart city mobility solutions.

</details>


### [9] [Placement Semantics for Distributed Deep Learning: A Systematic Framework for Analyzing Parallelism Strategies](https://arxiv.org/abs/2601.02311)
*Deep Pankajbhai Mehta*

Main category: cs.DC

TL;DR: 提出一种基于状态放置语义的统一框架，用于预测和组合分布式训练策略的内存与通信开销。


<details>
  <summary>Details</summary>
Motivation: 现有并行策略选择依赖试错，缺乏系统性理论框架预测其行为。

Method: 通过定义参数、优化器、梯度、激活四类状态在设备上的五种放置模式，推导内存与通信量，并建立正确性条件与组合规则。

Result: 精确复现ZeRO-3等策略的内存与通信开销，统一解释多种主流并行方法。

Conclusion: 该框架为分布式训练策略的设计与组合提供了理论基础和实用指导。

Abstract: Training large language models requires distributing computation across many accelerators, yet practitioners select parallelism strategies (data, tensor, pipeline, ZeRO) through trial and error because no unified systematic framework predicts their behavior. We introduce placement semantics: each strategy is specified by how it places four training states (parameters, optimizer, gradients, activations) across devices using five modes (replicated, sharded, sharded-with-gather, materialized, offloaded). From placement alone, without implementation details, we derive memory consumption and communication volume. Our predictions match published results exactly: ZeRO-3 uses 8x less memory than data parallelism at 1.5x communication cost, as reported in the original paper. We prove two conditions (gradient integrity, state consistency) are necessary and sufficient for distributed training to match single-device results, and provide composition rules for combining strategies safely. The framework unifies ZeRO Stages 1-3, Fully Sharded Data Parallel (FSDP), tensor parallelism, and pipeline parallelism as instances with different placement choices.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [10] [SeRe: A Security-Related Code Review Dataset Aligned with Real-World Review Activities](https://arxiv.org/abs/2601.01042)
*Zixiao Zhao,Yanjie Jiang,Hui Liu,Kui Liu,Lu Zhang*

Main category: cs.SE

TL;DR: 提出SeRe数据集，通过主动学习方法从大量代码评审中提取安全相关评论，以支持自动化安全评审研究。


<details>
  <summary>Details</summary>
Motivation: 现有代码评审数据缺乏安全标注或规模不足，难以支撑大规模安全研究。

Method: 采用基于主动学习的集成分类方法，结合人工标注迭代优化模型，从原始评审中筛选安全相关评论。

Result: 构建包含6732条安全评审的数据集SeRe，覆盖多语言且符合真实分布，并完成基准测试。

Conclusion: SeRe有助于推动自动化安全代码评审研究，提升软件工程安全性实践。

Abstract: Software security vulnerabilities can lead to severe consequences, making early detection essential. Although code review serves as a critical defense mechanism against security flaws, relevant feedback remains scarce due to limited attention to security issues or a lack of expertise among reviewers. Existing datasets and studies primarily focus on general-purpose code review comments, either lacking security-specific annotations or being too limited in scale to support large-scale research. To bridge this gap, we introduce \textbf{SeRe}, a \textbf{security-related code review dataset}, constructed using an active learning-based ensemble classification approach. The proposed approach iteratively refines model predictions through human annotations, achieving high precision while maintaining reasonable recall. Using the fine-tuned ensemble classifier, we extracted 6,732 security-related reviews from 373,824 raw review instances, ensuring representativeness across multiple programming languages. Statistical analysis indicates that SeRe generally \textbf{aligns with real-world security-related review distribution}. To assess both the utility of SeRe and the effectiveness of existing code review comment generation approaches, we benchmark state-of-the-art approaches on security-related feedback generation. By releasing SeRe along with our benchmark results, we aim to advance research in automated security-focused code review and contribute to the development of more effective secure software engineering practices.

</details>


### [11] [RovoDev Code Reviewer: A Large-Scale Online Evaluation of LLM-based Code Review Automation at Atlassian](https://arxiv.org/abs/2601.01129)
*Kla Tantithamthavorn,Yaotian Zou,Andy Wong,Michael Gupta,Zhe Wang,Mike Buller,Ryan Jiang,Matthew Watson,Minwoo Jeong,Kun Chen,Ming Wu*

Main category: cs.SE

TL;DR: RovoDev Code Reviewer 是一个企业级的、基于大语言模型的代码审查自动化工具，无需微调即可生成高质量评论，有效缩短 PR 周期并减轻人工负担。


<details>
  <summary>Details</summary>
Motivation: 解决现有 LLM 代码审查工具在企业实际部署中面临的引导性、上下文感知与质量控制等挑战。

Method: 设计并部署 RovoDev Code Reviewer，无缝集成至 Atlassian Bitbucket，通过离线、在线及用户反馈评估其效果。

Result: 38.7% 的评论引发代码修改；PR 周期缩短 30.8%；人工评论减少 35.6%；提升软件质量并提供可操作建议。

Conclusion: RovoDev Code Reviewer 在企业环境中切实有效，能显著优化代码审查流程并提高开发效率。

Abstract: Large Language Models (LLMs)-powered code review automation has the potential to transform code review workflows. Despite the advances of LLM-powered code review comment generation approaches, several practical challenges remain for designing enterprise-grade code review automation tools. In particular, this paper aims at answering the practical question: how can we design a review-guided, context-aware, quality-checked code review comment generation without fine-tuning?
  In this paper, we present RovoDev Code Reviewer, an enterprise-grade LLM-based code review automation tool designed and deployed at scale within Atlassian's development ecosystem with seamless integration into Atlassian's Bitbucket. Through the offline, online, user feedback evaluations over a one-year period, we conclude that RovoDev Code Reviewer is (1) effective in generating code review comments that could lead to code resolution for 38.70% (i.e., comments that triggered code changes in the subsequent commits); and (2) offers the promise of accelerating feedback cycles (i.e., decreasing the PR cycle time by 30.8%), alleviating reviewer workload (i.e., reducing the number of human-written comments by 35.6%), and improving overall software quality (i.e., finding errors with actionable suggestions).

</details>


### [12] [Abductive Vibe Coding (Extended Abstract)](https://arxiv.org/abs/2601.01199)
*Logan Murphy,Aren A. Babikian,Marsha Chechik*

Main category: cs.SE

TL;DR: 提出一种框架，用于提取AI生成代码的半形式化合理性条件，以评估其适用性而非直接验证正确性。


<details>
  <summary>Details</summary>
Motivation: AI生成代码（vibe coding）缺乏可形式化验证的需求，需新方法评估其适用性。

Method: 构建框架，输出代码适用性所依赖的条件集合，而非直接证明正确性。

Result: 正在实现该框架，并探索相关研究机会。

Conclusion: 该方法为无法形式化验证的AI生成代码提供了一种实用的评估路径。

Abstract: When software artifacts are generated by AI models ("vibe coding"), human engineers assume responsibility for validating them. Ideally, this validation would be done through the creation of a formal proof of correctness. However, this is infeasible for many real-world vibe coding scenarios, especially when requirements for the AI-generated artifacts resist formalization. This extended abstract describes ongoing work towards the extraction of analyzable, semi-formal rationales for the adequacy of vibe-coded artifacts. Rather than deciding correctness directly, our framework produces a set of conditions under which the generated code can be considered adequate. We describe current efforts towards implementing our framework and anticipated research opportunities.

</details>


### [13] [Correctness isnt Efficiency: Runtime Memory Divergence in LLM-Generated Code](https://arxiv.org/abs/2601.01215)
*Prateek Rajput,Yewei Song,Abdoul Aziz Bonkoungou,Iyiola E. Olatunji,Abdoul Kader Kabore,Jacques Klein,Tegawendé F. Bissyandé*

Main category: cs.SE

TL;DR: 提出基于内存稳定性的评估框架，用于衡量大语言模型生成代码在运行时的行为差异，以降低操作风险。


<details>
  <summary>Details</summary>
Motivation: 正确通过单元测试的代码仍可能存在运行时性能和内存行为不稳定的问题，带来潜在操作风险。

Method: 引入动态均值成对距离（DMPD）和模型不稳定性评分（MIS），结合单调峰值轮廓与动态时间规整分析内存轨迹。

Result: 实验表明不同正确解法间存在显著运行时差异，且不稳定性随采样温度升高而增加，同时与代码复杂度指标相关。

Conclusion: 支持在CI/CD中采用稳定性感知的选择策略，在保持正确性的同时降低运行风险。

Abstract: Large language models (LLMs) can generate programs that pass unit tests, but passing tests does not guarantee reliable runtime behavior. We find that different correct solutions to the same task can show very different memory and performance patterns, which can lead to hidden operational risks. We present a framework to measure execution-time memory stability across multiple correct generations. At the solution level, we introduce Dynamic Mean Pairwise Distance (DMPD), which uses Dynamic Time Warping to compare the shapes of memory-usage traces after converting them into Monotonic Peak Profiles (MPPs) to reduce transient noise. Aggregating DMPD across tasks yields a model-level Model Instability Score (MIS). Experiments on BigOBench and CodeContests show substantial runtime divergence among correct solutions. Instability often increases with higher sampling temperature even when pass@1 improves. We also observe correlations between our stability measures and software engineering indicators such as cognitive and cyclomatic complexity, suggesting links between operational behavior and maintainability. Our results support stability-aware selection among passing candidates in CI/CD to reduce operational risk without sacrificing correctness. Artifacts are available.

</details>


### [14] [HD-GEN: A High-Performance Software System for Human Mobility Data Generation Based on Patterns of Life](https://arxiv.org/abs/2601.01219)
*Hossein Amiri,Joon-Seok Kim,Hamdi Kavak,Andrew Crooks,Dieter Pfoser,Carola Wenk,Andreas Züfle*

Main category: cs.SE

TL;DR: 提出一个结合真实数据与模拟灵活性的综合软件管道，用于生成大规模个体级人类移动数据集。


<details>
  <summary>Details</summary>
Motivation: 解决真实轨迹数据稀疏性和偏差问题，同时提升合成数据的真实性。

Method: 构建包含四个模块的系统：基于OpenStreetMap的地理仿真引擎、遗传算法校准模块、数据处理套件和可视化分析模块。

Result: 生成的数据集兼具真实性与可扩展性，适用于模型训练与基准测试。

Conclusion: 该管道有效弥合了真实数据与合成数据之间的差距，提升了人类移动建模的实用性与解释性。

Abstract: Understanding individual-level human mobility is critical for a wide range of applications. Real-world trajectory datasets provide valuable insights into actual movement behaviors but are often constrained by data sparsity and participant bias. Synthetic data, by contrast, offer scalability and flexibility but frequently lack realism. To address this gap, we introduce a comprehensive software pipeline for calibrating, generating, processing, and visualizing large-scale individual-level human mobility datasets that combine the realism of empirical data with the control and extensibility of Patterns-of-Life simulations. Our system consists of four integrated components. (1) a data generation engine constructs geographically grounded simulations using OpenStreetMap data to produce diverse mobility logs. (2) a genetic algorithm-based calibration module fine-tunes simulation parameters to align with real-world mobility characteristics, such as daily trip counts and radius of gyration, enabling realistic behavioral modeling. (3) a data processing suite transforms raw simulation logs into structured formats suitable for downstream applications, including model training and benchmarking. (4) a visualization module extracts key mobility patterns and insights from the processed datasets and presents them through intuitive visual analytics for improved interpretability.

</details>


### [15] [Atomizer: An LLM-based Collaborative Multi-Agent Framework for Intent-Driven Commit Untangling](https://arxiv.org/abs/2601.01233)
*Kangchen Zhu,Zhiliang Tian,Shangwen Wang,Mingyue Leng,Xiaoguang Mao*

Main category: cs.SE

TL;DR: Atomizer是一种基于多智能体协作的新型框架，用于解耦复合提交，通过语义意图推理和迭代优化显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法过度依赖结构信息且缺乏语义理解与迭代优化能力，难以有效解耦复合提交。

Method: 引入意图导向思维链（IO-CoT）策略结合LLM推断代码变更语义，并采用分组-评审双智能体循环机制实现迭代优化。

Result: 在C#和Java数据集上分别平均超越当前最优图聚类方法6.0%和5.5%，复杂提交场景优势超16%。

Conclusion: Atomizer有效解决语义缺失与单次分组缺陷，在复合提交解耦任务中表现优越。

Abstract: Composite commits, which entangle multiple unrelated concerns, are prevalent in software development and significantly hinder program comprehension and maintenance. Existing automated untangling methods, particularly state-of-the-art graph clustering-based approaches, are fundamentally limited by two issues. (1) They over-rely on structural information, failing to grasp the crucial semantic intent behind changes, and (2) they operate as ``single-pass'' algorithms, lacking a mechanism for the critical reflection and refinement inherent in human review processes. To overcome these challenges, we introduce Atomizer, a novel collaborative multi-agent framework for composite commit untangling. To address the semantic deficit, Atomizer employs an Intent-Oriented Chain-of-Thought (IO-CoT) strategy, which prompts large language models (LLMs) to infer the intent of each code change according to both the structure and the semantic information of code. To overcome the limitations of ``single-pass'' grouping, we employ two agents to establish a grouper-reviewer collaborative refinement loop, which mirrors human review practices by iteratively refining groupings until all changes in a cluster share the same underlying semantic intent. Extensive experiments on two benchmark C# and Java datasets demonstrate that Atomizer significantly outperforms several representative baselines. On average, it surpasses the state-of-the-art graph-based methods by over 6.0% on the C# dataset and 5.5% on the Java dataset. This superiority is particularly pronounced on complex commits, where Atomizer's performance advantage widens to over 16%.

</details>


### [16] [CatchAll: Repository-Aware Exception Handling with Knowledge-Guided LLMs](https://arxiv.org/abs/2601.01271)
*Qingxiao Tao,Xiaodong Gu,Hao Zhong,Beijun Shen*

Main category: cs.SE

TL;DR: CatchAll是一种基于LLM的仓库级异常处理方法，通过API、仓库上下文和跨仓库模式三层知识提升代码生成质量。


<details>
  <summary>Details</summary>
Motivation: 现有LLM在仓库级异常处理中表现不佳，常因复杂依赖与上下文导致系统崩溃或资源泄漏。

Method: 构建API-异常映射、调用链上下文建模与跨项目处理模式挖掘，并编码为结构化提示引导LLM生成。

Result: 在RepoExEval基准上，CodeBLEU达0.31，意图预测准确率60.1%，Pass@1达29%，优于现有基线。

Conclusion: CatchAll有效提升了LLM在真实仓库环境中生成准确、上下文感知异常处理代码的能力。

Abstract: Exception handling is a vital forward error-recovery mechanism in many programming languages, enabling developers to manage runtime anomalies through structured constructs (e.g., try-catch blocks). Improper or missing exception handling often leads to severe consequences, including system crashes and resource leaks. While large language models (LLMs) have demonstrated strong capabilities in code generation, they struggle with exception handling at the repository level, due to complex dependencies and contextual constraints. In this work, we propose CatchAll, a novel LLM-based approach for repository-aware exception handling. CatchAll equips LLMs with three complementary layers of exception-handling knowledge: (1) API-level exception knowledge, obtained from an empirically constructed API-exception mapping that characterizes the exception-throwing behaviors of APIs in real-world codebases; (2) repository-level execution context, which captures exception propagation by modeling contextual call traces around the target code; and (3) cross-repository handling knowledge, distilled from reusable exception-handling patterns mined from historical code across projects. The knowledge is encoded into structured prompts to guide the LLM in generating accurate and context-aware exception-handling code. To evaluate CatchAll, we construct two new benchmarks for repository-aware exception handling: a large-scale dataset RepoExEval and an executable subset RepoExEval-Exec. Experiments demonstrate that RepoExEval consistently outperforms state-of-the-art baselines, achieving a CodeBLEU score of 0.31 (vs. 0.27% for the best baseline), intent prediction accuracy of 60.1% (vs. 48.0%), and Pass@1 of 29% (vs. 25%). These results affirm RepoExEval's effectiveness in real-world repository-level exception handling.

</details>


### [17] [Adaptive Hierarchical Evaluation of LLMs and SAST tools for CWE Prediction in Python](https://arxiv.org/abs/2601.01320)
*Muntasir Adnan,Carlos C. N. Kuhn*

Main category: cs.SE

TL;DR: ALPHA是一个新的Python函数级基准，通过层次化CWE特定惩罚评估LLM和SAST工具的漏洞检测能力。


<details>
  <summary>Details</summary>
Motivation: 现有二分类漏洞检测基准缺乏CWE级别细粒度，无法支持迭代修正系统。

Method: 提出ALPHA基准，区分过度泛化、过度具体化和横向错误，采用分层CWE惩罚机制评估模型。

Result: LLM整体优于SAST工具，但SAST在检出时精度更高；模型预测一致性差异显著（8.26%-81.87%）。

Conclusion: ALPHA为层次化漏洞检测提供新框架，未来可结合其惩罚机制进行监督微调以提升检测性能。

Abstract: Large Language Models have become integral to software development, yet they frequently generate vulnerable code. Existing code vulnerability detection benchmarks employ binary classification, lacking the CWE-level specificity required for actionable feedback in iterative correction systems. We present ALPHA (Adaptive Learning via Penalty in Hierarchical Assessment), the first function-level Python benchmark that evaluates both LLMs and SAST tools using hierarchically aware, CWE-specific penalties. ALPHA distinguishes between over-generalisation, over-specification, and lateral errors, reflecting practical differences in diagnostic utility. Evaluating seven LLMs and two SAST tools, we find LLMs substantially outperform SAST, though SAST demonstrates higher precision when detections occur. Critically, prediction consistency varies dramatically across models (8.26%-81.87% agreement), with significant implications for feedback-driven systems. We further outline a pathway for future work incorporating ALPHA penalties into supervised fine-tuning, which could provide principled hierarchy-aware vulnerability detection pending empirical validation.

</details>


### [18] [GlycoPy: An Equation-Oriented and Object-Oriented Software for Hierarchical Modeling, Optimization, and Control in Python](https://arxiv.org/abs/2601.01413)
*Yingjie Ma,Jing Guo,Richard D. Braatz*

Main category: cs.SE

TL;DR: GlycoPy是一个面向方程和对象的Python框架，支持非线性模型预测控制在复杂化工过程中的建模、优化与应用。


<details>
  <summary>Details</summary>
Motivation: 现有工业MPC多依赖线性模型，限制了其在非线性、宽工况过程中的性能与适用性。

Method: 开发GlycoPy框架，支持分层建模、参数估计、动态优化与NMPC，并允许用户自定义算法。

Result: 通过三个案例验证了GlycoPy在建模、优化与控制方面的有效性，涵盖从简单微分代数系统到多尺度生物过程。

Conclusion: GlycoPy有望弥合先进NMPC算法与实际（生化）工业过程应用之间的鸿沟。

Abstract: Most existing model predictive control (MPC) applications in process industries employ lin-ear models, although real-world (bio)chemical processes are typically nonlinear. The use of linear models limits the performance and applicability of MPC for processes that span a wide range of operating conditions. A challenge in employing nonlinear models in MPC for com-plex systems is the lack of tools that facilitate hierarchical model development, as well as lack of efficient implementations of the corresponding nonlinear MPC (NMPC) algorithms. As a step towards making NMPC more practical for hierarchical systems, we introduce Gly-coPy, an equation-oriented, object-oriented software framework for process modeling, opti-mization, and NMPC in Python. GlycoPy enables users to focus on writing equations for modeling while supporting hierarchical modeling. GlycoPy includes algorithms for parame-ter estimation, dynamic optimization, and NMPC, and allows users to customize the simula-tion, optimization, and control algorithms. Three case studies, ranging from a simple differ-ential algebraic equation system to a multiscale bioprocess model, validate the modeling, optimization, and NMPC capabilities of GlycoPy. GlycoPy has the potential to bridge the gap between advanced NMPC algorithms and their practical application in real-world (bio)chemical processes.

</details>


### [19] [SWE-Lego: Pushing the Limits of Supervised Fine-tuning for Software Issue Resolving](https://arxiv.org/abs/2601.01426)
*Chaofan Tao,Jierun Chen,Yuxin Jiang,Kaiqi Kou,Shaowei Wang,Ruoyu Wang,Xiaohui Li,Sidi Yang,Yiming Du,Jianbo Dai,Zhiming Mao,Xinyu Wang,Lifeng Shang,Haoli Bai*

Main category: cs.SE

TL;DR: SWE-Lego 是一种仅依赖监督微调的轻量级方法，在软件工程问题解决任务中达到开源模型的最先进性能。


<details>
  <summary>Details</summary>
Motivation: 探索如何通过简化训练范式，仅用监督微调实现高性能的软件工程任务解决能力。

Method: 构建包含高质量数据集、错误掩蔽与难度课程的精炼SFT流程，并结合测试时扩展策略提升性能。

Result: SWE-Lego-Qwen3-8B 和 32B 在 SWE-bench Verified 上分别达到 42.2% 和 52.6%，经 TTS@16 提升至 49.6% 和 58.8%。

Conclusion: 仅用监督微调配合精心设计的数据与训练策略，即可在SWE任务中实现媲美复杂训练范式的效果。

Abstract: We present SWE-Lego, a supervised fine-tuning (SFT) recipe designed to achieve state-ofthe-art performance in software engineering (SWE) issue resolving. In contrast to prevalent methods that rely on complex training paradigms (e.g., mid-training, SFT, reinforcement learning, and their combinations), we explore how to push the limits of a lightweight SFT-only approach for SWE tasks. SWE-Lego comprises three core building blocks, with key findings summarized as follows: 1) the SWE-Lego dataset, a collection of 32k highquality task instances and 18k validated trajectories, combining real and synthetic data to complement each other in both quality and quantity; 2) a refined SFT procedure with error masking and a difficulty-based curriculum, which demonstrably improves action quality and overall performance. Empirical results show that with these two building bricks alone,the SFT can push SWE-Lego models to state-of-the-art performance among open-source models of comparable size on SWE-bench Verified: SWE-Lego-Qwen3-8B reaches 42.2%, and SWE-Lego-Qwen3-32B attains 52.6%. 3) We further evaluate and improve test-time scaling (TTS) built upon the SFT foundation. Based on a well-trained verifier, SWE-Lego models can be significantly boosted--for example, 42.2% to 49.6% and 52.6% to 58.8% under TTS@16 for the 8B and 32B models, respectively.

</details>


### [20] [Group versus Individual Review Requests: Tradeoffs in Speed and Quality at Mozilla Firefox](https://arxiv.org/abs/2601.01514)
*Matej Kucera,Marco Castelluccio,Daniel Feitosa,Ayushi Rastogi*

Main category: cs.SE

TL;DR: 研究探讨了群体评审与个人评审对代码审查速度和质量的影响，发现群体评审能提升质量但对速度影响不大。


<details>
  <summary>Details</summary>
Motivation: 明确群体评审在代码审查中的作用，以优化开发流程和提升开发者满意度。

Method: 结合统计建模与焦点小组讨论，分析Mozilla Firefox项目中约66,000次修订数据。

Result: 群体评审与更少的回归问题相关，表明质量提升；但对审查速度无显著影响。

Conclusion: 群体评审有助于平衡工作分配和培训新评审者，是提高代码质量的有效策略。

Abstract: The speed at which code changes are integrated into the software codebase, also referred to as code review velocity, is a prevalent industry metric for improved throughput and developer satisfaction. While prior studies have explored factors influencing review velocity, the role of the review assignment process, particularly the `group review request', is unclear. In group review requests, available on platforms like Phabricator, GitHub, and Bitbucket, a code change is assigned to a reviewer group, allowing any member to review it, unlike individual review assignments to specific reviewers. Drawing parallels with shared task queues in Management Sciences, this study examines the effects of group versus individual review requests on velocity and quality. We investigate approximately 66,000 revisions in the Mozilla Firefox project, combining statistical modeling with practitioner views from a focus group discussion. Our study associates group reviews with improved review quality, characterized by fewer regressions, while having a negligible association with review velocity. Additional perceived benefits include balanced work distribution and training opportunities for new reviewers.

</details>


### [21] [LIA: Supervised Fine-Tuning of Large Language Models for Automatic Issue Assignment](https://arxiv.org/abs/2601.01780)
*Arsham Khosravani,Alireza Hosseinpour,Arshia Akhavan,Mehdi Keshani,Abbas Heydarnoori*

Main category: cs.SE

TL;DR: LIA通过微调LLM实现自动问题分配，显著提升准确率。


<details>
  <summary>Details</summary>
Motivation: 手动问题分配效率低且易出错，现有自动化方法依赖大量项目数据或稀疏关系信息，效果受限。

Method: 基于DeepSeek-R1-Distill-Llama-8B进行监督微调，利用LLM语义理解能力，从标题和描述直接生成开发者推荐排序。

Result: 相比基模型Hit@1提升187.8%，超越四个SOTA方法最高达211.2%。

Conclusion: 领域适配的LLM在软件维护任务中高效实用，LIA是高性能的问题分配解决方案。

Abstract: Issue assignment is a critical process in software maintenance, where new issue reports are validated and assigned to suitable developers. However, manual issue assignment is often inconsistent and error-prone, especially in large open-source projects where thousands of new issues are reported monthly. Existing automated approaches have shown promise, but many rely heavily on large volumes of project-specific training data or relational information that is often sparse and noisy, which limits their effectiveness. To address these challenges, we propose LIA (LLM-based Issue Assignment), which employs supervised fine-tuning to adapt an LLM, DeepSeek-R1-Distill-Llama-8B in this work, for automatic issue assignment. By leveraging the LLM's pretrained semantic understanding of natural language and software-related text, LIA learns to generate ranked developer recommendations directly from issue titles and descriptions. The ranking is based on the model's learned understanding of historical issue-to-developer assignments, using patterns from past tasks to infer which developers are most likely to handle new issues. Through comprehensive evaluation, we show that LIA delivers substantial improvements over both its base pretrained model and state-of-the-art baselines. It achieves up to +187.8% higher Hit@1 compared to the DeepSeek-R1-Distill-Llama-8B pretrained base model, and outperforms four leading issue assignment methods by as much as +211.2% in Hit@1 score. These results highlight the effectiveness of domain-adapted LLMs for software maintenance tasks and establish LIA as a practical, high-performing solution for issue assignment.

</details>


### [22] [The Machine Learning Canvas: Empirical Findings on Why Strategy Matters More Than AI Code Generation](https://arxiv.org/abs/2601.01839)
*Martin Prause*

Main category: cs.SE

TL;DR: 研究提出机器学习画布框架，揭示项目成功的四大关键因素：战略、流程、生态系统与支持，并指出AI编码助手虽提升效率，但无法替代战略思考。


<details>
  <summary>Details</summary>
Motivation: 解决80%以上机器学习项目未能实现商业价值的问题，探索成功的关键驱动因素。

Method: 通过调研150名数据科学家并使用统计建模分析其反馈，构建并验证机器学习画布框架。

Result: 发现四大成功因素相互关联，组织支持显著影响战略制定（β=0.432），进而优化流程（β=0.428）和基础设施（β=0.547）。

Conclusion: AI编码助手仅辅助技术实现，项目成功仍依赖清晰的战略规划与组织协同。

Abstract: Despite the growing popularity of AI coding assistants, over 80% of machine learning (ML) projects fail to deliver real business value. This study creates and tests a Machine Learning Canvas, a practical framework that combines business strategy, software engineering, and data science in order to determine the factors that lead to the success of ML projects. We surveyed 150 data scientists and analyzed their responses using statistical modeling. We identified four key success factors: Strategy (clear goals and planning), Process (how work gets done), Ecosystem (tools and infrastructure), and Support (organizational backing and resources). Our results show that these factors are interconnected - each one affects the next. For instance, strong organizational support results in a clearer strategy (β= 0.432, p < 0.001), which improves work processes (β= 0.428, p < 0.001) and builds better infrastructure (β= 0.547, p < 0.001). Together, these elements determine whether a project succeeds. The surprising finding? Although AI assistants make coding faster, they don't guarantee project success. AI assists with the "how" of coding but cannot replace the "why" and "what" of strategic thinking.

</details>


### [23] [A Defect is Being Born: How Close Are We? A Time Sensitive Forecasting Approach](https://arxiv.org/abs/2601.01921)
*Mikel Robredo,Matteo Esposito,Fabio Palomba,Rafael Peñaloza,Valentina Lenarduzzi*

Main category: cs.SE

TL;DR: 本研究旨在探索时间敏感技术在缺陷预测中的有效性，并识别缺陷发生前的早期指标。


<details>
  <summary>Details</summary>
Motivation: 随着软件系统的持续演进，亟需能够提前预测缺陷的时间敏感方法。

Method: 通过训练多种时间敏感预测技术，预测软件项目的未来缺陷密度并识别缺陷前兆。

Result: 预期结果将提供关于该方法在早期缺陷倾向估计中有效性的实证证据。

Conclusion: 研究有望为缺陷预测领域提供更高效的时间敏感解决方案。

Abstract: Background. Defect prediction has been a highly active topic among researchers in the Empirical Software Engineering field. Previous literature has successfully achieved the most accurate prediction of an incoming fault and identified the features and anomalies that precede it through just-in-time prediction. As software systems evolve continuously, there is a growing need for time-sensitive methods capable of forecasting defects before they manifest.
  Aim. Our study seeks to explore the effectiveness of time-sensitive techniques for defect forecasting. Moreover, we aim to investigate the early indicators that precede the occurrence of a defect.
  Method. We will train multiple time-sensitive forecasting techniques to forecast the future bug density of a software project, as well as identify the early symptoms preceding the occurrence of a defect.
  Expected results. Our expected results are translated into empirical evidence on the effectiveness of our approach for early estimation of bug proneness.

</details>


### [24] [The Invisible Hand of AI Libraries Shaping Open Source Projects and Communities](https://arxiv.org/abs/2601.01944)
*Matteo Esposito,Andrea Janes,Valentina Lenarduzzi,Davide Taibi*

Main category: cs.SE

TL;DR: 本研究旨在分析AI库在Python和Java开源项目中的采用情况及其对开发活动、社区参与和代码复杂性的影响。


<details>
  <summary>Details</summary>
Motivation: 尽管AI在开源软件中的重要性日益增加，但其对开源项目的影响尚未充分探索。

Method: 通过对157.7k个潜在开源仓库进行大规模分析，比较采用与未采用AI库项目的各项指标。

Result: 预期发现采用AI库的项目在开发活跃度、社区参与度和代码复杂性方面存在可测量差异。

Conclusion: AI集成正在重塑开源软件开发实践，提供基于证据的洞见。

Abstract: In the early 1980s, Open Source Software emerged as a revolutionary concept amidst the dominance of proprietary software. What began as a revolutionary idea has now become the cornerstone of computer science. Amidst OSS projects, AI is increasing its presence and relevance. However, despite the growing popularity of AI, its adoption and impacts on OSS projects remain underexplored.
  We aim to assess the adoption of AI libraries in Python and Java OSS projects and examine how they shape development, including the technical ecosystem and community engagement. To this end, we will perform a large-scale analysis on 157.7k potential OSS repositories, employing repository metrics and software metrics to compare projects adopting AI libraries against those that do not. We expect to identify measurable differences in development activity, community engagement, and code complexity between OSS projects that adopt AI libraries and those that do not, offering evidence-based insights into how AI integration reshapes software development practices.

</details>


### [25] [Context-Adaptive Requirements Defect Prediction through Human-LLM Collaboration](https://arxiv.org/abs/2601.01952)
*Max Unterbusch,Andreas Vogelsang*

Main category: cs.SE

TL;DR: 本文提出一种人与大语言模型协作（HLC）方法，通过链式思维与用户反馈实现需求缺陷预测的自适应学习，在少量标注样本下显著优于传统模型。


<details>
  <summary>Details</summary>
Motivation: 传统需求缺陷评估依赖通用模式，忽视项目和语境差异，难以适应不同场景下的“缺陷”定义。

Method: 采用HLC框架，结合LLM链式思维推理与用户验证反馈，通过少样本学习动态调整预测模型。

Result: 在QuRE基准测试中，仅需20个验证样本即可快速提升性能，且结合解释的预测显著优于标准少样本提示与微调BERT模型，同时保持高召回率。

Conclusion: LLM的上下文与链式思维能力支持构建可随用户反馈持续进化的自适应分类工具，突破传统静态模型局限。

Abstract: Automated requirements assessment traditionally relies on universal patterns as proxies for defectiveness, implemented through rule-based heuristics or machine learning classifiers trained on large annotated datasets. However, what constitutes a "defect" is inherently context-dependent and varies across projects, domains, and stakeholder interpretations. In this paper, we propose a Human-LLM Collaboration (HLC) approach that treats defect prediction as an adaptive process rather than a static classification task. HLC leverages LLM Chain-of-Thought reasoning in a feedback loop: users validate predictions alongside their explanations, and these validated examples adaptively guide future predictions through few-shot learning. We evaluate this approach using the weak word smell on the QuRE benchmark of 1,266 annotated Mercedes-Benz requirements. Our results show that HLC effectively adapts to the provision of validated examples, with rapid performance gains from as few as 20 validated examples. Incorporating validated explanations, not just labels, enables HLC to substantially outperform both standard few-shot prompting and fine-tuned BERT models while maintaining high recall. These results highlight how the in-context and Chain-of-Thought learning capabilities of LLMs enable adaptive classification approaches that move beyond one-size-fits-all models, creating opportunities for tools that learn continuously from stakeholder feedback.

</details>


### [26] [Reporting LLM Prompting in Automated Software Engineering: A Guideline Based on Current Practices and Expectations](https://arxiv.org/abs/2601.01954)
*Alexander Korn,Lea Zaruchas,Chetan Arora,Andreas Metzger,Sven Smolka,Fanyu Wang,Andreas Vogelsang*

Main category: cs.SE

TL;DR: 本文研究了大型语言模型在软件工程任务中的提示设计报告现状，提出了一套结构化指南以提升透明度和可复现性。


<details>
  <summary>Details</summary>
Motivation: 当前软件工程领域对提示设计的报告缺乏系统性和透明度，影响研究的可复现性和可比性。

Method: 通过分析近300篇顶会论文并调查105位程序委员会成员，归纳出提示报告的关键要素。

Result: 发现当前实践与审稿人期望存在显著差距，特别是在版本披露、提示合理性及有效性威胁方面。

Conclusion: 提出的结构化指南有助于提升基于LLM的软件工程研究的透明度、可复现性和方法严谨性。

Abstract: Large Language Models, particularly decoder-only generative models such as GPT, are increasingly used to automate Software Engineering tasks. These models are primarily guided through natural language prompts, making prompt engineering a critical factor in system performance and behavior. Despite their growing role in SE research, prompt-related decisions are rarely documented in a systematic or transparent manner, hindering reproducibility and comparability across studies. To address this gap, we conducted a two-phase empirical study. First, we analyzed nearly 300 papers published at the top-3 SE conferences since 2022 to assess how prompt design, testing, and optimization are currently reported. Second, we surveyed 105 program committee members from these conferences to capture their expectations for prompt reporting in LLM-driven research. Based on the findings, we derived a structured guideline that distinguishes essential, desirable, and exceptional reporting elements. Our results reveal significant misalignment between current practices and reviewer expectations, particularly regarding version disclosure, prompt justification, and threats to validity. We present our guideline as a step toward improving transparency, reproducibility, and methodological rigor in LLM-based SE research.

</details>


### [27] [The State of Open Science in Software Engineering Research: A Case Study of ICSE Artifacts](https://arxiv.org/abs/2601.02066)
*Al Muttakin,Saikat Mondal,Chanchal Roy*

Main category: cs.SE

TL;DR: 该研究评估了2015-2024年间ICSE会议的100个复制包，发现仅40%可执行、35%能复现结果，揭示了软件工程研究中复制包可用性与实用性之间的显著差距，并提出三项改进指南。


<details>
  <summary>Details</summary>
Motivation: 当前软件工程研究虽普遍共享复制包，但其实际可执行性与可复现性缺乏系统评估，阻碍了科研透明与重用。

Method: 人工投入约650小时，对100个ICSE复制包进行执行测试，记录所需修改、失败原因及结果复现情况，归纳挑战类型并提出改进建议。

Result: 仅40%复制包可执行（其中32.5%无需修改），82.5%需中高程度修改；仅35%成功复现原结果；识别出5类常见修改和13项执行障碍。

Conclusion: 复制包的可用性不等于可执行性或可复现性，亟需通过标准化文档、环境封装和审稿流程优化提升开放科学实践质量。

Abstract: Replication packages are crucial for enabling transparency, validation, and reuse in software engineering (SE) research. While artifact sharing is now a standard practice and even expected at premier SE venues such as ICSE, the practical usability of these replication packages remains underexplored. In particular, there is a marked lack of studies that comprehensively examine the executability and reproducibility of replication packages in SE research. In this paper, we aim to fill this gap by evaluating 100 replication packages published as part of ICSE proceedings over the past decade (2015--2024). We assess the (1) executability of the replication packages, (2) efforts and modifications required to execute them, (3) challenges that prevent executability, and (4) reproducibility of the original findings. We spent approximately 650 person-hours in total executing the artifacts and reproducing the study findings. Our findings reveal that only 40\% of the 100 evaluated artifacts were executable, of which 32.5\% (13 out of 40) ran without any modification. Regarding effort levels, 17.5\% (7 out of 40) required low effort, while 82.5\% (33 out of 40) required moderate to high effort to execute successfully. We identified five common types of modifications and 13 challenges leading to execution failure, spanning environmental, documentation, and structural issues. Among the executable artifacts, only 35\% (14 out of 40) reproduced the original results. These findings highlight a notable gap between artifact availability, executability, and reproducibility. Our study proposes three actionable guidelines to improve the preparation, documentation, and review of research artifacts, thereby strengthening the rigor and sustainability of open science practices in SE research.

</details>


### [28] [LLM-Empowered Functional Safety and Security by Design in Automotive Systems](https://arxiv.org/abs/2601.02215)
*Nenad Petrovic,Vahid Zolfaghari,Fengjunjie Pan,Alois Knoll*

Main category: cs.SE

TL;DR: 本文提出了一种基于大语言模型的工作流，用于支持软件定义汽车的开发，涵盖安全感知拓扑设计与事件驱动代码分析。


<details>
  <summary>Details</summary>
Motivation: 提升软件定义汽车在功能安全与信息安全方面的系统化验证能力。

Method: 采用事件链模型进行代码语义分析，结合MDE与OCL规则进行拓扑安全分析，评估本地部署与专有方案在ADAS场景中的适用性。

Result: 实现了对CAN与VSS消息语义的有效验证，并通过MDE/OCL增强了拓扑安全性分析能力。

Conclusion: 该工作流能有效支持SDV开发中安全与功能正确性的协同保障。

Abstract: This paper presents LLM-empowered workflow to support Software Defined Vehicle (SDV) software development, covering the aspects of security-aware system topology design, as well as event-driven decision-making code analysis. For code analysis we adopt event chains model which provides formal foundations to systematic validation of functional safety, taking into account the semantic validity of messages exchanged between key components, including both CAN and Vehicle Signal Specification (VSS). Analysis of security aspects for topology relies on synergy with Model-Driven Engineering (MDE) approach and Object Constraint Language (OCL) rules. Both locally deployable and proprietary solution are taken into account for evaluation within Advanced Driver-Assistance Systems (ADAS)-related scenarios.

</details>


### [29] [NQC2: A Non-Intrusive QEMU Code Coverage Plugin](https://arxiv.org/abs/2601.02238)
*Nils Bosbach,Alwalid Salama,Lukas Jünger,Mark Burton,Niko Zurstraßen,Rebecca Pelke,Rainer Leupers*

Main category: cs.SE

TL;DR: NQC2是一个用于嵌入式系统代码覆盖率分析的QEMU插件，无需目标软件插桩，性能优于Xilinx方案8.5倍。


<details>
  <summary>Details</summary>
Motivation: 传统代码覆盖率工具依赖操作系统和文件系统，无法适用于裸机嵌入式程序。

Method: 通过QEMU运行时提取覆盖率信息并存储于宿主机，避免对目标软件进行插桩。

Result: NQC2兼容修改版QEMU，性能比Xilinx方案最高提升8.5倍。

Conclusion: NQC2为嵌入式系统提供了一种高效、无侵入的代码覆盖率分析解决方案。

Abstract: Code coverage analysis has become a standard approach in software development, facilitating the assessment of test suite effectiveness, the identification of under-tested code segments, and the discovery of performance bottlenecks. When code coverage of software for embedded systems needs to be measured, conventional approaches quickly meet their limits. A commonly used approach involves instrumenting the source files with added code that collects and dumps coverage information during runtime. This inserted code usually relies on the existence of an operating and a file system to dump the collected data. These features are not available for bare-metal programs that are executed on embedded systems.
  To overcome this issue, we present NQC2, a plugin for QEMU.NQC2 extracts coverage information from QEMU during runtime and stores them into a file on the host machine. This approach is even compatible with modified QEMU versions and does not require target-software instrumentation. NQC2 outperforms a comparable approach from Xilinx by up to 8.5 x.

</details>


### [30] [Automatic Assertion Mining in Assertion-Based Verification: Techniques, Challenges, and Future Directions](https://arxiv.org/abs/2601.02248)
*Mohammad Reza Heidari Iman,Giorgio Di Natale,Katell Morin-Allory*

Main category: cs.SE

TL;DR: 本文综述了当前主流的自动断言挖掘技术，比较其方法优劣，为未来更强大的断言挖掘器发展提供方向。


<details>
  <summary>Details</summary>
Motivation: 功能验证日益依赖基于断言的验证，而自动断言挖掘器是其核心，需系统评估现有技术以推动进步。

Method: 对近期先进且广泛应用的断言挖掘器进行回顾与方法学对比分析。

Result: 揭示了现有断言挖掘器的能力与局限，为研究人员和从业者提供实用洞察。

Conclusion: 指出现有技术不足，并提出未来开发更强大断言挖掘器的研究方向。

Abstract: Functional verification increasingly relies on Assertion-Based Verification (ABV), which has become a key approach for verifying hardware designs due to its efficiency and effectiveness. Central to ABV are automatic assertion miners, which apply different techniques to generate assertions automatically. This paper reviews the most recent, advanced, and widely adopted assertion miners, offering a comparative analysis of their methodologies. The goal is to provide researchers and verification practitioners with insights into the capabilities and limitations of existing miners. By identifying their shortcomings, this work also points toward directions for developing more powerful and advanced assertion miners in the future.

</details>


### [31] [Question Answering for Multi-Release Systems: A Case Study at Ciena](https://arxiv.org/abs/2601.02345)
*Parham Khamsepour,Mark Cole,Ish Ashraf,Sandeep Puri,Mehrdad Sabetzadeh,Shiva Nejati*

Main category: cs.SE

TL;DR: QAMR是一个专为多版本系统文档设计的问答聊天机器人，通过改进检索增强生成技术提升准确率。


<details>
  <summary>Details</summary>
Motivation: 现有问答技术在处理多版本系统文档时准确性不足，需针对性优化。

Method: 结合预处理、查询重写、上下文选择及双分块策略，优化检索与生成过程。

Result: 在基准测试和实际数据中，QAMR显著提升答案正确率（+16.5%）和检索准确率（+12%），并缩短响应时间8%。

Conclusion: QAMR有效解决多版本文档问答难题，其评估指标与人工评估高度一致，方法可靠。

Abstract: Companies regularly have to contend with multi-release systems, where several versions of the same software are in operation simultaneously. Question answering over documents from multi-release systems poses challenges because different releases have distinct yet overlapping documentation. Motivated by the observed inaccuracy of state-of-the-art question-answering techniques on multi-release system documents, we propose QAMR, a chatbot designed to answer questions across multi-release system documentation. QAMR enhances traditional retrieval-augmented generation (RAG) to ensure accuracy in the face of highly similar yet distinct documentation for different releases. It achieves this through a novel combination of pre-processing, query rewriting, and context selection. In addition, QAMR employs a dual-chunking strategy to enable separately tuned chunk sizes for retrieval and answer generation, improving overall question-answering accuracy. We evaluate QAMR using a public software-engineering benchmark as well as a collection of real-world, multi-release system documents from our industry partner, Ciena. Our evaluation yields five main findings: (1) QAMR outperforms a baseline RAG-based chatbot, achieving an average answer correctness of 88.5% and an average retrieval accuracy of 90%, which correspond to improvements of 16.5% and 12%, respectively. (2) An ablation study shows that QAMR's mechanisms for handling multi-release documents directly improve answer accuracy. (3) Compared to its component-ablated variants, QAMR achieves a 19.6% average gain in answer correctness and a 14.0% average gain in retrieval accuracy over the best ablation. (4) QAMR reduces response time by 8% on average relative to the baseline. (5) The automatically computed accuracy metrics used in our evaluation strongly correlate with expert human assessments, validating the reliability of our methodology.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [32] [CONSENT: A Negotiation Framework for Leveraging User Flexibility in Vehicle-to-Building Charging under Uncertainty](https://arxiv.org/abs/2601.01581)
*Rishav Sen,Fangqi Liu,Jose Paolo Talusan,Ava Pettet,Yoshinori Suzue,Mark Bailey,Ayan Mukhopadhyay,Abhishek Dubey*

Main category: cs.MA

TL;DR: 提出一种基于协商的电动汽车充电框架，实现建筑运营商与用户双赢。


<details>
  <summary>Details</summary>
Motivation: 解决建筑运营商高能耗成本与用户充电便利性之间的冲突。

Method: 设计保证自愿参与、防策略性和预算可行性的协商机制，结合用户调研与真实数据验证。

Result: 建筑运营成本降低超3.5%，用户充电费用减少22%。

Conclusion: 该框架将EV充电从运营摩擦源转变为协作与共享节约平台。

Abstract: The growth of Electric Vehicles (EVs) creates a conflict in vehicle-to-building (V2B) settings between building operators, who face high energy costs from uncoordinated charging, and drivers, who prioritize convenience and a full charge. To resolve this, we propose a negotiation-based framework that, by design, guarantees voluntary participation, strategy-proofness, and budget feasibility. It transforms EV charging into a strategic resource by offering drivers a range of incentive-backed options for modest flexibility in their departure time or requested state of charge (SoC). Our framework is calibrated with user survey data and validated using real operational data from a commercial building and an EV manufacturer. Simulations show that our negotiation protocol creates a mutually beneficial outcome: lowering the building operator's costs by over 3.5\% compared to an optimized, non-negotiating smart charging policy, while simultaneously reducing user charging expenses by 22\% below the utility's retail energy rate. By aligning operator and EV user objectives, our framework provides a strategic bridge between energy and mobility systems, transforming EV charging from a source of operational friction into a platform for collaboration and shared savings.

</details>


### [33] [ARIES: A Scalable Multi-Agent Orchestration Framework for Real-Time Epidemiological Surveillance and Outbreak Monitoring](https://arxiv.org/abs/2601.01831)
*Aniket Wattamwar,Sampson Akwafuo*

Main category: cs.MA

TL;DR: ARIES是一个专为流行病学监测设计的多智能体框架，能动态整合WHO、CDC及研究数据，实现实时威胁识别。


<details>
  <summary>Details</summary>
Motivation: 通用AI在高风险流行病领域存在幻觉和数据孤岛问题，需专用系统提升监测能力。

Method: 构建分层指挥结构，利用GPT协调子智能体自动查询权威数据源并合成逻辑推理。

Result: ARIES在近实时识别新兴威胁和信号差异方面表现优异，超越通用模型。

Conclusion: 任务专用的智能体群架构可成为下一代疫情响应和全球健康情报的可靠扩展方案。

Abstract: Global health surveillance is currently facing a challenge of Knowledge Gaps. While general-purpose AI has proliferated, it remains fundamentally unsuited for the high-stakes epidemiological domain due to chronic hallucinations and an inability to navigate specialized data silos. This paper introduces ARIES (Agentic Retrieval Intelligence for Epidemiological Surveillance), a specialized, autonomous multi-agent framework designed to move beyond static, disease-specific dashboards toward a dynamic intelligence ecosystem. Built on a hierarchical command structure, ARIES utilizes GPTs to orchestrate a scalable swarm of sub-agents capable of autonomously querying World Health Organization (WHO), Center for Disease Control and Prevention (CDC), and peer-reviewed research papers. By automating the extraction and logical synthesis of surveillance data, ARIES provides a specialized reasoning that identifies emergent threats and signal divergence in near real-time. This modular architecture proves that a task-specific agentic swarm can outperform generic models, offering a robust, extensible for next-generation outbreak response and global health intelligence.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [34] [CounterPoint: Using Hardware Event Counters to Refute and Refine Microarchitectural Assumptions (Extended Version)](https://arxiv.org/abs/2601.01265)
*Nick Lindsay,Caroline Trippel,Anurag Khandelwal,Abhishek Bhattacharjee*

Main category: cs.AR

TL;DR: CounterPoint是一个用于验证微架构模型与性能计数器数据一致性的框架，能揭示隐藏的硬件特性。


<details>
  <summary>Details</summary>
Motivation: 硬件事件计数器因规格模糊、设计不透明和复用噪声导致数据难以解释。

Method: 使用μ路径决策图表达微架构模型，并通过多维置信区域缓解复用噪声以检测不匹配。

Result: 成功应用于Haswell内存管理单元，发现多个未文档化行为，如TLB预取器和可中止页表遍历。

Conclusion: CounterPoint帮助专家将噪声计数器数据与微架构心智模型对齐，揭示细微隐藏硬件特性。

Abstract: Hardware event counters offer the potential to reveal not only performance bottlenecks but also detailed microarchitectural behavior. In practice, this promise is undermined by their vague specifications, opaque designs, and multiplexing noise, making event counter data hard to interpret.
  We introduce CounterPoint, a framework that tests user-specified microarchitectural models - expressed as $μ$path Decision Diagrams - for consistency with performance counter data. When mismatches occur, CounterPoint pinpoints plausible microarchitectural features that could explain them, using multi-dimensional counter confidence regions to mitigate multiplexing noise. We apply CounterPoint to the Haswell Memory Management Unit as a case study, shedding light on multiple undocumented and underdocumented microarchitectural behaviors. These include a load-store queue-side TLB prefetcher, merging page table walkers, abortable page table walks, and more.
  Overall, CounterPoint helps experts reconcile noisy hardware performance counter measurements with their mental model of the microarchitecture - uncovering subtle, previously hidden hardware features along the way.

</details>


### [35] [Ageing Monitoring for Commercial Microcontrollers Based on Timing Windows](https://arxiv.org/abs/2601.02053)
*Leandro Lanzieri,Jiri Kral,Goerschwin Fey,Holger Schlarb,Thomas C. Schmidt*

Main category: cs.AR

TL;DR: 本文提出一种基于软件的自测试方法，用于监测微控制器硬件老化，通过可变长度的时间窗口确定设备最高工作频率，并在真实硬件上验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 微控制器在嵌入式和高可靠性应用中广泛使用，但缺乏可行的老化监测技术，导致依赖静态保护带，限制性能并可能引发突发故障。

Method: 采用软件自测试方法，利用可变长度时间窗口测量设备最大工作频率，以监测硬件退化情况。

Result: 实验表明该方法能稳定检测温度升高60°C时设备最高工作频率下降最多达13.79%。

Conclusion: 所提方法可在现场部署，有效监测微控制器因老化或温度变化引起的性能退化，优于传统静态保护带方案。

Abstract: Microcontrollers are increasingly present in embedded deployments and dependable applications, for which malfunctions due to hardware ageing can have severe impact. The lack of deployable techniques for ageing monitoring on these devices has spread the application of guard bands to prevent timing errors due to degradation. Applying this static technique can limit performance and lead to sudden failures as devices age. In this paper, we follow a software-based self-testing approach to design monitoring of hardware degradation for microcontrollers. Deployable in the field, our technique leverages timing windows of variable lengths to determine the maximum operational frequency of the devices. We empirically validate the method on real hardware and find that it consistently detects temperature-induced degradations in maximum operating frequency of up to 13.79 % across devices for 60 °C temperature increase.

</details>


### [36] [HFRWKV: A High-Performance Fully On-Chip Hardware Accelerator for RWKV](https://arxiv.org/abs/2601.02135)
*Liu Shijie,Zeng Zhenghao,Jiao Han,Huang Yihua*

Main category: cs.AR

TL;DR: HFRWKV是专为RWKV设计的FPGA硬件加速器，通过混合精度量化、可复用架构与片上计算系统显著提升吞吐量与能效。


<details>
  <summary>Details</summary>
Motivation: RWKV虽具线性内存优势，但受限于GPU并行效率低及频繁访存瓶颈，需专用硬件优化。

Method: 提出混合精度量化策略、复用架构结合查表/分段线性逼近复杂运算，并构建全片上并行流水线系统。

Result: 在Alveo平台实现相比CPU 63.48倍吞吐提升、139.17倍能效提升；相比GPU达32.33倍吞吐与171.36倍能效提升。

Conclusion: HFRWKV有效克服RWKV硬件执行瓶颈，在吞吐与能效方面显著优于通用CPU/GPU平台。

Abstract: RWKV is a modern RNN architecture that approaches the performance of Transformers, with the advantage of processing long contexts at a linear memory cost. However, its sequential computation pattern struggles to efficiently leverage GPU parallelism, which leads to low compute resource utilization. Furthermore, frequent off-chip weight accesses create a memory bottleneck. To address these challenges, we propose HFRWKV, an FPGA-based hardware accelerator specifically designed for RWKV. Within the matrix operation module, we propose a novel hardware-friendly hybrid-precision quantization strategy, which enhances performance while maintaining acceptable accuracy. For the complex operations including exponentiation and division, we introduce a method featuring reusable architectures combined with lookup tables or piecewise linear approximation, which is algorithmically refined to effectively balance precision and hardware resource consumption. Based on this foundation, we adopt a fully on-chip computing system integrating parallel matrix-vector processing array and an efficient pipeline architecture. Through computation reordering and chunked double buffering, it effectively eliminates data transfer bottlenecks and improves overall throughput. We implement HFRWKV on the Alveo U50 and U280 platform. Experimental results show that compared to a CPU, a throughput improvement of 63.48$\times$ and an energy efficiency improvement of 139.17$\times$. Compared to GPUs, achieves a throughput improvement of 32.33$\times$ and an energy efficiency improvement of 171.36$\times$.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [37] [Improving the Graph Challenge Reference Implementation](https://arxiv.org/abs/2601.00974)
*Inna Voloshchuk,Hayden Jananthan,Chansup Byun,Jeremy Kepner*

Main category: cs.NI

TL;DR: 本文重构并优化了MIT/IEEE/Amazon图挑战赛的参考代码，通过精简代码、引入并行计算，显著提升性能与可读性。


<details>
  <summary>Details</summary>
Motivation: 提升图挑战赛参考代码的清晰度、适应性和性能，以增强其对参赛者的实用价值。

Method: 将原Python实现从约1000行精简至325行，并利用pMatlab和pPython库实现并行映射与分布式计算。

Result: 代码规模减少67%，功能完整保留，并在大规模流量矩阵求和与分析中展现可扩展性能。

Conclusion: 重构后的实现为图挑战赛参与者提供了更清晰高效的基准，提升了赛事影响力。

Abstract: The MIT/IEEE/Amazon Graph Challenge provides a venue for individuals and teams to showcase new innovations in large-scale graph and sparse data analysis. The Anonymized Network Sensing Graph Challenge processes over 100 billion network packets to construct privacy-preserving traffic matrices, with a GraphBLAS reference implementation demonstrating how hypersparse matrices can be applied to this problem. This work presents a refactoring and benchmarking of a section of the reference code to improve clarity, adaptability, and performance. The original Python implementation spanning approximately 1000 lines across 3 files has been streamlined to 325 lines across two focused modules, achieving a 67% reduction in code size while maintaining full functionality. Using pMatlab and pPython distributed array programming libraries, the addition of parallel maps allowed for parallel benchmarking of the data. Scalable performance is demonstrated for large-scale summation and analysis of traffic matrices. The resulting implementation increases the potential impact of the Graph Challenge by providing a clear and efficient foundation for participants.

</details>


### [38] [Decision-Aware Semantic State Synchronization in Compute-First Networking](https://arxiv.org/abs/2601.01086)
*Jianpeng Qi,Chao Liu,Chengrui Wang,Rui Wang,Junyu Dong,Yanwei Yu*

Main category: cs.NI

TL;DR: SenseCFN是一种面向决策感知的状态同步框架，通过语义状态表示与语义偏差指数（SDI）触发更新，在保证任务成功率的同时大幅降低更新频率。


<details>
  <summary>Details</summary>
Motivation: 现有CFN系统中基于周期或信息年龄的状态同步策略忽视了状态变化对卸载决策的实际影响，导致资源浪费或决策性能下降。

Method: 提出轻量级语义状态表示和SDI指标，仅在决策相关状态显著变化时触发更新；AP端利用缓存语义状态并感知其陈旧性进行决策；采用CTDE联合优化更新与卸载策略。

Result: 仿真表明，在高负载场景下任务成功率可达99.6%，较基线方法提升超25%，同时更新频率降低70%至96%。

Conclusion: 决策感知的状态同步机制为CFN提供了一种比纯时间驱动策略更高效实用的替代方案。

Abstract: In Compute-First Networking (CFN), an Access Point (AP) makes task offloading decisions based on resource state information reported by a Service Node (SN). A fundamental challenge arises from the trade-off between update overhead and decision accuracy: Frequent state updates consume limited network resources, while infrequent updates lead to stale state views and degraded task performance, especially under high system load. Existing approaches based on periodic updates or Age of Information (AoI) mainly focus on temporal freshness and often overlook whether a state change is actually relevant to offloading decisions. This paper proposes SenseCFN, a decision-aware state synchronization framework for CFN. Instead of synchronizing raw resource states, SenseCFN focuses on identifying state changes that are likely to alter offloading decisions. To this end, we introduce a lightweight semantic state representation that captures decision-relevant system characteristics, along with a Semantic Deviation Index (SDI) to quantify the impact of state shifts on decision outcomes. Based on SDI, the SN triggers updates only when significant decision-impacting changes are detected. Meanwhile, the AP performs offloading decisions using cached semantic states with explicit awareness of potential staleness. The update and offloading policies are jointly optimized using a centralized training with distributed execution (CTDE) approach. Simulation results show that SenseCFN maintains a task success rate of up to 99.6% in saturation-prone scenarios, outperforming baseline methods by more than 25%, while reducing status update frequency by approximately 70% to 96%. These results indicate that decision-aware state synchronization provides an effective and practical alternative to purely time-based update strategies in CFN.

</details>


### [39] [Near-Field Multi-Cell ISCAP with Extremely Large-Scale Antenna Array](https://arxiv.org/abs/2601.01968)
*Yuan Guo,Yilong Chen,Zixiang Ren,Derrick Wing Kwan Ng,Jie Xu*

Main category: cs.NI

TL;DR: 该论文研究了一种多小区协同的近场集成感知、通信与供能系统，通过优化波束成形策略，在满足通信和能量收集需求的同时最大化最差情况下的检测概率。


<details>
  <summary>Details</summary>
Motivation: 解决近场环境下多小区系统中感知、通信与无线供能三者之间的资源分配与性能权衡问题。

Method: 提出鲁棒优化框架，结合半定松弛（SDR）将非凸问题转化为凸问题，并设计低复杂度的最大比传输（MRT）渐近闭式解。

Result: 数值结果揭示了感知精度、通信可靠性和供能效率之间的基本权衡关系，验证了所提方案的有效性。

Conclusion: 该系统架构与优化方法为未来近场智能无线系统提供了理论支撑与实用解决方案。

Abstract: This paper investigates a coordinated multi-cell integrated sensing, communication, and powering (ISCAP) system operating in the electromagnetic near field, where each base station (BS) employs an extremely large-scale antenna array (ELAA) to simultaneously support downlink communication, wireless power transfer (WPT), and environmental sensing. Three categories of communication users (CUs) with different interference cancellation capabilities are considered, and sensing is enabled through a distributed multiple-input multiple-output (MIMO) radar architecture. To address the resulting design challenges, a robust optimization framework is proposed by optimizing the beamforming strategy to maximize the worst-case detection probability over a prescribed sensing region, subject to per-user signal-to-interference-plus-noise ratio (SINR) constraints and energy harvesting requirements at energy receivers (ERs), while explicitly capturing the uncertainty in ER locations. By leveraging semidefinite relaxation (SDR), the original non-convex problem is reformulated as a convex semidefinite program with a provably tight relaxation. Furthermore, a low-complexity maximum ratio transmission (MRT)-based suboptimal scheme is developed, yielding a closed-form solution in the asymptotic regime as the number of antenna elements approaches infinity. Extensive numerical results reveal the fundamental trade-offs among sensing accuracy, communication reliability, and WPT efficiency.

</details>
