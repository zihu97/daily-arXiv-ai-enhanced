<div id=toc></div>

# Table of Contents

- [cs.MA](#cs.MA) [Total: 2]
- [cs.NI](#cs.NI) [Total: 3]
- [cs.SE](#cs.SE) [Total: 6]
- [cs.AR](#cs.AR) [Total: 2]
- [cs.DC](#cs.DC) [Total: 8]


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [1] [Consensus Based Task Allocation for Angles-Only Local Catalog Maintenance of Satellite Systems](https://arxiv.org/abs/2602.16678)
*Harrison Perone,Christopher W. Hays*

Main category: cs.MA

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: In order for close proximity satellites to safely perform their missions, the relative states of all satellites and pieces of debris must be well understood. This presents a problem for ground based tracking and orbit determination since it may not be practical to achieve the required accuracy. Using space-based sensors allows for more accurate relative state estimates, especially if multiple satellites are allowed to communicate. Of interest to this work is the case where several communicating satellites each need to maintain a local catalog of communicating and non-communicating objects using angles-only limited field of view (FOV) measurements. However, this introduces the problem of efficiently scheduling and coordinating observations among the agents. This paper presents a decentralized task allocation algorithm to address this problem and quantifies its performance in terms of fuel usage and overall catalog uncertainty via numerical simulation. It was found that the new method significantly outperforms the uncertainty-fuel Pareto frontier formed by current approaches.

</details>


### [2] [Fairness Dynamics in Digital Economy Platforms with Biased Ratings](https://arxiv.org/abs/2602.16695)
*J. Martin Smit,Fernando P. Santos*

Main category: cs.MA

TL;DR: 该论文研究数字平台评级系统如何设计以减少歧视，使用博弈论模型证明调整搜索结果的群体分布可降低偏见并维护服务质量。


<details>
  <summary>Details</summary>
Motivation: 推动研究的原因是评级系统可能在数字平台中强化对边缘化群体的偏见，阻碍公平交易，需探索在不影响服务质量的前提下降低歧视的设计方案。

Method: 引入进化博弈论 مرض建立一个理论模型，分析平台在推广高声誉提供商或受保护群体时的决策影响，评估歧视的存续与对抗机制。

Result: 结果显示emplos中存在用户体验与公平的权衡：推广高声誉提供商利于用户体验但加剧边缘化群体需求降低；调整搜索结果的人口分布能高效减少不公，最小化用户体验影响；即使缺乏精确偏见数据，忽略受保护特征的推荐系统仍有改进空间。

Conclusion: 模型强调在依赖评级促进合作的系统中，主动采用反歧视设计可显著优化整体公平性和平台效能。

Abstract: The digital services economy consists of online platforms that facilitate interactions between service providers and consumers. This ecosystem is characterized by short-term, often one-off, transactions between parties that have no prior familiarity. To establish trust among users, platforms employ rating systems which allow users to report on the quality of their previous interactions. However, while arguably crucial for these platforms to function, rating systems can perpetuate negative biases against marginalised groups. This paper investigates how to design platforms around biased reputation systems, reducing discrimination while maintaining incentives for all service providers to offer high quality service for users. We introduce an evolutionary game theoretical model to study how digital platforms can perpetuate or counteract rating-based discrimination. We focus on the platforms' decisions to promote service providers who have high reputations or who belong to a specific protected group. Our results demonstrate a fundamental trade-off between user experience and fairness: promoting highly-rated providers benefits users, but lowers the demand for marginalised providers against which the ratings are biased. Our results also provide evidence that intervening by tuning the demographics of the search results is a highly effective way of reducing unfairness while minimally impacting users. Furthermore, we show that even when precise measurements on the level of rating bias affecting marginalised service providers is unavailable, there is still potential to improve upon a recommender system which ignores protected characteristics. Altogether, our model highlights the benefits of proactive anti-discrimination design in systems where ratings are used to promote cooperative behaviour.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [3] [Managing Credible Anonymous Identities in Web 3.0 Services: A Scalable On-Chain Admission Framework with Recursive Proof Aggregation](https://arxiv.org/abs/2602.16130)
*Zibin Lin,Taotao Wang,Shengli Zhang,Long Shi,Shui Yu*

Main category: cs.NI

TL;DR: 提出ZK-AMS系统，通过零知识证明和递归聚合实现Web 3.0平台高效且隐私保护的准入控制。


<details>
  <summary>Details</summary>
Motivation: 解决Web 3.0平台在突发流量下难以同时保证女巫攻击防御、用户隐私和稳定上链成本的问题，避免现有方案成本随用户数量线性增长。

Method: 结合零知识凭证验证、无权限批次提交模型，以及基于Nova递归聚合与多方同态加密的隐私保护折叠管道，实现恒定成本的批量验证。

Result: 以太坊测试显示：批量处理下验证成本稳定，准入吞吐量显著提升，延迟与燃料消耗优于非递归基线。

Conclusion: 为大规模Web 3.0社区提供了经济可预测、高效实用的匿名账户准入基础设施。

Abstract: Open Web 3.0 platforms increasingly operate as \emph{service ecosystems} (e.g., DeFi, DAOs, and decentralized social applications) where \emph{admission control} and \emph{account provisioning} must be delivered as an always-on service under bursty demand. Service operators face a fundamental tension: enforcing Sybil resistance (one-person-one-account) while preserving user privacy, yet keeping on-chain verification cost and admission latency predictable at scale. Existing credential-based ZK admission approaches typically require per-request on-chain verification, making the provisioning cost grow with the number of concurrent joiners. We present \textbf{ZK-AMS}, a scalable admission and provisioning layer that bridges real-world \emph{Personhood Credentials} to anonymous on-chain service accounts. ZK-AMS combines (i) zero-knowledge credential validation, (ii) a \emph{permissionless} batch submitter model, and (iii) a decentralized, privacy-preserving folding pipeline that uses Nova-style recursive aggregation together with multi-key homomorphic encryption, enabling batch settlement with \emph{constant} on-chain verification per batch. We implement ZK-AMS end-to-end on an Ethereum testbed and evaluate admission throughput, end-to-end latency, and gas consumption. Results show stable verification cost across batch sizes and substantially improved admission efficiency over non-recursive baselines, providing a practical and cost-predictable admission service for large-scale Web 3.0 communities.

</details>


### [4] [Collection: UAV-Based Wireless Multi-modal Measurements from AERPAW Autonomous Data Mule (AADM) Challenge in Digital Twin and Real-World Environments](https://arxiv.org/abs/2602.16163)
*Md Sharif Hossen,Cole Dickerson,Ozgur Ozdemir,Anil Gurses,Mohamed Rabeek Sarbudeen,Thomas Zajkowski,Ahmed Manavi Alam,Everett Tucker,William Bjorndahl,Fred Solis,Sadaf Javed,Anirudh Kamath,Xiangyao Tang,Joarder Jafor Sadique,Kevin Liu Hermstein,Kaies Al Mahmud,Jose Angel Sanchez Viloria,Skyler Hawkins,Yuqing Cui,Annoy Dey,Yuchen Liu,Ali Gurbuz,Joseph Camp,Rizwan Ahmad,Jacobus van der Merwe,Ahmed Ibrahim Mohamed,Gil Zussman,Mehmet Kurum,Namuduri Kamesh,Zhangyu Guan,Dimitris Pados,George Skilvanitis,Ismail Guvenc,Mihail Sichitiu,Magreth Mushi,Rudra Dutta*

Main category: cs.NI

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: In this work, we present an unmanned aerial vehicle (UAV) wireless dataset collected as part of the AERPAW Autonomous Aerial Data Mule (AADM) challenge, organized by the NSF Aerial Experimentation and Research Platform for Advanced Wireless (AERPAW) project. The AADM challenge was the second competition in which an autonomous UAV acted as a data mule, where the UAV downloaded data from multiple base stations (BSs) in a dynamic wireless environment. Participating teams designed flight control and decision-making algorithms for choosing which BSs to communicate with and how to plan flight trajectories to maximize data download within a mission completion time. The competition was conducted in two stages: Stage 1 involved development and experimentation using a digital twin (DT) environment, and in Stage 2, the final test run was conducted on the outdoor testbed. The total score for each team was compiled from both stages. The resulting dataset includes link quality and data download measurements, both in DT and physical environments. Along with the USRP measurements used in the contest, the dataset also includes UAV telemetry, Keysight RF sensors position estimates, link quality measurements from LoRa receivers, and Fortem radar measurements. It supports reproducible research on autonomous UAV networking, multi-cell association and scheduling, air-to-ground propagation modeling, DT-to-real-world transfer learning, and integrated sensing and communication, which serves as a benchmark for future autonomous wireless experimentation.

</details>


### [5] [Multi-Agent Meta-Advisor for UAV Fleet Trajectory Design in Vehicular Networks](https://arxiv.org/abs/2602.16345)
*Leonardo Spampinato,Lorenzo Mario Amorosa,Enrico Testi,Chiara Buratti,Riccardo Marini*

Main category: cs.NI

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Future vehicular networks require continuous connectivity to serve highly mobile users in urban environments. To mitigate the coverage limitations of fixed terrestrial macro base stations (MBS) under non line-of-sight (NLoS) conditions, fleets of unmanned aerial base stations (UABSs) can be deployed as aerial base stations, dynamically repositioning to track vehicular users and traffic hotspots in coordination with the terrestrial network. This paper addresses cooperative multi-agent trajectory design under different service areas and takeoff configurations, where rapid and safe adaptation across scenarios is essential. We formulate the problem as a multi-task decentralized partially observable Markov decision process and solve it using centralized training and decentralized execution with double dueling deep Q-network (3DQN), enabling online training for real-world deployments. However, efficient exploration remains a bottleneck, with conventional strategies like $ε$-greedy requiring careful tuning. To overcome this, we propose the multi-agent meta-advisor with advisor override (MAMO). This framework guides agent exploration through a meta-policy learned jointly across tasks. It uses a dynamic override mechanism that allows agents to reject misaligned guidance when the advisor fails to generalize to a specific scenario. Simulation results across three realistic urban scenarios and multiple takeoff configurations show that MAMO achieves faster convergence and higher returns than tuned $ε$-greedy baselines, outperforming both an advisor-only ablation and a single generalized policy. Finally, we demonstrate that the learned UABS fleet significantly improves network performance compared to deployments without aerial support.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [6] [From Reflection to Repair: A Scoping Review of Dataset Documentation Tools](https://arxiv.org/abs/2602.15968)
*Pedro Reynolds-Cuéllar,Marisol Wong-Villacres,Adriana Alvarado Garcia,Heila Precel*

Main category: cs.SE

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Dataset documentation is widely recognized as essential for the responsible development of automated systems. Despite growing efforts to support documentation through different kinds of artifacts, little is known about the motivations shaping documentation tool design or the factors hindering their adoption. We present a systematic review supported by mixed-methods analysis of 59 dataset documentation publications to examine the motivations behind building documentation tools, how authors conceptualize documentation practices, and how these tools connect to existing systems, regulations, and cultural norms. Our analysis shows four persistent patterns in dataset documentation conceptualization that potentially impede adoption and standardization: unclear operationalizations of documentation's value, decontextualized designs, unaddressed labor demands, and a tendency to treat integration as future work. Building on these findings, we propose a shift in Responsible AI tool design toward institutional rather than individual solutions, and outline actions the HCI community can take to enable sustainable documentation practices.

</details>


### [7] [ReLoop: Structured Modeling and Behavioral Verification for Reliable LLM-Based Optimization](https://arxiv.org/abs/2602.15983)
*Junbo Jacob Lian,Yujun Sun,Huiling Chen,Chaoyu Zhang,Chung-Piaw Teo*

Main category: cs.SE

TL;DR: ReLoop框架解决大语言模型生成优化代码时存在的语义悄声失败问题，通过结构化生成和行为验证方法提升代码正确性和执行率。


<details>
  <summary>Details</summary>
Motivation: LLMs将自然语言转为优化代码时可能出现悄声失败：代码可运行但语义错误（可行性-正确性差距高达90%），尤其在组合问题上存在风险。

Method: 提出结构化生成（四步推理链：理解/形式化/综合/验证）避免源头错误，辅以行为验证（基于求解器参数扰动检测）处理残余错误；结合IIS诊断实现执行恢复。

Result: 在最强模型上：正确率从22.6%提升至31.1%，执行率从72.1%达100%；五类模型、三个基准测试均显著改进，并发布零售优化数据集RetailOpt-190。

Conclusion: ReLoop通过互补机制有效消除悄声失败，结构化生成在组合问题占优，行为验证定位局部缺陷；该框架显著提升LLM代码生成可靠性并提供新测试基准。

Abstract: Large language models (LLMs) can translate natural language into optimization code, but silent failures pose a critical risk: code that executes and returns solver-feasible solutions may encode semantically incorrect formulations, creating a feasibility-correctness gap of up to 90 percentage points on compositional problems. We introduce ReLoop, addressing silent failures from two complementary directions. Structured generation decomposes code production into a four-stage reasoning chain (understand, formalize, synthesize, verify) that mirrors expert modeling practice, with explicit variable-type reasoning and self-verification to prevent formulation errors at their source. Behavioral verification detects errors that survive generation by testing whether the formulation responds correctly to solver-based parameter perturbation, without requiring ground truth -- an external semantic signal that bypasses the self-consistency problem inherent in LLM-based code review. The two mechanisms are complementary: structured generation dominates on complex compositional problems, while behavioral verification becomes the largest single contributor on problems with localized formulation defects. Together with execution recovery via IIS-enhanced diagnostics, ReLoop raises correctness from 22.6% to 31.1% and execution from 72.1% to 100.0% on the strongest model, with consistent gains across five models spanning three paradigms (foundation, SFT, RL) and three benchmarks. We additionally release RetailOpt-190, 190 compositional retail optimization scenarios targeting the multi-constraint interactions where LLMs most frequently fail.

</details>


### [8] [Can Causality Cure Confusion Caused By Correlation (in Software Analytics)?](https://arxiv.org/abs/2602.16091)
*Amirali Rayegan,Tim Menzies*

Main category: cs.SE

TL;DR: 本研究探讨因果感知分割准则提升符号模型稳定性和鲁棒性，同时分析人类专家判断的稳定性，基于120项多目标优化任务进行实验。


<details>
  <summary>Details</summary>
Motivation: 背景显示软件工程中符号模型的关联性分割准则误将统计关联视为因果，导致模型不稳定，损害解释可靠性。目标为评估因果感知分割准则可否提升稳定性而不降低性能，并对比人类判断。

Method: 使用MOOT库120+任务，采用预注册自举集成协议评估稳定性（使用赢分分配），统计分析涵盖方差、基尼不纯度、KS检验和Cliff's delta；比较相关系数决策树（EZR）、人类因果评估及因果感知树（引入条件熵分裂和混杂过滤）。

Result: 摘要未提供实验结果细节。

Conclusion: 摘要未陈述结论部分，但目标强调探索因果准则对稳定性的潜在增益 global insights。

Abstract: Background: Symbolic models, particularly decision trees, are widely used in software engineering for explainable analytics in defect prediction, configuration tuning, and software quality assessment. Most of these models rely on correlational split criteria, such as variance reduction or information gain, which identify statistical associations but cannot imply causation between X and Y. Recent empirical studies in software engineering show that both correlational models and causal discovery algorithms suffer from pronounced instability. This instability arises from two complementary issues: 1-Correlation-based methods conflate association with causation. 2-Causal discovery algorithms rely on heuristic approximations to cope with the NP-hard nature of structure learning, causing their inferred graphs to vary widely under minor input perturbations. Together, these issues undermine trust, reproducibility, and the reliability of explanations in real-world SE tasks. Objective: This study investigates whether incorporating causality-aware split criteria into symbolic models can improve their stability and robustness, and whether such gains come at the cost of predictive or optimization performance. We additionally examine how the stability of human expert judgments compares to that of automated models. Method: Using 120+ multi-objective optimization tasks from the MOOT repository of multi-objective optimization tasks, we evaluate stability through a preregistered bootstrap-ensemble protocol that measures variance with win-score assignments. We compare the stability of human causal assessments with correlation-based decision trees (EZR). We would also compare the causality-aware trees, which leverage conditional-entropy split criteria and confounder filtering. Stability and performance differences are analyzed using statistical methods (variance, Gini Impurity, KS test, Cliff's delta)

</details>


### [9] [Algorithm-Based Pipeline for Reliable and Intent-Preserving Code Translation with LLMs](https://arxiv.org/abs/2602.16106)
*Shahriar Rumi Dipto,Saikat Mondal,Chanchal K. Roy*

Main category: cs.SE

TL;DR: 算法管道提升LLMs代码翻译准确性10.8%，减少错误类型。


<details>
  <summary>Details</summary>
Motivation: 直接单次翻译常未能保留程序意图，易引发控制流、类型处理和I/O行为错误。

Method: 在Avatar和CodeNet数据集上，使用5种LLMs进行Python和Java双向翻译对比实验（直接与算法管道），编译并运行测试程序，记录编译结果、运行时异常与测试通过率，使用统一分类映射失败案例。

Result: 算法管道微平均准确率从67.7%升至78.5%，消除词法和令牌错误100%，减少不完整结构72.7%及结构声明问题61.1%，降低运行时依赖失败78.4%。

Conclusion: 算法管道通过语言中立规范确保代码意图保留，为可靠跨语言编程助手奠定基础。

Abstract: Code translation, the automatic conversion of programs between languages, is a growing use case for Large Language Models (LLMs). However, direct one-shot translation often fails to preserve program intent, leading to errors in control flow, type handling, and I/O behavior. We propose an algorithm-based pipeline that introduces a language-neutral intermediate specification to capture these details before code generation. This study empirically evaluates the extent to which structured planning can improve translation accuracy and reliability relative to direct translation. We conduct an automated paired experiment - direct and algorithm-based to translate between Python and Java using five widely used LLMs on the Avatar and CodeNet datasets. For each combination (model, dataset, approach, and direction), we compile and execute the translated program and run the tests provided. We record compilation results, runtime behavior, timeouts (e.g., infinite loop), and test outcomes. We compute accuracy from these tests, counting a translation as correct only if it compiles, runs without exceptions or timeouts, and passes all tests. We then map every failed compile-time and runtime case to a unified, language-aware taxonomy and compare subtype frequencies between the direct and algorithm-based approaches. Overall, the Algorithm-based approach increases micro-average accuracy from 67.7% to 78.5% (10.8% increase). It eliminates lexical and token errors by 100%, reduces incomplete constructs by 72.7%, and structural and declaration issues by 61.1%. It also substantially lowers runtime dependency and entry-point failures by 78.4%. These results demonstrate that algorithm-based pipelines enable more reliable, intent-preserving code translation, providing a foundation for robust multilingual programming assistants.

</details>


### [10] [Software-heavy Asset Administration Shells: Classification and Use Cases](https://arxiv.org/abs/2602.16499)
*Carsten Ellwein,David Dietrich,Jessica Roth,Rozana Cvitkovic,Andreas Wortmann*

Main category: cs.SE

TL;DR: 分析集成软件服务的资产管理壳架构


<details>
  <summary>Details</summary>
Motivation: 制造业数字化和AI应用增长凸显软件建模与服务集成需求，但缺乏软件架构系统分析

Method: 基于软件质量标准和典型制造用例对架构进行差异化研究

Result: 填补研究空白，提供系统化架构解决方案

Conclusion: 为学术界和从业者提供软件密集型资产管理壳实施指南

Abstract: The Asset Administration Shell (AAS) is an emerging technology for the implementation of digital twins in the field of manufacturing. Software is becoming increasingly important, not only in general but specifically in relation to manufacturing, especially with regard to digital manufacturing and a shift towards the usage of artificial intelligence. This increases the need not only to model software, but also to integrate services directly into the AAS. The existing literature contains individual solutions to implement such software-heavy AAS. However, there is no systematic analysis of software architectures that integrate software services directly into the AAS. This paper aims to fill this research gap and differentiate architectures based on software quality criteria as well as typical manufacturing use cases. This work may be considered as an interpretation guideline for software-heavy AAS, both in academia and for practitioners.

</details>


### [11] [SPARC: Scenario Planning and Reasoning for Automated C Unit Test Generation](https://arxiv.org/abs/2602.16671)
*Jaid Monwar Chowdhury,Chi-An Fu,Reyhaneh Jabbarvand*

Main category: cs.SE

TL;DR: SPARC框架通过神经符号和场景基础的方法，显著提升C语言自动化单元测试生成的效能


<details>
  <summary>Details</summary>
Motivation: C语言的自动化单元测试因语义鸿沟、指针算术和内存管理的复杂性而具有挑战性，大型语言模型（LLM）易产生跳跃到代码错误，导致非可编译测试、低覆盖率和无关断言

Method: SPARC采用四阶段框架：（1）控制流图分析，（2）操作地图以实用工具为基础约束LLM推理，（3）路径目标测试生成，（4）基于编译器与运行时反馈的迭代自校正循环

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Automated unit test generation for C remains a formidable challenge due to the semantic gap between high-level program intent and the rigid syntactic constraints of pointer arithmetic and manual memory management. While Large Language Models (LLMs) exhibit strong generative capabilities, direct intent-to-code synthesis frequently suffers from the leap-to-code failure mode, where models prematurely emit code without grounding in program structure, constraints, and semantics. This will result in non-compilable tests, hallucinated function signatures, low branch coverage, and semantically irrelevant assertions that cannot properly capture bugs. We introduce SPARC, a neuro-symbolic, scenario-based framework that bridges this gap through four stages: (1) Control Flow Graph (CFG) analysis, (2) an Operation Map that grounds LLM reasoning in validated utility helpers, (3) Path-targeted test synthesis, and (4) an iterative, self-correction validation loop using compiler and runtime feedback. We evaluate SPARC on 59 real-world and algorithmic subjects, where it outperforms the vanilla prompt generation baseline by 31.36% in line coverage, 26.01% in branch coverage, and 20.78% in mutation score, matching or exceeding the symbolic execution tool KLEE on complex subjects. SPARC retains 94.3% of tests through iterative repair and produces code with significantly higher developer-rated readability and maintainability. By aligning LLM reasoning with program structure, SPARC provides a scalable path for industrial-grade testing of legacy C codebases.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [12] [Bit-Width-Aware Design Environment for Few-Shot Learning on Edge AI Hardware](https://arxiv.org/abs/2602.16024)
*R. Kanda,H. L. Blevec,N. Onizawa,M. Leonardon,V. Gripon,T. Hanyu*

Main category: cs.AR

TL;DR: 本研究提出一种在PYNQ-Z1等微型FPGA SoC上实现实时少样本学习的方法，支持任意位宽固定点运算。


<details>
  <summary>Details</summary>
Motivation: 传统Tensil设计局限于16或32位固定点位宽，需灵活降低位宽以提高硬件效率。

Method: 采用FINN框架，优化转置节点解决数据格式失配，并添加处理以将reduce操作转为全局平均池化。

Result: 在CIFAR-10数据集评估中，在保持精度的同时显著削减位宽，推理吞吐量提升约两倍。

Conclusion: 该方法优化了FPGA推理过程，实现高效资源利用与高精度平衡。

Abstract: In this study, we propose an implementation methodology of real-time few-shot learning on tiny FPGA SoCs such as the PYNQ-Z1 board with arbitrary fixed-point bit-widths. Tensil-based conventional design environments limited hardware implementations to fixed-point bit-widths of 16 or 32 bits. To address this, we adopt the FINN framework, enabling implementations with arbitrary bit-widths. Several customizations and minor adjustments are made, including: 1.Optimization of Transpose nodes to resolve data format mismatches, 2.Addition of handling for converting the final reduce mean operation to Global Average Pooling (GAP). These adjustments allow us to reduce the bit-width while maintaining the same accuracy as the conventional realization, and achieve approximately twice the throughput in evaluations using CIFAR-10 dataset.

</details>


### [13] [Energy-Efficient p-Bit-Based Fully-Connected Quantum-Inspired Simulated Annealer with Dual BRAM Architecture](https://arxiv.org/abs/2602.16143)
*Naoya Onizawa,Taiga Kubuta,Duckgyu Shin,Takahiro Hanyu*

Main category: cs.AR

TL;DR: 本文提出了一种基于FPGA的随机模拟量子退火（SSQA）架构，显著改进概率比特退火的可扩展性、能lehraft and资源效率，解决了扇出和内存开销问题。


<details>
  <summary>Details</summary>
Motivation: 现有概率比特退火加速器因扇出增长和内存开销导致可扩展性差，难以支持全连接图结构，需创新硬件设计来优化大规模组合优化问题。

Method: 采用自旋串行与副本并行更新策略，结合双-BRAM延迟线架构，支持全连通Ising模型，避免逻辑资源扇出增长，并利用SSQA仅需最终副本状态来减少内存需求。

Result: 在Xilinx ZC706 FPGA上实现800节点MAX-CUT基准测试，比先前FPGA概率比特退火架构节能50%，逻辑资源减少90%以上。

Conclusion: 该架构证明了量子启发式概率比特退火硬件在严格能源和资源约束下，适用于大规模组合优化的实际部署。

Abstract: Probabilistic bits (p-bits) offer an energy-efficient hardware abstraction for stochastic optimization; however, existing p-bit-based simulated annealing accelerators suffer from poor scalability and limited support for fully connected graphs due to fan-out and memory overhead. This paper presents an energy-efficient FPGA architecture for stochastic simulated quantum annealing (SSQA) that addresses these challenges. The proposed design combines a spin-serial and replica-parallel update schedule with a dual-BRAM delay-line architecture, enabling scalable support for fully connected Ising models while eliminating fan-out growth in logic resources. By exploiting SSQA, the architecture achieves fast convergence using only final replica states, significantly reducing memory requirements compared to conventional p-bit-based annealers. Implemented on a Xilinx ZC706 FPGA, the proposed system solves an 800-node MAX-CUT benchmark and achieves up to 50% reduction in energy consumption and over 90\% reduction in logic resources compared with prior FPGA-based p-bit annealing architectures. These results demonstrate the practicality of quantum-inspired, p-bit-based annealing hardware for large-scale combinatorial optimization under strict energy and resource constraints.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [14] [Distributed Order Recording Techniques for Efficient Record-and-Replay of Multi-threaded Programs](https://arxiv.org/abs/2602.15995)
*Xiang Fu,Shiman Meng,Weiping Zhang,Luanzheng Guo,Kento Sato,Dong H. Ahn,Ignacio Laguna,Gregory L. Lee,Martin Schulz*

Main category: cs.DC

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: After all these years and all these other shared memory programming frameworks, OpenMP is still the most popular one. However, its greater levels of non-deterministic execution makes debugging and testing more challenging. The ability to record and deterministically replay the program execution is key to address this challenge. However, scalably replaying OpenMP programs is still an unresolved problem. In this paper, we propose two novel techniques that use Distributed Clock (DC) and Distributed Epoch (DE) recording schemes to eliminate excessive thread synchronization for OpenMP record and replay. Our evaluation on representative HPC applications with ReOMP, which we used to realize DC and DE recording, shows that our approach is 2-5x more efficient than traditional approaches that synchronize on every shared-memory access. Furthermore, we demonstrate that our approach can be easily combined with MPI-level replay tools to replay non-trivial MPI+OpenMP applications. We achieve this by integrating \toolname into ReMPI, an existing scalable MPI record-and-replay tool, with only a small MPI-scale-independent runtime overhead.

</details>


### [15] [Scrutinizing Variables for Checkpoint Using Automatic Differentiation](https://arxiv.org/abs/2602.16010)
*Xin Huang,Weiping Zhang,Shiman Meng,Wubiao Xu,Xiang Fu,Luanzheng Guo,Kento Sato*

Main category: cs.DC

TL;DR: 提出一种基于自动微分分析变量的方法，识别检查点中的关键元素并排除非关键部分，优化资源利用。


<details>
  <summary>Details</summary>
Motivation: 检查点/重启机制消耗高，高性能计算应用中许多数据未被实际使用，排除可提升效率。

Method: 使用自动微分工具逐元素分析变量对应用输出的影响，区分关键与非关键元素，筛选检查点范围。

Result: 在NAS并行基准套件测试中，可视化关键/非关键区域，存储节省达20%，模式匹配算法物理逻辑。

Conclusion: 该策略有效减少存储开销，揭示数据使用规律，为检查点优化提供可扩展方案。

Abstract: Checkpoint/Restart (C/R) saves the running state of the programs periodically, which consumes considerable system resources. We observe that not every piece of data is involved in the computation in typical HPC applications; such unused data should be excluded from checkpointing for better storage/compute efficiency. To find out, we propose a systematic approach that leverages automatic differentiation (AD) to scrutinize every element within variables (e.g., arrays) for checkpointing allowing us to identify critical/uncritical elements and eliminate uncritical elements from checkpointing. Specifically, we inspect every single element within a variable for checkpointing with an AD tool to determine whether the element has an impact on the application output or not. We empirically validate our approach with eight benchmarks from the NAS Parallel Benchmark (NPB) suite. We successfully visualize critical/uncritical elements/regions within a variable with respect to its impact (yes or no) on the application output. We find patterns/distributions of critical/uncritical elements/regions quite interesting and follow the physical formulation/logic of the algorithm.The evaluation on NPB benchmarks shows that our approach saves storage for checkpointing by up to 20%.

</details>


### [16] [LLM-Driven Intent-Based Privacy-Aware Orchestration Across the Cloud-Edge Continuum](https://arxiv.org/abs/2602.16100)
*Zijie Su,Muhammed Tawfiqul Islam,Mohammad Goudarzi,Adel N. Toosi*

Main category: cs.DC

TL;DR: 提出动态流水线重配置方法条的规定，以最小化停机时间和性能损失在异构GPU环境下灵活服务LLM推理。


<details>
  <summary>Details</summary>
Motivation: 因LLM推理负载多元、GPU集群异构，且服务器对无环境需动态适应，而现有方案难以在线调整有状态的LLM部署配置。

Method: 设计支持在线调整流水线配置的动态重配置机制，使系统能根据工作负载变化选择最优配置。

Result: 在A100/L40 membre型GPU测试中，迁移机制实现<50ms停机时间，TTFT与TPOT指标额外开销均低于10%。

Conclusion: 该方法实现了服务器对无环境下LLM服务的高效弹性适配，为异构资源优化提供有效解决方案。

Abstract: With the rapid advancement of large language models (LLMs), efficiently serving LLM inference under limited GPU resources has become a critical challenge. Recently, an increasing number of studies have explored applying serverless computing paradigms to LLM serving in order to maximize resource utilization. However, LLM inference workloads are highly diverse, and modern GPU clusters are inherently heterogeneous, making it necessary to dynamically adjust deployment configurations online to better adapt to the elastic and dynamic nature of serverless environments. At the same time, enabling such online reconfiguration is particularly challenging due to the stateful nature of LLM inference and the massive size of model parameters. In this paper, we propose a dynamic pipeline reconfiguration approach that enables online adjustment of pipeline configurations while minimizing service downtime and performance degradation. Our method allows the system to select the optimal pipeline configuration in response to changing workloads. Experimental results on heterogeneous GPU platforms, including NVIDIA A100 and L40s, demonstrate that our migration mechanism incurs less than 50 ms of service downtime, while introducing under 10% overhead on both time-to-first-token (TTFT) and time-per-output-token (TPOT).

</details>


### [17] [Near-optimal population protocols on bounded-degree trees](https://arxiv.org/abs/2602.16222)
*Joel Rybicki,Jakob Solnerzik,Robin Vacus*

Main category: cs.DC

TL;DR: 本研究探索稀疏交互图中群体协议的时空权衡，发现在有界度树上，领导选举和精确多数问题无显著时空权衡新规，提供恒空间协议实现近最优稳定时间。


<details>
  <summary>Details</summary>
Motivation: 之前工作在完全图中已知最优时空权衡，但其他图类是否类似未知，因下界技术无法扩展到高密度图外；本研究动机为填补稀疏图领域知识空白。

Method: 提出两种新协议：一是基于随机漂移论证的自稳定2跳着色协议，二是自稳定树定向算法构建有根树；结合简化恒状态协议求解问题。

Result: 在树上实现领导选举和精确多数的恒空间协议，稳定时间接近最优；与当前最优相比实现线性加速，如定向树上精确多数问题在O(n^2 log n)步解决。

Conclusion: 结论表明稀疏图协议无显著时空权衡，新方法提供高效设计和显著性能提升，扩展群体协议应用潜力。

Abstract: We investigate space-time trade-offs for population protocols in sparse interaction graphs. In complete interaction graphs, optimal space-time trade-offs are known for the leader election and exact majority problems. However, it has remained open if other graph families exhibit similar space-time complexity trade-offs, as existing lower bound techniques do not extend beyond highly dense graphs.
  In this work, we show that -- unlike in complete graphs -- population protocols on bounded-degree trees do not exhibit significant asymptotic space-time trade-offs for leader election and exact majority. For these problems, we give constant-space protocols that have near-optimal worst-case expected stabilisation time. These new protocols achieve a linear speed-up compared to the state-of-the-art.
  Our results are based on two novel protocols, which we believe are of independent interest. First, we give a new fast self-stabilising 2-hop colouring protocol for general interaction graphs, whose stabilisation time we bound using a stochastic drift argument. Second, we give a self-stabilising tree orientation algorithm that builds a rooted tree in optimal time on any tree. As a consequence, we can use simple constant-state protocols designed for directed trees to solve leader election and exact majority fast. For example, we show that ``directed'' annihilation dynamics solve exact majority in $O(n^2 \log n)$ steps on directed trees.

</details>


### [18] [How Reliable is Your Service at the Extreme Edge? Analytical Modeling of Computational Reliability](https://arxiv.org/abs/2602.16362)
*MHD Saria Allahham,Hossam S. Hassanein*

Main category: cs.DC

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Extreme Edge Computing (XEC) distributes streaming workloads across consumer-owned devices, exploiting their proximity to users and ubiquitous availability. Many such workloads are AI-driven, requiring continuous neural network inference for tasks like object detection and video analytics. Distributed Inference (DI), which partitions model execution across multiple edge devices, enables these streaming services to meet strict throughput and latency requirements. Yet consumer devices exhibit volatile computational availability due to competing applications and unpredictable usage patterns. This volatility poses a fundamental challenge: how can we quantify the probability that a device, or ensemble of devices, will maintain the processing rate required by a streaming service? This paper presents an analytical framework for computational reliability in XEC, defined as the probability that instantaneous capacity meets demand at a specified Quality of Service (QoS) threshold. We derive closed-form reliability expressions under two information regimes: Minimal Information (MI), requiring only declared operational bounds, and historical data, which refines estimates via Maximum Likelihood Estimation from past observations. The framework extends to multi-device deployments, providing reliability expressions for series, parallel, and partitioned workload configurations. We derive optimal workload allocation rules and analytical bounds for device selection, equipping orchestrators with tractable tools to evaluate deployment feasibility and configure distributed streaming systems. We validate the framework using real-time object detection with YOLO11m model as a representative DI streaming workload; experiments on emulated XED environments demonstrate close agreement between analytical predictions, Monte Carlo sampling, and empirical measurements across diverse capacity and demand configurations.

</details>


### [19] [DistributedEstimator: Distributed Training of Quantum Neural Networks via Circuit Cutting](https://arxiv.org/abs/2602.16233)
*Prabhjot Singh,Adel N. Toosi,Rajkumar Buyya*

Main category: cs.DC

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Circuit cutting decomposes a large quantum circuit into a collection of smaller subcircuits. The outputs of these subcircuits are then classically reconstructed to recover the original expectation values. While prior work characterises cutting overhead largely in terms of subcircuit counts and sampling complexity, its end-to-end impact on iterative, estimator-driven training pipelines remains insufficiently measured from a systems perspective. In this paper, we propose a cut-aware estimator execution pipeline that treats circuit cutting as a staged distributed workload and instruments each estimator query into partitioning, subexperiment generation, parallel execution, and classical reconstruction phases. Using logged runtime traces and learning outcomes on two binary classification workloads (Iris and MNIST), we quantify cutting overheads, scaling limits, and sensitivity to injected stragglers, and we evaluate whether accuracy and robustness are preserved under matched training budgets. Our measurements show that cutting introduces substantial end-to-end overheads that grow with the number of cuts, and that reconstruction constitutes a dominant fraction of per-query time, bounding achievable speed-up under increased parallelism. Despite these systems costs, test accuracy and robustness are preserved in the measured regimes, with configuration-dependent improvements observed in some cut settings. These results indicate that practical scaling of circuit cutting for learning workloads hinges on reducing and overlapping reconstruction and on scheduling policies that account for barrier-dominated critical paths.

</details>


### [20] [Load Balanced Parallel Node Generation for Meshless Numerical Methods](https://arxiv.org/abs/2602.16347)
*Jon Vehovar,Miha Rot,Matjaž Depolli,Gregor Kosec*

Main category: cs.DC

TL;DR: 提出基于超树结构的并行化n维泊松圆盘采样算法，用于支持自适应分析的无网格数值计算


<details>
  <summary>Details</summary>
Motivation: 解决复杂几何和可变节点密度需求下，泊松圆盘采样并行化的锁竞争与计算效率问题

Method: 耦合空间索引与预构建工作分配超树，按密度分配平衡任务单元，线程通过叶节点级碰撞检测减少互斥锁使用

Result: 大幅降低节点插入时的锁争用，叶节点层级冲突处理显著减少互斥锁获取次数

Conclusion: 该并行算法优于现有方案，肮满足分布式系统适配需求，为复杂场景提供高效节点生成方案

Abstract: Meshless methods are used to solve partial differential equations by approximating differential operators at a node as a weighted sum of values at its neighbours. One of the algorithms for generating nodes suitable for meshless numerical analysis is an n-dimensional Poisson disc sampling based method. It can handle complex geometries and supports variable node density, a crucial feature for adaptive analysis. We modify this method for parallel execution using coupled spatial indexing and work distribution hypertrees. The latter is prebuilt according to the node density function, ensuring that each leaf represents a balanced work unit. Threads advance separate fronts and claim work hypertree leaves as needed while avoiding leaves neighbouring those claimed by other threads. Node placement constraints and the partially prebuilt spatial hypertree are combined to eliminate the need to lock the tree while it is being modified. Thread collision handling is managed by the work hypertree at the leaf level, drastically reducing the number of required mutex acquisitions for point insertion collision checks. We explore the behaviour of the proposed algorithm and compare the performance with existing attempts at parallelisation and consider the requirements for adapting the developed algorithm to distributed systems.

</details>


### [21] [FlowPrefill: Decoupling Preemption from Prefill Scheduling Granularity to Mitigate Head-of-Line Blocking in LLM Serving](https://arxiv.org/abs/2602.16603)
*Chia-chi Hsieh,Zan Zong,Xinyang Chen,Jianjiang Li,Jidong Zhai,Lijie Wen*

Main category: cs.DC

TL;DR: 本文提出FlowPrefill系统，优化大型语言模型服务的预填充阶段，通过解耦抢占粒度和调度频率，解决响应性和吞吐率之间的冲突，减少头线阻塞和TTFT SLO违规。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型服务中，高并发请求在预填充阶段易引发头线阻塞，导致优先级请求延迟和TTFT服务目标违规；固定的Chunk预填充虽可中断但存在响应性与吞吐率的固有矛盾，需自适应抢占机制。

Method: FlowPrefill引入两项创新：1）操作员级抢占，在无效率损失下实现细粒度中断；2）事件驱动调度，仅在请求到达或完成事件触发调度决策，确保高效响应并最小化开销。

Result: 实际生产跟踪评估显示，相比于先进系统，FlowPrefill最高将有效吞吐率提升达5.6倍，同时满足异构SLOs需求。

Conclusion: FlowPrefill通过解耦预填充调度，解决了LLM服务中阻塞和效率矛盾，显著优化性能并支持多样服务目标。

Abstract: The growing demand for large language models (LLMs) requires serving systems to handle many concurrent requests with diverse service level objectives (SLOs). This exacerbates head-of-line (HoL) blocking during the compute-intensive prefill phase, where long-running requests monopolize resources and delay higher-priority ones, leading to widespread time-to-first-token (TTFT) SLO violations. While chunked prefill enables interruptibility, it introduces an inherent trade-off between responsiveness and throughput: reducing chunk size improves response latency but degrades computational efficiency, whereas increasing chunk size maximizes throughput but exacerbates blocking. This necessitates an adaptive preemption mechanism. However, dynamically balancing execution granularity against scheduling overheads remains a key challenge.
  In this paper, we propose FlowPrefill, a TTFT-goodput-optimized serving system that resolves this conflict by decoupling preemption granularity from scheduling frequency. To achieve adaptive prefill scheduling, FlowPrefill introduces two key innovations: 1) Operator-Level Preemption, which leverages operator boundaries to enable fine-grained execution interruption without the efficiency loss associated with fixed small chunking; and 2) Event-Driven Scheduling, which triggers scheduling decisions only upon request arrival or completion events, thereby supporting efficient preemption responsiveness while minimizing control-plane overhead. Evaluation on real-world production traces shows that FlowPrefill improves maximum goodput by up to 5.6$\times$ compared to state-of-the-art systems while satisfying heterogeneous SLOs.

</details>
