<div id=toc></div>

# Table of Contents

- [cs.SE](#cs.SE) [Total: 20]
- [cs.MA](#cs.MA) [Total: 9]
- [cs.PF](#cs.PF) [Total: 2]
- [cs.NI](#cs.NI) [Total: 1]
- [cs.DC](#cs.DC) [Total: 28]
- [cs.AR](#cs.AR) [Total: 12]


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [1] [WITNESS: A lightweight and practical approach to fine-grained predictive mutation testing](https://arxiv.org/abs/2511.11999)
*Zeyu Lu,Peng Zhang,Chun Yong Chong,Shan Gao,Yibiao Yang,Yanhui Li,Lin Chen,Yuming Zhou*

Main category: cs.SE

TL;DR: WITNESS 是一种新的细粒度预测性变异测试方法，采用轻量级经典机器学习模型，支持方法内外的变异体预测，在保持高性能的同时显著降低计算成本，并提升杀矩阵预测效率。


<details>
  <summary>Details</summary>
Motivation: 现有基于深度学习的细粒度预测性变异测试方法存在计算成本高昂和适用范围受限（仅处理方法内变异体）两大问题，难以满足实际应用中对成本效益和全面性的需求。

Method: WITNESS 采用双重设计：(1) 收集方法内外变异体的特征，实现对所有变异体的预测；(2) 使用轻量级经典机器学习模型替代深度学习，以降低成本并增强模型可解释性。

Result: 在 Defects4J 项目上的评估表明，WITNESS 在不同场景下均达到最先进的预测性能，显著提升杀矩阵预测效率；后验分析显示包含变异前后信息的特征最为重要；基于预测杀矩阵的测试用例优先级排序结果更接近真实杀矩阵，优于基线方法。

Conclusion: WITNESS 有效克服了现有方法在计算成本和适用性方面的局限，在保证高预测性能的同时提升了效率与可解释性，具有更强的实际应用价值。

Abstract: Existing fine-grained predictive mutation testing studies predominantly rely on deep learning, which faces two critical limitations in practice: (1) Exorbitant computational costs. The deep learning models adopted in these studies demand significant computational resources for training and inference acceleration. This introduces high costs and undermines the cost-reduction goal of predictive mutation testing. (2) Constrained applicability. Although modern mutation testing tools generate mutants both inside and outside methods, current fine-grained predictive mutation testing approaches handle only inside-method mutants. As a result, they cannot predict outside-method mutants, limiting their applicability in real-world scenarios. We propose WITNESS, a new fine-grained predictive mutation testing approach. WITNESS adopts a twofold design: (1) With collected features from both inside-method and outside-method mutants, WITNESS is suitable for all generated mutants. (2) Instead of using computationally expensive deep learning, WITNESS employs lightweight classical machine learning models for training and prediction. This makes it more cost-effective and enabling straightforward explanations of the decision-making processes behind the adopted models. Evaluations on Defects4J projects show that WITNESS consistently achieves state-of-the-art predictive performance across different scenarios. Additionally, WITNESS significantly enhances the efficiency of kill matrix prediction. Post-hoc analysis reveals that features incorporating information from before and after the mutation are the most important among those used in WITNESS. Test case prioritization based on the predicted kill matrix shows that WITNESS delivers results much closer to those obtained by using the actual kill matrix, outperforming baseline approaches.

</details>


### [2] [Actionable Warning Is Not Enough: Recommending Valid Actionable Warnings with Weak Supervision](https://arxiv.org/abs/2511.12229)
*Zhipeng Xue,Zhipeng Gao,Tongtong Xu,Xing Hu,Xin Xia,Shanping Li*

Main category: cs.SE

TL;DR: 该论文提出了一种名为ACWRecommender的两阶段框架，通过构建首个大规模可操作警告数据集并引入弱监督学习，有效提升静态分析工具中真实缺陷警告的推荐准确率。


<details>
  <summary>Details</summary>
Motivation: 现有静态分析工具因高误报率而难以被广泛采用，且以往研究中对“可操作警告”的定义和收集假设不准确，导致大量无效标签，影响了机器学习模型的效果。

Method: 作者从GitHub上Top-500的C语言项目中挖掘68,274个回滚提交，构建首个大规模可操作警告数据集，并为每个警告分配表示其为真实缺陷可能性的弱标签；随后提出两阶段框架ACWRecommender：第一阶段使用预训练模型UniXcoder进行粗粒度可操作警告识别，第二阶段通过弱监督学习对高可能性真实缺陷警告（AWHB）进行细粒度重排序。

Result: 实验表明，ACWRecommender在nDCG和MRR指标上显著优于多个基线模型；在6个随机项目中人工验证了2,197个警告，向开发者报告的前10个推荐警告中有27个被确认为真实缺陷。

Conclusion: 该方法能有效帮助开发者从海量静态分析警告中快速定位真实缺陷，具有良好的实用价值。

Abstract: The use of static analysis tools has gained increasing popularity among developers in the last few years. However, the widespread adoption of static analysis tools is hindered by their high false alarm rates. Previous studies have introduced the concept of actionable warnings and built a machine-learning method to distinguish actionable warnings from false alarms. However, according to our empirical observation, the current assumption used for actionable warning(s) collection is rather shaky and inaccurate, leading to a large number of invalid actionable warnings. To address this problem, in this study, we build the first large actionable warning dataset by mining 68,274 reversions from Top-500 GitHub C repositories, we then take one step further by assigning each actionable warning a weak label regarding its likelihood of being a real bug. Following that, we propose a two-stage framework called ACWRecommender to automatically recommend the actionable warnings with high probability to be real bugs (AWHB). Our approach warms up the pre-trained model UniXcoder by identifying actionable warnings task (coarse-grained detection stage) and rerank AWHB to the top by weakly supervised learning (fine-grained reranking stage). Experimental results show that our proposed model outperforms several baselines by a large margin in terms of nDCG and MRR for AWHB recommendation. Moreover, we ran our tool on 6 randomly selected projects and manually checked the top-ranked warnings from 2,197 reported warnings, we reported top-10 recommended warnings to developers, 27 of them were already confirmed by developers as real bugs. Developers can quickly find real bugs among the massive amount of reported warnings, which verifies the practical usage of our tool.

</details>


### [3] [Reflections on the design, applications and implementations of the normative specification language eFLINT](https://arxiv.org/abs/2511.12276)
*L. Thomas van Binsbergen,Christopher A. Esterhuyse,Tim Müller*

Main category: cs.SE

TL;DR: 本文回顾了领域特定语言 eFLINT 的设计与应用，该语言旨在通过结合声明式与过程式元素，实现对法律合规性的自动化检查，并探讨其在不同应用场景下面临的冲突性需求及设计决策。


<details>
  <summary>Details</summary>
Motivation: 随着软件在社会实践中日益普及，确保其符合法律法规和合同变得愈发重要且成本高昂。现有合规实践需高度适应不断变化的法律环境，而自动化合规面临法律解释主观性、法规频繁更新及跨学科协作等挑战。

Method: 开发并反思领域特定语言 eFLINT，该语言融合声明式与过程式编程范式，显式建立法律概念与计算概念之间的联系，支持在软件运行前、中、后各阶段进行合规性检查。

Result: 通过多个实际应用场景分析 eFLINT 所面临的多样化（甚至冲突的）需求，总结了相应的语言设计决策，为自动化合规领域的语言开发者提供了经验与洞见。

Conclusion: eFLINT 的设计探索表明，在自动化合规领域，语言需兼顾法律与软件工程的双重需求；本文的经验可为未来合规语言的设计提供有益参考。

Abstract: Checking the compliance of software against laws, regulations and contracts is increasingly important and costly as the embedding of software into societal practices is getting more pervasive. Moreover, the digitalised services provided by governmental organisations and companies are governed by an increasing amount of laws and regulations, requiring highly adaptable compliance practices. A potential solution is to automate compliance using software. However, automating compliance is difficult for various reasons. Legal practices involve subjective processes such as interpretation and qualification. New laws and regulations come into effect regularly and laws and regulations, as well as their interpretations, are subjected to constant revision. In addition, computational reasoning with laws requires a cross-disciplinary process involving both legal and software expertise.
  This paper reflects on the domain-specific software language eFLINT developed to experiment with novel solutions. The language combines declarative and procedural elements to reason about situations and scenarios respectively, explicates and formalises connections between legal concepts and computational concepts, and is designed to automate compliance checks both before, during and after a software system runs. The various goals and applications areas for the language give rise to (conflicting) requirements. This paper reflects on the current design of the language by recalling various applications, the requirements they imposed, and subsequent design decisions. As such, this paper reports on results and insights of an investigation that can benefit language developers within the field of automated compliance.

</details>


### [4] [Reducing Hallucinations in LLM-Generated Code via Semantic Triangulation](https://arxiv.org/abs/2511.12288)
*Yihan Dai,Sijie Liang,Haotian Xu,Peichu Xie,Sergey Mechtaev*

Main category: cs.SE

TL;DR: 本文提出语义三角测量方法，通过变换编程问题的语义并保留解之间的可验证映射，提升大语言模型生成代码的可靠性，在低采样概率和多解场景下表现优于现有共识方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于采样的共识方法在正确程序采样概率低、存在多个非等价有效解或样本中无正确解时表现不佳，难以可靠选择正确程序或适时弃权。

Method: 引入语义三角测量：对编程问题进行非平凡语义变换，同时保持变换前后解之间精确且可验证的映射关系，并通过验证不同变换下生成程序的一致性来判断其泛化能力。

Result: 在LiveCodeBench和CodeElo基准上，使用GPT-4o和DeepSeek-V3模型，该方法相比仅选择置信度高于0.5的方案，代码可靠性提升21%，可在采样概率低至0.14时识别正确解，并能有效处理多有效解任务。

Conclusion: 语义三角测量能显著提升样本共识的可靠性与弃权能力，尤其适用于低概率正确解和多解场景，为代码生成提供更稳健的验证机制。

Abstract: When generating code from natural language prompts, an LLM samples programs from a probability distribution, many of which might be incorrect. Sample consensus techniques - such as majority voting or validation against generated tests or specifications - aim to identify a correct program in the sample or abstain if none is valid. However, existing methods often fail to select a correct solution when its sampling probability is low, or when the problem permits multiple valid but non-equivalent solutions. Additionally, they often fail to abstain when no correct solution is present in the sample. To overcome these limitations, we introduce semantic triangulation, which transforms a programming problem in a way that non-trivially alters its semantics while preserving an exact, verifiable mapping between solutions before and after transformation. We theoretically establish that verifying consistency across such problem transformations increases confidence that generated programs reflect accurate generalization rather than spurious statistical correlations, enabling more reliable sample consensus and abstention. On the LiveCodeBench and CodeElo benchmarks, using GPT-4o and DeepSeek-V3 models, semantic triangulation increases reliability of generated code by 21% compared to the method that selects only high-confidence solutions with the probability threshold 0.5, while being able to pinpoint correct solutions at sampling probabilities as low as 0.14. Apart from that, it is also the only approach to consistently form true consensus on tasks with multiple valid but non-equivalent solutions.

</details>


### [5] [High-level reasoning while low-level actuation in Cyber-Physical Systems: How efficient is it?](https://arxiv.org/abs/2511.12543)
*Burak Karaduman,Baris Tekin Tezel,Moharram Challenger*

Main category: cs.SE

TL;DR: 该研究通过实证比较六种编程语言和框架（C++、Java、Jade、Jason、松耦合与紧耦合的模糊Jason BDI）在最坏情况执行时间（WCET）和开发时间上的表现，揭示了抽象层次与推理能力对开发效率和运行时性能的具体权衡，为工业智能系统的技术选型提供依据。


<details>
  <summary>Details</summary>
Motivation: 工业信息集成系统日益复杂，亟需支持智能行为、实时响应和高效开发的软件技术，但工程师缺乏关于先进工业应用中工具选择的实证依据。

Method: 采用以开发者为中心的方法，通过测量和对比六种语言/框架在WCET和开发时间两个维度的表现，分析不同抽象层次和推理机制对系统性能与开发工作量的影响。

Result: 研究发现，随着抽象程度和推理能力的提升，开发效率提高但运行时性能（如WCET）可能下降，明确了工程工作量与执行效率之间的具体权衡。

Conclusion: 该研究为工业信息化中软件技术的选择提供了基于证据的指导，有助于提升系统集成的效率、可维护性和响应能力，并为未来在信息物理系统和智能制造中语言特性与开发及运行行为之间关系的研究奠定基础。

Abstract: The increasing complexity of industrial information-integration systems demands software technologies that enable intelligent behaviour, real-time response, and efficient development. Although many programming languages and frameworks exist, engineers still lack sufficient empirical evidence to guide the choice of tools for advanced industrial applications. This study addresses that need by measuring and comparing worst-case execution time (WCET) and development time across six languages and frameworks: C++, Java, Jade, Jason, and fuzzy Jason BDI with both loosely and tightly coupled integration. These technologies reflect a progression from procedural and object-oriented programming to agent-based frameworks capable of symbolic and fuzzy reasoning.
  Rather than relying on broad concepts such as paradigms or orientations, the study adopts a developer-centred approach grounded in measurable outcomes. The structured comparison examines how rising abstraction levels and reasoning capabilities affect both development effort and runtime behaviour. By analysing these dimensions, the study highlights concrete trade-offs between engineering workload and execution efficiency.
  The findings show how abstraction and reasoning mechanisms shape system performance and developer productivity, offering practical insight for designing intelligent, agent-based solutions that must operate under real-time constraints and complex decision-making requirements. Overall, the study contributes evidence-based guidance for selecting software technologies in industrial informatization, supporting improved integration efficiency, maintainability, and responsiveness, and laying groundwork for future research on the interplay between language features, development dynamics, and runtime behaviour in cyber-physical and smart manufacturing systems.

</details>


### [6] [Can Small GenAI Language Models Rival Large Language Models in Understanding Application Behavior?](https://arxiv.org/abs/2511.12576)
*Mohammad Meymani,Hamed Jelodar,Parisa Hamedi,Roozbeh Razavi-Far,Ali A. Ghorbani*

Main category: cs.SE

TL;DR: 该论文系统评估了大小型生成式AI语言模型在应用程序行为理解（以恶意软件检测为代表任务）中的表现，发现小型模型在保持较高准确率的同时，在计算效率和部署可行性方面具有显著优势。


<details>
  <summary>Details</summary>
Motivation: 探索小型与大型生成式AI语言模型在应用行为理解（特别是恶意软件检测）任务中的性能权衡，以评估小型模型是否能在资源受限环境中提供实用且高效的解决方案。

Method: 对多种小型和大型生成式AI语言模型在恶意软件检测任务中进行系统性评估，比较其在准确率、精确率、召回率和F1分数等指标上的表现，并分析其计算效率与部署可行性。

Result: 大型模型整体准确率更高，但小型模型在精确率和召回率方面表现具有竞争力，同时具备更快的推理速度和更低的资源消耗。

Conclusion: 小型生成式AI模型能够在实际应用行为分析中有效补充大型模型，在性能与资源效率之间取得实用平衡。

Abstract: Generative AI (GenAI) models, particularly large language models (LLMs), have transformed multiple domains, including natural language processing, software analysis, and code understanding. Their ability to analyze and generate code has enabled applications such as source code summarization, behavior analysis, and malware detection. In this study, we systematically evaluate the capabilities of both small and large GenAI language models in understanding application behavior, with a particular focus on malware detection as a representative task. While larger models generally achieve higher overall accuracy, our experiments show that small GenAI models maintain competitive precision and recall, offering substantial advantages in computational efficiency, faster inference, and deployment in resource-constrained environments. We provide a detailed comparison across metrics such as accuracy, precision, recall, and F1-score, highlighting each model's strengths, limitations, and operational feasibility. Our findings demonstrate that small GenAI models can effectively complement large ones, providing a practical balance between performance and resource efficiency in real-world application behavior analysis.

</details>


### [7] [LLM4SCREENLIT: Recommendations on Assessing the Performance of Large Language Models for Screening Literature in Systematic Reviews](https://arxiv.org/abs/2511.12635)
*Lech Madeyski,Barbara Kitchenham,Martin Shepperd*

Main category: cs.SE

TL;DR: 该论文指出当前在系统综述文献筛选中评估大语言模型（LLM）性能时存在严重方法论缺陷，如误用准确率等不适用于不平衡数据的指标、忽视遗漏证据的影响、未报告完整混淆矩阵等，并据此提出改进评估实践的具体建议。


<details>
  <summary>Details</summary>
Motivation: 由于大语言模型发布速度远超用户严谨评估的能力，尤其在支撑系统综述等关键研究任务时，亟需建立稳健的评估框架以确保其可靠性和有效性。

Method: 以一项近期大规模研究为例，分析传统评估指标在系统综述文献筛选任务中的适用性问题；同时系统回顾27篇相关论文，提取并评估其所采用的性能指标，识别良好实践与常见问题。

Result: 发现主要问题包括：使用对不平衡数据不鲁棒且无法反映优于随机水平的指标（如准确率）、忽视遗漏证据对工作量节省主张的影响、普遍未报告完整混淆矩阵。同时也总结出若干良好评估实践。

Conclusion: 系统综述筛选评估应优先关注遗漏证据/召回率和加权马修斯相关系数（WMCC），报告完整混淆矩阵，将不可分类输出视为需人工复核的阳性结果，采用防信息泄露设计并包含非LLM基线，且结论应基于假阴性代价更高的成本效益分析。

Abstract: Context: Large language models (LLMs) are released faster than users' ability to evaluate them rigorously. When LLMs underpin research, such as identifying relevant literature for systematic reviews (SRs), robust empirical assessment is essential. Objective: We identify and discuss key challenges in assessing LLM performance for selecting relevant literature, identify good (evaluation) practices, and propose recommendations. Method: Using a recent large-scale study as an example, we identify problems with the use of traditional metrics for assessing the performance of Gen-AI tools for identifying relevant literature in SRs. We analyzed 27 additional papers investigating this issue, extracted the performance metrics, and found both good practices and widespread problems, especially with the use and reporting of performance metrics for SR screening. Results: Major weaknesses included: i) a failure to use metrics that are robust to imbalanced data and do not directly indicate whether results are better than chance, e.g., the use of Accuracy, ii) a failure to consider the impact of lost evidence when making claims concerning workload savings, and iii) pervasive failure to report the full confusion matrix (or performance metrics from which it can be reconstructed) which is essential for future meta-analyses. On the positive side, we extract good (evaluation) practices on which our recommendations for researchers and practitioners, as well as policymakers, are built. Conclusions: SR screening evaluations should prioritize lost evidence/recall alongside chance-anchored and cost-sensitive Weighted MCC (WMCC) metric, report complete confusion matrices, treat unclassifiable outputs as referred-back positives for assessment, adopt leakage-aware designs with non-LLM baselines and open artifacts, and ground conclusions in cost-benefit analysis where FNs carry higher penalties than FPs.

</details>


### [8] [Enhancing LLM Code Generation Capabilities through Test-Driven Development and Code Interpreter](https://arxiv.org/abs/2511.12823)
*Sajed Jalil,Shuvo Saha,Hossain Mohammad Seym*

Main category: cs.SE

TL;DR: 本文提出一种结合测试驱动开发（TDD）与代码解释器（CI）的新方法，利用开源模型在无需微调的情况下显著提升针对孟加拉语提示的代码生成准确率，最高可达85%，小模型性能可达大模型的98%。


<details>
  <summary>Details</summary>
Motivation: 尽管孟加拉语拥有2.42亿母语使用者，但在大语言模型（LLM）训练中长期被忽视；现有代码生成技术依赖大量专业知识和资源，难以在资源受限的新兴市场普及。本文旨在通过本土语言赋能用户，推动代码生成工具的民主化。

Method: 结合测试驱动开发（TDD）与代码解释器（CI），使用开源权重模型处理孟加拉语提示进行代码生成，无需微调。

Result: 在孟加拉语提示下，代码生成整体准确率达到85%；同系列最小模型性能可达最大模型的98%。所有结果已在GitHub公开以供验证和复现。

Conclusion: 所提方法有效提升了资源受限环境下基于本地语言的代码生成能力，证明了无需微调的小型开源模型也能实现接近顶级大模型的性能，有助于推动代码生成技术在低资源语言社区的普及。

Abstract: Over the past few years, improving LLM code generation capabilities has been a key focus in NLP research. Despite Bengali having 242 million native speakers worldwide, it receives little attention when it comes to training LLMs. More recently, various fine-tuning and augmented generation techniques have been employed to significantly enhance code generation performance. However, they require considerable expertise and resources to utilize effectively as an end user. The goal of our work is to democratize access to powerful code generation tools in resource-constrained emerging markets, enabling users to leverage them in their native language.
  We introduce a novel approach that combines Test-Driven Development (TDD) and Code Interpreter (CI), utilizing open-weight models, which improves the baseline accuracy for code generation with Bengali prompts and achieves an overall accuracy of 85%. Our approach requires no finetuning and proves that even the smallest models in the same family can attain up to 98% accuracy compared to the largest models. All of our results are publicly shared in GitHub for validation and reproducibility.

</details>


### [9] [Human-Centred Requirements Engineering for Critical Systems: Insights from Disaster Early Warning Applications](https://arxiv.org/abs/2511.12856)
*Anuradha Madugalla,Jixuan Dong,Kai Lyne Loi,Matthew Crossman,John Grundy*

Main category: cs.SE

TL;DR: 本文提出一种以人为本的需求工程（RE）流程，将社会责任融入关键系统开发中，通过文献综述提炼出62项功能性与非功能性需求，并在早期预警系统原型中实现，经用户评估验证其提升系统可用性与可及性，主张以人为本是关键系统安全与公平的核心属性。


<details>
  <summary>Details</summary>
Motivation: 传统关键系统的需求工程过于侧重技术保障，忽视了系统运行中的人类与社会背景；作者认为，以人为本的考量是系统可靠性不可或缺的维度，尤其在服务弱势群体时更需纳入社会责任。

Method: 通过文献综述识别面向弱势群体的软件设计指南，并将其转化为62项功能与非功能需求；随后设计一个自适应早期预警系统原型，并通过6次访谈和8次认知走查进行评估。

Result: 评估结果表明，在需求阶段早期纳入以人为本的要求，能显著提升系统对所有用户的可用性与可访问性。

Conclusion: 以人为本不应被视为伦理附加项，而应作为安全、公平的关键系统的核心质量属性。

Abstract: Critical systems, such as those used in healthcare, defence, and disaster management, demand rigorous requirements engineering to ensure safety and reliability. Yet, much of this rigour has traditionally focused on technical assurance, often overlooking the human and social contexts in which these systems operate. This paper argues that considering human-centric aspects is an essential dimension of dependability, and presents a human-centred RE process designed to integrate social responsibility into critical system development. Drawing from a literature review, we identified a set of guidelines for designing software for vulnerable communities and translated these into sixty-two functional and non-functional requirements. These requirements were operationalised through the design of an adaptive early warning system prototype, which was subsequently evaluated through six interviews and eight cognitive walkthroughs to validate their relevance and applicability. The findings demonstrate that human-centric requirements, when addressed early, enhance the usability and accessibility of systems for all users. The paper concludes by positioning human-centricity not as an ethical add-on but as a defining quality of safe and equitable critical systems.

</details>


### [10] [Agent READMEs: An Empirical Study of Context Files for Agentic Coding](https://arxiv.org/abs/2511.12884)
*Worawalan Chatlatanagulchai,Hao Li,Yutaro Kashiwa,Brittany Reid,Kundjanasith Thonglek,Pattara Leelaprute,Arnon Rungsawang,Bundit Manaskasemsak,Bram Adams,Ahmed E. Hassan,Hajimu Iida*

Main category: cs.SE

TL;DR: 本文首次对2303个智能体上下文文件进行了大规模实证研究，发现开发者主要关注功能性内容（如构建命令、实现细节和架构），而很少指定安全性与性能等非功能性需求，表明当前工具在保障代码质量方面存在明显不足。


<details>
  <summary>Details</summary>
Motivation: 理解智能体上下文文件的实际使用情况及其在指导智能编码工具中的作用，尤其是识别当前实践中可能存在的安全性和性能保障缺失问题。

Method: 对来自1925个代码仓库的2303个智能体上下文文件进行大规模实证分析，包括结构、维护模式和内容类型，并对16种指令类型进行分类统计。

Result: 上下文文件并非静态文档，而是类似配置代码的动态产物；62.3%包含构建/运行命令，69.9%含实现细节，67.7%描述架构；但仅14.5%提及安全性或性能。

Conclusion: 当前开发者主要利用上下文文件确保智能体功能正确性，却忽视了对其生成代码的安全性和性能约束，亟需改进相关工具与实践以提供更全面的保障。

Abstract: Agentic coding tools receive goals written in natural language as input, break them down into specific tasks, and write or execute the actual code with minimal human intervention. Central to this process are agent context files ("READMEs for agents") that provide persistent, project-level instructions. In this paper, we conduct the first large-scale empirical study of 2,303 agent context files from 1,925 repositories to characterize their structure, maintenance, and content. We find that these files are not static documentation but complex, difficult-to-read artifacts that evolve like configuration code, maintained through frequent, small additions. Our content analysis of 16 instruction types shows that developers prioritize functional context, such as build and run commands (62.3%), implementation details (69.9%), and architecture (67.7%). We also identify a significant gap: non-functional requirements like security (14.5%) and performance (14.5%) are rarely specified. These findings indicate that while developers use context files to make agents functional, they provide few guardrails to ensure that agent-written code is secure or performant, highlighting the need for improved tooling and practices.

</details>


### [11] [Diffploit: Facilitating Cross-Version Exploit Migration for Open Source Library Vulnerabilities](https://arxiv.org/abs/2511.12950)
*Zirui Chen,Zhipeng Xue,Jiayuan Zhou,Xing Hu,Xin Xia,Xiaohu Yang*

Main category: cs.SE

TL;DR: 本文提出Diffploit，一种基于差异驱动的迭代式漏洞利用迁移方法，通过上下文模块和迁移模块协同工作，有效解决因库版本演化导致的漏洞利用失败问题，在大规模实验中显著优于现有工具，并发现多个CVE报告错误和未记录的漏洞版本。


<details>
  <summary>Details</summary>
Motivation: 现有漏洞利用迁移方法主要依赖代码级追踪对齐，耗时且难以处理环境级失败，尤其在面对复杂触发条件变化时效果不佳。因此需要一种能同时应对触发条件变更和动态环境破坏的高效迁移方法。

Method: Diffploit包含两个核心模块：上下文模块通过分析目标版本与参考版本间的行为差异，动态构建包含失败症状及相关差异片段的上下文；迁移模块则利用这些上下文，通过基于大语言模型（LLM）的迭代反馈机制，平衡差异候选探索与逐步优化，实现漏洞利用的有效迁移。

Result: 在包含102个Java CVE和689个版本迁移任务的大规模数据集上，Diffploit成功迁移了84.2%的漏洞利用，比TARGET工具高52.0%，比IDEA中的规则工具高61.6%。此外，还识别出5个CVE受影响版本范围错误（其中3个已确认），并发现GitHub Advisory Database中111个未报告的漏洞版本。

Conclusion: Diffploit通过结合差异分析与LLM驱动的迭代迁移，显著提升了跨版本漏洞利用迁移的效率与准确性，不仅验证了其技术优越性，还在实际安全数据库中发现了重要漏洞信息错误与遗漏。

Abstract: Exploits are commonly used to demonstrate the presence of library vulnerabilities and validate their impact across different versions. However, their direct application to alternative versions often fails due to breaking changes introduced during evolution. These failures stem from both changes in triggering conditions (e.g., API refactorings) and broken dynamic environments (e.g., build or runtime errors), which are challenging to interpret and adapt manually. Existing techniques primarily focus on code-level trace alignment through fuzzing, which is both time-consuming and insufficient for handling environment-level failures. Moreover, they often fall short when dealing with complicated triggering condition changes across versions. To overcome this, we propose Diffploit, an iterative, diff-driven exploit migration method structured around two key modules: the Context Module and the Migration Module. The Context Module dynamically constructs contexts derived from analyzing behavioral discrepancies between the target and reference versions, which capture the failure symptom and its related diff hunks. Leveraging these contexts, the Migration Module guides an LLM-based adaptation through an iterative feedback loop, balancing exploration of diff candidates and gradual refinement to resolve reproduction failures effectively. We evaluate Diffploit on a large-scale dataset containing 102 Java CVEs and 689 version-migration tasks across 79 libraries. Diffploit successfully migrates 84.2% exploits, outperforming the change-aware test repair tool TARGET by 52.0% and the rule-based tool in IDEA by 61.6%. Beyond technical effectiveness, Diffploit identifies 5 CVE reports with incorrect affected version ranges, three of which have been confirmed. It also discovers 111 unreported vulnerable versions in the GitHub Advisory Database.

</details>


### [12] [SmartPoC: Generating Executable and Validated PoCs for Smart Contract Bug Reports](https://arxiv.org/abs/2511.12993)
*Longfei Chen,Ruibin Yan,Taiyu Wong,Yiyang Chen,Chao Zhang*

Main category: cs.SE

TL;DR: 本文提出了SmartPoC框架，利用大语言模型将智能合约审计报告自动转化为可执行且经过验证的PoC测试用例，有效解决了输入噪声、模型幻觉和缺少运行时预言机三大挑战，在多个基准上实现了高成功率和低成本验证。


<details>
  <summary>Details</summary>
Motivation: 智能合约审计报告通常缺乏可复现、可执行的PoC测试用例，导致自动化验证困难，依赖昂贵且临时的手动验证；现有基于大语言模型的方法面临噪声输入、幻觉和缺少运行时预言机等问题。

Method: SmartPoC首先对审计报告进行预处理以减少噪声并提取与漏洞相关的函数作为上下文；然后利用大语言模型生成PoC测试用例，并通过专门设计的执行前后修复机制确保其可编译和可运行；最后采用差分验证作为预言机来确认漏洞的可利用性。

Result: 在SmartBugs-Vul和FORGE-Vul基准上，SmartPoC分别对85.61%和86.45%的目标生成了可执行且已验证的Foundry测试用例；在Etherscan最新验证源代码语料库中，以每项发现仅0.03美元的成本确认了545项审计发现中的236个真实漏洞。

Conclusion: SmartPoC提供了一种高效、低成本的自动化方法，能将非结构化审计报告转化为可验证的PoC测试，显著提升了智能合约漏洞验证的可扩展性和可靠性。

Abstract: Smart contracts are prone to vulnerabilities and are analyzed by experts as well as automated systems, such as static analysis and AI-assisted solutions. However, audit artifacts are heterogeneous and often lack reproducible, executable PoC tests suitable for automated validation, leading to costly, ad hoc manual verification. Large language models (LLMs) can be leveraged to turn audit reports into PoC test cases, but have three major challenges: noisy inputs, hallucinations, and missing runtime oracles. In this paper, we present SmartPoC, an automated framework that converts textual audit reports into executable, validated test cases. First, the input audit report is processed to reduce noise, and only bug-related functions are extracted and fed to LLMs as context. To curb hallucinations and ensure compile-and-run readiness, we leverage LLMs to synthesize PoC test cases with specially-designed pre-/post-execution repair. We further utilize differential verification as oracles to confirm exploitability of the PoC test cases. On the SmartBugs-Vul and FORGE-Vul benchmarks, SmartPoC generates executable, validated Foundry test cases for 85.61% and 86.45% of targets, respectively. Applied to the latest Etherscan verified-source corpus, SmartPoC confirms 236 real bugs out of 545 audit findings at a cost of only $0.03 per finding.

</details>


### [13] [Towards Requirements Engineering for GenAI-Enabled Software: Bridging Responsibility Gaps through Human Oversight Requirements](https://arxiv.org/abs/2511.13069)
*Zhenyu Mao,Jacky Keung,Yicheng Sun,Yifei Wang,Shuo Liu,Jialong Li*

Main category: cs.SE

TL;DR: 本文提出了一种三层设计方法论，用于在生成式人工智能（GenAI）软件系统中系统化地识别、概念化和表示责任缺口，并通过用户研究验证了其相较于传统目标导向需求工程方法的优越性。


<details>
  <summary>Details</summary>
Motivation: 现有需求工程方法在应对GenAI系统中日益突出的责任缺口问题时存在概念、方法和工件层面的不足，亟需建立从人类监督需求角度系统分析责任缺口的新视角。

Method: 该方法论包含三个分析层：概念层定义人与系统维度中的责任要素及其交互产生的责任缺口；方法层通过演绎流程识别责任缺口并导出监督需求；工件层以“演绎主干表”形式形式化表达从缺口识别到需求推导的路径。

Result: 用户研究在两个场景下比较了所提方法与基线目标导向RE方法，在六个维度上均显示出明显改进，证实了该方法在弥补三类研究缺口方面的有效性。

Conclusion: 该研究为GenAI赋能软件系统中的责任缺口问题提供了系统化的需求工程解决方案，增强了人类监督需求的可追溯性与可操作性。

Abstract: Context: Responsibility gaps, long-recognized challenges in socio-technical systems where accountability becomes diffuse or ambiguous, have become increasingly pronounced in GenAI-enabled software. The generative and adaptive nature complicates how human oversight and responsibility are specified, delegated, and traced. Existing requirements engineering (RE) approaches remain limited in addressing these phenomena, revealing conceptual, methodological, and artifact-level research gaps.. Objective: This study aims to analyze these research gaps in the context of GenAI-enabled software systems. It seeks to establish a coherent perspective for a systematic analysis of responsibility gaps from a human oversight requirements standpoint, encompassing how these responsibility gaps should be conceptualized, identified, and represented throughout the RE process. Methods: The proposed design methodology is structured across three analytical layers. At the conceptualization layer, it establishes a conceptual framing that defines the key elements of responsibility across the human and system dimensions and explains how potential responsibility gaps emerge from their interactions. At the methodological layer, it introduces a deductive pipeline for identifying responsibility gaps by analyzing interactions between these dimensions and deriving corresponding oversight requirements within established RE frameworks. At the artifact layer, it formalizes the results in a Deductive Backbone Table, a reusable representation that traces the reasoning path from responsibility gaps identification to human oversight requirements derivation. Results: A user study compared the proposed methodology with a baseline goal-oriented RE across two scenarios. Evaluation across six dimensions indicated clear improvements of the proposed methodology, confirming its effectiveness in addressing three research gaps.

</details>


### [14] [Examining the Usage of Generative AI Models in Student Learning Activities for Software Programming](https://arxiv.org/abs/2511.13271)
*Rufeng Chen,Shuaishuai Jiang,Jiyun Shen,AJung Moon,Lili Wei*

Main category: cs.SE

TL;DR: 本研究发现，尽管生成式AI（如ChatGPT）能显著提升编程任务表现（尤其对初学者），但并不总能带来知识增长；过度依赖或极少使用均不利于学习，建议将其作为学习工具而非解题工具，并呼吁教育者提供使用指导。


<details>
  <summary>Details</summary>
Motivation: 现有研究多关注生成式AI在完成教育任务和影响学生表现方面的能力，却忽视了其对知识获取的影响。因此，本文旨在探究生成式AI与传统在线资源在支持不同编程水平学生知识增长方面的差异。

Method: 研究通过一项对照用户实验，招募24名具有不同编程经验（初级、中级）的本科生，在解决编程任务时分析他们与ChatGPT的互动行为，并评估其任务表现、概念理解及交互模式。

Result: 结果表明，使用生成式AI生成完整解决方案可显著提高任务表现（尤其对初学者），但并未持续促进知识增长。初学者倾向于依赖AI完成任务而缺乏知识内化，中级学生则采取更选择性的策略。过度依赖或极少使用AI均导致较弱的知识获取效果。

Conclusion: 应将生成式AI视为学习工具而非解题工具，教育者需提供明确指导，以帮助学生在编程教育中有效利用AI，促进深层次理解。

Abstract: The rise of Generative AI (GenAI) tools like ChatGPT has created new opportunities and challenges for computing education. Existing research has primarily focused on GenAI's ability to complete educational tasks and its impact on student performance, often overlooking its effects on knowledge gains. In this study, we investigate how GenAI assistance compares to conventional online resources in supporting knowledge gains across different proficiency levels. We conducted a controlled user experiment with 24 undergraduate students of two different levels of programming experience (beginner, intermediate) to examine how students interact with ChatGPT while solving programming tasks. We analyzed task performance, conceptual understanding, and interaction behaviors. Our findings reveal that generating complete solutions with GenAI significantly improves task performance, especially for beginners, but does not consistently result in knowledge gains. Importantly, usage strategies differ by experience: beginners tend to rely heavily on GenAI toward task completion often without knowledge gain in the process, while intermediates adopt more selective approaches. We find that both over-reliance and minimal use result in weaker knowledge gains overall. Based on our results, we call on students and educators to adopt GenAI as a learning rather than a problem solving tool. Our study highlights the urgent need for guidance when integrating GenAI into programming education to foster deeper understanding.

</details>


### [15] [SAINT: Service-level Integration Test Generation with Program Analysis and LLM-based Agents](https://arxiv.org/abs/2511.13305)
*Rangeet Pan,Raju Pavuluri,Ruikai Huang,Rahul Krishna,Tyler Stennett,Alessandro Orso,Saurabh SInha*

Main category: cs.SE

TL;DR: SAINT 是一种结合静态分析与大语言模型（LLM）智能体的白盒测试方法，用于自动生成企业级 Java 应用的服务端点和场景化测试用例，显著提升覆盖率、缺陷检测能力及开发者认可度。


<details>
  <summary>Details</summary>
Motivation: 现有服务级测试工具（尤其针对 RESTful API）依赖 OpenAPI 规范或模糊测试，在真实企业代码库中难以适用，且难以生成有效覆盖有意义业务场景的功能性测试。

Method: SAINT 通过静态分析构建端点模型和操作依赖图，并利用 LLM 智能体在规划、执行与反思循环中生成两类测试：以覆盖代码和数据库交互为目标的端点测试，以及从代码中提取用例并精炼成可执行测试的场景化测试。

Result: 在八个 Java 应用（含一个专有企业应用）上的实验表明，SAINT 在测试覆盖率、缺陷发现能力和场景生成方面表现优异；开发者调查也高度认可其生成的场景化测试。

Conclusion: 将静态分析与基于智能体的 LLM 工作流相结合，能够实现更有效、功能性更强且更贴近开发者需求的服务级测试生成。

Abstract: Enterprise applications are typically tested at multiple levels, with service-level testing playing an important role in validating application functionality. Existing service-level testing tools, especially for RESTful APIs, often employ fuzzing and/or depend on OpenAPI specifications which are not readily available in real-world enterprise codebases. Moreover, these tools are limited in their ability to generate functional tests that effectively exercise meaningful scenarios. In this work, we present SAINT, a novel white-box testing approach for service-level testing of enterprise Java applications. SAINT combines static analysis, large language models (LLMs), and LLM-based agents to automatically generate endpoint and scenario-based tests. The approach builds two key models: an endpoint model, capturing syntactic and semantic information about service endpoints, and an operation dependency graph, capturing inter-endpoint ordering constraints. SAINT then employs LLM-based agents to generate tests. Endpoint-focused tests aim to maximize code and database interaction coverage. Scenario-based tests are synthesized by extracting application use cases from code and refining them into executable tests via planning, action, and reflection phases of the agentic loop. We evaluated SAINT on eight Java applications, including a proprietary enterprise application. Our results illustrate the effectiveness of SAINT in coverage, fault detection, and scenario generation. Moreover, a developer survey provides strong endorsement of the scenario-based tests generated by SAINT. Overall, our work shows that combining static analysis with agentic LLM workflows enables more effective, functional, and developer-aligned service-level test generation.

</details>


### [16] [LinkXplore: A Framework for Affordable High-Quality Blockchain Data](https://arxiv.org/abs/2511.13318)
*Peihao Li*

Main category: cs.SE

TL;DR: LinkXplore 是一个开源框架，用于低成本地收集和管理链上数据，绕过昂贵的区块链数据提供商，支持灵活集成各类链数据模块。


<details>
  <summary>Details</summary>
Motivation: 大规模区块链数据采集成本高昂，多数 RPC 提供商的高级 API 定价过高，限制了预算有限的研究与工业应用；同时缺乏可灵活扩展的系统性分析框架。

Method: 提出 LinkXplore 框架，通过直接解析 RPC 查询或数据流中的原始数据，结合简易 API 与后端处理逻辑，实现低成本、高质量的链上数据获取与模块化集成。

Result: LinkXplore 能以远低于商业 API 的成本提供高质量链上数据，并支持任意类型链数据的灵活接入，适用于资源受限的研究者与开发者。

Conclusion: LinkXplore 为区块链数据采集提供了一个经济、开放且可扩展的解决方案，有助于加速学术研究与产品开发。

Abstract: Blockchain technologies are rapidly transforming both academia and industry. However, large-scale blockchain data collection remains prohibitively expensive, as many RPC providers only offer enhanced APIs with high pricing tiers that are unsuitable for budget-constrained research or industrial-scale applications, which has significantly slowed down academic studies and product development. Moreover, there is a clear lack of a systematic framework that allows flexible integration of new modules for analyzing on-chain data.
  To address these challenges, we introduce LinkXplore, the first open framework for collecting and managing on-chain data. LinkXplore enables users to bypass costly blockchain data providers by directly analyzing raw data from RPC queries or streams, thereby offering high-quality blockchain data at a fraction of the cost. Through a simple API and backend processing logic, any type of chain data can be integrated into the framework. This makes it a practical alternative for both researchers and developers with limited budgets. Code and dataset used in this project are publicly available at https://github.com/Linkis-Project/LinkXplore

</details>


### [17] [An LLM-based Quantitative Framework for Evaluating High-Stealthy Backdoor Risks in OSS Supply Chains](https://arxiv.org/abs/2511.13341)
*Zihe Yan,Kai Luo,Haoyu Yang,Yang Yu,Zhuosheng Zhang,Guancheng Li*

Main category: cs.SE

TL;DR: 本文提出了一种细粒度的开源软件项目评估框架，用于检测高隐蔽性后门攻击风险，结合大语言模型对代码仓库进行语义评估，并在 Debian 生态系统中验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 现代软件开发广泛依赖开源组件，但底层依赖维护不足和社区审计缺失导致源码安全性和维护者合法性难以保障，尤其在类似 XZ-Util 事件的高隐蔽性后门攻击下风险加剧。

Method: 构建一个从攻击者视角建模后门攻击阶段的评估框架，定义各阶段针对性指标，并利用大语言模型对代码仓库进行无需人工规则的语义评估，以克服静态分析在评估维护活动（如提交者权限异常提升、评审参与度低）方面的局限。

Result: 在 Debian 生态系统中对 66 个高优先级软件包的实验表明，当前开源软件供应链面临多种安全风险。

Conclusion: 所提出的基于大语言模型的细粒度评估框架能有效识别开源项目中的后门风险，揭示了现有开源供应链在安全性方面的严重隐患。

Abstract: In modern software development workflows, the open-source software supply chain contributes significantly to efficient and convenient engineering practices. With increasing system complexity, using open-source software as third-party dependencies has become a common practice. However, the lack of maintenance for underlying dependencies and insufficient community auditing create challenges in ensuring source code security and the legitimacy of repository maintainers, especially under high-stealthy backdoor attacks exemplified by the XZ-Util incident. To address these problems, we propose a fine-grained project evaluation framework for backdoor risk assessment in open-source software. The framework models stealthy backdoor attacks from the viewpoint of the attacker and defines targeted metrics for each attack stage. In addition, to overcome the limitations of static analysis in assessing the reliability of repository maintenance activities such as irregular committer privilege escalation and limited participation in reviews, the framework uses large language models (LLMs) to conduct semantic evaluation of code repositories without relying on manually crafted patterns. The framework is evaluated on sixty six high-priority packages in the Debian ecosystem. The experimental results indicate that the current open-source software supply chain is exposed to various security risks.

</details>


### [18] [FLOWER: Flow-Oriented Entity-Relationship Tool](https://arxiv.org/abs/2511.13357)
*Dmitry Moskalev*

Main category: cs.SE

TL;DR: 本文提出了FLOWER，一种端到端的面向流程的实体关系建模工具，能自动检测数据库约束并动态构建显式与隐式依赖关系，显著优于现有方法，在分布表示、约束学习和数据叙事方面均取得更优性能。


<details>
  <summary>Details</summary>
Motivation: 传统实体关系模型构建高度依赖人工，难以高效处理大规模合成与有机数据中的显式和隐式依赖关系，亟需自动化、可扩展的解决方案以提升数据理解和洞察力。

Method: FLOWER采用动态采样与鲁棒数据分析技术，自动识别SQL数据库中的内置约束，并实时构建正确且必要的实体关系模型，支持SQL和自然语言进行数据叙事，兼容CPU/GPU并支持23种语言。

Result: 在STATS基准测试中，FLOWER在分布表示上比蓄水池采样快2.4倍，约束学习快2.6倍且加速2.15倍；在数据叙事方面，准确率提升1.19倍，上下文长度减少1.86倍，优于大语言模型（LLM）。

Conclusion: FLOWER是一种高效、可扩展且实用的实体关系建模工具，能够显著提升真实世界数据处理的质量与效率，适用于多种应用场景。

Abstract: Exploring relationships across data sources is a crucial optimization for entities recognition. Since databases can store big amount of information with synthetic and organic data, serving all quantity of objects correctly is an important task to deal with. However, the decision of how to construct entity relationship model is associated with human factor. In this paper, we present flow-oriented entity-relationship tool. This is first and unique end-to-end solution that eliminates routine and resource-intensive problems of processing, creating and visualizing both of explicit and implicit dependencies for prominent SQL dialects on-the-fly. Once launched, FLOWER automatically detects built-in constraints and starting to create own correct and necessary one using dynamic sampling and robust data analysis techniques. This approach applies to improve entity-relationship model and data storytelling to better understand the foundation of data and get unseen insights from DB sources using SQL or natural language. Evaluated on state-of-the-art STATS benchmark, experiments show that FLOWER is superior to reservoir sampling by 2.4x for distribution representation and 2.6x for constraint learning with 2.15x acceleration. For data storytelling, our tool archives 1.19x for accuracy enhance with 1.86x context decrease compare to LLM. Presented tool is also support 23 languages and compatible with both of CPU and GPU. Those results show that FLOWER can manage with real-world data a way better to ensure with quality, scalability and applicability for different use-cases.

</details>


### [19] [BIOMERO 2.0: end-to-end FAIR infrastructure for bioimaging data import, analysis, and provenance](https://arxiv.org/abs/2511.13611)
*Torec T. Luik,Joost de Folter,Rodrigo Rosas-Bertolini,Eric A. J. Reits,Ron A. Hoebe,Przemek M. Krawczyk*

Main category: cs.SE

TL;DR: BIOMERO 2.0 enhances OMERO into a FAIR-compliant, provenance-aware bioimaging platform by integrating containerized import, preprocessing, analysis, and real-time workflow tracking via an OMERO.web plugin.


<details>
  <summary>Details</summary>
Motivation: To transform OMERO into a FAIR-compliant and provenance-aware platform that supports traceable, reusable bioimaging workflows from data acquisition through analysis and sharing.

Method: BIOMERO 2.0 uses an OMERO.web plugin with containerized components: an importer subsystem for in-place data import and metadata enrichment, and an analyzer subsystem that coordinates and tracks containerized analyses on HPC systems using the BIOMERO Python library, while recording full provenance.

Result: The system enables end-to-end provenance tracking, integrates preprocessing and analysis into OMERO, and enhances FAIR compliance by making workflows traceable, interoperable, and reusable.

Conclusion: BIOMERO 2.0 successfully positions OMERO at the core of the bioimaging analysis lifecycle, bridging data import, processing, and sharing with robust provenance and FAIR support.

Abstract: We present BIOMERO 2.0, a major evolution of the BIOMERO framework that transforms OMERO into a FAIR-compliant (findable, accessible, interoperable, and reusable), provenance-aware bioimaging platform. BIOMERO 2.0 integrates data import, preprocessing, analysis, and workflow monitoring through an OMERO.web plugin and containerized components. The importer subsystem facilitates in-place import using containerized preprocessing and metadata enrichment via forms, while the analyzer subsystem coordinates and tracks containerized analyses on high-performance computing systems via the BIOMERO Python library. All imports and analyses are recorded with parameters, versions, and results, ensuring real-time provenance accessible through integrated dashboards. This dual approach places OMERO at the heart of the bioimaging analysis process: the importer ensures provenance from image acquisition through preprocessing and import into OMERO, while the analyzer records it for downstream processing. These integrated layers enhance OMEROs FAIRification, supporting traceable, reusable workflows for image analysis that bridge the gap between data import, analysis, and sharing.

</details>


### [20] [Live-SWE-agent: Can Software Engineering Agents Self-Evolve on the Fly?](https://arxiv.org/abs/2511.13646)
*Chunqiu Steven Xia,Zhe Wang,Yan Yang,Yuxiang Wei,Lingming Zhang*

Main category: cs.SE

TL;DR: 本文提出了Live-SWE-agent，这是首个能在运行时自主持续进化的软件智能体。它从仅具备基础bash工具的简单结构出发，在解决真实软件问题的过程中不断自我演化，无需离线训练即可在SWE-bench Verified和SWE-Bench Pro基准上取得领先性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于大语言模型的软件智能体通常需要专门设计且难以穷尽所有架构可能性，而自改进智能体又依赖昂贵的离线训练且泛化能力有限。因此，亟需一种能在运行时自主演化、无需预训练且具有良好泛化能力的软件智能体。

Method: Live-SWE-agent从最简智能体结构（如mini-SWE-agent）开始，仅提供bash工具访问权限，在解决真实软件任务的过程中动态地自我修改和优化其智能体架构，实现在线持续进化。

Result: 在SWE-bench Verified基准上，Live-SWE-agent在无测试时扩展的情况下达到75.4%的解决率，超越所有开源软件智能体并接近最佳闭源方案；在SWE-Bench Pro基准上以45.8%的解决率优于当前最先进的手工设计智能体。

Conclusion: Live-SWE-agent展示了运行时自主演化的可行性与优越性，为构建高效、通用且无需大量离线训练的软件智能体提供了新范式。

Abstract: Large Language Models (LLMs) are reshaping almost all industries, including software engineering. In recent years, a number of LLM agents have been proposed to solve real-world software problems. Such software agents are typically equipped with a suite of coding tools and can autonomously decide the next actions to form complete trajectories to solve end-to-end software tasks. While promising, they typically require dedicated design and may still be suboptimal, since it can be extremely challenging and costly to exhaust the entire agent scaffold design space. Recognizing that software agents are inherently software themselves that can be further refined/modified, researchers have proposed a number of self-improving software agents recently, including the Darwin-Gödel Machine (DGM). Meanwhile, such self-improving agents require costly offline training on specific benchmarks and may not generalize well across different LLMs or benchmarks. In this paper, we propose Live-SWE-agent, the first live software agent that can autonomously and continuously evolve itself on-the-fly during runtime when solving real-world software problems. More specifically, Live-SWE-agent starts with the most basic agent scaffold with only access to bash tools (e.g., mini-SWE-agent), and autonomously evolves its own scaffold implementation while solving real-world software problems. Our evaluation on the widely studied SWE-bench Verified benchmark shows that Live-SWE-agent can achieve an impressive solve rate of 75.4% without test-time scaling, outperforming all existing open-source software agents and approaching the performance of the best proprietary solution. Moreover, Live-SWE-agent outperforms state-of-the-art manually crafted software agents on the recent SWE-Bench Pro benchmark, achieving the best-known solve rate of 45.8%.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [21] [From Single to Societal: Analyzing Persona-Induced Bias in Multi-Agent Interactions](https://arxiv.org/abs/2511.11789)
*Jiayi Li,Xiao Liu,Yansong Feng*

Main category: cs.MA

TL;DR: 本文系统研究了基于大语言模型的多智能体系统中角色设定（personas）引发的偏见问题，发现具有历史上优势群体特征的角色在信任度和坚持性方面表现出系统性偏差，并存在明显的内群体偏好。


<details>
  <summary>Details</summary>
Motivation: 当前多智能体系统常通过赋予智能体角色以增强行为多样性，但角色是否引入偏见尚未被充分研究。作者旨在揭示角色设定对多智能体交互中社会属性（如信任度与坚持性）的影响，以评估其公平性风险。

Method: 通过在协作问题解决与说服任务中设计受控实验，分析不同角色（如性别、种族）对LLM智能体在信任度和坚持性上的影响，并考察模型、群体规模和交互轮次等因素下的偏见稳定性。

Result: (1) 具有历史上优势群体特征（如男性、白人）的角色被其他智能体视为更值得信赖且更具坚持性；(2) 智能体显著倾向于顺从与其角色相同（同群体）的其他智能体，表现出内群体偏好。这些偏见在不同模型和实验设置下均持续存在。

Conclusion: 角色设定会在多智能体交互中引入系统性偏见，可能影响系统的公平性与可靠性，亟需引起重视并开发相应的缓解策略。

Abstract: Large Language Model (LLM)-based multi-agent systems are increasingly used to simulate human interactions and solve collaborative tasks. A common practice is to assign agents with personas to encourage behavioral diversity. However, this raises a critical yet underexplored question: do personas introduce biases into multi-agent interactions? This paper presents a systematic investigation into persona-induced biases in multi-agent interactions, with a focus on social traits like trustworthiness (how an agent's opinion is received by others) and insistence (how strongly an agent advocates for its opinion). Through a series of controlled experiments in collaborative problem-solving and persuasion tasks, we reveal that (1) LLM-based agents exhibit biases in both trustworthiness and insistence, with personas from historically advantaged groups (e.g., men and White individuals) perceived as less trustworthy and demonstrating less insistence; and (2) agents exhibit significant in-group favoritism, showing a higher tendency to conform to others who share the same persona. These biases persist across various LLMs, group sizes, and numbers of interaction rounds, highlighting an urgent need for awareness and mitigation to ensure the fairness and reliability of multi-agent systems.

</details>


### [22] [Conflict-Free Flight Scheduling Using Strategic Demand Capacity Balancing for Urban Air Mobility Operations](https://arxiv.org/abs/2511.11854)
*Vahid Hemmati,Yonas Ayalew,Ahmad Mohammadi,Reza Ahmari,Parham Kebria,Abdollah Homaifar,Mehrdad Saif*

Main category: cs.MA

TL;DR: 本文提出了一种无冲突的多智能体飞行调度方法，通过基于延迟起飞的成对冲突避免（PCA）策略，在城市空中交通（UAM）受限空域中实现安全间隔，并扩展至多智能体场景，显著降低总延误同时保证无碰撞运行。


<details>
  <summary>Details</summary>
Motivation: 为应对城市空中交通（UAM）在受限空域中日益增长的飞行器密度所带来的冲突风险，亟需一种可扩展且高效的飞行调度方法，以确保多智能体系统中的安全分离与运行效率。

Method: 首先提出基于延迟起飞的成对冲突避免（PCA）机制，利用运动学原理维持安全距离；随后将其扩展至多智能体场景，构建优化模型以系统性地确定各飞行器的起飞时间，并通过平均延误等指标评估性能。

Result: 数值仿真表明，在多种多智能体环境和真实UAM应用场景下，该方法在确保无碰撞运行的同时，显著降低了总延误。

Conclusion: 该方法为新兴的城市空中交通系统提供了一个可扩展、高效且安全的飞行调度框架。

Abstract: In this paper, we propose a conflict-free multi- agent flight scheduling that ensures robust separation in con- strained airspace for Urban Air Mobility (UAM) operations application. First, we introduce Pairwise Conflict Avoidance (PCA) based on delayed departures, leveraging kinematic principles to maintain safe distances. Next, we expand PCA to multi-agent scenarios, formulating an optimization approach that systematically determines departure times under increasing traffic densities. Performance metrics, such as average delay, assess the effectiveness of our solution. Through numerical simulations across diverse multi-agent environments and real- world UAM use cases, our method demonstrates a significant reduction in total delay while ensuring collision-free operations. This approach provides a scalable framework for emerging urban air mobility systems.

</details>


### [23] [Goal-Oriented Multi-Agent Reinforcement Learning for Decentralized Agent Teams](https://arxiv.org/abs/2511.11992)
*Hung Du,Hy Nguyen,Srikanth Thudumu,Rajesh Vasa,Kon Mouzakis*

Main category: cs.MA

TL;DR: 本文提出了一种去中心化的多智能体强化学习框架，通过目标感知的通信策略，使陆、水、空自主载具在受限环境下高效协作，显著提升任务成功率与可扩展性。


<details>
  <summary>Details</summary>
Motivation: 现实世界中，跨域自主载具常面临动态不可预测环境、通信受限、无中心控制和部分可观测性等挑战，尤其在各载具追求个体目标时难以有效协调。

Method: 提出一种去中心化的多智能体强化学习（MARL）框架，引入基于局部目标与观测的目标感知通信机制，使智能体仅共享相关信息以增强协作。

Result: 在包含障碍物和动态智能体数量的复杂导航任务中，该方法相比非协作基线显著提高了任务成功率并缩短了到达目标时间，且在智能体数量增加时性能保持稳定。

Conclusion: 去中心化、目标驱动的MARL框架能有效支持跨域多载具系统在现实条件下的协调，具备良好的实用性与可扩展性。

Abstract: Connected and autonomous vehicles across land, water, and air must often operate in dynamic, unpredictable environments with limited communication, no centralized control, and partial observability. These real-world constraints pose significant challenges for coordination, particularly when vehicles pursue individual objectives. To address this, we propose a decentralized Multi-Agent Reinforcement Learning (MARL) framework that enables vehicles, acting as agents, to communicate selectively based on local goals and observations. This goal-aware communication strategy allows agents to share only relevant information, enhancing collaboration while respecting visibility limitations. We validate our approach in complex multi-agent navigation tasks featuring obstacles and dynamic agent populations. Results show that our method significantly improves task success rates and reduces time-to-goal compared to non-cooperative baselines. Moreover, task performance remains stable as the number of agents increases, demonstrating scalability. These findings highlight the potential of decentralized, goal-driven MARL to support effective coordination in realistic multi-vehicle systems operating across diverse domains.

</details>


### [24] [FINRS: A Risk-Sensitive Trading Framework for Real Financial Markets](https://arxiv.org/abs/2511.12599)
*Bijia Liu,Ronghao Dang*

Main category: cs.MA

TL;DR: 本文提出FinRS，一种结合多层次市场分析、双决策智能体和多时间尺度奖励反馈的风险敏感型交易框架，在盈利性和稳定性上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于大语言模型的交易智能体主要关注单步预测，缺乏整合的风险管理机制，在波动市场中效果有限。

Method: 提出FinRS框架，融合多层次市场分析、双决策智能体和多时间尺度奖励反馈，以兼顾收益目标与下行风险约束。

Result: 在多种股票和市场条件下实验表明，FinRS在盈利能力和稳定性方面优于当前最先进的方法。

Conclusion: FinRS通过引入风险敏感机制显著提升了大语言模型在金融交易中的实际表现，为构建更稳健的智能交易系统提供了新思路。

Abstract: Large language models (LLMs) have shown strong reasoning capabilities and are increasingly explored for financial trading. Existing LLM-based trading agents, however, largely focus on single-step prediction and lack integrated mechanisms for risk management, which reduces their effectiveness in volatile markets. We introduce FinRS, a risk-sensitive trading framework that combines hierarchical market analysis, dual-decision agents, and multi-timescale reward reflection to align trading actions with both return objectives and downside risk constraints. Experiments on multiple stocks and market conditions show that FinRS achieves superior profitability and stability compared to state-of-the-art methods.

</details>


### [25] [ENGRAM: Effective, Lightweight Memory Orchestration for Conversational Agents](https://arxiv.org/abs/2511.12960)
*Daivik Patel,Shrenik Patel*

Main category: cs.MA

TL;DR: ENGRAM 是一种轻量级记忆系统，通过将对话内容划分为三种标准记忆类型（情景、语义和程序性），利用单一路由与检索机制，在显著减少 token 使用的同时，在长时记忆任务上达到当前最优性能。


<details>
  <summary>Details</summary>
Motivation: 现有面向用户的大语言模型记忆系统架构复杂（如知识图谱、多阶段检索等），带来工程实现和可复现性难题，亟需一种更简洁高效的记忆管理方案。

Method: ENGRAM 将每轮用户交互转化为具有标准化模式和嵌入的类型化记忆记录，存储于数据库中；查询时对每种记忆类型进行稠密检索，通过简单集合操作融合结果，并将最相关证据作为上下文输入模型。

Result: 在 LoCoMo 基准上达到 SOTA，在 LongMemEval 上以仅约 1% 的 token 使用量超越全上下文基线 15 分。

Conclusion: 精细的记忆类型划分结合简单的稠密检索机制，即可实现高效长期记忆管理，无需依赖复杂架构。

Abstract: Large language models (LLMs) deployed in user-facing applications require long-horizon consistency: the ability to remember prior interactions, respect user preferences, and ground reasoning in past events. However, contemporary memory systems often adopt complex architectures such as knowledge graphs, multi-stage retrieval pipelines, and OS-style schedulers, which introduce engineering complexity and reproducibility challenges. We present ENGRAM, a lightweight memory system that organizes conversation into three canonical memory types (episodic, semantic, and procedural) through a single router and retriever. Each user turn is converted into typed memory records with normalized schemas and embeddings and stored in a database. At query time, the system retrieves top-k dense neighbors for each type, merges results with simple set operations, and provides the most relevant evidence as context to the model. ENGRAM attains state-of-the-art results on LoCoMo, a multi-session conversational QA benchmark for long-horizon memory, and exceeds the full-context baseline by 15 points on LongMemEval while using only about 1% of the tokens. These results show that careful memory typing and straightforward dense retrieval can enable effective long-term memory management in language models without requiring complex architectures.

</details>


### [26] [Reuse, Don't Recompute: Efficient Large Reasoning Model Inference via Memory Orchestration](https://arxiv.org/abs/2511.12987)
*Daivik Patel,Shrenik Patel*

Main category: cs.MA

TL;DR: ENGRAM-R 是一种推理时记忆层，通过结构化记忆重用显著减少大推理模型的 token 使用和延迟，同时保持甚至提升准确性。


<details>
  <summary>Details</summary>
Motivation: 大推理模型（LRMs）在测试时通过生成更长的思维链或采样多个解来提高准确性，但代价是高昂的 token 消耗和延迟。作者认为，若已有证据存在，模型应通过复用结构化记忆而非重复计算来实现高效推理。

Method: 提出 ENGRAM-R，一种推理时记忆层，结合类型化检索、紧凑的事实卡片表示和显式引用控制，以复用先前推导结果。

Result: 在 LoCoMo 基准上，相比完整上下文，ENGRAM-R 减少 85% 的输入 token 和 75% 的推理 token，同时保持高准确率；在 LongMemEval 的多跳子集上，也实现了类似效率并显著提升准确率。

Conclusion: 记忆不仅是实现长期推理正确性的关键，也是在计算、内存和延迟受限条件下提升推理效率的实用手段。

Abstract: Large reasoning models (LRMs) achieve strong accuracy through test-time scaling, generating longer chains of thought or sampling multiple solutions, but at steep costs in tokens and latency. We argue that memory is a core ingredient for efficient reasoning: when evidence already exists, models should think less by reusing structured memory instead of recomputing derivations. We present ENGRAM-R, an inference-time memory layer that integrates typed retrieval with compact fact card representations and explicit citation control. On the LoCoMo benchmark, ENGRAM-R reduces input tokens by 85% and reasoning tokens by 75% compared to full context while maintaining high accuracy. On a multi-hop slice of the LongMemEval benchmark, it achieves similar efficiency with substantial accuracy gains. These results show that memory is not only critical for long-horizon correctness but also a practical lever for efficient reasoning under tight compute, memory, and latency budgets.

</details>


### [27] [LLM-based Multi-Agent System for Simulating Strategic and Goal-Oriented Data Marketplaces](https://arxiv.org/abs/2511.13233)
*Jun Sashihara,Yukihisa Fujita,Kota Nakamura,Masahiro Kuwahara,Teruaki Hayashi*

Main category: cs.MA

TL;DR: 本文提出了一种基于大语言模型的多智能体系统（LLM-MAS）用于模拟数据市场中的买卖行为，通过赋予买家和卖家智能体自主决策与策略调整能力，更真实地再现了实际数据市场的交易模式和趋势演化。


<details>
  <summary>Details</summary>
Motivation: 现有对数据市场中参与者、数据与监管之间互动关系的理解仍不系统，传统基于规则的模拟方法难以捕捉复杂且动态的市场行为。

Method: 构建一个由大语言模型驱动的多智能体系统（LLM-MAS），其中买家和卖家智能体具备明确目标，能自主执行规划、搜索、购买、定价和更新数据等策略性行为，并通过自然语言推理适应市场变化。

Result: 通过三项分布指标（每数据集购买次数、每位买家购买次数、同一数据集重复购买次数）的仿真实验表明，LLM-MAS相比传统方法更能真实复现真实数据市场的交易模式，并能捕捉市场趋势的涌现与演化。

Conclusion: LLM-MAS为数据市场研究提供了一个更具适应性和表现力的模拟框架，有助于深入理解市场动态及其演化机制。

Abstract: Data marketplaces, which mediate the purchase and exchange of data from third parties, have attracted growing attention for reducing the cost and effort of data collection while enabling the trading of diverse datasets. However, a systematic understanding of the interactions between market participants, data, and regulations remains limited. To address this gap, we propose a Large Language Model-based Multi-Agent System (LLM-MAS) for data marketplaces. In our framework, buyer and seller agents powered by LLMs operate with explicit objectives and autonomously perform strategic actions, such as planning, searching, purchasing, pricing, and updating data. These agents can reason about market dynamics, forecast future demand, and adjust strategies accordingly. Unlike conventional model-based simulations, which are typically constrained to predefined rules, LLM-MAS supports broader and more adaptive behavior selection through natural language reasoning. We evaluated the framework via simulation experiments using three distribution-based metrics: (1) the number of purchases per dataset, (2) the number of purchases per buyer, and (3) the number of repeated purchases of the same dataset. The results demonstrate that LLM-MAS more faithfully reproduces trading patterns observed in real data marketplaces compared to traditional approaches, and further captures the emergence and evolution of market trends.

</details>


### [28] [How Hard is it to Explain Preferences Using Few Boolean Attributes?](https://arxiv.org/abs/2511.13445)
*Clemens Anzinger,Jiehua Chen,Christian Hatschka,Manuel Sorge,Alexander Temper*

Main category: cs.MA

TL;DR: 本文研究了用布尔属性模型（BAM）解释偏好数据的计算复杂性，证明当属性数k≤2时问题可在线性时间内求解，而k≥3时为NP完全；同时分析了部分信息已知情形下的两个变体问题的复杂性。


<details>
  <summary>Details</summary>
Motivation: 属性模型在理解偏好结构和提升决策效率方面具有潜力，因此有必要研究用布尔属性模型解释给定偏好数据的计算复杂性。

Method: 通过复杂性分析，对不同参数（如属性数量k、备选项数量m、投票者数量等）下的BAM问题及其变体（BAM WITH CARES和BAM WITH HAS）进行分类，采用归约和算法设计方法。

Result: 建立了关于属性数k的复杂性二分法：k≤2时线性时间可解，k≥3时NP完全；即使偏好序长度为2仍为NP难；以备选项数m为参数时固定参数可解；两人情形存在线性时间算法；变体问题中，BAM WITH CARES通常更难，而BAM WITH HAS除单投票者情形外更易处理。

Conclusion: 布尔属性模型解释偏好数据的问题在一般情况下计算困难，但在特定参数或限制条件下具备高效算法，为实际应用提供了理论依据。

Abstract: We study the computational complexity of explaining preference data through Boolean attribute models (BAMs), motivated by extensive research involving attribute models and their promise in understanding preference structure and enabling more efficient decision-making processes. In a BAM, each alternative has a subset of Boolean attributes, each voter cares about a subset of attributes, and voters prefer alternatives with more of their desired attributes. In the BAM problem, we are given a preference profile and a number k, and want to know whether there is a Boolean k-attribute model explaining the profile.
  We establish a complexity dichotomy for the number of attributes k: BAM is linear-time solvable for $k \le 2$ but NP-complete for $k \ge 3$. The problem remains hard even when preference orders have length two. On the positive side, BAM becomes fixed-parameter tractable when parameterized by the number of alternatives m. For the special case of two voters, we provide a linear-time algorithm.
  We also analyze variants where partial information is given: When voter preferences over attributes are known (BAM WITH CARES) or when alternative attributes are specified (BAM WITH HAS), we show that for most parameters BAM WITH CARES is more difficult whereas BAM WITH HAS is more tractable except for being NP-hard even for one voter.

</details>


### [29] [Market-Dependent Communication in Multi-Agent Alpha Generation](https://arxiv.org/abs/2511.13614)
*Jerick Shi,Burton Hollifield*

Main category: cs.MA

TL;DR: 该研究通过51个月内450次基于大语言模型的多智能体交易实验，发现沟通能提升对冲基金策略表现，但最优沟通方式取决于市场特性：竞争性对话在波动大的科技股中表现最佳，协作性对话在稳定股票中占优，而金融股则对所有沟通干预无响应；此外，策略趋同现象普遍存在，且对话质量与收益无关。


<details>
  <summary>Details</summary>
Motivation: 探讨多策略对冲基金内部分析师是否应沟通及如何沟通这一组织设计问题，以优化交易策略表现。

Method: 构建包含5个智能体的LLM交易系统，在21个月内进行450次实验，比较五种组织结构（从孤立基线到协作与竞争性对话）在不同股票市场环境下的表现。

Result: 沟通总体上提升绩效，但效果依赖于市场类型：竞争性对话在科技股中表现最佳，协作性对话在一般稳定股票中更优，金融股则不受沟通影响；所有结构下策略趋于一致，且对话质量与回报无相关性。

Conclusion: 最优沟通设计需匹配市场波动特征，复杂或高质量的讨论并不必然带来更好绩效，策略趋同并非仅由透明度导致。

Abstract: Multi-strategy hedge funds face a fundamental organizational choice: should analysts generating trading strategies communicate, and if so, how? We investigate this using 5-agent LLM-based trading systems across 450 experiments spanning 21 months, comparing five organizational structures from isolated baseline to collaborative and competitive conversation. We show that communication improves performance, but optimal communication design depends on market characteristics. Competitive conversation excels in volatile technology stocks, while collaborative conversation dominates stable general stocks. Finance stocks resist all communication interventions. Surprisingly, all structures, including isolated agents, converge to similar strategy alignments, challenging assumptions that transparency causes harmful diversity loss. Performance differences stem from behavioral mechanisms: competitive agents focus on stock-level allocation while collaborative agents develop technical frameworks. Conversation quality scores show zero correlation with returns. These findings demonstrate that optimal communication design must match market volatility characteristics, and sophisticated discussions don't guarantee better performance.

</details>


<div id='cs.PF'></div>

# cs.PF [[Back]](#toc)

### [30] [Large-scale Multigrid with Adaptive Galerkin Coarsening](https://arxiv.org/abs/2511.13109)
*Fabian Böhm,Nils Kohl,Harald Köstler,Ulrich Rüde*

Main category: cs.PF

TL;DR: 本文提出了一种针对强变系数偏微分方程的自适应粗网格校正方法，结合了几何均匀粗化与局部Galerkin算子，在保证收敛性的同时显著降低内存开销，并在十亿自由度规模的问题上验证了其高效性。


<details>
  <summary>Details</summary>
Motivation: 传统多重网格方法在处理具有强变系数（如粘度跳跃）的偏微分方程时，往往难以兼顾收敛鲁棒性与内存效率。全局使用Galerkin粗化虽准确但内存开销大，而简单粗化则可能丧失收敛性。因此，亟需一种既能保持收敛性能又能控制内存消耗的自适应粗网格策略。

Method: 该方法在几何多重网格框架下，对底层网格进行均匀粗化，但在粗网格算子构建时采用异构策略：在系数梯度大的区域局部使用Galerkin近似，在其余区域则采用轻量级的直接粗化。这种选择性应用仅在必要处计算并存储局部Galerkin算子。

Result: 在广义Stokes方程的一系列“sinker”基准测试中验证了方法的有效性，包括对齐/非对齐粘度跳跃、光滑大梯度粘度函数等情形。该方法成功求解了含10^10自由度、粘度跳跃达10^6、并行进程超10万的大规模问题，并定量分析了内存消耗。

Conclusion: 所提出的自适应粗网格校正方案在强变系数PDE问题中实现了鲁棒收敛与低内存占用的良好平衡，特别适用于大规模并行计算环境下的矩阵自由几何多重网格求解器。

Abstract: We propose a robust, adaptive coarse-grid correction scheme for matrix-free geometric multigrid targeting PDEs with strongly varying coefficients. The method combines uniform geometric coarsening of the underlying grid with heterogeneous coarse-grid operators: Galerkin coarse grid approximation is applied locally in regions with large coefficient gradients, while lightweight, direct coarse grid approximation is used elsewhere. This selective application ensures that local Galerkin operators are computed and stored only where necessary, minimizing memory requirements while maintaining robust convergence. We demonstrate the method on a suite of sinker benchmark problems for the generalized Stokes equation, including grid-aligned and unaligned viscosity jumps, smoothly varying viscosity functions with large gradients, and different viscosity evaluation techniques. We analytically quantify the solver's memory consumption and demonstrate its efficiency by solving Stokes problems with $10^{10}$ degrees of freedom, viscosity jumps of $10^{6}$ magnitude, and more than 100{,}000 parallel processes.

</details>


### [31] [Evaluation of Domain-Specific Architectures for General-Purpose Applications in Apple Silicon](https://arxiv.org/abs/2511.13450)
*Álvaro Corrochano López,Carlos García Sánchez*

Main category: cs.PF

TL;DR: 本文评估了苹果神经引擎（ANE）在通用高性能计算（HPC）任务中的性能与能效，发现经适当适配后，ANE在GEMM等经典算法上可实现与GPU相当的性能，同时显著优于GPU的能效表现。


<details>
  <summary>Details</summary>
Motivation: 随着AI计算需求增长，领域专用加速器（如GPU、TPU、NPU）被广泛部署。受GPGPU将GPU用于通用计算的启发，本文探索是否可将专为机器学习设计的NPU（如ANE）拓展应用于通用HPC场景。

Method: 在Apple M1和M4芯片上，对GEMM、Jacobi和Multigrid等经典HPC算法进行适配，并在ANE上运行，评估其性能（如TFlops）和能耗（瓦特）并与同平台GPU进行对比。

Result: 经适配后，ANE在M4-Pro上GEMM性能达3.8 TFlops（接近同平台GPU的4.7 TFlops），而功耗仅为5.2瓦，远低于GPU的24瓦，展现出卓越的能效优势。

Conclusion: 苹果神经引擎在适配后可用于通用HPC任务，在保持竞争力性能的同时显著提升能效，表明专用AI加速器有潜力扩展至更广泛的计算领域。

Abstract: The rise of AI and its growing computational demands have driven the integration of domain-specific accelerators (such as GPUs, TPUs, and NPUs) across the entire computing infrastructure. Following the precedent set by the GPGPU which popularized GPUs for general-purpose tasks, this research asks whether this phenomenon can be replicated with specialized accelerators like NPUs in new contexts. This paper evaluates the potential of the Apple Neural Engine (ANE) designed for high energy efficiency in Machine Learning workloads, in the context of general-purpose HPC applications. We evaluate the performance and energy consumption of classic HPC algorithms such as GEMM, Jacobi or Multigrid methods on Apple's ANE across the M1 and the latest M4 architectures. Results confirm that, when algorithms are properly adapted, the ANE achieves competitive performance (up to 3.8 TFlops on the M4-Pro, comparable to the GPU's 4.7 TFlops on the same SoC for GEMM operation) while demonstrating significantly superior energy efficiency (e.g., GEMM consumes 5.2 Watts on the ANE versus 24 Watts on GPU counterpart in M4 architectures).

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [32] [Joint Optimization of RU Allocation and C-SR in Multi-AP Coordinated Wi-Fi Systems](https://arxiv.org/abs/2511.12127)
*Md Rahat Hasan,Kazi Ahmed Akbar Munim,Md. Forkan Uddin*

Main category: cs.NI

TL;DR: 本文提出了一种联合RU分配与C-SR的优化方法，以提升多AP协同WiFi系统的吞吐量，并设计了一种低复杂度的启发式算法，在性能接近最优解的同时显著降低计算开销。


<details>
  <summary>Details</summary>
Motivation: 为提升多AP协同WiFi系统的吞吐量，需同时优化RU分配与C-SR机制，但该问题具有非线性整数规划特性，计算复杂度高。

Method: 构建联合RU分配与C-SR的非线性整数规划优化模型，并利用优化工具求解；同时提出一种启发式算法以降低计算复杂度。

Result: 联合设计方案相比非协同系统显著提升了吞吐量；所提启发式算法在吞吐量性能上接近基于优化工具的精确解。

Conclusion: 联合RU分配与C-SR能有效提升系统吞吐量，且所提出的启发式方法在保证性能的同时大幅降低了计算复杂度。

Abstract: We formulate an optimization problem for joint RU allocation and C-SR to maximize the throughput of a multi-AP coordinated WiFi system. The optimization problem is found to be a non-linear integer programming problem. We solve the problem for several network scenarios using an optimization tool. The joint design significantly improves throughput compared to a non-coordinated system. To reduce computational complexity, we also provide a heuristic solution to the problem. The proposed heuristic achieves throughput comparable to that of the computationally expensive optimization tool based solution approach.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [33] [ACE-GNN: Adaptive GNN Co-Inference with System-Aware Scheduling in Dynamic Edge Environments](https://arxiv.org/abs/2511.11586)
*Ao Zhou,Jianlei Yang,Tong Qiao,Yingjie Qi,Xinming Wei,Cenlin Duan,Weisheng Zhao,Chunming Hu*

Main category: cs.DC

TL;DR: ACE-GNN is an adaptive GNN co-inference framework for dynamic edge environments that combines system-level abstraction, novel prediction methods, and hybrid parallelism (pipeline and data) to significantly improve speed, energy efficiency, and stability over existing approaches.


<details>
  <summary>Details</summary>
Motivation: Existing GNN co-inference methods based on static model splitting and pipeline parallelism suffer performance degradation in dynamic edge environments due to network fluctuations and multi-device access scenarios, which they fail to adapt to.

Method: ACE-GNN introduces system-level abstraction and two novel prediction methods for runtime performance awareness, integrates a data parallelism mechanism alongside pipeline parallelism for adaptive scheduling, and employs an efficient batch inference strategy with specialized communication middleware.

Result: Experiments show ACE-GNN achieves up to 12.7× speedup and 82.3% energy savings compared to GCoDE, and 11.7× better energy efficiency than Fograph across diverse edge settings and applications.

Conclusion: ACE-GNN effectively addresses the limitations of static co-inference methods by enabling adaptive, high-performance, and energy-efficient GNN inference in dynamic edge computing environments.

Abstract: The device-edge co-inference paradigm effectively bridges the gap between the high resource demands of Graph Neural Networks (GNNs) and limited device resources, making it a promising solution for advancing edge GNN applications. Existing research enhances GNN co-inference by leveraging offline model splitting and pipeline parallelism (PP), which enables more efficient computation and resource utilization during inference. However, the performance of these static deployment methods is significantly affected by environmental dynamics such as network fluctuations and multi-device access, which remain unaddressed. We present ACE-GNN, the first Adaptive GNN Co-inference framework tailored for dynamic Edge environments, to boost system performance and stability. ACE-GNN achieves performance awareness for complex multi-device access edge systems via system-level abstraction and two novel prediction methods, enabling rapid runtime scheme optimization. Moreover, we introduce a data parallelism (DP) mechanism in the runtime optimization space, enabling adaptive scheduling between PP and DP to leverage their distinct advantages and maintain stable system performance. Also, an efficient batch inference strategy and specialized communication middleware are implemented to further improve performance. Extensive experiments across diverse applications and edge settings demonstrate that ACE-GNN achieves a speedup of up to 12.7x and an energy savings of 82.3% compared to GCoDE, as well as 11.7 better energy efficiency than Fograph.

</details>


### [34] [Mind the Gap: Revealing Inconsistencies Across Heterogeneous AI Accelerators](https://arxiv.org/abs/2511.11601)
*Elliott Wen,Sean Ma,Ewan Tempero,Jens Dietrich,Daniel Luo,Jiaxing Shen,Kaiqi Zhao,Bruce Sham,Yousong Song,Jiayi Hua,Jia Hong*

Main category: cs.DC

TL;DR: 本文首次实证研究了异构AI加速器上机器学习模型的行为差异，发现Mac和华为等新兴平台在算子支持、数值一致性及编译稳定性方面显著落后于NVIDIA，并揭示了PyTorch及各厂商平台中的多个实现缺陷。


<details>
  <summary>Details</summary>
Motivation: 随着AMD、Intel、Mac和华为等厂商推出声称兼容且具性价比的AI加速器，业界亟需了解这些异构硬件在实际机器学习任务中是否能提供与NVIDIA一致的行为，以评估其可靠性和部署风险。

Method: 构建自动化测试管道，从4,000个真实模型生成超过10万个变体，在五种企业级AI加速器上执行，系统比较算子支持度、输出一致性、异常数值处理、编译稳定性等方面。

Result: Mac和华为平台至少少支持17%的算子，输出差异率超5%；在编译加速阶段更易失败，且编译后模型输出可能明显偏离标准执行模式；发现PyTorch中7个实现缺陷及各平台共40个特定问题。

Conclusion: 在日益多样化的AI硬件生态中，实现一致的机器学习行为仍面临严峻挑战，新兴加速器在兼容性、数值稳定性和工程实现方面尚不成熟。

Abstract: While NVIDIA remains the dominant provider of AI accelerators within cloud data center, emerging vendors such as AMD, Intel, Mac, and Huawei offer cost-effective alternatives with claims of compatibility and performance. This paper presents the first empirical study investigating divergence in machine learning model across heterogeneous AI accelerators. Utilizing an automated pipeline, we synthesize over 100,000 variant models derived from 4,000 real-world models and execute them across five different enterprise-grade accelerators. Our findings suggest that newer AI platforms from Mac and Huawei support at least 17\% fewer operators than NVIDIA. These platforms also exhibit a higher rate of output discrepancies (exceeding 5\%), which stem from differences in operator implementations, handling of exceptional numerical values, and instruction scheduling. They are also more susceptible to failures during model compilation-based acceleration, and in some cases, the compiled models produce outputs that differ noticeably from those generated using the standard execution mode. In addition, we identify 7 implementation flaws in PyTorch and 40 platform-specific issues across vendors. These results underscore the challenges of achieving consistent machine learning behavior in an increasingly diverse hardware ecosystem.

</details>


### [35] [PACE Solver Description: twin_width_fmi](https://arxiv.org/abs/2511.11605)
*David Balaban,Adrian Miclăuş*

Main category: cs.DC

TL;DR: 本文提出了用于PACE 2025最小支配集竞赛启发式赛道的求解器twin_width_fmi，其核心组件hedom5结合了贪心构造、剪枝和局部交换优化策略，在保证完全支配的前提下有效减小支配集规模。


<details>
  <summary>Details</summary>
Motivation: 为应对PACE 2025竞赛中最小支配集问题的挑战，需设计高效启发式算法以在合理时间内获得高质量近似解。

Method: 算法hedom5首先对图进行预处理（如叶节点邻居强制、孤立点处理），然后采用基于收益的懒更新贪心策略构建初始支配集；接着通过逆序剪枝移除冗余顶点，并执行预算限制下的1-交换局部搜索以进一步缩小解规模，最后加入安全补丁确保支配完整性。

Result: 所提交的hedom5方法在竞赛测试中表现优异，能有效生成较小规模的支配集。

Conclusion: 结合迭代贪心、剪枝与局部搜索的混合策略在最小支配集问题上具有显著优势，为后续研究提供了有效框架。

Abstract: In this paper we present \texttt{twin\_width\_fmi}'s solver for the heuristic track of PACE's 2025 competition on Minimum Dominating Set.
  As a baseline, we implement \texttt{greedy-ln}, a standard greedy dominating-set heuristic that repeatedly selects the vertex that newly dominates the largest number of currently undominated vertices. We then use this greedy solution as the starting point for a simulated annealing local search: we attempt vertex removals and exchanges and accept worsening moves with decaying probability, in order to escape local minima while preserving domination.
  Our best-performing component, which we ultimately submitted, is \texttt{hedom5}. The design of \texttt{hedom5} is inspired by recent iterative-greedy style domination heuristics~\cite{IterativeGreedy22} that alternate between constructive steps, pruning, and focused repair rather than relying on a single pass. In \texttt{hedom5}, the input graph is first stored in a compact CSR structure and simplified using fast reductions such as forcing neighbors of leaves and handling isolates. We then run a lazy gain-based greedy stage using a priority queue: each candidate vertex is scored by how many currently undominated vertices its closed neighborhood would newly dominate, and scores are only recomputed when necessary. After this constructive phase, we perform an aggressive backward pruning pass that iterates over the chosen dominators in reverse insertion order and deletes any vertex whose closed neighborhood is still fully dominated by the remaining set. Finally, we run a budgeted 1-swap local improvement step that attempts to replace a dominator by an alternative vertex that covers all of its uniquely covered vertices, thereby reducing the size of the dominating set. A brief safety patch at the end guarantees full domination.

</details>


### [36] [AIvailable: A Software-Defined Architecture for LLM-as-a-Service on Heterogeneous and Legacy GPUs](https://arxiv.org/abs/2511.11621)
*Pedro Antunes,Ana Rita Ortigoso,Gabriel Vieira,Daniel Fuentes,Luís Frazão,Nuno Costa,António Pereira*

Main category: cs.DC

TL;DR: AIvailable is a low-cost, highly available LLM-as-a-Service platform that enables efficient inference of large language models across heterogeneous and legacy GPU hardware (including NVIDIA and AMD) by maximizing VRAM utilization and providing a unified client interface.


<details>
  <summary>Details</summary>
Motivation: Existing LLM inference frameworks assume homogeneous, resource-rich environments, which are impractical for academic or resource-constrained settings; there is a need for scalable solutions that can leverage heterogeneous and legacy GPU infrastructure.

Method: AIvailable uses a software-defined architecture with four components—Client Interface, Service Frontend, SDAI Controller, and Service Backend—to orchestrate LLM inference across diverse GPU nodes without CPU fallbacks, employing VRAM-aware dynamic model allocation and abstraction of GPU-specific details.

Result: The platform enables fully GPU-accelerated, resilient LLM inference on mixed legacy hardware, efficiently utilizing VRAM and supporting seamless deployment and interaction with diverse open-source LLMs.

Conclusion: AIvailable democratizes access to generative AI by making LLM inference feasible and cost-effective in resource-constrained environments through intelligent use of heterogeneous and legacy GPU resources.

Abstract: The rise of Large Language Models (LLM) has increased the need for scalable, high-performance inference systems, yet most existing frameworks assume homogeneous, resource-rich hardware, often unrealistic in academic, or resource-constrained settings. We introduce AIvailable, a low-cost, highly available LLM-as-a-Service (LLMaaS) platform, that uses a software-defined approach for running LLMs across heterogeneous and legacy GPU nodes, including NVIDIA and AMD devices, with a focus on fully utilizing each node's VRAM. AIvailable operates as a fully GPU-accelerated inference without CPU fallbacks, featuring a unified client interface that allows seamless interaction with all deployed LLMs through a single logical unit. The architecture comprises four main components: the Client Interface for user access, the Service Frontend for secure request routing and load balancing, the SDAI Controller for orchestration, deployment, and monitoring, and the Service Backend of heterogeneous GPU nodes executing workloads. By abstracting GPU-specific details and providing dynamic, VRAM-aware allocation and reallocation of models, AIvailable ensures efficient use of resources and resilience against failures or workload fluctuations. Targeting academic labs, private companies, and other constrained organizations, it supports diverse open LLMs helping democratize generative AI through the repurposing of legacy GPUs.

</details>


### [37] [Evaluating Large Language Models for Workload Mapping and Scheduling in Heterogeneous HPC Systems](https://arxiv.org/abs/2511.11612)
*Aasish Kumar Sharma,Julian Kunkel*

Main category: cs.DC

TL;DR: 该研究评估了21个开源大语言模型（LLMs）在自然语言描述的高性能计算任务调度问题上的表现，发现少数模型能精确复现最优解，多数接近最优但存在算术或依赖错误，表明LLMs更适合作为可解释的辅助决策工具而非自主求解器。


<details>
  <summary>Details</summary>
Motivation: 探索大语言模型是否能够从自然语言中执行结构化的约束优化任务，特别是在高性能计算工作负载映射与调度这一典型组合优化问题中的能力边界。

Method: 向21个公开可用的大语言模型提供相同的系统节点、任务需求和调度约束的文本描述，要求其分配任务、计算总完工时间（makespan）并解释推理过程，并以人工推导的9小时20秒最优解作为基准进行评估。

Result: 三个模型精确复现了最优解；十二个模型结果在最优解两分钟内；六个模型因算术或依赖错误而次优；所有模型均生成可行映射，但仅约一半严格遵守约束；十九个模型生成部分可执行验证代码，十八个提供连贯推理步骤。

Conclusion: 当前大语言模型在组合优化任务中展现出一定能力，顶尖模型可直接从自然语言重建最优调度方案，但多数仍难以精确处理时序、数据传输计算和依赖关系，因此更适合作为具备可解释性的协同决策助手，而非完全自主的优化求解器。

Abstract: Large language models (LLMs) are increasingly explored for their reasoning capabilities, yet their ability to perform structured, constraint-based optimization from natural language remains insufficiently understood. This study evaluates twenty-one publicly available LLMs on a representative heterogeneous high-performance computing (HPC) workload mapping and scheduling problem. Each model received the same textual description of system nodes, task requirements, and scheduling constraints, and was required to assign tasks to nodes, compute the total makespan, and explain its reasoning. A manually derived analytical optimum of nine hours and twenty seconds served as the ground truth reference. Three models exactly reproduced the analytical optimum while satisfying all constraints, twelve achieved near-optimal results within two minutes of the reference, and six produced suboptimal schedules with arithmetic or dependency errors. All models generated feasible task-to-node mappings, though only about half maintained strict constraint adherence. Nineteen models produced partially executable verification code, and eighteen provided coherent step-by-step reasoning, demonstrating strong interpretability even when logical errors occurred. Overall, the results define the current capability boundary of LLM reasoning in combinatorial optimization: leading models can reconstruct optimal schedules directly from natural language, but most still struggle with precise timing, data transfer arithmetic, and dependency enforcement. These findings highlight the potential of LLMs as explainable co-pilots for optimization and decision-support tasks rather than autonomous solvers.

</details>


### [38] [Beyond the GPU: The Strategic Role of FPGAs in the Next Wave of AI](https://arxiv.org/abs/2511.11614)
*Arturo Urías Jiménez*

Main category: cs.DC

TL;DR: FPGAs offer a reconfigurable, energy-efficient alternative to GPUs for AI acceleration by enabling custom hardware implementations of models with low latency, deterministic timing, and privacy benefits.


<details>
  <summary>Details</summary>
Motivation: The limitations of fixed-architecture accelerators like GPUs—such as high latency, limited energy efficiency, and lack of fine-grained control—motivate the exploration of more flexible platforms for AI workloads.

Method: The paper discusses leveraging FPGAs to map AI algorithms directly into hardware logic, utilizing their reconfigurability to implement parallel pipelines for operations like convolutions and attention mechanisms, and integrating them as SoCs with embedded processors.

Result: FPGAs enable deterministic, low-latency inference near sensors, reduce bandwidth and cloud dependency, enhance privacy, and offload specialized tasks from data center GPUs, with improved deployment workflows via partial reconfiguration and AI framework compilation.

Conclusion: FPGAs are a strategic platform for AI acceleration where performance predictability, customization, and efficiency are critical, especially in edge and privacy-sensitive applications.

Abstract: AI acceleration has been dominated by GPUs, but the growing need for lower latency, energy efficiency, and fine-grained hardware control exposes the limits of fixed architectures. In this context, Field-Programmable Gate Arrays (FPGAs) emerge as a reconfigurable platform that allows mapping AI algorithms directly into device logic. Their ability to implement parallel pipelines for convolutions, attention mechanisms, and post-processing with deterministic timing and reduced power consumption makes them a strategic option for workloads that demand predictable performance and deep customization.
  Unlike CPUs and GPUs, whose architecture is immutable, an FPGA can be reconfigured in the field to adapt its physical structure to a specific model, integrate as a SoC with embedded processors, and run inference near the sensor without sending raw data to the cloud. This reduces latency and required bandwidth, improves privacy, and frees GPUs from specialized tasks in data centers. Partial reconfiguration and compilation flows from AI frameworks are shortening the path from prototype to deployment, enabling hardware--algorithm co-design.

</details>


### [39] [AnchorTP: Resilient LLM Inference with State-Preserving Elastic Tensor Parallelism](https://arxiv.org/abs/2511.11617)
*Wendong Xu,Chujie Chen,He Xiao,Kuan Li,Jing Xiong,Chen Zhang,Wenyong Zhou,Chaofan Tao,Yang Bai,Bei Yu,Ngai Wong*

Main category: cs.DC

TL;DR: AnchorTP 是一种支持状态保存的弹性张量并行框架，通过不等宽划分、解耦守护进程和带宽感知迁移规划，在 GPU 故障时实现快速恢复，显著降低服务中断时间。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型推理服务对高可用性和低延迟要求极高，但多 GPU 张量并行（TP）易受单 GPU 故障影响，现有方法恢复慢、开销大。

Method: 提出 AnchorTP 框架：(i) 支持任意 GPU 数量的弹性张量并行（ETP）与 MoE 兼容；(ii) 通过与推理解耦的守护进程在 GPU 内存中保留模型参数和 KV 缓存；(iii) 设计基于连续最小迁移（CMM）算法的带宽感知规划器和执行调度器，以减少数据重载量并流水化点对点传输。

Result: 在典型故障场景下，相比重启重载方案，AnchorTP 将首次成功响应时间（TFS）最多缩短 11 倍，达到峰值吞吐时间（TTP）最多减少 59%。

Conclusion: AnchorTP 能在不改变服务接口的前提下，以最小数据迁移实现快速服务恢复，显著提升 LLM 推理系统的容错能力与可用性。

Abstract: Large Language Model (LLM) inference services demand exceptionally high availability and low latency, yet multi-GPU Tensor Parallelism (TP) makes them vulnerable to single-GPU failures. We present AnchorTP, a state-preserving elastic TP framework for fast recovery. It (i) enables Elastic Tensor Parallelism (ETP) with unequal-width partitioning over any number of GPUs and compatibility with Mixture-of-Experts (MoE), and (ii) preserves model parameters and KV caches in GPU memory via a daemon decoupled from the inference process. To minimize downtime, we propose a bandwidth-aware planner based on a Continuous Minimal Migration (CMM) algorithm that minimizes reload bytes under a byte-cost dominance assumption, and an execution scheduler that pipelines P2P transfers with reloads. These components jointly restore service quickly with minimal data movement and without changing service interfaces. In typical failure scenarios, AnchorTP reduces Time to First Success (TFS) by up to 11x and Time to Peak (TTP) by up to 59% versus restart-and-reload.

</details>


### [40] [HeteroSTA: A CPU-GPU Heterogeneous Static Timing Analysis Engine with Holistic Industrial Design Support](https://arxiv.org/abs/2511.11660)
*Zizheng Guo,Haichuan Liu,Xizhe Shi,Shenglu Hua,Zuodong Zhang,Chunyuan Zhao,Runsheng Wang,Yibo Lin*

Main category: cs.DC

TL;DR: 本文提出了HeteroSTA，首个支持CPU-GPU异构架构的静态时序分析引擎，提供多种精度-速度可选的延迟计算模型、完整的工业标准格式（如.sdc）支持，并实现端到端GPU加速，同时提供零开销的统一异构API。该工具已开源，可作为独立程序或嵌入式库使用，在多个应用场景中展现出显著加速效果和良好质量。


<details>
  <summary>Details</summary>
Motivation: 现有静态时序分析工具在精度、速度、对工业标准格式的支持以及硬件加速方面存在不足，缺乏一个高效、灵活且易于集成的异构解决方案。

Method: 开发了HeteroSTA异构时序分析引擎，集成了多种延迟计算模型，全面支持.sdc等工业约束格式，并对图基和路径基时序查询实现端到端GPU加速，通过统一的异构API对外提供服务。

Result: HeteroSTA在作为独立工具、与DREAMPlace 4.0集成以及时序驱动全局布线等用例中均实现了显著的运行时加速，同时保持了可比的分析质量。

Conclusion: HeteroSTA是一个高效、灵活且实用的开源异构静态时序分析解决方案，能够有效满足学术界和工业界对高性能时序分析的需求。

Abstract: We introduce in this paper, HeteroSTA, the first CPU-GPU heterogeneous timing analysis engine that efficiently supports: (1) a set of delay calculation models providing versatile accuracy-speed choices without relying on an external golden tool, (2) robust support for industry formats, including especially the .sdc constraints containing all common timing exceptions, clock domains, and case analysis modes, and (3) end-to-end GPU-acceleration for both graph-based and path-based timing queries, all exposed as a zero-overhead flattened heterogeneous application programming interface (API). HeteroSTA is publicly available with both a standalone binary executable and an embeddable shared library targeting ubiquitous academic and industry applications. Example use cases as a standalone tool, a timing-driven DREAMPlace 4.0 integration, and a timing-driven global routing integration have all demonstrated remarkable runtime speed-up and comparable quality.

</details>


### [41] [OSGym: Super-Scalable Distributed Data Engine for Generalizable Computer Agents](https://arxiv.org/abs/2511.11672)
*Zengyi Qin,Jinyuan Chen,Yunze Man,Shengcao Cao,Ziqi Pang,Zhuoyuan Wang,Xin Sun,Gen Lin,Han Fang,Ling Zhu,Zixin Xie,Zibu Wei,Tianshu Ran,Haoran Geng,Xander Wu,Zachary Bright,Qizhen Sun,Rui Wang,Yuyang Cai,Song Wang,Jiace Zhao,Han Cao,Yeyang Zhou,Tianrui Liu,Ray Pan,Chongye Yang,Xiang Ren,Bo Zhang,Yutong Ban,Jitendra Malik,Brian Anthony,Pieter Abbeel*

Main category: cs.DC

TL;DR: OSGym 是一个高可扩展的分布式数据引擎，支持在上千个操作系统副本上高效训练智能体，具备高扩展性、通用性和经济性，适用于多种计算机任务，并显著提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 现有智能体训练平台在处理多样化的计算机相关任务时面临可扩展性差、成本高和通用性不足的问题，亟需一种高效、经济且灵活的训练基础设施。

Method: OSGym 通过并行化管理上千个操作系统副本，构建动态运行环境，支持多类任务（如工具调用、浏览器交互、软件工程等），并兼容多种模型训练算法，同时利用低成本按需计算资源实现高效数据生成。

Result: OSGym 每分钟可生成多达1420条多轮轨迹，单个OS副本日均成本仅0.2–0.3美元；基于其训练的模型在多项任务上优于当前最先进的基线方法。

Conclusion: OSGym 为智能体研究提供了一个开源、经济、高扩展且通用的训练平台，有望推动未来智能体在可扩展性和普适性方面的进步。

Abstract: We introduce OSGym, a super-scalable distributed data engine for training agents across diverse computer-related tasks. OSGym efficiently scales to over a thousand operating system (OS) replicas at an academia-affordable cost, serving as dynamic runtime environments for intelligent agents. It offers three key advantages. (1) Scalability: Despite the intensive resource requirements of running multiple OS replicas, OSGym parallelizes over a thousand instances while maintaining operational efficiency under constrained resources, generating up to 1420 multi-turn trajectories per minute. (2) Generality and Customizability: OSGym supports a broad spectrum of tasks that run on OS platforms, including tool use, browser interactions, software engineering, and office applications, with flexible support for diverse model training algorithms. (3) Economic Viability: OSGym operates at only 0.2-0.3 USD per day per OS replica using accessible on-demand compute providers. It is fully open-source and freely available for both research and commercial use. Experiments show that OSGym enables comprehensive data collection, supervised fine-tuning, and reinforcement learning pipelines for computer agents. Models trained with OSGym outperform state-of-the-art baselines, demonstrating its potential to advance scalability and universality in future agent research.

</details>


### [42] [ECCENTRIC: Edge-Cloud Collaboration Framework for Distributed Inference Using Knowledge Adaptation](https://arxiv.org/abs/2511.11719)
*Mohammad Mahdi Kamani,Zhongwei Cheng,Lin Chen*

Main category: cs.DC

TL;DR: 本文提出了一种名为Eccentric的新框架，通过在边缘与云端模型之间进行知识迁移，在边缘-云推理系统中实现计算、通信开销与性能之间的有效权衡。


<details>
  <summary>Details</summary>
Motivation: 边缘AI的广泛应用受限于边缘设备计算资源有限，通常需依赖云端进行推理，但随着边缘设备数量增加，云端推理带来高昂的计算与通信成本，因此需要在性能与资源消耗之间取得平衡。

Method: 提出Eccentric框架，通过将边缘模型的知识迁移到云端模型，实现不同层级的计算、通信与性能权衡，作为一种适用于边缘-云推理系统的新型压缩方法。

Result: 在分类和目标检测任务上的实验验证了该框架在降低计算与通信成本的同时仍能保持优异性能的有效性。

Conclusion: Eccentric框架为边缘-云协同推理提供了一种高效解决方案，能够在资源受限条件下优化系统整体效率与性能。

Abstract: The massive growth in the utilization of edge AI has made the applications of machine learning models ubiquitous in different domains. Despite the computation and communication efficiency of these systems, due to limited computation resources on edge devices, relying on more computationally rich systems on the cloud side is inevitable in most cases. Cloud inference systems can achieve the best performance while the computation and communication cost is dramatically increasing by the expansion of a number of edge devices relying on these systems. Hence, there is a trade-off between the computation, communication, and performance of these systems. In this paper, we propose a novel framework, dubbed as Eccentric that learns models with different levels of trade-offs between these conflicting objectives. This framework, based on an adaptation of knowledge from the edge model to the cloud one, reduces the computation and communication costs of the system during inference while achieving the best performance possible. The Eccentric framework can be considered as a new form of compression method suited for edge-cloud inference systems to reduce both computation and communication costs. Empirical studies on classification and object detection tasks corroborate the efficacy of this framework.

</details>


### [43] [A Meta-Heuristic Load Balancer for Cloud Computing Systems](https://arxiv.org/abs/2511.11721)
*Leszek Sliwko,Vladimir Getov*

Main category: cs.DC

TL;DR: 本文提出了一种云系统服务分配策略，通过新颖的遗传算法实现负载均衡，在避免节点过载和保证系统稳定性的同时最小化成本。


<details>
  <summary>Details</summary>
Motivation: 在云计算环境中，需要有效分配服务以避免节点过载、维持系统稳定性，并尽可能降低资源使用和迁移成本。

Method: 构建了一个包含多种资源类型和服务迁移成本的云资源利用抽象模型，并设计了一种新型遗传算法，其初始种群由其他元启发式算法的输出结果构成；同时实现了一个原型元启发式负载均衡器进行验证。

Result: 实验结果表明所提出的算法在实现负载均衡和控制成本方面具有有效性。

Conclusion: 该研究提供了一种兼顾系统稳定性和成本效益的云服务分配方法，通过混合元启发式策略提升了负载均衡性能。

Abstract: This paper presents a strategy to allocate services on a Cloud system without overloading nodes and maintaining the system stability with minimum cost. We specify an abstract model of cloud resources utilization, including multiple types of resources as well as considerations for the service migration costs. A prototype meta-heuristic load balancer is demonstrated and experimental results are presented and discussed. We also propose a novel genetic algorithm, where population is seeded with the outputs of other meta-heuristic algorithms.

</details>


### [44] [Harli: Harvest Underutilized Resources in LLM Serving with Finetuning Tasks](https://arxiv.org/abs/2511.11729)
*Ao Xu,Han Zhao,Weihao Cui,Quan Chen,Yukang Chen,Shulai Zhang,Shuang Chen,Jiemin Jiang,Zhibin Yu,Minyi Guo*

Main category: cs.DC

TL;DR: Harli 是一个新型大语言模型（LLM）服务系统，通过将参数高效微调（PEFT）任务与 LLM 解码实例共置，显著提升 GPU 利用率，在保证推理服务质量（QoS）的同时，平均提升微调吞吐量 46.2%。


<details>
  <summary>Details</summary>
Motivation: 现有 LLM 服务系统在解耦预填充和解码阶段后，解码实例因内存受限和动态负载下批处理不足，导致 GPU 利用率低下，计算资源浪费。

Method: Harli 通过三个核心组件实现高效共置：统一内存分配器实现运行时内存复用、两阶段延迟预测器建模解码延迟、以及兼顾 QoS 保障与吞吐量最大化的调度器。

Result: 实验表明，Harli 相比当前最先进的服务系统，平均提升微调吞吐量 46.2%（最高达 92.0%），同时严格满足推理解码的 QoS 要求。

Conclusion: 将计算密集型且内存高效的 PEFT 任务与 LLM 解码实例安全共置，是提升 GPU 利用率并兼顾推理与训练效率的有效策略，Harli 系统为此提供了可行方案。

Abstract: Large language models (LLMs) are increasingly deployed under the Model-as-a-Service (MaaS) paradigm. To meet stringent quality-of-service (QoS) requirements, existing LLM serving systems disaggregate the prefill and decode phases of inference. However, decode instances often experience low GPU utilization due to their memory-bound nature and insufficient batching in dynamic workloads, leaving compute resources underutilized.
  We introduce Harli, a serving system that improves GPU utilization by co-locating parameter-efficient finetuning (PEFT) tasks with LLM decode instances. PEFT tasks are compute-bound and memory-efficient, making them ideal candidates for safe co-location. Specifically, Harli addresses key challenges--limited memory and unpredictable interference--using three components: a unified memory allocator for runtime memory reuse, a two-stage latency predictor for decode latency modeling, and a QoS-guaranteed throughput-maximizing scheduler for throughput maximization. Experimental results show that Harli improves the finetune throughput by 46.2% on average (up to 92.0%) over state-of-the-art serving systems, while maintaining strict QoS guarantees for inference decode.

</details>


### [45] [Noise-Aware Optimization in Nominally Identical Manufacturing and Measuring Systems for High-Throughput Parallel Workflows](https://arxiv.org/abs/2511.11739)
*Christina Schenk,Miguel Hernández-del-Valle,Luis Calero-Lumbreras,Marcus Noack,Maciej Haranczyk*

Main category: cs.DC

TL;DR: 本文提出一种噪声感知的决策算法，通过建模设备特异性噪声并利用设备间差异，在多设备自动化系统（如3D打印农场）中实现更高效、可靠和可复现的贝叶斯优化。


<details>
  <summary>Details</summary>
Motivation: 在高通量自动化系统（如增材制造集群）中，设备间的实验噪声差异严重影响结果的可复现性，并可能在大规模应用（如建筑3D打印）中引发结构或经济风险。传统方法假设设备同质或采用通用鲁棒策略，无法有效应对设备特异性噪声。

Method: 该方法通过分布分析和成对散度度量结合聚类，量化并建模各设备的噪声特征，据此自适应选择单设备或鲁棒多设备贝叶斯优化策略，显式利用设备间差异提升性能。

Result: 在三台名义上相同的3D打印机上的实验表明，该框架减少了冗余、降低了资源消耗，并提高了可靠性。

Conclusion: 该研究建立了一种面向精度与资源感知的优化范式，适用于可扩展的自动化实验平台，显著提升了多设备系统的可复现性与效率。

Abstract: Device-to-device variability in experimental noise critically impacts reproducibility, especially in automated, high-throughput systems like additive manufacturing farms. While manageable in small labs, such variability can escalate into serious risks at larger scales, such as architectural 3D printing, where noise may cause structural or economic failures. This contribution presents a noise-aware decision-making algorithm that quantifies and models device-specific noise profiles to manage variability adaptively. It uses distributional analysis and pairwise divergence metrics with clustering to choose between single-device and robust multi-device Bayesian optimization strategies. Unlike conventional methods that assume homogeneous devices or generic robustness, this framework explicitly leverages inter-device differences to enhance performance, reproducibility, and efficiency. An experimental case study involving three nominally identical 3D printers (same brand, model, and close serial numbers) demonstrates reduced redundancy, lower resource usage, and improved reliability. Overall, this framework establishes a paradigm for precision- and resource-aware optimization in scalable, automated experimental platforms.

</details>


### [46] [How Machine Learning-Data Driven Replication Strategies Enhance Fault Tolerance in Large-Scale Distributed Systems](https://arxiv.org/abs/2511.11749)
*Almond Kiruthu Murimi*

Main category: cs.DC

TL;DR: 本文研究了基于机器学习的数据复制策略如何提升大规模分布式系统的容错能力，提出利用预测分析和强化学习实现自适应复制机制，并通过文献综述与对比分析验证其优势与挑战。


<details>
  <summary>Details</summary>
Motivation: 传统静态数据复制方法难以应对动态负载和突发故障，导致资源利用效率低下和系统停机时间延长，亟需更智能、自适应的容错机制。

Method: 结合预测分析与强化学习，设计能实时预测系统故障并优化数据放置的自适应复制机制；通过文献综述、定性分析及与传统方法的对比评估进行研究。

Result: 识别出现有复制策略的关键局限性，验证了机器学习在构建更具弹性、自我优化系统中的变革潜力，同时揭示了实际部署中的挑战。

Conclusion: 机器学习驱动的复制策略在提升系统容错性和资源效率方面前景广阔，但需进一步研究以解决实际应用中的复杂性，为云和企业系统提供可行路径。

Abstract: This research paper investigates how machine learning-driven data replication strategies can enhance fault tolerance in large-scale distributed systems. Traditional replication methods, which rely on static configurations, often struggle to adapt to dynamic workloads and unexpected failures, leading to inefficient resource utilization and prolonged downtime. By integrating machine learning techniques-specifically predictive analytics and reinforcement learning. The study proposes adaptive replication mechanisms capable of forecasting system failures and optimizing data placement in real time. Through an extensive literature review, qualitative analysis, and comparative evaluations with traditional approaches, the paper identifies key limitations in existing replication strategies and highlights the transformative potential of machine learning in creating more resilient, self-optimizing systems. The findings underscore both the promise and the challenges of implementing ML-driven solutions in real-world environments, offering recommendations for future research and practical deployment in cloud-based and enterprise systems.

</details>


### [47] [TD-Orch: Scalable Load-Balancing for Distributed Systems with Applications to Graph Processing](https://arxiv.org/abs/2511.11843)
*Yiwei Zhao,Qiushi Lin,Hongbo Kang,Guy E. Blelloch,Laxman Dhulipala,Charles McGuffey,Phillip B. Gibbons*

Main category: cs.DC

TL;DR: 本文提出了一种任务-数据协同调度框架TD-Orch，通过分布式推拉机制实现高效负载均衡，并在此基础上构建了图处理系统TDO-GP，在通用图处理任务中显著优于现有系统。


<details>
  <summary>Details</summary>
Motivation: 在分布式应用（如图处理和键值存储）中，任务与所需数据通常分布在不同机器上，需将任务与目标数据协同调度至同一节点执行。现有调度方法在面对数据热点时难以高效扩展且通信开销大。

Method: 提出TD-Orch框架，采用分布式推拉技术，允许任务和数据双向流动，以实现可扩展的负载均衡；并基于此构建TDO-GP图处理系统，设计三类实现技术以充分利用TD-Orch的执行流程。

Result: 实验表明，TD-Orch相比现有调度基线最高提速2.7倍；TDO-GP在通用图处理任务中平均比现有开源系统快4.1倍。

Conclusion: TD-Orch提供了一种高效、可扩展的任务-数据协同调度抽象，能有效应对数据热点问题，显著提升分布式应用（尤其是图处理）的性能。

Abstract: In this paper, we highlight a task-data orchestration abstraction that supports a range of distributed applications, including graph processing and key-value stores. Given a batch of tasks each requesting one or more data items, where both tasks and data are distributed across multiple machines, each task must get co-located with its target data (by moving tasks and/or data) and executed. We present TD-Orch, an efficient and scalable orchestration framework featuring a simple application developer interface. TD-Orch employs a distributed push-pull technique, leveraging the bidirectional f low of both tasks and data to achieve scalable load balance across machines even under highly skewed data request (data hot spots), with minimal communication overhead. Experimental results show that TD-Orch achieves up to 2.7x speedup over existing distributed scheduling baselines. Building on TD-Orch, we present TDO-GP, a distributed graph processing system for general graph problems, demonstrating the effectiveness of the underlying framework. We design three families of implementation techniques to fully leverage the execution flow provided by TD-Orch. Experimental results show that TDO-GP achieves an average speedup of 4.1x over the best prior open-source distributed graph systems for general graph processing.

</details>


### [48] [Flash-Fusion: Enabling Expressive, Low-Latency Queries on IoT Sensor Streams with LLMs](https://arxiv.org/abs/2511.11885)
*Kausar Patherya,Ashutosh Dhekne,Francisco Romero*

Main category: cs.DC

TL;DR: Flash-Fusion 是一个端到端的边缘-云系统，通过边缘端统计摘要和云端查询规划，显著降低 IoT 数据分析的延迟、成本与用户负担，同时保持高质量的自然语言交互体验。


<details>
  <summary>Details</summary>
Motivation: 当前在利用大语言模型（LLM）分析物联网（IoT）数据时面临两大挑战：一是原始传感器数据量庞大且过于细粒度，直接使用成本高昂；二是数据分析过程缓慢，需要大量迭代和技术专业知识。此外，将全部遥测数据直接输入 LLM 不现实，受限于上下文窗口、高昂的 token 成本和高延迟。

Method: Flash-Fusion 系统采用两个核心设计原则：(1) 在边缘端进行统计摘要，减少 73.5% 的原始数据量；(2) 在云端进行查询规划，对行为数据聚类并构建富含上下文的提示，再调用 LLM 进行分析。

Result: 在大学校车车队上的部署实验表明，相比直接将原始数据输入最先进 LLM 的基线方法，Flash-Fusion 实现了 95% 的延迟降低和 98% 的 token 使用量及成本下降，同时保持高质量的回答。

Conclusion: Flash-Fusion 有效解决了 IoT 数据分析中数据量大、处理慢、使用门槛高的问题，使不同领域的用户（如安全员、城市规划师、车队经理和数据科学家）能够高效地通过自然语言与 IoT 数据交互，无需手动编写查询或预处理数据。

Abstract: Smart cities and pervasive IoT deployments have generated interest in IoT data analysis across transportation and urban planning. At the same time, Large Language Models offer a new interface for exploring IoT data - particularly through natural language. Users today face two key challenges when working with IoT data using LLMs: (1) data collection infrastructure is expensive, producing terabytes of low-level sensor readings that are too granular for direct use, and (2) data analysis is slow, requiring iterative effort and technical expertise. Directly feeding all IoT telemetry to LLMs is impractical due to finite context windows, prohibitive token costs at scale, and non-interactive latencies. What is missing is a system that first parses a user's query to identify the analytical task, then selects the relevant data slices, and finally chooses the right representation before invoking an LLM.
  We present Flash-Fusion, an end-to-end edge-cloud system that reduces the IoT data collection and analysis burden on users. Two principles guide its design: (1) edge-based statistical summarization (achieving 73.5% data reduction) to address data volume, and (2) cloud-based query planning that clusters behavioral data and assembles context-rich prompts to address data interpretation. We deploy Flash-Fusion on a university bus fleet and evaluate it against a baseline that feeds raw data to a state-of-the-art LLM. Flash-Fusion achieves a 95% latency reduction and 98% decrease in token usage and cost while maintaining high-quality responses. It enables personas across disciplines - safety officers, urban planners, fleet managers, and data scientists - to efficiently iterate over IoT data without the burden of manual query authoring or preprocessing.

</details>


### [49] [KVSwap: Disk-aware KV Cache Offloading for Long-Context On-device Inference](https://arxiv.org/abs/2511.11907)
*Huawei Zhang,Chunwei Xia,Zheng Wang*

Main category: cs.DC

TL;DR: KVSwap 是一种通过将语言模型的键值（KV）缓存卸载到磁盘来突破设备内存限制的软件框架，在有限内存下实现更高吞吐量并保持生成质量。


<details>
  <summary>Details</summary>
Motivation: 在移动和嵌入式设备上运行大语言模型进行长上下文推理时，KV 缓存随上下文长度和批大小线性增长，迅速超出设备内存容量，限制了本地部署的可行性。

Method: KVSwap 将完整的 KV 缓存存储在磁盘上，利用紧凑的内存元数据预测关键条目进行预加载，结合计算与硬件感知的磁盘访问，并优化读取模式以匹配存储设备特性。

Result: 实验表明，KVSwap 在多种语言模型和存储类型下，相比现有 KV 缓存卸载方案，在严格内存限制下实现了更高的吞吐量，同时保持了生成质量。

Conclusion: KVSwap 有效缓解了长上下文语言模型在设备端推理中的内存瓶颈问题，为隐私保护、离线使用和低成本部署提供了可行路径。

Abstract: Language models (LMs) underpin emerging mobile and embedded AI applications like meeting and video summarization and document analysis, which often require processing multiple long-context inputs. Running an LM locally on-device improves privacy, enables offline use, and reduces cost, but long-context inference quickly hits a \emph{memory capacity wall} as the key-value (KV) cache grows linearly with context length and batch size.
  We present KVSwap, a software framework to break this memory wall by offloading the KV cache to non-volatile secondary storage (disk). KVSwap leverages the observation that only a small, dynamically changing subset of KV entries is critical for generation. It stores the full cache on disk, uses a compact in-memory metadata to predict which entries to preload, overlaps computation with hardware-aware disk access, and orchestrates read patterns to match storage device characteristics. Our evaluation shows that across representative LMs and storage types, KVSwap delivers higher throughput under tight memory budgets while maintaining the generation quality when compared with existing KV cache offloading schemes.

</details>


### [50] [High-Performance N-Queens Solver on GPU: Iterative DFS with Zero Bank Conflicts](https://arxiv.org/abs/2511.12009)
*Guangchao Yao,Yali Li*

Main category: cs.DC

TL;DR: 本文提出了一种在NVIDIA GPU平台上高效求解N皇后问题的并行计算方法，通过迭代深度优先搜索、共享内存优化和避免bank冲突等技术，在8块RTX 5090 GPU上仅用28.4天验证了27皇后问题的解，并将28皇后问题的预计求解时间缩短至约11个月，相比现有GPU方法实现10倍以上加速。


<details>
  <summary>Details</summary>
Motivation: N皇后问题的解计数是经典的NP完全问题，计算复杂度极高。目前学术界仅严格验证到N≤26；2016年PreuBer团队使用FPGA耗时约一年求解27皇后问题但未被独立验证，而现有GPU方法仍需约17个月，时间和资源成本过高。

Method: 提出一种基于NVIDIA GPU的并行计算方法，核心包括：(1) 迭代式深度优先搜索算法；(2) 将所需栈结构完整映射至GPU共享内存；(3) 通过精心设计的内存访问模式有效避免bank冲突；(4) 应用多种优化技术以实现最佳性能。

Result: 在8块RTX 5090 GPU上仅用28.4天成功验证27皇后问题，确认了PreuBer团队结果的正确性；并将28皇后问题的预计求解时间缩短至约11个月。相比当前最先进的GPU方法，在相同硬件（8 A100）上实现超10倍加速，使用8 RTX 5090时加速比超过26倍。

Conclusion: 所提出的GPU并行优化框架显著提升了N皇后问题的求解效率，不仅验证了已有结果，还使更高规模问题的求解变得可行，为这一长期停滞的问题提供了新思路。

Abstract: The counting of solutions to the N-Queens problem is a classic NP-complete problem with extremely high computational complexity. As of now, the academic community has rigorously verified the number of solutions only up to N <= 26. In 2016, the research team led by PreuBer solved the 27-Queens problem using FPGA hardware, which took approximately one year, though the result remains unverified independently. Recent studies on GPU parallel computing suggest that verifying the 27-Queens solution would still require about 17 months, indicating excessively high time and computational resource costs. To address this challenge, we propose an innovative parallel computing method on NVIDIA GPU platform, with the following core contributions: (1) An iterative depth-first search (DFS) algorithm for solving the N-Queens problem; (2) Complete mapping of the required stack structure to GPU shared memory; (3) Effective avoidance of bank conflicts through meticulously designed memory access patterns; (4) Various optimization techniques are employed to achieve optimal performance. Under the proposed optimization framework, we successfully verified the 27-Queens problem in just 28.4 days using eight RTX 5090 GPUs, thereby confirming the correctness of PreuBer's computational results. Moreover, we have reduced the projected solving time for the next open case-the 28-Queens problem-to approximately 11 months, making its resolution computationally feasible. Compared to the state-of-the-art GPU methods, our method achieves over 10x speedup on identical hardware configurations (8 A100), while delivering over 26x acceleration when utilizing 8 RTX 5090 GPUs, and brings fresh perspectives to this long-stagnant problem.

</details>


### [51] [A Quick and Exact Method for Distributed Quantile Computation](https://arxiv.org/abs/2511.12025)
*Ivan Cao,Jaromir J. Saloni,David A. G. Harrison*

Main category: cs.DC

TL;DR: GK Select 是一种在 Spark 中高效计算精确分位数的新算法，它结合了 GK Sketch 的近似能力与局部筛选和树归约策略，在避免全局排序的同时实现与近似方法相当的性能。


<details>
  <summary>Details</summary>
Motivation: 现有 Spark 系统中，若需精确分位数则依赖代价高昂的全局排序，而近似方法（如 GK Sketch）虽快但不精确。因此需要一种既能保证精确性又具备高效率的分位数计算方法。

Method: GK Select 利用 GK Sketch 找到接近目标分位数的枢轴值，在每个分区线性时间内提取该枢轴误差范围内的所有候选值，再通过树归约合并候选集以获得精确结果。

Result: 理论分析表明 GK Select 在执行器端的时间复杂度与 GK Sketch 相当；实验显示其在 10^9 数据量、120 分区、30 核 AWS EMR 集群上比 Spark 全局排序快约 10.5 倍，且达到近似算法的延迟水平。

Conclusion: GK Select 成功在保持精确性的同时显著提升了 Spark 中分位数计算的效率，为大规模数据分析提供了一种实用且高效的解决方案。

Abstract: Quantile computation is a core primitive in large-scale data analytics. In Spark, practitioners typically rely on the Greenwald-Khanna (GK) Sketch, an approximate method. When exact quantiles are required, the default option is an expensive global sort. We present GK Select, an exact Spark algorithm that avoids full-data shuffles and completes in a constant number of actions. GK Select leverages GK Sketch to identify a near-target pivot, extracts all values within the error bound around this pivot in each partition in linear time, and then tree-reduces the resulting candidate sets. We show analytically that GK Select matches the executor-side time complexity of GK Sketch while returning the exact quantile. Empirically, GK Select achieves sketch-level latency and outperforms Spark's full sort by approximately 10.5x on 10^9 values across 120 partitions on a 30-core AWS EMR cluster.

</details>


### [52] [Striking the Right Balance between Compute and Copy: Improving LLM Inferencing Under Speculative Decoding](https://arxiv.org/abs/2511.12031)
*Arun Ramachandran,Ramaswamy Govindarajan,Murali Annavaram,Prakash Raghavendra,Hossein Entezari Zarch,Lei Gao,Chaoyi Jiang*

Main category: cs.DC

TL;DR: 本文提出了一种名为BMC（Balancing Memory and Compute）的新型KV缓存分配机制，通过每r次迭代一次性分配带有r个冗余行的KV张量，在避免频繁内存分配与拷贝开销的同时引入少量冗余计算，并进一步将这些冗余计算用于推测解码（Speculative Decoding），从而显著提升LLM推理吞吐量。实验表明，BMC在CPU上相较HuggingFace基线最高提速3.2倍，结合SD后额外提速1.39倍，并优于vLLM和DeepSpeed等先进推理系统，且在GPU上同样有效。


<details>
  <summary>Details</summary>
Motivation: 随着GPU及其云虚拟实例成本飙升，业界希望利用CPU进行大语言模型推理。然而，传统KV缓存更新方式（每次生成token时进行分配、拷贝和原地跨步更新）在长序列下带来严重性能瓶颈；而预先分配大KV张量虽可避免拷贝，却因零填充行导致冗余计算。因此，亟需一种兼顾内存效率与计算开销的新机制。

Method: 提出BMC机制：每r次迭代一次性分配包含r个冗余行的KV张量，实现r次迭代内无拷贝的原地更新；同时观察到这些冗余行可用于推测解码（SD）以提升生成效率；并构建分析模型以选择最优r值。

Result: BMC在CPU上相比HuggingFace基线平均吞吐量最高提升3.2倍；结合SD后额外获得最高1.39倍加速；相比vLLM和DeepSpeed分别最高提速1.36倍和2.29倍；且在GPU上也表现良好。

Conclusion: BMC通过平衡内存分配与计算冗余，有效解决了LLM推理中KV缓存更新的性能瓶颈，不仅显著提升CPU推理效率，还能与推测解码协同增效，并在多种硬件平台上具有通用性。

Abstract: With the skyrocketing costs of GPUs and their virtual instances in the cloud, there is a significant desire to use CPUs for large language model (LLM) inference. KV cache update, often implemented as allocation, copying, and in-place strided update for each generated token, incurs significant overhead. As the sequence length increases, the allocation and copy overheads dominate the performance. Alternate approaches may allocate large KV tensors upfront to enable in-place updates, but these matrices (with zero-padded rows) cause redundant computations. In this work, we propose a new KV cache allocation mechanism called Balancing Memory and Compute (BMC). BMC allocates, once every r iterations, KV tensors with r redundant rows, allowing in-place update without copy overhead for those iterations, but at the expense of a small amount of redundant computation. Second, we make an interesting observation that the extra rows allocated in the KV tensors and the resulting redundant computation can be repurposed for Speculative Decoding (SD) that improves token generation efficiency. Last, BMC represents a spectrum of design points with different values of r. To identify the best-performing design point(s), we derive a simple analytical model for BMC. The proposed BMC method achieves an average throughput acceleration of up to 3.2x over baseline HuggingFace (without SD). Importantly when we apply BMC with SD, it results in an additional speedup of up to 1.39x, over and above the speedup offered by SD. Further, BMC achieves a throughput acceleration of up to 1.36x and 2.29x over state-of-the-art inference servers vLLM and DeepSpeed, respectively. Although the BMC technique is evaluated extensively across different classes of CPUs (desktop and server class), we also evaluate the scheme with GPUs and demonstrate that it works well for GPUs.

</details>


### [53] [Combining Serverless and High-Performance Computing Paradigms to support ML Data-Intensive Applications](https://arxiv.org/abs/2511.12185)
*Mills Staylor,Arup Kumar Sarker,Gregor von Laszewski,Geoffrey Fox,Yue Cheng,Judy Fox*

Main category: cs.DC

TL;DR: 本文提出了一种名为Cylon的高性能分布式数据帧解决方案，通过受FMI库启发设计的无服务器通信器，解决了在AWS Lambda等无服务器环境中处理大数据时因依赖外部存储而导致的性能瓶颈问题，并展示了其在直接通信（基于NAT穿透TCP打洞）下的扩展性能远优于传统无服务器架构。


<details>
  <summary>Details</summary>
Motivation: 随着数据量激增和无服务器计算（如AWS Lambda）的普及，传统依赖外部存储的数据处理方式在无服务器环境中面临严重的性能瓶颈。作者旨在通过改进通信机制提升无服务器平台在大数据处理任务中的性能表现。

Method: 作者基于FMI库的设计理念，开发了一个专用于无服务器环境的通信器，利用NAT穿透TCP打洞技术实现Lambda函数间的直接通信，从而绕过慢速外部存储，提升数据处理效率。

Result: 实验表明，在采用直接通信机制后，AWS Lambda的强扩展性能不足传统服务器型AWS（EC2）和高性能计算集群（HPC）的1%。

Conclusion: 尽管无服务器计算提供了便捷的扩展性和计费模式，但在当前架构下其数据处理性能远落后于传统服务器或HPC系统；通过引入高效的直接通信机制（如Cylon所实现），可显著缓解这一问题，但仍存在巨大性能差距。

Abstract: Data is found everywhere, from health and human infrastructure to the surge of sensors and the proliferation of internet-connected devices. To meet this challenge, the data engineering field has expanded significantly in recent years in both research and industry. Traditionally, data engineering, Machine Learning, and AI workloads have been run on large clusters within data center environments, requiring substantial investment in hardware and maintenance. With the rise of the public cloud, it is now possible to run large applications across nodes without owning or maintaining hardware. Serverless functions such as AWS Lambda provide horizontal scaling and precise billing without the hassle of managing traditional cloud infrastructure. However, when processing large datasets, users often rely on external storage options that are significantly slower than direct communication typical of HPC clusters. We introduce Cylon, a high-performance distributed data frame solution that has shown promising results for data processing using Python. We describe how we took inspiration from the FMI library and designed a serverless communicator to tackle communication and performance issues associated with serverless functions. With our design, we demonstrate that the performance of AWS Lambda falls below one percent of strong scaling experiments compared to serverful AWS (EC2) and HPCs based on implementing direct communication via NAT Traversal TCP Hole Punching.

</details>


### [54] [Distributed Seasonal Temporal Pattern Mining](https://arxiv.org/abs/2511.12216)
*Van Ho-Long,Nguyen Ho,Anh-Vu Dinh-Duc,Ha Manh Tran,Ky Trung Nguyen,Tran Dung Pham,Quoc Viet Hung Nguyen*

Main category: cs.DC

TL;DR: 本文提出了首个用于挖掘时间序列中季节性时序模式（STP）的分布式框架DSTPM，通过分布式分层哈希结构显著提升了运行效率和可扩展性。


<details>
  <summary>Details</summary>
Motivation: 传统时序模式挖掘方法无法有效捕捉季节性特征，且缺乏反单调性导致搜索空间庞大；现有STP挖掘方法为串行处理，难以应对大规模数据。

Method: 提出分布式季节性时序模式挖掘框架DSTPM，利用分布式分层查找哈希结构实现高效计算。

Result: 实验表明，DSTPM在运行时间和内存使用方面显著优于串行基线方法，并能有效扩展到超大规模数据集。

Conclusion: DSTPM是首个支持大规模时间序列数据中季节性时序模式高效挖掘的分布式解决方案，具有良好的性能与可扩展性。

Abstract: The explosive growth of IoT-enabled sensors is producing enormous amounts of time series data across many domains, offering valuable opportunities to extract insights through temporal pattern mining. Among these patterns, an important class exhibits periodic occurrences, referred to as \textit{seasonal temporal patterns} (STPs). However, mining STPs poses challenges, as traditional measures such as support and confidence cannot capture seasonality, and the lack of the anti-monotonicity property results in an exponentially large search space. Existing STP mining methods operate sequentially and therefore do not scale to large datasets. In this paper, we propose the Distributed Seasonal Temporal Pattern Mining (DSTPM), the first distributed framework for mining seasonal temporal patterns from time series. DSTPM leverages efficient data structures, specifically distributed hierarchical lookup hash structures, to enable efficient computation. Extensive experimental evaluations demonstrate that DSTPM significantly outperforms sequential baselines in runtime and memory usage, while scaling effectively to very large datasets.

</details>


### [55] [Design of A Low-Latency and Parallelizable SVD Dataflow Architecture on FPGA](https://arxiv.org/abs/2511.12461)
*Fangqiang Du,Sixuan Chong,Zixuan Huang,Rui Qin,Fengnan Mi,Caibao Hu,Jiangang Chen*

Main category: cs.DC

TL;DR: 本文提出了一种基于数据流的SVD处理算法（DSB Jacobi），显著降低了片上BRAM使用量并提升了计算速度，适用于大规模数据流的实时SVD计算。


<details>
  <summary>Details</summary>
Motivation: 随着矩阵维度快速增长，传统SVD计算成本高昂，现有硬件加速方案存在可扩展性差、片上内存消耗高以及难以应对实时大规模数据流处理等问题。

Method: 提出一种名为DSB Jacobi的数据流式SVD处理算法，优化片上内存使用并提升计算效率。

Result: 实验结果表明，与先前工作相比，该方法减少了41.5%的片上RAM消耗，并将计算效率提高了23倍。

Conclusion: 所提出的DSB Jacobi算法为嵌入式系统中大规模数据流的实时SVD计算提供了一种高效且实用的解决方案。

Abstract: Singular value decomposition (SVD) is widely used for dimensionality reduction and noise suppression, and it plays a pivotal role in numerous scientific and engineering applications. As the dimensions of the matrix grow rapidly, the computational cost increases significantly, posing a serious challenge to the efficiency of data analysis and signal processing systems,especially in time-sensitive scenarios with large-scale datasets. Although various dedicated hardware architectures have been proposed to accelerate the computation of intensive SVD, many of these designs suffer from limited scalability and high consumption of on-chip memory resources. Moreover, they typically overlook the computational and data transfer challenges associated with SVD, enabling them unsuitable for real-time processing of large-scale data stream matrices in embedded systems. In this express, we propose a Data Stream-Based SVD processing algorithm (DSB Jacobi), which significantly reduces on-chip BRAM usage while improving computational speed, offering a practical solution for real-time SVD computation of large-scale data streams. Compared with previous works, our experimental results indicate that the proposed method reduces on-chip RAM consumption by 41.5 percent and improves computational efficiency by 23 times.

</details>


### [56] [A Decentralized Root Cause Localization Approach for Edge Computing Environments](https://arxiv.org/abs/2511.12486)
*Duneesha Fernando,Maria A. Rodriguez,Rajkumar Buyya*

Main category: cs.DC

TL;DR: 本文提出了一种面向边缘计算环境的去中心化根因定位（RCL）方法，利用个性化PageRank（PPR）算法在本地微服务集群内进行高效根因识别，并通过轻量级跨集群协调机制处理跨集群异常传播，显著降低定位延迟，同时保持甚至提升定位准确性。


<details>
  <summary>Details</summary>
Motivation: 现有RCL方法主要针对云环境设计，依赖集中式分析，在边缘计算场景下会带来高延迟和通信开销；而边缘环境中微服务应用复杂、资源受限，亟需低延迟、低开销的去中心化RCL方案。

Method: 将微服务按通信与共置关系聚类，在每个集群内本地运行PPR算法进行根因定位；对跨集群异常传播，采用点对点近似协调机制；同时引入针对边缘异构环境的新型异常评分机制以提升定位准确性。

Result: 在MicroCERCL公开数据集上的实验表明，所提方法相比集中式方法可减少最多34%的定位时间，同时达到相当或更高的定位准确率。

Conclusion: 去中心化的图驱动RCL方法能有效应对边缘环境中资源受限和低延迟需求的挑战，为微服务异常诊断提供实用高效的解决方案。

Abstract: Edge computing environments host increasingly complex microservice-based IoT applications, which are prone to performance anomalies that can propagate across dependent services. Identifying the true source of such anomalies, known as Root Cause Localization (RCL), is essential for timely mitigation. However, existing RCL approaches are designed for cloud environments and rely on centralized analysis, which increases latency and communication overhead when applied at the edge. This paper proposes a decentralized RCL approach that executes localization directly at the edge device level using the Personalized PageRank (PPR) algorithm. The proposed method first groups microservices into communication- and colocation-aware clusters, thereby confining most anomaly propagation within cluster boundaries. Within each cluster, PPR is executed locally to identify the root cause, significantly reducing localization time. For the rare cases where anomalies propagate across clusters, we introduce an inter-cluster peer-to-peer approximation process, enabling lightweight coordination among clusters with minimal communication overhead. To enhance the accuracy of localization in heterogeneous edge environments, we also propose a novel anomaly scoring mechanism tailored to the diverse anomaly triggers that arise across microservice, device, and network layers. Evaluation results on the publicly available edge dataset, MicroCERCL, demonstrate that the proposed decentralized approach achieves comparable or higher localization accuracy than its centralized counterpart while reducing localization time by up to 34%. These findings highlight that decentralized graph-based RCL can provide a practical and efficient solution for anomaly diagnosis in resource-constrained edge environments.

</details>


### [57] [Iris: First-Class Multi-GPU Programming Experience in Triton](https://arxiv.org/abs/2511.12500)
*Muhammad Awad,Muhammad Osama,Brandon Potter*

Main category: cs.DC

TL;DR: Iris 是一个基于 Python 和 Triton 实现的多 GPU 通信库，通过提供与 Triton 编程模型对齐的 tile-based 对称内存抽象，使开发者能用单源代码实现计算与通信的高效重叠，在简化编程的同时达到甚至超越现有优化库（如 PyTorch 和 RCCL）的性能。


<details>
  <summary>Details</summary>
Motivation: 传统多 GPU 编程在性能与可编程性之间存在权衡：低层 HIP/CUDA 库虽高性能但开发复杂，高层抽象则常牺牲性能。作者旨在消除这一权衡。

Method: 提出 Iris 库，完全用 Python 和 Triton 实现，采用 tile-based 对称内存抽象，支持在单一 Triton kernel 中无缝交织计算与通信，并涵盖从批量同步到细粒度工作组特化的多种重叠模式。

Result: 微基准测试中 Iris 实现接近最优带宽利用率；在 GEMM+All-Scatter 工作负载上，相比 PyTorch 和 RCCL 最高提速达 1.79 倍。

Conclusion: Iris 证明了高层抽象也能实现高性能，同时大幅简化多 GPU 编程，有效弥合了性能与可编程性之间的鸿沟。

Abstract: Multi-GPU programming traditionally requires developers to navigate complex trade-offs between performance and programmability. High-performance implementations typically rely on low-level HIP/CUDA communication libraries that demand substantial engineering effort for even basic overlap patterns, while simpler abstractions often sacrifice performance. We present Iris, a multi-GPU communication library implemented entirely in Python and Triton that eliminates this trade-off. Iris provides tile-based symmetric memory abstractions that naturally align with Triton's programming model, enabling developers to write single-source kernels that seamlessly interleave computation and communication. We demonstrate a taxonomy of compute-communication overlap patterns--from bulk-synchronous to fine-grained workgroup specialization--that can be implemented with minimal code changes in Iris, often requiring just a few additional lines within the same Triton kernel. Our evaluation shows that Iris achieves near-optimal bandwidth utilization in microbenchmarks and delivers up to 1.79x speedup over PyTorch and RCCL for GEMM+All-Scatter workloads, demonstrating that high-level implementations can match or exceed heavily-optimized libraries while dramatically simplifying multi-GPU programming.

</details>


### [58] [Artifact for A Non-Intrusive Framework for Deferred Integration of Cloud Patterns in Energy-Efficient Data-Sharing Pipelines](https://arxiv.org/abs/2511.12667)
*Sepideh Masoudi,Mark Edward Michael Daly,Jannis Kiesel*

Main category: cs.DC

TL;DR: 本文提出了一种基于 Kubernetes 的工具，可在不修改服务代码的前提下非侵入式地延迟应用云设计模式，从而在保持数据转换服务可重用性的同时支持能耗感知决策。


<details>
  <summary>Details</summary>
Motivation: 随着数据网格架构的发展，组织广泛使用模块化、基于云的转换服务构建面向消费者的共享管道；然而，传统云设计模式的应用会降低这些服务在不同管道中的可重用性。

Method: 开发了一个基于 Kubernetes 的工具，实现设计模式的非侵入式、延迟注入，并自动收集能耗指标。

Result: 该工具在不修改服务代码的情况下提升了服务在多种管道结构中的可重用性，并支持基于能耗的优化决策。

Conclusion: 所提出的工具有效平衡了云设计模式应用与服务可重用性之间的矛盾，同时为构建能效敏感的数据管道提供了支持。

Abstract: As data mesh architectures grow, organizations increasingly build consumer-specific data-sharing pipelines from modular, cloud-based transformation services. While reusable transformation services can improve cost and energy efficiency, applying traditional cloud design patterns can reduce reusability of services in different pipelines. We present a Kubernetes-based tool that enables non-intrusive, deferred application of design patterns without modifying services code. The tool automates pattern injection and collects energy metrics, supporting energy-aware decisions while preserving reusability of transformation services in various pipeline structures.

</details>


### [59] [The Time to Consensus in a Blockchain: Insights into Bitcoin's "6 Blocks Rule''](https://arxiv.org/abs/2511.12687)
*Partha S. Dey,Aditya S. Gopalan,Vijay G. Subramanian*

Main category: cs.DC

TL;DR: 本文利用排队论技术研究了Nakamoto区块链中达成共识所需的时间，重点分析诚实与对抗两种增长过程，并在考虑诚实过程存在随机延迟的情况下，计算并验证了共识时间的拉普拉斯变换。


<details>
  <summary>Details</summary>
Motivation: 在Nakamoto区块链中，理解诚实节点与对抗节点之间的竞争动态对于评估系统安全性至关重要。现有研究缺乏对诚实增长过程中随机延迟影响的精确建模，因此需要一种新方法来量化达成共识所需的时间。

Method: 采用排队论方法对诚实和对抗两种增长过程进行建模，并在简化的比特币模型中推导共识时间的拉普拉斯变换，同时通过仿真验证理论结果。

Result: 成功计算出考虑随机延迟下共识时间的拉普拉斯变换，并通过仿真实验验证了该解析结果的准确性。

Conclusion: 该研究为理解Nakamoto共识机制中随机延迟对安全性的影响提供了理论基础，所提出的方法可用于进一步分析区块链系统的稳健性。

Abstract: We investigate the time to consensus in Nakamoto blockchains. Specifically, we consider two competing growth processes, labeled \emph{honest} and \emph{adversarial}, and determine the time after which the honest process permananetly exceeds the adversarial process. This is done via queueing techniques. The predominant difficulty is that the honest growth process is subject to \emph{random delays}. In a stylized Bitcoin model, we compute the Laplace transform for the time to consensus and verify it via simulation.

</details>


### [60] [Learning Process Energy Profiles from Node-Level Power Data](https://arxiv.org/abs/2511.13155)
*Jonathan Bader,Julius Irion,Jannis Kappel,Joel Witzke,Niklas Fomin,Diellza Sherifi,Odej Kao*

Main category: cs.DC

TL;DR: 本文提出一种基于eBPF和perf收集的细粒度进程资源指标，结合节点级能耗测量，通过回归模型实现更精确的进程级能耗预测。


<details>
  <summary>Details</summary>
Motivation: 随着高性能计算、云计算和人工智能的发展，数据中心能耗急剧上升，亟需在进程级别深入理解能耗情况以提升能效。现有方法（如Intel RAPL）受限于特定硬件且仅提供粗粒度的能耗估算。

Method: 利用eBPF和perf采集细粒度的进程级资源使用数据，并与通过电源分配单元获取的节点级能耗数据同步，通过回归模型学习进程资源使用与整体能耗之间的关系，从而预测每个进程的能耗。

Result: 该方法能够实现比现有机制更细粒度、更通用的进程级能耗预测。

Conclusion: 所提出的基于统计学习和细粒度监控的方法有效提升了进程级能耗建模的精度和适用性，有助于数据中心能效优化。

Abstract: The growing demand for data center capacity, driven by the growth of high-performance computing, cloud computing, and especially artificial intelligence, has led to a sharp increase in data center energy consumption. To improve energy efficiency, gaining process-level insights into energy consumption is essential. While node-level energy consumption data can be directly measured with hardware such as power meters, existing mechanisms for estimating per-process energy usage, such as Intel RAPL, are limited to specific hardware and provide only coarse-grained, domain-level measurements. Our proposed approach models per-process energy profiles by leveraging fine-grained process-level resource metrics collected via eBPF and perf, which are synchronized with node-level energy measurements obtained from an attached power distribution unit. By statistically learning the relationship between process-level resource usage and node-level energy consumption through a regression-based model, our approach enables more fine-grained per-process energy predictions.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [61] [Uncertainty-Guided Live Measurement Sequencing for Fast SAR ADC Linearity Testing](https://arxiv.org/abs/2511.11895)
*Thorben Schey,Khaled Karoonlatifi,Michael Weyrich,Andrey Morozov*

Main category: cs.AR

TL;DR: 本文提出了一种基于扩展卡尔曼滤波器（EKF）的闭环自适应测试方法，用于高效测试高分辨率SAR ADC的线性度，显著减少了测试时间和计算开销。


<details>
  <summary>Details</summary>
Motivation: 现有SAR ADC线性度测试方法（如直方图法、正弦波测试和模型驱动重建）通常依赖密集数据采集和离线后处理，导致测试时间长、复杂度高。

Method: 采用基于扩展卡尔曼滤波器（EKF）的迭代行为模型，在线实时估计决定INL特性的电容失配参数，并根据当前模型不确定性动态选择测量点，以最大化参数估计的信息增益。

Result: 实验结果表明，所提方法大幅减少了总测试时间和计算负担，无需大规模数据采集和后期分析。

Conclusion: 该自适应闭环测试方法高效、实用，适用于高分辨率SAR ADC在量产环境中的集成测试。

Abstract: This paper introduces a novel closed-loop testing methodology for efficient linearity testing of high-resolution Successive Approximation Register (SAR) Analog-to-Digital Converters (ADCs). Existing test strategies, including histogram-based approaches, sine wave testing, and model-driven reconstruction, often rely on dense data acquisition followed by offline post-processing, which increases overall test time and complexity. To overcome these limitations, we propose an adaptive approach that utilizes an iterative behavioral model refined by an Extended Kalman Filter (EKF) in real time, enabling direct estimation of capacitor mismatch parameters that determine INL behavior. Our algorithm dynamically selects measurement points based on current model uncertainty, maximizing information gain with respect to parameter confidence and narrowing sampling intervals as estimation progresses. By providing immediate feedback and adaptive targeting, the proposed method eliminates the need for large-scale data collection and post-measurement analysis. Experimental results demonstrate substantial reductions in total test time and computational overhead, highlighting the method's suitability for integration in production environments.

</details>


### [62] [Advanced Strategies for Uncertainty-Guided Live Measurement Sequencing in Fast, Robust SAR ADC Linearity Testing](https://arxiv.org/abs/2511.11917)
*Thorben Schey,Khaled Karoonlatifi,Michael Weyrich,Andrey Morozov*

Main category: cs.AR

TL;DR: 本文提出了一种增强型不确定性引导的实时测量序列方法（UGLMS），通过秩-1 EKF更新、协方差膨胀策略、低阶载波多项式扩展和基于迹的终止机制，显著加快SAR ADC线性度测试速度，在保持精度的同时将16位ADC测试提速8倍，实现毫秒级INL/DNL重建。


<details>
  <summary>Details</summary>
Motivation: 传统SAR ADC线性度测试依赖全范围扫描和离线后处理，耗时且难以用于生产环境；现有UGLMS虽已实现闭环自适应测试，但仍存在计算开销大、收敛慢及模型表达能力有限等问题。

Method: 1）采用秩-1 EKF更新替代矩阵求逆，降低计算复杂度；2）引入与测量对齐的协方差膨胀策略以加速收敛；3）在静态失配模型中加入低阶载波多项式以建模系统非线性；4）设计基于协方差迹的自适应终止准则动态控制测试长度。

Result: 仿真表明，增强型UGLMS可在36 ms内完成16位ADC、70 ms内完成18位ADC的完整INL/DNL重建（含多项式扩展时为120 ms），相比原方法在相同精度下测试速度提升8倍。

Conclusion: 所提增强型UGLMS显著提升了SAR ADC线性度测试的效率与实用性，具备实时性和量产适用性，为高精度ADC的快速在线测试提供了有效解决方案。

Abstract: This paper builds on our Uncertainty-Guided Live Measurement Sequencing (UGLMS) method. UGLMS is a closed-loop test strategy that adaptively selects SAR ADC code edges based on model uncertainty and refines a behavioral mismatch model in real time via an Extended Kalman Filter (EKF), eliminating full-range sweeps and offline post-processing. We introduce an enhanced UGLMS that delivers significantly faster test runtimes while maintaining estimation accuracy. First, a rank-1 EKF update replaces costly matrix inversions with efficient vector operations, and a measurement-aligned covariance-inflation strategy accelerates convergence under unexpected innovations. Second, we extend the static mismatch model with a low-order carrier polynomial to capture systematic nonlinearities beyond pure capacitor mismatch. Third, a trace-based termination adapts test length to convergence, preventing premature stops and redundant iterations. Simulations show the enhanced UGLMS reconstructs full Integral- and Differential-Non-Linearity (INL/DNL) in just 36 ms for 16-bit and under 70 ms for 18-bit ADCs (120 ms with the polynomial extension). Combining the faster convergence from covariance inflation with reduced per-iteration runtime from the rank-1 EKF update, the method reaches equal accuracy 8x faster for 16-bit ADCs. These improvements enable real-time, production-ready SAR ADC linearity testing.

</details>


### [63] [TIMERIPPLE: Accelerating vDiTs by Understanding the Spatio-Temporal Correlations in Latent Space](https://arxiv.org/abs/2511.12035)
*Wenxuan Miao,Yulin Sun,Aiyue Chen,Jing Lin,Yiwu Yao,Yiming Gan,Jieru Zhao,Jingwen Leng,Mingyi Guo,Yu Feng*

Main category: cs.AR

TL;DR: 本文提出了一种轻量级自适应重用策略，通过利用视频扩散Transformer（vDiT）中潜在空间的时空相关性，在保持几乎相同视频质量的前提下，显著减少自注意力计算开销（节省85%计算量）。


<details>
  <summary>Details</summary>
Motivation: 现有基于vDiT的视频生成模型因自注意力机制导致推理延迟严重；以往加速方法忽略了视频流固有的时空相关性，直接套用大语言模型的稀疏模式，效果有限。

Method: 分析vDiT中自注意力模式，发现其主要源于token通道层面的主导时空相关性；据此设计一种轻量且自适应的注意力分数重用策略，对时空相关token在各通道上的部分注意力分数进行复用，以近似完整计算。

Result: 在4种vDiT模型上实验表明，该方法相比当前最优技术可节省85%的计算量，同时视频质量损失极小（VBench指标下降不到0.06%）。

Conclusion: 利用vDiT潜在空间中的时空相关性，可高效加速自注意力计算，实现高保真视频生成与显著推理加速的兼顾。

Abstract: The recent surge in video generation has shown the growing demand for high-quality video synthesis using large vision models. Existing video generation models are predominantly based on the video diffusion transformer (vDiT), however, they suffer from substantial inference delay due to self-attention. While prior studies have focused on reducing redundant computations in self-attention, they often overlook the inherent spatio-temporal correlations in video streams and directly leverage sparsity patterns from large language models to reduce attention computations.
  In this work, we take a principled approach to accelerate self-attention in vDiTs by leveraging the spatio-temporal correlations in the latent space. We show that the attention patterns within vDiT are primarily due to the dominant spatial and temporal correlations at the token channel level. Based on this insight, we propose a lightweight and adaptive reuse strategy that approximates attention computations by reusing partial attention scores of spatially or temporally correlated tokens along individual channels. We demonstrate that our method achieves significantly higher computational savings (85\%) compared to state-of-the-art techniques over 4 vDiTs, while preserving almost identical video quality ($<$0.06\% loss on VBench).

</details>


### [64] [A digital SRAM-based compute-in-memory macro for weight-stationary dynamic matrix multiplication in Transformer attention score computation](https://arxiv.org/abs/2511.12152)
*Jianyi Yu,Yuxuan Wang,Xiang Fu,Fei Qiao,Ying Wang,Rui Yuan,Liyuan Liu,Cong Shi*

Main category: cs.AR

TL;DR: 本文提出了一种用于计算Transformer注意力机制的数字存内计算（CIM）宏单元，通过重构注意力分数计算流程并优化电路设计，在65nm工艺下实现了高能效和高面积效率。


<details>
  <summary>Details</summary>
Motivation: 传统AI处理器中计算与存储单元间频繁的数据搬运导致功耗和延迟瓶颈，而现有CIM架构难以高效支持Transformer中的动态矩阵乘法操作。

Method: 将注意力分数计算重构为基于组合QK权重矩阵的形式，使输入可直接送入CIM单元；将二项式矩阵乘法分解为4组比特串行移位与加法操作，并采用零值比特跳过、数据驱动字线激活、读写分离6T单元及比特交替14T/28T加法器等技术提升能效。

Result: 在65nm工艺下实现0.35 mm²面积，峰值性能达42.27 GOPS，功耗仅1.24 mW（1.0 V, 100 MHz），能效达34.1 TOPS/W，面积效率达120.77 GOPS/mm²；相比CPU/GPU分别提升25倍和13倍能效，相较其他Transformer-CIM方案至少提升7倍能效与2倍面积效率。

Conclusion: 所提出的CIM宏单元在能效和面积效率方面显著优于现有方案，展现出在边缘智能应用中的巨大潜力。

Abstract: Compute-in-memory (CIM) techniques are widely employed in energy-efficient artificial intelligent (AI) processors. They alleviate power and latency bottlenecks caused by extensive data movements between compute and storage units. This work proposes a digital CIM macro to compute Transformer attention. To mitigate dynamic matrix multiplication that is unsuitable for the common weight-stationary CIM paradigm, we reformulate the attention score computation process based on a combined QK-weight matrix, so that inputs can be directly fed to CIM cells to obtain the score results. Moreover, the involved binomial matrix multiplication operation is decomposed into 4 groups of bit-serial shifting and additions, without costly physical multipliers in the CIM. We maximize the energy efficiency of the CIM circuit through zero-value bit-skipping, data-driven word line activation, read-write separate 6T cells and bit-alternating 14T/28T adders. The proposed CIM macro was implemented using a 65-nm process. It occupied only 0.35 mm2 area, and delivered a 42.27 GOPS peak performance with 1.24 mW power consumption at a 1.0 V power supply and a 100 MHz clock frequency, resulting in 34.1 TOPS/W energy efficiency and 120.77 GOPS/mm2 area efficiency. When compared to the CPU and GPU, our CIM macro is 25x and 13x more energy efficient on practical tasks, respectively. Compared with other Transformer-CIMs, our design exhibits at least 7x energy efficiency and at least 2x area efficiency improvements when scaled to the same technology node, showcasing its potential for edge-side intelligent applications.

</details>


### [65] [Sangam: Chiplet-Based DRAM-PIM Accelerator with CXL Integration for LLM Inferencing](https://arxiv.org/abs/2511.12286)
*Khyati Kiyawat,Zhenxing Fan,Yasas Seneviratne,Morteza Baradaran,Akhil Shekar,Zihan Xia,Mingu Kang,Kevin Skadron*

Main category: cs.AR

TL;DR: 本文提出了一种基于chiplet的内存模块Sangam，通过异构集成逻辑与存储芯片，利用CXL接口实现对大语言模型推理中内存密集型操作的高效加速，在延迟、吞吐量和能效方面显著优于H100 GPU。


<details>
  <summary>Details</summary>
Motivation: 大语言模型推理受内存带宽限制，现有存内计算（PIM）方案受限于DRAM内集成处理单元带来的容量减少和计算能力不足。

Method: 设计一种chiplet架构的内存模块Sangam，将逻辑与存储分离并采用异构工艺制造，通过中介层互连；逻辑chiplet包含脉动阵列和SRAM缓存，用于加速GEMM类内存密集型操作，并通过CXL接口连接主机，可替代或协同GPU工作。

Result: 在LLaMA 2-7B、Mistral-7B和LLaMA 3-70B模型上，相比H100 GPU，Sangam分别实现了最高3.93倍查询延迟降低、10.3倍解码吞吐提升及数量级级别的能效改进。

Conclusion: 通过chiplet与CXL技术结合的PIM架构能有效突破传统存内计算的局限性，为大语言模型推理提供高能效、高性能的硬件解决方案。

Abstract: Large Language Models (LLMs) are becoming increasingly data-intensive due to growing model sizes, and they are becoming memory-bound as the context length and, consequently, the key-value (KV) cache size increase. Inference, particularly the decoding phase, is dominated by memory-bound GEMV or flat GEMM operations with low operational intensity (OI), making it well-suited for processing-in-memory (PIM) approaches. However, existing in/near-memory solutions face critical limitations such as reduced memory capacity due to the high area cost of integrating processing elements (PEs) within DRAM chips, and limited PE capability due to the constraints of DRAM fabrication technology. This work presents a chiplet-based memory module that addresses these limitations by decoupling logic and memory into chiplets fabricated in heterogeneous technology nodes and connected via an interposer. The logic chiplets sustain high bandwidth access to the DRAM chiplets, which house the memory banks, and enable the integration of advanced processing components such as systolic arrays and SRAM-based buffers to accelerate memory-bound GEMM kernels, capabilities that were not feasible in prior PIM architectures. We propose Sangam, a CXL-attached PIM-chiplet based memory module that can either act as a drop-in replacement for GPUs or co-executes along side the GPUs. Sangam achieves speedup of 3.93, 4.22, 2.82x speedup in end-to-end query latency, 10.3, 9.5, 6.36x greater decoding throughput, and order of magnitude energy savings compared to an H100 GPU for varying input size, output length, and batch size on LLaMA 2-7B, Mistral-7B, and LLaMA 3-70B, respectively.

</details>


### [66] [Pushing the Memory Bandwidth Wall with CXL-enabled Idle I/O Bandwidth Harvesting](https://arxiv.org/abs/2511.12349)
*Divya Kiran Kadiyala,Alexandros Daglis*

Main category: cs.AR

TL;DR: 本文提出了一种名为SURGE的软硬件协同架构技术，通过动态复用闲置的I/O带宽来提升内存带宽，从而缓解多核服务器中每核内存带宽不足的问题，在带宽受限的服务器上可将内存密集型工作负载加速最多1.3倍。


<details>
  <summary>Details</summary>
Motivation: 随着服务器级CPU核心数量持续增加，内存系统面临引脚数量和数据传输速率扩展性有限的瓶颈，导致高端处理器每核可用内存带宽下降，影响内存密集型应用性能。传统设计将引脚固定分配给内存和I/O，造成带宽资源碎片化和利用率低下。

Method: 提出SURGE架构，利用CXL等多功能互连技术，在软件支持下动态复用内存与I/O流量共用同一处理器接口，实现内存与I/O带宽的灵活调配。

Result: 实验表明，采用SURGE的架构在带宽受限的服务器上可将内存密集型工作负载性能最高提升1.3倍。

Conclusion: 通过实现内存与I/O带宽的可互换性，SURGE有效提升了服务器内存系统的整体带宽利用率，为未来多核处理器的内存瓶颈问题提供了可行的解决方案。

Abstract: The continual increase of cores on server-grade CPUs raises demands on memory systems, which are constrained by limited off-chip pin and data transfer rate scalability. As a result, high-end processors typically feature lower memory bandwidth per core, at the detriment of memory-intensive workloads. We propose alleviating this challenge by improving the utility of the CPU's limited pins. In a typical CPU design process, the available pins are apportioned between memory and I/O traffic, each accounting for about half of the total off-chip bandwidth availability. Consequently, unless both memory and I/O are simultaneously highly utilized, such fragmentation leads to underutilization of the valuable off-chip bandwidth resources. An ideal architecture would offer I/O and memory bandwidth fungibility, allowing use of the aggregate off-chip bandwidth in the form required by each workload.
  In this work, we introduce SURGE, a software-supported architectural technique that boosts memory bandwidth availability by salvaging idle I/O bandwidth resources. SURGE leverages the capability of versatile interconnect technologies like CXL to dynamically multiplex memory and I/O traffic over the same processor interface. We demonstrate that SURGE-enhanced architectures can accelerate memory-intensive workloads on bandwidth-constrained servers by up to 1.3x.

</details>


### [67] [FERMI-ML: A Flexible and Resource-Efficient Memory-In-Situ SRAM Macro for TinyML acceleration](https://arxiv.org/abs/2511.12544)
*Mukul Lokhande,Akash Sankhe,S. V. Jaya Chand,Santosh Kumar Vishvakarma*

Main category: cs.AR

TL;DR: 本文提出了FERMI-ML，一种面向TinyML的灵活、资源高效的存内计算SRAM宏架构，支持可变精度MAC和CAM操作，在65 nm工艺下实现了高能效与高吞吐量。


<details>
  <summary>Details</summary>
Motivation: 为满足AIoT设备对低功耗、面积高效TinyML推理的需求，需减少数据移动并维持高计算效率，因此需要新型内存架构。

Method: 设计了一种基于9T XNOR的RX9T位单元，集成5T存储单元与4T XNOR计算单元，并采用22管晶体管压缩树累加器（C22T）实现1–64位对数级MAC运算；构建4 KB SRAM宏，支持Posit-4/FP-4精度下的存内计算与CAM查找。

Result: 在65 nm工艺下，该宏工作频率达350 MHz（0.9 V），吞吐量1.93 TOPS，能效364 TOPS/W，InceptionV4与ResNet-18模型QoR超过97.5%。

Conclusion: FERMI-ML是一种紧凑、可重构且节能的数字存内计算宏，适用于混合精度TinyML任务。

Abstract: The growing demand for low-power and area-efficient TinyML inference on AIoT devices necessitates memory architectures that minimise data movement while sustaining high computational efficiency. This paper presents FERMI-ML, a Flexible and Resource-Efficient Memory-In-Situ (MIS) SRAM macro designed for TinyML acceleration. The proposed 9T XNOR-based RX9T bit-cell integrates a 5T storage cell with a 4T XNOR compute unit, enabling variable-precision MAC and CAM operations within the same array. A 22-transistor (C22T) compressor-tree-based accumulator facilitates logarithmic 1-64-bit MAC computation with reduced delay and power compared to conventional adder trees. The 4 KB macro achieves dual functionality for in-situ computation and CAM-based lookup operations, supporting Posit-4 or FP-4 precision. Post-layout results at 65 nm show operation at 350 MHz with 0.9 V, delivering a throughput of 1.93 TOPS and an energy efficiency of 364 TOPS/W, while maintaining a Quality-of-Result (QoR) above 97.5% with InceptionV4 and ResNet-18. FERMI-ML thus demonstrates a compact, reconfigurable, and energy-aware digital Memory-In-Situ macro capable of supporting mixed-precision TinyML workloads.

</details>


### [68] [SynapticCore-X: A Modular Neural Processing Architecture for Low-Cost FPGA Acceleration](https://arxiv.org/abs/2511.12616)
*Arya Parameshwara*

Main category: cs.AR

TL;DR: 本文提出了SynapticCore-X，一种模块化、资源高效的神经处理架构，专为低成本FPGA平台优化，结合轻量级RISC-V核心与可配置神经计算单元，支持开源、可调参数和高效硬件验证。


<details>
  <summary>Details</summary>
Motivation: 现有FPGA加速器通常依赖重量级IP模块，缺乏开放性和灵活性，难以在低成本平台上进行神经微架构研究；作者旨在降低学术界和开源硬件社区在神经处理器设计上的门槛。

Method: 设计了一个集成RV32IMC RISC-V控制核心与可配置神经计算单元的架构，采用完全开源的SystemVerilog实现，并提供可调节的并行度、暂存存储深度和DMA突发行为；同时构建了自动化的Vivado构建流程。

Result: 在Zynq-7020上实现100 MHz时序收敛，仅占用6.1% LUT、32.5% DSP和21.4% BRAM；在PYNQ-Z2上完成硬件验证，确认寄存器级执行正确性、控制路径确定性及矩阵/卷积核的周期精确性能。

Conclusion: SynapticCore-X证明了在普通教育级FPGA上也能实现类NPU的能效加速，为神经微架构的学术与开源研究提供了低门槛、高灵活性的原型平台。

Abstract: This paper presents SynapticCore-X, a modular and resource-efficient neural processing architecture optimized for deployment on low-cost FPGA platforms. The design integrates a lightweight RV32IMC RISC-V control core with a configurable neural compute tile that supports fused matrix, activation, and data-movement operations. Unlike existing FPGA accelerators that rely on heavyweight IP blocks, SynapticCore-X provides a fully open-source SystemVerilog microarchitecture with tunable parallelism, scratchpad memory depth, and DMA burst behavior, enabling rapid exploration of hardware-software co-design trade-offs. We document an automated, reproducible Vivado build pipeline that achieves timing closure at 100 MHz on the Zynq-7020 while consuming only 6.1% LUTs, 32.5% DSPs, and 21.4% BRAMs. Hardware validation on PYNQ-Z2 confirms correct register-level execution, deterministic control-path behavior, and cycle-accurate performance for matrix and convolution kernels. SynapticCore-X demonstrates that energy-efficient NPU-like acceleration can be prototyped on commodity educational FPGAs, lowering the entry barrier for academic and open-hardware research in neural microarchitectures.

</details>


### [69] [Neo: Real-Time On-Device 3D Gaussian Splatting with Reuse-and-Update Sorting Acceleration](https://arxiv.org/abs/2511.12930)
*Changhun Oh,Seongryong Oh,Jinwoo Hwang,Yoonsung Kim,Hardik Sharma,Jongse Park*

Main category: cs.AR

TL;DR: 本文提出Neo系统，通过一种利用帧间高斯排序冗余性的重用与更新排序算法及配套硬件加速器，显著提升3D高斯泼溅（3DGS）渲染在资源受限设备上的实时性能。


<details>
  <summary>Details</summary>
Motivation: 现有3DGS渲染方案在高分辨率下难以在资源受限设备上实现实时高帧率，主要瓶颈在于排序阶段对内存带宽的高需求。

Method: 提出一种重用与更新的排序算法，利用连续帧之间高斯顺序的时间冗余性，并设计专用硬件加速器，避免每帧从头排序，仅跟踪和更新深度顺序。

Result: Neo相较当前最先进的边缘GPU和ASIC方案，吞吐量分别提升10.0倍和5.6倍，同时DRAM流量减少94.5%和81.3%。

Conclusion: Neo显著降低了3DGS渲染的计算与内存开销，使高质量、低延迟的端侧3D渲染更具可行性。

Abstract: 3D Gaussian Splatting (3DGS) rendering in real-time on resource-constrained devices is essential for delivering immersive augmented and virtual reality (AR/VR) experiences. However, existing solutions struggle to achieve high frame rates, especially for high-resolution rendering. Our analysis identifies the sorting stage in the 3DGS rendering pipeline as the major bottleneck due to its high memory bandwidth demand. This paper presents Neo, which introduces a reuse-and-update sorting algorithm that exploits temporal redundancy in Gaussian ordering across consecutive frames, and devises a hardware accelerator optimized for this algorithm. By efficiently tracking and updating Gaussian depth ordering instead of re-sorting from scratch, Neo significantly reduces redundant computations and memory bandwidth pressure. Experimental results show that Neo achieves up to 10.0x and 5.6x higher throughput than state-of-the-art edge GPU and ASIC solution, respectively, while reducing DRAM traffic by 94.5% and 81.3%. These improvements make high-quality and low-latency on-device 3D rendering more practical.

</details>


### [70] [Coliseum project: Correlating climate change data with the behavior of heritage materials](https://arxiv.org/abs/2511.13343)
*A Cormier,David Roqui,Fabrice Surma,Martin Labouré,Jean-Marc Vallet,Odile Guillon,N Grozavu,Ann Bourgès*

Main category: cs.AR

TL;DR: 本文介绍了COLISEUM项目在法国三个文化遗产地（斯特拉斯堡圣母院、比布拉克特考古遗址和维尔弗朗什-苏尔-梅尔圣皮埃尔小教堂）开展的多模态数据采集与气候监测方法，旨在通过人工智能模型预测气候变化对遗产材料劣化的影响，并展示了斯特拉斯堡大教堂的初步诊断与结果。


<details>
  <summary>Details</summary>
Motivation: 气候变化正加速遗产材料的劣化，但由于风化过程受多种因素影响且数据具有多模态特性，难以建立其与气候变化之间的明确关联。因此，亟需一种系统性方法来整合气候与劣化数据，以预测未来不同气候情景下遗产材料的行为。

Method: 在三个具有不同气候和材料特征的法国文化遗产地部署微气候传感器，持续记录环境参数；同时定期通过化学分析、测绘测量和科学成像手段监测材料劣化状态；将多源数据整合为劣化矩阵，并计算风化指数，用于构建基于人工智能的预测模型。

Result: 文章展示了斯特拉斯堡大教堂站点的仪器部署方法、初始诊断结果及初步数据分析，验证了所提多模态数据采集与整合方法的可行性。

Conclusion: 该研究为预测气候变化对文化遗产材料的长期影响提供了可行的技术路径，未来可结合IPCC气候情景进一步优化AI模型，支持遗产保护决策。

Abstract: Heritage materials are already affected by climate change, and increasing climatic variations reduces the lifespan of monuments. As weathering depends on many factors, it is also difficult to link its progression to climatic changes. To predict weathering, it is essential to gather climatic data while simultaneously monitoring the progression of deterioration. The multimodal nature of collected data (images, text{\ldots}) makes correlations difficult, particularly on different time scales. To address this issue, the COLISEUM project proposes a methodology for collecting data in three French sites to predict heritage material behaviour using artificial intelligence computer models. Over time, prediction models will allow the prediction of future material behaviours using known data from different climate change scenarios by the IPCC (Intergovernmental Panel on Climate Change). Thus, a climate monitoring methodology has been set up in three cultural sites in France: Notre-Dame cathedral in Strasbourg ( 67), Bibracte archaeological site (71), and the Saint-Pierre chapel in Villefranche-sur-Mer (06). Each site has a different climate and specific materials. In situ, microclimatic sensors continuously record variations parameters over time. The state of alteration is monitored at regular intervals by means of chemical analyses, cartographic measurements and scientific imaging campaigns. To implement weathering models, data is gathered in alteration matrix by mean of a calculated weathering index. This article presents the instrumentation methodology, the initial diagnostic and the first results with the example of Strasbourg Cathedral site.

</details>


### [71] [T-SAR: A Full-Stack Co-design for CPU-Only Ternary LLM Inference via In-Place SIMD ALU Reorganization](https://arxiv.org/abs/2511.13676)
*Hyunwoo Oh,KyungIn Nam,Rajat Bhattacharjya,Hanning Chen,Tamoghno Das,Sanggeon Yun,Suyeon Jang,Andrew Ding,Nikil Dutt,Mohsen Imani*

Main category: cs.AR

TL;DR: 本文提出了T-SAR，首个在CPU上实现可扩展三值大语言模型（LLM）推理的框架，通过复用SIMD寄存器文件动态生成查找表，显著提升能效与性能。


<details>
  <summary>Details</summary>
Motivation: 现有边缘设备主要依赖CPU，难以承载日益庞大的LLM；虽然三值量化可节省资源，但当前CPU方案依赖内存中的查找表（LUT），限制了可扩展性，而FPGA或GPU加速器又不适合边缘场景。

Method: T-SAR通过少量硬件修改，将SIMD寄存器文件用于动态、寄存器内的LUT生成，消除内存瓶颈并最大化数据级并行性。

Result: 在GEMM延迟和GEMV吞吐量上分别实现5.6–24.5倍和1.1–86.2倍的提升，SIMD单元仅增加3.2%功耗和1.4%面积开销；相比NVIDIA Jetson AGX Orin，能效提升达2.5–4.9倍。

Conclusion: T-SAR为边缘平台上的高效LLM推理提供了一种实用且可扩展的解决方案。

Abstract: Recent advances in LLMs have outpaced the computational and memory capacities of edge platforms that primarily employ CPUs, thereby challenging efficient and scalable deployment. While ternary quantization enables significant resource savings, existing CPU solutions rely heavily on memory-based lookup tables (LUTs) which limit scalability, and FPGA or GPU accelerators remain impractical for edge use. This paper presents T-SAR, the first framework to achieve scalable ternary LLM inference on CPUs by repurposing the SIMD register file for dynamic, in-register LUT generation with minimal hardware modifications. T-SAR eliminates memory bottlenecks and maximizes data-level parallelism, delivering 5.6-24.5x and 1.1-86.2x improvements in GEMM latency and GEMV throughput, respectively, with only 3.2% power and 1.4% area overheads in SIMD units. T-SAR achieves up to 2.5-4.9x the energy efficiency of an NVIDIA Jetson AGX Orin, establishing a practical approach for efficient LLM inference on edge platforms.

</details>


### [72] [QUILL: An Algorithm-Architecture Co-Design for Cache-Local Deformable Attention](https://arxiv.org/abs/2511.13679)
*Hyunwoo Oh,Hanning Chen,Sanggeon Yun,Yang Ni,Wenjun Huang,Tamoghno Das,Suyeon Jang,Mohsen Imani*

Main category: cs.AR

TL;DR: QUILL是一种面向可变形注意力的调度感知加速器，通过基于距离的乱序查询（DOOQ）和融合计算引擎，显著提升吞吐量与能效，同时保持高精度。


<details>
  <summary>Details</summary>
Motivation: 可变形Transformer虽然在检测任务中性能优越，但其不规则内存访问和低算术强度导致难以高效映射到硬件上。

Method: 提出QUILL加速器，采用DOOQ策略按空间邻近性排序查询，并结合预取机制形成调度感知的预取循环；同时设计融合的MSDeformAttn引擎，在单次遍历中完成插值、Softmax、聚合和投影，避免中间数据溢出，并利用片上存储和集成GEMM处理密集层。

Result: QUILL相比RTX 4090最高实现7.29倍吞吐量提升和47.3倍能效提升，优于先前加速器3.26–9.82倍吞吐量和2.01–6.07倍能效；在混合精度量化下，精度损失不超过0.9 AP。

Conclusion: QUILL通过将稀疏性转化为局部性、再将局部性转化为硬件利用率，实现了端到端一致的加速效果，在保持精度的同时大幅提升性能与能效。

Abstract: Deformable transformers deliver state-of-the-art detection but map poorly to hardware due to irregular memory access and low arithmetic intensity. We introduce QUILL, a schedule-aware accelerator that turns deformable attention into cache-friendly, single-pass work. At its core, Distance-based Out-of-Order Querying (DOOQ) orders queries by spatial proximity; the look-ahead drives a region prefetch into an alternate buffer--forming a schedule-aware prefetch loop that overlaps memory and compute. A fused MSDeformAttn engine executes interpolation, Softmax, aggregation, and the final projection (W''m) in one pass without spilling intermediates, while small tensors are kept on-chip and surrounding dense layers run on integrated GEMMs. Implemented as RTL and evaluated end-to-end, QUILL achieves up to 7.29x higher throughput and 47.3x better energy efficiency than an RTX 4090, and exceeds prior accelerators by 3.26-9.82x in throughput and 2.01-6.07x in energy efficiency. With mixed-precision quantization, accuracy tracks FP32 within <=0.9 AP across Deformable and Sparse DETR variants. By converting sparsity into locality--and locality into utilization--QUILL delivers consistent, end-to-end speedups.

</details>
