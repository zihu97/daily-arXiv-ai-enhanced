{"id": "2601.10953", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2601.10953", "abs": "https://arxiv.org/abs/2601.10953", "authors": ["Junming Zhang", "Qinyan Zhang", "Huajun Sun", "Feiyang Gao", "Sheng Hu", "Rui Nie", "Xiangshui Miao"], "title": "SwiftKV: An Edge-Oriented Attention Algorithm and Multi-Head Accelerator for Fast, Efficient LLM Decoding", "comment": null, "summary": "Edge acceleration for large language models is crucial for their widespread application; however, achieving fast attention inference and efficient decoding on resource-constrained edge accelerators remains challenging. This paper presents SwiftKV Attention, a per-token pipelined, low-latency single-pass attention inference algorithm, where every (kt, vt) in the KV cache is processed exactly once in a uniform per-token pipeline without score materialization, blockwise softmax, or a second pass, thereby enabling fast execution on edge accelerators with a single hardware set and no resource-intensive parallelism. Furthermore, to address the limited support for multi-head LLM decoding in existing accelerators, we design the SwiftKV-MHA accelerator, which enables high precision attention and low precision GEMV on the same processor array, achieving fast and efficient multi-head parallel decoding. Experimental results show that, on the edge accelerator, the SwiftKV Attention algorithm achieves a 7.16* speedup over native attention and significantly outperforms other attention algorithms. SwiftKV-MHA further reduces attention latency by 13.48*; under the same settings, it improves generation speed by 17.4% and increases token efficiency by 1.98* compared with state-of-the-art works.", "AI": {"tldr": "SwiftKV Attention\u7b97\u6cd5\u4e0eSwiftKV-MHA\u52a0\u901f\u5668\u663e\u8457\u63d0\u5347\u8fb9\u7f18\u8bbe\u5907\u4e0a\u5927\u8bed\u8a00\u6a21\u578b\u7684\u63a8\u7406\u901f\u5ea6\u548c\u6548\u7387\u3002", "motivation": "\u89e3\u51b3\u8fb9\u7f18\u8bbe\u5907\u8d44\u6e90\u53d7\u9650\u4e0b\u5927\u8bed\u8a00\u6a21\u578b\u6ce8\u610f\u529b\u673a\u5236\u63a8\u7406\u6162\u3001\u89e3\u7801\u6548\u7387\u4f4e\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u5355\u6b21\u5904\u7406\u3001\u65e0\u9700\u5f97\u5206\u7269\u5316\u6216\u4e8c\u6b21\u904d\u5386\u7684SwiftKV Attention\u7b97\u6cd5\uff0c\u5e76\u8bbe\u8ba1\u652f\u6301\u9ad8\u4f4e\u7cbe\u5ea6\u6df7\u5408\u8ba1\u7b97\u7684SwiftKV-MHA\u52a0\u901f\u5668\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cSwiftKV Attention\u6bd4\u539f\u751f\u6ce8\u610f\u529b\u5feb7.16\u500d\uff0cSwiftKV-MHA\u8fdb\u4e00\u6b65\u964d\u4f4e\u5ef6\u8fdf13.48\u500d\uff0c\u751f\u6210\u901f\u5ea6\u63d0\u534717.4%\uff0ctoken\u6548\u7387\u63d0\u9ad81.98\u500d\u3002", "conclusion": "SwiftKV\u65b9\u6848\u5728\u8fb9\u7f18\u8bbe\u5907\u4e0a\u5b9e\u73b0\u4e86\u9ad8\u6548\u3001\u4f4e\u5ef6\u8fdf\u7684\u5927\u8bed\u8a00\u6a21\u578b\u6ce8\u610f\u529b\u63a8\u7406\u4e0e\u591a\u5934\u5e76\u884c\u89e3\u7801\u3002"}}
{"id": "2601.11057", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2601.11057", "abs": "https://arxiv.org/abs/2601.11057", "authors": ["Hongshi Tan", "Yao Chen", "Xinyu Chen", "Qizhen Zhang", "Cheng Chen", "Weng-Fai Wong", "Bingsheng He"], "title": "RidgeWalker: Perfectly Pipelined Graph Random Walks on FPGAs", "comment": "Accepted by HPCA 2026", "summary": "Graph Random Walks (GRWs) offer efficient approximations of key graph properties and have been widely adopted in many applications. However, GRW workloads are notoriously difficult to accelerate due to their strong data dependencies, irregular memory access patterns, and imbalanced execution behavior. While recent work explores FPGA-based accelerators for GRWs, existing solutions fall far short of hardware potential due to inefficient pipelining and static scheduling. This paper presents RidgeWalker, a high-performance GRW accelerator designed for datacenter FPGAs. The key insight behind RidgeWalker is that the Markov property of GRWs allows decomposition into stateless, fine-grained tasks that can be executed out-of-order without compromising correctness. Building on this, RidgeWalker introduces an asynchronous pipeline architecture with a feedback-driven scheduler grounded in queuing theory, enabling perfect pipelining and adaptive load balancing. We prototype RidgeWalker on datacenter FPGAs and evaluated it across a range of GRW algorithms and real-world graph datasets. Experimental results demonstrate that RidgeWalker achieves an average speedup of 7.0x over state-of-the-art FPGA solutions and 8.1x over GPU solutions, with peak speedups of up to 71.0x and 22.9x, respectively. The source code is publicly available at https://github.com/Xtra-Computing/RidgeWalker.", "AI": {"tldr": "RidgeWalker\u662f\u4e00\u79cd\u4e13\u4e3a\u6570\u636e\u4e2d\u5fc3FPGA\u8bbe\u8ba1\u7684\u9ad8\u6027\u80fd\u56fe\u968f\u673a\u6e38\u8d70\u52a0\u901f\u5668\uff0c\u901a\u8fc7\u5f02\u6b65\u6d41\u6c34\u7ebf\u548c\u53cd\u9988\u9a71\u52a8\u8c03\u5ea6\u663e\u8457\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u73b0\u6709FPGA\u65b9\u6848\u56e0\u6d41\u6c34\u7ebf\u6548\u7387\u4f4e\u548c\u9759\u6001\u8c03\u5ea6\u65e0\u6cd5\u5145\u5206\u53d1\u6325\u786c\u4ef6\u6f5c\u529b\uff0c\u4e9f\u9700\u66f4\u9ad8\u6548\u7684GRW\u52a0\u901f\u65b9\u6cd5\u3002", "method": "\u5229\u7528GRW\u7684\u9a6c\u5c14\u53ef\u592b\u6027\u8d28\u5206\u89e3\u4e3a\u65e0\u72b6\u6001\u7ec6\u7c92\u5ea6\u4efb\u52a1\uff0c\u7ed3\u5408\u57fa\u4e8e\u6392\u961f\u7406\u8bba\u7684\u5f02\u6b65\u6d41\u6c34\u7ebf\u4e0e\u81ea\u9002\u5e94\u8c03\u5ea6\u67b6\u6784\u3002", "result": "\u5728\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u5e73\u5747\u63d0\u901f7.0\u500d\u4e8e\u5148\u8fdbFPGA\u65b9\u6848\u30018.1\u500d\u4e8eGPU\u65b9\u6848\uff0c\u5cf0\u503c\u63d0\u901f\u5206\u522b\u8fbe71.0\u500d\u548c22.9\u500d\u3002", "conclusion": "RidgeWalker\u6709\u6548\u89e3\u51b3\u4e86GRW\u52a0\u901f\u4e2d\u7684\u4f9d\u8d56\u6027\u4e0e\u8d1f\u8f7d\u4e0d\u5747\u95ee\u9898\uff0c\u663e\u8457\u91ca\u653e\u4e86FPGA\u786c\u4ef6\u6f5c\u529b\u3002"}}
{"id": "2601.11292", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2601.11292", "abs": "https://arxiv.org/abs/2601.11292", "authors": ["Yiqi Zhou", "JunHao Ma", "Xingyang Li", "Yule Sheng", "Yue Yuan", "Yikai Wang", "Bochang Wang", "Yiheng Wu", "Shan Shen", "Wei Xing", "Daying Sun", "Li Li", "Zhiqiang Xiao"], "title": "OpenACM: An Open-Source SRAM-Based Approximate CiM Compiler", "comment": "Accepted by DATE 2026", "summary": "The rise of data-intensive AI workloads has exacerbated the ``memory wall'' bottleneck. Digital Compute-in-Memory (DCiM) using SRAM offers a scalable solution, but its vast design space makes manual design impractical, creating a need for automated compilers. A key opportunity lies in approximate computing, which leverages the error tolerance of AI applications for significant energy savings. However, existing DCiM compilers focus on exact arithmetic, failing to exploit this optimization. This paper introduces OpenACM, the first open-source, accuracy-aware compiler for SRAM-based approximate DCiM architectures. OpenACM bridges the gap between application error tolerance and hardware automation. Its key contribution is an integrated library of accuracy-configurable multipliers (exact, tunable approximate, and logarithmic), enabling designers to make fine-grained accuracy-energy trade-offs. The compiler automates the generation of the DCiM architecture, integrating a transistor-level customizable SRAM macro with variation-aware characterization into a complete, open-source physical design flow based on OpenROAD and the FreePDK45 library. This ensures full reproducibility and accessibility, removing dependencies on proprietary tools. Experimental results on representative convolutional neural networks (CNNs) demonstrate that OpenACM achieves energy savings of up to 64\\% with negligible loss in application accuracy. The framework is available on \\href{https://github.com/ShenShan123/OpenACM}{OpenACM:URL}", "AI": {"tldr": "OpenACM\u662f\u9996\u4e2a\u5f00\u6e90\u3001\u7cbe\u5ea6\u611f\u77e5\u7684SRAM\u8fd1\u5b58\u8ba1\u7b97\u7f16\u8bd1\u5668\uff0c\u652f\u6301\u53ef\u914d\u7f6e\u7cbe\u5ea6\u4e58\u6cd5\u5668\uff0c\u5b9e\u73b0\u9ad8\u8fbe64%\u80fd\u8017\u8282\u7701\u4e14\u7cbe\u5ea6\u635f\u5931\u53ef\u5ffd\u7565\u3002", "motivation": "\u6570\u636e\u5bc6\u96c6\u578bAI\u8d1f\u8f7d\u52a0\u5267\u5185\u5b58\u5899\u74f6\u9888\uff0c\u73b0\u6709DCiM\u7f16\u8bd1\u5668\u672a\u5229\u7528\u8fd1\u4f3c\u8ba1\u7b97\u4f18\u5316\u6f5c\u529b\u3002", "method": "\u96c6\u6210\u53ef\u8c03\u7cbe\u5ea6\u4e58\u6cd5\u5668\u5e93\uff0c\u7ed3\u5408\u6676\u4f53\u7ba1\u7ea7SRAM\u5b8f\u4e0eOpenROAD\u7269\u7406\u8bbe\u8ba1\u6d41\u7a0b\uff0c\u81ea\u52a8\u5316\u751f\u6210\u8fd1\u4f3cDCiM\u67b6\u6784\u3002", "result": "\u5728CNN\u4e0a\u5b9e\u73b0\u6700\u9ad864%\u80fd\u8017\u964d\u4f4e\uff0c\u540c\u65f6\u4fdd\u6301\u5e94\u7528\u7cbe\u5ea6\u57fa\u672c\u65e0\u635f\u3002", "conclusion": "OpenACM\u586b\u8865\u4e86AI\u8bef\u5dee\u5bb9\u5fcd\u4e0e\u786c\u4ef6\u81ea\u52a8\u5316\u4e4b\u95f4\u7684\u7a7a\u767d\uff0c\u63d0\u4f9b\u5b8c\u5168\u5f00\u6e90\u53ef\u590d\u73b0\u7684\u8fd1\u4f3c\u8ba1\u7b97\u7f16\u8bd1\u65b9\u6848\u3002"}}
{"id": "2601.10874", "categories": ["cs.PF", "eess.SY"], "pdf": "https://arxiv.org/pdf/2601.10874", "abs": "https://arxiv.org/abs/2601.10874", "authors": ["Amer Diwan", "Prabhakar Raghavan", "Eli Upfal"], "title": "Balanced allocation: considerations from large scale service environments", "comment": null, "summary": "We study d-way balanced allocation, which assigns each incoming job to the lightest loaded among d randomly chosen servers. While prior work has extensively studied the performance of the basic scheme, there has been less published work on adapting this technique to many aspects of large-scale systems. Based on our experience in building and running planet-scale cloud applications, we extend the understanding of d-way balanced allocation along the following dimensions:\n  (i) Bursts: Events such as breaking news can produce bursts of requests that may temporarily exceed the servicing capacity of the system. Thus, we explore what happens during a burst and how long it takes for the system to recover from such bursts. (ii) Priorities: Production systems need to handle jobs with a mix of priorities (e.g., user facing requests may be high priority while other requests may be low priority). We extend d-way balanced allocation to handle multiple priorities. (iii) Noise: Production systems are often typically distributed and thus d-way balanced allocation must work with stale or incorrect information. Thus we explore the impact of noisy information and their interactions with bursts and priorities.\n  We explore the above using both extensive simulations and analytical arguments. Specifically we show, (i) using simulations, that d-way balanced allocation quickly recovers from bursts and can gracefully handle priorities and noise; and (ii) that analysis of the underlying generative models complements our simulations and provides insight into our simulation results.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86d\u8def\u5e73\u8861\u5206\u914d\u5728\u7a81\u53d1\u6d41\u91cf\u3001\u591a\u4f18\u5148\u7ea7\u4efb\u52a1\u548c\u4fe1\u606f\u566a\u58f0\u7b49\u5b9e\u9645\u751f\u4ea7\u73af\u5883\u4e2d\u7684\u8868\u73b0\uff0c\u5e76\u901a\u8fc7\u4eff\u771f\u4e0e\u5206\u6790\u9a8c\u8bc1\u5176\u6709\u6548\u6027\u3002", "motivation": "\u4e3a\u89e3\u51b3\u5927\u89c4\u6a21\u4e91\u7cfb\u7edf\u4e2d\u8d1f\u8f7d\u5747\u8861\u5728\u9762\u5bf9\u7a81\u53d1\u8bf7\u6c42\u3001\u4efb\u52a1\u4f18\u5148\u7ea7\u5dee\u5f02\u548c\u4fe1\u606f\u5ef6\u8fdf\u65f6\u7684\u9002\u5e94\u6027\u95ee\u9898\uff0c\u6269\u5c55d\u8def\u5e73\u8861\u5206\u914d\u7684\u5e94\u7528\u573a\u666f\u3002", "method": "\u7ed3\u5408\u5927\u89c4\u6a21\u4eff\u771f\u5b9e\u9a8c\u4e0e\u751f\u6210\u6a21\u578b\u7684\u6570\u5b66\u5206\u6790\uff0c\u8bc4\u4f30d\u8def\u5e73\u8861\u5206\u914d\u5728\u7a81\u53d1\u3001\u4f18\u5148\u7ea7\u548c\u566a\u58f0\u6761\u4ef6\u4e0b\u7684\u6027\u80fd\u8868\u73b0\u3002", "result": "\u4eff\u771f\u8868\u660e\u8be5\u65b9\u6cd5\u80fd\u5feb\u901f\u4ece\u7a81\u53d1\u4e2d\u6062\u590d\uff0c\u4f18\u96c5\u5904\u7406\u4f18\u5148\u7ea7\u548c\u566a\u58f0\uff1b\u7406\u8bba\u5206\u6790\u8fdb\u4e00\u6b65\u89e3\u91ca\u4e86\u4eff\u771f\u7ed3\u679c\u80cc\u540e\u7684\u673a\u5236\u3002", "conclusion": "d\u8def\u5e73\u8861\u5206\u914d\u5728\u590d\u6742\u751f\u4ea7\u73af\u5883\u4e2d\u4ecd\u5177\u9c81\u68d2\u6027\u548c\u5b9e\u7528\u6027\uff0c\u7406\u8bba\u4e0e\u4eff\u771f\u76f8\u8f85\u76f8\u6210\uff0c\u53ef\u6307\u5bfc\u5b9e\u9645\u7cfb\u7edf\u8bbe\u8ba1\u3002"}}
{"id": "2601.10998", "categories": ["cs.DC", "cs.MM", "cs.NI", "cs.PF"], "pdf": "https://arxiv.org/pdf/2601.10998", "abs": "https://arxiv.org/abs/2601.10998", "authors": ["Shinsuk Kang", "Youngjae Kim"], "title": "AFLL: Real-time Load Stabilization for MMO Game Servers Based on Circular Causality Learning", "comment": "16 pages, 7 figures, 5 tables. Submitted for publication", "summary": "Massively Multiplayer Online (MMO) game servers must handle thousands of simultaneous players while maintaining sub-100ms response times. When server load exceeds capacity, traditional approaches either uniformly throttle all message types regardless of importance (damaging gameplay) or apply fixed heuristic rules that fail to adapt to dynamic workloads. This paper presents AFLL (Adaptive Feedback Loop Learning), a real-time load stabilization system that learns the causal relationship between outgoing server messages and subsequent incoming client requests. AFLL employs backpropagation to continuously adjust message type weights, enabling predictive throttling that blocks low-priority messages before overload occurs while guaranteeing critical message delivery. Through controlled experiments with 1,000 concurrent players, AFLL reduced average CPU time by 48.3% (13.2ms to 6.8ms), peak CPU time by 51.7% (54.0ms to 26.1ms), and thread contention by 64.4% (19.6% to 7.0%), while maintaining zero learning overhead through background computation and caching optimizations. The system achieved remarkable reproducibility (CV < 2% across all metrics) and identified a three-stage causal chain linking message blocking to load reduction. AFLL demonstrates that circular causality learning enables practical real-time adaptation for latency-critical systems.", "AI": {"tldr": "AFLL\u901a\u8fc7\u5b66\u4e60\u670d\u52a1\u5668\u6d88\u606f\u4e0e\u5ba2\u6237\u7aef\u8bf7\u6c42\u7684\u56e0\u679c\u5173\u7cfb\uff0c\u5b9e\u73b0\u52a8\u6001\u9884\u6d4b\u6027\u9650\u6d41\uff0c\u5728\u4fdd\u8bc1\u5173\u952e\u6d88\u606f\u7684\u540c\u65f6\u964d\u4f4e\u670d\u52a1\u5668\u8d1f\u8f7d\u3002", "motivation": "\u4f20\u7edf\u65b9\u6cd5\u5728\u670d\u52a1\u5668\u8fc7\u8f7d\u65f6\u65e0\u6cd5\u667a\u80fd\u533a\u5206\u6d88\u606f\u4f18\u5148\u7ea7\uff0c\u5bfc\u81f4\u6e38\u620f\u4f53\u9a8c\u53d7\u635f\u6216\u89c4\u5219\u50f5\u5316\u3002", "method": "\u91c7\u7528\u53cd\u5411\u4f20\u64ad\u5b9e\u65f6\u8c03\u6574\u6d88\u606f\u7c7b\u578b\u6743\u91cd\uff0c\u7ed3\u5408\u540e\u53f0\u8ba1\u7b97\u4e0e\u7f13\u5b58\u4f18\u5316\u5b9e\u73b0\u96f6\u5b66\u4e60\u5f00\u9500\u7684\u81ea\u9002\u5e94\u9650\u6d41\u3002", "result": "\u5728\u5343\u4eba\u5e76\u53d1\u5b9e\u9a8c\u4e2d\uff0c\u5e73\u5747CPU\u65f6\u95f4\u964d\u4f4e48.3%\uff0c\u5cf0\u503c\u964d\u4f4e51.7%\uff0c\u7ebf\u7a0b\u4e89\u7528\u51cf\u5c1164.4%\uff0c\u6240\u6709\u6307\u6807\u53d8\u5f02\u7cfb\u6570<2%\u3002", "conclusion": "\u5faa\u73af\u56e0\u679c\u5b66\u4e60\u53ef\u4e3a\u4f4e\u5ef6\u8fdf\u7cfb\u7edf\u63d0\u4f9b\u5b9e\u7528\u7684\u5b9e\u65f6\u81ea\u9002\u5e94\u80fd\u529b\u3002"}}
{"id": "2601.11385", "categories": ["cs.NI"], "pdf": "https://arxiv.org/pdf/2601.11385", "abs": "https://arxiv.org/abs/2601.11385", "authors": ["Giovanni Apruzzese", "Aurore Fass"], "title": "X-raying the arXiv: A Large-Scale Analysis of arXiv Submissions' Source Files", "comment": null, "summary": "arXiv is the largest open-access repository for scientific literature. When submitting a paper, authors upload the manuscript's source files, from which the final PDF is compiled. These source files are also publicly downloadable, potentially exposing data unrelated to the published paper -- such as figures, documents, or comments -- that may unintentionally reveal confidential information or simply waste storage space. We thus ask ourselves: \"What can be found within the source files of arXiv submissions?\"\n  We present a longitudinal analysis of ~600,000 submissions appeared on arXiv between 2015--2025. For each submission, we examine the uploaded source files to quantify and characterize data not required for producing the respective PDF. On average, 27% of the data in each submission are unnecessary, totaling >580 GB of redundant content across our dataset. Qualitative inspection reveals the presence of offensive/inappropriate text (e.g., \"WTF does this mean?\") and experimental details that could disclose ongoing research. We have contacted arXiv's leadership team, as well as the authors of affected papers to alert them of these issues. Finally, we propose recommendations and an automated tool to detect and analyze arXiv submissions residual data at scale, aiming to improve data hygiene in the arXiv's ecosystem.", "AI": {"tldr": "\u672c\u6587\u5206\u6790\u4e86arXiv\u4e0a60\u4e07\u7bc7\u8bba\u6587\u6e90\u6587\u4ef6\uff0c\u53d1\u73b0\u5e73\u574727%\u5185\u5bb9\u4e3a\u975e\u5fc5\u8981\u6570\u636e\uff0c\u603b\u8ba1\u8d85580GB\u5197\u4f59\uff0c\u5e76\u63d0\u51fa\u81ea\u52a8\u5316\u5de5\u5177\u63d0\u5347\u6570\u636e\u6574\u6d01\u6027\u3002", "motivation": "\u63ed\u793aarXiv\u8bba\u6587\u6e90\u6587\u4ef6\u4e2d\u53ef\u80fd\u5305\u542b\u7684\u65e0\u5173\u6216\u654f\u611f\u4fe1\u606f\uff0c\u4ee5\u6539\u5584\u5e73\u53f0\u6570\u636e\u7ba1\u7406\u3002", "method": "\u5bf92015-2025\u5e74\u95f4\u7ea660\u4e07\u4efd\u63d0\u4ea4\u8fdb\u884c\u7eb5\u5411\u5206\u6790\uff0c\u68c0\u67e5\u6e90\u6587\u4ef6\u5e76\u91cf\u5316\u975e\u5fc5\u8981\u6570\u636e\uff0c\u8f85\u4ee5\u5b9a\u6027\u5ba1\u67e5\u3002", "result": "\u53d1\u73b0\u5927\u91cf\u5197\u4f59\u6570\u636e\u53ca\u6f5c\u5728\u4e0d\u5f53\u5185\u5bb9\uff0c\u5df2\u901a\u77e5\u76f8\u5173\u4f5c\u8005\u4e0earXiv\u56e2\u961f\uff0c\u5e76\u5f00\u53d1\u81ea\u52a8\u5316\u68c0\u6d4b\u5de5\u5177\u3002", "conclusion": "\u9700\u52a0\u5f3aarXiv\u751f\u6001\u7cfb\u7edf\u7684\u6570\u636e\u536b\u751f\uff0c\u9632\u6b62\u65e0\u610f\u6cc4\u9732\u6216\u8d44\u6e90\u6d6a\u8d39\u3002"}}
{"id": "2601.10849", "categories": ["cs.MA"], "pdf": "https://arxiv.org/pdf/2601.10849", "abs": "https://arxiv.org/abs/2601.10849", "authors": ["Cuong Le", "Symeon Chatzinotas", "Thang X. Vu"], "title": "Cooperative UAVs for Remote Data Collection under Limited Communications: An Asynchronous Multiagent Learning Framework", "comment": "Accepted to IEEE Transactions on Wireless Communications", "summary": "This paper addresses the joint optimization of trajectories and bandwidth allocation for multiple Unmanned Aerial Vehicles (UAVs) to enhance energy efficiency in the cooperative data collection problem. We focus on an important yet underestimated aspect of the system, where action synchronization across all UAVs is impossible. Since most existing learning-based solutions are not designed to learn in this asynchronous environment, we formulate the trajectory planning problem as a Decentralized Partially Observable Semi-Markov Decision Process and introduce an asynchronous multi-agent learning algorithm to learn UAVs' cooperative policies. Once the UAVs' trajectory policies are learned, the bandwidth allocation can be optimally solved based on local observations at each collection point. Comprehensive empirical results demonstrate the superiority of the proposed method over other learning-based and heuristic baselines in terms of both energy efficiency and mission completion time. Additionally, the learned policies exhibit robustness under varying environmental conditions.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u5f02\u6b65\u591a\u667a\u80fd\u4f53\u5b66\u4e60\u7b97\u6cd5\uff0c\u7528\u4e8e\u4f18\u5316\u65e0\u4eba\u673a\u8f68\u8ff9\u4e0e\u5e26\u5bbd\u5206\u914d\uff0c\u63d0\u5347\u6570\u636e\u91c7\u96c6\u80fd\u6548\u4e0e\u4efb\u52a1\u5b8c\u6210\u901f\u5ea6\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u5e94\u5bf9\u65e0\u4eba\u673a\u95f4\u65e0\u6cd5\u540c\u6b65\u884c\u52a8\u7684\u73b0\u5b9e\u573a\u666f\uff0c\u4e9f\u9700\u9002\u5e94\u5f02\u6b65\u73af\u5883\u7684\u5b66\u4e60\u65b9\u6848\u3002", "method": "\u5c06\u8f68\u8ff9\u89c4\u5212\u5efa\u6a21\u4e3a\u5206\u6563\u90e8\u5206\u53ef\u89c2\u6d4b\u534a\u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0b\uff0c\u7ed3\u5408\u672c\u5730\u89c2\u6d4b\u4f18\u5316\u5e26\u5bbd\u5206\u914d\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\u8be5\u65b9\u6cd5\u5728\u80fd\u6548\u4e0e\u4efb\u52a1\u65f6\u95f4\u4e0a\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\uff0c\u4e14\u7b56\u7565\u5bf9\u73af\u5883\u53d8\u5316\u5177\u6709\u9c81\u68d2\u6027\u3002", "conclusion": "\u6240\u63d0\u5f02\u6b65\u5b66\u4e60\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u591a\u65e0\u4eba\u673a\u534f\u540c\u6570\u636e\u91c7\u96c6\u4e2d\u7684\u8f68\u8ff9\u4e0e\u8d44\u6e90\u8054\u5408\u4f18\u5316\u95ee\u9898\u3002"}}
{"id": "2601.11156", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2601.11156", "abs": "https://arxiv.org/abs/2601.11156", "authors": ["Niklas Kowallik", "Trever Schirmer", "David Bermbach"], "title": "Konflux: Optimized Function Fusion for Serverless Applications", "comment": null, "summary": "Function-as-a-Service (FaaS) has become a central paradigm in serverless cloud computing, yet optimizing FaaS deployments remains challenging. Using function fusion, multiple functions can be combined into a single deployment unit, which can be used to reduce cost and latency of complex serverless applications comprising multiple functions. Even in small-scale applications, the number of possible fusion configurations is vast, making brute-force benchmarking in production both cost- and time-prohibitive.\n  In this paper, we present a system that can analyze every possible fusion setup of complex applications. By emulating the FaaS platform, our system enables local experimentation, eliminating the need to reconfigure the live platform and significantly reducing associated cost and time. We evaluate all fusion configurations across a number of example FaaS applications and resource limits. Our results reveal that, when analyzing cost and latency trade-offs, only a limited set of fusion configurations represent optimal solutions, which are strongly influenced by the specific pricing model in use.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u901a\u8fc7\u6a21\u62dfFaaS\u5e73\u53f0\u672c\u5730\u5b9e\u9a8c\u5206\u6790\u6240\u6709\u51fd\u6570\u878d\u5408\u914d\u7f6e\u7684\u7cfb\u7edf\uff0c\u4ee5\u4f18\u5316\u6210\u672c\u4e0e\u5ef6\u8fdf\u6743\u8861\u3002", "motivation": "\u89e3\u51b3FaaS\u90e8\u7f72\u4e2d\u56e0\u51fd\u6570\u7ec4\u5408\u7206\u70b8\u5bfc\u81f4\u751f\u4ea7\u73af\u5883\u7a77\u4e3e\u6d4b\u8bd5\u6210\u672c\u9ad8\u3001\u8017\u65f6\u957f\u7684\u95ee\u9898\u3002", "method": "\u6784\u5efa\u5e73\u53f0\u4eff\u771f\u7cfb\u7edf\uff0c\u5728\u672c\u5730\u8bc4\u4f30\u5404\u7c7b\u8d44\u6e90\u9650\u5236\u4e0b\u7684\u6240\u6709\u878d\u5408\u914d\u7f6e\uff0c\u907f\u514d\u5b9e\u9645\u5e73\u53f0\u91cd\u914d\u7f6e\u3002", "result": "\u53d1\u73b0\u4ec5\u5c11\u6570\u878d\u5408\u914d\u7f6e\u5728\u6210\u672c\u4e0e\u5ef6\u8fdf\u4e0a\u6700\u4f18\uff0c\u4e14\u53d7\u8ba1\u4ef7\u6a21\u578b\u663e\u8457\u5f71\u54cd\u3002", "conclusion": "\u51fd\u6570\u878d\u5408\u4f18\u5316\u9700\u7ed3\u5408\u5177\u4f53\u8ba1\u4ef7\u7b56\u7565\uff0c\u672c\u5730\u4eff\u771f\u53ef\u9ad8\u6548\u7b5b\u9009\u6700\u4f18\u90e8\u7f72\u65b9\u6848\u3002"}}
{"id": "2601.11327", "categories": ["cs.MA"], "pdf": "https://arxiv.org/pdf/2601.11327", "abs": "https://arxiv.org/abs/2601.11327", "authors": ["Agata \u017bywot", "Xinyi Chen", "Maarten de Rijke"], "title": "Can Small Agent Collaboration Beat a Single Big LLM?", "comment": null, "summary": "This report studies whether small, tool-augmented agents can match or outperform larger monolithic models on the GAIA benchmark. Using Qwen3 models (4B-32B) within an adapted Agentic-Reasoning framework, we isolate the effects of model scale, explicit thinking (no thinking, planner-only, or full), and tool use (search, code, mind-map). Tool augmentation provides the largest and most consistent gains. Using tools, 4B models can outperform 32B models without tool access on GAIA in our experimental setup. In contrast, explicit thinking is highly configuration- and difficulty-dependent: planner-only thinking can improve decomposition and constraint tracking, while unrestricted full thinking often degrades performance by destabilizing tool orchestration, leading to skipped verification steps, excessive tool calls, non-termination, and output-format drift.", "AI": {"tldr": "\u5c0f\u89c4\u6a21\u5de5\u5177\u589e\u5f3a\u667a\u80fd\u4f53\u5728GAIA\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53ef\u8d85\u8d8a\u5927\u89c4\u6a21\u5355\u4f53\u6a21\u578b\uff0c\u5de5\u5177\u4f7f\u7528\u5e26\u6765\u6700\u663e\u8457\u63d0\u5347\uff0c\u800c\u663e\u5f0f\u63a8\u7406\u6548\u679c\u4f9d\u8d56\u914d\u7f6e\u4e0e\u4efb\u52a1\u96be\u5ea6\u3002", "motivation": "\u63a2\u7a76\u5c0f\u89c4\u6a21\u5de5\u5177\u589e\u5f3a\u667a\u80fd\u4f53\u662f\u5426\u80fd\u5728\u6027\u80fd\u4e0a\u5339\u654c\u6216\u8d85\u8d8a\u5927\u89c4\u6a21\u5355\u4f53\u6a21\u578b\u3002", "method": "\u5728Agentic-Reasoning\u6846\u67b6\u4e0b\u4f7f\u7528Qwen3\u7cfb\u5217\u6a21\u578b\uff084B-32B\uff09\uff0c\u63a7\u5236\u53d8\u91cf\u5206\u6790\u6a21\u578b\u89c4\u6a21\u3001\u663e\u5f0f\u63a8\u7406\u65b9\u5f0f\u548c\u5de5\u5177\u4f7f\u7528\u7684\u5f71\u54cd\u3002", "result": "\u5de5\u5177\u589e\u5f3a\u5e26\u6765\u6700\u5927\u4e14\u6700\u7a33\u5b9a\u7684\u6027\u80fd\u63d0\u5347\uff0c4B\u6a21\u578b\u914d\u5408\u5de5\u5177\u53ef\u8d85\u8d8a\u65e0\u5de5\u5177\u768432B\u6a21\u578b\uff1b\u663e\u5f0f\u63a8\u7406\u6548\u679c\u4e0d\u7a33\u5b9a\uff0c\u5b8c\u6574\u63a8\u7406\u5e38\u56e0\u7834\u574f\u5de5\u5177\u534f\u8c03\u800c\u964d\u4f4e\u6027\u80fd\u3002", "conclusion": "\u5de5\u5177\u4f7f\u7528\u662f\u63d0\u5347\u5c0f\u6a21\u578b\u6027\u80fd\u7684\u5173\u952e\u56e0\u7d20\uff0c\u663e\u5f0f\u63a8\u7406\u9700\u8c28\u614e\u8bbe\u8ba1\u4ee5\u907f\u514d\u8d1f\u9762\u6548\u5e94\u3002"}}
{"id": "2601.10850", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2601.10850", "abs": "https://arxiv.org/abs/2601.10850", "authors": ["Eric L. Melin", "Nasir U. Eisty", "Gregory Watson", "Addi Malviya-Thakur"], "title": "Multi-Artifact Analysis of Self-Admitted Technical Debt in Scientific Software", "comment": null, "summary": "Context: Self-admitted technical debt (SATD) occurs when developers acknowledge shortcuts in code. In scientific software (SSW), such debt poses unique risks to the validity and reproducibility of results. Objective: This study aims to identify, categorize, and evaluate scientific debt, a specialized form of SATD in SSW, and assess the extent to which traditional SATD categories capture these domain-specific issues. Method: We conduct a multi-artifact analysis across code comments, commit messages, pull requests, and issue trackers from 23 open-source SSW projects. We construct and validate a curated dataset of scientific debt, develop a multi-source SATD classifier, and conduct a practitioner validation to assess the practical relevance of scientific debt. Results: Our classifier performs strongly across 900,358 artifacts from 23 SSW projects. SATD is most prevalent in pull requests and issue trackers, underscoring the value of multi-artifact analysis. Models trained on traditional SATD often miss scientific debt, emphasizing the need for its explicit detection in SSW. Practitioner validation confirmed that scientific debt is both recognizable and useful in practice. Conclusions: Scientific debt represents a unique form of SATD in SSW that that is not adequately captured by traditional categories and requires specialized identification and management. Our dataset, classification analysis, and practitioner validation results provide the first formal multi-artifact perspective on scientific debt, highlighting the need for tailored SATD detection approaches in SSW.", "AI": {"tldr": "\u672c\u7814\u7a76\u8bc6\u522b\u5e76\u8bc4\u4f30\u79d1\u5b66\u8f6f\u4ef6\u4e2d\u7684\u79d1\u5b66\u503a\u52a1\uff0c\u53d1\u73b0\u5176\u65e0\u6cd5\u88ab\u4f20\u7edf\u6280\u672f\u503a\u52a1\u5206\u7c7b\u5145\u5206\u6355\u6349\uff0c\u9700\u4e13\u95e8\u68c0\u6d4b\u65b9\u6cd5\u3002", "motivation": "\u79d1\u5b66\u8f6f\u4ef6\u4e2d\u7684\u81ea\u8ba4\u6280\u672f\u503a\u52a1\u53ef\u80fd\u5f71\u54cd\u7ed3\u679c\u7684\u6709\u6548\u6027\u548c\u53ef\u91cd\u590d\u6027\uff0c\u9700\u9488\u5bf9\u6027\u5206\u6790\u3002", "method": "\u5bf923\u4e2a\u5f00\u6e90\u79d1\u5b66\u8f6f\u4ef6\u9879\u76ee\u8fdb\u884c\u591a\u5de5\u4ef6\u5206\u6790\uff0c\u6784\u5efa\u6570\u636e\u96c6\u3001\u5f00\u53d1\u5206\u7c7b\u5668\u5e76\u5f00\u5c55\u5b9e\u8df5\u8005\u9a8c\u8bc1\u3002", "result": "\u5206\u7c7b\u5668\u572890\u4e07\u5de5\u4ef6\u4e2d\u8868\u73b0\u826f\u597d\uff1b\u79d1\u5b66\u503a\u52a1\u5728\u62c9\u53d6\u8bf7\u6c42\u548c\u95ee\u9898\u8ffd\u8e2a\u5668\u4e2d\u6700\u5e38\u89c1\uff0c\u4f20\u7edf\u6a21\u578b\u5e38\u9057\u6f0f\u6b64\u7c7b\u503a\u52a1\u3002", "conclusion": "\u79d1\u5b66\u503a\u52a1\u662f\u79d1\u5b66\u8f6f\u4ef6\u4e2d\u72ec\u7279\u4e14\u9700\u4e13\u95e8\u7ba1\u7406\u7684\u6280\u672f\u503a\u52a1\u5f62\u5f0f\uff0c\u5e94\u91c7\u7528\u5b9a\u5236\u5316\u68c0\u6d4b\u65b9\u6cd5\u3002"}}
{"id": "2601.11487", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2601.11487", "abs": "https://arxiv.org/abs/2601.11487", "authors": ["Paulo S\u00e9rgio Almeida"], "title": "Space-Optimal, Computation-Optimal, Topology-Agnostic, Throughput-Scalable Causal Delivery through Hybrid Buffering", "comment": "16 pages, 5 figures", "summary": "Message delivery respecting causal ordering (causal delivery) is one of the most classic and widely useful abstraction for inter-process communication in a distributed system. Most approaches tag messages with causality information and buffer them at the receiver until they can be safely delivered. Except for specific approaches that exploit communication topology, therefore not generally applicable, they incur a metadata overhead which is prohibitive for a large number of processes. Much less used are the approaches that enforce causal order by buffering messages at the sender, until it is safe to release them to the network, as the classic algorithm has too many drawbacks. In this paper, first we discuss the limitations of sender-only buffering approaches and introduce the Sender Permission to Send (SPS) enforcement strategy, showing that SPS + FIFO implies Causal. We analyze a recent sender-buffering algorithm, Cykas, which follows SPS + FIFO, albeit very conservatively, pointing out throughput scalability and liveness issues. Then, we introduce a novel SPS + FIFO based algorithm, which adopts a new hybrid approach: enforcing causality by combining sender-buffering to enforce SPS and receiver-buffering to enforce FIFO. The algorithm overcomes limitations of sender-only buffering, and achieves effectively constant metadata size per message. By a careful choice of data-structures, the algorithm is also computationally-optimal, with amortized effectively constant processing overhead. As far as we know, there is no other topology-agnostic causal delivery algorithm with these properties.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u65b0\u7684\u6df7\u5408\u7b97\u6cd5\uff0c\u7ed3\u5408\u53d1\u9001\u65b9\u548c\u63a5\u6536\u65b9\u7f13\u51b2\u7b56\u7565\uff0c\u4ee5\u5b9e\u73b0\u56e0\u679c\u987a\u5e8f\u6d88\u606f\u4f20\u9012\uff0c\u540c\u65f6\u4fdd\u6301\u5e38\u6570\u7ea7\u5143\u6570\u636e\u5f00\u9500\u548c\u8ba1\u7b97\u6700\u4f18\u6027\u3002", "motivation": "\u73b0\u6709\u56e0\u679c\u4f20\u9012\u65b9\u6cd5\u8981\u4e48\u5143\u6570\u636e\u5f00\u9500\u5927\uff0c\u8981\u4e48\u4ec5\u9002\u7528\u4e8e\u7279\u5b9a\u62d3\u6251\uff1b\u7eaf\u53d1\u9001\u65b9\u7f13\u51b2\u65b9\u6848\u5b58\u5728\u541e\u5410\u91cf\u548c\u6d3b\u6027\u95ee\u9898\uff0c\u9700\u6539\u8fdb\u3002", "method": "\u5f15\u5165SPS + FIFO\u7b56\u7565\uff0c\u8bbe\u8ba1\u6df7\u5408\u7f13\u51b2\u7b97\u6cd5\uff1a\u53d1\u9001\u65b9\u7f13\u51b2\u5b9e\u73b0SPS\uff0c\u63a5\u6536\u65b9\u7f13\u51b2\u4fdd\u8bc1FIFO\uff0c\u4f18\u5316\u6570\u636e\u7ed3\u6784\u5b9e\u73b0\u5e38\u6570\u5747\u644a\u5f00\u9500\u3002", "result": "\u65b0\u7b97\u6cd5\u5728\u4e0d\u4f9d\u8d56\u901a\u4fe1\u62d3\u6251\u7684\u524d\u63d0\u4e0b\uff0c\u5b9e\u73b0\u4e86\u5e38\u6570\u7ea7\u5143\u6570\u636e\u5927\u5c0f\u548c\u8ba1\u7b97\u6700\u4f18\u6027\uff0c\u514b\u670d\u4e86\u7eaf\u53d1\u9001\u65b9\u7f13\u51b2\u7684\u5c40\u9650\u3002", "conclusion": "\u8be5\u7b97\u6cd5\u662f\u9996\u4e2a\u517c\u5177\u5e38\u6570\u5143\u6570\u636e\u3001\u8ba1\u7b97\u6700\u4f18\u6027\u548c\u62d3\u6251\u65e0\u5173\u6027\u7684\u56e0\u679c\u4f20\u9012\u65b9\u6848\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002"}}
{"id": "2601.10942", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2601.10942", "abs": "https://arxiv.org/abs/2601.10942", "authors": ["Zitong Zhou", "Matteo Paltenghi", "Miryung Kim", "Michael Pradel"], "title": "Change And Cover: Last-Mile, Pull Request-Based Regression Test Augmentation", "comment": null, "summary": "Software is in constant evolution, with developers frequently submitting pull requests (PRs) to introduce new features or fix bugs. Testing PRs is critical to maintaining software quality. Yet, even in projects with extensive test suites, some PR-modified lines remain untested, leaving a \"last-mile\" regression test gap. Existing test generators typically aim to improve overall coverage, but do not specifically target the uncovered lines in PRs. We present Change And Cover (ChaCo), an LLM-based test augmentation technique that addresses this gap. It makes three contributions: (i) ChaCo considers the PR-specific patch coverage, offering developers augmented tests for code just when it is on the developers' mind. (ii) We identify providing suitable test context as a crucial challenge for an LLM to generate useful tests, and present two techniques to extract relevant test content, such as existing test functions, fixtures, and data generators. (iii) To make augmented tests acceptable for developers, ChaCo carefully integrates them into the existing test suite, e.g., by matching the test's structure and style with the existing tests, and generates a summary of the test addition for developer review. We evaluate ChaCo on 145 PRs from three popular and complex open-source projects - SciPy, Qiskit, and Pandas. The approach successfully helps 30% of PRs achieve full patch coverage, at the cost of $0.11, showing its effectiveness and practicality. Human reviewers find the tests to be worth adding (4.53/5.0), well integrated (4.2/5.0), and relevant to the PR (4.7/5.0). Ablations show test context is crucial for context-aware test generation, leading to 2x coverage. We submitted 12 tests, of which 8 have already been merged, and two previously unknown bugs were exposed and fixed. We envision our approach to be integrated into CI workflows, automating the last mile of regression test augmentation.", "AI": {"tldr": "ChaCo \u662f\u4e00\u79cd\u57fa\u4e8e LLM \u7684\u6d4b\u8bd5\u589e\u5f3a\u6280\u672f\uff0c\u4e13\u95e8\u9488\u5bf9 PR \u4e2d\u672a\u8986\u76d6\u4ee3\u7801\u884c\u751f\u6210\u6d4b\u8bd5\uff0c\u63d0\u5347\u8865\u4e01\u8986\u76d6\u7387\u5e76\u878d\u5165\u73b0\u6709\u6d4b\u8bd5\u6846\u67b6\u3002", "motivation": "PR \u4fee\u6539\u7684\u4ee3\u7801\u884c\u5e38\u7f3a\u4e4f\u6d4b\u8bd5\u8986\u76d6\uff0c\u73b0\u6709\u5de5\u5177\u672a\u9488\u5bf9\u6027\u89e3\u51b3\u6b64\u2018\u6700\u540e\u4e00\u516c\u91cc\u2019\u56de\u5f52\u6d4b\u8bd5\u7f3a\u53e3\u3002", "method": "ChaCo \u7ed3\u5408 PR \u8865\u4e01\u4e0a\u4e0b\u6587\uff0c\u63d0\u53d6\u76f8\u5173\u6d4b\u8bd5\u5185\u5bb9\u8f85\u52a9 LLM \u751f\u6210\u6d4b\u8bd5\uff0c\u5e76\u6309\u73b0\u6709\u98ce\u683c\u96c6\u6210\u6d4b\u8bd5\u3001\u63d0\u4f9b\u6458\u8981\u4f9b\u5ba1\u67e5\u3002", "result": "\u5728 SciPy\u3001Qiskit\u3001Pandas \u7684 145 \u4e2a PR \u4e0a\u8bc4\u4f30\uff0c30% \u5b9e\u73b0\u5b8c\u6574\u8865\u4e01\u8986\u76d6\uff0c\u6210\u672c $0.11\uff1b\u4eba\u5de5\u8bc4\u5206\u9ad8\uff0c8/12 \u6d4b\u8bd5\u88ab\u5408\u5e76\uff0c\u53d1\u73b0\u5e76\u4fee\u590d 2 \u4e2a\u672a\u77e5 bug\u3002", "conclusion": "ChaCo \u9ad8\u6548\u5b9e\u7528\uff0c\u9002\u5408\u96c6\u6210\u5230 CI \u5de5\u4f5c\u6d41\u4e2d\uff0c\u81ea\u52a8\u5316\u5b8c\u6210\u56de\u5f52\u6d4b\u8bd5\u7684\u6700\u540e\u4e00\u73af\u3002"}}
{"id": "2601.11138", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2601.11138", "abs": "https://arxiv.org/abs/2601.11138", "authors": ["Matteo Vaccargiu", "Riccardo Lai", "Maria Ilaria Lunesu", "Andrea Pinna", "Giuseppe Destefanis"], "title": "Patterns of Bot Participation and Emotional Influence in Open-Source Development", "comment": "The 7th International Workshop on Bots and Agents in Software Engineering (BoatSE 2026)", "summary": "We study how bots contribute to open-source discussions in the Ethereum ecosystem and whether they influence developers' emotional tone. Our dataset covers 36,875 accounts across ten repositories with 105 validated bots (0.28%). Human participation follows a U-shaped pattern, while bots engage in uniform (pull requests) or late-stage (issues) activity. Bots respond faster than humans in pull requests but play slower maintenance roles in issues. Using a model trained on 27 emotion categories, we find bots are more neutral, yet their interventions are followed by reduced neutrality in human comments, with shifts toward gratitude, admiration, and optimism and away from confusion. These findings indicate that even a small number of bots are associated with changes in both timing and emotional dynamics of developer communication.", "AI": {"tldr": "\u7814\u7a76\u4ee5\u592a\u574a\u751f\u6001\u7cfb\u7edf\u4e2d\u673a\u5668\u4eba\u5bf9\u5f00\u6e90\u8ba8\u8bba\u7684\u5f71\u54cd\uff0c\u53d1\u73b0\u5c11\u91cf\u673a\u5668\u4eba\u80fd\u6539\u53d8\u5f00\u53d1\u8005\u6c9f\u901a\u7684\u65f6\u95f4\u548c\u60c5\u611f\u52a8\u6001\u3002", "motivation": "\u63a2\u8ba8\u673a\u5668\u4eba\u5728\u5f00\u6e90\u793e\u533a\u4e2d\u7684\u4f5c\u7528\u53ca\u5176\u5bf9\u5f00\u53d1\u8005\u60c5\u611f\u8bed\u6c14\u7684\u5f71\u54cd\u3002", "method": "\u5206\u679036,875\u4e2a\u8d26\u6237\u7684\u6570\u636e\uff0c\u8bc6\u522b105\u4e2a\u673a\u5668\u4eba\uff0c\u5e76\u4f7f\u752827\u79cd\u60c5\u7eea\u7c7b\u522b\u7684\u6a21\u578b\u8bc4\u4f30\u60c5\u611f\u53d8\u5316\u3002", "result": "\u673a\u5668\u4eba\u66f4\u4e2d\u7acb\uff0c\u4f46\u5176\u4ecb\u5165\u540e\u4eba\u7c7b\u8bc4\u8bba\u7684\u60c5\u611f\u4e2d\u7acb\u6027\u964d\u4f4e\uff0c\u8f6c\u5411\u611f\u6fc0\u3001\u94a6\u4f69\u548c\u4e50\u89c2\u3002", "conclusion": "\u5373\u4f7f\u5c11\u91cf\u673a\u5668\u4eba\u4e5f\u80fd\u663e\u8457\u5f71\u54cd\u5f00\u53d1\u8005\u6c9f\u901a\u7684\u65f6\u95f4\u5b89\u6392\u548c\u60c5\u611f\u52a8\u6001\u3002"}}
{"id": "2601.11299", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2601.11299", "abs": "https://arxiv.org/abs/2601.11299", "authors": ["Hassan Onsori Delicheh", "Guillaume Cardoen", "Alexandre Decan", "Tom Mens"], "title": "Automation and Reuse Practices in GitHub Actions Workflows: A Practitioner's Perspective", "comment": "38 pages. Preprint submitted to ACM Transactions on Software Engineering Methodology. Under review. Exact publication details will be provided upon acceptance", "summary": "GitHub natively supports workflow automation through GitHub Actions. Yet, workflow maintenance is often considered a burden for software developers, who frequently face difficulties in writing, testing, debugging, and maintaining workflows. Little knowledge exists concerning the automation and reuse practices favoured by workflow practitioners. We therefore surveyed 419 practitioners to elucidate good and bad workflow development practices and to identify opportunities for supporting workflow maintenance. Specifically, we investigate the tasks that practitioners tend to automate using GitHub Actions, their preferred workflow creation mechanisms, and the non-functional characteristics they prioritise. We also examine the practices and challenges associated with GitHub's workflow reuse mechanisms. We observe a tendency to focus automation efforts on core CI/CD tasks, with less emphasis on crucial areas like security analysis and performance monitoring. Practitioners strongly rely on reusable Actions, but reusable workflows see less frequent adoption. Furthermore, we observed challenges with Action versioning and maintenance. Copy-pasting remains a common practice to have more control and avoid the complexity of depending on reusable components. These insights suggest the need for improved tooling, enhanced support for a wide range of automation tasks, and better mechanisms for discovering, managing, and trusting reusable workflow components.", "AI": {"tldr": "\u8be5\u7814\u7a76\u8c03\u67e5\u4e86419\u540d\u5f00\u53d1\u8005\u5728GitHub Actions\u5de5\u4f5c\u6d41\u5f00\u53d1\u4e2d\u7684\u5b9e\u8df5\u4e0e\u6311\u6218\uff0c\u53d1\u73b0\u81ea\u52a8\u5316\u96c6\u4e2d\u4e8eCI/CD\uff0c\u4f46\u5ffd\u89c6\u5b89\u5168\u4e0e\u6027\u80fd\u76d1\u63a7\uff1b\u53ef\u91cd\u7528\u7ec4\u4ef6\u4f7f\u7528\u4e0d\u8db3\u4e14\u5b58\u5728\u7248\u672c\u7ba1\u7406\u95ee\u9898\uff0c\u4e9f\u9700\u6539\u8fdb\u5de5\u5177\u652f\u6301\u3002", "motivation": "\u5e2e\u52a9\u5f00\u53d1\u8005\u51cf\u8f7b\u5de5\u4f5c\u6d41\u7ef4\u62a4\u8d1f\u62c5\uff0c\u8bc6\u522b\u5f53\u524dGitHub Actions\u5b9e\u8df5\u4e2d\u5b58\u5728\u7684\u4f18\u52a3\u505a\u6cd5\u53ca\u6539\u8fdb\u673a\u4f1a\u3002", "method": "\u901a\u8fc7\u95ee\u5377\u8c03\u67e5419\u540d\u5b9e\u9645\u4f7f\u7528GitHub Actions\u7684\u5f00\u53d1\u8005\uff0c\u5206\u6790\u5176\u81ea\u52a8\u5316\u4efb\u52a1\u504f\u597d\u3001\u521b\u5efa\u65b9\u5f0f\u3001\u975e\u529f\u80fd\u9700\u6c42\u53ca\u590d\u7528\u673a\u5236\u7684\u4f7f\u7528\u60c5\u51b5\u4e0e\u6311\u6218\u3002", "result": "\u53d1\u73b0\u5f00\u53d1\u8005\u4e3b\u8981\u81ea\u52a8\u5316CI/CD\u4efb\u52a1\uff0c\u8f83\u5c11\u5173\u6ce8\u5b89\u5168\u4e0e\u6027\u80fd\uff1b\u504f\u597d\u4f7f\u7528\u53ef\u91cd\u7528Actions\u4f46\u8f83\u5c11\u91c7\u7528\u53ef\u91cd\u7528\u5de5\u4f5c\u6d41\uff1b\u5b58\u5728\u7248\u672c\u63a7\u5236\u56f0\u96be\uff0c\u5e38\u4f9d\u8d56\u590d\u5236\u7c98\u8d34\u4ee5\u89c4\u907f\u590d\u6742\u6027\u3002", "conclusion": "\u9700\u63d0\u5347\u5de5\u5177\u652f\u6301\uff0c\u6269\u5c55\u81ea\u52a8\u5316\u4efb\u52a1\u8303\u56f4\uff0c\u5e76\u4f18\u5316\u53ef\u91cd\u7528\u7ec4\u4ef6\u7684\u53d1\u73b0\u3001\u7ba1\u7406\u4e0e\u4fe1\u4efb\u673a\u5236\uff0c\u4ee5\u6539\u5584\u5de5\u4f5c\u6d41\u5f00\u53d1\u4e0e\u7ef4\u62a4\u4f53\u9a8c\u3002"}}
{"id": "2601.11362", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2601.11362", "abs": "https://arxiv.org/abs/2601.11362", "authors": ["Manjeshwar Aniruddh Mallya", "Alessio Ferrari", "Mohammad Amin Zadenoori", "Jacek D\u0105browski"], "title": "RITA: A Tool for Automated Requirements Classification and Specification from Online User Feedback", "comment": null, "summary": "Context and motivation. Online user feedback is a valuable resource for requirements engineering, but its volume and noise make analysis difficult. Existing tools support individual feedback analysis tasks, but their capabilities are rarely integrated into end-to-end support. Problem. The lack of end-to-end integration limits the practical adoption of existing RE tools and makes it difficult to assess their real-world usefulness. Solution. To address this challenge, we present RITA, a tool that integrates lightweight open-source large language models into a unified workflow for feedback-driven RE. RITA supports automated request classification, non-functional requirement identification, and natural-language requirements specification generation from online feedback via a user-friendly interface, and integrates with Jira for seamless transfer of requirements specifications to development tools. Results and conclusions. RITA exploits previously evaluated LLM-based RE techniques to efficiently transform raw user feedback into requirements artefacts, helping bridge the gap between research and practice. A demonstration is available at: https://youtu.be/8meCLpwQWV8.", "AI": {"tldr": "RITA\u662f\u4e00\u4e2a\u96c6\u6210\u8f7b\u91cf\u7ea7\u5f00\u6e90\u5927\u8bed\u8a00\u6a21\u578b\u7684\u5de5\u5177\uff0c\u7528\u4e8e\u4ece\u5728\u7ebf\u7528\u6237\u53cd\u9988\u4e2d\u81ea\u52a8\u5316\u751f\u6210\u9700\u6c42\u89c4\u683c\uff0c\u5e76\u652f\u6301\u4e0eJira\u96c6\u6210\u3002", "motivation": "\u5728\u7ebf\u7528\u6237\u53cd\u9988\u867d\u6709\u4ef7\u503c\uff0c\u4f46\u56e0\u6570\u636e\u91cf\u5927\u4e14\u566a\u58f0\u591a\uff0c\u96be\u4ee5\u6709\u6548\u5206\u6790\uff1b\u73b0\u6709\u5de5\u5177\u7f3a\u4e4f\u7aef\u5230\u7aef\u6574\u5408\uff0c\u9650\u5236\u4e86\u5b9e\u9645\u5e94\u7528\u3002", "method": "RITA\u901a\u8fc7\u7edf\u4e00\u5de5\u4f5c\u6d41\u6574\u5408LLM\u6280\u672f\uff0c\u5b9e\u73b0\u53cd\u9988\u5206\u7c7b\u3001\u975e\u529f\u80fd\u6027\u9700\u6c42\u8bc6\u522b\u53ca\u81ea\u7136\u8bed\u8a00\u9700\u6c42\u751f\u6210\uff0c\u5e76\u63d0\u4f9b\u53cb\u597d\u754c\u9762\u548cJira\u96c6\u6210\u3002", "result": "RITA\u80fd\u9ad8\u6548\u5c06\u539f\u59cb\u53cd\u9988\u8f6c\u5316\u4e3a\u9700\u6c42\u5236\u54c1\uff0c\u5f25\u5408\u7814\u7a76\u4e0e\u5b9e\u8df5\u4e4b\u95f4\u7684\u9e3f\u6c9f\u3002", "conclusion": "RITA\u5c55\u793a\u4e86LLM\u5728\u9700\u6c42\u5de5\u7a0b\u4e2d\u7684\u5b9e\u7528\u4ef7\u503c\uff0c\u63a8\u52a8\u4e86\u53cd\u9988\u9a71\u52a8\u578bRE\u5de5\u5177\u7684\u5b9e\u9645\u843d\u5730\u3002"}}
{"id": "2601.11430", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2601.11430", "abs": "https://arxiv.org/abs/2601.11430", "authors": ["Marion Wiese"], "title": "A Practical Guide to Establishing Technical Debt Management", "comment": null, "summary": "This white paper provides an overview of the topic of \"technical debt\" and presents an approach for managing technical debt in teams. The white paper is based on the results of my dissertation, which aimed to translate scientific findings into practical guidance. To this end, I collaborated with other researchers to support three teams from different companies in adapting and establishing a technical debt management system tailored to their specific needs. Research findings were supplemented with details or additional approaches. Research results that were less practical were discarded. The result is a guide on establishing technical debt management within a team. The guide is intended to provide orientation and not be a rigid framework. We distinguish between \"best practices\" and \"nice-to-haves.\" \"Best practices\" are understood to be all approaches that were adopted by all three teams. \"Nice-to-haves\" were used by at least one team. In many places, it is explicitly mentioned that the team should decide together how to design the process. This also applies, of course, to all areas where this was not explicitly mentioned. This white paper explicitly does not cover the establishment of technical debt management across the entire company, but provides suggestions for this at the end.", "AI": {"tldr": "\u672c\u6587\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5173\u4e8e\u6280\u672f\u503a\u52a1\u7ba1\u7406\u7684\u5b9e\u7528\u6307\u5357\uff0c\u57fa\u4e8e\u4f5c\u8005\u535a\u58eb\u8bba\u6587\u7684\u7814\u7a76\u6210\u679c\uff0c\u65e8\u5728\u5e2e\u52a9\u56e2\u961f\u5efa\u7acb\u9002\u5408\u81ea\u8eab\u9700\u6c42\u7684\u6280\u672f\u503a\u52a1\u7ba1\u7406\u4f53\u7cfb\u3002", "motivation": "\u5c06\u5b66\u672f\u7814\u7a76\u6210\u679c\u8f6c\u5316\u4e3a\u5b9e\u9645\u53ef\u64cd\u4f5c\u7684\u56e2\u961f\u6307\u5bfc\uff0c\u4ee5\u5e94\u5bf9\u6280\u672f\u503a\u52a1\u95ee\u9898\u3002", "method": "\u4e0e\u591a\u5bb6\u516c\u53f8\u7684\u4e09\u4e2a\u56e2\u961f\u5408\u4f5c\uff0c\u6839\u636e\u5176\u5177\u4f53\u9700\u6c42\u8c03\u6574\u5e76\u5efa\u7acb\u6280\u672f\u503a\u52a1\u7ba1\u7406\u7cfb\u7edf\uff0c\u540c\u65f6\u7b5b\u9009\u548c\u8865\u5145\u7814\u7a76\u7ed3\u679c\u3002", "result": "\u5f62\u6210\u4e86\u4e00\u5957\u5305\u542b\u2018\u6700\u4f73\u5b9e\u8df5\u2019\u548c\u2018\u53ef\u9009\u5b9e\u8df5\u2019\u7684\u7075\u6d3b\u6307\u5357\uff0c\u5f3a\u8c03\u56e2\u961f\u5171\u540c\u51b3\u7b56\u7684\u91cd\u8981\u6027\u3002", "conclusion": "\u8be5\u6307\u5357\u4e13\u6ce8\u4e8e\u56e2\u961f\u5c42\u9762\u7684\u6280\u672f\u503a\u52a1\u7ba1\u7406\uff0c\u867d\u4e0d\u6db5\u76d6\u5168\u516c\u53f8\u8303\u56f4\uff0c\u4f46\u63d0\u4f9b\u4e86\u6269\u5c55\u5efa\u8bae\u3002"}}
