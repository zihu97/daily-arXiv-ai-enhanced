<div id=toc></div>

# Table of Contents

- [cs.SE](#cs.SE) [Total: 26]
- [cs.AR](#cs.AR) [Total: 18]
- [cs.OS](#cs.OS) [Total: 2]
- [cs.DC](#cs.DC) [Total: 7]
- [cs.MA](#cs.MA) [Total: 6]
- [cs.NI](#cs.NI) [Total: 7]


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [1] [Injecting Sustainability in Software Architecture: A Rapid Review](https://arxiv.org/abs/2512.00106)
*Markus Funke,Patricia Lago*

Main category: cs.SE

TL;DR: 本文通过文献快速综述与从业者焦点小组的混合方法研究，提炼出五个可操作的要点，以支持软件架构师在实践中整合可持续性。


<details>
  <summary>Details</summary>
Motivation: 随着可持续性成为软件工程中的核心责任，亟需系统性方法将其融入现有软件架构实践中，弥合学术研究与工业应用之间的差距。

Method: 采用混合方法：一方面对二次研究进行快速综述以识别挑战与机遇，另一方面组织从业者焦点小组以验证和丰富文献发现。

Result: 识别出在软件架构中嵌入可持续性的关键挑战与机遇，并结合业界反馈提炼出五项具体、可操作的实践建议。

Conclusion: 通过学术与工业界协作，本研究为软件架构师提供了切实可行的指导，有助于推动可持续性在软件架构实践中的有效整合。

Abstract: Sustainability has evolved from an emerging concern into a fundamental responsibility in software design, development, and operation. Research increasingly explores how sustainability can be systematically integrated into existing software engineering practices. Building on an industry-academia collaboration, we contribute to this discourse by conducting a mixed-method empirical study. We combine a rapid review of secondary studies with a focus group of practitioners. The review identifies challenges and opportunities in embedding sustainability in software architecture, while the focus group enriches and compares these findings. Based on the literature and industry synthesis, we derive five tangible takeaways to inform architects working in the field, and to guide our industry partners in the integration of sustainability concerns in architecture practices.

</details>


### [2] [Generating Verifiable CoT from Execution-Traces](https://arxiv.org/abs/2512.00127)
*Shailja Thakur,Vaibhav Saxena,Rohan Kulkarni,Shivdeep Singh,Parameswaran Selvam,Hima Patel,Hiroshi Kanayama*

Main category: cs.SE

TL;DR: 本文提出一种基于程序执行轨迹生成推理链（CoT）的方法，通过将代码动态执行过程转化为自然语言推理步骤，确保推理内容与程序实际行为一致，从而避免模型学习到逻辑错误的推理模式。实验表明，该方法在多个代码理解与生成任务上显著优于基线模型。


<details>
  <summary>Details</summary>
Motivation: 当前基于合成数据训练的语言模型在代码推理任务中存在严重缺陷：其推理步骤多为教师模型生成的看似合理但未经验证的解释，容易导致模型学习到表面可信但逻辑错误的推理方式。

Method: 作者构建了一个管道，首先对代码进行插桩以捕获其动态执行轨迹，然后将这些可验证的执行轨迹转化为自然语言形式的推理链，从而保证每一步推理都忠实反映程序的真实行为。

Result: 在CruxEval、LiveCodeBench-Exec和HumanEval等多个基准上，使用该方法训练的模型在输出预测任务上最高提升30分，在输入预测任务上提升28分，同时在代码解释和生成任务上也表现更优。

Conclusion: 将推理链建立在可验证的程序执行轨迹之上，能从根本上提升语言模型在代码相关任务中的推理能力和可靠性。

Abstract: Teaching language models to reason about code execution remains a fundamental challenge. While Chain-of-Thought (CoT) prompting has shown promise, current synthetic training data suffers from a critical weakness: the reasoning steps are often plausible-sounding explanations generated by teacher models, not verifiable accounts of what the code actually does. This creates a troubling failure mode where models learn to mimic superficially convincing but logically flawed reasoning patterns.
  We address this by grounding CoT generation directly in program execution traces. Our pipeline instruments code to capture its dynamic behavior, then narrates these verified execution traces into natural language rationales that are correct by construction. This execution-grounded approach ensures every reasoning step reflects what the program genuinely computes, eliminating logical hallucinations at the source. We evaluate our method on code reasoning tasks (forward reasoning on CruxEval and LiveCodeBench-Exec, backward reasoning on CruxEval-Input), as well as code generation and explanation tasks from HumanEval. Models trained on our bi-directional trace-grounded data achieve substantial improvements, with gains of up to 30 points on output prediction and 28 points on input prediction over base models, alongside improved explanation and code generation, demonstrating that verifiable reasoning fundamentally enhances model capabilities. https://github.ibm.com/IBM-Research-AI/Verified-Code-CoT

</details>


### [3] [Demystifying Errors in LLM Reasoning Traces: An Empirical Study of Code Execution Simulation](https://arxiv.org/abs/2512.00215)
*Mohammad Abdollahi,Khandaker Rifah Tasnia,Soumit Kanti Saha,Jinqiu Yang,Song Wang,Hadi Hemmati*

Main category: cs.SE

TL;DR: 该论文首次对推理型大语言模型（LLMs）在程序运行时行为推断中的表现进行了实证研究，构建了一个包含427个代码片段的基准数据集，并通过三种输入类型评估了四个先进模型。研究发现模型准确率在85%至98%之间，并提出了一个包含九类推理错误的分类体系；此外，通过工具增强推理可修复58%的计算类错误。


<details>
  <summary>Details</summary>
Motivation: 当前大多数关于大语言模型的研究聚焦于输出准确性和性能，将推理过程视为黑箱，缺乏对其推理轨迹结构和失败模式的理解。为填补这一空白，作者旨在深入探究LLMs在程序运行时推理行为中的错误特征。

Method: 作者从HumanEval Plus和LiveCodeBench中构建了一个包含427个代码片段的基准，每个片段配以常规、边界和无效三类输入，每类选取12个具体输入值及其真实执行结果。随后，评估了四个最先进的推理型LLM，并对其生成的推理轨迹进行分析，归纳出九类推理错误。最后，以“计算错误”类别为例，探索了工具增强推理的有效性。

Result: 四个被测LLM在不同输入类型下的准确率介于85%到98%之间；作者构建了一个包含九类推理错误的分类体系；在计算错误类别中，工具增强方法成功修正了58%的错误。

Conclusion: 推理型大语言模型虽在程序输出预测上表现良好，但其推理轨迹仍存在系统性错误；通过工具增强可有效提升其推理可靠性，表明结合外部工具是改进LLM推理能力的重要方向。

Abstract: Understanding a program's runtime reasoning behavior, meaning how intermediate states and control flows lead to final execution results, is essential for reliable code generation, debugging, and automated reasoning. Although large language models (LLMs) can accurately predict program outputs, most prior work has focused on output accuracy and performance, treating reasoning as a black box. As a result, little is known about the structure or failure modes of their reasoning traces. To address this gap, we conduct the first empirical study on runtime behavior inference with reasoning LLMs, aiming to uncover and characterize errors in their reasoning traces. We curate a benchmark from HumanEval Plus and LiveCodeBench, containing 427 code snippets. For each snippet, we test three input types: regular, edge, and invalid. Twelve input values are selected per snippet, each paired with its ground-truth execution result. We evaluate four state-of-the-art reasoning LLMs. Our results show that these models reach accuracies between 85 percent and 98 percent across input types. We also analyze the produced reasoning traces and develop a taxonomy with nine categories of inference errors. Finally, we explore tool-augmented reasoning. Using failures in the Computation Errors category as a case study, our experiments show that this approach corrects 58 percent of such errors, demonstrating the potential of tool support for improving LLM reasoning.

</details>


### [4] [CodeFlowLM: Incremental Just-In-Time Defect Prediction with Pretrained Language Models and Exploratory Insights into Defect Localization](https://arxiv.org/abs/2512.00231)
*Monique Louise Monteiro,George G. Cabral,Adriano L. I. OLiveira*

Main category: cs.SE

TL;DR: 本文提出了CodeFlowLM，一种基于预训练语言模型的增量学习框架，用于即时软件缺陷预测（JIT-SDP）和缺陷定位（JIT-DL）。该方法通过持续微调应对概念漂移、类别不平衡等问题，在JIT-SDP任务中显著优于基线方法；在JIT-DL任务中，对大语言模型（如GPT-5）与注意力模型进行了对比分析，揭示了当前提示驱动缺陷推理的优势与局限。


<details>
  <summary>Details</summary>
Motivation: 传统在线学习方法在JIT-SDP中难以有效处理概念漂移、类别不平衡和验证延迟等问题，且通常需要从头训练。同时，大语言模型在JIT缺陷定位中的能力尚缺乏系统评估和深入理解。

Method: 提出CodeFlowLM框架，采用持续微调策略，利用编码器-only和编码器-解码器结构的预训练语言模型（如CodeT5+、UniXCoder）进行增量式JIT-SDP；同时在JIT-DL任务中，将GPT-5等大语言模型与注意力模型进行对比，并进行定性错误分析。

Result: 在JIT-SDP任务中，CodeFlowLM相比基线BORB最高提升68% G-Mean；在JIT-DL任务中，GPT-5在Recall@20%和Effort@20%指标上表现稳定且具竞争力，但在Top-k和IFA等细粒度排序指标上仍逊于注意力模型。错误分析揭示了假阳性主要源于保守偏见、上下文不足和数据标注问题。

Conclusion: CodeFlowLM显著推进了增量式JIT-SDP的最新水平，展现出优异的适应性和鲁棒性；对LLM在JIT-DL中的探索不仅提供了性能基准，也揭示了当前基于提示的缺陷推理方法的关键局限。

Abstract: This work introduces CodeFlowLM, an incremental learning framework for Just-In-Time Software Defect Prediction (JIT-SDP) that leverages pre-trained language models (PLMs). Unlike traditional online learners, CodeFlowLM employs continual fine-tuning to address concept drift, class imbalance, and verification latency without retraining from scratch. We evaluated encoder-only and encoder-decoder PLMs (notably CodeT5+ and UniXCoder) in JIT-SDP scenarios within and between projects, comparing them with the incremental baseline BORB. The results show that CodeFlowLM achieves up to 68% G-Mean gains, confirming its superior adaptability and robustness in evolving software environments. We further extend the analysis to Just-in-Time Defect Localization (JIT-DL), benchmarking Large Language Models (LLMs) such as GPT-5, Claude Sonnet 4.5, and Gemini 2.5 Pro against attention-based models. GPT-5 delivers comparable performance for Recall@20% and Effort@20% with higher stability, although attention-based methods retain an advantage in fine-grained ranking metrics (Top-k, IFA). A qualitative error analysis reveals that most false positives arise from (1) human-like conservative bias, (2) insufficient contextual information in diff-based prompts, and (3) potential dataset mislabeling in JIT-Defects4J. These findings highlight both the promise and the current limitations of LLM reasoning in defect localization. False negatives occur in smaller proportions. Overall, CodeFlowLM significantly advances the state of the art in incremental JIT-SDP, demonstrating superior adaptability and robustness in evolving software environments. Furthermore, our exploratory analysis of LLMs in JIT-DL not only benchmarks their performance against established attention-based models but also provides critical insights into the current limitations of prompt-based defect reasoning.

</details>


### [5] [Progressive Code Integration for Abstractive Bug Report Summarization](https://arxiv.org/abs/2512.00325)
*Shaira Sadia Karim,Abrar Mahmud Rahim,Lamia Alam,Ishmam Tashdeed,Lutfun Nahar Lota,Md. Abu Raihan M. Kamal,Md. Azam Hossain*

Main category: cs.SE

TL;DR: 本文提出了一种渐进式代码融合框架，用于基于大语言模型（LLM）的抽象型缺陷报告摘要生成，通过逐步整合长代码片段与文本内容，在多个数据集和模型上显著优于抽取式基线方法。


<details>
  <summary>Details</summary>
Motivation: 现有缺陷报告摘要方法多依赖表层文本线索，常忽略关键的关联代码片段，导致摘要不完整或冗余，难以支持准确的缺陷诊断。

Method: 提出一种渐进式代码融合框架，将长代码片段与文本内容逐步整合进LLM中，以克服上下文窗口限制，生成语义丰富的摘要。

Result: 在四个基准数据集和八个LLM上评估，该方法比抽取式基线提升7.5%-58.2%，性能媲美当前最先进的抽象式方法。

Conclusion: 联合利用文本与代码信息能显著提升缺陷报告摘要质量，有助于开发者更高效地理解软件问题。

Abstract: Bug reports are often unstructured and verbose, making it challenging for developers to efficiently comprehend software issues. Existing summarization approaches typically rely on surface-level textual cues, resulting in incomplete or redundant summaries, and they frequently ignore associated code snippets, which are essential for accurate defect diagnosis. To address these limitations, we propose a progressive code-integration framework for LLM-based abstractive bug report summarization. Our approach incrementally incorporates long code snippets alongside textual content, overcoming standard LLM context window constraints and producing semantically rich summaries. Evaluated on four benchmark datasets using eight LLMs, our pipeline outperforms extractive baselines by 7.5%-58.2% and achieves performance comparable to state-of-the-art abstractive methods, highlighting the benefits of jointly leveraging textual and code information for enhanced bug comprehension.

</details>


### [6] [Framework-Aware Code Generation with API Knowledge Graph-Constructed Data: A Study on HarmonyOS](https://arxiv.org/abs/2512.00380)
*Mingwei Liu,Zheng Pei,Yanlin Wang,Zihao Wang,Zikang Li,Enci Lin,Xin Peng,Zibin Zheng*

Main category: cs.SE

TL;DR: 本文针对大语言模型（LLM）在低资源软件框架（如HarmonyOS）中代码生成表现不佳的问题，提出APIKG4SYN框架，利用API知识图谱构建面向API的问答-代码对数据集，无需可执行代码即可有效提升LLM在该类场景下的性能。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在预训练阶段缺乏对低资源软件框架（如HarmonyOS）的充分接触，导致其在生成相关代码时频繁出现API或语法错误，即使逻辑结构正确也无法保证代码可用性。

Method: 提出APIKG4SYN框架，通过API知识图谱构建单API与多API的问答-代码对；其中多API组合通过基于不确定性估计（UE）驱动的蒙特卡洛树搜索（MCTS）生成，用于微调LLM。

Result: 以HarmonyOS为案例构建首个相关代码生成基准，实验表明使用APIKG4SYN微调Qwen模型后，pass@1准确率达25.00%，优于基线GPT模型的17.59%。

Conclusion: 面向API的数据构造方法能显著提升大语言模型在低资源软件开发环境中的代码生成能力，验证了APIKG4SYN的有效性。

Abstract: In the context of software frameworks with limited resources (such as HarmonyOS), large language models (LLMs) often exhibit poor code generation performance because they lack sufficient exposure to such environments during pre-training. Although LLMs can usually maintain correct logical structures across programming languages, they frequently struggle when dealing with framework-specific APIs or syntax, resulting in errors. This indicates that while pre-training equips LLMs with general algorithmic capabilities, they remain unfamiliar with the distinctive syntax and API usage of underrepresented frameworks. As a result, even advanced commercial models like GPT-4o cannot reliably generate correct code without prior adaptation. To address this issue, we propose APIKG4SYN, a framework designed to exploit API knowledge graphs for the construction of API-oriented question-code pairs, specifically tailored for low-resource frameworks without requiring executable code. APIKG4SYN integrates both single-API and multi-API knowledge, where the latter is derived through uncertainty estimation (UE)-driven Monte Carlo Tree Search (MCTS), enabling the creation of a diverse and informative dataset for fine-tuning LLMs. Using HarmonyOS as a case study, we build the first benchmark for HarmonyOS code generation. Experimental results show that fine-tuning Qwen with APIKG4SYN raises pass@1 accuracy to 25.00%, compared with 17.59% for the baseline GPT model. These results confirm that API-oriented data significantly enhance LLM performance in low-resource software development scenarios.

</details>


### [7] [Bias Testing and Mitigation in Black Box LLMs using Metamorphic Relations](https://arxiv.org/abs/2512.00556)
*Sina Salimian,Gias Uddin,Sumon Biswas,Henry Leung*

Main category: cs.SE

TL;DR: 本文提出一种基于六种新型变质关系（MRs）的统一框架，用于系统评估和缓解大语言模型中的隐性社会偏见。该方法通过生成语义等价但更具挑战性的输入变体，有效揭示现有工具难以检测的隐藏偏见，并利用这些变体进行微调，显著提升模型的公平性。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型中存在难以被现有防护机制识别的隐性社会偏见，尤其在面对间接或上下文复杂的偏见诱导提示时表现不足，亟需更有效的评估与缓解方法。

Method: 引入六种基于变质测试原理的新型变质关系（MRs），将直接偏见诱导输入转化为语义等价但对抗性更强的变体，用于自动化检测模型偏见；同时利用这些变体生成多样化的偏见样本进行微调，实现评估与缓解的统一。

Result: 在六个主流大语言模型和BiasAsker基准子集上的实验表明，所提MRs比现有工具多发现最多14%的隐藏偏见；结合原始与MR变异样本进行微调后，模型安全响应率从54.7%提升至88.9%以上。

Conclusion: 变质关系是一种实用且有效的机制，能够显著提升对话式人工智能系统的公平性和对偏见的鲁棒性。

Abstract: The widespread deployment of Large Language Models (LLMs) has intensified concerns about subtle social biases embedded in their outputs. Existing guardrails often fail when faced with indirect or contextually complex bias-inducing prompts. To address these limitations, we propose a unified framework for both systematic bias evaluation and targeted mitigation. Our approach introduces six novel Metamorphic Relations (MRs) that, based on metamorphic testing principles, transform direct bias-inducing inputs into semantically equivalent yet adversarially challenging variants. These transformations enable an automated method for exposing hidden model biases: when an LLM responds inconsistently or unfairly across MR-generated variants, the underlying bias becomes detectable. We further show that the same MRs can be used to generate diverse bias-inducing samples for fine-tuning, directly linking the testing process to mitigation. Using six state-of-the-art LLMs - spanning open-source and proprietary models - and a representative subset of 385 questions from the 8,978-item BiasAsker benchmark covering seven protected groups, our MRs reveal up to 14% more hidden biases compared to existing tools. Moreover, fine-tuning with both original and MR-mutated samples significantly enhances bias resiliency, increasing safe response rates from 54.7% to over 88.9% across models. These results highlight metamorphic relations as a practical mechanism for improving fairness in conversational AI.

</details>


### [8] [SAGE: Semantic-Aware Gray-Box Game Regression Testing with Large Language Models](https://arxiv.org/abs/2512.00560)
*Jinyu Cai,Jialong Li,Nianyu Li,Zhenyu Mao,Mingyue Zhang,Kenji Tei*

Main category: cs.SE

TL;DR: SAGE 是一个面向灰盒游戏环境的语义感知回归测试框架，通过 LLM 引导的强化学习自动生成测试用例，结合语义多目标优化精简测试集，并利用 LLM 分析更新日志实现高效测试优先级排序，在 Overcooked Plus 和 Minecraft 上验证了其在缺陷检测、成本控制和版本适应性方面的优越性。


<details>
  <summary>Details</summary>
Motivation: 现代在线服务型游戏快速迭代，使得回归测试至关重要；然而现有方法在缺乏完整源码的灰盒场景下存在依赖人工构建测试用例、测试套件冗余难以维护、缺乏有效测试优先级机制等问题，导致测试成本高、自动化程度低、缺陷检出能力不足。

Method: SAGE 框架包含三个核心阶段：1）使用 LLM 引导的强化学习进行目标导向探索，自动生成多样化的基础测试套件；2）基于语义的多目标优化方法，平衡成本、覆盖率与稀有性，提炼出紧凑高价值的测试子集；3）利用 LLM 对版本更新日志进行语义分析，优先执行与变更最相关的测试用例。

Result: 在 Overcooked Plus 和 Minecraft 两个环境中评估表明，SAGE 相比自动化基线和人工录制的测试用例，在显著降低执行成本的同时实现了更优的缺陷检测效果，并展现出良好的版本迭代适应能力。

Conclusion: SAGE 有效解决了灰盒游戏环境下回归测试中的生成、维护与选择难题，为高频率迭代的游戏提供了高效、自动且语义感知的回归测试解决方案。

Abstract: The rapid iteration cycles of modern live-service games make regression testing indispensable for maintaining quality and stability. However, existing regression testing approaches face critical limitations, especially in common gray-box settings where full source code access is unavailable: they heavily rely on manual effort for test case construction, struggle to maintain growing suites plagued by redundancy, and lack efficient mechanisms for prioritizing relevant tests. These challenges result in excessive testing costs, limited automation, and insufficient bug detection. To address these issues, we propose SAGE, a semanticaware regression testing framework for gray-box game environments. SAGE systematically addresses the core challenges of test generation, maintenance, and selection. It employs LLM-guided reinforcement learning for efficient, goal-oriented exploration to automatically generate a diverse foundational test suite. Subsequently, it applies a semantic-based multi-objective optimization to refine this suite into a compact, high-value subset by balancing cost, coverage, and rarity. Finally, it leverages LLM-based semantic analysis of update logs to prioritize test cases most relevant to version changes, enabling efficient adaptation across iterations. We evaluate SAGE on two representative environments, Overcooked Plus and Minecraft, comparing against both automated baselines and human-recorded test cases. Across all environments, SAGE achieves superior bug detection with significantly lower execution cost, while demonstrating strong adaptability to version updates.

</details>


### [9] [Enhancing Analogy-Based Software Effort Estimation with Firefly Algorithm Optimization](https://arxiv.org/abs/2512.00571)
*Tarun Chintada,Uday Kiran Cheera*

Main category: cs.SE

TL;DR: 本文提出了一种结合萤火虫算法（FA）与类比估算（ABE）的新模型FAABE，以提升软件项目估算的准确性，并在多个公开数据集上验证了其优于传统模型的预测性能。


<details>
  <summary>Details</summary>
Motivation: 传统类比估算（ABE）方法在面对与历史项目差异较大的新软件项目时，难以保证高精度，且缺乏可靠的优化策略。

Method: 将萤火虫算法（Firefly Algorithm, FA）与ABE结合，构建FAABE模型，并采用特征选择技术提升预测效率，在Cocomo81、Desharnais、China、Albrecht、Kemerer和Maxwell等五个公开数据集上进行实验。

Result: 实验结果表明，FAABE在MMRE、MAE、MSE和RMSE等多种误差指标上均显著优于传统模型，显示出更高的预测精度。

Conclusion: FA与ABE的集成有效提升了软件成本估算的准确性，验证了FAABE模型的可行性和优越性。

Abstract: Analogy-Based Estimation (ABE) is a popular method for non-algorithmic estimation due to its simplicity and effectiveness. The Analogy-Based Estimation (ABE) model was proposed by researchers, however, no optimal approach for reliable estimation was developed. Achieving high accuracy in the ABE might be challenging for new software projects that differ from previous initiatives. This study (conducted in June 2024) proposes a Firefly Algorithm-guided Analogy-Based Estimation (FAABE) model that combines FA with ABE to improve estimation accuracy. The FAABE model was tested on five publicly accessible datasets: Cocomo81, Desharnais, China, Albrecht, Kemerer and Maxwell. To improve prediction efficiency, feature selection was used. The results were measured using a variety of evaluation metrics; various error measures include MMRE, MAE, MSE, and RMSE. Compared to conventional models, the experimental results show notable increases in prediction precision, demonstrating the efficacy of the Firefly-Analogy ensemble.

</details>


### [10] [FC-ADL: Efficient Microservice Anomaly Detection and Localisation Through Functional Connectivity](https://arxiv.org/abs/2512.00844)
*Giles Winchester,George Parisis,Luc Berthouze*

Main category: cs.SE

TL;DR: 本文提出FC-ADL方法，利用功能连接性高效检测和定位微服务系统中的异常，兼顾准确性与可扩展性。


<details>
  <summary>Details</summary>
Motivation: 微服务架构虽具模块化优势，但其动态性和复杂依赖关系使得异常检测与定位困难；现有方法要么忽略时变依赖信息，要么因依赖因果推断而难以扩展至大规模系统。

Method: 基于神经科学中的功能连接性概念，构建端到端可扩展方法FC-ADL，通过高效刻画微服务指标间时变依赖变化来实现异常检测与根因候选定位，避免因果推断和多变量方法的高开销。

Result: 在多种故障场景下，FC-ADL在检测与定位性能上优于现有最先进方法，并成功应用于阿里巴巴超大规模真实微服务部署，验证了其可扩展性。

Conclusion: FC-ADL有效解决了微服务系统中异常检测与定位的准确性与可扩展性难题，为大规模微服务运维提供了实用工具。

Abstract: Microservices have transformed software architecture through the creation of modular and independent services. However, they introduce operational complexities in service integration and system management that makes swift and accurate anomaly detection and localisation challenging. Despite the complex, dynamic, and interconnected nature of microservice architectures, prior works that investigate metrics for anomaly detection rarely include explicit information about time-varying interdependencies. And whilst prior works on fault localisation typically do incorporate information about dependencies between microservices, they scale poorly to real world large-scale deployments due to their reliance on computationally expensive causal inference. To address these challenges we propose FC-ADL, an end-to-end scalable approach for detecting and localising anomalous changes from microservice metrics based on the neuroscientific concept of functional connectivity. We show that by efficiently characterising time-varying changes in dependencies between microservice metrics we can both detect anomalies and provide root cause candidates without incurring the significant overheads of causal and multivariate approaches. We demonstrate that our approach can achieve top detection and localisation performance across a wide degree of different fault scenarios when compared to state-of-the-art approaches. Furthermore, we illustrate the scalability of our approach by applying it to Alibaba's extremely large real-world microservice deployment.

</details>


### [11] [Large Language Models for Software Engineering: A Reproducibility Crisis](https://arxiv.org/abs/2512.00651)
*Mohammed Latif Siddiq,Arvin Islam-Gomes,Natalie Sekerak,Joanna C. S. Santos*

Main category: cs.SE

TL;DR: 本文对640篇2017至2025年间发表的将大语言模型（LLM）应用于软件工程（SE）的研究论文进行了大规模实证分析，首次系统评估其可复现性实践。研究发现尽管近年来有所改善，但在代码、环境、版本控制和文档等方面仍存在显著缺陷；同时指出当前的成果徽章制度虽能反映产出物的存在，却难以保证执行一致性和长期可复现性，并据此提出可复现性成熟度模型（RMM）及改进建议。


<details>
  <summary>Details</summary>
Motivation: 当前在大语言模型用于软件工程的研究中，可复现性状况尚不清晰，缺乏系统性评估。作者旨在揭示该领域可复现性实践的真实水平，识别常见问题（“可复现性异味”），并为提升研究透明度与严谨性提供指导。

Method: 作者系统收集并分析了2017–2025年间发表于顶级软件工程、机器学习和自然语言处理会议的640篇论文，从论文、代码仓库和文档中提取结构化元数据；基于七类“可复现性异味”（代码与执行、数据、文档、环境与工具、版本控制、模型、访问与法律），对所有论文及其产出物进行人工标注，并围绕四个研究问题展开定量与定性分析。

Result: 研究发现：（1）产出物可用性、环境描述、版本控制和文档清晰度等方面存在持续性不足；（2）近年虽有小幅改进且顶级SE会议更广泛采用产出物评审机制，但问题仍未根本解决；（3）现有徽章主要反映产出物是否提供，而非能否成功复现或长期维持；（4）不同发表场所对透明度实践有显著影响。

Conclusion: 当前LLM-for-SE研究的可复现性整体仍不理想，需超越简单的“有无产出物”二元判断。作者提出可复现性成熟度模型（RMM）以支持多维度、渐进式的可复现性评估，并给出具体建议以减少“可复现性异味”，推动该领域研究向更高严谨性发展。

Abstract: Reproducibility is a cornerstone of scientific progress, yet its state in large language model (LLM)-based software engineering (SE) research remains poorly understood. This paper presents the first large-scale, empirical study of reproducibility practices in LLM-for-SE research. We systematically mined and analyzed 640 papers published between 2017 and 2025 across premier software engineering, machine learning, and natural language processing venues, extracting structured metadata from publications, repositories, and documentation. Guided by four research questions, we examine (i) the prevalence of reproducibility smells, (ii) how reproducibility has evolved over time, (iii) whether artifact evaluation badges reliably reflect reproducibility quality, and (iv) how publication venues influence transparency practices. Using a taxonomy of seven smell categories: Code and Execution, Data, Documentation, Environment and Tooling, Versioning, Model, and Access and Legal, we manually annotated all papers and associated artifacts. Our analysis reveals persistent gaps in artifact availability, environment specification, versioning rigor, and documentation clarity, despite modest improvements in recent years and increased adoption of artifact evaluation processes at top SE venues. Notably, we find that badges often signal artifact presence but do not consistently guarantee execution fidelity or long-term reproducibility. Motivated by these findings, we provide actionable recommendations to mitigate reproducibility smells and introduce a Reproducibility Maturity Model (RMM) to move beyond binary artifact certification toward multi-dimensional, progressive evaluation of reproducibility rigor.

</details>


### [12] [Code Comments for Quantum Software Development Kits: An Empirical Study on Qiskit](https://arxiv.org/abs/2512.00766)
*Zenghui Zhou,Yuechen Li,Yi Cai,Jinlong Wen,Xiaohan Yu,Zheng Zheng,Beibei Yin*

Main category: cs.SE

TL;DR: 本文构建了首个量子计算代码注释数据集CC4Q，通过人工标注和实证研究，分析了Qiskit中代码注释的结构、开发者意图及量子相关主题，揭示了量子软件与经典软件在注释上的关键差异，并提出了适用于量子领域的注释分类体系。


<details>
  <summary>Details</summary>
Motivation: 由于量子力学的非直观性，程序员在理解和维护量子软件时面临挑战，而代码注释作为解释程序功能和逻辑的重要手段，缺乏系统性研究和实用指导。为填补这一空白，作者聚焦于主流量子SDK Qiskit，开展对代码注释的深入分析。

Method: 作者构建了包含9677个代码注释对和21970个句子级注释单元的数据集CC4Q，并进行了大量人工标注；验证了经典程序中开发者意图分类法在量子软件中的适用性，同时提出考虑量子特有知识的新分类法；从注释结构与覆盖率、开发者意图和量子主题三个角度开展实证研究。

Result: 研究发现量子软件与经典软件在代码注释方面存在显著差异，并识别出与量子软件开发相关的特有知识内容。

Conclusion: 该研究不仅提供了首个量子计算代码注释数据集，还通过实证分析揭示了量子注释的特点，为未来量子软件的开发、维护和注释实践提供了理论基础和实用指导。

Abstract: Quantum computing is gaining attention from academia and industry. With the quantum Software Development Kits (SDKs), programmers can develop quantum software to explore the power of quantum computing. However, programmers may face challenges in understanding quantum software due to the non-intuitive quantum mechanics. To facilitate software development and maintenance, code comments offered in quantum SDKs serve as a natural language explanation of program functionalities and logical flows. Despite their importance, scarce research systematically reports their value and provides constructive guidelines for programmers. To address this gap, our paper focuses on Qiskit, one of the most popular quantum SDKs, and presents CC4Q, the first dataset of code comments for quantum computing. CC4Q incorporates 9677 code comment pairs and 21970 sentence-level code comment units, the latter of which involve heavy human annotation. Regarding the annotation, we validate the applicability of the developer-intent taxonomy used in classical programs, and also propose a new taxonomy considering quantum-specific knowledge. We conduct an empirical study comprehensively interpreting code comments from three perspectives: comment structure and coverage, developers' intentions, and associated quantum topics. Our findings uncover key differences in code comments between classical and quantum software, and also outline quantum-specific knowledge relevant to quantum software development.

</details>


### [13] [The Software Infrastructure Attitude Scale (SIAS): A Questionnaire Instrument for Measuring Professionals' Attitudes Toward Technical and Sociotechnical Infrastructure](https://arxiv.org/abs/2512.00855)
*Miikka Kuutila,Paul Ralph,Huilian Sophie Qiu,Ronnie de Souza Santos,Morakot Choetkiertikul,Amin Milani Fard,Rana Alkadhi,Xavier Devroey,Gregorio Robles,Hideaki Hata,Sebastian Baltes,Vladimir Kovalenko,Shalini Chakraborty,Eray Tuzun,Hera Arif,Gianisa Adisaputri,Kelly Garcés,Anielle S. L. Andrade,Eyram Amedzor,Bimpe Ayoola,Keisha Gaspard-Chickoree,Arazoo Hoseyni*

Main category: cs.SE

TL;DR: 本文开发并验证了一个用于测量软件工程师对技术和社会技术基础设施态度的心理测量量表，通过探索性和验证性因子分析证实了其良好的信效度。


<details>
  <summary>Details</summary>
Motivation: 当前软件工程研究强调社会技术视角的重要性，但缺乏专门用于评估相关态度的心理测量工具，因此需要构建一个有效、可靠的量表以支持实证研究。

Method: 基于基础设施理论与态度理论，设计量表条目，并对225名软件从业者进行调查；采用分样本策略，一半用于探索性因子分析（EFA），另一半用于验证性因子分析（CFA），并进一步评估内容效度、聚合效度、区分效度和准则效度。

Result: EFA支持技术和社技基础设施的二因子结构，解释65%方差；CFA显示模型拟合良好；量表在内容、聚合、区分和准则效度方面均表现优异。

Conclusion: 所开发的量表是测量软件工程师对技术和社技基础设施态度的有效工具，有助于提升软件工程中行为与实证研究的心理测量严谨性。

Abstract: Context: Recent software engineering (SE) research has highlighted the need for sociotechnical research, implying a demand for customized psychometric scales. Objective: We define the concepts of technical and sociotechnical infrastructure in software engineering, and develop and validate a psychometric scale that measures attitudes toward them. Method: Grounded in theories of infrastructure, attitudes, and prior work on psychometric measurement, we defined the target constructs and generated scale items. The scale was administered to 225 software professionals and evaluated using a split sample. We conducted an exploratory factor analysis (EFA) on one half of the sample to uncover the underlying factor structure and performed a confirmatory factor analysis (CFA) on the other half to validate the structure. Further analyses with the whole sample assessed face, criterion-related, and discriminant validity. Results: EFA supported a two-factor structure (technical and sociotechnical infrastructure), accounting for 65% of the total variance with strong loadings. CFA confirmed excellent model fit. Face and content validity were supported by the item content reflecting cognitive, affective, and behavioral components. Both subscales were correlated with job satisfaction, perceived autonomy, and feedback from the job itself, supporting convergent validity. Regression analysis supported criterion-related validity, while the Heterotrait-Monotrait ratio of correlations (HTMT), the Fornell-Larcker criterion, and model comparison all supported discriminant validity. Discussion: The resulting scale is a valid instrument for measuring attitudes toward technical and sociotechnical infrastructure in software engineering research. Our work contributes to ongoing efforts to integrate psychological measurement rigor into empirical and behavioral software engineering research.

</details>


### [14] [The AI Attribution Paradox: Transparency as Social Strategy in Open-Source Software Development](https://arxiv.org/abs/2512.00867)
*Obada Kraishan*

Main category: cs.SE

TL;DR: 该研究分析了2023-2025年间14,300个GitHub提交，揭示开发者在使用AI编程助手时存在“AI归属悖论”：尽管95.2%的提交涉及AI，但仅29.5%明确披露，且不同工具（如Claude 80.5% vs Copilot 9.0%）差异显著。明确归属会引发略多社区互动，但工具选择对社区反应影响更大；社区情绪总体中立，表明好奇而非敌意。归属行为随时间快速演变，反映其作为策略性沟通而非单纯透明。


<details>
  <summary>Details</summary>
Motivation: 探讨AI编程助手普及背景下开发者如何在承认AI协助与应对社区审视之间取得平衡，即所谓的“AI归属悖论”，以理解新兴技术采纳过程中的透明度、责任归属与社区规范形成机制。

Method: 分析2023至2025年间来自7,393个GitHub仓库的14,300次提交，考察八种主流AI工具的归属策略及社区反馈，采用定量内容分析与时间序列方法评估归属率、社区反应（提问、评论数量）及情感倾向。

Result: 研究发现AI使用极为普遍（95.2%的提交），但显式归属比例较低（29.5%），且因工具而异（Claude达80.5%，Copilot仅9.0%）。显式归属带来轻微更多社区互动（提问+23%，评论+21%），但工具类型对社区接受度的影响远大于归属方式（20–30倍）。社区情绪整体中性，且显式归属比例从2024年初接近零升至2025年底的40%，显示规范快速演化。

Conclusion: AI归属行为本质上是一种策略性沟通，而非单纯的透明度实践。研究深化了对算法问责和人机协作中规范形成的理解，并为开发者披露决策、平台设计归属机制及未来相关研究提供启示。

Abstract: AI coding assistants have transformed software development, raising questions about transparency and attribution practices. We examine the "AI attribution paradox": how developers strategically balance acknowledging AI assistance with managing community scrutiny. Analyzing 14,300 GitHub commits across 7,393 repositories from 2023-2025, we investigated attribution strategies and community responses across eight major AI tools. Results reveal widespread AI usage (95.2% of commits) but strategic attribution: only 29.5% employ explicit disclosure, with dramatic tool variation (Claude 80.5% versus Copilot 9.0%). Explicit attribution triggers modest scrutiny (23% more questions and 21% more comments) but tool choice matters 20-30 times more for predicting reception. Community sentiment remains neutral regardless of attribution type, suggesting curiosity rather than hostility. Temporal analyses show rapid norm evolution: explicit attribution increased from near-zero in early 2024 to 40% by late 2025, indicating community adaptation. These findings illuminate attribution as strategic communication rather than simple transparency, advancing understanding of algorithmic accountability and norm formation during technological transitions. We discuss implications for developers navigating disclosure decisions, platforms designing attribution mechanisms, and researchers studying emergent practices in AI-augmented collaborative work.

</details>


### [15] [Neural Variable Name Repair: Learning to Rename Identifiers for Readability](https://arxiv.org/abs/2512.01141)
*Muhammad Yousuf,Akshat Bagade,Chhittebbayi Penugonda,Maanas Baraya*

Main category: cs.SE

TL;DR: 本文研究C++代码中变量名修复问题，通过构建基于Llama 3.1-8B的微调与重排序流程，在保留原始语义的前提下显著提升模型生成准确、描述性强的变量名的能力。


<details>
  <summary>Details</summary>
Motivation: 开发者常面对变量命名泛化或具有误导性、函数缺乏文档的代码，这会降低理解效率、增加隐性错误风险，并影响人类和大语言模型对代码的推理能力。

Method: 从The Stack的C++代码中自动构建数据集，使用Tree-sitter解析函数并掩码单个标识符；在Llama 3.1-8B基础上采用带预热和Dropout的LoRA微调策略，并结合双编码器重排序器对Top-k候选进行筛选。

Result: 在200个C++函数测试集上，零样本Llama 3.1基线仅达6.1%精确匹配率，而最佳LoRA微调模型达到43.1%精确匹配、50.2% Top-5命中率和82.03的嵌入相似度得分；重排序器进一步提升了选择质量。

Conclusion: 任务特定的微调结合重排序是一种有前景的实用变量名修复方法，能有效提升代码可读性与模型推理能力。

Abstract: Developers routinely work with source files whose variable names are generic or misleading, and with teams moving quickly, many functions are left undocumented. This slows comprehension, increases the risk of subtle bugs, and makes it harder for both humans and large language models (LLMs) to reason about code. We study variable name repair: given a real C++ function where all occurrences of one local or parameter name have been replaced by a placeholder (e.g. ID 1), the goal is to generate a natural, descriptive replacement name. We automatically construct this task from the C++ portion of BigCode's The Stack by parsing functions with Tree-sitter, masking a single identifier, and treating the original name as supervision. On top of Llama 3.1-8B, we build a pipeline with (i) warmup and dropout schedules for more stable fine-tuning, (ii) LoRA adapters for efficient specialization on identifier repair, and (iii) a dual-encoder reranker over top-k generator candidates. We evaluate using exact match, Top-5 Hit, and an embedding-based partial similarity score (0-100) that gives credit for near synonyms and format variants (e.g., jsonValue vs. json). On a held-out set of 200 C++ functions, a zero-shot Llama 3.1 baseline reaches 6.1 percent exact match. Our best LoRA-tuned model (with warmup and dropout) achieves 43.1 percent exact match, 50.2 percent Top-5 Hit, and an 82.03 partial-match score. A dual encoder reranker further improves selection quality without modifying the underlying generator, suggesting that task-specific fine-tuning plus reranking is a promising approach for practical identifier repair tools.

</details>


### [16] [Beyond Greenfield: AI-Driven Productivity in Documentation and Brownfield Engineering](https://arxiv.org/abs/2512.01155)
*Krishna Kumaar Sharma*

Main category: cs.SE

TL;DR: 本文提出了一种名为Discover-Define-Deliver（D3）的结构化大语言模型辅助工作流，用于应对遗留系统工程中的模糊性和知识碎片化问题，并通过一项包含52名从业者的探索性调研验证了其在提升任务清晰度、文档质量和降低认知负荷方面的初步成效。


<details>
  <summary>Details</summary>
Motivation: 现有研究多聚焦于全新开发或合成任务，缺乏针对包含遗留系统、文档不全和架构知识碎片化等复杂情境下的结构化大语言模型（LLM）应用方法，因此亟需一种适用于“棕地”工程场景的可靠工作流程。

Method: 作者设计了D3框架，采用角色分离的提示策略，包括一个生成候选输出的Builder模型和一个提供结构化反馈的Reviewer模型，并在真实工程任务中对52名软件从业者进行了探索性调研。

Result: 参与者报告在任务清晰度、文档质量、认知负荷和返工率方面均有改善，自评平均生产力提升26.9%，约77%的人认知负荷降低，83%在Define阶段返工减少。

Conclusion: D3框架在棕地工程中展现出潜力，但当前结果基于自评数据，尚不能确立因果关系，未来需通过受控实验进一步验证其有效性。

Abstract: Brownfield engineering work involving legacy systems, incomplete documentation, and fragmented architectural knowledge poses unique challenges for the effective use of large language models (LLMs). Prior research has largely focused on greenfield or synthetic tasks, leaving a gap in structured workflows for complex, context-heavy environments. This paper introduces the Discover-Define-Deliver (D3) Framework, a disciplined LLM-assisted workflow that combines role-separated prompting strategies with applied best practices for navigating ambiguity in brownfield systems. The framework incorporates a dual-agent prompting architecture in which a Builder model generates candidate outputs and a Reviewer model provides structured critique to improve reliability. I conducted an exploratory survey study with 52 software practitioners who applied the D3 workflow to real-world engineering tasks such as legacy system exploration, documentation reconstruction, and architectural refactoring. Respondents reported perceived improvements in task clarity, documentation quality, and cognitive load, along with self-estimated productivity gains. In this exploratory study, participants reported a weighted average productivity improvement of 26.9%, reduced cognitive load for approximately 77% of participants, and reduced rework for 83% during the Define phase. As these findings are self-reported and not derived from controlled experiments, they should be interpreted as preliminary evidence of practitioner sentiment rather than causal effects. The results highlight both the potential and limitations of structured LLM workflows for legacy engineering systems and motivate future controlled evaluations.

</details>


### [17] [LAURA: Enhancing Code Review Generation with Context-Enriched Retrieval-Augmented LLM](https://arxiv.org/abs/2512.01356)
*Yuxin Zhang,Yuxia Zhang,Zeyu Sun,Yanjie Jiang,Hui Liu*

Main category: cs.SE

TL;DR: 本文提出了一种基于大语言模型（LLM）的代码审查生成框架LAURA，通过引入审查示例检索、上下文增强和系统性引导，显著提升了ChatGPT-4o和DeepSeek v3生成高质量代码审查评论的能力，并构建了一个高质量数据集以解决现有数据中低质量评论的问题。


<details>
  <summary>Details</summary>
Motivation: 代码审查对保障软件质量和可维护性至关重要，但其耗时、依赖专业知识且缺乏经验丰富的审查者，已成为开发流程中的瓶颈。现有自动代码审查方法忽略了代码变更上下文和历史审查知识等关键信息。

Method: 提出LAURA框架，结合审查示例检索、上下文增强和系统性引导，增强大语言模型（ChatGPT-4o和DeepSeek v3）生成代码审查评论的能力，并构建高质量数据集用于训练与评估。

Result: 实验表明，LAURA在两个模型上分别有42.2%和40.4%的生成评论完全正确或对开发者有帮助，显著优于当前最先进的基线方法；消融研究也验证了各组件对性能提升的积极贡献。

Conclusion: LAURA通过融合审查知识与上下文信息，有效提升了自动生成代码审查的质量，为缓解人工审查瓶颈提供了可行方案。

Abstract: Code review is critical for ensuring software quality and maintainability. With the rapid growth in software scale and complexity, code review has become a bottleneck in the development process because of its time-consuming and knowledge-intensive nature and the shortage of experienced developers willing to review code. Several approaches have been proposed for automatically generating code reviews based on retrieval, neural machine translation, pre-trained models, or large language models (LLMs). These approaches mainly leverage historical code changes and review comments. However, a large amount of crucial information for code review, such as the context of code changes and prior review knowledge, has been overlooked. This paper proposes an LLM-based review knowledge-augmented, context-aware framework for code review generation, named LAURA. The framework integrates review exemplar retrieval, context augmentation, and systematic guidance to enhance the performance of ChatGPT-4o and DeepSeek v3 in generating code review comments. Besides, given the extensive low-quality reviews in existing datasets, we also constructed a high-quality dataset. Experimental results show that for both models, LAURA generates review comments that are either completely correct or at least helpful to developers in 42.2% and 40.4% of cases, respectively, significantly outperforming SOTA baselines. Furthermore, our ablation studies demonstrate that all components of LAURA contribute positively to improving comment quality.

</details>


### [18] [BackportBench: A Multilingual Benchmark for Automated Backporting of Patches](https://arxiv.org/abs/2512.01396)
*Zhiqing Zhong,Jiaming Huang,Pinjia He*

Main category: cs.SE

TL;DR: 本文提出了 BackportBench，这是首个针对补丁回迁（patch backporting）任务的综合性多语言基准测试套件，包含202个来自 PyPI、Maven 和 npm 的回迁问题，并配有可执行的 Docker 环境和测试用例。研究评估了现有补丁迁移方法和基于大语言模型（LLM）的技术，发现基于智能体（agentic）的方法在需要逻辑和结构变更的案例中表现更优，但性能在不同编程语言间存在差异。


<details>
  <summary>Details</summary>
Motivation: 现代软件项目频繁更新以引入新功能和安全补丁，但用户常因升级困难而继续使用存在漏洞的旧版本。手动回迁安全补丁耗时且易错，而现有自动化回迁技术大多局限于代码片段或函数级别，且评估指标不完善，因此亟需一个全面的基准来推动和评估自动化回迁方法的发展。

Method: 作者构建了名为 BackportBench 的多语言基准测试套件，涵盖202个真实世界的补丁回迁问题，每个问题均配备 Docker 执行环境和测试用例。在此基础上，评估了传统补丁迁移方法与基于大语言模型（LLM）的新型技术（尤其是 agentic 方法）在该任务上的表现。

Result: 实验结果表明，agentic 方法在处理需要逻辑和结构性修改的回迁任务上显著优于传统方法，但其在不同编程语言中的表现存在差异。

Conclusion: BackportBench 为自动化补丁回迁研究提供了可靠评估平台；agentic 方法展现出潜力，但仍需针对多语言场景进行优化。研究为未来自动化回迁工具的开发和评估提供了重要启示。

Abstract: Many modern software projects evolve rapidly to incorporate new features and security patches. It is important for users to update their dependencies to safer versions, but many still use older, vulnerable package versions because upgrading can be difficult and may break their existing codebase. Software developers can mitigate this problem by backporting security patches to older releases. However, manually backporting is time-consuming and error-prone. The effectiveness of existing automated backporting techniques on general software remains unclear since they typically target only code-hunk or function-level patch porting scenarios and are evaluated with imperfect metrics.
  To facilitate the development and evaluation of automated backporting techniques, we introduce BackportBench, the first comprehensive benchmark suite for patch backporting problem. BackportBench is a multilingual benchmark that contains 202 patch backporting problems from PyPI, Maven, and npm, each with executable Docker environments and relevant test cases. We evaluated existing patch porting methods and LLM-based techniques that have the potential to adapt to this task using BackportBench. The results show that the agentic method has outperformed traditional patch porting methods, especially on cases that require logical and structural changes. However, the performance varies across different programming languages. Based on the findings, we draw several implications for researchers and software practitioners in future work on automated backporting.

</details>


### [19] [Teaching an Online Multi-Institutional Research Level Software Engineering Course with Industry - an Experience Report](https://arxiv.org/abs/2512.01523)
*Pankaj Jalote,Y. Raghu Reddy,Vasudeva Varma*

Main category: cs.SE

TL;DR: 本文介绍了一种跨校联合开设研究型在线课程“软件工程中的人工智能”的实验，结合产业界参与，旨在为资源有限的小型高校提供可行的研究级教学模式。


<details>
  <summary>Details</summary>
Motivation: 新冠疫情使在线教学被广泛接受，为缺乏足够师资或研究生生源的高校提供了通过多机构合作开设研究级课程的机会，尤其适用于软件工程等与产业紧密相关的领域。

Method: 两所高校联合开设一门名为“AI in Software Engineering”的在线研究级课程，并邀请产业专家积极参与教学与互动。

Result: 课程成功实施，师生及产业参与者均获得良好体验，验证了该协作教学模式的可行性。

Conclusion: 这种多方协作的在线教学模式可推广至其他应用计算机科学领域，帮助小型高校克服资源限制，有效开设研究级课程。

Abstract: Covid has made online teaching and learning acceptable and students, faculty, and industry professionals are all comfortable with this mode. This comfort can be leveraged to offer an online multi-institutional research-level course in an area where individual institutions may not have the requisite faculty to teach and/or research students to enroll. If the subject is of interest to industry, online offering also allows industry experts to contribute and participate with ease. Advanced topics in Software Engineering are ideally suited for experimenting with this approach as industry, which is often looking to incorporate advances in software engineering in their practices, is likely to agree to contribute and participate. In this paper we describe an experiment in teaching a course titled "AI in Software Engineering" jointly between two institutions with active industry participation, and share our and student's experience. We believe this collaborative teaching approach can be used for offering research level courses in any applied area of computer science by institutions who are small and find it difficult to offer research level courses on their own.

</details>


### [20] [OpenDORS: A dataset of openly referenced open research software](https://arxiv.org/abs/2512.01570)
*Stephan Druskat,Lars Grunske*

Main category: cs.SE

TL;DR: 本文发布了一个包含134,352个开源科研软件项目及其在开放获取文献中引用的134,154个代码仓库的大规模数据集，提供版本、许可证、编程语言等元数据，旨在支持科研软件工程（RSE）领域的实证研究。


<details>
  <summary>Details</summary>
Motivation: 尽管科研软件在学术研究中扮演关键角色且科研软件工程（RSE）已成形，但针对科研软件及其开发的大规模实证研究仍显不足，主要受限于缺乏系统性数据集。

Method: 作者构建并发布了一个大规模数据集，该数据集从开放获取文献中提取了科研软件项目及其关联的源代码仓库，并为其中122,425个仓库收集了包括最新版本、许可证、编程语言和描述性元数据文件在内的详细信息。

Result: 数据集成功汇总了科研软件在版本、许可证、编程语言等关键特征上的分布情况，并为未来扩展更多软件元数据奠定了基础。

Conclusion: 该数据集为科研软件工程领域提供了宝贵资源，可支持多种研究以深入理解科研软件的实践现状，从而推动RSE的发展。

Abstract: In many academic disciplines, software is created during the research process or for a research purpose. The crucial role of software for research is increasingly acknowledged. The application of software engineering to research software has been formalized as research software engineering, to create better software that enables better research. Despite this, large-scale studies of research software and its development are still lacking. To enable such studies, we present a dataset of 134,352 unique open research software projects and 134,154 source code repositories referenced in open access literature. Each dataset record identifies the referencing publication and lists source code repositories of the software project. For 122,425 source code repositories, the dataset provides metadata on latest versions, license information, programming languages and descriptive metadata files. We summarize the distributions of these features in the dataset and describe additional software metadata that extends the dataset in future work. Finally, we suggest examples of research that could use the dataset to develop a better understanding of research software practice in RSE research.

</details>


### [21] [GPTrace: Effective Crash Deduplication Using LLM Embeddings](https://arxiv.org/abs/2512.01609)
*Patrick Herter,Vincent Ahlrichs,Ridvan Açilan,Julian Horsch*

Main category: cs.SE

TL;DR: GPTrace 是一种基于大语言模型的崩溃去重方法，通过生成崩溃相关数据的嵌入向量并进行聚类，在超过30万个崩溃输入上的实验表明其优于现有手工设计或复杂但不够灵活的方法。


<details>
  <summary>Details</summary>
Motivation: 模糊测试会产生大量崩溃输入，其中许多共享相同的根本漏洞，人工分析成本高，因此需要有效的崩溃去重技术来减少需审查的数据量。

Method: 提出 GPTrace 流程，利用大语言模型为崩溃相关的多种数据源生成嵌入向量，并将这些向量输入聚类算法以评估相似性，实现崩溃去重。

Result: 在来自14个目标、包含50个真实标签的30多万个崩溃输入上评估，GPTrace 的去重效果明显优于基于栈跟踪的手工方法和更复杂但灵活性较差的前沿方法。

Conclusion: 利用大语言模型生成嵌入向量进行崩溃去重是一种有效且灵活的策略，显著提升了去重准确性，减轻了漏洞分析的人工负担。

Abstract: Fuzzing is a highly effective method for uncovering software vulnerabilities, but analyzing the resulting data typically requires substantial manual effort. This is amplified by the fact that fuzzing campaigns often find a large number of crashing inputs, many of which share the same underlying bug. Crash deduplication is the task of finding such duplicate crashing inputs and thereby reducing the data that needs to be examined. Many existing deduplication approaches rely on comparing stack traces or other information that is collected when a program crashes. Although various metrics for measuring the similarity of such pieces of information have been proposed, many do not yield satisfactory deduplication results. In this work, we present GPTrace, a deduplication workflow that leverages a large language model to evaluate the similarity of various data sources associated with crashes by computing embedding vectors and supplying those as input to a clustering algorithm. We evaluate our approach on over 300 000 crashing inputs belonging to 50 ground truth labels from 14 different targets. The deduplication results produced by GPTrace show a noticeable improvement over hand-crafted stack trace comparison methods and even more complex state-of-the-art approaches that are less flexible.

</details>


### [22] [When High-Performance Computing Meets Software Testing: Distributed Fuzzing using MPI](https://arxiv.org/abs/2512.01617)
*Pierciro Caliandro,Matteo Ciccaglione,Alessandro Pellegrini*

Main category: cs.SE

TL;DR: 该论文提出在分布式模糊测试框架中采用基于MPI的同步技术，以替代传统的基于文件系统的同步方法，从而显著提升性能和覆盖率。


<details>
  <summary>Details</summary>
Motivation: 传统基于文件系统的同步机制在分布式模糊测试中存在通信延迟高、效率低的问题，限制了模糊测试的扩展性和覆盖率增长。

Method: 引入轻量级MPI原语实现分布式模糊节点间的高效通信与输入语料同步，降低通信延迟并协调多个模糊器集群的数据交换。

Result: 实验表明，该方法在标准基准测试中从早期阶段即展现出更快的覆盖率增长，并有效缓解了覆盖率停滞问题，有助于在CI/CD流程中应用。

Conclusion: 基于MPI的同步机制在提升分布式模糊测试的可扩展性与有效性方面具有显著潜力。

Abstract: This paper explores the integration of MPI-based synchronization techniques into distributed fuzzing frameworks, highlighting possible substantial performance improvements compared to traditional filesystem-based synchronization methods. By employing lightweight MPI primitives, reductions in communication latency are achieved, facilitating more efficient data exchanges across distributed fuzzing nodes. Experimental results obtained over standard benchmarks demonstrate enhanced coverage progression from the early stages of the fuzzing process, which could be beneficial if fuzzing is employed in CI/CD pipelines at any stage of software development. Furthermore, the coordinated exchange of input corpora among clusters of fuzzers effectively addresses coverage stagnation, enabling a sustained exploration of complex and deep execution paths. Overall, the adoption of MPI-based synchronization approaches shows promising potential for significantly enhancing the scalability and efficacy of distributed fuzz testing.

</details>


### [23] [Package Dashboard: A Cross-Ecosystem Framework for Dual-Perspective Analysis of Software Packages](https://arxiv.org/abs/2512.01630)
*Ziheng Liu,Runzhi He,Minghui Zhou*

Main category: cs.SE

TL;DR: Package Dashboard 是一个跨生态系统的软件供应链分析框架，通过整合包元数据、漏洞信息和上游社区健康指标，提供统一的双重视角风险评估，提升开源生态系统的透明度、可信度与可追溯性。


<details>
  <summary>Details</summary>
Motivation: 现有软件成分分析（SCA）工具通常局限于单一生态系统，且仅单独评估软件制品或社区活动，导致开发者需手动整合分散数据，影响风险评估效果。

Method: 提出 Package Dashboard 框架，结合依赖解析与仓库分析，集成包元数据、漏洞信息及社区健康指标，在多个生态系统中实现统一的风险评估。

Result: 在涵盖五个 Linux 发行版、374,000 个包的大规模研究中，该框架不仅能识别传统漏洞和许可证冲突，还能发现被忽视的风险（如归档或不可访问的仓库）。

Conclusion: Package Dashboard 提供了统一的风险视图，帮助开发者和 DevSecOps 工程师获得可操作的洞察，从而增强开源软件供应链的透明度、可信度与可追溯性。

Abstract: Software supply chain attacks have revealed blind spots in existing SCA tools, which are often limited to a single ecosystem and assess either software artifacts or community activity in isolation. This fragmentation across tools and ecosystems forces developers to manually reconcile scattered data, undermining risk assessments. We present Package Dashboard, a cross-ecosystem framework that provides a unified platform for supply chain analysis, enabling a holistic, dual-perspective risk assessment by integrating package metadata, vulnerability information, and upstream community health metrics. By combining dependency resolution with repository analysis, it reduces cognitive load and improves traceability. Demonstrating the framework's versatility, a large-scale study of 374,000 packages across five Linux distributions shows its ability to uncover not only conventional vulnerabilities and license conflicts but also overlooked risks such as archived or inaccessible repositories. Ultimately, Package Dashboard provides a unified view of risk, equipping developers and DevSecOps engineers with actionable insights to strengthen the transparency, trustworthiness, and traceability of open-source ecosystems. Package Dashboard is publicly available at https://github.com/n19htfall/PackageDashboard, and a demonstration video can be found at https://youtu.be/y9ncftP8KPQ. Besides, the online version is available at https://pkgdash.osslab-pku.org.

</details>


### [24] [MIT Lincoln Laboratory: A Case Study on Improving Software Support for Research Projects](https://arxiv.org/abs/2512.01649)
*Daniel Strassler,Gabe Elkin,Curran Schiefelbein,Daniel Herring,Ian Jessen,David Johnson,Santiago A. Paredes,Tod Shannon,Jim Flavin*

Main category: cs.SE

TL;DR: MIT Lincoln Laboratory conducted a study to improve research software development effectiveness and culture, identifying key challenges and proposing actionable recommendations such as centralizing tooling, creating a talent-matching database, and forming a software stakeholder panel.


<details>
  <summary>Details</summary>
Motivation: To address challenges in effective and efficient research software development and to strengthen software engineering culture and execution in support of MIT Lincoln Laboratory’s mission.

Method: An internal study within the Homeland Protection and Air Traffic Control Division examined project attributes, centralization opportunities, and staffing/culture issues related to software development.

Result: The study identified three main categories of findings: (1) project attributes affecting software development practices, (2) efficiencies from centralizing resources, and (3) opportunities to enhance staffing and software culture.

Conclusion: Actionable recommendations were delivered, including standardizing software tooling, establishing a common database for matching talent to projects, and creating a stakeholder panel to drive ongoing improvements in software engineering practices.

Abstract: Software plays an ever increasing role in complex system development and prototyping, and in recent years, MIT Lincoln Laboratory has sought to improve both the effectiveness and culture surrounding software engineering in execution of its mission. The Homeland Protection and Air Traffic Control Division conducted an internal study to examine challenges to effective and efficient research software development, and to identify ways to strengthen both the culture and execution for greater impact on our mission. Key findings of this study fell into three main categories: project attributes that influence how software development activities must be conducted and managed, potential efficiencies from centralization, opportunities to improve staffing and culture with respect to software practitioners. The study delivered actionable recommendations, including centralizing and standardizing software support tooling, developing a common database to help match the right software talent and needs to projects, and creating a software stakeholder panel to assist with continued improvement.

</details>


### [25] [Generating REST API Tests With Descriptive Names](https://arxiv.org/abs/2512.01690)
*Philip Garrett,Juan P. Galeotti,Andrea Arcuri,Alexander Poth,Olsi Rrjolli*

Main category: cs.SE

TL;DR: 本文提出并评估了三种新的确定性方法，用于为自动生成的REST API测试用例生成描述性名称。实验表明，一种基于规则的方法在清晰度上优于其他确定性方法，与Gemini和GPT-4o等先进大语言模型效果相当，并显著优于GPT-3.5。工业案例研究进一步验证了该方法能有效提升测试套件的可读性。


<details>
  <summary>Details</summary>
Motivation: 自动生成的API测试用例通常使用非描述性名称（如test0、test1），降低了可读性，影响开发人员对测试的理解与维护。因此，亟需自动化生成更具描述性的测试名称。

Method: 提出了三种新的确定性命名技术，并与包括基于规则启发式和大语言模型（LLM）在内的八种方法进行比较。评估基于EvoMaster模糊器为9个开源API生成的10个测试用例，并通过两项涉及最多39名参与者的调查以及在Volkswagen AG开展的工业案例研究（涵盖4个API、74个测试用例）进行实证分析。

Result: 基于规则的方法在确定性方法中获得最高清晰度评分，性能与Gemini和GPT-4o相当，显著优于GPT-3.5；工业实践反馈也证实其生成的名称有效提升了测试套件的可读性。

Conclusion: 轻量级、确定性的命名技术可作为计算开销大且存在安全风险的大语言模型方法的有效替代方案，在系统级自动化测试命名中具有实用价值，有助于提升API测试的开发者友好性。

Abstract: Automated test generation has become a key technique for ensuring software quality, particularly in modern API-based architectures. However, automatically generated test cases are typically assigned non-descriptive names (e.g., test0, test1), which reduces their readability and hinders their usefulness during comprehension and maintenance. In this work, we present three novel deterministic techniques to generate REST API test names. We then compare eight techniques in total for generating descriptive names for REST API tests automatically produced by the fuzzer EvoMaster, using 10 test cases generated for 9 different open-source APIs. The eight techniques include rule-based heuristics and large language model (LLM)-based approaches. Their effectiveness was empirically evaluated through two surveys (involving up to 39 people recruited via LinkedIn). Our results show that a rule-based approach achieves the highest clarity ratings among deterministic methods, performs on par with state-of-the-art LLM-based models such as Gemini and GPT-4o, and significantly outperforms GPT-3.5.
  To further evaluate the practical impact of our results, an industrial case study was carried out with practitioners who actively use EvoMaster at Volkswagen AG. A developer questionnaire was then carried out based on the use of EvoMaster on four different APIs by four different users, for a total of 74 evaluated test cases. Feedback from practitioners further confirms that descriptive names produced by this approach improve test suite readability.
  These findings highlight that lightweight, deterministic techniques can serve as effective alternatives to computationally expensive and security-sensitive LLM-based approaches for automated system-level test naming, providing a practical step toward more developer-friendly API test generation.

</details>


### [26] [An Empirical Study of Agent Developer Practices in AI Agent Frameworks](https://arxiv.org/abs/2512.01939)
*Yanlin Wang,Xinyi Xu,Jiachi Chen,Tingting Bi,Wenchao Gu,Zibin Zheng*

Main category: cs.SE

TL;DR: 本文首次对基于大语言模型（LLM）的智能体框架进行了实证研究，通过分析11,910条开发者讨论，从开发效率、功能抽象、学习成本、性能优化和可维护性五个维度比较了十个主流框架，揭示了它们在满足开发者需求方面的显著差异，并为未来框架设计提供了建议。


<details>
  <summary>Details</summary>
Motivation: 尽管智能体框架被广泛使用，但其实际应用情况及其对开发过程的影响尚不明确；同时，超过80%的开发者难以选择最适合自己需求的框架，且多个框架面临相似问题，亟需系统性研究以指导改进。

Method: 作者开展了首个针对LLM智能体框架的实证研究，收集并分析了十个代表性框架的11,910条开发者讨论，从开发效率、功能抽象、学习成本、性能优化和可维护性五个维度进行比较分析。

Result: 分析发现不同框架在满足开发者需求方面存在显著差异，在五个评估维度上表现各异，揭示了当前框架设计中的共性问题与改进空间。

Conclusion: 本研究为LLM驱动的智能体框架生态系统提供了实证依据和设计启示，有助于指导未来智能体框架的优化与开发者工具的选择。

Abstract: The rise of large language models (LLMs) has sparked a surge of interest in agents, leading to the rapid growth of agent frameworks. Agent frameworks are software toolkits and libraries that provide standardized components, abstractions, and orchestration mechanisms to simplify agent development. Despite widespread use of agent frameworks, their practical applications and how they influence the agent development process remain underexplored. Different agent frameworks encounter similar problems during use, indicating that these recurring issues deserve greater attention and call for further improvements in agent framework design. Meanwhile, as the number of agent frameworks continues to grow and evolve, more than 80% of developers report difficulties in identifying the frameworks that best meet their specific development requirements. In this paper, we conduct the first empirical study of LLM-based agent frameworks, exploring real-world experiences of developers in building AI agents. To compare how well the agent frameworks meet developer needs, we further collect developer discussions for the ten previously identified agent frameworks, resulting in a total of 11,910 discussions. Finally, by analyzing these discussions, we compare the frameworks across five dimensions: development efficiency, functional abstraction, learning cost, performance optimization, and maintainability, which refers to how easily developers can update and extend both the framework itself and the agents built upon it over time. Our comparative analysis reveals significant differences among frameworks in how they meet the needs of agent developers. Overall, we provide a set of findings and implications for the LLM-driven AI agent framework ecosystem and offer insights for the design of future LLM-based agent frameworks and agent developers.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [27] [VeriPy - A New Python-Based Approach for SDR Pipelined/Unrolled Hardware Accelerator Generation](https://arxiv.org/abs/2512.00006)
*Yuqin Zhao,Linghui Ye,Haihang Xia,Luke Seed,Tiantai Deng*

Main category: cs.AR

TL;DR: 本文提出了一种名为VeriPy的Python高阶综合工具，专为通信工程师设计，无需硬件知识即可生成适用于软件定义无线电（SDR）的Verilog硬件加速器，支持展开和流水线架构，并在性能上优于Vivado HLS。


<details>
  <summary>Details</summary>
Motivation: 软件定义无线电（SDR）应用中广泛使用硬件加速器以提升性能，但通信工程师通常缺乏详细的硬件知识，因此需要一种面向SDR、易用的高阶综合（HLS）工具来降低硬件开发门槛。

Method: 开发了基于Python的HLS工具VeriPy，可自动生成适用于SDR的Verilog硬件加速器（包括展开和流水线结构），无需用户掌握硬件描述语言或底层硬件细节；同时支持自动测试平台生成、可扩展硬件库、性能与资源估算及算法与硬件层面的优化。

Result: VeriPy生成的设计相比经pragma优化的Vivado HLS设计，运行频率最高提升70%，资源消耗略高，但性能和资源使用与手工编写实现相当；且代码更简洁，无需pragma指令，对简单算法输入代码长度与Vivado HLS相当。

Conclusion: VeriPy有效降低了SDR工程师开发硬件加速器的门槛，在保持代码简洁性的同时实现了高性能硬件生成，具备良好的实用性和优化潜力。

Abstract: Software-defined radio (SDR) plays an important role in the communication field by providing a flexible and customized communication system for different purposes according to the needs. To enhance the performance of SDR applications, hardware accelerators have been widely deployed in recent years. In facing this obstacle, a necessity arises for a high-level synthesis (HLS) tool specifically designed for communication engineers without detailed hardware knowledge. To lower the barrier between SDR engineers and hardware development, this work proposed a Python-based HLS tool, VeriPy, which can generate both mainstream architecture for hardware accelerators in Verilog specifically for SDR designs including unrolled design and pipelined design, requiring no detailed digital hardware knowledge or Hardware Description Languages (HDL). Furthermore, VeriPy supports automatic testbench generation with random input stimulus, an extensible hardware library, performance and resource estimation, and offers strong optimisation potential at both the algorithmic and digital hardware levels. The generated hardware design by VeriPy can achieve up to 70% faster operating frequency compared to pragma-optimised Vivado HLS designs with a reasonably higher resource con-sumption while delivering comparable performance and resource consumption to hand-coded implementations. Regarding code complexity, VeriPy requires no pragmas, completely eliminating the need for low-level hardware knowledge. For straightforward algorithms, the input code length remains comparable to that of Vivado HLS.

</details>


### [28] [Architect in the Loop Agentic Hardware Design and Verification](https://arxiv.org/abs/2512.00016)
*Mubarek Mohammed*

Main category: cs.AR

TL;DR: 本文提出了一种结合大语言模型与工程师协同的自动化处理器设计与验证方法，通过智能体将设计任务分解为子模块，生成HDL代码和cocotb测试，并在调试与综合阶段引入人工指导。该方法成功用于设计并实现两款简单处理器（LEGv8 和 RISC-V），每款处理器约消耗一百万推理token，无需专用硬件即可低成本完成，具备良好可扩展性。


<details>
  <summary>Details</summary>
Motivation: 硬件设计日益复杂，现有基于生成式AI的方法多局限于小型组件，缺乏对完整处理器设计流程的自动化支持；同时，硬件设计天然具有层次化与模块化特性，亟需一种能系统利用这一特性的自动化方法。

Method: 采用“人在环路”的智能体架构，结合推理型（如Gemini-Pro）与非推理型（如GPT-5-Mini）大语言模型，根据可选规范将处理器设计分解为子组件，自动生成HDL代码与cocotb测试平台，并在验证、调试和综合阶段引入工程师指导。

Result: 成功设计并验证了两款处理器：一款LEGv8风格处理器已综合并部署到DE-10 Lite FPGA；另一款32位RISC-V风格处理器完成设计与综合但未部署。整个流程每处理器消耗约一百万推理token，成本可控，且方法可扩展至SoC级别。

Conclusion: 所提方法有效结合了生成式AI与工程师专业知识，实现了低成本、可扩展的自动化处理器设计与验证，为未来复杂系统（如SoC）的自动化设计奠定了基础。

Abstract: The ever increasing complexity of the hardware design process demands improved hardware design and verification methodologies. With the advent of generative AI various attempts have been made to automate parts of the design and verification process. Large language models (LLMs) as well as specialized models generate hdl and testbenches for small components, having a few leaf level components. However, there are only a few attempts to automate the entire processor design process. Hardware design demands hierarchical and modular design processes. We utilized this best practice systematically and effectively. We propose agentic automated processor design and verification with engineers in the loop. The agent with optional specification tries to break down the design into sub-components, generate HDL and cocotb tests, and verifies the components involving engineer guidance, especially during debugging and synthesis. We designed various digital systems using this approach. However, we selected two simple processors for demonstration purposes in this work. The first one is a LEGv8 like a simple processor verified, synthesized and programmed for the DE-10 Lite FPGA. The second one is a RISC-V like 32-bit processor designed and verified in similar manner and synthesized. However, it is not programmed into the DE-10 Lite. This process is accomplished usually using around a million inference tokens per processor, using a combination of reasoning (e.g gemini-pro) and non-reasoning models (eg. gpt-5-mini) based on the complexity of the task. This indicates that hardware design and verification experimentation can be done cost effectively without using any specialized hardware. The approach is scalable, we even attempted system-on-chip, which we want to experiment in our future work.

</details>


### [29] [Large Language Model for Verilog Code Generation: Literature Review and the Road Ahead](https://arxiv.org/abs/2512.00020)
*Guang Yang,Wei Zheng,Xiang Chen,Dong Liang,Peng Hu,Yukui Yang,Shaohang Peng,Zhenghan Li,Jiahui Feng,Xiao Wei,Kexin Sun,Deyuan Ma,Haotian Cheng,Yiheng Shen,Xing Hu,Terry Yue Zhuo,David Lo*

Main category: cs.AR

TL;DR: 本文系统综述了基于大语言模型（LLM）的Verilog代码生成研究，涵盖102篇论文，围绕四个关键问题分析了所用模型、数据集、评估指标、技术分类及对齐方法，并指出现有研究的局限性，提出了未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型在Verilog代码生成领域受到广泛关注，但目前尚缺乏对该方向的系统性综述。本文旨在填补这一空白，为自动硬件设计提供全面的研究概览。

Method: 作者对软件工程、人工智能和电子设计自动化领域的会议、期刊及高质量预印本中的102篇相关论文进行了系统性文献综述，围绕四个研究问题进行归纳与分析。

Result: 识别出当前研究所采用的主要大语言模型、常用数据集与评估指标，对Verilog生成技术进行了分类，并分析了针对Verilog的模型对齐方法，同时揭示了现有工作的若干局限性。

Conclusion: 现有基于LLM的Verilog生成研究虽取得进展，但仍存在诸多挑战；文章提出了一条未来研究路线图，以推动LLM在硬件设计自动化中的深入应用。

Abstract: Code generation has emerged as a critical research area at the intersection of Software Engineering (SE) and Artificial Intelligence (AI), attracting significant attention from both academia and industry. Within this broader landscape, Verilog, as a representative hardware description language (HDL), plays a fundamental role in digital circuit design and verification, making its automated generation particularly significant for Electronic Design Automation (EDA). Consequently, recent research has increasingly focused on applying Large Language Models (LLMs) to Verilog code generation, particularly at the Register Transfer Level (RTL), exploring how these AI-driven techniques can be effectively integrated into hardware design workflows. Despite substantial research efforts have explored LLM applications in this domain, a comprehensive survey synthesizing these developments remains absent from the literature. This review fill addresses this gap by providing a systematic literature review of LLM-based methods for Verilog code generation, examining their effectiveness, limitations, and potential for advancing automated hardware design. The review encompasses research work from conferences and journals in the fields of SE, AI, and EDA, encompassing 70 papers published on venues, along with 32 high-quality preprint papers, bringing the total to 102 papers. By answering four key research questions, we aim to (1) identify the LLMs used for Verilog generation, (2) examine the datasets and metrics employed in evaluation, (3) categorize the techniques proposed for Verilog generation, and (4) analyze LLM alignment approaches for Verilog generation. Based on our findings, we have identified a series of limitations of existing studies. Finally, we have outlined a roadmap highlighting potential opportunities for future research endeavors in LLM-assisted hardware design.

</details>


### [30] [Hardware-Aware Neural Network Compilation with Learned Optimization: A RISC-V Accelerator Approach](https://arxiv.org/abs/2512.00031)
*Ravindra Ganti,Steve Xu*

Main category: cs.AR

TL;DR: XgenSilicon ML Compiler 是一个端到端的自动化编译框架，可将高级机器学习模型直接编译为针对定制 ASIC 的优化 RISC-V 汇编代码，在性能、功耗和面积（PPA）方面显著优于通用组件和手工设计芯片。


<details>
  <summary>Details</summary>
Motivation: 现有机器学习部署流程在软硬件协同优化方面存在割裂，导致 ASIC 实现难以兼顾高性能、低功耗与小面积；同时，缺乏全自动化的工具链使得从模型到芯片的流程依赖大量人工干预。

Method: 该编译器通过五大创新实现软硬统一优化：(1) 多算法自动调优框架结合五种搜索策略与学习型代价模型；(2) 支持 FP32 到二值化的集成量化框架，包含 KL 散度校准与动量 QAT；(3) 硬件感知验证确保 ISA 合规与内存约束；(4) 动态形状支持与多配置特化；(5) 面向多级缓存的高级代价建模。

Result: 评估显示，由该编译器生成的 ASIC 在性能上提升 2.5–4.5 倍，功耗降低 3–6 倍，面积减少 40–60%；支持 100 多个 ONNX 算子、RISC-V Vector 扩展，并能生成可直接用于 ASIC 合成的硬件验证汇编代码。

Conclusion: XgenSilicon ML Compiler 实现了从模型到 ASIC 的全自动、高性能、高能效编译流程，显著提升了定制芯片的 PPA 表现，具备广泛的工业应用潜力。

Abstract: We present XgenSilicon ML Compiler, a fully automated end-to-end compilation framework that transforms high-level machine learning models into optimized RISC-V assembly code for custom ASIC accelerators. By unifying the system's cost model across software and hardware, the compiler achieves significant improvements in Power, Performance, and Area (PPA) metrics compared to standard off-the-shelf components and hand-designed chips through five key innovations: (1) a multi-algorithm auto-tuning framework with five search strategies (Bayesian Optimization, Genetic Algorithm, Simulated Annealing, Random Search, Grid Search) combined with a learned cost model, (2) an integrated quantization framework supporting extreme precisions from FP32 to Binary with full KL divergence calibration (2048-bin histogram optimization) and momentum-based QAT gradient updates, (3) hardware-aware validation ensuring 100 percent ISA compliance and memory constraint satisfaction, (4) dynamic shape support with multi-configuration specialization, and (5) advanced cache-aware cost modeling with multi-level cache hierarchy analysis. Our evaluation demonstrates that ASICs produced by this compiler achieve 2.5-4.5x better performance, 3-6x lower power consumption, and 40-60 percent area reduction compared to baseline implementations. The compiler supports more than 100 ONNX operators across 12 categories, implements advanced RISC-V Vector optimizations, and generates hardware-validated assembly code suitable for direct ASIC synthesis. All compilation steps are fully automated, requiring zero manual intervention from model input to ASIC-ready output.

</details>


### [31] [Decoupled Control Flow and Data Access in RISC-V GPGPUs](https://arxiv.org/abs/2512.00032)
*Giuseppe M. Sarda,Nimish Shah,Abubakr Nada,Debjyoti Bhattacharjee,Marian Verhelst*

Main category: cs.AR

TL;DR: 本文针对开源RISC-V GPGPU平台Vortex在控制流和内存访问方面的性能瓶颈，提出通过硬件控制流管理器和解耦内存流通道两项微架构改进，显著提升其性能，使其更适用于机器学习等领域的GPGPU研究。


<details>
  <summary>Details</summary>
Motivation: Vortex作为一个新兴的开源GPGPU平台，虽然为研究提供了新方向，但其性能尚无法与商用GPU竞争，主要受限于缺乏处理控制流管理和内存编排开销的复杂架构特性，尤其在内存密集型核心（如线性代数）中表现明显。

Method: 论文提出了两种简单而有效的微架构修改：1）引入硬件控制流管理器以加速规则循环执行中的分支和谓词操作；2）采用解耦的内存流通道，以在等待内存时进行有用计算，从而隐藏内存延迟。

Result: 评估结果显示，改进后的Vortex在不同核心上实现了8倍的执行速度提升、动态指令数减少10倍，整体性能从0.35 GFLOP/s/mm²提升至1.63 GFLOP/s/mm²。

Conclusion: 通过所提出的微架构增强，Vortex的性能得到显著提升，有望成为下一代机器学习GPGPU研究的理想平台。

Abstract: Vortex, a newly proposed open-source GPGPU platform based on the RISC-V ISA, offers a valid alternative for GPGPU research over the broadly-used modeling platforms based on commercial GPUs. Similarly to the push originating from the RISC-V movement for CPUs, Vortex can enable a myriad of fresh research directions for GPUs. However, as a young hardware platform, it currently lacks the performance competitiveness of commercial GPUs, which is crucial for widespread adoption. State-of-the-art GPUs, in fact, rely on complex architectural features, still unavailable in Vortex, to hide the micro-code overheads linked to control flow (CF) management and memory orchestration for data access. In particular, these components account for the majority of the dynamic instruction count in regular, memory-intensive kernels, such as linear algebra routines, which form the basis of many applications, including Machine Learning. To address these challenges with simple yet powerful micro-architecture modifications, this paper introduces decoupled CF and data access through 1.) a hardware CF manager to accelerate branching and predication in regular loop execution and 2.) decoupled memory streaming lanes to further hide memory latency with useful computation. The evaluation results for different kernels show 8$\times$ faster execution, 10$\times$ reduction in dynamic instruction count, and overall performance improvement from 0.35 to 1.63 $\mathrm{GFLOP/s/mm^2}$. Thanks to these enhancements, Vortex can become an ideal playground to enable GPGPU research for the next generation of Machine Learning.

</details>


### [32] [LLaMCAT: Optimizing Large Language Model Inference with Cache Arbitration and Throttling](https://arxiv.org/abs/2512.00083)
*Zhongchun Zhou,Chengtao Lai,Wei Zhang*

Main category: cs.AR

TL;DR: LLaMCAT 是一种针对大语言模型（LLM）推理中末级缓存（LLC）优化的新方法，通过结合 MSHR 感知与负载均衡的缓存仲裁机制及线程节流策略，有效缓解 KV Cache 访问中的带宽压力和缓存停顿。实验表明，LLaMCAT 在不同瓶颈场景下相比基线方法可实现最高 1.58 倍的加速。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在推理阶段对内存系统提出了极高要求，尤其在末级缓存架构（如 GPU 和 AI 加速器）中，KV Cache 的频繁访问导致严重的带宽竞争和缓存停顿，而现有方法未专门针对 LLM 解码阶段的 MSHR 争用问题进行优化。

Method: 提出 LLaMCAT 方法，结合 Miss Status Holding Register (MSHR) 感知与负载均衡的缓存仲裁机制，并引入线程节流策略；同时构建一个融合分析模型与周期级模拟器的混合仿真框架，通过内存轨迹平衡架构细节与仿真效率。

Result: 当系统主要受限于缺失处理吞吐量时，LLaMCAT 平均加速比达 1.26 倍；在缓存容量受限情况下，相比未优化版本提速 1.58 倍，优于最佳基线方法 dyncta 达 1.26 倍。

Conclusion: LLaMCAT 首次聚焦于 LLM 解码阶段特有的 MSHR 争用问题，填补了此前研究空白，为未来硬件平台上的 LLM 推理加速提供了实用解决方案。

Abstract: Large Language Models (LLMs) have achieved unprecedented success across various applications, but their substantial memory requirements pose significant challenges to current memory system designs, especially during inference. Our work targets last-level cache (LLC) based architectures, including GPUs (e.g., NVIDIA GPUs) and AI accelerators. We introduce LLaMCAT, a novel approach to optimize the LLC for LLM inference. LLaMCAT combines Miss Status Holding Register (MSHR)- and load balance-aware cache arbitration with thread throttling to address stringent bandwidth demands and minimize cache stalls in KV Cache access. We also propose a hybrid simulation framework integrating analytical models with cycle-level simulators via memory traces, balancing architecture detail and efficiency.
  Experiments demonstrate that LLaMCAT achieves an average speedup of 1.26x when the system is mainly bottlenecked by miss handling throughput, while baselines mostly show negative improvements since they are not optimized for this scenario. When the cache size is also limited, our policy achieves a speedup of 1.58x over the unoptimized version, and a 1.26x improvement over the best baseline (dyncta). Overall, LLaMCAT is the first to target LLM decoding-specific MSHR contention, a gap in previous work. It presents a practical solution for accelerating LLM inference on future hardware platforms.

</details>


### [33] [SetupKit: Efficient Multi-Corner Setup/Hold Time Characterization Using Bias-Enhanced Interpolation and Active Learning](https://arxiv.org/abs/2512.00044)
*Junzhuo Zhou,Ziwen Wang,Haoxuan Xia,Yuxin Yan,Chengyu Zhu,Ting-Jung Lin,Wei Xing,Lei He*

Main category: cs.AR

TL;DR: SetupKit 是一种结合统计建模、电路分析与主动学习的新框架，用于加速芯片库单元在多工艺-电压-温度（PVT）角下的建立/保持时间表征，相比传统方法减少2.4倍CPU时间。


<details>
  <summary>Details</summary>
Motivation: 现代芯片时序收敛依赖大量SPICE仿真进行建立/保持时间表征，在多PVT角下耗时极长（数周至数月），而现有方法收敛慢、探索效率低，亟需更高效的解决方案。

Method: 提出SetupKit框架，包含三项核心技术：基于统计误差建模的偏置增强插值搜索（BEIRA）、基于电路分析的初始搜索区间估计，以及利用高斯过程的主动学习策略，以智能选择最具信息量的PVT角进行仿真。

Result: 在22nm工业标准单元、16个PVT角的实验中，SetupKit将单核CPU总运行时间从720天降至290天，提速2.4倍，显著缩短表征周期。

Conclusion: SetupKit提供了一种基于学习的高效库表征方法，有效缓解EDA中的关键瓶颈，为智能化仿真管理开辟了新路径。

Abstract: Accurate setup/hold time characterization is crucial for modern chip timing closure, but its reliance on potentially millions of SPICE simulations across diverse process-voltagetemperature (PVT) corners creates a major bottleneck, often lasting weeks or months. Existing methods suffer from slow search convergence and inefficient exploration, especially in the multi-corner setting. We introduce SetupKit, a novel framework designed to break this bottleneck using statistical intelligence, circuit analysis and active learning (AL). SetupKit integrates three key innovations: BEIRA, a bias-enhanced interpolation search derived from statistical error modeling to accelerate convergence by overcoming stagnation issues, initial search interval estimation by circuit analysis and AL strategy using Gaussian Process. This AL component intelligently learns PVT-timing correlations, actively guiding the expensive simulations to the most informative corners, thus minimizing redundancy in multicorner characterization. Evaluated on industrial 22nm standard cells across 16 PVT corners, SetupKit demonstrates a significant 2.4x overall CPU time reduction (from 720 to 290 days on a single core) compared to standard practices, drastically cutting characterization time. SetupKit offers a principled, learningbased approach to library characterization, addressing a critical EDA challenge and paving the way for more intelligent simulation management.

</details>


### [34] [Assessing Large Language Models in Generating RTL Design Specifications](https://arxiv.org/abs/2512.00045)
*Hung-Ming Huang,Yu-Hsin Yang,Fu-Chieh Chang,Yun-Chia Hsu,Yin-Yu Lin,Ming-Fang Tsai,Chun-Chih Yang,Pei-Yuan Wu*

Main category: cs.AR

TL;DR: 本文研究了在IC设计中利用大语言模型（LLM）自动生成RTL代码对应规格说明的方法，探讨了不同提示策略对生成质量的影响，并提出了评估生成规格可靠性的新指标，同时对开源和商用LLM进行了基准测试。


<details>
  <summary>Details</summary>
Motivation: 随着IC设计日益复杂，手动解读RTL代码并撰写规格说明既耗时又易出错；尽管已有研究关注从规格生成RTL，但自动生成规格仍缺乏有效评估手段，亟需系统性探索。

Method: 通过分析不同提示（prompting）策略对RTL到规格生成质量的影响，提出用于忠实评估生成规格的新指标，并对多种开源与商业大语言模型进行基准测试。

Result: 揭示了提示策略对生成质量的显著影响，所提出的评估指标能有效衡量生成规格的准确性，并提供了不同LLM在该任务上的性能对比结果。

Conclusion: 本研究为IC设计中实现更自动化、高效的规格生成流程奠定了基础，推动了LLM在硬件文档自动化中的应用。

Abstract: As IC design grows more complex, automating comprehension and documentation of RTL code has become increasingly important. Engineers currently should manually interpret existing RTL code and write specifications, a slow and error-prone process. Although LLMs have been studied for generating RTL from specifications, automated specification generation remains underexplored, largely due to the lack of reliable evaluation methods. To address this gap, we investigate how prompting strategies affect RTL-to-specification quality and introduce metrics for faithfully evaluating generated specs. We also benchmark open-source and commercial LLMs, providing a foundation for more automated and efficient specification workflows in IC design.

</details>


### [35] [A Configurable Mixed-Precision Fused Dot Product Unit for GPGPU Tensor Computation](https://arxiv.org/abs/2512.00053)
*Nikhil Rout,Blaise Tine*

Main category: cs.AR

TL;DR: 本文提出了一种可扩展的混合精度点积单元，通过在统一融合架构中集成浮点与整数运算流水线，显著提升了GPGPU上深度学习任务的吞吐量和资源利用率，并在开源RISC-V Vortex GPGPU的Tensor Core Unit扩展中实现。


<details>
  <summary>Details</summary>
Motivation: 现有开源RTL实现的内积运算依赖离散的算术单元，导致吞吐量不足和资源利用效率低下，难以满足深度学习对高效混合精度矩阵乘加（MMA）操作的需求。

Method: 设计并实现了一个融合浮点与整数运算流水线的单一架构混合精度点积单元，支持多种低精度格式（FP16/BF16/FP8/BF8/INT8/UINT4）乘法与高精度（FP32/INT32）累加，并具备可扩展性以支持未来自定义数据格式。

Result: 在AMD Xilinx Alveo U55C FPGA上实现4周期操作延迟，时钟频率达306.6 MHz，在每warp 4线程配置下达到9.812 GFLOPS的理想满流水吞吐量。

Conclusion: 所提出的融合架构有效解决了现有实现的性能瓶颈，为开源GPGPU提供了高效、灵活且可扩展的混合精度计算解决方案。

Abstract: Efficient mixed-precision MMA operations are critical for accelerating Deep Learning workloads on GPGPUs. However, existing open-source RTL implementations of inner dot products rely on discrete arithmetic units, leading to suboptimal throughput and poor resource utilization. To address these challenges, we propose a scalable mixed-precision dot product unit that integrates floating-point and integer arithmetic pipelines within a singular fused architecture, implemented as part of the open-source RISC-V based Vortex GPGPU's Tensor Core Unit extension. Our design supports low-precision multiplication in (FP16/BF16/FP8/BF8/INT8/UINT4) formats and higher-precision accumulation in (FP32/INT32), with an extensible framework for adding and evaluating other custom representations in the future. Experimental results demonstrate 4-cycle operation latency at 306.6 MHz clock frequency on the AMD Xilinx Alveo U55C FPGA, delivering an ideal filled pipeline throughput of 9.812 GFLOPS in a 4-thread per warp configuration.

</details>


### [36] [Modeling and Simulation Frameworks for Processing-in-Memory Architectures](https://arxiv.org/abs/2512.00096)
*Mahdi Aghaei,Saba Ebrahimi,Mohammad Saleh Arafati,Elham Cheshmikhani,Dara Rahmati,Saeid Gorgin,Jungrae Kim*

Main category: cs.AR

TL;DR: 本文综述了面向存内计算（PIM）的仿真方法与工具，按抽象层级、设计目标和评估指标进行分类，并讨论了常用基准测试套件及仿真方法中的开放挑战。


<details>
  <summary>Details</summary>
Motivation: 为应对冯·诺依曼架构中的“内存墙”问题，PIM成为有前景的计算范式；而仿真在评估和比较不同PIM设计方案中起关键作用，因此需系统梳理现有仿真工具及其适用场景。

Method: 对过去十年提出的各类PIM仿真器进行分类综述，依据抽象层级、设计目标和评估指标组织内容，并分析其在保真度、可扩展性、支持技术及基准兼容性等方面的差异。

Result: 归纳出代表性PIM仿真工具的特点，提供选择指南，并总结常用基准测试套件，指出当前仿真方法中存在的开放性挑战。

Conclusion: 全面理解PIM仿真工具的权衡有助于研究者准确映射和验证其工作，未来需发展更可靠、可扩展且高效的PIM建模方法。

Abstract: Processing-in-Memory (PIM) has emerged as a promising computing paradigm to address the memory wall and the fundamental bottleneck of the von Neumann architecture by reducing costly data movement between memory and processing units. As with any engineering challenge, identifying the most effective solutions requires thorough exploration of diverse architectural proposals, device technologies, and application domains. In this context, simulation plays a critical role in enabling researchers to evaluate, compare, and refine PIM designs prior to fabrication. Over the past decade, a variety of PIM simulators have been introduced, spanning low-level device models, architectural frameworks, and application-oriented environments. These tools differ significantly in fidelity, scalability, supported memory/compute technologies, and benchmark compatibility. Understanding these trade-offs is essential for researchers to select appropriate simulators that accurately map and validate their research efforts. This chapter provides a comprehensive overview of PIM simulation methodologies and tools. We categorize simulators according to abstraction levels, design objectives, and evaluation metrics, highlighting representative examples. To improve accessibility, some content may appear in multiple contexts to guide readers with different backgrounds. We also survey benchmark suites commonly employed in PIM studies and discuss open challenges in simulation methodology, paving the way for more reliable, scalable, and efficient PIM modeling.

</details>


### [37] [An Analytical and Empirical Investigation of Tag Partitioning for Energy-Efficient Reliable Cache](https://arxiv.org/abs/2512.00112)
*Elham Cheshmikhani,Hamed Farbeh*

Main category: cs.AR

TL;DR: 本文提出了一种分析方法，用于确定缓存标签分区中的最优分割点k，以最大化降低标签读取能耗并提升可靠性，并通过实验验证了该方法的准确性。


<details>
  <summary>Details</summary>
Motivation: 由于缓存占据芯片面积的一半以上且易受故障影响，而标签阵列作为最活跃且关键的部分，其能耗和错误率尤为突出。现有标签分区方法在选择分割点k时缺乏理论依据，导致效果不稳定且难以泛化。

Method: 作者通过分析推导出一个基于缓存配置参数的凸且可微的公式，用于精确量化任意k值下的标签分区效率，并据此确定最优分割点k。

Result: 在多种缓存设计上进行的实验表明，所提出的分析模型与实际结果高度一致，能够准确预测最优k值及标签读取减少量。

Conclusion: 该研究为缓存标签分区提供了理论基础，使设计者能快速计算最优分割点并准确评估节能效果，解决了以往方法缺乏通用性和理论支撑的问题。

Abstract: Associative cache memory significantly influences processor performance and energy consumption. Because it occupies over half of the chip area, cache memory is highly susceptible to transient and permanent faults, posing reliability challenges. As the only hardware-managed memory module, the cache tag array is the most active and critical component, dominating both energy usage and error rate. Tag partitioning is a widely used technique to reduce tag-access energy and enhance reliability. It divides tag comparison into two phases: first comparing the k lower bits, and then activating only the matching tag entries to compare the remaining higher bits. The key design parameter is the selection of the tag-splitting point k, which determines how many reads are eliminated. However, prior studies have chosen k intuitively, randomly, or empirically, without justification. Even experimentally determined values are ad-hoc and do not generalize across cache configurations due to high sensitivity to architectural parameters.
  In this paper, we analytically show that choosing k too large or too small substantially reduces the effectiveness of tag partitioning. We then derive a formulation that determines the optimal splitting point based on cache configuration parameters. The formulation is convex, differentiable, and capable of precisely quantifying tag-partitioning efficiency for any k and configuration. To validate our model, we experimentally evaluate tag-partitioning efficiency and optimal k across a broad set of cache designs and demonstrate close agreement between analytical and experimental results. The proposed formulation enables designers and researchers to instantly compute the optimal tag-splitting point and accurately estimate tag-read reduction.

</details>


### [38] [Variable Point: A Number Format for Area- and Energy-Efficient Multiplication of High-Dynamic-Range Numbers](https://arxiv.org/abs/2512.00186)
*Seyed Hadi Mirfarshbafan,Nicolas Filliol,Oscar Castañeda,Christoph Studer*

Main category: cs.AR

TL;DR: 本文提出了一种名为可变点（VP）的新数值格式，在保持硬件复杂度不显著增加的前提下，相比定点数具有更大的动态范围，并在多天线无线通信系统的矩阵-向量乘法引擎中验证了其有效性，实现了面积和功耗分别节省20%和10%。


<details>
  <summary>Details</summary>
Motivation: 定点数在给定位宽下动态范围较小，而浮点数虽动态范围大但硬件复杂度高。为在不显著增加硬件复杂度的情况下提升动态范围，作者提出了可变点（VP）数值格式。

Method: 提出一种新的可变点（VP）数值表示方法，并将其应用于多天线无线通信系统中用于空间均衡的矩阵-向量乘法引擎；通过布局后VLSI实现进行评估。

Result: 与完全优化的定点设计相比，基于VP的设计在无明显性能下降的情况下，实现了20%的面积节省和10%的功耗降低。

Conclusion: 所提出的VP数值格式在处理高动态范围信号时，能够在几乎不增加硬件复杂度的前提下，有效提升硬件效率，优于传统定点实现。

Abstract: Fixed-point number representation is commonly employed in digital VLSI designs that have stringent hardware efficiency constraints. However, fixed-point numbers cover a relatively small dynamic range for a given bitwidth. In contrast, floating-point numbers offer a larger dynamic range at the cost of increased hardware complexity. In this paper, we propose a novel number format called variable-point (VP). VP numbers cover a larger dynamic range than fixed-point numbers with similar bitwidth, without notably increasing hardware complexity -- this allows for a more efficient representation of signals with high dynamic range. To demonstrate the efficacy of the proposed VP number format, we consider a matrix-vector multiplication engine for spatial equalization in multi-antenna wireless communication systems involving high-dynamic-range signals. Through post-layout VLSI implementation results, we demonstrate that the proposed VP-based design achieves 20% and 10% area and power savings, respectively, compared to a fully optimized fixed-point design, without incurring any noticeable performance degradation.

</details>


### [39] [Efficient Kernel Mapping and Comprehensive System Evaluation of LLM Acceleration on a CGLA](https://arxiv.org/abs/2512.00335)
*Takuto Ando,Yu Eto,Ayumu Takeuchi,Yasuhiko Nakashima*

Main category: cs.AR

TL;DR: 本文首次对非AI专用的粗粒度线性阵列（CGLA）加速器在Qwen大语言模型上的端到端性能进行了全面评估，结果表明其在能效方面显著优于高端GPU和边缘AI设备，尤其在功耗延迟积（PDP）和能量延迟积（EDP）指标上表现突出。


<details>
  <summary>Details</summary>
Motivation: 大语言模型（LLMs）在GPU上运行时能耗高，亟需更节能的硬件替代方案。作者旨在探索粗粒度可重构阵列（CGRAs）作为兼顾能效与可编程性的可行平台，用于可持续的LLM推理。

Method: 作者基于通用、任务无关的CGLA架构，在FPGA原型上使用llama.cpp框架进行评估，并将其性能投影至28nm ASIC工艺，与NVIDIA RTX 4090和Jetson AGX Orin进行对比分析，同时开展系统级瓶颈识别。

Result: 尽管GPU延迟更低，但该非AI专用加速器在能效方面显著领先：相比RTX 4090和Jetson，PDP分别提升最多44.4倍和13.6倍；相比高端GPU，EDP降低最多11.5倍。研究还发现主机-加速器间的数据传输是主要性能瓶颈。

Conclusion: CGRAs是一种适用于功率受限环境下LLM推理的有效平台，无需针对特定算法定制即可实现优异的性能-能效平衡，为下一代LLM加速器设计提供了重要指导。

Abstract: Large Language Models (LLMs) demand substantial computational resources, resulting in high energy consumption on GPUs. To address this challenge, we focus on Coarse-Grained Reconfigurable Arrays (CGRAs) as an effective alternative that provides a trade-off between energy efficiency and programmability. This paper presents the first comprehensive, end-to-end evaluation of a non-AI-specialized Coarse-Grained Linear Array (CGLA) accelerator for the state-of-the-art Qwen LLM family. The architecture has a general-purpose, task-agnostic design, yet its flexible instruction set allows for domain-specific adaptations. This flexibility enables the architecture to achieve high efficiency for sustainable LLM inference. We assess the performance of our architecture on an FPGA prototype using the widely adopted llama.cpp framework. We then project its potential as a 28nm ASIC and compare it against a high-performance GPU (NVIDIA RTX 4090) and an edge AI device (NVIDIA Jetson AGX Orin). While GPUs exhibit lower latency, our non-AI-specific accelerator achieves higher energy efficiency, improving the Power-Delay Product (PDP) by up to 44.4x and 13.6x compared with the RTX 4090 and Jetson, respectively. Similarly, it reduces the Energy-Delay Product (EDP) by up to 11.5x compared to the high-performance GPU, demonstrating a favorable performance-energy trade-off. Critically, our system-level analysis identifies host-accelerator data transfer as the primary performance bottleneck, a factor often overlooked in kernel-level studies. These findings provide design guidance for next-generation LLM accelerators. This work validates CGRAs as a suitable platform for LLM inference in power-constrained environments, without being confined to specific algorithms.

</details>


### [40] [A Novel 8T SRAM-Based In-Memory Computing Architecture for MAC-Derived Logical Functions](https://arxiv.org/abs/2512.00441)
*Amogh K M,Sunita M S*

Main category: cs.AR

TL;DR: 本文提出了一种基于8T SRAM的存内计算架构，支持多比特并行乘累加（MAC）和逻辑运算，克服了传统6T SRAM设计的可靠性问题，在90 nm工艺下实现了高性能低功耗操作。


<details>
  <summary>Details</summary>
Motivation: 传统6T SRAM存内计算架构存在读写路径耦合导致的可靠性问题，限制了其在高精度计算中的应用；因此需要一种更可靠且兼容现有SRAM技术的新型架构。

Method: 采用8×8的8T SRAM阵列，通过专用读位线上的电荷共享机制实现并行MAC运算与常规存储操作，并引入新颖的模数解码方案将模拟电压输出转换为数字信号，从而在同一阵列中实现多种基本逻辑功能。

Result: 在90 nm CMOS工艺、1.8 V供电条件下，该设计以142.85 MHz频率运行，MAC操作延迟为0.7 ns，每比特能耗为56.56 fJ，吞吐率达15.8 M ops/s，并成功实现8比特MAC及多种逻辑运算。

Conclusion: 所提出的8T SRAM存内计算架构有效解决了传统设计的可靠性瓶颈，兼具高性能与低功耗优势，展示了在存算一体系统中的实用潜力。

Abstract: This paper presents an in-memory computing (IMC) architecture developed on an 8x8 array of 8T SRAM cells. This architecture enables both multi-bit parallel Multiply-Accumulate (MAC) operations and standard memory processing through charge-sharing on dedicated read bit-lines. By leveraging the maturity of SRAM technology, this work introduces an 8T SRAM-based IMC architecture that decouples read and write paths, thereby overcoming the reliability limitations of prior 6T SRAM designs. A novel analog-to-digital decoding scheme converts the MAC voltage output into digital counts, which are subsequently interpreted to realize fundamental logic functions including AND/NAND, NOR/OR, XOR/XNOR, and 1-bit addition within the same array. Simulated in a 90 nm CMOS process at 1.8 V supply voltage, the proposed design achieves 8-bit MAC and logical operations at a frequency of 142.85 MHz, with a latency of 0.7 ns and energy consumption of 56.56 fJ/bit per MAC operation and throughput of 15.8 M operations/s.

</details>


### [41] [Leveraging Recurrent Patterns in Graph Accelerators](https://arxiv.org/abs/2512.01193)
*Masoud Rahimi,Sébastien Le Beux*

Main category: cs.AR

TL;DR: 本文提出一种通过识别频繁子图模式并将其分配至静态图引擎的图处理方法，以减少ReRAM交叉开关的重配置需求，从而显著提升性能、能效并延长电路寿命。


<details>
  <summary>Details</summary>
Motivation: 现有基于ReRAM交叉开关的图加速器因图划分数量庞大导致忆阻器访问开销高，引发执行时间增加、能耗上升和电路寿命缩短等问题。

Method: 通过识别频繁出现的子图模式，并将这些子图分配给称为“静态”的图引擎进行处理，从而避免大多数子图处理过程中对交叉开关进行重新配置，减少忆阻器写操作。

Result: 实验结果表明，与当前最先进的加速器相比，该方法实现了最高2.38倍的速度提升、7.23倍的能耗节省，并将电路寿命延长了2倍。

Conclusion: 所提出的图处理方法有效缓解了ReRAM图加速器中的忆阻器访问开销问题，在性能、能效和硬件耐久性方面均取得显著改进。

Abstract: Graph accelerators have emerged as a promising solution for processing large-scale sparse graphs, leveraging the in-situ compu-tation of ReRAM-based crossbars to maximize computational efficiency. However, existing designs suffer from memristor access overhead due to the large number of graph partitions. This leads to increased execution time, higher energy consumption, and re-duced circuit lifetime. This paper proposes a graph processing method that minimizes memristor write operations by identifying frequent subgraph patterns and assigning them to graph engines, referred to as static, allowing most subgraphs to be processed without a need for crossbar reconfiguration. Experimental results show speed up to 2.38x speedup and 7.23x energy savings com-pared to state-of-the-art accelerators. Furthermore, our method extends the circuit lifetime by 2x compared to state-of-the-art ReRAM graph accelerators.

</details>


### [42] [hls4ml: A Flexible, Open-Source Platform for Deep Learning Acceleration on Reconfigurable Hardware](https://arxiv.org/abs/2512.01463)
*Jan-Frederik Schulte,Benjamin Ramhorst,Chang Sun,Jovan Mitrevski,Nicolò Ghielmetti,Enrico Lupi,Dimitrios Danopoulos,Vladimir Loncar,Javier Duarte,David Burnette,Lauri Laatu,Stylianos Tzelepis,Konstantinos Axiotis,Quentin Berthet,Haoyan Wang,Paul White,Suleyman Demirsoy,Marco Colombo,Thea Aarrestad,Sioni Summers,Maurizio Pierini,Giuseppe Di Guglielmo,Jennifer Ngadiuba,Javier Campos,Ben Hawks,Abhijith Gandrakota,Farah Fahim,Nhan Tran,George Constantinides,Zhiqiang Que,Wayne Luk,Alexander Tapper,Duc Hoang,Noah Paladino,Philip Harris,Bo-Cheng Lai,Manuel Valentin,Ryan Forelli,Seda Ogrenci,Lino Gerlach,Rian Flynn,Mia Liu,Daniel Diaz,Elham Khoda,Melissa Quinnan,Russell Solares,Santosh Parajuli,Mark Neubauer,Christian Herwig,Ho Fung Tsoi,Dylan Rankin,Shih-Chieh Hsu,Scott Hauck*

Main category: cs.AR

TL;DR: hls4ml 是一个开源平台，可将主流深度学习框架中的机器学习模型自动转换为适用于 FPGA 或 ASIC 的高级综合（HLS）代码，支持多种 HLS 编译器，并已在多个对低延迟、资源占用和功耗敏感的商业与科研场景中实现部署。


<details>
  <summary>Details</summary>
Motivation: 在需要低延迟、低功耗和高效资源利用的场景中，将机器学习模型部署到 FPGA 或 ASIC 上具有重要意义，但缺乏通用、灵活的工具链来简化从深度学习模型到硬件实现的转换过程。

Method: 开发了 hls4ml 平台，该平台采用模块化架构，能够从多种深度学习框架中读取模型，并生成兼容多个厂商 HLS 编译器（如 Vitis HLS、Intel oneAPI、Catapult HLS）的代码，同时融入软硬件协同设计生态。

Result: hls4ml 已成功应用于多个实际场景，展示了其在加速机器学习推理方面的有效性，并提供了对生成 HLS 代码的关键设计考量及部分性能评估结果。

Conclusion: hls4ml 提供了一个灵活、开放且可扩展的解决方案，有效弥合了机器学习模型与定制硬件实现之间的鸿沟，推动了 ML 在边缘和嵌入式系统中的部署。

Abstract: We present hls4ml, a free and open-source platform that translates machine learning (ML) models from modern deep learning frameworks into high-level synthesis (HLS) code that can be integrated into full designs for field-programmable gate arrays (FPGAs) or application-specific integrated circuits (ASICs). With its flexible and modular design, hls4ml supports a large number of deep learning frameworks and can target HLS compilers from several vendors, including Vitis HLS, Intel oneAPI and Catapult HLS. Together with a wider eco-system for software-hardware co-design, hls4ml has enabled the acceleration of ML inference in a wide range of commercial and scientific applications where low latency, resource usage, and power consumption are critical. In this paper, we describe the structure and functionality of the hls4ml platform. The overarching design considerations for the generated HLS code are discussed, together with selected performance results.

</details>


### [43] [RoMe: Row Granularity Access Memory System for Large Language Models](https://arxiv.org/abs/2512.01541)
*Hwayong Nam,Seungmin Baek,Jumin Kim,Michael Jaemin Kim,Jung Ho Ahn*

Main category: cs.AR

TL;DR: RoMe is a novel HBM-based memory system that replaces fine-grained cache line accesses with row-granularity accesses to simplify memory scheduling and increase bandwidth for large language model (LLM) workloads.


<details>
  <summary>Details</summary>
Motivation: Modern HBM systems retain cache-line granularity, which introduces complex structures like bank groups and pseudo channels, increasing scheduling complexity—especially inefficient for LLMs that access large contiguous data blocks.

Method: RoMe accesses DRAM at row granularity and eliminates columns, bank groups, and pseudo channels from the memory interface, simplifying scheduling and repurposing freed pins to add more channels.

Result: RoMe increases overall memory bandwidth by 12.5% with minimal additional pin overhead while significantly simplifying memory controller scheduling for LLM workloads.

Conclusion: RoMe demonstrates that redesigning HBM interfaces around row-granularity access can better match LLM memory access patterns, offering higher bandwidth and simpler control logic with minimal hardware changes.

Abstract: Modern HBM-based memory systems have evolved over generations while retaining cache line granularity accesses. Preserving this fine granularity necessitated the introduction of bank groups and pseudo channels. These structures expand timing parameters and control overhead, significantly increasing memory controller scheduling complexity. Large language models (LLMs) now dominate deep learning workloads, streaming contiguous data blocks ranging from several kilobytes to megabytes per operation. In a conventional HBM-based memory system, these transfers are fragmented into hundreds of 32B cache line transactions. This forces the memory controller to employ unnecessarily intricate scheduling, leading to growing inefficiency.
  To address this problem, we propose RoMe. RoMe accesses DRAM at row granularity and removes columns, bank groups, and pseudo channels from the memory interface. This design simplifies memory scheduling, thereby requiring fewer pins per channel. The freed pins are aggregated to form additional channels, increasing overall bandwidth by 12.5% with minimal extra pins. RoMe demonstrates how memory scheduling logic can be significantly simplified for representative LLM workloads, and presents an alternative approach for next-generation HBM-based memory systems achieving increased bandwidth with minimal hardware overhead.

</details>


### [44] [A Systematic Characterization of LLM Inference on GPUs](https://arxiv.org/abs/2512.01644)
*Haonan Wang,Xuxin Xiao,Mingyu Yan,Zhuoyuan Zhu,Dengke Han,Duo Wang,Wenming Li,Xiaochun Ye,Cunchen Hu,Hongyang Chen,Guangyu Sun*

Main category: cs.AR

TL;DR: 本文通过系统性实验，构建了一个四维分析框架，全面刻画大语言模型（LLM）推理过程，从现象观察到硬件根源、系统扩展规律及新范式边界，为LLM推理优化提供实证基础与实用指导。


<details>
  <summary>Details</summary>
Motivation: 当前对大语言模型推理的理解较为碎片化，缺乏系统性的分析框架，限制了性能优化和新范式的探索。

Method: 通过全面实验，建立包含四个维度的分析框架：(1) 两阶段异构性观察；(2) 微架构根源分析；(3) 系统扩展原则；(4) 新兴范式边界，并依次从现象识别、硬件归因、系统验证到范式探索进行系统研究。

Result: 揭示了LLM推理中的关键性能现象及其微架构根源，验证了系统层面的扩展规律，并界定了新兴推理范式的适用边界。

Conclusion: 本研究不仅为现有LLM推理研究提供了可靠的实证基础，还带来了新发现，并为实际系统优化提供了指导。

Abstract: This work presents a systematic characterization of Large Language Model (LLM) inference to address fragmented understanding. Through comprehensive experiments, we establish a four-dimensional analytical framework: (1) Two-Phase Heterogeneity Observation; (2) Microarchitectural Root Cause Analysis; (3) System Scaling Principles; and (4) Emerging Paradigm Boundaries. Our investigation progresses systematically from observation to foresight: identifying performance phenomena, revealing hardware causes, validating system behavior, and exploring new paradigms. This study not only consolidates a reliable empirical foundation for existing research but also provides new discoveries and practical optimization guidance for LLM inference.

</details>


<div id='cs.OS'></div>

# cs.OS [[Back]](#toc)

### [45] [TenonOS: A Self-Generating Intelligent Embedded Operating System Framework for Edge Computing](https://arxiv.org/abs/2512.00400)
*Xinkui Zhao,Yifan Zhang,Haidan Zhao,Hao Zhang,Qingyu Ma,Lufei Zhang,Guanjie Cheng,Shuiguang Deng,Jianwei Yin,Zuoning Chen*

Main category: cs.OS

TL;DR: TenonOS 是一种面向边缘计算的轻量级、按需自生成操作系统框架，采用 LibOS-on-LibOS 架构，通过模块化微库和动态编排引擎构建定制化运行时环境，在实时性、内存占用和适应性方面显著优于现有方案。


<details>
  <summary>Details</summary>
Motivation: 传统操作系统和虚拟机监控器架构在异构边缘平台上难以满足资源受限和多样化需求，其单体或分层设计缺乏灵活性和可定制性。

Method: 提出 TenonOS 框架，包含模块化微库、动态编排引擎、最小化虚拟机监控器 Mortise 和实时 LibOS Tenon，实现按需组合与运行时自适应。

Result: 实验表明 TenonOS 在实时调度性能上提升 40.28%，内存占用仅 361 KiB，并具备对动态边缘工作负载的高适应性。

Conclusion: TenonOS 通过统一模块化架构有效解决了边缘计算中系统冗余、开销大和适应性差的问题，为资源受限的异构边缘系统提供了理想基础。

Abstract: The rapid evolution of edge computing has exposed fundamental limitations in traditional operating system and hypervisor architectures, particularly in managing heterogeneous platforms and meeting the constraints of limited resources. Existing solutions often rely on monolithic or layered combinations of hypervisors and guest OSes, which are difficult to tailor for the diverse and dynamic requirements of edge scenarios. To address these challenges, we propose TenonOS, a demand-driven, self-generating, and lightweight operating system framework that fundamentally rethinks and reconstructs both the hypervisor and OS architectures. TenonOS introduces a novel LibOS-on-LibOS approach, in which both virtualization and OS functionalities are modularized into fine-grained, reusable micro-libraries. A dynamic orchestration engine composes these modules on demand to construct customized, application-specific runtime environments. At the core of TenonOS are two key components: Mortise, a minimal, modularized hypervisor, and Tenon, a real-time LibOS. Mortise provides low-overhead resource isolation, fast inter-VM communication, and manages the full lifecycle of Tenon instances - including on-demand creation, suspension, and termination - enabling TenonOS to flexibly adapt its runtime layout to workload variations. Tenon delivers deterministic scheduling and multi-process support for time-critical applications. Through this unified and modular architecture, TenonOS eliminates redundant layers, reduces system overhead, and enhances scalability, security, and maintainability. Extensive evaluations demonstrate that TenonOS achieves superior real-time scheduling (40.28% improvement), a compact memory footprint (361 KiB), and high adaptability to dynamic edge workloads, making it an ideal foundation for heterogeneous, resource-constrained edge systems.

</details>


### [46] [Accelerating Probabilistic Response-Time Analysis: Revised Critical Instant and Optimized Convolution](https://arxiv.org/abs/2512.01381)
*Hiroto Takahashi,Atsushi Yano,Takuya Azumi*

Main category: cs.OS

TL;DR: 本文提出一种优化的聚合卷积方法，在修正的关键时刻假设下高效且安全地估计最坏情况截止期失败概率（WCDFP），显著降低计算开销同时保持估计精度。


<details>
  <summary>Details</summary>
Motivation: 传统关键时刻假设在概率性实时系统中可能导致WCDFP低估，无法确保安全性；同时高精度WCDFP估计通常计算成本高昂，亟需兼顾准确性与效率的新方法。

Method: 采用修正的关键时刻公式，研究基于卷积的WCDFP估计方法，并提出通过优化合并顺序来加速卷积计算的Aggregate Convolution优化技术。

Result: 在多种执行时间分布下的实验表明，所提优化方法相比Sequential Convolution可将计算时间减少近一个数量级，同时保持准确且偏保守（safe-sided）的WCDFP估计。

Conclusion: 该方法在保证安全性和准确性的同时显著提升了WCDFP估计效率，适用于对可靠性要求高的概率性实时系统。

Abstract: Accurate estimation of the Worst-Case Deadline Failure Probability (WCDFP) has attracted growing attention as a means to provide safety assurances in complex systems such as robotic platforms and autonomous vehicles. WCDFP quantifies the likelihood of deadline misses under the most pessimistic operating conditions, and safe estimation is essential for dependable real-time applications. However, achieving high accuracy in WCDFP estimation often incurs significant computational cost. Recent studies have revealed that the classical assumption of the critical instant, the activation pattern traditionally considered to trigger the worst-case behavior, can lead to underestimation of WCDFP in probabilistic settings. This observation motivates the use of a revised critical instant formulation that more faithfully captures the true worst-case scenario. This paper investigates convolution-based methods for WCDFP estimation under this revised setting and proposes an optimization technique that accelerates convolution by improving the merge order. Extensive experiments with diverse execution-time distributions demonstrate that the proposed optimized Aggregate Convolution reduces computation time by up to an order of magnitude compared to Sequential Convolution, while retaining accurate and safe-sided WCDFP estimates. These results highlight the potential of the approach to provide both efficiency and reliability in probabilistic timing analysis for safety-critical real-time applications.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [47] [Heimdall++: Optimizing GPU Utilization and Pipeline Parallelism for Efficient Single-Pulse Detection](https://arxiv.org/abs/2512.00398)
*Bingzheng Xia,Zujie Ren,Kuang Ma,Xiaoqian Li,Wenda Li,Shuibing He*

Main category: cs.DC

TL;DR: Heimdall++ is an optimized GPU-accelerated single-pulse detection tool that improves throughput and reduces latency by introducing fine-grained parallelization, better memory management, and a multi-threaded framework, achieving up to 2.66x speedup over the original Heimdall while preserving result accuracy.


<details>
  <summary>Details</summary>
Motivation: Real-time single-pulse detection is essential due to the high data volumes from modern radio telescopes, but existing tools like Heimdall suffer from GPU underutilization caused by sequential execution and resource contention.

Method: Heimdall++ introduces fine-grained GPU parallelization, enhanced memory management, and a multi-threaded architecture that decouples CPU- and GPU-bound tasks to reduce GPU stalls and improve efficiency.

Result: Heimdall++ achieves up to 2.66x speedup in single-file processing and 2.05x in multi-file batch processing on an NVIDIA RTX 3080 Ti system, with identical detection results to Heimdall.

Conclusion: Heimdall++ significantly enhances real-time single-pulse search performance without compromising result fidelity, making it well-suited for next-generation time-domain radio astronomy applications.

Abstract: With the increasing time and frequency resolution of modern radio telescopes and the exponential growth in observational data volumes, real-time single-pulse detection has become a critical requirement for time-domain radio astronomy. Heimdall, as a representative GPU-accelerated single-pulse search tool, offers substantial performance advantages over CPU-based approaches. However, its sequential execution model and resource contention in intermediate processing stages limit GPU utilization, leading to suboptimal throughput and increased computational latency. To address these limitations, we present Heimdall++, an optimized successor to Heimdall that incorporates fine-grained GPU parallelization, enhanced memory management, and a multi-threaded framework to decouple CPU-bound and GPU-bound processing stages. This design mitigates the GPU stall problem and improves end-to-end efficiency. We evaluated Heimdall++ on a system equipped with NVIDIA RTX 3080 Ti GPUs using both a single large-scale observational file and multiple files. Experimental results demonstrate that Heimdall++ achieves up to 2.66x speedup in single-file processing and 2.05x speedup in multi-file batch processing, while maintaining full consistency with the original Heimdall's search results.

</details>


### [48] [FlexiWalker: Extensible GPU Framework for Efficient Dynamic Random Walks with Runtime Adaptation](https://arxiv.org/abs/2512.00705)
*Seongyeon Park,Jaeyong Song,Changmin Shin,Sukjin Kim,Junguk Hong,Jinho Lee*

Main category: cs.DC

TL;DR: FlexiWalker 是首个支持高效、通用动态随机游走的 GPU 框架，通过优化采样策略、运行时内核选择和自动编译优化，在真实图数据上显著优于现有 CPU/GPU 基线。


<details>
  <summary>Details</summary>
Motivation: 动态随机游走因其运行时依赖的转移概率，无法利用现有针对静态随机游走的预计算优化策略，导致当前系统效率低下且缺乏对多样化工作负载的适应能力。

Method: 提出 FlexiWalker 框架，包含：(i) 高性能拒绝采样与蓄水池采样 GPU 内核，消除全局归约、冗余访存和随机数生成；(ii) 轻量级一阶代价模型，在运行时为每个节点选择更快的采样内核；(iii) 编译期组件，将用户自定义游走逻辑自动特化为优化构建块。

Result: 在多种真实图的动态随机游走任务上，FlexiWalker 相比最佳已发表的 CPU 和 GPU 基线分别平均提速 73.44 倍和 5.91 倍，并能执行先前系统无法处理的工作负载。

Conclusion: FlexiWalker 首次实现了高效、通用且易用的 GPU 动态随机游走支持，显著提升性能并扩展了可处理问题的范围，代码已开源。

Abstract: Dynamic random walks are fundamental to various graph analysis applications, offering advantages by adapting to evolving graph properties. Their runtime-dependent transition probabilities break down the pre-computation strategy that underpins most existing CPU and GPU static random walk optimizations. This leaves practitioners suffering from suboptimal frameworks and having to write hand-tuned kernels that do not adapt to workload diversity. To handle this issue, we present FlexiWalker, the first GPU framework that delivers efficient, workload-generic support for dynamic random walks. Our design-space study shows that rejection sampling and reservoir sampling are more suitable than other sampling techniques under massive parallelism. Thus, we devise (i) new high-performance kernels for them that eliminate global reductions, redundant memory accesses, and random-number generation. Given the necessity of choosing the best-fitting sampling strategy at runtime, we adopt (ii) a lightweight first-order cost model that selects the faster kernel per node at runtime. To enhance usability, we introduce (iii) a compile-time component that automatically specializes user-supplied walk logic into optimized building blocks. On various dynamic random walk workloads with real-world graphs, FlexiWalker outperforms the best published CPU/GPU baselines by geometric means of 73.44x and 5.91x, respectively, while successfully executing workloads that prior systems cannot support. We open-source FlexiWalker in https://github.com/AIS-SNU/FlexiWalker.

</details>


### [49] [SIMPLE: Disaggregating Sampling from GPU Inference into a Decision Plane for Faster Distributed LLM Serving](https://arxiv.org/abs/2512.00719)
*Bohan Zhao,Zane Cao,Yongchao He*

Main category: cs.DC

TL;DR: 本文提出 SIMPLE，一种与阶段无关、序列并行且可重叠的采样方案，通过将采样任务卸载到 CPU 并优化算法，显著提升大语言模型推理吞吐量并降低延迟，且无需用户修改代码。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型在张量并行（TP）和流水线并行（PP）下的扩展以及数据平面（如注意力机制、KV缓存）的高度优化，采样（将 logits 转换为 token 的决策过程）成为新的性能瓶颈。该瓶颈无法随 TP 扩展，也无法在 PP 各阶段间均衡负载，限制了整体推理效率。

Method: SIMPLE 方法包含三部分：(1) 序列并行采样，沿 batch 维度分片工作并消除词汇表维度的通信；(2) 基于 CPU 的单遍线性时间算法，采用列式惩罚与截断优先过滤；(3) 投机热词采样（SHVS），在小规模热词集上采样并通过拒绝校正确保正确性，并利用简单模型选择最优热词集大小以最大化吞吐。

Result: 实验表明，SIMPLE 最多可提升端到端吞吐量 96%，并将 P95 延迟降低 20%–65%。

Conclusion: SIMPLE 成功将采样开销降至可忽略水平，且兼容现有数据平面优化，无需用户改动代码，能随未来 GPU 发展持续获得扩展收益。

Abstract: As large language models (LLMs) scale out with tensor parallelism (TP) and pipeline parallelism (PP) and production stacks have aggressively optimized the data plane (attention/GEMM and KV cache), sampling, the decision plane that turns logits into tokens, becomes a new bottleneck. This creates a structural holdout: sampling neither expands with TP nor balances across PP stages, so its share of iteration time grows as GPUs get faster and it caps pipeline frequency at the last stage. We present SIMPLE, a stage-agnostic, sequence-parallel, overlappable decision plane that disaggregates sampling into a CPU-side service and shrinks its runtime footprint back to a minor, hidden role. SIMPLE combines: (1) sequence-parallel sampling, which shards work along the batch dimension and removes vocabulary-axis collectives; (2) a CPU-based algorithm with column-wise penalties and truncation-first filtering to realize single-pass, linear-time kernels; and (3) speculative hot-vocab sampling (SHVS), which samples on a small hot set with rejection-correctness and uses a simple sizing model to choose the hot-vocab size that maximizes throughput. In evaluation, SIMPLE improves end-to-end throughput by up to 96% and reduces P95 latency by 20-65%. Crucially, SIMPLE requires no user-side code changes and composes with existing data-plane optimizations, unlocking scaling benefits that compound with future GPU generations.

</details>


### [50] [Elastic Mixture of Rank-Wise Experts for Knowledge Reuse in Federated Fine-Tuning](https://arxiv.org/abs/2512.00902)
*Yebo Wu,Jingguang Li,Zhijiang Guo,Li Li*

Main category: cs.DC

TL;DR: 本文提出SmartFed，一种高效利用资源的联邦微调框架，通过复用现有LoRA模块中的知识，并引入MoRE和EEQA机制，在保证性能的同时显著提升训练效率。


<details>
  <summary>Details</summary>
Motivation: 联邦微调虽能保护数据隐私，但其高昂的计算与通信开销限制了在资源受限设备上的应用。

Method: 提出SmartFed框架，包括：1）复用已有LoRA模块中的知识；2）设计Mixture of Rank-Wise Experts (MoRE)，将LoRA分解为细粒度的秩级专家，按输入语义和资源预算动态激活；3）引入Elastic Expert Quota Allocation (EEQA)，根据各参数矩阵对性能的贡献自适应分配专家容量。

Result: 在多个基准上的实验表明，SmartFed在模型性能和训练效率方面均显著优于现有方法。

Conclusion: SmartFed通过智能复用LoRA知识并结合MoRE与EEQA机制，有效解决了联邦微调中资源消耗高的问题，为在资源受限设备上部署大语言模型提供了可行方案。

Abstract: Federated fine-tuning offers a promising solution for adapting Large Language Models (LLMs) to downstream tasks while safeguarding data privacy. However, its high computational and communication demands hinder its deployment on resource-constrained devices. In this paper, we propose SmartFed, a resource-efficient federated fine-tuning framework. SmartFed intelligently reuses knowledge embedded in existing LoRA modules, eliminating the need for expensive training from scratch when adapting LLMs to new tasks. To effectively exploit this knowledge and ensure scalability, we introduce the Mixture of Rank-Wise Experts (MoRE). MoRE decomposes LoRA modules into fine-grained rank-level experts. These experts are selectively activated and combined based on input semantics and resource budgets. Moreover, to optimize resource utilization, we present the Elastic Expert Quota Allocation (EEQA). EEQA adaptively allocates expert capacity across parameter matrices based on their contribution to model performance, focusing computing resources on the critical experts. Extensive evaluations across multiple benchmarks demonstrate that SmartFed significantly outperforms existing methods in model performance and training efficiency.

</details>


### [51] [Tangram: Accelerating Serverless LLM Loading through GPU Memory Reuse and Affinity](https://arxiv.org/abs/2512.01357)
*Wenbin Zhu,Zhaoyan Shen,Zili Shao,Hongjun Dai,Feng Chen*

Main category: cs.DC

TL;DR: Tangram 是一种通过高效复用 GPU 内存来加速无服务器大语言模型（Serverless LLM）加载的新系统，显著降低冷启动延迟和首 token 响应时间。


<details>
  <summary>Details</summary>
Motivation: 无服务器大语言模型在部署中面临严重的冷启动延迟问题，尤其是模型加载阶段，其耗时随模型规模线性增长，限制了大规模 LLM 服务的实际应用。

Method: Tangram 通过三个关键技术实现加速：统一的 GPU 内存池用于跨模型共享张量级参数、按需分配 KV 缓存以动态管理内存，以及感知 GPU 亲和性的调度策略以最大化资源利用率。

Result: 实验表明，Tangram 相比当前最先进的方法，模型加载速度最高提升 6.2 倍，冷启动下的首 token 时间（TTFT）减少 23%–55%。

Conclusion: Tangram 有效缓解了 Serverless LLM 平台中的内存使用效率低下和冷启动问题，为大规模语言模型的高效部署提供了可行方案。

Abstract: Serverless Large Language Models (LLMs) have emerged as a cost-effective solution for deploying AI services by enabling a 'pay-as-you-go' pricing model through GPU resource sharing. However, cold-start latency, especially the model loading phase, has become a critical performance bottleneck, as it scales linearly with model size and severely limits the practical deployment of large-scale LLM services. This paper presents Tangram, a novel system that accelerates Serverless LLM loading through efficient GPU memory reuse. By leveraging the unused GPU memory to retain model parameters, Tangram significantly reduces model transfer time and cold-start latency. Its design includes three key components: unified GPU memory pool for tensor-level parameter sharing across models, on-demand KV cache allocation for dynamic memory management, and GPU-affinity-aware scheduling for maximizing resource utilization. These techniques collectively address the critical challenges of inefficient memory usage and the cold-start problem in Serverless LLM platforms. We have implemented a fully functional prototype, and experiments show that Tangram achieves up to 6.2 times faster loading and reduces Time-To-First-Token (TTFT) during cold-start by 23--55% over state-of-the-art methods.

</details>


### [52] [StarDist: A Code Generator for Distributed Graph Algorithms](https://arxiv.org/abs/2512.01646)
*Barenya Kumar Nandy,Rupesh Nasre*

Main category: cs.DC

TL;DR: 本文提出了一种基于StarPlat MPI后端的分析-转换框架，通过优化邻域访问模式、聚合通信和机会缓存等手段提升分布式图算法性能，并在SSSP任务上显著优于d-Galois和DRONE。


<details>
  <summary>Details</summary>
Motivation: 大规模图数据在真实世界中普遍存在，但其不规则的访问模式、NUMA架构及物理内存限制导致传统串行/共享内存框架难以高效扩展。为解决分布式图算法编程复杂性和性能瓶颈问题，作者旨在简化高性能分布式图计算的实现。

Method: 设计了一个分析-转换框架，利用图迭代中节点及其邻居的一般语义，在StarPlat中自动重排邻域访问顺序、聚合通信操作，并在规约结构中引入机会缓存以减少通信；同时基于Open MPI的被动RMA机制构建了优化的批量规约底层。

Result: 在多个大规模图数据集上的单源最短路径（SSSP）任务中，优化后的StarPlat分布式后端性能达到d-Galois的2.05倍和DRONE的1.44倍。

Conclusion: 所提出的通信优化框架有效提升了分布式图算法的可扩展性与执行效率，验证了在高层抽象中集成自动通信优化策略的可行性与优势。

Abstract: Relational data, occurring in the real world, are often structured as graphs, which provide the logical abstraction required to make analytical derivations simpler. As graphs get larger, the irregular access patterns exhibited in most graph algorithms, hamper performance. This, along with NUMA and physical memory limits, results in scaling complexities with sequential/shared memory frameworks. StarPlat's MPI backend abstracts away the programmatic complexity involved in designing optimal distributed graph algorithms. It provides an instrument for coding graph algorithms that scale over distributed memory. In this work, we provide an analysis-transformation framework that leverages general semantics associated with iterations involving nodes and their neighbors, within StarPlat, to aggregate communication. The framework scans for patterns that warrant re-ordering in neighborhood access patterns, aggregate communication, and avoid communication altogether with opportunistic caching in reduction constructs. We also architect an optimized bulk-reduction substrate using Open MPI's passive Remote Memory Access (RMA) constructs. We applied our optimization logic to StarPlat's distributed backend and outperformed d-Galois by 2.05 and DRONE by 1.44 times in Single Source Shortest Paths across several big data graphs.

</details>


### [53] [Trace-based, time-resolved analysis of MPI application performance using standard metrics](https://arxiv.org/abs/2512.01764)
*Kingshuk Haldar*

Main category: cs.DC

TL;DR: 本文提出一种基于时间分段的MPI性能指标（负载均衡、串行化和传输效率）计算方法，通过处理Paraver轨迹数据，揭示被全局聚合指标掩盖的瞬时性能瓶颈。


<details>
  <summary>Details</summary>
Motivation: 传统MPI性能分析工具依赖全局聚合指标或可视化轨迹，难以有效识别瞬时或局部性能瓶颈，尤其在面对大规模轨迹数据和复杂通信行为时。

Method: 将执行轨迹划分为固定或自适应时间窗口，对每个窗口重建关键执行路径，并处理时钟不一致和未匹配MPI事件等异常，从而计算每个时间窗口内的标准MPI性能指标。

Result: 在合成基准测试和真实应用（LaMEM和ls1-MarDyn）上的评估表明，该方法能有效揭示被全局指标掩盖的局部性能瓶颈，且具有轻量级和可扩展性。

Conclusion: 所提方法为MPI应用性能分析提供了一种实用、高效的新途径，尤其适用于无法进行完整轨迹可视化的场景。

Abstract: Detailed trace analysis of MPI applications is essential for performance engineering, but growing trace sizes and complex communication behaviour often render comprehensive visual inspection impractical. This work presents a trace-based calculation of time-resolved values of standard MPI performance metrics, load balance, serialisation, and transfer efficiency, by discretising execution traces into fixed or adaptive time segments. The implementation processes Paraver traces postmortem, reconstructing critical execution paths and handling common event anomalies, such as clock inconsistencies and unmatched MPI events, to robustly calculate metrics for each segment. The calculated per-window metric values expose transient performance bottlenecks that the timeaggregated metrics from existing tools may conceal. Evaluations on a synthetic benchmark and real-world applications (LaMEM and ls1-MarDyn) demonstrate how time-resolved metrics reveal localised performance bottlenecks obscured by global aggregates, offering a lightweight and scalable alternative even when trace visualisation is impractical.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [54] [Toward a Safe Internet of Agents](https://arxiv.org/abs/2512.00520)
*Juan A. Wibowo,George C. Polyzos*

Main category: cs.MA

TL;DR: 本文提出了一种面向安全的架构化框架，用于分析和构建基于大语言模型的自主智能体系统，强调安全性应内生于系统架构而非事后附加。


<details>
  <summary>Details</summary>
Motivation: 当前“智能体互联网”（IoA）愿景虽具潜力，但带来了新的系统性安全风险；现有研究多聚焦于威胁分类，缺乏从系统架构层面理解与防范安全漏洞的工程化框架。

Method: 采用自底向上的方法，将智能体系统的每个组件视为具有双重用途的接口，依次分析单智能体、多智能体系统（MAS）和可互操作多智能体系统（IMAS）三个层级的核心架构及其固有安全风险。

Result: 识别出各层级架构中的具体安全漏洞，并提出了相应的缓解原则，表明智能体安全性本质上是一种架构原则。

Conclusion: 要实现安全可信的“智能体互联网”，必须将安全性作为系统设计的内在组成部分；本研究为构建可靠、安全的智能体系统提供了基础性指导。

Abstract: Background: Autonomous agents powered by Large Language Models (LLMs) are driving a paradigm shift toward an "Internet of Agents" (IoA). While offering immense potential, this vision also introduces novel and systemic risks to safety and security. Objectives: Unlike common threat-centric taxonomies, our survey provides a principled, architectural framework for engineering safe and reliable agentic systems. We aim to identify the architectural sources of vulnerabilities to establish a foundation for secure design. Methods: We perform a bottom-up deconstruction of agentic systems, treating each component as a dual-use interface. The analysis spans three levels of complexity: the foundational Single Agent, the collaborative Multi-Agent System (MAS), and the visionary Interoperable Multi-Agent System (IMAS). At each level, we identify core architectural components and their inherent security risks. Results & Conclusions: Our central finding is that agentic safety is an architectural principle, not an add-on. By identifying specific vulnerabilities and deriving mitigation principles at each level of the agentic stack, this survey serves as a foundational guide for building the capable, safe, and trustworthy AI needed to realize a secure Internet of Agents.

</details>


### [55] [AgentODRL: A Large Language Model-based Multi-agent System for ODRL Generation](https://arxiv.org/abs/2512.00602)
*Wanle Zhong,Keman Huang,Xiaoyong Du*

Main category: cs.MA

TL;DR: 本文提出AgentODRL，一种基于Orchestrator-Workers架构的多智能体系统，利用大语言模型提升自然语言到ODRL策略的自动翻译准确性与效率。


<details>
  <summary>Details</summary>
Motivation: 当前方法难以高效准确地将复杂自然语言规则翻译为ODRL格式，主要受限于授权策略的逻辑复杂性以及高质量训练数据的缺乏。

Method: 设计了一个多智能体系统AgentODRL，包含Generator、Decomposer和Rewriter三个专用Worker，并由Orchestrator动态协调；通过基于验证器的语法策略和基于LoRA微调模型的语义反思机制增强生成质量。

Result: 在包含770个不同复杂度用例的新构建数据集上进行实验，结果表明该系统在ODRL语法和语义评分上均优于现有方法。

Conclusion: 结合大语言模型与多智能体架构能有效提升自然语言到ODRL的自动化翻译质量，为数据权利管理提供可行解决方案。

Abstract: The Open Digital Rights Language (ODRL) is a pivotal standard for automating data rights management. However, the inherent logical complexity of authorization policies, combined with the scarcity of high-quality "Natural Language-to-ODRL" training datasets, impedes the ability of current methods to efficiently and accurately translate complex rules from natural language into the ODRL format. To address this challenge, this research leverages the potent comprehension and generation capabilities of Large Language Models (LLMs) to achieve both automation and high fidelity in this translation process. We introduce AgentODRL, a multi-agent system based on an Orchestrator-Workers architecture. The architecture consists of specialized Workers, including a Generator for ODRL policy creation, a Decomposer for breaking down complex use cases, and a Rewriter for simplifying nested logical relationships. The Orchestrator agent dynamically coordinates these Workers, assembling an optimal pathway based on the complexity of the input use case. Specifically, we enhance the ODRL Generator by incorporating a validator-based syntax strategy and a semantic reflection mechanism powered by a LoRA-finetuned model, significantly elevating the quality of the generated policies. Extensive experiments were conducted on a newly constructed dataset comprising 770 use cases of varying complexity, all situated within the context of data spaces. The results, evaluated using ODRL syntax and semantic scores, demonstrate that our proposed Orchestrator-Workers system, enhanced with these strategies, achieves superior performance on the ODRL generation task.

</details>


### [56] [Hierarchical Decentralized Multi-Agent Coordination with Privacy-Preserving Knowledge Sharing: Extending AgentNet for Scalable Autonomous Systems](https://arxiv.org/abs/2512.00614)
*Goutham Nalagatla*

Main category: cs.MA

TL;DR: AgentNet++ 是一种分层去中心化多智能体框架，在 AgentNet 基础上引入集群分层、差分隐私与安全聚合、自适应资源管理及收敛性保证，显著提升任务完成率、降低通信开销并保障隐私。


<details>
  <summary>Details</summary>
Motivation: 现有去中心化多智能体系统（如 AgentNet）在大规模部署时面临可扩展性差、通信开销高、缺乏隐私保障和资源分配低效等问题，亟需改进。

Method: 提出 AgentNet++ 框架，采用基于集群的分层结构使智能体自组织为专业小组，结合差分隐私与安全聚合实现隐私保护的知识共享，并引入自适应资源管理机制；同时提供收敛性和隐私边界的形式化分析。

Result: 实验表明，AgentNet++ 相比 AgentNet 和其他基线方法，任务完成率提高 23%，通信开销减少 40%，并保持强隐私保障；该框架可有效扩展至 1000+ 智能体。

Conclusion: AgentNet++ 在维持完全去中心化和涌现智能特性的同时，有效解决了可扩展性、通信效率、隐私保护和资源分配等关键挑战，为大规模 LLM 智能体协作提供了可行方案。

Abstract: Decentralized multi-agent systems have shown promise in enabling autonomous collaboration among LLM-based agents. While AgentNet demonstrated the feasibility of fully decentralized coordination through dynamic DAG topologies, several limitations remain: scalability challenges with large agent populations, communication overhead, lack of privacy guarantees, and suboptimal resource allocation. We propose AgentNet++, a hierarchical decentralized framework that extends AgentNet with multilevel agent organization, privacy-preserving knowledge sharing via differential privacy and secure aggregation, adaptive resource management, and theoretical convergence guarantees. Our approach introduces cluster-based hierarchies where agents self-organize into specialized groups, enabling efficient task routing and knowledge distillation while maintaining full decentralization. We provide formal analysis of convergence properties and privacy bounds, and demonstrate through extensive experiments on complex multi-agent tasks that AgentNet++ achieves 23% higher task completion rates, 40% reduction in communication overhead, and maintains strong privacy guarantees compared to AgentNet and other baselines. Our framework scales effectively to 1000+ agents while preserving the emergent intelligence properties of the original AgentNet.

</details>


### [57] [Chain of Unit-Physics: A Primitive-Centric Approach to Scientific Code Synthesis](https://arxiv.org/abs/2512.01010)
*Vansh Sharma,Venkat Raman*

Main category: cs.MA

TL;DR: 本文提出了一种名为“单元物理链”（Chain of Unit-Physics）的逆向代码生成框架，通过将专家知识编码为基于第一性原理的单元物理测试，约束多智能体系统生成科学计算代码。在燃烧任务上的实验表明，该方法能高效生成与人类专家实现高度一致、性能更优的求解器，克服了现有大语言模型在科学编程中的常见错误。


<details>
  <summary>Details</summary>
Motivation: 当前面向科学计算的自主代码生成大模型存在可靠性问题，主要受限于训练数据中领域代码稀疏以及小规模专家群体难以支撑强化学习人类反馈（RLHF）。因此，亟需一种能融合专家先验知识、确保物理一致性的新范式。

Method: 提出“单元物理链”框架：以第一性原理为核心，构建多智能体系统，将人类专家知识形式化为“单元物理测试”，用以显式约束代码生成过程，从而引导模型生成符合物理规律的科学计算程序。

Result: 在具代表性的燃烧任务上，现有闭源和开源代码智能体均无法生成正确端到端求解器，常犯四类错误；而所提框架仅需5–6次迭代即可收敛，结果与专家实现误差仅为3.1×10⁻³%，运行速度提升约33.4%，内存使用效率提高约30%，成本与中等商业API相当。

Conclusion: “单元物理链”框架通过嵌入第一性原理分析，为物理约束下的科学代码生成提供了一种可靠且实用的模板，超越了单纯依赖数据驱动的零样本代码生成方法。

Abstract: Agentic large language models are proposed as autonomous code generators for scientific computing, yet their reliability in high-stakes problems remains unclear. Developing computational scientific software from natural-language queries remains challenging broadly due to (a) sparse representation of domain codes during training and (b) the limited feasibility of RLHF with a small expert community. To address these limitations, this work conceptualizes an inverse approach to code design, embodied in the Chain of Unit-Physics framework: a first-principles (or primitives)-centric, multi-agent system in which human expert knowledge is encoded as unit-physics tests that explicitly constrain code generation. The framework is evaluated on a nontrivial combustion task, used here as a representative benchmark for scientific problem with realistic physical constraints. Closed-weight systems and code-focused agentic variants fail to produce correct end-to-end solvers, despite tool and web access, exhibiting four recurrent error classes: interface (syntax/API) hallucinations, overconfident assumptions, numerical/physical incoherence, and configuration fragility. Open-weight models with chain-of-thought (CoT) decoding reduce interface errors but still yield incorrect solutions. On the benchmark task, the proposed framework converges within 5-6 iterations, matches the human-expert implementation (mean error of $3.1\times10^{-3}$ %), with a $\sim$33.4 % faster runtime and a $\sim$30 % efficient memory usage at a cost comparable to mid-sized commercial APIs, yielding a practical template for physics-grounded scientific code generation. As datasets and models evolve, zero-shot code accuracy will improve; however, the Chain of Unit-Physics framework goes further by embedding first-principles analysis that is foundational to scientific codes.

</details>


### [58] [SocialDriveGen: Generating Diverse Traffic Scenarios with Controllable Social Interactions](https://arxiv.org/abs/2512.01363)
*Jiaguo Tian,Zhengbang Zhu,Shenyu Zhang,Li Xu,Bo Zheng,Xu Liu,Weiji Peng,Shizeng Yao,Weinan Zhang*

Main category: cs.MA

TL;DR: SocialDriveGen 提出了一种结合语义推理与社会偏好建模的分层框架，用于生成具有可控多样性的高保真交通场景，涵盖从合作到对抗的驾驶行为。


<details>
  <summary>Details</summary>
Motivation: 现有仿真框架多依赖基于规则或简化的模型，缺乏真实性和多样性；而当前生成模型虽提升真实性，却忽视了社会偏好对驾驶行为的影响。

Method: 提出 SocialDriveGen 分层框架，将利己与利他作为互补的社会维度，融合语义推理、社会偏好建模与轨迹生成，实现对驾驶个性和交互风格的可控生成。

Result: 在 Argoverse 2 数据集上的实验表明，该方法能生成高保真、多样化的交通场景，显著提升自动驾驶策略在罕见或高风险情境下的鲁棒性与泛化能力。

Conclusion: 通过引入社会偏好建模，SocialDriveGen 有效弥补了现有交通场景生成方法在行为多样性和真实性方面的不足，为自动驾驶系统的训练与评估提供了更可靠的仿真环境。

Abstract: The generation of realistic and diverse traffic scenarios in simulation is essential for developing and evaluating autonomous driving systems. However, most simulation frameworks rely on rule-based or simplified models for scene generation, which lack the fidelity and diversity needed to represent real-world driving. While recent advances in generative modeling produce more realistic and context-aware traffic interactions, they often overlook how social preferences influence driving behavior. SocialDriveGen addresses this gap through a hierarchical framework that integrates semantic reasoning and social preference modeling with generative trajectory synthesis. By modeling egoism and altruism as complementary social dimensions, our framework enables controllable diversity in driver personalities and interaction styles. Experiments on the Argoverse 2 dataset show that SocialDriveGen generates diverse, high-fidelity traffic scenarios spanning cooperative to adversarial behaviors, significantly enhancing policy robustness and generalization to rare or high-risk situations.

</details>


### [59] [Agent-Kernel: A MicroKernel Multi-Agent System Framework for Adaptive Social Simulation Powered by LLMs](https://arxiv.org/abs/2512.01610)
*Yuren Mao,Peigen Liu,Xinjian Wang,Rui Ding,Jing Miao,Hui Zou,Mingjie Qi,Wanxiang Luo,Longbin Lai,Kai Wang,Zhengping Qian,Peilun Yang,Yunjun Gao,Ying Zhang*

Main category: cs.MA

TL;DR: 本文提出了Agent-Kernel，一种基于社会中心化模块微内核架构的多智能体系统开发框架，以解决现有框架在大规模社会模拟中适应性、可配置性、可靠性和代码复用性方面的不足，并通过“宇宙25号”实验和浙江大学校园生活万级智能体模拟验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有面向大语言模型驱动的社会模拟的多智能体系统（MAS）开发框架在适应性、可配置性、可靠性与代码复用性方面存在局限，难以支持动态变化的智能体数量与角色特征的大规模模拟需求。

Method: 提出Agent-Kernel框架，采用新颖的“以社会为中心”的模块化微内核架构，将核心系统功能与模拟逻辑解耦，并分离认知过程与物理环境及动作执行。

Result: 通过两个应用验证：1）成功模拟“宇宙25号”实验中的种群从出生到死亡的快速动态变化；2）实现包含10,000个异构智能体（学生与教职工）的浙江大学校园生活大规模模拟。

Conclusion: Agent-Kernel在适应性、可配置性、可靠性与可复用性方面显著优于现有框架，为大规模、动态演化的LLM驱动社会模拟提供了高效可靠的基础设施。

Abstract: Multi-Agent System (MAS) developing frameworks serve as the foundational infrastructure for social simulations powered by Large Language Models (LLMs). However, existing frameworks fail to adequately support large-scale simulation development due to inherent limitations in adaptability, configurability, reliability, and code reusability. For example, they cannot simulate a society where the agent population and profiles change over time. To fill this gap, we propose Agent-Kernel, a framework built upon a novel society-centric modular microkernel architecture. It decouples core system functions from simulation logic and separates cognitive processes from physical environments and action execution. Consequently, Agent-Kernel achieves superior adaptability, configurability, reliability, and reusability. We validate the framework's superiority through two distinct applications: a simulation of the Universe 25 (Mouse Utopia) experiment, which demonstrates the handling of rapid population dynamics from birth to death; and a large-scale simulation of the Zhejiang University Campus Life, successfully coordinating 10,000 heterogeneous agents, including students and faculty.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [60] [Multi-Server FL with Overlapping Clients: A Latency-Aware Relay Framework](https://arxiv.org/abs/2512.00025)
*Yun Ji,Zeyu Chen,Xiaoxiong Zhong,Yanan Ma,Sheng Zhang,Yuguang Fang*

Main category: cs.NI

TL;DR: 本文提出了一种无云的多服务器联邦学习框架，利用重叠区域客户端作为中继，在不新增通信链路的情况下实现边缘服务器间的模型传播，并通过理论分析和优化算法提升模型传播范围与收敛性能。


<details>
  <summary>Details</summary>
Motivation: 传统单服务器联邦学习存在通信瓶颈，而多服务器架构中边缘服务器覆盖区域常有重叠，位于重叠区的客户端可访问多个边缘服务器模型。作者希望利用这些重叠客户端作为中继，实现服务器间模型交换，从而提升整体性能。

Method: 提出一种基于重叠客户端（OCs）的无云多服务器联邦学习框架，推导非凸目标下非独立同分布数据的收敛上界，构建以最大化模型传播范围为目标的优化问题，并设计基于冲突图的局部搜索算法来优化路由策略与传输调度。

Result: 实验结果表明，所提方案在模型传播范围和收敛性能方面显著优于现有方法。

Conclusion: 利用重叠客户端作为中继可在不增加新通信链路的前提下有效提升多服务器联邦学习的模型传播效率与系统性能，理论分析与实验验证了该方法的有效性。

Abstract: Multi-server Federated Learning (FL) has emerged as a promising solution to mitigate communication bottlenecks of single-server FL. In a typical multi-server FL architecture, the regions covered by different edge servers (ESs) may overlap. Under this architecture, clients located in the overlapping areas can access edge models from multiple ESs. Building on this observation, we propose a cloud-free multi-server FL framework that leverages Overlapping Clients (OCs) as relays for inter-server model exchange while uploading the local updated model to ESs. This enables ES models to be relayed across multiple hops through neighboring ESs by OCs without introducing new communication links. We derive a new convergence upper bound for non-convex objectives under non-IID data and an arbitrary number of cells, which explicitly quantifies the impact of inter-server propagation depth on convergence error. Guided by this theoretical result, we formulate an optimization problem that aims to maximize dissemination range of each ES model among all ESs within a limited latency. To solve this problem, we develop a conflict-graph-based local search algorithm optimizing the routing strategy and scheduling the transmission times of individual ESs to its neighboring ESs. This enables ES models to be relayed across multiple hops through neighboring ESs by OCs, achieving the widest possible transmission coverage for each model without introducing new communication links. Extensive experimental results show remarkable performance gains of our scheme compared to existing methods.

</details>


### [61] [An optimization framework for task allocation in the edge/hub/cloud paradigm](https://arxiv.org/abs/2512.00029)
*Andreas Kouloumpris,Georgios L. Stavrinides,Maria K. Michael,Theocharis Theocharides*

Main category: cs.NI

TL;DR: 本文针对边缘/中心/云三层架构中的任务分配问题，提出了一种基于二元整数线性规划（BILP）的优化方法，以最小化整体延迟或能耗，并通过真实和合成用例验证了其最优性和可扩展性。


<details>
  <summary>Details</summary>
Motivation: 随着物联网的发展，许多关键应用采用由单一边缘设备、单一中心设备和云服务器组成的精简架构。由于边缘设备在计算、通信和能量方面的限制，如何在此类架构中高效分配任务成为一个挑战，亟需一个能综合考虑多种约束并实现最优任务分配的框架。

Method: 作者提出一种完整的、基于二元整数线性规划（BILP）的建模方法，用于在设计阶段进行应用驱动的任务分配，目标是最小化整体延迟或总能耗，并纳入了以往研究常忽略的关键参数与约束。

Result: 通过真实应用场景和合成基准测试，实验表明该方法能够生成最优且可扩展的任务分配方案，有效支持不同应用和计算设备的设计空间探索。

Conclusion: 所提出的BILP框架能高效、最优地解决边缘/中心/云架构下的任务分配问题，在满足多种实际约束的同时，兼顾延迟与能耗优化，具有良好的实用性和扩展性。

Abstract: With the advent of the Internet of Things (IoT), novel critical applications have emerged that leverage the edge/hub/cloud paradigm, which diverges from the conventional edge computing perspective. A growing number of such applications require a streamlined architecture for their effective execution, often comprising a single edge device with sensing capabilities, a single hub device (e.g., a laptop or smartphone) for managing and assisting the edge device, and a more computationally capable cloud server. Typical examples include the utilization of an unmanned aerial vehicle (UAV) for critical infrastructure inspection or a wearable biomedical device (e.g., a smartwatch) for remote patient monitoring. Task allocation in this streamlined architecture is particularly challenging, due to the computational, communication, and energy limitations of the devices at the network edge. Consequently, there is a need for a comprehensive framework that can address the specific task allocation problem optimally and efficiently. To this end, we propose a complete, binary integer linear programming (BILP) based formulation for an application-driven design-time approach, capable of providing an optimal task allocation in the targeted edge/hub/cloud environment. The proposed method minimizes the desired objective, either the overall latency or overall energy consumption, while considering several crucial parameters and constraints often overlooked in related literature. We evaluate our framework using a real-world use-case scenario, as well as appropriate synthetic benchmarks. Our extensive experimentation reveals that the proposed approach yields optimal and scalable results, enabling efficient design space exploration for different applications and computational devices.

</details>


### [62] [LM4Opt-RA: A Multi-Candidate LLM Framework with Structured Ranking for Automating Network Resource Allocation](https://arxiv.org/abs/2512.00039)
*Tasnim Ahmed,Siana Rizwan,Naveed Ejaz,Salimur Choudhury*

Main category: cs.NI

TL;DR: 本文提出了NL4RA数据集和LM4Opt-RA框架，用于提升大语言模型在资源分配优化问题（如LP/ILP/MILP）中的建模能力，并引入新评估指标LAME以更准确衡量模型生成的数学公式质量。


<details>
  <summary>Details</summary>
Motivation: 现有基准和数据集无法有效应对具有动态环境、变量相互依赖和异构约束的复杂资源分配优化问题，且缺乏适用于数学建模任务的自动化评估方法。

Method: 构建包含50个资源分配优化问题的NL4RA数据集；提出LM4Opt-RA多候选框架，结合多种提示策略（直接提示、少样本、思维链）与结构化排序机制；设计LLM辅助的数学评估指标LAME。

Result: 在LM4Opt-RA框架下，Llama-3.1-70B取得0.8007的LAME分数，显著优于其他开源模型；所提方法在LAME及其他指标上均超越基线模型，但仍不及人类专家水平。

Conclusion: 大语言模型在复杂数学建模任务中展现出潜力，但需结合专门设计的提示策略、排序机制和评估指标才能有效提升性能；NL4RA和LAME为未来研究提供了新基准和工具。

Abstract: Building on advancements in Large Language Models (LLMs), we can tackle complex analytical and mathematical reasoning tasks requiring nuanced contextual understanding. A prime example of such complex tasks is modelling resource allocation optimization in networks, which extends beyond translating natural language inputs into mathematical equations or Linear Programming (LP), Integer Linear Programming (ILP), and Mixed-Integer Linear Programming (MILP) models. However, existing benchmarks and datasets cannot address the complexities of such problems with dynamic environments, interdependent variables, and heterogeneous constraints. To address this gap, we introduce NL4RA, a curated dataset comprising 50 resource allocation optimization problems formulated as LP, ILP, and MILP. We then evaluate the performance of well-known open-source LLMs with varying parameter counts. To enhance existing LLM based methods, we introduce LM4Opt RA, a multi candidate framework that applies diverse prompting strategies such as direct, few shot, and chain of thought, combined with a structured ranking mechanism to improve accuracy. We identified discrepancies between human judgments and automated scoring such as ROUGE, BLEU, or BERT scores. However, human evaluation is time-consuming and requires specialized expertise, making it impractical for a fully automated end-to-end framework. To quantify the difference between LLM-generated responses and ground truth, we introduce LLM-Assisted Mathematical Evaluation (LAME), an automated metric designed for mathematical formulations. Using LM4Opt-RA, Llama-3.1-70B achieved a LAME score of 0.8007, outperforming other models by a significant margin, followed closely by Llama-3.1-8B. While baseline LLMs demonstrate considerable promise, they still lag behind human expertise; our proposed method surpasses these baselines regarding LAME and other metrics.

</details>


### [63] [Improving Channel Estimation Through Gold Sequences](https://arxiv.org/abs/2512.00509)
*Sumita Majhi,Kaushal Shelke,Pinaki Mitra,Ujjwal Biswas*

Main category: cs.NI

TL;DR: 本文研究了结合Gold码与传统V-BLAST（C-V-BLAST）的非正交多址接入（NOMA）系统，提出一种利用分数功率分配和部分解码数据符号的新信道估计方法，在包含AWGN、瑞利衰落和阴影效应的仿真环境中，所提出的信道预测函数（CPF）优于传统基于导频的估计技术。


<details>
  <summary>Details</summary>
Motivation: 由于NOMA系统中用户信号在共享子载波上叠加，导致用户分离困难，而Gold序列具有良好的正交特性，有望提升用户分离性能与信道估计精度。

Method: 提出一种新型信道估计方法，结合分数功率分配、部分解码的数据符号及导频信号，构建信道预测函数（CPF），并在包含AWGN、瑞利衰落和阴影效应的仿真环境中进行验证。

Result: 所提出的CPF方法在 realistic 仿真条件下优于传统的基于导频的信道估计技术。

Conclusion: Gold码与C-V-BLAST结合的NOMA系统通过新提出的CPF信道估计方法，能有效提升用户分离和信道估计性能。

Abstract: This study evaluates Non-Orthogonal Multiple Access (NOMA) systems using Gold coding and Conventional-V-BLAST (C-V-BLAST). Superimposed signals on shared subcarriers make NOMA user separation difficult, unlike MIMO. Gold sequences' orthogonal features may enhance user separation and channel estimation. A novel channel estimation approach uses fractional power allocation and partially decoded data symbols. A realistic simulation environment was created using AWGN, Rayleigh fading, and shadowing. Using pilot signals, power allocation, and data symbols, our Channel Prediction Function (CPF) surpasses pilot-based techniques.

</details>


### [64] [Physical-Layer Analysis of LoRa Robustness in the Presence of Narrowband Interference](https://arxiv.org/abs/2512.01088)
*Jingxiang Huang,Samer Lahoud*

Main category: cs.NI

TL;DR: 本文研究了窄带干扰（BPSK和GMSK）对LoRa解调的物理层影响，发现将此类干扰建模为加性高斯白噪声（AWGN）会系统性高估Chirp扩频（CSS）解调的符号错误率（SER），并提出了适用于不同信噪比条件下的最大可容忍干扰噪声比（INR）分段函数模型。


<details>
  <summary>Details</summary>
Motivation: 随着物联网技术的发展，Sub-GHz免授权频谱被LoRa、Sigfox和LR-FHSS等多种协议共享，导致相互干扰。准确评估窄带干扰对LoRa性能的影响至关重要，而传统AWGN模型可能无法真实反映实际干扰效应。

Method: 采用符号级蒙特卡洛仿真，在给定信噪比（SNR）和噪声底条件下，分析干扰噪声比（INR）对LoRa符号错误率（SER）的影响，并与等功率AWGN情况对比；进一步拟合出确保正确解调的最大INR与SNR之间的两段式函数关系。

Result: 研究表明，将窄带干扰建模为AWGN会系统性高估CSS解调的SER；BPSK和GMSK干扰对LoRa造成的损伤程度明显不同于AWGN；在低SNR和高SNR区域分别适用不同的最大INR阈值。

Conclusion: 窄带干扰不能简单等效为AWGN，需采用更精确的干扰模型来评估LoRa系统性能；所提出的两段式INR-SNR函数可为实际系统设计提供参考。

Abstract: With the rapid development of Internet of Things (IoT) technologies, the sub-GHz unlicensed spectrum is increasingly being shared by protocols such as Long Range (LoRa), Sigfox, and Long-Range Frequency-Hopping Spread Spectrum (LR-FHSS). These protocols must coexist within the same frequency bands, leading to mutual interference. This paper investigates the physical-layer impact of two types of narrowband signals (BPSK and GMSK) on LoRa demodulation. We employ symbol-level Monte Carlo simulations to analyse how the interference-to-noise ratio (INR) affects the symbol error rate (SER) at a given signal-to-noise ratio (SNR) and noise floor, and then compare the results with those for additive white Gaussian noise (AWGN) of equal power. We demonstrate that modelling narrowband interference as additive white Gaussian noise (AWGN) systematically overestimates the SER of Chirp Spread Spectrum (CSS) demodulation. We also clarify the distinct impairment levels induced by AWGN and two types of narrowband interferers, and provide physical insight into the underlying mechanisms. Finally, we fit a two-segment function for the maximum INR that ensures correct demodulation across SNRs, with one segment for low SNR and the other for high SNR.

</details>


### [65] [Modeling and Simulation of Data Protection Systems for Business Continuity and Disaster Recovery](https://arxiv.org/abs/2512.01477)
*Saso Nikolovski,Pece Mitrevski*

Main category: cs.NI

TL;DR: 本文对比分析了部分或完全基于云环境的灾难恢复系统，通过实际部署与仿真评估其可靠性，并提出一个用于选择和维护数据保护与恢复方案的综合框架。


<details>
  <summary>Details</summary>
Motivation: 在高度依赖信息技术的企业环境中，制定有效的业务连续性和灾难恢复策略对快速从故障中恢复至关重要。

Method: 在真实生产环境中部署两个云相关恢复系统，利用仿真软件识别关键性能与可靠性指标，并对每个系统进行系统动力学分析。

Result: 提出了一个综合框架，包含选择和维护数据恢复解决方案的标准，以匹配组织运营需求并满足业务连续性计划中的时间要求。

Conclusion: 研究结果为组织在选择合适灾难恢复方案时提供了可操作的决策依据。

Abstract: In today's corporate landscape, particularly where operations rely heavily on information technologies, establishing a robust business continuity plan, including a disaster recovery strategy, is essential for ensuring swift recuperation following outages. This study presents a comparative analysis of recovery solutions, focusing on systems that operate partially or entirely within cloud environments and assessing their reliability in fulfilling organizational roles securely and dependably. Two such systems were deployed and evaluated in a real-world production setting. Key performance and reliability metrics were identified using simulation software to enhance these systems, alongside a System Dynamics analysis conducted for each. This work proposes a comprehensive framework for selecting and maintaining data protection and recovery solutions within organizational structures, outlining criteria for aligning chosen approaches with operational needs while adhering to predetermined timelines specified in business continuity and disaster recovery plans. The resulting analysis and findings offer actionable insights to guide decision-making when selecting appropriate recovery concepts.

</details>


### [66] [Velocity-Adaptive Access Scheme for Semantic-Aware Vehicular Networks: Joint Fairness and AoI Optimization](https://arxiv.org/abs/2512.01571)
*Xiao Xu,Qiong Wu,Pingyi Fan,Kezhi Wang,Nan Cheng,Wen Chen,Khaled B. Letaief*

Main category: cs.NI

TL;DR: 本文针对5G NR V2X Mode 2中因车辆速度差异导致通信时长不同、进而引发信息交换不公与Age of Information（AoI）恶化的问题，提出一种兼顾公平接入与AoI优化的方案。通过调整各车辆的选择窗口定义公平性指标，并结合图像语义通信降低时延；同时考虑5G NR中的重评估机制对AoI的影响。利用随机混合系统（SHS）分析AoI，构建多目标优化问题，并采用序列凸逼近（SCA）将其转化为凸问题求解，还设计了基于大语言模型（LLM）的算法，仿真验证了所提方法的有效性。


<details>
  <summary>Details</summary>
Motivation: 在5G NR V2X Mode 2场景下，车辆与路侧单元（RSU）通信时，由于车速不同造成通信持续时间差异，导致各车辆交换的数据量不均，可能带来高速环境下的安全隐患，因此需同时保障接入公平性与信息新鲜度（AoI）。

Method: 定义基于选择窗口的公平性指标，引入图像语义通信以降低传输延迟；利用随机混合系统（SHS）建模并分析AoI；构建公平接入与AoI优化的多目标问题，采用序列凸逼近（SCA）将非凸问题转化为凸优化问题进行求解，并提出一种基于大语言模型（LLM）的辅助算法。

Result: 通过数值仿真验证了所提方案在提升接入公平性和降低AoI方面的有效性，表明该方法能够在考虑5G NR重评估机制的前提下，实现性能与公平性的良好平衡。

Conclusion: 本文成功构建并解决了5G NR V2X Mode 2中公平接入与AoI联合优化的问题，所提方法兼顾了通信公平性、信息时效性与系统效率，为高可靠低时延车联网通信提供了可行方案。

Abstract: In this paper, we address the problem of fair access and Age of Information (AoI) optimization in 5G New Radio (NR) Vehicle to Everything (V2X) Mode 2. Specifically, vehicles need to exchange information with the road side unit (RSU). However, due to the varying vehicle speeds leading to different communication durations, the amount of data exchanged between different vehicles and the RSU may vary. This may poses significant safety risks in high-speed environments. To address this, we define a fairness index through tuning the selection window of different vehicles and consider the image semantic communication system to reduce latency. However, adjusting the selection window may affect the communication time, thereby impacting the AoI. Moreover, considering the re-evaluation mechanism in 5G NR, which helps reduce resource collisions, it may lead to an increase in AoI. We analyze the AoI using Stochastic Hybrid System (SHS) and construct a multi-objective optimization problem to achieve fair access and AoI optimization. Sequential Convex Approximation (SCA) is employed to transform the non-convex problem into a convex one, and solve it using convex optimization. We also provide a large language model (LLM) based algorithm. The scheme's effectiveness is validated through numerical simulations.

</details>
