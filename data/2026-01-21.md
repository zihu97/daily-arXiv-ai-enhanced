<div id=toc></div>

# Table of Contents

- [cs.AR](#cs.AR) [Total: 7]
- [cs.SE](#cs.SE) [Total: 50]
- [cs.OS](#cs.OS) [Total: 3]
- [cs.DC](#cs.DC) [Total: 26]
- [cs.NI](#cs.NI) [Total: 4]
- [cs.MA](#cs.MA) [Total: 6]


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [1] [NuRedact: Non-Uniform eFPGA Architecture for Low-Overhead and Secure IP Redaction](https://arxiv.org/abs/2601.11770)
*Voktho Das,Kimia Azar,Hadi Kamali*

Main category: cs.AR

TL;DR: NuRedact 是首个基于非均匀架构的全定制 eFPGA 电路保护框架，在保障安全的同时显著降低面积开销。


<details>
  <summary>Details</summary>
Motivation: 现有可重构红化方案因人为复杂化导致资源利用率低、成本过高，需兼顾安全与效率的优化方案。

Method: 提出三阶段方法：定制非均匀结构布线、VPR 层级非均匀布局优化、支持红化的 IP 映射与重配置。

Result: 相比传统均匀结构，面积减少最高达 9 倍，安全对抗 SAT、循环与序列攻击仍具强韧性，且设计开销可控。

Conclusion: NuRedact 在 OpenFPGA 基础上实现安全与效率的良好平衡，为电路供应链防护提供实用新方案。

Abstract: While logic locking has been extensively studied as a countermeasure against integrated circuit (IC) supply chain threats, recent research has shifted toward reconfigurable-based redaction techniques, e.g., LUT- and eFPGA-based schemes. While these approaches raise the bar against attacks, they incur substantial overhead, much of which arises not from genuine functional reconfigurability need, but from artificial complexity intended solely to frustrate reverse engineering (RE). As a result, fabrics are often underutilized, and security is achieved at disproportionate cost. This paper introduces NuRedact, the first full-custom eFPGA redaction framework that embraces architectural non-uniformity to balance security and efficiency. Built as an extension of the widely adopted OpenFPGA infrastructure, NuRedact introduces a three-stage methodology: (i) custom fabric generation with pin-mapping irregularity, (ii) VPR-level modifications to enable non-uniform placement guided by an automated Python-based optimizer, and (iii) redaction-aware reconfiguration and mapping of target IP modules. Experimental results show up to 9x area reduction compared to conventional uniform fabrics, achieving competitive efficiency with LUT-based and even transistor-level redaction techniques while retaining strong resilience. From a security perspective, NuRedact fabrics are evaluated against state-of-the-art attack models, including SAT-based, cyclic, and sequential variants, and show enhanced resilience while maintaining practical design overheads.

</details>


### [2] [Domain-specific Hardware Acceleration for Model Predictive Path Integral Control](https://arxiv.org/abs/2601.12089)
*Erwan Tanguy-Legac,Tommaso Belvedere,Gianluca Corsini,Marco Tognon,Marcello Traiola*

Main category: cs.AR

TL;DR: 本文提出了一种用于MPPI控制的硬件加速器，相比GPU实现能提供更精确的轨迹且功耗更低。


<details>
  <summary>Details</summary>
Motivation: 解决机器人系统实时控制中MPPI算法计算负载高、GPU功耗大的问题。

Method: 设计并仿真一种基于FPGA的MPPI定制硬件加速器。

Result: 该加速器在轨迹精度上优于GPU实现。

Conclusion: 定制硬件加速器是提升MPPI控制效率与能效的有效方案。

Abstract: Accurately controlling a robotic system in real time is a challenging problem. To address this, the robotics community has adopted various algorithms, such as Model Predictive Control (MPC) and Model Predictive Path Integral (MPPI) control. The first is difficult to implement on non-linear systems such as unmanned aerial vehicles, whilst the second requires a heavy computational load. GPUs have been successfully used to accelerate MPPI implementations; however, their power consumption is often excessive for autonomous or unmanned targets, especially when battery-powered. On the other hand, custom designs, often implemented on FPGAs, have been proposed to accelerate robotic algorithms while consuming considerably less energy than their GPU (or CPU) implementation. However, no MPPI custom accelerator has been proposed so far. In this work, we present a hardware accelerator for MPPI control and simulate its execution. Results show that the MPPI custom accelerator allows more accurate trajectories than GPU-based MPPI implementations.

</details>


### [3] [CD-PIM: A High-Bandwidth and Compute-Efficient LPDDR5-Based PIM for Low-Batch LLM Acceleration on Edge-Device](https://arxiv.org/abs/2601.12298)
*Ye Lin,Chao Fang,Xiaoyong Song,Qi Wu,Anying Jiang,Yichuan Bai,Li Du*

Main category: cs.AR

TL;DR: CD-PIM是一种面向边缘设备的新型PIM架构，通过高带宽计算模式、低批次交错模式和高效计算单元设计，显著提升LLM推理中GEMV操作的性能。


<details>
  <summary>Details</summary>
Motivation: 解决边缘设备部署LLM时因内存带宽瓶颈导致的GEMV运算效率低下问题，同时克服现有PIM架构在带宽、资源利用率和计算能力方面的不足。

Method: 提出HBCEM模式（分段全局位线实现伪银行）、LBIM模式（GEMV与GEMM操作重叠）、流水线式计算单元及键值缓存矩阵行列映射策略，优化带宽与资源利用。

Result: 相比纯GPU基线和先进PIM设计，CD-PIM在单批次下分别实现11.42倍和4.25倍平均加速；低批次场景下LBIM相较HBCEM再获1.12倍提速。

Conclusion: CD-PIM有效缓解边缘LLM推理中的内存瓶颈，在带宽、利用率和计算效率三方面取得突破，为边缘智能提供高性能低功耗解决方案。

Abstract: Edge deployment of low-batch large language models (LLMs) faces critical memory bandwidth bottlenecks when executing memory-intensive general matrix-vector multiplications (GEMV) operations. While digital processing-in-memory (PIM) architectures promise to accelerate GEMV operations, existing PIM-equipped edge devices still suffer from three key limitations: limited bandwidth improvement, component under-utilization in mixed workloads, and low compute capacity of computing units (CUs). In this paper, we propose CD-PIM to address these challenges through three key innovations. First, we introduce a high-bandwidth compute-efficient mode (HBCEM) that enhances bandwidth by dividing each bank into four pseudo-banks through segmented global bitlines. Second, we propose a low-batch interleaving mode (LBIM) to improve component utilization by overlapping GEMV operations with GEMM operations. Third, we design a compute-efficient CU that performs enhanced GEMV operations in a pipelined manner by serially feeding weight data into the computing core. Forth, we adopt a column-wise mapping for the key-cache matrix and row-wise mapping for the value-cache matrix, which fully utilizes CU resources. Our evaluation shows that compared to a GPU-only baseline and state-of-the-art PIM designs, our CD-PIM achieves 11.42x and 4.25x speedup on average within a single batch in HBCEM mode, respectively. Moreover, for low-batch sizes, the CD-PIM achieves an average speedup of 1.12x in LBIM compared to HBCEM.

</details>


### [4] [Best Practices for Large Load Interconnections: A North American Perspective on Data Centers](https://arxiv.org/abs/2601.12686)
*Rafi Zahedi,Amin Zamani,Rahul Anilkumar*

Main category: cs.AR

TL;DR: 本文综述北美大型负载并网最佳实践，聚焦数据中心等新兴负载带来的技术挑战，并提出实用指导建议。


<details>
  <summary>Details</summary>
Motivation: AI驱动的数据中心等大型负载快速增长，对电网互联带来新挑战，亟需统一技术规范。

Method: 综合分析手册、运营商指南及跨电力公司比较，结合欧洲经验进行归纳。

Result: 识别出现有规范在穿越能力、负载变化管理及扰动后恢复目标方面的不足。

Conclusion: 基于分析结果，为开发商与电力公司提供实用的并网技术指导。

Abstract: Large loads are expanding rapidly across North America, led by data centers, cryptocurrency mining, hydrogen production facilities, and heavy-duty charging stations. Each class presents distinct electrical characteristics, but data centers are drawing particular attention as AI deployment drives unprecedented capacity growth. Their scale, duty cycles, and converter-dominated interfaces introduce new challenges for transmission interconnections, especially regarding disturbance behavior, steady-state performance, and operational visibility. This paper reviews best practices for large-load interconnections across North America, synthesizing utility and system operator guidelines into a coherent set of technical requirements. The approach combines handbook and manual analysis with cross-utility comparisons and an outlook on European directions. The review highlights requirements on power quality, telemetry, commissioning tests, and protection coordination, while noting gaps in ride-through specifications, load-variation management, and post-disturbance recovery targets. Building on these findings, the paper proposes practical guidance for developers and utilities.

</details>


### [5] [The Non-Predictability of Mispredicted Branches using Timing Information](https://arxiv.org/abs/2601.13804)
*Ioannis Constantinou,Arthur Perais,Yiannakis Sazeides*

Main category: cs.AR

TL;DR: 本文研究利用微架构信息提升分支预测准确率，虽整体未超越TAGE-SC，但发现特定难预测分支可受益于时序信息。


<details>
  <summary>Details</summary>
Motivation: 降低现代处理器中因分支误预测导致的性能下降与能耗浪费。

Method: 提出SBR方法，在ROB分配后N周期收集时序信息重新预测，并在gem5模拟器中基于TAGE-Like预测器进行极限研究。

Result: 总体未优于无界TAGE-SC，但在两个难预测分支上观察到时序信息带来的优势。

Conclusion: 特定微架构信息可能提升难预测分支的准确率，后端覆盖预测或有潜力，需进一步探索有效信息向量。

Abstract: Branch misprediction latency is one of the most important contributors to performance degradation and wasted energy consumption in a modern core. State-of-the-art predictors generally perform very well but occasionally suffer from high Misprediction Per Kilo Instruction due to hard-to-predict branches. In this work, we investigate if predicting branches using microarchitectural information, in addition to traditional branch history, can improve prediction accuracy. Our approach considers branch timing information (resolution cycle) both for older branches in the Reorder Buffer (ROB) and recently committed, and for younger branches relative to the branch we re-predict. We propose Speculative Branch Resolution (SBR) in which, N cycles after a branch allocates in the ROB, various timing information is collected and used to re-predict. Using the gem5 simulator we implement and perform a limit-study of SBR using a TAGE-Like predictor. Our experiments show that the post-alloc timing information we used was not able to yield performance gains over an unbounded TAGE-SC. However, we find two hard to predict branches where timing information did provide an advantage and thoroughly analysed one of them to understand why. This finding suggests that predictors may benefit from specific microarchitectural information to increase accuracy on specific hard to predict branches and that overriding predictions in the backend may yet yield performance benefits, but that further research is needed to determine such information vectors.

</details>


### [6] [CREATE: Cross-Layer Resilience Characterization and Optimization for Efficient yet Reliable Embodied AI Systems](https://arxiv.org/abs/2601.14140)
*Tong Xie,Yijiahao Qi,Jinqi Wen,Zishen Wan,Yanchi Dong,Zihao Wang,Shaofei Cai,Yitao Liang,Tianyu Jia,Yuan Wang,Runsheng Wang,Meng Li*

Main category: cs.AR

TL;DR: CREATE是一种针对具身AI系统的能效与可靠性协同优化设计原则，通过电路、模型和应用层的异构容错机制，在不降低任务质量的前提下显著节省能耗并延长电池寿命。


<details>
  <summary>Details</summary>
Motivation: 具身AI系统计算需求高，难以在电池供电设备上部署，且低电压节能方法易引发位错误导致任务失败。

Method: 提出CREATE设计原则，包含电路层异常检测清除机制、模型层权重旋转增强规划算法、应用层自主适应电压调节技术，并实现电压调节电路的协同设计。

Result: 实验表明CREATE平均节省40.6%计算能耗，芯片级节能29.5%-37.3%，电池寿命提升15%-30%。

Conclusion: CREATE有效平衡了具身AI系统的能效与可靠性，为边缘部署提供了实用解决方案。

Abstract: Embodied Artificial Intelligence (AI) has recently attracted significant attention as it bridges AI with the physical world. Modern embodied AI systems often combine a Large Language Model (LLM)-based planner for high-level task planning and a reinforcement learning (RL)-based controller for low-level action generation, enabling embodied agents to tackle complex tasks in real-world environments. However, deploying embodied agents remains challenging due to their high computation requirements, especially for battery-powered local devices. Although techniques like lowering operating voltage can improve energy efficiency, they can introduce bit errors and result in task failures. In this work, we propose CREATE, a general design principle that leverages heterogeneous resilience at different layers for synergistic energy-reliability co-optimization. For the first time, we conduct a comprehensive error injection study on modern embodied AI systems and observe an inherent but heterogeneous fault tolerance. Building upon these insights, we develop an anomaly detection and clearance mechanism at the circuit level to eliminate outlier errors. At the model level, we propose a weight-rotation-enhanced planning algorithm to improve the fault tolerance of the LLM-based planner. Furthermore, we introduce an application-level technique, autonomy-adaptive voltage scaling, to dynamically adjust the operating voltage of the controllers. The voltage scaling circuit is co-designed to enable online voltage adjustment. Extensive experiments demonstrate that without compromising task quality, CREATE achieves 40.6% computational energy savings on average over nominal-voltage baselines and 35.0% over prior-art techniques. This further leads to 29.5% to 37.3% chip-level energy savings and approximately a 15% to 30% improvement in battery life.

</details>


### [7] [The Quest for Reliable AI Accelerators: Cross-Layer Evaluation and Design Optimization](https://arxiv.org/abs/2601.14148)
*Meng Li,Tong Xie,Zuodong Zhang,Runsheng Wang*

Main category: cs.AR

TL;DR: 本文提出一系列跨层可靠性感知的AI加速器设计方法，以应对纳米级CMOS技术下的老化与工艺变异问题。


<details>
  <summary>Details</summary>
Motivation: 传统基于保护带的设计牺牲性能效率，无法满足高性能AI计算需求，且缺乏系统性跨层分析工具。

Method: 提出老化与变异感知的动态时序分析器、关键输入模式减少的数据流优化、以及面向大语言模型的弹性架构设计。

Result: 通过整合跨层可靠性建模与AI负载特征，实现可靠且高效的AI加速。

Conclusion: 所提方法有效平衡可靠性与计算效率，为下一代AI加速器提供可行设计路径。

Abstract: As the CMOS technology pushes to the nanoscale, aging effects and process variations have become increasingly pronounced, posing significant reliability challenges for AI accelerators. Traditional guardband-based design approaches, which rely on pessimistic timing margin, sacrifice significant performance and computational efficiency, rendering them inadequate for high-performance AI computing demands. Current reliability-aware AI accelerator design faces two core challenges: (1) the lack of systematic cross-layer analysis tools to capture coupling reliability effects across device, circuit, architecture, and application layers; and (2) the fundamental trade-off between conventional reliability optimization and computational efficiency. To address these challenges, this paper systematically presents a series of reliability-aware accelerator designs, encompassing (1) aging and variation-aware dynamic timing analyzer, (2) accelerator dataflow optimization using critical input pattern reduction, and (3) resilience characterization and novel architecture design for large language models (LLMs). By tightly integrating cross-layer reliability modeling and AI workload characteristics, these co-optimization approaches effectively achieve reliable and efficient AI acceleration.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [8] [FlipFlop: A Static Analysis-based Energy Optimization Framework for GPU Kernels](https://arxiv.org/abs/2601.13345)
*Saurabhsingh Rajput,Alexander Brandt,Vadim Elisseev,Tushar Sharma*

Main category: cs.SE

TL;DR: FlipFlop是一个基于静态代码分析的框架，用于预测GPU内核能耗并推荐兼顾能耗与执行时间的帕累托最优线程块配置，无需运行时执行，准确率达83%，可减少93.4%优化搜索空间。


<details>
  <summary>Details</summary>
Motivation: 开发者缺乏硬件专业知识，难以高效优化GPU程序能耗，亟需自动化、低门槛的节能优化方案。

Method: 通过静态分析PTX代码，结合实时监控与可解释性指导，预测能耗并推荐帕累托最优配置。

Result: 在多头注意力等内核上实现最高79%能耗节省和106%吞吐量提升，相比NVIDIA启发式方法显著优化。

Conclusion: FlipFlop有效降低开发者优化负担，助力构建高性能且环境友好的GPU软件。

Abstract: Artificial Intelligence (AI) applications, such as Large Language Models, are primarily driven and executed by Graphics Processing Units (GPUs). These GPU programs (kernels) consume substantial amounts of energy, yet software developers often lack the hardware expertise and ad hoc knowledge required to optimize for power efficiency. We propose FlipFlop, a framework using static code analysis to predict energy consumption and recommend Pareto-optimal thread block configurations considering both power consumption and execution time. Our framework requires no runtime execution and analyzes PTX code, a low-level instruction set for CUDA-enabled GPUs. It is validated across a diverse set of GPUs and kernels, including multi-head attention, convolution, and matrix multiplication. FlipFlop achieves 83% accuracy in identifying locally optimal energy-efficient configurations, while also minimizing developer effort by reducing the optimization search space by 93.4%. For multi-head attention kernels, it yields up to 79% energy savings and 106% throughput gains relative to NVIDIA's occupancy heuristic. By integrating static analysis with real-time monitoring and providing explainable optimization guidance, FlipFlop empowers developers to create sustainable, high-performance GPU software which minimizes environmental and computational costs.

</details>


### [9] [The Llama 4 Herd: Architecture, Training, Evaluation, and Deployment Notes](https://arxiv.org/abs/2601.11659)
*Aaron Adcock,Aayushi Srivastava,Abhimanyu Dubey,Abhinav Jauhri,Abhinav Pande,Abhinav Pandey,Abhinav Sharma,Abhishek Kadian,Abhishek Kumawat,Adam Kelsey,Adam Stelle,Adeel Cheema,Adela Kabiljo,Adina Katz,Adithya Gangidi,Aditya Tayade,Adolfo Victoria,Adrian Samatan Alastuey,Adrien Conrath,Afroz Mohiuddin,Ahmed Sharif,Ahnaf Siddiqui,Ahuva Goldstand,Aijung Li,Aidan Boyd,Aidin Kazemi Daliri,Aisha Iqbal,Ajay Menon,Ajit Mathews,Akhil Mathur,Akshat Agarwal,Alan Schelten,Alana Shine,Alejandro Castillejo Muñoz,Aleksei Guliaev,Alex Radovic,Alex Song,Alex Vaughan,Alexander Simeonov,Alexandre Rezende,Alexandre Rezende,Alexei Baevski,Alexey Roubaud,Allen Ma,Alvin Lee,Alyssa Pereira,Aman Ahmed,Aman Shankar,Amanda Kallet,Amar Budhiraja,Ameya Khandekar,Amine Benhalloum,Amir Gershman,Amit Nagpal,Amit Zohar,Amr Sharaf,Anant Desai,Anastasia Razdaibiedina,Anca Agape,Andranik Kurghinyan,Andre Perunicic,Andrea Madotto,Andrei Darabanov,Andrés Alvarado,Andrew Brown,Andrew Cohen,Andrew Fang,Andrew Freeman,Andrew Gallagher,Andrew Gu,Andrew Prasetyo Jo,Andrew Ryan,Andrew Steffen,Andrew Wei,Andrey Rusakov,Andrii Golovei,Andy Shang,Angela Fan,Angela Fan,Angela Flewellen,Animesh Pathak,Anirudh Goyal,Ankit Ramchandani,Ankur Pai,Ankur Singh,Ankush Garg,Anlu Xing,Anna Cai,Anna Grosul,Anna Prochowska,Anna Sun,Annie Dong,Annie Franco,Anqi Hu,Anshul Chawla,Anthony Hartshorn,Antonia Sheng,Antony Thomas,Anuj Goyal,Anusha De,Anvit Bodiwala,Anvit Bodiwala,Aobo Yang,Aparajita Saraf,Apurva Samudra,Aran Mun,Arash Rahnama,Archi Mitra,Archie Sravankumar,Archit Gupta,Aria Haghighi,Ariel Stolerman,Arkabandhu Chowdhury,Arnab Choudhury,Artem Korenev,Arthur Guo,Arthur Hinsvark,Arun Mallya,Arvind Neelakantan,Arya Talebzadeh,Ashish Shah,Ashmitha Jeevaraj Shetty,Ashwin Bharambe,Asif Islam,Aston Zhang,Austen Gregerson,Avi Lewis,Aya Ibrahim,Ayaz Minhas,Ayelet Dahan,Ayelet Regev Dabah,Bangsheng Tang,Bar Ulman,Bardiya Sadeghi,Bartosz Jedrzejewski,Barys Skarabahaty,Beibei Zhu,Beibin Li,Ben Bharier,Benjamin Leonhardi,Benjamin Muller,Bennett Plessala,Bernie Huang,Beth Loyd,Bhargavi Paranjape,Bhavik Sheth,Bill Bonner,Bill Holland,Bill Wang,Bingzhe Liu,Binh Tang,Bo Liu,Bo Wu,Boduo Li,Bokai Yu,Bor-Chun Chen,Boris Araya,Boris Vidolov,Botao Chen,Boya Peng,Boyu Ni,Bradley Davis,Bram Wasti,Brandon Adams,Brandon Taylor,Brandon Wu,Brant Swidler,Brian Chiang,Brian Clerkin,Brian Fuller,Brooks Cutter,Bruno Novais,Bryan Gmyrek,Bysshe Easton,Cait Campos,Canaan Case,Carl Chengyan Fu,Carly Burton,Caro Diaz,Catherine Cole,Ce Liu,Cedric Fougerat,Cen Peng,Cen Peng,Cen Zhao,Changhan Wang,Changkyu Kim,Chantal Shaib,Chao Zhou,Charlotte Caucheteux,Chau Nguyen,Chawin Sitawarin,Chaya Nayak,Chelsea Asher,Chen Fan,Chen Zhu,Cheng Cheng,Cheng Zhang,Chenguang Zhu,Chengxiong Ruan,Chengzhu Yu,Chenheli Hua,Chenxi Whitehouse,Cheryl Holloway,Ching-Hsiang Chu,Ching-Yao Chuang,Chinmay Karande,Chirag Nagpal,Chloé Bakalar,Chloe Bi,Chris Cai,Chris Marra,Chris McConnell,Chris Thi,Chris Tindal,Chris Waterson,Christian Deverall,Christian Fuegen,Christian Keller,Christine Cheng,Christine Jou,Christine Smith,Christine Wang,Christoph Feichtenhofer,Christophe Touret,Christopher Luc,Christy Sauper,Chuanhao Zhuge,Chun-Yi Sung,Chunqiang Tang,Chunyang Wu,Clara Siegel,Cody Heale,Cody Wilbourn,Colin White,Congying Xia,Corinne Wong,Cornel Rat,Cristian Canton Ferrer,Cyrille Habis,Cyrus Nikolaidis,D Lohachov,Da Ju,Dalton Flanagan,Damien Allonsius,Damon Civin,Dan Johnson,Daniel Bolya,Daniel Francisco,Daniel Fried,Daniel Hawthorne,Daniel Haziza,Daniel Ho,Daniel Kreymer,Daniel Li,Daniel Machlab,Daniel McKinnon,Daniel Obenshain,Daniel Rodriguez,Daniel Song,Daniel Tse,Danielle Pintz,Danny Livshits,Daryl James Rodrigo,Dat Huynh,Daulet Askarov,David Brandfonbrener,David Esiobu,David Kant,David Levin,David Renardy,David Soofian,David Stevens,David Xu,David Zhang,Deep Shah,Delia David,Demi Douglas,Denis Boyda,Desh Raj,Devamanyu Hazarika,Dheeraj Mekala,Dhruv Choudhary,Dhruv Mahajan,Di Jin,Didac Suris Coll-Vinent,Didem Foss,Diego Garcia-Olano,Diego Perino,Dieuwke Hupkes,DiJia Su,Dilip Madathil,Dinesh Govindasamy,Dinesh Yeduguru,Dmitry Vengertsev,Dong He,Dong Li,Dong Wang,Dongzhuo Li,Duc Le,Dunant Hin,Dustin Holland,Duy Nguyen,Duy Nguyen,Ed Dowling,Eden Litt,Egor Lakomkin,Ehab AlBadawy,Ehsan K. Ardestani,Elad Eckstein,Elahe Dabir,Elaine Montgomery,Elina Lobanova,Elior Abramoviz,Eliot Hedeman,Elissa Li,Elizabeth Hilbert,Ellen Xiaoqing Tan,Elliot Yun,Elodie Stener,Emilian Stoimenov,Emilien Garreau,Emily Dinan,Emily Hahn,Emily Wood,Emma Li,Emmanuel Ademuwagun,Emrah Seker,Eric Alamillo,Eric Gan,Eric Han,Eric Huang,Eric Michael Smith,Eric-Tuan Le,Ernie Chang,Eryk Helenowski,Eslam Elnikety,Esteban Arcaute,Ethan Myers,Eugene Nho,Eugene Poliukhovych,Evan Dunbar,Evgeniy Litvinenko,Evrim Altıntaş,Eyal Hochman,Eyal Shtrauch,Fabian Mastenbroek,Faiza Zeb,Faizan Ahmad,Farhad Farahbakhshian,Fei Kou,Fei Sun,Feiyu Chen,Felix Chung,Feng Tian,Feng Xu,Filip Radenovic,Filippos Kokkinos,Francesco Barbieri,Francesco Caggioni,Francisco Esparza,Francisco Guzmán,Frank Kanayet,Frank Seide,Frank Zhang,Fred Lewis,Freda Huang,Fulton Wang,Gabriel Synnaeve,Gabriela Jacques-Silva,Gabriella Schwarz,Gaganjit Ghardhora,Gal Elfer,Garrett Dickson,Gaurav Chaurasia,Gautam Sewani,Geet Shingi,Gefei Zuo,Geonhwa Jeong,George Puthanpurackal,Georgia Swee,Gerard Moreno-Torres Bertran,Gil Keren,Gina Ling,Gjergji Stasa,Gobinda Saha,Gor Safran,Gordy French,Goutham Rajendran,Govind Thattai,Grace Cineas,Graeme Nail,Greg Fletcher,Grégoire Mialon,Griffin Adams,Grigory Sizov,Guan Pang,Hady Elsahar,Hai Dang Tran,Hailey Nguyen,Haiping Wu,Hakan Inan,Hamid Eghbalzadeh,Han Fang,Han Zou,Hannah Doyle,Hannah Korevaar,Hannah Wang,Hannah Werbel,Hanwen Zha,Hany Morsy,Hao Ma,Haoci Zhang,Haonan Sun,Haozhu Wang,Hardik Shah,Haroun Habeeb,Harrison Rudolph,Harsh Gupta,Harsh Poddar,Harshil Parikh,Hejia Zhang,Heming Wang,Hengduo Li,Himanshu Sharma,Hoang Phi Nguyen,Hongbo Zhang,Honghao Qiu,Hongjiang Lv,Hongli Xu,Hongyuan Zhan,Hossein Hamooni,Howard Huang,Hu Xu,Hugo Laurençon,Hugo Touvron,Hung Dinh,Hunter Goldman,Hussein Mehanna,Huy Nguyen,Hweimi Tsuo,Ian Graves,Ian Yu,Ibrahim Damlaj,Idan Cohen,Igor Tufanov,Ilan Goldenstein,Ilias Leontiadis,Iliyan Zarov,Imad Ahmed,Innocent Djiofack,Iosif Spulber,Irina-Elena Veliche,Isabella Ramos,Ishan Misra,Itai Gal,Ivan Evtimov,Ivan Evtimov,Ivan Obraztsov,Jack Wu,Jacqueline Romero Vertino,Jaemo Koo,Jaewon Lee,Jake Jung,Jake Weissman,James Beldock,James Crnkovich,James Grinage,James Hongyi Zeng,James Kohli,James Tian,Jamie Cahill,Jan Geffert,Jan Seidel,Jan Seidel,Janey Tracey,Jang Hyun Cho,Janice Wei,Jarrod Kahn,Jasmyn Howell,Jason Long Vu,Jason Park,Jason Yan,Jason Yip,Jay Li,Jay Mahadeokar,Jaya Bharath R Goluguri,Jayasi Mehar,Jean-Baptiste Gaya,Jeet Shah,Jeff Hanson,Jeff Marcus,Jeff Walsh,Jeff Yang,Jelmer van der Linde,Jemma Fan,Jennifer Chan,Jenny Zhen,Jenya Lee,Jeremy Fu,Jeremy Reizenstein,Jeremy Teboul,Jesse He,Jessica Zhong,Ji Hou,Ji Yang,Jia Ding,Jiabo Hu,Jiacheng Zhu,Jiadong Guo,Jialiang Wang,Jialin Ouyang,Jianfeng Chi,Jianyu Huang,Jianyun Zhao,Jiaowen Yang,Jiatong Zhou,Jiawei Zhao,Jiawen Liu,Jie Wang,Jie You,Jiecao Yu,Jillian Schwiep,Jilong Wu,Jing Huang,Jing Li,Jing Yu Koh,Jing Zhang,Jingxiang Chen,Jingyi Yang,Jingyue Shen,Jinho Hwang,Jinxi Guo,Jiwan Khatiwada,Joanna Bitton,Joe Li,Joe Quanaim,Joel Beales,Johan Schuijt,John Chang,John Quan,Johnnie Chan,Jon Shepard,Jona Harris,Jonah Rubin,Jonathan Janzen,Jonathan Kaldor,Jorge Lopez Silva,Jose Leitao,Joseph Greer,Joseph Moon,Joseph Rocca,Joseph Tighe,Josh Fromm,Joshua Deng,Joshua Fernandes,Joshua Saxe,Joyce Zheng,Juan Pino,Julien Prigent,Jun Chen,Junjiao Tian,Junjie Qi,Junjie Wang,Junteng Jia,Kade Baker,Kai Londenberg,Kai Wang,Kainan Peng,Kaiyan Peng,Kaiyue Yang,Kalyan Vasudev Alwala,Kam Hou Yu,Kanika Narang,Karan Chadha,Karan Sikka,Karen Zhang,Karina Schuberts,Karishma Mandyam,Karthik Abinav Sankararaman,Karthik Padthe,Karthik Prasad,Karthik Sivakumar,Kartikeya Upasani,Kate Plawiak,Kate Saenko,Kateřina Žmolíková,Kathryn Stadler,Kathy Matosich,Katie Doulgass,Kaveh Hassani,Kay Ji,Ke Li,Kenneth Heafield,Kenny Yu,Keqian Li,Kevin Chih-Yao Ma,Kevin Hannan,Keyu Man,Kezhen Chen,Khalid El-Arini,Khrystyna Hutsulyak,Kieran Nash,Kiran Jagadeesh,Kody Bartelt,Konstantin Topaloglou-Mundy,Konstantinos Chatziioannou,Konstantinos Karanasos,Konstantinos Vougioukas,Kostas Tsiampouris,Kristen Hamill,Kristy Choi,Krithika Iyer,Kshitiz Malik,Kuenley Chiu,Kun Huang,Kunal Bhalla,Kunal Chawla,Kunpeng Li,Kushal Lakhotia,Kyle Monk,Lakshya Garg,Lalit Chourey,Lars Hamre,Laura Gustafson,Lauren Deason,Laurence Rouesnel,Laurens van der Maaten,Lavender A,Lawrence Chen,Lawrence Jang,Leandro Silva,Leda Sari,Lee Hetherington,Lei Zhang,Leiyu Zhao,Lele Chen,Leo Chenghui Li,Leon Yang,Leon Zhan,Levi Corallo,Liang Tan,Licheng Yu,Lijuan Liu,Lilach Mor,Lincoln Lin,Linfeng Li,Lisa Titus,Liz Jenkins,Lovish Madaan,Lu Fang,Lu Yuan,Lucas Nava,Lucas Pasqualin,Lucas Switzer,Lucia Fang,Lucy Sun,Luka Tadic,Lukas Blecher,Lukas Landzaat,Luxin Zhang,Madhavi Rao,Madian Khabsa,Mahalia Miller,Mahendra Kariya,Mahesh Pasupuleti,Mahi Luthra,Manaal Faruqui,Manav Avlani,Manchen Wang,Mannat Singh,Manohar Paluri,Manoj Chakkaravarthy,Manoj Nair,Maquelle Tiffany,Marcin Pawlowski,Marcus Wu,Maria Lomeli,Mario Consuegra,Marion Boiteux,Marios Andreas Galanis,Marshall Chen,Martin Gleize,Maryam Fazel-Zarandi,Matan Hasson,Mathew Oldham,Mathieu Rita,Matt Dordal,Matt Setzler,Matt Staats,Matt Staats,Matt Wilde,Matthew Clark,Matthew Grange,Matthew Lennie,Matthew Schmohl,Max Raphael,Maxim Naumov,Maxim Samoylov,Maxime Lecanu,Maya Pavlova,Md Taha Bin Jawaid,Meghan Keneally,Melanie Kambadur,Meng Zhang,Mengchen Liu,Mengdi Lin,Mengjiao Wang,Mervyn Abraham,Miao Liu,Michael Au-Yeung,Michael Feldergraf,Michael Man,Michael Matheny,Michael Suo,Michael Tontchev,Michel Meyer,Michelle Ma,Mihir Patel,Mihir Sanjay Kale,Mik Vyatskov,Mikayla Alexander,Mike Andersland,Mike Clark,Mike Lewis,Mike Li,Mike Macey,Mike Macey,Mike Seltzer,Mikel Jimenez Fernandez,Mikhail Antonov,Mikhail Plekhanov,Milan Zhou,Min Si,Ming Qiao,Mingbo Ma,Mingjun Zhang,Mingyi Liang,Miquel Jubert Hermoso,Mirac Suzgun,Mirjam Skarica,Mitesh Kumar Singh,Mohammad Kabbani,Mohammad Rastegari,Mona Sarantakos,Monica Sim,Monika Gangapuram,Mor Moshe,Morrie Doulaty,Morvarid Metanat,Moya Chen,Mrinal Kumar,Munish Bansal,Murali Ramarao,Na Li,Nadav Azaria,Nahiyan Malik,Naman Goyal,Nancy Vargas Balderas,Nanshu Wang,Naoyuki Kanda,Natalia Gimelshein,Natalia Neverova,Nathan Aclander,Natt Sithiviraporn,Navneet Madhu Kumar,Ned Newton,Neeraj Bahl,Negar Ghorbani,Neil Patel,Neta-lee Golan,Nicholas Longenbaugh,Nick Egebo,Nikhil Johri,Nikhil Mehta,Nikhil Naik,Niko Moritz,Nikolay Bashlykov,Nikolay Bogoychev,Nikolay Pavlovich Laptev,Niladri Chatterji,Nile Jones,Nimish Shah,Ning Dong,Ning Li,Ning Li,Ning Zhang,Nishant Yadav,Noam Paz,Norman Cheng,Norman Cheng,Olaoluwa Adesanya,Oleg Repin,Oleksandr Maksymets,Omkar Salpekar,Omri Harosh,Onkar Pednekar,Onur Çelebi,Oran Gafni,Oren Edinger,Osama Hanna,Owais Khan Mohammed,Ozlem Kalinli,Paden Tomasello,Pankaj Singh,Paola Quevedo,Parag Jain,Paria Rashidinejad,Parker Tooley,Parth Parekh,Parth Thakkar,Parvin Taheri,Pasan Hapuarachchi,Pascal Kesseli,Patrick Alrassy,Paulo de Rezende Pinatti,Pavan Balaji,Pawan Sisodiya,Pedro Jose Ferreira Moreira,Pedro Rittner,Pedro Valenzuela,Peize Sun,Peizhao Zhang,Peng-Jen Chen,Pengchao Wang,Pengchuan Zhang,Pengwei Li,Petar Vasic,Peter Carras,Peter Ney,Peter Weng,Petru Dumea,Phil Hayes,Philip Woods,Pierre Andrews,Pierre Ménard,Ping-Hao Wu,Pingchuan Liu,Piotr Dollar,Plamen Dzhelepov,Polina Zvyagina,Posten A,Prabhav Agrawal,Pradhapan Rajendran,Pradyot Prakash,Prajjwal Bhargava,Pramono,Pranay Shah,Pranshu Dave,Prash Jain,Pratik Dubal,Praveen Gollakota,Praveen Krishnan,Pritish Yuvraj,Projjal Ghosh,Punit Singh Koura,Puxin Xu,Qi Qi,Qi Zhou,Qian Guan,Qian Sun,Qiang Liu,Qing He,Qinqing Zheng,Qirui Yang,Qizhen Guo,Quanzeng You,Quentin Carbonneaux,Quentin Carbonneaux,Quentin Duval,Quintin Fettes,Rachad Alao,Rachel Batish,Rachel Guo,Rachel Rodriguez,Radhika Bhargava,Rafael Asuncion,Raghotham Murthy,Rahul Dutta,Rahul Jha,Rahul Kindi,Rahul Mitra,Raj Ganapathy,Raj Shah,Rajarshi Das,Rajat Shrivastava,Rajesh Nishtala,Ramakant Shankar,Raman Shukhau,Ramon Calderer,Rangaprabhu Parthasarathy,Ranjan Subramanian,Raphael Bensadoun,Rares Bostan,Rashnil Chaturvedi,Ravi Agrawal,Ray Gao,Raymond Li,Rebecca Kogen,Ricardo Juan Palma Duran,Ricardo Silveira Cabral,Richard Lee,Richard Yuanzhe Pang,Riddhish Bhalodia,Riham Mansour,Rishabh Singh,Rishi Godugu,Ritun Patney,Rob Boyle,Robbie Goldfarb,Robert Caldwell,Robert Kuo,Roberta Raileanu,Robin Battey,Robin Sharma,Rochit Sapra,Rocky Wang,Rodolfo Granata,Rodrigo De Castro,Rodrigo Paim,Rohan Maheshwari,Rohan Varma,Rohit Girdhar,Rohit Patel,Roshan Sumbaly,Roy Sheaffer,Ruan Silva,Ruben Rodriguez Buchillon,Rui Hou,Ruiming Xie,Ruslan Mavlyutov,Ruslan Semenov,Rustam Dinov,Ruxiao Bao,Ryan Fox,Ryan Kilpatrick,Ryan Kwan,Ryan Lim,Ryan Smith,Saaketh Narayan,Sabrina Qiao,Sachin Mehta,Sachin Siby,Sagar Jain,Saghar Hosseini,Sagie Gur-Ari,Sahana Chennabasappa,Sahin Geyik,Sai Jayesh Bondu,Sai Mounika Chowdhary Nekkalapudi,Saif Hasan,Saisuke Okabayashi,Saketh Rambhatla,Salil Sawhney,Sam Dunster,Sam Zhao,Saman Keon,Samaneh Azadi,Sameet Sapra,Samuel Dooley,Samyak Datta,Sandeep Parab,Sang Michael Xie,Sanjay Singh,Sanyuan Chen,Sara Behn,Sara Khodeir,Sarah Shirazyan,Sargun Dhillon,Sarunya Pumma,Sasha Sidorov,Saskia Adaime,Saurabh Khanna,Sayem Wani,Scott Brenton,Sean Bell,Sean Kelly,Sean Koger,Sean Nunley,Sean Perry,Sebastian Caicedo,Sebastian Dahlgren,Sebastian Ruder,Seiji Yamamoto,Selam Mehretu,Selvan Sunitha Ravi,Sen Lyu,Senthil Chellapan,Serafeim Mellos,Sergey Edunov,Sergey Royt,Shaina Cohen,Shangfu Peng,Shannon Adams,Shaoliang Nie,Sharadh Ramaswamy,Sharan Narang,Shashank Pisupati,Shashi Gandham,Shaun Lim,Shaun Lindsay,Sheena Artrip,Shelly Sheynin,Shen Yan,Sheng Feng,Sheng Shen,Shengbao Zheng,Shenghao Lin,Shengjie Bi,Shengxin Cindy Zha,Shengye Wan,Shengyi Qian,Shengyong Cai,Shengzhi Shao,Shervin Shahidi,Shikai Li,Shimon Bernholtz,Shiqi Wang,Shishir G. Patil,Shiv Verma,Shiva Shankar P,Shiyang Chen,Sho Yaida,Shoubhik Debnath,Shreyas Siravara,Shruti Bhosale,Shuang Ma,Shun Zhang,Shuo Tang,Shuqiang Zhang,Shuyan Zhou,Sicong Che,Sidd Srinivisan,Siddharth Bhattacharya,Siddharth Patki,Sijia Chen,Sili Chen,Simon Vandenhende,Simone Merello,Sinong Wang,Sivan Barzily,Sixian Yi,Siyu Lin,SK Bong,Sky Yin,Sneha Agarwal,Sneha Agarwal,Soerian Lieve,Soji Sajuyigbe,Song Jiang,Songlin Li,Sonia Kim,Sopan Khosla,Soumi Maiti,Spencer Whitman,Sravya Popuri,Sreen Tallam,Srinivas Vaidyanathan,Srinivas Vaidyanathan,Sten Sootla,Stephane Collot,Stephanie Ding,Stephen Chen,Steven Cai,Suchin Gururangan,Sudarshan Govindaprasad,Sue Young,Suganthi Dewakar,Sujan Kumar Gonugondla,Sujeet Bhandari,Suman Gumudavelli,Suman Gumudavelli,Sumit Gupta,Summer Deng,Sungmin Cho,Suresh Ganapathy,Surjyendu Dhal,Susan Fedynak,Susana Contrera,Suyoun Kim,Sylvestre Rebuffi,Takshak Chahande,Tamar Herman,Tan Li,Tao Xu,Tara Fowler,Tarek Sheasha,Tarun Anand,Tarun Kalluri,Tarun Singh,Tatiana Shavrina,Ted Li,Teja Rao,Tejas Patil,Teng Li,Thach Bui,Thai Quach,Thamer Alharbash,Thanh Vinh Vo,Thawan Kooburat,Thilo Koehler,Thomas Georgiou,Thomas Scialom,Tian Ye,Tianhe Li,Tianjun Zhang,Tianyu Li,Tijmen Blankevoort,Timon Willi,Timothy Chou,Timothy Leung,TJ Lee,Todor Mihaylov,Tom Heatwole,Tong Xiao,Tony Cao,Tony Lee,Trang Le,Tristan Rice,Tsz Kei Serena Chan,Tuan Tran,Tudor Tiplea,Tyler Baumgartner,Uday Savagaonkar,Ujjwal Karn,Ulises Martinez Araiza,Umar Farooq,Uriel Cohen,Usman Sharif,Utkarsh Murarka,Van Phung,Varun Joginpalli,Varun Saravagi,Vasu Sharma,Vasudha Viswamurthy,Vedanuj Goswami,Vedika Seth,Venkat Ramesh,Venkat Ramesh,Vibhor Gupta,Victoria Montanez,Vidhya Natarajan,Vidya Sarma,Vignesh Ramanathan,Viktor Kerkez,Vinay Rao,Vincent Gonguet,Vincent Mauge,Virginie Do,Vish Vogeti,Vishrav Chaudhary,Viswesh Sankaran,Vítor Albiero,Vivek Miglani,Vivek Pai,Vlad Cojanu,Vlad Shubin,Vlad Tiberiu Mihailescu,Vladan Petrovic,Vladimir Ivanov,Vladislav Vorotilov,Vrushali Bhutada,Wai I Ng,Wei Cheng,Wei Sun,Wei Tu,Wei Wei,Wei Zhou,Wei-Ning Hsu,Weiwei Chu,Weizhe Yuan,Wenchen Wang,Wenjun Zhao,Wenwen Jiang,Wenyin Fu,Wenzhe Jiang,Whitney Meers,Will Constable,Will Wang,William R. Wong,Xavier Martinet,Xi Victoria Lin,Xi Yan,Xi Yin,Xian Li,Xianfeng Rui,Xianjun Yang,Xiaocheng Tang,Xiaodong Wang,Xiaofang Wang,Xiaolan Wang,Xiaoliang Dai,Xiaoliang Peng,Xiaopeng Li,Xiaozhu Meng,Xibei Zhang,Xide Xia,Xin Jin,xinbo Gao,Xinfeng Xie,Xingyi Zhou,Xu Ma,Xuan Ju,Xuanyi Zhao,Xubo Liu,Xuchao Jia,Xuedong Zhang,Xuefei Cao,Xuewei Wang,Xuewei Wu,Xunnan Xu,Xutai Ma,Xuyang Wang,Yan Cui,Yang Chen,Yang Li,Yang Shu,Yang Xia,Yanjun Chen,Yanjun Zhou,Yash Mehta,Yash Patel,Yash Tekena,Yashesh Gaur,Yasmine Babaei,Yaxuan Zhou,Ye Hu,Ye Qi,Yejin Lee,Yeming Wen,Yen-Cheng Liu,Yexin Bruce Wu,Yi Pan,Yi Yang,Yi-Hui Lin,Yifan Wang,Yifan Wu,Yifan Yang,Yifei Huang,Yiftah Ben Aharon,Yilin Yang,Yiling You,Ying Xu,Ying Zhang,Yingquan Yuan,Yingru Liu,Yingyi Ma,Yining Yang,Yiting Lu,Yonatan Komornik,Yongjie Lin,Yoni Goyhman,Yossi Moran Mamo,Youngjin Nam,Yu Wang,Yu Lu,Yu Zhao,Yu-Ho Hsieh,Yu-Jung Lo,Yuandong Tian,Yuanhan Zhang,Yuanhao Xiong,Yuanshun Yao,Yuchen Hao,Yuchen Zhang,Yuchuan Li,Yue Cao,Yue Yu,Yue Zhao,Yuhan Guo,Yuhao Wang,Yuheng Huang,Yujie Lu,Yujun Shi,Yulun Wang,Yun He,Yun Wang,Yundi Qian,Yunfan Wang,Yunhao Tang,Yuning Mao,Yunlu Li,Yuqi Dai,Yuriy Hulovatyy,Yushi Hu,Yuxuan Sun,Zach Rait,Zach Wentz,Zacharie Delpierre Coudert,Zachary Collins,Zahra Hankir,Zecheng He,Zeeshan Ahmed,Zeeshan Ahmed,Zef RosnBrick,Zhan Shu,Zhanna Rohalska,Zhaoduo Wen,Zhe Liu,Zhe Liu,Zhen Qiao,Zhenggang Xu,Zhengwen Zhou,Zhengxing Chen,Zhenyu Tang,Zhichen Wu,Zhicheng Ouyang,Zhihong Lei,Zhipeng Hong,Zhiping Xiu,Zhiwei Zhao,Zhong Meng,Zhou Jin,Zhouhao Zeng,Zichang Liu,Zihang Meng,Zihuan Qiao,Zinnia Zheng,Zixi Qi,Ziyi Luo,Zoe Foulkes Birkhead,Zoey Sun,Zohar Achdut*

Main category: cs.SE

TL;DR: 本文档汇总了Meta Llama 4模型家族的公开技术细节，涵盖架构、训练方法、基准测试结果及部署限制，旨在为研究人员提供精准技术参考。


<details>
  <summary>Details</summary>
Motivation: 为研究人员和从业者提供关于Llama 4模型家族的精确、有据可查的技术信息。

Method: 整理并分析公开发布的Llama 4技术文档，包括模型变体、架构设计、训练流程、基准结果及部署约束。

Result: 系统归纳了Llama 4的模型结构、训练策略、性能表现与实际部署限制，并总结了相关许可与安全评估实践。

Conclusion: 本文为Llama 4提供了紧凑且权威的技术参考，便于研究者快速掌握其核心特性与使用条件。

Abstract: This document consolidates publicly reported technical details about Metas Llama 4 model family. It summarizes (i) released variants (Scout and Maverick) and the broader herd context including the previewed Behemoth teacher model, (ii) architectural characteristics beyond a high-level MoE description covering routed/shared-expert structure, early-fusion multimodality, and long-context design elements reported for Scout (iRoPE and length generalization strategies), (iii) training disclosures spanning pre-training, mid-training for long-context extension, and post-training methodology (lightweight SFT, online RL, and lightweight DPO) as described in release materials, (iv) developer-reported benchmark results for both base and instruction-tuned checkpoints, and (v) practical deployment constraints observed across major serving environments, including provider-specific context limits and quantization packaging. The manuscript also summarizes licensing obligations relevant to redistribution and derivative naming, and reviews publicly described safeguards and evaluation practices. The goal is to provide a compact technical reference for researchers and practitioners who need precise, source-backed facts about Llama 4.

</details>


### [10] [From Everything-is-a-File to Files-Are-All-You-Need: How Unix Philosophy Informs the Design of Agentic AI Systems](https://arxiv.org/abs/2601.11672)
*Deepak Babu Piskala*

Main category: cs.SE

TL;DR: 本文探讨了当代智能体AI中类似Unix‘一切皆文件’的抽象统一趋势，提出以文件和代码为中心的交互模型可提升系统的可维护性与鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 受Unix系统‘一切皆文件’理念启发，探索如何在智能体AI中实现资源接口的统一与简化。

Method: 通过追溯从Unix到DevOps、基础设施即代码再到自主软件智能体的演进路径，分析文件式抽象与代码规范如何整合异构资源。

Result: 发现采用文件与代码为中心的交互模型有助于构建更易维护、可审计且运行稳健的智能体系统。

Conclusion: 文件与代码抽象在智能体AI中的推广，有望带来类似Unix时代接口统一的操作优势。

Abstract: A core abstraction in early Unix systems was the principle that 'everything is a file', enabling heterogeneous devices and kernel resources to be manipulated via uniform read/write interfaces. This paper explores how an analogous unification is emerging in contemporary agentic AI. We trace the evolution from Unix to DevOps, Infrastructure-as-Code, and finally autonomous software agents, highlighting how file-like abstractions and code-based specifications collapse diverse resources into consistent, composable interfaces. The resulting perspective suggests that adopting file- and code-centric interaction models may enable agentic systems that are more maintainable, auditable, and operationally robust.

</details>


### [11] [Semantic Caching and Intent-Driven Context Optimization for Multi-Agent Natural Language to Code Systems](https://arxiv.org/abs/2601.11687)
*Harmohit Singh*

Main category: cs.SE

TL;DR: 该论文提出了一种生产优化的多智能体系统，用于将自然语言查询高效转换为结构化数据分析的Python代码。


<details>
  <summary>Details</summary>
Motivation: 降低依赖昂贵前沿模型的成本，同时保持高准确率和效率，以支持企业级大规模部署。

Method: 采用语义缓存、双阈值决策机制和意图驱动的动态提示组装三项关键技术。

Result: 在企业库存管理中部署，处理超1万次查询，平均延迟8.2秒，语义准确率达94.3%，缓存命中率67%，节省40-60%令牌消耗。

Conclusion: 该系统在成本、性能与准确性之间取得良好平衡，适合大规模LLM分析系统实际部署。

Abstract: We present a production-optimized multi-agent system designed to translate natural language queries into executable Python code for structured data analytics. Unlike systems that rely on expensive frontier models, our approach achieves high accuracy and cost efficiency through three key innovations: (1) a semantic caching system with LLM-based equivalence detection and structured adaptation hints that provides cache hit rates of 67% on production queries; (2) a dual-threshold decision mechanism that separates exact-match retrieval from reference-guided generation; and (3) an intent-driven dynamic prompt assembly system that reduces token consumption by 40-60% through table-aware context filtering. The system has been deployed in production for enterprise inventory management, processing over 10,000 queries with an average latency of 8.2 seconds and 94.3% semantic accuracy. We describe the architecture, present empirical results from production deployment, and discuss practical considerations for deploying LLM-based analytics systems at scale.

</details>


### [12] [SpecMap: Hierarchical LLM Agent for Datasheet-to-Code Traceability Link Recovery in Systems Engineering](https://arxiv.org/abs/2601.11688)
*Vedant Nipane,Pulkit Agrawal,Amit Singh*

Main category: cs.SE

TL;DR: 提出一种分层的嵌入式系统数据手册到代码映射方法，结合大语言模型语义分析与多级抽象结构，显著提升映射准确率并降低计算开销。


<details>
  <summary>Details</summary>
Motivation: 解决传统基于词汇相似性的追溯方法在嵌入式系统中难以捕捉语义、结构和符号级关系的问题。

Method: 采用分层策略：先推断仓库结构，再估计文件相关性，最后进行细粒度符号对齐，覆盖函数、宏、结构体等系统级元素。

Result: 在多个开源项目上实现最高73.3%的文件映射准确率，LLM令牌消耗减少84%，端到端运行时间缩短约80%。

Conclusion: 该方法支持大规模嵌入式系统的自动化分析，适用于训练数据生成、合规验证和规范覆盖率分析等下游任务。

Abstract: Establishing precise traceability between embedded systems datasheets and their corresponding code implementations remains a fundamental challenge in systems engineering, particularly for low-level software where manual mapping between specification documents and large code repositories is infeasible. Existing Traceability Link Recovery approaches primarily rely on lexical similarity and information retrieval techniques, which struggle to capture the semantic, structural, and symbol level relationships prevalent in embedded systems software. We present a hierarchical datasheet-to-code mapping methodology that employs large language models for semantic analysis while explicitly structuring the traceability process across multiple abstraction levels. Rather than performing direct specification-to-code matching, the proposed approach progressively narrows the search space through repository-level structure inference, file-level relevance estimation, and fine-grained symbollevel alignment. The method extends beyond function-centric mapping by explicitly covering macros, structs, constants, configuration parameters, and register definitions commonly found in systems-level C/C++ codebases. We evaluate the approach on multiple open-source embedded systems repositories using manually curated datasheet-to-code ground truth. Experimental results show substantial improvements over traditional information-retrieval-based baselines, achieving up to 73.3% file mapping accuracy. We significantly reduce computational overhead, lowering total LLM token consumption by 84% and end-to-end runtime by approximately 80%. This methodology supports automated analysis of large embedded software systems and enables downstream applications such as training data generation for systems-aware machine learning models, standards compliance verification, and large-scale specification coverage analysis.

</details>


### [13] [Technical Lag as Latent Technical Debt: A Rapid Review](https://arxiv.org/abs/2601.11693)
*Shane K. Panter,Nasir U. Eisty*

Main category: cs.SE

TL;DR: 本文综述技术滞后研究，提出其作为隐性技术债务指标的潜力，并呼吁改进检测方法与标准化度量。


<details>
  <summary>Details</summary>
Motivation: 厘清技术滞后的定义、成因与影响，推动其作为被动积累技术债务的有效指标。

Method: 采用快速回顾法结合滚雪球策略，从ACM、IEEE、Scopus和Springer等数据库筛选相关研究。

Result: 技术滞后常因缺乏有效检测工具而被忽视，导致依赖过时、API陈旧等问题；管理策略包括自动化更新与持续集成。

Conclusion: 完善度量标准与实证研究，可显著提升对外部依赖代码库的维护能力，并为未来研究指明方向。

Abstract: Context: Technical lag accumulates when software systems fail to keep pace with technological advancements, leading to a deterioration in software quality. Objective: This paper aims to consolidate existing research on technical lag, clarify definitions, explore its detection and quantification methods, examine underlying causes and consequences, review current management practices, and lay out a vision as an indicator of passively accumulated technical debt. Method: We conducted a Rapid Review with snowballing to select the appropriate peer-reviewed studies. We leveraged the ACM Digital Library, IEEE Xplore, Scopus, and Springer as our primary source databases. Results: Technical lag accumulates passively, often unnoticed due to inadequate detection metrics and tools. It negatively impacts software quality through outdated dependencies, obsolete APIs, unsupported platforms, and aging infrastructure. Strategies to manage technical lag primarily involve automated dependency updates, continuous integration processes, and regular auditing. Conclusions: Enhancing and extending the current standardized metrics, detection methods, and empirical studies to use technical lag as an indication of accumulated latent debt can greatly improve the process of maintaining large codebases that are heavily dependent on external packages. We have identified the research gaps and outlined a future vision for researchers and practitioners to explore.

</details>


### [14] [Improved Bug Localization with AI Agents Leveraging Hypothesis and Dynamic Cognition](https://arxiv.org/abs/2601.12522)
*Asif Mohammed Samir,Mohammad Masudur Rahman*

Main category: cs.SE

TL;DR: CogniGent是一种新型多智能体技术，通过因果推理与调用图分析显著提升软件缺陷定位效果。


<details>
  <summary>Details</summary>
Motivation: 传统缺陷定位方法忽视代码组件间关联，现有LLM缺乏因果推理能力且上下文管理不足。

Method: 引入多AI智能体，结合因果推理、调用图根因分析与上下文工程，模拟开发者动态认知调试过程。

Result: 在591个缺陷报告上评估，MAP提升23.33%-38.57%，MRR提升25.14%-53.74%，统计显著优于基线方法。

Conclusion: CogniGent有效克服现有方法局限，将类人认知与智能体自动化结合，推动缺陷定位技术发展。

Abstract: Software bugs cost technology providers (e.g., AT&T) billions annually and cause developers to spend roughly 50% of their time on bug resolution. Traditional methods for bug localization often analyze the suspiciousness of code components (e.g., methods, documents) in isolation, overlooking their connections with other components in the codebase. Recent advances in Large Language Models (LLMs) and agentic AI techniques have shown strong potential for code understanding, but still lack causal reasoning during code exploration and struggle to manage growing context effectively, limiting their capability. In this paper, we present a novel agentic technique for bug localization -- CogniGent -- that overcomes the limitations above by leveraging multiple AI agents capable of causal reasoning, call-graph-based root cause analysis and context engineering. It emulates developers-inspired debugging practices (a.k.a., dynamic cognitive debugging) and conducts hypothesis testing to support bug localization. We evaluate CogniGent on a curated dataset of 591 bug reports using three widely adopted performance metrics and compare it against six established baselines from the literature. Experimental results show that our technique consistently outperformed existing traditional and LLM-based techniques, achieving MAP improvements of 23.33-38.57% at the document and method levels. Similar gains were observed in MRR, with increases of 25.14-53.74% at both granularity levels. Statistical significance tests also confirm the superiority of our technique. By addressing the reasoning, dependency, and context limitations, CogniGent advances the state of bug localization, bridging human-like cognition with agentic automation for improved performance.

</details>


### [15] [The Stability Trap: Evaluating the Reliability of LLM-Based Instruction Adherence Auditing](https://arxiv.org/abs/2601.11783)
*Murtuza N. Shergadwala*

Main category: cs.SE

TL;DR: 研究揭示LLM作为评判者在评估生成式AI系统指令时存在‘稳定性陷阱’，即高判决一致性掩盖了低推理一致性，建议将可验证逻辑交由代码处理，仅让LLM负责语义复杂任务。


<details>
  <summary>Details</summary>
Motivation: 探索LLM作为评判者在企业治理中评估不同类型系统指令的稳定性问题，以提升审计机制的可靠性。

Method: 提出Scoped Instruction Decomposition Framework框架，将指令分类为客观与主观类型，并在HR场景下测试四种评判架构的稳定性。

Result: 发现评判者在判决上高度一致（>99%），但推理一致性极低（最低≈19%），尤其在数值分析和特征检查中表现不稳定。

Conclusion: 高判决稳定性可能掩盖脆弱的推理过程，建议自动化审计协议应限定LLM仅用于复杂语义评估，其余交由确定性代码执行。

Abstract: The enterprise governance of Generative AI (GenAI) in regulated sectors, such as Human Resources (HR), demands scalable yet reproducible auditing mechanisms. While Large Language Model (LLM)-as-a-Judge approaches offer scalability, their reliability in evaluating adherence of different types of system instructions remains unverified. This study asks: To what extent does the instruction type of an Application Under Test (AUT) influence the stability of judge evaluations? To address this, we introduce the Scoped Instruction Decomposition Framework to classify AUT instructions into Objective and Subjective types, isolating the factors that drive judge instability. We applied this framework to two representative HR GenAI applications, evaluating the stability of four judge architectures over variable runs. Our results reveal a ``Stability Trap'' characterized by a divergence between Verdict Stability and Reasoning Stability. While judges achieved near-perfect verdict agreement ($>99\%$) for both objective and subjective evaluations, their accompanying justification traces diverged significantly. Objective instructions requiring quantitative analysis, such as word counting, exhibited reasoning stability as low as $\approx19\%$, driven by variances in numeric justifications. Similarly, reasoning stability for subjective instructions varied widely ($35\%$--$83\%$) based on evidence granularity, with feature-specific checks failing to reproduce consistent rationale. Conversely, objective instructions focusing on discrete entity extraction achieved high reasoning stability ($>90\%$). These findings demonstrate that high verdict stability can mask fragile reasoning. Thus, we suggest that auditors scope automated evaluation protocols strictly: delegate all deterministically verifiable logic to code, while reserving LLM judges for complex semantic evaluation.

</details>


### [16] [Changes in Coding Behavior and Performance Since the Introduction of LLMs](https://arxiv.org/abs/2601.11835)
*Yufan Zhang,Jaromir Savelka,Seth Goldstein,Michael Conway*

Main category: cs.SE

TL;DR: 研究发现ChatGPT发布后学生编程行为显著变化，提交代码变长、修改幅度增大但成绩提升减少，暗示过度依赖LLM可能损害学习效果。


<details>
  <summary>Details</summary>
Motivation: 探讨大语言模型普及对学生编程学习和教师评估方式的影响。

Method: 分析某研究生云计算课程五年内学生代码提交数据，比较ChatGPT发布前后五个学期的行为差异。

Result: 学生最终提交代码长度增加，连续提交间编辑距离上升而得分提升下降，且这些变化与总体表现显著相关。

Conclusion: 学生可能因过度依赖LLM导致学习成效下降，呼吁教育者与雇主重新思考评估真实能力的方法。

Abstract: The widespread availability of large language models (LLMs) has changed how students engage with coding and problem-solving. While these tools may increase student productivity, they also make it more difficult for instructors to assess students' learning and effort. In this quasi-longitudinal study, we analyze five years of student source code submissions in a graduate-level cloud computing course, focusing on an assignment that remained unchanged and examining students' behavior during the period spanning five semesters before the release of ChatGPT and five semesters after.
  Student coding behavior has changed significantly since Fall 2022. The length of their final submissions increased. Between consecutive submissions, average edit distances increased while average score improvement decreased, suggesting that both student productivity and learning have decreased after ChatGPT's release. Additionally, there are statistically significant correlations between these behavioral changes and their overall performance. Although we cannot definitively attribute them to LLM misuse, they are consistent with our hypothesis that some students are over-reliant on LLMs, which is negatively affecting their learning outcomes. Our findings raise an alarm around the first generation of graduates in the age of LLMs, calling upon both educators and employers to reflect on their evaluation methods for genuine expertise and productivity.

</details>


### [17] [Trace Validation of Unmodified Concurrent Systems with OmniLink](https://arxiv.org/abs/2601.11836)
*Finn Hackett,Evan Wrench,Peter Macko,A. Jesse Jiryu Davis,Yuanhao Wei,Ivan Beschastnikh*

Main category: cs.SE

TL;DR: OmniLink是一种新的并发系统验证方法，基于TLA+规范，通过黑盒事件和时间窗口求解逻辑全序，优于现有线性化检查工具。


<details>
  <summary>Details</summary>
Motivation: 并发系统难以验证，现有工具依赖侵入式插桩或不现实的执行模型，亟需更高效灵活的验证方案。

Method: 将系统事件视为带时间窗口的黑盒，在TLA+中赋予语义，利用现成模型检查器求解动作全序，支持非线性化行为建模。

Result: 成功验证WiredTiger、BAT和ConcurrentQueue系统，改进已有模型，发现两个此前未知的bug并获作者确认。

Conclusion: OmniLink在大规模验证任务中性能超越现有工具，为工业级与研究级并发系统提供强大而灵活的形式化验证支持。

Abstract: Concurrent systems are notoriously difficult to validate: subtle bugs may only manifest under rare thread interleavings, and existing tools often require intrusive instrumentation or unrealistic execution models. We present OmniLink, a new methodology for validating concurrent implementations against high-level specifications in TLA+. Unlike prior TLA+ based approaches which use a technique called trace validation, OmniLink treats system events as black boxes with a timebox in which they occurred and a meaning in TLA+, solving for a logical total order of actions. Unlike prior approaches based on linearizability checking, which already solves for total orders of actions with timeboxes, OmniLink uses a flexible specification language, and offers a different linearizability checking method based on off-the-shelf model checking. OmniLink offers different features compared existing linearizability checking tools, and we show that it outperforms the state of the art on large scale validation tasks.
  Our evaluation validates WiredTiger, a state-of-the-art industrial database storage layer, as well as Balanced Augmented Tree (BAT), a state-of-the art lock-free data structure from the research community, and ConcurrentQueue, a popular lock-free queue featuring aggressive performance optimizations. We use OmniLink to improve WiredTiger's existing TLA+ model, as well as develop new TLA+ models that closely match the behavior of the modeled systems, including non-linearizable behaviors. OmniLink is able to find known bugs injected into the systems under test, as well as help discover two previously unknown bugs (1 in BAT, 1 in ConcurrentQueue), which we have confirmed with the authors of those systems.

</details>


### [18] [Terminal-Bench: Benchmarking Agents on Hard, Realistic Tasks in Command Line Interfaces](https://arxiv.org/abs/2601.11868)
*Mike A. Merrill,Alexander G. Shaw,Nicholas Carlini,Boxuan Li,Harsh Raj,Ivan Bercovich,Lin Shi,Jeong Yeon Shin,Thomas Walshe,E. Kelly Buchanan,Junhong Shen,Guanghao Ye,Haowei Lin,Jason Poulos,Maoyu Wang,Marianna Nezhurina,Jenia Jitsev,Di Lu,Orfeas Menis Mastromichalakis,Zhiwei Xu,Zizhao Chen,Yue Liu,Robert Zhang,Leon Liangyu Chen,Anurag Kashyap,Jan-Lucas Uslu,Jeffrey Li,Jianbo Wu,Minghao Yan,Song Bian,Vedang Sharma,Ke Sun,Steven Dillmann,Akshay Anand,Andrew Lanpouthakoun,Bardia Koopah,Changran Hu,Etash Guha,Gabriel H. S. Dreiman,Jiacheng Zhu,Karl Krauth,Li Zhong,Niklas Muennighoff,Robert Amanfu,Shangyin Tan,Shreyas Pimpalgaonkar,Tushar Aggarwal,Xiangning Lin,Xin Lan,Xuandong Zhao,Yiqing Liang,Yuanli Wang,Zilong Wang,Changzhi Zhou,David Heineman,Hange Liu,Harsh Trivedi,John Yang,Junhong Lin,Manish Shetty,Michael Yang,Nabil Omi,Negin Raoof,Shanda Li,Terry Yue Zhuo,Wuwei Lin,Yiwei Dai,Yuxin Wang,Wenhao Chai,Shang Zhou,Dariush Wahdany,Ziyu She,Jiaming Hu,Zhikang Dong,Yuxuan Zhu,Sasha Cui,Ahson Saiyed,Arinbjörn Kolbeinsson,Jesse Hu,Christopher Michael Rytting,Ryan Marten,Yixin Wang,Alex Dimakis,Andy Konwinski,Ludwig Schmidt*

Main category: cs.SE

TL;DR: Terminal-Bench 2.0是一个包含89个真实工作流启发的终端任务的高难度基准，用于评估前沿AI模型和智能体在长程任务中的表现。


<details>
  <summary>Details</summary>
Motivation: 当前基准无法有效衡量AI智能体在真实复杂任务中的能力，因此需要更难、更贴近现实的评测标准。

Method: 构建包含独立环境、人工解决方案和完整验证测试的终端任务集，并对前沿模型进行评分与错误分析。

Result: 前沿模型和智能体得分低于65%，揭示了当前系统在复杂任务中的不足。

Conclusion: 该基准为未来AI智能体研发提供了可靠评估工具，并公开数据集和评测框架以促进研究。

Abstract: AI agents may soon become capable of autonomously completing valuable, long-horizon tasks in diverse domains. Current benchmarks either do not measure real-world tasks, or are not sufficiently difficult to meaningfully measure frontier models. To this end, we present Terminal-Bench 2.0: a carefully curated hard benchmark composed of 89 tasks in computer terminal environments inspired by problems from real workflows. Each task features a unique environment, human-written solution, and comprehensive tests for verification. We show that frontier models and agents score less than 65\% on the benchmark and conduct an error analysis to identify areas for model and agent improvement. We publish the dataset and evaluation harness to assist developers and researchers in future work at https://www.tbench.ai/ .

</details>


### [19] [Harmonica: A Self-Adaptation Exemplar for Sustainable MLOps](https://arxiv.org/abs/2601.11926)
*Ananya Halgatti,Shaunak Biswas,Hiya Bhatt,Srinivasan Rakhunathan,Karthik Vaidhyanathan*

Main category: cs.SE

TL;DR: Harmonica是一个基于HarmonE方法的自适应示例，通过MAPE-K循环实现结构化自适应控制，提升MLOps管道中机器学习系统的可持续性。


<details>
  <summary>Details</summary>
Motivation: 现有MLOps对运行时不确定性支持有限，影响机器学习系统长期可持续性，亟需自适应机制应对执行漂移。

Method: 引入MAPE-K循环，分离高层自适应策略与底层战术执行，持续监控可持续性指标并触发架构调整。

Result: 在时间序列回归和计算机视觉案例中验证了系统稳定性提升与人工干预减少。

Conclusion: Harmonica为依赖MLOps管道的机器学习系统提供了实用、可复用的自适应行为基础。

Abstract: Machine learning enabled systems (MLS) often operate in settings where they regularly encounter uncertainties arising from changes in their surrounding environment. Without structured oversight, such changes can degrade model behavior, increase operational cost, and reduce the usefulness of deployed systems. Although Machine Learning Operations (MLOps) streamlines the lifecycle of ML models, it provides limited support for addressing runtime uncertainties that influence the longer term sustainability of MLS. To support continued viability, these systems need a mechanism that detects when execution drifts outside acceptable bounds and adjusts system behavior in response. Despite the growing interest in sustainable and self-adaptive MLS, there has been limited work towards exemplars that allow researchers to study these challenges in MLOps pipelines. This paper presents Harmonica, a self-adaptation exemplar built on the HarmonE approach, designed to enable the sustainable operation of such pipelines. Harmonica introduces structured adaptive control through MAPE-K loop, separating high-level adaptation policy from low-level tactic execution. It continuously monitors sustainability metrics, evaluates them against dynamic adaptation boundaries, and automatically triggers architectural tactics when thresholds are violated. We demonstrate the tool through case studies in time series regression and computer vision, examining its ability to improve system stability and reduce manual intervention. The results show that Harmonica offers a practical and reusable foundation for enabling adaptive behavior in MLS that rely on MLOps pipelines for sustained operation.

</details>


### [20] [Enhancing Fuzz Testing Efficiency through Automated Fuzz Target Generation](https://arxiv.org/abs/2601.11972)
*Chi Thien Tran*

Main category: cs.SE

TL;DR: 本文提出一种基于静态分析的自动化模糊测试目标生成方法，提升C/C++库的覆盖率与漏洞检测效率。


<details>
  <summary>Details</summary>
Motivation: 大型软件项目中手动创建模糊测试目标耗时费力，需自动化技术提升效率与质量。

Method: 通过静态分析源码结构，构建函数调用、映射输入参数、合成编译信息并自动收集分析执行结果。

Result: 成功应用于C/C++库，有效生成高质量模糊测试目标，提升测试覆盖与执行分析自动化水平。

Conclusion: 该方法显著降低人工成本，增强模糊测试在复杂项目中的实用性与可扩展性。

Abstract: Fuzzing continues to be the most effective method for identifying security vulnerabilities in software. In the context of fuzz testing, the fuzzer supplies varied inputs to fuzz targets, which are designed to comprehensively exercise critical sections of the client code. Various studies have focused on optimizing and developing advanced fuzzers, such as AFL++, libFuzzer, Honggfuzz, syzkaller, ISP-Fuzzer, which have substantially enhanced vulnerability detection in widely used software and libraries. Nevertheless, achieving greater coverage necessitates improvements in both the quality and quantity of fuzz targets. In large-scale software projects and libraries -- characterized by numerous user defined functions and data types -- manual creation of fuzz targets is both labor-intensive and time-consuming. This challenge underscores the need for automated techniques not only to generate fuzz targets but also to streamline the execution and analysis of their results. In this paper, we introduce an approach to improving fuzz target generation through static analysis of library source code. The proposed method encompasses several key aspects: it analyzes source code structures to accurately construct function calls and generate fuzz targets; it maps fuzzer input data to the corresponding function parameters; it synthesizes compilation information for the fuzz targets; and it automatically collects and analyzes execution results. Our findings are demonstrated through the application of this approach to the generation of fuzz targets for C/C++ libraries.

</details>


### [21] [Many Hands Make Light Work: An LLM-based Multi-Agent System for Detecting Malicious PyPI Packages](https://arxiv.org/abs/2601.12148)
*Muhammad Umar Zeshan,Motunrayo Ibiyo,Claudio Di Sipio,Phuong T. Nguyen,Davide Di Ruscio*

Main category: cs.SE

TL;DR: LAMPS是一个基于多智能体LLM的系统，用于检测PyPI中的恶意包，在两个数据集上均显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 传统规则工具难以捕捉代码语义模式，而现有LLM应用缺乏可解释性和模块化，故需构建新型安全分析框架。

Method: 采用CrewAI框架协调四个角色智能体，结合微调CodeBERT与LLaMA-3进行分类与上下文推理。

Result: 在D1数据集准确率97.7%，D2达99.5%，显著优于MPHunter与RAG等基线方法。

Conclusion: 分布式LLM推理可行，模块化多智能体设计能有效提升软件供应链安全性。

Abstract: Malicious code in open-source repositories such as PyPI poses a growing threat to software supply chains. Traditional rule-based tools often overlook the semantic patterns in source code that are crucial for identifying adversarial components. Large language models (LLMs) show promise for software analysis, yet their use in interpretable and modular security pipelines remains limited. This paper presents LAMPS, a multi-agent system that employs collaborative LLMs to detect malicious PyPI packages. The system consists of four role-specific agents for package retrieval, file extraction, classification, and verdict aggregation, coordinated through the CrewAI framework. A prototype combines a fine-tuned CodeBERT model for classification with LLaMA-3 agents for contextual reasoning. LAMPS has been evaluated on two complementary datasets: D1, a balanced collection of 6,000 setup.py files, and D2, a realistic multi-file dataset with 1,296 files and natural class imbalance. On D1, LAMPS achieves 97.7% accuracy, surpassing MPHunter--one of the state-of-the-art approaches. On D2, it reaches 99.5% accuracy and 99.5% balanced accuracy, outperforming RAG-based approaches and fine-tuned single-agent baselines. McNemar's test confirmed these improvements as highly significant. The results demonstrate the feasibility of distributed LLM reasoning for malicious code detection and highlight the benefits of modular multi-agent designs in software supply chain security.

</details>


### [22] [Aletheia: What Makes RLVR For Code Verifiers Tick?](https://arxiv.org/abs/2601.12186)
*Vatsal Venkatkrishna,Indraneil Paul,Iryna Gurevych*

Main category: cs.SE

TL;DR: 本文提出Aletheia测试平台，评估基于RLVR训练的代码验证器在不同模型和分布偏移下的鲁棒性，并发现简化训练方案的潜力。


<details>
  <summary>Details</summary>
Motivation: 当前代码生成领域较少采用多域思维验证器，而执行反馈难以获取时，此类验证器具有重要价值。

Method: 构建Aletheia测试平台，系统研究RLVR训练中中间思维轨迹、负样本学习和在线策略训练三个关键组件的效果。

Result: 实验表明RLVR最优，但小规模验证器中在线策略学习最关键，大规模下思维轨迹训练最重要。

Conclusion: 代码验证器训练可针对性简化，为代码生成后训练工具箱提供高效新方案。

Abstract: Multi-domain thinking verifiers trained via Reinforcement Learning from Verifiable Rewards (RLVR) are a prominent fixture of the Large Language Model (LLM) post-training pipeline, owing to their ability to robustly rate and rerank model outputs. However, the adoption of such verifiers towards code generation has been comparatively sparse, with execution feedback constituting the dominant signal. Nonetheless, code verifiers remain valuable toward judging model outputs in scenarios where execution feedback is hard to obtain and are a potentially powerful addition to the code generation post-training toolbox. To this end, we create and open-source Aletheia, a controlled testbed that enables execution-grounded evaluation of code verifiers' robustness across disparate policy models and covariate shifts. We examine components of the RLVR-based verifier training recipe widely credited for its success: (1) intermediate thinking traces, (2) learning from negative samples, and (3) on-policy training. While experiments show the optimality of RLVR, we uncover important opportunities to simplify the recipe. Particularly, despite code verification exhibiting positive training- and inference-time scaling, on-policy learning stands out as the key component at small verifier sizes, and thinking-based training emerges as the most important component at larger scales.

</details>


### [23] [Environment-Aware Code Generation: How far are We?](https://arxiv.org/abs/2601.12262)
*Tongtong Wu,Rongyi Chen,Wenjie Du,Suyu Ma,Guilin Qi,Zhenchang Xing,Shahram Khadivi,Ramesh Periyathambi,Gholamreza Haffari*

Main category: cs.SE

TL;DR: 本文首次系统研究环境感知代码生成（EACG），提出新基准VersiBCB，并从数据、参数、缓存三方面改进LLM在不同软件环境下的代码可执行性。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型在特定软件环境下生成可直接执行代码的能力尚不明确，缺乏真实评估基准。

Method: 构建多包、执行验证、关注弃用的基准VersiBCB，从数据、参数、缓存三个维度设计适配策略进行实验。

Result: 现有LLM在环境相关代码生成上表现不佳，但所提适配方法显著提升环境兼容性与可执行性。

Conclusion: 环境感知代码生成仍具挑战，但通过针对性优化有望推动LLM在实际软件工程中的落地应用。

Abstract: Recent progress in large language models (LLMs) has improved code generation, but most evaluations still test isolated, small-scale code (e.g., a single function) under default or unspecified software environments. As a result, it is unclear whether LLMs can reliably generate executable code tailored to a user's specific environment. We present the first systematic study of Environment-Aware Code Generation (EACG), where generated code must be functionally correct and directly executable under arbitrary software configurations. To enable realistic evaluation, we introduce VersiBCB, a benchmark that is multi-package, execution-verified, and deprecation-aware, capturing complex and evolving environments that prior datasets often overlook. Using VersiBCB, we investigate three complementary adaptation axes: data, parameters, and cache, and develop representative strategies for each. Our results show that current LLMs struggle with environment-specific code generation, while our adaptations improve environment compatibility and executability. These findings highlight key challenges and opportunities for deploying LLMs in practical software engineering workflows.

</details>


### [24] [Leveraging Mutation Analysis for LLM-based Repair of Quantum Programs](https://arxiv.org/abs/2601.12273)
*Chihiro Yoshida,Yuta Ishimoto,Olivier Nourry,Masanari Kondo,Makoto Matsushita,Yasutaka Kamei,Yoshiki Higo*

Main category: cs.SE

TL;DR: 本文提出了一种结合大语言模型与变异分析的量子程序自动修复框架，显著提升修复成功率与解释质量。


<details>
  <summary>Details</summary>
Motivation: 现有量子程序自动修复方法存在修复成功率低或生成补丁可读性差的问题。

Method: 构建基于大语言模型的修复框架，并设计四种包含静态信息、动态信息和变异分析结果的不同提示配置，以研究上下文信息对修复性能的影响。

Result: 实验表明，引入变异分析可将修复成功率提升至94.4%，并在某些情况下改善生成解释的质量。

Conclusion: 变异分析能为量子程序自动修复提供有价值的上下文信息，有助于开发更可靠且可解释的修复技术。

Abstract: In recent years, Automated Program Repair (APR) techniques specifically designed for quantum programs have been proposed. However, existing approaches often suffer from low repair success rates or poor understandability of the generated patches. In this study, we construct a framework in which a large language model (LLM) generates code repairs along with a natural language explanation of the applied repairs. To investigate how the contextual information included in prompts influences APR performance for quantum programs, we design four prompt configurations with different combinations of static information, dynamic information, and mutation analysis results. Mutation analysis evaluates how small changes to specific parts of a program affect its execution results and provides more detailed dynamic information than simple execution outputs such as stack traces. Our experimental results show that mutation analysis can provide valuable contextual information for LLM-based APR of quantum programs, improving repair success rates (achieving 94.4% in our experiment) and in some cases also improving the quality of generated explanations. Our findings point toward new directions for developing APR techniques for quantum programs that enhance both reliability and explainability.

</details>


### [25] [Hybrid Concolic Testing with Large Language Models for Guided Path Exploration](https://arxiv.org/abs/2601.12274)
*Mahdi Eslamimehr*

Main category: cs.SE

TL;DR: 本文提出了一种结合大语言模型与符号执行的新型混合测试框架，显著提升路径覆盖率与缺陷检测效率。


<details>
  <summary>Details</summary>
Motivation: 传统符号执行面临路径爆炸与约束求解成本高的问题，限制了其在大规模系统中的应用。

Method: 利用大语言模型的语义推理能力引导路径探索、优先选择关键路径并辅助约束求解，构建新的算法框架。

Result: 在合成与真实金融科技应用中，该方法在分支覆盖率、路径覆盖率和覆盖时间上优于传统符号执行、随机测试和遗传算法。

Conclusion: 结合符号执行与大语言模型可更高效探索程序状态空间，提升软件测试效果。

Abstract: Concolic testing, a powerful hybrid software testing technique, has historically been plagued by fundamental limitations such as path explosion and the high cost of constraint solving, which hinder its practical application in large-scale, real-world software systems. This paper introduces a novel algorithmic framework that synergistically integrates concolic execution with Large Language Models (LLMs) to overcome these challenges. Our hybrid approach leverages the semantic reasoning capabilities of LLMs to guide path exploration, prioritize interesting execution paths, and assist in constraint solving. We formally define the system architecture and algorithms that constitute this new paradigm. Through a series of experiments on both synthetic and real-world Fintech applications, we demonstrate that our approach significantly outperforms traditional concolic testing, random testing, and genetic algorithm-based methods in terms of branch coverage, path coverage, and time-to-coverage. The results indicate that by combining the strengths of both concolic execution and LLMs, our method achieves a more efficient and effective exploration of the program state space, leading to improved bug detection capabilities.

</details>


### [26] [The Expert Validation Framework (EVF): Enabling Domain Expert Control in AI Engineering](https://arxiv.org/abs/2601.12327)
*Lucas Gren,Felix Dobslaw*

Main category: cs.SE

TL;DR: 提出专家验证框架以确保生成式AI在企业中的质量与可信度


<details>
  <summary>Details</summary>
Motivation: 当前生成式AI在企业部署中缺乏系统化的质量保障机制，影响组织信任

Method: 构建以领域专家为核心的四阶段框架：规范制定、系统创建、验证和生产监控

Result: 使组织能在保持专家监督和质量标准的前提下有效利用生成式AI能力

Conclusion: 该框架弥合了AI能力与组织信任之间的关键鸿沟，支持多样化GenAI应用的稳健落地

Abstract: Generative AI (GenAI) systems promise to transform knowledge work by automating a range of tasks, yet their deployment in enterprise settings remains hindered by the lack of systematic quality assurance mechanisms. We present an Expert Validation Framework that places domain experts at the center of building software with GenAI components, enabling them to maintain authoritative control over system behavior through structured specification, testing, validation, and continuous monitoring processes. Our framework addresses the critical gap between AI capabilities and organizational trust by establishing a rigorous, expert-driven methodology for ensuring quality across diverse GenAI applications. Through a four-stage implementation process encompassing specification, system creation, validation, and production monitoring, the framework enables organizations to leverage GenAI capabilities while maintaining expert oversight and quality standards.

</details>


### [27] [Discovering 100+ Compiler Defects in 72 Hours via LLM-Driven Semantic Logic Recomposition](https://arxiv.org/abs/2601.12360)
*Xinabang He,Yuanwei Chen,Hao Wu,Jikang Zhang,Zicheng Wang,Ligeng Chen,Junjie Peng,Haiyang Wei,Yi Qian,Tiantai Zhang,Linzhang Wang,Bing Mao*

Main category: cs.SE

TL;DR: FeatureFuzz 是一种基于语义特征组合的编译器模糊测试工具，显著提升缺陷发现能力。


<details>
  <summary>Details</summary>
Motivation: 现有模糊测试方法难以保留触发编译器缺陷的关键语义，限制了程序多样性与缺陷检出率。

Method: 从历史缺陷报告中提取语义特征（含自然语言描述与代码实例），组合生成符合语法的有效测试程序。

Result: 在 GCC 与 LLVM 上 24 小时内发现 167 个崩溃（领先工具 2.78 倍），72 小时确认 76 个新缺陷。

Conclusion: FeatureFuzz 能有效复用语义特征，显著增强对现代编译器的压力测试能力。

Abstract: Compilers constitute the foundational root-of-trust in software supply chains; however, their immense complexity inevitably conceals critical defects. Recent research has attempted to leverage historical bugs to design new mutation operators or fine-tune models to increase program diversity for compiler fuzzing.We observe, however, that bugs manifest primarily based on the semantics of input programs rather than their syntax. Unfortunately, current approaches, whether relying on syntactic mutation or general Large Language Model (LLM) fine-tuning, struggle to preserve the specific semantics found in the logic of bug-triggering programs. Consequently, these critical semantic triggers are often lost, resulting in a limitation of the diversity of generated programs.
  To explicitly reuse such semantics, we propose FeatureFuzz, a compiler fuzzer that combines features to generate programs. We define a feature as a decoupled primitive that encapsulates a natural language description of a bug-prone invariant, such as an out-of-bounds array access, alongside a concrete code witness of its realization. FeatureFuzz operates via a three-stage workflow: it first extracts features from historical bug reports, synthesizes coherent groups of features, and finally instantiates these groups into valid programs for compiler fuzzing.
  We evaluated FeatureFuzz on GCC and LLVM. Over 24-hour campaigns, FeatureFuzz uncovered 167 unique crashes, which is 2.78x more than the second-best fuzzer. Furthermore, through a 72-hour fuzzing campaign, FeatureFuzz identified 106 bugs in GCC and LLVM, 76 of which have already been confirmed by compiler developers, validating the approach's ability to stress-test modern compilers effectively.

</details>


### [28] [Evaluating Large Language Models for Time Series Anomaly Detection in Aerospace Software](https://arxiv.org/abs/2601.12448)
*Yang Liu,Yixing Luo,Xiaofeng Li,Xiaogang Dong,Bin Gu,Zhi Jin*

Main category: cs.SE

TL;DR: 本文提出ATSADBench基准，首次系统评估大语言模型在航空航天时序异常检测中的表现，并引入用户导向指标与增强策略。


<details>
  <summary>Details</summary>
Motivation: 填补大语言模型在复杂航空航天遥测场景下异常检测有效性研究的空白。

Method: 构建包含九项任务的基准数据集，采用Direct和Prediction-Based两种范式评估开源LLM，并结合少样本学习与RAG增强策略。

Result: LLM在单变量任务中表现良好，但在多变量任务中接近随机猜测；少样本学习略有提升，RAG无显著改善甚至加剧误报。

Conclusion: 为未来航空航天软件中基于LLM的时序异常检测提供实践指导与改进方向。

Abstract: Time series anomaly detection (TSAD) is essential for ensuring the safety and reliability of aerospace software systems. Although large language models (LLMs) provide a promising training-free alternative to unsupervised approaches, their effectiveness in aerospace settings remains under-examined because of complex telemetry, misaligned evaluation metrics, and the absence of domain knowledge. To address this gap, we introduce ATSADBench, the first benchmark for aerospace TSAD. ATSADBench comprises nine tasks that combine three pattern-wise anomaly types, univariate and multivariate signals, and both in-loop and out-of-loop feedback scenarios, yielding 108,000 data points. Using this benchmark, we systematically evaluate state-of-the-art open-source LLMs under two paradigms: Direct, which labels anomalies within sliding windows, and Prediction-Based, which detects anomalies from prediction errors. To reflect operational needs, we reformulate evaluation at the window level and propose three user-oriented metrics: Alarm Accuracy (AA), Alarm Latency (AL), and Alarm Contiguity (AC), which quantify alarm correctness, timeliness, and credibility. We further examine two enhancement strategies, few-shot learning and retrieval-augmented generation (RAG), to inject domain knowledge. The evaluation results show that (1) LLMs perform well on univariate tasks but struggle with multivariate telemetry, (2) their AA and AC on multivariate tasks approach random guessing, (3) few-shot learning provides modest gains whereas RAG offers no significant improvement, and (4) in practice LLMs can detect true anomaly onsets yet sometimes raise false alarms, which few-shot prompting mitigates but RAG exacerbates. These findings offer guidance for future LLM-based TSAD in aerospace software.

</details>


### [29] [Automated Tool Support for Category-Partition Testing: Design Decisions, UI and Examples of Use](https://arxiv.org/abs/2601.12559)
*Yvan Labiche*

Main category: cs.SE

TL;DR: 本文介绍了一种自动化Category-Partition功能测试技术的工具，支持图形界面与多种数据类型，通过案例研究验证其能力。


<details>
  <summary>Details</summary>
Motivation: 提升Category-Partition测试技术的自动化程度，减少人工操作，提高测试效率。

Method: 开发图形界面工具，支持用户定义参数、环境变量、类别与选择项，并自动组合生成测试框架与测试用例。

Result: 工具成功应用于九个案例研究，展示了其在不同场景下的实用性与有效性。

Conclusion: 该工具显著提升了Category-Partition方法的自动化水平，为功能测试提供了实用支持。

Abstract: Category-Partition is a functional testing technique that is based on the idea that the input domain of the system under test can be divided into sub-domains, with the assumption that inputs that belong to the same sub-domain trigger a similar behaviour and that therefore it is sufficient to select one input from each sub-domain. Category-Partition proceeds in several steps, from the identification of so-called categories and choices, possibly constrained, which are subsequently used to form test frames, i.e., combinations of choices, and eventually test cases. This paper reports on an ongoing attempt to automate as many of those steps as possible, with graphical-user interface tool support. Specifically, the user interface allows the user to specify parameters as well as so-called environment variables, further specify categories and choices with optional constraints. Choices are provided with precise specifications with operations specific to their types (e.g., Boolean, Integer, Real, String). Then, the tool automates the construction of test frames, which are combinations of choices, according to alternative selection criteria, and the identification of input values for parameters and environment variables for these test frames, thereby producing test cases. The paper illustrates the capabilities of the tool with the use of nine different case studies.

</details>


### [30] [OpenAI for OpenAPI: Automated generation of REST API specification via LLMs](https://arxiv.org/abs/2601.12735)
*Hao Chen,Yunchun Li,Chen Chen,Fengxu Lin,Wei Li*

Main category: cs.SE

TL;DR: OOPS是一种基于LLM的静态分析方法，用于跨技术栈自动生成高质量OpenAPI规范，显著减少人工干预并克服上下文限制与幻觉问题。


<details>
  <summary>Details</summary>
Motivation: 开发者在编写和维护OpenAPI规范时面临困难，现有静态分析方法受限于特定语言和框架，而LLM虽具潜力但受制于上下文长度和幻觉。

Method: 提出OOPS方法，通过构建API依赖图解决上下文限制，并采用多阶段生成与自优化机制缓解语法与语义幻觉，实现技术无关的OAS自动生成。

Result: 在12个真实REST API项目上测试，OOPS在端点推断、请求/响应参数及约束推断上F1值分别达98%、97%和92%，输入输出token均保持较低水平。

Conclusion: OOPS是首个技术无关的LLM驱动OAS生成方案，在多语言多框架场景下高效准确，大幅降低人工成本与技术依赖。

Abstract: REST APIs, based on the REpresentational State Transfer (REST) architecture, are the primary type of Web API. The OpenAPI Specification (OAS) serves as the de facto standard for describing REST APIs and is crucial for multiple software engineering tasks. However, developers face challenges in writing and maintaining OAS. Although static analysis shows potential for OAS generation, it is limited to specific programming languages and development frameworks. The powerful code understanding capabilities of LLMs offer new opportunities for OAS generation, yet they are constrained by context limitations and hallucinations. To address these challenges, we propose the OpenAI OpenAPI Project Scanner (OOPS), the first technology-agnostic LLM-based static analysis method for OAS generation, requiring fewer technology-specific rules and less human expert intervention. OOPS is implemented as an LLM agent workflow comprising two key steps: endpoint method extraction and OAS generation. By constructing an API dependency graph, it establishes necessary file associations to address LLMs' context limitations. Through multi-stage generation and self-refine, it mitigates both syntactic and semantic hallucinations during OAS generation. We evaluated OOPS on 12 real-world REST APIs spanning 5 programming languages and 8 development frameworks. Experimental results demonstrate that OOPS accurately generates high-quality OAS for REST APIs implemented with diverse technologies, achieving an average F1-score exceeding 98% for endpoint method inference, 97% for both request parameter and response inference, and 92% for parameter constraint inference. The input tokens average below 5.6K with a maximum of 16.2K, while the output tokens average below 0.9K with a maximum of 7.7K.

</details>


### [31] [Teaching LLMs to Learn Tool Trialing and Execution through Environment Interaction](https://arxiv.org/abs/2601.12762)
*Xingjie Gao,Pengcheng Huang,Zhenghao Liu,Yukun Yan,Shuo Wang,Zulong Chen,Chen Qian,Ge Yu,Yu Gu*

Main category: cs.SE

TL;DR: ToolMaster通过交互式学习提升大语言模型在新工具上的泛化与鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖静态轨迹记忆，难以应对新工具或动态环境。

Method: 采用试错执行范式，结合模仿学习与强化学习优化工具规划与调用。

Result: 实验表明ToolMaster在未见工具上显著优于基线方法。

Conclusion: ToolMaster能有效提升LLM对新工具的自主探索与执行能力。

Abstract: Equipping Large Language Models (LLMs) with external tools enables them to solve complex real-world problems. However, the robustness of existing methods remains a critical challenge when confronting novel or evolving tools. Existing trajectory-centric paradigms primarily rely on memorizing static solution paths during training, which limits the ability of LLMs to generalize tool usage to newly introduced or previously unseen tools. In this paper, we propose ToolMaster, a framework that shifts tool use from imitating golden tool-calling trajectories to actively learning tool usage through interaction with the environment. To optimize LLMs for tool planning and invocation, ToolMaster adopts a trial-and-execution paradigm, which trains LLMs to first imitate teacher-generated trajectories containing explicit tool trials and self-correction, followed by reinforcement learning to coordinate the trial and execution phases jointly. This process enables agents to autonomously explore correct tool usage by actively interacting with environments and forming experiential knowledge that benefits tool execution. Experimental results demonstrate that ToolMaster significantly outperforms existing baselines in terms of generalization and robustness across unseen or unfamiliar tools. All code and data are available at https://github.com/NEUIR/ToolMaster.

</details>


### [32] [Docker Does Not Guarantee Reproducibility](https://arxiv.org/abs/2601.12811)
*Julien Malka,Stefano Zacchiroli,Théo Zimmermann*

Main category: cs.SE

TL;DR: 本文通过文献综述和大规模实证研究，评估Docker在实际中实现环境可重现性的能力及其最佳实践的有效性。


<details>
  <summary>Details</summary>
Motivation: 尽管Docker常被理论认为支持环境可重现性，但其实际保障程度和局限性尚未充分探索。

Method: 首先进行系统文献综述，识别Dockerfile最佳实践；其次对5298个GitHub工作流中的Docker构建进行实证分析，重建镜像并比对历史版本。

Result: 实证结果显示Docker镜像的实际可重现性有限，部分文献推荐的最佳实践效果不佳。

Conclusion: Docker虽有助于环境封装，但需更严格的构建规范和工具支持才能真正实现可靠可重现性。

Abstract: The reproducibility of software environments is a critical concern in modern software engineering, with ramifications ranging from the effectiveness of collaboration workflows to software supply chain security and scientific reproducibility. Containerization technologies like Docker address this problem by encapsulating software environments into shareable filesystem snapshots known as images. While Docker is frequently cited in the literature as a tool that enables reproducibility in theory, the extent of its guarantees and limitations in practice remains under-explored.
  In this work, we address this gap through two complementary approaches. First, we conduct a systematic literature review to examine how Docker is framed in scientific discourse on reproducibility and to identify documented best practices for writing Dockerfiles enabling reproducible image building. Then, we perform a large-scale empirical study of 5298 Docker builds collected from GitHub workflows. By rebuilding these images and comparing the results with their historical counterparts, we assess the real reproducibility of Docker images and evaluate the effectiveness of the best practices identified in the literature.

</details>


### [33] [Automatic Generation of Formal Specification and Verification Annotations Using LLMs and Test Oracles](https://arxiv.org/abs/2601.12845)
*João Pascoal Faria,Emanuel Trigo,Vinicius Honorato,Rui Abreu*

Main category: cs.SE

TL;DR: 本文研究如何利用大语言模型（LLM）自动生成Dafny程序的验证注解，显著减少人工标注负担。


<details>
  <summary>Details</summary>
Motivation: 传统程序验证需大量手动标注前置条件、后置条件等，费时且依赖专家经验，亟需自动化方案。

Method: 结合Claude Opus 4.5与GPT-5.2的多模型方法，基于自然语言注释和测试代码生成Dafny验证注解，并通过验证器反馈迭代修复。

Result: 在110个Dafny程序中，98.2%在最多8次修复迭代内生成正确注解；测试断言可作为静态预言机自动验证前置/后置条件。

Conclusion: LLM能高效辅助程序形式化验证，证明助手注解是当前主要难点，集成IDE插件获得良好可用性反馈。

Abstract: Recent verification tools aim to make formal verification more accessible to software engineers by automating most of the verification process. However, annotating conventional programs with the formal specification and verification constructs (preconditions, postconditions, loop invariants, auxiliary predicates and functions and proof helpers) required to prove their correctness still demands significant manual effort and expertise. This paper investigates how LLMs can automatically generate such annotations for programs written in Dafny, a verification-aware programming language, starting from conventional code accompanied by natural language specifications (in comments) and test code. In experiments on 110 Dafny programs, a multimodel approach combining Claude Opus 4.5 and GPT-5.2 generated correct annotations for 98.2% of the programs within at most 8 repair iterations, using verifier feedback. A logistic regression analysis shows that proof-helper annotations contribute disproportionately to problem difficulty for current LLMs. Assertions in the test cases served as static oracles to automatically validate the generated pre/postconditions. We also compare generated and manual solutions and present an extension for Visual Studio Code to incorporate automatic generation into the IDE, with encouraging usability feedback.

</details>


### [34] [Efficient Code Analysis via Graph-Guided Large Language Models](https://arxiv.org/abs/2601.12890)
*Hang Gao,Tao Peng,Baoquan Cui,Hong Huang,Fengge Wu,Junsuo Zhao,Jian Zhang*

Main category: cs.SE

TL;DR: 提出一种基于图的注意力获取管道，增强大语言模型在复杂代码库中定位恶意行为的能力。


<details>
  <summary>Details</summary>
Motivation: 恶意行为常隐藏于小段代码中，且跨文件依赖使大语言模型难以可靠检测。

Method: 将项目解析为代码图，用LLM编码节点语义与结构信息，训练稀疏监督的GNN进行初步检测并回溯关键代码区域，引导LLM深入分析。

Result: 在多个公开和自建数据集上优于现有方法，显著降低无关上下文干扰且标注成本低。

Conclusion: 该方法具备实际部署于软件安全场景的潜力。

Abstract: Malicious behavior is often hidden in small, easily overlooked code fragments, especially within large and complex codebases. The cross-file dependencies of these fragments make it difficult for even powerful large language models (LLMs) to detect them reliably. We propose a graph-centric attention acquisition pipeline that enhances LLMs' ability to localize malicious behavior. The approach parses a project into a code graph, uses an LLM to encode nodes with semantic and structural signals, and trains a Graph Neural Network (GNN) under sparse supervision. The GNN performs an initial detection, and through backtracking of its predictions, identifies key code sections that are most likely to contain malicious behavior. These influential regions are then used to guide the LLM's attention for in-depth analysis. This strategy significantly reduces interference from irrelevant context while maintaining low annotation costs. Extensive experiments show that the method consistently outperforms existing methods on multiple public and self-built datasets, highlighting its potential for practical deployment in software security scenarios.

</details>


### [35] [A Benchmark for Language Models in Real-World System Building](https://arxiv.org/abs/2601.12927)
*Weilin Jin,Chenyu Zhao,Zeshun Huang,Chaoyun Zhang,Qingwei Lin,Chetan Bansal,Saravan Rajmohan,Shenglin Zhang,Yongqian Sun,Dan Pei,Yifan Wu,Tong Jia,Ying Li,Zhonghai Wu,Minghua Ma*

Main category: cs.SE

TL;DR: 本文提出一个新基准，用于评估跨指令集架构和多语言的软件包构建修复能力，揭示当前大模型在此任务上的不足。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注单一指令集和同质语言，缺乏对跨架构、多语言场景下软件构建修复的系统评估。

Method: 构建包含268个真实构建失败案例的基准数据集，并设计标准化评估流程，测试六种前沿大模型性能。

Result: 实验表明，当前大模型在跨指令集架构的软件包修复任务上表现不佳，仍需进一步技术突破。

Conclusion: 该基准为未来提升软件可移植性与弥合架构差异的研究奠定基础。

Abstract: During migration across instruction set architectures (ISAs), software package build repair is a critical task for ensuring the reliability of software deployment and the stability of modern operating systems. While Large Language Models (LLMs) have shown promise in tackling this challenge, prior work has primarily focused on single instruction set architecture (ISA) and homogeneous programming languages. To address this limitation, we introduce a new benchmark designed for software package build repair across diverse architectures and languages. Comprising 268 real-world software package build failures, the benchmark provides a standardized evaluation pipeline. We evaluate six state-of-the-art LLMs on the benchmark, and the results show that cross-ISA software package repair remains difficult and requires further advances. By systematically exposing this challenge, the benchmark establishes a foundation for advancing future methods aimed at improving software portability and bridging architectural gaps.

</details>


### [36] [Beyond Accuracy: Characterizing Code Comprehension Capabilities in (Large) Language Models](https://arxiv.org/abs/2601.12951)
*Felix Mächtle,Jan-Niclas Serr,Nils Loose,Thomas Eisenbarth*

Main category: cs.SE

TL;DR: 本文提出诊断框架，揭示大语言模型代码理解能力与人类软件度量相关性低，强调需实例级评估方法。


<details>
  <summary>Details</summary>
Motivation: 当前基准仅提供粗粒度性能总结，难以揭示大语言模型在代码理解中的真实能力与局限。

Method: 将代码理解重构为输入输出一致性任务，结合传统复杂度指标与影子模型进行大规模数据相关性分析。

Result: 人类指标与LLM表现相关性弱（AUROC 0.63），影子模型预测更强（AUROC 0.86），表明LLM遵循自身规律。

Conclusion: 需超越总体准确率的基准方法，采用实例级诊断，并承认预测正确结果存在根本性限制。

Abstract: Large Language Models (LLMs) are increasingly integrated into software engineering workflows, yet current benchmarks provide only coarse performance summaries that obscure the diverse capabilities and limitations of these models. This paper investigates whether LLMs' code-comprehension performance aligns with traditional human-centric software metrics or instead reflects distinct, non-human regularities. We introduce a diagnostic framework that reframes code understanding as a binary input-output consistency task, enabling the evaluation of classification and generative models. Using a large-scale dataset, we correlate model performance with traditional, human-centric complexity metrics, such as lexical size, control-flow complexity, and abstract syntax tree structure. Our analyses reveal minimal correlation between human-defined metrics and LLM success (AUROC 0.63), while shadow models achieve substantially higher predictive performance (AUROC 0.86), capturing complex, partially predictable patterns beyond traditional software measures. These findings suggest that LLM comprehension reflects model-specific regularities only partially accessible through either human-designed or learned features, emphasizing the need for benchmark methodologies that move beyond aggregate accuracy and toward instance-level diagnostics, while acknowledging fundamental limits in predicting correct outcomes.

</details>


### [37] [MeltRTL: Multi-Expert LLMs with Inference-time Intervention for RTL Code Generation](https://arxiv.org/abs/2601.13015)
*Nowfel Mashnoor,Mohammad Akyash,Hadi Kamali,Kimia Azar*

Main category: cs.SE

TL;DR: MeltRTL通过多专家注意力和推理时干预机制，在不重新训练模型的情况下显著提升LLM生成硬件RTL代码的准确率。


<details>
  <summary>Details</summary>
Motivation: 当前LLM在生成复杂数字设计的RTL代码时存在语法和功能错误，亟需改进。

Method: 引入多专家注意力架构、推理时干预机制和高效干预框架，动态路由设计规范并纠正硬件特定错误。

Result: 在VerilogEval基准测试中，综合性和功能性正确率分别提升至96%和60%，仅增加27%计算开销。

Conclusion: MeltRTL无需微调即可部署于现有预训练LLM，多专家架构与ITI协同作用显著。

Abstract: The automated generation of hardware register-transfer level (RTL) code with large language models (LLMs) shows promise, yet current solutions struggle to produce syntactically and functionally correct code for complex digital designs. This paper introduces MeltRTL, a novel framework that integrates multi-expert attention with inference-time intervention (ITI) to significantly improve LLM-based RTL code generation accuracy without retraining the base model. MeltRTL introduces three key innovations: (1) A multi-expert attention architecture that dynamically routes design specifications to specialized expert networks, enabling targeted reasoning across various hardware categories; (2) An inference-time intervention mechanism that employs non-linear probes to detect and correct hardware-specific inaccuracies during generation; and (3) An efficient intervention framework that selectively operates on expert-specific attention heads with minimal computational overhead. We evaluate MeltRTL on the VerilogEval benchmark, achieving 96% synthesizability and 60% functional correctness, compared to the base LLM's 85.3% and 45.3%, respectively. These improvements are obtained entirely at inference time, with only 27% computational overhead and no model fine-tuning, making MeltRTL immediately deployable on existing pre-trained LLMs. Ablation studies further show the complementary benefits of multi-expert architecture and ITI, highlighting their synergistic effects when combined.

</details>


### [38] [RM -RF: Reward Model for Run-Free Unit Test Evaluation](https://arxiv.org/abs/2601.13097)
*Elena Bruches,Daniil Grebenkin,Mikhail Klementev,Vadim Alperovich,Roman Derunets,Dari Baturova,Georgy Mkrtchyan,Oleg Sedukhin,Ivan Bondarenko,Nikolay Bushkov,Stanislav Moiseev*

Main category: cs.SE

TL;DR: RM-RF是一种轻量级奖励模型，无需运行即可评估自动生成的单元测试，预测编译成功、覆盖率提升和变异杀死率三项指标，显著降低延迟与成本。


<details>
  <summary>Details</summary>
Motivation: 传统测试评估依赖反复编译执行，成本高、延迟大，亟需高效替代方案。

Method: 基于源码和测试代码训练模型，预测三项执行信号；采用多语言数据集（Java/Python/Go），对比多种模型家族与调参策略（零样本、全微调、LoRA）。

Result: 在三项预测目标上平均F1达0.69，相比传统方法显著降低延迟与基础设施成本，同时保持预测准确性。

Conclusion: RM-RF适用于大规模测试生成与强化学习代码优化场景，提供快速可扩展反馈。

Abstract: We present RM-RF, a lightweight reward model for run-free evaluation of automatically generated unit tests. Instead of repeatedly compiling and executing candidate tests, RM-RF predicts - from source and test code alone - three execution-derived signals: (1) whether the augmented test suite compiles and runs successfully, (2) whether the generated test cases increase code coverage, and (3) whether the generated test cases improve the mutation kill rate. To train and evaluate RM-RF we assemble a multilingual dataset (Java, Python, Go) of focal files, test files, and candidate test additions labeled by an execution-based pipeline, and we release an associated dataset and methodology for comparative evaluation. We tested multiple model families and tuning regimes (zero-shot, full fine-tuning, and PEFT via LoRA), achieving an average F1 of 0.69 across the three targets. Compared to conventional compile-and-run instruments, RM-RF provides substantially lower latency and infrastructure cost while delivering competitive predictive fidelity, enabling fast, scalable feedback for large-scale test generation and RL-based code optimization.

</details>


### [39] [Guidelines to Prompt Large Language Models for Code Generation: An Empirical Characterization](https://arxiv.org/abs/2601.13118)
*Alessandro Midolo,Alessandro Giagnorio,Fiorella Zampetti,Rosalia Tufano,Gabriele Bavota,Massimiliano Di Penta*

Main category: cs.SE

TL;DR: 本文提出并评估了针对代码生成任务的10条提示优化指南，帮助开发者更有效地使用大语言模型。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏专门指导开发者编写适合代码生成提示的具体准则。

Method: 通过迭代、测试驱动的方法自动优化提示，并分析结果提炼出改进项，再经50名从业者评估其实际使用与感知效用。

Result: 提炼出10条提示优化指南，涵盖输入输出说明、前后条件、示例提供等；从业者反馈显示感知效用与实际使用存在差异。

Conclusion: 该研究对开发者、教育者及LLM辅助开发工具设计者均有实践意义。

Abstract: Large Language Models (LLMs) are nowadays extensively used for various types of software engineering tasks, primarily code generation. Previous research has shown how suitable prompt engineering could help developers in improving their code generation prompts. However, so far, there do not exist specific guidelines driving developers towards writing suitable prompts for code generation. In this work, we derive and evaluate development-specific prompt optimization guidelines. First, we use an iterative, test-driven approach to automatically refine code generation prompts, and we analyze the outcome of this process to identify prompt improvement items that lead to test passes. We use such elements to elicit 10 guidelines for prompt improvement, related to better specifying I/O, pre-post conditions, providing examples, various types of details, or clarifying ambiguities. We conduct an assessment with 50 practitioners, who report their usage of the elicited prompt improvement patterns, as well as their perceived usefulness, which does not always correspond to the actual usage before knowing our guidelines. Our results lead to implications not only for practitioners and educators, but also for those aimed at creating better LLM-aided software development tools.

</details>


### [40] [Earth Embeddings as Products: Taxonomy, Ecosystem, and Standardized Access](https://arxiv.org/abs/2601.13134)
*Heng Fang,Adam J. Stewart,Isaac Corley,Xiao Xiang Zhu,Hossein Azizpour*

Main category: cs.SE

TL;DR: 本文提出通过统一API标准化地理空间嵌入数据产品，以解决当前碎片化生态系统的互操作性问题，提升地球观测工作流的透明性和可访问性。


<details>
  <summary>Details</summary>
Motivation: 现有地理空间基础模型的预计算嵌入数据产品因格式和分辨率不兼容，导致工程瓶颈，阻碍模型比较与可复现性。

Method: 构建三层分类体系（数据、工具、价值），并扩展TorchGeo框架提供统一API加载和查询多样化嵌入产品。

Result: 成功将嵌入视为首要地理空间数据集，实现下游分析与模型特定工程的解耦。

Conclusion: 该方法为更透明、易用的地球观测工作流程提供了可行路径。

Abstract: Geospatial Foundation Models (GFMs) provide powerful representations, but high compute costs hinder their widespread use. Pre-computed embedding data products offer a practical "frozen" alternative, yet they currently exist in a fragmented ecosystem of incompatible formats and resolutions. This lack of standardization creates an engineering bottleneck that prevents meaningful model comparison and reproducibility. We formalize this landscape through a three-layer taxonomy: Data, Tools, and Value. We survey existing products to identify interoperability barriers. To bridge this gap, we extend TorchGeo with a unified API that standardizes the loading and querying of diverse embedding products. By treating embeddings as first-class geospatial datasets, we decouple downstream analysis from model-specific engineering, providing a roadmap for more transparent and accessible Earth observation workflows.

</details>


### [41] [From Human to Machine Refactoring: Assessing GPT-4's Impact on Python Class Quality and Readability](https://arxiv.org/abs/2601.13139)
*Alessandro Midolo,Emiliano Tramontana,Massimiliano Di Penta*

Main category: cs.SE

TL;DR: 本研究评估了GPT-4o在Python类重构中的表现，发现其能有效保持行为正确性并提升代码质量，但会降低可读性。


<details>
  <summary>Details</summary>
Motivation: 现有LLM驱动的重构工具缺乏对代码质量多维度影响的系统评估。

Method: 基于ClassEval基准中的100个Python类，采用Fowler重构目录，从行为正确性、代码质量和可读性三方面评估GPT-4o重构效果。

Result: GPT-4o能生成行为保持的重构，减少代码异味并改善质量指标，但导致可读性下降。

Conclusion: LLM在自动化重构中具有潜力，但需权衡质量与可读性，未来应优化其在实际工作流中的集成。

Abstract: Refactoring is a software engineering practice that aims to improve code quality without altering program behavior. Although automated refactoring tools have been extensively studied, their practical applicability remains limited. Recent advances in Large Language Models (LLMs) have introduced new opportunities for automated code refactoring. The evaluation of such an LLM-driven approach, however, leaves unanswered questions about its effects on code quality. In this paper, we present a comprehensive empirical study on LLM-driven refactoring using GPT-4o, applied to 100 Python classes from the ClassEval benchmark. Unlike prior work, our study explores a wide range of class-level refactorings inspired by Fowler's catalog and evaluates their effects from three complementary perspectives: (i) behavioral correctness, verified through unit tests; (ii) code quality, assessed via Pylint, Flake8, and SonarCloud; and (iii) readability, measured using a state-of-the-art readability tool. Our findings show that GPT-4o generally produces behavior-preserving refactorings that reduce code smells and improve quality metrics, albeit at the cost of decreased readability. Our results provide new evidence on the capabilities and limitations of LLMs in automated software refactoring, highlighting directions for integrating LLMs into practical refactoring workflows.

</details>


### [42] [KOCO-BENCH: Can Large Language Models Leverage Domain Knowledge in Software Development?](https://arxiv.org/abs/2601.13240)
*Xue Jiang,Jiaru Qian,Xianjie Shi,Chenjie Li,Hao Zhu,Ziyu Wang,Jielun Zhang,Zheyu Zhao,Kechi Zhang,Jia Li,Wenpin Jiao,Zhi Jin,Ge Li,Yihong Dong*

Main category: cs.SE

TL;DR: KOCO-BENCH是一个专为评估大语言模型领域特化方法而设计的新基准，涵盖6个新兴领域、11个框架和25个项目，强调从知识库中获取并应用领域知识解决多粒度编程任务。


<details>
  <summary>Details</summary>
Motivation: 现有代码评测基准无法有效评估LLM领域特化方法，因其仅关注模型已有知识而非学习与应用新知识的能力，且缺乏配套知识库。

Method: 构建包含精选知识库的KOCO-BENCH基准，设置从函数级到项目级的代码生成任务及多选问答形式的知识理解任务，强制模型从知识库中提取API、规则等信息完成评测。

Result: 实验表明当前最先进模型（如Claude Code）在该基准上表现不佳，即使结合SFT、RAG等特化方法，最高准确率仅34.2%，凸显现有方法效果有限。

Conclusion: KOCO-BENCH揭示了当前领域特化方法的不足，为后续研究提供了公开评测平台与基线，亟需更有效的领域知识注入与利用机制。

Abstract: Large language models (LLMs) excel at general programming but struggle with domain-specific software development, necessitating domain specialization methods for LLMs to learn and utilize domain knowledge and data. However, existing domain-specific code benchmarks cannot evaluate the effectiveness of domain specialization methods, which focus on assessing what knowledge LLMs possess rather than how they acquire and apply new knowledge, lacking explicit knowledge corpora for developing domain specialization methods. To this end, we present KOCO-BENCH, a novel benchmark designed for evaluating domain specialization methods in real-world software development. KOCO-BENCH contains 6 emerging domains with 11 software frameworks and 25 projects, featuring curated knowledge corpora alongside multi-granularity evaluation tasks including domain code generation (from function-level to project-level with rigorous test suites) and domain knowledge understanding (via multiple-choice Q&A). Unlike previous benchmarks that only provide test sets for direct evaluation, KOCO-BENCH requires acquiring and applying diverse domain knowledge (APIs, rules, constraints, etc.) from knowledge corpora to solve evaluation tasks. Our evaluations reveal that KOCO-BENCH poses significant challenges to state-of-the-art LLMs. Even with domain specialization methods (e.g., SFT, RAG, kNN-LM) applied, improvements remain marginal. Best-performing coding agent, Claude Code, achieves only 34.2%, highlighting the urgent need for more effective domain specialization methods. We release KOCO-BENCH, evaluation code, and baselines to advance further research at https://github.com/jiangxxxue/KOCO-bench.

</details>


### [43] [Why Does the LLM Stop Computing: An Empirical Study of User-Reported Failures in Open-Source LLMs](https://arxiv.org/abs/2601.13655)
*Guangba Yu,Zirui Wang,Yujie Huang,Renyi Zhong,Yuedong Zhong,Yilun Wang,Michael R. Lyu*

Main category: cs.SE

TL;DR: 本文首次大规模实证研究开源大语言模型部署中的可靠性问题，揭示白盒编排将瓶颈从算法缺陷转移至系统脆弱性，并提出三项关键发现以指导改进。


<details>
  <summary>Details</summary>
Motivation: 开源大语言模型的本地部署带来可靠性盲点，需填补用户管理编排可靠性的研究空白。

Method: 分析来自DeepSeek、Llama和Qwen生态系统的705个真实故障案例，进行大规模实证研究。

Result: 发现诊断分歧、系统同质性和生命周期升级三大现象，表明可靠性障碍源于共享生态而非特定架构。

Conclusion: 研究结果为提升大语言模型部署可靠性提供可操作指导，并公开数据集支持后续研究。

Abstract: The democratization of open-source Large Language Models (LLMs) allows users to fine-tune and deploy models on local infrastructure but exposes them to a First Mile deployment landscape. Unlike black-box API consumption, the reliability of user-managed orchestration remains a critical blind spot. To bridge this gap, we conduct the first large-scale empirical study of 705 real-world failures from the open-source DeepSeek, Llama, and Qwen ecosystems.
  Our analysis reveals a paradigm shift: white-box orchestration relocates the reliability bottleneck from model algorithmic defects to the systemic fragility of the deployment stack. We identify three key phenomena: (1) Diagnostic Divergence: runtime crashes distinctively signal infrastructure friction, whereas incorrect functionality serves as a signature for internal tokenizer defects. (2) Systemic Homogeneity: Root causes converge across divergent series, confirming reliability barriers are inherent to the shared ecosystem rather than specific architectures. (3) Lifecycle Escalation: Barriers escalate from intrinsic configuration struggles during fine-tuning to compounded environmental incompatibilities during inference. Supported by our publicly available dataset, these insights provide actionable guidance for enhancing the reliability of the LLM landscape.

</details>


### [44] [A Blockchain-Oriented Software Engineering Architecture for Carbon Credit Certification Systems](https://arxiv.org/abs/2601.13772)
*Matteo Vaccargiu,Azmat Ullah,Pierluigi Gallo*

Main category: cs.SE

TL;DR: 本文提出了一种基于区块链的碳信用认证架构，结合物联网实时数据与智能合约，支持中小型光伏项目生成可验证碳信用记录。


<details>
  <summary>Details</summary>
Motivation: 现有技术对中小型可再生能源项目的碳信用认证支持不足，需符合欧洲法规与自愿碳市场标准。

Method: 整合物联网边缘数据采集、许可链上存储与智能合约，以100 kWp光伏案例验证架构可行性。

Result: 系统能生成结构化、可审计的碳信用记录，支持第三方验证并满足合规要求。

Conclusion: 该架构为中小型清洁能源项目提供了实用、合规的碳信用认证解决方案。

Abstract: Carbon credit systems have emerged as a policy tool to incentivize emission reductions and support the transition to clean energy. Reliable carbon-credit certification depends on mechanisms that connect actual, measured renewable-energy production to verifiable emission-reduction records. Although blockchain and IoT technologies have been applied to emission monitoring and trading, existing work offers limited support for certification processes, particularly for small and medium-scale renewable installations. This paper introduces a blockchain-based carbon-credit certification architecture, demonstrated through a 100 kWp photovoltaic case study, that integrates real-time IoT data collection, edge-level aggregation, and secure on-chain storage on a permissioned blockchain with smart contracts. Unlike approaches focused on trading mechanisms, the proposed system aligns with European legislation and voluntary carbon-market standards, clarifying the practical requirements and constraints that apply to photovoltaic operators. The resulting architecture provides a structured pathway for generating verifiable carbon-credit records and supporting third-party verification.

</details>


### [45] [From Completion to Editing: Unlocking Context-Aware Code Infilling via Search-and-Replace Instruction Tuning](https://arxiv.org/abs/2601.13384)
*Jiajun Zhang,Zeyu Cui,Jiaxi Yang,Lei Zhang,Yuheng Jing,Zeyao Ma,Tianyi Bai,Zilei Wang,Qiang Liu,Liang Wang,Binyuan Hui,Junyang Lin*

Main category: cs.SE

TL;DR: 提出SRI框架，通过单次推理实现动态上下文编辑，提升代码补全性能并保持低延迟。


<details>
  <summary>Details</summary>
Motivation: 解决现有FIM范式无法纠错、依赖不安全基础模型，以及Chat LLM与Agentic工作流存在性能或延迟问题。

Method: 设计Search-and-Replace Infilling框架，结合显式搜索阶段与指令跟随先验，构建SRI-200K数据集并微调SRI-Coder系列模型。

Result: 仅用20k样本，SRI-Coder使Chat模型超越Base模型补全性能，同时保留通用编码能力且延迟接近标准FIM。

Conclusion: SRI框架有效统一了安全性、灵活性与高效性，适用于Qwen3-Coder系列，推动高级自动补全与辅助开发。

Abstract: The dominant Fill-in-the-Middle (FIM) paradigm for code completion is constrained by its rigid inability to correct contextual errors and reliance on unaligned, insecure Base models. While Chat LLMs offer safety and Agentic workflows provide flexibility, they suffer from performance degradation and prohibitive latency, respectively. To resolve this dilemma, we propose Search-and-Replace Infilling (SRI), a framework that internalizes the agentic verification-and-editing mechanism into a unified, single-pass inference process. By structurally grounding edits via an explicit search phase, SRI harmonizes completion tasks with the instruction-following priors of Chat LLMs, extending the paradigm from static infilling to dynamic context-aware editing. We synthesize a high-quality dataset, SRI-200K, and fine-tune the SRI-Coder series. Extensive evaluations demonstrate that with minimal data (20k samples), SRI-Coder enables Chat models to surpass the completion performance of their Base counterparts. Crucially, unlike FIM-style tuning, SRI preserves general coding competencies and maintains inference latency comparable to standard FIM. We empower the entire Qwen3-Coder series with SRI, encouraging the developer community to leverage this framework for advanced auto-completion and assisted development.

</details>


### [46] [A Tool for Automatically Cataloguing and Selecting Pre-Trained Models and Datasets for Software Engineering](https://arxiv.org/abs/2601.13460)
*Alexandra González,Oscar Cerezo,Xavier Franch,Silverio Martínez-Fernández*

Main category: cs.SE

TL;DR: MLAssetSelection是一个帮助软件工程师自动筛选和管理机器学习资产的Web应用，支持排行榜、需求匹配、实时更新与个性化功能。


<details>
  <summary>Details</summary>
Motivation: 解决在大型注册表中手动查找适合软件工程任务的模型和数据集效率低、易出错的问题。

Method: 开发具备排行榜、需求筛选、定时更新和用户个性化功能的Web应用程序。

Result: 提供高效、精准、自动化的机器学习资产筛选与管理方案，并附演示视频。

Conclusion: MLAssetSelection有效提升软件工程师在海量机器学习资源中定位合适资产的效率与准确性。

Abstract: The rapid growth of machine learning assets has made it increasingly difficult for software engineers to identify models and datasets that match their specific needs. Browsing large registries, such as Hugging Face, is time-consuming, error-prone, and rarely tailored to Software Engineering (SE) tasks. We present MLAssetSelection, a web application that automatically extracts SE assets and supports four key functionalities: (i) a configurable leaderboard for ranking models across multiple benchmarks and metrics; (ii) requirements-based selection of models and datasets; (iii) real-time automated updates through scheduled jobs that keep asset information current; and (iv) user-centric features including login, personalized asset lists, and configurable alert notifications. A demonstration video is available at https://youtu.be/t6CJ6P9asV4.

</details>


### [47] [Governance Matters: Lessons from Restructuring the data.table OSS Project](https://arxiv.org/abs/2601.13466)
*Pedro Oliveira,Doris Amoakohene,Toby Hocking,Marco Gerosa,Igor Steinmacher*

Main category: cs.SE

TL;DR: 本研究通过混合方法评估了data.table项目治理改革的影响，发现改革后新贡献者增长200%，PR解决时间从700天缩短至一周内，贡献者留存率提升3倍。


<details>
  <summary>Details</summary>
Motivation: 开源项目常因非正式或集中式治理面临运营风险，data.table项目为解决可扩展性与可持续性问题启动治理改革。

Method: 结合17名贡献者的问卷调查与项目仓库数据挖掘进行混合方法评估。

Result: 改革后新贡献者增加200%，PR解决时间大幅缩短，贡献者留存率提高3倍，社区透明度和士气改善。

Conclusion: 该案例为开源项目维护者、企业及基金会提供了增强治理结构的实用指导，尽管公平性与冲突解决仍需改进。

Abstract: Open source software (OSS) forms the backbone of industrial data workflows and enterprise systems. However, many OSS projects face operational risks due to informal or centralized governance. This paper presents a practical case study of data.table, a high-performance R package widely adopted in production analytics pipelines, which underwent a community-led governance reform to address scalability and sustainability concerns. Before the reform, data.table faced a growing backlog of unresolved issues and open pull requests, unclear contributor pathways, and bottlenecks caused by reliance on a single core maintainer. In response, the community initiated a redesign of its governance structure. In this paper, we evaluated the impact of this transition through a mixed-methods approach, combining a contributor survey (n=17) with mining project repository data. Our results show that following the reform, the project experienced a 200% increase in new contributor recruitment, a drop in pull request resolution time from over 700 days to under a week, and a 3x increase in contributor retention. Community sentiment improved around transparency, onboarding, and project momentum, though concerns around fairness and conflict resolution remain. This case study provides practical guidance for maintainers, companies, and foundations seeking to enhance OSS governance.

</details>


### [48] [CodeContests-O: Powering LLMs via Feedback-Driven Iterative Test Case Generation](https://arxiv.org/abs/2601.13682)
*Jianfeng Cai,Jinhua Zhu,Ruopei Sun,Kangwen Zhao,Dongyun Xue,Mingxiao Feng,Wengang Zhou,Houqiang Li*

Main category: cs.SE

TL;DR: 提出反馈驱动的迭代框架生成高质量测试用例，构建CodeContests-O数据集，显著提升模型评估性能。


<details>
  <summary>Details</summary>
Motivation: 现有编程题测试用例稀缺且LLM生成多样性不足，需引入外部反馈提升质量。

Method: 利用LLM生成初始用例，通过执行正误解法获得反馈，迭代优化生成高区分度测试用例。

Result: CodeContests-O在千万级解法池上TPR达89.37%，TNR达90.89%，优于基线4.32%和9.37%；微调Qwen2.5-7B在LiveCodeBench提升9.52% Pass@1。

Conclusion: 该框架有效提升测试用例质量，所构建数据集显著增强模型训练与评估效果，并开源代码与数据促进后续研究。

Abstract: The rise of reasoning models necessitates large-scale verifiable data, for which programming tasks serve as an ideal source. However, while competitive programming platforms provide abundant problems and solutions, high-quality test cases for verification remain scarce. Existing approaches attempt to synthesize test cases using Large Language Models (LLMs), but rely solely on the model's intrinsic generation capabilities without external feedback, frequently resulting in insufficiently diverse cases. To address this limitation, we propose a $\textbf{Feedback-Driven Iterative Framework}$ for comprehensive test case construction. Specifically, our method leverages the LLM to generate initial test cases, executes them against known correct and incorrect solutions, and utilizes the failed results as feedback to guide the LLM in refining the test cases toward high fidelity and discriminability. We then apply this method to the CodeContests dataset to construct an optimized high-quality derivative, $\textbf{CodeContests-O}$. Evaluating against the entire pool of solutions ($1.1 \times 10^7$ in total), our dataset achieves an average True Positive Rate (TPR) of $89.37\%$ and True Negative Rate (TNR) of $90.89\%$, significantly outperforming the CodeContests and CodeContests+ by margins of $4.32\%$ and $9.37\%$, respectively. Furthermore, fine-tuning the Qwen2.5-7B model on CodeContests-O results in a $9.52\%$ improvement on LiveCodeBench (Pass@1). Experiments demonstrate the effectiveness of our framework and the quality of CodeContests-O. To support reproducibility and facilitate future research, we release the $\href{https://github.com/cai-jianfeng/CodeContests-O}{code}$ and $\href{https://huggingface.co/datasets/caijanfeng/CodeContests-O}{dataset}$.

</details>


### [49] [SWE-Tester: Training Open-Source LLMs for Issue Reproduction in Real-World Repositories](https://arxiv.org/abs/2601.13713)
*Aditya Bharat Soni,Rajat Ghosh,Vaishnavi Bhargava,Valerie Chen,Debojyoti Dutta*

Main category: cs.SE

TL;DR: SWE-Tester是一个用于训练开源大语言模型自动生成问题复现测试的新框架，显著提升了成功率与变更覆盖率。


<details>
  <summary>Details</summary>
Motivation: 现有方法多依赖闭源大模型，缺乏对开源模型的探索，阻碍了开发者效率和自动化修复系统的进步。

Method: 构建包含4.1万样本的高质量数据集，基于多个开源GitHub仓库训练不同规模和架构的LLM。

Result: 微调模型在SWT-Bench Verified上成功率提升最高达10%，变更覆盖率提升21%，且随计算资源、数据量和模型规模增加持续改善。

Conclusion: SWE-Tester有效推动了开源大模型在自动生成复现测试任务中的性能，具备实际应用价值。

Abstract: Software testing is crucial for ensuring the correctness and reliability of software systems. Automated generation of issue reproduction tests from natural language issue descriptions enhances developer productivity by simplifying root cause analysis, promotes test-driven development -- "test first, write code later", and can be used for improving the effectiveness of automated issue resolution systems like coding agents. Existing methods proposed for this task predominantly rely on closed-source LLMs, with limited exploration of open models. To address this, we propose SWE-Tester -- a novel pipeline for training open-source LLMs to generate issue reproduction tests. First, we curate a high-quality training dataset of 41K instances from 2.6K open-source GitHub repositories and use it to train LLMs of varying sizes and families. The fine-tuned models achieve absolute improvements of up to 10\% in success rate and 21\% in change coverage on SWT-Bench Verified. Further analysis shows consistent improvements with increased inference-time compute, more data, and larger models. These results highlight the effectiveness of our framework for advancing open-source LLMs in this domain.

</details>


### [50] [Counterexample Classification against Signal Temporal Logic Specifications](https://arxiv.org/abs/2601.13743)
*Zhenya Zhang,Parv Kapoor,Jie An,Eunsuk Kang*

Main category: cs.SE

TL;DR: 本文提出了一种基于参数化信号时序逻辑（PSTL）的反例分类方法，通过建立类间包含关系并采用类二分搜索策略，显著提升分类效率。


<details>
  <summary>Details</summary>
Motivation: 为有效理解混合系统中违反STL规范的反例模式及其分布，需建立合理的分类标准以辅助缺陷定位。

Method: 利用PSTL形式化表示各类别，通过参数匹配识别反例所属类别，并构建类间包含关系以支持高效二分搜索式查询剪枝。

Result: 原型工具在两个典型系统上的实验表明，所提方法能有效提升反例分类效率。

Conclusion: 基于PSTL的分类方法结合类间包含关系与搜索优化，为反例分析提供了高效且可解释的解决方案。

Abstract: Signal Temporal Logic (STL) has been widely adopted as a specification language for specifying desirable behaviors of hybrid systems. By monitoring a given STL specification, we can detect the executions that violate it, which are often referred to as counterexamples. In practice, these counterexamples may arise from different causes and thus are relevant to different system defects. To effectively address this, we need a proper criterion for classifying these counterexamples, by which we can comprehend the possible violation patterns and the distributions of these counterexamples with respect to the patterns. In this paper, we propose a classification criterion by using parametric signal temporal logic (PSTL) to represent each class. Due to this formalism, identifying the classes of a counterexample requires finding proper parameter values of PSTL that enable a class to include the counterexample. To improve the efficiency of class identification, we further derive an inclusion relation between different classes, and then propose a binary search-like approach over it that significantly prunes the classes needed to query. We implement a prototype tool and experimentally evaluate its effectiveness on two widely-studied systems.

</details>


### [51] [On Autopilot? An Empirical Study of Human-AI Teaming and Review Practices in Open Source](https://arxiv.org/abs/2601.13754)
*Haoyu Gao,Peerachai Banyongrakkul,Hao Guan,Mansooreh Zahedi,Christoph Treude*

Main category: cs.SE

TL;DR: 研究发现AI协作的PR由无代码所有权的贡献者发起居多，且合并速度更快、反馈更少，但多数项目缺乏相关指南。


<details>
  <summary>Details</summary>
Motivation: 探索开发者与AI协作PR的交互模式及项目级指导缺失问题。

Method: 扩展AIDev数据集，分析AI协作PR与人工PR的对比数据。

Result: 67.5%的AI协作PR来自无代码所有权贡献者；80%无审查直接合并；项目普遍缺乏AI使用指南。

Conclusion: 需为开发者和研究人员制定AI协作开发的规范与最佳实践。

Abstract: Large Language Models (LLMs) increasingly automate software engineering tasks. While recent studies highlight the accelerated adoption of ``AI as a teammate'' in Open Source Software (OSS), developer interaction patterns remain under-explored. In this work, we investigated project-level guidelines and developers' interactions with AI-assisted pull requests (PRs) by expanding the AIDev dataset to include finer-grained contributor code ownership and a comparative baseline of human-created PRs. We found that over 67.5\% of AI-co-authored PRs originate from contributors without prior code ownership. Despite this, the majority of repositories lack guidelines for AI-coding agent usage. Notably, we observed a distinct interaction pattern: AI-co-authored PRs are merged significantly faster with minimal feedback. In contrast to human-created PRs where non-owner developers receive the most feedback, AI-co-authored PRs from non-owners receive the least, with approximately 80\% merged without any explicit review. Finally, we discuss implications for developers and researchers.

</details>


### [52] [RepoGenesis: Benchmarking End-to-End Microservice Generation from Readme to Repository](https://arxiv.org/abs/2601.13943)
*Zhiyuan Peng,Xin Yin,Pu Zhao,Fangkai Yang,Lu Wang,Ran Jia,Xu Chen,Qingwei Lin,Saravan Rajmohan,Dongmei Zhang*

Main category: cs.SE

TL;DR: 提出首个面向微服务仓库级生成的多语言基准RepoGenesis，评估现有模型在真实开发场景中的不足，并发布高质量数据集推动研究。


<details>
  <summary>Details</summary>
Motivation: 现有代码生成评测集中于函数/类级别或修改现有代码，缺乏反映真实0到1开发流程的完整微服务仓库生成评测。

Method: 构建包含106个仓库、覆盖18领域和11框架的多语言基准RepoGenesis，通过“评审-反驳”流程确保质量，并采用Pass@1、API覆盖率和部署成功率评估模型性能。

Result: 最佳系统在Python和Java上的Pass@1仅分别为23.67%和21.45%，暴露架构一致性、依赖管理等问题；微调后的GenesisAgent-8B表现接近GPT-5 mini。

Conclusion: RepoGenesis填补了仓库级微服务生成评测空白，揭示当前模型短板，其高质量数据有助于推动该领域发展。

Abstract: Large language models and agents have achieved remarkable progress in code generation. However, existing benchmarks focus on isolated function/class-level generation (e.g., ClassEval) or modifications to existing codebases (e.g., SWE-Bench), neglecting complete microservice repository generation that reflects real-world 0-to-1 development workflows. To bridge this gap, we introduce RepoGenesis, the first multilingual benchmark for repository-level end-to-end web microservice generation, comprising 106 repositories (60 Python, 46 Java) across 18 domains and 11 frameworks, with 1,258 API endpoints and 2,335 test cases verified through a "review-rebuttal" quality assurance process. We evaluate open-source agents (e.g., DeepCode) and commercial IDEs (e.g., Cursor) using Pass@1, API Coverage (AC), and Deployment Success Rate (DSR). Results reveal that despite high AC (up to 73.91%) and DSR (up to 100%), the best-performing system achieves only 23.67% Pass@1 on Python and 21.45% on Java, exposing deficiencies in architectural coherence, dependency management, and cross-file consistency. Notably, GenesisAgent-8B, fine-tuned on RepoGenesis (train), achieves performance comparable to GPT-5 mini, demonstrating the quality of RepoGenesis for advancing microservice generation. We release our benchmark at https://github.com/pzy2000/RepoGenesis.

</details>


### [53] [Software Testing in the Quantum World](https://arxiv.org/abs/2601.13996)
*Rui Abreu,Shaukat Ali,Paolo Arcaini,Jose Campos,Michael Felderer,Claude Gravel,Fuyuki Ishikawa,Stefan Klikovits,Andriy Miranskyy,Mohammad Mousavi,Masaomi Yamaguchi,Lei Zhang,Jianjun Zhao,Anila Mjeda*

Main category: cs.SE

TL;DR: 本文探讨了在量子软件复杂性增加的背景下，如何在真实量子计算机上进行质量保证的新方法。


<details>
  <summary>Details</summary>
Motivation: 随着量子软件复杂性的增长，传统的经典模拟方法已无法满足质量保证需求，亟需新的测试方法。

Method: 从软件工程的角度分析大规模量子软件测试的关键挑战并提出应对策略。

Result: 明确了当前量子软件测试的主要难点，并提供了可行的工程化解决思路。

Conclusion: 为未来在真实量子硬件上实现高效、可靠的量子软件质量保障奠定了基础。

Abstract: Quantum computing offers significant speedups for simulating physical, chemical, and biological systems, and for optimization and machine learning. As quantum software grows in complexity, the classical simulation of quantum computers, which has long been essential for quality assurance, becomes infeasible. This shift requires new quality-assurance methods that operate directly on real quantum computers. This paper presents the key challenges in testing large-scale quantum software and offers software engineering perspectives for addressing them.

</details>


### [54] [Analyzing the Availability of E-Mail Addresses for PyPI Libraries](https://arxiv.org/abs/2601.14034)
*Alexandros Tsakpinis,Alexander Pretschner*

Main category: cs.SE

TL;DR: 该研究分析了PyPI上686,034个Python库的联系信息可用性，发现81.6%提供有效邮箱，依赖链覆盖率高达97.7%，但存在大量无效条目，建议改进打包指引和邮箱验证机制。


<details>
  <summary>Details</summary>
Motivation: 开源软件长期可持续性依赖维护者可联系性，需评估其联系方式的有效性与覆盖范围。

Method: 对PyPI及其GitHub仓库中的Python库进行实证分析，检查邮箱地址的存在、有效性及在依赖链中的覆盖情况。

Result: 81.6%库含有效邮箱，PyPI为主要来源（79.5%），依赖链覆盖率达97.7%，同时发现超69.8万条无效记录。

Conclusion: 生态系统整体可达性良好，但可通过优化打包流程指引和引入邮箱验证机制进一步提升。

Abstract: Open Source Software (OSS) libraries form the backbone of modern software systems, yet their long-term sustainability often depends on maintainers being reachable for support, coordination, and security reporting. In this paper, we empirically analyze the availability of contact information - specifically e-mail addresses - across 686,034 Python libraries on the Python Package Index (PyPI) and their associated GitHub repositories. We examine how and where maintainers provide this information, assess its validity, and explore coverage across individual libraries and their dependency chains. Our findings show that 81.6% of libraries include at least one valid e-mail address, with PyPI serving as the primary source (79.5%). When analyzing dependency chains, we observe that up to 97.8% of direct and 97.7% of transitive dependencies provide valid contact information. At the same time, we identify over 698,000 invalid entries, primarily due to missing fields. These results demonstrate strong maintainer reachability across the ecosystem, while highlighting opportunities for improvement - such as offering clearer guidance to maintainers during the packaging process and introducing opt-in validation mechanisms for existing e-mail addresses.

</details>


### [55] [Feature-Aware Test Generation for Deep Learning Models](https://arxiv.org/abs/2601.14081)
*Xingcheng Chen,Oliver Weissl,Andrea Stocco*

Main category: cs.SE

TL;DR: Detect是一个基于特征感知的测试生成框架，通过在潜在空间中扰动解耦语义属性，实现对视觉深度学习模型的细粒度可控测试，提升模型可靠性。


<details>
  <summary>Details</summary>
Motivation: 现有生成式AI测试方法缺乏语义洞察力和细粒度控制能力，难以揭示模型失效的根本原因。

Method: Detect通过可控扰动潜在特征，观察输出变化，并结合视觉语言模型进行语义归因，区分任务相关与无关特征，实施针对性扰动。

Result: Detect在图像分类与检测任务中生成高质量测试用例，优于现有方法发现决策边界和定位虚假特征，揭示了不同架构模型的捷径行为。

Conclusion: 可解释、特征感知的测试有助于提升深度学习模型的可靠性，尤其能暴露准确率指标无法捕捉的缺陷。

Abstract: As deep learning models are widely used in software systems, test generation plays a crucial role in assessing the quality of such models before deployment. To date, the most advanced test generators rely on generative AI to synthesize inputs; however, these approaches remain limited in providing semantic insight into the causes of misbehaviours and in offering fine-grained semantic controllability over the generated inputs. In this paper, we introduce Detect, a feature-aware test generation framework for vision-based deep learning (DL) models that systematically generates inputs by perturbing disentangled semantic attributes within the latent space. Detect perturbs individual latent features in a controlled way and observes how these changes affect the model's output. Through this process, it identifies which features lead to behavior shifts and uses a vision-language model for semantic attribution. By distinguishing between task-relevant and irrelevant features, Detect applies feature-aware perturbations targeted for both generalization and robustness. Empirical results across image classification and detection tasks show that Detect generates high-quality test cases with fine-grained control, reveals distinct shortcut behaviors across model architectures (convolutional and transformer-based), and bugs that are not captured by accuracy metrics. Specifically, Detect outperforms a state-of-the-art test generator in decision boundary discovery and a leading spurious feature localization method in identifying robustness failures. Our findings show that fully fine-tuned convolutional models are prone to overfitting on localized cues, such as co-occurring visual traits, while weakly supervised transformers tend to rely on global features, such as environmental variances. These findings highlight the value of interpretable and feature-aware testing in improving DL model reliability.

</details>


### [56] [Practitioner Views on Mobile App Accessibility: Practices and Challenges](https://arxiv.org/abs/2601.14131)
*Amila Indika,Rick Kazman,Anthony Peruma*

Main category: cs.SE

TL;DR: 本研究通过调查110名来自43个国家的移动应用开发者，揭示了iOS与Android平台及开发者经验对无障碍实践的影响，提出促进包容性开发的可行建议。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏跨平台、全球代表性视角下开发者实际处理无障碍问题的研究，本文旨在填补这一空白。

Method: 采用混合方法调查110名跨43国的移动应用开发者，系统比较iOS与Android平台差异及不同经验水平开发者的行为模式。

Result: 开发者虽重视无障碍，但多依赖平台特定指南、晚期测试，且集中于文本功能；API限制与组织约束是主要障碍，不同平台和经验水平存在显著实践差异。

Conclusion: 研究揭示了实现无障碍的实际挑战，为各利益相关方提供了推动更一致、包容性应用开发的具体行动方向。

Abstract: As mobile applications (apps) become ubiquitous in everyday life, it is crucial for developers to prioritize accessibility for users with diverse abilities. While previous research has identified widespread accessibility issues and raised awareness of developer challenges, there remains a lack of cross-platform, globally representative insights into how practitioners approach accessibility in practice. This paper presents findings from a mixed-methods survey of 110 mobile app developers across 43 countries, examining how platform ecosystems (iOS vs. Android) and developer experience shape accessibility practices. Results show that while developers recognize the importance of accessibility, they often rely on platform-specific guidelines and typically perform compliance testing late in the development process. Developers primarily implement text-focused features while struggling with API limitations and organizational constraints. Through systematic cross-platform comparison, we identify novel platform-specific barriers and demonstrate how accessibility practices differ across developer experience levels. Our findings offer new insights into the challenges of achieving accessibility in practice and provide actionable steps for various stakeholders to promote more consistent and inclusive app development.

</details>


### [57] [An Empirical Study on Remote Code Execution in Machine Learning Model Hosting Ecosystems](https://arxiv.org/abs/2601.14163)
*Mohammed Latif Siddiq,Tanzim Hossain Romel,Natalie Sekerak,Beatrice Casey,Joanna C. S. Santos*

Main category: cs.SE

TL;DR: 本文首次大规模实证研究模型共享平台中自定义模型加载的安全风险，揭示普遍依赖不安全默认设置及开发者认知误区，并提出兼顾安全与易用性的改进建议。


<details>
  <summary>Details</summary>
Motivation: 模型共享平台广泛使用远程代码执行功能，但缺乏对潜在安全风险的系统评估，亟需厘清现状并提出改进方案。

Method: 结合静态分析工具（Bandit、CodeQL、Semgrep、YARA）检测漏洞，分析五大平台文档与API设计，并定性研究600余条开发者讨论。

Result: 发现平台普遍存在安全机制执行不均、开发者对远程代码风险认知模糊，且大量模型依赖不安全默认配置。

Conclusion: 建议重构模型共享基础设施安全设计，在保障易用性的同时强化默认安全策略与开发者教育。

Abstract: Model-sharing platforms, such as Hugging Face, ModelScope, and OpenCSG, have become central to modern machine learning development, enabling developers to share, load, and fine-tune pre-trained models with minimal effort. However, the flexibility of these ecosystems introduces a critical security concern: the execution of untrusted code during model loading (i.e., via trust_remote_code or trust_repo). In this work, we conduct the first large-scale empirical study of custom model loading practices across five major model-sharing platforms to assess their prevalence, associated risks, and developer perceptions. We first quantify the frequency with which models require custom code to function and identify those that execute arbitrary Python files during loading. We then apply three complementary static analysis tools: Bandit, CodeQL, and Semgrep, to detect security smells and potential vulnerabilities, categorizing our findings by CWE identifiers to provide a standardized risk taxonomy. We also use YARA to identify malicious patterns and payload signatures. In parallel, we systematically analyze the documentation, API design, and safety mechanisms of each platform to understand their mitigation strategies and enforcement levels. Finally, we conduct a qualitative analysis of over 600 developer discussions from GitHub, Hugging Face, and PyTorch Hub forums, as well as Stack Overflow, to capture community concerns and misconceptions regarding security and usability. Our findings reveal widespread reliance on unsafe defaults, uneven security enforcement across platforms, and persistent confusion among developers about the implications of executing remote code. We conclude with actionable recommendations for designing safer model-sharing infrastructures and striking a balance between usability and security in future AI ecosystems.

</details>


<div id='cs.OS'></div>

# cs.OS [[Back]](#toc)

### [58] [Nixie: Efficient, Transparent Temporal Multiplexing for Consumer GPUs](https://arxiv.org/abs/2601.11743)
*Yechen Xu,Yifei Wang,Nathanael Ren,Yiran Chen,Danyang Zhuo*

Main category: cs.OS

TL;DR: Nixie 是一种无需修改应用或驱动即可在消费级 GPU 上实现高效透明时间复用的系统，显著提升交互任务响应速度并降低 CPU 固定内存占用。


<details>
  <summary>Details</summary>
Motivation: 消费级 GPU 在运行多任务大模型时因内存不足和现有共享机制低效导致性能下降，亟需优化方案。

Method: 设计 Nixie 系统服务，协调 GPU 内存分配与内核启动行为，结合轻量级 MLFQ 启发式调度器优先处理延迟敏感任务。

Result: 实验表明 Nixie 最高可将交互式代码补全任务延迟降低 3.8 倍，相同延迟要求下节省最多 66.8% CPU 固定内存。

Conclusion: Nixie 有效解决了消费级 GPU 多任务场景下的内存与调度瓶颈，提升了用户体验和资源利用率。

Abstract: Consumer machines are increasingly running large ML workloads such as large language models (LLMs), text-to-image generation, and interactive image editing. Unlike datacenter GPUs, consumer GPUs serve single-user, rapidly changing workloads, and each model's working set often nearly fills the GPU memory. As a result, existing sharing mechanisms (e.g., NVIDIA Unified Virtual Memory) perform poorly due to memory thrashing and excessive use of CPU pinned memory when multiple applications are active.
  We design and implement Nixie, a system that enables efficient and transparent temporal multiplexing on consumer GPUs without requiring any application or driver changes. Nixie is a system service that coordinates GPU memory allocation and kernel launch behavior to efficiently utilize the CPU-GPU bi-directional bandwidth and CPU pinned memory. A lightweight scheduler in Nixie further improves responsiveness by automatically prioritizing latency-sensitive interactive jobs using MLFQ-inspired techniques. Our evaluations show that Nixie improves latency of real interactive code-completion tasks by up to $3.8\times$ and saves up to 66.8% CPU pinned memory usage given the same latency requirement.

</details>


### [59] [ContiguousKV: Accelerating LLM Prefill with Granularity-Aligned KV Cache Management](https://arxiv.org/abs/2601.13631)
*Jing Zou,Shangyu Wu,Hancong Duan,Qiao Li,Chun Jason Xue*

Main category: cs.OS

TL;DR: ContiguousKV 是一种高效的前缀 KV 缓存卸载系统，通过优化 I/O 粒度和异步预取机制，显著加速 Re-Prefill 阶段并保持高质量输出。


<details>
  <summary>Details</summary>
Motivation: 解决现有系统在卸载前缀 KV 缓存时存在的 I/O 瓶颈与资源利用率低的问题。

Method: 提出 ContiguousChunk 数据管理粒度、层内/层间异步预取机制及注意力引导缓存管理策略。

Result: 在 Qwen2.5 模型上，Re-Prefill 阶段速度提升 3.85 倍，同时保持高输出质量。

Conclusion: ContiguousKV 有效弥合算法语义与 I/O 效率之间的鸿沟，为大规模语言模型服务提供高性能支持。

Abstract: Efficiently serving Large Language Models (LLMs) with persistent Prefix Key-Value (KV) Cache is critical for applications like conversational search and multi-turn dialogue. Serving a request requires loading the pre-computed prefix KV cache and generating the first token, defined as the Re-Prefill Phase. Offloading this shared prefix cache to secondary storage is essential for memory scalability. Re-Prefill with offloading suffers from severe I/O bottlenecks in two aspects. First, semantic-aware KV cache pruning algorithms select important tokens in fine granularity, while systems manage I/O in coarse, fixed-size blocks, causing severe read amplification. Second, the sequential dependency between identifying important tokens and loading KV cache creates idle I/O and compute bubbles, under-utilizing system resources.
  This paper proposes \textit{ContiguousKV}, a high-performance prefix KV cache offloading system that bridges algorithmic semantics with I/O efficiency to accelerate the Re-Prefill phase. We first introduce \textit{ContiguousChunk}, a unified data management granularity that aligns KV cache pruning with I/O operations. All the mechanisms critical for I/O performance are performed at the granularity of ContiguousChunk, thereby eliminating read amplification. By exploiting the high similarity in important ContiguousChunk indices across layers, we propose intra- and inter-period asynchronous prefetching to break the sequential dependency between I/O and compute, effectively eliminating idle bubbles. Finally, we propose attention-guided cache management to retain semantically critical prefix data in memory. Evaluations on Qwen2.5 series models show that ContiguousKV achieves a 3.85x speedup in the Re-Prefill phase over the state-of-the-art offloading system IMPRESS, while maintaining high output quality.

</details>


### [60] ["Range as a Key" is the Key! Fast and Compact Cloud Block Store Index with RASK](https://arxiv.org/abs/2601.14129)
*Haoru Zhao,Mingkai Dong,Erci Xu,Zhongyu Wang,Haibo Chen*

Main category: cs.OS

TL;DR: RASK是一种基于范围索引的高效内存树结构，显著降低内存占用并提升性能。


<details>
  <summary>Details</summary>
Motivation: 云存储中索引内存消耗过大，而写请求常针对连续块范围，适合范围索引优化。

Method: 提出RASK，采用日志结构叶节点、范围定制搜索与垃圾回收、范围感知分裂合并机制。

Result: 在四个生产轨迹上，内存占用最多减少98.9%，吞吐量最高提升31倍。

Conclusion: RASK有效解决范围重叠与碎片问题，实现内存高效与高性能索引。

Abstract: In cloud block store, indexing is on the critical path of I/O operations and typically resides in memory. With the scaling of users and the emergence of denser storage media, the index has become a primary memory consumer, causing memory strain. Our extensive analysis of production traces reveals that write requests exhibit a strong tendency to target continuous block ranges in cloud storage systems. Thus, compared to current per-block indexing, our insight is that we should directly index block ranges (i.e., range-as-a-key) to save memory.
  In this paper, we propose RASK, a memory-efficient and high-performance tree-structured index that natively indexes ranges. While range-as-a-key offers the potential to save memory and improve performance, realizing this idea is challenging due to the range overlap and range fragmentation issues. To handle range overlap efficiently, RASK introduces the log-structured leaf, combined with range-tailored search and garbage collection. To reduce range fragmentation, RASK employs range-aware split and merge mechanisms. Our evaluations on four production traces show that RASK reduces memory footprint by up to 98.9% and increases throughput by up to 31.0x compared to ten state-of-the-art indexes.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [61] [Cost-Aware Logging: Measuring the Financial Impact of Excessive Log Retention in Small-Scale Cloud Deployments](https://arxiv.org/abs/2601.11584)
*Jody Almaida Putra*

Main category: cs.DC

TL;DR: 通过分析不同日志保留窗口的成本与效用，本研究发现将日志保留期从90天缩短至14天可节省高达78%的存储成本，同时保留超过97%的运维有用日志。


<details>
  <summary>Details</summary>
Motivation: 小型云环境常默认设置过长的日志保留期，导致不必要的存储开销，亟需成本导向的优化方案。

Method: 使用模拟真实日志量与访问模式的数据集，评估7、14、30和90天保留窗口在存储成本、有用日志比例及单位成本三个指标上的表现。

Result: 14天保留窗口在显著降低成本的同时几乎不影响运维效用；更长保留期带来边际效益递减且查询开销增加。

Conclusion: 无需新工具，仅通过轻量级策略调整即可实现可观成本节约，建议资源受限团队采用成本效益视角配置日志保留策略。

Abstract: Log data plays a critical role in observability, debugging, and performance monitoring in modern cloud-native systems. In small and early-stage cloud deployments, however, log retention policies are frequently configured far beyond operational requirements, often defaulting to 90 days or more, without explicit consideration of their financial and performance implications. As a result, excessive log retention becomes a hidden and recurring cost.
  This study examines the financial and operational impact of log retention window selection from a cost-aware perspective. Using synthetic log datasets designed to reflect real-world variability in log volume and access patterns, we evaluate retention windows of 7, 14, 30, and 90 days. The analysis focuses on three metrics: storage cost, operationally useful log ratio, and cost per useful log. Operational usefulness is defined as log data accessed during simulated debugging and incident analysis tasks.
  The results show that reducing log retention from 90 days to 14 days can lower log storage costs by up to 78 percent while preserving more than 97 percent of operationally useful logs. Longer retention windows provide diminishing operational returns while disproportionately increasing storage cost and query overhead. These findings suggest that modest configuration changes can yield significant cost savings without compromising system reliability.
  Rather than proposing new logging mechanisms, this work offers a lightweight and accessible framework to help small engineering teams reason about log retention policies through a cost-effectiveness lens. The study aims to encourage more deliberate observability configurations, particularly in resource-constrained cloud environments.

</details>


### [62] [Opportunistic Scheduling for Optimal Spot Instance Savings in the Cloud](https://arxiv.org/abs/2601.12266)
*Neelkamal Bhuyan,Randeep Bhatia,Murali Kodialam,TV Lakshman*

Main category: cs.DC

TL;DR: 本文研究如何在满足平均延迟约束下，通过调度延迟敏感任务以最小化平均成本。


<details>
  <summary>Details</summary>
Motivation: 优化云实例上的任务调度，平衡成本与延迟需求。

Method: 结合排队论、随机过程和优化方法，推导通用策略成本表达式并设计自适应算法。

Result: 证明低延迟目标下队列长度为1最优，高延迟目标下利用背包结构设计策略，自适应算法接近最优。

Conclusion: 所提方法能有效降低云任务调度成本，同时满足延迟约束。

Abstract: We study the problem of scheduling delay-sensitive jobs over spot and on-demand cloud instances to minimize average cost while meeting an average delay constraint. Jobs arrive as a general stochastic process, and incur different costs based on the instance type. This work provides the first analytical treatment of this problem using tools from queuing theory, stochastic processes, and optimization. We derive cost expressions for general policies, prove queue length one is optimal for low target delays, and characterize the optimal wait-time distribution. For high target delays, we identify a knapsack structure and design a scheduling policy that exploits it. An adaptive algorithm is proposed to fully utilize the allowed delay, and empirical results confirm its near-optimality.

</details>


### [63] [PerCache: Predictive Hierarchical Cache for RAG Applications on Mobile Devices](https://arxiv.org/abs/2601.11553)
*Kaiwei Liu,Liekang Zeng,Lilin Xu,Bufang Yang,Zhenyu Yan*

Main category: cs.DC

TL;DR: PerCache是一种为移动设备个性化RAG应用设计的分层缓存方案，通过预测性填充和动态资源配置显著降低端到端延迟。


<details>
  <summary>Details</summary>
Motivation: 移动设备上的RAG系统因资源受限和长提示导致高延迟，现有云环境缓存方法不适用。

Method: 采用分层架构匹配相似查询与QKV缓存，结合预测性缓存填充和动态负载适配机制。

Result: 在多种应用中比最佳基线降低34.4%延迟，并在资源动态变化下保持最优性能。

Conclusion: PerCache有效优化了移动端RAG系统的计算复用与资源效率，是面向移动场景的高效缓存解决方案。

Abstract: Retrieval-augmented generation (RAG) has been extensively used as a de facto paradigm in various large language model (LLM)-driven applications on mobile devices, such as mobile assistants leveraging personal emails or meeting records. However, due to the lengthy prompts and the resource constraints, mobile RAG systems exhibit significantly high response latency. On this issue, one promising approach is to reuse intermediate computational results across different queries to eliminate redundant computation. But most existing approaches, such as KV cache reuse and semantic cache reuse, are designed for cloud settings and perform poorly, overlooking the distinctive characteristics of mobile RAG.
  We propose PerCache, a novel hierarchical cache solution designed for reducing end-to-end latency of personalized RAG applications on mobile platforms. PerCache adopts a hierarchical architecture that progressively matches similar queries and QKV cache to maximize the reuse of intermediate results at different computing stages. To improve cache hit rate, PerCache applies a predictive method to populate cache with queries that are likely to be raised in the future. In addition, PerCache can adapt its configurations to dynamic system loads, aiming at maximizing the caching utility with minimal resource consumption. We implement PerCache on top of an existing mobile LLM inference engine with commodity mobile phones. Extensive evaluations show that PerCache can surpass the best-performing baseline by 34.4% latency reduction across various applications and maintain optimal latency performance under dynamic resource changes.

</details>


### [64] [Computation-Bandwidth-Memory Trade-offs: A Unified Paradigm for AI Infrastructure](https://arxiv.org/abs/2601.11577)
*Yuankai Fan,Qizhen Weng,Xuelong Li*

Main category: cs.DC

TL;DR: 提出AI Trinity框架，通过计算、带宽和内存三者动态权衡优化大规模AI系统性能。


<details>
  <summary>Details</summary>
Motivation: 大规模AI模型发展受限于硬件瓶颈，单一维度优化效果有限，需协同平衡计算、带宽与内存资源。

Method: 构建AI Trinity统一范式，定义三项核心权衡机制：计算换带宽、带宽换内存、内存换计算，并在多种系统场景中实现动态资源配置。

Result: 在边缘云通信、分布式训练与模型推理等场景验证了AI Trinity的有效性，显著缓解单点瓶颈并提升系统整体效率。

Conclusion: AI Trinity为下一代AI基础设施提供理论基础与实践指导，推动可扩展AI系统的协同发展。

Abstract: Large-scale artificial intelligence models are transforming industries and redefining human machine collaboration. However, continued scaling exposes critical limitations in hardware, including constraints on computation, bandwidth, and memory. These dimensions are tightly interconnected, so improvements in one often create bottlenecks in others, making isolated optimizations less effective. Balancing them to maximize system efficiency remains a central challenge in scalable AI design. To address this challenge, we introduce {Computation-Bandwidth-Memory Trade-offs}, termed the {AI Trinity}, a unified paradigm that positions {computation}, {bandwidth}, and {memory} as coequal pillars for next-generation AI infrastructure. AI Trinity enables dynamic allocation of resources across these pillars, alleviating single-resource bottlenecks and adapting to diverse scenarios to optimize system performance. Within this framework, AI Trinity identifies three fundamental trade-offs: (1) {More Computation$\rightarrow$Less Bandwidth}, wherein computational resources are exploited to reduce data transmission under limited bandwidth conditions, (2) {More Bandwidth$\rightarrow$Less Memory}, which exploits abundant communication capacity to populate or refresh memory when local storage resources are constrained, and (3) {More Memory$\rightarrow$Less Computation}, whereby storage capacity are utilized to mitigate redundant computation when computational costs are prohibitive. We illustrate the effectiveness of AI Trinity through representative system designs spanning edge-cloud communication, large-scale distributed training, and model inference. The innovations embodied in AI Trinity advance a new paradigm for scalable AI infrastructure, providing both a conceptual foundation and practical guidance for a broad range of application scenarios.

</details>


### [65] [PLA-Serve: A Prefill-Length-Aware LLM Serving System](https://arxiv.org/abs/2601.11589)
*Jianshu She,Zonghang Li,Hongchao Du,Shangyu Wu,Wenhao Zheng,Eric Xing,Zhengzhong Liu,Huaxiu Yao,Jason Xue,Qirong Ho*

Main category: cs.DC

TL;DR: PLA-Serve通过按提示长度解耦请求并采用智能批处理，显著降低LLM服务中的TTFT延迟和SLO违规率，提升吞吐量。


<details>
  <summary>Details</summary>
Motivation: 统一调度策略无法适应不同提示长度导致的性能瓶颈，需针对性优化。

Method: 采用双队列设计实现时空解耦，对短提示引入长度感知批处理与CUDA图聚类减少干扰。

Result: 相比SGLang，预填充延迟降低30%，多实例下SLO违规减少28%，高并发混合场景吞吐量提升35%。

Conclusion: PLA-Serve有效优化异构LLM服务负载，显著改善延迟、SLO合规性与系统吞吐能力。

Abstract: PLA-Serve identifies and disaggregates requests with different prompt lengths in LLM serving to reduce TTFT latency. While recent systems have decoupled the prefill and decode stages to improve throughput, they still rely on unified scheduling policies that fail to adapt to heterogeneous workload characteristics. We observe that prompt-length variations lead to distinct performance bottlenecks, motivating an adaptive scheduling strategy. PLA-Serve disaggregates multi-turn long-prefill requests from short-prefill ones and introduces a length-aware smart batching mechanism for short-prefill workloads. It adopts a dual-queue design that supports temporal disaggregation on a single prefill instance or spatial disaggregation across multiple instances. For short-prefill batches, a batch waiting window and CUDA Graph-based clustering mitigate interference from heterogeneous computation, reducing batching delay and lowering average latency. In real multi-turn workloads, PLA-Serve reduces prefill latency by over 30% compared to vanilla SGLang under prefill**--**decode disaggregation, and further decreases SLO violations by 28% in multi-instance deployments with vanilla data-parallel configuration. Compared to the SGLang router with load balancing, it further lowers SLO violations by 12% in multi-GPU settings. Under high concurrency and mixed-request scenarios, PLA-Serve improves request throughput by 35% serving Qwen2.5-32B model for prefill instance, demonstrating its effectiveness in optimizing heterogeneous LLM serving workloads.

</details>


### [66] [EPD-Serve: A Flexible Multimodal EPD Disaggregation Inference Serving System On Ascend](https://arxiv.org/abs/2601.11590)
*Fan Bai,Pai Peng,Zhengzhi Tang,Zhe Wang,Gong Chen,Xiang Lu,Yinuo Li,Huan Lin,Weizhe Lin,Yaoyuan Wang,Xiaosong Li*

Main category: cs.DC

TL;DR: EPD-Serve 提出了一种面向多模态大模型的阶段级解耦推理系统，通过异构硬件上的动态编排与通信优化，显著提升吞吐量并满足严格 SLO。


<details>
  <summary>Details</summary>
Motivation: 现有单体架构在多模态推理中资源利用率低、吞吐受限，需针对各阶段计算特性进行解耦优化。

Method: 将推理流程拆分为独立的 Encode、Prefill、Decode 阶段，结合异步特征预取、分层 KV 缓存传输、多路径调度与负载均衡等机制实现高效协同。

Result: 在高并发场景下，相比 PD 解耦部署，端到端吞吐提升 57.37-69.48%，同时满足 TTFT < 2000ms 与 TPOT < 50ms 的 SLO 要求。

Conclusion: 阶段级解耦能有效优化多模态大模型推理系统的性能与资源效率。

Abstract: With the widespread adoption of large multimodal models, efficient inference across text, image, audio, and video modalities has become critical. However, existing multimodal inference systems typically employ monolithic architectures that tightly couple the Encode, Prefill, and Decode stages on homogeneous hardware, neglecting the heterogeneous computational characteristics of each stage. This design leads to inefficient resource utilization and limited system throughput. To address these issues, we propose EPD-Serve, a stage-level disaggregated inference serving system for multimodal models. EPD-Serve decouples the inference pipeline into independent Encode, Prefill, and Decode stages, enabling logical isolation and flexible co-located deployment through dynamic orchestration. Leveraging the Ascend interconnect topology, EPD-Serve introduces asynchronous feature prefetching between Encode and Prefill stages and a hierarchical grouped KV cache transmission mechanism between Prefill and Decode stages to improve cross-node communication efficiency. In addition, EPD-Serve incorporates multi-route scheduling, instance-level load balancing, and multi-stage hardware co-location with spatial multiplexing to better support diverse multimodal workloads. Comprehensive experiments on multimodal understanding models demonstrate that, under high-concurrency scenarios, EPD-Serve improves end-to-end throughput by 57.37-69.48% compared to PD-disaggregated deployment, while satisfying strict SLO constraints, including TTFT below 2000 ms and TPOT below 50 ms. These results highlight the effectiveness of stage-level disaggregation for optimizing multimodal large model inference systems.

</details>


### [67] [Enhancing Model Context Protocol (MCP) with Context-Aware Server Collaboration](https://arxiv.org/abs/2601.11595)
*Meenakshi Amulya Jayanti,X. Y. Han*

Main category: cs.DC

TL;DR: 本文提出了一种上下文感知的MCP框架（CA-MCP），通过共享上下文存储提升多智能体协作效率，减少LLM调用次数与响应失败率。


<details>
  <summary>Details</summary>
Motivation: 传统MCP框架缺乏全局上下文，导致任务冗余与协调低效，亟需引入共享上下文机制优化多智能体工作流。

Method: 设计CA-MCP架构，使MCP服务器能读写共享上下文内存，以状态跟踪和变量共享实现自主实时协作。

Result: 在TravelPlanner与REALM-Bench数据集上实验表明，CA-MCP显著减少LLM调用次数并降低任务失败率，提升系统效率与响应性。

Conclusion: 引入共享上下文存储的CA-MCP能有效增强LLM驱动多智能体系统的协作连贯性与执行效率。

Abstract: The Model Context Protocol (MCP) has emerged as a widely used framework for enabling LLM-based agents to communicate with external tools and services. The most common implementation of MCP, proposed by Anthropic, heavily relies on a Large Language Model (LLM) to decompose tasks and issue instructions to servers, which act as stateless executors. In particular, the agents, models, and servers are stateless and do not have access to a global context. However, in tasks involving LLM-driven coordination, it is natural that a Shared Context Store (SCS) could improve the efficiency and coherence of multi-agent workflows by reducing redundancy and enabling knowledge transfer between servers. Thus, in this work, we design and assess the performance of a Context-Aware MCP (CA-MCP) that offloads execution logic to specialized MCP servers that read from and write to a shared context memory, allowing them to coordinate more autonomously in real time. In this design, context management serves as the central mechanism that maintains continuity across task executions by tracking intermediate states and shared variables, thereby enabling persistent collaboration among agents without repeated prompting. We present experiments showing that the CA-MCP can outperform the traditional MCP by reducing the number of LLM calls required for complex tasks and decreasing the frequency of response failures when task conditions are not satisfied, thereby improving overall efficiency and responsiveness. In particular, we conducted experiments on the TravelPlanner and REALM-Bench benchmark datasets and observed statistically significant results indicating the potential advantages of incorporating a shared context store via CA-MCP in LLM-driven multi-agent systems.

</details>


### [68] [Hardware-Aware Reformulation of Convolutions for Efficient Execution on Specialized AI Hardware: A Case Study on NVIDIA Tensor Cores](https://arxiv.org/abs/2601.11608)
*Ganesh Bikshandi*

Main category: cs.DC

TL;DR: 本文提出一种硬件感知的CNN计算重写方法，通过数学重构满足硬件对齐要求，无需修改权重，为语义调优奠定基础。


<details>
  <summary>Details</summary>
Motivation: 传统零填充方法效率低，需硬件感知优化以提升CNN在专用AI硬件上的性能。

Method: 使用重写规则对CNN计算进行数学重构，使其在训练后满足硬件对齐要求。

Result: 当前实现聚焦Tensor Core单变换，方法可推广至CPU及其他加速器。

Conclusion: 本研究是语义调优策略的初步探索，旨在系统化优化CNN模型在专用硬件上的部署效率。

Abstract: Convolutional Neural Networks (CNNs) are central to modern AI, but their performance is often limited by hardware constraints. NVIDIA Tensor Cores, for instance, require input channels to be multiples of 8 and sometimes 512 for efficient execution. {\em oneDNN} framework for CPU imposes such a requirement for the blocked format. Traditional approaches address such alignment issue using zero-padding, which can be inefficient. In this work, we present a first-step, hardware-aware reformulation of CNN computations using rewrite rules, restructuring the underlying math to satisfy hardware alignment entirely {\bf post-training} without modifying network weights. While our current implementation focuses on a single transformation for Tensor Cores, this approach is generalizable, laying the foundation to explore additional transformations for CPU and accelerators. This study represents an initial step toward {\em semantic tuning}, a systematic, hardware-aware optimization strategy for efficient deployment of CNN models on specialized AI hardware.

</details>


### [69] [A Forward Simulation-Based Hierarchy of Linearizable Concurrent Objects](https://arxiv.org/abs/2601.11646)
*Chao Wang,Ruijia Li,Yang Zhou,Peng Wu,Yi Lv,Jianwei Liao,Jim Woodcock,Zhiming Liu*

Main category: cs.DC

TL;DR: 本文研究线性化对象与前向模拟之间的关系，提出线性化的新等价刻画，并验证其在Herlihy-Wing队列等对象上的适用性。


<details>
  <summary>Details</summary>
Motivation: 厘清线性化对象与前向模拟的理论联系，为并发对象的正确性验证提供新工具。

Method: 通过构建有界格结构并利用前向模拟关系，将线性化验证转化为对特定对象的模拟检查。

Result: 证明了满足不同活性约束的线性化对象构成半格或格结构，并验证了时间戳队列与Herlihy-Wing队列间的模拟关系。

Conclusion: 所提出的线性化等价刻画可有效用于并发对象的线性化验证。

Abstract: In this paper, we systematically investigate the connection between linearizable objects and forward simulation. We prove that the sets of linearizable objects satisfying wait-freedom (resp., lock-freedom or obstruction-freedom) form a bounded join-semilattice under the forward simulation relation, and that the sets of linearizable objects without liveness constraints form a bounded lattice under the same relation. As part of our lattice result, we propose an equivalent characterization of linearizability by reducing checking linearizability w.r.t. sequential specification $Spec$ into checking forward simulation by an object $\mathcal{U}_{Spec}$. To demonstrate the forward simulation relation between linearizable objects, we prove that the objects that are strongly linearizable w.r.t. the same sequential specification and are wait-free (resp., lock-free, obstruction-free) simulate each other, and we prove that the time-stamped queue simulates the Herlihy-Wing queue. We also prove that the Herlihy-Wing queue is simulated by $\mathcal{U}_{Spec}$, and thus, our equivalent characterization of linearizability can be used in the verification of linearizability.

</details>


### [70] [WISP: Waste- and Interference-Suppressed Distributed Speculative LLM Serving at the Edge via Dynamic Drafting and SLO-Aware Batching](https://arxiv.org/abs/2601.11652)
*Xiangchen Li,Jiakun Fan,Qingyuan Wang,Dimitrios Spatharakis,Saeid Ghafouri,Hans Vandierendonck,Deepu John,Bo Ji,Ali R. Butt,Dimitrios S. Nikolopoulos*

Main category: cs.DC

TL;DR: WISP通过智能推测控制、验证时间估计和批处理调度，优化分布式大语言模型推理效率。


<details>
  <summary>Details</summary>
Motivation: 边缘设备未被充分利用，导致云端计算负载过重和资源浪费。

Method: 提出WISP系统，包含智能推测控制器、验证时间估计器和验证批调度器。

Result: 相比集中式服务和SLED，系统容量提升最高达2.1倍和4.1倍，系统吞吐提升最高达1.94倍和3.7倍。

Conclusion: WISP有效缓解了分布式推测解码中的瓶颈，显著提升推理效率与资源利用率。

Abstract: As Large Language Models (LLMs) become increasingly accessible to end users, an ever-growing number of inference requests are initiated from edge devices and computed on centralized GPU clusters. However, the resulting exponential growth in computation workload is placing significant strain on data centers, while edge devices remain largely underutilized, leading to imbalanced workloads and resource inefficiency across the network. Integrating edge devices into the LLM inference process via speculative decoding helps balance the workload between the edge and the cloud, while maintaining lossless prediction accuracy. In this paper, we identify and formalize two critical bottlenecks that limit the efficiency and scalability of distributed speculative LLM serving: Wasted Drafting Time and Verification Interference. To address these challenges, we propose WISP, an efficient and SLO-aware distributed LLM inference system that consists of an intelligent speculation controller, a verification time estimator, and a verification batch scheduler. These components collaboratively enhance drafting efficiency and optimize verification request scheduling on the server. Extensive numerical results show that WISP improves system capacity by up to 2.1x and 4.1x, and increases system goodput by up to 1.94x and 3.7x, compared to centralized serving and SLED, respectively.

</details>


### [71] [CPU-less parallel execution of lambda calculus in digital logic](https://arxiv.org/abs/2601.13040)
*Harry Fitchett,Charles Fox*

Main category: cs.DC

TL;DR: 本文提出一种将函数式语言直接编译为数字逻辑的并行计算架构，以摆脱传统CPU限制，通过λ演算实现并行化硬件执行。


<details>
  <summary>Details</summary>
Motivation: 晶体管密度持续增长但时钟频率停滞，促使探索无CPU的纯并行架构，利用函数式语言固有并行性突破冯·诺依曼瓶颈。

Method: 采用树状结构表示算法，节点本地化数据并通过总线通信；节点类型对应λ语法形式，并行执行β归约；以λ演算为源语言编译至数字逻辑块。

Result: 成功实现并仿真运行λ表达式，验证该架构可行性，表明可扩展至更复杂的函数式语言。

Conclusion: 该方法为无CPU的函数式计算提供了新模型基础，有望支持更大规模函数式语言的高效并行硬件实现。

Abstract: While transistor density is still increasing, clock speeds are not, motivating the search for new parallel architectures. One approach is to completely abandon the concept of CPU -- and thus serial imperative programming -- and instead to specify and execute tasks in parallel, compiling from programming languages to data flow digital logic. It is well-known that pure functional languages are inherently parallel, due to the Church-Rosser theorem, and CPU-based parallel compilers exist for many functional languages. However, these still rely on conventional CPUs and their von Neumann bottlenecks. An alternative is to compile functional languages directly into digital logic to maximize available parallelism. It is difficult to work with complete modern functional languages due to their many features, so we demonstrate a proof-of-concept system using lambda calculus as the source language and compiling to digital logic. We show how functional hardware can be tailored to a simplistic functional language, forming the ground for a new model of CPU-less functional computation. At the algorithmic level, we use a tree-based representation, with data localized within nodes and communicated data passed between them. This is implemented by physical digital logic blocks corresponding to nodes, and buses enabling message passing. Node types and behaviors correspond to lambda grammar forms, and beta-reductions are performed in parallel allowing branches independent from one another to perform transformations simultaneously. As evidence for this approach, we present an implementation, along with simulation results, showcasing successful execution of lambda expressions. This suggests that the approach could be scaled to larger functional languages. Successful execution of a test suite of lambda expressions suggests that the approach could be scaled to larger functional languages.

</details>


### [72] [RAPID-Serve: Resource-efficient and Accelerated P/D Intra-GPU Disaggregation](https://arxiv.org/abs/2601.11822)
*Amna Masood,Pratishtha Gaur,Nuwan Jayasena*

Main category: cs.DC

TL;DR: RAPID-Serve 提出在单 GPU 上并发执行预填充与解码，结合自适应资源管理，在满足延迟 SLO 的同时显著提升吞吐量和资源利用率。


<details>
  <summary>Details</summary>
Motivation: 现有混合批处理增加延迟，解耦服务导致资源浪费，需兼顾低延迟与高吞吐的方案。

Method: 在同 GPU 并发执行预填充与解码，辅以自适应资源分配及 AMD CU 掩码技术。

Result: 无约束下吞吐提升最高 4.1 倍（平均 1.7 倍），SLO 约束下最高 32 倍（平均 4.9 倍）。

Conclusion: RAPID-Serve 在资源受限场景中优于现有方法，兼顾延迟、吞吐与效率。

Abstract: Two widely adopted techniques for LLM inference serving systems today are hybrid batching and disaggregated serving. A hybrid batch combines prefill and decode tokens of different requests in the same batch to improve resource utilization and throughput at the cost of increased latency per token. In contrast, disaggregated serving decouples compute-bound prefill and bandwidth-bound decode phases to optimize for service level objectives (SLOs) at the cost of resource under-utilization and KV-cache transfer overheads. To address the limitations of these techniques, we propose RAPID-Serve: a technique to concurrently execute prefill and decode on the same GPU(s) to meet latency SLOs while maintaining high throughput and efficient resource utilization. Furthermore, we propose Adaptive Resource Management for runtime compute resource allocation, optionally leveraging CU masking (a fine-grained Compute Unit partitioning feature on AMD Instinct\textsuperscript{TM} GPUs). RAPID-Serve provides up to 4.1x (average 1.7x) unconstrained throughput improvement and 32x and higher (average 4.9x) throughput improvement under SLO constraints, showing it as an effective strategy compared to the state-of-the-art approaches, particularly in resource-constrained environments.

</details>


### [73] [Multi-Partner Project: Multi-GPU Performance Portability Analysis for CFD Simulations at Scale](https://arxiv.org/abs/2601.14159)
*Panagiotis-Eleftherios Eleftherakis,George Anagnostopoulos,Anastassis Kapetanakis,Mohammad Umair,Jean-Yves Vet,Konstantinos Iliakis,Jonathan Vincent,Jing Gong,Akshay Patil,Clara García-Sánchez,Gerardo Zampino,Ricardo Vinuesa,Sotirios Xydis*

Main category: cs.DC

TL;DR: 本文分析了SOD2D谱元法CFD框架在AMD与NVIDIA GPU上的性能可移植性，揭示内存优化效果差异及多级调优必要性。


<details>
  <summary>Details</summary>
Motivation: 异构超算架构普及下，CFD模拟需高效适配不同GPU硬件，实现性能可移植性以支撑城市气流预测等HPC应用。

Method: 通过构建涵盖应用、软件与硬件参数的全栈设计空间，在单卡与LUMI多卡集群上对SOD2D进行多层次性能表征与剖析。

Result: 单GPU测试显示内存访问优化带来0.69×至3.91×加速差异；多GPU集群中吞吐波动表明性能预测存在局限，需依赖多层次调优。

Conclusion: SOD2D在跨厂商GPU架构上具备性能潜力，但需结合多层次参数调优才能实现稳定高效的性能可移植性。

Abstract: As heterogeneous supercomputing architectures leveraging GPUs become increasingly central to high-performance computing (HPC), it is crucial for computational fluid dynamics (CFD) simulations, a de-facto HPC workload, to efficiently utilize such hardware. One of the key challenges of HPC codes is performance portability, i.e. the ability to maintain near-optimal performance across different accelerators. In the context of the \textbf{REFMAP} project, which targets scalable, GPU-enabled multi-fidelity CFD for urban airflow prediction, this paper analyzes the performance portability of SOD2D, a state-of-the-art Spectral Elements simulation framework across AMD and NVIDIA GPU architectures. We first discuss the physical and numerical models underlying SOD2D, highlighting its computational hotspots. Then, we examine its performance and scalability in a multi-level manner, i.e. defining and characterizing an extensive full-stack design space spanning across application, software and hardware infrastructure related parameters. Single-GPU performance characterization across server-grade NVIDIA and AMD GPU architectures and vendor-specific compiler stacks, show the potential as well as the diverse effect of memory access optimizations, i.e. 0.69$\times$ - 3.91$\times$ deviations in acceleration speedup. Performance variability of SOD2D at scale is further examined on the LUMI multi-GPU cluster, where profiling reveals similar throughput variations, highlighting the limits of performance projections and the need for multi-level, informed tuning.

</details>


### [74] [Big Data Workload Profiling for Energy-Aware Cloud Resource Management](https://arxiv.org/abs/2601.11935)
*Milan Parikh,Aniket Abhishek Soni,Sneja Mitinbhai Shah,Ayush Raj Jha*

Main category: cs.DC

TL;DR: 该论文提出了一种基于工作负载感知的节能调度框架，通过分析CPU、内存和存储IO行为优化虚拟机部署，在保证性能的同时实现15-20%的能耗降低。


<details>
  <summary>Details</summary>
Motivation: 应对大数据工作负载增长带来的云数据中心能耗压力，提升可持续性。

Method: 结合历史日志与实时遥测数据，预测候选部署方案的能耗与性能影响，实现自适应整合。

Result: 在多节点云测试平台上，相比基线调度器实现15%-20%的稳定节能，性能损失可忽略。

Conclusion: 工作负载画像是一种实用且可扩展的策略，有助于提升云环境大数据处理的能效与可持续性。

Abstract: Cloud data centers face increasing pressure to reduce operational energy consumption as big data workloads continue to grow in scale and complexity. This paper presents a workload aware and energy efficient scheduling framework that profiles CPU utilization, memory demand, and storage IO behavior to guide virtual machine placement decisions. By combining historical execution logs with real time telemetry, the proposed system predicts the energy and performance impact of candidate placements and enables adaptive consolidation while preserving service level agreement compliance. The framework is evaluated using representative Hadoop MapReduce, Spark MLlib, and ETL workloads deployed on a multi node cloud testbed. Experimental results demonstrate consistent energy savings of 15 to 20 percent compared to a baseline scheduler, with negligible performance degradation. These findings highlight workload profiling as a practical and scalable strategy for improving the sustainability of cloud based big data processing environments.

</details>


### [75] [DaggerFFT: A Distributed FFT Framework Using Task Scheduling in Julia](https://arxiv.org/abs/2601.12209)
*Sana Taghipour Anvari,Julian Samaroo,Matin Raayai Ardakani,David Kaeli*

Main category: cs.DC

TL;DR: DaggerFFT是一个基于Julia的分布式FFT框架，通过动态任务调度在CPU和GPU集群上实现优于现有库的性能。


<details>
  <summary>Details</summary>
Motivation: 传统FFT算法在异构高性能计算平台上存在性能瓶颈，需要更高效的分布式解决方案。

Method: 将FFT计算建模为动态调度的任务图，利用Dagger的动态调度器跨设备分配任务，避免静态分配与同步屏障限制。

Result: 在CPU集群上最高提速2.6倍，GPU集群上提速1.35倍，并成功集成到Oceananigans.jl用于真实大规模模拟。

Conclusion: 基于任务的高层运行时系统可在保持模块化的同时，在异构HPC系统上实现卓越FFT性能。

Abstract: The Fast Fourier Transform (FFT) is a fundamental numerical technique with widespread application in a range of scientific problems. As scientific simulations attempt to exploit exascale systems, there has been a growing demand for distributed FFT algorithms that can effectively utilize modern heterogeneous high-performance computing (HPC) systems. Conventional FFT algorithms commonly encounter performance bottlenecks, especially when run on heterogeneous platforms. Most distributed FFT approaches rely on static task distribution and require synchronization barriers, limiting scalability and impacting overall resource utilization. In this paper we present DaggerFFT, a distributed FFT framework, developed in Julia, that treats highly parallel FFT computations as a dynamically scheduled task graph. Each FFT stage operates on a separately defined distributed array. FFT operations are expressed as DTasks operating on pencil or slab partitioned DArrays. Each FFT stage owns its own DArray, and the runtime assigns DTasks across devices using Dagger's dynamic scheduler that uses work stealing. We demonstrate how DaggerFFT's dynamic scheduler can outperform state-of-the-art distributed FFT libraries on both CPU and GPU backends, achieving up to a 2.6x speedup on CPU clusters and up to a 1.35x speedup on GPU clusters. We have integrated DaggerFFT into Oceananigans.jl, a geophysical fluid dynamics framework, demonstrating that high-level, task-based runtimes can deliver both superior performance and modularity in large-scale, real-world simulations.

</details>


### [76] [Power Aware Dynamic Reallocation For Inference](https://arxiv.org/abs/2601.12241)
*Yiwei Jiang,Sangeeta Chowdhary,Nathaniel Morris,Rutwik Jain,Srilatha Manne,Sam Bayliss*

Main category: cs.DC

TL;DR: RAPID是一种面向功率感知的解耦推理框架，通过动态分配GPU角色与功率预算，在固定功耗限制下提升LLM推理性能与一致性。


<details>
  <summary>Details</summary>
Motivation: 随着模型和集群规模扩大，功耗而非算力成为性能与成本效率的主要瓶颈，需优化现有解耦推理方案。

Method: RAPID结合静态与动态功耗重分配及GPU重分配策略，在严格功耗约束下联合管理资源。

Result: 相比静态分配方案，RAPID在峰值负载下SLO达成率最高提升2倍，且不增加复杂度或成本。

Conclusion: RAPID有效突破当前解耦推理方案的性能瓶颈，显著提升大语言模型推理在功耗受限环境下的效率与稳定性。

Abstract: Disaggregation has emerged as a powerful strategy for optimizing large language model (LLM) inference by separating compute-intensive prefill and memory-bound decode phases across specialized GPUs. This separation improves utilization and throughput under fixed hardware capacity. However, as model and cluster scales grow, power, rather than compute, has become the dominant limiter of overall performance and cost efficiency. In this paper, we propose RAPID, a power-aware disaggregated inference framework that jointly manages GPU roles and power budgets to sustain goodput within strict power caps. RAPID utilizes static and dynamic power reallocation in addition to GPU reallocation to improve performance under fixed power bounds. RAPID improves overall performance and application consistency beyond what is achievable in current disaggregation solutions, resulting in up to a 2x improvement in SLO attainment at peak load when compared to a static assignment without an increase in complexity or cost.

</details>


### [77] [RIPPLE++: An Incremental Framework for Efficient GNN Inference on Evolving Graphs](https://arxiv.org/abs/2601.12347)
*Pranjal Naman,Parv Agarwal,Hrishikesh Haritas,Yogesh Simmhan*

Main category: cs.DC

TL;DR: RIPPLE++ 是一种用于动态图的流式 GNN 推理框架，支持高效准确地更新嵌入，适用于单机和分布式环境，显著提升吞吐量并降低通信开销。


<details>
  <summary>Details</summary>
Motivation: 现有 GNN 推理方法难以应对动态图频繁更新带来的冗余计算、高通信成本等问题，尤其在实时应用中延迟过高。

Method: 提出广义增量编程模型，捕捉 GNN 聚合语义，增量传播更新至受影响邻域，支持顶点/边增删及特征更新。

Result: 单机下最高达 56K 更新/秒（Arxiv），分布式下吞吐提升约 25 倍，通信成本降低 20 倍，优于现有基线 2.2–24 倍。

Conclusion: RIPPLE++ 显著提升动态图 GNN 推理效率与可扩展性，满足低延迟实时应用需求。

Abstract: Real-world graphs are dynamic, with frequent updates to their structure and features due to evolving vertex and edge properties. These continual changes pose significant challenges for efficient inference in graph neural networks (GNNs). Existing vertex-wise and layer-wise inference approaches are ill-suited for dynamic graphs, as they incur redundant computations, large neighborhood traversals, and high communication costs, especially in distributed settings. Additionally, while sampling-based approaches can be adopted to approximate final layer embeddings, these are often not preferred in critical applications due to their non-determinism. These limitations hinder low-latency inference required in real-time applications. To address this, we propose RIPPLE++, a framework for streaming GNN inference that efficiently and accurately updates embeddings in response to changes in the graph structure or features. RIPPLE++ introduces a generalized incremental programming model that captures the semantics of GNN aggregation functions and incrementally propagates updates to affected neighborhoods. RIPPLE++ accommodates all common graph updates, including vertex/edge addition/deletions and vertex feature updates. RIPPLE++ supports both single-machine and distributed deployments. On a single machine, it achieves up to $56$K updates/sec on sparse graphs like Arxiv ($169$K vertices, $1.2$M edges), and about $7.6$K updates/sec on denser graphs like Products ($2.5$M vertices, $123.7$M edges), with latencies of $0.06$--$960$ms, and outperforming state-of-the-art baselines by $2.2$--$24\times$ on throughput. In distributed settings, RIPPLE++ offers up to $\approx25\times$ higher throughput and $20\times$ lower communication costs compared to recomputing baselines.

</details>


### [78] [ASAS-BridgeAMM: Trust-Minimized Cross-Chain Bridge AMM with Failure Containment](https://arxiv.org/abs/2601.12434)
*Shengwei You,Aditya Joshi,Andrey Kuehlkamp,Jarek Nabrzyski*

Main category: cs.DC

TL;DR: ASAS-BridgeAMM 通过引入‘受控降级’机制，显著降低跨链桥系统性风险，在保持高交易量的同时提升安全性和抗攻击能力。


<details>
  <summary>Details</summary>
Motivation: 现有跨链桥因安全模型过于二元化（完全正常或彻底崩溃），缺乏中间容错状态，导致巨额资金损失。

Method: 将跨链消息延迟视为可量化执行风险，动态调整抵押折扣、滑点边界和提款限额，并在 Solidity 中实现参考协议。

Result: 历史回测显示最坏情况偿付能力损失减少73%，压力期交易量保留104.5%；对抗模拟中偿付概率>0.9999，每周期坏账<0.2%。

Conclusion: 该协议形式化证明了安全性、活性与抗操控性，为 DeFi 跨链桥提供更稳健的架构范式。

Abstract: Cross-chain bridges constitute the single largest vector of systemic risk in Decentralized Finance (DeFi), accounting for over \$2.8 billion in losses since 2021. The fundamental vulnerability lies in the binary nature of existing bridge security models: a bridge is either fully operational or catastrophically compromised, with no intermediate state to contain partial failures. We present ASAS-BridgeAMM, a bridge-coupled automated market maker that introduces Contained Degradation: a formally specified operational state where the system gracefully degrades functionality in response to adversarial signals. By treating cross-chain message latency as a quantifiable execution risk, the protocol dynamically adjusts collateral haircuts, slippage bounds, and withdrawal limits. Across 18 months of historical replay on Ethereum and two auxiliary chains, ASAS-BridgeAMM reduces worst-case bridge-induced insolvency by 73% relative to baseline mint-and-burn architectures, while preserving 104.5% of transaction volume during stress periods. In rigorous adversarial simulations involving delayed finality, oracle manipulation, and liquidity griefing, the protocol maintains solvency with probability $>0.9999$ and bounds per-epoch bad debt to $<0.2%$ of total collateral. We provide a reference implementation in Solidity and formally prove safety (bounded debt), liveness (settlement completion), and manipulation resistance under a Byzantine relayer model.

</details>


### [79] [SGCP: A Self-Organized Game-Theoretic Framework For Collaborative Perception](https://arxiv.org/abs/2601.12524)
*Zechuan Gong,Hui Zhang,Yuquan Yang,Wenyu Lu*

Main category: cs.DC

TL;DR: 提出一种去中心化协作感知框架，通过车辆自组织分组与选择性点云传输，在有限带宽下提升自动驾驶感知精度与覆盖范围。


<details>
  <summary>Details</summary>
Motivation: 解决大规模协作感知中通信带宽受限且无路侧设施支持的部署难题。

Method: 采用两阶段博弈论方法：第一阶段基于感知互补与运动一致性形成稳定簇并选举协调者；第二阶段由协调者引导成员通过非合作势博弈选择性传输显著区域点云，并跨簇交换紧凑检测消息实现全局场景理解。

Result: 在CARLA-OpenCDA-NS3平台实验表明，该方法相比基线降低通信开销，同时提高感知精度与有效覆盖范围。

Conclusion: 所提框架在资源受限环境下实现了高效、准确的协作感知，具备实际部署潜力。

Abstract: Collaborative perception holds great promise for improving safety in autonomous driving, particularly in dense traffic where vehicles can share sensory information to overcome individual blind spots and extend awareness. However, deploying such collaboration at scale remains difficult when communication bandwidth is limited and no roadside infrastructure is available. To overcome these limitations, we introduce a fully decentralized framework that enables vehicles to self organize into cooperative groups using only vehicle to vehicle communication. The approach decomposes the problem into two sequential game theoretic stages. In the first stage, vehicles form stable clusters by evaluating mutual sensing complementarity and motion coherence, and each cluster elects a coordinator. In the second stage, the coordinator guides its members to selectively transmit point cloud segments from perceptually salient regions through a non cooperative potential game, enabling efficient local fusion. Global scene understanding is then achieved by exchanging compact detection messages across clusters rather than raw sensor data. We design distributed algorithms for both stages that guarantee monotonic improvement of the system wide potential function. Comprehensive experiments on the CARLA-OpenCDA-NS3 co-simulation platform show that our method reduces communication overhead while delivering higher perception accuracy and wider effective coverage compared to existing baselines.

</details>


### [80] [Dynamic Detection of Inefficient Data Mapping Patterns in Heterogeneous OpenMP Applications](https://arxiv.org/abs/2601.12713)
*Luke Marzen,Junhyung Shim,Ali Jannesari*

Main category: cs.DC

TL;DR: OMPDataPerf 是一种动态分析工具，用于检测异构计算中低效的数据传输与分配模式，通过 OpenMP 工具接口实现，仅带来 5% 的运行时开销。


<details>
  <summary>Details</summary>
Motivation: 异构计算中设备间数据移动是性能瓶颈，现有工具需大量人工干预才能诊断问题。

Method: 提出动态分析技术，结合 OMPT 接口实现自动化检测与源码归因。

Result: 工具可生成详细的问题数据映射轨迹、源码定位及优化潜力评估，平均运行时开销仅 5%。

Conclusion: OMPDataPerf 有效降低异构应用开发复杂度，提升数据传输效率诊断能力。

Abstract: With the growing prevalence of heterogeneous computing, CPUs are increasingly being paired with accelerators to achieve new levels of performance and energy efficiency. However, data movement between devices remains a significant bottleneck, complicating application development. Existing performance tools require considerable programmer intervention to diagnose and locate data transfer inefficiencies. To address this, we propose dynamic analysis techniques to detect and profile inefficient data transfer and allocation patterns in heterogeneous applications. We implemented these techniques into OMPDataPerf, which provides detailed traces of problematic data mappings, source code attribution, and assessments of optimization potential in heterogeneous OpenMP applications. OMPDataPerf uses the OpenMP Tools Interface (OMPT) and incurs only a 5 % geometric-mean runtime overhead.

</details>


### [81] [Efficient Local-to-Global Collaborative Perception via Joint Communication and Computation Optimization](https://arxiv.org/abs/2601.12749)
*Hui Zhang,Yuquan Yang,Zechuan Gong,Xiaohua Xu,Dan Keun Sung*

Main category: cs.DC

TL;DR: 提出了一种局部到全局的协作感知框架（LGCP），通过区域划分与集中调度显著降低通信开销并提升感知效率。


<details>
  <summary>Details</summary>
Motivation: 解决协作感知中因数据冗余和计算负载增加导致的高通信开销与延迟问题。

Method: 将道路划分为非重叠区域，每区由专用车辆组进行局部感知，组长融合数据上传路侧单元（RSU），RSU聚合后广播全局视图；采用RSU集中调度策略管理分组、传输与结果聚合。

Result: 实验表明LGCP平均减少44倍数据传输量，同时保持或提升整体协作感知性能。

Conclusion: LGCP在保证感知精度的同时，有效实现了通信与计算资源的高效利用，适用于大规模车联网环境。

Abstract: Autonomous driving relies on accurate perception to ensure safe driving. Collaborative perception improves accuracy by mitigating the sensing limitations of individual vehicles, such as limited perception range and occlusion-induced blind spots. However, collaborative perception often suffers from high communication overhead due to redundant data transmission, as well as increasing computation latency caused by excessive load with growing connected and autonomous vehicles (CAVs) participation. To address these challenges, we propose a novel local-to-global collaborative perception framework (LGCP) to achieve collaboration in a communication- and computation-efficient manner. The road of interest is partitioned into non-overlapping areas, each of which is assigned a dedicated CAV group to perform localized perception. A designated leader in each group collects and fuses perception data from its members, and uploads the perception result to the roadside unit (RSU), establishing a link between local perception and global awareness. The RSU aggregates perception results from all groups and broadcasts a global view to all CAVs. LGCP employs a centralized scheduling strategy via the RSU, which assigns CAV groups to each area, schedules their transmissions, aggregates area-level local perception results, and propagates the global view to all CAVs. Experimental results demonstrate that the proposed LGCP framework achieves an average 44 times reduction in the amount of data transmission, while maintaining or even improving the overall collaborative performance.

</details>


### [82] [Unleashing Efficient Asynchronous RL Post-Training via Staleness-Constrained Rollout Coordination](https://arxiv.org/abs/2601.12784)
*Haoyang Li,Sheng Lin,Fangcheng Fu,Yuming Zhou,Xiaodong Ji,Yanfeng Zhao,Lefeng Wang,Jie Jiang,Bin Cui*

Main category: cs.DC

TL;DR: StaleFlow是一个新型强化学习后训练系统，通过全局一致性协议和重新设计的架构，同时解决数据陈旧性和数据倾斜问题，显著提升吞吐量且不损害收敛性。


<details>
  <summary>Details</summary>
Motivation: 现有系统无法统一处理数据陈旧性和数据倾斜问题，导致性能与收敛性之间的权衡。

Method: 引入全局一致性协议跟踪轨迹生命周期以控制陈旧性；构建数据服务器实现灵活的rollout协调，并设计一系列感知陈旧性、面向吞吐量的策略。

Result: 实验表明，StaleFlow相比现有最先进系统，吞吐量提升1.42-2.68倍（平均1.17-2.01倍），且不影响收敛性。

Conclusion: StaleFlow成功在不牺牲收敛性的前提下，高效解决了强化学习后训练中的数据陈旧性和倾斜问题。

Abstract: Reinforcement learning (RL) post-training has become pivotal for enhancing the capabilities of modern large models. A recent trend is to develop RL systems with a fully disaggregated architecture, which decouples the three RL phases (rollout, reward, and training) onto separate resources and executes them asynchronously. However, two critical data-level concerns arise: (1) asynchronous execution leads to data staleness in trajectories (the data generated by rollout) as the model parameters used in rollout may not be up to date, which impairs RL convergence; and (2) the length variation of trajectories introduces severe data skewness, leading to workload imbalance and degraded system performance.
  Existing systems fail to address these two concerns in a unified manner. Techniques that tightly control data staleness often constrain effective data skewness mitigation, while aggressive data skewness mitigation tends to exacerbate data staleness. As a result, systems are forced to trade off convergence for performance, or vice versa. To address this, we propose StaleFlow, an RL post-training system that jointly tackles data staleness and skewness. First, to control staleness, StaleFlow introduces a global consistency protocol that tracks the full lifecycle of each trajectory and constrains staleness. Second, to mitigate skewness, StaleFlow re-designs the RL system architecture by constructing data servers for trajectories and parameters to achieve flexible rollout coordination. Subsequently, we develop a suite of staleness-aware, throughput-oriented strategies to enhance system performance. Evaluations show that StaleFlow achieves up to 1.42-2.68$\times$ (1.17-2.01$\times$ on average) higher throughput than state-of-the-art systems, without compromising convergence.

</details>


### [83] [From Design to Deorbit: A Solar-Electric Autonomous Module for Multi-Debris Remediation](https://arxiv.org/abs/2601.12830)
*Om Mishra,Jayesh Patil,Sathwik Narkedimilli,G Srikantha Sharma,Ananda S,Manjunath K Vanahalli*

Main category: cs.DC

TL;DR: 提出一种结合机械夹持、太阳能推进和自主导航的新型轨道碎片清除架构，实现高效多目标移除。


<details>
  <summary>Details</summary>
Motivation: 当前燃料依赖的清除方法存在局限，需发展可持续的轨道碎片主动移除方案。

Method: 集成机械夹持系统、太阳能NEXT推进器与雷达-EKF导航及DTN通信协议，通过高保真仿真验证性能。

Result: 成功实现800km至100km逆行脱轨，位置RMSE<10m，1秒内数据传输效率达93%。

Conclusion: 该架构为可再生能源驱动的长期多目标碎片清除任务树立了新基准，显著提升轨道管理可持续性。

Abstract: The escalating accumulation of orbital debris threatens the sustainability of space operations, necessitating active removal solutions that overcome the limitations of current fuel-dependent methods. To address this, this study introduces a novel remediation architecture that integrates a mechanical clamping system for secure capture with a high-efficiency, solar-powered NASA Evolutionary Xenon Thruster (NEXT) and autonomous navigation protocols. High-fidelity simulations validate the architecture's capabilities, demonstrating a successful retrograde deorbit from 800 km to 100 km, <10m position Root Mean Square Errors (RMSE) via radar-based Extended Kalman Filter (EKF) navigation, and a 93\% data delivery efficiency within 1 second using Delay/Disruption Tolerant Network (DTN) protocols. This approach significantly advances orbital management by establishing a benchmark for renewable solar propulsion that minimizes reliance on conventional fuels and extends mission longevity for multi-target removal.

</details>


### [84] [Driving Computational Efficiency in Large-Scale Platforms using HPC Technologies](https://arxiv.org/abs/2601.13424)
*Alexander Martinez Mendez,Antonio J. Rubio-Montero,Carlos J. Barrios H.,Hernán Asorey,Rafael Mayo-García,Luis A. Núñez*

Main category: cs.DC

TL;DR: 该研究分析了LAGO项目在EGI FedCloud平台上的HPC资源使用效率，识别工作负载类型并提出优化建议以提升科学产出。


<details>
  <summary>Details</summary>
Motivation: 提高LAGO项目在复杂天体粒子物理模拟中对HPC资源的利用效率，以增强科研生产力与可持续性。

Method: 通过分析EGI FedCloud平台的历史作业数据，划分主要工作负载类别，并采用CPU利用率、运行时间利用率和I/O模式等关键指标评估性能。

Result: 发现单个模拟任务CPU效率高，但短时测试任务会扭曲整体指标，揭示了具体低效环节并提供数据驱动的优化方向。

Conclusion: 研究结果为优化资源申请、改进工作流管理及提升计算吞吐量提供了依据，有助于最大化LAGO项目HPC投资的科研回报。

Abstract: The Latin American Giant Observatory (LAGO) project utilizes extensive High-Performance Computing (HPC) resources for complex astroparticle physics simulations, making resource efficiency critical for scientific productivity and sustainability. This article presents a detailed analysis focused on quantifying and improving HPC resource utilization efficiency specifically within the LAGO computational environment. The core objective is to understand how LAGO's distinct computational workloads-characterized by a prevalent coarse-grained, task-parallel execution model-consume resources in practice. To achieve this, we analyze historical job accounting data from the EGI FedCloud platform, identifying primary workload categories (Monte Carlo simulations, data processing, user analysis/testing) and evaluating their performance using key efficiency metrics (CPU utilization, walltime utilization, and I/O patterns). Our analysis reveals significant patterns, including high CPU efficiency within individual simulation tasks contrasted with the distorting impact of short test jobs on aggregate metrics. This work pinpoints specific inefficiencies and provides data-driven insights into LAGO's HPC usage. The findings directly inform recommendations for optimizing resource requests, refining workflow management strategies, and guiding future efforts to enhance computational throughput, ultimately maximizing the scientific return from LAGO's HPC investments.

</details>


### [85] [RASC: Enhancing Observability & Programmability in Smart Spaces](https://arxiv.org/abs/2601.13496)
*Anna Karanika,Kai-Siang Wang,Han-Ting Liang,Shalni Sundram,Indranil Gupta*

Main category: cs.DC

TL;DR: RASC抽象通过在IoT设备动作的关键节点提供确认机制，提升了可观察性和可编程性，并支持更优的调度策略。


<details>
  <summary>Details</summary>
Motivation: 现有RPC机制难以满足面向用户的智能空间中IoT设备动作在时空维度上的多样性需求。

Method: 提出RASC（请求-确认-开始-完成）抽象，构建于现有RPC机制之上，集成到Home Assistant框架中。

Result: 实验表明RASC能满足延迟SLO，尤其对持续数分钟的长动作效果显著，调度策略性能提升10%-55%。

Conclusion: RASC为IoT系统提供了更适合用户场景的动作管理抽象，增强了功能性和效率。

Abstract: While RPCs form the bedrock of systems stacks, we posit that IoT device collections in smart spaces like homes, warehouses, and office buildings--which are all "user-facing"--require a more expressive abstraction. Orthogonal to prior work, which improved the reliability of IoT communication, our work focuses on improving the observability and programmability of IoT actions. We present the RASC (Request-Acknowledge-Start-Complete) abstraction, which provides acknowledgments at critical points after an IoT device action is initiated. RASC is a better fit for IoT actions, which naturally vary in length spatially (across devices) and temporally (across time, for a given device). RASC also enables the design of several new features: predicting action completion times accurately, detecting failures of actions faster, allowing fine-grained dependencies in programming, and scheduling. RASC is intended to be implemented atop today's available RPC mechanisms, rather than as a replacement. We integrated RASC into a popular and open-source IoT framework called Home Assistant. Our trace-driven evaluation finds that RASC meets latency SLOs, especially for long actions that last O(mins), which are common in smart spaces. Our scheduling policies for home automations (e.g., routines) outperform state-of-the-art counterparts by 10%-55%.

</details>


### [86] [torch-sla: Differentiable Sparse Linear Algebra with Adjoint Solvers and Sparse Tensor Parallelism for PyTorch](https://arxiv.org/abs/2601.13994)
*Mingyuan Chi*

Main category: cs.DC

TL;DR: torchsla是一个支持GPU加速、多GPU扩展和自动微分的PyTorch稀疏线性代数库。


<details>
  <summary>Details</summary>
Motivation: 工业科学计算中广泛使用稀疏矩阵，但缺乏高效、可微分且支持多GPU扩展的工具。

Method: 通过GPU加速求解器、域分解与光环交换实现多GPU扩展，并采用伴随法实现高效自动微分。

Result: 在3个GPU上实现了4亿自由度的线性求解，计算图节点为O(1)，内存占用为O(nnz)。

Conclusion: torchsla为端到端可微模拟提供了高效、灵活且易用的稀疏线性代数解决方案。

Abstract: Industrial scientific computing predominantly uses sparse matrices to represent unstructured data -- finite element meshes, graphs, point clouds. We present \torchsla{}, an open-source PyTorch library that enables GPU-accelerated, scalable, and differentiable sparse linear algebra. The library addresses three fundamental challenges: (1) GPU acceleration for sparse linear solves, nonlinear solves (Newton, Picard, Anderson), and eigenvalue computation; (2) Multi-GPU scaling via domain decomposition with halo exchange, reaching \textbf{400 million DOF linear solve on 3 GPUs}; and (3) Adjoint-based differentiation} achieving $\mathcal{O}(1)$ computational graph nodes (for autograd) and $\mathcal{O}(\text{nnz})$ memory -- independent of solver iterations. \torchsla{} supports multiple backends (SciPy, cuDSS, PyTorch-native) and seamlessly integrates with PyTorch autograd for end-to-end differentiable simulations. Code is available at https://github.com/walkerchi/torch-sla.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [87] [Path to Diversity: A Primer on ISAC-izing Commodity Wi-Fi for Practical Deployments](https://arxiv.org/abs/2601.12980)
*Hongbo Wang,Xin Li,Yinghui He,Jingzhi Hu,Mingming Xu,Zhe Chen,Fu Xiao,Jun Luo*

Main category: cs.NI

TL;DR: 本文从物理层多样性角度系统分析Wi-Fi技术演进如何提升感知性能，提出时间、频率、链路与空间四个正交维度，为实现商品化Wi-Fi的通感一体化提供底层技术指南。


<details>
  <summary>Details</summary>
Motivation: 现有文献多聚焦上层应用或深度学习模型，忽视物理层信号形成机制，缺乏突破感知性能物理瓶颈的技术指导。

Method: 采用自下而上方法，围绕时间、频率、链路与空间四维物理层多样性构建分析框架。

Result: 四维多样性协同解决时域、距离与空间的根本模糊性，打通物理能力与复杂感知需求间的鸿沟。

Conclusion: 本教程为‘通感一体化’商品Wi-Fi提供系统性实现路径，助力未来标准化与稳健部署。

Abstract: Integrated Sensing and Communication (ISAC) has emerged as a key paradigm in next-generation wireless networks. While the ubiquity and low cost of commodity Wi-Fi make it an ideal platform for wide-scale sensing, it is the continuous evolution of Wi-Fi standards-towards higher frequency bands, wider bandwidths, and larger antenna arrays-that fundamentally unlocks the physical resources required for high-performance ISAC. To structure this rapidly expanding field, numerous surveys have appeared. However, prevailing literature predominantly adopts a top-down perspective, emphasizing upper-layer applications or deep learning models while treating the physical layer as an opaque abstraction. Consequently, these works often fail to touch the bottom layer of signal formation and lack technical guidance on overcoming the physical barriers that constrain sensing performance. To bridge this gap, this tutorial takes a bottom-up approach, systematically analyzing the sensing gains brought by Wi-Fi advancements through the lens of physical-layer diversity. We organize the framework around four orthogonal dimensions: i) Temporal Diversity addresses synchronization gaps to enable absolute ranging; ii) Frequency Diversity expands the effective bandwidth to sharpen range resolution; iii) Link Diversity leverages distributed topologies and digital feedback to achieve ubiquitous observability; and iv) Spatial Diversity utilizes multi-antenna arrays to combine passive angular discrimination with active directional control. Collectively, these orthogonal dimensions resolve fundamental ambiguities in time, range, and space, bridging physical capabilities with challenging sensing diversities. By synthesizing these dimensions, this tutorial provides a comprehensive guide for "ISAC-izing" commodity Wi-Fi, paving the way for future standardization and robust deployment.

</details>


### [88] [No Traffic to Cry: Traffic-Oblivious Link Deactivation for Green Traffic Engineering](https://arxiv.org/abs/2601.13087)
*Max Ilsen,Daniel Otten,Nils Aschenbruck,Markus Chimani*

Main category: cs.NI

TL;DR: 提出一种流量无关的路由方案，通过最小化激活连接数实现骨干网节能，避免频繁重配置。


<details>
  <summary>Details</summary>
Motivation: 现有绿色流量工程方法依赖特定流量矩阵，难以快速适应流量变化，且复杂度高。

Method: 设计一个NP难问题的近似算法，保证任意缩放流量矩阵可路由，并引入两种后处理启发式优化解质量。

Result: 实验表明能快速生成近优解，无需频繁重配置，显著提升骨干网能效。

Conclusion: 该方法为骨干网节能提供实用、稳定的解决方案，具有实际部署潜力。

Abstract: As internet traffic grows, the underlying infrastructure consumes increasing amounts of energy. During off-peak hours, large parts of the networks remain underutilized, presenting significant potential for energy savings. Existing Green Traffic Engineering approaches attempt to leverage this potential by switching off those parts of the networks that are not required for the routing of specific traffic matrices. When traffic changes, the approaches need to adapt rapidly, which is hard to achieve given the complexity of the problem. We take a fundamentally different approach: instead of considering a specific traffic matrix, we rely on a traffic-oblivious routing scheme. We discuss the NP-hard problem of activating as few connections as possible while still guaranteeing that any down-scaled traffic matrix $\varrho\cdot T$ can be routed, where $\varrho \in (0,1)$ and $T$ is any traffic matrix routable in the original network. We present a $\max(\frac{1}{\varrho\cdotλ_{\text{min}}},2)$-approximation algorithm for this problem, with $λ_{\text{min}}$ denoting the minimum number of connections between any two connected routers. Additionally, we propose two post-processing heuristics to further improve solution quality. Our evaluation shows that we can quickly generate near-optimal solutions. By design, our method avoids the need for frequent reconfigurations and offers a promising direction to achieve practical energy savings in backbone networks.

</details>


### [89] [Interoperable rApp/xApp Control over O-RAN for Mobility-aware Dynamic Spectrum Allocation](https://arxiv.org/abs/2601.13769)
*Anastasios Giannopoulos,Sotirios Spantideas,Maria Lamprini Bartsioka,Panagiotis Trakadas*

Main category: cs.NI

TL;DR: 本文提出了一种基于O-RAN架构的rApp/xApp驱动动态频谱分配框架，结合图论建模与分层控制，在保障互操作性的同时显著提升资源分配成功率与公平性。


<details>
  <summary>Details</summary>
Motivation: 解决在密集多小区干扰和异构业务需求下，如何协同利用长期流量感知与近实时无线资源优化设计可互操作控制方案的挑战。

Method: 构建基于图论的PRB分配模型，由非实时rApp预测流量并生成频谱策略，近实时xApp构建用户中心冲突图并执行公平感知PRB分配，辅以冲突感知改进比例公平调度机制。

Result: 仿真表明该框架在不同信道配置和用户需求下，PRB分配成功率超90%，服务份额公平性超85%，同时保持O-RAN架构分离与rApp/xApp互操作性。

Conclusion: 所提框架有效实现了O-RAN环境下跨时间尺度的动态频谱分配，在性能与架构合规性之间取得良好平衡。

Abstract: Open Radio Access Networks (O-RAN) enable the disaggregation of radio access functions and the deployment of control applications across different timescales. However, designing interoperable control schemes that jointly exploit long-term traffic awareness and near-real-time radio resource optimization remains a challenging problem, particularly under dense multi-cell interference and heterogeneous service demands. This paper proposes an interoperable rApp/xApp-driven dynamic spectrum allocation (DSA) framework for O-RAN, based on a graph-theoretic formulation of physical resource block (PRB) assignment. The proposed architecture leverages a non-real-time radio intelligent controller (Non-RT RIC) rApp to predict aggregated traffic evolution and generate high-level spectrum policies at the minutes timescale, while a near-real-time RIC (Near-RT RIC) xApp constructs a user-centric conflict graph and performs fairness-aware PRB allocation at sub-second timescales. To mitigate persistent user starvation, a conflict-aware modified proportional fair (MPF) scheduling mechanism is applied, enabling controlled interference-free PRB time-sharing. Extensive simulation results demonstrate that the proposed framework significantly improves the PRB assignment success rate (above 90%) and service-share fairness (above 85%) across different channel configurations and user demands, while maintaining architectural separation and rApp/xApp interoperability in accordance with O-RAN principles.

</details>


### [90] [MANATEE: A DevOps Platform for xApp Lifecycle Management and Testing in Open RAN](https://arxiv.org/abs/2601.14009)
*Sofia Montebugnoli,Leonardo Bonati,Andrea Sabbioni,Luca Foschini,Paolo Bellavista,Salvatore D'Oro,Michele Polese,Tommaso Melodia*

Main category: cs.NI

TL;DR: MANATEE平台结合DevOps与服务网格技术，优化xApp在O-RAN环境中的部署、测试与观测，实现低延迟、高可靠性的渐进式发布。


<details>
  <summary>Details</summary>
Motivation: 当前Open RAN缺乏对xApp的全生命周期管理，导致部署缓慢且易出错，亟需自动化与可观测性支持。

Method: 构建MANATEE平台，集成CI/CD流水线与服务网格（如金丝雀发布、A/B测试），并在Kubernetes+O-RAN RIC环境中原型验证。

Result: 实验证明服务网格引入延迟低于1ms，支持精准流量控制与无冲突A/B测试，提升xApp交付效率与系统韧性。

Conclusion: MANATEE是首个融合DevOps与服务网格的O-RAN xApp管理平台，有效加速创新并保障异构环境性能。

Abstract: The shift to disaggregated 5G architectures introduces unprecedented flexibility but also significant complexity in Beyond 5G Radio Access Networks (RANs). Open RAN enables programmability through xApps, yet deploying and validating these applications is critical given the nature of the systems they aim to control. Current Open RAN ecosystems lack robust lifecycle management of xApps that enable automated testing, seamless migration, and production-grade observability, resulting in slow, error-prone xApp delivery. To address these issues, DevOps practices can streamline the xApp lifecycle by integrating Continuous Integration/Continuous Deployment (CI/CD) pipelines with advanced traffic management and monitoring, such as leveraging service mesh technologies to enable progressive deployment strategies (e.g., canary releases and A/B testing) to ensure fine-grained observability and resilience. The solution presented in this article, MANATEE (Mesh Architecture for Radio Access Network Automation and TEsting Ecosystems), is the first platform that combines these principles to simplify xApp delivery into production, accelerate innovation, and guarantee performance across heterogeneous O-RAN environments. We prototyped MANATEE on a Kubernetes cluster integrated with the O-RAN Software Community Near-Real Time RAN Intelligent Controller (RIC), as well as with service mesh technologies, to facilitate testing of xApps across simulated, emulated, and real testbed environments. Our experimental results demonstrate that service mesh integration introduces minimal overhead (below 1 ms latency), while enabling reliable canary deployments with fine-grained traffic control and conflict-free A/B testing through circuit-breaking mechanisms.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [91] [Generative AI Agents for Controllable and Protected Content Creation](https://arxiv.org/abs/2601.12348)
*Haris Khan,Sadia Asif*

Main category: cs.MA

TL;DR: 提出一种结合可控生成与内容保护的多智能体框架，实现可追溯的负责任生成式AI。


<details>
  <summary>Details</summary>
Motivation: 解决当前生成式AI在可控性和内容保护方面的关键挑战。

Method: 通过导演/规划、生成、评审、整合和保护等专用智能体角色，结合人机反馈与水印机制，构建联合优化流程。

Result: 实现了用户意图对齐、语义一致性及鲁棒水印嵌入的统一目标。

Conclusion: 多智能体架构可作为构建可信创意工作流与内嵌所有权追踪的有效方案。

Abstract: The proliferation of generative AI has transformed creative workflows, yet current systems face critical challenges in controllability and content protection. We propose a novel multi-agent framework that addresses both limitations through specialized agent roles and integrated watermarking mechanisms. Unlike existing multi-agent systems focused solely on generation quality, our approach uniquely combines controllable content synthesis with provenance protection during the generation process itself. The framework orchestrates Director/Planner, Generator, Reviewer, Integration, and Protection agents with human-in-the-loop feedback to ensure alignment with user intent while embedding imperceptible digital watermarks. We formalize the pipeline as a joint optimization objective unifying controllability, semantic alignment, and protection robustness. This work contributes to responsible generative AI by positioning multi-agent architectures as a solution for trustworthy creative workflows with built-in ownership tracking and content traceability.

</details>


### [92] [Semantic Fusion: Verifiable Alignment in Decentralized Multi-Agent Systems](https://arxiv.org/abs/2601.12580)
*Sofiya Zaichyk*

Main category: cs.MA

TL;DR: Semantic Fusion提供了一种去中心化多智能体语义协调框架，支持局部验证与全局一致性，具备可验证的确定性和概率性保障。


<details>
  <summary>Details</summary>
Motivation: 解决去中心化多智能体系统中缺乏形式化语义协调机制的问题，实现无需中央控制的安全自治。

Method: 提出语义融合框架，结合本体验证、局部投影与概率刷新机制，建立行为等价性定理并实现轻量架构仿真。

Result: 250智能体仿真验证了收敛性、通信有界性与容错性，11,000+更新均通过语义验证。

Conclusion: 语义融合为去中心化系统的可验证自主性提供了形式化且可扩展的基础。

Abstract: We present Semantic Fusion (SF), a formal framework for decentralized semantic coordination in multi-agent systems. SF allows agents to operate over scoped views of shared memory, propose structured updates, and maintain global coherence through local ontology-based validation and refresh without centralized control or explicit message passing. The central theoretical result is a bisimulation theorem showing that each agent's local execution is behaviorally equivalent to its projection of the global semantics, in both deterministic and probabilistic settings. This enables safety, liveness, and temporal properties to be verified locally and soundly lifted to the full system. SF supports agents whose update proposals vary across invocations, including those generated by learned or heuristic components, provided updates pass semantic validation before integration. We establish deterministic and probabilistic guarantees ensuring semantic alignment under asynchronous or degraded communication. To validate the model operationally, we implement a lightweight reference architecture that instantiates its core mechanisms. A 250-agent simulation evaluates these properties across over 11,000 validated updates, demonstrating convergence under probabilistic refresh, bounded communication, and resilience to agent failure. Together, these results show that Semantic Fusion can provide a formal and scalable basis for verifiable autonomy in decentralized systems.

</details>


### [93] [Communication Methods in Multi-Agent Reinforcement Learning](https://arxiv.org/abs/2601.12886)
*Christoph Wittner*

Main category: cs.MA

TL;DR: 本文综述了多智能体强化学习中的通信技术，分析29篇文献后指出没有通用最优框架，需依问题选择方法，并强调低计算开销与现实鲁棒性的重要性。


<details>
  <summary>Details</summary>
Motivation: 为解决部分可观测、非平稳性及动作空间爆炸等问题，推动多智能体系统中高效协作，需系统梳理现有通信方法优劣。

Method: 对29篇相关论文进行深度分析，比较显式、隐式、注意力、图结构及分层/角色通信等五类方法。

Result: 发现通信方法选择高度依赖具体问题，且低计算开销对多智能体扩展性至关重要；当前研究缺乏标准化评测和现实鲁棒性。

Conclusion: 未来应聚焦系统级指标标准化与现实通信条件下鲁棒性提升，以增强方法的实际应用价值。

Abstract: Multi-agent reinforcement learning is a promising research area that extends established reinforcement learning approaches to problems formulated as multi-agent systems. Recently, a multitude of communication methods have been introduced to this field to address problems such as partially observable environments, non-stationarity, and exponentially growing action spaces. Communication further enables efficient cooperation among all agents interacting in an environment. This work aims at providing an overview of communication techniques in multi-agent reinforcement learning. By an in-depth analysis of 29 publications on this topic, the strengths and weaknesses of explicit, implicit, attention-based, graph-based, and hierarchical/role-based communication are evaluated. The results of this comparison show that there is no general, optimal communication framework for every problem. On the contrary, the choice of communication depends heavily on the problem at hand. The comparison also highlights the importance of communication methods with low computational overhead to enable scalability to environments where many agents interact. Finally, the paper discusses current research gaps, emphasizing the need for standardized benchmarking of system-level metrics and improved robustness under realistic communication conditions to enhance the real-world applicability of these approaches.

</details>


### [94] [OFA-MAS: One-for-All Multi-Agent System Topology Design based on Mixture-of-Experts Graph Generative Models](https://arxiv.org/abs/2601.12996)
*Shiyuan Li,Yixin Liu,Yu Zheng,Mei Li,Quoc Viet Hung Nguyen,Shirui Pan*

Main category: cs.MA

TL;DR: OFA-TAD是一个通用框架，通过单一模型为任意自然语言描述的任务生成自适应协作图，显著优于传统专用模型。


<details>
  <summary>Details</summary>
Motivation: 当前基于图学习的多智能体系统协作拓扑设计方法泛化能力差，无法跨域共享结构知识。

Method: 提出TAGSE编码器与MoE架构，结合三阶段训练策略：无条件预训练、大规模条件预训练和监督微调。

Result: 在六个多样化基准测试中，OFA-TAD显著超越专用模型，能高效生成高度自适应的MAS拓扑。

Conclusion: OFA-TAD成功实现‘一通百通’的协作拓扑生成，提升多智能体系统在跨域任务中的性能与泛化能力。

Abstract: Multi-Agent Systems (MAS) offer a powerful paradigm for solving complex problems, yet their performance is critically dependent on the design of their underlying collaboration topology. As MAS become increasingly deployed in web services (e.g., search engines), designing adaptive topologies for diverse cross-domain user queries becomes essential. Current graph learning-based design methodologies often adhere to a "one-for-one" paradigm, where a specialized model is trained for each specific task domain. This approach suffers from poor generalization to unseen domains and fails to leverage shared structural knowledge across different tasks. To address this, we propose OFA-TAD, a one-for-all framework that generates adaptive collaboration graphs for any task described in natural language through a single universal model. Our approach integrates a Task-Aware Graph State Encoder (TAGSE) that filters task-relevant node information via sparse gating, and a Mixture-of-Experts (MoE) architecture that dynamically selects specialized sub-networks to drive node and edge prediction. We employ a three-stage training strategy: unconditional pre-training on canonical topologies for structural priors, large-scale conditional pre-training on LLM-generated datasets for task-topology mappings, and supervised fine-tuning on empirically validated graphs. Experiments across six diverse benchmarks show that OFA-TAD significantly outperforms specialized one-for-one models, generating highly adaptive MAS topologies. Code: https://github.com/Shiy-Li/OFA-MAS.

</details>


### [95] [A simulation of urban incidents involving pedestrians and vehicles based on Weighted A*](https://arxiv.org/abs/2601.13452)
*Edgar Gonzalez Fernandez*

Main category: cs.MA

TL;DR: 该论文提出了一种基于多智能体的二维城市环境仿真框架，用于模拟行人与车辆的交互行为并评估碰撞风险与通行效率。


<details>
  <summary>Details</summary>
Motivation: 研究旨在通过建模不同行为策略和环境因素，理解城市交通中行人与车辆的互动机制及其对安全与效率的影响。

Method: 采用加权A*算法驱动行人与车辆智能体，在包含人行道、斑马线、障碍物等要素的网格化城市环境中进行路径规划与行为模拟。

Result: 实验表明障碍密度、交通控制设施及行为偏差显著影响交通安全性和通行效率。

Conclusion: 该框架能有效支持城市交通风险评估与优化策略研究，为智慧城市建设提供仿真工具基础。

Abstract: This document presents a comprehensive simulation framework designed to model urban incidents involving pedestrians and vehicles. Using a multiagent systems approach, two types of agents (pedestrians and vehicles) are introduced within a 2D grid based urban environment. The environment encodes streets, sidewalks, buildings, zebra crossings, and obstacles such as potholes and infrastructure elements. Each agent employs a weighted A* algorithm for pathfinding, allowing for variation in decision making behavior such as reckless movement or strict rule-following. The model aims to simulate interactions, assess risk of collisions, and evaluate efficiency under varying environmental and behavioral conditions. Experimental results explore how factors like obstacle density, presence of traffic control mechanisms, and behavioral deviations affect safety and travel efficiency.

</details>


### [96] [The Orchestration of Multi-Agent Systems: Architectures, Protocols, and Enterprise Adoption](https://arxiv.org/abs/2601.13671)
*Apoorva Adimulam,Rajesh Gupta,Sumit Kumar*

Main category: cs.MA

TL;DR: 本文提出了一种统一的多智能体系统架构，整合规划、策略执行、状态管理与质量操作，并设计两种通信协议以实现可扩展、可审计、合规的分布式协作。


<details>
  <summary>Details</summary>
Motivation: 推动人工智能向结构化协作的多智能体系统演进，解决复杂共享目标下的协调与通信问题。

Method: 构建统一架构框架，设计Model Context Protocol与Agent2Agen协议，结合治理与可观测机制。

Result: 实现了跨分布式智能体的互操作通信基底，保障系统一致性、透明性与问责性。

Conclusion: 本研究为面向企业级AI生态的多智能体系统提供了从概念到落地的完整技术蓝图。

Abstract: Orchestrated multi-agent systems represent the next stage in the evolution of artificial intelligence, where autonomous agents collaborate through structured coordination and communication to achieve complex, shared objectives. This paper consolidates and formalizes the technical composition of such systems, presenting a unified architectural framework that integrates planning, policy enforcement, state management, and quality operations into a coherent orchestration layer. Another primary contribution of this work is the in-depth technical delineation of two complementary communication protocols - the Model Context Protocol, which standardizes how agents access external tools and contextual data, and the Agent2Agent protocol, which governs peer coordination, negotiation, and delegation. Together, these protocols establish an interoperable communication substrate that enables scalable, auditable, and policy-compliant reasoning across distributed agent collectives. Beyond protocol design, the paper details how orchestration logic, governance frameworks, and observability mechanisms collectively sustain system coherence, transparency, and accountability. By synthesizing these elements into a cohesive technical blueprint, this paper provides comprehensive treatments of orchestrated multi-agent systems - bridging conceptual architectures with implementation-ready design principles for enterprise-scale AI ecosystems.

</details>
