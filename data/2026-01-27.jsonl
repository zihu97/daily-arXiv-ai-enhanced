{"id": "2601.17454", "categories": ["cs.MA", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.17454", "abs": "https://arxiv.org/abs/2601.17454", "authors": ["Muhammad Ahmed Atif", "Nehal Naeem Haji", "Mohammad Shahid Shaikh", "Muhammad Ebad Atif"], "title": "Embodiment-Induced Coordination Regimes in Tabular Multi-Agent Q-Learning", "comment": null, "summary": "Centralized value learning is often assumed to improve coordination and stability in multi-agent reinforcement learning, yet this assumption is rarely tested under controlled conditions. We directly evaluate it in a fully tabular predator-prey gridworld by comparing independent and centralized Q-learning under explicit embodiment constraints on agent speed and stamina. Across multiple kinematic regimes and asymmetric agent roles, centralized learning fails to provide a consistent advantage and is frequently outperformed by fully independent learning, even under full observability and exact value estimation. Moreover, asymmetric centralized-independent configurations induce persistent coordination breakdowns rather than transient learning instability. By eliminating confounding effects from function approximation and representation learning, our tabular analysis isolates coordination structure as the primary driver of these effects. The results show that increased coordination can become a liability under embodiment constraints, and that the effectiveness of centralized learning is fundamentally regime and role dependent rather than universal."}
{"id": "2601.18284", "categories": ["cs.MA"], "pdf": "https://arxiv.org/pdf/2601.18284", "abs": "https://arxiv.org/abs/2601.18284", "authors": ["Hsiao-Chuan Chang", "Sheng-You Huang", "Yen-Chi Chen", "I-Chen Wu"], "title": "VissimRL: A Multi-Agent Reinforcement Learning Framework for Traffic Signal Control Based on Vissim", "comment": null, "summary": "Traffic congestion remains a major challenge for urban transportation, leading to significant economic and environmental impacts. Traffic Signal Control (TSC) is one of the key measures to mitigate congestion, and recent studies have increasingly applied Reinforcement Learning (RL) for its adaptive capabilities. With respect to SUMO and CityFlow, the simulator Vissim offers high-fidelity driver behavior modeling and wide industrial adoption but remains underutilized in RL research due to its complex interface and lack of standardized frameworks. To address this gap, this paper proposes VissimRL, a modular RL framework for TSC that encapsulates Vissim's COM interface through a high-level Python API, offering standardized environments for both single- and multi-agent training. Experiments show that VissimRL significantly reduces development effort while maintaining runtime efficiency, and supports consistent improvements in traffic performance during training, as well as emergent coordination in multi-agent control. Overall, VissimRL demonstrates the feasibility of applying RL in high-fidelity simulations and serves as a bridge between academic research and practical applications in intelligent traffic signal control."}
{"id": "2601.17534", "categories": ["cs.NI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2601.17534", "abs": "https://arxiv.org/abs/2601.17534", "authors": ["Mounir Bensalem", "Fin Gentzen", "Tuck-Wai Choong", "Yu-Chiao Jhuang", "Admela Jukan", "Jenq-Shiou Leu"], "title": "Efficient Self-Learning and Model Versioning for AI-native O-RAN Edge", "comment": "This paper is uploaded here for research community, thus it is for non-commercial purposes", "summary": "The AI-native vision of 6G requires Radio Access Networks to train, deploy, and continuously refine thousands of machine learning (ML) models that drive real-time radio network optimization. Although the Open RAN (O-RAN) architecture provides open interfaces and an intelligent control plane, it leaves the life-cycle management of these models unspecified. Consequently, operators still rely on ad-hoc, manual update practices that can neither scale across the heterogeneous, multi-layer stack of Cell-Site, Edge-, Regional-, and Central-Cloud domains, nor across the three O-RAN control loops (real-, near-real-, and non-real-time). We present a self-learning framework that provides an efficient closed-loop version management for an AI-native O-RAN edge. In this framework, training pipelines in the Central/Regional Cloud continuously generate new models, which are cataloged along with their resource footprints, security scores, and accuracy metrics in a shared version repository. An Update Manager consults this repository and applies a self-learning policy to decide when and where each new model version should be promoted into operation. A container orchestrator then realizes these decisions across heterogeneous worker nodes, enabling multiple services (rApps, xApps, and dApps) to obtain improved inference with minimal disruption. Simulation results show that an efficient RL-driven decision-making can guarantee quality of service, bounded latencies while balancing model accuracy, system stability, and resilience."}
{"id": "2601.17292", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2601.17292", "abs": "https://arxiv.org/abs/2601.17292", "authors": ["Zhiyin Zhou"], "title": "Risk-based test framework for LLM features in regulated software", "comment": null, "summary": "Large language models are increasingly embedded in regulated and safety-critical software, including clinical research platforms and healthcare information systems. While these features enable natural language search, summarization, and configuration assistance, they introduce risks such as hallucinations, harmful or out-of-scope advice, privacy and security issues, bias, instability under change, and adversarial misuse. Prior work on machine learning testing and AI assurance offers useful concepts but limited guidance for interactive, product-embedded assistants. This paper proposes a risk-based testing framework for LLM features in regulated software: a six-category risk taxonomy, a layered test strategy mapping risks to concrete tests across guardrail, orchestration, and system layers, and a case study applying the approach to a Knowledgebase assistant in a clinical research platform."}
{"id": "2601.17295", "categories": ["cs.NI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.17295", "abs": "https://arxiv.org/abs/2601.17295", "authors": ["Xinyu Zhu", "Parisa Fard Moshiri", "Poonam Lohan", "Burak Kantarci", "Emil Janulewicz"], "title": "Structure-Aware NL-to-SQL for SFC Provisioning via AST-Masking Empowered Language Models", "comment": "6 pages, 3 figures, accepted to IEEE International Conference on Communications (ICC) 2026", "summary": "Effective Service Function Chain (SFC) provisioning requires precise orchestration in dynamic and latency-sensitive networks. Reinforcement Learning (RL) improves adaptability but often ignores structured domain knowledge, which limits generalization and interpretability. Large Language Models (LLMs) address this gap by translating natural language (NL) specifications into executable Structured Query Language (SQL) commands for specification-driven SFC management. Conventional fine-tuning, however, can cause syntactic inconsistencies and produce inefficient queries. To overcome this, we introduce Abstract Syntax Tree (AST)-Masking, a structure-aware fine-tuning method that uses SQL ASTs to assign weights to key components and enforce syntax-aware learning without adding inference overhead. Experiments show that AST-Masking significantly improves SQL generation accuracy across multiple language models. FLAN-T5 reaches an Execution Accuracy (EA) of 99.6%, while Gemma achieves the largest absolute gain from 7.5% to 72.0%. These results confirm the effectiveness of structure-aware fine-tuning in ensuring syntactically correct and efficient SQL generation for interpretable SFC orchestration."}
{"id": "2601.17551", "categories": ["cs.PF", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.17551", "abs": "https://arxiv.org/abs/2601.17551", "authors": ["Thomas Ziller", "Shashikant Ilager", "Alessandro Tundo", "Ezio Bartocci", "Leonardo Mariani", "Ivona Brandic"], "title": "GreenServ: Energy-Efficient Context-Aware Dynamic Routing for Multi-Model LLM Inference", "comment": "Paper under submisison", "summary": "Large language models (LLMs) demonstrate remarkable capabilities, but their broad deployment is limited by significant computational resource demands, particularly energy consumption during inference. Static, one-model-fits-all inference strategies are often inefficient, as they do not exploit the diverse range of available models or adapt to varying query requirements.\n  This paper presents GreenServ, a dynamic, context-aware routing framework that optimizes the trade-off between inference accuracy and energy efficiency. GreenServ extracts lightweight contextual features from each query, including task type, semantic cluster, and text complexity, and routes queries to the most suitable model from a heterogeneous pool, based on observed accuracy and energy usage. We employ a multi-armed bandit approach to learn adaptive routing policies online. This approach operates under partial feedback, eliminates the need for extensive offline calibration, and streamlines the integration of new models into the inference pipeline.\n  We evaluated GreenServ across five benchmark tasks and a pool of 16 contemporary open-access LLMs. Experimental results show that GreenServ consistently outperforms static (single-model) and random baselines. In particular, compared to random routing, GreenServ achieved a 22% increase in accuracy while reducing cumulative energy consumption by 31%. Finally, we evaluated GreenServ with RouterBench, achieving an average accuracy of 71.7% with a peak accuracy of 75.7%. All artifacts are open-source and available as an anonymous repository for review purposes here: https://anonymous.4open.science/r/llm-inference-router-EBEA/README.md"}
{"id": "2601.17279", "categories": ["cs.AR", "cs.CV", "eess.IV"], "pdf": "https://arxiv.org/pdf/2601.17279", "abs": "https://arxiv.org/abs/2601.17279", "authors": ["Sonu Kumar", "Lavanya Vinnakota", "Mukul Lokhande", "Santosh Kumar Vishvakarma", "Adam Teman"], "title": "SPADE: A SIMD Posit-enabled compute engine for Accelerating DNN Efficiency", "comment": null, "summary": "The growing demand for edge-AI systems requires arithmetic units that balance numerical precision, energy efficiency, and compact hardware while supporting diverse formats. Posit arithmetic offers advantages over floating- and fixed-point representations through its tapered precision, wide dynamic range, and improved numerical robustness. This work presents SPADE, a unified multi-precision SIMD Posit-based multiplyaccumulate (MAC) architecture supporting Posit (8,0), Posit (16,1), and Posit (32,2) within a single framework. Unlike prior single-precision or floating/fixed-point SIMD MACs, SPADE introduces a regime-aware, lane-fused SIMD Posit datapath that hierarchically reuses Posit-specific submodules (LOD, complementor, shifter, and multiplier) across 8/16/32-bit precisions without datapath replication. FPGA implementation on a Xilinx Virtex-7 shows 45.13% LUT and 80% slice reduction for Posit (8,0), and up to 28.44% and 17.47% improvement for Posit (16,1) and Posit (32,2) over prior work, with only 6.9% LUT and 14.9% register overhead for multi-precision support. ASIC results across TSMC nodes achieve 1.38 GHz at 6.1 mW (28 nm). Evaluation on MNIST, CIFAR-10/100, and alphabet datasets confirms competitive inference accuracy."}
{"id": "2601.17136", "categories": ["cs.DC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.17136", "abs": "https://arxiv.org/abs/2601.17136", "authors": ["Julian Bellavita", "Matthew Rubino", "Nakul Iyer", "Andrew Chang", "Aditya Devarakonda", "Flavio Vella", "Giulia Guidi"], "title": "Communication-Avoiding Linear Algebraic Kernel K-Means on GPUs", "comment": null, "summary": "Clustering is an important tool in data analysis, with K-means being popular for its simplicity and versatility. However, it cannot handle non-linearly separable clusters. Kernel K-means addresses this limitation but requires a large kernel matrix, making it computationally and memory intensive. Prior work has accelerated Kernel K-means by formulating it using sparse linear algebra primitives and implementing it on a single GPU. However, that approach cannot run on datasets with more than approximately 80,000 samples due to limited GPU memory.\n  In this work, we address this issue by presenting a suite of distributed-memory parallel algorithms for large-scale Kernel K-means clustering on multi-GPU systems. Our approach maps the most computationally expensive components of Kernel K-means onto communication-efficient distributed linear algebra primitives uniquely tailored for Kernel K-means, enabling highly scalable implementations that efficiently cluster million-scale datasets. Central to our work is the design of partitioning schemes that enable communication-efficient composition of the linear algebra primitives that appear in Kernel K-means.\n  Our 1.5D algorithm consistently achieves the highest performance, enabling Kernel K-means to scale to data one to two orders of magnitude larger than previously practical. On 256 GPUs, it achieves a geometric mean weak scaling efficiency of $79.7\\%$ and a geometric mean strong scaling speedup of $4.2\\times$. Compared to our 1D algorithm, the 1.5D approach achieves up to a $3.6\\times$ speedup on 256 GPUs and reduces clustering time from over an hour to under two seconds relative to a single-GPU sliding window implementation. Our results show that distributed algorithms designed with application-specific linear algebraic formulations can achieve substantial performance improvement."}
{"id": "2601.17390", "categories": ["cs.SE", "cs.CR", "cs.PL"], "pdf": "https://arxiv.org/pdf/2601.17390", "abs": "https://arxiv.org/abs/2601.17390", "authors": ["Yayi Wang", "Shenao Wang", "Jian Zhao", "Shaosen Shi", "Ting Li", "Yan Cheng", "Lizhong Bian", "Kan Yu", "Yanjie Zhao", "Haoyu Wang"], "title": "YASA: Scalable Multi-Language Taint Analysis on the Unified AST at Ant Group", "comment": null, "summary": "Modern enterprises increasingly adopt diverse technology stacks with various programming languages, posing significant challenges for static application security testing (SAST). Existing taint analysis tools are predominantly designed for single languages, requiring substantial engineering effort that scales with language diversity. While multi-language tools like CodeQL, Joern, and WALA attempt to address these challenges, they face limitations in intermediate representation design, analysis precision, and extensibility, which make them difficult to scale effectively for large-scale industrial applications at Ant Group. To bridge this gap, we present YASA (Yet Another Static Analyzer), a unified multi-language static taint analysis framework designed for industrial-scale deployment. Specifically, YASA introduces the Unified Abstract Syntax Tree (UAST) that provides a unified abstraction for compatibility across diverse programming languages. Building on the UAST, YASA performs point-to analysis and taint propagation, leveraging a unified semantic model to manage language-agnostic constructs, while incorporating language-specific semantic models to handle other unique language features. When compared to 6 single- and 2 multi-language static analyzers on an industry-standard benchmark, YASA consistently outperformed all baselines across Java, JavaScript, Python, and Go. In real-world deployment within Ant Group, YASA analyzed over 100 million lines of code across 7.3K internal applications. It identified 314 previously unknown taint paths, with 92 of them confirmed as 0-day vulnerabilities. All vulnerabilities were responsibly reported, with 76 already patched by internal development teams, demonstrating YASA's practical effectiveness for securing large-scale industrial software systems."}
{"id": "2601.17534", "categories": ["cs.NI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2601.17534", "abs": "https://arxiv.org/abs/2601.17534", "authors": ["Mounir Bensalem", "Fin Gentzen", "Tuck-Wai Choong", "Yu-Chiao Jhuang", "Admela Jukan", "Jenq-Shiou Leu"], "title": "Efficient Self-Learning and Model Versioning for AI-native O-RAN Edge", "comment": "This paper is uploaded here for research community, thus it is for non-commercial purposes", "summary": "The AI-native vision of 6G requires Radio Access Networks to train, deploy, and continuously refine thousands of machine learning (ML) models that drive real-time radio network optimization. Although the Open RAN (O-RAN) architecture provides open interfaces and an intelligent control plane, it leaves the life-cycle management of these models unspecified. Consequently, operators still rely on ad-hoc, manual update practices that can neither scale across the heterogeneous, multi-layer stack of Cell-Site, Edge-, Regional-, and Central-Cloud domains, nor across the three O-RAN control loops (real-, near-real-, and non-real-time). We present a self-learning framework that provides an efficient closed-loop version management for an AI-native O-RAN edge. In this framework, training pipelines in the Central/Regional Cloud continuously generate new models, which are cataloged along with their resource footprints, security scores, and accuracy metrics in a shared version repository. An Update Manager consults this repository and applies a self-learning policy to decide when and where each new model version should be promoted into operation. A container orchestrator then realizes these decisions across heterogeneous worker nodes, enabling multiple services (rApps, xApps, and dApps) to obtain improved inference with minimal disruption. Simulation results show that an efficient RL-driven decision-making can guarantee quality of service, bounded latencies while balancing model accuracy, system stability, and resilience."}
{"id": "2601.18400", "categories": ["cs.DC", "cs.NI", "cs.PF"], "pdf": "https://arxiv.org/pdf/2601.18400", "abs": "https://arxiv.org/abs/2601.18400", "authors": ["Andrei Lebedev", "Vincent Gramoli"], "title": "On the Bandwidth Consumption of Blockchains", "comment": "11 pages, 6 figures", "summary": "With the advent of blockchain technology, the number of proposals has boomed. The network traffic imposed by these blockchain proposals increases the cost of hosting nodes. Unfortunately, as of today, we are not aware of any comparative study of the bandwidth consumption of blockchains.\n  In this paper, we propose the first empirical comparison of blockchain bandwidth consumption. To this end, we measure the network traffic of blockchain network nodes of five blockchain protocols: Algorand, Aptos, Avalanche, Redbelly and Solana. We study the variation over time, differentiate the receiving and sending traffic and analyze how this traffic varies with the number of nodes and validators.\n  We conclude that the transport protocol is the main factor impacting the network traffic, segregating node roles helps reduce traffic and different blockchains are differently impacted by the network size."}
{"id": "2601.17615", "categories": ["cs.AR", "cs.DC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.17615", "abs": "https://arxiv.org/abs/2601.17615", "authors": ["Rahul Bera", "Zhenrong Lang", "Caroline Hengartner", "Konstantinos Kanellopoulos", "Rakesh Kumar", "Mohammad Sadrosadati", "Onur Mutlu"], "title": "Athena: Synergizing Data Prefetching and Off-Chip Prediction via Online Reinforcement Learning", "comment": null, "summary": "Prefetching and off-chip prediction are two techniques proposed to hide long memory access latencies in high-performance processors. In this work, we demonstrate that: (1) prefetching and off-chip prediction often provide complementary performance benefits, yet (2) naively combining them often fails to realize their full performance potential, and (3) existing prefetcher control policies leave significant room for performance improvement behind.\n  Our goal is to design a holistic framework that can autonomously learn to coordinate an off-chip predictor with multiple prefetchers employed at various cache levels. To this end, we propose a new technique called Athena, which models the coordination between prefetchers and off-chip predictor (OCP) as a reinforcement learning (RL) problem. Athena acts as the RL agent that observes multiple system-level features (e.g., prefetcher/OCP accuracy, bandwidth usage) over an epoch of program execution, and uses them as state information to select a coordination action (i.e., enabling the prefetcher and/or OCP, and adjusting prefetcher aggressiveness). At the end of every epoch, Athena receives a numerical reward that measures the change in multiple system-level metrics (e.g., number of cycles taken to execute an epoch). Athena uses this reward to autonomously and continuously learn a policy to coordinate prefetchers with OCP.\n  Our extensive evaluation using a diverse set of memory-intensive workloads shows that Athena consistently outperforms prior state-of-the-art coordination policies across a wide range of system configurations with various combinations of underlying prefetchers, OCPs, and main memory bandwidths, while incurring only modest storage overhead. Athena is freely available at https://github.com/CMU-SAFARI/Athena."}
{"id": "2601.17546", "categories": ["cs.DC", "cs.DB"], "pdf": "https://arxiv.org/pdf/2601.17546", "abs": "https://arxiv.org/abs/2601.17546", "authors": ["Ravi Kiran Kodali", "Vinoth Punniyamoorthy", "Akash Kumar Agarwal", "Bikesh Kumar", "Balakrishna Pothineni", "Aswathnarayan Muthukrishnan Kirubakaran", "Sumit Saha", "Nachiappan Chockalingam"], "title": "Push Down Optimization for Distributed Multi Cloud Data Integration", "comment": null, "summary": "Enterprises increasingly adopt multi cloud architectures to take advantage of diverse database engines, regional availability, and cost models. In these environments, ETL pipelines must process large, distributed datasets while minimizing latency and transfer cost. Push down optimization, which executes transformation logic within database engines rather than within the ETL tool, has proven highly effective in single cloud systems. However, when applied across multiple clouds, it faces challenges related to data movement, heterogeneous SQL engines, orchestration complexity, and fragmented security controls. This paper examines the feasibility of push down optimization in multi cloud ETL pipelines and analyzes its benefits and limitations. It evaluates localized push down, hybrid models, and data federation techniques that reduce cross cloud traffic while improving performance. A case study across Redshift and BigQuery demonstrates measurable gains, including lower end to end runtime, reduced transfer volume, and improved cost efficiency. The study highlights practical strategies that organizations can adopt to improve ETL scalability and reliability in distributed cloud environments."}
{"id": "2601.17406", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2601.17406", "abs": "https://arxiv.org/abs/2601.17406", "authors": ["Taher A. Ghaleb"], "title": "Fingerprinting AI Coding Agents on GitHub", "comment": "Accepted at the 23rd International Conference on Mining Software Repositories (MSR '26)", "summary": "AI coding agents are reshaping software development through both autonomous and human-mediated pull requests (PRs). When developers use AI agents to generate code under their own accounts, code authorship attribution becomes critical for repository governance, research validity, and understanding modern development practices. We present the first study on fingerprinting AI coding agents, analyzing 33,580 PRs from five major agents (OpenAI Codex, GitHub Copilot, Devin, Cursor, Claude Code) to identify behavioral signatures. With 41 features spanning commit messages, PR structure, and code characteristics, we achieve 97.2% F1-score in multi-class agent identification. We uncover distinct fingerprints: Codex shows unique multiline commit patterns (67.5% feature importance), and Claude Code exhibits distinctive code structure (27.2% importance of conditional statements). These signatures reveal that AI coding tools produce detectable behavioral patterns, suggesting potential for identifying AI contributions in software repositories."}
{"id": "2601.18069", "categories": ["cs.NI", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.18069", "abs": "https://arxiv.org/abs/2601.18069", "authors": ["Haoyuan Pan", "Sizhao Chen", "Zhaorui Wang", "Tse-Tin Chan"], "title": "Diffusion Model-based Reinforcement Learning for Version Age of Information Scheduling: Average and Tail-Risk-Sensitive Control", "comment": "16 pages, 11 figures", "summary": "Ensuring timely and semantically accurate information delivery is critical in real-time wireless systems. While Age of Information (AoI) quantifies temporal freshness, Version Age of Information (VAoI) captures semantic staleness by accounting for version evolution between transmitters and receivers. Existing VAoI scheduling approaches primarily focus on minimizing average VAoI, overlooking rare but severe staleness events that can compromise reliability under stochastic packet arrivals and unreliable channels. This paper investigates both average-oriented and tail-risk-sensitive VAoI scheduling in a multi-user status update system with long-term transmission cost constraints. We first formulate the average VAoI minimization problem as a constrained Markov decision process and introduce a deep diffusion-based Soft Actor-Critic (D2SAC) algorithm. By generating actions through a diffusion-based denoising process, D2SAC enhances policy expressiveness and establishes a strong baseline for mean performance. Building on this foundation, we put forth RS-D3SAC, a risk-sensitive deep distributional diffusion-based Soft Actor-Critic algorithm. RS-D3SAC integrates a diffusion-based actor with a quantile-based distributional critic, explicitly modeling the full VAoI return distribution. This enables principled tail-risk optimization via Conditional Value-at-Risk (CVaR) while satisfying long-term transmission cost constraints. Extensive simulations show that, while D2SAC reduces average VAoI, RS-D3SAC consistently achieves substantial reductions in CVaR without sacrificing mean performance. The dominant gain in tail-risk reduction stems from the distributional critic, with the diffusion-based actor providing complementary refinement to stabilize and enrich policy decisions, highlighting their effectiveness for robust and risk-aware VAoI scheduling in multi-user wireless systems."}
{"id": "2601.17633", "categories": ["cs.AR", "cs.DC"], "pdf": "https://arxiv.org/pdf/2601.17633", "abs": "https://arxiv.org/abs/2601.17633", "authors": ["Rakesh Nadig", "Vamanan Arulchelvan", "Mayank Kabra", "Harshita Gupta", "Rahul Bera", "Nika Mansouri Ghiasi", "Nanditha Rao", "Qingcai Jiang", "Andreas Kosmas Kakolyris", "Yu Liang", "Mohammad Sadrosadati", "Onur Mutlu"], "title": "Conduit: Programmer-Transparent Near-Data Processing Using Multiple Compute-Capable Resources in Solid State Drives", "comment": "To appear in IEEE International Symposium on High-Performance Computer Architecture (HPCA) 2026", "summary": "Solid-state drives (SSDs) are well suited for near-data processing (NDP) because they: (1) store large application datasets, and (2) support three NDP paradigms: in-storage processing (ISP), processing using DRAM in the SSD (PuD-SSD), and in-flash processing (IFP). A large body of prior SSD-based NDP techniques operate in isolation, mapping computations to only one or two NDP paradigms (i.e., ISP, PuD-SSD, or IFP) within the SSD. These techniques (1) are tailored to specific workloads or kernels, (2) do not exploit the full computational potential of an SSD, and (3) lack programmer-transparency. While several prior works propose techniques to partition computation between the host and near-memory accelerators, adapting these techniques to SSDs has limited benefits because they (1) ignore the heterogeneity of the SSD resources, and (2) make offloading decisions based on limited factors such as bandwidth utilization, or data movement cost. We propose Conduit, a general-purpose, programmer-transparent NDP framework for SSDs that leverages multiple SSD computation resources. At compile time, Conduit executes a custom compiler (e.g., LLVM) pass that (i) vectorizes suitable application code segments into SIMD operations that align with the SSD's page layout, and (ii) embeds metadata (e.g., operation type, operand sizes) into the vectorized instructions to guide runtime offloading decisions. At runtime, within the SSD, Conduit performs instruction-granularity offloading by evaluating six key features, and uses a cost function to select the most suitable SSD resource. We evaluate Conduit and two prior NDP offloading techniques using an in-house event-driven SSD simulator on six data-intensive workloads. Conduit outperforms the best-performing prior offloading policy by 1.8x and reduces energy consumption by 46%."}
{"id": "2601.17578", "categories": ["cs.DC", "stat.CO"], "pdf": "https://arxiv.org/pdf/2601.17578", "abs": "https://arxiv.org/abs/2601.17578", "authors": ["Henrik Bengtsson"], "title": "A Unified Approach to Concurrent, Parallel Map-Reduce in R using Futures", "comment": "16 pages including 2.5 pages references, 1 figure", "summary": "The R ecosystem offers a rich variety of map-reduce application programming interfaces (APIs) for iterative computations, yet parallelizing code across these diverse frameworks requires learning multiple, often incompatible, parallel APIs. The futurize package addresses this challenge by providing a single function, futurize(), which transpiles sequential map-reduce expressions into their parallel equivalents in the future ecosystem, which performs all the heavy lifting. By leveraging R's native pipe operator, users can parallelize existing code with minimal refactoring -- often by simply appending `|> futurize()' to an expression. The package supports classical map-reduce functions from base R, purrr, crossmap, foreach, plyr, BiocParallel, e.g., lapply(xs, fcn) |> futurize() and map(xs, fcn) |> futurize(), as well as a growing set of domain-specific packages, e.g., boot, caret, glmnet, lme4, mgcv, and tm. By abstracting away the underlying parallel machinery, and unifying handling of future options, the package enables developers to declare what to parallelize via futurize(), and end-users to choose how via plan(). This article describes the philosophy, design, and implementation of futurize, demonstrates its usage across various map-reduce paradigms, and discusses its role in simplifying parallel computing in R."}
{"id": "2601.17413", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2601.17413", "abs": "https://arxiv.org/abs/2601.17413", "authors": ["Taher A. Ghaleb"], "title": "When AI Agents Touch CI/CD Configurations: Frequency and Success", "comment": "Accepted at the 23rd International Conference on Mining Software Repositories (MSR '26)", "summary": "AI agents are increasingly used in software development, yet their interaction with CI/CD configurations is not well studied. We analyze 8,031 agentic pull requests (PRs) from 1,605 GitHub repositories where AI agents touch YAML configurations. CI/CD configuration files account for 3.25% of agent changes, varying by agent (Devin: 4.83%, Codex: 2.01%, p < 0.001). When agents modify CI/CD, 96.77% target GitHub Actions. Agentic PRs with CI/CD changes merge slightly less often than others (67.77% vs. 71.80%), except for Copilot, whose CI/CD changes merge 15.63 percentage points more often. Across 99,930 workflow runs, build success rates are comparable for CI/CD and non-CI/CD changes (75.59% vs. 74.87%), though three agents show significantly higher success when modifying CI/CD. These results show that AI agents rarely modify CI/CD and focus mostly on GitHub Actions, yet their configuration changes are as reliable as regular code. Copilot's strong CI/CD performance despite lower acceptance suggests emerging configuration specialization, with implications for agent training and DevOps automation."}
{"id": "2601.18134", "categories": ["cs.NI"], "pdf": "https://arxiv.org/pdf/2601.18134", "abs": "https://arxiv.org/abs/2601.18134", "authors": ["Anshika Singh", "Siddhartha S. Borkotoky"], "title": "Accelerating Update Broadcasts Over LoRaWAN Downlink via D2D Cooperation", "comment": null, "summary": "Broadcast distribution of updates (e.g., security patches, machine learning models) from a server to end devices (EDs) is a critical requirement in the Internet of Things (IoT). In this paper, we consider the problem of reliable over-the-air broadcast of updates in Long Range Wide Area Networks (LoRaWANs). Existing broadcast techniques for LoRaWANs suffer from long delivery delays due to low data rates and duty-cycle constraints. We address this problem by proposing a device-level cooperative mechanism, in which updated EDs broadcast a few update fragments to accelerate delivery to their neighbors. We demonstrate large reductions in the delivery time compared to conventional methods. For instance, in a 400-node network spanning 1 km radius and operating at 1% duty-cycle, the proposed scheme reduces the time required to deliver a 10 kilobyte update to an ED at the network's edge from 42 hours to 45 minutes. The proposed solution thus provides a pathway toward improved security and efficient realization of edge intelligence in LoRaWAN IoT."}
{"id": "2601.17940", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2601.17940", "abs": "https://arxiv.org/abs/2601.17940", "authors": ["Luca Colagrande", "Luca Benini"], "title": "Late Breaking Results: Boosting Efficient Dual-Issue Execution on Lightweight RISC-V Cores", "comment": "Accepted at DATE 2026", "summary": "Large-scale ML accelerators rely on large numbers of PEs, imposing strict bounds on the area and energy budget of each PE. Prior work demonstrates that limited dual-issue capabilities can be efficiently integrated into a lightweight in-order open-source RISC-V core (Snitch), with a geomean IPC boost of 1.6x and a geomean energy efficiency gain of 1.3x, obtained by concurrently executing integer and FP instructions. Unfortunately, this required a complex and error-prone low level programming model (COPIFT). We introduce COPIFTv2 which augments Snitch with lightweight queues enabling direct, fine-grained communication and synchronization between integer and FP threads. By eliminating the tiling and software pipelining steps of COPIFT, we can remove much of its complexity and software overheads. As a result, COPIFTv2 achieves up to a 1.49x speedup and a 1.47x energy-efficiency gain over COPIFT, and a peak IPC of 1.81. Overall, COPIFTv2 significantly enhances the efficiency and programmability of dual-issue execution on lightweight cores. Our implementation is fully open source and performance experiments are reproducible using free software."}
{"id": "2601.17589", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2601.17589", "abs": "https://arxiv.org/abs/2601.17589", "authors": ["Thomas Sandholm", "Bernardo A. Huberman", "Klas Segeljakt", "Paris Carbone"], "title": "Lightspeed Data Compute for the Space Era", "comment": null, "summary": "While thousands of satellites photograph Earth every day, most of that data never makes it to the ground because downlink bandwidth simply cannot keep up. Processing data in the Low Earth Orbit (LEO) zone offers promising capabilities to overcome this limitation. We propose SpaceCoMP, a MapReduce-inspired processing model for LEO satellite mesh networks. Ground stations submit queries over an area of interest; satellites collect sensor data, process it cooperatively at light-speed using inter-satellite laser links, and return only the results. Our compute model leverages space physics to accelerate computations on LEO megaconstellations. Our distance-aware routing protocol exploits orbital geometry. In addition, our bipartite match scheduling strategy places map and reduce tasks within orbital regions while minimizing aggregation costs. We have simulated constellations of 1,000-10,000 satellites showcasing 61-79% improvement in map placement efficiency over baselines, 18-28% over greedy allocation, and 67-72% reduction in aggregation cost. SpaceCoMP demonstrates that the orbital mesh is not merely useful as a communication relay, as seen today, but can provide the foundations for faster data processing above the skies."}
{"id": "2601.17435", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.17435", "abs": "https://arxiv.org/abs/2601.17435", "authors": ["Maria Jesus Rodriguez-Sanchez", "Manuel Noguera", "Angel Ruiz-Zafra", "Kawtar Benghazi"], "title": "Towards a Declarative Agentic Layer for Intelligent Agents in MCP-Based Server Ecosystems", "comment": "12 pages, 3 figures", "summary": "Recent advances in Large Language Models (LLMs) have enabled the development of increasingly complex agentic and multi-agent systems capable of planning, tool use and task decomposition. However, empirical evidence shows that many of these systems suffer from fundamental reliability issues, including hallucinated actions, unexecutable plans and brittle coordination. Crucially, these failures do not stem from limitations of the underlying models themselves, but from the absence of explicit architectural structure linking goals, capabilities and execution. This paper presents a declarative, model-independent architectural layer for grounded agentic workflows that addresses this gap. The proposed layer, referred to as DALIA (Declarative Agentic Layer for Intelligent Agents), formalises executable capabilities, exposes tasks through a declarative discovery protocol, maintains a federated directory of agents and their execution resources, and constructs deterministic task graphs grounded exclusively in declared operations. By enforcing a clear separation between discovery, planning and execution, the architecture constrains agent behaviour to a verifiable operational space, reducing reliance on speculative reasoning and free-form coordination. We present the architecture and design principles of the proposed layer and illustrate its operation through a representative task-oriented scenario, demonstrating how declarative grounding enables reproducible and verifiable agentic workflows across heterogeneous environments."}
{"id": "2601.18148", "categories": ["cs.NI"], "pdf": "https://arxiv.org/pdf/2601.18148", "abs": "https://arxiv.org/abs/2601.18148", "authors": ["Jason Gerard", "Juan A. Fraire", "Sandra Cespedes"], "title": "Contact Plan Design For Optical Interplanetary Communications", "comment": null, "summary": "Space exploration missions generate rapidly increasing volumes of scientific telemetry that far exceed the capacity of today's manually scheduled, RF-based deep-space infrastructure. Free-space optical (FSO) communications promise orders of magnitude higher throughput, but their narrow beams require precise pointing, acquisition, and tracking (PAT) for link establishment and tightly synchronized contact schedules. Critically, no existing contact plan design (CPD) framework accounts for optical head retargeting delay, the time spent during coarse pointing and link acquisition before data transmission begins, which directly reduces usable contact time. Retargeting delay is the dominant impairment unique to optical networks, which induces a seconds-to-minutes-long mechanical pointing process for an optical terminal's laser from its current partner to the next receiver. This paper introduces the first PAT-aware CPD framework for optical interplanetary backhaul networks. The model captures directional temporal flows across both direct-to-Earth optical links and two-hop relay paths using delay/disruption-tolerant networking (DTN) satellites. We also introduce an optical network duty-cycle metric that quantifies the proportion of time spent transmitting to the contact window duration, exposing capacity lost to retargeting delay. Our results show that our MILP scheduler delivers over 30 percent higher network capacity than a greedy algorithm. More importantly, the results uncover a fundamental behavioral shift: when retargeting delays are modeled accurately, optimal schedules favor fewer but longer optical links that maximize throughput while minimizing retargeting overhead. These findings demonstrate that zero-delay assumptions substantially overestimate achievable performance and yield unrealistic contact plans."}
{"id": "2601.18007", "categories": ["cs.AR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.18007", "abs": "https://arxiv.org/abs/2601.18007", "authors": ["Duckgyu Shin", "Naoya Onizawa", "Warren J. Gross", "Takahiro Hanyu"], "title": "Memory-Efficient FPGA Implementation of Stochastic Simulated Annealing", "comment": "11 pages", "summary": "Simulated annealing (SA) is a well-known algorithm for solving combinatorial optimization problems. However, the computation time of SA increases rapidly, as the size of the problem grows. Recently, a stochastic simulated annealing (SSA) algorithm that converges faster than conventional SA has been reported. In this paper, we present a hardware-aware SSA (HA- SSA) algorithm for memory-efficient FPGA implementations. HA-SSA can reduce the memory usage of storing intermediate results while maintaining the computing speed of SSA. For evaluation purposes, the proposed algorithm is compared with the conventional SSA and SA approaches on maximum cut combinatorial optimization problems. HA-SSA achieves a convergence speed that is up to 114-times faster than that of the conventional SA algorithm depending on the maximum cut problem selected from the G-set which is a dataset of the maximum cut problems. HA-SSA is implemented on a field-programmable gate array (FPGA) (Xilinx Kintex-7), and it achieves up to 6-times the memory efficiency of conventional SSA while maintaining high solution quality for optimization problems."}
{"id": "2601.17606", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2601.17606", "abs": "https://arxiv.org/abs/2601.17606", "authors": ["Shannon Kinkead", "Jackson Wesley", "Whit Schonbein", "David DeBonis", "Matthew G. F. Dosanjh", "Amanda Bienz"], "title": "Scaling All-to-all Operations Across Emerging Many-Core Supercomputers", "comment": null, "summary": "Performant all-to-all collective operations in MPI are critical to fast Fourier transforms, transposition, and machine learning applications. There are many existing implementations for all-to-all exchanges on emerging systems, with the achieved performance dependent on many factors, including message size, process count, architecture, and parallel system partition. This paper presents novel all-to-all algorithms for emerging many-core systems. Further, the paper presents a performance analysis against existing algorithms and system MPI, with novel algorithms achieving up to 3x speedup over system MPI at 32 nodes of state-of-the-art Sapphire Rapids systems."}
{"id": "2601.17450", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2601.17450", "abs": "https://arxiv.org/abs/2601.17450", "authors": ["Qingchao Shen"], "title": "Data-driven Test Generation for Fuzzing AI Compiler", "comment": "This paper has been accepted by ICSE 2026 Doctoral Symposium track", "summary": "Artificial Intelligence (AI) compilers are critical for efficiently deploying AI models across diverse hardware platforms. However, they remain prone to bugs that can compromise both compiler reliability and model correctness. Thus, ensuring the quality of AI compilers is crucial. In this work, we present a unified data-driven testing framework that systematically addresses stage-specific challenges in AI compilers. Specifically, OPERA migrates tests for AI libraries to test various operator conversion logic in the model loading stage. OATest synthesizes diverse optimization-aware computational graphs for testing high-level optimizations. HARMONY generates and mutates diverse low-level IR seeds to generate hardware-optimization-aware tests for testing low-level optimizations. Together, these techniques provide a comprehensive, stage-aware framework that enhances testing coverage and effectiveness, detecting 266 previously unknown bugs in four widely used AI compilers."}
{"id": "2601.18256", "categories": ["cs.NI"], "pdf": "https://arxiv.org/pdf/2601.18256", "abs": "https://arxiv.org/abs/2601.18256", "authors": ["Akihito Taya", "Yuuki Nishiyama", "Kaoru Sezaki"], "title": "A Mechanical Wi-Fi Antenna Device for Automatic Orientation Tuning with Bayesian Optimization", "comment": "(c) 2026 IEEE. Personal use of this material is permitted. Permission from IEEE must be obtained for all other uses, in any current or future media, including reprinting/republishing this material for advertising or promotional purposes, creating new collective works, for resale or redistribution to servers or lists, or reuse of any copyrighted component of this work in other works", "summary": "Wi-Fi access points have been widely deployed in homes, offices, and public spaces. Some APs allow users to adjust the antenna orientation to improve communication performance by optimizing antenna polarization. However, it is difficult for non-expert users to determine the optimal orientation, and users often leave the antenna orientation in ineffective positions. To address this issue, we developed a mechanical Wi-Fi antenna device capable of automatically tuning its orientation. Experimental results show that antenna orientation could cause a throughput variation of approximately 70 Mbps under line-of-sight conditions. Furthermore, Bayesian optimization identified better configurations than random search, demonstrating its effectiveness for orientation tuning."}
{"id": "2601.18070", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2601.18070", "abs": "https://arxiv.org/abs/2601.18070", "authors": ["Jinwu Chen", "Yuhui Shi", "He Wang", "Zhe Jiang", "Jun Yang", "Xin Si", "Zhenhua Zhu"], "title": "CIM-Tuner: Balancing the Compute and Storage Capacity of SRAM-CIM Accelerator via Hardware-mapping Co-exploration", "comment": null, "summary": "As an emerging type of AI computing accelerator, SRAM Computing-In-Memory (CIM) accelerators feature high energy efficiency and throughput. However, various CIM designs and under-explored mapping strategies impede the full exploration of compute and storage balancing in SRAM-CIM accelerator, potentially leading to significant performance degradation. To address this issue, we propose CIM-Tuner, an automatic tool for hardware balancing and optimal mapping strategy under area constraint via hardware-mapping co-exploration. It ensures universality across various CIM designs through a matrix abstraction of CIM macros and a generalized accelerator template. For efficient mapping with different hardware configurations, it employs fine-grained two-level strategies comprising accelerator-level scheduling and macro-level tiling. Compared to prior CIM mapping, CIM-Tuner's extended strategy space achieves 1.58$\\times$ higher energy efficiency and 2.11$\\times$ higher throughput. Applied to SOTA CIM accelerators with identical area budget, CIM-Tuner also delivers comparable improvements. The simulation accuracy is silicon-verified and CIM-Tuner tool is open-sourced at https://github.com/champloo2878/CIM-Tuner.git."}
{"id": "2601.17707", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2601.17707", "abs": "https://arxiv.org/abs/2601.17707", "authors": ["Mekala Kiran", "Apurba Das", "Suman Banerjee", "Tathagata Ray"], "title": "Multi-core & GPU-based Balanced Butterfly Counting in Signed Bipartite Graphs", "comment": null, "summary": "Balanced butterfly counting, corresponding to counting balanced (2, 2)-bicliques, is a fundamental primitive in the analysis of signed bipartite graphs and provides a basis for studying higher-order structural properties such as clustering coefficients and community structure. Although prior work has proposed an efficient CPU-based serial method for counting balanced (2, k)-bicliques. The computational cost of balanced butterfly counting remains a major bottleneck on large-scale graphs. In this work, we present the highly parallel implementations for balanced butterfly counting for both multicore CPUs and GPUs. The proposed multi-core algorithm (M-BBC) employs fine-grained vertex-level parallelism to accelerate wedge-based counting while eliminating the generation of unbalanced substructures. To improve scalability, we develop a GPU-based method (G-BBC) that uses a tile-based parallel approach to effectively leverage shared memory while handling large vertex sets. We then present an improved variation, G-BBC++, which integrates dynamic scheduling to mitigate workload imbalance and maximize throughput. We conduct an experimental assessment of the proposed methods across 15 real-world datasets. Experimental results exhibit that M-BBC achieves speedups of up to 71.13x (average 38.13x) over the sequential baseline BB2K. The GPU-based algorithms deliver even greater improvements, achieving up to 13,320x speedup (average 2,600x) over BB2K and outperforming M-BBC by up to 186x (average 50x). These results indicate the substantial scalability and efficiency of our parallel algorithms and establish a robust foundation for high-performance signed motif analysis on massive bipartite graphs."}
{"id": "2601.17482", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2601.17482", "abs": "https://arxiv.org/abs/2601.17482", "authors": ["Yang Liu", "Kaiming Zhang", "Zhuangbin Chen", "Jinyang Liu", "Zibin Zheng"], "title": "LogPrism: Unifying Structure and Variable Encoding for Effective Log Compression", "comment": null, "summary": "The prevailing \"parse-then-compress\" paradigm in log compression fundamentally limits effectiveness by treating log parsing and compression as isolated objectives. While parsers prioritize semantic accuracy (i.e., event identification), they often obscure deep correlations between static templates and dynamic variables that are critical for storage efficiency. In this paper, we investigate this misalignment through a comprehensive empirical study and propose LogPrism, a framework that bridges the gap via unified redundancy encoding. Rather than relying on a rigid pre-parsing step, LogPrism dynamically integrates structural extraction with variable encoding by constructing a Unified Redundancy Tree (URT). This hierarchical approach effectively mines \"structure+variable\" co-occurrence patterns, capturing deep contextual redundancies while accelerating processing through pre-emptive pattern encoding. Extensive experiments on 16 benchmark datasets confirm that LogPrism establishes a new state-of-the-art. It achieves the highest compression ratio on 13 datasets, surpassing leading baselines by margins of 4.7% to 80.9%, while delivering superior throughput at 29.87 MB/s (1.68$\\times$~43.04$\\times$ faster than competitors). Moreover, when configured in single-archive mode to maximize global pattern discovery, LogPrism outperforms the best baseline by 19.39% in compression ratio while maintaining a 2.62$\\times$ speed advantage."}
{"id": "2601.18315", "categories": ["cs.NI"], "pdf": "https://arxiv.org/pdf/2601.18315", "abs": "https://arxiv.org/abs/2601.18315", "authors": ["Zhaozhi Liu", "Jiaxin Chen", "Yuanai Xie", "Yuna Jiang", "Minrui Xu", "Xiao Zhang", "Pan Lai", "Zan Zhou"], "title": "CovertComBench: The First Domain-Specific Testbed for LLMs in Wireless Covert Communication", "comment": "6pages", "summary": "The integration of Large Language Models (LLMs) into wireless networks presents significant potential for automating system design. However, unlike conventional throughput maximization, Covert Communication (CC) requires optimizing transmission utility under strict detection-theoretic constraints, such as Kullback-Leibler divergence limits. Existing benchmarks primarily focus on general reasoning or standard communication tasks and do not adequately evaluate the ability of LLMs to satisfy these rigorous security constraints. To address this limitation, we introduce CovertComBench, a unified benchmark designed to assess LLM capabilities across the CC pipeline, encompassing conceptual understanding (MCQs), optimization derivation (ODQs), and code generation (CGQs). Furthermore, we analyze the reliability of automated scoring within a detection-theoretic ``LLM-as-Judge'' framework. Extensive evaluations across state-of-the-art models reveal a significant performance discrepancy. While LLMs achieve high accuracy in conceptual identification (81%) and code implementation (83%), their performance in the higher-order mathematical derivations necessary for security guarantees ranges between 18% and 55%. This limitation indicates that current LLMs serve better as implementation assistants rather than autonomous solvers for security-constrained optimization. These findings suggest that future research should focus on external tool augmentation to build trustworthy wireless AI systems."}
{"id": "2601.18140", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2601.18140", "abs": "https://arxiv.org/abs/2601.18140", "authors": ["Yan Zhu", "Boru Chen", "Christopher W. Fletcher", "Nandeeka Nayak"], "title": "RTeAAL Sim: Using Tensor Algebra to Represent and Accelerate RTL Simulation (Extended Version)", "comment": null, "summary": "RTL simulation on CPUs remains a persistent bottleneck in hardware design. State-of-the-art simulators embed the circuit directly into the simulation binary, resulting in long compilation times and execution that is fundamentally CPU frontend-bound, with severe instruction-cache pressure.\n  This work proposes RTeAAL Sim, which reformulates RTL simulation as a sparse tensor algebra problem. By representing RTL circuits as tensors and simulation as a sparse tensor algebra kernel, RTeAAL Sim decouples simulation behavior from binary size and makes RTL simulation amenable to well-studied tensor algebra optimizations. We demonstrate that a prototype of our tensor-based simulator, even with a subset of these optimizations, already mitigates the compilation overhead and frontend pressure and achieves performance competitive with the highly optimized Verilator simulator across multiple CPUs and ISAs."}
{"id": "2601.17754", "categories": ["cs.DC", "cs.ET", "cs.PL"], "pdf": "https://arxiv.org/pdf/2601.17754", "abs": "https://arxiv.org/abs/2601.17754", "authors": ["Nicolai Stawinoga", "David Katz", "Anton Lydike", "Justs Zarins", "Nick Brown", "George Bisbas", "Tobias Grosser"], "title": "An MLIR Lowering Pipeline for Stencils at Wafer-Scale", "comment": "Paper in ACM International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS '26)", "summary": "The Cerebras Wafer-Scale Engine (WSE) delivers performance at an unprecedented scale of over 900,000 compute units, all connected via a single-wafer on-chip interconnect. Initially designed for AI, the WSE architecture is also well-suited for High Performance Computing (HPC). However, its distributed asynchronous programming model diverges significantly from the simple sequential or bulk-synchronous programs that one would typically derive for a given mathematical program description. Targeting the WSE requires a bespoke re-implementation when porting existing code. The absence of WSE support in compilers such as MLIR, meant that there was little hope for automating this process.\n  Stencils are ubiquitous in HPC, and in this paper we explore the hypothesis that domain specific information about stencils can be leveraged by the compiler to automatically target the WSE without requiring application-level code changes. We present a compiler pipeline that transforms stencil-based kernels into highly optimized CSL code for the WSE, bridging the semantic gap between the mathematical representation of the problem and the WSE's asynchronous execution model. Based upon five benchmarks across three HPC programming technologies, running on both the Cerebras WSE2 and WSE3, our approach delivers comparable, if not slightly better, performance than manually optimized code. Furthermore, without requiring any application level code changes, performance on the WSE3 is around 14 times faster than 128 Nvidia A100 GPUs and 20 times faster than 128 nodes of a CPU-based Cray-EX supercomputer when using our approach."}
{"id": "2601.17558", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2601.17558", "abs": "https://arxiv.org/abs/2601.17558", "authors": ["J. P. Fleischer", "Tanchanok Sirikanchittavon", "Chonlachart Jeenprasom", "Nooshin Yousefzadeh", "Sanjay Ranka", "Mohammed Hadi"], "title": "Measuring Braking Behavior Using Vehicle Tracking and Camera-to-Satellite Homography Rectification", "comment": "VEHITS 2026", "summary": "This paper presents an open-source software application for analyzing traffic camera footage, focusing on vehicle behavior and braking events at signalized urban highways. The core innovation is a robust ground-plane homography estimation that links fixed traffic camera views to satellite orthoimagery. This process rectifies the camera's oblique perspective, ensuring that pixel distances accurately represent real-world distances. This enables the acquisition of features such as vehicle trajectory, speed, deceleration, and braking severity without the need for camera calibration. The pipeline employs the MAGSAC++ estimator to build the homography, converting YOLO11 object detections into a rectified top-down coordinate system. All detection and trajectory data are stored in a ClickHouse database for subsequent analysis. A real-world case study at two signalized intersections in Key West, Florida, showcased the system's capabilities. Across two days of daytime footage, braking activity at the higher-volume intersection peaked around 4 PM at approximately 57.5 events per hour, while the second intersection peaked around 10 AM at roughly 15.5 events per hour. The spatial analysis revealed that most braking events initiated upstream, with mild and moderate braking mostly occurring 30 to 45+ meters away from the stop bar and severe braking distributed throughout, but particularly concentrated in lanes with higher interaction and merging activity. The findings highlight the significant potential of this centralized safety information system to support connected vehicles, facilitating proactive traffic management, crash mitigation, and data-driven roadway design and safety analysis."}
{"id": "2601.18361", "categories": ["cs.NI"], "pdf": "https://arxiv.org/pdf/2601.18361", "abs": "https://arxiv.org/abs/2601.18361", "authors": ["Jean Michel de Souza Sant'Ana", "Felipe Augusto Tondo", "Nurul Huda Mahmood", "Aamir Mahmood"], "title": "Integrating HAPS, LEO, and Terrestrial Networks: A Cost-Performance Study for IoT Connectivity", "comment": "10 pages, 6 figures. Submitted to the IEEE Transactions on Aerospace and Electronic Systems", "summary": "This work evaluates the potential of High-Altitude Platform Stations (HAPS) and Low Earth Orbit (LEO) satellites as alternative or complementary systems to enhance Internet of Things (IoT) connectivity. We first analyze the transmission erasure probability under different connectivity configurations, including only HAPS or LEO satellites, as well as hybrid architectures that integrate both aerial/spatial and terrestrial infrastructures. To make the analysis more realistic, we considered movement of LEO satellites regarding a fixed region, elevation angle between gateway and devices, and different fading models for terrestrial and non-terrestrial communication. We also analyze LR-FHSS (Long-Range Frequency Hopping Spread Spectrum) random access uplink technology as a potential use case for IoT connectivity, showing the scalability impact of the scenarios. The simulation results demonstrate that HAPS can effectively complement sparse terrestrial networks and improve the performance of satellite-based systems in specific scenarios. Furthermore, considering the deployment and operational costs, respectively, CAPEX and OPEX, the economic analysis reveals that although HAPS exhibits higher costs, these remain within a comparable order of magnitude to LEO and terrestrial deployments. In addition, specific use cases, such as natural disasters, transform HAPS into a competitive technology for conventional infrastructures."}
{"id": "2601.18159", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2601.18159", "abs": "https://arxiv.org/abs/2601.18159", "authors": ["Zizhen Liu", "Fangzhiyi Wang", "Mengdi Wang", "Jing Ye", "Hayden Kwok-Hay So", "Cheng Liu", "Huawei Li"], "title": "Lifecycle Cost-Effectiveness Modeling for Redundancy-Enhanced Multi-Chiplet Architectures", "comment": null, "summary": "The growing demand for compute-intensive applications has made multi-chiplet architectures a promising alternative to monolithic designs, offering improved scalability and manufacturing flexibility. However, effectively managing the economic effectiveness remains challenging. Existing cost models either overlook the amortization of compute value over a chip's operational lifetime or fail to evaluate how redundancy strategies, which are widely adopted to enhance yield and fault tolerance, impact long-term cost efficiency. This paper presents a comprehensive cost-effectiveness framework for multi-chiplet architectures, introducing a novel Lifecycle Cost Effectiveness (LCE) metric that evaluates amortized compute costs by jointly optimizing manufacturing expenses and operational lifetime. Our approach uniquely integrates: (1) redundancy-aware cost modeling spanning both intra- and inter-chiplet levels, (2) reliability-driven lifetime estimation, and (3) quantitative analysis of how redundancy configurations on overall economic effectiveness. Extensive trade-off and multi-objective optimization studies demonstrate the effectiveness of the model and reveal essential co-optimization strategies between module and chiplet-level redundancy to achieve cost-efficient multi-chiplet architecture designs."}
{"id": "2601.17774", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2601.17774", "abs": "https://arxiv.org/abs/2601.17774", "authors": ["Zizhao Zhang", "Yihan Xue", "Haotian Zhu", "Sijia Li", "Zhijun Wang", "Yujie Xiao"], "title": "CondenseGraph: Communication-Efficient Distributed GNN Training via On-the-Fly Graph Condensation", "comment": null, "summary": "Distributed Graph Neural Network (GNN) training suffers from substantial communication overhead due to the inherent neighborhood dependency in graph-structured data. This neighbor explosion problem requires workers to frequently exchange boundary node features across partitions, creating a communication bottleneck that severely limits training scalability. Existing approaches rely on static graph partitioning strategies that cannot adapt to dynamic network conditions. In this paper, we propose CondenseGraph, a novel communication-efficient framework for distributed GNN training. Our key innovation is an on-the-fly graph condensation mechanism that dynamically compresses boundary node features into compact super nodes before transmission. To compensate for the information loss introduced by compression, we develop a gradient-based error feedback mechanism that maintains convergence guarantees while reducing communication volume by 40-60%. Extensive experiments on four benchmark datasets demonstrate that CondenseGraph achieves comparable accuracy to full-precision baselines while significantly reducing communication costs and training time."}
{"id": "2601.17581", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.17581", "abs": "https://arxiv.org/abs/2601.17581", "authors": ["Daniel Ogenrwot", "John Businge"], "title": "How AI Coding Agents Modify Code: A Large-Scale Study of GitHub Pull Requests", "comment": "5 pages, 5 figures", "summary": "AI coding agents are increasingly acting as autonomous contributors by generating and submitting pull requests (PRs). However, we lack empirical evidence on how these agent-generated PRs differ from human contributions, particularly in how they modify code and describe their changes. Understanding these differences is essential for assessing their reliability and impact on development workflows. Using the MSR 2026 Mining Challenge version of the AIDev dataset, we analyze 24,014 merged Agentic PRs (440,295 commits) and 5,081 merged Human PRs (23,242 commits). We examine additions, deletions, commits, and files touched, and evaluate the consistency between PR descriptions and their diffs using lexical and semantic similarity. Agentic PRs differ substantially from Human PRs in commit count (Cliff's $= 0.5429$) and show moderate differences in files touched and deleted lines. They also exhibit slightly higher description-to-diff similarity across all measures. These findings provide a large-scale empirical characterization of how AI coding agents contribute to open source development."}
{"id": "2601.18563", "categories": ["cs.NI"], "pdf": "https://arxiv.org/pdf/2601.18563", "abs": "https://arxiv.org/abs/2601.18563", "authors": ["Fang Liu", "Erchao Zhu", "Jiedan Tan", "Jingwen Tong", "Taotao Wang", "Shengli Zhang"], "title": "An LLM-Agent-Based Framework for Age of Information Optimization in Heterogeneous Random Access Networks", "comment": null, "summary": "With the rapid expansion of the Internet of Things (IoT) and heterogeneous wireless networks, the Age of Information (AoI) has emerged as a critical metric for evaluating the performance of real-time and personalized systems. While AoI-based random access is essential for next-generation applications such as the low-altitude economy and indoor service robots, existing strategies, ranging from rule-based protocols to learning-based methods, face critical challenges, including idealized model assumptions, slow convergence, and poor generalization. In this article, we propose Reflex-Core, a novel Large Language Model (LLM) agent-based framework for AoI-driven random access in heterogeneous networks. By devising an \"Observe-Reflect-Decide-Execute\" closed-loop mechanism, this framework integrates Supervised Fine-Tuning (SFT) and Proximal Policy Optimization (PPO) to enable optimal, autonomous access control. Based on the Reflex-Core framework, we develop a Reflexive Multiple Access (RMA) protocol and a priority-based RMA variant for intelligent access control under different heterogeneous network settings. Experimental results demonstrate that in the investigated scenarios, the RMA protocol achieves up to a 14.9% reduction in average AoI compared with existing baselines, while the priority-based version improves the convergence rate by approximately 20%."}
{"id": "2601.17855", "categories": ["cs.DC", "stat.ML"], "pdf": "https://arxiv.org/pdf/2601.17855", "abs": "https://arxiv.org/abs/2601.17855", "authors": ["Zixi Chen", "Tianci Bu", "Chendong Song", "Xin Lu", "Yinyu Ye", "Zijie Zhou"], "title": "A Universal Load Balancing Principle and Its Application to Large Language Model Serving", "comment": null, "summary": "Load balancing-the allocation of work across parallel resources to reduce delay, energy and cost-is a pervasive challenge in science and engineering, from large-scale simulation and data processing to cloud and manufacturing operations. Motivated by the emerging bottleneck in large language model (LLM) serving, we study a particularly stringent regime of load balancing that arises in barrier-synchronized, stateful systems: work cannot be freely migrated and progress is gated by the slowest participant at each step, so heterogeneity and temporal drift in workloads create persistent stragglers and substantial idle time. LLM serving under data-parallel decoding provides a prominent modern instance: in production traces, barrier-induced idle can exceed 40% of compute time per decode step. Here we develop a universal load-balancing principle, which admits a step-wise finite-horizon integer-optimization formulation and yields worst-case guarantees: across LLM decode models and a broader class of non-decreasing workload drift processes, it reduces long-run imbalance by a factor that grows with batch size and system scale. Extensive experiments corroborate the theory, showing substantial improvements in throughput and latency together with reductions in energy consumption. These results provide a general, theoretically grounded framework for load balancing, with immediate implications for sustainable LLM serving and broad relevance to other synchronization-gated resource-allocation problems."}
{"id": "2601.17584", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.17584", "abs": "https://arxiv.org/abs/2601.17584", "authors": ["Mahmoud Samir Fayed", "Ahmed Samir Fayed"], "title": "Prompt Driven Development with Claude Code: Building a Complete TUI Framework for the Ring Programming Language", "comment": null, "summary": "Large language models are increasingly used in software development, yet their ability to generate and maintain large, multi module systems through natural language interaction remains insufficiently characterized. This study presents an empirical analysis of developing a 7420 line Terminal User Interface framework for the Ring programming language, completed in roughly ten hours of active work spread across three days using a purely prompt driven workflow with Claude Code, Opus 4.5. The system was produced through 107 prompts: 21 feature requests, 72 bug fix prompts, 9 prompts sharing information from Ring documentation, 4 prompts providing architectural guidance, and 1 prompt dedicated to generating documentation. Development progressed across five phases, with the Window Manager phase requiring the most interaction, followed by complex UI systems and controls expansion. Bug related prompts covered redraw issues, event handling faults, runtime errors, and layout inconsistencies, while feature requests focused primarily on new widgets, window manager capabilities, and advanced UI components. Most prompts were short, reflecting a highly iterative workflow in which the human role was limited to specifying requirements, validating behaviour, and issuing corrective prompts without writing any code manually. The resulting framework includes a complete windowing subsystem, event driven architecture, interactive widgets, hierarchical menus, grid and tree components, tab controls, and a multi window desktop environment. By combining quantitative prompt analysis with qualitative assessment of model behaviour, this study provides empirical evidence that modern LLMs can sustain architectural coherence and support the construction of production grade tooling for emerging programming languages, highlighting prompt driven development as a viable methodology within software engineering practice."}
{"id": "2601.18670", "categories": ["cs.NI", "cs.MM", "eess.IV"], "pdf": "https://arxiv.org/pdf/2601.18670", "abs": "https://arxiv.org/abs/2601.18670", "authors": ["Yulong Zhang", "Ying Cui", "Zili Meng", "Abhishek Kumar", "Dirk Kutscher"], "title": "COMETS: Coordinated Multi-Destination Video Transmission with In-Network Rate Adaptation", "comment": "Accepted to appear in IEEE Transactions on Multimedia (2026)", "summary": "Large-scale video streaming events attract millions of simultaneous viewers, stressing existing delivery infrastructures. Client-driven adaptation reacts slowly to shared congestion, while server-based coordination introduces scalability bottlenecks and single points of failure. We present COMETS, a coordinated multi-destination video transmission framework that leverages information-centric networking principles such as request aggregation and in-network state awareness to enable scalable, fair, and adaptive rate control. COMETS introduces a novel range-interest protocol and distributed in-network decision process that aligns video quality across receiver groups while minimizing redundant transmissions. To achieve this, we develop a lightweight distributed optimization framework that guides per-hop quality adaptation without centralized control. Extensive emulation shows that COMETS consistently improves bandwidth utilization, fairness, and user-perceived quality of experience over DASH, MoQ, and ICN baselines, particularly under high concurrency. The results highlight COMETS as a practical, deployable approach for next-generation scalable video delivery."}
{"id": "2601.18158", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2601.18158", "abs": "https://arxiv.org/abs/2601.18158", "authors": ["Karame Mohammadiporshokooh", "Panagiotis Syskakis", "Hartmut Kaiser"], "title": "An Initial Evaluation of Distributed Graph Algorithms using NWGraph and HPX", "comment": "Initial technical report. Extended version of work under submission", "summary": "Graphs are central to modeling relationships in scientific computing, data analysis, and AI/ML, but their growing scale can exceed the memory and compute capacity of single nodes, requiring distributed solutions. Existing distributed graph framework, however, face fundamental challenges: graph algorithms are latency-bound, suffer from irregular memory access, and often impose synchronization costs that limit scalability and efficiency. In this work, we present a distributed implementation of the NWGraph library integrated with the HPX runtime system. By leveraging HPX's asynchronous many-task model, our approach aims to reduce synchronization overhead, improve load balance, and provide a foundation for distributed graph analytics. We evaluate this approach using two representative algorithms: Breadth-First-Search (BFS) and (PageRank). Our initial results show that BFS achieves better performance than the distributed Boost Graph Library (BGL), while PageRank remains more challenging, with current implementation not yet outperforming BGL. These findings highlight both the promise and the open challenges of applying asynchronous task-based runtimes to graph processing, and point to opportunities for future optimizations and extensions."}
{"id": "2601.17604", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.17604", "abs": "https://arxiv.org/abs/2601.17604", "authors": ["Suborno Deb Bappon", "Saikat Mondal", "Chanchal K. Roy", "Kevin Schneider"], "title": "Human-Aligned Enhancement of Programming Answers with LLMs Guided by User Feedback", "comment": "Preprint", "summary": "Large Language Models (LLMs) are widely used to support software developers in tasks such as code generation, optimization, and documentation. However, their ability to improve existing programming answers in a human-like manner remains underexplored. On technical question-and-answer platforms such as Stack Overflow (SO), contributors often revise answers based on user comments that identify errors, inefficiencies, or missing explanations. Yet roughly one-third of this feedback is never addressed due to limited time, expertise, or visibility, leaving many answers incomplete or outdated. This study investigates whether LLMs can enhance programming answers by interpreting and incorporating comment-based feedback. We make four main contributions. First, we introduce ReSOlve, a benchmark consisting of 790 SO answers with associated comment threads, annotated for improvement-related and general feedback. Second, we evaluate four state-of-the-art LLMs on their ability to identify actionable concerns, finding that DeepSeek achieves the best balance between precision and recall. Third, we present AUTOCOMBAT, an LLM-powered tool that improves programming answers by jointly leveraging user comments and question context. Compared to human revised references, AUTOCOMBAT produces near-human quality improvements while preserving the original intent and significantly outperforming the baseline. Finally, a user study with 58 practitioners shows strong practical value, with 84.5 percent indicating they would adopt or recommend the tool. Overall, AUTOCOMBAT demonstrates the potential of scalable, feedback-driven answer refinement to improve the reliability and trustworthiness of technical knowledge platforms."}
{"id": "2601.18727", "categories": ["cs.NI"], "pdf": "https://arxiv.org/pdf/2601.18727", "abs": "https://arxiv.org/abs/2601.18727", "authors": ["Skanda Harisha", "Jimmy G. D. Hester", "Aline Eid"], "title": "An ISAC-ready Full-Duplex Backscatter Architecture for the mmWave IoT", "comment": null, "summary": "Achieving long-range, high-rate, concurrent two-way mmWave communication with power-constrained IoT devices is fundamental to scaling future ubiquitous sensing systems, yet the substantial power demands and high cost of mmWave hardware have long stood in the way of practical deployment. This paper presents the first mmWave full-duplex backscatter tag architecture, charting a genuinely low-cost path toward high-performance mmWave connectivity and localization for ISAC systems. The proposed tag operates at ranges beyond 45m on the uplink and beyond 200m on the downlink, delivering 20x the reach of state-of-the-art systems while being over 100x cheaper than existing mmWave backscatter platforms. Enabling this leap is a novel low-power regenerative amplifier that provides 30 dB of gain while consuming only 30 mW, paired with a regenerative rectifier that achieves state-of-the-art sensitivity down to -60 dBm. We integrate our circuits on a compact PCB and evaluate it across diverse uplink and downlink scenarios, where it achieves an downlink BER of $10^{-1}$ at 200 meters and a uplink BER of $10^{-2}$ at 45 meters, demonstrating resilient, high-quality communication even at extended ranges."}
{"id": "2601.18400", "categories": ["cs.DC", "cs.NI", "cs.PF"], "pdf": "https://arxiv.org/pdf/2601.18400", "abs": "https://arxiv.org/abs/2601.18400", "authors": ["Andrei Lebedev", "Vincent Gramoli"], "title": "On the Bandwidth Consumption of Blockchains", "comment": "11 pages, 6 figures", "summary": "With the advent of blockchain technology, the number of proposals has boomed. The network traffic imposed by these blockchain proposals increases the cost of hosting nodes. Unfortunately, as of today, we are not aware of any comparative study of the bandwidth consumption of blockchains.\n  In this paper, we propose the first empirical comparison of blockchain bandwidth consumption. To this end, we measure the network traffic of blockchain network nodes of five blockchain protocols: Algorand, Aptos, Avalanche, Redbelly and Solana. We study the variation over time, differentiate the receiving and sending traffic and analyze how this traffic varies with the number of nodes and validators.\n  We conclude that the transport protocol is the main factor impacting the network traffic, segregating node roles helps reduce traffic and different blockchains are differently impacted by the network size."}
{"id": "2601.17627", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2601.17627", "abs": "https://arxiv.org/abs/2601.17627", "authors": ["Dung Pham", "Taher A. Ghaleb"], "title": "Code Change Characteristics and Description Alignment: A Comparative Study of Agentic versus Human Pull Requests", "comment": "Accepted at the 23rd International Conference on Mining Software Repositories (MSR '26)", "summary": "AI coding agents can autonomously generate pull requests (PRs), yet little is known about how their contributions compare to those of humans. We analyze 33,596 agent-generated PRs (APRs) and 6,618 human PRs (HPRs) to compare code-change characteristics and message quality. We observe that APR-introduced symbols (functions and classes) are removed much sooner than those in HPRs (median time to removal 3 vs. 34 days) and are also removed more often (symbol churn 7.33% vs. 4.10%), reflecting a focus on other tasks like documentation and test updates. Agents generate stronger commit-level messages (semantic similarity 0.72 vs. 0.68) but lag humans at PR-level summarization (PR-commit similarity 0.86 vs. 0.88). Commit message length is the best predictor of description quality, indicating reliance on individual commits over full-PR reasoning. These findings highlight a gap between agents' micro-level precision and macro-level communication, suggesting opportunities to improve agent-driven development workflows."}
{"id": "2601.18400", "categories": ["cs.DC", "cs.NI", "cs.PF"], "pdf": "https://arxiv.org/pdf/2601.18400", "abs": "https://arxiv.org/abs/2601.18400", "authors": ["Andrei Lebedev", "Vincent Gramoli"], "title": "On the Bandwidth Consumption of Blockchains", "comment": "11 pages, 6 figures", "summary": "With the advent of blockchain technology, the number of proposals has boomed. The network traffic imposed by these blockchain proposals increases the cost of hosting nodes. Unfortunately, as of today, we are not aware of any comparative study of the bandwidth consumption of blockchains.\n  In this paper, we propose the first empirical comparison of blockchain bandwidth consumption. To this end, we measure the network traffic of blockchain network nodes of five blockchain protocols: Algorand, Aptos, Avalanche, Redbelly and Solana. We study the variation over time, differentiate the receiving and sending traffic and analyze how this traffic varies with the number of nodes and validators.\n  We conclude that the transport protocol is the main factor impacting the network traffic, segregating node roles helps reduce traffic and different blockchains are differently impacted by the network size."}
{"id": "2601.17615", "categories": ["cs.AR", "cs.DC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.17615", "abs": "https://arxiv.org/abs/2601.17615", "authors": ["Rahul Bera", "Zhenrong Lang", "Caroline Hengartner", "Konstantinos Kanellopoulos", "Rakesh Kumar", "Mohammad Sadrosadati", "Onur Mutlu"], "title": "Athena: Synergizing Data Prefetching and Off-Chip Prediction via Online Reinforcement Learning", "comment": null, "summary": "Prefetching and off-chip prediction are two techniques proposed to hide long memory access latencies in high-performance processors. In this work, we demonstrate that: (1) prefetching and off-chip prediction often provide complementary performance benefits, yet (2) naively combining them often fails to realize their full performance potential, and (3) existing prefetcher control policies leave significant room for performance improvement behind.\n  Our goal is to design a holistic framework that can autonomously learn to coordinate an off-chip predictor with multiple prefetchers employed at various cache levels. To this end, we propose a new technique called Athena, which models the coordination between prefetchers and off-chip predictor (OCP) as a reinforcement learning (RL) problem. Athena acts as the RL agent that observes multiple system-level features (e.g., prefetcher/OCP accuracy, bandwidth usage) over an epoch of program execution, and uses them as state information to select a coordination action (i.e., enabling the prefetcher and/or OCP, and adjusting prefetcher aggressiveness). At the end of every epoch, Athena receives a numerical reward that measures the change in multiple system-level metrics (e.g., number of cycles taken to execute an epoch). Athena uses this reward to autonomously and continuously learn a policy to coordinate prefetchers with OCP.\n  Our extensive evaluation using a diverse set of memory-intensive workloads shows that Athena consistently outperforms prior state-of-the-art coordination policies across a wide range of system configurations with various combinations of underlying prefetchers, OCPs, and main memory bandwidths, while incurring only modest storage overhead. Athena is freely available at https://github.com/CMU-SAFARI/Athena."}
{"id": "2601.17762", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2601.17762", "abs": "https://arxiv.org/abs/2601.17762", "authors": ["Zelong Zheng", "Jiayuan Zhou", "Xing Hu", "Yi Gao", "Shengyi Pan"], "title": "Multi-Agent End-to-End Vulnerability Management for Mitigating Recurring Vulnerabilities", "comment": null, "summary": "Software vulnerability management has become increasingly critical as modern systems scale in size and complexity. However, existing automated approaches remain insufficient. Traditional static analysis methods struggle to precisely capture contextual dependencies, especially when vulnerabilities span multiple functions or modules. Large language models (LLMs) often lack the ability to retrieve and exploit sufficient contextual information, resulting in incomplete reasoning and unreliable outcomes. Meanwhile, recurring vulnerabilities emerge repeatedly due to code reuse and shared logic, making historical vulnerability knowledge an indispensable foundation for effective vulnerability detection and repair. Nevertheless, prior approaches such as clone-based detection and patch porting, have not fully leveraged this knowledge. To address these challenges, we present MAVM, a multi-agent framework for end-to-end recurring vulnerability management. MAVM integrates five components, including a vulnerability knowledge base, detection, confirmation, repair, and validation, into a unified multi-agent pipeline. We construct a knowledge base from publicly disclosed vulnerabilities, thereby addressing the underuse of historical knowledge in prior work and mitigating the lack of domain-specific expertise in LLMs. Furthermore, we design context-retrieval tools that allow agents to extract and reason over repository-level information, overcoming the contextual limitations of previous methods. Based on agents, MAVM effectively simulates real-world security workflows. To evaluate the performance of MAVM, we construct a dataset containing 78 real-world patch-porting cases (covering 114 function-level migrations). On this dataset, MAVM successfully detects and repairs 51 real vulnerabilities, outperforming baselines by 31.9%-45.2% in repair accuracy, which demonstrates its effectiveness."}
{"id": "2601.17633", "categories": ["cs.AR", "cs.DC"], "pdf": "https://arxiv.org/pdf/2601.17633", "abs": "https://arxiv.org/abs/2601.17633", "authors": ["Rakesh Nadig", "Vamanan Arulchelvan", "Mayank Kabra", "Harshita Gupta", "Rahul Bera", "Nika Mansouri Ghiasi", "Nanditha Rao", "Qingcai Jiang", "Andreas Kosmas Kakolyris", "Yu Liang", "Mohammad Sadrosadati", "Onur Mutlu"], "title": "Conduit: Programmer-Transparent Near-Data Processing Using Multiple Compute-Capable Resources in Solid State Drives", "comment": "To appear in IEEE International Symposium on High-Performance Computer Architecture (HPCA) 2026", "summary": "Solid-state drives (SSDs) are well suited for near-data processing (NDP) because they: (1) store large application datasets, and (2) support three NDP paradigms: in-storage processing (ISP), processing using DRAM in the SSD (PuD-SSD), and in-flash processing (IFP). A large body of prior SSD-based NDP techniques operate in isolation, mapping computations to only one or two NDP paradigms (i.e., ISP, PuD-SSD, or IFP) within the SSD. These techniques (1) are tailored to specific workloads or kernels, (2) do not exploit the full computational potential of an SSD, and (3) lack programmer-transparency. While several prior works propose techniques to partition computation between the host and near-memory accelerators, adapting these techniques to SSDs has limited benefits because they (1) ignore the heterogeneity of the SSD resources, and (2) make offloading decisions based on limited factors such as bandwidth utilization, or data movement cost. We propose Conduit, a general-purpose, programmer-transparent NDP framework for SSDs that leverages multiple SSD computation resources. At compile time, Conduit executes a custom compiler (e.g., LLVM) pass that (i) vectorizes suitable application code segments into SIMD operations that align with the SSD's page layout, and (ii) embeds metadata (e.g., operation type, operand sizes) into the vectorized instructions to guide runtime offloading decisions. At runtime, within the SSD, Conduit performs instruction-granularity offloading by evaluating six key features, and uses a cost function to select the most suitable SSD resource. We evaluate Conduit and two prior NDP offloading techniques using an in-house event-driven SSD simulator on six data-intensive workloads. Conduit outperforms the best-performing prior offloading policy by 1.8x and reduces energy consumption by 46%."}
{"id": "2601.17888", "categories": ["cs.SE", "cs.CR", "cs.PL"], "pdf": "https://arxiv.org/pdf/2601.17888", "abs": "https://arxiv.org/abs/2601.17888", "authors": ["Monika Santra", "Bokai Zhang", "Mark Lim", "Vishnu Asutosh Dasu", "Dongrui Zeng", "Gang Tan"], "title": "iResolveX: Multi-Layered Indirect Call Resolution via Static Reasoning and Learning-Augmented Refinement", "comment": null, "summary": "Indirect call resolution remains a key challenge in reverse engineering and control-flow graph recovery, especially for stripped or optimized binaries. Static analysis is sound but often over-approximates, producing many false positives, whereas machine-learning approaches can improve precision but may sacrifice completeness and generalization. We present iResolveX, a hybrid multi-layered framework that combines conservative static analysis with learning-based refinement. The first layer applies a conservative value-set analysis (BPA) to ensure high recall. The second layer adds a learning-based soft-signature scorer (iScoreGen) and selective inter-procedural backward analysis with memory inspection (iScoreRefine) to reduce false positives. The final output, p-IndirectCFG, annotates indirect edges with confidence scores, enabling downstream analyses to choose appropriate precision--recall trade-offs. Across SPEC CPU2006 and real-world binaries, iScoreGen reduces predicted targets by 19.2% on average while maintaining BPA-level recall (98.2%). Combined with iScoreRefine, the total reduction reaches 44.3% over BPA with 97.8% recall (a 0.4% drop). iResolveX supports both conservative, recall-preserving and F1-optimized configurations and outperforms state-of-the-art systems."}
{"id": "2601.17903", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2601.17903", "abs": "https://arxiv.org/abs/2601.17903", "authors": ["Tolgahan Bardakci", "Andreas Faes", "Mutlu Beyazit", "Serge Demeyr"], "title": "Prompt-Based REST API Test Amplification in Industry: An Experience Report", "comment": null, "summary": "Large Language Models (LLMs) are increasingly used to support software testing tasks, yet there is little evidence of their effectiveness for REST API testing in industrial settings. To address this gap, we replicate our earlier work on LLM-based REST API test amplification within an industrial context at one of the largest logistics companies in Belgium. We apply LLM-based test amplification to six representative endpoints of a production microservice embedded in a large-scale, security-sensitive system, where there is in-depth complexity in authentication, stateful behavior, and organizational constraints. Our experience shows that LLM-based test amplification remains practically useful in industry by increasing coverage and revealing various observations and anomalies."}
{"id": "2601.18044", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2601.18044", "abs": "https://arxiv.org/abs/2601.18044", "authors": ["Melika Sepidband", "Hamed Taherkhani", "Hung Viet Pham", "Hadi Hemmati"], "title": "RGFL: Reasoning Guided Fault Localization for Automated Program Repair Using Large Language Models", "comment": "23 pages, 5 figures", "summary": "Fault Localization (FL) is a critical step in Automated Program Repair (APR), and its importance has increased with the rise of Large Language Model (LLM)-based repair agents. In realistic project-level repair scenarios, software repositories often span millions of tokens, far exceeding current LLM context limits. Consequently, models must first identify a small, relevant subset of code, making accurate FL essential for effective repair. We present a novel project-level FL approach that improves both file- and element-level localization. Our method introduces a hierarchical reasoning module that (i) generates structured, bug-specific explanations for candidate files and elements, and (ii) leverages these explanations in a two-stage ranking scheme combining LLM-based and embedding-based signals. We further propose a counterfactual upper-bound analysis to quantify the contribution of each localization stage to repair success. We evaluate our approach on Python and Java projects from SWE-bench Verified, Lite, and Java. Compared to state-of-the-art baselines, including Agentless and OpenHands, our method consistently improves localization accuracy. On SWE-bench Verified, file-level Hit@1 improves from 71.4% to 85%, and MRR from 81.8% to 88.8%. At the element level, Exact Match under top-3 files increases from 36% to 69%. Integrating our localization into Agentless yields a 12.8% end-to-end repair success improvement."}
{"id": "2601.18241", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.18241", "abs": "https://arxiv.org/abs/2601.18241", "authors": ["Elena Bruches", "Vadim Alperovich", "Dari Baturova", "Roman Derunets", "Daniil Grebenkin", "Georgy Mkrtchyan", "Oleg Sedukhin", "Mikhail Klementev", "Ivan Bondarenko", "Nikolay Bushkov", "Stanislav Moiseev"], "title": "TAM-Eval: Evaluating LLMs for Automated Unit Test Maintenance", "comment": "Accepted for publication at the 9th Workshop on Validation, Analysis and Evolution of Software Tests (VST 2026), co-located with the the 33rd IEEE International Conference on Software Analysis, Evolution and Reengineering (SANER 2026)", "summary": "While Large Language Models (LLMs) have shown promise in software engineering, their application to unit testing remains largely confined to isolated test generation or oracle prediction, neglecting the broader challenge of test suite maintenance. We introduce TAM-Eval (Test Automated Maintenance Evaluation), a framework and benchmark designed to evaluate model performance across three core test maintenance scenarios: creation, repair, and updating of test suites. Unlike prior work limited to function-level tasks, TAM-Eval operates at the test file level, while maintaining access to full repository context during isolated evaluation, better reflecting real-world maintenance workflows. Our benchmark comprises 1,539 automatically extracted and validated scenarios from Python, Java, and Go projects. TAM-Eval supports system-agnostic evaluation of both raw LLMs and agentic workflows, using a reference-free protocol based on test suite pass rate, code coverage, and mutation testing. Empirical results indicate that state-of-the-art LLMs have limited capabilities in realistic test maintenance processes and yield only marginal improvements in test effectiveness. We release TAM-Eval as an open-source framework to support future research in automated software testing. Our data and code are publicly available at https://github.com/trndcenter/TAM-Eval."}
{"id": "2601.18341", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2601.18341", "abs": "https://arxiv.org/abs/2601.18341", "authors": ["Romain Robbes", "Tho Matricon", "Thomas Degueule", "Andre Hora", "Stefano Zacchiroli"], "title": "Agentic Much? Adoption of Coding Agents on GitHub", "comment": null, "summary": "In the first half of 2025, coding agents have emerged as a category of development tools that have very quickly transitioned to the practice. Unlike ''traditional'' code completion LLMs such as Copilot, agents like Cursor, Claude Code, or Codex operate with high degrees of autonomy, up to generating complete pull requests starting from a developer-provided task description. This new mode of operation is poised to change the landscape in an even larger way than code completion LLMs did, making the need to study their impact critical. Also, unlike traditional LLMs, coding agents tend to leave more explicit traces in software engineering artifacts, such as co-authoring commits or pull requests. We leverage these traces to present the first large-scale study (129,134 projects) of the adoption of coding agents on GitHub, finding an estimated adoption rate of 15.85%--22.60%, which is very high for a technology only a few months old--and increasing. We carry out an in-depth study of the adopters we identified, finding that adoption is broad: it spans the entire spectrum of project maturity; it includes established organizations; and it concerns diverse programming languages or project topics. At the commit level, we find that commits assisted by coding agents are larger than commits only authored by human developers, and have a large proportion of features and bug fixes. These findings highlight the need for further investigation into the practical use of coding agents."}
{"id": "2601.18344", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2601.18344", "abs": "https://arxiv.org/abs/2601.18344", "authors": ["Alexandros Tsakpinis", "Efe Berk Erglec", "Emil Schwenger", "Alexander Pretschner"], "title": "Forecasting the Maintained Score from the OpenSSF Scorecard for GitHub Repositories linked to PyPI libraries", "comment": "11 pages, 9 figures, 2 tables", "summary": "The OpenSSF Scorecard is widely used to assess the security posture of open-source software repositories, with the Maintained metric indicating recent development activity and helping identify potentially abandoned dependencies. However, this metric is inherently retrospective, reflecting only the past 90 days of activity and providing no insight into future maintenance, which limits its usefulness for proactive risk assessment. In this paper, we study to what extent future maintenance activity, as captured by the OpenSSF Maintained score, can be forecasted. We analyze 3,220 GitHub repositories associated with the top 1% most central PyPI libraries by PageRank and reconstruct historical Maintained scores over a three-year period. We formulate the task as multivariate time series forecasting and consider four target representations: raw scores, bucketed maintenance levels, numerical trend slopes, and categorical trend types. We compare a statistical model (VARMA), a machine learning model (Random Forest), and a deep learning model (LSTM) across training windows of 3-12 months and forecasting horizons of 1-6 months. Our results show that future maintenance activity can be predicted with meaningful accuracy, particularly for aggregated representations such as bucketed scores and trend types, achieving accuracies above 0.95 and 0.80, respectively. Simpler statistical and machine learning models perform on par with deep learning approaches, indicating that complex architectures are not required. These findings suggest that predictive modeling can effectively complement existing Scorecard metrics, enabling more proactive assessment of open-source maintenance risks."}
{"id": "2601.18345", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2601.18345", "abs": "https://arxiv.org/abs/2601.18345", "authors": ["Romain Robes Tho Matricon", "Thomas Degueule", "Andre Hora", "Stefano Zacchiroli"], "title": "Promises, Perils, and (Timely) Heuristics for Mining Coding Agent Activity", "comment": "Preprint. Accepted for publication at MSR 2026", "summary": "In 2025, coding agents have seen a very rapid adoption. Coding agents leverage Large Language Models (LLMs) in ways that are markedly different from LLM-based code completion, making their study critical. Moreover, unlike LLM-based completion, coding agents leave visible traces in software repositories, enabling the use of MSR techniques to study their impact on SE practices. This paper documents the promises, perils, and heuristics that we have gathered from studying coding agent activity on GitHub."}
{"id": "2601.18418", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.18418", "abs": "https://arxiv.org/abs/2601.18418", "authors": ["Ji Zeng", "Dayuan Fu", "Tiantian Mi", "Yumin Zhuang", "Yaxing Huang", "Xuefeng Li", "Lyumanshan Ye", "Muhang Xie", "Qishuo Hua", "Zhen Huang", "Mohan Jiang", "Hanning Wang", "Jifan Lin", "Yang Xiao", "Jie Sun", "Yunze Wu", "Pengfei Liu"], "title": "daVinci-Dev: Agent-native Mid-training for Software Engineering", "comment": null, "summary": "Recently, the frontier of Large Language Model (LLM) capabilities has shifted from single-turn code generation to agentic software engineering-a paradigm where models autonomously navigate, edit, and test complex repositories. While post-training methods have become the de facto approach for code agents, **agentic mid-training**-mid-training (MT) on large-scale data that mirrors authentic agentic workflows-remains critically underexplored due to substantial resource requirements, despite offering a more scalable path to instilling foundational agentic behaviors than relying solely on expensive reinforcement learning. A central challenge in realizing effective agentic mid-training is the distribution mismatch between static training data and the dynamic, feedback-rich environment of real development. To address this, we present a systematic study of agentic mid-training, establishing both the data synthesis principles and training methodology for effective agent development at scale. Central to our approach is **agent-native data**-supervision comprising two complementary types of trajectories: **contextually-native trajectories** that preserve the complete information flow an agent experiences, offering broad coverage and diversity; and **environmentally-native trajectories** collected from executable repositories where observations stem from actual tool invocations and test executions, providing depth and interaction authenticity. We verify the model's agentic capabilities on `SWE-Bench Verified`. We demonstrate our superiority over the previous open software engineering mid-training recipe `Kimi-Dev` under two post-training settings with an aligned base model and agentic scaffold, while using less than half mid-training tokens (73.1B). Besides relative advantage, our best performing 32B and 72B models achieve **56.1%** and **58.5%** resolution rates, respectively, which are ..."}
{"id": "2601.18477", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2601.18477", "abs": "https://arxiv.org/abs/2601.18477", "authors": ["Giuseppe Destefanis", "Leila Yousefi", "Martin Shepperd", "Allan Tucker", "Stephen Swift", "Steve Counsell", "Mahir Arzoky"], "title": "An Audit of Machine Learning Experiments on Software Defect Prediction", "comment": null, "summary": "Background: Machine learning algorithms are widely used to predict defect prone software components. In this literature, computational experiments are the main means of evaluation, and the credibility of results depends on experimental design and reporting. Objective: This paper audits recent software defect prediction (SDP) studies by assessing their experimental design, analysis, and reporting practices against accepted norms from statistics, machine learning, and empirical software engineering. The aim is to characterise current practice and assess the reproducibility of published results. Method: We audited SDP studies indexed in SCOPUS between 2019 and 2023, focusing on design and analysis choices such as outcome measures, out of sample validation strategies, and the use of statistical inference. Nine study issues were evaluated. Reproducibility was assessed using the instrument proposed by Gonzlez Barahona and Robles. Results: The search identified approximately 1,585 SDP experiments published during the period. From these, we randomly sampled 101 papers, including 61 journal and 40 conference publications, with almost 50 percent behind paywalls. We observed substantial variation in research practice. The number of datasets ranged from 1 to 365, learners or learner variants from 1 to 34, and performance measures from 1 to 9. About 45 percent of studies applied formal statistical inference. Across the sample, we identified 427 issues, with a median of four per paper, and only one paper without issues. Reproducibility ranged from near complete to severely limited. We also identified two cases of tortured phrases and possible paper mill activity. Conclusions: Experimental design and reporting practices vary widely, and almost half of the studies provide insufficient detail to support reproduction. The audit indicates substantial scope for improvement."}
{"id": "2601.18566", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2601.18566", "abs": "https://arxiv.org/abs/2601.18566", "authors": ["Fabio Massacci", "Winnie Mbaka"], "title": "On the Abolition of the \"ICSE Paper\" and the Adoption of the \"Registered Proposal\" and the \"Results Report\"", "comment": "10 pages, 0 figures, International Conference on Software Engineering", "summary": "To address the 'novelty-vicious cycle' and the 'replicability crisis' of the field (both discussed in the survey) we propose abolishing the \"ICSE paper\" as we know it and replacing it with a two-tier system that also evolves the existing notion of 'Registered Report'. Authors proposing a new idea, experiment, or analysis would submit a \"Registered Proposal\" of their idea and the proposed experimental methodology to undergo peer review. The following year, anyone can submit (shorter) \"Results Reports\" on the realization of the empirical work based on the registered proposals of the previous ICSE (or FSE or ISSTA or ASE etc.). Both works should be first class citizens of the mainstream events. We argue that such a disruptive (heretical?) idea is supported and based on the responses of the community of the Future of Software Engineering pre-survey"}
{"id": "2601.18591", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2601.18591", "abs": "https://arxiv.org/abs/2601.18591", "authors": ["Fiorella Zampetti", "Federico Stocchetti", "Federica Razzano", "Damian Andrew Tamburri", "Massimiliano Di Penta"], "title": "How are MLOps Frameworks Used in Open Source Projects? An Empirical Characterization", "comment": null, "summary": "Machine Learning (ML) Operations (MLOps) frameworks have been conceived to support developers and AI engineers in managing the lifecycle of their ML models. While such frameworks provide a wide range of features, developers may leverage only a subset of them, while missing some highly desired features. This paper investigates the practical use and desired feature enhancements of eight popular open-source MLOps frameworks. Specifically, we analyze their usage by dependent projects on GitHub, examining how they invoke the frameworks' APIs and commands. Then, we qualitatively analyze feature requests and enhancements mined from the frameworks' issue trackers, relating these desired improvements to the previously identified usage features. Results indicate that MLOps frameworks are rarely used out-of-the-box and are infrequently integrated into GitHub Workflows, but rather, developers use their APIs to implement custom functionality in their projects. Used features concern core ML phases and whole infrastructure governance, sometimes leveraging multiple frameworks with complementary features. The mapping with feature requests highlights that users mainly ask for enhancements to core features of the frameworks, but also better API exposure and CI/CD integration."}
{"id": "2601.18749", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2601.18749", "abs": "https://arxiv.org/abs/2601.18749", "authors": ["Haruhiko Yoshioka", "Takahiro Monno", "Haruka Tokumasu", "Taiki Wakamatsu", "Yuki Ota", "Nimmi Weeraddana", "Kenichi Matsumoto"], "title": "Let's Make Every Pull Request Meaningful: An Empirical Analysis of Developer and Agentic Pull Requests", "comment": "Accepted for publication in the 23rd International Conference on Mining Software Repositories (MSR '26) : 5 pages, 3 figures, 3 tables", "summary": "The automatic generation of pull requests (PRs) using AI agents has become increasingly common. Although AI-generated PRs are fast and easy to create, their merge rates have been reported to be lower than those created by humans. In this study, we conduct a large-scale empirical analysis of 40,214 PRs collected from the AIDev dataset. We extract 64 features across six families and fit statistical regression models to compare PR merge outcomes for human and agentic PRs, as well as across three AI agents. Our results show that submitter attributes dominate merge outcomes for both groups, while review-related features exhibit contrasting effects between human and agentic PRs. The findings of this study provide insights into improving PR quality through human-AI collaboration."}
