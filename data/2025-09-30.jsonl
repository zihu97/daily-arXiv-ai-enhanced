{"id": "2509.22834", "categories": ["cs.NI", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.22834", "abs": "https://arxiv.org/abs/2509.22834", "authors": ["Anis Bekri", "Amar Abane", "Abdella Battou", "Saddek Bensalem"], "title": "Bridging Language Models and Formal Methods for Intent-Driven Optical Network Design", "comment": "Accepted at AICCSA 2025", "summary": "Intent-Based Networking (IBN) aims to simplify network management by enabling\nusers to specify high-level goals that drive automated network design and\nconfiguration. However, translating informal natural-language intents into\nformally correct optical network topologies remains challenging due to inherent\nambiguity and lack of rigor in Large Language Models (LLMs). To address this,\nwe propose a novel hybrid pipeline that integrates LLM-based intent parsing,\nformal methods, and Optical Retrieval-Augmented Generation (RAG). By enriching\ndesign decisions with domain-specific optical standards and systematically\nincorporating symbolic reasoning and verification techniques, our pipeline\ngenerates explainable, verifiable, and trustworthy optical network designs.\nThis approach significantly advances IBN by ensuring reliability and\ncorrectness, essential for mission-critical networking tasks."}
{"id": "2509.23125", "categories": ["cs.NI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.23125", "abs": "https://arxiv.org/abs/2509.23125", "authors": ["Yiqing Zhou", "Xule Zhou", "Zecan Cheng", "Chenao Lu", "Junhan Chen", "Jiahong Pan", "Yizhuo Liu", "Sihao Li", "Kyeong Soo Kim"], "title": "Impact of Environmental Factors on LoRa 2.4 GHz Time of Flight Ranging Outdoors", "comment": "5 pages, 8 figures, 2 tables, and under review for presentation at a\n  workshop", "summary": "In WSN/IoT, node localization is essential to long-running applications for\naccurate environment monitoring and event detection, often covering a large\narea in the field. Due to the lower time resolution of typical WSN/IoT\nplatforms (e.g., 1 microsecond on ESP32 platforms) and the jitters in\ntimestamping, packet-level localization techniques cannot provide meter-level\nresolution. For high-precision localization as well as world-wide\ninteroperability via 2.4-GHz ISM band, a new variant of LoRa, called LoRa 2.4\nGHz, was proposed by semtech, which provides a radio frequency (RF) time of\nflight (ToF) ranging method for meter-level localization. However, the existing\ndatasets reported in the literature are limited in their coverages and do not\ntake into account varying environmental factors such as temperature and\nhumidity. To address these issues, LoRa 2.4 GHz RF ToF ranging data was\ncollected on a sports field at the XJTLU south campus, where three LoRa nodes\nlogged samples of ranging with a LoRa base station, together with temperature\nand humidity, at reference points arranged as a 3x3 grid covering 400 square\nmeter over three weeks and uploaded all measurement records to the base station\nequipped with an ESP32-based transceiver for machine and user communications.\nThe results of a preliminary investigation based on a simple deep neural\nnetwork (DNN) model demonstrate that the environmental factors, including the\ntemperature and humidity, significantly affect the accuracy of ranging, which\ncalls for advanced methods of compensating for the effects of environmental\nfactors on LoRa RF ToF ranging outdoors."}
{"id": "2509.23216", "categories": ["cs.NI", "cs.IT", "cs.NA", "cs.PF", "cs.SY", "eess.SY", "math.IT", "math.NA", "68M10, 68M20, 60J20", "C.2.1; C.2.5; C.4"], "pdf": "https://arxiv.org/pdf/2509.23216", "abs": "https://arxiv.org/abs/2509.23216", "authors": ["Po-Heng Chou"], "title": "Unlicensed Band Allocation for Heterogeneous Networks", "comment": "14 pages, 12 figures, 1 table, published in IEICE Transactions on\n  Communications", "summary": "Based on the License-Assisted Access (LAA) small cell architecture, the LAA\ncoexisting with Wi-Fi heterogeneous networks provides LTE mobile users with\nhigh bandwidth efficiency as the unlicensed channels are shared among LAA and\nWi-Fi. However, LAA and Wi-Fi interfere with each other when both systems use\nthe same unlicensed channel in heterogeneous networks. In such a network,\nunlicensed band allocation for LAA and Wi-Fi is an important issue that may\naffect the quality of service (QoS) of both systems significantly. In this\npaper, we propose an analytical model and conduct simulation experiments to\nstudy four allocations for the unlicensed band: unlicensed full allocation\n(UFA), unlicensed time-division allocation (UTA), and UFA/UTA with buffering\nmechanism (UFAB and UTAB) for the LAA data packets. We evaluate the performance\nof these unlicensed band allocation schemes in terms of the acceptance rate of\nboth LAA and Wi-Fi packet data in the LAA buffer queue. Our study provides\nguidelines for designing the channel occupation phase and the buffer size of\nthe LAA small cell."}
{"id": "2509.23217", "categories": ["cs.NI", "cs.IT", "cs.NA", "cs.PF", "cs.SY", "eess.SY", "math.IT", "math.NA", "68M10, 68M20, 60J20", "C.2.1; C.2.5; C.4"], "pdf": "https://arxiv.org/pdf/2509.23217", "abs": "https://arxiv.org/abs/2509.23217", "authors": ["Po-Heng Chou"], "title": "Modeling the Unlicensed Band Allocation for LAA With Buffering Mechanism", "comment": "5 pages, 3 figures, 2 tables, published in IEEE Communications\n  Letters", "summary": "In this letter, we propose an analytical model and conduct simulation\nexperiments to study listen-before-talk-based unlicensed band allocation with\nthe buffering mechanism for the License-Assisted Access (LAA) packets in the\nheterogeneous networks. In such a network, unlicensed band allocation for LAA\nand Wi-Fi is an important issue, which may affect the quality of service for\nboth systems significantly. We evaluate the performance of these unlicensed\nband allocations in terms of the acceptance rate of both LAA and Wi-Fi packets.\nThis letter provides the guidelines for designing the channel occupation phase\nand buffer threshold of the LAA systems."}
{"id": "2509.25015", "categories": ["cs.OS"], "pdf": "https://arxiv.org/pdf/2509.25015", "abs": "https://arxiv.org/abs/2509.25015", "authors": ["Yanlin Du", "Ruslan Nikolaev"], "title": "Joyride: Rethinking Linux's network stack design for better performance, security, and reliability", "comment": null, "summary": "Contemporary distributed computing workloads, including scientific\ncomputation, data mining, and machine learning, increasingly demand OS\nnetworking with minimal latency as well as high throughput, security, and\nreliability. However, Linux's conventional TCP/IP stack becomes increasingly\nproblematic for high-end NICs, particularly those operating at 100 Gbps and\nbeyond.\n  These limitations come mainly from overheads associated with kernel space\nprocessing, mode switching, and data copying in the legacy architecture.\nAlthough kernel bypass techniques such as DPDK and RDMA offer alternatives,\nthey introduce significant adoption barriers: both often require extensive\napplication redesign, and RDMA is not universally available on commodity\nhardware.\n  This paper proposes Joyride, a high performance framework with a grand vision\nof replacing Linux's legacy network stack while providing compatibility with\nexisting applications. Joyride aims to integrate kernel bypass ideas,\nspecifically DPDK and a user-space TCP/IP stack, while designing a\nmicrokernel-style architecture for Linux networking."}
{"id": "2509.22951", "categories": ["cs.PF", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.22951", "abs": "https://arxiv.org/abs/2509.22951", "authors": ["Jack Cashman", "Jiaqi Nie"], "title": "Tiny-QMoE", "comment": null, "summary": "The QMoE model provides a practical approach for compression of massive\nMixture-of-Experts (MoE) models. QMoE offers a solution geared towards memory\nlimitations that often reach terabyte scales, and it has the advantage of\nworking with high sparsity models which implicitly lend themselves to\ncompression techniques. QMoE also has the advantage of only taking MoE models\ninto account and does not evaluate its use with non mixture of expert systems.\nAlthough this prior attempt focuses on the limitations of large servers with\nthe latest NVIDIA hardware which in the case of the H100 and V100 which have 80\nGB of HBM (High Bandwidth Memory), what is not being considered is a\nsignificantly more constrained environment, such as in the case of mobile\ndevices which may have in the case of the iPhone anywhere from 4 to 8 GB of\nunified memory which also needs to be shared with the operating system and\nadditional processes. Although edge devices such as phones and laptops are\nbecoming increasingly more computationally powerful, they are still not close\nto the level of advanced server machines such as NVIDIA. An additional\nconstraint that we must consider is that of latency. The communication time of\nsending a request to an LLM server and then getting it back is an additional\nwaiting time that can be removed. We may also want to use LLM technology in\nenvironments where there is no reliable network connection."}
{"id": "2509.23026", "categories": ["cs.MA", "cs.GT"], "pdf": "https://arxiv.org/pdf/2509.23026", "abs": "https://arxiv.org/abs/2509.23026", "authors": ["Yue Wang"], "title": "Game-Theoretic Understandings of Multi-Agent Systems with Multiple Objectives", "comment": "Preprint, work in progress", "summary": "In practical multi-agent systems, agents often have diverse objectives, which\nmakes the system more complex, as each agent's performance across multiple\ncriteria depends on the joint actions of all agents, creating intricate\nstrategic trade-offs. To address this, we introduce the Multi-Objective Markov\nGame (MOMG), a framework for multi-agent reinforcement learning with multiple\nobjectives. We propose the Pareto-Nash Equilibrium (PNE) as the primary\nsolution concept, where no agent can unilaterally improve one objective without\nsacrificing performance on another. We prove existence of PNE, and establish an\nequivalence between the PNE and the set of Nash Equilibria of MOMG's\ncorresponding linearly scalarized games, enabling solutions of MOMG by\ntransferring to a standard single-objective Markov game. However, we note that\ncomputing a PNE is theoretically and computationally challenging, thus we\npropose and study weaker but more tractable solution concepts. Building on\nthese foundations, we develop online learning algorithm that identify a single\nsolution to MOMGs. Furthermore, we propose a two-phase, preference-free\nalgorithm that decouples exploration from planning. Our algorithm enables\ncomputation of a PNE for any given preference profile without collecting new\nsamples, providing an efficient methodological characterization of the entire\nPareto-Nash front."}
{"id": "2509.22679", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2509.22679", "abs": "https://arxiv.org/abs/2509.22679", "authors": ["Abdessalam Benhari", "Yves Denneulin", "Frédéric Desprez", "Fanny Dufossé", "Denis Trystram"], "title": "Analysis of the carbon footprint of HPC", "comment": null, "summary": "The demand in computing power has never stopped growing over the years.\nToday, the performance of the most powerful systems exceeds the exascale.\nUnfortunately, this growth also comes with ever-increasing energy costs,\nleading to a high carbon footprint. This paper investigates the evolution of\nhigh performance systems in terms of carbon emissions. A lot of studies focus\non Top500 (and Green500) as the tip of an iceberg to identify trends in the\ndomain in terms of computing performance. We propose here to go further in\nconsidering the whole span life of several large scale systems and to link the\nevolution with trajectory toward 2030. More precisely, we introduce the energy\nmix in the analysis of Top500 systems and we derive a predictive model for\nestimating the weight of HPC for the next 5 years."}
{"id": "2509.22908", "categories": ["cs.SE", "cs.LG", "cs.PL"], "pdf": "https://arxiv.org/pdf/2509.22908", "abs": "https://arxiv.org/abs/2509.22908", "authors": ["Sergiu Bursuc", "Theodore Ehrenborg", "Shaowei Lin", "Lacramioara Astefanoaei", "Ionel Emilian Chiosa", "Jure Kukovec", "Alok Singh", "Oliver Butterley", "Adem Bizid", "Quinn Dougherty", "Miranda Zhao", "Max Tan", "Max Tegmark"], "title": "A benchmark for vericoding: formally verified program synthesis", "comment": "25 pages, 1 figure; data available at\n  https://github.com/Beneficial-AI-Foundation/vericoding-benchmark", "summary": "We present and test the largest benchmark for vericoding, LLM-generation of\nformally verified code from formal specifications - in contrast to vibe coding,\nwhich generates potentially buggy code from a natural language description. Our\nbenchmark contains 12,504 formal specifications, with 3,029 in Dafny, 2,334 in\nVerus/Rust and 7,141 in Lean. Of these, 6,174 are new unseen problems. We find\nvericoding success rates of 27% in Lean, 44% in Verus/Rust and 82% in Dafny\nusing off-the-shelf LLMs. Adding natural-language descriptions does not\nsignificantly improve performance. We also find that LLM progress has improved\nprogress on pure Dafny verification from 68% to 96% over the past year. The\nbenchmark and vericoding results are shared at\nhttps://github.com/Beneficial-AI-Foundation/vericoding-benchmark"}
{"id": "2509.22980", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2509.22980", "abs": "https://arxiv.org/abs/2509.22980", "authors": ["Jingyao Zhang", "Elaheh Sadredini"], "title": "\\textit{No One-Size-Fits-All}: A Workload-Driven Characterization of Bit-Parallel vs. Bit-Serial Data Layouts for Processing-using-Memory", "comment": null, "summary": "Processing-in-Memory (PIM) is a promising approach to overcoming the\nmemory-wall bottleneck. However, the PIM community has largely treated its two\nfundamental data layouts, Bit-Parallel (BP) and Bit-Serial (BS), as if they\nwere interchangeable. This implicit \"one-layout-fits-all\" assumption, often\nhard-coded into existing evaluation frameworks, creates a critical gap:\narchitects lack systematic, workload-driven guidelines for choosing the optimal\ndata layout for their target applications.\n  To address this gap, this paper presents the first systematic,\nworkload-driven characterization of BP and BS PIM architectures. We develop\niso-area, cycle-accurate BP and BS PIM architectural models and conduct a\ncomprehensive evaluation using a diverse set of benchmarks. Our suite includes\nboth fine-grained microworkloads from MIMDRAM to isolate specific operational\ncharacteristics, and large-scale applications from the PIMBench suite, such as\nthe VGG network, to represent realistic end-to-end workloads.\n  Our results quantitatively demonstrate that no single layout is universally\nsuperior; the optimal choice is strongly dependent on workload characteristics.\nBP excels on control-flow-intensive tasks with irregular memory access\npatterns, whereas BS shows substantial advantages in massively parallel,\nlow-precision (e.g., INT4/INT8) computations common in AI. Based on this\ncharacterization, we distill a set of actionable design guidelines for\narchitects. This work challenges the prevailing one-size-fits-all view on PIM\ndata layouts and provides a principled foundation for designing\nnext-generation, workload-aware, and potentially hybrid PIM systems."}
{"id": "2509.23218", "categories": ["cs.NI", "cs.IT", "cs.NA", "cs.PF", "cs.SY", "eess.SY", "math.IT", "math.NA", "68M10, 68M20, 60J20", "C.2.1; C.2.5; C.4"], "pdf": "https://arxiv.org/pdf/2509.23218", "abs": "https://arxiv.org/abs/2509.23218", "authors": ["Po-Heng Chou", "Yen-Ting Liu", "Wei-Chang Chen", "Walid Saad"], "title": "Markov Modeling for Licensed and Unlicensed Band Allocation in Underlay and Overlay D2D", "comment": "10 pages, 5 figures, published in 2024 IEEE ICC", "summary": "In this paper, a novel analytical model for resource allocation is proposed\nfor a device-to-device (D2D) assisted cellular network. The proposed model can\nbe applied to underlay and overlay D2D systems for sharing licensed bands and\noffloading cellular traffic. The developed model also takes into account the\nproblem of unlicensed band sharing with Wi-Fi systems. In the proposed model, a\nglobal system state reflects the interaction among D2D, conventional cellular,\nand Wi-Fi packets. Under the standard traffic model assumptions, a\nthreshold-based flow control is proposed for guaranteeing the\nquality-of-service (QoS) of Wi-Fi. The packet blockage probability is then\nderived. Simulation results show the proposed scheme sacrifices conventional\ncellular performance slightly to improve overlay D2D performance significantly\nwhile maintaining the performance for Wi-Fi users. Meanwhile, the proposed\nscheme has more flexible adjustments between D2D and Wi-Fi than the underlay\nscheme."}
{"id": "2509.23693", "categories": ["cs.AR", "cs.OS"], "pdf": "https://arxiv.org/pdf/2509.23693", "abs": "https://arxiv.org/abs/2509.23693", "authors": ["Tao Lu", "Jiapin Wang", "Yelin Shan", "Xiangping Zhang", "Xiang Chen"], "title": "ASIC-based Compression Accelerators for Storage Systems: Design, Placement, and Profiling Insights", "comment": "16 pages", "summary": "Lossless compression imposes significant computational over head on\ndatacenters when performed on CPUs. Hardware compression and decompression\nprocessing units (CDPUs) can alleviate this overhead, but optimal algorithm\nselection, microarchitectural design, and system-level placement of CDPUs are\nstill not well understood. We present the design of an ASIC-based in-storage\nCDPU and provide a comprehensive end-to-end evaluation against two leading ASIC\naccelerators, Intel QAT 8970 and QAT 4xxx. The evaluation spans three dominant\nCDPU placement regimes: peripheral, on-chip, and in-storage. Our results\nreveal: (i) acute sensitivity of throughput and latency to CDPU placement and\ninterconnection, (ii) strong correlation between compression efficiency and\ndata patterns/layouts, (iii) placement-driven divergences between\nmicrobenchmark gains and real-application speedups, (iv) discrepancies between\nmodule and system-level power efficiency, and (v) scalability and multi-tenant\ninterference is sues of various CDPUs. These findings motivate a\nplacement-aware, cross-layer rethinking of hardware (de)compression for\nhyperscale storage infrastructures."}
{"id": "2509.25090", "categories": ["cs.PF"], "pdf": "https://arxiv.org/pdf/2509.25090", "abs": "https://arxiv.org/abs/2509.25090", "authors": ["Rohan Basu Roy", "Vijay Gadepally", "Devesh Tiwari"], "title": "DarwinGame: Playing Tournaments for Tuning Applications in Noisy Cloud Environments", "comment": null, "summary": "This work introduces a new subarea of performance tuning -- performance\ntuning in a shared interference-prone computing environment. We demonstrate\nthat existing tuners are significantly suboptimal by design because of their\ninability to account for interference during tuning. Our solution, DarwinGame,\nemploys a tournament-based design to systematically compare application\nexecutions with different tunable parameter configurations, enabling it to\nidentify the relative performance of different tunable parameter configurations\nin a noisy environment. Compared to existing solutions, DarwinGame achieves\nmore than 27% reduction in execution time, with less than 0.5% performance\nvariability. DarwinGame is the first performance tuner that will help\ndevelopers tune their applications in shared, interference-prone, cloud\nenvironments."}
{"id": "2509.23425", "categories": ["cs.MA"], "pdf": "https://arxiv.org/pdf/2509.23425", "abs": "https://arxiv.org/abs/2509.23425", "authors": ["Benjamin Alcorn", "Eman Hammad"], "title": "Situational Awareness for Safe and Robust Multi-Agent Interactions Under Uncertainty", "comment": null, "summary": "Multi-agent systems are prevalent in a wide range of domains including power\nsystems, vehicular networks, and robotics. Two important problems to solve in\nthese types of systems are how the intentions of non-coordinating agents can be\ndetermined to predict future behavior and how the agents can achieve their\nobjectives under resource constraints without significantly sacrificing\nperformance. To study this, we develop a model where an autonomous agent\nobserves the environment within a safety radius of observation, determines the\nstate of a surrounding agent of interest (within the observation radius),\nestimates future actions to be taken, and acts in an optimal way. In the\nabsence of observations, agents are able to utilize an estimation algorithm to\npredict the future actions of other agents based on historical trajectory. The\nuse of the proposed estimation algorithm introduces uncertainty, which is\nmanaged via risk analysis. The proposed approach in this study is validated\nusing two different learning-based decision making frameworks: reinforcement\nlearning and game theoretic algorithms."}
{"id": "2509.22681", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2509.22681", "abs": "https://arxiv.org/abs/2509.22681", "authors": ["Xianwen Guo", "Bin Huang", "Xiaomeng Wu", "Guanlin Wu", "Fangjian Li", "Shijia Wang", "Qiang Xiao", "Chuanjiang Luo", "Yong Li"], "title": "FLAME: A Serving System Optimized for Large-Scale Generative Recommendation with Efficiency", "comment": null, "summary": "Generative recommendation (GR) models possess greater scaling power compared\nto traditional deep learning recommendation models (DLRMs), yet they also\nimpose a tremendous increase in computational burden. Measured in FLOPs, a\ntypical GR model's workload sits in $10^9 \\sim 10^{11}$ range, roughly four\norders of magnitude higher than traditional DLRMs. Delivering accurate results\nin a few tens of milliseconds while processing billions of such requests per\nday puts extreme demands on the performance of the online serving system.\nTherefore, for industry practitioners, the alluring gains of GR models are\ntempered by the formidable challenge of online deployment at scale in\nproduction services. In this work, we introduce a comprehensive solution of\nonline serving system tailored For Large-scale GenerAtive RecoMmendation with\nEfficiency (FLAME). Specifically, we leveraging CPU-GPU heterogeneous hardware\nto decouple feature pre-processing and model computation. We encapsulated\nseveral memory optimization features as the Proximal Data Accelerator (PDA)\nmodule to make full use of limited bandwidth and storage resources, which\nachieves a 1.9x throughput gain and a 1.7x latency reduction. We implement the\nFused Kernel Engine (FKE) module based on the functionality and interface of\nNVIDIA TensorRT to boost model computation, delivering a speedup ratio of\n4.6x-6.1x, throughput gain ratio of 4.7x-6.3x one step further. In addition, we\ndesign the Dynamic Stream Orchestrator (DSO) module to coordinate concurrent\nrequests, enhancing the system throughput performance with 1.3x improvement in\nthroughput and 2.3x speed-up under non-uniform distribution of upstream\ncandidates. Comprehensive evaluations demonstrate that our FLAME effectively\nsupports large-scale online deployment of GR models and achieves remarkable\nimprovements in system performance."}
{"id": "2509.22978", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.22978", "abs": "https://arxiv.org/abs/2509.22978", "authors": ["Teeradaj Racharak", "Chaiyong Ragkhitwetsagul", "Chayanee Junplong", "Akara Supratak"], "title": "Towards Human-interpretable Explanation in Code Clone Detection using LLM-based Post Hoc Explainer", "comment": null, "summary": "Recent studies highlight various machine learning (ML)-based techniques for\ncode clone detection, which can be integrated into developer tools such as\nstatic code analysis. With the advancements brought by ML in code\nunderstanding, ML-based code clone detectors could accurately identify and\nclassify cloned pairs, especially semantic clones, but often operate as black\nboxes, providing little insight into the decision-making process. Post hoc\nexplainers, on the other hand, aim to interpret and explain the predictions of\nthese ML models after they are made, offering a way to understand the\nunderlying mechanisms driving the model's decisions. However, current post hoc\ntechniques require white-box access to the ML model or are computationally\nexpensive, indicating a need for advanced post hoc explainers. In this paper,\nwe propose a novel approach that leverages the in-context learning capabilities\nof large language models to elucidate the predictions made by the ML-based code\nclone detectors. We perform a study using ChatGPT-4 to explain the code clone\nresults inferred by GraphCodeBERT. We found that our approach is promising as a\npost hoc explainer by giving the correct explanations up to 98% and offering\ngood explanations 95% of the time. However, the explanations and the code line\nexamples given by the LLM are useful in some cases. We also found that lowering\nthe temperature to zero helps increase the accuracy of the explanation. Lastly,\nwe list the insights that can lead to further improvements in future work. This\nstudy paves the way for future studies in using LLMs as a post hoc explainer\nfor various software engineering tasks."}
{"id": "2509.22999", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2509.22999", "abs": "https://arxiv.org/abs/2509.22999", "authors": ["Sachin Sachdeva", "Jincong Lu", "Wantong Li", "Sheldon X. -D. Tan"], "title": "Enhanced Hybrid Temporal Computing Using Deterministic Summations for Ultra-Low-Power Accelerators", "comment": "8 pages", "summary": "This paper presents an accuracy-enhanced Hybrid Temporal Computing (E-HTC)\nframework for ultra-low-power hardware accelerators with deterministic\nadditions. Inspired by the recently proposed HTC architecture, which leverages\npulse-rate and temporal data encoding to reduce switching activity and energy\nconsumption but loses accuracy due to its multiplexer (MUX)-based scaled\naddition, we propose two bitstream addition schemes: (1) an Exact\nMultiple-input Binary Accumulator (EMBA), which performs precise binary\naccumulation, and (2) a Deterministic Threshold-based Scaled Adder (DTSA),\nwhich employs threshold logic for scaled addition. These adders are integrated\ninto a multiplier accumulator (MAC) unit supporting both unipolar and bipolar\nencodings. To validate the framework, we implement two accelerators: a Finite\nImpulse Response (FIR) filter and an 8-point Discrete Cosine Transform\n(DCT)/iDCT engine. Results on a 4x4 MAC show that, in unipolar mode, E-HTC\nmatches the RMSE of state-of-the-art Counter-Based Stochastic Computing (CBSC)\nMAC, improves accuracy by 94% over MUX-based HTC, and reduces power and area by\n23% and 7% compared to MUX-based HTC and 64% and 74% compared to CBSC. In\nbipolar mode, E-HTC MAC achieves 2.09% RMSE -- an 83% improvement over\nMUX-based HTC -- and approaches CBSC's 1.40% RMSE with area and power savings\nof 28% and 43% vs. MUX-based HTC and about 76% vs. CBSC. In FIR experiments,\nboth E-HTC variants yield PSNR gains of 3--5 dB (30--45% RMSE reduction) while\nsaving 13% power and 3% area. For DCT/iDCT, E-HTC boosts PSNR by 10--13 dB\n(70--75% RMSE reduction) while saving area and power over both MUX- and\nCBSC-based designs."}
{"id": "2509.23389", "categories": ["cs.NI"], "pdf": "https://arxiv.org/pdf/2509.23389", "abs": "https://arxiv.org/abs/2509.23389", "authors": ["Tuğçe Bilen", "Mehmet Ozdem"], "title": "A Modular KDN-Based Framework for IT/OT Autonomy in Industrial Systems", "comment": null, "summary": "The convergence of Information Technology (IT) and Operational Technology\n(OT) is a critical enabler for achieving autonomous and intelligent industrial\nsystems. However, the increasing complexity, heterogeneity, and real-time\ndemands of industrial environments render traditional rule-based or static\nmanagement approaches insufficient. In this paper, we present a modular\nframework based on the Knowledge-Defined Networking (KDN) paradigm, enabling\nadaptive and autonomous control across IT-OT infrastructures. The proposed\narchitecture is composed of four core modules: Telemetry Collector, Knowledge\nBuilder, Decision Engine, and Control Enforcer. These modules operate in a\nclosed control loop to continuously observe system behavior, extract contextual\nknowledge, evaluate control actions, and apply policy decisions across\nprogrammable industrial endpoints. A graph-based abstraction is used to\nrepresent system state, and a utility-optimization mechanism guides control\ndecisions under dynamic conditions. The framework's performance is evaluated\nusing three key metrics: decision latency, control effectiveness, and system\nstability, demonstrating its capability to enhance resilience, responsiveness,\nand operational efficiency in smart industrial networks."}
{"id": "2509.22684", "categories": ["cs.DC", "cs.AR", "cs.CR", "cs.PF"], "pdf": "https://arxiv.org/pdf/2509.22684", "abs": "https://arxiv.org/abs/2509.22684", "authors": ["Tarunesh Verma", "Yichao Yuan", "Nishil Talati", "Todd Austin"], "title": "ZKProphet: Understanding Performance of Zero-Knowledge Proofs on GPUs", "comment": "To appear at 2025 IEEE International Symposium on Workload\n  Characterization", "summary": "Zero-Knowledge Proofs (ZKP) are protocols which construct cryptographic\nproofs to demonstrate knowledge of a secret input in a computation without\nrevealing any information about the secret. ZKPs enable novel applications in\nprivate and verifiable computing such as anonymized cryptocurrencies and\nblockchain scaling and have seen adoption in several real-world systems. Prior\nwork has accelerated ZKPs on GPUs by leveraging the inherent parallelism in\ncore computation kernels like Multi-Scalar Multiplication (MSM). However, we\nfind that a systematic characterization of execution bottlenecks in ZKPs, as\nwell as their scalability on modern GPU architectures, is missing in the\nliterature. This paper presents ZKProphet, a comprehensive performance study of\nZero-Knowledge Proofs on GPUs. Following massive speedups of MSM, we find that\nZKPs are bottlenecked by kernels like Number-Theoretic Transform (NTT), as they\naccount for up to 90% of the proof generation latency on GPUs when paired with\noptimized MSM implementations. Available NTT implementations under-utilize GPU\ncompute resources and often do not employ architectural features like\nasynchronous compute and memory operations. We observe that the arithmetic\noperations underlying ZKPs execute exclusively on the GPU's 32-bit integer\npipeline and exhibit limited instruction-level parallelism due to data\ndependencies. Their performance is thus limited by the available integer\ncompute units. While one way to scale the performance of ZKPs is adding more\ncompute units, we discuss how runtime parameter tuning for optimizations like\nprecomputed inputs and alternative data representations can extract additional\nspeedup. With this work, we provide the ZKP community a roadmap to scale\nperformance on GPUs and construct definitive GPU-accelerated ZKPs for their\napplication requirements and available hardware resources."}
{"id": "2509.24046", "categories": ["cs.MA", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.24046", "abs": "https://arxiv.org/abs/2509.24046", "authors": ["Lingyao Li", "Haolun Wu", "Zhenkun Li", "Jiabei Hu", "Yu Wang", "Xiaoshan Huang", "Wenyue Hua", "Wenqian Wang"], "title": "PartnerMAS: An LLM Hierarchical Multi-Agent Framework for Business Partner Selection on High-Dimensional Features", "comment": null, "summary": "High-dimensional decision-making tasks, such as business partner selection,\ninvolve evaluating large candidate pools with heterogeneous numerical,\ncategorical, and textual features. While large language models (LLMs) offer\nstrong in-context reasoning capabilities, single-agent or debate-style systems\noften struggle with scalability and consistency in such settings. We propose\nPartnerMAS, a hierarchical multi-agent framework that decomposes evaluation\ninto three layers: a Planner Agent that designs strategies, Specialized Agents\nthat perform role-specific assessments, and a Supervisor Agent that integrates\ntheir outputs. To support systematic evaluation, we also introduce a curated\nbenchmark dataset of venture capital co-investments, featuring diverse firm\nattributes and ground-truth syndicates. Across 140 cases, PartnerMAS\nconsistently outperforms single-agent and debate-based multi-agent baselines,\nachieving up to 10--15\\% higher match rates. Analysis of agent reasoning shows\nthat planners are most responsive to domain-informed prompts, specialists\nproduce complementary feature coverage, and supervisors play an important role\nin aggregation. Our findings demonstrate that structured collaboration among\nLLM agents can generate more robust outcomes than scaling individual models,\nhighlighting PartnerMAS as a promising framework for high-dimensional\ndecision-making in data-rich domains."}
{"id": "2509.22684", "categories": ["cs.DC", "cs.AR", "cs.CR", "cs.PF"], "pdf": "https://arxiv.org/pdf/2509.22684", "abs": "https://arxiv.org/abs/2509.22684", "authors": ["Tarunesh Verma", "Yichao Yuan", "Nishil Talati", "Todd Austin"], "title": "ZKProphet: Understanding Performance of Zero-Knowledge Proofs on GPUs", "comment": "To appear at 2025 IEEE International Symposium on Workload\n  Characterization", "summary": "Zero-Knowledge Proofs (ZKP) are protocols which construct cryptographic\nproofs to demonstrate knowledge of a secret input in a computation without\nrevealing any information about the secret. ZKPs enable novel applications in\nprivate and verifiable computing such as anonymized cryptocurrencies and\nblockchain scaling and have seen adoption in several real-world systems. Prior\nwork has accelerated ZKPs on GPUs by leveraging the inherent parallelism in\ncore computation kernels like Multi-Scalar Multiplication (MSM). However, we\nfind that a systematic characterization of execution bottlenecks in ZKPs, as\nwell as their scalability on modern GPU architectures, is missing in the\nliterature. This paper presents ZKProphet, a comprehensive performance study of\nZero-Knowledge Proofs on GPUs. Following massive speedups of MSM, we find that\nZKPs are bottlenecked by kernels like Number-Theoretic Transform (NTT), as they\naccount for up to 90% of the proof generation latency on GPUs when paired with\noptimized MSM implementations. Available NTT implementations under-utilize GPU\ncompute resources and often do not employ architectural features like\nasynchronous compute and memory operations. We observe that the arithmetic\noperations underlying ZKPs execute exclusively on the GPU's 32-bit integer\npipeline and exhibit limited instruction-level parallelism due to data\ndependencies. Their performance is thus limited by the available integer\ncompute units. While one way to scale the performance of ZKPs is adding more\ncompute units, we discuss how runtime parameter tuning for optimizations like\nprecomputed inputs and alternative data representations can extract additional\nspeedup. With this work, we provide the ZKP community a roadmap to scale\nperformance on GPUs and construct definitive GPU-accelerated ZKPs for their\napplication requirements and available hardware resources."}
{"id": "2509.23261", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.23261", "abs": "https://arxiv.org/abs/2509.23261", "authors": ["Fei Gu", "Zi Liang", "Hongzong LI", "Jiahao MA"], "title": "The Matthew Effect of AI Programming Assistants: A Hidden Bias in Software Evolution", "comment": null, "summary": "AI-assisted programming is rapidly reshaping software development, with large\nlanguage models (LLMs) enabling new paradigms such as vibe coding and agentic\ncoding. While prior works have focused on prompt design and code generation\nquality, the broader impact of LLM-driven development on the iterative dynamics\nof software engineering remains underexplored. In this paper, we conduct\nlarge-scale experiments on thousands of algorithmic programming tasks and\nhundreds of framework selection tasks to systematically investigate how\nAI-assisted programming interacts with the software ecosystem. Our analysis\nreveals \\textbf{a striking Matthew effect: the more popular a programming\nlanguage or framework, the higher the success rate of LLM-generated code}. The\nphenomenon suggests that AI systems may reinforce existing popularity\nhierarchies, accelerating convergence around dominant tools while hindering\ndiversity and innovation. We provide a quantitative characterization of this\neffect and discuss its implications for the future evolution of programming\necosystems."}
{"id": "2509.23179", "categories": ["cs.AR", "cs.CR"], "pdf": "https://arxiv.org/pdf/2509.23179", "abs": "https://arxiv.org/abs/2509.23179", "authors": ["Jingyao Zhang", "Elaheh Sadredini"], "title": "A Near-Cache Architectural Framework for Cryptographic Computing", "comment": null, "summary": "Recent advancements in post-quantum cryptographic algorithms have led to\ntheir standardization by the National Institute of Standards and Technology\n(NIST) to safeguard information security in the post-quantum era. These\nalgorithms, however, employ public keys and signatures that are 3 to 9$\\times$\nlonger than those used in pre-quantum cryptography, resulting in significant\nperformance and energy efficiency overheads. A critical bottleneck identified\nin our analysis is the cache bandwidth. This limitation motivates the adoption\nof on-chip in-/near-cache computing, a computing paradigm that offers\nhigh-performance, exceptional energy efficiency, and flexibility to accelerate\npost-quantum cryptographic algorithms. Our analysis of existing works reveals\nchallenges in integrating in-/near-cache computing into modern computer systems\nand performance limitations due to external bandwidth limitation, highlighting\nthe need for innovative solutions that can seamlessly integrate into existing\nsystems without performance and energy efficiency issues. In this paper, we\nintroduce a near-cache-slice computing paradigm with support of customization\nand virtual address, named Crypto-Near-Cache (CNC), designed to accelerate\npost-quantum cryptographic algorithms and other applications. By placing SRAM\narrays with bitline computing capability near cache slices, high internal\nbandwidth and short data movement are achieved with native support of virtual\naddressing. An ISA extension to facilitate CNC is also proposed, with detailed\ndiscussion on the implementation aspects of the core/cache datapath."}
{"id": "2509.23398", "categories": ["cs.NI"], "pdf": "https://arxiv.org/pdf/2509.23398", "abs": "https://arxiv.org/abs/2509.23398", "authors": ["Tuğçe Bilen", "Mehmet Özdem"], "title": "Knowledge-Defined and Twin-Assisted Network Management for 6G", "comment": null, "summary": "The increasing complexity, dynamism, and heterogeneity of 6G networks demand\nmanagement systems that can reason proactively and generalize beyond\npre-defined cases. In this paper, we propose a modular, knowledge-defined\narchitecture that integrates Digital Twin models with semantic reasoning and\nzero-shot learning to enable autonomous decision-making for previously unseen\nnetwork scenarios. Real-time data streams are used to maintain synchronized\nvirtual replicas of the physical network, which also forecast short-term state\ntransitions. These predictions feed into a knowledge plane that constructs and\nupdates a graph-based abstraction of the network, enabling context-aware intent\ngeneration via graph neural reasoning. To ensure adaptability without\nretraining, the management plane performs zero-shot policy matching by\nsemantically embedding candidate intents and selecting suitable pre-learned\nactions. The selected decisions are translated and enforced through the control\nplane, while a closed-loop feedback mechanism continuously refines predictions,\nknowledge, and policies over time. Simulation results confirm that the proposed\nframework observes notable improvements in policy response time, SLA compliance\nrate, and intent matching accuracy."}
{"id": "2509.23216", "categories": ["cs.NI", "cs.IT", "cs.NA", "cs.PF", "cs.SY", "eess.SY", "math.IT", "math.NA", "68M10, 68M20, 60J20", "C.2.1; C.2.5; C.4"], "pdf": "https://arxiv.org/pdf/2509.23216", "abs": "https://arxiv.org/abs/2509.23216", "authors": ["Po-Heng Chou"], "title": "Unlicensed Band Allocation for Heterogeneous Networks", "comment": "14 pages, 12 figures, 1 table, published in IEICE Transactions on\n  Communications", "summary": "Based on the License-Assisted Access (LAA) small cell architecture, the LAA\ncoexisting with Wi-Fi heterogeneous networks provides LTE mobile users with\nhigh bandwidth efficiency as the unlicensed channels are shared among LAA and\nWi-Fi. However, LAA and Wi-Fi interfere with each other when both systems use\nthe same unlicensed channel in heterogeneous networks. In such a network,\nunlicensed band allocation for LAA and Wi-Fi is an important issue that may\naffect the quality of service (QoS) of both systems significantly. In this\npaper, we propose an analytical model and conduct simulation experiments to\nstudy four allocations for the unlicensed band: unlicensed full allocation\n(UFA), unlicensed time-division allocation (UTA), and UFA/UTA with buffering\nmechanism (UFAB and UTAB) for the LAA data packets. We evaluate the performance\nof these unlicensed band allocation schemes in terms of the acceptance rate of\nboth LAA and Wi-Fi packet data in the LAA buffer queue. Our study provides\nguidelines for designing the channel occupation phase and the buffer size of\nthe LAA small cell."}
{"id": "2509.24088", "categories": ["cs.MA"], "pdf": "https://arxiv.org/pdf/2509.24088", "abs": "https://arxiv.org/abs/2509.24088", "authors": ["Yifan Yu", "Moyan Li", "Shaoyuan Xu", "Jinmiao Fu", "Xinhai Hou", "Fan Lai", "Bryan Wang"], "title": "CORRECT: COndensed eRror RECognition via knowledge Transfer in multi-agent systems", "comment": null, "summary": "Multi-agent systems (MAS) are increasingly capable of tackling complex\nreal-world tasks, yet their reliance on inter-agent coordination, tool use, and\nlong-horizon reasoning makes error recognition particularly challenging. Minor\nerrors can propagate across agents, escalating into task failures while\nproducing long, intertwined execution trajectories that impose significant\ncosts for both human developers and automated systems to debug and analyze. Our\nkey insight is that, despite surface differences in failure trajectories (e.g.,\nlogs), MAS errors often recur with similar structural patterns. This paper\npresents CORRECT, the first lightweight, training-free framework that leverages\nan online cache of distilled error schemata to recognize and transfer knowledge\nof failure structures across new requests. This cache-based reuse allows LLMs\nto perform targeted error localization at inference time, avoiding the need for\nexpensive retraining while adapting to dynamic MAS deployments in subseconds.\nTo support rigorous study in this domain, we also introduce CORRECT-Error, a\nlarge-scale dataset of over 2,000 annotated trajectories collected through a\nnovel error-injection pipeline guided by real-world distributions, and further\nvalidated through human evaluation to ensure alignment with natural failure\npatterns. Experiments across seven diverse MAS applications show that CORRECT\nimproves step-level error localization up to 19.8% over existing advances while\nat near-zero overhead, substantially narrowing the gap between automated and\nhuman-level error recognition."}
{"id": "2509.22701", "categories": ["cs.DC", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.22701", "abs": "https://arxiv.org/abs/2509.22701", "authors": ["Leszek Sliwko", "Jolanta Mizera-Pietraszko"], "title": "Enhancing Cluster Scheduling in HPC: A Continuous Transfer Learning for Real-Time Optimization", "comment": "This is the accepted version of the paper published in 2025 IEEE\n  International Parallel and Distributed Processing Symposium Workshops\n  (IPDPSW). The final version is available at:\n  https://doi.org/10.1109/IPDPSW66978.2025.00056", "summary": "This study presents a machine learning-assisted approach to optimize task\nscheduling in cluster systems, focusing on node-affinity constraints.\nTraditional schedulers like Kubernetes struggle with real-time adaptability,\nwhereas the proposed continuous transfer learning model evolves dynamically\nduring operations, minimizing retraining needs. Evaluated on Google Cluster\nData, the model achieves over 99% accuracy, reducing computational overhead and\nimproving scheduling latency for constrained tasks. This scalable solution\nenables real-time optimization, advancing machine learning integration in\ncluster management and paving the way for future adaptive scheduling\nstrategies."}
{"id": "2509.23297", "categories": ["cs.SE", "cs.GR", "cs.HC"], "pdf": "https://arxiv.org/pdf/2509.23297", "abs": "https://arxiv.org/abs/2509.23297", "authors": ["Anthony Savidis", "Christos Vasilopoulos"], "title": "Code Arcades: 3d Visualization of Classes, Dependencies and Software Metrics", "comment": null, "summary": "Software visualization seeks to represent software artifacts graphical-ly in\ntwo or three dimensions, with the goal of enhancing comprehension, anal-ysis,\nmaintenance, and evolution of the source code. In this context, visualiza-tions\nemploy graphical forms such as dependency structures, treemaps, or time-lines\nthat incorporate repository histories. These visualizations allow software\nengineers to identify structural patterns, detect complexity hotspots, and\ninfer system behaviors that are difficult to perceive directly from source\ntext. By adopting metaphor-based approaches, visualization tools provide\nmacroscopic overviews while enabling focused inspection of specific program\nelements, thus offering an accessible means of understanding large-scale\nsystems. The contri-bution of our work lies in three areas. First, we introduce\na configurable group-ing mechanism that supports flexible organization of code\nelements based on arbitrary relationships. Second, we combine fine-grained and\ncoarse-grained software metrics to provide a multi-level perspective on system\nproperties. Third, we present an interactive visualization engine that allows\ndevelopers to dynamically adjust rendering attributes. Collectively, these\nadvances provide a more adaptable and insightful approach to source code\ncomprehension."}
{"id": "2509.23674", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2509.23674", "abs": "https://arxiv.org/abs/2509.23674", "authors": ["Hongqin Lyu", "Yonghao Wang", "Yunlin Du", "Mingyu Shi", "Zhiteng Chao", "Wenxing Li", "Tiancheng Wang", "Huawei Li"], "title": "AssertGen: Enhancement of LLM-aided Assertion Generation through Cross-Layer Signal Bridging", "comment": "6 pages, 7 figures", "summary": "Assertion-based verification (ABV) serves as a crucial technique for ensuring\nthat register-transfer level (RTL) designs adhere to their specifications.\nWhile Large Language Model (LLM) aided assertion generation approaches have\nrecently achieved remarkable progress, existing methods are still unable to\neffectively identify the relationship between design specifications and RTL\ndesigns, which leads to the insufficiency of the generated assertions. To\naddress this issue, we propose AssertGen, an assertion generation framework\nthat automatically generates SystemVerilog assertions (SVA). AssertGen first\nextracts verification objectives from specifications using a chain-of-thought\n(CoT) reasoning strategy, then bridges corresponding signals between these\nobjectives and the RTL code to construct a cross-layer signal chain, and\nfinally generates SVAs based on the LLM. Experimental results demonstrate that\nAssertGen outperforms the existing state-of-the-art methods across several key\nmetrics, such as pass rate of formal property verification (FPV), cone of\ninfluence (COI), proof core and mutation testing coverage."}
{"id": "2509.23401", "categories": ["cs.NI"], "pdf": "https://arxiv.org/pdf/2509.23401", "abs": "https://arxiv.org/abs/2509.23401", "authors": ["Mustafa Yavuz Engin", "Mehmet Ozdem", "Tuğçe Bilen"], "title": "AUV-Assisted Underwater 6G: Environmental Modeling and Multi-Stage Optimization", "comment": null, "summary": "This study presents a simulation model for underwater 6G networks, focusing\non the optimized placement of sensors, AUVs, and hubs. The network architecture\nconsists of fixed hub stations, mobile autonomous underwater vehicles (AUVs),\nand numerous sensor nodes. Environmental parameters such as temperature,\nsalinity, and conductivity are considered in the transmission of\nelectromagnetic signals; signal attenuation and transmission delays are\ncalculated based on physical models. The optimization process begins with\nK-Means clustering, followed by sequential application of Genetic Algorithm\n(GA) and Particle Swarm Optimization (PSO) to refine the cluster\nconfigurations. The simulation includes key network dynamics such as multi-hop\ndata transmission, cluster leader selection, queue management, and traffic load\nbalancing. To compare performance, two distinct scenarios -- one with cluster\nleaders and one without -- are modeled and visualized through a PyQt5-based\nreal-time graphical interface. The results demonstrate that 6G network\narchitectures in underwater environments can be effectively modeled and\noptimized by incorporating environmental conditions."}
{"id": "2509.23217", "categories": ["cs.NI", "cs.IT", "cs.NA", "cs.PF", "cs.SY", "eess.SY", "math.IT", "math.NA", "68M10, 68M20, 60J20", "C.2.1; C.2.5; C.4"], "pdf": "https://arxiv.org/pdf/2509.23217", "abs": "https://arxiv.org/abs/2509.23217", "authors": ["Po-Heng Chou"], "title": "Modeling the Unlicensed Band Allocation for LAA With Buffering Mechanism", "comment": "5 pages, 3 figures, 2 tables, published in IEEE Communications\n  Letters", "summary": "In this letter, we propose an analytical model and conduct simulation\nexperiments to study listen-before-talk-based unlicensed band allocation with\nthe buffering mechanism for the License-Assisted Access (LAA) packets in the\nheterogeneous networks. In such a network, unlicensed band allocation for LAA\nand Wi-Fi is an important issue, which may affect the quality of service for\nboth systems significantly. We evaluate the performance of these unlicensed\nband allocations in terms of the acceptance rate of both LAA and Wi-Fi packets.\nThis letter provides the guidelines for designing the channel occupation phase\nand buffer threshold of the LAA systems."}
{"id": "2509.24323", "categories": ["cs.MA", "cs.CL"], "pdf": "https://arxiv.org/pdf/2509.24323", "abs": "https://arxiv.org/abs/2509.24323", "authors": ["Kun Wang", "Guibin Zhang", "ManKit Ye", "Xinyu Deng", "Dongxia Wang", "Xiaobin Hu", "Jinyang Guo", "Yang Liu", "Yufei Guo"], "title": "MAS$^2$: Self-Generative, Self-Configuring, Self-Rectifying Multi-Agent Systems", "comment": null, "summary": "The past two years have witnessed the meteoric rise of Large Language Model\n(LLM)-powered multi-agent systems (MAS), which harness collective intelligence\nand exhibit a remarkable trajectory toward self-evolution. This paradigm has\nrapidly progressed from manually engineered systems that require bespoke\nconfiguration of prompts, tools, roles, and communication protocols toward\nframeworks capable of automated orchestration. Yet, dominant automatic\nmulti-agent systems, whether generated by external modules or a single LLM\nagent, largely adhere to a rigid ``\\textit{generate-once-and-deploy}''\nparadigm, rendering the resulting systems brittle and ill-prepared for the\ndynamism and uncertainty of real-world environments. To transcend this\nlimitation, we introduce MAS$^2$, a paradigm predicated on the principle of\nrecursive self-generation: a multi-agent system that autonomously architects\nbespoke multi-agent systems for diverse problems. Technically, we devise a\n``\\textit{generator-implementer-rectifier}'' tri-agent team capable of\ndynamically composing and adaptively rectifying a target agent system in\nresponse to real-time task demands. Collaborative Tree Optimization is proposed\nto train and specialize these meta-agents. Extensive evaluation across seven\nbenchmarks reveals that MAS$^2$ achieves performance gains of up to $19.6\\%$\nover state-of-the-art MAS in complex scenarios such as deep research and code\ngeneration. Moreover, MAS$^2$ exhibits superior cross-backbone generalization,\neffectively leveraging previously unseen LLMs to yield improvements of up to\n$15.1\\%$. Crucially, these gains are attained without incurring excessive token\ncosts, as MAS$^2$ consistently resides on the Pareto frontier of\ncost-performance trade-offs. The source codes are available at\nhttps://github.com/yeyeyeah2/MAS2."}
{"id": "2509.22704", "categories": ["cs.DC", "cs.AI", "cs.SE"], "pdf": "https://arxiv.org/pdf/2509.22704", "abs": "https://arxiv.org/abs/2509.22704", "authors": ["Leszek Sliwko"], "title": "Intelligent Load Balancing in Cloud Computer Systems", "comment": "A thesis submitted in partial fulfilment of the requirements of the\n  University of Westminster for the degree of Doctor of Philosophy", "summary": "Cloud computing is an established technology allowing users to share\nresources on a large scale, never before seen in IT history. A cloud system\nconnects multiple individual servers in order to process related tasks in\nseveral environments at the same time. Clouds are typically more cost-effective\nthan single computers of comparable computing performance. The sheer physical\nsize of the system itself means that thousands of machines may be involved. The\nfocus of this research was to design a strategy to dynamically allocate tasks\nwithout overloading Cloud nodes which would result in system stability being\nmaintained at minimum cost. This research has added the following new\ncontributions to the state of knowledge: (i) a novel taxonomy and\ncategorisation of three classes of schedulers, namely OS-level, Cluster and Big\nData, which highlight their unique evolution and underline their different\nobjectives; (ii) an abstract model of cloud resources utilisation is specified,\nincluding multiple types of resources and consideration of task migration\ncosts; (iii) a virtual machine live migration was experimented with in order to\ncreate a formula which estimates the network traffic generated by this process;\n(iv) a high-fidelity Cloud workload simulator, based on a month-long workload\ntraces from Google's computing cells, was created; (v) two possible approaches\nto resource management were proposed and examined in the practical part of the\nmanuscript: the centralised metaheuristic load balancer and the decentralised\nagent-based system. The project involved extensive experiments run on the\nUniversity of Westminster HPC cluster, and the promising results are presented\ntogether with detailed discussions and a conclusion."}
{"id": "2509.23469", "categories": ["cs.SE", "68N30", "D.2.8"], "pdf": "https://arxiv.org/pdf/2509.23469", "abs": "https://arxiv.org/abs/2509.23469", "authors": ["Mykola Kuz", "Ivan Yaremiy", "Hanna Yaremii", "Mykola Pikuliak", "Ihor Lazarovych", "Mykola Kozlenko", "Denys Vekeryk"], "title": "Methods for evaluating software accessibility", "comment": "10 pages, 4 figures, 1 table", "summary": "The development and enhancement of methods for evaluating software\naccessibility is a relevant challenge in modern software engineering, as\nensuring equal access to digital services is a key factor in improving their\nefficiency and inclusivity. The increasing digitalization of society\nnecessitates the creation of software that complies with international\naccessibility standards such as ISO/IEC 25023 and WCAG. Adhering to these\nstandards helps eliminate barriers to software use for individuals with diverse\nphysical, sensory, and cognitive needs. Despite advancements in regulatory\nframeworks, existing accessibility evaluation methodologies are often\ngeneralized and fail to account for the specific needs of different user\ncategories or the unique ways they interact with digital systems. This\nhighlights the need for the development of new, more detailed methods for\ndefining metrics that influence the quality of user interaction with software\nproducts. Building a classification and mathematical model and developing\naccessibility assessment methods for software based on it. A method for\nassessing the quality subcharacteristic \"Accessibility\", which is part of the\n\"Usability\" quality characteristic, has been developed. This enabled the\nanalysis of a website's inclusivity for individuals with visual impairments,\nand the formulation of specific recommendations for further improvements, which\nis a crucial step toward creating an inclusive digital environment. Comparing\nto standardized approaches, a more detailed and practically oriented\naccessibility assessment methodology has been proposed. Using this methodology,\nan analysis of the accessibility of the main pages of Vasyl Stefanyk\nPrecarpathian National University's website was conducted, and improvements\nwere suggested to enhance its inclusivity."}
{"id": "2509.23693", "categories": ["cs.AR", "cs.OS"], "pdf": "https://arxiv.org/pdf/2509.23693", "abs": "https://arxiv.org/abs/2509.23693", "authors": ["Tao Lu", "Jiapin Wang", "Yelin Shan", "Xiangping Zhang", "Xiang Chen"], "title": "ASIC-based Compression Accelerators for Storage Systems: Design, Placement, and Profiling Insights", "comment": "16 pages", "summary": "Lossless compression imposes significant computational over head on\ndatacenters when performed on CPUs. Hardware compression and decompression\nprocessing units (CDPUs) can alleviate this overhead, but optimal algorithm\nselection, microarchitectural design, and system-level placement of CDPUs are\nstill not well understood. We present the design of an ASIC-based in-storage\nCDPU and provide a comprehensive end-to-end evaluation against two leading ASIC\naccelerators, Intel QAT 8970 and QAT 4xxx. The evaluation spans three dominant\nCDPU placement regimes: peripheral, on-chip, and in-storage. Our results\nreveal: (i) acute sensitivity of throughput and latency to CDPU placement and\ninterconnection, (ii) strong correlation between compression efficiency and\ndata patterns/layouts, (iii) placement-driven divergences between\nmicrobenchmark gains and real-application speedups, (iv) discrepancies between\nmodule and system-level power efficiency, and (v) scalability and multi-tenant\ninterference is sues of various CDPUs. These findings motivate a\nplacement-aware, cross-layer rethinking of hardware (de)compression for\nhyperscale storage infrastructures."}
{"id": "2509.23522", "categories": ["cs.NI"], "pdf": "https://arxiv.org/pdf/2509.23522", "abs": "https://arxiv.org/abs/2509.23522", "authors": ["Ehsan Eslami", "Walaa Hamouda"], "title": "Network Traffic Classification Using Self-Supervised Learning and Confident Learning", "comment": null, "summary": "Network traffic classification (NTC) is vital for efficient network\nmanagement, security, and performance optimization, particularly with 5G/6G\ntechnologies. Traditional methods, such as deep packet inspection (DPI) and\nport-based identification, struggle with the rise of encrypted traffic and\ndynamic port allocations. Supervised learning methods provide viable\nalternatives but rely on large labeled datasets, which are difficult to acquire\ngiven the diversity and volume of network traffic. Meanwhile, unsupervised\nlearning methods, while less reliant on labeled data, often exhibit lower\naccuracy. To address these limitations, we propose a novel framework that first\nleverages Self-Supervised Learning (SSL) with techniques such as autoencoders\nor Tabular Contrastive Learning (TabCL) to generate pseudo-labels from\nextensive unlabeled datasets, addressing the challenge of limited labeled data.\nWe then apply traffic-adopted Confident Learning (CL) to refine these\npseudo-labels, enhancing classification precision by mitigating the impact of\nnoise. Our proposed framework offers a generalizable solution that minimizes\nthe need for extensive labeled data while delivering high accuracy. Extensive\nsimulations and evaluations, conducted using three datasets (ISCX VPN-nonVPN,\nself-generated dataset, and UCDavis--QUIC), and demonstrate that our method\nachieves superior accuracy compared to state-of-the-art techniques in\nclassifying network traffic."}
{"id": "2509.23218", "categories": ["cs.NI", "cs.IT", "cs.NA", "cs.PF", "cs.SY", "eess.SY", "math.IT", "math.NA", "68M10, 68M20, 60J20", "C.2.1; C.2.5; C.4"], "pdf": "https://arxiv.org/pdf/2509.23218", "abs": "https://arxiv.org/abs/2509.23218", "authors": ["Po-Heng Chou", "Yen-Ting Liu", "Wei-Chang Chen", "Walid Saad"], "title": "Markov Modeling for Licensed and Unlicensed Band Allocation in Underlay and Overlay D2D", "comment": "10 pages, 5 figures, published in 2024 IEEE ICC", "summary": "In this paper, a novel analytical model for resource allocation is proposed\nfor a device-to-device (D2D) assisted cellular network. The proposed model can\nbe applied to underlay and overlay D2D systems for sharing licensed bands and\noffloading cellular traffic. The developed model also takes into account the\nproblem of unlicensed band sharing with Wi-Fi systems. In the proposed model, a\nglobal system state reflects the interaction among D2D, conventional cellular,\nand Wi-Fi packets. Under the standard traffic model assumptions, a\nthreshold-based flow control is proposed for guaranteeing the\nquality-of-service (QoS) of Wi-Fi. The packet blockage probability is then\nderived. Simulation results show the proposed scheme sacrifices conventional\ncellular performance slightly to improve overlay D2D performance significantly\nwhile maintaining the performance for Wi-Fi users. Meanwhile, the proposed\nscheme has more flexible adjustments between D2D and Wi-Fi than the underlay\nscheme."}
{"id": "2509.25034", "categories": ["cs.MA", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2509.25034", "abs": "https://arxiv.org/abs/2509.25034", "authors": ["Heming Fu", "Guojun Xiong", "Jian Li", "Shan Lin"], "title": "MARLIN: Multi-Agent Reinforcement Learning with Murmuration Intelligence and LLM Guidance for Reservoir Management", "comment": null, "summary": "As climate change intensifies extreme weather events, water disasters pose\ngrowing threats to global communities, making adaptive reservoir management\ncritical for protecting vulnerable populations and ensuring water security.\nModern water resource management faces unprecedented challenges from cascading\nuncertainties propagating through interconnected reservoir networks. These\nuncertainties, rooted in physical water transfer losses and environmental\nvariability, make precise control difficult. For example, sending 10 tons\ndownstream may yield only 8-12 tons due to evaporation and seepage. Traditional\ncentralized optimization approaches suffer from exponential computational\ncomplexity and cannot effectively handle such real-world uncertainties, while\nexisting multi-agent reinforcement learning (MARL) methods fail to achieve\neffective coordination under uncertainty. To address these challenges, we\npresent MARLIN, a decentralized reservoir management framework inspired by\nstarling murmurations intelligence. Integrating bio-inspired alignment,\nseparation, and cohesion rules with MARL, MARLIN enables individual reservoirs\nto make local decisions while achieving emergent global coordination. In\naddition, a LLM provides real-time reward shaping signals, guiding agents to\nadapt to environmental changes and human-defined preferences. Experiments on\nreal-world USGS data show that MARLIN improves uncertainty handling by 23\\%,\ncuts computation by 35\\%, and accelerates flood response by 68\\%, exhibiting\nsuper-linear coordination, with complexity scaling 5.4x from 400 to 10,000\nnodes. These results demonstrate MARLIN's potential for disaster prevention and\nprotecting communities through intelligent, scalable water resource management."}
{"id": "2509.22707", "categories": ["cs.DC", "cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2509.22707", "abs": "https://arxiv.org/abs/2509.22707", "authors": ["Jinqi Yan", "Fang He", "Qianlong Sang", "Bifeng Tong", "Peng Sun", "Yili Gong", "Chuang Hu", "Dazhao Cheng"], "title": "Metadata-Guided Adaptable Frequency Scaling across Heterogeneous Applications and Devices", "comment": null, "summary": "Dynamic Voltage and Frequency Scaling is essential for enhancing energy\nefficiency in mobile platforms. However, traditional heuristic-based governors\nare increasingly inadequate for managing the complexity of heterogeneous\nSystem-on-Chip designs and diverse application workloads. Although\nreinforcement learning approaches offer improved performance, their poor\ngeneralization capability and reliance on extensive retraining for each\nhardware and application combination leads to significant deployment costs. In\nthis work, we observe that device and application metadata inherently\nencapsulate valuable knowledge for DVFS, presenting an opportunity to overcome\nthese limitations. We formulate DVFS for heterogeneous devices and applications\nas a multi-task reinforcement learning problem. We introduce MetaDVFS, which is\na metadata-guided framework that systematically leverages metadata to discover\nand transfer shared knowledge across DVFS tasks. MetaDVFS can output a set of\nDVFS models with significant generalization capability for various applications\nof heterogeneous devices. Evaluations on five Google Pixel devices running six\napplications show that MetaDVFS achieves up to 17% improvement in\nPerformance-Power Ratio and up to 26% improvement in Quality of Experience.\nCompared to state-of-the-art methods, MetaDVFS delivers 70.8% faster adaptation\nand 5.8-27.6% higher performance over standalone device-application specific\ntraining, while avoiding negative transfer effects. These results establish\nMetaDVFS as an effective and scalable solution for DVFS deployment in\nheterogeneous mobile environments."}
{"id": "2509.23586", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.23586", "abs": "https://arxiv.org/abs/2509.23586", "authors": ["Yuan-An Xiao", "Pengfei Gao", "Chao Peng", "Yingfei Xiong"], "title": "Improving the Efficiency of LLM Agent Systems through Trajectory Reduction", "comment": "20 pages, 4 figures", "summary": "Multi-turn agent systems based on Large Language Models (LLMs) have been\nincreasingly popular for software engineering tasks. While LLM agents show\ndecent effectiveness, the high computational cost of input tokens due to the\never-growing trajectory remains an efficiency concern for their applications.\nEfficiency is largely neglected in existing studies and agent products, and\nthis paper fills the gap by introducing an inference-time trajectory reduction\napproach to reduce the cost of agents.\n  Through analyzing existing agent trajectories, we demonstrate that useless,\nredundant, and expired information is widespread in all trajectories, which can\nbe identified and reduced without harming the agent's performance. We then\ndesign a simple yet effective trajectory reduction approach, AgentDiet, which\nautomatically removes such waste information. We implement AgentDiet on a\ntop-performing coding agent, and the evaluation on two LLMs and two benchmarks\nshows that AgentDiet can reduce input tokens by 39.9% ~ 59.7%, or the final\ncomputational cost by 21.1% ~ 35.9%, while maintaining the same agent\nperformance. This indicates that trajectory reduction is a promising direction\nfor agent systems."}
{"id": "2509.23972", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2509.23972", "abs": "https://arxiv.org/abs/2509.23972", "authors": ["Hongqin Lyu", "Yunlin Du", "Yonghao Wang", "Zhiteng Chao", "Tiancheng Wang", "Huawei Li"], "title": "AssertFix: Empowering Automated Assertion Fix via Large Language Models", "comment": "6 pages, 6 figures", "summary": "Assertion-based verification (ABV) is critical in ensuring that\nregister-transfer level (RTL) designs conform to their functional\nspecifications. SystemVerilog Assertions (SVA) effectively specify design\nproperties, but writing and maintaining them manually is challenging and\nerror-prone. Although recent progress of assertion generation methods\nleveraging large language models (LLMs) have shown great potential in improving\nassertion quality, they typically treat assertion generation as a final step,\nleaving the burden of fixing of the incorrect assertions to human effects,\nwhich may significantly limits the application of these methods. To address the\nabove limitation, we propose an automatic assertion fix framework based on\nLLMs, named AssertFix. AsserFix accurately locates the RTL code related to the\nincorrect assertion, systematically identifies the root causes of the assertion\nerrors, classifies the error type and finally applies dedicated fix strategies\nto automatically correct these errors, improving the overall quality of the\ngenerated assertions. Experimental results show that AssertFix achieves\nnoticeable improvements in both fix rate and verification coverage across the\nOpencore benchmarks."}
{"id": "2509.23528", "categories": ["cs.NI"], "pdf": "https://arxiv.org/pdf/2509.23528", "abs": "https://arxiv.org/abs/2509.23528", "authors": ["Russell Ford", "Hao Chen", "Pranav Madadi", "Mandar Kulkarni", "Xiaochuan Ma", "Daoud Burghal", "Guanbo Chen", "Yeqing Hu", "Chance Tarver", "Panagiotis Skrimponis", "Vitali Loseu", "Yu Zhang", "Yan Xin", "Yang Li", "Jianzhong Zhang", "Shubham Khunteta", "Yeswanth Guddeti Reddy", "Ashok Kumar Reddy Chavva", "Mahantesh Kothiwale", "Davide Villa"], "title": "Sim2Field: End-to-End Development of AI RANs for 6G", "comment": null, "summary": "Following state-of-the-art research results, which showed the potential for\nsignificant performance gains by applying AI/ML techniques in the cellular\nRadio Access Network (RAN), the wireless industry is now broadly pushing for\nthe adoption of AI in 5G and future 6G technology. Despite this enthusiasm,\nAI-based wireless systems still remain largely untested in the field. Common\nsimulation methods for generating datasets for AI model training suffer from\n\"reality gap\" and, as a result, the performance of these simulation-trained\nmodels may not carry over to practical cellular systems. Additionally, the cost\nand complexity of developing high-performance proof-of-concept implementations\npresent major hurdles for evaluating AI wireless systems in the field. In this\nwork, we introduce a methodology which aims to address the challenges of\nbringing AI to real networks. We discuss how detailed Digital Twin simulations\nmay be employed for training site-specific AI Physical (PHY) layer functions.\nWe further present a powerful testbed for AI-RAN research and demonstrate how\nit enables rapid prototyping, field testing and data collection. Finally, we\nevaluate an AI channel estimation algorithm over-the-air with a commercial UE,\ndemonstrating that real-world throughput gains of up to 40% are achievable by\nincorporating AI in the physical layer."}
{"id": "2509.24063", "categories": ["cs.DC", "cs.CE", "cs.MA", "cs.PF", "q-bio.QM"], "pdf": "https://arxiv.org/pdf/2509.24063", "abs": "https://arxiv.org/abs/2509.24063", "authors": ["Lukas Breitwieser", "Ahmad Hesam", "Abdullah Giray Yağlıkçı", "Mohammad Sadrosadati", "Fons Rademakers", "Onur Mutlu"], "title": "TeraAgent: A Distributed Agent-Based Simulation Engine for Simulating Half a Trillion Agents", "comment": null, "summary": "Agent-based simulation is an indispensable paradigm for studying complex\nsystems. These systems can comprise billions of agents, requiring the computing\nresources of multiple servers to simulate. Unfortunately, the state-of-the-art\nplatform, BioDynaMo, does not scale out across servers due to its\nshared-memory-based implementation.\n  To overcome this key limitation, we introduce TeraAgent, a distributed\nagent-based simulation engine. A critical challenge in distributed execution is\nthe exchange of agent information across servers, which we identify as a major\nperformance bottleneck. We propose two solutions: 1) a tailored serialization\nmechanism that allows agents to be accessed and mutated directly from the\nreceive buffer, and 2) leveraging the iterative nature of agent-based\nsimulations to reduce data transfer with delta encoding.\n  Built on our solutions, TeraAgent enables extreme-scale simulations with half\na trillion agents (an 84x improvement), reduces time-to-result with additional\ncompute nodes, improves interoperability with third-party tools, and provides\nusers with more hardware flexibility."}
{"id": "2509.24063", "categories": ["cs.DC", "cs.CE", "cs.MA", "cs.PF", "q-bio.QM"], "pdf": "https://arxiv.org/pdf/2509.24063", "abs": "https://arxiv.org/abs/2509.24063", "authors": ["Lukas Breitwieser", "Ahmad Hesam", "Abdullah Giray Yağlıkçı", "Mohammad Sadrosadati", "Fons Rademakers", "Onur Mutlu"], "title": "TeraAgent: A Distributed Agent-Based Simulation Engine for Simulating Half a Trillion Agents", "comment": null, "summary": "Agent-based simulation is an indispensable paradigm for studying complex\nsystems. These systems can comprise billions of agents, requiring the computing\nresources of multiple servers to simulate. Unfortunately, the state-of-the-art\nplatform, BioDynaMo, does not scale out across servers due to its\nshared-memory-based implementation.\n  To overcome this key limitation, we introduce TeraAgent, a distributed\nagent-based simulation engine. A critical challenge in distributed execution is\nthe exchange of agent information across servers, which we identify as a major\nperformance bottleneck. We propose two solutions: 1) a tailored serialization\nmechanism that allows agents to be accessed and mutated directly from the\nreceive buffer, and 2) leveraging the iterative nature of agent-based\nsimulations to reduce data transfer with delta encoding.\n  Built on our solutions, TeraAgent enables extreme-scale simulations with half\na trillion agents (an 84x improvement), reduces time-to-result with additional\ncompute nodes, improves interoperability with third-party tools, and provides\nusers with more hardware flexibility."}
{"id": "2509.22832", "categories": ["cs.DC", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.22832", "abs": "https://arxiv.org/abs/2509.22832", "authors": ["Biyao Zhang", "Mingkai Zheng", "Debargha Ganguly", "Xuecen Zhang", "Vikash Singh", "Vipin Chaudhary", "Zhao Zhang"], "title": "Efficient Fine-Grained GPU Performance Modeling for Distributed Deep Learning of LLM", "comment": null, "summary": "Training Large Language Models(LLMs) is one of the most compute-intensive\ntasks in high-performance computing. Predicting end-to-end training time for\nmulti-billion parameter models distributed across hundreds of GPUs remains\nchallenging due to complex interactions between transformer components,\nparallelism strategies(data, model, pipeline, tensor), and multi-tier\ncommunication. Learned models require costly sampling, while analytical models\noften struggle with real-world network and hardware complexities. We address\nthis by decomposing LLMs into core computational primitives and modeling them\nwith: (1) operator-level decomposition for fine-grained analysis; (2)\nlightweight sampling based hardware-aware prediction models for key operations;\n(3) an end-to-end prediction system integrating these components across complex\nparallelization strategies. Crucially, our methodology has been validated on\ntwo large-scale HPC systems. Our framework achieves low average prediction\nerrors-4.98\\% on Perlmutter(A100) and 9.38\\% on Vista(GH200)-for models up to\n20B parameters across 128 GPUs. Importantly, it runs entirely on CPUs, enabling\nrapid iteration over hardware configurations and training strategies without\ncostly on-cluster experimentation."}
{"id": "2509.23645", "categories": ["cs.SE", "cs.DB"], "pdf": "https://arxiv.org/pdf/2509.23645", "abs": "https://arxiv.org/abs/2509.23645", "authors": ["A S M Shahadat Hossain", "Colin Brown", "David Koop", "Tanu Malik"], "title": "Similarity-Based Assessment of Computational Reproducibility in Jupyter Notebooks", "comment": "10 pages", "summary": "Computational reproducibility refers to obtaining consistent results when\nrerunning an experiment. Jupyter Notebook, a web-based computational notebook\napplication, facilitates running, publishing, and sharing computational\nexperiments along with their results. However, rerunning a Jupyter Notebook may\nnot always generate identical results due to various factors, such as\nrandomness, changes in library versions, or variations in the computational\nenvironment. This paper introduces the Similarity-based Reproducibility Index\n(SRI) -- a metric for assessing the reproducibility of results in Jupyter\nNotebooks. SRI employs novel methods developed based on similarity metrics\nspecific to different types of Python objects to compare rerun outputs against\noriginal outputs. For every cell generating an output in a rerun notebook, SRI\nreports a quantitative score in the range [0, 1] as well as some qualitative\ninsights to assess reproducibility. The paper also includes a case study in\nwhich the proposed metric is applied to a set of Jupyter Notebooks,\ndemonstrating how various similarity metrics can be leveraged to quantify\ncomputational reproducibility."}
{"id": "2509.24929", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2509.24929", "abs": "https://arxiv.org/abs/2509.24929", "authors": ["Hongwei Zhao", "Vianney Lapotre", "Guy Gogniat"], "title": "Fault Injection in On-Chip Interconnects: A Comparative Study of Wishbone, AXI-Lite, and AXI", "comment": "12 pages, 7 tables", "summary": "Fault injection attacks exploit physical disturbances to compromise the\nfunctionality and security of integrated circuits. As System on Chip (SoC)\narchitectures grow in complexity, the vulnerability of on chip communication\nfabrics has become increasingly prominent. Buses, serving as interconnects\namong various IP cores, represent potential vectors for fault-based\nexploitation. In this study, we perform simulation-driven fault injection\nacross three mainstream bus protocols Wishbone, AXI Lite, and AXI. We\nsystematically examine fault success rates, spatial vulnerability\ndistributions, and timing dependencies to characterize how faults interact with\nbus-level transactions. The results uncover consistent behavioral patterns\nacross protocols, offering practical insights for both attack modeling and the\ndevelopment of resilient SoC designs."}
{"id": "2509.23794", "categories": ["cs.NI"], "pdf": "https://arxiv.org/pdf/2509.23794", "abs": "https://arxiv.org/abs/2509.23794", "authors": ["Zhouyu Qu", "Andreas Willig", "Xiaobing Wu"], "title": "Short-Term Guidance Algorithm on a Drone Road System", "comment": "45 pages, 11 figures", "summary": "Unmanned Aerial Vehicles (UAVs), commonly known as drones, have experienced\nexpanding use in urban environments in recent years. However, the growing\ndensity of drones raises significant challenges, such as avoiding collisions\nand managing air traffic efficiently, especially in congested areas. To address\nthese issues, a structured road system and an effective guidance algorithm are\nessential. In this paper, we introduce a markup language allowing to describe\ndrone road systems (DRS), in which a road system is given by a set of\nindividual roads, each of which can have a varying number of lanes. Roads can\nbe linked through connecting lanes. Furthermore, we propose a novel short-term\ndecentralized greedy (STDG) guidance algorithm that uses only the position and\nspeed information of nearby drones -- communicated via periodically transmitted\nbeacons -- to make real-time decisions such as stopping, changing lanes, or\nadjusting speed for the next few seconds. Unlike existing methods that rely on\ncentralized coordination, our algorithm enables drones to operate independently\nwhile ensuring safety and efficiency. We present simulation results showing the\nimpact of key wireless and algorithm parameters on performance metrics like the\ndrone collision rate, average speed and throughput of the drone road system."}
{"id": "2509.24091", "categories": ["cs.SE", "cs.AI", "cs.PF"], "pdf": "https://arxiv.org/pdf/2509.24091", "abs": "https://arxiv.org/abs/2509.24091", "authors": ["Spandan Garg", "Roshanak Zilouchian Moghaddam"], "title": "PerfBench: Can Agents Resolve Real-World Performance Bugs?", "comment": null, "summary": "Performance bugs are inefficiencies in software that waste computational\nresources without causing functional failures, making them particularly\nchallenging to detect and fix. While recent advances in Software Engineering\nagents have shown promise in automated bug fixing, existing benchmarks\nprimarily focus on functional correctness and fail to evaluate agents'\nabilities to identify and resolve non-functional issues like performance bugs.\nWe introduce PerfBench, a benchmark comprising 81 real-world performance\nbug-fixing tasks from popular .NET repositories on GitHub. Unlike existing\nbenchmarks that rely on pre-existing test suites, PerfBench features a novel\nevaluation harness that allows agents to generate their own performance\nbenchmarks and validates fixes by comparing execution metrics collected for\ndeveloper fix and agent fix. Each task in PerfBench is derived from actual\ndeveloper fixes linked to performance-related issues, which are then verified\nby human experts, ensuring real-world relevance. Our evaluation reveals that\ncurrent state-of-the-art coding agents struggle with performance optimization\ntasks, with baseline OpenHands agent achieving only a ~3% success rate on our\nbenchmark. We develop OpenHands-Perf-Agent, which incorporates\nperformance-aware tooling and instructions and achieves a ~20% success rate on\nthe benchmark. We show that by ensuring the agent has proper instructions to\nbenchmark its changes and tooling for benchmark output processing, we can\nimprove the agent performance significantly, but room for improvement still\nremains. PerfBench provides a challenging test set for furthering the\ncapabilities of agents in fixing performance issues."}
{"id": "2509.22922", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2509.22922", "abs": "https://arxiv.org/abs/2509.22922", "authors": ["Pranjal Naman", "Yogesh Simmhan"], "title": "OptimES: Optimizing Federated Learning Using Remote Embeddings for Graph Neural Networks", "comment": "Extended full-length version of paper that appeared at Euro-Par 2024:\n  \"Optimizing Federated Learning Using Remote Embeddings for Graph Neural\n  Networks\", Pranjal Naman and Yogesh Simmhan, in International European\n  Conference on Parallel and Distributed Computing (Euro-Par), 2024. DOI:\n  https://doi.org/10.1007/978-3-031-69766-1_32", "summary": "Graph Neural Networks (GNNs) have experienced rapid advancements in recent\nyears due to their ability to learn meaningful representations from graph data\nstructures. However, in most real-world settings, such as financial transaction\nnetworks and healthcare networks, this data is localized to different data\nowners and cannot be aggregated due to privacy concerns. Federated Learning\n(FL) has emerged as a viable machine learning approach for training a shared\nmodel that iteratively aggregates local models trained on decentralized data.\nThis addresses privacy concerns while leveraging parallelism. State-of-the-art\nmethods enhance the privacy-respecting convergence accuracy of federated GNN\ntraining by sharing remote embeddings of boundary vertices through a server\n(EmbC). However, they are limited by diminished performance due to large\ncommunication costs. In this article, we propose OptimES, an optimized\nfederated GNN training framework that employs remote neighbourhood pruning,\noverlapping the push of embeddings to the server with local training, and\ndynamic pulling of embeddings to reduce network costs and training time. We\nperform a rigorous evaluation of these strategies for four common graph\ndatasets with up to $111M$ vertices and $1.8B$ edges. We see that a modest drop\nin per-round accuracy due to the preemptive push of embeddings is out-stripped\nby the reduction in per-round training time for large and dense graphs like\nReddit and Products, converging up to $\\approx 3.5\\times$ faster than EmbC and\ngiving up to $\\approx16\\%$ better accuracy than the default federated GNN\nlearning. While accuracy improvements over default federated GNNs are modest\nfor sparser graphs like Arxiv and Papers, they achieve the target accuracy\nabout $\\approx11\\times$ faster than EmbC."}
{"id": "2509.23675", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.23675", "abs": "https://arxiv.org/abs/2509.23675", "authors": ["Xinyue Zuo", "Yifan Zhang", "Hongshu Wang", "Yufan Cai", "Zhe Hou", "Jing Sun", "Jin Song Dong"], "title": "PAT-Agent: Autoformalization for Model Checking", "comment": "Accepted in ASE 2025 (International Conference on Automated Software\n  Engineering)", "summary": "Recent advances in large language models (LLMs) offer promising potential for\nautomating formal methods. However, applying them to formal verification\nremains challenging due to the complexity of specification languages, the risk\nof hallucinated output, and the semantic gap between natural language and\nformal logic. We introduce PAT-Agent, an end-to-end framework for natural\nlanguage autoformalization and formal model repair that combines the generative\ncapabilities of LLMs with the rigor of formal verification to automate the\nconstruction of verifiable formal models. In PAT-Agent, a Planning LLM first\nextracts key modeling elements and generates a detailed plan using semantic\nprompts, which then guides a Code Generation LLM to synthesize syntactically\ncorrect and semantically faithful formal models. The resulting code is verified\nusing the Process Analysis Toolkit (PAT) model checker against user-specified\nproperties, and when discrepancies occur, a Repair Loop is triggered to\niteratively correct the model using counterexamples. To improve flexibility, we\nbuilt a web-based interface that enables users, particularly non-FM-experts, to\ndescribe, customize, and verify system behaviors through user-LLM interactions.\nExperimental results on 40 systems show that PAT-Agent consistently outperforms\nbaselines, achieving high verification success with superior efficiency. The\nablation studies confirm the importance of both planning and repair components,\nand the user study demonstrates that our interface is accessible and supports\neffective formal modeling, even for users with limited formal methods\nexperience."}
{"id": "2509.22684", "categories": ["cs.DC", "cs.AR", "cs.CR", "cs.PF"], "pdf": "https://arxiv.org/pdf/2509.22684", "abs": "https://arxiv.org/abs/2509.22684", "authors": ["Tarunesh Verma", "Yichao Yuan", "Nishil Talati", "Todd Austin"], "title": "ZKProphet: Understanding Performance of Zero-Knowledge Proofs on GPUs", "comment": "To appear at 2025 IEEE International Symposium on Workload\n  Characterization", "summary": "Zero-Knowledge Proofs (ZKP) are protocols which construct cryptographic\nproofs to demonstrate knowledge of a secret input in a computation without\nrevealing any information about the secret. ZKPs enable novel applications in\nprivate and verifiable computing such as anonymized cryptocurrencies and\nblockchain scaling and have seen adoption in several real-world systems. Prior\nwork has accelerated ZKPs on GPUs by leveraging the inherent parallelism in\ncore computation kernels like Multi-Scalar Multiplication (MSM). However, we\nfind that a systematic characterization of execution bottlenecks in ZKPs, as\nwell as their scalability on modern GPU architectures, is missing in the\nliterature. This paper presents ZKProphet, a comprehensive performance study of\nZero-Knowledge Proofs on GPUs. Following massive speedups of MSM, we find that\nZKPs are bottlenecked by kernels like Number-Theoretic Transform (NTT), as they\naccount for up to 90% of the proof generation latency on GPUs when paired with\noptimized MSM implementations. Available NTT implementations under-utilize GPU\ncompute resources and often do not employ architectural features like\nasynchronous compute and memory operations. We observe that the arithmetic\noperations underlying ZKPs execute exclusively on the GPU's 32-bit integer\npipeline and exhibit limited instruction-level parallelism due to data\ndependencies. Their performance is thus limited by the available integer\ncompute units. While one way to scale the performance of ZKPs is adding more\ncompute units, we discuss how runtime parameter tuning for optimizations like\nprecomputed inputs and alternative data representations can extract additional\nspeedup. With this work, we provide the ZKP community a roadmap to scale\nperformance on GPUs and construct definitive GPU-accelerated ZKPs for their\napplication requirements and available hardware resources."}
{"id": "2509.23810", "categories": ["cs.NI"], "pdf": "https://arxiv.org/pdf/2509.23810", "abs": "https://arxiv.org/abs/2509.23810", "authors": ["Yan Sun", "Yinqiu Liu", "Shaoyong Guo", "Ruichen Zhang", "Jiacheng Wang", "Xuesong Qiu", "Geng Sun", "Weifeng Gong", "Dusit Niyato", "Qihui Wu"], "title": "A Synergy of Computing Power Networks and Low-Altitude Economy Intelligent Communications: Challenges, Design Principles, and Research Directions", "comment": "22 pages, 6 figures", "summary": "The rapid development of the Low-Altitude Economy (LAE) has created\nopportunities for emerging services such as autonomous aerial transportation,\naerial sensing, and emergency response, all of which rely on efficient and\nintelligent communications. However, LAE intelligent communications face\nseveral challenges, including the limited computational capacity of aerial\nnodes, the lack of cross-scenario generalization, and the complexity of\nheterogeneous demands. Meanwhile, Computing Power Networks (CPNs) have emerged\nas a new paradigm for integrating distributed computing, networking, and\nstorage resources, but they are also constrained by static deployment and\nlimited adaptability. In this survey, we explore the synergy between LAE\nintelligent communications and CPNs. We first analyze how CPNs can support LAE\nintelligent communications in areas such as air-ground collaborative control,\nAI training, communication-computation co-ptimization, and ubiquitous\nlow-altitude information processing. Conversely, we discuss how LAE intelligent\ncommunications can enhance CPNs through mobility-assisted control, distributed\nintelligent training, dynamic routing, and in-network aerial computing.\nFinally, based on these insights, we outline design principles and future\nresearch directions for integrated CPN-LAE systems. This work provides a\ncomprehensive foundation for building flexible, adaptive, and resilient\narchitectures that leverage the synergy between CPNs and LAE to deliver\nhigh-quality and sustainable low-altitude services."}
{"id": "2509.23013", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2509.23013", "abs": "https://arxiv.org/abs/2509.23013", "authors": ["Varad Kulkarni", "Nikhil Reddy", "Tuhin Khare", "Abhinandan S. Prasad", "Chitra Babu", "Yogesh Simmhan"], "title": "Characterizing FaaS Workflows on Public Clouds: The Good, the Bad and the Ugly", "comment": null, "summary": "Function-as-a-service (FaaS) is a popular serverless computing paradigm for\ndeveloping event-driven functions that elastically scale on public clouds. FaaS\nworkflows, such as AWS Step Functions and Azure Durable Functions, are composed\nfrom FaaS functions, like AWS Lambda and Azure Functions, to build practical\napplications. But, the complex interactions between functions in the workflow\nand the limited visibility into the internals of proprietary FaaS platforms are\nmajor impediments to gaining a deeper understanding of FaaS workflow platforms.\nWhile several works characterize FaaS platforms to derive such insights, there\nis a lack of a principled and rigorous study for FaaS workflow platforms, which\nhave unique scaling, performance and costing behavior influenced by the\nplatform design, dataflow and workloads. In this article, we perform extensive\nevaluations of three popular FaaS workflow platforms from AWS and Azure,\nrunning 25 micro-benchmark and application workflows over 132k invocations. Our\ndetailed analysis confirms some conventional wisdom but also uncovers unique\ninsights on the function execution, workflow orchestration, inter-function\ninteractions, cold-start scaling and monetary costs. Our observations help\ndevelopers better configure and program these platforms, set performance and\nscalability expectations, and identify research gaps on enhancing the\nplatforms."}
{"id": "2509.23679", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.23679", "abs": "https://arxiv.org/abs/2509.23679", "authors": ["Zeqin Liao", "Yuhong Nan", "Zixu Gao", "Henglong Liang", "Sicheng Hao", "Jiajing Wu", "Zibin Zheng"], "title": "Satellite: Detecting and Analyzing Smart Contract Vulnerabilities caused by Subcontract Misuse", "comment": "This is the author version of the article accepted for publication in\n  IEEE Transactions on Software Engineering. The final version is available at\n  10.1109/TSE.2025.3613470", "summary": "Developers of smart contracts pervasively reuse subcontracts to improve\ndevelopment efficiency. Like any program language, such subcontract reuse may\nunexpectedly include, or introduce vulnerabilities to the end-point smart\ncontract. Unfortunately, automatically detecting such issues poses several\nunique challenges. Particularly, in most cases, smart contracts are compiled as\nbytecode, whose class-level information (e.g., inheritance, virtual function\ntable), and even semantics (e.g., control flow and data flow) are fully\nobscured as a single smart contract after compilation.\n  In this paper, we propose Satellite, a new bytecode-level static analysis\nframework for subcontract misuse vulnerability (SMV) detection in smart\ncontracts. Satellite incorporates a series of novel designs to enhance its\noverall effectiveness.. Particularly, Satellite utilizes a transfer learning\nmethod to recover the inherited methods, which are critical for identifying\nsubcontract reuse in smart contracts. Further, Satellite extracts a set of\nfine-grained method-level features and performs a method-level comparison, for\nidentifying the reuse part of subcontract in smart contracts. Finally,\nSatellite summarizes a set of SMV indicators according to their types, and\nhence effectively identifies SMVs. To evaluate Satellite, we construct a\ndataset consisting of 58 SMVs derived from real-world attacks and collect\nadditional 56 SMV patterns from SOTA studies. Experiment results indicate that\nSatellite exhibits good performance in identifying SMV, with a precision rate\nof 84.68% and a recall rate of 92.11%. In addition, Satellite successfully\nidentifies 14 new/unknown SMV over 10,011 real-world smart contracts, affecting\na total amount of digital assets worth 201,358 USD."}
{"id": "2509.23913", "categories": ["cs.NI", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.23913", "abs": "https://arxiv.org/abs/2509.23913", "authors": ["Cheonjin Park", "Victoria Manfredi", "Xiaolan Zhang", "Chengyi Liu", "Alicia P Wolfe", "Dongjin Song", "Sarah Tasneem", "Bing Wang"], "title": "Continual Learning to Generalize Forwarding Strategies for Diverse Mobile Wireless Networks", "comment": "11 pages", "summary": "Deep reinforcement learning (DRL) has been successfully used to design\nforwarding strategies for multi-hop mobile wireless networks. While such\nstrategies can be used directly for networks with varied connectivity and\ndynamic conditions, developing generalizable approaches that are effective on\nscenarios significantly different from the training environment remains largely\nunexplored. In this paper, we propose a framework to address the challenge of\ngeneralizability by (i) developing a generalizable base model considering\ndiverse mobile network scenarios, and (ii) using the generalizable base model\nfor new scenarios, and when needed, fine-tuning the base model using a small\namount of data from the new scenarios. To support this framework, we first\ndesign new features to characterize network variation and feature quality,\nthereby improving the information used in DRL-based forwarding decisions. We\nthen develop a continual learning (CL) approach able to train DRL models across\ndiverse network scenarios without ``catastrophic forgetting.'' Using extensive\nevaluation, including real-world scenarios in two cities, we show that our\napproach is generalizable to unseen mobility scenarios. Compared to a\nstate-of-the-art heuristic forwarding strategy, it leads to up to 78% reduction\nin delay, 24% improvement in delivery rate, and comparable or slightly higher\nnumber of forwards."}
{"id": "2509.23241", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2509.23241", "abs": "https://arxiv.org/abs/2509.23241", "authors": ["Ankita Dutta", "Nabendu Chaki", "Rajat K. De"], "title": "Memory Efficient and Staleness Free Pipeline Parallel DNN Training Framework with Improved Convergence Speed", "comment": null, "summary": "High resource requirement for Deep Neural Network (DNN) training across\nmultiple GPUs necessitates development of various parallelism techniques. In\nthis paper, we introduce two interconnected DNN training frameworks, namely,\nV-TiMePReSt and I-TiMePReSt, based on pipeline parallelism, a variant of model\nparallelism. V-TiMePReSt is a completely staleness-free system which enables\nthe DNNs to be trained on the latest updated weights in each stage of all\nforward and backward passes. Developing staleness-aware systems at the expense\nof weight stashing reduces GPU-memory consumption, however, increases the\nnumber of epochs to converge. Thus, we introduce I-TiMePReSt, which is also a\nstaleness-aware system, but not at the expense of weight stashing. It does not\nrely solely on the stale weights or the latest updated weights. I-TiMePReSt\ncomputes an intermediate weight towards the latter and performs backward pass\non it. Additionally, we formulate the significance of the stale weights\nmathematically depending on the degree of staleness. In contrast to\nV-TiMePReSt, I-TiMePReSt works based on the assumption that stale weights have\na significant contribution in training, which can be quantified mathematically\nbased on the degree of staleness, although there are other contributory factors\nwhich should not be ignored. Experimental results show that V-TiMePReSt is\nadvantageous over existing models in terms of $1)$ the extent of staleness of\nthe weight parameter values and $2)$ GPU memory efficiency, while I-TiMePReSt\nis superior in terms of $1)$ removing staleness of the weight parameters\nwithout removing weight stashing and $2)$ maintaining the trade-off between GPU\nmemory consumption and convergence speed (number of epochs)."}
{"id": "2509.23806", "categories": ["cs.SE", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.23806", "abs": "https://arxiv.org/abs/2509.23806", "authors": ["Chih-Duo Hong", "Yu Wang", "Yao-Chen Chang", "Fang Yu"], "title": "Influence-Guided Concolic Testing of Transformer Robustness", "comment": null, "summary": "Concolic testing for deep neural networks alternates concrete execution with\nconstraint solving to search for inputs that flip decisions. We present an\n{influence-guided} concolic tester for Transformer classifiers that ranks path\npredicates by SHAP-based estimates of their impact on the model output. To\nenable SMT solving on modern architectures, we prototype a solver-compatible,\npure-Python semantics for multi-head self-attention and introduce practical\nscheduling heuristics that temper constraint growth on deeper models. In a\nwhite-box study on compact Transformers under small $L_0$ budgets, influence\nguidance finds label-flip inputs more efficiently than a FIFO baseline and\nmaintains steady progress on deeper networks. Aggregating successful attack\ninstances with a SHAP-based critical decision path analysis reveals recurring,\ncompact decision logic shared across attacks. These observations suggest that\n(i) influence signals provide a useful search bias for symbolic exploration,\nand (ii) solver-friendly attention semantics paired with lightweight scheduling\nmake concolic testing feasible for contemporary Transformer models, offering\npotential utility for debugging and model auditing."}
{"id": "2509.23921", "categories": ["cs.NI", "eess.SP"], "pdf": "https://arxiv.org/pdf/2509.23921", "abs": "https://arxiv.org/abs/2509.23921", "authors": ["João Paulo P. G. Marques", "Catherine Rosenberg"], "title": "Performance Analysis of Zero-Forcing Beamforming Strategies for the Uplink of an MU-MIMO System with Multi-Antenna Users", "comment": null, "summary": "We conduct a comprehensive evaluation of the performance of the uplink of\nOFDMA-based MU-MIMO systems with multi-antenna users, for three Zero-Forcing\n(ZF) Beamforming (BF) strategies: Coordinated-Transmit-Receive-1 (CTR1), where\nonly the strongest data stream is enabled per scheduled user; Block\nDiagonalization (BD), where all possible streams are enabled per scheduled\nuser; Coordinated-Transmit-Receive-Flexible (CTRF), which allows a flexible\nstream allocation per user. The Radio Resource Management (RRM) of the uplink\nof all OFDMA-based systems must be done over an entire Time-Slot (TS) due to\npower management, making it challenging. To enable this study, we propose an\nefficient heuristic based on greedy-up searches for stream-sets that provides\nfeasible solutions. It operates over the TS and considers fairness, practical\nModulation and Coding Schemes and all RRM processes. The results show that, for\nRural Macro scenarios, BD (resp. CTR1) could replace the more complex CTRF if\nthe number of users is small (resp. large), while for Urban Macro scenarios,\nCTR1 emerges as an alternative to CTRF due to its similar performance. We also\nshow that the system parameters can substantially impact the performance of the\nZF strategies and that BD performance is more impaired with a simpler power\nmanagement scheme than CTR1 and CTRF."}
{"id": "2509.23324", "categories": ["cs.DC", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.23324", "abs": "https://arxiv.org/abs/2509.23324", "authors": ["Zixu Hao", "Jianyu Wei", "Tuowei Wang", "Minxing Huang", "Huiqiang Jiang", "Shiqi Jiang", "Ting Cao", "Ju Ren"], "title": "Scaling LLM Test-Time Compute with Mobile NPU on Smartphones", "comment": null, "summary": "Deploying Large Language Models (LLMs) on mobile devices faces the challenge\nof insufficient performance in smaller models and excessive resource\nconsumption in larger ones. This paper highlights that mobile Neural Processing\nUnits (NPUs) have underutilized computational resources, particularly their\nmatrix multiplication units, during typical LLM inference. To leverage this\nwasted compute capacity, we propose applying parallel test-time scaling\ntechniques on mobile NPUs to enhance the performance of smaller LLMs. However,\nthis approach confronts inherent NPU challenges, including inadequate hardware\nsupport for fine-grained quantization and low efficiency in general-purpose\ncomputations. To overcome these, we introduce two key techniques: a\nhardware-aware tile quantization scheme that aligns group quantization with NPU\nmemory access patterns, and efficient LUT-based replacements for complex\noperations such as Softmax and dequantization. We design and implement an\nend-to-end inference system that leverages the NPU's compute capability to\nsupport test-time scaling on Qualcomm Snapdragon platforms. Experiments show\nour approach brings significant speedups: up to 19.0 for mixed-precision GEMM\nand 2.2 for Softmax. More importantly, we demonstrate that smaller models using\ntest-time scaling can match or exceed the accuracy of larger models, achieving\na new performance-cost Pareto frontier."}
{"id": "2509.23812", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.23812", "abs": "https://arxiv.org/abs/2509.23812", "authors": ["Dianshu Liao", "Xin Yin", "Shidong Pan", "Chao Ni", "Zhenchang Xing", "Xiaoyu Sun"], "title": "Navigating the Labyrinth: Path-Sensitive Unit Test Generation with Large Language Models", "comment": null, "summary": "Unit testing is essential for software quality assurance, yet writing and\nmaintaining tests remains time-consuming and error-prone. To address this\nchallenge, researchers have proposed various techniques for automating unit\ntest generation, including traditional heuristic-based methods and more recent\napproaches that leverage large language models (LLMs). However, these existing\napproaches are inherently path-insensitive because they rely on fixed\nheuristics or limited contextual information and fail to reason about deep\ncontrol-flow structures. As a result, they often struggle to achieve adequate\ncoverage, particularly for deep or complex execution paths. In this work, we\npresent a path-sensitive framework, JUnitGenie, to fill this gap by combining\ncode knowledge with the semantic capabilities of LLMs in guiding context-aware\nunit test generation. After extracting code knowledge from Java projects,\nJUnitGenie distills this knowledge into structured prompts to guide the\ngeneration of high-coverage unit tests. We evaluate JUnitGenie on 2,258 complex\nfocal methods from ten real-world Java projects. The results show that\nJUnitGenie generates valid tests and improves branch and line coverage by\n29.60% and 31.00% on average over both heuristic and LLM-based baselines. We\nfurther demonstrate that the generated test cases can uncover real-world bugs,\nwhich were later confirmed and fixed by developers."}
{"id": "2509.24038", "categories": ["cs.NI"], "pdf": "https://arxiv.org/pdf/2509.24038", "abs": "https://arxiv.org/abs/2509.24038", "authors": ["Toru Mano", "Hideki Nishizawa", "Takeo Sasai", "Soichiroh Usui", "Dmitrii Briantcev", "Devika Dass", "Brandt Bashaw", "Eoin Kenny", "Marco Ruffini", "Yoshiaki Sone", "Koichi Takasugi", "Daniel Kilper"], "title": "Beyond Redundancy: Toward Agile Resilience in Optical Networks to Overcome Unpredictable Disasters", "comment": null, "summary": "Resilience in optical networks has traditionally relied on redundancy and\npre-planned recovery strategies, both of which assume a certain level of\ndisaster predictability. However, recent environmental changes such as climate\nshifts, the evolution of communication services, and rising geopolitical risks\nhave increased the unpredictability of disasters, reducing the effectiveness of\nconventional resilience approaches. To address this unpredictability, this\npaper introduces the concept of agile resilience, which emphasizes dynamic\nadaptability across multiple operators and layers. We identify key requirements\nand challenges, and present enabling technologies for the realization of agile\nresilience. Using a field-deployed transmission system, we demonstrate rapid\nsystem characterization, optical path provisioning, and database migration\nwithin six hours. These results validate the effectiveness of the proposed\nenabling technologies and confirm the feasibility of agile resilience."}
{"id": "2509.23384", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2509.23384", "abs": "https://arxiv.org/abs/2509.23384", "authors": ["Yue Zhang", "Yuansheng Chen", "Xuan Mo", "Alex Xi", "Jialun Li", "WeiGang Wu"], "title": "A Predictive and Synergistic Two-Layer Scheduling Framework for LLM Serving", "comment": null, "summary": "LLM inference serving typically scales out with a two-tier architecture: a\ncluster router distributes requests to multiple inference engines, each of\nwhich then in turn performs its own internal scheduling. However, this commonly\nused paradigm suffers from critical, systemic inefficiency caused by the\ninformation gaps across two layers. At the cluster-layer, the router mainly\nrelies on lagging, coarse-grained metrics, such as average latency and queue\nlength to make decisions, resulting in \"decision lag\" that leads to suboptimal\nrequest routing. At the engine-layer, static heuristic scheduling policies\ncannot effectively handle the dynamic workloads, leading a poor balance between\nlatency and throughput. Besides, these gaps may cause SLO violations and\nresource waste, especially in heterogeneous cloud environments.\n  To bridge such gaps, we propose SynergySched, a cross-layer framework that\nshifts LLM serving system from reactive load balancing to predictive\norchestration. The core of SynergySched lies in a structurally-informed online\nperformance model that provides accurate, forward-looking per-step latency and\ncapacity estimations. This model empowers two key components. At the\nengine-layer, LENS performs SLO-aware, adaptive scheduling, dynamically\noptimizing batching to meet SLOs under real-time loads. At the cluster-layer,\nPRISM uses predictive signals to perform state-driven routing, maximizing\ncluster-wide performance and SLO attainment. Performance evaluations show that\nSynergySched improves SLO attainment by 43% on average and achieves up to 3x\nthroughput speedup in long-context and heterogeneous scenarios. Besides, we\nalso deploy SynergySched on FlowGPT's clusters to demonstrate its advantages in\nproduction environment."}
{"id": "2509.23824", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.23824", "abs": "https://arxiv.org/abs/2509.23824", "authors": ["Zhifan Ye", "Jiachi Chen", "Zhenzhe Shao", "Lingfeng Bao", "Xiaohu Yang", "Zhongxin Liu"], "title": "SolContractEval: A Benchmark for Evaluating Contract-Level Solidity Code Generation", "comment": null, "summary": "The rise of blockchain has brought smart contracts into mainstream use,\ncreating a demand for smart contract generation tools. While large language\nmodels (LLMs) excel at generating code in general-purpose languages, their\neffectiveness on Solidity, the primary language for smart contracts, remains\nunderexplored. Solidity constitutes only a small portion of typical LLM\ntraining data and differs from general-purpose languages in its\nversion-sensitive syntax and limited flexibility. These factors raise concerns\nabout the reliability of existing LLMs for Solidity code generation.\nCritically, existing evaluations, focused on isolated functions and synthetic\ninputs, fall short of assessing models' capabilities in real-world contract\ndevelopment.\n  To bridge this gap, we introduce SolContractEval, the first contract-level\nbenchmark for Solidity code generation. It comprises 124 tasks drawn from real\non-chain contracts across nine major domains. Each task input, consisting of\ncomplete context dependencies, a structured contract framework, and a concise\ntask prompt, is independently annotated and cross-validated by experienced\ndevelopers. To enable precise and automated evaluation of functional\ncorrectness, we also develop a dynamic evaluation framework based on historical\ntransaction replay. Building on SolContractEval, we perform a systematic\nevaluation of six mainstream LLMs. We find that Claude-3.7-Sonnet achieves the\nhighest overall performance, though evaluated models underperform relative to\ntheir capabilities on class-level generation tasks in general-purpose\nprogramming languages. Second, current models perform better on tasks that\nfollow standard patterns but struggle with complex logic and inter-contract\ndependencies. Finally, they exhibit limited understanding of Solidity-specific\nfeatures and contextual dependencies."}
{"id": "2509.24417", "categories": ["cs.NI"], "pdf": "https://arxiv.org/pdf/2509.24417", "abs": "https://arxiv.org/abs/2509.24417", "authors": ["Peter Schefczik", "Umar Toseef", "Paolo Baracca", "Ralf Klotsche", "Torsten Dudda", "Mai-Anh Phan", "Lorenzo Miretti", "David Ginthoer", "Bin Han"], "title": "Flexible and High-Performance Radio Access Networks for upcoming Sixth-Generation (6G) Systems", "comment": null, "summary": "The collaborative research project 6G-ANNA develops concepts for the 6G radio\naccess network (RAN) architecture and technology components. Previous RAN\ngenerations have become inherently more complex and reach their limits in\nhandling foreseen future traffic demands with their diverse characteristics in\nan efficient manner, e.g., for the use-case of mobile eXtended Reality (XR) on\na massive scale. One main objective of 6G is to regain both operational and\nenergy efficiency, i.e., by simplification and automation. To achieve this, in\nthis paper a flexible 6G RAN functional architecture and protocol stack as well\nas implementation and deployment options are described. We outline how\nperformance is optimized by distributed Multiple Input Multiple Output (MIMO)\nand distributed Carrier Aggregation (CA), and furthermore, how adaptiveness and\nscalability is enabled by Cloud RAN and service orchestration. Finally, the\nproposed zero-trust framework mitigates security risks in the described 6G RAN\narchitecture."}
{"id": "2509.23419", "categories": ["cs.DC", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.23419", "abs": "https://arxiv.org/abs/2509.23419", "authors": ["Asadullah Tariq", "Tariq Qayyum", "Mohamed Adel Serhani", "Farag Sallabi", "Ikbal Taleb", "Ezedin S. Barka"], "title": "Enhancing Communication Efficiency in FL with Adaptive Gradient Quantization and Communication Frequency Optimization", "comment": null, "summary": "Federated Learning (FL) enables participant devices to collaboratively train\ndeep learning models without sharing their data with the server or other\ndevices, effectively addressing data privacy and computational concerns.\nHowever, FL faces a major bottleneck due to high communication overhead from\nfrequent model updates between devices and the server, limiting deployment in\nresource-constrained wireless networks. In this paper, we propose a three-fold\nstrategy. Firstly, an Adaptive Feature-Elimination Strategy to drop less\nimportant features while retaining high-value ones; secondly, Adaptive Gradient\nInnovation and Error Sensitivity-Based Quantization, which dynamically adjusts\nthe quantization level for innovative gradient compression; and thirdly,\nCommunication Frequency Optimization to enhance communication efficiency. We\nevaluated our proposed model's performance through extensive experiments,\nassessing accuracy, loss, and convergence compared to baseline techniques. The\nresults show that our model achieves high communication efficiency in the\nframework while maintaining accuracy."}
{"id": "2509.23835", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.23835", "abs": "https://arxiv.org/abs/2509.23835", "authors": ["Yukai Zhao", "Menghan Wu", "Xing Hu", "Xin Xia"], "title": "HFuzzer: Testing Large Language Models for Package Hallucinations via Phrase-based Fuzzing", "comment": null, "summary": "Large Language Models (LLMs) are widely used for code generation, but they\nface critical security risks when applied to practical production due to\npackage hallucinations, in which LLMs recommend non-existent packages. These\nhallucinations can be exploited in software supply chain attacks, where\nmalicious attackers exploit them to register harmful packages. It is critical\nto test LLMs for package hallucinations to mitigate package hallucinations and\ndefend against potential attacks. Although researchers have proposed testing\nframeworks for fact-conflicting hallucinations in natural language generation,\nthere is a lack of research on package hallucinations. To fill this gap, we\npropose HFUZZER, a novel phrase-based fuzzing framework to test LLMs for\npackage hallucinations. HFUZZER adopts fuzzing technology and guides the model\nto infer a wider range of reasonable information based on phrases, thereby\ngenerating enough and diverse coding tasks. Furthermore, HFUZZER extracts\nphrases from package information or coding tasks to ensure the relevance of\nphrases and code, thereby improving the relevance of generated tasks and code.\nWe evaluate HFUZZER on multiple LLMs and find that it triggers package\nhallucinations across all selected models. Compared to the mutational fuzzing\nframework, HFUZZER identifies 2.60x more unique hallucinated packages and\ngenerates more diverse tasks. Additionally, when testing the model GPT-4o,\nHFUZZER finds 46 unique hallucinated packages. Further analysis reveals that\nfor GPT-4o, LLMs exhibit package hallucinations not only during code generation\nbut also when assisting with environment configuration."}
{"id": "2509.24446", "categories": ["cs.NI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.24446", "abs": "https://arxiv.org/abs/2509.24446", "authors": ["Jeremias Dötterl"], "title": "Contrastive Learning for Correlating Network Incidents", "comment": "Accepted at The 26th International Conference on Intelligent Data\n  Engineering and Automated Learning (IDEAL 2025). This work was partially\n  funded by the German Federal Ministry of Research, Technology and Space\n  (BMFTR) in the FRONT-RUNNER project (Grant 16KISR005K)", "summary": "Internet service providers monitor their networks to detect, triage, and\nremediate service impairments. When an incident is detected, it is important to\ndetermine whether similar incidents have occurred in the past or are happening\nconcurrently elsewhere in the network. Manual correlation of such incidents is\ninfeasible due to the scale of the networks under observation, making automated\ncorrelation a necessity. This paper presents a self-supervised learning method\nfor similarity-based correlation of network situations. Using this method, a\ndeep neural network is trained on a large unlabeled dataset of network\nsituations using contrastive learning. High precision achieved in experiments\non real-world network monitoring data suggests that contrastive learning is a\npromising approach to network incident correlation."}
{"id": "2509.23448", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2509.23448", "abs": "https://arxiv.org/abs/2509.23448", "authors": ["Hao Hao", "Dahlia Malkhi", "Maofan Yin", "Lizan Zhou"], "title": "Lyte Quorum: Off-Chain Ready Smart Contract Hosted with Choice", "comment": null, "summary": "This paper introduces Lyquor, a decentralized platform that reimagines\nblockchain infrastructure through a service-centric model where nodes\nselectively host smart contracts (called Lyquids) while preserving global\ncomposability. We present three key innovations: (1) Fate-Constrained Ordering\n(FCO), which decouples consensus from execution to enable selective hosting\nwithout sacrificing Layer-1 grade composability; (2) Direct Memory Architecture\n(DMA), which eliminates state access bottlenecks by providing each contract\nwith persistent, byte-addressable virtual memory; and (3) Universal Procedure\nCall (UPC), which enables fault-tolerant, programmable coordination across\ndistributed off-chain computation. Together, these components are powered by a\nRust-macroed unified programming model where on-chain and off-chain logic\ncoexist seamlessly, supporting both traditional smart contract patterns and\nnovel distributed applications. Lyquor addresses critical limitations in\nexisting systems while maintaining compatibility with Ethereum APIs, offering a\npath toward truly scalable decentralized computation."}
{"id": "2509.23961", "categories": ["cs.SE", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.23961", "abs": "https://arxiv.org/abs/2509.23961", "authors": ["Sheikh Md Mushfiqur Rahman", "Nasir Eisty"], "title": "Learning-Based Testing for Deep Learning: Enhancing Model Robustness with Adversarial Input Prioritization", "comment": null, "summary": "Context: Deep Neural Networks (DNNs) are increasingly deployed in critical\napplications, where resilience against adversarial inputs is paramount.\nHowever, whether coverage-based or confidence-based, existing test\nprioritization methods often fail to efficiently identify the most\nfault-revealing inputs, limiting their practical effectiveness. Aims: This\nproject aims to enhance fault detection and model robustness in DNNs by\nintegrating Learning-Based Testing (LBT) with hypothesis and mutation testing\nto efficiently prioritize adversarial test cases. Methods: Our method selects a\nsubset of adversarial inputs with a high likelihood of exposing model faults,\nwithout relying on architecture-specific characteristics or formal\nverification, making it adaptable across diverse DNNs. Results: Our results\ndemonstrate that the proposed LBT method consistently surpasses baseline\napproaches in prioritizing fault-revealing inputs and accelerating fault\ndetection. By efficiently organizing test permutations, it uncovers all\npotential faults significantly faster across various datasets, model\narchitectures, and adversarial attack techniques. Conclusion: Beyond improving\nfault detection, our method preserves input diversity and provides effective\nguidance for model retraining, further enhancing robustness. These advantages\nestablish our approach as a powerful and practical solution for adversarial\ntest prioritization in real-world DNN applications."}
{"id": "2509.24846", "categories": ["cs.NI"], "pdf": "https://arxiv.org/pdf/2509.24846", "abs": "https://arxiv.org/abs/2509.24846", "authors": ["Adam Zahir", "Milan Groshev", "Carlos J. Bernardos", "Antonio de la Oliva"], "title": "Blockchain-Driven Federation for Distributed Edge Systems: Design and Experimental Validation", "comment": null, "summary": "Edge computing brings computation near end users, enabling the provisioning\nof novel use cases. To satisfy end-user requirements, the concept of edge\nfederation has recently emerged as a key mechanism for dynamic resources and\nservices sharing across edge systems managed by different administrative\ndomains. However, existing federation solutions often rely on pre-established\nagreements and face significant limitations, including operational complexity,\ndelays caused by manual operations, high overhead costs, and dependence on\ntrusted third parties. In this context, blockchain can create dynamic\nfederation agreements that enable service providers to securely interact and\nshare services without prior trust.\n  This article first describes the problem of edge federation, using the\nstandardized ETSI multi-access edge computing framework as a reference\narchitecture, and how it is being addressed. Then, it proposes a novel solution\nusing blockchain and smart contracts to enable distributed MEC systems to\ndynamically negotiate and execute federation in a secure, automated, and\nscalable manner. We validate our framework's feasibility through a performance\nevaluation using a private Ethereum blockchain, built on the open-source\nHyperledger Besu platform. The testbed includes a large number of MEC systems\nand compares two blockchain consensus algorithms. Experimental results\ndemonstrate that our solution automates the entire federation lifecycle-from\nnegotiation to deployment-with a quantifiable overhead, achieving federation in\napproximately 18 seconds in a baseline scenario. The framework scales\nefficiently in concurrent request scenarios, where multiple MEC systems\ninitiate federation requests simultaneously. This approach provides a promising\ndirection for addressing the complexities of dynamic, multi-domain federations\nacross the edge-to-cloud continuum."}
{"id": "2509.23706", "categories": ["cs.DC", "cs.DS"], "pdf": "https://arxiv.org/pdf/2509.23706", "abs": "https://arxiv.org/abs/2509.23706", "authors": ["Bogdan-Ioan Popa", "Adrian-Marius Dumitran", "Livia Magureanu"], "title": "Parallel Algorithms for the One Sided Crossing Minimization Problem", "comment": null, "summary": "The One Sided Crossing Minimization (OSCM) problem is an optimization problem\nin graph drawing that aims to minimize the number of edge crossings in\nbipartite graph layouts. It has practical applications in areas such as network\nvisualization and VLSI (Very Large Scale Integration) design, where reducing\nedge crossings improves the arrangement of circuit components and their\ninterconnections. Despite the rise of multi-core systems, the parallelization\nof exact and fixed-parameter tractable (FPT) algorithms for OSCM remains\nlargely unexplored. Parallel variants offer significant potential for scaling\nto larger graphs but require careful handling of synchronization and memory\nmanagement. In this paper, we explore various previously studied exact and FPT\nalgorithms for OSCM, implementing and analyzing them in both sequential and\nparallel forms. Our main contribution lies in empirically proving that these\nalgorithms can achieve close to linear speedup under parallelization. In\nparticular, our best result achieves a speedup of nearly 19 on a 16-core,\n32-thread machine. We further investigate and discuss the reasons why linear\nspeedup is not always attained."}
{"id": "2509.24032", "categories": ["cs.SE", "cs.CR"], "pdf": "https://arxiv.org/pdf/2509.24032", "abs": "https://arxiv.org/abs/2509.24032", "authors": ["Jialun Zhang", "Merve Gülmez", "Thomas Nyman", "Gang Tan"], "title": "SandCell: Sandboxing Rust Beyond Unsafe Code", "comment": null, "summary": "Rust is a modern systems programming language that ensures memory safety by\nenforcing ownership and borrowing rules at compile time. While the unsafe\nkeyword allows programmers to bypass these restrictions, it introduces\nsignificant risks. Various approaches for isolating unsafe code to protect safe\nRust from vulnerabilities have been proposed, yet these methods provide only\nfixed isolation boundaries and do not accommodate expressive policies that\nrequire sandboxing both safe and unsafe code. This paper presents SandCell for\nflexible and lightweight isolation in Rust by leveraging existing syntactic\nboundaries. SandCell allows programmers to specify which components to sandbox\nwith minimal annotation effort, enabling fine-grained control over isolation.\nThe system also introduces novel techniques to minimize overhead when\ntransferring data between sandboxes. Our evaluation demonstrates SandCell's\neffectiveness in preventing vulnerabilities across various Rust applications\nwhile maintaining reasonable performance overheads."}
{"id": "2509.24944", "categories": ["cs.NI", "eess.SP"], "pdf": "https://arxiv.org/pdf/2509.24944", "abs": "https://arxiv.org/abs/2509.24944", "authors": ["Hongchuan Jia", "Fayu Wan", "Vladimir Mordachev", "Jérôme Rossignol", "Glauco Fontagalland", "Nour Murad", "Blaise Ravelo"], "title": "Experimental Study of Magnetic Near-Field Microstrip Electronic Probe for PCB EMC Emission Measurement", "comment": null, "summary": "An experimental study on magnetic near-field (NF) scanning of printed circuit\nboard (PCB) emission radiation is developed in this paper. The design and\ninstallation of the electromagnetic (EM) NF scanner is introduced. The test bed\nof magnetic NF emission in the microwave frequency range is described. The\nmethodology of the microstrip magnetic NF probe is discussed. The probe\ncalibration process was performed following the IEC 61967-1 NF scanning\nstandard. The NF scanner functioning is tested with passive microstrip circuit\nsquare loop probe and device under test (DUT) PCB radiation in the test plan\npositioned at 1-mm above the ground plane. Based on the standard test with\nI-shape 50-$\\Omega$ transmission line (TL), the calibration process of radiated\nmagnetic field was validated by comparison between HFSS__ simulation and\nexperimentation in very wideband frequency from 0.1-GHz to 3-GHz. Then, a\nnonstandard TL based DUT was experimented. Accordingly, the cartographies of\nscanned magnetic NF at two different test frequencies, 2 GHz and 3 GHz, are\ndiscussed. The NF scanner is under development for targeting the EMC radiated\nemission of PCB dedicated to operate in 6G wireless communication."}
{"id": "2509.23722", "categories": ["cs.DC", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.23722", "abs": "https://arxiv.org/abs/2509.23722", "authors": ["Jihu Guo", "Tenghui Ma", "Wei Gao", "Peng Sun", "Jiaxing Li", "Xun Chen", "Yuyang Jin", "Dahua Lin"], "title": "AdaPtis: Reducing Pipeline Bubbles with Adaptive Pipeline Parallelism on Heterogeneous Models", "comment": "13 pages, 15 Figures; Under Review;", "summary": "Pipeline parallelism is widely used to train large language models (LLMs).\nHowever, increasing heterogeneity in model architectures exacerbates pipeline\nbubbles, thereby reducing training efficiency. Existing approaches overlook the\nco-optimization of model partition, model placement, and workload scheduling,\nresulting in limited efficiency improvement or even performance degradation. To\nrespond, we propose AdaPtis, an LLM training system that supports adaptive\npipeline parallelism. First, we develop a pipeline performance model to\naccurately estimate training throughput. Second, AdaPtis jointly optimizes\nmodel partition, model placement, and workload scheduling policies guided by\nthis performance model. Third, we design a unified pipeline executor that\nefficiently supports the execution of diverse pipeline strategies. Extensive\nexperiments show that AdaPtis achieves an average speedup of 1.42x (up to\n2.14x) over Megatron-LM I-1F1B across various LLM architectures and scales."}
{"id": "2509.24091", "categories": ["cs.SE", "cs.AI", "cs.PF"], "pdf": "https://arxiv.org/pdf/2509.24091", "abs": "https://arxiv.org/abs/2509.24091", "authors": ["Spandan Garg", "Roshanak Zilouchian Moghaddam"], "title": "PerfBench: Can Agents Resolve Real-World Performance Bugs?", "comment": null, "summary": "Performance bugs are inefficiencies in software that waste computational\nresources without causing functional failures, making them particularly\nchallenging to detect and fix. While recent advances in Software Engineering\nagents have shown promise in automated bug fixing, existing benchmarks\nprimarily focus on functional correctness and fail to evaluate agents'\nabilities to identify and resolve non-functional issues like performance bugs.\nWe introduce PerfBench, a benchmark comprising 81 real-world performance\nbug-fixing tasks from popular .NET repositories on GitHub. Unlike existing\nbenchmarks that rely on pre-existing test suites, PerfBench features a novel\nevaluation harness that allows agents to generate their own performance\nbenchmarks and validates fixes by comparing execution metrics collected for\ndeveloper fix and agent fix. Each task in PerfBench is derived from actual\ndeveloper fixes linked to performance-related issues, which are then verified\nby human experts, ensuring real-world relevance. Our evaluation reveals that\ncurrent state-of-the-art coding agents struggle with performance optimization\ntasks, with baseline OpenHands agent achieving only a ~3% success rate on our\nbenchmark. We develop OpenHands-Perf-Agent, which incorporates\nperformance-aware tooling and instructions and achieves a ~20% success rate on\nthe benchmark. We show that by ensuring the agent has proper instructions to\nbenchmark its changes and tooling for benchmark output processing, we can\nimprove the agent performance significantly, but room for improvement still\nremains. PerfBench provides a challenging test set for furthering the\ncapabilities of agents in fixing performance issues."}
{"id": "2509.24932", "categories": ["cs.DC", "cs.LG", "cs.NI"], "pdf": "https://arxiv.org/pdf/2509.24932", "abs": "https://arxiv.org/abs/2509.24932", "authors": ["Fardis Nadimi", "Payam Abdisarabshali", "Jacob Chakareski", "Nicholas Mastronarde", "Seyyedali Hosseinalipour"], "title": "Graph Theory Meets Federated Learning over Satellite Constellations: Spanning Aggregations, Network Formation, and Performance Optimization", "comment": "8 Figures, 6 Appendix", "summary": "We introduce Fed-Span, a novel federated/distributed learning framework\ndesigned for low Earth orbit satellite constellations. By leveraging\ngraph-theoretic principles, Fed-Span addresses critical challenges inherent to\ndistributed learning in dynamic satellite networks, including intermittent\nsatellite connectivity, heterogeneous computational capabilities of satellites,\nand time-varying satellites' datasets. At its core, Fed-Span builds upon\nminimum spanning tree (MST) and minimum spanning forest (MSF) topologies,\nenabling spanning model aggregation and dispatching processes for distributed\nlearning. To formalize Fed-Span, we offer a fresh perspective on MST/MSF\ntopologies by formulating them through a set of continuous constraint\nrepresentations (CCRs), thereby devising graph-theoretical abstractions into an\noptimizable framework for satellite networks. Using these CCRs, we obtain the\nenergy consumption and latency of operations in Fed-Span. Moreover, we derive\nnovel convergence bounds for non-convex machine learning loss functions,\naccommodating the key system characteristics and degrees of freedom of\nFed-Span. Finally, we propose a comprehensive optimization problem that jointly\nminimizes model prediction loss, energy consumption, and latency of Fed-Span.\nWe unveil that this problem is NP-hard and develop a systematic approach to\ntransform it into a geometric programming formulation, solved via successive\nconvex optimization with performance guarantees. Through evaluations on\nreal-world datasets, we demonstrate that Fed-Span outperforms existing methods,\nwith faster model convergence, greater energy efficiency, and reduced latency.\nThese results highlight Fed-Span as a novel solution for efficient distributed\nlearning in satellite networks."}
{"id": "2509.24030", "categories": ["cs.DC", "cs.AI", "cs.SE"], "pdf": "https://arxiv.org/pdf/2509.24030", "abs": "https://arxiv.org/abs/2509.24030", "authors": ["Anjus George", "Michael Brim", "Christopher Zimmer", "David Rogers", "Sarp Oral", "Zach Mayes"], "title": "From Edge to HPC: Investigating Cross-Facility Data Streaming Architectures", "comment": null, "summary": "In this paper, we investigate three cross-facility data streaming\narchitectures, Direct Streaming (DTS), Proxied Streaming (PRS), and Managed\nService Streaming (MSS). We examine their architectural variations in data flow\npaths and deployment feasibility, and detail their implementation using the\nData Streaming to HPC (DS2HPC) architectural framework and the SciStream\nmemory-to-memory streaming toolkit on the production-grade Advanced Computing\nEcosystem (ACE) infrastructure at Oak Ridge Leadership Computing Facility\n(OLCF). We present a workflow-specific evaluation of these architectures using\nthree synthetic workloads derived from the streaming characteristics of\nscientific workflows. Through simulated experiments, we measure streaming\nthroughput, round-trip time, and overhead under work sharing, work sharing with\nfeedback, and broadcast and gather messaging patterns commonly found in AI-HPC\ncommunication motifs. Our study shows that DTS offers a minimal-hop path,\nresulting in higher throughput and lower latency, whereas MSS provides greater\ndeployment feasibility and scalability across multiple users but incurs\nsignificant overhead. PRS lies in between, offering a scalable architecture\nwhose performance matches DTS in most cases."}
{"id": "2509.24148", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.24148", "abs": "https://arxiv.org/abs/2509.24148", "authors": ["Yiran Hu", "Nan Jiang", "Shanchao Liang", "Yi Wu", "Lin Tan"], "title": "TENET: Leveraging Tests Beyond Validation for Code Generation", "comment": null, "summary": "Test-Driven Development (TDD) is a widely adopted software engineering\npractice that requires developers to create and execute tests alongside code\nimplementation, ensuring that software behavior is continuously validated and\nrefined. In the era of vibe coding, where developers increasingly delegate code\nwriting to large language models (LLMs) by specifying high-level intentions,\nTDD becomes even more crucial, as test cases serve as executable specifications\nthat explicitly define and verify intended functionality beyond what\nnatural-language descriptions and code context can convey. While vibe coding\nunder TDD is promising, there are three main challenges: (1) selecting a small\nyet effective test suite to improve the generation accuracy and control the\nexecution workload, (2) retrieving context such as relevant code effectively,\nand (3) systematically using test feedback for effective code refinement. To\naddress these challenges, we introduce TENET, an LLM agent for generating\nfunctions in complex real-world repositories under the TDD setting. TENET\nfeatures three components: (1) a novel test harness mechanism that selects a\nconcise test suite to maximize diversity of target usage scenarios; (2) a\ntailored agent toolset that performs efficient retrieval of relevant code with\ninteractive debugging; and (3) a reflection-based refinement workflow that\niteratively analyzes failures, replenishes context, and applies code\nrefinement. TENET achieves 69.08% and 81.77% Pass@1 on RepoCod and RepoEval\nbenchmarks, outperforming the best agentic baselines by 9.49 and 2.17\npercentage points, respectively. In addition, this is the first study of\ntest-driven code generation with repository-level context, examining how\ndifferent aspects of test suites affect the performance of LLM agents under the\nTDD setting."}
{"id": "2509.24063", "categories": ["cs.DC", "cs.CE", "cs.MA", "cs.PF", "q-bio.QM"], "pdf": "https://arxiv.org/pdf/2509.24063", "abs": "https://arxiv.org/abs/2509.24063", "authors": ["Lukas Breitwieser", "Ahmad Hesam", "Abdullah Giray Yağlıkçı", "Mohammad Sadrosadati", "Fons Rademakers", "Onur Mutlu"], "title": "TeraAgent: A Distributed Agent-Based Simulation Engine for Simulating Half a Trillion Agents", "comment": null, "summary": "Agent-based simulation is an indispensable paradigm for studying complex\nsystems. These systems can comprise billions of agents, requiring the computing\nresources of multiple servers to simulate. Unfortunately, the state-of-the-art\nplatform, BioDynaMo, does not scale out across servers due to its\nshared-memory-based implementation.\n  To overcome this key limitation, we introduce TeraAgent, a distributed\nagent-based simulation engine. A critical challenge in distributed execution is\nthe exchange of agent information across servers, which we identify as a major\nperformance bottleneck. We propose two solutions: 1) a tailored serialization\nmechanism that allows agents to be accessed and mutated directly from the\nreceive buffer, and 2) leveraging the iterative nature of agent-based\nsimulations to reduce data transfer with delta encoding.\n  Built on our solutions, TeraAgent enables extreme-scale simulations with half\na trillion agents (an 84x improvement), reduces time-to-result with additional\ncompute nodes, improves interoperability with third-party tools, and provides\nusers with more hardware flexibility."}
{"id": "2509.24215", "categories": ["cs.SE", "cs.AI", "cs.CL", "cs.MM"], "pdf": "https://arxiv.org/pdf/2509.24215", "abs": "https://arxiv.org/abs/2509.24215", "authors": ["Wenxuan Wang", "Yongjiang Wu", "Junyuan Zhang", "Shuqing Li", "Yun Peng", "Wenting Chen", "Shuai Wang", "Michael R. Lyu"], "title": "Metamorphic Testing for Audio Content Moderation Software", "comment": "Accepted by ASE 2025", "summary": "The rapid growth of audio-centric platforms and applications such as WhatsApp\nand Twitter has transformed the way people communicate and share audio content\nin modern society. However, these platforms are increasingly misused to\ndisseminate harmful audio content, such as hate speech, deceptive\nadvertisements, and explicit material, which can have significant negative\nconsequences (e.g., detrimental effects on mental health). In response,\nresearchers and practitioners have been actively developing and deploying audio\ncontent moderation tools to tackle this issue. Despite these efforts, malicious\nactors can bypass moderation systems by making subtle alterations to audio\ncontent, such as modifying pitch or inserting noise. Moreover, the\neffectiveness of modern audio moderation tools against such adversarial inputs\nremains insufficiently studied. To address these challenges, we propose MTAM, a\nMetamorphic Testing framework for Audio content Moderation software.\nSpecifically, we conduct a pilot study on 2000 audio clips and define 14\nmetamorphic relations across two perturbation categories: Audio Features-Based\nand Heuristic perturbations. MTAM applies these metamorphic relations to toxic\naudio content to generate test cases that remain harmful while being more\nlikely to evade detection. In our evaluation, we employ MTAM to test five\ncommercial textual content moderation software and an academic model against\nthree kinds of toxic content. The results show that MTAM achieves up to 38.6%,\n18.3%, 35.1%, 16.7%, and 51.1% error finding rates (EFR) when testing\ncommercial moderation software provided by Gladia, Assembly AI, Baidu,\nNextdata, and Tencent, respectively, and it obtains up to 45.7% EFR when\ntesting the state-of-the-art algorithms from the academy."}
{"id": "2509.24381", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2509.24381", "abs": "https://arxiv.org/abs/2509.24381", "authors": ["Tianyu Guo", "Tianming Xu", "Xianjie Chen", "Junru Chen", "Nong Xiao", "Xianwei Zhang"], "title": "RServe: Overlapping Encoding and Prefill for Efficient LMM Inference", "comment": null, "summary": "Large multimodal models (LMMs) typically employ an encoding module to\ntransform multimodal data inputs into embeddings, which are then fed to\nlanguage models for further processing. However, efficiently serving LMMs\nremains highly challenging due to the inherent complexity of their inference\npipelines. Traditional serving engines co-locate the encoding module and the\nlanguage model, leading to significant resource interference and tight data\ndependency. Recent studies have alleviated this issue by disaggregating the\nencoding module from the model, following a design style of prefill-decode\ndisaggregation. Nevertheless, these approaches fail to fully exploit\nparallelism both within individual requests (intra-request) and across multiple\nrequests (inter-request).\n  To overcome the limitation, we propose REDServe, an LMM inference system that\nefficiently orchestrates intra- and inter-request pipelines. REDServe is\ndesigned to reduce low latency and maximize parallelism at both intra- and\ninter-request granularities. Built on the disaggregated architecture of the\nencoding module and language model, REDServe adopts a fine-grained scheduling\nmethod that overlaps multimodal encoding with the forward computation of the\nlanguage model within a single request. For inter-request pipeline, REDServe\nleverages schedulable tokens and token budgets to balance computational loads\nacross micro-batches. Combined with chunked prefill, this enables a novel\nscheduling strategy that coordinates the execution of intra- and inter-request\npipelines. Experimental evaluations on representative LMMs show that REDServe\nachieves substantial latency reduction of up to 66% while improving throughput\nby up to 109%, significantly outperforming existing serving approaches."}
{"id": "2509.24344", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.24344", "abs": "https://arxiv.org/abs/2509.24344", "authors": ["Theo Koraag", "Niklas Wagner", "Felix Dobslaw", "Lucas Gren"], "title": "Comparing Open-Source and Commercial LLMs for Domain-Specific Analysis and Reporting: Software Engineering Challenges and Design Trade-offs", "comment": null, "summary": "Context: Large Language Models (LLMs) enable automation of complex natural\nlanguage processing across domains, but research on domain-specific\napplications like Finance remains limited. Objectives: This study explored\nopen-source and commercial LLMs for financial report analysis and commentary\ngeneration, focusing on software engineering challenges in implementation.\nMethods: Using Design Science Research methodology, an exploratory case study\niteratively designed and evaluated two LLM-based systems: one with local\nopen-source models in a multi-agent workflow, another using commercial GPT-4o.\nBoth were assessed through expert evaluation of real-world financial reporting\nuse cases. Results: LLMs demonstrated strong potential for automating financial\nreporting tasks, but integration presented significant challenges. Iterative\ndevelopment revealed issues including prompt design, contextual dependency, and\nimplementation trade-offs. Cloud-based models offered superior fluency and\nusability but raised data privacy and external dependency concerns. Local\nopen-source models provided better data control and compliance but required\nsubstantially more engineering effort for reliability and usability.\nConclusion: LLMs show strong potential for financial reporting automation, but\nsuccessful integration requires careful attention to architecture, prompt\ndesign, and system reliability. Implementation success depends on addressing\ndomain-specific challenges through tailored validation mechanisms and\nengineering strategies that balance accuracy, control, and compliance."}
{"id": "2509.24626", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2509.24626", "abs": "https://arxiv.org/abs/2509.24626", "authors": ["Qihui Zhou", "Peiqi Yin", "Pengfei Zuo", "James Cheng"], "title": "SparseServe: Unlocking Parallelism for Dynamic Sparse Attention in Long-Context LLM Serving", "comment": "14 pages, 16 figures", "summary": "Serving long-context LLMs is costly because attention computation grows\nlinearly with context length. Dynamic sparse attention algorithms (DSAs)\nmitigate this by attending only to the key-value (KV) cache of critical tokens.\nHowever, with DSAs, the main performance bottleneck shifts from HBM bandwidth\nto HBM capacity: KV caches for unselected tokens must remain in HBM for\nlow-latency decoding, constraining parallel batch size and stalling further\nthroughput gains. Offloading these underutilized KV caches to DRAM could free\nHBM capacity, allowing larger parallel batch sizes. Yet, achieving such\nhierarchical HBM-DRAM storage raises new challenges, including fragmented KV\ncache access, HBM cache contention, and high HBM demands of hybrid batching,\nthat remain unresolved in prior work.\n  This paper proposes SparseServe, an LLM serving system that unlocks the\nparallel potential of DSAs through efficient hierarchical HBM-DRAM management.\nSparseServe introduces three key innovations to address the challenges\nmentioned above: (1) fragmentation-aware KV cache transfer, which accelerates\nHBM-DRAM data movement through GPU-direct loading (FlashH2D) and CPU-assisted\nsaving (FlashD2H); (2) working-set-aware batch size control that adjusts batch\nsizes based on real-time working set estimation to minimize HBM cache\nthrashing; (3) layer-segmented prefill that bounds HBM use during prefill to a\nsingle layer, enabling efficient execution even for long prompts. Extensive\nexperimental results demonstrate that SparseServe achieves up to 9.26x lower\nmean time-to-first-token (TTFT) latency and up to 3.14x higher token generation\nthroughput compared to state-of-the-art LLM serving systems."}
{"id": "2509.24347", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.24347", "abs": "https://arxiv.org/abs/2509.24347", "authors": ["Junjie Meng", "Jie An", "Yong Li", "Andrea Turrini", "Fanjiang Xu", "Naijun Zhan", "Miaomiao Zhang"], "title": "Efficient Decomposition Identification of Deterministic Finite Automata from Examples", "comment": null, "summary": "The identification of deterministic finite automata (DFAs) from labeled\nexamples is a cornerstone of automata learning, yet traditional methods focus\non learning monolithic DFAs, which often yield a large DFA lacking simplicity\nand interoperability. Recent work addresses these limitations by exploring DFA\ndecomposition identification problems (DFA-DIPs), which model system behavior\nas intersections of multiple DFAs, offering modularity for complex tasks.\nHowever, existing DFA-DIP approaches depend on SAT encodings derived from\nAugmented Prefix Tree Acceptors (APTAs), incurring scalability limitations due\nto their inherent redundancy.\n  In this work, we advance DFA-DIP research through studying two variants: the\ntraditional Pareto-optimal DIP and the novel states-optimal DIP, which\nprioritizes a minimal number of states. We propose a novel framework that\nbridges DFA decomposition with recent advancements in automata representation.\nOne of our key innovations replaces APTA with 3-valued DFA (3DFA) derived\ndirectly from labeled examples. This compact representation eliminates\nredundancies of APTA, thus drastically reducing variables in the improved SAT\nencoding. Experimental results demonstrate that our 3DFA-based approach\nachieves significant efficiency gains for the Pareto-optimal DIP while enabling\na scalable solution for the states-optimal DIP."}
{"id": "2509.24859", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2509.24859", "abs": "https://arxiv.org/abs/2509.24859", "authors": ["Antian Liang", "Zhigang Zhao", "Kai Zhang", "Xuri Shi", "Chuantao Li", "Chunxiao Wang", "Zhenying He", "Yinan Jing", "X. Sean Wang"], "title": "HAPT: Heterogeneity-Aware Automated Parallel Training on Heterogeneous Clusters", "comment": null, "summary": "With the rapid evolution of GPU architectures, the heterogeneity of model\ntraining infrastructures is steadily increasing. In such environments,\neffectively utilizing all available heterogeneous accelerators becomes critical\nfor distributed model training. However, existing frameworks, which are\nprimarily designed for homogeneous clusters, often exhibit significant resource\nunderutilization when deployed on heterogeneous accelerators and networks. In\nthis paper, we present Hapt, an automated parallel training framework designed\nspecifically for heterogeneous clusters. Hapt introduces a fine-grained planner\nthat efficiently searches a wide space for the inter-operator parallel\nstrategy, enabling Hapt to alleviate communication overheads while maintaining\nbalanced loads across heterogeneous accelerators. In addition, Hapt implements\na heterogeneity-aware 1F1B scheduler that adaptively adjusts the execution\ntiming and ordering of microbatches based on network characteristics,\nmaximizing computation-communication overlap under cross-cluster interconnects\nwhile incurring only minimal memory overhead. Our evaluation results show that\nHapt can deliver 1.3x-1.6x higher performance on heterogeneous clusters than\nstate-of-the-art training frameworks."}
{"id": "2509.24352", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.24352", "abs": "https://arxiv.org/abs/2509.24352", "authors": ["Minghua He", "Tong Jia", "Chiming Duan", "Pei Xiao", "Lingzhe Zhang", "Kangjin Wang", "Yifan Wu", "Ying Li", "Gang Huang"], "title": "Walk the Talk: Is Your Log-based Software Reliability Maintenance System Really Reliable?", "comment": "Accepted by ASE 2025 (NIER Track)", "summary": "Log-based software reliability maintenance systems are crucial for sustaining\nstable customer experience. However, existing deep learning-based methods\nrepresent a black box for service providers, making it impossible for providers\nto understand how these methods detect anomalies, thereby hindering trust and\ndeployment in real production environments. To address this issue, this paper\ndefines a trustworthiness metric, diagnostic faithfulness, for models to gain\nservice providers' trust, based on surveys of SREs at a major cloud provider.\nWe design two evaluation tasks: attention-based root cause localization and\nevent perturbation. Empirical studies demonstrate that existing methods perform\npoorly in diagnostic faithfulness. Consequently, we propose FaithLog, a\nfaithful log-based anomaly detection system, which achieves faithfulness\nthrough a carefully designed causality-guided attention mechanism and\nadversarial consistency learning. Evaluation results on two public datasets and\none industrial dataset demonstrate that the proposed method achieves\nstate-of-the-art performance in diagnostic faithfulness."}
{"id": "2509.24932", "categories": ["cs.DC", "cs.LG", "cs.NI"], "pdf": "https://arxiv.org/pdf/2509.24932", "abs": "https://arxiv.org/abs/2509.24932", "authors": ["Fardis Nadimi", "Payam Abdisarabshali", "Jacob Chakareski", "Nicholas Mastronarde", "Seyyedali Hosseinalipour"], "title": "Graph Theory Meets Federated Learning over Satellite Constellations: Spanning Aggregations, Network Formation, and Performance Optimization", "comment": "8 Figures, 6 Appendix", "summary": "We introduce Fed-Span, a novel federated/distributed learning framework\ndesigned for low Earth orbit satellite constellations. By leveraging\ngraph-theoretic principles, Fed-Span addresses critical challenges inherent to\ndistributed learning in dynamic satellite networks, including intermittent\nsatellite connectivity, heterogeneous computational capabilities of satellites,\nand time-varying satellites' datasets. At its core, Fed-Span builds upon\nminimum spanning tree (MST) and minimum spanning forest (MSF) topologies,\nenabling spanning model aggregation and dispatching processes for distributed\nlearning. To formalize Fed-Span, we offer a fresh perspective on MST/MSF\ntopologies by formulating them through a set of continuous constraint\nrepresentations (CCRs), thereby devising graph-theoretical abstractions into an\noptimizable framework for satellite networks. Using these CCRs, we obtain the\nenergy consumption and latency of operations in Fed-Span. Moreover, we derive\nnovel convergence bounds for non-convex machine learning loss functions,\naccommodating the key system characteristics and degrees of freedom of\nFed-Span. Finally, we propose a comprehensive optimization problem that jointly\nminimizes model prediction loss, energy consumption, and latency of Fed-Span.\nWe unveil that this problem is NP-hard and develop a systematic approach to\ntransform it into a geometric programming formulation, solved via successive\nconvex optimization with performance guarantees. Through evaluations on\nreal-world datasets, we demonstrate that Fed-Span outperforms existing methods,\nwith faster model convergence, greater energy efficiency, and reduced latency.\nThese results highlight Fed-Span as a novel solution for efficient distributed\nlearning in satellite networks."}
{"id": "2509.24364", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.24364", "abs": "https://arxiv.org/abs/2509.24364", "authors": ["Minghua He", "Chiming Duan", "Pei Xiao", "Tong Jia", "Siyu Yu", "Lingzhe Zhang", "Weijie Hong", "Jin Han", "Yifan Wu", "Ying Li", "Gang Huang"], "title": "United We Stand: Towards End-to-End Log-based Fault Diagnosis via Interactive Multi-Task Learning", "comment": "ASE 2025 (Research Track)", "summary": "Log-based fault diagnosis is essential for maintaining software system\navailability. However, existing fault diagnosis methods are built using a\ntask-independent manner, which fails to bridge the gap between anomaly\ndetection and root cause localization in terms of data form and diagnostic\nobjectives, resulting in three major issues: 1) Diagnostic bias accumulates in\nthe system; 2) System deployment relies on expensive monitoring data; 3) The\ncollaborative relationship between diagnostic tasks is overlooked. Facing this\nproblems, we propose a novel end-to-end log-based fault diagnosis method,\nChimera, whose key idea is to achieve end-to-end fault diagnosis through\nbidirectional interaction and knowledge transfer between anomaly detection and\nroot cause localization. Chimera is based on interactive multi-task learning,\ncarefully designing interaction strategies between anomaly detection and root\ncause localization at the data, feature, and diagnostic result levels, thereby\nachieving both sub-tasks interactively within a unified end-to-end framework.\nEvaluation on two public datasets and one industrial dataset shows that Chimera\noutperforms existing methods in both anomaly detection and root cause\nlocalization, achieving improvements of over 2.92% - 5.00% and 19.01% - 37.09%,\nrespectively. It has been successfully deployed in production, serving an\nindustrial cloud platform."}
{"id": "2509.25041", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2509.25041", "abs": "https://arxiv.org/abs/2509.25041", "authors": ["Yu Han", "Lehan Pan", "Jie Peng", "Ziyang Tao", "Wuyang Zhang", "Yanyong Zhang"], "title": "GRACE-MoE: Grouping and Replication with Locality-Aware Routing for Efficient Distributed MoE Inference", "comment": null, "summary": "Sparse Mixture of Experts (SMoE) performs conditional computation by\nselectively activating a subset of experts, thereby enabling scalable parameter\ngrowth in large language models (LLMs). However, the expanded parameter scale\nexceeds the memory capacity of a single device, necessitating distributed\ndeployment for inference. This setup introduces two critical challenges: (1)\nCommunication Issue: Transferring features to devices with activated experts\nleads to significant communication overhead. (2) Computational Load Issue:\nSkewed expert activation overloads certain GPUs, resulting in load imbalance\nacross devices. Among these, communication overhead is identified as the main\nbottleneck in SMoE inference. Nevertheless, reducing communication between\ndevices may exacerbate computational load imbalance, leading to device idleness\nand resource waste. Therefore, we present GRACE-MoE, short for Grouping and\nReplication with Locality-Aware Routing for SMoE inference. GRACE-MoE is a\nco-optimization framework that jointly reduces communication overhead and\nalleviates computational load imbalance. Specifically, the framework comprises\ntwo key phases: (1) Grouping & Replication: This phase groups experts based on\ntheir affinity to reduce cross-device communication. Additionally, dynamic\nreplication is applied to address load skew, improving computational load\nbalance across GPUs. (2) Routing: This phase employs a locality-aware routing\nstrategy with load prediction. It prioritizes local replicas to minimize\ncommunication overhead and balances requests across remote replicas when\nnecessary. Experiments on diverse models and multi-node, multi-GPU environments\ndemonstrate that GRACE-MoE efficiently reduces end-to-end inference latency,\nachieving up to 3.79x speedup over state-of-the-art systems. Code for GRACE-MoE\nwill be released upon acceptance."}
{"id": "2509.24380", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.24380", "abs": "https://arxiv.org/abs/2509.24380", "authors": ["Shuiguang Deng", "Hailiang Zhao", "Ziqi Wang", "Guanjie Cheng", "Peng Chen", "Wenzhuo Qian", "Zhiwei Ling", "Jianwei Yin", "Albert Y. Zomaya", "Schahram Dustdar"], "title": "Agentic Services Computing", "comment": null, "summary": "The rise of LLM-powered agents is driving a fundamental transformation in\nservices computing: from static, request-response functions to dynamic,\ngoal-oriented, and autonomous multi-agent ecosystems. In response to this\nshift, we introduce Agentic Service Computing (ASC), a new paradigm that\nreimagines services as intelligent, self-adaptive, and socially embedded\nentities. This comprehensive survey presents a lifecycle-driven framework for\nASC, structured around four core phases: Design, Deployment, Operation, and\nEvolution. We systematically analyze ASC through four foundational research\ndimensions: (1) Perception, Context, and Environment Modeling, (2) Autonomous\nDecision-Making and Task Execution, (3) Multi-Agent Collaboration and\nOrganization, and (4) Evaluation, Value Alignment, and Trustworthiness. We\nexamine how these dimensions are instantiated, integrated, and continuously\nadapted across the service lifecycle. Our synthesis reveals that agentic\nservices are not merely assembled but orchestrated: contextual awareness\nenables robust deployment; autonomous reasoning supports real-time operation;\ncollaborative structures emerge and evolve through interaction; and\ntrustworthiness must be upheld as a cross-cutting, lifelong imperative. We\nfurther identify and discuss emerging trends shaping the future of ASC. By\nintegrating classical principles of services computing with advances in\nLLM-based multi-agent systems, this work establishes a holistic and\nforward-looking foundation for ASC. It provides a unified reference for\nresearchers and practitioners aiming to develop adaptive, accountable, and\nhuman-centered intelligent services."}
{"id": "2509.25121", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2509.25121", "abs": "https://arxiv.org/abs/2509.25121", "authors": ["Anvitha Ramachandran", "Dhruv Parikh", "Viktor Prasanna"], "title": "Accelerating Dynamic Image Graph Construction on FPGA for Vision GNNs", "comment": "IEEE HPEC 2025", "summary": "Vision Graph Neural Networks (Vision GNNs, or ViGs) represent images as\nunstructured graphs, achieving state of the art performance in computer vision\ntasks such as image classification, object detection, and instance\nsegmentation. Dynamic Image Graph Construction (DIGC) builds image graphs by\nconnecting patches (nodes) based on feature similarity, and is dynamically\nrepeated in each ViG layer following GNN based patch (node) feature updates.\nHowever, DIGC constitutes over 50% of end to end ViG inference latency, rising\nto 95% at high image resolutions, making it the dominant computational\nbottleneck. While hardware acceleration holds promise, prior works primarily\noptimize graph construction algorithmically, often compromising DIGC\nflexibility, accuracy, or generality. To address these limitations, we propose\na streaming, deeply pipelined FPGA accelerator for DIGC, featuring on chip\nbuffers that process input features in small, uniform blocks. Our design\nminimizes external memory traffic via localized computation and performs\nefficient parallel sorting with local merge sort and global k way merging\ndirectly on streaming input blocks via heap insertion. This modular\narchitecture scales seamlessly across image resolutions, ViG layer types, and\nmodel sizes and variants, and supports DIGC across diverse ViG based vision\nbackbones. The design achieves high clock frequencies post place and route due\nto the statically configured parallelism minimizing critical path delay and\ndelivers up to 16.6x and 6.8x speedups over optimized CPU and GPU DIGC\nbaselines."}
{"id": "2509.24419", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.24419", "abs": "https://arxiv.org/abs/2509.24419", "authors": ["Yuanhe Zhang", "Zhiquan Yang", "Shengyi Pan", "Zhongxin Liu"], "title": "Unit Test Update through LLM-Driven Context Collection and Error-Type-Aware Refinement", "comment": null, "summary": "Unit testing is critical for ensuring software quality and software system\nstability. The current practice of manually maintaining unit tests suffers from\nlow efficiency and the risk of delayed or overlooked fixes. Therefore, an\nautomated approach is required to instantly update unit tests, with the\ncapability to both repair and enhance unit tests. However, existing automated\ntest maintenance methods primarily focus on repairing broken tests, neglecting\nthe scenario of enhancing existing tests to verify new functionality.\nMeanwhile, due to their reliance on rule-based context collection and the lack\nof verification mechanisms, existing approaches struggle to handle complex code\nchanges and often produce test cases with low correctness. To address these\nchallenges, we propose TESTUPDATER, a novel LLM based approach that enables\nautomated just-in-time test updates in response to production code changes.\nTESTUPDATER first leverages the LLM to analyze code changes and identify\nrelevant context, which it then extracts and filters. Then, through carefully\ndesigned prompts, TESTUPDATER guides the LLM step by step to handle various\ntypes of code changes and introduce new dependencies, enabling both test repair\nand enhancement. Finally, we introduce an error-type-aware iterative refinement\nmechanism that executes the LLM-updated tests and repairs failures, which\nsignificantly improves the overall correctness of test updates. Since existing\ntest repair datasets lack scenarios of test enhancement, we further construct a\nnew benchmark, UPDATES4J, with 195 real-world samples from 7 projects.\nExperimental results show that TESTUPDATER achieves a compilation pass rate of\n94.4% and a test pass rate of 86.7%, outperforming the state-of-the-art method\nSYNTER by 15.9% and 20.0%, respectively. Furthermore, TESTUPDATER exhibits\n12.9% higher branch coverage and 15.2% greater line coverage than SYNTER."}
{"id": "2509.25155", "categories": ["cs.DC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.25155", "abs": "https://arxiv.org/abs/2509.25155", "authors": ["Neelesh Gupta", "Rakshith Jayanth", "Dhruv Parikh", "Viktor Prasanna"], "title": "Context-Driven Performance Modeling for Causal Inference Operators on Neural Processing Units", "comment": "IEEE HiPC 2025", "summary": "The proliferation of large language models (LLMs) has driven demand for long\ncontext inference on resource constrained edge devices. However, deploying\nthese models on Neural Processing Units (NPUs) presents significant challenges\ndue to the architectural mismatch: quadratic complexity of standard attention\nmechanisms conflicts with memory and compute patterns of edge accelerators.\nThis paper presents a comprehensive performance analysis of various causal\ninference operators on a modern NPU. We benchmark standard quadratic attention\nagainst several sub-quadratic alternatives, including structured state-space\nand linear attention models. Our analysis reveals that while sub-quadratic\nmethods offer superior scalability, they introduce distinct computational\nbottlenecks on the NPU's specialized execution units. We identify that\nquadratic attention becomes severely memory-bound, suffering from cache\ninefficiency and pipeline stalls exceeding 95% at long contexts. In contrast,\nsub-quadratic models can become compute-bound on programmable vector cores.\nThese findings provide critical insights for the co-design of hardware-aware\nmodels and optimization strategies to enable on-device AI inference with\nlong-contexts."}
{"id": "2509.24485", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.24485", "abs": "https://arxiv.org/abs/2509.24485", "authors": ["Vlad Stirbu", "Mateen Ahmed Abbasi", "Teerath Das", "Jesse Haimi", "Niko Iljin", "Pyry Kotilainen", "Petrus Lipsanen", "Niko Mäkitalo", "Maiju Sipilä", "Venla Veijalainen", "Tommi Mikkonen"], "title": "Towards Shift-Up: A Framework and a Prestudy on High-Value Activities in GenAI Native Software Development", "comment": null, "summary": "Generative AI (GenAI) has significantly influenced software engineering.\nAssociated tools have created a shift in software engineering, where\nspecialized agents, based on user-provided prompts, are replacing human\ndevelopers. In this paper, we propose a framework for GenAI native development\nthat we call \\textit{shift-up}, which helps software teams focus on high-value\nwork while being supported by GenAI. Furthermore, we also present a preliminary\nstudy testing these ideas with current GenAI tools. Towards the end of the\npaper, we propose future research goals to study shift-up in more detail."}
{"id": "2509.24498", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.24498", "abs": "https://arxiv.org/abs/2509.24498", "authors": ["Zhihao Li", "Chaozheng Wang", "Zongjie Li", "Xinyong Peng", "Zelin Su", "Qun Xia", "Haochuan Lu", "Ting Xiong", "Man Ho Lam", "Shuzheng Gao", "Yuchong Xie", "Cuiyun Gao", "Shuai Wang", "Yuetang Deng", "Huafeng Ma"], "title": "JSProtect: A Scalable Obfuscation Framework for Mini-Games in WeChat", "comment": "10 pages", "summary": "The WeChat mini-game ecosystem faces rampant intellectual property theft to\nother platforms via secondary development, yet existing JavaScript obfuscation\ntools are ill-equipped for large-scale applications, suffering from prohibitive\nprocessing times, severe runtime performance degradation, and unsustainable\ncode size inflation. This paper introduces JSProtect, a high-throughput\nparallelized obfuscation framework designed to overcome these fundamental\nlimitations. At the core of our framework is the Parallel-Aware Scope Analysis\n(PASA) algorithm, which enables two key optimizations: independent code\npartitioning for multi-core processing and independent namespace management\nthat aggressively reuses short identifiers to combat code bloat. Our evaluation\ndemonstrates that JSProtectprocesses 20MB codebases in minutes, maintaining\n100\\% semantic equivalence while controlling code size inflation to as low as\n20\\% compared to over 1,000\\% with baseline tools. Furthermore, it preserves\nnear-native runtime performance and provides superior security effectiveness\nagainst both static analysis tools and large language models. This work\npresents a new paradigm for industrial-scale JavaScript protection that\neffectively balances robust security with high performance and scalability."}
{"id": "2509.24507", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.24507", "abs": "https://arxiv.org/abs/2509.24507", "authors": ["Qinglin Wang", "Zhihong Sun", "Ruyun Wang", "Tao Huang", "Zhi Jin", "Ge Li", "Chen Lyu"], "title": "SemGuard: Real-Time Semantic Evaluator for Correcting LLM-Generated Code", "comment": "Accepted by the 40th IEEE/ACM Automated Software Engineering\n  Conference (ASE 2025)", "summary": "Large Language Models (LLMs) can translate natural language requirements into\ncode, yet empirical analyses of representative models reveal that semantic\nerrors-programs that compile but behave incorrectly-constitute the majority of\nobserved faults (e.g., >60% on DeepSeek-Coder-6.7B and QwenCoder-7B). Post-hoc\nrepair pipelines detect such faults only after execution, incurring latency,\nrelying on incomplete test suites, and often mis-localizing the defect. Since\nsemantic drift originates in the autoregressive decoding process, intervening\nwhile the code is being generated is a direct way to stop error propagation.\nConstrained-decoding approaches such as ROCODE attempt this, but still wait\nuntil the entire program runs to obtain feedback and use entropy heuristics\nthat do not truly capture semantics. A more effective solution must inject\nsemantic signals-early and precisely-into the decoding process.We present\nSemGuard, a semantic-evaluator-driven framework that performs real-time,\nline-level semantic supervision. To train the evaluator, we build SemDiff, the\nfirst dataset with fine-grained annotations that mark the exact line where a\ncorrect and an incorrect implementation diverge. The evaluator, once embedded\nin the LLM's decoder, flags deviations on partial code, rolls back to the\nfaulty line, and guides regeneration-without executing the program or requiring\ntest cases. Across four benchmarks, SemGuard consistently outperforms\nstate-of-the-art baselines. It lowers the semantic error rate by 19.86% on\nSemDiff relative to ROCODE, and lifts Pass@1 by 48.92% on the real-world\nLiveCodeBench with CodeLlama-7B. Similar gains hold for StarCoder2-7B on MBPP\nand for DeepSeekCoder-6.7B on the Java benchmark SemDiff-Java, demonstrating\nmodel- and language-agnostic effectiveness."}
{"id": "2509.24515", "categories": ["cs.SE", "cs.AI", "cs.CR", "cs.PL"], "pdf": "https://arxiv.org/pdf/2509.24515", "abs": "https://arxiv.org/abs/2509.24515", "authors": ["Yu-Fu Fu", "Meng Xu", "Taesoo Kim"], "title": "Agentic Specification Generator for Move Programs", "comment": "18 pages; Extended version of ASE'25 paper with extra appendices", "summary": "While LLM-based specification generation is gaining traction, existing tools\nprimarily focus on mainstream programming languages like C, Java, and even\nSolidity, leaving emerging and yet verification-oriented languages like Move\nunderexplored. In this paper, we introduce MSG, an automated specification\ngeneration tool designed for Move smart contracts. MSG aims to highlight key\ninsights that uniquely present when applying LLM-based specification generation\nto a new ecosystem. Specifically, MSG demonstrates that LLMs exhibit robust\ncode comprehension and generation capabilities even for non-mainstream\nlanguages. MSG successfully generates verifiable specifications for 84% of\ntested Move functions and even identifies clauses previously overlooked by\nexperts. Additionally, MSG shows that explicitly leveraging specification\nlanguage features through an agentic, modular design improves specification\nquality substantially (generating 57% more verifiable clauses than conventional\ndesigns). Incorporating feedback from the verification toolchain further\nenhances the effectiveness of MSG, leading to a 30% increase in generated\nverifiable specifications."}
{"id": "2509.24637", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.24637", "abs": "https://arxiv.org/abs/2509.24637", "authors": ["Zhensu Sun", "Chengran Yang", "Chao Peng", "Pengfei Gao", "Xiaoning Du", "Li Li", "David Lo"], "title": "Bridging Developer Instructions and Code Completion Through Instruction-Aware Fill-in-the-Middle Paradigm", "comment": "10 pages", "summary": "Large Language Models (LLMs) have significantly advanced code completion, yet\nthey often fail when the developer's intent is underspecified in the code\ncontext. To address this, developers usually add natural language instructions\n(e.g., comments) into the code context to clarify their intent. However,\nexisting code LLMs applied for code completion systems merely undergo a\nfill-in-the-middle (FIM) pre-training, which struggles to leverage this\ninformation effectively due to the lack of instruction-like training data.\nExisting instruction-tuning techniques, which improve instruction-following in\ngeneral code generation, paradoxically degrade FIM performance, forcing a\ntrade-off between instruction-following and infilling capabilities. To address\nthis gap, we introduce Instruction-aware Fill-in-the-Middle (IFIM), an\ninstruction-tuning method specifically designed to enhance FIM code completion\nmodels. IFIM extends the conventional FIM training objective by incorporating\nan explicit instruction section into the input, enabling the model to learn\nfrom (prefix, instruction, suffix) triplets. This approach allows the model to\neffectively leverage developer-provided directives while preserving its core\ncompletion abilities when no instructions are present. To facilitate this, we\nconstructed a large-scale dataset by using GPT-4o to generate concise,\nintent-focused instructions for code infilling examples. We evaluated IFIM by\napplying it to two popular base models, Deepseek-Coder and Qwen2.5-Coder, on\nthe benchmarks derived from HumanEval-infilling and RepoMasterEval. The results\ndemonstrate that IFIM significantly improves instruction-following\ncapabilities, boosting the Pass@1 score from 84.6% to 93.6% on\nHumanEval-infilling. Moreover, this enhancement does not compromise the models'\noriginal performance on FIM code completion tasks with no instructions\nprovided."}
{"id": "2509.24694", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.24694", "abs": "https://arxiv.org/abs/2509.24694", "authors": ["Gangda Xiong", "Tao Chen"], "title": "CoTune: Co-evolutionary Configuration Tuning", "comment": "Accepted by ASE 2025", "summary": "To automatically tune configurations for the best possible system performance\n(e.g., runtime or throughput), much work has been focused on designing\nintelligent heuristics in a tuner. However, existing tuner designs have mostly\nignored the presence of complex performance requirements (e.g., the latency\nshall ideally be 2 seconds), but simply assume that better performance is\nalways more preferred. This would not only waste valuable information in a\nrequirement but might also consume extensive resources to tune for a goal with\nlittle gain. Yet, prior studies have shown that simply incorporating the\nrequirement as a tuning objective is problematic since the requirement might be\ntoo strict, harming convergence; or its highly diverse satisfactions might lead\nto premature convergence. In this paper, we propose CoTune, a tool that takes\nthe information of a given target performance requirement into account through\nco-evolution. CoTune is unique in the sense that it creates an auxiliary\nperformance requirement to be co-evolved with the configurations, which assists\nthe target performance requirement when it becomes ineffective or even\nmisleading, hence allowing the tuning to be guided by the requirement while\nbeing robust to its harm. Experiment results on 162 cases (nine systems and 18\nrequirements) reveal that CoTune considerably outperforms existing tuners,\nranking as the best for 90% cases (against the 0%--35% for other tuners) with\nup to 2.9x overall improvements, while doing so under a much better efficiency."}
{"id": "2509.24782", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.24782", "abs": "https://arxiv.org/abs/2509.24782", "authors": ["Muhammad Laiq"], "title": "Large language models for behavioral modeling: A literature survey", "comment": "9 pages", "summary": "In recent years, large language models (LLMs) have been extensively utilized\nfor behavioral modeling, for example, to automatically generate sequence\ndiagrams. However, no overview of this work has been published yet. Such an\noverview will help identify future research directions and inform practitioners\nand educators about the effectiveness of LLMs in assisting behavioral modeling.\nThis study aims to provide an overview of the existing research on the use of\nLLMs for behavioral modeling, particularly focusing on use case and sequence\ndiagrams. Through a term-based search, we filtered and identified 14 relevant\nprimary studies. Our analysis of the selected primary studies reveals that LLMs\nhave demonstrated promising results in automatically generating use case and\nsequence diagrams. In addition, we found that most of the current literature\nlacks expert-based evaluations and has mainly used GPT-based models. Therefore,\nfuture work should evaluate a broader range of LLMs for behavioral modeling and\ninvolve domain experts to evaluate the output of LLMs."}
{"id": "2509.24828", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.24828", "abs": "https://arxiv.org/abs/2509.24828", "authors": ["Joshua Heisler", "Johannes Reisinger", "Andreas Fischer"], "title": "Evaluating SAP Joule for Code Generation", "comment": null, "summary": "SAP has released its own proprietary generative model SAP Joule, intended for\nvarious generative tasks, including serving as a code assistant for software\nengineers. While Joule is yet not focused on SAP-specific ABAP code generation,\nit can be used for other common languages, including Javascript. This paper\ncompares SAP Joules Javascript coding capabilities against a total of 29 other\nmodels using the HumanEval-X Javascript benchmark. SAP Joule achieves a strict\naccuracy of 80.49% as the fifth best model in our evaluation. To the best of\nour knowledge, this is the first comparative evaluation of SAP Joule code\ngeneration capabilities."}
{"id": "2509.24975", "categories": ["cs.SE", "cs.CL"], "pdf": "https://arxiv.org/pdf/2509.24975", "abs": "https://arxiv.org/abs/2509.24975", "authors": ["Lekang Yang", "Yuetong Liu", "Yitong Zhang", "Jia Li"], "title": "DiffTester: Accelerating Unit Test Generation for Diffusion LLMs via Repetitive Pattern", "comment": null, "summary": "Software development relies heavily on extensive unit testing, which makes\nthe efficiency of automated Unit Test Generation (UTG) particularly important.\nHowever, most existing LLMs generate test cases one token at a time in each\nforward pass, which leads to inefficient UTG. Recently, diffusion LLMs (dLLMs)\nhave emerged, offering promising parallel generation capabilities and showing\nstrong potential for efficient UTG. Despite this advantage, their application\nto UTG is still constrained by a clear trade-off between efficiency and test\nquality, since increasing the number of tokens generated in each step often\ncauses a sharp decline in the quality of test cases. To overcome this\nlimitation, we present DiffTester, an acceleration framework specifically\ntailored for dLLMs in UTG. The key idea of DiffTester is that unit tests\ntargeting the same focal method often share repetitive structural patterns. By\ndynamically identifying these common patterns through abstract syntax tree\nanalysis during generation, DiffTester adaptively increases the number of\ntokens produced at each step without compromising the quality of the output. To\nenable comprehensive evaluation, we extend the original TestEval benchmark,\nwhich was limited to Python, by introducing additional programming languages\nincluding Java and C++. Extensive experiments on three benchmarks with two\nrepresentative models show that DiffTester delivers significant acceleration\nwhile preserving test coverage. Moreover, DiffTester generalizes well across\ndifferent dLLMs and programming languages, providing a practical and scalable\nsolution for efficient UTG in software development. Code and data are publicly\navailable at https://github.com/wellbeingyang/DLM4UTG-open ."}
{"id": "2509.25043", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.25043", "abs": "https://arxiv.org/abs/2509.25043", "authors": ["Cristian Augusto", "Antonia Bertolino", "Guglielmo De Angelis", "Francesca Lonetti", "Jesús Morán"], "title": "Large Language Models for Software Testing: A Research Roadmap", "comment": "40 pages & 10 figures Submitted on 29th September 2025", "summary": "Large Language Models (LLMs) are starting to be profiled as one of the most\nsignificant disruptions in the Software Testing field.\n  Specifically, they have been successfully applied in software testing tasks\nsuch as generating test code, or summarizing documentation.\n  This potential has attracted hundreds of researchers, resulting in dozens of\nnew contributions every month, hardening researchers to\n  stay at the forefront of the wave. Still, to the best of our knowledge, no\nprior work has provided a structured vision of the progress\n  and most relevant research trends in LLM-based testing. In this article, we\naim to provide a roadmap that illustrates its current state,\n  grouping the contributions into different categories, and also sketching the\nmost promising and active research directions for the field.\n  To achieve this objective, we have conducted a semi-systematic literature\nreview, collecting articles and mapping them into the most\n  prominent categories, reviewing the current and ongoing status, and analyzing\nthe open challenges of LLM-based software testing.\n  Lastly, we have outlined several expected long-term impacts of LLMs over the\nwhole software testing field."}
{"id": "2509.25117", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.25117", "abs": "https://arxiv.org/abs/2509.25117", "authors": ["Sogol Masoumzadeh", "Keheliya Gallaba", "Dayi Lin", "Ahmed E. Hassan"], "title": "Towards Reliable Generation of Executable Workflows by Foundation Models", "comment": null, "summary": "Recent advancements in Foundation Models (FMs) have demonstrated significant\nprogress in comprehending complex natural language to perform intricate tasks.\nSuccessfully executing these tasks often requires orchestrating calls to FMs\nalongside other software components. However, manually decomposing a task into\na coherent sequence of smaller, logically aggregated steps, commonly referred\nto as workflows, demands considerable effort and specialized domain knowledge.\nWhile FMs can assist in generating such workflows specified in domain-specific\nlanguages (DSLs), achieving accuracy and reliability in this process remains a\nchallenge.\n  This work introduces a framework that leverages static analysis feedback to\nenable FMs to detect and repair defects in the DSL-based workflows they\ngenerate. We begin by presenting the first-ever taxonomy of incidences of\ndefects in FM-generated DSL workflows, categorizing them into 18 distinct\ntypes. Furthermore, we observe a high prevalence of defects across FM-generated\nDSL workflows, with 87.27% of the studied instances containing at least one\ndefect. This, in turn, emphasizes the magnitude of the problem in practice and\nunderscores the necessity for implementing mitigation strategies. Following\nthis, we demonstrate that nine types of these defects can be effectively\nidentified through static analysis of the workflows. For this purpose, we\ndevelop Timon, the first-of-its-kind static analyzer specifically designed for\nFM-generated DSL workflows. Finally, we show that by incorporating feedback\nfrom Timon, we can guide Pumbaa, an FM-based tool, to repair the detected\ndefect incidences. By systematically detecting and repairing defects, our work\nprovides a crucial step towards the reliable and automated generation of\nexecutable workflows from natural language requirements."}
{"id": "2509.22704", "categories": ["cs.DC", "cs.AI", "cs.SE"], "pdf": "https://arxiv.org/pdf/2509.22704", "abs": "https://arxiv.org/abs/2509.22704", "authors": ["Leszek Sliwko"], "title": "Intelligent Load Balancing in Cloud Computer Systems", "comment": "A thesis submitted in partial fulfilment of the requirements of the\n  University of Westminster for the degree of Doctor of Philosophy", "summary": "Cloud computing is an established technology allowing users to share\nresources on a large scale, never before seen in IT history. A cloud system\nconnects multiple individual servers in order to process related tasks in\nseveral environments at the same time. Clouds are typically more cost-effective\nthan single computers of comparable computing performance. The sheer physical\nsize of the system itself means that thousands of machines may be involved. The\nfocus of this research was to design a strategy to dynamically allocate tasks\nwithout overloading Cloud nodes which would result in system stability being\nmaintained at minimum cost. This research has added the following new\ncontributions to the state of knowledge: (i) a novel taxonomy and\ncategorisation of three classes of schedulers, namely OS-level, Cluster and Big\nData, which highlight their unique evolution and underline their different\nobjectives; (ii) an abstract model of cloud resources utilisation is specified,\nincluding multiple types of resources and consideration of task migration\ncosts; (iii) a virtual machine live migration was experimented with in order to\ncreate a formula which estimates the network traffic generated by this process;\n(iv) a high-fidelity Cloud workload simulator, based on a month-long workload\ntraces from Google's computing cells, was created; (v) two possible approaches\nto resource management were proposed and examined in the practical part of the\nmanuscript: the centralised metaheuristic load balancer and the decentralised\nagent-based system. The project involved extensive experiments run on the\nUniversity of Westminster HPC cluster, and the promising results are presented\ntogether with detailed discussions and a conclusion."}
{"id": "2509.24030", "categories": ["cs.DC", "cs.AI", "cs.SE"], "pdf": "https://arxiv.org/pdf/2509.24030", "abs": "https://arxiv.org/abs/2509.24030", "authors": ["Anjus George", "Michael Brim", "Christopher Zimmer", "David Rogers", "Sarp Oral", "Zach Mayes"], "title": "From Edge to HPC: Investigating Cross-Facility Data Streaming Architectures", "comment": null, "summary": "In this paper, we investigate three cross-facility data streaming\narchitectures, Direct Streaming (DTS), Proxied Streaming (PRS), and Managed\nService Streaming (MSS). We examine their architectural variations in data flow\npaths and deployment feasibility, and detail their implementation using the\nData Streaming to HPC (DS2HPC) architectural framework and the SciStream\nmemory-to-memory streaming toolkit on the production-grade Advanced Computing\nEcosystem (ACE) infrastructure at Oak Ridge Leadership Computing Facility\n(OLCF). We present a workflow-specific evaluation of these architectures using\nthree synthetic workloads derived from the streaming characteristics of\nscientific workflows. Through simulated experiments, we measure streaming\nthroughput, round-trip time, and overhead under work sharing, work sharing with\nfeedback, and broadcast and gather messaging patterns commonly found in AI-HPC\ncommunication motifs. Our study shows that DTS offers a minimal-hop path,\nresulting in higher throughput and lower latency, whereas MSS provides greater\ndeployment feasibility and scalability across multiple users but incurs\nsignificant overhead. PRS lies in between, offering a scalable architecture\nwhose performance matches DTS in most cases."}
