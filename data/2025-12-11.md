<div id=toc></div>

# Table of Contents

- [cs.MA](#cs.MA) [Total: 2]
- [cs.AR](#cs.AR) [Total: 2]
- [cs.OS](#cs.OS) [Total: 1]
- [cs.SE](#cs.SE) [Total: 8]
- [cs.DC](#cs.DC) [Total: 8]
- [cs.NI](#cs.NI) [Total: 1]


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [1] [WOLF: Werewolf-based Observations for LLM Deception and Falsehoods](https://arxiv.org/abs/2512.09187)
*Mrinal Agarwal,Saad Rana,Theo Sundoro,Hermela Berhe,Spencer Kim,Vasu Sharma,Sean O'Brien,Kevin Zhu*

Main category: cs.MA

TL;DR: WOLF是一个基于狼人杀的多智能体社交推理基准，用于分别衡量欺骗生成与检测能力，通过结构化日志和动态交互提升评估真实性。


<details>
  <summary>Details</summary>
Motivation: 现有欺骗评估多为静态分类，忽视了真实场景中对抗性、交互性和长期性的欺骗动态，需构建更贴近现实的测试环境。

Method: 设计基于LangGraph状态机的WOLF基准，嵌入角色化智能体，设定昼夜循环、辩论回合与投票机制，对每句话标注诚实度与欺骗性，并按标准分类欺骗类型。

Result: 狼人31%回合实施欺骗，同伴检测精度71-73%，整体准确率约52%；随轮次增加，对狼人怀疑度从52%升至60%，对村民和医生稳定在44-46%。

Conclusion: WOLF提供了一个动态可控的对抗性多智能体交互测试平台，能有效分离并测量欺骗与识骗能力，超越静态数据集评估局限。

Abstract: Deception is a fundamental challenge for multi-agent reasoning: effective systems must strategically conceal information while detecting misleading behavior in others. Yet most evaluations reduce deception to static classification, ignoring the interactive, adversarial, and longitudinal nature of real deceptive dynamics. Large language models (LLMs) can deceive convincingly but remain weak at detecting deception in peers. We present WOLF, a multi-agent social deduction benchmark based on Werewolf that enables separable measurement of deception production and detection. WOLF embeds role-grounded agents (Villager, Werewolf, Seer, Doctor) in a programmable LangGraph state machine with strict night-day cycles, debate turns, and majority voting. Every statement is a distinct analysis unit, with self-assessed honesty from speakers and peer-rated deceptiveness from others. Deception is categorized via a standardized taxonomy (omission, distortion, fabrication, misdirection), while suspicion scores are longitudinally smoothed to capture both immediate judgments and evolving trust dynamics. Structured logs preserve prompts, outputs, and state transitions for full reproducibility. Across 7,320 statements and 100 runs, Werewolves produce deceptive statements in 31% of turns, while peer detection achieves 71-73% precision with ~52% overall accuracy. Precision is higher for identifying Werewolves, though false positives occur against Villagers. Suspicion toward Werewolves rises from ~52% to over 60% across rounds, while suspicion toward Villagers and the Doctor stabilizes near 44-46%. This divergence shows that extended interaction improves recall against liars without compounding errors against truthful roles. WOLF moves deception evaluation beyond static datasets, offering a dynamic, controlled testbed for measuring deceptive and detective capacity in adversarial multi-agent interaction.

</details>


### [2] [GAIR: GUI Automation via Information-Joint Reasoning and Group Reflection](https://arxiv.org/abs/2512.09396)
*Zishu Wei,Qixiang Ma,Xavier Hu,Yuhang Liu,Hui Zang,Yudong Zhao,Tao Wang,Shengyu Zhang,Fei Wu*

Main category: cs.MA

TL;DR: GAIR框架通过信息联合推理与群体反思，整合异构模型能力以提升GUI自动化性能。


<details>
  <summary>Details</summary>
Motivation: GUI自动化任务多样且复杂，需要MLLM具备多维能力，但构建此类模型存在挑战。

Method: 提出GAIR框架，利用通用MLLM整合多个GUI专用模型的信息，并在信息不足时触发群体反思机制以优化决策。

Result: 在GUI基准测试中，GAIR表现出更高的性能和可靠性。

Conclusion: GAIR有效整合异构模型知识与能力，显著提升GUI自动化系统的决策质量与执行效果。

Abstract: Building AI systems for GUI automation task has attracted remarkable research efforts, where MLLMs are leveraged for processing user requirements and give operations. However, GUI automation includes a wide range of tasks, from document processing to online shopping, from CAD to video editing. Diversity between particular tasks requires MLLMs for GUI automation to have heterogeneous capabilities and master multidimensional expertise, raising problems on constructing such a model. To address such challenge, we propose GAIR: GUI Automation via Information-Joint Reasoning and Group Reflection, a novel MLLM-based GUI automation agent framework designed for integrating knowledge and combining capabilities from heterogeneous models to build GUI automation agent systems with higher performance. Since different GUI-specific MLLMs are trained on different dataset and thus have different strengths, GAIR introduced a general-purpose MLLM for jointly processing the information from multiple GUI-specific models, further enhancing performance of the agent framework. The general-purpose MLLM also serves as decision maker, trying to execute a reasonable operation based on previously gathered information. When the general-purpose model thinks that there isn't sufficient information for a reasonable decision, GAIR would transit into group reflection status, where the general-purpose model would provide GUI-specific models with different instructions and hints based on their strengths and weaknesses, driving them to gather information with more significance and accuracy that can support deeper reasoning and decision. We evaluated the effectiveness and reliability of GAIR through extensive experiments on GUI benchmarks.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [3] [RACAM: Enhancing DRAM with Reuse-Aware Computation and Automated Mapping for ML Inference](https://arxiv.org/abs/2512.09304)
*Siyuan Ma,Jiajun Hu,Jeeho Ryoo,Aman Arora,Lizy Kurian John*

Main category: cs.AR

TL;DR: RACAM是一种新型的DRAM内位串行架构，通过局部性缓冲区和工作负载映射机制，显著提升LLM推理性能。


<details>
  <summary>Details</summary>
Motivation: 现有DRAM-PIM方案缺乏数据复用、存在冗余数据传输且工作负载映射支持不足。

Method: 引入专用局部缓冲区、位串行处理单元、popcount缩减单元与广播单元，并设计工作负载映射机制。

Result: 相比GPU提速9-102倍，相比Proteus在GPT3上实现233倍/mm²性能提升。

Conclusion: RACAM有效解决DRAM-PIM现存瓶颈，大幅提升能效与并行效率。

Abstract: In-DRAM Processing-In-Memory (DRAM-PIM) has emerged as a promising approach to accelerate memory-intensive workloads by mitigating data transfer overhead between DRAM and the host processor. Bit-serial DRAM-PIM architectures, further enhance efficiency by supporting runtime variable data precision, which is critical for emerging workloads, such as large language model (LLM) inference. However, existing works still have major limitations: lack of data reuse, significant amounts of redundant data transfer, and insufficient support for workload mapping. To address these issues, we propose RACAM, the first in-DRAM bit-serial architecture which uses dedicated locality buffers, bit-serial PEs, popcount reduction units and broadcast units to enable data reuse and alleviate redundant data transfers. Furthermore, a workload mapping mechanism is proposed to fully explore the massive parallelism of DRAM architecture and identify the best mapping scheme of a given workload. We evaluate RACAM against GPUs and the state-of-the-art, in-DRAM PIM system, Proteus, across end-to-end LLM inferences. RACAM achieves 9x to 102x speedup over GPUs and 233x higher performance per mm2 compared to Proteus in case of GPT3.

</details>


### [4] [ODMA: On-Demand Memory Allocation Framework for LLM Serving on LPDDR-Class Accelerators](https://arxiv.org/abs/2512.09427)
*Guoqiang Zou,Wanyu Wang,Hao Zheng,Longxiang Yin,Yinhe Han*

Main category: cs.AR

TL;DR: ODMA是一种针对随机访问受限内存（RACM）加速器的按需内存分配框架，显著提升LLM服务效率。


<details>
  <summary>Details</summary>
Motivation: 现有内存管理方法在RACM设备上效率低下，无法兼顾内存利用率与访问性能。

Method: 结合轻量级长度预测、动态桶分区和大桶保护机制，周期性根据运行轨迹更新边界以优化内存利用。

Result: 在Cambricon MLU370-X4上运行DeepSeek-R1-Distill-Qwen-7B，内存利用率从55.05%提升至72.45%，RPS和TPS分别提高29%和27%。

Conclusion: 硬件感知的内存分配策略能有效释放RACM平台上的LLM服务潜力。

Abstract: Serving large language models (LLMs) on accelerators with poor random-access bandwidth (e.g., LPDDR5-based) is limited by current memory managers. Static pre-allocation wastes memory, while fine-grained paging (e.g., PagedAttention) is ill-suited due to high random-access costs. Existing HBM-centric solutions do not exploit the characteristics of random-access-constrained memory (RACM) accelerators like Cambricon MLU370. We present ODMA, an on-demand memory allocation framework for RACM. ODMA addresses distribution drift and heavy-tailed requests by coupling a lightweight length predictor with dynamic bucket partitioning and a large-bucket safeguard. Boundaries are periodically updated from live traces to maximize utilization. On Alpaca and Google-NQ, ODMA improves prediction accuracy of prior work significantly (e.g., from 82.68% to 93.36%). Serving DeepSeek-R1-Distill-Qwen-7B on Cambricon MLU370-X4, ODMA raises memory utilization from 55.05% to 72.45% and improves RPS and TPS by 29% and 27% over static baselines. This demonstrates that hardware-aware allocation unlocks efficient LLM serving on RACM platforms.

</details>


<div id='cs.OS'></div>

# cs.OS [[Back]](#toc)

### [5] [ZeroOS: A Universal Modular Library OS for zkVMs](https://arxiv.org/abs/2512.09300)
*Guangxian Zou,Isaac Zhang,Ryan Zarick,Kelvin Wong,Thomas Kim,Daniel L. -K. Wong,Saeid Yazdinejad,Dan Boneh*

Main category: cs.OS

TL;DR: ZeroOS 是一个为可验证应用（vApp）设计的模块化库操作系统，旨在解决 zkVM 中因静态链接运行时导致的版本混乱和可信计算基过大问题。


<details>
  <summary>Details</summary>
Motivation: 现代程序依赖操作系统和 libc，而 zkVM 通过维护语言特定运行时的分支并静态链接，导致版本管理困难和可信计算基膨胀。

Method: 开发 ZeroOS，允许开发者仅链接其 vApp 所需的 Linux ABI 子集，并通过编写引导加载程序使 zkVM 团队轻松接入 ZeroOS 生态。

Result: 降低 zkVM 维护负担，统一 zkVM 生态系统，集中开发与审计资源。

Conclusion: ZeroOS 提供了更高效、轻量且可扩展的解决方案，推动 zkVM 生态的发展。

Abstract: zkVMs promise general-purpose verifiable computation through ISA-level compatibility with modern programs and toolchains. However, compatibility extends further than just the ISA; modern programs often cannot run or even compile without an operating system and libc. zkVMs attempt to address this by maintaining forks of language-specific runtimes and statically linking them into applications to create self-contained unikernels, but this ad-hoc approach leads to version hell and burdens verifiable applications (vApps) with an unnecessarily large trusted computing base. We solve this problem with ZeroOS, a modular library operating system (libOS) for vApp unikernels; vApp developers can use off-the-shelf toolchains to compile and link only the exact subset of the Linux ABI their vApp needs. Any zkVM team can easily leverage the ZeroOS ecosystem by writing a ZeroOS bootloader for their platform, resulting in a reduced maintainence burden and unifying the entire zkVM ecosystem with consolidated development and audit resources. ZeroOS is free and open-sourced at https://github.com/LayerZero-Labs/ZeroOS.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [6] [Llama-based source code vulnerability detection: Prompt engineering vs Fine tuning](https://arxiv.org/abs/2512.09006)
*Dyna Soumhane Ouchebara,Stéphane Dupont*

Main category: cs.SE

TL;DR: 本文研究了大型语言模型（LLMs）在源代码漏洞检测中的应用，提出双微调方法并验证其有效性，同时探讨提示工程与RAG技术的表现。


<details>
  <summary>Details</summary>
Motivation: 软件开发周期加速导致漏洞激增，亟需自动化检测方法，AI尤其是LLM成为新兴解决方案。

Method: 基于Llama-3.1 8B模型，采用BigVul和PrimeVul数据集，测试多种微调策略（含新颖的Double Fine-tuning和Test-Time微调）及提示工程技术。

Result: 微调对任务至关重要，双微调表现优异，Llama模型具潜力；提示效果不佳，但RAG作为示例选择技术表现尚可。

Conclusion: 部分研究问题已解答，仍存大量待探索方向，为后续研究提供基础。

Abstract: The significant increase in software production, driven by the acceleration of development cycles over the past two decades, has led to a steady rise in software vulnerabilities, as shown by statistics published yearly by the CVE program. The automation of the source code vulnerability detection (CVD) process has thus become essential, and several methods have been proposed ranging from the well established program analysis techniques to the more recent AI-based methods. Our research investigates Large Language Models (LLMs), which are considered among the most performant AI models to date, for the CVD task. The objective is to study their performance and apply different state-of-the-art techniques to enhance their effectiveness for this task. We explore various fine-tuning and prompt engineering settings. We particularly suggest one novel approach for fine-tuning LLMs which we call Double Fine-tuning, and also test the understudied Test-Time fine-tuning approach. We leverage the recent open-source Llama-3.1 8B, with source code samples extracted from BigVul and PrimeVul datasets. Our conclusions highlight the importance of fine-tuning to resolve the task, the performance of Double tuning, as well as the potential of Llama models for CVD. Though prompting proved ineffective, Retrieval augmented generation (RAG) performed relatively well as an example selection technique. Overall, some of our research questions have been answered, and many are still on hold, which leaves us many future work perspectives. Code repository is available here: https://github.com/DynaSoumhaneOuchebara/Llama-based-vulnerability-detection.

</details>


### [7] [TritonForge: Profiling-Guided Framework for Automated Triton Kernel Optimization](https://arxiv.org/abs/2512.09196)
*Haonan Li,Keyu Man,Partha Kanuparthy,Hanning Chen,Wei Sun,Sreen Tallam,Chenguang Zhu,Kevin Zhu,Zhiyun Qian*

Main category: cs.SE

TL;DR: TritonForge是一个基于性能分析的自动化Triton内核优化框架，通过集成内核分析、运行时剖析和迭代代码转换，在多种GPU架构上平均实现1.76倍性能提升。


<details>
  <summary>Details</summary>
Motivation: 高性能GPU内核优化依赖专家经验且耗时费力，亟需自动化工具降低门槛并提升效率。

Method: 结合运行时性能剖析数据，自动识别瓶颈、提出代码修改建议并评估效果；当前原型使用LLM辅助推理，但框架本身模块化且模型无关。

Result: 在多种内核类型和GPU架构上最高达5倍加速，平均1.76倍成功提升性能。

Conclusion: TritonForge为自动化GPU性能优化研究提供了有效基础框架。

Abstract: High-performance GPU kernel optimization remains a critical yet labor-intensive task in modern machine learning workloads. Although Triton, a domain-specific language for GPU programming, enables developers to write efficient kernels with concise code, achieving expert-level performance still requires deep understanding of GPU architectures and low-level performance trade-offs. We present TritonForge, a profiling-guided framework for automated Triton kernel optimization. TritonForge integrates kernel analysis, runtime profiling, and iterative code transformation to streamline the optimization process. By incorporating data-driven feedback from profiling results, the system identifies performance bottlenecks, proposes targeted code modifications, and evaluates their impact automatically. While our prototype leverages large language models (LLMs) to assist in code reasoning and transformation, the framework remains modular and model-agnostic. Across diverse kernel types and GPU architectures, TritonForge achieves up to 5x performance improvement over baseline implementations and on average 1.76x of the cases are successful, providing a foundation for future research in automated GPU performance optimization.

</details>


### [8] [Bug Priority Change Prediction: An Exploratory Study on Apache Software](https://arxiv.org/abs/2512.09216)
*Guangzong Cai,Zengyang Li,Peng Liang,Ran Mo,Hui Liu,Yutao Ma*

Main category: cs.SE

TL;DR: 提出了一种基于缺陷修复演化特征和类别不平衡处理策略的两阶段缺陷报告优先级变更预测方法，显著提升了预测性能。


<details>
  <summary>Details</summary>
Motivation: 手动评估缺陷优先级变更依赖主观判断，易出错且效率低，亟需自动化预测方法。

Method: 将缺陷生命周期分为报告与修复两阶段，分别构建预测模型，并引入演化特征与类别不平衡处理策略。

Result: 在Apache项目数据集上，报告阶段F1-score达0.798，修复阶段F1-weighted为0.712、F1-macro为0.613；跨项目表现差异大但整体良好，各优先级预测性能稳定。

Conclusion: 所提方法能有效提升缺陷优先级变更预测准确性，有助于优化缺陷管理流程。

Abstract: Bug fixing is a critical activity in the software development process. In issue tracking systems such as JIRA, each bug report is assigned a priority level to indicate the urgency and importance level of the bug. The priority may change during the bug fixing process, indicating that the urgency and importance level of the bug will change with the bug fixing. However, manually evaluating priority changes for bugs is a tedious process that heavily relies on the subjective judgment of developers and project managers, leading to incorrect priority changes and thus hindering timely bug fixes. Given the lack of research on bug priority change prediction, we propose a novel two-phase bug report priority change prediction method based on bug fixing evolution features and class imbalance handling strategy. Specifically, we divided the bug lifecycle into two phases: bug reporting and bug fixing, and constructed bug priority change prediction models for each phase. To evaluate the performance of our method, we conducted experiments on a bug dataset constructed from 32 non-trivial Apache projects. The experimental results show that our proposed bug fixing evolution features and the adopted class imbalance handling strategy can effectively improve the performance of prediction models. The F1-score of the prediction model constructed for the bug reporting phase reached 0.798, while the F1-weighted and F1-macro of the prediction model constructed for the bug fixing phase were 0.712 and 0.613, respectively. Furthermore, we explored the cross-project applicability of our prediction models and their performance at different priority levels. The findings indicate large variations in model performance across different projects, although the overall scores remain decent. Meanwhile, the predictive performance across various priority levels remained relatively consistently high.

</details>


### [9] [Explainable Verification of Hierarchical Workflows Mined from Event Logs with Shapley Values](https://arxiv.org/abs/2512.09562)
*Radoslaw Klimek,Jakub Blazowski*

Main category: cs.SE

TL;DR: 该论文提出结合工作流挖掘与逻辑分析，利用Shapley值量化流程元素贡献，以实现可解释的工作流分析。


<details>
  <summary>Details</summary>
Motivation: 现有工作流模型缺乏对逻辑属性满足或违反原因的解释，也未能量化各元素对整体行为的影响。

Method: 将挖掘出的工作流转换为逻辑规范，使用自动定理证明器分析属性，并引入Shapley值量化元素贡献。

Result: 实验表明该方法能识别关键节点、揭示冗余并暴露有害结构，提升工作流可解释性。

Conclusion: 该方法为可解释工作流分析开辟新方向，有助于合规检查、流程优化与下一代流程挖掘工具设计。

Abstract: Workflow mining discovers hierarchical process trees from event logs, but it remains unclear why such models satisfy or violate logical properties, or how individual elements contribute to overall behavior. We propose to translate mined workflows into logical specifications and analyze properties such as satisfiability, liveness, and safety with automated theorem provers. On this basis, we adapt Shapley values from cooperative game theory to attribute outcomes to workflow elements and quantify their contributions. Experiments on benchmark datasets show that this combination identifies critical nodes, reveals redundancies, and exposes harmful structures. This outlines a novel direction for explainable workflow analysis with direct relevance to software engineering practice, supporting compliance checks, process optimization, redundancy reduction, and the design of next-generation process mining tools.

</details>


### [10] [Model management to support systems engineering workflows using ontology-based knowledge graphs](https://arxiv.org/abs/2512.09596)
*Arkadiusz Ryś,Lucas Lima,Joeri Exelmans,Dennis Janssens,Hans Vangheluwe*

Main category: cs.SE

TL;DR: 提出基于本体的知识图谱框架，用于管理CPS系统工程中的建模产物，支持存储、版本控制、查询与推理，提升开发效率。


<details>
  <summary>Details</summary>
Motivation: 应对模型驱动工程中多领域工作流产生的异构建模产物的存储、访问与复用难题，提升系统开发的可重复性与知识推理能力。

Method: 构建OML本体定义工作流概念与产物，建立知识图谱并开发配套工具链，支持设计、执行、存储、查询与推理，屏蔽底层复杂性。

Result: 在传动系智能传感器系统案例中验证框架有效性，显著改善存储与版本管理，缩短信息获取时间，并支持从图谱中推导新知识。

Conclusion: 该框架有效整合异构建模资产，通过知识图谱与工具链增强系统工程效率与智能化水平，具备实际部署价值。

Abstract: System engineering has been shifting from document-centric to model-based approaches, where assets are becoming more and more digital. Although digitisation conveys several benefits, it also brings several concerns (e.g., storage and access) and opportunities. In the context of Cyber- Physical Systems (CPS), we have experts from various domains executing complex workflows and manipulating models in a plethora of different formalisms, each with their own methods, techniques and tools. Storing knowledge on these workflows can reduce considerable effort during system development not only to allow their repeatability and replicability but also to access and reason on data generated by their execution. In this work, we propose a framework to manage modelling artefacts generated from workflow executions. The basic workflow concepts, related formalisms and artefacts are formally defined in an ontology specified in OML (Ontology Modelling Language). This ontology enables the construction of a knowledge graph that contains system engineering data to which we can apply reasoning. We also developed several tools to support system engineering during the design of workflows, their enactment, and artefact storage, considering versioning, querying and reasoning on the stored data. These tools also hide the complexity of manipulating the knowledge graph directly. Finally, we have applied our proposed framework in a real-world system development scenario of a drivetrain smart sensor system. Results show that our proposal not only helped the system engineer with fundamental difficulties like storage and versioning but also reduced the time needed to access relevant information and new knowledge that can be inferred from the knowledge graph.

</details>


### [11] [LogICL: Distilling LLM Reasoning to Bridge the Semantic Gap in Cross-Domain Log Anomaly Detection](https://arxiv.org/abs/2512.09627)
*Jingwei Ye,Zhi Wang,Chenbin Su,Jieshuai Yang,Jiayi Ding,Chunbo Liu,Ge Chu*

Main category: cs.SE

TL;DR: LogICL利用大语言模型推理能力，通过轻量编码器实现跨域日志异常检测，在资源有限场景下表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖表层词汇相似性，难以应对结构差异下的语义等价问题，且冷启动时数据稀缺。

Method: 构建delta矩阵衡量示例效用，结合多目标损失优化编码器，推理时利用语义相似与delta分数检索示例，驱动冻结LLM进行思维链推理。

Result: 在少样本与零样本跨域基准上达到最先进性能，可视化与案例分析验证其有效弥合语义鸿沟。

Conclusion: LogICL显著提升跨域日志异常检测的准确性与可解释性，支持快速部署。

Abstract: Effective log anomaly detection is critical to sustaining reliability in large-scale IT infrastructures. Transformer-based models require substantial resources and labeled data, exacerbating the cold-start problem in target domains where logs are scarce. Existing cross-domain methods leverage source logs but struggle with generalization due to reliance on surface lexical similarity, failing to capture latent semantic equivalence amid structural divergences. To address this, we propose LogICL, a framework distilling Large Language Model (LLM) reasoning into a lightweight encoder for cross-domain anomaly detection. During training, LogICL constructs a delta matrix measuring the utility of demonstrations selected via Maximal Marginal Relevance relative to zero-shot inference. The encoder is optimized via a multi-objective loss comprising an ICL-Guided term that aligns representations based on reasoning assistance utility, maximum mean discrepancy for domain alignment, and supervised contrastive loss. At inference, the optimized encoder retrieves reasoning-aware demonstrations using semantic similarity and delta scores, enabling frozen-LLM in-context learning with Chain-of-Thought for accurate and interpretable detection. Experiments on few-shot and zero-shot cross-domain benchmarks confirm LogICL achieves state-of-the-art performance across heterogeneous systems. Further analysis via visualizations and case studies confirms LogICL bridges the semantic gap beyond surface lexical similarity, effectively capturing latent semantic equivalence for rapid deployment.

</details>


### [12] [Understanding Chain-of-Thought Effectiveness in Code Generation: An Empirical and Information-Theoretic Analysis](https://arxiv.org/abs/2512.09679)
*Naizhu Jin,Zhong Li,Guang Yang,Tian Zhang,Qingkai Zeng*

Main category: cs.SE

TL;DR: 本文系统研究了Chain-of-Thought提示在神经代码生成中的有效性，发现结构化CoT方法在多种编程语言和模型规模下表现更优，且推理质量是关键因素。


<details>
  <summary>Details</summary>
Motivation: 探索CoT提示提升大语言模型代码生成性能的内在机制，为实际应用提供策略指导。

Method: 通过信息论视角（条件互信息）评估五种CoT范式，在六种Python基准、12种语言的多语言基准及7B至480B参数模型上进行实验。

Result: 外部引导的CoT优于直接生成，结构化方法平均提升Pass@1达5-12%，且推理质量显著影响效果，轻量级模板可能降低性能。

Conclusion: 应根据模型容量、语言特性和任务复杂度选择合适的CoT策略，高质量结构化推理最具实用价值。

Abstract: Large language models (LLMs) achieve strong performance on code generation, but the mechanisms by which Chain-of-Thought (CoT) prompting helps remain unclear. We present a systematic empirical and information-theoretic study of CoT effectiveness in neural code generation, evaluating five paradigms (Zero-Shot, Zero-Shot CoT, Self-Planning, Structured CoT, Reasoning-CoT) across six Python benchmarks, a multilingual benchmark with 12 programming languages, and six models from 7B to 480B parameters, using conditional mutual information $I(Y;C|X)$ as a conceptual lens. Our results show that externally guided CoT consistently outperforms direct generation, with structured methods improving Pass@1 by 5--12\% on average while using substantially fewer tokens than reflective reasoning, and that CoT benefits depend on language type systems and model capacity. We further find that reasoning \emph{quality} is critical: high-quality structured CoT from strong generators yields significantly higher accuracy than lightweight alternatives with the same template, whereas naive Zero-Shot CoT can even degrade performance. These findings provide practical guidance for choosing CoT strategies based on model capacity, language characteristics, and task complexity.

</details>


### [13] [Quantifying Uncertainty in Machine Learning-Based Pervasive Systems: Application to Human Activity Recognition](https://arxiv.org/abs/2512.09775)
*Vladimir Balditsyn,Philippe Lalanda,German Vega,Stéphanie Chollet*

Main category: cs.SE

TL;DR: 本文提出在基于机器学习的系统中量化不确定性，通过运行时评估模型预测的相关性，以辅助领域专家。


<details>
  <summary>Details</summary>
Motivation: 机器学习模型因训练数据驱动而非手动编码，其运行边界不确定且无法保证无错误，因此需量化其不确定性。

Method: 结合并适配一组选定技术，在运行时评估模型预测相关性，应用于人体活动识别领域。

Result: 实验结果验证了该方法的有效性，并详细讨论了对领域专家的辅助作用。

Conclusion: 量化不确定性有助于提升ML系统在复杂动态场景中的可靠性与可解释性。

Abstract: The recent convergence of pervasive computing and machine learning has given rise to numerous services, impacting almost all areas of economic and social activity. However, the use of AI techniques precludes certain standard software development practices, which emphasize rigorous testing to ensure the elimination of all bugs and adherence to well-defined specifications. ML models are trained on numerous high-dimensional examples rather than being manually coded. Consequently, the boundaries of their operating range are uncertain, and they cannot guarantee absolute error-free performance. In this paper, we propose to quantify uncertainty in ML-based systems. To achieve this, we propose to adapt and jointly utilize a set of selected techniques to evaluate the relevance of model predictions at runtime. We apply and evaluate these proposals in the highly heterogeneous and evolving domain of Human Activity Recognition (HAR). The results presented demonstrate the relevance of the approach, and we discuss in detail the assistance provided to domain experts.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [14] [Efficient MoE Serving in the Memory-Bound Regime: Balance Activated Experts, Not Tokens](https://arxiv.org/abs/2512.09277)
*Yanpeng Yu,Haiyue Ma,Krish Agarwal,Nicolai Oswald,Qijing Huang,Hugo Linsenmaier,Chunhui Mei,Ritchie Zhao,Ritika Borkar,Bita Darvish Rouhani,David Nellans,Ronny Krashinsky,Anurag Khandelwal*

Main category: cs.DC

TL;DR: METRO是一种针对内存受限场景的专家并行MoE服务新路由算法，通过平衡每GPU激活专家数而非令牌数，显著降低延迟并提升吞吐量。


<details>
  <summary>Details</summary>
Motivation: 现有方法在内存受限时通过平衡各GPU处理令牌数反而加剧性能下降，因增加激活专家数导致内存压力增大。

Method: 提出METRO算法，优化激活专家数平衡，结合高效算法与GPU并行能力，并采用低开销allGather获取全局top-k信息。

Result: 在A100和B200系统上，相比EPLB，METRO降低解码延迟11-22%，总吞吐量提升3-21%，固定SLO下解码吞吐量最高提升4.11倍。

Conclusion: METRO在内存受限场景中显著优于传统负载均衡策略，为专家并行MoE部署提供更优性能解决方案。

Abstract: Expert Parallelism (EP) permits Mixture of Experts (MoE) models to scale beyond a single GPU. To address load imbalance across GPUs in EP, existing approaches aim to balance the number of tokens each GPU processes. Surprisingly, we find that this objective degrades performance rather than improving it when processing is memory-bound - a common occurrence in MoE serving, especially in the decode phase. Our analysis reveals that balancing the number of tokens processed per GPU increases the number of activated experts, exacerbating memory pressure in the memory-bound regime.
  We propose Minimum Expert Token ROuting, a novel token-routing algorithm for high-performance expert-parallel MoE serving in the memory-bound regime that balances the number of activated experts per GPU rather than token counts. METRO achieves near-optimal routing quality with minimal computational overhead by jointly optimizing algorithmic efficiency and leveraging the GPU's parallel processing power. To guarantee routing quality, METRO also employs a novel allGather scheme to gather global top-k knowledge, which has minimal overhead compared to conventional allToAll. Our evaluation of METRO against EPLB on both real systems (vLLM over 8 A100 GPUs) and a proprietary simulator (8-16 B200 GPUs) shows that METRO reduces decode latency by 11 - 22%, and total token throughput by 3 - 21% for Qwen3 and DeepSeek-V3 serving, where prefill and decode phases are co-deployed. In addition, by trading latency headroom for throughput, METRO improves decode throughput by up to 4.11x over EPLB at a fixed decode SLO.

</details>


### [15] [A Distributed Framework for Privacy-Enhanced Vision Transformers on the Edge](https://arxiv.org/abs/2512.09309)
*Zihao Ding,Mufeng Zhu,Zhongze Tang,Sheng Wei,Yao Liu*

Main category: cs.DC

TL;DR: 提出一种分布式分层卸载框架，以增强视觉任务中的隐私保护。


<details>
  <summary>Details</summary>
Motivation: 解决视觉智能工具在云端处理时带来的隐私泄露问题。

Method: 通过本地可信边缘设备将图像分片并分布到多个云服务器，最终在边缘设备聚合结果。

Result: 在保持接近基准性能的同时显著降低内容重建和数据暴露风险。

Conclusion: 该框架为边缘-云连续体中的视觉任务提供可扩展且隐私保护的解决方案。

Abstract: Nowadays, visual intelligence tools have become ubiquitous, offering all kinds of convenience and possibilities. However, these tools have high computational requirements that exceed the capabilities of resource-constrained mobile and wearable devices. While offloading visual data to the cloud is a common solution, it introduces significant privacy vulnerabilities during transmission and server-side computation. To address this, we propose a novel distributed, hierarchical offloading framework for Vision Transformers (ViTs) that addresses these privacy challenges by design. Our approach uses a local trusted edge device, such as a mobile phone or an Nvidia Jetson, as the edge orchestrator. This orchestrator partitions the user's visual data into smaller portions and distributes them across multiple independent cloud servers. By design, no single external server possesses the complete image, preventing comprehensive data reconstruction. The final data merging and aggregation computation occurs exclusively on the user's trusted edge device. We apply our framework to the Segment Anything Model (SAM) as a practical case study, which demonstrates that our method substantially enhances content privacy over traditional cloud-based approaches. Evaluations show our framework maintains near-baseline segmentation performance while substantially reducing the risk of content reconstruction and user data exposure. Our framework provides a scalable, privacy-preserving solution for vision tasks in the edge-cloud continuum.

</details>


### [16] [Passing the Baton: High Throughput Distributed Disk-Based Vector Search with BatANN](https://arxiv.org/abs/2512.09331)
*Nam Anh Dang,Ben Landrum,Ken Birman*

Main category: cs.DC

TL;DR: BatANN 是首个开源的分布式磁盘向量检索系统，通过全局图结构与状态迁移实现高吞吐与低延迟。


<details>
  <summary>Details</summary>
Motivation: 应对未来超大规模向量数据集无法在单机存储与检索的挑战。

Method: 采用分布式架构，在跨机器访问邻居节点时迁移完整查询状态以提升局部性，并基于标准TCP通信。

Result: 在10台服务器、0.95召回率下，相比scatter-gather基线，1亿与10亿数据集分别提升6.21-6.49倍和2.5-5.10倍吞吐，平均延迟低于6ms。

Conclusion: BatANN 首次实现了基于单一全局图的分布式磁盘向量搜索，兼具效率与可扩展性。

Abstract: Vector search underpins modern information-retrieval systems, including retrieval-augmented generation (RAG) pipelines and search engines over unstructured text and images. As datasets scale to billions of vectors, disk-based vector search has emerged as a practical solution. However, looking to the future, we need to anticipate datasets too large for any single server. We present BatANN, a distributed disk-based approximate nearest neighbor (ANN) system that retains the logarithmic search efficiency of a single global graph while achieving near-linear throughput scaling in the number of servers. Our core innovation is that when accessing a neighborhood which is stored on another machine, we send the full state of the query to the other machine to continue executing there for improved locality. On 100M- and 1B-point datasets at 0.95 recall using 10 servers, BatANN achieves 6.21-6.49x and 2.5-5.10x the throughput of the scatter-gather baseline, respectively, while maintaining mean latency below 6 ms. Moreover, we get these results on standard TCP. To our knowledge, BatANN is the first open-source distributed disk-based vector search system to operate over a single global graph.

</details>


### [17] [WarmServe: Enabling One-for-Many GPU Prewarming for Multi-LLM Serving](https://arxiv.org/abs/2512.09472)
*Chiheng Lou,Sheng Qi,Rui Kang,Yong Zhang,Chen Sun,Pengcheng Wang,Bingyang Liu,Xuanzhe Liu,Xin Jin*

Main category: cs.DC

TL;DR: WarmServe通过预知未来工作负载，优化多LLM服务的GPU预热与内存管理，显著降低首token延迟并提升请求处理能力。


<details>
  <summary>Details</summary>
Motivation: 现有系统为提升GPU利用率牺牲推理性能，尤其首token延迟，而实际工作负载具有周期性与可预测性。

Method: 设计通用GPU工作者、采用驱逐感知模型放置、主动预热及零开销内存切换机制。

Result: 相比最先进系统，TTFT降低达50.8倍，请求处理量提升2.5倍。

Conclusion: WarmServe有效平衡资源效率与推理性能，适合真实场景下的多LLM部署。

Abstract: Deploying multiple models within shared GPU clusters is promising for improving resource efficiency in large language model (LLM) serving. Existing multi-LLM serving systems optimize GPU utilization at the cost of worse inference performance, especially time-to-first-token (TTFT). We identify the root cause of such compromise as their unawareness of future workload characteristics. In contrast, recent analysis on real-world traces has shown the high periodicity and long-term predictability of LLM serving workloads.
  We propose universal GPU workers to enable one-for-many GPU prewarming that loads models with knowledge of future workloads. Based on universal GPU workers, we design and build WarmServe, a multi-LLM serving system that (1) mitigates cluster-wide prewarming interference by adopting an evict-aware model placement strategy, (2) prepares universal GPU workers in advance by proactive prewarming, and (3) manages GPU memory with a zero-overhead memory switching mechanism. Evaluation under real-world datasets shows that WarmServe improves TTFT by up to 50.8$\times$ compared to the state-of-the-art autoscaling-based system, while being capable of serving up to 2.5$\times$ more requests compared to the GPU-sharing system.

</details>


### [18] [PHWSOA: A Pareto-based Hybrid Whale-Seagull Scheduling for Multi-Objective Tasks in Cloud Computing](https://arxiv.org/abs/2512.09568)
*Zhi Zhao,Hang Xiao,Wei Rang*

Main category: cs.DC

TL;DR: 提出PHWSOA算法，结合鲸鱼与海鸥优化算法，通过帕累托多目标优化显著提升云任务调度性能。


<details>
  <summary>Details</summary>
Motivation: 现有调度方法多关注单一指标，缺乏对完工时间、负载均衡和成本的综合优化。

Method: 融合WOA与SOA，引入Halton序列初始化、帕累托引导变异机制、并行处理及动态负载重分配策略。

Result: 在CloudSim上实验表明，相比基线算法，PHWSOA使完工时间减少72.1%，负载均衡提升36.8%，成本降低23.5%。

Conclusion: PHWSOA在多目标云任务调度中表现优异，具备实际部署潜力。

Abstract: Task scheduling is a critical research challenge in cloud computing, a transformative technology widely adopted across industries. Although numerous scheduling solutions exist, they predominantly optimize singular or limited metrics such as execution time or resource utilization often neglecting the need for comprehensive multi-objective optimization. To bridge this gap, this paper proposes the Pareto-based Hybrid Whale-Seagull Optimization Algorithm (PHWSOA). This algorithm synergistically combines the strengths of the Whale Optimization Algorithm (WOA) and the Seagull Optimization Algorithm (SOA), specifically mitigating WOA's limitations in local exploitation and SOA's constraints in global exploration. Leveraging Pareto dominance principles, PHWSOA simultaneously optimizes three key objectives: makespan, virtual machine (VM) load balancing, and economic cost. Key enhancements include: Halton sequence initialization for superior population diversity, a Pareto-guided mutation mechanism to avert premature convergence, and parallel processing for accelerated convergence. Furthermore, a dynamic VM load redistribution mechanism is integrated to improve load balancing during task execution. Extensive experiments conducted on the CloudSim simulator, utilizing real-world workload traces from NASA-iPSC and HPC2N, demonstrate that PHWSOA delivers substantial performance gains. Specifically, it achieves up to a 72.1% reduction in makespan, a 36.8% improvement in VM load balancing, and 23.5% cost savings. These results substantially outperform baseline methods including WOA, GA, PEWOA, and GCWOA underscoring PHWSOA's strong potential for enabling efficient resource management in practical cloud environments.

</details>


### [19] [SynthPix: A lightspeed PIV images generator](https://arxiv.org/abs/2512.09664)
*Antonio Terpin,Alan Bonomi,Francesco Banelli,Raffaello D'Andrea*

Main category: cs.DC

TL;DR: SynthPix是一个基于JAX的高性能并行合成图像生成器，专为粒子图像测速（PIV）设计，显著提升图像对生成速度，助力强化学习与实时流场估计研究。


<details>
  <summary>Details</summary>
Motivation: 加速数据密集型强化学习方法训练，缩短实时PIV反馈控制中流场估计算法的开发迭代时间。

Method: 采用JAX实现高性能并行化架构，支持与现有工具相同的配置参数。

Result: 图像对生成吞吐量比现有工具高出数个数量级。

Conclusion: SynthPix有望广泛服务于流体力学研究社区，推动快速流场估计与主动流体控制发展。

Abstract: We describe SynthPix, a synthetic image generator for Particle Image Velocimetry (PIV) with a focus on performance and parallelism on accelerators, implemented in JAX. SynthPix supports the same configuration parameters as existing tools but achieves a throughput several orders of magnitude higher in image-pair generation per second. SynthPix was developed to enable the training of data-hungry reinforcement learning methods for flow estimation and for reducing the iteration times during the development of fast flow estimation methods used in recent active fluids control studies with real-time PIV feedback. We believe SynthPix to be useful for the fluid dynamics community, and in this paper we describe the main ideas behind this software package.

</details>


### [20] [Straggler Tolerant and Resilient DL Training on Homogeneous GPUs](https://arxiv.org/abs/2512.09685)
*Zeyu Zhang,Haiying Shen*

Main category: cs.DC

TL;DR: STAR系统通过优化同步模式和资源分配，有效降低深度学习训练中的TTA并避免拖尾问题。


<details>
  <summary>Details</summary>
Motivation: 现有研究对GPU深度学习训练中拖尾问题的成因及缓解方法效果缺乏深入理解。

Method: 提出STAR系统，包含新的同步模式、启发式与机器学习方法选择最优模式，并通过资源重分配避免CPU与带宽过载。

Result: 在AWS上的评估表明，STAR在PS和all-reduce架构下分别降低TTA 48-84%和51-70%，同时保持收敛精度。

Conclusion: STAR显著提升训练效率且开源，为解决拖尾问题提供有效方案。

Abstract: Despite the popularity of homogeneous GPU-based deep learning (DL) training, the prevalence, causes and impact of stragglers and the effectiveness of existing straggler mitigation approaches are still not well understood in this scenario due to limited research on these questions. To fill this gap, we conducted comprehensive experiments and found that stragglers remain widespread due to CPU and bandwidth usage imbalances. Additionally, existing mitigation methods that switch from synchronous stochastic gradient descent (SSGD) to asynchronous SGD (ASGD) may not improve Time-To-Accuracy (TTA) and can even generate more stragglers due to its higher resource consumption. To address these newly found problems, we propose the Straggler Tolerant And Resilient DL training system (STAR). STAR includes new synchronization modes that group workers for each parameter updating. It has a heuristic and an ML method to choose the optimal synchronization mode for minimizing TTA, and reallocates resources to support the selected mode while minimizing the impact on co-located jobs. Moreover, it proactively prevents stragglers by avoiding overloading the CPU and bandwidth resources in allocating PSs (which consume high CPU and bandwidth) and in gradient transmission. Our trace-driven evaluation on AWS shows that STAR generates 48-84% and 51-70% lower TTA than state-of-the-art systems in the PS and all-reduce architectures, respectively, while maintaining the converged accuracy of SSGD. The code for STAR is open-sourced.

</details>


### [21] [Recoverable Lock-Free Locks](https://arxiv.org/abs/2512.09710)
*Hagit Attiya,Panagiota Fatourou,Eleftherios Kosmas,Yuanhao Wei*

Main category: cs.DC

TL;DR: 本文提出首个同时实现无锁性和可恢复性的转换方法。


<details>
  <summary>Details</summary>
Motivation: 为解决传统锁机制在故障恢复中的局限性，提升系统可靠性。

Method: 从基于锁的实现出发，替换锁获取与释放操作，支持嵌套锁并确保可恢复性。

Result: 成功实现了无锁且可恢复的替代方案，未影响原锁机制的正确性。

Conclusion: 该转换方法兼顾性能与容错能力，具有广泛适用性。

Abstract: This paper presents the first transformation that introduces both lock-freedom and recoverability. Our transformation starts with a lock-based implementation, and provides a recoverable, lock-free substitution to lock acquire and lock release operations. The transformation supports nested locks for generality and ensures recoverability without jeopardising the correctness of the lock-based implementation it is applied on.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [22] [Tyche: A Hybrid Computation Framework of Illumination Pattern for Satellite Beam Hopping](https://arxiv.org/abs/2512.09312)
*Ziheng Yang,Kun Qiu,Zhe Chen,Wenjun Zhu,Yue Gao*

Main category: cs.NI

TL;DR: Tyche框架结合MCTS-BH与G-BH算法，显著提升高通量卫星波束跳变的计算效率与吞吐量。


<details>
  <summary>Details</summary>
Motivation: 传统方法计算波束照明模式耗时过长，难以满足现代高通量卫星覆盖数百小区的实时需求。

Method: 提出Tyche混合框架，采用蒙特卡洛树搜索波束跳变算法（MCTS-BH）并辅以滑动窗口与剪枝技术，同时用贪心算法（G-BH）提供实时临时解。

Result: MCTS-BH在37小区场景下仅需12秒完成计算，吞吐量最高提升98.76%。

Conclusion: Tyche有效解决波束跳变计算效率瓶颈，具备实际部署潜力。

Abstract: High-Throughput Satellites (HTS) use beam hopping to handle non-uniform and time-varying ground traffic demand. A significant technical challenge in beam hopping is the computation of effective illumination patterns. Traditional algorithms, like the genetic algorithm, require over 300 seconds to compute a single illumination pattern for just 37 cells, whereas modern HTS typically covers over 300 cells, rendering current methods impractical for real-world applications. Advanced approaches, such as multi-agent deep reinforcement learning, face convergence issues when the number of cells exceeds 40. In this paper, we introduce Tyche, a hybrid computation framework designed to address this challenge. Tyche incorporates a Monte Carlo Tree Search Beam Hopping (MCTS-BH) algorithm for computing illumination patterns and employs sliding window and pruning techniques to significantly reduce computation time. Specifically, MCTS-BH can compute one illumination pattern for 37 cells in just 12 seconds. To ensure real-time computation, we use a Greedy Beam Hopping (G-BH) algorithm, which provides a provisional solution while MCTS-BH completes its computation in the background. Our evaluation results show that MCTS-BH can increase throughput by up to 98.76%, demonstrating substantial improvements over existing solutions.

</details>
