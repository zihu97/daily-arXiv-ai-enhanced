<div id=toc></div>

# Table of Contents

- [cs.SE](#cs.SE) [Total: 37]
- [cs.NI](#cs.NI) [Total: 3]
- [cs.MA](#cs.MA) [Total: 7]
- [cs.AR](#cs.AR) [Total: 2]
- [cs.DC](#cs.DC) [Total: 10]


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [1] [ScaleCall -- Agentic Tool Calling at Scale for Fintech: Challenges, Methods, and Deployment Insights](https://arxiv.org/abs/2511.00074)
*Richard Osuagwu,Thomas Cook,Maraim Masoud,Koustav Ghosal,Riccardo Mattivi*

Main category: cs.SE

TL;DR: 本文研究了在受监管的企业环境中（如金融科技）部署大语言模型工具调用能力的挑战，提出了名为ScaleCall的工具调用框架，并系统评估了基于嵌入的检索、基于提示的列表排序及混合方法在企业内部API编排和数据工程自动化中的表现。


<details>
  <summary>Details</summary>
Motivation: 在受监管的企业环境中部署大语言模型的工具调用能力面临本地部署限制、合规要求以及大量功能重叠工具难以区分等挑战，亟需针对企业场景优化工具检索方法。

Method: 开发并部署ScaleCall框架，系统评估嵌入检索、提示驱动的列表排序和混合方法，在企业真实数据集上进行实验，并集成到灵活架构中进行实际验证。

Result: 嵌入方法在大规模工具库中具有更低延迟，列表排序在功能重叠场景下表现更优，混合方法在特定情境下展现出潜力；方法效果高度依赖领域特性而非算法本身优劣。

Conclusion: 工具调用系统在企业应用中的设计需权衡检索准确性、计算效率与运营需求，本研究为受监管行业提供了实用的设计洞见和可部署的框架参考。

Abstract: While Large Language Models (LLMs) excel at tool calling, deploying these
capabilities in regulated enterprise environments such as fintech presents
unique challenges due to on-premises constraints, regulatory compliance
requirements, and the need to disambiguate large, functionally overlapping
toolsets. In this paper, we present a comprehensive study of tool retrieval
methods for enterprise environments through the development and deployment of
ScaleCall, a prototype tool-calling framework within Mastercard designed for
orchestrating internal APIs and automating data engineering workflows. We
systematically evaluate embedding-based retrieval, prompt-based listwise
ranking, and hybrid approaches, revealing that method effectiveness depends
heavily on domain-specific factors rather than inherent algorithmic
superiority. Through empirical investigation on enterprise-derived benchmarks,
we find that embedding-based methods offer superior latency for large tool
repositories, while listwise ranking provides better disambiguation for
overlapping functionalities, with hybrid approaches showing promise in specific
contexts. We integrate our findings into ScaleCall's flexible architecture and
validate the framework through real-world deployment in Mastercard's regulated
environment. Our work provides practical insights into the trade-offs between
retrieval accuracy, computational efficiency, and operational requirements,
contributing to the understanding of tool-calling system design for enterprise
applications in regulated industries.

</details>


### [2] [Adding New Capability in Existing Scientific Application with LLM Assistance](https://arxiv.org/abs/2511.00087)
*Anshu Dubey,Akash Dhruv*

Main category: cs.SE

TL;DR: 本文提出了一种利用大语言模型（LLM）从零开始为全新算法生成代码的新方法，并改进了已有代码翻译工具Code-Scribe以支持新代码生成。


<details>
  <summary>Details</summary>
Motivation: 现有研究多关注LLM在已有代码模式上的生成能力，而对训练数据中未包含类似示例的全新算法代码生成问题探索较少。

Method: 提出一种结合LLM辅助从头编写新算法代码的方法，并对已有的Code-Scribe代码翻译工具进行增强，用于支持新代码生成任务。

Result: 成功将Code-Scribe工具扩展至新算法代码生成场景，展示了LLM在无先验示例情况下的代码创作潜力。

Conclusion: LLM在全新算法代码生成方面具有应用前景，通过适当方法和工具增强，可有效支持从零开始的代码创作。

Abstract: With the emergence and rapid evolution of large language models (LLM),
automating coding tasks has become an important research topic. Many efforts
are underway and literature abounds about the efficacy of models and their
ability to generate code. A less explored aspect of code generation is for new
algorithms, where the training dataset would not have included any previous
example of similar code. In this paper we propose a new methodology for writing
code from scratch for a new algorithm using LLM assistance, and describe
enhancement of a previously developed code-translation tool, Code-Scribe, for
new code generation.

</details>


### [3] [Inferring multiple helper Dafny assertions with LLMs](https://arxiv.org/abs/2511.00125)
*Álvaro Silva,Alexandra Mendes,Ruben Martins*

Main category: cs.SE

TL;DR: 本文提出DAISY系统，利用大语言模型（LLM）自动推断Dafny程序中缺失的辅助断言，尤其针对多个断言缺失的情况。通过扩展DafnyBench基准并引入断言类型分类，结合LLM预测与错误信息启发式方法进行故障定位。实验表明，DAISY在单断言缺失时验证成功率达63.4%，多断言缺失时为31.7%，且常可用更少断言完成验证，显著降低形式化验证的工程负担。


<details>
  <summary>Details</summary>
Motivation: Dafny验证器虽能提供强正确性保证，但通常需要大量手动添加辅助断言，阻碍其广泛应用。因此，亟需自动化方法来推断缺失的断言，尤其是多个断言同时缺失的复杂场景。

Method: 作者扩展了DafnyBench基准，构建包含移除一个、两个或全部断言的数据集，并提出断言类型分类以分析推断难度。其方法结合大语言模型（LLM）预测与错误信息启发式策略，实现混合式故障定位，并在新工具DAISY中实现该方法。

Result: DAISY在单断言缺失情况下成功验证63.4%的程序，在多断言缺失情况下验证成功率为31.7%。研究还发现，许多程序可使用比原始更少的断言完成验证，说明存在多种有效修复策略，无需完全恢复原始断言。

Conclusion: 自动化断言推断能显著减少形式化验证中的证明工程工作量，DAISY展示了LLM在提升验证可扩展性和易用性方面的潜力，是迈向更高效形式化验证的重要一步。

Abstract: The Dafny verifier provides strong correctness guarantees but often requires
numerous manual helper assertions, creating a significant barrier to adoption.
We investigate the use of Large Language Models (LLMs) to automatically infer
missing helper assertions in Dafny programs, with a primary focus on cases
involving multiple missing assertions. To support this study, we extend the
DafnyBench benchmark with curated datasets where one, two, or all assertions
are removed, and we introduce a taxonomy of assertion types to analyze
inference difficulty. Our approach refines fault localization through a hybrid
method that combines LLM predictions with error-message heuristics. We
implement this approach in a new tool called DAISY (Dafny Assertion Inference
SYstem). While our focus is on multiple missing assertions, we also evaluate
DAISY on single-assertion cases. DAISY verifies 63.4% of programs with one
missing assertion and 31.7% with multiple missing assertions. Notably, many
programs can be verified with fewer assertions than originally present,
highlighting that proofs often admit multiple valid repair strategies and that
recovering every original assertion is unnecessary. These results demonstrate
that automated assertion inference can substantially reduce proof engineering
effort and represent a step toward more scalable and accessible formal
verification.

</details>


### [4] [What a diff makes: automating code migration with large language models](https://arxiv.org/abs/2511.00160)
*Katherine A. Rosenfeld,Cliff C. Kerr,Jessica Lundin*

Main category: cs.SE

TL;DR: 本文研究利用大语言模型（LLMs）辅助代码迁移，以应对依赖库版本更新带来的兼容性问题。通过引入包含代码差异（diffs）的上下文，显著提升了LLM在迁移任务中的性能，并开源了数据集和Python工具包AIMigrate。在真实项目TYPHOIDSIM的迁移实验中，AIMigrate单次运行可识别65%的必要变更，多次运行提升至80%，其中47%的变更被完美生成。


<details>
  <summary>Details</summary>
Motivation: 现代软件依赖的库频繁更新，可能导致项目兼容性问题。传统方法难以高效应对语义版本变更，因此探索利用LLMs自动化代码迁移以维持兼容性。

Method: 使用包含代码差异（diffs）的上下文信息增强LLM输入，对比原始代码输入与diff上下文对迁移效果的影响，并开发开源工具AIMigrate和配套数据集支持代码迁移任务。

Result: 实验表明，基于diff上下文的提示显著优于普通LLM输出，在某些情况下甚至优于仅使用代码的方法。在TYPHOIDSIM项目迁移中，AIMigrate单次识别65%的变更，多次运行达80%，47%的变更完全正确。

Conclusion: 结合diff上下文能有效提升LLM在代码迁移任务中的准确性和实用性，所提出的AIMigrate工具和数据集为该领域提供了有价值的资源。

Abstract: Modern software programs are built on stacks that are often undergoing
changes that introduce updates and improvements, but may also break any project
that depends upon them. In this paper we explore the use of Large Language
Models (LLMs) for code migration, specifically the problem of maintaining
compatibility with a dependency as it undergoes major and minor semantic
version changes. We demonstrate, using metrics such as test coverage and change
comparisons, that contexts containing diffs can significantly improve
performance against out of the box LLMs and, in some cases, perform better than
using code. We provide a dataset to assist in further development of this
problem area, as well as an open-source Python package, AIMigrate, that can be
used to assist with migrating code bases. In a real-world migration of
TYPHOIDSIM between STARSIM versions, AIMigrate correctly identified 65% of
required changes in a single run, increasing to 80% with multiple runs, with
47% of changes generated perfectly.

</details>


### [5] [Understanding Code Agent Behaviour: An Empirical Study of Success and Failure Trajectories](https://arxiv.org/abs/2511.00197)
*Oorja Majgaonkar,Zhiwei Fei,Xiang Li,Federica Sarro,He Ye*

Main category: cs.SE

TL;DR: 本文通过对三个先进代码智能体（OpenHands、SWE-agent 和 Prometheus）在 SWE-Bench 基准上的执行轨迹进行实证分析，揭示了其在解决软件问题时的成功与失败行为模式，发现成功更依赖于近似而非精确的代码修改，并指出轨迹分析有助于构建更可靠、可解释的自主软件工程系统。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型智能体在复杂软件工程任务中的广泛应用，仅靠成功指标已不足以理解其问题解决行为，亟需深入探究其决策过程和行为模式。

Method: 对三个先进代码智能体在 SWE-Bench 基准上的成功与失败执行轨迹进行实证分析，包括问题解决策略、轨迹长度与方差、故障定位准确性等方面。

Result: 研究发现：（1）不同策略（如防御性编程、上下文收集）在不同场景下促成成功；（2）失败轨迹更长且方差更大，失败模式因智能体而异；（3）多数轨迹能正确定位问题文件（失败案例中也有72–81%），但成功更依赖近似而非精确的代码修改。

Conclusion: 轨迹分析为理解代码智能体行为提供了基础，有助于开发更稳健、可解释的自主软件工程系统。

Abstract: The increasing deployment of Large Language Model (LLM) agents for complex
software engineering tasks has created a need to understand their
problem-solving behaviours beyond simple success metrics. While these agents
demonstrate impressive capabilities in automated issue resolution, their
decision-making processes remain largely opaque. This paper presents an
empirical study of agent trajectories, namely the execution traces capturing
the steps agents take when attempting to resolve software issues. We analyse
trajectories from three state-of-the-art code agents (OpenHands, SWE-agent, and
Prometheus) on the SWE-Bench benchmark, examining both successful and failed
attempts. Our investigation reveals several key insights into agent behaviour.
First, we identify how distinct problem-solving strategies, such as defensive
programming and context gathering, enable success in different scenarios.
Second, we find that failed trajectories are consistently longer and exhibit
higher variance than successful ones, with failure patterns differing
significantly between agents. Third, our fault localisation analysis shows that
while most trajectories correctly identify problematic files (72-81\% even in
failures), success depends more on achieving approximate rather than exact code
modifications. These and other findings unveiled by our study, provide a
foundation for understanding agent behaviour through trajectory analysis,
contributing to the development of more robust and interpretable autonomous
software engineering systems.

</details>


### [6] [Position: Vibe Coding Needs Vibe Reasoning: Improving Vibe Coding with Formal Verification](https://arxiv.org/abs/2511.00202)
*Jacqueline Mitchell,Yasser Shaaban*

Main category: cs.SE

TL;DR: 本文提出通过引入一种伴随式形式化方法系统（side-car system）来提升“氛围编程”（vibe coding）的可靠性，该系统能自动形式化规范、验证目标、向大语言模型提供可操作反馈，并支持开发者直观地影响规范。


<details>
  <summary>Details</summary>
Motivation: 当前氛围编程存在技术债累积、安全问题和代码频繁变更等问题，根源在于大语言模型无法有效协调开发过程中不断积累的人为约束，且优先响应用户指令而忽视代码一致性。

Method: 提出一种贯穿氛围编程全过程的伴随式系统，具备自动形式化规范、目标验证、向LLM提供可操作反馈以及支持开发者直观干预规范四大功能。

Result: 该方法有望缓解氛围编程中的关键缺陷，提升其可靠性和实用性。

Conclusion: 将形式化方法以新型伴随式系统形式融入氛围编程，可有效克服现有大语言模型在协调约束与保持代码一致性方面的不足。

Abstract: ``Vibe coding'' -- the practice of developing software through iteratively
conversing with a large language model (LLM) -- has exploded in popularity
within the last year. However, developers report key limitations including the
accumulation of technical debt, security issues, and code churn to achieve
satisfactory results. We argue that these pitfalls result from LLMs' inability
to reconcile accumulating human-imposed constraints during vibe coding, with
developers inadvertently failing to resolve contradictions because LLMs
prioritize user commands over code consistency. Given LLMs' receptiveness to
verification-based feedback, we argue that formal methods can mitigate these
pitfalls, making vibe coding more reliable. However, we posit that integrating
formal methods must transcend existing approaches that combine formal methods
and LLMs. We advocate for a side-car system throughout the vibe coding process
which: (1) \emph{Autoformalizes} specifications (2) Validates against targets,
(3) Delivers \emph{actionable} feedback to the LLM, and (4) Allows intuitive
developer influence on specifications.

</details>


### [7] [DocPrism: Local Categorization and External Filtering to Identify Relevant Code-Documentation Inconsistencies](https://arxiv.org/abs/2511.00215)
*Xiaomeng Xu,Zahin Wahab,Reid Holmes,Caroline Lemieux*

Main category: cs.SE

TL;DR: 本文提出DocPrism，一种多语言代码-文档不一致性检测工具，通过引入LCEF方法显著降低大语言模型（LLM）在此任务中的误报率，在未微调的情况下在多种语言中实现高精度检测。


<details>
  <summary>Details</summary>
Motivation: 代码与文档之间的不一致性普遍存在，容易导致开发者误解和软件缺陷；而直接使用大语言模型进行检测会产生过高误报率，因其将高层次文档与具体代码实现之间的自然差异误判为不一致。

Method: 提出Local Categorization, External Filtering（LCEF）方法，利用LLM的局部补全能力而非长期推理能力，以减少误报；该方法集成于多语言工具DocPrism中。

Result: 在消融实验中，LCEF将DocPrism的不一致标记率从98%降至14%，准确率从14%提升至94%；在Python、TypeScript、C++和Java上的广泛评估中，DocPrism保持15%的低标记率和0.62的精确率，且无需微调。

Conclusion: DocPrism结合LCEF方法有效解决了LLM在代码-文档一致性检测中的高误报问题，为多语言软件维护提供了一种高效、准确的自动化检测方案。

Abstract: Code-documentation inconsistencies are common and undesirable: they can lead
to developer misunderstandings and software defects. This paper introduces
DocPrism, a multi-language, code-documentation inconsistency detection tool.
DocPrism uses a standard large language model (LLM) to analyze and explain
inconsistencies. Plain use of LLMs for this task yield unacceptably high false
positive rates: LLMs identify natural gaps between high-level documentation and
detailed code implementations as inconsistencies. We introduce and apply the
Local Categorization, External Filtering (LCEF) methodology to reduce false
positives. LCEF relies on the LLM's local completion skills rather than its
long-term reasoning skills. In our ablation study, LCEF reduces DocPrism's
inconsistency flag rate from 98% to 14%, and increases accuracy from 14% to
94%. On a broad evaluation across Python, TypeScript, C++, and Java, DocPrism
maintains a low flag rate of 15%, and achieves a precision of 0.62 without
performing any fine-tuning.

</details>


### [8] [LLM-Driven Cost-Effective Requirements Change Impact Analysis](https://arxiv.org/abs/2511.00262)
*Romina Etezadi,Sallam Abualhaija,Chetan Arora,Lionel Briand*

Main category: cs.SE

TL;DR: 本文提出了一种基于大语言模型（LLM）的自动化方法 ProReFiCIA，用于在需求变更时高效识别受影响的需求，在两个数据集上分别达到了93.3%和95.8%的召回率，且工程师只需审查占总需求2.1%–8.5%的预测结果，成本较低。


<details>
  <summary>Details</summary>
Motivation: 需求在软件开发生命周期中经常发生变化，人工识别变更对其他需求的影响既容易出错又耗费大量精力，可能导致遗漏受影响需求，从而引发下游任务中的严重问题。

Method: 提出 ProReFiCIA 方法，利用大语言模型（LLM）自动识别需求变更所影响的其他需求，并通过多种 LLM 与提示模板组合进行广泛评估，选出最优配置。

Result: 在基准数据集和新构建的工业数据集上，ProReFiCIA 分别实现了93.3%和95.8%的召回率；工程师仅需审查占全部需求2.1%至8.5%的预测结果，显著降低人工成本。

Conclusion: ProReFiCIA 能高效、低成本地识别受变更影响的需求，显著优于传统人工方法，具有良好的实用性和可扩展性。

Abstract: Requirements are inherently subject to changes throughout the software
development lifecycle. Within the limited budget available to requirements
engineers, manually identifying the impact of such changes on other
requirements is both error-prone and effort-intensive. That might lead to
overlooked impacted requirements, which, if not properly managed, can cause
serious issues in the downstream tasks. Inspired by the growing potential of
large language models (LLMs) across diverse domains, we propose ProReFiCIA, an
LLM-driven approach for automatically identifying the impacted requirements
when changes occur. We conduct an extensive evaluation of ProReFiCIA using
several LLMs and prompts variants tailored to this task. Using the best
combination of an LLM and a prompt variant, ProReFiCIA achieves a recall of
93.3% on a benchmark dataset and 95.8% on a newly created industry dataset,
demonstrating its strong effectiveness in identifying impacted requirements.
Further, the cost of applying ProReFiCIA remains small, as the engineer only
needs to review the generated results, which represent between 2.1% and 8.5% of
the entire set of requirements.

</details>


### [9] [Human-AI Programming Role Optimization: Developing a Personality-Driven Self-Determination Framework](https://arxiv.org/abs/2511.00417)
*Marcel Valovy*

Main category: cs.SE

TL;DR: 该论文通过自我决定理论和人格心理学，提出ROMA框架，优化人类与AI在编程中的协作角色，发现人格特质与角色偏好之间的关联，并显著提升开发者的动机和团队协作效果。


<details>
  <summary>Details</summary>
Motivation: 探索在AI日益介入软件开发的背景下，如何基于开发者的人格特质优化其与AI系统的协作角色，以提升内在动机、自主性和团队协作效率。

Method: 采用设计科学研究方法，历经五个迭代周期，结合200名实验参与者和46名访谈对象，实证分析人格特质、编程角色偏好与协作结果之间的关系。

Result: 识别出五种人格原型（探索者、协调者、工匠、架构师、适应者），每种对AI协作角色（如Co-Pilot、Co-Navigator、Agent）有不同偏好；基于人格的角色分配使专业人士平均动机提升23%，本科生最高达65%。

Conclusion: 论文贡献了一个人格驱动的协作角色优化框架（ROMA）、AI协作模式分类法，并扩展了ISO/IEC 29110标准，支持极小型实体在标准框架内实施个性化人机协作策略。

Abstract: As artificial intelligence transforms software development, a critical
question emerges: how can developers and AI systems collaborate most
effectively? This dissertation optimizes human-AI programming roles through
self-determination theory and personality psychology, introducing the Role
Optimization Motivation Alignment (ROMA) framework.
  Through Design Science Research spanning five cycles, this work establishes
empirically-validated connections between personality traits, programming role
preferences, and collaborative outcomes, engaging 200 experimental participants
and 46 interview respondents.
  Key findings demonstrate that personality-driven role optimization
significantly enhances self-determination and team dynamics, yielding 23%
average motivation increases among professionals and up to 65% among
undergraduates. Five distinct personality archetypes emerge: The Explorer (high
Openness/low Agreeableness), The Orchestrator (high
Extraversion/Agreeableness), The Craftsperson (high Neuroticism/low
Extraversion), The Architect (high Conscientiousness), and The Adapter
(balanced profile). Each exhibits distinct preferences for programming roles
(Co-Pilot, Co-Navigator, Agent), with assignment modes proving crucial for
satisfaction.
  The dissertation contributes: (1) an empirically-validated framework linking
personality traits to role preferences and self-determination outcomes; (2) a
taxonomy of AI collaboration modalities mapped to personality profiles while
preserving human agency; and (3) an ISO/IEC 29110 extension enabling Very Small
Entities to implement personality-driven role optimization within established
standards.
  Keywords: artificial intelligence, human-computer interaction, behavioral
software engineering, self-determination theory, personality psychology,
phenomenology, intrinsic motivation, pair programming, design science research,
ISO/IEC 29110

</details>


### [10] [A Big Step Forward? A User-Centric Examination of iOS App Privacy Report and Enhancements](https://arxiv.org/abs/2511.00467)
*Liu Wang,Dong Wang,Shidong Pan,Zheng Jiang,Haoyu Wang,Yi Wang*

Main category: cs.SE

TL;DR: 本文研究了iOS 15.2引入的“App隐私报告”功能在现实中的效果，发现其对用户隐私的实际帮助有限，并提出了基于大语言模型和多技术融合的改进方案，以提升数据访问目的和域名描述的清晰度。


<details>
  <summary>Details</summary>
Motivation: 尽管苹果推出了App隐私报告以增强用户对应用数据行为的了解，但其在真实使用场景中对用户隐私和控制的实际影响尚未被系统评估。

Method: 研究采用端到端方法，包括对12名普通iOS用户的焦点小组研究，分析用户对App隐私报告的理解与体验；并提出包含目的推断框架和域名澄清管道的增强方案，从系统和用户双重视角进行综合评估。

Result: 研究发现App隐私报告因缺乏关键细节（如数据访问目的和域名说明）而实际影响有限；所提出的增强方案有效提升了透明度和用户体验。

Conclusion: App隐私报告虽具创新性，但需进一步优化以真正赋能用户；本研究为提升移动应用隐私透明度提供了实用见解，并指出了未来研究方向。

Abstract: The prevalent engagement with mobile apps underscores the importance of
understanding their data practices. Transparency plays a crucial role in this
context, ensuring users to be informed and give consent before any data access
occurs. Apple introduced a new feature since iOS 15.2, App Privacy Report, to
inform users about detailed insights into apps' data access and sharing. This
feature continues Apple's trend of privacy-focused innovations (following
Privacy Nutrition Labels), and has been marketed as a big step forward in user
privacy. However, its real-world impacts on user privacy and control remain
unexamined. We thus proposed an end-to-end study involving systematic
assessment of the App Privacy Report's real-world benefits and limitations,
LLM-enabled and multi-technique synthesized enhancements, and comprehensive
evaluation from both system and user perspectives. Through a structured focus
group study with twelve everyday iOS users, we explored their experiences,
understanding, and perceptions of the feature, suggesting its limited practical
impact resulting from missing important details. We identified two primary user
concerns: the clarity of data access purpose and domain description. In
response, we proposed enhancements including a purpose inference framework and
domain clarification pipeline. We demonstrated the effectiveness and benefits
of such enhancements for mobile app users. This work provides practical
insights that could help enhance user privacy transparency and discusses areas
for future research.

</details>


### [11] [Issue-Oriented Agent-Based Framework for Automated Review Comment Generation](https://arxiv.org/abs/2511.00517)
*Shuochuan Li,Dong Wang,Patanamon Thongtanunam,Zan Wang,Jiuqiao Yu,Junjie Chen*

Main category: cs.SE

TL;DR: 本文提出 RevAgent，一种基于多智能体的面向问题的代码审查评论生成框架，通过将任务分解为生成、判别和训练三个阶段，显著优于现有方法，在自动评价指标和人工评估中均表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有代码审查评论生成方法依赖单一模型处理多样化的代码问题，难以应对复杂场景（如缺陷修复），导致生成的评论信息量不足。

Method: RevAgent 框架包含三个阶段：(1) 五个面向特定问题类别的评论智能体生成候选评论；(2) 一个评判智能体选择最优的问题-评论对；(3) 所有智能体在类别特定数据上微调以增强专业化能力。

Result: RevAgent 在 BLEU、ROUGE-L、METEOR 和 SBERT 指标上分别提升 12.90%、10.87%、6.32% 和 8.57%，在问题类别识别准确率和人工评估（准确性、可读性、上下文感知）方面也表现更优，并在性能与效率之间取得良好平衡。

Conclusion: RevAgent 通过多智能体协同和任务分解，有效提升了代码审查评论生成的质量与实用性，尤其适用于复杂代码变更场景。

Abstract: Code review (CR) is a crucial practice for ensuring software quality. Various
automated review comment generation techniques have been proposed to streamline
the labor-intensive process. However, existing approaches heavily rely on a
single model to identify various issues within the code, limiting the model's
ability to handle the diverse, issue-specific nature of code changes and
leading to non-informative comments, especially in complex scenarios such as
bug fixes. To address these limitations, we propose RevAgent, a novel
agent-based issue-oriented framework, decomposes the task into three stages:
(1) Generation Stage, where five category-specific commentator agents analyze
code changes from distinct issue perspectives and generate candidate comments;
(2) Discrimination Stage, where a critic agent selects the most appropriate
issue-comment pair; and (3) Training Stage, where all agents are fine-tuned on
curated, category-specific data to enhance task specialization. Evaluation
results show that RevAgent significantly outperforms state-of-the-art PLM- and
LLM-based baselines, with improvements of 12.90\%, 10.87\%, 6.32\%, and 8.57\%
on BLEU, ROUGE-L, METEOR, and SBERT, respectively. It also achieves relatively
higher accuracy in issue-category identification, particularly for challenging
scenarios. Human evaluations further validate the practicality of RevAgent in
generating accurate, readable, and context-aware review comments. Moreover,
RevAgent delivers a favorable trade-off between performance and efficiency.

</details>


### [12] [HIP-LLM: A Hierarchical Imprecise Probability Approach to Reliability Assessment of Large Language Models](https://arxiv.org/abs/2511.00527)
*Robab Aghazadeh-Chakherlou,Qing Guo,Siddartha Khastgir,Peter Popov,Xiaoge Zhang,Xingyu Zhao*

Main category: cs.SE

TL;DR: 本文提出了HIP-LLM，一种基于分层不精确概率框架的方法，用于更准确地建模和评估大语言模型（LLM）在实际操作条件下的可靠性。


<details>
  <summary>Details</summary>
Motivation: 现有基于基准的评估方法主要提供模型在数据集上的准确率描述性统计，难以揭示LLM在真实运行环境中的概率行为，因此需要一种更严谨的可靠性评估方法。

Method: HIP-LLM基于软件可靠性工程，将LLM可靠性定义为在给定操作剖面（OP）下在未来若干任务中无故障运行的概率；该方法采用分层结构建模（子）领域间的依赖关系，嵌入不精确先验以刻画认知不确定性，并结合操作剖面反映使用场景，从而推导出后验可靠性包络。

Result: 在多个基准数据集上的实验表明，HIP-LLM相比现有基准方法和前沿技术，能提供更准确、标准化的可靠性刻画。

Conclusion: HIP-LLM为大语言模型的可靠性评估提供了一个兼顾不确定性、使用场景和层次结构的新框架，具有更强的实际应用价值。

Abstract: Large Language Models (LLMs) are increasingly deployed across diverse
domains, raising the need for rigorous reliability assessment methods. Existing
benchmark-based evaluations primarily offer descriptive statistics of model
accuracy over datasets, providing limited insight into the probabilistic
behavior of LLMs under real operational conditions. This paper introduces
HIP-LLM, a Hierarchical Imprecise Probability framework for modeling and
inferring LLM reliability. Building upon the foundations of software
reliability engineering, HIP-LLM defines LLM reliability as the probability of
failure-free operation over a specified number of future tasks under a given
Operational Profile (OP). HIP-LLM represents dependencies across (sub-)domains
hierarchically, enabling multi-level inference from subdomain to system-level
reliability. HIP-LLM embeds imprecise priors to capture epistemic uncertainty
and incorporates OPs to reflect usage contexts. It derives posterior
reliability envelopes that quantify uncertainty across priors and data.
Experiments on multiple benchmark datasets demonstrate that HIP-LLM offers a
more accurate and standardized reliability characterization than existing
benchmark and state-of-the-art approaches. A publicly accessible repository of
HIP-LLM is provided.

</details>


### [13] [Employee Performance when Implementing Agile Practices in an IT Workforce](https://arxiv.org/abs/2511.00528)
*Muhammad Hamid Raza Mookadam,Ridewaan Hanslo*

Main category: cs.SE

TL;DR: 本研究通过访谈南非IT从业者，发现敏捷实践显著提升员工绩效，主要体现在团队协作、沟通、个人发展等方面，但也面临采纳障碍、领导力不足等挑战。


<details>
  <summary>Details</summary>
Motivation: 非洲背景下缺乏关于敏捷实践对员工绩效影响的全面研究，本文旨在填补这一空白，特别是在南非IT行业中的应用情况。

Method: 采用解释主义的单一方法定性研究设计，通过17次半结构化访谈收集来自不同角色的敏捷从业者的观点。

Result: 敏捷实践显著影响员工绩效，积极方面包括规划、沟通、员工发展与福祉、协作、团队文化与进展；挑战包括敏捷采纳困难、团队参与度低、领导力不足及敏捷思维难以建立。

Conclusion: 若能有效应对敏捷实施中的挑战并提供额外支持，IT行业员工的绩效可显著提升。

Abstract: Adoption of agile practices has increased in IT workforces. However, there is
a lack of comprehensive studies in the African context on employee performance
when implementing agile practices. This study addresses this gap by exploring
employee performance in agile environments for IT workforces in South Africa.
An interpretivist mono-method qualitative approach was used, with the use of
interviews as a research strategy. Seventeen semi-structured interviews were
conducted with agile practitioners from various roles. Our results indicated
that agile practices influence employee performance significantly, with
participants reporting on aspects which included planning, communication,
employee development and well-being, collaboration, team culture and progress.
Additionally, our results reported obstacles when using agile practices that
included adoption, team engagement, leadership and instilling an agile mindset.
Agile practices influence employee performance in IT workforces by fostering
improved team dynamics, enhanced collaboration, improved efficiencies, risk
management, planning, continuous improvement, learning, personal development
and well-being. Conclusively, our findings suggest that if agile challenges are
addressed and additional support is provided, employee performance can be
significantly improved.

</details>


### [14] [GDPR-Bench-Android: A Benchmark for Evaluating Automated GDPR Compliance Detection in Android](https://arxiv.org/abs/2511.00619)
*Huaijin Ran,Haoyi Zhang,Xunzhu Tang*

Main category: cs.SE

TL;DR: 本文提出了 GDPR-Bench-Android，这是首个用于评估 Android 应用中 GDPR 合规性自动检测方法的综合基准，包含来自 15 个开源项目的 1951 个手动标注违规实例，并引入了名为 Formal-AST 的新型形式化方法作为确定性基线。研究评估了 11 种方法在两项任务上的表现，发现不同范式在不同任务中各有优势。


<details>
  <summary>Details</summary>
Motivation: 自动化检测源代码中的欧盟《通用数据保护条例》（GDPR）违规行为是一项关键但尚未充分探索的挑战。现有方法缺乏统一、全面的评估基准，难以比较不同技术在 GDPR 合规性检测中的有效性。

Method: 作者构建了 GDPR-Bench-Android 基准，涵盖 23 项 GDPR 条款，提供文件级、模块级和行级粒度的标注数据。同时提出了 Formal-AST 方法作为基于源代码的形式化基线，并定义了两个评估任务：多粒度违规定位（使用 Accuracy@k 评估）和代码片段级多标签分类（使用 macro-F1 等指标评估）。共对 11 种方法进行了评测，包括 8 个先进大语言模型（LLM）、Formal-AST、检索增强生成（RAG）方法和基于 ReAct 框架的智能体方法。

Result: 实验结果显示，没有任何单一范式在所有任务中表现最优。在任务 1 中，ReAct 智能体在文件级 Accuracy@1 上表现最佳（17.38%），而 Qwen2.5-72B 在行级达到最高（61.60%），远高于 Formal-AST 的 1.86%。在更具挑战性的任务 2 中，Claude-Sonnet-4.5 取得最佳 Macro-F1（5.75%），RAG 方法则获得最高 Macro-Precision（7.10%）。

Conclusion: 不同自动化方法在 GDPR 合规性检测任务中展现出任务依赖性的优势，GDPR-Bench-Android 基准能有效诊断各类方法的能力，为未来研究提供了重要基础和方向。

Abstract: Automating the detection of EU General Data Protection Regulation (GDPR)
violations in source code is a critical but underexplored challenge. We
introduce \textbf{GDPR-Bench-Android}, the first comprehensive benchmark for
evaluating diverse automated methods for GDPR compliance detection in Android
applications. It contains \textbf{1951} manually annotated violation instances
from \textbf{15} open-source repositories, covering 23 GDPR articles at file-,
module-, and line-level granularities. To enable a multi-paradigm evaluation,
we contribute \textbf{Formal-AST}, a novel, source-code-native formal method
that serves as a deterministic baseline. We define two tasks: (1)
\emph{multi-granularity violation localization}, evaluated via
Accuracy@\textit{k}; and (2) \emph{snippet-level multi-label classification},
assessed by macro-F1 and other classification metrics. We benchmark 11 methods,
including eight state-of-the-art LLMs, our Formal-AST analyzer, a
retrieval-augmented (RAG) method, and an agentic (ReAct) method. Our findings
reveal that no single paradigm excels across all tasks. For Task 1, the ReAct
agent achieves the highest file-level Accuracy@1 (17.38%), while the
Qwen2.5-72B LLM leads at the line level (61.60%), in stark contrast to the
Formal-AST method's 1.86%. For the difficult multi-label Task 2, the
Claude-Sonnet-4.5 LLM achieves the best Macro-F1 (5.75%), while the RAG method
yields the highest Macro-Precision (7.10%). These results highlight the
task-dependent strengths of different automated approaches and underscore the
value of our benchmark in diagnosing their capabilities. All resources are
available at: https://github.com/Haoyi-Zhang/GDPR-Bench-Android.

</details>


### [15] [Can Large Language Models Detect Real-World Android Software Compliance Violations?](https://arxiv.org/abs/2511.00624)
*Haoyi Zhang,Huaijin Ran,Xunzhu Tang*

Main category: cs.SE

TL;DR: 本文提出了CompliBench，一个用于评估大语言模型（LLMs）在检测Android应用中遵守LGPD、PDPA和PIPEDA等法规能力的新框架，包含代码检索定位和多标签判断两个任务，并引入新的稳定性感知评估指标，实验表明该框架能有效提升LLMs在合规性检测任务中的表现。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型在检测Android应用跨不同法律框架下的合规违规方面存在困难，缺乏有效的评估方法来衡量其在真实审计场景中的表现。

Method: 提出CompliBench评估框架，定义两个任务：任务1评估模型在文件、模块和行级别上的违规代码检索与定位能力；任务2评估模型对代码片段的多标签合规判断能力。同时引入稳定性感知的复合指标（SGS、RCS、CRGS、OCS）以更全面地评估模型性能。

Result: 在六个主流大模型上的实验表明，CompliBench能有效评估和提升合规检测能力，其中Claude-3.5-sonnet-20241022在OCS指标上表现最佳（0.3295），Gemini-2.5-pro表现最差（0.0538）。

Conclusion: CompliBench为评估和改进大语言模型在数据保护合规任务中的性能提供了有效工具和基准，为未来开发符合法规标准的自动化审计工具奠定了基础。

Abstract: The rapid development of Large Language Models (LLMs) has transformed
software engineering, showing promise in tasks like code generation, bug
detection, and compliance checking. However, current models struggle to detect
compliance violations in Android applications across diverse legal frameworks.
We propose \emph{CompliBench}, a novel evaluation framework for assessing LLMs'
ability to detect compliance violations under regulations like LGPD, PDPA, and
PIPEDA. The framework defines two tasks: Task 1 evaluates \emph{retrieval and
localization} at file, module, and line granularities, and Task 2 assesses
\emph{multi-label judgment} for code snippets. These tasks mirror the audit
process, where auditors locate problematic code and determine implicated
provisions. Traditional metrics fail to capture important aspects like
cross-granularity stability and jurisdictional consistency. Thus, we introduce
stability-aware composites (SGS, RCS, CRGS, and OCS) for a more comprehensive
assessment. Experiments with six models, including GPT-4O and Claude-3.5, show
\emph{CompliBench} improves compliance detection, with
Claude-3.5-sonnet-20241022 achieving the highest OCS score (0.3295), and
Gemini-2.5-pro the lowest (0.0538). This work demonstrates \emph{CompliBench}'s
potential for improving LLM performance in compliance tasks and provides a
foundation for future tools aligned with data protection standards. Our project
is available at https://github.com/Haoyi-Zhang/CompliBench.

</details>


### [16] [Lessons Learned from the Use of Generative AI in Engineering and Quality Assurance of a WEB System for Healthcare](https://arxiv.org/abs/2511.00658)
*Guilherme H. Travassos,Sabrina Rocha,Rodrigo Feitosa,Felipe Assis,Patricia Goncalves,Andre Gheventer,Larissa Galeno,Arthur Sasse,Julio Cesar Guimaraes,Carlos Brito,Joao Pedro Wieland*

Main category: cs.SE

TL;DR: 本文报告了在开发用于胸科疾病临床试验的Web软件系统过程中，团队在软件工程各阶段（如项目管理、需求、设计、开发和质量保证）应用生成式AI的初步经验，尽管技术尚不成熟，但提供了对提升软件质量具有参考价值的实践见解。


<details>
  <summary>Details</summary>
Motivation: 生成式人工智能技术快速发展并开始影响软件工程实践，尽管其在该领域的应用尚处早期且缺乏成熟研究成果，作者团队希望探索其在真实项目中的应用潜力，并分享经验以供其他组织参考。

Method: 通过在一个面向胸科疾病临床试验的Web软件系统开发项目中，将生成式AI技术应用于项目管理、需求规格说明、设计、开发和质量保证等软件工程活动，并记录团队的学习过程与实践反馈。

Result: 虽然尚未获得足以显著改变开发流程的确凿技术证据，但团队在实践中获得了有价值的见解，并提出了对希望借助生成式AI提升软件质量的组织具有参考意义的建议。

Conclusion: 生成式AI在软件工程中的应用仍处于探索阶段，但早期实践经验表明其具备提升开发效率与软件质量的潜力，值得软件组织持续关注和尝试。

Abstract: The advances and availability of technologies involving Generative Artificial
Intelligence (AI) are evolving clearly and explicitly, driving immediate
changes in various work activities. Software Engineering (SE) is no exception
and stands to benefit from these new technologies, enhancing productivity and
quality in its software development processes. However, although the use of
Generative AI in SE practices is still in its early stages, considering the
lack of conclusive results from ongoing research and the limited technological
maturity, we have chosen to incorporate these technologies in the development
of a web-based software system to be used in clinical trials by a thoracic
diseases research group at our university. For this reason, we decided to share
this experience report documenting our development team's learning journey in
using Generative AI during the software development process. Project
management, requirements specification, design, development, and quality
assurance activities form the scope of observation. Although we do not yet have
definitive technological evidence to evolve our development process
significantly, the results obtained and the suggestions shared here represent
valuable insights for software organizations seeking to innovate their
development practices to achieve software quality with generative AI.

</details>


### [17] [Repairing Responsive Layout Failures Using Retrieval Augmented Generation](https://arxiv.org/abs/2511.00678)
*Tasmia Zerin,Moumita Asad,B. M. Mainul Hossain,Kazi Sakib*

Main category: cs.SE

TL;DR: 本文提出了一种名为ReDeFix的自动化修复方法，结合大语言模型（LLM）与Stack Overflow中的领域知识，通过检索增强生成（RAG）技术修复响应式网页布局失效（RLFs），在实验中达到88%的修复准确率，并获得工程师对视觉效果和美观性的认可。


<details>
  <summary>Details</summary>
Motivation: 响应式网站在特定屏幕尺寸下常出现布局失真（RLFs），而人工修复过程繁琐且依赖反复试错，因此亟需一种自动化、高效且可靠的修复方案。

Method: 提出ReDeFix方法，利用检索增强生成（RAG）框架，从Stack Overflow中检索与RLF相关的讨论，并结合具体上下文构建提示，引导大语言模型生成针对性的CSS修复补丁。

Result: 实验表明，ReDeFix在修复RLFs方面准确率达到88%；软件工程师评估认为生成的修复结果不仅布局正确，而且保持了良好的视觉美观性。

Conclusion: 结合领域知识与大语言模型的ReDeFix能有效自动化修复响应式布局失效问题，具有高准确率和良好的实用性，为前端开发中的布局调试提供了新思路。

Abstract: Responsive websites frequently experience distorted layouts at specific
screen sizes, called Responsive Layout Failures (RLFs). Manually repairing
these RLFs involves tedious trial-and-error adjustments of HTML elements and
CSS properties. In this study, an automated repair approach, leveraging LLM
combined with domain-specific knowledge is proposed. The approach is named
ReDeFix, a Retrieval-Augmented Generation (RAG)-based solution that utilizes
Stack Overflow (SO) discussions to guide LLM on CSS repairs. By augmenting
relevant SO knowledge with RLF-specific contexts, ReDeFix creates a prompt that
is sent to the LLM to generate CSS patches. Evaluation demonstrates that our
approach achieves an 88\% accuracy in repairing RLFs. Furthermore, a study from
software engineers reveals that generated repairs produce visually correct
layouts while maintaining aesthetics.

</details>


### [18] [An Empirical Investigation of the Experiences of Dyslexic Software Engineers](https://arxiv.org/abs/2511.00706)
*Marcos Vinicius Cruz,Pragya Verma,Grischa Liebel*

Main category: cs.SE

TL;DR: 本文通过质性研究探讨了阅读障碍软件工程师在软件工程中的经历，发现他们在编程学习初期面临困难，但掌握后能胜任甚至在多项任务中表现出色；常用工具（如代码补全、代码检查器）可有效缓解其困难，且他们在视觉思维和创造力方面具有优势。


<details>
  <summary>Details</summary>
Motivation: 目前尚无研究系统探讨阅读障碍软件工程师的实际体验，以及其优势与困难之间的关系。鉴于阅读障碍在阅读和写作上的影响可能对软件工程核心任务（如编程）构成挑战，而初步研究表明其表现未必逊于非阅读障碍者，甚至可能具备独特优势，因此有必要深入理解这一群体在软件工程中的真实处境。

Method: 采用社会技术扎根理论（Socio-Technical Grounded Theory）的基本阶段，通过质性研究方法，收集并分析了10位阅读障碍软件工程师的访谈数据、3篇博客文章以及Reddit平台上153篇相关帖子。

Result: 研究发现：（1）阅读障碍软件工程师在编程学习初期困难较大，但一旦掌握基础后能在多项软件工程任务中成功甚至表现优异；（2）代码补全、代码检查器等常见工具能有效缓解其困难；（3）他们在视觉思维和创造力方面展现出明显优势。

Conclusion: 阅读障碍软件工程师虽在初期面临挑战，但可通过工具支持克服困难，并凭借其独特优势在软件工程中取得成功。研究结果对软件工程实践具有启示意义，并呼吁未来研究进一步探索影响阅读障碍者代码可理解性的因素。

Abstract: Dyslexia is a common learning disorder that primarily impairs an individual's
reading and writing abilities. In adults, dyslexia can affect both professional
and personal lives, often leading to mental challenges and difficulties
acquiring and keeping work. In Software Engineering (SE), reading and writing
difficulties appear to pose substantial challenges for core tasks such as
programming. However, initial studies indicate that these challenges may not
significantly affect their performance compared to non-dyslexic colleagues.
Conversely, strengths associated with dyslexia could be particularly valuable
in areas like programming and design. However, there is currently no work that
explores the experiences of dyslexic software engineers, and puts their
strengths into relation with their difficulties. To address this, we present a
qualitative study of the experiences of dyslexic individuals in SE. We followed
the basic stage of the Socio-Technical Grounded Theory method and base our
findings on data collected through 10 interviews with dyslexic software
engineers, 3 blog posts and 153 posts on the social media platform Reddit. We
find that dyslexic software engineers especially struggle at the programming
learning stage, but can succeed and indeed excel at many SE tasks once they
master this step. Common SE-specific support tools, such as code completion and
linters are especially useful to these individuals and mitigate many of the
experienced difficulties. Finally, dyslexic software engineers exhibit
strengths in areas such as visual thinking and creativity. Our findings have
implications to SE practice and motivate several areas of future research in
SE, such as investigating what makes code less/more understandable to dyslexic
individuals.

</details>


### [19] [A Systematic Literature Review of Code Hallucinations in LLMs: Characterization, Mitigation Methods, Challenges, and Future Directions for Reliable AI](https://arxiv.org/abs/2511.00776)
*Cuiyun Gao,Guodong Fan,Chun Yong Chong,Shizhan Chen,Chao Liu,David Lo,Zibin Zheng,Qing Liao*

Main category: cs.SE

TL;DR: 本文系统综述了面向代码的大语言模型中的幻觉问题，从定义与成因、通用缓解策略、代码特定挑战与任务应用、以及评估基准四个方面进行了分析。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在高风险代码智能任务中面临严重的幻觉问题，随着其在软件工程中的广泛应用，理解和缓解代码幻觉变得至关重要。

Method: 通过系统性文献综述，分析60篇相关论文，从四个维度探讨代码幻觉：定义与成因、通用缓解方法、代码特定挑战及新兴任务应用、评估基准。

Result: 总结了代码幻觉的主要成因（如数据噪声、暴露偏差、语义接地不足），归纳了通用与代码特定的缓解策略，并指出现有评估基准的不足及对专用基准的需求。

Conclusion: 代码幻觉是代码大模型面临的关键挑战，需结合软件工程特性开发针对性缓解方法，并建立面向幻觉的评估体系以推动研究进展。

Abstract: Model hallucination is one of the most critical challenges faced by Large
Language Models (LLMs), especially in high-stakes code intelligence tasks. As
LLMs become increasingly integrated into software engineering tasks,
understanding and mitigating hallucination in code becomes essential. In this
survey, we provide a systematic review of hallucination phenomena in
code-oriented LLMs from four key perspectives. First, we begin by surveying 60
papers to define hallucination in the context of code and summarize its primary
causes, such as data noise, exposure bias, and insufficient semantic grounding,
while also tracing recent trends in literature across natural language
processing (NLP) and software engineering communities. Second, we review model
hallucination surveys in a broader span and summarize representative
hallucination mitigation strategies, such as knowledge-enhanced generation,
constrained decoding, and post-editing. Third, we review approaches targeted
for code intelligence and highlight code-specific challenges that aggravate
hallucination, including syntax sensitivity, strict type systems, and
dependence on external libraries. Meanwhile, we analyze how emerging code
intelligence tasks, e.g., program analysis, symbolic execution, and unit
testing, are utilized to detect and mitigate hallucinations. Fourth, we
summarize current evaluation benchmarks, ranging from static metrics to dynamic
checks, e.g., compilation and execution correctness, and emphasize the need for
hallucination-oriented benchmarks.

</details>


### [20] [Can Language Models Go Beyond Coding? Assessing the Capability of Language Models to Build Real-World Systems](https://arxiv.org/abs/2511.00780)
*Chenyu Zhao,Shenglin Zhang,Zeshun Huang,Weilin Jin,Yongqian Sun,Dan Pei,Chaoyun Zhang,Qingwei Lin,Chetan Bansal,Saravan Rajmohan,Minghua Ma*

Main category: cs.SE

TL;DR: 本文提出了Build-bench，首个面向跨指令集架构（如x86_64与aarch64）迁移场景的端到端基准，用于评估大语言模型（LLM）在修复构建失败方面的能力。该基准包含268个真实失败软件包，并结合结构提取、文件内容修改与构建验证等辅助工具，支持模型进行自主、工具增强的迭代修复。实验表明，当前最优LLM的构建成功率仅为63%，且不同模型在工具使用模式上差异显著。


<details>
  <summary>Details</summary>
Motivation: 现有基准缺乏对大语言模型在跨指令集架构（ISA）迁移过程中修复软件构建失败能力的系统评估。跨ISA迁移涉及复杂依赖、异构工具链和冗长构建日志，亟需一个能结合真实构建环境与可验证结果的评测框架。

Method: 作者构建了Build-bench基准，收集268个真实世界中在跨ISA迁移时构建失败的软件包，并集成结构提取、文件内容提取、内容修改和构建验证等辅助工具。修复过程采用迭代循环机制：模型在构建失败后接收更新的构建日志和先前修复结果，以优化后续尝试。

Result: 在对六种代表性大语言模型的评估中，Build-bench显示当前模型最高构建成功率为63%，且不同模型在工具使用策略上存在显著差异。

Conclusion: Build-bench是首个结合真实构建环境与可验证结果的架构感知基准，为研究大语言模型在软件构建与修复任务中的能力提供了系统性评估平台。

Abstract: Large language models (LLMs) have shown growing potential in software
engineering, yet few benchmarks evaluate their ability to repair software
during migration across instruction set architectures (ISAs). Cross-ISA
migration, such as between x86_64 and aarch64, requires handling complex
dependencies, heterogeneous toolchains, and long build logs while ensuring
executable verification. To address this challenge, we present Build-bench, an
end-to-end benchmark that systematically evaluates the capability of LLMs to
repair build failures in cross-ISA settings. Build-bench collects 268
real-world failed packages and integrates auxiliary tools including Structure
Extraction, File Content Extraction, Content Modification, and Build
Verification to support autonomous, tool-augmented reasoning. The repair
process operates in an iterative loop where, upon failure, the model receives
updated build logs and previous repair outcomes to refine subsequent attempts.
Through a comparative evaluation of six representative LLMs, Build-bench
reveals that current models achieve a maximum build success rate of 63% and
tool usage patterns differ significantly across models. By coupling real build
environments with verifiable outcomes, Build-bench establishes the first
architecture-aware benchmark for studying LLM-based software build and repair.

</details>


### [21] [CodeClash: Benchmarking Goal-Oriented Software Engineering](https://arxiv.org/abs/2511.00839)
*John Yang,Kilian Lieret,Joyce Yang,Carlos E. Jimenez,Ofir Press,Ludwig Schmidt,Diyi Yang*

Main category: cs.SE

TL;DR: 本文提出了CodeClash，一个用于评估语言模型在无明确指导情况下迭代开发代码以实现开放式目标的新基准。该基准通过多轮锦标赛形式，让模型在竞争环境中自主改进代码库，并在六个不同场景中对8个语言模型进行了评估。结果表明，尽管模型展现出多样的开发风格，但在战略推理和长期代码维护方面存在明显不足，甚至顶尖模型在每一轮比赛中均败于人类专家。


<details>
  <summary>Details</summary>
Motivation: 现有代码生成基准主要关注具体、明确的任务（如修复特定bug或编写针对性测试），而现实中的软件开发通常围绕高层次目标（如提升用户留存或降低成本）展开。目前尚缺乏有效方法来评估语言模型在无显式指导的情况下，能否通过迭代开发代码来达成开放性目标。

Method: 作者设计了CodeClash基准，其中语言模型在多轮锦标赛中竞争，目标是构建最能实现特定竞争目标的代码库。每轮包括两个阶段：模型编辑其代码，然后在“代码竞技场”中与其他模型的代码库进行对抗，依据得分最大化、资源获取或生存能力等目标判定胜负。模型需自主决定如何改进代码，包括写笔记、查阅文档、分析对手日志或创建测试套件等。研究共运行1680场锦标赛（总计25,200轮），评估了8个语言模型在6个竞技场中的表现。

Result: 实验结果显示，尽管不同模型展现出多样化的开发策略，但它们在战略推理方面存在共同的根本性局限。此外，模型难以进行长期的代码库维护，导致代码库逐渐变得混乱和冗余。最显著的是，即使是表现最好的模型，在与人类专家程序员的对抗中也输掉了所有回合。

Conclusion: CodeClash揭示了当前语言模型在面向目标的自主代码开发中的关键短板，特别是在战略规划和代码可维护性方面。作者开源了CodeClash，以推动对自主、目标导向型代码生成的进一步研究。

Abstract: Current benchmarks for coding evaluate language models (LMs) on concrete,
well-specified tasks such as fixing specific bugs or writing targeted tests.
However, human programmers do not spend all day incessantly addressing isolated
tasks. Instead, real-world software development is grounded in the pursuit of
high-level goals, like improving user retention or reducing costs. Evaluating
whether LMs can also iteratively develop code to better accomplish open-ended
objectives without any explicit guidance remains an open challenge. To address
this, we introduce CodeClash, a benchmark where LMs compete in multi-round
tournaments to build the best codebase for achieving a competitive objective.
Each round proceeds in two phases: agents edit their code, then their codebases
compete head-to-head in a code arena that determines winners based on
objectives like score maximization, resource acquisition, or survival. Whether
it's writing notes, scrutinizing documentation, analyzing competition logs, or
creating test suites, models must decide for themselves how to improve their
codebases both absolutely and against their opponents. We run 1680 tournaments
(25,200 rounds total) to evaluate 8 LMs across 6 arenas. Our results reveal
that while models exhibit diverse development styles, they share fundamental
limitations in strategic reasoning. Models also struggle with long-term
codebase maintenance, as repositories become progressively messy and redundant.
These limitations are stark: top models lose every round against expert human
programmers. We open-source CodeClash to advance the study of autonomous,
goal-oriented code development.

</details>


### [22] [A Comprehensive Empirical Evaluation of Agent Frameworks on Code-centric Software Engineering Tasks](https://arxiv.org/abs/2511.00872)
*Zhuowen Yin,Cuifeng Gao,Chunsong Fan,Wenzhang Yang,Yinxing Xue,Lijun Zhang*

Main category: cs.SE

TL;DR: 本文对七个通用智能体框架在软件开发、漏洞检测和程序修复三项代码密集型任务中的表现进行了全面实证研究，从有效性、效率和开销三个维度进行评估，揭示了不同框架的能力特点与权衡。


<details>
  <summary>Details</summary>
Motivation: 现有研究多聚焦于特定任务或孤立方面，缺乏对智能体在软件工程中实际能力的全面理解，因此需要系统性评估以提供更完整的图景。

Method: 在三个代表性代码任务上使用标准基准对七个通用智能体框架进行评估，从有效性（任务成功率）、效率（执行过程）和开销（token消耗）三个互补视角进行系统分析，并深入探讨有效性与效率之间的关系。

Result: 智能体整体有效性中等；AgentOrchestra因协调开销导致执行轨迹最长、修正尝试最多，而OpenHands展现出更强的反思推理能力；软件开发任务成本最高，GPTswarm最具成本效益。

Conclusion: 研究结果揭示了不同智能体框架在软件工程任务中的能力模式与权衡，为实际应用和未来高效智能体的研究提供了指导。

Abstract: Unlike traditional automation tools or static LLM-based systems, agents
combine decision-making and tool utilization to accomplish complex tasks,
showing great potential in software engineering. However, existing studies
largely focus on specific tasks or isolated aspects, providing an incomplete
picture of agents' practical capabilities. To address this, we conduct a
comprehensive empirical study evaluating seven general-purpose agent frameworks
across three representative code-centric tasks: software development,
vulnerability detection, and program repair. Each task is assessed using
standard, widely adopted benchmarks to ensure objective and comparable
evaluation. Agent performance is systematically analyzed from three
complementary perspectives: effectiveness (task success), efficiency (execution
process), and overhead (token consumption). Our findings reveal distinct
capability patterns and trade-offs among the evaluated frameworks. In terms of
effectiveness, agents achieve moderate overall performance. Regarding
efficiency, AgentOrchestra tends to exhibit the longest trajectories and the
most correction attempts due to coordination overhead, whereas OpenHands
demonstrate stronger reflective reasoning abilities. For overhead, software
development incurs the highest monetary cost, while GPTswarm remains the most
cost-efficient. Furthermore, we conduct an in-depth cross-analysis of the
relationship between effectiveness and efficiency, exploring the underlying
reasons behind their interplay. These findings guide both practical adoption
and future research toward more efficient software engineering agents.

</details>


### [23] [Sustainability of Machine Learning-Enabled Systems: The Machine Learning Practitioner's Perspective](https://arxiv.org/abs/2511.00901)
*Vincenzo De Martino,Stefano Lambiase,Fabiano Pecorelli,Willem-Jan van den Heuvel,Filomena Ferrucci,Fabio Palomba*

Main category: cs.SE

TL;DR: 该论文通过访谈和大规模调查，研究了机器学习工程师如何理解、实践和应对软件可持续性挑战，发现尽管对可持续性有认知，但在系统性实施方面存在显著差距。


<details>
  <summary>Details</summary>
Motivation: 现有研究多聚焦于环境可持续性（如碳足迹），缺乏对社会、经济等更广泛可持续性维度的实证研究，也未充分揭示从业者在实际ML工作流中管理可持续性的具体做法与障碍。

Method: 结合定性与定量方法：首先对8名资深ML工程师进行访谈，再对203名ML从业者开展大规模问卷调查，从从业者视角分析可持续性在ML系统中的实践现状。

Result: 研究发现从业者对可持续性有基本认知，但缺乏系统实施；实践中缺少结构化指南、度量框架和监管支持，导致可持续性难以有效融入ML工作流。

Conclusion: 为促进ML系统的可持续性，亟需制定更明确的工程指南、可操作的度量体系以及政策支持，以弥合认知与实践之间的鸿沟。

Abstract: Software sustainability is a key multifaceted non-functional requirement that
encompasses environmental, social, and economic concerns, yet its integration
into the development of Machine Learning (ML)-enabled systems remains an open
challenge. While previous research has explored high-level sustainability
principles and policy recommendations, limited empirical evidence exists on how
sustainability is practically managed in ML workflows. Existing studies
predominantly focus on environmental sustainability, e.g., carbon footprint
reduction, while missing the broader spectrum of sustainability dimensions and
the challenges practitioners face in real-world settings. To address this gap,
we conduct an empirical study to characterize sustainability in ML-enabled
systems from a practitioner's perspective. We investigate (1) how ML engineers
perceive and describe sustainability, (2) the software engineering practices
they adopt to support it, and (3) the key challenges hindering its adoption. We
first perform a qualitative analysis based on interviews with eight experienced
ML engineers, followed by a large-scale quantitative survey with 203 ML
practitioners. Our key findings reveal a significant disconnection between
sustainability awareness and its systematic implementation, highlighting the
need for more structured guidelines, measurement frameworks, and regulatory
support.

</details>


### [24] [Empirical Derivations from an Evolving Test Suite](https://arxiv.org/abs/2511.00915)
*Jukka Ruohonen,Abhishek Tiwari*

Main category: cs.SE

TL;DR: 该论文对NetBSD操作系统的自动化、持续性、基于虚拟化的软件测试套件进行了纵向实证分析（2010年代初至2025年底），发现测试用例数量持续增长（现已超1万），失败率总体稳定，且代码变更与内核修改对失败率的统计解释力有限。


<details>
  <summary>Details</summary>
Motivation: 为了理解大规模、持续演化的软件测试套件在长期运行中的行为特征，特别是测试失败模式与代码变更之间的关系。

Method: 对NetBSD操作系统自2010年代初至2025年底的测试套件数据进行纵向实证分析，涵盖测试用例数量、失败类型（测试失败、构建失败、安装失败等）以及代码变更和内核修改的影响。

Result: 测试套件持续增长至超1万个测试用例；各类失败总体稳定，偶有短期波动；代码变更和内核修改对失败率的长期统计影响较小。

Conclusion: 尽管仅具探索性，该研究为从大规模演化测试套件中得出实证结论提供了初步依据，表明测试失败并不总是与代码或内核变更显著相关。

Abstract: The paper presents a longitudinal empirical analysis of the automated,
continuous, and virtualization-based software test suite of the NetBSD
operating system. The longitudinal period observed spans from the initial roll
out of the test suite in the early 2010s to late 2025. According to the
results, the test suite has grown continuously, currently covering over ten
thousand individual test cases. Failed test cases exhibit overall stability,
although there have been shorter periods marked with more frequent failures. A
similar observation applies to build failures, failures of the test suite to
complete, and installation failures, all of which are also captured by the
NetBSD's testing framework. Finally, code churn and kernel modifications do not
provide longitudinally consistent statistical explanations for the failures.
Although some periods exhibit larger effects, including particularly with
respect to the kernel modifications, the effects are small on average. Even
though only in an exploratory manner, these empirical observations contribute
to efforts to draw conclusions from large-scale and evolving software test
suites.

</details>


### [25] [DPO-F+: Aligning Code Repair Feedback with Developers' Preferences](https://arxiv.org/abs/2511.01043)
*Zihan Fang,Yifan Zhang,Yueke Zhang,Kevin Leach,Yu Huang*

Main category: cs.SE

TL;DR: DPO-f+ 是一个新颖的框架，通过结合开发者画像与领域特定指标，利用增强的直接偏好优化（DPO）方法，提升大语言模型在代码修复任务中生成的自然语言反馈的对齐度与实用性，从而改善人机协作效果。


<details>
  <summary>Details</summary>
Motivation: 现有代码修复方法主要关注修复代码本身，忽视了有助于开发者理解与迭代改进的自然语言反馈，限制了人与AI的有效协作。

Method: DPO-f+ 框架包含四个核心部分：(1) 基于开发者画像和领域知识定义反馈对齐指标；(2) 从代码修复任务中自动构建成对偏好数据集；(3) 使用带轻量边界信号的直接偏好优化（DPO）进行微调；(4) 提供自动化的反馈评估协议。

Result: 在多个基准测试中，DPO-f+ 在代码准确率和反馈对齐度上均优于基线模型和标准 DPO。在初级编程任务中，top-1 通过率比基线提升 5.71 个百分点，比 DPO 提升 3.30 个百分点；在 SWE-bench Lite 上，问题解决率分别提升 4.67 和 1.67 个百分点。

Conclusion: DPO-f+ 通过更贴合开发者需求的反馈，将 LLM 辅助代码修复从一次性输出转变为协作式理解流程，有效提升了代码理解能力与人机协作效率。

Abstract: Large Language Models (LLMs) are increasingly applied to software engineering
tasks, especially code repair. However, developers often struggle to interpret
model outputs, limiting effective human-AI teaming. Prior work largely
optimizes repaired code while under-addressing the natural-language feedback
that enables comprehension and iterative improvement. We present DPO-f+, a
novel framework that aligns code-repair feedback with developer needs and
profiles. It (1) formalizes developer-profiled, domain-specific metrics for
feedback alignment; (2) automatically constructs pairwise preference datasets
from code-repair tasks; (3) fine-tunes using Direct Preference Optimization
(DPO) augmented with a lightweight margin signal; and (4) provides an automated
feedback evaluation protocol. Empirically, DPO-f+ outperforms both the baseline
and standard DPO on generated-code accuracy and overall feedback alignment. On
novice programming tasks, DPO-f+ raises the top-1 pass rate by 5.71 percentage
points (pp) over the baseline and by 3.30 pp over DPO. On the more challenging
SWE-bench Lite benchmark, it increases the issue-resolution rate by 1.67 pp
over DPO and by 4.67 pp over the baseline. It also achieves the largest
improvement in feedback alignment, outperforming DPO and the baseline. By
aligning feedback more closely with developer needs, DPO-f+ turns LLM-assisted
repair from one-shot outputs into a collaborative sensemaking workflow,
providing a practical approach to enhancing code comprehension and fostering
more effective human-AI teaming in software engineering.

</details>


### [26] [HAFixAgent: History-Aware Automated Program Repair Agent](https://arxiv.org/abs/2511.01047)
*Yu Shi,Hao Li,Bram Adams,Ahmed E. Hassan*

Main category: cs.SE

TL;DR: 本文提出HAFixAgent，一种利用版本库历史信息增强修复能力的智能体系统，在多片段复杂缺陷修复任务中显著优于现有方法，同时保持高效和低成本。


<details>
  <summary>Details</summary>
Motivation: 现有基于大语言模型和智能体的自动程序修复（APR）系统大多仅依赖局部快照上下文，忽略了版本库历史信息；而先前研究表明，版本库历史对单行缺陷修复有效，作者因此探索其在复杂多片段缺陷修复中的潜力。

Method: 提出HAFixAgent，将基于blame的版本库启发式信息注入智能体修复循环；通过对Defects4J中854个真实缺陷的初步分析，验证历史信息的可用性与集中性，并整合多种历史启发式策略。

Result: HAFixAgent相比当前最先进的智能体基线和多片段基线，修复成功率分别提升212.3%和29.9%；修复过程未显著增加智能体步骤或token成本，尤其在复杂多文件多片段缺陷上成本更低；组合不同历史启发式策略可修复更多缺陷，具有明确的成本效益权衡。

Conclusion: HAFixAgent为历史感知的智能体自动程序修复提供了实用方案：以版本控制历史为基础，优先使用基于diff的历史上下文，并在需要时整合互补启发式策略。

Abstract: Automated program repair (APR) has recently shifted toward large language
models and agent-based systems, yet most systems rely on local snapshot
context, overlooking repository history. Prior work shows that repository
history helps repair single-line bugs, since the last commit touching the buggy
line is often the bug-introducing one. In this paper, we investigate whether
repository history can also improve agentic APR systems at scale, especially
for complex multi-hunk bugs. We present HAFixAgent, a History-Aware Bug-Fixing
Agent that injects blame-derived repository heuristics into its repair loop. A
preliminary study of all 854 real-world bugs from Defects4J motivates our
design, showing that bug-relevant history is both widely available and highly
concentrated. Empirical comparison of HAFixAgent with two state-of-the-art
baselines shows: (1) Effectiveness: HAFixAgent significantly improves over the
agent-based baseline (by 212.3%) and the multi-hunk baseline (by 29.9%). (2)
Efficiency: history does not significantly increase agent steps and keeps token
costs comparable, with notably lower median costs for complex
multi-file-multi-hunk bugs. (3) Practicality: combining different historical
heuristics repairs more bugs, offering a clear cost-benefit trade-off.
HAFixAgent offers a practical recipe for history-aware agentic APR: ground the
agent in version control history, prioritize diff-based historical context, and
integrate complementary heuristics when needed.

</details>


### [27] [HarnessLLM: Automatic Testing Harness Generation via Reinforcement Learning](https://arxiv.org/abs/2511.01104)
*Yujian Liu,Jiabao Ji,Yang Zhang,Wenbo Guo,Tommi Jaakkola,Shiyu Chang*

Main category: cs.SE

TL;DR: HarnessLLM 是一种两阶段训练的 LLM 方法，通过生成测试代码（而非仅输入-输出对）提升测试多样性和调试能力，并在缺陷检测和代码生成性能上表现更优。


<details>
  <summary>Details</summary>
Motivation: 现有基于 LLM 的自动测试生成方法主要生成输入与期望输出对，测试多样性有限且缺乏足够的调试信息。

Method: 提出 HarnessLLM，采用监督微调（SFT）加基于自定义奖励设计的强化学习（RLVR）两阶段训练流程，使 LLM 能生成包含输入合成与输出验证逻辑的测试代码。

Result: 实验表明 HarnessLLM 在发现缺陷和测试策略多样性方面优于传统输入-输出测试方法，并可通过测试时缩放提升代码生成性能。

Conclusion: HarnessLLM 有效提升了 LLM 在自动测试生成中的能力，不仅增强测试效果，还能反哺代码生成任务。

Abstract: Existing LLM-based automatic test generation methods mainly produce input and
expected output pairs to categorize the intended behavior of correct programs.
Although straightforward, these methods have limited diversity in generated
tests and cannot provide enough debugging information. We propose HarnessLLM, a
two-stage training pipeline that enables LLMs to write harness code for
testing. Particularly, LLMs generate code that synthesizes inputs and validates
the observed outputs, allowing complex test cases and flexible output
validation such as invariant checking. To achieve this, we train LLMs with SFT
followed by RLVR with a customized reward design. Experiments show that
HarnessLLM outperforms input-output-based testing in bug finding and testing
strategy diversity. HarnessLLM further benefits the code generation performance
through test-time scaling with our generated test cases as inference-phase
validation. Our code is available at
https://github.com/UCSB-NLP-Chang/HarnessLLM.git.

</details>


### [28] [An Empirical Study of LLM-Based Code Clone Detection](https://arxiv.org/abs/2511.01176)
*Wenqing Zhu,Norihiro Yoshida,Eunjong Choi,Yutaka Matsubara,Hiroaki Takada*

Main category: cs.SE

TL;DR: 本文研究了大语言模型（LLMs）在代码克隆检测任务中的跨数据集性能和响应一致性，发现LLMs在CodeNet数据集上表现优异，但在BigCloneBench数据集上性能显著下降，同时大多数模型具有高响应一致性。


<details>
  <summary>Details</summary>
Motivation: 现有研究尚未充分探讨大语言模型在不同代码克隆数据集上的性能一致性及其在多次运行中判断结果的稳定性。

Method: 构建七个基于CodeNet和BigCloneBench的代码克隆数据集，采用Levenshtein比值采样代码对，并在四种已有提示下评估五个LLMs的性能与一致性。

Result: LLMs在CodeNet相关数据集上F1分数高达0.943（o3-mini），但在BigCloneBench相关数据集上性能显著下降；多数模型响应一致性超过90%，F1分数因不一致性引起的波动小于0.03。

Conclusion: 大语言模型在代码克隆检测任务中表现出数据集依赖性，尽管其响应一致性较高，但在不同数据集上的泛化能力仍需提升。

Abstract: Large language models (LLMs) have demonstrated remarkable capabilities in
various software engineering tasks, such as code generation and debugging,
because of their ability to translate between programming languages and natural
languages. Existing studies have demonstrated the effectiveness of LLMs in code
clone detection. However, two crucial issues remain unaddressed: the ability of
LLMs to achieve comparable performance across different datasets and the
consistency of LLMs' responses in code clone detection. To address these
issues, we constructed seven code clone datasets and then evaluated five LLMs
in four existing prompts with these datasets. The datasets were created by
sampling code pairs using their Levenshtein ratio from two different code
collections, CodeNet and BigCloneBench. Our evaluation revealed that although
LLMs perform well in CodeNet-related datasets, with o3-mini achieving a 0.943
F1 score, their performance significantly decreased in BigCloneBench-related
datasets. Most models achieved a high response consistency, with over 90\% of
judgments remaining consistent across all five submissions. The fluctuations of
the F1 score affected by inconsistency are also tiny; their variations are less
than 0.03.

</details>


### [29] [Exploringand Unleashing the Power of Large Language Models in CI/CD Configuration Translation](https://arxiv.org/abs/2511.01316)
*Chong Wang,Chen Zhang,Jiajun Wu,Wunan Guo,Jianfeng Qu,Yewen Tian,Yang Liu*

Main category: cs.SE

TL;DR: 本文研究了基于大语言模型（LLM）的CI配置迁移，聚焦于从Travis CI到GitHub Actions的转换，量化了人工迁移的工作量，分析了LLM生成配置中的常见问题，并提出结合指南提示与迭代优化的策略，将构建成功率提升至75.5%。


<details>
  <summary>Details</summary>
Motivation: CI平台迁移普遍存在，但配置翻译复杂，需理解不同平台间的语义差异；随着LLM在软件工程中的兴起，探索其在CI配置翻译中的效果与改进方法具有实际意义。

Method: 基于811个真实迁移记录分析人工迁移工作量；评估四个LLM生成的翻译结果，对1,121个问题进行分类；测试三种增强策略，包括指南提示、上下文示例和迭代优化。

Result: 开发者平均阅读38行Travis配置、编写58行GitHub Actions配置，近半数迁移需多次提交；LLM翻译问题主要为逻辑不一致（38%）、平台差异（32%）、环境错误（25%）和语法错误（5%）；结合指南提示与迭代优化策略使构建成功率提升至75.5%，是基础GPT-4o提示的近三倍。

Conclusion: LLM在CI配置翻译中具有潜力，但需针对性优化；结合领域知识的提示工程与迭代机制可显著提升翻译质量与构建成功率。

Abstract: Continuous Integration (CI) is a cornerstone of modern collaborative software
development, and numerous CI platforms are available. Differences in
maintenance overhead, reliability, and integration depth with code-hosting
platforms make migration between CI platforms a common practice. A central step
in migration is translating CI configurations, which is challenging due to the
intrinsic complexity of CI configurations and the need to understand semantic
differences and relationships across CI platforms.
  With the advent of large language models (LLMs), recent advances in software
engineering highlight their potential for CI configuration translation. In this
paper, we present a study on LLM-based CI configuration translation, focusing
on the migration from Travis CI to GitHub Actions. First, using 811 migration
records, we quantify the effort involved and find that developers read an
average of 38 lines of Travis configuration and write 58 lines of GitHub
Actions configuration, with nearly half of the migrations requiring multiple
commits. We further analyze translations produced by each of the four LLMs and
identify 1,121 issues grouped into four categories: logic inconsistencies
(38%), platform discrepancies (32%), environment errors (25%), and syntax
errors (5%). Finally, we evaluate three enhancement strategies and show that
combining guideline-based prompting with iterative refinement achieves the best
performance, reaching a Build Success Rate of 75.5%-nearly a threefold
improvement over GPT-4o with a basic prompt.

</details>


### [30] [AI for Requirements Engineering: Industry adoption and Practitioner perspectives](https://arxiv.org/abs/2511.01324)
*Lekshmi Murali Rani,Richard Berntsson Svensson,Robert Feldt*

Main category: cs.SE

TL;DR: 一项针对55名软件从业者的调查显示，58.2%已在需求工程（RE）中使用AI，69.1%认为其影响积极；人机协作（HAIC）是主流模式（54.4%），完全自动化仅占5.4%，表明AI在RE中最有效的方式是作为人类专家的协作伙伴。


<details>
  <summary>Details</summary>
Motivation: 尽管需求工程（RE）在软件工程中至关重要，但关于AI在RE中应用的研究仍有限；作者旨在了解AI在RE各阶段的实际使用情况、从业者对其的看法以及面临的挑战与机遇。

Method: 通过问卷调查55名软件从业者，分析AI在RE四个阶段（获取、分析、规格说明、验证）中的应用，并考察四种决策模式（纯人工、AI验证、人机协作HAIC、全AI自动化）的使用情况。

Result: 58.2%的受访者已在RE中使用AI，69.1%持正面看法；HAIC占所有RE技术的54.4%，全自动化仅5.4%，被动AI验证使用率更低（4.4%–6.2%）。

Conclusion: AI在RE中最有效的角色是作为人类专家的协作伙伴而非替代者，未来需发展面向RE的人机协作框架，并加强负责任的AI治理。

Abstract: The integration of AI for Requirements Engineering (RE) presents significant
benefits but also poses real challenges.Although RE is fundamental to software
engineering, limited research has examined AI adoption in RE.We surveyed 55
software practitioners to map AI usage across four RE phases:Elicitation,
Analysis, Specification, and Validation, and four approaches for decision
making: human only decisions, AI validation, Human AI Collaboration (HAIC), and
full AI automation.Participants also shared their perceptions, challenges, and
opportunities when applying AI for RE tasks.Our data show that 58.2% of
respondents already use AI in RE, and 69.1% view its impact as positive or very
positive.HAIC dominates practice, accounting for 54.4% of all RE techniques,
while full AI automation remains minimal at 5.4%.Passive AI validation (4.4 to
6.2%) lags even further behind, indicating that practitioners value AI's active
support over passive oversight.These findings suggest that AI is most effective
when positioned as a collaborative partner rather than a replacement for human
expertise.It also highlights the need for RE specific HAIC frameworks along
with robust and responsible AI governance as AI adoption in RE grows.

</details>


### [31] [The Future of Generative AI in Software Engineering: A Vision from Industry and Academia in the European GENIUS Project](https://arxiv.org/abs/2511.01348)
*Robin Gröpler,Steffen Klepke,Jack Johns,Andreas Dreschinski,Klaus Schmid,Benedikt Dornauer,Eray Tüzün,Joost Noppen,Mohammad Reza Mousavi,Yongjian Tang,Johannes Viehmann,Selin Şirin Aslangül,Beum Seuk Lee,Adam Ziolkowski,Eric Zie*

Main category: cs.SE

TL;DR: 本文提出了一种关于生成式人工智能（GenAI）在软件工程中应用的前瞻性愿景，基于GENIUS项目中30多个欧洲产学研伙伴的经验与对话，系统探讨了GenAI在整个软件开发生命周期（SDLC）中的挑战、未来五年技术发展方向、软件从业者角色转变，以及GENIUS项目如何通过工具开发与工业验证推动这一转型。


<details>
  <summary>Details</summary>
Motivation: 尽管GenAI在编码任务中展现出巨大潜力，但其在整个软件开发生命周期中的应用仍不充分，且在可靠性、问责性、安全性和数据隐私等方面存在关键不确定性，亟需系统性研究与协同行动。

Method: 通过GENIUS联盟内部的跨行业对话、实践经验总结以及探索性文献综述，构建对GenAI驱动软件工程未来的共同愿景，并围绕四大核心要素展开分析。

Result: 论文系统梳理了GenAI在SDLC各阶段的当前挑战，提出了未来五年的技术与方法演进路径，预测了软件工程师角色与技能需求的变化，并明确了GENIUS项目在推动实用化工具和工业验证方面的贡献。

Conclusion: 通过将技术创新与业务价值对齐，该研究为学术界和工业界提供了可靠、可扩展且面向产业的GenAI软件工程解决方案的发展基础。

Abstract: Generative AI (GenAI) has recently emerged as a groundbreaking force in
Software Engineering, capable of generating code, suggesting fixes, and
supporting quality assurance. While its use in coding tasks shows considerable
promise, applying GenAI across the entire Software Development Life Cycle
(SDLC) has not yet been fully explored. Critical uncertainties in areas such as
reliability, accountability, security, and data privacy demand deeper
investigation and coordinated action. The GENIUS project, comprising over 30
European industrial and academic partners, aims to address these challenges by
advancing AI integration across all SDLC phases. It focuses on GenAI's
potential, the development of innovative tools, and emerging research
challenges, actively shaping the future of software engineering. This vision
paper presents a shared perspective on the future of GenAI-based software
engineering, grounded in cross-sector dialogue and experience within the GENIUS
consortium, supported by an exploratory literature review. The paper explores
four central elements: (1) a structured overview of current challenges in GenAI
adoption across the SDLC; (2) a forward-looking vision outlining key
technological and methodological advances expected over the next five years;
(3) anticipated shifts in the roles and required skill sets of software
professionals; and (4) the contribution of GENIUS in realizing this
transformation through practical tools and industrial validation. By aligning
technical innovation with business relevance, this paper aims to inform both
research agendas and industrial strategies, providing a foundation for
reliable, scalable, and industry-ready GenAI solutions for software engineering
teams.

</details>


### [32] [Characterizing Build Compromises Through Vulnerability Disclosure Analysis](https://arxiv.org/abs/2511.01395)
*Maimouna Tamah Diao,Moustapha Awwalou Diouf,Iyiola Emmanuel Olatunji,Abdoul Kader Kaboré,Gervais Mendy,Jacques Klein,Tegawendé F. Bissyandé*

Main category: cs.SE

TL;DR: 本文通过对621个CVE漏洞和168起软件供应链攻击事件的实证分析，构建并验证了一个针对软件构建过程的攻击向量分类体系，发现23.8%的供应链攻击利用构建阶段漏洞，其中依赖混淆和构建脚本注入最为常见。


<details>
  <summary>Details</summary>
Motivation: 软件构建过程是软件开发中的关键但脆弱环节，由于多组件系统复杂性、编译期间入侵检测困难以及构建非确定性掩盖恶意修改等问题，安全社区缺乏对构建特有攻击向量的系统性理解，阻碍了有效防御机制的设计。

Method: 通过大规模挖掘NVD数据库中的621个漏洞披露，构建一个基于构建流水线注入点（从源码篡改到编译器破坏）的攻击向量分类法，并利用168起已记录的软件供应链攻击事件进行验证。

Result: 在分析的供应链攻击中，有40起明确针对构建阶段，占比23.8%；依赖混淆和构建脚本注入是最主要的攻击向量。

Conclusion: 构建阶段是软件供应链攻击的重要目标，本文提出的实证分类法有助于提升对构建安全威胁的理解，并为针对性防御提供基础。

Abstract: The software build process transforms source code into deployable artifacts,
representing a critical yet vulnerable stage in software development. Build
infrastructure security poses unique challenges: the complexity of
multi-component systems (source code, dependencies, build tools), the
difficulty of detecting intrusions during compilation, and prevalent build
non-determinism that masks malicious modifications. Despite these risks, the
security community lacks a systematic understanding of build-specific attack
vectors, hindering effective defense design.
  This paper presents an empirically-derived taxonomy of attack vectors
targeting the build process, constructed through a large-scale CVE mining (of
621 vulnerability disclosures from the NVD database). We categorize attack
vectors by their injection points across the build pipeline, from source code
manipulation to compiler compromise. To validate our taxonomy, we analyzed 168
documented software supply chain attacks, identifying 40 incidents specifically
targeting build phases. Our analysis reveals that 23.8\% of supply chain
attacks exploit build vulnerabilities, with dependency confusion and build
script injection representing the most prevalent vectors.
  Dataset available at:
https://anonymous.4open.science/r/Taxonomizing-Build-Attacks-8BB0.

</details>


### [33] [LLM-Assisted Tool for Joint Generation of Formulas and Functions in Rule-Based Verification of Map Transformations](https://arxiv.org/abs/2511.01423)
*Ruidi He,Yu Zhang,Meng Zhang,Andreas Rausch*

Main category: cs.SE

TL;DR: 本文提出了一种基于大语言模型（LLM）的半自动化流程，用于生成高精地图变换中的逻辑公式与可执行谓词，以提升地图验证的可扩展性与正确性。


<details>
  <summary>Details</summary>
Motivation: 现有基于规则的地图验证框架依赖人工编写的公式和领域特定函数，难以扩展，且难以保证语义正确性。

Method: 构建一个LLM辅助的流水线，在一阶逻辑（FOL）计算框架下联合生成逻辑公式及其对应的可执行谓词，并集成到CommonRoad场景设计器的地图验证器中，新增对高程信息的支持；通过提示工程引导LLM生成符合语法的规则和谓词。

Result: 在合成的桥梁与坡道场景中进行原型评估，结果表明该方法在保持正确性的同时显著减少了人工工程量。

Conclusion: 该研究验证了采用人机协同、半自动化的LLM辅助方法在高精地图变换验证中的可行性与可扩展性。

Abstract: High-definition map transformations are essential in autonomous driving
systems, enabling interoperability across tools. Ensuring their semantic
correctness is challenging, since existing rule-based frameworks rely on
manually written formulas and domain-specific functions, limiting scalability.
  In this paper, We present an LLM-assisted pipeline that jointly generates
logical formulas and corresponding executable predicates within a computational
FOL framework, extending the map verifier in CommonRoad scenario designer with
elevation support. The pipeline leverages prompt-based LLM generation to
produce grammar-compliant rules and predicates that integrate directly into the
existing system.
  We implemented a prototype and evaluated it on synthetic bridge and slope
scenarios. The results indicate reduced manual engineering effort while
preserving correctness, demonstrating the feasibility of a scalable,
semi-automated human-in-the-loop approach to map-transformation verification.

</details>


### [34] [Hidden in Plain Sight: Where Developers Confess Self-Admitted Technical Debt](https://arxiv.org/abs/2511.01529)
*Murali Sridharan,Mikel Robredo,Leevi Rantala,Matteo Esposito,Valentina Lenarduzzi,Mika Mantyla*

Main category: cs.SE

TL;DR: 该研究通过分析超过22.5万条自认技术债务（SATD）评论，揭示了SATD主要出现在定义、条件判断和异常处理附近的内联代码中，表明其是开发者在代码变更过程中有意识的风险提示，而非单纯的疏忽。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注SATD的检测与优先级排序，却较少关注受SATD影响的具体源代码。本文旨在将SATD注释与其周围的代码结构关联起来，以更深入理解其出现位置和上下文。

Method: 利用包含9000多个Java开源项目注释的PENTACET数据集，定量分析SATD最常出现的位置及其影响的代码结构或语句类型。

Result: 研究将225,000多条SATD评论与周围代码关联，发现SATD主要集中在定义、条件语句和异常处理附近的内联代码中。

Conclusion: SATD是开发者在面临不确定性与权衡时有意留下的信号，反映了对技术债务的主动意识，而非代码疏忽。

Abstract: Context. Detecting Self-Admitted Technical Debt (SATD) is crucial for
proactive software maintenance. Previous research has primarily targeted
detecting and prioritizing SATD, with little focus on the source code afflicted
with SATD. Our goal in this work is to connect the SATD comments with source
code constructs that surround them.
  Method. We leverage the extensive SATD dataset PENTACET, containing code
comments from over 9000 Java Open Source Software (OSS) repositories. We
quantitatively infer where SATD most commonly occurs and which code
constructs/statements it most frequently affects.
  Results and Conclusions. Our large-scale study links over 225,000 SATD
comments to their surrounding code, showing that SATD mainly arises in inline
code near definitions, conditionals, and exception handling, where developers
face uncertainty and trade-offs, revealing it as an intentional signal of
awareness during change rather than mere neglect.

</details>


### [35] [Towards LLM-Powered Task-Aware Retrieval of Scientific Workflows for Galaxy](https://arxiv.org/abs/2511.01757)
*Shamse Tasnim Cynthia,Banani Roy*

Main category: cs.SE

TL;DR: 本文提出一种结合稠密向量检索与大语言模型重排序的两阶段任务感知检索框架，显著提升Galaxy平台中科学工作流的语义搜索效果，尤其在处理长或模糊查询时表现优异。


<details>
  <summary>Details</summary>
Motivation: Galaxy等科学工作流管理系统依赖关键词检索，缺乏语义理解能力，难以在无精确术语匹配时返回相关工作流，限制了用户（尤其是新手和跨领域研究者）的使用体验。

Method: 首先利用先进嵌入模型进行稠密向量检索获取候选工作流，再通过指令微调的大语言模型（如GPT-4o、Mistral-7B）根据语义任务对齐程度进行重排序；同时构建了基于BERTopic标注的Galaxy工作流基准数据集，并用LLM合成任务导向查询用于评估。

Result: 实验表明，该方法在top-k准确率和相关性方面显著优于传统词汇、稠密检索方法，尤其在长或描述不充分的查询上效果突出；并在Galaxy中集成原型工具验证了可行性。

Conclusion: 该工作通过引入LLM增强的检索机制，提升了科学工作流系统的可用性与可访问性，为生物信息学等领域用户提供了更智能、更友好的工作流发现体验。

Abstract: Scientific Workflow Management Systems (SWfMSs) such as Galaxy have become
essential infrastructure in bioinformatics, supporting the design, execution,
and sharing of complex multi-step analyses. Despite hosting hundreds of
reusable workflows across domains, Galaxy's current keyword-based retrieval
system offers limited support for semantic query interpretation and often fails
to surface relevant workflows when exact term matches are absent. To address
this gap, we propose a task-aware, two-stage retrieval framework that
integrates dense vector search with large language model (LLM)-based reranking.
Our system first retrieves candidate workflows using state-of-the-art embedding
models and then reranks them using instruction-tuned generative LLMs (GPT-4o,
Mistral-7B) based on semantic task alignment. To support robust evaluation, we
construct a benchmark dataset of Galaxy workflows annotated with semantic
topics via BERTopic and synthesize realistic task-oriented queries using LLMs.
We conduct a comprehensive comparison of lexical, dense, and reranking models
using standard IR metrics, presenting the first systematic evaluation of
retrieval performance in the Galaxy ecosystem. Results show that our approach
significantly improves top-k accuracy and relevance, particularly for long or
under-specified queries. We further integrate our system as a prototype tool
within Galaxy, providing a proof-of-concept for LLM-enhanced workflow search.
This work advances the usability and accessibility of scientific workflows,
especially for novice users and interdisciplinary researchers.

</details>


### [36] [Context-Guided Decompilation: A Step Towards Re-executability](https://arxiv.org/abs/2511.01763)
*Xiaohan Wang,Yuxin Hu,Kevin Leach*

Main category: cs.SE

TL;DR: 本文提出了一种名为ICL4Decomp的混合反编译框架，利用上下文学习（ICL）引导大语言模型生成可重新编译执行的源代码，在多个数据集和编译器设置下相比现有方法在可重执行性上提升了约40%。


<details>
  <summary>Details</summary>
Motivation: 现有反编译技术（包括基于大语言模型的方法）难以生成可成功重新编译和执行的源代码，尤其在面对经过优化的二进制文件时，主要受限于编译器优化导致的语义信息丢失，而大语言模型缺乏上下文指导难以恢复这些信息。

Method: 提出ICL4Decomp框架，结合上下文学习（ICL）来引导大语言模型，使其在反编译过程中生成具备可重执行性的源代码。

Result: 在多种数据集、优化级别和编译器环境下评估，ICL4Decomp相比当前最先进的反编译方法在可重执行性方面提升了约40%，同时保持了良好的鲁棒性。

Conclusion: 通过引入上下文学习机制，ICL4Decomp有效提升了大语言模型在反编译任务中生成可执行代码的能力，显著增强了反编译结果的实用性和可靠性。

Abstract: Binary decompilation plays an important role in software security analysis,
reverse engineering, and malware understanding when source code is unavailable.
However, existing decompilation techniques often fail to produce source code
that can be successfully recompiled and re-executed, particularly for optimized
binaries. Recent advances in large language models (LLMs) have enabled neural
approaches to decompilation, but the generated code is typically only
semantically plausible rather than truly executable, limiting their practical
reliability. These shortcomings arise from compiler optimizations and the loss
of semantic cues in compiled code, which LLMs struggle to recover without
contextual guidance. To address this challenge, we propose ICL4Decomp, a hybrid
decompilation framework that leverages in-context learning (ICL) to guide LLMs
toward generating re-executable source code. We evaluate our method across
multiple datasets, optimization levels, and compilers, demonstrating around
40\% improvement in re-executability over state-of-the-art decompilation
methods while maintaining robustness.

</details>


### [37] [SmartMLOps Studio: Design of an LLM-Integrated IDE with Automated MLOps Pipelines for Model Development and Monitoring](https://arxiv.org/abs/2511.01850)
*Jiawei Jin,Yingxin Su,Xiaotong Zhu*

Main category: cs.SE

TL;DR: 本文提出了一种集成大语言模型（LLM）的智能IDE——SmartMLOps Studio，将代码开发与自动化MLOps流程融合，显著提升模型开发效率与可复现性。


<details>
  <summary>Details</summary>
Motivation: 传统IDE仅支持代码编写，缺乏对机器学习全生命周期的智能支持；现有MLOps平台又与编码流程脱节。为弥合这一差距，作者设计了一个集成LLM与自动化MLOps的统一开发环境。

Method: 系统在IDE中嵌入LLM助手，提供代码生成、调试建议和自动流水线配置；后端实现自动数据验证、特征存储、漂移检测、重训练触发及CI/CD部署编排，并在UCI Adult和M5数据集上进行评估。

Result: 实验表明，SmartMLOps Studio相比传统工作流，流水线配置时间减少61%，实验可复现性提升45%，漂移检测准确率提高14%。

Conclusion: 该研究将IDE从静态编码工具转变为动态、全生命周期感知的智能平台，为AI工程提供了一种新范式。

Abstract: The rapid expansion of artificial intelligence and machine learning (ML)
applications has intensified the demand for integrated environments that unify
model development, deployment, and monitoring. Traditional Integrated
Development Environments (IDEs) focus primarily on code authoring, lacking
intelligent support for the full ML lifecycle, while existing MLOps platforms
remain detached from the coding workflow. To address this gap, this study
proposes the design of an LLM-Integrated IDE with automated MLOps pipelines
that enables continuous model development and monitoring within a single
environment. The proposed system embeds a Large Language Model (LLM) assistant
capable of code generation, debugging recommendation, and automatic pipeline
configuration. The backend incorporates automated data validation, feature
storage, drift detection, retraining triggers, and CI/CD deployment
orchestration. This framework was implemented in a prototype named SmartMLOps
Studio and evaluated using classification and forecasting tasks on the UCI
Adult and M5 datasets. Experimental results demonstrate that SmartMLOps Studio
reduces pipeline configuration time by 61%, improves experiment reproducibility
by 45%, and increases drift detection accuracy by 14% compared to traditional
workflows. By bridging intelligent code assistance and automated operational
pipelines, this research establishes a novel paradigm for AI engineering -
transforming the IDE from a static coding tool into a dynamic, lifecycle-aware
intelligent platform for scalable and efficient model development.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [38] [Mist-Assisted Federated Learning for Intrusion Detection in Heterogeneous IoT Networks](https://arxiv.org/abs/2511.00271)
*Saadat Izadi,Shakib Komasi,Ali Salimi,Alireza Rezaei,Mahmood Ahmadi*

Main category: cs.NI

TL;DR: 本文提出了一种基于Mist辅助的分层联邦学习框架，用于在异构、大规模物联网环境中实现高效且隐私保护的入侵检测。


<details>
  <summary>Details</summary>
Motivation: 物联网设备的快速增长带来了新的安全挑战，传统集中式入侵检测方法难以应对数据异构性、非独立同分布（non-IID）以及资源受限等问题，而现有联邦学习框架在这些条件下表现不佳。

Method: 提出一个四层架构：Mist层将原始数据抽象为统一特征空间并进行轻量级异常检测；Edge层基于效用选择客户端；Fog层使用FedProx进行区域聚合以稳定训练；Cloud层整合并分发全局模型。

Result: 在TON-IoT数据集上的实验表明，该框架在异构和大规模设置下实现了98-99%的准确率、PR-AUC>0.97，并具备稳定的收敛性、高效性和隐私保护能力。

Conclusion: 所提出的Mist辅助分层联邦学习框架有效解决了物联网环境中入侵检测面临的异构性、隐私和效率挑战，具有良好的实用性和可扩展性。

Abstract: The rapid growth of the Internet of Things (IoT) offers new opportunities but
also expands the attack surface of distributed, resource-limited devices.
Intrusion detection in such environments is difficult due to data heterogeneity
from diverse sensing modalities and the non-IID distribution of samples across
clients. Federated Learning (FL) provides a privacy-preserving alternative to
centralized training, yet conventional frameworks struggle under these
conditions. To address this, we propose a Mist-assisted hierarchical framework
for IoT intrusion detection. The architecture spans four layers: (i) Mist,
where raw data are abstracted into a unified feature space and lightweight
models detect anomalies; (ii) Edge, which applies utility-based client
selection; (iii) Fog, where multiple regional aggregators use FedProx to
stabilize training; and (iv) Cloud, which consolidates and disseminates global
models. Evaluations on the TON-IoT dataset show the framework achieves 98-99%
accuracy, PR-AUC> 0.97, and stable convergence under heterogeneous and
large-scale settings, while maintaining efficiency and preserving privacy.

</details>


### [39] [Impact of Antenna Arrays Misalignment on the Near Field Distance in Terahertz Communications](https://arxiv.org/abs/2511.00502)
*Peng Zhang,Vitaly Petrov,Emil Björnson*

Main category: cs.NI

TL;DR: 本文研究了太赫兹通信中由于收发器空间错位对近场边界计算的影响，推导了在任意错位条件下均匀线阵和面阵配置下的近场边界的精确解析表达式与简化近似，并通过仿真验证了理论模型。


<details>
  <summary>Details</summary>
Motivation: 现有太赫兹通信系统中近场边界的计算通常假设收发器理想对齐，忽略了实际因移动性或机械误差导致的空间错位问题，这在极短波长下尤为关键。

Method: 推导了ULA–ULA和UPA–UPA配置在任意错位偏移下的近场边界的精确解析表达式和简化近似公式，并通过数值仿真验证理论模型。

Result: 仿真结果表明，空间错位显著改变了近场区域的形状和范围，所提出的模型能准确描述这一现象。

Conclusion: 该研究为太赫兹系统在实际部署中的近场建模与优化提供了重要理论依据和设计指导。

Abstract: The extremely short wavelength of terahertz (THz) communications leads to an
extended radiative near-field region, in which some canonical far-field
assumptions fail. Existing near-field boundary formulations (Fraunhofer
distance) for uniform linear/planar array (ULA/UPA) configurations assume ideal
alignment between transceivers, overlooking practical misalignments caused by
mobility or mechanical imperfections. This paper addresses this critical gap by
analyzing the impact of spatial misalignment on near-field distance
calculations in THz systems. We derive exact analytical expressions and
simplified approximations for the near-field boundary in both ULA--ULA and
UPA--UPA configurations under arbitrary misalignment offsets. Through numerical
simulations, we validate our theoretical models and quantify how misalignment
reshapes the near-field region. These findings provide essential guidelines for
optimizing THz system deployment in realistic scenarios.

</details>


### [40] [3D Gaussian Radiation Field Modeling for Integrated RIS-FAS Systems: Analysis and Optimization](https://arxiv.org/abs/2511.01373)
*Kaining Wang,Bo Yang,Yusheng Lei,Zhiwen Yu,Xuelin Cao,Liang Wang,Bin Guo,George C. Alexandropoulos,Mérouane Debbah,Zhu Han*

Main category: cs.NI

TL;DR: 本文提出了一种基于三维高斯辐射场建模的场信息驱动优化方法，用于在快衰落信道下对FAS-RIS集成系统进行实时联合优化，显著提升了频谱预测精度、收敛速度和最小可达速率。


<details>
  <summary>Details</summary>
Motivation: 在快衰落信道条件下，传统方法难以实时高效地联合优化流体天线系统（FAS）的天线位置与可重构智能表面（RIS）的相位配置，因其依赖复杂的迭代计算。

Method: 提出一种基于三维高斯辐射场建模的场信息驱动优化方法，将障碍物视为虚拟发射机，分别学习幅度和相位变化以快速生成高精度信道信息，并在此基础上设计交替优化方案联合优化FAS位置与RIS相位。

Result: 仿真结果表明，所提方法在频谱预测精度、收敛速度和最小可达速率方面显著优于现有方法。

Conclusion: 该方法有效解决了FAS-RIS系统在快衰落场景下的实时优化难题，具有良好的实用性和性能优势。

Abstract: The integration of reconfigurable intelligent surfaces (RIS) and fluid
antenna systems (FAS) has attracted considerable attention due to its
tremendous potential in enhancing wireless communication performance. However,
under fast-fading channel conditions, rapidly and effectively performing joint
optimization of the antenna positions in an FAS system and the RIS phase
configuration remains a critical challenge. Traditional optimization methods
typically rely on complex iterative computations, thus making it challenging to
obtain optimal solutions in real time within dynamic channel environments. To
address this issue, this paper introduces a field information-driven
optimization method based on three-dimensional Gaussian radiation-field
modeling for real-time optimization of integrated FAS-RIS systems. In the
proposed approach, obstacles are treated as virtual transmitters and, by
separately learning the amplitude and phase variations, the model can quickly
generate high-precision channel information based on the transmitter's
position. This design eliminates the need for extensive pilot overhead and
cumbersome computations. On this framework, an alternating optimization scheme
is presented to jointly optimize the FAS position and the RIS phase
configuration. Simulation results demonstrate that the proposed method
significantly outperforms existing approaches in terms of spectrum prediction
accuracy, convergence speed, and minimum achievable rate, validating its
effectiveness and practicality in fast-fading scenarios.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [41] [On the Fundamental Limitations of Decentralized Learnable Reward Shaping in Cooperative Multi-Agent Reinforcement Learning](https://arxiv.org/abs/2511.00034)
*Aditya Akella*

Main category: cs.MA

TL;DR: 本文研究了在合作多智能体环境中去中心化可学习奖励塑形的有效性，提出DMARL-RSA方法，发现尽管其能提高地标覆盖率，但整体性能远低于中心化训练方法（如MAPPO），揭示了去中心化协调的根本局限。


<details>
  <summary>Details</summary>
Motivation: 探索去中心化可学习奖励塑形在合作多智能体强化学习中的有效性，填补该领域研究空白。

Method: 提出DMARL-RSA，一种完全去中心化的系统，每个智能体独立学习奖励塑形，并在simple_spread_v3环境中进行评估。

Result: DMARL-RSA平均奖励为-24.20，显著低于中心化MAPPO（1.92）；虽地标覆盖率更高，但整体性能更差，揭示局部优化与全局性能之间的协调悖论。

Conclusion: 去中心化奖励学习存在根本性限制，有效多智能体协作仍需依赖中心化协调机制。

Abstract: Recent advances in learnable reward shaping have shown promise in
single-agent reinforcement learning by automatically discovering effective
feedback signals. However, the effectiveness of decentralized learnable reward
shaping in cooperative multi-agent settings remains poorly understood. We
propose DMARL-RSA, a fully decentralized system where each agent learns
individual reward shaping, and evaluate it on cooperative navigation tasks in
the simple_spread_v3 environment. Despite sophisticated reward learning,
DMARL-RSA achieves only -24.20 +/- 0.09 average reward, compared to MAPPO with
centralized training at 1.92 +/- 0.87 -- a 26.12-point gap. DMARL-RSA performs
similarly to simple independent learning (IPPO: -23.19 +/- 0.96), indicating
that advanced reward shaping cannot overcome fundamental decentralized
coordination limitations. Interestingly, decentralized methods achieve higher
landmark coverage (0.888 +/- 0.029 for DMARL-RSA, 0.960 +/- 0.045 for IPPO out
of 3 total) but worse overall performance than centralized MAPPO (0.273 +/-
0.008 landmark coverage) -- revealing a coordination paradox between local
optimization and global performance. Analysis identifies three critical
barriers: (1) non-stationarity from concurrent policy updates, (2) exponential
credit assignment complexity, and (3) misalignment between individual reward
optimization and global objectives. These results establish empirical limits
for decentralized reward learning and underscore the necessity of centralized
coordination for effective multi-agent cooperation.

</details>


### [42] [Urban-MAS: Human-Centered Urban Prediction with LLM-Based Multi-Agent System](https://arxiv.org/abs/2511.00096)
*Shangyu Lou*

Main category: cs.MA

TL;DR: 本文提出了Urban-MAS，一种基于大语言模型（LLM）的多智能体系统框架，用于在零样本设置下进行以人为中心的城市预测。该框架包含三种智能体：预测因子引导智能体、可靠城市信息提取智能体和多源城市信息推理智能体。实验表明，Urban-MAS在东京、米兰和西雅图的跑步量预测和城市感知任务中显著优于单LLM基线。


<details>
  <summary>Details</summary>
Motivation: 大语言模型虽能整合多模态输入以处理复杂城市系统中的异构数据，但在特定领域任务上表现不佳。因此，需要一种更有效的框架来提升LLM在以人为中心的城市预测任务中的性能。

Method: 提出Urban-MAS框架，包含三种智能体：1）预测因子引导智能体，用于识别关键预测因子并指导知识提取；2）可靠城市信息提取智能体，通过多输出比对和一致性验证提升鲁棒性；3）多源城市信息推理智能体，融合多维信息进行预测。

Result: 在东京、米兰和西雅图的跑步量预测和城市感知任务中，Urban-MAS相比单LLM基线显著降低了预测误差；消融实验表明预测因子引导智能体对性能提升最关键。

Conclusion: Urban-MAS是一种可扩展的、适用于以人为中心城市AI预测的有效范式，尤其在零样本设置下展现出优越性能。

Abstract: Urban Artificial Intelligence (Urban AI) has advanced human-centered urban
tasks such as perception prediction and human dynamics. Large Language Models
(LLMs) can integrate multimodal inputs to address heterogeneous data in complex
urban systems but often underperform on domain-specific tasks. Urban-MAS, an
LLM-based Multi-Agent System (MAS) framework, is introduced for human-centered
urban prediction under zero-shot settings. It includes three agent types:
Predictive Factor Guidance Agents, which prioritize key predictive factors to
guide knowledge extraction and enhance the effectiveness of compressed urban
knowledge in LLMs; Reliable UrbanInfo Extraction Agents, which improve
robustness by comparing multiple outputs, validating consistency, and
re-extracting when conflicts occur; and Multi-UrbanInfo Inference Agents, which
integrate extracted multi-source information across dimensions for prediction.
Experiments on running-amount prediction and urban perception across Tokyo,
Milan, and Seattle demonstrate that Urban-MAS substantially reduces errors
compared to single-LLM baselines. Ablation studies indicate that Predictive
Factor Guidance Agents are most critical for enhancing predictive performance,
positioning Urban-MAS as a scalable paradigm for human-centered urban AI
prediction. Code is available on the project
website:https://github.com/THETUREHOOHA/UrbanMAS

</details>


### [43] [AgentGit: A Version Control Framework for Reliable and Scalable LLM-Powered Multi-Agent Systems](https://arxiv.org/abs/2511.00628)
*Yang Li,Siqi Ping,Xiyu Chen,Xiaojian Qi,Zigan Wang,Ye Luo,Xiaowei Zhang*

Main category: cs.MA

TL;DR: AgentGit 是一个基于 LangGraph 的多智能体系统框架，引入类似 Git 的回滚与分支机制，显著提升系统在复杂任务中的可靠性与可扩展性。


<details>
  <summary>Details</summary>
Motivation: 当前多智能体系统（MAS）在处理复杂任务时面临可靠性和可扩展性不足的问题，亟需一种支持状态管理与错误恢复的基础设施。

Method: 提出 AgentGit 框架，通过在 LangGraph 上构建基础设施层，实现状态提交、回滚和分支功能，支持多轨迹并行探索与高效比较。

Result: 在论文摘要检索与分析任务中，AgentGit 相较 LangGraph、AutoGen 和 Agno 三大基线，显著减少冗余计算、降低运行时间和 token 消耗，并支持多分支并行探索。

Conclusion: AgentGit 为多智能体系统提供了一种实用的鲁棒设计路径，支持错误恢复、安全探索、迭代调试和 A/B 测试，有效提升 MAS 的可靠性与可扩展性。

Abstract: With the rapid progress of large language models (LLMs), LLM-powered
multi-agent systems (MAS) are drawing increasing interest across academia and
industry. However, many current MAS frameworks struggle with reliability and
scalability, especially on complex tasks. We present AgentGit, a framework that
brings Git-like rollback and branching to MAS workflows. Built as an
infrastructure layer on top of LangGraph, AgentGit supports state commit,
revert, and branching, allowing agents to traverse, compare, and explore
multiple trajectories efficiently. To evaluate AgentGit, we designed an
experiment that optimizes target agents by selecting better prompts. We ran a
multi-step A/B test against three baselines -- LangGraph, AutoGen, and Agno --
on a real-world task: retrieving and analyzing paper abstracts. Results show
that AgentGit significantly reduces redundant computation, lowers runtime and
token usage, and supports parallel exploration across multiple branches,
enhancing both reliability and scalability in MAS development. This work offers
a practical path to more robust MAS design and enables error recovery, safe
exploration, iterative debugging, and A/B testing in collaborative AI systems.

</details>


### [44] [Predictive Auxiliary Learning for Belief-based Multi-Agent Systems](https://arxiv.org/abs/2511.01078)
*Qinwei Huang,Stefan Wang,Simon Khan,Garrett Katz,Qinru Qiu*

Main category: cs.MA

TL;DR: 本文提出了一种名为BEPAL的多智能体强化学习框架，通过引入辅助预测任务（如预测其他智能体的奖励或运动方向）来增强策略学习的效率与稳定性，在多个环境中实现了约16%的性能提升。


<details>
  <summary>Details</summary>
Motivation: 现有大多数多智能体系统仅依赖奖励信号进行策略训练，忽略了观测和通信中蕴含的丰富信息；作者旨在通过引入辅助预测任务来提升学习效率和训练稳定性。

Method: 提出Belief-based Predictive Auxiliary Learning（BEPAL）框架，采用集中训练、分散执行范式，每个智能体同时学习一个信念模型（用于预测不可观测状态）和策略模型，利用辅助任务丰富隐状态表示。

Result: 在捕食者-猎物环境和Google Research Football中，BEPAL相比基线方法平均提升约16%的性能指标，并展现出更稳定的收敛性。

Conclusion: 引入基于信念的辅助预测任务能有效提升多智能体强化学习在部分可观测环境中的性能与训练稳定性，验证了辅助学习在MARL中的价值。

Abstract: The performance of multi-agent reinforcement learning (MARL) in partially
observable environments depends on effectively aggregating information from
observations, communications, and reward signals. While most existing
multi-agent systems primarily rely on rewards as the only feedback for policy
training, our research shows that introducing auxiliary predictive tasks can
significantly enhance learning efficiency and stability. We propose
Belief-based Predictive Auxiliary Learning (BEPAL), a framework that
incorporates auxiliary training objectives to support policy optimization.
BEPAL follows the centralized training with decentralized execution paradigm.
Each agent learns a belief model that predicts unobservable state information,
such as other agents' rewards or motion directions, alongside its policy model.
By enriching hidden state representations with information that does not
directly contribute to immediate reward maximization, this auxiliary learning
process stabilizes MARL training and improves overall performance. We evaluate
BEPAL in the predator-prey environment and Google Research Football, where it
achieves an average improvement of about 16 percent in performance metrics and
demonstrates more stable convergence compared to baseline methods.

</details>


### [45] [From Pixels to Cooperation Multi Agent Reinforcement Learning based on Multimodal World Models](https://arxiv.org/abs/2511.01310)
*Sureyya Akin,Kavita Srivastava,Prateek B. Kapoor,Pradeep G. Sethi,Sunita Q. Patel,Rahu Srivastava*

Main category: cs.MA

TL;DR: 本文提出了一种基于共享生成式多模态世界模型（MWM）的多智能体强化学习框架，通过融合多智能体的多模态观测学习环境动态的压缩潜在表示，并在该模型的潜在空间中训练协作策略，显著提升了样本效率、鲁棒性及在感知不对称环境中的性能。


<details>
  <summary>Details</summary>
Motivation: 直接从高维多模态感官输入（如像素和音频）中学习协作多智能体策略存在样本效率低下的问题，现有无模型多智能体强化学习（MARL）方法难以同时应对表征学习、部分可观测性和信用分配三大挑战。

Method: 提出一种基于共享生成式多模态世界模型（MWM）的新框架，利用可扩展的注意力机制融合所有智能体的分布式多模态观测，学习环境动态的压缩潜在表示；随后在该MWM的潜在空间中作为“想象”模拟器训练MARL策略（如MAPPO），实现表征学习与策略学习的解耦。

Result: 在新构建的基于3D物理模拟器的多模态多智能体基准测试中，MWM-MARL框架相比当前最先进的无模型MARL基线实现了数量级更高的样本效率；同时验证了多模态融合对感知不对称任务成功至关重要，并展现出对传感器失效的更强鲁棒性。

Conclusion: 所提出的MWM-MARL框架有效解决了高维多模态输入下多智能体协作策略学习的样本效率与鲁棒性问题，为现实世界部署提供了可行路径。

Abstract: Learning cooperative multi-agent policies directly from high-dimensional,
multimodal sensory inputs like pixels and audio (from pixels) is notoriously
sample-inefficient. Model-free Multi-Agent Reinforcement Learning (MARL)
algorithms struggle with the joint challenge of representation learning,
partial observability, and credit assignment. To address this, we propose a
novel framework based on a shared, generative Multimodal World Model (MWM). Our
MWM is trained to learn a compressed latent representation of the environment's
dynamics by fusing distributed, multimodal observations from all agents using a
scalable attention-based mechanism. Subsequently, we leverage this learned MWM
as a fast, "imagined" simulator to train cooperative MARL policies (e.g.,
MAPPO) entirely within its latent space, decoupling representation learning
from policy learning. We introduce a new set of challenging multimodal,
multi-agent benchmarks built on a 3D physics simulator. Our experiments
demonstrate that our MWM-MARL framework achieves orders-of-magnitude greater
sample efficiency compared to state-of-the-art model-free MARL baselines. We
further show that our proposed multimodal fusion is essential for task success
in environments with sensory asymmetry and that our architecture provides
superior robustness to sensor-dropout, a critical feature for real-world
deployment.

</details>


### [46] [An Explanation-oriented Inquiry Dialogue Game for Expert Collaborative Recommendations](https://arxiv.org/abs/2511.01489)
*Qurat-ul-ain Shaheen,Katarzyna Budzynska,Carles Sierra*

Main category: cs.MA

TL;DR: 本文提出了一种面向医疗专家协作对话的需求分析，并基于此设计了一个探究式对话博弈模型，以在多智能体系统中引入可解释性。该模型通过结合基于解释的言语行为，在协作推荐过程中生成丰富的推理轨迹，并通过原型系统和用户研究验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 医疗专家在协作决策中需要透明、可解释的推理过程，现有系统缺乏支持此类协作对话的有效机制。

Method: 基于需求分析构建一个探究式对话博弈模型，整合解释性言语行为，开发原型Web应用，并通过形成性用户研究进行评估。

Result: 用户研究证实该对话博弈满足医疗专家协作需求，并揭示了基于对话的沟通工具在医疗实践中的实际价值。

Conclusion: 将解释性融入多智能体协作对话系统是可行且有价值的，尤其在医疗领域可有效支持专家间的协同决策与推理透明化。

Abstract: This work presents a requirement analysis for collaborative dialogues among
medical experts and an inquiry dialogue game based on this analysis for
incorporating explainability into multiagent system design. The game allows
experts with different knowledge bases to collaboratively make recommendations
while generating rich traces of the reasoning process through combining
explanation-based illocutionary forces in an inquiry dialogue. The dialogue
game was implemented as a prototype web-application and evaluated against the
specification through a formative user study. The user study confirms that the
dialogue game meets the needs for collaboration among medical experts. It also
provides insights on the real-life value of dialogue-based communication tools
for the medical community.

</details>


### [47] [Learning what to say and how precisely: Efficient Communication via Differentiable Discrete Communication Learning](https://arxiv.org/abs/2511.01554)
*Aditya Kapoor,Yash Bhisikar,Benjamin Freed,Jan Peters,Mingfei Sun*

Main category: cs.MA

TL;DR: 本文提出一种扩展的可微分离散通信学习（DDCL）方法，支持无界信号，使多智能体强化学习中的通信消息能在比特级别动态调整精度，在大幅降低带宽的同时保持甚至提升任务性能，并验证了简单架构结合该通信机制可媲美复杂专用设计。


<details>
  <summary>Details</summary>
Motivation: 现有MARL通信方法多依赖复杂门控机制，仅决定是否通信，而无法优化通信的精度；在比特级别优化消息精度因离散化阻碍梯度传播而极具挑战。

Method: 扩展DDCL框架，使其支持无界信号，从而作为通用即插即用通信层嵌入任意MARL架构，实现端到端优化离散消息的精度。

Result: 1）在受控环境中定性展示智能体如何根据任务信息需求动态调节消息精度；2）在四种先进MARL算法中集成该方法，带宽降低一个数量级以上且性能相当或更优；3）验证“苦涩教训”：结合DDCL的简单Transformer策略可媲美复杂专用通信架构。

Conclusion: 通过将DDCL扩展为支持无界信号的通用通信层，可在显著节省通信带宽的同时维持高性能，表明复杂通信设计未必必要，简单架构结合高效通信机制即可取得优异效果。

Abstract: Effective communication in multi-agent reinforcement learning (MARL) is
critical for success but constrained by bandwidth, yet past approaches have
been limited to complex gating mechanisms that only decide \textit{whether} to
communicate, not \textit{how precisely}. Learning to optimize message precision
at the bit-level is fundamentally harder, as the required discretization step
breaks gradient flow. We address this by generalizing Differentiable Discrete
Communication Learning (DDCL), a framework for end-to-end optimization of
discrete messages. Our primary contribution is an extension of DDCL to support
unbounded signals, transforming it into a universal, plug-and-play layer for
any MARL architecture. We verify our approach with three key results. First,
through a qualitative analysis in a controlled environment, we demonstrate
\textit{how} agents learn to dynamically modulate message precision according
to the informational needs of the task. Second, we integrate our variant of
DDCL into four state-of-the-art MARL algorithms, showing it reduces bandwidth
by over an order of magnitude while matching or exceeding task performance.
Finally, we provide direct evidence for the \enquote{Bitter Lesson} in MARL
communication: a simple Transformer-based policy leveraging DDCL matches the
performance of complex, specialized architectures, questioning the necessity of
bespoke communication designs.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [48] [H-FA: A Hybrid Floating-Point and Logarithmic Approach to Hardware Accelerated FlashAttention](https://arxiv.org/abs/2511.00295)
*Kosmas Alexandridis,Giorgos Dimitrakopoulos*

Main category: cs.AR

TL;DR: 该论文提出了一种名为H-FA的硬件优化方法，通过结合浮点与对数域定点表示来实现FlashAttention，从而在不牺牲性能的前提下显著降低硬件面积和功耗。


<details>
  <summary>Details</summary>
Motivation: 长序列注意力计算存在计算瓶颈，尽管FlashAttention通过融合softmax和矩阵运算缓解了这一问题，但其纯浮点硬件实现代价较高，因此需要更高效的硬件实现方式。

Method: 将FlashAttention中的部分计算转换到对数域，使用浮点计算查询和键的注意力得分，而softmax归一化与值矩阵相乘则通过对数域中的定点加减法高效完成，并省略显式的指数函数计算。

Result: 在28nm工艺下，H-FA相比纯浮点实现的FlashAttention硬件架构平均减少26.5%面积和23.4%功耗，且未影响性能。

Conclusion: H-FA通过混合浮点与对数域定点表示，在保持FlashAttention性能的同时显著提升了硬件效率，为注意力机制的硬件加速提供了有效方案。

Abstract: Transformers have significantly advanced AI and machine learning through
their powerful attention mechanism. However, computing attention on long
sequences can become a computational bottleneck. FlashAttention mitigates this
by fusing the softmax and matrix operations into a tiled computation pattern
that decouples performance from sequence length. Though designed for GPUs, its
simplicity also makes it well suited for direct hardware acceleration. To
improve hardware implementation, we compute FlashAttention using a mixture of
floating-point and fixed-point logarithm domain representations. Floating-point
is used to compute attention scores from query and key matrices, while
logarithmic computation simplifies the fused computation of softmax
normalization and the multiplication with the value matrix. This
transformation, called H-FA, replaces vector-wide floating-point multiplication
and division operations by additions and subtractions implemented efficiently
with fixed-point arithmetic in the logarithm domain. Exponential function
evaluations are effectively omitted and fused with the rest operations, and the
final result is directly returned to floating-point arithmetic without any
additional hardware overhead. Hardware implementation results at 28nm
demonstrate that H-FA achieves a 26.5% reduction in area and a 23.4% reduction
in power, on average, compared to FlashAttention parallel hardware
architectures built solely with floating-point datapaths, without hindering
performance.

</details>


### [49] [Scalable Processing-Near-Memory for 1M-Token LLM Inference: CXL-Enabled KV-Cache Management Beyond GPU Limits](https://arxiv.org/abs/2511.00321)
*Dowon Kim,MinJae Lee,Janghyeon Kim,HyuckSung Kwon,Hyeonggyu Jeong,Sang-Soo Park,Minyong Yoon,Si-Dong Roh,Yongsuk Kwon,Jinin So,Jungwook Choi*

Main category: cs.AR

TL;DR: 本文提出一种基于CXL的近内存计算（PNM）系统，用于高效管理百万级上下文长度大语言模型推理中的KV缓存，显著提升吞吐量、能效和成本效益。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型上下文窗口扩展至百万级token，KV缓存带来严重的内存与计算瓶颈；现有基于CXL的非驱逐框架虽可将KV缓存卸载至外部内存，但在上下文增长时仍需频繁将非驻留KV token调回GPU内存，造成高昂的数据传输开销。

Method: 设计一种CXL支持的PNM架构，将token页选择任务卸载至CXL内存内的PNM加速器，避免昂贵的数据召回；引入混合并行策略与稳态token选择机制，提升计算效率与可扩展性。

Result: 在高达405B参数和1M-token上下文的LLM上实现稳定性能提升：PNM-KV方案和PnG-KV方案分别实现最高21.9倍吞吐量提升、60倍每token能耗降低和7.3倍总成本效率提升。

Conclusion: CXL支持的多PNM架构可作为未来长上下文大语言模型推理的可扩展基础架构，有效解决KV缓存管理瓶颈。

Abstract: The expansion of context windows in large language models (LLMs) to
multi-million tokens introduces severe memory and compute bottlenecks,
particularly in managing the growing Key-Value (KV) cache. While Compute
Express Link (CXL) enables non-eviction frameworks that offload the full
KV-cache to scalable external memory, these frameworks still suffer from costly
data transfers when recalling non-resident KV tokens to limited GPU memory as
context lengths increase. This work proposes scalable Processing-Near-Memory
(PNM) for 1M-Token LLM Inference, a CXL-enabled KV-cache management system that
coordinates memory and computation beyond GPU limits. Our design offloads token
page selection to a PNM accelerator within CXL memory, eliminating costly
recalls and enabling larger GPU batch sizes. We further introduce a hybrid
parallelization strategy and a steady-token selection mechanism to enhance
compute efficiency and scalability. Implemented atop a state-of-the-art CXL-PNM
system, our solution delivers consistent performance gains for LLMs with up to
405B parameters and 1M-token contexts. Our PNM-only offloading scheme (PNM-KV)
and GPU-PNM hybrid with steady-token execution (PnG-KV) achieve up to 21.9x
throughput improvement, up to 60x lower energy per token, and up to 7.3x better
total cost efficiency than the baseline, demonstrating that CXL-enabled
multi-PNM architectures can serve as a scalable backbone for future
long-context LLM inference.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [50] [Towards Portability at Scale: A Cross-Architecture Performance Evaluation of a GPU-enabled Shallow Water Solver](https://arxiv.org/abs/2511.01001)
*Johansell Villalobos,Daniel Caviedes-Voullième,Silvio Rizzi,Esteban Meneses*

Main category: cs.DC

TL;DR: 本文对浅水方程求解器SERGHEI-SWE在四种主流异构高性能计算系统上的性能进行了全面评估，展示了其良好的强弱扩展性（最高达32倍加速比和90%以上效率），并通过屋顶线分析指出内存带宽是主要性能瓶颈；研究还表明，通过Kokkos框架的细粒度优化可进一步提升其性能可移植性。


<details>
  <summary>Details</summary>
Motivation: 应对气候变化带来的复杂多尺度数值模拟挑战，满足水文建模对高分辨率、实时仿真的需求，评估并优化GPU加速平台上的性能可移植性。

Method: 在Frontier、JUWELS Booster、JEDI和Aurora四套异构HPC系统上对SERGHEI-SWE求解器进行强弱扩展性测试（最多达2048块GPU），结合屋顶线模型分析性能瓶颈，并采用调和平均与算术平均指标评估其在不同问题规模下的性能可移植性。

Result: SERGHEI-SWE在多数测试范围内展现出良好的扩展性（加速比达32，效率超90%）；屋顶线分析表明其核心核函数受内存带宽限制；在调优问题规模下可实现跨架构的性能可移植性（<70%），但仍有通过Kokkos团队机制和架构特定参数进一步优化的空间。

Conclusion: SERGHEI-SWE是一个在不断演进的高性能计算架构上具有鲁棒性、可扩展性和良好性能可移植性的大规模地球物理模拟工具，具备通过细粒度优化进一步提升性能的潜力。

Abstract: Current climate change has posed a grand challenge in the field of numerical
modeling due to its complex, multiscale dynamics. In hydrological modeling, the
increasing demand for high-resolution, real-time simulations has led to the
adoption of GPU-accelerated platforms and performance portable programming
frameworks such as Kokkos. In this work, we present a comprehensive performance
study of the SERGHEI-SWE solver, a shallow water equations code, across four
state-of-the-art heterogeneous HPC systems: Frontier (AMD MI250X), JUWELS
Booster (NVIDIA A100), JEDI (NVIDIA H100), and Aurora (Intel Max 1550). We
assess strong scaling up to 1024 GPUs and weak scaling upwards of 2048 GPUs,
demonstrating consistent scalability with a speedup of 32 and an efficiency
upwards of 90\% for most almost all the test range. Roofline analysis reveals
that memory bandwidth is the dominant performance bottleneck, with key solver
kernels residing in the memory-bound region. To evaluate performance
portability, we apply both harmonic and arithmetic mean-based metrics while
varying problem size. Results indicate that while SERGHEI-SWE achieves
portability across devices with tuned problem sizes (<70\%), there is room for
kernel optimization within the solver with more granular control of the
architecture specifically by using Kokkos teams and architecture specific
tunable parameters. These findings position SERGHEI-SWE as a robust, scalable,
and portable simulation tool for large-scale geophysical applications under
evolving HPC architectures with potential to enhance its performance.

</details>


### [51] [AeroResQ: Edge-Accelerated UAV Framework for Scalable, Resilient and Collaborative Escape Route Planning in Wildfire Scenarios](https://arxiv.org/abs/2511.00038)
*Suman Raj,Radhika Mittal,Rajiv Mayani,Pawel Zuk,Anirban Mandal,Michael Zink,Yogesh Simmhan,Ewa Deelman*

Main category: cs.DC

TL;DR: 本文提出了AeroResQ，一种面向野火应急响应的边缘加速无人机协作框架，通过服务无人机与协调无人机的分层架构，实现低延迟、高可靠性的实时逃生路径规划与任务分配。


<details>
  <summary>Details</summary>
Motivation: 野火应急响应需要实时、可靠的决策支持系统，现有方法在无人机协同、路径规划效率与系统韧性方面存在不足，亟需一种可扩展且具备故障恢复能力的边缘智能框架。

Method: AeroResQ采用多层编排架构，包括执行火情监测与人员识别的服务无人机（SDs）和负责路径规划与数据管理的协调无人机（CDs）；路径规划基于加权A*算法，系统集成Apache IoTDB轻量存储，并设计了针对CD和SD故障的负载均衡与工作区重分配机制。

Result: 在模拟南加州野火场景的实验中，AeroResQ端到端延迟≤500ms（远低于2秒请求间隔），任务重分配与完成成功率超过98%。

Conclusion: AeroResQ具备低延迟、高可靠性和强韧性，适用于野火等紧急场景下的实时现场部署，能有效提升消防员安全与救援效率。

Abstract: Drone fleets equipped with onboard cameras, computer vision, and Deep Neural
Network (DNN) models present a powerful paradigm for real-time spatio-temporal
decision-making. In wildfire response, such drones play a pivotal role in
monitoring fire dynamics, supporting firefighter coordination, and facilitating
safe evacuation. In this paper, we introduce AeroResQ, an edge-accelerated UAV
framework designed for scalable, resilient, and collaborative escape route
planning during wildfire scenarios. AeroResQ adopts a multi-layer orchestration
architecture comprising service drones (SDs) and coordinator drones (CDs), each
performing specialized roles. SDs survey fire-affected areas, detect stranded
individuals using onboard edge accelerators running fire detection and human
pose identification DNN models, and issue requests for assistance. CDs,
equipped with lightweight data stores such as Apache IoTDB, dynamically
generate optimal ground escape routes and monitor firefighter movements along
these routes. The framework proposes a collaborative path-planning approach
based on a weighted A* search algorithm, where CDs compute context-aware escape
paths. AeroResQ further incorporates intelligent load-balancing and resilience
mechanisms: CD failures trigger automated data redistribution across IoTDB
replicas, while SD failures initiate geo-fenced re-partitioning and
reassignment of spatial workloads to operational SDs. We evaluate AeroResQ
using realistic wildfire emulated setup modeled on recent Southern California
wildfires. Experimental results demonstrate that AeroResQ achieves a nominal
end-to-end latency of <=500ms, much below the 2s request interval, while
maintaining over 98% successful task reassignment and completion, underscoring
its feasibility for real-time, on-field deployment in emergency response and
firefighter safety operations.

</details>


### [52] [Tetris: An SLA-aware Application Placement Strategy in the Edge-Cloud Continuum](https://arxiv.org/abs/2511.00294)
*Lucas Almeida,Maycon Peixoto*

Main category: cs.DC

TL;DR: Tetris 是一种面向 Edge-Cloud Continuum 的应用放置策略，通过启发式算法根据 SLA 紧迫性和资源效率分配服务，在减少系统过载的同时将 SLA 违规降低约 76%，显著提升服务质量。


<details>
  <summary>Details</summary>
Motivation: 在 Edge-Cloud Continuum 环境中，需在满足用户需求和基础设施约束之间取得平衡，以实现高效资源利用并降低延迟，因此需要一种智能的应用模块放置策略。

Method: 提出 Tetris 策略，采用启发式算法在边缘和云资源间分配计算服务，优先考虑 SLA 紧迫性和资源效率，避免系统过载。

Result: 实验表明，与基线方法相比，Tetris 将 SLA 违规减少了约 76%。

Conclusion: Tetris 是一种高效的应用放置方法，适用于管理 Edge-Cloud Continuum 中对延迟敏感的应用，能显著提升用户的服务质量。

Abstract: An Edge-Cloud Continuum integrates edge and cloud resources to provide a
flexible and scalable infrastructure. This paradigm can minimize latency by
processing data closer to the source at the edge while leveraging the vast
computational power of the cloud for more intensive tasks. In this context,
module application placement requires strategic allocation plans that align
user demands with infrastructure constraints, aiming for efficient resource
use. Therefore, we propose Tetris, an application placement strategy that
utilizes a heuristic algorithm to distribute computational services across edge
and cloud resources efficiently. Tetris prioritizes services based on SLA
urgencies and resource efficiency to avoid system overloading. Our results
demonstrate that Tetris reduces SLA violations by approximately 76% compared to
the baseline method, which serves as a reference point for benchmarking
performance in this scenario. Therefore, Tetris offers an effective placement
approach for managing latency-sensitive applications in Edge-Cloud Continuum
environments, enhancing Quality of Service (QoS) for users.

</details>


### [53] [EPARA: Parallelizing Categorized AI Inference in Edge Clouds](https://arxiv.org/abs/2511.00603)
*Yubo Wang,Yubo Cui,Tuo Shi,Danyang Li,Wenxin Li,Lide Suo,Tao Wang,Xin Xie*

Main category: cs.DC

TL;DR: EPARA 是一种面向边缘计算的端到端 AI 并行推理框架，通过任务分类与资源分配策略，在不增加硬件的前提下显著提升边缘 AI 推理系统的吞吐能力。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型和计算机视觉等 AI 应用的普及，边缘云中 AI 推理任务的计算需求不断增长，如何利用现有硬件提升任务处理能力成为关键挑战。

Method: EPARA 框架根据任务对延迟/频率的敏感性和 GPU 资源需求进行分类，实现请求级和服务级的任务-资源分配。其包含三个核心组件：任务分类并行分配器、分布式请求处理器和状态感知调度器。

Result: 在包含边缘服务器、嵌入式设备和微型计算机的测试平台上，EPARA 在实际工作负载下相比现有框架实现了最高 2.1 倍的 goodput 提升，并能适应多种边缘 AI 推理任务。

Conclusion: EPARA 有效提升了边缘 AI 推理系统的服务能力，为资源受限的边缘环境提供了高效、灵活的并行推理解决方案。

Abstract: With the increasing adoption of AI applications such as large language models
and computer vision AI, the computational demands on AI inference systems are
continuously rising, making the enhancement of task processing capacity using
existing hardware a primary objective in edge clouds. We propose EPARA, an
end-to-end AI parallel inference framework in edge, aimed at enhancing the edge
AI serving capability. Our key idea is to categorize tasks based on their
sensitivity to latency/frequency and requirement for GPU resources, thereby
achieving both request-level and service-level task-resource allocation. EPARA
consists of three core components: 1) a task-categorized parallelism allocator
that decides the parallel mode of each task, 2) a distributed request handler
that performs the calculation for the specific request, and 3) a state-aware
scheduler that periodically updates service placement in edge clouds. We
implement a EPARA prototype and conduct a case study on the EPARA operation for
LLMs and segmentation tasks. Evaluation through testbed experiments involving
edge servers, embedded devices, and microcomputers shows that EPARA achieves up
to 2.1$\times$ higher goodput in production workloads compared to prior
frameworks, while adapting to various edge AI inference tasks.

</details>


### [54] [AReaL-Hex: Accommodating Asynchronous RL Training over Heterogeneous GPUs](https://arxiv.org/abs/2511.00796)
*Ran Yan,Youhe Jiang,Tianyuan Wu,Jiaxuan Gao,Zhiyu Mei,Wei Fu,Haohui Mai,Wei Wang,Yi Wu,Binhang Yuan*

Main category: cs.DC

TL;DR: AReaL-Hex 是一种面向异构 GPU 的异步强化学习（RL）训练系统，通过两阶段调度策略优化生成与训练阶段的资源分配，在保证数据新鲜度的前提下显著提升训练吞吐量并降低成本。


<details>
  <summary>Details</summary>
Motivation: 为降低大语言模型（LLM）强化学习训练的成本并提升吞吐量，需有效利用异构 GPU 资源。传统同构部署难以匹配 RL 三阶段（rollout 生成、奖励计算、策略/价值更新）在计算强度、内存占用和通信模式上的显著差异。

Method: 提出 AReaL-Hex 系统，基于完全异步 RL 架构，采用两阶段调度器：(i) 使用混合整数线性规划（MILP）在资源预算约束下选择各阶段的并行策略与工作负载分配；(ii) 通过图划分方法分配异构 GPU 与互连资源以最大化端到端吞吐量，并控制数据陈旧性。

Result: 在数学推理任务上（模型规模 1.5B、7B、14B），相比同构部署的先进异步 RL 系统：(i) 相同预算下，AReaL-Hex 吞吐量最高提升 1.50 倍；(ii) 相同吞吐量下，训练成本最多降低 1.46 倍。

Conclusion: AReaL-Hex 通过异构感知的异步调度机制，有效平衡了生成与优化阶段的资源需求，在真实异构 GPU 环境中实现了更高的训练效率与成本效益。

Abstract: Maximizing training throughput and cost-efficiency of RL for LLMs is
essential to democratize this advanced technique. One promising but challenging
approach is to deploy such a computational workflow over heterogeneous GPUs.
Unlike conventional large-scale LLM pretraining, RL training generally
decomposes into three coupled stages, i.e., rollout generation, reward
computation, and policy/value updates, which exhibit markedly different compute
intensities, memory footprints, and communication patterns. Recent research
shows that fully asynchronous RL training can disaggregate these stages across
disjoint hardware pools without sacrificing training stability, creating a
great opportunity for real-world heterogeneous deployment. To this end, we
present AReaL-Hex, a heterogeneity-aware asynchronous RL training system that
effectively schedules how to execute rollout generation and policy model
training over heterogeneous GPUs while enforcing data staleness bounds.
Concretely, we use a two-phase scheduler: (i) a constrained search with MILP to
select per-stage parallelization strategies and workload assignments given a
resource budget, and (ii) a graph-partitioning step that allocates
heterogeneous GPUs and interconnects to maximize end-to-end throughput. Built
atop a fully asynchronous RL architecture, AReaL-Hex maps HBM-I/O-bound
generation and compute-bound optimization to more cost-efficient resources and
balances their producer-consumer interactions to avoid both idleness and stale
rollout trajectories. On the mathematical reasoning task with various model
scales (1.5B, 7B, and 14B), compared to homogeneous deployments of
state-of-the-art asynchronous RL systems: (i) When maintaining the same total
budgets, AReaL-Hex delivers up to 1.50x higher training throughput; (ii) When
achieving the same training throughput, AReaL-Hex results in up to 1.46x
reduction in training cost.

</details>


### [55] [FREESH: Fair, Resource- and Energy-Efficient Scheduling for LLM Serving on Heterogeneous GPUs](https://arxiv.org/abs/2511.00807)
*Xuan He,Zequan Fang,Jinzhao Lian,Danny H. K. Tsang,Baosen Zhang,Yize Chen*

Main category: cs.DC

TL;DR: FREESH 是一种面向地理分布式异构 GPU 集群的 LLM 服务系统优化框架，通过联合路由与调度，在满足延迟和公平性的同时显著降低能耗与碳排放。


<details>
  <summary>Details</summary>
Motivation: 大模型（LLM）和 AI 智能体的计算与能耗需求不断增长，而实际部署中 GPU 集群具有地理分布性和异构性，且 LLM 查询负载在流量和模式上存在多样性。不同地点、时间、硬件配置下的查询执行会导致显著不同的碳足迹，因此需要一种能综合考虑时空计算灵活性的优化方法。

Method: FREESH 框架通过匹配不同 GPU 实例的功耗-吞吐特性与可预测的 LLM 查询长度和负载，确定最优负载均衡配置；结合动态 GPU 频率调节实现节能，并采用 Least-Laxity-First (LLF) 策略进行查询调度，同时优化并行度与查询路由以满足延迟与公平性要求。

Result: 在真实生产负载的一小时服务测试中，FREESH 实现了 28.6% 的能耗降低和 45.45% 的碳排放减少，同时提升了服务等级目标（SLO）达成率和公平性。

Conclusion: FREESH 有效利用时空计算灵活性，在保障服务质量的前提下显著优化了 LLM 服务系统的能效与环保性能，为绿色 AI 提供了可行路径。

Abstract: The ever-increasing computation and energy demand for LLM and AI agents call
for holistic and efficient optimization of LLM serving systems. In practice,
heterogeneous GPU clusters can be deployed in a geographically distributed
manner, while LLM load also observes diversity in terms of both query traffic
and serving patterns. LLM queries running on advanced GPUs during a
high-emission hour at one location can lead to significantly higher carbon
footprints versus same queries running on mid-level GPUs at a low-emission time
and location. By observing LLM serving requirements and leveraging
spatiotemporal computation flexibility, we consider the joint routing and
scheduling problem, and propose FREESH to cooperatively run a group of data
centers while minimizing user-specified carbon or energy objectives. FREESH
identifies the optimal configurations of balanced load serving by matching
distinct GPU instance's power-throughput characteristics with predictable LLM
query length and workloads. To ensure both latency and fairness requirements,
FREESH identifies optimized parallelism and query routing schedules together
with dynamic GPU frequency scaling for power saving, and Least-Laxity-First
(LLF) serving strategy for query scheduling. During the 1-hour serving on
production workloads, FREESH reduces energy by 28.6% and emissions by 45.45%
together with improvements in SLO attainment and fairness.

</details>


### [56] [Scalable Maxflow Processing for Dynamic Graphs](https://arxiv.org/abs/2511.01235)
*Shruthi Kannappan,Ashwina Kumar,Rupesh Nasre*

Main category: cs.DC

TL;DR: 本文提出了一种新颖的GPU并行最大流算法，支持在动态图上增量重计算最大流，并设计了高性能的静态图初始最大流计算方法，同时引入多种CUDA优化以提升GPU上的性能、可扩展性和内存效率。


<details>
  <summary>Details</summary>
Motivation: 最大流问题是图论和组合优化中的核心问题，具有广泛的实际应用。由于Push-Relabel算法具备局部顶点操作特性，适合并行化，因此推动了基于GPU加速的最大流计算研究。然而，现有方法在处理动态图更新时效率不足，亟需一种高效支持增量更新和静态计算的GPU算法。

Method: 作者设计了一种新的GPU并行最大流算法，能够对一批边更新后的动态图进行增量式最大流重计算；同时开发了一个高效的静态GPU算法用于初始最大流求解，并结合多种CUDA特定优化技术提升整体性能。

Result: 该方法在GPU平台上实现了更高的性能、可扩展性和内存效率，有效支持动态图的最大流增量更新与静态图的快速初始计算。

Conclusion: 所提出的GPU并行最大流算法及其优化策略显著提升了动态和静态图场景下的计算效率，为大规模图处理提供了实用且高效的解决方案。

Abstract: The Maximum Flow (Max-Flow) problem is a cornerstone in graph theory and
combinatorial optimization, aiming to determine the largest possible flow from
a designated source node to a sink node within a capacitated flow network. It
has extensive applications across diverse domains such as computer networking,
transportation systems, and image segmentation. The objective is to maximize
the total throughput while respecting edge capacity constraints and maintaining
flow conservation at all intermediate vertices.
  Among the various algorithms proposed for solving the Max-Flow problem, the
Push--Relabel algorithm is particularly notable for its efficiency and
suitability for parallelization, owing to its localized vertex-based
operations. This property has motivated extensive research into GPU-accelerated
Max-Flow computation, leveraging the high degree of parallelism inherent to
modern GPU architectures.
  In this paper, we present a novel GPU-parallel Max-Flow algorithm capable of
incrementally recomputing the maximum flow of a dynamic graph following a batch
of edge updates. In addition, we introduce a high-performance static GPU
algorithm designed for efficiently computing the initial Max-Flow on static
graphs. We further describe a series of CUDA-specific implementation
optimizations that enhance performance, scalability, and memory efficiency on
GPU platforms.

</details>


### [57] [Design of quasi phase matching crystal based on differential gray wolf algorithm](https://arxiv.org/abs/2511.01255)
*He Chen,ZiHua Zheng,JingHua Sun*

Main category: cs.DC

TL;DR: 本文提出一种融合差分进化（DE）与灰狼优化（GWO）的混合优化算法，并结合GPU并行加速技术，高效解决非线性光学中准相位匹配晶体结构的高维离散优化难题，显著提升设计效率与精度。


<details>
  <summary>Details</summary>
Motivation: 非线性光学技术发展中，准相位匹配晶体性能优化面临高维离散组合的“NP难”问题，传统算法收敛慢、易陷入局部最优，而遗传算法等启发式方法受限于CPU串行计算效率低下。

Method: 采用DE算法进行全局搜索，GWO算法强化局部搜索与收敛速度，二者协同平衡全局与局部优化；同时利用GPU多核架构实现线程级并行计算，提升整体优化效率。

Result: 相比传统CPU串行计算，该方案将准相位匹配设计效率提升数百至数千倍，显著提高晶体畴结构控制精度，有效突破高维离散空间优化瓶颈。

Conclusion: 所提方法为复杂非线性光学器件设计提供了新范式，有助于推动量子光学和激光加工等领域相关器件的性能突破与工业应用。

Abstract: This paper focuses on the key problem in the development of nonlinear optical
technology, the performance optimization of aperiodically polarized crystals.
The performance of the crystal depends on the precise control of the micro
distribution of crystal domains, but its optimization belongs to the
high-dimensional discrete combination "NP hard" problem. The traditional
algorithm has the bottleneck of slow convergence and easy to fall into local
optimization, while the heuristic methods such as genetic algorithm are limited
by the CPU serial calculation and inefficient. In order to solve the above
challenges, this paper proposes the fusion scheme of hwsda hybrid optimization
algorithm and GPU parallel acceleration technology: the differential evolution
algorithm (DE) is used to realize the global search, and the gray wolf
optimization algorithm (GWO) is used to strengthen the local search and
convergence speed, and the two coordinate to balance the global and local
optimization requirements; At the same time, it relies on GPU multi-core
architecture to realize thread level parallel computing and improve
optimization efficiency. This scheme effectively breaks through the
optimization problem of high-dimensional discrete space, improves the accuracy
of crystal domain control, improves the efficiency of quasi phase matching
design by hundreds to thousands of times compared with traditional CPU serial
computing, provides a new paradigm for the design of complex nonlinear optical
devices, and helps promote the performance breakthrough and industrial
application of related devices in the fields of quantum optics and laser
processing.

</details>


### [58] [Adaptive Multidimensional Quadrature on Multi-GPU Systems](https://arxiv.org/abs/2511.01573)
*Melanie Tonarelli,Simone Riva,Pietro Benedusi,Fabrizio Ferrandi,Rolf Krause*

Main category: cs.DC

TL;DR: 本文提出了一种面向多GPU架构的分布式自适应积分方法，通过分层域分解和去中心化负载重分配策略，在高维积分中实现了更高的效率和更强的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 高维数值积分在多GPU系统上面临负载不均衡问题，现有方法在效率和对被积函数正则性及目标精度的鲁棒性方面存在不足。

Method: 将多维积分建模为分层域分解问题，利用局部误差估计器递归划分积分区域；每个子域在独立GPU上演化，并采用基于循环轮询策略的去中心化负载重分配机制，通过非阻塞、CUDA感知的MPI通信与计算重叠实现动态负载均衡。

Result: 相比当前最先进的GPU专用积分包，该方法在高维情形下效率更高，且对被积函数的正则性和目标精度具有更好的鲁棒性。

Conclusion: 所提出的分布式自适应积分方法有效解决了多GPU环境下高维积分的负载不均衡问题，显著提升了计算效率与适用性。

Abstract: We introduce a distributed adaptive quadrature method that formulates
multidimensional integration as a hierarchical domain decomposition problem on
multi-GPU architectures. The integration domain is recursively partitioned into
subdomains whose refinement is guided by local error estimators. Each subdomain
evolves independently on a GPU, which exposes a significant load imbalance as
the adaptive process progresses. To address this challenge, we introduce a
decentralised load redistribution schemes based on a cyclic round-robin policy.
This strategy dynamically rebalance subdomains across devices through
non-blocking, CUDA-aware MPI communication that overlaps with computation. The
proposed strategy has two main advantages compared to a state-of-the-art
GPU-tailored package: higher efficiency in high dimensions; and improved
robustness w.r.t the integrand regularity and the target accuracy.

</details>


### [59] [LARK -- Linearizability Algorithms for Replicated Keys in Aerospike](https://arxiv.org/abs/2511.01843)
*Andrew Goodng,Kevin Porter,Thomas Lopatic,Ashish Shinde,Sunil Sayyaparaju,Srinivasan Seshadri,V. Srinivasan*

Main category: cs.DC

TL;DR: LARK 是一种同步复制协议，通过引入分区可用性条件（PAC）和摒弃有序日志，在保证线性一致性的同时显著提升可用性、降低延迟与基础设施成本。


<details>
  <summary>Details</summary>
Motivation: 传统基于法定日志的共识协议（如 Raft、Paxos）在节点故障时需暂停提交以重建副本，导致可用性受限；LARK 旨在解决这一问题，在相同存储开销下实现更高可用性和更低延迟。

Method: LARK 引入分区可用性条件（PAC），在全集群范围内而非固定副本集中判断可用性，并去除有序日志机制，允许在领导者变更后立即恢复分区服务，仅在必要时进行每键一次的重复解决往返。

Result: 在容忍一个故障时，LARK 的分区可用性提升约 3 倍；容忍两个故障时提升约 10 倍。在仅维护 f+1 个数据副本的条件下，LARK 能在数据节点故障期间持续提交，而日志型协议必须暂停。此外，LARK 支持仅两副本下的零停机滚动重启。

Conclusion: LARK 通过创新的可用性判断机制和无日志设计，在保证线性一致性的前提下显著优于传统共识协议，尤其在高可用性和运维成本方面具有明显优势。

Abstract: We present LARK (Linearizability Algorithms for Replicated Keys), a
synchronous replication protocol that achieves linearizability while minimizing
latency and infrastructure cost, at significantly higher availability than
traditional quorum-log consensus. LARK introduces Partition Availability
Conditions (PAC) that reason over the entire database cluster rather than fixed
replica sets, improving partition availability under independent failures by
roughly 3x when tolerating one failure and 10x when tolerating two. Unlike
Raft, Paxos, and Viewstamped Replication, LARK eliminates ordered logs,
enabling immediate partition readiness after leader changes -- with at most a
per-key duplicate-resolution round trip when the new leader lacks the latest
copy. Under equal storage budgets -- where both systems maintain only f+1 data
copies to tolerate f failures -- LARK continues committing through data-node
failures while log-based protocols must pause commits for replica rebuilding.
These properties also enable zero-downtime rolling restarts even when
maintaining only two copies. We provide formal safety arguments and a TLA+
specification, and we demonstrate through analysis and experiments that LARK
achieves significant availability gains.

</details>
