<div id=toc></div>

# Table of Contents

- [cs.NI](#cs.NI) [Total: 3]
- [cs.MA](#cs.MA) [Total: 4]
- [cs.DC](#cs.DC) [Total: 5]
- [cs.AR](#cs.AR) [Total: 3]
- [cs.SE](#cs.SE) [Total: 32]
- [cs.OS](#cs.OS) [Total: 3]


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [1] [Performance Evaluation of V2X Communication Using Large-Scale Traffic Data](https://arxiv.org/abs/2602.07244)
*John Pravin Arockiasamy,Alexey Vinel*

Main category: cs.NI

TL;DR: 本文利用真实交通数据集进行了大规模V2X通信性能评估，证实现实交通条件下协作感知服务可扩展运行，并揭示合成交通模拟可能高估信道拥堵。


<details>
  <summary>Details</summary>
Motivation: V2X技术虽被视作自动驾驶关键，但实际部署有限；现有评估多依赖模拟交通场景，可能无法准确反映真实交通特征，需基于真实数据补充研究。

Method: 将HighD和InD无人机采集的真实车辆轨迹转换为仿真格式，结合标准化V2X协议栈，对十余万辆车的消息级性能进行分析，评估高速公路与城市路口的核心性能指标。

Result: 协作感知服务在真实交通场景中仍具可行性；交通密度、移动模式和通信范围显著影响性能；合成交通模型可能高估信道繁忙率。

Conclusion: Conclusion extraction failed

Abstract: Vehicular communication (V2X) technologies are widely regarded as a cornerstone for cooperative and automated driving, yet their large-scale real-world deployment remains limited. As a result, understanding V2X performance under realistic, full-scale traffic conditions continues to be relevant. Most existing performance evaluations rely on synthetic traffic scenarios generated by simulators, which, while useful, may not fully capture the features of real-world traffic. In this paper, we present a large-scale, data-driven evaluation of V2X communication performance using real-world traffic datasets. Vehicle trajectories derived from the Highway Drone (HighD) and Intersection Drone (InD) datasets are converted into simulation-ready formats and coupled with a standardized V2X networking stack to enable message-level performance analysis for entire traffic populations comprising over hundred thousands vehicles across multiple locations. We evaluate key V2X performance indicators, including inter-generation gap, inter-packet gap, packet delivery ratio, and channel busy ratio, across both highway and urban intersection environments. Our results show that cooperative awareness services remain feasible at scale under realistic traffic conditions. In addition, the findings highlight how traffic density, mobility patterns, and communication range influence V2X performance and how synthetic traffic assumptions may overestimate channel congestion.

</details>


### [2] [Mirage: Transmitting a Video as a Perceptual Illusion for 50,000X Speedup](https://arxiv.org/abs/2602.07396)
*Junjie Wu,Tianrui Li,Yi Zhang,Ziyuan Yang*

Main category: cs.NI

TL;DR: 本文提出了一种无视觉数据的通信框架Mirage，用于高效视频传输，通过语义表示替代原始数据，显著降低带宽消耗并保护隐私。


<details>
  <summary>Details</summary>
Motivation: 现有框架注重信号级保真度重构，导致高通信开销和系统复杂性，尤其在视频传输中造成巨大带宽消耗。

Method: Mirage将视频分解为时间序列信息（捕捉动作动态）和空间外貌表示（描述视觉结构），前者通过视频描述生成保留，后者编码为紧凑语义表示；传输这些表示后，接收端用生成模型合成视频，并支持个性化约束以实现灵活权衡。

Result: 实验证明，Mirage相比原始视频传输实现高达50000倍数据级压缩加速，且规模越大增益越显著。

Conclusion: 这是一种高效、隐私保护且灵活的框架，允许在效率、隐私、控制与感知质量间平衡。

Abstract: The existing communication framework mainly aims at accurate reconstruction of source signals to ensure reliable transmission. However, this signal-level fidelity-oriented design often incurs high communication overhead and system complexity, particularly in video communication scenarios where mainstream frameworks rely on transmitting visual data itself, resulting in significant bandwidth consumption. To address this issue, we propose a visual data-free communication framework, Mirage, for extremely efficient video transmission while preserving semantic information. Mirage decomposes video content into two complementary components: temporal sequence information capturing motion dynamics and spatial appearance representations describing overall visual structure. Temporal information is preserved through video captioning, while key frames are encoded into compact semantic representations for spatial appearance. These representations are transmitted to the receiver, where videos are synthesized using generative video models. Since no raw visual data is transmitted, Mirage is inherently privacy-preserving. Mirage also supports personalized adaptation across deployment scenarios. The sender, network, and receiver can independently impose constraints on semantic representation, transmission, and generation, enabling flexible trade-offs between efficiency, privacy, control, and perceptual quality. Experimental results in video transmission demonstrate that Mirage achieves up to a 50000X data-level compression speedup over raw video transmission, with gains expected to scale with larger video content sizes.

</details>


### [3] [DHEA-MECD: An Embodied Intelligence-Powered DRL Algorithm for AUV Tracking in Underwater Environments with High-Dimensional Features](https://arxiv.org/abs/2602.07947)
*Kai Tian,Chuan Lin,Guangjie Han,Chen An,Qian Zhu,Shengzhao Zhu,Zhenyu Wang*

Main category: cs.NI

TL;DR: 提出分层具身智能架构和DHEA-MECD深度强化学习算法，解决自主水下航行器在复杂海洋环境中多目标跟踪的鲁棒性问题。


<details>
  <summary>Details</summary>
Motivation: 现实水下环境存在高维特征耦合、空间约束和时变干扰等挑战，导致AUV多目标跟踪效率不足。

Method: 设计双头编码器注意力框架分解感知数据并建模异构特征依赖；引入运动阶段感知的多专家决策机制与Top-k专家选择策略实现自适应决策。

Result: 相比主流DRL方法，在干扰复杂环境中实现了更高成功率、更快收敛和更优运动轨迹。

Conclusion: DHEA-MECD算法有效提升了AUV在扰动环境下多目标跟踪的智能化、稳定性和抗干扰能力。

Abstract: In recent years, autonomous underwater vehicle (AUV) systems have demonstrated significant potential in complex marine exploration. However, effective AUV-based tracking remains challenging in realistic underwater environments characterized by high-dimensional features, including coupled kinematic states, spatial constraints, time-varying environmental disturbances, etc. To address these challenges, this paper proposes a hierarchical embodied-intelligence (EI) architecture for underwater multi-target tracking with AUVs in complex underwater environments. Built upon this architecture, we introduce the Double-Head Encoder-Attention-based Multi-Expert Collaborative Decision (DHEA-MECD), a novel Deep Reinforcement Learning (DRL) algorithm designed to support efficient and robust multi-target tracking. Specifically, in DHEA-MECD, a Double-Head Encoder-Attention-based information extraction framework is designed to semantically decompose raw sensory observations and explicitly model complex dependencies among heterogeneous features, including spatial configurations, kinematic states, structural constraints, and stochastic perturbations. On this basis, a motion-stage-aware multi-expert collaborative decision mechanism with Top-k expert selection strategy is introduced to support stage-adaptive decision-making. Furthermore, we propose the DHEA-MECD-based underwater multitarget tracking algorithm to enable AUV smart, stable, and anti-interference multi-target tracking. Extensive experimental results demonstrate that the proposed approach achieves superior tracking success rates, faster convergence, and improved motion optimality compared with mainstream DRL-based methods, particularly in complex and disturbance-rich marine environments.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [4] [Altruism and Fair Objective in Mixed-Motive Markov games](https://arxiv.org/abs/2602.08389)
*Yao-hua Franck Xu,Tayeb Lemlouma,Arnaud Braud,Jean-Marie Bonnin*

Main category: cs.MA

TL;DR: 本文提出比例公平框架取代功利主义，以促进多智能体合作公平性，通过新造公平利他效用函数和算法在顺序环境中评估。


<details>
  <summary>Details</summary>
Motivation: 动机是解决社会困境中个人利益与集体福祉的冲突，功利主义方法虽高效但可能加剧不公平。

Method: 方法是定义基于个体对数收益比例的公平利他实用函数，推导合作条件；扩展至顺序情境，创建公平马尔可夫博弈并引入公平Actor-Critic算法。

Result: 结果在多种社会困境环境中评估，显示框架有效促进公平合作。

Conclusion: 结论为该框架可替代功利主义，实现更公平的合作过程。

Abstract: Cooperation is fundamental for society's viability, as it enables the emergence of structure within heterogeneous groups that seek collective well-being. However, individuals are inclined to defect in order to benefit from the group's cooperation without contributing the associated costs, thus leading to unfair situations. In game theory, social dilemmas entail this dichotomy between individual interest and collective outcome. The most dominant approach to multi-agent cooperation is the utilitarian welfare which can produce efficient highly inequitable outcomes. This paper proposes a novel framework to foster fairer cooperation by replacing the standard utilitarian objective with Proportional Fairness. We introduce a fair altruistic utility for each agent, defined on the individual log-payoff space and derive the analytical conditions required to ensure cooperation in classic social dilemmas. We then extend this framework to sequential settings by defining a Fair Markov Game and deriving novel fair Actor-Critic algorithms to learn fair policies. Finally, we evaluate our method in various social dilemma environments.

</details>


### [5] [EvoCorps: An Evolutionary Multi-Agent Framework for Depolarizing Online Discourse](https://arxiv.org/abs/2602.08529)
*Ning Lin,Haolun Li,Mingshu Liu,Chengyun Ruan,Kaibo Huang,Yukun Wei,Zhongliang Yang,Linna Zhou*

Main category: cs.MA

TL;DR: EvoCorps是用于主动减少在线讨论极化的进化多智能体框架，通过在对抗环境中模拟动态社会游戏，优于基准方法。


<details>
  <summary>Details</summary>
Motivation: 在线极化侵蚀社会信任并加速错误信息传播，现有治理方法滞后、静态，难应对实时演化的协调对抗放大。

Method: 框架将治理框定为动态社会游戏，协调角色如监控、规划、事实生成和多身份扩散；利用检索增强集体认知核心进行事实锚定和实践学习，闭环进化自适应策略。

Result: 在MOSAIC平台模拟对抗新闻流中，EvoCorps显著提升情感极化、观点极端性和论证理性指标，优于对抗基线。

Conclusion: 证实从业后检测转向闭环干预的实用路径，代码开源提供可扩展性。

Abstract: Polarization in online discourse erodes social trust and accelerates misinformation, yet technical responses remain largely diagnostic and post-hoc. Current governance approaches suffer from inherent latency and static policies, struggling to counter coordinated adversarial amplification that evolves in real-time. We present EvoCorps, an evolutionary multi-agent framework for proactive depolarization. EvoCorps frames discourse governance as a dynamic social game and coordinates roles for monitoring, planning, grounded generation, and multi-identity diffusion. A retrieval-augmented collective cognition core provides factual grounding and action--outcome memory, while closed-loop evolutionary learning adapts strategies as the environment and attackers change. We implement EvoCorps on the MOSAIC social-AI simulation platform for controlled evaluation in a multi-source news stream with adversarial injection and amplification. Across emotional polarization, viewpoint extremity, and argumentative rationality, EvoCorps improves discourse outcomes over an adversarial baseline, pointing to a practical path from detection and post-hoc mitigation to in-process, closed-loop intervention. The code is available at https://github.com/ln2146/EvoCorps.

</details>


### [6] [Teaching an Old Dynamics New Tricks: Regularization-free Last-iterate Convergence in Zero-sum Games via BNN Dynamics](https://arxiv.org/abs/2602.08938)
*Tuo Zhang,Leonardo Stella*

Main category: cs.MA

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Zero-sum games are a fundamental setting for adversarial training and decision-making in multi-agent learning (MAL). Existing methods often ensure convergence to (approximate) Nash equilibria by introducing a form of regularization. Yet, regularization requires additional hyperparameters, which must be carefully tuned--a challenging task when the payoff structure is known, and considerably harder when the structure is unknown or subject to change. Motivated by this problem, we repurpose a classical model in evolutionary game theory, i.e., the Brown-von Neumann-Nash (BNN) dynamics, by leveraging the intrinsic convergence of this dynamics in zero-sum games without regularization, and provide last-iterate convergence guarantees in noisy normal-form games (NFGs). Importantly, to make this approach more applicable, we develop a novel framework with theoretical guarantees that integrates the BNN dynamics in extensive-form games (EFGs) through counterfactual weighting. Furthermore, we implement an algorithm that instantiates our framework with neural function approximation, enabling scalable learning in both NFGs and EFGs. Empirical results show that our method quickly adapts to nonstationarities, outperforming the state-of-the-art regularization-based approach.

</details>


### [7] [Learning to Coordinate via Quantum Entanglement in Multi-Agent Reinforcement Learning](https://arxiv.org/abs/2602.08965)
*John Gardiner,Orlando Romero,Brendan Tivnan,Nicolò Dal Fabbro,George J. Pappas*

Main category: cs.MA

TL;DR: 首次提出利用共享量子纠缠的MARL框架，突破经典共享随机性的协调局限，实现无通信下的量子优势协作策略。


<details>
  <summary>Details</summary>
Motivation: 量子物理研究表明纠缠能在无通信协作任务中超越经典随机性，为MARL协调难题提供理论依据。

Method: 设计可微分策略参数化优化量子测量，采用量子协调器+分散执行者的架构分解联合策略。

Result: 在单轮黑盒游戏与序列化Dec-POMDP中验证：纯经验学习能实现超越经典方法的量子优势策略。

Conclusion: 该框架扩展了无通信协调策略的边界，为复杂MARL问题开辟量子增强新路径。

Abstract: The inability to communicate poses a major challenge to coordination in multi-agent reinforcement learning (MARL). Prior work has explored correlating local policies via shared randomness, sometimes in the form of a correlation device, as a mechanism to assist in decentralized decision-making. In contrast, this work introduces the first framework for training MARL agents to exploit shared quantum entanglement as a coordination resource, which permits a larger class of communication-free correlated policies than shared randomness alone. This is motivated by well-known results in quantum physics which posit that, for certain single-round cooperative games with no communication, shared quantum entanglement enables strategies that outperform those that only use shared randomness. In such cases, we say that there is quantum advantage. Our framework is based on a novel differentiable policy parameterization that enables optimization over quantum measurements, together with a novel policy architecture that decomposes joint policies into a quantum coordinator and decentralized local actors. To illustrate the effectiveness of our proposed method, we first show that we can learn, purely from experience, strategies that attain quantum advantage in single-round games that are treated as black box oracles. We then demonstrate how our machinery can learn policies with quantum advantage in an illustrative multi-agent sequential decision-making problem formulated as a decentralized partially observable Markov decision process (Dec-POMDP).

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [8] [Parallel Track Transformers: Enabling Fast GPU Inference with Reduced Synchronization](https://arxiv.org/abs/2602.07306)
*Chong Wang,Nan Du,Tom Gunter,Tao Lei,Kulin Seth,Senyu Tong,Jianyu Wang,Guoli Yin,Xiyou Zhou,Kelvin Zou,Ruoming Pang*

Main category: cs.DC

TL;DR: 本文提出并行轨迹（PT）变压器架构，通过在多个GPU上减少同步操作，解决大规模语言模型推理中的通信效率瓶颈问题。


<details>
  <summary>Details</summary>
Motivation: 传统张量并行在多GPU推理中引入大量设备间同步，造成通信瓶颈和可扩展性下降，需要优化以提升性能。

Method: 开发PT变压器架构，重构计算过程以最小化跨设备依赖，集成到Tensor-RT-LLM和vLLM等服务系统中。

Result: 相比标准张量并行，同步操作减少最高16倍；服务效率显著提升，首次令牌时间减少15-30%，输出令牌时间缩短2-12%，吞吐量增加31.90%。

Conclusion: PT架构在保持模型质量的前提下，有效提升LLM服务效率和性能，具有广泛应用潜力。

Abstract: Efficient large-scale inference of transformer-based large language models (LLMs) remains a fundamental systems challenge, frequently requiring multi-GPU parallelism to meet stringent latency and throughput targets. Conventional tensor parallelism decomposes matrix operations across devices but introduces substantial inter-GPU synchronization, leading to communication bottlenecks and degraded scalability. We propose the Parallel Track (PT) Transformer, a novel architectural paradigm that restructures computation to minimize cross-device dependencies. PT achieves up to a 16x reduction in synchronization operations relative to standard tensor parallelism, while maintaining competitive model quality in our experiments. We integrate PT into two widely adopted LLM serving stacks-Tensor-RT-LLM and vLLM-and report consistent improvements in serving efficiency, including up to 15-30% reduced time to first token, 2-12% reduced time per output token, and up to 31.90% increased throughput in both settings.

</details>


### [9] [Privacy-Preserving Coding Schemes for Multi-Access Distributed Computing Models](https://arxiv.org/abs/2602.07850)
*Shanuja Sasi*

Main category: cs.DC

TL;DR: 本文在分布式计算框架MADC中引入隐私约束，为两种连接模型开发私有编码方案，确保减速器函数隐私。


<details>
  <summary>Details</summary>
Motivation: 动机是保护分布式计算中减速器功能的机密性，避免函数泄露风险，同时减少通信瓶颈。

Method: 方法包括构建新型扩展放置交付阵列家族，并推导隐私保证的编码方案。

Result: 实现了减速器函数在特定模型下的隐私保障，提供高效计算。

Conclusion: 结论是私有编码方案成功提升MADC的隐私安全性。

Abstract: Distributed computing frameworks such as MapReduce have become essential for large-scale data processing by decomposing tasks across multiple nodes. The multi-access distributed computing (MADC) model further advances this paradigm by decoupling mapper and reducer roles: dedicated mapper nodes store data and compute intermediate values, while reducer nodes are connected to multiple mappers and aggregate results to compute final outputs. This separation reduces communication bottlenecks without requiring file replication. In this paper, we introduce privacy constraints into MADC and develop private coded schemes for two specific connectivity models. We construct new families of extended placement delivery arrays and derive corresponding coding schemes that guarantee privacy of each reducer's assigned function.

</details>


### [10] [HEAL: Online Incremental Recovery for Leaderless Distributed Systems Across Persistency Models](https://arxiv.org/abs/2602.08257)
*Antonis Psistakis,Burak Ocalan,Fabien Chaix,Ramnatthan Alagappan,Josep Torrellas*

Main category: cs.DC

TL;DR: 该论文提出了一种名为 HEAL 的高效低开销恢复机制，用于现代无领导分布式系统，以优化故障时的在线增量恢复。


<details>
  <summary>Details</summary>
Motivation: 为应对分布式系统在故障时需实现快速恢复并最小化吞吐量影响的需求，尤其是在现代环境中确保韧性尤为重要。

Method: 提出 HEAL 方案，包括针对线性一致性和多种内存持久性模型的算法，通过优化在线增量恢复来处理节点故障。

Result: 实验基于6节点Intel集群运行TAOBench工作负载：HEAL平均恢复时间为120毫秒，吞吐量下降仅8.7%，速度比传统方案快（360秒恢复/16.2%下降），且延迟减少20.7倍，吞吐劣化降低62.4%。

Conclusion: HEAL验证为非常有效，显著提升恢复效率和系统韧性。在分布式 weak-consistency-like 系统中应用潜力大

Abstract: Ensuring resilience in distributed systems has become an acute concern. In today's environment, it is crucial to develop light-weight mechanisms that recover a distributed system from faults quickly and with only a small impact on the live-system throughput. To address this need, this paper proposes a new low-overhead, general recovery scheme for modern non-transactional leaderless distributed systems. We call our scheme HEAL. On a node failure, HEAL performs an optimized online incremental recovery. This paper presents HEAL's algorithms for settings with Linearizable consistency and different memory persistency models. We implement HEAL on a 6-node Intel cluster. Our experiments running TAOBench workloads show that HEAL is very effective. HEAL recovers the cluster in 120 milliseconds on average, while reducing the throughput of the running workload by an average of 8.7%. In contrast, a conventional recovery scheme for leaderless systems needs 360 seconds to recover, reducing the throughput of the system by 16.2%. Finally, compared to an incremental recovery scheme for a state-of-the-art leader-based system, HEAL reduces the average recovery latency by 20.7x and the throughput degradation by 62.4%.

</details>


### [11] [Towards CXL Resilience to CPU Failures](https://arxiv.org/abs/2602.08271)
*Antonis Psistakis,Burak Ocalan,Chloe Alverti,Fabien Chaix,Ramnatthan Alagappan,Josep Torrellas*

Main category: cs.DC

TL;DR: 本文提出了ReCXL系统，扩展CXL协议以解决节点故障下的分布式计算弹性问题，通过复制写入和日志记录实现故障恢复。


<details>
  <summary>Details</summary>
Motivation: CXL 3.0及以上支持基于硬件缓存一致性的共享内存，但未处理节点故障；故障可能导致脏数据丢失并破坏应用状态，且指定规范无一致性恢复机制。

Method: ReCXL在写入事务中添加消息，将更新复制到副本节点，存储在硬件日志单元；定期将日志转储到内存；恢复时利用日志修复目录和内存状态。

Result: ReCXL系统支持故障容忍执行，与无容错平台相比性能仅下降30%。

Conclusion: ReCXL有效增强了CXL弹性，为分布式计算提供成本效益高的故障恢复方案。

Abstract: Compute Express Link (CXL) 3.0 and beyond allows the compute nodes of a cluster to share data with hardware cache coherence and at the granularity of a cache line. This enables shared-memory semantics for distributed computing, but introduces new resilience challenges: a node failure leads to the loss of the dirty data in its caches, corrupting application state. Unfortunately, the CXL specification does not consider processor failures. Moreover, when a component fails, the specification tries to isolate it and continue application execution; there is no attempt to bring the application to a consistent state. To address these limitations, this paper extends the CXL specification to be resilient to node failures, and to correctly recover the application after node failures. We call the system ReCXL. To handle the failure of nodes, ReCXL augments the coherence transaction of a write with messages that propagate the update to a small set of other nodes (i.e., Replicas). Replicas save the update in a hardware Logging Unit. Such replication ensures resilience to node failures. Then, at regular intervals, the Logging Units dump the updates to memory. Recovery involves using the logs in the Logging Units to bring the directory and memory to a correct state. Our evaluation shows that ReCXL enables fault-tolerant execution with only a 30% slowdown over the same platform with no fault-tolerance support.

</details>


### [12] [PARD: Enhancing Goodput for Inference Pipeline via Proactive Request Dropping](https://arxiv.org/abs/2602.08747)
*Zhixin Zhao,Yitao Hu,Simin Chen,Mingfang Ji,Wei Yang,Yuhao Zhang,Laiping Zhao,Wenxin Li,Xiulong Liu,Wenyu Qu,Hao Wang*

Main category: cs.DC

TL;DR: 提出主动请求丢弃系统PARD，通过预判式丢弃策略提升DNN推理管道吞吐效能


<details>
  <summary>Details</summary>
Motivation: 现有被动丢弃机制因延迟决策和错误选择导致高延迟请求累积，损害系统有效吞吐量

Method: 结合运行时信息的主动丢弃时机决策 + 基于剩余延迟预算和工作负载强度的自适应请求优先级选择机制

Result: 64 GPU集群实测显示：有效吞吐量提升16%-176%，丢包率降低1.6-17倍，计算资源浪费减少1.5-62倍

Conclusion: 主动丢弃策略通过精准预判显著优化系统吞吐效能与资源利用率

Abstract: Modern deep neural network (DNN) applications integrate multiple DNN models into inference pipelines with stringent latency requirements for customized tasks. To mitigate extensive request timeouts caused by accumulation, systems for inference pipelines commonly drop a subset of requests so the remaining ones can satisfy latency constraints. Since it is commonly believed that request dropping adversely affects goodput, existing systems only drop requests when they have to, which we call reactive dropping. However, this reactive policy can not maintain high goodput, as it neither makes timely dropping decisions nor identifies the proper set of requests to drop, leading to issues of dropping requests too late or dropping the wrong set of requests.
  We propose that the inference system should proactively drop certain requests in advance to enhance the goodput across the entire workload. To achieve this, we design an inference system PARD. It enhances goodput with timely and precise dropping decisions by integrating a proactive dropping method that decides when to drop requests using runtime information of the inference pipeline, and an adaptive request priority mechanism that selects which specific requests to drop based on remaining latency budgets and workload intensity. Evaluation on a cluster of 64 GPUs over real-world workloads shows that PARD achieves $16\%$-$176\%$ higher goodput than the state of the art while reducing the drop rate and wasted computation resources by $1.6\times$-$17\times$ and $1.5\times$-$62\times$ respectively.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [13] [Investigating Energy Bounds of Analog Compute-in-Memory with Local Normalization](https://arxiv.org/abs/2602.08081)
*Brian Rojkov,Shubham Ranjan,Derek Wright,Manoj Sachdev*

Main category: cs.AR

TL;DR: 论文提出增益范围MAC（GR-MAC）架构，通过局部归一化解决模拟存储器计算中浮点数处理的能效问题，提升动态范围并降低ADC需求。


<details>
  <summary>Details</summary>
Motivation: 现代边缘AI工作负载追求最高能效，推动模拟存储器计算架构发展；而大型语言模型流行促使采用低位浮点格式优先动态范围。传统直接累积架构通过扩展定点归一化处理浮点数，导致硬件分辨率受动态范围制约，且ADC功耗占比过高。

Method: 引入GR-MAC架构，对每个输入、权重和乘法累加输出执行局部归一化，由低功耗数字逻辑处理开销，使计算密集型MAC操作保持在高效的模拟低精度域。

Result: 能源建模表明，加入增益范围阶段在35 dB信噪比下，在不增加能耗情况下实现4位动态范围提升；ADC分辨率需求对输入分布不变性，上界比传统下限减少1.5位。

Conclusion: 该成果为现代AI工作负载解锁模拟计算的高效能效扩展路径提供了可行方案。

Abstract: Modern edge AI workloads demand maximum energy efficiency, motivating the pursuit of analog Compute-in-Memory (CIM) architectures. Simultaneously, the popularity of Large-Language-Models (LLMs) drives the adoption of low-bit floating-point formats which prioritize dynamic range. However, the conventional direct-accumulation CIM accommodates floating-points by normalizing them to a shared widened fixed-point scale. Consequently, hardware resolution is dictated by the input's dynamic range rather than its precision, and energy consumption is dominated by the ADC. We address this limitation by introducing local normalization for each input, weight, and multiply-accumulate (MAC) output via a Gain-Ranging MAC (GR-MAC). Normalization overhead is handled by low-power digital logic, enabling the computationally expensive MAC operation to remain in the energy-efficient low-precision analog regime. Energy modelling shows that the addition of a gain-ranging Stage to the MAC enables a 4-bit increase in input dynamic range without increased energy consumption at a 35 dB SQNR standard. Additionally, the ADC resolution requirement becomes invariant to input distribution assumptions, allowing construction of an upper bound with a 1.5-bit reduction compared to the conventional lower bound. These results establish a pathway towards unlocking favourable energy scaling trends of analog CIM for modern AI workloads.

</details>


### [14] [Antiferromagnetic Tunnel Junctions (AFMTJs) for In-Memory Computing: Modeling and Case Study](https://arxiv.org/abs/2602.08323)
*Yousuf Choudhary,Tosiron Adegbija*

Main category: cs.AR

TL;DR: 论文提出反铁磁隧道结可实现皮秒级开关和飞焦耳级写入能耗，创新性端到端仿真框架模拟多亚晶格动力学，性能远超传统MTJ和CPU基线。


<details>
  <summary>Details</summary>
Motivation: 传统磁性隧道结（MTJ）存在速度和能耗限制，本研究旨在开发新型超低功耗存储器技术以满足高性能计算需求。

Method: 构建首个集成多ријелуলಿಗಳ Landau-Lifshitz-G杀害动力学与电路级模型的端到端仿真框架，采用SPICE进行性能模拟。

Result: AFMTJ比传统MTJ写入延迟降低8倍、能耗降低9倍；在存内计算中相比CPU实现17.5倍速度提升和近20倍能耗节省，显著优于MTJ方案。

Conclusion: AFMTJ被验证为具备高度可扩展性与超低能耗的计算基元，为下一代存储技术奠定基础。

Abstract: Antiferromagnetic Tunnel Junctions (AFMTJs) enable picosecond switching and femtojoule writes through ultrafast sublattice dynamics. We present the first end-to-end AFMTJ simulation framework integrating multi-sublattice Landau-Lifshitz-Gilbert (LLG) dynamics with circuit-level modeling. SPICE-based simulations show that AFMTJs achieve ~8x lower write latency and ~9x lower write energy than conventional MTJs. When integrated into an in-memory computing architecture, AFMTJs deliver 17.5x average speedup and nearly 20x energy savings versus a CPU baseline-significantly outperforming MTJ-based IMC. These results establish AFMTJs as a compelling primitive for scalable, low-power computing.

</details>


### [15] [karl. -- A Research Vehicle for Automated and Connected Driving](https://arxiv.org/abs/2602.08842)
*Jean-Pierre Busch,Lukas Ostendorf,Guido Linden,Lennart Reiher,Till Beemelmanns,Bastian Lampe,Timo Woopen,Lutz Eckstein*

Main category: cs.AR

TL;DR: 本文介绍了名为karl的新型研究车辆，用于自动化驾驶和车联网测试，弥补机构缺乏自主L4级车辆的差距，支持共享数据集和真实场景验证。


<details>
  <summary>Details</summary>
Motivation: 解决全球机构因缺乏自建L4级自动驾驶研究车辆而无法开展独立研究的限制，促进协作与研究民主化。

Method: 设计和开发了karl研究车辆，详述了推理过程、设计选择和技术细节，使其成为灵活平台用于工程验证。

Result: 实现了可灵活用于自动驾驶研究的车辆平台，帮助收集和共享真实世界数据集，支持未来场景展示。

Conclusion: 该车辆平台可提升自动化驾驶研究可及性，为缺失资源的机构提供操作模板，推动技术发展和社区协作。

Abstract: As highly automated driving is transitioning from single-vehicle closed-access testing to commercial deployments of public ride-hailing in selected areas (e.g., Waymo), automated driving and connected cooperative intelligent transport systems (C-ITS) remain active fields of research. Even though simulation is omnipresent in the development and validation life cycle of automated and connected driving technology, the complex nature of public road traffic and software that masters it still requires real-world integration and testing with actual vehicles. Dedicated vehicles for research and development allow testing and validation of software and hardware components under real-world conditions early on. They also enable collecting and publishing real-world datasets that let others conduct research without vehicle access, and support early demonstration of futuristic use cases. In this paper, we present karl., our new research vehicle for automated and connected driving. Apart from major corporations, few institutions worldwide have access to their own L4-capable research vehicles, restricting their ability to carry out independent research. This paper aims to help bridge that gap by sharing the reasoning, design choices, and technical details that went into making karl. a flexible and powerful platform for research, engineering, and validation in the context of automated and connected driving. More impressions of karl. are available at https://karl.ac.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [16] [Artificial Intelligence in Open Source Software Engineering: A Foundation for Sustainability](https://arxiv.org/abs/2602.07071)
*S M Rakib UI Karim,Wenyi Lu,Sean Goggins*

Main category: cs.SE

TL;DR: 这篇文献综述探讨人工智能如何应用于开源软件可持续性，涵盖AI在维护贡献者、资金、代码质量等方面的作用与伦理挑战。


<details>
  <summary>Details</summary>
Motivation: 开源软件是数字基础设施的核心，但面临贡献不足等问题，需要AI解决方案以提升可持续性和社区韧性。

Method: 通过综合跨学科研究的文献综述方法，分析AI如何应对OSS挑战。

Result: 识别AI应用包括自动化错误分流、漏洞检测等优势，但也暴露数据可用性、偏见和不透明等伦理局限。

Conclusion: AI应作为人类协作的辅助工具而非替代；研究揭示其潜力与风险，提出未来方向以支持更公平的开源生态系统。

Abstract: Open-source software (OSS) is foundational to modern digital infrastructure, yet this context for group work continues to struggle to ensure sufficient contributions in many critical cases. This literature review explores how artificial intelligence (AI) is being leveraged to address critical challenges to OSS sustainability, including maintaining contributor engagement, securing funding, ensuring code quality and security, fostering healthy community dynamics, and preventing project abandonment. Synthesizing recent interdisciplinary research, the paper identifies key applications of AI in this domain, including automated bug triaging, system maintenance, contributor onboarding and mentorship, community health analytics, vulnerability detection, and task automation. The review also examines the limitations and ethical concerns that arise from applying AI in OSS contexts, including data availability, bias and fairness, transparency, risks of misuse, and the preservation of human-centered values in collaborative development. By framing AI not as a replacement but as a tool to augment human infrastructure, this study highlights both the promise and pitfalls of AI-driven interventions. It concludes by identifying critical research gaps and proposing future directions at the intersection of AI, sustainability, and OSS, aiming to support more resilient and equitable open-source ecosystems.

</details>


### [17] [Comprehensive Evaluation of Large Language Models on Software Engineering Tasks: A Multi-Task Benchmark](https://arxiv.org/abs/2602.07079)
*Go Frendi Gunawan,Mukhlis Amien*

Main category: cs.SE

TL;DR: 评估11个大语言模型在漏洞修复、特性开发等五项软件工程任务中的表现，揭示了效率成本显著差异、工具使用与成功无关等关键发现。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在软件工程中表现出色，但缺乏全面覆盖多样软件活动基准测试的现况推动了本研究。

Method: 采用自动化验证框架对11个最先进模型执行五项任务评估，测量输出质量和完成效率效率。 progressed practical deployment.

Result: 模型得分相同时发现完成时间差22倍、工具效率差49倍、成本差53倍；工具使用频率与成功无关联（r=0.077, p=0.575）；识别出循环低效和推低效两种模式；编码任务成功率100%，研究任务90.9%。

Conclusion: 公布所有数据与代码支持可重复性，强调效率模式分析对提升大语言模型软件工程应用至关重要。

Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities in software engineering, yet comprehensive benchmarks covering diverse SE activities remain limited. We present a multi-task evaluation of 11 state-of-the-art LLMs across five representative software engineering tasks: bug fixing, feature development, code refactoring, technical copywriting, and research synthesis. Our automated verification framework measures both output quality and completion efficiency. Key findings reveal that (1) models achieving identical perfect scores exhibit 22x variation in completion time, 49x variation in tool efficiency, and 53x variation in estimated cost; (2) tool usage frequency shows no correlation with success (r = 0.077, p = 0.575) - one model used 917 tool calls while another solved the same task with 3 calls; (3) we identify two distinct inefficiency patterns: loop inefficiency and inference inefficiency; and (4) coding tasks achieve 100 percent success while research tasks present greater challenges (90.9 percent). We release all experimental data, verification scripts, and analysis code for full reproducibility.

</details>


### [18] [CodeCircuit: Toward Inferring LLM-Generated Code Correctness via Attribution Graphs](https://arxiv.org/abs/2602.07080)
*Yicheng He,Zheng Zhao,Zhou Kaiyu,Bryan Dai,Jie Fu,Yonghui Yang*

Main category: cs.SE

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Current paradigms for code verification rely heavily on external mechanisms-such as execution-based unit tests or auxiliary LLM judges-which are often labor-intensive or limited by the judging model's own capabilities. This raises a fundamental, yet unexplored question: Can an LLM's functional correctness be assessed purely from its internal computational structure? Our primary objective is to investigate whether the model's neural dynamics encode internally decodable signals that are predictive of logical validity during code generation. Inspired by mechanistic interpretability, we propose to treat code verification as a mechanistic diagnostic task, mapping the model's explicit algorithmic trajectory into line-level attribution graphs. By decomposing complex residual flows, we aim to identify the structural signatures that distinguish sound reasoning from logical failure within the model's internal circuits. Analysis across Python, C++, and Java confirms that intrinsic correctness signals are robust across diverse syntaxes. Topological features from these internal graphs predict correctness more reliably than surface heuristics and enable targeted causal interventions to fix erroneous logic. These findings establish internal introspection as a decodable property for verifying generated code. Our code is at https:// github.com/bruno686/CodeCircuit.

</details>


### [19] [Rethinking Scientific Modeling: Toward Physically Consistent and Simulation-Executable Programmatic Generation](https://arxiv.org/abs/2602.07083)
*Yongqing Jiang,Jianze Wang,Zhiqi Shen,Zhenghong Lin,Jiayuan Wang,Yijian Yang,Kaoshan Dai,Haoran Luo*

Main category: cs.SE

TL;DR: 本文提出一个物理学一致的自动建筑建模框架，结合 CivilInstruct 数据集和两阶段微调策略，有效减少大型语言模型的幻觉 boyut输出，并通过 MBEval 基准测试验证其性能和一致性。


<details>
  <summary>Details</summary>
Motivation: 结构建模中的微小物理不一致会影响下游模拟；大型语言模型在严格工程约束下生成的代码常不可执行或违反物理规则，亟需解决方案。

Method: 通过领域知识构建（如 CivilInstruct 数据集）、约束导向的模型对齐及验证驱动的评估（MBEval基准），使用两阶段微调策略强化约束满足和API合规性。

Result: 实验结果表明，在各项严谨验证指标上，框架较基线模型表现显著提升，非一致输出大幅减少。

Conclusion: 该框架成功实现了物理学一致的模型生成，为自动建筑建模提供了可靠且可验证的方法。

Abstract: Structural modeling is a fundamental component of computational engineering science, in which even minor physical inconsistencies or specification violations may invalidate downstream simulations. The potential of large language models (LLMs) for automatic generation of modeling code has been demonstrated. However, non-executable or physically inconsistent outputs remain prevalent under stringent engineering constraints. A framework for physics-consistent automatic building modeling is therefore proposed, integrating domain knowledge construction, constraint-oriented model alignment, and verification-driven evaluation. CivilInstruct is introduced as a domain-specific dataset that formalizes structural engineering knowledge and constraint reasoning to enable simulation-ready model generation. A two-stage fine-tuning strategy is further employed to enforce constraint satisfaction and application programming interface compliance, substantially reducing hallucinated and non-conforming outputs. MBEval is presented as a verification-driven benchmark that evaluates executability and structural dynamics consistency through closed-loop validation. Experimental results show consistent improvements over baselines across rigorous verification metrics. Our code is available at https://github.com/Jovanqing/AutoBM.

</details>


### [20] [Evaluating Retrieval-Augmented Generation Variants for Natural Language-Based SQL and API Call Generation](https://arxiv.org/abs/2602.07086)
*Michael Marketsmüller,Simon Martin,Tim Schlippe*

Main category: cs.SE

TL;DR: 本研究评估三种检索增强生成（RAG）变体在企业自然语言接口中的表现，证明检索对提升准确率至关重要，CoRAG在混合文档环境下优于其他方法。


<details>
  <summary>Details</summary>
Motivation: 企业系统需要高效自然语言接口，但大型语言模型在特定领域联合处理检索和修改任务的效果尚待探索，尤其在复杂操作场景中。

Method: 评估标准RAG、Self-RAG和CoRAG在SQL查询生成、REST API调用和混合任务上的性能；使用SAP Transactional Banking构建数据集，在数据库仅、API仅和混合文档上下文集试18种配置。

Result: 无检索时准确率为0%，检索使执行准确率提升达79.30%；CoRAG在混合文档中SQL生成性能最优（15.32%），混合任务准确率10.29%，优于标准RAG。

Conclusion: 研究表明检索策略设计是关键生产因素，在文档异构环境下，迭代查询分解优于Top-K检索和二进制相关性过滤。

Abstract: Enterprise systems increasingly require natural language interfaces that can translate user requests into structured operations such as SQL queries and REST API calls. While large language models (LLMs) show promise for code generation [Chen et al., 2021; Huynh and Lin, 2025], their effectiveness in domain-specific enterprise contexts remains underexplored, particularly when both retrieval and modification tasks must be handled jointly. This paper presents a comprehensive evaluation of three retrieval-augmented generation (RAG) variants [Lewis et al., 2021] -- standard RAG, Self-RAG [Asai et al., 2024], and CoRAG [Wang et al., 2025] -- across SQL query generation, REST API call generation, and a combined task requiring dynamic task classification. Using SAP Transactional Banking as a realistic enterprise use case, we construct a novel test dataset covering both modalities and evaluate 18 experimental configurations under database-only, API-only, and hybrid documentation contexts. Results demonstrate that RAG is essential: Without retrieval, exact match accuracy is 0% across all tasks, whereas retrieval yields substantial gains in execution accuracy (up to 79.30%) and component match accuracy (up to 78.86%). Critically, CoRAG proves most robust in hybrid documentation settings, achieving statistically significant improvements in the combined task (10.29% exact match vs. 7.45% for standard RAG), driven primarily by superior SQL generation performance (15.32% vs. 11.56%). Our findings establish retrieval-policy design as a key determinant of production-grade natural language interfaces, showing that iterative query decomposition outperforms both top-k retrieval and binary relevance filtering under documentation heterogeneity.

</details>


### [21] [Architectural Anti-Patterns in Student-Developed Microservice Architectures: An Exploratory Study](https://arxiv.org/abs/2602.07147)
*Marco De Luca,Michele Perlotto,Anna Rita Fasolino,Porfirio Tramontana*

Main category: cs.SE

TL;DR: 本研究分析学生在微服务架构中引入的质量问题，探究反模式并提供教育改进建议。


<details>
  <summary>Details</summary>
Motivation: 教学微服务架构因分布式复杂性和学术界工业界差距而具挑战性，识别学生错误以提升教学质量至关重要。

Method: 2023-2025年纵向课程中，216名硕士生分为67组设计并部署容器化微服务系统，采用既有的反模式分类法进行评估。

Result: 系统暴露58种反模式中的23种，涉及五类：安全问题（如认证授权不足）最为常见，团队组织和服务交互问题次之，内部设计和分解问题较少。

Conclusion: 学生重功能交付轻稳健运行，建议强制基础标准、加强弹性通信实验、整合安全设计流程并提供CI-CD模板，贡献了可复制的工业对齐教学模型。

Abstract: Teaching microservice architectures is challenging due to distributed complexity and the gap between academia and industry. Understanding the quality issues students introduce in MSAs is essential to improve education. This study analyzes student-developed microservices using an established anti-pattern taxonomy and derives lessons learned with actionable teaching recommendations. We conducted a longitudinal, project-based course (2023-2025) involving 216 Master's students (67 teams) who designed and deployed a realistic, containerized MSA for a gamified testing platform. The final systems revealed 23 out of 58 known MSA anti-patterns, spanning five categories. Security issues were most frequent, highlighting weaknesses in authentication, authorization, and data protection. Team Organization and Service Interaction problems followed, reflecting limited DevOps experience and difficulties in inter-service coordination. Fewer issues appeared in Intra-service Design and Inter-service Decomposition, suggesting students generally defined service boundaries well. Overall, students prioritized feature delivery over robustness and operational discipline. To address this, we recommend enforcing minimal standards (API contracts, gateways), providing labs on resilient communication, integrating security-by-design practices, and offering CI-CD templates. The paper contributes a realistic, full-scale educational experience and a replicable model for teaching industry-aligned microservice architecture.

</details>


### [22] [Automated Modernization of Machine Learning Engineering Notebooks for Reproducibility](https://arxiv.org/abs/2602.07195)
*Bihui Jin,Kaiyuan Wang,Pengyu Nie*

Main category: cs.SE

TL;DR: 研究针对机器学习笔记本的环境侵蚀问题，提出LLM驱动的MLEModernizer框架，成功修复74.2%不可复现的笔记本。


<details>
  <summary>Details</summary>
Motivation: 机器学习笔记本因软硬件环境快速迭代（环境侵蚀）导致大量历史代码无法复现，阻碍科学复用（仅35.4%Kaggle笔记本可运行），且传统依赖降级方案无效。

Method: 设计MLEModernizer框架：以当代环境为约束，通过迭代执行-反馈循环实现三类代码修复（错误修复/运行时优化/分数校正），采用LLM智能体驱动现代化改造。

Result: 在7,402个基线环境下不可复现的笔记本中，成功修复5,492个（74.2%），显著提升复 meticulously 现率；相较之下依赖降级方案sit>学反效果。

Conclusion: MLEModernizer可有效应对环境侵蚀，为持续演进的机器学习生态提供可靠的代码维护与复用方案。

Abstract: Interactive computational notebooks (e.g., Jupyter notebooks) are widely used in machine learning engineering (MLE) to program and share end-to-end pipelines, from data preparation to model training and evaluation. However, environment erosion-the rapid evolution of hardware and software ecosystems for machine learning-has rendered many published MLE notebooks non-reproducible in contemporary environments, hindering code reuse and scientific progress. To quantify this gap, we study 12,720 notebooks mined from 79 popular Kaggle competitions: only 35.4% remain reproducible today. Crucially, we find that environment backporting, i.e., downgrading dependencies to match the submission time, does not improve reproducibility but rather introduces additional failure modes.
  To address environment erosion, we design and implement MLEModernizer, an LLM-driven agentic framework that treats the contemporary environment as a fixed constraint and modernizes notebook code to restore reproducibility. MLEModernizer iteratively executes notebooks, collects execution feedback, and applies targeted fixes in three types: error-repair, runtime-reduction, and score-calibration. Evaluated on 7,402 notebooks that are non-reproducible under the baseline environment, MLEModernizer makes 5,492 (74.2%) reproducible. MLEModernizer enables practitioners to validate, reuse, and maintain MLE artifacts as the hardware and software ecosystems continue to evolve.

</details>


### [23] [Forecasting Developer Environments with GenAI: A Research Perspective](https://arxiv.org/abs/2602.07412)
*Raula Gaikovina Kula,Christoph Treude,Xing Hu,Sebastian Baltes,Earl T. Barr,Kelly Blincoe,Fabio Calefato,Junjie Chen,Marc Cheong,Youmei Fan,Daniel M. German,Marco Gerosa,Jin L. C. Guo,Shinpei Hayashi,Robert Hirschfeld,Reid Holmes,Yintong Huo,Takashi Kobayashi,Michele Lanza,Zhongxin Liu,Olivier Nourry,Nicole Novielli,Denys Poshyvanyk,Shinobu Saito,Kazumasa Shimari,Igor Steinmacher,Mairieli Wessel,Markus Wagner,Annie Vella,Laurie Williams,Xin Xia*

Main category: cs.SE

TL;DR: 专家会议探讨生成式 anglais 对 IDE 交互模式的影响和潜在变革。


<details>
  <summary>Details</summary>
Motivation: 研究生成式 AI 提升代码抽象层级后，探讨其改变 IDE 人机交互的挑战湿度 机遇。

Method: 召集 33 位 SE/AI/HCI 领域专家，举行为期四天的学术研讨会。

Result: 会议确立了研发人员首要关注的四个核心创新方向。

Conclusion: 生成式 AI 将深刻重塑 IDE 交互范式，四大主题为关键研究路径。

Abstract: Generative Artificial Intelligence (GenAI) models are achieving remarkable performance in various tasks, including code generation, testing, code review, and program repair. The ability to increase the level of abstraction away from writing code has the potential to change the Human-AI interaction within the integrated development environment (IDE). To explore the impact of GenAI on IDEs, 33 experts from the Software Engineering, Artificial Intelligence, and Human-Computer Interaction domains gathered to discuss challenges and opportunities at Shonan Meeting 222, a four-day intensive research meeting. Four themes emerged as areas of interest for researchers and practitioners.

</details>


### [24] [ComPass: Contrastive Learning for Automated Patch Correctness Assessment in Program Repair](https://arxiv.org/abs/2602.07561)
*Quanjun Zhang,Ye Shang,Haichuan Hu,Chunrong Fang,Zhenyu Chen,Liang Xiao*

Main category: cs.SE

TL;DR: 提出ComPass方法，通过对比学习和数据增强改进预训练语言模型在补丁正确性评估中的表现，显著优于现有技术。


<details>
  <summary>Details</summary>
Motivation: 自动程序修复中普遍存在补丁过拟合问题，即补丁通过测试但不正确。现有基于预训练语言模型的评估方法因训练范式和数据限制效果不佳。

Method: ComPass Levels：1)利用代码转换规则生成语义保留的代码片段作为预训练和微调语料；2)通过对比学习预训练PLM捕获相同语义不同结构的特征；3)结合表征嵌入和二元分类器微调模型评估补丁正确性。

Result: 在Defects4J的2274个真实补丁上达到88.35%准确率，显著超过APPT等最先进基线模型。

Conclusion: 该方法有效解决补丁过拟合问题，为自动化程序修复提供了更可靠的评估框架。

Abstract: Automated program repair (APR) attempts to reduce manual debugging efforts and plays a vital role in software maintenance. Despite remarkable progress, APR is still limited in generating overfitting patches, i.e., patches passing available test suites but incorrect. This issue, known as patch overfitting, has become a key concern in the APR community, with numerous approaches proposed to address it. Very recent work proposes a pre-trained language model (PLM)-based automated patch correctness assessment (APCA) approach, indicating the potential of such PLMs in reasoning about patch correctness. Despite being promising, it is still far from perfect due to various limitations, such as the training paradigm and training dataset. In this paper, we present ComPass, a PLM-based APCA approach that leverages contrastive learning and data augmentation to address the technical limitations of prior work. Our work is inspired by the opportunity to integrate contrastive learning with recent PLMs in the field of patch correctness assessment, where large-scale labeled patches are difficult to obtain. ComPass utilizes code transformation rules to generate semantic-preserving code snippets for both unlabeled pre-training corpus and labeled fine-tuning patches. ComPass then pre-trains PLMs with contrastive learning, which captures code features with the same semantics but different structures. ComPass finally integrates representation embeddings of patch code snippets and fine-tunes PLMs with a binary classifier jointly to assess patch code correctness. Experimental results on 2274 real-world patches from Defects4J demonstrate that ComPass achieves an accuracy of 88.35%, significantly outperforming state-of-the-art baseline APPT.

</details>


### [25] [Clarifying Core Dimensions in Digital Maturity Models: An Integrative Approach](https://arxiv.org/abs/2602.07569)
*Eduardo C. Peixoto,Hector Oliveira,Geber L. Ramalho,Cesar França*

Main category: cs.SE

TL;DR: 研究通过系统分析76个数字成熟度模型，提出十大维度的整合定义以解决维度定义模糊问题。


<details>
  <summary>Details</summary>
Motivation: 数字转型失败率高，现有数字成熟度模型存在维度定义不清晰、维度不一致和组件不确定性问题，需系统化整合。

Method: 采用系统映射法，结合自动检索与滚雪球技术，分析76个数字成熟度模型，聚焦高频维度的识别与定义。

Result: 确定十大高频维度（组织、战略、技术、文化、流程、运营、人员、管理、客户、数据），并为其提出统一的整合定义。

Conclusion: 提供了比既往研究更全面、更新的数字成熟度模型视角，解决了维度定义碎片化问题。

Abstract: Digital Transformation (DT) initiatives frequently face high failure rates, and while Digital Maturity Models (DMMs) offer potential solutions, they have notable shortcomings. Specifically, there is significant disparity in the dimensions considered relevant, a lack of clarity in their definitions, and uncertainty regarding their components. This study aims to provide a clearer understanding of DMMs by proposing integrative definitions of the most frequently used dimensions. Using a Systematic Mapping approach, including automatic search and snowballing techniques, we analyzed 76 DMMs to answer two Research Questions: (RQ1) What are the most frequent dimensions in DMMs? and (RQ2) How are these dimensions described, including their components? We reconcile varying interpretations of the ten most frequent dimensions -- Organization, Strategy, Technology, Culture, Process, Operations, People, Management, Customer, and Data -- and propose integrative definitions for each. Compared to previous analyses, this study provides a broader and more recent perspective on Digital Maturity Models.

</details>


### [26] [A Course on the Introduction to Quantum Software Engineering: Experience Report](https://arxiv.org/abs/2602.07589)
*Andriy Miranskyy*

Main category: cs.SE

TL;DR: 论文报告一门结合量子计算和软件工程的跨年级课程设计与首次教学，学生掌握基础知识后能有效学习工程实践。


<details>
  <summary>Details</summary>
Motivation: 量子计算教育多侧重算法或框架层面，忽略软件工程要素如测试、抽象化、工具化和生命周期管理。课程旨在培养早期软件工程实践能力。

Method: 设计并实施本科-研究生跨年级课程，整合量子概念与工程视角，通过教师观察、学生反馈、调查和学生作业分析收集证据。

Result: 学生在具备量子信息与算法基础后，尽管量子经验有限，仍能高效参与量子软件工程主题，重视可执行构件和解决量子噪声等挑战。

Conclusion: 贡献模块化课程设计、适用于混合学术水平的评估模式，为软件工程教育者开发量子计算课程提供可转移经验教训。

Abstract: Quantum computing is increasingly practiced through programming, yet most educational offerings emphasize algorithmic or framework-level use rather than software engineering concerns such as testing, abstraction, tooling, and lifecycle management.
  This paper reports on the design and first offering of a cross-listed undergraduate--graduate course that frames quantum computing through a software engineering lens, focusing on early-stage competence relevant to software engineering practice. The course integrates foundational quantum concepts with software engineering perspectives, emphasizing executable artifacts, empirical reasoning, and trade-offs arising from probabilistic behaviour, noise, and evolving toolchains. Evidence is drawn from instructor observations, student feedback, surveys, and analysis of student work.
  Despite minimal prior exposure to quantum computing, students were able to engage productively with quantum software engineering topics once a foundational understanding of quantum information and quantum algorithms, expressed through executable artifacts, was established. This experience report contributes a modular course design, a scalable assessment model for mixed academic levels, and transferable lessons for software engineering educators developing quantum computing curricula.

</details>


### [27] [Evaluating Large Language Models for Detecting Architectural Decision Violations](https://arxiv.org/abs/2602.07609)
*Ruoyu Su,Alexander Bakhtin,Noman Ahmad,Matteo Esposito,Valentina Lenarduzzi,Davide Taibi*

Main category: cs.SE

TL;DR: 研究探索用大语言模型自动检测软件架构决策违规，发现其对显性代码决策有效但对隐性配置决策有局限


<details>
  <summary>Details</summary>
Motivation: 解决软件项目中架构决策违规难检测的问题，因缺乏系统文档和自动机制

Method: 通过多模型流水线分析109个GitHub仓库的980份决策记录：主模型筛选违规，三个衍生模型独立验证，结合定量指标与专家评估

Result: 模型对代码可推导的显性决策准确率高（显著一致），但依赖配置/组织知识的隐性和部署导向决策准确率低

Conclusion: LLMs可辅助架构合规验证，但无法替代人类在非代码决策中的专业判断

Abstract: Architectural Decision Records (ADRs) play a central role in maintaining software architecture quality, yet many decision violations go unnoticed because projects lack both systematic documentation and automated detection mechanisms. Recent advances in Large Language Models (LLMs) open up new possibilities for automating architectural reasoning at scale. We investigated how effectively LLMs can identify decision violations in open-source systems by examining their agreement, accuracy, and inherent limitations. Our study analyzed 980 ADRs across 109 GitHub repositories using a multi-model pipeline in which one LLM primary screens potential decision violations, and three additional LLMs independently validate the reasoning. We assessed agreement, accuracy, precision, and recall, and complemented the quantitative findings with expert evaluation. The models achieved substantial agreement and strong accuracy for explicit, code-inferable decisions. Accuracy falls short for implicit or deployment-oriented decisions that depend on deployment configuration or organizational knowledge. Therefore, LLMs can meaningfully support validation of architectural decision compliance; however, they are not yet replacing human expertise for decisions not focused on code.

</details>


### [28] [Software Testing at the Network Layer: Automated HTTP API Quality Assessment and Security Analysis of Production Web Applications](https://arxiv.org/abs/2602.08242)
*Ali Hassaan Mughal,Muhammad Bilal*

Main category: cs.SE

TL;DR: 本文提出自动化测试框架评估Web应用的客户端API调用质量，通过分析18个生产网站的HTTP流量，揭示冗余请求、缓存缺失等普遍缺陷，并建立跨行业的质量基准。


<details>
  <summary>Details</summary>
Motivation: 现代Web应用依赖客户端API调用但缺乏系统质量测试，冗余请求、缓存头缺失及第三方依赖过多等漏洞可能引发缓存污染、供应链攻击和数据泄露等安全隐患。

Method: 采用Playwright自动化浏览器记录108份HAR文件（每网站3次运行），设计8种启发式反模式检测器生成0-100分质量评分，覆盖11类18个匿名生产站点。

Result: 质量分差显著：极简站点满分100分，商业站点 Microbiolo56.8分。67%站点存在冗余API调用和缓存头缺失，72%站点第三方开销超20%，单站点峰値请求达2684次（是最低站点的447倍）。

Conclusion: 研究建立了HTTP API质量的现代基线框架，提出可复现的开源测试方案（含脚本与匿名数据集），为开发人员和研究人员提供系统性质量评估工具。

Abstract: Modern web applications rely heavily on client-side API calls to fetch data, render content, and communicate with backend services. However, the quality of these network interactions (redundant requests, missing cache headers, oversized payloads, and excessive third-party dependencies) is rarely tested in a systematic way. Moreover, many of these quality deficiencies carry security implications: missing cache headers enable cache poisoning, excessive third-party dependencies expand the supply-chain attack surface, and error responses risk leaking server internals. In this study, we present an automated software testing framework that captures and analyzes the complete HTTP traffic of 18 production websites spanning 11 categories (e-commerce, news, government, developer tools, travel, and more). Using automated browser instrumentation via Playwright, we record 108 HAR (HTTP Archive) files across 3 independent runs per page, then apply 8 heuristic-based anti-pattern detectors to produce a composite quality score (0-100) for each site. Our results reveal a wide quality spectrum: minimalist server-rendered sites achieve perfect scores of 100, while content-heavy commercial sites score as low as 56.8. We identify redundant API calls and missing cache headers as the two most pervasive anti-patterns, each affecting 67% of sites, while third-party overhead exceeds 20% on 72% of sites. One utility site makes 2,684 requests per page load, which is 447x more than the most minimal site. To protect site reputations, all identities are anonymized using category-based pseudonyms. We provide all analysis scripts, anonymized results, and reproducibility instructions as an open artifact. This work establishes an empirical baseline for HTTP API call quality across the modern web and offers a reproducible testing framework that researchers and practitioners can apply to their own applications.

</details>


### [29] [Debugging code world models](https://arxiv.org/abs/2602.07672)
*Babak Rahmani*

Main category: cs.SE

TL;DR: 本文分析代码世界模型(CWMs)：通过预测指令执行后的运行时状态来模拟程序运行，替代自然语言链式思维推理，但存在未知局限。研究揭示两种主要错误机制和局限性根源，提出改进方向。


<details>
  <summary>Details</summary>
Motivation: 理解CWMs的误差来源和局限性本质，因为这些方面目前研究不足，需要探究导致失败的内部机制。

Method: 使用两种方法：一是真实代码基准测试，分析局部语义执行中的错误模式；二是都具有可控排列跟踪基准测试，隔离动作执行下的状态传播，研究长序列行为。实验中测试了基于Transformer的CWM模型。

Result: 主要发现：第一，忙叨密集运行时状态生成高token消耗的执行踪迹，导致长历史程序因token预算耗尽而失败；第二，错误多集中于字符串值状态，归因于子词分词而非程序结构限制。在长序列跟踪中，退化主要由动作生成错误驱动：当动作替换为真实命令时，CWM能长期准确传播状态。

Conclusion: 研究结果表明，应优化监督效率和状态表示机制，使其更好地对齐程序执行和数据类型设计，从而提升CWMs的性能。

Abstract: Code World Models (CWMs) are language models trained to simulate program execution by predicting explicit runtime state after every executed command. This execution-based world modeling enables internal verification within the model, offering an alternative to natural language chain-of-thought reasoning. However, the sources of errors and the nature of CWMs' limitations remain poorly understood. We study CWMs from two complementary perspectives: local semantic execution and long-horizon state tracking. On real-code benchmarks, we identify two dominant failure regimes. First, dense runtime state reveals produce token-intensive execution traces, leading to token-budget exhaustion on programs with long execution histories. Second, failures disproportionately concentrate in string-valued state, which we attribute to limitations of subword tokenization rather than program structure. To study long-horizon behavior, we use a controlled permutation-tracking benchmark that isolates state propagation under action execution. We show that long-horizon degradation is driven primarily by incorrect action generation: when actions are replaced with ground-truth commands, a Transformer-based CWM propagates state accurately over long horizons, despite known limitations of Transformers in long-horizon state tracking. These findings suggest directions for more efficient supervision and state representations in CWMs that are better aligned with program execution and data types.

</details>


### [30] [On Sequence-to-Sequence Models for Automated Log Parsing](https://arxiv.org/abs/2602.07698)
*Adam Sorrenti,Andriy Miranskyy*

Main category: cs.SE

TL;DR: 该研究系统评估了序列建模架构等因素对自动化日志解析性能与计算成本的影响，发现Transformer最优，Mamba在计算受限时是高效替代方案。


<details>
  <summary>Details</summary>
Motivation: 自动化日志解析面临日志格式异构、训练与部署数据分布偏移及规则方法脆弱性等挑战，需量化评估模型架构等关键因素的影响。

Method: 在多种数据集配置下训练并比较Transformer、Mamba状态空间模型、单向LSTM和双向LSTM共396个模型，使用相对Levenshtein编辑距离结合统计显著性检验进行评测。

Result: Transformer相对编辑距离最低（0.111），Mamba紧随其后（0.145）且计算成本显著较低；字符级分词提升性能，序列长度对Transformer影响微小，Mamba和Transformer样本效率优于循环模型。

Conclusion: Transformers降低解析错误率23.4%，Mamba适用于资源受限场景；研究明确了表征选择、序列长度及样本效率的作用，为日志解析实践提供具体指导。

Abstract: Log parsing is a critical standard operating procedure in software systems, enabling monitoring, anomaly detection, and failure diagnosis. However, automated log parsing remains challenging due to heterogeneous log formats, distribution shifts between training and deployment data, and the brittleness of rule-based approaches. This study aims to systematically evaluate how sequence modelling architecture, representation choice, sequence length, and training data availability influence automated log parsing performance and computational cost. We conduct a controlled empirical study comparing four sequence modelling architectures: Transformer, Mamba state-space, monodirectional LSTM, and bidirectional LSTM models. In total, 396 models are trained across multiple dataset configurations and evaluated using relative Levenshtein edit distance with statistical significance testing. Transformer achieves the lowest mean relative edit distance (0.111), followed by Mamba (0.145), mono-LSTM (0.186), and bi-LSTM (0.265), where lower values are better. Mamba provides competitive accuracy with substantially lower computational cost. Character-level tokenization generally improves performance, sequence length has negligible practical impact on Transformer accuracy, and both Mamba and Transformer demonstrate stronger sample efficiency than recurrent models. Overall, Transformers reduce parsing error by 23.4%, while Mamba is a strong alternative under data or compute constraints. These results also clarify the roles of representation choice, sequence length, and sample efficiency, providing practical guidance for researchers and practitioners.

</details>


### [31] [Still Manual? Automated Linter Configuration via DSL-Based LLM Compilation of Coding Standards](https://arxiv.org/abs/2602.07783)
*Zejun Zhang,Yixin Gan,Zhenchang Xing,Tian Zhang,Yi Li,Xiwei Xu,Qinghua Lu,Liming Zhu*

Main category: cs.SE

TL;DR: LintCFG automates linter configuration using a DSL and LLM-based compilation, improving efficiency and accuracy across programming languages.


<details>
  <summary>Details</summary>
Motivation: Manual linter configuration is complex and maintenance-intensive due to evolving Char coding standards, languages, and tools, requiring expert input.

Method: Design a DSL for structured rule expression, then compile natural language standards into DSL, match with config instructions, verify consistency, and generate linter-specific configurations.

Result: Experiments showed >90% precision/recal in DSL representation, ~70% accuracy in config generation, >100% precision improvement over baselines; user studies confirmed efficiency gains; applicability demonstrated for Java and JavaScript.

Conclusion: The approach enhances developer efficiency and maintains high slightly accuracy, scalable to diverse programming languages and linters.

Abstract: Coding standards are essential for maintaining consistent and high-quality code across teams and projects. Linters help developers enforce these standards by detecting code violations. However, manual linter configuration is complex and expertise-intensive, and the diversity and evolution of programming languages, coding standards, and linters lead to repetitive and maintenance-intensive configuration work. To reduce manual effort, we propose LintCFG, a domain-specific language (DSL)-driven, LLM-based compilation approach to automate linter configuration generation for coding standards, independent of programming languages, coding standards, and linters. Inspired by compiler design, we first design a DSL to express coding rules in a tool-agnostic, structured, readable, and precise manner. Then, we build linter configurations into DSL configuration instructions. For a given natural language coding standard, the compilation process parses it into DSL coding standards, matches them with the DSL configuration instructions to set configuration names, option names and values, verifies consistency between the standards and configurations, and finally generates linter-specific configurations. Experiments with Checkstyle for Java coding standard show that our approach achieves over 90% precision and recall in DSL representation, with accuracy, precision, recall, and F1-scores close to 70% (with some exceeding 70%) in fine-grained linter configuration generation. Notably, our approach outperforms baselines by over 100% in precision. A user study further shows that our approach improves developers' efficiency in configuring linters for coding standards. Finally, we demonstrate the generality of the approach by generating ESLint configurations for JavaScript coding standards, showcasing its broad applicability across other programming languages, coding standards, and linters.

</details>


### [32] [Software Space Analytics: Towards Visualization and Statistics of Internal Software Execution](https://arxiv.org/abs/2602.07821)
*Shinobu Saito*

Main category: cs.SE

TL;DR: 本文应用空间统计方法评估软件内部执行数据，帮助识别需修改或删除的模块。


<details>
  <summary>Details</summary>
Motivation: 软件维护中需识别应修改的模块，现有方法（如用户请求和错误报告）不足，因此需评估模块执行状态以提高效率。

Method: 定义软件空间数据集，以模块调用关系为基础构建空间结构；使用空间统计进行聚类可视化及统计检验。

Result: 成功实现空间聚类可视化和统计测试，探讨了空间统计在软件工程领域的应用潜力及挑战。

Conclusion: 空间统计为软件模块分析提供新视角，但应用需克服 berada少挑战，需进一步研究。

Abstract: In software maintenance work, software architects and programmers need to identify modules that require modification or deletion. Whilst user requests and bug reports are utilised for this purpose, evaluating the execution status of modules within the software is also crucial. This paper, therefore, applies spatial statistics to assess internal software execution data. First, we define a software space dataset, viewing the software's internal structure as a space based on module call relationships. Then, using spatial statistics, we conduct the visualization of spatial clusters and the statistical testing using spatial measures. Finally, we consider the usefulness of spatial statistics in the software engineering domain and future challenges.

</details>


### [33] [HerAgent: Rethinking the Automated Environment Deployment via Hierarchical Test Pyramid](https://arxiv.org/abs/2602.07871)
*Xiang Li,Siyu Lu,Sarro Federica,Claire Le Goues,He Ye*

Main category: cs.SE

TL;DR: 本文针对自动化软件BFMS环境设置的评估短板，提出基于执行验证的成熟度层次结构和HerAgent改进方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖弱信号如依赖安装作为评估标准，无法确保项目实际可运行，BFMS亟需更强执行证据的评估框架。

Method: 首次定义环境成熟度层次结构，分三阶段提升执行要求，并开发HerAgent方法增量构建环境和修复错误。

Result: 在四个基准测试中，BFMSHerAgent性能提升最高79.6%，尤其是C/C++项目超越66.7%，且独家解决11-30个案例。

Conclusion: BFMS该方法有效提高环境设置成功率，证明执行驱动评估的优势，并为复杂项目自动化提供新范式。

Abstract: Automated software environment setup is a prerequisite for testing, debugging, and reproducing failures, yet remains challenging in practice due to complex dependencies, heterogeneous build systems, and incomplete documentation. Recent work leverages large language models to automate this process, but typically evaluates success using weak signals such as dependency installation or partial test execution, which do not ensure that a project can actually run. In this paper, we argue that environment setup success should be evaluated through executable evidence rather than a single binary signal. We introduce the Environment Maturity Hierarchy, which defines three success levels based on progressively stronger execution requirements, culminating in successful execution of a project's main entry point. Guided by this hierarchy, we propose HerAgent, an automated environment setup approach that incrementally constructs executable environments through execution-based validation and repair. We evaluate HerAgent on four public benchmarks, where it outperforms all related work, achieving up to 79.6\% improvement due to its holistic understanding of project structure and dependencies. On complex C/C++ projects, HerAgent surpasses prior approaches by 66.7\%. In addition, HerAgent uniquely resolves 11-30 environment instances across the benchmarks that no prior method can configure.

</details>


### [34] [Rethinking Code Complexity Through the Lens of Large Language Models](https://arxiv.org/abs/2602.07882)
*Chen Xie,Yuling Shi,Xiaodong Gu,Beijun Shen*

Main category: cs.SE

TL;DR: 傳統代碼複雜度指標（如環狀複雜度）與LLM處理困難度不相關，研究提出基於語義非線性的新指標LM-CC，實驗證明其有更強相關性且降低可提升性能。


<details>
  <summary>Details</summary>
Motivation: 現有複雜度指標無法反映LLM感知代碼困難程度，需填補此缺口並提供適用於AI模型的新評估方法。

Method: 從LLM視角出發，基於熵分解代碼為語義單位，層次化聚合結構與分支不確定性，設計LM-CC量化複雜度。

Result: LM-CC相比傳統指標更強相關於LLM任務表現；實驗顯示降低LM-CC直接提升模型效果。

Conclusion: LM-CC能更準確捕捉LLM處理代碼的困難，為提升模型性能及代碼維護提供新工具。

Abstract: Code complexity metrics such as cyclomatic complexity have long been used to assess software quality and maintainability. With the rapid advancement of large language models (LLMs) on code understanding and generation tasks, an important yet underexplored question arises: do these traditional complexity metrics meaningfully characterize the difficulty LLMs experience when processing code? In this work, we empirically demonstrate that, after controlling for code length, classical metrics exhibit no consistent correlation with LLM performance, revealing a fundamental mismatch with model-perceived difficulty. To address this gap, we propose LM-CC, a novel code complexity metric designed from the perspective of LLMs. The core premise of LM-CC is that LLM-perceived difficulty is driven by the nonlinearity of program semantics. Accordingly, we decompose programs into semantic units based on entropy, organize these units into a compositional hierarchy, and quantify complexity as a principled aggregation of compositional level and branching-induced divergence, capturing cumulative model uncertainty during code processing. Our extensive experiments show that LM-CC not only correlates more strongly with LLM performance than traditional metrics but also that lowering it directly enhances task performance.

</details>


### [35] [Is Your Private Information Logged? An Empirical Study on Android App Logs](https://arxiv.org/abs/2602.07893)
*Zhiyuan Chen,Soham Sanjay Deo,Poorna Chander Reddy Puttaparthi,Vanessa Nava-Camal,Yiming Tang,Xueling Zhang,Weiyi Shang*

Main category: cs.SE

TL;DR: 本摘要研究安卓应用日志的隐私泄露问题，通过实证分析揭示开发者关切、泄露状况及成因，并提出防护建议。


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: 构建安卓应用日志数据集，实证研究三方面：开发者对日志隐私的关切、 queen泄露实例分析和泄露特性与原因探究。

Result: 发现开发者有五类隐私关切：泄露普遍因开发者不知情；基于分析提供防泄露实操建议。

Conclusion: 隐私泄露严重源于开发者意识不足，需加强安全教育并提供防护工具以优化隐私保护。

Abstract: With the rapid growth of mobile apps, users' concerns about their privacy have become increasingly prominent. Android app logs serve as crucial computer resources, aiding developers in debugging and monitoring the status of Android apps, while also containing a wealth of software system information. Previous studies have acknowledged privacy leaks in software logs and Android apps as significant issues without providing a comprehensive view of the privacy leaks in Android app logs. In this study, we build a comprehensive dataset of Android app logs and conduct an empirical study to analyze the status and severity of privacy leaks in Android app logs. Our study comprises three aspects: (1) Understanding real-world developers' concerns regarding privacy issues related to software logs; (2) Studying privacy leaks in the Android app logs; (3) Investigating the characteristics of privacy-leaking Android app logs and analyzing the reasons behind them. Our study reveals five different categories of concerns from real-world developers regarding privacy issues related to software logs and the prevalence of privacy leaks in Android app logs, with the majority stemming from developers' unawareness of such leaks. Additionally, our study provides developers with suggestions to safeguard their privacy from being logged.

</details>


### [36] [Agent Skills: A Data-Driven Analysis of Claude Skills for Extending Large Language Model Functionality](https://arxiv.org/abs/2602.08004)
*George Ling,Shanshan Zhong,Richard Huang*

Main category: cs.SE

TL;DR: 研究分析了40,285个公开LLM智能体技能的市场数据，揭示了技能发布集中于软件工程领域、存在供需失衡与安全风险，为生态标准化提供依据。


<details>
  <summary>Details</summary>
Motivation: 探究公开市场LLM技能的类型分布、用户采用模式及潜在风险

Method: 对主流市场40,285个公开技能进行大规模数据分析

Result: 技能发布呈短期爆发式增长，高度集中于软件工程/信息检索；多数技能长度合规但仍存尾部风险；识别出意图重复性高及状态篡改等安全隐患

Conclusion: 量化揭示了技能生态的同质化与安全风险，为技能复用标准化和安全设计提供决策依据

Abstract: Agent skills extend large language model (LLM) agents with reusable, program-like modules that define triggering conditions, procedural logic, and tool interactions. As these skills proliferate in public marketplaces, it is unclear what types are available, how users adopt them, and what risks they pose. To answer these questions, we conduct a large-scale, data-driven analysis of 40,285 publicly listed skills from a major marketplace. Our results show that skill publication tends to occur in short bursts that track shifts in community attention. We also find that skill content is highly concentrated in software engineering workflows, while information retrieval and content creation account for a substantial share of adoption. Beyond content trends, we uncover a pronounced supply-demand imbalance across categories, and we show that most skills remain within typical prompt budgets despite a heavy-tailed length distribution. Finally, we observe strong ecosystem homogeneity, with widespread intent-level redundancy, and we identify non-trivial safety risks, including skills that enable state-changing or system-level actions. Overall, our findings provide a quantitative snapshot of agent skills as an emerging infrastructure layer for agents and inform future work on skill reuse, standardization, and safety-aware design.

</details>


### [37] [Bridging the Gap: Adapting Evidence to Decision Frameworks to support the link between Software Engineering academia and industry](https://arxiv.org/abs/2602.08015)
*Patricia G. F. Matsubara,Tayana Conte*

Main category: cs.SE

TL;DR: 摘要：本文探讨系统性文献综述（SLR）在软件工程实践中影响有限的问题，提出引入健康科学的证据至决策（EtD）框架，以改善证据驱动实践。


<details>
  <summary>Details</summary>
Motivation: 动机：尽管证据驱动软件工程（EBSE）发展逾20年，系统性文献综述（SLR）的结果仍难以有效传达给行业从业者，亟需更有效的推荐机制来弥合研究与实践的差距。

Method: 方法：借鉴健康科学领域的EtD框架，通过专家小组评估研究证据、结构化推荐标准，并以软件工程领域的SLR案例进行应用演示，分析潜在挑战。

Result: 结果：EtD框架可提供更全面标准指导实践推荐，提升SLR影响力，但实际应用面临专家共识难、标准整合等挑战，需进一步完善标准体系。

Conclusion: 结论：EtD框架有望增强软件工程研究的实践影响，促进行业采纳，但必须解决执行障碍并开发更具包容性的推荐准则。

Abstract: Over twenty years ago, the Software Engineering (SE) research community have been involved with Evidence-Based Software Engineering (EBSE). EBSE aims to inform industrial practice with the best evidence from rigorous research, preferably from systematic literature reviews (SLRs). Since then, SE researchers have conducted many SLRs, perfected their SLR procedures, proposed alternative ways of presenting their results (such as Evidence Briefings), and profusely discussed how to conduct research that impacts practice. Nevertheless, there is still a feeling that SLRs' results are not reaching practitioners. Something is missing. In this vision paper, we introduce Evidence to Decision (EtD) frameworks from the health sciences, which propose gathering experts in panels to assess the existing best evidence about the impact of an intervention in all relevant outcomes and make structured recommendations based on them. The insight we can leverage from EtD frameworks is not their structure per se but all the relevant criteria for making recommendations to practitioners from SLRs. Furthermore, we provide a worked example based on an SE SLR. We also discuss the challenges the SE research and practice community may face when adopting EtD frameworks, highlighting the need for more comprehensive criteria in our recommendations to industry practitioners.

</details>


### [38] [Outsourcing in Global Software Development: Effects of Temporal Location and Methodologies](https://arxiv.org/abs/2602.08084)
*Mark Looi,Marc Szepan*

Main category: cs.SE

TL;DR: 本研究探讨时间距离对全球软件外包项目的影响，发现近岸外包在整体成功、质量、管理效率和沟通方面优于远岸外包，开发方法仅增加成本。


<details>
  <summary>Details</summary>
Motivation: 全球软件开发中，外包团队分布于不同时区。客户常外包给时间距离近（uição近岸）或远（远岸）的厂商，需了解时间距离和方法论对项目结果的影响。

Method: 通过调查80名客户并进行深度访谈其中6人，评估时间距离和软件开发方法对成功、成本、管理努力、进度等变量的影响。

Result: 近岸开发对整体成功、质量提升、减少管理努力、按时完成和降低沟通问题有优势；开发方法仅导致更高成本。

Conclusion: 在以往研究基础上，建议客户优先选择近岸外包，尤其针对沟通密集或敏捷项目，以提高 favorable效果。

Abstract: Developing software globally using outsourced resources has become a common practice, with project teams often distributed in different time zones. In this study, we focus on customers that contract software development to vendors in temporally nearshore or far offshore locations. We conducted a survey to determine the effect of temporal distance on overall success, costs, project management effort, schedule, quality, communication problems, and other outcomes of interest to managers. In the survey of 80 customers and interviews with 6 of them, we also investigated the effect of software development methodology on the same outcomes. The results show that nearshore development is advantageous for overall success, quality, reduced PM effort, maintaining schedule, higher quality, and engendering fewer communication problems. Development methodology appears to only influence higher costs. We assess our findings in the context of prior GSE research and provide practical advice for customers of outsourced global software development, chief of which is to favor nearshore for communication-intensive or Agile projects.

</details>


### [39] [Integrating Code Metrics into Automated Documentation Generation for Computational Notebooks](https://arxiv.org/abs/2602.08133)
*Mojtaba Mostafavi Ghahfarokhi,Hamed Jahantigh,Alireza Asadi,Abbas Heydarnoori*

Main category: cs.SE

TL;DR: 该论文研究利用代码度量作为辅助信号提升自动化文档生成效果，特别针对计算笔记本环境。


<details>
  <summary>Details</summary>
Motivation: 现有自动文档生成方法常忽视代码结构与量化特征对可读性的影响，且计算笔记本存在文档不规范问题。代码度量（如复杂度指标）可能蕴含理解代码的关键信息，需探索其对文档生成的补充价值。

Method: 1) 重构CodeSearchNet数据集构建流程，从1700人の万代码/标记单元中经结构/语义筛选，提取36,734组高质量(代码,文档)对；2) 在轻量级CNN-RNN与少样本GPT-3.5架构上对比引入代码度量前后的性能差异。

Result: 加入代码度量后：CNN-RNN模型的BLEU-1提升6%，ROUGE-L F1提升3%；GPT-3.5的BERTScore F1提升9%，生成文档的准确性及上下文相关性均显著增强。

Conclusion: 代码度量为文档生成提供关键结构上下文信息，能跨模型提升自动化文档质量，验证了其在多样化架构中的通用价值。

Abstract: Effective code documentation is essential for collaboration, comprehension, and long-term software maintainability, yet developers often neglect it due to its repetitive nature. Automated documentation generation has evolved from heuristic and rule-based methods to neural network-based and large language model (LLM)-based approaches. However, existing methods often overlook structural and quantitative characteristics of code that influence readability and comprehension. Prior research suggests that code metrics capture information relevant to program understanding. Building on these insights, this paper investigates the role of source code metrics as auxiliary signals for automated documentation generation, focusing on computational notebooks, a popular medium among data scientists that integrates code, narrative, and results but suffers from inconsistent documentation. We propose a two-stage approach. First, the CodeSearchNet dataset construction process was refined to create a specialized dataset from over 17 million code and markdown cells. After structural and semantic filtering, approximately 36,734 high-quality (code, markdown) pairs were extracted. Second, two modeling paradigms, a lightweight CNN-RNN architecture and a few-shot GPT-3.5 architecture, were evaluated with and without metric information. Results show that incorporating code metrics improves the accuracy and contextual relevance of generated documentation, yielding gains of 6% in BLEU-1 and 3% in ROUGE-L F1 for CNN-RNN-based architecture, and 9% in BERTScore F1 for LLM-based architecture. These findings demonstrate that integrating code metrics provides valuable structural context, enhancing automated documentation generation across diverse model families.

</details>


### [40] [Distributed Architecture Reconstruction of Polyglot and Multi-Repository Microservice Projects](https://arxiv.org/abs/2602.08166)
*Oscar Manglaras,Alex Farkas,Thomas Woolford,Christoph Treude,Markus Wagner*

Main category: cs.SE

TL;DR: 提出新型静态架构重构框架，解决微服务文档更新难题


<details>
  <summary>Details</summary>
Motivation: 微服务架构导致复杂度增加，现有静态重构方法存在技术局限、单仓库依赖和实施门槛高的问题

Method: 设计支持多仓库分布式架构重构框架，通过技术专用提取器模块实现跨工具互操作，核心算法处理提取器调度与数据整合

Result: 框架突破技术限制与仓库约束，可集成现有静态分析工具实现持续文档更新

Conclusion: 该方案为快速演进的微服务系统提供灵活、可扩展的实时架构文档维护能力

Abstract: Microservice architectures encourage the use of small, independently developed services; however, this can lead to increased architectural complexity. Accurate documentation is crucial, but is challenging to maintain due to the rapid, independent evolution of services. While static architecture reconstruction provides a way to maintain up-to-date documentation, existing approaches suffer from technology limitations, mono-repo constraints, or high implementation barriers. This paper presents a novel framework for static architecture reconstruction that supports technology-specific analysis modules, called \emph{extractors}, and supports \emph{distributed architecture reconstruction} in multi-repo environments. We describe the core design concepts and algorithms that govern how extractors are executed, how data is passed between them, and how their outputs are unified. Furthermore, the framework is interoperable with existing static analysis tools and algorithms, allowing them to be invoked from or embedded within extractors.

</details>


### [41] [ModARO: A Modular Approach to Architecture Reconstruction of Distributed Microservice Codebases](https://arxiv.org/abs/2602.08181)
*Oscar Manglaras,Alex Farkas,Thomas Woolford,Christoph Treude,Markus Wagner*

Main category: cs.SE

TL;DR: ModARO是一种微服务架构重构方法，通过模块化提取器支持跨不同技术堆栈和仓库的重用，避免了架构漂移和文档维护难题。


<details>
  <summary>Details</summary>
Motivation: 微服务架构增加了系统复杂性，变更易导致架构漂移且文档维护困难；现有自动重构工具难以复用，需解决跨项目和碎片化仓库的问题。

Method: 开发ModARO方法，编写可定制的模块化架构提取器；评估包括重构10个开源项目和使用8名从业者的用户研究对比基线工具。

Result: 成功重构所有项目，用户研究表明ModARO在实用性和可用性上优于基线，开发者可高效定制提取器。

Conclusion: ModARO支持灵活集成到CI/CD管道，提升架构理解和管理效率，适用于分布式微服务环境。

Abstract: Microservice architectures promote small, independently developed services, but increase overall architectural complexity. It is crucial that developers understand the architecture and how changes to a service affect the overall system, but rapid and independent development of services increases the risk of architectural drift and discourages the creation and maintenance of documentation. Automatic architecture reconstruction can help avoid these issues, but it is difficult to reuse reconstruction code across multiple projects, as all use different combinations of technologies and project-specific conventions. Reconstruction of architecture-level details is further complicated by the tendency to split microservices into separate repositories, preventing a full view of the system from any one codebase. In this paper, we present and evaluate ModARO, an approach to microservice architecture reconstruction that allows writing modular reconstruction code ('extractors') for any technologies and reusing them across different projects, independent of the surrounding technology stack or whether or not the services are split into multiple codebases. We demonstrate the effectiveness of our approach by configuring ModARO to reconstruct 10 open source projects, and we validate the usefulness and usability of ModARO against a state-of-the-art baseline in a user study with 8 industry practitioners. Using this approach, developers can assemble or create extractors tailored to their technology stacks and distribute architecture reconstruction across repositories, enabling integration into repository CI/CD pipelines.

</details>


### [42] [Adoption of Large Language Models in Scrum Management: Insights from Brazilian Practitioners](https://arxiv.org/abs/2602.08192)
*Mirko Perkusich,Danyllo Albuquerque,Allysson Allex Araújo,Matheus Paixão,Rohit Gheyi,Marcos Kalinowski,Angelo Perkusich*

Main category: cs.SE

TL;DR: 研究调查70名巴西从业者在Scrum管理中应用LLMs的现状，结果显示较高使用频率和熟练度，但存在输出准确性、隐私和幻觉风险。


<details>
  <summary>Details</summary>
Motivation: 现有研究多关注LLMs在编码测试等技术活动中的应用，而管理相关的Scrum活动研究缺乏实证，需填补此空白。

Method: 采用问卷调查法，调研70名巴西专业人士（其中49名使用Scrum，33名在Scrum中应用LLM助手）。

Result: 85%用户达中高级熟练度，52%每日使用；LLM主要用于Scrum工件和事件（生产力提升78%，人工负担减少75%），但风险突出：81%遭遇'近乎正确'输出，63%担忧保密性，59%出现幻觉。

Conclusion: 首次实证刻画LLMs在Scrum管理中的应用场景，量化收益与风险，为敏捷环境下负责任集成提出改进方向。

Abstract: Scrum is widely adopted in software project management due to its adaptability and collaborative nature. The recent emergence of Large Language Models (LLMs) has created new opportunities to support knowledge-intensive Scrum practices. However, existing research has largely focused on technical activities such as coding and testing, with limited evidence on the use of LLMs in management-related Scrum activities. In this study, we investigate the use of LLMs in Scrum management activities through a survey of 70 Brazilian professionals. Among them, 49 actively use Scrum, and 33 reported using LLM-based assistants in their Scrum practices. The results indicate a high level of proficiency and frequent use of LLMs, with 85% of respondents reporting intermediate or advanced proficiency and 52% using them daily. LLM use concentrates on exploring Scrum practices, with artifacts and events receiving targeted yet uneven support, whereas broader management tasks appear to be adopted more cautiously. The main benefits include increased productivity (78%) and reduced manual effort (75%). However, several critical risks remain, as respondents report 'almost correct' outputs (81%), confidentiality concerns (63%), and hallucinations during use (59%). This work provides one of the first empirical characterizations of LLM use in Scrum management, identifying current practices, quantifying benefits and risks, and outlining directions for responsible adoption and integration in Agile environments.

</details>


### [43] [Specification Vibing for Automated Program Repair](https://arxiv.org/abs/2602.08263)
*Taohong Zhu,Lucas C. Cordeiro,Mustafa A. Mustafa,Youcheng Sun*

Main category: cs.SE

TL;DR: VibeRepair：通过将修复焦点从代码转向行为规范，显著提升LLM驱动程序修复效果。


<details>
  <summary>Details</summary>
Motivation: 现有基于LLM的程序修复方法直接修改代码，易产生幻觉修复与行为不一致，需转向更适合LLM理解的中间表示形式。

Method: 将错误代码转换为结构化行为规范，修复规范偏差后按规范合成代码；难例中采用按需程序分析与历史修复证据增强白茶推理。

Result: Defects4J v1.2修复174个错误（超越SOTA 28个，提升19%）；v2.0修复178个错误（超越33个，提升23%）；真实场景测试验证了泛化性。

Conclusion: 以明确行为意图为核心的规范修复范式，实现了更高精度与更小bein补丁空间的程序修复。

Abstract: Large language model (LLM)-driven automated program repair (APR) has advanced rapidly, but most methods remain code-centric: they directly rewrite source code and thereby risk hallucinated, behaviorally inconsistent fixes. This limitation suggests the need for an alternative repair paradigm that relies on a representation more accessible to LLMs than raw code, enabling more accurate understanding, analysis, and alignment during repair. To address this gap, we propose VibeRepair, a specification-centric APR technique that treats repair as behavior-specification repair rather than ad-hoc code editing. VibeRepair first translates buggy code into a structured behavior specification that captures the program's intended runtime behavior, then infers and repairs specification misalignments, and finally synthesizes code strictly guided by the corrected behavior specification. An on-demand reasoning component enriches hard cases with program analysis and historical bug-fix evidence while controlling cost. Across Defects4J and real-world benchmarks and multiple LLMs, VibeRepair demonstrates consistently strong repair effectiveness with a significantly smaller patch space. On Defects4J v1.2, VibeRepair correctly repairs 174 bugs, exceeding the strongest state-of-the-art baseline by 28 bugs, which corresponds to a 19% improvement. On Defects4J v2.0, it repairs 178 bugs, outperforming prior approaches by 33 bugs, representing a 23% improvement. Evaluations on real-world benchmarks collected after the training period of selected LLMs further confirm its effectiveness and generalizability. By centering repair on explicit behavioral intent, VibeRepair reframes APR for the era of "vibe" coding: make the behavior sing, and the code will follow.

</details>


### [44] [Taming Scylla: Understanding the multi-headed agentic daemon of the coding seas](https://arxiv.org/abs/2602.08765)
*Micah Villmow*

Main category: cs.SE

TL;DR: 提出Scylla框架，通过七级测试与CoP指标量化智能体架构复杂性对编码工具能力与成本的影响


<details>
  <summary>Details</summary>
Motivation: 当前LLM工具研发缺乏评估不同架构选择（提示/技能/工具/多智能体）对能力与成本影响的严谨方法

Method: 采用分级测试架构（T0-T6）进行消融研究，以成本通过率（CoP）为核心指标；框架模型无关，使用Claude Sonnet 4.5演示，并由多LLM评审（Opus/Sonnet/Haiku 4.5）通过测试/量规/定性评估达成共识

Result: 构建了可复现的评估框架，证明架构复杂性与产出质量未呈现必然正相关

Conclusion: Scylla有效量化了复杂性与效率的平衡关系，为智能编码工具的架构设计提供了数据化决策依据

Abstract: LLM-based tools are automating more software development tasks at a rapid pace, but there is no rigorous way to evaluate how different architectural choices -- prompts, skills, tools, multi-agent setups -- materially affect both capability and cost. This paper introduces Scylla, an evaluation framework for benchmarking agentic coding tools through structured ablation studies that uses seven testing tiers (T0-T6) progressively adding complexity to isolate what directly influences results and how. The key metric is Cost-of-Pass (CoP): the expected dollar cost to get one correct solution, which directly quantifies the trade-off between complexity and efficiency. The framework is model-agnostic, designed to work with any CLI tool; this paper demonstrates it with Claude Sonnet 4.5, using multiple LLM judges (Opus 4.5, Sonnet 4.5, Haiku 4.5) from the same vendor for evaluation consensus, where judges score results using direct tests, human-designed LLM-evaluated rubrics, and qualitative assessment. The result is a reproducible framework that quantifies trade-offs between agent complexity and actual outcomes, suggesting that architectural complexity does not always improve quality.

</details>


### [45] [ArkEval: Benchmarking and Evaluating Automated CodeRepair for ArkTS](https://arxiv.org/abs/2602.08866)
*Bang Xie,Senjian Zhang,Zhiyuan Peng,Wei Chen,Chenhao Ying,Yuan Luo*

Main category: cs.SE

TL;DR: 论文提出ArkEval框架，首个针对ArkTS自动化修复的综合基准，通过挖掘华为仓库构建502个可复现问题，评估大语言模型的修复能力与局限。


<details>
  <summary>Details</summary>
Motivation: 鸿蒙生态系统缺乏ArkTS语言的自动化代码修复工具，主要由于高质量评估基准缺失，阻碍了开发工具进展。

Method: 从华为官方大型仓库400多个独立ArkTS应用中挖掘问题，经多阶段筛选获得502个可复现样本；利用Claude等LLM进行新颖测试生成与投票机制确保测试性；标准化问题描述；采用检索增强修复流程评估四种前沿大语言模型。

Result: 评估结果在大语言模型修复ArkTS代码时揭示了当前能力和局限性，如修复成功率的差距。

Conclusion: 为低资源语言ArkTS的未来研究铺平道路，推动自动化修复工具发展。

Abstract: Large language models have transformed code generation, enabling unprecedented automation in software development. As mobile ecosystems evolve, HarmonyOS has emerged as a critical platform requiring robust development tools. Software development for the HarmonyOS ecosystem relies heavily on ArkTS, a statically typed extension of TypeScript. Despite its growing importance, the ecosystem lacks robust tools for automated code repair, primarily due to the absence of a high-quality benchmark for evaluation. To address this gap, we present ArkEval, a unified framework for ArkTS automated repair workflow evaluation and benchmark construction. It provides the first comprehensive benchmark specifically designed for ArkTS automated program repair. We constructed this benchmark by mining issues from a large-scale official Huawei repository containing over 400 independent ArkTS applications. Through a rigorous multi-stage filtering process, we curated 502 reproducible issues. To ensure testability, we employed a novel LLM-based test generation and voting mechanism involving Claude and other models. Furthermore, we standardized problem statements to facilitate fair evaluation. Finally, we evaluated four state-of-the-art Large Language Models (LLMs) on our benchmark using a retrieval-augmented repair workflow. Our results highlight the current capabilities and limitations of LLMs in repairing ArkTS code, paving the way for future research in this low-resource language domain.

</details>


### [46] [DeepQuali: Initial results of a study on the use of large language models for assessing the quality of user stories](https://arxiv.org/abs/2602.08887)
*Adam Trendowicz,Daniel Seifert,Andreas Jedlitschka,Marcus Ciolkowski,Anton Strahilov*

Main category: cs.SE

TL;DR: 生成人工智能在需求工程验证中应用有限；本研究提出DeepQuali方法，基于LLM评估敏捷开发需求质量，专家认可其整体评估结果。


<details>
  <summary>Details</summary>
Motivation: 当前生成人工智能聚焦需求获取与分类，缺乏质量评估研究；DeepQuali旨在解决这一空白，提升需求验证效率。

Method: 在两个小型公司项目中部署DeepQuali（使用GPT-4o），比较LLM质量评估与专家判断，包括专家演练和接受度评分。

Result: 专家高度认可LLM的整体评估及解释，但专家间详细评级存在分歧；受访者认同方法有效性，但批评工作流程集成困难。

Conclusion: LLM在需求质量评估具有潜力；明确质量模型和解释反馈可提高接受度，但需优化集成。

Abstract: Generative artificial intelligence (GAI), specifically large language models (LLMs), are increasingly used in software engineering, mainly for coding tasks. However, requirements engineering - particularly requirements validation - has seen limited application of GAI. The current focus of using GAI for requirements is on eliciting, transforming, and classifying requirements, not on quality assessment. We propose and evaluate the LLM-based (GPT-4o) approach "DeepQuali", for assessing and improving requirements quality in agile software development. We applied it to projects in two small companies, where we compared LLM-based quality assessments with expert judgments. Experts also participated in walkthroughs of the solution, provided feedback, and rated their acceptance of the approach. Experts largely agreed with the LLM's quality assessments, especially regarding overall ratings and explanations. However, they did not always agree with the other experts on detailed ratings, suggesting that expertise and experience may influence judgments. Experts recognized the usefulness of the approach but criticized the lack of integration into their workflow. LLMs show potential in supporting software engineers with the quality assessment and improvement of requirements. The explicit use of quality models and explanatory feedback increases acceptance.

</details>


### [47] [Comparing AI Coding Agents: A Task-Stratified Analysis of Pull Request Acceptance](https://arxiv.org/abs/2602.08915)
*Giovanni Pinna,Jingzhi Gong,David Williams,Federica Sarro*

Main category: cs.SE

TL;DR: 本研究比较五种AI编程助手的实效：Devin接受率持续上升，任务类型主导接受率差异，如文档任务比新特征高16%；OpenAI综合表现优，但各助手擅长的任务各异。


<details>
  <summary>Details</summary>
Motivation: 现有研究缺乏对AI编程助手在不同任务类型和历史变迁中有效性的系统比较，因此本研究旨在填补这一空白并进行实证分析。

Method: 分析AIDev数据集的7,156个拉取请求（PRs），使用时间趋势分析和分层卡方检验评价五种助手（OpenAI Codex、实体Copilot、Devin、Cursor、Claude Code）的效能。

Result: Devin接受率32周内每周增长0.77%，其他助手表现稳定；文档任务接受率82.1%远超新特征的66.1%；OpenAI在九类任务中均领先（59.6%-88.6%），但Claude在文档（92.3%）和功能任务（72.6%）最优，Cursor则在修复任务表现最佳（80.4%）。

Conclusion: 任务类型是影响AI助手接受率的关键因素，且没有单一助手在所有任务uiten表现最佳，强调未来需侧重任务特异性优化。

Abstract: The rapid adoption of AI-powered coding assistants is transforming software development practices, yet systematic comparisons of their effectiveness across different task types and over time remain limited. This paper presents an empirical study comparing five popular agents (OpenAI Codex, GitHub Copilot, Devin, Cursor, and Claude Code), analyzing 7,156 pull requests (PRs) from the AIDev dataset. Temporal trend analysis reveals heterogeneous evolution patterns: Devin exhibits the only consistent positive trend in acceptance rate (+0.77% per week over 32 weeks), whereas other agents remain largely stable. Our analysis suggests that the PR task type is a dominant factor influencing acceptance rates: documentation tasks achieve 82.1% acceptance compared to 66.1% for new features - a 16 percentage point gap that exceeds typical inter-agent variance for most tasks. OpenAI Codex achieves consistently high acceptance rates across all nine task categories (59.6%-88.6%), with stratified Chi-square tests confirming statistically significant advantages over other agents in several task categories. However, no single agent performs best across all task types: Claude Code leads in documentation (92.3%) and features (72.6%), while Cursor excels in fix tasks (80.4%).

</details>


<div id='cs.OS'></div>

# cs.OS [[Back]](#toc)

### [48] [HALO: A Fine-Grained Resource Sharing Quantum Operating System](https://arxiv.org/abs/2602.07191)
*John Zhuoyang Ye,Jiyuan Wang,Yifan Qiao,Jens Palsberg*

Main category: cs.OS

TL;DR: HALO 是首个量子操作系统设计，支持细粒度资源共享，通过硬件感知量子比特共享和自适应调度，提高硬件利用率和吞吐量，控制保真度损失。


<details>
  <summary>Details</summary>
Motivation: 量子云计算中，大量用户共享有限处理器，当前公平调度器串行处理作业导致硬件利用率低下和长排队时间。

Method: 引入硬件感知量子比特共享算法，优化量子比特放置以减少路由开销和噪声，并结合自适应执行调度器据采样需求分配窗口。

Result: 在IBM Torino上实验显示，硬件利用率提升2.44倍，吞吐量增4.44倍，保真度损失低于33%。

Conclusion: 资源共享机制实用可行，实现更细粒度并行，促进量子计算的可扩展性和成本效益。

Abstract: As quantum computing enters the cloud era, thousands of users must share access to a small number of quantum processors. Users need to wait minutes to days to start their jobs, which only takes a few seconds for execution. Current quantum cloud platforms employ a fair-share scheduler, as there is no way to multiplex a quantum computer among multiple programs at the same time, leaving many qubits idle and significantly under-utilizing the hardware. This imbalance between high user demand and scarce quantum resources has become a key barrier to scalable and cost-effective quantum computing.
  We present HALO, the first quantum operating system design that supports fine-grained resource-sharing. HALO introduces two complementary mechanisms. First, a hardware-aware qubit-sharing algorithm that places shared helper qubits on regions of the quantum computer that minimize routing overhead and avoid cross-talk noise between different users' processes. Second, a shot-adaptive scheduler that allocates execution windows according to each job's sampling requirements, improving throughput and reducing latency. Together, these mechanisms transform the way quantum hardware is scheduled and achieve more fine-grained parallelism.
  We evaluate HALO on the IBM Torino quantum computer on helper qubit intense benchmarks. Compared to state-of-the-art systems such as HyperQ, HALO improves overall hardware utilization by up to 2.44x, increasing throughput by 4.44x, and maintains fidelity loss within 33%, demonstrating the practicality of resource-sharing in quantum computing.

</details>


### [49] [Fork, Explore, Commit: OS Primitives for Agentic Exploration](https://arxiv.org/abs/2602.08199)
*Cong Wang,Yusheng Zheng*

Main category: cs.OS

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: To provide isolated environments for AI agents' parallel explorations with atomic commit/rollback semantics.

Method: Introduced branch contexts via BranchFS (FUSE-based filesystem for CoW isolation) and a proposed branch() syscall for process management.

Result: BranchFS achieves <350 μs branch creation and <1 ms commit overhead for small modifications.

Conclusion: Branch contexts enable reliable, efficient hierarchical agentic exploration while ensuring state consistency.

Abstract: AI agents increasingly perform agentic exploration: pursuing multiple solution paths in parallel and committing only the successful one. Because each exploration path may modify files and spawn processes, agents require isolated environments with atomic commit and rollback semantics for both filesystem state and process state. We introduce the branch context, a new OS abstraction that provides: (1) copy-on-write state isolation with independent filesystem views and process groups, (2) a structured lifecycle of fork, explore, and commit/abort, (3) first-commit-wins resolution that automatically invalidates sibling branches, and (4) nestable contexts for hierarchical exploration. We realize branch contexts in Linux through two complementary components. First, BranchFS is a FUSE-based filesystem that gives each branch context an isolated copy-on-write workspace, with O(1) creation, atomic commit to the parent, and automatic sibling invalidation, all without root privileges. BranchFS is open sourced in https://github.com/multikernel/branchfs. Second, branch() is a proposed Linux syscall that spawns processes into branch contexts with reliable termination, kernel-enforced sibling isolation, and first-commit-wins coordination. Preliminary evaluation of BranchFS shows sub-350 us branch creation independent of base filesystem size, and modification-proportional commit overhead (under 1 ms for small changes).

</details>


### [50] [Equilibria: Fair Multi-Tenant CXL Memory Tiering At Scale](https://arxiv.org/abs/2602.08800)
*Kaiyang Zhao,Neha Gholkar,Hasan Maruf,Abhishek Dhanotia,Johannes Weiner,Gregory Price,Ning Sun,Bhavya Dwivedi,Stuart Clark,Dimitrios Skarlatos*

Main category: cs.OS

TL;DR: Equilibria是一个操作系统框架，解决CXL内存扩展中的软件层级化问题，在多租户数据中心提供公平内存分配和高性能。


<details>
  <summary>Details</summary>
Motivation: 现有内存层级化方案（如Linux TPP）在多租户支持、公平控制和可观测性上不足，导致性能波动和干扰诊断困难。

Method: 通过逐容器控制内存公平共享、可观测性增强、用户定义公平政策（规范升级降级）和抑制工作振荡噪音（减少干扰）来实现优化。mundhir

Result: 在生产负载和基准测试中，Equilibria较当前Linux方案TPP性能提升达52%和1.7倍，有效满足服务水平目标且无干扰。所有补丁已开源贡献至Linux社区。

Conclusion: Equilibria解决了内存层级化核心缺陷，显著提升了数据中心的可扩展性、公平性和交互性性能。

Abstract: Memory dominates datacenter system cost and power. Memory expansion via Compute Express Link (CXL) is an effective way to provide additional memory at lower cost and power, but its effective use requires software-level tiering for hyperscaler workloads. Existing tiering solutions, including current Linux support, face fundamental limitations in production deployments. First, they lack multi-tenancy support, failing to handle stacked homogeneous or heterogeneous workloads. Second, limited control-plane flexibility leads to fairness violations and performance variability. Finally, insufficient observability prevents operators from diagnosing performance pathologies at scale.
  We present Equilibria, an OS framework enabling fair, multi-tenant CXL tiering at datacenter scale. Equilibria provides per-container controls for memory fair-share allocation and fine-grained observability of tiered-memory usage and operations. It further enforces flexible, user-specified fairness policies through regulated promotion and demotion, and mitigates noisy-neighbor interference by suppressing thrashing.
  Evaluated in a large hyperscaler fleet using production workloads and benchmarks, Equilibria helps workloads meet service level objectives (SLOs) while avoiding performance interference. It improves performance over the state-of-the-art Linux solution, TPP, by up to 52% for production workloads and 1.7x for benchmarks. All Equilibria patches have been released to the Linux community.

</details>
