{"id": "2512.00398", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2512.00398", "abs": "https://arxiv.org/abs/2512.00398", "authors": ["Bingzheng Xia", "Zujie Ren", "Kuang Ma", "Xiaoqian Li", "Wenda Li", "Shuibing He"], "title": "Heimdall++: Optimizing GPU Utilization and Pipeline Parallelism for Efficient Single-Pulse Detection", "comment": null, "summary": "With the increasing time and frequency resolution of modern radio telescopes and the exponential growth in observational data volumes, real-time single-pulse detection has become a critical requirement for time-domain radio astronomy. Heimdall, as a representative GPU-accelerated single-pulse search tool, offers substantial performance advantages over CPU-based approaches. However, its sequential execution model and resource contention in intermediate processing stages limit GPU utilization, leading to suboptimal throughput and increased computational latency. To address these limitations, we present Heimdall++, an optimized successor to Heimdall that incorporates fine-grained GPU parallelization, enhanced memory management, and a multi-threaded framework to decouple CPU-bound and GPU-bound processing stages. This design mitigates the GPU stall problem and improves end-to-end efficiency. We evaluated Heimdall++ on a system equipped with NVIDIA RTX 3080 Ti GPUs using both a single large-scale observational file and multiple files. Experimental results demonstrate that Heimdall++ achieves up to 2.66x speedup in single-file processing and 2.05x speedup in multi-file batch processing, while maintaining full consistency with the original Heimdall's search results.", "AI": {"tldr": "Heimdall++ is an optimized GPU-accelerated single-pulse detection tool that improves throughput and reduces latency by introducing fine-grained parallelization, better memory management, and a multi-threaded framework, achieving up to 2.66x speedup over the original Heimdall while preserving result accuracy.", "motivation": "Real-time single-pulse detection is essential due to the high data volumes from modern radio telescopes, but existing tools like Heimdall suffer from GPU underutilization caused by sequential execution and resource contention.", "method": "Heimdall++ introduces fine-grained GPU parallelization, enhanced memory management, and a multi-threaded architecture that decouples CPU- and GPU-bound tasks to reduce GPU stalls and improve efficiency.", "result": "Heimdall++ achieves up to 2.66x speedup in single-file processing and 2.05x in multi-file batch processing on an NVIDIA RTX 3080 Ti system, with identical detection results to Heimdall.", "conclusion": "Heimdall++ significantly enhances real-time single-pulse search performance without compromising result fidelity, making it well-suited for next-generation time-domain radio astronomy applications."}}
{"id": "2512.00400", "categories": ["cs.OS", "eess.SY"], "pdf": "https://arxiv.org/pdf/2512.00400", "abs": "https://arxiv.org/abs/2512.00400", "authors": ["Xinkui Zhao", "Yifan Zhang", "Haidan Zhao", "Hao Zhang", "Qingyu Ma", "Lufei Zhang", "Guanjie Cheng", "Shuiguang Deng", "Jianwei Yin", "Zuoning Chen"], "title": "TenonOS: A Self-Generating Intelligent Embedded Operating System Framework for Edge Computing", "comment": null, "summary": "The rapid evolution of edge computing has exposed fundamental limitations in traditional operating system and hypervisor architectures, particularly in managing heterogeneous platforms and meeting the constraints of limited resources. Existing solutions often rely on monolithic or layered combinations of hypervisors and guest OSes, which are difficult to tailor for the diverse and dynamic requirements of edge scenarios. To address these challenges, we propose TenonOS, a demand-driven, self-generating, and lightweight operating system framework that fundamentally rethinks and reconstructs both the hypervisor and OS architectures. TenonOS introduces a novel LibOS-on-LibOS approach, in which both virtualization and OS functionalities are modularized into fine-grained, reusable micro-libraries. A dynamic orchestration engine composes these modules on demand to construct customized, application-specific runtime environments. At the core of TenonOS are two key components: Mortise, a minimal, modularized hypervisor, and Tenon, a real-time LibOS. Mortise provides low-overhead resource isolation, fast inter-VM communication, and manages the full lifecycle of Tenon instances - including on-demand creation, suspension, and termination - enabling TenonOS to flexibly adapt its runtime layout to workload variations. Tenon delivers deterministic scheduling and multi-process support for time-critical applications. Through this unified and modular architecture, TenonOS eliminates redundant layers, reduces system overhead, and enhances scalability, security, and maintainability. Extensive evaluations demonstrate that TenonOS achieves superior real-time scheduling (40.28% improvement), a compact memory footprint (361 KiB), and high adaptability to dynamic edge workloads, making it an ideal foundation for heterogeneous, resource-constrained edge systems.", "AI": {"tldr": "TenonOS \u662f\u4e00\u79cd\u9762\u5411\u8fb9\u7f18\u8ba1\u7b97\u7684\u8f7b\u91cf\u7ea7\u3001\u6309\u9700\u81ea\u751f\u6210\u64cd\u4f5c\u7cfb\u7edf\u6846\u67b6\uff0c\u91c7\u7528 LibOS-on-LibOS \u67b6\u6784\uff0c\u901a\u8fc7\u6a21\u5757\u5316\u5fae\u5e93\u548c\u52a8\u6001\u7f16\u6392\u5f15\u64ce\u6784\u5efa\u5b9a\u5236\u5316\u8fd0\u884c\u65f6\u73af\u5883\uff0c\u5728\u5b9e\u65f6\u6027\u3001\u5185\u5b58\u5360\u7528\u548c\u9002\u5e94\u6027\u65b9\u9762\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6848\u3002", "motivation": "\u4f20\u7edf\u64cd\u4f5c\u7cfb\u7edf\u548c\u865a\u62df\u673a\u76d1\u63a7\u5668\u67b6\u6784\u5728\u5f02\u6784\u8fb9\u7f18\u5e73\u53f0\u4e0a\u96be\u4ee5\u6ee1\u8db3\u8d44\u6e90\u53d7\u9650\u548c\u591a\u6837\u5316\u9700\u6c42\uff0c\u5176\u5355\u4f53\u6216\u5206\u5c42\u8bbe\u8ba1\u7f3a\u4e4f\u7075\u6d3b\u6027\u548c\u53ef\u5b9a\u5236\u6027\u3002", "method": "\u63d0\u51fa TenonOS \u6846\u67b6\uff0c\u5305\u542b\u6a21\u5757\u5316\u5fae\u5e93\u3001\u52a8\u6001\u7f16\u6392\u5f15\u64ce\u3001\u6700\u5c0f\u5316\u865a\u62df\u673a\u76d1\u63a7\u5668 Mortise \u548c\u5b9e\u65f6 LibOS Tenon\uff0c\u5b9e\u73b0\u6309\u9700\u7ec4\u5408\u4e0e\u8fd0\u884c\u65f6\u81ea\u9002\u5e94\u3002", "result": "\u5b9e\u9a8c\u8868\u660e TenonOS \u5728\u5b9e\u65f6\u8c03\u5ea6\u6027\u80fd\u4e0a\u63d0\u5347 40.28%\uff0c\u5185\u5b58\u5360\u7528\u4ec5 361 KiB\uff0c\u5e76\u5177\u5907\u5bf9\u52a8\u6001\u8fb9\u7f18\u5de5\u4f5c\u8d1f\u8f7d\u7684\u9ad8\u9002\u5e94\u6027\u3002", "conclusion": "TenonOS \u901a\u8fc7\u7edf\u4e00\u6a21\u5757\u5316\u67b6\u6784\u6709\u6548\u89e3\u51b3\u4e86\u8fb9\u7f18\u8ba1\u7b97\u4e2d\u7cfb\u7edf\u5197\u4f59\u3001\u5f00\u9500\u5927\u548c\u9002\u5e94\u6027\u5dee\u7684\u95ee\u9898\uff0c\u4e3a\u8d44\u6e90\u53d7\u9650\u7684\u5f02\u6784\u8fb9\u7f18\u7cfb\u7edf\u63d0\u4f9b\u4e86\u7406\u60f3\u57fa\u7840\u3002"}}
{"id": "2512.00006", "categories": ["cs.AR", "cs.CL"], "pdf": "https://arxiv.org/pdf/2512.00006", "abs": "https://arxiv.org/abs/2512.00006", "authors": ["Yuqin Zhao", "Linghui Ye", "Haihang Xia", "Luke Seed", "Tiantai Deng"], "title": "VeriPy - A New Python-Based Approach for SDR Pipelined/Unrolled Hardware Accelerator Generation", "comment": "13 Pages, 16 figures, and 9 tables. Aim to submit to IEEE TCAD", "summary": "Software-defined radio (SDR) plays an important role in the communication field by providing a flexible and customized communication system for different purposes according to the needs. To enhance the performance of SDR applications, hardware accelerators have been widely deployed in recent years. In facing this obstacle, a necessity arises for a high-level synthesis (HLS) tool specifically designed for communication engineers without detailed hardware knowledge. To lower the barrier between SDR engineers and hardware development, this work proposed a Python-based HLS tool, VeriPy, which can generate both mainstream architecture for hardware accelerators in Verilog specifically for SDR designs including unrolled design and pipelined design, requiring no detailed digital hardware knowledge or Hardware Description Languages (HDL). Furthermore, VeriPy supports automatic testbench generation with random input stimulus, an extensible hardware library, performance and resource estimation, and offers strong optimisation potential at both the algorithmic and digital hardware levels. The generated hardware design by VeriPy can achieve up to 70% faster operating frequency compared to pragma-optimised Vivado HLS designs with a reasonably higher resource con-sumption while delivering comparable performance and resource consumption to hand-coded implementations. Regarding code complexity, VeriPy requires no pragmas, completely eliminating the need for low-level hardware knowledge. For straightforward algorithms, the input code length remains comparable to that of Vivado HLS.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aVeriPy\u7684Python\u9ad8\u9636\u7efc\u5408\u5de5\u5177\uff0c\u4e13\u4e3a\u901a\u4fe1\u5de5\u7a0b\u5e08\u8bbe\u8ba1\uff0c\u65e0\u9700\u786c\u4ef6\u77e5\u8bc6\u5373\u53ef\u751f\u6210\u9002\u7528\u4e8e\u8f6f\u4ef6\u5b9a\u4e49\u65e0\u7ebf\u7535\uff08SDR\uff09\u7684Verilog\u786c\u4ef6\u52a0\u901f\u5668\uff0c\u652f\u6301\u5c55\u5f00\u548c\u6d41\u6c34\u7ebf\u67b6\u6784\uff0c\u5e76\u5728\u6027\u80fd\u4e0a\u4f18\u4e8eVivado HLS\u3002", "motivation": "\u8f6f\u4ef6\u5b9a\u4e49\u65e0\u7ebf\u7535\uff08SDR\uff09\u5e94\u7528\u4e2d\u5e7f\u6cdb\u4f7f\u7528\u786c\u4ef6\u52a0\u901f\u5668\u4ee5\u63d0\u5347\u6027\u80fd\uff0c\u4f46\u901a\u4fe1\u5de5\u7a0b\u5e08\u901a\u5e38\u7f3a\u4e4f\u8be6\u7ec6\u7684\u786c\u4ef6\u77e5\u8bc6\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u9762\u5411SDR\u3001\u6613\u7528\u7684\u9ad8\u9636\u7efc\u5408\uff08HLS\uff09\u5de5\u5177\u6765\u964d\u4f4e\u786c\u4ef6\u5f00\u53d1\u95e8\u69db\u3002", "method": "\u5f00\u53d1\u4e86\u57fa\u4e8ePython\u7684HLS\u5de5\u5177VeriPy\uff0c\u53ef\u81ea\u52a8\u751f\u6210\u9002\u7528\u4e8eSDR\u7684Verilog\u786c\u4ef6\u52a0\u901f\u5668\uff08\u5305\u62ec\u5c55\u5f00\u548c\u6d41\u6c34\u7ebf\u7ed3\u6784\uff09\uff0c\u65e0\u9700\u7528\u6237\u638c\u63e1\u786c\u4ef6\u63cf\u8ff0\u8bed\u8a00\u6216\u5e95\u5c42\u786c\u4ef6\u7ec6\u8282\uff1b\u540c\u65f6\u652f\u6301\u81ea\u52a8\u6d4b\u8bd5\u5e73\u53f0\u751f\u6210\u3001\u53ef\u6269\u5c55\u786c\u4ef6\u5e93\u3001\u6027\u80fd\u4e0e\u8d44\u6e90\u4f30\u7b97\u53ca\u7b97\u6cd5\u4e0e\u786c\u4ef6\u5c42\u9762\u7684\u4f18\u5316\u3002", "result": "VeriPy\u751f\u6210\u7684\u8bbe\u8ba1\u76f8\u6bd4\u7ecfpragma\u4f18\u5316\u7684Vivado HLS\u8bbe\u8ba1\uff0c\u8fd0\u884c\u9891\u7387\u6700\u9ad8\u63d0\u534770%\uff0c\u8d44\u6e90\u6d88\u8017\u7565\u9ad8\uff0c\u4f46\u6027\u80fd\u548c\u8d44\u6e90\u4f7f\u7528\u4e0e\u624b\u5de5\u7f16\u5199\u5b9e\u73b0\u76f8\u5f53\uff1b\u4e14\u4ee3\u7801\u66f4\u7b80\u6d01\uff0c\u65e0\u9700pragma\u6307\u4ee4\uff0c\u5bf9\u7b80\u5355\u7b97\u6cd5\u8f93\u5165\u4ee3\u7801\u957f\u5ea6\u4e0eVivado HLS\u76f8\u5f53\u3002", "conclusion": "VeriPy\u6709\u6548\u964d\u4f4e\u4e86SDR\u5de5\u7a0b\u5e08\u5f00\u53d1\u786c\u4ef6\u52a0\u901f\u5668\u7684\u95e8\u69db\uff0c\u5728\u4fdd\u6301\u4ee3\u7801\u7b80\u6d01\u6027\u7684\u540c\u65f6\u5b9e\u73b0\u4e86\u9ad8\u6027\u80fd\u786c\u4ef6\u751f\u6210\uff0c\u5177\u5907\u826f\u597d\u7684\u5b9e\u7528\u6027\u548c\u4f18\u5316\u6f5c\u529b\u3002"}}
{"id": "2512.00106", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2512.00106", "abs": "https://arxiv.org/abs/2512.00106", "authors": ["Markus Funke", "Patricia Lago"], "title": "Injecting Sustainability in Software Architecture: A Rapid Review", "comment": "(Accepted/In press) 10th IEEE/ACM International Workshop on Green and Sustainable Software (GREENS '26): GREENS@ICSE 2026", "summary": "Sustainability has evolved from an emerging concern into a fundamental responsibility in software design, development, and operation. Research increasingly explores how sustainability can be systematically integrated into existing software engineering practices. Building on an industry-academia collaboration, we contribute to this discourse by conducting a mixed-method empirical study. We combine a rapid review of secondary studies with a focus group of practitioners. The review identifies challenges and opportunities in embedding sustainability in software architecture, while the focus group enriches and compares these findings. Based on the literature and industry synthesis, we derive five tangible takeaways to inform architects working in the field, and to guide our industry partners in the integration of sustainability concerns in architecture practices.", "AI": {"tldr": "\u672c\u6587\u901a\u8fc7\u6587\u732e\u5feb\u901f\u7efc\u8ff0\u4e0e\u4ece\u4e1a\u8005\u7126\u70b9\u5c0f\u7ec4\u7684\u6df7\u5408\u65b9\u6cd5\u7814\u7a76\uff0c\u63d0\u70bc\u51fa\u4e94\u4e2a\u53ef\u64cd\u4f5c\u7684\u8981\u70b9\uff0c\u4ee5\u652f\u6301\u8f6f\u4ef6\u67b6\u6784\u5e08\u5728\u5b9e\u8df5\u4e2d\u6574\u5408\u53ef\u6301\u7eed\u6027\u3002", "motivation": "\u968f\u7740\u53ef\u6301\u7eed\u6027\u6210\u4e3a\u8f6f\u4ef6\u5de5\u7a0b\u4e2d\u7684\u6838\u5fc3\u8d23\u4efb\uff0c\u4e9f\u9700\u7cfb\u7edf\u6027\u65b9\u6cd5\u5c06\u5176\u878d\u5165\u73b0\u6709\u8f6f\u4ef6\u67b6\u6784\u5b9e\u8df5\u4e2d\uff0c\u5f25\u5408\u5b66\u672f\u7814\u7a76\u4e0e\u5de5\u4e1a\u5e94\u7528\u4e4b\u95f4\u7684\u5dee\u8ddd\u3002", "method": "\u91c7\u7528\u6df7\u5408\u65b9\u6cd5\uff1a\u4e00\u65b9\u9762\u5bf9\u4e8c\u6b21\u7814\u7a76\u8fdb\u884c\u5feb\u901f\u7efc\u8ff0\u4ee5\u8bc6\u522b\u6311\u6218\u4e0e\u673a\u9047\uff0c\u53e6\u4e00\u65b9\u9762\u7ec4\u7ec7\u4ece\u4e1a\u8005\u7126\u70b9\u5c0f\u7ec4\u4ee5\u9a8c\u8bc1\u548c\u4e30\u5bcc\u6587\u732e\u53d1\u73b0\u3002", "result": "\u8bc6\u522b\u51fa\u5728\u8f6f\u4ef6\u67b6\u6784\u4e2d\u5d4c\u5165\u53ef\u6301\u7eed\u6027\u7684\u5173\u952e\u6311\u6218\u4e0e\u673a\u9047\uff0c\u5e76\u7ed3\u5408\u4e1a\u754c\u53cd\u9988\u63d0\u70bc\u51fa\u4e94\u9879\u5177\u4f53\u3001\u53ef\u64cd\u4f5c\u7684\u5b9e\u8df5\u5efa\u8bae\u3002", "conclusion": "\u901a\u8fc7\u5b66\u672f\u4e0e\u5de5\u4e1a\u754c\u534f\u4f5c\uff0c\u672c\u7814\u7a76\u4e3a\u8f6f\u4ef6\u67b6\u6784\u5e08\u63d0\u4f9b\u4e86\u5207\u5b9e\u53ef\u884c\u7684\u6307\u5bfc\uff0c\u6709\u52a9\u4e8e\u63a8\u52a8\u53ef\u6301\u7eed\u6027\u5728\u8f6f\u4ef6\u67b6\u6784\u5b9e\u8df5\u4e2d\u7684\u6709\u6548\u6574\u5408\u3002"}}
{"id": "2512.00520", "categories": ["cs.MA"], "pdf": "https://arxiv.org/pdf/2512.00520", "abs": "https://arxiv.org/abs/2512.00520", "authors": ["Juan A. Wibowo", "George C. Polyzos"], "title": "Toward a Safe Internet of Agents", "comment": "Submitted to the Journal of Artificial Intelligence Research (JAIR) for peer review. 43 pages", "summary": "Background: Autonomous agents powered by Large Language Models (LLMs) are driving a paradigm shift toward an \"Internet of Agents\" (IoA). While offering immense potential, this vision also introduces novel and systemic risks to safety and security. Objectives: Unlike common threat-centric taxonomies, our survey provides a principled, architectural framework for engineering safe and reliable agentic systems. We aim to identify the architectural sources of vulnerabilities to establish a foundation for secure design. Methods: We perform a bottom-up deconstruction of agentic systems, treating each component as a dual-use interface. The analysis spans three levels of complexity: the foundational Single Agent, the collaborative Multi-Agent System (MAS), and the visionary Interoperable Multi-Agent System (IMAS). At each level, we identify core architectural components and their inherent security risks. Results & Conclusions: Our central finding is that agentic safety is an architectural principle, not an add-on. By identifying specific vulnerabilities and deriving mitigation principles at each level of the agentic stack, this survey serves as a foundational guide for building the capable, safe, and trustworthy AI needed to realize a secure Internet of Agents.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9762\u5411\u5b89\u5168\u7684\u67b6\u6784\u5316\u6846\u67b6\uff0c\u7528\u4e8e\u5206\u6790\u548c\u6784\u5efa\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u81ea\u4e3b\u667a\u80fd\u4f53\u7cfb\u7edf\uff0c\u5f3a\u8c03\u5b89\u5168\u6027\u5e94\u5185\u751f\u4e8e\u7cfb\u7edf\u67b6\u6784\u800c\u975e\u4e8b\u540e\u9644\u52a0\u3002", "motivation": "\u5f53\u524d\u201c\u667a\u80fd\u4f53\u4e92\u8054\u7f51\u201d\uff08IoA\uff09\u613f\u666f\u867d\u5177\u6f5c\u529b\uff0c\u4f46\u5e26\u6765\u4e86\u65b0\u7684\u7cfb\u7edf\u6027\u5b89\u5168\u98ce\u9669\uff1b\u73b0\u6709\u7814\u7a76\u591a\u805a\u7126\u4e8e\u5a01\u80c1\u5206\u7c7b\uff0c\u7f3a\u4e4f\u4ece\u7cfb\u7edf\u67b6\u6784\u5c42\u9762\u7406\u89e3\u4e0e\u9632\u8303\u5b89\u5168\u6f0f\u6d1e\u7684\u5de5\u7a0b\u5316\u6846\u67b6\u3002", "method": "\u91c7\u7528\u81ea\u5e95\u5411\u4e0a\u7684\u65b9\u6cd5\uff0c\u5c06\u667a\u80fd\u4f53\u7cfb\u7edf\u7684\u6bcf\u4e2a\u7ec4\u4ef6\u89c6\u4e3a\u5177\u6709\u53cc\u91cd\u7528\u9014\u7684\u63a5\u53e3\uff0c\u4f9d\u6b21\u5206\u6790\u5355\u667a\u80fd\u4f53\u3001\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\uff08MAS\uff09\u548c\u53ef\u4e92\u64cd\u4f5c\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\uff08IMAS\uff09\u4e09\u4e2a\u5c42\u7ea7\u7684\u6838\u5fc3\u67b6\u6784\u53ca\u5176\u56fa\u6709\u5b89\u5168\u98ce\u9669\u3002", "result": "\u8bc6\u522b\u51fa\u5404\u5c42\u7ea7\u67b6\u6784\u4e2d\u7684\u5177\u4f53\u5b89\u5168\u6f0f\u6d1e\uff0c\u5e76\u63d0\u51fa\u4e86\u76f8\u5e94\u7684\u7f13\u89e3\u539f\u5219\uff0c\u8868\u660e\u667a\u80fd\u4f53\u5b89\u5168\u6027\u672c\u8d28\u4e0a\u662f\u4e00\u79cd\u67b6\u6784\u539f\u5219\u3002", "conclusion": "\u8981\u5b9e\u73b0\u5b89\u5168\u53ef\u4fe1\u7684\u201c\u667a\u80fd\u4f53\u4e92\u8054\u7f51\u201d\uff0c\u5fc5\u987b\u5c06\u5b89\u5168\u6027\u4f5c\u4e3a\u7cfb\u7edf\u8bbe\u8ba1\u7684\u5185\u5728\u7ec4\u6210\u90e8\u5206\uff1b\u672c\u7814\u7a76\u4e3a\u6784\u5efa\u53ef\u9760\u3001\u5b89\u5168\u7684\u667a\u80fd\u4f53\u7cfb\u7edf\u63d0\u4f9b\u4e86\u57fa\u7840\u6027\u6307\u5bfc\u3002"}}
{"id": "2512.00705", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2512.00705", "abs": "https://arxiv.org/abs/2512.00705", "authors": ["Seongyeon Park", "Jaeyong Song", "Changmin Shin", "Sukjin Kim", "Junguk Hong", "Jinho Lee"], "title": "FlexiWalker: Extensible GPU Framework for Efficient Dynamic Random Walks with Runtime Adaptation", "comment": "To appear at EuroSys 2026", "summary": "Dynamic random walks are fundamental to various graph analysis applications, offering advantages by adapting to evolving graph properties. Their runtime-dependent transition probabilities break down the pre-computation strategy that underpins most existing CPU and GPU static random walk optimizations. This leaves practitioners suffering from suboptimal frameworks and having to write hand-tuned kernels that do not adapt to workload diversity. To handle this issue, we present FlexiWalker, the first GPU framework that delivers efficient, workload-generic support for dynamic random walks. Our design-space study shows that rejection sampling and reservoir sampling are more suitable than other sampling techniques under massive parallelism. Thus, we devise (i) new high-performance kernels for them that eliminate global reductions, redundant memory accesses, and random-number generation. Given the necessity of choosing the best-fitting sampling strategy at runtime, we adopt (ii) a lightweight first-order cost model that selects the faster kernel per node at runtime. To enhance usability, we introduce (iii) a compile-time component that automatically specializes user-supplied walk logic into optimized building blocks. On various dynamic random walk workloads with real-world graphs, FlexiWalker outperforms the best published CPU/GPU baselines by geometric means of 73.44x and 5.91x, respectively, while successfully executing workloads that prior systems cannot support. We open-source FlexiWalker in https://github.com/AIS-SNU/FlexiWalker.", "AI": {"tldr": "FlexiWalker \u662f\u9996\u4e2a\u652f\u6301\u9ad8\u6548\u3001\u901a\u7528\u52a8\u6001\u968f\u673a\u6e38\u8d70\u7684 GPU \u6846\u67b6\uff0c\u901a\u8fc7\u4f18\u5316\u91c7\u6837\u7b56\u7565\u3001\u8fd0\u884c\u65f6\u5185\u6838\u9009\u62e9\u548c\u81ea\u52a8\u7f16\u8bd1\u4f18\u5316\uff0c\u5728\u771f\u5b9e\u56fe\u6570\u636e\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709 CPU/GPU \u57fa\u7ebf\u3002", "motivation": "\u52a8\u6001\u968f\u673a\u6e38\u8d70\u56e0\u5176\u8fd0\u884c\u65f6\u4f9d\u8d56\u7684\u8f6c\u79fb\u6982\u7387\uff0c\u65e0\u6cd5\u5229\u7528\u73b0\u6709\u9488\u5bf9\u9759\u6001\u968f\u673a\u6e38\u8d70\u7684\u9884\u8ba1\u7b97\u4f18\u5316\u7b56\u7565\uff0c\u5bfc\u81f4\u5f53\u524d\u7cfb\u7edf\u6548\u7387\u4f4e\u4e0b\u4e14\u7f3a\u4e4f\u5bf9\u591a\u6837\u5316\u5de5\u4f5c\u8d1f\u8f7d\u7684\u9002\u5e94\u80fd\u529b\u3002", "method": "\u63d0\u51fa FlexiWalker \u6846\u67b6\uff0c\u5305\u542b\uff1a(i) \u9ad8\u6027\u80fd\u62d2\u7edd\u91c7\u6837\u4e0e\u84c4\u6c34\u6c60\u91c7\u6837 GPU \u5185\u6838\uff0c\u6d88\u9664\u5168\u5c40\u5f52\u7ea6\u3001\u5197\u4f59\u8bbf\u5b58\u548c\u968f\u673a\u6570\u751f\u6210\uff1b(ii) \u8f7b\u91cf\u7ea7\u4e00\u9636\u4ee3\u4ef7\u6a21\u578b\uff0c\u5728\u8fd0\u884c\u65f6\u4e3a\u6bcf\u4e2a\u8282\u70b9\u9009\u62e9\u66f4\u5feb\u7684\u91c7\u6837\u5185\u6838\uff1b(iii) \u7f16\u8bd1\u671f\u7ec4\u4ef6\uff0c\u5c06\u7528\u6237\u81ea\u5b9a\u4e49\u6e38\u8d70\u903b\u8f91\u81ea\u52a8\u7279\u5316\u4e3a\u4f18\u5316\u6784\u5efa\u5757\u3002", "result": "\u5728\u591a\u79cd\u771f\u5b9e\u56fe\u7684\u52a8\u6001\u968f\u673a\u6e38\u8d70\u4efb\u52a1\u4e0a\uff0cFlexiWalker \u76f8\u6bd4\u6700\u4f73\u5df2\u53d1\u8868\u7684 CPU \u548c GPU \u57fa\u7ebf\u5206\u522b\u5e73\u5747\u63d0\u901f 73.44 \u500d\u548c 5.91 \u500d\uff0c\u5e76\u80fd\u6267\u884c\u5148\u524d\u7cfb\u7edf\u65e0\u6cd5\u5904\u7406\u7684\u5de5\u4f5c\u8d1f\u8f7d\u3002", "conclusion": "FlexiWalker \u9996\u6b21\u5b9e\u73b0\u4e86\u9ad8\u6548\u3001\u901a\u7528\u4e14\u6613\u7528\u7684 GPU \u52a8\u6001\u968f\u673a\u6e38\u8d70\u652f\u6301\uff0c\u663e\u8457\u63d0\u5347\u6027\u80fd\u5e76\u6269\u5c55\u4e86\u53ef\u5904\u7406\u95ee\u9898\u7684\u8303\u56f4\uff0c\u4ee3\u7801\u5df2\u5f00\u6e90\u3002"}}
{"id": "2512.01381", "categories": ["cs.OS", "cs.DS", "cs.RO"], "pdf": "https://arxiv.org/pdf/2512.01381", "abs": "https://arxiv.org/abs/2512.01381", "authors": ["Hiroto Takahashi", "Atsushi Yano", "Takuya Azumi"], "title": "Accelerating Probabilistic Response-Time Analysis: Revised Critical Instant and Optimized Convolution", "comment": "8 pages, 5 figures. Proceedings of APRIS2025", "summary": "Accurate estimation of the Worst-Case Deadline Failure Probability (WCDFP) has attracted growing attention as a means to provide safety assurances in complex systems such as robotic platforms and autonomous vehicles. WCDFP quantifies the likelihood of deadline misses under the most pessimistic operating conditions, and safe estimation is essential for dependable real-time applications. However, achieving high accuracy in WCDFP estimation often incurs significant computational cost. Recent studies have revealed that the classical assumption of the critical instant, the activation pattern traditionally considered to trigger the worst-case behavior, can lead to underestimation of WCDFP in probabilistic settings. This observation motivates the use of a revised critical instant formulation that more faithfully captures the true worst-case scenario. This paper investigates convolution-based methods for WCDFP estimation under this revised setting and proposes an optimization technique that accelerates convolution by improving the merge order. Extensive experiments with diverse execution-time distributions demonstrate that the proposed optimized Aggregate Convolution reduces computation time by up to an order of magnitude compared to Sequential Convolution, while retaining accurate and safe-sided WCDFP estimates. These results highlight the potential of the approach to provide both efficiency and reliability in probabilistic timing analysis for safety-critical real-time applications.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u4f18\u5316\u7684\u805a\u5408\u5377\u79ef\u65b9\u6cd5\uff0c\u5728\u4fee\u6b63\u7684\u5173\u952e\u65f6\u523b\u5047\u8bbe\u4e0b\u9ad8\u6548\u4e14\u5b89\u5168\u5730\u4f30\u8ba1\u6700\u574f\u60c5\u51b5\u622a\u6b62\u671f\u5931\u8d25\u6982\u7387\uff08WCDFP\uff09\uff0c\u663e\u8457\u964d\u4f4e\u8ba1\u7b97\u5f00\u9500\u540c\u65f6\u4fdd\u6301\u4f30\u8ba1\u7cbe\u5ea6\u3002", "motivation": "\u4f20\u7edf\u5173\u952e\u65f6\u523b\u5047\u8bbe\u5728\u6982\u7387\u6027\u5b9e\u65f6\u7cfb\u7edf\u4e2d\u53ef\u80fd\u5bfc\u81f4WCDFP\u4f4e\u4f30\uff0c\u65e0\u6cd5\u786e\u4fdd\u5b89\u5168\u6027\uff1b\u540c\u65f6\u9ad8\u7cbe\u5ea6WCDFP\u4f30\u8ba1\u901a\u5e38\u8ba1\u7b97\u6210\u672c\u9ad8\u6602\uff0c\u4e9f\u9700\u517c\u987e\u51c6\u786e\u6027\u4e0e\u6548\u7387\u7684\u65b0\u65b9\u6cd5\u3002", "method": "\u91c7\u7528\u4fee\u6b63\u7684\u5173\u952e\u65f6\u523b\u516c\u5f0f\uff0c\u7814\u7a76\u57fa\u4e8e\u5377\u79ef\u7684WCDFP\u4f30\u8ba1\u65b9\u6cd5\uff0c\u5e76\u63d0\u51fa\u901a\u8fc7\u4f18\u5316\u5408\u5e76\u987a\u5e8f\u6765\u52a0\u901f\u5377\u79ef\u8ba1\u7b97\u7684Aggregate Convolution\u4f18\u5316\u6280\u672f\u3002", "result": "\u5728\u591a\u79cd\u6267\u884c\u65f6\u95f4\u5206\u5e03\u4e0b\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u6240\u63d0\u4f18\u5316\u65b9\u6cd5\u76f8\u6bd4Sequential Convolution\u53ef\u5c06\u8ba1\u7b97\u65f6\u95f4\u51cf\u5c11\u8fd1\u4e00\u4e2a\u6570\u91cf\u7ea7\uff0c\u540c\u65f6\u4fdd\u6301\u51c6\u786e\u4e14\u504f\u4fdd\u5b88\uff08safe-sided\uff09\u7684WCDFP\u4f30\u8ba1\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u4fdd\u8bc1\u5b89\u5168\u6027\u548c\u51c6\u786e\u6027\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u4e86WCDFP\u4f30\u8ba1\u6548\u7387\uff0c\u9002\u7528\u4e8e\u5bf9\u53ef\u9760\u6027\u8981\u6c42\u9ad8\u7684\u6982\u7387\u6027\u5b9e\u65f6\u7cfb\u7edf\u3002"}}
{"id": "2512.00025", "categories": ["cs.NI"], "pdf": "https://arxiv.org/pdf/2512.00025", "abs": "https://arxiv.org/abs/2512.00025", "authors": ["Yun Ji", "Zeyu Chen", "Xiaoxiong Zhong", "Yanan Ma", "Sheng Zhang", "Yuguang Fang"], "title": "Multi-Server FL with Overlapping Clients: A Latency-Aware Relay Framework", "comment": null, "summary": "Multi-server Federated Learning (FL) has emerged as a promising solution to mitigate communication bottlenecks of single-server FL. In a typical multi-server FL architecture, the regions covered by different edge servers (ESs) may overlap. Under this architecture, clients located in the overlapping areas can access edge models from multiple ESs. Building on this observation, we propose a cloud-free multi-server FL framework that leverages Overlapping Clients (OCs) as relays for inter-server model exchange while uploading the local updated model to ESs. This enables ES models to be relayed across multiple hops through neighboring ESs by OCs without introducing new communication links. We derive a new convergence upper bound for non-convex objectives under non-IID data and an arbitrary number of cells, which explicitly quantifies the impact of inter-server propagation depth on convergence error. Guided by this theoretical result, we formulate an optimization problem that aims to maximize dissemination range of each ES model among all ESs within a limited latency. To solve this problem, we develop a conflict-graph-based local search algorithm optimizing the routing strategy and scheduling the transmission times of individual ESs to its neighboring ESs. This enables ES models to be relayed across multiple hops through neighboring ESs by OCs, achieving the widest possible transmission coverage for each model without introducing new communication links. Extensive experimental results show remarkable performance gains of our scheme compared to existing methods.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u4e91\u7684\u591a\u670d\u52a1\u5668\u8054\u90a6\u5b66\u4e60\u6846\u67b6\uff0c\u5229\u7528\u91cd\u53e0\u533a\u57df\u5ba2\u6237\u7aef\u4f5c\u4e3a\u4e2d\u7ee7\uff0c\u5728\u4e0d\u65b0\u589e\u901a\u4fe1\u94fe\u8def\u7684\u60c5\u51b5\u4e0b\u5b9e\u73b0\u8fb9\u7f18\u670d\u52a1\u5668\u95f4\u7684\u6a21\u578b\u4f20\u64ad\uff0c\u5e76\u901a\u8fc7\u7406\u8bba\u5206\u6790\u548c\u4f18\u5316\u7b97\u6cd5\u63d0\u5347\u6a21\u578b\u4f20\u64ad\u8303\u56f4\u4e0e\u6536\u655b\u6027\u80fd\u3002", "motivation": "\u4f20\u7edf\u5355\u670d\u52a1\u5668\u8054\u90a6\u5b66\u4e60\u5b58\u5728\u901a\u4fe1\u74f6\u9888\uff0c\u800c\u591a\u670d\u52a1\u5668\u67b6\u6784\u4e2d\u8fb9\u7f18\u670d\u52a1\u5668\u8986\u76d6\u533a\u57df\u5e38\u6709\u91cd\u53e0\uff0c\u4f4d\u4e8e\u91cd\u53e0\u533a\u7684\u5ba2\u6237\u7aef\u53ef\u8bbf\u95ee\u591a\u4e2a\u8fb9\u7f18\u670d\u52a1\u5668\u6a21\u578b\u3002\u4f5c\u8005\u5e0c\u671b\u5229\u7528\u8fd9\u4e9b\u91cd\u53e0\u5ba2\u6237\u7aef\u4f5c\u4e3a\u4e2d\u7ee7\uff0c\u5b9e\u73b0\u670d\u52a1\u5668\u95f4\u6a21\u578b\u4ea4\u6362\uff0c\u4ece\u800c\u63d0\u5347\u6574\u4f53\u6027\u80fd\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u91cd\u53e0\u5ba2\u6237\u7aef\uff08OCs\uff09\u7684\u65e0\u4e91\u591a\u670d\u52a1\u5668\u8054\u90a6\u5b66\u4e60\u6846\u67b6\uff0c\u63a8\u5bfc\u975e\u51f8\u76ee\u6807\u4e0b\u975e\u72ec\u7acb\u540c\u5206\u5e03\u6570\u636e\u7684\u6536\u655b\u4e0a\u754c\uff0c\u6784\u5efa\u4ee5\u6700\u5927\u5316\u6a21\u578b\u4f20\u64ad\u8303\u56f4\u4e3a\u76ee\u6807\u7684\u4f18\u5316\u95ee\u9898\uff0c\u5e76\u8bbe\u8ba1\u57fa\u4e8e\u51b2\u7a81\u56fe\u7684\u5c40\u90e8\u641c\u7d22\u7b97\u6cd5\u6765\u4f18\u5316\u8def\u7531\u7b56\u7565\u4e0e\u4f20\u8f93\u8c03\u5ea6\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u6240\u63d0\u65b9\u6848\u5728\u6a21\u578b\u4f20\u64ad\u8303\u56f4\u548c\u6536\u655b\u6027\u80fd\u65b9\u9762\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "\u5229\u7528\u91cd\u53e0\u5ba2\u6237\u7aef\u4f5c\u4e3a\u4e2d\u7ee7\u53ef\u5728\u4e0d\u589e\u52a0\u65b0\u901a\u4fe1\u94fe\u8def\u7684\u524d\u63d0\u4e0b\u6709\u6548\u63d0\u5347\u591a\u670d\u52a1\u5668\u8054\u90a6\u5b66\u4e60\u7684\u6a21\u578b\u4f20\u64ad\u6548\u7387\u4e0e\u7cfb\u7edf\u6027\u80fd\uff0c\u7406\u8bba\u5206\u6790\u4e0e\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2512.00016", "categories": ["cs.AR", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.00016", "abs": "https://arxiv.org/abs/2512.00016", "authors": ["Mubarek Mohammed"], "title": "Architect in the Loop Agentic Hardware Design and Verification", "comment": null, "summary": "The ever increasing complexity of the hardware design process demands improved hardware design and verification methodologies. With the advent of generative AI various attempts have been made to automate parts of the design and verification process. Large language models (LLMs) as well as specialized models generate hdl and testbenches for small components, having a few leaf level components. However, there are only a few attempts to automate the entire processor design process. Hardware design demands hierarchical and modular design processes. We utilized this best practice systematically and effectively. We propose agentic automated processor design and verification with engineers in the loop. The agent with optional specification tries to break down the design into sub-components, generate HDL and cocotb tests, and verifies the components involving engineer guidance, especially during debugging and synthesis. We designed various digital systems using this approach. However, we selected two simple processors for demonstration purposes in this work. The first one is a LEGv8 like a simple processor verified, synthesized and programmed for the DE-10 Lite FPGA. The second one is a RISC-V like 32-bit processor designed and verified in similar manner and synthesized. However, it is not programmed into the DE-10 Lite. This process is accomplished usually using around a million inference tokens per processor, using a combination of reasoning (e.g gemini-pro) and non-reasoning models (eg. gpt-5-mini) based on the complexity of the task. This indicates that hardware design and verification experimentation can be done cost effectively without using any specialized hardware. The approach is scalable, we even attempted system-on-chip, which we want to experiment in our future work.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u5927\u8bed\u8a00\u6a21\u578b\u4e0e\u5de5\u7a0b\u5e08\u534f\u540c\u7684\u81ea\u52a8\u5316\u5904\u7406\u5668\u8bbe\u8ba1\u4e0e\u9a8c\u8bc1\u65b9\u6cd5\uff0c\u901a\u8fc7\u667a\u80fd\u4f53\u5c06\u8bbe\u8ba1\u4efb\u52a1\u5206\u89e3\u4e3a\u5b50\u6a21\u5757\uff0c\u751f\u6210HDL\u4ee3\u7801\u548ccocotb\u6d4b\u8bd5\uff0c\u5e76\u5728\u8c03\u8bd5\u4e0e\u7efc\u5408\u9636\u6bb5\u5f15\u5165\u4eba\u5de5\u6307\u5bfc\u3002\u8be5\u65b9\u6cd5\u6210\u529f\u7528\u4e8e\u8bbe\u8ba1\u5e76\u5b9e\u73b0\u4e24\u6b3e\u7b80\u5355\u5904\u7406\u5668\uff08LEGv8 \u548c RISC-V\uff09\uff0c\u6bcf\u6b3e\u5904\u7406\u5668\u7ea6\u6d88\u8017\u4e00\u767e\u4e07\u63a8\u7406token\uff0c\u65e0\u9700\u4e13\u7528\u786c\u4ef6\u5373\u53ef\u4f4e\u6210\u672c\u5b8c\u6210\uff0c\u5177\u5907\u826f\u597d\u53ef\u6269\u5c55\u6027\u3002", "motivation": "\u786c\u4ef6\u8bbe\u8ba1\u65e5\u76ca\u590d\u6742\uff0c\u73b0\u6709\u57fa\u4e8e\u751f\u6210\u5f0fAI\u7684\u65b9\u6cd5\u591a\u5c40\u9650\u4e8e\u5c0f\u578b\u7ec4\u4ef6\uff0c\u7f3a\u4e4f\u5bf9\u5b8c\u6574\u5904\u7406\u5668\u8bbe\u8ba1\u6d41\u7a0b\u7684\u81ea\u52a8\u5316\u652f\u6301\uff1b\u540c\u65f6\uff0c\u786c\u4ef6\u8bbe\u8ba1\u5929\u7136\u5177\u6709\u5c42\u6b21\u5316\u4e0e\u6a21\u5757\u5316\u7279\u6027\uff0c\u4e9f\u9700\u4e00\u79cd\u80fd\u7cfb\u7edf\u5229\u7528\u8fd9\u4e00\u7279\u6027\u7684\u81ea\u52a8\u5316\u65b9\u6cd5\u3002", "method": "\u91c7\u7528\u201c\u4eba\u5728\u73af\u8def\u201d\u7684\u667a\u80fd\u4f53\u67b6\u6784\uff0c\u7ed3\u5408\u63a8\u7406\u578b\uff08\u5982Gemini-Pro\uff09\u4e0e\u975e\u63a8\u7406\u578b\uff08\u5982GPT-5-Mini\uff09\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u6839\u636e\u53ef\u9009\u89c4\u8303\u5c06\u5904\u7406\u5668\u8bbe\u8ba1\u5206\u89e3\u4e3a\u5b50\u7ec4\u4ef6\uff0c\u81ea\u52a8\u751f\u6210HDL\u4ee3\u7801\u4e0ecocotb\u6d4b\u8bd5\u5e73\u53f0\uff0c\u5e76\u5728\u9a8c\u8bc1\u3001\u8c03\u8bd5\u548c\u7efc\u5408\u9636\u6bb5\u5f15\u5165\u5de5\u7a0b\u5e08\u6307\u5bfc\u3002", "result": "\u6210\u529f\u8bbe\u8ba1\u5e76\u9a8c\u8bc1\u4e86\u4e24\u6b3e\u5904\u7406\u5668\uff1a\u4e00\u6b3eLEGv8\u98ce\u683c\u5904\u7406\u5668\u5df2\u7efc\u5408\u5e76\u90e8\u7f72\u5230DE-10 Lite FPGA\uff1b\u53e6\u4e00\u6b3e32\u4f4dRISC-V\u98ce\u683c\u5904\u7406\u5668\u5b8c\u6210\u8bbe\u8ba1\u4e0e\u7efc\u5408\u4f46\u672a\u90e8\u7f72\u3002\u6574\u4e2a\u6d41\u7a0b\u6bcf\u5904\u7406\u5668\u6d88\u8017\u7ea6\u4e00\u767e\u4e07\u63a8\u7406token\uff0c\u6210\u672c\u53ef\u63a7\uff0c\u4e14\u65b9\u6cd5\u53ef\u6269\u5c55\u81f3SoC\u7ea7\u522b\u3002", "conclusion": "\u6240\u63d0\u65b9\u6cd5\u6709\u6548\u7ed3\u5408\u4e86\u751f\u6210\u5f0fAI\u4e0e\u5de5\u7a0b\u5e08\u4e13\u4e1a\u77e5\u8bc6\uff0c\u5b9e\u73b0\u4e86\u4f4e\u6210\u672c\u3001\u53ef\u6269\u5c55\u7684\u81ea\u52a8\u5316\u5904\u7406\u5668\u8bbe\u8ba1\u4e0e\u9a8c\u8bc1\uff0c\u4e3a\u672a\u6765\u590d\u6742\u7cfb\u7edf\uff08\u5982SoC\uff09\u7684\u81ea\u52a8\u5316\u8bbe\u8ba1\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2512.00127", "categories": ["cs.SE", "cs.AI", "cs.PL"], "pdf": "https://arxiv.org/pdf/2512.00127", "abs": "https://arxiv.org/abs/2512.00127", "authors": ["Shailja Thakur", "Vaibhav Saxena", "Rohan Kulkarni", "Shivdeep Singh", "Parameswaran Selvam", "Hima Patel", "Hiroshi Kanayama"], "title": "Generating Verifiable CoT from Execution-Traces", "comment": null, "summary": "Teaching language models to reason about code execution remains a fundamental challenge. While Chain-of-Thought (CoT) prompting has shown promise, current synthetic training data suffers from a critical weakness: the reasoning steps are often plausible-sounding explanations generated by teacher models, not verifiable accounts of what the code actually does. This creates a troubling failure mode where models learn to mimic superficially convincing but logically flawed reasoning patterns.\n  We address this by grounding CoT generation directly in program execution traces. Our pipeline instruments code to capture its dynamic behavior, then narrates these verified execution traces into natural language rationales that are correct by construction. This execution-grounded approach ensures every reasoning step reflects what the program genuinely computes, eliminating logical hallucinations at the source. We evaluate our method on code reasoning tasks (forward reasoning on CruxEval and LiveCodeBench-Exec, backward reasoning on CruxEval-Input), as well as code generation and explanation tasks from HumanEval. Models trained on our bi-directional trace-grounded data achieve substantial improvements, with gains of up to 30 points on output prediction and 28 points on input prediction over base models, alongside improved explanation and code generation, demonstrating that verifiable reasoning fundamentally enhances model capabilities. https://github.ibm.com/IBM-Research-AI/Verified-Code-CoT", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u7a0b\u5e8f\u6267\u884c\u8f68\u8ff9\u751f\u6210\u63a8\u7406\u94fe\uff08CoT\uff09\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u5c06\u4ee3\u7801\u52a8\u6001\u6267\u884c\u8fc7\u7a0b\u8f6c\u5316\u4e3a\u81ea\u7136\u8bed\u8a00\u63a8\u7406\u6b65\u9aa4\uff0c\u786e\u4fdd\u63a8\u7406\u5185\u5bb9\u4e0e\u7a0b\u5e8f\u5b9e\u9645\u884c\u4e3a\u4e00\u81f4\uff0c\u4ece\u800c\u907f\u514d\u6a21\u578b\u5b66\u4e60\u5230\u903b\u8f91\u9519\u8bef\u7684\u63a8\u7406\u6a21\u5f0f\u3002\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u591a\u4e2a\u4ee3\u7801\u7406\u89e3\u4e0e\u751f\u6210\u4efb\u52a1\u4e0a\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u6a21\u578b\u3002", "motivation": "\u5f53\u524d\u57fa\u4e8e\u5408\u6210\u6570\u636e\u8bad\u7ec3\u7684\u8bed\u8a00\u6a21\u578b\u5728\u4ee3\u7801\u63a8\u7406\u4efb\u52a1\u4e2d\u5b58\u5728\u4e25\u91cd\u7f3a\u9677\uff1a\u5176\u63a8\u7406\u6b65\u9aa4\u591a\u4e3a\u6559\u5e08\u6a21\u578b\u751f\u6210\u7684\u770b\u4f3c\u5408\u7406\u4f46\u672a\u7ecf\u9a8c\u8bc1\u7684\u89e3\u91ca\uff0c\u5bb9\u6613\u5bfc\u81f4\u6a21\u578b\u5b66\u4e60\u5230\u8868\u9762\u53ef\u4fe1\u4f46\u903b\u8f91\u9519\u8bef\u7684\u63a8\u7406\u65b9\u5f0f\u3002", "method": "\u4f5c\u8005\u6784\u5efa\u4e86\u4e00\u4e2a\u7ba1\u9053\uff0c\u9996\u5148\u5bf9\u4ee3\u7801\u8fdb\u884c\u63d2\u6869\u4ee5\u6355\u83b7\u5176\u52a8\u6001\u6267\u884c\u8f68\u8ff9\uff0c\u7136\u540e\u5c06\u8fd9\u4e9b\u53ef\u9a8c\u8bc1\u7684\u6267\u884c\u8f68\u8ff9\u8f6c\u5316\u4e3a\u81ea\u7136\u8bed\u8a00\u5f62\u5f0f\u7684\u63a8\u7406\u94fe\uff0c\u4ece\u800c\u4fdd\u8bc1\u6bcf\u4e00\u6b65\u63a8\u7406\u90fd\u5fe0\u5b9e\u53cd\u6620\u7a0b\u5e8f\u7684\u771f\u5b9e\u884c\u4e3a\u3002", "result": "\u5728CruxEval\u3001LiveCodeBench-Exec\u548cHumanEval\u7b49\u591a\u4e2a\u57fa\u51c6\u4e0a\uff0c\u4f7f\u7528\u8be5\u65b9\u6cd5\u8bad\u7ec3\u7684\u6a21\u578b\u5728\u8f93\u51fa\u9884\u6d4b\u4efb\u52a1\u4e0a\u6700\u9ad8\u63d0\u534730\u5206\uff0c\u5728\u8f93\u5165\u9884\u6d4b\u4efb\u52a1\u4e0a\u63d0\u534728\u5206\uff0c\u540c\u65f6\u5728\u4ee3\u7801\u89e3\u91ca\u548c\u751f\u6210\u4efb\u52a1\u4e0a\u4e5f\u8868\u73b0\u66f4\u4f18\u3002", "conclusion": "\u5c06\u63a8\u7406\u94fe\u5efa\u7acb\u5728\u53ef\u9a8c\u8bc1\u7684\u7a0b\u5e8f\u6267\u884c\u8f68\u8ff9\u4e4b\u4e0a\uff0c\u80fd\u4ece\u6839\u672c\u4e0a\u63d0\u5347\u8bed\u8a00\u6a21\u578b\u5728\u4ee3\u7801\u76f8\u5173\u4efb\u52a1\u4e2d\u7684\u63a8\u7406\u80fd\u529b\u548c\u53ef\u9760\u6027\u3002"}}
{"id": "2512.00602", "categories": ["cs.MA", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.00602", "abs": "https://arxiv.org/abs/2512.00602", "authors": ["Wanle Zhong", "Keman Huang", "Xiaoyong Du"], "title": "AgentODRL: A Large Language Model-based Multi-agent System for ODRL Generation", "comment": "Accepted by AAAI 2026. 9 pages, 1 figure", "summary": "The Open Digital Rights Language (ODRL) is a pivotal standard for automating data rights management. However, the inherent logical complexity of authorization policies, combined with the scarcity of high-quality \"Natural Language-to-ODRL\" training datasets, impedes the ability of current methods to efficiently and accurately translate complex rules from natural language into the ODRL format. To address this challenge, this research leverages the potent comprehension and generation capabilities of Large Language Models (LLMs) to achieve both automation and high fidelity in this translation process. We introduce AgentODRL, a multi-agent system based on an Orchestrator-Workers architecture. The architecture consists of specialized Workers, including a Generator for ODRL policy creation, a Decomposer for breaking down complex use cases, and a Rewriter for simplifying nested logical relationships. The Orchestrator agent dynamically coordinates these Workers, assembling an optimal pathway based on the complexity of the input use case. Specifically, we enhance the ODRL Generator by incorporating a validator-based syntax strategy and a semantic reflection mechanism powered by a LoRA-finetuned model, significantly elevating the quality of the generated policies. Extensive experiments were conducted on a newly constructed dataset comprising 770 use cases of varying complexity, all situated within the context of data spaces. The results, evaluated using ODRL syntax and semantic scores, demonstrate that our proposed Orchestrator-Workers system, enhanced with these strategies, achieves superior performance on the ODRL generation task.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faAgentODRL\uff0c\u4e00\u79cd\u57fa\u4e8eOrchestrator-Workers\u67b6\u6784\u7684\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\uff0c\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\u63d0\u5347\u81ea\u7136\u8bed\u8a00\u5230ODRL\u7b56\u7565\u7684\u81ea\u52a8\u7ffb\u8bd1\u51c6\u786e\u6027\u4e0e\u6548\u7387\u3002", "motivation": "\u5f53\u524d\u65b9\u6cd5\u96be\u4ee5\u9ad8\u6548\u51c6\u786e\u5730\u5c06\u590d\u6742\u81ea\u7136\u8bed\u8a00\u89c4\u5219\u7ffb\u8bd1\u4e3aODRL\u683c\u5f0f\uff0c\u4e3b\u8981\u53d7\u9650\u4e8e\u6388\u6743\u7b56\u7565\u7684\u903b\u8f91\u590d\u6742\u6027\u4ee5\u53ca\u9ad8\u8d28\u91cf\u8bad\u7ec3\u6570\u636e\u7684\u7f3a\u4e4f\u3002", "method": "\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u591a\u667a\u80fd\u4f53\u7cfb\u7edfAgentODRL\uff0c\u5305\u542bGenerator\u3001Decomposer\u548cRewriter\u4e09\u4e2a\u4e13\u7528Worker\uff0c\u5e76\u7531Orchestrator\u52a8\u6001\u534f\u8c03\uff1b\u901a\u8fc7\u57fa\u4e8e\u9a8c\u8bc1\u5668\u7684\u8bed\u6cd5\u7b56\u7565\u548c\u57fa\u4e8eLoRA\u5fae\u8c03\u6a21\u578b\u7684\u8bed\u4e49\u53cd\u601d\u673a\u5236\u589e\u5f3a\u751f\u6210\u8d28\u91cf\u3002", "result": "\u5728\u5305\u542b770\u4e2a\u4e0d\u540c\u590d\u6742\u5ea6\u7528\u4f8b\u7684\u65b0\u6784\u5efa\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u5b9e\u9a8c\uff0c\u7ed3\u679c\u8868\u660e\u8be5\u7cfb\u7edf\u5728ODRL\u8bed\u6cd5\u548c\u8bed\u4e49\u8bc4\u5206\u4e0a\u5747\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "\u7ed3\u5408\u5927\u8bed\u8a00\u6a21\u578b\u4e0e\u591a\u667a\u80fd\u4f53\u67b6\u6784\u80fd\u6709\u6548\u63d0\u5347\u81ea\u7136\u8bed\u8a00\u5230ODRL\u7684\u81ea\u52a8\u5316\u7ffb\u8bd1\u8d28\u91cf\uff0c\u4e3a\u6570\u636e\u6743\u5229\u7ba1\u7406\u63d0\u4f9b\u53ef\u884c\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2512.00719", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2512.00719", "abs": "https://arxiv.org/abs/2512.00719", "authors": ["Bohan Zhao", "Zane Cao", "Yongchao He"], "title": "SIMPLE: Disaggregating Sampling from GPU Inference into a Decision Plane for Faster Distributed LLM Serving", "comment": null, "summary": "As large language models (LLMs) scale out with tensor parallelism (TP) and pipeline parallelism (PP) and production stacks have aggressively optimized the data plane (attention/GEMM and KV cache), sampling, the decision plane that turns logits into tokens, becomes a new bottleneck. This creates a structural holdout: sampling neither expands with TP nor balances across PP stages, so its share of iteration time grows as GPUs get faster and it caps pipeline frequency at the last stage. We present SIMPLE, a stage-agnostic, sequence-parallel, overlappable decision plane that disaggregates sampling into a CPU-side service and shrinks its runtime footprint back to a minor, hidden role. SIMPLE combines: (1) sequence-parallel sampling, which shards work along the batch dimension and removes vocabulary-axis collectives; (2) a CPU-based algorithm with column-wise penalties and truncation-first filtering to realize single-pass, linear-time kernels; and (3) speculative hot-vocab sampling (SHVS), which samples on a small hot set with rejection-correctness and uses a simple sizing model to choose the hot-vocab size that maximizes throughput. In evaluation, SIMPLE improves end-to-end throughput by up to 96% and reduces P95 latency by 20-65%. Crucially, SIMPLE requires no user-side code changes and composes with existing data-plane optimizations, unlocking scaling benefits that compound with future GPU generations.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa SIMPLE\uff0c\u4e00\u79cd\u4e0e\u9636\u6bb5\u65e0\u5173\u3001\u5e8f\u5217\u5e76\u884c\u4e14\u53ef\u91cd\u53e0\u7684\u91c7\u6837\u65b9\u6848\uff0c\u901a\u8fc7\u5c06\u91c7\u6837\u4efb\u52a1\u5378\u8f7d\u5230 CPU \u5e76\u4f18\u5316\u7b97\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u5927\u8bed\u8a00\u6a21\u578b\u63a8\u7406\u541e\u5410\u91cf\u5e76\u964d\u4f4e\u5ef6\u8fdf\uff0c\u4e14\u65e0\u9700\u7528\u6237\u4fee\u6539\u4ee3\u7801\u3002", "motivation": "\u968f\u7740\u5927\u8bed\u8a00\u6a21\u578b\u5728\u5f20\u91cf\u5e76\u884c\uff08TP\uff09\u548c\u6d41\u6c34\u7ebf\u5e76\u884c\uff08PP\uff09\u4e0b\u7684\u6269\u5c55\u4ee5\u53ca\u6570\u636e\u5e73\u9762\uff08\u5982\u6ce8\u610f\u529b\u673a\u5236\u3001KV\u7f13\u5b58\uff09\u7684\u9ad8\u5ea6\u4f18\u5316\uff0c\u91c7\u6837\uff08\u5c06 logits \u8f6c\u6362\u4e3a token \u7684\u51b3\u7b56\u8fc7\u7a0b\uff09\u6210\u4e3a\u65b0\u7684\u6027\u80fd\u74f6\u9888\u3002\u8be5\u74f6\u9888\u65e0\u6cd5\u968f TP \u6269\u5c55\uff0c\u4e5f\u65e0\u6cd5\u5728 PP \u5404\u9636\u6bb5\u95f4\u5747\u8861\u8d1f\u8f7d\uff0c\u9650\u5236\u4e86\u6574\u4f53\u63a8\u7406\u6548\u7387\u3002", "method": "SIMPLE \u65b9\u6cd5\u5305\u542b\u4e09\u90e8\u5206\uff1a(1) \u5e8f\u5217\u5e76\u884c\u91c7\u6837\uff0c\u6cbf batch \u7ef4\u5ea6\u5206\u7247\u5de5\u4f5c\u5e76\u6d88\u9664\u8bcd\u6c47\u8868\u7ef4\u5ea6\u7684\u901a\u4fe1\uff1b(2) \u57fa\u4e8e CPU \u7684\u5355\u904d\u7ebf\u6027\u65f6\u95f4\u7b97\u6cd5\uff0c\u91c7\u7528\u5217\u5f0f\u60e9\u7f5a\u4e0e\u622a\u65ad\u4f18\u5148\u8fc7\u6ee4\uff1b(3) \u6295\u673a\u70ed\u8bcd\u91c7\u6837\uff08SHVS\uff09\uff0c\u5728\u5c0f\u89c4\u6a21\u70ed\u8bcd\u96c6\u4e0a\u91c7\u6837\u5e76\u901a\u8fc7\u62d2\u7edd\u6821\u6b63\u786e\u4fdd\u6b63\u786e\u6027\uff0c\u5e76\u5229\u7528\u7b80\u5355\u6a21\u578b\u9009\u62e9\u6700\u4f18\u70ed\u8bcd\u96c6\u5927\u5c0f\u4ee5\u6700\u5927\u5316\u541e\u5410\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cSIMPLE \u6700\u591a\u53ef\u63d0\u5347\u7aef\u5230\u7aef\u541e\u5410\u91cf 96%\uff0c\u5e76\u5c06 P95 \u5ef6\u8fdf\u964d\u4f4e 20%\u201365%\u3002", "conclusion": "SIMPLE \u6210\u529f\u5c06\u91c7\u6837\u5f00\u9500\u964d\u81f3\u53ef\u5ffd\u7565\u6c34\u5e73\uff0c\u4e14\u517c\u5bb9\u73b0\u6709\u6570\u636e\u5e73\u9762\u4f18\u5316\uff0c\u65e0\u9700\u7528\u6237\u6539\u52a8\u4ee3\u7801\uff0c\u80fd\u968f\u672a\u6765 GPU \u53d1\u5c55\u6301\u7eed\u83b7\u5f97\u6269\u5c55\u6536\u76ca\u3002"}}
{"id": "2512.00029", "categories": ["cs.NI", "cs.DC"], "pdf": "https://arxiv.org/pdf/2512.00029", "abs": "https://arxiv.org/abs/2512.00029", "authors": ["Andreas Kouloumpris", "Georgios L. Stavrinides", "Maria K. Michael", "Theocharis Theocharides"], "title": "An optimization framework for task allocation in the edge/hub/cloud paradigm", "comment": "This version of the manuscript has been accepted for publication in Future Generation Computer Systems after peer review (Author Accepted Manuscript). It is not the final published version (Version of Record) and does not reflect any post-acceptance improvements. The Version of Record is available online at https://doi.org/10.1016/j.future.2024.02.005", "summary": "With the advent of the Internet of Things (IoT), novel critical applications have emerged that leverage the edge/hub/cloud paradigm, which diverges from the conventional edge computing perspective. A growing number of such applications require a streamlined architecture for their effective execution, often comprising a single edge device with sensing capabilities, a single hub device (e.g., a laptop or smartphone) for managing and assisting the edge device, and a more computationally capable cloud server. Typical examples include the utilization of an unmanned aerial vehicle (UAV) for critical infrastructure inspection or a wearable biomedical device (e.g., a smartwatch) for remote patient monitoring. Task allocation in this streamlined architecture is particularly challenging, due to the computational, communication, and energy limitations of the devices at the network edge. Consequently, there is a need for a comprehensive framework that can address the specific task allocation problem optimally and efficiently. To this end, we propose a complete, binary integer linear programming (BILP) based formulation for an application-driven design-time approach, capable of providing an optimal task allocation in the targeted edge/hub/cloud environment. The proposed method minimizes the desired objective, either the overall latency or overall energy consumption, while considering several crucial parameters and constraints often overlooked in related literature. We evaluate our framework using a real-world use-case scenario, as well as appropriate synthetic benchmarks. Our extensive experimentation reveals that the proposed approach yields optimal and scalable results, enabling efficient design space exploration for different applications and computational devices.", "AI": {"tldr": "\u672c\u6587\u9488\u5bf9\u8fb9\u7f18/\u4e2d\u5fc3/\u4e91\u4e09\u5c42\u67b6\u6784\u4e2d\u7684\u4efb\u52a1\u5206\u914d\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u4e8c\u5143\u6574\u6570\u7ebf\u6027\u89c4\u5212\uff08BILP\uff09\u7684\u4f18\u5316\u65b9\u6cd5\uff0c\u4ee5\u6700\u5c0f\u5316\u6574\u4f53\u5ef6\u8fdf\u6216\u80fd\u8017\uff0c\u5e76\u901a\u8fc7\u771f\u5b9e\u548c\u5408\u6210\u7528\u4f8b\u9a8c\u8bc1\u4e86\u5176\u6700\u4f18\u6027\u548c\u53ef\u6269\u5c55\u6027\u3002", "motivation": "\u968f\u7740\u7269\u8054\u7f51\u7684\u53d1\u5c55\uff0c\u8bb8\u591a\u5173\u952e\u5e94\u7528\u91c7\u7528\u7531\u5355\u4e00\u8fb9\u7f18\u8bbe\u5907\u3001\u5355\u4e00\u4e2d\u5fc3\u8bbe\u5907\u548c\u4e91\u670d\u52a1\u5668\u7ec4\u6210\u7684\u7cbe\u7b80\u67b6\u6784\u3002\u7531\u4e8e\u8fb9\u7f18\u8bbe\u5907\u5728\u8ba1\u7b97\u3001\u901a\u4fe1\u548c\u80fd\u91cf\u65b9\u9762\u7684\u9650\u5236\uff0c\u5982\u4f55\u5728\u6b64\u7c7b\u67b6\u6784\u4e2d\u9ad8\u6548\u5206\u914d\u4efb\u52a1\u6210\u4e3a\u4e00\u4e2a\u6311\u6218\uff0c\u4e9f\u9700\u4e00\u4e2a\u80fd\u7efc\u5408\u8003\u8651\u591a\u79cd\u7ea6\u675f\u5e76\u5b9e\u73b0\u6700\u4f18\u4efb\u52a1\u5206\u914d\u7684\u6846\u67b6\u3002", "method": "\u4f5c\u8005\u63d0\u51fa\u4e00\u79cd\u5b8c\u6574\u7684\u3001\u57fa\u4e8e\u4e8c\u5143\u6574\u6570\u7ebf\u6027\u89c4\u5212\uff08BILP\uff09\u7684\u5efa\u6a21\u65b9\u6cd5\uff0c\u7528\u4e8e\u5728\u8bbe\u8ba1\u9636\u6bb5\u8fdb\u884c\u5e94\u7528\u9a71\u52a8\u7684\u4efb\u52a1\u5206\u914d\uff0c\u76ee\u6807\u662f\u6700\u5c0f\u5316\u6574\u4f53\u5ef6\u8fdf\u6216\u603b\u80fd\u8017\uff0c\u5e76\u7eb3\u5165\u4e86\u4ee5\u5f80\u7814\u7a76\u5e38\u5ffd\u7565\u7684\u5173\u952e\u53c2\u6570\u4e0e\u7ea6\u675f\u3002", "result": "\u901a\u8fc7\u771f\u5b9e\u5e94\u7528\u573a\u666f\u548c\u5408\u6210\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5b9e\u9a8c\u8868\u660e\u8be5\u65b9\u6cd5\u80fd\u591f\u751f\u6210\u6700\u4f18\u4e14\u53ef\u6269\u5c55\u7684\u4efb\u52a1\u5206\u914d\u65b9\u6848\uff0c\u6709\u6548\u652f\u6301\u4e0d\u540c\u5e94\u7528\u548c\u8ba1\u7b97\u8bbe\u5907\u7684\u8bbe\u8ba1\u7a7a\u95f4\u63a2\u7d22\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684BILP\u6846\u67b6\u80fd\u9ad8\u6548\u3001\u6700\u4f18\u5730\u89e3\u51b3\u8fb9\u7f18/\u4e2d\u5fc3/\u4e91\u67b6\u6784\u4e0b\u7684\u4efb\u52a1\u5206\u914d\u95ee\u9898\uff0c\u5728\u6ee1\u8db3\u591a\u79cd\u5b9e\u9645\u7ea6\u675f\u7684\u540c\u65f6\uff0c\u517c\u987e\u5ef6\u8fdf\u4e0e\u80fd\u8017\u4f18\u5316\uff0c\u5177\u6709\u826f\u597d\u7684\u5b9e\u7528\u6027\u548c\u6269\u5c55\u6027\u3002"}}
{"id": "2512.00614", "categories": ["cs.MA", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.00614", "abs": "https://arxiv.org/abs/2512.00614", "authors": ["Goutham Nalagatla"], "title": "Hierarchical Decentralized Multi-Agent Coordination with Privacy-Preserving Knowledge Sharing: Extending AgentNet for Scalable Autonomous Systems", "comment": "6 pages, 4 figures", "summary": "Decentralized multi-agent systems have shown promise in enabling autonomous collaboration among LLM-based agents. While AgentNet demonstrated the feasibility of fully decentralized coordination through dynamic DAG topologies, several limitations remain: scalability challenges with large agent populations, communication overhead, lack of privacy guarantees, and suboptimal resource allocation. We propose AgentNet++, a hierarchical decentralized framework that extends AgentNet with multilevel agent organization, privacy-preserving knowledge sharing via differential privacy and secure aggregation, adaptive resource management, and theoretical convergence guarantees. Our approach introduces cluster-based hierarchies where agents self-organize into specialized groups, enabling efficient task routing and knowledge distillation while maintaining full decentralization. We provide formal analysis of convergence properties and privacy bounds, and demonstrate through extensive experiments on complex multi-agent tasks that AgentNet++ achieves 23% higher task completion rates, 40% reduction in communication overhead, and maintains strong privacy guarantees compared to AgentNet and other baselines. Our framework scales effectively to 1000+ agents while preserving the emergent intelligence properties of the original AgentNet.", "AI": {"tldr": "AgentNet++ \u662f\u4e00\u79cd\u5206\u5c42\u53bb\u4e2d\u5fc3\u5316\u591a\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u5728 AgentNet \u57fa\u7840\u4e0a\u5f15\u5165\u96c6\u7fa4\u5206\u5c42\u3001\u5dee\u5206\u9690\u79c1\u4e0e\u5b89\u5168\u805a\u5408\u3001\u81ea\u9002\u5e94\u8d44\u6e90\u7ba1\u7406\u53ca\u6536\u655b\u6027\u4fdd\u8bc1\uff0c\u663e\u8457\u63d0\u5347\u4efb\u52a1\u5b8c\u6210\u7387\u3001\u964d\u4f4e\u901a\u4fe1\u5f00\u9500\u5e76\u4fdd\u969c\u9690\u79c1\u3002", "motivation": "\u73b0\u6709\u53bb\u4e2d\u5fc3\u5316\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\uff08\u5982 AgentNet\uff09\u5728\u5927\u89c4\u6a21\u90e8\u7f72\u65f6\u9762\u4e34\u53ef\u6269\u5c55\u6027\u5dee\u3001\u901a\u4fe1\u5f00\u9500\u9ad8\u3001\u7f3a\u4e4f\u9690\u79c1\u4fdd\u969c\u548c\u8d44\u6e90\u5206\u914d\u4f4e\u6548\u7b49\u95ee\u9898\uff0c\u4e9f\u9700\u6539\u8fdb\u3002", "method": "\u63d0\u51fa AgentNet++ \u6846\u67b6\uff0c\u91c7\u7528\u57fa\u4e8e\u96c6\u7fa4\u7684\u5206\u5c42\u7ed3\u6784\u4f7f\u667a\u80fd\u4f53\u81ea\u7ec4\u7ec7\u4e3a\u4e13\u4e1a\u5c0f\u7ec4\uff0c\u7ed3\u5408\u5dee\u5206\u9690\u79c1\u4e0e\u5b89\u5168\u805a\u5408\u5b9e\u73b0\u9690\u79c1\u4fdd\u62a4\u7684\u77e5\u8bc6\u5171\u4eab\uff0c\u5e76\u5f15\u5165\u81ea\u9002\u5e94\u8d44\u6e90\u7ba1\u7406\u673a\u5236\uff1b\u540c\u65f6\u63d0\u4f9b\u6536\u655b\u6027\u548c\u9690\u79c1\u8fb9\u754c\u7684\u5f62\u5f0f\u5316\u5206\u6790\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cAgentNet++ \u76f8\u6bd4 AgentNet \u548c\u5176\u4ed6\u57fa\u7ebf\u65b9\u6cd5\uff0c\u4efb\u52a1\u5b8c\u6210\u7387\u63d0\u9ad8 23%\uff0c\u901a\u4fe1\u5f00\u9500\u51cf\u5c11 40%\uff0c\u5e76\u4fdd\u6301\u5f3a\u9690\u79c1\u4fdd\u969c\uff1b\u8be5\u6846\u67b6\u53ef\u6709\u6548\u6269\u5c55\u81f3 1000+ \u667a\u80fd\u4f53\u3002", "conclusion": "AgentNet++ \u5728\u7ef4\u6301\u5b8c\u5168\u53bb\u4e2d\u5fc3\u5316\u548c\u6d8c\u73b0\u667a\u80fd\u7279\u6027\u7684\u540c\u65f6\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u53ef\u6269\u5c55\u6027\u3001\u901a\u4fe1\u6548\u7387\u3001\u9690\u79c1\u4fdd\u62a4\u548c\u8d44\u6e90\u5206\u914d\u7b49\u5173\u952e\u6311\u6218\uff0c\u4e3a\u5927\u89c4\u6a21 LLM \u667a\u80fd\u4f53\u534f\u4f5c\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848\u3002"}}
{"id": "2512.00902", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2512.00902", "abs": "https://arxiv.org/abs/2512.00902", "authors": ["Yebo Wu", "Jingguang Li", "Zhijiang Guo", "Li Li"], "title": "Elastic Mixture of Rank-Wise Experts for Knowledge Reuse in Federated Fine-Tuning", "comment": null, "summary": "Federated fine-tuning offers a promising solution for adapting Large Language Models (LLMs) to downstream tasks while safeguarding data privacy. However, its high computational and communication demands hinder its deployment on resource-constrained devices. In this paper, we propose SmartFed, a resource-efficient federated fine-tuning framework. SmartFed intelligently reuses knowledge embedded in existing LoRA modules, eliminating the need for expensive training from scratch when adapting LLMs to new tasks. To effectively exploit this knowledge and ensure scalability, we introduce the Mixture of Rank-Wise Experts (MoRE). MoRE decomposes LoRA modules into fine-grained rank-level experts. These experts are selectively activated and combined based on input semantics and resource budgets. Moreover, to optimize resource utilization, we present the Elastic Expert Quota Allocation (EEQA). EEQA adaptively allocates expert capacity across parameter matrices based on their contribution to model performance, focusing computing resources on the critical experts. Extensive evaluations across multiple benchmarks demonstrate that SmartFed significantly outperforms existing methods in model performance and training efficiency.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faSmartFed\uff0c\u4e00\u79cd\u9ad8\u6548\u5229\u7528\u8d44\u6e90\u7684\u8054\u90a6\u5fae\u8c03\u6846\u67b6\uff0c\u901a\u8fc7\u590d\u7528\u73b0\u6709LoRA\u6a21\u5757\u4e2d\u7684\u77e5\u8bc6\uff0c\u5e76\u5f15\u5165MoRE\u548cEEQA\u673a\u5236\uff0c\u5728\u4fdd\u8bc1\u6027\u80fd\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u8bad\u7ec3\u6548\u7387\u3002", "motivation": "\u8054\u90a6\u5fae\u8c03\u867d\u80fd\u4fdd\u62a4\u6570\u636e\u9690\u79c1\uff0c\u4f46\u5176\u9ad8\u6602\u7684\u8ba1\u7b97\u4e0e\u901a\u4fe1\u5f00\u9500\u9650\u5236\u4e86\u5728\u8d44\u6e90\u53d7\u9650\u8bbe\u5907\u4e0a\u7684\u5e94\u7528\u3002", "method": "\u63d0\u51faSmartFed\u6846\u67b6\uff0c\u5305\u62ec\uff1a1\uff09\u590d\u7528\u5df2\u6709LoRA\u6a21\u5757\u4e2d\u7684\u77e5\u8bc6\uff1b2\uff09\u8bbe\u8ba1Mixture of Rank-Wise Experts (MoRE)\uff0c\u5c06LoRA\u5206\u89e3\u4e3a\u7ec6\u7c92\u5ea6\u7684\u79e9\u7ea7\u4e13\u5bb6\uff0c\u6309\u8f93\u5165\u8bed\u4e49\u548c\u8d44\u6e90\u9884\u7b97\u52a8\u6001\u6fc0\u6d3b\uff1b3\uff09\u5f15\u5165Elastic Expert Quota Allocation (EEQA)\uff0c\u6839\u636e\u5404\u53c2\u6570\u77e9\u9635\u5bf9\u6027\u80fd\u7684\u8d21\u732e\u81ea\u9002\u5e94\u5206\u914d\u4e13\u5bb6\u5bb9\u91cf\u3002", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cSmartFed\u5728\u6a21\u578b\u6027\u80fd\u548c\u8bad\u7ec3\u6548\u7387\u65b9\u9762\u5747\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "SmartFed\u901a\u8fc7\u667a\u80fd\u590d\u7528LoRA\u77e5\u8bc6\u5e76\u7ed3\u5408MoRE\u4e0eEEQA\u673a\u5236\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u8054\u90a6\u5fae\u8c03\u4e2d\u8d44\u6e90\u6d88\u8017\u9ad8\u7684\u95ee\u9898\uff0c\u4e3a\u5728\u8d44\u6e90\u53d7\u9650\u8bbe\u5907\u4e0a\u90e8\u7f72\u5927\u8bed\u8a00\u6a21\u578b\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848\u3002"}}
{"id": "2512.00020", "categories": ["cs.AR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.00020", "abs": "https://arxiv.org/abs/2512.00020", "authors": ["Guang Yang", "Wei Zheng", "Xiang Chen", "Dong Liang", "Peng Hu", "Yukui Yang", "Shaohang Peng", "Zhenghan Li", "Jiahui Feng", "Xiao Wei", "Kexin Sun", "Deyuan Ma", "Haotian Cheng", "Yiheng Shen", "Xing Hu", "Terry Yue Zhuo", "David Lo"], "title": "Large Language Model for Verilog Code Generation: Literature Review and the Road Ahead", "comment": "WIP", "summary": "Code generation has emerged as a critical research area at the intersection of Software Engineering (SE) and Artificial Intelligence (AI), attracting significant attention from both academia and industry. Within this broader landscape, Verilog, as a representative hardware description language (HDL), plays a fundamental role in digital circuit design and verification, making its automated generation particularly significant for Electronic Design Automation (EDA). Consequently, recent research has increasingly focused on applying Large Language Models (LLMs) to Verilog code generation, particularly at the Register Transfer Level (RTL), exploring how these AI-driven techniques can be effectively integrated into hardware design workflows. Despite substantial research efforts have explored LLM applications in this domain, a comprehensive survey synthesizing these developments remains absent from the literature. This review fill addresses this gap by providing a systematic literature review of LLM-based methods for Verilog code generation, examining their effectiveness, limitations, and potential for advancing automated hardware design. The review encompasses research work from conferences and journals in the fields of SE, AI, and EDA, encompassing 70 papers published on venues, along with 32 high-quality preprint papers, bringing the total to 102 papers. By answering four key research questions, we aim to (1) identify the LLMs used for Verilog generation, (2) examine the datasets and metrics employed in evaluation, (3) categorize the techniques proposed for Verilog generation, and (4) analyze LLM alignment approaches for Verilog generation. Based on our findings, we have identified a series of limitations of existing studies. Finally, we have outlined a roadmap highlighting potential opportunities for future research endeavors in LLM-assisted hardware design.", "AI": {"tldr": "\u672c\u6587\u7cfb\u7edf\u7efc\u8ff0\u4e86\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684Verilog\u4ee3\u7801\u751f\u6210\u7814\u7a76\uff0c\u6db5\u76d6102\u7bc7\u8bba\u6587\uff0c\u56f4\u7ed5\u56db\u4e2a\u5173\u952e\u95ee\u9898\u5206\u6790\u4e86\u6240\u7528\u6a21\u578b\u3001\u6570\u636e\u96c6\u3001\u8bc4\u4f30\u6307\u6807\u3001\u6280\u672f\u5206\u7c7b\u53ca\u5bf9\u9f50\u65b9\u6cd5\uff0c\u5e76\u6307\u51fa\u73b0\u6709\u7814\u7a76\u7684\u5c40\u9650\u6027\uff0c\u63d0\u51fa\u4e86\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002", "motivation": "\u5c3d\u7ba1\u5927\u8bed\u8a00\u6a21\u578b\u5728Verilog\u4ee3\u7801\u751f\u6210\u9886\u57df\u53d7\u5230\u5e7f\u6cdb\u5173\u6ce8\uff0c\u4f46\u76ee\u524d\u5c1a\u7f3a\u4e4f\u5bf9\u8be5\u65b9\u5411\u7684\u7cfb\u7edf\u6027\u7efc\u8ff0\u3002\u672c\u6587\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\uff0c\u4e3a\u81ea\u52a8\u786c\u4ef6\u8bbe\u8ba1\u63d0\u4f9b\u5168\u9762\u7684\u7814\u7a76\u6982\u89c8\u3002", "method": "\u4f5c\u8005\u5bf9\u8f6f\u4ef6\u5de5\u7a0b\u3001\u4eba\u5de5\u667a\u80fd\u548c\u7535\u5b50\u8bbe\u8ba1\u81ea\u52a8\u5316\u9886\u57df\u7684\u4f1a\u8bae\u3001\u671f\u520a\u53ca\u9ad8\u8d28\u91cf\u9884\u5370\u672c\u4e2d\u7684102\u7bc7\u76f8\u5173\u8bba\u6587\u8fdb\u884c\u4e86\u7cfb\u7edf\u6027\u6587\u732e\u7efc\u8ff0\uff0c\u56f4\u7ed5\u56db\u4e2a\u7814\u7a76\u95ee\u9898\u8fdb\u884c\u5f52\u7eb3\u4e0e\u5206\u6790\u3002", "result": "\u8bc6\u522b\u51fa\u5f53\u524d\u7814\u7a76\u6240\u91c7\u7528\u7684\u4e3b\u8981\u5927\u8bed\u8a00\u6a21\u578b\u3001\u5e38\u7528\u6570\u636e\u96c6\u4e0e\u8bc4\u4f30\u6307\u6807\uff0c\u5bf9Verilog\u751f\u6210\u6280\u672f\u8fdb\u884c\u4e86\u5206\u7c7b\uff0c\u5e76\u5206\u6790\u4e86\u9488\u5bf9Verilog\u7684\u6a21\u578b\u5bf9\u9f50\u65b9\u6cd5\uff0c\u540c\u65f6\u63ed\u793a\u4e86\u73b0\u6709\u5de5\u4f5c\u7684\u82e5\u5e72\u5c40\u9650\u6027\u3002", "conclusion": "\u73b0\u6709\u57fa\u4e8eLLM\u7684Verilog\u751f\u6210\u7814\u7a76\u867d\u53d6\u5f97\u8fdb\u5c55\uff0c\u4f46\u4ecd\u5b58\u5728\u8bf8\u591a\u6311\u6218\uff1b\u6587\u7ae0\u63d0\u51fa\u4e86\u4e00\u6761\u672a\u6765\u7814\u7a76\u8def\u7ebf\u56fe\uff0c\u4ee5\u63a8\u52a8LLM\u5728\u786c\u4ef6\u8bbe\u8ba1\u81ea\u52a8\u5316\u4e2d\u7684\u6df1\u5165\u5e94\u7528\u3002"}}
{"id": "2512.00215", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2512.00215", "abs": "https://arxiv.org/abs/2512.00215", "authors": ["Mohammad Abdollahi", "Khandaker Rifah Tasnia", "Soumit Kanti Saha", "Jinqiu Yang", "Song Wang", "Hadi Hemmati"], "title": "Demystifying Errors in LLM Reasoning Traces: An Empirical Study of Code Execution Simulation", "comment": null, "summary": "Understanding a program's runtime reasoning behavior, meaning how intermediate states and control flows lead to final execution results, is essential for reliable code generation, debugging, and automated reasoning. Although large language models (LLMs) can accurately predict program outputs, most prior work has focused on output accuracy and performance, treating reasoning as a black box. As a result, little is known about the structure or failure modes of their reasoning traces. To address this gap, we conduct the first empirical study on runtime behavior inference with reasoning LLMs, aiming to uncover and characterize errors in their reasoning traces. We curate a benchmark from HumanEval Plus and LiveCodeBench, containing 427 code snippets. For each snippet, we test three input types: regular, edge, and invalid. Twelve input values are selected per snippet, each paired with its ground-truth execution result. We evaluate four state-of-the-art reasoning LLMs. Our results show that these models reach accuracies between 85 percent and 98 percent across input types. We also analyze the produced reasoning traces and develop a taxonomy with nine categories of inference errors. Finally, we explore tool-augmented reasoning. Using failures in the Computation Errors category as a case study, our experiments show that this approach corrects 58 percent of such errors, demonstrating the potential of tool support for improving LLM reasoning.", "AI": {"tldr": "\u8be5\u8bba\u6587\u9996\u6b21\u5bf9\u63a8\u7406\u578b\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u7a0b\u5e8f\u8fd0\u884c\u65f6\u884c\u4e3a\u63a8\u65ad\u4e2d\u7684\u8868\u73b0\u8fdb\u884c\u4e86\u5b9e\u8bc1\u7814\u7a76\uff0c\u6784\u5efa\u4e86\u4e00\u4e2a\u5305\u542b427\u4e2a\u4ee3\u7801\u7247\u6bb5\u7684\u57fa\u51c6\u6570\u636e\u96c6\uff0c\u5e76\u901a\u8fc7\u4e09\u79cd\u8f93\u5165\u7c7b\u578b\u8bc4\u4f30\u4e86\u56db\u4e2a\u5148\u8fdb\u6a21\u578b\u3002\u7814\u7a76\u53d1\u73b0\u6a21\u578b\u51c6\u786e\u7387\u572885%\u81f398%\u4e4b\u95f4\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u4e2a\u5305\u542b\u4e5d\u7c7b\u63a8\u7406\u9519\u8bef\u7684\u5206\u7c7b\u4f53\u7cfb\uff1b\u6b64\u5916\uff0c\u901a\u8fc7\u5de5\u5177\u589e\u5f3a\u63a8\u7406\u53ef\u4fee\u590d58%\u7684\u8ba1\u7b97\u7c7b\u9519\u8bef\u3002", "motivation": "\u5f53\u524d\u5927\u591a\u6570\u5173\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u7814\u7a76\u805a\u7126\u4e8e\u8f93\u51fa\u51c6\u786e\u6027\u548c\u6027\u80fd\uff0c\u5c06\u63a8\u7406\u8fc7\u7a0b\u89c6\u4e3a\u9ed1\u7bb1\uff0c\u7f3a\u4e4f\u5bf9\u5176\u63a8\u7406\u8f68\u8ff9\u7ed3\u6784\u548c\u5931\u8d25\u6a21\u5f0f\u7684\u7406\u89e3\u3002\u4e3a\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\uff0c\u4f5c\u8005\u65e8\u5728\u6df1\u5165\u63a2\u7a76LLMs\u5728\u7a0b\u5e8f\u8fd0\u884c\u65f6\u63a8\u7406\u884c\u4e3a\u4e2d\u7684\u9519\u8bef\u7279\u5f81\u3002", "method": "\u4f5c\u8005\u4eceHumanEval Plus\u548cLiveCodeBench\u4e2d\u6784\u5efa\u4e86\u4e00\u4e2a\u5305\u542b427\u4e2a\u4ee3\u7801\u7247\u6bb5\u7684\u57fa\u51c6\uff0c\u6bcf\u4e2a\u7247\u6bb5\u914d\u4ee5\u5e38\u89c4\u3001\u8fb9\u754c\u548c\u65e0\u6548\u4e09\u7c7b\u8f93\u5165\uff0c\u6bcf\u7c7b\u9009\u53d612\u4e2a\u5177\u4f53\u8f93\u5165\u503c\u53ca\u5176\u771f\u5b9e\u6267\u884c\u7ed3\u679c\u3002\u968f\u540e\uff0c\u8bc4\u4f30\u4e86\u56db\u4e2a\u6700\u5148\u8fdb\u7684\u63a8\u7406\u578bLLM\uff0c\u5e76\u5bf9\u5176\u751f\u6210\u7684\u63a8\u7406\u8f68\u8ff9\u8fdb\u884c\u5206\u6790\uff0c\u5f52\u7eb3\u51fa\u4e5d\u7c7b\u63a8\u7406\u9519\u8bef\u3002\u6700\u540e\uff0c\u4ee5\u201c\u8ba1\u7b97\u9519\u8bef\u201d\u7c7b\u522b\u4e3a\u4f8b\uff0c\u63a2\u7d22\u4e86\u5de5\u5177\u589e\u5f3a\u63a8\u7406\u7684\u6709\u6548\u6027\u3002", "result": "\u56db\u4e2a\u88ab\u6d4bLLM\u5728\u4e0d\u540c\u8f93\u5165\u7c7b\u578b\u4e0b\u7684\u51c6\u786e\u7387\u4ecb\u4e8e85%\u523098%\u4e4b\u95f4\uff1b\u4f5c\u8005\u6784\u5efa\u4e86\u4e00\u4e2a\u5305\u542b\u4e5d\u7c7b\u63a8\u7406\u9519\u8bef\u7684\u5206\u7c7b\u4f53\u7cfb\uff1b\u5728\u8ba1\u7b97\u9519\u8bef\u7c7b\u522b\u4e2d\uff0c\u5de5\u5177\u589e\u5f3a\u65b9\u6cd5\u6210\u529f\u4fee\u6b63\u4e8658%\u7684\u9519\u8bef\u3002", "conclusion": "\u63a8\u7406\u578b\u5927\u8bed\u8a00\u6a21\u578b\u867d\u5728\u7a0b\u5e8f\u8f93\u51fa\u9884\u6d4b\u4e0a\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5176\u63a8\u7406\u8f68\u8ff9\u4ecd\u5b58\u5728\u7cfb\u7edf\u6027\u9519\u8bef\uff1b\u901a\u8fc7\u5de5\u5177\u589e\u5f3a\u53ef\u6709\u6548\u63d0\u5347\u5176\u63a8\u7406\u53ef\u9760\u6027\uff0c\u8868\u660e\u7ed3\u5408\u5916\u90e8\u5de5\u5177\u662f\u6539\u8fdbLLM\u63a8\u7406\u80fd\u529b\u7684\u91cd\u8981\u65b9\u5411\u3002"}}
{"id": "2512.00039", "categories": ["cs.NI", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2512.00039", "abs": "https://arxiv.org/abs/2512.00039", "authors": ["Tasnim Ahmed", "Siana Rizwan", "Naveed Ejaz", "Salimur Choudhury"], "title": "LM4Opt-RA: A Multi-Candidate LLM Framework with Structured Ranking for Automating Network Resource Allocation", "comment": null, "summary": "Building on advancements in Large Language Models (LLMs), we can tackle complex analytical and mathematical reasoning tasks requiring nuanced contextual understanding. A prime example of such complex tasks is modelling resource allocation optimization in networks, which extends beyond translating natural language inputs into mathematical equations or Linear Programming (LP), Integer Linear Programming (ILP), and Mixed-Integer Linear Programming (MILP) models. However, existing benchmarks and datasets cannot address the complexities of such problems with dynamic environments, interdependent variables, and heterogeneous constraints. To address this gap, we introduce NL4RA, a curated dataset comprising 50 resource allocation optimization problems formulated as LP, ILP, and MILP. We then evaluate the performance of well-known open-source LLMs with varying parameter counts. To enhance existing LLM based methods, we introduce LM4Opt RA, a multi candidate framework that applies diverse prompting strategies such as direct, few shot, and chain of thought, combined with a structured ranking mechanism to improve accuracy. We identified discrepancies between human judgments and automated scoring such as ROUGE, BLEU, or BERT scores. However, human evaluation is time-consuming and requires specialized expertise, making it impractical for a fully automated end-to-end framework. To quantify the difference between LLM-generated responses and ground truth, we introduce LLM-Assisted Mathematical Evaluation (LAME), an automated metric designed for mathematical formulations. Using LM4Opt-RA, Llama-3.1-70B achieved a LAME score of 0.8007, outperforming other models by a significant margin, followed closely by Llama-3.1-8B. While baseline LLMs demonstrate considerable promise, they still lag behind human expertise; our proposed method surpasses these baselines regarding LAME and other metrics.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86NL4RA\u6570\u636e\u96c6\u548cLM4Opt-RA\u6846\u67b6\uff0c\u7528\u4e8e\u63d0\u5347\u5927\u8bed\u8a00\u6a21\u578b\u5728\u8d44\u6e90\u5206\u914d\u4f18\u5316\u95ee\u9898\uff08\u5982LP/ILP/MILP\uff09\u4e2d\u7684\u5efa\u6a21\u80fd\u529b\uff0c\u5e76\u5f15\u5165\u65b0\u8bc4\u4f30\u6307\u6807LAME\u4ee5\u66f4\u51c6\u786e\u8861\u91cf\u6a21\u578b\u751f\u6210\u7684\u6570\u5b66\u516c\u5f0f\u8d28\u91cf\u3002", "motivation": "\u73b0\u6709\u57fa\u51c6\u548c\u6570\u636e\u96c6\u65e0\u6cd5\u6709\u6548\u5e94\u5bf9\u5177\u6709\u52a8\u6001\u73af\u5883\u3001\u53d8\u91cf\u76f8\u4e92\u4f9d\u8d56\u548c\u5f02\u6784\u7ea6\u675f\u7684\u590d\u6742\u8d44\u6e90\u5206\u914d\u4f18\u5316\u95ee\u9898\uff0c\u4e14\u7f3a\u4e4f\u9002\u7528\u4e8e\u6570\u5b66\u5efa\u6a21\u4efb\u52a1\u7684\u81ea\u52a8\u5316\u8bc4\u4f30\u65b9\u6cd5\u3002", "method": "\u6784\u5efa\u5305\u542b50\u4e2a\u8d44\u6e90\u5206\u914d\u4f18\u5316\u95ee\u9898\u7684NL4RA\u6570\u636e\u96c6\uff1b\u63d0\u51faLM4Opt-RA\u591a\u5019\u9009\u6846\u67b6\uff0c\u7ed3\u5408\u591a\u79cd\u63d0\u793a\u7b56\u7565\uff08\u76f4\u63a5\u63d0\u793a\u3001\u5c11\u6837\u672c\u3001\u601d\u7ef4\u94fe\uff09\u4e0e\u7ed3\u6784\u5316\u6392\u5e8f\u673a\u5236\uff1b\u8bbe\u8ba1LLM\u8f85\u52a9\u7684\u6570\u5b66\u8bc4\u4f30\u6307\u6807LAME\u3002", "result": "\u5728LM4Opt-RA\u6846\u67b6\u4e0b\uff0cLlama-3.1-70B\u53d6\u5f970.8007\u7684LAME\u5206\u6570\uff0c\u663e\u8457\u4f18\u4e8e\u5176\u4ed6\u5f00\u6e90\u6a21\u578b\uff1b\u6240\u63d0\u65b9\u6cd5\u5728LAME\u53ca\u5176\u4ed6\u6307\u6807\u4e0a\u5747\u8d85\u8d8a\u57fa\u7ebf\u6a21\u578b\uff0c\u4f46\u4ecd\u4e0d\u53ca\u4eba\u7c7b\u4e13\u5bb6\u6c34\u5e73\u3002", "conclusion": "\u5927\u8bed\u8a00\u6a21\u578b\u5728\u590d\u6742\u6570\u5b66\u5efa\u6a21\u4efb\u52a1\u4e2d\u5c55\u73b0\u51fa\u6f5c\u529b\uff0c\u4f46\u9700\u7ed3\u5408\u4e13\u95e8\u8bbe\u8ba1\u7684\u63d0\u793a\u7b56\u7565\u3001\u6392\u5e8f\u673a\u5236\u548c\u8bc4\u4f30\u6307\u6807\u624d\u80fd\u6709\u6548\u63d0\u5347\u6027\u80fd\uff1bNL4RA\u548cLAME\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u57fa\u51c6\u548c\u5de5\u5177\u3002"}}
{"id": "2512.00231", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.00231", "abs": "https://arxiv.org/abs/2512.00231", "authors": ["Monique Louise Monteiro", "George G. Cabral", "Adriano L. I. OLiveira"], "title": "CodeFlowLM: Incremental Just-In-Time Defect Prediction with Pretrained Language Models and Exploratory Insights into Defect Localization", "comment": null, "summary": "This work introduces CodeFlowLM, an incremental learning framework for Just-In-Time Software Defect Prediction (JIT-SDP) that leverages pre-trained language models (PLMs). Unlike traditional online learners, CodeFlowLM employs continual fine-tuning to address concept drift, class imbalance, and verification latency without retraining from scratch. We evaluated encoder-only and encoder-decoder PLMs (notably CodeT5+ and UniXCoder) in JIT-SDP scenarios within and between projects, comparing them with the incremental baseline BORB. The results show that CodeFlowLM achieves up to 68% G-Mean gains, confirming its superior adaptability and robustness in evolving software environments. We further extend the analysis to Just-in-Time Defect Localization (JIT-DL), benchmarking Large Language Models (LLMs) such as GPT-5, Claude Sonnet 4.5, and Gemini 2.5 Pro against attention-based models. GPT-5 delivers comparable performance for Recall@20% and Effort@20% with higher stability, although attention-based methods retain an advantage in fine-grained ranking metrics (Top-k, IFA). A qualitative error analysis reveals that most false positives arise from (1) human-like conservative bias, (2) insufficient contextual information in diff-based prompts, and (3) potential dataset mislabeling in JIT-Defects4J. These findings highlight both the promise and the current limitations of LLM reasoning in defect localization. False negatives occur in smaller proportions. Overall, CodeFlowLM significantly advances the state of the art in incremental JIT-SDP, demonstrating superior adaptability and robustness in evolving software environments. Furthermore, our exploratory analysis of LLMs in JIT-DL not only benchmarks their performance against established attention-based models but also provides critical insights into the current limitations of prompt-based defect reasoning.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86CodeFlowLM\uff0c\u4e00\u79cd\u57fa\u4e8e\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\u7684\u589e\u91cf\u5b66\u4e60\u6846\u67b6\uff0c\u7528\u4e8e\u5373\u65f6\u8f6f\u4ef6\u7f3a\u9677\u9884\u6d4b\uff08JIT-SDP\uff09\u548c\u7f3a\u9677\u5b9a\u4f4d\uff08JIT-DL\uff09\u3002\u8be5\u65b9\u6cd5\u901a\u8fc7\u6301\u7eed\u5fae\u8c03\u5e94\u5bf9\u6982\u5ff5\u6f02\u79fb\u3001\u7c7b\u522b\u4e0d\u5e73\u8861\u7b49\u95ee\u9898\uff0c\u5728JIT-SDP\u4efb\u52a1\u4e2d\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\uff1b\u5728JIT-DL\u4efb\u52a1\u4e2d\uff0c\u5bf9\u5927\u8bed\u8a00\u6a21\u578b\uff08\u5982GPT-5\uff09\u4e0e\u6ce8\u610f\u529b\u6a21\u578b\u8fdb\u884c\u4e86\u5bf9\u6bd4\u5206\u6790\uff0c\u63ed\u793a\u4e86\u5f53\u524d\u63d0\u793a\u9a71\u52a8\u7f3a\u9677\u63a8\u7406\u7684\u4f18\u52bf\u4e0e\u5c40\u9650\u3002", "motivation": "\u4f20\u7edf\u5728\u7ebf\u5b66\u4e60\u65b9\u6cd5\u5728JIT-SDP\u4e2d\u96be\u4ee5\u6709\u6548\u5904\u7406\u6982\u5ff5\u6f02\u79fb\u3001\u7c7b\u522b\u4e0d\u5e73\u8861\u548c\u9a8c\u8bc1\u5ef6\u8fdf\u7b49\u95ee\u9898\uff0c\u4e14\u901a\u5e38\u9700\u8981\u4ece\u5934\u8bad\u7ec3\u3002\u540c\u65f6\uff0c\u5927\u8bed\u8a00\u6a21\u578b\u5728JIT\u7f3a\u9677\u5b9a\u4f4d\u4e2d\u7684\u80fd\u529b\u5c1a\u7f3a\u4e4f\u7cfb\u7edf\u8bc4\u4f30\u548c\u6df1\u5165\u7406\u89e3\u3002", "method": "\u63d0\u51faCodeFlowLM\u6846\u67b6\uff0c\u91c7\u7528\u6301\u7eed\u5fae\u8c03\u7b56\u7565\uff0c\u5229\u7528\u7f16\u7801\u5668-only\u548c\u7f16\u7801\u5668-\u89e3\u7801\u5668\u7ed3\u6784\u7684\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\uff08\u5982CodeT5+\u3001UniXCoder\uff09\u8fdb\u884c\u589e\u91cf\u5f0fJIT-SDP\uff1b\u540c\u65f6\u5728JIT-DL\u4efb\u52a1\u4e2d\uff0c\u5c06GPT-5\u7b49\u5927\u8bed\u8a00\u6a21\u578b\u4e0e\u6ce8\u610f\u529b\u6a21\u578b\u8fdb\u884c\u5bf9\u6bd4\uff0c\u5e76\u8fdb\u884c\u5b9a\u6027\u9519\u8bef\u5206\u6790\u3002", "result": "\u5728JIT-SDP\u4efb\u52a1\u4e2d\uff0cCodeFlowLM\u76f8\u6bd4\u57fa\u7ebfBORB\u6700\u9ad8\u63d0\u534768% G-Mean\uff1b\u5728JIT-DL\u4efb\u52a1\u4e2d\uff0cGPT-5\u5728Recall@20%\u548cEffort@20%\u6307\u6807\u4e0a\u8868\u73b0\u7a33\u5b9a\u4e14\u5177\u7ade\u4e89\u529b\uff0c\u4f46\u5728Top-k\u548cIFA\u7b49\u7ec6\u7c92\u5ea6\u6392\u5e8f\u6307\u6807\u4e0a\u4ecd\u900a\u4e8e\u6ce8\u610f\u529b\u6a21\u578b\u3002\u9519\u8bef\u5206\u6790\u63ed\u793a\u4e86\u5047\u9633\u6027\u4e3b\u8981\u6e90\u4e8e\u4fdd\u5b88\u504f\u89c1\u3001\u4e0a\u4e0b\u6587\u4e0d\u8db3\u548c\u6570\u636e\u6807\u6ce8\u95ee\u9898\u3002", "conclusion": "CodeFlowLM\u663e\u8457\u63a8\u8fdb\u4e86\u589e\u91cf\u5f0fJIT-SDP\u7684\u6700\u65b0\u6c34\u5e73\uff0c\u5c55\u73b0\u51fa\u4f18\u5f02\u7684\u9002\u5e94\u6027\u548c\u9c81\u68d2\u6027\uff1b\u5bf9LLM\u5728JIT-DL\u4e2d\u7684\u63a2\u7d22\u4e0d\u4ec5\u63d0\u4f9b\u4e86\u6027\u80fd\u57fa\u51c6\uff0c\u4e5f\u63ed\u793a\u4e86\u5f53\u524d\u57fa\u4e8e\u63d0\u793a\u7684\u7f3a\u9677\u63a8\u7406\u65b9\u6cd5\u7684\u5173\u952e\u5c40\u9650\u3002"}}
{"id": "2512.01010", "categories": ["cs.MA", "cs.AI", "cs.LG", "cs.SE", "physics.comp-ph", "physics.flu-dyn"], "pdf": "https://arxiv.org/pdf/2512.01010", "abs": "https://arxiv.org/abs/2512.01010", "authors": ["Vansh Sharma", "Venkat Raman"], "title": "Chain of Unit-Physics: A Primitive-Centric Approach to Scientific Code Synthesis", "comment": null, "summary": "Agentic large language models are proposed as autonomous code generators for scientific computing, yet their reliability in high-stakes problems remains unclear. Developing computational scientific software from natural-language queries remains challenging broadly due to (a) sparse representation of domain codes during training and (b) the limited feasibility of RLHF with a small expert community. To address these limitations, this work conceptualizes an inverse approach to code design, embodied in the Chain of Unit-Physics framework: a first-principles (or primitives)-centric, multi-agent system in which human expert knowledge is encoded as unit-physics tests that explicitly constrain code generation. The framework is evaluated on a nontrivial combustion task, used here as a representative benchmark for scientific problem with realistic physical constraints. Closed-weight systems and code-focused agentic variants fail to produce correct end-to-end solvers, despite tool and web access, exhibiting four recurrent error classes: interface (syntax/API) hallucinations, overconfident assumptions, numerical/physical incoherence, and configuration fragility. Open-weight models with chain-of-thought (CoT) decoding reduce interface errors but still yield incorrect solutions. On the benchmark task, the proposed framework converges within 5-6 iterations, matches the human-expert implementation (mean error of $3.1\\times10^{-3}$ %), with a $\\sim$33.4 % faster runtime and a $\\sim$30 % efficient memory usage at a cost comparable to mid-sized commercial APIs, yielding a practical template for physics-grounded scientific code generation. As datasets and models evolve, zero-shot code accuracy will improve; however, the Chain of Unit-Physics framework goes further by embedding first-principles analysis that is foundational to scientific codes.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u201c\u5355\u5143\u7269\u7406\u94fe\u201d\uff08Chain of Unit-Physics\uff09\u7684\u9006\u5411\u4ee3\u7801\u751f\u6210\u6846\u67b6\uff0c\u901a\u8fc7\u5c06\u4e13\u5bb6\u77e5\u8bc6\u7f16\u7801\u4e3a\u57fa\u4e8e\u7b2c\u4e00\u6027\u539f\u7406\u7684\u5355\u5143\u7269\u7406\u6d4b\u8bd5\uff0c\u7ea6\u675f\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u751f\u6210\u79d1\u5b66\u8ba1\u7b97\u4ee3\u7801\u3002\u5728\u71c3\u70e7\u4efb\u52a1\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u80fd\u9ad8\u6548\u751f\u6210\u4e0e\u4eba\u7c7b\u4e13\u5bb6\u5b9e\u73b0\u9ad8\u5ea6\u4e00\u81f4\u3001\u6027\u80fd\u66f4\u4f18\u7684\u6c42\u89e3\u5668\uff0c\u514b\u670d\u4e86\u73b0\u6709\u5927\u8bed\u8a00\u6a21\u578b\u5728\u79d1\u5b66\u7f16\u7a0b\u4e2d\u7684\u5e38\u89c1\u9519\u8bef\u3002", "motivation": "\u5f53\u524d\u9762\u5411\u79d1\u5b66\u8ba1\u7b97\u7684\u81ea\u4e3b\u4ee3\u7801\u751f\u6210\u5927\u6a21\u578b\u5b58\u5728\u53ef\u9760\u6027\u95ee\u9898\uff0c\u4e3b\u8981\u53d7\u9650\u4e8e\u8bad\u7ec3\u6570\u636e\u4e2d\u9886\u57df\u4ee3\u7801\u7a00\u758f\u4ee5\u53ca\u5c0f\u89c4\u6a21\u4e13\u5bb6\u7fa4\u4f53\u96be\u4ee5\u652f\u6491\u5f3a\u5316\u5b66\u4e60\u4eba\u7c7b\u53cd\u9988\uff08RLHF\uff09\u3002\u56e0\u6b64\uff0c\u4e9f\u9700\u4e00\u79cd\u80fd\u878d\u5408\u4e13\u5bb6\u5148\u9a8c\u77e5\u8bc6\u3001\u786e\u4fdd\u7269\u7406\u4e00\u81f4\u6027\u7684\u65b0\u8303\u5f0f\u3002", "method": "\u63d0\u51fa\u201c\u5355\u5143\u7269\u7406\u94fe\u201d\u6846\u67b6\uff1a\u4ee5\u7b2c\u4e00\u6027\u539f\u7406\u4e3a\u6838\u5fc3\uff0c\u6784\u5efa\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\uff0c\u5c06\u4eba\u7c7b\u4e13\u5bb6\u77e5\u8bc6\u5f62\u5f0f\u5316\u4e3a\u201c\u5355\u5143\u7269\u7406\u6d4b\u8bd5\u201d\uff0c\u7528\u4ee5\u663e\u5f0f\u7ea6\u675f\u4ee3\u7801\u751f\u6210\u8fc7\u7a0b\uff0c\u4ece\u800c\u5f15\u5bfc\u6a21\u578b\u751f\u6210\u7b26\u5408\u7269\u7406\u89c4\u5f8b\u7684\u79d1\u5b66\u8ba1\u7b97\u7a0b\u5e8f\u3002", "result": "\u5728\u5177\u4ee3\u8868\u6027\u7684\u71c3\u70e7\u4efb\u52a1\u4e0a\uff0c\u73b0\u6709\u95ed\u6e90\u548c\u5f00\u6e90\u4ee3\u7801\u667a\u80fd\u4f53\u5747\u65e0\u6cd5\u751f\u6210\u6b63\u786e\u7aef\u5230\u7aef\u6c42\u89e3\u5668\uff0c\u5e38\u72af\u56db\u7c7b\u9519\u8bef\uff1b\u800c\u6240\u63d0\u6846\u67b6\u4ec5\u97005\u20136\u6b21\u8fed\u4ee3\u5373\u53ef\u6536\u655b\uff0c\u7ed3\u679c\u4e0e\u4e13\u5bb6\u5b9e\u73b0\u8bef\u5dee\u4ec5\u4e3a3.1\u00d710\u207b\u00b3%\uff0c\u8fd0\u884c\u901f\u5ea6\u63d0\u5347\u7ea633.4%\uff0c\u5185\u5b58\u4f7f\u7528\u6548\u7387\u63d0\u9ad8\u7ea630%\uff0c\u6210\u672c\u4e0e\u4e2d\u7b49\u5546\u4e1aAPI\u76f8\u5f53\u3002", "conclusion": "\u201c\u5355\u5143\u7269\u7406\u94fe\u201d\u6846\u67b6\u901a\u8fc7\u5d4c\u5165\u7b2c\u4e00\u6027\u539f\u7406\u5206\u6790\uff0c\u4e3a\u7269\u7406\u7ea6\u675f\u4e0b\u7684\u79d1\u5b66\u4ee3\u7801\u751f\u6210\u63d0\u4f9b\u4e86\u4e00\u79cd\u53ef\u9760\u4e14\u5b9e\u7528\u7684\u6a21\u677f\uff0c\u8d85\u8d8a\u4e86\u5355\u7eaf\u4f9d\u8d56\u6570\u636e\u9a71\u52a8\u7684\u96f6\u6837\u672c\u4ee3\u7801\u751f\u6210\u65b9\u6cd5\u3002"}}
{"id": "2512.01357", "categories": ["cs.DC", "cs.AI", "cs.AR"], "pdf": "https://arxiv.org/pdf/2512.01357", "abs": "https://arxiv.org/abs/2512.01357", "authors": ["Wenbin Zhu", "Zhaoyan Shen", "Zili Shao", "Hongjun Dai", "Feng Chen"], "title": "Tangram: Accelerating Serverless LLM Loading through GPU Memory Reuse and Affinity", "comment": null, "summary": "Serverless Large Language Models (LLMs) have emerged as a cost-effective solution for deploying AI services by enabling a 'pay-as-you-go' pricing model through GPU resource sharing. However, cold-start latency, especially the model loading phase, has become a critical performance bottleneck, as it scales linearly with model size and severely limits the practical deployment of large-scale LLM services. This paper presents Tangram, a novel system that accelerates Serverless LLM loading through efficient GPU memory reuse. By leveraging the unused GPU memory to retain model parameters, Tangram significantly reduces model transfer time and cold-start latency. Its design includes three key components: unified GPU memory pool for tensor-level parameter sharing across models, on-demand KV cache allocation for dynamic memory management, and GPU-affinity-aware scheduling for maximizing resource utilization. These techniques collectively address the critical challenges of inefficient memory usage and the cold-start problem in Serverless LLM platforms. We have implemented a fully functional prototype, and experiments show that Tangram achieves up to 6.2 times faster loading and reduces Time-To-First-Token (TTFT) during cold-start by 23--55% over state-of-the-art methods.", "AI": {"tldr": "Tangram \u662f\u4e00\u79cd\u901a\u8fc7\u9ad8\u6548\u590d\u7528 GPU \u5185\u5b58\u6765\u52a0\u901f\u65e0\u670d\u52a1\u5668\u5927\u8bed\u8a00\u6a21\u578b\uff08Serverless LLM\uff09\u52a0\u8f7d\u7684\u65b0\u7cfb\u7edf\uff0c\u663e\u8457\u964d\u4f4e\u51b7\u542f\u52a8\u5ef6\u8fdf\u548c\u9996 token \u54cd\u5e94\u65f6\u95f4\u3002", "motivation": "\u65e0\u670d\u52a1\u5668\u5927\u8bed\u8a00\u6a21\u578b\u5728\u90e8\u7f72\u4e2d\u9762\u4e34\u4e25\u91cd\u7684\u51b7\u542f\u52a8\u5ef6\u8fdf\u95ee\u9898\uff0c\u5c24\u5176\u662f\u6a21\u578b\u52a0\u8f7d\u9636\u6bb5\uff0c\u5176\u8017\u65f6\u968f\u6a21\u578b\u89c4\u6a21\u7ebf\u6027\u589e\u957f\uff0c\u9650\u5236\u4e86\u5927\u89c4\u6a21 LLM \u670d\u52a1\u7684\u5b9e\u9645\u5e94\u7528\u3002", "method": "Tangram \u901a\u8fc7\u4e09\u4e2a\u5173\u952e\u6280\u672f\u5b9e\u73b0\u52a0\u901f\uff1a\u7edf\u4e00\u7684 GPU \u5185\u5b58\u6c60\u7528\u4e8e\u8de8\u6a21\u578b\u5171\u4eab\u5f20\u91cf\u7ea7\u53c2\u6570\u3001\u6309\u9700\u5206\u914d KV \u7f13\u5b58\u4ee5\u52a8\u6001\u7ba1\u7406\u5185\u5b58\uff0c\u4ee5\u53ca\u611f\u77e5 GPU \u4eb2\u548c\u6027\u7684\u8c03\u5ea6\u7b56\u7565\u4ee5\u6700\u5927\u5316\u8d44\u6e90\u5229\u7528\u7387\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cTangram \u76f8\u6bd4\u5f53\u524d\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\uff0c\u6a21\u578b\u52a0\u8f7d\u901f\u5ea6\u6700\u9ad8\u63d0\u5347 6.2 \u500d\uff0c\u51b7\u542f\u52a8\u4e0b\u7684\u9996 token \u65f6\u95f4\uff08TTFT\uff09\u51cf\u5c11 23%\u201355%\u3002", "conclusion": "Tangram \u6709\u6548\u7f13\u89e3\u4e86 Serverless LLM \u5e73\u53f0\u4e2d\u7684\u5185\u5b58\u4f7f\u7528\u6548\u7387\u4f4e\u4e0b\u548c\u51b7\u542f\u52a8\u95ee\u9898\uff0c\u4e3a\u5927\u89c4\u6a21\u8bed\u8a00\u6a21\u578b\u7684\u9ad8\u6548\u90e8\u7f72\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848\u3002"}}
{"id": "2512.01363", "categories": ["cs.MA", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.01363", "abs": "https://arxiv.org/abs/2512.01363", "authors": ["Jiaguo Tian", "Zhengbang Zhu", "Shenyu Zhang", "Li Xu", "Bo Zheng", "Xu Liu", "Weiji Peng", "Shizeng Yao", "Weinan Zhang"], "title": "SocialDriveGen: Generating Diverse Traffic Scenarios with Controllable Social Interactions", "comment": null, "summary": "The generation of realistic and diverse traffic scenarios in simulation is essential for developing and evaluating autonomous driving systems. However, most simulation frameworks rely on rule-based or simplified models for scene generation, which lack the fidelity and diversity needed to represent real-world driving. While recent advances in generative modeling produce more realistic and context-aware traffic interactions, they often overlook how social preferences influence driving behavior. SocialDriveGen addresses this gap through a hierarchical framework that integrates semantic reasoning and social preference modeling with generative trajectory synthesis. By modeling egoism and altruism as complementary social dimensions, our framework enables controllable diversity in driver personalities and interaction styles. Experiments on the Argoverse 2 dataset show that SocialDriveGen generates diverse, high-fidelity traffic scenarios spanning cooperative to adversarial behaviors, significantly enhancing policy robustness and generalization to rare or high-risk situations.", "AI": {"tldr": "SocialDriveGen \u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u8bed\u4e49\u63a8\u7406\u4e0e\u793e\u4f1a\u504f\u597d\u5efa\u6a21\u7684\u5206\u5c42\u6846\u67b6\uff0c\u7528\u4e8e\u751f\u6210\u5177\u6709\u53ef\u63a7\u591a\u6837\u6027\u7684\u9ad8\u4fdd\u771f\u4ea4\u901a\u573a\u666f\uff0c\u6db5\u76d6\u4ece\u5408\u4f5c\u5230\u5bf9\u6297\u7684\u9a7e\u9a76\u884c\u4e3a\u3002", "motivation": "\u73b0\u6709\u4eff\u771f\u6846\u67b6\u591a\u4f9d\u8d56\u57fa\u4e8e\u89c4\u5219\u6216\u7b80\u5316\u7684\u6a21\u578b\uff0c\u7f3a\u4e4f\u771f\u5b9e\u6027\u548c\u591a\u6837\u6027\uff1b\u800c\u5f53\u524d\u751f\u6210\u6a21\u578b\u867d\u63d0\u5347\u771f\u5b9e\u6027\uff0c\u5374\u5ffd\u89c6\u4e86\u793e\u4f1a\u504f\u597d\u5bf9\u9a7e\u9a76\u884c\u4e3a\u7684\u5f71\u54cd\u3002", "method": "\u63d0\u51fa SocialDriveGen \u5206\u5c42\u6846\u67b6\uff0c\u5c06\u5229\u5df1\u4e0e\u5229\u4ed6\u4f5c\u4e3a\u4e92\u8865\u7684\u793e\u4f1a\u7ef4\u5ea6\uff0c\u878d\u5408\u8bed\u4e49\u63a8\u7406\u3001\u793e\u4f1a\u504f\u597d\u5efa\u6a21\u4e0e\u8f68\u8ff9\u751f\u6210\uff0c\u5b9e\u73b0\u5bf9\u9a7e\u9a76\u4e2a\u6027\u548c\u4ea4\u4e92\u98ce\u683c\u7684\u53ef\u63a7\u751f\u6210\u3002", "result": "\u5728 Argoverse 2 \u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u80fd\u751f\u6210\u9ad8\u4fdd\u771f\u3001\u591a\u6837\u5316\u7684\u4ea4\u901a\u573a\u666f\uff0c\u663e\u8457\u63d0\u5347\u81ea\u52a8\u9a7e\u9a76\u7b56\u7565\u5728\u7f55\u89c1\u6216\u9ad8\u98ce\u9669\u60c5\u5883\u4e0b\u7684\u9c81\u68d2\u6027\u4e0e\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "\u901a\u8fc7\u5f15\u5165\u793e\u4f1a\u504f\u597d\u5efa\u6a21\uff0cSocialDriveGen \u6709\u6548\u5f25\u8865\u4e86\u73b0\u6709\u4ea4\u901a\u573a\u666f\u751f\u6210\u65b9\u6cd5\u5728\u884c\u4e3a\u591a\u6837\u6027\u548c\u771f\u5b9e\u6027\u65b9\u9762\u7684\u4e0d\u8db3\uff0c\u4e3a\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u7684\u8bad\u7ec3\u4e0e\u8bc4\u4f30\u63d0\u4f9b\u4e86\u66f4\u53ef\u9760\u7684\u4eff\u771f\u73af\u5883\u3002"}}
{"id": "2512.00031", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2512.00031", "abs": "https://arxiv.org/abs/2512.00031", "authors": ["Ravindra Ganti", "Steve Xu"], "title": "Hardware-Aware Neural Network Compilation with Learned Optimization: A RISC-V Accelerator Approach", "comment": "18 pages, 7 figures, 6 tables", "summary": "We present XgenSilicon ML Compiler, a fully automated end-to-end compilation framework that transforms high-level machine learning models into optimized RISC-V assembly code for custom ASIC accelerators. By unifying the system's cost model across software and hardware, the compiler achieves significant improvements in Power, Performance, and Area (PPA) metrics compared to standard off-the-shelf components and hand-designed chips through five key innovations: (1) a multi-algorithm auto-tuning framework with five search strategies (Bayesian Optimization, Genetic Algorithm, Simulated Annealing, Random Search, Grid Search) combined with a learned cost model, (2) an integrated quantization framework supporting extreme precisions from FP32 to Binary with full KL divergence calibration (2048-bin histogram optimization) and momentum-based QAT gradient updates, (3) hardware-aware validation ensuring 100 percent ISA compliance and memory constraint satisfaction, (4) dynamic shape support with multi-configuration specialization, and (5) advanced cache-aware cost modeling with multi-level cache hierarchy analysis. Our evaluation demonstrates that ASICs produced by this compiler achieve 2.5-4.5x better performance, 3-6x lower power consumption, and 40-60 percent area reduction compared to baseline implementations. The compiler supports more than 100 ONNX operators across 12 categories, implements advanced RISC-V Vector optimizations, and generates hardware-validated assembly code suitable for direct ASIC synthesis. All compilation steps are fully automated, requiring zero manual intervention from model input to ASIC-ready output.", "AI": {"tldr": "XgenSilicon ML Compiler \u662f\u4e00\u4e2a\u7aef\u5230\u7aef\u7684\u81ea\u52a8\u5316\u7f16\u8bd1\u6846\u67b6\uff0c\u53ef\u5c06\u9ad8\u7ea7\u673a\u5668\u5b66\u4e60\u6a21\u578b\u76f4\u63a5\u7f16\u8bd1\u4e3a\u9488\u5bf9\u5b9a\u5236 ASIC \u7684\u4f18\u5316 RISC-V \u6c47\u7f16\u4ee3\u7801\uff0c\u5728\u6027\u80fd\u3001\u529f\u8017\u548c\u9762\u79ef\uff08PPA\uff09\u65b9\u9762\u663e\u8457\u4f18\u4e8e\u901a\u7528\u7ec4\u4ef6\u548c\u624b\u5de5\u8bbe\u8ba1\u82af\u7247\u3002", "motivation": "\u73b0\u6709\u673a\u5668\u5b66\u4e60\u90e8\u7f72\u6d41\u7a0b\u5728\u8f6f\u786c\u4ef6\u534f\u540c\u4f18\u5316\u65b9\u9762\u5b58\u5728\u5272\u88c2\uff0c\u5bfc\u81f4 ASIC \u5b9e\u73b0\u96be\u4ee5\u517c\u987e\u9ad8\u6027\u80fd\u3001\u4f4e\u529f\u8017\u4e0e\u5c0f\u9762\u79ef\uff1b\u540c\u65f6\uff0c\u7f3a\u4e4f\u5168\u81ea\u52a8\u5316\u7684\u5de5\u5177\u94fe\u4f7f\u5f97\u4ece\u6a21\u578b\u5230\u82af\u7247\u7684\u6d41\u7a0b\u4f9d\u8d56\u5927\u91cf\u4eba\u5de5\u5e72\u9884\u3002", "method": "\u8be5\u7f16\u8bd1\u5668\u901a\u8fc7\u4e94\u5927\u521b\u65b0\u5b9e\u73b0\u8f6f\u786c\u7edf\u4e00\u4f18\u5316\uff1a(1) \u591a\u7b97\u6cd5\u81ea\u52a8\u8c03\u4f18\u6846\u67b6\u7ed3\u5408\u4e94\u79cd\u641c\u7d22\u7b56\u7565\u4e0e\u5b66\u4e60\u578b\u4ee3\u4ef7\u6a21\u578b\uff1b(2) \u652f\u6301 FP32 \u5230\u4e8c\u503c\u5316\u7684\u96c6\u6210\u91cf\u5316\u6846\u67b6\uff0c\u5305\u542b KL \u6563\u5ea6\u6821\u51c6\u4e0e\u52a8\u91cf QAT\uff1b(3) \u786c\u4ef6\u611f\u77e5\u9a8c\u8bc1\u786e\u4fdd ISA \u5408\u89c4\u4e0e\u5185\u5b58\u7ea6\u675f\uff1b(4) \u52a8\u6001\u5f62\u72b6\u652f\u6301\u4e0e\u591a\u914d\u7f6e\u7279\u5316\uff1b(5) \u9762\u5411\u591a\u7ea7\u7f13\u5b58\u7684\u9ad8\u7ea7\u4ee3\u4ef7\u5efa\u6a21\u3002", "result": "\u8bc4\u4f30\u663e\u793a\uff0c\u7531\u8be5\u7f16\u8bd1\u5668\u751f\u6210\u7684 ASIC \u5728\u6027\u80fd\u4e0a\u63d0\u5347 2.5\u20134.5 \u500d\uff0c\u529f\u8017\u964d\u4f4e 3\u20136 \u500d\uff0c\u9762\u79ef\u51cf\u5c11 40\u201360%\uff1b\u652f\u6301 100 \u591a\u4e2a ONNX \u7b97\u5b50\u3001RISC-V Vector \u6269\u5c55\uff0c\u5e76\u80fd\u751f\u6210\u53ef\u76f4\u63a5\u7528\u4e8e ASIC \u5408\u6210\u7684\u786c\u4ef6\u9a8c\u8bc1\u6c47\u7f16\u4ee3\u7801\u3002", "conclusion": "XgenSilicon ML Compiler \u5b9e\u73b0\u4e86\u4ece\u6a21\u578b\u5230 ASIC \u7684\u5168\u81ea\u52a8\u3001\u9ad8\u6027\u80fd\u3001\u9ad8\u80fd\u6548\u7f16\u8bd1\u6d41\u7a0b\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5b9a\u5236\u82af\u7247\u7684 PPA \u8868\u73b0\uff0c\u5177\u5907\u5e7f\u6cdb\u7684\u5de5\u4e1a\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2512.00325", "categories": ["cs.SE", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2512.00325", "abs": "https://arxiv.org/abs/2512.00325", "authors": ["Shaira Sadia Karim", "Abrar Mahmud Rahim", "Lamia Alam", "Ishmam Tashdeed", "Lutfun Nahar Lota", "Md. Abu Raihan M. Kamal", "Md. Azam Hossain"], "title": "Progressive Code Integration for Abstractive Bug Report Summarization", "comment": null, "summary": "Bug reports are often unstructured and verbose, making it challenging for developers to efficiently comprehend software issues. Existing summarization approaches typically rely on surface-level textual cues, resulting in incomplete or redundant summaries, and they frequently ignore associated code snippets, which are essential for accurate defect diagnosis. To address these limitations, we propose a progressive code-integration framework for LLM-based abstractive bug report summarization. Our approach incrementally incorporates long code snippets alongside textual content, overcoming standard LLM context window constraints and producing semantically rich summaries. Evaluated on four benchmark datasets using eight LLMs, our pipeline outperforms extractive baselines by 7.5%-58.2% and achieves performance comparable to state-of-the-art abstractive methods, highlighting the benefits of jointly leveraging textual and code information for enhanced bug comprehension.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u6e10\u8fdb\u5f0f\u4ee3\u7801\u878d\u5408\u6846\u67b6\uff0c\u7528\u4e8e\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u62bd\u8c61\u578b\u7f3a\u9677\u62a5\u544a\u6458\u8981\u751f\u6210\uff0c\u901a\u8fc7\u9010\u6b65\u6574\u5408\u957f\u4ee3\u7801\u7247\u6bb5\u4e0e\u6587\u672c\u5185\u5bb9\uff0c\u5728\u591a\u4e2a\u6570\u636e\u96c6\u548c\u6a21\u578b\u4e0a\u663e\u8457\u4f18\u4e8e\u62bd\u53d6\u5f0f\u57fa\u7ebf\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u7f3a\u9677\u62a5\u544a\u6458\u8981\u65b9\u6cd5\u591a\u4f9d\u8d56\u8868\u5c42\u6587\u672c\u7ebf\u7d22\uff0c\u5e38\u5ffd\u7565\u5173\u952e\u7684\u5173\u8054\u4ee3\u7801\u7247\u6bb5\uff0c\u5bfc\u81f4\u6458\u8981\u4e0d\u5b8c\u6574\u6216\u5197\u4f59\uff0c\u96be\u4ee5\u652f\u6301\u51c6\u786e\u7684\u7f3a\u9677\u8bca\u65ad\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u6e10\u8fdb\u5f0f\u4ee3\u7801\u878d\u5408\u6846\u67b6\uff0c\u5c06\u957f\u4ee3\u7801\u7247\u6bb5\u4e0e\u6587\u672c\u5185\u5bb9\u9010\u6b65\u6574\u5408\u8fdbLLM\u4e2d\uff0c\u4ee5\u514b\u670d\u4e0a\u4e0b\u6587\u7a97\u53e3\u9650\u5236\uff0c\u751f\u6210\u8bed\u4e49\u4e30\u5bcc\u7684\u6458\u8981\u3002", "result": "\u5728\u56db\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u548c\u516b\u4e2aLLM\u4e0a\u8bc4\u4f30\uff0c\u8be5\u65b9\u6cd5\u6bd4\u62bd\u53d6\u5f0f\u57fa\u7ebf\u63d0\u53477.5%-58.2%\uff0c\u6027\u80fd\u5ab2\u7f8e\u5f53\u524d\u6700\u5148\u8fdb\u7684\u62bd\u8c61\u5f0f\u65b9\u6cd5\u3002", "conclusion": "\u8054\u5408\u5229\u7528\u6587\u672c\u4e0e\u4ee3\u7801\u4fe1\u606f\u80fd\u663e\u8457\u63d0\u5347\u7f3a\u9677\u62a5\u544a\u6458\u8981\u8d28\u91cf\uff0c\u6709\u52a9\u4e8e\u5f00\u53d1\u8005\u66f4\u9ad8\u6548\u5730\u7406\u89e3\u8f6f\u4ef6\u95ee\u9898\u3002"}}
{"id": "2512.01610", "categories": ["cs.MA"], "pdf": "https://arxiv.org/pdf/2512.01610", "abs": "https://arxiv.org/abs/2512.01610", "authors": ["Yuren Mao", "Peigen Liu", "Xinjian Wang", "Rui Ding", "Jing Miao", "Hui Zou", "Mingjie Qi", "Wanxiang Luo", "Longbin Lai", "Kai Wang", "Zhengping Qian", "Peilun Yang", "Yunjun Gao", "Ying Zhang"], "title": "Agent-Kernel: A MicroKernel Multi-Agent System Framework for Adaptive Social Simulation Powered by LLMs", "comment": null, "summary": "Multi-Agent System (MAS) developing frameworks serve as the foundational infrastructure for social simulations powered by Large Language Models (LLMs). However, existing frameworks fail to adequately support large-scale simulation development due to inherent limitations in adaptability, configurability, reliability, and code reusability. For example, they cannot simulate a society where the agent population and profiles change over time. To fill this gap, we propose Agent-Kernel, a framework built upon a novel society-centric modular microkernel architecture. It decouples core system functions from simulation logic and separates cognitive processes from physical environments and action execution. Consequently, Agent-Kernel achieves superior adaptability, configurability, reliability, and reusability. We validate the framework's superiority through two distinct applications: a simulation of the Universe 25 (Mouse Utopia) experiment, which demonstrates the handling of rapid population dynamics from birth to death; and a large-scale simulation of the Zhejiang University Campus Life, successfully coordinating 10,000 heterogeneous agents, including students and faculty.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86Agent-Kernel\uff0c\u4e00\u79cd\u57fa\u4e8e\u793e\u4f1a\u4e2d\u5fc3\u5316\u6a21\u5757\u5fae\u5185\u6838\u67b6\u6784\u7684\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u5f00\u53d1\u6846\u67b6\uff0c\u4ee5\u89e3\u51b3\u73b0\u6709\u6846\u67b6\u5728\u5927\u89c4\u6a21\u793e\u4f1a\u6a21\u62df\u4e2d\u9002\u5e94\u6027\u3001\u53ef\u914d\u7f6e\u6027\u3001\u53ef\u9760\u6027\u548c\u4ee3\u7801\u590d\u7528\u6027\u65b9\u9762\u7684\u4e0d\u8db3\uff0c\u5e76\u901a\u8fc7\u201c\u5b87\u5b9925\u53f7\u201d\u5b9e\u9a8c\u548c\u6d59\u6c5f\u5927\u5b66\u6821\u56ed\u751f\u6d3b\u4e07\u7ea7\u667a\u80fd\u4f53\u6a21\u62df\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u73b0\u6709\u9762\u5411\u5927\u8bed\u8a00\u6a21\u578b\u9a71\u52a8\u7684\u793e\u4f1a\u6a21\u62df\u7684\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\uff08MAS\uff09\u5f00\u53d1\u6846\u67b6\u5728\u9002\u5e94\u6027\u3001\u53ef\u914d\u7f6e\u6027\u3001\u53ef\u9760\u6027\u4e0e\u4ee3\u7801\u590d\u7528\u6027\u65b9\u9762\u5b58\u5728\u5c40\u9650\uff0c\u96be\u4ee5\u652f\u6301\u52a8\u6001\u53d8\u5316\u7684\u667a\u80fd\u4f53\u6570\u91cf\u4e0e\u89d2\u8272\u7279\u5f81\u7684\u5927\u89c4\u6a21\u6a21\u62df\u9700\u6c42\u3002", "method": "\u63d0\u51faAgent-Kernel\u6846\u67b6\uff0c\u91c7\u7528\u65b0\u9896\u7684\u201c\u4ee5\u793e\u4f1a\u4e3a\u4e2d\u5fc3\u201d\u7684\u6a21\u5757\u5316\u5fae\u5185\u6838\u67b6\u6784\uff0c\u5c06\u6838\u5fc3\u7cfb\u7edf\u529f\u80fd\u4e0e\u6a21\u62df\u903b\u8f91\u89e3\u8026\uff0c\u5e76\u5206\u79bb\u8ba4\u77e5\u8fc7\u7a0b\u4e0e\u7269\u7406\u73af\u5883\u53ca\u52a8\u4f5c\u6267\u884c\u3002", "result": "\u901a\u8fc7\u4e24\u4e2a\u5e94\u7528\u9a8c\u8bc1\uff1a1\uff09\u6210\u529f\u6a21\u62df\u201c\u5b87\u5b9925\u53f7\u201d\u5b9e\u9a8c\u4e2d\u7684\u79cd\u7fa4\u4ece\u51fa\u751f\u5230\u6b7b\u4ea1\u7684\u5feb\u901f\u52a8\u6001\u53d8\u5316\uff1b2\uff09\u5b9e\u73b0\u5305\u542b10,000\u4e2a\u5f02\u6784\u667a\u80fd\u4f53\uff08\u5b66\u751f\u4e0e\u6559\u804c\u5de5\uff09\u7684\u6d59\u6c5f\u5927\u5b66\u6821\u56ed\u751f\u6d3b\u5927\u89c4\u6a21\u6a21\u62df\u3002", "conclusion": "Agent-Kernel\u5728\u9002\u5e94\u6027\u3001\u53ef\u914d\u7f6e\u6027\u3001\u53ef\u9760\u6027\u4e0e\u53ef\u590d\u7528\u6027\u65b9\u9762\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u6846\u67b6\uff0c\u4e3a\u5927\u89c4\u6a21\u3001\u52a8\u6001\u6f14\u5316\u7684LLM\u9a71\u52a8\u793e\u4f1a\u6a21\u62df\u63d0\u4f9b\u4e86\u9ad8\u6548\u53ef\u9760\u7684\u57fa\u7840\u8bbe\u65bd\u3002"}}
{"id": "2512.01646", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2512.01646", "abs": "https://arxiv.org/abs/2512.01646", "authors": ["Barenya Kumar Nandy", "Rupesh Nasre"], "title": "StarDist: A Code Generator for Distributed Graph Algorithms", "comment": null, "summary": "Relational data, occurring in the real world, are often structured as graphs, which provide the logical abstraction required to make analytical derivations simpler. As graphs get larger, the irregular access patterns exhibited in most graph algorithms, hamper performance. This, along with NUMA and physical memory limits, results in scaling complexities with sequential/shared memory frameworks. StarPlat's MPI backend abstracts away the programmatic complexity involved in designing optimal distributed graph algorithms. It provides an instrument for coding graph algorithms that scale over distributed memory. In this work, we provide an analysis-transformation framework that leverages general semantics associated with iterations involving nodes and their neighbors, within StarPlat, to aggregate communication. The framework scans for patterns that warrant re-ordering in neighborhood access patterns, aggregate communication, and avoid communication altogether with opportunistic caching in reduction constructs. We also architect an optimized bulk-reduction substrate using Open MPI's passive Remote Memory Access (RMA) constructs. We applied our optimization logic to StarPlat's distributed backend and outperformed d-Galois by 2.05 and DRONE by 1.44 times in Single Source Shortest Paths across several big data graphs.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eStarPlat MPI\u540e\u7aef\u7684\u5206\u6790-\u8f6c\u6362\u6846\u67b6\uff0c\u901a\u8fc7\u4f18\u5316\u90bb\u57df\u8bbf\u95ee\u6a21\u5f0f\u3001\u805a\u5408\u901a\u4fe1\u548c\u673a\u4f1a\u7f13\u5b58\u7b49\u624b\u6bb5\u63d0\u5347\u5206\u5e03\u5f0f\u56fe\u7b97\u6cd5\u6027\u80fd\uff0c\u5e76\u5728SSSP\u4efb\u52a1\u4e0a\u663e\u8457\u4f18\u4e8ed-Galois\u548cDRONE\u3002", "motivation": "\u5927\u89c4\u6a21\u56fe\u6570\u636e\u5728\u771f\u5b9e\u4e16\u754c\u4e2d\u666e\u904d\u5b58\u5728\uff0c\u4f46\u5176\u4e0d\u89c4\u5219\u7684\u8bbf\u95ee\u6a21\u5f0f\u3001NUMA\u67b6\u6784\u53ca\u7269\u7406\u5185\u5b58\u9650\u5236\u5bfc\u81f4\u4f20\u7edf\u4e32\u884c/\u5171\u4eab\u5185\u5b58\u6846\u67b6\u96be\u4ee5\u9ad8\u6548\u6269\u5c55\u3002\u4e3a\u89e3\u51b3\u5206\u5e03\u5f0f\u56fe\u7b97\u6cd5\u7f16\u7a0b\u590d\u6742\u6027\u548c\u6027\u80fd\u74f6\u9888\u95ee\u9898\uff0c\u4f5c\u8005\u65e8\u5728\u7b80\u5316\u9ad8\u6027\u80fd\u5206\u5e03\u5f0f\u56fe\u8ba1\u7b97\u7684\u5b9e\u73b0\u3002", "method": "\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u5206\u6790-\u8f6c\u6362\u6846\u67b6\uff0c\u5229\u7528\u56fe\u8fed\u4ee3\u4e2d\u8282\u70b9\u53ca\u5176\u90bb\u5c45\u7684\u4e00\u822c\u8bed\u4e49\uff0c\u5728StarPlat\u4e2d\u81ea\u52a8\u91cd\u6392\u90bb\u57df\u8bbf\u95ee\u987a\u5e8f\u3001\u805a\u5408\u901a\u4fe1\u64cd\u4f5c\uff0c\u5e76\u5728\u89c4\u7ea6\u7ed3\u6784\u4e2d\u5f15\u5165\u673a\u4f1a\u7f13\u5b58\u4ee5\u51cf\u5c11\u901a\u4fe1\uff1b\u540c\u65f6\u57fa\u4e8eOpen MPI\u7684\u88ab\u52a8RMA\u673a\u5236\u6784\u5efa\u4e86\u4f18\u5316\u7684\u6279\u91cf\u89c4\u7ea6\u5e95\u5c42\u3002", "result": "\u5728\u591a\u4e2a\u5927\u89c4\u6a21\u56fe\u6570\u636e\u96c6\u4e0a\u7684\u5355\u6e90\u6700\u77ed\u8def\u5f84\uff08SSSP\uff09\u4efb\u52a1\u4e2d\uff0c\u4f18\u5316\u540e\u7684StarPlat\u5206\u5e03\u5f0f\u540e\u7aef\u6027\u80fd\u8fbe\u5230d-Galois\u76842.05\u500d\u548cDRONE\u76841.44\u500d\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u901a\u4fe1\u4f18\u5316\u6846\u67b6\u6709\u6548\u63d0\u5347\u4e86\u5206\u5e03\u5f0f\u56fe\u7b97\u6cd5\u7684\u53ef\u6269\u5c55\u6027\u4e0e\u6267\u884c\u6548\u7387\uff0c\u9a8c\u8bc1\u4e86\u5728\u9ad8\u5c42\u62bd\u8c61\u4e2d\u96c6\u6210\u81ea\u52a8\u901a\u4fe1\u4f18\u5316\u7b56\u7565\u7684\u53ef\u884c\u6027\u4e0e\u4f18\u52bf\u3002"}}
{"id": "2512.00032", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2512.00032", "abs": "https://arxiv.org/abs/2512.00032", "authors": ["Giuseppe M. Sarda", "Nimish Shah", "Abubakr Nada", "Debjyoti Bhattacharjee", "Marian Verhelst"], "title": "Decoupled Control Flow and Data Access in RISC-V GPGPUs", "comment": null, "summary": "Vortex, a newly proposed open-source GPGPU platform based on the RISC-V ISA, offers a valid alternative for GPGPU research over the broadly-used modeling platforms based on commercial GPUs. Similarly to the push originating from the RISC-V movement for CPUs, Vortex can enable a myriad of fresh research directions for GPUs. However, as a young hardware platform, it currently lacks the performance competitiveness of commercial GPUs, which is crucial for widespread adoption. State-of-the-art GPUs, in fact, rely on complex architectural features, still unavailable in Vortex, to hide the micro-code overheads linked to control flow (CF) management and memory orchestration for data access. In particular, these components account for the majority of the dynamic instruction count in regular, memory-intensive kernels, such as linear algebra routines, which form the basis of many applications, including Machine Learning. To address these challenges with simple yet powerful micro-architecture modifications, this paper introduces decoupled CF and data access through 1.) a hardware CF manager to accelerate branching and predication in regular loop execution and 2.) decoupled memory streaming lanes to further hide memory latency with useful computation. The evaluation results for different kernels show 8$\\times$ faster execution, 10$\\times$ reduction in dynamic instruction count, and overall performance improvement from 0.35 to 1.63 $\\mathrm{GFLOP/s/mm^2}$. Thanks to these enhancements, Vortex can become an ideal playground to enable GPGPU research for the next generation of Machine Learning.", "AI": {"tldr": "\u672c\u6587\u9488\u5bf9\u5f00\u6e90RISC-V GPGPU\u5e73\u53f0Vortex\u5728\u63a7\u5236\u6d41\u548c\u5185\u5b58\u8bbf\u95ee\u65b9\u9762\u7684\u6027\u80fd\u74f6\u9888\uff0c\u63d0\u51fa\u901a\u8fc7\u786c\u4ef6\u63a7\u5236\u6d41\u7ba1\u7406\u5668\u548c\u89e3\u8026\u5185\u5b58\u6d41\u901a\u9053\u4e24\u9879\u5fae\u67b6\u6784\u6539\u8fdb\uff0c\u663e\u8457\u63d0\u5347\u5176\u6027\u80fd\uff0c\u4f7f\u5176\u66f4\u9002\u7528\u4e8e\u673a\u5668\u5b66\u4e60\u7b49\u9886\u57df\u7684GPGPU\u7814\u7a76\u3002", "motivation": "Vortex\u4f5c\u4e3a\u4e00\u4e2a\u65b0\u5174\u7684\u5f00\u6e90GPGPU\u5e73\u53f0\uff0c\u867d\u7136\u4e3a\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\uff0c\u4f46\u5176\u6027\u80fd\u5c1a\u65e0\u6cd5\u4e0e\u5546\u7528GPU\u7ade\u4e89\uff0c\u4e3b\u8981\u53d7\u9650\u4e8e\u7f3a\u4e4f\u5904\u7406\u63a7\u5236\u6d41\u7ba1\u7406\u548c\u5185\u5b58\u7f16\u6392\u5f00\u9500\u7684\u590d\u6742\u67b6\u6784\u7279\u6027\uff0c\u5c24\u5176\u5728\u5185\u5b58\u5bc6\u96c6\u578b\u6838\u5fc3\uff08\u5982\u7ebf\u6027\u4ee3\u6570\uff09\u4e2d\u8868\u73b0\u660e\u663e\u3002", "method": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e24\u79cd\u7b80\u5355\u800c\u6709\u6548\u7684\u5fae\u67b6\u6784\u4fee\u6539\uff1a1\uff09\u5f15\u5165\u786c\u4ef6\u63a7\u5236\u6d41\u7ba1\u7406\u5668\u4ee5\u52a0\u901f\u89c4\u5219\u5faa\u73af\u6267\u884c\u4e2d\u7684\u5206\u652f\u548c\u8c13\u8bcd\u64cd\u4f5c\uff1b2\uff09\u91c7\u7528\u89e3\u8026\u7684\u5185\u5b58\u6d41\u901a\u9053\uff0c\u4ee5\u5728\u7b49\u5f85\u5185\u5b58\u65f6\u8fdb\u884c\u6709\u7528\u8ba1\u7b97\uff0c\u4ece\u800c\u9690\u85cf\u5185\u5b58\u5ef6\u8fdf\u3002", "result": "\u8bc4\u4f30\u7ed3\u679c\u663e\u793a\uff0c\u6539\u8fdb\u540e\u7684Vortex\u5728\u4e0d\u540c\u6838\u5fc3\u4e0a\u5b9e\u73b0\u4e868\u500d\u7684\u6267\u884c\u901f\u5ea6\u63d0\u5347\u3001\u52a8\u6001\u6307\u4ee4\u6570\u51cf\u5c1110\u500d\uff0c\u6574\u4f53\u6027\u80fd\u4ece0.35 GFLOP/s/mm\u00b2\u63d0\u5347\u81f31.63 GFLOP/s/mm\u00b2\u3002", "conclusion": "\u901a\u8fc7\u6240\u63d0\u51fa\u7684\u5fae\u67b6\u6784\u589e\u5f3a\uff0cVortex\u7684\u6027\u80fd\u5f97\u5230\u663e\u8457\u63d0\u5347\uff0c\u6709\u671b\u6210\u4e3a\u4e0b\u4e00\u4ee3\u673a\u5668\u5b66\u4e60GPGPU\u7814\u7a76\u7684\u7406\u60f3\u5e73\u53f0\u3002"}}
{"id": "2512.00380", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2512.00380", "abs": "https://arxiv.org/abs/2512.00380", "authors": ["Mingwei Liu", "Zheng Pei", "Yanlin Wang", "Zihao Wang", "Zikang Li", "Enci Lin", "Xin Peng", "Zibin Zheng"], "title": "Framework-Aware Code Generation with API Knowledge Graph-Constructed Data: A Study on HarmonyOS", "comment": null, "summary": "In the context of software frameworks with limited resources (such as HarmonyOS), large language models (LLMs) often exhibit poor code generation performance because they lack sufficient exposure to such environments during pre-training. Although LLMs can usually maintain correct logical structures across programming languages, they frequently struggle when dealing with framework-specific APIs or syntax, resulting in errors. This indicates that while pre-training equips LLMs with general algorithmic capabilities, they remain unfamiliar with the distinctive syntax and API usage of underrepresented frameworks. As a result, even advanced commercial models like GPT-4o cannot reliably generate correct code without prior adaptation. To address this issue, we propose APIKG4SYN, a framework designed to exploit API knowledge graphs for the construction of API-oriented question-code pairs, specifically tailored for low-resource frameworks without requiring executable code. APIKG4SYN integrates both single-API and multi-API knowledge, where the latter is derived through uncertainty estimation (UE)-driven Monte Carlo Tree Search (MCTS), enabling the creation of a diverse and informative dataset for fine-tuning LLMs. Using HarmonyOS as a case study, we build the first benchmark for HarmonyOS code generation. Experimental results show that fine-tuning Qwen with APIKG4SYN raises pass@1 accuracy to 25.00%, compared with 17.59% for the baseline GPT model. These results confirm that API-oriented data significantly enhance LLM performance in low-resource software development scenarios.", "AI": {"tldr": "\u672c\u6587\u9488\u5bf9\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5728\u4f4e\u8d44\u6e90\u8f6f\u4ef6\u6846\u67b6\uff08\u5982HarmonyOS\uff09\u4e2d\u4ee3\u7801\u751f\u6210\u8868\u73b0\u4e0d\u4f73\u7684\u95ee\u9898\uff0c\u63d0\u51faAPIKG4SYN\u6846\u67b6\uff0c\u5229\u7528API\u77e5\u8bc6\u56fe\u8c31\u6784\u5efa\u9762\u5411API\u7684\u95ee\u7b54-\u4ee3\u7801\u5bf9\u6570\u636e\u96c6\uff0c\u65e0\u9700\u53ef\u6267\u884c\u4ee3\u7801\u5373\u53ef\u6709\u6548\u63d0\u5347LLM\u5728\u8be5\u7c7b\u573a\u666f\u4e0b\u7684\u6027\u80fd\u3002", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b\u5728\u9884\u8bad\u7ec3\u9636\u6bb5\u7f3a\u4e4f\u5bf9\u4f4e\u8d44\u6e90\u8f6f\u4ef6\u6846\u67b6\uff08\u5982HarmonyOS\uff09\u7684\u5145\u5206\u63a5\u89e6\uff0c\u5bfc\u81f4\u5176\u5728\u751f\u6210\u76f8\u5173\u4ee3\u7801\u65f6\u9891\u7e41\u51fa\u73b0API\u6216\u8bed\u6cd5\u9519\u8bef\uff0c\u5373\u4f7f\u903b\u8f91\u7ed3\u6784\u6b63\u786e\u4e5f\u65e0\u6cd5\u4fdd\u8bc1\u4ee3\u7801\u53ef\u7528\u6027\u3002", "method": "\u63d0\u51faAPIKG4SYN\u6846\u67b6\uff0c\u901a\u8fc7API\u77e5\u8bc6\u56fe\u8c31\u6784\u5efa\u5355API\u4e0e\u591aAPI\u7684\u95ee\u7b54-\u4ee3\u7801\u5bf9\uff1b\u5176\u4e2d\u591aAPI\u7ec4\u5408\u901a\u8fc7\u57fa\u4e8e\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\uff08UE\uff09\u9a71\u52a8\u7684\u8499\u7279\u5361\u6d1b\u6811\u641c\u7d22\uff08MCTS\uff09\u751f\u6210\uff0c\u7528\u4e8e\u5fae\u8c03LLM\u3002", "result": "\u4ee5HarmonyOS\u4e3a\u6848\u4f8b\u6784\u5efa\u9996\u4e2a\u76f8\u5173\u4ee3\u7801\u751f\u6210\u57fa\u51c6\uff0c\u5b9e\u9a8c\u8868\u660e\u4f7f\u7528APIKG4SYN\u5fae\u8c03Qwen\u6a21\u578b\u540e\uff0cpass@1\u51c6\u786e\u7387\u8fbe25.00%\uff0c\u4f18\u4e8e\u57fa\u7ebfGPT\u6a21\u578b\u768417.59%\u3002", "conclusion": "\u9762\u5411API\u7684\u6570\u636e\u6784\u9020\u65b9\u6cd5\u80fd\u663e\u8457\u63d0\u5347\u5927\u8bed\u8a00\u6a21\u578b\u5728\u4f4e\u8d44\u6e90\u8f6f\u4ef6\u5f00\u53d1\u73af\u5883\u4e2d\u7684\u4ee3\u7801\u751f\u6210\u80fd\u529b\uff0c\u9a8c\u8bc1\u4e86APIKG4SYN\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2512.01764", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2512.01764", "abs": "https://arxiv.org/abs/2512.01764", "authors": ["Kingshuk Haldar"], "title": "Trace-based, time-resolved analysis of MPI application performance using standard metrics", "comment": "Presented at and submitted to the International Parallel Tools Workshop 2025", "summary": "Detailed trace analysis of MPI applications is essential for performance engineering, but growing trace sizes and complex communication behaviour often render comprehensive visual inspection impractical. This work presents a trace-based calculation of time-resolved values of standard MPI performance metrics, load balance, serialisation, and transfer efficiency, by discretising execution traces into fixed or adaptive time segments. The implementation processes Paraver traces postmortem, reconstructing critical execution paths and handling common event anomalies, such as clock inconsistencies and unmatched MPI events, to robustly calculate metrics for each segment. The calculated per-window metric values expose transient performance bottlenecks that the timeaggregated metrics from existing tools may conceal. Evaluations on a synthetic benchmark and real-world applications (LaMEM and ls1-MarDyn) demonstrate how time-resolved metrics reveal localised performance bottlenecks obscured by global aggregates, offering a lightweight and scalable alternative even when trace visualisation is impractical.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u65f6\u95f4\u5206\u6bb5\u7684MPI\u6027\u80fd\u6307\u6807\uff08\u8d1f\u8f7d\u5747\u8861\u3001\u4e32\u884c\u5316\u548c\u4f20\u8f93\u6548\u7387\uff09\u8ba1\u7b97\u65b9\u6cd5\uff0c\u901a\u8fc7\u5904\u7406Paraver\u8f68\u8ff9\u6570\u636e\uff0c\u63ed\u793a\u88ab\u5168\u5c40\u805a\u5408\u6307\u6807\u63a9\u76d6\u7684\u77ac\u65f6\u6027\u80fd\u74f6\u9888\u3002", "motivation": "\u4f20\u7edfMPI\u6027\u80fd\u5206\u6790\u5de5\u5177\u4f9d\u8d56\u5168\u5c40\u805a\u5408\u6307\u6807\u6216\u53ef\u89c6\u5316\u8f68\u8ff9\uff0c\u96be\u4ee5\u6709\u6548\u8bc6\u522b\u77ac\u65f6\u6216\u5c40\u90e8\u6027\u80fd\u74f6\u9888\uff0c\u5c24\u5176\u5728\u9762\u5bf9\u5927\u89c4\u6a21\u8f68\u8ff9\u6570\u636e\u548c\u590d\u6742\u901a\u4fe1\u884c\u4e3a\u65f6\u3002", "method": "\u5c06\u6267\u884c\u8f68\u8ff9\u5212\u5206\u4e3a\u56fa\u5b9a\u6216\u81ea\u9002\u5e94\u65f6\u95f4\u7a97\u53e3\uff0c\u5bf9\u6bcf\u4e2a\u7a97\u53e3\u91cd\u5efa\u5173\u952e\u6267\u884c\u8def\u5f84\uff0c\u5e76\u5904\u7406\u65f6\u949f\u4e0d\u4e00\u81f4\u548c\u672a\u5339\u914dMPI\u4e8b\u4ef6\u7b49\u5f02\u5e38\uff0c\u4ece\u800c\u8ba1\u7b97\u6bcf\u4e2a\u65f6\u95f4\u7a97\u53e3\u5185\u7684\u6807\u51c6MPI\u6027\u80fd\u6307\u6807\u3002", "result": "\u5728\u5408\u6210\u57fa\u51c6\u6d4b\u8bd5\u548c\u771f\u5b9e\u5e94\u7528\uff08LaMEM\u548cls1-MarDyn\uff09\u4e0a\u7684\u8bc4\u4f30\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u80fd\u6709\u6548\u63ed\u793a\u88ab\u5168\u5c40\u6307\u6807\u63a9\u76d6\u7684\u5c40\u90e8\u6027\u80fd\u74f6\u9888\uff0c\u4e14\u5177\u6709\u8f7b\u91cf\u7ea7\u548c\u53ef\u6269\u5c55\u6027\u3002", "conclusion": "\u6240\u63d0\u65b9\u6cd5\u4e3aMPI\u5e94\u7528\u6027\u80fd\u5206\u6790\u63d0\u4f9b\u4e86\u4e00\u79cd\u5b9e\u7528\u3001\u9ad8\u6548\u7684\u65b0\u9014\u5f84\uff0c\u5c24\u5176\u9002\u7528\u4e8e\u65e0\u6cd5\u8fdb\u884c\u5b8c\u6574\u8f68\u8ff9\u53ef\u89c6\u5316\u7684\u573a\u666f\u3002"}}
{"id": "2512.00556", "categories": ["cs.SE", "cs.CL"], "pdf": "https://arxiv.org/pdf/2512.00556", "abs": "https://arxiv.org/abs/2512.00556", "authors": ["Sina Salimian", "Gias Uddin", "Sumon Biswas", "Henry Leung"], "title": "Bias Testing and Mitigation in Black Box LLMs using Metamorphic Relations", "comment": null, "summary": "The widespread deployment of Large Language Models (LLMs) has intensified concerns about subtle social biases embedded in their outputs. Existing guardrails often fail when faced with indirect or contextually complex bias-inducing prompts. To address these limitations, we propose a unified framework for both systematic bias evaluation and targeted mitigation. Our approach introduces six novel Metamorphic Relations (MRs) that, based on metamorphic testing principles, transform direct bias-inducing inputs into semantically equivalent yet adversarially challenging variants. These transformations enable an automated method for exposing hidden model biases: when an LLM responds inconsistently or unfairly across MR-generated variants, the underlying bias becomes detectable. We further show that the same MRs can be used to generate diverse bias-inducing samples for fine-tuning, directly linking the testing process to mitigation. Using six state-of-the-art LLMs - spanning open-source and proprietary models - and a representative subset of 385 questions from the 8,978-item BiasAsker benchmark covering seven protected groups, our MRs reveal up to 14% more hidden biases compared to existing tools. Moreover, fine-tuning with both original and MR-mutated samples significantly enhances bias resiliency, increasing safe response rates from 54.7% to over 88.9% across models. These results highlight metamorphic relations as a practical mechanism for improving fairness in conversational AI.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u516d\u79cd\u65b0\u578b\u53d8\u8d28\u5173\u7cfb\uff08MRs\uff09\u7684\u7edf\u4e00\u6846\u67b6\uff0c\u7528\u4e8e\u7cfb\u7edf\u8bc4\u4f30\u548c\u7f13\u89e3\u5927\u8bed\u8a00\u6a21\u578b\u4e2d\u7684\u9690\u6027\u793e\u4f1a\u504f\u89c1\u3002\u8be5\u65b9\u6cd5\u901a\u8fc7\u751f\u6210\u8bed\u4e49\u7b49\u4ef7\u4f46\u66f4\u5177\u6311\u6218\u6027\u7684\u8f93\u5165\u53d8\u4f53\uff0c\u6709\u6548\u63ed\u793a\u73b0\u6709\u5de5\u5177\u96be\u4ee5\u68c0\u6d4b\u7684\u9690\u85cf\u504f\u89c1\uff0c\u5e76\u5229\u7528\u8fd9\u4e9b\u53d8\u4f53\u8fdb\u884c\u5fae\u8c03\uff0c\u663e\u8457\u63d0\u5347\u6a21\u578b\u7684\u516c\u5e73\u6027\u3002", "motivation": "\u5f53\u524d\u5927\u8bed\u8a00\u6a21\u578b\u4e2d\u5b58\u5728\u96be\u4ee5\u88ab\u73b0\u6709\u9632\u62a4\u673a\u5236\u8bc6\u522b\u7684\u9690\u6027\u793e\u4f1a\u504f\u89c1\uff0c\u5c24\u5176\u5728\u9762\u5bf9\u95f4\u63a5\u6216\u4e0a\u4e0b\u6587\u590d\u6742\u7684\u504f\u89c1\u8bf1\u5bfc\u63d0\u793a\u65f6\u8868\u73b0\u4e0d\u8db3\uff0c\u4e9f\u9700\u66f4\u6709\u6548\u7684\u8bc4\u4f30\u4e0e\u7f13\u89e3\u65b9\u6cd5\u3002", "method": "\u5f15\u5165\u516d\u79cd\u57fa\u4e8e\u53d8\u8d28\u6d4b\u8bd5\u539f\u7406\u7684\u65b0\u578b\u53d8\u8d28\u5173\u7cfb\uff08MRs\uff09\uff0c\u5c06\u76f4\u63a5\u504f\u89c1\u8bf1\u5bfc\u8f93\u5165\u8f6c\u5316\u4e3a\u8bed\u4e49\u7b49\u4ef7\u4f46\u5bf9\u6297\u6027\u66f4\u5f3a\u7684\u53d8\u4f53\uff0c\u7528\u4e8e\u81ea\u52a8\u5316\u68c0\u6d4b\u6a21\u578b\u504f\u89c1\uff1b\u540c\u65f6\u5229\u7528\u8fd9\u4e9b\u53d8\u4f53\u751f\u6210\u591a\u6837\u5316\u7684\u504f\u89c1\u6837\u672c\u8fdb\u884c\u5fae\u8c03\uff0c\u5b9e\u73b0\u8bc4\u4f30\u4e0e\u7f13\u89e3\u7684\u7edf\u4e00\u3002", "result": "\u5728\u516d\u4e2a\u4e3b\u6d41\u5927\u8bed\u8a00\u6a21\u578b\u548cBiasAsker\u57fa\u51c6\u5b50\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u6240\u63d0MRs\u6bd4\u73b0\u6709\u5de5\u5177\u591a\u53d1\u73b0\u6700\u591a14%\u7684\u9690\u85cf\u504f\u89c1\uff1b\u7ed3\u5408\u539f\u59cb\u4e0eMR\u53d8\u5f02\u6837\u672c\u8fdb\u884c\u5fae\u8c03\u540e\uff0c\u6a21\u578b\u5b89\u5168\u54cd\u5e94\u7387\u4ece54.7%\u63d0\u5347\u81f388.9%\u4ee5\u4e0a\u3002", "conclusion": "\u53d8\u8d28\u5173\u7cfb\u662f\u4e00\u79cd\u5b9e\u7528\u4e14\u6709\u6548\u7684\u673a\u5236\uff0c\u80fd\u591f\u663e\u8457\u63d0\u5347\u5bf9\u8bdd\u5f0f\u4eba\u5de5\u667a\u80fd\u7cfb\u7edf\u7684\u516c\u5e73\u6027\u548c\u5bf9\u504f\u89c1\u7684\u9c81\u68d2\u6027\u3002"}}
{"id": "2512.00560", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2512.00560", "abs": "https://arxiv.org/abs/2512.00560", "authors": ["Jinyu Cai", "Jialong Li", "Nianyu Li", "Zhenyu Mao", "Mingyue Zhang", "Kenji Tei"], "title": "SAGE: Semantic-Aware Gray-Box Game Regression Testing with Large Language Models", "comment": "This paper has been submitted to the Automated Software Engineering journal", "summary": "The rapid iteration cycles of modern live-service games make regression testing indispensable for maintaining quality and stability. However, existing regression testing approaches face critical limitations, especially in common gray-box settings where full source code access is unavailable: they heavily rely on manual effort for test case construction, struggle to maintain growing suites plagued by redundancy, and lack efficient mechanisms for prioritizing relevant tests. These challenges result in excessive testing costs, limited automation, and insufficient bug detection. To address these issues, we propose SAGE, a semanticaware regression testing framework for gray-box game environments. SAGE systematically addresses the core challenges of test generation, maintenance, and selection. It employs LLM-guided reinforcement learning for efficient, goal-oriented exploration to automatically generate a diverse foundational test suite. Subsequently, it applies a semantic-based multi-objective optimization to refine this suite into a compact, high-value subset by balancing cost, coverage, and rarity. Finally, it leverages LLM-based semantic analysis of update logs to prioritize test cases most relevant to version changes, enabling efficient adaptation across iterations. We evaluate SAGE on two representative environments, Overcooked Plus and Minecraft, comparing against both automated baselines and human-recorded test cases. Across all environments, SAGE achieves superior bug detection with significantly lower execution cost, while demonstrating strong adaptability to version updates.", "AI": {"tldr": "SAGE \u662f\u4e00\u4e2a\u9762\u5411\u7070\u76d2\u6e38\u620f\u73af\u5883\u7684\u8bed\u4e49\u611f\u77e5\u56de\u5f52\u6d4b\u8bd5\u6846\u67b6\uff0c\u901a\u8fc7 LLM \u5f15\u5bfc\u7684\u5f3a\u5316\u5b66\u4e60\u81ea\u52a8\u751f\u6210\u6d4b\u8bd5\u7528\u4f8b\uff0c\u7ed3\u5408\u8bed\u4e49\u591a\u76ee\u6807\u4f18\u5316\u7cbe\u7b80\u6d4b\u8bd5\u96c6\uff0c\u5e76\u5229\u7528 LLM \u5206\u6790\u66f4\u65b0\u65e5\u5fd7\u5b9e\u73b0\u9ad8\u6548\u6d4b\u8bd5\u4f18\u5148\u7ea7\u6392\u5e8f\uff0c\u5728 Overcooked Plus \u548c Minecraft \u4e0a\u9a8c\u8bc1\u4e86\u5176\u5728\u7f3a\u9677\u68c0\u6d4b\u3001\u6210\u672c\u63a7\u5236\u548c\u7248\u672c\u9002\u5e94\u6027\u65b9\u9762\u7684\u4f18\u8d8a\u6027\u3002", "motivation": "\u73b0\u4ee3\u5728\u7ebf\u670d\u52a1\u578b\u6e38\u620f\u5feb\u901f\u8fed\u4ee3\uff0c\u4f7f\u5f97\u56de\u5f52\u6d4b\u8bd5\u81f3\u5173\u91cd\u8981\uff1b\u7136\u800c\u73b0\u6709\u65b9\u6cd5\u5728\u7f3a\u4e4f\u5b8c\u6574\u6e90\u7801\u7684\u7070\u76d2\u573a\u666f\u4e0b\u5b58\u5728\u4f9d\u8d56\u4eba\u5de5\u6784\u5efa\u6d4b\u8bd5\u7528\u4f8b\u3001\u6d4b\u8bd5\u5957\u4ef6\u5197\u4f59\u96be\u4ee5\u7ef4\u62a4\u3001\u7f3a\u4e4f\u6709\u6548\u6d4b\u8bd5\u4f18\u5148\u7ea7\u673a\u5236\u7b49\u95ee\u9898\uff0c\u5bfc\u81f4\u6d4b\u8bd5\u6210\u672c\u9ad8\u3001\u81ea\u52a8\u5316\u7a0b\u5ea6\u4f4e\u3001\u7f3a\u9677\u68c0\u51fa\u80fd\u529b\u4e0d\u8db3\u3002", "method": "SAGE \u6846\u67b6\u5305\u542b\u4e09\u4e2a\u6838\u5fc3\u9636\u6bb5\uff1a1\uff09\u4f7f\u7528 LLM \u5f15\u5bfc\u7684\u5f3a\u5316\u5b66\u4e60\u8fdb\u884c\u76ee\u6807\u5bfc\u5411\u63a2\u7d22\uff0c\u81ea\u52a8\u751f\u6210\u591a\u6837\u5316\u7684\u57fa\u7840\u6d4b\u8bd5\u5957\u4ef6\uff1b2\uff09\u57fa\u4e8e\u8bed\u4e49\u7684\u591a\u76ee\u6807\u4f18\u5316\u65b9\u6cd5\uff0c\u5e73\u8861\u6210\u672c\u3001\u8986\u76d6\u7387\u4e0e\u7a00\u6709\u6027\uff0c\u63d0\u70bc\u51fa\u7d27\u51d1\u9ad8\u4ef7\u503c\u7684\u6d4b\u8bd5\u5b50\u96c6\uff1b3\uff09\u5229\u7528 LLM \u5bf9\u7248\u672c\u66f4\u65b0\u65e5\u5fd7\u8fdb\u884c\u8bed\u4e49\u5206\u6790\uff0c\u4f18\u5148\u6267\u884c\u4e0e\u53d8\u66f4\u6700\u76f8\u5173\u7684\u6d4b\u8bd5\u7528\u4f8b\u3002", "result": "\u5728 Overcooked Plus \u548c Minecraft \u4e24\u4e2a\u73af\u5883\u4e2d\u8bc4\u4f30\u8868\u660e\uff0cSAGE \u76f8\u6bd4\u81ea\u52a8\u5316\u57fa\u7ebf\u548c\u4eba\u5de5\u5f55\u5236\u7684\u6d4b\u8bd5\u7528\u4f8b\uff0c\u5728\u663e\u8457\u964d\u4f4e\u6267\u884c\u6210\u672c\u7684\u540c\u65f6\u5b9e\u73b0\u4e86\u66f4\u4f18\u7684\u7f3a\u9677\u68c0\u6d4b\u6548\u679c\uff0c\u5e76\u5c55\u73b0\u51fa\u826f\u597d\u7684\u7248\u672c\u8fed\u4ee3\u9002\u5e94\u80fd\u529b\u3002", "conclusion": "SAGE \u6709\u6548\u89e3\u51b3\u4e86\u7070\u76d2\u6e38\u620f\u73af\u5883\u4e0b\u56de\u5f52\u6d4b\u8bd5\u4e2d\u7684\u751f\u6210\u3001\u7ef4\u62a4\u4e0e\u9009\u62e9\u96be\u9898\uff0c\u4e3a\u9ad8\u9891\u7387\u8fed\u4ee3\u7684\u6e38\u620f\u63d0\u4f9b\u4e86\u9ad8\u6548\u3001\u81ea\u52a8\u4e14\u8bed\u4e49\u611f\u77e5\u7684\u56de\u5f52\u6d4b\u8bd5\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2512.00083", "categories": ["cs.AR", "cs.DC"], "pdf": "https://arxiv.org/pdf/2512.00083", "abs": "https://arxiv.org/abs/2512.00083", "authors": ["Zhongchun Zhou", "Chengtao Lai", "Wei Zhang"], "title": "LLaMCAT: Optimizing Large Language Model Inference with Cache Arbitration and Throttling", "comment": "Accepted to ICPP 2025", "summary": "Large Language Models (LLMs) have achieved unprecedented success across various applications, but their substantial memory requirements pose significant challenges to current memory system designs, especially during inference. Our work targets last-level cache (LLC) based architectures, including GPUs (e.g., NVIDIA GPUs) and AI accelerators. We introduce LLaMCAT, a novel approach to optimize the LLC for LLM inference. LLaMCAT combines Miss Status Holding Register (MSHR)- and load balance-aware cache arbitration with thread throttling to address stringent bandwidth demands and minimize cache stalls in KV Cache access. We also propose a hybrid simulation framework integrating analytical models with cycle-level simulators via memory traces, balancing architecture detail and efficiency.\n  Experiments demonstrate that LLaMCAT achieves an average speedup of 1.26x when the system is mainly bottlenecked by miss handling throughput, while baselines mostly show negative improvements since they are not optimized for this scenario. When the cache size is also limited, our policy achieves a speedup of 1.58x over the unoptimized version, and a 1.26x improvement over the best baseline (dyncta). Overall, LLaMCAT is the first to target LLM decoding-specific MSHR contention, a gap in previous work. It presents a practical solution for accelerating LLM inference on future hardware platforms.", "AI": {"tldr": "LLaMCAT \u662f\u4e00\u79cd\u9488\u5bf9\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u63a8\u7406\u4e2d\u672b\u7ea7\u7f13\u5b58\uff08LLC\uff09\u4f18\u5316\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u7ed3\u5408 MSHR \u611f\u77e5\u4e0e\u8d1f\u8f7d\u5747\u8861\u7684\u7f13\u5b58\u4ef2\u88c1\u673a\u5236\u53ca\u7ebf\u7a0b\u8282\u6d41\u7b56\u7565\uff0c\u6709\u6548\u7f13\u89e3 KV Cache \u8bbf\u95ee\u4e2d\u7684\u5e26\u5bbd\u538b\u529b\u548c\u7f13\u5b58\u505c\u987f\u3002\u5b9e\u9a8c\u8868\u660e\uff0cLLaMCAT \u5728\u4e0d\u540c\u74f6\u9888\u573a\u666f\u4e0b\u76f8\u6bd4\u57fa\u7ebf\u65b9\u6cd5\u53ef\u5b9e\u73b0\u6700\u9ad8 1.58 \u500d\u7684\u52a0\u901f\u3002", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b\u5728\u63a8\u7406\u9636\u6bb5\u5bf9\u5185\u5b58\u7cfb\u7edf\u63d0\u51fa\u4e86\u6781\u9ad8\u8981\u6c42\uff0c\u5c24\u5176\u5728\u672b\u7ea7\u7f13\u5b58\u67b6\u6784\uff08\u5982 GPU \u548c AI \u52a0\u901f\u5668\uff09\u4e2d\uff0cKV Cache \u7684\u9891\u7e41\u8bbf\u95ee\u5bfc\u81f4\u4e25\u91cd\u7684\u5e26\u5bbd\u7ade\u4e89\u548c\u7f13\u5b58\u505c\u987f\uff0c\u800c\u73b0\u6709\u65b9\u6cd5\u672a\u4e13\u95e8\u9488\u5bf9 LLM \u89e3\u7801\u9636\u6bb5\u7684 MSHR \u4e89\u7528\u95ee\u9898\u8fdb\u884c\u4f18\u5316\u3002", "method": "\u63d0\u51fa LLaMCAT \u65b9\u6cd5\uff0c\u7ed3\u5408 Miss Status Holding Register (MSHR) \u611f\u77e5\u4e0e\u8d1f\u8f7d\u5747\u8861\u7684\u7f13\u5b58\u4ef2\u88c1\u673a\u5236\uff0c\u5e76\u5f15\u5165\u7ebf\u7a0b\u8282\u6d41\u7b56\u7565\uff1b\u540c\u65f6\u6784\u5efa\u4e00\u4e2a\u878d\u5408\u5206\u6790\u6a21\u578b\u4e0e\u5468\u671f\u7ea7\u6a21\u62df\u5668\u7684\u6df7\u5408\u4eff\u771f\u6846\u67b6\uff0c\u901a\u8fc7\u5185\u5b58\u8f68\u8ff9\u5e73\u8861\u67b6\u6784\u7ec6\u8282\u4e0e\u4eff\u771f\u6548\u7387\u3002", "result": "\u5f53\u7cfb\u7edf\u4e3b\u8981\u53d7\u9650\u4e8e\u7f3a\u5931\u5904\u7406\u541e\u5410\u91cf\u65f6\uff0cLLaMCAT \u5e73\u5747\u52a0\u901f\u6bd4\u8fbe 1.26 \u500d\uff1b\u5728\u7f13\u5b58\u5bb9\u91cf\u53d7\u9650\u60c5\u51b5\u4e0b\uff0c\u76f8\u6bd4\u672a\u4f18\u5316\u7248\u672c\u63d0\u901f 1.58 \u500d\uff0c\u4f18\u4e8e\u6700\u4f73\u57fa\u7ebf\u65b9\u6cd5 dyncta \u8fbe 1.26 \u500d\u3002", "conclusion": "LLaMCAT \u9996\u6b21\u805a\u7126\u4e8e LLM \u89e3\u7801\u9636\u6bb5\u7279\u6709\u7684 MSHR \u4e89\u7528\u95ee\u9898\uff0c\u586b\u8865\u4e86\u6b64\u524d\u7814\u7a76\u7a7a\u767d\uff0c\u4e3a\u672a\u6765\u786c\u4ef6\u5e73\u53f0\u4e0a\u7684 LLM \u63a8\u7406\u52a0\u901f\u63d0\u4f9b\u4e86\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2512.00044", "categories": ["cs.AR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.00044", "abs": "https://arxiv.org/abs/2512.00044", "authors": ["Junzhuo Zhou", "Ziwen Wang", "Haoxuan Xia", "Yuxin Yan", "Chengyu Zhu", "Ting-Jung Lin", "Wei Xing", "Lei He"], "title": "SetupKit: Efficient Multi-Corner Setup/Hold Time Characterization Using Bias-Enhanced Interpolation and Active Learning", "comment": null, "summary": "Accurate setup/hold time characterization is crucial for modern chip timing closure, but its reliance on potentially millions of SPICE simulations across diverse process-voltagetemperature (PVT) corners creates a major bottleneck, often lasting weeks or months. Existing methods suffer from slow search convergence and inefficient exploration, especially in the multi-corner setting. We introduce SetupKit, a novel framework designed to break this bottleneck using statistical intelligence, circuit analysis and active learning (AL). SetupKit integrates three key innovations: BEIRA, a bias-enhanced interpolation search derived from statistical error modeling to accelerate convergence by overcoming stagnation issues, initial search interval estimation by circuit analysis and AL strategy using Gaussian Process. This AL component intelligently learns PVT-timing correlations, actively guiding the expensive simulations to the most informative corners, thus minimizing redundancy in multicorner characterization. Evaluated on industrial 22nm standard cells across 16 PVT corners, SetupKit demonstrates a significant 2.4x overall CPU time reduction (from 720 to 290 days on a single core) compared to standard practices, drastically cutting characterization time. SetupKit offers a principled, learningbased approach to library characterization, addressing a critical EDA challenge and paving the way for more intelligent simulation management.", "AI": {"tldr": "SetupKit \u662f\u4e00\u79cd\u7ed3\u5408\u7edf\u8ba1\u5efa\u6a21\u3001\u7535\u8def\u5206\u6790\u4e0e\u4e3b\u52a8\u5b66\u4e60\u7684\u65b0\u6846\u67b6\uff0c\u7528\u4e8e\u52a0\u901f\u82af\u7247\u5e93\u5355\u5143\u5728\u591a\u5de5\u827a-\u7535\u538b-\u6e29\u5ea6\uff08PVT\uff09\u89d2\u4e0b\u7684\u5efa\u7acb/\u4fdd\u6301\u65f6\u95f4\u8868\u5f81\uff0c\u76f8\u6bd4\u4f20\u7edf\u65b9\u6cd5\u51cf\u5c112.4\u500dCPU\u65f6\u95f4\u3002", "motivation": "\u73b0\u4ee3\u82af\u7247\u65f6\u5e8f\u6536\u655b\u4f9d\u8d56\u5927\u91cfSPICE\u4eff\u771f\u8fdb\u884c\u5efa\u7acb/\u4fdd\u6301\u65f6\u95f4\u8868\u5f81\uff0c\u5728\u591aPVT\u89d2\u4e0b\u8017\u65f6\u6781\u957f\uff08\u6570\u5468\u81f3\u6570\u6708\uff09\uff0c\u800c\u73b0\u6709\u65b9\u6cd5\u6536\u655b\u6162\u3001\u63a2\u7d22\u6548\u7387\u4f4e\uff0c\u4e9f\u9700\u66f4\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u63d0\u51faSetupKit\u6846\u67b6\uff0c\u5305\u542b\u4e09\u9879\u6838\u5fc3\u6280\u672f\uff1a\u57fa\u4e8e\u7edf\u8ba1\u8bef\u5dee\u5efa\u6a21\u7684\u504f\u7f6e\u589e\u5f3a\u63d2\u503c\u641c\u7d22\uff08BEIRA\uff09\u3001\u57fa\u4e8e\u7535\u8def\u5206\u6790\u7684\u521d\u59cb\u641c\u7d22\u533a\u95f4\u4f30\u8ba1\uff0c\u4ee5\u53ca\u5229\u7528\u9ad8\u65af\u8fc7\u7a0b\u7684\u4e3b\u52a8\u5b66\u4e60\u7b56\u7565\uff0c\u4ee5\u667a\u80fd\u9009\u62e9\u6700\u5177\u4fe1\u606f\u91cf\u7684PVT\u89d2\u8fdb\u884c\u4eff\u771f\u3002", "result": "\u572822nm\u5de5\u4e1a\u6807\u51c6\u5355\u5143\u300116\u4e2aPVT\u89d2\u7684\u5b9e\u9a8c\u4e2d\uff0cSetupKit\u5c06\u5355\u6838CPU\u603b\u8fd0\u884c\u65f6\u95f4\u4ece720\u5929\u964d\u81f3290\u5929\uff0c\u63d0\u901f2.4\u500d\uff0c\u663e\u8457\u7f29\u77ed\u8868\u5f81\u5468\u671f\u3002", "conclusion": "SetupKit\u63d0\u4f9b\u4e86\u4e00\u79cd\u57fa\u4e8e\u5b66\u4e60\u7684\u9ad8\u6548\u5e93\u8868\u5f81\u65b9\u6cd5\uff0c\u6709\u6548\u7f13\u89e3EDA\u4e2d\u7684\u5173\u952e\u74f6\u9888\uff0c\u4e3a\u667a\u80fd\u5316\u4eff\u771f\u7ba1\u7406\u5f00\u8f9f\u4e86\u65b0\u8def\u5f84\u3002"}}
{"id": "2512.00571", "categories": ["cs.SE", "cs.AI", "cs.LG", "cs.NE"], "pdf": "https://arxiv.org/pdf/2512.00571", "abs": "https://arxiv.org/abs/2512.00571", "authors": ["Tarun Chintada", "Uday Kiran Cheera"], "title": "Enhancing Analogy-Based Software Effort Estimation with Firefly Algorithm Optimization", "comment": "12 pages, 3 figures, 2 tables. Research conducted in June 2024", "summary": "Analogy-Based Estimation (ABE) is a popular method for non-algorithmic estimation due to its simplicity and effectiveness. The Analogy-Based Estimation (ABE) model was proposed by researchers, however, no optimal approach for reliable estimation was developed. Achieving high accuracy in the ABE might be challenging for new software projects that differ from previous initiatives. This study (conducted in June 2024) proposes a Firefly Algorithm-guided Analogy-Based Estimation (FAABE) model that combines FA with ABE to improve estimation accuracy. The FAABE model was tested on five publicly accessible datasets: Cocomo81, Desharnais, China, Albrecht, Kemerer and Maxwell. To improve prediction efficiency, feature selection was used. The results were measured using a variety of evaluation metrics; various error measures include MMRE, MAE, MSE, and RMSE. Compared to conventional models, the experimental results show notable increases in prediction precision, demonstrating the efficacy of the Firefly-Analogy ensemble.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u8424\u706b\u866b\u7b97\u6cd5\uff08FA\uff09\u4e0e\u7c7b\u6bd4\u4f30\u7b97\uff08ABE\uff09\u7684\u65b0\u6a21\u578bFAABE\uff0c\u4ee5\u63d0\u5347\u8f6f\u4ef6\u9879\u76ee\u4f30\u7b97\u7684\u51c6\u786e\u6027\uff0c\u5e76\u5728\u591a\u4e2a\u516c\u5f00\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86\u5176\u4f18\u4e8e\u4f20\u7edf\u6a21\u578b\u7684\u9884\u6d4b\u6027\u80fd\u3002", "motivation": "\u4f20\u7edf\u7c7b\u6bd4\u4f30\u7b97\uff08ABE\uff09\u65b9\u6cd5\u5728\u9762\u5bf9\u4e0e\u5386\u53f2\u9879\u76ee\u5dee\u5f02\u8f83\u5927\u7684\u65b0\u8f6f\u4ef6\u9879\u76ee\u65f6\uff0c\u96be\u4ee5\u4fdd\u8bc1\u9ad8\u7cbe\u5ea6\uff0c\u4e14\u7f3a\u4e4f\u53ef\u9760\u7684\u4f18\u5316\u7b56\u7565\u3002", "method": "\u5c06\u8424\u706b\u866b\u7b97\u6cd5\uff08Firefly Algorithm, FA\uff09\u4e0eABE\u7ed3\u5408\uff0c\u6784\u5efaFAABE\u6a21\u578b\uff0c\u5e76\u91c7\u7528\u7279\u5f81\u9009\u62e9\u6280\u672f\u63d0\u5347\u9884\u6d4b\u6548\u7387\uff0c\u5728Cocomo81\u3001Desharnais\u3001China\u3001Albrecht\u3001Kemerer\u548cMaxwell\u7b49\u4e94\u4e2a\u516c\u5f00\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u5b9e\u9a8c\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cFAABE\u5728MMRE\u3001MAE\u3001MSE\u548cRMSE\u7b49\u591a\u79cd\u8bef\u5dee\u6307\u6807\u4e0a\u5747\u663e\u8457\u4f18\u4e8e\u4f20\u7edf\u6a21\u578b\uff0c\u663e\u793a\u51fa\u66f4\u9ad8\u7684\u9884\u6d4b\u7cbe\u5ea6\u3002", "conclusion": "FA\u4e0eABE\u7684\u96c6\u6210\u6709\u6548\u63d0\u5347\u4e86\u8f6f\u4ef6\u6210\u672c\u4f30\u7b97\u7684\u51c6\u786e\u6027\uff0c\u9a8c\u8bc1\u4e86FAABE\u6a21\u578b\u7684\u53ef\u884c\u6027\u548c\u4f18\u8d8a\u6027\u3002"}}
{"id": "2512.00844", "categories": ["cs.SE", "cs.DC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.00844", "abs": "https://arxiv.org/abs/2512.00844", "authors": ["Giles Winchester", "George Parisis", "Luc Berthouze"], "title": "FC-ADL: Efficient Microservice Anomaly Detection and Localisation Through Functional Connectivity", "comment": "13 pages, 6 figures, 2 tables", "summary": "Microservices have transformed software architecture through the creation of modular and independent services. However, they introduce operational complexities in service integration and system management that makes swift and accurate anomaly detection and localisation challenging. Despite the complex, dynamic, and interconnected nature of microservice architectures, prior works that investigate metrics for anomaly detection rarely include explicit information about time-varying interdependencies. And whilst prior works on fault localisation typically do incorporate information about dependencies between microservices, they scale poorly to real world large-scale deployments due to their reliance on computationally expensive causal inference. To address these challenges we propose FC-ADL, an end-to-end scalable approach for detecting and localising anomalous changes from microservice metrics based on the neuroscientific concept of functional connectivity. We show that by efficiently characterising time-varying changes in dependencies between microservice metrics we can both detect anomalies and provide root cause candidates without incurring the significant overheads of causal and multivariate approaches. We demonstrate that our approach can achieve top detection and localisation performance across a wide degree of different fault scenarios when compared to state-of-the-art approaches. Furthermore, we illustrate the scalability of our approach by applying it to Alibaba's extremely large real-world microservice deployment.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faFC-ADL\u65b9\u6cd5\uff0c\u5229\u7528\u529f\u80fd\u8fde\u63a5\u6027\u9ad8\u6548\u68c0\u6d4b\u548c\u5b9a\u4f4d\u5fae\u670d\u52a1\u7cfb\u7edf\u4e2d\u7684\u5f02\u5e38\uff0c\u517c\u987e\u51c6\u786e\u6027\u4e0e\u53ef\u6269\u5c55\u6027\u3002", "motivation": "\u5fae\u670d\u52a1\u67b6\u6784\u867d\u5177\u6a21\u5757\u5316\u4f18\u52bf\uff0c\u4f46\u5176\u52a8\u6001\u6027\u548c\u590d\u6742\u4f9d\u8d56\u5173\u7cfb\u4f7f\u5f97\u5f02\u5e38\u68c0\u6d4b\u4e0e\u5b9a\u4f4d\u56f0\u96be\uff1b\u73b0\u6709\u65b9\u6cd5\u8981\u4e48\u5ffd\u7565\u65f6\u53d8\u4f9d\u8d56\u4fe1\u606f\uff0c\u8981\u4e48\u56e0\u4f9d\u8d56\u56e0\u679c\u63a8\u65ad\u800c\u96be\u4ee5\u6269\u5c55\u81f3\u5927\u89c4\u6a21\u7cfb\u7edf\u3002", "method": "\u57fa\u4e8e\u795e\u7ecf\u79d1\u5b66\u4e2d\u7684\u529f\u80fd\u8fde\u63a5\u6027\u6982\u5ff5\uff0c\u6784\u5efa\u7aef\u5230\u7aef\u53ef\u6269\u5c55\u65b9\u6cd5FC-ADL\uff0c\u901a\u8fc7\u9ad8\u6548\u523b\u753b\u5fae\u670d\u52a1\u6307\u6807\u95f4\u65f6\u53d8\u4f9d\u8d56\u53d8\u5316\u6765\u5b9e\u73b0\u5f02\u5e38\u68c0\u6d4b\u4e0e\u6839\u56e0\u5019\u9009\u5b9a\u4f4d\uff0c\u907f\u514d\u56e0\u679c\u63a8\u65ad\u548c\u591a\u53d8\u91cf\u65b9\u6cd5\u7684\u9ad8\u5f00\u9500\u3002", "result": "\u5728\u591a\u79cd\u6545\u969c\u573a\u666f\u4e0b\uff0cFC-ADL\u5728\u68c0\u6d4b\u4e0e\u5b9a\u4f4d\u6027\u80fd\u4e0a\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5\uff0c\u5e76\u6210\u529f\u5e94\u7528\u4e8e\u963f\u91cc\u5df4\u5df4\u8d85\u5927\u89c4\u6a21\u771f\u5b9e\u5fae\u670d\u52a1\u90e8\u7f72\uff0c\u9a8c\u8bc1\u4e86\u5176\u53ef\u6269\u5c55\u6027\u3002", "conclusion": "FC-ADL\u6709\u6548\u89e3\u51b3\u4e86\u5fae\u670d\u52a1\u7cfb\u7edf\u4e2d\u5f02\u5e38\u68c0\u6d4b\u4e0e\u5b9a\u4f4d\u7684\u51c6\u786e\u6027\u4e0e\u53ef\u6269\u5c55\u6027\u96be\u9898\uff0c\u4e3a\u5927\u89c4\u6a21\u5fae\u670d\u52a1\u8fd0\u7ef4\u63d0\u4f9b\u4e86\u5b9e\u7528\u5de5\u5177\u3002"}}
{"id": "2512.00509", "categories": ["cs.NI"], "pdf": "https://arxiv.org/pdf/2512.00509", "abs": "https://arxiv.org/abs/2512.00509", "authors": ["Sumita Majhi", "Kaushal Shelke", "Pinaki Mitra", "Ujjwal Biswas"], "title": "Improving Channel Estimation Through Gold Sequences", "comment": null, "summary": "This study evaluates Non-Orthogonal Multiple Access (NOMA) systems using Gold coding and Conventional-V-BLAST (C-V-BLAST). Superimposed signals on shared subcarriers make NOMA user separation difficult, unlike MIMO. Gold sequences' orthogonal features may enhance user separation and channel estimation. A novel channel estimation approach uses fractional power allocation and partially decoded data symbols. A realistic simulation environment was created using AWGN, Rayleigh fading, and shadowing. Using pilot signals, power allocation, and data symbols, our Channel Prediction Function (CPF) surpasses pilot-based techniques.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u7ed3\u5408Gold\u7801\u4e0e\u4f20\u7edfV-BLAST\uff08C-V-BLAST\uff09\u7684\u975e\u6b63\u4ea4\u591a\u5740\u63a5\u5165\uff08NOMA\uff09\u7cfb\u7edf\uff0c\u63d0\u51fa\u4e00\u79cd\u5229\u7528\u5206\u6570\u529f\u7387\u5206\u914d\u548c\u90e8\u5206\u89e3\u7801\u6570\u636e\u7b26\u53f7\u7684\u65b0\u4fe1\u9053\u4f30\u8ba1\u65b9\u6cd5\uff0c\u5728\u5305\u542bAWGN\u3001\u745e\u5229\u8870\u843d\u548c\u9634\u5f71\u6548\u5e94\u7684\u4eff\u771f\u73af\u5883\u4e2d\uff0c\u6240\u63d0\u51fa\u7684\u4fe1\u9053\u9884\u6d4b\u51fd\u6570\uff08CPF\uff09\u4f18\u4e8e\u4f20\u7edf\u57fa\u4e8e\u5bfc\u9891\u7684\u4f30\u8ba1\u6280\u672f\u3002", "motivation": "\u7531\u4e8eNOMA\u7cfb\u7edf\u4e2d\u7528\u6237\u4fe1\u53f7\u5728\u5171\u4eab\u5b50\u8f7d\u6ce2\u4e0a\u53e0\u52a0\uff0c\u5bfc\u81f4\u7528\u6237\u5206\u79bb\u56f0\u96be\uff0c\u800cGold\u5e8f\u5217\u5177\u6709\u826f\u597d\u7684\u6b63\u4ea4\u7279\u6027\uff0c\u6709\u671b\u63d0\u5347\u7528\u6237\u5206\u79bb\u6027\u80fd\u4e0e\u4fe1\u9053\u4f30\u8ba1\u7cbe\u5ea6\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u65b0\u578b\u4fe1\u9053\u4f30\u8ba1\u65b9\u6cd5\uff0c\u7ed3\u5408\u5206\u6570\u529f\u7387\u5206\u914d\u3001\u90e8\u5206\u89e3\u7801\u7684\u6570\u636e\u7b26\u53f7\u53ca\u5bfc\u9891\u4fe1\u53f7\uff0c\u6784\u5efa\u4fe1\u9053\u9884\u6d4b\u51fd\u6570\uff08CPF\uff09\uff0c\u5e76\u5728\u5305\u542bAWGN\u3001\u745e\u5229\u8870\u843d\u548c\u9634\u5f71\u6548\u5e94\u7684\u4eff\u771f\u73af\u5883\u4e2d\u8fdb\u884c\u9a8c\u8bc1\u3002", "result": "\u6240\u63d0\u51fa\u7684CPF\u65b9\u6cd5\u5728 realistic \u4eff\u771f\u6761\u4ef6\u4e0b\u4f18\u4e8e\u4f20\u7edf\u7684\u57fa\u4e8e\u5bfc\u9891\u7684\u4fe1\u9053\u4f30\u8ba1\u6280\u672f\u3002", "conclusion": "Gold\u7801\u4e0eC-V-BLAST\u7ed3\u5408\u7684NOMA\u7cfb\u7edf\u901a\u8fc7\u65b0\u63d0\u51fa\u7684CPF\u4fe1\u9053\u4f30\u8ba1\u65b9\u6cd5\uff0c\u80fd\u6709\u6548\u63d0\u5347\u7528\u6237\u5206\u79bb\u548c\u4fe1\u9053\u4f30\u8ba1\u6027\u80fd\u3002"}}
{"id": "2512.00045", "categories": ["cs.AR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.00045", "abs": "https://arxiv.org/abs/2512.00045", "authors": ["Hung-Ming Huang", "Yu-Hsin Yang", "Fu-Chieh Chang", "Yun-Chia Hsu", "Yin-Yu Lin", "Ming-Fang Tsai", "Chun-Chih Yang", "Pei-Yuan Wu"], "title": "Assessing Large Language Models in Generating RTL Design Specifications", "comment": null, "summary": "As IC design grows more complex, automating comprehension and documentation of RTL code has become increasingly important. Engineers currently should manually interpret existing RTL code and write specifications, a slow and error-prone process. Although LLMs have been studied for generating RTL from specifications, automated specification generation remains underexplored, largely due to the lack of reliable evaluation methods. To address this gap, we investigate how prompting strategies affect RTL-to-specification quality and introduce metrics for faithfully evaluating generated specs. We also benchmark open-source and commercial LLMs, providing a foundation for more automated and efficient specification workflows in IC design.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u5728IC\u8bbe\u8ba1\u4e2d\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u81ea\u52a8\u751f\u6210RTL\u4ee3\u7801\u5bf9\u5e94\u89c4\u683c\u8bf4\u660e\u7684\u65b9\u6cd5\uff0c\u63a2\u8ba8\u4e86\u4e0d\u540c\u63d0\u793a\u7b56\u7565\u5bf9\u751f\u6210\u8d28\u91cf\u7684\u5f71\u54cd\uff0c\u5e76\u63d0\u51fa\u4e86\u8bc4\u4f30\u751f\u6210\u89c4\u683c\u53ef\u9760\u6027\u7684\u65b0\u6307\u6807\uff0c\u540c\u65f6\u5bf9\u5f00\u6e90\u548c\u5546\u7528LLM\u8fdb\u884c\u4e86\u57fa\u51c6\u6d4b\u8bd5\u3002", "motivation": "\u968f\u7740IC\u8bbe\u8ba1\u65e5\u76ca\u590d\u6742\uff0c\u624b\u52a8\u89e3\u8bfbRTL\u4ee3\u7801\u5e76\u64b0\u5199\u89c4\u683c\u8bf4\u660e\u65e2\u8017\u65f6\u53c8\u6613\u51fa\u9519\uff1b\u5c3d\u7ba1\u5df2\u6709\u7814\u7a76\u5173\u6ce8\u4ece\u89c4\u683c\u751f\u6210RTL\uff0c\u4f46\u81ea\u52a8\u751f\u6210\u89c4\u683c\u4ecd\u7f3a\u4e4f\u6709\u6548\u8bc4\u4f30\u624b\u6bb5\uff0c\u4e9f\u9700\u7cfb\u7edf\u6027\u63a2\u7d22\u3002", "method": "\u901a\u8fc7\u5206\u6790\u4e0d\u540c\u63d0\u793a\uff08prompting\uff09\u7b56\u7565\u5bf9RTL\u5230\u89c4\u683c\u751f\u6210\u8d28\u91cf\u7684\u5f71\u54cd\uff0c\u63d0\u51fa\u7528\u4e8e\u5fe0\u5b9e\u8bc4\u4f30\u751f\u6210\u89c4\u683c\u7684\u65b0\u6307\u6807\uff0c\u5e76\u5bf9\u591a\u79cd\u5f00\u6e90\u4e0e\u5546\u4e1a\u5927\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u57fa\u51c6\u6d4b\u8bd5\u3002", "result": "\u63ed\u793a\u4e86\u63d0\u793a\u7b56\u7565\u5bf9\u751f\u6210\u8d28\u91cf\u7684\u663e\u8457\u5f71\u54cd\uff0c\u6240\u63d0\u51fa\u7684\u8bc4\u4f30\u6307\u6807\u80fd\u6709\u6548\u8861\u91cf\u751f\u6210\u89c4\u683c\u7684\u51c6\u786e\u6027\uff0c\u5e76\u63d0\u4f9b\u4e86\u4e0d\u540cLLM\u5728\u8be5\u4efb\u52a1\u4e0a\u7684\u6027\u80fd\u5bf9\u6bd4\u7ed3\u679c\u3002", "conclusion": "\u672c\u7814\u7a76\u4e3aIC\u8bbe\u8ba1\u4e2d\u5b9e\u73b0\u66f4\u81ea\u52a8\u5316\u3001\u9ad8\u6548\u7684\u89c4\u683c\u751f\u6210\u6d41\u7a0b\u5960\u5b9a\u4e86\u57fa\u7840\uff0c\u63a8\u52a8\u4e86LLM\u5728\u786c\u4ef6\u6587\u6863\u81ea\u52a8\u5316\u4e2d\u7684\u5e94\u7528\u3002"}}
{"id": "2512.00651", "categories": ["cs.SE", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.00651", "abs": "https://arxiv.org/abs/2512.00651", "authors": ["Mohammed Latif Siddiq", "Arvin Islam-Gomes", "Natalie Sekerak", "Joanna C. S. Santos"], "title": "Large Language Models for Software Engineering: A Reproducibility Crisis", "comment": "Submitted to Empirical Software Engineering (EMSE) journal; 112 pages (81 pages of references)", "summary": "Reproducibility is a cornerstone of scientific progress, yet its state in large language model (LLM)-based software engineering (SE) research remains poorly understood. This paper presents the first large-scale, empirical study of reproducibility practices in LLM-for-SE research. We systematically mined and analyzed 640 papers published between 2017 and 2025 across premier software engineering, machine learning, and natural language processing venues, extracting structured metadata from publications, repositories, and documentation. Guided by four research questions, we examine (i) the prevalence of reproducibility smells, (ii) how reproducibility has evolved over time, (iii) whether artifact evaluation badges reliably reflect reproducibility quality, and (iv) how publication venues influence transparency practices. Using a taxonomy of seven smell categories: Code and Execution, Data, Documentation, Environment and Tooling, Versioning, Model, and Access and Legal, we manually annotated all papers and associated artifacts. Our analysis reveals persistent gaps in artifact availability, environment specification, versioning rigor, and documentation clarity, despite modest improvements in recent years and increased adoption of artifact evaluation processes at top SE venues. Notably, we find that badges often signal artifact presence but do not consistently guarantee execution fidelity or long-term reproducibility. Motivated by these findings, we provide actionable recommendations to mitigate reproducibility smells and introduce a Reproducibility Maturity Model (RMM) to move beyond binary artifact certification toward multi-dimensional, progressive evaluation of reproducibility rigor.", "AI": {"tldr": "\u672c\u6587\u5bf9640\u7bc72017\u81f32025\u5e74\u95f4\u53d1\u8868\u7684\u5c06\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5e94\u7528\u4e8e\u8f6f\u4ef6\u5de5\u7a0b\uff08SE\uff09\u7684\u7814\u7a76\u8bba\u6587\u8fdb\u884c\u4e86\u5927\u89c4\u6a21\u5b9e\u8bc1\u5206\u6790\uff0c\u9996\u6b21\u7cfb\u7edf\u8bc4\u4f30\u5176\u53ef\u590d\u73b0\u6027\u5b9e\u8df5\u3002\u7814\u7a76\u53d1\u73b0\u5c3d\u7ba1\u8fd1\u5e74\u6765\u6709\u6240\u6539\u5584\uff0c\u4f46\u5728\u4ee3\u7801\u3001\u73af\u5883\u3001\u7248\u672c\u63a7\u5236\u548c\u6587\u6863\u7b49\u65b9\u9762\u4ecd\u5b58\u5728\u663e\u8457\u7f3a\u9677\uff1b\u540c\u65f6\u6307\u51fa\u5f53\u524d\u7684\u6210\u679c\u5fbd\u7ae0\u5236\u5ea6\u867d\u80fd\u53cd\u6620\u4ea7\u51fa\u7269\u7684\u5b58\u5728\uff0c\u5374\u96be\u4ee5\u4fdd\u8bc1\u6267\u884c\u4e00\u81f4\u6027\u548c\u957f\u671f\u53ef\u590d\u73b0\u6027\uff0c\u5e76\u636e\u6b64\u63d0\u51fa\u53ef\u590d\u73b0\u6027\u6210\u719f\u5ea6\u6a21\u578b\uff08RMM\uff09\u53ca\u6539\u8fdb\u5efa\u8bae\u3002", "motivation": "\u5f53\u524d\u5728\u5927\u8bed\u8a00\u6a21\u578b\u7528\u4e8e\u8f6f\u4ef6\u5de5\u7a0b\u7684\u7814\u7a76\u4e2d\uff0c\u53ef\u590d\u73b0\u6027\u72b6\u51b5\u5c1a\u4e0d\u6e05\u6670\uff0c\u7f3a\u4e4f\u7cfb\u7edf\u6027\u8bc4\u4f30\u3002\u4f5c\u8005\u65e8\u5728\u63ed\u793a\u8be5\u9886\u57df\u53ef\u590d\u73b0\u6027\u5b9e\u8df5\u7684\u771f\u5b9e\u6c34\u5e73\uff0c\u8bc6\u522b\u5e38\u89c1\u95ee\u9898\uff08\u201c\u53ef\u590d\u73b0\u6027\u5f02\u5473\u201d\uff09\uff0c\u5e76\u4e3a\u63d0\u5347\u7814\u7a76\u900f\u660e\u5ea6\u4e0e\u4e25\u8c28\u6027\u63d0\u4f9b\u6307\u5bfc\u3002", "method": "\u4f5c\u8005\u7cfb\u7edf\u6536\u96c6\u5e76\u5206\u6790\u4e862017\u20132025\u5e74\u95f4\u53d1\u8868\u4e8e\u9876\u7ea7\u8f6f\u4ef6\u5de5\u7a0b\u3001\u673a\u5668\u5b66\u4e60\u548c\u81ea\u7136\u8bed\u8a00\u5904\u7406\u4f1a\u8bae\u7684640\u7bc7\u8bba\u6587\uff0c\u4ece\u8bba\u6587\u3001\u4ee3\u7801\u4ed3\u5e93\u548c\u6587\u6863\u4e2d\u63d0\u53d6\u7ed3\u6784\u5316\u5143\u6570\u636e\uff1b\u57fa\u4e8e\u4e03\u7c7b\u201c\u53ef\u590d\u73b0\u6027\u5f02\u5473\u201d\uff08\u4ee3\u7801\u4e0e\u6267\u884c\u3001\u6570\u636e\u3001\u6587\u6863\u3001\u73af\u5883\u4e0e\u5de5\u5177\u3001\u7248\u672c\u63a7\u5236\u3001\u6a21\u578b\u3001\u8bbf\u95ee\u4e0e\u6cd5\u5f8b\uff09\uff0c\u5bf9\u6240\u6709\u8bba\u6587\u53ca\u5176\u4ea7\u51fa\u7269\u8fdb\u884c\u4eba\u5de5\u6807\u6ce8\uff0c\u5e76\u56f4\u7ed5\u56db\u4e2a\u7814\u7a76\u95ee\u9898\u5c55\u5f00\u5b9a\u91cf\u4e0e\u5b9a\u6027\u5206\u6790\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff1a\uff081\uff09\u4ea7\u51fa\u7269\u53ef\u7528\u6027\u3001\u73af\u5883\u63cf\u8ff0\u3001\u7248\u672c\u63a7\u5236\u548c\u6587\u6863\u6e05\u6670\u5ea6\u7b49\u65b9\u9762\u5b58\u5728\u6301\u7eed\u6027\u4e0d\u8db3\uff1b\uff082\uff09\u8fd1\u5e74\u867d\u6709\u5c0f\u5e45\u6539\u8fdb\u4e14\u9876\u7ea7SE\u4f1a\u8bae\u66f4\u5e7f\u6cdb\u91c7\u7528\u4ea7\u51fa\u7269\u8bc4\u5ba1\u673a\u5236\uff0c\u4f46\u95ee\u9898\u4ecd\u672a\u6839\u672c\u89e3\u51b3\uff1b\uff083\uff09\u73b0\u6709\u5fbd\u7ae0\u4e3b\u8981\u53cd\u6620\u4ea7\u51fa\u7269\u662f\u5426\u63d0\u4f9b\uff0c\u800c\u975e\u80fd\u5426\u6210\u529f\u590d\u73b0\u6216\u957f\u671f\u7ef4\u6301\uff1b\uff084\uff09\u4e0d\u540c\u53d1\u8868\u573a\u6240\u5bf9\u900f\u660e\u5ea6\u5b9e\u8df5\u6709\u663e\u8457\u5f71\u54cd\u3002", "conclusion": "\u5f53\u524dLLM-for-SE\u7814\u7a76\u7684\u53ef\u590d\u73b0\u6027\u6574\u4f53\u4ecd\u4e0d\u7406\u60f3\uff0c\u9700\u8d85\u8d8a\u7b80\u5355\u7684\u201c\u6709\u65e0\u4ea7\u51fa\u7269\u201d\u4e8c\u5143\u5224\u65ad\u3002\u4f5c\u8005\u63d0\u51fa\u53ef\u590d\u73b0\u6027\u6210\u719f\u5ea6\u6a21\u578b\uff08RMM\uff09\u4ee5\u652f\u6301\u591a\u7ef4\u5ea6\u3001\u6e10\u8fdb\u5f0f\u7684\u53ef\u590d\u73b0\u6027\u8bc4\u4f30\uff0c\u5e76\u7ed9\u51fa\u5177\u4f53\u5efa\u8bae\u4ee5\u51cf\u5c11\u201c\u53ef\u590d\u73b0\u6027\u5f02\u5473\u201d\uff0c\u63a8\u52a8\u8be5\u9886\u57df\u7814\u7a76\u5411\u66f4\u9ad8\u4e25\u8c28\u6027\u53d1\u5c55\u3002"}}
{"id": "2512.00053", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2512.00053", "abs": "https://arxiv.org/abs/2512.00053", "authors": ["Nikhil Rout", "Blaise Tine"], "title": "A Configurable Mixed-Precision Fused Dot Product Unit for GPGPU Tensor Computation", "comment": "3 pages, 2 figures", "summary": "Efficient mixed-precision MMA operations are critical for accelerating Deep Learning workloads on GPGPUs. However, existing open-source RTL implementations of inner dot products rely on discrete arithmetic units, leading to suboptimal throughput and poor resource utilization. To address these challenges, we propose a scalable mixed-precision dot product unit that integrates floating-point and integer arithmetic pipelines within a singular fused architecture, implemented as part of the open-source RISC-V based Vortex GPGPU's Tensor Core Unit extension. Our design supports low-precision multiplication in (FP16/BF16/FP8/BF8/INT8/UINT4) formats and higher-precision accumulation in (FP32/INT32), with an extensible framework for adding and evaluating other custom representations in the future. Experimental results demonstrate 4-cycle operation latency at 306.6 MHz clock frequency on the AMD Xilinx Alveo U55C FPGA, delivering an ideal filled pipeline throughput of 9.812 GFLOPS in a 4-thread per warp configuration.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u53ef\u6269\u5c55\u7684\u6df7\u5408\u7cbe\u5ea6\u70b9\u79ef\u5355\u5143\uff0c\u901a\u8fc7\u5728\u7edf\u4e00\u878d\u5408\u67b6\u6784\u4e2d\u96c6\u6210\u6d6e\u70b9\u4e0e\u6574\u6570\u8fd0\u7b97\u6d41\u6c34\u7ebf\uff0c\u663e\u8457\u63d0\u5347\u4e86GPGPU\u4e0a\u6df1\u5ea6\u5b66\u4e60\u4efb\u52a1\u7684\u541e\u5410\u91cf\u548c\u8d44\u6e90\u5229\u7528\u7387\uff0c\u5e76\u5728\u5f00\u6e90RISC-V Vortex GPGPU\u7684Tensor Core Unit\u6269\u5c55\u4e2d\u5b9e\u73b0\u3002", "motivation": "\u73b0\u6709\u5f00\u6e90RTL\u5b9e\u73b0\u7684\u5185\u79ef\u8fd0\u7b97\u4f9d\u8d56\u79bb\u6563\u7684\u7b97\u672f\u5355\u5143\uff0c\u5bfc\u81f4\u541e\u5410\u91cf\u4e0d\u8db3\u548c\u8d44\u6e90\u5229\u7528\u6548\u7387\u4f4e\u4e0b\uff0c\u96be\u4ee5\u6ee1\u8db3\u6df1\u5ea6\u5b66\u4e60\u5bf9\u9ad8\u6548\u6df7\u5408\u7cbe\u5ea6\u77e9\u9635\u4e58\u52a0\uff08MMA\uff09\u64cd\u4f5c\u7684\u9700\u6c42\u3002", "method": "\u8bbe\u8ba1\u5e76\u5b9e\u73b0\u4e86\u4e00\u4e2a\u878d\u5408\u6d6e\u70b9\u4e0e\u6574\u6570\u8fd0\u7b97\u6d41\u6c34\u7ebf\u7684\u5355\u4e00\u67b6\u6784\u6df7\u5408\u7cbe\u5ea6\u70b9\u79ef\u5355\u5143\uff0c\u652f\u6301\u591a\u79cd\u4f4e\u7cbe\u5ea6\u683c\u5f0f\uff08FP16/BF16/FP8/BF8/INT8/UINT4\uff09\u4e58\u6cd5\u4e0e\u9ad8\u7cbe\u5ea6\uff08FP32/INT32\uff09\u7d2f\u52a0\uff0c\u5e76\u5177\u5907\u53ef\u6269\u5c55\u6027\u4ee5\u652f\u6301\u672a\u6765\u81ea\u5b9a\u4e49\u6570\u636e\u683c\u5f0f\u3002", "result": "\u5728AMD Xilinx Alveo U55C FPGA\u4e0a\u5b9e\u73b04\u5468\u671f\u64cd\u4f5c\u5ef6\u8fdf\uff0c\u65f6\u949f\u9891\u7387\u8fbe306.6 MHz\uff0c\u5728\u6bcfwarp 4\u7ebf\u7a0b\u914d\u7f6e\u4e0b\u8fbe\u52309.812 GFLOPS\u7684\u7406\u60f3\u6ee1\u6d41\u6c34\u541e\u5410\u91cf\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u878d\u5408\u67b6\u6784\u6709\u6548\u89e3\u51b3\u4e86\u73b0\u6709\u5b9e\u73b0\u7684\u6027\u80fd\u74f6\u9888\uff0c\u4e3a\u5f00\u6e90GPGPU\u63d0\u4f9b\u4e86\u9ad8\u6548\u3001\u7075\u6d3b\u4e14\u53ef\u6269\u5c55\u7684\u6df7\u5408\u7cbe\u5ea6\u8ba1\u7b97\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2512.00766", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2512.00766", "abs": "https://arxiv.org/abs/2512.00766", "authors": ["Zenghui Zhou", "Yuechen Li", "Yi Cai", "Jinlong Wen", "Xiaohan Yu", "Zheng Zheng", "Beibei Yin"], "title": "Code Comments for Quantum Software Development Kits: An Empirical Study on Qiskit", "comment": "Zenghui Zhou and Yuechen Li contributed equally to this work. Corresponding author is Zheng Zheng", "summary": "Quantum computing is gaining attention from academia and industry. With the quantum Software Development Kits (SDKs), programmers can develop quantum software to explore the power of quantum computing. However, programmers may face challenges in understanding quantum software due to the non-intuitive quantum mechanics. To facilitate software development and maintenance, code comments offered in quantum SDKs serve as a natural language explanation of program functionalities and logical flows. Despite their importance, scarce research systematically reports their value and provides constructive guidelines for programmers. To address this gap, our paper focuses on Qiskit, one of the most popular quantum SDKs, and presents CC4Q, the first dataset of code comments for quantum computing. CC4Q incorporates 9677 code comment pairs and 21970 sentence-level code comment units, the latter of which involve heavy human annotation. Regarding the annotation, we validate the applicability of the developer-intent taxonomy used in classical programs, and also propose a new taxonomy considering quantum-specific knowledge. We conduct an empirical study comprehensively interpreting code comments from three perspectives: comment structure and coverage, developers' intentions, and associated quantum topics. Our findings uncover key differences in code comments between classical and quantum software, and also outline quantum-specific knowledge relevant to quantum software development.", "AI": {"tldr": "\u672c\u6587\u6784\u5efa\u4e86\u9996\u4e2a\u91cf\u5b50\u8ba1\u7b97\u4ee3\u7801\u6ce8\u91ca\u6570\u636e\u96c6CC4Q\uff0c\u901a\u8fc7\u4eba\u5de5\u6807\u6ce8\u548c\u5b9e\u8bc1\u7814\u7a76\uff0c\u5206\u6790\u4e86Qiskit\u4e2d\u4ee3\u7801\u6ce8\u91ca\u7684\u7ed3\u6784\u3001\u5f00\u53d1\u8005\u610f\u56fe\u53ca\u91cf\u5b50\u76f8\u5173\u4e3b\u9898\uff0c\u63ed\u793a\u4e86\u91cf\u5b50\u8f6f\u4ef6\u4e0e\u7ecf\u5178\u8f6f\u4ef6\u5728\u6ce8\u91ca\u4e0a\u7684\u5173\u952e\u5dee\u5f02\uff0c\u5e76\u63d0\u51fa\u4e86\u9002\u7528\u4e8e\u91cf\u5b50\u9886\u57df\u7684\u6ce8\u91ca\u5206\u7c7b\u4f53\u7cfb\u3002", "motivation": "\u7531\u4e8e\u91cf\u5b50\u529b\u5b66\u7684\u975e\u76f4\u89c2\u6027\uff0c\u7a0b\u5e8f\u5458\u5728\u7406\u89e3\u548c\u7ef4\u62a4\u91cf\u5b50\u8f6f\u4ef6\u65f6\u9762\u4e34\u6311\u6218\uff0c\u800c\u4ee3\u7801\u6ce8\u91ca\u4f5c\u4e3a\u89e3\u91ca\u7a0b\u5e8f\u529f\u80fd\u548c\u903b\u8f91\u7684\u91cd\u8981\u624b\u6bb5\uff0c\u7f3a\u4e4f\u7cfb\u7edf\u6027\u7814\u7a76\u548c\u5b9e\u7528\u6307\u5bfc\u3002\u4e3a\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\uff0c\u4f5c\u8005\u805a\u7126\u4e8e\u4e3b\u6d41\u91cf\u5b50SDK Qiskit\uff0c\u5f00\u5c55\u5bf9\u4ee3\u7801\u6ce8\u91ca\u7684\u6df1\u5165\u5206\u6790\u3002", "method": "\u4f5c\u8005\u6784\u5efa\u4e86\u5305\u542b9677\u4e2a\u4ee3\u7801\u6ce8\u91ca\u5bf9\u548c21970\u4e2a\u53e5\u5b50\u7ea7\u6ce8\u91ca\u5355\u5143\u7684\u6570\u636e\u96c6CC4Q\uff0c\u5e76\u8fdb\u884c\u4e86\u5927\u91cf\u4eba\u5de5\u6807\u6ce8\uff1b\u9a8c\u8bc1\u4e86\u7ecf\u5178\u7a0b\u5e8f\u4e2d\u5f00\u53d1\u8005\u610f\u56fe\u5206\u7c7b\u6cd5\u5728\u91cf\u5b50\u8f6f\u4ef6\u4e2d\u7684\u9002\u7528\u6027\uff0c\u540c\u65f6\u63d0\u51fa\u8003\u8651\u91cf\u5b50\u7279\u6709\u77e5\u8bc6\u7684\u65b0\u5206\u7c7b\u6cd5\uff1b\u4ece\u6ce8\u91ca\u7ed3\u6784\u4e0e\u8986\u76d6\u7387\u3001\u5f00\u53d1\u8005\u610f\u56fe\u548c\u91cf\u5b50\u4e3b\u9898\u4e09\u4e2a\u89d2\u5ea6\u5f00\u5c55\u5b9e\u8bc1\u7814\u7a76\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u91cf\u5b50\u8f6f\u4ef6\u4e0e\u7ecf\u5178\u8f6f\u4ef6\u5728\u4ee3\u7801\u6ce8\u91ca\u65b9\u9762\u5b58\u5728\u663e\u8457\u5dee\u5f02\uff0c\u5e76\u8bc6\u522b\u51fa\u4e0e\u91cf\u5b50\u8f6f\u4ef6\u5f00\u53d1\u76f8\u5173\u7684\u7279\u6709\u77e5\u8bc6\u5185\u5bb9\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e0d\u4ec5\u63d0\u4f9b\u4e86\u9996\u4e2a\u91cf\u5b50\u8ba1\u7b97\u4ee3\u7801\u6ce8\u91ca\u6570\u636e\u96c6\uff0c\u8fd8\u901a\u8fc7\u5b9e\u8bc1\u5206\u6790\u63ed\u793a\u4e86\u91cf\u5b50\u6ce8\u91ca\u7684\u7279\u70b9\uff0c\u4e3a\u672a\u6765\u91cf\u5b50\u8f6f\u4ef6\u7684\u5f00\u53d1\u3001\u7ef4\u62a4\u548c\u6ce8\u91ca\u5b9e\u8df5\u63d0\u4f9b\u4e86\u7406\u8bba\u57fa\u7840\u548c\u5b9e\u7528\u6307\u5bfc\u3002"}}
{"id": "2512.00855", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2512.00855", "abs": "https://arxiv.org/abs/2512.00855", "authors": ["Miikka Kuutila", "Paul Ralph", "Huilian Sophie Qiu", "Ronnie de Souza Santos", "Morakot Choetkiertikul", "Amin Milani Fard", "Rana Alkadhi", "Xavier Devroey", "Gregorio Robles", "Hideaki Hata", "Sebastian Baltes", "Vladimir Kovalenko", "Shalini Chakraborty", "Eray Tuzun", "Hera Arif", "Gianisa Adisaputri", "Kelly Garc\u00e9s", "Anielle S. L. Andrade", "Eyram Amedzor", "Bimpe Ayoola", "Keisha Gaspard-Chickoree", "Arazoo Hoseyni"], "title": "The Software Infrastructure Attitude Scale (SIAS): A Questionnaire Instrument for Measuring Professionals' Attitudes Toward Technical and Sociotechnical Infrastructure", "comment": "Accepted to ICSE 2026, 11 pages + 2 for references, 1 figure, 7 tables", "summary": "Context: Recent software engineering (SE) research has highlighted the need for sociotechnical research, implying a demand for customized psychometric scales. Objective: We define the concepts of technical and sociotechnical infrastructure in software engineering, and develop and validate a psychometric scale that measures attitudes toward them. Method: Grounded in theories of infrastructure, attitudes, and prior work on psychometric measurement, we defined the target constructs and generated scale items. The scale was administered to 225 software professionals and evaluated using a split sample. We conducted an exploratory factor analysis (EFA) on one half of the sample to uncover the underlying factor structure and performed a confirmatory factor analysis (CFA) on the other half to validate the structure. Further analyses with the whole sample assessed face, criterion-related, and discriminant validity. Results: EFA supported a two-factor structure (technical and sociotechnical infrastructure), accounting for 65% of the total variance with strong loadings. CFA confirmed excellent model fit. Face and content validity were supported by the item content reflecting cognitive, affective, and behavioral components. Both subscales were correlated with job satisfaction, perceived autonomy, and feedback from the job itself, supporting convergent validity. Regression analysis supported criterion-related validity, while the Heterotrait-Monotrait ratio of correlations (HTMT), the Fornell-Larcker criterion, and model comparison all supported discriminant validity. Discussion: The resulting scale is a valid instrument for measuring attitudes toward technical and sociotechnical infrastructure in software engineering research. Our work contributes to ongoing efforts to integrate psychological measurement rigor into empirical and behavioral software engineering research.", "AI": {"tldr": "\u672c\u6587\u5f00\u53d1\u5e76\u9a8c\u8bc1\u4e86\u4e00\u4e2a\u7528\u4e8e\u6d4b\u91cf\u8f6f\u4ef6\u5de5\u7a0b\u5e08\u5bf9\u6280\u672f\u548c\u793e\u4f1a\u6280\u672f\u57fa\u7840\u8bbe\u65bd\u6001\u5ea6\u7684\u5fc3\u7406\u6d4b\u91cf\u91cf\u8868\uff0c\u901a\u8fc7\u63a2\u7d22\u6027\u548c\u9a8c\u8bc1\u6027\u56e0\u5b50\u5206\u6790\u8bc1\u5b9e\u4e86\u5176\u826f\u597d\u7684\u4fe1\u6548\u5ea6\u3002", "motivation": "\u5f53\u524d\u8f6f\u4ef6\u5de5\u7a0b\u7814\u7a76\u5f3a\u8c03\u793e\u4f1a\u6280\u672f\u89c6\u89d2\u7684\u91cd\u8981\u6027\uff0c\u4f46\u7f3a\u4e4f\u4e13\u95e8\u7528\u4e8e\u8bc4\u4f30\u76f8\u5173\u6001\u5ea6\u7684\u5fc3\u7406\u6d4b\u91cf\u5de5\u5177\uff0c\u56e0\u6b64\u9700\u8981\u6784\u5efa\u4e00\u4e2a\u6709\u6548\u3001\u53ef\u9760\u7684\u91cf\u8868\u4ee5\u652f\u6301\u5b9e\u8bc1\u7814\u7a76\u3002", "method": "\u57fa\u4e8e\u57fa\u7840\u8bbe\u65bd\u7406\u8bba\u4e0e\u6001\u5ea6\u7406\u8bba\uff0c\u8bbe\u8ba1\u91cf\u8868\u6761\u76ee\uff0c\u5e76\u5bf9225\u540d\u8f6f\u4ef6\u4ece\u4e1a\u8005\u8fdb\u884c\u8c03\u67e5\uff1b\u91c7\u7528\u5206\u6837\u672c\u7b56\u7565\uff0c\u4e00\u534a\u7528\u4e8e\u63a2\u7d22\u6027\u56e0\u5b50\u5206\u6790\uff08EFA\uff09\uff0c\u53e6\u4e00\u534a\u7528\u4e8e\u9a8c\u8bc1\u6027\u56e0\u5b50\u5206\u6790\uff08CFA\uff09\uff0c\u5e76\u8fdb\u4e00\u6b65\u8bc4\u4f30\u5185\u5bb9\u6548\u5ea6\u3001\u805a\u5408\u6548\u5ea6\u3001\u533a\u5206\u6548\u5ea6\u548c\u51c6\u5219\u6548\u5ea6\u3002", "result": "EFA\u652f\u6301\u6280\u672f\u548c\u793e\u6280\u57fa\u7840\u8bbe\u65bd\u7684\u4e8c\u56e0\u5b50\u7ed3\u6784\uff0c\u89e3\u91ca65%\u65b9\u5dee\uff1bCFA\u663e\u793a\u6a21\u578b\u62df\u5408\u826f\u597d\uff1b\u91cf\u8868\u5728\u5185\u5bb9\u3001\u805a\u5408\u3001\u533a\u5206\u548c\u51c6\u5219\u6548\u5ea6\u65b9\u9762\u5747\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "\u6240\u5f00\u53d1\u7684\u91cf\u8868\u662f\u6d4b\u91cf\u8f6f\u4ef6\u5de5\u7a0b\u5e08\u5bf9\u6280\u672f\u548c\u793e\u6280\u57fa\u7840\u8bbe\u65bd\u6001\u5ea6\u7684\u6709\u6548\u5de5\u5177\uff0c\u6709\u52a9\u4e8e\u63d0\u5347\u8f6f\u4ef6\u5de5\u7a0b\u4e2d\u884c\u4e3a\u4e0e\u5b9e\u8bc1\u7814\u7a76\u7684\u5fc3\u7406\u6d4b\u91cf\u4e25\u8c28\u6027\u3002"}}
{"id": "2512.01088", "categories": ["cs.NI"], "pdf": "https://arxiv.org/pdf/2512.01088", "abs": "https://arxiv.org/abs/2512.01088", "authors": ["Jingxiang Huang", "Samer Lahoud"], "title": "Physical-Layer Analysis of LoRa Robustness in the Presence of Narrowband Interference", "comment": null, "summary": "With the rapid development of Internet of Things (IoT) technologies, the sub-GHz unlicensed spectrum is increasingly being shared by protocols such as Long Range (LoRa), Sigfox, and Long-Range Frequency-Hopping Spread Spectrum (LR-FHSS). These protocols must coexist within the same frequency bands, leading to mutual interference. This paper investigates the physical-layer impact of two types of narrowband signals (BPSK and GMSK) on LoRa demodulation. We employ symbol-level Monte Carlo simulations to analyse how the interference-to-noise ratio (INR) affects the symbol error rate (SER) at a given signal-to-noise ratio (SNR) and noise floor, and then compare the results with those for additive white Gaussian noise (AWGN) of equal power. We demonstrate that modelling narrowband interference as additive white Gaussian noise (AWGN) systematically overestimates the SER of Chirp Spread Spectrum (CSS) demodulation. We also clarify the distinct impairment levels induced by AWGN and two types of narrowband interferers, and provide physical insight into the underlying mechanisms. Finally, we fit a two-segment function for the maximum INR that ensures correct demodulation across SNRs, with one segment for low SNR and the other for high SNR.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u7a84\u5e26\u5e72\u6270\uff08BPSK\u548cGMSK\uff09\u5bf9LoRa\u89e3\u8c03\u7684\u7269\u7406\u5c42\u5f71\u54cd\uff0c\u53d1\u73b0\u5c06\u6b64\u7c7b\u5e72\u6270\u5efa\u6a21\u4e3a\u52a0\u6027\u9ad8\u65af\u767d\u566a\u58f0\uff08AWGN\uff09\u4f1a\u7cfb\u7edf\u6027\u9ad8\u4f30Chirp\u6269\u9891\uff08CSS\uff09\u89e3\u8c03\u7684\u7b26\u53f7\u9519\u8bef\u7387\uff08SER\uff09\uff0c\u5e76\u63d0\u51fa\u4e86\u9002\u7528\u4e8e\u4e0d\u540c\u4fe1\u566a\u6bd4\u6761\u4ef6\u4e0b\u7684\u6700\u5927\u53ef\u5bb9\u5fcd\u5e72\u6270\u566a\u58f0\u6bd4\uff08INR\uff09\u5206\u6bb5\u51fd\u6570\u6a21\u578b\u3002", "motivation": "\u968f\u7740\u7269\u8054\u7f51\u6280\u672f\u7684\u53d1\u5c55\uff0cSub-GHz\u514d\u6388\u6743\u9891\u8c31\u88abLoRa\u3001Sigfox\u548cLR-FHSS\u7b49\u591a\u79cd\u534f\u8bae\u5171\u4eab\uff0c\u5bfc\u81f4\u76f8\u4e92\u5e72\u6270\u3002\u51c6\u786e\u8bc4\u4f30\u7a84\u5e26\u5e72\u6270\u5bf9LoRa\u6027\u80fd\u7684\u5f71\u54cd\u81f3\u5173\u91cd\u8981\uff0c\u800c\u4f20\u7edfAWGN\u6a21\u578b\u53ef\u80fd\u65e0\u6cd5\u771f\u5b9e\u53cd\u6620\u5b9e\u9645\u5e72\u6270\u6548\u5e94\u3002", "method": "\u91c7\u7528\u7b26\u53f7\u7ea7\u8499\u7279\u5361\u6d1b\u4eff\u771f\uff0c\u5728\u7ed9\u5b9a\u4fe1\u566a\u6bd4\uff08SNR\uff09\u548c\u566a\u58f0\u5e95\u6761\u4ef6\u4e0b\uff0c\u5206\u6790\u5e72\u6270\u566a\u58f0\u6bd4\uff08INR\uff09\u5bf9LoRa\u7b26\u53f7\u9519\u8bef\u7387\uff08SER\uff09\u7684\u5f71\u54cd\uff0c\u5e76\u4e0e\u7b49\u529f\u7387AWGN\u60c5\u51b5\u5bf9\u6bd4\uff1b\u8fdb\u4e00\u6b65\u62df\u5408\u51fa\u786e\u4fdd\u6b63\u786e\u89e3\u8c03\u7684\u6700\u5927INR\u4e0eSNR\u4e4b\u95f4\u7684\u4e24\u6bb5\u5f0f\u51fd\u6570\u5173\u7cfb\u3002", "result": "\u7814\u7a76\u8868\u660e\uff0c\u5c06\u7a84\u5e26\u5e72\u6270\u5efa\u6a21\u4e3aAWGN\u4f1a\u7cfb\u7edf\u6027\u9ad8\u4f30CSS\u89e3\u8c03\u7684SER\uff1bBPSK\u548cGMSK\u5e72\u6270\u5bf9LoRa\u9020\u6210\u7684\u635f\u4f24\u7a0b\u5ea6\u660e\u663e\u4e0d\u540c\u4e8eAWGN\uff1b\u5728\u4f4eSNR\u548c\u9ad8SNR\u533a\u57df\u5206\u522b\u9002\u7528\u4e0d\u540c\u7684\u6700\u5927INR\u9608\u503c\u3002", "conclusion": "\u7a84\u5e26\u5e72\u6270\u4e0d\u80fd\u7b80\u5355\u7b49\u6548\u4e3aAWGN\uff0c\u9700\u91c7\u7528\u66f4\u7cbe\u786e\u7684\u5e72\u6270\u6a21\u578b\u6765\u8bc4\u4f30LoRa\u7cfb\u7edf\u6027\u80fd\uff1b\u6240\u63d0\u51fa\u7684\u4e24\u6bb5\u5f0fINR-SNR\u51fd\u6570\u53ef\u4e3a\u5b9e\u9645\u7cfb\u7edf\u8bbe\u8ba1\u63d0\u4f9b\u53c2\u8003\u3002"}}
{"id": "2512.00867", "categories": ["cs.SE", "cs.HC"], "pdf": "https://arxiv.org/pdf/2512.00867", "abs": "https://arxiv.org/abs/2512.00867", "authors": ["Obada Kraishan"], "title": "The AI Attribution Paradox: Transparency as Social Strategy in Open-Source Software Development", "comment": "23 pages, 7 figures, 9 tables", "summary": "AI coding assistants have transformed software development, raising questions about transparency and attribution practices. We examine the \"AI attribution paradox\": how developers strategically balance acknowledging AI assistance with managing community scrutiny. Analyzing 14,300 GitHub commits across 7,393 repositories from 2023-2025, we investigated attribution strategies and community responses across eight major AI tools. Results reveal widespread AI usage (95.2% of commits) but strategic attribution: only 29.5% employ explicit disclosure, with dramatic tool variation (Claude 80.5% versus Copilot 9.0%). Explicit attribution triggers modest scrutiny (23% more questions and 21% more comments) but tool choice matters 20-30 times more for predicting reception. Community sentiment remains neutral regardless of attribution type, suggesting curiosity rather than hostility. Temporal analyses show rapid norm evolution: explicit attribution increased from near-zero in early 2024 to 40% by late 2025, indicating community adaptation. These findings illuminate attribution as strategic communication rather than simple transparency, advancing understanding of algorithmic accountability and norm formation during technological transitions. We discuss implications for developers navigating disclosure decisions, platforms designing attribution mechanisms, and researchers studying emergent practices in AI-augmented collaborative work.", "AI": {"tldr": "\u8be5\u7814\u7a76\u5206\u6790\u4e862023-2025\u5e74\u95f414,300\u4e2aGitHub\u63d0\u4ea4\uff0c\u63ed\u793a\u5f00\u53d1\u8005\u5728\u4f7f\u7528AI\u7f16\u7a0b\u52a9\u624b\u65f6\u5b58\u5728\u201cAI\u5f52\u5c5e\u6096\u8bba\u201d\uff1a\u5c3d\u7ba195.2%\u7684\u63d0\u4ea4\u6d89\u53caAI\uff0c\u4f46\u4ec529.5%\u660e\u786e\u62ab\u9732\uff0c\u4e14\u4e0d\u540c\u5de5\u5177\uff08\u5982Claude 80.5% vs Copilot 9.0%\uff09\u5dee\u5f02\u663e\u8457\u3002\u660e\u786e\u5f52\u5c5e\u4f1a\u5f15\u53d1\u7565\u591a\u793e\u533a\u4e92\u52a8\uff0c\u4f46\u5de5\u5177\u9009\u62e9\u5bf9\u793e\u533a\u53cd\u5e94\u5f71\u54cd\u66f4\u5927\uff1b\u793e\u533a\u60c5\u7eea\u603b\u4f53\u4e2d\u7acb\uff0c\u8868\u660e\u597d\u5947\u800c\u975e\u654c\u610f\u3002\u5f52\u5c5e\u884c\u4e3a\u968f\u65f6\u95f4\u5feb\u901f\u6f14\u53d8\uff0c\u53cd\u6620\u5176\u4f5c\u4e3a\u7b56\u7565\u6027\u6c9f\u901a\u800c\u975e\u5355\u7eaf\u900f\u660e\u3002", "motivation": "\u63a2\u8ba8AI\u7f16\u7a0b\u52a9\u624b\u666e\u53ca\u80cc\u666f\u4e0b\u5f00\u53d1\u8005\u5982\u4f55\u5728\u627f\u8ba4AI\u534f\u52a9\u4e0e\u5e94\u5bf9\u793e\u533a\u5ba1\u89c6\u4e4b\u95f4\u53d6\u5f97\u5e73\u8861\uff0c\u5373\u6240\u8c13\u7684\u201cAI\u5f52\u5c5e\u6096\u8bba\u201d\uff0c\u4ee5\u7406\u89e3\u65b0\u5174\u6280\u672f\u91c7\u7eb3\u8fc7\u7a0b\u4e2d\u7684\u900f\u660e\u5ea6\u3001\u8d23\u4efb\u5f52\u5c5e\u4e0e\u793e\u533a\u89c4\u8303\u5f62\u6210\u673a\u5236\u3002", "method": "\u5206\u67902023\u81f32025\u5e74\u95f4\u6765\u81ea7,393\u4e2aGitHub\u4ed3\u5e93\u768414,300\u6b21\u63d0\u4ea4\uff0c\u8003\u5bdf\u516b\u79cd\u4e3b\u6d41AI\u5de5\u5177\u7684\u5f52\u5c5e\u7b56\u7565\u53ca\u793e\u533a\u53cd\u9988\uff0c\u91c7\u7528\u5b9a\u91cf\u5185\u5bb9\u5206\u6790\u4e0e\u65f6\u95f4\u5e8f\u5217\u65b9\u6cd5\u8bc4\u4f30\u5f52\u5c5e\u7387\u3001\u793e\u533a\u53cd\u5e94\uff08\u63d0\u95ee\u3001\u8bc4\u8bba\u6570\u91cf\uff09\u53ca\u60c5\u611f\u503e\u5411\u3002", "result": "\u7814\u7a76\u53d1\u73b0AI\u4f7f\u7528\u6781\u4e3a\u666e\u904d\uff0895.2%\u7684\u63d0\u4ea4\uff09\uff0c\u4f46\u663e\u5f0f\u5f52\u5c5e\u6bd4\u4f8b\u8f83\u4f4e\uff0829.5%\uff09\uff0c\u4e14\u56e0\u5de5\u5177\u800c\u5f02\uff08Claude\u8fbe80.5%\uff0cCopilot\u4ec59.0%\uff09\u3002\u663e\u5f0f\u5f52\u5c5e\u5e26\u6765\u8f7b\u5fae\u66f4\u591a\u793e\u533a\u4e92\u52a8\uff08\u63d0\u95ee+23%\uff0c\u8bc4\u8bba+21%\uff09\uff0c\u4f46\u5de5\u5177\u7c7b\u578b\u5bf9\u793e\u533a\u63a5\u53d7\u5ea6\u7684\u5f71\u54cd\u8fdc\u5927\u4e8e\u5f52\u5c5e\u65b9\u5f0f\uff0820\u201330\u500d\uff09\u3002\u793e\u533a\u60c5\u7eea\u6574\u4f53\u4e2d\u6027\uff0c\u4e14\u663e\u5f0f\u5f52\u5c5e\u6bd4\u4f8b\u4ece2024\u5e74\u521d\u63a5\u8fd1\u96f6\u5347\u81f32025\u5e74\u5e95\u768440%\uff0c\u663e\u793a\u89c4\u8303\u5feb\u901f\u6f14\u5316\u3002", "conclusion": "AI\u5f52\u5c5e\u884c\u4e3a\u672c\u8d28\u4e0a\u662f\u4e00\u79cd\u7b56\u7565\u6027\u6c9f\u901a\uff0c\u800c\u975e\u5355\u7eaf\u7684\u900f\u660e\u5ea6\u5b9e\u8df5\u3002\u7814\u7a76\u6df1\u5316\u4e86\u5bf9\u7b97\u6cd5\u95ee\u8d23\u548c\u4eba\u673a\u534f\u4f5c\u4e2d\u89c4\u8303\u5f62\u6210\u7684\u7406\u89e3\uff0c\u5e76\u4e3a\u5f00\u53d1\u8005\u62ab\u9732\u51b3\u7b56\u3001\u5e73\u53f0\u8bbe\u8ba1\u5f52\u5c5e\u673a\u5236\u53ca\u672a\u6765\u76f8\u5173\u7814\u7a76\u63d0\u4f9b\u542f\u793a\u3002"}}
{"id": "2512.01477", "categories": ["cs.NI"], "pdf": "https://arxiv.org/pdf/2512.01477", "abs": "https://arxiv.org/abs/2512.01477", "authors": ["Saso Nikolovski", "Pece Mitrevski"], "title": "Modeling and Simulation of Data Protection Systems for Business Continuity and Disaster Recovery", "comment": "17 pages, 11 figures, 9 tables", "summary": "In today's corporate landscape, particularly where operations rely heavily on information technologies, establishing a robust business continuity plan, including a disaster recovery strategy, is essential for ensuring swift recuperation following outages. This study presents a comparative analysis of recovery solutions, focusing on systems that operate partially or entirely within cloud environments and assessing their reliability in fulfilling organizational roles securely and dependably. Two such systems were deployed and evaluated in a real-world production setting. Key performance and reliability metrics were identified using simulation software to enhance these systems, alongside a System Dynamics analysis conducted for each. This work proposes a comprehensive framework for selecting and maintaining data protection and recovery solutions within organizational structures, outlining criteria for aligning chosen approaches with operational needs while adhering to predetermined timelines specified in business continuity and disaster recovery plans. The resulting analysis and findings offer actionable insights to guide decision-making when selecting appropriate recovery concepts.", "AI": {"tldr": "\u672c\u6587\u5bf9\u6bd4\u5206\u6790\u4e86\u90e8\u5206\u6216\u5b8c\u5168\u57fa\u4e8e\u4e91\u73af\u5883\u7684\u707e\u96be\u6062\u590d\u7cfb\u7edf\uff0c\u901a\u8fc7\u5b9e\u9645\u90e8\u7f72\u4e0e\u4eff\u771f\u8bc4\u4f30\u5176\u53ef\u9760\u6027\uff0c\u5e76\u63d0\u51fa\u4e00\u4e2a\u7528\u4e8e\u9009\u62e9\u548c\u7ef4\u62a4\u6570\u636e\u4fdd\u62a4\u4e0e\u6062\u590d\u65b9\u6848\u7684\u7efc\u5408\u6846\u67b6\u3002", "motivation": "\u5728\u9ad8\u5ea6\u4f9d\u8d56\u4fe1\u606f\u6280\u672f\u7684\u4f01\u4e1a\u73af\u5883\u4e2d\uff0c\u5236\u5b9a\u6709\u6548\u7684\u4e1a\u52a1\u8fde\u7eed\u6027\u548c\u707e\u96be\u6062\u590d\u7b56\u7565\u5bf9\u5feb\u901f\u4ece\u6545\u969c\u4e2d\u6062\u590d\u81f3\u5173\u91cd\u8981\u3002", "method": "\u5728\u771f\u5b9e\u751f\u4ea7\u73af\u5883\u4e2d\u90e8\u7f72\u4e24\u4e2a\u4e91\u76f8\u5173\u6062\u590d\u7cfb\u7edf\uff0c\u5229\u7528\u4eff\u771f\u8f6f\u4ef6\u8bc6\u522b\u5173\u952e\u6027\u80fd\u4e0e\u53ef\u9760\u6027\u6307\u6807\uff0c\u5e76\u5bf9\u6bcf\u4e2a\u7cfb\u7edf\u8fdb\u884c\u7cfb\u7edf\u52a8\u529b\u5b66\u5206\u6790\u3002", "result": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u7efc\u5408\u6846\u67b6\uff0c\u5305\u542b\u9009\u62e9\u548c\u7ef4\u62a4\u6570\u636e\u6062\u590d\u89e3\u51b3\u65b9\u6848\u7684\u6807\u51c6\uff0c\u4ee5\u5339\u914d\u7ec4\u7ec7\u8fd0\u8425\u9700\u6c42\u5e76\u6ee1\u8db3\u4e1a\u52a1\u8fde\u7eed\u6027\u8ba1\u5212\u4e2d\u7684\u65f6\u95f4\u8981\u6c42\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u4e3a\u7ec4\u7ec7\u5728\u9009\u62e9\u5408\u9002\u707e\u96be\u6062\u590d\u65b9\u6848\u65f6\u63d0\u4f9b\u4e86\u53ef\u64cd\u4f5c\u7684\u51b3\u7b56\u4f9d\u636e\u3002"}}
{"id": "2512.01571", "categories": ["cs.NI"], "pdf": "https://arxiv.org/pdf/2512.01571", "abs": "https://arxiv.org/abs/2512.01571", "authors": ["Xiao Xu", "Qiong Wu", "Pingyi Fan", "Kezhi Wang", "Nan Cheng", "Wen Chen", "Khaled B. Letaief"], "title": "Velocity-Adaptive Access Scheme for Semantic-Aware Vehicular Networks: Joint Fairness and AoI Optimization", "comment": "This paper has been submitted to IEEE transactions on moblie computing", "summary": "In this paper, we address the problem of fair access and Age of Information (AoI) optimization in 5G New Radio (NR) Vehicle to Everything (V2X) Mode 2. Specifically, vehicles need to exchange information with the road side unit (RSU). However, due to the varying vehicle speeds leading to different communication durations, the amount of data exchanged between different vehicles and the RSU may vary. This may poses significant safety risks in high-speed environments. To address this, we define a fairness index through tuning the selection window of different vehicles and consider the image semantic communication system to reduce latency. However, adjusting the selection window may affect the communication time, thereby impacting the AoI. Moreover, considering the re-evaluation mechanism in 5G NR, which helps reduce resource collisions, it may lead to an increase in AoI. We analyze the AoI using Stochastic Hybrid System (SHS) and construct a multi-objective optimization problem to achieve fair access and AoI optimization. Sequential Convex Approximation (SCA) is employed to transform the non-convex problem into a convex one, and solve it using convex optimization. We also provide a large language model (LLM) based algorithm. The scheme's effectiveness is validated through numerical simulations.", "AI": {"tldr": "\u672c\u6587\u9488\u5bf95G NR V2X Mode 2\u4e2d\u56e0\u8f66\u8f86\u901f\u5ea6\u5dee\u5f02\u5bfc\u81f4\u901a\u4fe1\u65f6\u957f\u4e0d\u540c\u3001\u8fdb\u800c\u5f15\u53d1\u4fe1\u606f\u4ea4\u6362\u4e0d\u516c\u4e0eAge of Information\uff08AoI\uff09\u6076\u5316\u7684\u95ee\u9898\uff0c\u63d0\u51fa\u4e00\u79cd\u517c\u987e\u516c\u5e73\u63a5\u5165\u4e0eAoI\u4f18\u5316\u7684\u65b9\u6848\u3002\u901a\u8fc7\u8c03\u6574\u5404\u8f66\u8f86\u7684\u9009\u62e9\u7a97\u53e3\u5b9a\u4e49\u516c\u5e73\u6027\u6307\u6807\uff0c\u5e76\u7ed3\u5408\u56fe\u50cf\u8bed\u4e49\u901a\u4fe1\u964d\u4f4e\u65f6\u5ef6\uff1b\u540c\u65f6\u8003\u86515G NR\u4e2d\u7684\u91cd\u8bc4\u4f30\u673a\u5236\u5bf9AoI\u7684\u5f71\u54cd\u3002\u5229\u7528\u968f\u673a\u6df7\u5408\u7cfb\u7edf\uff08SHS\uff09\u5206\u6790AoI\uff0c\u6784\u5efa\u591a\u76ee\u6807\u4f18\u5316\u95ee\u9898\uff0c\u5e76\u91c7\u7528\u5e8f\u5217\u51f8\u903c\u8fd1\uff08SCA\uff09\u5c06\u5176\u8f6c\u5316\u4e3a\u51f8\u95ee\u9898\u6c42\u89e3\uff0c\u8fd8\u8bbe\u8ba1\u4e86\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u7b97\u6cd5\uff0c\u4eff\u771f\u9a8c\u8bc1\u4e86\u6240\u63d0\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "motivation": "\u57285G NR V2X Mode 2\u573a\u666f\u4e0b\uff0c\u8f66\u8f86\u4e0e\u8def\u4fa7\u5355\u5143\uff08RSU\uff09\u901a\u4fe1\u65f6\uff0c\u7531\u4e8e\u8f66\u901f\u4e0d\u540c\u9020\u6210\u901a\u4fe1\u6301\u7eed\u65f6\u95f4\u5dee\u5f02\uff0c\u5bfc\u81f4\u5404\u8f66\u8f86\u4ea4\u6362\u7684\u6570\u636e\u91cf\u4e0d\u5747\uff0c\u53ef\u80fd\u5e26\u6765\u9ad8\u901f\u73af\u5883\u4e0b\u7684\u5b89\u5168\u9690\u60a3\uff0c\u56e0\u6b64\u9700\u540c\u65f6\u4fdd\u969c\u63a5\u5165\u516c\u5e73\u6027\u4e0e\u4fe1\u606f\u65b0\u9c9c\u5ea6\uff08AoI\uff09\u3002", "method": "\u5b9a\u4e49\u57fa\u4e8e\u9009\u62e9\u7a97\u53e3\u7684\u516c\u5e73\u6027\u6307\u6807\uff0c\u5f15\u5165\u56fe\u50cf\u8bed\u4e49\u901a\u4fe1\u4ee5\u964d\u4f4e\u4f20\u8f93\u5ef6\u8fdf\uff1b\u5229\u7528\u968f\u673a\u6df7\u5408\u7cfb\u7edf\uff08SHS\uff09\u5efa\u6a21\u5e76\u5206\u6790AoI\uff1b\u6784\u5efa\u516c\u5e73\u63a5\u5165\u4e0eAoI\u4f18\u5316\u7684\u591a\u76ee\u6807\u95ee\u9898\uff0c\u91c7\u7528\u5e8f\u5217\u51f8\u903c\u8fd1\uff08SCA\uff09\u5c06\u975e\u51f8\u95ee\u9898\u8f6c\u5316\u4e3a\u51f8\u4f18\u5316\u95ee\u9898\u8fdb\u884c\u6c42\u89e3\uff0c\u5e76\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u8f85\u52a9\u7b97\u6cd5\u3002", "result": "\u901a\u8fc7\u6570\u503c\u4eff\u771f\u9a8c\u8bc1\u4e86\u6240\u63d0\u65b9\u6848\u5728\u63d0\u5347\u63a5\u5165\u516c\u5e73\u6027\u548c\u964d\u4f4eAoI\u65b9\u9762\u7684\u6709\u6548\u6027\uff0c\u8868\u660e\u8be5\u65b9\u6cd5\u80fd\u591f\u5728\u8003\u86515G NR\u91cd\u8bc4\u4f30\u673a\u5236\u7684\u524d\u63d0\u4e0b\uff0c\u5b9e\u73b0\u6027\u80fd\u4e0e\u516c\u5e73\u6027\u7684\u826f\u597d\u5e73\u8861\u3002", "conclusion": "\u672c\u6587\u6210\u529f\u6784\u5efa\u5e76\u89e3\u51b3\u4e865G NR V2X Mode 2\u4e2d\u516c\u5e73\u63a5\u5165\u4e0eAoI\u8054\u5408\u4f18\u5316\u7684\u95ee\u9898\uff0c\u6240\u63d0\u65b9\u6cd5\u517c\u987e\u4e86\u901a\u4fe1\u516c\u5e73\u6027\u3001\u4fe1\u606f\u65f6\u6548\u6027\u4e0e\u7cfb\u7edf\u6548\u7387\uff0c\u4e3a\u9ad8\u53ef\u9760\u4f4e\u65f6\u5ef6\u8f66\u8054\u7f51\u901a\u4fe1\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848\u3002"}}
{"id": "2512.01141", "categories": ["cs.SE", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.01141", "abs": "https://arxiv.org/abs/2512.01141", "authors": ["Muhammad Yousuf", "Akshat Bagade", "Chhittebbayi Penugonda", "Maanas Baraya"], "title": "Neural Variable Name Repair: Learning to Rename Identifiers for Readability", "comment": null, "summary": "Developers routinely work with source files whose variable names are generic or misleading, and with teams moving quickly, many functions are left undocumented. This slows comprehension, increases the risk of subtle bugs, and makes it harder for both humans and large language models (LLMs) to reason about code. We study variable name repair: given a real C++ function where all occurrences of one local or parameter name have been replaced by a placeholder (e.g. ID 1), the goal is to generate a natural, descriptive replacement name. We automatically construct this task from the C++ portion of BigCode's The Stack by parsing functions with Tree-sitter, masking a single identifier, and treating the original name as supervision. On top of Llama 3.1-8B, we build a pipeline with (i) warmup and dropout schedules for more stable fine-tuning, (ii) LoRA adapters for efficient specialization on identifier repair, and (iii) a dual-encoder reranker over top-k generator candidates. We evaluate using exact match, Top-5 Hit, and an embedding-based partial similarity score (0-100) that gives credit for near synonyms and format variants (e.g., jsonValue vs. json). On a held-out set of 200 C++ functions, a zero-shot Llama 3.1 baseline reaches 6.1 percent exact match. Our best LoRA-tuned model (with warmup and dropout) achieves 43.1 percent exact match, 50.2 percent Top-5 Hit, and an 82.03 partial-match score. A dual encoder reranker further improves selection quality without modifying the underlying generator, suggesting that task-specific fine-tuning plus reranking is a promising approach for practical identifier repair tools.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76C++\u4ee3\u7801\u4e2d\u53d8\u91cf\u540d\u4fee\u590d\u95ee\u9898\uff0c\u901a\u8fc7\u6784\u5efa\u57fa\u4e8eLlama 3.1-8B\u7684\u5fae\u8c03\u4e0e\u91cd\u6392\u5e8f\u6d41\u7a0b\uff0c\u5728\u4fdd\u7559\u539f\u59cb\u8bed\u4e49\u7684\u524d\u63d0\u4e0b\u663e\u8457\u63d0\u5347\u6a21\u578b\u751f\u6210\u51c6\u786e\u3001\u63cf\u8ff0\u6027\u5f3a\u7684\u53d8\u91cf\u540d\u7684\u80fd\u529b\u3002", "motivation": "\u5f00\u53d1\u8005\u5e38\u9762\u5bf9\u53d8\u91cf\u547d\u540d\u6cdb\u5316\u6216\u5177\u6709\u8bef\u5bfc\u6027\u3001\u51fd\u6570\u7f3a\u4e4f\u6587\u6863\u7684\u4ee3\u7801\uff0c\u8fd9\u4f1a\u964d\u4f4e\u7406\u89e3\u6548\u7387\u3001\u589e\u52a0\u9690\u6027\u9519\u8bef\u98ce\u9669\uff0c\u5e76\u5f71\u54cd\u4eba\u7c7b\u548c\u5927\u8bed\u8a00\u6a21\u578b\u5bf9\u4ee3\u7801\u7684\u63a8\u7406\u80fd\u529b\u3002", "method": "\u4eceThe Stack\u7684C++\u4ee3\u7801\u4e2d\u81ea\u52a8\u6784\u5efa\u6570\u636e\u96c6\uff0c\u4f7f\u7528Tree-sitter\u89e3\u6790\u51fd\u6570\u5e76\u63a9\u7801\u5355\u4e2a\u6807\u8bc6\u7b26\uff1b\u5728Llama 3.1-8B\u57fa\u7840\u4e0a\u91c7\u7528\u5e26\u9884\u70ed\u548cDropout\u7684LoRA\u5fae\u8c03\u7b56\u7565\uff0c\u5e76\u7ed3\u5408\u53cc\u7f16\u7801\u5668\u91cd\u6392\u5e8f\u5668\u5bf9Top-k\u5019\u9009\u8fdb\u884c\u7b5b\u9009\u3002", "result": "\u5728200\u4e2aC++\u51fd\u6570\u6d4b\u8bd5\u96c6\u4e0a\uff0c\u96f6\u6837\u672cLlama 3.1\u57fa\u7ebf\u4ec5\u8fbe6.1%\u7cbe\u786e\u5339\u914d\u7387\uff0c\u800c\u6700\u4f73LoRA\u5fae\u8c03\u6a21\u578b\u8fbe\u523043.1%\u7cbe\u786e\u5339\u914d\u300150.2% Top-5\u547d\u4e2d\u7387\u548c82.03\u7684\u5d4c\u5165\u76f8\u4f3c\u5ea6\u5f97\u5206\uff1b\u91cd\u6392\u5e8f\u5668\u8fdb\u4e00\u6b65\u63d0\u5347\u4e86\u9009\u62e9\u8d28\u91cf\u3002", "conclusion": "\u4efb\u52a1\u7279\u5b9a\u7684\u5fae\u8c03\u7ed3\u5408\u91cd\u6392\u5e8f\u662f\u4e00\u79cd\u6709\u524d\u666f\u7684\u5b9e\u7528\u53d8\u91cf\u540d\u4fee\u590d\u65b9\u6cd5\uff0c\u80fd\u6709\u6548\u63d0\u5347\u4ee3\u7801\u53ef\u8bfb\u6027\u4e0e\u6a21\u578b\u63a8\u7406\u80fd\u529b\u3002"}}
{"id": "2512.00096", "categories": ["cs.AR", "cs.ET"], "pdf": "https://arxiv.org/pdf/2512.00096", "abs": "https://arxiv.org/abs/2512.00096", "authors": ["Mahdi Aghaei", "Saba Ebrahimi", "Mohammad Saleh Arafati", "Elham Cheshmikhani", "Dara Rahmati", "Saeid Gorgin", "Jungrae Kim"], "title": "Modeling and Simulation Frameworks for Processing-in-Memory Architectures", "comment": null, "summary": "Processing-in-Memory (PIM) has emerged as a promising computing paradigm to address the memory wall and the fundamental bottleneck of the von Neumann architecture by reducing costly data movement between memory and processing units. As with any engineering challenge, identifying the most effective solutions requires thorough exploration of diverse architectural proposals, device technologies, and application domains. In this context, simulation plays a critical role in enabling researchers to evaluate, compare, and refine PIM designs prior to fabrication. Over the past decade, a variety of PIM simulators have been introduced, spanning low-level device models, architectural frameworks, and application-oriented environments. These tools differ significantly in fidelity, scalability, supported memory/compute technologies, and benchmark compatibility. Understanding these trade-offs is essential for researchers to select appropriate simulators that accurately map and validate their research efforts. This chapter provides a comprehensive overview of PIM simulation methodologies and tools. We categorize simulators according to abstraction levels, design objectives, and evaluation metrics, highlighting representative examples. To improve accessibility, some content may appear in multiple contexts to guide readers with different backgrounds. We also survey benchmark suites commonly employed in PIM studies and discuss open challenges in simulation methodology, paving the way for more reliable, scalable, and efficient PIM modeling.", "AI": {"tldr": "\u672c\u6587\u7efc\u8ff0\u4e86\u9762\u5411\u5b58\u5185\u8ba1\u7b97\uff08PIM\uff09\u7684\u4eff\u771f\u65b9\u6cd5\u4e0e\u5de5\u5177\uff0c\u6309\u62bd\u8c61\u5c42\u7ea7\u3001\u8bbe\u8ba1\u76ee\u6807\u548c\u8bc4\u4f30\u6307\u6807\u8fdb\u884c\u5206\u7c7b\uff0c\u5e76\u8ba8\u8bba\u4e86\u5e38\u7528\u57fa\u51c6\u6d4b\u8bd5\u5957\u4ef6\u53ca\u4eff\u771f\u65b9\u6cd5\u4e2d\u7684\u5f00\u653e\u6311\u6218\u3002", "motivation": "\u4e3a\u5e94\u5bf9\u51af\u00b7\u8bfa\u4f9d\u66fc\u67b6\u6784\u4e2d\u7684\u201c\u5185\u5b58\u5899\u201d\u95ee\u9898\uff0cPIM\u6210\u4e3a\u6709\u524d\u666f\u7684\u8ba1\u7b97\u8303\u5f0f\uff1b\u800c\u4eff\u771f\u5728\u8bc4\u4f30\u548c\u6bd4\u8f83\u4e0d\u540cPIM\u8bbe\u8ba1\u65b9\u6848\u4e2d\u8d77\u5173\u952e\u4f5c\u7528\uff0c\u56e0\u6b64\u9700\u7cfb\u7edf\u68b3\u7406\u73b0\u6709\u4eff\u771f\u5de5\u5177\u53ca\u5176\u9002\u7528\u573a\u666f\u3002", "method": "\u5bf9\u8fc7\u53bb\u5341\u5e74\u63d0\u51fa\u7684\u5404\u7c7bPIM\u4eff\u771f\u5668\u8fdb\u884c\u5206\u7c7b\u7efc\u8ff0\uff0c\u4f9d\u636e\u62bd\u8c61\u5c42\u7ea7\u3001\u8bbe\u8ba1\u76ee\u6807\u548c\u8bc4\u4f30\u6307\u6807\u7ec4\u7ec7\u5185\u5bb9\uff0c\u5e76\u5206\u6790\u5176\u5728\u4fdd\u771f\u5ea6\u3001\u53ef\u6269\u5c55\u6027\u3001\u652f\u6301\u6280\u672f\u53ca\u57fa\u51c6\u517c\u5bb9\u6027\u7b49\u65b9\u9762\u7684\u5dee\u5f02\u3002", "result": "\u5f52\u7eb3\u51fa\u4ee3\u8868\u6027PIM\u4eff\u771f\u5de5\u5177\u7684\u7279\u70b9\uff0c\u63d0\u4f9b\u9009\u62e9\u6307\u5357\uff0c\u5e76\u603b\u7ed3\u5e38\u7528\u57fa\u51c6\u6d4b\u8bd5\u5957\u4ef6\uff0c\u6307\u51fa\u5f53\u524d\u4eff\u771f\u65b9\u6cd5\u4e2d\u5b58\u5728\u7684\u5f00\u653e\u6027\u6311\u6218\u3002", "conclusion": "\u5168\u9762\u7406\u89e3PIM\u4eff\u771f\u5de5\u5177\u7684\u6743\u8861\u6709\u52a9\u4e8e\u7814\u7a76\u8005\u51c6\u786e\u6620\u5c04\u548c\u9a8c\u8bc1\u5176\u5de5\u4f5c\uff0c\u672a\u6765\u9700\u53d1\u5c55\u66f4\u53ef\u9760\u3001\u53ef\u6269\u5c55\u4e14\u9ad8\u6548\u7684PIM\u5efa\u6a21\u65b9\u6cd5\u3002"}}
{"id": "2512.01155", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.01155", "abs": "https://arxiv.org/abs/2512.01155", "authors": ["Krishna Kumaar Sharma"], "title": "Beyond Greenfield: AI-Driven Productivity in Documentation and Brownfield Engineering", "comment": "53 pages, 7 figures", "summary": "Brownfield engineering work involving legacy systems, incomplete documentation, and fragmented architectural knowledge poses unique challenges for the effective use of large language models (LLMs). Prior research has largely focused on greenfield or synthetic tasks, leaving a gap in structured workflows for complex, context-heavy environments. This paper introduces the Discover-Define-Deliver (D3) Framework, a disciplined LLM-assisted workflow that combines role-separated prompting strategies with applied best practices for navigating ambiguity in brownfield systems. The framework incorporates a dual-agent prompting architecture in which a Builder model generates candidate outputs and a Reviewer model provides structured critique to improve reliability. I conducted an exploratory survey study with 52 software practitioners who applied the D3 workflow to real-world engineering tasks such as legacy system exploration, documentation reconstruction, and architectural refactoring. Respondents reported perceived improvements in task clarity, documentation quality, and cognitive load, along with self-estimated productivity gains. In this exploratory study, participants reported a weighted average productivity improvement of 26.9%, reduced cognitive load for approximately 77% of participants, and reduced rework for 83% during the Define phase. As these findings are self-reported and not derived from controlled experiments, they should be interpreted as preliminary evidence of practitioner sentiment rather than causal effects. The results highlight both the potential and limitations of structured LLM workflows for legacy engineering systems and motivate future controlled evaluations.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aDiscover-Define-Deliver\uff08D3\uff09\u7684\u7ed3\u6784\u5316\u5927\u8bed\u8a00\u6a21\u578b\u8f85\u52a9\u5de5\u4f5c\u6d41\uff0c\u7528\u4e8e\u5e94\u5bf9\u9057\u7559\u7cfb\u7edf\u5de5\u7a0b\u4e2d\u7684\u6a21\u7cca\u6027\u548c\u77e5\u8bc6\u788e\u7247\u5316\u95ee\u9898\uff0c\u5e76\u901a\u8fc7\u4e00\u9879\u5305\u542b52\u540d\u4ece\u4e1a\u8005\u7684\u63a2\u7d22\u6027\u8c03\u7814\u9a8c\u8bc1\u4e86\u5176\u5728\u63d0\u5347\u4efb\u52a1\u6e05\u6670\u5ea6\u3001\u6587\u6863\u8d28\u91cf\u548c\u964d\u4f4e\u8ba4\u77e5\u8d1f\u8377\u65b9\u9762\u7684\u521d\u6b65\u6210\u6548\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u591a\u805a\u7126\u4e8e\u5168\u65b0\u5f00\u53d1\u6216\u5408\u6210\u4efb\u52a1\uff0c\u7f3a\u4e4f\u9488\u5bf9\u5305\u542b\u9057\u7559\u7cfb\u7edf\u3001\u6587\u6863\u4e0d\u5168\u548c\u67b6\u6784\u77e5\u8bc6\u788e\u7247\u5316\u7b49\u590d\u6742\u60c5\u5883\u4e0b\u7684\u7ed3\u6784\u5316\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5e94\u7528\u65b9\u6cd5\uff0c\u56e0\u6b64\u4e9f\u9700\u4e00\u79cd\u9002\u7528\u4e8e\u201c\u68d5\u5730\u201d\u5de5\u7a0b\u573a\u666f\u7684\u53ef\u9760\u5de5\u4f5c\u6d41\u7a0b\u3002", "method": "\u4f5c\u8005\u8bbe\u8ba1\u4e86D3\u6846\u67b6\uff0c\u91c7\u7528\u89d2\u8272\u5206\u79bb\u7684\u63d0\u793a\u7b56\u7565\uff0c\u5305\u62ec\u4e00\u4e2a\u751f\u6210\u5019\u9009\u8f93\u51fa\u7684Builder\u6a21\u578b\u548c\u4e00\u4e2a\u63d0\u4f9b\u7ed3\u6784\u5316\u53cd\u9988\u7684Reviewer\u6a21\u578b\uff0c\u5e76\u5728\u771f\u5b9e\u5de5\u7a0b\u4efb\u52a1\u4e2d\u5bf952\u540d\u8f6f\u4ef6\u4ece\u4e1a\u8005\u8fdb\u884c\u4e86\u63a2\u7d22\u6027\u8c03\u7814\u3002", "result": "\u53c2\u4e0e\u8005\u62a5\u544a\u5728\u4efb\u52a1\u6e05\u6670\u5ea6\u3001\u6587\u6863\u8d28\u91cf\u3001\u8ba4\u77e5\u8d1f\u8377\u548c\u8fd4\u5de5\u7387\u65b9\u9762\u5747\u6709\u6539\u5584\uff0c\u81ea\u8bc4\u5e73\u5747\u751f\u4ea7\u529b\u63d0\u534726.9%\uff0c\u7ea677%\u7684\u4eba\u8ba4\u77e5\u8d1f\u8377\u964d\u4f4e\uff0c83%\u5728Define\u9636\u6bb5\u8fd4\u5de5\u51cf\u5c11\u3002", "conclusion": "D3\u6846\u67b6\u5728\u68d5\u5730\u5de5\u7a0b\u4e2d\u5c55\u73b0\u51fa\u6f5c\u529b\uff0c\u4f46\u5f53\u524d\u7ed3\u679c\u57fa\u4e8e\u81ea\u8bc4\u6570\u636e\uff0c\u5c1a\u4e0d\u80fd\u786e\u7acb\u56e0\u679c\u5173\u7cfb\uff0c\u672a\u6765\u9700\u901a\u8fc7\u53d7\u63a7\u5b9e\u9a8c\u8fdb\u4e00\u6b65\u9a8c\u8bc1\u5176\u6709\u6548\u6027\u3002"}}
{"id": "2512.00112", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2512.00112", "abs": "https://arxiv.org/abs/2512.00112", "authors": ["Elham Cheshmikhani", "Hamed Farbeh"], "title": "An Analytical and Empirical Investigation of Tag Partitioning for Energy-Efficient Reliable Cache", "comment": null, "summary": "Associative cache memory significantly influences processor performance and energy consumption. Because it occupies over half of the chip area, cache memory is highly susceptible to transient and permanent faults, posing reliability challenges. As the only hardware-managed memory module, the cache tag array is the most active and critical component, dominating both energy usage and error rate. Tag partitioning is a widely used technique to reduce tag-access energy and enhance reliability. It divides tag comparison into two phases: first comparing the k lower bits, and then activating only the matching tag entries to compare the remaining higher bits. The key design parameter is the selection of the tag-splitting point k, which determines how many reads are eliminated. However, prior studies have chosen k intuitively, randomly, or empirically, without justification. Even experimentally determined values are ad-hoc and do not generalize across cache configurations due to high sensitivity to architectural parameters.\n  In this paper, we analytically show that choosing k too large or too small substantially reduces the effectiveness of tag partitioning. We then derive a formulation that determines the optimal splitting point based on cache configuration parameters. The formulation is convex, differentiable, and capable of precisely quantifying tag-partitioning efficiency for any k and configuration. To validate our model, we experimentally evaluate tag-partitioning efficiency and optimal k across a broad set of cache designs and demonstrate close agreement between analytical and experimental results. The proposed formulation enables designers and researchers to instantly compute the optimal tag-splitting point and accurately estimate tag-read reduction.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5206\u6790\u65b9\u6cd5\uff0c\u7528\u4e8e\u786e\u5b9a\u7f13\u5b58\u6807\u7b7e\u5206\u533a\u4e2d\u7684\u6700\u4f18\u5206\u5272\u70b9k\uff0c\u4ee5\u6700\u5927\u5316\u964d\u4f4e\u6807\u7b7e\u8bfb\u53d6\u80fd\u8017\u5e76\u63d0\u5347\u53ef\u9760\u6027\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\u7684\u51c6\u786e\u6027\u3002", "motivation": "\u7531\u4e8e\u7f13\u5b58\u5360\u636e\u82af\u7247\u9762\u79ef\u7684\u4e00\u534a\u4ee5\u4e0a\u4e14\u6613\u53d7\u6545\u969c\u5f71\u54cd\uff0c\u800c\u6807\u7b7e\u9635\u5217\u4f5c\u4e3a\u6700\u6d3b\u8dc3\u4e14\u5173\u952e\u7684\u90e8\u5206\uff0c\u5176\u80fd\u8017\u548c\u9519\u8bef\u7387\u5c24\u4e3a\u7a81\u51fa\u3002\u73b0\u6709\u6807\u7b7e\u5206\u533a\u65b9\u6cd5\u5728\u9009\u62e9\u5206\u5272\u70b9k\u65f6\u7f3a\u4e4f\u7406\u8bba\u4f9d\u636e\uff0c\u5bfc\u81f4\u6548\u679c\u4e0d\u7a33\u5b9a\u4e14\u96be\u4ee5\u6cdb\u5316\u3002", "method": "\u4f5c\u8005\u901a\u8fc7\u5206\u6790\u63a8\u5bfc\u51fa\u4e00\u4e2a\u57fa\u4e8e\u7f13\u5b58\u914d\u7f6e\u53c2\u6570\u7684\u51f8\u4e14\u53ef\u5fae\u7684\u516c\u5f0f\uff0c\u7528\u4e8e\u7cbe\u786e\u91cf\u5316\u4efb\u610fk\u503c\u4e0b\u7684\u6807\u7b7e\u5206\u533a\u6548\u7387\uff0c\u5e76\u636e\u6b64\u786e\u5b9a\u6700\u4f18\u5206\u5272\u70b9k\u3002", "result": "\u5728\u591a\u79cd\u7f13\u5b58\u8bbe\u8ba1\u4e0a\u8fdb\u884c\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684\u5206\u6790\u6a21\u578b\u4e0e\u5b9e\u9645\u7ed3\u679c\u9ad8\u5ea6\u4e00\u81f4\uff0c\u80fd\u591f\u51c6\u786e\u9884\u6d4b\u6700\u4f18k\u503c\u53ca\u6807\u7b7e\u8bfb\u53d6\u51cf\u5c11\u91cf\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u7f13\u5b58\u6807\u7b7e\u5206\u533a\u63d0\u4f9b\u4e86\u7406\u8bba\u57fa\u7840\uff0c\u4f7f\u8bbe\u8ba1\u8005\u80fd\u5feb\u901f\u8ba1\u7b97\u6700\u4f18\u5206\u5272\u70b9\u5e76\u51c6\u786e\u8bc4\u4f30\u8282\u80fd\u6548\u679c\uff0c\u89e3\u51b3\u4e86\u4ee5\u5f80\u65b9\u6cd5\u7f3a\u4e4f\u901a\u7528\u6027\u548c\u7406\u8bba\u652f\u6491\u7684\u95ee\u9898\u3002"}}
{"id": "2512.01356", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2512.01356", "abs": "https://arxiv.org/abs/2512.01356", "authors": ["Yuxin Zhang", "Yuxia Zhang", "Zeyu Sun", "Yanjie Jiang", "Hui Liu"], "title": "LAURA: Enhancing Code Review Generation with Context-Enriched Retrieval-Augmented LLM", "comment": "Accepted by the 2025 40th IEEE/ACM International Conference on Automated Software Engineering (ASE). Copyright 2025 IEEE. This is the author's accepted manuscript. The final published version may differ and will be available from IEEE Xplore", "summary": "Code review is critical for ensuring software quality and maintainability. With the rapid growth in software scale and complexity, code review has become a bottleneck in the development process because of its time-consuming and knowledge-intensive nature and the shortage of experienced developers willing to review code. Several approaches have been proposed for automatically generating code reviews based on retrieval, neural machine translation, pre-trained models, or large language models (LLMs). These approaches mainly leverage historical code changes and review comments. However, a large amount of crucial information for code review, such as the context of code changes and prior review knowledge, has been overlooked. This paper proposes an LLM-based review knowledge-augmented, context-aware framework for code review generation, named LAURA. The framework integrates review exemplar retrieval, context augmentation, and systematic guidance to enhance the performance of ChatGPT-4o and DeepSeek v3 in generating code review comments. Besides, given the extensive low-quality reviews in existing datasets, we also constructed a high-quality dataset. Experimental results show that for both models, LAURA generates review comments that are either completely correct or at least helpful to developers in 42.2% and 40.4% of cases, respectively, significantly outperforming SOTA baselines. Furthermore, our ablation studies demonstrate that all components of LAURA contribute positively to improving comment quality.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u4ee3\u7801\u5ba1\u67e5\u751f\u6210\u6846\u67b6LAURA\uff0c\u901a\u8fc7\u5f15\u5165\u5ba1\u67e5\u793a\u4f8b\u68c0\u7d22\u3001\u4e0a\u4e0b\u6587\u589e\u5f3a\u548c\u7cfb\u7edf\u6027\u5f15\u5bfc\uff0c\u663e\u8457\u63d0\u5347\u4e86ChatGPT-4o\u548cDeepSeek v3\u751f\u6210\u9ad8\u8d28\u91cf\u4ee3\u7801\u5ba1\u67e5\u8bc4\u8bba\u7684\u80fd\u529b\uff0c\u5e76\u6784\u5efa\u4e86\u4e00\u4e2a\u9ad8\u8d28\u91cf\u6570\u636e\u96c6\u4ee5\u89e3\u51b3\u73b0\u6709\u6570\u636e\u4e2d\u4f4e\u8d28\u91cf\u8bc4\u8bba\u7684\u95ee\u9898\u3002", "motivation": "\u4ee3\u7801\u5ba1\u67e5\u5bf9\u4fdd\u969c\u8f6f\u4ef6\u8d28\u91cf\u548c\u53ef\u7ef4\u62a4\u6027\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u5176\u8017\u65f6\u3001\u4f9d\u8d56\u4e13\u4e1a\u77e5\u8bc6\u4e14\u7f3a\u4e4f\u7ecf\u9a8c\u4e30\u5bcc\u7684\u5ba1\u67e5\u8005\uff0c\u5df2\u6210\u4e3a\u5f00\u53d1\u6d41\u7a0b\u4e2d\u7684\u74f6\u9888\u3002\u73b0\u6709\u81ea\u52a8\u4ee3\u7801\u5ba1\u67e5\u65b9\u6cd5\u5ffd\u7565\u4e86\u4ee3\u7801\u53d8\u66f4\u4e0a\u4e0b\u6587\u548c\u5386\u53f2\u5ba1\u67e5\u77e5\u8bc6\u7b49\u5173\u952e\u4fe1\u606f\u3002", "method": "\u63d0\u51faLAURA\u6846\u67b6\uff0c\u7ed3\u5408\u5ba1\u67e5\u793a\u4f8b\u68c0\u7d22\u3001\u4e0a\u4e0b\u6587\u589e\u5f3a\u548c\u7cfb\u7edf\u6027\u5f15\u5bfc\uff0c\u589e\u5f3a\u5927\u8bed\u8a00\u6a21\u578b\uff08ChatGPT-4o\u548cDeepSeek v3\uff09\u751f\u6210\u4ee3\u7801\u5ba1\u67e5\u8bc4\u8bba\u7684\u80fd\u529b\uff0c\u5e76\u6784\u5efa\u9ad8\u8d28\u91cf\u6570\u636e\u96c6\u7528\u4e8e\u8bad\u7ec3\u4e0e\u8bc4\u4f30\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cLAURA\u5728\u4e24\u4e2a\u6a21\u578b\u4e0a\u5206\u522b\u670942.2%\u548c40.4%\u7684\u751f\u6210\u8bc4\u8bba\u5b8c\u5168\u6b63\u786e\u6216\u5bf9\u5f00\u53d1\u8005\u6709\u5e2e\u52a9\uff0c\u663e\u8457\u4f18\u4e8e\u5f53\u524d\u6700\u5148\u8fdb\u7684\u57fa\u7ebf\u65b9\u6cd5\uff1b\u6d88\u878d\u7814\u7a76\u4e5f\u9a8c\u8bc1\u4e86\u5404\u7ec4\u4ef6\u5bf9\u6027\u80fd\u63d0\u5347\u7684\u79ef\u6781\u8d21\u732e\u3002", "conclusion": "LAURA\u901a\u8fc7\u878d\u5408\u5ba1\u67e5\u77e5\u8bc6\u4e0e\u4e0a\u4e0b\u6587\u4fe1\u606f\uff0c\u6709\u6548\u63d0\u5347\u4e86\u81ea\u52a8\u751f\u6210\u4ee3\u7801\u5ba1\u67e5\u7684\u8d28\u91cf\uff0c\u4e3a\u7f13\u89e3\u4eba\u5de5\u5ba1\u67e5\u74f6\u9888\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848\u3002"}}
{"id": "2512.01396", "categories": ["cs.SE", "cs.CL", "cs.CR"], "pdf": "https://arxiv.org/pdf/2512.01396", "abs": "https://arxiv.org/abs/2512.01396", "authors": ["Zhiqing Zhong", "Jiaming Huang", "Pinjia He"], "title": "BackportBench: A Multilingual Benchmark for Automated Backporting of Patches", "comment": "Under review", "summary": "Many modern software projects evolve rapidly to incorporate new features and security patches. It is important for users to update their dependencies to safer versions, but many still use older, vulnerable package versions because upgrading can be difficult and may break their existing codebase. Software developers can mitigate this problem by backporting security patches to older releases. However, manually backporting is time-consuming and error-prone. The effectiveness of existing automated backporting techniques on general software remains unclear since they typically target only code-hunk or function-level patch porting scenarios and are evaluated with imperfect metrics.\n  To facilitate the development and evaluation of automated backporting techniques, we introduce BackportBench, the first comprehensive benchmark suite for patch backporting problem. BackportBench is a multilingual benchmark that contains 202 patch backporting problems from PyPI, Maven, and npm, each with executable Docker environments and relevant test cases. We evaluated existing patch porting methods and LLM-based techniques that have the potential to adapt to this task using BackportBench. The results show that the agentic method has outperformed traditional patch porting methods, especially on cases that require logical and structural changes. However, the performance varies across different programming languages. Based on the findings, we draw several implications for researchers and software practitioners in future work on automated backporting.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86 BackportBench\uff0c\u8fd9\u662f\u9996\u4e2a\u9488\u5bf9\u8865\u4e01\u56de\u8fc1\uff08patch backporting\uff09\u4efb\u52a1\u7684\u7efc\u5408\u6027\u591a\u8bed\u8a00\u57fa\u51c6\u6d4b\u8bd5\u5957\u4ef6\uff0c\u5305\u542b202\u4e2a\u6765\u81ea PyPI\u3001Maven \u548c npm \u7684\u56de\u8fc1\u95ee\u9898\uff0c\u5e76\u914d\u6709\u53ef\u6267\u884c\u7684 Docker \u73af\u5883\u548c\u6d4b\u8bd5\u7528\u4f8b\u3002\u7814\u7a76\u8bc4\u4f30\u4e86\u73b0\u6709\u8865\u4e01\u8fc1\u79fb\u65b9\u6cd5\u548c\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u6280\u672f\uff0c\u53d1\u73b0\u57fa\u4e8e\u667a\u80fd\u4f53\uff08agentic\uff09\u7684\u65b9\u6cd5\u5728\u9700\u8981\u903b\u8f91\u548c\u7ed3\u6784\u53d8\u66f4\u7684\u6848\u4f8b\u4e2d\u8868\u73b0\u66f4\u4f18\uff0c\u4f46\u6027\u80fd\u5728\u4e0d\u540c\u7f16\u7a0b\u8bed\u8a00\u95f4\u5b58\u5728\u5dee\u5f02\u3002", "motivation": "\u73b0\u4ee3\u8f6f\u4ef6\u9879\u76ee\u9891\u7e41\u66f4\u65b0\u4ee5\u5f15\u5165\u65b0\u529f\u80fd\u548c\u5b89\u5168\u8865\u4e01\uff0c\u4f46\u7528\u6237\u5e38\u56e0\u5347\u7ea7\u56f0\u96be\u800c\u7ee7\u7eed\u4f7f\u7528\u5b58\u5728\u6f0f\u6d1e\u7684\u65e7\u7248\u672c\u3002\u624b\u52a8\u56de\u8fc1\u5b89\u5168\u8865\u4e01\u8017\u65f6\u4e14\u6613\u9519\uff0c\u800c\u73b0\u6709\u81ea\u52a8\u5316\u56de\u8fc1\u6280\u672f\u5927\u591a\u5c40\u9650\u4e8e\u4ee3\u7801\u7247\u6bb5\u6216\u51fd\u6570\u7ea7\u522b\uff0c\u4e14\u8bc4\u4f30\u6307\u6807\u4e0d\u5b8c\u5584\uff0c\u56e0\u6b64\u4e9f\u9700\u4e00\u4e2a\u5168\u9762\u7684\u57fa\u51c6\u6765\u63a8\u52a8\u548c\u8bc4\u4f30\u81ea\u52a8\u5316\u56de\u8fc1\u65b9\u6cd5\u7684\u53d1\u5c55\u3002", "method": "\u4f5c\u8005\u6784\u5efa\u4e86\u540d\u4e3a BackportBench \u7684\u591a\u8bed\u8a00\u57fa\u51c6\u6d4b\u8bd5\u5957\u4ef6\uff0c\u6db5\u76d6202\u4e2a\u771f\u5b9e\u4e16\u754c\u7684\u8865\u4e01\u56de\u8fc1\u95ee\u9898\uff0c\u6bcf\u4e2a\u95ee\u9898\u5747\u914d\u5907 Docker \u6267\u884c\u73af\u5883\u548c\u6d4b\u8bd5\u7528\u4f8b\u3002\u5728\u6b64\u57fa\u7840\u4e0a\uff0c\u8bc4\u4f30\u4e86\u4f20\u7edf\u8865\u4e01\u8fc1\u79fb\u65b9\u6cd5\u4e0e\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u65b0\u578b\u6280\u672f\uff08\u5c24\u5176\u662f agentic \u65b9\u6cd5\uff09\u5728\u8be5\u4efb\u52a1\u4e0a\u7684\u8868\u73b0\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cagentic \u65b9\u6cd5\u5728\u5904\u7406\u9700\u8981\u903b\u8f91\u548c\u7ed3\u6784\u6027\u4fee\u6539\u7684\u56de\u8fc1\u4efb\u52a1\u4e0a\u663e\u8457\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\uff0c\u4f46\u5176\u5728\u4e0d\u540c\u7f16\u7a0b\u8bed\u8a00\u4e2d\u7684\u8868\u73b0\u5b58\u5728\u5dee\u5f02\u3002", "conclusion": "BackportBench \u4e3a\u81ea\u52a8\u5316\u8865\u4e01\u56de\u8fc1\u7814\u7a76\u63d0\u4f9b\u4e86\u53ef\u9760\u8bc4\u4f30\u5e73\u53f0\uff1bagentic \u65b9\u6cd5\u5c55\u73b0\u51fa\u6f5c\u529b\uff0c\u4f46\u4ecd\u9700\u9488\u5bf9\u591a\u8bed\u8a00\u573a\u666f\u8fdb\u884c\u4f18\u5316\u3002\u7814\u7a76\u4e3a\u672a\u6765\u81ea\u52a8\u5316\u56de\u8fc1\u5de5\u5177\u7684\u5f00\u53d1\u548c\u8bc4\u4f30\u63d0\u4f9b\u4e86\u91cd\u8981\u542f\u793a\u3002"}}
{"id": "2512.00186", "categories": ["cs.AR", "eess.SP"], "pdf": "https://arxiv.org/pdf/2512.00186", "abs": "https://arxiv.org/abs/2512.00186", "authors": ["Seyed Hadi Mirfarshbafan", "Nicolas Filliol", "Oscar Casta\u00f1eda", "Christoph Studer"], "title": "Variable Point: A Number Format for Area- and Energy-Efficient Multiplication of High-Dynamic-Range Numbers", "comment": "Presented at the 59th Asilomar Conference on Signals, Systems, and Computers", "summary": "Fixed-point number representation is commonly employed in digital VLSI designs that have stringent hardware efficiency constraints. However, fixed-point numbers cover a relatively small dynamic range for a given bitwidth. In contrast, floating-point numbers offer a larger dynamic range at the cost of increased hardware complexity. In this paper, we propose a novel number format called variable-point (VP). VP numbers cover a larger dynamic range than fixed-point numbers with similar bitwidth, without notably increasing hardware complexity -- this allows for a more efficient representation of signals with high dynamic range. To demonstrate the efficacy of the proposed VP number format, we consider a matrix-vector multiplication engine for spatial equalization in multi-antenna wireless communication systems involving high-dynamic-range signals. Through post-layout VLSI implementation results, we demonstrate that the proposed VP-based design achieves 20% and 10% area and power savings, respectively, compared to a fully optimized fixed-point design, without incurring any noticeable performance degradation.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u53ef\u53d8\u70b9\uff08VP\uff09\u7684\u65b0\u6570\u503c\u683c\u5f0f\uff0c\u5728\u4fdd\u6301\u786c\u4ef6\u590d\u6742\u5ea6\u4e0d\u663e\u8457\u589e\u52a0\u7684\u524d\u63d0\u4e0b\uff0c\u76f8\u6bd4\u5b9a\u70b9\u6570\u5177\u6709\u66f4\u5927\u7684\u52a8\u6001\u8303\u56f4\uff0c\u5e76\u5728\u591a\u5929\u7ebf\u65e0\u7ebf\u901a\u4fe1\u7cfb\u7edf\u7684\u77e9\u9635-\u5411\u91cf\u4e58\u6cd5\u5f15\u64ce\u4e2d\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\uff0c\u5b9e\u73b0\u4e86\u9762\u79ef\u548c\u529f\u8017\u5206\u522b\u8282\u770120%\u548c10%\u3002", "motivation": "\u5b9a\u70b9\u6570\u5728\u7ed9\u5b9a\u4f4d\u5bbd\u4e0b\u52a8\u6001\u8303\u56f4\u8f83\u5c0f\uff0c\u800c\u6d6e\u70b9\u6570\u867d\u52a8\u6001\u8303\u56f4\u5927\u4f46\u786c\u4ef6\u590d\u6742\u5ea6\u9ad8\u3002\u4e3a\u5728\u4e0d\u663e\u8457\u589e\u52a0\u786c\u4ef6\u590d\u6742\u5ea6\u7684\u60c5\u51b5\u4e0b\u63d0\u5347\u52a8\u6001\u8303\u56f4\uff0c\u4f5c\u8005\u63d0\u51fa\u4e86\u53ef\u53d8\u70b9\uff08VP\uff09\u6570\u503c\u683c\u5f0f\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u65b0\u7684\u53ef\u53d8\u70b9\uff08VP\uff09\u6570\u503c\u8868\u793a\u65b9\u6cd5\uff0c\u5e76\u5c06\u5176\u5e94\u7528\u4e8e\u591a\u5929\u7ebf\u65e0\u7ebf\u901a\u4fe1\u7cfb\u7edf\u4e2d\u7528\u4e8e\u7a7a\u95f4\u5747\u8861\u7684\u77e9\u9635-\u5411\u91cf\u4e58\u6cd5\u5f15\u64ce\uff1b\u901a\u8fc7\u5e03\u5c40\u540eVLSI\u5b9e\u73b0\u8fdb\u884c\u8bc4\u4f30\u3002", "result": "\u4e0e\u5b8c\u5168\u4f18\u5316\u7684\u5b9a\u70b9\u8bbe\u8ba1\u76f8\u6bd4\uff0c\u57fa\u4e8eVP\u7684\u8bbe\u8ba1\u5728\u65e0\u660e\u663e\u6027\u80fd\u4e0b\u964d\u7684\u60c5\u51b5\u4e0b\uff0c\u5b9e\u73b0\u4e8620%\u7684\u9762\u79ef\u8282\u7701\u548c10%\u7684\u529f\u8017\u964d\u4f4e\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684VP\u6570\u503c\u683c\u5f0f\u5728\u5904\u7406\u9ad8\u52a8\u6001\u8303\u56f4\u4fe1\u53f7\u65f6\uff0c\u80fd\u591f\u5728\u51e0\u4e4e\u4e0d\u589e\u52a0\u786c\u4ef6\u590d\u6742\u5ea6\u7684\u524d\u63d0\u4e0b\uff0c\u6709\u6548\u63d0\u5347\u786c\u4ef6\u6548\u7387\uff0c\u4f18\u4e8e\u4f20\u7edf\u5b9a\u70b9\u5b9e\u73b0\u3002"}}
{"id": "2512.01523", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.01523", "abs": "https://arxiv.org/abs/2512.01523", "authors": ["Pankaj Jalote", "Y. Raghu Reddy", "Vasudeva Varma"], "title": "Teaching an Online Multi-Institutional Research Level Software Engineering Course with Industry - an Experience Report", "comment": "7 pages", "summary": "Covid has made online teaching and learning acceptable and students, faculty, and industry professionals are all comfortable with this mode. This comfort can be leveraged to offer an online multi-institutional research-level course in an area where individual institutions may not have the requisite faculty to teach and/or research students to enroll. If the subject is of interest to industry, online offering also allows industry experts to contribute and participate with ease. Advanced topics in Software Engineering are ideally suited for experimenting with this approach as industry, which is often looking to incorporate advances in software engineering in their practices, is likely to agree to contribute and participate. In this paper we describe an experiment in teaching a course titled \"AI in Software Engineering\" jointly between two institutions with active industry participation, and share our and student's experience. We believe this collaborative teaching approach can be used for offering research level courses in any applied area of computer science by institutions who are small and find it difficult to offer research level courses on their own.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u4e00\u79cd\u8de8\u6821\u8054\u5408\u5f00\u8bbe\u7814\u7a76\u578b\u5728\u7ebf\u8bfe\u7a0b\u201c\u8f6f\u4ef6\u5de5\u7a0b\u4e2d\u7684\u4eba\u5de5\u667a\u80fd\u201d\u7684\u5b9e\u9a8c\uff0c\u7ed3\u5408\u4ea7\u4e1a\u754c\u53c2\u4e0e\uff0c\u65e8\u5728\u4e3a\u8d44\u6e90\u6709\u9650\u7684\u5c0f\u578b\u9ad8\u6821\u63d0\u4f9b\u53ef\u884c\u7684\u7814\u7a76\u7ea7\u6559\u5b66\u6a21\u5f0f\u3002", "motivation": "\u65b0\u51a0\u75ab\u60c5\u4f7f\u5728\u7ebf\u6559\u5b66\u88ab\u5e7f\u6cdb\u63a5\u53d7\uff0c\u4e3a\u7f3a\u4e4f\u8db3\u591f\u5e08\u8d44\u6216\u7814\u7a76\u751f\u751f\u6e90\u7684\u9ad8\u6821\u63d0\u4f9b\u4e86\u901a\u8fc7\u591a\u673a\u6784\u5408\u4f5c\u5f00\u8bbe\u7814\u7a76\u7ea7\u8bfe\u7a0b\u7684\u673a\u4f1a\uff0c\u5c24\u5176\u9002\u7528\u4e8e\u8f6f\u4ef6\u5de5\u7a0b\u7b49\u4e0e\u4ea7\u4e1a\u7d27\u5bc6\u76f8\u5173\u7684\u9886\u57df\u3002", "method": "\u4e24\u6240\u9ad8\u6821\u8054\u5408\u5f00\u8bbe\u4e00\u95e8\u540d\u4e3a\u201cAI in Software Engineering\u201d\u7684\u5728\u7ebf\u7814\u7a76\u7ea7\u8bfe\u7a0b\uff0c\u5e76\u9080\u8bf7\u4ea7\u4e1a\u4e13\u5bb6\u79ef\u6781\u53c2\u4e0e\u6559\u5b66\u4e0e\u4e92\u52a8\u3002", "result": "\u8bfe\u7a0b\u6210\u529f\u5b9e\u65bd\uff0c\u5e08\u751f\u53ca\u4ea7\u4e1a\u53c2\u4e0e\u8005\u5747\u83b7\u5f97\u826f\u597d\u4f53\u9a8c\uff0c\u9a8c\u8bc1\u4e86\u8be5\u534f\u4f5c\u6559\u5b66\u6a21\u5f0f\u7684\u53ef\u884c\u6027\u3002", "conclusion": "\u8fd9\u79cd\u591a\u65b9\u534f\u4f5c\u7684\u5728\u7ebf\u6559\u5b66\u6a21\u5f0f\u53ef\u63a8\u5e7f\u81f3\u5176\u4ed6\u5e94\u7528\u8ba1\u7b97\u673a\u79d1\u5b66\u9886\u57df\uff0c\u5e2e\u52a9\u5c0f\u578b\u9ad8\u6821\u514b\u670d\u8d44\u6e90\u9650\u5236\uff0c\u6709\u6548\u5f00\u8bbe\u7814\u7a76\u7ea7\u8bfe\u7a0b\u3002"}}
{"id": "2512.00335", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2512.00335", "abs": "https://arxiv.org/abs/2512.00335", "authors": ["Takuto Ando", "Yu Eto", "Ayumu Takeuchi", "Yasuhiko Nakashima"], "title": "Efficient Kernel Mapping and Comprehensive System Evaluation of LLM Acceleration on a CGLA", "comment": "This paper is published at IEEE Access", "summary": "Large Language Models (LLMs) demand substantial computational resources, resulting in high energy consumption on GPUs. To address this challenge, we focus on Coarse-Grained Reconfigurable Arrays (CGRAs) as an effective alternative that provides a trade-off between energy efficiency and programmability. This paper presents the first comprehensive, end-to-end evaluation of a non-AI-specialized Coarse-Grained Linear Array (CGLA) accelerator for the state-of-the-art Qwen LLM family. The architecture has a general-purpose, task-agnostic design, yet its flexible instruction set allows for domain-specific adaptations. This flexibility enables the architecture to achieve high efficiency for sustainable LLM inference. We assess the performance of our architecture on an FPGA prototype using the widely adopted llama.cpp framework. We then project its potential as a 28nm ASIC and compare it against a high-performance GPU (NVIDIA RTX 4090) and an edge AI device (NVIDIA Jetson AGX Orin). While GPUs exhibit lower latency, our non-AI-specific accelerator achieves higher energy efficiency, improving the Power-Delay Product (PDP) by up to 44.4x and 13.6x compared with the RTX 4090 and Jetson, respectively. Similarly, it reduces the Energy-Delay Product (EDP) by up to 11.5x compared to the high-performance GPU, demonstrating a favorable performance-energy trade-off. Critically, our system-level analysis identifies host-accelerator data transfer as the primary performance bottleneck, a factor often overlooked in kernel-level studies. These findings provide design guidance for next-generation LLM accelerators. This work validates CGRAs as a suitable platform for LLM inference in power-constrained environments, without being confined to specific algorithms.", "AI": {"tldr": "\u672c\u6587\u9996\u6b21\u5bf9\u975eAI\u4e13\u7528\u7684\u7c97\u7c92\u5ea6\u7ebf\u6027\u9635\u5217\uff08CGLA\uff09\u52a0\u901f\u5668\u5728Qwen\u5927\u8bed\u8a00\u6a21\u578b\u4e0a\u7684\u7aef\u5230\u7aef\u6027\u80fd\u8fdb\u884c\u4e86\u5168\u9762\u8bc4\u4f30\uff0c\u7ed3\u679c\u8868\u660e\u5176\u5728\u80fd\u6548\u65b9\u9762\u663e\u8457\u4f18\u4e8e\u9ad8\u7aefGPU\u548c\u8fb9\u7f18AI\u8bbe\u5907\uff0c\u5c24\u5176\u5728\u529f\u8017\u5ef6\u8fdf\u79ef\uff08PDP\uff09\u548c\u80fd\u91cf\u5ef6\u8fdf\u79ef\uff08EDP\uff09\u6307\u6807\u4e0a\u8868\u73b0\u7a81\u51fa\u3002", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728GPU\u4e0a\u8fd0\u884c\u65f6\u80fd\u8017\u9ad8\uff0c\u4e9f\u9700\u66f4\u8282\u80fd\u7684\u786c\u4ef6\u66ff\u4ee3\u65b9\u6848\u3002\u4f5c\u8005\u65e8\u5728\u63a2\u7d22\u7c97\u7c92\u5ea6\u53ef\u91cd\u6784\u9635\u5217\uff08CGRAs\uff09\u4f5c\u4e3a\u517c\u987e\u80fd\u6548\u4e0e\u53ef\u7f16\u7a0b\u6027\u7684\u53ef\u884c\u5e73\u53f0\uff0c\u7528\u4e8e\u53ef\u6301\u7eed\u7684LLM\u63a8\u7406\u3002", "method": "\u4f5c\u8005\u57fa\u4e8e\u901a\u7528\u3001\u4efb\u52a1\u65e0\u5173\u7684CGLA\u67b6\u6784\uff0c\u5728FPGA\u539f\u578b\u4e0a\u4f7f\u7528llama.cpp\u6846\u67b6\u8fdb\u884c\u8bc4\u4f30\uff0c\u5e76\u5c06\u5176\u6027\u80fd\u6295\u5f71\u81f328nm ASIC\u5de5\u827a\uff0c\u4e0eNVIDIA RTX 4090\u548cJetson AGX Orin\u8fdb\u884c\u5bf9\u6bd4\u5206\u6790\uff0c\u540c\u65f6\u5f00\u5c55\u7cfb\u7edf\u7ea7\u74f6\u9888\u8bc6\u522b\u3002", "result": "\u5c3d\u7ba1GPU\u5ef6\u8fdf\u66f4\u4f4e\uff0c\u4f46\u8be5\u975eAI\u4e13\u7528\u52a0\u901f\u5668\u5728\u80fd\u6548\u65b9\u9762\u663e\u8457\u9886\u5148\uff1a\u76f8\u6bd4RTX 4090\u548cJetson\uff0cPDP\u5206\u522b\u63d0\u5347\u6700\u591a44.4\u500d\u548c13.6\u500d\uff1b\u76f8\u6bd4\u9ad8\u7aefGPU\uff0cEDP\u964d\u4f4e\u6700\u591a11.5\u500d\u3002\u7814\u7a76\u8fd8\u53d1\u73b0\u4e3b\u673a-\u52a0\u901f\u5668\u95f4\u7684\u6570\u636e\u4f20\u8f93\u662f\u4e3b\u8981\u6027\u80fd\u74f6\u9888\u3002", "conclusion": "CGRAs\u662f\u4e00\u79cd\u9002\u7528\u4e8e\u529f\u7387\u53d7\u9650\u73af\u5883\u4e0bLLM\u63a8\u7406\u7684\u6709\u6548\u5e73\u53f0\uff0c\u65e0\u9700\u9488\u5bf9\u7279\u5b9a\u7b97\u6cd5\u5b9a\u5236\u5373\u53ef\u5b9e\u73b0\u4f18\u5f02\u7684\u6027\u80fd-\u80fd\u6548\u5e73\u8861\uff0c\u4e3a\u4e0b\u4e00\u4ee3LLM\u52a0\u901f\u5668\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u91cd\u8981\u6307\u5bfc\u3002"}}
{"id": "2512.01570", "categories": ["cs.SE", "cs.DL"], "pdf": "https://arxiv.org/pdf/2512.01570", "abs": "https://arxiv.org/abs/2512.01570", "authors": ["Stephan Druskat", "Lars Grunske"], "title": "OpenDORS: A dataset of openly referenced open research software", "comment": "5 pages, 3 figures, 1 table", "summary": "In many academic disciplines, software is created during the research process or for a research purpose. The crucial role of software for research is increasingly acknowledged. The application of software engineering to research software has been formalized as research software engineering, to create better software that enables better research. Despite this, large-scale studies of research software and its development are still lacking. To enable such studies, we present a dataset of 134,352 unique open research software projects and 134,154 source code repositories referenced in open access literature. Each dataset record identifies the referencing publication and lists source code repositories of the software project. For 122,425 source code repositories, the dataset provides metadata on latest versions, license information, programming languages and descriptive metadata files. We summarize the distributions of these features in the dataset and describe additional software metadata that extends the dataset in future work. Finally, we suggest examples of research that could use the dataset to develop a better understanding of research software practice in RSE research.", "AI": {"tldr": "\u672c\u6587\u53d1\u5e03\u4e86\u4e00\u4e2a\u5305\u542b134,352\u4e2a\u5f00\u6e90\u79d1\u7814\u8f6f\u4ef6\u9879\u76ee\u53ca\u5176\u5728\u5f00\u653e\u83b7\u53d6\u6587\u732e\u4e2d\u5f15\u7528\u7684134,154\u4e2a\u4ee3\u7801\u4ed3\u5e93\u7684\u5927\u89c4\u6a21\u6570\u636e\u96c6\uff0c\u63d0\u4f9b\u7248\u672c\u3001\u8bb8\u53ef\u8bc1\u3001\u7f16\u7a0b\u8bed\u8a00\u7b49\u5143\u6570\u636e\uff0c\u65e8\u5728\u652f\u6301\u79d1\u7814\u8f6f\u4ef6\u5de5\u7a0b\uff08RSE\uff09\u9886\u57df\u7684\u5b9e\u8bc1\u7814\u7a76\u3002", "motivation": "\u5c3d\u7ba1\u79d1\u7814\u8f6f\u4ef6\u5728\u5b66\u672f\u7814\u7a76\u4e2d\u626e\u6f14\u5173\u952e\u89d2\u8272\u4e14\u79d1\u7814\u8f6f\u4ef6\u5de5\u7a0b\uff08RSE\uff09\u5df2\u6210\u5f62\uff0c\u4f46\u9488\u5bf9\u79d1\u7814\u8f6f\u4ef6\u53ca\u5176\u5f00\u53d1\u7684\u5927\u89c4\u6a21\u5b9e\u8bc1\u7814\u7a76\u4ecd\u663e\u4e0d\u8db3\uff0c\u4e3b\u8981\u53d7\u9650\u4e8e\u7f3a\u4e4f\u7cfb\u7edf\u6027\u6570\u636e\u96c6\u3002", "method": "\u4f5c\u8005\u6784\u5efa\u5e76\u53d1\u5e03\u4e86\u4e00\u4e2a\u5927\u89c4\u6a21\u6570\u636e\u96c6\uff0c\u8be5\u6570\u636e\u96c6\u4ece\u5f00\u653e\u83b7\u53d6\u6587\u732e\u4e2d\u63d0\u53d6\u4e86\u79d1\u7814\u8f6f\u4ef6\u9879\u76ee\u53ca\u5176\u5173\u8054\u7684\u6e90\u4ee3\u7801\u4ed3\u5e93\uff0c\u5e76\u4e3a\u5176\u4e2d122,425\u4e2a\u4ed3\u5e93\u6536\u96c6\u4e86\u5305\u62ec\u6700\u65b0\u7248\u672c\u3001\u8bb8\u53ef\u8bc1\u3001\u7f16\u7a0b\u8bed\u8a00\u548c\u63cf\u8ff0\u6027\u5143\u6570\u636e\u6587\u4ef6\u5728\u5185\u7684\u8be6\u7ec6\u4fe1\u606f\u3002", "result": "\u6570\u636e\u96c6\u6210\u529f\u6c47\u603b\u4e86\u79d1\u7814\u8f6f\u4ef6\u5728\u7248\u672c\u3001\u8bb8\u53ef\u8bc1\u3001\u7f16\u7a0b\u8bed\u8a00\u7b49\u5173\u952e\u7279\u5f81\u4e0a\u7684\u5206\u5e03\u60c5\u51b5\uff0c\u5e76\u4e3a\u672a\u6765\u6269\u5c55\u66f4\u591a\u8f6f\u4ef6\u5143\u6570\u636e\u5960\u5b9a\u4e86\u57fa\u7840\u3002", "conclusion": "\u8be5\u6570\u636e\u96c6\u4e3a\u79d1\u7814\u8f6f\u4ef6\u5de5\u7a0b\u9886\u57df\u63d0\u4f9b\u4e86\u5b9d\u8d35\u8d44\u6e90\uff0c\u53ef\u652f\u6301\u591a\u79cd\u7814\u7a76\u4ee5\u6df1\u5165\u7406\u89e3\u79d1\u7814\u8f6f\u4ef6\u7684\u5b9e\u8df5\u73b0\u72b6\uff0c\u4ece\u800c\u63a8\u52a8RSE\u7684\u53d1\u5c55\u3002"}}
{"id": "2512.00441", "categories": ["cs.AR", "cs.ET"], "pdf": "https://arxiv.org/pdf/2512.00441", "abs": "https://arxiv.org/abs/2512.00441", "authors": ["Amogh K M", "Sunita M S"], "title": "A Novel 8T SRAM-Based In-Memory Computing Architecture for MAC-Derived Logical Functions", "comment": "6 pages, 6 figures, Accepted at 39th VLSID 2026 conference", "summary": "This paper presents an in-memory computing (IMC) architecture developed on an 8x8 array of 8T SRAM cells. This architecture enables both multi-bit parallel Multiply-Accumulate (MAC) operations and standard memory processing through charge-sharing on dedicated read bit-lines. By leveraging the maturity of SRAM technology, this work introduces an 8T SRAM-based IMC architecture that decouples read and write paths, thereby overcoming the reliability limitations of prior 6T SRAM designs. A novel analog-to-digital decoding scheme converts the MAC voltage output into digital counts, which are subsequently interpreted to realize fundamental logic functions including AND/NAND, NOR/OR, XOR/XNOR, and 1-bit addition within the same array. Simulated in a 90 nm CMOS process at 1.8 V supply voltage, the proposed design achieves 8-bit MAC and logical operations at a frequency of 142.85 MHz, with a latency of 0.7 ns and energy consumption of 56.56 fJ/bit per MAC operation and throughput of 15.8 M operations/s.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e8T SRAM\u7684\u5b58\u5185\u8ba1\u7b97\u67b6\u6784\uff0c\u652f\u6301\u591a\u6bd4\u7279\u5e76\u884c\u4e58\u7d2f\u52a0\uff08MAC\uff09\u548c\u903b\u8f91\u8fd0\u7b97\uff0c\u514b\u670d\u4e86\u4f20\u7edf6T SRAM\u8bbe\u8ba1\u7684\u53ef\u9760\u6027\u95ee\u9898\uff0c\u572890 nm\u5de5\u827a\u4e0b\u5b9e\u73b0\u4e86\u9ad8\u6027\u80fd\u4f4e\u529f\u8017\u64cd\u4f5c\u3002", "motivation": "\u4f20\u7edf6T SRAM\u5b58\u5185\u8ba1\u7b97\u67b6\u6784\u5b58\u5728\u8bfb\u5199\u8def\u5f84\u8026\u5408\u5bfc\u81f4\u7684\u53ef\u9760\u6027\u95ee\u9898\uff0c\u9650\u5236\u4e86\u5176\u5728\u9ad8\u7cbe\u5ea6\u8ba1\u7b97\u4e2d\u7684\u5e94\u7528\uff1b\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u66f4\u53ef\u9760\u4e14\u517c\u5bb9\u73b0\u6709SRAM\u6280\u672f\u7684\u65b0\u578b\u67b6\u6784\u3002", "method": "\u91c7\u75288\u00d78\u76848T SRAM\u9635\u5217\uff0c\u901a\u8fc7\u4e13\u7528\u8bfb\u4f4d\u7ebf\u4e0a\u7684\u7535\u8377\u5171\u4eab\u673a\u5236\u5b9e\u73b0\u5e76\u884cMAC\u8fd0\u7b97\u4e0e\u5e38\u89c4\u5b58\u50a8\u64cd\u4f5c\uff0c\u5e76\u5f15\u5165\u65b0\u9896\u7684\u6a21\u6570\u89e3\u7801\u65b9\u6848\u5c06\u6a21\u62df\u7535\u538b\u8f93\u51fa\u8f6c\u6362\u4e3a\u6570\u5b57\u4fe1\u53f7\uff0c\u4ece\u800c\u5728\u540c\u4e00\u9635\u5217\u4e2d\u5b9e\u73b0\u591a\u79cd\u57fa\u672c\u903b\u8f91\u529f\u80fd\u3002", "result": "\u572890 nm CMOS\u5de5\u827a\u30011.8 V\u4f9b\u7535\u6761\u4ef6\u4e0b\uff0c\u8be5\u8bbe\u8ba1\u4ee5142.85 MHz\u9891\u7387\u8fd0\u884c\uff0cMAC\u64cd\u4f5c\u5ef6\u8fdf\u4e3a0.7 ns\uff0c\u6bcf\u6bd4\u7279\u80fd\u8017\u4e3a56.56 fJ\uff0c\u541e\u5410\u7387\u8fbe15.8 M ops/s\uff0c\u5e76\u6210\u529f\u5b9e\u73b08\u6bd4\u7279MAC\u53ca\u591a\u79cd\u903b\u8f91\u8fd0\u7b97\u3002", "conclusion": "\u6240\u63d0\u51fa\u76848T SRAM\u5b58\u5185\u8ba1\u7b97\u67b6\u6784\u6709\u6548\u89e3\u51b3\u4e86\u4f20\u7edf\u8bbe\u8ba1\u7684\u53ef\u9760\u6027\u74f6\u9888\uff0c\u517c\u5177\u9ad8\u6027\u80fd\u4e0e\u4f4e\u529f\u8017\u4f18\u52bf\uff0c\u5c55\u793a\u4e86\u5728\u5b58\u7b97\u4e00\u4f53\u7cfb\u7edf\u4e2d\u7684\u5b9e\u7528\u6f5c\u529b\u3002"}}
{"id": "2512.01609", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2512.01609", "abs": "https://arxiv.org/abs/2512.01609", "authors": ["Patrick Herter", "Vincent Ahlrichs", "Ridvan A\u00e7ilan", "Julian Horsch"], "title": "GPTrace: Effective Crash Deduplication Using LLM Embeddings", "comment": "Original publication in 2026 IEEE/ACM 48th International Conference on Software Engineering (ICSE '26), April 12-18, 2026, Rio de Janeiro, Brazil. ACM, New York, NY, USA, 12 pages", "summary": "Fuzzing is a highly effective method for uncovering software vulnerabilities, but analyzing the resulting data typically requires substantial manual effort. This is amplified by the fact that fuzzing campaigns often find a large number of crashing inputs, many of which share the same underlying bug. Crash deduplication is the task of finding such duplicate crashing inputs and thereby reducing the data that needs to be examined. Many existing deduplication approaches rely on comparing stack traces or other information that is collected when a program crashes. Although various metrics for measuring the similarity of such pieces of information have been proposed, many do not yield satisfactory deduplication results. In this work, we present GPTrace, a deduplication workflow that leverages a large language model to evaluate the similarity of various data sources associated with crashes by computing embedding vectors and supplying those as input to a clustering algorithm. We evaluate our approach on over 300 000 crashing inputs belonging to 50 ground truth labels from 14 different targets. The deduplication results produced by GPTrace show a noticeable improvement over hand-crafted stack trace comparison methods and even more complex state-of-the-art approaches that are less flexible.", "AI": {"tldr": "GPTrace \u662f\u4e00\u79cd\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u5d29\u6e83\u53bb\u91cd\u65b9\u6cd5\uff0c\u901a\u8fc7\u751f\u6210\u5d29\u6e83\u76f8\u5173\u6570\u636e\u7684\u5d4c\u5165\u5411\u91cf\u5e76\u8fdb\u884c\u805a\u7c7b\uff0c\u5728\u8d85\u8fc730\u4e07\u4e2a\u5d29\u6e83\u8f93\u5165\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\u5176\u4f18\u4e8e\u73b0\u6709\u624b\u5de5\u8bbe\u8ba1\u6216\u590d\u6742\u4f46\u4e0d\u591f\u7075\u6d3b\u7684\u65b9\u6cd5\u3002", "motivation": "\u6a21\u7cca\u6d4b\u8bd5\u4f1a\u4ea7\u751f\u5927\u91cf\u5d29\u6e83\u8f93\u5165\uff0c\u5176\u4e2d\u8bb8\u591a\u5171\u4eab\u76f8\u540c\u7684\u6839\u672c\u6f0f\u6d1e\uff0c\u4eba\u5de5\u5206\u6790\u6210\u672c\u9ad8\uff0c\u56e0\u6b64\u9700\u8981\u6709\u6548\u7684\u5d29\u6e83\u53bb\u91cd\u6280\u672f\u6765\u51cf\u5c11\u9700\u5ba1\u67e5\u7684\u6570\u636e\u91cf\u3002", "method": "\u63d0\u51fa GPTrace \u6d41\u7a0b\uff0c\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\u4e3a\u5d29\u6e83\u76f8\u5173\u7684\u591a\u79cd\u6570\u636e\u6e90\u751f\u6210\u5d4c\u5165\u5411\u91cf\uff0c\u5e76\u5c06\u8fd9\u4e9b\u5411\u91cf\u8f93\u5165\u805a\u7c7b\u7b97\u6cd5\u4ee5\u8bc4\u4f30\u76f8\u4f3c\u6027\uff0c\u5b9e\u73b0\u5d29\u6e83\u53bb\u91cd\u3002", "result": "\u5728\u6765\u81ea14\u4e2a\u76ee\u6807\u3001\u5305\u542b50\u4e2a\u771f\u5b9e\u6807\u7b7e\u768430\u591a\u4e07\u4e2a\u5d29\u6e83\u8f93\u5165\u4e0a\u8bc4\u4f30\uff0cGPTrace \u7684\u53bb\u91cd\u6548\u679c\u660e\u663e\u4f18\u4e8e\u57fa\u4e8e\u6808\u8ddf\u8e2a\u7684\u624b\u5de5\u65b9\u6cd5\u548c\u66f4\u590d\u6742\u4f46\u7075\u6d3b\u6027\u8f83\u5dee\u7684\u524d\u6cbf\u65b9\u6cd5\u3002", "conclusion": "\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\u751f\u6210\u5d4c\u5165\u5411\u91cf\u8fdb\u884c\u5d29\u6e83\u53bb\u91cd\u662f\u4e00\u79cd\u6709\u6548\u4e14\u7075\u6d3b\u7684\u7b56\u7565\uff0c\u663e\u8457\u63d0\u5347\u4e86\u53bb\u91cd\u51c6\u786e\u6027\uff0c\u51cf\u8f7b\u4e86\u6f0f\u6d1e\u5206\u6790\u7684\u4eba\u5de5\u8d1f\u62c5\u3002"}}
{"id": "2512.01617", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2512.01617", "abs": "https://arxiv.org/abs/2512.01617", "authors": ["Pierciro Caliandro", "Matteo Ciccaglione", "Alessandro Pellegrini"], "title": "When High-Performance Computing Meets Software Testing: Distributed Fuzzing using MPI", "comment": null, "summary": "This paper explores the integration of MPI-based synchronization techniques into distributed fuzzing frameworks, highlighting possible substantial performance improvements compared to traditional filesystem-based synchronization methods. By employing lightweight MPI primitives, reductions in communication latency are achieved, facilitating more efficient data exchanges across distributed fuzzing nodes. Experimental results obtained over standard benchmarks demonstrate enhanced coverage progression from the early stages of the fuzzing process, which could be beneficial if fuzzing is employed in CI/CD pipelines at any stage of software development. Furthermore, the coordinated exchange of input corpora among clusters of fuzzers effectively addresses coverage stagnation, enabling a sustained exploration of complex and deep execution paths. Overall, the adoption of MPI-based synchronization approaches shows promising potential for significantly enhancing the scalability and efficacy of distributed fuzz testing.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u5728\u5206\u5e03\u5f0f\u6a21\u7cca\u6d4b\u8bd5\u6846\u67b6\u4e2d\u91c7\u7528\u57fa\u4e8eMPI\u7684\u540c\u6b65\u6280\u672f\uff0c\u4ee5\u66ff\u4ee3\u4f20\u7edf\u7684\u57fa\u4e8e\u6587\u4ef6\u7cfb\u7edf\u7684\u540c\u6b65\u65b9\u6cd5\uff0c\u4ece\u800c\u663e\u8457\u63d0\u5347\u6027\u80fd\u548c\u8986\u76d6\u7387\u3002", "motivation": "\u4f20\u7edf\u57fa\u4e8e\u6587\u4ef6\u7cfb\u7edf\u7684\u540c\u6b65\u673a\u5236\u5728\u5206\u5e03\u5f0f\u6a21\u7cca\u6d4b\u8bd5\u4e2d\u5b58\u5728\u901a\u4fe1\u5ef6\u8fdf\u9ad8\u3001\u6548\u7387\u4f4e\u7684\u95ee\u9898\uff0c\u9650\u5236\u4e86\u6a21\u7cca\u6d4b\u8bd5\u7684\u6269\u5c55\u6027\u548c\u8986\u76d6\u7387\u589e\u957f\u3002", "method": "\u5f15\u5165\u8f7b\u91cf\u7ea7MPI\u539f\u8bed\u5b9e\u73b0\u5206\u5e03\u5f0f\u6a21\u7cca\u8282\u70b9\u95f4\u7684\u9ad8\u6548\u901a\u4fe1\u4e0e\u8f93\u5165\u8bed\u6599\u540c\u6b65\uff0c\u964d\u4f4e\u901a\u4fe1\u5ef6\u8fdf\u5e76\u534f\u8c03\u591a\u4e2a\u6a21\u7cca\u5668\u96c6\u7fa4\u7684\u6570\u636e\u4ea4\u6362\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u6807\u51c6\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u4ece\u65e9\u671f\u9636\u6bb5\u5373\u5c55\u73b0\u51fa\u66f4\u5feb\u7684\u8986\u76d6\u7387\u589e\u957f\uff0c\u5e76\u6709\u6548\u7f13\u89e3\u4e86\u8986\u76d6\u7387\u505c\u6ede\u95ee\u9898\uff0c\u6709\u52a9\u4e8e\u5728CI/CD\u6d41\u7a0b\u4e2d\u5e94\u7528\u3002", "conclusion": "\u57fa\u4e8eMPI\u7684\u540c\u6b65\u673a\u5236\u5728\u63d0\u5347\u5206\u5e03\u5f0f\u6a21\u7cca\u6d4b\u8bd5\u7684\u53ef\u6269\u5c55\u6027\u4e0e\u6709\u6548\u6027\u65b9\u9762\u5177\u6709\u663e\u8457\u6f5c\u529b\u3002"}}
{"id": "2512.01630", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2512.01630", "abs": "https://arxiv.org/abs/2512.01630", "authors": ["Ziheng Liu", "Runzhi He", "Minghui Zhou"], "title": "Package Dashboard: A Cross-Ecosystem Framework for Dual-Perspective Analysis of Software Packages", "comment": null, "summary": "Software supply chain attacks have revealed blind spots in existing SCA tools, which are often limited to a single ecosystem and assess either software artifacts or community activity in isolation. This fragmentation across tools and ecosystems forces developers to manually reconcile scattered data, undermining risk assessments. We present Package Dashboard, a cross-ecosystem framework that provides a unified platform for supply chain analysis, enabling a holistic, dual-perspective risk assessment by integrating package metadata, vulnerability information, and upstream community health metrics. By combining dependency resolution with repository analysis, it reduces cognitive load and improves traceability. Demonstrating the framework's versatility, a large-scale study of 374,000 packages across five Linux distributions shows its ability to uncover not only conventional vulnerabilities and license conflicts but also overlooked risks such as archived or inaccessible repositories. Ultimately, Package Dashboard provides a unified view of risk, equipping developers and DevSecOps engineers with actionable insights to strengthen the transparency, trustworthiness, and traceability of open-source ecosystems. Package Dashboard is publicly available at https://github.com/n19htfall/PackageDashboard, and a demonstration video can be found at https://youtu.be/y9ncftP8KPQ. Besides, the online version is available at https://pkgdash.osslab-pku.org.", "AI": {"tldr": "Package Dashboard \u662f\u4e00\u4e2a\u8de8\u751f\u6001\u7cfb\u7edf\u7684\u8f6f\u4ef6\u4f9b\u5e94\u94fe\u5206\u6790\u6846\u67b6\uff0c\u901a\u8fc7\u6574\u5408\u5305\u5143\u6570\u636e\u3001\u6f0f\u6d1e\u4fe1\u606f\u548c\u4e0a\u6e38\u793e\u533a\u5065\u5eb7\u6307\u6807\uff0c\u63d0\u4f9b\u7edf\u4e00\u7684\u53cc\u91cd\u89c6\u89d2\u98ce\u9669\u8bc4\u4f30\uff0c\u63d0\u5347\u5f00\u6e90\u751f\u6001\u7cfb\u7edf\u7684\u900f\u660e\u5ea6\u3001\u53ef\u4fe1\u5ea6\u4e0e\u53ef\u8ffd\u6eaf\u6027\u3002", "motivation": "\u73b0\u6709\u8f6f\u4ef6\u6210\u5206\u5206\u6790\uff08SCA\uff09\u5de5\u5177\u901a\u5e38\u5c40\u9650\u4e8e\u5355\u4e00\u751f\u6001\u7cfb\u7edf\uff0c\u4e14\u4ec5\u5355\u72ec\u8bc4\u4f30\u8f6f\u4ef6\u5236\u54c1\u6216\u793e\u533a\u6d3b\u52a8\uff0c\u5bfc\u81f4\u5f00\u53d1\u8005\u9700\u624b\u52a8\u6574\u5408\u5206\u6563\u6570\u636e\uff0c\u5f71\u54cd\u98ce\u9669\u8bc4\u4f30\u6548\u679c\u3002", "method": "\u63d0\u51fa Package Dashboard \u6846\u67b6\uff0c\u7ed3\u5408\u4f9d\u8d56\u89e3\u6790\u4e0e\u4ed3\u5e93\u5206\u6790\uff0c\u96c6\u6210\u5305\u5143\u6570\u636e\u3001\u6f0f\u6d1e\u4fe1\u606f\u53ca\u793e\u533a\u5065\u5eb7\u6307\u6807\uff0c\u5728\u591a\u4e2a\u751f\u6001\u7cfb\u7edf\u4e2d\u5b9e\u73b0\u7edf\u4e00\u7684\u98ce\u9669\u8bc4\u4f30\u3002", "result": "\u5728\u6db5\u76d6\u4e94\u4e2a Linux \u53d1\u884c\u7248\u3001374,000 \u4e2a\u5305\u7684\u5927\u89c4\u6a21\u7814\u7a76\u4e2d\uff0c\u8be5\u6846\u67b6\u4e0d\u4ec5\u80fd\u8bc6\u522b\u4f20\u7edf\u6f0f\u6d1e\u548c\u8bb8\u53ef\u8bc1\u51b2\u7a81\uff0c\u8fd8\u80fd\u53d1\u73b0\u88ab\u5ffd\u89c6\u7684\u98ce\u9669\uff08\u5982\u5f52\u6863\u6216\u4e0d\u53ef\u8bbf\u95ee\u7684\u4ed3\u5e93\uff09\u3002", "conclusion": "Package Dashboard \u63d0\u4f9b\u4e86\u7edf\u4e00\u7684\u98ce\u9669\u89c6\u56fe\uff0c\u5e2e\u52a9\u5f00\u53d1\u8005\u548c DevSecOps \u5de5\u7a0b\u5e08\u83b7\u5f97\u53ef\u64cd\u4f5c\u7684\u6d1e\u5bdf\uff0c\u4ece\u800c\u589e\u5f3a\u5f00\u6e90\u8f6f\u4ef6\u4f9b\u5e94\u94fe\u7684\u900f\u660e\u5ea6\u3001\u53ef\u4fe1\u5ea6\u4e0e\u53ef\u8ffd\u6eaf\u6027\u3002"}}
{"id": "2512.01193", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2512.01193", "abs": "https://arxiv.org/abs/2512.01193", "authors": ["Masoud Rahimi", "S\u00e9bastien Le Beux"], "title": "Leveraging Recurrent Patterns in Graph Accelerators", "comment": "Accepted at DATE 2026", "summary": "Graph accelerators have emerged as a promising solution for processing large-scale sparse graphs, leveraging the in-situ compu-tation of ReRAM-based crossbars to maximize computational efficiency. However, existing designs suffer from memristor access overhead due to the large number of graph partitions. This leads to increased execution time, higher energy consumption, and re-duced circuit lifetime. This paper proposes a graph processing method that minimizes memristor write operations by identifying frequent subgraph patterns and assigning them to graph engines, referred to as static, allowing most subgraphs to be processed without a need for crossbar reconfiguration. Experimental results show speed up to 2.38x speedup and 7.23x energy savings com-pared to state-of-the-art accelerators. Furthermore, our method extends the circuit lifetime by 2x compared to state-of-the-art ReRAM graph accelerators.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u901a\u8fc7\u8bc6\u522b\u9891\u7e41\u5b50\u56fe\u6a21\u5f0f\u5e76\u5c06\u5176\u5206\u914d\u81f3\u9759\u6001\u56fe\u5f15\u64ce\u7684\u56fe\u5904\u7406\u65b9\u6cd5\uff0c\u4ee5\u51cf\u5c11ReRAM\u4ea4\u53c9\u5f00\u5173\u7684\u91cd\u914d\u7f6e\u9700\u6c42\uff0c\u4ece\u800c\u663e\u8457\u63d0\u5347\u6027\u80fd\u3001\u80fd\u6548\u5e76\u5ef6\u957f\u7535\u8def\u5bff\u547d\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8eReRAM\u4ea4\u53c9\u5f00\u5173\u7684\u56fe\u52a0\u901f\u5668\u56e0\u56fe\u5212\u5206\u6570\u91cf\u5e9e\u5927\u5bfc\u81f4\u5fc6\u963b\u5668\u8bbf\u95ee\u5f00\u9500\u9ad8\uff0c\u5f15\u53d1\u6267\u884c\u65f6\u95f4\u589e\u52a0\u3001\u80fd\u8017\u4e0a\u5347\u548c\u7535\u8def\u5bff\u547d\u7f29\u77ed\u7b49\u95ee\u9898\u3002", "method": "\u901a\u8fc7\u8bc6\u522b\u9891\u7e41\u51fa\u73b0\u7684\u5b50\u56fe\u6a21\u5f0f\uff0c\u5e76\u5c06\u8fd9\u4e9b\u5b50\u56fe\u5206\u914d\u7ed9\u79f0\u4e3a\u201c\u9759\u6001\u201d\u7684\u56fe\u5f15\u64ce\u8fdb\u884c\u5904\u7406\uff0c\u4ece\u800c\u907f\u514d\u5927\u591a\u6570\u5b50\u56fe\u5904\u7406\u8fc7\u7a0b\u4e2d\u5bf9\u4ea4\u53c9\u5f00\u5173\u8fdb\u884c\u91cd\u65b0\u914d\u7f6e\uff0c\u51cf\u5c11\u5fc6\u963b\u5668\u5199\u64cd\u4f5c\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u4e0e\u5f53\u524d\u6700\u5148\u8fdb\u7684\u52a0\u901f\u5668\u76f8\u6bd4\uff0c\u8be5\u65b9\u6cd5\u5b9e\u73b0\u4e86\u6700\u9ad82.38\u500d\u7684\u901f\u5ea6\u63d0\u5347\u30017.23\u500d\u7684\u80fd\u8017\u8282\u7701\uff0c\u5e76\u5c06\u7535\u8def\u5bff\u547d\u5ef6\u957f\u4e862\u500d\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u56fe\u5904\u7406\u65b9\u6cd5\u6709\u6548\u7f13\u89e3\u4e86ReRAM\u56fe\u52a0\u901f\u5668\u4e2d\u7684\u5fc6\u963b\u5668\u8bbf\u95ee\u5f00\u9500\u95ee\u9898\uff0c\u5728\u6027\u80fd\u3001\u80fd\u6548\u548c\u786c\u4ef6\u8010\u4e45\u6027\u65b9\u9762\u5747\u53d6\u5f97\u663e\u8457\u6539\u8fdb\u3002"}}
{"id": "2512.01649", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2512.01649", "abs": "https://arxiv.org/abs/2512.01649", "authors": ["Daniel Strassler", "Gabe Elkin", "Curran Schiefelbein", "Daniel Herring", "Ian Jessen", "David Johnson", "Santiago A. Paredes", "Tod Shannon", "Jim Flavin"], "title": "MIT Lincoln Laboratory: A Case Study on Improving Software Support for Research Projects", "comment": "Strassler, D., et al. MIT Lincoln Laboratory: A Case Study on Improving Software Support for Research Projects. Zenodo, 2025, https://doi.org/10.5281/zenodo.16878161", "summary": "Software plays an ever increasing role in complex system development and prototyping, and in recent years, MIT Lincoln Laboratory has sought to improve both the effectiveness and culture surrounding software engineering in execution of its mission. The Homeland Protection and Air Traffic Control Division conducted an internal study to examine challenges to effective and efficient research software development, and to identify ways to strengthen both the culture and execution for greater impact on our mission. Key findings of this study fell into three main categories: project attributes that influence how software development activities must be conducted and managed, potential efficiencies from centralization, opportunities to improve staffing and culture with respect to software practitioners. The study delivered actionable recommendations, including centralizing and standardizing software support tooling, developing a common database to help match the right software talent and needs to projects, and creating a software stakeholder panel to assist with continued improvement.", "AI": {"tldr": "MIT Lincoln Laboratory conducted a study to improve research software development effectiveness and culture, identifying key challenges and proposing actionable recommendations such as centralizing tooling, creating a talent-matching database, and forming a software stakeholder panel.", "motivation": "To address challenges in effective and efficient research software development and to strengthen software engineering culture and execution in support of MIT Lincoln Laboratory\u2019s mission.", "method": "An internal study within the Homeland Protection and Air Traffic Control Division examined project attributes, centralization opportunities, and staffing/culture issues related to software development.", "result": "The study identified three main categories of findings: (1) project attributes affecting software development practices, (2) efficiencies from centralizing resources, and (3) opportunities to enhance staffing and software culture.", "conclusion": "Actionable recommendations were delivered, including standardizing software tooling, establishing a common database for matching talent to projects, and creating a stakeholder panel to drive ongoing improvements in software engineering practices."}}
{"id": "2512.01463", "categories": ["cs.AR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.01463", "abs": "https://arxiv.org/abs/2512.01463", "authors": ["Jan-Frederik Schulte", "Benjamin Ramhorst", "Chang Sun", "Jovan Mitrevski", "Nicol\u00f2 Ghielmetti", "Enrico Lupi", "Dimitrios Danopoulos", "Vladimir Loncar", "Javier Duarte", "David Burnette", "Lauri Laatu", "Stylianos Tzelepis", "Konstantinos Axiotis", "Quentin Berthet", "Haoyan Wang", "Paul White", "Suleyman Demirsoy", "Marco Colombo", "Thea Aarrestad", "Sioni Summers", "Maurizio Pierini", "Giuseppe Di Guglielmo", "Jennifer Ngadiuba", "Javier Campos", "Ben Hawks", "Abhijith Gandrakota", "Farah Fahim", "Nhan Tran", "George Constantinides", "Zhiqiang Que", "Wayne Luk", "Alexander Tapper", "Duc Hoang", "Noah Paladino", "Philip Harris", "Bo-Cheng Lai", "Manuel Valentin", "Ryan Forelli", "Seda Ogrenci", "Lino Gerlach", "Rian Flynn", "Mia Liu", "Daniel Diaz", "Elham Khoda", "Melissa Quinnan", "Russell Solares", "Santosh Parajuli", "Mark Neubauer", "Christian Herwig", "Ho Fung Tsoi", "Dylan Rankin", "Shih-Chieh Hsu", "Scott Hauck"], "title": "hls4ml: A Flexible, Open-Source Platform for Deep Learning Acceleration on Reconfigurable Hardware", "comment": null, "summary": "We present hls4ml, a free and open-source platform that translates machine learning (ML) models from modern deep learning frameworks into high-level synthesis (HLS) code that can be integrated into full designs for field-programmable gate arrays (FPGAs) or application-specific integrated circuits (ASICs). With its flexible and modular design, hls4ml supports a large number of deep learning frameworks and can target HLS compilers from several vendors, including Vitis HLS, Intel oneAPI and Catapult HLS. Together with a wider eco-system for software-hardware co-design, hls4ml has enabled the acceleration of ML inference in a wide range of commercial and scientific applications where low latency, resource usage, and power consumption are critical. In this paper, we describe the structure and functionality of the hls4ml platform. The overarching design considerations for the generated HLS code are discussed, together with selected performance results.", "AI": {"tldr": "hls4ml \u662f\u4e00\u4e2a\u5f00\u6e90\u5e73\u53f0\uff0c\u53ef\u5c06\u4e3b\u6d41\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\u4e2d\u7684\u673a\u5668\u5b66\u4e60\u6a21\u578b\u81ea\u52a8\u8f6c\u6362\u4e3a\u9002\u7528\u4e8e FPGA \u6216 ASIC \u7684\u9ad8\u7ea7\u7efc\u5408\uff08HLS\uff09\u4ee3\u7801\uff0c\u652f\u6301\u591a\u79cd HLS \u7f16\u8bd1\u5668\uff0c\u5e76\u5df2\u5728\u591a\u4e2a\u5bf9\u4f4e\u5ef6\u8fdf\u3001\u8d44\u6e90\u5360\u7528\u548c\u529f\u8017\u654f\u611f\u7684\u5546\u4e1a\u4e0e\u79d1\u7814\u573a\u666f\u4e2d\u5b9e\u73b0\u90e8\u7f72\u3002", "motivation": "\u5728\u9700\u8981\u4f4e\u5ef6\u8fdf\u3001\u4f4e\u529f\u8017\u548c\u9ad8\u6548\u8d44\u6e90\u5229\u7528\u7684\u573a\u666f\u4e2d\uff0c\u5c06\u673a\u5668\u5b66\u4e60\u6a21\u578b\u90e8\u7f72\u5230 FPGA \u6216 ASIC \u4e0a\u5177\u6709\u91cd\u8981\u610f\u4e49\uff0c\u4f46\u7f3a\u4e4f\u901a\u7528\u3001\u7075\u6d3b\u7684\u5de5\u5177\u94fe\u6765\u7b80\u5316\u4ece\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u5230\u786c\u4ef6\u5b9e\u73b0\u7684\u8f6c\u6362\u8fc7\u7a0b\u3002", "method": "\u5f00\u53d1\u4e86 hls4ml \u5e73\u53f0\uff0c\u8be5\u5e73\u53f0\u91c7\u7528\u6a21\u5757\u5316\u67b6\u6784\uff0c\u80fd\u591f\u4ece\u591a\u79cd\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\u4e2d\u8bfb\u53d6\u6a21\u578b\uff0c\u5e76\u751f\u6210\u517c\u5bb9\u591a\u4e2a\u5382\u5546 HLS \u7f16\u8bd1\u5668\uff08\u5982 Vitis HLS\u3001Intel oneAPI\u3001Catapult HLS\uff09\u7684\u4ee3\u7801\uff0c\u540c\u65f6\u878d\u5165\u8f6f\u786c\u4ef6\u534f\u540c\u8bbe\u8ba1\u751f\u6001\u3002", "result": "hls4ml \u5df2\u6210\u529f\u5e94\u7528\u4e8e\u591a\u4e2a\u5b9e\u9645\u573a\u666f\uff0c\u5c55\u793a\u4e86\u5176\u5728\u52a0\u901f\u673a\u5668\u5b66\u4e60\u63a8\u7406\u65b9\u9762\u7684\u6709\u6548\u6027\uff0c\u5e76\u63d0\u4f9b\u4e86\u5bf9\u751f\u6210 HLS \u4ee3\u7801\u7684\u5173\u952e\u8bbe\u8ba1\u8003\u91cf\u53ca\u90e8\u5206\u6027\u80fd\u8bc4\u4f30\u7ed3\u679c\u3002", "conclusion": "hls4ml \u63d0\u4f9b\u4e86\u4e00\u4e2a\u7075\u6d3b\u3001\u5f00\u653e\u4e14\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u6709\u6548\u5f25\u5408\u4e86\u673a\u5668\u5b66\u4e60\u6a21\u578b\u4e0e\u5b9a\u5236\u786c\u4ef6\u5b9e\u73b0\u4e4b\u95f4\u7684\u9e3f\u6c9f\uff0c\u63a8\u52a8\u4e86 ML \u5728\u8fb9\u7f18\u548c\u5d4c\u5165\u5f0f\u7cfb\u7edf\u4e2d\u7684\u90e8\u7f72\u3002"}}
{"id": "2512.01690", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2512.01690", "abs": "https://arxiv.org/abs/2512.01690", "authors": ["Philip Garrett", "Juan P. Galeotti", "Andrea Arcuri", "Alexander Poth", "Olsi Rrjolli"], "title": "Generating REST API Tests With Descriptive Names", "comment": null, "summary": "Automated test generation has become a key technique for ensuring software quality, particularly in modern API-based architectures. However, automatically generated test cases are typically assigned non-descriptive names (e.g., test0, test1), which reduces their readability and hinders their usefulness during comprehension and maintenance. In this work, we present three novel deterministic techniques to generate REST API test names. We then compare eight techniques in total for generating descriptive names for REST API tests automatically produced by the fuzzer EvoMaster, using 10 test cases generated for 9 different open-source APIs. The eight techniques include rule-based heuristics and large language model (LLM)-based approaches. Their effectiveness was empirically evaluated through two surveys (involving up to 39 people recruited via LinkedIn). Our results show that a rule-based approach achieves the highest clarity ratings among deterministic methods, performs on par with state-of-the-art LLM-based models such as Gemini and GPT-4o, and significantly outperforms GPT-3.5.\n  To further evaluate the practical impact of our results, an industrial case study was carried out with practitioners who actively use EvoMaster at Volkswagen AG. A developer questionnaire was then carried out based on the use of EvoMaster on four different APIs by four different users, for a total of 74 evaluated test cases. Feedback from practitioners further confirms that descriptive names produced by this approach improve test suite readability.\n  These findings highlight that lightweight, deterministic techniques can serve as effective alternatives to computationally expensive and security-sensitive LLM-based approaches for automated system-level test naming, providing a practical step toward more developer-friendly API test generation.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u5e76\u8bc4\u4f30\u4e86\u4e09\u79cd\u65b0\u7684\u786e\u5b9a\u6027\u65b9\u6cd5\uff0c\u7528\u4e8e\u4e3a\u81ea\u52a8\u751f\u6210\u7684REST API\u6d4b\u8bd5\u7528\u4f8b\u751f\u6210\u63cf\u8ff0\u6027\u540d\u79f0\u3002\u5b9e\u9a8c\u8868\u660e\uff0c\u4e00\u79cd\u57fa\u4e8e\u89c4\u5219\u7684\u65b9\u6cd5\u5728\u6e05\u6670\u5ea6\u4e0a\u4f18\u4e8e\u5176\u4ed6\u786e\u5b9a\u6027\u65b9\u6cd5\uff0c\u4e0eGemini\u548cGPT-4o\u7b49\u5148\u8fdb\u5927\u8bed\u8a00\u6a21\u578b\u6548\u679c\u76f8\u5f53\uff0c\u5e76\u663e\u8457\u4f18\u4e8eGPT-3.5\u3002\u5de5\u4e1a\u6848\u4f8b\u7814\u7a76\u8fdb\u4e00\u6b65\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\u80fd\u6709\u6548\u63d0\u5347\u6d4b\u8bd5\u5957\u4ef6\u7684\u53ef\u8bfb\u6027\u3002", "motivation": "\u81ea\u52a8\u751f\u6210\u7684API\u6d4b\u8bd5\u7528\u4f8b\u901a\u5e38\u4f7f\u7528\u975e\u63cf\u8ff0\u6027\u540d\u79f0\uff08\u5982test0\u3001test1\uff09\uff0c\u964d\u4f4e\u4e86\u53ef\u8bfb\u6027\uff0c\u5f71\u54cd\u5f00\u53d1\u4eba\u5458\u5bf9\u6d4b\u8bd5\u7684\u7406\u89e3\u4e0e\u7ef4\u62a4\u3002\u56e0\u6b64\uff0c\u4e9f\u9700\u81ea\u52a8\u5316\u751f\u6210\u66f4\u5177\u63cf\u8ff0\u6027\u7684\u6d4b\u8bd5\u540d\u79f0\u3002", "method": "\u63d0\u51fa\u4e86\u4e09\u79cd\u65b0\u7684\u786e\u5b9a\u6027\u547d\u540d\u6280\u672f\uff0c\u5e76\u4e0e\u5305\u62ec\u57fa\u4e8e\u89c4\u5219\u542f\u53d1\u5f0f\u548c\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5728\u5185\u7684\u516b\u79cd\u65b9\u6cd5\u8fdb\u884c\u6bd4\u8f83\u3002\u8bc4\u4f30\u57fa\u4e8eEvoMaster\u6a21\u7cca\u5668\u4e3a9\u4e2a\u5f00\u6e90API\u751f\u6210\u768410\u4e2a\u6d4b\u8bd5\u7528\u4f8b\uff0c\u5e76\u901a\u8fc7\u4e24\u9879\u6d89\u53ca\u6700\u591a39\u540d\u53c2\u4e0e\u8005\u7684\u8c03\u67e5\u4ee5\u53ca\u5728Volkswagen AG\u5f00\u5c55\u7684\u5de5\u4e1a\u6848\u4f8b\u7814\u7a76\uff08\u6db5\u76d64\u4e2aAPI\u300174\u4e2a\u6d4b\u8bd5\u7528\u4f8b\uff09\u8fdb\u884c\u5b9e\u8bc1\u5206\u6790\u3002", "result": "\u57fa\u4e8e\u89c4\u5219\u7684\u65b9\u6cd5\u5728\u786e\u5b9a\u6027\u65b9\u6cd5\u4e2d\u83b7\u5f97\u6700\u9ad8\u6e05\u6670\u5ea6\u8bc4\u5206\uff0c\u6027\u80fd\u4e0eGemini\u548cGPT-4o\u76f8\u5f53\uff0c\u663e\u8457\u4f18\u4e8eGPT-3.5\uff1b\u5de5\u4e1a\u5b9e\u8df5\u53cd\u9988\u4e5f\u8bc1\u5b9e\u5176\u751f\u6210\u7684\u540d\u79f0\u6709\u6548\u63d0\u5347\u4e86\u6d4b\u8bd5\u5957\u4ef6\u7684\u53ef\u8bfb\u6027\u3002", "conclusion": "\u8f7b\u91cf\u7ea7\u3001\u786e\u5b9a\u6027\u7684\u547d\u540d\u6280\u672f\u53ef\u4f5c\u4e3a\u8ba1\u7b97\u5f00\u9500\u5927\u4e14\u5b58\u5728\u5b89\u5168\u98ce\u9669\u7684\u5927\u8bed\u8a00\u6a21\u578b\u65b9\u6cd5\u7684\u6709\u6548\u66ff\u4ee3\u65b9\u6848\uff0c\u5728\u7cfb\u7edf\u7ea7\u81ea\u52a8\u5316\u6d4b\u8bd5\u547d\u540d\u4e2d\u5177\u6709\u5b9e\u7528\u4ef7\u503c\uff0c\u6709\u52a9\u4e8e\u63d0\u5347API\u6d4b\u8bd5\u7684\u5f00\u53d1\u8005\u53cb\u597d\u6027\u3002"}}
{"id": "2512.01541", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2512.01541", "abs": "https://arxiv.org/abs/2512.01541", "authors": ["Hwayong Nam", "Seungmin Baek", "Jumin Kim", "Michael Jaemin Kim", "Jung Ho Ahn"], "title": "RoMe: Row Granularity Access Memory System for Large Language Models", "comment": "15 pages, 14 figures, accepted at HPCA 2026", "summary": "Modern HBM-based memory systems have evolved over generations while retaining cache line granularity accesses. Preserving this fine granularity necessitated the introduction of bank groups and pseudo channels. These structures expand timing parameters and control overhead, significantly increasing memory controller scheduling complexity. Large language models (LLMs) now dominate deep learning workloads, streaming contiguous data blocks ranging from several kilobytes to megabytes per operation. In a conventional HBM-based memory system, these transfers are fragmented into hundreds of 32B cache line transactions. This forces the memory controller to employ unnecessarily intricate scheduling, leading to growing inefficiency.\n  To address this problem, we propose RoMe. RoMe accesses DRAM at row granularity and removes columns, bank groups, and pseudo channels from the memory interface. This design simplifies memory scheduling, thereby requiring fewer pins per channel. The freed pins are aggregated to form additional channels, increasing overall bandwidth by 12.5% with minimal extra pins. RoMe demonstrates how memory scheduling logic can be significantly simplified for representative LLM workloads, and presents an alternative approach for next-generation HBM-based memory systems achieving increased bandwidth with minimal hardware overhead.", "AI": {"tldr": "RoMe is a novel HBM-based memory system that replaces fine-grained cache line accesses with row-granularity accesses to simplify memory scheduling and increase bandwidth for large language model (LLM) workloads.", "motivation": "Modern HBM systems retain cache-line granularity, which introduces complex structures like bank groups and pseudo channels, increasing scheduling complexity\u2014especially inefficient for LLMs that access large contiguous data blocks.", "method": "RoMe accesses DRAM at row granularity and eliminates columns, bank groups, and pseudo channels from the memory interface, simplifying scheduling and repurposing freed pins to add more channels.", "result": "RoMe increases overall memory bandwidth by 12.5% with minimal additional pin overhead while significantly simplifying memory controller scheduling for LLM workloads.", "conclusion": "RoMe demonstrates that redesigning HBM interfaces around row-granularity access can better match LLM memory access patterns, offering higher bandwidth and simpler control logic with minimal hardware changes."}}
{"id": "2512.01939", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.01939", "abs": "https://arxiv.org/abs/2512.01939", "authors": ["Yanlin Wang", "Xinyi Xu", "Jiachi Chen", "Tingting Bi", "Wenchao Gu", "Zibin Zheng"], "title": "An Empirical Study of Agent Developer Practices in AI Agent Frameworks", "comment": null, "summary": "The rise of large language models (LLMs) has sparked a surge of interest in agents, leading to the rapid growth of agent frameworks. Agent frameworks are software toolkits and libraries that provide standardized components, abstractions, and orchestration mechanisms to simplify agent development. Despite widespread use of agent frameworks, their practical applications and how they influence the agent development process remain underexplored. Different agent frameworks encounter similar problems during use, indicating that these recurring issues deserve greater attention and call for further improvements in agent framework design. Meanwhile, as the number of agent frameworks continues to grow and evolve, more than 80% of developers report difficulties in identifying the frameworks that best meet their specific development requirements. In this paper, we conduct the first empirical study of LLM-based agent frameworks, exploring real-world experiences of developers in building AI agents. To compare how well the agent frameworks meet developer needs, we further collect developer discussions for the ten previously identified agent frameworks, resulting in a total of 11,910 discussions. Finally, by analyzing these discussions, we compare the frameworks across five dimensions: development efficiency, functional abstraction, learning cost, performance optimization, and maintainability, which refers to how easily developers can update and extend both the framework itself and the agents built upon it over time. Our comparative analysis reveals significant differences among frameworks in how they meet the needs of agent developers. Overall, we provide a set of findings and implications for the LLM-driven AI agent framework ecosystem and offer insights for the design of future LLM-based agent frameworks and agent developers.", "AI": {"tldr": "\u672c\u6587\u9996\u6b21\u5bf9\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u667a\u80fd\u4f53\u6846\u67b6\u8fdb\u884c\u4e86\u5b9e\u8bc1\u7814\u7a76\uff0c\u901a\u8fc7\u5206\u679011,910\u6761\u5f00\u53d1\u8005\u8ba8\u8bba\uff0c\u4ece\u5f00\u53d1\u6548\u7387\u3001\u529f\u80fd\u62bd\u8c61\u3001\u5b66\u4e60\u6210\u672c\u3001\u6027\u80fd\u4f18\u5316\u548c\u53ef\u7ef4\u62a4\u6027\u4e94\u4e2a\u7ef4\u5ea6\u6bd4\u8f83\u4e86\u5341\u4e2a\u4e3b\u6d41\u6846\u67b6\uff0c\u63ed\u793a\u4e86\u5b83\u4eec\u5728\u6ee1\u8db3\u5f00\u53d1\u8005\u9700\u6c42\u65b9\u9762\u7684\u663e\u8457\u5dee\u5f02\uff0c\u5e76\u4e3a\u672a\u6765\u6846\u67b6\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u5efa\u8bae\u3002", "motivation": "\u5c3d\u7ba1\u667a\u80fd\u4f53\u6846\u67b6\u88ab\u5e7f\u6cdb\u4f7f\u7528\uff0c\u4f46\u5176\u5b9e\u9645\u5e94\u7528\u60c5\u51b5\u53ca\u5176\u5bf9\u5f00\u53d1\u8fc7\u7a0b\u7684\u5f71\u54cd\u5c1a\u4e0d\u660e\u786e\uff1b\u540c\u65f6\uff0c\u8d85\u8fc780%\u7684\u5f00\u53d1\u8005\u96be\u4ee5\u9009\u62e9\u6700\u9002\u5408\u81ea\u5df1\u9700\u6c42\u7684\u6846\u67b6\uff0c\u4e14\u591a\u4e2a\u6846\u67b6\u9762\u4e34\u76f8\u4f3c\u95ee\u9898\uff0c\u4e9f\u9700\u7cfb\u7edf\u6027\u7814\u7a76\u4ee5\u6307\u5bfc\u6539\u8fdb\u3002", "method": "\u4f5c\u8005\u5f00\u5c55\u4e86\u9996\u4e2a\u9488\u5bf9LLM\u667a\u80fd\u4f53\u6846\u67b6\u7684\u5b9e\u8bc1\u7814\u7a76\uff0c\u6536\u96c6\u5e76\u5206\u6790\u4e86\u5341\u4e2a\u4ee3\u8868\u6027\u6846\u67b6\u768411,910\u6761\u5f00\u53d1\u8005\u8ba8\u8bba\uff0c\u4ece\u5f00\u53d1\u6548\u7387\u3001\u529f\u80fd\u62bd\u8c61\u3001\u5b66\u4e60\u6210\u672c\u3001\u6027\u80fd\u4f18\u5316\u548c\u53ef\u7ef4\u62a4\u6027\u4e94\u4e2a\u7ef4\u5ea6\u8fdb\u884c\u6bd4\u8f83\u5206\u6790\u3002", "result": "\u5206\u6790\u53d1\u73b0\u4e0d\u540c\u6846\u67b6\u5728\u6ee1\u8db3\u5f00\u53d1\u8005\u9700\u6c42\u65b9\u9762\u5b58\u5728\u663e\u8457\u5dee\u5f02\uff0c\u5728\u4e94\u4e2a\u8bc4\u4f30\u7ef4\u5ea6\u4e0a\u8868\u73b0\u5404\u5f02\uff0c\u63ed\u793a\u4e86\u5f53\u524d\u6846\u67b6\u8bbe\u8ba1\u4e2d\u7684\u5171\u6027\u95ee\u9898\u4e0e\u6539\u8fdb\u7a7a\u95f4\u3002", "conclusion": "\u672c\u7814\u7a76\u4e3aLLM\u9a71\u52a8\u7684\u667a\u80fd\u4f53\u6846\u67b6\u751f\u6001\u7cfb\u7edf\u63d0\u4f9b\u4e86\u5b9e\u8bc1\u4f9d\u636e\u548c\u8bbe\u8ba1\u542f\u793a\uff0c\u6709\u52a9\u4e8e\u6307\u5bfc\u672a\u6765\u667a\u80fd\u4f53\u6846\u67b6\u7684\u4f18\u5316\u4e0e\u5f00\u53d1\u8005\u5de5\u5177\u7684\u9009\u62e9\u3002"}}
{"id": "2512.01644", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2512.01644", "abs": "https://arxiv.org/abs/2512.01644", "authors": ["Haonan Wang", "Xuxin Xiao", "Mingyu Yan", "Zhuoyuan Zhu", "Dengke Han", "Duo Wang", "Wenming Li", "Xiaochun Ye", "Cunchen Hu", "Hongyang Chen", "Guangyu Sun"], "title": "A Systematic Characterization of LLM Inference on GPUs", "comment": null, "summary": "This work presents a systematic characterization of Large Language Model (LLM) inference to address fragmented understanding. Through comprehensive experiments, we establish a four-dimensional analytical framework: (1) Two-Phase Heterogeneity Observation; (2) Microarchitectural Root Cause Analysis; (3) System Scaling Principles; and (4) Emerging Paradigm Boundaries. Our investigation progresses systematically from observation to foresight: identifying performance phenomena, revealing hardware causes, validating system behavior, and exploring new paradigms. This study not only consolidates a reliable empirical foundation for existing research but also provides new discoveries and practical optimization guidance for LLM inference.", "AI": {"tldr": "\u672c\u6587\u901a\u8fc7\u7cfb\u7edf\u6027\u5b9e\u9a8c\uff0c\u6784\u5efa\u4e86\u4e00\u4e2a\u56db\u7ef4\u5206\u6790\u6846\u67b6\uff0c\u5168\u9762\u523b\u753b\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u63a8\u7406\u8fc7\u7a0b\uff0c\u4ece\u73b0\u8c61\u89c2\u5bdf\u5230\u786c\u4ef6\u6839\u6e90\u3001\u7cfb\u7edf\u6269\u5c55\u89c4\u5f8b\u53ca\u65b0\u8303\u5f0f\u8fb9\u754c\uff0c\u4e3aLLM\u63a8\u7406\u4f18\u5316\u63d0\u4f9b\u5b9e\u8bc1\u57fa\u7840\u4e0e\u5b9e\u7528\u6307\u5bfc\u3002", "motivation": "\u5f53\u524d\u5bf9\u5927\u8bed\u8a00\u6a21\u578b\u63a8\u7406\u7684\u7406\u89e3\u8f83\u4e3a\u788e\u7247\u5316\uff0c\u7f3a\u4e4f\u7cfb\u7edf\u6027\u7684\u5206\u6790\u6846\u67b6\uff0c\u9650\u5236\u4e86\u6027\u80fd\u4f18\u5316\u548c\u65b0\u8303\u5f0f\u7684\u63a2\u7d22\u3002", "method": "\u901a\u8fc7\u5168\u9762\u5b9e\u9a8c\uff0c\u5efa\u7acb\u5305\u542b\u56db\u4e2a\u7ef4\u5ea6\u7684\u5206\u6790\u6846\u67b6\uff1a(1) \u4e24\u9636\u6bb5\u5f02\u6784\u6027\u89c2\u5bdf\uff1b(2) \u5fae\u67b6\u6784\u6839\u6e90\u5206\u6790\uff1b(3) \u7cfb\u7edf\u6269\u5c55\u539f\u5219\uff1b(4) \u65b0\u5174\u8303\u5f0f\u8fb9\u754c\uff0c\u5e76\u4f9d\u6b21\u4ece\u73b0\u8c61\u8bc6\u522b\u3001\u786c\u4ef6\u5f52\u56e0\u3001\u7cfb\u7edf\u9a8c\u8bc1\u5230\u8303\u5f0f\u63a2\u7d22\u8fdb\u884c\u7cfb\u7edf\u7814\u7a76\u3002", "result": "\u63ed\u793a\u4e86LLM\u63a8\u7406\u4e2d\u7684\u5173\u952e\u6027\u80fd\u73b0\u8c61\u53ca\u5176\u5fae\u67b6\u6784\u6839\u6e90\uff0c\u9a8c\u8bc1\u4e86\u7cfb\u7edf\u5c42\u9762\u7684\u6269\u5c55\u89c4\u5f8b\uff0c\u5e76\u754c\u5b9a\u4e86\u65b0\u5174\u63a8\u7406\u8303\u5f0f\u7684\u9002\u7528\u8fb9\u754c\u3002", "conclusion": "\u672c\u7814\u7a76\u4e0d\u4ec5\u4e3a\u73b0\u6709LLM\u63a8\u7406\u7814\u7a76\u63d0\u4f9b\u4e86\u53ef\u9760\u7684\u5b9e\u8bc1\u57fa\u7840\uff0c\u8fd8\u5e26\u6765\u4e86\u65b0\u53d1\u73b0\uff0c\u5e76\u4e3a\u5b9e\u9645\u7cfb\u7edf\u4f18\u5316\u63d0\u4f9b\u4e86\u6307\u5bfc\u3002"}}
