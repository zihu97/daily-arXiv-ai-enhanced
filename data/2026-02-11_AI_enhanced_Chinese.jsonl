{"id": "2602.09473", "categories": ["cs.NI", "cs.PF", "eess.SY"], "pdf": "https://arxiv.org/pdf/2602.09473", "abs": "https://arxiv.org/abs/2602.09473", "authors": ["Yuejie Wang", "Chenchen Shou", "Jiaxu Qian", "Guyue Liu"], "title": "XLB: A High Performance Layer-7 Load Balancer for Microservices using eBPF-based In-kernel Interposition", "comment": null, "summary": "L7 load balancers are a fundamental building block in microservices as they enable fine-grained traffic distribution. Compared to monolithic applications, microservices demand higher performance and stricter isolation from load balancers. This is due to the increased number of instances, longer service chains, and the necessity for co-location with services on the same host. Traditional sidecar-based load balancers are ill-equipped to meet these demands, often resulting in significant performance degradation.\n  In this work, we present XLB, a novel architecture that reshapes L7 load balancers as in-kernel interposition operating on the socket layer. We leverage eBPF to implement the core load balancing logic in the kernel, and address the connection management and state maintenance challenges through novel socket layer redirection and nested eBPF maps designs. XLB eliminates the extra overhead of scheduling, communication, and data movement, resulting in a more lightweight, scalable, and efficient L7 load balancer architecture. Compared to the widely used microservices load balancers (Istio and Cilium), over 50 microservice instances, XLB achieves up to 1.5x higher throughput and 60% lower end-to-end latency.", "AI": {"tldr": "\u63d0\u51faXLB\u67b6\u6784\uff0c\u4e00\u79cd\u57fa\u4e8eeBPF\u7684\u5185\u6838\u5c42L7\u8d1f\u8f7d\u5747\u8861\u5668\uff0c\u5927\u5e45\u63d0\u5347\u5fae\u670d\u52a1\u6027\u80fd", "motivation": "\u4f20\u7edf\u8d1f\u8f7d\u5747\u8861\u5668\u5728\u5fae\u670d\u52a1\u573a\u666f\u4e2d\u6027\u80fd\u4e0d\u8db3\uff0c\u65e0\u6cd5\u6ee1\u8db3\u9ad8\u5b9e\u4f8b\u6570\u3001\u957f\u670d\u52a1\u94fe\u7b49\u9700\u6c42\u800c\u5bfc\u81f4\u6027\u80fd\u4e0b\u964d", "method": "\u91cd\u65b0\u8bbe\u8ba1\u8d1f\u8f7d\u5747\u8861\u67b6\u6784\uff0c\u5229\u7528eBPF\u5728\u5185\u6838\u5c42\u5b9e\u73b0\u6838\u5fc3\u903b\u8f91\uff0c\u901a\u8fc7\u5957\u63a5\u5b57\u91cd\u5b9a\u5411\u548c\u5d4c\u5957eBPF\u6620\u5c04\u89e3\u51b3\u8fde\u63a5\u7ba1\u7406\u95ee\u9898", "result": "\u6bd4Istio\u548cCilium\u6027\u80fd\u663e\u8457\u63d0\u5347\uff1a50\u5b9e\u4f8b\u73af\u5883\u4e0b\u541e\u5410\u91cf\u6700\u9ad8\u589e\u52a01.5\u500d\uff0c\u7aef\u5230\u7aef\u5ef6\u8fdf\u964d\u4f4e60%", "conclusion": "XLB\u63d0\u4f9b\u66f4\u8f7b\u91cf\u3001\u53ef\u6269\u5c55\u4e14\u9ad8\u6548\u7684\u8d1f\u8f7d\u5747\u8861\u67b6\u6784\uff0c\u6d88\u9664\u4e86\u4f20\u7edf\u65b9\u6848\u7684\u8c03\u5ea6\u548c\u901a\u4fe1\u5f00\u9500"}}
{"id": "2602.09148", "categories": ["cs.NI"], "pdf": "https://arxiv.org/pdf/2602.09148", "abs": "https://arxiv.org/abs/2602.09148", "authors": ["Muhammad Haseeb", "Jinkun Geng", "Aurojit Panda", "Radhika Mittal", "Nirav Atre", "Srinivas Narayana", "Anirudh Sivaraman"], "title": "Probabilistic Fair Ordering of Events", "comment": null, "summary": "A growing class of applications depends on fair ordering, where events that occur earlier should be processed before later ones. Providing such guarantees is difficult in practice because clock synchronization is inherently imperfect: events generated at different clients within a short time window may carry timestamps that cannot be reliably ordered. Rather than attempting to eliminate synchronization error, we embrace it and establish a probabilistically fair sequencing process. Tommy is a sequencer that uses a statistical model of per-clock synchronization error to compare noisy timestamps probabilistically. Although this enables ordering of two events, the probabilistic comparator is intransitive, making global ordering non-trivial. We address this challenge by mapping the sequencing problem to a classical ranking problem from social choice theory, which offers principled mechanisms for reasoning with intransitive comparisons. Using this formulation, Tommy produces a partial order of events, achieving significantly better fairness than a Spanner TrueTime-based baseline approach.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faTommy\u7cfb\u7edf\uff0c\u5229\u7528\u65f6\u949f\u8bef\u5dee\u7edf\u8ba1\u6a21\u578b\u548c\u793e\u4ea4\u9009\u62e9\u7406\u8bba\u5b9e\u73b0\u6982\u7387\u516c\u5e73\u7684\u4e8b\u4ef6\u6392\u5e8f", "motivation": "\u5728\u65f6\u949f\u540c\u6b65\u5b58\u5728\u56fa\u6709\u8bef\u5dee\u7684\u80cc\u666f\u4e0b\uff0c\u5e38\u89c4\u65f6\u95f4\u6233\u65e0\u6cd5\u53ef\u9760\u6392\u5e8f\u4e8b\u4ef6\uff0c\u9700\u8981\u89e3\u51b3\u77ed\u6682\u65f6\u95f4\u7a97\u5185\u4e8b\u4ef6\u7684\u516c\u5e73\u6392\u5e8f\u95ee\u9898", "method": "\u5efa\u7acb\u65f6\u949f\u540c\u6b65\u8bef\u5dee\u7edf\u8ba1\u6a21\u578b\uff0c\u901a\u8fc7\u6982\u7387\u6bd4\u8f83\u65f6\u95f4\u6233\uff1b\u7ed3\u5408\u793e\u4ea4\u9009\u62e9\u7406\u8bba\u7684\u6392\u540d\u673a\u5236\u5904\u7406\u6982\u7387\u6bd4\u8f83\u7684\u975e\u4f20\u9012\u6027\u95ee\u9898\uff0c\u751f\u6210\u4e8b\u4ef6\u504f\u5e8f\u5173\u7cfb", "result": "\u76f8\u8f83\u4e8eSpanner TrueTime\u57fa\u51c6\u65b9\u6cd5\uff0cTommy\u663e\u8457\u63d0\u5347\u516c\u5e73\u6027\uff0c\u5b9e\u73b0\u9ad8\u6548\u90e8\u5206\u6392\u5e8f", "conclusion": "\u63a5\u53d7\u65f6\u949f\u8bef\u5dee\u800c\u975e\u6d88\u9664\u5b83\uff0c\u901a\u8fc7\u7edf\u8ba1\u5b66\u4e0e\u793e\u4f1a\u9009\u62e9\u7406\u8bba\u7ed3\u5408\uff0c\u5b9e\u73b0\u4e86\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u7684\u516c\u5e73\u4e8b\u4ef6\u5e8f\u5217"}}
{"id": "2602.09051", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.09051", "abs": "https://arxiv.org/abs/2602.09051", "authors": ["Avaljot Singh", "Dushyant Bharadwaj", "Stefanos Baziotis", "Kaushik Varadharajan", "Charith Mendis"], "title": "RuleFlow : Generating Reusable Program Optimizations with LLMs", "comment": null, "summary": "Optimizing Pandas programs is a challenging problem. Existing systems and compiler-based approaches offer reliability but are either heavyweight or support only a limited set of optimizations. Conversely, using LLMs in a per-program optimization methodology can synthesize nontrivial optimizations, but is unreliable, expensive, and offers a low yield. In this work, we introduce a hybrid approach that works in a 3-stage manner that decouples discovery from deployment and connects them via a novel bridge. First, it discovers per-program optimizations (discovery). Second, they are converted into generalised rewrite rules (bridge). Finally, these rules are incorporated into a compiler that can automatically apply them wherever applicable, eliminating repeated reliance on LLMs (deployment). We demonstrate that RuleFlow is the new state-of-the-art (SOTA) Pandas optimization framework on PandasBench, a challenging Pandas benchmark consisting of Python notebooks. Across these notebooks, we achieve a speedup of up to 4.3x over Dias, the previous compiler-based SOTA, and 1914.9x over Modin, the previous systems-based SOTA.\n  Our code is available at https://github.com/ADAPT-uiuc/RuleFlow.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faRuleFlow\u6df7\u5408\u6846\u67b6\uff0c\u901a\u8fc7\u4e09\u9636\u6bb5\u65b9\u6cd5\u4f18\u5316Pandas\u7a0b\u5e8f\uff1a\u63a2\u7d22\u5355\u7a0b\u5e8f\u4f18\u5316\u2192\u8f6c\u6362\u4e3a\u901a\u7528\u89c4\u5219\u2192\u7f16\u8bd1\u5668\u81ea\u52a8\u90e8\u7f72\uff0c\u5728PandasBench\u4e0a\u5b9e\u73b0\u6700\u9ad84.3\u500d\u548c1914.9\u500d\u52a0\u901f", "motivation": "\u73b0\u6709Pandas\u4f18\u5316\u6280\u672f\u5b58\u5728\u4e24\u6781\u5316\u7f3a\u9677\uff1a\u7f16\u8bd1\u5668\u65b9\u6848\u8f7b\u91cf\u4f46\u4f18\u5316\u6709\u9650\uff0c\u7cfb\u7edf\u65b9\u6848\u5168\u9762\u4f46\u7b28\u91cd\uff1bLLM\u53ef\u5b9e\u73b0\u590d\u6742\u4f18\u5316\u4f46\u4e0d\u53ef\u9760\u3001\u6210\u672c\u9ad8\u4e14\u6548\u7387\u4f4e\u4e0b", "method": "\u4e09\u9636\u6bb5\u6df7\u5408\u67b6\u6784\uff1a1\uff09\u63a2\u7d22\u9636\u6bb5\u5229\u7528LLM\u751f\u6210\u5355\u7a0b\u5e8f\u4f18\u5316\u65b9\u6848\uff1b2\uff09\u6865\u63a5\u9636\u6bb5\u5c06\u4f18\u5316\u8f6c\u5316\u4e3a\u901a\u7528\u91cd\u5199\u89c4\u5219\uff1b3\uff09\u90e8\u7f72\u9636\u6bb5\u901a\u8fc7\u7f16\u8bd1\u5668\u81ea\u52a8\u5e94\u7528\u89c4\u5219\uff0c\u907f\u514d\u91cd\u590d\u4f9d\u8d56LLM", "result": "\u5728PandasBench\u6d4b\u8bd5\u4e2d\uff1a\u76f8\u6bd4\u7f16\u8bd1\u5668\u6700\u4f18\u65b9\u6848Dias\u52a0\u901f4.3\u500d\uff0c\u8d85\u8d8a\u7cfb\u7edf\u6700\u4f18\u65b9\u6848Modin 1914.9\u500d\uff0c\u6210\u4e3a\u65b0SOTA\u6846\u67b6", "conclusion": "RuleFlow\u901a\u8fc7\u5206\u79bb\u4f18\u5316\u53d1\u73b0\u4e0e\u90e8\u7f72\u6d41\u7a0b\u5e76\u5efa\u7acb\u89c4\u5219\u8f6c\u5316\u673a\u5236\uff0c\u5e73\u8861\u4e86\u4f18\u5316\u80fd\u529b\u4e0e\u6267\u884c\u6548\u7387\uff0c\u4e3aPandas\u7a0b\u5e8f\u63d0\u4f9b\u4e86\u53ef\u9760\u4e14\u9ad8\u6548\u7684\u81ea\u52a8\u5316\u4f18\u5316\u65b9\u6848"}}
{"id": "2602.09174", "categories": ["cs.DC", "cs.AR"], "pdf": "https://arxiv.org/pdf/2602.09174", "abs": "https://arxiv.org/abs/2602.09174", "authors": ["Marzieh Barkhordar", "Alireza Tabatabaeian", "Mohammad Sadrosadati", "Christina Giannoula", "Juan Gomez Luna", "Izzat El Hajj", "Onur Mutlu", "Alaa R. Alameldeen"], "title": "ALPHA-PIM: Analysis of Linear Algebraic Processing for High-Performance Graph Applications on a Real Processing-In-Memory System", "comment": null, "summary": "Processing large-scale graph datasets is computationally intensive and time-consuming. Processor-centric CPU and GPU architectures, commonly used for graph applications, often face bottlenecks caused by extensive data movement between the processor and memory units due to low data reuse. As a result, these applications are often memory-bound, limiting both performance and energy efficiency due to excessive data transfers. Processing-In-Memory (PIM) offers a promising approach to mitigate data movement bottlenecks by integrating computation directly within or near memory. Although several previous studies have introduced custom PIM proposals for graph processing, they do not leverage real-world PIM systems.\n  This work aims to explore the capabilities and characteristics of common graph algorithms on a real-world PIM system to accelerate data-intensive graph workloads. To this end, we (1) implement representative graph algorithms on UPMEM's general-purpose PIM architecture; (2) characterize their performance and identify key bottlenecks; (3) compare results against CPU and GPU baselines; and (4) derive insights to guide future PIM hardware design.\n  Our study underscores the importance of selecting optimal data partitioning strategies across PIM cores to maximize performance. Additionally, we identify critical hardware limitations in current PIM architectures and emphasize the need for future enhancements across computation, memory, and communication subsystems. Key opportunities for improvement include increasing instruction-level parallelism, developing improved DMA engines with non-blocking capabilities, and enabling direct interconnection networks among PIM cores to reduce data transfer overheads.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u5728UPMEM\u771f\u5b9e\u5185\u5b58\u8ba1\u7b97\u7cfb\u7edf\u4e0a\u8fd0\u884c\u56fe\u7b97\u6cd5\u7684\u6027\u80fd\uff0c\u4e0eCPU/GPU\u5bf9\u6bd4\uff0c\u63ed\u793a\u786c\u4ef6\u9650\u5236\u5e76\u6307\u660e\u4f18\u5316\u65b9\u5411", "motivation": "\u4f20\u7edfCPU/GPU\u5904\u7406\u56fe\u6570\u636e\u65f6\u5b58\u5728\u5185\u5b58\u74f6\u9888\uff0c\u5185\u5b58\u8ba1\u7b97(PIM)\u867d\u80fd\u7f13\u89e3\u4f46\u7f3a\u4e4f\u771f\u5b9e\u7cfb\u7edf\u9a8c\u8bc1", "method": "\u5728UPMEM\u7684PIM\u67b6\u6784\u4e0a\u5b9e\u73b0\u5178\u578b\u56fe\u7b97\u6cd5\uff0c\u5206\u6790\u6027\u80fd\u74f6\u9888\u5e76\u4e0eCPU/GPU\u57fa\u51c6\u5bf9\u6bd4", "result": "\u53d1\u73b0\u4f18\u5316PIM\u6838\u5fc3\u95f4\u6570\u636e\u5212\u5206\u81f3\u5173\u91cd\u8981\uff0c\u6307\u51fa\u786c\u4ef6\u9700\u63d0\u5347\u6307\u4ee4\u7ea7\u5e76\u884c\u3001\u6539\u8fdbDMA\u5f15\u64ce\u3001\u5efa\u7acb\u6838\u5fc3\u95f4\u76f4\u63a5\u4e92\u8054", "conclusion": " upgraded\u672a\u6765\u7684PIM\u786c\u4ef6\u8bbe\u8ba1\u9700\u5f3a\u5316\u8ba1\u7b97/\u5b58\u50a8/\u901a\u4fe1\u5b50\u7cfb\u7edf\uff0c\u91cd\u70b9\u964d\u4f4e\u6570\u636e\u4f20\u8f93\u5f00\u9500"}}
{"id": "2602.09379", "categories": ["cs.MA", "cs.CL"], "pdf": "https://arxiv.org/pdf/2602.09379", "abs": "https://arxiv.org/abs/2602.09379", "authors": ["Shihao Xu", "Tiancheng Zhou", "Jiatong Ma", "Yanli Ding", "Yiming Yan", "Ming Xiao", "Guoyi Li", "Haiyang Geng", "Yunyun Han", "Jianhua Chen", "Yafeng Deng"], "title": "LingxiDiagBench: A Multi-Agent Framework for Benchmarking LLMs in Chinese Psychiatric Consultation and Diagnosis", "comment": null, "summary": "Mental disorders are highly prevalent worldwide, but the shortage of psychiatrists and the inherent subjectivity of interview-based diagnosis create substantial barriers to timely and consistent mental-health assessment. Progress in AI-assisted psychiatric diagnosis is constrained by the absence of benchmarks that simultaneously provide realistic patient simulation, clinician-verified diagnostic labels, and support for dynamic multi-turn consultation. We present LingxiDiagBench, a large-scale multi-agent benchmark that evaluates LLMs on both static diagnostic inference and dynamic multi-turn psychiatric consultation in Chinese. At its core is LingxiDiag-16K, a dataset of 16,000 EMR-aligned synthetic consultation dialogues designed to reproduce real clinical demographic and diagnostic distributions across 12 ICD-10 psychiatric categories. Through extensive experiments across state-of-the-art LLMs, we establish key findings: (1) although LLMs achieve high accuracy on binary depression--anxiety classification (up to 92.3%), performance deteriorates substantially for depression--anxiety comorbidity recognition (43.0%) and 12-way differential diagnosis (28.5%); (2) dynamic consultation often underperforms static evaluation, indicating that ineffective information-gathering strategies significantly impair downstream diagnostic reasoning; (3) consultation quality assessed by LLM-as-a-Judge shows only moderate correlation with diagnostic accuracy, suggesting that well-structured questioning alone does not ensure correct diagnostic decisions. We release LingxiDiag-16K and the full evaluation framework to support reproducible research at https://github.com/Lingxi-mental-health/LingxiDiagBench.", "AI": {"tldr": "\u9488\u5bf9\u7cbe\u795e\u75be\u75c5\u8bca\u65ad\u4e2dAI\u5e94\u7528\u74f6\u9888\uff0c\u7814\u7a76\u8005\u63a8\u51faLingxiDiagBench\u57fa\u51c6\u6d4b\u8bd5\u548cLingxiDiag-16K\u5408\u6210\u5bf9\u8bdd\u6570\u636e\u96c6\uff0c\u8bc4\u4f30LLM\u5728\u8bca\u65ad\u4e2d\u7684\u8868\u73b0\uff0c\u53d1\u73b0\u4e8c\u5143\u5206\u7c7b\u6548\u679c\u4f73\u4f46\u590d\u6742\u573a\u666f\u80fd\u529b\u4e0d\u8db3\uff0c\u52a8\u6001\u54a8\u8be2\u6548\u7387\u4f4e\u4e0b\u4e14\u63d0\u95ee\u8d28\u91cf\u4e0e\u8bca\u65ad\u51c6\u786e\u5ea6\u5173\u8054\u6709\u9650\u3002", "motivation": "\u5168\u7403\u7cbe\u795e\u969c\u788d\u9ad8\u53d1\uff0c\u4f46\u7cbe\u795e\u79d1\u533b\u751f\u77ed\u7f3a\u548c\u4e3b\u89c2\u8bca\u65ad\u5bfc\u81f4\u8bc4\u4f30\u5ef6\u8bef\u3002AI\u8f85\u52a9\u8bca\u65ad\u7f3a\u4e4f\u540c\u65f6\u652f\u6301\u771f\u5b9e\u60a3\u8005\u6a21\u62df\u3001\u4e34\u5e8a\u8ba4\u8bc1\u6807\u7b7e\u53ca\u52a8\u6001\u5bf9\u8bdd\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u963b\u788d\u8fdb\u5c55\u3002", "method": "\u6784\u5efaLingxiDiagBench\uff0c\u5305\u542b16,000\u6761EMR\u5bf9\u9f50\u7684\u5408\u6210\u54a8\u8be2\u5bf9\u8bdd\u6570\u636e\u96c6LingxiDiag-16K\uff0c\u8986\u76d612\u7c7bICD-10\u75be\u75c5\uff1b\u8bc4\u4f30LLM\u5728\u9759\u6001\u8bca\u65ad\u63a8\u7406\u548c\u52a8\u6001\u591a\u8f6e\u54a8\u8be2\u7684\u6027\u80fd\uff0c\u5e76\u8fdb\u884c\u5e7f\u6cdb\u5b9e\u9a8c\u6bd4\u8f83\u6700\u65b0LLM\u3002", "result": "LLM\u5728\u6291\u90c1-\u7126\u8651\u4e8c\u5143\u5206\u7c7b\u51c6\u786e\u7387\u8fbe92.3%\uff0c\u4f46\u5171\u75c5\u8bc6\u522b\u4ec543.0%\u300112\u7c7b\u9274\u522b\u8bca\u65ad\u4ec528.5%\uff1b\u52a8\u6001\u54a8\u8be2\u6548\u679c\u4e0d\u5982\u9759\u6001\u8bc4\u4f30\uff0c\u663e\u793a\u4fe1\u606f\u6536\u96c6\u7b56\u7565\u65e0\u6548\uff1bLLM\u8bc4\u4f30\u7684\u54a8\u8be2\u8d28\u91cf\u4e0e\u8bca\u65ad\u51c6\u786e\u5ea6\u4ec5\u4e2d\u5ea6\u76f8\u5173\uff0c\u826f\u597d\u7ed3\u6784\u63d0\u95ee\u4e0d\u8db3\u4ee5\u4fdd\u8bc1\u6b63\u786e\u8bca\u65ad\u3002", "conclusion": "LLM\u5728\u7cbe\u795e\u75be\u75c5\u8bca\u65ad\u4e2d\u9762\u4e34\u590d\u6742\u573a\u666f\u6311\u6218\uff0c\u9700\u4f18\u5316\u4fe1\u606f\u6536\u96c6\u65b9\u6cd5\uff1b\u53d1\u5e03\u6570\u636e\u96c6\u548c\u8bc4\u4f30\u6846\u67b6\u4fc3\u8fdb\u53ef\u590d\u73b0\u7814\u7a76\uff0c\u5f3a\u8c03\u672a\u6765\u5e94\u63d0\u5347\u6a21\u578b\u5728\u5904\u7406\u5171\u75c5\u548c\u591a\u8f6e\u4e92\u52a8\u4e2d\u7684\u80fd\u529b\u3002"}}
{"id": "2602.09410", "categories": ["cs.AR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.09410", "abs": "https://arxiv.org/abs/2602.09410", "authors": ["Yuchao Liao", "Tosiron Adegbija", "Roman Lysecky"], "title": "Accelerating Post-Quantum Cryptography via LLM-Driven Hardware-Software Co-Design", "comment": "Accepted at the 27th International Symposium on Quality Electronic Design (ISQED 2026)", "summary": "Post-quantum cryptography (PQC) is crucial for securing data against emerging quantum threats. However, its algorithms are computationally complex and difficult to implement efficiently on hardware. In this paper, we explore the potential of Large Language Models (LLMs) to accelerate the hardware-software co-design process for PQC, with a focus on the FALCON digital signature scheme. We present a novel framework that leverages LLMs to analyze PQC algorithms, identify performance-critical components, and generate candidate hardware descriptions for FPGA implementation. We present the first quantitative comparison between LLM-driven synthesis and conventional HLS-based approaches for low-level compute-intensive kernels in FALCON, showing that human-in-the-loop LLM-generated accelerators can achieve up to 2.6x speedup in kernel execution time with shorter critical paths, while highlighting trade-offs in resource utilization and power consumption. Our results suggest that LLMs can minimize design effort and development time by automating FPGA accelerator design iterations for PQC algorithms, offering a promising new direction for rapid and adaptive PQC accelerator design on FPGAs.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\u52a0\u901f\u540e\u91cf\u5b50\u5bc6\u7801\u7684FPGA\u786c\u4ef6\u8bbe\u8ba1\u6846\u67b6\uff0c\u9488\u5bf9FALCON\u7b7e\u540d\u65b9\u6848\u5b9e\u73b0\u8f6f\u786c\u4ef6\u534f\u540c\u8bbe\u8ba1\u81ea\u52a8\u5316\uff0c\u5b9e\u9a8c\u663e\u793a\u6027\u80fd\u663e\u8457\u63d0\u5347\u3002", "motivation": "\u540e\u91cf\u5b50\u5bc6\u7801\u7b97\u6cd5\u8ba1\u7b97\u590d\u6742\uff0c\u786c\u4ef6\u5b9e\u73b0\u6548\u7387\u4f4e\u4e0b\uff0c\u96be\u4ee5\u5e94\u5bf9\u91cf\u5b50\u5a01\u80c1\uff0c\u9700\u521b\u65b0\u65b9\u6cd5\u52a0\u901f\u8bbe\u8ba1\u4e0e\u4f18\u5316\u3002", "method": "\u5f00\u53d1\u65b0\u578b\u6846\u67b6\uff0c\u5229\u7528LLM\u5206\u6790PQC\u7b97\u6cd5\u7ed3\u6784\uff0c\u8bc6\u522b\u6027\u80fd\u74f6\u9888\u7ec4\u4ef6\uff0c\u81ea\u52a8\u5316\u751f\u6210FPGA\u786c\u4ef6\u63cf\u8ff0\u5019\u9009\u65b9\u6848\uff0c\u5e76\u4e0e\u4f20\u7edfHLS\u65b9\u6cd5\u5728FALCON\u5185\u6838\u4e0a\u8fdb\u884c\u6bd4\u8f83\u3002", "result": "LLM\u9a71\u52a8\u5408\u6210\u7684\u52a0\u901f\u5668\u5185\u6838\u6267\u884c\u65f6\u95f4\u63d0\u901f\u6700\u9ad8\u8fbe2.6\u500d\uff0c\u4e34\u754c\u8def\u5f84\u7f29\u77ed\uff0c\u4f46\u8d44\u6e90\u5229\u7528\u7387\u548c\u529f\u8017\u5b58\u5728\u6743\u8861\uff0c\u9700\u4eba\u5de5\u5e72\u9884\u4f18\u5316\u3002", "conclusion": "LLM\u53ef\u5927\u5e45\u51cf\u5c11\u8bbe\u8ba1\u8fed\u4ee3\u65f6\u95f4\u548c\u5f00\u53d1\u6210\u672c\uff0c\u81ea\u52a8\u5316FPGA\u52a0\u901f\u5668\u8bbe\u8ba1\uff0c\u4e3a\u540e\u91cf\u5b50\u5bc6\u7801\u63d0\u4f9b\u5feb\u901f\u9002\u5e94\u6027\u7684\u65b0\u65b9\u5411\uff0c\u63d0\u5347\u5b89\u5168\u6027\u6548\u7387\u3002"}}
{"id": "2602.09188", "categories": ["cs.NI", "cs.DC"], "pdf": "https://arxiv.org/pdf/2602.09188", "abs": "https://arxiv.org/abs/2602.09188", "authors": ["Mahir Rahman", "Samuel Joseph", "Nihar Kodkani", "Behnaz Arzani", "Vamsi Addanki"], "title": "Harvest: Adaptive Photonic Switching Schedules for Collective Communication in Scale-up Domains", "comment": null, "summary": "As chip-to-chip silicon photonics gain traction for their bandwidth and energy efficiency, their circuit-switched nature raises a fundamental question for collective communication: when and how should the interconnect be reconfigured to realize these benefits? Establishing direct optical paths can reduce congestion and propagation delay, but each reconfiguration incurs non-negligible overhead, making naive per-step reconfiguration impractical.\n  We present Harvest, a systematic approach for synthesizing topology reconfiguration schedules that minimize collective completion time in photonic interconnects. Given a collective communication algorithm and its fixed communication schedule, Harvest determines how the interconnect should evolve over the course of the collective, explicitly balancing reconfiguration delay against congestion and propagation delay. We reduce the synthesis problem into a dynamic program with an underlying topology optimization subproblem and show that the approach applies to arbitrary collective communication algorithms. Furthermore, we exploit the algorithmic structure of a well-known AllReduce algorithm (Recursive Doubling) to synthesize optimal reconfiguration schedules without using any optimizers. By parameterizing the formulation using reconfiguration delay, Harvest naturally adapts to various photonic technologies. Using packet-level and flow-level evaluations, as well as hardware emulation on commercial GPUs, we show that the schedules synthesized by Harvest significantly reduce collective completion time across multiple collective algorithms compared to static interconnects and reconfigure-every-step baselines.", "AI": {"tldr": "Harvest\u662f\u4e00\u79cd\u5408\u6210\u5149\u5b50\u4e92\u8fde\u62d3\u6251\u91cd\u65b0\u914d\u7f6e\u8c03\u5ea6\u7684\u7cfb\u7edf\u65b9\u6cd5\uff0c\u65e8\u5728\u6700\u5c0f\u5316\u96c6\u4f53\u901a\u4fe1\u5b8c\u6210\u65f6\u95f4\u3002", "motivation": "\u5149\u5b50\u4e92\u8fde\u5177\u6709\u9ad8\u5e26\u5bbd\u548c\u80fd\u6548\uff0c\u4f46\u6bcf\u6b21\u91cd\u65b0\u914d\u7f6e\u4ea7\u751f\u663e\u8457\u5f00\u9500\uff0c\u5bfc\u81f4\u6bcf\u6b21\u6b65\u9aa4\u90fd\u91cd\u65b0\u914d\u7f6e\u5728\u9ad8\u6548\u96c6\u4f53\u901a\u4fe1\u4e2d\u4e0d\u5207\u5b9e\u9645\u3002", "method": "\u5229\u7528\u52a8\u6001\u89c4\u5212\u6846\u67b6\u7ed3\u5408\u62d3\u6251\u4f18\u5316\u5b50\u95ee\u9898\uff0c\u5408\u6210\u8c03\u5ea6\u5e73\u8861\u91cd\u65b0\u914d\u7f6e\u5ef6\u8fdf\u3001\u62e5\u585e\u548c\u4f20\u64ad\u5ef6\u8fdf\uff1b\u5e76\u9488\u5bf9\u9012\u5f52\u52a0\u500d\u7b97\u6cd5\u5185\u90e8\u4f18\u5316\u751f\u6210\u6700\u4f18\u8c03\u5ea6\uff0c\u65e0\u9700\u5916\u90e8\u4f18\u5316\u5668\u3002", "result": "\u901a\u8fc7\u5728\u5305\u7ea7\u3001\u6d41\u7ea7\u8bc4\u4f30\u53caGPU\u786c\u4ef6\u4eff\u771f\u6d4b\u8bd5\u4e2d\uff0c\u76f8\u6bd4\u9759\u6001\u4e92\u8fde\u548c\u6bcf\u6b21\u6b65\u9aa4\u91cd\u65b0\u914d\u7f6e\u57fa\u51c6\uff0c\u663e\u8457\u51cf\u5c11\u591a\u79cd\u96c6\u4f53\u7b97\u6cd5\u7684\u5b8c\u6210\u65f6\u95f4\u3002", "conclusion": "Harvest\u80fd\u81ea\u9002\u5e94\u4e0d\u540c\u5149\u5b50\u6280\u672f\uff0c\u6709\u6548\u63d0\u5347\u96c6\u4f53\u901a\u4fe1\u6548\u7387\uff0c\u5e76\u51cf\u5c11\u5b8c\u6210\u65f6\u95f4\u3002"}}
{"id": "2602.09064", "categories": ["cs.SE", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.09064", "abs": "https://arxiv.org/abs/2602.09064", "authors": ["S M Rakib Ul Karim", "Wenyi Lu", "Enock Kasaadha", "Sean Goggins"], "title": "Predicting Open Source Software Sustainability with Deep Temporal Neural Hierarchical Architectures and Explainable AI", "comment": null, "summary": "Open Source Software (OSS) projects follow diverse lifecycle trajectories shaped by evolving patterns of contribution, coordination, and community engagement. Understanding these trajectories is essential for stakeholders seeking to assess project organization and health at scale. However, prior work has largely relied on static or aggregated metrics, such as project age or cumulative activity, providing limited insight into how OSS sustainability unfolds over time. In this paper, we propose a hierarchical predictive framework that models OSS projects as belonging to distinct lifecycle stages grounded in established socio-technical categorizations of OSS development. Rather than treating sustainability solely as project longevity, these lifecycle stages operationalize sustainability as a multidimensional construct integrating contribution activity, community participation, and maintenance dynamics. The framework combines engineered tabular indicators with 24-month temporal activity sequences and employs a multi-stage classification pipeline to distinguish lifecycle stages associated with different coordination and participation regimes. To support transparency, we incorporate explainable AI techniques to examine the relative contribution of feature categories to model predictions. Evaluated on a large corpus of OSS repositories, the proposed approach achieves over 94\\% overall accuracy in lifecycle stage classification. Attribution analyses consistently identify contribution activity and community-related features as dominant signals, highlighting the central role of collective participation dynamics.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u4e2a\u5206\u5c42\u9884\u6d4b\u6846\u67b6\uff0c\u5efa\u6a21\u5f00\u653e\u6e90\u7801\u8f6f\u4ef6\u9879\u76ee\u7684\u751f\u547d\u5468\u671f\u9636\u6bb5\uff0c\u7ed3\u5408\u591a\u7ef4\u6307\u6807\u548c\u53ef\u89e3\u91caAI\u6280\u672f\uff0c\u5b9e\u73b0\u9ad8\u7cbe\u5ea6\u5206\u7c7b\u5e76\u5f3a\u8c03\u8d21\u732e\u548c\u793e\u533a\u7684\u52a8\u6001\u4f5c\u7528\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u4f9d\u8d56\u9879\u76ee\u5e74\u9f84\u6216\u7d2f\u8ba1\u6d3b\u52a8\u7b49\u9759\u6001\u6307\u6807\uff0c\u65e0\u6cd5\u63ed\u793aOSS\u9879\u76ee\u968f\u65f6\u95f4\u7684\u53ef\u6301\u7eed\u53d1\u5c55\u8f68\u8ff9\uff1b\u4e9f\u9700\u8bc4\u4f30\u5176\u5065\u5eb7\u548c\u52a8\u6001\u7ec4\u7ec7\uff0c\u4ee5\u652f\u6301\u5229\u76ca\u76f8\u5173\u8005\u8fdb\u884c\u89c4\u6a21\u5316\u7ba1\u7406\u3002", "method": "\u4f7f\u7528\u5206\u5c42\u6846\u67b6\uff0c\u57fa\u4e8e\u793e\u4f1a\u6280\u672f\u5206\u7c7b\u5c06OSS\u9879\u76ee\u5212\u5206\u4e3a\u4e0d\u540c\u751f\u547d\u5468\u671f\u9636\u6bb5\uff1b\u6574\u5408\u5de5\u7a0b\u5316\u8868\u683c\u6307\u6807\u4e0e24\u4e2a\u6708\u65f6\u95f4\u5e8f\u5217\u6570\u636e\uff0c\u91c7\u7528\u5206\u7c7b\u7ba1\u9053\u5e76\u878d\u5165\u53ef\u89e3\u91caAI\u5206\u6790\u7279\u5f81\u8d21\u732e\u3002", "result": "\u5728\u5927\u89c4\u6a21OSS\u4ed3\u5e93\u8bed\u6599\u5e93\u8bc4\u4f30\u4e2d\uff0c\u751f\u547d\u5468\u671f\u9636\u6bb5\u5206\u7c7b\u51c6\u786e\u7387\u8d85\u8fc794%\uff1b\u7279\u5f81\u5f52\u56e0\u5206\u6790\u4e00\u81f4\u8868\u660e\u8d21\u732e\u6d3b\u52a8\u548c\u793e\u533a\u76f8\u5173\u7279\u5f81\u4e3a\u4e3b\u5bfc\u4fe1\u53f7\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u7a81\u51fa\u4e86\u96c6\u4f53\u53c2\u4e0e\u52a8\u6001\u5728OSS\u53ef\u6301\u7eed\u6027\u4e2d\u7684\u6838\u5fc3\u89d2\u8272\uff0c\u63a8\u52a8\u4e86\u591a\u7ef4\u5ea6\u9879\u76ee\u7ec4\u7ec7\u7684\u7406\u89e3\uff0c\u4e3a\u5b9e\u8df5\u63d0\u4f9b\u4e86\u900f\u660e\u4e14\u9ad8\u6548\u7684\u8bc4\u4f30\u5de5\u5177\u3002"}}
{"id": "2602.09323", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2602.09323", "abs": "https://arxiv.org/abs/2602.09323", "authors": ["Jie Kong", "Wei Wang", "Jiehan Zhou", "Chen Yu"], "title": "LLM-CoOpt: A Co-Design and Optimization Framework for Efficient LLM Inference on Heterogeneous Platforms", "comment": null, "summary": "Major challenges in LLMs inference remain frequent memory bandwidth bottlenecks, computational redundancy, and inefficiencies in long-sequence processing. To address these issues, we propose LLM-CoOpt, a comprehensive algorithmhardware co-design framework aimed at improving both throughput and latency in LLM inference. LLM-CoOpt integrates three key strategies: (1) Key-Value Cache Optimization, termed Opt-KV, which improves memory access efficiency by optimizing both KV cache write and read paths, and introduces FP8 quantization to reduce memory footprint while maintaining accuracy; (2) Grouped-Query Attention for Computational Efficiency, termed Opt-GQA, which reduces the overall computational complexity by restructuring multi-head self-attention into grouped-query attention with shared key-value projections, enabling higher throughput and lower resource consumption; (3) Paged Attention for Long- Sequence Processing, termed Opt-Pa, which adopts a two-step strategy to first segment long sequences into manageable chunks and then apply lazy memory mapping and computation, significantly reducing memory pressure and improving performance on long-context inputs.Experiments on the LLaMa-13BGPTQ model demonstrate that LLM-CoOpt increases inference throughput by up to 13.43%, reduces latency by up to 16.79%, and maintains model accuracy. These results confirm that LLM-CoOpt provides a practical, high-performance optimization path for real-world inference of large-scale language models.", "AI": {"tldr": "\u63d0\u51faLLM-CoOpt\u6846\u67b6\uff0c\u901a\u8fc7\u7b97\u6cd5\u786c\u4ef6\u534f\u540c\u8bbe\u8ba1\u4f18\u5316\u5927\u578b\u8bed\u8a00\u6a21\u578b\u63a8\u7406\u6027\u80fd\uff0c\u5305\u62ec\u952e\u503c\u7f13\u5b58\u4f18\u5316\u3001\u5206\u7ec4\u67e5\u8be2\u6ce8\u610f\u529b\u548c\u5206\u9875\u6ce8\u610f\u529b\u7b56\u7565\uff0c\u63d0\u5347\u541e\u5410uture\u80fd\u529b\u91cf\u5e76\u964d\u4f4e\u5ef6\u8fdf\u3002", "motivation": "\u89e3\u51b3\u5927\u578b\u8bed\u8a00\u6a21\u578b\u63a8\u7406\u4e2d\u7684\u5185\u5b58\u5e26\u5bbd\u74f6\u9888\u3001\u8ba1\u7b97\u5197\u4f59\u548c\u957f\u5e8f\u5217\u5904\u7406\u6548\u7387\u4f4e\u4e0b\u95ee\u9898\u3002", "method": "\u96c6\u6210\u4e09\u5927\u7b56\u7565\uff1aOpt-KV\u4f18\u5316KV\u7f13\u5b58\u8bfb\u5199\u8def\u5f84\u5e76\u4f7f\u7528FP8\u91cf\u5316\u51cf\u5c11\u5185\u5b58\u5360\u7528\uff1bOpt-GQA\u91cd\u6784\u67e5\u8be2\u5206\u7ec4\u6ce8\u610f\u529b\u964d\u4f4e\u8ba1\u7b97\u590d\u6742\u5ea6\uff1bOpt-Pa\u5206\u5757\u7ba1\u7406\u957f\u5e8f\u5217\u5e76\u901a\u8fc7\u61d2\u8ba1\u7b97\u7f13\u89e3\u5185\u5b58\u538b\u529b\u3002", "result": "\u5728LLaMa-13BGPTQ\u6a21\u578b\u4e0a\u5b9e\u9a8c\u663e\u793a\uff1a\u63a8\u7406\u541e\u5410\u91cf\u6700\u9ad8\u63d0\u534713.43%\uff0c\u5ef6\u8fdf\u964d\u4f4e16.79%\uff0c\u6a21\u578b\u51c6\u786e\u6027\u7ef4\u6301\u4e0d\u53d8\u3002", "conclusion": "LLM-CoOpt\u4e3a\u5927\u89c4\u6a21\u8bed\u8a00\u6a21\u578b\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u4e86\u9ad8\u6548\u5b9e\u7528\u7684\u4f18\u5316\u8def\u5f84\u3002"}}
{"id": "2602.09412", "categories": ["cs.MA", "math.OC"], "pdf": "https://arxiv.org/pdf/2602.09412", "abs": "https://arxiv.org/abs/2602.09412", "authors": ["Minh Hoang Trinh", "Hieu Minh Nguyen"], "title": "Dieu khien he da tac tu", "comment": "\"Multi-Agent System Control\", 252 pages; in Vietnamese language, 82 figures", "summary": "Since the early 2000s, control of multiagent systems has attracted significant research interest, with applications ranging from natural collective behaviors and social dynamics to engineered systems such as autonomous vehicles, sensor networks, and smart grids. Although research on multi-agent systems has diversified into numerous specialized directions, textbooks -- including those in English -- that provide a systematic treatment of the fundamental principles of multi-agent system control remain scarce. The material presented in this book has been developed and used in teaching since 2021, initially as a concise Vietnamese-language reference for the courses Networked Control Systems and Control of Multi-Agent Systems at Hanoi University of Science and Technology. The book focuses on a selection of fundamental topics of broad and continuing interest in the field. The complexity of several topics is asymptotic to that encountered in research-level studies, however, the analysis is presented in a step-by-step manner to facilitate access to commonly used methods and tools.\n  The material is divided into three main parts. Part I introduces multiagent systems and basic graph-theoretic concepts. Part II addresses the design and analysis of linear consensus algorithms. Part III covers selected applications and research directions, including formation control, network localization, distributed optimization, opinion dynamics, and matrix-weighted networks. Each chapter concludes with notes on notable researchers in this field, further reading, and exercises.\n  This book cannot be completed without the encouragement, support and suggestions from families, colleagues and friends. The authors appreciate feedback from readers to further improve the content of the book.", "AI": {"tldr": "\u672c\u4e66\u7cfb\u7edf\u4ecb\u7ecd\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u63a7\u5236\u57fa\u7840\u539f\u7406\uff0c\u586b\u8865\u8be5\u9886\u57df\u6559\u6750\u7a7a\u7f3a\u3002\u5185\u5bb9\u6e90\u81ea\u6cb3\u5185\u7406\u5de5\u5927\u5b66\u6559\u5b66\u5b9e\u8df5\uff0c\u6db5\u76d6\u56fe\u8bba\u57fa\u7840\u3001\u7ebf\u6027\u5171\u8bc6\u7b97\u6cd5\u53ca\u5e94\u7528\u7814\u7a76\u4e09\u5927\u90e8\u5206\u3002", "motivation": "\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u63a7\u5236\u7814\u7a76\u867d\u53d1\u5c55\u8fc5\u901f\u4e14\u5e94\u7528\u5e7f\u6cdb\uff0c\u4f46\u7cfb\u7edf\u5316\u9610\u8ff0\u57fa\u7840\u539f\u7406\u7684\u6559\u6750\u532e\u4e4f\uff0c\u5c24\u5176\u975e\u82f1\u8bed\u8d44\u6e90\u7a00\u7f3a\u3002\u672c\u4e66\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u5e76\u63d0\u4f9b\u6559\u5b66\u652f\u6301\u3002", "method": "\u91c7\u7528\u6e10\u8fdb\u5f0f\u6559\u5b66\u6cd5\uff1a\u5148\u901a\u8fc7\u56fe\u8bba\u5f15\u5165\u7cfb\u7edf\u57fa\u7840\uff1b\u518d\u89e3\u6790\u7ebf\u6027\u5171\u8bc6\u7b97\u6cd5\u8bbe\u8ba1\u4e0e\u5206\u6790\uff1b\u6700\u540e\u63a2\u8ba8\u7f16\u961f\u63a7\u5236\u3001\u5206\u5e03\u5f0f\u4f18\u5316\u7b49\u5e94\u7528\u3002\u5404\u7ae0\u9644\u7814\u7a76\u6ce8\u91ca\u3001\u4e60\u9898\u53ca\u5ef6\u4f38\u9605\u8bfb\u3002", "result": "\u5f62\u6210\u5206\u5c42\u6559\u6750\u6846\u67b6\uff0c\u7406\u8bba\u6df1\u5ea6\u8d8b\u8fd1\u7814\u7a76\u6c34\u5e73\u4f46\u4fdd\u6301\u9010\u6b65\u89e3\u6790\u7684\u6613\u61c2\u6027\u3002\u5df2\u5728\u6cb3\u5185\u7406\u5de5\u5927\u5b66\u8bfe\u7a0b\u4e2d\u9a8c\u8bc1\u6559\u5b66\u6709\u6548\u6027\uff0c\u8986\u76d6\u6301\u7eed\u70ed\u70b9\u7684\u6838\u5fc3\u8bae\u9898\u3002", "conclusion": "\u672c\u4e66\u4e3a\u591a\u667a\u80fd\u4f53\u63a7\u5236\u9886\u57df\u63d0\u4f9b\u7cfb\u7edf\u6027\u6559\u5b66\u8d44\u6e90\uff0c\u672a\u6765\u5c06\u901a\u8fc7\u8bfb\u8005\u53cd\u9988\u6301\u7eed\u4f18\u5316\u3002\u5f3a\u8c03\u4ece\u539f\u7406\u5230\u5e94\u7528\u7684\u5b8c\u6574\u77e5\u8bc6\u94fe\u6761\u6784\u5efa\u3002"}}
{"id": "2602.09554", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2602.09554", "abs": "https://arxiv.org/abs/2602.09554", "authors": ["Thomas Benz"], "title": "Development of an Energy-Efficient and Real-Time Data Movement Strategy for Next-Generation Heterogeneous Mixed-Criticality Systems", "comment": "Doctoral Thesis", "summary": "Industrial domains such as automotive, robotics, and aerospace are rapidly evolving to satisfy the increasing demand for machine-learning-driven Autonomy, Connectivity, Electrification, and Shared mobility (ACES). This paradigm shift inherently and significantly increases the requirement for onboard computing performance and high-performance communication infrastructure. At the same time, Moore's Law and Dennard Scaling are grinding to a halt, in turn, driving computing systems to larger scales and higher levels of heterogeneity and specialization, through application-specific hardware accelerators, instead of relying on technological scaling only. Approaching ACES requires this substantial amount of compute at an increasingly high energy-efficiency, since most use cases are fundamentally resource-bound. This increase in compute performance and heterogeneity goes hand in hand with a growing demand for high memory bandwidth and capacity as the driving applications grow in complexity, operating on huge and progressively irregular data sets and further requiring a steady influx of sensor data, increasing pressure both on on-chip and off-chip interconnect systems. Further, ACES combines real-time time-critical with general compute tasks on the same physical platform, sharing communication, storage, and micro-architectural resources. These heterogeneous mixed-criticality systems (MCSs) place additional pressure on the interconnect, demanding minimal contention between the different criticality levels to sustain a high degree of predictability. Fulfilling the performance and energy-efficiency requirements across a wide range of industrial applications requires a carefully co-designed process of the memory system with the use cases as well as the compute units and accelerators.", "AI": {"tldr": "\u5de5\u4e1aACES\u9886\u57df\u9762\u4e34\u6469\u5c14\u5b9a\u5f8b\u505c\u6ede\u7684\u6311\u6218\uff0c\u9700\u901a\u8fc7\u5f02\u6784\u52a0\u901f\u5668\u548c\u5185\u5b58\u534f\u540c\u8bbe\u8ba1\u5b9e\u73b0\u9ad8\u6548\u80fd\u8ba1\u7b97\u3002", "motivation": "\u6c7d\u8f66\u3001\u673a\u5668\u4eba\u4e0e\u822a\u5929\u7b49\u5de5\u4e1a\u9886\u57df\u5bf9\u81ea\u4e3b\u6027\u3001\u8fde\u63a5\u6027\u7b49\u9700\u6c42\u6fc0\u589e\uff0c\u4f46\u786c\u4ef6\u6280\u672f\u7f29\u653e\u53d7\u9650\uff0c\u5bfc\u81f4\u8ba1\u7b97\u8d44\u6e90\u538b\u529b\u589e\u5927\uff0c\u4e9f\u9700\u63d0\u5347\u80fd\u6548\u5e76\u89e3\u51b3\u6df7\u5408\u5173\u952e\u6027\u7cfb\u7edf\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u5185\u5b58\u7cfb\u7edf\u4e0e\u7528\u4f8b\u3001\u8ba1\u7b97\u5355\u5143\u53ca\u52a0\u901f\u5668\u7684\u534f\u540c\u8bbe\u8ba1\u65b9\u6848\uff0c\u4f18\u5316\u8d44\u6e90\u5206\u914d\u4ee5\u51cf\u5c11\u5173\u952e\u4efb\u52a1\u95f4\u7684\u8d44\u6e90\u7ade\u4e89\u3002", "result": "\u8be5\u8bbe\u8ba1\u9884\u671f\u63d0\u5347\u8ba1\u7b97\u6548\u80fd\u4e0e\u80fd\u6548\uff0c\u652f\u6491\u5927\u89c4\u6a21\u4e0d\u89c4\u5219\u6570\u636e\u5904\u7406\uff0c\u6ee1\u8db3\u6df7\u5408\u5173\u952e\u6027\u7cfb\u7edf\u5bf9\u53ef\u9884\u6d4b\u6027\u7684\u4e25\u82db\u9700\u6c42\u3002", "conclusion": "\u534f\u540c\u8bbe\u8ba1\u662f\u5e94\u5bf9ACES\u8ba1\u7b97\u6311\u6218\u7684\u6838\u5fc3\u7b56\u7565\uff0c\u987b\u6574\u5408\u5185\u5b58\u3001\u8ba1\u7b97\u5355\u5143\u53ca\u7528\u4f8b\u4ee5\u5b9e\u73b0\u9ad8\u6027\u80fd\u4e0e\u9ad8\u6548\u7387\u76ee\u6807\u3002"}}
{"id": "2602.09071", "categories": ["cs.SE", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.09071", "abs": "https://arxiv.org/abs/2602.09071", "authors": ["Stefano Balla", "Stefano Zacchiroli", "Thomas Degueule", "Jean-R\u00e9my Falleri", "Romain Robbes"], "title": "DRAGON: Robust Classification for Very Large Collections of Software Repositories", "comment": null, "summary": "The ability to automatically classify source code repositories with ''topics'' that reflect their content and purpose is very useful, especially when navigating or searching through large software collections. However, existing approaches often rely heavily on README files and other metadata, which are frequently missing, limiting their applicability in real-world large-scale settings. We present DRAGON, a repository classifier designed for very large and diverse software collections. It operates entirely on lightweight signals commonly stored in version control systems: file and directory names, and optionally the README when available. In repository classification at scale, DRAGON improves F1@5 from 54.8% to 60.8%, surpassing the state of the art. DRAGON remains effective even when README files are absent, with performance degrading by only 6% w.r.t. when they are present. This robustness makes it practical for real-world settings where documentation is sparse or inconsistent. Furthermore, many of the remaining classification errors are near misses, where predicted labels are semantically close to the correct topics. This property increases the practical value of the predictions in real-world software collections, where suggesting a few related topics can still guide search and discovery. As a byproduct of developing DRAGON, we also release the largest open dataset to date for repository classification, consisting of 825 thousand repositories with associated ground-truth topics, sourced from the Software Heritage archive, providing a foundation for future large-scale and language-agnostic research on software repository understanding.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faDRAGON\u4ed3\u5e93\u5206\u7c7b\u5668\uff0c\u5229\u7528\u8f7b\u91cf\u4fe1\u53f7\u6539\u8fdb\u5927\u89c4\u6a21\u8f6f\u4ef6\u5e93\u5206\u7c7b\uff0c\u5728README\u7f3a\u5931\u4e0b\u4ecd\u6709\u6548\uff0c\u6027\u80fd\u63d0\u5347\u663e\u8457\u5e76\u53d1\u5e03\u5927\u578b\u6570\u636e\u96c6\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u8fc7\u5ea6\u4f9d\u8d56README\u6587\u4ef6\u548c\u5143\u6570\u636e\uff0c\u4f46\u8fd9\u4e9b\u5e38\u7f3a\u5931\uff0c\u5bfc\u81f4\u5927\u89c4\u6a21\u8f6f\u4ef6\u96c6\u5408\u4e2d\u7684\u5e94\u7528\u53d7\u9650\u3002", "method": "\u8bbe\u8ba1DRAGON\u5206\u7c7b\u5668\uff0c\u4ec5\u57fa\u4e8e\u7248\u672c\u63a7\u5236\u7cfb\u7edf\u4e2d\u7684\u6587\u4ef6\u548c\u76ee\u5f55\u540d\u79f0\uff08\u53ef\u9009README\uff09\uff0c\u517c\u987e\u8f7b\u91cf\u4fe1\u53f7\u3002", "result": "F1@5\u6307\u6807\u4ece54.8%\u63d0\u5347\u81f360.8%\uff0c\u4f18\u4e8e\u73b0\u6709\u6280\u672f\uff1b\u65e0README\u65f6\u6027\u80fd\u4ec5\u4e0b\u964d6%\uff1b\u9519\u8bef\u591a\u4e3a\u8bed\u4e49\u63a5\u8fd1\u7684\u8bef\u5224\uff1b\u540c\u65f6\u53d1\u5e03\u8fc4\u4eca\u6700\u5927\u5f00\u653e\u6570\u636e\u96c6\uff0882.5\u4e07\u4ed3\u5e93\uff09\u3002", "conclusion": "DRAGON\u63d0\u9ad8\u4e86\u6587\u6863\u7a00\u7f3a\u73af\u5883\u4e0b\u7684\u9c81\u68d2\u6027\uff0c\u8bef\u5224\u6807\u7b7e\u7684\u8bed\u4e49\u63a5\u8fd1\u7279\u6027\u589e\u5f3a\u4e86\u5b9e\u9645\u641c\u7d22\u4ef7\u503c\uff0c\u9002\u7528\u4e8e\u5927\u578b\u8f6f\u4ef6\u96c6\u5408\u7684\u81ea\u52a8\u5316\u7ba1\u7406\u3002"}}
{"id": "2602.09435", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2602.09435", "abs": "https://arxiv.org/abs/2602.09435", "authors": ["Joseph M. Hellerstein"], "title": "The Coordination Criterion", "comment": "10 body pages; 24 pages with appendices and references", "summary": "When is coordination intrinsically required by a distributed specification, rather than imposed by a particular protocol or implementation strategy? We give a general answer using minimal assumptions. In an asynchronous message-passing model, we show that a specification admits a coordination-free implementation if and only if it is monotone with respect to history extension under an appropriate order on observable outcomes.\n  This Coordination Criterion is stated directly over Lamport histories -- partially ordered executions under happens-before -- and specification-defined observable outcomes, without assuming any particular programming language, object implementation, or protocol structure. It yields a sharp boundary between specifications that can be implemented without coordination and those for which coordination is unavoidable. The criterion provides a uniform explanation for a range of classical results, including CAP-style impossibility, CALM-style coordination-freedom, agreement and snapshot tasks, transactional isolation levels, and invariant confluence -- all instances of the same underlying semantic phenomenon.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u534f\u8c03\u6027\u6807\u51c6\uff1a\u5f53\u5206\u5e03\u5f0f\u89c4\u8303\u5728\u5386\u53f2\u6269\u5c55\u4e0b\u5177\u6709\u5355\u8c03\u6027\u65f6\uff0c\u65b9\u53ef\u5b9e\u73b0\u65e0\u534f\u8c03\u6267\u884c\uff0c\u6b64\u6807\u51c6\u7edf\u4e00\u4e86CAP\u5b9a\u7406\u3001CALM\u6846\u67b6\u7b49\u7ecf\u5178\u7ed3\u8bba\u3002", "motivation": "\u63a2\u7a76\u5206\u5e03\u5f0f\u89c4\u8303\u7684\u534f\u8c03\u6838\u5fc3\u9700\u6c42\uff1a\u660e\u786e\u4f55\u65f6\u534f\u8c03\u662f\u89c4\u8303\u672c\u8eab\u56fa\u6709\u5c5e\u6027\uff0c\u800c\u975e\u7279\u5b9a\u534f\u8bae\u6216\u5b9e\u73b0\u7b56\u7565\u7684\u9644\u52a0\u8981\u6c42\u3002", "method": "\u57fa\u4e8e\u5f02\u6b65\u6d88\u606f\u4f20\u9012\u6a21\u578b\uff0c\u5229\u7528Lamport\u5386\u53f2\uff08happens-before\u504f\u5e8f\u6267\u884c\uff09\u548c\u89c4\u8303\u5b9a\u4e49\u7684\u89c2\u5bdf\u7ed3\u679c\uff0c\u6784\u5efa\u4e0e\u5b9e\u73b0\u65e0\u5173\u7684\u534f\u8c03\u6027\u5224\u5b9a\u6846\u67b6\u3002", "result": "\u786e\u7acb\u5145\u8981\u6761\u4ef6\uff1a\u89c4\u8303\u53ef\u65e0\u534f\u8c03\u5b9e\u73b0\u5f53\u4e14\u4ec5\u5f53\u5176\u5728\u89c2\u5bdf\u7ed3\u679c\u5e8f\u5173\u7cfb\u4e0b\u6ee1\u8db3\u5386\u53f2\u6269\u5c55\u5355\u8c03\u6027\uff0c\u4e3a\u534f\u8c03\u5b58\u5728\u6027\u5212\u5b9a\u4e25\u683c\u8fb9\u754c\u3002", "conclusion": "\u8be5\u534f\u8c03\u6807\u51c6\u63ed\u793a\u4e86CAP\u5b9a\u7406\u3001\u5171\u8bc6\u4efb\u52a1\u3001\u5feb\u7167\u7b49\u7ecf\u5178\u95ee\u9898\u672c\u8d28\u540c\u6e90\uff0c\u5747\u4e3a\u540c\u4e00\u8bed\u4e49\u73b0\u8c61\u7684\u4e0d\u540c\u5b9e\u4f8b\uff0c\u4e3a\u5206\u5e03\u5f0f\u7cfb\u7edf\u8bbe\u8ba1\u63d0\u4f9b\u7edf\u4e00\u7406\u8bba\u57fa\u77f3\u3002"}}
{"id": "2602.09801", "categories": ["cs.MA"], "pdf": "https://arxiv.org/pdf/2602.09801", "abs": "https://arxiv.org/abs/2602.09801", "authors": ["Agnieszka Dobrowolska", "Rogier Hintzen", "Martin Balla", "Karl Gemayel", "Sabine Reichert", "Thomas Charman", "Jen Ning Lim", "Lindsay Edwards", "Anna Gogleva"], "title": "Tiny Moves: Game-based Hypothesis Refinement", "comment": null, "summary": "Most machine learning approaches to scientific discovery frame hypotheses as end-to-end predictions, obscuring the incremental structure of scientific reasoning. We propose The Hypothesis Game, a symbolic formalism for hypothesis refinement in which LLM agents operate on a shared hypothesis state using a fixed grammar of reasoning moves. The framework is motivated by the observation that scientific progress often proceeds through small, localized revisions, grounded in domain context, rather than extensive rewrites. We instantiate a minimal game with LLM agents and evaluate it on pathway-level mechanistic refinement tasks. In the primary setting of corruption recovery, where hypotheses contain controlled errors, the game-based approach consistently removes more errors and achieves higher precision than strong prompting baselines, while preserving valid structure through incremental edits. In a secondary reconstruction setting from partial cues, it performs comparably to the strongest baseline, indicating that explicit move-based refinement remains competitive even when ground-truth recovery is difficult. These findings support game-based reasoning as a principled route to more controllable, interpretable, and transferable hypothesis refinement systems for scientific discovery.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2602.09493", "categories": ["cs.NI", "eess.SY"], "pdf": "https://arxiv.org/pdf/2602.09493", "abs": "https://arxiv.org/abs/2602.09493", "authors": ["Yuma Abe", "Mariko Sekiguchi", "Amane Miura"], "title": "QoS Identifier and Slice Mapping in 5G and Non-Terrestrial Network Interconnected Systems", "comment": null, "summary": "The interconnection of 5G and non-terrestrial networks (NTNs) has been actively studied to expand connectivity beyond conventional terrestrial infrastructure. In the 3GPP standardization of 5G systems, the 5G Quality of Service (QoS) Identifier (5QI) is defined to characterize the QoS requirements of different traffic requirements. However, it falls short in capturing the diverse latency, capacity, and reliability profiles of NTN environments, particularly when NTNs are used as backhaul. Furthermore, it is difficult to manage individual traffic flows and perform efficient resource allocation and routing when a large number of 5G traffic flows are present in NTN systems. To address these challenges, we propose an optimization framework that enhances QoS handling by introducing an NTN QoS Identifier (NQI) and grouping 5G traffic into NTN slices based on similar requirements. This enables unified resource control and routing for a large number of 5G flows in NTN systems. In this paper, we present the detailed procedure of the proposed framework, which consists of 5QI to NQI mapping, NTN traffic to NTN slice mapping, and slice-level flow and routing optimization. We evaluate the framework by comparing multiple mapping schemes through numerical simulations and analyze their impact on overall network performance.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2602.09633", "categories": ["cs.NI", "eess.SY"], "pdf": "https://arxiv.org/pdf/2602.09633", "abs": "https://arxiv.org/abs/2602.09633", "authors": ["Timo Oksanen"], "title": "ISO FastLane: Faster ISO 11783 with Dual Stack Approach as a Short Term Solution", "comment": null, "summary": "The agricultural industry has been searching for a high-speed successor to the 250~kbit/s CAN bus backbone of ISO~11783 (ISOBUS) for over a decade, yet no protocol-level solution has reached standardization. Meanwhile, modern planters, sprayers, and Virtual Terminals are already constrained by the bus bandwidth. This paper presents ISO FastLane, a gateway-less dual-stack approach that routes point-to-point ISOBUS traffic over Ethernet while keeping broadcast messages on the existing CAN bus. The solution requires no new state machines, no middleware, and no changes to application layer code: only a simple Layer~3 routing decision and a lightweight peer discovery mechanism called Augmented Address Claim (AACL). Legacy devices continue to operate unmodified and unaware of FastLane traffic. Preliminary tests reported on the paper demonstrate that ISO FastLane accelerates Virtual Terminal object pool uploads by factor of 8 and sustains Task Controller message rates over 100 times beyond the current specification limit. Because ISO FastLane builds entirely on existing J1939 and ISO~11783 conventions, it can be implemented by ISOBUS engineers in a matter of weeks. This is delivering tangible performance gains today, without waiting for the long-term High Speed ISOBUS solution.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2602.09292", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2602.09292", "abs": "https://arxiv.org/abs/2602.09292", "authors": ["Ana B. M. Bett", "Thais S. Nepomuceno", "Edson OliveiraJr", "Maria Teresa Baldassarre", "Valdemar V. Graciano Neto", "Marcos Kalinowski"], "title": "Towards an OSF-based Registered Report Template for Software Engineering Controlled Experiments", "comment": "Author version of paper accepted at the 3rd International Workshop on Methodological Issues with Empirical Studies in Software Engineering (WSESE@ICSE 2026)", "summary": "Context: The empirical software engineering (ESE) community has contributed to improving experimentation over the years. However, there is still a lack of rigor in describing controlled experiments, hindering reproducibility and transparency. Registered Reports (RR) have been discussed in the ESE community to address these issues. A RR registers a study's hypotheses, methods, and/or analyses before execution, involving peer review and potential acceptance before data collection. This helps mitigate problematic practices such as p-hacking, publication bias, and inappropriate post hoc analysis. Objective: This paper presents initial results toward establishing an RR template for Software Engineering controlled experiments using the Open Science Framework (OSF). Method: We analyzed templates of selected OSF RR types in light of documentation guidelines for controlled experiments. Results: The observed lack of rigor motivated our investigation of OSF-based RR types. Our analysis showed that, although one of the RR types aligned with many of the documentation suggestions contained in the guidelines, none of them covered the guidelines comprehensively. The study also highlights limitations in OSF RR template customization. Conclusion: Despite progress in ESE, planning and documenting experiments still lack rigor, compromising reproducibility. Adopting OSF-based RRs is proposed. However, no currently available RR type fully satisfies the guidelines. Establishing RR-specific guidelines for SE is deemed essential.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2602.09604", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2602.09604", "abs": "https://arxiv.org/abs/2602.09604", "authors": ["Ruimin Shi", "Gabin Schieffer", "Pei-Hung Lin", "Maya Gokhale", "Andreas Herten", "Ivy Peng"], "title": "High-performance Vector-length Agnostic Quantum Circuit Simulations on ARM Processors", "comment": "To be published in IPDPS2026", "summary": "ARM SVE and RISC-V RVV are emerging vector architectures in high-end processors that support vectorization of flexible vector length. In this work, we leverage an important workload for quantum computing, quantum state-vector simulations, to understand whether high-performance portability can be achieved in a vector-length agnostic (VLA) design. We propose a VLA design and optimization techniques critical for achieving high performance, including VLEN-adaptive memory layout adjustment, load buffering, fine-grained loop control, and gate fusion-based arithmetic intensity adaptation. We provide an implementation in Google's Qsim and evaluate five quantum circuits of up to 36 qubits on three ARM processors, including NVIDIA Grace, AWS Graviton3, and Fujitsu A64FX. By defining new metrics and PMU events to quantify vectorization activities, we draw generic insights for future VLA designs. Our single-source implementation of VLA quantum simulations achieves up to 4.5x speedup on A64FX, 2.5x speedup on Grace, and 1.5x speedup on Graviton.", "AI": {"tldr": "\u7814\u7a76\u9488\u5bf9ARM SVE\u548cRISC-V RVV\u5411\u91cf\u67b6\u6784\uff0c\u5f00\u53d1\u5411\u91cf\u957f\u5ea6\u65e0\u5173\u8bbe\u8ba1\u4ee5\u63d0\u5347\u91cf\u5b50\u6001\u6a21\u62df\u6027\u80fd\uff0c\u5728\u4e09\u79cdARM\u5904\u7406\u5668\u4e0a\u5b9e\u73b0\u6700\u9ad84.5\u500d\u52a0\u901f\u3002", "motivation": "\u63a2\u7d22\u91cf\u5b50\u6001\u6a21\u62df\u5728\u5411\u91cf\u957f\u5ea6\u65e0\u5173\u67b6\u6784\u4e2d\u662f\u5426\u5177\u5907\u9ad8\u6027\u80fd\u53ef\u79fb\u690d\u6027\uff0c\u89e3\u51b3\u65b0\u5174\u5904\u7406\u5668\u67b6\u6784\u4e0b\u7684\u5411\u91cf\u5316\u6548\u7387\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u5411\u91cf\u957f\u5ea6\u81ea\u9002\u5e94\u5185\u5b58\u5e03\u5c40\u8c03\u6574\u3001\u8d1f\u8f7d\u7f13\u51b2\u3001\u7ec6\u7c92\u5ea6\u5faa\u73af\u63a7\u5236\u548c\u95e8\u878d\u5408\u7b97\u672f\u5f3a\u5ea6\u9002\u5e94\u7b49\u4f18\u5316\u6280\u672f\uff0c\u5728Google Qsim\u5b9e\u73b0\u5e76\u8bc4\u4f3036\u91cf\u5b50\u4f4d\u7535\u8def\u4e8eNVIDIA Grace\u3001AWS Graviton3\u548cFujitsu A64FX\u5e73\u53f0\uff0c\u5b9a\u4e49\u65b0\u6307\u6807\u91cf\u5316\u5411\u91cf\u5316\u6d3b\u52a8\u3002", "result": "\u5728A64FX\u5b9e\u73b0\u6700\u9ad84.5\u500d\u52a0\u901f\uff0cGrace\u83b7\u5f972.5\u500d\u52a0\u901f\uff0cGraviton\u8fbe1.5\u500d\u52a0\u901f\uff0c\u9a8c\u8bc1\u4e86\u5411\u91cf\u957f\u5ea6\u65e0\u5173\u8bbe\u8ba1\u7684\u6027\u80fd\u53ef\u79fb\u690d\u6027\u5e76\u4e3a\u672a\u6765\u67b6\u6784\u63d0\u4f9b\u901a\u7528\u542f\u793a\u3002", "conclusion": "\u5355\u4ee3\u7801\u6e90\u5411\u91cf\u957f\u5ea6\u65e0\u5173\u91cf\u5b50\u6a21\u62df\u5b9e\u73b0\u4e86\u8de8\u5e73\u53f0\u663e\u8457\u6027\u80fd\u63d0\u5347\uff0c\u8bc1\u660e\u8be5\u8bbe\u8ba1\u53ef\u6709\u6548\u5e94\u5bf9\u591a\u6837\u5316\u5411\u91cf\u67b6\u6784\u6311\u6218\u3002"}}
{"id": "2602.09311", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2602.09311", "abs": "https://arxiv.org/abs/2602.09311", "authors": ["Tao Xiao", "Dong Wang", "Shane McIntosh", "Hideaki Hata", "Yasutaka Kamei"], "title": "Cross-Project Flakiness: A Case Study of the OpenStack Ecosystem", "comment": null, "summary": "Automated regression testing is a cornerstone of modern software development, often contributing directly to code review and Continuous Integration (CI). Yet some tests suffer from flakiness, where their outcomes vary non-deterministically. Flakiness erodes developer trust in test results, wastes computational resources, and undermines CI reliability. While prior research has examined test flakiness within individual projects, its broader ecosystem-wide impact remains largely unexplored. In this paper, we present an empirical study of test flakiness in the OpenStack ecosystem, which focuses on (1) cross-project flakiness, where flaky tests impact multiple projects, and (2) inconsistent flakiness, where a test exhibits flakiness in some projects but remains stable in others. By analyzing 649 OpenStack projects, we identify 1,535 cross-project flaky tests and 1,105 inconsistently flaky tests. We find that cross-project flakiness affects 55% of OpenStack projects and significantly increases both review time and computational costs. Surprisingly, 70% of unit tests exhibit cross-project flakiness, challenging the assumption that unit tests are inherently insulated from issues that span modules like integration and system-level tests. Through qualitative analysis, we observe that race conditions in CI, inconsistent build configurations, and dependency mismatches are the primary causes of inconsistent flakiness. These findings underline the need for better coordination across complex ecosystems, standardized CI configurations, and improved test isolation strategies.", "AI": {"tldr": "\u672c\u6587\u901a\u8fc7\u5206\u6790649\u4e2aOpenStack\u9879\u76ee\uff0c\u63ed\u793a\u4e86\u6d4b\u8bd5\u4e0d\u7a33\u5b9a\u6027\u5728\u751f\u6001\u7cfb\u7edf\u5c42\u9762\u7684\u5e7f\u6cdb\u5f71\u54cd\uff0c\u5305\u62ec55%\u7684\u9879\u76ee\u53d7\u8de8\u9879\u76ee\u6d6e\u52a8\u5f71\u54cd\u53ca\u5355\u5143\u6d4b\u8bd5\u7684\u5e38\u89c1\u6d6e\u52a8\u95ee\u9898\u3002", "motivation": "\u63a2\u7d22\u6d4b\u8bd5\u4e0d\u7a33\u5b9a\u6027\u5728\u66f4\u5e7f\u6cdb\u751f\u6001\u7cfb\u7edf\u4e2d\u7684\u5f71\u54cd\uff0c\u4f20\u7edf\u7814\u7a76\u591a\u96c6\u4e2d\u4e8e\u5355\u4e2a\u9879\u76ee\u5185\u90e8\uff0c\u800c\u8de8\u9879\u76ee\u548c\u4e0d\u4e00\u81f4\u7684\u6d6e\u52a8\u95ee\u9898\u9c9c\u6709\u6d89\u53ca\uff0c\u8fd9\u635f\u5bb3\u5f00\u53d1\u4fe1\u4efb\u5e76\u6d6a\u8d39\u8d44\u6e90\u3002", "method": "\u5bf9OpenStack\u751f\u6001\u7cfb\u7edf\u8fdb\u884c\u5b9e\u8bc1\u7814\u7a76\uff0c\u5206\u6790649\u4e2a\u9879\u76ee\uff0c\u805a\u7126\u8de8\u9879\u76ee\u6d6e\u52a8\uff08\u5f71\u54cd\u591a\u4e2a\u9879\u76ee\uff09\u548c\u4e0d\u4e00\u81f4\u6d6e\u52a8\uff08\u4e0d\u540c\u9879\u76ee\u8868\u73b0\u4e0d\u4e00\uff09\uff0c\u5b9a\u6027\u5206\u6790CI\u7ade\u6001\u6761\u4ef6\u3001\u6784\u5efa\u914d\u7f6e\u4e0d\u4e00\u81f4\u548c\u4f9d\u8d56\u4e0d\u5339\u914d\u7b49\u539f\u56e0\u3002", "result": "\u8bc6\u522b1535\u4e2a\u8de8\u9879\u76ee\u6d6e\u52a8\u6d4b\u8bd5\u548c1105\u4e2a\u4e0d\u4e00\u81f4\u6d6e\u52a8\u6d4b\u8bd5\uff1b55%\u7684\u9879\u76ee\u53d7\u5f71\u54cd\uff0c\u663e\u8457\u589e\u52a0\u5ba1\u67e5\u65f6\u95f4\u548c\u8ba1\u7b97\u6210\u672c\uff1b70%\u7684\u5355\u5143\u6d4b\u8bd5\u663e\u793a\u6d6e\u52a8\uff0c\u6311\u6218\u9694\u79bb\u7a33\u5b9a\u6027\uff1b\u4e3b\u8981\u539f\u56e0\u5305\u62ecCI\u7ade\u6001\u95ee\u9898\u3001\u914d\u7f6e\u5dee\u5f02\u548c\u4f9d\u8d56\u95ee\u9898\u3002", "conclusion": "\u5f3a\u8c03\u751f\u6001\u7cfb\u7edf\u534f\u4f5c\u7684\u91cd\u8981\u6027\uff0c\u5efa\u8bae\u6807\u51c6\u5316CI\u914d\u7f6e\u3001\u6539\u8fdb\u6d4b\u8bd5\u9694\u79bb\u7b56\u7565\uff0c\u4ee5\u589e\u5f3aCI\u53ef\u9760\u6027\u548c\u8d44\u6e90\u6548\u7387\u3002"}}
{"id": "2602.09721", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2602.09721", "abs": "https://arxiv.org/abs/2602.09721", "authors": ["Guowei Liu", "Hongming Li", "Yaning Guo", "Yongxi Lyu", "Mo Zhou", "Yi Liu", "Zhaogeng Li", "Yanpeng Wang"], "title": "Revealing the Challenges of Attention-FFN Disaggregation for Modern MoE Models and Hardware Systems", "comment": null, "summary": "Deploying large-scale MoE models presents challenges in memory capacity and bandwidth for expert activation. While Attention-FFN Disaggregation (AFD) has emerged as a potential architecture to decouple compute and memory resources, its performance boundaries compared to standard large-scale Expert Parallelism (EP) remain underexplored. In this paper, we conduct a systematic analysis of AFD by extending the roofline model to the communication level, correlating interconnect bandwidth, arithmetic intensity, and Hardware FLOPS Utilization (HFU). Our analysis reveals a dead zone on standard clusters: increasing FFN instance count fails to improve HFU as computational workload is capped by scale-out bandwidth, causing operator active time to shrink relative to the fixed latency budget. We further show that AFD's discrete node-level scaling incurs higher imbalance penalties than EP's continuous batch adjustment. Nevertheless, these limitations diminish under specific conditions: Superpod-class hardware with abundant interconnect bandwidth and models with coarse-grained experts and lower sparsity are more likely to benefit from AFD. These findings position AFD as a promising approach for specific hardware-model combinations rather than a universal solution.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2602.09834", "categories": ["cs.NI"], "pdf": "https://arxiv.org/pdf/2602.09834", "abs": "https://arxiv.org/abs/2602.09834", "authors": ["Baidyanath Mandal", "Aniruddha Chandra", "Rastislav Roka", "Jaros\u0142aw Wojtun", "Jan Kelner", "Cezary Zio\u0142kowski"], "title": "6G NTN Waveforms: A Comparison of OTFS, AFDM and OCDM in LEO Satellite Channels", "comment": null, "summary": "Sixth generation (6G) physical layer (PHY) is evolving beyond the legacy orthogonal frequency division multiplexing (OFDM)-based waveforms. In this paper, we compare the bit error rate (BER) performance of three beyond-OFDM waveforms, namely, orthogonal time-frequency-space (OTFS) modulation, affine frequency division multiplexing (AFDM), and orthogonal chirp division multiplexing (OCDM), which are particularly suitable for the highly mobile non-terrestrial network (NTN) vertical of 6G. In order to characterize the effect of mobility and Doppler shift in low Earth orbit (LEO) satellites, we performed BER comparisons over four different NTN tapped-delay-line (TDL) models, TDL-A, TDL-B, TDL-C, and TDL-D, as specified in the 3rd generation partnership project (3GPP) technical report TR 38.811. After channel equalization, a minimum mean squared error with successive detection (MMSE-SD) algorithm was used to enhance the BER performance. It was found that AFDM and OTFS consistently outperformed OCDM across all TDL models, while AFDM performed better than OTFS in TDL-B and TDL-C, in the high signal-to-noise ratio (SNR) regime. The complete simulation framework is made available as an open-source code for quick validation and further development.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2602.09447", "categories": ["cs.SE", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2602.09447", "abs": "https://arxiv.org/abs/2602.09447", "authors": ["Zhirui Zhang", "Hongbo Zhang", "Haoxiang Fei", "Zhiyuan Bao", "Yubin Chen", "Zhengyu Lei", "Ziyue Liu", "Yixuan Sun", "Mingkun Xiao", "Zihang Ye", "Yu Zhang", "Hongcheng Zhu", "Yuxiang Wen", "Heung-Yeung Shum"], "title": "SWE-AGI: Benchmarking Specification-Driven Software Construction with MoonBit in the Era of Autonomous Agents", "comment": "20 pages, 3 figures", "summary": "Although large language models (LLMs) have demonstrated impressive coding capabilities, their ability to autonomously build production-scale software from explicit specifications remains an open question. We introduce SWE-AGI, an open-source benchmark for evaluating end-to-end, specification-driven construction of software systems written in MoonBit. SWE-AGI tasks require LLM-based agents to implement parsers, interpreters, binary decoders, and SAT solvers strictly from authoritative standards and RFCs under a fixed API scaffold. Each task involves implementing 1,000-10,000 lines of core logic, corresponding to weeks or months of engineering effort for an experienced human developer. By leveraging the nascent MoonBit ecosystem, SWE-AGI minimizes data leakage, forcing agents to rely on long-horizon architectural reasoning rather than code retrieval. Across frontier models, gpt-5.3-codex achieves the best overall performance (solving 19/22 tasks, 86.4%), outperforming claude-opus-4.6 (15/22, 68.2%), and kimi-2.5 exhibits the strongest performance among open-source models. Performance degrades sharply with increasing task difficulty, particularly on hard, specification-intensive systems. Behavioral analysis further reveals that as codebases scale, code reading, rather than writing, becomes the dominant bottleneck in AI-assisted development. Overall, while specification-driven autonomous software engineering is increasingly viable, substantial challenges remain before it can reliably support production-scale development.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2602.09467", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2602.09467", "abs": "https://arxiv.org/abs/2602.09467", "authors": ["Sota Nakashima", "Masanari Kondo", "Mahmoud Alfadel", "Aly Ahmad", "Toshihiro Nakae", "Hidenori Matsuzaki"], "title": "Toward Linking Declined Proposals and Source Code: An Exploratory Study on the Go Repository", "comment": "11 pages, MSR2026 Technical Track", "summary": "Traceability links are key information sources for software developers, connecting software artifacts (e.g., linking requirements to the corresponding source code). In open-source software (OSS) projects, such links play an important role, particularly between the contributions (e.g., GitHub issues) and the corresponding source code. Through these links, developers can trace the discussions in contributions and uncover design rationales, constraints, and security concerns. Previous studies have mainly examined accepted contributions, while those declined after discussion have been overlooked. The discussions behind declined contributions contain valuable design rationales and implicit knowledge about software decision-making, as the reasons behind the decline often reveal the criteria used to judge what should or should not be implemented. In this study, we present the first attempt to establish traceability links between declined contributions and related source code. We propose an initial linking approach and conduct an empirical analysis of the generated links to discuss factors affecting link generation. As our dataset, we use proposals from the official Go repository, which are GitHub issues used to propose new features or language changes. To link declined proposals to source code, we designed an LLM-driven pipeline. Our results showed that the pipeline selected the correct granularity for each declined proposal with an accuracy of 0.836, and generated correct links at that granularity with a mean precision of 0.643. To clarify the challenges of linking declined proposals, we performed a failure analysis. In the declined proposals where the pipeline failed to generate links, the discussions were often redundant and lacked concrete information (e.g., how the feature should be implemented).", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2602.09971", "categories": ["cs.NI"], "pdf": "https://arxiv.org/pdf/2602.09971", "abs": "https://arxiv.org/abs/2602.09971", "authors": ["Chuan-Chi Lai"], "title": "SCOPE: A Training-Free Online 3D Deployment for UAV-BSs with Theoretical Analysis and Comparative Study", "comment": "11 pages, 5 figures, Full research paper providing a training-free determinisitc 3D UAV deployment algorithm, SCOPE. Submitted to IEEE Journal for possible publication", "summary": "Unmanned Aerial Vehicle (UAV)-mounted Base Stations (UAV-BSs) offer a flexible solution for serving ground users in temporary hotspot scenarios. However, efficiently deploying UAV-BSs to satisfy heterogeneous user distributions remains a challenging optimization problem. While recent data-driven approaches, particularly Deep Reinforcement Learning (DRL), have shown promise in dynamic environments, they often suffer from prohibitive training overhead, poor generalization to topology changes, and high computational complexity. To address these limitations, this paper proposes Satisfaction-driven Coverage Optimization via Perimeter Extraction (SCOPE), a training-free and online 3D deployment framework. Unlike heuristic baselines that rely on fixed-altitude assumptions, SCOPE integrates a perimeter extraction mechanism with the Smallest Enclosing Circle (SEC) algorithm to dynamically optimize 3D UAV positions. Theoretically, we provide a rigorous convergence proof of the proposed algorithm and derive its polynomial time complexity of $O(N^2 \\log N)$. Experimentally, we conduct a comprehensive comparative study against state-of-the-art DRL baselines (e.g., PPO). Simulation results demonstrate that SCOPE achieves comparable user satisfaction to DRL methods but significantly lower computational latency (milliseconds vs. hours of training) and superior energy efficiency, making it an ideal solution for real-time, on-demand emergency deployment.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2602.09540", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2602.09540", "abs": "https://arxiv.org/abs/2602.09540", "authors": ["Muxin Tian", "Zhe Wang", "Blair Yang", "Zhenwei Tang", "Kunlun Zhu", "Honghua Dong", "Hanchen Li", "Xinni Xie", "Guangjing Wang", "Jiaxuan You"], "title": "SWE-Bench Mobile: Can Large Language Model Agents Develop Industry-Level Mobile Applications?", "comment": null, "summary": "Can large language model agents develop industry-level mobile applications? We introduce \\textbf{SWE-Bench Mobile}, a benchmark for evaluating coding agents on realistic software engineering tasks derived from a production iOS codebase. Unlike existing benchmarks that focus on isolated problems or bug fixes, SWE-Bench Mobile captures the full complexity of industrial development: multi-modal inputs (PRDs and Figma designs), a large-scale mixed Swift/Objective-C codebase, and comprehensive test suites. We evaluate 22 agent-model configurations across four coding agents -- three commercial (Cursor, Codex, Claude Code) and one open-source (OpenCode) -- and find that even the best configurations achieve only 12\\% task success rate. Our analysis reveals that (1) agent design matters as much as model capability -- the same model shows up to 6$\\times$ performance gap across agents, (2) commercial agents consistently outperform open-source alternatives, and (3) simple ``Defensive Programming'' prompts outperform complex ones by 7.4\\%. These findings highlight a significant gap between current agent capabilities and industrial requirements, while providing actionable insights for practitioners and researchers. We release SWE-Bench Mobile as a \\textit{hosted benchmark challenge} to prevent data contamination and ensure fair evaluation. The public leaderboard and development toolkit are available at https://swebenchmobile.com.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2602.09994", "categories": ["cs.NI"], "pdf": "https://arxiv.org/pdf/2602.09994", "abs": "https://arxiv.org/abs/2602.09994", "authors": ["Chuan-Chi Lai", "Chi Jai Choy"], "title": "ORCHID: Fairness-Aware Orchestration in Mission-Critical Air-Ground Integrated Networks", "comment": "15 pages, 7 figures. Full research paper providing a resilient two-stage orchestration framework to optimize collaborative UAV-GBS deployment in mission-critical air-ground integrated networks (AGINs). Under review at an IEEE Journal", "summary": "In the era of 6G Air-Ground Integrated Networks (AGINs), Unmanned Aerial Vehicles (UAVs) are pivotal for providing on-demand wireless coverage in mission-critical environments, such as post-disaster rescue operations. However, traditional Deep Reinforcement Learning (DRL) approaches for multi-UAV orchestration often face critical challenges: instability due to the non-stationarity of multi-agent environments and the difficulty of balancing energy efficiency with service equity. To address these issues, this paper proposes ORCHID (Orchestration of Resilient Coverage via Hybrid Intelligent Deployment), a novel stability-enhanced two-stage learning framework. First, ORCHID leverages a GBS-aware topology partitioning strategy to mitigate the exploration cold-start problem. Second, we introduce a Reset-and-Finetune (R\\&F) mechanism within the MAPPO architecture that stabilizes the learning process via synchronized learning rate decay and optimizer state resetting. This mechanism effectively suppresses gradient variance to prevent policy degradation, thereby ensuring algorithmic resilience in dynamic environments. Furthermore, we uncover a counter-intuitive efficiency-fairness synergy: contrary to the conventional trade-off, our results demonstrate that the proposed Max-Min Fairness (MMF) design not only guarantees service for cell-edge users but also achieves superior energy efficiency compared to Proportional Fairness (PF), which tends to converge to suboptimal greedy equilibria. Extensive experiments confirm that ORCHID occupies a superior Pareto-dominant position compared to state-of-the-art baselines, ensuring robust convergence and resilient connectivity in mission-critical scenarios.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2602.09846", "categories": ["cs.SE", "cs.CY"], "pdf": "https://arxiv.org/pdf/2602.09846", "abs": "https://arxiv.org/abs/2602.09846", "authors": ["Malik Abdul Sami", "Zeeshan Rasheed", "Meri Olenius", "Muhammad Waseem", "Kai-Kristian Kemell", "Jussi Rasku", "Pekka Abrahamsson"], "title": "Generative AI Adoption in an Energy Company: Exploring Challenges and Use Cases", "comment": null, "summary": "Organisations are examining how generative AI can support their operational work and decision-making processes. This study investigates how employees in a energy company understand AI adoption and identify areas where AI and LLMs-based agentic workflows could assist daily activities. Data was collected in four weeks through sixteen semi-structured interviews across nine departments, supported by internal documents and researcher observations. The analysis identified areas where employees positioned AI as useful, including reporting work, forecasting, data handling, maintenance-related tasks, and anomaly detection. Participants also described how GenAI and LLM-based tools could be introduced through incremental steps that align with existing workflows. The study provides an overview view of AI adoption in the energy sector and offers a structured basis for identifying entry points for practical implementation and comparative research across industries.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2602.09892", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2602.09892", "abs": "https://arxiv.org/abs/2602.09892", "authors": ["Jiale Zhao", "Guoxin Chen", "Fanzhe Meng", "Minghao Li", "Jie Chen", "Hui Xu", "Yongshuai Sun", "Xin Zhao", "Ruihua Song", "Yuan Zhang", "Peng Wang", "Cheng Chen", "Jirong Wen", "Kai Jia"], "title": "Immersion in the GitHub Universe: Scaling Coding Agents to Mastery", "comment": null, "summary": "Achieving mastery in real world software engineering tasks is fundamentally bottlenecked by the scarcity of large scale, high quality training data. Scaling such data has been limited by the complexity of environment setup, unit test generation, and problem statement curation. In this paper, we propose ScaleSWE, an automated, sandboxed multi agent workflow designed to construct high quality SWE data at scale. The system coordinates three specialized agents for environment setup, test creation, and problem description synthesis to process 6 million pull requests across 5200 repositories, producing Scale SWE Data: 100k verified SWE instances, the largest such dataset to date. It substantially surpasses existing real world datasets in repository diversity and reflects realistic task complexity. We further demonstrate the dataset utility for training by distilling 71498 high quality trajectories and finetuning Qwen30BA3BInstruct to produce ScaleSWE Agent. Our agent achieves a 64 resolve rate on SWE Bench Verified a nearly three fold improvement over the base model. ScaleSWE provides a scalable, reproducible approach for data construction to advance LLM based software engineering. Scale SWE will be publicly available.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2602.09921", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2602.09921", "abs": "https://arxiv.org/abs/2602.09921", "authors": ["Everaldo Silva J\u00fanior", "Lina Marsso", "Ricardo Caldas", "Marsha Chechik", "Gena\u00edna Nunes Rodrigues"], "title": "Operationalizing Human Values in the Requirements Engineering Process of Ethics-Aware Autonomous Systems", "comment": null, "summary": "Operationalizing human values alongside functional and adaptation requirements remains challenging due to their ambiguous, pluralistic, and context-dependent nature. Explicit representations are needed to support the elicitation, analysis, and negotiation of value conflicts beyond traditional software engineering abstractions. In this work, we propose a requirements engineering approach for ethics-aware autonomous systems that captures human values as normative goals and aligns them with functional and adaptation goals. These goals are systematically operationalized into Social, Legal, Ethical, Empathetic, and Cultural (SLEEC) requirements, enabling automated well-formedness checking, conflict detection, and early design-time negotiation. We demonstrate the feasibility of the approach through a medical Body Sensor Network case study.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2602.09930", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2602.09930", "abs": "https://arxiv.org/abs/2602.09930", "authors": ["Nishil Amin", "Zhiwei Fei", "Xiang Li", "Justyna Petke", "He Ye"], "title": "JMigBench: A Benchmark for Evaluating LLMs on Source Code Migration (Java 8 to Java 11)", "comment": null, "summary": "We build a benchmark to evaluate large language models (LLMs) for source code migration tasks, specifically upgrading functions from Java 8 to Java 11. We first collected a dataset of function pairs from open-source repositories, but limitations in data quality led us to construct a refined dataset covering eight categories of deprecated APIs. Using this dataset, the Mistral Codestral model was evaluated with CodeBLEU and keyword-based metrics to measure lexical and semantic similarity as well as migration correctness. Results show that the evaluated model (Mistral Codestral) can handle trivial one-to-one API substitutions with moderate success, achieving identical migrations in 11.11% of the cases, but it struggles with more complex migrations such as CORBA or JAX-WS. These findings suggest Mistral Codestral can partially reduce developer effort by automating repetitive migration tasks but cannot yet replace humans within the scope of the JMigBench benchmark. The benchmark and analysis provide a foundation for future work on expanding datasets, refining prompting strategies, and improving migration performance across different LLMs.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2602.09942", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2602.09942", "abs": "https://arxiv.org/abs/2602.09942", "authors": ["Junjie Luo", "Shangzhou Xia", "Fuyuan Zhang", "Jianjun Zhao"], "title": "QEMI: A Quantum Software Stacks Testing Framework via Equivalence Modulo Inputs", "comment": null, "summary": "As quantum algorithms and hardware continue to evolve, ensuring the correctness of the quantum software stack (QSS) has become increasingly important. However, testing QSSes remains challenging due to the oracle problem, i.e., the lack of a reliable ground truth for expected program behavior. Existing metamorphic testing approaches often rely on equivalent circuit transformations, backend modifications, or parameter tuning to address this issue. In this work, inspired by Equivalence Modulo Inputs (EMI), we propose Quantum EMI (QEMI), a new testing approach for QSSes. Our key contributions include: (1) a random quantum program generator that produces code with dead code based on quantum control-flow structures, and (2) an adaptation of the EMI technique from classical compiler testing to generate variants by removing dead code. By comparing the behavior of these variants, we can detect potential bugs in QSS implementations. We applied QEMI to Qiskit, Q#, and Cirq, and successfully identified 11 crash bugs and 1 behavioral inconsistency. QEMI expands the limited set of testing techniques available for quantum software stacks by going beyond structural transformations and incorporating semantics-preserving ones into quantum program analysis.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2602.09944", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2602.09944", "abs": "https://arxiv.org/abs/2602.09944", "authors": ["Xiang Li", "Zhiwei Fei", "Ying Ma", "Jerry Zhang", "Sarro Federica", "He Ye"], "title": "Environment-in-the-Loop: Rethinking Code Migration with LLM-based Agents", "comment": null, "summary": "Modern software systems continuously undergo code upgrades to enhance functionality, security, and performance, and Large Language Models (LLMs) have demonstrated remarkable capabilities in code migration tasks. However, while research on automated code migration which including refactoring, API adaptation, and dependency updates has advanced rapidly, the exploration of the automated environment interaction that must accompany it remains relatively scarce. In practice, code and its environment are intricately intertwined. Relying solely on static analysis of the environment leads to an inadequate understanding of the target setting, prolongs feedback cycles, and consequently causes significant rework and project delays, thereby reducing overall efficiency. We contend that successful software evolution demands a holistic perspective that integrates both code and environment migration. To understand the current landscape and challenges, we first provide an overview of the status of automated environment construction. We then propose a novel framework paradigm that tightly integrates automated environment setup with the code migration workflow. Finally, we explore the challenges and future directions for automated environment interaction within the code migration domain. Our findings emphasize that without automated environment interaction, the automation of code migration is only half complete.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
