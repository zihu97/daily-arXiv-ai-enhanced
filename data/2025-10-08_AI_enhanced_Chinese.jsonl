{"id": "2510.05245", "categories": ["cs.AR", "cs.ET", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.05245", "abs": "https://arxiv.org/abs/2510.05245", "authors": ["Yue Pan", "Zihan Xia", "Po-Kai Hsu", "Lanxiang Hu", "Hyungyo Kim", "Janak Sharda", "Minxuan Zhou", "Nam Sung Kim", "Shimeng Yu", "Tajana Rosing", "Mingu Kang"], "title": "Stratum: System-Hardware Co-Design with Tiered Monolithic 3D-Stackable DRAM for Efficient MoE Serving", "comment": null, "summary": "As Large Language Models (LLMs) continue to evolve, Mixture of Experts (MoE)\narchitecture has emerged as a prevailing design for achieving state-of-the-art\nperformance across a wide range of tasks. MoE models use sparse gating to\nactivate only a handful of expert sub-networks per input, achieving\nbillion-parameter capacity with inference costs akin to much smaller models.\nHowever, such models often pose challenges for hardware deployment due to the\nmassive data volume introduced by the MoE layers. To address the challenges of\nserving MoE models, we propose Stratum, a system-hardware co-design approach\nthat combines the novel memory technology Monolithic 3D-Stackable DRAM (Mono3D\nDRAM), near-memory processing (NMP), and GPU acceleration. The logic and Mono3D\nDRAM dies are connected through hybrid bonding, whereas the Mono3D DRAM stack\nand GPU are interconnected via silicon interposer. Mono3D DRAM offers higher\ninternal bandwidth than HBM thanks to the dense vertical interconnect pitch\nenabled by its monolithic structure, which supports implementations of\nhigher-performance near-memory processing. Furthermore, we tackle the latency\ndifferences introduced by aggressive vertical scaling of Mono3D DRAM along the\nz-dimension by constructing internal memory tiers and assigning data across\nlayers based on access likelihood, guided by topic-based expert usage\nprediction to boost NMP throughput. The Stratum system achieves up to 8.29x\nimprovement in decoding throughput and 7.66x better energy efficiency across\nvarious benchmarks compared to GPU baselines.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faStratum\uff0c\u4e00\u79cd\u7ed3\u5408\u65b0\u578b\u5355\u72473D\u5806\u53e0DRAM\uff08Mono3D DRAM\uff09\u3001\u8fd1\u5b58\u8ba1\u7b97\uff08NMP\uff09\u548cGPU\u52a0\u901f\u7684\u7cfb\u7edf-\u786c\u4ef6\u534f\u540c\u8bbe\u8ba1\uff0c\u7528\u4e8e\u9ad8\u6548\u90e8\u7f72\u7a00\u758f\u6fc0\u6d3b\u7684Mixture of Experts\uff08MoE\uff09\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u663e\u8457\u63d0\u5347\u63a8\u7406\u541e\u5410\u91cf\u548c\u80fd\u6548\u3002", "motivation": "MoE\u67b6\u6784\u867d\u80fd\u5b9e\u73b0\u5927\u89c4\u6a21\u53c2\u6570\u91cf\u4e0e\u8f83\u4f4e\u63a8\u7406\u6210\u672c\uff0c\u4f46\u5176\u5f15\u5165\u7684\u5927\u91cf\u6570\u636e\u5bf9\u786c\u4ef6\u90e8\u7f72\u6784\u6210\u6311\u6218\uff0c\u5c24\u5176\u5728\u5185\u5b58\u5e26\u5bbd\u548c\u80fd\u6548\u65b9\u9762\u3002", "method": "\u91c7\u7528Mono3D DRAM\u4e0e\u8fd1\u5b58\u8ba1\u7b97\u7ed3\u5408GPU\u7684\u5f02\u6784\u67b6\u6784\uff0c\u901a\u8fc7\u6df7\u5408\u952e\u5408\u8fde\u63a5\u903b\u8f91\u4e0e\u5b58\u50a8\u82af\u7247\uff0c\u5e76\u5229\u7528\u7845\u4e2d\u4ecb\u5c42\u8fde\u63a5DRAM\u5806\u6808\u4e0eGPU\uff1b\u540c\u65f6\u57fa\u4e8e\u4e3b\u9898\u9884\u6d4b\u4e13\u5bb6\u4f7f\u7528\u60c5\u51b5\uff0c\u5bf9Mono3D DRAM\u5185\u90e8\u8fdb\u884c\u5206\u5c42\u6570\u636e\u653e\u7f6e\u4ee5\u4f18\u5316\u8bbf\u95ee\u5ef6\u8fdf\u548cNMP\u541e\u5410\u3002", "result": "Stratum\u7cfb\u7edf\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u76f8\u6bd4\u7eafGPU\u57fa\u7ebf\u5b9e\u73b0\u4e86\u6700\u9ad88.29\u500d\u7684\u89e3\u7801\u541e\u5410\u91cf\u63d0\u5347\u548c7.66\u500d\u7684\u80fd\u6548\u63d0\u5347\u3002", "conclusion": "\u901a\u8fc7\u7cfb\u7edf-\u786c\u4ef6\u534f\u540c\u8bbe\u8ba1\uff0cStratum\u6709\u6548\u89e3\u51b3\u4e86MoE\u6a21\u578b\u90e8\u7f72\u4e2d\u7684\u5185\u5b58\u74f6\u9888\u95ee\u9898\uff0c\u4e3a\u9ad8\u6548\u63a8\u7406\u63d0\u4f9b\u4e86\u53ef\u884c\u8def\u5f84\u3002"}}
{"id": "2510.05556", "categories": ["cs.DC", "cs.OS"], "pdf": "https://arxiv.org/pdf/2510.05556", "abs": "https://arxiv.org/abs/2510.05556", "authors": ["Jiakai Xu", "Tianle Zhou", "Eugene Wu", "Kostis Kaffes"], "title": "Toward Systems Foundations for Agentic Exploration", "comment": null, "summary": "Agentic exploration, letting LLM-powered agents branch, backtrack, and search\nacross many execution paths, demands systems support well beyond today's\npass-at-k resets. Our benchmark of six snapshot/restore mechanisms shows that\ngeneric tools such as CRIU or container commits are not fast enough even in\nisolated testbeds, and they crumble entirely in real deployments where agents\nshare files, sockets, and cloud APIs with other agents and human users. In this\ntalk, we pinpoint three open fundamental challenges: fork semantics, which\nconcerns how branches reveal or hide tentative updates; external side-effects,\nwhere fork awareness must be added to services or their calls intercepted; and\nnative forking, which requires cloning databases and runtimes in microseconds\nwithout bulk copying.", "AI": {"tldr": "\u672c\u6587\u6307\u51fa\u5f53\u524d\u901a\u7528\u5feb\u7167/\u6062\u590d\u673a\u5236\uff08\u5982CRIU\uff09\u65e0\u6cd5\u6ee1\u8db3\u5927\u8bed\u8a00\u6a21\u578b\u667a\u80fd\u4f53\u5728\u771f\u5b9e\u73af\u5883\u4e2d\u9ad8\u6548\u5206\u652f\u4e0e\u56de\u6eaf\u7684\u9700\u6c42\uff0c\u5e76\u63d0\u51fa\u4e86\u4e09\u5927\u6838\u5fc3\u6311\u6218\uff1a\u5206\u652f\u8bed\u4e49\u3001\u5916\u90e8\u526f\u4f5c\u7528\u5904\u7406\u548c\u5fae\u79d2\u7ea7\u539f\u751f\u5206\u652f\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u7cfb\u7edf\uff08\u5982CRIU\u6216\u5bb9\u5668\u63d0\u4ea4\uff09\u5728\u652f\u6301\u5927\u8bed\u8a00\u6a21\u578b\u667a\u80fd\u4f53\u8fdb\u884c\u591a\u8def\u5f84\u63a2\u7d22\u65f6\u6027\u80fd\u4e0d\u8db3\uff0c\u5c24\u5176\u5728\u771f\u5b9e\u90e8\u7f72\u4e2d\u56e0\u5171\u4eab\u8d44\u6e90\uff08\u6587\u4ef6\u3001\u5957\u63a5\u5b57\u3001\u4e91API\uff09\u800c\u5931\u6548\uff0c\u4e9f\u9700\u65b0\u7684\u7cfb\u7edf\u652f\u6301\u3002", "method": "\u901a\u8fc7\u8bc4\u6d4b\u516d\u79cd\u5feb\u7167/\u6062\u590d\u673a\u5236\uff0c\u5206\u6790\u5176\u5728\u9694\u79bb\u73af\u5883\u548c\u771f\u5b9e\u90e8\u7f72\u4e2d\u7684\u8868\u73b0\uff0c\u8bc6\u522b\u51fa\u667a\u80fd\u4f53\u63a2\u7d22\u7cfb\u7edf\u6240\u9700\u89e3\u51b3\u7684\u5173\u952e\u95ee\u9898\u3002", "result": "\u53d1\u73b0\u901a\u7528\u5de5\u5177\u5728\u901f\u5ea6\u548c\u5171\u4eab\u73af\u5883\u9002\u5e94\u6027\u65b9\u9762\u5b58\u5728\u4e25\u91cd\u4e0d\u8db3\uff0c\u5e76\u660e\u786e\u6307\u51fa\u4e09\u5927\u5f00\u653e\u6027\u6311\u6218\uff1a\u5206\u652f\u8bed\u4e49\u3001\u5916\u90e8\u526f\u4f5c\u7528\u5904\u7406\u3001\u539f\u751f\u5fae\u79d2\u7ea7\u5206\u652f\u3002", "conclusion": "\u4e3a\u652f\u6301\u5927\u8bed\u8a00\u6a21\u578b\u667a\u80fd\u4f53\u7684\u9ad8\u6548\u63a2\u7d22\uff0c\u5fc5\u987b\u91cd\u65b0\u8bbe\u8ba1\u7cfb\u7edf\u673a\u5236\uff0c\u4ee5\u5e94\u5bf9\u5206\u652f\u8bed\u4e49\u3001\u5916\u90e8\u526f\u4f5c\u7528\u548c\u9ad8\u6027\u80fd\u539f\u751f\u5206\u652f\u7b49\u6839\u672c\u6027\u6311\u6218\u3002"}}
{"id": "2510.05327", "categories": ["cs.AR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.05327", "abs": "https://arxiv.org/abs/2510.05327", "authors": ["Zahin Ibnat", "Paul E. Calzada", "Rasin Mohammed Ihtemam", "Sujan Kumar Saha", "Jingbo Zhou", "Farimah Farahmandi", "Mark Tehranipoor"], "title": "DeepV: A Model-Agnostic Retrieval-Augmented Framework for Verilog Code Generation with a High-Quality Knowledge Base", "comment": "22 pages, 6 figures", "summary": "As large language models (LLMs) continue to be integrated into modern\ntechnology, there has been an increased push towards code generation\napplications, which also naturally extends to hardware design automation.\nLLM-based solutions for register transfer level (RTL) code generation for\nintellectual property (IP) designs have grown, especially with fine-tuned LLMs,\nprompt engineering, and agentic approaches becoming popular in literature.\nHowever, a gap has been exposed in these techniques, as they fail to integrate\nnovel IPs into the model's knowledge base, subsequently resulting in poorly\ngenerated code. Additionally, as general-purpose LLMs continue to improve,\nfine-tuned methods on older models will not be able to compete to produce more\naccurate and efficient designs. Although some retrieval augmented generation\n(RAG) techniques exist to mitigate challenges presented in fine-tuning\napproaches, works tend to leverage low-quality codebases, incorporate\ncomputationally expensive fine-tuning in the frameworks, or do not use RAG\ndirectly in the RTL generation step. In this work, we introduce DeepV: a\nmodel-agnostic RAG framework to generate RTL designs by enhancing context\nthrough a large, high-quality dataset without any RTL-specific training. Our\nframework benefits the latest commercial LLM, OpenAI's GPT-5, with a near 17%\nincrease in performance on the VerilogEval benchmark. We host DeepV for use by\nthe community in a Hugging Face (HF) Space:\nhttps://huggingface.co/spaces/FICS-LLM/DeepV.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86DeepV\uff0c\u4e00\u79cd\u4e0e\u6a21\u578b\u65e0\u5173\u7684\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u6846\u67b6\uff0c\u7528\u4e8e\u5728\u65e0\u9700RTL\u7279\u5b9a\u8bad\u7ec3\u7684\u60c5\u51b5\u4e0b\uff0c\u5229\u7528\u9ad8\u8d28\u91cf\u6570\u636e\u96c6\u63d0\u5347\u5bc4\u5b58\u5668\u4f20\u8f93\u7ea7\uff08RTL\uff09\u4ee3\u7801\u751f\u6210\u6027\u80fd\uff0c\u5e76\u5728VerilogEval\u57fa\u51c6\u4e0a\u4f7fGPT-5\u6027\u80fd\u63d0\u5347\u8fd117%\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684RTL\u4ee3\u7801\u751f\u6210\u65b9\u6cd5\u96be\u4ee5\u6574\u5408\u65b0\u578bIP\u5230\u6a21\u578b\u77e5\u8bc6\u5e93\u4e2d\uff0c\u5bfc\u81f4\u751f\u6210\u4ee3\u7801\u8d28\u91cf\u5dee\uff1b\u540c\u65f6\uff0c\u9488\u5bf9\u65e7\u6a21\u578b\u7684\u5fae\u8c03\u65b9\u6cd5\u96be\u4ee5\u8ddf\u4e0a\u901a\u7528LLM\u7684\u8fdb\u5c55\u3002\u73b0\u6709RAG\u65b9\u6cd5\u5b58\u5728\u4f7f\u7528\u4f4e\u8d28\u91cf\u4ee3\u7801\u5e93\u3001\u8ba1\u7b97\u5f00\u9500\u5927\u6216\u672a\u76f4\u63a5\u7528\u4e8eRTL\u751f\u6210\u7b49\u95ee\u9898\u3002", "method": "\u63d0\u51faDeepV\u6846\u67b6\uff0c\u4e00\u79cd\u6a21\u578b\u65e0\u5173\u7684RAG\u65b9\u6cd5\uff0c\u901a\u8fc7\u5927\u89c4\u6a21\u9ad8\u8d28\u91cf\u6570\u636e\u96c6\u589e\u5f3a\u4e0a\u4e0b\u6587\uff0c\u5728\u4e0d\u8fdb\u884cRTL\u7279\u5b9a\u8bad\u7ec3\u7684\u524d\u63d0\u4e0b\u8f85\u52a9LLM\u751f\u6210RTL\u4ee3\u7801\u3002", "result": "\u5728VerilogEval\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cDeepV\u4f7fOpenAI\u7684GPT-5\u6027\u80fd\u63d0\u5347\u8fd117%\uff0c\u5e76\u5df2\u5728Hugging Face\u5e73\u53f0\u5f00\u6e90\u3002", "conclusion": "DeepV\u6709\u6548\u89e3\u51b3\u4e86\u73b0\u6709RTL\u751f\u6210\u65b9\u6cd5\u5728\u77e5\u8bc6\u66f4\u65b0\u548c\u6a21\u578b\u9002\u914d\u65b9\u9762\u7684\u5c40\u9650\u6027\uff0c\u4e3a\u786c\u4ef6\u8bbe\u8ba1\u81ea\u52a8\u5316\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u3001\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.05632", "categories": ["cs.AR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.05632", "abs": "https://arxiv.org/abs/2510.05632", "authors": ["Tianhao Zhu", "Dahu Feng", "Erhu Feng", "Yubin Xia"], "title": "From Principles to Practice: A Systematic Study of LLM Serving on Multi-core NPUs", "comment": null, "summary": "With the widespread adoption of Large Language Models (LLMs), the demand for\nhigh-performance LLM inference services continues to grow. To meet this demand,\na growing number of AI accelerators have been proposed, such as Google TPU,\nHuawei NPU, Graphcore IPU, and Cerebras WSE, etc. Most of these accelerators\nadopt multi-core architectures to achieve enhanced scalability, but lack the\nflexibility of SIMT architectures. Therefore, without careful configuration of\nthe hardware architecture, as well as deliberate design of tensor parallelism\nand core placement strategies, computational resources may be underutilized,\nresulting in suboptimal inference performance.\n  To address these challenges, we first present a multi-level simulation\nframework with both transaction-level and performance-model-based simulation\nfor multi-core NPUs. Using this simulator, we conduct a systematic analysis and\nfurther propose the optimal solutions for tensor parallelism strategies, core\nplacement policies, memory management methods, as well as the selection between\nPD-disaggregation and PD-fusion on multi-core NPUs. We conduct comprehensive\nexperiments on representative LLMs and various NPU configurations. The\nevaluation results demonstrate that, our solution can achieve 1.32x-6.03x\nspeedup compared to SOTA designs for multi-core NPUs across different hardware\nconfigurations. As for LLM serving, our work offers guidance on designing\noptimal hardware architectures and serving strategies for multi-core NPUs\nacross various LLM workloads.", "AI": {"tldr": "\u672c\u6587\u9488\u5bf9\u591a\u6838NPU\u4e0a\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u63a8\u7406\u6027\u80fd\u4f18\u5316\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u5c42\u6b21\u4eff\u771f\u6846\u67b6\uff0c\u5e76\u7cfb\u7edf\u5206\u6790\u4e86\u5f20\u91cf\u5e76\u884c\u7b56\u7565\u3001\u6838\u653e\u7f6e\u7b56\u7565\u3001\u5185\u5b58\u7ba1\u7406\u65b9\u6cd5\u53caPD\u62c6\u5206/\u878d\u5408\u9009\u62e9\uff0c\u5b9e\u9a8c\u8868\u660e\u5176\u65b9\u6848\u76f8\u6bd4\u73b0\u6709\u6700\u4f18\u8bbe\u8ba1\u53ef\u5b9e\u73b01.32x\u81f36.03x\u7684\u52a0\u901f\u3002", "motivation": "\u968f\u7740\u5927\u8bed\u8a00\u6a21\u578b\u7684\u5e7f\u6cdb\u5e94\u7528\uff0c\u5bf9\u9ad8\u6027\u80fd\u63a8\u7406\u670d\u52a1\u7684\u9700\u6c42\u4e0d\u65ad\u589e\u957f\u3002\u5f53\u524d\u591a\u6570AI\u52a0\u901f\u5668\u91c7\u7528\u591a\u6838\u67b6\u6784\u4f46\u7f3a\u4e4fSIMT\u67b6\u6784\u7684\u7075\u6d3b\u6027\uff0c\u82e5\u786c\u4ef6\u914d\u7f6e\u4e0e\u5e76\u884c\u7b56\u7565\u8bbe\u8ba1\u4e0d\u5f53\uff0c\u6613\u5bfc\u81f4\u8ba1\u7b97\u8d44\u6e90\u5229\u7528\u7387\u4f4e\u3001\u63a8\u7406\u6027\u80fd\u4e0d\u4f73\u3002", "method": "\u63d0\u51fa\u4e00\u4e2a\u7ed3\u5408\u4e8b\u52a1\u7ea7\u4e0e\u6027\u80fd\u6a21\u578b\u7684\u591a\u7ea7\u4eff\u771f\u6846\u67b6\uff0c\u7528\u4e8e\u591a\u6838NPU\uff1b\u57fa\u4e8e\u8be5\u6846\u67b6\u7cfb\u7edf\u8bc4\u4f30\u5e76\u4f18\u5316\u5f20\u91cf\u5e76\u884c\u7b56\u7565\u3001\u6838\u653e\u7f6e\u7b56\u7565\u3001\u5185\u5b58\u7ba1\u7406\u65b9\u6cd5\u4ee5\u53caPD-disaggregation\u4e0ePD-fusion\u7684\u9009\u62e9\u3002", "result": "\u5728\u591a\u79cdLLM\u548cNPU\u914d\u7f6e\u4e0b\u5b9e\u9a8c\u8868\u660e\uff0c\u6240\u63d0\u65b9\u6848\u76f8\u6bd4\u5f53\u524d\u6700\u5148\u8fdb\u7684\u591a\u6838NPU\u8bbe\u8ba1\u53ef\u5b9e\u73b01.32\u500d\u81f36.03\u500d\u7684\u63a8\u7406\u52a0\u901f\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u4e3a\u5728\u591a\u6838NPU\u4e0a\u9488\u5bf9\u4e0d\u540cLLM\u8d1f\u8f7d\u8bbe\u8ba1\u6700\u4f18\u786c\u4ef6\u67b6\u6784\u548c\u63a8\u7406\u670d\u52a1\u7b56\u7565\u63d0\u4f9b\u4e86\u6709\u6548\u6307\u5bfc\u3002"}}
{"id": "2510.05109", "categories": ["cs.DC", "cs.AI", "cs.CL", "eess.SP"], "pdf": "https://arxiv.org/pdf/2510.05109", "abs": "https://arxiv.org/abs/2510.05109", "authors": ["Yilong Li", "Shuai Zhang", "Yijing Zeng", "Hao Zhang", "Xinmiao Xiong", "Jingyu Liu", "Pan Hu", "Suman Banerjee"], "title": "Tiny but Mighty: A Software-Hardware Co-Design Approach for Efficient Multimodal Inference on Battery-Powered Small Devices", "comment": null, "summary": "Large Multimodal Models (LMMs) are inherently modular, consisting of vision\nand audio encoders, projectors, and large language models. Yet, they are almost\nalways executed monolithically, which underutilizes the heterogeneous\naccelerators (NPUs, GPUs, DSPs) in modern SoCs and leads to high end-to-end\nlatency. In this paper, we present NANOMIND, a hardware--software co-design\ninference framework for Large Multimodal Models (LMMs) that breaks large models\ninto modular ``bricks'' (vision, language, audio, etc.) and maps each to its\nideal accelerator. The key insight is that large models can be broken into\nmodular components and scheduled to run on the most appropriate compute units.\nIt performs module-level dynamic offloading across accelerators on\nunified-memory SoCs. By combining customized hardware design, system-level\nscheduling, and optimized low-bit computation kernels, we demonstrate our\nframework with a compact, battery-powered device capable of running LMMs\nentirely on device. This prototype functions as a self-contained intelligent\nassistant that requires no network connectivity, while achieving higher\nthroughput and superior power efficiency under strict resource constraints. The\ndesign further bypasses CPU bottlenecks and reduces redundant memory usage\nthrough token-aware buffer management and module-level coordination. Our system\noutperforms existing implementations in resource efficiency, cutting energy\nconsumption by 42.3\\% and GPU memory usage by 11.2\\%. This enables a\nbattery-powered device to run LLaVA-OneVision with a camera for nearly half a\nday and LLaMA-3-8B for voice interactions up to almost 20.8 hours.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faNANOMIND\uff0c\u4e00\u79cd\u9762\u5411\u5927\u6a21\u578b\u7684\u8f6f\u786c\u4ef6\u534f\u540c\u63a8\u7406\u6846\u67b6\uff0c\u901a\u8fc7\u5c06\u591a\u6a21\u6001\u5927\u6a21\u578b\u62c6\u5206\u4e3a\u6a21\u5757\u5316\u201c\u7816\u5757\u201d\u5e76\u52a8\u6001\u8c03\u5ea6\u81f3\u6700\u9002\u5408\u7684\u5f02\u6784\u52a0\u901f\u5668\u4e0a\u6267\u884c\uff0c\u5728\u7edf\u4e00\u5185\u5b58SoC\u4e0a\u663e\u8457\u63d0\u5347\u80fd\u6548\u4e0e\u541e\u5410\u91cf\uff0c\u5b9e\u73b0\u5b8c\u5168\u7aef\u4fa7\u8fd0\u884c\u3002", "motivation": "\u5f53\u524d\u5927\u6a21\u578b\u901a\u5e38\u4ee5\u6574\u4f53\u65b9\u5f0f\u6267\u884c\uff0c\u672a\u80fd\u5145\u5206\u5229\u7528\u73b0\u4ee3SoC\u4e2d\u7684\u5f02\u6784\u52a0\u901f\u5668\uff08\u5982NPU\u3001GPU\u3001DSP\uff09\uff0c\u5bfc\u81f4\u9ad8\u5ef6\u8fdf\u548c\u8d44\u6e90\u6d6a\u8d39\u3002", "method": "\u5c06\u5927\u6a21\u578b\u62c6\u5206\u4e3a\u89c6\u89c9\u3001\u8bed\u8a00\u3001\u97f3\u9891\u7b49\u6a21\u5757\u5316\u7ec4\u4ef6\uff0c\u7ed3\u5408\u5b9a\u5236\u786c\u4ef6\u8bbe\u8ba1\u3001\u7cfb\u7edf\u7ea7\u8c03\u5ea6\u548c\u4f4e\u6bd4\u7279\u8ba1\u7b97\u5185\u6838\uff0c\u5728\u7edf\u4e00\u5185\u5b58SoC\u4e0a\u5b9e\u73b0\u6a21\u5757\u7ea7\u52a8\u6001\u5378\u8f7d\u548c\u52a0\u901f\u5668\u6620\u5c04\uff0c\u5e76\u901a\u8fc7token\u611f\u77e5\u7684\u7f13\u51b2\u7ba1\u7406\u51cf\u5c11CPU\u74f6\u9888\u548c\u5185\u5b58\u5197\u4f59\u3002", "result": "\u5728\u7535\u6c60\u4f9b\u7535\u8bbe\u5907\u4e0a\u5b9e\u73b0\u5b8c\u5168\u7aef\u4fa7\u8fd0\u884cLLaVA-OneVision\u548cLLaMA-3-8B\uff0c\u5206\u522b\u652f\u6301\u8fd1\u534a\u5929\u7684\u6444\u50cf\u5934\u63a8\u7406\u548c20.8\u5c0f\u65f6\u7684\u8bed\u97f3\u4ea4\u4e92\uff1b\u76f8\u6bd4\u73b0\u6709\u65b9\u6848\uff0c\u80fd\u8017\u964d\u4f4e42.3%\uff0cGPU\u5185\u5b58\u4f7f\u7528\u51cf\u5c1111.2%\u3002", "conclusion": "NANOMIND\u901a\u8fc7\u6a21\u5757\u5316\u8c03\u5ea6\u4e0e\u8f6f\u786c\u4ef6\u534f\u540c\u8bbe\u8ba1\uff0c\u663e\u8457\u63d0\u5347\u7aef\u4fa7\u591a\u6a21\u6001\u5927\u6a21\u578b\u7684\u80fd\u6548\u4e0e\u8d44\u6e90\u5229\u7528\u7387\uff0c\u5b9e\u73b0\u65e0\u7f51\u7edc\u4f9d\u8d56\u7684\u9ad8\u6027\u80fd\u667a\u80fd\u52a9\u7406\u3002"}}
{"id": "2510.05787", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2510.05787", "abs": "https://arxiv.org/abs/2510.05787", "authors": ["Panagiota Nikolaou", "Freddy Gabbay", "Jawad Haj-Yahya", "Yiannakis Sazeides"], "title": "An opportunity to improve Data Center Efficiency: Optimizing the Server's Upgrade Cycle", "comment": "This work was has been submitted and presented at the 1st\n  International Workshop on Data Center Energy Efficiency (DCEE-2025) at\n  ISCA-2025, June 21, 2025, Tokyo, Japan", "summary": "This work aims to improve a data center's efficiency by optimizing the server\nupgrade plan: determine the optimal timing for replacing old servers with new\nones. The opportunity presented by this approach is demonstrated through a\nstudy based on historical server data. The study establishes a significant\nopportunity to increase the QPS/(TCOxCO2) metric by formulating a global\nupgrade plan at the data center's design time covering its entire life cycle.\nThis plan leverages information, such as server entry year, performance, and\nactive power consumption for both existing and future servers. Our findings\nreveal that an optimal global upgrade plan, may involve upgrades at non fixed\ntime periods and outperforms local upgrade plans. Local upgrade plans follow a\nfixed, equal-length cycle and make decisions based only on currently available\nserver models. These local plans select the best available server at each\nupgrade cycle without accounting for future server releases.", "AI": {"tldr": "\u672c\u6587\u901a\u8fc7\u4f18\u5316\u6570\u636e\u4e2d\u5fc3\u670d\u52a1\u5668\u7684\u5168\u5c40\u5347\u7ea7\u8ba1\u5212\uff0c\u5728\u8bbe\u8ba1\u9636\u6bb5\u5236\u5b9a\u8986\u76d6\u6574\u4e2a\u751f\u547d\u5468\u671f\u7684\u7b56\u7565\uff0c\u4ee5\u63d0\u5347QPS/(TCO\u00d7CO2)\u6307\u6807\uff0c\u4f18\u4e8e\u4f20\u7edf\u7684\u56fa\u5b9a\u5468\u671f\u5c40\u90e8\u5347\u7ea7\u65b9\u6848\u3002", "motivation": "\u73b0\u6709\u670d\u52a1\u5668\u5347\u7ea7\u7b56\u7565\u591a\u91c7\u7528\u56fa\u5b9a\u5468\u671f\u7684\u5c40\u90e8\u51b3\u7b56\uff0c\u5ffd\u89c6\u672a\u6765\u670d\u52a1\u5668\u578b\u53f7\u4fe1\u606f\uff0c\u65e0\u6cd5\u5b9e\u73b0\u6574\u4f53\u80fd\u6548\u4e0e\u6210\u672c\u7684\u6700\u4f18\u5e73\u8861\uff1b\u672c\u6587\u65e8\u5728\u901a\u8fc7\u5168\u5c40\u89c4\u5212\u6316\u6398\u63d0\u5347\u6570\u636e\u4e2d\u5fc3\u6548\u7387\u7684\u6f5c\u529b\u3002", "method": "\u57fa\u4e8e\u5386\u53f2\u670d\u52a1\u5668\u6570\u636e\uff0c\u6784\u5efa\u6db5\u76d6\u670d\u52a1\u5668\u4e0a\u7ebf\u5e74\u4efd\u3001\u6027\u80fd\u548c\u529f\u8017\u7b49\u4fe1\u606f\u7684\u5168\u5c40\u5347\u7ea7\u6a21\u578b\uff0c\u5728\u6570\u636e\u4e2d\u5fc3\u8bbe\u8ba1\u9636\u6bb5\u5236\u5b9a\u8986\u76d6\u5168\u751f\u547d\u5468\u671f\u7684\u975e\u56fa\u5b9a\u5468\u671f\u5347\u7ea7\u8ba1\u5212\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff0c\u6700\u4f18\u5168\u5c40\u5347\u7ea7\u8ba1\u5212\u901a\u5e38\u91c7\u7528\u975e\u56fa\u5b9a\u65f6\u95f4\u95f4\u9694\uff0c\u76f8\u6bd4\u4ec5\u57fa\u4e8e\u5f53\u524d\u53ef\u7528\u673a\u578b\u3001\u6309\u56fa\u5b9a\u5468\u671f\u5347\u7ea7\u7684\u5c40\u90e8\u7b56\u7565\uff0c\u80fd\u663e\u8457\u63d0\u5347QPS/(TCO\u00d7CO2)\u6307\u6807\u3002", "conclusion": "\u5728\u6570\u636e\u4e2d\u5fc3\u8bbe\u8ba1\u521d\u671f\u5236\u5b9a\u8003\u8651\u672a\u6765\u670d\u52a1\u5668\u6f14\u8fdb\u7684\u5168\u5c40\u5347\u7ea7\u8ba1\u5212\uff0c\u53ef\u6709\u6548\u63d0\u5347\u80fd\u6548\u4e0e\u7ecf\u6d4e\u6027\u7efc\u5408\u6307\u6807\uff0c\u4f18\u4e8e\u4f20\u7edf\u5c40\u90e8\u3001\u56fa\u5b9a\u5468\u671f\u7684\u5347\u7ea7\u65b9\u6cd5\u3002"}}
{"id": "2510.05111", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2510.05111", "abs": "https://arxiv.org/abs/2510.05111", "authors": ["Ian McDougall", "Noah Scott", "Joon Huh", "Kirthevasan Kandasamy", "Karthikeyan Sankaralingam"], "title": "Agora: Bridging the GPU Cloud Resource-Price Disconnect", "comment": "15 pages, 6 figures", "summary": "The historic trend of Moore's Law, which predicted exponential growth in\ncomputational performance per dollar, has diverged for modern Graphics\nProcessing Units (GPUs). While Floating Point Operations per Second (FLOPs)\ncapabilities have continued to scale economically, memory bandwidth has not,\ncreating a significant price-performance disconnect. This paper argues that the\nprevailing time-based pricing models for cloud GPUs are economically\ninefficient for bandwidth-bound workloads. These models fail to account for the\nrising marginal cost of memory bandwidth, leading to market distortions and\nsuboptimal hardware allocation. To address this, we propose a novel\nfeature-based pricing framework that directly links cost to resource\nconsumption, including but not limited to memory bandwidth. We provide a robust\neconomic and algorithmic definition of this framework and introduce Agora, a\npractical and secure system architecture for its implementation. Our\nimplementation of Agora shows that a 50us sampling provides nearly perfect\npricing as what ideal sampling would provide - losing only 5\\% of revenue. 10us\nsampling is even better result in 2.4\\% loss. Modern telemetry systems can\nalready provide this rate of measurement, and our prototype implementation\nshows the system design for feature-based pricing is buildable. Our evaluation\nacross diverse GPU applications and hardware generations empirically validates\nthe effectiveness of our approach in creating a more transparent and efficient\nmarket for cloud GPU resources.", "AI": {"tldr": "\u672c\u6587\u6307\u51fa\u5f53\u524d\u4e91GPU\u6309\u65f6\u95f4\u8ba1\u8d39\u7684\u6a21\u5f0f\u5bf9\u5185\u5b58\u5e26\u5bbd\u53d7\u9650\u4efb\u52a1\u5b58\u5728\u7ecf\u6d4e\u4f4e\u6548\u95ee\u9898\uff0c\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u8d44\u6e90\u7279\u5f81\uff08\u5982\u5185\u5b58\u5e26\u5bbd\uff09\u7684\u65b0\u578b\u5b9a\u4ef7\u6846\u67b6\uff0c\u5e76\u8bbe\u8ba1\u4e86\u540d\u4e3aAgora\u7684\u53ef\u5b9e\u73b0\u7cfb\u7edf\u67b6\u6784\uff0c\u5b9e\u9a8c\u8bc1\u660e\u8be5\u65b9\u6cd5\u80fd\u663e\u8457\u63d0\u5347\u4e91GPU\u8d44\u6e90\u5e02\u573a\u7684\u900f\u660e\u5ea6\u4e0e\u6548\u7387\u3002", "motivation": "\u6469\u5c14\u5b9a\u5f8b\u5728\u73b0\u4ee3GPU\u4e0a\u5df2\u51fa\u73b0\u5206\u5316\uff1a\u867d\u7136FLOPs\u6027\u80fd\u6301\u7eed\u7ecf\u6d4e\u5730\u63d0\u5347\uff0c\u4f46\u5185\u5b58\u5e26\u5bbd\u672a\u540c\u6b65\u589e\u957f\uff0c\u5bfc\u81f4\u73b0\u6709\u57fa\u4e8e\u65f6\u95f4\u7684\u4e91GPU\u8ba1\u8d39\u6a21\u5f0f\u65e0\u6cd5\u53cd\u6620\u5185\u5b58\u5e26\u5bbd\u65e5\u76ca\u589e\u957f\u7684\u8fb9\u9645\u6210\u672c\uff0c\u9020\u6210\u5e02\u573a\u626d\u66f2\u548c\u786c\u4ef6\u5206\u914d\u4f4e\u6548\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u8d44\u6e90\u7279\u5f81\uff08\u5305\u62ec\u5185\u5b58\u5e26\u5bbd\u7b49\uff09\u7684\u5b9a\u4ef7\u6846\u67b6\uff0c\u7ed3\u5408\u7ecf\u6d4e\u5b66\u4e0e\u7b97\u6cd5\u5b9a\u4e49\uff0c\u5e76\u5b9e\u73b0\u540d\u4e3aAgora\u7684\u7cfb\u7edf\u67b6\u6784\uff0c\u901a\u8fc7\u9ad8\u9891\u91c7\u6837\uff08\u598210\u201350\u5fae\u79d2\uff09\u7cbe\u786e\u8ba1\u91cf\u8d44\u6e90\u4f7f\u7528\u4ee5\u652f\u6301\u8be5\u5b9a\u4ef7\u6a21\u578b\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c50\u5fae\u79d2\u91c7\u6837\u4ec5\u635f\u59315%\u6536\u5165\uff0c10\u5fae\u79d2\u91c7\u6837\u635f\u5931\u4ec52.4%\uff1b\u73b0\u4ee3\u9065\u6d4b\u7cfb\u7edf\u5df2\u652f\u6301\u8be5\u91c7\u6837\u9891\u7387\uff0c\u4e14\u5728\u591a\u79cdGPU\u5e94\u7528\u548c\u786c\u4ef6\u4ee3\u9645\u4e0a\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u57fa\u4e8e\u8d44\u6e90\u7279\u5f81\u7684\u5b9a\u4ef7\u6846\u67b6\u80fd\u66f4\u51c6\u786e\u53cd\u6620\u4e91GPU\u8d44\u6e90\u7684\u771f\u5b9e\u6210\u672c\uff0c\u63d0\u5347\u5e02\u573a\u6548\u7387\u4e0e\u900f\u660e\u5ea6\uff0c\u4e14\u5177\u5907\u5b9e\u9645\u90e8\u7f72\u53ef\u884c\u6027\u3002"}}
{"id": "2510.05174", "categories": ["cs.MA", "cs.AI", "I.2; I.2.11"], "pdf": "https://arxiv.org/pdf/2510.05174", "abs": "https://arxiv.org/abs/2510.05174", "authors": ["Christoph Riedl"], "title": "Emergent Coordination in Multi-Agent Language Models", "comment": null, "summary": "When are multi-agent LLM systems merely a collection of individual agents\nversus an integrated collective with higher-order structure? We introduce an\ninformation-theoretic framework to test -- in a purely data-driven way --\nwhether multi-agent systems show signs of higher-order structure. This\ninformation decomposition lets us measure whether dynamical emergence is\npresent in multi-agent LLM systems, localize it, and distinguish spurious\ntemporal coupling from performance-relevant cross-agent synergy. We implement\nboth a practical criterion and an emergence capacity criterion operationalized\nas partial information decomposition of time-delayed mutual information (TDMI).\nWe apply our framework to experiments using a simple guessing game without\ndirect agent communication and only minimal group-level feedback with three\nrandomized interventions. Groups in the control condition exhibit strong\ntemporal synergy but only little coordinated alignment across agents. Assigning\na persona to each agent introduces stable identity-linked differentiation.\nCombining personas with an instruction to ``think about what other agents might\ndo'' shows identity-linked differentiation and goal-directed complementarity\nacross agents. Taken together, our framework establishes that multi-agent LLM\nsystems can be steered with prompt design from mere aggregates to higher-order\ncollectives. Our results are robust across emergence measures and entropy\nestimators, and not explained by coordination-free baselines or temporal\ndynamics alone. Without attributing human-like cognition to the agents, the\npatterns of interaction we observe mirror well-established principles of\ncollective intelligence in human groups: effective performance requires both\nalignment on shared objectives and complementary contributions across members.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u4fe1\u606f\u8bba\u6846\u67b6\uff0c\u7528\u4e8e\u5224\u65ad\u591a\u667a\u80fd\u4f53\u5927\u8bed\u8a00\u6a21\u578b\u7cfb\u7edf\u662f\u5426\u5c55\u73b0\u51fa\u9ad8\u9636\u96c6\u4f53\u7ed3\u6784\uff0c\u800c\u975e\u4ec5\u662f\u72ec\u7acb\u667a\u80fd\u4f53\u7684\u96c6\u5408\u3002\u901a\u8fc7\u90e8\u5206\u4fe1\u606f\u5206\u89e3\u65b9\u6cd5\u5206\u6790\u65f6\u5ef6\u4e92\u4fe1\u606f\uff0c\u4f5c\u8005\u5728\u65e0\u76f4\u63a5\u901a\u4fe1\u7684\u731c\u8c1c\u6e38\u620f\u4e2d\u9a8c\u8bc1\u4e86\u63d0\u793a\u8bbe\u8ba1\uff08\u5982\u8d4b\u4e88\u89d2\u8272\u548c\u5f15\u5bfc\u6362\u4f4d\u601d\u8003\uff09\u53ef\u4fc3\u4f7f\u7cfb\u7edf\u4ece\u7b80\u5355\u805a\u5408\u4f53\u8f6c\u53d8\u4e3a\u5177\u6709\u534f\u540c\u4e92\u8865\u6027\u7684\u9ad8\u9636\u96c6\u4f53\u3002", "motivation": "\u533a\u5206\u591a\u667a\u80fd\u4f53\u5927\u8bed\u8a00\u6a21\u578b\u7cfb\u7edf\u662f\u4ec5\u7531\u72ec\u7acb\u667a\u80fd\u4f53\u7ec4\u6210\uff0c\u8fd8\u662f\u5f62\u6210\u4e86\u5177\u6709\u9ad8\u9636\u7ed3\u6784\u7684\u6574\u5408\u96c6\u4f53\uff0c\u5bf9\u4e8e\u7406\u89e3\u5176\u534f\u540c\u673a\u5236\u548c\u63d0\u5347\u96c6\u4f53\u667a\u80fd\u81f3\u5173\u91cd\u8981\u3002\u73b0\u6709\u65b9\u6cd5\u7f3a\u4e4f\u6570\u636e\u9a71\u52a8\u7684\u624b\u6bb5\u6765\u68c0\u6d4b\u548c\u91cf\u5316\u8fd9\u79cd\u9ad8\u9636\u7ed3\u6784\u3002", "method": "\u5f15\u5165\u4e00\u79cd\u57fa\u4e8e\u4fe1\u606f\u5206\u89e3\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u65f6\u5ef6\u4e92\u4fe1\u606f\uff08TDMI\uff09\u7684\u90e8\u5206\u4fe1\u606f\u5206\u89e3\uff0c\u5b9e\u73b0\u5bf9\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u4e2d\u52a8\u6001\u6d8c\u73b0\u7684\u68c0\u6d4b\u3001\u5b9a\u4f4d\uff0c\u5e76\u533a\u5206\u865a\u5047\u7684\u65f6\u95f4\u8026\u5408\u4e0e\u771f\u6b63\u5f71\u54cd\u6027\u80fd\u7684\u8de8\u667a\u80fd\u4f53\u534f\u540c\u3002\u5728\u65e0\u76f4\u63a5\u901a\u4fe1\u7684\u731c\u8c1c\u6e38\u620f\u4e2d\uff0c\u901a\u8fc7\u4e09\u79cd\u968f\u673a\u5e72\u9884\uff08\u63a7\u5236\u7ec4\u3001\u8d4b\u4e88\u89d2\u8272\u3001\u5f15\u5bfc\u6362\u4f4d\u601d\u8003\uff09\u6d4b\u8bd5\u8be5\u6846\u67b6\u3002", "result": "\u63a7\u5236\u7ec4\u8868\u73b0\u51fa\u5f3a\u65f6\u95f4\u534f\u540c\u4f46\u8de8\u667a\u80fd\u4f53\u534f\u8c03\u5f31\uff1b\u8d4b\u4e88\u89d2\u8272\u5f15\u5165\u4e86\u7a33\u5b9a\u7684\u8eab\u4efd\u5dee\u5f02\u5316\uff1b\u7ed3\u5408\u89d2\u8272\u4e0e\u6362\u4f4d\u601d\u8003\u6307\u4ee4\u5219\u540c\u65f6\u5c55\u73b0\u51fa\u8eab\u4efd\u5dee\u5f02\u5316\u548c\u76ee\u6807\u5bfc\u5411\u7684\u4e92\u8865\u6027\u3002\u7ed3\u679c\u5728\u4e0d\u540c\u6d8c\u73b0\u5ea6\u91cf\u548c\u71b5\u4f30\u8ba1\u5668\u4e0b\u5747\u7a33\u5065\uff0c\u4e14\u4e0d\u80fd\u7531\u65e0\u534f\u8c03\u57fa\u7ebf\u6216\u7eaf\u65f6\u95f4\u52a8\u6001\u89e3\u91ca\u3002", "conclusion": "\u901a\u8fc7\u63d0\u793a\u8bbe\u8ba1\u53ef\u5f15\u5bfc\u591a\u667a\u80fd\u4f53LLM\u7cfb\u7edf\u4ece\u7b80\u5355\u805a\u5408\u4f53\u8f6c\u53d8\u4e3a\u9ad8\u9636\u96c6\u4f53\uff1b\u5176\u4e92\u52a8\u6a21\u5f0f\u867d\u4e0d\u4f9d\u8d56\u4eba\u7c7b\u8ba4\u77e5\u5047\u8bbe\uff0c\u5374\u4e0e\u4eba\u7c7b\u7fa4\u4f53\u96c6\u4f53\u667a\u80fd\u7684\u6838\u5fc3\u539f\u5219\u4e00\u81f4\uff1a\u9ad8\u6548\u8868\u73b0\u9700\u5171\u4eab\u76ee\u6807\u7684\u4e00\u81f4\u6027\u4e0e\u6210\u5458\u95f4\u8d21\u732e\u7684\u4e92\u8865\u6027\u3002"}}
{"id": "2510.05255", "categories": ["cs.NI"], "pdf": "https://arxiv.org/pdf/2510.05255", "abs": "https://arxiv.org/abs/2510.05255", "authors": ["Farhad Rezazadeh", "Hatim Chergui", "Merouane Debbah", "Houbing Song", "Dusit Niyato", "Lingjia Liu"], "title": "Rivaling Transformers: Multi-Scale Structured State-Space Mixtures for Agentic 6G O-RAN", "comment": "12 pages, 2 Figures, 5 Tables", "summary": "In sixth-generation (6G) Open Radio Access Networks (O-RAN), proactive\ncontrol is preferable. A key open challenge is delivering control-grade\npredictions within Near-Real-Time (Near-RT) latency and computational\nconstraints under multi-timescale dynamics. We therefore cast RAN Intelligent\nController (RIC) analytics as an agentic perceive-predict xApp that turns\nnoisy, multivariate RAN telemetry into short-horizon per-User Equipment (UE)\nkey performance indicator (KPI) forecasts to drive anticipatory control. In\nthis regard, Transformers are powerful for sequence learning and time-series\nforecasting, but they are memory-intensive, which limits Near-RT RIC use.\nTherefore, we need models that maintain accuracy while reducing latency and\ndata movement. To this end, we propose a lightweight Multi-Scale Structured\nState-Space Mixtures (MS3M) forecaster that mixes HiPPO-LegS kernels to capture\nmulti-timescale radio dynamics. We develop stable discrete state-space models\n(SSMs) via bilinear (Tustin) discretization and apply their causal impulse\nresponses as per-feature depthwise convolutions. Squeeze-and-Excitation gating\ndynamically reweights KPI channels as conditions change, and a compact gated\nchannel-mixing layer models cross-feature nonlinearities without\nTransformer-level cost. The model is KPI-agnostic -- Reference Signal Received\nPower (RSRP) serves as a canonical use case -- and is trained on sliding\nwindows to predict the immediate next step. Empirical evaluations conducted\nusing our bespoke O-RAN testbed KPI time-series dataset (59,441 windows across\n13 KPIs). Crucially for O-RAN constraints, MS3M achieves a 0.057 s\nper-inference latency with 0.70M parameters, yielding 3-10x lower latency than\nthe Transformer baselines evaluated on the same hardware, while maintaining\ncompetitive accuracy.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u8f7b\u91cf\u7ea7\u591a\u5c3a\u5ea6\u7ed3\u6784\u5316\u72b6\u6001\u7a7a\u95f4\u6df7\u5408\u6a21\u578b\uff08MS3M\uff09\uff0c\u7528\u4e8e\u57286G O-RAN\u8fd1\u5b9e\u65f6\u7ea6\u675f\u4e0b\u9ad8\u6548\u9884\u6d4b\u7528\u6237\u8bbe\u5907\u7684\u5173\u952e\u6027\u80fd\u6307\u6807\uff08KPI\uff09\uff0c\u5728\u4fdd\u6301\u51c6\u786e\u7387\u7684\u540c\u65f6\u663e\u8457\u964d\u4f4e\u63a8\u7406\u5ef6\u8fdf\u548c\u6a21\u578b\u590d\u6742\u5ea6\u3002", "motivation": "\u57286G O-RAN\u4e2d\uff0c\u5b9e\u73b0\u8fd1\u5b9e\u65f6\u3001\u4f4e\u5f00\u9500\u7684\u63a7\u5236\u7ea7\u9884\u6d4b\u9762\u4e34\u591a\u65f6\u95f4\u5c3a\u5ea6\u52a8\u6001\u3001\u5ef6\u8fdf\u548c\u8ba1\u7b97\u8d44\u6e90\u9650\u5236\u7b49\u6311\u6218\uff0c\u800c\u4f20\u7edfTransformer\u6a21\u578b\u56e0\u5185\u5b58\u6d88\u8017\u5927\u96be\u4ee5\u6ee1\u8db3\u8fd9\u4e9b\u8981\u6c42\u3002", "method": "\u63d0\u51faMS3M\u6a21\u578b\uff0c\u878d\u5408HiPPO-LegS\u6838\u4ee5\u6355\u6349\u591a\u5c3a\u5ea6\u65e0\u7ebf\u52a8\u6001\uff1b\u91c7\u7528\u53cc\u7ebf\u6027\u79bb\u6563\u5316\u6784\u5efa\u7a33\u5b9a\u7684\u72b6\u6001\u7a7a\u95f4\u6a21\u578b\uff0c\u5e76\u5c06\u5176\u56e0\u679c\u8109\u51b2\u54cd\u5e94\u4f5c\u4e3a\u6df1\u5ea6\u53ef\u5206\u79bb\u5377\u79ef\uff1b\u7ed3\u5408Squeeze-and-Excitation\u95e8\u63a7\u673a\u5236\u52a8\u6001\u8c03\u6574KPI\u901a\u9053\u6743\u91cd\uff0c\u5e76\u901a\u8fc7\u7d27\u51d1\u7684\u95e8\u63a7\u901a\u9053\u6df7\u5408\u5c42\u5efa\u6a21\u8de8\u7279\u5f81\u975e\u7ebf\u6027\u3002", "result": "\u5728\u81ea\u5efaO-RAN\u6d4b\u8bd5\u5e73\u53f0\u6570\u636e\u96c6\uff0813\u4e2aKPI\uff0c59,441\u4e2a\u6ed1\u52a8\u7a97\u53e3\uff09\u4e0a\u8bc4\u4f30\uff0cMS3M\u63a8\u7406\u5ef6\u8fdf\u4ec5\u4e3a0.057\u79d2\uff0c\u53c2\u6570\u91cf0.70M\uff0c\u5728\u76f8\u540c\u786c\u4ef6\u4e0a\u6bd4Transformer\u57fa\u7ebf\u5feb3\u201310\u500d\uff0c\u540c\u65f6\u4fdd\u6301\u6709\u7ade\u4e89\u529b\u7684\u9884\u6d4b\u7cbe\u5ea6\u3002", "conclusion": "MS3M\u662f\u4e00\u79cd\u9002\u7528\u4e8eO-RAN\u8fd1\u5b9e\u65f6\u667a\u80fd\u63a7\u5236\u5668\u7684\u9ad8\u6548\u3001\u8f7b\u91cf\u4e14KPI\u65e0\u5173\u7684\u9884\u6d4b\u6a21\u578b\uff0c\u80fd\u6709\u6548\u652f\u6301\u524d\u77bb\u6027\u65e0\u7ebf\u8d44\u6e90\u63a7\u5236\u3002"}}
{"id": "2510.05147", "categories": ["cs.SE", "cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.05147", "abs": "https://arxiv.org/abs/2510.05147", "authors": ["Yu Zhu"], "title": "Adaptive Reinforcement Learning for Dynamic Configuration Allocation in Pre-Production Testing", "comment": null, "summary": "Ensuring reliability in modern software systems requires rigorous\npre-production testing across highly heterogeneous and evolving environments.\nBecause exhaustive evaluation is infeasible, practitioners must decide how to\nallocate limited testing resources across configurations where failure\nprobabilities may drift over time. Existing combinatorial optimization\napproaches are static, ad hoc, and poorly suited to such non-stationary\nsettings. We introduce a novel reinforcement learning (RL) framework that\nrecasts configuration allocation as a sequential decision-making problem. Our\nmethod is the first to integrate Q-learning with a hybrid reward design that\nfuses simulated outcomes and real-time feedback, enabling both sample\nefficiency and robustness. In addition, we develop an adaptive online-offline\ntraining scheme that allows the agent to quickly track abrupt probability\nshifts while maintaining long-run stability. Extensive simulation studies\ndemonstrate that our approach consistently outperforms static and\noptimization-based baselines, approaching oracle performance. This work\nestablishes RL as a powerful new paradigm for adaptive configuration\nallocation, advancing beyond traditional methods and offering broad\napplicability to dynamic testing and resource scheduling domains.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u52a8\u6001\u914d\u7f6e\u5206\u914d\u65b9\u6cd5\uff0c\u901a\u8fc7\u7ed3\u5408Q\u5b66\u4e60\u4e0e\u6df7\u5408\u5956\u52b1\u673a\u5236\uff0c\u5728\u975e\u5e73\u7a33\u73af\u5883\u4e2d\u9ad8\u6548\u3001\u7a33\u5065\u5730\u5206\u914d\u6d4b\u8bd5\u8d44\u6e90\u3002", "motivation": "\u73b0\u4ee3\u8f6f\u4ef6\u7cfb\u7edf\u9700\u5728\u9ad8\u5ea6\u5f02\u6784\u4e14\u4e0d\u65ad\u6f14\u5316\u7684\u73af\u5883\u4e2d\u8fdb\u884c\u53ef\u9760\u6d4b\u8bd5\uff0c\u4f46\u7a77\u4e3e\u6d4b\u8bd5\u4e0d\u53ef\u884c\uff0c\u4e14\u6545\u969c\u6982\u7387\u4f1a\u968f\u65f6\u95f4\u6f02\u79fb\uff0c\u73b0\u6709\u9759\u6001\u7ec4\u5408\u4f18\u5316\u65b9\u6cd5\u96be\u4ee5\u5e94\u5bf9\u8fd9\u79cd\u975e\u5e73\u7a33\u573a\u666f\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u5c06\u914d\u7f6e\u5206\u914d\u5efa\u6a21\u4e3a\u5e8f\u5217\u51b3\u7b56\u95ee\u9898\uff0c\u7ed3\u5408Q\u5b66\u4e60\u4e0e\u878d\u5408\u6a21\u62df\u7ed3\u679c\u548c\u5b9e\u65f6\u53cd\u9988\u7684\u6df7\u5408\u5956\u52b1\u8bbe\u8ba1\uff0c\u5e76\u91c7\u7528\u81ea\u9002\u5e94\u7684\u5728\u7ebf-\u79bb\u7ebf\u8bad\u7ec3\u7b56\u7565\u3002", "result": "\u5927\u91cf\u4eff\u771f\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u6027\u80fd\u4e0a\u6301\u7eed\u4f18\u4e8e\u9759\u6001\u548c\u57fa\u4e8e\u4f18\u5316\u7684\u57fa\u7ebf\u65b9\u6cd5\uff0c\u63a5\u8fd1\u7406\u60f3Oracle\u8868\u73b0\u3002", "conclusion": "\u5f3a\u5316\u5b66\u4e60\u4e3a\u52a8\u6001\u6d4b\u8bd5\u8d44\u6e90\u914d\u7f6e\u63d0\u4f9b\u4e86\u5f3a\u5927\u65b0\u8303\u5f0f\uff0c\u8d85\u8d8a\u4f20\u7edf\u65b9\u6cd5\uff0c\u5177\u6709\u5728\u52a8\u6001\u6d4b\u8bd5\u4e0e\u8d44\u6e90\u8c03\u5ea6\u9886\u57df\u5e7f\u6cdb\u5e94\u7528\u7684\u6f5c\u529b\u3002"}}
{"id": "2510.05476", "categories": ["cs.DC", "cs.AR", "cs.NI"], "pdf": "https://arxiv.org/pdf/2510.05476", "abs": "https://arxiv.org/abs/2510.05476", "authors": ["Xi Wang", "Bin Ma", "Jongryool Kim", "Byungil Koh", "Hoshik Kim", "Dong Li"], "title": "cMPI: Using CXL Memory Sharing for MPI One-Sided and Two-Sided Inter-Node Communications", "comment": null, "summary": "Message Passing Interface (MPI) is a foundational programming model for\nhigh-performance computing. MPI libraries traditionally employ network\ninterconnects (e.g., Ethernet and InfiniBand) and network protocols (e.g., TCP\nand RoCE) with complex software stacks for cross-node communication. We present\ncMPI, the first work to optimize MPI point-to-point communication (both\none-sided and two-sided) using CXL memory sharing on a real CXL platform,\ntransforming cross-node communication into memory transactions and data copies\nwithin CXL memory, bypassing traditional network protocols. We analyze\nperformance across various interconnects and find that CXL memory sharing\nachieves 7.2x-8.1x lower latency than TCP-based interconnects deployed in\nsmall- and medium-scale clusters. We address challenges of CXL memory sharing\nfor MPI communication, including data object management over the dax\nrepresentation [50], cache coherence, and atomic operations. Overall, cMPI\noutperforms TCP over standard Ethernet NIC and high-end SmartNIC by up to 49x\nand 72x in latency and bandwidth, respectively, for small messages.", "AI": {"tldr": "cMPI \u662f\u9996\u4e2a\u5229\u7528 CXL \u5185\u5b58\u5171\u4eab\u4f18\u5316 MPI \u70b9\u5bf9\u70b9\u901a\u4fe1\u7684\u5de5\u4f5c\uff0c\u901a\u8fc7\u5c06\u8de8\u8282\u70b9\u901a\u4fe1\u8f6c\u5316\u4e3a CXL \u5185\u5b58\u4e2d\u7684\u5185\u5b58\u4e8b\u52a1\u548c\u6570\u636e\u62f7\u8d1d\uff0c\u663e\u8457\u964d\u4f4e\u5ef6\u8fdf\u5e76\u63d0\u5347\u5e26\u5bbd\u3002", "motivation": "\u4f20\u7edf MPI \u5e93\u4f9d\u8d56\u590d\u6742\u7684\u7f51\u7edc\u534f\u8bae\u6808\uff08\u5982 TCP\u3001RoCE\uff09\u548c\u4e92\u8fde\u7f51\u7edc\uff08\u5982\u4ee5\u592a\u7f51\u3001InfiniBand\uff09\u8fdb\u884c\u8de8\u8282\u70b9\u901a\u4fe1\uff0c\u5b58\u5728\u9ad8\u5ef6\u8fdf\u548c\u4f4e\u6548\u95ee\u9898\u3002\u4f5c\u8005\u65e8\u5728\u5229\u7528 CXL \u5185\u5b58\u5171\u4eab\u673a\u5236\u7b80\u5316\u901a\u4fe1\u8def\u5f84\uff0c\u63d0\u5347\u6027\u80fd\u3002", "method": "cMPI \u5229\u7528\u771f\u5b9e CXL \u5e73\u53f0\u4e0a\u7684 CXL \u5185\u5b58\u5171\u4eab\u673a\u5236\uff0c\u5c06 MPI \u7684\u5355\u8fb9\u548c\u53cc\u8fb9\u70b9\u5bf9\u70b9\u901a\u4fe1\u8f6c\u5316\u4e3a CXL \u5185\u5b58\u4e2d\u7684\u5185\u5b58\u4e8b\u52a1\u548c\u6570\u636e\u62f7\u8d1d\uff0c\u7ed5\u8fc7\u4f20\u7edf\u7f51\u7edc\u534f\u8bae\uff0c\u5e76\u89e3\u51b3 dax \u8868\u793a\u4e0b\u7684\u6570\u636e\u5bf9\u8c61\u7ba1\u7406\u3001\u7f13\u5b58\u4e00\u81f4\u6027\u548c\u539f\u5b50\u64cd\u4f5c\u7b49\u6311\u6218\u3002", "result": "\u5728\u5c0f\u89c4\u6a21\u548c\u4e2d\u7b49\u89c4\u6a21\u96c6\u7fa4\u4e2d\uff0cCXL \u5185\u5b58\u5171\u4eab\u76f8\u6bd4\u57fa\u4e8e TCP \u7684\u4e92\u8fde\u53ef\u5b9e\u73b0 7.2x\u20138.1x \u7684\u5ef6\u8fdf\u964d\u4f4e\uff1b\u5bf9\u4e8e\u5c0f\u6d88\u606f\uff0ccMPI \u76f8\u6bd4\u6807\u51c6\u4ee5\u592a\u7f51 NIC \u548c\u9ad8\u7aef SmartNIC\uff0c\u5728\u5ef6\u8fdf\u548c\u5e26\u5bbd\u4e0a\u5206\u522b\u6700\u9ad8\u63d0\u5347 49 \u500d\u548c 72 \u500d\u3002", "conclusion": "cMPI \u8bc1\u660e\u4e86\u5229\u7528 CXL \u5185\u5b58\u5171\u4eab\u4f18\u5316 MPI \u901a\u4fe1\u7684\u53ef\u884c\u6027\u4e0e\u663e\u8457\u6027\u80fd\u4f18\u52bf\uff0c\u4e3a\u9ad8\u6027\u80fd\u8ba1\u7b97\u4e2d\u7684\u4f4e\u5ef6\u8fdf\u901a\u4fe1\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\u3002"}}
{"id": "2510.05112", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2510.05112", "abs": "https://arxiv.org/abs/2510.05112", "authors": ["Lijuan Jiang", "Xingjian Qian", "Zhenxiang Ma", "Zan Zong", "Hengjie Li", "Chao Yang", "Jidong Zhai"], "title": "A Flexible Programmable Pipeline Parallelism Framework for Efficient DNN Training", "comment": null, "summary": "Pipeline parallelism is an essential distributed parallelism method.\nIncreasingly complex and diverse DNN models necessitate meticulously customized\npipeline schedules for performance. However, existing practices typically rely\non predefined schedules, each with strengths, but fail to adapt automatically\nto the emerging model architectures. Exploring novel high-efficiency schedules\nis daunting due to the enormous and varying schedule space. Besides, manually\nimplementing schedules can be challenging due to the onerous coding burdens and\nconstantly changing needs. Unfortunately, existing frameworks have limitations\nin automated schedule exploration and lack flexibility and controllability.\n  This paper presents FlexPipe, a programmable pipeline parallelism framework\nwith enhanced productivity, programmability, debuggability, and ease of tuning.\nFlexPipe has two main components: a succinct domain-specific language (DSL) and\nan automated scheduler. FlexPipe enables automated schedule exploration for\nvarious parallel scenarios within a broad spectrum of schedule types at a small\nsearch cost. Besides, users can swiftly develop and customize schedules using\nthe FlexPipe DSL, which embodies flexible controllability in the pipeline order\nof micro-batch computations over stages. It also provides convenient mechanisms\nto include new operations in schedules to meet changing demands. Our evaluation\nresults demonstrate that FlexPipe achieves up to 2.28X performance speedup\ncompared to the popular large-scale parallel framework Megtron-LM, and gains up\nto 1.49X performance speedup compared to the state-of-the-art automated\npipeline parallelism framework.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86FlexPipe\uff0c\u4e00\u4e2a\u53ef\u7f16\u7a0b\u7684\u6d41\u6c34\u7ebf\u5e76\u884c\u6846\u67b6\uff0c\u901a\u8fc7\u9886\u57df\u7279\u5b9a\u8bed\u8a00\uff08DSL\uff09\u548c\u81ea\u52a8\u8c03\u5ea6\u5668\uff0c\u5b9e\u73b0\u9ad8\u6548\u3001\u7075\u6d3b\u4e14\u53ef\u5b9a\u5236\u7684\u6d41\u6c34\u7ebf\u8c03\u5ea6\uff0c\u5728\u6027\u80fd\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u6846\u67b6\u3002", "motivation": "\u73b0\u6709\u6d41\u6c34\u7ebf\u5e76\u884c\u65b9\u6cd5\u4f9d\u8d56\u9884\u5b9a\u4e49\u8c03\u5ea6\u7b56\u7565\uff0c\u96be\u4ee5\u81ea\u52a8\u9002\u914d\u4e0d\u65ad\u6f14\u5316\u7684DNN\u6a21\u578b\u67b6\u6784\uff1b\u540c\u65f6\uff0c\u624b\u52a8\u8bbe\u8ba1\u9ad8\u6548\u8c03\u5ea6\u65b9\u6848\u6210\u672c\u9ad8\u3001\u7075\u6d3b\u6027\u5dee\uff0c\u73b0\u6709\u6846\u67b6\u5728\u81ea\u52a8\u5316\u63a2\u7d22\u548c\u53ef\u63a7\u6027\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\u3002", "method": "FlexPipe\u5305\u542b\u4e24\u4e2a\u6838\u5fc3\u7ec4\u4ef6\uff1a\u4e00\u79cd\u7b80\u6d01\u7684\u9886\u57df\u7279\u5b9a\u8bed\u8a00\uff08DSL\uff09\u7528\u4e8e\u7075\u6d3b\u5b9a\u4e49\u548c\u5b9a\u5236\u6d41\u6c34\u7ebf\u8c03\u5ea6\uff0c\u4ee5\u53ca\u4e00\u4e2a\u81ea\u52a8\u8c03\u5ea6\u5668\uff0c\u53ef\u5728\u5e7f\u6cdb\u7684\u8c03\u5ea6\u7a7a\u95f4\u4e2d\u4ee5\u8f83\u4f4e\u641c\u7d22\u5f00\u9500\u81ea\u52a8\u63a2\u7d22\u9ad8\u6548\u8c03\u5ea6\u7b56\u7565\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cFlexPipe\u76f8\u6bd4\u4e3b\u6d41\u5927\u89c4\u6a21\u5e76\u884c\u6846\u67b6Megatron-LM\u6700\u9ad8\u63d0\u901f2.28\u500d\uff0c\u76f8\u6bd4\u5f53\u524d\u6700\u5148\u8fdb\u7684\u81ea\u52a8\u6d41\u6c34\u7ebf\u5e76\u884c\u6846\u67b6\u6700\u9ad8\u63d0\u901f1.49\u500d\u3002", "conclusion": "FlexPipe\u901a\u8fc7\u53ef\u7f16\u7a0b\u6027\u548c\u81ea\u52a8\u5316\u8c03\u5ea6\uff0c\u5728\u63d0\u5347\u6d41\u6c34\u7ebf\u5e76\u884c\u6548\u7387\u7684\u540c\u65f6\u589e\u5f3a\u4e86\u7075\u6d3b\u6027\u3001\u53ef\u8c03\u4f18\u6027\u548c\u5f00\u53d1\u6548\u7387\uff0c\u4e3a\u590d\u6742\u591a\u53d8\u7684DNN\u6a21\u578b\u63d0\u4f9b\u4e86\u66f4\u4f18\u7684\u5e76\u884c\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.05185", "categories": ["cs.MA", "cs.CE", "cs.CY", "cs.NE", "cs.SI"], "pdf": "https://arxiv.org/pdf/2510.05185", "abs": "https://arxiv.org/abs/2510.05185", "authors": ["Vrinda Malhotra", "Jiaman Li", "Nandini Pisupati"], "title": "AgentZero++: Modeling Fear-Based Behavior", "comment": null, "summary": "We present AgentZero++, an agent-based model that integrates cognitive,\nemotional, and social mechanisms to simulate decentralized collective violence\nin spatially distributed systems. Building on Epstein's Agent\\_Zero framework,\nwe extend the original model with eight behavioral enhancements: age-based\nimpulse control; memory-based risk estimation; affect-cognition coupling;\nendogenous destructive radius; fight-or-flight dynamics; affective homophily;\nretaliatory damage; and multi-agent coordination. These additions allow agents\nto adapt based on internal states, previous experiences, and social feedback,\nproducing emergent dynamics such as protest asymmetries, escalation cycles, and\nlocalized retaliation. Implemented in Python using the Mesa ABM framework,\nAgentZero++ enables modular experimentation and visualization of how\nmicro-level cognitive heterogeneity shapes macro-level conflict patterns. Our\nresults highlight how small variations in memory, reactivity, and affective\nalignment can amplify or dampen unrest through feedback loops. By explicitly\nmodeling emotional thresholds, identity-driven behavior, and adaptive networks,\nthis work contributes a flexible and extensible platform for analyzing\naffective contagion and psychologically grounded collective action.", "AI": {"tldr": "AgentZero++ \u662f\u4e00\u4e2a\u57fa\u4e8e Agent \u7684\u6a21\u578b\uff0c\u901a\u8fc7\u6574\u5408\u8ba4\u77e5\u3001\u60c5\u611f\u548c\u793e\u4f1a\u673a\u5236\uff0c\u6a21\u62df\u7a7a\u95f4\u5206\u5e03\u7cfb\u7edf\u4e2d\u7684\u53bb\u4e2d\u5fc3\u5316\u96c6\u4f53\u66b4\u529b\uff0c\u6269\u5c55\u4e86\u539f\u59cb Agent_Zero \u6846\u67b6\u5e76\u5f15\u5165\u516b\u9879\u884c\u4e3a\u589e\u5f3a\u673a\u5236\u3002", "motivation": "\u73b0\u6709\u6a21\u578b\u5728\u6a21\u62df\u96c6\u4f53\u66b4\u529b\u65f6\u7f3a\u4e4f\u5bf9\u4e2a\u4f53\u8ba4\u77e5\u3001\u60c5\u611f\u53ca\u793e\u4f1a\u4e92\u52a8\u7684\u7efc\u5408\u5efa\u6a21\uff0c\u96be\u4ee5\u6355\u6349\u5fae\u89c2\u5fc3\u7406\u673a\u5236\u5982\u4f55\u9a71\u52a8\u5b8f\u89c2\u51b2\u7a81\u6a21\u5f0f\u3002", "method": "\u5728 Epstein \u7684 Agent_Zero \u6846\u67b6\u57fa\u7840\u4e0a\uff0c\u5f15\u5165\u516b\u9879\u884c\u4e3a\u589e\u5f3a\u673a\u5236\uff08\u5982\u57fa\u4e8e\u5e74\u9f84\u7684\u51b2\u52a8\u63a7\u5236\u3001\u8bb0\u5fc6\u98ce\u9669\u8bc4\u4f30\u3001\u60c5\u611f-\u8ba4\u77e5\u8026\u5408\u7b49\uff09\uff0c\u4f7f\u7528 Python \u548c Mesa ABM \u6846\u67b6\u5b9e\u73b0\u6a21\u5757\u5316\u5efa\u6a21\u4e0e\u53ef\u89c6\u5316\u3002", "result": "\u6a21\u578b\u6210\u529f\u518d\u73b0\u4e86\u6297\u8bae\u4e0d\u5bf9\u79f0\u6027\u3001\u5347\u7ea7\u5faa\u73af\u548c\u5c40\u90e8\u62a5\u590d\u7b49\u6d8c\u73b0\u52a8\u6001\uff0c\u8868\u660e\u8bb0\u5fc6\u3001\u53cd\u5e94\u6027\u548c\u60c5\u611f\u4e00\u81f4\u6027\u7b49\u5fae\u5c0f\u5dee\u5f02\u53ef\u901a\u8fc7\u53cd\u9988\u56de\u8def\u663e\u8457\u653e\u5927\u6216\u6291\u5236\u793e\u4f1a\u52a8\u8361\u3002", "conclusion": "AgentZero++ \u63d0\u4f9b\u4e86\u4e00\u4e2a\u7075\u6d3b\u53ef\u6269\u5c55\u7684\u5e73\u53f0\uff0c\u6709\u52a9\u4e8e\u6df1\u5165\u7406\u89e3\u60c5\u611f\u4f20\u67d3\u548c\u57fa\u4e8e\u5fc3\u7406\u673a\u5236\u7684\u96c6\u4f53\u884c\u52a8\uff0c\u5f3a\u8c03\u5fae\u89c2\u8ba4\u77e5\u5f02\u8d28\u6027\u5bf9\u5b8f\u89c2\u51b2\u7a81\u6a21\u5f0f\u7684\u5173\u952e\u5f71\u54cd\u3002"}}
{"id": "2510.05290", "categories": ["cs.NI", "cs.ET"], "pdf": "https://arxiv.org/pdf/2510.05290", "abs": "https://arxiv.org/abs/2510.05290", "authors": ["Manuel Eppler", "Steffen Lindner", "Lukas Osswald", "Thomas St\u00fcber", "Michael Menth"], "title": "Impact of Packet Loss and Timing Errors on Scheduled Periodic Traffic with Time-Aware Shaping (TAS) in Time-Sensitive Networking (TSN)", "comment": null, "summary": "Time-Sensitive Networking (TSN) is a collection of mechanisms to enhance the\nrealtime transmission capability of Ethernet networks. TSN combines priority\nqueuing, traffic scheduling, and the Time-Aware Shaper (TAS) to carry periodic\ntraffic with ultra-low latency and jitter. That is, so-called Talkers send\nperiodic traffic with highest priority according to a schedule. The schedule is\ndesigned such that the scheduled traffic is forwarded by the TSN bridges with\nno or only little queuing delay. To protect that traffic against other frames,\nthe TAS is configured on all interfaces such that lower-priority queues can\nsend only when high-priority traffic is not supposed to be forwarded. In the\nliterature on scheduling algorithms for the TAS there is mostly the explicit or\nimplicit assumption that the TAS also limits transmission slots of\nhigh-priority traffic.\n  In this paper we show that this assumption can lead to tremendous problems\nlike very long queuing delay or even packet loss in case of faulty frames. A\nfaulty frame arrives too early or too late according to the schedule, it is\nmissing or additional. We construct minimal examples to illustrate basic\neffects of faulty frames on a single link and demonstrate how this effect can\npropagate through the networks and cause remote problems. We further show using\nsimulations that a single slightly delayed frame may lead to frame loss on\nmultiple links. We show that these problems can be alleviated or avoided when\nTAS-based transmission slots for high-priority traffic are configured longer\nthan needed or if they are not limited at all.", "AI": {"tldr": "\u672c\u6587\u6307\u51fa\u5728\u65f6\u95f4\u654f\u611f\u7f51\u7edc\uff08TSN\uff09\u4e2d\uff0c\u82e5\u5bf9\u9ad8\u4f18\u5148\u7ea7\u6d41\u91cf\u7684\u4f20\u8f93\u65f6\u9699\u8fdb\u884c\u4e25\u683c\u9650\u5236\uff0c\u6545\u969c\u5e27\uff08\u5982\u8fc7\u65e9\u3001\u8fc7\u665a\u3001\u7f3a\u5931\u6216\u591a\u4f59\u5e27\uff09\u53ef\u80fd\u5bfc\u81f4\u4e25\u91cd\u6392\u961f\u5ef6\u8fdf\u751a\u81f3\u4e22\u5305\uff0c\u5e76\u901a\u8fc7\u793a\u4f8b\u548c\u4eff\u771f\u9a8c\u8bc1\u8be5\u95ee\u9898\uff0c\u63d0\u51fa\u5ef6\u957f\u6216\u4e0d\u9650\u5236\u9ad8\u4f18\u5148\u7ea7\u65f6\u9699\u53ef\u7f13\u89e3\u6b64\u95ee\u9898\u3002", "motivation": "\u73b0\u6709TSN\u8c03\u5ea6\u7b97\u6cd5\u901a\u5e38\u5047\u8bbe\u9ad8\u4f18\u5148\u7ea7\u6d41\u91cf\u7684\u4f20\u8f93\u65f6\u9699\u4e5f\u53d7TAS\u9650\u5236\uff0c\u4f46\u8fd9\u79cd\u5047\u8bbe\u5728\u51fa\u73b0\u6545\u969c\u5e27\u65f6\u53ef\u80fd\u5bfc\u81f4\u4e25\u91cd\u6027\u80fd\u95ee\u9898\uff0c\u5982\u5ef6\u8fdf\u548c\u4e22\u5305\uff0c\u56e0\u6b64\u9700\u8981\u91cd\u65b0\u5ba1\u89c6\u8be5\u5047\u8bbe\u5e76\u63d0\u51fa\u6539\u8fdb\u65b9\u6cd5\u3002", "method": "\u901a\u8fc7\u6784\u5efa\u6700\u5c0f\u5316\u793a\u4f8b\u8bf4\u660e\u6545\u969c\u5e27\u5728\u5355\u94fe\u8def\u4e0a\u7684\u5f71\u54cd\uff0c\u5e76\u901a\u8fc7\u4eff\u771f\u5c55\u793a\u5176\u5728\u7f51\u7edc\u4e2d\u7684\u4f20\u64ad\u6548\u5e94\uff1b\u540c\u65f6\u8bc4\u4f30\u5ef6\u957f\u6216\u4e0d\u9650\u5236\u9ad8\u4f18\u5148\u7ea7TAS\u65f6\u9699\u5bf9\u7f13\u89e3\u95ee\u9898\u7684\u6548\u679c\u3002", "result": "\u4eff\u771f\u7ed3\u679c\u8868\u660e\uff0c\u5373\u4f7f\u5355\u4e2a\u8f7b\u5fae\u5ef6\u8fdf\u7684\u5e27\u4e5f\u53ef\u80fd\u5728\u591a\u4e2a\u94fe\u8def\u4e0a\u5f15\u53d1\u4e22\u5305\uff1b\u800c\u5ef6\u957f\u6216\u4e0d\u9650\u5236\u9ad8\u4f18\u5148\u7ea7\u6d41\u91cf\u7684TAS\u65f6\u9699\u53ef\u6709\u6548\u7f13\u89e3\u6216\u907f\u514d\u6b64\u7c7b\u95ee\u9898\u3002", "conclusion": "TSN\u4e2d\u5bf9\u9ad8\u4f18\u5148\u7ea7\u6d41\u91cf\u4e25\u683c\u9650\u5236TAS\u65f6\u9699\u7684\u505a\u6cd5\u5b58\u5728\u9690\u60a3\uff0c\u5e94\u8003\u8651\u914d\u7f6e\u66f4\u5bbd\u677e\u6216\u4e0d\u9650\u5236\u7684\u65f6\u9699\u4ee5\u63d0\u5347\u7cfb\u7edf\u9c81\u68d2\u6027\u3002"}}
{"id": "2510.05156", "categories": ["cs.SE", "cs.AI", "cs.CR", "I.2.7"], "pdf": "https://arxiv.org/pdf/2510.05156", "abs": "https://arxiv.org/abs/2510.05156", "authors": ["Lesly Miculicich", "Mihir Parmar", "Hamid Palangi", "Krishnamurthy Dj Dvijotham", "Mirko Montanari", "Tomas Pfister", "Long T. Le"], "title": "VeriGuard: Enhancing LLM Agent Safety via Verified Code Generation", "comment": "22 pages", "summary": "The deployment of autonomous AI agents in sensitive domains, such as\nhealthcare, introduces critical risks to safety, security, and privacy. These\nagents may deviate from user objectives, violate data handling policies, or be\ncompromised by adversarial attacks. Mitigating these dangers necessitates a\nmechanism to formally guarantee that an agent's actions adhere to predefined\nsafety constraints, a challenge that existing systems do not fully address. We\nintroduce VeriGuard, a novel framework that provides formal safety guarantees\nfor LLM-based agents through a dual-stage architecture designed for robust and\nverifiable correctness. The initial offline stage involves a comprehensive\nvalidation process. It begins by clarifying user intent to establish precise\nsafety specifications. VeriGuard then synthesizes a behavioral policy and\nsubjects it to both testing and formal verification to prove its compliance\nwith these specifications. This iterative process refines the policy until it\nis deemed correct. Subsequently, the second stage provides online action\nmonitoring, where VeriGuard operates as a runtime monitor to validate each\nproposed agent action against the pre-verified policy before execution. This\nseparation of the exhaustive offline validation from the lightweight online\nmonitoring allows formal guarantees to be practically applied, providing a\nrobust safeguard that substantially improves the trustworthiness of LLM agents.", "AI": {"tldr": "VeriGuard \u662f\u4e00\u4e2a\u4e3a\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u667a\u80fd\u4f53\u63d0\u4f9b\u5f62\u5f0f\u5316\u5b89\u5168\u4fdd\u8bc1\u7684\u65b0\u6846\u67b6\uff0c\u901a\u8fc7\u79bb\u7ebf\u9a8c\u8bc1\u4e0e\u5728\u7ebf\u76d1\u63a7\u4e24\u9636\u6bb5\u67b6\u6784\uff0c\u786e\u4fdd\u5176\u884c\u4e3a\u59cb\u7ec8\u7b26\u5408\u9884\u5b9a\u4e49\u7684\u5b89\u5168\u7ea6\u675f\u3002", "motivation": "\u5728\u533b\u7597\u7b49\u654f\u611f\u9886\u57df\u90e8\u7f72\u81ea\u4e3bAI\u667a\u80fd\u4f53\u5b58\u5728\u5b89\u5168\u3001\u9690\u79c1\u548c\u5bf9\u6297\u653b\u51fb\u7b49\u98ce\u9669\uff0c\u73b0\u6709\u7cfb\u7edf\u7f3a\u4e4f\u5bf9\u667a\u80fd\u4f53\u884c\u4e3a\u63d0\u4f9b\u5f62\u5f0f\u5316\u5b89\u5168\u4fdd\u8bc1\u7684\u6709\u6548\u673a\u5236\u3002", "method": "VeriGuard \u91c7\u7528\u53cc\u9636\u6bb5\u67b6\u6784\uff1a\u79bb\u7ebf\u9636\u6bb5\u660e\u786e\u7528\u6237\u610f\u56fe\u3001\u751f\u6210\u884c\u4e3a\u7b56\u7565\u5e76\u8fdb\u884c\u6d4b\u8bd5\u4e0e\u5f62\u5f0f\u5316\u9a8c\u8bc1\uff1b\u5728\u7ebf\u9636\u6bb5\u4f5c\u4e3a\u8fd0\u884c\u65f6\u76d1\u63a7\u5668\uff0c\u5bf9\u6bcf\u4e2a\u52a8\u4f5c\u8fdb\u884c\u5b9e\u65f6\u9a8c\u8bc1\u3002", "result": "\u8be5\u6846\u67b6\u5b9e\u73b0\u4e86\u5bf9LLM\u667a\u80fd\u4f53\u884c\u4e3a\u7684\u5f62\u5f0f\u5316\u5b89\u5168\u4fdd\u8bc1\uff0c\u5728\u4fdd\u969c\u6b63\u786e\u6027\u7684\u540c\u65f6\u517c\u987e\u8fd0\u884c\u6548\u7387\uff0c\u663e\u8457\u63d0\u5347\u4e86\u667a\u80fd\u4f53\u7684\u53ef\u4fe1\u5ea6\u3002", "conclusion": "\u901a\u8fc7\u5206\u79bb\u7e41\u91cd\u7684\u79bb\u7ebf\u9a8c\u8bc1\u4e0e\u8f7b\u91cf\u7684\u5728\u7ebf\u76d1\u63a7\uff0cVeriGuard \u4e3aLLM\u667a\u80fd\u4f53\u5728\u654f\u611f\u573a\u666f\u4e2d\u7684\u5b89\u5168\u53ef\u9760\u90e8\u7f72\u63d0\u4f9b\u4e86\u5b9e\u7528\u4e14\u53ef\u9a8c\u8bc1\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.05497", "categories": ["cs.DC", "cs.AI", "cs.AR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.05497", "abs": "https://arxiv.org/abs/2510.05497", "authors": ["Zhongkai Yu", "Yue Guan", "Zihao Yu", "Chenyang Zhou", "Shuyi Pei", "Yangwook Kang", "Yufei Ding", "Po-An Tsai"], "title": "Orders in Chaos: Enhancing Large-Scale MoE LLM Serving with Data Movement Forecasting", "comment": null, "summary": "Large Language Models (LLMs) with Mixture of Experts (MoE) architectures\nachieve remarkable performance improvements, but their random expert selection\nmechanism introduces significant data movement overhead that becomes the\ndominant bottleneck in multi-unit serving systems. To forecast the patterns\nunderlying this data movement, we conduct comprehensive data-movement-centric\nprofiling across three state-of-the-art large-scale MoE models (200B- 671B)\nusing over 24,000 requests spanning diverse workloads. With the resulting\n150GB+ trace files, we perform systematic analysis from both temporal and\nspatial perspectives and distill six key insights to guide the design of\ndiverse future serving systems. Taking wafer-scale GPUs as a case study, we\ndemonstrate that minor architectural modifications leveraging our insights\nachieve substantial performance gains, delivering 6.3X and 4.0X average\nspeedups on DeepSeek V3 and Qwen3, respectively. Our work provides the first\ncomprehensive data-centric analysis of MoE models at scale. Our profiling\ntraces and analysis results are publicly available at\n{https://huggingface.co/datasets/core12345/MoE_expert_selection_trace. We will\nalso release our simulation framework shortly to facilitate future research in\nthis area.", "AI": {"tldr": "\u672c\u6587\u9996\u6b21\u5bf9\u5927\u89c4\u6a21Mixture of Experts\uff08MoE\uff09\u6a21\u578b\u7684\u6570\u636e\u79fb\u52a8\u884c\u4e3a\u8fdb\u884c\u4e86\u5168\u9762\u5206\u6790\uff0c\u901a\u8fc7\u5206\u6790150GB\u4ee5\u4e0a\u7684\u4e13\u5bb6\u9009\u62e9\u8f68\u8ff9\uff0c\u63d0\u70bc\u51fa\u516d\u9879\u5173\u952e\u6d1e\u5bdf\uff0c\u5e76\u4ee5\u6676\u5706\u7ea7GPU\u4e3a\u4f8b\uff0c\u5c55\u793a\u4e86\u57fa\u4e8e\u8fd9\u4e9b\u6d1e\u5bdf\u7684\u5fae\u5c0f\u67b6\u6784\u6539\u8fdb\u53ef\u5e26\u6765\u663e\u8457\u6027\u80fd\u63d0\u5347\uff08DeepSeek V3\u548cQwen3\u5206\u522b\u63d0\u901f6.3\u500d\u548c4.0\u500d\uff09\u3002", "motivation": "MoE\u67b6\u6784\u7684\u5927\u8bed\u8a00\u6a21\u578b\u867d\u6027\u80fd\u5353\u8d8a\uff0c\u4f46\u5176\u968f\u673a\u4e13\u5bb6\u9009\u62e9\u673a\u5236\u5728\u591a\u5355\u5143\u90e8\u7f72\u7cfb\u7edf\u4e2d\u5f15\u53d1\u4e25\u91cd\u7684\u6570\u636e\u79fb\u52a8\u5f00\u9500\uff0c\u6210\u4e3a\u4e3b\u8981\u6027\u80fd\u74f6\u9888\uff0c\u4e9f\u9700\u7cfb\u7edf\u6027\u7406\u89e3\u4e0e\u4f18\u5316\u3002", "method": "\u5bf9\u4e09\u4e2a200B\u2013671B\u89c4\u6a21\u7684\u5148\u8fdbMoE\u6a21\u578b\uff0c\u5728\u8d85\u8fc724,000\u4e2a\u591a\u6837\u5316\u8bf7\u6c42\u4e0b\u8fdb\u884c\u4ee5\u6570\u636e\u79fb\u52a8\u4e3a\u4e2d\u5fc3\u7684\u5168\u9762\u5256\u6790\uff0c\u4ece\u65f6\u95f4\u548c\u7a7a\u95f4\u7ef4\u5ea6\u7cfb\u7edf\u5206\u6790150GB+\u7684\u8f68\u8ff9\u6570\u636e\uff0c\u5e76\u63d0\u70bc\u8bbe\u8ba1\u6307\u5bfc\u539f\u5219\u3002", "result": "\u63d0\u70bc\u51fa\u516d\u9879\u5173\u952e\u6d1e\u5bdf\uff1b\u5728\u6676\u5706\u7ea7GPU\u4e0a\u5e94\u7528\u8fd9\u4e9b\u6d1e\u5bdf\u8fdb\u884c\u5fae\u67b6\u6784\u8c03\u6574\uff0c\u4f7fDeepSeek V3\u548cQwen3\u5e73\u5747\u6027\u80fd\u5206\u522b\u63d0\u53476.3\u500d\u548c4.0\u500d\uff1b\u516c\u5f00\u53d1\u5e03\u8f68\u8ff9\u6570\u636e\u96c6\u4e0e\u5373\u5c06\u5f00\u6e90\u4eff\u771f\u6846\u67b6\u3002", "conclusion": "\u8be5\u7814\u7a76\u9996\u6b21\u63d0\u4f9b\u4e86\u5927\u89c4\u6a21MoE\u6a21\u578b\u6570\u636e\u79fb\u52a8\u884c\u4e3a\u7684\u5168\u9762\u5206\u6790\uff0c\u5176\u6d1e\u5bdf\u53ef\u6709\u6548\u6307\u5bfc\u672a\u6765\u9ad8\u6548MoE\u670d\u52a1\u7cfb\u7edf\u7684\u8bbe\u8ba1\uff0c\u5e76\u901a\u8fc7\u5f00\u6e90\u8d44\u6e90\u4fc3\u8fdb\u793e\u533a\u7814\u7a76\u3002"}}
{"id": "2510.05118", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2510.05118", "abs": "https://arxiv.org/abs/2510.05118", "authors": ["Cynthia Marcelino", "Noah Krennmair", "Thomas Pusztai", "Stefan Nastic"], "title": "Lumos: Performance Characterization of WebAssembly as a Serverless Runtime in the Edge-Cloud Continuum", "comment": null, "summary": "WebAssembly has emerged as a lightweight and portable runtime to execute\nserverless functions, particularly in heterogeneous and resource-constrained\nenvironments such as the Edge Cloud Continuum. However, the performance\nbenefits versus trade-offs remain insufficiently understood. This paper\npresents Lumos, a performance model and benchmarking tool for characterizing\nserverless runtimes. Lumos identifies workload, system, and environment-level\nperformance drivers in the Edge-Cloud Continuum. We benchmark state-of-the-art\ncontainers and the Wasm runtime in interpreted mode and with ahead-of-time\ncompilation. Our performance characterization shows that AoT-compiled Wasm\nimages are up to 30x smaller and decrease cold-start latency by up to 16%\ncompared to containers, while interpreted Wasm suffers up to 55x higher warm\nlatency and up to 10x I/O-serialization overhead.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86Lumos\uff0c\u4e00\u4e2a\u7528\u4e8e\u8bc4\u4f30\u65e0\u670d\u52a1\u5668\u8fd0\u884c\u65f6\u5728\u8fb9\u7f18-\u4e91\u8fde\u7eed\u4f53\u4e2d\u6027\u80fd\u7684\u6a21\u578b\u4e0e\u57fa\u51c6\u6d4b\u8bd5\u5de5\u5177\uff0c\u5e76\u5bf9\u6bd4\u4e86WebAssembly\uff08Wasm\uff09\u4e0e\u5bb9\u5668\u7684\u6027\u80fd\u8868\u73b0\u3002", "motivation": "\u5f53\u524d\u5bf9WebAssembly\u5728\u65e0\u670d\u52a1\u5668\u573a\u666f\u4e0b\uff0c\u7279\u522b\u662f\u5728\u8fb9\u7f18-\u4e91\u8fde\u7eed\u4f53\u4e2d\u7684\u6027\u80fd\u4f18\u52bf\u4e0e\u6743\u8861\u7f3a\u4e4f\u5145\u5206\u7406\u89e3\u3002", "method": "\u5f00\u53d1Lumos\u6027\u80fd\u6a21\u578b\u548c\u57fa\u51c6\u6d4b\u8bd5\u5de5\u5177\uff0c\u8bc6\u522b\u5de5\u4f5c\u8d1f\u8f7d\u3001\u7cfb\u7edf\u548c\u73af\u5883\u5c42\u9762\u7684\u6027\u80fd\u9a71\u52a8\u56e0\u7d20\uff0c\u5e76\u5bf9\u4e3b\u6d41\u5bb9\u5668\u4e0eWasm\u8fd0\u884c\u65f6\uff08\u89e3\u91ca\u6267\u884c\u4e0eAOT\u7f16\u8bd1\u6a21\u5f0f\uff09\u8fdb\u884c\u57fa\u51c6\u6d4b\u8bd5\u3002", "result": "AOT\u7f16\u8bd1\u7684Wasm\u955c\u50cf\u4f53\u79ef\u6700\u591a\u7f29\u5c0f30\u500d\uff0c\u51b7\u542f\u52a8\u5ef6\u8fdf\u6700\u591a\u964d\u4f4e16%\uff1b\u800c\u89e3\u91ca\u6267\u884c\u7684Wasm\u5728\u70ed\u5ef6\u8fdf\u4e0a\u6700\u591a\u9ad855\u500d\uff0cI/O\u5e8f\u5217\u5316\u5f00\u9500\u6700\u591a\u9ad810\u500d\u3002", "conclusion": "Wasm\u5728AOT\u7f16\u8bd1\u6a21\u5f0f\u4e0b\u5728\u4f53\u79ef\u548c\u51b7\u542f\u52a8\u65b9\u9762\u663e\u8457\u4f18\u4e8e\u5bb9\u5668\uff0c\u4f46\u89e3\u91ca\u6267\u884c\u6a21\u5f0f\u6027\u80fd\u8f83\u5dee\uff0c\u9700\u8c28\u614e\u9009\u62e9\u6267\u884c\u6a21\u5f0f\u4ee5\u5e73\u8861\u6027\u80fd\u4e0e\u8d44\u6e90\u3002"}}
{"id": "2510.06042", "categories": ["cs.MA"], "pdf": "https://arxiv.org/pdf/2510.06042", "abs": "https://arxiv.org/abs/2510.06042", "authors": ["Shang Ma", "Xusheng Xiao", "Yanfang Ye"], "title": "Agent+P: Guiding UI Agents via Symbolic Planning", "comment": null, "summary": "Large Language Model (LLM)-based UI agents show great promise for UI\nautomation but often hallucinate in long-horizon tasks due to their lack of\nunderstanding of the global UI transition structure. To address this, we\nintroduce AGENT+P, a novel framework that leverages symbolic planning to guide\nLLM-based UI agents. Specifically, we model an app's UI transition structure as\na UI Transition Graph (UTG), which allows us to reformulate the UI automation\ntask as a pathfinding problem on the UTG. This further enables an off-the-shelf\nsymbolic planner to generate a provably correct and optimal high-level plan,\npreventing the agent from redundant exploration and guiding the agent to\nachieve the automation goals. AGENT+P is designed as a plug-and-play framework\nto enhance existing UI agents. Evaluation on the AndroidWorld benchmark\ndemonstrates that AGENT+P improves the success rates of state-of-the-art UI\nagents by up to 14% and reduces the action steps by 37.7%.", "AI": {"tldr": "AGENT+P \u662f\u4e00\u4e2a\u7ed3\u5408\u7b26\u53f7\u89c4\u5212\u4e0e\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684 UI \u81ea\u52a8\u5316\u6846\u67b6\uff0c\u901a\u8fc7\u6784\u5efa UI \u8f6c\u6362\u56fe\uff08UTG\uff09\u5c06\u4efb\u52a1\u8f6c\u5316\u4e3a\u8def\u5f84\u89c4\u5212\u95ee\u9898\uff0c\u4ece\u800c\u63d0\u5347\u6210\u529f\u7387\u5e76\u51cf\u5c11\u64cd\u4f5c\u6b65\u9aa4\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e LLM \u7684 UI \u667a\u80fd\u4f53\u5728\u957f\u7a0b\u4efb\u52a1\u4e2d\u5bb9\u6613\u4ea7\u751f\u5e7b\u89c9\uff0c\u56e0\u5176\u7f3a\u4e4f\u5bf9\u5168\u5c40 UI \u8f6c\u6362\u7ed3\u6784\u7684\u7406\u89e3\u3002", "method": "\u63d0\u51fa AGENT+P \u6846\u67b6\uff0c\u5c06\u5e94\u7528\u7684 UI \u8f6c\u6362\u7ed3\u6784\u5efa\u6a21\u4e3a UI \u8f6c\u6362\u56fe\uff08UTG\uff09\uff0c\u5229\u7528\u73b0\u6210\u7684\u7b26\u53f7\u89c4\u5212\u5668\u751f\u6210\u6b63\u786e\u4e14\u6700\u4f18\u7684\u9ad8\u5c42\u8ba1\u5212\uff0c\u5f15\u5bfc LLM \u667a\u80fd\u4f53\u6267\u884c\u4efb\u52a1\u3002", "result": "\u5728 AndroidWorld \u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cAGENT+P \u5c06\u5f53\u524d\u6700\u4f18 UI \u667a\u80fd\u4f53\u7684\u6210\u529f\u7387\u6700\u591a\u63d0\u5347 14%\uff0c\u52a8\u4f5c\u6b65\u9aa4\u51cf\u5c11 37.7%\u3002", "conclusion": "AGENT+P \u4f5c\u4e3a\u4e00\u79cd\u5373\u63d2\u5373\u7528\u6846\u67b6\uff0c\u6709\u6548\u63d0\u5347\u4e86 LLM \u667a\u80fd\u4f53\u5728 UI \u81ea\u52a8\u5316\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\u548c\u6548\u7387\u3002"}}
{"id": "2510.05625", "categories": ["cs.NI", "cs.AI", "cs.CL", "cs.MA", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2510.05625", "abs": "https://arxiv.org/abs/2510.05625", "authors": ["Yao Zhang", "Yuchen Song", "Shengnan Li", "Yan Shi", "Shikui Shen", "Xiongyan Tang", "Min Zhang", "Danshi Wang"], "title": "Generative AI-Driven Hierarchical Multi-Agent Framework for Zero-Touch Optical Networks", "comment": "7 pages,6 figures, Accepted by lEEE Communications Magazine, Open\n  call", "summary": "The rapid development of Generative Artificial Intelligence (GenAI) has\ncatalyzed a transformative technological revolution across all walks of life.\nAs the backbone of wideband communication, optical networks are expecting\nhigh-level autonomous operation and zero-touch management to accommodate their\nexpanding network scales and escalating transmission bandwidth. The integration\nof GenAI is deemed as the pivotal solution for realizing zero-touch optical\nnetworks. However, the lifecycle management of optical networks involves a\nmultitude of tasks and necessitates seamless collaboration across multiple\nlayers, which poses significant challenges to the existing single-agent GenAI\nsystems. In this paper, we propose a GenAI-driven hierarchical multi-agent\nframework designed to streamline multi-task autonomous execution for zero-touch\noptical networks. We present the architecture, implementation, and applications\nof this framework. A field-deployed mesh network is utilized to demonstrate\nthree typical scenarios throughout the lifecycle of optical network: quality of\ntransmission estimation in the planning stage, dynamic channel adding/dropping\nin the operation stage, and system capacity increase in the upgrade stage. The\ncase studies, illustrate the capabilities of multi-agent framework in\nmulti-task allocation, coordination, execution, evaluation, and summarization.\nThis work provides a promising approach for the future development of\nintelligent, efficient, and collaborative network management solutions, paving\nthe way for more specialized and adaptive zero-touch optical networks.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u751f\u6210\u5f0f\u4eba\u5de5\u667a\u80fd\uff08GenAI\uff09\u7684\u5206\u5c42\u591a\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u7528\u4e8e\u5b9e\u73b0\u96f6\u63a5\u89e6\u5149\u7f51\u7edc\u4e2d\u7684\u591a\u4efb\u52a1\u81ea\u4e3b\u6267\u884c\uff0c\u6db5\u76d6\u89c4\u5212\u3001\u8fd0\u884c\u548c\u5347\u7ea7\u9636\u6bb5\u7684\u5178\u578b\u5e94\u7528\u573a\u666f\u3002", "motivation": "\u73b0\u6709\u5355\u667a\u80fd\u4f53GenAI\u7cfb\u7edf\u96be\u4ee5\u5e94\u5bf9\u5149\u7f51\u7edc\u5168\u751f\u547d\u5468\u671f\u4e2d\u591a\u4efb\u52a1\u3001\u8de8\u5c42\u534f\u4f5c\u7684\u590d\u6742\u9700\u6c42\uff0c\u4e9f\u9700\u66f4\u9ad8\u6548\u3001\u534f\u540c\u7684\u667a\u80fd\u7ba1\u7406\u65b9\u6848\u4ee5\u5b9e\u73b0\u96f6\u63a5\u89e6\u5149\u7f51\u7edc\u3002", "method": "\u8bbe\u8ba1\u5e76\u5b9e\u73b0\u4e86\u4e00\u4e2aGenAI\u9a71\u52a8\u7684\u5206\u5c42\u591a\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u901a\u8fc7\u591a\u667a\u80fd\u4f53\u534f\u4f5c\u5b8c\u6210\u5149\u7f51\u7edc\u751f\u547d\u5468\u671f\u4e2d\u7684\u591a\u4efb\u52a1\u81ea\u4e3b\u6267\u884c\uff0c\u5e76\u5728\u5b9e\u9645\u90e8\u7f72\u7684\u7f51\u72b6\u7f51\u7edc\u4e2d\u9a8c\u8bc1\u5176\u5728\u4f20\u8f93\u8d28\u91cf\u4f30\u8ba1\u3001\u52a8\u6001\u4fe1\u9053\u589e\u5220\u548c\u7cfb\u7edf\u6269\u5bb9\u7b49\u573a\u666f\u4e2d\u7684\u5e94\u7528\u3002", "result": "\u6848\u4f8b\u7814\u7a76\u8868\u660e\uff0c\u8be5\u591a\u667a\u80fd\u4f53\u6846\u67b6\u5728\u591a\u4efb\u52a1\u5206\u914d\u3001\u534f\u8c03\u3001\u6267\u884c\u3001\u8bc4\u4f30\u4e0e\u603b\u7ed3\u65b9\u9762\u5c55\u73b0\u51fa\u826f\u597d\u80fd\u529b\uff0c\u6709\u6548\u652f\u6301\u5149\u7f51\u7edc\u7684\u667a\u80fd\u5316\u4e0e\u81ea\u9002\u5e94\u7ba1\u7406\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u672a\u6765\u667a\u80fd\u3001\u9ad8\u6548\u3001\u534f\u540c\u7684\u96f6\u63a5\u89e6\u5149\u7f51\u7edc\u7ba1\u7406\u63d0\u4f9b\u4e86\u53ef\u884c\u8def\u5f84\uff0c\u63a8\u52a8\u5149\u7f51\u7edc\u5411\u66f4\u4e13\u4e1a\u5316\u548c\u81ea\u9002\u5e94\u65b9\u5411\u53d1\u5c55\u3002"}}
{"id": "2510.05365", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.05365", "abs": "https://arxiv.org/abs/2510.05365", "authors": ["Irtaza Sajid Qureshi", "Zhen Ming", "Jiang"], "title": "Test Case Generation from Bug Reports via Large Language Models: A Cognitive Layered Evaluation Framework", "comment": null, "summary": "Large Language Models (LLMs) are increasingly applied to automated software\ntesting, yet their ability to generalize beyond memorized patterns and reason\nabout natural language bug reports remains unclear. We present a systematic\nevaluation of LLM reasoning in test case generation, structured around the\ncognitive layers of Bloom's taxonomy: \\textit{Remember}, \\textit{Understand},\n\\textit{Apply}, \\textit{Analyze}, \\textit{Evaluate}, and \\textit{Create}, which\nprogressively assess higher levels of cognitive and reasoning capabilities.\nBuilding on the LIBRO framework, we evaluate StarCoder and GPT-4o on Defects4J,\nGHRB, and mutated variants that introduce linguistic and semantic challenges.\nOur findings show that both models largely reproduce prior results with minor\ndeviations (\\textit{Remember}), exhibit partial robustness to linguistic\nrephrasings and translations while uncovering unique reproducible bugs\n(\\textit{Understand}), but suffer severe performance drops exceeding 60\\% under\nidentifier mutations (\\textit{Apply}). Conversely, providing near-identical\nfew-shot examples in an open-book setting improves success rates by up to three\ntimes, and component-level analysis reveals that structured technical elements,\nsuch as test code and method names, are far more impactful than narrative\ndescriptions for successful test generation (\\textit{Analyze}). These insights\nilluminate the cognitive processes underlying LLM-generated tests, suggest\nconcrete directions for improving performance, and establish a robust and\nrealistic evaluation paradigm for this task.", "AI": {"tldr": "\u672c\u6587\u7cfb\u7edf\u8bc4\u4f30\u4e86\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5728\u57fa\u4e8e\u81ea\u7136\u8bed\u8a00\u7f3a\u9677\u62a5\u544a\u751f\u6210\u6d4b\u8bd5\u7528\u4f8b\u4efb\u52a1\u4e2d\u7684\u63a8\u7406\u80fd\u529b\uff0c\u4f9d\u636e\u5e03\u9c81\u59c6\u5206\u7c7b\u6cd5\u7684\u8ba4\u77e5\u5c42\u6b21\u8fdb\u884c\u5206\u6790\uff0c\u53d1\u73b0\u6a21\u578b\u5728\u8bb0\u5fc6\u548c\u7406\u89e3\u5c42\u9762\u8868\u73b0\u5c1a\u53ef\uff0c\u4f46\u5728\u5e94\u7528\u5c42\u9762\uff08\u5982\u6807\u8bc6\u7b26\u53d8\u5f02\uff09\u6027\u80fd\u5927\u5e45\u4e0b\u964d\uff0c\u800c\u7ed3\u6784\u5316\u6280\u672f\u5143\u7d20\u5bf9\u6d4b\u8bd5\u751f\u6210\u6548\u679c\u5f71\u54cd\u663e\u8457\u3002", "motivation": "\u5f53\u524d\u5927\u8bed\u8a00\u6a21\u578b\u5728\u81ea\u52a8\u5316\u8f6f\u4ef6\u6d4b\u8bd5\u4e2d\u7684\u5e94\u7528\u65e5\u76ca\u5e7f\u6cdb\uff0c\u4f46\u5176\u662f\u5426\u80fd\u8d85\u8d8a\u8bb0\u5fc6\u6a21\u5f0f\u3001\u771f\u6b63\u7406\u89e3\u5e76\u63a8\u7406\u81ea\u7136\u8bed\u8a00\u7f3a\u9677\u62a5\u544a\u5c1a\u4e0d\u660e\u786e\uff0c\u4e9f\u9700\u7cfb\u7edf\u6027\u8bc4\u4f30\u5176\u8ba4\u77e5\u4e0e\u63a8\u7406\u80fd\u529b\u3002", "method": "\u57fa\u4e8e\u5e03\u9c81\u59c6\u5206\u7c7b\u6cd5\u7684\u516d\u4e2a\u8ba4\u77e5\u5c42\u6b21\uff08\u8bb0\u5fc6\u3001\u7406\u89e3\u3001\u5e94\u7528\u3001\u5206\u6790\u3001\u8bc4\u4ef7\u3001\u521b\u9020\uff09\uff0c\u5728LIBRO\u6846\u67b6\u4e0b\u5bf9StarCoder\u548cGPT-4o\u5728Defects4J\u3001GHRB\u53ca\u5176\u8bed\u8a00\u4e0e\u8bed\u4e49\u53d8\u5f02\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u6d4b\u8bd5\u7528\u4f8b\u751f\u6210\u8bc4\u4f30\uff0c\u5e76\u5206\u6790\u4e0d\u540c\u63d0\u793a\u7b56\u7565\u548c\u8f93\u5165\u5143\u7d20\u7684\u5f71\u54cd\u3002", "result": "\u6a21\u578b\u4e3b\u8981\u590d\u73b0\u5df2\u6709\u7ed3\u679c\uff08\u8bb0\u5fc6\uff09\uff1b\u5bf9\u8bed\u8a00\u6539\u5199\u548c\u7ffb\u8bd1\u5177\u6709\u4e00\u5b9a\u9c81\u68d2\u6027\u5e76\u80fd\u53d1\u73b0\u65b0bug\uff08\u7406\u89e3\uff09\uff1b\u4f46\u5728\u6807\u8bc6\u7b26\u53d8\u5f02\u4e0b\u6027\u80fd\u4e0b\u964d\u8d8560%\uff08\u5e94\u7528\uff09\uff1b\u63d0\u4f9b\u8fd1\u4f3c\u793a\u4f8b\u53ef\u63d0\u5347\u6210\u529f\u7387\u6700\u591a3\u500d\uff1b\u7ed3\u6784\u5316\u6280\u672f\u5143\u7d20\uff08\u5982\u6d4b\u8bd5\u4ee3\u7801\u3001\u65b9\u6cd5\u540d\uff09\u6bd4\u53d9\u8ff0\u6027\u63cf\u8ff0\u66f4\u5173\u952e\uff08\u5206\u6790\uff09\u3002", "conclusion": "LLM\u5728\u6d4b\u8bd5\u751f\u6210\u4e2d\u7684\u63a8\u7406\u80fd\u529b\u6709\u9650\uff0c\u5c24\u5176\u5728\u9700\u8981\u6cdb\u5316\u548c\u9002\u5e94\u4ee3\u7801\u7ed3\u6784\u53d8\u5316\u65f6\u8868\u73b0\u4e0d\u4f73\uff1b\u7ed3\u6784\u5316\u4fe1\u606f\u5bf9\u6027\u80fd\u81f3\u5173\u91cd\u8981\uff1b\u7814\u7a76\u4e3a\u63d0\u5347LLM\u6d4b\u8bd5\u751f\u6210\u80fd\u529b\u63d0\u4f9b\u4e86\u5177\u4f53\u65b9\u5411\uff0c\u5e76\u5efa\u7acb\u4e86\u66f4\u73b0\u5b9e\u7684\u8bc4\u4f30\u8303\u5f0f\u3002"}}
{"id": "2510.05127", "categories": ["cs.DC", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.05127", "abs": "https://arxiv.org/abs/2510.05127", "authors": ["Harshit Goyal"], "title": "Artificial Intelligence for Cost-Aware Resource Prediction in Big Data Pipelines", "comment": "14 pages, 3 figures", "summary": "Efficient resource allocation is a key challenge in modern cloud computing.\nOver-provisioning leads to unnecessary costs, while under-provisioning risks\nperformance degradation and SLA violations. This work presents an artificial\nintelligence approach to predict resource utilization in big data pipelines\nusing Random Forest regression. We preprocess the Google Borg cluster traces to\nclean, transform, and extract relevant features (CPU, memory, usage\ndistributions). The model achieves high predictive accuracy (R Square = 0.99,\nMAE = 0.0048, RMSE = 0.137), capturing non-linear relationships between\nworkload characteristics and resource utilization. Error analysis reveals\nimpressive performance on small-to-medium jobs, with higher variance in rare\nlarge-scale jobs. These results demonstrate the potential of AI-driven\nprediction for cost-aware autoscaling in cloud environments, reducing\nunnecessary provisioning while safeguarding service quality.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u968f\u673a\u68ee\u6797\u56de\u5f52\u7684\u4eba\u5de5\u667a\u80fd\u65b9\u6cd5\uff0c\u7528\u4e8e\u9884\u6d4b\u5927\u6570\u636e\u6d41\u6c34\u7ebf\u4e2d\u7684\u8d44\u6e90\u5229\u7528\u7387\uff0c\u5728Google Borg\u96c6\u7fa4\u6570\u636e\u4e0a\u5b9e\u73b0\u4e86\u9ad8\u7cbe\u5ea6\u9884\u6d4b\uff08R\u00b2=0.99\uff09\uff0c\u5c55\u793a\u4e86\u5176\u5728\u4e91\u73af\u5883\u4e2d\u5b9e\u73b0\u6210\u672c\u611f\u77e5\u81ea\u52a8\u6269\u7f29\u5bb9\u7684\u6f5c\u529b\u3002", "motivation": "\u73b0\u4ee3\u4e91\u8ba1\u7b97\u4e2d\uff0c\u8d44\u6e90\u5206\u914d\u6548\u7387\u81f3\u5173\u91cd\u8981\uff1a\u8fc7\u5ea6\u914d\u7f6e\u4f1a\u5e26\u6765\u4e0d\u5fc5\u8981\u6210\u672c\uff0c\u800c\u914d\u7f6e\u4e0d\u8db3\u5219\u53ef\u80fd\u5bfc\u81f4\u6027\u80fd\u4e0b\u964d\u548c\u670d\u52a1\u7b49\u7ea7\u534f\u8bae\uff08SLA\uff09\u8fdd\u89c4\u3002", "method": "\u4f7f\u7528\u968f\u673a\u68ee\u6797\u56de\u5f52\u6a21\u578b\u9884\u6d4b\u8d44\u6e90\u5229\u7528\u7387\uff1b\u5bf9Google Borg\u96c6\u7fa4\u8ffd\u8e2a\u6570\u636e\u8fdb\u884c\u6e05\u6d17\u3001\u8f6c\u6362\u5e76\u63d0\u53d6\u5173\u952e\u7279\u5f81\uff08\u5982CPU\u3001\u5185\u5b58\u53ca\u4f7f\u7528\u5206\u5e03\uff09\u3002", "result": "\u6a21\u578b\u53d6\u5f97\u4e86\u9ad8\u9884\u6d4b\u7cbe\u5ea6\uff08R\u00b2 = 0.99\uff0cMAE = 0.0048\uff0cRMSE = 0.137\uff09\uff0c\u80fd\u6709\u6548\u6355\u6349\u5de5\u4f5c\u8d1f\u8f7d\u7279\u5f81\u4e0e\u8d44\u6e90\u4f7f\u7528\u4e4b\u95f4\u7684\u975e\u7ebf\u6027\u5173\u7cfb\uff1b\u5728\u4e2d\u5c0f\u578b\u4efb\u52a1\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5728\u7f55\u89c1\u7684\u5927\u89c4\u6a21\u4efb\u52a1\u4e0a\u8bef\u5dee\u65b9\u5dee\u8f83\u9ad8\u3002", "conclusion": "AI\u9a71\u52a8\u7684\u8d44\u6e90\u5229\u7528\u7387\u9884\u6d4b\u5728\u4e91\u73af\u5883\u4e2d\u5177\u6709\u663e\u8457\u5e94\u7528\u6f5c\u529b\uff0c\u53ef\u5728\u4fdd\u969c\u670d\u52a1\u8d28\u91cf\u7684\u540c\u65f6\u51cf\u5c11\u4e0d\u5fc5\u8981\u7684\u8d44\u6e90\u5206\u914d\uff0c\u652f\u6301\u6210\u672c\u611f\u77e5\u7684\u81ea\u52a8\u6269\u7f29\u5bb9\u7b56\u7565\u3002"}}
{"id": "2510.05145", "categories": ["cs.DC", "cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2510.05145", "abs": "https://arxiv.org/abs/2510.05145", "authors": ["Lunyiu Nie", "Nedim Lipka", "Ryan A. Rossi", "Swarat Chaudhuri"], "title": "FlashResearch: Real-time Agent Orchestration for Efficient Deep Research", "comment": null, "summary": "Deep research agents, which synthesize information across diverse sources,\nare significantly constrained by their sequential reasoning processes. This\narchitectural bottleneck results in high latency, poor runtime adaptability,\nand inefficient resource allocation, making them impractical for interactive\napplications. To overcome this, we introduce FlashResearch, a novel framework\nfor efficient deep research that transforms sequential processing into\nparallel, runtime orchestration by dynamically decomposing complex queries into\ntree-structured sub-tasks. Our core contributions are threefold: (1) an\nadaptive planner that dynamically allocates computational resources by\ndetermining research breadth and depth based on query complexity; (2) a\nreal-time orchestration layer that monitors research progress and prunes\nredundant paths to reallocate resources and optimize efficiency; and (3) a\nmulti-dimensional parallelization framework that enables concurrency across\nboth research breadth and depth. Experiments show that FlashResearch\nconsistently improves final report quality within fixed time budgets, and can\ndeliver up to a 5x speedup while maintaining comparable quality.", "AI": {"tldr": "FlashResearch \u662f\u4e00\u4e2a\u65b0\u578b\u9ad8\u6548\u6df1\u5ea6\u7814\u7a76\u6846\u67b6\uff0c\u901a\u8fc7\u5c06\u4e32\u884c\u5904\u7406\u8f6c\u4e3a\u52a8\u6001\u5e76\u884c\u7684\u6811\u72b6\u5b50\u4efb\u52a1\u7ed3\u6784\uff0c\u663e\u8457\u63d0\u5347\u7814\u7a76\u4ee3\u7406\u7684\u6548\u7387\u4e0e\u54cd\u5e94\u901f\u5ea6\u3002", "motivation": "\u73b0\u6709\u6df1\u5ea6\u7814\u7a76\u4ee3\u7406\u53d7\u9650\u4e8e\u4e32\u884c\u63a8\u7406\u8fc7\u7a0b\uff0c\u5bfc\u81f4\u9ad8\u5ef6\u8fdf\u3001\u8fd0\u884c\u65f6\u9002\u5e94\u6027\u5dee\u548c\u8d44\u6e90\u5229\u7528\u4f4e\u6548\uff0c\u96be\u4ee5\u7528\u4e8e\u4ea4\u4e92\u5f0f\u5e94\u7528\u3002", "method": "\u63d0\u51fa FlashResearch \u6846\u67b6\uff0c\u5305\u542b\uff1a(1) \u81ea\u9002\u5e94\u89c4\u5212\u5668\uff0c\u6839\u636e\u67e5\u8be2\u590d\u6742\u5ea6\u52a8\u6001\u5206\u914d\u8ba1\u7b97\u8d44\u6e90\uff1b(2) \u5b9e\u65f6\u7f16\u6392\u5c42\uff0c\u76d1\u63a7\u8fdb\u5ea6\u5e76\u526a\u679d\u5197\u4f59\u8def\u5f84\uff1b(3) \u591a\u7ef4\u5e76\u884c\u5316\u673a\u5236\uff0c\u652f\u6301\u5e7f\u5ea6\u4e0e\u6df1\u5ea6\u7684\u5e76\u53d1\u5904\u7406\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cFlashResearch \u5728\u56fa\u5b9a\u65f6\u95f4\u9884\u7b97\u4e0b\u6301\u7eed\u63d0\u5347\u6700\u7ec8\u62a5\u544a\u8d28\u91cf\uff0c\u5e76\u5728\u4fdd\u6301\u76f8\u5f53\u8d28\u91cf\u7684\u524d\u63d0\u4e0b\u5b9e\u73b0\u6700\u9ad8 5 \u500d\u52a0\u901f\u3002", "conclusion": "FlashResearch \u6709\u6548\u89e3\u51b3\u4e86\u6df1\u5ea6\u7814\u7a76\u4ee3\u7406\u7684\u6548\u7387\u74f6\u9888\uff0c\u4e3a\u4ea4\u4e92\u5f0f\u5e94\u7528\u573a\u666f\u63d0\u4f9b\u4e86\u53ef\u884c\u7684\u9ad8\u6027\u80fd\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.05686", "categories": ["cs.NI"], "pdf": "https://arxiv.org/pdf/2510.05686", "abs": "https://arxiv.org/abs/2510.05686", "authors": ["Jos\u00e9 G\u00f3mez-delaHiz", "Mohamed Faten Zhani", "Jaime Gal\u00e1n-Jim\u00e9nez", "John Kaippallimalil"], "title": "On Enhancing Delay SLAs in TCP Networks through Joint Routing and Transport Assistant Deployment", "comment": "10 pages, 17 figures", "summary": "The Transport Control Protocol has long been the primary transport protocol\nfor applications requiring performance and reliability over the Internet.\nUnfortunately, due its retransmission mechanism, TCP incurs high packet\ndelivery delays when segments are lost. To address this issue, previous\nresearch proposed to use a novel network function, namely Transport Assistant,\ndeployed within the network to cache and retransmit lost packets, thus reducing\nretransmission delays. In this paper, we propose to jointly route the flows and\ndeploy TAs in order to minimize packet delivery delays in best-effort networks\n(scenario 1) or to satisfy delay-based Service Level Agreements in QoS-based\nnetworks (scenario 2). We hence formulate the joint routing and TA deployment\nproblem as Integer Linear Program for the two scenarios and propose a heuristic\nsolution for large-scale instances of the problem. Through extensive\nsimulations, we demonstrate the benefits of performing joint routing flows and\nTA deployment in reducing packet delivery delays (up to 16.4%) while minimizing\ndeployment costs (up to 60.98%).", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u8054\u5408\u4f18\u5316\u6d41\u8def\u7531\u4e0e\u4f20\u8f93\u8f85\u52a9\u5668\uff08TA\uff09\u90e8\u7f72\uff0c\u4ee5\u964d\u4f4e\u5c3d\u529b\u800c\u4e3a\u7f51\u7edc\u4e2d\u7684\u6570\u636e\u5305\u4f20\u8f93\u5ef6\u8fdf\u6216\u6ee1\u8db3QoS\u7f51\u7edc\u4e2d\u7684\u5ef6\u8fdfSLA\uff0c\u5e76\u901a\u8fc7\u6574\u6570\u7ebf\u6027\u89c4\u5212\u5efa\u6a21\u53ca\u542f\u53d1\u5f0f\u7b97\u6cd5\u6c42\u89e3\uff0c\u5728\u964d\u4f4e\u5ef6\u8fdf\uff08\u6700\u591a16.4%\uff09\u7684\u540c\u65f6\u663e\u8457\u51cf\u5c11\u90e8\u7f72\u6210\u672c\uff08\u6700\u591a60.98%\uff09\u3002", "motivation": "TCP\u56e0\u91cd\u4f20\u673a\u5236\u5728\u4e22\u5305\u65f6\u5bfc\u81f4\u9ad8\u4f20\u8f93\u5ef6\u8fdf\uff0c\u5df2\u6709\u7814\u7a76\u5f15\u5165\u7f51\u7edc\u5185\u7f13\u5b58\u4e0e\u91cd\u4f20\u7684\u4f20\u8f93\u8f85\u52a9\u5668\uff08TA\uff09\u6765\u7f13\u89e3\u6b64\u95ee\u9898\uff1b\u7136\u800c\uff0c\u5982\u4f55\u6709\u6548\u90e8\u7f72TA\u5e76\u7ed3\u5408\u8def\u7531\u4f18\u5316\u4ee5\u6700\u5c0f\u5316\u5ef6\u8fdf\u6216\u6ee1\u8db3SLA\u4ecd\u9700\u7814\u7a76\u3002", "method": "\u5c06\u8054\u5408\u8def\u7531\u4e0eTA\u90e8\u7f72\u95ee\u9898\u5efa\u6a21\u4e3a\u4e24\u79cd\u573a\u666f\u4e0b\u7684\u6574\u6570\u7ebf\u6027\u89c4\u5212\uff08ILP\uff09\uff0c\u5e76\u4e3a\u5927\u89c4\u6a21\u5b9e\u4f8b\u8bbe\u8ba1\u542f\u53d1\u5f0f\u7b97\u6cd5\u3002", "result": "\u4eff\u771f\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u5c3d\u529b\u800c\u4e3a\u7f51\u7edc\u4e2d\u6700\u591a\u964d\u4f4e16.4%\u7684\u6570\u636e\u5305\u4ea4\u4ed8\u5ef6\u8fdf\uff0c\u5728QoS\u7f51\u7edc\u4e2d\u6709\u6548\u6ee1\u8db3\u5ef6\u8fdfSLA\uff0c\u540c\u65f6\u90e8\u7f72\u6210\u672c\u6700\u591a\u51cf\u5c1160.98%\u3002", "conclusion": "\u8054\u5408\u4f18\u5316\u6d41\u8def\u7531\u4e0eTA\u90e8\u7f72\u80fd\u663e\u8457\u964d\u4f4eTCP\u4f20\u8f93\u5ef6\u8fdf\u5e76\u8282\u7701\u90e8\u7f72\u6210\u672c\uff0c\u9002\u7528\u4e8e\u4e0d\u540c\u7f51\u7edc\u573a\u666f\uff0c\u5177\u6709\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2510.05390", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.05390", "abs": "https://arxiv.org/abs/2510.05390", "authors": ["Felicity Anderson", "Julien Sindt", "Neil Chue Hong"], "title": "Who Do You Think You Are? Creating RSE Personas from GitHub Interactions", "comment": "36 pages. Invited extended paper of original poster at deRSE2025. To\n  be published in ECEASST", "summary": "We describe data-driven RSE personas: an approach combining software\nrepository mining and data-driven personas applied to research software (RS),\nan attempt to describe and identify common and rare patterns of Research\nSoftware Engineering (RSE) development. This allows individuals and RS project\nteams to understand their contributions, impact and repository dynamics - an\nimportant foundation for improving RSE. We evaluate the method on different\npatterns of collaborative interaction behaviours by contributors to mid-sized\npublic RS repositories (those with 10-300 committers) on GitHub. We demonstrate\nhow the RSE personas method successfully characterises a sample of 115,174\nrepository contributors across 1,284 RS repositories on GitHub, sampled from\n42,284 candidate software repository records queried from Zenodo. We identify,\nname and summarise seven distinct personas from low to high interactivity:\nEphemeral Contributor; Occasional Contributor; Project Organiser; Moderate\nContributor; Low-Process Closer; Low-Coding Closer; and Active Contributor.\nThis demonstrates that large datasets can be analysed despite difficulties of\ncomparing software projects with different project management factors, research\ndomains and contributor backgrounds.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u8f6f\u4ef6\u4ed3\u5e93\u6316\u6398\u4e0e\u6570\u636e\u9a71\u52a8\u7528\u6237\u753b\u50cf\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u523b\u753b\u79d1\u7814\u8f6f\u4ef6\u5de5\u7a0b\uff08RSE\uff09\u4e2d\u7684\u8d21\u732e\u8005\u884c\u4e3a\u6a21\u5f0f\uff0c\u5e76\u5728GitHub\u4e0a\u5bf91,284\u4e2a\u79d1\u7814\u8f6f\u4ef6\u4ed3\u5e93\u7684115,174\u540d\u8d21\u732e\u8005\u8fdb\u884c\u4e86\u5206\u6790\uff0c\u8bc6\u522b\u51fa\u4e03\u79cd\u5178\u578bRSE\u89d2\u8272\u3002", "motivation": "\u79d1\u7814\u8f6f\u4ef6\u5de5\u7a0b\uff08RSE\uff09\u7f3a\u4e4f\u5bf9\u8d21\u732e\u8005\u884c\u4e3a\u6a21\u5f0f\u7684\u7cfb\u7edf\u7406\u89e3\uff0c\u963b\u788d\u4e86\u56e2\u961f\u534f\u4f5c\u4e0e\u9879\u76ee\u6539\u8fdb\u3002\u4f5c\u8005\u5e0c\u671b\u901a\u8fc7\u6570\u636e\u9a71\u52a8\u7684\u65b9\u5f0f\u63ed\u793aRSE\u4e2d\u7684\u5e38\u89c1\u4e0e\u7f55\u89c1\u5f00\u53d1\u6a21\u5f0f\uff0c\u5e2e\u52a9\u4e2a\u4f53\u548c\u56e2\u961f\u66f4\u597d\u5730\u7406\u89e3\u5176\u8d21\u732e\u3001\u5f71\u54cd\u53ca\u4ed3\u5e93\u52a8\u6001\u3002", "method": "\u7ed3\u5408\u8f6f\u4ef6\u4ed3\u5e93\u6316\u6398\u4e0e\u6570\u636e\u9a71\u52a8\u7528\u6237\u753b\u50cf\u6280\u672f\uff0c\u5206\u6790GitHub\u4e0a\u4e2d\u7b49\u89c4\u6a21\uff0810\u2013300\u540d\u63d0\u4ea4\u8005\uff09\u79d1\u7814\u8f6f\u4ef6\u4ed3\u5e93\u7684\u534f\u4f5c\u884c\u4e3a\u6570\u636e\uff0c\u4eceZenodo\u7b5b\u9009\u51fa42,284\u4e2a\u5019\u9009\u4ed3\u5e93\uff0c\u6700\u7ec8\u5bf91,284\u4e2a\u4ed3\u5e93\u4e2d\u7684115,174\u540d\u8d21\u732e\u8005\u8fdb\u884c\u5efa\u6a21\u3002", "result": "\u8bc6\u522b\u5e76\u547d\u540d\u4e86\u4e03\u79cdRSE\u8d21\u732e\u8005\u753b\u50cf\uff0c\u6309\u4e92\u52a8\u7a0b\u5ea6\u4ece\u4f4e\u5230\u9ad8\u4f9d\u6b21\u4e3a\uff1a\u77ed\u6682\u8d21\u732e\u8005\u3001\u5076\u5c14\u8d21\u732e\u8005\u3001\u9879\u76ee\u7ec4\u7ec7\u8005\u3001\u4e2d\u7b49\u8d21\u732e\u8005\u3001\u4f4e\u6d41\u7a0b\u5173\u95ed\u8005\u3001\u4f4e\u7f16\u7801\u5173\u95ed\u8005\u548c\u6d3b\u8dc3\u8d21\u732e\u8005\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u80fd\u6709\u6548\u5e94\u5bf9\u79d1\u7814\u8f6f\u4ef6\u9879\u76ee\u5728\u7ba1\u7406\u65b9\u5f0f\u3001\u7814\u7a76\u9886\u57df\u548c\u8d21\u732e\u8005\u80cc\u666f\u7b49\u65b9\u9762\u7684\u5f02\u8d28\u6027\uff0c\u8bc1\u660e\u5927\u89c4\u6a21\u6570\u636e\u53ef\u7528\u4e8e\u523b\u753bRSE\u884c\u4e3a\u6a21\u5f0f\uff0c\u4e3a\u6539\u8fdb\u79d1\u7814\u8f6f\u4ef6\u5de5\u7a0b\u5b9e\u8df5\u63d0\u4f9b\u57fa\u7840\u3002"}}
{"id": "2510.05621", "categories": ["cs.DC", "cs.MA"], "pdf": "https://arxiv.org/pdf/2510.05621", "abs": "https://arxiv.org/abs/2510.05621", "authors": ["Zhiyuan Ren", "Tao Zhang", "Wenchi Chen"], "title": "Decoupling Correctness from Policy: A Deterministic Causal Structure for Multi-Agent Systems", "comment": null, "summary": "In distributed multi-agent systems, correctness is often entangled with\noperational policies such as scheduling, batching, or routing, which makes\nsystems brittle since performance-driven policy evolution may break integrity\nguarantees. This paper introduces the Deterministic Causal Structure (DCS), a\nformal foundation that decouples correctness from policy. We develop a minimal\naxiomatic theory and prove four results: existence and uniqueness,\npolicy-agnostic invariance, observational equivalence, and axiom minimality.\nThese results show that DCS resolves causal ambiguities that value-centric\nconvergence models such as CRDTs cannot address, and that removing any axiom\ncollapses determinism into ambiguity. DCS thus emerges as a boundary principle\nof asynchronous computation, analogous to CAP and FLP: correctness is preserved\nonly within the expressive power of a join-semilattice. All guarantees are\nestablished by axioms and proofs, with only minimal illustrative constructions\nincluded to aid intuition. This work establishes correctness as a fixed,\npolicy-agnostic substrate, a Correctness-as-a-Chassis paradigm, on which\ndistributed intelligent systems can be built modularly, safely, and evolvably.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u786e\u5b9a\u6027\u56e0\u679c\u7ed3\u6784\uff08DCS\uff09\uff0c\u901a\u8fc7\u516c\u7406\u5316\u65b9\u6cd5\u5c06\u5206\u5e03\u5f0f\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u4e2d\u7684\u6b63\u786e\u6027\u4e0e\u8c03\u5ea6\u3001\u6279\u5904\u7406\u7b49\u64cd\u4f5c\u7b56\u7565\u89e3\u8026\uff0c\u4ece\u800c\u5728\u5f02\u6b65\u8ba1\u7b97\u4e2d\u63d0\u4f9b\u4e00\u79cd\u4e0e\u7b56\u7565\u65e0\u5173\u7684\u6b63\u786e\u6027\u57fa\u7840\u3002", "motivation": "\u5728\u5206\u5e03\u5f0f\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u4e2d\uff0c\u6b63\u786e\u6027\u5e38\u4e0e\u8c03\u5ea6\u3001\u6279\u5904\u7406\u6216\u8def\u7531\u7b49\u64cd\u4f5c\u7b56\u7565\u4ea4\u7ec7\uff0c\u5bfc\u81f4\u7cfb\u7edf\u8106\u5f31\u2014\u2014\u7b56\u7565\u4e3a\u63d0\u5347\u6027\u80fd\u800c\u6f14\u8fdb\u65f6\u53ef\u80fd\u7834\u574f\u5b8c\u6574\u6027\u4fdd\u8bc1\u3002\u56e0\u6b64\uff0c\u4e9f\u9700\u4e00\u79cd\u5c06\u6b63\u786e\u6027\u4e0e\u7b56\u7565\u89e3\u8026\u7684\u5f62\u5f0f\u5316\u57fa\u7840\u3002", "method": "\u4f5c\u8005\u6784\u5efa\u4e86\u4e00\u4e2a\u6700\u5c0f\u516c\u7406\u5316\u7406\u8bba\u2014\u2014\u786e\u5b9a\u6027\u56e0\u679c\u7ed3\u6784\uff08DCS\uff09\uff0c\u5e76\u57fa\u4e8e\u8be5\u7406\u8bba\u8bc1\u660e\u4e86\u56db\u4e2a\u5173\u952e\u6027\u8d28\uff1a\u5b58\u5728\u6027\u4e0e\u552f\u4e00\u6027\u3001\u7b56\u7565\u65e0\u5173\u4e0d\u53d8\u6027\u3001\u89c2\u6d4b\u7b49\u4ef7\u6027\u4ee5\u53ca\u516c\u7406\u6700\u5c0f\u6027\u3002", "result": "DCS \u80fd\u89e3\u51b3 CRDT \u7b49\u4ee5\u503c\u4e3a\u4e2d\u5fc3\u7684\u6536\u655b\u6a21\u578b\u65e0\u6cd5\u5904\u7406\u7684\u56e0\u679c\u6b67\u4e49\u95ee\u9898\uff1b\u4e14\u4efb\u4f55\u516c\u7406\u7684\u7f3a\u5931\u90fd\u4f1a\u5bfc\u81f4\u786e\u5b9a\u6027\u5d29\u6e83\u4e3a\u6b67\u4e49\u3002\u6240\u6709\u4fdd\u8bc1\u5747\u7531\u516c\u7406\u548c\u5f62\u5f0f\u5316\u8bc1\u660e\u786e\u7acb\u3002", "conclusion": "DCS \u6784\u6210\u4e86\u5f02\u6b65\u8ba1\u7b97\u4e2d\u6b63\u786e\u6027\u7684\u8fb9\u754c\u539f\u5219\uff0c\u7c7b\u4f3c\u4e8e CAP \u548c FLP \u5b9a\u7406\uff0c\u786e\u7acb\u4e86\u201c\u6b63\u786e\u6027\u5373\u5e95\u76d8\u201d\uff08Correctness-as-a-Chassis\uff09\u8303\u5f0f\uff0c\u4e3a\u6784\u5efa\u6a21\u5757\u5316\u3001\u5b89\u5168\u4e14\u53ef\u6f14\u5316\u7684\u5206\u5e03\u5f0f\u667a\u80fd\u7cfb\u7edf\u63d0\u4f9b\u57fa\u7840\u3002"}}
{"id": "2510.05762", "categories": ["cs.NI"], "pdf": "https://arxiv.org/pdf/2510.05762", "abs": "https://arxiv.org/abs/2510.05762", "authors": ["Kotha Kartheek", "Shankar K. Ghosh", "Megha Iyengar", "Vinod Sharma", "Souvik Deb"], "title": "A Deep Q-Network based power control mechanism to Minimize RLF driven Handover Failure in 5G Network", "comment": null, "summary": "The impact of Radio link failure (RLF) has been largely ignored in designing\nhandover algorithms, although RLF is a major contributor towards causing\nhandover failure (HF). RLF can cause HF if it is detected during an ongoing\nhandover. The objective of this work is to propose an efficient power control\nmechanism based on Deep Q-Network (DQN), considering handover parameters (i.e.,\ntime-to-preparation, time-to-execute, preparation offset, execution offset) and\nradio link monitoring parameters (T310 and N310) as input. The proposed DRL\nbased power control algorithm decides on a possible increase of transmitting\npower to avoid RLF driven HF. Simulation results show that the traditional\nconditional handover, when equipped with the proposed DRL based power control\nalgorithm can significantly reduce both RLFs and subsequent HFs, as compared to\nthe existing state of the art approaches.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u6df1\u5ea6Q\u7f51\u7edc\uff08DQN\uff09\u7684\u529f\u7387\u63a7\u5236\u673a\u5236\uff0c\u901a\u8fc7\u7ed3\u5408\u5207\u6362\u53c2\u6570\u548c\u65e0\u7ebf\u94fe\u8def\u76d1\u6d4b\u53c2\u6570\uff0c\u52a8\u6001\u8c03\u6574\u53d1\u5c04\u529f\u7387\u4ee5\u51cf\u5c11\u65e0\u7ebf\u94fe\u8def\u5931\u8d25\uff08RLF\uff09\u53ca\u5176\u5f15\u53d1\u7684\u5207\u6362\u5931\u8d25\uff08HF\uff09\uff0c\u4eff\u771f\u8868\u660e\u8be5\u65b9\u6cd5\u4f18\u4e8e\u73b0\u6709\u6280\u672f\u3002", "motivation": "\u73b0\u6709\u5207\u6362\u7b97\u6cd5\u8bbe\u8ba1\u4e2d\u666e\u904d\u5ffd\u89c6\u4e86\u65e0\u7ebf\u94fe\u8def\u5931\u8d25\uff08RLF\uff09\u7684\u5f71\u54cd\uff0c\u800cRLF\u662f\u5bfc\u81f4\u5207\u6362\u5931\u8d25\uff08HF\uff09\u7684\u4e3b\u8981\u539f\u56e0\u4e4b\u4e00\uff0c\u7279\u522b\u662f\u5728\u5207\u6362\u8fc7\u7a0b\u4e2d\u53d1\u751fRLF\u65f6\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\uff08DRL\uff09\u7684\u529f\u7387\u63a7\u5236\u7b97\u6cd5\uff0c\u5229\u7528\u6df1\u5ea6Q\u7f51\u7edc\uff08DQN\uff09\uff0c\u4ee5\u5207\u6362\u53c2\u6570\uff08\u5982\u51c6\u5907\u65f6\u95f4\u3001\u6267\u884c\u65f6\u95f4\u3001\u504f\u79fb\u91cf\uff09\u548c\u65e0\u7ebf\u94fe\u8def\u76d1\u63a7\u53c2\u6570\uff08T310\u3001N310\uff09\u4f5c\u4e3a\u8f93\u5165\uff0c\u52a8\u6001\u51b3\u7b56\u662f\u5426\u63d0\u5347\u53d1\u5c04\u529f\u7387\u4ee5\u907f\u514dRLF\u5f15\u53d1\u7684HF\u3002", "result": "\u4eff\u771f\u5b9e\u9a8c\u8868\u660e\uff0c\u5c06\u6240\u63d0DRL\u529f\u7387\u63a7\u5236\u7b97\u6cd5\u4e0e\u4f20\u7edf\u6761\u4ef6\u5207\u6362\u673a\u5236\u7ed3\u5408\uff0c\u80fd\u663e\u8457\u964d\u4f4eRLF\u548c\u7531\u6b64\u5bfc\u81f4\u7684HF\uff0c\u6027\u80fd\u4f18\u4e8e\u5f53\u524d\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\u3002", "conclusion": "\u5c06DRL\u9a71\u52a8\u7684\u529f\u7387\u63a7\u5236\u673a\u5236\u878d\u5165\u5207\u6362\u6d41\u7a0b\uff0c\u53ef\u6709\u6548\u7f13\u89e3RLF\u95ee\u9898\u5e76\u63d0\u5347\u5207\u6362\u53ef\u9760\u6027\uff0c\u4e3a\u672a\u6765\u65e0\u7ebf\u7f51\u7edc\u5207\u6362\u7b97\u6cd5\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2510.05441", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.05441", "abs": "https://arxiv.org/abs/2510.05441", "authors": ["Yiannis Charalambous", "Claudionor N. Coelho Jr", "Luis Lamb", "Lucas C. Cordeiro"], "title": "UnitTenX: Generating Tests for Legacy Packages with AI Agents Powered by Formal Verification", "comment": null, "summary": "This paper introduces UnitTenX, a state-of-the-art open-source AI multi-agent\nsystem designed to generate unit tests for legacy code, enhancing test coverage\nand critical value testing. UnitTenX leverages a combination of AI agents,\nformal methods, and Large Language Models (LLMs) to automate test generation,\naddressing the challenges posed by complex and legacy codebases. Despite the\nlimitations of LLMs in bug detection, UnitTenX offers a robust framework for\nimproving software reliability and maintainability. Our results demonstrate the\neffectiveness of this approach in generating high-quality tests and identifying\npotential issues. Additionally, our approach enhances the readability and\ndocumentation of legacy code.", "AI": {"tldr": "UnitTenX \u662f\u4e00\u4e2a\u7ed3\u5408 AI \u591a\u667a\u80fd\u4f53\u3001\u5f62\u5f0f\u5316\u65b9\u6cd5\u4e0e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u5f00\u6e90\u7cfb\u7edf\uff0c\u7528\u4e8e\u81ea\u52a8\u751f\u6210\u9057\u7559\u4ee3\u7801\u7684\u5355\u5143\u6d4b\u8bd5\uff0c\u63d0\u5347\u6d4b\u8bd5\u8986\u76d6\u7387\u3001\u4ee3\u7801\u53ef\u8bfb\u6027\u4e0e\u8f6f\u4ef6\u53ef\u9760\u6027\u3002", "motivation": "\u9057\u7559\u4ee3\u7801\u901a\u5e38\u7f3a\u4e4f\u6d4b\u8bd5\u8986\u76d6\uff0c\u4e14\u7ed3\u6784\u590d\u6742\uff0c\u4f20\u7edf\u65b9\u6cd5\u96be\u4ee5\u9ad8\u6548\u751f\u6210\u9ad8\u8d28\u91cf\u5355\u5143\u6d4b\u8bd5\uff1b\u540c\u65f6\u5927\u8bed\u8a00\u6a21\u578b\u5728\u7f3a\u9677\u68c0\u6d4b\u65b9\u9762\u5b58\u5728\u5c40\u9650\uff0c\u9700\u8981\u66f4\u9c81\u68d2\u7684\u6846\u67b6\u6765\u63d0\u5347\u8f6f\u4ef6\u53ef\u7ef4\u62a4\u6027\u3002", "method": "\u7ed3\u5408\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u3001\u5f62\u5f0f\u5316\u65b9\u6cd5\u548c\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u6784\u5efa\u540d\u4e3a UnitTenX \u7684\u81ea\u52a8\u5316\u5355\u5143\u6d4b\u8bd5\u751f\u6210\u6846\u67b6\u3002", "result": "UnitTenX \u80fd\u6709\u6548\u751f\u6210\u9ad8\u8d28\u91cf\u5355\u5143\u6d4b\u8bd5\uff0c\u53d1\u73b0\u6f5c\u5728\u95ee\u9898\uff0c\u5e76\u63d0\u5347\u9057\u7559\u4ee3\u7801\u7684\u53ef\u8bfb\u6027\u4e0e\u6587\u6863\u5316\u6c34\u5e73\u3002", "conclusion": "UnitTenX \u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u4e14\u9c81\u68d2\u7684\u65b9\u6cd5\uff0c\u663e\u8457\u6539\u5584\u4e86\u9057\u7559\u4ee3\u7801\u7684\u6d4b\u8bd5\u8986\u76d6\u4e0e\u8f6f\u4ef6\u53ef\u9760\u6027\uff0c\u5f25\u8865\u4e86\u7eaf\u5927\u8bed\u8a00\u6a21\u578b\u5728\u6d4b\u8bd5\u751f\u6210\u4e2d\u7684\u4e0d\u8db3\u3002"}}
{"id": "2510.05149", "categories": ["cs.DC", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.05149", "abs": "https://arxiv.org/abs/2510.05149", "authors": ["Clarisse Sousa", "Tiago Fonseca", "Luis Lino Ferreira", "Ricardo Ven\u00e2ncio", "Ricardo Severino"], "title": "Percepta: High Performance Stream Processing at the Edge", "comment": null, "summary": "The rise of real-time data and the proliferation of Internet of Things (IoT)\ndevices have highlighted the limitations of cloud-centric solutions,\nparticularly regarding latency, bandwidth, and privacy. These challenges have\ndriven the growth of Edge Computing. Associated with IoT appears a set of other\nproblems, like: data rate harmonization between multiple sources, protocol\nconversion, handling the loss of data and the integration with Artificial\nIntelligence (AI) models. This paper presents Percepta, a lightweight Data\nStream Processing (DSP) system tailored to support AI workloads at the edge,\nwith a particular focus on such as Reinforcement Learning (RL). It introduces\nspecialized features such as reward function computation, data storage for\nmodel retraining, and real-time data preparation to support continuous\ndecision-making. Additional functionalities include data normalization,\nharmonization across heterogeneous protocols and sampling rates, and robust\nhandling of missing or incomplete data, making it well suited for the\nchallenges of edge-based AI deployment.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86Percepta\uff0c\u4e00\u79cd\u8f7b\u91cf\u7ea7\u8fb9\u7f18\u6570\u636e\u6d41\u5904\u7406\u7cfb\u7edf\uff0c\u4e13\u4e3a\u652f\u6301\u8fb9\u7f18AI\uff08\u5c24\u5176\u662f\u5f3a\u5316\u5b66\u4e60\uff09\u800c\u8bbe\u8ba1\uff0c\u5177\u5907\u5956\u52b1\u51fd\u6570\u8ba1\u7b97\u3001\u6570\u636e\u5b58\u50a8\u3001\u5b9e\u65f6\u6570\u636e\u51c6\u5907\u3001\u534f\u8bae\u4e0e\u91c7\u6837\u7387\u534f\u8c03\u53ca\u7f3a\u5931\u6570\u636e\u5904\u7406\u7b49\u529f\u80fd\u3002", "motivation": "\u4e91\u4e2d\u5fc3\u67b6\u6784\u5728\u5b9e\u65f6\u6570\u636e\u548c\u7269\u8054\u7f51\uff08IoT\uff09\u573a\u666f\u4e0b\u9762\u4e34\u5ef6\u8fdf\u3001\u5e26\u5bbd\u548c\u9690\u79c1\u7b49\u95ee\u9898\uff0c\u4e14IoT\u8fd8\u5e26\u6765\u6570\u636e\u901f\u7387\u534f\u8c03\u3001\u534f\u8bae\u8f6c\u6362\u3001\u6570\u636e\u4e22\u5931\u5904\u7406\u4ee5\u53ca\u4e0eAI\u6a21\u578b\u96c6\u6210\u7b49\u6311\u6218\u3002", "method": "\u8bbe\u8ba1\u5e76\u5b9e\u73b0\u540d\u4e3aPercepta\u7684\u8f7b\u91cf\u7ea7\u6570\u636e\u6d41\u5904\u7406\u7cfb\u7edf\uff0c\u96c6\u6210\u5956\u52b1\u51fd\u6570\u8ba1\u7b97\u3001\u6a21\u578b\u518d\u8bad\u7ec3\u6570\u636e\u5b58\u50a8\u3001\u5b9e\u65f6\u6570\u636e\u51c6\u5907\u3001\u6570\u636e\u5f52\u4e00\u5316\u3001\u5f02\u6784\u534f\u8bae\u4e0e\u91c7\u6837\u7387\u534f\u8c03\u3001\u7f3a\u5931\u6570\u636e\u9c81\u68d2\u5904\u7406\u7b49\u7279\u6027\u3002", "result": "Percepta\u80fd\u6709\u6548\u652f\u6301\u8fb9\u7f18\u7aefAI\u5de5\u4f5c\u8d1f\u8f7d\uff0c\u7279\u522b\u662f\u5f3a\u5316\u5b66\u4e60\u4efb\u52a1\uff0c\u5e94\u5bf9\u8fb9\u7f18\u73af\u5883\u4e2d\u6570\u636e\u5f02\u6784\u6027\u4e0e\u4e0d\u5b8c\u6574\u6027\u7b49\u5b9e\u9645\u6311\u6218\u3002", "conclusion": "Percepta\u4e3a\u8fb9\u7f18AI\u90e8\u7f72\u63d0\u4f9b\u4e86\u4e00\u4e2a\u9ad8\u6548\u3001\u8f7b\u91cf\u4e14\u529f\u80fd\u5168\u9762\u7684\u6570\u636e\u6d41\u5904\u7406\u89e3\u51b3\u65b9\u6848\uff0c\u7279\u522b\u9002\u7528\u4e8e\u9700\u8981\u6301\u7eed\u51b3\u7b56\u7684\u5f3a\u5316\u5b66\u4e60\u5e94\u7528\u573a\u666f\u3002"}}
{"id": "2510.05797", "categories": ["cs.NI"], "pdf": "https://arxiv.org/pdf/2510.05797", "abs": "https://arxiv.org/abs/2510.05797", "authors": ["Mathias Thorsager", "Israel Leyva-Mayorga", "Petar Popovski"], "title": "Leveraging Generative AI for large-scale prediction-based networking", "comment": "6 pages, 3 figures", "summary": "The traditional role of the network layer is to create an end-to-end route,\nthrough which the intermediate nodes replicate and forward the packets towards\nthe destination. This role can be radically redefined by exploiting the power\nof Generative AI (GenAI) to pivot towards a prediction-based network layer,\nwhich addresses the problems of throughput limits and uncontrollable latency.\nIn the context of real-time delivery of image content, the use of GenAI-aided\nnetwork nodes has been shown to improve the flow arriving at the destination by\nmore than 100%. However, to successfully exploit GenAI nodes and achieve such\ntransition, we must provide solutions for the problems which arise as we scale\nthe networks to include large amounts of users and multiple data modalities\nother than images. We present three directions that play a significant role in\nenabling the use of GenAI as a network layer tool at a large scale. In terms of\ndesign, we emphasize the need for initialization protocols to select the prompt\nsize efficiently. Next, we consider the use case of GenAI as a tool to ensure\ntimely delivery of data, as well as an alternative to traditional TCP\ncongestion control algorithms.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u5229\u7528\u751f\u6210\u5f0f\u4eba\u5de5\u667a\u80fd\uff08GenAI\uff09\u91cd\u6784\u7f51\u7edc\u5c42\uff0c\u4ece\u4f20\u7edf\u7684\u7aef\u5230\u7aef\u8def\u7531\u8f6c\u5411\u57fa\u4e8e\u9884\u6d4b\u7684\u7f51\u7edc\u5c42\uff0c\u4ee5\u7a81\u7834\u541e\u5410\u91cf\u9650\u5236\u5e76\u63a7\u5236\u5ef6\u8fdf\uff0c\u5728\u56fe\u50cf\u5185\u5bb9\u5b9e\u65f6\u4f20\u8f93\u4e2d\u5df2\u5b9e\u73b0\u8d85\u8fc7100%\u7684\u6027\u80fd\u63d0\u5347\uff0c\u5e76\u63a2\u8ba8\u4e86\u5728\u5927\u89c4\u6a21\u591a\u6a21\u6001\u573a\u666f\u4e0b\u90e8\u7f72GenAI\u7f51\u7edc\u8282\u70b9\u7684\u4e09\u4e2a\u5173\u952e\u65b9\u5411\u3002", "motivation": "\u4f20\u7edf\u7f51\u7edc\u5c42\u5728\u541e\u5410\u91cf\u548c\u5ef6\u8fdf\u63a7\u5236\u65b9\u9762\u5b58\u5728\u5c40\u9650\uff0c\u5c24\u5176\u5728\u5b9e\u65f6\u56fe\u50cf\u4f20\u8f93\u7b49\u573a\u666f\u4e0b\u96be\u4ee5\u6ee1\u8db3\u9700\u6c42\u3002\u4f5c\u8005\u5e0c\u671b\u901a\u8fc7\u5f15\u5165\u751f\u6210\u5f0f\u4eba\u5de5\u667a\u80fd\uff08GenAI\uff09\u91cd\u6784\u7f51\u7edc\u5c42\u529f\u80fd\uff0c\u4ee5\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u5e76\u63d0\u5347\u7f51\u7edc\u6027\u80fd\u3002", "method": "\u6587\u7ae0\u63d0\u51fa\u5c06GenAI\u96c6\u6210\u5230\u7f51\u7edc\u8282\u70b9\u4e2d\uff0c\u6784\u5efa\u57fa\u4e8e\u9884\u6d4b\u7684\u7f51\u7edc\u5c42\uff1b\u5e76\u4ece\u4e09\u4e2a\u65b9\u9762\u652f\u6301\u5176\u5927\u89c4\u6a21\u90e8\u7f72\uff1a\u8bbe\u8ba1\u9ad8\u6548\u7684\u63d0\u793a\uff08prompt\uff09\u521d\u59cb\u5316\u534f\u8bae\u3001\u5229\u7528GenAI\u4fdd\u969c\u6570\u636e\u7684\u53ca\u65f6\u4ea4\u4ed8\u3001\u4ee5\u53ca\u5c06\u5176\u4f5c\u4e3a\u4f20\u7edfTCP\u62e5\u585e\u63a7\u5236\u7b97\u6cd5\u7684\u66ff\u4ee3\u65b9\u6848\u3002", "result": "\u5728\u56fe\u50cf\u5185\u5bb9\u5b9e\u65f6\u4f20\u8f93\u573a\u666f\u4e2d\uff0c\u91c7\u7528GenAI\u8f85\u52a9\u7684\u7f51\u7edc\u8282\u70b9\u53ef\u4f7f\u5230\u8fbe\u76ee\u7684\u5730\u7684\u6570\u636e\u6d41\u6027\u80fd\u63d0\u5347\u8d85\u8fc7100%\u3002", "conclusion": "GenAI\u6709\u6f5c\u529b\u5f7b\u5e95\u6539\u53d8\u7f51\u7edc\u5c42\u7684\u8bbe\u8ba1\u8303\u5f0f\uff0c\u4f46\u9700\u89e3\u51b3\u5927\u89c4\u6a21\u90e8\u7f72\u4e2d\u7684\u63d0\u793a\u521d\u59cb\u5316\u3001\u591a\u6a21\u6001\u652f\u6301\u548c\u4f20\u8f93\u63a7\u5236\u7b49\u5173\u952e\u95ee\u9898\uff0c\u624d\u80fd\u5b9e\u73b0\u5176\u4f5c\u4e3a\u7f51\u7edc\u5c42\u5de5\u5177\u7684\u5e7f\u6cdb\u5e94\u7528\u3002"}}
{"id": "2510.05450", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.05450", "abs": "https://arxiv.org/abs/2510.05450", "authors": ["Saul Goldman", "Hong Yi Lin", "Jirat Pasuksmit", "Patanamon Thongtanunam", "Kla Tantithamthavorn", "Zhe Wang", "Ray Zhang", "Ali Behnaz", "Fan Jiang", "Michael Siers", "Ryan Jiang", "Mike Buller", "Minwoo Jeong", "Ming Wu"], "title": "What Types of Code Review Comments Do Developers Most Frequently Resolve?", "comment": "The paper has been accepted the 40th IEEE/ACM International\n  Conference on Automated Software Engineering, ASE 2025", "summary": "Large language model (LLM)-powered code review automation tools have been\nintroduced to generate code review comments. However, not all generated\ncomments will drive code changes. Understanding what types of generated review\ncomments are likely to trigger code changes is crucial for identifying those\nthat are actionable. In this paper, we set out to investigate (1) the types of\nreview comments written by humans and LLMs, and (2) the types of generated\ncomments that are most frequently resolved by developers. To do so, we\ndeveloped an LLM-as-a-Judge to automatically classify review comments based on\nour own taxonomy of five categories. Our empirical study confirms that (1) the\nLLM reviewer and human reviewers exhibit distinct strengths and weaknesses\ndepending on the project context, and (2) readability, bugs, and\nmaintainability-related comments had higher resolution rates than those focused\non code design. These results suggest that a substantial proportion of\nLLM-generated comments are actionable and can be resolved by developers. Our\nwork highlights the complementarity between LLM and human reviewers and offers\nsuggestions to improve the practical effectiveness of LLM-powered code review\ntools.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u4e0e\u4eba\u7c7b\u5728\u4ee3\u7801\u5ba1\u67e5\u8bc4\u8bba\u4e2d\u7684\u7c7b\u578b\u5dee\u5f02\uff0c\u5e76\u53d1\u73b0\u53ef\u8bfb\u6027\u3001\u7f3a\u9677\u548c\u53ef\u7ef4\u62a4\u6027\u76f8\u5173\u7684LLM\u8bc4\u8bba\u66f4\u6613\u88ab\u5f00\u53d1\u8005\u91c7\u7eb3\uff0c\u8868\u660eLLM\u751f\u6210\u7684\u8bc4\u8bba\u5177\u6709\u8f83\u9ad8\u53ef\u64cd\u4f5c\u6027\u3002", "motivation": "\u7406\u89e3\u54ea\u4e9b\u7c7b\u578b\u7684LLM\u751f\u6210\u7684\u4ee3\u7801\u5ba1\u67e5\u8bc4\u8bba\u66f4\u53ef\u80fd\u4fc3\u4f7f\u5f00\u53d1\u8005\u4fee\u6539\u4ee3\u7801\uff0c\u6709\u52a9\u4e8e\u8bc6\u522b\u53ef\u64cd\u4f5c\u7684\u8bc4\u8bba\uff0c\u63d0\u5347LLM\u9a71\u52a8\u4ee3\u7801\u5ba1\u67e5\u5de5\u5177\u7684\u5b9e\u9645\u6548\u679c\u3002", "method": "\u4f5c\u8005\u6784\u5efa\u4e86\u4e00\u4e2a\u201cLLM-as-a-Judge\u201d\u7cfb\u7edf\uff0c\u57fa\u4e8e\u81ea\u5b9a\u4e49\u7684\u4e94\u7c7b\u5206\u7c7b\u6cd5\u81ea\u52a8\u5bf9\u4eba\u7c7b\u548cLLM\u751f\u6210\u7684\u4ee3\u7801\u5ba1\u67e5\u8bc4\u8bba\u8fdb\u884c\u5206\u7c7b\uff0c\u5e76\u901a\u8fc7\u5b9e\u8bc1\u7814\u7a76\u6bd4\u8f83\u5176\u7c7b\u578b\u5206\u5e03\u4e0e\u89e3\u51b3\u7387\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff1a(1) LLM\u4e0e\u4eba\u7c7b\u5ba1\u67e5\u8005\u5728\u4e0d\u540c\u9879\u76ee\u80cc\u666f\u4e0b\u5404\u6709\u4f18\u52a3\uff1b(2) \u53ef\u8bfb\u6027\u3001\u7f3a\u9677\u548c\u53ef\u7ef4\u62a4\u6027\u76f8\u5173\u7684\u8bc4\u8bba\u6bd4\u4ee3\u7801\u8bbe\u8ba1\u7c7b\u8bc4\u8bba\u6709\u66f4\u9ad8\u7684\u89e3\u51b3\u7387\u3002", "conclusion": "LLM\u751f\u6210\u7684\u8bc4\u8bba\u4e2d\u6709\u76f8\u5f53\u4e00\u90e8\u5206\u662f\u53ef\u64cd\u4f5c\u7684\uff0cLLM\u4e0e\u4eba\u7c7b\u5ba1\u67e5\u8005\u5177\u6709\u4e92\u8865\u6027\uff0c\u7814\u7a76\u7ed3\u679c\u4e3a\u63d0\u5347LLM\u4ee3\u7801\u5ba1\u67e5\u5de5\u5177\u7684\u6709\u6548\u6027\u63d0\u4f9b\u4e86\u5efa\u8bae\u3002"}}
{"id": "2510.05164", "categories": ["cs.DC", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.05164", "abs": "https://arxiv.org/abs/2510.05164", "authors": ["Yuanzhe Shen", "Yide Liu", "Zisu Huang", "Ruicheng Yin", "Xiaoqing Zheng", "Xuanjing Huang"], "title": "SATER: A Self-Aware and Token-Efficient Approach to Routing and Cascading", "comment": "Accepted to EMNLP 2025 Main", "summary": "Large language models (LLMs) demonstrate remarkable performance across\ndiverse tasks, yet their effectiveness frequently depends on costly commercial\nAPIs or cloud services. Model selection thus entails a critical trade-off\nbetween performance and cost: high-performing LLMs typically incur substantial\nexpenses, whereas budget-friendly small language models (SLMs) are constrained\nby limited capabilities. Current research primarily proposes two routing\nstrategies: pre-generation routing and cascade routing. Both approaches have\ndistinct characteristics, with cascade routing typically offering superior\ncost-effectiveness and accuracy despite its higher latency. To further address\nthe limitations of both approaches, we introduce SATER, a dual-mode compatible\napproach that fine-tunes models through shortest-response preference\noptimization and a confidence-aware rejection mechanism. SATER significantly\nreduces redundant outputs and response times, while improving both the\nperformance of pre-generation routing and the efficiency of cascade routing.\nExperiments across three SLMs and six datasets, varying in type and complexity,\ndemonstrate that SATER achieves comparable performance while consistently\nreducing computational costs by over 50\\% and cascade latency by over 80\\%.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faSATER\uff0c\u4e00\u79cd\u517c\u5bb9\u9884\u751f\u6210\u8def\u7531\u548c\u7ea7\u8054\u8def\u7531\u7684\u53cc\u6a21\u5f0f\u65b9\u6cd5\uff0c\u901a\u8fc7\u6700\u77ed\u54cd\u5e94\u504f\u597d\u4f18\u5316\u548c\u7f6e\u4fe1\u5ea6\u611f\u77e5\u62d2\u7edd\u673a\u5236\uff0c\u5728\u4fdd\u6301\u6027\u80fd\u7684\u540c\u65f6\u663e\u8457\u964d\u4f4e\u8ba1\u7b97\u6210\u672c\u548c\u5ef6\u8fdf\u3002", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u867d\u6027\u80fd\u5f3a\u5927\uff0c\u4f46\u4f9d\u8d56\u6602\u8d35\u7684\u5546\u4e1aAPI\u6216\u4e91\u670d\u52a1\uff0c\u800c\u5c0f\u578b\u8bed\u8a00\u6a21\u578b\uff08SLM\uff09\u867d\u6210\u672c\u4f4e\u4f46\u80fd\u529b\u6709\u9650\u3002\u73b0\u6709\u8def\u7531\u7b56\u7565\uff08\u9884\u751f\u6210\u8def\u7531\u548c\u7ea7\u8054\u8def\u7531\uff09\u5404\u6709\u5c40\u9650\uff0c\u4e9f\u9700\u517c\u987e\u6027\u80fd\u3001\u6210\u672c\u4e0e\u6548\u7387\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u63d0\u51faSATER\u65b9\u6cd5\uff0c\u7ed3\u5408\u6700\u77ed\u54cd\u5e94\u504f\u597d\u4f18\u5316\u548c\u7f6e\u4fe1\u5ea6\u611f\u77e5\u62d2\u7edd\u673a\u5236\uff0c\u5bf9\u6a21\u578b\u8fdb\u884c\u5fae\u8c03\uff0c\u4ee5\u51cf\u5c11\u5197\u4f59\u8f93\u51fa\u548c\u54cd\u5e94\u65f6\u95f4\uff0c\u540c\u65f6\u63d0\u5347\u9884\u751f\u6210\u8def\u7531\u6027\u80fd\u548c\u7ea7\u8054\u8def\u7531\u6548\u7387\u3002", "result": "\u5728\u4e09\u79cdSLM\u548c\u516d\u4e2a\u4e0d\u540c\u7c7b\u578b\u7684\u590d\u6742\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cSATER\u5728\u6027\u80fd\u76f8\u5f53\u7684\u524d\u63d0\u4e0b\uff0c\u8ba1\u7b97\u6210\u672c\u964d\u4f4e\u8d85\u8fc750%\uff0c\u7ea7\u8054\u5ef6\u8fdf\u51cf\u5c11\u8d85\u8fc780%\u3002", "conclusion": "SATER\u6709\u6548\u5e73\u8861\u4e86\u8bed\u8a00\u6a21\u578b\u7684\u6027\u80fd\u4e0e\u6210\u672c\uff0c\u663e\u8457\u63d0\u5347\u4e86\u73b0\u6709\u8def\u7531\u7b56\u7565\u7684\u6548\u7387\uff0c\u4e3a\u5b9e\u9645\u90e8\u7f72\u63d0\u4f9b\u4e86\u66f4\u5177\u6027\u4ef7\u6bd4\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.05604", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.05604", "abs": "https://arxiv.org/abs/2510.05604", "authors": ["Rintaro Kanaji", "Brittany Reid", "Yutaro Kashiwa", "Raula Gaikovina Kula", "Hajimu Iida"], "title": "An Empirical Study of Security-Policy Related Issues in Open Source Projects", "comment": "Accepted in PROFES 2025", "summary": "GitHub recommends that projects adopt a SECURITY.md file that outlines\nvulnerability reporting procedures. However, the effectiveness and operational\nchallenges of such files are not yet fully understood. This study aims to\nclarify the challenges that SECURITY.md files face in the vulnerability\nreporting process within open-source communities. Specifically, we classified\nand analyzed the content of 711 randomly sampled issues related to SECURITY.md.\nWe also conducted a quantitative comparative analysis of the close time and\nnumber of responses for issues concerning six community health files, including\nSECURITY.md. Our analysis revealed that 79.5% of SECURITY.md-related issues\nwere requests to add the file, and reports that included links were closed,\nwith a median time that was 2 days shorter. These findings offer practical\ninsights for improving security reporting policies and community management,\nultimately contributing to a more secure open-source ecosystem.", "AI": {"tldr": "\u8be5\u7814\u7a76\u5206\u6790\u4e86\u5f00\u6e90\u9879\u76ee\u4e2dSECURITY.md\u6587\u4ef6\u5728\u6f0f\u6d1e\u62a5\u544a\u6d41\u7a0b\u4e2d\u7684\u4f5c\u7528\u4e0e\u6311\u6218\uff0c\u53d1\u73b0\u5927\u591a\u6570\u76f8\u5173\u8bae\u9898\u662f\u8bf7\u6c42\u6dfb\u52a0\u8be5\u6587\u4ef6\uff0c\u4e14\u5305\u542b\u94fe\u63a5\u7684\u8bae\u9898\u5173\u95ed\u66f4\u5feb\u3002", "motivation": "GitHub\u5efa\u8bae\u9879\u76ee\u91c7\u7528SECURITY.md\u6587\u4ef6\u4ee5\u89c4\u8303\u6f0f\u6d1e\u62a5\u544a\u6d41\u7a0b\uff0c\u4f46\u5176\u5b9e\u9645\u6548\u679c\u548c\u64cd\u4f5c\u6311\u6218\u5c1a\u4e0d\u6e05\u695a\uff0c\u56e0\u6b64\u9700\u8981\u6df1\u5165\u7814\u7a76\u3002", "method": "\u7814\u7a76\u5bf9711\u4e2a\u968f\u673a\u62bd\u6837\u7684SECURITY.md\u76f8\u5173\u8bae\u9898\u8fdb\u884c\u5206\u7c7b\u548c\u5185\u5bb9\u5206\u6790\uff0c\u5e76\u5bf9\u5305\u62ecSECURITY.md\u5728\u5185\u7684\u516d\u79cd\u793e\u533a\u5065\u5eb7\u6587\u4ef6\u76f8\u5173\u8bae\u9898\u7684\u5173\u95ed\u65f6\u95f4\u548c\u56de\u590d\u6570\u91cf\u8fdb\u884c\u4e86\u5b9a\u91cf\u6bd4\u8f83\u5206\u6790\u3002", "result": "79.5%\u7684SECURITY.md\u76f8\u5173\u8bae\u9898\u662f\u8bf7\u6c42\u6dfb\u52a0\u8be5\u6587\u4ef6\uff1b\u5305\u542b\u94fe\u63a5\u7684\u8bae\u9898\u4e2d\u4f4d\u5173\u95ed\u65f6\u95f4\u7f29\u77ed\u4e862\u5929\u3002", "conclusion": "\u7814\u7a76\u4e3a\u6539\u8fdb\u5b89\u5168\u62a5\u544a\u653f\u7b56\u548c\u793e\u533a\u7ba1\u7406\u63d0\u4f9b\u4e86\u5b9e\u7528\u89c1\u89e3\uff0c\u6709\u52a9\u4e8e\u6784\u5efa\u66f4\u5b89\u5168\u7684\u5f00\u6e90\u751f\u6001\u7cfb\u7edf\u3002"}}
{"id": "2510.05186", "categories": ["cs.DC", "cs.AI", "math.OC"], "pdf": "https://arxiv.org/pdf/2510.05186", "abs": "https://arxiv.org/abs/2510.05186", "authors": ["Hongpei Li", "Han Zhang", "Huikang Liu", "Dongdong Ge", "Yinyu Ye"], "title": "OptPipe: Memory- and Scheduling-Optimized Pipeline Parallelism for LLM Training", "comment": "Use Mathematical Programming to model Pipeline Parallelism with\n  Offloading to balance efficiency and memory requirement", "summary": "Pipeline parallelism (PP) has become a standard technique for scaling large\nlanguage model (LLM) training across multiple devices. However, despite recent\nprogress in reducing memory consumption through activation offloading, existing\napproaches remain largely heuristic and coarse-grained, often overlooking the\nfine-grained trade-offs between memory, computation, and scheduling latency. In\nthis work, we revisit the pipeline scheduling problem from a principled\noptimization perspective. We observe that prevailing strategies either rely on\nstatic rules or aggressively offload activations without fully leveraging the\ninteraction between memory constraints and scheduling efficiency. To address\nthis, we formulate scheduling as a constrained optimization problem that\njointly accounts for memory capacity, activation reuse, and pipeline bubble\nminimization. Solving this model yields fine-grained schedules that reduce\npipeline bubbles while adhering to strict memory budgets. Our approach\ncomplements existing offloading techniques: whereas prior approaches trade\nmemory for time in a fixed pattern, we dynamically optimize the tradeoff with\nrespect to model structure and hardware configuration. Experimental results\ndemonstrate that our method consistently improves both throughput and memory\nutilization. In particular, we reduce idle pipeline time by up to 50% under the\nsame per-device memory limit, and in some cases, enable the training of larger\nmodels within limited memory budgets.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u4f18\u5316\u7684\u7ec6\u7c92\u5ea6\u6d41\u6c34\u7ebf\u8c03\u5ea6\u65b9\u6cd5\uff0c\u901a\u8fc7\u8054\u5408\u4f18\u5316\u5185\u5b58\u9650\u5236\u3001\u6fc0\u6d3b\u91cd\u7528\u548c\u6d41\u6c34\u7ebf\u6c14\u6ce1\u6700\u5c0f\u5316\uff0c\u5728\u76f8\u540c\u5185\u5b58\u9650\u5236\u4e0b\u6700\u591a\u51cf\u5c1150%\u7684\u7a7a\u95f2\u65f6\u95f4\uff0c\u5e76\u63d0\u5347\u541e\u5410\u91cf\u4e0e\u5185\u5b58\u5229\u7528\u7387\u3002", "motivation": "\u73b0\u6709\u6d41\u6c34\u7ebf\u5e76\u884c\u4e2d\u7684\u6fc0\u6d3b\u5378\u8f7d\u7b56\u7565\u591a\u4e3a\u542f\u53d1\u5f0f\u4e14\u7c92\u5ea6\u8f83\u7c97\uff0c\u672a\u80fd\u5145\u5206\u6743\u8861\u5185\u5b58\u3001\u8ba1\u7b97\u4e0e\u8c03\u5ea6\u5ef6\u8fdf\u4e4b\u95f4\u7684\u7ec6\u7c92\u5ea6\u5173\u7cfb\uff0c\u4e5f\u672a\u6709\u6548\u7ed3\u5408\u5185\u5b58\u7ea6\u675f\u4e0e\u8c03\u5ea6\u6548\u7387\u3002", "method": "\u5c06\u6d41\u6c34\u7ebf\u8c03\u5ea6\u5efa\u6a21\u4e3a\u4e00\u4e2a\u5e26\u7ea6\u675f\u7684\u4f18\u5316\u95ee\u9898\uff0c\u7efc\u5408\u8003\u8651\u8bbe\u5907\u5185\u5b58\u5bb9\u91cf\u3001\u6fc0\u6d3b\u91cd\u7528\u673a\u4f1a\u548c\u6d41\u6c34\u7ebf\u6c14\u6ce1\u6700\u5c0f\u5316\uff0c\u52a8\u6001\u751f\u6210\u7ec6\u7c92\u5ea6\u8c03\u5ea6\u7b56\u7565\u3002", "result": "\u5728\u76f8\u540c\u6bcf\u8bbe\u5907\u5185\u5b58\u9650\u5236\u4e0b\uff0c\u7a7a\u95f2\u6d41\u6c34\u7ebf\u65f6\u95f4\u6700\u591a\u51cf\u5c1150%\uff0c\u540c\u65f6\u63d0\u5347\u541e\u5410\u91cf\u548c\u5185\u5b58\u5229\u7528\u7387\uff0c\u90e8\u5206\u573a\u666f\u4e0b\u8fd8\u80fd\u652f\u6301\u66f4\u5927\u6a21\u578b\u7684\u8bad\u7ec3\u3002", "conclusion": "\u901a\u8fc7\u5c06\u8c03\u5ea6\u95ee\u9898\u5f62\u5f0f\u5316\u4e3a\u4f18\u5316\u6a21\u578b\uff0c\u672c\u6587\u65b9\u6cd5\u80fd\u52a8\u6001\u9002\u5e94\u6a21\u578b\u7ed3\u6784\u4e0e\u786c\u4ef6\u914d\u7f6e\uff0c\u5728\u4e25\u683c\u5185\u5b58\u7ea6\u675f\u4e0b\u663e\u8457\u63d0\u5347\u6d41\u6c34\u7ebf\u5e76\u884c\u6548\u7387\u3002"}}
{"id": "2510.05705", "categories": ["cs.SE", "cs.DL", "q-bio.OT"], "pdf": "https://arxiv.org/pdf/2510.05705", "abs": "https://arxiv.org/abs/2510.05705", "authors": ["Eva Mart\u00edn del Pico", "Josep Llu\u00eds Gelp\u00ed", "Salvador Capella-Guti\u00e9rrez"], "title": "The Software Observatory: aggregating and analysing software metadata for trend computation and FAIR assessment", "comment": null, "summary": "In the ever-changing realm of research software development, it is crucial\nfor the scientific community to grasp current trends to identify gaps that can\npotentially hinder scientific progress. The adherence to the FAIR (Findable,\nAccessible, Interoperable, Reusable) principles can serve as a proxy to\nunderstand those trends and provide a mechanism to propose specific actions.\n  The Software Observatory at OpenEBench\n(https://openebench.bsc.es/observatory) is a novel web portal that consolidates\nsoftware metadata from various sources, offering comprehensive insights into\ncritical research software aspects. Our platform enables users to analyse\ntrends, identify patterns and advancements within the Life Sciences research\nsoftware ecosystem, and understand its evolution over time. It also evaluates\nresearch software according to FAIR principles for research software, providing\nscores for different indicators.\n  Users have the ability to visualise this metadata at different levels of\ngranularity, ranging from the entire software landscape to specific communities\nto individual software entries through the FAIRsoft Evaluator. Indeed, the\nFAIRsoft Evaluator component streamlines the assessment process, helping\ndevelopers efficiently evaluate and obtain guidance to improve their software's\nFAIRness.\n  The Software Observatory represents a valuable resource for researchers and\nsoftware developers, as well as stakeholders, promoting better software\ndevelopment practices and adherence to FAIR principles for research software.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86OpenEBench\u7684Software Observatory\u5e73\u53f0\uff0c\u8be5\u5e73\u53f0\u6574\u5408\u591a\u6e90\u7814\u7a76\u8f6f\u4ef6\u5143\u6570\u636e\uff0c\u901a\u8fc7FAIRsoft Evaluator\u8bc4\u4f30\u5e76\u53ef\u89c6\u5316\u8f6f\u4ef6\u5728FAIR\uff08\u53ef\u53d1\u73b0\u3001\u53ef\u8bbf\u95ee\u3001\u4e92\u64cd\u4f5c\u3001\u53ef\u91cd\u7528\uff09\u539f\u5219\u4e0b\u7684\u8868\u73b0\uff0c\u4ee5\u4fc3\u8fdb\u751f\u547d\u79d1\u5b66\u9886\u57df\u7814\u7a76\u8f6f\u4ef6\u7684\u53d1\u5c55\u4e0e\u6539\u8fdb\u3002", "motivation": "\u79d1\u7814\u8f6f\u4ef6\u5f00\u53d1\u9886\u57df\u53d8\u5316\u8fc5\u901f\uff0c\u79d1\u5b66\u754c\u9700\u638c\u63e1\u5f53\u524d\u8d8b\u52bf\u4ee5\u8bc6\u522b\u53ef\u80fd\u963b\u788d\u79d1\u7814\u8fdb\u5c55\u7684\u5dee\u8ddd\uff1bFAIR\u539f\u5219\u53ef\u4f5c\u4e3a\u7406\u89e3\u8d8b\u52bf\u548c\u63d0\u51fa\u6539\u8fdb\u63aa\u65bd\u7684\u4f9d\u636e\u3002", "method": "\u6784\u5efaSoftware Observatory\u7f51\u7edc\u95e8\u6237\uff0c\u6574\u5408\u591a\u6e90\u8f6f\u4ef6\u5143\u6570\u636e\uff0c\u5e76\u901a\u8fc7FAIRsoft Evaluator\u7ec4\u4ef6\u5bf9\u7814\u7a76\u8f6f\u4ef6\u8fdb\u884cFAIR\u6027\u8bc4\u4f30\uff0c\u63d0\u4f9b\u4e0d\u540c\u7c92\u5ea6\u7684\u53ef\u89c6\u5316\u5206\u6790\u3002", "result": "\u5e73\u53f0\u652f\u6301\u7528\u6237\u5206\u6790\u751f\u547d\u79d1\u5b66\u7814\u7a76\u8f6f\u4ef6\u751f\u6001\u7684\u8d8b\u52bf\u4e0e\u6f14\u8fdb\uff0c\u8bc4\u4f30\u8f6f\u4ef6FAIR\u6027\u5e76\u83b7\u5f97\u6539\u8fdb\u5efa\u8bae\uff0c\u63d0\u5347\u8f6f\u4ef6\u5f00\u53d1\u5b9e\u8df5\u3002", "conclusion": "Software Observatory\u4e3a\u7814\u7a76\u4eba\u5458\u3001\u5f00\u53d1\u8005\u548c\u5229\u76ca\u76f8\u5173\u8005\u63d0\u4f9b\u4e86\u5b9d\u8d35\u7684\u8d44\u6e90\uff0c\u6709\u52a9\u4e8e\u63a8\u52a8\u7814\u7a76\u8f6f\u4ef6\u9075\u5faaFAIR\u539f\u5219\u5e76\u63d0\u5347\u6574\u4f53\u8d28\u91cf\u3002"}}
{"id": "2510.05254", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2510.05254", "abs": "https://arxiv.org/abs/2510.05254", "authors": ["Filipp Sporykhin", "Holger Homann"], "title": "Performance of a high-order MPI-Kokkos accelerated fluid solver", "comment": "12 pages, 16 figures. submitted to Computer Physics Communications", "summary": "This work discusses the performance of a modern numerical scheme for fluid\ndynamical problems on modern high-performance computing architectures. Our code\nimplements a spatial nodal discontinuous Galerkin scheme that we test up to an\norder of convergence of eight. It is temporally coupled to a set of Runge-Kutta\nmethods of orders up to six. The code integrates the linear advection equations\nas well as the isothermal Euler equations in one, two, and three dimensions. In\norder to target modern hardware involving many-core Central Processing Units\nand accelerators such as Graphic Processing Units we use the Kokkos library in\nconjunction with the Message Passing Interface to run our single source code on\nvarious GPU systems. We find that the higher the order the faster is the code.\nEighth-order simulations attain a given global error with much less computing\ntime than third- or fourth-order simulations. The RK scheme has a smaller\nimpact on the code performance and a classical fourth-order scheme seems to\ngenerally be a good choice. The code performs very well on all considered GPUs.\nThe many-CPU performance is also very good and perfect weak scaling is observed\nup to many hundreds of CPU cores using MPI. We note that small grid-size\nsimulations are faster on CPUs than on GPUs while GPUs win significantly over\nCPUs for simulations involving more than $10^7$ degrees of freedom ($\\approx\n3100^2$ grid points). When it comes to the environmental impact of numerical\nsimulations we estimate that GPUs consume less energy than CPUs for large\ngrid-size simulations but more energy on small grids. We observe a tendency\nthat the more modern is the GPU the larger needs to be the grid in order to use\nit efficiently. This yields a rebound effect because larger simulations need\nlonger computing times and in turn more energy that is not compensated by the\nenergy efficiency gain of the newer GPUs.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u4e00\u79cd\u57fa\u4e8e\u4e0d\u8fde\u7eedGalerkin\u65b9\u6cd5\u548cRunge-Kutta\u65f6\u95f4\u79ef\u5206\u7684\u9ad8\u9636\u6570\u503c\u683c\u5f0f\u5728\u73b0\u4ee3\u9ad8\u6027\u80fd\u8ba1\u7b97\u67b6\u6784\uff08\u5305\u62ec\u591a\u6838CPU\u548cGPU\uff09\u4e0a\u7684\u6027\u80fd\u8868\u73b0\uff0c\u53d1\u73b0\u9ad8\u9636\u65b9\u6cd5\uff08\u5982\u516b\u9636\uff09\u5728\u8fbe\u5230\u76f8\u540c\u5168\u5c40\u8bef\u5dee\u65f6\u8ba1\u7b97\u6548\u7387\u66f4\u9ad8\uff0c\u4e14GPU\u5728\u5927\u89c4\u6a21\u95ee\u9898\u4e0a\u4f18\u4e8eCPU\uff0c\u4f46\u5c0f\u89c4\u6a21\u95ee\u9898\u5219\u76f8\u53cd\uff1b\u540c\u65f6\u6307\u51fa\u65b0\u4e00\u4ee3GPU\u867d\u80fd\u6548\u66f4\u9ad8\uff0c\u4f46\u9700\u66f4\u5927\u89c4\u6a21\u95ee\u9898\u624d\u80fd\u53d1\u6325\u4f18\u52bf\uff0c\u53ef\u80fd\u5f15\u53d1\u80fd\u8017\u53cd\u5f39\u6548\u5e94\u3002", "motivation": "\u8bc4\u4f30\u9ad8\u9636\u6570\u503c\u65b9\u6cd5\u5728\u73b0\u4ee3\u5f02\u6784\u9ad8\u6027\u80fd\u8ba1\u7b97\u5e73\u53f0\uff08CPU/GPU\uff09\u4e0a\u7684\u8ba1\u7b97\u6548\u7387\u3001\u53ef\u6269\u5c55\u6027\u4e0e\u80fd\u8017\u8868\u73b0\uff0c\u4e3a\u5927\u89c4\u6a21\u6d41\u4f53\u52a8\u529b\u5b66\u6a21\u62df\u63d0\u4f9b\u9ad8\u6548\u3001\u8282\u80fd\u7684\u5b9e\u73b0\u7b56\u7565\u3002", "method": "\u91c7\u7528\u7a7a\u95f4\u4e0a\u6700\u9ad8\u516b\u9636\u7684\u8282\u70b9\u578b\u4e0d\u8fde\u7eedGalerkin\u65b9\u6cd5\uff0c\u65f6\u95f4\u4e0a\u8026\u5408\u6700\u9ad8\u516d\u9636\u7684Runge-Kutta\u65b9\u6cd5\uff0c\u6c42\u89e3\u7ebf\u6027\u5bf9\u6d41\u65b9\u7a0b\u548c\u7b49\u6e29Euler\u65b9\u7a0b\uff081D/2D/3D\uff09\uff1b\u4f7f\u7528Kokkos\u5e93\u7ed3\u5408MPI\u5b9e\u73b0\u5355\u4e00\u6e90\u7801\u5728\u591a\u79cdGPU\u548c\u591a\u6838CPU\u7cfb\u7edf\u4e0a\u7684\u53ef\u79fb\u690d\u5e76\u884c\u8ba1\u7b97\u3002", "result": "\u9ad8\u9636\u7a7a\u95f4\u79bb\u6563\uff08\u5982\u516b\u9636\uff09\u663e\u8457\u51cf\u5c11\u8fbe\u5230\u6307\u5b9a\u8bef\u5dee\u6240\u9700\u7684\u8ba1\u7b97\u65f6\u95f4\uff1b\u56db\u9636Runge-Kutta\u901a\u5e38\u6027\u80fd\u826f\u597d\uff1bGPU\u5728\u81ea\u7531\u5ea6\u8d85\u8fc710\u2077\u65f6\u663e\u8457\u4f18\u4e8eCPU\uff0c\u800c\u5c0f\u89c4\u6a21\u95ee\u9898CPU\u66f4\u5feb\uff1bGPU\u5728\u5927\u89c4\u6a21\u6a21\u62df\u4e2d\u80fd\u8017\u66f4\u4f4e\uff0c\u4f46\u65b0\u4e00\u4ee3GPU\u9700\u66f4\u5927\u95ee\u9898\u89c4\u6a21\u624d\u80fd\u9ad8\u6548\u5229\u7528\uff0c\u53ef\u80fd\u56e0\u6a21\u62df\u89c4\u6a21\u6269\u5927\u5bfc\u81f4\u603b\u80fd\u8017\u589e\u52a0\uff08\u53cd\u5f39\u6548\u5e94\uff09\uff1bMPI\u5b9e\u73b0\u5c55\u73b0\u51fa\u5b8c\u7f8e\u7684\u5f31\u53ef\u6269\u5c55\u6027\u3002", "conclusion": "\u9ad8\u9636\u4e0d\u8fde\u7eedGalerkin\u65b9\u6cd5\u7ed3\u5408\u73b0\u4ee3\u5f02\u6784\u67b6\u6784\u53ef\u663e\u8457\u63d0\u5347\u6d41\u4f53\u6a21\u62df\u6548\u7387\uff0c\u4f46\u9700\u6839\u636e\u95ee\u9898\u89c4\u6a21\u5408\u7406\u9009\u62e9\u786c\u4ef6\uff1b\u5c3d\u7ba1GPU\u5728\u5927\u89c4\u6a21\u95ee\u9898\u4e2d\u66f4\u9ad8\u6548\u8282\u80fd\uff0c\u4f46\u786c\u4ef6\u66f4\u65b0\u5e26\u6765\u7684\u6548\u7387\u589e\u76ca\u53ef\u80fd\u88ab\u66f4\u5927\u89c4\u6a21\u6a21\u62df\u7684\u80fd\u8017\u53cd\u5f39\u6240\u62b5\u6d88\u3002"}}
{"id": "2510.05768", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.05768", "abs": "https://arxiv.org/abs/2510.05768", "authors": ["Robin Kimmel", "Judith Michael", "Andreas Wortmann", "Jingxi Zhang"], "title": "Digital Twins for Software Engineering Processes", "comment": null, "summary": "Digital twins promise a better understanding and use of complex systems. To\nthis end, they represent these systems at their runtime and may interact with\nthem to control their processes. Software engineering is a wicked challenge in\nwhich stakeholders from many domains collaborate to produce software artifacts\ntogether. In the presence of skilled software engineer shortage, our vision is\nto leverage DTs as means for better rep- resenting, understanding, and\noptimizing software engineering processes to (i) enable software experts making\nthe best use of their time and (ii) support domain experts in producing\nhigh-quality software. This paper outlines why this would be beneficial, what\nsuch a digital twin could look like, and what is missing for realizing and\ndeploying software engineering digital twins.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u5c06\u6570\u5b57\u5b6a\u751f\uff08DT\uff09\u5e94\u7528\u4e8e\u8f6f\u4ef6\u5de5\u7a0b\uff0c\u4ee5\u5e94\u5bf9\u8f6f\u4ef6\u5de5\u7a0b\u5e08\u77ed\u7f3a\u95ee\u9898\uff0c\u63d0\u5347\u4e13\u5bb6\u6548\u7387\u5e76\u652f\u6301\u9886\u57df\u4e13\u5bb6\u5f00\u53d1\u9ad8\u8d28\u91cf\u8f6f\u4ef6\u3002", "motivation": "\u9762\u5bf9\u719f\u7ec3\u8f6f\u4ef6\u5de5\u7a0b\u5e08\u77ed\u7f3a\u7684\u6311\u6218\uff0c\u4f5c\u8005\u5e0c\u671b\u901a\u8fc7\u6570\u5b57\u5b6a\u751f\u66f4\u597d\u5730\u8868\u793a\u3001\u7406\u89e3\u548c\u4f18\u5316\u8f6f\u4ef6\u5de5\u7a0b\u8fc7\u7a0b\uff0c\u4f7f\u8f6f\u4ef6\u4e13\u5bb6\u9ad8\u6548\u5229\u7528\u65f6\u95f4\uff0c\u5e76\u5e2e\u52a9\u9886\u57df\u4e13\u5bb6\u4ea7\u51fa\u9ad8\u8d28\u91cf\u8f6f\u4ef6\u3002", "method": "\u8bba\u6587\u5e76\u672a\u63d0\u51fa\u5177\u4f53\u6280\u672f\u65b9\u6cd5\uff0c\u800c\u662f\u6784\u60f3\u6570\u5b57\u5b6a\u751f\u5728\u8f6f\u4ef6\u5de5\u7a0b\u4e2d\u7684\u5e94\u7528\u5f62\u5f0f\uff0c\u63a2\u8ba8\u5176\u5b9e\u73b0\u4e0e\u90e8\u7f72\u6240\u9700\u7684\u5173\u952e\u8981\u7d20\u3002", "result": "\u8bba\u6587\u52fe\u52d2\u4e86\u8f6f\u4ef6\u5de5\u7a0b\u6570\u5b57\u5b6a\u751f\u7684\u6f5c\u5728\u4f18\u52bf\u3001\u53ef\u80fd\u5f62\u6001\u4ee5\u53ca\u5f53\u524d\u7f3a\u5931\u7684\u5173\u952e\u7ec4\u6210\u90e8\u5206\u3002", "conclusion": "\u8f6f\u4ef6\u5de5\u7a0b\u6570\u5b57\u5b6a\u751f\u5177\u6709\u5de8\u5927\u6f5c\u529b\uff0c\u4f46\u5176\u5b9e\u73b0\u4ecd\u9700\u586b\u8865\u82e5\u5e72\u6280\u672f\u548c\u534f\u4f5c\u65b9\u9762\u7684\u7a7a\u767d\u3002"}}
{"id": "2510.05788", "categories": ["cs.SE", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.05788", "abs": "https://arxiv.org/abs/2510.05788", "authors": ["Nikita Pavlichenko", "Iurii Nazarov", "Ivan Dolgov", "Ekaterina Garanina", "Dmitry Ustalov", "Ivan Bondyrev", "Kseniia Lysaniuk", "Evgeniia Vu", "Kirill Chekmenev", "Joseph Shtok", "Yaroslav Golubev", "Anton Semenkin", "Uladzislau Sazanovich"], "title": "Mellum: Production-Grade in-IDE Contextual Code Completion with Multi-File Project Understanding", "comment": "11 pages, 4 figures, 3 tables", "summary": "We present the Mellum models family, open-weight code completion models\ndesigned for interactive use in JetBrains IDEs. Mellums have 4B parameters,\nadopt a Llama-style architecture, and are pre-trained on ~4T tokens of\npermissively licensed, multi-language code. Our studies show that (i) careful\ndata curation and staged training significantly improve the model's quality,\n(ii) editor-critical capabilities such as context packing are necessary for\nhigh-quality suggestions, and (iii) a compact, task-focused model can meet the\ncost and latency constraints of interactive completion.\n  In the paper, we describe an end-to-end industrial pipeline for producing\ncontextualized in-editor completion: disciplined data governance, multi-stage\ntraining that includes fill-in-the-middle and project context via supervised\nfine-tuning, and alignment via direct preference optimization using feedback\nfrom real-world scenarios. Our quality evaluations include both large-scale\noffline benchmarks and online telemetry from production deployments in\nJetBrains IDEs. Mellums are released under the Apache-2.0 license on\nHuggingFace, with a public model card providing a reproducible reference for\npractitioners. Our experience offers a pragmatic blueprint for taking a\nfocused, open model from a research prototype to at scale production for\nhundreds of thousands of users.", "AI": {"tldr": "Mellum \u662f\u4e00\u4e2a\u5f00\u6e90\u7684 4B \u53c2\u6570\u4ee3\u7801\u8865\u5168\u6a21\u578b\u5bb6\u65cf\uff0c\u4e13\u4e3a JetBrains IDE \u4e2d\u7684\u4ea4\u4e92\u5f0f\u4f7f\u7528\u800c\u8bbe\u8ba1\uff0c\u91c7\u7528 Llama \u67b6\u6784\u5e76\u5728\u7ea6 4T \u4e2a\u5bbd\u677e\u8bb8\u53ef\u7684\u591a\u8bed\u8a00\u4ee3\u7801 token \u4e0a\u9884\u8bad\u7ec3\u3002\u901a\u8fc7\u7cbe\u5fc3\u7684\u6570\u636e\u6cbb\u7406\u3001\u591a\u9636\u6bb5\u8bad\u7ec3\uff08\u5305\u62ec\u4e2d\u95f4\u586b\u5145\u548c\u9879\u76ee\u4e0a\u4e0b\u6587\uff09\u4ee5\u53ca\u57fa\u4e8e\u771f\u5b9e\u7528\u6237\u53cd\u9988\u7684\u76f4\u63a5\u504f\u597d\u4f18\u5316\uff0cMellum \u5728\u79bb\u7ebf\u548c\u5728\u7ebf\u8bc4\u4f30\u4e2d\u5747\u8868\u73b0\u51fa\u9ad8\u8d28\u91cf\uff0c\u5e76\u5728 Apache-2.0 \u8bb8\u53ef\u4e0b\u516c\u5f00\u53d1\u5e03\u3002", "motivation": "\u4e3a\u6ee1\u8db3 JetBrains IDE \u4e2d\u4ea4\u4e92\u5f0f\u4ee3\u7801\u8865\u5168\u5bf9\u4f4e\u5ef6\u8fdf\u3001\u4f4e\u6210\u672c\u548c\u9ad8\u8d28\u91cf\u7684\u9700\u6c42\uff0c\u5f00\u53d1\u4e00\u4e2a\u4e13\u6ce8\u4e8e\u4efb\u52a1\u3001\u53ef\u6269\u5c55\u4e14\u5f00\u6e90\u7684\u4ee3\u7801\u8865\u5168\u6a21\u578b\u3002", "method": "\u6784\u5efa\u7aef\u5230\u7aef\u5de5\u4e1a\u7ea7\u6d41\u7a0b\uff0c\u5305\u62ec\uff1a\u4e25\u683c\u7684\u6570\u636e\u6cbb\u7406\u3001\u591a\u9636\u6bb5\u8bad\u7ec3\uff08\u542b fill-in-the-middle \u548c\u9879\u76ee\u4e0a\u4e0b\u6587\u7684\u76d1\u7763\u5fae\u8c03\uff09\u3001\u4ee5\u53ca\u5229\u7528\u771f\u5b9e\u573a\u666f\u53cd\u9988\u8fdb\u884c\u76f4\u63a5\u504f\u597d\u4f18\u5316\uff08DPO\uff09\u5bf9\u9f50\u3002", "result": "Mellum \u6a21\u578b\u5728\u5927\u89c4\u6a21\u79bb\u7ebf\u57fa\u51c6\u548c JetBrains IDE \u4e2d\u7684\u5728\u7ebf\u9065\u6d4b\u6570\u636e\u4e0a\u5747\u8868\u73b0\u4f18\u5f02\uff0c\u9a8c\u8bc1\u4e86\u6570\u636e\u7b5b\u9009\u3001\u5206\u9636\u6bb5\u8bad\u7ec3\u548c\u4e0a\u4e0b\u6587\u6253\u5305\u5bf9\u6a21\u578b\u8d28\u91cf\u7684\u5173\u952e\u4f5c\u7528\uff0c\u5e76\u6210\u529f\u90e8\u7f72\u670d\u52a1\u6570\u5341\u4e07\u7528\u6237\u3002", "conclusion": "\u4e00\u4e2a\u7d27\u51d1\u3001\u4efb\u52a1\u805a\u7126\u4e14\u7ecf\u8fc7\u826f\u597d\u5de5\u7a0b\u5316\u7684\u5f00\u6e90\u6a21\u578b\uff0c\u7ed3\u5408\u4e25\u8c28\u7684\u6570\u636e\u4e0e\u8bad\u7ec3\u6d41\u7a0b\uff0c\u80fd\u591f\u6709\u6548\u4ece\u7814\u7a76\u539f\u578b\u8f6c\u5316\u4e3a\u5927\u89c4\u6a21\u751f\u4ea7\u5e94\u7528\uff0c\u4e3a\u5de5\u4e1a\u754c\u63d0\u4f9b\u53ef\u590d\u73b0\u7684\u5b9e\u8df5\u84dd\u56fe\u3002"}}
{"id": "2510.05878", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.05878", "abs": "https://arxiv.org/abs/2510.05878", "authors": ["Darja Smite", "Franz Zieris", "Lars-Ola Damm"], "title": "A Wave of Resignations in the Aftermath of Remote Onboarding", "comment": "9 pages, submitted to the Journal of Systems and Software, In\n  Practice track", "summary": "The COVID-19 pandemic has permanently altered workplace structures,\nnormalizing remote work. However, critical evidence highlights challenges with\nfully remote arrangements, particularly for software teams. This study\ninvestigates employee resignation patterns at Ericsson, a global developer of\nsoftware-intensive systems, before, during, and after the pandemic. Using HR\ndata from 2016-2025 in Ericsson Sweden, we analyze how different work\nmodalities (onsite, remote, and hybrid) influence employee retention. Our\nfindings show a marked increase in resignations from summer 2021 to summer\n2023, especially among employees with less than five years of tenure. Employees\nonboarded remotely during the pandemic were significantly more likely to resign\nwithin their first three years, even after returning to the office. Exit\nsurveys suggest that remote onboarding may fail to establish the necessary\norganizational attachment, the feeling of belonging and long-term retention. By\ncontrast, the company's eventual successful return to pre-pandemic retention\nrates illustrates the value of differentiated work policies and supports\nreconsidering selective return-to-office (RTO) mandates. Our study demonstrates\nthe importance of employee integration practices in hybrid environments where\nthe requirement for in-office presence for recent hires shall be accompanied by\nin-office presence from their team members and more senior staff whose\nmentoring and social interactions contribute to integration into the corporate\nwork environment. We hope these actionable insights will inform HR leaders and\npolicymakers in shaping post-pandemic work practices, demonstrating that\ncarefully crafted hybrid models anchored in organizational attachment and\nmentorship can sustain retention in knowledge-intensive companies.", "AI": {"tldr": "\u8be5\u7814\u7a76\u5206\u6790\u4e86\u7231\u7acb\u4fe1\u745e\u5178\u516c\u53f82016-2025\u5e74\u5458\u5de5\u79bb\u804c\u6570\u636e\uff0c\u53d1\u73b0\u75ab\u60c5\u671f\u95f4\u8fdc\u7a0b\u5165\u804c\u7684\u5458\u5de5\u5728\u4e09\u5e74\u5185\u79bb\u804c\u7387\u663e\u8457\u66f4\u9ad8\uff0c\u4e3b\u8981\u56e0\u7f3a\u4e4f\u7ec4\u7ec7\u5f52\u5c5e\u611f\uff1b\u7814\u7a76\u5f3a\u8c03\u5728\u6df7\u5408\u529e\u516c\u6a21\u5f0f\u4e0b\uff0c\u65b0\u5458\u5de5\u4e0e\u56e2\u961f\u53ca\u8d44\u6df1\u5458\u5de5\u7684\u7ebf\u4e0b\u4e92\u52a8\u5bf9\u63d0\u5347\u7559\u4efb\u7387\u81f3\u5173\u91cd\u8981\u3002", "motivation": "\u65b0\u51a0\u75ab\u60c5\u4f7f\u8fdc\u7a0b\u529e\u516c\u5e38\u6001\u5316\uff0c\u4f46\u8f6f\u4ef6\u56e2\u961f\u5728\u5b8c\u5168\u8fdc\u7a0b\u6a21\u5f0f\u4e0b\u9762\u4e34\u6311\u6218\uff0c\u5c24\u5176\u5728\u5458\u5de5\u7559\u4efb\u65b9\u9762\u3002\u4f5c\u8005\u65e8\u5728\u63a2\u7a76\u4e0d\u540c\u529e\u516c\u6a21\u5f0f\uff08\u73b0\u573a\u3001\u8fdc\u7a0b\u3001\u6df7\u5408\uff09\u5bf9\u5458\u5de5\u79bb\u804c\u884c\u4e3a\u7684\u5f71\u54cd\uff0c\u4ee5\u6307\u5bfc\u540e\u75ab\u60c5\u65f6\u4ee3\u7684\u4eba\u529b\u8d44\u6e90\u653f\u7b56\u3002", "method": "\u5229\u7528\u7231\u7acb\u4fe1\u745e\u5178\u516c\u53f82016-2025\u5e74\u7684\u4eba\u529b\u8d44\u6e90\u6570\u636e\uff0c\u7ed3\u5408\u79bb\u804c\u8c03\u67e5\uff0c\u5206\u6790\u75ab\u60c5\u524d\u540e\u4e0d\u540c\u529e\u516c\u6a21\u5f0f\u4e0b\u5458\u5de5\uff08\u5c24\u5176\u662f\u65b0\u5165\u804c\u5458\u5de5\uff09\u7684\u79bb\u804c\u8d8b\u52bf\u4e0e\u539f\u56e0\u3002", "result": "2021\u5e74\u590f\u81f32023\u5e74\u590f\u79bb\u804c\u7387\u663e\u8457\u4e0a\u5347\uff0c\u7279\u522b\u662f\u5165\u804c\u4e0d\u8db3\u4e94\u5e74\u7684\u5458\u5de5\uff1b\u75ab\u60c5\u671f\u95f4\u8fdc\u7a0b\u5165\u804c\u8005\u5373\u4f7f\u8fd4\u5c97\uff0c\u4e09\u5e74\u5185\u79bb\u804c\u6982\u7387\u4ecd\u66f4\u9ad8\uff1b\u79bb\u804c\u8c03\u67e5\u6307\u51fa\u8fdc\u7a0b\u5165\u804c\u96be\u4ee5\u5efa\u7acb\u7ec4\u7ec7\u5f52\u5c5e\u611f\uff1b\u516c\u53f8\u901a\u8fc7\u5dee\u5f02\u5316\u529e\u516c\u653f\u7b56\u6210\u529f\u6062\u590d\u81f3\u75ab\u60c5\u524d\u7559\u4efb\u6c34\u5e73\u3002", "conclusion": "\u5728\u77e5\u8bc6\u5bc6\u96c6\u578b\u4f01\u4e1a\u4e2d\uff0c\u6df7\u5408\u529e\u516c\u6a21\u5f0f\u9700\u4ee5\u7ec4\u7ec7\u5f52\u5c5e\u611f\u548c\u5bfc\u5e08\u5236\u4e3a\u6838\u5fc3\uff0c\u65b0\u5458\u5de5\u8fd4\u5c97\u65f6\u5e94\u786e\u4fdd\u56e2\u961f\u6210\u5458\u548c\u8d44\u6df1\u5458\u5de5\u540c\u6b65\u5728\u5c97\uff0c\u4ee5\u4fc3\u8fdb\u6709\u6548\u878d\u5165\u5e76\u63d0\u5347\u7559\u4efb\u7387\u3002"}}
{"id": "2510.05968", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.05968", "abs": "https://arxiv.org/abs/2510.05968", "authors": ["Scott Frees"], "title": "Extending ResourceLink: Patterns for Large Dataset Processing in MCP Applications", "comment": null, "summary": "Large language models translate natural language into database queries, yet\ncontext window limitations prevent direct deployment in reporting systems where\ncomplete datasets exhaust available tokens. The Model Context Protocol\nspecification defines ResourceLink for referencing external resources, but\npractical patterns for implementing scalable reporting architectures remain\nundocumented. This paper presents patterns for building LLM-powered reporting\nsystems that decouple query generation from data retrieval. We introduce a\ndual-response pattern extending ResourceLink to support both iterative query\nrefinement and out-of-band data access, accompanied by patterns for\nmulti-tenant security and resource lifecycle management. These patterns address\nfundamental challenges in LLM-driven reporting applications and provide\npractical guidance for developers building them.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9002\u7528\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u9a71\u52a8\u7684\u62a5\u8868\u7cfb\u7edf\u7684\u67b6\u6784\u6a21\u5f0f\uff0c\u901a\u8fc7\u5c06\u67e5\u8be2\u751f\u6210\u4e0e\u6570\u636e\u68c0\u7d22\u89e3\u8026\uff0c\u89e3\u51b3\u4e0a\u4e0b\u6587\u7a97\u53e3\u9650\u5236\u95ee\u9898\uff0c\u5e76\u5f15\u5165\u652f\u6301\u8fed\u4ee3\u67e5\u8be2\u4f18\u5316\u548c\u5e26\u5916\u6570\u636e\u8bbf\u95ee\u7684\u53cc\u54cd\u5e94\u6a21\u5f0f\uff0c\u540c\u65f6\u6db5\u76d6\u591a\u79df\u6237\u5b89\u5168\u4e0e\u8d44\u6e90\u751f\u547d\u5468\u671f\u7ba1\u7406\u3002", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b\u5728\u5c06\u81ea\u7136\u8bed\u8a00\u8f6c\u6362\u4e3a\u6570\u636e\u5e93\u67e5\u8be2\u65f6\u53d7\u9650\u4e8e\u4e0a\u4e0b\u6587\u7a97\u53e3\uff0c\u65e0\u6cd5\u76f4\u63a5\u7528\u4e8e\u5b8c\u6574\u6570\u636e\u96c6\u7684\u62a5\u8868\u7cfb\u7edf\uff1b\u540c\u65f6\uff0c\u5c3d\u7ba1Model Context Protocol\u5b9a\u4e49\u4e86ResourceLink\u673a\u5236\uff0c\u4f46\u7f3a\u4e4f\u53ef\u6269\u5c55\u62a5\u8868\u67b6\u6784\u7684\u5b9e\u7528\u5b9e\u73b0\u6a21\u5f0f\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u53cc\u54cd\u5e94\u6a21\u5f0f\uff0c\u6269\u5c55ResourceLink\u4ee5\u652f\u6301\u8fed\u4ee3\u67e5\u8be2\u4f18\u5316\u548c\u5e26\u5916\u6570\u636e\u8bbf\u95ee\uff0c\u5e76\u8bbe\u8ba1\u591a\u79df\u6237\u5b89\u5168\u4e0e\u8d44\u6e90\u751f\u547d\u5468\u671f\u7ba1\u7406\u7684\u914d\u5957\u6a21\u5f0f\u3002", "result": "\u6240\u63d0\u51fa\u7684\u6a21\u5f0f\u6709\u6548\u89e3\u51b3\u4e86LLM\u9a71\u52a8\u62a5\u8868\u7cfb\u7edf\u4e2d\u7684\u6838\u5fc3\u6311\u6218\uff0c\u4e3a\u5f00\u53d1\u8005\u63d0\u4f9b\u4e86\u53ef\u843d\u5730\u7684\u67b6\u6784\u6307\u5bfc\u3002", "conclusion": "\u901a\u8fc7\u89e3\u8026\u67e5\u8be2\u751f\u6210\u4e0e\u6570\u636e\u68c0\u7d22\uff0c\u5e76\u7ed3\u5408\u53cc\u54cd\u5e94\u3001\u5b89\u5168\u4e0e\u8d44\u6e90\u7ba1\u7406\u6a21\u5f0f\uff0c\u672c\u6587\u4e3a\u6784\u5efa\u53ef\u6269\u5c55\u3001\u5b89\u5168\u7684LLM\u9a71\u52a8\u62a5\u8868\u7cfb\u7edf\u63d0\u4f9b\u4e86\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.06000", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.06000", "abs": "https://arxiv.org/abs/2510.06000", "authors": ["Daniel Otten", "Trevor Stalnaker", "Nathan Wintersgill", "Oscar Chaparro", "Denys Poshyvanyk"], "title": "Prompting in Practice: Investigating Software Developers' Use of Generative AI Tools", "comment": null, "summary": "The integration of generative artificial intelligence (GenAI) tools has\nfundamentally transformed software development. Although prompt engineering has\nemerged as a critical skill, existing research focuses primarily on individual\ntechniques rather than software developers' broader workflows. This study\npresents a systematic investigation of how software engineers integrate GenAI\ntools into their professional practice through a large-scale survey examining\nprompting strategies, conversation patterns, and reliability assessments across\nvarious software engineering tasks.\n  We surveyed 91 software engineers, including 72 active GenAI users, to\nunderstand AI usage patterns throughout the development process. Our 14 key\nfindings show that while code generation is nearly universal, proficiency\nstrongly correlates with using AI for more nuanced tasks such as debugging and\ncode review, and that developers prefer iterative multi-turn conversations to\nsingle-shot prompting. Documentation tasks are perceived as most reliable,\nwhile complex code generation and debugging present sizable challenges. Our\ninsights provide an empirical baseline of current developer practices, from\nsimple code generation to deeper workflow integration, with actionable insights\nfor future improvements.", "AI": {"tldr": "\u672c\u7814\u7a76\u901a\u8fc7\u4e00\u9879\u9488\u5bf991\u540d\u8f6f\u4ef6\u5de5\u7a0b\u5e08\uff08\u5176\u4e2d72\u540d\u4e3aGenAI\u6d3b\u8dc3\u7528\u6237\uff09\u7684\u5927\u89c4\u6a21\u8c03\u67e5\uff0c\u7cfb\u7edf\u5206\u6790\u4e86\u751f\u6210\u5f0f\u4eba\u5de5\u667a\u80fd\uff08GenAI\uff09\u5728\u8f6f\u4ef6\u5f00\u53d1\u4e2d\u7684\u5b9e\u9645\u5e94\u7528\u65b9\u5f0f\uff0c\u63ed\u793a\u4e86\u5f00\u53d1\u8005\u5728\u63d0\u793a\u7b56\u7565\u3001\u5bf9\u8bdd\u6a21\u5f0f\u548c\u4efb\u52a1\u53ef\u9760\u6027\u8bc4\u4f30\u7b49\u65b9\u9762\u7684\u5b9e\u8df5\u6a21\u5f0f\u3002", "motivation": "\u5c3d\u7ba1\u63d0\u793a\u5de5\u7a0b\u5df2\u6210\u4e3a\u5173\u952e\u6280\u80fd\uff0c\u4f46\u73b0\u6709\u7814\u7a76\u591a\u805a\u7126\u4e8e\u4e2a\u522b\u6280\u672f\uff0c\u7f3a\u4e4f\u5bf9\u8f6f\u4ef6\u5f00\u53d1\u8005\u5982\u4f55\u5728\u6574\u4f53\u5de5\u4f5c\u6d41\u4e2d\u6574\u5408GenAI\u5de5\u5177\u7684\u7cfb\u7edf\u7406\u89e3\u3002", "method": "\u901a\u8fc7\u5927\u89c4\u6a21\u95ee\u5377\u8c03\u67e5\uff0c\u6536\u96c691\u540d\u8f6f\u4ef6\u5de5\u7a0b\u5e08\uff08\u542b72\u540dGenAI\u6d3b\u8dc3\u7528\u6237\uff09\u5728\u5404\u7c7b\u8f6f\u4ef6\u5de5\u7a0b\u4efb\u52a1\u4e2d\u4f7f\u7528GenAI\u7684\u63d0\u793a\u7b56\u7565\u3001\u5bf9\u8bdd\u6a21\u5f0f\u548c\u53ef\u9760\u6027\u8bc4\u4f30\u6570\u636e\uff0c\u5e76\u8fdb\u884c\u7cfb\u7edf\u5206\u6790\u3002", "result": "\u7814\u7a76\u5f97\u51fa14\u9879\u5173\u952e\u53d1\u73b0\uff1a\u4ee3\u7801\u751f\u6210\u51e0\u4e4e\u666e\u904d\u5b58\u5728\uff1b\u719f\u7ec3\u5f00\u53d1\u8005\u66f4\u503e\u5411\u4e8e\u5c06AI\u7528\u4e8e\u8c03\u8bd5\u548c\u4ee3\u7801\u5ba1\u67e5\u7b49\u590d\u6742\u4efb\u52a1\uff1b\u5f00\u53d1\u8005\u504f\u597d\u591a\u8f6e\u8fed\u4ee3\u5bf9\u8bdd\u800c\u975e\u5355\u6b21\u63d0\u793a\uff1b\u6587\u6863\u4efb\u52a1\u88ab\u8ba4\u4e3a\u6700\u53ef\u9760\uff0c\u800c\u590d\u6742\u4ee3\u7801\u751f\u6210\u4e0e\u8c03\u8bd5\u4ecd\u5177\u6311\u6218\u6027\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u5f53\u524d\u5f00\u53d1\u8005\u4f7f\u7528GenAI\u7684\u5b9e\u8df5\u63d0\u4f9b\u4e86\u5b9e\u8bc1\u57fa\u7ebf\uff0c\u6db5\u76d6\u4ece\u7b80\u5355\u4ee3\u7801\u751f\u6210\u5230\u6df1\u5ea6\u5de5\u4f5c\u6d41\u6574\u5408\u7684\u591a\u4e2a\u5c42\u9762\uff0c\u5e76\u4e3a\u672a\u6765\u5de5\u5177\u6539\u8fdb\u63d0\u4f9b\u4e86\u53ef\u64cd\u4f5c\u7684\u89c1\u89e3\u3002"}}
{"id": "2510.05711", "categories": ["cs.DC", "cs.CE"], "pdf": "https://arxiv.org/pdf/2510.05711", "abs": "https://arxiv.org/abs/2510.05711", "authors": ["Ailiya Borjigin", "Cong He"], "title": "Intertemporal Pricing of Time-Bound Stablecoins: Measuring and Controlling the Liquidity-of-Time Premium", "comment": "23 pages, 5 figures", "summary": "Time-bound stablecoins are DeFi assets that temporarily tokenize traditional\nsecurities during market off-hours, enabling continuous cross-market liquidity.\nWe introduce the Liquidity-of-Time Premium (TLP): the extra return or cost of\nproviding liquidity when the primary market is closed. We build a no-arbitrage\npricing model that yields a band for fair values over different expiries, and a\ndynamic risk-control mechanism that adjusts loan-to-value (LTV) ratios in real\ntime to keep TLP within a target range. Our analysis blends financial\nengineering (no-arbitrage conditions, option-style pricing) with empirical\nfinance (event studies on cross-listed stocks and futures) to measure TLP under\ntime-zone frictions. We define TLP formally, derive closed-form expressions for\nits term structure under idealized assumptions, and simulate scenarios that\nvary volatility and collateralization. We then propose an LTV policy that\nraises or lowers collateral to expand or curtail time-bound stablecoin supply,\nanalogous to a central bank adjusting rates to defend a peg. We outline\nempirical proxies for TLP, including ADR premiums, overseas index futures\nversus cash index divergence, and pre-market versus official close gaps.\nResults show that TLP grows with closure length and volatility, yet can be\ncontained by adaptive LTV. We provide backtests and figures (term-structure\ncurves, capital-efficiency versus tail-risk trade-offs, time-liquidity\nheatmaps) and discuss protocol design (vault structure, closing-price oracles,\non-chain auction liquidations). The findings position time-bound stablecoins as\na tool to reduce temporal market inefficiencies and inform future research and\ndeployment.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u201c\u65f6\u95f4\u6d41\u52a8\u6027\u6ea2\u4ef7\u201d\uff08TLP\uff09\u6982\u5ff5\uff0c\u7528\u4e8e\u91cf\u5316\u5728\u4e3b\u5e02\u573a\u4f11\u5e02\u671f\u95f4\u63d0\u4f9b\u6d41\u52a8\u6027\u7684\u989d\u5916\u6210\u672c\u6216\u6536\u76ca\uff0c\u5e76\u6784\u5efa\u65e0\u5957\u5229\u5b9a\u4ef7\u6a21\u578b\u4e0e\u52a8\u6001LTV\u8c03\u63a7\u673a\u5236\uff0c\u4ee5\u7ba1\u7406\u65f6\u95f4\u7ed1\u5b9a\u7a33\u5b9a\u5e01\u7684\u4f9b\u5e94\u548c\u98ce\u9669\u3002", "motivation": "\u4f20\u7edf\u91d1\u878d\u5e02\u573a\u5b58\u5728\u65f6\u533a\u5272\u88c2\u548c\u4f11\u5e02\u671f\u95f4\u6d41\u52a8\u6027\u7f3a\u5931\u7684\u95ee\u9898\uff0c\u65f6\u95f4\u7ed1\u5b9a\u7a33\u5b9a\u5e01\u53ef\u5728\u4e3b\u5e02\u573a\u5173\u95ed\u65f6\u4e34\u65f6\u4ee3\u5e01\u5316\u4f20\u7edf\u8bc1\u5238\uff0c\u63d0\u5347\u8de8\u5e02\u573a\u8fde\u7eed\u6d41\u52a8\u6027\uff0c\u4f46\u9700\u89e3\u51b3\u5176\u5b9a\u4ef7\u4e0e\u98ce\u9669\u7ba1\u7406\u95ee\u9898\u3002", "method": "\u7ed3\u5408\u91d1\u878d\u5de5\u7a0b\uff08\u65e0\u5957\u5229\u6761\u4ef6\u3001\u7c7b\u671f\u6743\u5b9a\u4ef7\uff09\u4e0e\u5b9e\u8bc1\u91d1\u878d\uff08\u4ea4\u53c9\u4e0a\u5e02\u80a1\u7968\u4e0e\u671f\u8d27\u7684\u4e8b\u4ef6\u7814\u7a76\uff09\uff0c\u5efa\u7acbTLP\u7684\u7406\u8bba\u6a21\u578b\u4e0e\u5c01\u95ed\u89e3\uff0c\u6a21\u62df\u4e0d\u540c\u6ce2\u52a8\u7387\u548c\u62b5\u62bc\u7387\u4e0b\u7684\u60c5\u666f\uff0c\u5e76\u8bbe\u8ba1\u52a8\u6001\u8c03\u6574LTV\u7684\u653f\u7b56\u673a\u5236\u3002", "result": "TLP\u968f\u5e02\u573a\u5173\u95ed\u65f6\u957f\u548c\u6ce2\u52a8\u7387\u4e0a\u5347\u800c\u589e\u5927\uff0c\u4f46\u53ef\u901a\u8fc7\u81ea\u9002\u5e94LTV\u673a\u5236\u6709\u6548\u63a7\u5236\uff1b\u7814\u7a76\u8fd8\u63d0\u4f9b\u4e86TLP\u7684\u5b9e\u8bc1\u4ee3\u7406\u6307\u6807\u3001\u56de\u6d4b\u7ed3\u679c\u53ca\u534f\u8bae\u8bbe\u8ba1\u5efa\u8bae\u3002", "conclusion": "\u65f6\u95f4\u7ed1\u5b9a\u7a33\u5b9a\u5e01\u53ef\u4f5c\u4e3a\u7f13\u89e3\u65f6\u95f4\u7ef4\u5ea6\u5e02\u573a\u4f4e\u6548\u7684\u5de5\u5177\uff0c\u5176\u5b9a\u4ef7\u4e0e\u98ce\u63a7\u6846\u67b6\u4e3a\u672a\u6765DeFi\u534f\u8bae\u8bbe\u8ba1\u548c\u5b66\u672f\u7814\u7a76\u63d0\u4f9b\u4e86\u57fa\u7840\u3002"}}
{"id": "2510.06104", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.06104", "abs": "https://arxiv.org/abs/2510.06104", "authors": ["Elijah Kayode Adejumo", "Brittany Johnson"], "title": "Explaining Code Risk in OSS: Towards LLM-Generated Fault Prediction Interpretations", "comment": null, "summary": "Open Source Software (OSS) has become a very important and crucial\ninfrastructure worldwide because of the value it provides. OSS typically\ndepends on contributions from developers across diverse backgrounds and levels\nof experience. Making safe changes, such as fixing a bug or implementing a new\nfeature, can be challenging, especially in object-oriented systems where\ncomponents are interdependent. Static analysis and defect-prediction tools\nproduce metrics (e.g., complexity,coupling) that flag potentially fault-prone\ncomponents, but these signals are often hard for contributors new or unfamiliar\nwith the codebase to interpret. Large Language Models (LLMs) have shown strong\nperformance on software engineering tasks such as code summarization and\ndocumentation generation. Building on this progress, we investigate whether\nLLMs can translate fault-prediction metrics into clear, human-readable risk\nexplanations and actionable guidance to help OSS contributors plan and review\ncode modifications. We outline explanation types that an LLM-generated\nassistant could provide (descriptive, contextual, and actionable explanations).\nWe also outline our next steps to assess usefulness through a task-based study\nwith OSS contributors, comparing metric-only baselines to LLM-generated\nexplanations on decision quality, time-to-completion, and error rates", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5c06\u9759\u6001\u5206\u6790\u548c\u7f3a\u9677\u9884\u6d4b\u5de5\u5177\u751f\u6210\u7684\u590d\u6742\u6307\u6807\u8f6c\u5316\u4e3a\u6e05\u6670\u3001\u53ef\u64cd\u4f5c\u7684\u98ce\u9669\u89e3\u91ca\uff0c\u4ee5\u5e2e\u52a9\u5f00\u6e90\u8f6f\u4ef6\uff08OSS\uff09\u8d21\u732e\u8005\u66f4\u5b89\u5168\u5730\u8fdb\u884c\u4ee3\u7801\u4fee\u6539\u3002", "motivation": "\u5f00\u6e90\u8f6f\u4ef6\u4f9d\u8d56\u591a\u6837\u80cc\u666f\u7684\u5f00\u53d1\u8005\uff0c\u4f46\u5b89\u5168\u5730\u4fee\u6539\u4ee3\u7801\uff08\u5982\u4fee\u590dbug\u6216\u6dfb\u52a0\u529f\u80fd\uff09\u5728\u9ad8\u5ea6\u8026\u5408\u7684\u9762\u5411\u5bf9\u8c61\u7cfb\u7edf\u4e2d\u5177\u6709\u6311\u6218\u6027\uff1b\u73b0\u6709\u7f3a\u9677\u9884\u6d4b\u5de5\u5177\u63d0\u4f9b\u7684\u6307\u6807\u96be\u4ee5\u88ab\u4e0d\u719f\u6089\u4ee3\u7801\u5e93\u7684\u8d21\u732e\u8005\u7406\u89e3\u3002", "method": "\u63d0\u51fa\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\u5c06\u6545\u969c\u9884\u6d4b\u6307\u6807\u8f6c\u5316\u4e3a\u4e09\u7c7b\u4eba\u7c7b\u53ef\u8bfb\u7684\u89e3\u91ca\uff1a\u63cf\u8ff0\u6027\u3001\u4e0a\u4e0b\u6587\u6027\u548c\u53ef\u64cd\u4f5c\u6027\u89e3\u91ca\uff0c\u5e76\u8ba1\u5212\u901a\u8fc7\u4efb\u52a1\u578b\u7528\u6237\u7814\u7a76\u8bc4\u4f30\u5176\u6709\u6548\u6027\u3002", "result": "\u5c1a\u672a\u62a5\u544a\u5177\u4f53\u5b9e\u9a8c\u7ed3\u679c\uff0c\u4f46\u63d0\u51fa\u4e86LLM\u8f85\u52a9\u89e3\u91ca\u7684\u6846\u67b6\u548c\u672a\u6765\u901a\u8fc7\u5bf9\u6bd4\u5b9e\u9a8c\uff08\u4ec5\u6307\u6807 vs. LLM\u89e3\u91ca\uff09\u8bc4\u4f30\u5176\u5728\u51b3\u7b56\u8d28\u91cf\u3001\u5b8c\u6210\u65f6\u95f4\u548c\u9519\u8bef\u7387\u65b9\u9762\u7684\u6709\u6548\u6027\u3002", "conclusion": "LLMs\u6709\u6f5c\u529b\u901a\u8fc7\u63d0\u4f9b\u6e05\u6670\u7684\u98ce\u9669\u89e3\u91ca\u548c\u53ef\u64cd\u4f5c\u5efa\u8bae\uff0c\u964d\u4f4e\u5f00\u6e90\u8d21\u732e\u8005\u7406\u89e3\u548c\u4f7f\u7528\u7f3a\u9677\u9884\u6d4b\u6307\u6807\u7684\u95e8\u69db\uff0c\u4ece\u800c\u63d0\u5347\u4ee3\u7801\u4fee\u6539\u7684\u5b89\u5168\u6027\u548c\u6548\u7387\u3002"}}
{"id": "2510.05738", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2510.05738", "abs": "https://arxiv.org/abs/2510.05738", "authors": ["Ritesh Chandra", "Sonali Agarwal", "Navjot Singh", "Sadhana Tiwari"], "title": "A Review of Ontology-Driven Big Data Analytics in Healthcare: Challenges, Tools, and Applications", "comment": null, "summary": "Exponential growth in heterogeneous healthcare data arising from electronic\nhealth records (EHRs), medical imaging, wearable sensors, and biomedical\nresearch has accelerated the adoption of data lakes and centralized\narchitectures capable of handling the Volume, Variety, and Velocity of Big Data\nfor advanced analytics. However, without effective governance, these\nrepositories risk devolving into disorganized data swamps. Ontology-driven\nsemantic data management offers a robust solution by linking metadata to\nhealthcare knowledge graphs, thereby enhancing semantic interoperability,\nimproving data discoverability, and enabling expressive, domain-aware access.\nThis review adopts a systematic research strategy, formulating key research\nquestions and conducting a structured literature search across major academic\ndatabases, with selected studies analyzed and classified into six categories of\nontology-driven healthcare analytics: (i) ontology-driven integration\nframeworks, (ii) semantic modeling for metadata enrichment, (iii)\nontology-based data access (OBDA), (iv) basic semantic data management, (v)\nontology-based reasoning for decision support, and (vi) semantic annotation for\nunstructured data. We further examine the integration of ontology technologies\nwith Big Data frameworks such as Hadoop, Spark, Kafka, and so on, highlighting\ntheir combined potential to deliver scalable and intelligent healthcare\nanalytics. For each category, recent techniques, representative case studies,\ntechnical and organizational challenges, and emerging trends such as artificial\nintelligence, machine learning, the Internet of Things (IoT), and real-time\nanalytics are reviewed to guide the development of sustainable, interoperable,\nand high-performance healthcare data ecosystems.", "AI": {"tldr": "\u672c\u6587\u7efc\u8ff0\u4e86\u672c\u4f53\u9a71\u52a8\u7684\u8bed\u4e49\u6570\u636e\u7ba1\u7406\u5728\u533b\u7597\u5927\u6570\u636e\u5206\u6790\u4e2d\u7684\u5e94\u7528\uff0c\u7cfb\u7edf\u5206\u7c7b\u4e86\u516d\u7c7b\u7814\u7a76\u65b9\u5411\uff0c\u5e76\u63a2\u8ba8\u4e86\u5176\u4e0e\u4e3b\u6d41\u5927\u6570\u636e\u6846\u67b6\u7684\u96c6\u6210\u53ca\u672a\u6765\u8d8b\u52bf\u3002", "motivation": "\u533b\u7597\u6570\u636e\u5448\u7206\u70b8\u5f0f\u589e\u957f\u4e14\u6765\u6e90\u5f02\u6784\uff0c\u4f20\u7edf\u6570\u636e\u6e56\u82e5\u7f3a\u4e4f\u6709\u6548\u6cbb\u7406\u6613\u6ca6\u4e3a\u201c\u6570\u636e\u6cbc\u6cfd\u201d\uff0c\u4e9f\u9700\u63d0\u5347\u8bed\u4e49\u4e92\u64cd\u4f5c\u6027\u3001\u6570\u636e\u53ef\u53d1\u73b0\u6027\u4e0e\u667a\u80fd\u8bbf\u95ee\u80fd\u529b\u3002", "method": "\u91c7\u7528\u7cfb\u7edf\u6027\u6587\u732e\u7efc\u8ff0\u65b9\u6cd5\uff0c\u6784\u5efa\u7814\u7a76\u95ee\u9898\uff0c\u5728\u4e3b\u8981\u5b66\u672f\u6570\u636e\u5e93\u4e2d\u7ed3\u6784\u5316\u68c0\u7d22\uff0c\u5e76\u5c06\u5165\u9009\u7814\u7a76\u5206\u4e3a\u516d\u7c7b\u672c\u4f53\u9a71\u52a8\u7684\u533b\u7597\u5206\u6790\u65b9\u6cd5\u8fdb\u884c\u5f52\u7eb3\u5206\u6790\u3002", "result": "\u8bc6\u522b\u51fa\u516d\u7c7b\u672c\u4f53\u9a71\u52a8\u7684\u533b\u7597\u5206\u6790\u8303\u5f0f\uff0c\u603b\u7ed3\u4e86\u5404\u7c7b\u7684\u6280\u672f\u65b9\u6848\u3001\u5178\u578b\u6848\u4f8b\u3001\u6311\u6218\u4e0e\u8d8b\u52bf\uff0c\u5e76\u63a2\u8ba8\u4e86\u672c\u4f53\u6280\u672f\u4e0eHadoop\u3001Spark\u7b49\u5927\u6570\u636e\u5e73\u53f0\u7684\u878d\u5408\u6f5c\u529b\u3002", "conclusion": "\u672c\u4f53\u9a71\u52a8\u7684\u8bed\u4e49\u6570\u636e\u7ba1\u7406\u80fd\u6709\u6548\u652f\u6491\u53ef\u6269\u5c55\u3001\u4e92\u64cd\u4f5c\u4e14\u9ad8\u6027\u80fd\u7684\u533b\u7597\u6570\u636e\u751f\u6001\u7cfb\u7edf\uff0c\u7ed3\u5408AI\u3001IoT\u548c\u5b9e\u65f6\u5206\u6790\u7b49\u65b0\u5174\u6280\u672f\uff0c\u6709\u671b\u63a8\u52a8\u667a\u80fd\u533b\u7597\u5206\u6790\u7684\u53d1\u5c55\u3002"}}
{"id": "2510.06187", "categories": ["cs.SE", "cs.AI", "cs.CY"], "pdf": "https://arxiv.org/pdf/2510.06187", "abs": "https://arxiv.org/abs/2510.06187", "authors": ["Griffin Pitts", "Aum Pandya", "Darsh Rank", "Tirth Bhatt", "Muntasir Hoq", "Bita Akram"], "title": "Automated Program Repair of Uncompilable Student Code", "comment": null, "summary": "A significant portion of student programming submissions in CS1 learning\nenvironments are uncompilable, limiting their use in student modeling and\ndownstream knowledge tracing. Traditional modeling pipelines often exclude\nthese cases, discarding observations of student learning. This study\ninvestigates automated program repair as a strategy to recover uncompilable\ncode while preserving students' structural intent for use in student modeling.\nWithin this framework, we assess large language models (LLMs) as repair agents,\nincluding GPT-5 (OpenAI), Claude 3.5 Haiku (Anthropic), and Gemini 2.5 Flash\n(Google), under high- and low-context prompting conditions. Repairs were\nevaluated for compilability, edit distance, and preservation of students'\noriginal structure and logic. We find that while all three LLMs are capable of\nproducing compilable repairs, their behavior diverges in how well they preserve\nstudents' control flow and code structure, which affects their pedagogical\nutility. By recovering uncompilable submissions, this work enables richer and\nmore comprehensive analyses of learners' coding processes and development over\ntime.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u81ea\u52a8\u4fee\u590dCS1\u8bfe\u7a0b\u4e2d\u5b66\u751f\u63d0\u4ea4\u7684\u4e0d\u53ef\u7f16\u8bd1\u4ee3\u7801\uff0c\u5728\u4fdd\u8bc1\u4fee\u590d\u540e\u4ee3\u7801\u53ef\u7f16\u8bd1\u7684\u540c\u65f6\u5c3d\u53ef\u80fd\u4fdd\u7559\u5b66\u751f\u539f\u59cb\u4ee3\u7801\u7684\u7ed3\u6784\u4e0e\u903b\u8f91\uff0c\u4ee5\u652f\u6301\u66f4\u5168\u9762\u7684\u5b66\u751f\u5efa\u6a21\u548c\u77e5\u8bc6\u8ffd\u8e2a\u3002", "motivation": "\u5927\u91cf\u5b66\u751f\u5728CS1\u5b66\u4e60\u73af\u5883\u4e2d\u63d0\u4ea4\u7684\u4ee3\u7801\u65e0\u6cd5\u7f16\u8bd1\uff0c\u4f20\u7edf\u5b66\u751f\u5efa\u6a21\u65b9\u6cd5\u901a\u5e38\u76f4\u63a5\u4e22\u5f03\u8fd9\u4e9b\u6570\u636e\uff0c\u5bfc\u81f4\u4e22\u5931\u91cd\u8981\u7684\u5b66\u4e60\u8fc7\u7a0b\u4fe1\u606f\u3002\u56e0\u6b64\uff0c\u4e9f\u9700\u4e00\u79cd\u65b9\u6cd5\u5728\u4fdd\u7559\u5b66\u751f\u7f16\u7a0b\u610f\u56fe\u7684\u524d\u63d0\u4e0b\u4fee\u590d\u8fd9\u4e9b\u4ee3\u7801\u3002", "method": "\u8bc4\u4f30GPT-5\u3001Claude 3.5 Haiku\u548cGemini 2.5 Flash\u4e09\u79cd\u5927\u8bed\u8a00\u6a21\u578b\u5728\u9ad8/\u4f4e\u4e0a\u4e0b\u6587\u63d0\u793a\u6761\u4ef6\u4e0b\u5bf9\u4e0d\u53ef\u7f16\u8bd1\u4ee3\u7801\u7684\u81ea\u52a8\u4fee\u590d\u80fd\u529b\uff0c\u4ece\u53ef\u7f16\u8bd1\u6027\u3001\u7f16\u8f91\u8ddd\u79bb\u53ca\u539f\u59cb\u7ed3\u6784\u903b\u8f91\u4fdd\u7559\u7a0b\u5ea6\u4e09\u4e2a\u65b9\u9762\u8fdb\u884c\u8861\u91cf\u3002", "result": "\u4e09\u79cdLLM\u5747\u80fd\u751f\u6210\u53ef\u7f16\u8bd1\u7684\u4fee\u590d\u4ee3\u7801\uff0c\u4f46\u5728\u4fdd\u7559\u5b66\u751f\u63a7\u5236\u6d41\u548c\u4ee3\u7801\u7ed3\u6784\u65b9\u9762\u8868\u73b0\u4e0d\u540c\uff0c\u5f71\u54cd\u5176\u5728\u6559\u5b66\u4e2d\u7684\u5b9e\u7528\u6027\u3002", "conclusion": "\u901a\u8fc7\u81ea\u52a8\u4fee\u590d\u4e0d\u53ef\u7f16\u8bd1\u4ee3\u7801\uff0c\u672c\u7814\u7a76\u4e3a\u66f4\u4e30\u5bcc\u3001\u5168\u9762\u5730\u5206\u6790\u5b66\u751f\u7f16\u7a0b\u8fc7\u7a0b\u548c\u80fd\u529b\u53d1\u5c55\u63d0\u4f9b\u4e86\u53ef\u80fd\u3002"}}
{"id": "2510.05943", "categories": ["cs.DC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.05943", "abs": "https://arxiv.org/abs/2510.05943", "authors": ["Zheyue Tan", "Mustapha Abdullahi", "Tuo Shi", "Huining Yuan", "Zelai Xu", "Chao Yu", "Boxun Li", "Bo Zhao"], "title": "EARL: Efficient Agentic Reinforcement Learning Systems for Large Language Models", "comment": null, "summary": "Reinforcement learning (RL) has become a pivotal component of large language\nmodel (LLM) post-training, and agentic RL extends this paradigm to operate as\nagents through multi-turn interaction and tool use. Scaling such systems\nexposes two practical bottlenecks: (1) context length grows rapidly during\ntraining, inflating memory usage and latency, and triggering out-of-memory\n(OOM) failures; and (2) intermediate tensors accumulate with context length,\nmaking cross-device data movement a major system bottleneck.\n  We present EARL, a scalable system for efficient agentic RL. EARL designs a\nparallelism selector that dynamically adapts model and training parallelism\nacross RL stages based on sequence length and system load, and a data\ndispatcher that performs layout-aware, decentralized exchange of intermediate\ndata batches. Together, these components increase throughput, reduce\nlong-context failures, and enable stable large-scale training of agentic LLMs\nwithout relying on hard limits or penalties of context length.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86EARL\uff0c\u4e00\u79cd\u7528\u4e8e\u9ad8\u6548\u4ee3\u7406\u5f0f\u5f3a\u5316\u5b66\u4e60\uff08agentic RL\uff09\u7684\u53ef\u6269\u5c55\u7cfb\u7edf\uff0c\u901a\u8fc7\u52a8\u6001\u8c03\u6574\u5e76\u884c\u7b56\u7565\u548c\u4f18\u5316\u4e2d\u95f4\u6570\u636e\u4ea4\u6362\uff0c\u89e3\u51b3\u4e86\u957f\u4e0a\u4e0b\u6587\u8bad\u7ec3\u4e2d\u7684\u5185\u5b58\u548c\u901a\u4fe1\u74f6\u9888\u95ee\u9898\u3002", "motivation": "\u5728\u5927\u89c4\u6a21\u4ee3\u7406\u5f0f\u5f3a\u5316\u5b66\u4e60\u4e2d\uff0c\u968f\u7740\u4e0a\u4e0b\u6587\u957f\u5ea6\u8fc5\u901f\u589e\u957f\uff0c\u7cfb\u7edf\u9762\u4e34\u5185\u5b58\u5360\u7528\u9ad8\u3001\u5ef6\u8fdf\u5927\u3001OOM\u9519\u8bef\u9891\u53d1\u4ee5\u53ca\u8de8\u8bbe\u5907\u6570\u636e\u4f20\u8f93\u5f00\u9500\u5927\u7684\u95ee\u9898\uff0c\u4e9f\u9700\u9ad8\u6548\u53ef\u6269\u5c55\u7684\u8bad\u7ec3\u7cfb\u7edf\u3002", "method": "EARL\u5f15\u5165\u4e86\u4e24\u4e2a\u6838\u5fc3\u7ec4\u4ef6\uff1a\u4e00\u662f\u5e76\u884c\u9009\u62e9\u5668\uff0c\u6839\u636e\u5e8f\u5217\u957f\u5ea6\u548c\u7cfb\u7edf\u8d1f\u8f7d\u52a8\u6001\u8c03\u6574\u6a21\u578b\u4e0e\u8bad\u7ec3\u5e76\u884c\u7b56\u7565\uff1b\u4e8c\u662f\u6570\u636e\u5206\u53d1\u5668\uff0c\u6267\u884c\u5e03\u5c40\u611f\u77e5\u7684\u53bb\u4e2d\u5fc3\u5316\u4e2d\u95f4\u6570\u636e\u6279\u4ea4\u6362\u3002", "result": "\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u8bad\u7ec3\u541e\u5410\u91cf\uff0c\u51cf\u5c11\u4e86\u957f\u4e0a\u4e0b\u6587\u5bfc\u81f4\u7684\u5931\u8d25\uff0c\u5e76\u652f\u6301\u5728\u4e0d\u9650\u5236\u4e0a\u4e0b\u6587\u957f\u5ea6\u7684\u524d\u63d0\u4e0b\u7a33\u5b9a\u8fdb\u884c\u5927\u89c4\u6a21\u4ee3\u7406\u5f0f\u5927\u8bed\u8a00\u6a21\u578b\u8bad\u7ec3\u3002", "conclusion": "EARL\u6709\u6548\u89e3\u51b3\u4e86\u4ee3\u7406\u5f0f\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u7cfb\u7edf\u74f6\u9888\uff0c\u4e3a\u5927\u89c4\u6a21\u3001\u957f\u4e0a\u4e0b\u6587\u7684LLM\u4ee3\u7406\u8bad\u7ec3\u63d0\u4f9b\u4e86\u9ad8\u6548\u4e14\u7a33\u5b9a\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2405.14209", "categories": ["cs.PF", "cs.AR", "cs.DC"], "pdf": "https://arxiv.org/pdf/2405.14209", "abs": "https://arxiv.org/abs/2405.14209", "authors": ["Xi Wang", "Jie Liu", "Jianbo Wu", "Shuangyan Yang", "Jie Ren", "Bhanu Shankar", "Dong Li"], "title": "Exploring and Evaluating Real-world CXL: Use Cases and System Adoption", "comment": null, "summary": "Compute eXpress Link (CXL) is emerging as a promising memory interface\ntechnology. However, its performance characteristics remain largely unclear due\nto the limited availability of production hardware. Key questions include: What\nare the use cases for the CXL memory? What are the impacts of the CXL memory on\napplication performance? How to use the CXL memory in combination with existing\nmemory components? In this work, we study the performance of three genuine CXL\nmemory-expansion cards from different vendors. We characterize the basic\nperformance of the CXL memory, study how HPC applications and large language\nmodels (LLM) can benefit from the CXL memory, and study the interplay between\nmemory tiering and page interleaving. We also propose a novel data object-level\ninterleaving policy to match the interleaving policy with memory access\npatterns. Our findings reveal the challenges and opportunities of using the CXL\nmemory.", "AI": {"tldr": "\u672c\u6587\u901a\u8fc7\u5b9e\u6d4b\u4e09\u79cd\u4e0d\u540c\u5382\u5546\u7684CXL\u5185\u5b58\u6269\u5c55\u5361\uff0c\u7cfb\u7edf\u8bc4\u4f30\u4e86CXL\u5185\u5b58\u7684\u57fa\u672c\u6027\u80fd\u3001\u5bf9HPC\u5e94\u7528\u548c\u5927\u8bed\u8a00\u6a21\u578b\u7684\u6027\u80fd\u5f71\u54cd\uff0c\u5e76\u63d0\u51fa\u4e86\u5339\u914d\u5185\u5b58\u8bbf\u95ee\u6a21\u5f0f\u7684\u5bf9\u8c61\u7ea7\u4ea4\u9519\u7b56\u7565\u3002", "motivation": "\u7531\u4e8eCXL\u786c\u4ef6\u5c1a\u672a\u5e7f\u6cdb\u53ef\u7528\uff0c\u5176\u6027\u80fd\u7279\u5f81\u548c\u9002\u7528\u573a\u666f\u5c1a\u4e0d\u660e\u786e\uff0c\u4e9f\u9700\u5b9e\u8bc1\u7814\u7a76\u4ee5\u6307\u5bfc\u5b9e\u9645\u90e8\u7f72\u548c\u4f18\u5316\u3002", "method": "\u5bf9\u4e09\u4e2a\u5382\u5546\u7684CXL\u5185\u5b58\u6269\u5c55\u5361\u8fdb\u884c\u5b9e\u6d4b\uff0c\u5206\u6790\u5176\u57fa\u7840\u6027\u80fd\uff0c\u8bc4\u4f30\u5176\u5728\u9ad8\u6027\u80fd\u8ba1\u7b97\uff08HPC\uff09\u548c\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u4e2d\u7684\u5e94\u7528\u6548\u679c\uff0c\u5e76\u7814\u7a76\u5185\u5b58\u5206\u5c42\u4e0e\u9875\u9762\u4ea4\u9519\u7b56\u7565\uff1b\u63d0\u51fa\u4e00\u79cd\u65b0\u7684\u6570\u636e\u5bf9\u8c61\u7ea7\u4ea4\u9519\u7b56\u7565\u3002", "result": "\u63ed\u793a\u4e86CXL\u5185\u5b58\u4f7f\u7528\u4e2d\u7684\u6311\u6218\u4e0e\u673a\u9047\uff0c\u9a8c\u8bc1\u4e86\u5176\u5728\u7279\u5b9a\u8d1f\u8f7d\u4e0b\u7684\u6027\u80fd\u6f5c\u529b\uff0c\u5e76\u5c55\u793a\u4e86\u6240\u63d0\u5bf9\u8c61\u7ea7\u4ea4\u9519\u7b56\u7565\u7684\u6709\u6548\u6027\u3002", "conclusion": "CXL\u5185\u5b58\u5177\u6709\u5e94\u7528\u6f5c\u529b\uff0c\u4f46\u9700\u7ed3\u5408\u8bbf\u95ee\u6a21\u5f0f\u8fdb\u884c\u7cbe\u7ec6\u5316\u7ba1\u7406\uff1b\u5408\u7406\u7684\u5185\u5b58\u5c42\u7ea7\u4e0e\u4ea4\u9519\u7b56\u7565\u53ef\u663e\u8457\u63d0\u5347\u6027\u80fd\u3002"}}
