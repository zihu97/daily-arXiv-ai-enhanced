<div id=toc></div>

# Table of Contents

- [cs.SE](#cs.SE) [Total: 12]
- [cs.AR](#cs.AR) [Total: 1]
- [cs.PF](#cs.PF) [Total: 1]
- [cs.DC](#cs.DC) [Total: 9]


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [1] [An Experimental Study of Real-Life LLM-Proposed Performance Improvements](https://arxiv.org/abs/2510.15494)
*Lirong Yi,Gregory Gay,Philipp Leitner*

Main category: cs.SE

TL;DR: 本文研究大语言模型（LLMs）是否能生成高效的代码，通过对65个真实Java优化任务的实验发现，LLM生成的代码通常优于原始基线，但显著落后于人类开发者编写的优化方案；约三分之二的LLM方案与人类思路相似，其余为原创但较少带来显著性能提升。


<details>
  <summary>Details</summary>
Motivation: 探究大语言模型在代码性能优化方面的能力，特别是在真实世界任务中是否能生成媲美人类开发者的高效代码。

Method: 从开源Java项目中挖掘65个真实性能优化任务，使用两个主流LLM在四种提示策略下自动生成补丁，并通过严格基准测试将其与原始代码和人类优化方案进行比较。

Result: LLM生成的代码在多数情况下优于基线，但人类方案在性能上显著优于LLM；约66%的LLM方案与人类优化思路语义相似，其余为原创但仅偶尔带来显著性能提升。

Conclusion: 尽管LLM能在代码性能优化上取得一定成效，但其生成的解决方案通常不如人类开发者高效，尤其在提出真正最优或高效益的原创优化策略方面仍有明显差距。

Abstract: Large Language Models (LLMs) can generate code, but can they generate fast
code? In this paper, we study this question using a dataset of 65 real-world
tasks mined from open-source Java programs. We specifically select tasks where
developers achieved significant speedups, and employ an automated pipeline to
generate patches for these issues using two leading LLMs under four prompt
variations. By rigorously benchmarking the results against the baseline and
human-authored solutions, we demonstrate that LLM-generated code indeed
improves performance over the baseline in most cases. However, patches proposed
by human developers outperform LLM fixes by a statistically significant margin,
indicating that LLMs often fall short of finding truly optimal solutions. We
further find that LLM solutions are semantically identical or similar to the
developer optimization idea in approximately two-thirds of cases, whereas they
propose a more original idea in the remaining one-third. However, these
original ideas only occasionally yield substantial performance gains.

</details>


### [2] [Automated Snippet-Alignment Data Augmentation for Code Translation](https://arxiv.org/abs/2510.15004)
*Zhiming Zhang,Qingfu Zhu,Xianzhen Luo,Yixuan Wang,Bohan Li,Wanxiang Che*

Main category: cs.SE

TL;DR: 本文提出一种利用大语言模型自动生成代码片段对齐（SA）数据的数据增强方法，并结合程序对齐（PA）数据，采用两阶段训练策略，在代码翻译任务中显著提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 现有代码翻译研究主要依赖程序对齐（PA）数据进行数据增强，但PA数据因长度较长难以提供细粒度的训练信号；而片段对齐（SA）数据虽适合细粒度对齐学习，但缺乏有效生成方法。因此，本文旨在通过大语言模型自动生成高质量SA数据，并结合PA数据提升代码翻译效果。

Method: 利用大语言模型（LLMs）自动生成片段对齐（SA）的平行语料，并设计一种两阶段训练策略：先在PA数据上训练，再在增强后的SA数据上微调，以充分利用两类数据的优势。

Result: 在TransCoder-test数据集上的实验表明，所提出的方法相比仅使用PA数据微调的基线模型，在pass@k指标上最高提升了3.78%。

Conclusion: 结合自动生成的SA数据与两阶段训练策略能有效提升代码翻译模型的性能，验证了细粒度对齐数据在代码翻译任务中的重要价值。

Abstract: Code translation aims to translate the code from its source language to the
target language and is used in various software development scenarios. Recent
developments in Large Language Models (LLMs) have showcased their capabilities
in code translation, and parallel corpora play a crucial role in training
models for code translation. Parallel corpora can be categorized into
program-alignment (PA) and snippet-alignment (SA) data. Although PA data has
complete context and is suitable for semantic alignment learning, it may not
provide adequate fine-grained training signals due to its extended length,
while the brevity of SA data enables more fine-grained alignment learning. Due
to limited parallel corpora, researchers explore several augmentation methods
for code translation. Previous studies mainly focus on augmenting PA data. In
this paper, we propose a data augmentation method that leverages LLMs to
generate SA data automatically. To fully leverage both PA data and SA data, we
explore a simple yet effective two-stage training strategy, which consistently
enhances model performance compared to fine-tuning solely on PA data.
Experiments on TransCoder-test demonstrate that our augmented SA data combined
with the two-stage training approach yields consistent improvements over the
baseline, achieving a maximum gain of 3.78% on pass@k.

</details>


### [3] [Assessing Coherency and Consistency of Code Execution Reasoning by Large Language Models](https://arxiv.org/abs/2510.15079)
*Changshu Liu,Yang Chen,Reyhaneh Jabbarvand*

Main category: cs.SE

TL;DR: 本文提出了CES任务，用于评估大语言模型（LLMs）在模拟程序执行及其在编程任务中推理能力的表现，引入“一致性”概念和新的推理一致性度量，并发现尽管部分LLMs能正确预测输出，其执行模拟常缺乏逻辑一致性，且在不同测试间推理表现不稳定。


<details>
  <summary>Details</summary>
Motivation: 现有评估方法难以区分LLMs是通过真实执行推理还是依赖自然语言捷径、幻觉或数据泄露来获得正确输出，因此需要一种能衡量执行模拟逻辑一致性和推理稳定性的新评估框架。

Method: CES任务通过评估变量预测的正确性与执行模拟是否符合常识逻辑（即“一致性”）来判断LLMs的执行推理质量，并引入基于路径覆盖的推理一致性度量（强、弱、随机）；在HumanEval上对16个LLMs进行评估，并与bug预测/定位/修复任务进行对比分析。

Result: 在HumanEval上，LLMs平均达到81.42%的一致性执行模拟，其中46.92%导致正确输出，53.08%为错误输出；GPT-4和DeepSeek-R1等前沿模型表现出最多的不一致推理；跨测试的推理一致性主要为随机（48.87%）或弱（45.37%）；在bug相关任务中，LLMs几乎未使用执行推理，成功主要依赖模式匹配或语言捷径。

Conclusion: CES能有效揭示LLMs在编程任务中推理能力的局限性，特别是其执行模拟缺乏逻辑一致性和跨测试稳定性，表明当前模型在需要路径敏感分析的任务中表现不足，CES可作为系统性检验LLMs可疑成功的方法。

Abstract: This paper proposes CES, a task to evaluate the abilities of LLMs in
simulating program execution and using that reasoning in programming tasks.
Besides measuring the correctness of variable predictions during execution
simulation, CES introduces the notion of coherence to determine whether the
simulation complies with commonsense execution logic, even if the predicted
values along the simulations are incorrect. This enables CES to rule out
suspiciously correct output predictions due to reasoning shortcuts,
hallucinations, or potential data leakage. CES also introduces a novel metric
to measure reasoning consistency across tests with the same or different prime
path coverage in a spectrum: strong, weak, and random. Evaluating 16 LLMs
(including three reasoning LLMs) using CES indicates 81.42% coherent execution
simulation on HumanEval, 46.92% and 53.08% of which result in correct and
incorrect output predictions. Frontier LLMs such as GPT-4 and DeepSeek-R1 have
the most incoherent execution reasoning, mostly due to natural language
shortcuts. Despite relatively coherent execution simulation, LLMs' reasoning
performance across different tests is inconsistent, mostly random (48.87%) or
weak (45.37%), potentially explaining their weakness in programming tasks that
require path-sensitive program analysis to succeed. We also compare CES with
bug prediction/localization/repair, which intuitively requires control- and
data-flow awareness. We observe that LLMs barely incorporate execution
reasoning into their analysis for bug-related tasks, and their success is
primarily due to inherent abilities in pattern matching or natural language
shortcuts, if not data leakage. Without reasoning, there is a threat to the
generalizability of LLMs in dealing with unseen bugs or patterns in different
contexts. CES can be used to vet the suspicious success of LLMs in these tasks
systematically.

</details>


### [4] [Community Engagement and the Lifespan of Open-Source Software Projects](https://arxiv.org/abs/2510.15408)
*Mohit,Kuljit Kaur Chahal*

Main category: cs.SE

TL;DR: 本研究通过分析33,946个GitHub仓库，定义并验证了开源软件（OSS）项目中的社区参与（CE）指标，发现CE与项目动态（如发布、提交等）显著相关，并揭示了CE在项目生命周期不同阶段对项目寿命的复杂影响。


<details>
  <summary>Details</summary>
Motivation: 开源软件项目的长期存续依赖于社区参与，但社区参与对项目动态和寿命的可量化影响尚未被充分研究。

Method: 研究分析了33,946个GitHub仓库，定义并操作化社区参与指标（包括每月的问题数、评论数、关注者和星标数），并通过非参数检验和相关性分析评估这些指标与项目动态及寿命之间的关系。

Result: 社区参与指标与项目动态显著相关，且在高参与度项目中相关性更强；在项目寿命方面，年轻项目每月社区参与率最高，随项目年龄增长而下降，但部分长寿项目仍保持极高活跃度。初期的社区参与爆发对项目建立至关重要，而持续高参与则推动极端长寿。随着项目老化，主动参与（如问题互动）的影响增强，而被动关注的影响减弱。

Conclusion: 社区参与动态地驱动开源软件项目的寿命与发展，本研究建立了经过验证的社区参与指标，并深入揭示了不同类型社区活动对项目长期存续的贡献机制。

Abstract: Open-source software (OSS) projects depend on community engagement (CE) for
longevity. However, CE's quantifiable impact on project dynamics and lifespan
is underexplored. Objectives: This study defines CE in OSS, identifies key
metrics, and evaluates their influence on project dynamics (releases, commits,
branches) and lifespan. Methods: We analyzed 33,946 GitHub repositories,
defining and operationalizing CE with validated per-month metrics (issues,
comments, watchers, stargazers). Non-parametric tests and correlations assessed
relationships with project dynamics and lifespan across quartiles. Results: CE
metrics significantly associate with project dynamics, with stronger
correlations in highly engaged projects. For lifespan, a complex pattern
emerged: per-month CE rates are highest in younger projects, declining with
age. Yet, a subset of long-lived projects maintains exceptionally high
activity. Initial CE bursts appear crucial for establishment, while sustained
high engagement drives extreme longevity. Active issue engagement's influence
intensifies with age, but passive attention's declines. Conclusion: CE
dynamically drives OSS project longevity and development. Our findings
establish validated CE metrics and offer deeper insights into how diverse
community activity patterns contribute to project longevity.

</details>


### [5] [Selecting and Combining Large Language Models for Scalable Code Clone Detection](https://arxiv.org/abs/2510.15480)
*Muslim Chochlov,Gul Aftab Ahmed,James Vincent Patten,Yuanhua Han,Guoxian Lu,David Gregg,Jim Buckley*

Main category: cs.SE

TL;DR: 本文研究了大语言模型（LLM）在代码克隆检测中的适用性与集成效果，发现CodeT5+110M、CuBERT和SPTCode表现最佳，并证明通过适当的集成策略（如最大值或求和）可进一步提升检测精度。


<details>
  <summary>Details</summary>
Motivation: 代码克隆可能引发知识产权侵权和安全漏洞，而现有方法在高效、可扩展地检测高度差异化的克隆方面仍存在挑战；同时，LLM的快速发展使得如何选择合适模型及是否集成多个LLM成为亟待解决的问题。

Method: 作者筛选出76个LLM并评估其在BigCloneBench和一个商业大规模数据集上的表现；随后探索了多种LLM集成策略，包括分数归一化与不同融合方式（如平均、最大值、求和）。

Result: 实验表明没有统一最优的LLM，但CodeT5+110M等模型表现突出；在商业数据集上，CodeT5+110M的精确率达39.71%，是此前CodeBERT的两倍；集成方法进一步将精确率提升至46.91%。

Conclusion: 针对代码克隆检测任务，模型选择应考虑嵌入维度、词表大小和训练数据适配性；合理设计的LLM集成策略能显著提升检测效果，尤其在大规模数据集上具有统计显著性和实用性。

Abstract: Source code clones pose risks ranging from intellectual property violations
to unintended vulnerabilities. Effective and efficient scalable clone
detection, especially for diverged clones, remains challenging. Large language
models (LLMs) have recently been applied to clone detection tasks. However, the
rapid emergence of LLMs raises questions about optimal model selection and
potential LLM-ensemble efficacy.
  This paper addresses the first question by identifying 76 LLMs and filtering
them down to suitable candidates for large-scale clone detection. The
candidates were evaluated on two public industrial datasets, BigCloneBench, and
a commercial large-scale dataset. No uniformly 'best-LLM' emerged, though
CodeT5+110M, CuBERT and SPTCode were top-performers. Analysis of LLM-candidates
suggested that smaller embedding sizes, smaller tokenizer vocabularies and
tailored datasets are advantageous. On commercial large-scale dataset a
top-performing CodeT5+110M achieved 39.71\% precision: twice the precision of
previously used CodeBERT.
  To address the second question, this paper explores ensembling of the
selected LLMs: effort-effective approach to improving effectiveness. Results
suggest the importance of score normalization and favoring ensembling methods
like maximum or sum over averaging. Also, findings indicate that ensembling
approach can be statistically significant and effective on larger datasets: the
best-performing ensemble achieved even higher precision of 46.91\% over
individual LLM on the commercial large-scale code.

</details>


### [6] [Enhancing Code Review through Fuzzing and Likely Invariants](https://arxiv.org/abs/2510.15512)
*Wachiraphan Charoenwet,Patanamon Thongtanunam,Van-Thuan Pham,Christoph Treude*

Main category: cs.SE

TL;DR: FuzzSight 是一个结合模糊测试与动态不变量分析的框架，用于在代码审查阶段早期识别由代码变更引发的非崩溃型行为异常，从而帮助发现回归缺陷和漏洞。


<details>
  <summary>Details</summary>
Motivation: 传统代码审查依赖静态检查，难以覆盖程序的动态行为；而模糊测试虽能生成多样化输入，但其产生的行为数据缺乏有效分析机制，难以在审查中实用化。

Method: 提出 FuzzSight 框架，通过非崩溃型模糊测试输入提取程序在特定位置的“可能不变量”（likely invariants），以此表征程序行为，并对比不同版本间的行为差异，识别潜在缺陷。

Result: 在评估中，FuzzSight 能标记 75% 的回归缺陷和高达 80% 的漏洞；相比静态分析工具（SAST），其在识别缺陷代码块方面检测率高出十倍且误报更少。

Conclusion: 将模糊测试与不变量分析结合，可有效补充静态代码审查，为早期发现非崩溃型行为异常提供实用、高效的动态洞察。

Abstract: Many software projects employ manual code review to gatekeep defects and
vulnerabilities in the code before integration. However, reviewers often work
under time pressure and rely primarily on static inspection, leaving the
dynamic aspects of the program unexplored. Dynamic analyses could reveal such
behaviors, but they are rarely integrated into reviews. Among them, fuzzing is
typically applied later to uncover crashing bugs. Yet its ability to exercise
code with diverse inputs makes it promising for exposing non-crashing, but
unexpected, behaviors earlier. Still, without suitable mechanisms to analyze
program behaviors, the rich data produced during fuzzing remains inaccessible
to reviewers, limiting its practical value in this context.
  We hypothesize that unexpected variations in program behaviors could signify
potential bugs. The impact of code changes can be automatically captured at
runtime. Representing program behavior as likely invariants, dynamic properties
consistently observed at specific program points, can provide practical signals
of behavioral changes. Such signals offer a way to distinguish between intended
changes and unexpected behavioral shifts from code changes.
  We present FuzzSight, a framework that leverages likely invariants from
non-crashing fuzzing inputs to highlight behavioral differences across program
versions. By surfacing such differences, it provides insights into which code
blocks may need closer attention. In our evaluation, FuzzSight flagged 75% of
regression bugs and up to 80% of vulnerabilities uncovered by 24-hour fuzzing.
It also outperformed SAST in identifying buggy code blocks, achieving ten times
higher detection rates with fewer false alarms. In summary, FuzzSight
demonstrates the potential and value of leveraging fuzzing and invariant
analysis for early-stage code review, bridging static inspection with dynamic
behavioral insights.

</details>


### [7] [Colepp: uma ferramenta multiplataforma para coleta de dados de dispositivos vestiveis](https://arxiv.org/abs/2510.15565)
*Vinicius Moraes de Jesus,Andre Georghton Cardoso Pacheco*

Main category: cs.SE

TL;DR: Colepp is an open-source, cross-platform tool that synchronizes and collects physiological and motion data from multiple wearable devices via a smartphone hub, enabling customizable real-world datasets for research applications.


<details>
  <summary>Details</summary>
Motivation: The lack of large, high-quality public datasets and limited control over data collection conditions impede the development of reliable algorithms for wearable-based applications.

Method: Colepp uses a smartphone as a central hub to collect and synchronize data from a Polar H10 chest strap (ECG) and a Wear OS smartwatch (PPG, accelerometer, gyroscope), employing a custom synchronization protocol and exporting data in CSV format.

Result: A use case demonstrates that Colepp effectively produces consistent and synchronized physiological and motion signals suitable for tasks like human activity recognition and heart rate estimation.

Conclusion: Colepp addresses key challenges in wearable data collection by providing an accessible, open-source solution for generating high-quality, synchronized, real-world datasets.

Abstract: The widespread adoption of wearable devices such as smartwatches and fitness
trackers has fueled the demand for reliable physiological and movement data
collection tools. However, challenges such as limited access to large,
high-quality public datasets and a lack of control over data collection
conditions hinder the development of robust algorithms. This work presents
Colepp, an open-source, cross-platform tool designed to collect and synchronize
data from multiple wearable devices, including heart rate (via ECG and PPG) and
motion signals (accelerometer and gyroscope). The system integrates a
smartphone as a central hub, receiving data from a Polar H10 chest strap and a
Wear OS smartwatch, and exporting synchronized datasets in CSV format. Through
a custom synchronization protocol and user-friendly interface, Colepp
facilitates the generation of customizable, real-world datasets suitable for
applications such as human activity recognition and heart rate estimation. A
use case shows the effectiveness of the tool in producing consistent and
synchronized signals.

</details>


### [8] [Leveraging Test Driven Development with Large Language Models for Reliable and Verifiable Spreadsheet Code Generation: A Research Framework](https://arxiv.org/abs/2510.15585)
*Dr Simon Thorne,Dr Advait Sarkar*

Main category: cs.SE

TL;DR: 本文提出将测试驱动开发（TDD）与大语言模型（LLM）生成相结合，以提升生成代码（包括电子表格公式和传统编程语言）的正确性、可靠性与用户信心，尤其面向缺乏编程训练但对准确性要求高的用户群体。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在生成代码时存在幻觉、逻辑不一致和语法错误等问题，在金融建模和科学计算等高风险领域尤为危险，因此需要一种可靠的方法来提升生成结果的准确性与可信度。

Method: 提出一个结合测试驱动开发（TDD）与LLM生成的结构化研究框架，采用“先写测试”的方法对模型输出进行技术约束和认知引导，并设计了包含参与者分组、评估指标和TDD提示示例的实验方案。

Result: 该框架适用于电子表格、Python、Rust等多种编程环境，有望提升用户的计算思维、提示工程技能和参与度，尤其帮助非专业编程用户减少逻辑错误。

Conclusion: 通过强调测试驱动思维，该方法有望在教育和专业开发实践中实现负责任且可靠的LLM集成，作者呼吁合作以进一步完善和实证评估该框架。

Abstract: Large Language Models (LLMs), such as ChatGPT, are increasingly leveraged for
generating both traditional software code and spreadsheet logic. Despite their
impressive generative capabilities, these models frequently exhibit critical
issues such as hallucinations, subtle logical inconsistencies, and syntactic
errors, risks particularly acute in high stakes domains like financial
modelling and scientific computations, where accuracy and reliability are
paramount. This position paper proposes a structured research framework that
integrates the proven software engineering practice of Test-Driven Development
(TDD) with Large Language Model (LLM) driven generation to enhance the
correctness of, reliability of, and user confidence in generated outputs. We
hypothesise that a "test first" methodology provides both technical constraints
and cognitive scaffolding, guiding LLM outputs towards more accurate,
verifiable, and comprehensible solutions. Our framework, applicable across
diverse programming contexts, from spreadsheet formula generation to scripting
languages such as Python and strongly typed languages like Rust, includes an
explicitly outlined experimental design with clearly defined participant
groups, evaluation metrics, and illustrative TDD based prompting examples. By
emphasising test driven thinking, we aim to improve computational thinking,
prompt engineering skills, and user engagement, particularly benefiting
spreadsheet users who often lack formal programming training yet face serious
consequences from logical errors. We invite collaboration to refine and
empirically evaluate this approach, ultimately aiming to establish responsible
and reliable LLM integration in both educational and professional development
practices.

</details>


### [9] [Interact and React: Exploring Gender Patterns in Development and the Impact on Innovation and Robustness of a User Interface Tool](https://arxiv.org/abs/2510.15642)
*Sian Brooke*

Main category: cs.SE

TL;DR: 该研究分析了React项目11年来的贡献数据，发现女性在功能增强和依赖管理方面贡献显著更多，表明性别多样性有助于提升软件的创新性与稳健性。


<details>
  <summary>Details</summary>
Motivation: 探讨女性参与如何根本性地改变开源软件的开发模式，而不仅停留在象征性包容层面。

Method: 对React项目11年间的贡献数据进行分析，比较不同性别在稳健性、创新性及版本发布前贡献模式上的差异。

Result: 女性在功能增强和依赖管理方面贡献显著更多，排除女性对软件质量有害。

Conclusion: 提升性别多样性可促进软件更具包容性、创新性和稳健性。

Abstract: In open-source software design, the inclusion of women is often highlighted
simply to remind programmers that women exist. Yet, little attention is given
to how greater gender diversity, specifically women's participation, could
fundamentally alter development patterns. To understand the potential impact of
gender inclusion, this study investigates React, a widely used JavaScript
library for building user interfaces with an active contributor community. I
examine gender differences in metrics of robustness and innovation, as well as
shifts in contribution patterns leading up to major version releases over 11
years of the React project. My results show that the exclusion of women is
detrimental to software as women contribute significantly more to feature
enhancement and dependency management. By exploring how gender influences
innovation and robustness in the development of React, the study offers
critical insights into how increasing gender diversity could lead to more
inclusive, innovative, and robust software.

</details>


### [10] [MirrorFuzz: Leveraging LLM and Shared Bugs for Deep Learning Framework APIs Fuzzing](https://arxiv.org/abs/2510.15690)
*Shiwen Ou,Yuwei Li,Lu Yu,Chengkun Wei,Tingke Wen,Qiangpu Chen,Yu Chen,Haizhi Tang,Zulie Pan*

Main category: cs.SE

TL;DR: MirrorFuzz is an automated API fuzzing tool that leverages historical bug data and large language models to detect shared bugs across deep learning frameworks by identifying and testing similar APIs, significantly improving code coverage and uncovering hundreds of new bugs.


<details>
  <summary>Details</summary>
Motivation: Bugs in deep learning frameworks can propagate to critical failures in downstream applications. Despite existing bug detection methods, there is limited research on shared API patterns across frameworks and the associated risks of common vulnerabilities.

Method: MirrorFuzz uses a three-stage approach: (1) collecting historical bug data to identify potentially buggy APIs, (2) matching these APIs with similar ones within and across frameworks, and (3) using large language models to generate test code that exploits historical bug patterns to trigger analogous bugs in similar APIs.

Result: Evaluated on TensorFlow, PyTorch, OneFlow, and Jittor, MirrorFuzz improves code coverage by 39.92% and 98.20% over state-of-the-art methods on TensorFlow and PyTorch, respectively. It discovered 315 bugs, 262 of which were previously unknown, and 80 have been fixed, with 52 assigned CNVD IDs.

Conclusion: MirrorFuzz effectively uncovers shared bugs across deep learning frameworks by exploiting API similarities and historical bug data, demonstrating significant improvements in bug detection and code coverage, thereby enhancing framework reliability and security.

Abstract: Deep learning (DL) frameworks serve as the backbone for a wide range of
artificial intelligence applications. However, bugs within DL frameworks can
cascade into critical issues in higher-level applications, jeopardizing
reliability and security. While numerous techniques have been proposed to
detect bugs in DL frameworks, research exploring common API patterns across
frameworks and the potential risks they entail remains limited. Notably, many
DL frameworks expose similar APIs with overlapping input parameters and
functionalities, rendering them vulnerable to shared bugs, where a flaw in one
API may extend to analogous APIs in other frameworks. To address this
challenge, we propose MirrorFuzz, an automated API fuzzing solution to discover
shared bugs in DL frameworks. MirrorFuzz operates in three stages: First,
MirrorFuzz collects historical bug data for each API within a DL framework to
identify potentially buggy APIs. Second, it matches each buggy API in a
specific framework with similar APIs within and across other DL frameworks.
Third, it employs large language models (LLMs) to synthesize code for the API
under test, leveraging the historical bug data of similar APIs to trigger
analogous bugs across APIs. We implement MirrorFuzz and evaluate it on four
popular DL frameworks (TensorFlow, PyTorch, OneFlow, and Jittor). Extensive
evaluation demonstrates that MirrorFuzz improves code coverage by 39.92\% and
98.20\% compared to state-of-the-art methods on TensorFlow and PyTorch,
respectively. Moreover, MirrorFuzz discovers 315 bugs, 262 of which are newly
found, and 80 bugs are fixed, with 52 of these bugs assigned CNVD IDs.

</details>


### [11] [EASELAN: An Open-Source Framework for Multimodal Biosignal Annotation and Data Management](https://arxiv.org/abs/2510.15767)
*Rathi Adarshi Rammohan,Moritz Meier,Dennis Küster,Tanja Schultz*

Main category: cs.SE

TL;DR: 本文提出了EASELAN标注框架，通过扩展ELAN工具以支持多模态与生物信号数据的高效标注，涵盖从数据准备、多通道设置、版本控制到后处理的全流程，并成功应用于日常活动（如摆桌）的高维生物信号数据集构建。


<details>
  <summary>Details</summary>
Motivation: 随着融合模型对多模态和生物信号数据需求的增长，传统标注工具难以应对日益复杂的数据标注任务，亟需更高效的标注流程支持。

Method: 在ELAN工具基础上开发EASELAN框架，新增支持标注全流程的功能模块，包括标注文件准备、多通道配置、GitHub集成版本控制和简化后处理。

Result: EASELAN成功应用于DFG资助的EASE项目中的人类日常活动（摆桌）高维生物信号数据集构建，实现了生物信号与丰富标注的无缝整合，并公开了代码和完整标注数据库。

Conclusion: EASELAN显著提升了多模态与生物信号数据的标注效率与质量，为认知机器人和机器学习研究提供了有力支持，同时通过开源促进相关领域研究发展。

Abstract: Recent advancements in machine learning and adaptive cognitive systems are
driving a growing demand for large and richly annotated multimodal data. A
prominent example of this trend are fusion models, which increasingly
incorporate multiple biosignals in addition to traditional audiovisual
channels. This paper introduces the EASELAN annotation framework to improve
annotation workflows designed to address the resulting rising complexity of
multimodal and biosignals datasets. It builds on the robust ELAN tool by adding
new components tailored to support all stages of the annotation pipeline: From
streamlining the preparation of annotation files to setting up additional
channels, integrated version control with GitHub, and simplified
post-processing. EASELAN delivers a seamless workflow designed to integrate
biosignals and facilitate rich annotations to be readily exported for further
analyses and machine learning-supported model training. The EASELAN framework
is successfully applied to a high-dimensional biosignals collection initiative
on human everyday activities (here, table setting) for cognitive robots within
the DFG-funded Collaborative Research Center 1320 Everyday Activity Science and
Engineering (EASE). In this paper we discuss the opportunities, limitations,
and lessons learned when using EASELAN for this initiative. To foster research
on biosignal collection, annotation, and processing, the code of EASELAN is
publicly available(https://github.com/cognitive-systems-lab/easelan), along
with the EASELAN-supported fully annotated Table Setting Database.

</details>


### [12] [Towards Supporting Open Source Library Maintainers with Community-Based Analytics](https://arxiv.org/abs/2510.15794)
*Rachna Raj,Diego Elias Costa*

Main category: cs.SE

TL;DR: 该论文提出通过社区使用数据分析开源库的实际API使用情况，发现仅有16%的API被依赖项目使用，且其中仅74%被测试覆盖，并提出了两个新指标帮助维护者优化测试策略。


<details>
  <summary>Details</summary>
Motivation: 开源软件维护者缺乏对其API在依赖项目中实际使用情况的持续反馈，限制了其在测试、变更影响评估和库演进方面的决策能力。

Method: 对10个流行Java库及其各自50个依赖项目进行实证研究，分析API使用情况和测试覆盖度，并提出两个新指标；同时通过调查开源开发者评估这些洞察的实用价值。

Result: 平均仅有16%的API方法被依赖生态系统实际使用，且这些被使用的API中仅有74%被库自身的测试套件部分或完全覆盖。

Conclusion: 社区驱动的API使用分析可为开源库维护者提供有价值的反馈，帮助其优化测试策略和维护决策，提升库的可持续性和实用性。

Abstract: Open-source software (OSS) is a pillar of modern software development. Its
success depends on the dedication of maintainers who work constantly to keep
their libraries stable, adapt to changing needs, and support a growing
community. Yet, they receive little to no continuous feedback on how the
projects that rely on their libraries actually use their APIs. We believe that
gaining these insights can help maintainers make better decisions, such as
refining testing strategies, understanding the impact of changes, and guiding
the evolution of their libraries more effectively. We propose the use of
community-based analytics to analyze how an OSS library is used across its
dependent ecosystem. We conduct an empirical study of 10 popular Java libraries
and each with their respective dependent ecosystem of 50 projects. Our results
reveal that while library developers offer a wide range of API methods, only
16% on average are actively used by their dependent ecosystem. Moreover, only
74% of the used API methods are partially or fully covered by their library
test suite. We propose two metrics to help developers evaluate their test suite
according to the APIs used by their community, and we conduct a survey on
open-source practitioners to assess the practical value of these insights in
guiding maintenance decisions.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [13] [Cleaning up the Mess](https://arxiv.org/abs/2510.15744)
*Haocong Luo,Ataberk Olgun,Maria Makeenkova,F. Nisa Bostanci,Geraldo F. Oliveira,A. Giray Yaglikci,Onur Mutlu*

Main category: cs.AR

TL;DR: 本文指出MICRO 2024最佳论文亚军“Mess”在Ramulator 2.0和DAMOV模拟器使用中存在严重配置与统计错误，导致其核心结论不成立，并揭示其成果不可完全复现，呼吁社区纠正错误并加强仿真验证流程。


<details>
  <summary>Details</summary>
Motivation: 确保计算机体系结构研究中仿真结果的准确性与可复现性，纠正Mess论文中因人为配置错误导致的错误结论，维护科研记录的可靠性。

Method: 通过重新配置Ramulator 2.0并验证其输出，对比真实系统性能；检查Mess论文使用的DAMOV仿真统计指标；审查其开源仓库是否包含完整复现材料。

Result: 发现Mess论文在Ramulator 2.0配置和DAMOV统计使用上存在明显错误；正确配置后Ramulator 2.0能准确反映真实内存系统行为；其开源仓库缺少关键复现代码。

Conclusion: Mess论文关于Ramulator 2.0无法准确模拟内存性能的核心主张不成立；强调仿真研究需严谨验证并积极与工具开发者协作；呼吁社区修正错误并反思评审与成果评估机制。

Abstract: A MICRO 2024 best paper runner-up publication (the Mess paper) with all three
artifact badges awarded (including "Reproducible") proposes a new benchmark to
evaluate real and simulated memory system performance. In this paper, we
demonstrate that the Ramulator 2.0 simulation results reported in the Mess
paper are incorrect and, at the time of the publication of the Mess paper,
irreproducible. We find that the authors of Mess paper made multiple trivial
human errors in both the configuration and usage of the simulators. We show
that by correctly configuring Ramulator 2.0, Ramulator 2.0's simulated memory
system performance actually resembles real system characteristics well, and
thus a key claimed contribution of the Mess paper is factually incorrect. We
also identify that the DAMOV simulation results in the Mess paper use wrong
simulation statistics that are unrelated to the simulated DRAM performance.
Moreover, the Mess paper's artifact repository lacks the necessary sources to
fully reproduce all the Mess paper's results.
  Our work corrects the Mess paper's errors regarding Ramulator 2.0 and
identifies important issues in the Mess paper's memory simulator evaluation
methodology. We emphasize the importance of both carefully and rigorously
validating simulation results and contacting simulator authors and developers,
in true open source spirit, to ensure these simulators are used with correct
configurations and as intended. We encourage the computer architecture
community to correct the Mess paper's errors. This is necessary to prevent the
propagation of inaccurate and misleading results, and to maintain the
reliability of the scientific record. Our investigation also opens up questions
about the integrity of the review and artifact evaluation processes. To aid
future work, our source code and scripts are openly available at https:
//github.com/CMU-SAFARI/ramulator2/tree/mess.

</details>


<div id='cs.PF'></div>

# cs.PF [[Back]](#toc)

### [14] [Impact of AI-Triage on Radiologist Report Turnaround Time: Real-World Time-Savings and Insights from Model Predictions](https://arxiv.org/abs/2510.15237)
*Yee Lam Elim Thompson,Jonathan Fergus,Jonathan Chung,Jana G. Delfino,Weijie Chen,Gary M. Levine,Frank W. Samuelson*

Main category: cs.PF

TL;DR: 本研究通过回顾性分析评估AI分诊设备在肺栓塞（PE）胸部CT肺动脉造影（CTPA）检查中对报告周转时间（TAT）节省的影响，发现工作时间显著缩短TAT，而非工作时间无显著差异，并通过计算模型验证了结果。


<details>
  <summary>Details</summary>
Motivation: 准确评估AI分诊设备在临床工作流中对报告周转时间的实际影响，需考虑并量化工作流参数的作用。

Method: 回顾性分析11252例成人CTPA检查，分为AI部署前后两个阶段，比较PE阳性病例从扫描完成到初步报告完成的时间（TAT），并提取PACS系统中的527234条记录构建计算模型，预测并分析工作流参数对时间节省的影响。

Result: 工作时间TAT显著缩短22.2分钟（p=0.004），非工作时间无显著变化（p=0.345）；观测结果与模型预测一致。

Conclusion: 临床工作流参数的量化有助于准确评估AI分诊设备部署后对TAT的时间节省效果，尤其在工作时间表现显著。

Abstract: Objective: To quantify the impact of workflow parameters on time-savings in
report turnaround time (TAT) due to an AI-triage device that prioritized
pulmonary embolism (PE) in chest CT pulmonary angiography (CTPA) exams.
Methods: This retrospective study analyzed 11252 adult CTPA exams conducted for
suspected PE at a single tertiary academic medical center. Data was divided
into two periods: pre-AI and post-AI. For PE-positive exams, TAT - defined as
the duration from patient scan completion to the first preliminary report
completion - was compared between the two periods. Time-savings were reported
separately for work-hour and off-hour cohorts. To characterize radiologist
workflow, 527234 records were retrieved from the PACS and workflow parameters
such as exam inter-arrival time and radiologist read-time extracted. These
parameters were input into a computational model to predict time-savings
following deployment of an AI-triage device and to study the impact of workflow
parameters. Results: The pre-AI dataset included 4694 chest CTPA exams with
13.3% being PE-positive. The post-AI dataset comprised 6558 exams with 16.2%
being PE-positive. The mean TAT for pre-AI and post-AI during work hours are
68.9 [95% CI" 55.0, 82.8] and 46.7 [38.1, 55.2] minutes respectively, and those
during off-hours are 44.8 [33.7, 55.9] and 42.0 [33.6, 50.3] minutes.
Clinically-observed time-savings during work hours (22.2 [95% CI: 5.85, 38.6]
minutes) were significant (p=0.004), while off-hour (2.82 [-11.1, 16.7]
minutes) were not (p=0.345). Observed time-savings aligned with model
predictions (29.6 [95% range: 23.2, 38.1] minutes for work hours; 2.10 [1.76,
2.58] minutes for off-hours). Discussion: Consideration and quantification of
clinical workflow contribute to an accurate assessment of the expected
time-savings in TAT following deployment of an AI-triage device.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [15] [Hive Hash Table: A Warp-Cooperative, Dynamically Resizable Hash Table for GPUs](https://arxiv.org/abs/2510.15095)
*Md Sabbir Hossain Polak,David Troendle,Byunghyun Jang*

Main category: cs.DC

TL;DR: Hive hash table 是一种高性能、支持动态调整大小的 GPU 哈希表，通过优化内存布局、减少原子操作冲突和高效的动态扩容策略，在高负载因子下仍能保持优异性能。


<details>
  <summary>Details</summary>
Motivation: 现有 GPU 哈希表在并发更新、高负载因子和不规则内存访问方面存在性能瓶颈，难以适应多变的工作负载。

Method: 提出 Hive 哈希表，采用缓存对齐的紧凑桶布局、warp 同步的无锁并发协议（WABC 和 WCME），以及基于线性哈希的动态扩容策略；同时引入四阶段插入机制处理高竞争场景。

Result: 在 NVIDIA RTX 4090 上，Hive 在混合操作负载下比现有先进方案（Slab-Hash、DyCuckoo、WarpCore）吞吐量高 1.5–2 倍，支持高达 95% 的负载因子，更新和查找分别达 35 亿次/秒和近 40 亿次/秒。

Conclusion: Hive 哈希表通过软硬件协同设计，在保证无锁快速路径和有界恢复开销的同时，显著提升了 GPU 哈希表在高并发和高负载下的性能与可扩展性。

Abstract: Hash tables are essential building blocks in data-intensive applications, yet
existing GPU implementations often struggle with concurrent updates, high load
factors, and irregular memory access patterns. We present Hive hash table, a
high-performance, warp-cooperative and dynamically resizable GPU hash table
that adapts to varying workloads without global rehashing.
  Hive hash table makes three key contributions. First, a cache-aligned packed
bucket layout stores key-value pairs as 64-bit words, enabling coalesced memory
access and atomic updates via single-CAS operations. Second, warp-synchronous
concurrency protocols - Warp-Aggregated-Bitmask-Claim (WABC) and
Warp-Cooperative Match-and-Elect (WCME) - reduce contention to one atomic
operation per warp while ensuring lock-free progress. Third, a
load-factor-aware dynamic resizing strategy expands or contracts capacity in
warp-parallel K-bucket batches using linear hashing, maintaining balanced
occupancy. To handle insertions under heavy contention, Hive hash table employs
a four-step strategy: replace, claim-and-commit, bounded cuckoo eviction, and
overflow-stash fallback. This design provides lock-free fast paths and bounded
recovery cost under contention determined by a fixed eviction depth, while
eliminating ABA hazards during concurrent updates.
  Experimental evaluation on an NVIDIA RTX 4090 shows Hive hash table sustains
load factors up to 95% while delivering 1.5-2x higher throughput than
state-of-the-art GPU hash tables (Slab-Hash, DyCuckoo, WarpCore) under mixed
insert-delete-lookup workloads. On balanced workload, Hive hash table reaches
3.5 billion updates/s and nearly 4 billion lookups/s, demonstrating scalability
and efficiency for GPU-accelerated data processing.

</details>


### [16] [NEMO: Faster Parallel Execution for Highly Contended Blockchain Workloads (Full version)](https://arxiv.org/abs/2510.15122)
*François Ezard,Can Umut Ileri,Jérémie Decouchant*

Main category: cs.DC

TL;DR: NEMO 是一种结合乐观并发控制（OCC）与对象数据模型的新型区块链执行引擎，通过四项核心创新显著提升高竞争负载下的吞吐量，相比 Block-STM 和悲观并发控制基线分别提升最多 42% 和 61%。


<details>
  <summary>Details</summary>
Motivation: 当前区块链执行层在高竞争负载下成为性能瓶颈，现有基于 OCC 或 PCC 的并行执行框架在此类场景中性能下降，亟需更高效的执行引擎。

Method: NEMO 引入四项创新：(i) 仅使用自有对象的事务采用贪心提交规则；(ii) 优化依赖处理以减少重执行；(iii) 利用静态可推导但不完整的读/写提示指导执行；(iv) 采用优先级调度器优先执行能解除其他事务阻塞的事务。

Result: 模拟实验表明，NEMO 显著减少了冗余计算，在 16 个工作线程下，吞吐量比 Block-STM 高最多 42%，比悲观并发控制基线高最多 61%。

Conclusion: NEMO 通过融合 OCC 与对象数据模型，并引入多项优化机制，有效缓解了高竞争场景下的执行瓶颈，显著优于现有方法。

Abstract: Following the design of more efficient blockchain consensus algorithms, the
execution layer has emerged as the new performance bottleneck of blockchains,
especially under high contention. Current parallel execution frameworks either
rely on optimistic concurrency control (OCC) or on pessimistic concurrency
control (PCC), both of which see their performance decrease when workloads are
highly contended, albeit for different reasons. In this work, we present NEMO,
a new blockchain execution engine that combines OCC with the object data model
to address this challenge. NEMO introduces four core innovations: (i) a greedy
commit rule for transactions using only owned objects; (ii) refined handling of
dependencies to reduce re-executions; (iii) the use of incomplete but
statically derivable read/write hints to guide execution; and (iv) a
priority-based scheduler that favors transactions that unblock others. Through
simulated execution experiments, we demonstrate that NEMO significantly reduces
redundant computation and achieves higher throughput than representative
approaches. For example, with 16 workers NEMO's throughput is up to 42% higher
than the one of Block-STM, the state-of-the-art OCC approach, and 61% higher
than the pessimistic concurrency control baseline used.

</details>


### [17] [An Elastic Job Scheduler for HPC Applications on the Cloud](https://arxiv.org/abs/2510.15147)
*Aditya Bhosale,Kavitha Chandrasekar,Laxmikant Kale,Sara Kokkila-Schumacher*

Main category: cs.DC

TL;DR: 本文提出了一种基于Kubernetes的弹性调度器，结合Charm++的动态扩缩容能力，显著提升了HPC作业在云环境中的资源利用率和高优先级任务的响应速度。


<details>
  <summary>Details</summary>
Motivation: 云环境中HPC应用日益普及，但传统并行编程模型（如MPI）缺乏对动态扩缩容的支持，难以高效利用按需付费的云资源，亟需支持自动伸缩的调度与编程模型。

Method: 利用Charm++原生支持动态重缩放的特性，开发了一个Kubernetes Operator来部署Charm++应用，并设计了一种基于优先级的弹性作业调度器，根据集群状态动态调整作业规模。

Result: 实验表明，该弹性调度器在最小化高优先级作业响应时间的同时，显著优于传统静态调度器，在资源利用率和性能方面均有明显提升。

Conclusion: 结合Charm++与Kubernetes的弹性调度方案能有效支持HPC作业在云环境中的动态扩缩容，为高效利用云资源提供了可行路径。

Abstract: The last few years have seen an increase in adoption of the cloud for running
HPC applications. The pay-as-you-go cost model of these cloud resources has
necessitated the development of specialized programming models and schedulers
for HPC jobs for efficient utilization of cloud resources. A key aspect of
efficient utilization is the ability to rescale applications on the fly to
maximize the utilization of cloud resources. Most commonly used parallel
programming models like MPI have traditionally not supported autoscaling either
in a cloud environment or on supercomputers. While more recent work has been
done to implement this functionality in MPI, it is still nascent and requires
additional programmer effort. Charm++ is a parallel programming model that
natively supports dynamic rescaling through its migratable objects paradigm. In
this paper, we present a Kubernetes operator to run Charm++ applications on a
Kubernetes cluster. We then present a priority-based elastic job scheduler that
can dynamically rescale jobs based on the state of a Kubernetes cluster to
maximize cluster utilization while minimizing response time for high-priority
jobs. We show that our elastic scheduler, with the ability to rescale HPC jobs
with minimal overhead, demonstrates significant performance improvements over
traditional static schedulers.

</details>


### [18] [BeLLMan: Controlling LLM Congestion](https://arxiv.org/abs/2510.15330)
*Tella Rajashekhar Reddy,Atharva Deshmukh,Karan Tandon,Rohan Gandhi,Anjaly Parayil,Debopam Bhattacherjee*

Main category: cs.DC

TL;DR: beLLMan 是一种控制器，通过动态调整大语言模型（LLM）输出长度以响应系统负载变化，在真实 H100 GPU 测试平台上显著降低端到端推理延迟（最高达8倍）、减少25%能耗，并在拥塞期间多处理19%的请求。


<details>
  <summary>Details</summary>
Motivation: 当前 LLM 应用对底层基础设施无感知，以自回归方式生成 token，不考虑系统负载，容易导致推理延迟增加和用户体验下降。

Method: 提出 beLLMan 控制器，使 LLM 基础设施能主动、渐进地向第一方 LLM 应用发送信号，动态调整输出长度以应对系统负载变化。

Result: 在 H100 GPU 真实测试平台上，beLLMan 在摘要任务负载下将端到端延迟最多降低8倍，能耗减少25%，并在系统拥塞期间多处理19%的请求。

Conclusion: beLLMan 有效提升了 LLM 推理系统的响应性与能效，证明了基础设施与应用协同优化在大模型部署中的重要性。

Abstract: Large language model (LLM) applications are blindfolded to the infrastructure
underneath and generate tokens autoregressively, indifferent to the system
load, thus risking inferencing latency inflation and poor user experience. Our
first-cut controller, named beLLMan, enables the LLM infrastructure to actively
and progressively signal the first-party LLM application to adjust the output
length in response to changing system load. On a real testbed with H100 GPUs,
beLLMan helps keep inferencing latency under control (upto 8X lower end-to-end
latency) and reduces energy consumption by 25% (while serving 19% more
requests) during periods of congestion for a summarization workload.

</details>


### [19] [Cloud-Enabled Virtual Prototypes](https://arxiv.org/abs/2510.15355)
*Tim Kraus,Axel Sauer,Ingo Feldner*

Main category: cs.DC

TL;DR: 本文探讨了本地与云端仿真环境在嵌入式AI开发中的权衡，重点关注可扩展性与数据隐私之间的平衡，并提出提升远程仿真可信度的方案。


<details>
  <summary>Details</summary>
Motivation: 嵌入式系统和AI算法日益复杂，亟需基于虚拟原型技术的高效软硬件协同设计方法；同时，远程计算资源的普及带来了新的基础设施选择，也引发了性能与安全方面的考量。

Method: 分析本地与云仿真环境的差异，评估计算基础设施设置对执行性能和数据安全的影响，并结合嵌入式AI开发流程，提出优化虚拟原型实践的策略。

Result: 明确了本地与云端仿真在可扩展性、隐私保护和执行效率方面的权衡关系，为提升远程仿真的可信度和推动虚拟原型技术应用提供了依据。

Conclusion: 通过合理选择和配置仿真基础设施，可以在保障数据安全的同时提升嵌入式AI算法的开发效率，从而增强对远程仿真的信任并促进虚拟原型方法的广泛应用。

Abstract: The rapid evolution of embedded systems, along with the growing variety and
complexity of AI algorithms, necessitates a powerful hardware/software
co-design methodology based on virtual prototyping technologies. The market
offers a diverse range of simulation solutions, each with its unique
technological approach and therefore strengths and weaknesses. Additionally,
with the increasing availability of remote on-demand computing resources and
their adaptation throughout the industry, the choice of the host infrastructure
for execution opens even more new possibilities for operational strategies.
This work explores the dichotomy between local and cloud-based simulation
environments, focusing on the trade-offs between scalability and privacy. We
discuss how the setup of the compute infrastructure impacts the performance of
the execution and security of data involved in the process. Furthermore, we
highlight the development workflow associated with embedded AI and the critical
role of efficient simulations in optimizing these algorithms. With the proposed
solution, we aim to sustainably improve trust in remote simulations and
facilitate the adoption of virtual prototyping practices.

</details>


### [20] [(Almost) Perfect Discrete Iterative Load Balancing](https://arxiv.org/abs/2510.15473)
*Petra Berenbrink,Robert Elsässer,Tom Friedetzky,Hamed Hosseinpour,Dominik Kaaser,Peter Kling,Thomas Sauerwald*

Main category: cs.DC

TL;DR: 本文研究了在任意图上通过匹配进行的离散迭代负载均衡问题，提出了一类通用的局部均衡策略，并证明该策略能以高概率在与连续负载均衡谱界相匹配的轮数内达到最大负载差为3的均衡状态。


<details>
  <summary>Details</summary>
Motivation: 负载均衡是分布式计算中的核心问题，现有工作多关注正则图或仅能保证较大的常数偏差，本文旨在将结果推广至任意图并实现更小且明确的常数偏差。

Method: 作者考虑一类基于匹配的局部离散负载均衡方案，包括匹配模型、平衡电路模型和异步模型，在每轮中对匹配节点的令牌进行平均分配（奇数时随机分配多余令牌），并通过分析负载向量的偏差（最大与最小负载之差）来评估性能。

Result: 在高概率下，所提出的离散均衡方案能在轮数上渐近匹配连续负载均衡的谱界，并达到偏差为3的均衡状态，适用于任意图而非仅限于正则图。

Conclusion: 该工作显著改进了以往结果，不仅实现了小且明确的常数偏差，还将适用范围扩展至任意图，表明在所考虑的通用模型中，离散负载均衡并不比连续情形更难。

Abstract: We consider discrete, iterative load balancing via matchings on arbitrary
graphs. Initially each node holds a certain number of tokens, defining the load
of the node, and the objective is to redistribute the tokens such that
eventually each node has approximately the same number of tokens. We present
results for a general class of simple local balancing schemes where the tokens
are balanced via matchings. In each round the process averages the tokens of
any two matched nodes. If the sum of their tokens is odd, the node to receive
the one excess token is selected at random. Our class covers three popular
models: in the matching model a new matching is generated randomly in each
round, in the balancing circuit model a fixed sequence of matchings is applied
periodically, and in the asynchronous model the load is balanced over a
randomly chosen edge.
  We measure the quality of a load vector by its discrepancy, defined as the
difference between the maximum and minimum load across all nodes. As our main
result we show that with high probability our discrete balancing scheme reaches
a discrepancy of $3$ in a number of rounds which asymptotically matches the
spectral bound for continuous load balancing with fractional load.
  This result improves and tightens a long line of previous works, by not only
achieving a small constant discrepancy (instead of a non-explicit, large
constant) but also holding for arbitrary instead of regular graphs. The result
also demonstrates that in the general model we consider, discrete load
balancing is no harder than continuous load balancing.

</details>


### [21] [Balancing Fairness and Performance in Multi-User Spark Workloads with Dynamic Scheduling (extended version)](https://arxiv.org/abs/2510.15485)
*Dāvis Kažemaks,Laurens Versluis,Burcu Kulahcioglu Ozkan,Jérémie Decouchant*

Main category: cs.DC

TL;DR: 本文提出了一种名为用户加权公平队列（UWFQ）的新型调度器，用于Apache Spark，旨在在多用户共享环境中同时实现用户级公平性和低平均响应时间。UWFQ通过模拟虚拟公平队列并基于估计完成时间调度作业，并结合运行时分区技术动态调整任务粒度以缓解任务倾斜和优先级反转问题。实验表明，UWFQ相比现有调度器可将小作业的平均响应时间最多降低74%。


<details>
  <summary>Details</summary>
Motivation: Apache Spark内置的调度器（如FIFO和公平调度）在工业分析环境中难以兼顾用户级公平性和低平均响应时间，尤其在长期运行的共享应用中。现有方案多关注作业级公平，无意中偏向提交更多作业的用户，而Spark的公平调度器缺乏对动态用户工作负载的适应性，可能降低整体性能。

Method: 提出用户加权公平队列（UWFQ）调度器，通过模拟虚拟公平队列并基于作业在有界公平模型下的估计完成时间进行调度；同时引入运行时分区方法，根据预期运行时间动态细化任务粒度，以应对任务倾斜和优先级反转。

Result: 在Spark框架中实现UWFQ，并使用多用户合成工作负载和Google集群跟踪数据进行评估，结果表明UWFQ相比现有内置调度器和先进公平调度算法，可将小作业的平均响应时间最多减少74%。

Conclusion: UWFQ调度器有效解决了Spark在多用户共享环境中用户级公平性与低响应时间难以兼顾的问题，显著提升了小作业的响应性能，同时保障了用户间资源分配的公平性。

Abstract: Apache Spark is a widely adopted framework for large-scale data processing.
However, in industrial analytics environments, Spark's built-in schedulers,
such as FIFO and fair scheduling, struggle to maintain both user-level fairness
and low mean response time, particularly in long-running shared applications.
Existing solutions typically focus on job-level fairness which unintentionally
favors users who submit more jobs. Although Spark offers a built-in fair
scheduler, it lacks adaptability to dynamic user workloads and may degrade
overall job performance. We present the User Weighted Fair Queuing (UWFQ)
scheduler, designed to minimize job response times while ensuring equitable
resource distribution across users and their respective jobs. UWFQ simulates a
virtual fair queuing system and schedules jobs based on their estimated finish
times under a bounded fairness model. To further address task skew and reduce
priority inversions, which are common in Spark workloads, we introduce runtime
partitioning, a method that dynamically refines task granularity based on
expected runtime. We implement UWFQ within the Spark framework and evaluate its
performance using multi-user synthetic workloads and Google cluster traces. We
show that UWFQ reduces the average response time of small jobs by up to 74%
compared to existing built-in Spark schedulers and to state-of-the-art fair
scheduling algorithms.

</details>


### [22] [A Post-Quantum Lower Bound for the Distributed Lovász Local Lemma](https://arxiv.org/abs/2510.15698)
*Sebastian Brandt,Tim Göttlicher*

Main category: cs.DC

TL;DR: 本文在分布式量子计算背景下研究Lovász局部引理（LLL）问题，证明了在quantum-LOCAL模型中LLL问题的复杂度下界为$2^{\Omega(\log^* n)}$，该下界甚至在更强的randomized online-LOCAL模型中对sinkless orientation这一经典特例也成立，并由此推广到多个相关计算模型，首次给出了这些模型中超常数的下界。


<details>
  <summary>Details</summary>
Motivation: 近期量子计算的发展推动了对分布式量子计算中Lovász局部引理（LLL）问题的研究，而此前在多个模型中尚缺乏对LLL及其特例sinkless orientation的超常数下界结果，本文旨在填补这一空白并回应近期提出的开放问题。

Method: 作者提出了一种全新的下界证明技术，在randomized online-LOCAL模型中针对sinkless orientation问题建立$2^{\Omega(\log^* n)}$的复杂度下界，并将该结果推广至quantum-LOCAL及其他多种分布式计算模型。

Result: 获得了sinkless orientation和分布式LLL问题在多种模型中的首个超常数下界$2^{\Omega(\log^* n)}$，并提出了一种有望用于证明更多局部性相关问题后量子下界的新技术。

Conclusion: 本文不仅解决了多个模型中LLL与sinkless orientation问题的下界开放问题，还引入了一种具有广泛应用潜力的新型下界证明方法，为后量子分布式计算复杂性研究提供了新工具。

Abstract: In this work, we study the Lov\'asz local lemma (LLL) problem in the area of
distributed quantum computing, which has been the focus of attention of recent
advances in quantum computing [STOC'24, STOC'25, STOC'25]. We prove a lower
bound of $2^{\Omega(\log^* n)}$ for the complexity of the distributed LLL in
the quantum-LOCAL model. More specifically, we obtain our lower bound already
for a very well-studied special case of the LLL, called sinkless orientation,
in a stronger model than quantum-LOCAL, called the randomized online-LOCAL
model. As a consequence, we obtain the same lower bounds for sinkless
orientation and the distributed LLL also in a variety of other models studied
across different research communities.
  Our work provides the first superconstant lower bound for sinkless
orientation and the distributed LLL in all of these models, addressing recently
stated open questions. Moreover, to obtain our results, we develop an entirely
new lower bound technique that we believe has the potential to become the first
generic technique for proving post-quantum lower bounds for many of the most
important problems studied in the context of locality.

</details>


### [23] [Funky: Cloud-Native FPGA Virtualization and Orchestration](https://arxiv.org/abs/2510.15755)
*Atsushi Koshiba,Charalampos Mainas,Pramod Bhatotia*

Main category: cs.DC

TL;DR: 本文提出了Funky，一种面向云原生环境的全栈FPGA感知编排引擎，通过FPGA虚拟化、状态管理和兼容CRI/OCI标准的编排组件，解决了当前云环境中FPGA缺乏虚拟化、隔离和抢占支持的问题。


<details>
  <summary>Details</summary>
Motivation: 当前云原生环境中FPGA的使用受限，主要因为FPGA自身限制以及现有编排器以CPU为中心的设计，缺乏对FPGA的虚拟化、隔离和抢占支持，导致云服务商无法提供FPGA编排服务，影响了系统的可扩展性、灵活性和弹性。

Method: Funky通过三项核心技术实现FPGA感知编排：(1) 支持轻量级沙箱的FPGA虚拟化；(2) 支持任务抢占和检查点的FPGA状态管理；(3) 遵循CRI/OCI行业标准的FPGA感知编排组件。

Result: 在四台配备Alveo U50 FPGA卡的x86服务器上实现和评估表明：Funky仅需修改3.4%的源代码即可移植23个OpenCL应用，OCI镜像体积比AMD方案小28.7倍；相比原生执行仅引入7.4%性能开销，并提供强隔离的虚拟化支持；在基于Google生产轨迹的大规模集群中验证了其可扩展性、容错性和调度效率。

Conclusion: Funky有效解决了云原生环境中FPGA编排的关键挑战，显著提升了FPGA资源的性能、利用率、可扩展性和容错能力，为FPGA在云环境中的广泛应用提供了可行路径。

Abstract: The adoption of FPGAs in cloud-native environments is facing impediments due
to FPGA limitations and CPU-oriented design of orchestrators, as they lack
virtualization, isolation, and preemption support for FPGAs. Consequently,
cloud providers offer no orchestration services for FPGAs, leading to low
scalability, flexibility, and resiliency.
  This paper presents Funky, a full-stack FPGA-aware orchestration engine for
cloud-native applications. Funky offers primary orchestration services for FPGA
workloads to achieve high performance, utilization, scalability, and fault
tolerance, accomplished by three contributions: (1) FPGA virtualization for
lightweight sandboxes, (2) FPGA state management enabling task preemption and
checkpointing, and (3) FPGA-aware orchestration components following the
industry-standard CRI/OCI specifications.
  We implement and evaluate Funky using four x86 servers with Alveo U50 FPGA
cards. Our evaluation highlights that Funky allows us to port 23 OpenCL
applications from the Xilinx Vitis and Rosetta benchmark suites by modifying
3.4% of the source code while keeping the OCI image sizes 28.7 times smaller
than AMD's FPGA-accessible Docker containers. In addition, Funky incurs only
7.4% performance overheads compared to native execution, while providing
virtualization support with strong hypervisor-enforced isolation and
cloud-native orchestration for a set of distributed FPGAs. Lastly, we evaluate
Funky's orchestration services in a large-scale cluster using Google production
traces, showing its scalability, fault tolerance, and scheduling efficiency.

</details>
