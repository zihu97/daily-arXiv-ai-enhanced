<div id=toc></div>

# Table of Contents

- [cs.DC](#cs.DC) [Total: 9]
- [cs.SE](#cs.SE) [Total: 12]
- [cs.NI](#cs.NI) [Total: 1]
- [cs.AR](#cs.AR) [Total: 1]
- [cs.PF](#cs.PF) [Total: 1]


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [1] [Hive Hash Table: A Warp-Cooperative, Dynamically Resizable Hash Table for GPUs](https://arxiv.org/abs/2510.15095)
*Md Sabbir Hossain Polak,David Troendle,Byunghyun Jang*

Main category: cs.DC

TL;DR: 本文提出了Hive哈希表，一种高性能、线程束协同且动态可调整大小的GPU哈希表，在高负载因子和高并发下仍能保持高效性能，无需全局重哈希。


<details>
  <summary>Details</summary>
Motivation: 现有GPU哈希表在并发更新、高负载因子和不规则内存访问方面存在性能瓶颈，难以适应多变的工作负载。

Method: Hive哈希表采用三项关键技术：1）缓存对齐的紧凑桶布局，以64位字存储键值对，支持合并内存访问和单CAS原子更新；2）线程束同步并发协议（WABC和WCME），每线程束仅需一次原子操作并保证无锁进展；3）基于负载因子的动态扩容策略，利用线性哈希以线程束并行方式按K桶批次调整容量。此外，通过四步插入策略（替换、声明提交、有界布谷鸟驱逐、溢出暂存回退）处理高竞争场景。

Result: 在NVIDIA RTX 4090上，Hive哈希表在混合插入-删除-查找负载下支持高达95%的负载因子，吞吐量比当前最先进的GPU哈希表（Slab-Hash、DyCuckoo、WarpCore）高1.5–2倍；在均衡负载下，更新速度达35亿次/秒，查找接近40亿次/秒。

Conclusion: Hive哈希表通过创新的内存布局、并发协议和动态调整机制，在GPU上实现了高负载因子下的高性能与可扩展性，显著优于现有方案，适用于GPU加速的数据密集型应用。

Abstract: Hash tables are essential building blocks in data-intensive applications, yet
existing GPU implementations often struggle with concurrent updates, high load
factors, and irregular memory access patterns. We present Hive hash table, a
high-performance, warp-cooperative and dynamically resizable GPU hash table
that adapts to varying workloads without global rehashing.
  Hive hash table makes three key contributions. First, a cache-aligned packed
bucket layout stores key-value pairs as 64-bit words, enabling coalesced memory
access and atomic updates via single-CAS operations. Second, warp-synchronous
concurrency protocols - Warp-Aggregated-Bitmask-Claim (WABC) and
Warp-Cooperative Match-and-Elect (WCME) - reduce contention to one atomic
operation per warp while ensuring lock-free progress. Third, a
load-factor-aware dynamic resizing strategy expands or contracts capacity in
warp-parallel K-bucket batches using linear hashing, maintaining balanced
occupancy. To handle insertions under heavy contention, Hive hash table employs
a four-step strategy: replace, claim-and-commit, bounded cuckoo eviction, and
overflow-stash fallback. This design provides lock-free fast paths and bounded
recovery cost under contention determined by a fixed eviction depth, while
eliminating ABA hazards during concurrent updates.
  Experimental evaluation on an NVIDIA RTX 4090 shows Hive hash table sustains
load factors up to 95% while delivering 1.5-2x higher throughput than
state-of-the-art GPU hash tables (Slab-Hash, DyCuckoo, WarpCore) under mixed
insert-delete-lookup workloads. On balanced workload, Hive hash table reaches
3.5 billion updates/s and nearly 4 billion lookups/s, demonstrating scalability
and efficiency for GPU-accelerated data processing.

</details>


### [2] [NEMO: Faster Parallel Execution for Highly Contended Blockchain Workloads (Full version)](https://arxiv.org/abs/2510.15122)
*François Ezard,Can Umut Ileri,Jérémie Decouchant*

Main category: cs.DC

TL;DR: NEMO 是一种结合乐观并发控制（OCC）与对象数据模型的新型区块链执行引擎，通过四项核心创新显著提升高竞争负载下的吞吐量，相比现有方法如 Block-STM 和悲观并发控制基线分别提升最高 42% 和 61%。


<details>
  <summary>Details</summary>
Motivation: 当前区块链的执行层在高竞争场景下成为性能瓶颈，而现有的乐观（OCC）和悲观（PCC）并发控制方法在高竞争工作负载下性能均显著下降，亟需更高效的执行引擎。

Method: NEMO 引入四大创新：(i) 仅使用自有对象的事务采用贪心提交规则；(ii) 优化依赖处理以减少重执行；(iii) 利用静态可推导但不完整的读/写提示指导执行；(iv) 采用优先级调度器优先执行能解除其他事务阻塞的事务。

Result: 模拟实验表明，NEMO 显著减少了冗余计算，在 16 个工作线程下，吞吐量比 Block-STM 高出最多 42%，比悲观并发控制基线高出最多 61%。

Conclusion: NEMO 有效解决了高竞争场景下区块链执行层的性能瓶颈问题，通过结合 OCC 与对象模型及四项关键技术，显著优于现有并发控制方法。

Abstract: Following the design of more efficient blockchain consensus algorithms, the
execution layer has emerged as the new performance bottleneck of blockchains,
especially under high contention. Current parallel execution frameworks either
rely on optimistic concurrency control (OCC) or on pessimistic concurrency
control (PCC), both of which see their performance decrease when workloads are
highly contended, albeit for different reasons. In this work, we present NEMO,
a new blockchain execution engine that combines OCC with the object data model
to address this challenge. NEMO introduces four core innovations: (i) a greedy
commit rule for transactions using only owned objects; (ii) refined handling of
dependencies to reduce re-executions; (iii) the use of incomplete but
statically derivable read/write hints to guide execution; and (iv) a
priority-based scheduler that favors transactions that unblock others. Through
simulated execution experiments, we demonstrate that NEMO significantly reduces
redundant computation and achieves higher throughput than representative
approaches. For example, with 16 workers NEMO's throughput is up to 42% higher
than the one of Block-STM, the state-of-the-art OCC approach, and 61% higher
than the pessimistic concurrency control baseline used.

</details>


### [3] [An Elastic Job Scheduler for HPC Applications on the Cloud](https://arxiv.org/abs/2510.15147)
*Aditya Bhosale,Kavitha Chandrasekar,Laxmikant Kale,Sara Kokkila-Schumacher*

Main category: cs.DC

TL;DR: 本文提出了一种基于Kubernetes的弹性调度器，结合Charm++的动态重缩放能力，实现高性能计算（HPC）作业在云环境中的高效资源利用和低响应延迟。


<details>
  <summary>Details</summary>
Motivation: 传统MPI等并行编程模型缺乏对动态扩缩容的支持，难以高效利用云资源；而云环境中按需付费的模式要求HPC应用具备弹性调度能力以提升资源利用率。

Method: 利用Charm++原生支持动态重缩放的特性，开发了一个Kubernetes Operator来部署Charm++应用，并设计了一种基于优先级的弹性作业调度器，根据集群状态动态调整作业规模。

Result: 实验表明，所提出的弹性调度器在最小化高优先级作业响应时间的同时，显著优于传统静态调度器，在资源利用率和性能方面均有明显提升。

Conclusion: 结合Charm++与Kubernetes的弹性调度方案能有效支持HPC应用在云环境中的动态资源调整，为高效、灵活的云上HPC提供了可行路径。

Abstract: The last few years have seen an increase in adoption of the cloud for running
HPC applications. The pay-as-you-go cost model of these cloud resources has
necessitated the development of specialized programming models and schedulers
for HPC jobs for efficient utilization of cloud resources. A key aspect of
efficient utilization is the ability to rescale applications on the fly to
maximize the utilization of cloud resources. Most commonly used parallel
programming models like MPI have traditionally not supported autoscaling either
in a cloud environment or on supercomputers. While more recent work has been
done to implement this functionality in MPI, it is still nascent and requires
additional programmer effort. Charm++ is a parallel programming model that
natively supports dynamic rescaling through its migratable objects paradigm. In
this paper, we present a Kubernetes operator to run Charm++ applications on a
Kubernetes cluster. We then present a priority-based elastic job scheduler that
can dynamically rescale jobs based on the state of a Kubernetes cluster to
maximize cluster utilization while minimizing response time for high-priority
jobs. We show that our elastic scheduler, with the ability to rescale HPC jobs
with minimal overhead, demonstrates significant performance improvements over
traditional static schedulers.

</details>


### [4] [BeLLMan: Controlling LLM Congestion](https://arxiv.org/abs/2510.15330)
*Tella Rajashekhar Reddy,Atharva Deshmukh,Karan Tandon,Rohan Gandhi,Anjaly Parayil,Debopam Bhattacherjee*

Main category: cs.DC

TL;DR: beLLMan is a controller that enables LLM applications to dynamically adjust output length based on system load, significantly reducing latency and energy consumption while increasing request throughput.


<details>
  <summary>Details</summary>
Motivation: LLM applications currently generate tokens without considering underlying system load, leading to high inference latency and poor user experience during congestion.

Method: The authors propose beLLMan, a controller that actively signals the LLM application to adjust output length in response to real-time system load changes.

Result: On an H100 GPU testbed, beLLMan reduces end-to-end latency by up to 8×, cuts energy consumption by 25%, and serves 19% more requests during congestion for a summarization task.

Conclusion: Dynamic output-length adaptation guided by infrastructure feedback (via beLLMan) effectively mitigates latency and energy inefficiencies in LLM inference under load.

Abstract: Large language model (LLM) applications are blindfolded to the infrastructure
underneath and generate tokens autoregressively, indifferent to the system
load, thus risking inferencing latency inflation and poor user experience. Our
first-cut controller, named beLLMan, enables the LLM infrastructure to actively
and progressively signal the first-party LLM application to adjust the output
length in response to changing system load. On a real testbed with H100 GPUs,
beLLMan helps keep inferencing latency under control (upto 8X lower end-to-end
latency) and reduces energy consumption by 25% (while serving 19% more
requests) during periods of congestion for a summarization workload.

</details>


### [5] [Cloud-Enabled Virtual Prototypes](https://arxiv.org/abs/2510.15355)
*Tim Kraus,Axel Sauer,Ingo Feldner*

Main category: cs.DC

TL;DR: 本文探讨了本地与云端仿真环境在嵌入式AI开发中的权衡，重点关注可扩展性与数据隐私之间的平衡，并提出提升远程仿真可信度的方案。


<details>
  <summary>Details</summary>
Motivation: 嵌入式系统和AI算法日益复杂，需要高效的软硬件协同设计方法，而虚拟原型技术是关键；同时，随着远程计算资源普及，如何选择合适的仿真基础设施成为重要问题。

Method: 分析本地与云仿真环境在性能、数据安全及开发流程中的差异，评估计算基础设施设置对仿真执行效率与数据安全的影响。

Result: 明确了本地与云端仿真在可扩展性与隐私保护方面的优劣，强调高效仿真对嵌入式AI算法优化的关键作用。

Conclusion: 通过合理选择和配置仿真基础设施，可提升远程仿真的可信度，推动虚拟原型技术在嵌入式AI开发中的可持续应用。

Abstract: The rapid evolution of embedded systems, along with the growing variety and
complexity of AI algorithms, necessitates a powerful hardware/software
co-design methodology based on virtual prototyping technologies. The market
offers a diverse range of simulation solutions, each with its unique
technological approach and therefore strengths and weaknesses. Additionally,
with the increasing availability of remote on-demand computing resources and
their adaptation throughout the industry, the choice of the host infrastructure
for execution opens even more new possibilities for operational strategies.
This work explores the dichotomy between local and cloud-based simulation
environments, focusing on the trade-offs between scalability and privacy. We
discuss how the setup of the compute infrastructure impacts the performance of
the execution and security of data involved in the process. Furthermore, we
highlight the development workflow associated with embedded AI and the critical
role of efficient simulations in optimizing these algorithms. With the proposed
solution, we aim to sustainably improve trust in remote simulations and
facilitate the adoption of virtual prototyping practices.

</details>


### [6] [(Almost) Perfect Discrete Iterative Load Balancing](https://arxiv.org/abs/2510.15473)
*Petra Berenbrink,Robert Elsässer,Tom Friedetzky,Hamed Hosseinpour,Dominik Kaaser,Peter Kling,Thomas Sauerwald*

Main category: cs.DC

TL;DR: 该论文研究了在任意图上通过匹配进行离散迭代负载均衡的问题，提出了一类通用的局部均衡方案，并证明该方案在高概率下可在与连续负载均衡谱界相匹配的轮数内达到最大与最小负载差值为3的均衡状态。


<details>
  <summary>Details</summary>
Motivation: 负载均衡是分布式计算中的基本问题，现有工作多关注正则图或仅能保证较大的常数偏差，本文旨在设计适用于任意图且能达到小常数偏差的离散负载均衡算法。

Method: 作者考虑一类基于匹配的局部均衡机制，每轮对匹配节点的令牌数取平均（奇数时随机分配余数），涵盖匹配模型、平衡电路模型和异步模型三种经典设定，并通过分析负载向量的偏差（最大与最小负载之差）来评估性能。

Result: 在高概率下，所提出的离散均衡方案可在轮数上渐近匹配连续负载均衡的谱界，并达到偏差为3的均衡状态，优于以往工作且适用于任意图。

Conclusion: 该研究证明在所考虑的通用模型中，离散负载均衡并不比连续负载均衡更难，且首次在任意图上实现了小常数偏差的高效均衡。

Abstract: We consider discrete, iterative load balancing via matchings on arbitrary
graphs. Initially each node holds a certain number of tokens, defining the load
of the node, and the objective is to redistribute the tokens such that
eventually each node has approximately the same number of tokens. We present
results for a general class of simple local balancing schemes where the tokens
are balanced via matchings. In each round the process averages the tokens of
any two matched nodes. If the sum of their tokens is odd, the node to receive
the one excess token is selected at random. Our class covers three popular
models: in the matching model a new matching is generated randomly in each
round, in the balancing circuit model a fixed sequence of matchings is applied
periodically, and in the asynchronous model the load is balanced over a
randomly chosen edge.
  We measure the quality of a load vector by its discrepancy, defined as the
difference between the maximum and minimum load across all nodes. As our main
result we show that with high probability our discrete balancing scheme reaches
a discrepancy of $3$ in a number of rounds which asymptotically matches the
spectral bound for continuous load balancing with fractional load.
  This result improves and tightens a long line of previous works, by not only
achieving a small constant discrepancy (instead of a non-explicit, large
constant) but also holding for arbitrary instead of regular graphs. The result
also demonstrates that in the general model we consider, discrete load
balancing is no harder than continuous load balancing.

</details>


### [7] [Balancing Fairness and Performance in Multi-User Spark Workloads with Dynamic Scheduling (extended version)](https://arxiv.org/abs/2510.15485)
*Dāvis Kažemaks,Laurens Versluis,Burcu Kulahcioglu Ozkan,Jérémie Decouchant*

Main category: cs.DC

TL;DR: 本文提出了一种名为用户加权公平队列（UWFQ）的新型调度器，用于Apache Spark，旨在在多用户共享环境中同时实现用户级公平性和低平均响应时间。UWFQ通过模拟虚拟公平队列并基于估计完成时间调度作业，结合运行时动态分区策略缓解任务倾斜和优先级反转问题，在实验中将小作业的平均响应时间最多降低了74%。


<details>
  <summary>Details</summary>
Motivation: 现有Spark内置调度器（如FIFO和公平调度）在工业分析环境中难以兼顾用户级公平性与低平均响应时间，尤其在长期运行的共享应用中。当前方案多关注作业级公平，无意中偏向提交更多作业的用户，且缺乏对动态用户负载的适应性。

Method: 提出用户加权公平队列（UWFQ）调度器，模拟虚拟公平队列，基于作业在有界公平模型下的估计完成时间进行调度；同时引入运行时分区方法，根据任务预期运行时间动态调整任务粒度，以缓解任务倾斜和优先级反转。

Result: 在Spark框架中实现UWFQ，并通过多用户合成工作负载和Google集群轨迹进行评估，结果表明UWFQ相比现有Spark内置调度器和先进公平调度算法，最多可将小作业的平均响应时间降低74%。

Conclusion: UWFQ有效解决了Spark在多用户共享场景下公平性与响应时间之间的权衡问题，显著提升了小作业的响应性能，同时保障了用户间的资源公平分配。

Abstract: Apache Spark is a widely adopted framework for large-scale data processing.
However, in industrial analytics environments, Spark's built-in schedulers,
such as FIFO and fair scheduling, struggle to maintain both user-level fairness
and low mean response time, particularly in long-running shared applications.
Existing solutions typically focus on job-level fairness which unintentionally
favors users who submit more jobs. Although Spark offers a built-in fair
scheduler, it lacks adaptability to dynamic user workloads and may degrade
overall job performance. We present the User Weighted Fair Queuing (UWFQ)
scheduler, designed to minimize job response times while ensuring equitable
resource distribution across users and their respective jobs. UWFQ simulates a
virtual fair queuing system and schedules jobs based on their estimated finish
times under a bounded fairness model. To further address task skew and reduce
priority inversions, which are common in Spark workloads, we introduce runtime
partitioning, a method that dynamically refines task granularity based on
expected runtime. We implement UWFQ within the Spark framework and evaluate its
performance using multi-user synthetic workloads and Google cluster traces. We
show that UWFQ reduces the average response time of small jobs by up to 74%
compared to existing built-in Spark schedulers and to state-of-the-art fair
scheduling algorithms.

</details>


### [8] [A Post-Quantum Lower Bound for the Distributed Lovász Local Lemma](https://arxiv.org/abs/2510.15698)
*Sebastian Brandt,Tim Göttlicher*

Main category: cs.DC

TL;DR: 本文在分布式量子计算背景下研究了Lovász局部引理（LLL）问题，证明了在量子-LOCAL模型中分布式LLL的复杂度下界为 $2^{\Omega(\log^* n)}$，该下界甚至适用于更强的随机在线-LOCAL模型中的sinkless orientation问题，并为多个相关模型提供了首个超常数下界，同时提出了一种全新的下界证明技术。


<details>
  <summary>Details</summary>
Motivation: 近期量子计算的进展促使研究者关注分布式量子计算中的Lovász局部引理（LLL）问题，而此前在多种模型中缺乏对LLL及其特例sinkless orientation的超常数下界结果，因此本文旨在填补这一理论空白并回应近期提出的开放问题。

Method: 作者通过在比量子-LOCAL更强的随机在线-LOCAL模型中分析sinkless orientation这一经典LLL特例，构建了一个全新的下界证明技术，从而推导出分布式LLL在多种模型中的复杂度下界。

Result: 证明了在量子-LOCAL模型及更广泛的模型中，sinkless orientation和分布式LLL的复杂度下界为 $2^{\Omega(\log^* n)}$，这是这些模型中首个超常数下界。

Conclusion: 本文不仅解决了关于分布式LLL和sinkless orientation下界的开放问题，还提出了一种有潜力成为通用方法的新下界技术，可用于未来众多局部性相关问题的后量子下界研究。

Abstract: In this work, we study the Lov\'asz local lemma (LLL) problem in the area of
distributed quantum computing, which has been the focus of attention of recent
advances in quantum computing [STOC'24, STOC'25, STOC'25]. We prove a lower
bound of $2^{\Omega(\log^* n)}$ for the complexity of the distributed LLL in
the quantum-LOCAL model. More specifically, we obtain our lower bound already
for a very well-studied special case of the LLL, called sinkless orientation,
in a stronger model than quantum-LOCAL, called the randomized online-LOCAL
model. As a consequence, we obtain the same lower bounds for sinkless
orientation and the distributed LLL also in a variety of other models studied
across different research communities.
  Our work provides the first superconstant lower bound for sinkless
orientation and the distributed LLL in all of these models, addressing recently
stated open questions. Moreover, to obtain our results, we develop an entirely
new lower bound technique that we believe has the potential to become the first
generic technique for proving post-quantum lower bounds for many of the most
important problems studied in the context of locality.

</details>


### [9] [Funky: Cloud-Native FPGA Virtualization and Orchestration](https://arxiv.org/abs/2510.15755)
*Atsushi Koshiba,Charalampos Mainas,Pramod Bhatotia*

Main category: cs.DC

TL;DR: 本文提出了Funky，一种面向云原生环境的全栈FPGA感知编排引擎，通过FPGA虚拟化、状态管理和符合CRI/OCI标准的编排组件，解决了当前云环境中FPGA缺乏虚拟化、隔离和抢占支持的问题。


<details>
  <summary>Details</summary>
Motivation: 由于FPGA自身限制以及现有编排器以CPU为中心的设计，当前云环境缺乏对FPGA的虚拟化、隔离和抢占支持，导致云服务商无法提供FPGA编排服务，限制了FPGA在云原生场景中的可扩展性、灵活性和弹性。

Method: Funky通过三项关键技术实现FPGA感知编排：(1) 轻量级沙箱的FPGA虚拟化；(2) 支持任务抢占和检查点的FPGA状态管理；(3) 遵循CRI/OCI行业标准的FPGA感知编排组件。

Result: 在四台配备Alveo U50 FPGA卡的x86服务器上评估表明，Funky仅需修改3.4%的源代码即可移植23个OpenCL应用，OCI镜像体积比AMD方案小28.7倍，性能开销仅7.4%，并支持强隔离虚拟化和分布式FPGA编排。基于Google生产追踪的大规模集群实验验证了其可扩展性、容错性和调度效率。

Conclusion: Funky有效解决了云原生环境中FPGA编排的关键挑战，在保持接近原生性能的同时，提供了轻量、安全、可扩展的FPGA资源管理能力，为FPGA在云中的广泛应用奠定了基础。

Abstract: The adoption of FPGAs in cloud-native environments is facing impediments due
to FPGA limitations and CPU-oriented design of orchestrators, as they lack
virtualization, isolation, and preemption support for FPGAs. Consequently,
cloud providers offer no orchestration services for FPGAs, leading to low
scalability, flexibility, and resiliency.
  This paper presents Funky, a full-stack FPGA-aware orchestration engine for
cloud-native applications. Funky offers primary orchestration services for FPGA
workloads to achieve high performance, utilization, scalability, and fault
tolerance, accomplished by three contributions: (1) FPGA virtualization for
lightweight sandboxes, (2) FPGA state management enabling task preemption and
checkpointing, and (3) FPGA-aware orchestration components following the
industry-standard CRI/OCI specifications.
  We implement and evaluate Funky using four x86 servers with Alveo U50 FPGA
cards. Our evaluation highlights that Funky allows us to port 23 OpenCL
applications from the Xilinx Vitis and Rosetta benchmark suites by modifying
3.4% of the source code while keeping the OCI image sizes 28.7 times smaller
than AMD's FPGA-accessible Docker containers. In addition, Funky incurs only
7.4% performance overheads compared to native execution, while providing
virtualization support with strong hypervisor-enforced isolation and
cloud-native orchestration for a set of distributed FPGAs. Lastly, we evaluate
Funky's orchestration services in a large-scale cluster using Google production
traces, showing its scalability, fault tolerance, and scheduling efficiency.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [10] [Automated Snippet-Alignment Data Augmentation for Code Translation](https://arxiv.org/abs/2510.15004)
*Zhiming Zhang,Qingfu Zhu,Xianzhen Luo,Yixuan Wang,Bohan Li,Wanxiang Che*

Main category: cs.SE

TL;DR: 本文提出一种利用大语言模型自动生成代码片段对齐（SA）数据的数据增强方法，并结合程序对齐（PA）数据，采用两阶段训练策略提升代码翻译性能，在TransCoder-test上最高提升3.78%的pass@k指标。


<details>
  <summary>Details</summary>
Motivation: 现有代码翻译研究主要依赖程序对齐（PA）数据进行数据增强，但PA数据因长度较长难以提供细粒度的训练信号；而片段对齐（SA）数据虽适合细粒度学习，却因稀缺而未被充分利用。因此，作者希望利用大语言模型自动生成SA数据，并结合PA与SA数据提升模型性能。

Method: 利用大语言模型（LLMs）自动生成片段对齐（SA）的平行语料，并设计一种两阶段训练策略：先在PA数据上预训练，再在增强后的SA数据上微调，以兼顾语义对齐与细粒度学习。

Result: 在TransCoder-test数据集上的实验表明，所提出的SA数据增强方法结合两阶段训练策略，相比仅使用PA数据微调的基线模型，在pass@k指标上最高提升3.78%。

Conclusion: 结合自动生成的SA数据与两阶段训练策略能有效提升代码翻译模型的性能，验证了细粒度对齐数据在代码翻译任务中的价值。

Abstract: Code translation aims to translate the code from its source language to the
target language and is used in various software development scenarios. Recent
developments in Large Language Models (LLMs) have showcased their capabilities
in code translation, and parallel corpora play a crucial role in training
models for code translation. Parallel corpora can be categorized into
program-alignment (PA) and snippet-alignment (SA) data. Although PA data has
complete context and is suitable for semantic alignment learning, it may not
provide adequate fine-grained training signals due to its extended length,
while the brevity of SA data enables more fine-grained alignment learning. Due
to limited parallel corpora, researchers explore several augmentation methods
for code translation. Previous studies mainly focus on augmenting PA data. In
this paper, we propose a data augmentation method that leverages LLMs to
generate SA data automatically. To fully leverage both PA data and SA data, we
explore a simple yet effective two-stage training strategy, which consistently
enhances model performance compared to fine-tuning solely on PA data.
Experiments on TransCoder-test demonstrate that our augmented SA data combined
with the two-stage training approach yields consistent improvements over the
baseline, achieving a maximum gain of 3.78% on pass@k.

</details>


### [11] [An Experimental Study of Real-Life LLM-Proposed Performance Improvements](https://arxiv.org/abs/2510.15494)
*Lirong Yi,Gregory Gay,Philipp Leitner*

Main category: cs.SE

TL;DR: 本文研究大语言模型（LLMs）是否能生成高效的代码，通过对65个真实Java优化任务的实验发现，LLM生成的代码通常优于原始基线，但显著逊色于人类开发者编写的优化代码；约三分之二的LLM方案与人类思路相似，其余为原创但较少带来显著性能提升。


<details>
  <summary>Details</summary>
Motivation: 探究大语言模型在生成高性能代码方面的能力，特别是在真实世界中开发者已实现显著加速的场景下，LLM是否能复现或超越人类优化效果。

Method: 从开源Java项目中收集65个真实性能优化任务，使用两种主流LLM在四种提示策略下自动生成补丁，并通过严格基准测试将其与原始代码和人类优化方案进行性能对比。

Result: LLM生成的代码在大多数情况下优于基线，但人类开发者方案在性能上显著优于LLM；约66%的LLM方案与人类优化思路语义相似，其余为原创但仅偶尔带来显著性能提升。

Conclusion: 尽管LLM能在一定程度上生成性能改进的代码，但其在发现真正最优解方面仍明显落后于人类开发者，原创优化思路的有效性也较为有限。

Abstract: Large Language Models (LLMs) can generate code, but can they generate fast
code? In this paper, we study this question using a dataset of 65 real-world
tasks mined from open-source Java programs. We specifically select tasks where
developers achieved significant speedups, and employ an automated pipeline to
generate patches for these issues using two leading LLMs under four prompt
variations. By rigorously benchmarking the results against the baseline and
human-authored solutions, we demonstrate that LLM-generated code indeed
improves performance over the baseline in most cases. However, patches proposed
by human developers outperform LLM fixes by a statistically significant margin,
indicating that LLMs often fall short of finding truly optimal solutions. We
further find that LLM solutions are semantically identical or similar to the
developer optimization idea in approximately two-thirds of cases, whereas they
propose a more original idea in the remaining one-third. However, these
original ideas only occasionally yield substantial performance gains.

</details>


### [12] [Assessing Coherency and Consistency of Code Execution Reasoning by Large Language Models](https://arxiv.org/abs/2510.15079)
*Changshu Liu,Yang Chen,Reyhaneh Jabbarvand*

Main category: cs.SE

TL;DR: 本文提出了CES任务，用于评估大语言模型（LLMs）在模拟程序执行及其在编程任务中推理能力的表现，引入“一致性”概念和新的推理一致性度量标准，发现尽管部分模型输出正确，但其执行模拟常缺乏逻辑一致性，且在不同测试中推理表现不稳定。


<details>
  <summary>Details</summary>
Motivation: 现有评估方法难以判断LLMs在编程任务中的正确输出是否源于真实的执行推理，还是依赖自然语言捷径、幻觉或数据泄露，因此需要一种能衡量执行模拟逻辑一致性的新评估框架。

Method: CES任务通过评估变量预测的正确性与执行模拟的“一致性”（即是否符合常识性执行逻辑）来判断LLMs的推理质量，并引入覆盖不同主路径的测试用例，按强、弱、随机三类衡量推理一致性；同时将CES与缺陷预测/定位/修复任务进行对比分析。

Result: 在HumanEval上，16个LLMs平均达到81.42%的一致性执行模拟，其中46.92%带来正确输出，53.08%仍输出错误；GPT-4和DeepSeek-R1等前沿模型一致性最差；模型在不同测试间的推理一致性多为随机（48.87%）或弱（45.37%）；在缺陷相关任务中，LLMs主要依赖模式匹配或语言捷径，而非执行推理。

Conclusion: LLMs在编程任务中的成功常缺乏可靠的执行推理支持，CES可有效识别此类“可疑成功”，揭示模型在路径敏感分析上的不足，强调提升逻辑一致性和泛化能力的重要性。

Abstract: This paper proposes CES, a task to evaluate the abilities of LLMs in
simulating program execution and using that reasoning in programming tasks.
Besides measuring the correctness of variable predictions during execution
simulation, CES introduces the notion of coherence to determine whether the
simulation complies with commonsense execution logic, even if the predicted
values along the simulations are incorrect. This enables CES to rule out
suspiciously correct output predictions due to reasoning shortcuts,
hallucinations, or potential data leakage. CES also introduces a novel metric
to measure reasoning consistency across tests with the same or different prime
path coverage in a spectrum: strong, weak, and random. Evaluating 16 LLMs
(including three reasoning LLMs) using CES indicates 81.42% coherent execution
simulation on HumanEval, 46.92% and 53.08% of which result in correct and
incorrect output predictions. Frontier LLMs such as GPT-4 and DeepSeek-R1 have
the most incoherent execution reasoning, mostly due to natural language
shortcuts. Despite relatively coherent execution simulation, LLMs' reasoning
performance across different tests is inconsistent, mostly random (48.87%) or
weak (45.37%), potentially explaining their weakness in programming tasks that
require path-sensitive program analysis to succeed. We also compare CES with
bug prediction/localization/repair, which intuitively requires control- and
data-flow awareness. We observe that LLMs barely incorporate execution
reasoning into their analysis for bug-related tasks, and their success is
primarily due to inherent abilities in pattern matching or natural language
shortcuts, if not data leakage. Without reasoning, there is a threat to the
generalizability of LLMs in dealing with unseen bugs or patterns in different
contexts. CES can be used to vet the suspicious success of LLMs in these tasks
systematically.

</details>


### [13] [Community Engagement and the Lifespan of Open-Source Software Projects](https://arxiv.org/abs/2510.15408)
*Mohit,Kuljit Kaur Chahal*

Main category: cs.SE

TL;DR: 本研究通过分析33,946个GitHub仓库，定义并验证了开源软件项目中的社区参与（CE）指标，发现CE与项目动态（如发布、提交等）显著相关，且对项目寿命具有复杂但关键的影响：初期高参与有助于项目建立，而持续高参与则推动极长寿命；活跃的问题互动随项目年龄增长作用增强，而被动关注则减弱。


<details>
  <summary>Details</summary>
Motivation: 开源软件项目的长期存续依赖社区参与，但社区参与对项目动态和寿命的可量化影响尚未被充分研究。

Method: 研究分析了33,946个GitHub仓库，定义并操作化社区参与指标（包括每月的问题数、评论数、关注者和星标数），并通过非参数检验和相关性分析评估这些指标与项目动态及寿命在不同四分位数中的关系。

Result: 社区参与指标与项目动态显著相关，且在高参与项目中相关性更强；就项目寿命而言，年轻项目每月社区参与率最高，随年龄增长而下降，但部分长寿项目仍保持极高活跃度；初期参与爆发对项目建立至关重要，持续高参与推动极端长寿；活跃问题互动的影响随项目年龄增强，而被动关注的影响减弱。

Conclusion: 社区参与动态地驱动开源软件项目的长期发展和寿命，本研究确立了经过验证的社区参与指标，并深入揭示了不同类型社区活动对项目寿命的贡献机制。

Abstract: Open-source software (OSS) projects depend on community engagement (CE) for
longevity. However, CE's quantifiable impact on project dynamics and lifespan
is underexplored. Objectives: This study defines CE in OSS, identifies key
metrics, and evaluates their influence on project dynamics (releases, commits,
branches) and lifespan. Methods: We analyzed 33,946 GitHub repositories,
defining and operationalizing CE with validated per-month metrics (issues,
comments, watchers, stargazers). Non-parametric tests and correlations assessed
relationships with project dynamics and lifespan across quartiles. Results: CE
metrics significantly associate with project dynamics, with stronger
correlations in highly engaged projects. For lifespan, a complex pattern
emerged: per-month CE rates are highest in younger projects, declining with
age. Yet, a subset of long-lived projects maintains exceptionally high
activity. Initial CE bursts appear crucial for establishment, while sustained
high engagement drives extreme longevity. Active issue engagement's influence
intensifies with age, but passive attention's declines. Conclusion: CE
dynamically drives OSS project longevity and development. Our findings
establish validated CE metrics and offer deeper insights into how diverse
community activity patterns contribute to project longevity.

</details>


### [14] [Selecting and Combining Large Language Models for Scalable Code Clone Detection](https://arxiv.org/abs/2510.15480)
*Muslim Chochlov,Gul Aftab Ahmed,James Vincent Patten,Yuanhua Han,Guoxian Lu,David Gregg,Jim Buckley*

Main category: cs.SE

TL;DR: 本文研究了大语言模型（LLMs）在代码克隆检测中的适用性与集成策略，发现CodeT5+110M、CuBERT和SPTCode表现最佳，并证明通过合理的集成方法可进一步提升检测精度。


<details>
  <summary>Details</summary>
Motivation: 代码克隆可能带来知识产权侵犯和安全漏洞等问题，而现有方法在检测高度变异的克隆代码时仍面临效率与效果的挑战。尽管大语言模型被用于该任务，但如何选择合适模型及是否可通过集成提升性能尚不明确。

Method: 作者筛选出76个LLM并评估其在BigCloneBench和一个商业大规模数据集上的表现；随后探索不同集成策略（如最大值、求和、平均）对检测效果的影响，并分析模型结构特征（如嵌入大小、词表规模）与性能的关系。

Result: 没有单一“最佳”LLM，但CodeT5+110M、CuBERT和SPTCode表现突出；CodeT5+110M在商业数据集上达到39.71%的精度，是此前CodeBERT的两倍；集成方法中最大值或求和优于平均，最佳集成方案在商业数据集上精度达46.91%。

Conclusion: 针对代码克隆检测任务，较小的嵌入维度、较小的词表及专用训练数据有助于提升LLM性能；合理集成多个LLM可显著提高检测精度，尤其在大规模数据集上效果更佳。

Abstract: Source code clones pose risks ranging from intellectual property violations
to unintended vulnerabilities. Effective and efficient scalable clone
detection, especially for diverged clones, remains challenging. Large language
models (LLMs) have recently been applied to clone detection tasks. However, the
rapid emergence of LLMs raises questions about optimal model selection and
potential LLM-ensemble efficacy.
  This paper addresses the first question by identifying 76 LLMs and filtering
them down to suitable candidates for large-scale clone detection. The
candidates were evaluated on two public industrial datasets, BigCloneBench, and
a commercial large-scale dataset. No uniformly 'best-LLM' emerged, though
CodeT5+110M, CuBERT and SPTCode were top-performers. Analysis of LLM-candidates
suggested that smaller embedding sizes, smaller tokenizer vocabularies and
tailored datasets are advantageous. On commercial large-scale dataset a
top-performing CodeT5+110M achieved 39.71\% precision: twice the precision of
previously used CodeBERT.
  To address the second question, this paper explores ensembling of the
selected LLMs: effort-effective approach to improving effectiveness. Results
suggest the importance of score normalization and favoring ensembling methods
like maximum or sum over averaging. Also, findings indicate that ensembling
approach can be statistically significant and effective on larger datasets: the
best-performing ensemble achieved even higher precision of 46.91\% over
individual LLM on the commercial large-scale code.

</details>


### [15] [Enhancing Code Review through Fuzzing and Likely Invariants](https://arxiv.org/abs/2510.15512)
*Wachiraphan Charoenwet,Patanamon Thongtanunam,Van-Thuan Pham,Christoph Treude*

Main category: cs.SE

TL;DR: FuzzSight 是一个结合模糊测试与动态不变量分析的框架，用于在代码审查阶段早期识别由代码变更引发的非崩溃型行为异常，从而帮助发现回归缺陷和漏洞。


<details>
  <summary>Details</summary>
Motivation: 传统代码审查主要依赖静态检查，难以覆盖程序的动态行为；而模糊测试虽能生成多样输入，但其产生的动态行为数据缺乏有效分析机制，难以融入代码审查流程。

Method: 提出 FuzzSight 框架，通过非崩溃型模糊测试输入提取程序在特定位置的“可能不变量”（likely invariants），以此表征程序行为，并对比不同版本间的行为差异，识别潜在异常。

Result: 实验表明，FuzzSight 能检测到 75% 的回归缺陷和高达 80% 的漏洞；相比静态分析工具（SAST），其在定位缺陷代码块方面检测率高十倍且误报更少。

Conclusion: 将模糊测试与不变量分析结合，可有效补充静态代码审查，为早期发现非崩溃型行为异常提供实用且高效的动态洞察。

Abstract: Many software projects employ manual code review to gatekeep defects and
vulnerabilities in the code before integration. However, reviewers often work
under time pressure and rely primarily on static inspection, leaving the
dynamic aspects of the program unexplored. Dynamic analyses could reveal such
behaviors, but they are rarely integrated into reviews. Among them, fuzzing is
typically applied later to uncover crashing bugs. Yet its ability to exercise
code with diverse inputs makes it promising for exposing non-crashing, but
unexpected, behaviors earlier. Still, without suitable mechanisms to analyze
program behaviors, the rich data produced during fuzzing remains inaccessible
to reviewers, limiting its practical value in this context.
  We hypothesize that unexpected variations in program behaviors could signify
potential bugs. The impact of code changes can be automatically captured at
runtime. Representing program behavior as likely invariants, dynamic properties
consistently observed at specific program points, can provide practical signals
of behavioral changes. Such signals offer a way to distinguish between intended
changes and unexpected behavioral shifts from code changes.
  We present FuzzSight, a framework that leverages likely invariants from
non-crashing fuzzing inputs to highlight behavioral differences across program
versions. By surfacing such differences, it provides insights into which code
blocks may need closer attention. In our evaluation, FuzzSight flagged 75% of
regression bugs and up to 80% of vulnerabilities uncovered by 24-hour fuzzing.
It also outperformed SAST in identifying buggy code blocks, achieving ten times
higher detection rates with fewer false alarms. In summary, FuzzSight
demonstrates the potential and value of leveraging fuzzing and invariant
analysis for early-stage code review, bridging static inspection with dynamic
behavioral insights.

</details>


### [16] [Colepp: uma ferramenta multiplataforma para coleta de dados de dispositivos vestiveis](https://arxiv.org/abs/2510.15565)
*Vinicius Moraes de Jesus,Andre Georghton Cardoso Pacheco*

Main category: cs.SE

TL;DR: Colepp 是一个开源跨平台工具，用于从多种可穿戴设备同步采集心率和运动数据，并通过智能手机整合生成可用于人体活动识别等任务的高质量数据集。


<details>
  <summary>Details</summary>
Motivation: 现有可穿戴设备数据采集面临缺乏大规模高质量公开数据集及数据采集条件不可控的问题，限制了鲁棒算法的开发。

Method: 开发 Colepp 系统，利用智能手机作为中心枢纽，同步采集 Polar H10 胸带（ECG）和 Wear OS 智能手表（PPG、加速度计、陀螺仪）的数据，并通过自定义同步协议导出为 CSV 格式。

Result: 通过实际用例验证，Colepp 能有效生成一致且同步的生理与运动信号数据集。

Conclusion: Colepp 为研究者提供了一个灵活、易用的工具，有助于推动基于可穿戴设备的人体活动识别与心率估计算法的发展。

Abstract: The widespread adoption of wearable devices such as smartwatches and fitness
trackers has fueled the demand for reliable physiological and movement data
collection tools. However, challenges such as limited access to large,
high-quality public datasets and a lack of control over data collection
conditions hinder the development of robust algorithms. This work presents
Colepp, an open-source, cross-platform tool designed to collect and synchronize
data from multiple wearable devices, including heart rate (via ECG and PPG) and
motion signals (accelerometer and gyroscope). The system integrates a
smartphone as a central hub, receiving data from a Polar H10 chest strap and a
Wear OS smartwatch, and exporting synchronized datasets in CSV format. Through
a custom synchronization protocol and user-friendly interface, Colepp
facilitates the generation of customizable, real-world datasets suitable for
applications such as human activity recognition and heart rate estimation. A
use case shows the effectiveness of the tool in producing consistent and
synchronized signals.

</details>


### [17] [Leveraging Test Driven Development with Large Language Models for Reliable and Verifiable Spreadsheet Code Generation: A Research Framework](https://arxiv.org/abs/2510.15585)
*Dr Simon Thorne,Dr Advait Sarkar*

Main category: cs.SE

TL;DR: 本文提出将测试驱动开发（TDD）与大语言模型（LLM）生成相结合，以提升生成代码（包括电子表格公式和多种编程语言）的准确性、可靠性和用户信心，尤其面向缺乏编程训练但对逻辑正确性要求高的用户群体。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在生成代码时存在幻觉、逻辑不一致和语法错误等问题，在金融建模和科学计算等高风险领域尤为严重，亟需提升其输出的可靠性与可验证性。

Method: 提出一个结合测试驱动开发（TDD）与LLM生成的结构化研究框架，采用“先写测试”的方法对LLM输出进行技术约束和认知引导，并设计了包含参与者分组、评估指标和提示示例的实验方案。

Result: 该框架适用于电子表格、Python、Rust等多种编程场景，有望提升用户的计算思维、提示工程能力和参与度，尤其帮助非专业编程用户减少逻辑错误。

Conclusion: 通过强调测试驱动思维，该方法有望在教育和专业开发实践中实现更负责任、可靠的LLM集成，作者呼吁合作以进一步完善和实证评估该框架。

Abstract: Large Language Models (LLMs), such as ChatGPT, are increasingly leveraged for
generating both traditional software code and spreadsheet logic. Despite their
impressive generative capabilities, these models frequently exhibit critical
issues such as hallucinations, subtle logical inconsistencies, and syntactic
errors, risks particularly acute in high stakes domains like financial
modelling and scientific computations, where accuracy and reliability are
paramount. This position paper proposes a structured research framework that
integrates the proven software engineering practice of Test-Driven Development
(TDD) with Large Language Model (LLM) driven generation to enhance the
correctness of, reliability of, and user confidence in generated outputs. We
hypothesise that a "test first" methodology provides both technical constraints
and cognitive scaffolding, guiding LLM outputs towards more accurate,
verifiable, and comprehensible solutions. Our framework, applicable across
diverse programming contexts, from spreadsheet formula generation to scripting
languages such as Python and strongly typed languages like Rust, includes an
explicitly outlined experimental design with clearly defined participant
groups, evaluation metrics, and illustrative TDD based prompting examples. By
emphasising test driven thinking, we aim to improve computational thinking,
prompt engineering skills, and user engagement, particularly benefiting
spreadsheet users who often lack formal programming training yet face serious
consequences from logical errors. We invite collaboration to refine and
empirically evaluate this approach, ultimately aiming to establish responsible
and reliable LLM integration in both educational and professional development
practices.

</details>


### [18] [Interact and React: Exploring Gender Patterns in Development and the Impact on Innovation and Robustness of a User Interface Tool](https://arxiv.org/abs/2510.15642)
*Sian Brooke*

Main category: cs.SE

TL;DR: 该研究分析了React项目11年来的贡献数据，发现女性在功能增强和依赖管理方面贡献显著更多，表明性别多样性有助于提升软件的创新性与稳健性。


<details>
  <summary>Details</summary>
Motivation: 探讨性别多样性（尤其是女性参与）如何根本性地改变开源软件的开发模式，而不仅停留在象征性包容层面。

Method: 对React项目11年间的贡献数据进行分析，比较不同性别在稳健性、创新性指标及重大版本发布前的贡献模式差异。

Result: 女性在功能增强和依赖管理方面贡献显著更多，排除女性对软件质量有害。

Conclusion: 提升性别多样性可促进软件更具包容性、创新性和稳健性。

Abstract: In open-source software design, the inclusion of women is often highlighted
simply to remind programmers that women exist. Yet, little attention is given
to how greater gender diversity, specifically women's participation, could
fundamentally alter development patterns. To understand the potential impact of
gender inclusion, this study investigates React, a widely used JavaScript
library for building user interfaces with an active contributor community. I
examine gender differences in metrics of robustness and innovation, as well as
shifts in contribution patterns leading up to major version releases over 11
years of the React project. My results show that the exclusion of women is
detrimental to software as women contribute significantly more to feature
enhancement and dependency management. By exploring how gender influences
innovation and robustness in the development of React, the study offers
critical insights into how increasing gender diversity could lead to more
inclusive, innovative, and robust software.

</details>


### [19] [MirrorFuzz: Leveraging LLM and Shared Bugs for Deep Learning Framework APIs Fuzzing](https://arxiv.org/abs/2510.15690)
*Shiwen Ou,Yuwei Li,Lu Yu,Chengkun Wei,Tingke Wen,Qiangpu Chen,Yu Chen,Haizhi Tang,Zulie Pan*

Main category: cs.SE

TL;DR: MirrorFuzz 是一种基于大语言模型的自动化 API 模糊测试工具，用于发现深度学习框架中的共享漏洞，显著提升代码覆盖率并成功检测出大量新漏洞。


<details>
  <summary>Details</summary>
Motivation: 深度学习框架中的漏洞可能引发上层应用的严重问题，而现有研究较少关注跨框架的通用 API 模式及其潜在风险。由于多个框架提供功能和参数相似的 API，一个框架中的漏洞可能在其他框架中重复出现，因此亟需一种能系统性发现这类共享漏洞的方法。

Method: MirrorFuzz 分三步工作：首先收集各 API 的历史漏洞数据以识别潜在易错 API；其次在框架内及跨框架匹配相似 API；最后利用大语言模型结合历史漏洞信息生成测试代码，触发相似 API 中的同类漏洞。

Result: 在 TensorFlow、PyTorch、OneFlow 和 Jittor 四个主流框架上的实验表明，MirrorFuzz 相比现有最先进方法在 TensorFlow 和 PyTorch 上分别提升代码覆盖率 39.92% 和 98.20%；共发现 315 个漏洞，其中 262 个为新漏洞，80 个已被修复，52 个获得 CNVD 编号。

Conclusion: MirrorFuzz 能有效识别深度学习框架中因 API 相似性导致的共享漏洞，显著优于现有模糊测试方法，为提升 DL 框架的可靠性与安全性提供了实用工具。

Abstract: Deep learning (DL) frameworks serve as the backbone for a wide range of
artificial intelligence applications. However, bugs within DL frameworks can
cascade into critical issues in higher-level applications, jeopardizing
reliability and security. While numerous techniques have been proposed to
detect bugs in DL frameworks, research exploring common API patterns across
frameworks and the potential risks they entail remains limited. Notably, many
DL frameworks expose similar APIs with overlapping input parameters and
functionalities, rendering them vulnerable to shared bugs, where a flaw in one
API may extend to analogous APIs in other frameworks. To address this
challenge, we propose MirrorFuzz, an automated API fuzzing solution to discover
shared bugs in DL frameworks. MirrorFuzz operates in three stages: First,
MirrorFuzz collects historical bug data for each API within a DL framework to
identify potentially buggy APIs. Second, it matches each buggy API in a
specific framework with similar APIs within and across other DL frameworks.
Third, it employs large language models (LLMs) to synthesize code for the API
under test, leveraging the historical bug data of similar APIs to trigger
analogous bugs across APIs. We implement MirrorFuzz and evaluate it on four
popular DL frameworks (TensorFlow, PyTorch, OneFlow, and Jittor). Extensive
evaluation demonstrates that MirrorFuzz improves code coverage by 39.92\% and
98.20\% compared to state-of-the-art methods on TensorFlow and PyTorch,
respectively. Moreover, MirrorFuzz discovers 315 bugs, 262 of which are newly
found, and 80 bugs are fixed, with 52 of these bugs assigned CNVD IDs.

</details>


### [20] [EASELAN: An Open-Source Framework for Multimodal Biosignal Annotation and Data Management](https://arxiv.org/abs/2510.15767)
*Rathi Adarshi Rammohan,Moritz Meier,Dennis Küster,Tanja Schultz*

Main category: cs.SE

TL;DR: 本文提出了EASELAN标注框架，通过扩展ELAN工具以支持多模态与生物信号数据的高效标注，涵盖从数据准备、多通道设置、版本控制到后处理的全流程，并已成功应用于日常活动（如摆桌）的高维生物信号数据集构建。


<details>
  <summary>Details</summary>
Motivation: 随着融合模型对多模态及生物信号数据需求的增长，传统标注工具难以应对日益复杂的数据标注任务，亟需更高效、集成化的标注流程支持。

Method: 在ELAN工具基础上开发EASELAN框架，新增支持标注文件准备、多通道配置、GitHub集成版本控制及简化后处理等功能，实现生物信号与多模态数据的无缝标注与导出。

Result: EASELAN已成功应用于DFG资助的EASE项目中的人类日常活动（摆桌）高维生物信号数据采集与标注，构建了完整的标注数据库，并公开了代码与数据集。

Conclusion: EASELAN显著提升了多模态与生物信号数据的标注效率与质量，为认知机器人等领域提供了可复用的标注基础设施，同时通过开源促进相关研究发展。

Abstract: Recent advancements in machine learning and adaptive cognitive systems are
driving a growing demand for large and richly annotated multimodal data. A
prominent example of this trend are fusion models, which increasingly
incorporate multiple biosignals in addition to traditional audiovisual
channels. This paper introduces the EASELAN annotation framework to improve
annotation workflows designed to address the resulting rising complexity of
multimodal and biosignals datasets. It builds on the robust ELAN tool by adding
new components tailored to support all stages of the annotation pipeline: From
streamlining the preparation of annotation files to setting up additional
channels, integrated version control with GitHub, and simplified
post-processing. EASELAN delivers a seamless workflow designed to integrate
biosignals and facilitate rich annotations to be readily exported for further
analyses and machine learning-supported model training. The EASELAN framework
is successfully applied to a high-dimensional biosignals collection initiative
on human everyday activities (here, table setting) for cognitive robots within
the DFG-funded Collaborative Research Center 1320 Everyday Activity Science and
Engineering (EASE). In this paper we discuss the opportunities, limitations,
and lessons learned when using EASELAN for this initiative. To foster research
on biosignal collection, annotation, and processing, the code of EASELAN is
publicly available(https://github.com/cognitive-systems-lab/easelan), along
with the EASELAN-supported fully annotated Table Setting Database.

</details>


### [21] [Towards Supporting Open Source Library Maintainers with Community-Based Analytics](https://arxiv.org/abs/2510.15794)
*Rachna Raj,Diego Elias Costa*

Main category: cs.SE

TL;DR: 该论文提出通过社区使用数据分析开源库的实际API使用情况，发现仅有16%的API被依赖项目广泛使用，且其中仅74%被库自身的测试覆盖，并提出了两个新指标帮助维护者优化测试与维护策略。


<details>
  <summary>Details</summary>
Motivation: 开源软件维护者缺乏对其API在依赖项目中实际使用情况的持续反馈，限制了其在测试、变更影响评估和库演进方面的决策能力。

Method: 对10个流行的Java开源库及其各自50个依赖项目进行实证研究，分析API使用情况和测试覆盖率，并提出两个新指标；同时通过问卷调查评估这些洞察对维护实践的价值。

Result: 平均仅有16%的API方法被依赖生态系统实际使用，且这些被使用的API中仅有74%被库的测试套件部分或完全覆盖。

Conclusion: 社区驱动的使用数据分析能为开源库维护者提供有价值的反馈，有助于优化测试策略和指导库的演进。

Abstract: Open-source software (OSS) is a pillar of modern software development. Its
success depends on the dedication of maintainers who work constantly to keep
their libraries stable, adapt to changing needs, and support a growing
community. Yet, they receive little to no continuous feedback on how the
projects that rely on their libraries actually use their APIs. We believe that
gaining these insights can help maintainers make better decisions, such as
refining testing strategies, understanding the impact of changes, and guiding
the evolution of their libraries more effectively. We propose the use of
community-based analytics to analyze how an OSS library is used across its
dependent ecosystem. We conduct an empirical study of 10 popular Java libraries
and each with their respective dependent ecosystem of 50 projects. Our results
reveal that while library developers offer a wide range of API methods, only
16% on average are actively used by their dependent ecosystem. Moreover, only
74% of the used API methods are partially or fully covered by their library
test suite. We propose two metrics to help developers evaluate their test suite
according to the APIs used by their community, and we conduct a survey on
open-source practitioners to assess the practical value of these insights in
guiding maintenance decisions.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [22] [Uno: A One-Stop Solution for Inter- and Intra-Datacenter Congestion Control and Reliable Connectivity](https://arxiv.org/abs/2510.15802)
*Tommaso Bonato,Sepehr Abdous,Abdul Kabbani,Ahmad Ghalayini,Nadeen Gebara,Terry Lam,Anup Agarwal,Tiancheng Chen,Zhuolong Yu,Konstantin Taranov,Mahmoud Elhaddad,Daniele De Sensi,Soudeh Ghorbani,Torsten Hoefler*

Main category: cs.NI

TL;DR: 论文提出了一种名为Uno的统一系统，用于同时优化数据中心内部和跨数据中心的流量传输，通过整合快速拥塞响应的传输协议与结合纠删码和自适应路由的负载均衡方案，显著提升了两类流量的完成时间。


<details>
  <summary>Details</summary>
Motivation: 现有方案在处理数据中心内部与跨数据中心流量时存在不足：由于两类流量的往返时延（RTT）差异，导致拥塞控制不公平；跨数据中心流量还面临丢包恢复慢和可靠性需求高的问题。当前方法通常采用分离的控制环路或不同粒度处理，无法有效应对这些挑战。

Method: 提出Uno系统，统一处理数据中心内与跨数据中心流量。该系统结合一种支持快速拥塞响应和公平速率控制的传输协议，以及一种融合纠删码与自适应路由的负载均衡机制。

Result: 实验结果表明，相比Gemini等现有先进方法，Uno显著缩短了数据中心内部和跨数据中心流量的完成时间。

Conclusion: Uno通过统一架构有效解决了数据中心内与跨数据中心流量在拥塞控制、公平性和可靠性方面的关键挑战，为云和AI工作负载提供了更高效的通信支持。

Abstract: Cloud computing and AI workloads are driving unprecedented demand for
efficient communication within and across datacenters. However, the coexistence
of intra- and inter-datacenter traffic within datacenters plus the disparity
between the RTTs of intra- and inter-datacenter networks complicates congestion
management and traffic routing. Particularly, faster congestion responses of
intra-datacenter traffic causes rate unfairness when competing with slower
inter-datacenter flows. Additionally, inter-datacenter messages suffer from
slow loss recovery and, thus, require reliability. Existing solutions overlook
these challenges and handle inter- and intra-datacenter congestion with
separate control loops or at different granularities. We propose Uno, a unified
system for both inter- and intra-DC environments that integrates a transport
protocol for rapid congestion reaction and fair rate control with a load
balancing scheme that combines erasure coding and adaptive routing. Our
findings show that Uno significantly improves the completion times of both
inter- and intra-DC flows compared to state-of-the-art methods such as Gemini.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [23] [Cleaning up the Mess](https://arxiv.org/abs/2510.15744)
*Haocong Luo,Ataberk Olgun,Maria Makeenkova,F. Nisa Bostanci,Geraldo F. Oliveira,A. Giray Yaglikci,Onur Mutlu*

Main category: cs.AR

TL;DR: 该论文指出MICRO 2024最佳论文亚军“Mess”在使用Ramulator 2.0和DAMOV模拟器时存在配置和使用错误，导致其关键结论不成立，并强调了严谨验证模拟结果及与模拟器开发者协作的重要性。


<details>
  <summary>Details</summary>
Motivation: 纠正Mess论文中关于内存系统模拟器（Ramulator 2.0和DAMOV）的错误使用，防止错误结论在学术界传播，维护科研记录的可靠性，并反思评审与成果复现评估流程的完整性。

Method: 通过复现Mess论文的实验，检查其模拟器配置与使用方式，发现其在Ramulator 2.0和DAMOV中的配置错误和统计指标误用，并基于正确配置重新运行实验进行对比验证。

Result: 正确配置Ramulator 2.0后，其模拟结果能较好反映真实系统性能，推翻了Mess论文的关键主张；同时发现DAMOV结果使用了与DRAM性能无关的错误统计量，且Mess论文的成果仓库缺少完整复现所需代码。

Conclusion: Mess论文关于内存模拟器性能评估的结论因人为配置错误而不成立；研究强调了严格验证模拟结果、与工具开发者协作的重要性，并呼吁社区纠正错误，以维护科研可信度。

Abstract: A MICRO 2024 best paper runner-up publication (the Mess paper) with all three
artifact badges awarded (including "Reproducible") proposes a new benchmark to
evaluate real and simulated memory system performance. In this paper, we
demonstrate that the Ramulator 2.0 simulation results reported in the Mess
paper are incorrect and, at the time of the publication of the Mess paper,
irreproducible. We find that the authors of Mess paper made multiple trivial
human errors in both the configuration and usage of the simulators. We show
that by correctly configuring Ramulator 2.0, Ramulator 2.0's simulated memory
system performance actually resembles real system characteristics well, and
thus a key claimed contribution of the Mess paper is factually incorrect. We
also identify that the DAMOV simulation results in the Mess paper use wrong
simulation statistics that are unrelated to the simulated DRAM performance.
Moreover, the Mess paper's artifact repository lacks the necessary sources to
fully reproduce all the Mess paper's results.
  Our work corrects the Mess paper's errors regarding Ramulator 2.0 and
identifies important issues in the Mess paper's memory simulator evaluation
methodology. We emphasize the importance of both carefully and rigorously
validating simulation results and contacting simulator authors and developers,
in true open source spirit, to ensure these simulators are used with correct
configurations and as intended. We encourage the computer architecture
community to correct the Mess paper's errors. This is necessary to prevent the
propagation of inaccurate and misleading results, and to maintain the
reliability of the scientific record. Our investigation also opens up questions
about the integrity of the review and artifact evaluation processes. To aid
future work, our source code and scripts are openly available at https:
//github.com/CMU-SAFARI/ramulator2/tree/mess.

</details>


<div id='cs.PF'></div>

# cs.PF [[Back]](#toc)

### [24] [Impact of AI-Triage on Radiologist Report Turnaround Time: Real-World Time-Savings and Insights from Model Predictions](https://arxiv.org/abs/2510.15237)
*Yee Lam Elim Thompson,Jonathan Fergus,Jonathan Chung,Jana G. Delfino,Weijie Chen,Gary M. Levine,Frank W. Samuelson*

Main category: cs.PF

TL;DR: 该研究通过回顾性分析评估AI分诊系统在肺栓塞（PE）胸部CT肺动脉造影（CTPA）检查中对报告周转时间（TAT）的节省效果，并结合工作流参数建立计算模型预测时间节省，发现工作时间内的TAT显著缩短，而模型预测与实际观察结果一致。


<details>
  <summary>Details</summary>
Motivation: 准确评估AI分诊设备在临床实践中对报告周转时间的实际影响，需考虑并量化放射科医生的工作流参数，以避免高估或低估AI带来的效益。

Method: 回顾性分析11252例疑似PE的成人CTPA检查，分为AI部署前后两个阶段；提取PACS系统中527234条记录以获取工作流参数（如检查到达间隔、读片时间等），并构建计算模型预测AI分诊后的时间节省效果；比较PE阳性病例在工作时间和非工作时间的TAT差异。

Result: AI部署后，工作时间内PE阳性病例的平均TAT从68.9分钟降至46.7分钟，节省22.2分钟（p=0.004），具有统计学意义；非工作时间节省2.82分钟（p=0.345），不显著；模型预测结果与实际观察高度一致。

Conclusion: 将临床工作流参数纳入评估框架，有助于更准确地预测和理解AI分诊系统在真实临床环境中的时间节省效果，尤其在工作时间内的效益更为显著。

Abstract: Objective: To quantify the impact of workflow parameters on time-savings in
report turnaround time (TAT) due to an AI-triage device that prioritized
pulmonary embolism (PE) in chest CT pulmonary angiography (CTPA) exams.
Methods: This retrospective study analyzed 11252 adult CTPA exams conducted for
suspected PE at a single tertiary academic medical center. Data was divided
into two periods: pre-AI and post-AI. For PE-positive exams, TAT - defined as
the duration from patient scan completion to the first preliminary report
completion - was compared between the two periods. Time-savings were reported
separately for work-hour and off-hour cohorts. To characterize radiologist
workflow, 527234 records were retrieved from the PACS and workflow parameters
such as exam inter-arrival time and radiologist read-time extracted. These
parameters were input into a computational model to predict time-savings
following deployment of an AI-triage device and to study the impact of workflow
parameters. Results: The pre-AI dataset included 4694 chest CTPA exams with
13.3% being PE-positive. The post-AI dataset comprised 6558 exams with 16.2%
being PE-positive. The mean TAT for pre-AI and post-AI during work hours are
68.9 [95% CI" 55.0, 82.8] and 46.7 [38.1, 55.2] minutes respectively, and those
during off-hours are 44.8 [33.7, 55.9] and 42.0 [33.6, 50.3] minutes.
Clinically-observed time-savings during work hours (22.2 [95% CI: 5.85, 38.6]
minutes) were significant (p=0.004), while off-hour (2.82 [-11.1, 16.7]
minutes) were not (p=0.345). Observed time-savings aligned with model
predictions (29.6 [95% range: 23.2, 38.1] minutes for work hours; 2.10 [1.76,
2.58] minutes for off-hours). Discussion: Consideration and quantification of
clinical workflow contribute to an accurate assessment of the expected
time-savings in TAT following deployment of an AI-triage device.

</details>
