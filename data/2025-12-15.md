<div id=toc></div>

# Table of Contents

- [cs.DC](#cs.DC) [Total: 10]
- [cs.AR](#cs.AR) [Total: 1]
- [cs.SE](#cs.SE) [Total: 4]


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [1] [An Efficient Approach for Energy Conservation in Cloud Computing Environment](https://arxiv.org/abs/2512.10974)
*Sohan Kumar Pande,Sanjaya Kumar Panda,Preeti Ranjan Sahu*

Main category: cs.DC

TL;DR: 本文提出了一种新的任务调度算法，通过优化CPU、磁盘和I/O资源利用率来降低云服务能耗，仿真表明其比现有MaxUtil算法更节能。


<details>
  <summary>Details</summary>
Motivation: 当前云服务能耗高且依赖有限能源，亟需开发兼顾多种资源利用的节能调度算法。

Method: 设计基于CPU、磁盘、I/O利用率及任务处理时间的适应度函数，用于任务调度以提升整体资源利用率。

Result: 仿真实验显示，新算法相比MaxUtil在合成数据集上实现了更低能耗。

Conclusion: 该算法有效提升多维资源利用率，是更优的节能型云任务调度方案。

Abstract: Recent trends of technology have explored a numerous applications of cloud services, which require a significant amount of energy. In the present scenario, most of the energy sources are limited and have a greenhouse effect on the environment. Therefore, it is the need of the hour that the energy consumed by the cloud service providers must be reduced and it is a great challenge to the research community to develop energy-efficient algorithms. To design the same, some researchers tried to maximize the average resource utilization, whereas some researchers tried to minimize the makespan. However, they have not considered different types of resources that are present in the physical machines. In this paper, we propose a task scheduling algorithm, which tries to improve utilization of resources (like CPU, disk, I/O) explicitly, which in turn increases the utilization of active resources. For this, the proposed algorithm uses a fitness value, which is a function of CPU, disk and I/O utilization, and processing time of the task. To demonstrate the performance of the proposed algorithm, extensive simulations are performed on both proposed algorithm and existing algorithm MaxUtil using synthetic datasets. From the simulation results, it can be observed that the proposed algorithm is a better energy-efficient algorithm and consumes less energy than the MaxUtil algorithm.

</details>


### [2] [Agentic Operator Generation for ML ASICs](https://arxiv.org/abs/2512.10977)
*Alec M. Hammond,Aram Markosyan,Aman Dontula,Simon Mahns,Zacharias Fisches,Dmitrii Pedchenko,Keyur Muzumdar,Natacha Supper,Mark Saroufim,Joe Isaacson,Laura Wang,Warren Hunt,Kaustubh Gondkar,Roman Levenstein,Gabriel Synnaeve,Richard Li,Jacob Kahn,Ajit Mathews*

Main category: cs.DC

TL;DR: TritorX 是一个基于 AI 的系统，可大规模生成功能正确的 Triton PyTorch ATen 内核，支持新兴加速器平台。


<details>
  <summary>Details</summary>
Motivation: 解决现有内核生成方法覆盖范围有限的问题，强调运算符集的正确性与通用性。

Method: 结合开源大模型、自定义 linter、JIT 编译和 OpInfo 测试框架，兼容真实硬件与仿真环境。

Result: 成功为 481 个 ATen 算子生成通过全部 20,000+ 测试的内核与封装代码。

Conclusion: TritorX 可在一夜之间为新加速器平台生成完整的 PyTorch ATen 后端，推动硬件适配效率。

Abstract: We present TritorX, an agentic AI system designed to generate functionally correct Triton PyTorch ATen kernels at scale for emerging accelerator platforms. TritorX integrates open-source large language models with a custom linter, JIT compilation, and a PyTorch OpInfo-based test harness. This pipeline is compatible with both real Meta Training and Inference Accelerator (MTIA) silicon and in hardware simulation environments for next-generation devices. In contrast to previous kernel-generation approaches that prioritize performance for a limited set of high-usage kernels, TritorX prioritizes coverage. Our system emphasizes correctness and generality across the entire operator set, including diverse data types, shapes, and argument patterns. In our experiments, TritorX successfully generated kernels and wrappers for 481 unique ATen operators that pass all corresponding PyTorch OpInfo tests (over 20,000 in total). TritorX paves the way for overnight generation of complete PyTorch ATen backends for new accelerator platforms.

</details>


### [3] [Seamless Transitions: A Comprehensive Review of Live Migration Technologies](https://arxiv.org/abs/2512.10979)
*Sima Attar-Khorasani,Lincoln Sherpa,Matthias Lieber,Siavash Ghiasvand*

Main category: cs.DC

TL;DR: 本文全面分析了实时迁移技术，重点关注容器和虚拟机迁移方法，探讨其在实际应用中的挑战与效能，并为未来研究提供指导。


<details>
  <summary>Details</summary>
Motivation: 现有综述常忽略实时迁移在真实场景中的关键技术细节与实践难题，本文旨在填补这一空白。

Method: 整合现有综述内容，从迁移技术、迁移单元和基础设施特性等多维度进行综合分析。

Result: 揭示了容器与虚拟机迁移技术的现状与采用差异，指出了系统依赖性带来的复杂性和资源开销问题。

Conclusion: 本文不仅为爱好者提供实用参考，也为推动实时迁移技术在多样化计算环境中的实际部署提供方向。

Abstract: Live migration, a technology enabling seamless transition of operational computational entities between various hosts while preserving continuous functionality and client connectivity, has been the subject of extensive research. However, existing reviews often overlook critical technical aspects and practical challenges integral to the usage of live migration techniques in real-world scenarios. This work bridges this gap by integrating the aspects explored in existing reviews together with a comprehensive analysis of live migration technologies across multiple dimensions, with focus on migration techniques, migration units, and infrastructure characteristics. Despite efforts to make live migration widely accessible, its reliance on multiple system factors can create challenges. In certain cases, the complexities and resource demands outweigh the benefits, making its implementation hard to justify. The focus of this work is mainly on container based and virtual machine-based migration technologies, examining the current state of the art and the disparity in adoption between these two approaches. Furthermore, this work explores the impact of migration objectives and operational constraints on the usability and efficacy of existing technologies. By outlining current technical challenges and providing guidelines for future research and development directions, this work serves a dual purpose: first, to equip enthusiasts with a valuable resource on live migration, and second, to contribute to the advancement of live migration technologies and their practical implementation across diverse computing environments.

</details>


### [4] [Reducing Fragmentation and Starvation in GPU Clusters through Dynamic Multi-Objective Scheduling](https://arxiv.org/abs/2512.10980)
*Akhmadillo Mamirov*

Main category: cs.DC

TL;DR: 该论文提出三种动态调度器（HPS、PBS、SBS），在异构多租户GPU集群中显著提升利用率、吞吐量与公平性，超越静态调度策略。


<details>
  <summary>Details</summary>
Motivation: 当前GPU集群因碎片化、异构负载和静态调度限制导致平均利用率仅约50%，亟需更高效调度方案。

Method: 设计并评估三种动态调度器：混合优先级（HPS）、预测回填（PBS）和智能批处理（SBS），在64-GPU仿真环境中测试1000个AI任务。

Result: HPS实现最高利用率78.2%、吞吐量25.8 jobs/h、最低饥饿数12；PBS改善碎片处理达76.1%利用率；SBS提升结构相似任务效率至74.6%。

Conclusion: 动态多目标调度器在关键指标上全面优于单目标启发式方法，为未来生产级GPU调度框架提供实用基础。

Abstract: GPU clusters have become essential for training and deploying modern AI systems, yet real deployments continue to report average utilization near 50%. This inefficiency is largely caused by fragmentation, heterogeneous workloads, and the limitations of static scheduling policies. This work presents a systematic evaluation of these issues and introduces three specialized dynamic schedulers: Hybrid Priority (HPS), Predictive Backfill (PBS), and Smart Batch (SBS). These schedulers are designed to improve utilization, fairness, and overall throughput in multi-tenant GPU clusters. We evaluate all schedulers using a controlled simulation of 1,000 AI jobs on a 64-GPU, 8-node cluster that includes a realistic mix of training, inference, and research workloads. Static baselines (FIFO, SJF, Shortest, Shortest-GPU) achieve 45 to 67% GPU utilization and 12.5 to 18.3 jobs per hour and experience severe starvation, with as many as 156 jobs waiting longer than 30 minutes. The dynamic schedulers significantly outperform these policies. HPS achieves the highest utilization (78.2%), highest throughput (25.8 jobs per hour), and the lowest fairness variance among dynamic methods (457), reducing starvation to 12 jobs. PBS improves fragmentation handling and reaches 76.1% utilization, while SBS increases efficiency for structurally similar jobs and reaches 74.6% utilization. Across all key metrics, including throughput, job wait times, fairness variance, and starvation, dynamic multi-objective schedulers consistently outperform single-objective heuristics. These results show that targeted and transparent scheduling strategies can meaningfully increase GPU efficiency in heterogeneous AI clusters and provide a practical foundation for future production scheduling frameworks.

</details>


### [5] [Evaluation Framework for Centralized and Decentralized Aggregation Algorithm in Federated Systems](https://arxiv.org/abs/2512.10987)
*Sumit Chongder*

Main category: cs.DC

TL;DR: 本文比较了集中式分层联邦学习（HFL）与去中心化聚合联邦学习（AFL）及持续联邦学习（CFL），发现AFL和CFL在多个指标上优于HFL，凸显去中心化方法的优势。


<details>
  <summary>Details</summary>
Motivation: 解决HFL存在的通信瓶颈与隐私问题，探索更高效的去中心化联邦学习架构。

Method: 通过Fashion MNIST和MNIST数据集评估HFL、AFL和CFL在精度、召回率、F1分数和平衡准确率上的表现。

Result: AFL和CFL在各项性能指标上均优于HFL，证明其去中心化聚合机制更有效。

Conclusion: 去中心化联邦学习架构（如AFL和CFL）更适合分布式协作训练场景，为未来研究提供方向。

Abstract: In recent years, the landscape of federated learning has witnessed significant advancements, particularly in decentralized methodologies. This research paper presents a comprehensive comparison of Centralized Hierarchical Federated Learning (HFL) with Decentralized Aggregated Federated Learning (AFL) and Decentralized Continual Federated Learning (CFL) architectures. While HFL, in its centralized approach, faces challenges such as communication bottlenecks and privacy concerns due to centralized data aggregation, AFL and CFL provide promising alternatives by distributing computation and aggregation processes across devices. Through evaluation of Fashion MNIST and MNIST datasets, this study demonstrates the advantages of decentralized methodologies, showcasing how AFL and CFL outperform HFL in precision, recall, F1 score, and balanced accuracy. The analysis highlights the importance of decentralized aggregation mechanisms in AFL and CFL, which effectively enables collaborative model training across distributed devices. This comparative study contributes valuable insights into the evolving landscape of federated learning, guiding researchers and practitioners towards decentralized methodologies for enhanced performance in collaborative model training scenarios.

</details>


### [6] [Theoretical Foundations of GPU-Native Compilation for Rapid Code Iteration](https://arxiv.org/abs/2512.11200)
*Adilet Metinov,Gulida M. Kudakeeva,Gulnara D. Kabaeva*

Main category: cs.DC

TL;DR: 本文提出三种GPU原生编译方法以消除CPU-GPU数据传输瓶颈，显著提升AI代码生成速度。


<details>
  <summary>Details</summary>
Motivation: 当前AI代码生成系统因编译、执行和测试阶段的CPU-GPU数据传输导致严重延迟。

Method: 提出并分析三种方法：传统并行编译适配GPU、神经序列翻译编译加概率验证、以及两者混合架构。

Result: 理论推导显示传统方法提速2-5倍，神经方法达10-100倍，混合方案兼顾正确性与实用性。

Conclusion: 该框架支持在精度与并行探索间权衡，为自改进AI系统及未来模拟计算提供新路径。

Abstract: Current AI code generation systems suffer from significant latency bottlenecks due to CPU-GPU data transfers during compilation, execution, and testing phases. We establish theoretical foundations for three complementary approaches to GPU-native compilation that eliminate these transfers: (1) parallel traditional compilation adapted for GPU execution, (2) neural compilation using learned sequence-to-sequence translation with probabilistic verification, and (3) hybrid architectures combining both strategies. We derive latency and energy bounds demonstrating potential speedups of 10-100x for code iteration cycles. Our analysis shows that traditional GPU compilation provides 2-5x improvements through transfer elimination, neural compilation achieves 10-100x speedups via massive parallelism, and hybrid approaches offer practical deployment paths with guaranteed correctness. We formalize the probabilistic verification framework that enables trading compilation accuracy for parallel exploration, and discuss implications for self-improving AI systems and future analog computing substrates.

</details>


### [7] [RollMux: Phase-Level Multiplexing for Disaggregated RL Post-Training](https://arxiv.org/abs/2512.11306)
*Tianyuan Wu,Lunxi Cao,Yining Wei,Wei Gao,Yuheng Zhao,Dakai An,Shaopan Xiong,Zhiqiang Lv,Ju Huang,Siran Yang,Yinghao Yu,Jiamang Wang,Lin Qu,Wei Wang*

Main category: cs.DC

TL;DR: RollMux 是一种通过跨集群调度消除资源空闲气泡的强化学习后训练框架，显著提升成本效率并保证服务等级目标达成。


<details>
  <summary>Details</summary>
Motivation: 解决因策略同步导致的集群间资源闲置问题，提升硬件利用率。

Method: 引入 co-execution group 抽象，结合两层调度架构（组间保守随机规划 + 组内最优轮询）与驻留约束实现高效协同。

Result: 在 328 H20 + 328 H800 GPU 测试平台上，成本效率较标准方案提升 1.84 倍，较先进共置方案提升 1.38 倍，SLO 达成率 100%。

Conclusion: RollMux 有效利用结构化空闲资源，为强化学习后训练提供高性价比、高可靠性的集群调度方案。

Abstract: Rollout-training disaggregation is emerging as the standard architecture for Reinforcement Learning (RL) post-training, where memory-bound rollout and compute-bound training are physically disaggregated onto purpose-built clusters to maximize hardware efficiency. However, the strict synchronization required by on-policy algorithms introduces severe dependency bubbles, forcing one cluster to idle while the dependent phase is running on the other. We present RollMux, a cluster scheduling framework that reclaims these bubbles through cross-cluster orchestration. RollMux is built on the insight that the structural idleness of one job can be effectively utilized by the active phase of another. To realize this, we introduce the co-execution group abstraction, which partitions the cluster into isolated locality domains. This abstraction enables a two-tier scheduling architecture: an inter-group scheduler that optimizes job placement using conservative stochastic planning, and an intra-group scheduler that orchestrates a provably optimal round-robin schedule. The group abstraction also imposes a residency constraint, ensuring that massive model states remain cached in host memory to enable "warm-star" context switching. We evaluate RollMux on a production-scale testbed with 328 H20 and 328 H800 GPUs. RollMux improves cost efficiency by 1.84x over standard disaggregation and 1.38x over state-of-the-art co-located baselines, all while achieving 100% SLO attainment.

</details>


### [8] [Parallax: Runtime Parallelization for Operator Fallbacks in Heterogeneous Edge Systems](https://arxiv.org/abs/2512.11532)
*Chong Tang,Hao Dai,Jagmohan Chauhan*

Main category: cs.DC

TL;DR: Parallax 是一种无需重构模型或自定义算子即可加速移动端 DNN 推理的框架，通过计算图划分、分支感知内存管理和自适应调度显著降低延迟与能耗。


<details>
  <summary>Details</summary>
Motivation: 现有框架在处理动态控制流和不支持算子时导致 CPU 空闲、高延迟和内存峰值，难以满足实时 DNN 应用需求。

Method: Parallax 采用计算 DAG 分区、专用内存池与缓冲复用、自适应调度及细粒度子图控制实现异构推理。

Result: 在三款设备上测试五种 DNN 模型，延迟最高降低 46%，平均内存开销仅增 26.5%，能耗最多节省 30%。

Conclusion: Parallax 有效提升移动端实时推理响应能力，优于现有主流框架。

Abstract: The growing demand for real-time DNN applications on edge devices necessitates faster inference of increasingly complex models. Although many devices include specialized accelerators (e.g., mobile GPUs), dynamic control-flow operators and unsupported kernels often fall back to CPU execution. Existing frameworks handle these fallbacks poorly, leaving CPU cores idle and causing high latency and memory spikes. We introduce Parallax, a framework that accelerates mobile DNN inference without model refactoring or custom operator implementations. Parallax first partitions the computation DAG to expose parallelism, then employs branch-aware memory management with dedicated arenas and buffer reuse to reduce runtime footprint. An adaptive scheduler executes branches according to device memory constraints, meanwhile, fine-grained subgraph control enables heterogeneous inference of dynamic models. By evaluating on five representative DNNs across three different mobile devices, Parallax achieves up to 46% latency reduction, maintains controlled memory overhead (26.5% on average), and delivers up to 30% energy savings compared with state-of-the-art frameworks, offering improvements aligned with the responsiveness demands of real-time mobile inference.

</details>


### [9] [Stateless Snowflake: A Cloud-Agnostic Distributed ID Generator Using Network-Derived Identity](https://arxiv.org/abs/2512.11643)
*Manideep Reddy Chinthareddy*

Main category: cs.DC

TL;DR: 提出一种无需手动分配工作节点ID的云原生分布式ID生成方案，利用容器IP地址作为熵源，实现无状态、高扩展性ID生成。


<details>
  <summary>Details</summary>
Motivation: 解决传统Snowflake算法在容器化环境中依赖中心化协调或有状态管理的问题。

Method: 通过容器私有IPv4地址派生唯一节点标识，采用1-41-16-6位分配方案，确保单调性和去中心化。

Result: 在AWS/GCP/Azure实测中单集群达31,000 TPS，接近传统方案性能，同时支持近乎无限横向扩展。

Conclusion: 该方案消除了对稳定工作节点ID的依赖，在保持高性能的同时显著简化了云原生环境下的部署运维复杂度。

Abstract: Snowflake-style distributed ID generators are the industry standard for producing k-ordered, unique identifiers at scale. However, the traditional requirement for manually assigned or centrally coordinated worker IDs introduces significant friction in modern container-orchestrated environments (e.g., Kubernetes), where workloads are ephemeral and autoscaled. In such systems, maintaining stable worker identities requires complex stateful sets or external coordination services (e.g., ZooKeeper), negating the operational benefits of stateless microservices.
  This paper presents a cloud-agnostic, container-native ID generation protocol that eliminates the dependency on explicit worker IDs. By deriving node uniqueness deterministically from ephemeral network properties - specifically the container's private IPv4 address - the proposed method removes the need for centralized coordination. We introduce a modified bit-allocation scheme (1-41-16-6) that accommodates 16 bits of network-derived entropy while preserving strict monotonicity. We validate the approach across AWS, GCP, and Azure environments. Evaluation results demonstrate that while the design has a theoretical single-node ceiling of approximately 64,000 TPS, in practical microservice deployments the network I/O dominates latency, resulting in end-to-end performance (approximately 31,000 TPS on a 3-node cluster) comparable to classic stateful generators while offering effectively unbounded horizontal scalability.

</details>


### [10] [ECCO: Leveraging Cross-Camera Correlations for Efficient Live Video Continuous Learning](https://arxiv.org/abs/2512.11727)
*Yuze He,Ferdi Kossmann,Srinivasan Seshan,Peter Steenkiste*

Main category: cs.DC

TL;DR: ECCO是一种面向视频分析的高效持续学习框架，通过动态分组和资源共享降低计算与通信开销。


<details>
  <summary>Details</summary>
Motivation: 当前为每个摄像头单独重训练模型成本高、不可扩展，亟需资源高效的解决方案。

Method: ECCO提出轻量分组算法、GPU资源分配器和传输控制器，利用时空相关性共享模型。

Result: 在三个数据集上，ECCO在相同资源下提升6.7%-18.1%准确率，或支持3.3倍并发摄像头。

Conclusion: ECCO显著提升视频分析系统在持续学习场景下的效率与可扩展性。

Abstract: Recent advances in video analytics address real-time data drift by continuously retraining specialized, lightweight DNN models for individual cameras. However, the current practice of retraining a separate model for each camera suffers from high compute and communication costs, making it unscalable. We present ECCO, a new video analytics framework designed for resource-efficient continuous learning. The key insight is that the data drift, which necessitates model retraining, often shows temporal and spatial correlations across nearby cameras. By identifying cameras that experience similar drift and retraining a shared model for them, ECCO can substantially reduce the associated compute and communication costs. Specifically, ECCO introduces: (i) a lightweight grouping algorithm that dynamically forms and updates camera groups; (ii) a GPU allocator that dynamically assigns GPU resources across different groups to improve retraining accuracy and ensure fairness; and (iii) a transmission controller at each camera that configures frame sampling and coordinates bandwidth sharing with other cameras based on its assigned GPU resources. We conducted extensive evaluations on three distinctive datasets for two vision tasks. Compared to leading baselines, ECCO improves retraining accuracy by 6.7%-18.1% using the same compute and communication resources, or supports 3.3 times more concurrent cameras at the same accuracy.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [11] [PD-Swap: Prefill-Decode Logic Swapping for End-to-End LLM Inference on Edge FPGAs via Dynamic Partial Reconfiguration](https://arxiv.org/abs/2512.11550)
*Yifan Zhang,Zhiheng Chen,Ye Qiao,Sitao Huang*

Main category: cs.AR

TL;DR: PD-Swap是一种基于动态部分重构的LLM加速器，通过分离预填充与解码阶段优化边缘FPGA上的长上下文推理性能。


<details>
  <summary>Details</summary>
Motivation: 解决边缘设备上长序列LLM推理因二次复杂度和KV缓存带宽压力导致的性能骤降问题。

Method: 采用动态部分重构技术，在FPGA上时间复用注意力模块，分别部署计算密集型预填充引擎和带宽优化型解码引擎。

Result: 在不增加面积开销前提下，解码吞吐达27 tokens/s，较现有方案提升1.3x–2.1x，且长上下文优势更显著。

Conclusion: PD-Swap有效缓解了静态架构资源浪费与性能瓶颈，为边缘端高效部署超长上下文LLM提供可行方案。

Abstract: Aggressively quantized large language models (LLMs), such as BitNet-style 1.58-bit Transformers with ternary weights, make it feasible to deploy generative AI on low-power edge FPGAs. However, as prompts grow to tens of thousands of tokens, edge hardware performance drops sharply with sequence length due to quadratic prefill cost and rapidly increasing KV-cache bandwidth demands, making inference latency of longer context length a first-order system concern. Recent studies on LLMs expose a fundamental prefill-decode asymmetry: prefill is compute-bound and dominated by dense matrix-matrix operations, whereas decoding is memory-bandwidth-bound and dominated by KV-cache traffic. A static accelerator must provision resources and a single dataflow for both regimes, leading to duplicated attention logic, underutilized fabric, and tight LUT/URAM limits that cap model size and usable context. We propose a prefill--decode disaggregated LLM accelerator, PD-Swap, that uses Dynamic Partial Reconfiguration (DPR) to time-multiplex the attention module on edge FPGAs. The core table-lookup ternary matrix multiplication and weight-buffering engines remain static, while the attention subsystem is a reconfigurable partition with two phase-specialized architectures: a compute-heavy, token-parallel prefill engine and a bandwidth-optimized, KV-cache-centric decoding engine. A roofline-inspired model and design space exploration jointly optimize reconfigurable-region size, parallelism under reconfiguration and routability constraints, and reconfiguration latency is hidden by computation latency. PD-Swap achieves up to 27~tokens/s decoding throughput, outperforming prior state-of-the-art works by 1.3x--2.1x (larger gains at longer context lengths), without extra area cost.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [12] [Coverage Isn't Enough: SBFL-Driven Insights into Manually Created vs. Automatically Generated Tests](https://arxiv.org/abs/2512.11223)
*Sasara Shimizu,Yoshiki Higo*

Main category: cs.SE

TL;DR: 本研究比较了自动生成测试与手动创建测试在SBFL得分和代码覆盖率上的表现，发现自动生成测试覆盖率更高但故障定位能力较弱。


<details>
  <summary>Details</summary>
Motivation: 减少开发者负担并提升测试效率，探索自动生成测试在故障定位中的实际效果。

Method: 使用SBFL评分和分支覆盖率作为评估指标，对比自动生成与手动测试的效果。

Result: 自动生成测试分支覆盖率更高，但SBFL得分较低，尤其在深层嵌套代码中表现更差。

Conclusion: 应结合自动与手动测试方法，以兼顾覆盖率与故障定位能力。

Abstract: The testing phase is an essential part of software development, but manually creating test cases can be time-consuming. Consequently, there is a growing need for more efficient testing methods. To reduce the burden on developers, various automated test generation tools have been developed, and several studies have been conducted to evaluate the effectiveness of the tests they produce. However, most of these studies focus primarily on coverage metrics, and only a few examine how well the tests support fault localization-particularly using artificial faults introduced through mutation testing. In this study, we compare the SBFL (Spectrum-Based Fault Localization) score and code coverage of automatically generated tests with those of manually created tests. The SBFL score indicates how accurately faults can be localized using SBFL techniques. By employing SBFL score as an evaluation metric-an approach rarely used in prior studies on test generation-we aim to provide new insights into the respective strengths and weaknesses of manually created and automatically generated tests. Our experimental results show that automatically generated tests achieve higher branch coverage than manually created tests, but their SBFL score is lower, especially for code with deeply nested structures. These findings offer guidance on how to effectively combine automatically generated and manually created testing approaches.

</details>


### [13] [REMODEL-LLM: Transforming C code to Java using LLMs](https://arxiv.org/abs/2512.11402)
*Aryan Gupta,Y. Raghu Reddy*

Main category: cs.SE

TL;DR: 本文评估了19个小型量化LLM在C到Java代码翻译任务中的表现，发现仅有3个模型表现尚可，但仍无法处理复杂C语言特性。


<details>
  <summary>Details</summary>
Motivation: 自动化C到Java代码翻译因范式、内存模型和数据类型差异而极具挑战，需探索小型LLM在此任务中的能力边界。

Method: 采用基于AST语义分解与强约束规则提示的混合流水线，对19个参数小于20B的量化LLM进行系统评估。

Result: 绝大多数模型完全失败；少数生成可运行代码但语义错误严重；仅phi4、deepseek-coder-v2、codeqwen三者通过超50%测试，仍无法处理函数指针等复杂结构。

Conclusion: 当前量化LLM在C到Java翻译中存在明显能力上限，尤其在处理底层C语言概念时表现不足，难以满足工业级需求。

Abstract: The automated translation of C code to Java code is a notoriously difficult task, fraught with challenges stemming from fundamental paradigm shifts (procedural vs. Object Oriented), memory models (manual pointers vs. Garbage Collection), and incompatible data types. This paper investigates the efficacy of 19 small, quantized LLMs (under 20 billion parameters) for the C to Java translation task. We use a novel, hybrid pipeline that leverages Abstract Syntax Trees (ASTs) for semantic decomposition and employs a highly constrained, rule based prompting strategy. The results are stark: a clear multi tiered performance divide emerged. The vast majority of models (Tier 3, e.g., llama3.1, gemma3, starcoder2) failed 100\% of the tests, proving incapable of generating even basic, runnable Java boilerplate. A small middle tier (Tier 2, e.g., mistral-nemo and mistral) produced runnable code but was plagued by dangerous semantic failures and wrong translations. Only three models (Tier 1: phi4, deepseek-coder-v2, codeqwen) proved viable, passing over 50\% of the test suite. Even these top models failed on the most complex C concepts, such as function pointers, sizeof, and enum logic, revealing a hard ceiling for the reasoning capabilities of current quantized models.

</details>


### [14] [Towards Privacy-Preserving Code Generation: Differentially Private Code Language Models](https://arxiv.org/abs/2512.11482)
*Melih Catal,Pooja Rani,Harald C. Gall*

Main category: cs.SE

TL;DR: 本文首次系统评估差分隐私（DP）在代码大语言模型（CodeLLMs）中的应用，发现DP能有效降低模型对训练数据的无意识记忆，同时保持甚至提升代码生成能力，且不影响训练效率与能耗。


<details>
  <summary>Details</summary>
Motivation: CodeLLMs存在无意识记忆训练数据的风险，可能导致隐私泄露和知识产权侵犯，限制其在敏感领域的部署。

Method: 引入差分隐私机制，在训练过程中加入校准噪声，实证评估其对记忆缓解与代码生成能力的影响。

Result: DP显著降低各类代码片段的记忆风险，对易记忆片段效果尤佳；轻微增加困惑度但不损害、甚至增强代码生成能力；对训练时间和能耗影响微乎其微。

Conclusion: DP是实现隐私保护型CodeLLMs训练的实用方案，兼顾安全性与实用性。

Abstract: Large language models specialized for code (CodeLLMs) have demonstrated remarkable capabilities in generating code snippets, documentation, and test cases. However, despite their promising capabilities, CodeLLMs can inadvertently memorize and reproduce snippets from their training data, which poses risks of privacy breaches and intellectual property violations. These risks restrict the deployment of CodeLLMs in sensitive domains and limit their training datasets to publicly available sources. To mitigate the memorization risk without compromising their task performance, we apply Differential Privacy (DP) to CodeLLMs. To the best of our knowledge, this is the first comprehensive study that systematically evaluates the effectiveness of DP in CodeLLMs. DP adds calibrated noise to the training process to protect individual data points while still allowing the model to learn useful patterns. To this end, we first identify and understand the driving reasons of the memorization behaviour of the CodeLLMs during their fine-tuning. Then, to address this issue, we empirically evaluate the effect of DP on mitigating memorization while preserving code generation capabilities. Our findings show that DP substantially reduces memorization in CodeLLMs across all the tested snippet types. The snippet types most prone to memorization are also the most effectively mitigated by DP. Furthermore, we observe that DP slightly increases perplexity but preserves, and can even enhance, the code generation capabilities of CodeLLMs, which makes it feasible to apply DP in practice without significantly compromising model utility. Finally, we analyze the impact of DP on training efficiency and energy consumption, finding that DP does not significantly affect training time or energy usage, making it a practical choice for privacy-preserving CodeLLMs training.

</details>


### [15] [Mini-SFC: A Comprehensive Simulation Framework for Orchestration and Management of Service Function Chains](https://arxiv.org/abs/2512.11527)
*Xi Wang,Shuo Shi,Chenyu Wu*

Main category: cs.SE

TL;DR: Mini-SFC是一个支持数值与容器仿真、动态拓扑调整的开源模块化SFC仿真框架，旨在简化算法验证与服务部署测试。


<details>
  <summary>Details</summary>
Motivation: 现有SFC仿真工具存在局限性，需要更灵活、易用且可扩展的平台支持研究与实践。

Method: 设计模块化架构，提供标准化求解器接口，支持数值仿真与容器化虚拟仿真及在线动态拓扑调整。

Result: 显著降低研究人员学习成本，提升SFC管理与优化的灵活性和可扩展性。

Conclusion: Mini-SFC为SFC相关研究与部署提供了高效、开放、用户友好的仿真平台。

Abstract: In the continuously evolving cloud computing and network environment, service function chain (SFC) plays a crucial role in implementing complex services in the network with its flexible deployment capabilities. To address the limitations of existing SFC simulation tools, this paper introduces Mini-SFC, a modular simulation framework that supports both numerical and container-based virtual simulations, while also supporting online dynamic topology adjustments. As an open-source platform emphasizing user-friendliness, Mini-SFC facilitates rapid algorithm verification and realistic service deployment validation. By simplifying module design and providing standardized solver interfaces, Mini-SFC significantly shortens the learning curve for researchers and enhances the flexibility and scalability required for advanced SFC management and optimization. For readers interested in exploring or utilizing Mini-SFC, more information is available on the official project page.

</details>
