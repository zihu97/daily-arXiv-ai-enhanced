<div id=toc></div>

# Table of Contents

- [cs.SE](#cs.SE) [Total: 10]
- [cs.NI](#cs.NI) [Total: 2]
- [cs.MA](#cs.MA) [Total: 2]
- [cs.DC](#cs.DC) [Total: 6]
- [cs.AR](#cs.AR) [Total: 2]


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [1] [SpIDER: Spatially Informed Dense Embedding Retrieval for Software Issue Localization](https://arxiv.org/abs/2512.16956)
*Shravan Chaudhari,Rahul Thomas Jacob,Mononito Goswami,Jiajun Cao,Shihab Rashid,Christian Bock*

Main category: cs.SE

TL;DR: SpIDER是一种结合图结构探索和LLM推理的增强型稠密检索方法，显著提升代码检索性能。


<details>
  <summary>Details</summary>
Motivation: 现有稠密检索方法缺乏对代码库图结构的探索和利用，限制了检索效果。

Method: 提出SpIDER方法，通过图结构探索获取辅助上下文，并结合LLM推理增强稠密检索。

Result: 实验表明，SpIDER在多种编程语言中均能稳定提升稠密检索性能。

Conclusion: SpIDER有效弥补了现有方法对代码库结构信息利用不足的问题，是更优的代码检索方案。

Abstract: Retrieving code units (e.g., files, classes, functions) that are semantically relevant to a given user query, bug report, or feature request from large codebases is a fundamental challenge for LLM-based coding agents. Agentic approaches typically employ sparse retrieval methods like BM25 or dense embedding strategies to identify relevant units. While embedding-based approaches can outperform BM25 by large margins, they often lack exploration of the codebase and underutilize its underlying graph structure. To address this, we propose SpIDER (Spatially Informed Dense Embedding Retrieval), an enhanced dense retrieval approach that incorporates LLM-based reasoning over auxiliary context obtained through graph-based exploration of the codebase. Empirical results show that SpIDER consistently improves dense retrieval performance across several programming languages.

</details>


### [2] [Bridging Natural Language and Formal Specification--Automated Translation of Software Requirements to LTL via Hierarchical Semantics Decomposition Using LLMs](https://arxiv.org/abs/2512.17334)
*Zhi Ma,Cheng Wen,Zhexin Su,Xiao Liang,Cong Tian,Shengchao Qin,Mengfei Yang*

Main category: cs.SE

TL;DR: Req2LTL框架通过中间表示OnionL结合LLM与规则方法，高效准确地将自然语言需求转为LTL形式化规范。


<details>
  <summary>Details</summary>
Motivation: 现有方法难以处理工业级需求的复杂性与歧义性，阻碍形式化验证在关键领域的规模化应用。

Method: 提出模块化框架Req2LTL，利用LLM进行语义分解，结合确定性规则合成确保语法正确与语义保真。

Result: 在航空航天真实需求上达到88.4%语义准确率与100%语法正确率，显著优于现有方法。

Conclusion: Req2LTL有效弥合自然语言与形式逻辑间的鸿沟，为工业级形式化验证提供实用解决方案。

Abstract: Automating the translation of natural language (NL) software requirements into formal specifications remains a critical challenge in scaling formal verification practices to industrial settings, particularly in safety-critical domains. Existing approaches, both rule-based and learning-based, face significant limitations. While large language models (LLMs) like GPT-4o demonstrate proficiency in semantic extraction, they still encounter difficulties in addressing the complexity, ambiguity, and logical depth of real-world industrial requirements. In this paper, we propose Req2LTL, a modular framework that bridges NL and Linear Temporal Logic (LTL) through a hierarchical intermediate representation called OnionL. Req2LTL leverages LLMs for semantic decomposition and combines them with deterministic rule-based synthesis to ensure both syntactic validity and semantic fidelity. Our comprehensive evaluation demonstrates that Req2LTL achieves 88.4% semantic accuracy and 100% syntactic correctness on real-world aerospace requirements, significantly outperforming existing methods.

</details>


### [3] [What You Trust Is Insecure: Demystifying How Developers (Mis)Use Trusted Execution Environments in Practice](https://arxiv.org/abs/2512.17363)
*Yuqing Niu,Jieke Shi,Ruidong Han,Ye Liu,Chengyan Ma,Yunbo Lyu,David Lo*

Main category: cs.SE

TL;DR: 本文首次对241个开源TEE项目进行大规模实证研究，揭示开发者在物联网安全、AI模型保护等领域的实际使用模式及常见安全缺陷。


<details>
  <summary>Details</summary>
Motivation: 了解开发者如何实际使用TEE，以改进SDK可用性并支持可信软件开发。

Method: 结合人工检查与定制静态分析脚本，从GitHub收集并分析使用Intel SGX和ARM TrustZone的241个项目。

Result: 发现30%项目用于物联网安全，32.4%项目自行实现加密功能，25.3%项目存在硬编码密钥等不安全行为。

Conclusion: 当前TEE SDK可用性不足，需优化以减少开发者误用并提升整体安全性。

Abstract: Trusted Execution Environments (TEEs), such as Intel SGX and ARM TrustZone, provide isolated regions of CPU and memory for secure computation and are increasingly used to protect sensitive data and code across diverse application domains. However, little is known about how developers actually use TEEs in practice. This paper presents the first large-scale empirical study of real-world TEE applications. We collected and analyzed 241 open-source projects from GitHub that utilize the two most widely-adopted TEEs, Intel SGX and ARM TrustZone. By combining manual inspection with customized static analysis scripts, we examined their adoption contexts, usage patterns, and development practices across three phases. First, we categorized the projects into 8 application domains and identified trends in TEE adoption over time. We found that the dominant use case is IoT device security (30%), which contrasts sharply with prior academic focus on blockchain and cryptographic systems (7%), while AI model protection (12%) is rapidly emerging as a growing domain. Second, we analyzed how TEEs are integrated into software and observed that 32.4% of the projects reimplement cryptographic functionalities instead of using official SDK APIs, suggesting that current SDKs may have limited usability and portability to meet developers' practical needs. Third, we examined security practices through manual inspection and found that 25.3% (61 of 241) of the projects exhibit insecure coding behaviors when using TEEs, such as hardcoded secrets and missing input validation, which undermine their intended security guarantees. Our findings have important implications for improving the usability of TEE SDKs and supporting developers in trusted software development.

</details>


### [4] [LLM-based Behaviour Driven Development for Hardware Design](https://arxiv.org/abs/2512.17814)
*Rolf Drechsler,Qian Liu*

Main category: cs.SE

TL;DR: 本文探讨利用大语言模型（LLM）自动化支持硬件设计中的行为驱动开发（BDD），以降低测试与验证的复杂性。


<details>
  <summary>Details</summary>
Motivation: 硬件设计中测试与验证复杂度随系统规模增长，而BDD尚未在该领域广泛应用，主要因需手动从文本规范提取行为场景。

Method: 研究基于LLM的技术，用于自动化生成硬件设计中的行为场景。

Result: 初步表明LLM有潜力辅助硬件BDD流程，减少人工干预。

Conclusion: LLM为硬件设计中的BDD提供了可行的自动化路径，有望提升效率并推动其实际应用。

Abstract: Test and verification are essential activities in hardware and system design, but their complexity grows significantly with increasing system sizes. While Behavior Driven Development (BDD) has proven effective in software engineering, it is not yet well established in hardware design, and its practical use remains limited. One contributing factor is the manual effort required to derive precise behavioral scenarios from textual specifications.
  Recent advances in Large Language Models (LLMs) offer new opportunities to automate this step. In this paper, we investigate the use of LLM-based techniques to support BDD in the context of hardware design.

</details>


### [5] [CIFE: Code Instruction-Following Evaluation](https://arxiv.org/abs/2512.17387)
*Sravani Gunnu,Shanmukha Guttula,Hima Patel*

Main category: cs.SE

TL;DR: 提出包含1000个Python任务的基准，评估大语言模型在代码生成中对开发者约束的遵守情况，并引入C2A评分衡量正确性与约束符合度。


<details>
  <summary>Details</summary>
Motivation: 现有基准主要关注功能正确性，忽视开发者对鲁棒性、格式和安全等显式约束的遵循需求。

Method: 通过人机协作四阶段流程构建含13类约束的任务集，评估14个开源与闭源模型，设计C2A综合评分。

Result: 强模型部分符合率达90%以上，但严格符合率仅39-66%，显示当前模型在精确遵循意图方面仍有显著差距。

Conclusion: 可信代码生成需兼顾功能正确性与开发者意图的稳定遵循。

Abstract: Large Language Models (LLMs) are increasingly applied to real-world code generation, where functional correctness alone is insufficient for reliable deployment, developers also expect adherence to explicit requirements for robustness, formatting, and security. Existing benchmarks primarily assess correctness through test-case execution, offering limited insight into how reliably models follow such constraints. We introduce a benchmark of 1,000 Python tasks, each paired with an average of 7 developer-specified constraints spanning 13 categories. Constraints are curated through a four-stage human-LLM pipeline to ensure they are atomic, relevant, and objective. We evaluate 14 open- and closed-source models using complementary adherence metrics and propose the C2A Score, a composite measure that jointly captures correctness and constraint compliance. Results reveal a substantial gap between partial and strict satisfaction, while strong models achieve over 90% partial adherence, strict adherence remains between 39-66%. These findings highlight that trustworthy code generation requires not only correctness but also consistent adherence to developer intent.

</details>


### [6] [SWE-Bench++: A Framework for the Scalable Generation of Software Engineering Benchmarks from Open-Source Repositories](https://arxiv.org/abs/2512.17419)
*Lilin Wang,Lucas Ramalho,Alan Celestino,Phuc Anthony Pham,Yu Liu,Umang Kumar Sinha,Andres Portillo,Onassis Osunwa,Gabriel Maduekwe*

Main category: cs.SE

TL;DR: SWE-Bench++是一个自动化框架，从GitHub拉取请求中生成跨11种语言的代码任务，用于评估和训练大语言模型在仓库级编程任务上的能力。


<details>
  <summary>Details</summary>
Motivation: 现有基准如SWE-bench受限于人工整理、静态数据集和仅支持Python，亟需更通用、多语言、可扩展的评估方案。

Method: 通过四个阶段：程序化采集、环境合成、测试预言提取和质量保证，将真实GitHub PR转化为可执行任务，并加入提示引导轨迹合成用于训练。

Result: 构建包含11,133个实例的多语言基准，在1,782个子集上Claude Sonnet 4.5表现最佳（36.20% pass@10），且微调后能显著提升多语言SWE-bench表现。

Conclusion: SWE-Bench++提供了一个可扩展、多语言的仓库级代码生成评估与训练基准，推动LLM在真实软件工程场景中的进步。

Abstract: Benchmarks like SWE-bench have standardized the evaluation of Large Language Models (LLMs) on repository-level software engineering tasks. However, these efforts remain limited by manual curation, static datasets, and a focus on Python-based bug fixes. We introduce SWE-Bench++, an automated framework that generates repository-level coding tasks from open-source GitHub projects. Unlike synthetic approaches, our pipeline harvests live pull requests to cover both bug fixes and feature requests across 11 languages. SWE-Bench++ turns GitHub pull requests (PRs) into reproducible, execution-based tasks via four stages: programmatic sourcing, environment synthesis, test oracle extraction, and quality assurance. A final hint-guided trajectory synthesis step converts instances that strong models fail on into training trajectories. Our initial benchmark consists of 11,133 instances from 3,971 repositories across 11 languages. On a subset of 1,782 instances of this benchmark, today's strongest models perform as follows: claude-sonnet-4.5 achieves 36.20% pass@10, gpt-5-2025-08-07 34.57%, gemini/gemini-2.5-pro 24.92%, and gpt-4o 16.89%. We further demonstrate the utility of our dataset by showing that fine-tuning on SWE-Bench++ instances yields measurable improvements on the SWE-bench Multilingual benchmark. SWE-Bench++ provides a scalable, multilingual benchmark for evaluating and improving repository-level code generation.

</details>


### [7] [An Investigation on How AI-Generated Responses Affect SoftwareEngineering Surveys](https://arxiv.org/abs/2512.17455)
*Ronnie de Souza Santos,Italo Santos,Maria Teresa Baldassarre,Cleyton Magalhaes,Mairieli Wessel*

Main category: cs.SE

TL;DR: 本研究探讨了大语言模型（LLM）在软件工程调查中的滥用现象及其对数据真实性、有效性和研究诚信的影响，并提出结合自动化与人工验证方法以保障调查可信度。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型的普及，受访者可能利用生成式工具伪造或操纵调查回答，威胁软件工程调查的数据完整性与研究有效性。

Method: 通过Prolific平台收集2025年两次调查数据，采用定性模式分析、叙事特征识别及Scribbr AI检测器自动筛查可疑回答。

Result: 在49份回答中发现重复句式、统一措辞和表面个性化等合成文本特征，这些虚假叙述损害了构念效度、内部效度和外部效度。

Conclusion: 数据真实性已成为软件工程调查的新效度维度，需结合透明报告、社区标准与混合验证机制防范AI生成内容，维护研究可信度。

Abstract: Survey research is a fundamental empirical method in software engineering, enabling the systematic collection of data on professional practices, perceptions, and experiences. However, recent advances in large language models (LLMs) have introduced new risks to survey integrity, as participants can use generative tools to fabricate or manipulate their responses. This study explores how LLMs are being misused in software engineering surveys and investigates the methodological implications of such behavior for data authenticity, validity, and research integrity. We collected data from two survey deployments conducted in 2025 through the Prolific platform and analyzed the content of participants' answers to identify irregular or falsified responses. A subset of responses suspected of being AI generated was examined through qualitative pattern inspection, narrative characterization, and automated detection using the Scribbr AI Detector. The analysis revealed recurring structural patterns in 49 survey responses indicating synthetic authorship, including repetitive sequencing, uniform phrasing, and superficial personalization. These false narratives mimicked coherent reasoning while concealing fabricated content, undermining construct, internal, and external validity. Our study identifies data authenticity as an emerging dimension of validity in software engineering surveys. We emphasize that reliable evidence now requires combining automated and interpretive verification procedures, transparent reporting, and community standards to detect and prevent AI generated responses, thereby protecting the credibility of surveys in software engineering.

</details>


### [8] [When Data Quality Issues Collide: A Large-Scale Empirical Study of Co-Occurring Data Quality Issues in Software Defect Prediction](https://arxiv.org/abs/2512.17460)
*Emmanuel Charleson Dapaah,Jens Grabowski*

Main category: cs.SE

TL;DR: 该研究首次大规模分析了软件缺陷预测中五种数据质量问题的共现及其对模型性能的影响，揭示了问题间的交互作用和性能阈值。


<details>
  <summary>Details</summary>
Motivation: 现有研究通常孤立分析单一数据问题，忽略了现实数据问题常共存且相互影响，导致模型效果受限。

Method: 使用可解释提升机与分层交互分析，在374个数据集和5个分类器上量化五种数据质量问题的直接与条件效应。

Result: 数据问题几乎普遍存在；类重叠最具破坏性；识别出各类问题的性能拐点；发现异常值在特定条件下反而提升性能；无单一模型在所有条件下均占优。

Conclusion: 需从孤立分析转向整体、数据驱动的理解，以更准确评估和提升软件缺陷预测模型在真实场景中的表现。

Abstract: Software Defect Prediction (SDP) models are central to proactive software quality assurance, yet their effectiveness is often constrained by the quality of available datasets. Prior research has typically examined single issues such as class imbalance or feature irrelevance in isolation, overlooking that real-world data problems frequently co-occur and interact. This study presents, to our knowledge, the first large-scale empirical analysis in SDP that simultaneously examines five co-occurring data quality issues (class imbalance, class overlap, irrelevant features, attribute noise, and outliers) across 374 datasets and five classifiers. We employ Explainable Boosting Machines together with stratified interaction analysis to quantify both direct and conditional effects under default hyperparameter settings, reflecting practical baseline usage.
  Our results show that co-occurrence is nearly universal: even the least frequent issue (attribute noise) appears alongside others in more than 93% of datasets. Irrelevant features and imbalance are nearly ubiquitous, while class overlap is the most consistently harmful issue. We identify stable tipping points around 0.20 for class overlap, 0.65-0.70 for imbalance, and 0.94 for irrelevance, beyond which most models begin to degrade. We also uncover counterintuitive patterns, such as outliers improving performance when irrelevant features are low, underscoring the importance of context-aware evaluation. Finally, we expose a performance-robustness trade-off: no single learner dominates under all conditions.
  By jointly analyzing prevalence, co-occurrence, thresholds, and conditional effects, our study directly addresses a persistent gap in SDP research. Hence, moving beyond isolated analyses to provide a holistic, data-aware understanding of how quality issues shape model performance in real-world settings.

</details>


### [9] [SGCR: A Specification-Grounded Framework for Trustworthy LLM Code Review](https://arxiv.org/abs/2512.17540)
*Kai Wang,Bingcheng Mao,Shuai Jia,Yujie Ding,Dongming Han,Tianyi Ma,Bin Cao*

Main category: cs.SE

TL;DR: SGCR框架通过结合显式规则和隐式启发式方法，显著提升LLM代码审查的可靠性与采纳率。


<details>
  <summary>Details</summary>
Motivation: 解决当前LLM在代码审查中缺乏可靠性、上下文感知和控制的问题。

Method: 提出双路径架构：显式路径确保符合人工规范，隐式路径发现并验证额外问题。

Result: 在工业环境中实现42%的开发者采纳率，较基线LLM提升90.9%。

Conclusion: 规范引导是弥合LLM生成能力与软件工程可靠性需求之间差距的有效范式。

Abstract: Automating code review with Large Language Models (LLMs) shows immense promise, yet practical adoption is hampered by their lack of reliability, context-awareness, and control. To address this, we propose Specification-Grounded Code Review (SGCR), a framework that grounds LLMs in human-authored specifications to produce trustworthy and relevant feedback. SGCR features a novel dual-pathway architecture: an explicit path ensures deterministic compliance with predefined rules derived from these specifications, while an implicit path heuristically discovers and verifies issues beyond those rules. Deployed in a live industrial environment at HiThink Research, SGCR's suggestions achieved a 42% developer adoption rate-a 90.9% relative improvement over a baseline LLM (22%). Our work demonstrates that specification-grounding is a powerful paradigm for bridging the gap between the generative power of LLMs and the rigorous reliability demands of software engineering.

</details>


### [10] [A Practical Solution to Systematically Monitor Inconsistencies in SBOM-based Vulnerability Scanners](https://arxiv.org/abs/2512.17710)
*Martin Rosso,Muhammad Asad Jahangir Jaffar,Alessandro Brighente,Mauro Conti*

Main category: cs.SE

TL;DR: 本文提出SVS-TEST工具，用于评估SBOM漏洞扫描工具在真实场景下的能力、成熟度与失效条件，发现多个工具存在静默失败问题。


<details>
  <summary>Details</summary>
Motivation: 当前SBOM漏洞扫描工具存在不一致行为和静默失败，导致误报漏报，亟需系统化评估方法。

Method: 设计并实现SVS-TEST方法与工具，通过16个精心构造的SBOM及真实数据，对7款主流SVS工具进行实证分析。

Result: 实验揭示不同工具在可靠性和错误处理上差异显著，部分工具对合法输入静默失败，带来虚假安全感。

Conclusion: 建议研究者与实践者使用SVS-TEST持续监控工具能力，并已向开发者披露全部结果，相关成果公开共享。

Abstract: Software Bill of Materials (SBOM) provides new opportunities for automated vulnerability identification in software products. While the industry is adopting SBOM-based Vulnerability Scanning (SVS) to identify vulnerabilities, we increasingly observe inconsistencies and unexpected behavior, that result in false negatives and silent failures. In this work, we present the background necessary to understand the underlying complexity of SVS and introduce SVS-TEST, a method and tool to analyze the capability, maturity, and failure conditions of SVS-tools in real-world scenarios. We showcase the utility of SVS-TEST in a case study evaluating seven real-world SVS-tools using 16 precisely crafted SBOMs and their respective ground truth. Our results unveil significant differences in the reliability and error handling of SVS-tools; multiple SVS-tools silently fail on valid input SBOMs, creating a false sense of security. We conclude our work by highlighting implications for researchers and practitioners, including how organizations and developers of SVS-tools can utilize SVS-TEST to monitor SVS capability and maturity. All results and research artifacts are made publicly available and all findings were disclosed to the SVS-tool developers ahead of time.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [11] [Enhancing AIGC Service Efficiency with Adaptive Multi-Edge Collaboration in A Distributed System](https://arxiv.org/abs/2512.17158)
*Changfu Xu,Jianxiong Guo,Jiandian Zeng,Houming Qiu,Tian Wang,Xiaowen Chu,Jiannong Cao*

Main category: cs.NI

TL;DR: 提出AMCoEdge方法，通过自适应多服务器协作MEC优化AIGC服务效率，显著降低延迟与失败率。


<details>
  <summary>Details</summary>
Motivation: 现有AIGC服务依赖中心化架构，响应延迟高；协作MEC方案灵活性与资源利用率不足。

Method: 设计基于深度强化学习的在线分布式算法，实现自适应多边缘服务器选择与动态负载分配。

Result: 仿真表明任务卸载跨度减少至少11.04%，失败率下降44.86%；原型系统实测延迟降低9.23%-31.98%。

Conclusion: AMCoEdge有效提升AIGC服务在边缘计算环境中的性能，兼顾低延迟与高可靠性。

Abstract: The Artificial Intelligence Generated Content (AIGC) technique has gained significant traction for producing diverse content. However, existing AIGC services typically operate within a centralized framework, resulting in high response times. To address this issue, we integrate collaborative Mobile Edge Computing (MEC) technology to reduce processing delays for AIGC services. Current collaborative MEC methods primarily support single-server offloading or facilitate interactions among fixed Edge Servers (ESs), limiting flexibility and resource utilization across all ESs to meet the varying computing and networking requirements of AIGC services. We propose AMCoEdge, an adaptive multi-server collaborative MEC approach to enhancing AIGC service efficiency. The AMCoEdge fully utilizes the computing and networking resources across all ESs through adaptive multi-ES selection and dynamic workload allocation, thereby minimizing the offloading make-span of AIGC services. Our design features an online distributed algorithm based on deep reinforcement learning, accompanied by theoretical analyses that confirm an approximate linear time complexity. Simulation results show that our method outperforms state-of-the-art baselines, achieving at least an 11.04% reduction in task offloading make-span and a 44.86% decrease in failure rate. Additionally, we develop a distributed prototype system to implement and evaluate our AMCoEdge method for real AIGC service execution, demonstrating service delays that are 9.23% - 31.98% lower than the three representative methods.

</details>


### [12] [Timely Information Updating for Mobile Devices Without and With ML Advice](https://arxiv.org/abs/2512.17381)
*Yu-Pin Hsu,Yi-Hsuan Tseng*

Main category: cs.NI

TL;DR: 提出一种在线算法，在对抗多种不确定性的同时优化信息更新成本与及时性之间的权衡，并结合机器学习建议实现最优一致性-鲁棒性平衡。


<details>
  <summary>Details</summary>
Motivation: 解决移动设备向接入点发送状态更新时，信息时效性与更新成本之间的基本权衡问题。

Method: 设计仅依赖可观测数据的在线算法，并引入未知可靠性的机器学习建议以增强性能。

Result: 算法渐近达到最优竞争比和一致性-鲁棒性权衡；最优算法对ML建议呈现‘全信或全不信’的阈值响应特性。

Conclusion: 理论分析与仿真实验表明，所提方法在对抗性和随机环境下均有效，且竞争比仅与更新成本范围线性相关。

Abstract: This paper investigates an information update system in which a mobile device monitors a physical process and sends status updates to an access point (AP). A fundamental trade-off arises between the timeliness of the information maintained at the AP and the update cost incurred at the device. To address this trade-off, we propose an online algorithm that determines when to transmit updates using only available observations. The proposed algorithm asymptotically achieves the optimal competitive ratio against an adversary that can simultaneously manipulate multiple sources of uncertainty, including the operation duration, the information staleness, the update cost, and the availability of update opportunities. Furthermore, by incorporating machine learning (ML) advice of unknown reliability into the design, we develop an ML-augmented algorithm that asymptotically attains the optimal consistency-robustness trade-off, even when the adversary can additionally corrupt the ML advice. The optimal competitive ratio scales linearly with the range of update costs, but is unaffected by other uncertainties. Moreover, an optimal competitive online algorithm exhibits a threshold-like response to the ML advice: it either fully trusts or completely ignores the ML advice, as partially trusting the advice cannot improve the consistency without severely degrading the robustness. Extensive simulations in stochastic settings further validate the theoretical findings in the adversarial environment.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [13] [On the Role of Contextual Information and Ego States in LLM Agent Behavior for Transactional Analysis Dialogues](https://arxiv.org/abs/2512.17060)
*Monika Zamojska,Jarosław A. Chudziak*

Main category: cs.MA

TL;DR: 本文提出一种基于交互分析理论的多智能体系统，通过划分Parent、Adult、Child三种自我状态并结合信息检索机制，提升LLM智能体在模拟人类行为时的心理深度与真实性。


<details>
  <summary>Details</summary>
Motivation: 当前LLM智能体缺乏心理深度和一致性，难以真实模拟人类思维模式与社会互动，因此需要引入心理学理论增强其行为真实性。

Method: 构建受交互分析理论启发的多智能体架构，每个智能体包含Parent、Adult、Child三个独立知识结构，并配备向量库支持的上下文信息检索机制，通过消融实验评估效果。

Result: 实验证明该架构能有效提升智能体对话的真实性，信息检索机制对表现有积极影响，为心理学驱动的智能体行为建模开辟新方向。

Conclusion: 融合交互分析理论与上下文检索的智能体架构显著增强了LLM多智能体系统在模拟人类心理和社会行为方面的表现力与可信度。

Abstract: LLM-powered agents are now used in many areas, from customer support to education, and there is increasing interest in their ability to act more like humans. This includes fields such as social, political, and psychological research, where the goal is to model group dynamics and social behavior. However, current LLM agents often lack the psychological depth and consistency needed to capture the real patterns of human thinking. They usually provide direct or statistically likely answers, but they miss the deeper goals, emotional conflicts, and motivations that drive real human interactions. This paper proposes a Multi-Agent System (MAS) inspired by Transactional Analysis (TA) theory. In the proposed system, each agent is divided into three ego states - Parent, Adult, and Child. The ego states are treated as separate knowledge structures with their own perspectives and reasoning styles. To enrich their response process, they have access to an information retrieval mechanism that allows them to retrieve relevant contextual information from their vector stores. This architecture is evaluated through ablation tests in a simulated dialogue scenario, comparing agents with and without information retrieval. The results are promising and open up new directions for exploring how psychologically grounded structures can enrich agent behavior. The contribution is an agent architecture that integrates Transactional Analysis theory with contextual information retrieval to enhance the realism of LLM-based multi-agent simulations.

</details>


### [14] [MAPPO-LCR: Multi-Agent Policy Optimization with Local Cooperation Reward in Spatial Public Goods Games](https://arxiv.org/abs/2512.17187)
*Zhaoqilin Yang,Axin Xiang,Kedi Yang,Tianjun Liu,Youliang Tian*

Main category: cs.MA

TL;DR: 本文首次将多智能体近端策略优化（MAPPO）引入空间公共物品博弈，提出MAPPO-LCR方法，通过局部合作奖励促进合作涌现并实现稳定收敛。


<details>
  <summary>Details</summary>
Motivation: 现有方法难以处理大规模交互群体中的收益耦合与非平稳性问题，亟需更有效的学习框架。

Method: 采用集中式评论家架构的MAPPO，并设计局部合作奖励机制（MAPPO-LCR），在不改变原博弈结构前提下引导策略更新。

Result: 仿真表明MAPPO-LCR能稳定促进合作行为，在不同增强因子下均可靠收敛；统计分析证实其优于传统PPO方法。

Conclusion: MAPPO-LCR有效解决了空间公共物品博弈中收益耦合与合作激励难题，为群体协作建模提供了新思路。

Abstract: Spatial public goods games model collective dilemmas where individual payoffs depend on population-level strategy configurations. Most existing studies rely on evolutionary update rules or value-based reinforcement learning methods. These approaches struggle to represent payoff coupling and non-stationarity in large interacting populations. This work introduces Multi-Agent Proximal Policy Optimization (MAPPO) into spatial public goods games for the first time. In these games, individual returns are intrinsically coupled through overlapping group interactions. Proximal Policy Optimization (PPO) treats agents as independent learners and ignores this coupling during value estimation. MAPPO addresses this limitation through a centralized critic that evaluates joint strategy configurations. To study neighborhood-level cooperation signals under this framework, we propose MAPPO with Local Cooperation Reward, termed MAPPO-LCR. The local cooperation reward aligns policy updates with surrounding cooperative density without altering the original game structure. MAPPO-LCR preserves decentralized execution while enabling population-level value estimation during training. Extensive simulations demonstrate stable cooperation emergence and reliable convergence across enhancement factors. Statistical analyses further confirm the learning advantage of MAPPO over PPO in spatial public goods games.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [15] [Fixed-Priority and EDF Schedules for ROS2 Graphs on Uniprocessor](https://arxiv.org/abs/2512.16926)
*Oren Bell,Harun Teper,Mario Günzel,Chris Gill,Jian-Jia Chen*

Main category: cs.DC

TL;DR: 提出基于事件执行器的固定作业级优先级调度方法，支持ROS2中任意DAG任务调度，弥合理论与实践差距。


<details>
  <summary>Details</summary>
Motivation: 现有ROS2调度方法局限于简单链式任务，缺乏对任意DAG结构的支持和严谨实时分析。

Method: 利用事件执行器实现固定作业级优先级调度器，将ROS2应用抽象为树森林并映射到传统实时DAG模型，需定制LIFO消息队列与中间件。

Result: 在无前置信息条件下生成与传统固定优先级DAG调度器一致的调度结果，验证方法有效性。

Conclusion: 该方法有效弥合了经典实时系统理论与ROS2实际调度分析之间的鸿沟。

Abstract: This paper addresses limitations of current scheduling methods in the Robot Operating System (ROS)2, focusing on scheduling tasks beyond simple chains and analyzing arbitrary Directed Acyclic Graphs (DAGs). While previous research has focused mostly on chain-based scheduling with ad-hoc response time analyses, we propose a novel approach using the events executor to implement fixed-job-level-priority schedulers for arbitrary ROS2 graphs on uniprocessor systems. We demonstrate that ROS 2 applications can be abstracted as forests of trees, enabling the mapping of ROS 2 applications to traditional real-time DAG task models. Our usage of the events executor requires a special implementation of the events queue and a communication middleware that supports LIFO-ordered message delivery, features not yet standard in ROS2. We show that our implementation generates the same schedules as a conventional fixed-priority DAG task scheduler, in spite of lacking access to the precedence information that usually is required. This further closes the gap between established real-time systems theory and ROS2 scheduling analyses.

</details>


### [16] [LLM-HPC++: Evaluating LLM-Generated Modern C++ and MPI+OpenMP Codes for Scalable Mandelbrot Set Computation](https://arxiv.org/abs/2512.17023)
*Patrick Diehl,Noujoud Nader,Deepti Gupta*

Main category: cs.DC

TL;DR: 评估大型语言模型在生成高性能计算C++代码中的表现，特别是针对Mandelbrot集的并行实现。


<details>
  <summary>Details</summary>
Motivation: 并行编程复杂且需要深厚专业知识，希望借助LLM降低开发门槛。

Method: 系统评估ChatGPT 4/5、Claude和LLaMA生成的C++代码，在共享内存、指令式和分布式内存范式下的正确性与性能。

Result: ChatGPT-4和ChatGPT-5在语法准确性和可扩展性方面表现优异。

Conclusion: 部分LLM已具备生成高效HPC代码的能力，但整体仍需进一步优化与验证。

Abstract: Parallel programming remains one of the most challenging aspects of High-Performance Computing (HPC), requiring deep knowledge of synchronization, communication, and memory models. While modern C++ standards and frameworks like OpenMP and MPI have simplified parallelism, mastering these paradigms is still complex. Recently, Large Language Models (LLMs) have shown promise in automating code generation, but their effectiveness in producing correct and efficient HPC code is not well understood. In this work, we systematically evaluate leading LLMs including ChatGPT 4 and 5, Claude, and LLaMA on the task of generating C++ implementations of the Mandelbrot set using shared-memory, directive-based, and distributed-memory paradigms. Each generated program is compiled and executed with GCC 11.5.0 to assess its correctness, robustness, and scalability. Results show that ChatGPT-4 and ChatGPT-5 achieve strong syntactic precision and scalable performance.

</details>


### [17] [Taming the Memory Footprint Crisis: System Design for Production Diffusion LLM Serving](https://arxiv.org/abs/2512.17077)
*Jiakun Fan,Yanglin Zhang,Xiangchen Li,Dimitrios S. Nikolopoulos*

Main category: cs.DC

TL;DR: dLLM-Serve 是首个面向扩散式大语言模型的高效服务系统，通过内存、调度与稀疏注意力协同优化，在消费级和服务器级 GPU 上实现 1.6 倍以上吞吐提升与近 4 倍尾延迟降低。


<details>
  <summary>Details</summary>
Motivation: 现有扩散语言模型缺乏适配其内存动态的服务框架，面临计算与带宽阶段资源震荡及日志张量内存峰值问题。

Method: 提出 Logit-Aware 激活预算分解张量峰值、Phase-Multiplexed 调度器交错异构请求、Head-Centric 稀疏注意力解耦逻辑与物理存储。

Result: 在 RTX 4090 和 L40S 上分别实现 1.61–1.81 倍和 1.60–1.74 倍吞吐提升，重负载下尾延迟降低近 4 倍。

Conclusion: dLLM-Serve 首次实现扩散语言模型推理规模化，将算法稀疏性转化为跨硬件的实际加速效果。

Abstract: Diffusion Large Language Models (dLLMs) have emerged as a promising alternative to Autoregressive Models (ARMs), utilizing parallel decoding to overcome sequential bottlenecks. However, existing research focuses primarily on kernel-level optimizations, lacking a holistic serving framework that addresses the unique memory dynamics of diffusion processes in production. We identify a critical "memory footprint crisis" specific to dLLMs, driven by monolithic logit tensors and the severe resource oscillation between compute-bound "Refresh" phases and bandwidth-bound "Reuse" phases. To bridge this gap, we present dLLM-Serve, an efficient dLLM serving system that co-optimizes memory footprint, computational scheduling, and generation quality. dLLM-Serve introduces Logit-Aware Activation Budgeting to decompose transient tensor peaks, a Phase-Multiplexed Scheduler to interleave heterogeneous request phases, and Head-Centric Sparse Attention to decouple logical sparsity from physical storage. We evaluate dLLM-Serve on diverse workloads (LiveBench, Burst, OSC) and GPUs (RTX 4090, L40S). Relative to the state-of-the-art baseline, dLLM-Serve improves throughput by 1.61$\times$-1.81$\times$ on the consumer-grade RTX 4090 and 1.60$\times$-1.74$\times$ on the server-grade NVIDIA L40S, while reducing tail latency by nearly 4$\times$ under heavy contention. dLLM-Serve establishes the first blueprint for scalable dLLM inference, converting theoretical algorithmic sparsity into tangible wall-clock acceleration across heterogeneous hardware.

</details>


### [18] [Scalable Distributed Vector Search via Accuracy Preserving Index Construction](https://arxiv.org/abs/2512.17264)
*Yuming Xu,Qianxi Zhang,Qi Chen,Baotong Lu,Menghao Li,Philip Adams,Mingqin Li,Zengzhong Li,Jing Liu,Cheng Li,Fan Yang*

Main category: cs.DC

TL;DR: SPIRE是一种可扩展的向量索引，通过平衡分区粒度和递归构建多级索引，在数十亿向量规模下实现高吞吐与稳定精度。


<details>
  <summary>Details</summary>
Motivation: 现有近似最近邻搜索系统在精度、延迟和吞吐量之间难以取得平衡，亟需更优的分布式索引设计。

Method: 采用平衡分区粒度避免读取成本激增，并通过递归构建多级索引确保搜索成本可预测且精度稳定。

Result: 在46节点、80亿向量规模实验中，SPIRE吞吐量比现有最优系统提升最高达9.64倍。

Conclusion: SPIRE有效解决了大规模向量检索中的扩展性与性能平衡问题，是当前最先进的分布式向量索引方案之一。

Abstract: Scaling Approximate Nearest Neighbor Search (ANNS) to billions of vectors requires distributed indexes that balance accuracy, latency, and throughput. Yet existing index designs struggle with this tradeoff. This paper presents SPIRE, a scalable vector index based on two design decisions. First, it identifies a balanced partition granularity that avoids read-cost explosion. Second, it introduces an accuracy-preserving recursive construction that builds a multi-level index with predictable search cost and stable accuracy. In experiments with up to 8 billion vectors across 46 nodes, SPIRE achieves high scalability and up to 9.64X higher throughput than state-of-the-art systems.

</details>


### [19] [The HEAL Data Platform](https://arxiv.org/abs/2512.17506)
*Brienna M. Larrick,L. Philip Schumm,Mingfei Shao,Craig Barnes,Anthony Juehne,Hara Prasad Juvvla,Michael B. Kranz,Michael Lukowski,Clint Malson,Jessica N. Mazerik,Christopher G. Meyer,Jawad Qureshi,Erin Spaniol,Andrea Tentner,Alexander VanTol,Peter Vassilatos,Sara Volk de Garcia,Robert L. Grossman*

Main category: cs.DC

TL;DR: HEAL数据平台基于Gen3开源平台，提供统一的数据搜索、发现与分析服务，支持跨多个数据仓库的互操作性，确保数据符合FAIR原则。


<details>
  <summary>Details</summary>
Motivation: 为解决HEAL计划资助研究产生的多源异构数据难以统一发现与分析的问题，构建一个集中式联邦系统。

Method: 采用Gen3开源平台，集成认证授权、持久标识符、元数据管理等框架服务，并通过API实现与NIH及第三方数据仓库的互操作。

Result: 平台已整合19个数据仓库，支持每月数百用户访问，提供丰富元数据和安全云环境以促进二次分析。

Conclusion: HEAL数据平台有效实现了跨库数据的可发现、可访问、可互操作与可重用，最大化HEAL计划数据价值。

Abstract: Objective: The objective was to develop a cloud-based, federated system to serve as a single point of search, discovery and analysis for data generated under the NIH Helping to End Addiction Long-term (HEAL) Initiative.
  Materials and methods: The HEAL Data Platform is built on the open source Gen3 platform, utilizing a small set of framework services and exposed APIs to interoperate with both NIH and non-NIH data repositories. Framework services include those for authentication and authorization, creating persistent identifiers for data objects, and adding and updating metadata.
  Results: The HEAL Data Platform serves as a single point of discovery of over one thousand studies funded under the HEAL Initiative. With hundreds of users per month, the HEAL Data Platform provides rich metadata and interoperates with data repositories and commons to provide access to shared datasets. Secure, cloud-based compute environments that are integrated with STRIDES facilitate secondary analysis of HEAL data. The HEAL Data Platform currently interoperates with nineteen data repositories.
  Discussion: Studies funded under the HEAL Initiative generate a wide variety of data types, which are deposited across multiple NIH and third-party data repositories. The mesh architecture of the HEAL Data Platform provides a single point of discovery of these data resources, accelerating and facilitating secondary use.
  Conclusion: The HEAL Data Platform enables search, discovery, and analysis of data that are deposited in connected data repositories and commons. By ensuring that these data are fully Findable, Accessible, Interoperable and Reusable (FAIR), the HEAL Data Platform maximizes the value of data generated under the HEAL Initiative.

</details>


### [20] [Enabling Disaggregated Multi-Stage MLLM Inference via GPU-Internal Scheduling and Resource Sharing](https://arxiv.org/abs/2512.17574)
*Lingxiao Zhao,Haoran Zhou,Yuezhi Che,Dazhao Cheng*

Main category: cs.DC

TL;DR: 本文提出FlashCodec和UnifiedServe，优化多模态大语言模型的端到端推理流程，显著提升吞吐量并降低延迟。


<details>
  <summary>Details</summary>
Motivation: 解决多模态大语言模型推理中预处理与视觉编码阶段造成的系统瓶颈问题。

Method: FlashCodec采用多GPU协同视频解码加速预处理；UnifiedServe通过逻辑解耦、物理资源共享消除阶段阻塞，提升GPU利用率。

Result: 相比现有系统，支持3倍请求量或1.5倍更严SLO，吞吐量最高提升4.4倍。

Conclusion: 该框架有效优化了多模态大语言模型的端到端推理效率，显著提升系统性能。

Abstract: Multimodal large language models (MLLMs) extend LLMs with visual understanding through a three-stage pipeline: multimodal preprocessing, vision encoding, and LLM inference. While these stages enhance capability, they introduce significant system bottlenecks. First, multimodal preprocessing-especially video decoding-often dominates Time-to-First-Token (TTFT). Most systems rely on CPU-based decoding, which severely limits throughput, while existing GPU-based approaches prioritize throughput-oriented parallelism and fail to meet the latency-sensitive requirements of MLLM inference. Second, the vision encoder is a standalone, compute-intensive stage that produces visual embeddings and cannot be co-batched with LLM prefill or decoding. This heterogeneity forces inter-stage blocking and increases token-generation latency. Even when deployed on separate GPUs, these stages underutilize available compute and memory resources, reducing overall utilization and constraining system throughput.
  To address these challenges, we present FlashCodec and UnifiedServe, two complementary designs that jointly optimize the end-to-end MLLM pipeline. FlashCodec accelerates the multimodal preprocessing stage through collaborative multi-GPU video decoding, reducing decoding latency while preserving high throughput. UnifiedServe optimizes the vision-to-text and inference stages using a logically decoupled their execution to eliminate inter-stage blocking, yet physically sharing GPU resources to maximize GPU system utilization. By carefully orchestrating execution across stages and minimizing interference, UnifiedServe Together, our proposed framework forms an end-to-end optimized stack that can serve up to 3.0$\times$ more requests or enforce 1.5$\times$ tighter SLOs, while achieving up to 4.4$\times$ higher throughput compared to state-of-the-art systems.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [21] [Torrent: A Distributed DMA for Efficient and Flexible Point-to-Multipoint Data Movement](https://arxiv.org/abs/2512.17589)
*Yunhao Deng,Fanchen Kong,Xiaoling Yi,Ryan Antonio,Marian Verhelst*

Main category: cs.AR

TL;DR: Torrent是一种新型分布式DMA架构，通过链式写入机制实现高效的点对多点数据传输，无需修改现有NoC硬件或协议，兼具高性能、低开销与良好扩展性。


<details>
  <summary>Details</summary>
Motivation: 现代SoC中计算能力与片上通信带宽的差距成为瓶颈，尤其在AI等数据并行负载中，缺乏原生组播支持的标准互连协议限制了性能。

Method: 提出Torrent架构，利用逻辑链在NoC上传输数据，保持点对点特性同时支持无限目标节点；并设计两种调度算法优化链顺序以提升性能与能效。

Result: 实测显示Torrent相比单播基线最高提速7.88倍，16nm ASIC实现仅占1.2%面积和2.3%功耗，每目标节点仅增加82周期和207um²开销。

Conclusion: Torrent在不改动现有硬件和协议前提下，实现了高效、灵活、可扩展的P2MP数据传输，是解决现代SoC通信瓶颈的有效方案。

Abstract: The growing disparity between computational power and on-chip communication bandwidth is a critical bottleneck in modern Systems-on-Chip (SoCs), especially for data-parallel workloads like AI. Efficient point-to-multipoint (P2MP) data movement, such as multicast, is essential for high performance. However, native multicast support is lacking in standard interconnect protocols. Existing P2MP solutions, such as multicast-capable Network-on-Chip (NoC), impose additional overhead to the network hardware and require modifications to the interconnect protocol, compromising scalability and compatibility.
  This paper introduces Torrent, a novel distributed DMA architecture that enables efficient P2MP data transfers without modifying NoC hardware and interconnect protocol. Torrent conducts P2MP data transfers by forming logical chains over the NoC, where the data traverses through targeted destinations resembling a linked list. This Chainwrite mechanism preserves the P2P nature of every data transfer while enabling flexible data transfers to an unlimited number of destinations. To optimize the performance and energy consumption of Chainwrite, two scheduling algorithms are developed to determine the optimal chain order based on NoC topology.
  Our RTL and FPGA prototype evaluations using both synthetic and real workloads demonstrate significant advantages in performance, flexibility, and scalability over network-layer multicast. Compared to the unicast baseline, Torrent achieves up to a 7.88x speedup. ASIC synthesis on 16nm technology confirms the architecture's minimal footprint in area (1.2%) and power (2.3%). Thanks to the Chainwrite, Torrent delivers scalable P2MP data transfers with a small cycle overhead of 82CC and area overhead of 207um2 per destination.

</details>


### [22] [A 14ns-Latency 9Gb/s 0.44mm$^2$ 62pJ/b Short-Blocklength LDPC Decoder ASIC in 22FDX](https://arxiv.org/abs/2512.17834)
*Darja Nonaca,Jérémy Guichemerre,Reinhard Wiesmayr,Nihat Engin Tunali,Christoph Studer*

Main category: cs.AR

TL;DR: 提出一种适用于URLLC的新型短块长多速率LDPC码及高效ASIC解码器，实现14ns最低延迟与9Gb/s吞吐量。


<details>
  <summary>Details</summary>
Motivation: 解决5G URLLC场景下极低延迟需求与现有SCL解码器高延迟、低面积效率的矛盾。

Method: 设计新型多速率二进制LDPC码并采用全并行消息传递解码，在22FDX工艺上实现ASIC硬件。

Result: 在128位块长、1/2码率下达成14ns解码延迟、9Gb/s信息吞吐量及62pJ/b能效，性能优于5G-LDPC码。

Conclusion: 该LDPC码及解码器架构显著提升短块长场景下的延迟与能效表现，适合URLLC应用部署。

Abstract: Ultra-reliable low latency communication (URLLC) is a key part of 5G wireless systems. Achieving low latency necessitates codes with short blocklengths for which polar codes with successive cancellation list (SCL) decoding typically outperform message-passing (MP)-based decoding of low-density parity-check (LDPC) codes. However, SCL decoders are known to exhibit high latency and poor area efficiency. In this paper, we propose a new short-blocklength multi-rate binary LDPC code that outperforms the 5G-LDPC code for the same blocklength and is suitable for URLLC applications using fully parallel MP. To demonstrate our code's efficacy, we present a 0.44mm$^2$ GlobalFoundries 22FDX LDPC decoder ASIC which supports three rates and achieves the lowest-in-class decoding latency of 14ns while reaching an information throughput of 9Gb/s at 62pJ/b energy efficiency for a rate-1/2 code with 128-bit blocklength.

</details>
