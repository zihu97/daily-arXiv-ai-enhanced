<div id=toc></div>

# Table of Contents

- [cs.DC](#cs.DC) [Total: 3]
- [cs.MA](#cs.MA) [Total: 2]
- [cs.AR](#cs.AR) [Total: 3]
- [cs.SE](#cs.SE) [Total: 23]


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [1] [Maple: A Multi-agent System for Portable Deep Learning across Clusters](https://arxiv.org/abs/2510.08842)
*Molang Wu,Zhao Zhang*

Main category: cs.DC

TL;DR: Maple 是一个多智能体系统，能根据用户的自然语言输入自动生成适用于异构 GPU 集群的深度学习训练命令行，在 567 个测试用例中准确率达 92.0%，展现出在高性能计算环境中实现可移植、可扩展分布式深度学习的实用价值。


<details>
  <summary>Details</summary>
Motivation: 在 GPU 集群上训练深度学习模型需用户手动编写复杂的命令行，涉及多种启动器、调度器、亲和性设置、框架参数和环境变量，过程易错且繁琐，阻碍研究效率并浪费计算资源。

Method: Maple 采用四个智能体协同工作，分别负责信息提取、模板检索、命令行验证和错误修正，结合多个总计 10B 参数的语言模型，支持多种调度器（SLURM/PBS）和异构 GPU 架构（如 NVIDIA A100/H200 和 Intel Max 系列）。

Result: 在覆盖美国国家计算中心的 9 个 GPU 集群、5 类深度学习模型和 4 种并行训练范式上，Maple 在 567 个测试用例中达到 92.0% 的命令行生成准确率，性能媲美 GPT-5、Claude 和 Gemini 等前沿模型。

Conclusion: Maple 能有效简化异构高性能计算环境中分布式深度学习的命令行配置，显著提升用户效率与系统资源利用率，具有实际部署价值。

Abstract: Training deep learning (DL) models across Graphics Processing Unit (GPU)
clusters is technically challenging. One aspect is that users have to compose
command lines to adapt to the heterogeneous launchers, schedulers, affinity
options, DL framework arguments, and environment variables. Composing correct
command lines is error-prone and can easily frustrate users, impeding research
or wasting resources. In this work, we present Maple, a multi-agent system that
generates correct DL command lines with users' natural language input. Maple
consists of four agents with the functionalities of information extraction,
template retrieval, command line verification, and error correction. We
evaluate Maple on nine GPU clusters across national computing centers in the
U.S., five representative deep learning model families, and four commonly used
parallel DL training paradigms. Our experiments also cover schedulers of SLURM
and PBS and heterogeneous architectures, such as NVIDIA A100/H200 GPUs and
Intel Max series GPUs. Maple achieves 92.0% accuracy in generating command
lines across the 567 test cases. Leverage multiple language models with an
aggregated size of 10B parameters, Maple delivers comparable performance to the
state-of-the-art models of GPT-5, Claude, and Gemini. Together, these results
highlight Maple's practical value in enabling portable and scalable distributed
DL across heterogeneous HPC environments.

</details>


### [2] [Slicing Is All You Need: Towards A Universal One-Sided Algorithm for Distributed Matrix Multiplication](https://arxiv.org/abs/2510.08874)
*Benjamin Brock,Renato Golin*

Main category: cs.DC

TL;DR: 本文提出了一种通用的单边分布式矩阵乘法算法，支持所有划分方式和复制因子组合，避免了因缺乏对应算法实现而导致的数据重分布开销，并在多种配置下展现出与PyTorch DTensor相当的性能。


<details>
  <summary>Details</summary>
Motivation: 现有分布式矩阵乘法算法仅支持部分划分方式（如1D、2D等），若目标划分无对应实现，则需重分布数据，增加通信开销。因此，亟需一种能支持任意划分组合的通用算法。

Method: 提出一种基于切片（索引算术）的通用单边算法，通过计算重叠数据块的乘法任务列表，支持所有划分与复制因子组合；该任务列表可直接执行或优化调度后执行。算法在基于C++的PGAS框架中实现，支持GPU间直接通信。

Result: 在多种划分和复制因子配置下评估表明，该算法性能与高度优化的PyTorch DTensor相当。

Conclusion: 该通用算法有效消除了对多种专用实现的需求，减少了通信开销，并在实际性能上具有竞争力，适用于科学计算、数据分析和AI等广泛场景。

Abstract: Many important applications across science, data analytics, and AI workloads
depend on distributed matrix multiplication. Prior work has developed a large
array of algorithms suitable for different problem sizes and partitionings
including 1D, 2D, 1.5D, and 2.5D algorithms. A limitation of current work is
that existing algorithms are limited to a subset of partitionings. Multiple
algorithm implementations are required to support the full space of possible
partitionings. If no algorithm implementation is available for a particular set
of partitionings, one or more operands must be redistributed, increasing
communication costs. This paper presents a universal one-sided algorithm for
distributed matrix multiplication that supports all combinations of
partitionings and replication factors. Our algorithm uses slicing (index
arithmetic) to compute the sets of overlapping tiles that must be multiplied
together. This list of local matrix multiplies can then either be executed
directly, or reordered and lowered to an optimized IR to maximize overlap. We
implement our algorithm using a high-level C++-based PGAS programming framework
that performs direct GPU-to-GPU communication using intra-node interconnects.
We evaluate performance for a wide variety of partitionings and replication
factors, finding that our work is competitive with PyTorch DTensor, a highly
optimized distributed tensor library targeting AI models.

</details>


### [3] [Co-designing a Programmable RISC-V Accelerator for MPC-based Energy and Thermal Management of Many-Core HPC Processors](https://arxiv.org/abs/2510.09163)
*Alessandro Ottaviano,Andrino Meli,Paul Scheffler,Giovanni Bambini,Robert Balas,Davide Rossi,Andrea Bartolini,Luca Benini*

Main category: cs.DC

TL;DR: 本文提出一种软硬件协同设计的轻量级模型预测控制器（MPC），用于高效管理众核高性能计算处理器的能耗与热分布，在控制144个处理单元时实现亚毫秒级延迟、7.9倍能效提升，且仅占用小于1.5%芯片面积和1 MiB内存。


<details>
  <summary>Details</summary>
Motivation: 传统MPC控制器在处理单元（PE）上运行受操作系统开销影响，导致抖动并限制控制带宽；而专用片上控制器虽可实现快速确定性控制，却面临面积与功耗开销问题。因此，亟需一种兼顾性能、能效与资源开销的新型MPC实现方案。

Method: 采用软硬件协同设计方法，基于算子分裂二次规划求解器和嵌入式多核RISC-V控制器构建轻量级MPC；通过剪枝弱热耦合以减少模型内存，并利用提前调度策略高效并行执行优化问题中产生的稀疏三角系统。

Result: 所提控制器在500 MHz下控制144个PE时实现亚毫秒延迟，相比单核基线延迟降低33倍、能效提升7.9倍；内存占用小于1 MiB，功耗低至325 mW，芯片面积占用不足典型HPC处理器的1.5%。

Conclusion: 该轻量级MPC控制器在满足高控制带宽需求的同时，显著降低了延迟、功耗与面积开销，为众核HPC处理器的能效与热管理提供了高效可行的解决方案。

Abstract: Managing energy and thermal profiles is critical for many-core HPC processors
with hundreds of application-class processing elements (PEs). Advanced model
predictive control (MPC) delivers state-of-the-art performance but requires
solving an online optimization problem over a thousand times per second (1 kHz
control bandwidth), with computational and memory demands scaling with PE
count. Traditional MPC approaches execute the controller on the PEs, but
operating system overheads create jitter and limit control bandwidth. Running
MPC on dedicated on-chip controllers enables fast, deterministic control but
raises concerns about area and power overhead. In this work, we tackle these
challenges by proposing a hardware-software codesign of a lightweight MPC
controller, based on an operator-splitting quadratic programming solver and an
embedded multi-core RISC-V controller. Key innovations include pruning weak
thermal couplings to reduce model memory and ahead-of-time scheduling for
efficient parallel execution of sparse triangular systems arising from the
optimization problem. The proposed controller achieves sub-millisecond latency
when controlling 144 PEs at 500 MHz, delivering 33x lower latency and 7.9x
higher energy efficiency than a single-core baseline. Operating within a
compact less than 1 MiB memory footprint, it consumes as little as 325 mW while
occupying less than 1.5% of a typical HPC processor's die area.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [4] [GRPO-GCC: Enhancing Cooperation in Spatial Public Goods Games via Group Relative Policy Optimization with Global Cooperation Constraint](https://arxiv.org/abs/2510.08607)
*Zhaoqilin Yang,Chanchan Li,Tianqi Liu,Hongxin Zhao,Youliang Tian*

Main category: cs.MA

TL;DR: 本文提出了GRPO-GCC框架，将群体相对策略优化与全局合作约束相结合，用于空间公共品博弈，实现了更快的合作启动、稳定的策略适应和长期可持续性。


<details>
  <summary>Details</summary>
Motivation: 受集体制度中自调节合作原则的启发，旨在解决现有方法在结构化群体中难以维持可持续合作的问题，避免陷入全面背叛或无条件合作的极端状态。

Method: GRPO-GCC结合了群体归一化优势估计、参考锚定的KL惩罚项和动态调整合作收益的全局激励项，在群体相对策略优化基础上引入全局合作约束。

Result: 该框架在空间公共品博弈中显著加速了合作的出现，稳定了策略适应过程，并实现了长期可持续的合作行为。

Conclusion: GRPO-GCC通过一个简单而全局的信号重塑多智能体激励机制，为社会技术系统中的多智能体强化学习提供了新范式。

Abstract: Inspired by the principle of self-regulating cooperation in collective
institutions, we propose the Group Relative Policy Optimization with Global
Cooperation Constraint (GRPO-GCC) framework. This work is the first to
introduce GRPO into spatial public goods games, establishing a new deep
reinforcement learning baseline for structured populations. GRPO-GCC integrates
group relative policy optimization with a global cooperation constraint that
strengthens incentives at intermediate cooperation levels while weakening them
at extremes. This mechanism aligns local decision making with sustainable
collective outcomes and prevents collapse into either universal defection or
unconditional cooperation. The framework advances beyond existing approaches by
combining group-normalized advantage estimation, a reference-anchored KL
penalty, and a global incentive term that dynamically adjusts cooperative
payoffs. As a result, it achieves accelerated cooperation onset, stabilized
policy adaptation, and long-term sustainability. GRPO-GCC demonstrates how a
simple yet global signal can reshape incentives toward resilient cooperation,
and provides a new paradigm for multi-agent reinforcement learning in
socio-technical systems.

</details>


### [5] [Scalable Multi-Agent Path Finding using Collision-Aware Dynamic Alert Mask and a Hybrid Execution Strategy](https://arxiv.org/abs/2510.09469)
*Bharath Muppasani,Ritirupa Dey,Biplav Srivastava,Vignesh Narayanan*

Main category: cs.MA

TL;DR: 本文提出了一种结合去中心化路径规划与轻量级中心化协调器的混合框架，利用强化学习进行去中心化规划，并通过中心协调器动态提供最小化的冲突信息（如冲突单元标志或简短冲突轨迹），在减少信息共享的同时实现高效、无碰撞的多智能体路径规划。


<details>
  <summary>Details</summary>
Motivation: 传统集中式多智能体路径规划方法（如CBS）虽能提供高质量解，但在大规模场景中因冲突组合爆炸而计算开销大；而分布式方法（尤其是基于学习的方法）虽具可扩展性，却常牺牲解的质量。因此，亟需一种兼顾可扩展性与解质量的新方法。

Method: 提出一种混合框架：使用强化学习进行去中心化路径规划，同时引入一个轻量级中心协调器，动态向智能体提供最小化、有针对性的冲突信息（如静态冲突单元标志或简短冲突轨迹），以辅助其调整路径并避免冲突。

Result: 实验表明，该方法相比完全集中式或分布式方法显著减少了智能体间的信息共享量，同时在高密度、大规模场景中仍能稳定生成可行且无碰撞的路径解。

Conclusion: 该混合框架有效平衡了计算效率、信息开销与解的质量，为大规模多智能体路径规划提供了一种实用且高效的解决方案。

Abstract: Multi-agent pathfinding (MAPF) remains a critical problem in robotics and
autonomous systems, where agents must navigate shared spaces efficiently while
avoiding conflicts. Traditional centralized algorithms that have global
information, such as Conflict-Based Search (CBS), provide high-quality
solutions but become computationally expensive in large-scale scenarios due to
the combinatorial explosion of conflicts that need resolution. Conversely,
distributed approaches that have local information, particularly learning-based
methods, offer better scalability by operating with relaxed information
availability, yet often at the cost of solution quality. To address these
limitations, we propose a hybrid framework that combines decentralized path
planning with a lightweight centralized coordinator. Our framework leverages
reinforcement learning (RL) for decentralized planning, enabling agents to
adapt their planning based on minimal, targeted alerts--such as static
conflict-cell flags or brief conflict tracks--that are dynamically shared
information from the central coordinator for effective conflict resolution. We
empirically study the effect of the information available to an agent on its
planning performance. Our approach reduces the inter-agent information sharing
compared to fully centralized and distributed methods, while still consistently
finding feasible, collision-free solutions--even in large-scale scenarios
having higher agent counts.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [6] [A High-Efficiency SoC for Next-Generation Mobile DNA Sequencing](https://arxiv.org/abs/2510.08940)
*Abel Beyene,Zhongpan Wu,Yunus Dawji,Karim Hammad,Ebrahim Ghafar-Zadeh,Sebastian Magierowski*

Main category: cs.AR

TL;DR: 本文提出了一种基于22纳米CMOS工艺的片上系统（SoC），集成了RISC-V通用处理器和专用DNA检测加速器，显著提升了便携式DNA测序仪的性能与能效。


<details>
  <summary>Details</summary>
Motivation: 当前手持式DNA测序仪因嵌入式计算能力不足，需依赖外部设备处理大量传感数据，导致通信负担重且无法实现实时移动测序。

Method: 设计并实现了一个基于RISC-V核心的SoC，采用22纳米CMOS工艺，并集成专用DNA检测硬件加速器。

Result: 该系统相比商用嵌入式多核处理器实现了13倍的性能提升和近3000倍的能效提升。

Conclusion: 所提出的SoC架构为下一代便携式DNA测序设备提供了高性能、高能效的嵌入式处理解决方案，有望实现真正的实时移动测序。

Abstract: Hand-sized Deoxyribonucleic acid (DNA) sequencing machines are of growing
importance in several life sciences fields as their small footprints enable a
broader range of use cases than their larger, stationary counterparts. However,
as currently designed, they lack sufficient embedded computing to process the
large volume of measurements generated by their internal sensory system. As a
consequence, they rely on external devices for additional processing
capability. This dependence on external processing places a significant
communication burden on the sequencer's embedded electronics. Moreover, it also
prevents a truly mobile solution for sequencing in real-time. Anticipating
next-generation machines that include suitably advanced processing, we present
a System-on-Chip (SoC) fabricated in 22-nm complementary metal-oxide
semiconductor (CMOS). Our design, based on a general-purpose reduced
instruction set computing (RISC-V) core, also includes accelerators for DNA
detection that allow our system to demonstrate a 13X performance improvement
over commercial embedded multicore processors combined with a near 3000X boost
in energy efficiency.

</details>


### [7] [HERO: Hardware-Efficient RL-based Optimization Framework for NeRF Quantization](https://arxiv.org/abs/2510.09010)
*Yipu Zhang,Chaofang Ma,Jinming Ge,Lin Jiang,Jiang Xu,Wei Zhang*

Main category: cs.AR

TL;DR: 本文提出了HERO，一种基于强化学习的硬件感知NeRF量化框架，通过集成NeRF加速器模拟器实现自动化优化，在延迟、成本效率和模型大小方面优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有NeRF量化方法未考虑硬件架构，导致在精度、延迟和模型大小之间的设计空间中表现次优；同时，现有加速器依赖人工探索设计空间，效率低下且难以找到最优解。

Method: 提出HERO框架，利用强化学习进行硬件感知的NeRF量化，并集成NeRF加速器模拟器以提供实时硬件反馈，实现全自动适配硬件约束。

Result: HERO相比先前最先进的CAQ框架，在延迟上提升1.31–1.33倍，成本效率提升1.29–1.33倍，并获得更紧凑的模型。

Conclusion: HERO能有效在硬件与算法需求之间复杂的设计空间中导航，自动发现更优的NeRF量化策略。

Abstract: Neural Radiance Field (NeRF) has emerged as a promising 3D reconstruction
method, delivering high-quality results for AR/VR applications. While
quantization methods and hardware accelerators have been proposed to enhance
NeRF's computational efficiency, existing approaches face crucial limitations.
Current quantization methods operate without considering hardware architecture,
resulting in sub-optimal solutions within the vast design space encompassing
accuracy, latency, and model size. Additionally, existing NeRF accelerators
heavily rely on human experts to explore this design space, making the
optimization process time-consuming, inefficient, and unlikely to discover
optimal solutions. To address these challenges, we introduce HERO, a
reinforcement learning framework performing hardware-aware quantization for
NeRF. Our framework integrates a NeRF accelerator simulator to generate
real-time hardware feedback, enabling fully automated adaptation to hardware
constraints. Experimental results demonstrate that HERO achieves 1.31-1.33
$\times$ better latency, 1.29-1.33 $\times$ improved cost efficiency, and a
more compact model size compared to CAQ, a previous state-of-the-art NeRF
quantization framework. These results validate our framework's capability to
effectively navigate the complex design space between hardware and algorithm
requirements, discovering superior quantization policies for NeRF
implementation. Code is available at https://github.com/ypzhng/HERO.

</details>


### [8] [Sequencing on Silicon: AI SoC Design for Mobile Genomics at the Edge](https://arxiv.org/abs/2510.09339)
*Sebastian Magierowski,Zhongpan Wu,Abel Beyene,Karim Hammad*

Main category: cs.AR

TL;DR: 本文提出了一种面向移动基因组分析的CMOS片上系统（SoC），结合多核RISC-V处理器与深度学习及生物信息学专用加速器，通过软硬件协同设计实现高效能、低功耗的实时片上基因组分析。


<details>
  <summary>Details</summary>
Motivation: 纳米孔DNA测序在移动场景中产生极高的原始数据速率（比音频高100倍以上），对边缘端的计算和内存处理能力提出严峻挑战，亟需高效、低功耗的机器学习解决方案。

Method: 设计一种集成多核RISC-V处理器与紧耦合深度学习和生物信息学加速器的CMOS SoC，采用软硬件协同设计策略，在异构计算架构上实现能效优化。

Result: 该SoC支持实时、设备端的基因组分析，有效应对纳米孔测序带来的高数据率挑战，展示了深度学习、边缘计算与领域专用硬件融合的可行性。

Conclusion: 通过将深度学习、边缘计算与专用硬件集成，该工作推动了下一代移动基因组学的发展，为高通量生物数据的实时处理提供了高效能解决方案。

Abstract: Miniature DNA sequencing hardware has begun to succeed in mobile contexts,
driving demand for efficient machine learning at the edge. This domain
leverages deep learning techniques familiar from speech and time-series
analysis for both low-level signal processing and high-level genomic
interpretation. Unlike audio, however, nanopore sequencing presents raw data
rates over 100X higher, requiring more aggressive compute and memory handling.
In this paper, we present a CMOS system-on-chip (SoC) designed for mobile
genetic analysis. Our approach combines a multi-core RISC-V processor with
tightly coupled accelerators for deep learning and bioinformatics. A
hardware/software co-design strategy enables energy-efficient operation across
a heterogeneous compute fabric, targeting real-time, on-device genome analysis.
This work exemplifies the integration of deep learning, edge computing, and
domain-specific hardware to advance next-generation mobile genomics.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [9] [Comparative Analysis of Large Language Models for the Machine-Assisted Resolution of User Intentions](https://arxiv.org/abs/2510.08576)
*Justus Flerlage,Alexander Acker,Odej Kao*

Main category: cs.SE

TL;DR: 本文探讨了开源、可本地部署的大语言模型（LLMs）在实现用户意图解析和工作流编排方面的可行性，将其与OpenAI的GPT-4系统进行对比，评估其在下一代操作系统中作为本地智能组件的潜力。


<details>
  <summary>Details</summary>
Motivation: 当前基于云的专有大语言模型在隐私、自主性和可扩展性方面存在局限，阻碍了语言优先交互范式的发展。为构建可信、稳健的意图驱动操作系统，亟需评估本地部署的开源LLM的可行性。

Method: 对多个开源、开放访问的大语言模型进行能力评估，通过对比OpenAI的GPT-4系统，分析它们在根据用户意图生成工作流方面的表现。

Result: 研究提供了关于开源LLM在实际应用中的可行性、性能权衡及其作为本地可操作组件潜力的实证见解。

Conclusion: 开源、本地部署的LLM有望支撑去中心化和民主化的AI基础设施，推动用户与设备之间实现更无缝、自适应且注重隐私的交互。

Abstract: Large Language Models (LLMs) have emerged as transformative tools for natural
language understanding and user intent resolution, enabling tasks such as
translation, summarization, and, increasingly, the orchestration of complex
workflows. This development signifies a paradigm shift from conventional,
GUI-driven user interfaces toward intuitive, language-first interaction
paradigms. Rather than manually navigating applications, users can articulate
their objectives in natural language, enabling LLMs to orchestrate actions
across multiple applications in a dynamic and contextual manner. However,
extant implementations frequently rely on cloud-based proprietary models, which
introduce limitations in terms of privacy, autonomy, and scalability. For
language-first interaction to become a truly robust and trusted interface
paradigm, local deployment is not merely a convenience; it is an imperative.
This limitation underscores the importance of evaluating the feasibility of
locally deployable, open-source, and open-access LLMs as foundational
components for future intent-based operating systems. In this study, we examine
the capabilities of several open-source and open-access models in facilitating
user intention resolution through machine assistance. A comparative analysis is
conducted against OpenAI's proprietary GPT-4-based systems to assess
performance in generating workflows for various user intentions. The present
study offers empirical insights into the practical viability, performance
trade-offs, and potential of open LLMs as autonomous, locally operable
components in next-generation operating systems. The results of this study
inform the broader discussion on the decentralization and democratization of AI
infrastructure and point toward a future where user-device interaction becomes
more seamless, adaptive, and privacy-conscious through locally embedded
intelligence.

</details>


### [10] [Which Is Better For Reducing Outdated and Vulnerable Dependencies: Pinning or Floating?](https://arxiv.org/abs/2510.08609)
*Imranur Rahman,Jill Marley,William Enck,Laurie Williams*

Main category: cs.SE

TL;DR: 该研究通过大规模实证分析，评估了不同依赖版本约束类型（如固定版本 pinning 与浮动版本 floating）对依赖项过时或存在漏洞可能性的影响，发现 floating-minor 最常用且最不易产生漏洞，而 floating-major 最不易导致依赖过时。


<details>
  <summary>Details</summary>
Motivation: 开发者在依赖管理中需在防止破坏性变更（通过 pinning）与自动获取修复（通过 floating）之间权衡，但尚不清楚不同版本约束类型如何影响依赖过时或漏洞的风险，因此需要实证指导。

Method: 研究分析了 npm、PyPI 和 Cargo 生态中版本约束的使用趋势及变更模式，并采用生存分析建模依赖状态转换，比较 pinning 与其他约束类型在导致过时或漏洞方面的差异。

Result: 在过时和存在漏洞的依赖中，最常用的约束类型是 floating-minor，其次为 pinning；floating-major 最不易导致依赖过时，而 floating-minor 最不易导致漏洞。

Conclusion: 开发者应根据安全与维护目标谨慎选择版本约束类型，floating-minor 在保持安全性的同时兼顾更新，可能是较优选择。

Abstract: Developers consistently use version constraints to specify acceptable
versions of the dependencies for their project. \emph{Pinning} dependencies can
reduce the likelihood of breaking changes, but comes with a cost of manually
managing the replacement of outdated and vulnerable dependencies. On the other
hand, \emph{floating} can be used to automatically get bug fixes and security
fixes, but comes with the risk of breaking changes. Security practitioners
advocate \emph{pinning} dependencies to prevent against software supply chain
attacks, e.g., malicious package updates. However, since \emph{pinning} is the
tightest version constraint, \emph{pinning} is the most likely to result in
outdated dependencies. Nevertheless, how the likelihood of becoming outdated or
vulnerable dependencies changes across version constraint types is unknown. The
goal of this study is to aid developers in making an informed dependency
version constraint choice by empirically evaluating the likelihood of
dependencies becoming outdated or vulnerable across version constraint types at
scale. In this study, we first identify the trends in dependency version
constraint usage and the patterns of version constraint type changes made by
developers in the npm, PyPI, and Cargo ecosystems. We then modeled the
dependency state transitions using survival analysis and estimated how the
likelihood of becoming outdated or vulnerable changes when using \emph{pinning}
as opposed to the rest of the version constraint types. We observe that among
outdated and vulnerable dependencies, the most commonly used version constraint
type is \emph{floating-minor}, with \emph{pinning} being the next most common.
We also find that \emph{floating-major} is the least likely to result in
outdated and \emph{floating-minor} is the least likely to result in vulnerable
dependencies.

</details>


### [11] [Relative Positioning Based Code Chunking Method For Rich Context Retrieval In Repository Level Code Completion Task With Code Language Model](https://arxiv.org/abs/2510.08610)
*Imranur Rahman,Md Rayhanur Rahman*

Main category: cs.SE

TL;DR: 本文提出了一种有效的上下文收集策略，通过将代码库预处理为小代码块，并结合语法与语义相似性检索及相对位置信息，提升大语言模型在代码补全任务中的表现。


<details>
  <summary>Details</summary>
Motivation: 现有研究缺乏对如何构建适合大语言模型进行代码补全的有效上下文的深入理解，尽管现代IDE已提供代码补全功能。

Method: 将代码仓库预处理为较小的代码块，利用基于语法和语义相似性的检索方法，并结合代码块的相对位置信息构建上下文。

Result: 代码分块与代码块在最终上下文中的相对位置显著提升了代码补全任务的性能。

Conclusion: 有效的上下文构建策略，特别是结合代码分块与相对位置信息，能够显著增强大语言模型在代码补全中的效果。

Abstract: Code completion can help developers improve efficiency and ease the
development lifecycle. Although code completion is available in modern
integrated development environments (IDEs), research lacks in determining what
makes a good context for code completion based on the information available to
the IDEs for the large language models (LLMs) to perform better. In this paper,
we describe an effective context collection strategy to assist the LLMs in
performing better at code completion tasks. The key idea of our strategy is to
preprocess the repository into smaller code chunks and later use syntactic and
semantic similarity-based code chunk retrieval with relative positioning. We
found that code chunking and relative positioning of the chunks in the final
context improve the performance of code completion tasks.

</details>


### [12] [Faver: Boosting LLM-based RTL Generation with Function Abstracted Verifiable Middleware](https://arxiv.org/abs/2510.08664)
*Jianan Mu,Mingyu Shi,Yining Wang,Tianmeng Yang,Bin Sun,Xing Hu,Jing Ye,Huawei Li*

Main category: cs.SE

TL;DR: 本文提出了一种名为Faver的函数抽象可验证中间件，通过结合LLM友好的代码结构与基于规则的模板，解耦电路验证细节，使大语言模型（LLM）能专注于功能实现，从而在RTL生成任务中将准确率提升最多14%。


<details>
  <summary>Details</summary>
Motivation: 当前基于大语言模型（LLM）的RTL生成面临高阶规范与RTL之间语义鸿沟大、训练数据有限的问题，导致生成准确性不足；同时RTL测试平台数据更加稀缺，不利于LLM学习，且Python/C等高层语言与硬件代码在时空粒度上差异显著，增加了LLM建模难度。

Method: 提出一种函数抽象可验证中间件（Faver），融合LLM友好的代码结构与基于规则的模板，将电路验证细节解耦，使LLM专注于功能语义，简化RTL验证流程。

Result: 在SFT模型和开源模型上的实验表明，Faver最多可将模型的RTL生成准确率提升14%。

Conclusion: Faver有效缓解了LLM在RTL生成中因语义鸿沟和数据稀缺导致的准确性问题，为自动化芯片设计提供了可行的新路径。

Abstract: LLM-based RTL generation is an interesting research direction, as it holds
the potential to liberate the least automated stage in the current chip design.
However, due to the substantial semantic gap between high-level specifications
and RTL, coupled with limited training data, existing models struggle with
generation accuracy. Drawing on human experience, design with verification
helps improving accuracy. However, as the RTL testbench data are even more
scarce, it is not friendly for LLMs. Although LLMs excel at higher-level
languages like Python/C, they have a huge semantic gap from RTL. When
implementing the same functionality, Python/C code and hardware code differ
significantly in the spatiotemporal granularity, requiring the LLM not only to
consider high-level functional semantics but also to ensure the low-level
details align with the circuit code. It is not an easy task. In this paper, we
propose a function abstracted verifiable middleware (Faver) that streamlines
RTL verification in LLM-based workflows. By mixing LLM-friendly code structures
with a rule-based template, Faver decouples the details of circuit
verification, allowing the LLM to focus on the functionality itself. In our
experiments on the SFT model and open-source models, Faver improved the model's
generation accuracy by up to 14%.

</details>


### [13] [RA-Gen: A Controllable Code Generation Framework Using ReAct for Multi-Agent Task Execution](https://arxiv.org/abs/2510.08665)
*Aofan Liu,Haoxuan Li,Bin Wang,Ao Yang,Hui Li*

Main category: cs.SE

TL;DR: 本文提出了一种基于ReAct范式的多智能体可控代码生成框架，通过规划器、搜索器、代码生成器和提取器四个专用智能体协同工作，实现高效、精确且可解释的代码生成，并在安全性和可控性方面显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于大语言模型的代码生成方法在安全性、准确性和可控性方面存在不足，尤其缺乏对外部工具的动态集成、透明推理过程以及用户对安全性的控制。

Method: 提出一个基于ReAct范式的多智能体框架，包含四个专用智能体：负责任务分解的Planner、基于ReAct进行推理与工具调用的Searcher、负责代码生成的CodeGen，以及用于结构化数据提取的Extractor；其中Searcher通过交替生成推理轨迹与执行动作，动态结合内部知识与外部工具（如搜索引擎）。

Result: 该框架在多语言任务中表现优异，在SVEN数据集上使用CodeQL评估达到94.8%的安全率，优于现有方法，同时其透明推理过程增强了用户信任与可控性。

Conclusion: 所提出的多智能体ReAct框架有效提升了代码生成的安全性、准确性与可控性，为复杂代码生成任务提供了可解释且用户可控的解决方案。

Abstract: Code generation models based on large language models (LLMs) have gained wide
adoption, but challenges remain in ensuring safety, accuracy, and
controllability, especially for complex tasks. Existing methods often lack
dynamic integration of external tools, transparent reasoning, and user control
over safety. To address these issues, we propose a controllable code generation
framework utilizing the ReAct paradigm for multi-agent task execution. This
framework is a multi-agent system designed to enable efficient, precise, and
interpretable code generation through dynamic interactions between LLMs and
external resources. The framework adopts a collaborative architecture
comprising four specialized agents: a Planner for task decomposition, a
Searcher that leverages the ReAct framework for reasoning and tool integration,
a CodeGen agent for accurate code generation, and an Extractor for structured
data retrieval. The ReAct-based Searcher alternates between generating
reasoning traces and executing actions, facilitating seamless integration of
internal knowledge with external tools (such as search engines) to enhance
accuracy and user control. Experimental results show the framework's
effectiveness across multiple languages, achieving a 94.8% security rate on the
SVEN dataset with CodeQL, outperforming existing approaches. Its transparent
reasoning process fosters user trust and improves controllability.

</details>


### [14] [RAG4Tickets: AI-Powered Ticket Resolution via Retrieval-Augmented Generation on JIRA and GitHub Data](https://arxiv.org/abs/2510.08667)
*Mohammad Baqar*

Main category: cs.SE

TL;DR: 本文提出一种结合Sentence-Transformers与FAISS的检索增强生成（RAG）框架，用于整合JIRA和GitHub中的异构数据，通过语义检索历史工单与PR信息，利用大语言模型生成可解释的问题解决建议，显著提升DevOps环境中问题解决的准确性与知识复用效率。


<details>
  <summary>Details</summary>
Motivation: 现代软件团队在解决重复或相关问题时常常因知识碎片化（分散在JIRA工单、开发者讨论和GitHub PR中）而延误，亟需一种能整合并智能利用这些信息的方法。

Method: 采用检索增强生成（RAG）框架，使用Sentence-Transformers生成语义嵌入，通过FAISS进行向量检索，整合历史JIRA工单、用户评论和关联PR元数据，利用大语言模型基于检索到的相似案例生成解释性强的解决方案。

Result: 实验表明，该系统在准确率、召回率、问题解决时间缩短和开发者接受度等指标上表现优异，显著提升了问题解决准确率、修复质量和知识复用效果。

Conclusion: 所提出的RAG框架有效整合了JIRA与GitHub中的异构软件开发数据，通过语义检索与生成机制，为DevOps环境提供了高效、可解释且实用的问题解决支持。

Abstract: Modern software teams frequently encounter delays in resolving recurring or
related issues due to fragmented knowledge scattered across JIRA tickets,
developer discussions, and GitHub pull requests (PRs). To address this
challenge, we propose a Retrieval-Augmented Generation (RAG) framework that
integrates Sentence-Transformers for semantic embeddings with FAISS-based
vector search to deliver context-aware ticket resolution recommendations. The
approach embeds historical JIRA tickets, user comments, and linked PR metadata
to retrieve semantically similar past cases, which are then synthesized by a
Large Language Model (LLM) into grounded and explainable resolution
suggestions. The framework contributes a unified pipeline linking JIRA and
GitHub data, an embedding and FAISS indexing strategy for heterogeneous
software artifacts, and a resolution generation module guided by retrieved
evidence. Experimental evaluation using precision, recall, resolution time
reduction, and developer acceptance metrics shows that the proposed system
significantly improves resolution accuracy, fix quality, and knowledge reuse in
modern DevOps environments.

</details>


### [15] [BigCodeArena: Unveiling More Reliable Human Preferences in Code Generation via Execution](https://arxiv.org/abs/2510.08697)
*Terry Yue Zhuo,Xiaolong Jin,Hange Liu,Juyong Jiang,Tianyang Liu,Chen Gong,Bhupesh Bishnoi,Vaisakhi Mishra,Marek Suppa,Noah Ziems,Saiteja Utpala,Ming Xu,Guangyu Song,Kaixin Li,Yuhan Cao,Bo Liu,Zheng Liu,Sabina Abdurakhmanova,Wenhao Yu,Mengzhao Jia,Jihan Yao,Kenneth Hamilton,Kumar Shridhar,Minh Chien Vu,Dingmin Wang,Jiawei Liu,Zijian Wang,Qian Liu,Binyuan Hui,Meg Risdal,Ahsen Khaliq,Atin Sood,Zhenchang Xing,Wasi Uddin Ahmad,John Grundy,David Lo,Banghua Zhu,Xiaoning Du,Torsten Scholak,Leandro von Werra*

Main category: cs.SE

TL;DR: 本文提出了BigCodeArena，一个支持实时代码执行与交互的开源人工评估平台，用于评估大语言模型在代码生成任务中的表现，并基于收集的数据构建了两个新基准：BigCodeReward（用于评估奖励模型与人类偏好的一致性）和AutoCodeArena（无需人工参与的自动Elo评分基准）。


<details>
  <summary>Details</summary>
Motivation: 在代码领域，人工评估大语言模型生成内容的质量极具挑战性，因为需要理解大量原始代码并模拟执行过程。现有平台缺乏对代码执行的支持，难以全面评估模型的代码生成能力。

Method: 基于Chatbot Arena构建BigCodeArena平台，集成即时代码执行环境，支持人类评估者与代码执行过程交互；收集了14,000多个涵盖10种语言和8类执行环境的对话会话，从中提取4,700多个多轮对话样本及人类偏好数据；基于此构建BigCodeReward和AutoCodeArena两个基准。

Result: 分析揭示了大语言模型在不同任务、语言和框架下的细粒度偏好；BigCodeReward评估显示，当有执行结果时，大多数模型在判断代码偏好方面表现更优；AutoCodeArena自动评估表明，GPT-5、Claude-Sonnet-4和Claude-Opus-4等闭源模型在代码生成方面仍处于领先地位。

Conclusion: BigCodeArena为代码生成模型提供了更真实、可交互的人类评估环境，所构建的两个基准有助于系统性评估模型的代码理解与生成能力，且自动评估方法在减少人工依赖的同时仍能有效反映模型性能。

Abstract: Crowdsourced model evaluation platforms, such as Chatbot Arena, enable
real-time evaluation from human perspectives to assess the quality of model
responses. In the coding domain, manually examining the quality of
LLM-generated content is extremely challenging, as it requires understanding
long chunks of raw code and deliberately simulating code execution. To this
end, we introduce BigCodeArena, an open human evaluation platform for code
generation backed by a comprehensive and on-the-fly execution environment.
Built on top of Chatbot Arena, BigCodeArena enables the execution of
LLM-generated code and allows humans to interact with the execution process and
outcomes. We collected over 14,000 raw code-centric conversation sessions
across 10 widely used LLMs, spanning 10 languages and 8 types of execution
environments. Among these conversations, we identified more than 4,700
multi-turn samples with pairwise human preferences. Further analysis uncovers
underexplored preferences of LLMs in fine-grained domains characterized by
tasks, languages, and frameworks. To systematically examine code understanding
and generation capabilities of frontier LLMs, we curated two benchmarks based
on the collected data, namely BigCodeReward and AutoCodeArena. For
BigCodeReward, we post-processed the 4,700 conversations and evaluated the
consistency between reward models and human preferences. The evaluation shows
that most LLMs have superior performance in judging coding preferences when the
execution results are available. Inspired by these findings, we propose
AutoCodeArena, an automatic Elo rating benchmark designed to assess the coding
quality of LLMs without human involvement. We find that proprietary LLMs like
GPT-5, Claude-Sonnet-4, and Claude-Opus-4 still lead in code generation
performance among recent emerging models.

</details>


### [16] [Search-based Hyperparameter Tuning for Python Unit Test Generation](https://arxiv.org/abs/2510.08716)
*Stephan Lukasczyk,Gordon Fraser*

Main category: cs.SE

TL;DR: 本文研究使用差分进化算法对DynaMOSA和MIO多目标搜索算法的超参数进行调优，结果表明调优后的DynaMOSA显著提升了测试套件的覆盖率，且差分进化比网格搜索更高效。


<details>
  <summary>Details</summary>
Motivation: 搜索式测试生成算法有大量配置选项，但用户通常使用默认值，这可能导致次优结果；而传统超参数调优方法资源消耗高，因此需要更高效的调优策略。

Method: 采用差分进化算法对Pynguin框架中DynaMOSA和MIO算法的超参数进行自动调优，并与网格搜索进行效率对比。

Result: 调优后的DynaMOSA显著提高了测试套件的覆盖率，且差分进化在调优效率上优于基本的网格搜索。

Conclusion: 差分进化是一种高效且有效的超参数调优方法，能显著提升搜索式测试生成算法的性能。

Abstract: Search-based test-generation algorithms have countless configuration options.
Users rarely adjust these options and usually stick to the default values,
which may not lead to the best possible results. Tuning an algorithm's
hyperparameters is a method to find better hyperparameter values, but it
typically comes with a high demand of resources. Meta-heuristic search
algorithms -- that effectively solve the test-generation problem -- have been
proposed as a solution to also efficiently tune parameters. In this work we
explore the use of differential evolution as a means for tuning the
hyperparameters of the DynaMOSA and MIO many-objective search algorithms as
implemented in the Pynguin framework. Our results show that significant
improvement of the resulting test suite's coverage is possible with the tuned
DynaMOSA algorithm and that differential evolution is more efficient than basic
grid search.

</details>


### [17] [PyMigTool: a tool for end-to-end Python library migration](https://arxiv.org/abs/2510.08810)
*Mohayeminul Islam,Ajay Kumar Jha,May Mahmoud,Sarah Nadi*

Main category: cs.SE

TL;DR: 本文提出了一种基于大语言模型（LLM）的端到端Python库迁移工具PyMigTool，结合静态与动态分析，在717个真实项目中实现了32%完全正确的自动迁移。


<details>
  <summary>Details</summary>
Motivation: 手动库迁移耗时且易错，现有自动化方法大多仅支持有限库或停留在API映射阶段，缺乏通用性和端到端能力。

Method: 利用大语言模型作为核心引擎，结合静态分析和动态分析，构建命令行工具PyMigTool，并在包含321个真实迁移案例的基准上评估LLM能力后进行优化。

Result: 在717个非基准真实Python项目中，PyMigTool实现了32%完全正确的迁移；其余项目中，超过一半只需开发者修复不到14%的迁移相关代码。

Conclusion: 大语言模型在库迁移任务中表现良好，结合后处理与程序分析技术可显著提升迁移准确率，PyMigTool为通用库迁移提供了可行的端到端解决方案。

Abstract: Library migration is the process of replacing a library with a similar one in
a software project. Manual library migration is time consuming and error prone,
as it requires developers to understand the Application Programming Interfaces
(API) of both libraries, map equivalent APIs, and perform the necessary code
transformations. Due to the difficulty of the library migration process, most
of the existing automated techniques and tooling stop at the API mapping stage
or support a limited set of libraries and code transformations. In this paper,
we develop an end-to-end solution that can automatically migrate code between
any arbitrary pair of Python libraries that provide similar functionality. Due
to the promising capabilities of Large Language Models (LLMs) in code
generation and transformation, we use LLMs as the primary engine for migration.
Before building the tool, we first study the capabilities of LLMs for library
migration on a benchmark of 321 real-world library migrations. We find that
LLMs can effectively perform library migration, but some post-processing steps
can further improve the performance. Based on this, we develop PyMigTool, a
command line application that combines the power of LLMs, static analysis, and
dynamic analysis to provide accurate library migration. We evaluate PyMigTool
on 717 real-world Python applications that are not from our benchmark. We find
that PyMigTool can migrate 32% of the migrations with complete correctness. Of
the remaining migrations, only 14% of the migration-related changes are left
for developers to fix for more than half of the projects.

</details>


### [18] [McMining: Automated Discovery of Misconceptions in Student Code](https://arxiv.org/abs/2510.08827)
*Erfan Al-Hossami,Razvan Bunescu*

Main category: cs.SE

TL;DR: 本文提出McMining任务，用于从学生代码中挖掘编程误解，并构建了一个包含误解标注和代码样本的基准数据集；实验表明Gemini、Claude和GPT系列大语言模型能有效识别这些误解。


<details>
  <summary>Details</summary>
Motivation: 学生在学习编程时常对语言概念产生误解，这不仅导致代码错误或低效，还阻碍相关概念的学习，因此需要系统化方法来识别和分析这些误解。

Method: 作者提出了McMining任务，构建了一个可扩展的误解基准数据集及配套的学生代码样本，并设计了两种基于大语言模型（LLM）的McMiner方法进行误解挖掘。

Result: 通过大量实验评估，发现Gemini、Claude和GPT系列模型在识别学生代码中的编程误解方面表现有效。

Conclusion: 大语言模型可用于有效挖掘学生代码中的编程误解，所提出的McMining任务和数据集为后续研究提供了基础。

Abstract: When learning to code, students often develop misconceptions about various
programming language concepts. These can not only lead to bugs or inefficient
code, but also slow down the learning of related concepts. In this paper, we
introduce McMining, the task of mining programming misconceptions from samples
of code from a student. To enable the training and evaluation of McMining
systems, we develop an extensible benchmark dataset of misconceptions together
with a large set of code samples where these misconceptions are manifested. We
then introduce two LLM-based McMiner approaches and through extensive
evaluations show that models from the Gemini, Claude, and GPT families are
effective at discovering misconceptions in student code.

</details>


### [19] [Identifying Video Game Debugging Bottlenecks: An Industry Perspective](https://arxiv.org/abs/2510.08834)
*Carlos Pinto Gomez,Fabio Petrillo*

Main category: cs.SE

TL;DR: 本文研究了视频游戏开发中特有的调试技术，分析了20位资深开发者在处理关键漏洞（如崩溃、对象行为和对象持久性）时的调试活动、工具使用及跨职能协作，发现开发者36.6%的时间用于检查游戏产物，35.1%用于本地复现漏洞。


<details>
  <summary>Details</summary>
Motivation: 传统软件调试方法不足以应对视频游戏的独特需求，因此需要探索适用于游戏开发的专属调试技术与实践。

Method: 通过记录游戏工作室中20位资深开发者在处理关键漏洞时的调试会话，进行主题分析，识别调试活动、工具使用及团队协作模式。

Result: 发现开发者在调试中最耗时的活动是检查游戏产物（36.6%）和本地复现漏洞（35.1%），并识别出常用调试工具及技术角色在协作中的核心地位。

Conclusion: 视频游戏调试依赖于特定技术和跨学科协作，其中技术角色起关键作用；优化调试流程应聚焦于减少检查与复现漏洞所需时间。

Abstract: Conventional debugging techniques used in traditional software are similarly
used when debugging video games. However, the reality of video games require
its own set of unique debugging techniques such as On-Screen Console, Debug
Draws, Debug Camera, Cheats and In-Game Menus, and Data Scrubbing. In this
article, we provide insights from a video game studio on how 20 seasoned
industry game developers debug during the production of a game. Our experiments
rely on the recordings of debugging sessions for the most critical bugs
categorized as Crashes, Object Behaviors, and Object Persistence. In this
paper, we focus on identifying the debugging activities that bottleneck bug
resolution. We also identify the debugging tools used to perform debugging
techniques. Lastly, we present how different disciplines collaborate during
debugging and how technical roles are at the core of debugging. Our thematic
analysis has identified game developers spend 36.6\% of their time inspecting
game artifacts and 35.1\% of their time reproducing the bug locally.

</details>


### [20] [Repository-Aware File Path Retrieval via Fine-Tuned LLMs](https://arxiv.org/abs/2510.08850)
*Vasudha Yanuganti,Ishaan Puri,Swapnil Chhatre,Mantinder Singh,Ashok Jallepalli,Hritvik Shrivastava,Pradeep Kumar Sharma*

Main category: cs.SE

TL;DR: 本文提出一种通过微调大语言模型（Qwen3-8B）直接从自然语言查询中预测相关文件路径的方法，结合六种基于代码结构的训练数据生成策略，在多个Python项目上实现了高达91%的精确匹配和93%的召回率。


<details>
  <summary>Details</summary>
Motivation: 现代代码库使得开发者和AI编程助手难以在回答“该功能如何工作？”或“错误是在哪里引入的？”等问题时准确定位相关源文件。传统代码搜索方法缺乏语义上下文和跨文件链接信息，而大语言模型虽理解自然语言但缺少代码库特定细节。

Method: 使用QLoRA和Unsloth优化对Qwen3-8B模型进行微调，使其能直接从自然语言查询预测相关文件路径；通过六种基于抽象语法树（AST）和代码库内容的策略生成包含文件路径作为答案的问答对用于训练。

Result: 在Flask、Click、Jinja、FastAPI和PyTorch等Python项目上微调后，模型在保留查询上达到最高91%的精确匹配率和93%的召回率；在约4000个Python文件的PyTorch大型代码库中，召回率达59%，展现出良好的可扩展性。

Conclusion: 多层级代码信号有助于大语言模型理解跨文件上下文；该方法显著优于单一策略训练，但在超大代码库中仍受限于上下文长度等问题，未来可结合检索与大模型代码智能进一步优化。

Abstract: Modern codebases make it hard for developers and AI coding assistants to find
the right source files when answering questions like "How does this feature
work?" or "Where was the bug introduced?" Traditional code search (keyword or
IR based) often misses semantic context and cross file links, while large
language models (LLMs) understand natural language but lack repository specific
detail. We present a method for file path retrieval that fine tunes a strong
LLM (Qwen3-8B) with QLoRA and Unsloth optimizations to predict relevant file
paths directly from a natural language query. To build training data, we
introduce six code aware strategies that use abstract syntax tree (AST)
structure and repository content to generate realistic question-answer pairs,
where answers are sets of file paths. The strategies range from single file
prompts to hierarchical repository summaries, providing broad coverage. We fine
tune on Python projects including Flask, Click, Jinja, FastAPI, and PyTorch,
and obtain high retrieval accuracy: up to 91\% exact match and 93\% recall on
held out queries, clearly beating single strategy training. On a large codebase
like PyTorch (about 4,000 Python files), the model reaches 59\% recall, showing
scalability. We analyze how multi level code signals help the LLM reason over
cross file context and discuss dataset design, limits (for example, context
length in very large repos), and future integration of retrieval with LLM based
code intelligence.

</details>


### [21] [Vector Graph-Based Repository Understanding for Issue-Driven File Retrieval](https://arxiv.org/abs/2510.08876)
*Kostiantyn Bevziuk,Andrii Fatula,Svetozar Lashin Yaroslav Opanasenko,Anna Tukhtarova,Ashok Jallepalli Pradeepkumar Sharma,Hritvik Shrivastava*

Main category: cs.SE

TL;DR: 本文提出了一种将大型软件仓库转化为向量化知识图谱的系统，该图谱反映项目的架构与语义结构，支持后续开发的高度自动化。


<details>
  <summary>Details</summary>
Motivation: 为了解决大型软件仓库在理解和自动化开发方面的挑战，作者旨在构建一个能同时捕捉语法关系和语义信息的知识表示系统。

Method: 该系统构建一个知识图谱，编码包含、实现、引用、调用和继承等语法关系，并利用大语言模型（LLM）为节点生成摘要和向量嵌入；通过结合语义检索与图感知扩展的混合检索流程，以及基于LLM的助手生成只读图查询和面向人类的解释。

Result: 该方法成功将软件仓库转化为结构化、语义丰富的知识图谱，显著提升了仓库理解与自动化开发的能力。

Conclusion: 所提出的系统有效融合了软件结构与语义信息，为软件仓库的自动化分析与开发提供了可行路径。

Abstract: We present a repository decomposition system that converts large software
repositories into a vectorized knowledge graph which mirrors project
architectural and semantic structure, capturing semantic relationships and
allowing a significant level of automatization of further repository
development. The graph encodes syntactic relations such as containment,
implementation, references, calls, and inheritance, and augments nodes with
LLM-derived summaries and vector embeddings. A hybrid retrieval pipeline
combines semantic retrieval with graph-aware expansion, and an LLM-based
assistant formulates constrained, read-only graph requests and produces
human-oriented explanations.

</details>


### [22] [SEER: Sustainability Enhanced Engineering of Software Requirements](https://arxiv.org/abs/2510.08981)
*Mandira Roy,Novarun Deb,Nabendu Chaki,Agostino Cortesi*

Main category: cs.SE

TL;DR: 本文提出SEER框架，利用大语言模型和RAG技术，在软件开发早期阶段识别、评估并优化可持续性需求，实验表明其在多个领域中有效识别广泛的可持续性问题。


<details>
  <summary>Details</summary>
Motivation: 现有可持续软件开发方法多为高层指导，实施耗时且依赖团队适应性，且通常聚焦于设计或实现阶段，而忽视了在需求工程阶段就进行可持续性评估的重要性。

Method: SEER框架分为三个阶段：(i) 从通用分类法中识别特定软件产品的可持续性需求（SRs）；(ii) 基于SRs评估系统需求的可持续性；(iii) 对未满足SRs的系统需求进行优化。该框架结合大语言模型的推理能力和RAG（检索增强生成）方法实现。

Result: 在四个不同领域的软件项目上进行实验，使用Gemini 2.5推理模型生成的结果表明，SEER能准确识别跨领域的广泛可持续性问题。

Conclusion: SEER框架有效支持在软件开发早期阶段系统化地处理可持续性需求，为实现联合国可持续发展目标提供了可行的技术路径。

Abstract: The rapid expansion of software development has significant environmental,
technical, social, and economic impacts. Achieving the United Nations
Sustainable Development Goals by 2030 compels developers to adopt sustainable
practices. Existing methods mostly offer high-level guidelines, which are
time-consuming to implement and rely on team adaptability. Moreover, they focus
on design or implementation, while sustainability assessment should start at
the requirements engineering phase. In this paper, we introduce SEER, a
framework which addresses sustainability concerns in the early software
development phase. The framework operates in three stages: (i) it identifies
sustainability requirements (SRs) relevant to a specific software product from
a general taxonomy; (ii) it evaluates how sustainable system requirements are
based on the identified SRs; and (iii) it optimizes system requirements that
fail to satisfy any SR. The framework is implemented using the reasoning
capabilities of large language models and the agentic RAG (Retrieval Augmented
Generation) approach. SEER has been experimented on four software projects from
different domains. Results generated using Gemini 2.5 reasoning model
demonstrate the effectiveness of the proposed approach in accurately
identifying a broad range of sustainability concerns across diverse domains.

</details>


### [23] [Towards a Taxonomy of Sustainability Requirements for Software Design](https://arxiv.org/abs/2510.08990)
*Mandira Roy,Novarun Deb,Nabendu Chaki,Agostino Cortesi*

Main category: cs.SE

TL;DR: 本文通过系统性文献综述构建了一个涵盖环境、技术、社会和经济四个维度的可持续性需求综合分类体系，并提供了各类别的定义、度量指标及维度间的协同与冲突关系矩阵，以支持可持续软件开发中的需求制定与权衡管理。


<details>
  <summary>Details</summary>
Motivation: 现有研究中的可持续性需求往往碎片化、局限于特定维度或应用领域，缺乏统一全面的分类体系，阻碍了软件工程社区在需求工程阶段系统性地应对可持续性挑战。

Method: 开展系统性文献综述（SLR），从现有研究中提取并整可持续性需求，构建涵盖四个可持续性维度的分类体系，并为每个类别提供定义、度量指标，同时建立跨维度类别间的协同与冲突关系矩阵。

Result: 提出了一个全面的可持续性需求分类法，包含清晰的类别定义、相关度量指标，并展示了不同维度间类别的正负影响关系（协同与冲突）。

Conclusion: 该分类体系为软件开发者和研究人员提供了一个系统化的参考框架，有助于在可持续软件开发中有效制定、管理可持续性需求并协调其中的权衡关系。

Abstract: Software systems are a significant contributor to global sustainability
concerns, demanding that environmental, social, technical, and economic factors
be systematically addressed from the initial requirements engineering phase.
Although existing research provides various sustainability requirements (SRs),
these contributions are often fragmented, specific to certain dimensions, or
limited to particular application domains, resulting in a critical lack of a
unified, comprehensive taxonomy for the software engineering community. To
address this gap, this research conducts a Systematic Literature Review (SLR)
to extract and organize sustainability requirements from the state-of-the-art.
The primary contribution is a comprehensive taxonomy of SRs across the four
dimensions of sustainability (environmental, technical, social, and economic).
For each identified category, we provide clear definitions, associated metrics,
and measures. Furthermore, we depict a correlation matrix that projects the
positive and negative influences (synergies and conflicts) among categories
across different dimensions. This systematized reference assists both software
developers and researchers in effectively formulating, managing, and
reconciling trade-offs within sustainable software development.

</details>


### [24] [Saving SWE-Bench: A Benchmark Mutation Approach for Realistic Agent Evaluation](https://arxiv.org/abs/2510.08996)
*Spandan Garg,Ben Steenhoek,Yufan Huang*

Main category: cs.SE

TL;DR: 本文提出了一种新的基准测试框架，通过分析开发者与聊天式编码助手的真实交互模式，将现有正式基准（如SWE-Bench Verified）转化为更贴近实际的用户查询，发现当前基准显著高估了智能体在真实场景中的能力。


<details>
  <summary>Details</summary>
Motivation: 现有软件工程智能体的评估基准（如SWE-Bench Verified）主要基于GitHub问题，无法准确反映开发者在IDE中与聊天式编码助手的真实交互方式，导致对智能体能力的系统性高估，尤其是在修复bug任务中。

Method: 作者通过分析流行聊天式智能体的遥测数据，系统性地将正式的GitHub问题描述转化为贴近真实用户行为的查询，并将该方法应用于SWE-Bench Verified、Multi-SWE-Bench的TypeScript子集以及私有的SWE-Bench C#基准。

Result: 实验结果显示，现有基准对某些模型的能力高估超过50%（公开基准）和约10–16%（内部基准），表明当前评估方式与真实使用场景存在显著差距。

Conclusion: 该研究提出了一种通过基准变异技术评估交互式聊天软件工程智能体的新范式，强调了构建更贴近真实用户交互的评估体系的重要性。

Abstract: Current benchmarks for evaluating software engineering agents, such as
SWE-Bench Verified, are predominantly derived from GitHub issues and fail to
accurately reflect how developers interact with chat-based coding assistants in
integrated development environments (IDEs). We posit that this mismatch leads
to a systematic overestimation of agent's capabilities in real-world scenarios,
especially bug fixing. We introduce a novel benchmarking framework that
transforms existing formal benchmarks into realistic user queries through
systematic analysis of developer interaction patterns with chat-based agents.
Our methodology is flexible and can be easily extended to existing benchmarks.
In this paper, we apply our testing framework to SWE-Bench Verified, the
TypeScript subset of Multi-SWE-Bench and a private benchmark, SWE-Bench C# and
transform formal GitHub issue descriptions into realistic user-style queries
based on telemetry analysis of a popular chat-based agent interactions. Our
findings reveal that existing benchmarks significantly overestimate agent
capabilities for some models by >50% over baseline performance for public
benchmarks and ~10-16% for our internal benchmark. This work establishes a new
paradigm for evaluating interactive chat-based software engineering agents
through benchmark mutation techniques.

</details>


### [25] [Cost-Efficient Long Code Translation using LLMs while Leveraging Identifier Replacements](https://arxiv.org/abs/2510.09045)
*Manojit Chakraborty,Madhusudan Ghosh,Rishabh Gupta*

Main category: cs.SE

TL;DR: 提出一种零样本代码翻译方法，通过替换长标识符为通用占位符，减少上下文长度，提升大语言模型对长代码的翻译准确性和效率。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在处理超出上下文窗口的长代码时，难以准确完成代码翻译任务。

Method: 在零样本设置下，将用户提供的长标识符替换为通用占位符，以降低输入长度和内存占用，使模型更专注于代码逻辑结构。

Result: 该方法在保持代码语法和层次结构信息的同时，显著减少了翻译所需的token数量。

Conclusion: 所提方法有效提升了长代码翻译的效率、准确性和成本效益。

Abstract: In the domain of software development, LLMs have been utilized to automate
tasks such as code translation, where source code from one programming language
is translated to another while preserving its functionality. However, LLMs
often struggle with long source codes that don't fit into the context window,
which produces inaccurate translations. To address this, we propose a novel
zero-shot code translation method that incorporates identifier replacement. By
substituting user-given long identifiers with generalized placeholders during
translation, our method allows the LLM to focus on the logical structure of the
code, by reducing token count and memory usage, which improves the efficiency
and cost-effectiveness of long code translation. Our empirical results
demonstrate that our approach preserves syntactical and hierarchical
information and produces translation results with reduced tokens.

</details>


### [26] [Model-Assisted and Human-Guided: Perceptions and Practices of Software Professionals Using LLMs for Coding](https://arxiv.org/abs/2510.09058)
*Italo Santos,Cleyton Magalhaes,Ronnie de Souza Santos*

Main category: cs.SE

TL;DR: 一项针对131名软件从业者的全球调查显示，大型语言模型（LLMs）在编码任务中被广泛用作辅助工具，能提升生产力并降低认知负担，但也存在输出不准确、上下文理解有限和伦理风险等问题。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型（LLMs）在软件开发中日益普及，但对其在实际中的使用方式及从业者对其优缺点的看法仍缺乏深入理解。

Method: 通过全球问卷调查收集131名软件从业者的反馈，分析LLMs在软件开发中的实际应用、感知优势与局限性。

Result: 从业者主要将LLMs用于编码相关任务，认为其提高了生产力、减轻了认知负担并加速学习，但也担忧其输出不准确、上下文感知能力有限及伦理风险；多数人将其视为辅助工具而非独立解决方案。

Conclusion: 该研究提供了早期、以从业者为中心的LLM应用视角，强调了未来研究和在软件工程中负责任使用LLM的关键考量。

Abstract: Large Language Models have quickly become a central component of modern
software development workflows, and software practitioners are increasingly
integrating LLMs into various stages of the software development lifecycle.
Despite the growing presence of LLMs, there is still a limited understanding of
how these tools are actually used in practice and how professionals perceive
their benefits and limitations. This paper presents preliminary findings from a
global survey of 131 software practitioners. Our results reveal how LLMs are
utilized for various coding-specific tasks. Software professionals report
benefits such as increased productivity, reduced cognitive load, and faster
learning, but also raise concerns about LLMs' inaccurate outputs, limited
context awareness, and associated ethical risks. Most developers treat LLMs as
assistive tools rather than standalone solutions, reflecting a cautious yet
practical approach to their integration. Our findings provide an early,
practitioner-focused perspective on LLM adoption, highlighting key
considerations for future research and responsible use in software engineering.

</details>


### [27] [Literate Tracing](https://arxiv.org/abs/2510.09073)
*Matthew Sotoudeh*

Main category: cs.SE

TL;DR: 本文提出“文学化追踪”（literate tracing）作为一种新的程序文档范式，通过带注释的具体执行轨迹来解释软件系统，并介绍了支持该范式的工具TReX，该工具能生成交互式、可视化且语义忠实的文档。


<details>
  <summary>Details</summary>
Motivation: 随着计算机系统日益庞大复杂，系统专家向新手清晰传达程序工作原理变得至关重要。现有文档方式（如代码注释和设计文档）分别缺乏全局上下文或与代码的具体联系，因此需要一种更有效的解释方法。

Method: 作者提出“文学化追踪”方法，结合具体执行轨迹与注释来解释系统，并开发了名为TReX的工具，用于生成交互式、可视化且语义保真的文学化追踪文档。

Result: TReX已被用于为Linux内核、Git版本控制系统和GCC编译器等大型系统软件的关键组件编写文学化追踪文档，验证了该方法的实用性与有效性。

Conclusion: 文学化追踪有效弥补了传统文档方式的不足，TReX工具支持生成高质量、可信赖的程序解释文档，有助于提升软件系统的可理解性与可维护性。

Abstract: As computer systems grow ever larger and more complex, a crucial task in
software development is for one person (the system expert) to communicate to
another (the system novice) how a certain program works. This paper reports on
the author's experiences with a paradigm for program documentation that we call
literate tracing. A literate trace explains a software system using annotated,
concrete execution traces of the system. Literate traces complement both
in-code comments (which often lack global context) and out-of-band design docs
(which often lack a concrete connection to the code). We also describe TReX,
our tool for making literate traces that are interactive, visual, and
guaranteed by construction to be faithful to the program semantics. We have
used TReX to write literate traces explaining components of large systems
software including the Linux kernel, Git source control system, and GCC
compiler.

</details>


### [28] [Constraint-Guided Unit Test Generation for Machine Learning Libraries](https://arxiv.org/abs/2510.09108)
*Lukas Krodinger,Altin Hajdari,Stephan Lukasczyk,Gordon Fraser*

Main category: cs.SE

TL;DR: 本文提出PynguinML，通过利用机器学习API的输入约束改进Pynguin测试生成器，显著提升对PyTorch和TensorFlow库的测试覆盖率。


<details>
  <summary>Details</summary>
Motivation: 现有自动化测试工具（如Pynguin）不了解机器学习API的复杂输入约束，导致生成无效输入、测试提前失败及代码覆盖率低。

Method: PynguinML改进了Pynguin测试生成器，使其能够利用从官方API文档中提取的输入约束，生成符合要求的测试输入。

Result: 在165个PyTorch和TensorFlow模块上的实验表明，PynguinML相比原始Pynguin最多可提升63.9%的代码覆盖率。

Conclusion: 将API输入约束整合到测试生成过程中，能有效提升机器学习库的测试质量和覆盖范围。

Abstract: Machine learning (ML) libraries such as PyTorch and TensorFlow are essential
for a wide range of modern applications. Ensuring the correctness of ML
libraries through testing is crucial. However, ML APIs often impose strict
input constraints involving complex data structures such as tensors. Automated
test generation tools such as Pynguin are not aware of these constraints and
often create non-compliant inputs. This leads to early test failures and
limited code coverage. Prior work has investigated extracting constraints from
official API documentation. In this paper, we present PynguinML, an approach
that improves the Pynguin test generator to leverage these constraints to
generate compliant inputs for ML APIs, enabling more thorough testing and
higher code coverage. Our evaluation is based on 165 modules from PyTorch and
TensorFlow, comparing PynguinML against Pynguin. The results show that
PynguinML significantly improves test effectiveness, achieving up to 63.9 %
higher code coverage.

</details>


### [29] [A Semantic Framework for Patient Digital Twins in Chronic Care](https://arxiv.org/abs/2510.09134)
*Amal Elgammal,Bernd J. Krämer,Michael P. Papazoglou,Mira Raheem*

Main category: cs.SE

TL;DR: 本文提出了患者医疗数字孪生（PMDT）框架，通过本体驱动的方式整合多模态健康数据，在保障隐私的前提下支持慢性病的精准、主动和个性化管理。


<details>
  <summary>Details</summary>
Motivation: 当前数字孪生应用多局限于单一器官或孤立数据类型，缺乏统一且保护隐私的整合框架，难以支持个性化慢性病管理所需的多维度数据融合与智能决策。

Method: 构建基于OWL 2.0的本体驱动数字孪生模型PMDT，围绕患者、疾病、治疗等七个模块化蓝图设计，并通过专家研讨、问卷及欧盟QUALITOP项目中的免疫治疗患者试点进行迭代验证。

Result: 评估表明PMDT在本体覆盖度、推理正确性、可用性及GDPR合规性方面表现良好，能有效整合异构数据，支持描述性、预测性和规范性分析，并在联邦架构下保障隐私。

Conclusion: PMDT为下一代数字健康生态系统提供了经过验证的基础，有助于实现慢性病管理向主动、持续优化和公平方向的转型。

Abstract: Personalized chronic care requires the integration of multimodal health data
to enable precise, adaptive, and preventive decision-making. Yet most current
digital twin (DT) applications remain organ-specific or tied to isolated data
types, lacking a unified and privacy-preserving foundation. This paper
introduces the Patient Medical Digital Twin (PMDT), an ontology-driven in
silico patient framework that integrates physiological, psychosocial,
behavioral, and genomic information into a coherent, extensible model.
Implemented in OWL 2.0, the PMDT ensures semantic interoperability, supports
automated reasoning, and enables reuse across diverse clinical contexts. Its
ontology is structured around modular Blueprints (patient, disease and
diagnosis, treatment and follow-up, trajectories, safety, pathways, and adverse
events), formalized through dedicated conceptual views. These were iteratively
refined and validated through expert workshops, questionnaires, and a pilot
study in the EU H2020 QUALITOP project with real-world immunotherapy patients.
Evaluation confirmed ontology coverage, reasoning correctness, usability, and
GDPR compliance. Results demonstrate the PMDT's ability to unify heterogeneous
data, operationalize competency questions, and support descriptive, predictive,
and prescriptive analytics in a federated, privacy-preserving manner. By
bridging gaps in data fragmentation and semantic standardization, the PMDT
provides a validated foundation for next-generation digital health ecosystems,
transforming chronic care toward proactive, continuously optimized, and
equitable management.

</details>


### [30] [A Model-Driven Engineering Approach to AI-Powered Healthcare Platforms](https://arxiv.org/abs/2510.09308)
*Mira Raheem,Amal Elgammal,Michael Papazoglou,Bernd Krämer,Neamat El-Tazi*

Main category: cs.SE

TL;DR: 本文提出了一种面向医疗AI的模型驱动工程（MDE）框架，结合图形化领域特定语言MILA与联邦学习架构，在保护患者隐私的同时实现跨机构语义一致的协作建模，并在癌症免疫治疗研究中验证了其高预测准确率和开发效率。


<details>
  <summary>Details</summary>
Motivation: 医疗AI的实际应用受限于数据孤岛、隐私法规和技术复杂性，亟需一种能兼顾语义一致性、隐私保护和工程效率的解决方案。

Method: 提出基于模型驱动工程的框架，利用形式化元模型、领域特定语言MILA和自动化转换技术，结合联邦学习架构，支持临床人员与数据科学家协作定义查询与机器学习流程。

Result: 在多中心癌症免疫治疗研究中，生成的管道在关键任务上达到98.5%和98.3%的准确率，并显著减少手动编码工作量。

Conclusion: 模型驱动工程的核心原则（元建模、语义集成和自动代码生成）为构建可互操作、可复现且可信的数字健康平台提供了可行路径。

Abstract: Artificial intelligence (AI) has the potential to transform healthcare by
supporting more accurate diagnoses and personalized treatments. However, its
adoption in practice remains constrained by fragmented data sources, strict
privacy rules, and the technical complexity of building reliable clinical
systems. To address these challenges, we introduce a model driven engineering
(MDE) framework designed specifically for healthcare AI. The framework relies
on formal metamodels, domain-specific languages (DSLs), and automated
transformations to move from high level specifications to running software. At
its core is the Medical Interoperability Language (MILA), a graphical DSL that
enables clinicians and data scientists to define queries and machine learning
pipelines using shared ontologies. When combined with a federated learning
architecture, MILA allows institutions to collaborate without exchanging raw
patient data, ensuring semantic consistency across sites while preserving
privacy. We evaluate this approach in a multi center cancer immunotherapy
study. The generated pipelines delivered strong predictive performance, with
support vector machines achieving up to 98.5 percent and 98.3 percent accuracy
in key tasks, while substantially reducing manual coding effort. These findings
suggest that MDE principles metamodeling, semantic integration, and automated
code generation can provide a practical path toward interoperable,
reproducible, and trustworthy digital health platforms.

</details>


### [31] [TIT: A Tree-Structured Instruction Tuning Approach for LLM-Based Code Translation](https://arxiv.org/abs/2510.09400)
*He Jiang,Yufu Wang,Hao Lin,Peiyu Zou,Zhide Zhou,Ang Jia,Xiaochen Li,Zhilei Ren*

Main category: cs.SE

TL;DR: 本文提出TIT（Tree-structured Instruction Tuning）方法，通过结构化解析和细粒度对齐提升大语言模型在代码翻译中的语法正确性与语义一致性，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 主流大语言模型在代码翻译中存在两个关键问题：一是对语言特异性特征敏感，导致输出混入源语言语法或词汇，引发语法混淆；二是过度依赖函数级平行数据，缺乏细粒度语义对齐，造成翻译结果与源代码语义不一致。

Method: TIT包含三个模块：1）语法信息表示模块，通过结构化解析引入语言无关的语法特征以缓解语法混淆；2）细粒度平行数据增强模块，通过语句级分割与对比匹配对齐代码片段；3）双阶段树结构指令微调模块，第一阶段进行语法感知微调以理解结构化语法信息，第二阶段进行代码生成微调以基于函数级语法依赖生成准确目标代码。

Result: 实验表明，TIT在多个大语言模型上显著优于现有方法，代码翻译成功率提升1.22至1.75倍，并显著减少语法混淆。

Conclusion: TIT通过融合语言无关的语法结构与细粒度语义对齐，有效解决了大语言模型在代码翻译中的语法混淆与语义错位问题，显著提升了翻译质量。

Abstract: Large Language Models (LLMs) have shown strong performance in automated
source-to-target code translation through pretraining on extensive code
corpora. However, mainstream LLM-based code translation methods suffer from two
critical limitations. First, they are highly sensitive to language-specific
features, which often introduce source-language syntax or lexicon into the
output, leading to syntactic confusion. Second, they lack fine-grained semantic
alignment due to an over-reliance on function-level parallel datasets,
resulting in semantic misalignment between the translated code and the original
source. To overcome these limitations, we propose TIT, a Tree-structured
Instruction Tuning paradigm for LLM-based code translation. Specifically, TIT
consists of three modules. First, to mitigate syntactic confusion, the
syntactic information representation module integrates language-agnostic
syntactic features via structured parsing. Then, to generate high-quality
fine-grained parallel data, the fine-grained parallel dataset augmentation
module aligns nodes with code segments through statement-level segmentation and
contrastive matching. Finally, we leverage the dual-stage tree instruction
tuning module to alleviate the contextual processing burden on the LLM caused
by the introduction of syntactic information. The first stage employs
syntax-aware fine-tuning to enable the LLM to autonomously comprehend
structured syntactic information, while the second stage utilizes code
generation fine-tuning to guide the model in generating accurate target code
based on function-level syntactic dependencies. The experimental results
demonstrate that the proposed method significantly outperforms existing
approaches in multiple LLMs, achieving a success rate 1.22x-1.75x higher in
code translation while markedly reducing syntactic confusion.

</details>
