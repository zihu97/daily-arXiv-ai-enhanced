{"id": "2602.18620", "categories": ["cs.NI"], "pdf": "https://arxiv.org/pdf/2602.18620", "abs": "https://arxiv.org/abs/2602.18620", "authors": ["Luca Lusvarghi", "Javier Gozalvez"], "title": "On the Inherent Resilience of Task-Oriented V2X Networks to Content-Selection Errors", "comment": null, "summary": "Task-oriented Vehicle-to-Everything (V2X) networks have recently been proposed to scalably support the large-scale deployment of connected vehicles within the Internet of Vehicles (IoV) vision. In task-oriented V2X networks, vehicles select the content of the transmitted messages based on its relevance to the intended receivers. However, relevance estimation can be quite challenging, especially in highly dynamic and complex vehicular scenarios. Relevance estimation errors can cause a vehicle to omit relevant information from its transmitted message, leading to a content-selection error. Content-selection errors reduce the amount of relevant information available at the receivers and can potentially impair their situational awareness. This work analyses the impact of content-selection errors on task-oriented V2X networks. Our analysis reveals that task-oriented V2X networks feature an inherent resilience to content-selection errors that guarantees a consistent delivery of relevant information even under high relevance estimation error conditions. Moreover, we identify the fundamental conditions underpinning such inherent resilience. These conditions can be encountered in other task-oriented networks where multiple transmitters select the content of their messages based on the task-related requirements of a common set of intended receivers."}
{"id": "2602.18627", "categories": ["cs.NI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.18627", "abs": "https://arxiv.org/abs/2602.18627", "authors": ["Mohammad Heydari", "Terence D. Todd", "Dongmei Zhao", "George Karakostas"], "title": "Federated Learning-Assisted Optimization of Mobile Transmission with Digital Twins", "comment": null, "summary": "A Digital Twin (DT) may protect information that is considered private to its associated physical system. For a mobile device, this may include its mobility profile, recent location(s), and experienced channel conditions. Online schedulers, however, typically use this type of information to perform tasks such as shared bandwidth and channel time slot assignments. In this paper, we consider three transmission scheduling problems with energy constraints, where such information is needed, and yet must remain private: minimizing total transmission time when (i) fixed-power or (ii) fixed-rate time slotting with power control is used, and (iii) maximizing the amount of data uploaded in a fixed time period. Using a real-time federated optimization framework, we show how the scheduler can iteratively interact only with the DTs to produce global fractional solutions to these problems, without the latter revealing their private information. Then dependent rounding is used to round the fractional solution into a channel transmission schedule for the physical systems. Experiments show consistent makespan reductions with near-zero bandwidth/energy violations and millisecond-order end-to-end runtime for typical edge server hardware. To the best of our knowledge, this is the first framework that enables channel sharing across DTs using operations that do not expose private data."}
{"id": "2602.19252", "categories": ["cs.NI"], "pdf": "https://arxiv.org/pdf/2602.19252", "abs": "https://arxiv.org/abs/2602.19252", "authors": ["Junling Wang", "Yi Guo", "Bojun Yang", "Yazhou Yuan", "Zhenlin An"], "title": "MetaBlue: A Metasurface-Assisted Acoustic Underwater Localization System", "comment": null, "summary": "Underwater localization is essential for marine exploration and autonomous underwater operations, yet existing radio frequency and optical approaches are limited by rapid attenuation or limited visibility. Acoustic sensing remains the most practical choice, but conventional acoustic systems typically rely on large arrays or multiple synchronized anchors, resulting in high hardware costs and complex deployment. This paper introduces a novel low-cost passive acoustic metasurface, MetaBlue , explicitly designed for underwater localization, which, when attached to an ordinary ultrasonic transmitter, transforms it into a directional \"super-transmitter.\" The metasurface embeds direction-dependent spectral patterns into the transmitted waveform, enabling accurate angle-of-arrival (AoA) estimation using only a single hydrophone. For ranging, we present a new EM-acoustic mixed time-of-arrival (ToA) method that leverages the acoustic transducer's inherent low-frequency EM leakage as a timing reference, enabling precise ranging without shared clocks. This allows complete 3D localization with a single low-cost anchor. We evaluate the system across diverse real-world underwater settings, including pools, tanks, and outdoor environments. Experiments show that our design achieves an average AoA error of 8.7 degree and 3D localization error of 0.37 m at distances over 10 m. Even with a single anchor, the system maintains 0.73 m precision."}
{"id": "2602.18641", "categories": ["cs.DC", "physics.hist-ph", "quant-ph"], "pdf": "https://arxiv.org/pdf/2602.18641", "abs": "https://arxiv.org/abs/2602.18641", "authors": ["Paul Borrill"], "title": "The Category Mistake of Cislunar Time: Why NASA Cannot Synchronize What Doesn't Exist", "comment": "13 pages, no figures", "summary": "In April 2024, the White House directed NASA to establish Coordinated Lunar Time (LTC) by December 2026. The programme assumes that a unified time standard can be constructed by deploying atomic clocks on the lunar surface, computing relativistic corrections, and distributing synchronized time via LunaNet. This paper argues that the entire enterprise rests on a category mistake in the sense introduced by Ryle and developed by Spekkens in quantum foundations: it treats \"synchronized time\" as an ontic entity -- something that exists independently and can be transmitted from authoritative sources to dependent receivers -- when it is in fact an epistemic construct: a model-dependent representation of observer-relative clock relationships. We analyze the cislunar time programme through the lens of Forward-In-Time-Only (FITO) assumptions, Spekkens' Leibnizian operationalism, the Wood-Spekkens fine-tuning argument, and the distinction between ontic and epistemic interpretations that has dissolved long-standing puzzles in quantum mechanics. We show that the same conceptual move that dissolves quantum \"mysteries\" -- recognizing what is epistemic versus what is ontic -- dissolves the apparent coherence of the cislunar time programme and reveals it as an engineering project built on a philosophical confusion. We sketch a transactional alternative grounded in bilateral atomic interactions rather than unidirectional time distribution."}
{"id": "2602.19433", "categories": ["cs.DC", "cs.OS"], "pdf": "https://arxiv.org/pdf/2602.19433", "abs": "https://arxiv.org/abs/2602.19433", "authors": ["Paul Borrill"], "title": "Why iCloud Fails: The Category Mistake of Cloud Synchronization", "comment": "18 pages, 3 figures, 37 references", "summary": "iCloud Drive presents a filesystem interface but implements cloud synchronization semantics that diverge from POSIX in fundamental ways. This divergence is not an implementation bug; it is a Category Mistake -- the same one that pervades distributed computing wherever Forward-In-Time-Only (FITO) assumptions are embedded into protocol design. Parker et al. showed in 1983 that network partitioning destroys mutual consistency; iCloud adds a user interface that conceals this impossibility behind a facade of seamlessness. This document presents a unified analysis of why iCloud fails when composed with Time Machine, git, automated toolchains, and general-purpose developer workflows, supported by direct evidence including documented corruption events and a case study involving 366 GB of divergent state accumulated through normal use. We show that the failures arise from five interlocking incompatibilities rooted in a single structural error: the projection of a distributed causal graph onto a linear temporal chain. We then show how the same Category Mistake, when it occurs in network fabrics as link flapping, destroys topology knowledge through epistemic collapse. Finally, we argue that Open Atomic Ethernet (OAE) transactional semantics -- bilateral, reversible, and conservation-preserving -- provide the structural foundation for resolving these failures, not by defeating physics, but by aligning protocol behavior with physical reality."}
{"id": "2602.19485", "categories": ["cs.NI"], "pdf": "https://arxiv.org/pdf/2602.19485", "abs": "https://arxiv.org/abs/2602.19485", "authors": ["Angzi Xu", "Zezhong Zhang", "Zhi Liu", "Shuguang Cui"], "title": "EMS-FL: Federated Tuning of Mixture-of-Experts in Satellite-Terrestrial Networks via Expert-Driven Model Splitting", "comment": "Submitted to IEEE TMC", "summary": "The rapid advancement of large AI models imposes stringent demands on data volume and computational resources. Federated learning, though designed to exploit distributed data and computational resources, faces data shortage from limited network coverage and computational constraints from edge devices. To address these issues, both the mixture-of-experts (MoE) and satellite-terrestrial network (STN) provide promising solutions, offering lightweight computation overhead and broad coverage, respectively. However, the satellite-ground relative motion results in intermittent connectivity, hindering conventional federated learning that relies on model synchronization across devices. To leverage the coverage of STN while preserving training efficiency, we propose EMS-FL, an expert-driven model splitting and federated learning method. EMS-FL assigns each device cluster only the experts highly correlated to their local data. Through non-overlapping expert assignments, asynchronous local learning is further proposed, where each device cluster trains its assigned experts consecutively and only uploads local parameters to the satellite during connected phases for aggregation and model updates. Consequently, EMS-FL effectively reduces the training overhead and achieves both faster convergence and higher accuracy compared with conventional federated learning. Rigorous convergence analysis is provided to theoretically characterize the learning performance. Furthermore, comprehensive experiments are conducted using public datasets and large models, validating the superiority of EMS-FL."}
{"id": "2602.18723", "categories": ["cs.DC", "cs.LO", "quant-ph"], "pdf": "https://arxiv.org/pdf/2602.18723", "abs": "https://arxiv.org/abs/2602.18723", "authors": ["Paul Borrill"], "title": "What Distributed Computing Got Wrong: The Category Mistake That Turned Design Choices into Laws of Nature", "comment": "15 pages, no figures", "summary": "The foundational impossibility results of distributed computing -- the Fischer-Lynch-Paterson theorem, the Two Generals Problem, the CAP theorem -- are widely understood as discoveries about the physical limits of coordination. This paper argues that they are nothing of the sort. They are consequences of a category mistake: treating Forward-In-Time-Only (FITO) information flow as a law of nature rather than recognizing it as a design choice inherited from Shannon's channel model and Lamport's happened-before relation. We develop this argument in six steps. First, we introduce the category mistake framework from Ryle through Spekkens' ontic/epistemic distinction in quantum foundations. Second, we identify FITO as the hidden axiom that unifies the classical impossibility results. Third, we apply Spekkens' Leibnizian principle to show that FITO-based models contain surplus ontological structure. Fourth, we develop the counterfactual: what changes when FITO is dropped. Fifth, we demonstrate that the impossibility theorems are theorems about FITO systems, not about physics. Sixth, we sketch the transactional alternative -- bilateral interactions that dissolve the apparent impossibilities by replacing unidirectional message passing with atomic bilateral transactions. The implication is that distributed computing has spent fifty years optimizing within the wrong design space."}
{"id": "2602.18568", "categories": ["cs.AR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.18568", "abs": "https://arxiv.org/abs/2602.18568", "authors": ["Matthew Adiletta", "Gu-Yeon Wei", "David Brooks"], "title": "RPU -- A Reasoning Processing Unit", "comment": null, "summary": "Large language model (LLM) inference performance is increasingly bottlenecked by the memory wall. While GPUs continue to scale raw compute throughput, they struggle to deliver scalable performance for memory bandwidth bound workloads. This challenge is amplified by emerging reasoning LLM applications, where long output sequences, low arithmetic intensity, and tight latency constraints demand significantly higher memory bandwidth. As a result, system utilization drops and energy per inference rises, highlighting the need for an optimized system architecture for scalable memory bandwidth.\n  To address these challenges we present the Reasoning Processing Unit (RPU), a chiplet-based architecture designed to address the challenges of the modern memory wall. RPU introduces: (1) A Capacity-Optimized High-Bandwidth Memory (HBM-CO) that trades capacity for lower energy and cost; (2) a scalable chiplet architecture featuring a bandwidth-first power and area provisioning design; and (3) a decoupled microarchitecture that separates memory, compute, and communication pipelines to sustain high bandwidth utilization. Simulation results show that RPU performs up to 45.3x lower latency and 18.6x higher throughput over an H100 system at ISO-TDP on Llama3-405B."}
{"id": "2602.18498", "categories": ["cs.MA", "cs.GT"], "pdf": "https://arxiv.org/pdf/2602.18498", "abs": "https://arxiv.org/abs/2602.18498", "authors": ["Zhao Song", "Theodor Cimpeanu", "Chen Shen", "The Anh Han"], "title": "Evolution of fairness in hybrid populations with specialised AI agents", "comment": null, "summary": "Fairness in hybrid societies hinges on a simple choice: should AI be a generous host or a strict gatekeeper? Moving beyond symmetric models, we show that asymmetric social structures--like those in hiring, regulation, and negotiation--AI that guards fairness outperforms AI that gifts it. We bridge this gap with a bipartite hybrid population model of the Ultimatum Game, separating humans and AI into distinct proposer and receiver groups. We first introduce Samaritan AI agents, which act as either unconditional fair proposers or strict receivers. Our results reveal a striking asymmetry: Samaritan AI receivers drive population-wide fairness far more effectively than Samaritan AI proposers. To overcome the limitations of the Samaritan AI proposer, we design the Discriminatory AI proposer, which predicts co-players' expectations and only offers fair portions to those with high acceptance thresholds. Our results demonstrate that this Discriminatory AI outperforms both types of Samaritan AI, especially in strong selection scenarios. It not only sustains fairness across both populations but also significantly lowers the critical mass of agents required to reach an equitable steady state. By transitioning from unconditional modelling to strategic enforcement, our work provides a pivotal framework for deploying asymmetric AIs in the increasingly hybrid society."}
{"id": "2602.18534", "categories": ["cs.SE", "cs.PL"], "pdf": "https://arxiv.org/pdf/2602.18534", "abs": "https://arxiv.org/abs/2602.18534", "authors": ["Hanliang Zhang", "Arindam Sharma", "Cristina David", "Meng Wang", "Brandon Paulsen", "Daniel Kroening", "Wenjia Ye", "Taro Sekiyama"], "title": "Validated Code Translation for Projects with External Libraries", "comment": null, "summary": "Large Language Models (LLMs) have shown promise for program translation, particularly for migrating systems code to memory-safe languages such as Rust. However, existing approaches struggle when source programs depend on external libraries: LLMs frequently hallucinate non-existent target APIs and fail to generate call-enabling imports; moreover, validating semantic equivalence is challenging when the code manipulates opaque, library-defined types. We present a translation and validation framework for translating Go projects with external dependencies to Rust. Our approach combines (i) a retrieval mechanism that maps Go library APIs to Rust APIs, and (ii) a cross-language validation pipeline that establishes language interoperability in the presence of opaque library types by synthesising adapters exclusively from public library APIs, prior to validating I/O equivalence. We evaluate our system on six real-world Go repositories with non-trivial external dependencies. Our approach significantly increases both the compilation and equivalence success rate (up to 100% in the most dependency-heavy case; approx. 2x on average) by enabling validated translation that manipulate opaque, library-defined types."}
{"id": "2602.19567", "categories": ["cs.NI"], "pdf": "https://arxiv.org/pdf/2602.19567", "abs": "https://arxiv.org/abs/2602.19567", "authors": ["Tommaso Bonato", "Ales Kubicek", "Abdul Kabbani", "Ahmad Ghalayini", "Maciej Besta", "Torsten Hoefler"], "title": "Spritz: Path-Aware Load Balancing in Low-Diameter Networks", "comment": null, "summary": "Low-diameter topologies such as Dragonfly and Slim Fly are increasingly adopted in HPC and datacenter networks, yet existing load balancing techniques either rely on proprietary in-network mechanisms or fail to utilize the full path diversity of these topologies. We introduce Spritz, a flexible sender-based load balancing framework that shifts adaptive topology-aware routing to the endpoints using only standard Ethernet features. We propose two algorithms, Spritz-Scout and Spritz-Spray that, respectively, explore and adaptively cache efficient paths using ECN, packet trimming, and timeout feedback. Through simulation on Dragonfly and Slim Fly topologies with over 1000 endpoints, Spritz outperforms ECMP, UGAL-L, and prior sender-based approaches by up to 1.8x in flow completion time under AI training and datacenter workloads, while offering robust failover with performance improvements of up to 25.4x under link failures, all without additional hardware support. Spritz enables datacenter-scale, commodity Ethernet networks to efficiently leverage low-diameter topologies, offering unified routing and load balancing for the Ultra Ethernet era."}
{"id": "2602.18755", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2602.18755", "abs": "https://arxiv.org/abs/2602.18755", "authors": ["Omar Basit", "Yunzhao Liu", "Z. Jonny Kong", "Y. Charlie Hu"], "title": "BiScale: Energy-Efficient Disaggregated LLM Serving via Phase-Aware Placement and DVFS", "comment": null, "summary": "Prefill/decode disaggregation is increasingly adopted in LLM serving to improve the latency-throughput tradeoff and meet strict TTFT and TPOT SLOs. However, LLM inference remains energy-hungry: autoscaling alone is too coarse-grained to track fast workload fluctuations, and applying fine-grained DVFS under disaggregation is complicated by phase-asymmetric dynamics and coupling between provisioning and frequency control.\n  We present BiScale, a two-tier energy optimization framework for disaggregated LLM serving. BiScale jointly optimizes placement and DVFS across prefill and decode using predictive latency and power models. At coarse timescales, BiScale computes phase-aware placement and baseline frequencies that minimize energy while satisfying SLO constraints. At fine timescales, BiScale dynamically adapts GPU frequency per iteration using stage-specific control: model predictive control (MPC) for prefill to account for queue evolution and future TTFT impact, and lightweight slack-aware adaptation for decode to exploit its smoother, memory-bound dynamics. This hierarchical design enables coordinated control across timescales while preserving strict serving SLOs.\n  Evaluation on a 16x H100 cluster serving Llama 3.3 70B with production-style traces shows that BiScale meets TTFT/TPOT SLOs while reducing energy by up to 39% in prefill and 48% in decode relative to DistServe."}
{"id": "2602.18750", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2602.18750", "abs": "https://arxiv.org/abs/2602.18750", "authors": ["He Sun", "Li Li", "Mingjun Xiao"], "title": "HillInfer: Efficient Long-Context LLM Inference on the Edge with Hierarchical KV Eviction using SmartSSD", "comment": "12 pages, 12 figures, under review", "summary": "Deploying Large Language Models (LLMs) on edge devices such as PCs enables low-latency inference with strong privacy guarantees, but long-context inference is fundamentally constrained by limited memory and compute resources. Beyond model parameters, the KV cache becomes the dominant bottleneck due to its linear growth with context length. Although prior work exploits contextual sparsity to evict unimportant KV data, these approaches are largely designed for memory-rich platforms and incur prohibitive data transfer overhead when applied to resource-constrained edge devices with external storage. In this paper, we propose HillInfer, an importance-aware long-context LLM inference framework on the edge that leverages SmartSSD-assisted hierarchical KV cache management. HillInfer jointly manages KV cache pools across the CPU and SmartSSD, and performs in-storage importance evaluation to reduce unnecessary data movement. Furthermore, we design an adaptive, prefetch-based pipeline that overlaps computation and KV data transfer across GPU, CPU, and SmartSSD, minimizing end-to-end inference latency without sacrificing accuracy. We implement HillInfer on a PC with a commodity GPU, and experiments across multiple models and benchmarks demonstrate up to 8.56 $\\times$ speedup over baselines while preserving model accuracy."}
{"id": "2602.18650", "categories": ["cs.MA", "cs.AI", "cs.IR"], "pdf": "https://arxiv.org/pdf/2602.18650", "abs": "https://arxiv.org/abs/2602.18650", "authors": ["Junwei Wu", "Runze Yan", "Hanqi Luo", "Darren Liu", "Minxiao Wang", "Kimberly L. Townsend", "Lydia S. Hartwig", "Derek Milketinas", "Xiao Hu", "Carl Yang"], "title": "NutriOrion: A Hierarchical Multi-Agent Framework for Personalized Nutrition Intervention Grounded in Clinical Guidelines", "comment": null, "summary": "Personalized nutrition intervention for patients with multimorbidity is critical for improving health outcomes, yet remains challenging because it requires the simultaneous integration of heterogeneous clinical conditions, medications, and dietary guidelines. Single-agent large language models (LLMs) often suffer from context overload and attention dilution when processing such high-dimensional patient profiles. We introduce NutriOrion, a hierarchical multi-agent framework with a parallel-then-sequential reasoning topology. NutriOrion decomposes nutrition planning into specialized domain agents with isolated contexts to mitigate anchoring bias, followed by a conditional refinement stage. The framework includes a multi-objective prioritization algorithm to resolve conflicting dietary requirements and a safety constraint mechanism that injects pharmacological contraindications as hard negative constraints during synthesis, ensuring clinical validity by construction rather than post-hoc filtering. For clinical interoperability, NutriOrion maps synthesized insights into the ADIME standard and FHIR R4 resources. Evaluated on 330 stroke patients with multimorbidity, NutriOrion outperforms multiple baselines, including GPT-4.1 and alternative multi-agent architectures. It achieves a 12.1 percent drug-food interaction violation rate, demonstrates strong personalization with negative correlations (-0.26 to -0.35) between patient biomarkers and recommended risk nutrients, and yields clinically meaningful dietary improvements, including a 167 percent increase in fiber and a 27 percent increase in potassium, alongside reductions in sodium (9 percent) and sugars (12 percent)."}
{"id": "2602.18537", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2602.18537", "abs": "https://arxiv.org/abs/2602.18537", "authors": ["Yiran Wang", "José Antonio Hernández López", "Ulf Nilsson", "Dániel Varró"], "title": "Runtime-Augmented LLMs for Crash Detection and Diagnosis in ML Notebooks", "comment": null, "summary": "Jupyter notebooks are widely used for machine learning (ML) development due to their support for interactive and iterative experimentation. However, ML notebooks are highly prone to bugs, with crashes being among the most disruptive. Despite their practical importance, systematic methods for crash detection and diagnosis in ML notebooks remain largely unexplored. We present CRANE-LLM, a novel approach that augments large language models (LLMs) with structured runtime information extracted from the notebook kernel state to detect and diagnose crashes before executing a target cell. Given previously executed cells and a target cell, CRANE-LLM combines static code context with runtime information, including object types, tensor shapes, and data attributes, to predict whether the target cell will crash (detection) and explain the underlying cause (diagnosis). We evaluate CRANE-LLM on JunoBench, a benchmark of 222 ML notebooks comprising 111 pairs of crashing and corresponding non-crashing notebooks across multiple ML libraries and crash root causes. Across three state-of-the-art LLMs (Gemini, Qwen, and GPT-5), runtime information improves crash detection and diagnosis by 7-10 percentage points in accuracy and 8-11 in F1-score, with larger gains for diagnosis. Improvements vary across ML libraries, crash causes, and LLMs, and depends on the integration of complementary categories of runtime information."}
{"id": "2602.19603", "categories": ["cs.NI"], "pdf": "https://arxiv.org/pdf/2602.19603", "abs": "https://arxiv.org/abs/2602.19603", "authors": ["Kasra Ekrad", "Bjarne Johansson", "Inés Alvarez Vadillo", "Saad Mubeen", "Mohammad Ashjaei"], "title": "Traffic-Aware Configuration of OPC UA PubSub in Industrial Automation Networks", "comment": "8 pages, 2 figures, IEEE International Conference on Industrial Technology (ICIT 2026)", "summary": "Interoperability across industrial automation systems is a cornerstone of Industry 4.0. To address this need, the OPC Unified Architecture (OPC UA) Publish-Subscribe (PubSub) model offers a promising mechanism for enabling efficient communication among heterogeneous devices. PubSub facilitates resource sharing and communication configuration between devices, but it lacks clear guidelines for mapping diverse industrial traffic types to appropriate PubSub configurations. This gap can lead to misconfigurations that degrade network performance and compromise real-time requirements. This paper proposes a set of guidelines for mapping industrial traffic types, based on their timing and quality-of-service specifications, to OPC UA PubSub configurations. The goal is to ensure predictable communication and support real-time performance in industrial networks. The proposed guidelines are evaluated through an industrial use case that demonstrates the impact of incorrect configuration on latency and throughput. The results underline the importance of traffic-aware PubSub configuration for achieving interoperability in Industry 4.0 systems."}
{"id": "2602.18797", "categories": ["cs.DC", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.18797", "abs": "https://arxiv.org/abs/2602.18797", "authors": ["Mubshra Zulfiqar", "Muhammad Ayzed Mirza", "Basit Qureshi"], "title": "Carbon-aware decentralized dynamic task offloading in MIMO-MEC networks via multi-agent reinforcement learning", "comment": null, "summary": "Massive internet of things microservices require integrating renewable energy harvesting into mobile edge computing (MEC) for sustainable eScience infrastructures. Spatiotemporal mismatches between stochastic task arrivals and intermittent green energy along with complex inter-user interference in multi-antenna (MIMO) uplinks complicate real-time resource management. Traditional centralized optimization and off-policy reinforcement learning struggle with scalability and signaling overhead in dense networks. This paper proposes CADDTO-PPO, a carbon-aware decentralized dynamic task offloading framework based on multi-agent proximal policy optimization. The multi-user MIMO-MEC system is modeled as a Decentralized Partially Observable Markov Decision Process (DEC-POMDP) to jointly minimize carbon emissions and buffer latency and energy wastage. A scalable architecture utilizes decentralized execution with parameter sharing (DEPS), which enables autonomous IoT agents to make fine-grained power control and offloading decisions based solely on local observations. Additionally, a carbon-first reward structure adaptively prioritizes green time slots for data transmission to decouple system throughput from grid-dependent carbon footprints. Finally, experimental results demonstrate CADDTO-PPO outperforms deep deterministic policy gradient (DDPG) and lyapunov-based baselines. The framework achieves the lowest carbon intensity and maintains near-zero packet overflow rates under extreme traffic loads. Architectural profiling validates the framework to demonstrate a constant $O(1)$ inference complexity and theoretical lightweight feasibility for future generation sustainable IoT deployments."}
{"id": "2602.19007", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2602.19007", "abs": "https://arxiv.org/abs/2602.19007", "authors": ["Md Rownak Hossain Chowdhury", "Mostafizur Rahman"], "title": "A Logic-Reuse Approach to Nibble-based Multiplier Design for Low Power Vector Computing", "comment": null, "summary": "Vector multiplication is a fundamental operation for AI acceleration, responsible for over 85% of computational load in convolution tasks. While essential, these operations are primary drivers of area, power, and delay in modern datapath designs. Conventional multiplier architectures often force a compromise between latency and complexity: high-speed array multipliers demand significant power, whereas sequential designs offer efficiency at the cost of throughput. This paper presents a precompute-reuse nibble multiplier architecture that bridges this gap by reformulating multiplication as a structured composition of reusable nibble-level precomputed values. The proposed design treats each operand as an independent low-precision element, decomposes it into fixed-width nibbles, and generates scaled multiples of a broadcast operand using compact shift-add logic. By replacing wide lookup tables and multiway multiplexers with logic-based precomputation and regular accumulation, the architecture decouples cycle complexity from gate delay. The design completes each 8-bit multiplication in two deterministic cycles with a short critical path, scales efficiently across vector lanes, and significantly reduces area and energy consumption. RTL implementations synthesized in TSMC 28 nm technology demonstrate up to 1.69x area reduction and 1.63x power improvement over shift-add, and nearly 2.6x area and 2.7x power savings compared to LUT-based array multipliers at 128 bit scale."}
{"id": "2602.18673", "categories": ["cs.MA", "cs.DC"], "pdf": "https://arxiv.org/pdf/2602.18673", "abs": "https://arxiv.org/abs/2602.18673", "authors": ["Harang Ju"], "title": "When Coordination Is Avoidable: A Monotonicity Analysis of Organizational Tasks", "comment": "24 pages, 1 figure, 9 tables", "summary": "Organizations devote substantial resources to coordination, yet which tasks actually require it for correctness remains unclear. The problem is acute in multi-agent AI systems, where coordination overhead is directly measurable and routinely exceeds the cost of the work itself. However, distributed systems theory provides a precise answer: coordination is necessary if and only if a task is non-monotonic, meaning new information can invalidate prior conclusions. Here we show that a classic taxonomy of organizational interdependence maps onto the monotonicity criterion, yielding a decision rule and a measure of avoidable overhead (the Coordination Tax). Multi-agent simulations confirm both predictions. We classify 65 enterprise workflows and find that 48 (74%) are monotonic, then replicate on 13,417 occupational tasks from the O*NET database (42% monotonic). These classification rates imply that 24-57% of coordination spending is unnecessary for correctness."}
{"id": "2602.18541", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2602.18541", "abs": "https://arxiv.org/abs/2602.18541", "authors": ["Daniel Garcia"], "title": "LAPIS: Lightweight API Specification for Intelligent Systems", "comment": null, "summary": "Large Language Models (LLMs) increasingly serve as consumers of API specifications, whether for code generation, autonomous agent interaction, or API-assisted reasoning. The de facto standard for API description, OpenAPI, was designed for documentation tools and code generators, resulting in substantial token overhead when used as LLM context.\n  We present LAPIS (Lightweight API Specification for Intelligent Systems), a domain-specific format optimized for LLM consumption that preserves the semantic information necessary for API reasoning while minimizing token usage. Through empirical evaluation against five real-world production API specifications including GitHub (1,080 endpoints), Twilio (197 endpoints), DigitalOcean (545 endpoints), Petstore, and HTTPBin we demonstrate an average token reduction of 85.5% compared to OpenAPI YAML and 88.6% compared to OpenAPI JSON, measured with the cl100k_base tokenizer. LAPIS introduces domain-specific structural innovations, including centralized error definitions, webhook trigger conditions, structured rate limit descriptions, and operation flow declarations information that OpenAPI either duplicates redundantly or cannot represent at all.\n  The format is fully convertible from OpenAPI 3.x via an automated converter, requires no special parser for LLM consumption, and is released as an open specification under CC BY 4.0."}
{"id": "2602.19758", "categories": ["cs.NI"], "pdf": "https://arxiv.org/pdf/2602.19758", "abs": "https://arxiv.org/abs/2602.19758", "authors": ["Abdul Wadud", "Nima Afraz", "Fatemeh Golpayegani"], "title": "AI-Powered Conflict Management in Open RAN: Detection, Classification, and Mitigation", "comment": null, "summary": "Open Radio Access Network (RAN) was designed with native Artificial Intelligence (AI) as a core pillar, enabling AI- driven xApps and rApps to dynamically optimize network performance. However, the independent ICP adjustments made by these applications can inadvertently create conflicts- direct, indirect, and implicit, which lead to network instability and KPI degradation. Traditional rule-based conflict management becomes increasingly impractical as Open RAN scales in terms of xApps, associated ICPs, and relevant KPIs, struggling to handle the complexity of multi-xApp interactions. This highlights the necessity for AI-driven solutions that can efficiently detect, classify, and mitigate conflicts in real-time. This paper proposes an AI-powered framework for conflict detection, classification, and mitigation in Open RAN. We introduce GenC, a synthetic conflict generation framework for large-scale labeled datasets with controlled parameter sharing and realistic class imbalance, enabling robust training and evaluation of AI models. Our classification pipeline leverages GNNs, Bi-LSTM, and SMOTE-enhanced GNNs, with results demonstrating SMOTE-GNN's superior robustness in handling imbalanced data. Experimental validation using both synthetic datasets (5-50 xApps) and realistic ns3-oran simulations with OpenCellID-derived Dublin topology shows that AI-based methods achieve 3.2x faster classification than rule-based approaches while maintaining near-perfect accuracy. Our framework successfully addresses Energy Saving (ES)/Mobility Robustness Optimization (MRO) conflict scenarios using realistic ns3-oran and scales efficiently to large-scale xApp environments. By embedding this workflow into Open RAN's AI-driven architecture, our solution ensures autonomous and self-optimizing conflict management, paving the way for resilient, ultra-low-latency, and energy-efficient 6G networks."}
{"id": "2602.18931", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2602.18931", "abs": "https://arxiv.org/abs/2602.18931", "authors": ["Noah Martin", "Fahad Dogar"], "title": "WANSpec: Leveraging Global Compute Capacity for LLM Inference", "comment": null, "summary": "Data centers capable of running large language models (LLMs) are spread across the globe. Some have high end GPUs for running the most advanced models (100B+ parameters), and others are only suitable for smaller models (1B parameters). The most capable GPUs are under high demand thanks to the rapidly expanding applications of LLMs. Choosing the right location to run an LLM inference workload can have consequences on the latency of requests due to these high demands. In this work, we explore options to shift some aspects of inference to the under-utilized data centers. We first observe the varying delays affecting inference in AWS services from different regions, demonstrating that load is not spread evenly. We then introduce WANSpec, which offloads part of LLM generation to the under-utilized data centers. In doing so, WANSpec can mitigate capacity issues as well as effectively use on-site compute (ie at universities) to augment cloud providers. This is done with speculative decoding, a widely used technique to speed up auto-regressive decoding, by moving the draft model to the under-utilized compute resources. Our experiments in simulation and cloud deployments show that WANSpec can judiciously employ redundancy to avoid increases in latency while still reducing the forward passes of speculative decoding's draft model in high demand data centers by over 50%."}
{"id": "2602.19242", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2602.19242", "abs": "https://arxiv.org/abs/2602.19242", "authors": ["Zheng Li", "Guangyi Zeng", "Paul Delestrac", "Enyi Yao", "Simei Yang"], "title": "pHNSW: PCA-Based Filtering to Accelerate HNSW Approximate Nearest Neighbor Search", "comment": "6 pages for ASPDAC2026 conference", "summary": "Hierarchical Navigable Small World (HNSW) has demonstrated impressive accuracy and low latency for high-dimensional nearest neighbor searches. However, its high computational demands and irregular, large-volume data access patterns present significant challenges to search efficiency. To address these challenges, we introduce pHNSW, an algorithm-hardware co-optimized solution that accelerates HNSW through Principal Component Analysis (PCA) filtering. On the algorithm side, we apply PCA filtering to reduce the dimensionality of the dataset, thereby lowering the volume of neighbor access and decreasing the computational load for distance calculations. On the hardware side, we design the pHNSW processor with custom instructions to optimize search throughput and energy efficiency. In the experiments, we synthesized the pHNSW processor RTL design with a 65nm technology node and evaluated it using DDR4 and HBM1.0 DRAM standards. The results show that pHNSW boosts Queries per Second (QPS) by 14.47x-21.37x on a CPU and 5.37x-8.46x on a GPU, while reducing energy consumption by up to 57.4% compared to standard HNSW implementation."}
{"id": "2602.18705", "categories": ["cs.MA", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.18705", "abs": "https://arxiv.org/abs/2602.18705", "authors": ["Wenjing Zhai", "Jianbin Zhang", "Tao Liu"], "title": "EDU-MATRIX: A Society-Centric Generative Cognitive Digital Twin Architecture for Secondary Education", "comment": null, "summary": "Existing multi-agent simulations often suffer from the \"Agent-Centric Paradox\": rules are hard-coded into individual agents, making complex social dynamics rigid and difficult to align with educational values. This paper presents EDU-MATRIX, a society-centric generative cognitive digital twin architecture that shifts the paradigm from simulating \"people\" to simulating a \"social space with a gravitational field.\" We introduce three architectural contributions: (1) An Environment Context Injection Engine (ECIE), which acts as a \"social microkernel,\" dynamically injecting institutional rules (Gravity) into agents based on their spatial-temporal coordinates; (2) A Modular Logic Evolution Protocol (MLEP), where knowledge exists as \"fluid\" capsules that agents synthesize to generate new paradigms, ensuring high dialogue consistency (94.1%); and (3) Endogenous Alignment via Role-Topology, where safety constraints emerge from the agent's position in the social graph rather than external filters. Deployed as a digital twin of a secondary school with 2,400 agents, the system demonstrates how \"social gravity\" (rules) and \"cognitive fluids\" (knowledge) interact to produce emergent, value-aligned behaviors (Social Clustering Coefficient: 0.72)."}
{"id": "2602.18545", "categories": ["cs.SE", "cs.PL"], "pdf": "https://arxiv.org/pdf/2602.18545", "abs": "https://arxiv.org/abs/2602.18545", "authors": ["Alperen Keles", "Justine Frank", "Ceren Mert", "Harrison Goldstein", "Leonidas Lampropoulos"], "title": "Programmable Property-Based Testing", "comment": null, "summary": "Property-based testing (PBT) is a popular technique for establishing confidence in software, where users write properties -- i.e., executable specifications -- that can be checked many times in a loop by a testing framework. In modern PBT frameworks, properties are usually written in shallowly embedded domain-specific languages, and their definition is tightly coupled to the way they are tested. Such frameworks often provide convenient configuration options to customize aspects of the testing process, but users are limited to precisely what library authors had the prescience to allow for when developing the framework; if they want more flexibility, they may need to write a new framework from scratch.\n  We propose a new, deeper language for properties based on a mixed embedding that we call deferred binding abstract syntax, which reifies properties as a data structure and decouples them from the property runners that execute them. We implement this language in Rocq and Racket, leveraging the power of dependent and dynamic types, respectively. Finally, we showcase the flexibility of this new approach by rapidly prototyping a variety of property runners, highlighting domain-specific testing improvements that can be unlocked by more programmable testing."}
{"id": "2602.19929", "categories": ["cs.NI"], "pdf": "https://arxiv.org/pdf/2602.19929", "abs": "https://arxiv.org/abs/2602.19929", "authors": ["Chenran Kou", "Changsheng You", "Mingjiang Wu", "Dingzhu Wen", "Zezhong Zhang", "Chengwen Xing"], "title": "BeamVLM for Low-altitude Economy: Generative Beam Prediction via Vision-language Models", "comment": "We propose a novel end-to-end generative framework for beam prediction by using vision-language models", "summary": "For low-altitude economy (LAE), fast and accurate beam prediction between high-mobility unmanned aerial vehicles (UAVs) and ground base stations is of paramount importance, which ensures seamless coverage and reliable communications. However, existing deep learning-based beam prediction methods lack high-level semantic understanding of dynamic environments, resulting in poor generalization. On the other hand, the emerging large language model (LLM) based approaches show promise in enhancing generalization, but they typically lack rich environmental perception, thereby failing to capture fine-grained spatial semantics essential for precise beam alignment. To tackle these limitations, we propose in this correspondence a novel end-to-end generative framework for beam prediction, called BeamVLM, which treats beam prediction as a vision question answering task capitalizing on powerful existing vision-language models (VLMs). By projecting raw visual patches directly into the language domain and judiciously designing an instructional prompt, the proposed BeamVLM enables the VLM to jointly reason over UAV trajectories and environmental context. Last, experimental results on real-world datasets demonstrate that the proposed BeamVLM outperforms state-of-the-art methods in prediction accuracy and also exhibits superior generalization for other scenarios such as vehicle-to-infrastructure (V2I) beam prediction."}
{"id": "2602.19084", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2602.19084", "abs": "https://arxiv.org/abs/2602.19084", "authors": ["Emir Gencer", "Mohammad Kefah Taha Issa", "Ilyas Turimbetov", "James D. Trotter", "Didem Unat"], "title": "ucTrace: A Multi-Layer Profiling Tool for UCX-driven Communication", "comment": "11 pages, 8 figures. To appear in the 40th IEEE International Parallel & Distributed Processing Symposium (IPDPS 2026)", "summary": "UCX is a communication framework that enables low-latency, high-bandwidth communication in HPC systems. With its unified API, UCX facilitates efficient data transfers across multi-node CPU-GPU clusters. UCX is widely used as the transport layer for MPI, particularly in GPU-aware implementations. However, existing profiling tools lack fine-grained communication traces at the UCX level, do not capture transport-layer behavior, or are limited to specific MPI implementations.\n  To address these gaps, we introduce ucTrace, a novel profiler that exposes and visualizes UCX-driven communication in HPC environments. ucTrace provides insights into MPI workflows by profiling message passing at the UCX level, linking operations between hosts and devices (e.g., GPUs and NICs) directly to their originating MPI functions. Through interactive visualizations of process- and device-specific interactions, ucTrace helps system administrators, library and application developers optimize performance and debug communication patterns in large-scale workloads. We demonstrate ucTrace's features through a wide range of experiments including MPI point-to-point behavior under different UCX settings, Allreduce comparisons across MPI libraries, communication analysis of a linear solver, NUMA binding effects, and profiling of GROMACS MD simulations with GPU acceleration at scale. ucTrace is publicly available at https://github.com/ParCoreLab/ucTrace."}
{"id": "2602.19268", "categories": ["cs.AR", "cs.AI", "cs.CV", "cs.NE", "eess.IV"], "pdf": "https://arxiv.org/pdf/2602.19268", "abs": "https://arxiv.org/abs/2602.19268", "authors": ["Sonu Kumar", "Mohd Faisal Khan", "Mukul Lokhande", "Santosh Kumar Vishvakarma"], "title": "CORVET: A CORDIC-Powered, Resource-Frugal Mixed-Precision Vector Processing Engine for High-Throughput AIoT applications", "comment": null, "summary": "This brief presents a runtime-adaptive, performance-enhanced vector engine featuring a low-resource, iterative CORDIC-based MAC unit for edge AI acceleration. The proposed design enables dynamic reconfiguration between approximate and accurate modes, exploiting the latency-accuracy trade-off for a wide range of workloads. Its resource-efficient approach further enables up to 4x throughput improvement within the same hardware resources by leveraging vectorised, time-multiplexed execution and flexible precision scaling. With a time-multiplexed multi-AF block and a lightweight pooling and normalisation unit, the proposed vector engine supports flexible precision (4/8/16-bit) and high MAC density. The ASIC implementation results show that each MAC stage can save up to 33% of time and 21% of power, with a 256-PE configuration that achieves higher compute density (4.83 TOPS/mm2 ) and energy efficiency (11.67 TOPS/W) than previous state-of-the-art work. A detailed hardware-software co-design methodology for object detection and classification tasks on Pynq-Z2 is discussed to assess the proposed architecture, demonstrating a scalable, energy-efficient solution for edge AI applications."}
{"id": "2602.18916", "categories": ["cs.MA", "cs.AI", "cs.SC"], "pdf": "https://arxiv.org/pdf/2602.18916", "abs": "https://arxiv.org/abs/2602.18916", "authors": ["Hoang-Loc Cao", "Phuc Ho", "Truong Thanh Hung Nguyen", "Phuc Truong Loc Nguyen", "Dinh Thien Loc Nguyen", "Hung Cao"], "title": "Adaptive Collaboration of Arena-Based Argumentative LLMs for Explainable and Contestable Legal Reasoning", "comment": null, "summary": "Legal reasoning requires not only high accuracy but also the ability to justify decisions through verifiable and contestable arguments. However, existing Large Language Model (LLM) approaches, such as Chain-of-Thought (CoT) and Retrieval-Augmented Generation (RAG), often produce unstructured explanations that lack a formal mechanism for verification or user intervention. To address this limitation, we propose Adaptive Collaboration of Argumentative LLMs (ACAL), a neuro-symbolic framework that integrates adaptive multi-agent collaboration with an Arena-based Quantitative Bipolar Argumentation Framework (A-QBAF). ACAL dynamically deploys expert agent teams to construct arguments, employs a clash resolution mechanism to adjudicate conflicting claims, and utilizes uncertainty-aware escalation for borderline cases. Crucially, our framework supports a Human-in-the-Loop (HITL) contestability workflow, enabling users to directly audit and modify the underlying reasoning graph to influence the final judgment. Empirical evaluations on the LegalBench benchmark demonstrate that ACAL outperforms strong baselines across Gemini-2.5-Flash-Lite and Gemini-2.5-Flash architectures, effectively balancing efficient predictive performance with structured transparency and contestability. Our implementation is available at: https://github.com/loc110504/ACAL."}
{"id": "2602.18548", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.18548", "abs": "https://arxiv.org/abs/2602.18548", "authors": ["Qiao Xu", "Yipeng Yu", "Chengxiao Feng", "Xu Liu"], "title": "1D-Bench: A Benchmark for Iterative UI Code Generation with Visual Feedback in Real-World", "comment": null, "summary": "Design-to-code translates high-fidelity UI designs into executable front-end implementations, but progress remains hard to compare due to inconsistent datasets, toolchains, and evaluation protocols. We introduce 1D-Bench, a benchmark grounded in real e-commerce workflows, where each instance provides a reference rendering and an exported intermediate representation that may contain extraction errors. 1D is short for one day, representing the efficient completion of design-to-code tasks in less than one day. Models take both as input, using the intermediate representation as structural cues while being evaluated against the reference rendering, which tests robustness to intermediate representation defects rather than literal adherence.\n  1D-Bench requires generating an executable React codebase under a fixed toolchain with an explicit component hierarchy, and defines a multi-round setting in which models iteratively apply component-level edits using execution feedback. Experiments on commercial and open-weight multimodal models show that iterative editing generally improves final performance by increasing rendering success and often improving visual similarity. We further conduct a pilot study on post-training with synthetic repair trajectories and reinforcement learning based editing, and observe limited and unstable gains that may stem from sparse terminal rewards and high-variance file-level updates."}
{"id": "2602.20105", "categories": ["cs.NI", "eess.SP"], "pdf": "https://arxiv.org/pdf/2602.20105", "abs": "https://arxiv.org/abs/2602.20105", "authors": ["Fabio Busacca", "Andrea Panebianco", "Yin Sun"], "title": "Adaptive Underwater Acoustic Communications with Limited Feedback: An AoI-Aware Hierarchical Bandit Approach", "comment": "6 pages, 9 figures, Accepted for IEEE Globecom 2025", "summary": "Underwater Acoustic (UWA) networks are vital for remote sensing and ocean exploration but face inherent challenges such as limited bandwidth, long propagation delays, and highly dynamic channels. These constraints hinder real-time communication and degrade overall system performance. To address these challenges, this paper proposes a bilevel Multi-Armed Bandit (MAB) framework. At the fast inner level, a Contextual Delayed MAB (CD-MAB) jointly optimizes adaptive modulation and transmission power based on both channel state feedback and its Age of Information (AoI), thereby maximizing throughput. At the slower outer level, a Feedback Scheduling MAB dynamically adjusts the channel-state feedback interval according to throughput dynamics: stable throughput allows longer update intervals, while throughput drops trigger more frequent updates. This adaptive mechanism reduces feedback overhead and enhances responsiveness to varying network conditions. The proposed bilevel framework is computationally efficient and well-suited to resource-constrained UWA networks. Simulation results using the DESERT Underwater Network Simulator demonstrate throughput gains of up to 20.61% and energy savings of up to 36.60% compared with Deep Reinforcement Learning (DRL) baselines reported in the existing literature."}
{"id": "2602.19088", "categories": ["cs.DC", "cs.SC"], "pdf": "https://arxiv.org/pdf/2602.19088", "abs": "https://arxiv.org/abs/2602.19088", "authors": ["Ziwei Zhou", "Si Liu", "Zhou Zhou", "Peixin Wang", "MIn Zhang"], "title": "A Formal Framework for Predicting Distributed System Performance under Faults", "comment": "32 pages, 3 figures. Accepted by FM 2026", "summary": "Today's distributed systems operate in complex environments that inevitably involve faults and even adversarial behaviors. Predicting their performance under such environments directly from formal designs remains a longstanding challenge. We present the first formal framework that systematically enables performance prediction of distributed systems across diverse faulty scenarios. Our framework features a fault injector together with a wide range of faults, reusable as a library, and model compositions that integrate the system and the fault injector into a unified model suitable for statistical analysis of performance properties such as throughput and latency. We formalize the framework in Maude and implement it as an automated tool, PERF. Applied to representative distributed systems, PERF accurately predicts system performance under varying fault settings, with estimations from formal designs consistent with evaluations on real deployments."}
{"id": "2602.19305", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2602.19305", "abs": "https://arxiv.org/abs/2602.19305", "authors": ["Irisha M. Goswami", "D. G. Perera"], "title": "Closed-Loop Environmental Control System on Embedded Systems", "comment": "19 pages, 6 figures", "summary": "In this paper, our objective is to design, build, and verify a closed-loop environmental control system tailored for small-scale agriculture applications. This project aims to develop a low-cost, safety-critical embedded solution using the Nuvoton NUC140 microcontroller to automate temperature regulation. The goal was to mitigate crop yield losses caused by environmental fluctuations in a greenhouse. Our final implemented system successfully meets all design specifications, demonstrating robust temperature regulation through a PID control loop and ensuring hardware safety through galvanic isolation"}
{"id": "2602.18925", "categories": ["cs.MA", "cs.GT"], "pdf": "https://arxiv.org/pdf/2602.18925", "abs": "https://arxiv.org/abs/2602.18925", "authors": ["Philipp Lakheshar", "Sharwin Rezagholi"], "title": "A potentialization algorithm for games with applications to multi-agent learning in repeated games", "comment": null, "summary": "We investigate an algorithm that assigns to any game in normal form an approximating game that admits an ordinal potential function. Due to the properties of potential games, the algorithm equips every game with a surrogate reward structure that allows efficient multi-agent learning. Numerical simulations using the replicator dynamics show that 'potentialization' guarantees convergence to stable agent behavior."}
{"id": "2602.18571", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.18571", "abs": "https://arxiv.org/abs/2602.18571", "authors": ["Spandan Garg", "Yufan Huang"], "title": "Debug2Fix: Supercharging Coding Agents with Interactive Debugging Capabilities", "comment": "In Review", "summary": "While significant progress has been made in automating various aspects of software development through coding agents, there is still significant room for improvement in their bug fixing capabilities. Debugging and investigation of runtime behavior remains largely a manual, developer-driven process. Popular coding agents typically rely on either static analysis of the code or iterative test-fix cycles, which is akin to trial and error debugging. We posit that there is a wealth of rich runtime information that developers routinely access while debugging code, which agents are currently deprived of due to design limitations. Despite how prevalent debuggers are in modern IDEs and command-line tools, they have surprisingly not made their way into coding agents. In this work, we introduce Debug2Fix, a novel framework that incorporates interactive debugging as a core component of a software engineering agent via a subagent architecture. We incorporate debuggers for Java and Python into our agent framework and evaluate against GitBug-Java and SWE-Bench-Live and achieve >20% improvement in performance compared to the baseline for certain models. Furthermore, using our framework, we're able to make weaker models like GPT-5 and Claude Haiku 4.5 match or exceed the performances of stronger models like Claude Sonnet 4.5, showing that better tool design is often just as important as switching to a more expensive model. Finally, we conduct systematic ablations demonstrating the importance of both the subagent architecture and debugger integration."}
{"id": "2602.19121", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2602.19121", "abs": "https://arxiv.org/abs/2602.19121", "authors": ["Matthias Függer", "Thomas Nowak"], "title": "Asymptotic Subspace Consensus in Dynamic Networks", "comment": null, "summary": "We introduce the problem of asymptotic subspace consensus, which requires the outputs of processes to converge onto a common subspace while remaining inside the convex hull of initial vectors.This is a relaxation of asymptotic consensus in which outputs have to converge to a single point, i.e., a zero-dimensional affine subspace.\n  We give a complete characterization of the solvability of asymptotic subspace consensus in oblivious message adversaries. In particular, we show that a large class of algorithms used for asymptotic consensus gracefully degrades to asymptotic subspace consensus in distributed systems with weaker assumptions on the communication network. We also present bounds on the rate by which a lower-than-initial dimension is reached."}
{"id": "2602.19720", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2602.19720", "abs": "https://arxiv.org/abs/2602.19720", "authors": ["Xiaoke Wang", "Raveena Raikar", "Markus Rein", "Ruiqi Chen", "Chang Meng", "Dirk Stroobandt"], "title": "Interconnect-Aware Logic Resynthesis for Multi-Die FPGAs", "comment": null, "summary": "Multi-die FPGAs enable device scaling beyond reticle limits but introduce severe interconnect overhead across die boundaries. Inter-die connections, commonly referred to as super-long lines (SLLs), incur high delay and consume scarce interposer interconnect resources, often dominating critical paths and complicating physical design. To address this, this work proposes an interconnect-aware logic resynthesis method that restructures the LUT-level netlist to reduce the number of SLLs. The resynthesis engine uses die partitioning information to apply logic resubstitutions, which simplifies local circuit structures and eliminates SLLs. By reducing the number of SLLs early in the design flow, prior to physical implementation, the proposed method shortens critical paths, alleviates pressure on scarce interposer interconnect resources, and improves overall physical design flexibility. We further build a tool flow for multi-die FPGAs by integrating the proposed resynthesis method with packing and placement. Experimental results on the EPFL benchmarks show that, compared with a state-of-the-art framework, the proposed method reduces the number of SLLs by up to 24.8% for a 2-die FPGA and up to 27.38% for a 3-die FPGA. On MCNC benchmarks, our tool flow achieves an average SLL reduction of 1.65% while preserving placement quality. On Koios benchmarks, where fewer removable SLLs exist, several designs still exhibit considerable inter-die edge reductions. Overall, the results confirm that reducing inter-die connections at the logic level is an effective approach for multi-die FPGAs."}
{"id": "2602.19309", "categories": ["cs.MA"], "pdf": "https://arxiv.org/pdf/2602.19309", "abs": "https://arxiv.org/abs/2602.19309", "authors": ["Xiangyu Liu", "Di Wang", "Zhe Feng", "Aranyak Mehta"], "title": "Scaling Inference-Time Computation via Opponent Simulation: Enabling Online Strategic Adaptation in Repeated Negotiation", "comment": null, "summary": "While large language models (LLMs) have emerged as powerful decision-makers across a wide range of single-agent and stationary environments, fewer efforts have been devoted to settings where LLMs must engage in \\emph{repeated} and \\emph{strategic} interactions with unknown or dynamic opponents. In such settings, recipes built upon \\emph{offline} pre-training or fine-tuning, though robust against worst-case adversaries, do not fully exploit the capability of LLMs to adapt \\emph{online} based on interaction feedback. Instead, we explore the more natural perspective of scaling inference-time computation as a mechanism for adaptation, embedding the principles of a classical game-theoretical learning dynamic, \\emph{smooth Fictitious Play (sFP)}, into LLM inference: (i) for belief formation, we employ an auxiliary opponent model that in-context learns to imitate the time-averaged behavior of the opponent; (ii) for best response, we advance best-of-$N$ (BoN) sampling by simulating against the opponent model. Empirical evaluations on two distinct forms of repeated negotiation games demonstrate that our method enables significant performance improvement over repeated online interaction compared to various baselines, offering a scalable and principled approach to repeated strategic decision-making without any parameter updates."}
{"id": "2602.18579", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2602.18579", "abs": "https://arxiv.org/abs/2602.18579", "authors": ["José Aldo Silva da Costa", "Rohit Gheyi", "José Júnior Silva da Costa", "Márcio Ribeiro", "Rodrigo Bonifácio", "Hyggo Almeida", "Ana Carla Bibiano", "Alessandro Garcia"], "title": "Refactoring for Novices in Java: An Eye Tracking Study on the Extract vs. Inline Methods", "comment": null, "summary": "Developers often extract methods to improve readability, understanding, and reuse, while inlining keeps logic in one block. Prior work based on static metrics has not shown clear differences between these practices, and the human side of comprehension and navigation remains underexplored. We investigate Inline Method vs. Extract Method refactorings using a dynamic approach: eye tracking while participants read and solve tasks. We analyze key code areas and compare visual effort and reading behavior (fixation duration and count, regressions, revisits), alongside time and attempts. We ran a controlled experiment with 32 Java novices, followed by short interviews. Each participant solved eight simple tasks across four programs presented in an inlined version and four in an extracted version. We also surveyed 58 additional novices for complementary quantitative and qualitative data. Results show that effects depend on task difficulty. In two tasks, method extraction improved performance and reduced visual effort, with time decreasing by up to 78.8% and regressions by 84.6%. For simpler tasks (e.g., square area), extraction hurt performance: time increased by up to 166.9% and regressions by 200%. Even with meaningful method names, novices often switched back and forth between call sites and extracted methods, increasing navigation and cognitive load. Preferences frequently favored extraction for readability and reuse, but did not always match measured performance. These findings suggest educators should be cautious about premature modularization for novices and highlight eye tracking as a useful complement to static metrics."}
{"id": "2602.19231", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2602.19231", "abs": "https://arxiv.org/abs/2602.19231", "authors": ["Georgii Semenov", "Vitaly Aksenov"], "title": "Semantic Conflict Model for Collaborative Data Structures", "comment": "6 pages, 7 figures, submitted to PaPoC 2026", "summary": "Digital collaboration systems support asynchronous work over replicated data, where conflicts arise when concurrent operations cannot be unambiguously integrated into a shared history. While Conflict-Free Replicated Data Types (CRDTs) ensure convergence through built-in conflict resolution, this resolution is typically implicit and opaque to users, whereas existing reconciliation techniques often rely on centralized coordination. This paper introduces a conflict model for collaborative data structures that enables explicit, local-first conflict resolution without central coordination. The model identifies conflicts using semantic dependencies between operations and resolves them by rebasing conflicting operations onto a reconciling operation via a three-way merge over a replicated journal. We demonstrate our approach on collaborative registers, including an explicit formulation of the Last-Writer-Wins Register and a multi-register entity supporting semi-automatic reconciliation."}
{"id": "2602.19884", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2602.19884", "abs": "https://arxiv.org/abs/2602.19884", "authors": ["Harry Fitchett", "Jasmine Ritchie", "Charles Fox"], "title": "Extending CPU-less parallel execution of lambda calculus in digital logic with lists and arithmetic", "comment": null, "summary": "Computer architecture is searching for new ways to make use of increasingly available digital logic without the serial bottlenecks of CPU-based design. Recent work has demonstrated a fully CPU-less approach to executing functional programs, by exploiting their inherent parallelisability to compile them directly into parallel digital logic. This work uses lambda-calculus as a hyper simple functional language to prove the concept, but is impractical for real-world programming due to the well-known inefficiencies of pure lambda$-calculus. It is common in language design to extend basic lambda-calculus with additional primitives to short-cut common tasks such as arithmetic and lists. In this work, we build upon our previous research to examine how such extensions may be applied to CPU-less functional execution in digital logic, with the objective of advancing the approach toward practical implementation. We present a set of structures and algorithms for representing new primitives, describe a systematic process for selecting, implementing, and evaluating them, and demonstrate substantial reductions in execution time and node usage. These improvements are implemented in an open-source system, which is shown to correctly evaluate a range of representative lambda expressions."}
{"id": "2602.19326", "categories": ["cs.MA", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.19326", "abs": "https://arxiv.org/abs/2602.19326", "authors": ["Rui Liu", "Steven Jige Quan", "Zhong-Ren Peng", "Zijun Yao", "Han Wang", "Zhengzhang Chen", "Kunpeng Liu", "Yanjie Fu", "Dongjie Wang"], "title": "City Editing: Hierarchical Agentic Execution for Dependency-Aware Urban Geospatial Modification", "comment": null, "summary": "As cities evolve over time, challenges such as traffic congestion and functional imbalance increasingly necessitate urban renewal through efficient modification of existing plans, rather than complete re-planning. In practice, even minor urban changes require substantial manual effort to redraw geospatial layouts, slowing the iterative planning and decision-making procedure. Motivated by recent advances in agentic systems and multimodal reasoning, we formulate urban renewal as a machine-executable task that iteratively modifies existing urban plans represented in structured geospatial formats. More specifically, we represent urban layouts using GeoJSON and decompose natural-language editing instructions into hierarchical geometric intents spanning polygon-, line-, and point-level operations. To coordinate interdependent edits across spatial elements and abstraction levels, we propose a hierarchical agentic framework that jointly performs multi-level planning and execution with explicit propagation of intermediate spatial constraints. We further introduce an iterative execution-validation mechanism that mitigates error accumulation and enforces global spatial consistency during multi-step editing. Extensive experiments across diverse urban editing scenarios demonstrate significant improvements in efficiency, robustness, correctness, and spatial validity over existing baselines."}
{"id": "2602.18644", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2602.18644", "abs": "https://arxiv.org/abs/2602.18644", "authors": ["Mohamed Benchat", "Dominique Briechle", "Raj Chanchad", "Mitbhai Chauhan", "Meet Chavda", "Ruidi He", "Dhruv Jajadiya", "Dhruv Kapadiya", "Nidhiben Kaswala", "Daniel Osterholz", "Andreas Rausch", "Meng Zhang"], "title": "Modeling and Recovering Hierarchical Structural Architectures of ROS 2 Systems from Code and Launch Configurations using LLM-based Agents", "comment": null, "summary": "Model-Driven Engineering (MDE) relies on explicit architecture models to document and evolve systems across abstraction levels. For ROS~2, subsystem structure is often encoded implicitly in distributed configuration artifacts -- most notably launch files -- making hierarchical structural decomposition hard to capture and maintain. Existing ROS~2 modeling approaches cover node-level entities and wiring, but do not make hierarchical structural (de-)composition a first-class architectural view independent of launch artifacts.\n  We contribute (1) a UML-based modeling concept for hierarchical structural architectures of ROS~2 systems and (2) a blueprint-guided automated recovery pipeline that reconstructs such models from code and configuration artifacts by combining deterministic extraction with LLM-based agents. The ROS~2 architectural blueprint (nodes, topics, interfaces, launch-induced wiring) is encoded as structural contracts to constrain synthesis and enable deterministic validation, improving reliability.\n  We evaluate the approach on three ROS~2 repositories, including an industrial-scale code subset. Results show high precision across abstraction levels, while subsystem-level recall drops with repository complexity due to implicit launch semantics, making high-level recovery the remaining challenge."}
{"id": "2602.19338", "categories": ["cs.DC", "cs.SE"], "pdf": "https://arxiv.org/pdf/2602.19338", "abs": "https://arxiv.org/abs/2602.19338", "authors": ["Halit Uyanık", "Tolga Ovatman"], "title": "Complex Event Processing in the Edge: A Combined Optimization Approach for Data and Code Placement", "comment": null, "summary": "The increasing variety of input data and complexity of tasks that are handled by the devices of internet of things (IoT) environments require solutions that consider the limited hardware and computation power of the edge devices. Complex event processing (CEP), can be given as an example, which involves reading and aggregating data from multiple sources to infer triggering of important events. In this study, we balance the execution costs between different paths of the CEP task graph with a constrained programming optimization approach and improve critical path performance. The proposed approach is implemented as a Python library, allowing small-scale IoT devices to adaptively optimize code and I/O assignments and improve overall latency and throughput. The implemented library abstracts away the communication details and allows virtualization of a shared memory between IoT devices. The results show that optimizing critical path performance increases throughput and reduces delay across multiple devices during CEP operations."}
{"id": "2602.19639", "categories": ["cs.MA"], "pdf": "https://arxiv.org/pdf/2602.19639", "abs": "https://arxiv.org/abs/2602.19639", "authors": ["Made Krisnanda", "Raymond Chiong", "Yang Yang", "Kirill Glavatskiy"], "title": "Effects of Property Recovery Incentives and Social Interaction on Self-Evacuation Decisions in Natural Disasters: An Agent-Based Modelling Approach", "comment": "21 pages, 9 figures", "summary": "Understanding evacuation decision-making behaviour is one of the key components for designing disaster mitigation policies. This study investigates how communications between household agents in a community influence self-evacuation decisions. We develop an agent-based model that simulates household agents' decisions to evacuate or stay. These agents interact within the framework of evolutionary game theory, effectively competing for limited shared resources, which include property recovery funds and coordination services. We explore four scenarios that model different prioritisations of access to government-provided incentives. We discover that the impact of the incentive diminishes both with increasing funding value and the household agent prioritisation, indicating that there is an optimal level of government support beyond which further increases become impractical. Furthermore, the overall evacuation rate depends on the structure of the underlying social network, showing discontinuous jumps when the prioritisation moves across the node degree. We identify the so-called \"community influencers\", prioritisation of whom significantly increases the overall evacuation rate. In contrast, prioritising household agents with low connectivity may actually impede collective evacuation. These findings demonstrate the importance of social connectivity between household agents. The results of this study are useful for designing optimal government policies to incentivise and prioritise community evacuation under limited resources."}
{"id": "2602.18689", "categories": ["cs.SE", "cs.CR"], "pdf": "https://arxiv.org/pdf/2602.18689", "abs": "https://arxiv.org/abs/2602.18689", "authors": ["Harrison Green", "Fraser Brown", "Claire Le Goues"], "title": "Automatic, Expressive, and Scalable Fuzzing with Stitching", "comment": null, "summary": "Fuzzing is a powerful technique for finding bugs in software libraries, but scaling it remains difficult. Automated harness generation commits to fixed API sequences at synthesis time, limiting the behaviors each harness can test. Approaches that instead explore new sequences dynamically lack the expressiveness to model real-world usage constraints leading to false positives from straightforward API misuse.\n  We propose stitching, a technique that encodes API usage constraints in pieces that a fuzzer dynamically assembles at runtime. A static type system governs how objects flow between blocks, while a dynamically-checked extrinsic typestate tracks arbitrary metadata across blocks, enabling specifications to express rich semantic constraints such as object state dependencies and cross-function preconditions. This allows a single specification to describe an open-ended space of valid API interactions that the fuzzer explores guided by coverage feedback.\n  We implement stitching in STITCH, using LLMs to automatically configure projects for fuzzing, synthesize a specification, triage crashes, and repair the specification itself. We evaluated STITCH against four state-of-the-art tools on 33 benchmarks, where it achieved the highest code coverage on 21 and found 30 true-positive bugs compared to 10 by all other tools combined, with substantially higher precision (70% vs. 12% for the next-best LLM-based tool). Deployed automatically on 1365 widely used open-source projects, STITCH discovered 131 new bugs across 102 projects, 73 of which have already been patched."}
{"id": "2602.19433", "categories": ["cs.DC", "cs.OS"], "pdf": "https://arxiv.org/pdf/2602.19433", "abs": "https://arxiv.org/abs/2602.19433", "authors": ["Paul Borrill"], "title": "Why iCloud Fails: The Category Mistake of Cloud Synchronization", "comment": "18 pages, 3 figures, 37 references", "summary": "iCloud Drive presents a filesystem interface but implements cloud synchronization semantics that diverge from POSIX in fundamental ways. This divergence is not an implementation bug; it is a Category Mistake -- the same one that pervades distributed computing wherever Forward-In-Time-Only (FITO) assumptions are embedded into protocol design. Parker et al. showed in 1983 that network partitioning destroys mutual consistency; iCloud adds a user interface that conceals this impossibility behind a facade of seamlessness. This document presents a unified analysis of why iCloud fails when composed with Time Machine, git, automated toolchains, and general-purpose developer workflows, supported by direct evidence including documented corruption events and a case study involving 366 GB of divergent state accumulated through normal use. We show that the failures arise from five interlocking incompatibilities rooted in a single structural error: the projection of a distributed causal graph onto a linear temporal chain. We then show how the same Category Mistake, when it occurs in network fabrics as link flapping, destroys topology knowledge through epistemic collapse. Finally, we argue that Open Atomic Ethernet (OAE) transactional semantics -- bilateral, reversible, and conservation-preserving -- provide the structural foundation for resolving these failures, not by defeating physics, but by aligning protocol behavior with physical reality."}
{"id": "2602.20078", "categories": ["cs.MA", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.20078", "abs": "https://arxiv.org/abs/2602.20078", "authors": ["Shan Yang", "Yang Liu"], "title": "Descent-Guided Policy Gradient for Scalable Cooperative Multi-Agent Learning", "comment": "10 pages, 5 figures, 5 tables; plus 16 pages of appendices", "summary": "Scaling cooperative multi-agent reinforcement learning (MARL) is fundamentally limited by cross-agent noise: when agents share a common reward, the actions of all $N$ agents jointly determine each agent's learning signal, so cross-agent noise grows with $N$. In the policy gradient setting, per-agent gradient estimate variance scales as $Θ(N)$, yielding sample complexity $\\mathcal{O}(N/ε)$. We observe that many domains -- cloud computing, transportation, power systems -- have differentiable analytical models that prescribe efficient system states. In this work, we propose Descent-Guided Policy Gradient (DG-PG), a framework that constructs noise-free per-agent guidance gradients from these analytical models, decoupling each agent's gradient from the actions of all others. We prove that DG-PG reduces gradient variance from $Θ(N)$ to $\\mathcal{O}(1)$, preserves the equilibria of the cooperative game, and achieves agent-independent sample complexity $\\mathcal{O}(1/ε)$. On a heterogeneous cloud scheduling task with up to 200 agents, DG-PG converges within 10 episodes at every tested scale -- from $N=5$ to $N=200$ -- directly confirming the predicted scale-invariant complexity, while MAPPO and IPPO fail to converge under identical architectures."}
{"id": "2602.18768", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2602.18768", "abs": "https://arxiv.org/abs/2602.18768", "authors": ["Jakub Zelek", "Jakub Ruszil", "Adam Roman", "Artur Polański"], "title": "Efficient Dynamic Test Case Generation for Path-Based Coverage Criteria", "comment": null, "summary": "We present a novel approach to test-case generation that satisfies four white-box, path-based coverage criteria: Prime Path, Simple Cycle, Simple Path, and Edge-Acyclic Path. Our method builds on a modified version of Johnson algorithm and enables test cases to be generated incrementally and on demand, rather than requiring the entire test suite to be computed upfront. This streaming capability represents a substantial advancement over existing approaches, as it allows testers to begin executing and refining tests immediately, thereby significantly improving the efficiency of test design. Our solution is inherently memory efficient, as it does not store all discovered coverage items; instead, it retains only the minimal set of paths required to generate subsequent coverage items on the fly. As a result, the approach scales to arbitrarily large graphs. In addition, the algorithm gives testers explicit control over the size of the generated test suite by allowing them to restrict the number of cycles permitted in a test path. The approach is grounded in new theoretical insights, most notably a novel characterization of prime paths in terms of the strongly connected components of control-flow graphs. We complement these theoretical contributions with a practical implementation and a comprehensive empirical evaluation. The results demonstrate that our method not only outperforms existing techniques in terms of execution time and memory consumption, but also provides testers with a more flexible and efficient tool for achieving high coverage while substantially reducing test design overhead."}
{"id": "2602.19683", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2602.19683", "abs": "https://arxiv.org/abs/2602.19683", "authors": ["Henrik Möllmann", "Dirk Pflüger", "Alexander Strack"], "title": "GPU-Resident Gaussian Process Regression Leveraging Asynchronous Tasks with HPX", "comment": "13 pages, 7 figures, Workshop on Asynchronous Many-Task Systems and Applications 2026", "summary": "Gaussian processes (GPs) are a widely used regression tool, but the cubic complexity of exact solvers limits their scalability. To address this challenge, we extend the GPRat library by incorporating a fully GPU-resident GP prediction pipeline. GPRat is an HPX-based library that combines task-based parallelism with an intuitive Python API.\n  We implement tiled algorithms for the GP prediction using optimized CUDA libraries, thereby exploiting massive parallelism for linear algebra operations. We evaluate the optimal number of CUDA streams and compare the performance of our GPU implementation to the existing CPU-based implementation. Our results show the GPU implementation provides speedups for datasets larger than 128 training samples. We observe speedups of up to 4.3 for the Cholesky decomposition itself and 4.6 for the GP prediction. Furthermore, combining HPX with multiple CUDA streams allows GPRat to match, and for large datasets, surpass cuSOLVER's performance by up to 11 percent."}
{"id": "2602.19218", "categories": ["cs.SE", "cs.MA"], "pdf": "https://arxiv.org/pdf/2602.19218", "abs": "https://arxiv.org/abs/2602.19218", "authors": ["Zeyu Zhang", "Guohao Li", "Zhenchang Xing", "Alexandros Apostolopoulos", "Yu Lin Lee", "Liang Zheng"], "title": "Gecko: A Simulation Environment with Stateful Feedback for Refining Agent Tool Calls", "comment": null, "summary": "The ability to use tools is fundamental for large language model (LLM) agents. Given a task, existing systems use LLMs to plan and generate tool calls, which are executed by real-world tools to complete the task. However, tool calls are prone to errors because they are derived merely from LLM intrinsic capabilities. What is more, while it is useful to let LLMs iteratively refine the tool-call sequence using execution results from real tools, this process can be expensive and lead to unsafe results. To improve LLM tool calls and address issues caused by using real tools for refinement, we introduce Gecko, a comprehensive environment that simulates tool responses using a combination of rules and LLMs. Specifically, Gecko checks the validity of tool calls including input arguments and tool names, synthesizes reasonable responses that adhere to the output schema, and assesses whether all task objectives have been achieved. These three types of feedback provided by Gecko allow LLMs to refine their tool calls, forming a simple yet effective test-time scaling method named GATS. On BFCLv3 and $τ^2$-bench, GATS consistently improves the tool calling performance of various LLMs including GPT-4o, GPT-5, and Gemini-3.0-pro. We further discuss working mechanisms of our method and share future possibilities."}
{"id": "2602.18800", "categories": ["cs.SE", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.18800", "abs": "https://arxiv.org/abs/2602.18800", "authors": ["Debalina Ghosh Paul", "Hong Zhu", "Ian Bayley"], "title": "Operational Robustness of LLMs on Code Generation", "comment": null, "summary": "It is now common practice in software development for large language models (LLMs) to be used to generate program code. It is desirable to evaluate the robustness of LLMs for this usage. This paper is concerned in particular with how sensitive LLMs are to variations in descriptions of the coding tasks. However, existing techniques for evaluating this robustness are unsuitable for code generation because the input data space of natural language descriptions is discrete. To address this problem, we propose a robustness evaluation method called scenario domain analysis, which aims to find the expected minimal change in the natural language descriptions of coding tasks that would cause the LLMs to produce incorrect outputs. We have formally proved the theoretical properties of the method and also conducted extensive experiments to evaluate the robustness of four state-of-the-art art LLMs: Gemini-pro, Codex, Llamma2 and Falcon 7B, and have found that we are able to rank these with confidence from best to worst. Moreover, we have also studied how robustness varies in different scenarios, including the variations with the topic of the coding task and with the complexity of its sample solution, and found that robustness is lower for more complex tasks and also lower for more advanced topics, such as multi-threading and data structures."}
{"id": "2602.19742", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2602.19742", "abs": "https://arxiv.org/abs/2602.19742", "authors": ["Yulun Huang", "Zhiyu Wang", "Rajkumar Buyya"], "title": "A Risk-Aware UAV-Edge Service Framework for Wildfire Monitoring and Emergency Response", "comment": null, "summary": "Wildfire monitoring demands timely data collection and processing for early detection and rapid response. UAV-assisted edge computing is a promising approach, but jointly minimizing end-to-end service response time while satisfying energy, revisit time, and capacity constraints remains challenging. We propose an integrated framework that co-optimizes UAV route planning, fleet sizing, and edge service provisioning for wildfire monitoring. The framework combines fire-history-weighted clustering to prioritize high-risk areas, Quality of Service (QoS)-aware edge assignment balancing proximity and computational load, 2-opt route optimization with adaptive fleet sizing, and a dynamic emergency rerouting mechanism. The key insight is that these subproblems are interdependent: clustering decisions simultaneously shape patrol efficiency and edge workloads, while capacity constraints feed back into feasible configurations. Experiments show that the proposed framework reduces average response time by 70.6--84.2%, energy consumption by 73.8--88.4%, and fleet size by 26.7--42.1% compared to GA, PSO, and greedy baselines. The emergency mechanism responds within 233 seconds, well under the 300-second deadline, with negligible impact on normal operations."}
{"id": "2602.18914", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2602.18914", "abs": "https://arxiv.org/abs/2602.18914", "authors": ["Peiran Wang", "Ying Li", "Yuqiang Sun", "Chengwei Liu", "Yang Liu", "Yuan Tian"], "title": "From Docs to Descriptions: Smell-Aware Evaluation of MCP Server Descriptions", "comment": null, "summary": "The Model Context Protocol (MCP) has rapidly become a de facto standard for connecting LLM-based agents with external tools via reusable MCP servers. In practice, however, server selection and onboarding rely heavily on free-text tool descriptions that are intentionally loosely constrained. Although this flexibility largely ensures the scalability of MCP servers, it also creates a reliability gap that descriptions often misrepresent or omit key semantics, increasing trial-and-error integration, degrading agent behavior, and potentially introducing security risks. To this end, we present the first systematic study of description smells in MCP tool descriptions and their impact on usability. Specifically, we synthesize software/API documentation practices and agentic tool-use requirements into a four-dimensional quality standard: accuracy, functionality, information completeness, and conciseness, covering 18 specific smell categories. Using this standard, we conducted a large-scale empirical study on a well-constructed dataset of 10,831 MCP servers. We find that description smells are pervasive (e.g., 73% repeated tool names, thousands with incorrect parameter semantics or missing return descriptions), reflecting a \"code-first, description-last\" pattern. Through a controlled mutation-based study, we show these smells significantly affect LLM tool selection, with functionality and accuracy having the largest effects (+11.6% and +8.8%, p < 0.001). In competitive settings with functionally equivalent servers, standard-compliant descriptions reach 72% selection probability (260% over a 20% baseline), demonstrating that smell-guided remediation yields substantial practical benefits. We release our labeled dataset and standards to support future work on reliable and secure MCP ecosystems."}
{"id": "2602.19802", "categories": ["cs.DC", "cs.NE", "math.CV", "math.DS"], "pdf": "https://arxiv.org/pdf/2602.19802", "abs": "https://arxiv.org/abs/2602.19802", "authors": ["Romain de Coudenhove", "Yannis Bendi-Ouis", "Anthony Strock", "Xavier Hinaut"], "title": "Linear Reservoir: A Diagonalization-Based Optimization", "comment": null, "summary": "We introduce a diagonalization-based optimization for Linear Echo State Networks (ESNs) that reduces the per-step computational complexity of reservoir state updates from O(N^2) to O(N). By reformulating reservoir dynamics in the eigenbasis of the recurrent matrix, the recurrent update becomes a set of independent element-wise operations, eliminating the matrix multiplication. We further propose three methods to use our optimization depending on the situation: (i) Eigenbasis Weight Transformation (EWT), which preserves the dynamics of standard and trained Linear ESNs, (ii) End-to-End Eigenbasis Training (EET), which directly optimizes readout weights in the transformed space and (iii) Direct Parameter Generation (DPG), that bypasses matrix diagonalization by directly sampling eigenvalues and eigenvectors, achieving comparable performance than standard Linear ESNs. Across all experiments, both our methods preserve predictive accuracy while offering significant computational speedups, making them a replacement of standard Linear ESNs computations and training, and suggesting a shift of paradigm in linear ESN towards the direct selection of eigenvalues."}
{"id": "2602.18928", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2602.18928", "abs": "https://arxiv.org/abs/2602.18928", "authors": ["Yang Chen", "Shuyang Liu", "Reyhaneh Jabbarvand"], "title": "Narrowing the Complexity Gap in the Evaluation of Large Language Models", "comment": null, "summary": "Evaluating Large Language Models (LLMs) with respect to real-world code complexity is essential. Otherwise, there is a risk of overestimating LLMs' programming abilities based on simplistic benchmarks, only to be disappointed when using them in real-world settings. Recently, researchers explored the construction of more realistic benchmarks by mining or augmenting open-source repositories. Such solutions are usually task-specific. Data quality control from real-world projects can also be time-consuming and error-prone. More importantly, evaluating LLMs on fixed benchmark problems is subject to data contamination and overfitting. We propose GeneBench, an automated technique to add real-world complexities to any programming benchmark. GeneBench leverages a multi-objective optimization to increase the complexity of programming problems while maintaining the readability of code similar to real-world programs. Transforming four widely-used programming benchmarks using GeneBench and evaluating 13 LLMs (including two reasoning LLMs) on them shows a notable performance drop across all programming tasks (14.9%-60.5%, avg=35.2%), demonstrating LLMs' struggle under real-world complexities. The struggle persists even when LLMs are few-shot prompted or fine-tuned with examples from different versions of GeneBench, demonstrating the challenging nature of the problems. Finally, we show that the performance of the studied LLMs in bug repair is similar under GeneBench and SWE-Bench. This, along with the consistent reproduction of performance drop of all studied LLMs across four tasks under different versions of GeneBench, makes the technique suitable to evaluate LLMs without costly construction of real-world benchmarks."}
{"id": "2602.20097", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2602.20097", "abs": "https://arxiv.org/abs/2602.20097", "authors": ["Pu Jiao", "Sheng Di", "Jiannan Tian", "Mingze Xia", "Xuan Wu", "Yang Zhang", "Xin Liang", "Franck Cappello"], "title": "Mitigating Artifacts in Pre-quantization Based Scientific Data Compressors with Quantization-aware Interpolation", "comment": null, "summary": "Error-bounded lossy compression has been regarded as a promising way to address the ever-increasing amount of scientific data in today's high-performance computing systems. Pre-quantization, a critical technique to remove sequential dependency and enable high parallelism, is widely used to design and develop high-throughput error-controlled data compressors. Despite the extremely high throughput of pre-quantization based compressors, they generally suffer from low data quality with medium or large user-specified error bounds. In this paper, we investigate the artifacts generated by pre-quantization based compressors and propose a novel algorithm to mitigate them. Our contributions are fourfold: (1) We carefully characterize the artifacts in pre-quantization based compressors to understand the correlation between the quantization index and compression error; (2) We propose a novel quantization-aware interpolation algorithm to improve the decompressed data; (3) We parallelize our algorithm in both shared-memory and distributed-memory environments to obtain high performance; (4) We evaluate our algorithm and validate it with two leading pre-quantization based compressors using five real-world datasets. Experiments demonstrate that our artifact mitigation algorithm can effectively improve the quality of decompressed data produced by pre-quantization based compressors while maintaining their high compression throughput."}
{"id": "2602.19098", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2602.19098", "abs": "https://arxiv.org/abs/2602.19098", "authors": ["Negar Hashemi", "Amjed Tahir", "August Shi", "Shawn Rasheed", "Rachel Blagojevic"], "title": "A Systematic Evaluation of Environmental Flakiness in JavaScript Tests", "comment": "Accepted at ICST 2026", "summary": "Test flakiness is a significant issue in industry, affecting test efficiency and product quality. While extensive research has examined the impact of flaky tests, many root causes remain unexplored, particularly in the context of dynamic languages such as JavaScript. In this paper, we conduct a systematic evaluation of the impact of environmental factors on test flakiness in JavaScript. We first executed test suites across multiple environmental configurations to determine whether changes in the environment could lead to flaky behavior. We selected three environmental factors to manipulate: the operating system, the Node.js version, and the browser. We identified a total of 65 environmental flaky projects, with 28 related to operating system issues, five to Node.js version compatibility, 16 to a combination of operating system and Node.js issues, and 17 related to browser compatibility. To address environmental flakiness, we developed a lightweight mitigation approach, js-env-sanitizer, that can sanitize environmental-related flaky tests by skipping and reporting them (rather than failing), allowing CI builds to continue/succeed without rerunning entire test suites. The tool achieves high accuracy with minimal performance or configuration overhead, and currently supports three popular JavaScript testing frameworks (Jest, Mocha, and Vitest)"}
{"id": "2602.18673", "categories": ["cs.MA", "cs.DC"], "pdf": "https://arxiv.org/pdf/2602.18673", "abs": "https://arxiv.org/abs/2602.18673", "authors": ["Harang Ju"], "title": "When Coordination Is Avoidable: A Monotonicity Analysis of Organizational Tasks", "comment": "24 pages, 1 figure, 9 tables", "summary": "Organizations devote substantial resources to coordination, yet which tasks actually require it for correctness remains unclear. The problem is acute in multi-agent AI systems, where coordination overhead is directly measurable and routinely exceeds the cost of the work itself. However, distributed systems theory provides a precise answer: coordination is necessary if and only if a task is non-monotonic, meaning new information can invalidate prior conclusions. Here we show that a classic taxonomy of organizational interdependence maps onto the monotonicity criterion, yielding a decision rule and a measure of avoidable overhead (the Coordination Tax). Multi-agent simulations confirm both predictions. We classify 65 enterprise workflows and find that 48 (74%) are monotonic, then replicate on 13,417 occupational tasks from the O*NET database (42% monotonic). These classification rates imply that 24-57% of coordination spending is unnecessary for correctness."}
{"id": "2602.19218", "categories": ["cs.SE", "cs.MA"], "pdf": "https://arxiv.org/pdf/2602.19218", "abs": "https://arxiv.org/abs/2602.19218", "authors": ["Zeyu Zhang", "Guohao Li", "Zhenchang Xing", "Alexandros Apostolopoulos", "Yu Lin Lee", "Liang Zheng"], "title": "Gecko: A Simulation Environment with Stateful Feedback for Refining Agent Tool Calls", "comment": null, "summary": "The ability to use tools is fundamental for large language model (LLM) agents. Given a task, existing systems use LLMs to plan and generate tool calls, which are executed by real-world tools to complete the task. However, tool calls are prone to errors because they are derived merely from LLM intrinsic capabilities. What is more, while it is useful to let LLMs iteratively refine the tool-call sequence using execution results from real tools, this process can be expensive and lead to unsafe results. To improve LLM tool calls and address issues caused by using real tools for refinement, we introduce Gecko, a comprehensive environment that simulates tool responses using a combination of rules and LLMs. Specifically, Gecko checks the validity of tool calls including input arguments and tool names, synthesizes reasonable responses that adhere to the output schema, and assesses whether all task objectives have been achieved. These three types of feedback provided by Gecko allow LLMs to refine their tool calls, forming a simple yet effective test-time scaling method named GATS. On BFCLv3 and $τ^2$-bench, GATS consistently improves the tool calling performance of various LLMs including GPT-4o, GPT-5, and Gemini-3.0-pro. We further discuss working mechanisms of our method and share future possibilities."}
{"id": "2602.19276", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2602.19276", "abs": "https://arxiv.org/abs/2602.19276", "authors": ["Jingyu Xiao", "Jiantong Qin", "Shuoqi Li", "Man Ho Lam", "Yuxuan Wan", "Jen-tse Huang", "Yintong Huo", "Michael R. Lyu"], "title": "ComUICoder: Component-based Reusable UI Code Generation for Complex Websites via Semantic Segmentation and Element-wise Feedback", "comment": null, "summary": "Multimodal Large Language Models (MLLMs) have demonstrated strong performance on the UI-to-code task, which aims to generate UI code from design mock-ups. However, when applied to long and complex websites, they often struggle with fragmented segmentation, redundant code generation for repetitive components, and frequent UI inconsistencies. To systematically investigate and address these challenges, we introduce ComUIBench, a new multi-page complex webpage benchmark with component annotations, designed to evaluate MLLMs' ability to generate reusable UI code in realistic website scenarios. Building upon this benchmark, we propose ComUICoder, a component-based UI code generation framework that emphasizes semantic-aware segmentation, code reuse, and fine-grained refinement. Specifically, ComUICoder incorporates (1) Hybrid Semantic-aware Block Segmentation for accurate UI semantic coherent block detection, (2) Visual-aware Graph-based Block Merge to consolidate structurally similar components within and across webpages for reusable implementation, and (3) Priority-based Element-wise Feedback to refine generated code and reduce element-level inconsistencies. Extensive experiments demonstrate that ComUICoder significantly improves overall generation quality and code reusability on complex multipage websites. Our datasets and code are publicly available at https://github.com/WebPAI/ComUICoder."}
{"id": "2602.19294", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2602.19294", "abs": "https://arxiv.org/abs/2602.19294", "authors": ["Betül Karagöz", "Filippo Ricca", "Matteo Biagiola", "Andrea Stocco"], "title": "Towards Automated Page Object Generation for Web Testing using Large Language Models", "comment": "In proceedings of the 19th IEEE International Conference on Software Testing, Verification and Validation 2026 (ICST '26)", "summary": "Page Objects (POs) are a widely adopted design pattern for improving the maintainability and scalability of automated end-to-end web tests. However, creating and maintaining POs is still largely a manual, labor-intensive activity, while automated solutions have seen limited practical adoption. In this context, the potential of Large Language Models (LLMs) for these tasks has remained largely unexplored. This paper presents an empirical study on the feasibility of using LLMs, specifically GPT-4o and DeepSeek Coder, to automatically generate POs for web testing. We evaluate the generated artifacts on an existing benchmark of five web applications for which manually written POs are available (the ground truth), focusing on accuracy (i.e., the proportion of ground truth elements correctly identified) and element recognition rate (i.e., the proportion of ground truth elements correctly identified or marked for modification). Our results show that LLMs can generate syntactically correct and functionally useful POs with accuracy values ranging from 32.6% to 54.0% and element recognition rate exceeding 70% in most cases. Our study contributes the first systematic evaluation of LLMs strengths and open challenges for automated PO generation, and provides directions for further research on integrating LLMs into practical testing workflows."}
{"id": "2602.19353", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2602.19353", "abs": "https://arxiv.org/abs/2602.19353", "authors": ["Ian A. Cosden", "Elizabeth Holtz", "Joel U. Bretheim"], "title": "Designing and Implementing a Comprehensive Research Software Engineer Career Ladder: A Case Study from Princeton University", "comment": "Submitted to Future Generation Computer Systems: Special Issue on Research Software Engineering - Software-Enabled Discovery and Beyond", "summary": "Research Software Engineers (RSEs) have become indispensable to computational research and scholarship. The fast rise of RSEs in higher education and the trend of universities to be slow creating or adopting models for new technology roles means a lack of structured career pathways that recognize technical mastery, scholarly impact, and leadership growth. In response to an immense demand for RSEs at Princeton University, and dedicated funding to grow the RSE group at least two-fold, Princeton was forced to strategize how to cohesively define job descriptions to match the rapid hiring of RSE positions but with enough flexibility to recognize the unique nature of each individual position. This case study describes our design and implementation of a comprehensive RSE career ladder spanning Associate through Principal levels, with parallel team-lead and managerial tracks. We outline the guiding principles, competency framework, Human Resources (HR) alignment, and implementation process, including engagement with external consultants and mapping to a standard job leveling framework utilizing market benchmarks. We share early lessons learned and outcomes including improved hiring efficiency, clearer promotion pathways, and positive reception among staff."}
{"id": "2602.19360", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2602.19360", "abs": "https://arxiv.org/abs/2602.19360", "authors": ["Natallia Kokash", "Adam Belloum", "Paola Grosso"], "title": "Compliance Management for Federated Data Processing", "comment": null, "summary": "Federated data processing (FDP) offers a promising approach for enabling collaborative analysis of sensitive data without centralizing raw datasets. However, real-world adoption remains limited due to the complexity of managing heterogeneous access policies, regulatory requirements, and long-running workflows across organizational boundaries. In this paper, we present a framework for compliance-aware FDP that integrates policy-as-code, workflow orchestration, and large language model (LLM)-assisted compliance management. Through the implemented prototype, we show how legal and organizational requirements can be collected and translated into machine-actionable policies in FDP networks."}
{"id": "2602.19383", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2602.19383", "abs": "https://arxiv.org/abs/2602.19383", "authors": ["Jens Dietrich", "Behnaz Hassanshahi"], "title": "On the Variability of Source Code in Maven Package Rebuilds", "comment": null, "summary": "Rebuilding packages from open source is a common practice to improve the security of software supply chains, and is now done at an industrial scale. The basic principle is to acquire the source code used to build a package published in a repository such as Maven Central (for Java), rebuild the package independently with hardened security, and publish it in some alternative repository. In this paper we test the assumption that the same source code is being used by those alternative builds. To study this, we compare the sources released with packages on Maven Central, with the sources associated with independently built packages from Google's Assured Open Source and Oracle's Build-from-Source projects. We study non-equivalent sources for alternative builds of 28 popular packages with 85 releases. We investigate the causes of non-equivalence, and find that the main cause is build extensions that generate code at build time, which are difficult to reproduce. We suggest strategies to address this issue."}
{"id": "2602.19407", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2602.19407", "abs": "https://arxiv.org/abs/2602.19407", "authors": ["Indira Vats", "Sanjukta De", "Subhayan Roy", "Saurabh Bodhe", "Lejin Varghese", "Max Kiehn", "Yonas Bedasso", "Marsha Chechik"], "title": "Multi-CoLoR: Context-Aware Localization and Reasoning across Multi-Language Codebases", "comment": "This paper has been accepted for publication at the 33rd IEEE International Conference on Software Analysis, Evolution and Reengineering (SANER 2026)", "summary": "Large language models demonstrate strong capabilities in code generation but struggle to navigate complex, multi-language repositories to locate relevant code. Effective code localization requires understanding both organizational context (e.g., historical issue-fix patterns) and structural relationships within heterogeneous codebases. Existing methods either (i) focus narrowly on single-language benchmarks, (ii) retrieve code across languages via shallow textual similarity, or (iii) assume no prior context. We present Multi-CoLoR, a framework for Context-aware Localization and Reasoning across Multi-Language codebases, which integrates organizational knowledge retrieval with graph-based reasoning to traverse complex software ecosystems. Multi-CoLoR operates in two stages: (i) a similar issue context (SIC) module retrieves semantically and organizationally related historical issues to prune the search space, and (ii) a code graph traversal agent (an extended version of LocAgent, a state-of-the-art localization framework) performs structural reasoning within C++ and QML codebases. Evaluations on a real-world enterprise dataset show that incorporating SIC reduces the search space and improves localization accuracy, and graph-based reasoning generalizes effectively beyond Python-only repositories. Combined, Multi-CoLoR improves Acc@5 over both lexical and graph-based baselines while reducing tool calls on an AMD codebase."}
{"id": "2602.19441", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.19441", "abs": "https://arxiv.org/abs/2602.19441", "authors": ["Costain Nachuma", "Minhaz Zibran"], "title": "When AI Teammates Meet Code Review: Collaboration Signals Shaping the Integration of Agent-Authored Pull Requests", "comment": "5 pages, 2 figures, 1 table. Accepted at the 23rd International Conference on Mining Software Repositories (MSR 2026), Rio de Janeiro, Brazil", "summary": "Autonomous coding agents increasingly contribute to software development by submitting pull requests on GitHub; yet, little is known about how these contributions integrate into human-driven review workflows. We present a large empirical study of agent-authored pull requests using the public AIDev dataset, examining integration outcomes, resolution speed, and review-time collaboration signals. Using logistic regression with repository-clustered standard errors, we find that reviewer engagement has the strongest correlation with successful integration, whereas larger change sizes and coordination-disrupting actions, such as force pushes, are associated with a lower likelihood of merging. In contrast, iteration intensity alone provides limited explanatory power once collaboration signals are considered. A qualitative analysis further shows that successful integration occurs when agents engage in actionable review loops that converge toward reviewer expectations. Overall, our results highlight that the effective integration of agent-authored pull requests depends not only on code quality but also on alignment with established review and coordination practices."}
{"id": "2602.19446", "categories": ["cs.SE", "cs.CY"], "pdf": "https://arxiv.org/pdf/2602.19446", "abs": "https://arxiv.org/abs/2602.19446", "authors": ["Masudul Hasan Masud Bhuiyan", "Manish Kumar Bala Kumar", "Cristian-Alexandru Staicu"], "title": "\"Write in English, Nobody Understands Your Language Here\": A Study of Non-English Trends in Open-Source Repositories", "comment": null, "summary": "The open-source software (OSS) community has historically been dominated by English as the primary language for code, documentation, and developer interactions. However, with growing global participation and better support for non-Latin scripts through standards like Unicode, OSS is gradually becoming more multilingual. This study investigates the extent to which OSS is becoming more multilingual, analyzing 9.14 billion GitHub issues, pull requests, and discussions, and 62,500 repositories across five programming languages and 30 natural languages, covering the period from 2015 to 2025. We examine six research questions to track changes in language use across communication, code, and documentation. We find that multilingual participation has steadily increased, especially in Korean, Chinese, and Russian. This growth appears not only in issues and discussions but also in code comments, string literals, and documentation files. While this shift reflects greater inclusivity and language diversity in OSS, it also creates language tension. The ability to express oneself in a native language can clash with shared norms around English use, especially in collaborative settings. Non-English or multilingual projects tend to receive less visibility and participation, suggesting that language remains both a resource and a barrier, shaping who gets heard, who contributes, and how open collaboration unfolds."}
{"id": "2602.19614", "categories": ["cs.SE", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.19614", "abs": "https://arxiv.org/abs/2602.19614", "authors": ["Chih-Hong Cheng", "Brian Hsuan-Cheng Liao", "Adam Molin", "Hasan Esen"], "title": "Workflow-Level Design Principles for Trustworthy GenAI in Automotive System Engineering", "comment": null, "summary": "The adoption of large language models in safety-critical system engineering is constrained by trustworthiness, traceability, and alignment with established verification practices. We propose workflow-level design principles for trustworthy GenAI integration and demonstrate them in an end-to-end automotive pipeline, from requirement delta identification to SysML v2 architecture update and re-testing. First, we show that monolithic (\"big-bang\") prompting misses critical changes in large specifications, while section-wise decomposition with diversity sampling and lightweight NLP sanity checks improves completeness and correctness. Then, we propagate requirement deltas into SysML v2 models and validate updates via compilation and static analysis. Additionally, we ensure traceable regression testing by generating test cases through explicit mappings from specification variables to architectural ports and states, providing practical safeguards for GenAI used in safety-critical automotive engineering."}
{"id": "2602.19628", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2602.19628", "abs": "https://arxiv.org/abs/2602.19628", "authors": ["Pasan Peiris", "Matthias Galster", "Antonija Mitrovic", "Sanna Malinen", "Raul Vincent Lumapas", "Jay Holland"], "title": "Towards Understanding Views on Combining Videos and Gamification in Software Engineering Training", "comment": "2 pages, ICSE-Companion '26", "summary": "Watching training videos passively leads to superficial learning. Adding gamification can increase engagement. We study how software engineering students and industry practitioners view gamifying video-based training. We conducted a survey with students and professionals. Students and professionals share similar perceptions toward video-based training in general and support combining gamification and video-based training. Our findings can inform the design of gamified training solutions for software engineers."}
{"id": "2602.19718", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.19718", "abs": "https://arxiv.org/abs/2602.19718", "authors": ["Mateen A. Abbasi", "Tommi J. Mikkonen", "Petri J. Ihantola", "Muhammad Waseem", "Pekka Abrahamsson", "Niko K. Mäkitalo"], "title": "Carbon-Aware Governance Gates: An Architecture for Sustainable GenAI Development", "comment": "5 pages, 1 figure. Preprint version under review", "summary": "The rapid adoption of Generative AI (GenAI) in the software development life cycle (SDLC) increases computational demand, which can raise the carbon footprint of development activities. At the same time, organizations are increasingly embedding governance mechanisms into GenAI-assisted development to support trust, transparency, and accountability. However, these governance mechanisms introduce additional computational workloads, including repeated inference, regeneration cycles, and expanded validation pipelines, increasing energy use and the carbon footprint of GenAI-assisted development. This paper proposes Carbon-Aware Governance Gates (CAGG), an architectural extension that embeds carbon budgets, energy provenance, and sustainability-aware validation orchestration into human-AI governance layers. CAGG comprises three components: (i) an Energy and Carbon Provenance Ledger, (ii) a Carbon Budget Manager, and (iii) a Green Validation Orchestrator, operationalized through governance policies and reusable design patterns."}
{"id": "2602.19843", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.19843", "abs": "https://arxiv.org/abs/2602.19843", "authors": ["Jin Jia", "Zhiling Deng", "Zhuangbin Chen", "Yingqi Wang", "Zibin Zheng"], "title": "MAS-FIRE: Fault Injection and Reliability Evaluation for LLM-Based Multi-Agent Systems", "comment": null, "summary": "As LLM-based Multi-Agent Systems (MAS) are increasingly deployed for complex tasks, ensuring their reliability has become a pressing challenge. Since MAS coordinate through unstructured natural language rather than rigid protocols, they are prone to semantic failures (e.g., hallucinations, misinterpreted instructions, and reasoning drift) that propagate silently without raising runtime exceptions. Prevailing evaluation approaches, which measure only end-to-end task success, offer limited insight into how these failures arise or how effectively agents recover from them. To bridge this gap, we propose MAS-FIRE, a systematic framework for fault injection and reliability evaluation of MAS. We define a taxonomy of 15 fault types covering intra-agent cognitive errors and inter-agent coordination failures, and inject them via three non-invasive mechanisms: prompt modification, response rewriting, and message routing manipulation. Applying MAS-FIRE to three representative MAS architectures, we uncover a rich set of fault-tolerant behaviors that we organize into four tiers: mechanism, rule, prompt, and reasoning. This tiered view enables fine-grained diagnosis of where and why systems succeed or fail. Our findings reveal that stronger foundation models do not uniformly improve robustness. We further show that architectural topology plays an equally decisive role, with iterative, closed-loop designs neutralizing over 40% of faults that cause catastrophic collapse in linear workflows. MAS-FIRE provides the process-level observability and actionable guidance needed to systematically improve multi-agent systems."}
{"id": "2602.19338", "categories": ["cs.DC", "cs.SE"], "pdf": "https://arxiv.org/pdf/2602.19338", "abs": "https://arxiv.org/abs/2602.19338", "authors": ["Halit Uyanık", "Tolga Ovatman"], "title": "Complex Event Processing in the Edge: A Combined Optimization Approach for Data and Code Placement", "comment": null, "summary": "The increasing variety of input data and complexity of tasks that are handled by the devices of internet of things (IoT) environments require solutions that consider the limited hardware and computation power of the edge devices. Complex event processing (CEP), can be given as an example, which involves reading and aggregating data from multiple sources to infer triggering of important events. In this study, we balance the execution costs between different paths of the CEP task graph with a constrained programming optimization approach and improve critical path performance. The proposed approach is implemented as a Python library, allowing small-scale IoT devices to adaptively optimize code and I/O assignments and improve overall latency and throughput. The implemented library abstracts away the communication details and allows virtualization of a shared memory between IoT devices. The results show that optimizing critical path performance increases throughput and reduces delay across multiple devices during CEP operations."}
