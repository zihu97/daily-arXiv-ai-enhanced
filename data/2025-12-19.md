<div id=toc></div>

# Table of Contents

- [cs.PF](#cs.PF) [Total: 2]
- [cs.SE](#cs.SE) [Total: 10]
- [cs.NI](#cs.NI) [Total: 1]
- [cs.AR](#cs.AR) [Total: 2]
- [cs.OS](#cs.OS) [Total: 1]
- [cs.DC](#cs.DC) [Total: 8]


<div id='cs.PF'></div>

# cs.PF [[Back]](#toc)

### [1] [XTC, A Research Platform for Optimizing AI Workload Operators](https://arxiv.org/abs/2512.16512)
*Pompougnac Hugo,Guillon Christophe,Noiry Sylvain,Dutilleul Alban,Iooss Guillaume,Rastello Fabrice*

Main category: cs.PF

TL;DR: XTC提供统一接口，实现跨编译器的调度与性能评估，促进优化策略研究。


<details>
  <summary>Details</summary>
Motivation: 现有调度语言受限于特定编译器生态，缺乏统一接口进行公平比较与复用。

Method: 提出XTC平台，通过通用API和可复现测量框架解耦调度规范与代码生成。

Result: 支持跨编译器的可移植实验，加速优化策略研究。

Conclusion: XTC有助于推动AI算子高效调度与性能评估的标准化和协作创新。

Abstract: Achieving high efficiency on AI operators demands precise control over computation and data movement. However, existing scheduling languages are locked into specific compiler ecosystems, preventing fair comparison, reuse, and evaluation across frameworks. No unified interface currently decouples scheduling specification from code generation and measurement. We introduce XTC, a platform that unifies scheduling and performance evaluation across compilers. With its common API and reproducible measurement framework, XTC enables portable experimentation and accelerates research on optimization strategies.

</details>


### [2] [An Upper Bound on the M/M/k Queue With Deterministic Setup Times](https://arxiv.org/abs/2512.16854)
*Jalani Williams,Weina Wang,Mor Harchol-Balter*

Main category: cs.PF

TL;DR: 本文首次对具有确定性启动时间的多服务器系统中的平均等待时间进行了全面刻画，并提出了新的分析方法MIST。


<details>
  <summary>Details</summary>
Motivation: 现代系统中服务器频繁启停以降低成本，但启动时间对排队性能影响巨大，尤其在多服务器系统中缺乏有效分析方法。

Method: 提出‘介入停止时间法’（MIST），推导出平均等待时间的上下界并构造近似公式。

Result: 上下界在常数倍内紧致，且可组合成高精度近似解，适用于实际非指数分布的确定性启动场景。

Conclusion: 该研究填补了多服务器启动时间分析的理论空白，为高效高性能系统设计提供量化依据。

Abstract: In many systems, servers do not turn on instantly; instead, a setup time must pass before a server can begin work. These "setup times" can wreak havoc on a system's queueing; this is especially true in modern systems, where servers are regularly turned on and off as a way to reduce operating costs (energy, labor, CO2, etc.). To design modern systems which are both efficient and performant, we need to understand how setup times affect queues.
  Unfortunately, despite successes in understanding setup in a single-server system, setup in a multiserver system remains poorly understood. To circumvent the main difficulty in analyzing multiserver setup, all existing results assume that setup times are memoryless, i.e. distributed Exponentially. However, in most practical settings, setup times are close to Deterministic, and the widely used Exponential-setup assumption leads to unrealistic model behavior and a dramatic underestimation of the true harm caused by setup times.
  This paper provides a comprehensive characterization of the average waiting time in a multiserver system with Deterministic setup times, the M/M/k/Setup-Deterministic. In particular, we derive upper and lower bounds on the average waiting time in this system, and show these bounds are within a multiplicative constant of each other. These bounds are the first closed-form characterization of waiting time in any finite-server system with setup times. Further, we demonstrate how to combine our upper and lower bounds to derive a simple and accurate approximation for the average waiting time. These results are all made possible via a new technique for analyzing random time integrals that we named the Method of Intervening Stopping Times, or MIST.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [3] [XBIDetective: Leveraging Vision Language Models for Identifying Cross-Browser Visual Inconsistencies](https://arxiv.org/abs/2512.15804)
*Balreet Grewal,James Graham,Jeff Muizelaar,Jan Honza Odvarko,Suhaib Mujahid,Marco Castelluccio,Cor-Paul Bezemer*

Main category: cs.SE

TL;DR: 本文提出XBIDetective工具，利用视觉语言模型自动检测跨浏览器不一致性，准确率达79%，并适用于回归测试与网站监控等场景。


<details>
  <summary>Details</summary>
Motivation: 浏览器渲染缺陷难以发现，而跨浏览器不一致性可作为有效线索，现有方法对动态元素效果不佳。

Method: 开发XBIDetective工具，结合Firefox与Chrome截图，使用预训练及微调的视觉语言模型分析差异。

Result: 在1052个网站上测试，微调模型检测XBI准确率79%，动态元素和广告识别分别达84%与85%。

Conclusion: XBIDetective能有效辅助发现渲染问题，具备自动化测试、大规模监控与快速分诊等实用价值。

Abstract: Browser rendering bugs can be challenging to detect for browser developers, as they may be triggered by very specific conditions that are exhibited on only a very small subset of websites. Cross-browser inconsistencies (XBIs), variations in how a website is interpreted and displayed on different browsers, can be helpful guides to detect such rendering bugs. Although visual and Document Object Model (DOM)-based analysis techniques exist for detecting XBIs, they often struggle with dynamic and interactive elements. In this study, we discuss our industry experience with using vision language models (VLMs) to identify XBIs. We present the XBIDetective tool which automatically captures screenshots of a website in Mozilla Firefox and Google Chrome, and analyzes them with a VLM for XBIs. We evaluate XBIDetective's performance with an off-the-shelf and a fine-tuned VLM on 1,052 websites. We show that XBIDetective can identify cross-browser discrepancies with 79% accuracy and detect dynamic elements and advertisements with 84% and 85% accuracy, respectively, when using the fine-tuned VLM. We discuss important lessons learned, and we present several potential practical use cases for XBIDetective, including automated regression testing, large-scale monitoring of websites, and rapid triaging of XBI bug reports.

</details>


### [4] [OLAF: Towards Robust LLM-Based Annotation Framework in Empirical Software Engineering](https://arxiv.org/abs/2512.15979)
*Mia Mohammad Imran,Tarannum Shaila Zaman*

Main category: cs.SE

TL;DR: 本文提出OLAF框架，旨在提升LLM标注在软件工程研究中的透明性与可复现性。


<details>
  <summary>Details</summary>
Motivation: 当前LLM用于软件工程标注任务时，缺乏对可靠性、校准和漂移等标准度量，且配置细节常被忽略。

Method: 构建名为OLAF的概念框架，整合可靠性、校准、漂移、共识、聚合与透明性六大核心要素。

Result: 为LLM标注建立测量过程视角，推动方法论讨论与未来实证研究。

Conclusion: LLM标注应被视为测量过程，需通过框架化方法确保其科学严谨性。

Abstract: Large Language Models (LLMs) are increasingly used in empirical software engineering (ESE) to automate or assist annotation tasks such as labeling commits, issues, and qualitative artifacts. Yet the reliability and reproducibility of such annotations remain underexplored. Existing studies often lack standardized measures for reliability, calibration, and drift, and frequently omit essential configuration details. We argue that LLM-based annotation should be treated as a measurement process rather than a purely automated activity. In this position paper, we outline the \textbf{Operationalization for LLM-based Annotation Framework (OLAF)}, a conceptual framework that organizes key constructs: \textit{reliability, calibration, drift, consensus, aggregation}, and \textit{transparency}. The paper aims to motivate methodological discussion and future empirical work toward more transparent and reproducible LLM-based annotation in software engineering research.

</details>


### [5] [Embedding Software Intent: Lightweight Java Module Recovery](https://arxiv.org/abs/2512.15980)
*Yirui He,Yuqi Huai,Xingyu Chen,Joshua Garcia*

Main category: cs.SE

TL;DR: ClassLAR是一种基于类名和语言模型的轻量高效架构恢复方法，用于将单体Java项目模块化为JPMS模块。


<details>
  <summary>Details</summary>
Motivation: 现有架构恢复技术难以有效支持Java项目向JPMS模块化迁移，亟需更高效的解决方案。

Method: 利用全限定类名结合语言模型提取语义信息，同时捕捉结构与功能意图，实现轻量级架构恢复。

Result: 在20个流行Java项目中，ClassLAR在架构相似性指标上优于现有技术，执行速度提升3.99至10.50倍。

Conclusion: ClassLAR为Java项目模块化提供了高效且准确的新途径，显著提升了JPMS迁移的可行性。

Abstract: As an increasing number of software systems reach unprecedented scale, relying solely on code-level abstractions is becoming impractical. While architectural abstractions offer a means to manage these systems, maintaining their consistency with the actual code has been problematic. The Java Platform Module System (JPMS), introduced in Java 9, addresses this limitation by enabling explicit module specification at the language level. JPMS enhances architectural implementation through improved encapsulation and direct specification of ground-truth architectures within Java projects. Although many projects are written in Java, modularizing existing monolithic projects to JPMS modules is an open challenge due to ineffective module recovery by existing architecture recovery techniques. To address this challenge, this paper presents ClassLAR (Class-and Language model-based Architectural Recovery), a novel, lightweight, and efficient approach that recovers Java modules from monolithic Java systems using fully-qualified class names. ClassLAR leverages language models to extract semantic information from package and class names, capturing both structural and functional intent. In evaluations across 20 popular Java projects, ClassLAR outperformed all state-of-the-art techniques in architectural-level similarity metrics while achieving execution times that were 3.99 to 10.50 times faster.

</details>


### [6] [LLM4Perf: Large Language Models Are Effective Samplers for Multi-Objective Performance Modeling (Copy)](https://arxiv.org/abs/2512.16070)
*Xin Wang,Zhenhao Li,Zishuo Ding*

Main category: cs.SE

TL;DR: 本文提出LLM4Perf框架，证明大语言模型可有效用于多目标性能建模采样，显著优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 现有采样方法在多目标优化和语义信息利用上存在不足，探索LLM是否能作为更优采样器。

Method: 设计并实现基于反馈的LLM4Perf框架，在四个真实系统中评估LLM引导采样效果。

Result: LLM4Perf在68.8%场景中表现最优，且其配置空间剪枝使基线方法在91.5%案例中提升性能。

Conclusion: LLM在性能工程中具有显著有效性，其成功源于剪枝能力与反馈驱动策略优化。

Abstract: The performance of modern software systems is critically dependent on their complex configuration options. Building accurate performance models to navigate this vast space requires effective sampling strategies, yet existing methods often struggle with multi-objective optimization and cannot leverage semantic information from documentation. The recent success of Large Language Models (LLMs) motivates the central question of this work: Can LLMs serve as effective samplers for multi-objective performance modeling? To explore this, we present a comprehensive empirical study investigating the capabilities and characteristics of LLM-driven sampling. We design and implement LLM4Perf, a feedback-based framework, and use it to systematically evaluate the LLM-guided sampling process across four highly configurable, real-world systems. Our study reveals that the LLM-guided approach outperforms traditional baselines in most cases. Quantitatively, LLM4Perf achieves the best performance in nearly 68.8% (77 out of 112) of all evaluation scenarios, demonstrating its superior effectiveness. We find this effectiveness stems from the LLM's dual capabilities of configuration space pruning and feedback-driven strategy refinement. The effectiveness of this pruning is further validated by the fact that it also improves the performance of the baseline methods in nearly 91.5% (410 out of 448) of cases. Furthermore, we show how the LLM choices for each component and hyperparameters within LLM4Perf affect its effectiveness. Overall, this paper provides strong evidence for the effectiveness of LLMs in performance engineering and offers concrete insights into the mechanisms that drive their success.

</details>


### [7] [Analysis of Design Patterns and Benchmark Practices in Apache Kafka Event-Streaming Systems](https://arxiv.org/abs/2512.16146)
*Muzeeb Mohammad*

Main category: cs.SE

TL;DR: 本文系统综述了2015至2025年间42篇Kafka相关研究，提炼9种核心架构模式与基准测试方法，旨在提升系统设计的可复现性与性能。


<details>
  <summary>Details</summary>
Motivation: 当前Kafka架构模式与基准测试研究分散且缺乏统一标准，阻碍跨研究比较与实际复用。

Method: 对42篇同行评审文献进行结构化综合分析，识别设计模式、共用趋势、领域部署及基准实践，并构建分类体系与决策启发式。

Result: 发现配置披露不全、评估严谨性不足、可复现性差等关键问题，提出模式-基准矩阵与统一分类法以指导实践。

Conclusion: 本研究为构建高性能、容错、可复现的Kafka事件流系统提供了实用架构指南与研究基准框架。

Abstract: Apache Kafka has become a foundational platform for high throughput event streaming, enabling real time analytics, financial transaction processing, industrial telemetry, and large scale data driven systems. Despite its maturity and widespread adoption, consolidated research on reusable architectural design patterns and reproducible benchmarking methodologies remains fragmented across academic and industrial publications. This paper presents a structured synthesis of forty two peer reviewed studies published between 2015 and 2025, identifying nine recurring Kafka design patterns including log compaction, CQRS bus, exactly once pipelines, change data capture, stream table joins, saga orchestration, tiered storage, multi tenant topics, and event sourcing replay. The analysis examines co usage trends, domain specific deployments, and empirical benchmarking practices using standard suites such as TPCx Kafka and the Yahoo Streaming Benchmark, as well as custom workloads. The study highlights significant inconsistencies in configuration disclosure, evaluation rigor, and reproducibility that limit cross study comparison and practical replication. By providing a unified taxonomy, pattern benchmark matrix, and actionable decision heuristics, this work offers practical guidance for architects and researchers designing reproducible, high performance, and fault tolerant Kafka based event streaming systems.

</details>


### [8] [Beyond Blind Spots: Analytic Hints for Mitigating LLM-Based Evaluation Pitfalls](https://arxiv.org/abs/2512.16272)
*Ora Nova Fandina,Eitan Farchi,Shmulik Froimovich,Raviv Gal,Wesam Ibraheem,Rami Katan,Alice Podolsky*

Main category: cs.SE

TL;DR: 本文研究大语言模型作为代码生成评估者时的局限性，提出结合轻量分析工具提示的方法，显著提升其在COBOL现代化任务中的错误检测率与解释质量。


<details>
  <summary>Details</summary>
Motivation: 大语言模型作为评估者常忽略领域特定问题，影响关键任务可靠性，需改进其在工业场景如COBOL代码现代化中的表现。

Method: 构建领域错误分类体系，开发轻量分析检查器生成提示，并动态注入大语言模型评估过程，形成混合评估框架。

Result: 实验显示单独使用大语言模型仅能检测约45%错误，而结合提示后最高可达94%覆盖率，且解释更丰富准确。

Conclusion: 分析型-LLM混合方法可显著增强部署管道中评估的可靠性，适用于对领域知识要求高的代码生成任务。

Abstract: Large Language Models are increasingly deployed as judges (LaaJ) in code generation pipelines. While attractive for scalability, LaaJs tend to overlook domain specific issues raising concerns about their reliability in critical evaluation tasks. To better understand these limitations in practice, we examine LaaJ behavior in a concrete industrial use case: legacy code modernization via COBOL code generation. In this setting, we find that even production deployed LaaJs can miss domain critical errors, revealing consistent blind spots in their evaluation capabilities.
  To better understand these blind spots, we analyze generated COBOL programs and associated LaaJs judgments, drawing on expert knowledge to construct a preliminary taxonomy. Based on this taxonomy, we develop a lightweight analytic checker tool that flags over 30 domain specific issues observed in practice. We use its outputs as analytic hints, dynamically injecting them into the judges prompt to encourage LaaJ to revisit aspects it may have overlooked.
  Experiments on a test set of 100 programs using four production level LaaJs show that LaaJ alone detects only about 45% of the errors present in the code (in all judges we tested), while the analytic checker alone lacks explanatory depth. When combined, the LaaJ+Hints configuration achieves up to 94% coverage (for the best performing judge and injection prompt) and produces qualitatively richer, more accurate explanations, demonstrating that analytic-LLM hybrids can substantially enhance evaluation reliability in deployed pipelines. We release the dataset and all used prompts.

</details>


### [9] [Using a Sledgehammer to Crack a Nut? Revisiting Automated Compiler Fault Isolation](https://arxiv.org/abs/2512.16335)
*Yibiao Yang,Qingyang Li,Maolin Sun,Jiangchang Wu,Yuming Zhou*

Main category: cs.SE

TL;DR: 该研究比较了基于BIC的策略与SBFL技术在编译器故障定位中的表现，发现BIC策略在关键指标上表现更优或相当。


<details>
  <summary>Details</summary>
Motivation: 评估SBFL技术在实际编译器调试中相对于广泛使用的BIC策略的有效性。

Method: 采用二分搜索定位引发故障的提交，并将修改文件标记为潜在故障，对比60个GCC和60个LLVM缺陷案例。

Result: BIC策略在Top-1和Top-5排名指标上表现优于或等同于先进SBFL技术。

Conclusion: 建议未来研究将BIC策略作为新方法评估的基准。

Abstract: Background: Compilers are fundamental to software development, translating high-level source code into executable software systems. Faults in compilers can have severe consequences and thus effective localization and resolution of compiler bugs are crucial. Problem: In practice, developers often examine version history to identify and investigate bug-inducing commit (BIC) for fixing bugs. However, while numerous sophisticated Spectrum-Based Fault Localization (SBFL) techniques have been proposed for compiler fault isolation, their effectiveness has not been evaluated against the BIC-based strategies widely adopted in practice. Objective: This study aims to bridge this gap by directly comparing a BIC-based strategy, Basic, with representative SBFL techniques in the context of compiler fault localization. The BIC-based strategy closely aligns with common developer practices, as it directly identifies the BIC and treats the files modified in that commit as faulty candidates. Method: The Basic identifies the most recent good release and earliest bad release, and then employs a binary search to pinpoint the bug-inducing commit. All files modified in the identified commit are flagged as potentially faulty. We rigorously compare Basic against SBFL-based techniques using a benchmark consisting of 60 GCC bugs and 60 LLVM bugs. Result: Our analysis reveals that Basic performs comparably to, and in many cases outperforms, state-of-the-art SBFL-based techniques, particularly on the critical Top-1 and Top-5 ranking metrics. Conclusion: This study provides new insights into the practical effectiveness of SBFL-based techniques in real-world compiler debugging scenarios. We recommend that future research adopt Basic as a baseline when developing and evaluating new compiler fault isolation methods.

</details>


### [10] [An Empirical Study of the Realism of Mutants in Deep Learning](https://arxiv.org/abs/2512.16741)
*Zaheed Ahmed,Philip Makedonski,Jens Grabowski*

Main category: cs.SE

TL;DR: 本研究首次比较了深度学习中预训练和后训练突变方法的真实性，发现预训练突变更贴近真实故障，但计算成本高，需改进后训练方法。


<details>
  <summary>Details</summary>
Motivation: 验证深度学习中突变分析是否如传统软件般能有效模拟真实故障，以支持测试、修复与鲁棒性评估等任务。

Method: 基于CleanML、DeepFD等公开数据集，使用前沿工具生成预训练与后训练突变体，并构建统计框架量化其与真实故障的耦合强度与行为相似性。

Result: 预训练突变在耦合强度和行为相似性上显著优于后训练突变，表明其更真实，但计算开销大。

Conclusion: 预训练突变更真实，但为兼顾效率，亟需开发能匹敌其真实性的高效后训练突变算子。

Abstract: Mutation analysis is a well-established technique for assessing test quality in the traditional software development paradigm by injecting artificial faults into programs. Its application to deep learning (DL) has expanded beyond classical testing to support tasks such as fault localization, repair, data generation, and model robustness evaluation. The core assumption is that mutants behave similarly to real faults, an assumption well established in traditional software systems but largely unverified for DL.
  This study presents the first empirical comparison of pre-training and post-training mutation approaches in DL with respect to realism. We introduce a statistical framework to quantify their coupling strength and behavioral similarity to real faults using publicly available bugs datasets: CleanML, DeepFD, DeepLocalize, and defect4ML. Mutants are generated using state-of-the-art tools representing both approaches.
  Results show that pre-training mutants exhibit consistently stronger coupling and higher behavioral similarity to real faults than post-training mutants, indicating greater realism. However, the substantial computational cost of pre-training mutation underscores the need for more effective post-training operators that match or exceed the realism demonstrated by pre-training mutants.

</details>


### [11] [Inside Out: Uncovering How Comment Internalization Steers LLMs for Better or Worse](https://arxiv.org/abs/2512.16790)
*Aaron Imani,Mohammad Moshirpour,Iftekhar Ahmed*

Main category: cs.SE

TL;DR: 本文首次从概念层面研究大语言模型在软件工程任务中对代码注释的内部表征及其影响，揭示了不同注释类型如何被模型区分并显著影响任务性能。


<details>
  <summary>Details</summary>
Motivation: 理解大语言模型在软件工程任务中如何依赖和内化代码注释，以提升模型透明度与工具设计能力。

Method: 使用概念激活向量（CAV）分析模型内部注释表征，并通过激活/去激活实验测量其对10种SE任务性能的影响。

Result: 模型能区分不同注释类型，其激活程度随任务变化，代码摘要最强、代码补全最弱；性能波动范围达-90%至+67%。

Conclusion: 未来SE工具应关注操控模型内部概念表征，而非仅依赖表面输入，以实现更精准可控的任务执行。

Abstract: While comments are non-functional elements of source code, Large Language Models (LLM) frequently rely on them to perform Software Engineering (SE) tasks. Yet, where in the model this reliance resides, and how it affects performance, remains poorly understood. We present the first concept-level interpretability study of LLMs in SE, analyzing three tasks - code completion, translation, and refinement - through the lens of internal comment representation. Using Concept Activation Vectors (CAV), we show that LLMs not only internalize comments as distinct latent concepts but also differentiate between subtypes such as Javadocs, inline, and multiline comments. By systematically activating and deactivating these concepts in the LLMs' embedding space, we observed significant, model-specific, and task-dependent shifts in performance ranging from -90% to +67%. Finally, we conducted a controlled experiment using the same set of code inputs, prompting LLMs to perform 10 distinct SE tasks while measuring the activation of the comment concept within their latent representations. We found that code summarization consistently triggered the strongest activation of comment concepts, whereas code completion elicited the weakest sensitivity. These results open a new direction for building SE tools and models that reason about and manipulate internal concept representations rather than relying solely on surface-level input.

</details>


### [12] [Toward Systematic Counterfactual Fairness Evaluation of Large Language Models: The CAFFE Framework](https://arxiv.org/abs/2512.16816)
*Alessandra Parziale,Gianmario Voria,Valeria Pontillo,Gemma Catolino,Andrea De Lucia,Fabio Palomba*

Main category: cs.SE

TL;DR: 本文提出CAFFE框架，用于更全面、可靠地检测大语言模型中的公平性问题。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型在软件系统中广泛应用，其公平性问题日益受到关注，现有方法存在覆盖不足和检测不稳定的问题。

Method: 提出CAFFE框架，通过明确定义测试组件（如提示意图、对话上下文等），自动生成测试数据，并使用语义相似度评估模型响应。

Result: 在三种不同架构的LLM上实验表明，CAFFE比现有方法覆盖更广、检测更可靠。

Conclusion: CAFFE为大语言模型的反事实公平性测试提供了一种结构化、意图感知的新方案，显著优于传统蜕变测试方法。

Abstract: Nowadays, Large Language Models (LLMs) are foundational components of modern software systems. As their influence grows, concerns about fairness have become increasingly pressing. Prior work has proposed metamorphic testing to detect fairness issues, applying input transformations to uncover inconsistencies in model behavior. This paper introduces an alternative perspective for testing counterfactual fairness in LLMs, proposing a structured and intent-aware framework coined CAFFE (Counterfactual Assessment Framework for Fairness Evaluation). Inspired by traditional non-functional testing, CAFFE (1) formalizes LLM-Fairness test cases through explicitly defined components, including prompt intent, conversational context, input variants, expected fairness thresholds, and test environment configuration, (2) assists testers by automatically generating targeted test data, and (3) evaluates model responses using semantic similarity metrics. Our experiments, conducted on three different architectural families of LLM, demonstrate that CAFFE achieves broader bias coverage and more reliable detection of unfair behavior than existing metamorphic approaches.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [13] [Acoustic RIS for Massive Spatial Multiplexing: Unleashing Degrees of Freedom and Capacity in Underwater Communications](https://arxiv.org/abs/2512.16470)
*Longfei Zhao,Jingbo Tan,Jintao Wang,Ian F Akyildiz,Zhi Sun*

Main category: cs.NI

TL;DR: 本文提出利用声学可重构智能表面（aRIS）增强水下声通信MIMO系统的空间自由度与信道容量，通过ASTAR架构和最优部署策略，在浅海和深海场景分别实现最高265%和170%的容量提升。


<details>
  <summary>Details</summary>
Motivation: 传统水下声MIMO系统受限于阵列分辨率低、角度模糊和空间自由度不足，难以满足高速海洋数据传输需求。

Method: 建立海洋专用自由度-信道耦合模型，推导空间秩增强条件；解析确定最优部署位置Light-Point；提出ASTAR-aRIS架构，结合UUV与声强梯度感知实现自适应波束跟踪。

Result: 仿真表明，所提联合部署与波束成形框架在浅海和深海环境中分别最多提升信道容量265%和170%。

Conclusion: aRIS能有效生成正交虚拟路径，显著提升水下声通信的空间自由度与系统容量，为未来高可靠高速水下通信提供新范式。

Abstract: Underwater acoustic (UWA) communications are essential for high-speed marine data transmission but remain severely constrained by limited bandwidth, significant propagation loss, and sparse multipath structures. Conventional underwater acoustic multiple-input multiple-output (MIMO) systems primarily utilize spatial diversity but suffer from limited array resolution, causing angular ambiguity and insufficient spatial degrees of freedom (DoFs). This paper addresses these limitations through acoustic Reconfigurable Intelligent Surfaces (aRIS) to actively generate orthogonally distinguishable virtual paths, significantly enhancing spatial DoFs and channel capacity. An ocean-specific DoF-channel coupling model is established, explicitly deriving conditions for spatial rank enhancement. Subsequently, the optimal geometric locus, termed the Light-Point, is analytically identified, where deploying a single aRIS maximizes DoFs by introducing two and three additional resolvable paths in deep-sea and shallow-sea environments, respectively. Furthermore, an active simultaneous transmitting and reflecting (ASTAR) aRIS architecture with independent beam control and adaptive beam-tracking mechanism integrating unmanned underwater vehicles (UUVs) and acoustic intensity gradient sensing is proposed. Extensive simulations validate the proposed joint aRIS deployment and beamforming framework, demonstrating substantial UWA channel capacity improvements-up to 265% and 170% in shallow-sea and deep-sea scenarios, respectively.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [14] [Workload Characterization for Branch Predictability](https://arxiv.org/abs/2512.15827)
*FNU Vikas,Paul Gratz,Daniel Jiménez*

Main category: cs.AR

TL;DR: 本文提出了一种基于工作负载的分支预测准确性分析方法，定义了分支工作集大小和可预测性两个新指标，用于评估现代分支预测器性能。


<details>
  <summary>Details</summary>
Motivation: 提升分支预测准确性以优化处理器性能与能效，需深入理解工作负载对预测器的影响。

Method: 通过分析2451个跟踪数据，提取分支地址、全局与局部历史构成的上下文，计算工作集大小与可预测性，并分类研究其与TAGE、感知机等预测器误判率的相关性。

Result: 发现所提指标与现代预测器误判率高度相关，成功将工作负载划分为7类工作集大小与9类可预测性，并揭示了预测器偏好的工作负载类型。

Conclusion: 该方法为分支预测器设计与优化提供了有效的负载特征刻画工具，有助于针对性提升预测精度。

Abstract: Conditional branch prediction predicts the likely direction of a conditional branch instruction to support ILP extraction. Branch prediction is a pattern recognition problem that learns mappings between a context to the branch outcome. An accurate predictor reduces the number of instructions executed on the wrong path resulting in an improvement of performance and energy consumption. In this paper, we present a workload characterization methodology for branch prediction. We propose two new workload-driven branch prediction accuracy identifiers -- branch working set size and branch predictability. These parameters are highly correlated with misprediction rates of modern branch prediction schemes (e.g. TAGE and perceptron). We define the branch working set of a trace as a group of most frequently occurring branch contexts, i.e. the 3-part tuple of branch address, and associated global and local history. We analyze the branch working set's size and predictability on a per-trace basis to study its relationship with a modern branch predictor's accuracy. We have characterized 2,451 workload traces into seven branch working set size and nine predictability categories after analyzing their branch behavior. We present further insights into the source of prediction accuracy and favored workload categories for modern branch predictors.

</details>


### [15] [Full System Architecture Modeling for Wearable Egocentric Contextual AI](https://arxiv.org/abs/2512.16045)
*Vincent T. Lee,Tanfer Alan,Sung Kim,Ecenur Ustun,Amr Suleiman,Ajit Krisshna,Tim Balbekov,Armin Alaghi,Richard Newcombe*

Main category: cs.AR

TL;DR: 本文探讨了支持情境AI的可穿戴设备系统架构，强调全系统视角对优化功耗与性能的重要性，并分享Aria2设计中的经验教训。


<details>
  <summary>Details</summary>
Motivation: 推动全天候、空间感知的情境AI助手发展，需克服可穿戴设备在功耗与系统复杂性上的挑战。

Method: 通过构建端到端系统模型，对Aria2可穿戴设备进行架构分析与设计空间探索。

Result: 发现系统功耗无单一主导组件，需全局协同优化以避免瓶颈限制；提出对未来全天候情境AI系统的关键设计启示。

Conclusion: 实现全天候可穿戴情境AI需采用全系统视角进行协同设计，避免局部优化导致整体性能受限。

Abstract: The next generation of human-oriented computing will require always-on, spatially-aware wearable devices to capture egocentric vision and functional primitives (e.g., Where am I? What am I looking at?, etc.). These devices will sense an egocentric view of the world around us to observe all human- relevant signals across space and time to construct and maintain a user's personal context. This personal context, combined with advanced generative AI, will unlock a powerful new generation of contextual AI personal assistants and applications. However, designing a wearable system to support contextual AI is a daunting task because of the system's complexity and stringent power constraints due to weight and battery restrictions. To understand how to guide design for such systems, this work provides the first complete system architecture view of one such wearable contextual AI system (Aria2), along with the lessons we have learned through the system modeling and design space exploration process. We show that an end-to-end full system model view of such systems is vitally important, as no single component or category overwhelmingly dominates system power. This means long-range design decisions and power optimizations need to be made in the full system context to avoid running into limits caused by other system bottlenecks (i.e., Amdahl's law as applied to power) or as bottlenecks change. Finally, we reflect on lessons and insights for the road ahead, which will be important toward eventually enabling all-day, wearable, contextual AI systems.

</details>


<div id='cs.OS'></div>

# cs.OS [[Back]](#toc)

### [16] [Trustworthy and Controllable Professional Knowledge Utilization in Large Language Models with TEE-GPU Execution](https://arxiv.org/abs/2512.16238)
*Yifeng Cai,Zhida An,Yuhan Meng,Houqian Liu,Pengli Wang,Yao Guo,Ding Li*

Main category: cs.OS

TL;DR: PKUS 是一种专业知知识利用系统，通过将专业知识作为独立可分离的组件，在可信执行环境中运行适配器，实现高效且安全的LLM服务。


<details>
  <summary>Details</summary>
Motivation: 解决数据提供者在贡献专业知识时面临的收益与风险失衡问题，提升其参与意愿。

Method: 使用硬件可信执行环境运行紧凑适配器，结合生命周期协议、适配器剪枝、多提供方聚合和分拆执行调度。

Result: 在多个基准测试中保持模型性能，同时实现8.1-11.9倍的推理加速。

Conclusion: PKUS 在保障数据提供者权益的同时，显著提升专业领域LLM服务的效率与实用性。

Abstract: Future improvements in large language model (LLM) services increasingly hinge on access to high-value professional knowledge rather than more generic web data. However, the data providers of this knowledge face a skewed tradeoff between income and risk: they receive little share of downstream value yet retain copyright and privacy liability, making them reluctant to contribute their assets to LLM services. Existing techniques do not offer a trustworthy and controllable way to use professional knowledge, because they keep providers in the dark and combine knowledge parameters with the underlying LLM backbone.
  In this paper, we present PKUS, the Professional Knowledge Utilization System, which treats professional knowledge as a first-class, separable artifact. PKUS keeps the backbone model on GPUs and encodes each provider's contribution as a compact adapter that executes only inside an attested Trusted Execution Environment (TEE). A hardware-rooted lifecycle protocol, adapter pruning, multi-provider aggregation, and split-execution scheduling together make this design practical at serving time. On SST-2, MNLI, and SQuAD with GPT-2 Large and Llama-3.2-1B, PKUS preserves model utility, matching the accuracy and F1 of full fine-tuning and plain LoRA, while achieving the lowest per-request latency with 8.1-11.9x speedup over CPU-only TEE inference and naive CPU-GPU co-execution.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [17] [LOG.io: Unified Rollback Recovery and Data Lineage Capture for Distributed Data Pipelines](https://arxiv.org/abs/2512.16038)
*Eric Simon,Renato B. Hoffmann,Lucas Alf,Dalvan Griebler*

Main category: cs.DC

TL;DR: LOG.io 是一种针对无服务器可扩展架构设计的分布式数据管道解决方案，支持细粒度数据溯源与非阻塞回滚恢复。


<details>
  <summary>Details</summary>
Motivation: 解决分布式数据管道中容错恢复效率低、缺乏细粒度数据溯源能力的问题。

Method: 采用基于日志的回滚恢复协议，支持非确定算子、外部系统交互及任意自定义代码，允许动态扩缩容和并行恢复。

Result: 在存在慢速算子且吞吐适中的场景下，LOG.io 恢复性能优于 ABS；数据并行可显著降低其开销，而 ABS 无法受益；数据溯源开销低于 1.5%。

Conclusion: LOG.io 在特定负载条件下具备更优恢复能力与极低溯源开销，适合需灵活容错与溯源追踪的现代数据管道架构。

Abstract: This paper introduces LOG.io, a comprehensive solution designed for correct rollback recovery and fine-grain data lineage capture in distributed data pipelines. It is tailored for serverless scalable architectures and uses a log-based rollback recovery protocol. LOG.io supports a general programming model, accommodating non-deterministic operators, interactions with external systems, and arbitrary custom code. It is non-blocking, allowing failed operators to recover independently without interrupting other active operators, thereby leveraging data parallelization, and it facilitates dynamic scaling of operators during pipeline execution. Performance evaluations, conducted within the SAP Data Intelligence system, compare LOG.io with the Asynchronous Barrier Snapshotting (ABS) protocol, originally implemented in Flink. Our experiments show that when there are straggler operators in a data pipeline and the throughput of events is moderate (e.g., 1 event every 100 ms), LOG.io performs as well as ABS during normal processing and outperforms ABS during recovery. Otherwise, ABS performs better than LOG.io for both normal processing and recovery. However, we show that in these cases, data parallelization can largely reduce the overhead of LOG.io while ABS does not improve. Finally, we show that the overhead of data lineage capture, at the granularity of the event and between any two operators in a pipeline, is marginal, with less than 1.5% in all our experiments.

</details>


### [18] [MultiPath Transfer Engine: Breaking GPU and Host-Memory Bandwidth Bottlenecks in LLM Services](https://arxiv.org/abs/2512.16056)
*Lingfeng Tang,Daoping Zhang,Junjie Chen,Peihao Huang,Feng Jin,Chengguang Xu,Yuxin Chen,Feiqiang Sun,Guo Chen*

Main category: cs.DC

TL;DR: MMA通过多路径内存访问显著提升GPU与主机内存间的数据传输带宽，优化LLM服务性能。


<details>
  <summary>Details</summary>
Motivation: PCIe带宽限制导致LLM性能瓶颈，现有异构协议无法充分利用服务器内带宽资源。

Method: 提出Multipath Memory Access方案，通过动态库注入实现无需代码修改的无缝部署。

Result: 测试中带宽峰值达245GB/s（4.62倍提升），TTFT降低1.14x-2.38x，模型切换延迟减少1.12x-2.48x。

Conclusion: MMA有效突破PCIe瓶颈，大幅提升LLM服务效率，具备实际部署价值。

Abstract: The limited bandwidth of PCIe has emerged as the critical bottleneck for large language model (LLM) performance, such as prefix cache fetching and model switching. Although intra-server multipath data transfer between GPU and host memory is theoretically possible, heterogeneous protocols such as PCIe and NVLink currently limit the bandwidth between host memory and GPUs to that of a single PICe link. This limitation resuals in underutilized intra-server bandwidth. To address this issue, we propose Multipath Memory Access (MMA), a scheme that, to the best of our knowledge, is the first to enalbe efficient multipath data transfer between GPU and host memory. MMA supports seamless deployment via dynamic library injection, enabling LLM applications to benefit from MMA without requiring any code modification. In our testbed, MMA significantly improves the data transfer bandwidth between the GPU and memory, achieving a peak bandwidth of 245 GB/s-representing a 4.62x speedup compared to the natice single-path bandwidth. End-to-end evaluations demonstrate that MMA reduces the time-to-first-token (TTFT) for LLM serving by 1.14x to 2.38x and decreases model-switching latency in vLLM's sleep mode by 1.12x to 2.48x.

</details>


### [19] [Cold-Start Anti-Patterns and Refactorings in Serverless Systems: An Empirical Study](https://arxiv.org/abs/2512.16066)
*Syed Salauddin Mohammad Tariq,Foyzul Hassan,Amiangshu Bosu,Probir Roy*

Main category: cs.DC

TL;DR: 本文通过分析开源无服务器系统中的冷启动问题，提出SCABENCH基准和INITSCOPE分析框架，显著提升诊断准确率并降低开发负担。


<details>
  <summary>Details</summary>
Motivation: 冷启动延迟是无服务器计算的主要性能瓶颈，现有研究多将其视为黑盒优化问题，缺乏开发者视角的设计分析。

Method: 从81个开源项目问题报告中归纳反模式与修复策略，构建可复现基准SCABENCH及轻量分析工具INITSCOPE，结合实验与开发者研究验证效果。

Result: INITSCOPE在定位准确性上提升40%，诊断工作量减少64%，开发者任务完成更准确且更快速。

Conclusion: 本研究推动了基于实证、注重性能的无服务器冷启动缓解设计实践。

Abstract: Serverless computing simplifies deployment and scaling, yet cold-start latency remains a major performance bottleneck. Unlike prior work that treats mitigation as a black-box optimization, we study cold starts as a developer-visible design problem. From 81 adjudicated issue reports across open-source serverless systems, we derive taxonomies of initialization anti-patterns, remediation strategies, and diagnostic challenges spanning design, packaging, and runtime layers. Building on these insights, we introduce SCABENCH, a reproducible benchmark, and INITSCOPE, a lightweight analysis framework linking what code is loaded with what is executed. On SCABENCH, INITSCOPE improved localization accuracy by up to 40% and reduced diagnostic effort by 64% compared with prior tools, while a developer study showed higher task accuracy and faster diagnosis. Together, these results advance evidence-driven, performance-aware practices for cold-start mitigation in serverless design. Availability: The research artifact is publicly accessible for future studies and improvements.

</details>


### [20] [An Online Fragmentation-Aware Scheduler for Managing GPU-Sharing Workloads on Multi-Instance GPUs](https://arxiv.org/abs/2512.16099)
*Hsu-Tzu Ting,Jerry Chou,Ming-Hung Chen,I-Hsin Chung*

Main category: cs.DC

TL;DR: 提出一种结合条件负载均衡、动态分区和作业迁移的在线调度框架，以优化MIG GPU资源利用并减少碎片化。


<details>
  <summary>Details</summary>
Motivation: MIG虽提供强隔离性，但存在PCIe带宽竞争与配置受限导致的碎片化问题，影响效率。

Method: 通过动态调整作业放置、重组GPU分配，缓解资源争用与内外部碎片。

Result: 实验表明，综合应用所有技术后，系统完成时间最多缩短35%。

Conclusion: 所提框架有效提升MIG环境下GPU资源共享效率，解决碎片化与资源争用瓶颈。

Abstract: Modern GPU workloads increasingly demand efficient resource sharing, as many jobs do not require the full capacity of a GPU. Among sharing techniques, NVIDIA's Multi-Instance GPU (MIG) offers strong resource isolation by enabling hardware-level GPU partitioning. However, leveraging MIG effectively introduces new challenges. First, resource contention persists due to shared components such as PCIe bandwidth. Second, GPU fragmentation becomes a critical issue, which is different from prior fine-grained GPU sharing work due to MIG's limited number of valid MIG configurations. Fragmentation arises not only from spatial discontinuity but also from rigid profile placement constraints, especially after job arrivals and terminations. To address these issues, we propose an online scheduling framework that integrates conditional load balancing, dynamic partitioning, and job migration. Our approach dynamically adapts job placement to minimize contention and reorganizes GPU allocations to combat both internal and external fragmentation. Experimental results show that our method significantly improves system efficiency. When all techniques are applied, the makespan improves by up to 35%.

</details>


### [21] [Staggered Batch Scheduling: Co-optimizing Time-to-First-Token and Throughput for High-Efficiency LLM Inference](https://arxiv.org/abs/2512.16134)
*Jian Tian,Shuailong Li,Yang Cao,Wenbo Cui,Minghan Zhu,Wenkang Wu,Jianming Zhang,Yanpeng Wang,Zhiwen Xiao,Zhenyu Hou,Dou Shen*

Main category: cs.DC

TL;DR: 提出交错批处理调度（SBS）与负载感知全局分配策略，优化LLM在DP+EP架构下的TTFT与吞吐量。


<details>
  <summary>Details</summary>
Motivation: DP+EP架构因内部同步开销大，导致即时调度引发队列堆积与并行气泡，恶化首Token延迟。

Method: 通过缓冲请求形成最优执行批次消除内部气泡，并利用缓冲窗口实现Prefill/Decode阶段的负载均衡调度。

Result: 在H800集群部署Deepseek-V3，相比先进基线降低TTFT 30%-40%，提升吞吐量15%-20%。

Conclusion: SBS机制有效解决分布式LLM服务调度瓶颈，兼顾延迟优化与高吞吐需求。

Abstract: The evolution of Large Language Model (LLM) serving towards complex, distributed architectures--specifically the P/D-separated, large-scale DP+EP paradigm--introduces distinct scheduling challenges. Unlike traditional deployments where schedulers can treat instances as black boxes, DP+EP architectures exhibit high internal synchronization costs. We identify that immediate request dispatching in such systems leads to severe in-engine queuing and parallelization bubbles, degrading Time-to-First-Token (TTFT). To address this, we propose Staggered Batch Scheduling (SBS), a mechanism that deliberately buffers requests to form optimal execution batches. This temporal decoupling eliminates internal queuing bubbles without compromising throughput. Furthermore, leveraging the scheduling window created by buffering, we introduce a Load-Aware Global Allocation strategy that balances computational load across DP units for both Prefill and Decode phases. Deployed on a production H800 cluster serving Deepseek-V3, our system reduces TTFT by 30%-40% and improves throughput by 15%-20% compared to state-of-the-art immediate scheduling baselines.

</details>


### [22] [Lotus: Optimizing Disaggregated Transactions with Disaggregated Locks](https://arxiv.org/abs/2512.16136)
*Zhisheng Hu,Pengfei Zuo,Junliang Hu,Yizou Chen,Yingjia Wang,Ming-Chang Yang*

Main category: cs.DC

TL;DR: Lotus是一种在解聚内存上实现锁解聚的可扩展分布式事务系统，通过将锁从数据中分离并在计算节点上执行，消除了内存节点RDMA网卡的性能瓶颈。


<details>
  <summary>Details</summary>
Motivation: 现有分布式事务系统在解聚内存架构下因大量单边原子操作导致内存节点RDMA网卡成为性能瓶颈，限制了系统扩展性。

Method: Lotus将锁与数据分离并在计算节点上执行，采用应用感知的锁管理机制、锁优先事务协议及无锁重建恢复机制以提升效率和容错能力。

Result: 实验表明，Lotus相比现有最先进的解聚内存事务系统，事务吞吐量最高提升2.1倍，延迟降低最多49.4%。

Conclusion: Lotus有效解决了DM架构下锁操作引发的性能瓶颈问题，显著提升了分布式事务系统的可扩展性和效率。

Abstract: Disaggregated memory (DM) separates compute and memory resources, allowing flexible scaling to achieve high resource utilization. To ensure atomic and consistent data access on DM, distributed transaction systems have been adapted, where compute nodes (CNs) rely on one-sided RDMA operations to access remote data in memory nodes (MNs). However, we observe that in existing transaction systems, the RDMA network interface cards at MNs become a primary performance bottleneck. This bottleneck arises from the high volume of one-sided atomic operations used for locks, which hinders the system's ability to scale efficiently.
  To address this issue, this paper presents Lotus, a scalable distributed transaction system with lock disaggregation on DM. The key innovation of Lotus is to disaggregate locks from data and execute all locks on CNs, thus eliminating the bottleneck at MN RNICs. To achieve efficient lock management on CNs, Lotus employs an application-aware lock management mechanism that leverages the locality of the OLTP workloads to shard locks while maintaining load balance. To ensure consistent transaction processing with lock disaggregation, Lotus introduces a lock-first transaction protocol, which separates the locking phase as the first step in each read-write transaction execution. This protocol allows the system to determine the success of lock acquisitions early and proactively abort conflicting transactions, improving overall efficiency. To tolerate lock loss during CN failures, Lotus employs a lock-rebuild-free recovery mechanism that treats locks as ephemeral and avoids their reconstruction, ensuring lightweight recovery for CN failures. Experimental results demonstrate that Lotus improves transaction throughput by up to 2.1$\times$ and reduces latency by up to 49.4% compared to state-of-the-art transaction systems on DM.

</details>


### [23] [AI4EOSC: a Federated Cloud Platform for Artificial Intelligence in Scientific Research](https://arxiv.org/abs/2512.16455)
*Ignacio Heredia,Álvaro López García,Germán Moltó,Amanda Calatrava,Valentin Kozlov,Alessandro Costantini,Viet Tran,Mario David,Daniel San Martín,Marcin Płóciennik,Marta Obregón Ruiz,Saúl Fernandez,Judith Sáinz-Pardo Díaz,Miguel Caballer,Caterina Alarcón Marín,Stefan Dlugolinsky,Martin Šeleng,Lisana Berberi,Khadijeh Alibabaei,Borja Esteban Sanchis,Pedro Castro,Giacinto Donvito,Diego Aguirre,Sergio Langarita,Vicente Rodriguez,Leonhard Duda,Andrés Heredia Canales,Susana Rebolledo Ruiz,João Machado,Giang Nguyen,Fernando Aguilar Gómez,Jaime Díez*

Main category: cs.DC

TL;DR: 该论文介绍了一个支持科学工作中人工智能的联邦计算平台，提供从模型开发到部署的全生命周期服务。


<details>
  <summary>Details</summary>
Motivation: 为科学工作负载中的AI研究提供可复现、透明且集成的联邦计算环境。

Method: 构建包含交互式开发环境、GPU训练资源、联邦学习支持及多样化部署选项的综合服务平台。

Result: 平台实现了跨分布式基础设施的一致访问，支持模型可追溯性与生态系统集成，并易于外部社区定制使用。

Conclusion: 该平台有效降低了AI科研的采用门槛，推动了跨机构协作与模型复现能力。

Abstract: In this paper, we describe a federated compute platform dedicated to support Artificial Intelligence in scientific workloads. Putting the effort into reproducible deployments, it delivers consistent, transparent access to a federation of physically distributed e-Infrastructures. Through a comprehensive service catalogue, the platform is able to offer an integrated user experience covering the full Machine Learning lifecycle, including model development (with dedicated interactive development environments), training (with GPU resources, annotation tools, experiment tracking, and federated learning support) and deployment (covering a wide range of deployment options all along the Cloud Continuum). The platform also provides tools for traceability and reproducibility of AI models, integrates with different Artificial Intelligence model providers, datasets and storage resources, allowing users to interact with the broader Machine Learning ecosystem. Finally, it is easily customizable to lower the adoption barrier by external communities.

</details>


### [24] [Efficient CPU-GPU Collaborative Inference for MoE-based LLMs on Memory-Limited Systems](https://arxiv.org/abs/2512.16473)
*En-Ming Huang,Li-Shang Lin,Chun-Yi Lee*

Main category: cs.DC

TL;DR: 提出一种CPU-GPU协同推理框架，通过专家缓存机制提升消费级硬件上MoE模型的推理效率。


<details>
  <summary>Details</summary>
Motivation: 解决MoE模型在消费级GPU上内存不足及传统CPU-GPU权重传输延迟高的问题。

Method: 在GPU上引入专家缓存减少数据传输，利用CPU多线程处理缓存未命中计算。

Result: 实验表明该框架显著提升推理性能，并有效利用硬件资源。

Conclusion: CPU-GPU协同推理是消费级系统部署大模型的可行方案。

Abstract: Large Language Models (LLMs) have achieved impressive results across various tasks, yet their high computational demands pose deployment challenges, especially on consumer-grade hardware. Mixture of Experts (MoE) models provide an efficient solution through selective activation of parameter subsets, which reduces computation requirements. Despite this efficiency, state-of-the-art MoE models still require substantial memory beyond typical consumer GPU capacities. Traditional offloading methods that transfer model weights between CPU and GPU introduce latency, limiting inference performance. This paper presents a novel CPU-GPU collaborative inference framework that incorporates an expert caching mechanism on the GPU to reduce data transfer requirements and enable faster inference through cache hits. Computations are offloaded to CPU for efficient cache miss handling, which benefits from CPU multithreading optimizations. The evaluations of our framework demonstrate performance improvements and highlight the potential of CPU-GPU collaboration to maximize hardware utilization for single-request inference scenarios on consumer-grade systems. The implementation of our framework is available at https://github.com/elsa-lab/MoE-CPU-GPU-Collaborative-Inference.

</details>
