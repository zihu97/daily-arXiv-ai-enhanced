<div id=toc></div>

# Table of Contents

- [cs.MA](#cs.MA) [Total: 2]
- [cs.AR](#cs.AR) [Total: 3]
- [cs.DC](#cs.DC) [Total: 7]
- [cs.SE](#cs.SE) [Total: 14]


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [1] [Communication to Completion: Modeling Collaborative Workflows with Intelligent Multi-Agent Communication](https://arxiv.org/abs/2510.19995)
*Yiming Lu,Xun Wang,Simin Ma,Shujian Liu,Sathish Reddy Indurthi,Song Wang,Haoyun Deng,Fei Liu,Kaiqiang Song*

Main category: cs.MA

TL;DR: 本文提出了Communication to Completion（C2C）框架，通过引入对齐因子（AF）和顺序行动机制，使多智能体系统能在复杂任务中进行高效、成本感知的通信，显著减少任务完成时间。


<details>
  <summary>Details</summary>
Motivation: 当前多智能体大语言模型系统缺乏面向任务的系统性通信框架，难以支持复杂工作场景中所需的多样化通信策略。

Method: 提出C2C框架，包含两个核心创新：(1) 对齐因子（AF），用于量化智能体任务对齐程度；(2) 顺序行动框架，将逐步执行与智能通信决策相结合，实现动态、目标导向的通信。

Result: 在三种复杂度等级和5至17个智能体的编码任务中，C2C相比无通信和固定步骤基线，任务完成时间减少约40%，通信成本可控，且在标准配置下成功完成所有任务，具备良好的可扩展性。

Conclusion: C2C不仅为多智能体系统中的通信有效性提供了理论度量基础，还构建了一个适用于复杂协作任务的实用通信框架。

Abstract: Teamwork in workspace for complex tasks requires diverse communication
strategies, but current multi-agent LLM systems lack systematic frameworks for
task oriented communication. We introduce Communication to Completion (C2C), a
scalable framework that addresses this gap through two key innovations: (1) the
Alignment Factor (AF), a novel metric quantifying agent task alignment that
directly impacts work efficiency, and (2) a Sequential Action Framework that
integrates stepwise execution with intelligent communication decisions. C2C
enables agents to make cost aware communication choices, dynamically improving
task understanding through targeted interactions. We evaluated C2C on realistic
coding workflows across three complexity tiers and team sizes from 5 to 17
agents, comparing against no communication and fixed steps baselines. The
results show that C2C reduces the task completion time by about 40% with
acceptable communication costs. The framework completes all tasks successfully
in standard configurations and maintains effectiveness at scale. C2C
establishes both a theoretical foundation for measuring communication
effectiveness in multi-agent systems and a practical framework for complex
collaborative tasks.

</details>


### [2] [Structures generated in a multiagent system performing information fusion in peer-to-peer resource-constrained networks](https://arxiv.org/abs/2510.20469)
*Horacio Paggi,Juan A. Lara,Javier Soriano*

Main category: cs.MA

TL;DR: 本文探讨了信息融合从传统层级结构向全息（holonic）协作结构的范式转变，指出在资源受限条件下，全息结构能通过多智能体系统有效应对不确定性与通信限制，具备适应性、自主性和协作性等优势。


<details>
  <summary>Details</summary>
Motivation: 传统信息融合采用静态层级结构，难以适应民用和边缘组织中动态、资源受限的环境；随着信息融合在非军事领域的扩展，需要更灵活的结构来应对不确定性、通信限制及系统组件失效等问题。

Method: 基于多智能体系统模型，研究在能量、消息数量、时间等资源受限条件下，完全互连的对等节点如何通过信息融合自发形成全息结构，并通过示例展示其运行机制。

Result: 研究表明，全息结构在资源受限环境中能够有效优化信息交换中的模糊性与不确定性，展现出对环境突变或系统组成变化的适应能力、一定程度的自主性以及协作达成共同目标的能力。

Conclusion: 全息信息融合结构相比传统层级结构更具灵活性和鲁棒性，特别适用于资源受限、动态变化的非军事应用场景，为未来信息融合系统设计提供了新范式。

Abstract: There has recently been a major advance with respect to how information
fusion is performed. Information fusion has gone from being conceived as a
purely hierarchical procedure, as is the case of traditional military
applications, to now being regarded collaboratively, as holonic fusion, which
is better suited for civil applications and edge organizations. The above
paradigm shift is being boosted as information fusion gains ground in different
non-military areas, and human-computer and machine-machine communications,
where holarchies, which are more flexible structures than ordinary, static
hierarchies, become more widespread. This paper focuses on showing how holonic
structures tend to be generated when there are constraints on resources
(energy, available messages, time, etc.) for interactions based on a set of
fully intercommunicating elements (peers) whose components fuse information as
a means of optimizing the impact of vagueness and uncertainty present message
exchanges. Holon formation is studied generically based on a multiagent system
model, and an example of its possible operation is shown. Holonic structures
have a series of advantages, such as adaptability, to sudden changes in the
environment or its composition, are somewhat autonomous and are capable of
cooperating in order to achieve a common goal. This can be useful when the
shortage of resources prevents communications or when the system components
start to fail.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [3] [HALOC-AxA: An Area/-Energy-Efficient Approximate Adder for Image Processing Application](https://arxiv.org/abs/2510.20137)
*Hasnain A. Ziad,Ashiq A. Sakib*

Main category: cs.AR

TL;DR: 本文提出了一种新型近似加法器，在能效和面积效率方面优于现有设计，同时在精度方面达到或优于现有水平，并通过图像处理任务验证了其高质量图像重建能力。


<details>
  <summary>Details</summary>
Motivation: 近似加法器被广泛研究以推动面向计算密集型多媒体应用（如图像、音频、视频处理）的高能效硬件设计，现有方案需在性能、计算精度和能效之间进行权衡。

Method: 提出一种新型近似加法器设计，通过仿真评估其能效、面积和精度，并将其应用于图像处理任务以验证图像重建质量。

Result: 仿真结果表明，所提出的加法器在能效和面积方面优于现有加法器，同时精度相当或更优；在图像处理任务中成功实现了高质量图像的数字重建。

Conclusion: 该新型近似加法器在能效、面积和精度方面具有综合优势，适用于计算密集型多媒体应用。

Abstract: The design of approximate adders has been widely researched to advance
energy-efficient hardware for computation-intensive multimedia applications,
such as image, audio, or video processing. The design of approximate adders has
been widely researched to advance energy-efficient hardware for computation
intensive multimedia applications, such as image/audio/video processing.
Several static and dynamic approximate adders exist in the literature, each of
which endeavors to balance the conflicting demands of high performance,
computational accuracy, and energy efficiency. This work introduces a novel
approximate adder that is more energy- and area-efficient than existing adders,
while achieving improved or comparable accuracy, as demonstrated by simulation
results. The proposed adder's ability to digitally reconstruct high quality
images is further demonstrated by the deployment of the design for an image
processing task.

</details>


### [4] [In-DRAM True Random Number Generation Using Simultaneous Multiple-Row Activation: An Experimental Study of Real DRAM Chips](https://arxiv.org/abs/2510.20269)
*Ismail Emir Yuksel,Ataberk Olgun,F. Nisa Bostanci,Oguzhan Canpolat,Geraldo F. Oliveira,Mohammad Sadrosadati,Abdullah Giray Yaglikci,Onur Mutlu*

Main category: cs.AR

TL;DR: 该论文通过在商用DRAM芯片中利用同时多行激活（SiMRA）技术，实现了高吞吐量、低延迟的真随机数生成，并通过96个DDR4芯片的广泛测试验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 探索在商用现成（COTS）DRAM芯片中利用SiMRA机制实现高效、可靠的真随机数生成（TRNG）的潜力，以满足对安全随机数日益增长的需求。

Method: 对96个DDR4 DRAM芯片进行广泛表征，系统分析不同同时激活行数（2、4、8、16、32）、数据模式、温度和空间变化对SiMRA真随机数生成的熵、延迟和吞吐量的影响，并使用NIST随机性测试套件评估生成质量。

Result: 1）所有SiMRA-based TRNG设计均通过NIST随机性测试；2）多行激活设计在吞吐量上优于现有DRAM-based TRNG；3）熵随同时激活行数增加而提升；4）操作参数（如数据模式、温度）显著影响熵，例如32行激活的平均熵是2行的2.51倍，而温度从50°C升至90°C会使32行激活的熵降低1.53倍。

Conclusion: SiMRA是一种在商用DRAM中实现高效、高质量真随机数生成的有效机制，其性能受多种操作条件影响，研究结果为未来基于DRAM的安全硬件设计提供了重要参考，并已开源相关基础设施。

Abstract: In this work, we experimentally demonstrate that it is possible to generate
true random numbers at high throughput and low latency in commercial
off-the-shelf (COTS) DRAM chips by leveraging simultaneous multiple-row
activation (SiMRA) via an extensive characterization of 96 DDR4 DRAM chips. We
rigorously analyze SiMRA's true random generation potential in terms of
entropy, latency, and throughput for varying numbers of simultaneously
activated DRAM rows (i.e., 2, 4, 8, 16, and 32), data patterns, temperature
levels, and spatial variations. Among our 11 key experimental observations, we
highlight four key results. First, we evaluate the quality of our TRNG designs
using the commonly-used NIST statistical test suite for randomness and find
that all SiMRA-based TRNG designs successfully pass each test. Second, 2-, 8-,
16-, and 32-row activation-based TRNG designs outperform the state-of-theart
DRAM-based TRNG in throughput by up to 1.15x, 1.99x, 1.82x, and 1.39x,
respectively. Third, SiMRA's entropy tends to increase with the number of
simultaneously activated DRAM rows. Fourth, operational parameters and
conditions (e.g., data pattern and temperature) significantly affect entropy.
For example, for most of the tested modules, the average entropy of 32-row
activation is 2.51x higher than that of 2-row activation. For example,
increasing the temperature from 50{\deg}C to 90{\deg}C decreases SiMRA's
entropy by 1.53x for 32-row activation. To aid future research and development,
we open-source our infrastructure at https://github.com/CMU-SAFARI/SiMRA-TRNG.

</details>


### [5] [Squire: A General-Purpose Accelerator to Exploit Fine-Grain Parallelism on Dependency-Bound Kernels](https://arxiv.org/abs/2510.20400)
*Rubén Langarita,Jesús Alastruey-Benedé,Pablo Ibáñez-Marín,Santiago Marco-Sola,Miquel Moretó,Adrià Armejach*

Main category: cs.AR

TL;DR: Squire 是一种新型通用加速器，通过集成低功耗有序核心并优化细粒度并行性，有效提升依赖密集型内核的性能与能效。


<details>
  <summary>Details</summary>
Motivation: 传统通用加速器（如SIMD、GPGPU）难以高效处理具有复杂数据依赖模式的计算密集型内核，而定制化FPGA/ASIC虽性能优越但缺乏灵活性且开发复杂。因此，亟需一种兼具通用性、高性能与能效的加速方案。

Method: 提出 Squire 加速器架构，每个加速器包含多个通用低功耗顺序核心，支持核心间快速通信并直接访问 L2 缓存；将其集成到典型多核系统中，每核心配一个 Squire 单元，以最小软件改动加速依赖密集型内核。

Result: 在五个复杂依赖模式内核上评估，Squire 在动态规划内核中最高实现 7.64 倍加速，端到端应用加速达 3.66 倍，能耗最多降低 56%，面积开销仅 10.5%（相比 Neoverse-N1 基线）。

Conclusion: Squire 能有效提升依赖密集型 HPC 应用的性能与能效，在保持通用性和低编程复杂度的同时，显著优于传统加速器方案。

Abstract: Multiple HPC applications are often bottlenecked by compute-intensive kernels
implementing complex dependency patterns (data-dependency bound). Traditional
general-purpose accelerators struggle to effectively exploit fine-grain
parallelism due to limitations in implementing convoluted data-dependency
patterns (like SIMD) and overheads due to synchronization and data transfers
(like GPGPUs). In contrast, custom FPGA and ASIC designs offer improved
performance and energy efficiency at a high cost in hardware design and
programming complexity and often lack the flexibility to process different
workloads. We propose Squire, a general-purpose accelerator designed to exploit
fine-grain parallelism effectively on dependency-bound kernels. Each Squire
accelerator has a set of general-purpose low-power in-order cores that can
rapidly communicate among themselves and directly access data from the L2
cache. Our proposal integrates one Squire accelerator per core in a typical
multicore system, allowing the acceleration of dependency-bound kernels within
parallel tasks with minimal software changes. As a case study, we evaluate
Squire's effectiveness by accelerating five kernels that implement complex
dependency patterns. We use three of these kernels to build an end-to-end
read-mapping tool that will be used to evaluate Squire. Squire obtains speedups
up to 7.64$\times$ in dynamic programming kernels. Overall, Squire provides an
acceleration for an end-to-end application of 3.66$\times$. In addition, Squire
reduces energy consumption by up to 56% with a minimal area overhead of 10.5%
compared to a Neoverse-N1 baseline.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [6] [New Hardness Results for the LOCAL Model via a Simple Self-Reduction](https://arxiv.org/abs/2510.19972)
*Alkida Balliu,Filippo Casagrande,Francesco d'Amore,Dennis Olivetti*

Main category: cs.DC

TL;DR: 本文简化了Khoury和Schild提出的“通过自归约的轮次消除”技术，并利用该简化方法证明了最大b-匹配和边着色问题在LOCAL模型中的新下界。


<details>
  <summary>Details</summary>
Motivation: Khoury和Schild关于最大匹配问题的下界证明技术虽具创新性，但过于复杂（超过25页），难以理解与推广。因此，作者旨在简化该技术，以促进对图问题复杂性的深入理解并拓展其应用。

Method: 作者提出了一种简化的“通过自归约的轮次消除”技术，并将其应用于分析最大b-匹配和边着色问题的随机LOCAL算法复杂度。

Result: 1. 给出了最大b-匹配问题的随机LOCAL算法下界：Ω(min{log_{1+b}Δ, log_Δ n}) 和 Ω(√log_{1+b} n)；  
2. 作为特例（b=1），得到了最大匹配下界的简洁证明；  
3. 证明了使用Δ + k种颜色进行边着色（k ≤ Δ^{1−ε}）的随机LOCAL算法下界为Ω(min{log Δ, log_Δ n}) 和 Ω(√log n)。

Conclusion: 通过简化现有技术，本文不仅重现了已有结果，还将其推广到更广泛的图问题，为未来研究提供了更清晰、易用的工具。

Abstract: Very recently, Khoury and Schild [FOCS 2025] showed that any randomized LOCAL
algorithm that solves maximal matching requires $\Omega(\min\{\log \Delta,
\log_\Delta n\})$ rounds, where $n$ is the number of nodes in the graph and
$\Delta$ is the maximum degree. This result is shown through a new technique,
called round elimination via self-reduction. The lower bound proof is beautiful
and presents very nice ideas. However, it spans more than 25 pages of technical
details, and hence it is hard to digest and generalize to other problems.
Historically, the simplification of proofs and techniques has marked an
important turning point in our understanding of the complexity of graph
problems. Our paper makes a step forward towards this direction, and provides
the following contributions.
  1. We present a short and simplified version of the round elimination via
self-reduction technique. The simplification of this technique enables us to
obtain the following two hardness results.
  2. We show that any randomized LOCAL algorithm that solves the maximal
$b$-matching problem requires $\Omega(\min\{\log_{1+b}\Delta, \log_\Delta n\})$
and $\Omega(\sqrt{\log_{1+b} n})$ rounds. We recall that the $b$-matching
problem is a generalization of the matching problem where each vertex can have
up to $b$ incident edges in the matching. As a corollary, for $b=1$, we obtain
a short proof for the maximal matching lower bound shown by Khoury and Schild.
  3. Finally, we show that any randomized LOCAL algorithm that properly colors
the edges of a graph with $\Delta + k$ colors requires $\Omega(\min\{\log
\Delta, \log_\Delta n\})$ and $\Omega(\sqrt{\log n})$ rounds, for any $k\le
\Delta^{1-\varepsilon}$ and any constant $\varepsilon > 0$.

</details>


### [7] [AsyncHZP: Hierarchical ZeRO Parallelism with Asynchronous Scheduling for Scalable LLM Training](https://arxiv.org/abs/2510.20111)
*Huawei Bai,Yifan Huang,Wenqi Shi,Ansheng You,Feifan Shao,Tengfei Han,Minghui Yu*

Main category: cs.DC

TL;DR: 本文提出了一种名为异步分层零并行（AsyncHZP）的新方法，通过自适应重分片和多流异步调度，在保持内存效率和实现简单的同时，显著降低了通信开销，提升了大规模语言模型训练的效率和可扩展性。


<details>
  <summary>Details</summary>
Motivation: 当前语言模型在大规模集群上的训练效率和可扩展性受限，主流的ND并行方法复杂笨重，而ZeRO等灵活方法又受限于通信开销。

Method: AsyncHZP是一种ZeRO的异步变体，通过在不同副本组间自适应地重分片参数、梯度和优化器状态，并采用多流异步调度机制，在后台线程中执行通信操作，实现通信与计算的重叠。

Result: 在Dense和MoE模型上的实验表明，AsyncHZP在大规模训练中保持稳定，性能优于经典ND并行方法，且无需复杂的策略调优。

Conclusion: AsyncHZP在简化大规模训练的同时实现了领先的性能，为高效训练大型语言模型提供了一条更简洁的路径。

Abstract: The training efficiency and scalability of language models on massive
clusters currently remain a critical bottleneck. Mainstream approaches like ND
parallelism are often cumbersome and complex, while flexible alternatives such
as the Zero Redundancy Optimizer (ZeRO) are frequently hampered by
communication overhead. In this paper, we propose Asynchronous Hierarchical
Zero Parallelism (AsyncHZP), a novel asynchronous variant of ZeRO designed to
achieve superior performance while maintaining simplicity and memory
efficiency. Unlike traditional ZeRO, which employs over-fine-grained sharding
that can lead to inefficient communication, AsyncHZP adaptively reshards
parameters, gradients, and optimizer states across different replica groups.
This strategy optimizes device memory utilization and significantly reduces
communication overhead. In addition, we also design a multi-stream asynchronous
scheduling method that executes parameter all-gather and gradient
reduce-scatter operations in dedicated background threads, effectively
overlapping communication with computation while incurring negligible memory
fragmentation. Empirical evaluations on both Dense and Mixture-of-Experts (MoE)
models confirm that AsyncHZP maintains robust stability at scale. It
consistently outperforms classic ND parallelism, achieving state-of-the-art
performance without complex strategic tuning, thereby simplifying the path to
efficient large-scale training.

</details>


### [8] [A Full Stack Framework for High Performance Quantum-Classical Computing](https://arxiv.org/abs/2510.20128)
*Xin Zhan,K. Grace Johnson,Aniello Esposito,Barbara Chapman,Marco Fiorentino,Kirk M. Bresniker,Raymond G. Beausoleil,Masoud Mohseni*

Main category: cs.DC

TL;DR: 本文提出了一种HPC-QC全栈框架，支持在经典高性能计算环境中集成量子计算，实现跨硬件的混合工作负载开发，并展示了多节点CPU/GPU上的实际应用案例。


<details>
  <summary>Details</summary>
Motivation: 为应对高性能计算（HPC）与量子计算（QC）日益增长的融合需求，亟需一种可扩展、模块化且与硬件无关的软件集成方案，以支持在现有HPC环境中高效开发和执行混合量子-经典工作负载。

Method: 开发了一个HPC-QC全栈框架，包括：1）支持C/C++、Fortran和Python调用商业量子SDK的量子编程接口库；2）用于分割大型量子电路的自适应电路编织虚拟机；3）基于Cray LLVM的编译框架，可将量子中间表示（QIR）和LLVM IR重定向至不同硬件架构。

Result: 在HPE EX超级计算机上成功演示了多个混合HPC-QC工作负载（如线性方程组求解、量子优化和量子相变模拟），验证了所开发三大组件的功能性和执行可行性。

Conclusion: 该工作构建了一个基于经典HPC软件栈（编译器、库、并行运行时和进程调度）的统一量子-经典编程环境，为未来可扩展的混合计算提供了基础框架。

Abstract: To address the growing needs for scalable High Performance Computing (HPC)
and Quantum Computing (QC) integration, we present our HPC-QC full stack
framework and its hybrid workload development capability with modular
hardware/device-agnostic software integration approach. The latest development
in extensible interfaces for quantum programming, dispatching, and compilation
within existing mature HPC programming environment are demonstrated. Our HPC-QC
full stack enables high-level, portable invocation of quantum kernels from
commercial quantum SDKs within HPC meta-program in compiled languages (C/C++
and Fortran) as well as Python through a quantum programming interface library
extension. An adaptive circuit knitting hypervisor is being developed to
partition large quantum circuits into sub-circuits that fit on smaller noisy
quantum devices and classical simulators. At the lower-level, we leverage Cray
LLVM-based compilation framework to transform and consume LLVM IR and Quantum
IR (QIR) from commercial quantum software frontends in a retargetable fashion
to different hardware architectures. Several hybrid HPC-QC multi-node multi-CPU
and GPU workloads (including solving linear system of equations, quantum
optimization, and simulating quantum phase transitions) have been demonstrated
on HPE EX supercomputers to illustrate functionality and execution viability
for all three components developed so far. This work provides the framework for
a unified quantum-classical programming environment built upon classical HPC
software stack (compilers, libraries, parallel runtime and process scheduling).

</details>


### [9] [Collective Communication for 100k+ GPUs](https://arxiv.org/abs/2510.20171)
*Min Si,Pavan Balaji,Yongzhou Chen,Ching-Hsiang Chu,Adi Gangidi,Saif Hasan,Subodh Iyengar,Dan Johnson,Bingzhe Liu,Jingliang Ren,Ashmitha Jeevaraj Shetty,Greg Steinbrecher,Xinfeng Xie,Yulun Wang,Bruce Wu,Jingyi Yang,Mingran Yang,Minlan Yu,Cen Zhao,Wes Bland,Denis Boyda,Suman Gumudavelli,Cristian Lumezanu,Rui Miao,Zhe Qu,Venkat Ramesh,Maxim Samoylov,Jan Seidel,Feng Tian,Qiye Tan,Shuqiang Zhang,Yimeng Zhao,Shengbao Zheng,Art Zhu,Hongyi Zeng*

Main category: cs.DC

TL;DR: 本文提出了Meta开发的NCCLX集体通信框架，旨在优化大规模语言模型（LLM）在训练和推理全生命周期中的通信效率，支持超过10万GPU的集群，并在Llama4模型上验证了其显著性能提升。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型规模的不断扩大，训练任务已扩展至数十万GPU，传统通信方法在吞吐量和延迟方面面临严重瓶颈，制约了先进模型的开发与部署。

Method: 提出并实现了NCCLX集体通信框架，专为大规模GPU集群设计，兼顾训练阶段的同步通信需求与推理阶段的低延迟要求。

Result: 在Llama4模型上的实验证明，NCCLX显著提升了通信效率，支持高吞吐、低延迟且可靠的数据交换。

Conclusion: NCCLX为下一代超大规模语言模型提供了一个高效、可靠的通信解决方案，使其能够在前所未有的规模上运行。

Abstract: The increasing scale of large language models (LLMs) necessitates highly
efficient collective communication frameworks, particularly as training
workloads extend to hundreds of thousands of GPUs. Traditional communication
methods face significant throughput and latency limitations at this scale,
hindering both the development and deployment of state-of-the-art models. This
paper presents the NCCLX collective communication framework, developed at Meta,
engineered to optimize performance across the full LLM lifecycle, from the
synchronous demands of large-scale training to the low-latency requirements of
inference. The framework is designed to support complex workloads on clusters
exceeding 100,000 GPUs, ensuring reliable, high-throughput, and low-latency
data exchange. Empirical evaluation on the Llama4 model demonstrates
substantial improvements in communication efficiency. This research contributes
a robust solution for enabling the next generation of LLMs to operate at
unprecedented scales.

</details>


### [10] [FLAS: a combination of proactive and reactive auto-scaling architecture for distributed services](https://arxiv.org/abs/2510.20388)
*Víctor Rampérez,Javier Soriano,David Lizcano,Juan A. Lara*

Main category: cs.DC

TL;DR: 本文提出了一种名为FLAS的自动扩缩容系统，结合了预测性和反应性策略，用于内容分发型分布式服务，能以超过99%的可靠性满足性能需求。


<details>
  <summary>Details</summary>
Motivation: 云平台的弹性依赖于自动扩缩容系统，但现有方案往往仅采用预测性或反应性方法，难以兼顾准确性和适应性。作者旨在设计一种能融合两者优势、适用于内容分发中间件等分布式系统的通用自动扩缩容机制。

Method: FLAS引入两个核心机制：(i) 一种高层指标趋势预测模型，用于提前预判SLA相关参数（如响应时间、吞吐量）的变化；(ii) 一种基于资源使用指标估算高层指标的反应式应急系统，减少侵入性并提升对不同应用的适配能力。作者在E-SilboPS中间件上实现了FLAS，并采用边界值分析法设计测试用例进行评估。

Result: 在多种测试场景（包括最坏情况）下，FLAS能够确保服务性能需求在99%以上的时间内得到满足，验证了其有效性。

Conclusion: FLAS是首个面向内容分发型发布/订阅系统的自动扩缩容方案，兼具预测与反应能力，具有通用性，能高效保障SLA。

Abstract: Cloud computing has established itself as the support for the vast majority
of emerging technologies, mainly due to the characteristic of elasticity it
offers. Auto-scalers are the systems that enable this elasticity by acquiring
and releasing resources on demand to ensure an agreed service level. In this
article we present FLAS (Forecasted Load Auto-Scaling), an auto-scaler for
distributed services that combines the advantages of proactive and reactive
approaches according to the situation to decide the optimal scaling actions in
every moment. The main novelties introduced by FLAS are (i) a predictive model
of the high-level metrics trend which allows to anticipate changes in the
relevant SLA parameters (e.g. performance metrics such as response time or
throughput) and (ii) a reactive contingency system based on the estimation of
high-level metrics from resource use metrics, reducing the necessary
instrumentation (less invasive) and allowing it to be adapted agnostically to
different applications. We provide a FLAS implementation for the use case of a
content-based publish-subscribe middleware (E-SilboPS) that is the cornerstone
of an event-driven architecture. To the best of our knowledge, this is the
first auto-scaling system for content-based publish-subscribe distributed
systems (although it is generic enough to fit any distributed service). Through
an evaluation based on several test cases recreating not only the expected
contexts of use, but also the worst possible scenarios (following the
Boundary-Value Analysis or BVA test methodology), we have validated our
approach and demonstrated the effectiveness of our solution by ensuring
compliance with performance requirements over 99% of the time.

</details>


### [11] [Accurate Performance Predictors for Edge Computing Applications](https://arxiv.org/abs/2510.20495)
*Panagiotis Giannakopoulos,Bart van Knippenberg,Kishor Chandra Joshi,Nicola Calabretta,George Exarchakos*

Main category: cs.DC

TL;DR: 本文提出一种自动构建和评估性能预测器的方法，通过联合优化准确率与推理延迟，在动态边缘环境中实现高达90%的预测准确率，且推理时间不到往返时间的1%。


<details>
  <summary>Details</summary>
Motivation: 在资源受限且动态变化的边缘环境中，由于多应用共置和节点异构性，实现可预测的应用性能极具挑战，因此需要一种兼顾准确性和推理效率的性能预测方法。

Method: 自动构建并评估多种性能预测器，基于与应用性能最相关的监控指标历史数据进行训练，并在多个服务器的动态共置场景中进行评估，优先选择在准确率和推理时间之间取得最佳平衡的模型。

Result: 所提出的预测器在电子显微镜工作流等严苛实时场景中实现了高达90%的准确率，同时推理时间低于往返时间的1%。

Conclusion: 在动态共置的边缘环境中，应采用系统化方法为每台服务器选择兼顾准确率与推理延迟的专用预测器，以提升资源利用率并实现可预测的性能。

Abstract: Accurate prediction of application performance is critical for enabling
effective scheduling and resource management in resource-constrained dynamic
edge environments. However, achieving predictable performance in such
environments remains challenging due to the co-location of multiple
applications and the node heterogeneity. To address this, we propose a
methodology that automatically builds and assesses various performance
predictors. This approach prioritizes both accuracy and inference time to
identify the most efficient model. Our predictors achieve up to 90% accuracy
while maintaining an inference time of less than 1% of the Round Trip Time.
These predictors are trained on the historical state of the most correlated
monitoring metrics to application performance and evaluated across multiple
servers in dynamic co-location scenarios. As usecase we consider electron
microscopy (EM) workflows, which have stringent real-time demands and diverse
resource requirements. Our findings emphasize the need for a systematic
methodology that selects server-specific predictors by jointly optimizing
accuracy and inference latency in dynamic co-location scenarios. Integrating
such predictors into edge environments can improve resource utilization and
result in predictable performance.

</details>


### [12] [Morpheus: Lightweight RTT Prediction for Performance-Aware Load Balancing](https://arxiv.org/abs/2510.20506)
*Panagiotis Giannakopoulos,Bart van Knippenberg,Kishor Chandra Joshi,Nicola Calabretta,George Exarchakos*

Main category: cs.DC

TL;DR: 本文提出一种基于往返时间（RTT）预测器的预测性负载均衡方法，通过轻量级模型利用少量高相关性监控指标，在Kubernetes管理的GPU集群中实现高精度、低开销的请求路由，显著降低应用RTT并减少资源浪费。


<details>
  <summary>Details</summary>
Motivation: 传统负载均衡策略通常依赖过时或粗粒度的指标，导致次优路由决策和尾部延迟增加，难以满足边缘和云环境中对低端到端延迟的需求。

Method: 开发轻量且准确的RTT预测器，基于从Kubernetes GPU集群收集的时间序列监控数据进行训练，仅使用少量高相关性指标以保持低开销，并适应多样化的共置场景和异构硬件。

Result: 预测器准确率高达95%，预测延迟控制在应用RTT的10%以内；仿真评估表明，该方法能显著降低应用RTT并减少资源浪费。

Conclusion: 预测性负载均衡在资源受限集群中具有可行性，未来可集成到生产系统中以提升性能与资源效率。

Abstract: Distributed applications increasingly demand low end-to-end latency,
especially in edge and cloud environments where co-located workloads contend
for limited resources. Traditional load-balancing strategies are typically
reactive and rely on outdated or coarse-grained metrics, often leading to
suboptimal routing decisions and increased tail latencies. This paper
investigates the use of round-trip time (RTT) predictors to enhance request
routing by anticipating application latency. We develop lightweight and
accurate RTT predictors that are trained on time-series monitoring data
collected from a Kubernetes-managed GPU cluster. By leveraging a reduced set of
highly correlated monitoring metrics, our approach maintains low overhead while
remaining adaptable to diverse co-location scenarios and heterogeneous
hardware. The predictors achieve up to 95% accuracy while keeping the
prediction delay within 10% of the application RTT. In addition, we identify
the minimum prediction accuracy threshold and key system-level factors required
to ensure effective predictor deployment in resource-constrained clusters.
Simulation-based evaluation demonstrates that performance-aware load balancing
can significantly reduce application RTT and minimize resource waste. These
results highlight the feasibility of integrating predictive load balancing into
future production systems.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [13] [E-Test: E'er-Improving Test Suites](https://arxiv.org/abs/2510.19860)
*Ketai Qiu,Luca Di Grazia,Leonardo Mariani,Mauro Pezzè*

Main category: cs.SE

TL;DR: E-Test 利用大语言模型（LLMs）从生产环境中识别尚未被现有测试套件覆盖的执行场景，并自动生成新的测试用例以增强测试套件，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有测试套件难以覆盖所有执行场景，尤其在长期维护大型测试套件时，手动发现并补充未覆盖场景成本高昂且极具挑战性。

Method: E-Test 结合生产环境监控数据与大语言模型，识别当前测试套件未覆盖的执行场景，并生成对应的新测试用例以扩充测试套件。

Result: 在包含1,975个场景的数据集上评估，E-Test 的 F1 分数达到 0.55，明显优于现有回归测试、现场测试方法（最高 0.34）和普通 LLM 方法（最高 0.39）。

Conclusion: E-Test 能有效识别并覆盖未测试的执行场景，显著提升测试套件质量，同时减少人工维护成本。

Abstract: Test suites are inherently imperfect, and testers can always enrich a suite
with new test cases that improve its quality and, consequently, the reliability
of the target software system. However, finding test cases that explore
execution scenarios beyond the scope of an existing suite can be extremely
challenging and labor-intensive, particularly when managing large test suites
over extended periods.
  In this paper, we propose E-Test, an approach that reduces the gap between
the execution space explored with a test suite and the executions experienced
after testing by augmenting the test suite with test cases that explore
execution scenarios that emerge in production. E-Test (i) identifies executions
that have not yet been tested from large sets of scenarios, such as those
monitored during intensive production usage, and (ii) generates new test cases
that enhance the test suite. E-Test leverages Large Language Models (LLMs) to
pinpoint scenarios that the current test suite does not adequately cover, and
augments the suite with test cases that execute these scenarios.
  Our evaluation on a dataset of 1,975 scenarios, collected from highly-starred
open-source Java projects already in production and Defects4J, demonstrates
that E-Test retrieves not-yet-tested execution scenarios significantly better
than state-of-the-art approaches. While existing regression testing and field
testing approaches for this task achieve a maximum F1-score of 0.34, and
vanilla LLMs achieve a maximum F1-score of 0.39, E-Test reaches 0.55. These
results highlight the impact of E-Test in enhancing test suites by effectively
targeting not-yet-tested execution scenarios and reducing manual effort
required for maintaining test suites.

</details>


### [14] [SODBench: A Large Language Model Approach to Documenting Spreadsheet Operations](https://arxiv.org/abs/2510.19864)
*Amila Indika,Igor Molybog*

Main category: cs.SE

TL;DR: 本文提出了一种名为“电子表格操作文档”（SOD）的新任务，旨在利用大语言模型（LLMs）将电子表格操作代码自动转化为人类可读的自然语言说明，并构建了一个包含111个样本的基准数据集，评估了五种主流LLM在该任务上的表现，结果表明LLMs具备生成准确文档的能力，有助于提升电子表格的可复现性与协作效率。


<details>
  <summary>Details</summary>
Motivation: 电子表格在商业、会计和金融等领域被广泛使用，但由于缺乏系统化的文档方法，导致自动化、协作和知识传承困难，存在关键机构知识丢失的风险。因此，亟需一种能将电子表格操作自动转化为可读文档的方法。

Method: 作者构建了一个包含111个电子表格操作代码片段及其对应自然语言摘要的基准数据集，并使用BLEU、GLEU、ROUGE-L和METEOR等指标对GPT-4o、GPT-4o-mini、LLaMA-3.3-70B、Mixtral-8x7B和Gemma2-9B五种大语言模型在SOD任务上的表现进行了评估。

Result: 实验结果表明，当前的大语言模型能够生成较为准确的电子表格操作文档，在SOD任务上展现出可行性，为提升电子表格的可维护性和协作效率提供了基础，但仍存在一些挑战需要解决。

Conclusion: SOD任务是可行且有价值的，大语言模型在生成电子表格操作文档方面具有潜力，可作为提升电子表格工作流可复现性、可维护性和协作性的关键前置步骤。

Abstract: Numerous knowledge workers utilize spreadsheets in business, accounting, and
finance. However, a lack of systematic documentation methods for spreadsheets
hinders automation, collaboration, and knowledge transfer, which risks the loss
of crucial institutional knowledge. This paper introduces Spreadsheet
Operations Documentation (SOD), an AI task that involves generating
human-readable explanations from spreadsheet operations. Many previous studies
have utilized Large Language Models (LLMs) for generating spreadsheet
manipulation code; however, translating that code into natural language for SOD
is a less-explored area. To address this, we present a benchmark of 111
spreadsheet manipulation code snippets, each paired with a corresponding
natural language summary. We evaluate five LLMs, GPT-4o, GPT-4o-mini,
LLaMA-3.3-70B, Mixtral-8x7B, and Gemma2-9B, using BLEU, GLEU, ROUGE-L, and
METEOR metrics. Our findings suggest that LLMs can generate accurate
spreadsheet documentation, making SOD a feasible prerequisite step toward
enhancing reproducibility, maintainability, and collaborative workflows in
spreadsheets, although there are challenges that need to be addressed.

</details>


### [15] [Knowledge-Guided Multi-Agent Framework for Application-Level Software Code Generation](https://arxiv.org/abs/2510.19868)
*Qian Xiong,Bo Yang,Weisong Sun,Yiran Zhang,Tianlin Li,Yang Liu,Zhi Jin*

Main category: cs.SE

TL;DR: 本文提出了一种名为KGACG的知识引导型多智能体框架，用于自动生成应用级软件代码，通过规划、编码与测试智能体的闭环协作，将需求与架构文档转化为可执行代码。


<details>
  <summary>Details</summary>
Motivation: 现有大模型驱动的代码生成方法在复杂应用级软件开发中难以保证项目代码的合理组织结构，且代码生成过程不易维护，因此需要一种更有效的多智能体协作框架。

Method: 提出KGACG框架，整合代码组织与规划智能体（COPA）、编码智能体（CA）和测试智能体（TA），结合反馈机制，形成闭环协作流程，将软件需求与架构设计文档转化为可执行代码。

Result: 在Java坦克大战游戏案例中展示了KGACG框架中各智能体的协作过程，并验证了其在应对应用级代码生成挑战方面的潜力。

Conclusion: KGACG框架有望推动应用级软件开发的自动化，提升复杂软件系统的代码生成质量与可维护性。

Abstract: Automated code generation driven by Large Lan- guage Models (LLMs) has
enhanced development efficiency, yet generating complex application-level
software code remains challenging. Multi-agent frameworks show potential, but
existing methods perform inadequately in large-scale application-level software
code generation, failing to ensure reasonable orga- nizational structures of
project code and making it difficult to maintain the code generation process.
To address this, this paper envisions a Knowledge-Guided Application-Level Code
Generation framework named KGACG, which aims to trans- form software
requirements specification and architectural design document into executable
code through a collaborative closed- loop of the Code Organization & Planning
Agent (COPA), Coding Agent (CA), and Testing Agent (TA), combined with a
feedback mechanism. We demonstrate the collaborative process of the agents in
KGACG in a Java Tank Battle game case study while facing challenges. KGACG is
dedicated to advancing the automation of application-level software
development.

</details>


### [16] [BugPilot: Complex Bug Generation for Efficient Learning of SWE Skills](https://arxiv.org/abs/2510.19898)
*Atharv Sonwane,Isadora White,Hyunji Lee,Matheus Pereira,Lucas Caccia,Minseon Kim,Zhengyan Shi,Chinmay Singh,Alessandro Sordoni,Marc-Alexandre Côté,Xingdi Yuan*

Main category: cs.SE

TL;DR: 本文提出了一种新颖的合成生成高质量、多样化软件缺陷的方法，通过让软件工程（SWE）智能体在尝试添加新功能时无意引入缺陷，从而更真实地模拟人类开发过程。实验表明，该方法生成的缺陷数据在监督微调中效率更高，并用于训练出当前SWE-bench Verified榜单上表现领先的FrogBoss（32B）和FrogMini（14B）模型。


<details>
  <summary>Details</summary>
Motivation: 现有合成缺陷生成方法通常通过有意引入局部扰动等方式生成缺陷，导致分布外（out-of-distribution）问题，无法真实反映实际开发中人类引入缺陷的模式。因此，需要一种更贴近真实开发流程的高质量缺陷生成方法，以提升语言模型在软件工程任务上的训练效果。

Method: 作者提出让SWE智能体在代码库中尝试添加新功能，过程中可能无意破坏已有测试，从而自然产生缺陷。该方法避免了人为刻意引入缺陷，使生成的缺陷更贴近人类开发者的行为模式。

Result: 定性分析表明该方法生成的缺陷更接近人类编写的编辑模式；定量实验显示，使用该方法生成的缺陷数据进行监督微调，仅用1.2k个缺陷（约为其他数据集3k的一半）即可获得高出2%的性能。基于此方法训练的FrogBoss（32B）和FrogMini（14B）模型在SWE-bench Verified上分别达到54.6%和45.3%的pass@1，均为当前最优。

Conclusion: 通过模拟真实开发中无意引入缺陷的过程，本文方法能高效生成高质量、多样化的训练数据，显著提升SWE智能体的性能，为未来语言模型在软件工程领域的训练提供了更有效的数据生成范式。

Abstract: High quality bugs are key to training the next generation of language model
based software engineering (SWE) agents. We introduce a novel method for
synthetic generation of difficult and diverse bugs. Our method instructs SWE
Agents to introduce a feature into the codebase whereby they may
unintentionally break tests, resulting in bugs. Prior approaches often induce
an out-of-distribution effect by generating bugs intentionally (e.g. by
introducing local perturbation to existing code), which does not reflect
realistic development processes. We perform qualitative analysis to demonstrate
that our approach for generating bugs more closely reflects the patterns found
in human-authored edits. Through extensive experiments, we demonstrate that our
bugs provide more efficient training data for supervised fine-tuning,
outperforming other bug datasets by 2% with half the training data (1.2k vs. 3k
bugs). We train on our newly generated bugs in addition to existing bug
datasets to get FrogBoss a state-of-the-art 32B parameter model on SWE-bench
Verified with a pass@1 of 54.6% and FrogMini a state-of-the-art 14B model on
SWE-bench Verified with a pass@1 of 45.3% on SWE-bench Verified averaged over
three seeds.

</details>


### [17] [On Interaction Effects in Greybox Fuzzing](https://arxiv.org/abs/2510.19984)
*Konstantinos Kitsios,Marcel Böhme,Alberto Bacchelli*

Main category: cs.SE

TL;DR: 本文提出MuoFuzz，一种新型灰盒模糊测试工具，通过学习并选择最有效的变异算子序列来提升测试效率。相比AFL++和MOPT，MuoFuzz在FuzzBench和MAGMA基准测试中实现了更高的代码覆盖率，并发现了多个被其他工具遗漏的漏洞。


<details>
  <summary>Details</summary>
Motivation: 现有灰盒模糊测试工具通常随机或独立地选择变异算子，忽略了变异算子顺序对测试效果的影响。作者假设变异算子的应用顺序会影响模糊测试的有效性，并希望通过建模这种顺序关系来提升模糊测试性能。

Method: MuoFuzz通过学习给定前一个变异算子条件下，下一个变异算子产生有效输入的条件概率，并利用随机游走从所学概率分布中采样生成变异序列。该方法考虑了变异算子之间的交互效应，而非独立优化每个算子的选择概率。

Result: 在FuzzBench和MAGMA基准测试中，MuoFuzz相比AFL++和MOPT取得了更高的代码覆盖率，并额外发现了4个AFL++未发现的漏洞，以及1个AFL++和MOPT均未发现的漏洞。

Conclusion: 变异算子的顺序对灰盒模糊测试效果有显著影响。MuoFuzz通过建模并利用这种顺序依赖关系，有效提升了模糊测试的效率和漏洞发现能力，优于现有主流工具。

Abstract: A greybox fuzzer is an automated software testing tool that generates new
test inputs by applying randomly chosen mutators (e.g., flipping a bit or
deleting a block of bytes) to a seed input in random order and adds all
coverage-increasing inputs to the corpus of seeds. We hypothesize that the
order in which mutators are applied to a seed input has an impact on the
effectiveness of greybox fuzzers. In our experiments, we fit a linear model to
a dataset that contains the effectiveness of all possible mutator pairs and
indeed observe the conjectured interaction effect. This points us to more
efficient fuzzing by choosing the most promising mutator sequence with a higher
likelihood. We propose MuoFuzz, a greybox fuzzer that learns and chooses the
most promising mutator sequences. MuoFuzz learns the conditional probability
that the next mutator will yield an interesting input, given the previously
selected mutator. Then, it samples from the learned probability using a random
walk to generate mutator sequences. We compare the performance of MuoFuzz to
AFL++, which uses a fixed selection probability, and MOPT, which optimizes the
selection probability of each mutator in isolation. Experimental results on the
FuzzBench and MAGMA benchmarks show that MuoFuzz achieves the highest code
coverage and finds four bugs missed by AFL++ and one missed by both AFL++ and
MOPT.

</details>


### [18] [Symmetry in Software Platforms as an Architectural Principle](https://arxiv.org/abs/2510.20389)
*Bjorn Remseth*

Main category: cs.SE

TL;DR: 该论文探讨了软件平台通过保持结构不变性（即对称性）来实现架构鲁棒性的机制。


<details>
  <summary>Details</summary>
Motivation: 软件平台通常作为结构保持系统，其接口和行为在特定变换下保持稳定；作者旨在研究这种结构性规律如何促成架构的鲁棒性。

Method: 通过分析软件平台中的对称性（即特定变换下的结构不变性）来研究架构鲁棒性的来源。

Result: 表明架构鲁棒性源于对结构性规律（对称性）的强制执行。

Conclusion: 强制执行结构上的对称性是实现软件架构鲁棒性的关键因素。

Abstract: Software platforms often act as structure preserving systems. They provide
consistent interfaces and behaviors that remain stable under specific
transformations that we denote as symmetries. This paper explores the idea that
architectural robustness emerges from enforcing such structural regularities

</details>


### [19] [A Framework for the Adoption and Integration of Generative AI in Midsize Organizations and Enterprises (FAIGMOE)](https://arxiv.org/abs/2510.19997)
*Abraham Itzhak Weinberg*

Main category: cs.SE

TL;DR: 本文提出了FAIGMOE框架，旨在解决中型组织和大型企业在生成式人工智能（GenAI）采纳过程中面临的独特挑战，提供分阶段、可扩展的实施指导。


<details>
  <summary>Details</summary>
Motivation: 现有技术采纳理论（如TAM、TOE、DOI）缺乏针对GenAI在不同规模组织中实施的具体指导，无法有效应对中型组织的资源与专业限制以及大型企业的复杂协调问题，因此亟需一个专门的框架填补这一研究空白。

Method: 作者整合技术采纳理论、组织变革管理和创新扩散视角，构建了一个名为FAIGMOE的概念框架，包含四个相互关联的阶段：战略评估、规划与用例开发、实施与集成、运营化与优化，并嵌入GenAI特有的考量因素。

Result: FAIGMOE为不同规模组织提供了可扩展的GenAI采纳路径，涵盖准备度评估、战略对齐、风险治理、技术架构和变革管理，并首次系统整合了提示工程、模型编排和幻觉管理等GenAI特有要素。

Conclusion: FAIGMOE是首个专门面向中型和大型组织的GenAI采纳综合概念框架，提供了可操作的实施协议、评估工具和治理模板，但需通过后续实证研究加以验证。

Abstract: Generative Artificial Intelligence (GenAI) presents transformative
opportunities for organizations, yet both midsize organizations and larger
enterprises face distinctive adoption challenges. Midsize organizations
encounter resource constraints and limited AI expertise, while enterprises
struggle with organizational complexity and coordination challenges. Existing
technology adoption frameworks, including TAM (Technology Acceptance Model),
TOE (Technology Organization Environment), and DOI (Diffusion of Innovations)
theory, lack the specificity required for GenAI implementation across these
diverse contexts, creating a critical gap in adoption literature. This paper
introduces FAIGMOE (Framework for the Adoption and Integration of Generative AI
in Midsize Organizations and Enterprises), a conceptual framework addressing
the unique needs of both organizational types. FAIGMOE synthesizes technology
adoption theory, organizational change management, and innovation diffusion
perspectives into four interconnected phases: Strategic Assessment, Planning
and Use Case Development, Implementation and Integration, and
Operationalization and Optimization. Each phase provides scalable guidance on
readiness assessment, strategic alignment, risk governance, technical
architecture, and change management adaptable to organizational scale and
complexity. The framework incorporates GenAI specific considerations including
prompt engineering, model orchestration, and hallucination management that
distinguish it from generic technology adoption frameworks. As a perspective
contribution, FAIGMOE provides the first comprehensive conceptual framework
explicitly addressing GenAI adoption across midsize and enterprise
organizations, offering actionable implementation protocols, assessment
instruments, and governance templates requiring empirical validation through
future research.

</details>


### [20] [The Cost of Downgrading Build Systems: A Case Study of Kubernetes](https://arxiv.org/abs/2510.20041)
*Gareema Ranjan,Mahmoud Alfadel,Gengyi Sun,Shane McIntosh*

Main category: cs.SE

TL;DR: 本文通过研究Kubernetes项目从Bazel（基于产物的构建工具）降级到Go Build（语言特定构建工具）的案例，发现尽管Bazel在构建速度上更快，但其内存占用和CPU负载更高；降级虽提升了可维护性，却带来了显著的性能代价，尤其在大型项目中。作者进一步在四个其他项目中复现该研究，验证了Bazel普遍具有更高内存消耗的特性。


<details>
  <summary>Details</summary>
Motivation: 开发者频繁使用构建系统，其性能直接影响生产力。尽管现代基于产物的构建工具（如Bazel）能加速构建，但团队有时会因可维护性问题放弃它们。然而，这种“降级”行为带来的性能影响尚未被充分研究。

Method: 作者对Kubernetes项目在从Bazel降级到Go Build期间的完整构建和增量构建进行了复现与分析，并扩展到另外四个同样从Bazel降级的项目，比较构建时间、内存占用和CPU负载等指标。

Result: Bazel构建速度更快（完整构建快23.06–75.19秒），但内存占用显著更高（81.42–351.07 MB），且在高并行度下CPU负载更大；降级可能导致CI资源成本增加高达76%；在其他项目中也观察到Bazel始终消耗更多内存。

Conclusion: 尽管放弃基于产物的构建工具可能带来可维护性优势，但对于大型项目而言，通常会带来显著的性能代价。研究结果有助于项目决策者在构建工具选型时权衡利弊。

Abstract: Since developers invoke the build system frequently, its performance can
impact productivity. Modern artifact-based build tools accelerate builds, yet
prior work shows that teams may abandon them for alternatives that are easier
to maintain. While prior work shows why downgrades are performed, the
implications of downgrades remain largely unexplored. In this paper, we
describe a case study of the Kubernetes project, focusing on its downgrade from
an artifact-based build tool (Bazel) to a language-specific solution (Go
Build). We reproduce and analyze the full and incremental builds of change sets
during the downgrade period. On the one hand, we find that Bazel builds are
faster than Go Build, completing full builds in 23.06-38.66 up to 75.19 impose
a larger memory footprint than Go Build of 81.42-351.07 respectively. Bazel
builds also impose a greater CPU load at parallelism settings above eight for
full builds and above one for incremental builds. We estimate that downgrading
from Bazel can increase CI resource costs by up to 76 explore whether our
observations generalize by replicating our Kubernetes study on four other
projects that also downgraded from Bazel to older build tools. We observe that
while build time penalties decrease, Bazel consistently consumes more memory.
We conclude that abandoning artifact-based build tools, despite perceived
maintainability benefits, tends to incur considerable performance costs for
large projects. Our observations may help stakeholders to balance trade-offs in
build tool adoption

</details>


### [21] [Developing a Model-Driven Reengineering Approach for Migrating PL/SQL Triggers to Java: A Practical Experience](https://arxiv.org/abs/2510.20121)
*Carlos J. Fernandez-Candel,Jesus Garcia-Molina,Francisco Javier Bermudez Ruiz,Jose Ramon Hoyos Barcelo,Diego Sevilla Ruiz,Benito Jose Cuesta Viera*

Main category: cs.SE

TL;DR: 本文提出了一种基于模型驱动的重构方法，用于将Oracle Forms中的PL/SQL单体代码迁移至分层的Java代码，并结合类TDD的方式逐步开发模型转换，同时对生成代码进行三类验证。


<details>
  <summary>Details</summary>
Motivation: 企业因现代软件技术的发展而需要将上世纪九十年代流行的RAD平台（如Oracle Forms）上的遗留PL/SQL应用迁移到现代架构（如Java多层架构），以提升可维护性和可扩展性。

Method: 采用模型驱动重构方法，将遗留代码表示为KDM模型，并设计了一个包含类TDD开发流程和三类代码验证机制的模型转换开发过程。

Result: 成功实现并验证了从PL/SQL到Java的模型驱动迁移工具，详细说明了重构方法的实施过程，并评估了MDE应用中的一些关键问题。

Conclusion: 所提出的模型驱动重构流程能有效支持PL/SQL到Java的迁移任务，验证机制和增量开发方式提升了迁移工具的可靠性和开发效率。

Abstract: Model-driven software engineering (MDE) techniques are not only useful in
forward engineering scenarios, but can also be successfully applied to evolve
existing systems. RAD (Rapid Application Development) platforms emerged in the
nineties, but the success of modern software technologies motivated that a
large number of enterprises tackled the migration of their RAD applications,
such as Oracle Forms. Our research group has collaborated with a software
company in developing a solution to migrate PL/SQL monolithic code on Forms
triggers and program units to Java code separated in several tiers.
  Our research focused on the model-driven reengineering process applied to
develop the migration tool for the conversion of PL/SQL code to Java. Legacy
code is represented in form of KDM (Knowledge-Discovery Metamodel) models. In
this paper, we propose a software process to implement a model-driven
re-engineering. This process integrates a TDD-like approach to incrementally
develop model transformations with three kinds of validations for the generated
code. The implementation and validation of the re-engineering approach are
explained in detail, as well as the evaluation of some issues related with the
application of MDE.

</details>


### [22] [Automated Cloud Infrastructure-as-Code Reconciliation with AI Agents](https://arxiv.org/abs/2510.20211)
*Zhenning Yang,Hui Guan,Victor Nicolet,Brandon Paulsen,Joey Dodds,Daniel Kroening,Ang Chen*

Main category: cs.SE

TL;DR: 本文提出了NSync，一种自动化的基础设施即代码（IaC）协调系统，通过分析云API调用痕迹检测并修复由控制台、CLI或SDK等外部操作引起的基础设施漂移，利用大语言模型（LLM）推断用户意图并生成精准的IaC更新，在真实Terraform项目中显著提升了准确率和效率。


<details>
  <summary>Details</summary>
Motivation: 当IaC与云控制台、CLI或SDK混合使用时，IaC无法感知外部变更，导致基础设施漂移，可能引发配置过时、误删有效资源或操作错误。

Method: NSync通过监控底层云API调用轨迹检测非IaC变更，采用基于LLM的智能体架构推断高层意图，结合专用工具生成IaC更新，并通过自进化知识库持续优化；同时设计了注入真实漂移场景的评估流水线。

Result: 在5个真实Terraform项目和372个漂移场景的实验中，NSync将准确率（pass@3）从0.71提升至0.97，并实现1.47倍的token效率提升。

Conclusion: NSync有效解决了IaC与外部工具混用导致的基础设施漂移问题，通过API痕迹驱动和LLM增强的自动化协调机制，显著提高了IaC配置的准确性和维护效率。

Abstract: Cloud infrastructure is managed through a mix of interfaces -- traditionally,
cloud consoles, command-line interfaces (CLI), and SDKs are the tools of
choice. Recently, Infrastructure-as-Code/IaC frameworks (e.g., Terraform) have
quickly gained popularity. Unlike conventional tools, IaC~frameworks encode the
infrastructure in a "source-of-truth" configuration. They are capable of
automatically carrying out modifications to the cloud -- deploying, updating,
or destroying resources -- to bring the actual infrastructure into alignment
with the IaC configuration. However, when IaC is used alongside consoles, CLIs,
or SDKs, it loses visibility into external changes, causing infrastructure
drift, where the configuration becomes outdated, and later IaC operations may
undo valid updates or trigger errors.
  We present NSync, an automated system for IaC reconciliation that propagates
out-of-band changes back into the IaC program. Our key insight is that
infrastructure changes eventually all occur via cloud API invocations -- the
lowest layer for cloud management operations. NSync gleans insights from API
traces to detect drift (i.e., non-IaC changes) and reconcile it (i.e., update
the IaC configuration to capture the changes). It employs an agentic
architecture that leverages LLMs to infer high-level intents from noisy API
sequences, synthesize targeted IaC updates using specialized tools, and
continually improve through a self-evolving knowledge base of past
reconciliations. We further introduce a novel evaluation pipeline for injecting
realistic drifts into cloud infrastructure and assessing reconciliation
performance. Experiments across five real-world Terraform projects and 372
drift scenarios show that NSync outperforms the baseline both in terms of
accuracy (from 0.71 to 0.97 pass@3) and token efficiency (1.47$\times$
improvement).

</details>


### [23] [Classport: Designing Runtime Dependency Introspection for Java](https://arxiv.org/abs/2510.20340)
*Serena Cofano,Daniel Williams,Aman Sharma,Martin Monperrus*

Main category: cs.SE

TL;DR: Classport 是一种将依赖信息嵌入 Java 类文件的系统，使 Java 程序能在运行时进行依赖自省，从而支持软件供应链安全中的完整性检查。


<details>
  <summary>Details</summary>
Motivation: Java 缺乏对运行时依赖自省的支持，而这种能力对于软件供应链安全至关重要。

Method: 提出 Classport 系统，将依赖信息嵌入 Java 类文件中，以便在运行时检索依赖信息。

Result: 在六个真实项目上的评估表明，Classport 能有效实现运行时依赖识别。

Conclusion: Classport 实现了 Java 中运行时依赖自省，为运行时完整性检查等安全应用开辟了新途径。

Abstract: Runtime introspection of dependencies, i.e., the ability to observe which
dependencies are currently used during program execution, is fundamental for
Software Supply Chain security. Yet, Java has no support for it. We solve this
problem with Classport, a system that embeds dependency information into Java
class files, enabling the retrieval of dependency information at runtime. We
evaluate Classport on six real-world projects, demonstrating the feasibility in
identifying dependencies at runtime. Runtime dependency introspection with
Classport opens important avenues for runtime integrity checking.

</details>


### [24] [Toward Practical Deductive Verification: Insights from a Qualitative Survey in Industry and Academia](https://arxiv.org/abs/2510.20514)
*Lea Salome Brugger,Xavier Denis,Peter Müller*

Main category: cs.SE

TL;DR: 本文通过访谈30位工业界与学术界的验证实践者，系统分析了演绎验证成功应用的促成因素与阻碍其广泛采用的关键问题，并据此为从业者、工具开发者和研究人员提出了具体建议。


<details>
  <summary>Details</summary>
Motivation: 尽管演绎验证在特定项目中已被证明有效可行，但尚未成为主流技术。为推动其广泛应用，有必要深入理解其实际应用中的促成因素与阻碍因素。

Method: 采用半结构化访谈方式，对来自工业界和学术界的30位验证实践者进行调研，并通过主题分析方法系统分析所收集的数据。

Result: 除验证了已有挑战（如需要高水平专业知识）外，还揭示了若干被忽视的障碍，如证明维护困难、对自动化控制不足及可用性问题，并据此提炼出演绎验证的促进因素与阻碍因素。

Conclusion: 研究结果为实践者、工具开发者和研究人员提供了具体建议，涵盖可用性设计、自动化策略及与现有工作流集成的原则，以推动演绎验证的更广泛应用。

Abstract: Deductive verification is an effective method to ensure that a given system
exposes the intended behavior. In spite of its proven usefulness and
feasibility in selected projects, deductive verification is still not a
mainstream technique. To pave the way to widespread use, we present a study
investigating the factors enabling successful applications of deductive
verification and the underlying issues preventing broader adoption. We
conducted semi-structured interviews with 30 practitioners of verification from
both industry and academia and systematically analyzed the collected data
employing a thematic analysis approach. Beside empirically confirming familiar
challenges, e.g., the high level of expertise needed for conducting formal
proofs, our data reveal several underexplored obstacles, such as proof
maintenance, insufficient control over automation, and usability concerns. We
further use the results from our data analysis to extract enablers and barriers
for deductive verification and formulate concrete recommendations for
practitioners, tool builders, and researchers, including principles for
usability, automation, and integration with existing workflows.

</details>


### [25] [Large Language Models for Fault Localization: An Empirical Study](https://arxiv.org/abs/2510.20521)
*YingJian Xiao,RongQun Hu,WeiWei Gong,HongWei Li,AnQuan Jie*

Main category: cs.SE

TL;DR: 本文系统评估了多个大语言模型在语句级代码错误定位任务中的表现，发现结合缺陷报告上下文能显著提升性能，少样本学习存在边际效益递减，而思维链推理效果依赖模型自身推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有研究缺乏对大语言模型在上游错误定位任务中的系统性评估，而错误定位的性能直接影响后续自动程序修复的效果。

Method: 在HumanEval-Java和Defects4J数据集上，评估开源模型（如Qwen2.5-coder-32b-instruct、DeepSeek-V3）和闭源模型（如GPT-4.1 mini、Gemini-2.5-flash）在不同提示策略（标准提示、少样本示例、思维链）下的错误定位能力，并从准确性、时间效率和经济成本三个维度进行分析。

Result: 引入缺陷报告上下文显著提升模型性能；少样本学习有改进潜力但边际收益递减；思维链推理的效果高度依赖模型自身的推理能力。

Conclusion: 该研究揭示了不同大语言模型在错误定位任务中的性能特征与权衡，为提升错误定位效果提供了实用策略和见解。

Abstract: Large language models (LLMs) have demonstrated remarkable capabilities in
code-related tasks, particularly in automated program repair. However, the
effectiveness of such repairs is highly dependent on the performance of
upstream fault localization, for which comprehensive evaluations are currently
lacking. This paper presents a systematic empirical study on LLMs in the
statement-level code fault localization task. We evaluate representative
open-source models (Qwen2.5-coder-32b-instruct, DeepSeek-V3) and closed-source
models (GPT-4.1 mini, Gemini-2.5-flash) to assess their fault localization
capabilities on the HumanEval-Java and Defects4J datasets. The study
investigates the impact of different prompting strategies--including standard
prompts, few-shot examples, and chain-of-reasoning--on model performance, with
a focus on analysis across accuracy, time efficiency, and economic cost
dimensions. Our experimental results show that incorporating bug report context
significantly enhances model performance. Few-shot learning shows potential for
improvement but exhibits noticeable diminishing marginal returns, while
chain-of-thought reasoning's effectiveness is highly contingent on the model's
inherent reasoning capabilities. This study not only highlights the performance
characteristics and trade-offs of different models in fault localization tasks,
but also offers valuable insights into the strengths of current LLMs and
strategies for improving fault localization effectiveness.

</details>


### [26] [Exploring Large Language Models for Access Control Policy Synthesis and Summarization](https://arxiv.org/abs/2510.20692)
*Adarsh Vatsa,Bethel Hall,William Eiers*

Main category: cs.SE

TL;DR: 本文研究了大语言模型（LLMs）在访问控制策略生成与分析中的有效性，发现LLMs虽能生成语法正确的策略，但在语义准确性上存在不足；然而，结合符号方法时，LLMs在策略分析方面展现出良好潜力。


<details>
  <summary>Details</summary>
Motivation: 云环境中访问控制策略需手动编写，复杂且易出错，现有策略难以精确分析其行为是否符合预期，因此探索利用大语言模型自动生成或辅助理解这些策略。

Method: 首先评估多种LLMs在访问控制策略生成任务中的表现，衡量其生成策略与规范的等效性；其次提出一种基于语义的请求摘要方法，利用LLMs对策略所允许的请求进行精确刻画，并结合符号方法进行策略分析。

Result: 非推理型LLMs生成等效策略的成功率为45.8%，而推理型LLMs达93.7%；LLMs在策略生成中存在过度宽松问题，但在结合符号方法进行策略分析时效果显著。

Conclusion: 尽管LLMs在自动化生成访问控制策略方面仍面临挑战，但在与符号方法结合用于策略分析时具有广阔前景。

Abstract: Cloud computing is ubiquitous, with a growing number of services being hosted
on the cloud every day. Typical cloud compute systems allow administrators to
write policies implementing access control rules which specify how access to
private data is governed. These policies must be manually written, and due to
their complexity can often be error prone. Moreover, existing policies often
implement complex access control specifications and thus can be difficult to
precisely analyze in determining their behavior works exactly as intended.
Recently, Large Language Models (LLMs) have shown great success in automated
code synthesis and summarization. Given this success, they could potentially be
used for automatically generating access control policies or aid in
understanding existing policies. In this paper, we explore the effectiveness of
LLMs for access control policy synthesis and summarization. Specifically, we
first investigate diverse LLMs for access control policy synthesis, finding
that: although LLMs can effectively generate syntactically correct policies,
they have permissiveness issues, generating policies equivalent to the given
specification 45.8% of the time for non-reasoning LLMs, and 93.7% of the time
for reasoning LLMs. We then investigate how LLMs can be used to analyze
policies by introducing a novel semantic-based request summarization approach
which leverages LLMs to generate a precise characterization of the requests
allowed by a policy. Our results show that while there are significant hurdles
in leveraging LLMs for automated policy generation, LLMs show promising results
when combined with symbolic approaches in analyzing existing policies.

</details>
