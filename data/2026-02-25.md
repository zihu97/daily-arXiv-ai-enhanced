<div id=toc></div>

# Table of Contents

- [cs.OS](#cs.OS) [Total: 1]
- [cs.MA](#cs.MA) [Total: 1]
- [cs.DC](#cs.DC) [Total: 4]
- [cs.SE](#cs.SE) [Total: 2]
- [cs.AR](#cs.AR) [Total: 4]


<div id='cs.OS'></div>

# cs.OS [[Back]](#toc)

### [1] [Exploiting Dependency and Parallelism: Real-Time Scheduling and Analysis for GPU Tasks](https://arxiv.org/abs/2602.20826)
*Yuanhai Zhang,Songyang He,Ruizhe Gou,Mingyue Cui,Boyang Li,Shuai Zhao,Kai Huang*

Main category: cs.OS

TL;DR: 该论文提出了一种针对DAG结构GPU任务的可预测调度与分析方法，通过扩展内核级并行性和建立内核间依赖关系，在不假设内核优先级的情况下，实现了更安全且非悲观的makespan边界。实验表明，该方法比现有方法最多可减少32.8%的最坏情况makespan和21.3%的实际任务执行时间。


<details>
  <summary>Details</summary>
Motivation: 随着人工智能的快速发展，GPU在安全关键应用领域变得越来越重要，但其复杂的数据依赖性和资源竞争可能导致执行时间不可预测，这限制了其在需要可预测执行时间的安全关键系统中的应用。

Method: 提出了一种针对有向无环图(DAG)结构GPU任务的调度与分析方法，通过扩展内核级并行性和建立内核间依赖关系来提供简化和可预测的DAG响应时间，并采用标准CUDA API实现，无需额外硬件或软件支持。

Result: 在合成和真实世界基准测试中，该方法比现有方法最多可减少32.8%的最坏情况makespan和21.3%的实测任务执行时间。

Conclusion: 该方法有效解决了GPU任务执行时间不可预测的问题，为安全关键应用提供了更可靠、更高效的GPU计算解决方案。

Abstract: With the rapid advancement of Artificial Intelligence, the Graphics Processing Unit (GPU) has become increasingly essential across a growing number of safety-critical application domains. Applying a GPU is indispensable for parallel computing; however, the complex data dependencies and resource contention across kernels within a GPU task may unpredictably delay its execution time. To address these problems, this paper presents a scheduling and analysis method for Directed Acyclic Graph (DAG)-structured GPU tasks. Given a DAG representation, the proposed scheduling scales the kernel-level parallelism and establishes inter-kernel dependencies to provide a reduced and predictable DAG response time. The corresponding timing analysis yields a safe yet nonpessimistic makespan bound without any assumption on kernel priorities. The proposed method is implemented using the standard CUDA API, requiring no additional software or hardware support. Experimental results under synthetic and real-world benchmarks demonstrate that the proposed approach effectively reduces the worst-case makespan and measured task execution time compared to the existing methods up to 32.8% and 21.3%, respectively.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [2] [HieraMAS: Optimizing Intra-Node LLM Mixtures and Inter-Node Topology for Multi-Agent Systems](https://arxiv.org/abs/2602.20229)
*Tianjun Yao,Zhaoyi Li,Zhiqiang Shen*

Main category: cs.MA

TL;DR: 本文提出了一种名为HieraMAS的分层多智能体协作框架，通过结合节点内大语言模型混合和节点间通信拓扑结构，显著提升了推理和编程基准测试中的性能，同时提供了更好的成本效益权衡。


<details>
  <summary>Details</summary>
Motivation: 现有多智能体系统通常仅改善单一方面（如通信拓扑、角色分配或LLM路由），并将每个智能体视为单一不可分割单元，未能充分利用智能体内多种大语言模型混合来增强角色特定能力。

Method: HieraMAS引入超级节点概念，每个功能角色由多个异构大语言模型通过提议-合成结构实现，并采用两阶段算法：1) 多级奖励归因，提供节点级别和系统级别的细粒度反馈；2) 图分类进行拓扑选择，将通信结构选择视为整体决策而非逐边优化。

Result: 在推理和编程基准测试中，HieraMAS显著优于现有方法，同时提供更好的成本效益权衡。

Conclusion: 通过结合节点内LLM混合和节点间通信优化，HieraMAS实现了多智能体系统的性能突破，为复杂任务解决提供了新的架构范式。

Abstract: Multi-agent systems (MAS) built on large language models (LLMs) have shown strong performance across many tasks. Most existing approaches improve only one aspect at a time, such as the communication topology, role assignment, or LLM routing, while treating each agent as a single, indivisible unit. This misses the opportunity to use mixtures of LLMs within an agent to strengthen role-specific abilities. We propose HieraMAS, a hierarchical collaboration framework that combines intra-node LLM mixtures with an inter-node communication topology. HieraMAS introduces supernodes, where each functional role is implemented by multiple heterogeneous LLMs using a propose-synthesis structure. Optimizing HieraMAS creates unique credit-assignment challenges: final task performance depends heavily on the underlying LLMs' capabilities, which can lead reinforcement methods to incorrectly reward suboptimal configurations. To address this, we use a two-stage algorithm: (1) multi-level reward attribution, which provides fine-grained feedback at both the node level and the overall system level; (2) graph classification for topology selection, which treats choosing the communication structure as a holistic decision rather than optimizing edges one by one. Experiments on reasoning and coding benchmarks show that HieraMAS substantially outperforms existing methods while also delivering better cost-performance trade-offs.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [3] [The Tragedy of Chain Commons](https://arxiv.org/abs/2602.20341)
*Ignacio Amores-Sesar,Mirza Ahad Baig,Seth Gilbert,Ray Neiheiser,Michelle X. Yeo*

Main category: cs.DC

TL;DR: 本文研究了模块化区块链架构中分离共识与交易执行设计的安全问题，发现了一种称为'gaslighting'的新型攻击，并证明了在解耦模型中无法同时实现对此攻击的抵抗能力和资源利用率的最优化。作者提出了一个针对leader协议的中间模型来解决这个问题。


<details>
  <summary>Details</summary>
Motivation: 拜占庭容错(BFT)共识是许多现代区块链的基础，但交易执行和验证成为性能瓶颈。模块化设计将排序与执行分离可提高性能，但可能导致无效交易留在账本中，增加存储成本并引发新的战略行为。

Method: 作者提供了第一个系统性研究，建立了形式化框架来分析共识与执行的交互作用，并使用此框架识别并分析了一种先前未知的'gaslighting'攻击。他们证明了在解耦模型中，抵抗此攻击与资源利用率之间存在根本性权衡。

Result: 研究表明，在解耦模型中，无法同时确定性地抵抗gaslighting攻击并实现资源容量利用率的最优化。这被视为系统设计中的一个基本权衡。

Conclusion: 为解决这一权衡问题，作者讨论了一个针对leader协议的中间模型，该模型对gaslighting攻击具有鲁棒性，同时仍能实现高吞吐量和低延迟。

Abstract: Byzantine Fault Tolerant (BFT) consensus forms the foundation of many modern blockchains striving for both high throughput and low latency. A growing bottleneck is transaction execution and validation on the critical path of consensus, which has led to modular decoupled designs that separate ordering from execution: Consensus orders only metadata, while transactions are executed and validated concurrently. While this approach improves performance, it can leave invalid transactions in the ledger, increasing storage costs and enabling new forms of strategic behavior. We present the first systematic study of this setting, providing a formal framework to reason about the interaction between consensus and execution. Using this framework, we show that the decoupled design enables a previously unidentified attack, which we term gaslighting. We prove a fundamental trade-off between resilience to this attack and resource capacity utilization, where both are impossible to achieve deterministically in the decoupled model. To address this trade-off, we discuss an intermediate model for leader-based protocols that is robust to gaslighting attacks while achieving high throughput and low latency.

</details>


### [4] [Circumventing the FLP Impossibility Result with Open Atomic Ethernet](https://arxiv.org/abs/2602.20444)
*Paul Borrill*

Main category: cs.DC

TL;DR: 本文挑战了FLP不可能性结果的绝对性，提出Open Atomic Ethernet (OAE)通过'双同步'特性绕过了这一限制，实现了确定性原子协调。


<details>
  <summary>Details</summary>
Motivation: 四十年来，分布式计算领域一直将FLP不可能性结果视为不可逾越的约束，该结果表明在存在单个故障进程的异步系统中，没有确定性协议能保证共识。本文旨在证明这一结果并非物理定律，而是特定系统模型的定理。

Method: 作者提出了'双同步'概念来描述OAE的关键特性，即每轮边界上双方都能在有限时间内达成结果的共同知识，这比单纯的同步性更强。通过在第二层构建双同步、基于交换的协议，OAE绕过了FLP异步模型的基础假设。

Result: OAE成功实现了确定性原子协调，无需违反任何不可能性结果。通过拒绝异步模型的基础假设，该系统避免了FLP不可能性结果所依赖的关键条件。

Conclusion: FLP不是物理定律，而是关于特定系统模型的定理。通过采用双同步方法而非异步模型，系统可以绕过FLP不可能性，实现确定性分布式协调。

Abstract: The Fischer--Lynch--Paterson (FLP) impossibility result is widely regarded as one of the most fundamental negative results in distributed computing: no deterministic protocol can guarantee consensus in an asynchronous system with even one faulty process. For forty years, the field has treated this as an immovable constraint, designing around it with randomized protocols, failure detectors, and weakened consistency models. This essay argues that FLP is not a law of physics but a theorem about a particular system model -- and that Open Atomic Ethernet (OAE) circumvents it by rejecting the asynchronous model at its foundation. We introduce the term bisynchronous to describe OAE's key property: bounded-time bilateral resolution in which both parties reach common knowledge of outcome at every round boundary -- a strictly stronger guarantee than synchrony alone. By constructing a bisynchronous, swap-based protocol at Layer 2, OAE sidesteps the load-bearing assumptions of FLP's asynchronous model, achieving deterministic atomic coordination without violating any impossibility result.

</details>


### [5] [Heterogeneity-Aware Client Selection Methodology For Efficient Federated Learning](https://arxiv.org/abs/2602.20450)
*Nihal Balivada,Shrey Gupta,Shashank Shreedhar Bhatt,Suyash Gupta*

Main category: cs.DC

TL;DR: Terraform是一种新颖的联邦学习客户选择方法，通过结合梯度更新和确定性选择算法，有效解决了因客户端异质性导致的模型准确性降低问题，相比先前方法最高可提高47%的准确性。


<details>
  <summary>Details</summary>
Motivation: 联邦学习在保护数据隐私方面有优势，但由于客户端间的统计异质性，通常导致模型准确性低于传统机器学习方法。现有的客户选择方法使用模型更新如损失和偏差来选择参与者，但这些更新不能准确表示客户端异质性，且选择方法非确定。

Method: Terraform采用双管齐下的方法：1)使用梯度更新而非简单的损失或偏差来更准确地表示客户端异质性；2)实施确定性选择算法而非随机选择，确保选择过程的可靠性。

Result: Terraform在多个场景下实现了比先前方法高达47%的准确性提升。通过全面的消融研究和训练时间分析，证明了该方法在提高模型准确性和计算效率方面的有效性。

Conclusion: Terraform通过结合梯度更新和确定性选择算法，有效解决了联邦学习中因客户端异质性导致的模型准确性问题，为保护数据隐私的同时保持高模型准确性提供了可行的解决方案。

Abstract: Federated Learning (FL) enables a distributed client-server architecture where multiple clients collaboratively train a global Machine Learning (ML) model without sharing sensitive local data. However, FL often results in lower accuracy than traditional ML algorithms due to statistical heterogeneity across clients. Prior works attempt to address this by using model updates, such as loss and bias, from client models to select participants that can improve the global model's accuracy. However, these updates neither accurately represent a client's heterogeneity nor are their selection methods deterministic. We mitigate these limitations by introducing Terraform, a novel client selection methodology that uses gradient updates and a deterministic selection algorithm to select heterogeneous clients for retraining. This bi-pronged approach allows Terraform to achieve up to 47 percent higher accuracy over prior works. We further demonstrate its efficiency through comprehensive ablation studies and training time analyses, providing strong justification for the robustness of Terraform.

</details>


### [6] [A Granularity Characterization of Task Scheduling Effectiveness](https://arxiv.org/abs/2602.20561)
*Sana Taghipour Anvar,David Kaeli*

Main category: cs.DC

TL;DR: 本文提出了一种任务粒度表征框架，将调度开销增长与任务图依赖拓扑直接关联，表明依赖结构而非问题规模决定开销如何随并行性扩展，使系统能准确预测强扩展极限并动态选择动态或静态执行策略。


<details>
  <summary>Details</summary>
Motivation: 任务运行时系统虽为并行科学应用提供了灵活的负载均衡和可移植性，但其强扩展性对任务粒度高度敏感。随着并行度增加，调度开销可能从可忽略转变为主导，导致某些算法性能急剧下降，而对其他算法影响较小。目前缺乏对算法结构如何影响动态调度益处的系统理解。

Method: 引入了粒度表征框架，将调度开销增长直接链接到任务图依赖拓扑。展示出依赖结构而非问题规模单独决定了开销如何随并行性扩展。基于此，使用简单的粒度度量来表征执行行为，指示何时调度开销可被并行计算分摊，何时调度开销主导性能。

Result: 通过在具有不同依赖模式的代表性并行工作负载上的实验评估，证明所提表征能够解释实践中观察到的渐变性和突发性强扩展失效。进一步表明，从依赖拓扑导出的开销模型能准确预测强扩展极限，并实现实用的运行时决策规则来选择动态或静态执行，无需进行详尽的强扩展研究或大量离线调优。

Conclusion: 该框架通过理解算法结构如何影响调度开销扩展，为并行系统提供了预测扩展极限和优化执行策略的理论基础和实践方法。

Abstract: Task-based runtime systems provide flexible load balancing and portability for parallel scientific applications, but their strong scaling is highly sensitive to task granularity. As parallelism increases, scheduling overhead may transition from negligible to dominant, leading to rapid drops in performance for some algorithms, while remaining negligible for others. Although such effects are widely observed empirically, there is a general lack of understanding how algorithmic structure impacts whether dynamic scheduling is always beneficial. In this work, we introduce a granularity characterization framework that directly links scheduling overhead growth to task-graph dependency topology. We show that dependency structure, rather than problem size alone, governs how overhead scales with parallelism. Based on this observation, we characterize execution behavior using a simple granularity measure that indicates when scheduling overhead can be amortized by parallel computation and when scheduling overhead dominates performance. Through experimental evaluation on representative parallel workloads with diverse dependency patterns, we demonstrate that the proposed characterization explains both gradual and abrupt strong-scaling breakdowns observed in practice. We further show that overhead models derived from dependency topology accurately predict strong-scaling limits and enable a practical runtime decision rule for selecting dynamic or static execution without requiring exhaustive strong-scaling studies or extensive offline tuning.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [7] [Mitigating "Epistemic Debt" in Generative AI-Scaffolded Novice Programming using Metacognitive Scripts](https://arxiv.org/abs/2602.20206)
*Sreecharan Sankaranarayanan*

Main category: cs.SE

TL;DR: 研究显示 unrestricted AI 助编程导致认知技能获取不足，形成'脆弱专家'，而结构化 AI 辅助可减轻问题同时保持生产力。


<details>
  <summary>Details</summary>
Motivation: 探究 LLM 民主化带来的'氛围编程'对新手程序员认知技能获取的影响，以及如何防止产生'认知债务'和'脆弱专家'。

Method: 进行 78 名参与者的受试者间实验，比较三种条件：手动编程（对照组）、无限制 AI（外包）和结构化 AI（卸载），使用 Claude 3.5 Sonnet 和自定义 Cursor IDE 插件。

Result: 无限制 AI 用户生产力与结构化组相当，但在后续 AI 黑盒维护任务中失败率高达 77%，而结构化组仅为 39%。成功的氛围编程者自然地进行自我支架，将 AI 视为顾问而非承包商。

Conclusion: 未来学习系统必须强制执行'元认知摩擦'，防止产生不可维护的代码，平衡 AI 辅助与认知技能发展。

Abstract: The democratization of Large Language Models (LLMs) has given rise to ``Vibe Coding," a workflow where novice programmers prioritize semantic intent over syntactic implementation. While this lowers barriers to entry, we hypothesize that without pedagogical guardrails, it is fundamentally misaligned with cognitive skill acquisition. Drawing on the distinction between Cognitive Offloading and Cognitive Outsourcing, we argue that unrestricted AI encourages novices to outsource the Intrinsic Cognitive Load required for schema formation, rather than merely offloading Extraneous Load. This accumulation of ``Epistemic Debt" creates ``Fragile Experts" whose high functional utility masks critically low corrective competence.
  To quantify and mitigate this debt, we conducted a between-subjects experiment (N=78) using a custom Cursor IDE plugin backed by Claude 3.5 Sonnet. Participants represented "AI-Native" learners across three conditions: Manual (Control), Unrestricted AI (Outsourcing), and Scaffolded AI (Offloading). The Scaffolded condition utilized a novel ``Explanation Gate," leveraging a real-time LLM-as-a-Judge framework to enforce a ``Teach-Back" protocol before generated code could be integrated.
  Results reveal a ``Collapse of Competence": while Unrestricted AI users matched the productivity of the Scaffolded group (p < .001 vs. Manual), they suffered a 77% failure rate in a subsequent AI-Blackout maintenance task, compared to only 39% in the Scaffolded group. Qualitative analysis suggests that successful vibe coders naturally engage in self-scaffolding, treating the AI as a consultant rather than a contractor. We discuss the implications for the maintainability of AI-generated software and propose that future learning systems must enforce Metacognitive Friction to prevent the mass production of unmaintainable code.

</details>


### [8] [PhantomRun: Auto Repair of Compilation Errors in Embedded Open Source Software](https://arxiv.org/abs/2602.20284)
*Han Fu,Andreas Ermedahl,Sigrid Eldh,Kristian Wiklund,Philipp Haller,Cyrille Artho*

Main category: cs.SE

TL;DR: 这篇论文提出了PhantomRun框架，利用大型语言模型自动修复嵌入式系统的CI编译错误，成功修复了高达45%的编译失败。


<details>
  <summary>Details</summary>
Motivation: 嵌入式系统的CI管道经常在编译阶段失败，占用开发者大量调试时间。研究发现硬件依赖是导致编译失败的主要原因，且大多数修复只需要较小的代码更改。

Method: 开发了PhantomRun框架，通过GitHub Actions和GitLab CI的适配层以及四种不同的构建系统处理多样化的构建环境。该框架利用构建日志、源代码、历史修复记录和编译器错误消息，使用大型语言模型生成和验证修复方案。

Result: 在目标项目上评估显示，PhantomRun成功修复了高达45%的CI编译失败，证明了基于LLM的修复方法对嵌入式系统CI管道的可行性。

Conclusion: 大型语言模型可以有效地应用于嵌入式系统CI管道的编译错误修复，特别是当考虑到多样化的设置和缺乏测试数据等挑战时，PhantomRun框架为此提供了可行的解决方案。

Abstract: Continuous Integration (CI) pipelines for embedded software sometimes fail during compilation, consuming significant developer time for debugging. We study four major open-source embedded system projects, spanning over 4000 build failures from the project's CI runs. We find that hardware dependencies account for the majority of compilation failures, followed by syntax errors and build-script issues. Most repairs need relatively small changes, making automated repair potentially suitable as long as the diverse setups and lack of test data can be handled.
  In this paper, we present PhantomRun, an automated framework that leverages large language models (LLMs) to generate and validate fixes for CI compilation failures. The framework addresses the challenge of diverse build infrastructures and tool chains across embedded system projects by providing an adaptation layer for GitHub Actions and GitLab CI and four different build systems. PhantomRun utilizes build logs, source code, historical fixes, and compiler error messages to synthesize fixes using LLMs. Our evaluations show that PhantomRun successfully repairs up to 45% of CI compilation failures across the targeted projects, demonstrating the viability of LLM-based repairs for embedded-system CI pipelines.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [9] [SegSEM: Enabling and Enhancing SAM2 for SEM Contour Extraction](https://arxiv.org/abs/2602.20471)
*Da Chen,Guangyu Hu,Kaihong Xu,Kaichao Liang,Songjiang Li,Wei Yang,XiangYu Wen,Mingxuan Yuan*

Main category: cs.AR

TL;DR: 研究从SEM图像提取2D轮廓的案例，提出SegSEM框架在少量样本设置下有效适应SAM2模型


<details>
  <summary>Details</summary>
Motivation: 在数据稀缺的工业环境中，需要有效的基础模型适应方法来从SEM图像提取高保真轮廓用于OPC模型校准

Method: SegSEM框架，结合数据高效微调(仅训练编码器)和传统算法作为置信感知的混合架构

Result: 使用60张生产图像的小数据集验证了该方法的可行性

Conclusion: 为在数据受限的工业应用中利用基础模型提供了一种有效的方法论

Abstract: Extracting high-fidelity 2D contours from Scanning Electron Microscope (SEM) images is critical for calibrating Optical Proximity Correction (OPC) models. While foundation models like Segment Anything 2 (SAM2) are promising, adapting them to specialized domains with scarce annotated data is a major challenge. This paper presents a case study on adapting SAM2 for SEM contour extraction in a few-shot setting. We propose SegSEM, a framework built on two principles: a data-efficient fine-tuning strategy that adapts by selectively training only the model's encoders, and a robust hybrid architecture integrating a traditional algorithm as a confidence-aware fallback. Using a small dataset of 60 production images, our experiments demonstrate this methodology's viability. The primary contribution is a methodology for leveraging foundation models in data-constrained industrial applications.

</details>


### [10] [FAST-Prefill: FPGA Accelerated Sparse Attention for Long Context LLM Prefill](https://arxiv.org/abs/2602.20515)
*Rakshith Jayanth,Viktor Prasanna*

Main category: cs.AR

TL;DR: FAST-Prefill是一个FPGA加速器，通过动态稀疏注意力技术解决了长上下文LLM预填充推理的内存限制问题，在TTFT和能效方面显著优于GPU实现。


<details>
  <summary>Details</summary>
Motivation: 长上下文LLM预填充推理因动态注意力稀疏模式和有限的数据重用而成为内存限制型任务，且在GPU上进行长上下文推理能耗高。FPGA被提出作为高效替代方案。

Method: 解决方案包括：具有内存感知执行顺序的融合流水线单元用于稀疏索引生成；基于活跃性的双层缓存用于KV访问；结合DSP和位平面分解的混合MPU用于矩阵乘法。

Result: 在Alveo U280 FPGA上评估了Llama和Qwen模型（4K-128K上下文），FAST-Prefill实现了2.5倍的TTFT加速和相比Nvidia A5000 GPU 4.5倍的能效提升。

Conclusion: FAST-Prefill通过专门设计的硬件架构解决了内存限制挑战，展示了FPGA在加速动态长上下文LLM推理方面的潜力。

Abstract: In long-context large language model (LLM) inference, the prefill stage dominates computation due to self-attention over the complete input context. Sparse attention significantly reduces self-attention computation by limiting each token's interactions to a subset of tokens. The attention sparsity pattern varies across input prompts, and within a prompt, each attention head can follow a distinct pattern. This makes attention sparsity dynamic. The requirement of generating the sparsity pattern, combined with limited data reuse in attention, shifts the prefill compute to being memory-bound. This, in addition to the huge energy requirements for long-context inference on GPU, motivates FPGAs as good candidates for accelerating dynamic long-context inference.
  To tackle these challenges, we propose FAST-Prefill, the first FPGA accelerator for long-context prefill-stage inference with dynamic sparse attention. To efficiently generate sparse indices, we propose a \textit{fused pipeline unit with a memory-aware execution order} to reduce large tensors and irregular memory accesses. To reduce off-chip memory traffic for accessing the KV cache, we utilize the memory hierarchy to design a \textit{liveness-driven, dual-tier cache}. For high-throughput matrix multiplication, we design a \textit{hybrid Matrix Processing Unit (MPU)} with DSPs and bit-plane decomposition using LUTs. We implement FAST-Prefill on Alveo U280 and evaluate it on the Llama and Qwen models (batch size = 1) for context lengths ranging from 4K to 128K tokens. We demonstrate an average speedup of up to 2.5$\times$ in TTFT and 4.5$\times$ improvement in energy efficiency over GPU implementation on Nvidia A5000 GPU.

</details>


### [11] [TOM: A Ternary Read-only Memory Accelerator for LLM-powered Edge Intelligence](https://arxiv.org/abs/2602.20662)
*Hongyi Guan,Yijia Zhang,Wenqiang Wang,Yizhao Gao,Shijie Cao,Chen Zhang,Ningyi Xu*

Main category: cs.AR

TL;DR: 本文提出TOM，一种结合三元量化的混合ROM-SRAM加速器，用于解决边缘设备部署大语言模型的内存墙问题，实现了3306 TPS的推理吞吐量，同时保持高能效。


<details>
  <summary>Details</summary>
Motivation: 解决边缘设备上内存容量和带宽有限对可部署模型大小、推理速度和设备端适应性的制约，即内存墙挑战。

Method: TOM是一种混合ROM-SRAM加速器，与三元量化协同设计，包括：(1)稀疏感知ROM架构，将三元权重合成为标准单元逻辑，消除零值比特的面积开销；(2)分布式处理架构，将高密度ROM库与灵活的基于SRAM的QLoRA适配器和计算单元协同定位；(3)工作负载感知的动态电源门控方案，利用ROM的逻辑特性关闭非活动存储库，最小化动态能耗。

Result: 使用BitNet-2B模型实现了3306 TPS的推理吞吐量，证明了其在提供实时、能效边缘智能方面的有效性。

Conclusion: TOM通过结合三元量化和混合ROM-SRAM架构，成功解决了边缘设备LLM部署的内存墙挑战，实现了高推理吞吐量和能效。

Abstract: The deployment of Large Language Models (LLMs) for real-time intelligence on edge devices is rapidly growing. However, conventional hardware architectures face a fundamental memory wall challenge, where limited on-device memory capacity and bandwidth severely constrain the size of deployable models and their inference speed, while also limiting on-device adaptation. To address this challenge, we propose TOM, a hybrid ROM-SRAM accelerator co-designed with ternary quantization, which balances extreme density with on-device tunability. TOM exploits the synergy between ternary quantization and ROM to achieve extreme memory density and bandwidth, while preserving flexibility through a hybrid ROM-SRAM architecture designed for QLoRA-based tunability. Specifically, we introduce: (1) a sparsity-aware ROM architecture that synthesizes ternary weights as standard-cell logic, eliminating area overhead from zero-valued bits; (2) a distributed processing architecture that co-locates high-density ROM banks with flexible SRAM-based QLoRA adapters and compute units; and (3) a workload-aware dynamic power gating scheme that exploits the logic-based nature of ROM to power down inactive banks, minimizing dynamic energy consumption. TOM achieves an inference throughput of 3,306 TPS using BitNet-2B model, demonstrating its effectiveness in delivering real-time, energy-efficient edge intelligence.

</details>


### [12] [LUTstructions: Self-loading FPGA-based Reconfigurable Instructions](https://arxiv.org/abs/2602.20802)
*Philippos Papaphilippou*

Main category: cs.AR

TL;DR: 本文提出了一种具有可重构指令的软核处理器架构，通过在FPGA上实现LUTstruction架构，实现了从主内存动态加载指令位流的功能，无明显频率开销。


<details>
  <summary>Details</summary>
Motivation: 通用处理器指令集有限，即使是包含数百或数千条指令的向量扩展，通常也无法有效表达任意任务，需要更灵活的可重构指令方案。

Method: 采用软核处理器并结合可重构区域，实现从主内存动态加载指令实现位流的技术，使用名为LUTstruction的自定义FPGA架构，针对低延迟自定义指令和宽范围重构进行优化。

Result: 在FPGA上成功实现了完全评估的软核，实质上创建了'在FPGA上的FPGA'用于指令实现，且无明显操作频率开销。

Conclusion: LUTstruction架构为可重构指令处理器提供了有效实现方式，通过软实现支持了架构探索，为未来处理器设计提供了新思路。

Abstract: General-purpose processors feature a limited number of instructions based on an instruction set. They can be numerous, such as with vector extensions that include hundreds or thousands of instructions, but this comes at a cost; they are often unable to express arbitrary tasks efficiently. This paper explores the concept of having reconfigurable instructions by incorporating reconfigurable areas in a softcore. It follows a relatively-recently proposed computer architecture concept for seamlessly loading instruction implementation-carrying bitstreams from main memory. The resulting softcore is entirely evaluated on an FPGA, essentially having an FPGA-on-an-FPGA for the instruction implementations, with no notable operating frequency overhead. This is achieved with a custom FPGA architecture called LUTstruction, which is tailored towards low-latency for custom instructions and wide reconfiguration, as well as a soft implementation for the purposes of architectural exploration.

</details>
