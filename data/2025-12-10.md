<div id=toc></div>

# Table of Contents

- [cs.SE](#cs.SE) [Total: 16]
- [cs.OS](#cs.OS) [Total: 1]
- [cs.DC](#cs.DC) [Total: 9]
- [cs.PF](#cs.PF) [Total: 1]
- [cs.AR](#cs.AR) [Total: 1]
- [cs.MA](#cs.MA) [Total: 3]
- [cs.NI](#cs.NI) [Total: 3]


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [1] [CFD-copilot: leveraging domain-adapted large language model and model context protocol to enhance simulation automation](https://arxiv.org/abs/2512.07917)
*Zhehao Dong,Shanghai Du,Zhen Lu,Yue Yang*

Main category: cs.SE

TL;DR: CFD-copilot 是一个专为计算流体力学设计的多智能体 LLM 框架，支持从自然语言描述到后处理的全流程自动化。


<details>
  <summary>Details</summary>
Motivation: 降低非专家使用 CFD 仿真的门槛，实现端到端自动化工作流。

Method: 采用微调 LLM + 多智能体系统 + MCP 协议，实现自然语言转仿真配置、错误自纠与模块化后处理。

Result: 在 NACA~0012 与 30P-30N 翼型测试中，验证了领域适配与 MCP 协议可提升可靠性与效率。

Conclusion: 领域专用 LLM 与模块化工具集成能有效推动工程仿真自动化。

Abstract: Configuring computational fluid dynamics (CFD) simulations requires significant expertise in physics modeling and numerical methods, posing a barrier to non-specialists. Although automating scientific tasks with large language models (LLMs) has attracted attention, applying them to the complete, end-to-end CFD workflow remains a challenge due to its stringent domain-specific requirements. We introduce CFD-copilot, a domain-specialized LLM framework designed to facilitate natural language-driven CFD simulation from setup to post-processing. The framework employs a fine-tuned LLM to directly translate user descriptions into executable CFD setups. A multi-agent system integrates the LLM with simulation execution, automatic error correction, and result analysis. For post-processing, the framework utilizes the model context protocol (MCP), an open standard that decouples LLM reasoning from external tool execution. This modular design allows the LLM to interact with numerous specialized post-processing functions through a unified and scalable interface, improving the automation of data extraction and analysis. The framework was evaluated on benchmarks including the NACA~0012 airfoil and the three-element 30P-30N airfoil. The results indicate that domain-specific adaptation and the incorporation of the MCP jointly enhance the reliability and efficiency of LLM-driven engineering workflows.

</details>


### [2] [An Empirical Framework for Evaluating Semantic Preservation Using Hugging Face](https://arxiv.org/abs/2512.07983)
*Nan Jia,Anita Raja,Raffi Khatchadourian*

Main category: cs.SE

TL;DR: 本文提出一个评估学习型软件系统语义保持性的实证框架，基于HuggingFace模型演化数据检测语义漂移并识别重构模式。


<details>
  <summary>Details</summary>
Motivation: 机器学习系统的非确定性使传统重构方法失效，需确保智能组件优化不改变系统功能行为。

Method: 从HuggingFace提取170万条模型数据，构建可复现管道分析536个模型的4000+指标，结合提交信息与性能变化进行案例研究。

Result: 成功检测到跨版本语义漂移，识别常见重构模式，为社区定义语义保持边界提供基础。

Conclusion: 本研究推动了可维护、可信机器学习系统的基础建设，提供了大规模模型演化数据集与实用评估管道。

Abstract: As machine learning (ML) becomes an integral part of high-autonomy systems, it is critical to ensure the trustworthiness of learning-enabled software systems (LESS). Yet, the nondeterministic and run-time-defined semantics of ML complicate traditional software refactoring. We define semantic preservation in LESS as the property that optimizations of intelligent components do not alter the system's overall functional behavior. This paper introduces an empirical framework to evaluate semantic preservation in LESS by mining model evolution data from HuggingFace. We extract commit histories, $\textit{Model Cards}$, and performance metrics from a large number of models. To establish baselines, we conducted case studies in three domains, tracing performance changes across versions. Our analysis demonstrates how $\textit{semantic drift}$ can be detected via evaluation metrics across commits and reveals common refactoring patterns based on commit message analysis. Although API constraints limited the possibility of estimating a full-scale threshold, our pipeline offers a foundation for defining community-accepted boundaries for semantic preservation. Our contributions include: (1) a large-scale dataset of ML model evolution, curated from 1.7 million Hugging Face entries via a reproducible pipeline using the native HF hub API, (2) a practical pipeline for the evaluation of semantic preservation for a subset of 536 models and 4000+ metrics and (3) empirical case studies illustrating semantic drift in practice. Together, these contributions advance the foundations for more maintainable and trustworthy ML systems.

</details>


### [3] [A Gray Literature Study on Fairness Requirements in AI-enabled Software Engineering](https://arxiv.org/abs/2512.07990)
*Thanh Nguyen,Chaima Boufaied,Ronnie de Souza Santos*

Main category: cs.SE

TL;DR: 本文综述灰色文献，探讨AI系统中公平性需求的定义、管理方法及违反后果，强调需在软件开发生命周期中同等重视公平性与有效性。


<details>
  <summary>Details</summary>
Motivation: 当前AI应用过度关注模型效能指标（如F1分数），而忽视公平性，亟需系统性研究以填补此空白。

Method: 通过灰色文献综述，分析不同领域中公平性需求的定义、SDLC中的管理实践、违规原因及其社会影响。

Result: 发现公平性常聚焦非歧视与平等对待；其管理在训练、监控与数据处理阶段差异显著；违规主因包括数据偏见、算法设计缺陷等；后果涵盖社会伤害、信任丧失等。

Conclusion: 必须建立一致框架，在AI软件开发中系统整合公平性实践，使其与效能指标获得同等重视。

Abstract: Today, with the growing obsession with applying Artificial Intelligence (AI), particularly Machine Learning (ML), to software across various contexts, much of the focus has been on the effectiveness of AI models, often measured through common metrics such as F1- score, while fairness receives relatively little attention. This paper presents a review of existing gray literature, examining fairness requirements in AI context, with a focus on how they are defined across various application domains, managed throughout the Software Development Life Cycle (SDLC), and the causes, as well as the corresponding consequences of their violation by AI models. Our gray literature investigation shows various definitions of fairness requirements in AI systems, commonly emphasizing non-discrimination and equal treatment across different demographic and social attributes. Fairness requirement management practices vary across the SDLC, particularly in model training and bias mitigation, fairness monitoring and evaluation, and data handling practices. Fairness requirement violations are frequently linked, but not limited, to data representation bias, algorithmic and model design bias, human judgment, and evaluation and transparency gaps. The corresponding consequences include harm in a broad sense, encompassing specific professional and societal impacts as key examples, stereotype reinforcement, data and privacy risks, and loss of trust and legitimacy in AI-supported decisions. These findings emphasize the need for consistent frameworks and practices to integrate fairness into AI software, paying as much attention to fairness as to effectiveness.

</details>


### [4] [What Pulls the Strings? Understanding the Characteristics and Role of Argumentation in Open-Source Software Usability Discussions](https://arxiv.org/abs/2512.08032)
*Arghavan Sanei,Chaima Amiri,Atefeh Shokrizadeh,Jinghui Cheng*

Main category: cs.SE

TL;DR: 该研究分析了开源软件可用性讨论中的论证话语，发现其质量参差且影响参与者行为，旨在提升论证有效性与可用性。


<details>
  <summary>Details</summary>
Motivation: 开源软件可用性常被忽视，论证话语特征不明阻碍有效支持讨论参与者。

Method: 对五个开源项目中的论证话语与质量进行综合分析。

Result: 可用性讨论以论证为主导，但评论质量低于帖子，论证质量影响后续参与者行为。

Conclusion: 研究为利益相关者提供改进论证的洞见，有助于提升开源软件可用性并启发其他分布式协作社区研究。

Abstract: The usability of open-source software (OSS) is important but frequently overlooked in favor of technical and functional complexity. Argumentation can be a pivotal device for diverse stakeholders in OSS usability discussions to express opinions and persuade others. However, the characteristics of argument discourse in those discussions remain unknown, resulting in difficulties in providing effective support for discussion participants. We address this through a comprehensive analysis of argument discourse and quality in five OSS projects. Our results indicated that usability discussions are predominantly argument-driven, although their qualities vary. Issue comments exhibit lower-quality arguments than the issue posts, suggesting a shortage of collective intelligence about usability in OSS communities. Moreover, argument discourse and quality have various impacts on the subsequent behavior of participants. Overall, this research offers insights to help OSS stakeholders build more effective arguments and eventually improve OSS usability. These insights can also inform studies about other distributed collaborative communities.

</details>


### [5] [Secure or Suspect? Investigating Package Hallucinations of Shell Command in Original and Quantized LLMs](https://arxiv.org/abs/2512.08213)
*Md Nazmul Haque,Elizabeth Lin,Lawrence Arkoh,Biruk Tadesse,Bowen Xu*

Main category: cs.SE

TL;DR: 量化显著增加LLM生成Go包时的幻觉率和安全漏洞风险，尤其4-bit模型退化最严重。


<details>
  <summary>Details</summary>
Motivation: 研究量化对LLM生成代码依赖项的正确性与安全性影响，填补该领域实证空白。

Method: 评估五种Qwen模型在全精度、8-bit和4-bit量化下于三个数据集上的表现，分析包幻觉率与漏洞存在率变化。

Result: 量化导致包幻觉率上升，4-bit模型最差；即使正确生成的包，其漏洞率也随精度降低而升高；幻觉包多模仿真实URL路径。

Conclusion: 部署量化LLM用于代码生成需谨慎，应权衡效率与安全可靠性。

Abstract: Large Language Models for code (LLMs4Code) are increasingly used to generate software artifacts, including library and package recommendations in languages such as Go. However, recent evidence shows that LLMs frequently hallucinate package names or generate dependencies containing known security vulnerabilities, posing significant risks to developers and downstream software supply chains. At the same time, quantization has become a widely adopted technique to reduce inference cost and enable deployment of LLMs on resource-constrained environments. Despite its popularity, little is known about how quantization affects the correctness and security of LLM-generated software dependencies while generating shell commands for package installation.
  In this work, we conduct the first systematic empirical study of the impact of quantization on package hallucination and vulnerability risks in LLM-generated Go packages. We evaluate five Qwen model sizes under full-precision, 8-bit, and 4-bit quantization across three datasets (SO, MBPP, and paraphrase). Our results show that quantization substantially increases the package hallucination rate (PHR), with 4-bit models exhibiting the most severe degradation. We further find that even among the correctly generated packages, the vulnerability presence rate (VPR) rises as precision decreases, indicating elevated security risk in lower-precision models. Finally, our analysis of hallucinated outputs reveals that most fabricated packages resemble realistic URL-based Go module paths, such as most commonly malformed or non-existent GitHub and golang.org repositories, highlighting a systematic pattern in how LLMs hallucinate dependencies. Overall, our findings provide actionable insights into the reliability and security implications of deploying quantized LLMs for code generation and dependency recommendation.

</details>


### [6] [Migrating QAOA from Qiskit 1.x to 2.x: An experience report](https://arxiv.org/abs/2512.08245)
*Julien Cardinal,Imen Benzarti,Ghizlane El boussaidi,Christophe Pere*

Main category: cs.SE

TL;DR: 迁移量子算法时，隐藏参数如采样次数会显著影响结果准确性，需调整以确保可复现性。


<details>
  <summary>Details</summary>
Motivation: 解决量子算法在框架迁移中因隐式参数导致的行为差异问题。

Method: 将QAOA从Qiskit 1.x迁移到2.x，对比不同采样次数对结果的影响。

Result: 默认10,000次采样仅覆盖23%状态空间，增至250,000次后恢复精度。

Conclusion: 量子-经典交互层的隐藏参数主导混合算法性能，开发者应显式控制关键参数。

Abstract: Migrating quantum algorithms across evolving frameworks introduces subtle behavioral changes that affect accuracy and reproducibility. This paper reports our experience converting the Quantum Approximate Optimization Algorithm (QAOA) from Qiskit Algorithms with Qiskit 1.x (v1 primitives) to a custom implementation using Qiskit 2.x (v2 primitives). Despite identical circuits, optimizers, and Hamiltonians, the new version produced drastically different results. A systematic analysis revealed the root cause: the sampling budget -- the number of circuit executions (shots) per iteration. The library's implicit use of unlimited shots yielded dense probability distributions, whereas the v2 default of 10 000 shots captured only 23% of the state space. Increasing shots to 250 000 restored library-level accuracy. This study highlights how hidden parameters at the quantum-classical interaction level can dominate hybrid algorithm performance and provides actionable recommendations for developers and framework designers to ensure reproducible results in quantum software migration.

</details>


### [7] [Token Sugar: Making Source Code Sweeter for LLMs through Token-Efficient Shorthand](https://arxiv.org/abs/2512.08266)
*Zhensu Sun,Chengran Yang,Xiaoning Du,Zhou Yang,Li Li,David Lo*

Main category: cs.SE

TL;DR: Token Sugar通过语义级代码模式压缩减少LLM推理开销，节省高达11.2%的token且保持性能。


<details>
  <summary>Details</summary>
Motivation: 编程语言冗长导致LLM计算成本高，现有方法仅限语法优化，未挖掘语义层面的压缩潜力。

Method: 提出Token Sugar概念，从代码库挖掘高频冗长模式并映射为简写，融入LLM预训练过程。

Result: 实现最高15.1%源码token压缩，模型生成时节省11.2% token，Pass@1得分与基线相当。

Conclusion: Token Sugar在语义层面有效降低LLM推理成本，且与语法优化方法互补，具备实用价值。

Abstract: Large language models (LLMs) have shown exceptional performance in code generation and understanding tasks, yet their high computational costs hinder broader adoption. One important factor is the inherent verbosity of programming languages, such as unnecessary formatting elements and lengthy boilerplate code. This leads to inflated token counts in both input and generated outputs, which increases inference costs and slows down the generation process. Prior work improves this through simplifying programming language grammar, reducing token usage across both code understanding and generation tasks. However, it is confined to syntactic transformations, leaving significant opportunities for token reduction unrealized at the semantic level.
  In this work, we propose Token Sugar, a concept that replaces frequent and verbose code patterns with reversible, token-efficient shorthand in the source code. To realize this concept in practice, we designed a systematic solution that mines high-frequency, token-heavy patterns from a code corpus, maps each to a unique shorthand, and integrates them into LLM pretraining via code transformation. With this solution, we obtain 799 (code pattern, shorthand) pairs, which can reduce up to 15.1% token count in the source code and is complementary to existing syntax-focused methods. We further trained three widely used LLMs on Token Sugar-augmented data. Experimental results show that these models not only achieve significant token savings (up to 11.2% reduction) during generation but also maintain near-identical Pass@1 scores compared to baselines trained on unprocessed code.

</details>


### [8] [FedLAD: A Modular and Adaptive Testbed for Federated Log Anomaly Detection](https://arxiv.org/abs/2512.08277)
*Yihan Liao,Jacky Keung,Zhenyu Mao,Jingyu Zhang,Jialong Li*

Main category: cs.SE

TL;DR: FedLAD是一个支持联邦学习环境下日志异常检测的统一实验平台，具备插件式模型集成与自适应运行时功能。


<details>
  <summary>Details</summary>
Motivation: 现有日志异常检测方法多依赖集中式训练，难以满足隐私与分布式日志场景需求，且缺乏专用联邦学习测试平台。

Method: 构建FedLAD平台，支持多种LAD模型、数据集与聚合策略的即插即用，并提供自监控、自配置与自适应控制功能。

Result: FedLAD实现了可复现、可扩展的联邦LAD实验，弥合了联邦学习框架与日志异常检测需求之间的鸿沟。

Conclusion: FedLAD为未来联邦日志异常检测研究提供了坚实基础，相关代码已开源。

Abstract: Log-based anomaly detection (LAD) is critical for ensuring the reliability of large-scale distributed systems. However, most existing LAD approaches assume centralized training, which is often impractical due to privacy constraints and the decentralized nature of system logs. While federated learning (FL) offers a promising alternative, there is a lack of dedicated testbeds tailored to the needs of LAD in federated settings. To address this, we present FedLAD, a unified platform for training and evaluating LAD models under FL constraints. FedLAD supports plug-and-play integration of diverse LAD models, benchmark datasets, and aggregation strategies, while offering runtime support for validation logging (self-monitoring), parameter tuning (self-configuration), and adaptive strategy control (self-adaptation). By enabling reproducible and scalable experimentation, FedLAD bridges the gap between FL frameworks and LAD requirements, providing a solid foundation for future research. Project code is publicly available at: https://github.com/AA-cityu/FedLAD.

</details>


### [9] [Measuring Agile Agreement: Development and Validation of the Manifesto and Principle Scales](https://arxiv.org/abs/2512.08461)
*Nicolas Matton,Anthony Simonofski,Marie-Ange Remiche,Benoît Vanderose*

Main category: cs.SE

TL;DR: 本文设计并验证了两个测量敏捷协议的量表：宣言协议量表（MAS）和原则协议量表（PAS），以区分对敏捷宣言价值观与12项原则实践的认同差异。


<details>
  <summary>Details</summary>
Motivation: 现有研究未能区分对敏捷宣言抽象价值观与具体实践原则的认同，导致测量模糊，本文旨在填补此方法论空白。

Method: 通过系统化的条目创建、问卷设计与验证流程，结合内部一致性检验、建构效度分析及收敛发散分析（如比例优势逻辑回归、Bland-Altman图、组内相关系数）评估量表性能。

Result: MAS与PAS均具有良好信效度，二者中度相关但不可互换，能捕捉敏捷协议的不同维度。

Conclusion: 本研究提供了一对公开可用的量表工具，有助于更细致地测量个体与敏捷方法的匹配度，推动敏捷人因研究精细化。

Abstract: While the importance of human factors in agile software development is widely acknowledged, the measurement of an individual's "agile agreement" remains an ill-defined and challenging area. A key limitation in existing research is the failure to distinguish between agreement with the abstract, high-level values of the Agile Manifesto and agreement with the concrete, day-to-day practices derived from the 12 Principles. This paper addresses this methodological gap by presenting the design and validation of two distinct instruments: the novel Manifesto Agreement Scale (MAS), and the Principle Agreement Scale (PAS), which is a systematic adaptation and refinement of a prior instrument.
  We detail the systematic process of item creation and selection, survey design, and validation. The results demonstrate that both scales possess important internal consistency and construct validity. A convergence and divergence analysis, including Proportional Odds Logistic Regression, a Bland-Altman plot, and an Intraclass Correlation Coefficient (ICC), reveals that while the two scales are moderately correlated, they are not interchangeable and capture distinct dimensions of agile agreement. The primary contribution of this work is a pair of publicly available instruments, validated within a specific demographic of Belgian IT professionals. These scales represent a critical initial step toward facilitating a more nuanced measurement of agile agreement, distinguishing agile agreement across various levels of perception and aiding in a more refined interpretation of person-agile fit.

</details>


### [10] [Measuring Computer Science Enthusiasm: A Questionnaire-Based Analysis of Age and Gender Effects on Students' Interest](https://arxiv.org/abs/2512.08472)
*Kai Marquardt,Robert Hanak,Anne Koziolek,Lucia Happe*

Main category: cs.SE

TL;DR: 本研究通过区分年龄和性别对计算机科学兴趣的影响，提出了一种动态、年龄敏感的教育框架。


<details>
  <summary>Details</summary>
Motivation: 探索青少年在计算机科学教育中的兴趣变化，特别是年龄与性别的独立影响。

Method: 基于人-物兴趣理论，设计问卷评估短期干预活动对兴趣激发的效果，并分析400多名学生数据。

Result: 发现年龄比性别更显著影响兴趣发展，早期青少年尤其是女生兴趣明显下降，但年长学生干预后提升最大。

Conclusion: 应依据发展阶段调整教学策略，短期优质活动可在较晚年龄有效重燃学习兴趣。

Abstract: This study offers new insights into students' interest in computer science (CS) education by disentangling the distinct effects of age and gender across a diverse adolescent sample. Grounded in the person-object theory of interest (POI), we conceptualize enthusiasm as a short-term, activating expression of interest that combines positive affect, perceived relevance, and intention to re-engage. Experiencing such enthusiasm can temporarily shift CS attitudes and strengthen future engagement intentions, making it a valuable lens for evaluating brief outreach activities. To capture these dynamics, we developed a theoretically grounded questionnaire for pre-post assessment of the enthusiasm potential of CS interventions. Using data from more than 400 students participating in online CS courses, we examined age- and gender-related patterns in enthusiasm. The findings challenge the prevailing belief that early exposure is the primary pathway to sustained interest in CS. Instead, we identify a marked decline in enthusiasm during early adolescence, particularly among girls, alongside substantial variability in interest trajectories across age groups. Crucially, our analyses reveal that age is a more decisive factor than gender in shaping interest development and uncover key developmental breakpoints. Despite starting with lower baseline attitudes, older students showed the largest positive changes following the intervention, suggesting that well-designed short activities can effectively re-activate interest even at later ages. Overall, the study highlights the need for a dynamic, age-sensitive framework for CS education in which instructional strategies are aligned with developmental trajectories.

</details>


### [11] [Gamification with Purpose: What Learners Prefer to Motivate Their Learning](https://arxiv.org/abs/2512.08551)
*Kai Marquardt,Mona Schulz,Anne Koziolek,Lucia Happe*

Main category: cs.SE

TL;DR: 研究通过系统文献综述和最佳-最差量表调查，分析学习者对游戏化设计元素的偏好，发现学习者更青睐直接支持学习过程的元素，如进度条、概念图等。


<details>
  <summary>Details</summary>
Motivation: 为开发目标导向的游戏化策略，需了解学习者对游戏化设计元素的偏好，以避免削弱内在动机并提升教育效果。

Method: 采用系统文献综述识别十种常见游戏化设计元素，制作视觉原型并通过125名参与者的BWS调查获取偏好排序，辅以定性反馈分析动机主题。

Result: 学习者最偏好直接支持学习的元素，如进度条、概念图、即时反馈和成就；定性分析提炼出六个动机主题，包括可见进步、内容相关性和建设性反馈。

Conclusion: 应优先选择与教育内容深度融合、可视化学习进展并提供可操作反馈的游戏化工具，而非单纯依赖外在激励。

Abstract: This study investigates learners' preferences for game design elements (GDEs) in educational contexts to inform the development of purpose-driven gamification strategies. It emphasizes a learner-centered approach that aligns gamification design with pedagogical goals, while mitigating risks such as the erosion of intrinsic motivation. A systematic literature review was conducted to identify ten widely discussed GDEs. Visual prototypes representing each element were developed, and a best-worst scaling (BWS) survey with 125 participants was administered to elicit preference rankings. Qualitative feedback was also collected to uncover motivational drivers. Learners consistently preferred GDEs that support learning processes directly-most notably progress bars, concept maps, immediate feedback, and achievements. Qualitative analysis revealed six recurring motivational themes, including visible progress, content relevance, and constructive feedback. The findings suggest that learners value gamification elements that are meaningfully integrated with educational content and support intrinsic motivation. Purpose-aligned gamification should prioritize tools that visualize learning progress and provide actionable feedback, rather than relying solely on extrinsic incentives.

</details>


### [12] [Reusability in MLOps: Leveraging Ports and Adapters to Build a Microservices Architecture for the Maritime Domain](https://arxiv.org/abs/2512.08657)
*Renato Cordeiro Ferreira,Aditya Dhinavahi,Rowanne Trapmann,Willem-Jan van den Heuvel*

Main category: cs.SE

TL;DR: 该论文通过构建海洋守护系统，展示了在机器学习系统中复用软件架构的经验。


<details>
  <summary>Details</summary>
Motivation: 提升机器学习系统的架构复用性，降低开发复杂度。

Method: 采用Ports and Adapters模式，基于单一代码库构建多个微服务。

Result: 成功实现架构复用，为同类系统提供实践参考。

Conclusion: 推荐软件与数据工程师采用六边形架构模式构建机器学习系统。

Abstract: ML-Enabled Systems (MLES) are inherently complex since they require multiple components to achieve their business goal. This experience report showcases the software architecture reusability techniques applied while building Ocean Guard, an MLES for anomaly detection in the maritime domain. In particular, it highlights the challenges and lessons learned to reuse the Ports and Adapters pattern to support building multiple microservices from a single codebase. This experience report hopes to inspire software engineers, machine learning engineers, and data scientists to apply the Hexagonal Architecture pattern to build their MLES.

</details>


### [13] [RESTifAI: LLM-Based Workflow for Reusable REST API Testing](https://arxiv.org/abs/2512.08706)
*Leon Kogler,Maximilian Ehrhart,Benedikt Dornauer,Eduard Paul Enoiu*

Main category: cs.SE

TL;DR: RESTifAI 是一种基于大语言模型的工具，用于生成可复用且支持CI/CD的REST API测试，兼顾正向与负向测试场景。


<details>
  <summary>Details</summary>
Motivation: 现有工具多关注内部服务器错误，缺乏对可复用性、断言复杂性和集成支持的优化。

Method: 利用LLM系统构建有效测试路径（happy path）并推导负向用例，验证功能正确性与输入鲁棒性。

Result: 性能媲美最新LLM工具AutoRestTest与LogiAgent，并在工业服务中验证适用性。

Conclusion: RESTifAI解决了现有工具在可复用性、断言和集成方面的局限，已开源供社区使用。

Abstract: With this paper, we introduce RESTifAI, an LLM-driven approach for generating reusable, CI/CD ready REST API tests, following the happy-path approach. Unlike existing tools that often focus primarily on internal server errors, RESTifAI systematically constructs valid test scenarios (happy paths) and derives negative cases to verify both intended functionality (2xx responses) and robustness against invalid inputs or business-rule violations (4xx responses). The results indicate that RESTifAI performs on par with the latest LLM tools, i.e., AutoRestTest and LogiAgent, while addressing limitations related to reusability, oracle complexity, and integration. To support this, we provide common comparative results and demonstrate the tool's applicability in industrial services. For tool demonstration, please refer to https://www.youtube.com/watch?v=2vtQo0T0Lo4. RESTifAI is publicly available at https://github.com/casablancahotelsoftware/RESTifAI.

</details>


### [14] [Multicalibration for LLM-based Code Generation](https://arxiv.org/abs/2512.08810)
*Viola Campos,Robin Kuschnereit,Adrian Ulges*

Main category: cs.SE

TL;DR: 本文研究多校准方法在代码大模型中的应用，以提升其置信度评分与实际正确率的一致性。


<details>
  <summary>Details</summary>
Motivation: 随着AI代码生成普及，需确保模型置信度真实反映代码正确概率。

Method: 在三个基准上测试四种多校准方法，使用Qwen3 Coder等最新代码大模型。

Result: 多校准相较未校准和基线方法分别提升1.03和0.37技能分数。

Conclusion: 多校准有效提升校准性能，并公开数据集供后续研究。

Abstract: As AI-based code generation becomes widespread, researchers are investigating the calibration of code LLMs - ensuring their confidence scores faithfully represent the true likelihood of code correctness. To do so, we investigate multicalibration, which can capture additional factors about a coding problem, such as complexity, code length, or programming language used. We study four multicalibration approaches on three function synthesis benchmarks, using latest-generation code LLMs (Qwen3 Coder, GPT-OSS, DeepSeek-R1-Distill). Our results demonstrate that multicalibration can yield distinct improvements over both uncalibrated token likelihoods (+1.03 in skill score) and baseline calibrations (+0.37 in skill score). We study the influence of the aforementioned factors in ablations, and make our dataset (consisting of code generations, likelihoods, and correctness labels) available for future research on code LLM calibration.

</details>


### [15] [SimpleDevQA: Benchmarking Large Language Models on Development Knowledge QA](https://arxiv.org/abs/2512.08867)
*Jing Zhang,Lianghong Guo,Yanlin Wang,Mingwei Liu,Jiachi Chen,Yuchi Ma,Ensheng Shi,Terry Yue Zhuo,Hongyu Zhang,Zibin Zheng*

Main category: cs.SE

TL;DR: 本文提出SimpleDevQA基准，用于评估LLM在软件开发知识问答中的能力，并发现代码LLM表现优于通用LLM，RAG可提升准确率。


<details>
  <summary>Details</summary>
Motivation: 现有基准覆盖范围有限且缺乏真实用户查询，难以全面评估LLM的开发知识问答能力。

Method: 通过三阶段流程从真实对话中构建多语言、简洁可验证的问答对基准SimpleDevQA。

Result: 实验表明代码LLM优于通用LLM，RAG平均提升11.3%准确率，且LLM自信度与准确率正相关。

Conclusion: 构建贴近真实场景的开发知识问答基准有助于更准确评估和提升LLM在此任务上的表现。

Abstract: The Development Knowledge Question Answering (Dev Knowledge QA) task aims to provide natural language answers to knowledge-seeking questions during software development. To investigate its importance and to what extent it has been explored, we analyze real user-LLM dialogues from WildChat and find that: (1) The Dev Knowledge QA task accounts for 39.6% of interactions(highest among all tasks), revealing broad knowledge needs beyond code generation (32.3%). (2) Only 27.5% of real Dev Knowledge QA dialogues focus on code understanding, leaving out development knowledge-seeking. (3) Only 17.1% of real-world Dev Knowledge QA dialogues can be used for constructing a benchmark. Existing benchmarks have two primary limitations for evaluating the Dev Knowledge QA capability of LLMs. First, existing benchmarks offer a limited development knowledge scope, mainly focusing on code understanding and neglecting broader knowledge during development. Second, some benchmarks are not built from real user queries. To bridge this gap, we design a three-phase pipeline that transforms real-world dialogue into simple development knowledge-seeking QA pairs. Through this pipeline, we introduce SimpleDevQA, a multilingual benchmark derived from real user dialogues. It contains 2,740 QA pairs in three languages (English, Chinese, and Russian), and focuses on questions with unique, short, and verifiable answers for accurate and simple evaluation. Experiments show that: Code LLMs generally outperform general LLMs of similar scale; Knowledge injection with the Retrieval-Augmented Generation (RAG) strategy can boost LLM accuracy by 11.3% on average; LLMs show systematic overconfidence in Dev Knowledge QA, and the answering accuracy of LLMs shows a positive correlation with their stated confidence; Generally, LLMs with stronger code generation performance also exhibit stronger performance in Dev Knowledge QA.

</details>


### [16] [Exploring the Garden of Forking Paths in Empirical Software Engineering Research: A Multiverse Analysis](https://arxiv.org/abs/2512.08910)
*Nathan Cassee,Robert Feldt*

Main category: cs.SE

TL;DR: 本文通过多宇宙分析揭示了软件工程研究中分析决策对结果的深远影响，倡导研究者进行稳健性检验或明确论证方法选择。


<details>
  <summary>Details</summary>
Motivation: 软件工程研究中的分析自由可能导致结果不稳健和不可复现，需评估其风险。

Method: 对一篇已发表的MSR研究进行多宇宙分析，系统测试3072种分析路径。

Result: 仅不到0.2%的分析路径复现原结果，多数得出不同甚至相反结论。

Conclusion: 分析决策对结果影响远超预期，建议结合稳健性检验并采用结构化模型论证方法选择。

Abstract: In empirical software engineering (SE) research, researchers have considerable freedom to decide how to process data, what operationalizations to use, and which statistical model to fit. Gelman and Loken refer to this freedom as leading to a "garden of forking paths". Although this freedom is often seen as an advantage, it also poses a threat to robustness and replicability: variations in analytical decisions, even when justifiable, can lead to divergent conclusions.
  To better understand this risk, we conducted a so-called multiverse analysis on a published empirical SE paper. The paper we picked is a Mining Software Repositories study, as MSR studies commonly use non-trivial statistical models to analyze post-hoc, observational data. In the study, we identified nine pivotal analytical decisions-each with at least one equally defensible alternative and systematically reran all the 3,072 resulting analysis pipelines on the original dataset. Interestingly, only 6 of these universes (<0.2%) reproduced the published results; the overwhelming majority produced qualitatively different, and sometimes even opposite, findings.
  This case study of a data analytical method commonly applied to empirical software engineering data reveals how methodological choices can exert a more profound influence on outcomes than is often acknowledged. We therefore advocate that SE researchers complement standard reporting with robustness checks across plausible analysis variants or, at least, explicitly justify each analytical decision. We propose a structured classification model to help classify and improve justification for methodological choices. Secondly, we show how the multiverse analysis is a practical tool in the methodological arsenal of SE researchers, one that can help produce more reliable, reproducible science.

</details>


<div id='cs.OS'></div>

# cs.OS [[Back]](#toc)

### [17] [NecoFuzz: Effective Fuzzing of Nested Virtualization via Fuzz-Harness Virtual Machines](https://arxiv.org/abs/2512.08858)
*Reima Ishii,Takaaki Fukai,Takahiro Shinagawa*

Main category: cs.OS

TL;DR: NecoFuzz是首个针对嵌套虚拟化逻辑的模糊测试框架，通过规范引导和边界导向生成有效测试用例，显著提升代码覆盖率并发现多个未知漏洞。


<details>
  <summary>Details</summary>
Motivation: 嵌套虚拟化增加主机管理程序复杂性并引入新攻击面，现有模糊测试方法未有效覆盖该领域。

Method: 基于硬件虚拟化规范近似模型，合成接近有效/无效边界的可执行模糊测试虚拟机，并扩展AFL++支持Intel VT-x与AMD-V架构。

Result: 在Intel VT-x和AMD-V上分别实现84.7%和74.2%的嵌套虚拟化代码覆盖率，发现6个未知漏洞，其中2个获分配CVE编号。

Conclusion: NecoFuzz能高效挖掘嵌套虚拟化相关安全缺陷，为云平台提供更可靠的安全保障。

Abstract: Nested virtualization is now widely supported by major cloud vendors, allowing users to leverage virtualization-based technologies in the cloud. However, supporting nested virtualization significantly increases host hypervisor complexity and introduces a new attack surface in cloud platforms. While many prior studies have explored hypervisor fuzzing, none has explicitly addressed nested virtualization due to the challenge of generating effective virtual machine (VM) instances with a vast state space as fuzzing inputs.
  We present NecoFuzz, the first fuzzing framework that systematically targets nested virtualization-specific logic in hypervisors. NecoFuzz synthesizes executable fuzz-harness VMs with internal states near the boundary between valid and invalid, guided by an approximate model of hardware-assisted virtualization specifications. Since vulnerabilities in nested virtualization often stem from incorrect handling of unexpected VM states, this specification-guided, boundary-oriented generation significantly improves coverage of security-critical code across different hypervisors.
  We implemented NecoFuzz on Intel VT-x and AMD-V by extending AFL++ to support fuzz-harness VMs. NecoFuzz achieved 84.7% and 74.2% code coverage for nested virtualization-specific code on Intel VT-x and AMD-V, respectively, and uncovered six previously unknown vulnerabilities across three hypervisors, including two assigned CVEs.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [18] [Modeling the Potential of Message-Free Communication via CXL.mem](https://arxiv.org/abs/2512.08005)
*Stepan Vanecek,Matthew Turner,Manisha Gajbe,Matthew Wolf,Martin Schulz*

Main category: cs.DC

TL;DR: 本文提出了一种结合性能模型与工具链的方法，用于评估CXL.mem在MPI通信中的性能优势，并通过两个应用验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 为解决HPC系统中的内存墙问题，探索CXL.mem在跨节点通信中的优化潜力。

Method: 扩展Mitos工具采集MPI应用内存访问模式，构建细粒度性能模型预测CXL.mem替代传统MPI通信的收益。

Result: 在2D热传导miniapp和HPCG基准测试中验证模型，成功识别可优化的MPI调用并展示CXL.mem带来的性能提升潜力。

Conclusion: 该方法能有效指导针对CXL.mem的通信优化，提升异构内存架构下HPC应用的性能。

Abstract: Heterogeneous memory technologies are increasingly important instruments in addressing the memory wall in HPC systems. While most are deployed in single node setups, CXL.mem is a technology that implements memories that can be attached to multiple nodes simultaneously, enabling shared memory pooling. This opens new possibilities, particularly for efficient inter-node communication.
  In this paper, we present a novel performance evaluation toolchain combined with an extended performance model for message-based communication, which can be used to predict potential performance benefits from using CXL.mem for data exchange. Our approach analyzes data access patterns of MPI applications: it analyzes on-node accesses to/from MPI buffers, as well as cross-node MPI traffic to gather a full understanding of the impact of memory performance. We combine this data in an extended performance model to predict which data transfers could benefit from direct CXL.mem implementations as compared to traditional MPI messages. Our model works on a per-MPI call granularity, allowing the identification and later optimizations of those MPI invocations in the code with the highest potential for speedup by using CXL.mem.
  For our toolchain, we extend the memory trace sampling tool Mitos and use it to extract data access behavior. In the post-processing step, the raw data is automatically analyzed to provide performance models for each individual MPI call. We validate the models on two sample applications -- a 2D heat transfer miniapp and the HPCG benchmark -- and use them to demonstrate their support for targeted optimizations by integrating CXL.mem.

</details>


### [19] [Chopper: A Multi-Level GPU Characterization Tool & Derived Insights Into LLM Training Inefficiency](https://arxiv.org/abs/2512.08242)
*Marco Kurzynski,Shaizeen Aga,Di Wu*

Main category: cs.DC

TL;DR: Chopper框架首次对AMD MI300X上Llama 3 8B训练进行多粒度剖析，揭示频率调控是性能瓶颈主因。


<details>
  <summary>Details</summary>
Motivation: 现有研究缺乏对多GPU LLM训练中通信、计算、内存与功耗协同行为的系统刻画。

Method: 提出Chopper框架，采集并可视化跨粒度GPU内核轨迹与硬件计数器，分析FSDP下八卡MI300X训练行为。

Result: 发现内存确定性可提升频率稳定性，DVFS频率开销是理论与实测性能差距最大来源，超越MFMA利用率损失等其他因素。

Conclusion: 该工作为优化训练框架、功耗策略及未来GPU架构设计提供首个AMD平台上的全景洞察。

Abstract: Training large language models (LLMs) efficiently requires a deep understanding of how modern GPU systems behave under real-world distributed training workloads. While prior work has focused primarily on kernel-level performance or single-GPU microbenchmarks, the complex interaction between communication, computation, memory behavior, and power management in multi-GPU LLM training remains poorly characterized. In this work, we introduce Chopper, a profiling and analysis framework that collects, aligns, and visualizes GPU kernel traces and hardware performance counters across multiple granularities (i.e., from individual kernels to operations, layers, phases, iterations, and GPUs). Using Chopper, we perform a comprehensive end-to-end characterization of Llama 3 8B training under fully sharded data parallelism (FSDP) on an eight-GPU AMD InstinctTM MI300X node. Our analysis reveals several previously underexplored bottlenecks and behaviors, such as memory determinism enabling higher, more stable GPU and memory frequencies. We identify several sources of inefficiencies, with frequency overhead (DVFS effects) being the single largest contributor to the gap between theoretical and observed performance, exceeding the impact of MFMA utilization loss, communication/computation overlap, and kernel launch overheads. Overall, Chopper provides the first holistic, multi-granularity characterization of LLM training on AMD InstinctTM MI300X GPUs, yielding actionable insights for optimizing training frameworks, improving power-management strategies, and guiding future GPU architecture and system design.

</details>


### [20] [CapsuleFS A Multi-credential DataCapsule Filesystem](https://arxiv.org/abs/2512.08067)
*Qingyang Hu,Yucheng Huang,Manshi Yang*

Main category: cs.DC

TL;DR: CapsuleFS (CFS) 是首个在 POSIX 兼容框架内集成多凭证功能的文件系统，基于边缘计算中的全局数据平面，通过 DataCapsule 存储提供支持，适用于真实软件开发场景。


<details>
  <summary>Details</summary>
Motivation: 为边缘计算环境提供一个支持多凭证、POSIX 兼容且功能正确的通用访问接口文件系统。

Method: 将 CFS 架构分为三部分：DataCapsule 服务器负责边缘存储与分发；中间件在可信执行环境中管理写权限；客户端提供跨架构的 POSIX 文件系统接口。

Result: 实验表明 CFS 读写性能较保守，但功能正确性高，适合实际软件开发应用。

Conclusion: CFS 成功实现多凭证 POSIX 文件系统目标，未来可进一步优化实用性。

Abstract: CapsuleFS (CFS) is the first filesystem to integrate multi-credential functionality within a POSIX-compliant framework, utilizing DataCapsule as the storage provider. This innovative system is established based on the Global Data Plane in the area of edge computing. Our comprehensive design and implementation of CFS successfully fulfill the objective of providing a multi-credential Common Access API. The architecture of CFS is methodically segmented into three integral components: Firstly, the DataCapsule server, tasked with the storage, dissemination, and replication of DataCapsules on the edge. Secondly, the middleware, a crucial element running in a Trusted Execution Environment responsible for the enforcement and management of write permissions and requests. Finally, the client component, which manifests as a POSIX-compliant filesystem, is adaptable and operational across many architectures. Experimental evaluations of CFS reveal that, while its read and write performances are comparatively modest, it upholds a high degree of functional correctness. This attribute distinctly positions CFS as a viable candidate for application in real-world software development scenarios. The paper also delineates potential future enhancements, aimed at augmenting the practicality of CFS in the landscape of software development.

</details>


### [21] [Synergizing Monetization, Orchestration, and Semantics in Computing Continuum](https://arxiv.org/abs/2512.08288)
*Chinmaya Kumar Dehury,Lauri Lovén,Praveen Kumar Donta,Ilir Murturi,Schahram Dustdar*

Main category: cs.DC

TL;DR: HERMES框架通过资源货币化、编排和语义互操作性，解决超分布式应用在云边协同中的扩展性、互操作性和信任问题。


<details>
  <summary>Details</summary>
Motivation: 当前解决方案难以满足智能制造、交通和农业等领域对超分布式应用的需求，因其在可扩展性、互操作性和信任方面存在固有局限。

Method: 提出HERMES框架，整合资源货币化、智能编排与语义互操作性，构建开放、无缝且安全的计算连续体环境。

Result: 实现从云端到边缘设备的资源智能调度、分布式市场中的数据与服务货币化，以及通过语义共享知识。

Conclusion: HERMES为新一代高效、可信、自主的分布式应用奠定基础。

Abstract: Industry demands are growing for hyper-distributed applications that span from the cloud to the edge in domains such as smart manufacturing, transportation, and agriculture. Yet today's solutions struggle to meet these demands due to inherent limitations in scalability, interoperability, and trust. In this article, we introduce HERMES (Heterogeneous Computing Continuum with Resource Monetization, Orchestration, and Semantic) - a novel framework designed to transform connectivity and data utilization across the computing continuum. HERMES establishes an open, seamless, and secure environment where resources, from cloud servers to tiny edge devices, can be orchestrated intelligently, data and services can be monetized in a distributed marketplace, and knowledge is shared through semantic interoperability. By bridging these key facets, HERMES lays a foundation for a new generation of distributed applications that are more efficient, trustworthy, and autonomous.

</details>


### [22] [Emulation of Complex Matrix Multiplication based on the Chinese Remainder Theorem](https://arxiv.org/abs/2512.08321)
*Yuki Uchino,Qianxiang Ma,Toshiyuki Imamura,Katsuhisa Ozaki,Patrick Lars Gutsche*

Main category: cs.DC

TL;DR: 本文提出基于Ozaki-II方案的高效方法，在INT8矩阵引擎上模拟单双精度复数矩阵乘法，显著提升速度并支持灵活精度调整。


<details>
  <summary>Details</summary>
Motivation: 现代计算架构中低精度矩阵运算单元吞吐量更高，因此研究如何用低精度硬件高效模拟高精度矩阵乘法具有重要意义。

Method: 基于Ozaki-II框架，设计适用于INT8引擎的单双精度复数矩阵乘法模拟算法。

Result: 在NVIDIA B200 GPU上，相比cuBLAS原生实现，速度提升4.0x–6.5x；且支持精度与速度间的灵活权衡。

Conclusion: 该方法具备成为广泛应用默认算法的潜力，兼顾高性能、能效与精度可调性。

Abstract: Modern computing architectures feature low-precision matrix multiplication units that achieve substantially higher throughput than their high-precision counterparts. Motivated by this architectural trend, the emulation of high-precision matrix multiplication using low-precision hardware has attracted significant interest in the high-performance computing community. Ozaki, Uchino, and Imamura introduced the Ozaki-II scheme as a general framework for emulating matrix multiplication. Building on this framework, Uchino, Ozaki, and Imamura developed high-performance and power-efficient techniques for emulating single- and double-precision real matrix multiplication on INT8 matrix engines. Extending this line of research, the present study proposes high-performance emulation methods for single- and double-precision complex matrix multiplication on INT8 matrix engines, based on the Ozaki-II scheme. On an NVIDIA B200 GPU, the proposed methods achieve 4.0x--5.6x and 4.4x--6.5x speedups over the native single- and double-precision complex matrix multiplication routines from cuBLAS, respectively, for sufficiently large problem sizes. When lower accuracy than that of the standard routine is acceptable, the proposed methods can operate at even higher speed. Conversely, with only a modest increase in computation time, they can also deliver higher accuracy than the standard routines. These properties suggest that the proposed approach has the potential to serve as a default algorithm across a wide range of applications.

</details>


### [23] [Magneton: Optimizing Energy Efficiency of ML Systems via Differential Energy Debugging](https://arxiv.org/abs/2512.08365)
*Yi Pan,Wenbo Qian,Dedong Xie,Ruiyan Hu,Yigong Hu,Baris Kasikci*

Main category: cs.DC

TL;DR: 提出Magneton工具，通过差分能耗调试方法自动定位机器学习系统中的软件能耗浪费问题。


<details>
  <summary>Details</summary>
Motivation: 当前机器学习系统存在因软件设计不佳导致的能耗浪费，但缺乏有效检测工具。

Method: 设计并实现Magneton，基于操作符级别对比相似ML系统的能耗差异，自动识别高耗能代码区域与配置。

Result: 在9个主流ML系统中发现16个已知和8个未知能耗问题，其中7个新问题获开发者确认。

Conclusion: 差分能耗调试可有效揭示ML系统中隐藏的软件级能耗低效，为绿色计算提供新思路。

Abstract: The training and deployment of machine learning (ML) models have become extremely energy-intensive. While existing optimization efforts focus primarily on hardware energy efficiency, a significant but overlooked source of inefficiency is software energy waste caused by poor software design. This often includes redundant or poorly designed operations that consume more energy without improving performance. These inefficiencies arise in widely used ML frameworks and applications, yet developers often lack the visibility and tools to detect and diagnose them.
  We propose differential energy debugging, a novel approach that leverages the observation that competing ML systems often implement similar functionality with vastly different energy consumption. Building on this insight, we design and implement Magneton, an energy profiler that compares energy consumption between similar ML systems at the operator level and automatically pinpoints code regions and configuration choices responsible for excessive energy use. Applied to 9 popular ML systems spanning LLM inference, general ML frameworks, and image generation, Magneton detects and diagnoses 16 known cases of software energy inefficiency and further discovers 8 previously unknown cases, 7 of which have been confirmed by developers.

</details>


### [24] [Basic Lock Algorithms in Lightweight Thread Environments](https://arxiv.org/abs/2512.08563)
*Taras Skazhenik,Nikolai Korobenikov,Andrei Churbanov,Anton Malakhov,Vitaly Aksenov*

Main category: cs.DC

TL;DR: 本文提出针对轻量级线程优化的锁机制，改进TTAS与MCS锁，并推荐使用兼顾性能与通用性的cohort锁。


<details>
  <summary>Details</summary>
Motivation: 轻量级线程因手动上下文切换特性，使传统面向OS线程设计的锁易引发死锁且缺乏跨库通用性。

Method: 改造TTAS与MCS锁以适配轻量级线程的yielding与sleeping机制，并引入结合两者优势的cohort锁结构。

Result: 实验证明不同设置下TTAS与MCS性能差异显著，而cohort锁在各类轻量级线程库中表现稳定均衡。

Conclusion: 为保障正确性与跨库兼容性，推荐在轻量级线程环境中优先采用cohort锁作为通用解决方案。

Abstract: Traditionally, multithreaded data structures have been designed for access by the threads of Operating Systems (OS). However, implementations for access by programmable alternatives known as lightweight threads (also referred to as asynchronous calls or coroutines) have not been thoroughly studied. The main advantage of lightweight threads is their significantly lower overhead during launch and context switching. However, this comes at a cost: to achieve proper parallelism, context switches must be manually invoked in the code; without these switches, new lightweight threads will never be executed.
  In this paper, we focus on the simplest multithreaded data structure: a mutex (also known as a lock). We demonstrate that original implementations for OS threads cannot be used effectively in this new context due to the potential for deadlocks. Furthermore, correctness is not the only concern. In certain languages, such as C++, there are various lightweight thread libraries, each with different implementations and interfaces, which necessitate distinct lock implementations.
  In this work, we present a modification of TTAS and MCS locks for the use from lightweight threads and demonstrate that the two context switch mechanisms of lightweight threads, yielding and sleeping, are crucial. However, the performance of TTAS and MCS may differ significantly depending on the settings. If one wants to have a lock that works well for any library, we suggest using the cohort lock, which strikes a balance between MCS and TTAS by utilizing several MCS queues with a common TTAS.

</details>


### [25] [Model-based Testing of Practical Distributed Systems in Actor Model](https://arxiv.org/abs/2512.08698)
*Ilya Kokorin,Evgeny Chernatskiy,Vitaly Aksenov*

Main category: cs.DC

TL;DR: 本文提出一种基于模型的测试方法，用于高效生成分布式系统（基于Actor模型）的完备测试套件，无需修改代码或干扰运行环境，并以Viewstamped Replication算法实现为例进行验证。


<details>
  <summary>Details</summary>
Motivation: 弥合分布式系统实现与其形式化规范之间的差距，确保实现无缺陷。

Method: 将系统模型解释为有限状态自动机，自动生成覆盖所有状态和转移的测试套件，适用于Actor模型且无需侵入代码或运行环境。

Result: 成功验证了实际系统中使用的Viewstamped Replication复制算法实现。

Conclusion: 该方法能有效提升分布式系统实现的正确性保障，同时保持对原有系统的非侵入性。

Abstract: Designing and implementing distributed systems correctly can be quite challenging. Although these systems are often accompanied by formal specifications that are verified using model-checking techniques, a gap still exists between the implementation and its formal specification: there is no guarantee that the implementation is free of bugs.
  To bridge this gap, we can use model-based testing. Specifically, if the model of the system can be interpreted as a finite-state automaton, we can generate an exhaustive test suite for the implementation that covers all possible states and transitions.
  In this paper, we discuss how to efficiently generate such a test suite for distributed systems written in the actor model. Importantly, our approach does not require any modifications to the code or interfering with the distributed system execution environment. As an example, we verified an implementation of a replication algorithm based on Viewstamped Replication, which is used in a real-world system.

</details>


### [26] [Spatio-Temporal Shifting to Reduce Carbon, Water, and Land-Use Footprints of Cloud Workloads](https://arxiv.org/abs/2512.08725)
*Giulio Attenni,Youssef Moawad,Novella Bartolini,Lauritz Thamsen*

Main category: cs.DC

TL;DR: 通过空间和时间负载转移，显著降低云计算的碳、水和土地足迹。


<details>
  <summary>Details</summary>
Motivation: 减少云计算对环境的影响，尤其是碳、水和土地资源的消耗。

Method: 基于AWS和Azure的真实数据及不同应用负载轨迹进行模拟研究。

Result: 空间转移可减少20%-85%的环境足迹，时间转移效果较小，二者结合效果最佳。

Conclusion: 空间与时间负载转移是降低云环境足迹的有效且稳健策略。

Abstract: In this paper, we investigate the potential of spatial and temporal cloud workload shifting to reduce carbon, water, and land-use footprints. Specifically, we perform a simulation study using real-world data from multiple cloud providers (AWS and Azure) and workload traces for different applications (big data analytics and FaaS). Our simulation results indicate that spatial shifting can substantially lower carbon, water, and land use footprints, with observed reductions ranging from 20% to 85%, depending on the scenario and optimization criteria. Temporal shifting also decreases the footprint, though to a lesser extent. When applied together, the two strategies yield the greatest overall reduction, driven mainly by spatial shifting with temporal adjustments providing an additional, incremental benefit. Sensitivity analysis demonstrates that such shifting is robust to prediction errors in grid mix data and to variations across different seasons.

</details>


<div id='cs.PF'></div>

# cs.PF [[Back]](#toc)

### [27] [Multi-domain performance analysis with scores tailored to user preferences](https://arxiv.org/abs/2512.08715)
*Sébastien Piérard,Adrien Deliège,Marc Van Droogenbroeck*

Main category: cs.PF

TL;DR: 本文提出一种基于概率框架的性能评估方法，通过加权平均分析算法在不同领域中的表现，并定义了四种与用户偏好相关的领域类型，同时为二分类任务开发了新的可视化工具。


<details>
  <summary>Details</summary>
Motivation: 算法性能高度依赖于应用场景的数据分布，跨领域评估后需深入分析加权平均过程以获得更全面的理解。

Method: 采用概率框架将性能视为概率测度，计算加权平均性能，并结合用户偏好参数化分析。

Result: 推导出仅特定评分（如排序评分族）能使汇总性能等于各领域性能的加权算术平均，并据此定义四类领域；同时为二分类任务构建新可视化工具。

Conclusion: 该理论框架有助于更严谨地解释跨领域性能评估结果，并为模型优化提供依据。

Abstract: The performance of algorithms, methods, and models tends to depend heavily on the distribution of cases on which they are applied, this distribution being specific to the applicative domain. After performing an evaluation in several domains, it is highly informative to compute a (weighted) mean performance and, as shown in this paper, to scrutinize what happens during this averaging. To achieve this goal, we adopt a probabilistic framework and consider a performance as a probability measure (e.g., a normalized confusion matrix for a classification task). It appears that the corresponding weighted mean is known to be the summarization, and that only some remarkable scores assign to the summarized performance a value equal to a weighted arithmetic mean of the values assigned to the domain-specific performances. These scores include the family of ranking scores, a continuum parameterized by user preferences, and that the weights to consider in the arithmetic mean depend on the user preferences. Based on this, we rigorously define four domains, named easiest, most difficult, preponderant, and bottleneck domains, as functions of user preferences. After establishing the theory in a general setting, regardless of the task, we develop new visual tools for two-class classification.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [28] [NysX: An Accurate and Energy-Efficient FPGA Accelerator for Hyperdimensional Graph Classification at the Edge](https://arxiv.org/abs/2512.08089)
*Jebacyril Arockiaraj,Dhruv Parikh,Viktor Prasanna*

Main category: cs.AR

TL;DR: NysX是首个面向边缘设备的Nyström-HDC图分类FPGA加速器，通过四项优化显著提升速度、能效与准确率。


<details>
  <summary>Details</summary>
Motivation: 在资源受限的边缘设备上实现实时、高能效的图分类推理，克服现有HDC方法在Nyström近似中的效率瓶颈。

Method: 提出混合地标选择、流式投影矩阵架构、最小完美哈希查找引擎与稀疏感知SpMV引擎四大优化技术。

Result: 在ZCU104 FPGA上相比CPU/GPU基线提速6.85×/4.32×，能效提升169×/314×，平均准确率提升3.4%。

Conclusion: NysX为边缘图分类提供高效、高精度的硬件加速方案，验证了HDC在边缘计算场景的实用性与可扩展性。

Abstract: Real-time, energy-efficient inference on edge devices is essential for graph classification across a range of applications. Hyperdimensional Computing (HDC) is a brain-inspired computing paradigm that encodes input features into low-precision, high-dimensional vectors with simple element-wise operations, making it well-suited for resource-constrained edge platforms. Recent work enhances HDC accuracy for graph classification via Nyström kernel approximations. Edge acceleration of such methods faces several challenges: (i) redundancy among (landmark) samples selected via uniform sampling, (ii) storing the Nyström projection matrix under limited on-chip memory, (iii) expensive, contention-prone codebook lookups, and (iv) load imbalance due to irregular sparsity in SpMV. To address these challenges, we propose NysX, the first end-to-end FPGA accelerator for Nyström-based HDC graph classification at the edge. NysX integrates four key optimizations: (i) a hybrid landmark selection strategy combining uniform sampling with determinantal point processes (DPPs) to reduce redundancy while improving accuracy; (ii) a streaming architecture for Nyström projection matrix maximizing external memory bandwidth utilization; (iii) a minimal-perfect-hash lookup engine enabling $O(1)$ key-to-index mapping with low on-chip memory overhead; and (iv) sparsity-aware SpMV engines with static load balancing. Together, these innovations enable real-time, energy-efficient inference on resource-constrained platforms. Implemented on an AMD Zynq UltraScale+ (ZCU104) FPGA, NysX achieves $6.85\times$ ($4.32\times$) speedup and $169\times$ ($314\times$) energy efficiency gains over optimized CPU (GPU) baselines, while improving classification accuracy by $3.4\%$ on average across TUDataset benchmarks, a widely used standard for graph classification.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [29] [CrowdLLM: Building LLM-Based Digital Populations Augmented with Generative Models](https://arxiv.org/abs/2512.07890)
*Ryan Feng Lin,Keyu Tian,Hanming Zheng,Congjing Zhang,Li Zeng,Shuai Huang*

Main category: cs.MA

TL;DR: 提出CrowdLLM框架，结合预训练大语言模型与生成模型，以低成本构建高保真、多样化的数字人群。


<details>
  <summary>Details</summary>
Motivation: 现有基于大语言模型的数字人群缺乏真实人类群体的准确性与多样性，限制了其在社会模拟等领域的应用效果。

Method: 集成预训练大语言模型与生成模型，通过理论分析与多领域实验验证其性能。

Result: 在众包、投票、用户评分等多个场景中，CrowdLLM在准确性和分布保真度上均优于现有方法。

Conclusion: CrowdLLM能有效构建具有代表性、可扩展且成本低廉的数字人群，逼近真实人群质量。

Abstract: The emergence of large language models (LLMs) has sparked much interest in creating LLM-based digital populations that can be applied to many applications such as social simulation, crowdsourcing, marketing, and recommendation systems. A digital population can reduce the cost of recruiting human participants and alleviate many concerns related to human subject study. However, research has found that most of the existing works rely solely on LLMs and could not sufficiently capture the accuracy and diversity of a real human population. To address this limitation, we propose CrowdLLM that integrates pretrained LLMs and generative models to enhance the diversity and fidelity of the digital population. We conduct theoretical analysis of CrowdLLM regarding its great potential in creating cost-effective, sufficiently representative, scalable digital populations that can match the quality of a real crowd. Comprehensive experiments are also conducted across multiple domains (e.g., crowdsourcing, voting, user rating) and simulation studies which demonstrate that CrowdLLM achieves promising performance in both accuracy and distributional fidelity to human data.

</details>


### [30] [MARINE: Theoretical Optimization and Design for Multi-Agent Recursive IN-context Enhancement](https://arxiv.org/abs/2512.07898)
*Hongwei Zhang,Ji Lu,Yongsheng Du,Yanqin Gao,Lingjun Huang,Baoli Wang,Fang Tan,Peng Zou*

Main category: cs.MA

TL;DR: MARINE框架通过迭代优化推理路径，显著提升大语言模型的单次输出性能，并在计算资源受限下实现参数高效推理。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型受限于单次输出，未能充分发挥其推理潜力，亟需更高效的测试时推理方法。

Method: 提出MARINE框架，将推理过程建模为对参考轨迹的递归优化，结合最小批次与对数增长调度策略。

Result: 在BrowserComp-ZH基准上，685B模型达46.0% pass@1准确率；80B模型性能媲美1000B模型，参数效率提升超十倍。

Conclusion: MARINE为推理优化提供新范式，在固定算力下优于传统采样排序策略，有望大幅提升后训练效率。

Abstract: Large Language Model (LLM)-based agents demonstrate advanced reasoning capabilities, yet practical constraints frequently limit outputs to single responses, leaving significant performance potential unrealized. This paper introduces MARINE (Multi-Agent Recursive IN-context Enhancement), a theoretically grounded framework that reconceptualizes test-time reasoning as iterative refinement of a persistent reference trajectory, fundamentally departing from conventional one-shot or multi-sample paradigms. The MARINE refinement operator systematically converts a base model's pass@N capabilities into near-optimal pass@1 performance. Rigorous theoretical analysis establishes that minimal feasible batches maximize expected performance gains under fixed invocation budgets, while logarithmically growing batch schedules ensure continuous improvement without computational constraints. Comprehensive evaluation on the BrowserComp-ZH benchmark demonstrates state-of-the-art results, with a 685B-parameter implementation achieving 46.0% pass@1 accuracy. Meanwhile, MARINE establishes a new paradigm for parameter-efficient reasoning: an 80B-parameter model augmented with MARINE matches the performance of standalone 1000B-parameter agents, reducing parameter requirements by over an order of magnitude. Notably, within a fixed computational budget, the proposed MARINE delivers higher-quality samples to alignment and optimization processes than traditional sampling-and-ranking strategies. Consequently, it has great potential to boost post-training efficiency.

</details>


### [31] [Probabilistic Multi-Agent Aircraft Landing Time Prediction](https://arxiv.org/abs/2512.08281)
*Kyungmin Kim,Seokbin Yoon,Keumjin Lee*

Main category: cs.MA

TL;DR: 提出了一种概率多智能体飞机着陆时间预测框架，可输出着陆时间分布并提升准确性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 飞机轨迹和交通流的不确定性对预测准确性与可信度构成挑战，需同时提供点估计与不确定性量化，并考虑多机交互影响。

Method: 构建概率型多智能体预测框架，利用注意力机制建模空域内多机交互，并在仁川机场终端空域数据上进行验证。

Result: 相比基线模型，该框架预测更准确，能有效量化不确定性，并通过注意力得分揭示空中交通管制潜在模式，增强模型可解释性。

Conclusion: 所提框架兼顾精度、不确定性量化与可解释性，适用于复杂空域环境下的资源调度与流量管理。

Abstract: Accurate and reliable aircraft landing time prediction is essential for effective resource allocation in air traffic management. However, the inherent uncertainty of aircraft trajectories and traffic flows poses significant challenges to both prediction accuracy and trustworthiness. Therefore, prediction models should not only provide point estimates of aircraft landing times but also the uncertainties associated with these predictions. Furthermore, aircraft trajectories are frequently influenced by the presence of nearby aircraft through air traffic control interventions such as radar vectoring. Consequently, landing time prediction models must account for multi-agent interactions in the airspace. In this work, we propose a probabilistic multi-agent aircraft landing time prediction framework that provides the landing times of multiple aircraft as distributions. We evaluate the proposed framework using an air traffic surveillance dataset collected from the terminal airspace of the Incheon International Airport in South Korea. The results demonstrate that the proposed model achieves higher prediction accuracy than the baselines and quantifies the associated uncertainties of its outcomes. In addition, the model uncovered underlying patterns in air traffic control through its attention scores, thereby enhancing explainability.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [32] [Improvement and Stabilization of Output Voltages in a Vertical Tidal Turbine Using Intelligent Control Strategies](https://arxiv.org/abs/2512.08416)
*Fanambinantsoa Philibert Andriniriniaimalaza,Nour Murad,Randriamaitso Telesphore,Bilal Habachi,Randriatefison Nirilalaina,Manasina Ruffin,Andrianirina Charles Bernard,Ravelo Blaise*

Main category: cs.NI

TL;DR: 本文通过AI混合优化方法提升垂直轴潮流涡轮机驱动的永磁同步发电机输出电压稳定性。


<details>
  <summary>Details</summary>
Motivation: 提高潮流能系统输出电压的稳定性和效率，增强可再生能源应用的可靠性。

Method: 采用TSR-MPPT、ANN模糊控制、PSO及ANN-PSO混合优化策略动态调整涡轮转速。

Result: 在1.5m/s流速下，PSO与ANN-PSO方法相比传统控制显著提升电压稳定性与调节能力。

Conclusion: AI混合优化策略有效增强潮流发电系统的电压稳定性和运行效率。

Abstract: This article investigates on the improvement and stabilization of alternating current (AC) and direct current (DC) output voltages in a Permanent Magnet Synchronous Generator (PMSG) driven by a vertical-axis tidal turbine using advanced control strategies. The research integrates artificial intelligence (AI)-based techniques to enhance voltage stability and efficiency. Initially, the Maximum Power Point Tracking (MPPT) approach based on Tip Speed Ratio (TSR) and Artificial Neural Network (ANN) Fuzzy logic controllers is explored. To further optimize the performance, Particle Swarm Optimization (PSO) and a hybrid ANN-PSO methodology are implemented. These strategies aim to refine the reference rotational speed of the turbine while minimizing deviations from optimal power extraction conditions. The simulation results of a tidal turbine operating at a water flow velocity of 1.5 m/s demonstrate that the PSO-based control approach significantly enhances the voltage stability compared to conventional MPPT-TSR and ANN-Fuzzy controllers. The hybrid ANN-PSO technique improves the voltage regulation by dynamically adapting to system variations and providing real-time reference speed adjustments. This research highlights the AI-based hybrid optimization benefit to stabilize the output voltage of tidal energy systems, thereby increasing reliability and efficiency in renewable energy applications.

</details>


### [33] [Inferring Causal Relationships to Improve Caching for Clients with Correlated Requests: Applications to VR](https://arxiv.org/abs/2512.08626)
*Agrim Bari,Gustavo de Veciana,Yuqi Zhou*

Main category: cs.NI

TL;DR: 提出LFRU缓存策略，动态适应客户端请求相关性，在VR场景下性能显著优于LRU和LFU。


<details>
  <summary>Details</summary>
Motivation: 传统缓存策略如LRU和LFU无法有效处理现实场景中因共享上下文导致的请求相关性，尤其在VR环境中表现不足。

Method: 提出分组客户端请求模型并设计LFRU策略，通过推断因果关系优化缓存替换决策。

Result: LFRU在VR数据集上性能稳定优于LRU与LFU，最高分别提升2.9倍和1.9倍，接近离线最优Belady策略。

Conclusion: LFRU能有效应对具有相关性的请求模式，为边缘缓存提供更优解决方案。

Abstract: Efficient edge caching reduces latency and alleviates backhaul congestion in modern networks. Traditional caching policies, such as Least Recently Used (LRU) and Least Frequently Used (LFU), perform well under specific request patterns. LRU excels in workloads with strong temporal locality, while LFU is effective when content popularity remains static. However, real-world client requests often exhibit correlations due to shared contexts and coordinated activities. This is particularly evident in Virtual Reality (VR) environments, where groups of clients navigate shared virtual spaces, leading to correlated content requests.
  In this paper, we introduce the \textit{grouped client request model}, a generalization of the Independent Reference Model that explicitly captures different types of request correlations. Our theoretical analysis of LRU under this model reveals that the optimal causal caching policy depends on cache size: LFU is optimal for small to moderate caches, while LRU outperforms it for larger caches. To address the limitations of existing policies, we propose Least Following and Recently Used (LFRU), a novel online caching policy that dynamically infers and adapts to causal relationships in client requests to optimize evictions. LFRU prioritizes objects likely to be requested based on inferred dependencies, achieving near-optimal performance compared to the offline optimal Belady policy in structured correlation settings.
  We develop VR based datasets to evaluate caching policies under realistic correlated requests. Our results show that LFRU consistently performs at least as well as LRU and LFU, outperforming LRU by up to 2.9x and LFU by up to1.9x in certain settings.

</details>


### [34] [WikIPedia: Unearthing a 20-Year History of IPv6 Client Addressing](https://arxiv.org/abs/2512.08808)
*Erik Rye,Dave Levin*

Main category: cs.NI

TL;DR: 本文通过分析维基媒体站点的编辑数据，提取了1900万个IPv6地址，用于研究IPv6在不同国家和设备中的普及情况及随时间的变化趋势。


<details>
  <summary>Details</summary>
Motivation: 利用维基媒体站点无意中保存的IPv6地址数据，研究其分配模式、地域分布及设备使用情况。

Method: 从2003年至2024年的维基媒体站点编辑记录中提取IPv6地址，并分析其在不同语言对应国家的普及率、时间增长趋势及EUI-64地址使用情况。

Result: 成功提取并分析了1900万个唯一IPv6地址，揭示了IPv6在全球范围内的采用趋势及客户端设备的地址分配特征。

Conclusion: 维基媒体站点的数据为研究IPv6地址分配和演变提供了独特且有价值的长期观测视角。

Abstract: Due to their article editing policies, Wikimedia sites like Wikipedia have become inadvertent time capsules for IPv6 addresses. When Wikimedia users make edits without signing into an account, their IP addresses are used in lieu of a username. Wikimedia site dumps therefore provide researchers with over two decades worth of timestamped client IPv6 addresses to understand address assignments and how they have changed over time and space.
  In this work, we extract 19M unique IPv6 addresses from Wikimedia sites like Wikipedia that were used by editors from 2003 to 2024. We use these addresses to understand the prevalence of IPv6 in countries corresponding to Wikimedia site languages, how IPv6 adoption has grown over time, and the prevalence of EUI-64 addressing on client devices like desktops, laptops, and mobile phones.

</details>
